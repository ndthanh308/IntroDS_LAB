\section{}
To give the proofs of lemmas in the paper, we first introduce a few results including the definition of a convexity and a lemma for it. 
\begin{definition}[q-convex]\footnote{
The notion of q-convex here does not imply the convexity of $f$, nor is it limited to the uniqueness of $x^*$.
This definition is different from ``quasi-convex'', which means something else, e.g., see \citep{Kiwiel_quasi_convex,quasi_convex_hu2019convergence}. In particular, $f(\lambda x + (1-\lambda)y)\le \max\{f(x), f(y)\}$ holds for all $x,y\in \mathcal{D}(f)$ and any $\lambda \in[0,1]$. }  
Let a differentiable function $f$ be defined by $f:\RR^d \to \RR$. 
Let $x^*=\min_{x\in \RR^d }f(x)$. In addition, $f'(x^*)=0$.
If 
\begin{equation}\label{eq:qconvex1}
f(x^*) \ge f(x) + f'(x)^\tr (x^*-x)
\end{equation}
holds for all $x\in \RR^d$, then we call $f$ {\em q-convex}. 
\end{definition}

\begin{lem}\label{lem:qconvexiif}
Assume $f$ is q-convex. Recall that $x^*$ is the optimum with $f(x^*)\le f(x)$ for all $x$. 
For any $ x\in \RR^d$, let $y = \lambda x^* + (1-\lambda)x$. We have,
\[
-f'(y)^\tr (x^*-x)
\begin{cases}
        \ge 0, & \text{if}\ \lambda\in[0,1]; \\
      <0, & \text{otherwise.}
    \end{cases}
\]


\end{lem}
That is, any intermediate point between a current point $x$ and the optimum $x^*$ is guaranteed to have a negative gradient that is positively correlated with the direction of $x^*-x$. Extrapolation outside of the two points gives a reverse relationship. 

\begin{proof}
Setting $x=y$ in equation \ref{eq:qconvex1}, this still holds. Thus
\begin{align*}
    f(x^*) &\ge f(y) + f'(y)^\tr (x^*-y)\\
    &= f(y) + f'(y)^\tr (x^*-(\lambda x^* + (1-\lambda)x))\\
    &= f(y) + (1-\lambda) f'(y)^\tr (x^*-x).
\end{align*}
Note that $f(y)\ge f(x^*)$. 
Thus 
\[
 -(1-\lambda)f'(y)^\tr (x^*-x) \ge 0.
\]

\end{proof}

\begin{lem}\label{lem:g2andf2}
Refer to Definition \ref{def:L-lambda-smooth} for the generation process of $g_t(x)$. 
Let $\mathcal{D}$ be any distribution that satisfies $\EE_{\mathcal{D}} [g_t(x)|x] = f'(x)$ for all $x \in \RR^d$. Given $f$ and $\mathcal{D}$, we define $
\sigma_v^2 = \min_x \EE_{\mathcal{D}}\norm{g_t(x)-f'(x)}^2$.
For all $x$, we have 
\[
\EED \norm{g_t(x) - f'(x)}^2=\EED \norm{g_t(x)}^2 - \norm{f'(x)}^2;  \quad \EE_{\mathcal{D}}\norm{g_t(x)}^2\ge \norm{f'(x)}^2 + \sigma_v^2.
\]
\end{lem}
\begin{proof}
This can be seen from 
\begin{align*}
\sigma_v^2 &\le 
\EE_{\mathcal{D}}\left[\norm{g_t(x) - f'(x)}^2|x\right] \\
&=
\EE_{\mathcal{D}}\left[\norm{g_t(x)}^2|x\right] - 2 \EE_{\mathcal{D}}\left[g_t(x)^\tr f'(x)|x\right] + \EE_{\mathcal{D}}\left[\norm{f'(x)}^2 |x\right]\\
&=
\EED\norm{g_t(x)}^2 - 2 \EED[g_t(x)]^\tr \EED f'(x) + \norm{f'(x)}^2 \\
&=
\EED\norm{g_t(x)}^2 - 2 \EED[g_t(x)]^\tr f'(x) + \norm{f'(x)}^2 \\
&=
\EED\norm{g_t(x)}^2 - 2 \norm{f'(x)}^2 + \norm{f'(x)}^2 \\
&=
\EED\norm{g_t(x)}^2 -  \norm{f'(x)}^2,  
\end{align*}
where the conditional on $x$ is omitted whenever there is no need to emphasize.
\end{proof}

\subsection{Proof for Lemma \ref{lem:ED_avg}}\label{appendix:ED_avg}
\begin{proof}
Using the familiar notation of $\bar{X}$ to denote the empirical mean of multiple i.i.d (gradient) samples and $X$ refers to the stochastic gradient in this context.  We have
\begin{align*}
\EED \norm{\avgx}^2 &=  \EED \norm{\frac{1}{m}\sum_{i=1}^m
g(x|X_i,Y_i)}^2 \\
&=  \EED   
\norm{\bar{X}}^2 \\
&= \EED   
\sum_{i=1}^d\bar{X}[i]^2 \\
&=  
\sum_{i=1}^d\EED \bar{X}[i]^2 \\
&=  
\sum_{i=1}^d Var(\bar{X}[i]) + (\EED\bar{X}[i])^2 \\
&=  
\sum_{i=1}^d \frac{1}{m}Var({X}[i]) + (\EED{X}[i])^2 
\end{align*}
where we used $Var(\bar{X})=\frac{1}{m}Var(X)$ because all the samples are i.i.d. It follows that 
\begin{align*}
\EED \norm{\avgx}^2&=  
\sum_{i=1}^d \frac{1}{m}\left[\EED{X}[i]^2 - (\EED X[i])^2\right] + (\EED{X}[i])^2 \\
&=  
\frac{1}{m}\sum_{i=1}^d \EED{X}[i]^2  + \frac{m-1}{m}\sum_{i=1}^d(\EED{X}[i])^2 \\
&= \frac{1}{m}\EED \norm{g_t(x)}^2 + \left(1-\frac{1}{m}\right) \norm{f'(x_t)}^2. 
\end{align*}
where in the third line we plugged in the original notations. 
\end{proof}
This lemma shows that the expected squared norm of the averaged gradient is not just shrinking by a factor of one over the batch size, but also it adds $(1-1/m)$ times the squared norm of the true gradient. The appearance is a convex sum of the two. 

\subsection{Proof of Lemma \ref{lem:utrongly_norm_grad}}\label{appendix:u_norm_grad}
\begin{proof}
Because $f$ is $\mu$-strongly quasi-convex, we have 
\[
f(x^*) \ge f(x) + \nabla f(x)^\top (x^* -x ) + \frac{\mu}{2} \norm{x^* -x}^2.
\]
Let $g(x) = f(x) -\frac{\mu}{2}\norm{x-x^*}^2$. Note $g'(x^*)=0$. 
We have, $g(x^*)\ge g(x)+g'(x)^\tr (x^*-x)$ holds if and only if $f$ is $\mu$-strongly quasi-convex. Thus $g$ is q-convex and we have $-g'(y)^\tr (x^*-x) \ge 0$, for $y=\lambda x^*+ (1-\lambda) x$, $\forall \lambda\in[0,1]$, according to Lemma \ref{lem:qconvexiif}. Note that $y-x^*=(1-\lambda) (x-x^*)$. Thus $g'(y)^\tr (y-x^*) \ge 0$. 
Expanding this inequality we have
\begin{align*}
    0&\le  g'(y)^\tr (y-x^*) \\
    &= (f'(y) - \mu(y-x^*))^\tr (y-x^*). 
\end{align*}
This gives $f'(y)^\tr (y-x^*)\ge \mu\norm{y-x^*}^2$. Using Cauchy–Schwarz inequality, we have 
\begin{align*}
\norm{f'(y)} \norm{y-x^*}\ge f'(y)^\tr (y-x^*)\ge \mu\norm{y-x^*}^2. 
\end{align*}
If $y=x^*$, the lemma holds; otherwise, 
dividing by $\norm{y-x^*}$, we have 
$\norm{f'(y)}\ge \mu\norm{y-x^*}$. Since this holds for $y$ generated for any $\lambda\in[0,1]$, setting $\lambda=1$ also holds. Thus this holds for any $x\in \RR^d$.   
\end{proof}

\subsection{Proof of Lemma \ref{lem:ES_qLsmooth}}

\begin{proof}\label{appendix:es_qls}
We have
\[
\sigma^2_v + \norm{f'(x)}^2 \le \EE_{\mathcal{D}}\norm{g_t(x)}^2 \le 2L (f(x) - f(x^*))  + \lambda \norm{x-x^*}^2+ \sigma^2, 
\]
where the first inequality uses Lemma \ref{lem:g2andf2} and the second the $L$-$\lambda$ smoothness condition.  
\end{proof}


\subsection{RW-inv}\label{exp:rwinv}

In this random-walk problem, RW-inv, the representation inverts the tabular representation, and switches zero for one, and one for zero for the features. Then normalization is applied row-wise to the feature matrix.
First we run the baselines and measured the RMSPBE, and the results are shown in Figure \ref{fig:rwinvpbe}. The hyper-parameters were the same as \citep{martha2020gradient}. If we compare with the Figure 1 for this problem in their paper,  they showed that TDRC performed the best. However, note that there the metrics were taken for 3000 steps. As learning extends more steps,  TDC gets much better than TDRC. HTD is pretty close to TD. GTD2 and Vtrace have the largest RMSPBE after 4000 steps.


% Figure environment removed

Next we run all the algorithms and measure the RMSVE in Figure \ref{fig:rwinvrmsve}. Surprisingly, after the initial learning stage, GTD2 and TDC perform the best in all the baselines, much better than TD, TDRC and the other baselines. Thus we should take care in interpreting MSPBE. A large MSPBE does not necessarily mean bad learning, e.g., GTD2, as shown in Figure \ref{fig:rwinvpbe}. Impression GTD used a batch size of 32 and a step-size of 1.0. After the initial learning stage, Impression GTD performs the best, even better than the unusually fast GTD2 and TDC in this case.

% Figure environment removed


% Figure environment removed

The batch size effect for Impression GTD is shown in Figure \ref{fig:rwinv_batchsize}. 
The step-size for Impression GTD agents is uniformly 1.0.   We also plotted GTD2, the best performing agent for this problem. 
This shows that a smaller batch size like 8 is slower in learning. Impression GTD agents with batch sizes 16, 32 and 64 all perform much better than GTD2 after the initial learning stage.



% Figure environment removed

Figure \ref{fig:rwinv_alpha} shows the effect of step-size for Impression GTD. The batch size is 32. There is a convergence rate overturn similar to what we observed in the RW-tab problem. This suggests a decaying step-size  can further improve the convergence rate of Impression GTD.  

\subsection{RW-dep}\label{exp:rwdep}
In this random-walk problem, 
the representation for states 1 and 5 is the set of the two unit basis vectors. For states 2 and 4, the feature vectors are [1, 1, 0] and [0, 1, 1], respectively. State 3 is [1, 1, 1]. Finally, each feature vector is $\ell_2$ normalized.


First, we compared the algorithms in the MSPBE measure and the results are shown in Figure \ref{fig:rwdep_pbe}. Impression GTD used a batch size of 32 and a step-size of 0.05. The baseline algorithms used the same hyper-parameters as in the TDRC code base. TDRC performed better than the other baselines. However, the advantage of TDRC over TD/HTD is small. This is probably due to that TDRC mixes TD and TDC via regularizing the helper iterator \citep{martha2020gradient}. \footnote{Mixing the on-policy TD and the off-policy GTD algorithms is also the principle under which Hybrid TD (HTD) was designed \citep{HTD_leah,htd_marhta}.}. After about 2500 steps,  Impression GTD is much faster than TDRC and the others.


% Figure environment removed

We then plotted the RMSVE metric in Figure \ref{fig:rwdep_rmsve}. It shows that Vtrace finds a solution that is far from the others, although under the MSPBE measure the solution is not very far, shown in Figure \ref{fig:rwdep_pbe}. This is another example that we should interpret the MSPBE measure carefully. GTD2 and TDC are faster than TDRC after about 2300 steps. GTD2 is also faster than TDC. This is surprising because usually GTD2 is slower than TDC in terms of the MSPBE. Impression GTD is still faster than GTD2 and the others after an initial learning time.


% Figure environment removed

Figure \ref{fig:rwdep_batchsize} shows the effect of the batch size for Impression GTD. We also plotted the best performing GTD2. The step-size for all the ImpressoinGTD agents is 0.05. For this problem, the learning is not very hard in the beginning because of the generalization between the features. What’s interesting for this problem is that for all the algorithms, the learning deteriorates and the RMSVE metric makes a way back. This should be because the features are strongly correlated for this representation. Nonetheless,  ImpressoinGTD still learns much better than GTD2 whether in terms of the lowest RMSVE or the final plateau for all the batch sizes.  Bigger batch sizes (e.g., 32 and 64) perform slightly better than smaller ones  (e.g., 8, 16). 


% Figure environment removed

Figure \ref{fig:rwdep_stepsize} shows the effects of the step-size for Impression GTD. The batch size for all the Impression GTD agents is uniformly 32. A bigger step-size like 0.5 leads to fastest learning in the beginning. However, because this learning task benefits from the generalization in the representation, the other smaller step-sizes can also quickly minimize the learning error. We don’t know exactly why the lowest point for the blue line (step-size 0.5) in the beginning is higher than the others. Probably it is because a big step-size couldn’t go all the way to reach the bottom of the valley in the loss.  Smaller step-sizes like 0.1, 0.05 and 0.025 seems to have a lower low as we decrease the step-size. For example, the step-size 0.025 is slow before about 4000 steps. However, after that, the drop in the error is fast and the final solution is the best.


% Figure environment removed


\subsection{Baird counterexample}\label{exp:baird}
We use the 7-state version of the problem by \citet{sutton2018reinforcement}. 
Although appearing simple, this problem in fact is very challenging for off-policy learning \citep{baird1995residual}. The discount factor is reduced from 0.99 to 0.9 to induce more contraction, and make it not so challenging in terms of the convergence rate. This rules out the possibility that GTD and TDC are slow because the problem is too challenging.  

The performance of Impression GTD algorithms is shown in Figure \ref{fig:baird_results}. For a clear visualization, we only show the curves of GTD, TDC and TDRC. The other baselines were not as fast as the chosen baselines. 
For TDRC, all the hyper-parameters are used the same as the TDRC paper, which were selected by the original authors from an extensive sweep search. The $\alpha$ was 0.03125, $\beta$ (the regularization factor) was 1.0, and $\eta$ was 1.0, too. We also tried bigger values of $\alpha$ (without changing $\beta$ or $\eta$), including 0.04 and 0.05. They had either much bigger variances or diverged. Impression GTD used a batch size of 10. All the algorithms are corrected by $\rho$, the importance sampling ratio.   

The Impression GTD didn’t start learning until 100 steps of following the behavior policy, filling the buffer with some content. Impression GTD agents learn very fast, with a steep drop in the value estimation error, all the way down to near zero. With a small $\alpha$ like 0.001, the algorithm converges slower, but it also drives the RMSVE down to near zero. The curves of Impression GTD exhibit the pattern of linear convergence rate that we prove in Section \ref{sec:theory}.


% Figure environment removed