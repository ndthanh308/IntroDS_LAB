\documentclass[twoside,11pt]{article}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

\usepackage{jmlr2e}
%\usepackage[abbrvbib, preprint]{jmlr2e}

% Definitions of handy macros can go here

%% Choose your variant of English; be consistent
\usepackage[american]{babel}
% \usepackage[british]{babel}

%% Some suggested packages, as needed:
%\usepackage{natbib} % has a nice set of citation styles and commands
    % \bibliographystyle{plainnat}
%\bibliographystyle{ieee_fullname}
\renewcommand{\bibsection}{\subsubsection*{References}}
\usepackage{mathtools} % amsmath with fixes and additions
% \usepackage{siunitx} % for proper typesetting of numbers and units
\usepackage{booktabs} % commands to create good-looking tables
\usepackage{tikz} % nice language for creating drawings and diagrams

%% Provided macros
% \smaller: Because the class footnote size is essentially LaTeX's \small,
%           redefining \footnotesize, we provide the original \footnotesize
%           using this macro.
%           (Use only sparingly, e.g., in drawings, as it is quite small.)
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
% \usepackage[colorlinks=true]{hyperref}      % hyperlinks
% \usepackage{url}            % simple URL typesetting
%\usepackage{booktabs}       % professional-quality tables  
\usepackage{amsmath}
\usepackage{bbm}
% blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage[export]{adjustbox}
\usepackage{wrapfig}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{enumitem}
% \usepackage{apptools}
% \AtAppendix{\counterwithin{lemma}{section}}
\usepackage{algorithm}
\usepackage{algpseudocode}

\newtheorem{thm}{Theorem}
\newtheorem{lem}[theorem]{Lemma}
\newtheorem{assumption}{Assumption}

\renewcommand{\SS}{\mathcal{S}}
\renewcommand{\AA}{\mathcal{A}}
\newcommand{\PP}{\mathbbm{P}}
\newcommand{\RR}{\mathcal{R}}
\newcommand{\EE}{\mathbbm{E}}
\newcommand{\EED}{\mathbbm{E}_{\mathcal{D}}}
\newcommand{\Efgrad}{\mathbbm{E}\norm{f'(x_t)}^2}
\newcommand{\normg}{\norm{g_t(x_t)}^2}
\newcommand{\norm}[1]{\left\|#1\right\|}
\newcommand{\tr}{\top}
\newcommand{\avg}{\bold{avg}_m(x_t)}
\newcommand{\avgx}{\bold{avg}_m(x)}
\newcommand{\avgt}{\bold{avg}_m(\theta_t)}
% Heading arguments are {volume}{year}{pages}{submitted}{published}{author-full-names}
\newcommand{\oneexptd}{R1-GTD\,}

%\jmlrheading{1}{2000}{1-48}{4/00}{10/00}{Marina Meil\u{a} and Michael I. Jordan}

% Short headings should be running head and authors last names

\ShortHeadings{A new Gradient TD algorithm with only one step-size}{**}
\firstpageno{1}

\begin{document}

\title{A new Gradient TD Algorithm with only One Step-size:\\  Convergence Rate Analysis using $L$-$\lambda$ Smoothness
}
% \title{A New Gradient TD Algorithm with Only One Step-size:\\
% Baird Counterexample Mystery is Solved
% }

\author{\name Hengshuai Yao\email hengshuai.yao@sony.com \\
       \addr SonyAI
       }

\editor{***}

\maketitle

\input{abstract}

\begin{keywords}
  Off-policy learning, Gradient-based Temporal Difference learning, The NEU objective, MSPBE, SGD, Convergence rate analysis, Batch size effect, expected smoothness, linear convergence rate %Baird Counterexample 
\end{keywords}


\input{introduction}

\input{background}

\input{impresssion_GTD}

\input{proofs_imgtd}

\input{experiments}

\input{conclusion}

% Acknowledgements should go at the end, before appendices and references

\acks{
We appreciate Thomas Walsh, James MacGlashan, Peter Stone, Varun Kompella, Dustin Morrill and Ishan Durugkar for insightful discussions on the topics of off-policy learning and deep reinforcement learning, who also helped improve the paper in many ways. Tom spotted a problem in an early draft of Theorem 2. Tom and Peter also gave lots of advice that greatly helped improve the presentation of the paper.   
We would like to thank Declan Oller for the pointer to the paper by \citet*{lihong_kernel_sim}, which helped improve our understanding of Impression GTD. %At the time, an internal document at Sony AI of this paper was shared and reviewed, with writing,  experiments, and exactly the same algorithm layout presented herein (with the $\bold{sim}$ notation and independence sampling). 
We appreciate Sina Ghiassian, Andrew Patterson, Shivam Garg, Dhawal Gupta, Adam White and Martha White for making their TDRC code available, and Shangtong Zhang for the Baird counterexample, both of which greatly facilitate the experiment studies in this paper. We appreciate Shangtong Zhang also for helpful discussions on importance sampling and off-policy learning.  
}

% Manual newpage inserted to improve layout of sample file - not
% needed in general before appendices/bibliography.

%\newpage

% \appendix
% \section*{Appendix A.}
% \label{app:theorem}

% Note: in this sample, the section number is hard-coded in. Following
% proper LaTeX conventions, it should properly be coded as a reference:

%In this appendix we prove the following theorem from
%Section~\ref{sec:textree-generalization}:

% In this appendix we prove the following theorem from
% Section~6.2:

% \noindent
% {\bf Theorem} {\it Let $u,v,w$ be discrete variables such that $v, w$ do
% not co-occur with $u$ (i.e., $u\neq0\;\Rightarrow \;v=w=0$ in a given
% dataset $\dataset$). Let $N_{v0},N_{w0}$ be the number of data points for
% which $v=0, w=0$ respectively, and let $I_{uv},I_{uw}$ be the
% respective empirical mutual information values based on the sample
% $\dataset$. Then
% \[
% 	N_{v0} \;>\; N_{w0}\;\;\Rightarrow\;\;I_{uv} \;\leq\;I_{uw}
% \]
% with equality only if $u$ is identically 0.} \hfill\BlackBox

% \noindent
% {\bf Proof}. We use the notation:
% \[
% P_v(i) \;=\;\frac{N_v^i}{N},\;\;\;i \neq 0;\;\;\;
% P_{v0}\;\equiv\;P_v(0)\; = \;1 - \sum_{i\neq 0}P_v(i).
% \]
% These values represent the (empirical) probabilities of $v$
% taking value $i\neq 0$ and 0 respectively.  Entropies will be denoted
% by $H$. We aim to show that $\fracpartial{I_{uv}}{P_{v0}} < 0$....\\

% {\noindent \em Remainder omitted in this sample. See http://www.jmlr.org/papers/ for full paper.}


\vskip 0.2in
\bibliography{ref}

\appendix
\input{appendix}

\end{document}