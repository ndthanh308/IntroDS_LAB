\section{Conclusion and Discussion}\label{sec:conclusion}
We proposed a new Gradient TD algorithm for off-policy learning in reinforcement learning. 
Because of the use of two or more independent TD errors, weighted by the similarity between the feature vectors of the states where the TD errors happen, off-policy learning becomes a fully supervised learning problem once the data has been collected. 
In this paper, we have focused on the formulation and optimization parts of the problem, and established the convergence and convergence rates of the resulting {\em truly single-time-scale} SGD algorithm for off-policy learning. With only one step-size parameter, the new algorithm is much easier to use for practitioners.  

For the diminishing step-size, the convergence of the new GTD algorithm is established under the same MDP conditions as GTD \citep{gtd}. We remove byproducts (e.g., $\sqrt{\eta}I$ in the $G$ matrix of their Theorem 4.1) due to the two-time-scale formulation that is not essential for the convergence guarantee, which greatly simplifies the algorithm analysis.


For the constant step-size case, our first rate shows that Impression GTD converges at least as fast as $O(1/t)$. The rates of GTD, GTD2 and TDC were established previously in a saddle-point formulation  \citep{bo_gtd_finite}, and a two-time-scale formulation \citep{dalal2018finite_twotimescale,dalal2020tale_twotimescale}. Both analysis show the three GTD algorithms converge slower than $1/\sqrt{t}$. 

Our second rate analysis is a linear rate result for Impression GTD under constant step-sizes. The analysis draws on a SGD result that is novel in two ways. First, the expected smoothness condition \citep{gower2019sgd_general} is relaxed to a new class of loss functions, in the sense that the smoothness measure is allowed to have some extra noise. Second, our rate yields a tighter bound, introducing a squared term of the strong quasi-convexity factor. This technique can be used to improve bounds of the SGD rate in literature, e.g., Theorem 3.1 of \citep{gower2019sgd_general}, as we showed in the paper. 
Besides Impression GTD, we also proved the convergence rates of three other GTD algorithms, including the one by \citet{ptd_yao} and another discussed by \citet{gtd}.

The empirical results of on/off-policy learning on three problems show that Impression GTD learns much faster than exiting off-policy TD algorithms. Our parameter studies of the step-size and the batch size shows that the new algorithm is very easy to use. Tuning the single step-size for Impression GTD is much easier than tuning the two step-sizes of existing GTD algorithms. 

Empirical results show that larger step-sizes make Impression GTD converge faster. However, after increasing to certain step-size values, the acceleration gets smaller. The phenomenon is well explained by our theory. In particular, Theorem \ref{thm:rates_all} shows that the step-size should be inverse proportional to the ratio between the variance of the transition features and the batch size. 
This means two things. First, larger batch sizes induce less disturbance into the smoothness measure and as a result, a larger step-size gives a faster convergence. 
Second, increasing the batch size beyond a certain threshold (proportional to the variance of the transition features) does not help with the convergence rate much any more. The rate will then be bounded by the convergence factors of the full gradient descent, assuming the true loss function is given. 

Another phenomenon we observed in the experiments is that larger step-sizes induced larger biases in the end, though faster convergence in the beginning. This can be explained by Theorem \ref{thm:rates_all} as well. In particular, the bias is proportional to 
\[
\frac{\alpha^2}{m}\propto \left(\frac{1}{5\frac{\norm{\Sigma_A}^2}{m} +  4\norm{A}^2}\right)^2 \frac{1}{m}.
\]
Thus the bias reduces as we decrease the batch size $m$. Unless the transition variance is small (comparing to $\norm{A}^2$), the bias also increases when we increase the batch size. 

A limitation of the present paper is that all the theory and experiments are about policy evaluation. It is interesting to extend the results to control and nonlinear function approximation in the future.   
 


    