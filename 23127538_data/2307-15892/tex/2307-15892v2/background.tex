\section{Background, Related Work \& Discussions}\label{sec:background}
In this section, we review MDPs for the problem setting. We also review GTD, GTD2 and TDC algorithms for off-policy learning, as well as some latest works for a discussion. 

\subsection{MDP}
Assume the problem is a Markovian Decision Process (MDP) \citep{bertsekas2012dynamic,sutton2018reinforcement,csaba_book}. The state space is $\SS$ with $N$ possible states. For simplicity, we denote the space by $\SS=\{1, 2, \ldots, N\}$, and a state in $\SS$ is denoted by the integer $i$, and a state sample by $s$. The action space is $\AA$ and an action is denoted by $a$. 
Let $\PP$ be a probability measure assigned to a state $i\in \SS$, which we denote as $\PP(\cdot|i,a)$. Define $R: \SS\times \AA \to \RR$ as the reward function, where $\RR$ is the real space. The reward from a state $i$ to state $j$ is denoted by $r(i,j)$.  
Let $\gamma \in (0,1)$ be the discount factor. 
 
We consider the general case of stochastic policies. Denote a stochastic policy by a probability measure $\pi$ applied to a state $s$: $\pi(\cdot|s) \to [0,1]$. 
At a time step $t$,
the agent observes the current state $s_t$ and takes an action $a_t$. The environment provides the agent with the next state $s_{t+1}$ and a scalar reward $r_{t+1}=R(s_t, a_t)$. The main task of the policy evaluation agent is to approximate the value function that is associated with a policy $\pi$:
\begin{gather*}
  V_\pi(s) = \EE _{\pi}\Big[\sum_{t=0}^{\infty } \gamma^t  r_{t+1} \Big],\notag
\end{gather*}
where $a_t \sim \pi(\cdot|s_t)$ and $s_{t+1} \sim \PP(\cdot|s_t,a_t)$ for all $t\ge 0$. 

\subsection{The NEU objective, Expected GTD and GTD}
Temporal Difference (TD) methods \citep{td} are a class of algorithms for approximating the value function of a policy $\pi$. Using a number of features, we represent the approximation by $\hat{V}^\pi (s)=\phi(s)^\tr  \theta$. Given a transition sample $(s_t, r_t, s_{t+1})$ following $\pi$, the TD($0$) algorithm updates the weight vector by
\[
\theta_{t+1} = \theta_t + \alpha \delta_t \phi_t; \quad \phi_t=\phi(s_t)
\]
where $\delta_t=r_t + \gamma \phi_{t+1}^\tr \theta_t - \phi_t^\tr \theta_t$, called the TD error. Under mild conditions, TD methods are guaranteed to converge to a solution to a linear system of equations, $
\EE[\delta_t\phi_t] = 0$. However, when the distribution of the states is not from policy $\pi$ (the so-called ``off-policy'' learning problem), TD methods can diverge, e.g., see \citep{baird1995residual,sutton1995generalization,bertsekas1996neuro,tsi_td}. 

The Gradient temporal difference (GTD) algorithm \citep{gtd} is guaranteed to converge for off-policy learning with linear function approximation. It minimizes the NEU objective \citep{tdc}, the $\ell_2$ norm of the expected TD update:
\begin{align}
\bold{NEU}(\theta) &= \norm{\EE[\delta\phi]}^2 \nonumber\\
&= (A\theta + b)^\top (A\theta + b), \label{eq:Ab}
\end{align}
where $\EE[\delta\phi]=A\theta + b$, with $A=\EE[\phi(\gamma \phi' - \phi)^\top]$ and $b=\EE[\phi r]$.\footnote{We focus on TD(0) in this paper.} The expectation operator is taken for a transition tuple, $(\phi, r, \phi')$, which follows policy $\pi$. Note that the distribution of the state underlying $\phi$ is not necessarily the stationary distribution of $\pi$ (which would be on-policy). We follow the convention of defining the matrix $A$ in this format as in \citep{tsi_td,bertsekas1996neuro}, which is negative definite in the on-policy case.  

The NEU objective function first appeared in \citep{ptd_yao}. For off-policy learning, the matrix $A$ is not necessarily negative definite. This is the source of the instability and divergence troubles with TD methods. The nice property of NEU is that it introduces a stable O.D.E., by bringing a symmetry into the underlying system. The gradient of the NEU objective is $A^\tr (A\theta +b)=0$, in contrast to the original O.D.E. of TD(0), which is, $A\theta +b=0$. As long as $A$ is non-singular (but it is not necessarily negative definite), the gradient descent update, $\theta_{\tau+1} = \theta_\tau -\alpha A^\tr (A\theta_\tau +b)$ is stable and convergent for some positive step-size $\alpha$. It gives the same solution as the original one to $A\theta+b=0$, i.e., the so-called ``LSTD solution'' \citep{lstd} or ``TD solution''.  Note that this reinforcement learning approach is very special in that this method is usually not used for solving linear system of equations in iterative methods, because the system $A^\tr (A\theta_\tau +b)=0$, called the {\em normal equation}, induces slower iterations than the original one. This is due to that $A^\tr A$ has worse conditioning than $A$ (the condition number is squared). The normal equation is usually only used in iterative methods when $A$ is not a square matrix, e.g., in an over-determined system. The context of off-policy learning makes this method meaningful because the necessity of a stable O.D.E. even though matrix $A$ is a square matrix here.        

\citeauthor{ptd_yao} has a 
 gradient descent algorithm for TD too. It is $O(d^2)$ because it builds data structures in the form of a matrix and a vector from the samples \citep{bradtke1996linear,lspe96,lstd,lspe03} or the inversion of the matrix \citep*{xu2002efficient}. GTD \citep{gtd} reduces the complexity of \citeauthor{ptd_yao}'s algorithm to $O(d)$ by a two-time scale stochastic approximation trick. 

To understand how GTD makes $O(d)$ computation possible, let's start with the least-mean-squares (LMS) algorithm. Let $\phi$ be the feature vector we observe every time step, and $y$ is the output. Then the LMS update, $\Delta \theta =-(\phi^\tr \theta - y) \phi$, converges to the solution to the linear system, $A\theta = b$ where $A =  \EE[\phi\phi^\tr ]$, and $b= \EE[\phi y]$. LMS is a stochastic gradient method that is $O(d)$ per time step. TD methods are similar in this regard. Let $\phi$ be the feature vector we observe every time step, $r$ is the reward and $\phi'$ is the next feature vector. 
For TD(0), see equation \ref{eq:Ab} for the definition of $A$ and $b$. 

Now we aim for an $O(d)$ method that approximates $\Delta \theta = A^\tr (A\theta +b)$, which is just the stationary form of \citeauthor{ptd_yao}'s gradient descent algorithm. 
What would be a good sample of $A^\tr $? That would be $(\gamma\phi' - \phi)\phi^\tr $. How to get an estimate of $A\theta +b$? We already have this in the TD algorithm. That is $\delta\phi$. So it's a question of putting these two estimations together. 
Note that here just putting them together by multiplication does not work, because the expectation of the two terms cannot be taken individually: they are dependent on each other.\footnote{ \citet{shangtong_imgtd} started with this same observation too (independent work). \citet{gtd} also said that if we sample both
of the terms to form a product, then the result will be biased by their correlation. This arises from the well-known double-sampling issue in reinforcement learning. \citet*{mengdi_stochastic_2exp} considered a general class of optimization problems that involve the composition of two expectation operators, for which SGD does not apply. }  
GTD's idea is to slow down the second estimation, $A\theta +b$, in such a way that we don't use the latest transition. 
This was done by estimating the TD update separately, introducing a helper vector $u$:\footnote{The GTD paper \citep{gtd} has a typo in their equation (4). With $\EE[\delta \phi]$ defined therein, the algorithm updates in equation 8 and equation 10 would be unstable. %A personal communication between Hengshuai and Csaba and Shalabh in 2008 fixed this issue in the proofs.
}
\begin{align}
\theta_{t+1} &= \theta_t - \alpha_t (\gamma \phi_t' - \phi_t)\phi_t^\tr u_t, \nonumber \\
u_{t+1} &= u_t + \beta_t (\delta_t \phi_t - u_t), \label{eq:gtd}
\end{align}
where $\alpha_t, \beta_t>0$ are two step-size parameters.
Note that $u_t$ can be viewed as a historical average of the TD update. 
In \citet{ptd_yao}, the following gradient descent algorithm was proposed:
\begin{equation}\label{eq:expectedGTD}
\theta_{t+1} = \theta_t - \alpha_t A_t^\tr (A_t\theta_t + b_t), 
\end{equation}
where $A_t$ and $b_t$ are consistent estimations of $A$ and $b$, which are guaranteed to converge due to the law of large numbers \citep{tadic_td}. We call the algorithm in equation \ref{eq:expectedGTD} the {\em Expected GTD} algorithm because it is a GTD algorithm from the expected update under the empirical distribution. 

In \cite{gtd}, they discussed an alternative algorithm that applies $A_t^\tr $ to the sample of $A \theta_t + b$ (which is the TD update, $\delta_t\phi_t$). It is a hybrid between TD and Expected GTD:
\begin{equation}\label{eq:attd}
\theta_{t+1} = \theta_t -\alpha_tA_t^\tr \delta_t\phi_t, 
\end{equation}
which was called $\mbox{A}^\tr$TD \citep{gtd}. There is another hybrid 
\begin{equation}\label{eq:R1-GTD}
\theta_{t+1} = \theta_t -\alpha_t (\gamma \phi_{t+1} - \phi_t)\phi_t^\tr (A_t\theta_t + b_t), 
\end{equation}
which we call {\em Rank-1 GTD} or {\em R1-GTD} for short. $\mbox{A}^\tr$TD  and R1-GTD are counterparts that apply sampling either to the TD update or to the preconditioner. The rank-1 matrix applies for the purpose of stabilizing the TD update on average.   

About the signs in the updates of $\theta$ and $u$: $u$ is the averaged TD update, which gives $\EE[\delta \phi]= A\theta + b$ in the long run. In the update for $\theta$, the rank-1 matrix is a sample of $A^\tr $. Together with $u$, the expected update provides an unbiased estimate for $A^\tr (A\theta+b)$. It thus makes sense to use the minus sign in the update of $\theta$, instead of the positive sign in the GTD paper \citep{gtd}, following the convention of gradient descent.   

The complexity reduction with GTD is nicely delivered. It is $O(d)$ and guaranteed to converge for off-policy learning. However, there is a non-trivial practical problem. In particular, to decouple the two terms in the expectation, the price we paid is an additional update, requiring us to tune two step-size parameters when using the GTD algorithm in practice. 

\cite*{maei2009convergent_nonlinear} generalized the MSPBE to the nonlinear function approximation, and proved the convergence of the generalized GTD2 and TDC to a local minima. The LMS2 algorithm is an extension of the idea of GTD to supervised learning \citep*{lms2}. % which shows that the convergence rate is between LMS and the $O(d^2)$ recursive least-squares \citep{ljung1998system}, if the two step-sizes are tuned well.  

The neural GTD algorithm \citep*{neural_gtd2} is a projected primal-dual gradient method. It has only one step-size, however, with the addition of a projection ball operation (probably needed to keep the algorithm bounded). The algorithm is actually more similar to GTD2, because the helper iterator reduces to the same as that of GTD2 in the linear case.  


\subsection{The MSPBE objective, GTD2 and TDC}\label{tdc}
The algorithms of GTD2 and TDC (TD with a correction) were derived using the MSPBE (mean-squared Projected Bellman Error) \citep{tdc}. They showed that for on-policy evaluation, they are faster than GTD. However, the reason of the speedup was not explained by the paper and not known to the literature. Here we provide a simpler derivation for the two algorithms and it also explains why they are faster.  

{\bfseries GTD2}. 
In the first iterator of the GTD update, we used the averaged TD update, $u_t$. Now let's see if we can speed up $u_t$. This can be done by applying a preconditioner to the update of $u_t$. In particular, if we replace the second iterator $u_t$ with 
\[
\bar{w}_{t+1} =  C^{-1}u_t. 
\]
The expected behavior of $\bar{w}$ is then described by the following iteration:
\[
\bar{w}_{\tau+1} = C^{-1} (A \theta_\tau +b ). 
\]
Note that $h =  C^{-1} (A \theta_\tau +b ) $ is just the O.D.E. underlying LSPE \citep*{lspe96,lspe03,lspe04}. \citet{ptd_yao} also showed that LSPE is a preconditioning technique. 
GTD2 and TDC can be derived using two ways of writing the O.D.E. 

Using the form of $h$ above, we can solve $\bar{w}$ with stochastic approximation (which is just the LMS algorithm), treating the TD error, $\delta_t$, as the target signal and predicting it with the feature vector. This leads to GTD2:
\begin{align}\label{eq:gtd2}
    \theta_{t+1} &= \theta_t - \alpha_t (\gamma\phi_{t}' - \phi_t)\phi_t^\tr w_t, \nonumber \\
    w_{t+1} &= w_t - \beta_t(  \phi_t^\tr  w_t-\delta_t)\phi_t. 
\end{align}
The O.D.E. for the $\theta$ update is $h'= A^\tr  C^{-1} (A \theta_{\tau} + b)$. 
Thus the underlying matrix is symmetric and the stability of the system can be achieved provided that (1) $A$ is non-singular (but not necessarily negative definite, which is the case of general off-policy learning); and (2) $C$ is symmetric and positive definite.
This O.D.E. is just the gradient of the MSPBE objective \citep{tdc}. In the matrix-vector form, it can be written as
\[
\bold{MSPBE}(\theta)= (A \theta + b)^\tr  C^{-1} (A \theta + b).
\]

{\bfseries TDC}. 
Let's write $h$ in another form. Note that $A$ can be split into two parts, $A = D - C$, where $D=\gamma \EE[\phi \phi'^\tr ]$. Thus $h= C^{-1}((D-C)\theta +b)=-\theta + C^{-1}(D\theta + b)$. %\footnote{Note that $C^{-1}D$ is also the matrix that is taken as the world model in linear Dyna \citep{lin_dyna}.} 
Thus we can simply apply stochastic approximation again to solve $h$ incrementally. This is a LMS procedure too. This time we treat the stochastic sample given by $\gamma \phi_t'^\tr \theta + r$, as the target for regression. This leads to the following update:
\begin{align}
    \theta_{t+1} &= \theta_t - \alpha_t (\gamma\phi_{t}' - \phi_t)\phi_t^\tr w_t, \nonumber \\
w_{t+1} &= w_t - \beta_t \theta_t + \beta_t(\gamma\phi_t'^\tr \theta_t +r_t - \phi_t^\tr w_t)\phi_t.  
\end{align}
This is another form of TDC \citep{tdc}. In the original form of TDC, $\theta$ has a different iteration from GTD2 while $w$ is the same as that in GTD2. This form is a ``transposed'' version of TDC: the $\theta$ update is the same as GTD2, while the $w$ update is different. 

To derive the original TDC, we start with the same transformation but this time to $h'$:
\begin{align*}
h' & =(A^\tr  C^{-1}) (A \theta_{\tau} + b) \\
& = (D^\tr  -C)C^{-1} (A \theta_{\tau} + b)\\
&= -(A \theta_{\tau} + b) + D^\tr  \left(C^{-1} (A \theta_{\tau} + b)\right).
\end{align*}
The first term is just the expected update of TD. 
The second term can be approximated by breaking the rank-1 matrix vector product, and not forming the matrix explicitly \citep{tdc}. 
Note that $\gamma \phi_t'\phi_t^\tr$ is a sample of $D^\tr$. 
This O.D.E. derives the $\theta$ update of the TDC algorithm, while the helper update remains the same as GTD2:
\begin{align}\label{eq:tdc}
    \theta_{t+1} &= \theta_t - \alpha_t \left[- \delta_t(\theta_t)\phi_t + \gamma \phi_t' (\phi_t^\tr  w_t)\right], \nonumber \\
    w_{t+1} &= w_t - \beta_t(\phi_t^\tr  w_t-\delta_t)\phi_t. 
\end{align}

These are real-world applications of the preconditioning technique from iterative algorithms and numerical analysis \citep{saad03:IMS,horn2012matrix,golub2013matrix} to reinforcement learning. Accelerated learning experiments can be found in \citep{ptd_yao}, covering TD, iLSTD \citep*{ilstd} and LSPE, which shows the spectral radius of the preconditioned iterations improves over expected GTD and TD. %For linear Dyna, the improved implementation and faster learning results are in \citep{multi-step-dyna}.
GTD2 and TDC were shown to converge faster than GTD, and TDC is slightly faster than GTD2, for on-policy learning problems \citep{tdc}. %This  experimentation for off-policy learning algorithms was first used by Yao in a number of communications before their GTD submission to NIPS 2008. I struggle with this... include? not include? 


\citet{baird1995residual}, 
\citet*{scherrer2010should_bellerr}, 
\citet*{sutton2018reinforcement}, \citet*{zhang2019_rg} and \citet*{patterson2021investigating_etd} had good discussions on the learning
objectives for off-policy learning. In some sense, the Bellman error is indeed a tricky objective to minimize because it involves two expectation operators. In particular, take the mean squared Bellman error for example, where the transition follows the policy $\pi$ and the dynamics of the MDP, $\EE[r+\gamma V(s') - V(s)]^2= \EE[\EE(r+\gamma V(s') - V(s))^2 |s]$. 
In the nonlinear and off-policy i.i.d. case, the inside, the conditional expectation is problematic. One either needs access to a simulator, by resetting it to the same state we just proceeded from there, or hopes the environment is deterministic. This needs {\em two independently sampled successor states}, which is the so-called {\em double-sampling} problem, a well-known challenge in reinforcement learning. Sampling two independent successors is not practical in online learning and other scenarios, because we cannot go back in time. \citet{ran_gtd} circumvents this issue with a two-time scale approach, by generalizing the ``waiting'' idea of GTD and GTD2/TDC. They proposed a few algorithms that are based on residual gradient \citep{baird1995residual}, regularization \citep*{hastie2009elements}, and momentum \citep{polyak1992acceleration}. It is also possible to extend GTD, TDC and our method to add momentum. One of their algorithms performs much faster than GTD2. However, the algorithm has four hyper-parameters, and the two step-sizes used in experiments do not satisfy the two-time scale requirement and thus their empirical results are not covered by their theory. The helper iterator was actually updated much slower than the main iterator. In this paper, we focus on momentum-free algorithms and our method has convergence guarantees with only one step-size parameter.  

Policy evaluation algorithms are generally in the dimensions of first-/second-order and on-/-off policy learning. In particular, the second-order TD method is on-policy and off-policy {\em invariant}, in contrast to the diverse forms of first-order off-policy TD algorithms, which all have different updates from the online TD methods.  
To be concrete, let's consider an off-policy learning algorithm that minimizes a generic loss of the form, 
\[
E(\theta) = \EE[\delta\phi] ^\top U^{-1}\EE[\delta\phi],
\]
where $U$ is any S.P.D matrix.

The derivation by \citet*{pan2017accelerated_atd} follows through without any problem. In particular, let $H$ be the Hessian matrix of $E$. Then the Newton method minimizing $E$ takes the form of (in the expectation)
\begin{align*}
\theta_{\tau+1} &= \theta_\tau - \alpha_\tau H^{-1} \nabla_\theta E|_{\theta=\theta_\tau}\\
&=\theta_\tau -  \left(A^\top U^{-1}A  \right)^{-1} \nabla_\theta E |_{\theta=\theta_\tau}\\
& =\theta_\tau - \alpha_\tau  \left(A^\top U^{-1}A  \right)^{-1} A^\top U^{-1}(A\theta_\tau+b) \\
& = \theta_\tau -  \alpha_\tau A^{-1}  U (A^\top)^{-1}   A^\top U^{-1}(A\theta_\tau+b) \\
& = \theta_\tau -  \alpha_\tau A^{-1} (A\theta_\tau+b) \\
& = \theta_\tau -  \alpha_\tau A^{-1} \EE[\delta(\theta_\tau) \phi ].
\end{align*}
Using the stochastic approximation trick, 
this means for such a generic function $E$, Newton method has a form that is {\em invariant} in $U$:
\begin{equation}\label{eq:2ndordertd}
\theta_{\tau+1} = \theta_\tau -  \alpha_\tau A^{-1} \delta \phi. 
\end{equation}
Why is it invariant in $U$? At a high level, this is because $U$ is an artifact, in particular, a preconditioner that improves the conditioning of the underlying O.D.E. of GTD. Preconditioning, by definition, is to accelerate convergence without changing the solution.

The update \ref{eq:2ndordertd} is exactly the Newton TD method proposed and analyzed by \citeauthor{Yao_direct_preconditioning} for policy evaluation, by using an estimation of matrix $A$. 
\citet{pan2017accelerated_atd} rediscovered this algorithm by minimizing MSPBE for off-policy learning.   
In fact, their derivation will hold for minimizing NEU as well. The Newton TD method minimizing the NEU objective also leads to this update. This can be shown from $E$ by setting $U=I$, the identity matrix. This means while there are a number of diverse first-order off-policy TD algorithms, the second-order TD is invariant both in the sense of on-policy or off-policy, and a generic loss in the form of $E$. 
Thus probably we don't have to differentiate between on-policy or off-policy TD for the second-order methods, especially for Newton. No changes (like the case of the first-order TD methods) are required to make the algorithm in equation \ref{eq:2ndordertd} in order for it to converge for off-policy learning.  

Most of second order TD methods are $O(d^2)$ per time step in computation. In certain problems, when $A$ is sparse or low-rank, one can gain acceleration by taking advantage of the structure, e.g., sparse transitions \citep{ptd_yao} and low-rank approximation \citep{pan2017accelerated_atd}. However, in general, the second-order methods are not as efficient as the first-order methods in computation when deployed online. Readers are referred to a linear-complexity approximate Newton method \citep{ran_gtd}, which accelerates gradient-based TD algorithms for minimizing MSBE.
Our work in this paper is in the thread of $O(d)$, first-order TD for off-policy learning, for which there is no such invariance like that holds for the second-order TD methods. 

\subsection{The Saddle-Point Formulation}
The saddle-point or mini-max formulation of GTD, GTD2 and TDC \citep{bo_gtd} can be derived by observing that the helper iterator in the three algorithms is expected to give a good estimation of the expected TD update (GTD), or a least-squares solution (GTD2 and TDC). Take the helper iterator in GTD (equation \ref{eq:gtd}) for example. We want the helper iterator to get to $A\theta+b$ as close as possible. Thus the following loss containing the inner product will be maximized if $u=A\theta +b$:
\[
L(u|\theta) = u^\tr (A\theta + b) - \frac{1}{2} u^\tr u,
\]
which can be seen from $\nabla_u L(u|\theta)=0$. That is, $\arg\max_u L(u|\theta)=u^*=A\theta+b$. Intuitively, $u$ should be along the direction of $A\theta+b$ (from the inner product), and the magnitudes should be the same too (from the $\ell_2$-norm, which gives the length requirement). Therefore, $L(u^*|\theta) = \frac{1}{2}\norm{A\theta+b}^2=\frac{1}{2}\bold{NEU}(\theta)$. 
This gives a mini-max formulation of GTD:
\[
\theta^* = \arg\min_{\theta} \max_u L(u|\theta). 
\]
This is a very interesting formulation of GTD. Seeking the saddle-point solution is an important class of problems in optimization, e.g., see \citep{saddle_point_Nemirovski}. The problem also dates back to game theory from the beginning \citep{gametheory_Neumann}. Many later works on GTD are built on this formulation, e.g., see \citep*{saddle_point_du,mengdi_wang_gtd_like_nasa,saddle_doina_svrg,csaba_gtd_22}. Here we briefly review \citeauthor{mengdi_wang_gtd_like_nasa}'s Nested Averaged Stochastic Approximation (NASA) algorithm.    

The NASA algorithm aims to minimize a nested loss function of the form,\footnote{This nested function is a further extension of the stochastic composition problem \citep{mengdi_stochastic_2exp}.} $\min_\theta f_1(f_2(\theta))$, in a stochastic fashion.
For example, in the GTD setting, $f_1(x)=\norm{x}^2$, and $f_2=A\theta+b$. NASA features in the use of the averaging technique. Let's interpret their algorithm in the GTD setting. For minimizing the NEU objective, we can write their algorithm by the following:
\begin{align*}
 g_t &= \arg\max_g \left\{g^\tr z_t -\frac{\beta_t}{2}\norm{g}^2  \right\}\\
 \theta_{t+1} &= \theta_t - \tau_t g_t\\
 z_{t+1} &= (1-a \tau_t)z_t + a\tau_t (\gamma \phi_t' - \phi_t)\phi_t ^\tr u_t\\
 u_{t+1} & = (1-b\tau_t)u_t + b\tau_t \phi_t \delta_t(\theta_{t+1}),
\end{align*}
where $\delta_t(\theta_{t+1})$ is the TD error realized with the weight vector $\theta_{t+1}$. 
We can start understanding NASA with the simplest connection. The iteration $u_t$ is similar to GTD (equation \ref{eq:gtd}), hereby using $b\tau_t$ to smooth the TD updates. The vector $z$ provides another layer of averaging over the GTD update (i.e., the change in $\theta$ in equation \ref{eq:gtd}), using $a\tau_t$ to smooth. That is, $z$ is expected to get close to the gradient of NEU. For $g_t$, we use here the equivalent $\arg\max$ formulation instead of the original $\arg\min$, to see the saddle-point formulation clearly. The update of $\theta$, as a result, switches to the negative sign, which is the gradient descent style. The major update 2.6 in their Algorithm 1 is an averaging style.

Therefore, in the context of minimizing NEU, the major improvement of NASA over GTD is that there is an additional averaging over the GTD update, and an introduction of $\ell_2$ regularization. NASA also generalizes to minimize other nested loss functions than NEU, which include Stochastic Variational Inequality, and low rank approximation. They proved the almost sure convergence of NASA under the diminishing step-size for $\tau_k$ and constant $a, b$ and $\beta$, for the class of functions of $f_1$ and $f_2$ with Lipschitz continuity in their gradients. Algorithms and analysis of averaged updates over GTD algorithms can also be found in, e.g., \citep{csaba_lin_stochastic18,csaba_gtd_22}. For  analysis on more general averaging algorithms, one can refer to, e.g.,  \citep{polyak1992acceleration,averaging_sgd_lin,svrg}.  


\subsection{Related work}
\citet*{bo_gtd_finite} performed the first finite-sample analysis for GTD algorithms, and showed that GTD and TDC/GTD2 are SGD algorithms in the formulation of minimizing a primal-dual saddle-point objective function, with a convergence rate of about $t^{-1/4}$ in terms of value function approximation. 
\citet*{dalal2018finite_twotimescale} established the convergence rates of GTD, GTD2 and TDC under diminishing step-sizes. Later they showed that, with the step-sizes scheduled by $1/t^\alpha$ and $1/t^\beta$ where $0<\beta<\alpha<1$, the convergence rates are $O(t^{-\alpha/2})$ and $O(t^{-\beta/2})$ for the two iterators, and the bounds are tight \citep*{dalal2020tale_twotimescale}. \citet*{xu2019two_time_gtd} had the first non-asymptotic convergence analysis for
TDC under Markovian sampling. 
\citet*{xu2021sample_twotimescale} analyzed the convergence rate of linear and nonlinear TDC with constant step-sizes. \citet*{xu2020finite_q_learning} analyzed the convergence rate of a Q-learning algorithm with a deep ReLU network. Their algorithm also has a projection ball applied to the TD update. \citet{yu2018convergence_gtd} had a comprehensive convergence analysis of GTD and mirror-descent GTD, with an extensive treatment of the eligibility trace under both constant and diminishing step-sizes.

\citet*{gupta2019finite} gave an error bound for stochastic linear two-time scale algorithms with fixed step-sizes. They also derived an adaptive learning rate for the faster iteration. 
The convergence rate of general two-time scale stochastic approximation is studied by \citet*{hong2020two_bilevel_optimization} and \citet*{doan2021finite}. 

An important early off-policy learning exploration is based on importance sampling \citep*{precup2000eligibility,precup2001off}. However, importance sampling algorithms have an inherent problem in the reinforcement learning context. The variance is high due to small probabilities of taking certain actions in the behavior policy, because their products appear in the denominator(s) of certain quantities. The variance of importance sampling ratios may grow exponentially with respect to the time horizon, e.g., see \citep*{xie2019towards_IS}.  
Weighted importance sampling \citep*{mahmood2014weighted} and clipped ratios \citep*{vtrace} can mitigate the issue and reduce the high variances, however, at the price of providing a biased solution. Return-conditioned importance sampling  \citep*{IS_conditioned_return} reduces variances by ruling out the actions that have no effect on the return.  
Some methods are based on the importance sampling over the stationary distributions of behavior and target policies \citep*{hallak2017consistent,liu2018breaking_IS_stationary,xie2019towards_IS,gelada2019off_IS_stationary}, instead of the product of policy ratios. 

Emphatic TD (ETD) \citep*{etd,yu2015convergence_etd}, a non-gradient-based method, has only one step-size in its update rule. However, ETD does not converge to the TD solution and it suffers from high variances. One has to use small step-sizes for ETD, which results in slow convergence \citep*{ghiassian2017first_etd,ghiassian2021empirical_etd}. ETD is still problematic on Baird counterexample due to high variances even though very small step-sizes were used \citep{sutton2018reinforcement}. Interested readers may refer to \citep{hallak2015generalized_etd,gelada2019offpolicy_etd} for bias-variance analysis, and variance reduction \citep{lyu2020variancereduced_etd} on ETD. 



