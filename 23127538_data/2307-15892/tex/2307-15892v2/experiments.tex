\section{Experiments}\label{sec:experiments}
This section contains empirical results of Impression GTD, for on-policy learning on Boyan chain, and off-policy learning on Random Walks (with tabular representation). Experiments on the inverted- and the dependent- representation for Random Walks are in Appendix \ref{exp:rwinv} and Appendix \ref{exp:rwdep}, respectively. Baird counterexample is in Appendix \ref{exp:baird}. All  the curves reported were averaged over 100 independent runs.

\subsection{Boyan Chain}
The problem is the same as \citep{lstd}. It has 13 states and the rewards are all -3.0 except that the transition from state 1 to state 0 incurs a reward of -2.0. The features are generated by a linear interpolation from four unit basis vectors at states $4i$, $i=0, 1, 2, 3$. Each episode starts from state $12$, and from state $i$ it goes to either $i+1$ or $i+1$, with an equal probability of 0.5. The features can represent the value function for this policy accurately. 

The compared algorithms include GTD, HTD, Vtrace, GTD2, TDC, TDRC, Impression GTD and mini-batch TD. 
At time step $t$, an algorithm gives $\theta_t$, and the metrics is computed by
\[
\mbox{RMSVE}(\theta_t) = \sqrt{\frac{1}{N}\sum_{s=1}^N (V^\pi(s) - \phi(s)^\tr \theta_t)^2}. 
\]


% Figure environment removed

Figure \ref{fig:boyan_rmse} compares the RMSVE of the algorithms.
The batch size for mini-batch TD and Impression GTD are both 10. For Impression GTD, it converges with large step-sizes for this example. So a step-size of 10.0 is used. MiniBatchTD used a step-size of 0.05. All the hyper-parameter of the other algorithms were the same as in \citep{martha2020gradient}. Impression GTD waited until both buffers are bigger than the batch size. So there is a flat curve in the beginning. 
HTDâ€™s curve was almost the same as TD and thus it is not shown.


Figure \ref{fig:boyan_rmse_imGTD_bsz} shows the effect of the batch size for Impression GTD. We select the top two baselines after about 1,500 steps in Figure \ref{fig:boyan_rmse}, which are TD and TDRC. Because this problem is on-policy learning, TD converges fast and it stands for the ceiling  for $O(n)$ gradient TD methods in the convergence rate. 
Comparing to TD and TDRC, all the impression GTD algorithms have a steeper drop in the loss, though bigger batch sizes need to wait a bit longer to kick start learning.    Impression GTD with bigger batch sizes (e.g, 32, 64, and 128) is able to learn significantly faster than TD and TDRC. The acceleration in convergence rate seems to decrease after batch size 32.  

% Figure environment removed

Figure \ref{fig:boyan_rmse_imGTD_stepsize} shows the effect of the step-size for Impression GTD, which all used batch sizes equal to 16. All the four step-sizes performed faster than TD whose step-size was tuned near optimal by \citeauthor{martha2020gradient}, which was 0.0625. The four step-sizes used for plotting this figure were 0.1, 1.0, 5.0 and 10.0. Their value range being big whilst learning all faster than TD means that tuning the step-size for Impression GTD is not as sensitive as the GTD algorithms.  

% Figure environment removed


\subsubsection{\bfseries Are we getting a linear rate?} By just looking at the rate plots of Impression GTD, e.g., Figure \ref{fig:boyan_rmse_imGTD_bsz} and Figure \ref{fig:boyan_rmse_imGTD_stepsize}, it is hard to see whether the algorithm converges at a linear rate or a rate of $O(1/t)$. The overall curve looks more like the rate of $O(1/t)$ actually.  
However, note that there is a nonzero bias that plays a role in the end. Thus the convergence rate curves shown in the figures can still be a linear convergence, to a biased solution. 

To investigate which rate Impression GTD is getting, we re-plot Figure \ref{fig:boyan_rmse_imGTD_bsz}, using exactly  the same data. The only change in the re-plotting is that we subtracted a bias estimate, which is taken to be the average of the last 100 steps for each curve, discounted by a factor of 0.8, to consider the errors shown in our running steps are still decreasing in the end. Thus this just shifts the curves of the algorithms in Figure \ref{fig:boyan_rmse_imGTD_bsz} by an algorithm-dependent constant, in the $y$-axis. Then finally, we use the log scale for the $y$-axis. This is shown in Figure \ref{fig:rmsve_boyan_batchsize_linear_rate}. First, we can take a look at the curve of TD. After the subtraction, there are still significant errors (around $0.08$) throughout the most learning steps. Thus {\em TD does not have a linear convergence rate}.  Now take a look at Impression GTD. For small batch sizes like 4, the algorithm has a similar rate to TD. As we increase the batch size, the rate becomes approximately linear. This is reflected in that in linear-$x$ and log-$y$ plot, the error curve is almost linear. To confirm this in another way, we additionally plotted the learning curve of Expected GTD, using also a step-size of 5.0. After some number of sufficient samples, Expected GTD basically iterates using a good matrix that is close to the one underlying the NEU objective, and thus the rate is approximately linear. We see in Figure \ref{fig:rmsve_boyan_batchsize_linear_rate} that Impression GTD with a large batch size 128 gets very close to the rate of Expected GTD. For clarity of the presentation, only a subset of curves from Figure \ref{fig:boyan_rmse_imGTD_bsz} are shown in Figure \ref{fig:rmsve_boyan_batchsize_linear_rate}. The omitted curves of the other batch-sizes are between the shown ones, and they support this conclusion as well.    


% Figure environment removed


\subsection{Random Walks}
There are five intermediate states, and two terminal states (which can be treated as one terminal state). The problem is off-policy learning. The target policy goes to left with probability 40\% and to the right with 60\% probability. The behavior policy chooses the left and right actions with equal (50\%) probabilities. For the experiments in this section, the tabular representation is used.  

% Figure environment removed

Figure \ref{fig:rwtab} shows the results. 
All the algorithms were run with the same, near-optimal hyper-parameters as used in the git repository provided by \citet{martha2020gradient}. Impression GTD used step-size 1.0, and MiniBatchTD used a step-size of 0.05.  The batch size for both algorithms is 32. Impression GTD converges much faster than the baselines including TDRC. Vtrace is a very simple algorithm. It just modifies the importance sampling ratio so that it is upper clipped at 1.0. The motivation of the algorithm is to control the variances caused by importance sampling ratios. It looks Vtrace introduces a bias with the variance reduction. 
TD, HTD and TDRC are  faster than the other baselines, and the gap among the three are small. 

% Figure environment removed
The effect of the batch size for Impression GTD is shown in Figure \ref{fig:rwtab_batch}. The step-size of Impression GTD is uniformly 0.5.   After 4500 steps, all the Impression GTD agents were faster than TD. The acceleration is more with a bigger batch size. However, note that a bigger batch size also means more computation complexity per time step.  This can be accelerated with GPU computation for the mini-batch policy evaluation procedure.

% Figure environment removed

Figure \ref{fig:rwtab_alpha} shows the effect of the step-size for Impression GTD. This shows that a bigger step-size is faster in the beginning. However, there is a {\em convergence rate  overturn}. For example, Impression GTD with $\alpha=1.0$ crosses with $\alpha=0.25$ and 0.5 at about 8100 steps and 7000 steps, respectively.  After the crossing points, $\alpha=1.0$ is slower than a smaller step-size.  This is consistent with the Boyan chain results. We also plotted the MiniBatchTD, which used the same step-size as the TD algorithm. It converges faster than TD after about 2000 steps. In the end, MiniBatchTD was fastest algorithm. However, keep in mind that TD and MiniBatchTD just converges for this off-policy learning task by chance, and they may diverge for general off-policy learning. The batch size used for this plot is 8.



\subsection{Discussion on the Empirical Results}
In a summary, Impression GTD is fastest in all the compared algorithms, with a single step-size that is much easier to tune than the two-time scale GTD algorithms. Our results show that MSPBE should be interpreted with care, as the literature seems to favor this measure over NEU or RMSVE \citep{sutton2018reinforcement,martha2020gradient}. Our results show that a larger MSPBE does not necessarily mean a worse RMSVE. Vice versa. A small MSPBE does not necessarily imply good learning either. 
What this finding means is that {\em MSPBE is a surrogate loss for NEU}. 

At the time of writing this, we are not sure which one is better or not than the other. However, there is one thing we are sure: If the preconditioner is not well conditioned,  MSPBE does not delegate NEU well. In numerical analysis and iterative algorithms, people would normally avoid choosing an ill-conditioned preconditioner, at least not worsening the conditioning of the original problem. To accelerate TD and GTD, the spectral radius of the underlying iteration should be reduced \citep{ptd_yao}. With ill-conditioned preconditioner, this is hardly achieved.  \citet{ran_gtd} had similar comments on the influence of the conditioning of matrix $C$ on learning objectives.
In the off-policy learning setting, the preconditioner in MSPBE is not directly chosen by the users like in numerical analysis (where the preconditioner matrix is chosen on the fly). Instead, the matrix $C$ is dependent on the behavior policy and the features, and its preconditioning effect is realized in a {\em stochastic} way (this is very different from iterative algorithm in numerical analysis), because only samples of $C$ are applied in the learning update. Given this distinctiveness, the preconditioner is often not obvious to algorithm users in off-policy learning.  
%One can achieve a low MSPBE but still a poor solution. 
%Given that the current literature has a favor of MSPBE over NEU, 
We should perhaps avoid possible pitfalls of using preconditioning such as MSPBE in off-policy learning, and also take caution in interpreting the learning results. An example is a debug analysis of TDC on Baird counterexample performed by the authors \citep{}, which thoroughly revealed why it performed so slow as illustrated by \citep{sutton2018reinforcement}. We found it was due to the use of a singular preconditioner.   