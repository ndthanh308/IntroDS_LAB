\section{Analysis}\label{sec:theory}
This section contains convergence rate analysis of Impression GTD %The convergence of Impression GTD under the diminishing step-size is first established using standard stochastic approximation, thanks to independence sampling. 
 with constant step-sizes. The first result is an $O(1/t)$ rate. For the second result, we first give a new condition of smoothness, called $L$-$\lambda$ smoothness. Under this weaker smoothness condition, we establish a tighter convergence rate for SGD than Theorem 3.1 of \citet{gower2019sgd_general}. 
Then by showing that the NEU objective and the independence sampling satisfies $L$-$\lambda$ smoothness, we prove that Impression GTD converges at a linear rate.

Our algorithm analysis is conducted in a generic GTD algorithmic framework. The $O(1/t)$ rate and the linear rate are both applicable to Expected GTD, A$^\tr$TD, and R1-GTD.  

Both the $O(1/t)$ rate and the linear rate depend on the i.i.d. sampling ensured by our independence sampling method. Thus we first introduce a lemma for that. 

\begin{lem}[Independence Sampling]\label{lem:independence}
For episodic problems, our sampling method according to equations \ref{eq:buffer1} and \ref{eq:buffer2} ensures that the transition samples from the two mini-batches are independent: 
\[
Pr(i_{t_1}  =s_1 \cap j_{t_2} = s_2) =Pr(i_{t_1}  =s_1 ) Pr(j_{t_2} = s_2),
\]
where $i_{t_1}$ and $j_{t_2}$ are the time steps that we insert the two samples into buffer $B_1$ and buffer $B_2$, respectively. 
\end{lem}
\begin{proof}
Without loss of generality, let us consider the batch size equal to 1. 
Let $(s_1, r_1, s_1')$ and $(s_2, r_2, s_2')$ be two samples drawn at time step $t$ by the sampling method. Then it suffices to prove that $s_1$ and $s_2$ are independent. For notation convenience, let $i_{t_1}$ be $s_1$, and the state sequence up to $s_1$ is $\{i_0, i_1, \ldots, i_{t_1}\}$, in the episode where we put $s_1$ into the buffer. Similarly, $j_{t_2}$ is for aliasing $s_2$. 
We just need to prove that $Pr(i_{t_1}=s_1 \cap j_{t_2} = s_2) = Pr(i_{t_1}=s_1) Pr(j_{t_2} = s_2)$. To see this, we first have
\begin{align*}
&Pr(i_{t_1}  =s_1 \cap j_{t_2} = s_2) \\
=& \sum_{i_0, i_1, \ldots, i_{t_1}-1}  \quad \sum_{j_0, j_1, \ldots, j_{t_2}-1}
Pr(\underline{i_0, i_1, \ldots, i_{t_1}  =s_1 } \cap \underline{j_0, j_1, \ldots, j_{t_2} = s_2})  \\
=&\sum_{i_0, i_1, \ldots, i_{t_1}-1}  Pr(i_0, i_1, \ldots, i_{t_1}  =s_1 )  \sum_{j_0, j_1, \ldots, j_{t_2}-1} Pr( j_0, j_1, \ldots, j_{t_2} = s_2)  
\end{align*}
The first equality is according to the law of total probability, which sums over all possible trajectories that lead to these two observations.  
The second equality is because the two episodes are independent due to that $i_0$ and $j_0$ are i.i.d. samples (which is ensured by the environment). 

It suffices to focus on the first term in the second equality. The second term  can be calculated similarly. We have 
\begin{align*}
&\sum_{i_0, i_1, \ldots, i_{t_1}-1}  Pr(i_0, i_1, \ldots, i_{t_1}  =s_1 )  \\
=& \sum_{i_0, i_1, \ldots, i_{t_1}-1}  Pr(i_{t_1}  =s_1 ) Pr(i_0, \ldots, i_{t_1}-1| i_{t_1}  =s_1 )   \\
=&   Pr(i_{t_1}  =s_1 ) \sum_{i_0, i_1, \ldots, i_{t_1}-1} Pr(i_0, \ldots, i_{t_1}-1)    \\
=&   Pr(i_{t_1}  =s_1 ) \sum_{i_0, i_1, \ldots, i_{t_1}-1} Pr(i_{t_1}-1)Pr(i_0, \ldots, i_{t_1}-2| i_{t_1}-1)    \\
=&   Pr(i_{t_1}  =s_1 ) \sum_{i_0, i_1, \ldots, i_{t_1}-2}\sum_{i_{t_1}-1}  Pr(i_{t_1}-1)Pr(i_0, \ldots, i_{t_1}-2| i_{t_1}-1)   \\
=&   Pr(i_{t_1}  =s_1 ) \sum_{i_0, i_1, \ldots, i_{t_1}-2}Pr(i_0, \ldots, i_{t_1}-2)   \\
=&   Pr(i_{t_1}  =s_1 ) \sum_{i_0} d_0(i_0)    \\
=& Pr(i_{t_1}  =s_1 ).
\end{align*}
The first equality is according to the conditional probability formula. The next equality is because historical observations are independent of later ones.

The remaining of the derivation breaks down according to the conditional probability formula. The third equality applies this one step,   
Then the next equality splits the sum over $i_{t_1}-1$, and the law of total probability follows. 
We recursively apply to the beginning to get the last second equality. Note $d_0$ is the sampling distribution of the initial state, and $\sum_{i_0}d_0(i_0)=1$.

\end{proof}

We analyze the convergence rates of Impression GTD under constant step-sizes. 
Many SGD analysis is conducted in the setting of the finite-sum loss function, e.g., see \citep{gower2019sgd_general,sps_stepsize}. In that setting, the function is of the form, $f(x)=\frac{1}{n}\sum_{i=1}^nf_i(x)$. This setting covers important applications in machine learning, especially supervised learning problems, where there are $n$ training samples and each $f_i$ is the  loss on sample $i$. However, it does not cover the application we consider in this paper, because the NEU objective is not a finite-sum form in a straightforward sense. Towards this end, we consider the ``expected form'' of the loss. That is, the loss function can be sampled via simulation, in particular, 
\[
f(x)=\EE[f_t(x)],
\]
where $f_t$ is the loss on the sample drawn at simulation step $t$,  according to a distribution $\mathcal{D}$. This covers the finite-sum loss and it is general enough to cover our GTD setting. Let $x^*$ be the optimum and $f(x^*)=\min_x f(x)$.   

We assume the gradient of the loss can be queried for each stochastic sample. Thus equivalently, we can also say that our simulation process keeps drawing the stochastic gradient. In particular, we draw $f_t'(x)$ to get a random sample for the true gradient $f'(x)$. 
At drawing step $t$, denote the gradient sample as $g_t(x)=f_t'(x)$ for a given $x\in \RR^d$.
\begin{lem}\label{lem:ED_avg}
Let us draw a batch of $m$ i.i.d. samples according to $\mathcal{D}$. 
Let $\avgx$ be the average of the sampled gradients in this batch, i.e.,  
\[
\avgx=\frac{1}{m}\sum_{t=1}^m g_t(x).
\]
We have, for any distribution $\mathcal{D}$ that satisfies $\EED [g_t(x)]=f'(x)$, the following holds:
    \[
    \EED \norm{\avgx}^2 = \frac{1}{m}\EED\norm{g_t(x)}^2+ \left(1-\frac{1}{m}\right) \norm{f'(x)}^2.
    \]
\end{lem}
The proof is in Appendix \ref{appendix:ED_avg}. 

Instead of analyzing Impression GTD and each of the three GTD algorithms that are discussed in Section \ref{sec:background} individually, 
we use a generic algorithmic framework that enables us to study their convergence rates at one time. 
Define $\tilde{A}_m=\frac{1}{m}\sum_{i=1}^m \phi_i (\gamma \phi_{i+1}- \phi_i)^\tr$ as a normalized matrix from $m$ samples.
Consider this algorithm:
\begin{equation}\label{alg:generic_gtd}
\theta_{t+1} = \theta_t - \alpha_t\tilde{A}_{m_1}^\tr (\tilde{A}_{m_2} \theta_t + \tilde{b}_{m_2}). 
\end{equation}
The above the algorithm is for mathematical definition only.  
Note the matrix and the transpose may not be explicitly formed or computed using matrix-vector product for certain algorithms. 
We compute $\tilde{A}_{m_1}$ and $\tilde{A}_{m_2}$ for different algorithms as follows:
\begin{itemize}
\item Impression GTD. 
$\tilde{A}_{m_1}$ and $\tilde{A}_{m_2}$ are computed
from buffer $B_1$ and buffer $B_2$, respectively. 

\item Expected GTD. For the algorithm that is discussed in Section \ref{sec:background} (equation \ref{eq:expectedGTD}), a single matrix is built from all the samples in the two buffers. To fit into the independence sampling and the generic TD framework, we consider here the version described in Algorithm \ref{alg:imGTD}. Thus $m_1=|B_1|$ and $m_2=|B_2|$. For simplicity of argument and without loss of generality, we assume $|B_1|=|B_2|=t/2$.

\item A$^\tr$TD. $\tilde{A}_{m_1}$ is computed from both buffers and $\tilde{A}_{m_2}$ is the rank-1 matrix from the latest transition, $\phi_t(\gamma \phi_{t+1} - \phi_t)^\tr$. Thus  
$m_1=t$ and $m_2=1$ for A$^\tr$TD.  

\item \oneexptd. $\tilde{A}_{m_1}$ is the rank-1 matrix from the latest transition, and $\tilde{A}_{m_2}$ is computed from both buffers instead. Thus $m_1=1$ and $m_2=t$ for \oneexptd. 

 
\end{itemize}

We first introduce an assumption that is fairly general in the analysis of TD methods, e.g., see \citep*{tsi_td,bertsekas1996neuro,gtd,zhang2020gradientdice,zhang2020provably}. 


\begin{assumption}\label{assumption:phir}
The feature functions in $\phi(\cdot): \SS\to \RR^d$, are independent. All the feature vectors and rewards are finite. 
\end{assumption}


We show that all the four discussed GTD algorithms are faster than GTD2 and TDC, even though the latter two were developed to improve the convergence rate of the GTD algorithm. Note that the above four GTD algorithms all have the same O.D.E. as the GTD algorithm. Thus the convergence is accelerated by them not by improving the conditioning of the problem (like GTD2 and TDC do). Instead, improvement is achieved by a single-time scale formulation of minimizing NEU. 
%The following theorem shows that all the four GTD algorithms converge at a rate of $O(1/t)$.  

First consider this term, $\delta_i \phi_i^\tr\phi_j \delta_j$, which is a sample of NEU using two independent sample transitions from $\phi_i$ and $\phi_j$. Its gradient is $(\gamma \phi_{i+1}-\phi_i) \phi_i^\tr \phi_j \delta_j + (\gamma \phi_{j+1}-\phi_j) \phi_j^\tr \phi_i \delta_i $. For simplicity, we only consider the first term in the following analysis. The second term is symmetric and our proof can be extended to including it in a straightforward way. Define $f'_{i,j}=(\gamma \phi_{i+1}-\phi_i) \phi_i^\tr \phi_j \delta_j$. One can show that $\EE[f'_{i,j}]=\EE\nabla[\delta_i \phi_i^\tr\phi_j \delta_j]$. 
We have
\begin{align*}
\norm{f'_{i,j}(x) - f'_{i,j}(y)} &= \norm{(\gamma \phi_{i+1}-\phi_i) \phi_i^\tr \phi_j (\gamma \phi_{j+1}-\phi_j)^\tr (x-y)}  \\
&\le \underbrace{\norm{(\gamma \phi_{i+1}-\phi_i) \phi_i^\tr \phi_j (\gamma \phi_{j+1}-\phi_j)^\tr}}_\text{$L_{i,j}$}\norm{x-y}   = L_{i,j} \norm{x-y}.
\end{align*}
That is, each $f_{i,j}(x)$ is $L_{i,j}$ smooth. Given that all the feature vectors are finite according to Assumption \ref{assumption:phir}, $L_{i,j}$ must be finite.  

We are now ready to give the $O(1/t)$ rate. 
\begin{thm}\label{thm:rates_1_over_t}
Let Assumption \ref{assumption:phir} hold. Also assume matrix $A$ is non-singular. 
Impression GTD, Expected GTD, A$^\tr$TD and \oneexptd converge at a rate of $O(1/t)$ with $\alpha \le \frac{2}{L_{\max}}$, 
where $L_{\max}=\max_{i,j}L_{i,j}$. 
In particular, 
\[
\min_{k=0, \ldots, t-1} 
 f(\theta_k) \le \max\left\{\frac{2}{t \alpha \left(2- \alpha L_{\max} \right)\sigma^2_{\min}(A)}f (\theta_{0}) - \frac{1}{m_1m_2\sigma^2_{\min}(A)} \sigma_v^2, \, 0\right\}.
\]
\end{thm}
\begin{proof}
Consider the generic GTD update in \ref{alg:generic_gtd}. 
Because each $f_{i,j}$ is $L_{i,j}$-smooth, we have
\begin{align*}
f_{i,j}(\theta_{t+1}) &\le f_{i,j}(\theta_t) + f_{i,j}'(\theta_t)^\tr (\theta_{t+1}-\theta_t) +\frac{L_{i,j}}{2} \norm{\theta_{t+1} - \theta_t}^2 \\
&= f_{i,j}(\theta_t) - \alpha_t f_{i,j}'(\theta_t)^\tr  \tilde{A}_{m_1}^\tr (\tilde{A}_{m_2} \theta_t + \tilde{b}_{m_2}) +\frac{\alpha_t^2 L_{i,j}}{2} \norm{\tilde{A}_{m_1}^\tr (\tilde{A}_{m_2} \theta_t + \tilde{b}_{m_2})}^2. 
\end{align*}
Summing above for all the samples $i$ in batch $b_1$ and all the samples $j$ in batch $b_2$ gives
\begin{align*}
\sum_{i,j} f_{i,j}(\theta_{t+1}) &\le \sum_{i,j}f_{i,j}(\theta_t)- \alpha_t \sum_{i,j}f_{i,j}'(\theta_t)^\tr  \tilde{A}_{m_1}^\tr (\tilde{A}_{m_2} \theta_t + \tilde{b}_{m_2}) \\
&\quad +\frac{\alpha_t^2 \sum_{i,j}L_{i,j}}{2} \norm{\tilde{A}_{m_1}^\tr (\tilde{A}_{m_2} \theta_t + \tilde{b}_{m_2})}^2.
\end{align*}
Note that 
\begin{align*}
\frac{1}{m_1m_2}\sum_{i,j}f_{i,j}'(\theta_t)
&=\frac{1}{m_1} \sum_{i=1}^{m_1} (\gamma \phi_{i+1}-\phi_i) \phi_i^\tr\frac{1}{m_2}\sum_{j=1}^{m_2}  \phi_j \delta_j\\
%two terms
% &=\frac{1}{m_1} \sum_{i=1}^{m_1} (\gamma \phi_{i+1}-\phi_i) \phi_i^\tr\frac{1}{m_2}\sum_{j=1}^{m_2}  \phi_j \delta_j + \frac{1}{m_2}\sum_{j=1}^{m_2} (\gamma \phi_{j+1}-\phi_j) \phi_j^\tr \frac{1}{m_1} \sum_{i=1}^{m_1}\phi_i \delta_i\\
&= \tilde{A}_{m_1}^\tr (\tilde{A}_{m_2} \theta_t + \tilde{b}_{m_2}).
\end{align*}
Thus 
\[
\frac{1}{m_1m_2}\sum_{i,j} f_{i,j}(\theta_{t+1}) \le \frac{1}{m_1m_2} \sum_{i,j}f_{i,j}(\theta_t)- \alpha_t\left(1 -\frac{\alpha_t\sum_{i,j}L_{i,j}}{2m_1m_2}\right) \norm{\tilde{A}_{m_1}^\tr (\tilde{A}_{m_2} \theta_t + \tilde{b}_{m_2})}^2
\]
Let $L_{\max}=\max_{i,j}L_{i,j}$, then for $\alpha_t \le \frac{2}{L_{\max}}$, the averaged pair-wise loss across the two batches is guaranteed to reduce because the following also holds:
\begin{equation}\label{eq:fb1b2}
\frac{1}{m_1m_2}\sum_{i,j} f_{i,j}(\theta_{t+1}) \le \frac{1}{m_1m_2} \sum_{i,j}f_{i,j}(\theta_t)- \alpha_t\left(1 -\frac{\alpha_tL_{\max}}{2}\right) \norm{\tilde{A}_{m_1}^\tr (\tilde{A}_{m_2} \theta_t + \tilde{b}_{m_2})}^2.
\end{equation}
For notation convenience, let $\bar{f}_{b_1, b_2}(x)= \frac{1}{m_1m_2}\sum_{i\in b_1,j\in b_2} f_{i,j}(x)$. We have, for a constant step-size $\alpha \le \frac{2}{L_{\max}}$,
\begin{align}
\bar{f}_{b_1,b_2}(\theta_t) &\le  \bar{f}_{b_1,b_2}(\theta_{t-1}) - \alpha\left(1 -\frac{\alpha L_{\max}}{2}\right) \norm{\tilde{A}_{m_1}^\tr (\tilde{A}_{m_2} \theta_{t-1} + \tilde{b}_{m_2})}^2\label{eq:fbatch}\\
&= \bar{f}_{b_1,b_2} (\theta_{0}) - \alpha\left(1 -\frac{\alpha L_{\max}}{2}\right)  \sum_{k=0}^{t-1}\norm{\tilde{A}_{m_1}^\tr (\tilde{A}_{m_2} \theta_{k} + \tilde{b}_{m_2})}^2 \nonumber\\
&\le \bar{f}_{b_1,b_2} (\theta_{0}) - \alpha\left(1 -\frac{\alpha L_{\max}}{2}\right)  t \min_{k=0, \ldots, t-1} \norm{\tilde{A}_{m_1}^\tr (\tilde{A}_{m_2} \theta_{k} + \tilde{b}_{m_2})}^2. \nonumber
\end{align}
Thus 
\begin{align*}
\min_{k=0, \ldots, t-1} \norm{\tilde{A}_{m_1}^\tr (\tilde{A}_{m_2} \theta_{k} + \tilde{b}_{m_2})}^2 &\le \frac{2}{t \alpha \left(2- \alpha L_{\max} \right)}\left(\bar{f}_{b_1,b_2} (\theta_{0}) - \bar{f}_{b_1,b_2} (\theta_{t})\right)\\
&\le \frac{2}{t \alpha \left(2- \alpha L_{\max} \right)}\left(\bar{f}_{b_1,b_2} (\theta_{0}) - \bar{f}_{b_1,b_2} (\theta^*)\right), 
\end{align*}
where the second line is because the averaged loss keeps decreasing according to equation \ref{eq:fbatch}, and furthermore, as $t$ goes to infinity, the loss is bounded below and thus $\theta^* = \lim_{t\to\infty}\theta_t$. 

This proves the $\ell_2$ norm of the update of the generic GTD converges at a rate of $O(1/t)$. The above steps can also be conducted after taking expectation of equation \ref{eq:fb1b2} with respect to the sampling. This gives 
\begin{align*}
\min_{k=0, \ldots, t-1} \EED \norm{\tilde{A}_{m_1}^\tr (\tilde{A}_{m_2} \theta_{k} + \tilde{b}_{m_2})}^2 &\le \frac{2}{t \alpha \left(2- \alpha L_{\max} \right)}f (\theta_{0}). 
\end{align*}
Note that in the context of generic GTD, the stochastic gradient is $f'_{i,j}(\theta)$, and the batch size is actually $m_1m_2$ because the generic GTD  essentially uses this number of pairs of the correlated TD errors from the two buffers (one has $m_1$ samples and the other $m_2$). Thus we can use Lemma \ref{lem:ED_avg} (which depends on the i.i.d. sampling that is ensured by Lemma \ref{lem:independence}) to get
\begin{align*}
\EED \norm{\tilde{A}_{m_1}^\tr (\tilde{A}_{m_2} \theta_{k} + \tilde{b}_{m_2})}^2 &=
\frac{1}{m_1m_2}\EED\norm{f'_{i,j}(x)}^2+ \left(1-\frac{1}{m_1m_2}\right) \norm{f'(x_t)}^2\\
&\ge \norm{f'(x_t)}^2 +\frac{1}{m_1m_2} \sigma_v^2 \\
&= \norm{A^\tr (A\theta_k+b)}^2+\frac{1}{m_1m_2} \sigma_v^2\\
&\ge \sigma_{\min}^2(A) \norm{A\theta_k+b}^2 +\frac{1}{m_1m_2} \sigma_v^2,
\end{align*}
where the first inequality is because of Lemma \ref{lem:g2andf2}.

Therefore, 
\begin{align*}
\min_{k=0, \ldots, t-1} 
 f(\theta_k) &= \min_{k=0, \ldots, t-1} 
 \norm{A\theta_k+b}^2 \\
 &\le \max\left\{\frac{2}{t \alpha \left(2- \alpha L_{\max} \right)\sigma^2_{\min}(A)}f (\theta_{0}) - \frac{1}{m_1m_2\sigma^2_{\min}(A)} \sigma_v^2, \, 0\right\}.
\end{align*}

\end{proof}
Theorem \ref{thm:rates_1_over_t} shows that out of the historical learning steps, we are guaranteed to find a moment with an $O(1/t)$ reduction of the initial loss.\footnote{It also shows that the minibatch update is helpful, because it enables more reduction than $O(1/t)$. For larger batch-sizes $m_1$ and $m_2$, this benefit grows smaller, indicating that smaller batch-sizes should converge faster. 
Although this is interesting, we found this is contradictory to our empirical results which we cannot explain why.} 

\citet{bo_gtd_finite} proved that certain variants of GTD and GTD2 converge at a rate of $O(t^{-1/4})$ with a high probability. The algorithm variants apply projections to the iterators to keep them bounded. The rate was proved by applying the rate analysis of the saddle point problem \citep{saddle_point_Nemirovski}. A key condition that guarantees this rate is the use of a {\em fixed} step-size, $1/\sqrt{t}$, by knowing the total number of iteration steps before hand. For example, if we want to learn $10000$ steps, at all the learning steps, the step-size is $0.01$. 

\citet*{dalal2018finite_twotimescale} established the convergence rates of a variant of GTD that projects the update back to a ball sparsely. With diminishing step-sizes $\alpha_t=t^{-(1-\tau)}$ and $\beta_t=t^{-2/3(1-\tau)}$, where $\kappa$ is some constant in $(0,1)$, they showed that the convergence rate is $O(t^{-1/3 + \kappa/3})$, which is roughly $O(t^{-1/3})$ at best. This is slightly faster than \citet{bo_gtd_finite}'s rate if $\kappa$ is small. One can understand that this is due to the use of a bigger step-size than the fixed $1/\sqrt{t}$ step-size.  
The rate also applies to GTD2 and TDC. This result was obtained by drawing inspiration from single-time-scale stochastic approximation \citep{borkar2008book}, in bounding the distance of the two-time-scale iterations to the trajectories that are generated by the O.D.E. 
An important condition for this distance to remain bounded is that the two step-sizes are scheduled to satisfy the two-time-scale condition.     

Later with the step-sizes $t^{-\alpha}$ and $t^{-\beta}$ respectively for the two iterators, which satisfy $0<\beta<\alpha<1$, they showed that the convergence rates are $O(t^{-\alpha/2})$ and $O(t^{-\beta/2})$ for the two iterators, and the bounds are tight \citep*{dalal2020tale_twotimescale}.\footnote{Note that in this paper, analysis of the GTD algorithm was applied with a projection operator very $2^i$ ($i=0, 1, \ldots$) steps to keep the update bounded. Our analysis does not use any projection. 
}  Given that GTD learns slower than GTD2 and TDC as found by empirical studies \citep{tdc,sutton2018reinforcement}, it is probably safe to say that GTD (without projection) converges no faster than this rate. 
In short, all the three GTD algorithms converge slower than $O(1/\sqrt{t})$. In fact, $O(1/\sqrt{t})$ is the theoretical rate limit of stochastic saddle-point problem \citep{bo_gtd_finite}. This means even if one uses advanced optimizers such as Stochastic Mirror-Prox \citep*{Mirror_prox}, GTD, GTD2 and TDC will not converge any faster than $O(1/\sqrt{t})$. 

In contrast, 
our Theorem \ref{thm:rates_1_over_t}  shows that the Impression GTD algorithm together with the three other GTD algorithms converge at least as fast as $O(1/t)$, much faster than GTD, GTD2 and TDC. \cite*{shangtong_imgtd} proved an $O(\xi(t)ln(t)/t)$ rate for their Direct GTD under the diminishing step-size that scales with $1/t$, where $\xi(t)$ is some slowly growing function such as $ln^2(t)$. This rate is almost $O(1/t)$, and it does not need to take the minimum over the historical learning steps.    

% \begin{assumption}\label{assumption:stepsize} 
% Let $\alpha_k>0$ be a sequence that satisfies $\sum_t \alpha_t^2 <\infty $ and $\sum_t \alpha_t =\infty$. 
% \end{assumption}



% The GTD paper \citep{gtd} contains the theorems and proofs for the cases of i.i.d. setting and the off-policy sub-sampling process individually. 
% There is actually no need to deal with them separately. In the following theorem, the distribution $\bold{d}$ is the state distribution of a data set that can be from any source, e.g., samples collected by a behavior policy or multiple policies that may be either behavior or target policies. 

% The proof for the following theorem is for the algorithm using the batch sizes equal to 1, which is easily generalized to any batch size. 


% \begin{thm} \label{thm:convergence_imgtd}
% Consider independence sampling with batch sizes $m_1=m_2=1$. 
% In particular, the algorithm maintains a buffer that inserts the samples up to the current time step according to the method in equations \ref{eq:buffer1} and \ref{eq:buffer2}.
% This means in the limit the buffer size grows to infinity as well. 
% Denote the distribution of the states in the buffer by $d_t$ at time step $t$. Assume $\lim_{t\to\infty}d_t$ exists, and let $\lim_{t\to\infty}d_t=d$. The feature matrix is $\Phi$, whose $i$th row is $\phi(i)^\tr$. Let the target policy be $\pi$. 


% The Impression GTD algorithm converges with probability one to the solution of a linear system, $A\theta + b=0$, where $A = \Phi^\tr D(\gamma P^\pi -I)\Phi$, and $b = \Phi^\tr D \bar{r}^\pi$. Here $D$ is the diagonal matrix with $D(i,i)=d(i)$, where $i\in \SS$; $P^\pi(i,j)=\sum_{a\in\AA}\pi(a|i)\PP(j|i,a)$; and $\bar{r}^\pi(i) = \sum_jP^\pi_{i,j}r(i,j) $. Assume that $A$ is non-singular. However, it is not necessarily negative definite.   
% \end{thm}
% \begin{proof}
% Let us start with the update, which can be written as
% \begin{align*}
% -\Delta \theta 
% &\propto \phi(\gamma\phi_2' - \phi_2)^\tr\theta + \phi r_2.\\
% &= (\gamma\phi'_1 - \phi_1) \phi_1^\tr \phi_2(\gamma\phi_2' - \phi_2)^\tr\theta + (\gamma\phi'_1 - \phi_1) \phi_1^\tr\phi_2 r_2.\\
% &=\left[(\gamma\phi'_1 - \phi_1)\phi_1^\tr \right]\phi_2\left[ (\gamma\phi_2' - \phi_2)^\tr\theta + r_2 \right],
% % &= (\gamma\phi'_1 - \phi_1) \phi_1^\tr \left[\phi_2(\gamma\phi_2' - \phi_2)^\tr\theta + \phi_2 r_2\right].\\
% \end{align*}
% where the second line is according to the definition of $\phi$. 

% Let us first understand the expected update of this algorithm, or the behavior of the algorithm in the ``average'' sense until time step $t$. Assume the buffer has samples $\{(s, s', r)\}$, whose number keeps growing as learning proceeds. Let the distribution of $s$ be $d_t$ at time step $t$, and the expectation operator conditioned on $d_t$ be $\EE_t$. At time step $t$, Impression GTD is a stochastic version of the following update:
% \begin{align*}
% \bar{\theta}_{\tau+1}
% &= \bar{\theta}_{\tau} -  \EE_t\left\{\Delta \theta|\theta_{\tau}\right\}\\
% &= \bar{\theta}_{\tau} - \alpha_\tau \EE_t\left\{ \left[(\gamma\phi'_1 - \phi_1)\phi_1^\tr \right]\phi_2\left[ (\gamma\phi_2' - \phi_2)^\tr\theta_{\tau} + r_2 \right]\right\}\\
% &= \bar{\theta}_{\tau} - \alpha_\tau \EE_t\left[(\gamma\phi'_1 - \phi_1)\phi_1^\tr \right] 
% \EE_t
%  \left[ \phi_2(\gamma\phi_2' - \phi_2)^\tr\theta_{\tau} + r_2 \right]\\
% &= \bar{\theta}_{\tau} - \alpha_\tau \tilde{A}_{1}^\tr(\tilde{A}_{2}\theta + \tilde{b}_{2}),
% \end{align*}
% where $\tilde{A}_1$ and $\tilde{A}_2$ are estimates of $A$ from the two buffers (by the empirical means), and $\tilde{b}_2$ is the estimate of $b$ from Buffer $B_2$. The second line is because of independence sampling. 
% The third line follows due to Lemma \ref{lem:independence}, and the last line is because $\EE
%  \left[ \phi(\gamma\phi' - \phi)^\tr\right]=A$ and $\EE
%   [\phi r]= b$, given a state distribution $d(s)$ where $\phi(s)$, the  feature vectors are computed, e.g., see \citep{tsi_td}. 

% Assume the two buffers have the same number of samples. According to the law of large numbers, we can re-write the last line by 
% \begin{align*}
% \bar{\theta}_{\tau+1}&= \bar{\theta}_{\tau} - \alpha_\tau (A+ O(2/t)I)^\tr\left[(A+O(2/t)I)\theta + (b+O(2/t) \bold{1})\right]\\
% &= \bar{\theta}_{\tau} - \alpha_\tau A^\tr(A\theta_\tau + b)\\
% & \quad\quad\,\,  - \alpha_\tau A^\tr\left[O(2/t)\theta + O(2/t) \bold{1}\right]\\
% & \quad\quad\,\,  - \alpha_\tau O(2/t)  \left[(A+O(2/t)I)\theta + (b+O(2/t) \bold{1})\right]. 
% \end{align*}
% Thus the second and third lines in the last equality show that the perturbation noises both diminish at a rate of $O(1/t)$. 

% The idea of the proof is to formulate the Impression GTD update using this expected update plus some noise $\epsilon_t$. 
% \begin{align*}
% \theta_{t+1} &= \theta_t - \alpha_t \left[ A^\tr(A\theta_t +b) + \epsilon_t \right].
% \end{align*}
% It is straightforward to show that $\EE[\epsilon_t]=0$ because $\EE[\epsilon_t(A)]=0$ and $\EE[\epsilon_t(b)]=0$, where
% \[
% \epsilon_t(A)= A^\tr A-  \left[(\gamma\phi'_1 - \phi_1)\phi_1^\tr \right]\phi_2  (\gamma\phi_2' - \phi_2)^\tr,
% \]
% and 
% \[
% \epsilon_t(b)= A^\tr b -  \left[(\gamma\phi'_1 - \phi_1)\phi_1^\tr \right]\phi_2 r_2.
% \]
% Furthermore, 
% $\EE\norm{\epsilon_t(A)}$ and $\EE\norm{\epsilon_t(b})$ are both bounded. 
% Standard results from stochastic approximation applies \citep{borkar2000ode,yin1997stochastic}, e.g., Theorem 2 of \citep{tsi_td} applies because $-A^\tr A$ is guaranteed to be negative definite due to that $A$ is non-singular.\footnote{The process does not need to be Markovian though, as required by \citet{tsi_td}'s Theorem 2.} With the bounded moments from Assumption \ref{assumption:phir} and the step-size sequence in Assumption \ref{assumption:stepsize},  
% it follows that $\theta_t$ converges to the solution with probability one to the linear system $A^\tr(A\theta^*+b)=0$, which also satisfies $A\theta^* +b=0$ because $A$ is a non-singular, square matrix in this setting.  
% \end{proof}

We further show that the four algorithms actually converge in a linear rate to a biased solution. For that purpose, we first establish a convergence rate result for SGD, under the $L$-$\lambda$ smoothness. 

\begin{definition}[$L$-$\lambda$ smoothness] \label{def:L-lambda-smooth}  
If for all $x\in \RR^d$, function $f$ and $\mathcal{D}$ satisfy
\[
\EE_{\mathcal{D}}\norm{g_t(x)}^2 \le 2L (f(x) - f(x^*)) + \lambda \norm{x-x^*}^2 +  \sigma^2, 
\] 
we say that $f$ is $L$-$\lambda$ smooth smooth under distribution $\mathcal{D}$, or simply, $(f, \mathcal{D})\sim L$-$\lambda(  \sigma^2)$,
\end{definition}
This new definition of expected smoothness has a background in our Impression GTD setting. 
Note that in this definition, $\sigma^2$ can be any positive real number. \citeauthor{gower2019sgd_general} used $\sigma^2=\EED\norm{g_t(x^*)}^2$. %and they showed that in the over-parameterization case, $\sigma^2=0$.   
We will show that in our analysis, $\sigma^2$ is some different number.  
The new definition adds a term of $\lambda\norm{x-x^*}$ to allow for convergence analysis of GTD algorithms. This term improves the expected smoothness to be more {\em noise tolerant}, and thus the induced smoothness is more general. %The following three lemmas are independent of the SGD setting.     


\begin{lem}\label{lem:utrongly_norm_grad}
If $f$ is $\mu$-strongly quasi-convex, then we have for any $x\in \RR^d$, 
\[
\norm{f'(x)} \ge \mu \norm{x-x^*}.  
\]
\end{lem}
Appendix \ref{appendix:u_norm_grad} has the proof. 

\begin{lem} \label{lem:ES_qLsmooth}
    If $(f, \mathcal{D})$ $\sim$ $L$-$\lambda$ $( \sigma^2)$ for some $\sigma^2\ge 0$, then for any $x\in \RR^d$, we have 
    \[
   f(x) - f(x^*) \ge \frac{\norm{f'(x)}^2- \lambda\norm{x-x^*}^2 - (\sigma^2-\sigma_v^2)}{2L}, 
    \]
    where $\sigma_v^2 = \min_{x}\EED \norm{g_t(x)-f'(x)}^2$.
\end{lem}
The proof is in Appendix \ref{appendix:es_qls}.


The following theorem improves \citet*{gower2019sgd_general}'s Theorem 3.1 by removing the factor of two in the bias term because of the use of a refined definition of expected smoothness. The rate is also tightened for a faster rate with a $\mu^2$ term. Analysis on SGD usually drops $\EE \norm{\nabla f(x_t)}^2$ by relating it to $f(x_t)-f(x^*)$ first, and then drops $f(x_t)-f(x^*)$ due to $L$-smoothness and $f(x)\ge f(x^*)$, e.g., see \citep{gower2019sgd_general} and \citep*{sps_stepsize}.
This means their bounds on the convergence rate can be further tightened.  
Our proof keeps $f(x_t)-f(x^*)$, relates it to $\EE \norm{\nabla f(x_t)}^2$, and bounds the latter.  
This can be done by noting that $f(x) - f(x^*)$ can be lower bounded by the norm of the gradient together with the perturbation and the constant. 
By using Lemma \ref{lem:utrongly_norm_grad} for the strongly quasi-convexity of $f$, we have further
$\EE \norm{\nabla f(x_t) }^2\ge \mu^2 \EE \norm{x_t-x^*}^2$. 
\begin{thm}\label{thm:sgd_rate}
Assume $(f, \mathcal{D})$ $\sim$ $L\mbox{-}\lambda(\sigma^2)$ and $f$ is $\mu$-strongly quasi-convex. For SGD with batch update:
\[
x_{t+1} = x_t - \alpha_t\avg,
\]
we have 
\begin{align*}
\EE \norm{x_{t+1}-x^*}^2  &\le   \left[1-\left(\mu -\frac{\lambda}{L} \right) \alpha_t-\mu^2\alpha_t\left(\frac{1}{L}-\alpha_t  \right)\right] \EE\norm{\Delta_t}^2  + \frac{\alpha_t }{L}(\sigma^2-\sigma^2_v) + \frac{\alpha_t^2\sigma^2_v}{m},
\end{align*}
A linear convergence rate 
can be guaranteed for $\lambda\le L\mu$. 
Specifically, for a constant step-size $\alpha\le \frac{1}{L}$, we have 
\[
\EE \norm{x_{t}-x^*}^2 \le q^t\EE \norm{x_0-x^*}^2 +  \alpha\frac{   m(\sigma^2-\sigma^2_v) + L\alpha\sigma^2_v }{L m\left[\left(\mu -\frac{\lambda}{L} \right)+\mu^2\left(\frac{1}{L}-\alpha\right)\right]},
\]
where 
\[
q= 1-\left(\mu -\frac{\lambda}{L} \right) \alpha-\mu^2\alpha\left(\frac{1}{L}-\alpha  \right).
\]
\end{thm}
\begin{proof}
Let $\Delta_t = x_t - x^*$.
We have $\Delta_{t+1}= \Delta_t -  \alpha_t \avg$. 
Taking the squared $\ell_2$ norm and the conditional expectation gives 
\begin{align*}
\EED  \norm{\Delta_{t+1}}^2  & = \EED(\Delta_t -  \alpha_t \avg)^\top (\Delta_t -  \alpha_t \avg\nonumber \\
&= \norm{\Delta_t}^2  - 2 \alpha_t \EED\left[ \avg^\top \Delta_t|x_t\right] + \alpha_t^2 \EED \norm{\avg}^2\nonumber\\
&= \norm{\Delta_t}^2  - 2 \alpha_t \EED\left[ \avg\right]^\top \EED\left[\Delta_t\right] + \alpha_t^2 \EED \norm{\avg}^2\nonumber\\
&=
\norm{\Delta_t}^2  - 2 \alpha_t  \nabla f(x_t)^\top \Delta_t + \alpha_t^2 \EED \norm{\avg}^2
\end{align*}
where the third line is because $\Delta_t$ is independent of $\avg$ given $x_t$. The last line is due to the expected form of $f$, which gives $\EE[ g_t(x)]=\nabla f(x)$ for any $x$. 

Taking expectation over $x_t$ gives
\begin{align*}
\EE \norm{\Delta_{t+1}}^2 
&=\EE\norm{\Delta_t}^2  - 2 \alpha_t  \EE\left[\nabla f(x_t)^\top \Delta_t\right] + \alpha_t^2\EE \EED \norm{\avg}^2\\
&\le 
\EE \norm{\Delta_t}^2  - 2 \alpha_t  \EE\left(f(x_t)-f(x^*)+\frac{\mu}{2}\norm{\Delta_t}^2\right)  + \alpha_t^2 \EE\EED \norm{\avg}^2
\end{align*}
where the inequality is by the $\mu$-strongly quasi-convexity of $f$.


We have
\begin{align*}
\EE \norm{\Delta_{t+1}}^2 &\le (1-\mu\alpha_t) \EE \norm{\Delta_t}^2  - 2 \alpha_t   \EE\left(f(x_t)-f(x^*)\right)  + \alpha_t^2 \EE\EED \norm{\avg}^2\\
&= (1-\mu\alpha_t) \EE \norm{\Delta_t}^2  - 2\alpha_t  \EE\left(f(x_t)-f(x^*)\right) \\
& \quad + \alpha_t^2 \left( \frac{1}{m}\EE\EED\norm{g_t(x)}^2+ \left(1-\frac{1}{m}\right) \EE\norm{f'(x_t)}^2 \right)\\
&\le (1-\mu\alpha_t) \EE \norm{\Delta_t}^2  - 2\alpha_t\EE  \left(f(x_t)-f(x^*)\right) \\
& \quad + \alpha_t^2 \left( \frac{1}{m}\left( 2L\EE(f(x_t)-f(x^*)) + \EE \lambda\norm{\Delta_t}^2 + \sigma^2 \right)+ \left(1-\frac{1}{m}\right) 
\EE\norm{f'(x_t)}^2 \right)\\
&= \left(1-\mu\alpha_t +\frac{\lambda \alpha_t^2}{m} \right) \EE \norm{\Delta_t}^2 - 2\alpha_t\left(1 - \frac{\alpha_tL}{m} \right)\EE(f(x_t)-f(x^*)) \\
& \quad + \alpha_t^2\left(1-\frac{1}{m}\right) \EE\norm{f'(x_t)}^2 + \frac{\alpha_t^2\sigma^2}{m}
\end{align*}
in which line 2 is by Lemma \ref{lem:ED_avg}, and line 3 is according to $L$-$\lambda$ smoothness.
The above holds for any $\alpha_t$. 
Then with $\alpha_t\le \frac{m}{L}$,
\begin{align*}
\EE \norm{\Delta_{t+1}}^2&\le \left(1-\mu\alpha_t+ \frac{\lambda \alpha_t^2}{m}  \right) \EE \norm{\Delta_t}^2 +
\alpha_t^2\left(1-\frac{1}{m}\right) \EE\norm{f'(x_t)}^2 + \frac{\alpha_t^2\sigma^2}{m} \\
& \quad  - 2\alpha_t\left(1 - \frac{\alpha_tL}{m} \right)\frac{1}{2L}\left( \EE \norm{f'(x_t)}^2 -\EE\lambda \norm{\Delta_t}^2 -(\sigma^2 - \sigma^2_v) \right)\\
% &= \left(1-\mu\alpha_t + \frac{\lambda\alpha_t}{L}  \right) \EE \norm{\Delta_t}^2  - \alpha_t\left( \frac{1}{L} - \alpha_t\right)   \EE \norm{f'(x_t)}^2 \\
% &\quad +  \alpha_t\left(1 - \frac{\alpha_tL}{m} \right)\frac{1}{L}(\sigma^2-\sigma^2_v)
% +  \frac{\alpha_t^2\sigma^2 }{m}  \\
&= \left(1-\mu\alpha_t+ \frac{\lambda \alpha_t}{L}\right) \EE \norm{\Delta_t}^2  - \alpha_t\left( \frac{1}{L} -\alpha_t\right)   \EE \norm{f'(x_t)}^2 +   \frac{\alpha_t }{L}(\sigma^2-\sigma^2_v) + \frac{\alpha_t^2\sigma^2_v}{m},  
\end{align*}
where line 1 is by Lemma \ref{lem:ES_qLsmooth}.
Furthermore, if $\alpha_t\le \frac{1}{L}$, we can use Lemma \ref{lem:utrongly_norm_grad} to get 
\begin{align*}
\EE \norm{\Delta_{t+1}}^2&\le 
\left(1-\mu\alpha_t+ \frac{\lambda \alpha_t}{L}\right) \EE \norm{\Delta_t}^2  - \alpha_t\left( \frac{1}{L} -\alpha_t\right)   \mu^2 \EE\norm{\Delta_t}^2 +   \frac{\alpha_t }{L}(\sigma^2-\sigma^2_v) + \frac{\alpha_t^2\sigma^2_v}{m} \\
&= 
\left(1-\left(\mu -\frac{\lambda}{L} \right)\alpha_t  - \mu^2 \alpha_t\left( \frac{1}{L} -\alpha_t\right)  \right) \EE \norm{\Delta_t}^2    +   \frac{\alpha_t }{L}(\sigma^2-\sigma^2_v) + \frac{\alpha_t^2\sigma^2_v}{m},
\end{align*}

In the constant step-size case, with the choice of $\alpha\le \frac{1}{L}$, a linear rate is guaranteed because
\begin{align*}
0\le q&\stackrel{def}{=}1-\left(\mu -\frac{\lambda}{L} \right)\alpha  - \mu^2 \alpha\left( \frac{1}{L} -\alpha\right) \le 1- \mu^2 \alpha\left( \frac{1}{L} -\alpha\right)\le 1,
\end{align*}
due to that $\lambda\le L\mu$.

We run the recursion repeatedly until the beginning and get
\begin{align*}
\EE\norm{\Delta_{t}}^2& \le q^t \EE\norm{\Delta_0}^2 +  \left( \frac{\alpha }{L}(\sigma^2-\sigma^2_v) + \frac{\alpha^2\sigma^2_v}{m} \right)  \sum_{s=0}^\infty q^s \\
&= q^t \EE\norm{\Delta_0}^2 + \alpha\frac{   m(\sigma^2-\sigma^2_v) + L\alpha\sigma^2_v }{L m\left[\mu - \frac{\lambda}{L}+\mu^2\left(\frac{1}{L}-\alpha\right)\right]}.
\end{align*} 
\end{proof}

This theorem extends Theorem 3.1 of \citep{gower2019sgd_general} in three ways. First, the SGD rate is established under $L$-$\lambda$ smoothness, which is weaker than expected smoothness. Second, the linear rate is tightened with a $\mu^2$ term even for $\lambda=0$. Third, the bias term is more refined in the numerator too, with the difference between $\sigma^2$ and the minimum variance. 


In the extreme case of $\lambda=L\mu$, although it guarantees a linear rate, for problems where $\mu$ is small, the rate can be very slow.
The factor $\lambda$ can be understood as the amount of perturbation to the expected smoothness condition. In particular, the more perturbation, the slower the rate. If the perturbation reduces, e.g.,  
if $\lambda \le (1-\rho) L\mu$ where $\rho \in [0, 1]$, then a much faster rate can be achieved:
\[
q\le 1- \rho\mu\alpha -\mu^2 \alpha\left( \frac{1}{L} -\alpha\right). 
\]
Luckily, as we will show later, in our Impression GTD setting, $\lambda$ can be reduced by increasing the batch sizes.  

%review later in the SGD paper
% Many SGD analysis works assume that $\EE \norm{g_t(x_t)}^2$ is bounded by a constant. For example, $\sigma$-bounded gradient \citep{reddi2016stochastic}.  \citet{ghadimi2013stochastic} assumes the global (unconditional) variance for non-convex but $L$-smooth functions.  Their final bounds have no batch size and it thus does not reveal its effect for variance reduction.



% The following theorems give the convergence rates of all the four GTD algorithms. 


%using two terms
% \begin{align*}
% &\quad \norm{f'_{i,j}(x) - f'_{i,j}(y)}\\ &= \norm{(\gamma \phi_{i+1}-\phi_i) \phi_i^\tr \phi_j (\gamma \phi_{j+1}-\phi_j)^\tr (x-y)  + 
% (\gamma \phi_{j+1}-\phi_j) \phi_j^\tr \phi_i (\gamma \phi_{i+1}-\phi_i)^\tr (x-y)} \\
% &\le \norm{(\gamma \phi_{i+1}-\phi_i) \phi_i^\tr \phi_j (\gamma \phi_{j+1}-\phi_j)^\tr (x-y)}  + \norm{
% (\gamma \phi_{j+1}-\phi_j) \phi_j^\tr \phi_i (\gamma \phi_{i+1}-\phi_i)^\tr (x-y)} \\
% &\le \norm{(\gamma \phi_{i+1}-\phi_i) \phi_i^\tr \phi_j (\gamma \phi_{j+1}-\phi_j)^\tr}\norm{x-y}  + \norm{
% (\gamma \phi_{j+1}-\phi_j) \phi_j^\tr \phi_i (\gamma \phi_{i+1}-\phi_i)^\tr}\norm{ x-y} \\
% &= L_{i,j} \norm{x-y}.
% \end{align*}

 
% We first give a general SGD rate result. For that purpose, we start with a variant of expected smoothness that was proposed by \citet{gower2019sgd_general}. After that, a few lemmas are proved, followed by the main SGD rate result under the new smoothness condition.


Now we are ready to prove the linear rate result of Impression GTD. This is achieved by applying the SGD rate in Theorem \ref{thm:sgd_rate}.
First we introduce a lemma to show that in the Impression GTD problem, the loss function (NEU) and the independence sampling is $L$-$\lambda$ smooth, which is required by Theorem \ref{thm:sgd_rate}. 

\begin{lem}\label{thm_item:Llambdasmoooth}
Let $\mu=\sigma^2_{\min}(A)$, i.e., the minimum singular values of $A$. Assume $\mu>0$. Let Assumption \ref{assumption:phir} hold. 
Let $\Sigma_A$ be the matrix of the standard deviations of the rank-1 sample matrix $\phi(\gamma \phi'-\phi)^\tr$. That is, $\Sigma_A(i,j)= \sqrt{Var(\phi(i)(\gamma \phi(j)-\phi(j)))}$. Let $\Sigma_b$ be the vector of the standard deviations of $\phi r$, i.e., $\Sigma_b(i)=\sqrt{Var(\phi(i)r)}$.\footnote{These are all properties of the considered MDP, feature functions, the behavior policy and the target policy.} 

Define the following constants due to NEU and the independence sampling, respectively:
\[
L_1 =4\left(\frac{\norm{\Sigma_A}^2}{m_1} +  \norm{A}^2\right), \quad \sigma^2 = 16\left(\frac{\norm{\Sigma_A}^2}{m_1} +  \norm{A}^2\right) \left(\frac{\norm{\Sigma_A}^2}{m_2} \norm{\theta^*}^2 + \frac{\norm{\Sigma_b}^2}{m_2}\right).
\]
and
\[
L_2 = \frac{\norm{\Sigma_A}^2}{m_2}, \quad \lambda = \frac{2\norm{\Sigma_A}^4}{m_1m_2}; 
\]
The NEU objective function and the independence sampling satisfy the $L$-$\lambda(\sigma^2)$ smoothness with $L=L_1+L_2$, $\lambda=\lambda$, and $\sigma^2=\sigma^2$.  
\end{lem}

\begin{proof}

First we have $x^\tr H x = x^\tr H^\tr x$ holds even for a non-symmetric matrix $H$. This is because 
\begin{align*}
x^\tr H x  = \sum_{i}\sum_j H_{i,j}x_ix_j= \sum_{j}\sum_i H_{i,j}x_ix_j= \sum_{i}\sum_j H_{j,i}x_ix_j=x^\tr H^\tr x,
\end{align*}
where equality 2 is by switching the order of the two sums, and equality 3 is by swapping $i$ and $j$. Thus $\norm{Hx} = \norm{H^\tr x}$ holds for any real matrix $H$ and real vector $x$. This equality is crucial in the analysis below. 

We have
\begin{align*}
&\EED\norm{\tilde{A}_{m_1}^\tr (\tilde{A}_{m_2} \theta_t + \tilde{b}_{m_2})}^2 \\
&= \EED\norm{(\tilde{A}_{m_1}-A+A) ^\tr \left((\tilde{A}_{m_2}-A) \theta_t + {A} \theta_t + b + (\tilde{b}_{m_2}-b)\right)}^2\\
&= \EED\norm{(\Delta^A_{m_1} + A)^\tr \left(\Delta^A_{m_2}(\theta_t-\theta^*) + \Delta^A_{m_2}\theta^* + (A\theta_t +b) + \Delta^b_{m_2} \right)}^2\\
&\le 2\EED\norm{(\Delta^A_{m_1} + A)^\tr \Delta^A_{m_2}(\theta_t-\theta^*)}^2 + 2\EED \norm{(\Delta^A_{m_1} + A)^\tr}^2 \norm{ \Delta^A_{m_2}\theta^*  +  (A\theta_t +b) + \Delta^b_{m_2} )}^2
\end{align*}
where we define $\Delta^A_{m}= \tilde{A}_m -A$, and $\Delta^b_{m}= \tilde{b}_m -b$.
Let's first examine the second term in the above equation: 
\begin{align*}
&\EED\norm{(\Delta^A_{m_1} + A)}^2 \norm{ \Delta^A_{m_2}\theta^*  +  (A\theta_t +b) + \Delta^b_{m_2} )}^2\\
&=\EED\norm{(\Delta^A_{m_1} + A)}^2 \EED \norm{ \Delta^A_{m_2}\theta^*  +  (A\theta_t +b) + \Delta^b_{m_2} )}^2\\
&\le 2
\EED \norm{\Delta^A_{m_1} + A}^2 \left(\EED \norm{ \Delta^A_{m_2}\theta^* + \Delta^b_{m_2}}^2   + \norm{A\theta_t +b}^2 \right)\\
&\le 8 \left(\frac{\norm{\Sigma_A}^2}{m_1} +  \norm{A}^2\right) \left(2\EED\norm{\Delta^A_{m_2}}^2 \norm{\theta^*}^2 + 2\EED \norm{\Delta^b_{m_2}}^2     + f(\theta_t) \right)\\
&\le \underbrace{16\left(\frac{\norm{\Sigma_A}^2}{m_1} +  \norm{A}^2\right) \left(\frac{\norm{\Sigma_A}^2}{m_2} \norm{\theta^*}^2 + \frac{\norm{\Sigma_b}^2}{m_2}\right)}_\text{$\sigma^2$}     + \underbrace{8 \left(\frac{\norm{\Sigma_A}^2}{m_1} +  \norm{A}^2\right)}_\text{2$L_1$}\left(f(\theta_t) -f(\theta^*) \right)\\
& = \sigma^2 + 2L_1 \left(f(\theta_t)-f(\theta^*) \right), 
\end{align*}
where the equality is due to the independence sampling. The first inequality uses Jensen's inequality. The second inequality uses Jensen's inequality, $Var(\frac{1}{m_1}\sum_{i=1}^m X_i) = \frac{1}{m_1}Var(X)$, where $\{X_i\}$ are i.i.d. samples of the random variable $X$; and the triangle inequality. The third inequality uses the above variance relationship again. 

The first term is 
\begin{align*}
&\quad\EED\norm{(\Delta^A_{m_1} + A)^\tr \Delta^A_{m_2}(\theta_t-\theta^*)}^2\\
&\le 2\EED\norm{{\Delta^A_{m_1}}^\tr \Delta^A_{m_2}(\theta_t-\theta^*)}^2 + 2\EED \norm{A^\tr \Delta^A_{m_2}(\theta_t-\theta^*)}^2\\
&\le 2\EED\norm{{\Delta^A_{m_1}}}^2 \norm{ \Delta^A_{m_2}}^2 \norm{\theta_t-\theta^*}^2 + 2\EED \norm{ {\Delta^A_{m_2}}^\tr A(\theta_t-\theta^*)}^2\\
&\le 
2\EED\norm{{\Delta^A_{m_1}}}^2 \EED\norm{ \Delta^A_{m_2}}^2 \norm{\theta_t-\theta^*}^2 + 2\EED \norm{\Delta^A_{m_2}}^2\norm{A(\theta_t-\theta^*)}^2\\
&= 2\frac{\norm{\Sigma_A}^2}{m_1}\frac{\norm{\Sigma_A}^2}{m_2}\norm{\theta_t-\theta^*}^2  + 2\frac{\norm{\Sigma_A}^2}{m_2}\norm{A\theta_t+b}^2\\
&= \underbrace{2\frac{\norm{\Sigma_A}^4}{m_1m_2}}_\text{$\lambda$}\norm{\theta_t-\theta^*}^2  + 2\underbrace{\frac{\norm{\Sigma_A}^2}{m_2}}_\text{$L_2$}\left(f(\theta_t) -f(\theta^*)  \right)\\
&= \lambda \norm{\theta_t-\theta^*}^2  + 2L_2 \left(f(\theta_t) -f(\theta^*)\right). 
\end{align*}
The first inequality uses Jensen's inequality. The second inequality uses the triangle inequality, and $\norm{Hx}= \norm{H^\tr x}$ due to that $x^\tr H x = x^\tr H^\tr x$. The third inequality uses the independence sampling and the triangle inequality. The first equality uses the variance equality that was used in proving the second term, and $b= -A\theta^*$. 

Therefore, $\EED\norm{\tilde{A}_{m_1}^\tr (\tilde{A}_{m_2} \theta_t + \tilde{b}_{m_2})}^2 \le 2(L_1+L_2) \left(f(\theta_t) -f(\theta^*)\right) + \lambda \norm{\theta_t-\theta^*}^2  +\sigma^2$. This proves that the NEU objective function and the independence sampling satisfy the $L$-$\lambda(\sigma^2)$ smoothness with the specified constants. 

\end{proof}

The following theorem is shows that, for Impression GTD, the linear rate can be obtained by large batch sizes, and we show a sufficient choice is $m_1=m_2\ge \lceil \frac{1}{\sqrt{2\mu}}\frac{\norm{\Sigma_A}^2}{\norm{A}^2} \rceil$. 
For Expected GTD, linear rate can be achieved after a key metric, $\frac{\norm{\Sigma_A}^2}{t^2}$ gets small for Expected GTD. For A$^\tr$TD and \oneexptd, the rate becomes linear once $\frac{\norm{\Sigma_A}^2}{t}$ gets small. This can be understood as that after we have a big enough number of samples that is proportional to the variance of the problem (or simply put, our buffers are {\em representative} of the true data distribution in the sense of the variances), the algorithms converge fast. The results also show that Expected GTD is faster than \oneexptd and A$^\tr$TD, \oneexptd is faster than A$^\tr$TD. 

\begin{thm}[Conv. Rates of Impression GTD, Expected GTD, A$^\tr$TD, \oneexptd]\label{thm:rates_all}  

We have the following convergence rate results. 

\begin{enumerate}

\item \label{thm_item:imGTD}
Impression GTD (\ref{eq:imgtd}). 
With batch sizes $m_1=m_2 \ge \lceil \frac{1}{\sqrt{2\mu}}\frac{\norm{\Sigma_A}^2}{\norm{A}} \rceil =m$,\footnote{The $1/\norm{A}$ can be roughly interpretted as the condition number of NEU. Thus this shows that the batch sizes should increase with the condition number of NEU and the variances of the feature transitions. The constant $\frac{1}{\sqrt{\mu}}$ is a good sign because it is much smaller than $1/\mu$, if $\mu$ is very small. }
the algorithm converges linearly and
the rate is given by Theorem \ref{thm:sgd_rate} by using a step-size
\[
\alpha \le \frac{1}{5\frac{\norm{\Sigma_A}^2}{m} +  4\norm{A}^2}.
\]  

\item \label{thm_item:expGTD}
Expected GTD (\ref{eq:expectedGTD}). 
There exists $t_0$, such that $t>t_0$, we have
$\frac{2\norm{\Sigma_A}^2}{t}\le \epsilon$, and with $\alpha \le \frac{1}{4\norm{A}^2}$, 
\begin{align*}
\EE \norm{x_{t+1}-x^*}^2  &\le   q \EE\norm{x_{t}-x^*}^2  + \frac{\alpha }{4\norm{A}^2}(\sigma^2-\sigma^2_v) + \frac{4\alpha^2\sigma^2_v}{t^2},
\end{align*}
where 
\[
q= 1- \mu \alpha - \mu^2  \alpha\left(\frac{1}{4\norm{A}^2+5\epsilon} -\alpha \right)  + \frac{2\alpha}{4\norm{A}^2}\epsilon^2.   
\]

 

\item \label{thm_item:attd} 
A$^\tr$TD (\ref{eq:attd}). For $t>t_0$ such that
$\frac{\norm{\Sigma_A}^2}{t}\le \epsilon$, with $\alpha\le \frac{1}{\max\{ 4\norm{A}^2, \norm{\Sigma_A}^2\}}$, we have
\begin{align*}
\EE \norm{x_{t+1}-x^*}^2  &\le   q \EE\norm{x_{t}-x^*}^2  + \frac{\alpha }{\max\{ 4\norm{A}^2, \norm{\Sigma_A}^2\}}(\sigma^2-\sigma^2_v) + \frac{\alpha^2\sigma^2_v}{t},
\end{align*}
where 
\[
q=  1- \mu \alpha - \mu^2  \alpha\left(\frac{1}{4\norm{A}^2 + {\norm{\Sigma_A}^2} + 4\epsilon} -\alpha \right)  + \frac{\norm{\Sigma_A}^2}{\max\{ 4\norm{A}^2, \norm{\Sigma_A}^2\}}\alpha\epsilon.  
\]

\item \label{thm_item:r1etd} 
\oneexptd (\ref{eq:R1-GTD}).\footnote{It is straightforward to extend this result to the shrinked R1-GTD algorithm that is discussed in Section \ref{sec:minibatchPE}. The result remains the same by just replacing $t$ with $m_2$ and requiring $m_2$ to be sufficiently large. Similarly, this can be done for a shrinked version of A$^\tr$TD. }
After $t>t_0$ such that
$\frac{\norm{\Sigma_A}^2}{t}\le \epsilon$, the algorithm converges linearly with $\alpha \le \frac{1}{4\left({\norm{\Sigma_A}^2} +  \norm{A}^2\right)}$: 
\begin{align*}
\EE \norm{x_{t+1}-x^*}^2  &\le   q \EE\norm{x_{t}-x^*}^2  + \frac{\alpha }{4\left({\norm{\Sigma_A}^2} +  \norm{A}^2\right)}(\sigma^2-\sigma^2_v) + \frac{\alpha^2\sigma^2_v}{t},
\end{align*}
where 
\[
q= 1- \mu \alpha - \mu^2  \alpha\left(\frac{1}{4\left({\norm{\Sigma_A}^2} +  \norm{A}^2\right) + \epsilon} -\alpha \right)  + \frac{\norm{\Sigma_A}^2}{4\left({\norm{\Sigma_A}^2} +  \norm{A}^2\right)}\alpha\epsilon.   
\]


\end{enumerate}

\end{thm}
\begin{proof}
\ref{thm_item:Llambdasmoooth}. 


\ref{thm_item:imGTD}. Impression GTD.  
Consider $m_1=m_2=m$.  
With 
\[
m \ge \left\lceil \frac{1}{\sqrt{2\mu}}\frac{\norm{\Sigma_A}^2}{\norm{A}} \right\rceil, 
\]
we have 
\[
\frac{4\norm{A}^2}{\norm{\Sigma_A}^2}m^2  + 5m -\frac{2\norm{\Sigma_A}^2}{\mu}> 0 
\]
This gives
\begin{align*}
 \frac{2\norm{\Sigma_A}^4}{\mu} &< 4\norm{A}^2m^2  + 5{\norm{\Sigma_A}^2}m \\
 &= 4\left(\norm{A}^2 + \frac{\norm{\Sigma_A}^2}{m} \right)m^2+\frac{\norm{\Sigma_A}^2}{m}m^2 \\
&=
{(L_1 + L_2)m^2}, 
\end{align*}
or equivalently, $\lambda < (L_1+L_2)\mu=L\mu$. Thus Theorem \ref{thm:sgd_rate} is applicable. The step-size condition can be derived by requiring that $\alpha \le \frac{1}{L}$. 

Next let's get the $\mu$ constant in the context of Impression GTD. 
Because $f'(\theta) = A^\tr (A\theta + b)$, 
we have  
\begin{align*}
 (x-y)^\tr (f'(x) -f'(y)) &= (x-y)^\tr  A^\tr A (x-y)\ge \sigma_{\min}^2(A)\norm{(x-y)}^2. 
\end{align*}
Thus $\mu = \sigma_{\min}^2(A)$. Thus we can apply Theorem \ref{thm:sgd_rate} and completes the proof for Impression GTD. 

\ref{thm_item:expGTD}. Expected GTD.  
For a sufficiently large $t>t_0$, we have
$\frac{2\norm{\Sigma_A}^2}{t}\le \epsilon$.
Note that 
\[
4\norm{A}^2\le L_1<L=L_1+L_2\le L_1 + \epsilon\le  4\norm{A}^2 + 5\epsilon,
\] 
which gives
\begin{align*}
\frac{\lambda}{L} &\le \frac{8\norm{\Sigma_A}^4}{4\norm{A}^2t^2} \le \frac{\epsilon^2}{2\norm{A}^2}; \quad -\frac{1}{L} \le - \frac{1}{4\norm{A}^2+5\epsilon}.
\end{align*}
According to Theorem \ref{thm:sgd_rate}, with $\alpha \le \frac{1}{4\norm{A}^2}$, 
\begin{align*}
\EE \norm{x_{t+1}-x^*}^2  &\le   q \EE\norm{\Delta_t}^2  + \frac{\alpha_t }{L}(\sigma^2-\sigma^2_v) + \frac{4\alpha_t^2\sigma^2_v}{t^2},
\end{align*}
where for the linear rate we have
\begin{align*}
q&=1-\left(\mu -\frac{\lambda}{L} \right)\alpha  - \mu^2 \alpha\left( \frac{1}{L} -\alpha\right) \\
&\le 1- \mu \alpha - \mu^2  \alpha\left(\frac{1}{4\norm{A}^2+5\epsilon} -\alpha \right)  + \frac{\alpha}{2\norm{A}^2}\epsilon^2.   
\end{align*}

\ref{thm_item:attd}. A$^\tr$TD. $m_1=t$ and $m_2=1$.
Note that $L_1$ still has a diminishing term but $L_2$ itself does not any more: 
\[
\quad L_1 = 4\left(\frac{\norm{\Sigma_A}^2}{t} +  \norm{A}^2\right), \quad 
L_2 = {\norm{\Sigma_A}^2}, \quad \lambda = \frac{2\norm{\Sigma_A}^4}{t},
\]
For $t>t_0$ such that
$\frac{\norm{\Sigma_A}^2}{t}\le \epsilon$, we have
\[
\max\{ 4\norm{A}^2, \norm{\Sigma_A}^2\}<L=L_1+L_2 \le  4\norm{A}^2 + {\norm{\Sigma_A}^2} + 4\epsilon.
\] 
Thus 
\begin{align*}
\frac{\lambda}{L} &< \frac{2\norm{\Sigma_A}^4}{\max\{ 4\norm{A}^2, \norm{\Sigma_A}^2\}t} \le \frac{2\norm{\Sigma_A}^2}{\max\{ 4\norm{A}^2, \norm{\Sigma_A}^2\}}\epsilon; \quad -\frac{1}{L} \le - \frac{1}{4\norm{A}^2 + {\norm{\Sigma_A}^2} + 4\epsilon}.
\end{align*}
Therefore, with $\alpha\le \frac{1}{\max\{ 4\norm{A}^2, \norm{\Sigma_A}^2\}}$,  the linear rate for A$^\tr$TD satisfies
\[
q= 1- \mu \alpha - \mu^2  \alpha\left(\frac{1}{4\norm{A}^2 + {\norm{\Sigma_A}^2} + 4\epsilon} -\alpha \right)  + \frac{2\norm{\Sigma_A}^2}{\max\{ 4\norm{A}^2, \norm{\Sigma_A}^2\}}\alpha\epsilon.   
\]
The bias in the rate can be bounded according to the lower bound of $L$. 

\ref{thm_item:r1etd}. \oneexptd. $m_1=1$ and $m_2=t$. The constants are now 
\[
L_1= 4\left({\norm{\Sigma_A}^2} +  \norm{A}^2\right), \quad L_2 =\frac{\norm{\Sigma_A}^2}{t}; \quad \lambda =  \frac{2\norm{\Sigma_A}^4}{t}.
\]
and the lower and upper bounds of $L$ are thus
\[
4\left({\norm{\Sigma_A}^2} +  \norm{A}^2\right)\le L_1<L=L_1+L_2\le L_1 + \epsilon\le  4\left({\norm{\Sigma_A}^2} +  \norm{A}^2\right) + \epsilon,
\] 
which gives
\begin{align*}
\frac{\lambda}{L} &< \frac{2\norm{\Sigma_A}^4}{4\left({\norm{\Sigma_A}^2} +  \norm{A}^2\right)t} \le \frac{\norm{\Sigma_A}^2}{2\left({\norm{\Sigma_A}^2} +  \norm{A}^2\right)}\epsilon; \quad -\frac{1}{L} \le - \frac{1}{4\left({\norm{\Sigma_A}^2} +  \norm{A}^2\right) + \epsilon}.
\end{align*}
With $\alpha \le \frac{1}{4\left({\norm{\Sigma_A}^2} +  \norm{A}^2\right)}$, the linear rate of \oneexptd  is thus
\[
q= 1- \mu \alpha - \mu^2  \alpha\left(\frac{1}{4\left({\norm{\Sigma_A}^2} +  \norm{A}^2\right) + \epsilon} -\alpha \right)  + \frac{\norm{\Sigma_A}^2}{2\left({\norm{\Sigma_A}^2} +  \norm{A}^2\right)}\alpha\epsilon.   
\]
\end{proof}
Comparing Theorem \ref{thm:rates_all} and Theorem \ref{thm:rates_1_over_t}, we can see that in Theorem \ref{thm:rates_1_over_t}, the step-size is much smaller, because in practice $L_{\max}$ can be very large. In that case, we are guaranteed to converge to the optimal solution, but with a slower rate. By using a much larger step-size in Theorem \ref{thm:rates_all}, we are only guaranteed to converge to a neighborhood of the optimal solution, however, with a much faster, linear rate.     


Theorem \ref{thm:rates_all}.\ref{thm_item:imGTD} shows that the step-size of Impression GTD for the fastest convergence depends on three factors: the variance in the transition, the $\ell_2$ norm of $A$ and the batch size. In particular, the higher is the variance of the transition or the bigger is $\norm{A}$, the smaller the step-size we need to use for Impression GTD. A bigger batch size enables a larger step-size and faster convergence. The side effect of a larger step-size, though, is that the bias term in the convergence rate increases, which means the final convergence point may be located in a larger neighborhood of the optimal solution.    

\begin{table}
\centering
% \begin{tabular}{l|l| l| l|l| l|l} 
%  \hline
%    & Batch size & $L_1-  4\norm{A}^2$ & $L_2$ & $\lambda$ & Linear rate if& Bias\\
%  \hline
%  Im.GTD &$m^2$ & ${4\norm{\Sigma_A}^2}/{m} $ & ${\norm{\Sigma_A}^2}/{m}$ & ${2\norm{\Sigma_A}^4}/{m^2}$ & $m \ge \frac{\norm{\Sigma_A}^2}{\norm{A}\sqrt{2\mu}}$ & ${\alpha^2\sigma^2_v}/{m}$ \\ 
%  Ex.GTD & $t^2/4$ & ${8\norm{\Sigma_A}^2}/{t} $ & ${2\norm{\Sigma_A}^2}/{t}$ & ${8\norm{\Sigma_A}^4}/{t^2}$ & ${2\norm{\Sigma_A}^2}/{t}\le \epsilon$ & ${4\alpha^2\sigma^2_v}/{t^2}$\\
%  A$^\tr$TD & $m_1=t, m_2=1$ & ${4\norm{\Sigma_A}^2}/{t}$ & $\norm{\Sigma_A}^2$ & ${2\norm{\Sigma_A}^4}/{t}$ & ${\norm{\Sigma_A}^2}/{t}\le \epsilon$ & ${\alpha^2\sigma^2_v}/{t}$ \\
%  R1-GTD& $m_1=1, m_2=t$ & $4{\norm{\Sigma_A}^2} $ & ${\norm{\Sigma_A}^2}/{t}$ & ${2\norm{\Sigma_A}^4}/{t}$& ${\norm{\Sigma_A}^2}/{t}\le \epsilon$& ${\alpha^2\sigma^2_v}/{t}$\\ 
%  \hline
% \end{tabular}
%delete the linear rate if column
\begin{tabular}{l|l| l| l| l|l} 
 \hline
   & Batch size & $L_1-  4\norm{A}^2$ & $L_2$ & $\lambda$ & Bias\\
 \hline
 Im.GTD &$m^2$ & ${4\norm{\Sigma_A}^2}/{m} $ & ${\norm{\Sigma_A}^2}/{m}$ & ${2\norm{\Sigma_A}^4}/{m^2}$ & ${\alpha^2\sigma^2_v}/{m}$ \\ 
 Expected GTD & $t^2/4$ & ${8\norm{\Sigma_A}^2}/{t} $ & ${2\norm{\Sigma_A}^2}/{t}$ & ${8\norm{\Sigma_A}^4}/{t^2}$ & ${4\alpha^2\sigma^2_v}/{t^2}$\\
 A$^\tr$TD & $m_1=t, m_2=1$ & ${4\norm{\Sigma_A}^2}/{t}$ & $\norm{\Sigma_A}^2$ & ${2\norm{\Sigma_A}^4}/{t}$  & ${\alpha^2\sigma^2_v}/{t}$ \\
 R1-GTD& $m_1=1, m_2=t$ & $4{\norm{\Sigma_A}^2} $ & ${\norm{\Sigma_A}^2}/{t}$ & ${2\norm{\Sigma_A}^4}/{t}$& ${\alpha^2\sigma^2_v}/{t}$\\ 
 \hline
\end{tabular}
\caption{GTD Algorithm factors. For Impression GTD, we consider $m_1=m_2=m$. The first column is the effective batch size. }
\label{table:alg_L_mu_lambda}
\end{table}

The constants are summarized in Table \ref{table:alg_L_mu_lambda} for comparison.
Let's take a look at the smoothness constants of A$^\tr$TD and \oneexptd. 
For A$^\tr$TD, 
\[
\quad L_1 = 4\left(\frac{\norm{\Sigma_A}^2}{t} +  \norm{A}^2\right), \quad 
L_2 = {\norm{\Sigma_A}^2}, \quad \lambda = \frac{2\norm{\Sigma_A}^4}{t},
\]
For \oneexptd,
\[
L_1= 4\left({\norm{\Sigma_A}^2} +  \norm{A}^2\right), \quad L_2 =\frac{\norm{\Sigma_A}^2}{t}; \quad \lambda =  \frac{2\norm{\Sigma_A}^4}{t}.
\]
Clearly, the two algorithms have the same $\lambda$. In one extreme (A$^\tr$TD), $L_1$ is small but $L_2$ is large. In the other extreme (\oneexptd), $L_1$ is big but $L_2$ is small and in fact diminishing. Thus Impression GTD can be viewed as a balance between the two algorithms in $L_1$ and $L_2$. Its complexity is much lighter than the two algorithms, but it is still linear in the number of features. Though still higher than GTD, the order is the same, both in $O(d)$. The storage of Impression GTD is much higher due to the buffers. However, memory is not usually not a concern in modern computers, with a wide application in deep learning and deep reinforcement learning. 
After a sufficiently large number of learning steps, Impression GTD converges slower than A$^\tr$TD and \oneexptd, but the rate is still a linear rate, which is much faster than GTD, GTD2 and TDC. Our result also shows that A$^\tr$TD is slower than R1-GTD and with a larger bias term.     

Comparing Expected GTD, A$^\tr$TD and \oneexptd, we can see that there is wait time for the algorithms to be converge linearly. In particular, the wait time is proportional to $\lambda/L$, i.e., the ratio of the perturbation to the expected smoothness. For Expected GTD, this perturbation in the rate $q$ is $\frac{\norm{\Sigma_A}^4}{4\norm{A}^2t^2}$.\footnote{
The constant of the perturbation is also interesting. In particular, this ratio shows that the condition number of NEU and the variances in the feature transitions all contribute to the perturbation.} For the latter two algorithms, the perturbations are  
\[
\mbox{A$^\tr$TD: }
\frac{2\norm{\Sigma_A}^4}{\max\{ 4\norm{A}^2, \norm{\Sigma_A}^2\}t}; \quad \mbox{\oneexptd: } \frac{2\norm{\Sigma_A}^4}{4\left({\norm{\Sigma_A}^2} +  \norm{A}^2\right)t}. 
\]
We can see that the perturbation is diminishing in time. For the case of Expected GTD, the diminishing rate is very fast, which is $O(1/t^2)$. 
Thus the wait time for Expected GTD to converge linearly is much shorter than A$^\tr$TD and \oneexptd.\footnote{One can show that the wait time of Expected GTD is $1/\sqrt{\epsilon}$ for achieving a bias proportional to $\epsilon$. In the theorem, we let the algorithm wait the same amount of time as A$^\tr$TD and \oneexptd, for which case, the bias of Expected GTD is $O(\epsilon^2)$. The two presentation forms are equivalent.} A$^\tr$TD and \oneexptd have similar perturbation, both in the order of $O(1/t)$. At a constant scale, the perturbation in R1-GTD is smaller, and thus it waits shorter than A$^\tr$TD for the linear rate to arrive. 
The results also show that there is no guarantee that the three GTD algorithms (and Impression GTD as well) would converge fast before a sufficiently large number of samples in the buffers. This can be understood as that we need a sufficient amount of statistics built in our buffers and it takes time to grow it.  

Note that if we use mini-batch versions for GTD, GTD2 and TDC, their convergence rate may be expected to converge faster as well. Algorithm 1 by \citet{xu2021sample_twotimescale} shows how such update can be done for TDC. They showed that this mini-batch TDC also converges at a linear rate. The rate was established by requiring that the two step-sizes are smaller than some upper bound number. The number for $\alpha$ is fairly complex, containing quite a few terms from the minimum eigenvalues of $A^\tr C^{-1}A$ and $C$, the maximum importance sampling ratio, the ergodicity factor of the underlying Markov chain and $\beta$, the other step-size as well. On one hand, their result and ours show that mini-batch training is indeed a very useful tool for accelerating stochastic approximation, and effective for both single-time scale and two-time scale algorithms. However, on the other hand, regardless of that the mini-batch TDC algorithm also has two step-sizes, which is hard to use in practice just like TDC, the rate they proved is a fairly slow one even though it is linear. To be concrete, in their Theorem 1, let $\lambda_1 = \lambda_{\min} (A^\tr C^{-1}A)$ and $\lambda_2 = \lambda_{\min}(C)$. The $\lambda_1$ and $\lambda_2$ factors correspond to $\mu$, the strong convexity factor for solving the underlying O.D.Es of the main iterator and helper iterator, respectively. Also let $\rho_{\max} = \max_{s, a} \frac{\pi(a|s)}{\pi_b(a|s)}$, the maximum importance sampling ratio across all state-action pairs. The condition for the theorem requires that $\alpha$ should be at least as small as the minimum of $\frac{\lambda_1\lambda_2}{12}$ and $\frac{\lambda_1\lambda_2^2}{256 \rho_{\max}^2} $. Both numbers are extremely small because in practice the minimum eigenvalues are usually small. The factor $\rho_{\max}$ is very large in off-policy learning, and scales like $1000$  or even much higher aren't uncommon. %Though earlier experiments on off-policy learning uses problems where the importance sampling ratios are not crazily big to demonstrate the efficacy of off-policy learning \citep{precup2001off}, in practice, the ratios can be very large. For example, see \citep{chen2022sufficiency} in the context of PPO \citep{ppo}, and \citep{munos2016safe,xie2019towards_IS} for methods to tame the ratios in the context of off-policy learning.   


In contrast, our result does not depend on $\rho_{\max}$ (at least not explicitly, it may still play a role in the conditioning number of $A$). In addition to the condition number of $A$, the (single) step-size in our result depends on the ratio between the variances in the feature transitions and the batch size(s), which means we can increase the step-size for larger batch sizes and also for problems in which feature transition variances are small.         

\footnote{This paragraph is due to a discussion with Csaba Szepesvari.} In literature, there is a result of $O(1/t)$ rate established for linear stochastic approximation algorithms, which also holds for a variant of GTD \citep*{csaba_lin_stochastic18}. The technique they used is iterate averaging \citep{polyak1992acceleration}, which iteratively averages the weight vector over all historical steps. Later, by adapting the step-size or using constant step-sizes that require prior knowledge of certain problem-dependent data structures, the same rate is also established for TDC with iterative averaging (over both the main and the helper iterators) \citep*{csaba_gtd_22}. This $O(1/t)$ is known to be information-theoretically near-optimal, e.g., see \citep{1_over_t_information_optiaml}. Thus it appears that our linear rate is contradictory to this well-known result. The catch is that our result has a bias because of the use of constant step-sizes. Although the results by \citeauthor{csaba_gtd_22} also contain the case of a special constant step-size, the averaging on the top of iterations provides a similar effect to the diminishing step-size, which enables their solution to converge to the true solution without a bias. To have a closer look of why our result is not contradictory, take the main result of \citeauthor{1_over_t_information_optiaml} (their Theorem 1) for example. The result states that, for any algorithm that comes up with a solution $\hat{x}$, there exists a data distribution (underlying the expectation operator in $f$) such that 
\begin{equation}\label{eq:worst_rate_1_over_t}
f(\hat{x}) - f(x^*) \ge c\, \min \left\{Y^2, \frac{B^2+dY^2}{t}, \frac{BY}{\sqrt{t}}
\right\}
\end{equation}
holds, 
where $B$ and $Y$ are some constants, $B\ge 2Y$, and $c$ is some positive constant. Now if $t$ is sufficiently large, the $O(1/t)$ term is the minimum of the three. Thus the result quantifies {\em the worst convergence rate to the optimal solution}. Precisely, the distance from any algorithmic solution to the optimal solution (in terms of the loss) cannot be anywhere closer than $O(1/t)$ for certain data distributions. For our result in Theorem \ref{thm:sgd_rate}, when $t$ is sufficiently large, for a constant step-size $\alpha\le \frac{1}{L}$, the first term becomes negligible, and we are left with the bias term, which is usually bigger than zero. Thus our theorem states that SGD converges to a {\em neighborhood} of the optimal solution $x^*$ {\em linearly fast}, but caution that it does not necessarily converge to $x^*$ linearly fast. The $O(1/t)$ rate to $x^*$ still applies to SGD and Impression GTD with diminishing step-size or iterate averaging. Note that the $O(1/t)$ information-theoretically near-optimal rate is the worst case, and it is realized on certain data distributions. For example, the proof of Theorem 1 in \citep{1_over_t_information_optiaml} is constructed by using an example in which the data distributions depend on the sample size. In practice, we are usually not that unlucky that our data distributions are screwed like such, and we may often get a faster rate than $O(1/t)$ when using SGD with mini-batch update. 

There is a special case that SGD will converge to the optimal solution $x^*$ linearly fast, no longer to just a neighbourhood of $x^*$. This corresponds to $B=Y=0$ in equation \ref{eq:worst_rate_1_over_t}. In this case, this bound is only an obvious fact instead of a rate. %In fact, even though the bound above is still correct for $B=Y=0$, it wasn't proved correctly. 
%This is because the proof of Theorem 1 in \citep{1_over_t_information_optiaml} fails to discuss this case. For example, the proof of Theorem 1 compares the sample size with $B^2/Y^2$, and it depends on Theorem 3, which uses $Y/B$ in the two designed distributions. 
Our bound such as Theorem \ref{thm:sgd_rate} correctly covers a subclass of this case, with $L$-$\lambda$ smoothness for the loss and the sampling, the convergence of SGD is linear, with a zero bias. 
Interestingly, in Baird counterexample (\ref{exp:baird}), we actually see this linear rate to the optimal solution in experiments, because the bias term there is zero due to that $B=Y=0$. 
%Because the four GTD algorithms we analyzed are SGD algorithms, the above arguments also hold for them.  
