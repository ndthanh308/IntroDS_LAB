 \section{Impression GTD}\label{sec:imgtd}
GTD has two step-sizes. In this section, we introduce a new Gradient TD algorithm that has only one step-size, e.g., see the six design desiderata as discussed in Section \ref{sec:introduction}. 
 
 Our idea is to decouple the two estimations in GTD by a special sampling method that is going to be detailed later. To do this, we use a buffer that stores transitions. At a time step $t$, we sample two i.i.d. transitions from the buffer,   
$(\phi_1, r_1, \phi_1')$ and $(\phi_2, r_2, \phi_2')$. Note the shorthand $\phi_1=\phi(s_1)$ is for some state $s_1 \in \SS$, and $\phi_2=\phi(s_2)$ for some $s_2\in \SS $. 

Our algorithm updates the parameter vector by
\begin{equation}\label{eq:one_update}
\Delta\theta_{t} = - \alpha_t (\gamma\phi'_1 - \phi_1) \bold{sim}(\phi_1,\phi_2)\left[(\gamma\phi_2' - \phi_2)^\tr\theta_t + r_2\right],
\end{equation}
where $\alpha_t$ is a positive step-size and $\bold{sim}$ is some similarity measure for the two input feature vectors. The update is interesting that the similarity seemingly ``pairs'' the gradient of a TD error on a transition with the TD error on another transition.  

Let's understand this update. If $r_2$ is a big reward, it likely creates a large TD error (the last term in the bracket). This TD error is bridged to adjust $V(s_1)$ and $V(s_1')$. That is, a TD error {\em impresses} another (independent) sample, based on which the parameters are adjusted. 
The bigger is the similarity between the two feature vectors, the larger impression of the TD error from one sample is going to make on the other. We call this new algorithm the {\em Impression} GTD.  

In this paper, we focus on the similarity measure being the correlation between the two feature vectors. 
Let us define $\phi = (\gamma\phi'_1 - \phi_1) \phi_1^\tr\phi_2$. The update can be rewritten into
\begin{align*}
\theta_{t+1} &=  \theta_t - \alpha_t \left[\gamma{\phi_2'}^\tr\theta_t +  r_2 - \phi_2^\tr\theta_t \right]\phi\\
 &=  \theta_t - \alpha_t \left[\gamma V(s_2')+  r_2 - V(s_2) \right]\nabla_{\theta} J.
\end{align*}
where $\alpha$ is the step-size and $\nabla_{\theta} J=\phi$. The overloading notation $\nabla_{\theta} J$ will be explained shortly. 

Interestingly, most incremental $O(d)$ TD algorithms known to the authors update the parameter based on one sample. This algorithm use two independent transitions for the update. It looks like the TD update, but not exactly so (because the transposed term has $\phi_2$ in the first line instead of $\phi$). In fact, it is a modification of the TD(0) update (or the so-called bootstrapping), whose key idea is to treat $V(s_{t+1})$ as a constant target in taking the gradient of the TD error, by combing the two sample transitions to form {\em truly an SGD} algorithm that minimizes the NEU objective.\footnote{\citet{gtd} had a comment that GTD is a SGD method. This is not very precise. GTD is two-time scale, and it is not the standard, single-time-scale SGD. We noted in literature this interpretation of GTD (and GTD2 and TDC) is not rare, e.g., see \citep{td_survey}. See also the discussions on page 35 by \citet{csaba_book}. A better terminology for GTD, GTD2 and TDC may be that they are pseudo-gradient methods as suggested. The exception is when GTD uses exactly the same step-size for the two iterators in the saddle-point formulation \citep{bo_gtd_finite}. Empirical results show that in order for good convergence across domains, one has to use different ratios for the two step-sizes, e.g., see \citep{tdc,martha2020gradient}. For example, Figure 2 of \citep{martha2020gradient} shows that GTD2 generally prefers a larger step-size for the helper iterator in four out of five domains.  However, for TDC, in three domains, it prefers actually slower update for the helper iterator. This is not covered by the theory of two-time scale stochastic approximation. }

There has been a mystery about the function $J$ for decades. In particular, what form should $J$ take for the convergence guarantee of TD methods? The TD methods were developed by treating $V(s_2')$ as the target and taking just $-\nabla_\theta V(s_2)$ as $\nabla_\theta J$. For example, it is common in literature to call $V(s_2')$ (or $\gamma V(s_2')+r_2$ ) the ``TD target'', the essential quantity for bootstrapping \citep{sutton2018reinforcement}.
Treating $V(s_2')$ as the target is also the essential idea for using neural networks for TD methods. For example, \citet{tdgammon}'s TD-gammon is the first such successful example. In DQN, \citet{mnih2015human} used the target network that is a historical snapshot of the network to generate relatively stable targets.  
Counterexamples show that TD can diverge if (1) nonlinear function approximation is used (even for on-policy learning); (2) learning is off-policy (even in the linear case); and (3) bootstrapping (TD methods with the eligibility trace factor smaller than one). This is referred to as the deadly triad \citep{sutton2018reinforcement,zhang2021breaking}.
Historical efforts that research into what form of $J$ guarantees convergence include re-weighted least-squares \citep{bertsekas1995counterexample}, residual gradient \citep{baird1995residual}, and Grow-Support \citep{boyan1994generalization}, etc. These algorithms attempted to derive an algorithm that is either a contraction mapping or a stochastic gradient with the current transition. See also \citep{bo_gtd_finite} for a good discussion and the long history of seeking gradient descent methods for temporal difference learning.  

Our algorithm may imply that this cannot be done with a single sample, if one wants to achieve the TD solution. In order to achieve that, we have to use two samples, in particular, 
\[
\nabla_\theta J = \left[\gamma \nabla_\theta V(s_1') - \nabla_\theta V(s_1)\right] \nabla_\theta V(s_1)^\tr \nabla_\theta V(s_2).
\]
In contrast, residual gradient takes $\nabla_\theta J=\gamma \nabla_\theta V(s_2') - \nabla_\theta V(s_2)$,  calculated on the same transition as where the TD error is computed. 
This shows why the residual gradient algorithm does not converge to the TD solution as discussed by \citet{tdc}. In order to converge to the TD solution, one needs to {\em compute the TD error and the gradient on two different (and independent) samples, also with a similarity measure to bridge them, instead of computing the TD error and the gradient on a single sample}. While the resulting algorithm is indeed an SGD algorithm, the independence sampling mechanism of two samples is different from supervised learning. That is, in supervised learning, one i.i.d. sample suffices for a well-defined SGD update. It has guaranteed convergence (with probability one) to the correct optimum. However, in the reinforcement learning setting, only one sample is not enough for ensuring this unless for deterministic environments.  
Although this still requires (at least) two i.i.d. samples at a time, note that the two samples do not need to be the i.i.d. transitions from the same state, because it is not practical to reset our state to the previous state to start over from there, ``passed is passed''.

Note the above update does not use the reward signal $r_1$. To take advantage of the two transitions, we also perform
\[
\theta_{t+1} =  \theta_t - \alpha_t \left[\gamma{\phi_1'}^\tr\theta_t + r_1 - \phi_1 ^\tr\theta_t\right]\phi,
\]
in which $\phi = (\gamma\phi'_2 - \phi_2) \phi_2^\tr\phi_1$ this time. This is due to that in using the two samples, the operation is symmetric.  
To ensure the two transitions are independent, in sampling we also require that they are from two different episodes. This can be done by an adding 
the episode index for each transition. Ours uses this special and novel sampling method to the best of our knowledge.\footnote{\citet*{shangtong_averagereward_two_iid} considered two i.i.d. samples from a given distribution in an average-reward off-policy learning algorithm, but not in a buffer setting like our method.} To differentiate from the uniform random sampling and prioritized sampling methods widely practised in literature, we call it the {\em independence sampling} method.   


The merit of independence sampling and Impression GTD is that together they remove the two steps-sizes and the resulting  tuning efforts and slow convergence. They achieve the decoupling of the two terms in GTD in a novel way. 
From a practical view, carrying a buffer is acceptable. Similar ideas appear in experience replay \citep{lin1992experiencereplay}, %model-based RL such as the linear Dyna \citep{lin_dyna} implementation \citep{multi-step-dyna} that saves the online feature vectors for later planing, 
and deep reinforcement learning \citep{mnih2015human,schaul2015prioritized}. 

Recently, \citet{shangtong_imgtd} developed a GTD algorithm that is very similar to ours as in equation \ref{eq:one_update}. They started with the same observation as ours, in that the gradient $A^\tr$ and the expected TD error in GTD's O.D.E. can be estimated separately. Their algorithm has a buffer as well, but the buffer length does not need to grow linearly as learning proceeds, while our analysis does have such a limitation.  
They also focused on the infinite-horizon setting, and the analysis is very much involved in the discretization of the underlying O.D.E. Our analysis is focused on the episodic problems though occasionally there are also discussions about infinite horizon problems as well. Our independence sampling is also an important ingredient, which facilitates an SGD analysis framework.  
In general, their direct GTD and our Impression GTD can be viewed as algorithms in the same family, with the same motivation and similar algorithmic flavour.   

In a summary, Impression GTD is guaranteed to converge under the same conditions on the MDP and linear features as GTD. Together with direct GTD \citep{shangtong_imgtd}, ours is the first theoretically sound, truly single-time-scale SGD off-policy learning algorithm, with $O(d)$ complexity and one step-size. In Section \ref{sec:theory} and Section \ref{sec:experiments}, we conduct theoretical analysis and empirical studies to show that the new algorithm converges much faster than GTD, GTD2 and TDC. 

We will detail the sampling process in the next section, which also introduces a more general form of this algorithm. 

\section{Mini-batch Policy Evaluation}\label{sec:minibatchPE}
This section further extends the Impression GTD. It is common to use mini-batch training in deep learning and deep reinforcement learning. There the mini-batch training paradigm is necessary mostly because the size of the data sets and the high dimensional inputs. Here we show that it also makes sense to use mini-batch training for off-policy learning, even in the linear case and even the problem size is not big. The motivation of using a buffer here has a different motivation from in deep learning and deep reinforcement learning though it also has the merit of improving sample efficiency and scaling to large problems. In short, the buffer is a tool for decoupling the error and gradient estimations in GTD. 

Let's start with on-policy learning. Suppose we maintain a buffer that is large enough. At each time step, we take an action according to the policy that is evaluated, observing a transition, $(\phi_t, \phi_t', r_t)$. We put the sample into the buffer. Next we sample a mini-batch of samples, $\{(\phi_i, \phi_i', r_i)\}, i=1, 2, \ldots, m$, where $m$ is the batch size.  We then update the parameter vector by the averaged TD update:
\[
\theta_{t+1} = \theta_t + \alpha_t \frac{1}{m}\sum_{i=1}^m \left(\gamma {\phi_i'}^\tr\theta_t +r_t - \phi_i^\tr\theta_t\right)\phi_i.
\]
We call this algorithm the {\em mini-batch TD}.

We follow by extending the Impression GTD for off-policy learning to work with mini-batch sampling. The buffer saves for each sample also the episode index within which a sample is encountered. At a time step, we sample two batches of samples,
\begin{equation}\label{eq:buffer1}
b_1=\{(\phi_i, \phi_i', r_i, e_i)|i=1, 2, \ldots, m_1\}, \quad b_2=\{(\phi_j, \phi_j', r_j, e_j)|j=1, 2, \ldots, m_2\}
\end{equation}
where $e_k$ is the episode index for the $k$th sample. In order for the samples in $b_1$ and $b_2$ to be independent, for any sample index pair, $i$ of $b_1$ and $j$ of $b_2$, we require that they are from different episodes:
\begin{equation}\label{eq:buffer2}
e_i\neq e_j, \quad for \quad \forall i=1, 2, \ldots, m_1; j=1, 2, \ldots, m_2.
\end{equation}
We first generate the averaged TD update from the samples in $b_2$, just like in the mini-batch TD:
\[
\bar{u}_{t} = \frac{1}{m_2} \sum_{j=1}^{m_2} \left(\gamma \phi_j'^\tr\theta_t +r_j - \phi_j^\tr\theta_t\right)\phi_j.
\]
Then for each sample in $b_1$, we compute $\bar{\delta}_t(i)=\phi_i^\tr \bar{u}_t$. 
Finally, the {\em mini-batch Impression GTD} update is
\begin{equation}\label{eq:imgtd}
\theta_{t+1} = \theta_t - \alpha_t \frac{1}{m_1}\sum_{i=1}^{m_1}(\gamma \phi_i' - \phi_i) \bar{\delta}_t(i).
\end{equation}

In the lookup table case,\footnote{In this case, with batch sizes $m_1=m_2=1$, the algorithm is a variant of Baird's RG, equipped with double-sampling. The algorithm converges to the true value function, while RG does not because RG only converges to the correct value function for deterministic MDPs. 
In fact, this is the place where double-sampling and independence sampling meet. Update to the weights happens only when $\phi_i$ and $\phi_j$ are the same, or, the two i.i.d. transitions are from the same state of the MDP. This is rare though, which also shows why mini-batch sampling leads to faster convergence than using batch sizes equal one. This observation was due to James MacGlashan.} this means the bigger is this $\bar{\delta}_t(i)$, the more eligible is this sample for a big update. Thus the update for $\theta(s_i)$ (or $V(s_i)$), and $\theta(s_i')$ (or $V(s_i')$) is big if $\bar{\delta}_t(i)$ is large. Note because $\bar{\delta}_t(i)= u_t(s_i)$ in this case, this largely agrees with prioritized sweeping \citep{moore1993prioritized}. Consider the table lookup case. When $|u_t(s_i)|$ is large, it means the TD update for the the component, $\theta(s_i)$, is big. Thus we can view Impression GTD as a way of adjusting the magnitude of the TD update in the original TD(0) algorithm and update based on the adjusted.



Consider for batch $b_1$, we have only one sample, e.g., the latest online sample, and $b_2$ has $m$ samples. This in fact is the standard online learning paradigm, hereby aided with some historical samples:\footnote{This is actually a ``shrinked'' version of R1-GTD. }
\begin{align*}
\Delta \theta_t &=  - \alpha_t (\gamma \phi_t' - \phi_t) \phi_t^\tr\frac{1}{m} \sum_{j=1}^m \left(\gamma \phi_j'^\tr\theta_t +r_j - \phi_j^\tr\theta_t\right)\phi_j\\
&=  - \alpha_t (\gamma \phi_t' - \phi_t) \frac{1}{m} \sum_{j=1}^m \left(\gamma \phi_j'^\tr\theta_t +r_j - \phi_j^\tr\theta_t\right)\phi_t^\tr\phi_j\\
&=  - \alpha_t (\gamma \phi_t' - \phi_t) \frac{1}{m} \sum_{j=1}^m \delta_j(\theta_t) \phi_t^\tr\phi_j=  - \alpha_t (\gamma \phi_t' - \phi_t) \frac{1}{m} \sum_{j=1}^m \delta_j(\theta_t) \mbox{sim}(s_t, s_j).
\end{align*}
The first line means, if the current feature vector is greatly correlated the averaged TD update from historical samples, the update for $\theta$ is likely to be big for the current transition, to reduce the difference between  $V(s_t)$ and $\gamma V(s_t')$. We could also say that it reduces the difference between $V(s_t)$ and $\gamma V(s_t')+r_t$ because the reward is a constant bias whose gradient is zero. The reward does not appear in $(\gamma \phi_t' - \phi_t)$ because it is already taken care of in the averaged TD update, which will be driven to zero as the update proceeds. The effect is that we use the averaged TD update (estimated independently) {\em projected} on the current feature vector for the parameter update.  

The second and third lines give a different interpretation of the algorithm. The algorithm replaces the TD error in the standard TD with an average TD error, {\em similarity weighted}. In particular, instead of using the current TD error, $\delta_t$, calculated on the latest transition, to trigger learning, as in the standard TD(0), it uses an average of the TD errors that are computed on independent samples, weighted by the similarity of the sampled historical feature vectors to the current feature vector, for learning. Thus our algorithm takes an approach that comes with an improved estimation for the error signal to prevent the divergence of TD(0) for off-policy learning, for which using the latest TD error is problematic.

Notably, this interpretation gives a connection to \citet{baird1995residual}'s Residual Gradient (RG) algorithm. If we replace the weighted averaged TD error in the  third line with the latest TD error, it becomes exactly RG. RG is guaranteed to converge, however, not to the TD solution, e.g., see \citep{tdc}. TD(0) uses the latest TD error in another way, however, it suffers from divergence for off-policy learning. This update is guaranteed to converge to the TD solution under general and the same conditions as GTD. Furthermore, the convergence is orders faster than GTD, as we will show in Section \ref{sec:theory}.

The complexity of mini-batch Impression GTD is $O((m_1+m_2)d)$ per step, where $m_1$ and $m_2$ are the batch sizes. It is more complex than the Impression GTD in Section \ref{sec:imgtd} and GTD. However, it is still a linear complexity that is scalable to large problems.

An easier implementation for independence sampling is to have two buffers. Before the start of an episode, we can choose a random number that is either zero or one with equal probability. If itâ€™s zero, then all the samples in this episode will be saved to the first buffer; otherwise, they will be saved to the second buffer. At sampling time, we just sample a batch from the first buffer and another batch from the second. In this way, we can also save extra memory for the episode index in each sample. Using the odd-even episode number for switching the buffers also works. This two-buffer implementation is shown in Algorithm \ref{alg:imGTD}. The similarity computation is also consumed so as to vectorize. 

\begin{algorithm}[t]
\caption{Impression GTD for off-policy learning, with independence sampling.}\label{alg:imGTD}
\begin{algorithmic}
\Require $\gamma \in (0, 1)$, the discount factor; $\alpha>0$, the step-size; $\phi(\cdot): \SS \to \RR^d$, the features
\State $\theta \gets \theta_0$
\Comment{Initialize the parameter vector}
\State buffer $B_1 \gets []$
\State buffer $B_2 \gets []$
\Comment{Initialize the buffers}
\For{episode $e=0, 1, \ldots$}

\State Environment resets to an initial state, $s_0$, drawn i.i.d. from some distribution 

\State $s\gets s_0$

\For{time step $t=0, 1, \ldots$}

    \State Observe $\phi(s)=\phi$, and take an action according to the behavior policy $\pi_b$

    \State Observe the next feature vector $\phi(s')=\phi'$ and reward $r$
    
\If{$e$ is odd} 
\Comment{Append the data in the same episode to same buffer}
    \State $B_1.\mbox{append}((\phi, \phi', r))$
\Else
    \State $B_2.\mbox{append}((\phi, \phi', r))$
\EndIf

\If{len$(B_2)>M$}
\State  
Sample a batch of $m_2$ samples from $B_2$, $\{(\phi_j, \phi_j', r_j)\}$, and compute
\[
\bar{u} \gets \frac{1}{m_2} \sum_{j=1}^{m_2} \left(\gamma \phi_j'^\tr\theta +r_j - \phi_j^\tr\theta\right)\phi_j.
\]

\State Sample $\{(\phi_i, \phi_i', r_i), i=1, \ldots, m_1\}$ from $B_1$ 

\State Form a feature matrix $\Phi$ with $\Phi[i,:] = \phi_i^\tr$
\Comment{The $i$th row of the matrix is $\phi_i^\tr$}

\State Compute $\bar{\bold{\delta}}=\Phi \bar{u}$

\State Update the parameters by
\[
\theta \gets \theta - \alpha \frac{1}{m_1}\sum_{i=1}^{m_1}(\gamma \phi_i' - \phi_i) \bar{\bold{\delta}}(i).
\]

\EndIf

\State $s\gets s'$
\EndFor
\EndFor
\end{algorithmic}
\end{algorithm}



In terms of the similarity measure used by our algorithm, the most relevant work is a recent new loss,  called the ``K-loss'' function \citep*{lihong_kernel_sim}, %\footnote{This paper was noted by Declan Oller internally at Sony AI after the writing and experiments of the paper were almost finished with exactly the same algorithm layout presented herein (with the $\bold{sim}$ notation and independence sampling). It is a great research coincidence that \citet{lihong_kernel_sim} used kernel embedding for the similarity measure. }, 
defined by the product of the Bellman errors calculated on two i.i.d. transition samples, weighted by a kernel encoding of the similarity between the two samples. They are probably the first to find that considering the similarity interplay between i.i.d. transitions can circumvent the double-sampling problem for reinforcement learning. Using their method, we can actually derive our algorithm in a second way. In particular, the NEU objective is
\begin{align}
\bold{NEU} &= \norm{\EE\delta\phi}^2\label{eq:neu_L2}\\
&=\EE[\delta\phi]^\top \EE[\delta\phi] \nonumber\\ 
&= \EE[\delta_1\phi_1]^\top \EE[\delta_2\phi_2] \Longleftrightarrow \EE[\delta_1\phi_1^\top \delta_2\phi_2] \nonumber \\
&= \EE[\phi_1^\top \phi_2 \delta_1 \delta_2] \nonumber\\
&= \EE[\bold{sim}(s_1, s_2) \delta_1 \delta_2].\label{eq:neu_2deltas}
\end{align}
This is exactly the place where \citeauthor*{lihong_kernel_sim} and we converge to. The  double-sampling problem arises when one aims to optimize using the single, online sample (the first line), which has held back the off-policy learning field for decades. 
The third line means we are realizing the $\ell_2$ norm on two independent transitions instead of on a single transition. The arrow annotated equality is due to the independence sampling.\footnote{Strictly speaking, the K-loss was defined using the Bellman errors, e.g., equation (3) of \citep{lihong_kernel_sim} uses the Bellman operator. The proof of their Corollary 3.5 mentioned "TD error". However the proof was done for deterministic MDPs for which TD error is the same as Bellman error.  Writing the loss in terms of  the weighted independent TD errors is more direct, also easier to interpret (without a model) and it entails direct optimization for practitioners. This is also interesting because minimizing the usual, online TD error via gradient descent has pitfalls \citep{tdc}, in particular the way represented by residual gradient \citep{baird1995residual}.} 
Therefore besides minimizing NEU, another way of interpreting our method is that it is a SGD method for minimizing the {\em expected product of two i.i.d. TD errors}, weighted by the similarity between the two feature vectors where the TD errors happened.

\begin{proposition}
Using independence sampling, we sample two independent transitions, $(s_1, r_1, s_1')$ and $(s_2, r_2, s_2')$ from the two buffers that have an infinity length. 
Consider a generic loss $\bold{N}(\theta) = \EE[\bold{sim}(s_1, s_2) \delta_1 \delta_2]$, where $\bold{sim}(s_1, s_2)$ is some similarity measure. Define $C= \EE[\phi(s)\phi(s)^\tr]$, where the expectation is taken with respect to the behavior policy (i.e., the distribution of $s$). Assume $C$ is non-singular. If $\bold{sim}(s_1, s_2)= \phi(s_1)^\tr C^{-1} \phi(s_2)$, then we have, $\bold{N}(\theta) = \bold{MSPBE}(\theta)$.  
\end{proposition}
\begin{proof}
We have 
\begin{align*}
\bold{N}(\theta) &= \EE[\bold{sim}(s_1, s_2) \delta_1 \delta_2] \\
&= \EE[\phi_1^\tr C^{-1} \phi_2 \delta_1\delta_2]\\
&= \EE[\delta_1 \phi_1^\tr C^{-1} \delta_2\phi_2 
]\\
&= \EE[\delta_1 \phi_1^\tr]  C^{-1} \EE[\delta_2\phi_2 
]\\
&= \bold{MSPBE}(\theta), 
\end{align*}
where the last second line is because of independence sampling, and $C^{-1}$ is a constant. The last line is because the buffers are sufficiently long so that the empirical distribution is the true data distribution. 
\end{proof}
Thus this shows that NEU and MSPBE belong to the same family of objective functions that are only different in a similarity measure, under independence sampling. Note this observation actually holds for any S.P.D matrix $U$ besides $C$. In particular, the generic loss $E(\theta)$ discussed in Section \ref{tdc} is also a special case of $\bold{N}(\theta)$.  
While these observations are interesting, we focus on minimizing NEU in this paper. 

Our method of deriving the Impression GTD algorithm by decoupling the estimations of $A^\top$ and $A\theta+b$ in GTD also entails an empirical form of the NEU loss, given multiple samples:
\[
\widehat{\bold{NEU}}(\theta|B_1, B_2) =\sum_{s_1 \in B_1}\sum_{s_2 \in B_2}\bold{sim}(s_1, s_2) \delta_1(\theta) \delta_2(\theta).
\]
For episodic problems, $B_1$ and $B_2$ are from our two-buffer implementation, for which samples in $B_1$ are always independent from the samples in $B_2$. 

For infinite horizon problems, $B_1$ and $B_2$ can be collected such that samples in them have a sufficiently large time window. For example, every $10000$ steps, we switch the collection buffer. In the first $T_0$ time steps, all the samples are inserted into $B_1$ and for the next $T_0$ time steps, the samples go into $B_2$; etc. A large $T_0$ ensures that no samples, for which the similarities are computed, happened close to each other in time, thus controlling their dependence strength at sampling time.    

Writing the NEU loss in terms of multiple samples is more reminiscent of the general machine learning problem where one minimizes an empirical loss on data sets. This is especially interesting because it transforms off-policy learning, an important problem of reinforcement learning, into a supervised learning problem, except that the data still needs to be collected for which there is the issue of exploration, etc. Nonetheless, we think it is an important connection to establish between reinforcement learning and supervised learning. This view is also interesting because the $\bold{sim}$ is a matrix form now, which measures inter-similarity between independent samples across the two buffers.\footnote{Note that \citeauthor{lihong_kernel_sim} did not have this form of the loss. Instead, they estimated the loss and the gradient using V-statistics. It has a problem that is discussed later in this section.}
Suppose the buffers keep adding the data and never drop any sample. Taking the gradient descent for the empirical NEU gives
\begin{align*}
\theta_{t+1} &= \theta_t - \frac{\alpha}{2} \nabla \widehat{\bold{NEU}} \\
&= \theta_t - \frac{\alpha}{2} \sum_{s_1 \in B_1}\sum_{s_2 \in B_2}\bold{sim}(s_1, s_2) \nabla(\delta_1 \delta_2) \\
&= \theta_t - \frac{\alpha}{2} \sum_{s_1 \in B_1}\sum_{s_2 \in B_2}\phi_1^\tr\phi_2 (\delta_1\nabla\delta_2 + \nabla\delta_1 \delta_2 )\\
&= \theta_t - \frac{\alpha}{2} \sum_{s_1 \in B_1}\sum_{s_2 \in B_2}\phi_1^\tr\phi_2 \left[ (r_1+\gamma\phi_1'^\tr \theta_t - \phi_1^\tr \theta_t) \nabla\delta_2 + (r_2+\gamma\phi_2'^\tr \theta_t - \phi_2^\tr \theta_t) \nabla\delta_1 \right]
\end{align*}
The two terms in the bracket is similar. For example, the first one is  (dropping the subscript of $\theta$ for simplicity)
\begin{align*}
 \phi_1^\tr \phi_2(r_1+\gamma\phi_1'^\tr \theta - \phi_1'^\tr \theta) \nabla\delta_2
% &= \phi_1^\tr\phi_2(r_1+\gamma\phi_1'^\tr \theta - \phi_1^\tr \theta) (\gamma \phi'_2 - \phi_2)\\
% &=(r_1+\gamma\phi_1'^\tr \theta - \phi_1^\tr \theta)\phi_1^\tr\phi_2 (\gamma \phi'_2 - \phi_2)\\
% &=(r_1+\gamma\phi_1'^\tr \theta - \phi_1^\tr \theta)\phi_2^\tr \phi_1 (\gamma \phi'_2 - \phi_2)\\
&=\phi_2^\tr \phi_1(r_1+\gamma\phi_1'^\tr \theta - \phi_1^\tr \theta) (\gamma \phi'_2 - \phi_2). 
\end{align*}
Suppose $T_1$ samples are stored in buffer $B_1$ and $T_2$ samples are in buffer $B_2$. We have 
\begin{align*}
&\sum_{s_1 \in B_1}\sum_{s_2 \in B_2} 
    \phi_2^\tr \phi_1(r_1+\gamma\phi_1'^\tr \theta - \phi_1^\tr \theta) (\gamma \phi'_2 - \phi_2) \\
    % =& \sum_{t_1=1}^{T_1}\sum_{t_2=1}^{T_2} 
    % \phi_2^\tr \phi_1(r_1+\gamma\phi_1'^\tr \theta - \phi_1^\tr \theta) (\gamma \phi'_2 - \phi_2) \\
    =& \sum_{t_2=1}^{T_2} \sum_{t_1=1}^{T_1}\phi_2^\tr\phi_1(r_1+\gamma\phi_1'^\tr \theta - \phi_1^\tr \theta)
     (\gamma \phi'_2 - \phi_2) \\
    =& \sum_{t_2=1}^{T_2} \phi_2^\tr(\tilde{A}_1 \theta+\tilde{b}_1)
  (\gamma \phi'_2 - \phi_2) \\
      =& \sum_{t_2=1}^{T_2}   (\gamma \phi'_2 - \phi_2)\phi_2^\tr(\tilde{A}_1 \theta+\tilde{b}_1)
 \\
  %    =& \sum_{t_2=1}^{2T+1} \phi_2^\tr(A_{2T} \theta+b_{2T})
  % (\gamma \phi'_2 - \phi_2) \\
      =& \tilde{A}_2^\tr (\tilde{A}_1 \theta+\tilde{b}_1),
\end{align*}
in which we define $\tilde{A}_1 = \sum_{t_1=1}^{T_1} \phi_1(\gamma \phi_1' - \phi_1)^\tr$, and $\tilde{A}_{2} = \sum_{t_2=1}^{T_2} \phi_2(\gamma \phi_2' - \phi_2)^\tr$. 
The normalized matrices, i.e., $\tilde{A}_1/T_1$ and $\tilde{A}_2/T_2$, are both consistent estimations of the matrix, $A=E[\phi(\gamma \phi' - \phi)^\tr]$. Note this algorithm can be implemented in a complexity that is linear in the number of samples ($n$), i.e., $O(d^2)$ per sample, where $d$ is the number of features, by forming the matrices explicitly. 


Therefore, if we go for a direct approach of minimizing the empirical NEU, it ends up with a variant of the expected GTD \citep{ptd_yao}, 
\[
  \theta_{t+1}   = \theta_t -\frac{\alpha}{2} \left[\tilde{A}_1^\tr (\tilde{A}_2 \theta_t+\tilde{b}_2) + \tilde{A}_2^\tr (\tilde{A}_1 \theta_t+\tilde{b}_1) \right],
\]
which is $O(d^2)$ per step (the two matrices can be aggregated incrementally).
Though an interesting variant of the expected GTD, this algorithm is presented purely for the understanding of Impression GTD. Our convergence and convergence rate analysis apply to this variant in a straightforward way. 

 The Impression GTD applies the successful mini-batch training in deep learning to off-policy learning and reduces to a linear complexity in the number of features, without resorting to two-time scale stochastic approximation. This observation was also made by \citet{lihong_kernel_sim}. They noted that their loss function ``coincides'' with NEU in the linear case (see their Section 3.3). However, the reason was not well understood or explained. Hopefully it is clear that our derivation above showed that this is not an coincidence. In matrix notation, the minibatch Impression TD uses the batch samples to build two matrices, $\tilde{A}_1^\tr = \sum_{b=1}^m (\gamma\phi_1' - \phi_1)\phi_1^\tr $, from the batch samples in $B_1$, and  $\tilde{A}_2 = \sum_{b=1}^m \phi_2  (\gamma\phi_2' - \phi_2)^\tr$ (and $\tilde{b}_2 = \sum_{b=1}^m \phi_2 r_2$), from the minibatch samples of $B_2$. The terms were transformed equivalently using the $\bold{sim}$ measure such that these matrices do not form explicitly and thus avoid the $O(d^2)$ complexity, e.g., see 
equation \ref{eq:imgtd}.\footnote{We found it's interesting that the two implementations, one that forms the matrices explicitly, and the other that doesn't (instead using $\bold{sim}$), gives the flexibility of switching for the higher computation efficiency given different numbers of samples (e.g., the batch sizes). }


\citeauthor{lihong_kernel_sim} also had a batch version of their algorithm, e.g., see their equation 4 and Section B.1 therein. However, the implementation is not technically sound because the independence of samples would break.  
The nature is a bit tricky.\footnote{We also refer the readers to \citep{shangtong_imgtd} for more detailed discussions about this problem.} Random sampling from the buffer does not necessarily means the samples in the buffer are i.i.d. 
Let's say we have two samples, $(s_1, s_1', r_1)$ and $(s_2, s_2', r_2)$. They are sampled i.i.d. from the buffer. However, what if they occurred in the same episode when we inserted them? Let's say $(s_2, s_2', r_2)$ was inserted into the buffer right after $(s_1, s_1', r_1)$. That is, $s_2=s_1'$. The second sample is dependent on the first one. In general, as long as the two samples are from the same episode, the one that happens at a later time depends on the former one and they are not independent.
It may be easier to understand in the infinite horizon setting. Suppose the Markov chain is irreducible and aperiodic and there exists a unique stationary distribution under the behavior policy. Then the samples are only independent of each other if the empirical distribution of the states in the buffer gets sufficiently close to the stationary distribution. Before this happens, the samples in the buffer are all dependent on each other. This depends on how fast the chain is mixing. It can take a very long time to reach the stationary distribution for slowly mixing chains. After the stationary distribution is reached, the Markovian argument for the above two samples still holds. However, because the distribution of states becomes stationary, the samples from the chain exhibit independence: the distribution of a state is just a property on its own. 
Consider a simple example. Assume that $s_2$ can only be reached from $s_1$. Note, however, from $s_1$ one can reach other state(s) than $s_2$. 
Let $\mu$ be the empirical distribution of the states in the buffer (a single buffer that stores all the samples up to the current time step). 
Then we have 
\[
\mbox{Prob}(s_2|s_1) = \mu(s_1) \mbox{Prob}(s1\to s_2)
\]
As long as $\mu(s_1) \mbox{Prob}(s1\to s_2)\neq \mu(s_2)$, $\mbox{Prob}(s_2|s_1)$ is not equal to $\mbox{Prob}(s_2)$, and thus the dependence between the two states holds. Before the chain reaches the stationary distribution, $\pi_0$, we don't have the equality. Only after $\mu$ gets sufficiently close to $\pi_0$, we have $\mu(s_1)\mbox{Prob}(s_1\to s_2)\approx \mu(s_2)$, and the independence between the states starts to exhibit. 

For episodic problems, one can define a similar chain from the distribution of the initial states (where the episodes are started), the behavior policy and the transition dynamics of the MDPs. If a unique stationary distribution exists, similar argument holds for the episodic problems. Most reinforcement learning problems in practice are episodic. Luckily, our independence sampling ensures the samples are independent even when the underlying chain has not reached the stationary distribution yet. This is shown by Lemma \ref{lem:independence} in Section \ref{sec:theory}. 

 

