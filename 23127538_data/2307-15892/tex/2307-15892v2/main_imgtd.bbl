\begin{thebibliography}{100}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Baird(1995)]{baird1995residual}
Leemon Baird.
\newblock Residual algorithms: Reinforcement learning with function
  approximation.
\newblock In \emph{Machine Learning Proceedings 1995}, pages 30--37. Elsevier,
  1995.

\bibitem[Bertsekas(2012)]{bertsekas2012dynamic}
Dimitri Bertsekas.
\newblock \emph{Dynamic programming and optimal control: Volume I}, volume~1.
\newblock Athena scientific, 2012.

\bibitem[Bertsekas(1995)]{bertsekas1995counterexample}
Dimitri~P Bertsekas.
\newblock A counterexample to temporal differences learning.
\newblock \emph{Neural computation}, 7\penalty0 (2):\penalty0 270--279, 1995.

\bibitem[Bertsekas and Ioffe(1996)]{lspe96}
Dimitri~P. Bertsekas and Sergey Ioffe.
\newblock Temporal differences-based policy iteration and applications in
  neuro-dynamic programming.
\newblock Technical report, MIT, 1996.

\bibitem[Bertsekas and Tsitsiklis(1996)]{bertsekas1996neuro}
Dimitri~P Bertsekas and John~N Tsitsiklis.
\newblock \emph{Neuro-dynamic programming}.
\newblock Athena Scientific, 1996.

\bibitem[Bertsekas et~al.(2004)Bertsekas, Borkar, and Nedic]{lspe04}
Dimitri~P Bertsekas, Vivek~S Borkar, and Angelia Nedic.
\newblock Improved temporal difference methods with linear function
  approximation.
\newblock \emph{Learning and Approximate Dynamic Programming}, pages 231--255,
  2004.

\bibitem[Borkar(2008)]{borkar2008book}
Vivek~S Borkar.
\newblock \emph{Stochastic approximation: a dynamical systems viewpoint}.
\newblock Cambridge University Press, 2008.

\bibitem[Boyan and Moore(1994)]{boyan1994generalization}
Justin Boyan and Andrew Moore.
\newblock Generalization in reinforcement learning: Safely approximating the
  value function.
\newblock \emph{Advances in neural information processing systems}, 7, 1994.

\bibitem[Boyan(2002)]{lstd}
Justin~A Boyan.
\newblock Technical update: Least-squares temporal difference learning.
\newblock \emph{Machine learning}, 49\penalty0 (2):\penalty0 233--246, 2002.

\bibitem[Bradtke and Barto(1996)]{bradtke1996linear}
Steven~J Bradtke and Andrew~G Barto.
\newblock Linear least-squares algorithms for temporal difference learning.
\newblock \emph{Machine learning}, 22\penalty0 (1):\penalty0 33--57, 1996.

\bibitem[Chandak et~al.(2021)Chandak, Niekum, da~Silva, Learned-Miller,
  Brunskill, and Thomas]{chandak2021universal_off_eval}
Yash Chandak, Scott Niekum, Bruno da~Silva, Erik Learned-Miller, Emma
  Brunskill, and Philip~S Thomas.
\newblock Universal off-policy evaluation.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 27475--27490, 2021.

\bibitem[Dalal et~al.(2018)Dalal, Thoppe, Sz{\"o}r{\'e}nyi, and
  Mannor]{dalal2018finite_twotimescale}
Gal Dalal, Gugan Thoppe, Bal{\'a}zs Sz{\"o}r{\'e}nyi, and Shie Mannor.
\newblock Finite sample analysis of two-timescale stochastic approximation with
  applications to reinforcement learning.
\newblock In \emph{Conference On Learning Theory}, pages 1199--1233. PMLR,
  2018.

\bibitem[Dalal et~al.(2020)Dalal, Szorenyi, and
  Thoppe]{dalal2020tale_twotimescale}
Gal Dalal, Balazs Szorenyi, and Gugan Thoppe.
\newblock A tale of two-timescale reinforcement learning with the tightest
  finite-time bound.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pages 3701--3708, 2020.

\bibitem[Dann et~al.(2014)Dann, Neumann, and Peters]{td_survey}
Christoph Dann, Gerhard Neumann, and Jan Peters.
\newblock Policy evaluation with temporal differences: A survey and comparison.
\newblock \emph{Journal of Machine Learning Research}, 15\penalty0
  (24):\penalty0 809--883, 2014.
\newblock URL \url{http://jmlr.org/papers/v15/dann14a.html}.

\bibitem[Dayan and Sejnowski(1994)]{dayan1994td}
Peter Dayan and Terrence~J Sejnowski.
\newblock {TD}($\lambda$) converges with probability 1.
\newblock \emph{Machine Learning}, 14\penalty0 (3):\penalty0 295--301, 1994.

\bibitem[Doan(2021)]{doan2021finite}
Thinh~T Doan.
\newblock Finite-time analysis and restarting scheme for linear two-time-scale
  stochastic approximation.
\newblock \emph{SIAM Journal on Control and Optimization}, 59\penalty0
  (4):\penalty0 2798--2819, 2021.

\bibitem[Du et~al.(2017)Du, Chen, Li, Xiao, and Zhou]{saddle_point_du}
Simon~S. Du, Jianshu Chen, Lihong Li, Lin Xiao, and Dengyong Zhou.
\newblock Stochastic variance reduction methods for policy evaluation.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning - Volume 70}, ICML'17, page 1049–1058. JMLR.org, 2017.

\bibitem[Espeholt et~al.(2018)Espeholt, Soyer, Munos, Simonyan, Mnih, Ward,
  Doron, Firoiu, Harley, Dunning, Legg, and Kavukcuoglu]{vtrace}
Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom
  Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, and
  Koray Kavukcuoglu.
\newblock Impala: Scalable distributed deep-rl with importance weighted
  actor-learner architectures, 2018.
\newblock URL \url{https://arxiv.org/abs/1802.01561}.

\bibitem[Feng et~al.(2019)Feng, Li, and Liu]{lihong_kernel_sim}
Yihao Feng, Lihong Li, and Qiang Liu.
\newblock A kernel loss for solving the bellman equation.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2019/file/48e59000d7dfcf6c1d96ce4a603ed738-Paper.pdf}.

\bibitem[Gelada and Bellemare(2019{\natexlab{a}})]{gelada2019off_IS_stationary}
Carles Gelada and Marc~G Bellemare.
\newblock Off-policy deep reinforcement learning by bootstrapping the covariate
  shift.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pages 3647--3655, 2019{\natexlab{a}}.

\bibitem[Gelada and Bellemare(2019{\natexlab{b}})]{gelada2019offpolicy_etd}
Carles Gelada and Marc~G. Bellemare.
\newblock Off-policy deep reinforcement learning by bootstrapping the covariate
  shift, 2019{\natexlab{b}}.

\bibitem[Geramifard et~al.(2006)Geramifard, Bowling, and Sutton]{ilstd}
Alborz Geramifard, Michael Bowling, and Richard~S Sutton.
\newblock Incremental least-squares temporal difference learning.
\newblock In \emph{Proceedings of the National Conference on Artificial
  Intelligence}, volume~21, page 356. Menlo Park, CA; Cambridge, MA; London;
  AAAI Press; MIT Press; 1999, 2006.

\bibitem[Ghadimi et~al.(2019)Ghadimi, Ruszczyński, and
  Wang]{mengdi_wang_gtd_like_nasa}
Saeed Ghadimi, Andrzej Ruszczyński, and Mengdi Wang.
\newblock A single time-scale stochastic approximation method for nested
  stochastic optimization, 2019.

\bibitem[Ghiassian and Sutton(2021)]{ghiassian2021empirical_etd}
Sina Ghiassian and Richard~S. Sutton.
\newblock An empirical comparison of off-policy prediction learning algorithms
  in the four rooms environment.
\newblock \emph{arXiv:2109.05110}, 2021.

\bibitem[Ghiassian et~al.(2017)Ghiassian, Rafiee, and
  Sutton]{ghiassian2017first_etd}
Sina Ghiassian, Banafsheh Rafiee, and Richard~S Sutton.
\newblock A first empirical study of emphatic temporal difference learning.
\newblock \emph{arXiv:1705.04185}, 2017.

\bibitem[Ghiassian et~al.(2020)Ghiassian, Patterson, Garg, Gupta, White, and
  White]{martha2020gradient}
Sina Ghiassian, Andrew Patterson, Shivam Garg, Dhawal Gupta, Adam White, and
  Martha White.
\newblock Gradient temporal difference learning with regularized corrections.
\newblock In \emph{Proceedings of the 37th International Conference on
  International Conference on Machine Learning}, 2020.

\bibitem[Golub and Van~Loan(2013)]{golub2013matrix}
Gene~H Golub and Charles~F Van~Loan.
\newblock \emph{Matrix computations}.
\newblock JHU press, 2013.

\bibitem[Gordon(1995)]{gordon1995stable}
Geoffrey~J Gordon.
\newblock Stable function approximation in dynamic programming.
\newblock In \emph{Machine learning proceedings 1995}, pages 261--268.
  Elsevier, 1995.

\bibitem[Gower et~al.(2019)Gower, Loizou, Qian, Sailanbayev, Shulgin, and
  Richtarik]{gower2019sgd_general}
Robert~Mansel Gower, Nicolas Loizou, Xun Qian, Alibek Sailanbayev, Egor
  Shulgin, and Peter Richtarik.
\newblock {SGD}: General analysis and improved rates.
\newblock \emph{arXiv:1901.09401}, 2019.

\bibitem[Gupta et~al.(2019)Gupta, Srikant, and Ying]{gupta2019finite}
Harsh Gupta, Rayadurgam Srikant, and Lei Ying.
\newblock Finite-time performance bounds and adaptive learning rate selection
  for two time-scale reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Hackman(2012)]{HTD_leah}
Leah Hackman.
\newblock Faster gradient-td algorithms.
\newblock Master's thesis, University of Alberta, 2012.

\bibitem[Hallak and Mannor(2017)]{hallak2017consistent}
Assaf Hallak and Shie Mannor.
\newblock Consistent on-line off-policy evaluation.
\newblock In \emph{International Conference on Machine Learning}, pages
  1372--1383. PMLR, 2017.

\bibitem[Hallak et~al.(2016)Hallak, Tamar, Munos, and
  Mannor]{hallak2015generalized_etd}
Assaf Hallak, Aviv Tamar, Remi Munos, and Shie Mannor.
\newblock Generalized emphatic temporal difference learning: Bias-variance
  analysis.
\newblock \emph{Proceedings of the AAAI Conference on Artificial Intelligence},
  30\penalty0 (1), Feb. 2016.
\newblock \doi{10.1609/aaai.v30i1.10227}.
\newblock URL \url{https://ojs.aaai.org/index.php/AAAI/article/view/10227}.

\bibitem[Hastie et~al.(2009)Hastie, Tibshirani, and
  Friedman]{hastie2009elements}
Trevor Hastie, Robert Tibshirani, and Jerome~H Friedman.
\newblock \emph{The elements of statistical learning: data mining, inference,
  and prediction}, volume~2.
\newblock Springer, 2009.

\bibitem[Hong et~al.(2020)Hong, Wai, Wang, and
  Yang]{hong2020two_bilevel_optimization}
Mingyi Hong, Hoi-To Wai, Zhaoran Wang, and Zhuoran Yang.
\newblock A two-timescale framework for bilevel optimization: Complexity
  analysis and application to actor-critic.
\newblock \emph{arXiv preprint arXiv:2007.05170}, 2020.

\bibitem[Horn and Johnson(2012)]{horn2012matrix}
Roger~A Horn and Charles~R Johnson.
\newblock \emph{Matrix analysis}.
\newblock Cambridge university press, 2012.

\bibitem[Hu et~al.(2019)Hu, Li, and Yu]{quasi_convex_hu2019convergence}
Yaohua Hu, Jiawen Li, and Carisa Kwok~Wai Yu.
\newblock Convergence rates of subgradient methods for quasi-convex
  optimization problems.
\newblock \emph{arXiv:1910.10879}, 2019.

\bibitem[Johnson and Zhang(2013)]{svrg}
Rie Johnson and Tong Zhang.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock \emph{Advances in neural information processing systems}, 26, 2013.

\bibitem[Juditsky et~al.(2011)Juditsky, Nemirovski, and Tauvel]{Mirror_prox}
Anatoli Juditsky, Arkadi Nemirovski, and Claire Tauvel.
\newblock {Solving variational inequalities with stochastic mirror-prox
  algorithm}.
\newblock \emph{Stochastic Systems}, 1\penalty0 (1):\penalty0 17 -- 58, 2011.
\newblock \doi{10.1214/10-SSY011}.
\newblock URL \url{https://doi.org/10.1214/10-SSY011}.

\bibitem[Kiwiel(2001)]{Kiwiel_quasi_convex}
Krzysztof Kiwiel.
\newblock Convergence and efficiency of subgradient methods for quasiconvex
  minimization.
\newblock \emph{Mathematical Programming, Series B}, 90:\penalty0 1--25, 01
  2001.
\newblock \doi{10.1007/PL00011414}.

\bibitem[Lakshminarayanan and Szepesvari(2018)]{csaba_lin_stochastic18}
Chandrashekar Lakshminarayanan and Csaba Szepesvari.
\newblock Linear stochastic approximation: How far does constant step-size and
  iterate averaging go?
\newblock In Amos Storkey and Fernando Perez-Cruz, editors, \emph{Proceedings
  of the Twenty-First International Conference on Artificial Intelligence and
  Statistics}, volume~84 of \emph{Proceedings of Machine Learning Research},
  pages 1347--1355. PMLR, 09--11 Apr 2018.
\newblock URL \url{https://proceedings.mlr.press/v84/lakshminarayanan18a.html}.

\bibitem[Lin(1992)]{lin1992experiencereplay}
Long-Ji Lin.
\newblock Self-improving reactive agents based on reinforcement learning,
  planning and teaching.
\newblock \emph{Machine learning}, 8\penalty0 (3):\penalty0 293--321, 1992.

\bibitem[Liu et~al.(2012)Liu, Mahadevan, and Liu]{bo_gtd}
Bo~Liu, Sridhar Mahadevan, and Ji~Liu.
\newblock Regularized off-policy td-learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 25, 2012.

\bibitem[Liu et~al.(2015)Liu, Liu, Ghavamzadeh, Mahadevan, and
  Petrik]{bo_gtd_finite}
Bo~Liu, Ji~Liu, Mohammad Ghavamzadeh, Sridhar Mahadevan, and Marek Petrik.
\newblock Finite-sample analysis of proximal gradient {TD} algorithms.
\newblock In Marina Meila and Tom Heskes, editors, \emph{Proceedings of the
  Thirty-First Conference on Uncertainty in Artificial Intelligence, {UAI}
  2015, July 12-16, 2015, Amsterdam, The Netherlands}, pages 504--513. {AUAI}
  Press, 2015.
\newblock URL \url{http://auai.org/uai2015/proceedings/papers/38.pdf}.

\bibitem[Liu et~al.(2018)Liu, Li, Tang, and
  Zhou]{liu2018breaking_IS_stationary}
Qiang Liu, Lihong Li, Ziyang Tang, and Dengyong Zhou.
\newblock Breaking the curse of horizon: Infinite-horizon off-policy
  estimation.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Loizou et~al.(2021)Loizou, Vaswani, Laradji, and
  Lacoste-Julien]{sps_stepsize}
Nicolas Loizou, Sharan Vaswani, Issam Laradji, and Simon Lacoste-Julien.
\newblock Stochastic {Polyak} step-size for {SGD}: An adaptive learning rate
  for fast convergence.
\newblock \emph{arXiv:2002.10542}, 2021.

\bibitem[Lyu et~al.(2020)Lyu, Qi, Ghavamzadeh, Yao, Yang, and
  Liu]{lyu2020variancereduced_etd}
Daoming Lyu, Qi~Qi, Mohammad Ghavamzadeh, Hengshuai Yao, Tianbao Yang, and
  Bo~Liu.
\newblock Variance-reduced off-policy memory-efficient policy search, 2020.

\bibitem[Maei et~al.(2009)Maei, Szepesvari, Bhatnagar, Precup, Silver, and
  Sutton]{maei2009convergent_nonlinear}
Hamid Maei, Csaba Szepesvari, Shalabh Bhatnagar, Doina Precup, David Silver,
  and Richard~S Sutton.
\newblock Convergent temporal-difference learning with arbitrary smooth
  function approximation.
\newblock \emph{Advances in neural information processing systems}, 22, 2009.

\bibitem[Mahmood et~al.(2014)Mahmood, Van~Hasselt, and
  Sutton]{mahmood2014weighted}
A~Rupam Mahmood, Hado~P Van~Hasselt, and Richard~S Sutton.
\newblock Weighted importance sampling for off-policy learning with linear
  function approximation.
\newblock \emph{Advances in Neural Information Processing Systems}, 27, 2014.

\bibitem[Mahmood et~al.(2015)Mahmood, Yu, White, and Sutton]{mahmood2015etd}
Ashique~Rupam Mahmood, Huizhen Yu, Martha White, and Richard~S Sutton.
\newblock Emphatic temporal-difference learning.
\newblock \emph{arXiv preprint arXiv:1507.01569}, 2015.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015human}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,
  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg
  Ostrovski, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Moore and Atkeson(1993)]{moore1993prioritized}
Andrew~W Moore and Christopher~G Atkeson.
\newblock Prioritized sweeping: Reinforcement learning with less data and less
  time.
\newblock \emph{Machine learning}, 13\penalty0 (1):\penalty0 103--130, 1993.

\bibitem[Nedic and Bertsekas(2003)]{lspe03}
Angelia Nedic and Dimitri~P. Bertsekas.
\newblock Least squares policy evaluation algorithms with linear function
  approximation.
\newblock \emph{Discret. Event Dyn. Syst.}, 13\penalty0 (1-2):\penalty0
  79--110, 2003.
\newblock \doi{10.1023/A:1022192903948}.
\newblock URL \url{https://doi.org/10.1023/A:1022192903948}.

\bibitem[Nemirovski et~al.(2009)Nemirovski, Juditsky, Lan, and
  Shapiro]{saddle_point_Nemirovski}
A.~Nemirovski, A.~Juditsky, G.~Lan, and A.~Shapiro.
\newblock Robust stochastic approximation approach to stochastic programming.
\newblock \emph{SIAM Journal on Optimization}, 19\penalty0 (4):\penalty0
  1574--1609, 2009.
\newblock \doi{10.1137/070704277}.
\newblock URL \url{https://doi.org/10.1137/070704277}.

\bibitem[Neumann and Morgenstern(1947)]{gametheory_Neumann}
John~Von Neumann and Oskar Morgenstern.
\newblock \emph{Theory of Games and Economic Behavior}.
\newblock Princeton University Press, 1947.

\bibitem[Pan et~al.(2017)Pan, White, and White]{pan2017accelerated_atd}
Yangchen Pan, Adam White, and Martha White.
\newblock Accelerated gradient temporal difference learning.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~31, 2017.

\bibitem[Patterson et~al.(2021)Patterson, Ghiassian, Gupta, White, and
  White]{patterson2021investigating_etd}
Andrew Patterson, Sina Ghiassian, D~Gupta, A~White, and M~White.
\newblock Investigating objectives for off-policy value estimation in
  reinforcement learning, 2021.

\bibitem[Peng et~al.(2020)Peng, Touati, Vincent, and Precup]{saddle_doina_svrg}
Zilun Peng, Ahmed Touati, Pascal Vincent, and Doina Precup.
\newblock Svrg for policy evaluation with fewer gradient evaluations, 2020.

\bibitem[Polyak and Juditsky(1992)]{polyak1992acceleration}
Boris~T Polyak and Anatoli~B Juditsky.
\newblock Acceleration of stochastic approximation by averaging.
\newblock \emph{SIAM journal on control and optimization}, 30\penalty0
  (4):\penalty0 838--855, 1992.

\bibitem[Precup(2000)]{precup2000eligibility}
Doina Precup.
\newblock Eligibility traces for off-policy policy evaluation.
\newblock \emph{Computer Science Department Faculty Publication Series},
  page~80, 2000.

\bibitem[Precup et~al.(2001)Precup, Sutton, and Dasgupta]{precup2001off}
Doina Precup, Richard~S Sutton, and Sanjoy Dasgupta.
\newblock Off-policy temporal-difference learning with function approximation.
\newblock In \emph{ICML}, pages 417--424, 2001.

\bibitem[Qian and Zhang(2023)]{shangtong_imgtd}
Xiaochi Qian and Shangtong Zhang.
\newblock Direct gradient temporal difference learning.
\newblock \emph{arXiv:2308.01170}, 2023.

\bibitem[Raj et~al.(2022)Raj, Joulani, Gyorgy, and Szepesvari]{csaba_gtd_22}
Anant Raj, Pooria Joulani, Andras Gyorgy, and Csaba Szepesvari.
\newblock Faster rates, adaptive algorithms, and finite-time bounds for linear
  composition optimization and gradient td learning.
\newblock In Gustau Camps-Valls, Francisco J.~R. Ruiz, and Isabel Valera,
  editors, \emph{Proceedings of The 25th International Conference on Artificial
  Intelligence and Statistics}, volume 151 of \emph{Proceedings of Machine
  Learning Research}, pages 7176--7186. PMLR, 28--30 Mar 2022.
\newblock URL \url{https://proceedings.mlr.press/v151/raj22a.html}.

\bibitem[Rowland et~al.(2020)Rowland, Harutyunyan, van Hasselt, Borsa, Schaul,
  Munos, and Dabney]{IS_conditioned_return}
Mark Rowland, Anna Harutyunyan, Hado van Hasselt, Diana Borsa, Tom Schaul, Remi
  Munos, and Will Dabney.
\newblock Conditional importance sampling for off-policy learning.
\newblock In Silvia Chiappa and Roberto Calandra, editors, \emph{Proceedings of
  the Twenty Third International Conference on Artificial Intelligence and
  Statistics}, volume 108 of \emph{Proceedings of Machine Learning Research},
  pages 45--55. PMLR, 26--28 Aug 2020.
\newblock URL \url{https://proceedings.mlr.press/v108/rowland20b.html}.

\bibitem[Saad(2003)]{saad03:IMS}
Yousef Saad.
\newblock \emph{{Iterative Methods for Sparse Linear Systems}}.
\newblock Other Titles in Applied Mathematics. SIAM, second edition, 2003.
\newblock ISBN 978-0-89871-534-7.
\newblock \doi{10.1137/1.9780898718003}.
\newblock URL
  \url{http://www-users.cs.umn.edu/\~{}saad/IterMethBook\_2ndEd.pdf}.

\bibitem[Schaul et~al.(2015)Schaul, Quan, Antonoglou, and
  Silver]{schaul2015prioritized}
Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver.
\newblock Prioritized experience replay.
\newblock \emph{arXiv preprint arXiv:1511.05952}, 2015.

\bibitem[Scherrer(2010)]{scherrer2010should_bellerr}
Bruno Scherrer.
\newblock Should one compute the temporal difference fix point or minimize the
  bellman residual? the unified oblique projection view.
\newblock \emph{arXiv preprint arXiv:1011.4362}, 2010.

\bibitem[Shamir(2015)]{1_over_t_information_optiaml}
Ohad Shamir.
\newblock The sample complexity of learning linear predictors with the squared
  loss.
\newblock \emph{Journal of Machine Learning Research}, 16\penalty0
  (108):\penalty0 3475--3486, 2015.
\newblock URL \url{http://jmlr.org/papers/v16/shamir15a.html}.

\bibitem[Sharifnassab and Sutton(2023)]{ran_gtd}
Arsalan Sharifnassab and Richard Sutton.
\newblock Toward efficient gradient-based value estimation.
\newblock In \emph{ICML}, 2023.

\bibitem[Sutton(1988)]{td}
Richard~S. Sutton.
\newblock Learning to predict by the methods of temporal differences.
\newblock \emph{Machine Learning}, 3\penalty0 (1):\penalty0 9--44, August 1988.
\newblock URL \url{http://www.cs.ualberta.ca/~sutton/papers/sutton-88.pdf}.

\bibitem[Sutton(1995)]{sutton1995generalization}
Richard~S Sutton.
\newblock Generalization in reinforcement learning: Successful examples using
  sparse coarse coding.
\newblock \emph{Advances in neural information processing systems}, 8, 1995.

\bibitem[Sutton and Barto(2018)]{sutton2018reinforcement}
Richard~S Sutton and Andrew~G Barto.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2nd edition, 2018.

\bibitem[Sutton et~al.(2008)Sutton, Maei, and Szepesv{\'a}ri]{gtd}
Richard~S Sutton, Hamid Maei, and Csaba Szepesv{\'a}ri.
\newblock A convergent {$O(n)$} temporal-difference algorithm for off-policy
  learning with linear function approximation.
\newblock \emph{Advances in neural information processing systems}, 21, 2008.

\bibitem[Sutton et~al.(2009)Sutton, Maei, Precup, Bhatnagar, Silver,
  Szepesv{\'a}ri, and Wiewiora]{tdc}
Richard~S Sutton, Hamid~Reza Maei, Doina Precup, Shalabh Bhatnagar, David
  Silver, Csaba Szepesv{\'a}ri, and Eric Wiewiora.
\newblock Fast gradient-descent methods for temporal-difference learning with
  linear function approximation.
\newblock In \emph{Proceedings of the 26th annual international conference on
  machine learning}, pages 993--1000, 2009.

\bibitem[Sutton et~al.(2011)Sutton, Modayil, Delp, Degris, Pilarski, White, and
  Precup]{sutton2011horde}
Richard~S Sutton, Joseph Modayil, Michael Delp, Thomas Degris, Patrick~M
  Pilarski, Adam White, and Doina Precup.
\newblock Horde: A scalable real-time architecture for learning knowledge from
  unsupervised sensorimotor interaction.
\newblock In \emph{The 10th International Conference on Autonomous Agents and
  Multiagent Systems-Volume 2}, pages 761--768, 2011.

\bibitem[Sutton et~al.(2016)Sutton, Mahmood, and White]{etd}
Richard~S Sutton, A~Rupam Mahmood, and Martha White.
\newblock An emphatic approach to the problem of off-policy temporal-difference
  learning.
\newblock \emph{The Journal of Machine Learning Research}, 17\penalty0
  (1):\penalty0 2603--2631, 2016.

\bibitem[Szepesv{\'a}ri(2010)]{csaba_book}
Csaba Szepesv{\'a}ri.
\newblock Algorithms for reinforcement learning.
\newblock \emph{Synthesis lectures on artificial intelligence and machine
  learning}, 4\penalty0 (1):\penalty0 1--103, 2010.

\bibitem[Tadi{\'c}(2001)]{tadic_td}
Vladislav Tadi{\'c}.
\newblock On the convergence of temporal-difference learning with linear
  function approximation.
\newblock \emph{Machine learning}, 42:\penalty0 241--267, 2001.

\bibitem[Tesauro(1995)]{tdgammon}
Gerald Tesauro.
\newblock Temporal difference learning and td-gammon.
\newblock \emph{Commun. ACM}, 38\penalty0 (3):\penalty0 58–68, mar 1995.
\newblock ISSN 0001-0782.
\newblock \doi{10.1145/203330.203343}.
\newblock URL \url{https://doi.org/10.1145/203330.203343}.

\bibitem[Tsitsiklis and Van~Roy(1997)]{tsi_td}
J.N. Tsitsiklis and B.~Van~Roy.
\newblock An analysis of temporal-difference learning with function
  approximation.
\newblock \emph{IEEE Transactions on Automatic Control}, 42\penalty0
  (5):\penalty0 674--690, 1997.
\newblock \doi{10.1109/9.580874}.

\bibitem[Tsitsiklis and Van~Roy(1996)]{tsitsiklis1996feature}
John~N Tsitsiklis and Benjamin Van~Roy.
\newblock Feature-based methods for large scale dynamic programming.
\newblock \emph{Machine Learning}, 22\penalty0 (1):\penalty0 59--94, 1996.

\bibitem[Wai et~al.(2020)Wai, Yang, Wang, and Hong]{neural_gtd2}
Hoi-To Wai, Zhuoran Yang, Zhaoran Wang, and Mingyi Hong.
\newblock Provably efficient neural gtd for off-policy learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 10431--10442, 2020.

\bibitem[Wang et~al.(2017)Wang, Liu, and Fang]{mengdi_stochastic_2exp}
Mengdi Wang, Ji~Liu, and Ethan~X. Fang.
\newblock Accelerating stochastic composition optimization.
\newblock \emph{J. Mach. Learn. Res.}, 18\penalty0 (1):\penalty0 3721–3743,
  jan 2017.
\newblock ISSN 1532-4435.

\bibitem[White and White(2016)]{htd_marhta}
Adam White and Martha White.
\newblock Investigating practical linear temporal difference learning.
\newblock \emph{arXiv:1602.08771}, 2016.

\bibitem[Xiao(2009)]{averaging_sgd_lin}
Lin Xiao.
\newblock Dual averaging method for regularized stochastic learning and online
  optimization.
\newblock In Y.~Bengio, D.~Schuurmans, J.~Lafferty, C.~Williams, and
  A.~Culotta, editors, \emph{Advances in Neural Information Processing
  Systems}, volume~22. Curran Associates, Inc., 2009.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2009/file/7cce53cf90577442771720a370c3c723-Paper.pdf}.

\bibitem[Xie et~al.(2019)Xie, Ma, and Wang]{xie2019towards_IS}
Tengyang Xie, Yifei Ma, and Yu-Xiang Wang.
\newblock Towards optimal off-policy evaluation for reinforcement learning with
  marginalized importance sampling.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Xu and Gu(2020)]{xu2020finite_q_learning}
Pan Xu and Quanquan Gu.
\newblock A finite-time analysis of q-learning with neural network function
  approximation.
\newblock In \emph{International Conference on Machine Learning}, pages
  10555--10565. PMLR, 2020.

\bibitem[Xu and Liang(2021)]{xu2021sample_twotimescale}
Tengyu Xu and Yingbin Liang.
\newblock Sample complexity bounds for two timescale value-based reinforcement
  learning algorithms.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 811--819. PMLR, 2021.

\bibitem[Xu et~al.(2019)Xu, Zou, and Liang]{xu2019two_time_gtd}
Tengyu Xu, Shaofeng Zou, and Yingbin Liang.
\newblock Two time-scale off-policy td learning: Non-asymptotic analysis over
  markovian samples.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Xu et~al.(2002)Xu, He, and Hu]{xu2002efficient}
Xin Xu, Han-gen He, and Dewen Hu.
\newblock Efficient reinforcement learning using recursive least-squares
  methods.
\newblock \emph{Journal of Artificial Intelligence Research}, 16:\penalty0
  259--292, 2002.

\bibitem[Yao and Liu(2008)]{ptd_yao}
Hengshuai Yao and Zhi-Qiang Liu.
\newblock Preconditioned temporal difference learning.
\newblock In \emph{Proceedings of the 25th International Conference on Machine
  Learning}, ICML '08, page 1208–1215, New York, NY, USA, 2008. Association
  for Computing Machinery.
\newblock ISBN 9781605582054.
\newblock \doi{10.1145/1390156.1390308}.
\newblock URL \url{https://doi.org/10.1145/1390156.1390308}.

\bibitem[Yao et~al.(2009{\natexlab{a}})Yao, Bhatnagar, and
  Szepesv{\'a}ri]{Yao_direct_preconditioning}
Hengshuai Yao, Shalabh Bhatnagar, and Csaba Szepesv{\'a}ri.
\newblock Temporal difference learning by direct preconditioning.
\newblock In \emph{Multidisciplinary Symposium on Reinforcement Learning
  (MSRL)}, 2009{\natexlab{a}}.

\bibitem[Yao et~al.(2009{\natexlab{b}})Yao, Bhatnagar, and Szepesvári]{lms2}
Hengshuai Yao, Shalabh Bhatnagar, and Csaba Szepesvári.
\newblock Lms-2: Towards an algorithm that is as cheap as lms and almost as
  efficient as rls.
\newblock In \emph{Proceedings of the 48h IEEE Conference on Decision and
  Control (CDC) held jointly with 2009 28th Chinese Control Conference}, pages
  1181--1188, 2009{\natexlab{b}}.
\newblock \doi{10.1109/CDC.2009.5400370}.

\bibitem[Yu(2015)]{yu2015convergence_etd}
Huizhen Yu.
\newblock On convergence of emphatic temporal-difference learning.
\newblock In \emph{Conference on learning theory}, pages 1724--1751. PMLR,
  2015.

\bibitem[Yu(2018)]{yu2018convergence_gtd}
Huizhen Yu.
\newblock On convergence of some gradient-based temporal-differences algorithms
  for off-policy learning.
\newblock \emph{arXiv:1712.09652}, 2018.

\bibitem[Zhang et~al.(2019)Zhang, Boehmer, and Whiteson]{zhang2019_rg}
Shangtong Zhang, Wendelin Boehmer, and Shimon Whiteson.
\newblock Deep residual reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1905.01072}, 2019.

\bibitem[Zhang et~al.(2020{\natexlab{a}})Zhang, Liu, and
  Whiteson]{zhang2020gradientdice}
Shangtong Zhang, Bo~Liu, and Shimon Whiteson.
\newblock Gradientdice: Rethinking generalized offline estimation of stationary
  values.
\newblock In \emph{International Conference on Machine Learning}, pages
  11194--11203. PMLR, 2020{\natexlab{a}}.

\bibitem[Zhang et~al.(2020{\natexlab{b}})Zhang, Liu, Yao, and
  Whiteson]{zhang2020provably}
Shangtong Zhang, Bo~Liu, Hengshuai Yao, and Shimon Whiteson.
\newblock Provably convergent two-timescale off-policy actor-critic with
  function approximation.
\newblock In \emph{International Conference on Machine Learning}, pages
  11204--11213. PMLR, 2020{\natexlab{b}}.

\bibitem[Zhang et~al.(2021)Zhang, Yao, and Whiteson]{zhang2021breaking}
Shangtong Zhang, Hengshuai Yao, and Shimon Whiteson.
\newblock Breaking the deadly triad with a target network.
\newblock In \emph{International Conference on Machine Learning}, pages
  12621--12631. PMLR, 2021.

\bibitem[Zhang et~al.(2022)Zhang, Wan, Sutton, and
  Whiteson]{shangtong_averagereward_two_iid}
Shangtong Zhang, Yi~Wan, Richard~S. Sutton, and Shimon Whiteson.
\newblock Average-reward off-policy policy evaluation with function
  approximation.
\newblock \emph{arXiv:2101.02808}, 2022.

\end{thebibliography}
