\section{Results} \label{sec:results}

Here we present several implementations for computing reciprocal square root with differing degrees for the refinement polynomial, together with a few examples for reciprocal and reciprocal cube root. All were found using the methods of this paper. The peak relative error is shown with each listing. Aside from the very first listing, for which the optimal solution had already been published, we believe all versions here yield greater accuracy than any others in their respective classes, demonstrating the usefulness of our techniques.

\subsection{Testing environment}

All tests were performed using code compiled with \texttt{gcc} for a 64-bit target, and run on an Intel Core i5 processor. This setup appears to be equivalent to that used by the majority of other authors, since the peak errors we find when testing their code agree precisely with the values published by those authors. Consistent with the work of other authors, the peak relative error of each function was measured over all positive normal floats (except where noted).

\subsection{Reciprocal square root}

A function returning the coarse approximation corresponds to a refinement step which uses the constant monic polynomial. In this case the optimal magic constant is the one found by \citet{lomont2003}:

\begin{minipage}{\linewidth}
\begin{lstlisting}[caption={$x^{-\frac{1}{2}}$ with degree-0 monic, $\epsilon$ =  $3.421284 \times 10^{-2}$}, label={lst:FRSR_Mon0}, language=C]
float FRSR_Mon0(float x)
{
  uint32_t X = *(uint32_t *)&x;
  uint32_t Y = 0x5F37642F - (X >> 1);
  return *(float *)&Y;
}
\end{lstlisting}
\end{minipage}

A general degree-0 polynomial incurs one extra multiply, but only yields a slight improvement in accuracy over the monic version:

\begin{minipage}{\linewidth}
\begin{lstlisting}[caption={$x^{-\frac{1}{2}}$ with degree-0 polynomial, $\epsilon$ = $2.943730 \times 10^{-2}$}, label={lst:FRSR_Deg0}, language=C]
float FRSR_Deg0(float x)
{
  uint32_t X = *(uint32_t *)&x;
  uint32_t Y = (0xBEBFFDAA - X) >> 1;
  float y = *(float *)&Y;
  return y * 0.79247999f;
}
\end{lstlisting}
\end{minipage}

With a degree-1 monic, the error reduces dramatically to $8.802292 \times 10^{-4}$, making it almost exactly twice as accurate as the original Quake code despite being faster than it by one multiply instruction:

\begin{minipage}{\linewidth}
\begin{lstlisting}[caption={$x^{-\frac{1}{2}}$ with degree-1 monic, $\epsilon$ = $8.802292 \times 10^{-4}$}, label={lst:FRSR_Mon1}, language=C]
float FRSR_Mon1(float x)
{
  uint32_t X = *(uint32_t *)&x;
  uint32_t Y = (0xBE167122 - X) >> 1;
  float y = *(float *)&Y;
  return y * (1.8909901f - x*y*y);
}
\end{lstlisting}
\end{minipage}

Using a general degree-1 polynomial corresponds to the classic FRSR algorithm. Our result improves on the \citet{kadlec2010} version:

\begin{minipage}{\linewidth}
\begin{lstlisting}[caption={$x^{-\frac{1}{2}}$ with degree-1 polynomial, $\epsilon$ = $6.501791 \times 10^{-4}$}, label={lst:FRSR_Deg1}, language=C]
float FRSR_Deg1(float x)
{
  uint32_t X = *(uint32_t *)&x;
  uint32_t Y = 0x5F5FFF00 - (X >> 1);
  float y = *(float *)&Y;
  return y * (1.1893165f - x*y*y*0.24889956f);
}
\end{lstlisting}
\end{minipage}

If we relax the requirement that same accuracy must hold all normal floats, there is a version even more accurate for $x < 1.8822997 \times 10^{38}$, beyond which it degrades slightly to a peak error of $ 6.502243 \times 10^{-4}$:

\begin{minipage}{\linewidth}
\begin{lstlisting}[caption={$x^{-\frac{1}{2}}$ with degree-1 polynomial, $\epsilon$ = $6.501686 \times 10^{-4}$ for $x < 1.8822997 \times 10^{38}$ }, label={lst:FRSR_Deg1Alt}, language=C]
float FRSR_Deg1Alt(float x)
{
  uint32_t X = *(uint32_t *)&x;
  uint32_t Y = 0x5F6004CC - (X >> 1);
  float y = *(float *)&Y;
  return y * (1.1891762f - y*y*x*0.24881148f);
}
\end{lstlisting}
\end{minipage}

A degree-$2$ monic uses one extra floating point add, but yields a $32$-fold improvement over our \texttt{FRSR\_Deg1()}, and an $86$-fold improvement over the original Quake code:

\begin{minipage}{\linewidth}
\begin{lstlisting}[caption={$x^{-\frac{1}{2}}$ with degree-2 monic, $\epsilon$ = $2.020644 \times 10^{-5}$}, label={lst:FRSR_Mon2}, language=C]
float FRSR_Mon2(float x)
{
  uint32_t X = *(uint32_t *)&x;
  uint32_t Y = 0x5F11107D - (X >> 1);
  float y = *(float *)&Y;
  float z = x*y*y;
  return y * (2.2825186f + z*(z-2.253305f));
}
\end{lstlisting}
\end{minipage}

A general degree-2 polynomial (not shown here) only improves the peak error by roughly $25\%$ over a degree-2 monic. Motivated by this finding and the corresponding ones for degrees 0 and 1, we have computed the peak relative errors of FRSR versions using both monic and general polynomials up to degree 6, assuming unlimited precision. The comparison is shown in Figure \ref{fig:monics}, with the errors plotted against the number of floating point operations assuming no FMA is available. We find that the versions which employ monic polynomials are much better placed with respect to the cost/accuracy trade-off than those using general polynomials. Of particular note is degree 6, where the monic peak relative error $8.027828 \times 10^{-12}$ is highly comparable to that for a general polynomial, $8.027660 \times 10^{-12}$.

% Figure environment removed



\subsection{Reciprocal}

Only the degree-1 version is shown, corresponding to the classic FRSR case:

\begin{minipage}{\linewidth}
\begin{lstlisting}[caption={$x^{-1}$ with degree-1 polynomial, $\epsilon$ = $1.116995 \times 10^{-4}$ for $x < 9.0209911 \times 10^{37}$}, label={lst:FRCP_Deg1}, language=C]
float FRCP_Deg1(float x)
{
  uint32_t X = *(uint32_t *)&x;
  uint32_t Y = 0x7FB504EC - X;
  float y = *(float *)&Y;
  return y * (0.6966215f - x*y*0.12130684f);
}
\end{lstlisting}
\end{minipage}



\subsection{Reciprocal cube root}

$x^{-\frac{1}{3}}$ is implemented here using a degree-1 general polynomial, corresponding to the classic FRSR case:

\begin{minipage}{\linewidth}
\begin{lstlisting}[caption={$x^{-\frac{1}{3}}$ with degree-1 polynomial, $\epsilon$ = $8.014543 \times 10^{-4}$}, label={lst:FRCR_Deg1}, language=C]
float FRCR_Deg1(float x)
{
  uint32_t X = *(uint32_t *)&x;
  uint32_t Y = 0x54638AFE - X/3;
  float y = *(float *)&Y;
  return y * (1.8696972f - (x*y)*(y*y)*1.2857759f);
}
\end{lstlisting}
\end{minipage}

Our degree-2 version improves on the one found by \citet{moroz2021} by using Algorithm \ref{alg:FRGR-constants} to derive an optimal value for the magic constant\footnote{It appears that \citet{moroz2021} chose a criterion which did not minimise $\rho$. Instead, by inverting equation \eqref{eq:C}, it is found that their magic constant corresponds to setting $t^*=t_1$; whereas, observing line \ref{alg:FRGR-constants:line:t_star} of Algorithm \ref{alg:FRGR-constants} we see that, for optimality, this value must be clamped to the interval [$\frac{1}{3}$,$\frac{2}{3}$]. Doing so yields $t^*=\frac{1}{3}$ and, together with $s=0$, the superior magic constant shown in listing \ref{lst:FRCR_Deg2}.}:

\begin{minipage}{\linewidth}
\begin{lstlisting}[caption={$x^{-\frac{1}{3}}$ with degree-2 polynomial, $\epsilon$ = $2.662789 \times 10^{-5}$}, label={lst:FRCR_Deg2}, language=C]
float FRCR_Deg2(float x)
{
  uint32_t X = *(uint32_t *)&x;
  uint32_t Y = 0x54B8E38E - X/3;
  float y = *(float *)&Y;
  float z = x*y*y*y;
  return y * (1.3739948f-z*(0.47285829f-z*0.092823250f));
}
\end{lstlisting}
\end{minipage}

The FRGR algorithm can also be used to implement $x^{-\frac{2}{3}}$. Only the version using a degree-1 general polynomial is presented:

\begin{minipage}{\linewidth}
\begin{lstlisting}[caption={$x^{-\frac{2}{3}}$ with degree-1 polynomial, $\epsilon$ = $1.190003 \times 10^{-3}$}, label={lst:FRCR2_Deg1}, language=C]
float FRCR2_Deg1(float x)
{
  uint32_t X = *(uint32_t *)&x;
  uint32_t Y = 0x69BC56FC - 2*X/3;
  float y = *(float *)&Y;
  float w = 0.8152238f * y;
  float v = x * w;
  return w * (1.7563311f - v*v*w);
}
\end{lstlisting}
\end{minipage}

A more accurate way to approximate $x^{-\frac{2}{3}}$ (though on a narrower domain) is to pass $x*x$ to the function \texttt{FRCR\_Deg1()}, at the cost of an additional multiply. (Squaring the \textit{result} of \texttt{FRCR\_Deg1()} is \textit{less} accurate.) Either method can then be used to calculate a cube root using $x * x^{-\frac{2}{3}}$. If using \texttt{FRCR2\_Deg1()} for this purpose, the final multiplication by $x$ can be eliminated by reworking the code.


\subsection{Multiple iterations for reciprocal square root}

Our 2-iteration version of FRSR is established by tuning the coefficients and instruction sequence for the first iteration, then finding the actual range $[z_{min}, z_{max}]$ of $z$ values produced by the resulting code, and then using this range to tune the second iteration in a separate pass. It would almost certainly be possible to obtain a slightly superior result by tuning all 5 numbers and both instruction sequences in concert.

\begin{minipage}{\linewidth}
\begin{lstlisting}[caption={$x^{-\frac{1}{2}}$ with 2nd iteration, $\epsilon$ =  $4.612440 \times 10^{-7}$}, label={lst:FRSR_Iter}, language=C]
float FRSR_Iter(float x)
{
  uint32_t X = *(uint32_t *)&x;
  uint32_t Y = 0x5F5FFF00 - (X >> 1);
  float y = *(float *)&Y;
  y *= 1.1893165f - x*y*y*0.24889956f;
  y *= 1.4999996f - (0.49999934f*y)*(x*y);
  return y;
}
\end{lstlisting}
\end{minipage}

We now apply the transformation of coefficients described in Section \ref{sec:acc-multi-iter} to make the second polynomial monic, eliminating one multiply instruction. The resulting code has execution cost equal to that of the original 2-iteration version of the Quake code, but more than 10 times better accuracy:

\begin{minipage}{\linewidth}
\begin{lstlisting}[caption={$x^{-\frac{1}{2}}$ with monic 2nd iteration, $\epsilon$ =  $4.639856 \times 10^{-7}$}, label={lst:FRSR_IterFast}, language=C]
float FRSR_IterFast(float x)
{
  uint32_t X = *(uint32_t *)&x;
  uint32_t Y = 0x5F5FFF00 - (X >> 1);
  float y = *(float *)&Y;
  y *= 0.9439607f - x*y*y*0.19755164f;
  y *= 1.8898820f - x*y*y;
  return y;
}
\end{lstlisting}
\end{minipage}
