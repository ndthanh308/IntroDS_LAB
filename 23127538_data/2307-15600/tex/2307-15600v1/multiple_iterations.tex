\section{Multiple iterations}

The original Quake code \citep{id1998} contained an optional second Newtonian iteration to further refine the result, at the cost of some additional floating point operations. \citet{walczyk2021} used analysis to dramatically improve the accuracy of the 2-iteration version while incurring only a single extra multiply instruction over Quake's 2-iteration version. Here we show how to optimise the coefficients used in each iteration step in the \textit{general} case, further extending the FRGR algorithm. We also show how to remove a multiply instruction from each surplus iteration while retaining the improved accuracy. In the FRSR case, this allows us to keep both the improved accuracy \textit{and} the original execution cost.


\subsection{Generalised multiple iterations} \label{sec:generalised-multiple-iterations}

For this section we will use a slight modification to our notation. We will rewrite the coarse approximation $y$ as $y_0$, and the refined approximation $\widetilde{y}$ as $y_1$; we will then use $y_2$, $y_3$, ... to denote further refined approximations, each having been computed by applying an iteration of the refinement process to the previous approximation. Similarly, we will write $z_0$ in place of $z$ and $p_0$ in place of $p$, and use $z_1$, $p_1$, $z_2$, $p_2$ etc. for their counterparts in subsequent iterations.

By a simple extension of the single-iteration case, a sequence of $m$ refinement steps takes on the following structure:
\begin{align} \label{eq:iterations}
  & z_0 = x^a y_0^b \,, \nonumber \\
  & y_1 = y_0 p_0 (z_0) \,, \nonumber \\
  & z_1 = x^a y_1^b \,, \nonumber \\
  & y_2 = y_1 p_1 (z_1) \,, \nonumber \\
  & ... \nonumber \\
  & z_{m-1} = x^a y_{m-1}^b \,, \nonumber \\
  & y_m = y_{m-1} p_{m-1} (z_{m-1}) \,.
\end{align}
Note that we are free to choose a different degree for each of the $p_i$ if we so wish.

We use a greedy algorithm (a choice which we justify shortly), so that each iteration reduces its own peak relative error to be as small as possible, given the required polynomial degrees. To achieve this, the constant $c$ and refinement polynomial $p_0(z_0)$ are chosen according to Algorithm \ref{alg:FRGR-constants}, and for $i>0$ we choose for $p_i(z_i)$ the minimax polynomial approximation to $z_i^{-\frac{1}{b}}$ on the interval $z_i \in [z_{i;\text{min}}, z_{i;\text{max}}]$. We determine each successive interval as follows.

Given the $i^\text{th}$ approximation $y_i$, we use \eqref{eq:iterations} to calculate $z_i = x^a y_i^b$, and supply this value as the argument to the minimax polynomial $p_i(z_i)$ whose coefficients and domain we assume are already known. Suppose the magnitude of the resulting minimax error is $\epsilon_i$; then for $z_i \in [z_{i;\text{min}},z_{i;\text{max}}]$, we have
\[  -\epsilon_i \leqslant 1 - z_i^{\frac{1}{b}} p_i(z_i) \leqslant \epsilon_i \,,  \]
which we can rearrange as
\[  1-\epsilon_i \leqslant z_i^{\frac{1}{b}} p_i(z_i) \leqslant 1+\epsilon_i \,.  \]
It is easy to show (see Appendix \ref{appendix:eps-bound}) that $1-\epsilon_i > 0$, and hence
\[  (1-\epsilon_i)^b \leqslant z_i p_i(z_i)^b \leqslant (1+\epsilon_i)^b\,.  \]
Now using \eqref{eq:iterations}, we observe that $z_i p_i(z_i)^b = z_{i+1}$, so that this becomes simply
\begin{equation} \label{eq:iter-z-range}
  z_{i+1} \in [(1-\epsilon_i)^b, (1+\epsilon_i)^b]\,,
\end{equation}
which supplies the domain on which we should optimise the polynomial $p_{i+1}(z_{i+1})$.

We justify the use of a greedy algorithm by noting that applying equation \eqref{eq:iter-z-range} to the $(i+1)^\text{st}$ version of the ratio defined in \eqref{eq:rho} yields
\[  \rho_{i+1} = \frac{z_{i+1;\text{max}}}{z_{i+1;\text{min}}} = \left( \frac{1+\epsilon_i}{1-\epsilon_i} \right)^b \,,  \]
and this ratio is clearly minimised by making $\epsilon_i$ as small as possible, which is in turn ensured by choosing for $p_i$ the relevant minimax polynomial. Induction on the number of iterations then completes the argument.

It is worth noting here that if we extend the FRSR algorithm with a second iteration, using a linear polynomial in each iteration as is done in the 2-iteration version of the Quake code, by combining result \eqref{eq:iter-z-range} above with equations \eqref{eq:linear-minimax-solution} from earlier, we find that if $\epsilon_0$ is the analytical minimax error from the first iteration, then that of the second iteration is
\[  \epsilon_1 = \frac{ \left( 1+\frac{\epsilon_0^2}{3} \right)^{\frac{3}{2}} - 1 + \epsilon_0^2 } { \left( 1+\frac{\epsilon_0^2}{3} \right)^{\frac{3}{2}} + 1 - \epsilon_0^2 }\,.  \]
Setting $\epsilon_0$ to the value we obtained in \eqref{eq:c0-c1-eps-FRSR}, we find that the best we can expect from such a 2-iteration FRSR is an error of approximately $3.16943580 \times 10^{-7}$. This value agrees with the one found by \citet{walczyk2021}.

\subsection{Accelerating multiple iterations} \label{sec:acc-multi-iter}

Although Section \ref{sec:generalised-multiple-iterations} specified a recipe for explicitly computing all the coefficients of each polynomial in the sequence of refinements \eqref{eq:iterations}, we actually have some freedom to modify them without changing the final value $y_m$.

To see this, we note that the concatenation of all the steps in \eqref{eq:iterations} yields
\[  y_m = y_0 \prod_{i=0}^{m-1} p_i (z_i)\,,  \]
which contains the product of $m$ polynomials. If we scale each of the $p_i$ by a scaling factor, possibly different for each $i$, then provided the product of the scaling factors is constrained to equal $1$, $y_m$ will be unaltered. Since this scaling scheme has $m$ parameters and one constraint, there are $m-1$ degrees of freedom, which we can use to accelerate the resulting implementation, as we shall see.

However, the modification required to scale a given polynomial is not in general simply a matter of scaling all its coefficients by the same factor, because the value of the polynomial in one such modified iteration affects the argument of the polynomial in the subsequent iteration.

Consider the effect of scaling the coefficients of a single $p_i$ by a non-zero factor $k_i$ to yield a scaled polynomial $p_i'$:
\begin{equation} \label{eq:scale-p_i}
  p_i'(z_i) = k_i p_i(z_i) \,.
\end{equation}
In turn, this scales $y_{i+1}$ to a new value,
\[  y_{i+1}' = y_i k_i p_i(z_i) = k_i y_{i+1}\,,  \]
and hence $z_{i+1}$ to a new value,
\[  z_{i+1}' = x^a y_{i+1}'^b = k_i^b z_{i+1}\,. \]
If we wish the value computed for the polynomial in the next iteration, $p_{i+1}$, to remain unaffected, we must modify its coefficients to counteract the change in its argument, yielding a new polynomial $p_{i+1}'$ where
\begin{equation} \label{eq:scale-p_i+1}
  p_{i+1}' (k_i^b z_{i+1}) = p_{i+1} (z_{i+1})\,.
\end{equation}
Hence the coefficient of the degree-$r$ term in $p_{i+1}'$ should be set equal to the corresponding coefficient in $p_{i+1}$ divided by $k_i^{rb}$.

Together, the transformations \eqref{eq:scale-p_i} and \eqref{eq:scale-p_i+1} give the procedure for scaling the value of a single one of the polynomials by a given constant. We then repeat this procedure for each polynomial we wish to scale. (Note that when scaling the final polynomial $p_{m-1}$ there is no need for the compensating step \eqref{eq:scale-p_i+1}, since no subsequent polynomials will be affected.)

We can apply this scheme in a couple of alternative ways to accelerate the code:
\begin{itemize}
  \item Scale one of the $p_i$ by the reciprocal of its leading coefficient. This transforms it to a monic polynomial which, as observed in Section \ref{sec:monics}, eliminates one multiply instruction from the code (in the absence of FMA instructions). We can repeat this process for all but one of the polynomials, whose scaling factor is now fully constrained to be the reciprocal of the product of all the other scale factors.
  \item Scale each of the $p_i$ so that its leading coefficient has magnitude $u^{n_i}$, for a suitable constant $u$, with $n_i$ the degree of $p_i$. Now, where the unmodified algorithm would compute the value $x^a$ to be reused in each iteration, we can instead compute the product $u x^a$ at a cost of one extra multiply instruction and then reuse this value in every iteration to save one multiply per iteration.
\end{itemize}
In both cases, the saving in execution cost will be $m-1$ multiply instructions, but the computed result will be unaffected except by rounding differences.
