\subsection{Refined Approximation} \label{sec:refined-approximation}


We begin by formalising the definition of the pseudolog function $L(x)$. Given $x \in \mathbb{R^+}$, let
\begin{equation} \label{eq:Em(x)}
  E_x = \floor{\log_2 {x}}\,, \quad m_x = 2^{-E_x} x\ - 1,
\end{equation}
with $\floor{\ }$ denoting the floor function, so that $E_x \in \mathbb{Z}$ and $0\leqslant m_x < 1$. We will refer to $E_x$ and $m_x$ as the \textit{exponent} and \textit{mantissa}, respectively, of $x$, consistent with their counterparts in the terminology of floating point numbers. We may now define $L(x)$ using
\begin{equation} \label{eq:L(x)}
  L(x)=E_x+m_x\,.
\end{equation}
The inverse function $L^{-1}(X)$ can be computed as follows. Given $X \in \mathbb{R}$, let
\begin{equation} \label{eq:Em(X)}
  E_{x} = \floor{X}\,, \quad m_{x} = X - E_{x}\,.
\end{equation}
Then
\begin{equation} \label{eq:L_inv(X)}
  L^{-1}(X) = 2^{E_{x}} (1+m_{x})\,.
\end{equation}
To develop a refined approximation, first let $g(x)$ be the factor by which we would need to scale the coarse approximation $y$ to exactly match the target function $f$:
\[  g(x) = \frac{f(x)}{y(x)} = x^{-\frac{a}{b}}y^{-\frac{b}{b}} = z^{-\frac{1}{b}}\,,  \]
where the auxiliary function $z(x)$ is defined by
\begin{equation} \label{eq:z}
  z(x) = x^a (y(x))^b\,.
\end{equation}
Intuitively, $z$ measures the goodness of fit of $y$ to the target function $f$, with a value of $1$ indicating a perfect match, leading to the notion that the deviation of $z$ from $1$ can be used in computing a correction factor.

If we had an efficient way to compute $g$ exactly, we could generate the precise value of the target function $f$. This is of course an unreasonable expectation, and in practice the best we can do is to approximate $g(x)$ via some method. A polynomial in $x$ might seem an obvious choice, but we run into a problem: the domain is infinite. However, it can easily be shown that $z$ is bounded. Indeed, by decomposing $x$ and $y$ into their respective exponents and mantissas using transformations \eqref{eq:Em(x)} - \eqref{eq:L_inv(X)}, and substituting into equation \eqref{eq:z}, we have
\begin{equation} \label{eq:z(X,Y)}
  z = 2^{a E_x + b E_y} (1+m_x)^a (1+m_y)^b \,,
\end{equation}
and since $X-1<E_x\leqslant X$ and $1\leqslant1+m_x<2$, and similarly for $E_y$ and $m_y$, we can apply the linear relationship \eqref{eq:line-XY} to show that
\[  2^{c-a-b} = 2^{a(X-1)+b(Y-1)} 1^a 1^b < z < 2^{aX+bY} 2^a 2^b = 2^{a+b+c}\,,  \]
giving lower and upper bounds for $z$. In the following sections we develop sharp bounds, but for now it suffices to observe that the bounded nature of $z$ will allow us to formulate an approximation for $g$ in terms of $z$. We will use an $n^\text{th}$-degree polynomial $p(z)$ to approximate $g$; our refined approximation will then be
\begin{equation} \label{eq:y-tilde}
  \widetilde{y}(x) = y(x) p(z)\,.
\end{equation}
Notice that this form matches that of the standard FRSR algorithm, where $p$ has degree $1$.

This leads us to the generalised form of the FRSR algorithm, shown in Algorithm \ref{alg:FRGR}, and referred to here as the Fast Reciprocal General Root (FRGR) algorithm.

\begin{algorithm}
\caption{FRGR algorithm}
\label{alg:FRGR}
\begin{algorithmic}[1]
\STATE \textbf{function} FRGR($x,a,b,n$)
\STATE $X = L(x)$                                                                       \label{alg:FRGR:line:X}
\STATE $Y = \frac{c}{b}-\frac{a}{b}X$                                      \label{alg:FRGR:line:Y}
\STATE $y = L^{-1}(Y)$                                                              \label{alg:FRGR:line:y}
\STATE $z = x^ay^b$                                                                 \label{alg:FRGR:line:z}
\STATE \textbf{return} $ yp(z) \quad // \enskip p $ has degree $n$        \label{alg:FRGR:line:return}
\end{algorithmic}
\end{algorithm}

Consider now the relative error $\widetilde{e}$ incurred by the refined approximation $\widetilde{y}$. It is\footnote{For relative error, we use the definition given in many texts on numerical analysis. Many works on the FRSR algorithm use the negative of this definition; it does not affect the analysis in any significant way.}
\begin{equation} \label{eq:e-tilde}
  \widetilde{e}(x) = \frac{f(x)-\widetilde{y}(x)}{f(x)} = 1 - x^{\frac{a}{b}} y p(z) = \frac{z^{-\frac{1}{b}} - p(z)}{z^{-\frac{1}{b}}}\,.
\end{equation}
Note that the rightmost expression here is the relative error incurred when approximating the function $z^{-\frac{1}{b}}$ by the polynomial $p(z)$ on a particular \textit{finite} domain, say $[z_\text{min},z_\text{max}]$. Finding the optimal $p(z)$ is thus a problem which yields to standard minimax theory (see, for example, \citet{fike1968}). The unique solution is given by setting $p(z)$ equal to the appropriate minimax polynomial\footnote{Throughout this paper, unless otherwise stated, we use the term ``minimax polynomial" to mean the polynomial of prescribed maximum degree which minimises the maximum magnitude of the \textit{relative} error, rather than of the absolute error used in some other contexts. }, which can be found using standard numerical methods, such as the Remez Exchange Algorithm \citep{remez1934}.

However, determination of the minimax coefficients requires that  we know the range of values $[z_\text{min}, z_\text{max}]$ over which $z(x)$ can vary for $x \in (0,\infty)$. This range is not yet fixed, because it is dependent on the value $c$. We now turn our attention to investigating the nature of this dependency, i.e. to the problem of finding expressions for $z_\text{min}$ and $z_\text{max}$ as functions of $c$.
