
\section{Introduction}

There is a well-known algorithm for calculating a fast approximation to the reciprocal square root function using single-precision floating point arithmetic, the key to which is to treat the bits of a floating point number as if they represent an integer, and to manipulate them using integer instructions. (For an overview of this class of techniques, see \citet{blinn1997}). When the result is reinterpreted as a floating point number, it holds a coarse approximation to the reciprocal square root which can then be refined using Newtonian iteration. The algorithm was valuable in the 1990s, because it enabled rapid approximate normalisation of vectors for use in lighting calculations in graphics engines for video games.

Subsequent work (notably by \citet{lomont2003}, \citet{pizer2008}, \citet{kadlec2010}, \citet{moroz2016}, and \citet{walczyk2021}) analysed the code and found that by changing its numerical constants a roughly threefold reduction in error could be achieved, without changing the execution cost of the algorithm. However, no overarching mathematical framework was developed which would allow the same technique to be applied to the approximation of arbitrary powers of $x$, or which would automatically generate optimal coefficients for polynomials of arbitrary degree in the refinement steps.

Present-day microprocessors lessen the usefulness of the original technique since many hardware platforms provide one or more machine-level instructions to accelerate the calculation of reciprocal square roots. However, since the library function \texttt{pow()} is usually a much costlier operation, it may yet be beneficial to develop approximations which extend the technique to work for other fixed powers of $x$ (for example, those which arise in gamma correction).
