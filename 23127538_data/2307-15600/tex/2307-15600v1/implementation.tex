\section{Implementation details} \label{sec:implementation}

\subsection{Calculation of the ``magic constant"}

The FRSR and FRGR algorithms as presented contain a constant $c$, whose value is computed by Algorithm \ref{alg:FRGR-constants}. Recall that in section \ref{sec:standard_alg}, we alluded to a correspondence between $c$ and the so-called ``magic constant" of Listing \ref{lst:Quake} and the family of improved versions it has led to. Naming this latter constant $C$, we now develop a formula for computing $C$ from $c$. We shall assume a mantissa with $k_\text{mant}$ bits and an exponent bias of $k_\text{bias}$.

Corresponding to lines \ref{alg:FRGR:line:X} and \ref{alg:FRGR:line:y} of Algorithm \ref{alg:FRGR}, let $I(x)$ denote the value obtained when interpreting the bit pattern of the floating point number $x$ as an integer of the same word length, and let $F(X)$ denote the value obtained when the integer $X$ is interpreted as a floating point number.

We can use our pseudolog function $L(x)$ of equation \eqref{eq:L(x)} to write
\begin{align*}
  I(x) &= 2^{k_\text{mant}} (L(x) + k_\text{bias}) \,, \\
  F(X) &= L^{-1} (2^{-k_\text{mant}} X - k_\text{bias})\,.
\end{align*}
We can then transform lines \ref{alg:FRGR:line:X} - \ref{alg:FRGR:line:y} of  Algorithm \ref{alg:FRGR} into new versions which utilise the functions $I$ and $F$ and the constant $C$. Since the intermediate variables will now take on different values, we replace $X$ with $X'$, etc. The transformation can be written as
\begin{align*}
  X &= L(x)                                     & \xleftrightarrow{} && X' &= I(x) \,, \\
  Y &= \frac{c}{b} - \frac{a}{b} X & \xleftrightarrow{} && Y' &= C - \frac{a}{b} X' \,, \\
  y &= L^{-1} (Y)                           & \xleftrightarrow{} && y' &= F(Y') \,.
\end{align*}
Since we wish the value of the coarse approximation to remain unchanged by this transformation, we impose $y'=y$ and solve the resulting system for $C$ to obtain
\begin{equation} \label{eq:C}
  C = \frac{2^{k_\text{mant}}}{b} (c + k_\text{bias} (a+b))\,.
\end{equation}
If we now assume the IEEE 754 single-precision format, using the value $c=-\frac{1}{2}$ for FRSR obtained in Section \ref{sec:app-FRSR}, and applying equation \eqref{eq:C}, we find for $C$ a value $2^{21}\times761$, which can be written as the hexadecimal constant \texttt{0x5F200000}.

In practice, we cannot attain the theoretical accuracy quoted in that section, because evaluation error due to floating point rounding affects the result. However, there are some measures we can take to reduce the evaluation error, which are presented next.

\subsection{Implementation alternatives}

In this section we are interested in ways to reorder the operations without changing the execution cost. We will only consider the FRSR case, but the same technique can be applied to other cases.

Assuming that intermediate results are rounded to the nearest floating point value, there are often multiple ways to express a sequence of floating point operations all of which are algebraically equivalent but which may yield slightly different results depending on the order of operations. The IEEE 754 standard ensures that the result of the expression $f_0 * f_1$ will equal that of $f_1 * f_0$. However, the result of $(f_0 * f_1) * f_2$ may differ from that of $f_0 * (f_1 * f_2)$. In the FRSR case, this lack of associativity implies that we can change the order in which we compute the product $c_1 x y^2$ occurring in the evaluation of $p(z)$. We find that there are nine alternatives, which are easily enumerated:
\begin{align*}
  & c_1*x*y*y && c_1*y*x*y && c_1*y*y*x \\
  & x*y*c_1*y && x*y*y*c_1 && y*y*c_1*x \\ 
  & y*y*x*c_1 && (c_1*x)*(y*y) && (c_1*y)*(x*y)
\end{align*}
Another set of possible implementations can be derived by observing the way \citet{kadlec2010} factored the expression for the refinement step. By suitably transforming the coefficients $\{c_0,c_1\} \rightarrow \{c_0',c_1'\}$ and defining a temporary variable $w = c_1' * y$, we can use one of the following expressions for the refined approximation (Kadlec's version corresponding to the first expression):
\begin{align*}
  & w * (c_0' - x * y * y) && w * (c_0' - y * y * x) \\
  & w * (c_0' - x * y * w) && w * (c_0' - x * w * y) && w * (c_0' - y * w * x) \\
  & w * (c_0' - x * w * w) && w * (c_0' - w * w * x)
\end{align*}
We now have a total of 16 ways to implement the refinement computation, and some will yield better accuracy than others.\footnote{Note that some ways of ordering the products have implications for the domain on which the function retains its full accuracy. This will be addressed in the results section.}

Furthermore, in listing \ref{lst:Quake}, the right-shift operation is performed before the subtraction, but the order could be swapped to become
\[  Y = (C' - X) >> 1;  \]
where the modified magic constant $C'$ is whichever of $2 C$ or $2 C + 1$ yields the best result, effectively giving the constant one additional bit of precision.\footnote{The right-shift in this case should be performed as an unsigned operation since the topmost bit of $C'-X$ will often be $1$.}


\subsection{Alternative values for $c$}

We observed that Algorithm \ref{alg:FRGR-constants} gives us a free choice for the integer $s$ without affecting the value computed. In a practical application, though, this choice can affect the evaluation error.

Referring to Algorithm \ref{alg:FRGR}, if we increase $c$ by an amount $b$, $Y$ will increase by $1$, doubling $y$. This doubling is counteracted by scaling the coefficients of $p$ by appropriate powers of $2$. Since multiplication by a power of $2$ can be represented exactly, the final result is unaffected. However, adding any of $1, ..., b-1$ to $c$ modifies the mantissa of $y$ and there is in general no exact floating point representation for the correspondingly modified coefficients of $p$. Thus we have a set of $b$ alternative implementations (orthogonally with those of the previous section). In the FRSR case, we have two alternatives - $s$ even, and $s$ odd. As the results section will show, the best choice for the parity of $s$ in the classic FRSR is not the one which previous authors have traditionally chosen (although \citet{kadlec2010} alluded to the existence of such a choice).

\subsection{Tuning the values of $c, c_0, c_1$}

In a general setting, an analytic approach to minimising evaluation error by modifying the coefficients is a complex topic. The interested reader may refer to \citet{arzelier2019}, for example. One might consider a brute-force search over all possible combinations of constants, but even in the FRSR case the reader will quickly see that the search space is intractably large. Fortunately, it is possible to dramatically reduce the search space using considerations motivated by the methods of this paper (although we omit the details here). This approach can then be combined with the other alternatives explained in this section, and in doing so we find that the implementation yielding the best results is the function \texttt{FRSR\_Deg1()} shown in Section \ref{sec:results}, with a peak relative error of $6.501791 \times 10^{-4}$ (although see the note there regarding an alternative version).
