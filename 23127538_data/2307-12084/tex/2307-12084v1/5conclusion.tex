\section{Conclusion}

We propose a novel framework for semantic image synthesis.
It introduces four core components: edge guided image generation strategy, attention guided edge transfer module, semantic preserving module, and multi-scale contrastive learning module.
The first one is employed to generate edge maps from input semantic labels. 
The second one is used to selectively transfer the useful structure information from the edge branch to the image branch. 
The third one is adopted to alleviate the problem of spatial resolution losses caused by different operations in the deep nets.
The last one is utilized to investigate global-local semantic relations between training pixels from different scales, guiding pixel embeddings toward cross-image category-discriminative representations. 
Extensive experiments on three public datasets show that the proposed methods achieve significantly better results than existing models.

\hao{Although our method achieves good results on different datasets, our method also has a limitation, that is, it needs to be retrained on different datasets. In future work, we will design a new framework that can achieve good results on different datasets with only one training, which saves training time and training resources and is also more convenient to deploy to reality in the application.}

\noindent \textbf{Acknowledgments.}
This work was partly supported by the EU H2020 project AI4Media under Grant 951911.
