\section{Experiments}

% Figure environment removed


\subsection{Experimental Setups} 
\noindent\textbf{Datasets.}
We follow GauGAN \cite{park2019semantic} and conduct experiments on three datasets, i.e., Cityscapes \cite{cordts2016cityscapes}, ADE20K \cite{zhou2017scene}, and COCO-Stuff \cite{caesar2018coco}.
For more detail about these datasets, please refer to GauGAN \cite{park2019semantic}.

\noindent\textbf{Evaluation Metrics.}
We employ the mean Intersection-over-Union (mIoU), Pixel Accuracy (Acc), and Fr\'echet Inception Distance (FID)~\cite{heusel2017gans} as the evaluation metrics.
For more detail about these evaluation metrics, please refer to GauGAN \cite{park2019semantic}.

\subsection{Experimental Results} 

\noindent \textbf{Qualitative Comparisons.}
We adopt GauGAN as the encoder $E$ to validate the effectiveness of the proposed method.
Visual comparison results on all three datasets with the state-of-the-art method (i.e., OASIS \cite{sushko2020you}) are shown in Figure~\ref{fig:sota}. 
\hao{
We can see that ECGAN and ECGAN++ achieve visually better results with fewer visual artifacts than the existing state-of-the-art method.
Examining Figure \ref{fig:sota}, it is evident that the SOTA method produces numerous visual artifacts across varied categories like vegetation, cars, buses, roads, buildings, fences, beds, cabinets, curtains, elephants, etc. In contrast, our approach generates significantly more realistic content, as can be observed on both sides of the figure. Moreover, the proposed methods generate more local structures and details than the SOTA method.}

\noindent\textbf{User Study.} 
We follow the same evaluation protocol as GauGAN and conduct a user study.
Specifically, we provide the participants with an input layout and two generated images from different models and ask them to choose the generated image that looks more like a corresponding image of the layout. 
The users are given unlimited time to make the decision. 
For each comparison, we randomly generate 400 questions for each dataset, and each question is answered by 10 different participants. For other methods, we use the public code and pretrained models provided by the authors to generate images.
As shown in Table \ref{tab:atm1}, users favor our synthesized results on all three datasets compared with other competing methods, further validating that the generated images by ECGAN are more natural.
Moreover, we can see in Table \ref{tab:atm2} that users favor our synthesized results by the proposed ECGAN++ compared with the proposed ECGAN, validating the effectiveness of the proposed multi-scale
contrastive learning method.

\noindent \textbf{Quantitative Comparisons.}
Although the user study is more suitable for evaluating the quality of the generated images, we also follow previous works and use mIoU, Acc, and FID for quantitative evaluation.
The results of the three datasets are shown in Table~\ref{tab:sota}. 
The proposed ECGAN and ECGAN++ outperform other leading methods by a large margin on all three datasets, validating the effectiveness of the proposed methods.

\noindent\hao{\textbf{Memory Usage.} The proposed method is memory-efficient compared to those methods which model the generation of different image classes individuals such as LGGAN \cite{tang2020local}. Thus, we compare the memory usage during training/testing when the batch size is set to 1. The memory (GB) of LGGAN on CityScapes (30 categories),  ADE20K (150 categories), and COCO-Stuff (182 categories) datasets are about 17.8, 23.9, and 28.1, respectively. The memory (GB) of our proposed method on the Cityscapes, ADE20K, and COCO-Stuff datasets is about 6.3, 5.6, and 5.9 respectively. It is clear that LGGAN's memory requirement significantly escalates as category numbers increase, whereas our method maintains comparable memory demands. This advantage becomes even more prominent when using larger batch sizes, implying we can train/test the model with larger batches on the same GPU devices.}


\noindent \textbf{Visualization of Edge and Attention Maps.}
We also visualize the generated edge and attention maps in Figure~\ref{fig:ade_gaugan}.
We observe that the proposed method can generate reasonable edge maps according to the input labels. Thus the generated edge maps can provide more local structure information for generating more photo-realistic images.

\noindent \textbf{Visualization of Segmentation Maps.}
We follow GauGAN and apply pre-trained segmentation  networks \cite{yu2017dilated,xiao2018unified} on the generated images to produce segmentation maps.
Results compared with the baseline method are shown in Figure~\ref{fig:city_seg}.
We observe that the proposed method consistently generates better semantic labels than the baseline on both datasets.

\begin{table}[!t] \small
	\centering
 	\caption{Multi-modal synthesis evaluation on ADE20K.}
	\begin{tabular}{rccc} \toprule
		{Method} & {Multi-Modal} &  {LPIPS $\uparrow$}  \\ \midrule
		{GauGAN+ \cite{park2019semantic}} & {Encoder} & {0.16} \\
		{GauGAN+ \cite{park2019semantic}} & {3D Noise} & {0.50} \\
		{OASIS \cite{sushko2020you}} & 
		{3D Noise} & {0.35} \\
		ECGAN (Ours) & Encoder & 0.18 \\ 
		ECGAN (Ours) & 3D Noise & 0.52 \\
        ECGAN++ (Ours) & Encoder & 0.22\\ 
        ECGAN++ (Ours) & 3D Noise & \textbf{0.54}\\ \bottomrule
	\end{tabular}
 \vspace{-0.4cm}
	\label{tab:multimodal}
\end{table}

% Figure environment removed


% Figure environment removed


\noindent \textbf{Multi-Modal Image and Edge Synthesis.}
We follow GauGAN \cite{park2019semantic} and apply a style encoder and a KL-Divergence loss with a loss weight of 0.05 to enable multi-modal image and edge synthesis.
As shown in Figure~\ref{fig:diff}, our model generates different edges and images from the same input layout, which we believe will benefit other tasks, e.g., image restoration \cite{shi2022rcrn}, and image/video super-resolution \cite{wu2022compiler,cao2022towards}.
Moreover, we follow OASIS \cite{sushko2020you} and use LPIPS \cite{zhang2018unreasonable} to evaluate the variation in the multi-model image synthesis on the ADE20K dataset.
Following in OASIS, we generate 20 images and compute the mean pairwise scores, and then average over all label maps. 
The higher the LPIPS scores, the more diverse the generated images are. 
We follow OASIS and GauGAN, and employ two settings (i.e., encoder and 3D noise) to evaluate multi-modal image synthesis.
Table \ref{tab:multimodal} shows that the proposed ECGAN and ECGAN++ achieve better results than OASIS and GauGAN in both settings.
Note that existing methods (e.g., OASIS \cite{sushko2020you} and GauGAN \cite{park2019semantic}) can only achieve multi-modal image synthesis.

% Figure environment removed



\begin{table*}[!t]\small
	\centering
 	\caption{mIoU of small objects on Cityscapes.}
	% \resizebox{1\linewidth}{!}{% 
	\begin{tabular}{lccccccc}	\toprule
		{mIoU $\uparrow$}                               & {Pole} & {Light}  & {Sign} & {Rider}  & {Mbike} & {Bike}  & {Overall} \\ \midrule
		{OASIS} \cite{sushko2020you}  & {23.4} & {32.6} & {14.9} & {27.3} & {31.2} & {26.6} & {26.0} \\
		ECGAN (Ours) & 26.2 & 36.7 & 17.4 & 30.2 & 33.5 & 28.7 & 28.8 \\ 
        ECGAN++ (Ours) & \textbf{26.7} & \textbf{37.0} & \textbf{17.9} & \textbf{30.8} & \textbf{34.2} & \textbf{29.5} & \textbf{29.4} \\\bottomrule
	\end{tabular}
 \vspace{-0.2cm}
	\label{tab:small}
\end{table*}

\begin{table*}[!t] \small
	\centering
 	\caption{Ablation study of the proposed method on Cityscapes, ADE20K, and COCO-Stuff.}
	\resizebox{1\linewidth}{!}{% 
		\begin{tabular}{clccccccccc} \toprule
           \multirow{2}{*}{\#} & \multirow{2}{*}{Setting}  & \multicolumn{3}{c}{Cityscapes} & \multicolumn{3}{c}{ADE20K} & \multicolumn{3}{c}{COCO-Stuff} \\ \cmidrule(lr){3-5} \cmidrule(lr){6-8} \cmidrule(lr){9-11} 
		  &	&  mIoU $\uparrow$ & Acc $\uparrow$ & FID $\downarrow$  & {mIoU $\uparrow$} & {Acc $\uparrow$} & {FID $\downarrow$}  &  {mIoU $\uparrow$} & {Acc $\uparrow$} & {FID $\downarrow$} \\ \midrule	
		B1& $E$+$G_i$                    &  58.6      & 81.4      & 65.7 & 36.9 & 78.5 & 38.2 & 36.8 & 65.1 & 24.5 \\
		B2& $E$+$G_i$+$G_e$              &  60.2    & 81.7     & 61.0  & {38.7} & {79.2} & {36.3} & {37.5} & {66.3} & {22.9}\\
		B3& $E$+$G_i$+$G_e$+$G_t$        &  61.5     & 82.0     & 59.0  & {40.6} & {80.3} & {34.6} & {39.1} & {67.0} & {21.7}\\
		B4& $E$+$G_i$+$G_e$+$G_t$+$G_s$  &  64.5       & 82.5     & 57.1 & {42.0} & {82.0} & {32.4} & {41.4} & {68.2} & {19.8} \\  
		B5&	$E$+$G_i$+$G_e$+$G_t$+$G_s$+$G_l$ & 66.8 & 82.7 & 52.2 & {45.8} & {82.4} & {29.9} & {43.7} & {69.1} & {17.6} \\ 
		B6&	$E$+$G_i$+$G_e$+$G_t$+$G_s$+$G_l$+$G_c$ & 72.2 & 83.1 & 44.5 & {50.6} & {83.1} & {25.8} & {46.3} & {70.5} & {15.7} \\ 
  B7& $E$+$G_i$+$G_e$+$G_t$+$G_s$+$G_l$+$G_c$+Eq. \eqref{eq:contrastive_multi} & 72.8 & 83.5 & 43.7 & 51.6 & 84.3 & 25.3 & 47.1 & 71.4 & 15.4 \\
		B8& $E$+$G_i$+$G_e$+$G_t$+$G_s$+$G_l$+$G_c$+Eq. \eqref{eq:contrastive_multi}+Eq. \eqref{eq:contrastive_cross}  & \textbf{73.3} & \textbf{83.9} & \textbf{42.2} & \textbf{52.7} & \textbf{85.9} & \textbf{24.7} & \textbf{47.9} & \textbf{72.3} & \textbf{14.9} \\ \bottomrule
	\end{tabular}}
    \vspace{-0.4cm}
	\label{tab:sota2}
\end{table*}

\noindent \textbf{Evaluation Focused on Small Objects.}
We report mIoU on six small object categories of Cityscapes (i.e., pole, light, sign, rider, mbike, and bike) in Table \ref{tab:small}. Our ECGAN and ECGAN++ generate better mIoU than the state-of-the-art method (i.e., OASIS \cite{sushko2020you}) on all these small object classes.  We also show visualization results in Figure \ref{fig:small}, clearly confirming that the proposed method is highly capable of preserving small objects in the output.

\subsection{Ablation Study} 
\noindent \textbf{Baselines.} We conduct extensive ablation studies on three datasets to evaluate different components of the proposed method.
Our method has 7 baselines (i.e., B1, B2, B3, B4, B5, B6, B7) as shown in Table~\ref{tab:sota2}: 
B1 means only using the encoder $E$ and the proposed image generator $G_i$ to synthesize the targeted images;
B2 means adopting the proposed image generator $G_i$ and edge generator $G_e$ to produce both edge maps and images simultaneously;
B3 connects the image generator $G_i$ and the edge generator $G_e$ by using the proposed attention guided edge transfer module $G_t$;
B4 employs the proposed semantic preserving module $G_s$ to further improve the quality of the final results.
B5 uses the proposed label generator $G_l$ to produce the label from the generated image and then calculate the similarity loss between the generated label and the real one.
B6 uses the proposed pixel-wise contrastive learning and class-specific pixel generation methods to capture more semantic relations by explicitly exploring the structures of labeled pixels from multiple input semantic layouts.
B7 uses the proposed multi-scale contrastive learning method proposed in Eq. \eqref{eq:contrastive_multi} to learn more semantic relations from multi-scale features.
B8 is our full model and uses the proposed cross-scale contrastive learning method proposed in Eq. \eqref{eq:contrastive_cross} to learn more semantic relations from cross-scale features.
As shown in Table \ref{tab:sota2}, each proposed module improves the performance on all three metrics, validating the effectiveness.


\noindent \textbf{Effect of Edge Guided Generation Strategy.}
When using the edge generator $G_e$ to produce the corresponding edge map from the input label, performance on all evaluation metrics is improved.
We also provide several visualization results of the differences (see Eq.~\eqref{eqn:image}) after the edge-guided refinement in Figure~\ref{fig:diff2}.

% Figure environment removed

% Figure environment removed

\noindent \textbf{Effect of Edge Extraction Methods.}
We also conduct experiments on Cityscapes with HED \cite{xie2015holistically}, leading to the following results: 56.7 (FID), 64.5 (mIoU), and 82.3 (Acc), which are slightly worse than the results of Canny in Table \ref{tab:sota2}. 
The reason is that the edges from HED are very thick and cannot accurately represent the edge of objects. It also ignores some local details since it focuses on extracting the contours of objects. 
Thus, HED is unsuitable for our setting as we aim to generate more local details/structures.
Moreover, we see in Figure \ref{fig:edge} that the generated HED edges contain artifacts, as indicated in the red boxes, which makes the generated images tend to have blurred edges. 


\noindent \textbf{Effect of Attention Guided Edge Transfer Module.}
We observe that the implicitly learned edge structure information by the ``$E$+$G_i$+$G_e$'' baseline is not enough for such a challenging task.
Thus we further adopt the transfer module $G_t$ to transfer useful edge structure information from the edge generation branch to the image generation branch.
We observe that performance gains are obtained on the mIoU, Acc, and FID metrics in all three datasets. 
This means that $G_t$ indeed learns rich feature representations with more convincing structure cues and details and then transfers them from the generator $G_e$ to the generator $G_i$.

\noindent \textbf{Effect of Semantic Preserving Module.}
By adding $G_s$, the overall performance is further boosted on all the three datasets.
This means $G_s$ indeed learns and highlights class-specific semantic feature maps, leading to better generation results.
In Figure~\ref{fig:city_seg}, we show some samples of the generated semantic maps. 
We observe that the semantic maps produced by the results with $G_s$ (i.e., ``Label by Ours II'' in Figure \ref{fig:city_seg}) are more accurate than those without using $G_s$ (``Label by Ours I'' in Figure \ref{fig:city_seg}). 
Moreover, we visualize three channels in~$\mathcal{F}^{'}$ on Cityscapes in Figure~\ref{fig:semantic_block}(right), i.e., road, car, and vegetation.
Each channel learns well the class-level deep representations.


\noindent \textbf{Effect of Similarity Loss.}
By adding the proposed label generator $G_l$ and similarity loss, the overall performance is further boosted on all three metrics. This means the proposed similarity loss indeed captures more intra-class and inter-class semantic dependencies, leading to better semantic layouts in the generated images.

% Figure environment removed

\noindent \textbf{Effect of Contrastive Learning.}
When adopting the proposed pixel-wise contrastive learning module $G_c$ and class-specific pixel generation method to produce the results, the results are significantly improved on all three datasets on all three evaluation metrics.
This means that the model does indeed learn a more discriminative class-specific feature representation, confirming the superiority of our design.

\noindent \textbf{ECGAN vs. ECGAN++.}
We provide user study results in Table \ref{tab:atm2}. We can see that users favor the results generated by ECGAN++ on all three datasets compared
with those results generated by ECGAN.
We also provide quantitative comparison results of ECGAN~\cite{tang2023edge} and ECGAN++ in Tables~\ref{tab:sota} and \ref{tab:sota2}.
ECGAN++ achieves better results than ECGAN on all metrics on all the datasets.
Specifically, We see in Table \ref{tab:sota2} that B7 has better results than B6 on all datasets, and B8 has better results than B7 on the evaluation metrics, which verifies the effectiveness of our proposed multi-scale contrastive learning loss (Eq. \eqref{eq:contrastive_multi}) and cross-scale contrastive learning loss (Eq. \eqref{eq:contrastive_cross}).
Moreover, we note that ECGAN++ generates better results than ECGAN on the four datasets (including a face dataset CelebAMask-HQ \cite{CelebAMask-HQ}), as shown in Figure \ref{fig:ecgan}.
Finally, we compare the results of both ECGAN and ECGAN++ on the multi-model synthesis and small object generation evaluations in Table \ref{tab:multimodal} and \ref{tab:small}, respectively. We can see that ECGAN++ achieves much better results than ECGAN on both multi-model synthesis and small object generation evaluations, which verifies the effectiveness of the proposed multi-scale contrastive learning method.



\begin{table}[!t] \small
	\centering
 	\caption{Weight $w_s$ selection for the multi-scale contrastive learning loss in Eq. \eqref{eq:contrastive_multi}.}
% 	\resizebox{1\linewidth}{!}{% 
		\begin{tabular}{cccc} \toprule
 $w_s$ &  mIoU $\uparrow$ & Acc $\uparrow$ & FID $\downarrow$ \\ \midrule
           1.0 1.0 1.0 1.0 & 72.8 & 83.3 & 43.9 \\
           0.1 0.1 0.1 0.1 & 72.7 & 83.4 & 43.7 \\
           1.0 0.7 0.4 0.1 & \textbf{73.3} & \textbf{83.9} & \textbf{42.2} \\ \bottomrule
	\end{tabular}
    \vspace{-0.4cm}
	\label{tab:sele1}
\end{table}

\begin{table}[!t] \small
	\centering
 	\caption{Cross-scale pair selection for the cross-scale contrastive learning loss in Eq. \eqref{eq:contrastive_cross}.}
% 	\resizebox{1\linewidth}{!}{% 
		\begin{tabular}{ccccc} \toprule
Pairs & Setting &  mIoU $\uparrow$ & Acc $\uparrow$ & FID $\downarrow$ \\ \midrule
         0 & - & 72.8 & 83.5 & 43.7\\
         1 & (s4, s8) & 73.0 & 83.6 & 43.2 \\
         2 & (s4, s8), (s4, s16) & \textbf{73.3} & \textbf{83.9} & \textbf{42.2} \\ \bottomrule
	\end{tabular}
    \vspace{-0.4cm}
	\label{tab:sele2}
\end{table}

\noindent \textbf{Hyper-Parameter Selection.}
We also investigate the influence of
$w_s$ in Eq. \eqref{eq:contrastive_multi} on the performance of our model. The results of Cityscapes
are shown in Table \ref{tab:sele1}.
We see that the proposed method achieves the best results when applying a decreasing function (i.e., 1.0, 0.7, 0.4, 0.1) to the weights according to the output stride.
Moreover, we conduct ablation study experiments on the Cityscapes dataset to choose the number of cross-scale pairs in Eq. \eqref{eq:contrastive_cross}.
The results are shown in Table \ref{tab:sele2}, showing that two cross-scale pairs achieve the best results.
Considering the balance of training time and performance, we do not consider increasing the number of cross-scale pairs.

