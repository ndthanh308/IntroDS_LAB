\section{Related Work}

\noindent \textbf{Generative Adversarial Networks (GANs)} \cite{goodfellow2014generative} have two important components, i.e., a generator and a discriminator. Both are trained in an adversarial way to achieve a balance. 
Recently, GANs have shown the capability of generating realistic images \cite{tang2021total,tang2020unified}.
Moreover, to generate user-specific content, Conditional GANs (CGANs) \cite{mirza2014conditional} have been proposed.
CGANs usually combine a vanilla GAN and some external information such as class labels \cite{tang2019attribute,tang2019expression}, human poses \cite{tang2022bipartite,tang2022facial,tang2020xinggan,tang2019cycle,tang2018gesturegan}, text descriptions \cite{xu2022predict,tao2022df,tao2023galip,tao2022net}, graphs \cite{tang2023graph}, and segmentation maps \cite{gu2019mask,wu2022cross,tang2022local,tang2020dual,tang2020local,wu2022cross_tmm,tang2021layout}.

\noindent \textbf{Image-to-Image Translation} aims to generate the target image based on an input image. 
CGANs have achieved decent results in both paired \cite{isola2017image,albahar2019guided} and unpaired \cite{zhu2017unpaired} image translation tasks.
For instance, Isola et al. propose Pix2pix~\cite{isola2017image}, which employs a CGAN to learn a translation mapping from input to output image domains such as map-to-photo and day-to-night. 
Moreover, Zhu et al. \cite{zhu2017unpaired} introduce CycleGAN, which targets unpaired image-to-image translation using the cycle-consistency loss.
To further improve the quality of the generated images, the attention mechanism has been recently investigated in image translation tasks \cite{tang2019multi,tang2019attention,mejjati2018unsupervised,chen2018attention,tang2021attentiongan}. 
Attention mechanism assigns context elements weights which define a weighted sum over context representation \cite{wu2019pay,chen2019graph}, which has been used in many other computer vision tasks such as depth estimation \cite{xu2018structured} and semantic segmentation \cite{fu2019dual}, and have shown great effectiveness. 

Different from previous attention-related image generation works, we propose a novel attention guided edge transfer module to transfer useful edge structure information from the edge generation branch to the image generation branch at two different levels, i.e., feature level and content level.
To the best of our knowledge, our module is the first attempt to incorporate both edge feature attention and edge content attention within a GAN framework for image-to-image translation tasks.

\noindent \textbf{Edge Guided Image Generation.}
Edge maps are usually adopted in image inpainting \cite{ren2019structureflow,nazeri2019edgeconnect,li2019progressive} and image super-resolution \cite{nazeri2019edge} tasks to reconstruct the missing structure information of the inputs.
For example, 
Pix2pix~\cite{isola2017image} adopts edge maps as input and aims to generate realistic shoes and handbags, which can be seen as an edge-to-image translation problem.
\hao{Moreover, \cite{nazeri2019edgeconnect} proposed an edge generator to hallucinate edges in the missing regions given edges, which can be regarded as an edge completion problem. 
Using edge images as the structural guidance, EdgeConnect \cite{nazeri2019edgeconnect} achieves good results even for some highly structured scenes.
To recover meaningful structures, \cite{ren2019structureflow} implemented edge-preserved smooth images, serving as representations of the overarching structures inherent in image scenes. When these images are used as a navigational tool for the structure reconstructor, the network has the capacity to concentrate on the recuperation of these global structures, undeterred by any extraneous texture data.}

Unlike previous works, including \cite{nazeri2019edgeconnect,ren2019structureflow}, we propose a novel edge generator to perform a new task, i.e., semantic label-to-edge translation. To the best of our knowledge, we are the first to generate edge maps from semantic labels. Then the generated edge maps, with more local structure information, can be used to improve the quality of the image results.

\noindent \textbf{Semantic Image Synthesis} aims to generate a photo-realistic image from a semantic label map \cite{chen2017photographic,qi2018semi,park2019semantic,liu2019learning,bansal2019shapes,zhu2020sean,ntavelis2020sesame,zhu2020semantically,sushko2020you,tan2021efficient,tan2021diverse,zhu2020semantically,zhang2023adding,zeng2023scenecomposer,shi2022semanticstylegan}.
With semantic information as guidance, existing methods have achieved promising performance.
However, we can still observe unsatisfying aspects, especially on the generation of the small-scale objects, which we believe is mainly due to the problem of spatial resolution losses associated with deep network operations such as convolution, normalization, down-sampling, etc.
To solve this problem, \cite{park2019semantic}~proposed GauGAN, which uses the input semantic labels to modulate the activations in normalization layers through a spatially-adaptive transformation.
However, the spatial resolution losses caused by other operations, such as convolution and down-sampling, have not been resolved.
Moreover, we observe that the input label map has only a few semantic classes in the entire dataset. 
Thus the generator should focus more on learning these existing semantic classes rather than all the semantic classes.

To tackle both limitations, we propose a novel semantic preserving module, which aims to selectively highlight class-dependent feature maps according to the input labels for generating semantically consistent images. 
We also propose a new similarity loss to model the intra-class and inter-class semantic dependencies.

% Figure environment removed

\noindent \textbf{Contrastive Learning.}
Recently, the most compelling
methods for learning representations without labels have
been unsupervised contrastive learning \cite{van2018representation,hjelm2018learning,pan2021videomoco,wu2018unsupervised,chen2020simple,zhao2021contrastive}, which significantly outperform other pretext task-based alternatives \cite{gidaris2018unsupervised,doersch2015unsupervised,noroozi2016unsupervised}. 
Contrastive learning aims to learn the general features of unlabeled data by teaching and guiding the model which data points are different or similar.
For example, 
\cite{pan2021videomoco} proposed VideoMoCo for unsupervised video representation learning.
\cite{chen2020simple} introduced a simple framework for contrastive learning of visual representations, which we call SimCLR.
\cite{hu2021region} designed a region-aware contrastive learning to explore semantic relations for the specific semantic segmentation problem.
Both \cite{wang2021exploring} and \cite{zhao2021contrastive} also proposed two new contrastive learning-based strategies for semantic segmentation.

However, we propose a novel contrastive learning method for semantic image synthesis in this paper. 
This synthesis task is very different from the semantic segmentation task, which requires us to tailor the network structure and loss function.
Specifically, we propose a new training protocol that explores global pixel relations in labeled layouts for regularizing the generation embedding space. 
Moreover, we extend it to a multi-scale version that can enforce local-global feature consistency between low-resolution global and high-resolution local features by introducing two new multi-scale and cross-scale contrastive learning losses.
