\section{Edge Guided GANs with Contrastive Learning}

\noindent \textbf{Framework Overview.}
Figure~\ref{fig:method} shows the overall structure of ECGAN for semantic image synthesis, which consists of a semantic and edge guided generator $G$ and a multi-modality discriminator $D$.
The generator $G$ consists of eight components:
(1) a parameter-sharing convolutional encoder $E$ is proposed to produce deep feature maps $F$; 
(2) an edge generator $G_e$ is adopted to generate edge maps $I'_e$ taking as input deep features from the encoder;
(3) an image generator $G_i$ is used  to produce intermediate images $I'$;
(4) an attention guided edge transfer module $G_t$ is designed to forward useful structure information from the edge generator to the image generator;
(5) the semantic preserving module $G_s$ is developed to selectively highlight class-dependent feature maps according to the input label for generating semantically consistent images $I''$;
(6) a label generator $G_l$ is employed to produce the label from $I''$;
(7) the similarity loss is proposed to calculate the intra-class and inter-class relationships.
(8) the contrastive learning module $G_c$ aims to 
model global semantic relations between training pixels, guiding pixel embeddings towards cross-image
category-discriminative representations that eventually improve the generation performance.

Meanwhile, to effectively train the network, we propose a multi-modality discriminator $D$ that distinguishes the outputs from both modalities, i.e., edge and image.

\subsection{Edge Guided Semantic Image Synthesis}
\noindent \textbf{Parameter-Sharing Encoder.}
The backbone encoder $E$ can employ any deep network architecture, e.g., the commonly used AlexNet \cite{krizhevsky2012imagenet},
VGG \cite{simonyan2015very}, and ResNet \cite{he2016deep}. 
We directly utilize the feature maps  from the last convolutional layer as deep feature representations, i.e., $F {=} E(S)$, where $E$ represents the encoder; $S{\in} \mathbb{R}^{N \times H \times W}$ is the input label, with $H$ and $W$ as width and height of the input semantic labels, and $N$ as the total number of semantic classes.
Optionally, one can always combine multiple intermediate feature maps to enhance the feature representation.
The encoder is shared by the edge generator and the image generator.
Then, the gradients from the two generators all contribute  to updating the parameters of the encoder.
This compact design can potentially enhance the deep representations as the encoder can simultaneously learn structure representations from the edge generation branch and appearance representations from the image generation branch.

% Figure environment removed

\noindent \textbf{Edge Guided Image Generation.}
As discussed, the lack of detailed structure or geometry guidance makes it extremely difficult for the generator to produce realistic local structures and details.
To overcome this limitation, we propose to adopt the edge as guidance.
A novel edge generator $G_e$ is designed to directly generate the edge maps from the input semantic labels.
This also facilitates the shared encoder to learn more local structures of the targeted images.
Meanwhile, the image generator $G_i$ aims to generate photo-realistic images from the input labels.
In this way, the encoder is boosted to learn the appearance information of the targeted images.

 

Previous works \cite{park2019semantic,liu2019learning,qi2018semi,chen2017photographic,wang2018high} directly use deep networks to generate the target image, which is challenging since the network needs to simultaneously learn appearance and structure information from the input labels.
In contrast, the proposed method  learns structure and appearance separately via the proposed edge generator and image generator. 
Moreover, the explicit guidance from the ground truth edge maps can also facilitate the training of the encoder.
The framework of both edge and image generators is illustrated in Figure~\ref{fig:edge_block}.
Given the feature maps from the last convolutional layer of the encoder, i.e., $F {\in} \mathbb{R}^{C \times H \times W}$, 
where $H$ and $W$ are the width and height of the features, and $C$ is the number of channels, the edge generator produces edge features and edge maps which are further utilized to guide the image generator to generate the intermediate image $I'$.
The edge generator $G_e$ contains $n$ convolution layers and correspondingly produces $n$ intermediate feature maps $F_e {=} \{ F_e^j\}_{j=1}^n$.
After that, another convolution layer with Tanh non-linear activation is utilized to generate the edge map $I'_e{\in} \mathbb{R}^{3 \times H \times W}$.
Meanwhile, the feature maps $F$ is also fed into the image generator $G_i$ to generate $n$ intermediate feature maps $F_i{=}\{F_i^j\}_{j=1}^n$.
Then another convolution operation with Tanh non-linear activation is adopted to produce the intermediate image $I'_i{\in} \mathbb{R}^{3 \times H \times W}$.
In addition, the intermediate edge feature maps $F_e$ and the edge map $I'_e$ are utilized to guide the generation of the image feature maps $F_i$ and the intermediate image $I'$ via the Attention Guided Edge Transfer as detailed below.

\noindent \textbf{Attention Guided Edge Transfer.}
We further propose a novel attention guided edge transfer module $G_t$ to explicitly employ the edge structure information to refine the intermediate image representations.
The architecture of the proposed transfer module $G_t$ is illustrated in Figure~\ref{fig:edge_block}.
To transfer useful structure information from edge feature maps $F_e {=} \{ F_e^j\}_{j=1}^n$ to the image feature maps $F_i{=}\{F_i^j\}_{j=1}^n$, the edge feature maps are firstly processed by a Sigmoid activation function to generate the corresponding attention maps $F_a {=}{\rm Sigmoid}(F_e) {=} \{ F_a^j\}_{j=1}^n$.
The attention aims to provide structural information (which cannot be provided by the input label map) within each semantic class.
Then, we multiply the generated attention maps with the corresponding image feature maps to obtain the refined maps, which incorporate local structures and details.
Finally, the edge refined features are element-wisely summed with the original image features to produce the final edge refined  features, which are further fed to the next convolution layer as $F_i^j {=}{\rm Sigmoid}(F_e^j) {\times} F_i^j {+} F_i^j  (j  {=}  1, \cdots, n)$.
In this way, the image feature maps also contain the local structure information provided by the edge feature maps.
Similarly, to directly employ the structure information from the generated edge map $I_e^{'}$ for image generation, we adopt the attention guided edge transfer module to refine the generated image directly with edge information as
\begin{equation}
\begin{aligned}
I' = {\rm Sigmoid}(I'_e) \times I'_i +  I'_i,
\end{aligned}\label{eqn:image}
\end{equation}
where $I'_a{=}{\rm Sigmoid}(I'_e)$ is the generated attention map. We also visualize the results in Figure~\ref{fig:diff2}.


% Figure environment removed


\subsection{Semantic Preserving Image Enhancement}

\noindent \textbf{Semantic Preserving Module}. Due to the spatial resolution loss caused by  convolution, normalization, and down-sampling layers, existing models \cite{wang2018high,park2019semantic,qi2018semi,chen2017photographic} cannot fully preserve the semantic information of the input labels as illustrated in Figure~\ref{fig:city_seg}.
For instance, the small ``pole'' is missing, and the large ``fence'' is incomplete.
To tackle this problem, we propose a novel semantic preserving module, which aims to select class-dependent feature maps and further enhance it through the guidance of the original semantic layout. 
An overview of the proposed semantic preserving module $G_s$ is shown in Figure \ref{fig:semantic_block}(left).
Specifically, the input of the module denoted as $\mathcal{F}$,
is the concatenation of the input label $S$, the generated intermediate edge map $I'_e$ and image $I'$, and the deep feature $F$ produced from the shared encoder $E$.
Then, we apply a convolution operation on $\mathcal{F}$ to produce a new feature map $\mathcal{F}_c$ with the number of channels equal to the number of semantic categories, where each channel corresponds to a specific semantic category (a similar conclusion can be found in \cite{fu2019dual}).
Next, we apply the averaging pooling operation on $\mathcal{F}_c$ to obtain the global information of each class, followed by a  Sigmoid activation function to derive scaling factors $\gamma'$ as in $\gamma' {=} {\rm Sigmoid} ({\rm AvgPool}(\mathcal{F}_c))$, where each value represents the importance of the corresponding class. 
Then, the scaling factor $\gamma'$ is adopted to reweight the feature map $\mathcal{F}_c$ and highlight corresponding class-dependent feature maps.
The reweighted feature map is further added with the original feature $\mathcal{F}_c$ to compensate for information loss due to multiplication, and 
produces $\mathcal{F}'_c {=} \mathcal{F}_c  {\times} \gamma' {+} \mathcal{F}_c$, where $\mathcal{F}'_c {\in} \mathbb{R}^{N \times H \times W}$.

After that, we perform another convolution operation on $\mathcal{F}'_c$ to obtain the feature map $\mathcal{F}' {\in} \mathbb{R}^{(C+N+3+3) \times H \times W}$ to enhance the representative capability of the feature. In addition, $\mathcal{F}'$ has the same size as the original input one $\mathcal{F}$, which makes the module flexible and can be plugged into other existing architectures without modifications of other parts to refine the output.
In Figure~\ref{fig:semantic_block}(right), we visualize three channels in~$\mathcal{F}'$ on Cityscapes, i.e., road, car, and vegetation.
We can easily observe that each channel learns well the class-level deep
representations.

Finally, the feature map $\mathcal{F}'$ is fed into a convolution layer followed by a Tanh non-linear activation layer to obtain the final result $I''$.
Our semantic preserving module enhances the representational power of the model by adaptively recalibrating semantic class-dependent feature maps, and shares similar spirits with style transfer \cite{huang2017arbitrary}, and SENet \cite{hu2018squeeze} and EncNet \cite{zhang2018context}. 
One intuitive example of the utility of the module is for the generation of small object classes: these classes are easily missed in the generation results due to spatial resolution loss, while our scaling factor can put an emphasis on small objects and help preserve them.

\noindent \textbf{Similarity Loss.} 
Preserving semantic information from isolated pixels is very challenging for deep networks. To explicitly enforce the network to capture the relationship between semantic categories, a new similarity loss is introduced. This loss forces the network to consider both intra-class and inter-class pixels for each pixel in the label. Specifically, a state-of-the-art pretrained model (i.e., SegFormer \cite{xie2021segformer}) is used to transfer the generated image $I''$ back to a label $S'' {\in} \mathbb{R}^{N \times H \times W}$, where $N$ is the total number of semantic classes, and $H$ and $W$ represent the width and height of the image, respectively. A conventional method uses the cross entropy loss between $S''$ and $S$ to address this problem. 
However, such a loss only considers the isolated pixel while ignoring the semantic correlation with other pixels.

To address this limitation, we construct a similarity map from $S{\in} \mathbb{R}^{N \times H \times W}$. 
Firstly, we reshape $S$ to $\hat{S}{\in} \mathbb{R}^{N {\times} M}$, where $M {=} H {×}W$. Next, we perform a matrix multiplication to obtain a similarity map $A{=}\hat{S}\hat{S}^\top {\in} \mathbb{R}^{M{\times}M}$. This similarity map encodes which pixels belong to the same category, meaning that if the j-\textit{th} pixel and the i-\textit{th} pixel belong to the same category, then the value of the j-\textit{th} row and the i-\textit{th} column in $A$ is 1; otherwise, it is 0.
Similarly, we can obtain a similarity map $A''$ from the label $S''$.
Finally, we calculate the binary cross entropy loss between the two similarity maps $\{a_m {\in}A, m{\in} [1, M^2]\}$ and $\{a''_m{\in}A'', m{\in}[1, M^2]\}$ as
\begin{equation}
	\begin{aligned}
\mathcal{L}_{sim}(S, S'') = - \frac{1}{M^2} \sum_{m=1}^{M^2} (a_m \log a''_m + (1-a_m)\log (1-a''_m)).	\end{aligned}\label{eq:similarityloss}
\end{equation}
This loss explicitly captures intra-class and inter-class semantic correlation, leading to better generation results.



% Figure environment removed


\subsection{Contrastive Learning for Semantic Image Synthesis}

\noindent\textbf{Pixel-Wise Contrastive Learning.}
Existing semantic image synthesis models use deep networks to map labeled pixels to a non-linear embedding space. However, these models often only take into account the ``local'' context of pixel samples within an individual input semantic layout, and fail to consider the ``global'' context of the entire dataset, which includes the semantic relationships between pixels across different input layouts. This oversight raises an important question: what should the ideal semantic image synthesis embedding space look like? Ideally, such a space should not only enable accurate categorization of individual pixel embeddings, but also exhibit a well-structured organization that promotes intra-class similarity and inter-class difference. 
That is, pixels from the same class should generate more similar image content than those from different classes in the embedding space.
Previous approaches to representation learning propose that incorporating the inherent structure of training data can enhance feature discriminativeness. 
Hence, we conjecture that despite the impressive performance of existing algorithms, there is potential to create a more well-structured pixel embedding space by integrating both the local and global context.

The objective of unsupervised representation learning is to train an encoder that maps each training semantic layout $S$ to a feature vector $v {=} B(S)$, where $B$ represents the backbone encoder network. The resulting vector $v$ should be an accurate representation of $S$. To accomplish this task, contrastive learning approaches use a training method that distinguishes a positive from multiple negatives, based on the similarity principle between samples. The InfoNCE \cite{van2018representation,gutmann2010noise} loss function, a popular choice for contrastive learning, can be expressed as
\begin{equation}
		\mathcal{L}_S =   -\log
		\frac {\exp(v  \cdot  v_+ /  \tau  )}{
			\exp (v  \cdot  v_+  / \tau) +  \sum _ {v_- \in N_ {S}} {\exp(v\cdot v_- / \tau) }},
\end{equation}
where $v_+$ represents an embedding of a positive for $S$, and $N_S$ includes embeddings of negatives. The symbol ``·'' refers to the inner (dot) product, and $\tau {>}0$ is a temperature hyper-parameter. It is worth noting that the embeddings used in the loss function are normalized using the $L_2$ method.

One limitation of this training objective design is that it only penalizes pixel-wise predictions independently, without considering the cross-relationship between pixels. 
To overcome this limitation, we take inspiration from \cite{wang2021exploring,khosla2020supervised} and propose a contrastive learning method that operates at the pixel level and is intended to regularize the embedding space while also investigating the global structures present in the training data (see Figure \ref{fig:method_contrastive}).
Specifically, our contrastive loss computation uses training semantic layout pixels as data samples. For a given pixel $i$ with its ground-truth semantic label $c$, the positive samples consist of other pixels that belong to the same class $c$, while the negative samples include pixels belonging to other classes $C{\setminus}{c}$. As a result, the proposed pixel-wise contrastive learning loss is defined as follows
\begin{equation}
		\mathcal{L}_i =   \frac {1}{|P_ {i}|}   \sum_{i_+ \in P_i}  -\log
		\frac {\exp(i  \cdot  i_+ /  \tau  )}{
			\exp (i  \cdot  i_+  / \tau) +  \sum _ {i_- \in N_ {i}} {\exp(i\cdot i_- / \tau) }}.
\label{eq:contrastive}
\end{equation}
For each pixel $i$, we use $P_i$ and $N_i$ to represent the pixel embedding collections of positive and negative samples, respectively. Importantly, the positive and negative samples and the anchor $i$ are not required to come from the same layout. The goal of this pixel-wise contrastive learning approach is to create an embedding space in which same-class pixels are pulled closer together, and different-class pixels are pushed further apart. The result of this process is that pixels with the same class generate image contents that are more similar, which can lead to superior generation performance.


\noindent \textbf{Multi-Scale Contrastive Learning.}
In this part, we extend the pixel-level loss function $\mathcal{L}_i$ in Eq. \eqref{eq:contrastive} to an
arbitrary scale loss function $\mathcal{L}_i^s$ for calculating the contrastive learning loss, where $s$ means the $s$-\textit{th} scale feature representation, and we have a total of $\mathcal{S}$ different scales. 
This strategy regularizes the feature space of different scales by pulling features of the same class closer and pulling features of different classes apart, leading to a more well-structured feature space.

The overview framework of the proposed multi-scale contrastive learning is shown in Figure \ref{fig:method_contrastive_multi}. 
First, the input layouts go through the backbone encoder network $B$ to obtain multi-scale representation.
Next, we use a weighted sum at different scales to constraint the multi-scale features
\begin{equation}
\begin{aligned}
& \mathcal{L}_{i}^{ms}  =  \sum_{s=1}^\mathcal{S} w_s \mathcal{L}_i^s = w_1 \mathcal{L}_i^1 + \cdots + w_s \mathcal{L}_i^s + \cdots + w_\mathcal{S} \mathcal{L}_i^\mathcal{S} = \\
&  w_1 \frac {1}{|P_{i}^1|}   \sum_{i_+^1 \in P_i^1}  {-}\log \frac {\exp(i^1  \cdot  i_+^1 /  \tau  )}{\exp (i^1  \cdot  i_+^1  / \tau) +  \sum _ {i_-^1 \in N_{i}^1} {\exp(i^1\cdot i_-^1 / \tau) }} \\
& + \cdots + \\
&  w_s \frac {1}{|P_{i}^s|}   \sum_{i_+^s \in P_i^s}  {-}\log \frac {\exp(i^s  \cdot  i_+^s /  \tau  )}{\exp (i^s  \cdot  i_+^s  / \tau) +  \sum _ {i_-^s \in N_{i}^s} {\exp(i^s\cdot i_-^s / \tau) }} \\
   & + \cdots + \\
& w_\mathcal{S} \frac {1}{|P_{i}^\mathcal{S}|}   \sum_{i_+^\mathcal{S} \in P_i^\mathcal{S}} {-}\log \frac {\exp(i^\mathcal{S}  \cdot  i_+^\mathcal{S} /  \tau  )}{
\exp (i^\mathcal{S}  \cdot  i_+^\mathcal{S}  / \tau) +  \sum _ {i_-^\mathcal{S} \in N_{i}^\mathcal{S}} {\exp(i^\mathcal{S}\cdot i_-^\mathcal{S} / \tau) }}.
\label{eq:contrastive_multi}
\end{aligned}
\end{equation}
To identify the semantic classes in each pixel of different scale feature maps, we use the original input layout downsampled to the spatial dimensions.
We select the feature pairs with the same semantic label and at the same scale as positive pairs. On the contrary, we choose the feature pairs with different semantic labels and within the same scale as negative pairs.
Specifically, for each pixel $i^s$, we use $P_i^s$ and $N_i^s$ to represent the pixel embedding collections of positive and negative samples at the $s$-\textit{th} scale feature representation, respectively. Noth that the positive and negative samples and the anchor $i^s$ are from different layouts but the same scale feature embedding space. The weights $[w_1, \cdots, w_s, \cdots, w_\mathcal{S}]$ control the contribution of each scale to the overall loss.
Note that the first scale loss $\mathcal{L}_i^1$ is the same as the pixel-wise contrastive learning $\mathcal{L}_i$ in Eq. \eqref{eq:contrastive}.

As shown in Figure \ref{fig:method_contrastive_multi}, we also need to push same-class features from different scales closer together and pull different-class features apart.
For instance, if we have two scales $s_p$ and $s_q$, we hope features of the same class to be close on scales $s_p$ and $s_q$ ($s_p{\neq}s_q$), and features of different classes to be far apart on both scales $s_p$ and $s_q$.
That is, local features should describe parts of objects/regions of their global structure of the object and vice versa.
Thus the cross-scale contrastive learning loss can be formulated as
\begin{equation}
\begin{aligned}
&\mathcal{L}_{i}^{cs} =  \sum_{s_p=1}^{s_p=S} \sum_{s_q=1}^{s_q=S} w_{s_p, s_q} \mathcal{L}_i^{s_p, s_q} = \\
& w_{1, 2} \mathcal{L}_i^{1, 2} + \cdots  + w_{1, s} \mathcal{L}_i^{1, s} + \cdots + w_{1, \mathcal{S}} \mathcal{L}_i^{1, \mathcal{S}} + \cdots + w_{s, \mathcal{S}} \mathcal{L}_i^{s, \mathcal{S}}.
\end{aligned}
\label{eq:contrastive_cross}
\end{equation}
We downsample the original input layout into layouts of different scales on the spatial dimension so that we can obtain the semantic labels at each scale. We select the feature pairs with the same semantic label but at different scales as positive samples. In contrast, we select feature pairs with different semantic labels and at different scales as negative samples.
By doing so, we can achieve a bidirectional local-global consistency for learning the encoder network.
The weights $[w_{1,2}, \cdots, w_{1,s}, \cdots, w_{1,\mathcal{S}}, \cdots, w_{s, \mathcal{S}}]$ control the contribution of each scale to the overall loss.

Eq. \eqref{eq:contrastive_multi} and \eqref{eq:contrastive_cross} can be added together to obtain our complete contrastive learning loss.

\noindent \textbf{Class-Specific Pixel Generation.}
To overcome the challenges posed by training data imbalance between different classes and size discrepancies between different semantic objects, we introduce a new approach that is specifically designed to generate small object classes and fine details. Our proposed method is a class-specific pixel generation approach that focuses on generating image content for each semantic class. Doing so can avoid the interference from large object classes during joint optimization, and each subgeneration branch can concentrate on a specific class generation, resulting in similar generation quality for different classes and yielding richer local image details.

An overview of the class-specific pixel generation method is provided in Figure~\ref{fig:method_contrastive}. 
After the proposed pixel-wise contrastive learning, we obtain a class-specific feature map for each pixel. 
Then, the feature map is fed into a decoder for the corresponding semantic class, which generates an output image $\hat{I}_{i}$. 
Since we have the proposed contrastive learning loss, we can use the parameter-shared decoder to generate all classes.
To better learn each class, we also utilize a pixel-wise $L_1$ reconstruction loss, which can be expressed as $\mathcal{L}_{L_1} {=} \sum_{i=1}^{N} \mathbb{E}_{I_i, \hat{I}_i} \lbrack \vert\vert I_i {-} \hat{I}_i \vert\vert_1 \rbrack.$
The final output $I_g$ from the pixel generation network can be obtained by performing an element-wise addition of all the class-specific outputs:
$I_g {=} I_{g_1} \oplus I_{g_2} \oplus \cdots \oplus I_{g_N}.$


\subsection{Model Training}
\noindent \textbf{Multi-Modality Discriminator.}
To facilitate the training of the proposed method for high-quality edge and image generation, a novel multi-modality discriminator is developed to simultaneously distinguish outputs from two modality spaces, i.e., edge and image. 
Since the edges and RGB images share the same structure, they can be learned using the multi-modality discriminator. In the preliminary experiment, we also tried to use two discriminators (i.e., an edge discriminator and an image discriminator), but no performance improvement was observed while increasing the model complexity. Thus, we use the proposed multi-modality discriminator.
The framework of the multi-modality discriminator is shown in Figure~\ref{fig:method}, which is capable of discriminating both real/fake images and edges. 
To discriminate real/fake edges, the discriminator loss considering the semantic label $S$ and the generated edge $I'_e$ (or the real edge $I_e$) is as
\begin{equation}
\begin{aligned}
\mathcal{L}_{\mathrm{CGAN}}(G_e, D) & = 
\mathbb{E}_{S, I_e} \left[ \log D(S, I_e) \right] \\
& +  \mathbb{E}_{S, I'_e} \left[\log (1 - D(S, I'_e)) \right],
\end{aligned}
\label{eqn:discriminator1}
\end{equation}
which guides the model to distinguish real edges from fake generated edges.
Further, to discriminate real/fake images, the discriminator loss regarding  semantic label $S$ and the generated images $I'$, $I''$ (or the real image $I$) is as Eq.~\eqref{eqn:discriminator2}, which guides the model to discriminate real/fake images,
\begin{equation}
	\begin{aligned}
\mathcal{L}_{\mathrm{CGAN}}(G_i, G_s, D)  & = (\lambda + 1) \mathbb{E}_{S, I} \left[ \log D(S, I) \right]  \\
& +  \mathbb{E}_{S, I'} \left[\log (1 - D(S, I')) \right] \\
& +  \lambda \mathbb{E}_{S, I''} \left[\log (1 - D(S, I'')) \right],
\end{aligned}
\label{eqn:discriminator2}
\end{equation}
where $\lambda$ controls the losses of the two generated images. 
The inclusion of $I'$ and $I''$ is a cascaded coarse-to-fine generation strategy \cite{tang2019multi}, i.e., $I'$ is the coarse result, while $I''$ is the refined result. 
The intuition is that $I''$ will be better generated based on $I'$, so we provide $I'$ to the discriminator to ensure that $I'$ is also realistic. 

% Figure environment removed



\noindent \textbf{Optimization Objective.}
Equipped with the multi-modality discriminator, we elaborate on the training objective for the proposed method as follows.
Five different losses, i.e., the multi-modality adversarial loss, the similarity loss, the contrastive learning loss, the discriminator feature matching loss $\mathcal{L}_{f}$, and the perceptual loss $\mathcal{L}_{p}$ are used to optimize the proposed ECGAN,
\begin{equation}
\begin{aligned}
\min_{G} \max_{D} \mathcal{L} & = \lambda_{c} \underbrace{(\mathcal{L}_{\mathrm{CGAN}}(G_e, D) 
+ \mathcal{L}_{\mathrm{CGAN}}(G_i, G_s, D))}_{\text{Multi-Modality Adversarial Loss}} \\
&  + \lambda_{s} \underbrace{\mathcal{L}_{sim}(S, S')  + \mathcal{L}_{sim}(S, S'')}_{\text{Similarity Loss}} \\
& +  \lambda_{l} \underbrace{\mathcal{L}_{i}^{ms} + \mathcal{L}_{i}^{cs} + \mathcal{L}_{L_1}}_{\text{Contrastive Learning Loss}}  \\
& + \lambda_{f}\underbrace{(\mathcal{L}_{f}(I_e, I'_e) {+} \mathcal{L}_{f}(I, I') {+} \lambda \mathcal{L}_{f}(I, I''))}_{\text{Discriminator  Feature Matching Loss}} \\
& + \lambda_{p} \underbrace{(\mathcal{L}_{p}(I_e, I'_e) {+} \mathcal{L}_{p}(I, I') {+} \lambda \mathcal{L}_{p}(I, I''))}_{\text{Perceptual Loss}},
\label{eq:loss} 
\end{aligned}
\end{equation}
where $\lambda_{c}$, $\lambda_{s}$, $\lambda_{l}$, $\lambda_{f}$, and $\lambda_{p}$ are the parameters of the corresponding loss that contributes to the total loss $\mathcal{L}$;
where $\mathcal{L}_{f}$ matches the discriminator intermediate features between the generated images/edges and the real images/edges; where $\mathcal{L}_{p}$ matches the VGG extracted features between the generated images/edges and the real images/edges.
By maximizing the discriminator loss, the generator is promoted to simultaneously  generate reasonable edge maps that can capture the local-aware structure information and generate realistic images semantically aligned with the input labels.




\subsection{Implementation Details}
\label{sm:Implementation}

For both the image generator $G_i$ and edge generator $G_e$, the kernel size and padding size of convolution layers are all $3 {\times} 3$ and 1 for preserving the feature map size.
We 	set $n{=}3$ for  generators $G_i$, $G_s$, and $G_t$.
The channel size of feature $F$ is set to $C{=}64$. 
For the semantic preserving module $G_s$, we adopt an adaptive average pooling operation.
Spectral normalization \cite{miyato2018spectral} is applied to all the layers in both the generator and discriminator.
Our method incorporates the use of the Canny edge detector \cite{canny1986computational} for the purpose of deriving edge maps essential to our training process. In the subsequent testing phase, our approach necessitates no supplemental data, maintaining the fairness of comparisons with other existing methods.

\begin{table*}[!t] \small
	\centering
	\caption{User study on Cityscapes, ADE20K, and COCO-Stuff. The numbers indicate the percentage of users who favor the results of the proposed ECGAN over the competing methods.}
%		\resizebox{1\linewidth}{!}{% 
	\begin{tabular}{lccc} \toprule
		AMT $\uparrow$                               & Cityscapes & ADE20K  &  COCO-Stuff \\ \midrule
		Our ECGAN vs. CRN~\cite{chen2017photographic}     & 88.8 {$\pm$ 3.4}      & 94.8 {$\pm$ 2.7} & 95.3 {$\pm$ 2.1}\\
		Our ECGAN vs. Pix2pixHD~\cite{wang2018high}        & 87.2 {$\pm$ 2.9}       & 93.6 {$\pm$ 3.1}  &  93.9 {$\pm$ 2.4} \\ 
		Our ECGAN vs. SIMS~\cite{qi2018semi}                      & 85.3 {$\pm$ 3.8}       & -     & - \\
		Our ECGAN vs. GauGAN~\cite{park2019semantic}    & 84.7 {$\pm$ 4.3}       & 88.4 {$\pm$ 3.7}  &  90.8 {$\pm$ 2.5} \\ 
		Our ECGAN vs. DAGAN~\cite{tang2020dual}             & 81.8 {$\pm$ 3.9}       & 86.2 {$\pm$ 3.6}  & -\\
		Our ECGAN vs. CC-FPSE~\cite{liu2019learning}         & 79.5 {$\pm$ 4.2}       & 85.1 {$\pm$ 3.9}  &  86.7 {$\pm$ 2.8} \\  
		Our ECGAN vs. LGGAN \cite{tang2020local}              & 78.4 {$\pm$ 4.7}       & 82.7 {$\pm$ 4.5} & - \\
		Our ECGAN vs. OASIS \cite{sushko2020you}             & 76.7 {$\pm$ 4.8}        & 80.6 {$\pm$ 4.5}  & 82.5 {$\pm$ 3.1}  \\	\bottomrule
	\end{tabular}
	\label{tab:atm1}
		\vspace{-0.2cm}
\end{table*}

\begin{table*}[!t] \small
	\centering
	\caption{User study on Cityscapes, ADE20K, and COCO-Stuff. The numbers indicate the percentage of users who favor the results of the proposed ECGAN++ over the proposed ECGAN.}
%		\resizebox{1\linewidth}{!}{% 
	\begin{tabular}{lccc} \toprule
		AMT $\uparrow$                               & Cityscapes & ADE20K  &  COCO-Stuff \\ \midrule
		Our ECGAN++ vs. Our ECGAN \cite{tang2023edge}           & 64.3 {$\pm$ 3.2}        & 67.5 {$\pm$ 3.8}  & 70.4 {$\pm$ 2.6}  \\	\bottomrule
	\end{tabular}
	\label{tab:atm2}
		\vspace{-0.2cm}
\end{table*}

\begin{table*}[!t] \small
	\centering
	\caption{Quantitative comparison of different methods on Cityscapes, ADE20K, and COCO-Stuff.
	}
	\resizebox{1\linewidth}{!}{% 
	\begin{tabular}{rlllllllll} \toprule
		\multirow{2}{*}{Method}  & \multicolumn{3}{c}{Cityscapes} & \multicolumn{3}{c}{ADE20K} & \multicolumn{3}{c}{COCO-Stuff} \\ \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10} 
		& mIoU $\uparrow$    & Acc $\uparrow$  & FID  $\downarrow$ & mIoU $\uparrow$    & Acc $\uparrow$  & FID  $\downarrow$  & mIoU $\uparrow$    & Acc $\uparrow$  & FID  $\downarrow$ \\ \midrule
		CRN~\cite{chen2017photographic}  & 52.4  & 77.1 & 104.7  & 22.4 & 68.8 & 73.3 & 23.7 & 40.4 & 70.4\\
		SIMS~\cite{qi2018semi}           & 47.2  & 75.5 & 49.7 & - & - & - & - & - & - \\
		Pix2pixHD~\cite{wang2018high}    & 58.3  & 81.4 & 95.0  & 20.3 & 69.2 & 81.8  & 14.6 & 45.8 & 111.5  \\ 
  		GauGAN~\cite{park2019semantic}   & 62.3  & 81.9 & 71.8  & 38.5 & 79.9 & 33.9 & 37.4 & 67.9 & 22.6\\
         DPGAN \cite{tang2021layout} & 65.2 & 82.6 & 53.0 & 39.2 & 80.4 & 31.7 & - & - & - \\
		DAGAN \cite{tang2020dual} & 66.1 & 82.6 & 60.3 &   40.5 &  81.6 & 31.9 & - & - & -\\
            SelectionGAN \cite{tang2019multi} & 83.8 & 82.4 & 65.2 & 40.1 & 81.2 & 33.1 & - & - & -\\
            SelectionGAN++ \cite{tang2022multi} & 64.5 & 82.7 & 63.4 & 41.7 & 81.5 & 32.2 & - & - & - \\
		LGGAN \cite{tang2020local} & 68.4 & 83.0 & 57.7 & 41.6 & 81.8 & 31.6 & - & - & -\\
            LGGAN++ \cite{tang2022local}  & 67.7 & 82.9 & 48.1 & 41.4 & 81.5 & 30.5 & - & - & - \\
		CC-FPSE~\cite{liu2019learning}  & 65.5 & 82.3 & 54.3 & 43.7 & 82.9 & 31.7 & 41.6 & 70.7 & 19.2 \\
            \hao{SCG \cite{wang2021image}} & \hao{66.9} & \hao{82.5} & \hao{49.5} & \hao{45.2} & \hao{83.8} & \hao{29.3} & \hao{42.0} & \hao{72.0} & \hao{18.1}\\
		OASIS \cite{sushko2020you} &   69.3 & - & 47.7 & 48.8 & - & 28.3 & 44.1 & - & 17.0  \\
		\hao{RESAIL \cite{shi2022retrieval}} & \hao{69.7} & \hao{83.2}
		& \hao{45.5} & \hao{49.3} & \hao{84.8} &\hao{30.2} & \hao{44.7} & \hao{73.1} & \hao{18.3} \\
		\hao{SAFM \cite{lv2022semantic}} &\hao{70.4} & \hao{83.1} &\hao{49.5} &\hao{50.1}&\textbf{\hao{86.6}}&\hao{32.8}& \hao{43.3} & \textbf{\hao{73.4}} & \hao{24.6} \\
            \hao{PITI \cite{wang2022pretraining}} & \hao{-} & \hao{-} & \hao{-} & \hao{-} & \hao{-} & \hao{-} & \hao{-} & \hao{-} & \hao{19.36}\\
            \hao{T2I-Adapter \cite{mou2023t2i}} & \hao{-} & \hao{-} & \hao{-} & \hao{-} & \hao{-} & \hao{-} & \hao{-} & \hao{-} & \hao{16.78}\\
            \hao{SDM \cite{wang2022semantic}} & \hao{-} & \hao{-} & \hao{\textbf{42.1}} & \hao{-} & \hao{-} & \hao{27.5} & \hao{-} & \hao{-} & \hao{15.9}\\
		ECGAN (Ours)                        & 72.2    & 83.1 & 44.5 & 50.6 & 83.1 & 25.8 & 46.3 & 70.5 & 15.7 \\
		ECGAN++ (Ours) & \textbf{73.3} (+1.1) & \textbf{83.9} (+0.8) & 42.2 (-2.3) & \textbf{52.7} (+2.1) & 85.9 (+2.8) & \textbf{24.7} (-1.1) & \textbf{47.9} (+1.6) & 72.3 (+1.8) & \textbf{14.9} (-0.8) \\
		\bottomrule
	\end{tabular}}
	\label{tab:sota}
	\vspace{-0.4cm}
\end{table*}

In our computation of the contrastive learning loss, we observe a direct correlation between the number of layouts used and the resultant performance, i.e., more layouts lead to enhanced performance. However, a plateau is reached when the count exceeds 8 layouts; additional layouts contribute only marginal improvements to performance, while significantly slowing down the overall training process. Thus, with the objective of striking a balance between performance efficiency and computational time, we elect to use 8 layouts as input for the calculation of contrastive learning loss.
We use features from four scales in Eq. \eqref{eq:contrastive_multi}, with feature map output strides of 1, 4, 8, and 16, to calculate the multi-scale contrastive learning loss. 
Meanwhile, we also downsample the input layout by 4, 8, and 16 times to obtain the label of the corresponding scale for calculating the multi-scale contrastive learning loss.
The weights $w_s$ in Eq. \eqref{eq:contrastive_multi} are set to  1, 0.7, 0.4, and 0.1 in a decreasing way for feature maps of strides 1, 4, 8, and 16, respectively.
Moreover, in order to balance the performance and efficiency, we adopt two cross-scale contrastive learning in Eq. \eqref{eq:contrastive_cross}, i.e., (s4, s8) and (s4, s16).
We set both weights in Eq. \eqref{eq:contrastive_cross} to 0.1.

Also, we follow the training procedures of GANs \cite{goodfellow2014generative} and alternatively train the generator $G$ and discriminator $D$, i.e., one gradient descent step on the discriminator and generator alternately. 
We use the Adam solver \cite{kingma2014adam} and set $\beta_1{=}0$, $\beta_2{=}0.999$.
$\lambda_{c}$,  $\lambda_{s}$, $\lambda_{l}$, $\lambda_{f}$, and $\lambda_{p}$ in Eq.~\eqref{eq:loss} is set to 1, 1, 1, 10, and 10, respectively.
All $\lambda$ in both Eq.~\eqref{eqn:discriminator2} and \eqref{eq:loss} are set to 2.
We conduct experiments on an NVIDIA DGX1 with 8 V100 GPUs. 
