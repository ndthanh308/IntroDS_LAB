\section{Introduction}

Semantic image synthesis refers to generating photo-realistic images conditioned on pixel-level semantic labels.
This task has a wide range of applications such as image editing and content generation~\cite{chen2017photographic,isola2017image,gu2019mask,liu2019learning,qi2018semi}.
Although existing methods conducted interesting explorations, we still observe unsatisfactory aspects, mainly in the generated local structures and details, as well as small-scale objects, which we believe are mainly due to three reasons:
1) Conventional methods~\cite{park2019semantic,wang2018high,liu2019learning} generally take the semantic label map as input directly. 
However, the input label map provides only structural information between different semantic-class regions and does not contain any structural information within each semantic-class region, making it difficult to synthesize rich local structures within each class.
Taking label map $S$ in Figure~\ref{fig:method} as an example, the generator does not have enough structural guidance to produce a realistic bed, window, and curtain from only the input label ($S$).  
2) The classic deep network architectures are constructed by stacking convolutional, down-sampling, normalization, non-linearity, and up-sampling layers, which will cause the problem of spatial resolution losses of the input semantic labels.
3) Existing methods for this task are typically based on global image-level generation.
In other words, they accept a semantic layout containing several object classes and aim to generate the appearance of each one using the same network. In this way, all the classes are treated equally. 
However, because different semantic classes have distinct properties, using specified network learning for each would intuitively facilitate the complex generation of multiple classes. 


% Figure environment removed


To address these three issues, in this paper, we propose a novel \underline{e}dge guided \underline{g}enerative \underline{a}dversarial \underline{n}etwork with \underline{c}ontrastive learning (ECGAN) for semantic image synthesis. 
The overall framework of ECGAN is shown in Figure~\ref{fig:method}. To tackle 1), we first propose an edge generator to produce the edge features and edge maps. 
Then the generated edge features and edge maps are selectively transferred to the image generator and improve the quality of the synthesized image by using our attention guided edge transfer module. To tackle 2), we propose an effective semantic preserving module, which aims at selectively highlighting class-dependent feature maps according to the original semantic layout. 
We also propose a new similarity loss to model the relationship between semantic categories. Specifically, given a generated label $S''$ and corresponding ground truth $S$, similarity loss constructs a similarity map to supervise the learning. To tackle 3), a straightforward solution would be to model the generation of different image classes individually. By so doing, each class could have its own generation network structure or parameters, thus greatly avoiding the learning of a biased generation space. 
However, there is a fatal disadvantage to this. That is, the number of parameters of the network will increase linearly with the number of semantic classes $N$, which will cause memory overflow and make it impossible to train the model.
If we use $p_e$ and $p_d$ to denote the number of parameters of the encoder and decoder, respectively, then the total number of the network parameter should be $p_e {+} N{\times} p_d$ since we need a new decoder for each class.
To further address this limitation, we introduce a pixel-wise contrastive learning approach that elevates the current image-wise training method to a pixel-wise method. By leveraging the global semantic similarities present in labeled training layouts, this method leads to the development of a well-structured feature space.
In this case, the total number of the network parameter only is $p_e {+} p_d$.
Moreover, we explore image generation from a class-specific context, which is beneficial for generating richer details compared to the existing image-level generation methods. A new class-specific pixel generation strategy is proposed for this purpose. It can effectively handle the generation of small objects and details, which are common difficulties encountered by the global-based generation.

With the proposed ECGAN, we achieve new state-of-the-art results on Cityscapes~\cite{cordts2016cityscapes}, ADE20K~\cite{zhou2017scene}, and COCO-Stuff \cite{caesar2018coco} datasets, demonstrating the effectiveness of our approach in generating images with complex scenes and showing significantly better results compared with existing methods.
To summarize, our contributions are as follows:

\begin{itemize}
	\item We propose a novel ECGAN for the challenging semantic image synthesis task. To the best of our knowledge, we are the first to explore the edge generation from semantic layouts and then utilize the generated edges to guide the generation of realistic images.  
    \item  We propose an effective attention guided edge transfer module to selectively transfer useful edge structure information from the edge generation branch to the image generation branch.
    \item  We design a new semantic preserving module to highlight class-dependent feature maps based on the input semantic label map for generating semantically consistent results. 
    \item We propose a new similarity loss to capture the intra-class and inter-class semantic dependencies, leading to robust training.
    \item We propose a novel contrastive learning method, which learns a well-structured pixel semantic embedding space by utilizing global semantic similarities among labeled layouts. Moreover, we propose a multi-scale contrastive learning method with two novel multi-scale and cross-scale losses that enforces local-global feature consistency between low-resolution global and high-resolution local features extracted from different scales.
    \item We conduct extensive experiments on three challenging datasets under diverse scenarios, i.e., Cityscapes~\cite{cordts2016cityscapes}, ADE20K~\cite{zhou2017scene}, and COCO-Stuff~\cite{caesar2018coco}. Both qualitative and quantitative results show that the proposed methods are able to produce remarkably better results than existing baseline models regarding both visual fidelity and alignment with the input semantic layouts. Moreover, our methods can generate multi-modal images and edges, which have not been considered by existing state-of-the-art methods. 
\end{itemize}

Part of the material presented here appeared in \cite{tang2023edge}. The current paper extends \cite{tang2023edge} in several ways.
(1) We present a more detailed analysis of related works by including recently published works dealing with semantic image synthesis and contrastive learning.
(2) We propose a novel module, i.e., multi-scale contrastive learning, to push the same-class features from different scales to be similar by using the proposed multi-scale and cross-scale contrastive learning losses.
Equipped with this new module, our ECGAN proposed in \cite{tang2023edge} is upgraded to ECGAN++.
(3) We extend the quantitative and qualitative experiments by comparing our ECGAN and ECGAN++ with the very recent works on three public datasets. Extensive experiments show that the proposed ECGAN++ achieves the best results compared with existing methods.
