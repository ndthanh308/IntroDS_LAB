% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[reprint, amsmath, amssymb, aps, pre,twocolumn,nofootinbib]{revtex4}
%\documentclass[reprint, amsmath, amssymb, aps, prl]{revtex4-2}

%\documentclass[aps,pre,floatfix,epsf,epsfig,nofootinbib,twocolumn]{article} 
%\usepackage[russian]{babel}
%\usepackage{dcolumn}% Align table columns on decimal point
\usepackage[sort&compress]{natbib}
%\usepackage{slashbox}
\usepackage{hyperref} 
\usepackage{url} 
\usepackage{tikz}
\usepackage{bm}
\usepackage[dvips]{epsfig}
\usepackage{graphicx} \usepackage{amsmath} \usepackage{amssymb}
\usepackage{epsfig}
\usepackage{subfigure}
\usepackage{amssymb}
\usepackage[]{amsmath}
\usepackage{IEEEtrantools}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{appendix}
\usepackage{xcolor}

%%\usepackage{slashbox}
%%\usepackage{epigraph}
\newcommand\mybox[2][]{\tikz[overlay]\node[fill=blue!20,inner sep=2pt, anchor=text, rectangle, rounded corners=1mm,#1] {#2};\phantom{#2}}

\newcommand{\comment}[1]{}

\newcommand{\p}{\pi}
\renewcommand{\P}{\pi_{\rm tot}}

\newcommand{\BEQ}{\begin{equation}}
\newcommand{\EEQ}{\end{equation}}
\newcommand{\BEA}{\begin{eqnarray}}
\newcommand{\EEA}{\end{eqnarray}}
\renewcommand{\d}{{\rm d}}
\renewcommand{\t}{\tau}


\usepackage{float}
\newcommand{\nn}{\nonumber \\}
\renewcommand{\d}{{\rm d}}
\newcommand{\1}{$^{(1)}$}
\newcommand{\2}{$^{(2)}$}
\newcommand{\3}{$^{(3)}$}
\newcommand{\4}{$^{(4)}$}
\newcommand{\5}{$^{(5)}$}
\newcommand{\6}{$^{(6)}$}
\newcommand{\7}{$^{(7)}$}
\newcommand{\8}{$^{(8)}$}
\newcommand{\9}{$^{(9)}$}
\newcommand{\0}{$^{(10)}$}
\newcommand{\m}{{\bf{m}}}
\newcommand{\half}{\frac{1}{2}}
\newcommand{\kk}{{\rm k}}
\newcommand{\nk}{{\rm nk}}
%\renewcommand{\t}{\tau}
\def\dbarrm {{\mathchar'26\mkern-11mu{\rm d}}}  
\def\dbarit {{\mathchar'26\mkern-11mud}}            
%\usepackage[review]{acl}
\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}

\begin{document}

\title{Unsupervised extraction of local and global keywords from a single text}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}



\author{Lida Aleksanyan and Armen Allahverdyan \\ Alikhanyan National Laboratory, Alikhanian Brothers Street 2, Yerevan 0036, Armenia}



\begin{abstract}
We propose an unsupervised, corpus-independent method to extract keywords from a single text. It is based on the spatial distribution of words and the response of this distribution to a random permutation of words. As compared to existing methods (such as e.g. YAKE) our method has three advantages. First, it is significantly more effective at extracting keywords from long texts. Second, it allows inference of two types of keywords: local and global. Third, it uncovers basic themes in texts. 
Additionally, our method is language-independent and applies to short texts. The results are obtained via human annotators with previous knowledge of texts from our database of classical literary works (the agreement between annotators is from moderate to substantial). Our results are supported via human-independent arguments based on the average length of extracted content words and on the average number of nouns in extracted words. We discuss relations of keywords with higher-order textual features and reveal a connection between keywords and chapter divisions. 
\end{abstract}

\maketitle

\section{Introduction}

Keyword identification in texts is important for information retrieval, NLP, and discourse analysis \cite{manning,keyness,textual,review4, review1, review2, review3}. Keywords are used as a search tool to identify a group of texts within a larger database. As an analytical tool, keywords reflect the meaning of a text and help to uncover its themes. Our focus will be on this analytical aspect of keywords. 

Keyword identification is challenging, as witnessed by poor results of evaluation metrics for keyword extraction \cite{review1,review2,review3,review4}. A possible explanation is the lack of a sufficiently comprehensive definition of the keyword concept. The general understanding is that keywords are non-polysemic nouns relating to the themes of the text, rather than its rhymes. 

Several approaches for keyword extraction employ linguistic-based handcrafted rules \cite{review4, textrank,hulth}. They lack language independence power and the ability to rank keywords via their relevance. The mathematical approaches fall into two main categories: unsupervised \cite{Ando2005,tfidf1,tfidf2,review4,textrank,topicrank,positionrank,collabrank,tfidf1,tfidf2,rake,yake,matsuo} and supervised \cite{gollapalli, kea, genex, kpspotter}. \href{https://github.com/MaartenGr/KeyBERT}{KeyBERT}, which leverages pre-trained BERT-based embeddings for keyword extraction \cite{bert}, is intermediate between supervised and unsupervised. 

Unsupervised approaches include methods from  statistics, information-theory and graph-based ranking \cite{textrank,topicrank,positionrank,collabrank,tfidf1,tfidf2,rake,yake,matsuo}. 
The best known and widely used statistical approach is perhaps TF-IDF scoring function \cite{manning,Ando2005,tfidf1,tfidf2,review4}. Ideas that are similar to TF-IDF were independently researched in corpus linguistics \cite{textual,keyness}. The method assumes that important words appear frequently in the given text and rarely in other texts in the corpus. Thus, the TF-IDF function relies on the existence of the corpus, i.e. it does not apply to a single text. Other unsupervised methods do apply to a single text. The first such approach was proposed by Luhn \cite{luhn}. It employs Zipf's law for selecting frequent content words as keyword candidates \cite{luhn}. In graph-based methods \cite{textrank, topicrank, positionrank, collabrank,cake} text is represented as a graph where nodes are words and relations between words are expressed by edges. Better connected nodes (as determined by PageRank algorithm) relate to keywords \cite{pagerank}. These methods mainly differ by the principles used to generate edges between words \cite{topicrank}. Graph-based methods need only text information, and hence are corpus independent in contrast to TF-IDF. They can be semantically driven and agnostic of languages \cite{cake}.

Ref.~\cite{spatial5_ortuno} was one of the first attempts to use spatial distribution of words in detecting keywords. In \cite{spatial5_ortuno}, the variance of the spatial distribution is used for ranking keywords. Later works \cite{spatial1, spatial2, spatial3, spatial4, spatial5} suggest several modifications that appear to improve the results; e.g. Ref.~\cite{spatial5} proposes an alternative metric for keyword extraction. It was however unclear how the spatial distribution-based methods are to be implemented in practice, to which texts they apply, how they compare to existing unsupervised single-text methods, and eventually for which keyword-extracting tasks they can be useful. 

These questions are researched and to a large extent answered in the present paper. We use the spatial distribution of words for keyword detection, and our unsupervised and corpus-independent method is based on comparing this distribution before and after a random permutation of words. By doing so, we capture two types of keywords: global and local. Global keywords are spread through the text and the variance of their spatial distribution decreases after a random permutation of words. In contrast, local keywords are localized in certain parts of the text, so that the variance increases after a random permutation. Analyzing several classical texts, we saw that this structural difference between the keywords indeed closely relates to the content of the text; e.g. global and local keywords refer to (resp.) main and secondary characters of the text. Thus, global keywords give the general idea of the text, whereas local keywords focus our attention on some part of the text. We note that the importance of global and local keywords was already understood in linguistics \cite{textual}, but no systematic methods were proposed for their detection. 

Our method provides significantly better efficiency of keyword extraction than several known methods including LUHN \cite{luhn} and YAKE \cite{yake,yake!}. (We noted that despite its relative sophistication, for single-word keywords (i.e.not key-phrases) YAKE provides results that always approximately coincide with those of LUHN, though it outperforms graph-based methods; hence, we do not show comparison results with the latter group of methods.) The superiority of our method is found via human annotators who determine if the extracted words are keywords based on their previous knowledge of classic literature texts in our database. There is moderate to substantial agreement between annotators. Additionally, we gave two human-independent indications of the advantage of our method over LUHN and YAKE. First, extracted words have a longer length (in letters) than English content words on average. Therefore, we can infer indirectly that our method extracts text-specific words. Second, our method extracts more nouns. This is a proxy for keyword extraction, since keywords are mostly nouns. 

In contrast to LUHN and YAKE, our method allows to uncover themes of the text. Our method is also nearly language-independent, as verified using translations in three languages: English, Russian and French. It is only for long texts that our keyword extraction method is more efficient. For short texts our method does apply, but its efficiency of keyword extraction is similar to those of LUHN and YAKE. Still its power in uncovering the textual themes remains even for short texts, as shown in Appendix \ref{hd}.   

We aimed to understand a keyword's conceptual meaning by relating it to a text's higher-order structures, i.e. to the fact that sufficiently long literary texts are divided into chapters. This was accomplished by developing a method of keyword extraction that is based on chapter division. Even though this method is less efficient than our main method, it is easier to use in practice (for texts that already have many chapters), and it has the potential for further development. 

The rest of the paper is organized as follows. In section \ref{method}, we discuss the main method analyzed in this work. Section \ref{anna} shows how the method applies to a classic and well-known text: {\it Anna Karenina} by L. Tolstoy. Section \ref{more} discusses further texts of our database, and shows how the method should be modified in application to short texts. The inter-annotator agreement is also discussed in this section. Section \ref{chapters} is devoted to the keyword extraction method that employs the fact that a long text is divided over sufficiently many chapters. The final section summarizes the discussion. Here we emphasize that we considered only single-word keywords, and the extension of our method to extraction of key-phrases is an open problem. 

\section{Method}
\label{method}

\subsection{Spatial distribution of words}

\comment{
Spatial distribution of words along the text is studied in \cite{spatial1, spatial2, spatial3, spatial4, spatial5, spatial5_ortuno, yngve, carpena, zano}. For a given text with $n$ different words, we extract their frequencies:
\BEA
\label{w1} \{f(w_r)\}_{r=1}^{n}, \qquad
{\sum}_{r=1}^{n} f(w_r) =1. 
\EEA
}


Our texts were lemmatized and freed from functional words (stop-words); see Appendix \ref{lemma_ap} for details.
Let $w_{[1]},...,w_{[\ell]}$ denote all occurrences of a word
$w$ along the text. Let $\zeta_{i}$ denotes the number of words
(different from $w$) between $w_{[i]}$ and $w_{[i+1]}$; i.e. 
$\zeta_{\,i}+1\geq 1$ is the number of space symbols between 
$w_{[i]}$ and $w_{[i+1]}$. Define the average period $t(w)$ of this word $w$, and the spatial frequency $\tau(w)$ via \cite{yngve,halves}:
\BEA
\label{durnovo}
&& t(w)=\frac{1}{\ell-1}{\sum}_{i=1}^{\ell-1} \,(\zeta_{\,i}+1),\\
&& \tau(w)\equiv 1/t(w).
\EEA
Eq.~(\ref{durnovo}) is not defined for $\ell=1$, i.e. for words that occur only once; hence such words are to be excluded from consideration. Note that $(\ell-1)(t(w)-1)$ equals to the number of words that differ from $w$ and occur between $w_{[1]}$ and $w_{[\ell]}$. Hence $t(w)$ will stay intact under redistributing $w_{[2]},...,w_{[\ell-1]}$ for fixed $w_{[1]}$ and $w_{[\ell]}$. We expect that a random permutation of all words in the text will leave $t(w)$ nearly intact for frequent words, and will increase it for not frequent words. These expectations are confirmed empirically; see Fig.~\ref{annafinn}. 

It is interesting to compare $\tau(w)$ with the ordinary frequency $f(w)$ of word $w$ \cite{halves}. Recall that it is defined as 
\BEA
\label{ordinary}
f(w)=N_w/N, 
\EEA
where $N_w$ is the number of times $w$ appeared in the text, while $N$ is the full number of words in the text. Now $f(w)$ is invariant under any permutation of words. It appears that natural texts hold the following relation for the majority of words $w$:
\BEA
\tau(w)>f(w),
\label{dag}
\EEA
Appendix \ref{cuba} explains the origin of (\ref{dag}); cf.~Fig.~\ref{annafinn}. 

% Figure environment removed

Given the average (\ref{durnovo}), let us define also the variance of the spatial period for word $w$ \cite{spatial1, spatial2, spatial3, spatial4, spatial5, spatial5_ortuno, yngve, carpena, zano}:
\begin{equation}
\label{3}
{\rm var}(w)=\frac{1}{\ell-1}{\sum}_{i=1}^{\ell-1} \,(\zeta_{\,i}+1-t(w))^2.
\end{equation}
This quantity is already not invariant with respect to word permutations. Using (\ref{3}), we define
\begin{equation}
\label{37}
{A(w)}=\frac{{\rm var}_{\rm perm}(w)}{{\rm var}(w)},
\end{equation}
where ${\rm var}_{\rm perm}(w)$ is calculated via (\ref{3}) but after a random permutation of all words of the text.


\subsection{Keyword extraction method}
\label{kkk}

When checking the values of $A(w)$ for all distinct words of several texts, our annotators concluded that sufficiently small and sufficiently large values of $A(w)$ in (\ref{37}), 
\BEA
\label{40}
&& A(w)\leq \frac{1}{5},\\
&& A(w)\geq 5,
\label{41}
\EEA
can be employed for deducing certain keywords of the text. Eq.~(\ref{40}) uncovers global keywords of the text, i.e. keywords that go through the whole text. To understand this result heuristically, let us look at Fig.~\ref{annafinn} which presents a well-known text: {\it Anna Karenina} by L. Tolstoy \cite{anna}. Here the words are arranged with respect to decreasing frequency (\ref{ordinary}). We checked that global keywords appear as local minima of $1/{\rm var}(w)$; cf.~(\ref{3}). These local minima do not survive a random permutation leading to a small value of $A(w)$ in (\ref{40}); see Fig.~\ref{annafinn}. Taking a smaller value 
\BEA
\frac{1}{5}\leq A(w)\leq \frac{1}{3}, 
\label{ade}
\EEA
in (\ref{40}) leads to selecting a group of lower-frequency global keywords. 

Likewise, (\ref{41}) refers to local keywords, i.e. those that appear in specific places of the text. In Fig.~\ref{annafinn} they are seen as local maxima of $1/{\rm var}(w)$. In contrast to local minima, maxima are located in the domain of infrequent words, as it should be for local keywords. Local maxima also disappear after a random permutation. Hence $A(w)$ in (\ref{40}) assumes a larger value. 

These relations of (\ref{40}) and (\ref{41}) with (resp.) global and local keywords make intuitive sense. As we checked in detail, spaces between global keywords assume a broad range of values. This distribution becomes more uniform after the random permutation, hence the variance decreases; cf.~(\ref{40}). Local keywords refer to infrequent words and are localized in a limited range of text. Hence a random permutation obviously increases the dispersion, as implied by (\ref{41}).

As our method relies on random permutations, our results are formally dependent on the realization of these permutations. (Random permutations of words were generated via Python's numpy library \cite{permutation}.) Such a dependence is weak: we noted that only a few keywords change from one realization to another. However, we cannot avoid random permutations. In particular, we cannot rely on theoretical models of a random text; see e.g. \cite{spatial1,spatial3}. In a long text, the probability $p(s)$ for spaces $\zeta_{\,i}(w)=s$ [c.f.~(\ref{durnovo})] of a word $w$ after a random permutation is asymptotically geometrical, $p(s)=(1-f(w))f^s(w)$ ($s\geq 0$) provided that its frequency $f(w)$ is big enough; cf.~(\ref{ordinary}). But for the majority of keywords this asymptotic limit is not reached, since $f(w)$ is not big. 

\comment{%%To be employed
Ref.~\cite{spatial1} suggested to calibrate the variance by taking the ratio of standard deviations of original and random texts justifying this approach by the fact that the variance contains the frequencies of words, hence the need to eliminate this dependence. Here the problem is to find a good model for a random version of original text. The authors suggest to use geometric distribution for $\zeta_{\,i}-$s where the probability of success (occurrence of a specific word $f(w_r)$) is taken as its frequency in original text $f(w_r)$. We see that this intuition does not really work. Alternative versions of a randomly permuted text is described also in \cite{spatial3}. }

\section{Keywords extracted from {\it Anna Karenina}}
\label{anna}

The above keyword extraction method was applied to several texts of classic literature; see Table~\ref{tab_gogo}. (Our data for texts from Table~\ref{tab_gogo} is freely available \cite{our_data}.) Among them, we choose one of the most known works of classic literature, {\it Anna Karenina} by L. Tolstoy, and analyzed in detail the implications of our method in uncovering and interpreting its keywords. The evaluation of extracted keywords was done by annotators with expert knowledge of classic Russian literature and specifically works by Tolstoy. 

\subsection{Comparison with known methods of keyword extraction and language independence}

Using {\it Anna Karenina} \cite{anna}, we compared our approach discussed in section \ref{kkk} with two well-known methods that also apply to a single text (i.e. do not require corpus): LUHN \cite{luhn} and YAKE \cite{yake,yake2}; see also \cite{yake!} that discusses advantages of YAKE with respect to several other methods. YAKE was implemented via Python package \cite{yake_pip}. 
LUHN employs Zipf's law and can have several implementations \cite{luhn}. We followed the implementation that was employed for testing Zipf's law itself \cite{zipf_pre}; see Appendix \ref{lemma_ap} for details. 

-- 282 words were extracted via each method. Then keywords were identified using our general expertise on classic Russian literature. Table~\ref{tab_lang} shows that for three languages (English, Russian, French) our method is better in terms of the percentage of extracted keywords. The relatively poor performance of YAKE and LUHN can be explained via the fact they focus on relatively short content words that are not likely to be keywords. We quantified this by calculating the mean number of letter in each set of 282 words. For our method, LUHN and YAKE the mean is (resp.) 6.95, 5.43 and 5.5; cf.~the fact that the average number of letters in English content word is 6.47 (for stop word it is 3.13) \cite{miller}.

-- The three methods have scores for words. In LUHN and our method the score coincides with the word frequency. However, for LUHN and YAKE the score did not correlate with the feature of being keyword. For our method it certainly did, i.e. by selecting only high-score words we can significantly enlarge the percentage of keywords compared to what is seen in Table~\ref{tab_gogo}. These two facts (low density of keywords plus no correlation with their score) make impossible to extract thematic groups of keywords via LUHN and YAKE; cf.~the discussion after (\ref{41}). 

-- Another comparison criteria between the three methods is the amount of nouns in words that were not identified as keywords. This criterion is a proxy for the difficulty of identifying keywords, which are known to be mostly nouns. Our method again fares better than both LUHN and YAKE; see Table~\ref{tab_lang}. 

Table~\ref{tab_lang} also addresses the language independence of the three methods that were studied in three versions (English, Russian and French) of {\it Anna Karenina}. It is seen that our method performs comparably for English and Russian, which are morphologically quite distinct languages. For French the performance is worse, but overall still comparable with English and Russian. Altogether, our method is language-independent.  

\subsection{Thematic groups uncovered via extracted keywords}

Recall that {\it Anna Karenina} is a classic novel with more than 800 pages, which features more than a dozen major characters and many lesser characters. Keywords were separated into 9 thematic groups: proper names of major characters; proper names of secondary characters; animal names; trains and railway; hunting; rural life and agriculture; local governance; nobility life and habits; religion; see Table~\ref{tab_groups}. 

The names of these characters are keywords, because they inform us about the character's gender ('anna' {\it vs.} 'vronsky'), age ('alexandrovitch' {\it vs.} 'seryozha') and the social strata; e.g. 'tit' {\it vs.} 'levin'. Proper nouns provide additional information due to name symbolism employed by Tolstoy; e.g. 'anna'='grace'; 'alexey'='reflector'; 'levin'='leo' is the {\it alter ego} of Tolstoy \cite{gustafson}. 

All the main character names came out from our method as strong global keywords holding condition $A(w)\leq \frac{1}{5}$ in (\ref{40}): 'levin', 'anna', 'vronsky', 'kitty', 'alexey', 'stepan', 'dolly', 'sergey'; see Table~\ref{tab_groups} for details. Many pertinent lesser characters came out as local keywords, as determined via condition (\ref{41}); e.g. 'vassenka', 'golenishtchev', 'varvara'; see Table~\ref{tab_groups}. Important characters that are not the main actors came out as weak global keywords, e.g. 'seryozha', 'yashvin', 'sviazhsky'.

The novel is also known for its animal characters that play an important role in Tolstoy's symbolism \cite{gustafson}. Our method extracted as local keywords the four main animal characters: 'froufrou', 'gladiator' 'laska', 'krak'. Trains are a motif throughout the novel (they symbolize the modernization of Russia), with several major plot points taking place either on passenger trains or at stations in Russia \cite{anna,gustafson}. Our method extracted among the global keywords 'carriage', 'platform' and 'rail'. Hunting scenes are important in the novel depicting the life of Russian nobility. Accordingly, our method uncovered keywords related to that activity: 'snipe', 'gun', 'shoot'. Two major social themes considered in the novel are local democratic governance (Zemstvo) and the agricultural life of by then mostly rural Russia. For the first we extracted keywords:  'district', 'bailiff', 'election' {\it etc}. And for the second: 'mow', 'lord', 'acre', {\it etc}. A large set of keywords are provided by Russian nobility's living and manners, including their titles, professions and habits; see Table~\ref{tab_groups}. Religion and Christian faith is an important subject of the novel. In this context, we noted keyword 'Lord', 'priest', 'deacon'; see Table~\ref{tab_groups}. Finally, a few words stayed out of these thematic groups but was identified as keywords: 'lesson', 'crime', 'cheat', 'salary', 'irrational', 'law', 'skate', 'tribe'. 

\comment{ 'princess', 'prince', 'countess', 'madame', 'officer', 'telegram', 'bedroom', 'nobleman', 'article', 'professor', 'sportsman', 'armchair', 'director', 'minister', 'captain', 'matrona', 'duel', 'mazurka', 'villa', 'bridgeroom', 'photograph', 'architect', 'pa', 'mais', 'bedchamber', 'opium', 'palazzo', 'crown', 'adultery', 'musical'.}


\begin{table*}\caption{Analyzed long texts: {\it Anna Karenina}, {\it War and Peace, part I}, and {\it War and Peace, part II} by L. Tolstoy; {\it Master and Margarita} by M. Bulgakov; {\it Twelve Chairs} by I. Ilf and E. Petrov; {\it The Glass Bead Game} by H. Hesse; {\it Crime and Punishment} by F. Dostoevsky. Shorter texts: {\it The Heart of Dog} by M. Bulgakov; {\it Animal Farm} by G. Orwell. {\it Alchemist} by  P. Coelho. Next to each text we indicate the number of words in it, stop-words included. \\ For long texts we extracted for each text 
the same number of $\approx 300$ potential keywords via each method: our method (implemented via (\ref{40}, \ref{41}, \ref{ade})), LUHN and YAKE. The precise number of extracted words depends on the text. The percentages below show the fraction of keywords that were identified as keywords by human annotators. For short texts, we extracted via each method the same number of $\sim 100$ words, while our method was implemented via (\ref{sharik}). \\  
For longer texts our method provides sizable advantages compared with LUHN and YAKE. For shorter texts the three methods are comparable. 
}
\begin{tabularx}{1\textwidth} 
{ 
%  | >{\raggedright\arraybackslash}X 
%  | >{\raggedleft\arraybackslash}X |
  | >{\raggedleft\arraybackslash}X |
  | >{\raggedleft\arraybackslash}X |
  | >{\raggedleft\arraybackslash}X |
  | >{\raggedleft\arraybackslash}X |}
 \hline
 Text/Method & LUHN & YAKE & Our method\\
 \hline
 {\it Anna Karenina} (349762) & 15.6\% & 15.6\% & 55.6\% \\
  \hline
 {\it War and Peace, part I} (142254) & 23\% & 23\% & 43\% \\
\hline
 {\it War and Peace, part II} (128146) & 28\% & 27\% & 51\% \\
 \hline
 {\it Master and Margarita} (145286) & 19\% & 18\% & 56\% \\
 \hline
 {\it Twelve Chairs} (102485) & 20\% & 20\% & 40\% \\
 \hline
 {\it The Glass Bead Game} (192311) & 18\% & 19\% & 34\% \\
 \hline
 {\it Crime and Punishment} (203505) & 30\% & 19\% & 30\% \\
 \hline\hline
 {\it The Heart of Dog} (34950)   & 23\% & 23\% & 26\% \\
 \hline
 {\it Animal Farm} (30037)   & 50\% & 47\% & 46\% \\
 \hline
 {\it Alchemist} (39004) & 32.5\% & 33\% & 29.5\% \\
 \hline
\end{tabularx}
\label{tab_gogo}
\end{table*}

%\begin{center}
\begin{table}\caption{The values of Cohen's kappa (\ref{kappa1}) for the agreement in keyword extraction tasks between two annotators for three different texts; see section \ref{inter2}. The keyword extraction employed the method discussed in Table~\ref{tab_gogo} and section \ref{kkk}. Results for global and local keywords are shown separately. It is seen that the agreement is better for global keywords. A possible explanation is that the annotators do not focus on details in the texts. }
\begin{tabular}{ |c|| c| c| }
 \hline
 Text/type of keyword & global & local \\ 
 \hline
{\it Animal Farm} & 0.68 & 0.40  \\  
\hline
{\it Alchemist} & 0.83 & 0.53   \\
\hline
{\it Master and Margarita} & 0.78  & 0.65 \\
  \hline
\end{tabular}
\label{tab_inter}
\end{table}
%\end{center}

\begin{table*}\caption{Words of {\it Anna Karenina} extracted via our method. For global keywords strong and weak cases mean (resp.) that the words $w$ were chosen according to $A(w)\leq \frac{1}{5}$ and $\frac{1}{5}\leq A(w)\leq \frac{1}{3}$; cf.~(\ref{40}, \ref{ade}). Local keywords were chosen according to $A(w)\geq {5}$; see (\ref{41}). For each column, the words were arranged according to their frequency (\ref{ordinary}). \\
Keyword classes are denoted by upper indices. \1 proper names of major characters; \2 proper names of secondary characters; \3 animals; \4 trains and railway; \5 hunting; \6 rural life and agriculture; \7 local government (zemstvo); \8 nobility life and habits; \9 religion. The last group \0 denotes words that were identified as keywords, but did not belong to any of the above groups. Words without the upper index were not identified as keywords. }

\begin{tabularx}{\textwidth} 
{ 
  | >{\raggedright\arraybackslash}X 
  | >{\raggedright\arraybackslash}X |
   | >{\raggedright\arraybackslash}X | }
 \hline
 Global keywords strong cases & 
  Global keywords weak cases & 
   Local keywords \\ \hline
  levin\1, anna\1, vronsky\1, kitty\1, alexey\1, stepan\1, alexandrovitch\1, arkadyevitch\1, dolly\1, sergey\1, ivanovitch\1, peasant, darya\1, alexandrovna\1, varenka\1, lidia\1, death, ivanovna\1, laborer\6, mow\6, district\7, stahl\1, bailiff\5, gun\5, snipe\5, plough\6, rain, lesson\0, lord\9, acre\6, platform\4, natalia\1, built, rich, overlook, river, crime\0, rail\6, relate, throb, contrast, puzzle, cheat\0, oppress, irrational\0 
   &
love, princess\8, brother, carriage\4, horse\8, prince\8, doctor\8, countess\8, madame\8, sviazhsky\1, land\6, seryozha\1, konstantin\1, picture, oblonsky\1, nikolay\1, agafea\2, katavasov\2, grass\6, yashvin\1, shoot\5, mihalovna\2, officer\8, box, marshal\7, mare\6, priest\9, tree\6, forest\6, laska\3, law\0, landowner\6, realize, scythe\6, telegram\8, meadow\6, bedroom\8, argument, sledge, nobleman\8, paint, article\8, professor\8, scream, sky, trap, birch\6, cow\6, debt\0, rent, punish, sow\6, annushka\2, lightly, sportsman\8, myakaya\2, invalid, smart, parent, vividly, maman\8, institution\7, stable, distance, salary\0, educate, firm, skirt, mahotin\2, reconciliation, yellow, plump, childrens, tatar\2, outer, steward\8, cousin, loathsome, sharp, splash, armchair\8, understands, coarse, quicken, grace, delicious, director\8, unseen, selfpossession, cheese, rate, physically, timidity, tucked, reassure, sunday, compartment, frost, minister\8, won, king, repent, clock, wage, shock, uncertain, deliver, cream, silently, monday, captain\8, shaft\6, matrona\8, strictly, original 
&
 vassenka\2, golenishtchev\2, election\7, skate\0, varvara\2, pyotr\2, lizaveta\2, landau\2, petrovna\2, gladiator\3, metrov\2, tit\2, vote\7, froufrou\3, ryabinin\2, volunteer\8, nevyedovsky\2, duel\8, scandal\8, tribe\0, snetkov\2, lukitch\2, mower\6, deacon\9, native, korsunsky\2, hospital, remote, mazurka\8, pilate\0, sappho\0, villa\8, rival, reed\6, bridegroom\8, krak\3, merkalova\2, vorkuev\2, photograph\8, yegor\2, mitya\2, kapitonitch\2, architect\8, intensely, elect\7, golenishtchevs\2, pa\8, birthday, trousseau\8, transition, chalk, potato\6, kritsky\2, ergushovo\6, katya\2, weep, sympathetic, repair, mais\8, seryozhas\2, ballroom\8, classical, vozdvizhenskoe\6, technique, bedchamber\8, opium\8, penetrate, tchirikov\2, rider, palazzo\8, crown\8, remove, miracle, intolerable, turk\2, ballot\7, custom, nevsky\8, adultery\8, ditch, musical \\
\hline
\end{tabularx}
\label{tab_groups}
\end{table*}


\begin{table*} 
\caption{Comparison of 3 different keyword extraction methods for English, Russian and French version of {\it Anna Karenina}. Here ``nouns'' means the percentage of nouns in non-keywords. For all cases our method fares better than LUHN and YAKE, whose performances are comparable.  }
\begin{tabularx}{1\textwidth} 
{ 
  | >{\raggedright\arraybackslash}X |
  | >{\raggedleft\arraybackslash}X 
  | >{\raggedleft\arraybackslash}X |
  | >{\raggedleft\arraybackslash}X 
  | >{\raggedleft\arraybackslash}X |
  | >{\raggedleft\arraybackslash}X 
  | >{\raggedleft\arraybackslash}X |
  }
 \hline
 Method & English keywords & English nouns & Russian keywords & Russian nouns & French keywords & French nouns    \\
 \hline
 LUHN & 15.6 \% & 54 \%  & 14.1\%  & 51.1\%  & 19.2\%  & 62.3 \% \\
\hline
 YAKE & 15.6 \% & 55 \%  & 14.8\%  & 49.2\%  & 18\% & 60 \%  \\
\hline
Our method & 55.6 \%  & 82 \% & 55\%   & 86.2\%   & 50.7\%  & 77.3\%  \\
\hline
\end{tabularx}
\label{tab_lang}
\end{table*}

\comment{

\begin{table*}\caption{Comparison of 3 different keyword extraction methods for English version of {\it Anna Karenina}}
\begin{tabularx}{1\textwidth} 
{ 
  | >{\raggedright\arraybackslash}X 
  | >{\raggedleft\arraybackslash}X |
  | >{\raggedleft\arraybackslash}X |
  | >{\raggedleft\arraybackslash}X |
  | >{\raggedleft\arraybackslash}X |
  | >{\raggedleft\arraybackslash}X |}
 \hline
 Method & 
Number of extracted words & keywords/words & nouns(in non keyword set)/words & verbs(in non keyword set)/words & adj+adverbs(in non keyword set)/words\\
 \hline
 LUHN & 
282 & 10.6 & 42.2 & 34.4 & 12.8 \\
\hline
 YAKE &
 282 & 11.3 & 41.8 & 32.3 & 14.6 \\
\hline
OUR METHOD &
282 & 31.6 & 48 & 9.4 & 11 \\
\hline
\end{tabularx}
\label{table_english}
\end{table*}

\begin{table*}\caption{Comparison of 3 different keyword extraction methods for Russian version of {\it Anna Karenina}}
\begin{tabularx}{1\textwidth} 
{ 
  | >{\raggedright\arraybackslash}X 
  | >{\raggedleft\arraybackslash}X |
  | >{\raggedleft\arraybackslash}X |
  | >{\raggedleft\arraybackslash}X |
  | >{\raggedleft\arraybackslash}X |
  | >{\raggedleft\arraybackslash}X |}
 \hline
 Method & 
Number of extracted words & keywords/words & nouns(in non keyword set)/words & verbs(in non keyword set)/words & adj+adverbs(in non keyword set)/words\\
 \hline
 LUHN & 
276 & 10.5 & 40 & 35.5 & 14 \\
\hline
 YAKE &
 276 & 10.5 & 38.4 & 35.5 & 15.6 \\
\hline
OUR METHOD &
276 & 31 & 52 & 8 & 9 \\
\hline
\end{tabularx}
\label{table_russian}
\end{table*}
}

\subsection{Comparison with topic modeling methods}

Above we described how human annotators can deduce thematic groups of {\it Anna Karenina} using keywords extracted by our method. The same task -- known as topic modeling -- is achieved algorithmically (i.e.without human intervention) by several known NLP methods \cite{nnmf,svd,scikit,lda}. Now we compare their efficiency with our results.

Non-Negative Matrix Factorization (NNMF) is an unsupervised method of topic modeling that applies to a single text \cite{nnmf}. Its implementation for {\it Anna Karenina} produced several potential topics (their number is a hyper-parameter of the model). Only one of these topics was reliable and approximately coincided with the first thematic group (proper names of major characters) discussed above; see Table~\ref{tab_groups}. This is not surprising, since this thematic group contains the most frequent content words of the text. Other topics extracted by NNMF turned out out to be meaningless, i.e. they do not correspond to any thematic group of the text; see Appendix \ref{nnmf} for details.

A similar situation (only one sensible topic is recovered) was observed with other topic modeling methods: Truncated SVD \cite{svd} and LDA (Latent Dirichlet Allocation) \cite{lda}. Truncated SVD applied to a single text. LDA was attempted both for a single text and a group of $\approx 100$ long texts. In contrast to NNMF, both Truncated SVD and LDA gave priority to this single topic. Note that all 3 methods were applied without supervision.



\section{More texts}
\label{more}

Results obtained for {\it Anna Karenina} are confirmed for a number of other texts; cf.~Table~\ref{tab_gogo}. For long texts (with the length roughly comparable with {\it Anna Karenina}) our method definitely outperforms both LUHN and YAKE in terms of the relative number of extracted keywords; see Table~\ref{tab_gogo}. In this context, we need to distinguish between long and short texts; cf.~Table~\ref{tab_gogo}. For short texts our method needs modifications, since the criteria (\ref{40}, \ref{41}) applied minima of $A(w)$ in (\ref{37}) are not sufficiently powerful for discriminating between the keywords and ordinary words. We found two nearly equivalent modifications of the method that applies to short texts. The first option is to look at local maxima and minima of $A(w)$. The second, more streamlined option is to modify the order of the variance in (\ref{3}). Instead of the usual variance in (\ref{3}) we employed the six-order variance 
\BEA
\label{66}
{\rm var}_6(w)=\frac{1}{\ell-1}{\sum}_{i=1}^{\ell-1} \,(\zeta_{\,i}+1-t(w))^6.
\EEA
Now $A_6(w)$ is defined analogously to (\ref{37}), but via (\ref{66}), 
\BEA
\label{dog}
{A_6(w)}=\frac{{\rm var}_{6\, {\rm perm}}(w)}{{\rm var}_6(w)},
\EEA
and for uncovering keywords we can apply [cf.~(\ref{40}, \ref{41})]:
\BEA
A_6(w)\leq \frac{1}{3}, ~~~ A_6(w)\geq 3.
\label{sharik}
\EEA
After this modification, our method implemented via (\ref{sharik}) produces for short texts nearly the same results as LUHN and YAKE; see Table~\ref{tab_gogo}. However, our method still has an important advantage, since it allows us to uncover thematic groups of short texts nearly in the same way as for long texts; see Appendix \ref{hd} where we analyze thematic groups of {\it The Heart of Dog} by M. Bulgakov. We emphasize that this feature is absent for LUHN and YAKE.  

\subsection{Inter-anontator agreement}
\label{inter2}

The performance of any keyword extraction method is evaluated by annotators. First, annotators should be provided with guidelines on the extraction process; e.g. characters are keywords, pay more attention to nouns and less to verbs and adjectives {\it etc}. Second, two (or more) annotators are independently given the set of keywords extracted by our algorithm from the same set of texts, and they mark words that they consider as keywords. So each annotator will get at the end a list of keywords {\it versus} non-keyword. Annotators are influenced by various subjective factors: background, prior knowledge, taste {\it etc}. However, the situation will not be subjective if different annotators produce similar results. To quantify the agreement between annotators we employed Cohen's kappa $\kappa$; see Table~\ref{tab_inter}. This statistical measure is used to assess inter-annotator agreement when working on categorical data in linguistics, psychology, and information retrieval; see \cite{cook_kappa} for review. It accounts for chance agreement and provides a more robust evaluation of agreement than the simple percentage. Cohen's $\kappa$ reads
\BEA
\label{kappa1}
&& \kappa=\frac{p_o-p_e}{1-p_e},~~ -1\geq \kappa\geq 1,\\
&& p_o=p(A=\kk,B=\kk)+p(A=\nk,B=\nk),\\
&& p_e=p(A=\kk)p(B=\kk)+p(A=\nk)p(B=\nk),~~
\EEA
where $p(A=\kk,B=\kk)$ is the joint probability for annotators $A$ and $B$ to identify keyword, $p(A=\nk,B=\nk)$ is the same for non-keywords (denoted by $\nk$), $p(A=\kk)$ is the marginal probability {\it etc}. 
Hence, $p_o$ is the agreement probability, while $p_e$ is the probability to agree by chance. Now $\kappa\to p_o$ for $p_e\to 0$, while for $p_e\lesssim 1$ even a relatively small, but positive difference $p_o-p_e$ is sufficient for $\kappa\to 1$.

Numerical interpretation of $\kappa$ is as follows \cite{cook_kappa}. No agreement: $\kappa<0$. Slight agreement: $0.2>\kappa>0$. Fair agreement: $0.4>\kappa>0.2$. Moderate agreement: $0.6>\kappa>0.4$. Substantial agreement: $0.8>\kappa>0.6$. Following these steps we got an agreement, which is between moderate and substantial both for short and long texts; see Table~\ref{tab_inter}. The agreement is expectedly better for global keywords. 

\section{Keyword extraction and distribution of words over chapters}
\label{chapters}

\begin{table*}\caption{First column: 36 words from {\it Anna Karenina} that have the highest score of YAKE \cite{yake,yake!}. Keywords are indicated by the number of their group; see Table \ref{tab_groups}. Among 36 words there are 25 non-keywords. Keywords refer mostly to group \1.\\
Second column: 36 words of {\it Anna Karenina} extracted via looking at distribution of words over chapters, i.e. at the largest value of (\ref{9}). Only 2 words out of 36 are not keywords. Several keyword groups are represented. }
\begin{tabularx}{1\textwidth} 
{ 
  | >{\raggedright\arraybackslash}X 
  | >{\raggedleft\arraybackslash}X | }
 \hline
 36 words having largest score of YAKE  &
 36 words having largest values of (\ref{9}) \\
 \hline 
levin\1, anna\1, vronsky\1, alexey\1, kitty\1, stepan\1, 
hand, alexandrovitch\1, smile, thought, arkadyevitch\1, time, 
love, face, eye, felt, man, feel, 
talk, life, answer, day, wife, begin, 
long, knew, turn, child, sergey\1, husband, 
work, princess\8, room, ivanovitch\1, people, woman 
&
levin\1, alexey\1, alexandrovitch\1, varenka\2, vronsky\1, kitty\1, doctor\8, stepan\1, scythe\6, anna\1,
arkadyevitch\1, marsh\6, countess\8, katavasov\2, priest\9, darya\1, veslovsky\2, alexandrovna\1, seryozha\1,
mare\6, sviazhsky\2, mihailov\2, brother, dolly\1, grass\6, sergey\1, princess\8, mow\6, marshal\7, konstantin\2, ivanovitch\2, peasant\6, lidia\1, sick, petritsky\2 \\ 
\hline
\end{tabularx}
\label{tab_h}
\end{table*}


Long texts are frequently divided into sufficiently many chapters. It is an interesting question whether this fact can be employed as an independent criterion for extracting keywords. To search for such criteria, let us introduce the following basic quantities. Given a word $w$ and chapters $c=1,..,N_{\rm chap}$ we define $m_w(c)\geq 0$ as the number of times $w$ appeared in chapter $c$. Likewise, let $V_w(s)$ be the number of chapters, where $w$ appeared $s\geq 0$ times; i.e. $\sum_{s\geq s_0}V_w(s)$ is the number of chapters, where $w$ appears at least $s_0$ times. We have 
\BEA
&& {\sum}_{c=1}^{N_{\rm chap}}m_w(c)=N_w,\\
&& {\sum}_{s\geq 0}sV_w(s)=N_w,
\EEA
where $N_w$ is the number of times $w$ appears in the text; cf.~(\ref{ordinary}). Hence, 
when taking a random occurrence of word $w$, we shall see $w$ appearing in chapter $c$ with probability
$m_w(c)/N_w$. Likewise, $sV_w(s)/N_w$ is the probability $w$ will appear in a chapter, where $w$ is encountered $s$ times.

It appears that quantities deduced from $m_w(c)/N_w$ do not lead to useful predictions concerning keywords. In particular, this concerns the entropy $-\sum_{c=1}^{N_{\rm chap}}\frac{m_w(c)}{N_w}\ln \frac{m_w(c)}{N_w}$ and correlation function $\sum_{c_1,c_2=1}^{N_{\rm chap}} |c_1-c_2|m_w(c_1)m_w(c_2)$ together with some of its generalizations. In contrast, the following mean
\BEA
\label{9}
{\sum}_{s\geq 0}\frac{s^2V_w(s)}{N_w},
\EEA
related to $sV_w(s)/N_w$ predicts sufficiently many global keywords; see Table~\ref{tab_h}. Similar results are found upon using the entropy $-{\sum}_{s\geq 0}\frac{sV_w(s)}{N_w}\ln \frac{sV_w(s)}{N_w}$ instead of (\ref{9}). Eq.~(\ref{9}) is calculated for each word and then words with largest value of (\ref{9}) are selected. For {\it Anna Karenina}, at least the first 35-36 words selected in this way are keywords. Minor exclusions are seen in Table~\ref{tab_h}, which also shows that this method is much better than YAKE both in quantity and quality of keyword extraction. The advantage of this chapter-based method is that it does not depend on random permutations. Hence, it will be easier in practical implementations. The drawbacks are seen above: it depends on the existence of sufficiently many chapters (hence it certainly does not apply to texts with a few or no chapters), and it addresses only some of the keywords. 

\comment{
$V_w(s)$ effectively appears in scientometry: the word $w$, chapters of the text, and $V_w(s)$ can be mapped to (resp.) a scientist, papers he/she produced, and the number of citations each paper got \cite{index}. Using this analogy, one can define for a word $w$ its h-index $h_w$: $w$ appears $h_w$ times in at most $h_w$ chapters \cite{index}. A bigger $h_w$ means that $w$ appears more in a larger number of chapters. However, when it comes to uncovering keywords, $h_w$ is less useful than (\ref{9}).
}



% Using the analogy back, it may appear that (\ref{9}) are better in evaluating scientists. 
% which may not be surprising given its weak (but uncritically popular) performance on evaluating 
% scientists. 

\section{Summary and conclusion}

We proposed a method for extracting keywords from a single text. The method employs a spatial structure in word distribution. Our unsupervised method extends previous proposals, applies to a single text (i.e. it does not need databases), and demonstrates two pertinent applications: uncovering the main themes of the text and separating between local and global keywords. For long texts, our analysis confirms that such a separation is semantically meaningful. The method was illustrated in several classic literature texts. In researching the performance of our method we relied on expert evaluation of keywords (that show from moderate to substantial inter-annotator agreement) and were able to extract themes of text by looking at keywords. In particular, we focused on the analysis of themes of {\it Anna Karenina} and {\it Heart of Dog}. 

%The semantic difference between local and global keywords is blurred for short texts. 

%Given how much we were able to achieve via keywords, one would expect even more advanced results from key phrases. 

Our method outperforms several existing methods for keyword extraction, such as LUHN \cite{luhn} and YAKE \cite{yake,yake!} (in its turn the later method outperforms graph-based methods). These methods also apply to a single text, and they do not require a text database; i.e. they are not of TF-IDF type \cite{review4}. LUHN is one of the oldest methods for keyword extraction. It uses the known fact that content words of a single text hold Zipf's law \cite{luhn}. YAKE is a more recent and rather popular method \cite{yake,yake!}, though (as we witnessed) its performance does not differ much from LUHN at least when extracting single keywords.   

For sufficiently long texts, the superiority of our method with respect to LUHN and YAKE is both quantitative (since it extracts $2-3$ times more words than LUHN and YAKE), and qualitative, because LUHN and YAKE do not uncover the themes, do not distinguish between local and global keywords, and do not have efficient ranking of keywords. For shorter text only one advantage persists: our method helps to extract thematic groups; see Table~\ref{tab_gogo} and Appendix \ref{hd}. 

We confirm that our method generally extracts more nouns and longer content words than YAKE and LUHN. There is generally a correlation between both of these features and being a keyword. Our method is also language-independent, as we checked with several translations of the same text. It shares this advantage with LUHN and YAKE. 

We demonstrated that our method of identifying thematic groups of texts (where human annotators employ keywords extracted by our method) produced significantly better results than algorithmic methods of topic modeling such as Non-Negative Matrix Factorization \cite{nnmf}, Truncated SVD \cite{svd}, and LDA (Latent Dirichlet Allocation) \cite{lda}. 

We also worked out a method of keyword extraction that uses the fact that a text has sufficiently many chapters. This method is working better than LUHN and YAKE, but it is inferior to the previous one. However, we believe this method does have the potential for further development, e.g., for clarifying the conceptual meaning of keywords and relating it with higher-order textual structures. 

Our future work will be adding some functionality for n-grams analysis so that we can extract from a text not only single words but also phrases of length 2 and bigger. In particular, these key phrases are important for reflecting the aspects that were not studied in this work, i.e. relations of keywords to the style of the text. Refs.~\cite{pagerank,papa} provide useful information on how to find key-phrases from keyword extractions. Yet another feature we are going to implement is to modify the spatial mean and variance of the word [see (\ref{durnovo}, \ref{37})] such that they reflect the local frequency of the word. 
A more remote but important application will be to employ keywords for facilitating text compression methods \cite{allahverdyan2023optimal}. 

\section*{Acknowledgements}

This work was supported by SCS of Armenia, grant No. 21AG-1C038. It is a pleasure to thank Narek Martirosyan for participating in initial stages of this work. We acknowledge discussions with A. Khachatryan, K. Avetisyan and Ts. Ghukasyan. We thank S. Tamazyan for helping us with French texts. 

%\bibliographystyle{acl}
%\bibliographystyle{unsrt}
\bibliographystyle{IEEEtran} 
%\bibliographystyle{ieeetr}
\bibliography{lit}

\appendix

\section{Thematic groups for {\it Heart of Dog}}
\label{hd}

Here we discuss thematic groups uncovered from a short text. {\it Heart of Dog} by M. Bulgakov is a known satirical novella that shows the post-revolutionary Moscow (first half of the 1920s) under social changes, the emergence of new elites of Stalin's era, and science-driven eugenic ideas of the intelligentsia. Eventually, the novella is about the life of a homeless dog Sharik (a standard name for an unpedigreed dog in Russia) picked up for medical and social experiments. 

Keyword classes (themes) for {\it Heart of Dog} uncovered via (\ref{dog}, \ref{sharik}): 

\1 Canine features: 'dog', 'sharikov', 'salami', 'sharikovs', 'bite', 'cracow', 'scald', 'animal', 'sharik', 'cat', 'bitten', 'canine', 'phewphew', 'cur', 'claw', 'bitch', 'mange', 'shaggy', 'phew', 'paw', 'bark', 'wild', 'biting', 'oooo'.

\2 Medical terms: 'skull', 'camphor', 'weight', 'temperature', 'method', 'stitch', 'pulse', 'organism', 'injection', 'laboratory', 'sore', 'needle', 'scissors', hospital', 'respiratory', 'gauze', 'adrenaline', 'clinic', 'doze', â€˜heal', 'transplant', 'phonograph', 'hypothesis', 'organism', 'nostril', 'injection',  'subdepartment', 'department', 'laboratory', 'hospital', 'rejuvenation', 'throat', 'scholar', 'brow', 'cheek', 'lip', 'strip', 'experiment', 'forehead', 'hormone', 'breast', 'science', 'hypophysis', 'brain'.

\3 Revolution: 'proletariat', 'terror', 'kautsky', 'council', 'bourgeois', 'proletarian', 'revolution', 'war', 'worker', 'engels', 'pest', 'revolver', 'social', 'chairman', 'committee'.

\4 Moscow: 'moscow', 'kalabukhov', 'blizzard', 'bolshoi', 'mosselprom', 'nikitins', 'prechistenka', 'swearword'.

The majority of these words were not even uncovered via LUHN and/or YAKE.

\section{Lemmatization of texts}
\label{lemma_ap}

{\bf 1.} English texts were preprocessed using WordNetLemmatizer imported from nltk.stem \cite{nltk}. This library looks for lemmas of words from the WordNet Database. The lemmatization uses corpus for excluding stop-words (functional words) and WordNet corpus to produce lemmas. WordNetLemmatizer identifies the intended part of speech and meaning of a word in a sentence, as well as within the larger context surrounding that sentence, such as neighboring sentences or even an entire text. We applied this lemmatization algorithm on nouns, adjectives, verbs and adverbs to get maximal clean up of the text. Any stemming procedure will be inappropriate for our purposes of extracting keywords, since stemming may mix different parts of speech. 

For inflected languages (e.g. Russian), the lemmatization rules are more complex. 
For French and Russian texts we used lemmatizers LEFFF \cite{french_lem} and pymystem3  \cite{rus_lem}, respectively. 

{\bf 2.} The LUHN method for keyword extraction is based on Zipf's law. We implemented it as follows
\cite{zipf_pre}. Stop-words (functional words) were eliminated from the text, and all remaining words were ranked according to their frequencies (\ref{ordinary}): more frequent words got a larger rank. Hence the frequencies (\ref{ordinary}) are represented via the rank: $\{f_r\}_{r=1}$. Then we selected the rank $r_{max}$ so that there are $\geq 10$ words having the frequency $f_{r_{max}}$. The proper Zipf's law becomes ill-defined for $r>r_{max}$ \cite{zipf_pre}. Words with frequencies $\{f_r \}_{r=1}^{r_{max}}$ are presented as potential keywords extracted by LUHN.



\section{Spatial frequency versus ordinary frequency}
\label{cuba}

Here we discuss two features of space-frequency $\tau(w)$ of a word $w$ [see (\ref{durnovo})], and the ordinary frequency $f(w)$; cf.~(\ref{ordinary}). 

{\bf 1.} If a word $w$ is distributed homogeneously, then $\tau(w)$ defined via (\ref{durnovo}) is expressed via the ordinary frequency $f(w)$. If in addition, this is a sufficiently frequent word, then 
\BEA
\tau(w)\approx f(w)=N_w/N, {\rm ~~for~~} N\gg 1 {\rm ~~and~~} N_w\gg 1.
\EEA
Indeed, for the homogeneous distribution of $w$ within the text all $\zeta_{i}$ in (\ref{durnovo}) are equal: $\zeta_{i}=\zeta$, where $\zeta$ is defined from placing the word $w$ among $N$ words (placing $Nf(w)$ times with equal intervals). Hence $Nf(w)+(Nf(w)+1)\zeta=N$ and 
\BEA
\tau(w)=\frac{1}{\zeta+1}=\frac{\frac{1}{N}+f(w)}{\frac{1}{N}+1}. 
\label{braza}
\EEA
Whenever $f(w)\gg \frac{1}{N}$ (and naturally $1\gg \frac{1}{N}$) we get $\tau(w)=f(w)$, i.e. the space frequency coincides with the ordinary one. It is seen that the largest value $\tau(w)=1$ is achieved for $\zeta_{\,i}=0$ when all appearances of the word $w$ come after each other without any other word in between. The smallest value of $\tau(w)=\frac{1}{N-1}$ is achieved for $\zeta_{1}=N-2$ with just two appearances of $w$ that come as the first and last words of the text.

%Hence the difference between $\tau(w)$ and $f(w)$ (for sufficiently frequent words) can tell how the 
% distribution of $w$ deviates from the homogeneous one. 

{\bf 2.} In all texts we studied we noted that relation (\ref{dag})
holds for $\sim 80$ \% of text words $w$. This set includes frequent words. We validated the following explanation for (\ref{dag}). After (\ref{durnovo}) we indicated that $\tau(w)$ stays invariant with respect to a certain class of permutations of words in the text. Hence, aiming to calculate $\tau(w)$ for a given frequent word $w$ we can employ the Bernoulli process of text generation, assuming that each word is generated independently from others, and equals $w$ not ($w$) with probability $f(w)$ ($1-f(w)$). For spatial intervals $s$ between the occurrences of $w$ the Bernoulli process produces the geometric distribution ${p}(s)=(1-f)^sf$. Now the mean of this distribution is 
\BEA
f{\sum}_{s=0}^{\infty}s(1-f)^s=\frac{(1-f)}{f}, 
\label{geom}
\EEA
whose inverse $\tau(w)\simeq f(w)/(1-f(w))$ holds (\ref{dag}). Note that (\ref{braza}) also leads to a relation similar to (\ref{dag}), but it is not a reliable explanation, since $f(w)$ (ordinary frequency) and $\tau(w)$ (spatial frequency) appear to be too close to each other. 

\section{Topics uncovered by Non-Negative Matrix Factorization Method for {\it Anna Karenina}}
\label{nnmf}

Among the following 10 topics proposed by the method, only Topic 1 approximately corresponds to the first thematic group uncovered via our method. Other topics are meaningless. 

Topic 1: {\it levin, vronsky, anna, kitty, alexey, alexandrovitch,
arkadyevitch, stepan, room, wife}

Topic 2: {\it walked, beauty, maid, gentleman, people, frou, complete,
carriage, completely, coming}

Topic 3: {\it living, sviazhsky, meaning, mare, kitty, book, natural,
listening, friends, suppose}

Topic 4: {\it read, work, ivanovitch, coat, drove, agriculture, lack,
hearing, matters, living}

Topic 5: {\it serpuhovskoy, received, young, desire, pass, asleep, set,
action, clerk, stay}

Topic 6: {\it doctor, stahl, coming, today, passion, porter, silence,
movement, object, levin}

Topic 7: {\it tanya, remember, game, deal, live, mamma, walking, bare,
easy, hurriedly}

Topic 8: {\it noticed, possibility, christian, dmitrievitch, feelings,
fall, forget, success, stopped, suffer}

Topic 9: {\it early, covered, recognized, angrily, connection, expression,
figure, breathing, nice, friend}

Topic 10: {\it scythe, nobility, elections, minutes, promised, extreme,
afraid, decided, ordered, lifting}



\comment{
\section{Short texts: analyzing a scientific paper}
\label{papers}

Our example is a known paper by Jaynes \cite{jaynes} in the cross-link of statistical physics (that studies features of many-particle systems in terms of entropy, energy and temperature) and probabilistic inference, which deals with random events, (subjective) probability events, estimation {\it etc}. These two different fields became mutually beneficial after Ref.~\cite{jaynes} proposed the maximum-entropy method \cite{jaynes2}. Hence we expect two different sets of keywords. 

It turns out that a relatively short length of Ref.~\cite{jaynes} prevents the direct applicability of (\ref{40}, \ref{41}). Instead, we followed the logic of Figs.~\ref{anna} and \ref{finn}: we ranked all distinct words of Ref.~\cite{jaynes} with their frequencies, and then looked within this sequence for local minimas of $A(w)$; cf.~(\ref{37}). In a very few cases, where the local maxima was quasi-degenerate, i.e. two nearby words have close values of $A(w)$, we took the word that also provided a local maxima for $A_4(w)$ that is defined analogously to $A(w)$ in (\ref{37}), but with the four-order variance ${\rm var}_4(w)=\frac{1}{\ell-1}{\sum}_{i=1}^{\ell-1} \,(\zeta_{\,i}+1-t(w))^4$ instead of the usual variance in (\ref{3}). Words from the first column of Table~\ref{table4} came out in this way (we mention only the first such 15 words, and the number in brackets is the frequency rank for each word). It is seen that not much keywords related to statistical physics came out. Looking at local maxima of $A(w)$ among the ranked words produced the the second column of Table~\ref{table4}. This set provides more non-keywords than in the first column. Still the majority are keywords, and some of them are highly-relevant, e.g. 'maximum-entropy'. 

The method is limited (as compared e.g. to the analysis of {\it Anna Karenina}), since Ref.~\cite{jaynes} is a relatively short text. Hence we tried the following extension of the method: we repeated the text two times, then applied a random permutation to the whole (twice longer) text and implemented (\ref{37}). A new set of keywords came out via selecting local minimas of $A(w)$; see the third column of Table~\ref{table4}.
It is seen that most keywords now relate to statistical physics. Combining the three columns of Table~\ref{table4} together we get a set of keywords that does reflect the interdisciplinary character of \cite{jaynes}. A peculiar point of scientific papers is that the first 5-10 most probable words do likely contain keywords. However, many keywords are not among the most-probable words. Our method was able to find them, as seen in Table~\ref{table4}. We should mention that some obvious keywords of \cite{jaynes} were not detected via our method.

\begin{table*}\caption{Keywords of Ref.~\cite{jaynes} extracted via various means. We shadowed non-keywords and underlined keywords related to statistical physics. Other words are keywords related to probabilistic inference. Square brackets indicate the rank of the word (ranked according to the frequency).
}
\begin{tabularx}{1\textwidth} 
{ 
  | >{\raggedright\arraybackslash}X 
  | >{\raggedleft\arraybackslash}X 
  | >{\raggedleft\arraybackslash}X |}
 \hline
Local minima of $A(w)$ defned via (\ref{37}) & Local maxima of $A(w)$  & Local minima of $A(w)$ for the text repeated two times \\
 \hline 
probability [1], distribution [4], function [7], prediction [12], \underline{temperature} [14], \mybox[fill=blue!20]{fact} [24], 
subjective [26], \mybox[fill=blue!20]{argument} [29], event [34], uncertainty [36], mathematical [42], 
\mybox[fill=blue!20]{form} [47], method [50], \mybox[fill=blue!20]{equal} [54], expectation [58], 
&
statistical [1], theory [5], \mybox[fill=blue!20]{problem} [9], \mybox[fill=blue!20]{case} [11],  \underline{maximum-entropy} [13], inference [15],
\mybox[fill=blue!20]{type} [20], 
\mybox[fill=blue!20]{value} [24],
\underline{macroscopic} [27], \mybox[fill=blue!20]{point} [32], knowledge [40], \underline{photon} [44], objective [48], 
average [53], \mybox[fill=blue!20]{question} [57], \mybox[fill=blue!20]{total} [62], \mybox[fill=blue!20]{maximum} [66]
&
probability [1], \underline{entropy} [6], \underline{energy} [8], prediction [12],
\underline{temperature} [14],
estimate [18], \mybox[fill=blue!20]{condition} [20], reason [25], \mybox[fill=blue!20]{argument} [29], event [32], \underline{noise} [36],
\mybox[fill=blue!20]{total} [56], \underline{heat} [62], \mybox[fill=blue!20]{definite} [78], \underline{particle} [94]
\\ 
\hline
\end{tabularx}
\label{table4}
\end{table*}
}



\comment{
\section{Generalized variances} 
It's worth to mention that besides mean and variance we decided to study also some generalized variances, but unfortunately got no interesting results. See those generalizations of variances below.
For a given word, let us write (\ref{3}) as
\begin{equation}
\label{33}
{\rm var}_2=\frac{1}{\ell-1}{\sum}_{i=1}^{\ell-1} \,(\zeta_{\,i}+1-t(w))^2.
\end{equation}
We can then introduce two generalizations of (\ref{33}):
\begin{equation}
\label{34}
{\rm var}_1=\frac{1}{\ell-1}{\sum}_{i=1}^{\ell-1} \,\left|\zeta_{\,i}+1-t(w)\right|.
\end{equation}

\begin{equation}
\label{35}
{\rm var}_4=\frac{1}{\ell-1}{\sum}_{i=1}^{\ell-1} \,(\zeta_{\,i}+1-t(w))^4.
\end{equation}

Now if the above quantities are to be compared with the mean, then ${\rm var}_1$ stays unchanged, while instead of ${\rm var}_2$ and ${\rm var}_4$ we employ, respectively
\begin{equation}
\label{36}
\left[{\rm var}_2\right]^{1/2}, \qquad
\left[{\rm var}_4\right]^{1/4}.
\end{equation}
}

\comment{
\section{Keywords of {\it Anna Karenina}}
\label{boro}

\begin{table}\caption{Words of {\it Anna Karenina} extracted via our method. For global keywords strong and weak cases mean (resp.) that the words $w$ were chosen according to $A(w)\leq \frac{1}{5}$ and $A(w)\leq \frac{1}{3}$; cf.~(\ref{40}). Local keywords were chosen according to $A(w)\geq {3}$; see (\ref{41}). Keyword classes are denoted by upper indices. \1: proper names of major characters; \2: proper names of secondary characters; \3: animals; \4: trains and railway; \5: hunting; \6: rural life and agriculture; \7: local government (zemstvo); \8: nobility life and habits; \9: religion. The last group \0 denotes words that were identified as keywords, but did not belong to any of the above groups.     }
\begin{tabularx}{\textwidth} 
{ 
  | >{\raggedright\arraybackslash}X 
  | >{\raggedleft\arraybackslash}X |
   | >{\raggedleft\arraybackslash}X | }
 \hline
 Global keywords strong cases & 
  Global keywords weak cases & 
   Local keywords \\ \hline
   'levin\1', 'anna\1', 'vronsky\1', 'kitty\1', 'alexey\1', 'stepan\1', 'alexandrovitch\1', 'arkadyevitch\1', 'dolly\1', 'sergey\1', 'ivanovitch\1', 'peasant', 'darya\1', 'alexandrovna\1', 'varenka\1', 'lidia\1', 'death', 'ivanovna\1', 'laborer\6', 'mow\6', 'district\7', 'stahl\1', 'bailiff\5', 'gun\5', 'snipe\5', 'plough\6', 'rain', 'lesson\0', 'lord\9', 'acre\6', 'platform\4', 'natalia\1', 'built', 'rich', 'overlook', 'river', 'crime\0', 'rail\6', 'relate', 'throb', 'contrast', 'puzzle', 'cheat\0', 'oppress', 'irrational\0' 
   &
'love', 'princess\8', 'brother', 'carriage\4', 'horse\8', 'prince\8', 'doctor\8', 'countess\8', 'madame\8', 'sviazhsky\1', 'land\6', 'seryozha\1', 'konstantin\1', 'picture', 'oblonsky\1', 'nikolay\1', 'agafea\2', 'katavasov\2', 'grass\6', 'yashvin\1', 'shoot\5', 'mihalovna\2', 'officer\8', 'box', 'marshal\7', 'mare\6', 'priest\9', 'tree\6', 'forest\6', 'laska\3', 'law\0', 'landowner\6', 'realize', 'scythe\6', 'telegram\8', 'meadow\6', 'bedroom\8', 'argument', 'sledge', 'nobleman\8', 'paint', 'article\8', 'professor\8', 'scream', 'sky', 'trap', 'birch\6', 'cow\6', 'debt\0', 'rent', 'punish', 'sow\6', 'annushka\2', 'lightly', 'sportsman\8', 'myakaya\2', 'invalid', 'smart', 'parent', 'vividly', 'maman\8', 'institution\7', 'stable', 'distance', 'salary\0', 'educate', 'firm', 'skirt', 'mahotin\2', 'reconciliation', 'yellow', 'plump', 'childrens', 'tatar\2', 'outer', 'steward\8', 'cousin', 'loathsome', 'sharp', 'splash', 'armchair\8', 'understands', 'coarse', 'quicken', 'grace', 'delicious', 'director\8', 'unseen', 'selfpossession', 'cheese', 'rate', 'physically', 'timidity', 'tucked', 'reassure', 'sunday', 'compartment', 'frost', 'minister\8', 'won', 'king', 'repent', 'clock', 'wage', 'shock', 'uncertain', 'deliver', 'cream', 'silently', 'monday', 'captain\8', 'shaft\6', 'matrona\8', 'strictly', 'original' 
&
 'vassenka\2', 'golenishtchev\2', 'election\7', 'skate\0', 'varvara\2', 'pyotr\2', 'lizaveta\2', 'landau\2', 'petrovna\2', 'gladiator\3', 'metrov\2', 'tit\2', 'vote\7', 'froufrou\3', 'ryabinin\2', 'volunteer\8', 'nevyedovsky\2', 'duel\8', 'scandal\8', 'tribe\0', 'snetkov\2', 'lukitch\2', 'mower\6', 'deacon\9', 'native', 'korsunsky\2', 'hospital', 'remote', 'mazurka\8', 'pilate\0', 'sappho\0', 'villa\8', 'rival', 'reed\6', 'bridegroom\8', 'krak\3', 'merkalova\2', 'vorkuev\2', 'photograph\8', 'yegor\2', 'mitya\2', 'kapitonitch\2', 'architect\8', 'intensely', 'elect\7', 'golenishtchevs\2', 'pa\8', 'birthday', 'trousseau\8', 'transition', 'chalk', 'potato\6', 'kritsky\2', 'ergushovo\6', 'katya\2', 'weep', 'sympathetic', 'repair', 'mais\8', 'seryozhas\2', 'ballroom\8', 'classical', 'vozdvizhenskoe\6', 'technique', 'bedchamber\8', 'opium\8', 'penetrate', 'tchirikov\2', 'rider', 'palazzo\8', 'crown\8', 'remove', 'miracle', 'intolerable', 'turk\2', 'ballot\7', 'custom', 'nevsky\8', 'adultery\8', 'ditch', 'musical'  \\
\hline
\end{tabularx}
\label{table1}
\end{table}


\begin{table}\caption{Keywords of {\it Anna Karenina} extracted via looking at distribution of words over chapters; see (\ref{9}, \ref{99}). For the first set of words only 'wife', 'man', 'marsh', 'sick' do not appear in the main list of global keywords; see Table~\ref{table1}. Among them 'marsh' is a keyword referring to hunting and agricultural themes of the novel. The second set contains a new pertinent local keywords of one of secondary actors of the novel ('petritsky'). Both sets are close to each other.   }
\begin{tabularx}{1\textwidth} 
{ 
  | >{\raggedright\arraybackslash}X 
  | >{\raggedleft\arraybackslash}X | }
 \hline
 36 words having largest values of (\ref{99})  &
 36 words having largest values of (\ref{9}) \\
 \hline 
'levin\1', 'vronsky\1', 'alexey\1', 'alexandrovitch\1', 'kitty\1', 'doctor\8', 'stepan\1', 'varenka\1', 'arkadyevitch\1',
'anna\1', 'countess\8', 'brother', 'darya\1', 'katavasov\2', 'princess\1', 'seryozha\1', 'dolly\1', 'alexandrovna\1',
'veslovsky\2', 'priest\9', 'peasant\6', 'love', 'sviazhsky\2', 'grass\6', 'child', 'konstantin\1', 'sergey\1', 'prince\8',
'wife', 'ivanovitch\1', 'man', 'picture', 'scythe\6', 'marsh\6', 'mow\6', 'sick', 'mare\6' 
&
 'levin\1', 'alexey\1', 'alexandrovitch\1', 'varenka\2', 'vronsky\1', 'kitty\1', 'doctor\8', 'stepan\1', 'scythe\6', 'anna\1',
'arkadyevitch\1', 'marsh\6', 'countess\8', 'katavasov\2', 'priest\9', 'darya\1', 'veslovsky\2', 'alexandrovna\1', 'seryozha\1',
'mare\6', sviazhsky\2', 'mihailov\2', 'brother', 'dolly\1', 'grass\6', 'sergey\1', 'princess\8', 'mow\6', 'marshal\7', 'konstantin\2', 'ivanovitch\2', 'peasant\6', 'lidia\1', 'sick', 'petritsky\2' \\
\hline
\end{tabularx}
\label{table2}
\end{table}
}

\end{document}

% Figure environment removed

%%%%%%%%%%%%%%%%%%%%%%%%

Scientometrics
Journal of Information Science
Journal of Informetrics
Journal of the Association for Information Science & Technology
Online Information Review
Annals of Library and Information Studies
Transactions of the Association for Computational Linguistics

Hartley J and Kostoff RN, How useful are `Key Words' in scientific journals? Journal of Information Science, 29 (5) (2003) 433-38.

Hurt CD, Automatically generated keywords: a comparison to author generated keywords in the sciences, Journal of Information and Organizational Sciences, 34 (1) (2010) 81-8.

Gbur EE and Trumbo BE, Key words and phrasesâ€”the key to scholarly visibility and efficiency in an information explosion, The American Statistician, 49 (1995) 29â€“33.

Song, D., Lalmas, M., Van Rijsbergen, K., Frommholz, I., Piwowarski, B., Wang, J., Zhang,
P., Zuccon, G., Bruza, P., Arafat, S., et al.: How quantum theory is developing the field of
information retrieval. In: 2010 AAAI Fall Symposium Series (2010)

