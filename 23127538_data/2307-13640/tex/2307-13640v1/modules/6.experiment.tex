

\section{Experimental results}
\label{sec:exp}

To evaluate the effectiveness of the proposed training procedure, we perform experiments on both unsupervised object localization and unsupervised semantic segmentation using the features before and after motion-guided fine-tuning. We implement this procedure using PyTorch~\cite{paszke2019pytorch} on top of the released implementation of DINO~\cite{caron2021emerging}, and apply it on ViT-Small and ViT-Base~\cite{dosovitskiy2020image} architectures with patch sizes of 8 and 16 respectively, and with weights initialized from the pre-trained DINO models~\cite{caron2021emerging}. We set temperature $\tau$ to 0.1 and radius $\sigma$ to 0.7 in all experiments.
During fine-tuning, each batch consists of half of the images taken from ImageNet~\cite{deng2009imagenet}, and the other half is made of video frames. Our final loss is the sum of DINO's loss on the ImageNet images and the optical flow loss on the video frames.

\vspace{1em}

\noindent\textbf{Dataset Preparation.} To create datasets  with optical flow information, we merge the following existing video datasets: UVO~\cite{wang2021unidentified}, VSPW~\cite{miao2021vspw}, and Youtube-VOS~\cite{xu2018youtube}. We extract frames from about 10,000 videos with a frame interval of five and estimate the optical flow between each adjacent frames with the Raft-Large model~\cite{raft2020teed}. We also apply the same procedure on the Moment-in-Time dataset~\cite{monfortmoments}, but we use only the first dataset except when otherwise noted. Compared to images, each pixel in the optical flow is stored in two 32-bit float numbers rather than a single 8-bit unsigned integer. Optical flow does not have a widely available compression format. Therefore, storing the optical flow requires roughly one to two orders of magnitude more space than an equivalent number of images. To overcome this challenge, we quantize the 
 optical flow into 16-bit integers and concatenate the flow in the x and y directions into a single 32-bit float number.
Next, we save the 32-bit stream in the TIFF image format, which supports 32-bit float pixel values, and we apply the TIFF compression protocol. By applying this approach to store the optical flow, we observe about a ten-times reduction in disk space usage.



\subsection{Object Localization}

We extract features with our motion fine-tuned ViT models and apply the latest unsupervised object localization method from Deep-Spectral~\cite{melas2022deepspectral}. 
As is standard practice, we compare the results to prior work on three datasets: Pascal VOC 2007, Pascal VOC 2012~\cite{everingham2009pascal}, and COCO-20k~\cite{vo2020toward} (a subset of 20K images from MS-COCO dataset~\cite{lin2014microsoft}). 
We follow the evaluation procedure used in~\cite{vo2020toward, simeoni2021lost}, which accepts one bounding box for each image. 
Results are reported in the Correct Localization (CorLoc) metric, which measures the percentage of images on which one object can be correctly localized by the given bounding box. An object is considered to be correctly localized if the predicted bounding box has a greater than 50\% intersection-over-union (IoU) with the object's ground-truth bounding box.


\begin{table}[h]\centering
% \scriptsize
\setlength\belowcaptionskip{-10pt}
\begin{tabular}{lccc}\toprule
\textbf{Method} &\textbf{VOC-07} &\textbf{VOC-12} &\textbf{COCO-20k} \\\midrule
Selective Search~\cite{uijlings2013selective} &18.8 &20.9 &16 \\
EdgeBoxes~\cite{zitnick2014edge} &31.1 &31.6 &28.8 \\
Kim et al.~\cite{kim2009unsupervised} &43.9 &46.4 &35.1 \\
Zhang et al.~\cite{zhang2020object} &46.2 &50.5 &34.8 \\
DDT+~\cite{wei2019unsupervised} &50.2 &53.1 &38.2 \\
rOSD~\cite{vo2020toward} &54.5 &55.3 &48.5 \\
LOD~\cite{vo2021large} &53.6 &55.1 &48.5 \\
DINO-[CLS]~\cite{caron2021emerging} &45.8 &46.2 &42.1 \\
LOST~\cite{simeoni2021lost} &61.9 &64 &50.7 \\
Deep-Spectral~\cite{melas2022deepspectral} &62.7 &66.4 &52.2 \\
Deep-Spectral* &60.5 &65.7 &48.5 \\
% Token-Cut & 68.8 & 72.4 & 59.0 \\
% Token-Cut* & TBD & TBD & TBD \\ \midrule
Ours (Deep-Spectral) &\textbf{63.1(+2.6)} &\textbf{68.5(+2.8)} &\textbf{53.6(+5.1)} \\
% Ours (Token-Cut) & TBD & TBD & TBD \\
\bottomrule
\end{tabular}
\caption{Single-object localization performance (CorLoc). Our results are obtained by reusing the post-processing procedures in Deep-Spectral~\cite{melas2022deepspectral} with our motion fine-tuned features, without any supervision. `*' denotes the results we reproduce from the official released implementations.}\label{tab:obj_loc}
\end{table}

The quantitative results are summarized in Table~\ref{tab:obj_loc}. 
We reproduce the localization performance in Deep-Spectral~\cite{melas2022deepspectral} using ViT-B8 (ViT-Base network with patch size 8) and the released implementation and hyper-parameters. We report our results by reusing the same implementation on the ViT-B8 architecture with our fine-tuned weights. Clear and consistent improvement over the original features can be seen in all three datasets. Note that this improvement is obtained automatically, without any human effort, because our approach is fully self-supervised. In Figure~\ref{fig:obj_loc}, we show some qualitative examples of our methods.



% Figure environment removed


\begin{table}[h]\centering
% \scriptsize
\begin{tabular}{lrrrrr}\toprule
&\textbf{Network} &\textbf{VOC-07} &\textbf{VOC-12} &\textbf{COCO-20k} \\\midrule
\multirow{4}{*}{Baseline} &ViT-S16 &57.4 &63.4 &46.4 \\
&ViT-B16 &56.7 &62.8 &46 \\
&ViT-S8 &59.4 &64.1 &47.6 \\
&ViT-B8 &60.5 &65.7 &48.5 \\ \midrule
\multirow{4}{*}{Ours} &ViT-S16 &\textbf{58.4} &\textbf{64.3} &\textbf{48.1} \\
&ViT-B16 &\textbf{59} &\textbf{64.1} &\textbf{48.5} \\
&ViT-S8 &58.4 &\textbf{64.2} &46.7 \\
&ViT-B8 &\textbf{63.1} &\textbf{68.5} &\textbf{53.6} \\
\bottomrule
\end{tabular}
\caption{Comparing, in terms of object localization performance with Deep-Spectral algorithm~\cite{melas2022deepspectral}, different ViT architectures before vs. after fine-tuning them with motion. Motion-driven fine-tuning generally yields better results.}\label{tab:obj_loc_comp}
\end{table}
\begin{table}[h]\centering
% \scriptsize
\footnotesize
\begin{tabular}{lcccc}\toprule
\textbf{Arch} &\textbf{\scriptsize Motion Removal} &\textbf{\scriptsize  Patch Size} &\textbf{\scriptsize Loss Reweight} &\textbf{VOC-12} \\\midrule
ViT-S16 &\checkmark &5 &\checkmark &62.8 \\
ViT-S16 &\checkmark &3 &\checkmark &64.3 \\
ViT-S16 & &3 &\checkmark &62.8 \\
ViT-S16 & &3 & &59.85 \\ \midrule
ViT-S16 &\multicolumn{3}{l}{Baseline} &63.4 \\
ViT-S16 &\multicolumn{3}{l}{Only add video frames} &62.6 \\
ViT-S16 &\multicolumn{3}{l}{add video frames + flow loss} & 64.3 \\
\midrule
ViT-B8 &\multicolumn{3}{l}{Baseline} &65.7 \\ 
ViT-B8 &\multicolumn{3}{l}{Only add video frames} &64.88 \\
ViT-B8 &\multicolumn{3}{l}{add video frames + flow loss} & 68.5 \\
\bottomrule
\end{tabular}
\caption{Comparing different options of motion-driven fine-tuning on the object localization task (CorLoc).}\label{tab:main_ablation}
\end{table}


The results in Table~\ref{tab:obj_loc_comp} demonstrate that our motion-driven fine-tuning approach improves object localization performance across different ViT architectures. Additionally, in Table~\ref{tab:main_ablation}, we conduct an ablation study to assess the impact of our optical flow loss. Specifically, we compare our proposed optical flow loss with using only the DINO's self-supervised loss while adding video frames as extra training data. Our results reveal that solely adding video frames as new training images does not lead to performance gains. Moreover, we assess the individual contributions of background motion removal and motion-based loss re-weighting and demonstrate that performance deteriorates when background motion is not removed or when the learning is not focused on regions with substantial motion.

\subsection{Semantic Segmentation}

We evaluate our motion fine-tuned ViT models on unsupervised semantic segmentation. Our evaluation protocol follows prior work in self-supervised learning~\cite{ziegler2022leopart}, i.e., linear probing and cluster probing. Linear probing protocol involves training an extra linear projection from model outputs to ground-truth labels with supervision while freezing the model weights. Cluster probing protocol involves dividing spatial features into separate groups with clustering algorithms and applying Hungarian matching \cite{kuhn1955hungarian} to match clusters to ground-truth labels optimally. Both linear and cluster probing are done solely for the purpose of evaluating the learned features. 
We compare our results to prior methods on Cocostuff-27~\cite{caesar2018cvpr} and Pascal VOC 2012. Results are measured in terms of mean intersection-over-union (mIoU), which  denotes the percentage of overlaps between the predicted segmentation mask and the ground-truth across different classes.



\begin{table}[!htp]\centering
% \setlength\belowcaptionskip{-5pt}
\small
\begin{minipage}{0.49\linewidth}
\begin{tabular}{lc}\toprule
\textbf{Method } &\textbf{\footnotesize Cocostuff-27} \\\midrule
ResNet50\cite{he2016deep} &10.2 \\
MoCoV2\cite{chen2020improved} &13.2 \\
MDC\cite{cho2021picie} &13.3 \\
PiCIE\cite{cho2021picie} &13.9 \\
PiCIE+H\cite{cho2021picie} &14.8 \\
STEGO\cite{hamilton2022stego} &41.2 \\
DINO\cite{caron2021emerging} &42.2 \\
Leopart\cite{ziegler2022leopart} &44.1 \\\midrule
Ours &\textbf{46.1(+2.0)} \\
\bottomrule
\end{tabular}
\end{minipage}\hfill
\begin{minipage}{0.49\linewidth}
\begin{tabular}{lc}
\toprule
\textbf{Method} &\textbf{VOC-12} \\\midrule
IIC\cite{ji2019invariant} &28 \\
MoCoV2\cite{chen2020improved} &45 \\
InfoMin\cite{tian2020makes} &45.2 \\
SWAV\cite{caron2020clustercontrast} &50.7 \\
SegSort\cite{hwang2019segsort} &36.2 \\
Hierach. Group.\cite{zhang2020self}\hspace{-2em} &48.8 \\
MaskContrast\cite{van2021maskcontrast}\hspace{-2em} &63.9 \\
Leopart~\cite{ziegler2022leopart} &68 \\\midrule
Ours &\textbf{68.7(+0.7)} \\
\bottomrule
\end{tabular}

\end{minipage}
\caption{Semantic Segmentation (mIoU) on Cocostuff-27 and Pascal VOC 2012 with linear probing. Our results are obtained using the evaluation protocol of Leopart~\cite{ziegler2022leopart}.}
\label{tab:uss_linear}
\end{table}


% Figure environment removed

Table~\ref{tab:uss_linear} compares our approach with previous methods through linear probing, including the state-of-the-art technique Leopart~\cite{ziegler2022leopart}. Our results are obtained by fine-tuning on ViT-B8 with the Moment-in-Time dataset~\cite{monfortmoments} without ImageNet. It should be noted that while the result reported by Leopart~\cite{ziegler2022leopart} is achieved using a smaller ViT architecture (ViT-S16), our method demonstrates the potential to improve semantic segmentation performance solely by utilizing motion information, without relying on existing visual continuity through spatial clustering. Qualitative examples of our approach are shown in Figure~\ref{fig:uss_cocostuff}.


\begin{table}[!htp]\centering
% \setlength\belowcaptionskip{-10pt}
\begin{tabular}{lrr}\toprule
\textbf{Method} &\textbf{mIoU} \\\midrule
Co-Occurrence\cite{isola2015learning} &4 \\
CMP\cite{zhan2019condmotionprop} &4.3 \\
Colorization\cite{zhang2016colorful} &4.9 \\ 
IIC\cite{ji2019invariant} &9.8 \\
MaskContrast\cite{van2021maskcontrast} &35 \\
Deep-Spectral\cite{melas2022deepspectral} (w/o self-training) &30.8 ± 2.7 \\
Deep-Spectral\cite{melas2022deepspectral} &37.2 ± 3.8 \\ \midrule
\textit{DINO Baselines} & \\ \midrule
ViT-B16 &27.9 ± 1.18 \\
ViT-S16 &30.2 ± 1.15 \\ \midrule
\textit{Ours (w/o self-training)} \\ \midrule
ViT-B16 &31.0 ± 1.6 (\textbf{+3.1}) \\
ViT-S16 &35.35 ± 2.2 (\textbf{+5.15}) \\
\bottomrule
\end{tabular}
\caption{Semantic Segmentation (mIoU) on Pascal VOC 2012 through cluster probing. We adopt the evaluation protocol from MaskContrast~\cite{van2021maskcontrast} and use the same ViT from Deep-Spectral~\cite{melas2022deepspectral} without the self-training part of~\cite{melas2022deepspectral}.
}\label{tab:uss_clst}
\end{table}




% Figure environment removed


In Table~\ref{tab:uss_clst}, we compare to previous methods through cluster probing on Pascal VOC 2012. Due to the lack of a  standardized practice on cluster probing, we adopt the evaluation protocol from MaskContrast~\cite{van2021maskcontrast}, but use the ViT attention as estimated saliency instead of a supervised saliency network. We select other existing works based on similarity of their evaluation protocols. 
Notably, our approach outperforms the DINO baselines and achieves comparable results to Deep-Spectral without self-training. It is worth mentioning that self-training is a general performance boosting technique that involves training on pseudo-labels generated for the target dataset. 
Qualitative examples are shown in Figure~\ref{fig:uss_voc}.


\begin{table}[!htp]\centering
% \setlength\belowcaptionskip{-10pt}
% \scriptsize
\begin{tabular}{lrrrrr}\toprule
& &\textbf{VOC-12} &\textbf{Cocostuff-27} &\textbf{ImageNet} \\\midrule
\multirow{2}{*}{Baseline} &ViT-S16 &47.41 &36.1 &74.44 \\
% &ViT-B16 &58.82 &40.5 &75.87 \\
&ViT-S8 &49.53 &38.6 &78.33 \\ \midrule
% &ViT-B8 &62.1 &42.2 &77.28 \\\midrule
Leopart~\cite{ziegler2022leopart} &ViT-S16 &68 &44.1 &51.99 \\\midrule
\multirow{2}{*}{Ours} &ViT-S16 &59.39 &39.96 &73 \\
% &ViT-B16 &59.53 &41.6 &74.2 \\
&ViT-S8 &63.28 &41.26 &77.46 \\
% &ViT-B8 &63.56 &43.75 &74.74 \\
\bottomrule
\end{tabular}
\caption{Comparing networks before vs. after motion-driven fine-tuning on unsupervised semantic segmentation (mIoU), and ImageNet classification (top-1 accuracy) tasks.}\label{tab:uss_lc_imagenet}
\end{table}


\begin{table}[!htp]\centering
% \scriptsize
\setlength\belowcaptionskip{-10pt}
\begin{tabular}{lcccc}\toprule
&ViT-S16 &ViT-B16 &ViT-S8 &ViT-B8 \\\midrule
Baseline &74.44 &75.87 &78.33 &77.28 \\
Fine-tuned &73.6 &74.65 &77.45 &76.59 \\
\bottomrule
\end{tabular}
\caption{Comparing before vs. after fine-tuning using video frames but without our optical flow loss on ImageNet classification (top-1 accuracy). %The performance on other tasks are almost unchanged after fine-tuning.
}\label{tab:imagenet_without_flow_loss}
\end{table}





Table~\ref{tab:uss_lc_imagenet} compares our approach with the baselines DINO and Leopart~\cite{ziegler2022leopart} on linear probe segmentation and ImageNet classification performance. The ImageNet classification accuracy is evaluated using a weighted nearest neighbor classifier (k-NN) as in \cite{wu2018unsupervised}. It should be emphasized that our approach not only boosts semantic segmentation performance compared to the baselines, but also preserves the discriminative power on ImageNet. In contrast, the discriminative power is significantly affected in Leopart despite the remarkable improvement on segmentation tasks. Table~\ref{tab:imagenet_without_flow_loss} shows the ImageNet top-1 classification accuracy after fine-tuning with video frames without our optical flow loss. It can be observed that a similar amount of performance drop occurs even without using our optical flow loss. This further suggests that the optical flow loss may not be the main reason for the accuracy drop observed in Table~\ref{tab:uss_lc_imagenet}.