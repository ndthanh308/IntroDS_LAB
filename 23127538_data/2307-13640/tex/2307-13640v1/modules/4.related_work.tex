\section{Related work}

\noindent\textbf{Self-supervised Learning.} 
Learning self-supervised visual representations has drawn significant interest in 
computer vision. Early work was based on learning through solving pretext tasks such as inpainting, 
jigsaw puzzles, and colorization~\cite{zhang2016colorful, pathak2016context, noroozi2016unsupervised}. Recent major successes are broadly based on
momentum-based contrastive learning~\cite{he2020momentum, chen2020improved, caron2021emerging}, and 
masked auto-encoding~\cite{he2022masked}, and natural language supervision~\cite{radford2021learning}. In particular, DINO~\cite{caron2021emerging} showed that self-supervised features obtained through vision transformer (ViT) architectures and self-distillation explicitly contain 
scene layout and boundary information.
These emerging properties inspired pioneering work on unsupervised object discovery and semantic segmentation based on existing
self-supervised ViT models \cite{van2021maskcontrast, van2022discovering, melas2022deepspectral, hamilton2022stego, ziegler2022leopart, tokencut2022}. While existing works employ pretrained ViT models as a sub-component of larger systems, our approach provides a loss function that can be seamlessly incorporated within existing self-supervised frameworks. 


% Not only did we obtain the state-of-the-art 
% unsupervised segmentation performance, our method also preserves the discriminative power 
% of the original model on ImageNet, which is a key model capacity measure \cite{kornblith2019better}
% while previous USS methods mostly disregard. 
% Additionally, our method delivers improvements on two self-supervised training paradigms \cite{caron2021emerging, he2022masked},
% while existing work narrowly focuses on the framework from \cite{caron2021emerging}.


%%%%%%%%%%%%%

\vspace{1em}

\noindent \textbf{Optical Flow.} Optical flow is defined as the per-pixel motion between adjacent video frames. This concept was introduced to describe the visual stimulus of moving objects~\cite{lucaskanade1981iterative}. Based on the object continuity property, optical flow has drawn constant interest in video object segmentation~\cite{zhou2020matnet, liu2021emergence, yang2021motiongroup, guesswhatmove}.
With the recent advances in deep learning~\cite{lecun2015deep}, and the success in learning  optical flow from 
 synthetic datasets~\cite{flyingchairs2015dosovitskiy, flyingthings2016mayer}, off-the-shelf dense optical flow estimation networks have now become easily available~\cite{raft2020teed}. For example, CMP~\cite{zhan2019condmotionprop} predicts optical flow as a pretext task for self-supervised learning, and 
CrossPixel~\cite{mahendran2019cross} embeds pixels to match similarity of corresponding flow vectors. While these early work emphasize learning general representations from motion, few attempts have been made to leverage optical flow in learning for unsupervised object localization and semantic segmentation of still images. 


\vspace{1em}

\noindent \textbf{Unsupervised Object Localization.} Object localization refers to the prediction of  bounding boxes of foreground objects in images. Early unsupervised techniques for learning object localization focused on mining co-occurring patterns amongst image collections~\cite{vo2021large, vo2020toward, cho2015unsupervised, vo2021large}.
Recent work explores mining examples from single images~\cite{collins2018deep, zhang2020object}. Significantly increased performance has been achieved with graph-based partitioning procedures that use pre-trained ViT architectures~\cite{simeoni2021lost, tokencut2022, melas2022deepspectral}. In this work, we build upon existing localization procedures, and show the significant benefit of fine-tuning ViT features with optical flow.



\vspace{1em}

\noindent \textbf{Unsupervised Semantic Segmentation.} Unsupervised semantic segmentation involves generating pixel-level prediction that can be closely mapped to semantic labels through clustering or linear projection. 
A popular paradigm is to extract structures from images, including foreground masks~\cite{nguyen2019deepusps, qin2019basnet, chen2019unsupervised},  contours~\cite{arbelaez2010contour} or invariant mapping under transformation~\cite{ji2019invariant}, then use the structures to guide the learning of pixel-level embeddings~\cite{ji2019invariant, van2021maskcontrast, ke2022multiview, van2022discovering}. Another rising approach originates from discovering emergent object information from DINO ViTs~\cite{caron2021emerging}. For example, both Deep-Spectral~\cite{melas2022deepspectral} and STEGO~\cite{hamilton2022stego} achieve impressive segmentation results using features of frozen ViTs with spectral clustering and self-training. Leopart~\cite{ziegler2022leopart} achieves significant improvement by leveraging visual appearance continuity and incorporating cluster assignment loss~\cite{caron2020unsupervised} into DINO's self-supervised framework.
% Our method follows this latter paradigm while using motion rather than visual continuity. We therefore preserve the discriminative power  of original model, and our formulation does not tie to DINO framework.
Our method utilizes a loss fomulation to finetune pretrained ViTs with motion cues from optical flow. We increase the performance on unsupervised semantic segmentation while preserving the discriminative power of original ViTs, and our loss formulation does not depend on DINO's self-supervised framework.
% \textcolor{red}{This last sentence is unclear}. \xy{updated}




\vspace{1em}

\noindent \textbf{Vision Transformers.} Transformer architectures are the key behind the recent significant  success in natural language processing~\cite{vaswani2017attention}. Vision transformers (ViT) employ positional embedding and self-attention layers instead of convolutional layers~\cite{dosovitskiy2020image}.
Recent variants of ViT have demonstrated various advantages over traditional CNN architectures, including
higher computational efficiency~\cite{liu2021swin}, improved self-supervised learning efficacy~\cite{chen2020improved, caron2021emerging},
stronger performance on downstream vision tasks~\cite{carion2020detr, xie2021segformer}, and more interpretable features~\cite{raghu2021vit_vs_cnn}.
In this work, our technique is applied to ViTs. 








% Figure environment removed