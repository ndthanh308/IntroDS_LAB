\section{Introduction}

 The ability to localize and recognize objects in images is crucial for intelligent robots to effectively operate in the real world~\cite{spelke2007core}. 
A feature representation that can distinguish and localize different semantic entities in a given image is important for building downstream algorithms, and for making the robot's behavior naturally more interpretable by humans~\cite{kulkarni2019unsupervised}.
Dense prediction tasks, such as detection and segmentation, are downstream tasks that are particularly important in robotics. 
%in describing the ability of visual structure understanding. 
The current prevailing approach to these tasks is to train deep neural networks with large amounts of human-labeled dense image annotations.
Despite the development of self-supervised learning~\cite{he2022masked}, and the utilization of vast amounts of Internet images and descriptions~\cite{radford2021learning}, densely annotated datasets are still scarce and obtained through manual annotation, which is not only a labor-intensive and expensive process, but also constrained to a fixed set of object categories without the ability to discover new objects. Therefore, unsupervised learning of dense prediction tasks without labeled data, including detection and segmentation, is an important open research challenge.


Current methods for unsupervised dense image understanding generally involve discovering basic structures such as foreground masks~\cite{nguyen2019deepusps, qin2019basnet, chen2019unsupervised}, contours~\cite{arbelaez2010contour}, or invariant mappings under transformations~\cite{ji2019invariant}. These structures then guide the learning of pixel-level embeddings, enabling the spatial differentiation of different objects~\cite{ji2019invariant, van2021maskcontrast, ke2022multiview, van2022discovering}. A new paradigm of unsupervised dense prediction has emerged in the past year to leverage self-distilled vision transformers (ViTs)~\cite{caron2021emerging}. Deep-Spectral\cite{melas2022deepspectral} and STEGO~\cite{hamilton2022stego} have achieved state-of-the-art segmentation results through spectral clustering and self-training on top of features extracted from frozen ViTs. Leopart~\cite{ziegler2022leopart} incorporates spatial feature clustering into the training of ViTs. These pioneering works apply the principle of visual appearance continuity as object cues on self-supervised features, pushing the boundaries of unsupervised image understanding to complex images.


In this paper, we draw inspiration from the common fate principle~\cite{koffka2013principles}, which posits that pixels tend to belong to the same object if they move in the same direction at the same speed, i.e., if they have the same optical flow. Optical flow has been extensively used for video object segmentation and tracking for its ability to easily capture moving objects~\cite{zhou2020matnet, liu2021emergence, yang2021motiongroup, guesswhatmove}. 
However, few attempts have been made to transfer the motion information in optical flow to the task of localizing and segmenting objects in still images. 
Rather than using optical flow for object tracking in videos, our approach is to mimic the human ability to observe objects moving patterns and learn transferable objects concept  for static images.

We follow the assumption studied in CrossPixel~\cite{mahendran2019cross}: \textit{pixels sharing similar motion are likely to belong to the same object and vice versa}. We refine this assumption by only considering pixels within a local neighborhood, removing background motion, and concentrating learning on regions with substantial movements. 
Our approach utilizes optical flow as an auxiliary regularization for ViT features in self-supervised learning to encourage the ViT network to produce similar features in locations that exhibit similar pixel motions.
Specifically, we first estimate optical flows from adjacent frames in existing unlabeled raw video datasets~\cite{wang2021unidentified, miao2021vspw, xu2018youtube} using off-the-shelf optical flow model~\cite{raft2020teed}. We then split feature and optical flow maps into local patches. For each patch, we minimize the KL divergence between the feature cosine similarity and the flow similarity measured with a customized RBF kernel. We use this flow-based loss function to fine-tune the vision transformers proposed in DINO~\cite{caron2021emerging}. We evaluate the features from the fine-tuned networks in two downstream tasks: (1) the latest unsupervised object localization procedure proposed in~\cite{melas2022deepspectral}, and (2) the unsupervised semantic segmentation evaluation protocols proposed in~\cite{ziegler2022leopart, van2021maskcontrast}. We demonstrate increased performance over the original ViT networks across these unsupervised dense vision prediction tasks. Our proposed approach outperforms the state-of-the-art in unsupervised semantic segmentation on Pascal VOC 2012 and Cocostuff-27 through linear probing, while preserving the discriminative power on ImageNet. We summarize our contributions as follows:

\begin{enumerate}
    \item  A new unsupervised fine-tuning procedure using optical flow that leverages the correlation between motion and objectness to encourage self-supervised ViT features to become closer if their corresponding spatial locations share similar flows in a local vicinity.
    
    \item Implementation and evaluation of the proposed procedure in the DINO self-supervised framework~\cite{caron2021emerging}. We demonstrate 
    increased performance over original networks across unsupervised object localization and semantic segmentation tasks, and outperform state-of-the-art techniques in unsupervised semantic segmentation through linear probing.
\end{enumerate}
