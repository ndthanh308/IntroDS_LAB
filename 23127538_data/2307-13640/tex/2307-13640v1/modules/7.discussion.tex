\section{Discussion and Conclusion}
\label{sec:discussion}

We have shown that improved object understanding can be achieved for certain self-supervised learners through learning from motion information that is embedded in adjacent video frames. This information is readily available from off-the shelf  optical flow estimators. 
Some open questions are, however, still worth discussing. 
The background motion removal through mean reduction is likely to leave extra background motions around the image corners due to camera distortion, and it may also  
 diminish small object movements. 
 Despite the generality of our procedure and its independence of DINO's loss, our current implementation and experiments are still closely linked to the self-supervision training of DINO. This suggests the potential for designing more general modules that could translate motion into objectness information, which can be agnostically digested by other networks.


As visual continuity and motion are both intrinsic clues for determining objectness, 
the possibility of unifying them in a single framework has yet to be fully explored, while the lack of quality video datasets like ImageNet will likely continue to be a limiting factor.
Beyond only leveraging adjacent frames, it is possible to extract long-term spatial-temporal correspondences from videos to further improve representation learning for still images.


% may be useful for emergent condition