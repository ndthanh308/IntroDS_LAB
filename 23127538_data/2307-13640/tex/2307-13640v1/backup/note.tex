
\begin{comment}

Optical flow is computationally defined as the per-pixel motion between adjacent frames, whose concept was introduced 
to describe the visual stimulus provided to animals moving through the world.

Lucas Kanada method is one of the most well-known optical flow estimation is , 
its main assumption is local smoothness assumption, i.e., flow is constant within pixels of a local neighbor.
but it could only provide sparse esitmation. 

Dense approaches like home-shock, but at cost of noisy output. 

Recent advances in deep learning enables data-driven approaches.
Despite still limited by fast-moving objects,..., deep learning approaches provide much better dense optical flow estimation

Because optical flow kind of describe the object information, successful attempts to use them in self-supervised learning is surprising scarse.
predict motion -> shall not used alone, because motion and visual information can hardly correlate
cross -> suffer from global smoothness in traditional methods. 

while we provide a simple way of using optical flow on top of existing SSL frameworks, preserving perf 
+ achieving good USS perf (extract the object information)

Hornâ€“Schunck method \cite{horn1981determining}

\end{comment}





\begin{comment}
% shall be a very short intro

Vision transformer (ViT) has been marked with great successes over the past few years \general_citations.

Unlike CNN which applying convolution over input, ViT architectures embed the image to features a lower resolution , applying positional embedding, 
and apply self-attention layers. 

Numerous variants of ViT have been proposed which deliver higher effiency, better self-supervised learning efficacy, better detection / segmentation performance
and ability to automatically attend foreground objects. In this work, given the benefits of ViT over SOTA cnn architectures, we focus our methods on ViT. 

\end{comment}


\begin{comment}


draw the idea of visual continuity, can be generalized as a form of data distillation
dog face, and dog body, frequently appear continuisly in visual signals, then we could group them as a single object.
but this would rely on the SSL model to be able to pick up this information. 

% **message**: previous methods if they freeze, then they could not reach higher accuracy, but if they learn
% they lose performance on imagenet. because of our simplicity, we can outperform + preserve the performance on imagenet


% **message**: optical flow information is widely used in video object tracking, because of its objectness information
% but here instead of using them for tracking during inference. We use them for training, in another words, we draw on the 
fact that people can segment each object on a static images, but just because we only learn from static images, but have seen them moving. 
% we emulate the natural learning
% scenaria people can segment stuff from a static images, but this ability partly comes from sees object moving. 


% **message**:  previous methods mostly draw from the idea of visual continuity, so they clustering and create peseudo labels and use
% the label information to improve semantic segmentation perf. But we pick another path, use motion information, and we argue that our method
% is simpler.
% from a long-term learning intelligent agent, we shall both rely on these two information sources, but we argue that the object movement provide the 
% most immediate, and less-biased information and could potentially serve as an important role in building an intelligent AI. 

\end{comment}




% The constraints we have presented here possess a close analogy to the foundational principles of the Lucas-Kanade Optical flow method. Lucas-Kanade method assumes the flow remains constant in a local neighbourhood of the pixel under consideration, and we  focus on a local vicinity and assumes flow similarity indicates the likelihood of belonging to the same object.
% At the same time, we concentrate on pixels with substantial motion, and Lucas-Kanade method only estimates sparse optical flows  on interest points that are selected by traditional visual features. 


\begin{comment}

for the second point, we shall only create learning signals at locations where motions exist

for the third point, we need to consider the relative instead of the absolute movement 

Our core idea:

In a local neighbourhood, locations sharing similar non-negligible motion relative to camera are likely to belong to the same object and vice versa. 


To leverage this concept for representation learning, we could bridge the motion similary to feature similarity 
for contrastive learning, based on our assumption that motion similarity corresponds 
to the likelihood of belonging to the same object. 

\end{comment}



$$
S_f (x, y) = \parallel \mathbf{y} \parallel_2 \exp (- \cos(\mathbf{x} , \mathbf{y})  / \sigma )
$$



$$
\mathcal{L} =  \sum_{p=1}^L w_{p}  D_{KL}(\mathbf{p}_{v,p} \parallel \mathbf{p}_{f,p}) 
$$

$$
w_p = \frac{\parallel \mathbf{v}_p \parallel}{\sum_{p=1}^L \parallel \mathbf{v}_p \parallel}
$$


$$
\mathbf{p}_{\cdot,p} = \text{softmax}( \mathbf{z}_{\cdot, p} / \tau)
$$

$$
\mathbf{z}_{v,p} = [..., S_f(\mathbf{v}_{p, s_p} \cdot \mathbf{v}_{p,i}),...]^\top
$$

$$
\mathbf{z}_{f,p} = [..., \mathbf{f}_{p, s_p} \cdot \mathbf{f}_{p,i},...]^\top  \text{ where } i\in [K^2]
$$

$$
\frac{\mathbf{v} - \overline{\mathbf{v}}}{\parallel \mathbf{v} - \overline{\mathbf{v}} \parallel_\infty}
$$


% consider cut the title (remove `self-supervised learners`)

