% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[]{ACL2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{graphicx}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Turkish Native Language Identification}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Ahmet Yavuz Uluslu \\
  Universität Zürich \& PRODAFT \\
  \texttt{ahmetyavuz.uluslu@uzh.ch} \\\And
  Gerold Schneider \\
  Universität Zürich \\
  \texttt{gschneid@cl.uzh.ch} \\}

\begin{document}
\maketitle
\begin{abstract}
In this paper, we present the first application of Native Language Identification (NLI) for the Turkish language. NLI involves predicting the writer's first language by analysing their writing in different languages. While most NLI research has focused on English, our study extends its scope to Turkish. We used the recently constructed Turkish Learner Corpus and employed a combination of three syntactic features (CFG production rules, part-of-speech n-grams and function words) with L2 texts to demonstrate their effectiveness in this task. 
\end{abstract}

\section{Introduction}

Native Language Identification (NLI) is the task of automatically identifying the native language (L1) of an individual based on their linguistic productions in another language (L2). The underlying hypothesis is that the L1 influences learners’ second language writing as a result of the language transfer effect \citep{yu2016new}. It is used for a variety of purposes including forensics applications in cybercrime \citep{perkins2021application} and secondary language acquisition \citep{swanson2014data}. 

Research in NLI is mainly conducted with learner corpora, which comprise collections of writings by individuals learning a new language. These writings are annotated with metadata such as the author's native language (L1) or their fluency level. Recent NLI studies on languages other than English include Portuguese \citep{malmasi2018portuguese}, Arabic \citep{malmasi2014arabic}, and Chinese \citep{malmasi2014chinese}. The learner corpus is the backbone of NLI research, which means that extending research to a novel language depends on acquiring the appropriate learner corpora for that language. In the past, studies have focused on L2 English because of the prominence of this language in language research and the relatively large amount of data available. To the best of our knowledge, this study presents the first detailed NLI experiments on L2 Turkish. We employ the recently constructed Turkish Learner Corpus (TLC) \citep{anna2022error} and investigate widely used linguistic features for NLI. The remainder of the paper is organised as follows: Section 2 discusses related work in NLI, Section 3 and 4 describes the methodology and dataset used in our experiments, and Section 5 presents the experimental results. Finally, Section 6 presents a brief discussion and concludes this paper with directions for further research.

\section{Related Work}
NLI is typically modeled as a supervised multi-class classification task. In this experimental design, the individual writings of learners are used to train a model while the author’s L1 information serves as class labels. A variety of feature types at the syntactic and lexical levels were studied to capture distinct characteristics of the language interference phenomenon: spelling errors, word and lemma n-grams, dependency parsing, and morphosyntax. A more detailed review of feature extraction-based methods can be found in two shared task reports on the NLI task organised in 2013 and 2017 \citep{tetreault2013report, malmasi2017report}.

In recent years, there has been increased experimentation with deep learning methods, including pre-trained transformers \citep{steinbakken2020native} and generative models \citep{lotfi2020deep}. While these models slightly outperformed the state-of-the-art performance achieved by feature-based stacked classifiers, questions about their interpretability, inherent biases, and practical shortcomings in industrial applications remain unexplored. Traditional methods based on hand-crafted features continue to be preferred in many implementations due to their simplicity in training and resource efficiency. Within this context, \citet{uluslu2022scaling} approached the NLI scalability problem in the context of cybercrime through the use of adapter fine-tuning.  

\section{Data}
In this study, we use data from the TLC \citep{anna2022error}. TLC is a learner corpus composed of the writings of learners of Turkish. These texts are essays written as part of a test of Turkish as a secondary language. Each text includes additional metadata such as the nationality of the author and the genre of the text. The corpus also includes error codes and corrections, although we do not make use of this information. 

We used a subset of the dataset containing texts for five L1 groups: Arabic (ARA), Albanian (AL), Azeri Turkish (AZ), Farsi (IR), and Afghani (Pashto) (AFG). We chose these five languages mainly because of the immigration trend observed in Turkey, which will result in a need for additional capabilities for cybercrime forensics and language learning applications for educational institutions. We limit our study to the genre of essays. The other genres (letters of different forms) in the corpus are unbalanced and scarce which may introduce linguistic biases across different registers. We also do not attempt to adjust the dataset based on the writing prompts because they were unbalanced across languages. We randomly selected sentences from the same L1 and combined them to produce documents of approximately the same length. This methodology ensures that the texts for each L1 are a mix of different authoring styles, topics, and language proficiency. The composition of our data is shown in Table 1. 

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{L1} & \textbf{Docs} & \textbf{Tokens} & \textbf{TTR} & \textbf{Avg Words} \\ 
\hline
AFG & 55 & 12546 & 0.47 & 278.8 \\
\hline
AL & 58 & 15001 & 0.52 & 250.6 \\
\hline
ARA & 65 & 16969 & 0.49 & 252.9 \\
\hline
AZ & 54 & 15850 & 0.47 & 273.5 \\
\hline
IR & 52 & 12870 & 0.51 & 246.4 \\
\hline
\end{tabular}
\caption{Distribution of the five L1s in terms of texts, tokens, type/token ratio (TTR) and average words.}
\label{table:language_data}
\end{table}

\section{Methodology}

\subsection{Classifier}
In our study, we use the standard supervised multi-class classification approach for NLI. A linear Support Vector Machine (SVM) is used for classification and feature vectors are created using a TF-IDF weighting scheme, in line with previous research \citep{gebre2013improving}. We initially experimented with relative frequencies but obtained better preliminary results with TF-IDF. We performed a grid search in parameter space for the regularisation parameter C in the range 10e-6 to 10e-1 and set max\_iteration = 5k to ensure model convergence. We find that the generalisation of the model reaches its limit at C = 1, we, therefore, choose this value. 

\subsection{Evaluation}
Following the previous NLI studies, we present our findings using classification accuracy through 10-fold cross-validation (10FCV), which has become the standard for NLI result reporting in recent years. Our cross-validation approach is randomised and stratified, aiming to maintain consistent class proportions across partitions. Since our dataset is slightly unbalanced, we provide detailed metrics in addition to accuracy, including precision per class, recall, and F1 values. We also compare these results to a random baseline.

\subsection{Linguistic Features}
We experimented using a combination of three syntactic features: context-free grammar (CFG) production rules, part-of-speech n-grams, and function words. Due to the imbalance in the topic distribution in the TLC corpus, we decided not to include lexical features such as word n-grams in our study. Topic bias can arise when certain subjects or topics are not equally represented across different classes \citep{brooke2013native}. For example, only two L1 backgrounds (Azeri and Farsi) were given writing prompts on topics related to the concepts of happiness and time, which means that our classifier had also been implicitly trained on these topics. Even if we attempted to balance the topics across languages, similar to the TOEFL11 dataset \citep{blanchard2013toefl11}, we found that particular rhetoric strongly influences certain backgrounds. For example, Afghani writings were predominantly religious, regardless of the topic. Therefore, we focus only on content-independent features, similar to the studies on NLI in other languages \citep{malmasi2015norwegian}. 

 \textbf{Function words} are content-independent words, including prepositions, articles, and auxiliary verbs, that play a crucial role in conveying grammatical relationships between words. It is often challenging for L2 speakers to use the appropriate function words and production errors may be due to the influence of their L1 \citep{schneider2016detecting}. These function words are recognised as valuable features for the NLI task. We extracted frequencies of 75 Turkish words from different grammatical categories. However, it's worth noting that many grammatical aspects, which are morphologically expressed in Turkish, may not be as strongly captured as they would be in English.

\textbf{Part-of-Speech tags} are linguistic categories or word classes that signify the syntactic role of each word in a sentence. They include basic categories such as verbs, nouns, and adjectives. Assigning POS tags to words in a text introduces a level of linguistic abstraction, meaning that we can work with the underlying structure rather than the content. We use the Turkish POS module from Stanza \citep{qi2020stanza} to extract universal POS tags, from which we create n-grams of sizes 1 to 3. These n-grams serve to capture preferences for specific word classes and their localised ordering patterns. Our experiments indicated that sequences of order 4 or higher lead to lower accuracy due to the limited size of our corpus. Therefore, we excluded such higher-order n-grams from our analysis.


\textbf{CFG production rules} are used to generate constituent parts of sentences, such as noun and verb phrases. We use the Turkish parsing module of Stanza \citep{qi2020stanza} to extract the constituency tree for the documents. The production rules are then extracted and each rule is used as a standalone feature. We exclude lexicalizations to focus on more abstract and general syntactic patterns. These production rules can encode highly idiosyncratic constructions that are specific to particular L1 groups. They have been widely utilized in various ensemble methods for NLI and have been shown to complement other features effectively \citep{malmasi2018native}.

\section{Results}
In this section, we present the results in terms of accuracy achieved by individual feature types. Subsequently, we report the performance obtained using the combination of all features. Finally, we examine the performance obtained by the best system for each L1 class. 

Table 2 displays the results of the systems trained with different feature types in terms of accuracy. We found that all feature types individually outperform the baseline. The CFG rules are the features that individually perform the best, achieving an accuracy of 41.4\%. This demonstrates the importance of the syntactic differences between the L1 groups. The full combination, using all feature types, obtains performance higher than CFG features achieving 44.2\% accuracy. These trends are very similar to previous research using the same features \cite{malmasi2018portuguese, malmasi2014arabic} with comparable corpora.

\begin{table}[htbp]
  \centering
  \renewcommand{\arraystretch}{1.2} % Adjust the spacing between rows
  \begin{tabular}{cc}
    \hline
    \textbf{Feature Type} & \textbf{Accuracy (\%)} \\
    \hline
    Random Baseline & 20.0 \\  % Replace 20.0 with the actual accuracy of the random baseline
    \hline
    POS 1-grams & 33.4 \\
    POS 2-grams & 38.9 \\
    POS 3-grams & 38.6 \\
    Function Words & 37.2 \\
    CFG Production Rules & 41.4 \\
    \hline
    \textbf{Full Combination} & \textbf{44.2} \\
    \hline
  \end{tabular}
  \caption{10-FCV Accuracy Classification Results}
\end{table}

Table 3 shows the results obtained for each L1 in terms of precision, recall, and F1 score, as well as the average results for the five classes. Across all classes, we obtain a micro-averaged F1 score of 0.40 and a macro-averaged F1 score of 0.43.

\begin{table}[htbp]
  \centering
    \renewcommand{\arraystretch}{1.2} % Adjust the spacing between rows
  \begin{tabular}{cccc}
    \hline
    \textbf{L1} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-score} \\
    \hline
    AFG & 0.50 & 0.29 & 0.37 \\
    AL & 0.45 & 0.54 & 0.49 \\
    ARA & 0.43 & 0.65 & 0.52 \\
    AZ & 0.47 & 0.41 & 0.44 \\
    IR & 0.37 & 0.15 & 0.21 \\
    \hline
    \textbf{Average} & 0.44 & 0.41 & 0.40 \\
    \hline
  \end{tabular}
    \caption{Full combination per-class results: precision, recall and the F1-score.}
\end{table}

To provide a visual representation of these findings and to highlight any error patterns, we present a heatmap confusion matrix of the classification errors in Figure 1. 

% Figure environment removed


In most NLI studies, the expected difficulty is to distinguish closely related L1s that may belong to the same language family. Based on the analysis of the confusion matrix, the most notable confusion occurs between Persian and Arabic, both of which have strong lexical connections to the Turkish language. However, since we are working with content-independent features, we attribute this confusion to corpus representation and do not seek any linguistic explanations. We observed no confusion between Afghani and Persian, even though both languages belong to the same language family and have strong similarities. 

Our analysis also brings attention to two potential limitations that might impede drawing connections between model errors. Firstly, the size of our corpora is relatively limited when compared to other NLI studies, such as Portuguese (5x) \cite{malmasi2018portuguese} and Norwegian (10x) \cite{malmasi2015norwegian}, which are significantly larger. This difference in data size might impact the model's generalisation capabilities. Secondly, we acknowledge that our parser might not be entirely suitable for learner language, which could introduce additional noise into the feature space \cite{van2002effect}. Depending on the corpus and experimental design, the previous studies did not always find a meaningful relationships in the error analysis. We believe that the most important finding is that by using only syntactic features, we were able to outperform the baseline and identify syntactic differences in L1 groups based on their texts in L2 Turkish.

\section{Conclusion \& Discussion}
In this study, we presented the first experiments with Turkish NLI and achieved a level of performance comparable to previous results for other languages. Our main focus was to investigate the effectiveness of syntactic features for Turkish, a language that differs from English in certain aspects, particularly in morphological complexity. Another significant contribution of our work is the introduction of a new dataset for NLI, specifically designed to address L1-based language transfer effects. This corpus can serve as a valuable resource for researchers to validate and refine their methodologies across various datasets and languages. 

We identify several promising directions for future research. Firstly, we plan to expand the corpus by incorporating more learner writings and extending the analysis to encompass other L1 languages. Additionally, we believe that assessing the proficiency level of learners can shed further light on the observed challenges.  Finally, we plan to explore more linguistically sophisticated features in our investigation. For instance, leveraging L1 mistakes from a morphological perspective as a content-independent feature could yield valuable insights. To this end, our follow-up study will incorporate a broader range of features to enhance the robustness and comprehensiveness of our analysis.

\section*{Ethics Statement}
Our study only processes information from publicly available learner corpora. We place great emphasis on protecting the privacy of individuals and ensure that no sensitive personal data is accessed, stored or processed at any stage of the project. Our research adheres to the ethical guidelines of the University of Zurich and PRODAFT.

\section*{Acknowledgements}
This work was partially supported by the collaboration between the University of Zurich and PRODAFT within the Innosuisse Innovation Project AUCH (103.188 IP-ICT).


% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\end{document}
