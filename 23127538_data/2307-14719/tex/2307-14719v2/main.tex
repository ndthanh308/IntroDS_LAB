\documentclass[lettersize,journal]{IEEEtran}
\usepackage{array}
% \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{soul}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{numprint}
\npthousandsep{\,}
\usepackage[inline]{enumitem}
\usepackage{tcolorbox}
\usepackage{float}
\usepackage[font=small,skip=0pt]{caption}
\usepackage{makecell}
\usepackage{arydshln}
\usepackage{times}
\usepackage{amsmath,bm,amssymb,amsthm,amsfonts}
\usepackage{listings}%
\usepackage[ruled,linesnumbered]{algorithm2e}
\usepackage[italic]{mathastext} 
\usepackage{graphicx}%
\usepackage{booktabs}
\usepackage{setspace,hyperref}
\usepackage{color}
\usepackage{etoolbox}
\usepackage{multirow}
\usepackage{circuitikz}
\usepackage{tikz}
\usepackage{pgfkeys}
\usepackage{cuted}
\usepackage{mathtools}
\usepackage[labelformat=simple]{subcaption}
\renewcommand\thesubfigure{(\alph{subfigure})}
\usepackage{multirow}
\usepackage[normalem]{ulem}
\useunder{\uline}{\ul}{}
\usepackage{cancel}
\usepackage{tkz-tab}
\usepackage{latexsym}
\usepackage{subcaption}


\usetikzlibrary{shapes.geometric, arrows}
\usetikzlibrary{positioning}
\usetikzlibrary{shapes.symbols}
\usetikzlibrary{angles,quotes}

\newtheorem{thm}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{coro}{Corollary}
\newtheorem{rmk}{Remark}
\newtheorem{defi}{Definition}
\newtheorem{lemma}{Lemma}

\renewcommand{\arraystretch}{1.4}

\setlength\stripsep{3pt plus 1pt minus 1pt}

\definecolor{darkgreen}{rgb}{0.1333,0.545,0.1333}
\definecolor{darkred}{rgb}{0.698,0,0}
\lstset{basicstyle=\small,%
				tabsize=4,%
				stringstyle=\ttfamily\color{darkred},%
        keywordstyle=\ttfamily\color{blue},%
        identifierstyle=\ttfamily,%
        showstringspaces=false,%
        commentstyle=\ttfamily\color{darkgreen},
        numbers=left,
        stepnumber=5,
        breaklines=true}%

\hypersetup{colorlinks=true,%
            breaklinks=true,%
            linkcolor=black,%
            pdfstartview=FitH,
            pdfpagelayout=TwoColumnRight}%
%
% useful macros
%
\newcommand{\reffig}[1]{Figure~\ref{#1}}%
\newcommand{\reftab}[1]{Table~\ref{#1}}%
\newcommand{\refchap}[1]{Chapter~\ref{#1}}%
\newcommand{\refsec}[1]{Section~\ref{#1}}%
\newcommand{\ie}{i.e.,~}%
\newcommand{\eg}{e.g.,~}%
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\newcommand{\squeezeup}{\vspace{-2.5mm}}

\setlength{\parskip}{0cm}
% \renewcommand\baselinestretch{.99}
\newcommand{\RNum}[1]{\uppercase\expandafter{\romannumeral #1\relax}}
\setlength{\textfloatsep}{0pt}
\tikzstyle{box} = [rectangle, rounded corners, minimum width=3cm, minimum height=1cm,text centered, draw=black, fill=red!30,text width=4.5cm]
\tikzstyle{boxwithin} = [rectangle, rounded corners, minimum width=3cm, minimum height=1cm,text centered, draw=black, fill=cyan,text width=4.5cm]
\tikzstyle{decision} = [diamond,aspect=5, minimum width=2cm, minimum height=0.5cm, text centered, draw=black, fill=green!30,text width=3.5cm]
\tikzstyle{arrow} = [thick,->,>=stealth]
\tikzset{
    block/.style={rectangle, draw, line width=0.5mm, black, text width=5em, text centered,                 rounded corners, minimum height=2em},
    line/.style={draw, -latex}
}% <- if you insist in using this in the document add this % here.
\input{irs.tex}
\input{basestation.tex}
\input{mimoshapes}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\trace}{tr}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\vect}{vec}
% updated with editorial comments 8/9/2021

\begin{document}

\title{Bayesian Algorithms for Kronecker-structured Sparse Vector Recovery With Application to IRS-MIMO Channel Estimation}

\author{Yanbin He and Geethu Joseph
        % <-this % stops a space ~\IEEEmembership{Staff,~IEEE,}
\thanks{The material in this paper was presented
in part at the IEEE International Conference on Acoustics, Speech, \& Signal Processing (ICASSP), June 2023, Rhodes, Greece~\cite{he2022structure}.

The authors are with the Signal Processing Systems group, Electrical Engineering, Mathematics, and Computer Science faculty, at the Delft University of Technology, The Netherlands. Emails:$\{\text{y.he-1, g.joseph}\}\text{@tudelft.nl}$.}% <-this % stops a space
% \thanks{Manuscript received April 19, 2021; revised August 16, 2021.}
}

% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2021}%
{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

\IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

\maketitle

\begin{abstract}
We study the sparse recovery problem with an underdetermined linear system characterized by a Kronecker-structured dictionary and a Kronecker-supported sparse vector. We cast this problem into the sparse Bayesian learning (SBL) framework and rely on the expectation-maximization method for a solution. To this end, we model the Kronecker-structured support with a hierarchical Gaussian prior distribution parameterized by a Kronecker-structured hyperparameter, leading to a non-convex optimization problem. The optimization problem is solved using the alternating minimization (AM) method and a singular value decomposition (SVD)-based method, resulting in two algorithms. Further, we analytically guarantee that the AM-based method converges to the stationary point of the SBL cost function. The SVD-based method, though it adopts approximations, is empirically shown to be more efficient and accurate. We then apply our algorithm to estimate the uplink wireless channel in an intelligent reflecting surface-aided MIMO system and extend the AM-based algorithm to address block sparsity in the channel. We also study the SBL cost to show that the minima of the cost function are achieved at sparse solutions and that incorporating the Kronecker structure reduces the number of local minima of the SBL cost function. Our numerical results demonstrate the effectiveness of our algorithms compared to the state-of-the-art.
\end{abstract}

\begin{IEEEkeywords}
Compressed sensing, sparse Bayesian learning, block sparsity, alternating minimization, singular value decomposition, Kronecker product, convergence analysis, IRS-aided MIMO channel estimation
\end{IEEEkeywords}

\section{Introduction}\label{sec:intro}
\IEEEPARstart{T}{he} basis expansion model (BEM) is widely used in various fields, such as image processing \cite{he2009exploiting,he2009tree,ma2003maximum,barhumi2005mmse} and wireless communications \cite{giannakis1998basis,han2018low,zheng2022survey}, to obtain flexible linear representations for non-linear functions. It is achieved by approximating non-linear functions as linear combinations of simple \emph{basis functions} \cite{hastie2009elements}. BEM comprises a linear model with a \emph{dictionary} of basis functions and \emph{coefficients} associated with the dictionary. This paradigm, united with compressed sensing (CS), can be employed to estimate the unknown parameters of non-linear functions. The idea is to sample the unknown parameter over a range to construct known functions that comprise an over-complete dictionary. Here, only a few basis functions corresponding to the true parameters are activated, resulting in sparse coefficients. Thus, CS techniques can be leveraged to reconstruct the sparse coefficients from the linear model. Further, many signal representation problems in wireless communications \cite{araujo2019tensor,chang2021sparse,xu2022sparse,he2022structure} and image processing \cite{zhao2019exploiting,yang2015compressive} applications use multidimensional BEM \cite{caiafa2012block,caiafa2013computing,duarte2010kronecker,duarte2011kronecker,caiafa2013multidimensional}. These models can be represented using a sparse vector with Kronecker-structured support, i.e., the support of the sparse vector is the Kronecker product of several low-dimensional support vectors. This work investigates the CS problem of solving a high-dimensional underdetermined linear system of equations to find a Kronecker-structured sparse solution.



% Further, in many cases, the dictionary is Kronecker-structured, i.e., is the Kronecker product of multiple matrices. This arises in the context of multidimensional signals \cite{caiafa2012block,caiafa2013computing,duarte2010kronecker,duarte2011kronecker,caiafa2013multidimensional} and is naturally revealed after the vectorization of multidimensional signal model. In this work, we are going to investigate the linear model with such structured dictionary.

The multidimensional basis expansion models are also typically associated with Kronecker-structured dictionaries, i.e., the Kronecker product of multiple dictionary matrices. For example, consider the channel estimation problem for the intelligent reflecting surfaces (IRS)-aided wireless communication systems \cite{he2022structure}. The BEM for the unknown channel matrix is constructed by sampling pre-defined spatial angle grids and forming a dictionary using the corresponding steering vectors. Then, the channel matrix can be represented using a three-dimensional sparse BEM coefficient vector where the three dimensions (mode) are the angle-of-departure (AoD), angle-of-arrival (AoA), and difference of the AoA and AoD at the IRS. Furthermore, different combinations of AoDs and AoAs naturally elicit the Kronecker product, leading to a Kronecker-structured dictionary and sparse coefficients. Motivated by such multi-parameter estimation problems, we consider the following Kronecker-structured linear inversion problem,\IEEEpubidadjcol
\begin{equation}\label{eq.problem_basic}
    \bm y = \bm H \bm x+\bm n,
\end{equation}
where $\bm y \in \mathbb{C}^{\bar{M} \times 1}$ is the noisy measurement, $\bm H \in \mathbb{C}^{\bar{M} \times \bar{N}}$ is the Kronecker-structured dictionary with $\bar{M} < \bar{N}$, $\bm x \in \mathbb{C}^{\bar{N} \times 1}$ is the unknown sparse BEM coefficient vector, and $\bm n$ is the measurement noise. Specifically,
\begin{equation}\label{eq.separable_dict}
    \bm H = \otimes_{i=1}^I \bm H_i,
\end{equation}
where $\bm H_i \in \mathbb{C}^{M_i \times N_i}$ with $\prod_{i=1}^I M_i = \bar{M}$, and $\prod_{i=1}^I N_i = \bar{N}$. The sparse vector $\bm x$ possesses a Kronecker-structured support vector given by $\otimes_{i=1}^I \bm b_i\in\{0,1\}^{\bar{N}\times 1}$ where $\bm b_i \in \{0,1\}^{N_i \times 1}$ is a binary (support) vector. We aim to reconstruct the sparse vector $\bm x$ given the Kronecker-structured dictionary $\bm H$, noisy measurement $\bm y$ by exploiting its 
%and prior knowledge of 
Kronecker-structured support.

Various signal processing techniques have been proposed to reconstruct the sparse vector $\bm x$ from the linear measurement $\bm y$ and Kronecker-structured dictionary $\bm H$. \cite{caiafa2013computing} designed a greedy method called Kronecker-orthogonal matching pursuit (KOMP) to generalize the traditional OMP for multidimensional signals. 
%Kronecker-OMP is similar in spirit to OMP and uses the notion of coherence extended to multidimensional signals. 
Like OMP, the KOMP algorithm has low complexity but requires hand-tuning of a sensitive stopping threshold. Recently, parameter tuning-free approaches based on sparse Bayesian learning (SBL)  were studied \cite{chang2021sparse}. SBL is known to have superior performance~\cite{wipf2004sparse} and flexibility to incorporate several additional structures along with sparsity \cite{fang2014pattern,wang2018alternative,prasanna2021mmwave,wu2022clustered}. The Kronecker structure can also be incorporated into the SBL framework. \cite{zhao2019exploiting} introduced the SBL approach for the Kronecker structure in~\eqref{eq.separable_dict} with $I=3$, using tensor-wise hyperparameters. The linear inversion problem with $I=2$ was considered in \cite{bualtoiu2021sparse}. This framework was later generalized to the $I$-dimensional tensor and applied to wireless communications \cite{chang2021sparse,xu2022sparse}. However, the derivation of the SBL algorithm in \cite{chang2021sparse,xu2022sparse} relied on several approximations leading to a suboptimal recovery accuracy. Hence, we seek novel Bayesian algorithms exploiting the Kronecker and sparse structures to improve reconstruction accuracy and efficiency.

Furthermore, the existing Bayesian algorithms exploiting the Kronecker structure \cite{chang2021sparse,xu2022sparse} lack theoretical guarantees on their convergence and performance, and these properties are only demonstrated empirically. 
% In this work, we target to develop a novel approach for~\eqref{eq.problem_basic} with~\eqref{eq.separable_dict} and~\eqref{eq.kron_vector}, and aim to offer theoretical guarantees on the proposed approach.
% The convergence guarantee for the classic SBL is inherited from the expectation-maximization (EM) algorithm. This is because the classic SBL, as in \cite{wipf2004sparse}, relies on the EM algorithm for sparse vector inference. 
% Therefore, the convergence property of the SBL is closely related to that of the EM algorithm.
 % SBL incorporates a parameterized prior to encourage sparsity and estimates the sparse vector through the maximum a posteriori estimator. To compute the posterior distribution, the so-called hyperparameters of the prior are determined using the Type-\RNum{2} maximum likelihood (ML) estimation. However, closed-form solutions are usually unavailable. Therefore, the EM algorithm is implemented to provide a surrogate solution. The E-step provides a lower bound for the likelihood (or log-likelihood), and the M-step maximizes this lower bound. Thus, the convergence property of SBL closely relies on that of the EM algorithm. 
% The convergence property of the EM algorithm has been extensively discussed in \cite{wu1983convergence}. And it is concluded that under specific conditions, EM guarantees convergence to the stationary points of the SBL cost function.
The study on convergence is limited because the convergence guarantee of the classic SBL using the expectation-maximization (EM) algorithm cannot be trivially extended for these algorithms. \cite{xu2022sparse,chang2021sparse} claimed that the solution to the underlying optimization problem should be obtained at the stationary point of the cost function. However, it is unclear whether the stationary point can be reached due to its iterative nature and approximations. 
%Also, achieving the stationary point is insufficient to guarantee convergence of the EM algorithm while a sufficient condition to arrive at the global maximum~\cite{wu1983convergence}. 
Similarly, the performance improvement due to incorporating the Kronecker structure into the SBL has been shown in \cite{chang2021sparse} without any theoretical justification.
% by comparing with other sparse recovery techniques in terms of recovery error. 
% Intuitively, better performance is induced by the reduced feasible solution space. Specifically, solving~\eqref{eq.problem_basic} with the classic SBL may return un-Kronecker-supported solutions, which can be eradicated by imposing Kronecker structure in the inference process. However, this claim lacks theoretical evidence. 
%However, there is no theoretical justification for why the new formulation outperforms the classical SBL. 
Given the shallow theoretical analysis of the existing algorithms and analysis of the SBL cost function, we make progress on these problems by analyzing our new Bayesian algorithms.

\emph{Our contributions:} We devote this paper to the algorithm development and convergence analysis of algorithms for the Kronecker-structured sparse recovery problem in~\eqref{eq.problem_basic}.
% We proposed the alternating-minimization (AM)-based and singular value decomposition (SVD)-based approaches to solve the non-convex problem in the M-step. This work will first review the algorithms in \cite{he2022structure} and generalize them to multiple hyperparameters for notation introduction and the block sparsity scenario. Our focus will then shift to the convergence properties of the AM-based KroSBL algorithm and the discussion on the local minima of the SBL cost function. In the end, we provide numerical simulation results and discussions. 
Our main contributions are as follows:
\begin{itemize}[leftmargin=*]
    \item \emph{Algorithm development:} We present our new Bayesian recovery algorithms in Sec.~\ref{sec:sbl}. We first present two novel SBL algorithms for Kronecker-structured sparse recovery called KroSBL. The first KroSBL algorithm, which is based on alternating minimization (AM), solves the underlying optimization problem of the SBL algorithm using the AM procedure. The second KroSBL algorithm, based on singular value decomposition (SVD),  is faster and uses a simple approximation to obtain the SBL algorithm. 

    \item \emph{Application:} We apply our problem to a prototypical application of IRS-aided MIMO channel estimation in Sec.~\ref{sec:channelesti}. Besides the Kronecker-structure sparsity, the BEM representation of IRS-cascaded channels also exhibits block sparsity where the nonzero entries occur in clusters. To handle this additional structure, we extend our AM-based algorithm for Kronecker-structured block sparsity based on non-negative least squares.
    \item \emph{Convergence guarantee:} We derive convergence guarantees for the AM-based algorithm in Sec.~\ref{sec.con_AM}. We establish that \emph{i)} the AM procedure can attain the stationary point of the cost function in the M-step, and $ii)$ the AM-based algorithm is guaranteed to converge to the stationary point of the SBL cost function in the noisy setting.
    % , by using the property of the generalized EM (GEM) algorithm \cite{wu1983convergence,dempster1977maximum}.
    
    \item \emph{Cost function analysis:} We examine the local minima of the KroSBL cost function in Sec.~\ref{sec:local_minima}. Assuming the sparse vector in~\eqref{eq.problem_basic} to satisfy $\bm x = \otimes_{i=1}^I \bm x_i$, we prove that all local minima are sparse in the noiseless case. Besides, we demonstrate that incorporating the Kronecker structure in the hyperparameter vector as the KroSBL cost function can significantly reduce the local minima under the unique representation property (URP) assumption for $\bm H$.

    \item \emph{Numerical Results:} We assess the our schemes in two scenarios in Sec.~\ref{sec:numsimu}. Firstly, we study the sparse recovery performance of the presented schemes against algorithms in the literature to demonstrate its superior recovery accuracy and run time. Secondly, we conduct a case study on channel estimation for IRS-aided systems with our approach and illustrate that incorporating block sparsity in our scheme can further enhance performance.
    
\end{itemize}

Overall, we present two algorithms for sparse signal recovery that arises in BEM with multiple unknown parameters. 
%algorithms are useful for sparse signal recovery that arise in a basis expansion problem with multiple unknown parameters. The 
The first algorithm, AM-KroSBL, enjoys solid theoretical guarantees, while the other algorithm, SVD-KroSBL, is computationally light and practically more relevant. %Presented analyses can shed light on understanding the behavior of structure-aware SBL.

\emph{Notation:} Boldface small letters denote vectors, and boldface capital letters denote matrices. The symbols $[\bm x]_i$, $[\bm X]_{i}$, and $[\bm X]_{ij}$ represent the $i$-th entry of vector $\bm x$, the $i$-th column of matrix $\bm X$, and the entry on the $i$-th row and $j$-th column of matrix $\bm X$, respectively. We denote the all-one vector with length $N$ as $\bm 1_N$. The symbol $\|\cdot\|_p$ denotes the vector $\ell_p$ norm. We use $\bm x > 0$ (or $\bm x \geq$) to denote that all the entries in $\bm x$ are positive (or non-negative). %The operator $\min(\cdot)$ denotes the smallest entry of a vector. 
Depending on the argument, the vector element-wise inversion or matrix inversion is denoted as $(\cdot)^{-1}$. If the argument is a vector, operator $\diag(\cdot)$ returns a diagonal matrix with the argument along the diagonal, and it returns a vector of its diagonal entries if the argument is a matrix. The symbols $(\cdot)^\mathsf{T}$, $(\cdot)^*$, $(\cdot)^\mathsf{H}$, $|\cdot|$, and $(\cdot)^\dagger$ are the matrix operations of transpose, conjugate, conjugate transpose, determinant, and pseudo-inverse, respectively. Also, $\otimes$ and $\odot$ represent the Kronecker and Khatri-Rao products, respectively. The matrix $\bm I_N$ denotes the identity matrix of size $N\times N$. We use $\mathcal{CN}(\bm a,\bm B)$ to denote the complex Gaussian distribution with mean $\bm a$ and covariance $\bm B$. The set of real, complex matrices of size $M\times N$ is represented by $\mathbb{R}^{M\times N}$ and $\mathbb{C}^{M\times N}$, respectively. 
% We denote the set $\{x\in\mathbb{R}\!:\!x \geq 0\}$ as $\mathbb{R}_+$ and $\{x\in\mathbb{R}\!:\!x > 0\}$ as $\mathbb{R}_{>}$.

% {\color{red} I see $\bm 1$ and $\bm I$ instead of $\bm 1_N$ and $\bm I_N$ at several places. So mention $\bm 1_N$ and $\bm I_N$ instead of $\bm 1$ and $\bm I$ here}

\section{Kronecker-structured Sparse Bayesian Learning}
\label{sec:sbl}
% This section presents the details of KroSBL. We will discuss sparse recovery algorithms for~\eqref{eq.problem_basic}, including two algorithms based on AM and SVD. We will then extend the scenario to handle block sparsity.

We study the model in~\eqref{eq.problem_basic} with additive Gaussian noise $\bm n$ following $\mathcal{CN}(\bm 0,\sigma^2 \bm I_{\bar{M}})$. For simplicity, we assume that the noise variance $\sigma^2$ is known and $N_i = N$ for $i=1,2,\ldots,I$. This section presents new recovery algorithms to solve for $\bm x$ from \eqref{eq.problem_basic}, exploiting the Kronecker structure.

Inspired by the SBL framework~\cite{wipf2004sparse}, we impose a fictitious sparsity promoting zero-mean Gaussian prior on $\bm x$ with an unknown covariance matrix $\bm\Gamma\in\mathbb{R}^{N^I\times N^I}$. We construct the covariance matrix as $\bm \Gamma = \diag(\bm \gamma)$ with $\bm \gamma = \otimes_{i=1}^I \bm \gamma_i$ and $\bm \gamma_i\in\mathbb{R}^{N\times 1}$, mimicking the Kronecker-structured support. Specifically,
% where $\bm \gamma_i\in\mathbb{R}^{N\times 1}$ is the unknown hyperparameter vector corresponding to the low-dimensional sparse vector $\bm x_i$, respectively. 
\begin{equation}\label{eq.sparseprior}
p(\bm x;\{\bm \gamma_i\})=\mathcal{CN}(\bm 0,\bm \Gamma),
\end{equation}
where $\{\bm \gamma_i\}$ is the simplified notation for $\{\bm \gamma_i\}_{i=1}^I$ used henceforth. Then, we turn to the type-II ML estimation, i.e., we first estimate the hyperparameters $\{\bm \gamma_i\}$, based on which the MAP estimate of $\bm x$ obtained as $\arg \max_{\bm x} p(\bm x|\bm y;\{\bm \gamma_i\})$ \cite{kreutz2024dictionaries}. The ML estimates of $\{\bm \gamma_i\}$ are obtained by minimizing the negative log-likelihood, i.e., the KroSBL cost function is given by
\begin{equation}\label{eq.cost_ml}
    \mathcal{L}\left(\{\bm\gamma_i\}\right)=-\log p(\bm y;\{\bm \gamma_i\}) = \log |\bm \Sigma_{\bm y}| + \bm y^\mathsf{H} \bm \Sigma_{\bm y}^{-1} \bm y,
\end{equation}
where $\bm \Sigma_{\bm y} = \sigma^2 \bm I_{\bar{M}} + \bm H \bm \Gamma \bm H^\mathsf{H}$ \cite{wipf2004sparse}. We note that $\bm \gamma = \otimes_{i=1}^I \bm \gamma_i=\otimes_{i=1}^I \alpha_i \bm \gamma_i$ for any $\alpha_i\!>\!0$ when $\prod_{i=1}^I \alpha_i = 1$. Thus, if $\{\bm \gamma_i\}$ maximizes~\eqref{eq.cost_ml}, then $\{\alpha_i\bm \gamma_i\}$ also achieves the maximum for any $\alpha_i>0$ with $\prod_i\alpha_i=1$. To eliminate this scaling ambiguity, we normalize the hyperparameter vectors, i.e., we impose the constraint $\|\bm \gamma_i\|_2 = 1$ for $i=1,2,\ldots,I-1$. So the ML problem to estimate $\{\gamma_i\}$ reduces to
\begin{equation}\label{eq.ml_problem}
    \underset{{\{\bm \gamma_i\}\in \mathcal{C}}}{\min} \mathcal{L}\left(\{\bm\gamma_i\}\right),
\end{equation}
where we define the constraint set
\begin{equation}
    \mathcal{C}\!=\!\left\{\{\bm \gamma_i\}\bigg|\bm \gamma_i \geq 0,i=1,2,\ldots,I,\|\bm \gamma_i\|_2=1, \forall i\neq I \right\}.
\end{equation}

The problem in \eqref{eq.ml_problem} does not admit a closed-form solution. Thus, we resort to the standard EM algorithm~\cite{wipf2004sparse} for an iterative solution. Specifically, the $r$th iteration of~EM~is
\begin{align}
&\text{\bf E-step:}~\text{Compute }\mathbb{E}_{\bm x|\bm y;\bm \gamma^{(r)}}\{\log[p(\bm y,\bm x;\{\bm \gamma_i\})]\},\label{eq.estep}\\
&\text{\bf M-step:}~\{\bm \gamma_i\}^{(r+1)} = \underset{\substack{\{\bm \gamma_i\} \in \mathcal{C}_+}}{\arg\max} ~\mathbb{E}_{\bm x|\bm y;\bm \gamma^{(r)}}\{\log[p(\bm y,\bm x;\{\bm \gamma_i\})]\},\label{eq.mstep_basic}  
\end{align}
where $\bm\gamma^{(r)} = \otimes_{i=1}^I \bm \gamma_i^{(r)}$ is the estimate in the $r$th iteration and
\begin{equation}
\mathcal{C}_+\!=\!\left\{\{\bm \gamma_i\}\in\mathcal{C}\bigg|\bm \gamma_i >0,i=1,2,\!\ldots\!,I\right\}.
\end{equation}
Here, we restrict the feasible set in~\eqref{eq.mstep_basic} to $\mathcal{C}_+$ instead of $\mathcal{C}$ in~\eqref{eq.ml_problem} to avoid degenerate distributions. Further, using straightforward algebraic simplifications \cite{bishop2006pattern}, we can reduce~\eqref{eq.mstep_basic} to
\begin{equation}\label{eq.qfunc}
\{\bm \gamma_i\}^{(r+1)}
% & = \underset{\substack{\{\bm \gamma_i\} \in \mathcal{C}_+}}{\arg\max} \mathbb{E}_{\bm x|\bm y;\bm \gamma^{(r-1)}}\{\log[p(\bm y,\bm x;\{\bm \gamma_i\})]\}\\
% & \overset{\mathclap{\mathrm{(a)}}}{=} \underset{\substack{\{\bm \gamma_i\} \in \mathcal{C}_+}}{\arg\max} \mathbb{E}_{\bm x|\bm y;\bm \gamma^{(r-1)}}\{\log[p(\bm x;\{\bm \gamma_i\})]\}\\
= \underset{\substack{\{\bm \gamma_i\} \in \mathcal{C}_+}}{\arg\min} %\underbrace{
\log\!|\!\diag(\bm\gamma)|\!+\!(\bm d^{(r)})^{\mathsf{T}}\bm\gamma^{-1},
%}_{Q(\{\bm \gamma_i\}|\bm \gamma^{(r-1)})}
%= \underset{\substack{\{\bm \gamma_i\} \in \mathcal{C}_+}}{\arg\min} Q(\{\bm \gamma_i\}|\bm \gamma^{(r-1)}),
\end{equation}    
where we define \begin{equation}\label{eq.compute_d}
\bm d^{(r)} = \diag(\bm \Sigma_{\bm x}+\bm \mu_{\bm x}\bm \mu_{\bm x}^\mathsf{H}).
\end{equation}
Here, $\bm \mu_{\bm x}$ and $\bm \Sigma_{\bm x}$, which depend on $\bm \gamma^{(r)}$, are the mean and variance of conditional distribution $p(\bm x|\bm y;\bm \gamma^{(r)})$, respectively,% depending on $\bm \gamma^{(r-1)}$ as
\begin{equation}
\label{eq.post_meva}
\bm \mu_{\bm x} = \sigma^{-2}\bm \Sigma_{\bm x}\bm H^\mathsf{H}\bm y,\ \bm \Sigma_{\bm x} = \!\left[\sigma^{-2}\bm H^\mathsf{H}\bm H\!+\!\diag(\bm \gamma^{(r)})^{-1}\right]^{-1}\!. 
\end{equation}

\begin{algorithm}[t!]
  \SetAlgoLined
  \DontPrintSemicolon
  \KwData{Measurements $\bm y$, matrix $\bm H$, noise power $\sigma^2$}
  \textbf{Parameters}: Threshold $\epsilon$ and $\epsilon_{\mathsf{AM}}$
   
  \textbf{Initialization}: $\{\bm \gamma_i\}^{(0)}=\bm 0$, $\{\bm \gamma_i\}^{(1)} \in \mathcal{C}$, set $r=1$
  
   \While{$\| \otimes_{i=1}^I \bm \gamma_i^{(r)}-\otimes_{i=1}^I \bm \gamma_i^{(r-1)} \|_2 > \epsilon$}{
    
    Compute $\bm d^{(r)}$ using~\eqref{eq.compute_d} and~\eqref{eq.post_meva}
    % \tcp*{E-step}
    
    Set $t=1$, $\{\bm \gamma_i\}^{(r,1)} = \{\bm \gamma_i\}^{(r)}$, $\{\bm \gamma_i\}^{(r,0)} = \bm 0$ 
    % \tcp*{M-step}
    
    \While{$\| \otimes_{i=1}^I \bm \gamma_i^{(r,t)}-\otimes_{i=1}^I \bm \gamma_i^{(r,t-1)} \|_2 > \epsilon_\mathsf{AM}$}{
    
    
    % \For{$i=1,\ldots,I$}
    % { Compute $\tilde{\bm \gamma_i}$ using \eqref{eq.update}
    % }
    
    Compute $\{\bm \gamma_i\}^{(r,t+1)}$ using \eqref{eq.update} and \eqref{eq.projection}
    %Projection to $\mathcal{C}_+$ as~\eqref{eq.projection}
    
    % : $\bm \gamma_i^{(t+1)} = \frac{\tilde{\bm \gamma}_i}{\|\tilde{\bm \gamma}_i\|_2}$, for $i=1,\ldots,I-1$ \\ $\bm \gamma_I^{(t+1)} = \prod_{i=1}^{I-1}\|\tilde{\bm \gamma}_i\|_2\tilde{\bm \gamma}_I$

    Update AM iteration index $t \leftarrow t + 1$

  }

    $\{\bm \gamma_i\}^{(r+1)} = \{\bm \gamma_i\}^{(r,t)}$
  
    Update iteration index $r \leftarrow r + 1$

   } 
   
   \KwResult{Output $\bm x=\bm \mu_{\bm x}$ using~\eqref{eq.post_meva}}
  \caption{AM-KroSBL}
  \label{al.AMKroSBL}
\end{algorithm}



Let $Q(\{\bm \gamma_i\}|\bm \gamma^{(r)})=\log|\diag(\bm\gamma)|+(\bm d^{(r)})^{\mathsf{T}}\bm\gamma^{-1}$, which depends on $\bm \gamma^{(r)}$ via $\bm d^{(r)}$. Then, the M-step is written~as
\begin{equation}\label{eq.mstep}
\underset{\substack{\{\bm \gamma_i\}}}{\min}\ Q(\{\bm \gamma_i\}|\bm \gamma^{(r)}) \hspace{0.3cm} \text{s.t.}\
\bm\gamma = \otimes_{i=1}^I \bm \gamma_i,\ \{\bm \gamma_i\}\in \mathcal{C}_+.
\end{equation}
The solution to~\eqref{eq.mstep} without the constraint $\bm\gamma = \otimes_{i=1}^I \bm \gamma_i$ is straightforward \cite{wipf2004sparse}. However, the Kronecker constraint poses a challenge to derive a closed-form solution. To solve~\eqref{eq.mstep} with the Kronecker constraint, two distinct ways are presented: AM-based and SVD-based approaches.

\subsection{AM-based KroSBL (AM-KroSBL)}

AM-KroSBL solves \eqref{eq.mstep} by alternatingly updating one hyperparameter vector while keeping the others fixed. We first compute the gradient of cost function\footnote{We interchangeably use $Q(\{\bm \gamma_i\}|\bm \gamma^{(r)})$, $Q(\{\bm \gamma_i\})$, and $Q$ in the paper.} 
$Q(\{\bm \gamma_i\})$ with respect to $\{\bm \gamma_i\}$ and set it to zero, leading to
\begin{equation}
\label{eq.update}
    \tilde{\bm \gamma}_i\!=\!N^{-I+1}\!\big[(\otimes_{j=1}^{i-1}(\tilde{\bm \gamma}_j)^{-1})\otimes \bm I_N \otimes (\otimes_{j=i+1}^I(\bm \gamma_j^{(r,t)})^{-1})\big]^\mathsf{T}\bm d^{(r)},
\end{equation}
% {\color{red} I fixed a typo in the above equation, please check!}
for $i=1,2,\ldots,I$, with $\bm\gamma_l^{(r,t)}$ is the estimate in the $t$th iteration of AM and the $r$th iteration of EM. To avoid the scaling ambiguity, in each iteration $t$, we project the hyperparameter vectors to $\mathcal{C}_+$
as
\begin{equation}\label{eq.projection}
\bm \gamma_i^{(r,t+1)} = \begin{cases}
\frac{\tilde{\bm \gamma}_i}{\|\tilde{\bm \gamma}_i\|_2} &\text{for}\; i=1,2,\ldots,I-1\\
\prod_{j=1}^{I-1}\|\tilde{\bm \gamma}_i\|_2\tilde{\bm \gamma}_I &\text{for}\; i=I.
\end{cases}
\end{equation}
% \begin{equation}\label{eq.projection}
% \bm \gamma_I^{(t+1)} = \prod_{i=1}^{I-1}\|\tilde{\bm \gamma}_i\|_2\tilde{\bm \gamma}_I \; \text{and} \; \bm \gamma_i^{(r,t+1)} = \frac{\tilde{\bm \gamma}_i}{\|\tilde{\bm \gamma}_i\|_2}, i=1,2,\ldots,I-1; .    
% \end{equation}
The projection~\eqref{eq.projection} does not change the cost function value $Q$ as $\otimes_{i=1}^I\bm \gamma_i^{(r,t+1)}=\otimes_{i=1}^I\tilde{\bm \gamma_i}$. The steps are summarized in Algorithm \ref{al.AMKroSBL}. The AM-KroSBL is guaranteed to improve the cost function given by~\eqref{eq.qfunc} after every iteration. But due to its iterative nature, it is computationally inefficient. Thus, we next present a non-iterative method to solve~\eqref{eq.mstep} based on SVD.

\subsection{SVD-based KroSBL (SVD-KroSBL)}

This method solves \eqref{eq.mstep} without the constraint $\bm\gamma = \otimes_{i=1}^I \bm \gamma_i$ first and then projects the solution to the constraint set. We note from \cite{wipf2004sparse} that
\begin{equation}\label{eq.mstep_svd}
\underset{\substack{\bm\gamma }}{\arg\min}\log|\diag(\bm\gamma)|+(\bm d^{(r)})^\mathsf{T}\bm\gamma^{-1} = \bm d^{(r)}.
\end{equation}
To project the above solution to the constraint set, we solve for $\{\bm \gamma_i\}$ that minimizes $\| \bm d^{(r)} - \otimes_{i=1}^I \bm \gamma_i \|_2$.
We further approximate this optimization problem as $(I-1)$ rank-1 approximations:
% \begin{equation}\label{prob.bidecom}
% \underset{\substack{\bm\gamma_{i},\bar{\bm\gamma}_{i},\|\bm \gamma_{i}\|_2=1}}{\min} \| \bar{\bm\gamma}_{i-1} - \bm \gamma_{i}\otimes\bar{\bm\gamma}_{i} \|_2,\ \forall i=1,2,\ldots,I-1,   
% \end{equation}
\begin{equation}\label{prob.bidecom}
\bm\gamma_{i}^{(r+1)} =\underset{\bm\gamma_{i}: \|\bm \gamma_{i}\|_2=1,
\bar{\bm\gamma}_{i}\in\mathbb{R}^{N(I-i)}}{\arg\min} \| \bar{\bm\gamma}_{i-1} - \bm \gamma_{i}\otimes\bar{\bm\gamma}_{i} \|_2,\ \forall i=1,2,\ldots,I-1,   
\end{equation}
where $\bar{\bm \gamma}_0=\bm d^{(r)}$ and $\bar{\bm \gamma}_{I-1}=\bm \gamma_I$. The problem in \eqref{prob.bidecom} is solved using SVD. This approach is summarized in Algorithm \ref{al.SVDKroSBL}.

\begin{algorithm}[t]
  \SetAlgoLined
  \DontPrintSemicolon
\SetNoFillComment
  \KwData{Measurements $\bm y$, matrix $\bm H$, noise power $\sigma^2$}
  \textbf{Parameters}: Threshold $\epsilon$
   
  \textbf{Initialization}: $\{\bm \gamma_i\}^{(0)}=\bm 0$, $\{\bm \gamma_i\}^{(1)} \in \mathcal{C}$, set $r=1$
  
   \While{$\| \otimes_{i=1}^I \bm \gamma_i^{(r)}-\otimes_{i=1}^I \bm \gamma_i^{(r-1)} \|_2 > \epsilon$}{
    
    Compute $\bm d^{(r)}$ using~\eqref{eq.compute_d} and~\eqref{eq.post_meva} 
    % \tcp*{E-step}
    
    \For{$i=1,\ldots,I-1$} 
    { Solve~\eqref{prob.bidecom} for $\bm \gamma_i^{(r+1)}$         
    % \tcp*{M-step}
    }

%$\{\bm \gamma_i\}^{(r+1)} = \{\bm \gamma_i\}^{(r,t)}$
    
    Update iteration index: $r \leftarrow r + 1$

   }
   
   \KwResult{Output $\bm x=\bm \mu_{\bm x}$ using~\eqref{eq.post_meva}}
  \caption{SVD-KroSBL}
  \label{al.SVDKroSBL}
\end{algorithm}

%We extend AM-KroSBL to handle block sparsity where the nonzero coefficients occur in clusters with unknown lengths and positions. For example, the unknown AoAs of received signals can span an unknown range in the angular domain due to scattering along transmission paths \cite{you2022structured}, prompting block sparsity in the BEM of channels with unknown boundaries. Utilizing block sparsity has been proven to have better performance \cite{wang2018alternative,joseph2019anomaly}. The following demonstrates the so-called pattern-coupled-KroSBL (PC-KroSBL) to handle Kronecker structure and block sparsity with unknown block boundaries in the context of IRS-aided MIMO channel estimation.


\section{Application and Extension}\label{sec:channelesti}
In this section, we discuss the application of our algorithm to channel estimation in an IRS-assisted MIMO system. 

\subsection{Cascaded channel estimation for IRS-aided MIMO}
Consider an uplink MIMO millimeter-wave/terahertz band system with a $T$-antenna transmitter mobile station (MS), an $R$-antenna receiver base station (BS), and an $L$-element uniform linear array IRS. %We assume that the line-of-sight (LOS) path between the BS and MS is blocked, and the LOS paths between the BS and IRS and the IRS and MS are much stronger than the non-LOS paths. 
% {\color{red} Later $M$ is used to denote measurements}
%Further, we assume a narrowband fading channel following the Saleh-Valenzuela model \cite{you2022structured,he2022structure}. 
Let $\bm H_\mathrm{MS}\in \mathbb{C}^{L\times T}$ and $\bm H_\mathrm{BS} \in \mathbb{C}^{R\times L}$ denote the MS-IRS and IRS-BS (narrowband) channels, respectively, %following the Saleh-Valenzuela model \cite{you2022structured}:
\begin{align}
\label{eq.channelmodel1}
\bm H_\mathrm{MS}&=\sum_{p=1}^{P_{\mathrm{MS}}}\sqrt{\frac{LT}{P_{\mathrm{MS}}}}\beta_{{\mathrm{MS}},p}\bm a_L(\phi_{{\mathrm{MS}},p})\bm a_{T}(\alpha_{{\mathrm{MS}}})^\mathsf{H}\\\label{eq.channelmodel2}
\bm H_\mathrm{BS}&=\sum_{p=1}^{P_{\mathrm{BS}}}\sqrt{\frac{RL}{P_{\mathrm{BS}}}}\beta_{{\mathrm{BS}},p}\bm a_{R}(\alpha_{{\mathrm{BS}},p})\bm a_L(\phi_{{\mathrm{BS}}})^\mathsf{H},
% &\overset{\text{(a)}}{=} \bm A_{BS} \bm g_{BS} \bm g_{L,d}^H \bm A_{L,d}^H,
% &\overset{\text{(b)}}{=} \bm A_{L,a}\bm g_{L,a} \bm g_{MS}^H \bm A_{MS}^H,
\end{align}    
where $P_{\mathrm{MS}}$ and $P_{\mathrm{BS}}$ are the number of rays. Also, for any integer $Q$ and angle $\psi$, steering vector $\bm a_Q(\psi)\in\mathbb{C}^{Q\times 1}$ with half-wavelength spacing is
\begin{equation}\label{eq.steer}
\bm a_Q(\psi) = \frac{1}{\sqrt{Q}}\begin{bmatrix}
1 & e^{j\pi  \cos{\psi}}\cdots e^{j\pi (Q-1)  \cos{\psi}}\end{bmatrix}^\mathsf{T}.
\end{equation}
The angles $\phi_{{\mathrm{MS}},p}$, $\alpha_{\mathrm{MS}}$, $\alpha_{{\mathrm{BS}},p}$, and $\phi_{{\mathrm{BS}}}$ denote the $p$th AoA of the IRS, and the $p$th AoD of the MS, the $p$th AoA of the BS, and the AoD of the IRS, respectively (see Fig.~\ref{fig:irs_sys}). Then, the cascaded MS-IRS-BS channel is then given by $\bm H_\mathrm{BS}\diag (\bm \theta)\bm H_\mathrm{MS}$ for a given IRS configuration $\bm\theta \in \mathbb{C}^{L \times 1}$. Here, the $i$th entry of $\bm\theta$ represents the gain and phase shift due to the $i$th IRS element. %Besides, we also consider the angular spread due to scatters (see Fig.~\ref{fig:irs_sys}) \cite{you2022structured,rappaport2013millimeter}. 
We aim to estimate the cascaded channel $\bm H_\mathrm{BS}\diag (\bm \theta)\bm H_\mathrm{MS}$ for any $\bm \theta$. 

To estimate the channel, we send pilot symbols over $K$ time slots over which $\bm H_\mathrm{MS}$ and $\bm H_\mathrm{BS}$ is assumed to be constant. We choose $K_{\mathrm{I}}<K$ IRS configurations, and for each configuration, we transmit pilot data $\bm X \in \mathbb{C}^{T\times K_{\mathrm{P}}}$ over $K_{\mathrm{P}}$ time slots such that $K=K_{\mathrm{I}}K_{\mathrm{P}}$. Hence, the received signal $\bm Y_k\in \mathbb{C}^{R \times K_{\mathrm{P}}}$ corresponding to the $k$th configuration $\bm \theta_k$~is
\begin{equation}\label{eq.basicdatamodel}
\bm Y_k=\bm H_\mathrm{BS}\diag (\bm \theta_k)\bm H_\mathrm{MS} \bm X+\bm W_k,
\end{equation}
where $\bm W_k \in \mathbb{C}^{R\times K_{\mathrm{P}}}$ is the additive white Gaussian noise with zero mean and variance $\sigma^2$. %Our objective is to estimate $\bm H_\mathrm{MS}^\mathsf{T} \odot \bm H_\mathrm{BS}$ as $\vect(\bm H_\mathrm{BS}\diag (\bm \theta_k)\bm H_\mathrm{MS}) = (\bm H_\mathrm{MS}^\mathsf{T} \odot \bm H_\mathrm{BS})\bm \theta_k$, where $\bm \theta_k$ is known, using the data model in \eqref{eq.basicdatamodel} and the knowledge of $\bm X$ and $\{\bm Y_k,\bm \theta_k\}_{k=1}^{K_{\mathrm{I}}}$. Here, the vectorization operator $\vect(\cdot)$ transforms a matrix into a vector by concatenating its columns. We vectorize both sides of~\eqref{eq.basicdatamodel} to obtain
% \begin{equation}\label{eq.vecmodel1}
% \bar{\bm y}_k = \big((\bm X^\mathsf{T} \bm H_\mathrm{MS}^\mathsf{T}) \odot \bm H_\mathrm{BS}\big)\bm \theta_k + \bar{\bm w}_k\in \mathbb{C}^{BK_{\mathrm{P}} \times 1},
% \end{equation}
% So, the received data $\bar{\bm Y}\in \mathbb{C}^{B K_{\mathrm{P}} \times K_{\mathrm{I}}}$ for the IRS configurations $\bm \Theta\!=\![\bm \theta_1,\ldots,\bm \theta_{K_{\mathrm{I}}}]\in \mathbb{C}^{L \times K_{\mathrm{I}}}$ is
% \begin{equation}\label{eq.k1k2model}
% \bar{\bm Y} = [\bar{\bm y}_1,\!\ldots\!,\bar{\bm y}_{K_{\mathrm{I}}}]\!=\!\big(\!(\bm X^\mathsf{T} \bm H_\mathrm{MS}^\mathsf{T}) \odot \bm H_\mathrm{BS} \big)\bm \Theta + \bar{\bm W},    
% \end{equation}
% where $\bar{\bm W} \!=\! [\bar{\bm w}_1,\ldots,\bar{\bm w}_{K_{\mathrm{I}}}]\! \in \mathbb{C}^{B K_{\mathrm{P}} \times K_{\mathrm{I}}}$.
To estimate the cascaded channel, we exploit angular sparsity in the channel matrices $\bm H_\mathrm{MS}$ and $\bm H_\mathrm{BS}$. For this, we apply the BEM by sampling the angular domain using a set of $N$ grid angles $\{\psi_n\}_{n=1}^N$ such that $\cos(\psi_n)= 2 n/N-1$ \cite{mao2022channel}. Then, \eqref{eq.channelmodel1} and~\eqref{eq.channelmodel2} reduce to
\begin{equation}
\label{eq.channelmodelsparse2}
\bm H_\mathrm{BS}= \bm A_{R} \bm g_{\mathrm{R}} \bm g_{\mathrm{L,d}}^\mathsf{H} \bm A_{L}^\mathsf{H}  \hspace{0.3cm} \text{and }  \hspace{0.3cm}
\bm H_\mathrm{MS}= \bm A_{L}\bm g_{\mathrm{L,a}} \bm g_{\mathrm{T}}^\mathsf{H} \bm A_{T}^\mathsf{H},
\end{equation}
where for any integer $Q>0$, using \eqref{eq.steer}, we define
\begin{equation}
\bm A_{Q} = \begin{bmatrix}
\bm a_{Q}(\psi_1) & \bm a_{Q}(\psi_2)&\ldots&\bm a_{Q}(\psi_N)
\end{bmatrix}\in \mathbb{C}^{Q\times N}.
\end{equation}
Also, $\bm g_{\mathrm{R}},\bm g_{\mathrm{L,d}},\bm g_{\mathrm{L,a}},\bm g_{\mathrm{T}}\in\mathbb{C}^{N\times 1}$ are the unknown sparse channel representations corresponding to the AoAs/AoD of the channel. % whose positions and values of non-zero entries indicate the true angles and the corresponding path gains, respectively. 
Substituting \eqref{eq.channelmodelsparse2} into \eqref{eq.basicdatamodel}, and vectorizing the received signal $\{\bm Y_k\}_{k=1}^{K_I}$ followed by some algebraic simplifications (see \cite{he2022structure} for details), we arrive at 
%set of $N$ grid angles for the AoAs and AoDs of the cascaded channel and obtain \cite{he2022structure}
% \begin{multline}\label{eq.matrixsparseprob}
% \bar{\bm Y} =\big[(\bm X^\mathsf{T} \bm A_{\mathrm{M}}^*) \otimes \bm A_{\mathrm{B}}\big]
% 	 \big[(\bm g_{\mathrm{L,a}} \bm g_{\mathrm{M}}^\mathsf{H})^\mathsf{T} \otimes (\bm g_{\mathrm{B}} \bm g_{\mathrm{L,d}}^\mathsf{H})\big] \\
% 	 \times (\bm A_{\mathrm{L}}^\mathsf{T} \odot \bm A_{\mathrm{L}}^\mathsf{H})\bm \Theta+\bar{\bm W},	
% \end{multline}
% where $\bm A_{\mathrm{L}}$, $\bm A_{\mathrm{M}}$, and $\bm A_{\mathrm{B}}$ are the BEM over-complete dictionaries of the AoAs/AoD of IRS, AoD of MS, and AoAs of BS, respectively. After some manipulations, we have
\begin{equation}\label{eq.datamodel}
\tilde{\bm y} = (\tilde{\bm \Phi}_{\mathrm{L}} \otimes \bm \Phi_{\mathrm{T}} \otimes \bm \Phi_{\mathrm{R}}) (\bm g_{\mathrm{L,a}} \otimes \bm g_{\mathrm{L,d}}^* \otimes \bm g_{\mathrm{T}}^*\otimes\bm g_{\mathrm{R}}) + \tilde{\bm w}\in\mathbb{C}^{RK\times 1},
\end{equation}
where $\tilde{\bm \Phi}_{\mathrm{L}} = \bm \Theta^\mathsf{T}(\bm A_{L}^\mathsf{T} \odot \bm A_{L}^\mathsf{H})^\mathsf{T}$, $\bm \Phi_{\mathrm{T}}=\bm X^\mathsf{T} \bm A_{T}^*$, and $\bm \Phi_{\mathrm{R}}=\bm A_{R}$. Here, only the first $N$ columns of $\tilde{\bm \Phi}_{\mathrm{L}}$ are distinct~\cite{wang2020compressed}. Hence, removing the redundant columns to reduce the dimension of the representation leads to
\begin{equation}\label{eq.datamodelcs}
\tilde{\bm y} = (\bm \Phi_{\mathrm{L}} \otimes \bm \Phi_{\mathrm{T}} \otimes \bm \Phi_{\mathrm{R}})\bm g + \tilde{\bm w} = \tilde{\bm H}\bm g + \tilde{\bm w}\in\mathbb{C}^{RK\times 1},
\end{equation}
where $\bm \Phi_{\mathrm{L}}\in\mathbb{C}^{K_{\mathrm{I}}\times N}$ is formed by the first $N$ columns of $\tilde{\bm \Phi}_{\mathrm{L}}$ and $\tilde{\bm H}=\bm \Phi_{\mathrm{L}} \otimes \bm \Phi_{\mathrm{T}} \otimes \bm \Phi_{\mathrm{R}}\in\mathbb{C}^{RK\times N^3}$. Also, we define $\bm g = \bm g_{\mathrm{L}} \otimes \bm g_{\mathrm{T}}^* \otimes \bm g_{\mathrm{R}}\in\mathbb{C}^{N^3\times 1}$ with $\bm g_{\mathrm{L}}\in\mathbb{C}^{N\times 1}$ being the scaled version of the first $N$ entries of $\bm g_{\mathrm{L,a}} \otimes \bm g_{\mathrm{L,d}}^*$. Hence,~\eqref{eq.datamodelcs} translates the channel estimation problem into the sparse recovery problem in \eqref{eq.problem_basic} with unknown vector $\bm g$. Now we can apply our algorithms (AM-KroSBL and SVD-KroSBL) with $I=3$ to reconstruct the Kronecker-structured sparse channel vector $\bm g$. 

\subsection{Extension of AM-KroSBL for block sparsity}
In the IRS-aided channel model, the scatters lead to spreading AoAs (see Fig.~\ref{fig:irs_sys}), causing clustered non-zero BEM coefficients. In our model, sparse vectors $\bm g_{\mathrm{L}}$ and $\bm g_{\mathrm{R}}$, containing the BEM coefficients of the AoAs of IRS and BS, possess block sparsity structures with unknown block boundaries.
% Figure environment removed

To tackle block sparsity, we draw inspiration from the PC-SBL algorithm~\cite{fang2014pattern} and impose a prior on each entry of the sparse vector, which not only depends on its hyperparameter but also the hyperparameters of its neighbors. This method connects the sparsity patterns of the adjacent entries, promoting block sparsity. We assume that $\bm \gamma_1$ exhibits block sparsity for ease of exposition. However, this idea can be readily extended when multiple hyperparameter vectors possess block sparsity. We adopt the prior on $\bm x$ with hyperparameters $\{\bm\gamma_i\}$ as
% \begin{multline}
%     p([\bm x]_{\overline{n_1n_2...n_I}}|\{\bm \gamma_i\})\\=\mathcal{CN}\big(0,\left([\bm \gamma_1]_{n_1}+\beta[\bm \gamma_1]_{n_1-1}+\beta[\bm \gamma_1]_{n_1+1}\right)\prod_{i=2}^I[\bm \gamma_i]_{n_i}\big),
% \end{multline}
% where $\overline{n_1n_2...n_I} = \sum_{i=1}^{I-1}(n_i-1)N^{I-i}+n_I$, and $\beta$ is the pattern-coupled coefficient. This can be further rewritten as
%\begin{align}\label{eq.modiprior}
    $p(\bm x|\{\bm \gamma_i\}) = \mathcal{CN}\big(0,\hat{\bm \gamma}\big)$,
%\end{align}
where $\hat{\bm \gamma} = \hat{\bm \gamma_1} \otimes (\otimes_{i=2}^I \bm \gamma_i)$ and %. Likewise, we define $\bm\gamma_1$ as
\begin{equation}\label{eq.hatnohat}
    \hat{\bm \gamma_1} = \bm C_{\beta} \bm \gamma_1,%\  \bm C_{\beta} = \mathrm{Tri}(1,\beta)
    % \begin{bmatrix}
    % 1 & \beta & 0 & \cdots & 0\\
    % \beta & 1 & \beta & \cdots & 0\\
    % 0 & \beta & 1 &  \cdots & 0\\
    % \vdots & \vdots & \vdots & \ddots & \vdots\\
    % 0 & \cdots & 0 & \beta & 1
    % \end{bmatrix}
    %,
\end{equation}
where %$\mathrm{Tri}(1,\beta)$ denotes 
$\bm C_{\beta}\in \mathbb{R}^{N \times N}$ is a tridiagonal Toeplitz matrix with ones along its diagonal and $\beta$ along its first sub- and super-diagonals~\cite{noschese2013tridiagonal}. The parameter $\beta>0$ is the pattern-coupled coefficient. Using the new prior, the mean and variance of conditional distribution in the $r$th EM iteration are modified by replacing $\bm \gamma^{(r)}$ with $\hat{\bm \gamma}^{(r)}$ in~\eqref{eq.post_meva}.
% as
% \begin{equation}\label{eq.post_meva_pc}
%     \bm \mu_{\bm x}\!=\!\sigma^{-2}\bm \Sigma_{\bm x}\bm H^\mathsf{H}\bm y,\bm \Sigma_{\bm x}\! =\! \left[\sigma^{-2}\bm H^\mathsf{H}\bm H\!+\!\diag(\hat{\bm \gamma}^{(r-1)})^{-1}\right]^{-1},
% \end{equation}
Thus, the optimization problem in the M-step is
\begin{equation}\label{eq.mstep2}
\underset{\substack{\{\bm \gamma_i\}}}{\min}\log|\diag(\hat{\bm\gamma})|+(\bm d^{(r)})^{\mathsf{T}}\hat{\bm\gamma}^{-1} \ \text{s.t.} \ \left\{\hat{\bm \gamma_1}, \{\bm \gamma_i\}_{i=2}^I\right\} \in\mathcal{C}_+%\!,\!\bm \gamma_1\!>\!0
.
\end{equation}
We solve~\eqref{eq.mstep2} using an iterative algorithm similar in spirit to the AM-KroSBL algorithm, by setting the gradient of  the cost function with respect to all hyperparameter vectors to zero. Here, the update for $\{\bm \gamma_i\}_{i=2}^I$ is given by~\eqref{eq.update}. However, due to the entanglement of hyperparameters in $\hat{\bm \gamma_1}$, the update for $\bm \gamma_1^{(r,t+1)}$ is
% % Figure environment removed
% where $\bm d_{n_1}^{(r-1)} = [\bm d^{(r-1)}]_{(n_1-1)N^{I-1}+1:n_1N^{I-1}}$. Computing the gradient for all $n_1 = 1,2,\ldots,N$ and stacking them into a vector leads to
\begin{multline}\label{eq.gradg1compact}
     N^{I-1} \bm C_{\beta} (\hat{\bm \gamma_1}^{(r,t+1)})^{-1}
     =\bm C_{\beta} \diag\left((\hat{\bm \gamma_1}^{(r,t+1)})^{-2}\right)\\\times\left(\bm I_N\otimes (\otimes_{i=2}^I(\bm \gamma_i^{(r,t)})^{-1})\right)^\mathsf{T}\bm d^{(r)}.
\end{multline}
Solving~\eqref{eq.gradg1compact} is not trivial due to the matrix-vector multiplication on both sides. However, if $\bm C_\beta$ is invertible, we can simplify~\eqref{eq.gradg1compact} using \eqref{eq.hatnohat} as 
\begin{equation}
    \bm C_{\beta} \bm \gamma_1^{(r,t+1)}= \hat{\bm \gamma_1}^{(r,t+1)}=N^{-I+1}(\bm I_N\otimes(\otimes_{i=2}^I(\bm \gamma_i^{(r,t)})^{-1}))^\mathsf{T}\bm d^{(r)}.
\end{equation}
Also, we notice that the $n$-th eigenvalue of $\bm C_{\beta}$ is $\lambda_n = 1 + 2\beta \cos(\frac{n\pi}{N+1})$ for $n=1,2,\ldots,N$ \cite{noschese2013tridiagonal}. Thus, we choose $\beta \neq -(2\cos(\frac{n\pi}{N+1}))^{-1}$ so that $\bm C_{\beta}$ is non-singular.
% \begin{align}
%      N^{I-1} (\hat{\bm \gamma_1}^{(t+1)})^{-1}
%      =\diag\left((\hat{\bm \gamma_1}^{(t+1)})^{-2}\right)\left(\bm I\otimes (\otimes_{i=2}^I(\bm \gamma_i^{(t)})^{-1})\right)^\mathsf{T}\bm d^{(r-1)},
% \end{align}
% which leads to
% \begin{align}\label{eq.update_pc_1}
%     \bm C_{\beta} \bm \gamma_1^{(t+1)}
%      =N^{-I+1}\left(\bm I\otimes(\otimes_{i=2}^I(\bm \gamma_i^{(t)})^{-1})\right)^\mathsf{T}\bm d^{(r-1)}.
% \end{align}
% Although it seems straightforward to invert $\bm C_{\beta}$ to obtain $\bm \gamma_1$, this is not a reasonable way. Since direct inverting $\bm C_{\beta}$ may lead to negative entries in $\bm \gamma_1$, which violets the non-negativity of hyperparameters. 
% Thus, finding $\bm \gamma_1^{(t+1)}$ that equates the gradient to zero is equivalent to solving~\eqref{eq.update_pc_1}. 
With this choice, we update $\bm \gamma_1$ along with the constraint $\bm \gamma_1^{(r,t+1)} > 0$ with
\begin{equation}\label{eq.nnls1}
    \min_{\bm \gamma_1^{(r,t+1)} \geq 0} \|\bm C_{\beta} \bm \gamma_1^{(r,t+1)}
     \!-\!N^{-I+1}\left(\bm I_N\otimes (\otimes_{i=2}^I(\bm \gamma_i^{(r,t)})^{-1})\right)^\mathsf{T}\!\bm d^{(r)}\|^2_2.
\end{equation}
%Here, solving~\eqref{eq.nnls1} may return $\bm \gamma_1^{(r,t+1)}$ with zero entries. We add a small term to the obtained $\bm \gamma_1^{(r,t+1)}$ to ensure $\bm \gamma_1^{(r,t+1)} > 0$ and practical stability. 
The resulting algorithm, namley PC-KroSBL, is summarized in Algorithm \ref{al.PCKroSBL}.

\begin{algorithm}[t]
\SetAlgoLined
\DontPrintSemicolon
\SetNoFillComment
  \KwData{Measurements $\bm y$, matrix $\bm H$, noise power $\sigma^2$}
  \textbf{Parameters}: Threshold $\epsilon$ and $\epsilon_{\mathsf{AM}}$
   
  \textbf{Initialization}: $\{\bm \gamma_i\}^{(0)}=\bm 0$, $\{\bm \gamma_i\}^{(1)} \in \mathcal{C}$, set $r=1$.
  
   \While{$\| \hat{\bm \gamma}^{(r)}-\hat{\bm \gamma}^{(r-1)} \|_2 > \epsilon$}{
   
   
    Compute $\bm d^{(r)}$ using~\eqref{eq.compute_d} and~\eqref{eq.post_meva} with $\bm\gamma^{(r)}=\hat{\bm \gamma}^{(r)}$ 
    % \tcp*{E-step}

    Set $t=1$, $\{\bm \gamma_i\}^{(r,1)} = \{\bm \gamma_i\}^{(r)} \in \mathcal{C}_+$, $\{\bm \gamma_i\}^{(r,0)} = \bm 0$
    % \tcp*{M-step}
    
    \While{$\| \hat{\bm \gamma}^{(r,t)}-\hat{\bm \gamma}^{(r,t-1)} \|_2 > \epsilon_\mathsf{AM}$}{ 

    Compute $\bm \gamma_1^{(r,t+1)}$ using~\eqref{eq.nnls1}
    
    Compute $\tilde{\bm \gamma_1}=\hat{\bm \gamma_1}^{(r,t+1)}$ using~\eqref{eq.hatnohat}
    
    % \For{$i=2,\ldots,I$}
    % { Compute~\eqref{eq.update} for $\tilde{\bm \gamma_i}$ with $\tilde{\bm \gamma_1}$ being replaced with $\hat{\bm \gamma_1}^{(r,t+1)}$
    % }
    Compute $\{\bm \gamma_i\}^{(r,t+1)}$ using \eqref{eq.update} and \eqref{eq.projection}
    
    % Projection to $\mathcal{C}_+$ as~\eqref{eq.projection} with $\tilde{\bm \gamma}_1$ being replaced with $\hat{\bm \gamma_1}^{(r,t+1)}$

    Update AM iteration index $t \leftarrow t + 1$

  }

    $\{\bm \gamma_i\}^{(r+1)} = \{\bm \gamma_i\}^{(r,t)}$
  
    Update iteration index $r \leftarrow r + 1$

   }
   
   \KwResult{Output $\bm x=\bm \mu_{\bm x}$ using~\eqref{eq.post_meva}}
  \caption{PC-KroSBL}
  \label{al.PCKroSBL}
\end{algorithm}


\section{Theoretical Analysis of KroSBL}\label{sec.convergence}

This section focuses on the theoretical analysis of KroSBL. We discuss the convergence guarantee for AM-KroSBL in Sec.~\ref{sec.con_AM}. Then, we present our results on the values to which the algorithm converges by studying the local minima of the KroSBL cost function in~\eqref{eq.cost_ml}.

\subsection{Convergence property of AM-KroSBL}\label{sec.con_AM}
% The classic SBL framework determines the estimate of the sparse vector $\bm x$ by estimating $\bm \gamma=\otimes_{i=1}^I \bm \gamma_i$ through maximizing the marginalized log-likelihood $\mathcal{L}$. This optimization problem, however, generally does not adopt a closed-form solution. Then the EM algorithm is introduced to provide a surrogate solution by maximizing the lower bound of $\mathcal{L}$. Thus, the convergence property of SBL is closely related to the convergence property of the EM algorithm. So is AM-KroSBL. 

The convergence of AM-KroSBL is established using the properties of the EM algorithm, which is well studied in \cite{wu1983convergence}. It is known that under certain conditions, the EM algorithm guarantees convergence to stationary points of $\mathcal{L}$. Nonetheless, the guarantees of the EM algorithm in KroSBL depend on the convergence of the AM algorithm (inner loop). So, this section answers two questions: \emph{What are the convergence properties of the AM algorithm? Do the properties of AM guarantee the convergence of AM-KroSBL?} The first question is answered by Proposition \ref{thm.am_stationary}, serving as a cornerstone to the answer to the second question via Theorem \ref{thm.stationary}.
% The first is the convergence property of the AM step (inner loop) is yet unknown while in the classic SBL, the optimization problem in M-step is globally maximized. The second  and we are going to address these two aspects one after another in this section.
% AM-KroSBL can be viewed as a two-layered hierarchical algorithm. The first (outer) layer iterating between E-step and M-step. The second (inner) layer embedded in M-step iterating within different hyperparameters $\{\bm \gamma_i\}$. The second layer in the SBL framework, i.e., solving the M-step, attains the global maximum of $Q$ in~\eqref{eq.qfunc}, while the convergence property of the proposed AM algorithm in the second layer is yet unknown. Therefore, before discussing the AM-KroSBL algorithm, we need to discuss the convergence property of the AM algorithm first, i.e., the inner layer. Based on the convergence of the inner layer, we will then demonstrate the convergence property of the AM-KroSBL, i.e., the outer layer. 
% We formally define the notion of stationary point of $\mathcal{L}$ and $Q(\{\bm \gamma_i\}|\bm \gamma^{(r-1)})$ as follows:
% \begin{defi}\label{eq.defi_stationary_point}
% (Stationary point) The point $\{\bm \gamma_i^\mathsf{s}\}$ $\in \mathcal{C}_+$ is a stationary point of $Q$ if it satisfies 
% \begin{equation}
% \begin{aligned}
% \frac{\partial Q(\{\bm \gamma_i\}|\bm \gamma^{(r-1)})}{\partial \{\bm \gamma_i\}}\bigg|_{\{\bm \gamma_i^\mathsf{s}\}}=
% \begin{bmatrix}
% \frac{\partial Q}{\partial \bm \gamma_1}\\
% \vdots\\
% \frac{\partial Q}{\partial \bm \gamma_I}
% \end{bmatrix}_{\{\bm \gamma_i^\mathsf{s}\}}
% =
% \bm 0,
% \end{aligned}
% \end{equation}
% where changing $Q$ to $\mathcal{L}$ and $\mathcal{C}_+$ to $\mathcal{C}$, we similarly define the stationary point of $\mathcal{L}$.
% \end{defi}
% The stationary point $\{\bm \gamma_1^\mathsf{s},\bm \gamma_2^\mathsf{s},\bm \gamma_3^\mathsf{s}\} \in \mathcal{C}_+$ of $Q(\bm \gamma_1,\bm \gamma_2,\bm \gamma_3)$ satisfies
% \begin{equation}
% \begin{aligned}
% \nabla Q(\bm \gamma_1,\bm \gamma_2,\bm \gamma_3)|_{\{\bm \gamma_1,\bm \gamma_2,\bm \gamma_3\}=\{\bm \gamma_1^\mathsf{s},\bm \gamma_2^\mathsf{s},\bm \gamma_3^\mathsf{s}\}}= \bm 0,
% \end{aligned}
% \end{equation}
% which indicates
% \begin{equation}
%     \begin{bmatrix}
%     \bm \gamma_1^\mathsf{s}\\\bm \gamma_2^\mathsf{s}\\\bm \gamma_3^\mathsf{s}
%     \end{bmatrix}
%   =
%     N^{-2}
%     \begin{bmatrix}
%     [\bm I \otimes (\bm \gamma_2^\mathsf{s})^{-1} \otimes (\bm \gamma_3^\mathsf{s})^{-1}]^\mathsf{T}\bm d\\
%     [(\bm \gamma_1^\mathsf{s})^{-1} \otimes \bm I \otimes (\bm \gamma_3^\mathsf{s})^{-1}]^\mathsf{T}\bm d\\
%     [(\bm \gamma_1^\mathsf{s})^{-1} \otimes (\bm \gamma_2^\mathsf{s})^{-1} \otimes \bm I  ]^\mathsf{T}\bm d
%     \end{bmatrix},
% \end{equation}
% where $\|\bm \gamma_1^\mathsf{s}\|_2 = \|\bm \gamma_2^\mathsf{s}\|_2 = 1$.
% \subsection{The convergence of the AM algorithm}\label{sec.AM_stationary}
% The first result is about the convergence property of AM algorithm, summarized in the Proposition \ref{thm.am_stationary}.
We first introduce a lemma that supports the main results.
\begin{lemma}\label{thm.am_cost}
 Consider the AM algorithm that solves the M-step optimization problem of the $r$th EM iteration of AM-KroSBL (Algorithm \ref{al.AMKroSBL}) given by \eqref{eq.mstep}, for a fixed iteration index $r>0$. If $\bm d^{(r)} > 0$ in \eqref{eq.compute_d}, then the cost function sequence $Q(\{\bm\gamma_i\}^{(r,t)})|_{t=1}^\infty$ generated by the AM algorithm is non-increasing.
\end{lemma}
\begin{proof}
    The non-increasing nature of sequence $Q(\{\bm\gamma_i\}^{(r,t)})|_{t=1}^\infty$ is because the AM algorithm in every iteration optimizes one hyperparameter vector while keeping the others fixed, i.e.,
\begin{multline}\label{eq.nonincrease}
    Q(\{\bm \gamma_i\}^{(r,t)})\geq Q\left(\tilde{\bm \gamma}_1,\{\bm \gamma_i^{(r,t)}\}_{i=2}^I\right)\geq Q\left(\{\tilde{\bm \gamma}_i\}_{i=1}^2\!,\!\{\bm \gamma_i^{(r,t)}\}_{i=3}^I\right) \\
    \geq Q(\{\tilde{\bm \gamma}_i\})=Q(\{\bm \gamma_i\}^{(r,t+1)}),
\end{multline}
where the last step follows because the projection step in \eqref{eq.projection} does not change the cost function value.
\end{proof}
The following proposition uses the above lemma to show that the iterates of the AM algorithm converges.% to a single limit point.
\begin{proposition}\label{thm.am_stationary}
[AM algorithm convergence] Consider the AM algorithm that solves the M-step optimization problem of the $r$th EM iteration of AM-KroSBL (Algorithm \ref{al.AMKroSBL}) given by \eqref{eq.mstep}, for a fixed iteration index $r>0$. If $\bm d^{(r)} > 0$ in \eqref{eq.compute_d}, then the sequence $\{\bm\gamma_i\}^{(r,t)}|_{t=1}^\infty$ converges to the set of stationary points of the M-step cost function $Q(\{\bm \gamma_i\})$ in $\mathcal{C}_+$.
\end{proposition}

\begin{proof}
See Appendix \ref{appe.am}.
\end{proof}

We note that Proposition \ref{thm.am_stationary} only guarantees the AM algorithm converges to a stationary point which is not necessarily a global minimum. However, the following result establishes that the convergence of AM to a stationary point is sufficient to ensure the convergence of AM-KroSBL.

% The next is to prove that the sequence $\{\bm \gamma_1^{(r)},\bm \gamma_2^{(r)},\bm \gamma_3^{(r)}\}_{r=0}^\infty$ produced by AM-KroSBL converges to the set of stationary points of $\mathcal{L}$ regarding $\{\bm \gamma_1,\bm \gamma_2,\bm \gamma_3\}$.

\begin{thm}\label{thm.stationary}
Consider the model in~\eqref{eq.problem_basic} with the assumptions $i)$ the noise variance $\sigma^2 > 0$, $ii)$ there exists $\epsilon>0$  such that the dictionary satisfies $\Vert [\bm H]_i\Vert_2>\epsilon$, for $i=1,2,\ldots,N^I$, and $iii)$ the starting point of AM-KroSBL $\{\bm\gamma_i\}^{(1)} >0$. Then, the sequence $\{\bm\gamma_i\}^{(r)}|_{r=1}^{\infty}$ generated by AM-KroSBL (Algorithm \ref{al.AMKroSBL}) convergence to the set of the stationary points of its cost function $\mathcal{L}$ given by \eqref{eq.cost_ml}.
\end{thm}

\begin{proof}
See Appendix \ref{appe.em}.
\end{proof}
%\vspace{-0.1cm}
We note that the assumptions of Theorem \ref{thm.stationary} are realistic. In particular, the assumption on the dictionary holds when $\bm H$ has no zero columns. If the norm of a column in $\bm H$ is zero, indicating that all its elements are zero, then that column does not contribute to the measurement and can be removed. %assumes a nonzero noise variance, which is realistic as the measurement noise is inevitable in practice. The second assumption on the dictionary holds when $\bm H$ has no zero columns. If the norm of a column in $\bm H$ is zero, indicating that all its elements are zero, then that particular column does not contribute to the measurement and can be removed. The third assumption is required to ensure $Q$ in \eqref{eq.mstep} is well-defined. Thus, all three assumptions of Theorem \ref{thm.stationary} are reasonable.

Furthermore, Proposition \ref{thm.am_stationary} suggests that $\{\bm\gamma_i\}^{(r)}\in\mathcal{C}_+$ and thus $\{\bm\gamma_i\}^{(r)}\!>\!0$, which seems to contradict the expected sparsity of the estimates $\{\bm\gamma_i\}^{(r)}$. However, $\{\bm\gamma_i\}^{(r)}\!>\!0$ only holds under the assumption $\bm d^{(r)}\!>\!0$ and the sequence $\{\bm d^{(r)}\}_{r=1}^\infty$ belongs to an open set $\{\bm d|\bm d > 0\}$. From our experiments, we observe that the sequence converges to $\bm d^{(\infty)}$ that belongs to the boundary of the open set, leading to sparse $\{\bm \gamma_i\}\in\mathcal{C}\setminus\mathcal{C}_+$. Intuitively, this behavior can be viewed as follows. If  the $n^*$th entry of $\bm \gamma_{i^*}^{(r)}$ goes to zero for some $i^*$ and $n^*$, then all the $N^{I-1}$ entries in $\bm\gamma^{(r)} = \otimes_{i=1}^I\bm\gamma_i^{(r)}$ involving  $[\bm \gamma_{i^*}]_{n^*}$ are zeros. Let $\mathcal{M}$ be the set of indices in $\otimes_{i=1}^I\bm\gamma_i^{(r)}$ approaching zero.  Then, the submatrix $[\bm \Sigma_{\bm x}]_{\mathcal{M}}$ goes to zero because from \eqref{eq.post_meva},
\begin{equation}
    \bm \Sigma_{\bm x} = \bm \Gamma^{(r)}-\bm \Gamma^{(r)}\bm H^{\mathsf{H}}\left(\sigma^2
    \bm I_{N^I}+\bm H\bm \Gamma^{(r)}\bm H^{\mathsf{H}}\right)\bm H \bm \Gamma^{(r)},
\end{equation}
where the rows of $ \bm \Gamma^{(r)}=\diag{\otimes_{i=1}^I\bm\gamma_i^{(r)}}$ indexed by $\mathcal{M}$ are close to zero. Consequently,  $[\bm d^{(r)}]_{\mathcal{M}}$ also goes to zero due to the following relation from \eqref{eq.compute_d} and \eqref{eq.post_meva},
\begin{equation}
    \bm d^{(r)} = \diag\left(\bm \Sigma_{\bm x}\left[\bm I_{N^I}+\sigma^{-4}\bm H^\mathsf{H}\bm y\bm y^\mathsf{H}\bm H \bm \Sigma_{\bm x}\right]\right). 
\end{equation}
Conversely, suppose that $[\bm d^{(r)}]_{\mathcal{M}}$  goes to zero, where $\mathcal{M}$ is index set of the entries in $\diag{\otimes_{i=1}^I\bm\gamma_i^{(r)}}$ corresponding to a particular element $[\bm \gamma_{i^*}^{(r)}]_{n^*}$ in $\bm \gamma_{i^*}^{(r)}$ for some $i^*$. Then, the second term in $Q$ of \eqref{eq.qfunc} involving $[\bm \gamma_{i^*}]_{n^*}$ vanishes and minimizing $\log [\bm \gamma_{i^*}]_{n^*}$ drives $[\bm \gamma_{i^*}^{(r)}]_{n^*}$ to zero. Thus, a sparse $\bm d^{(r)}$ encourages a sparse $\bm \gamma^{(r)}$ and vice versa, leading to a sparse convergent point $\{\bm \gamma_i\}^{(\infty)} \in \mathcal{C}\setminus\mathcal{C}_+$. 


% In the following, we will show that as EM proceeds, if $\bm d^{(r)}$ goes to the closure of $\{\bm d|\bm d\!>\!0\}$, $\{\bm \gamma_i\}^{(r)}$ will also go to $\mathcal{C}\setminus\mathcal{C}_+$, and the other way around, leading to zeros in $\{\bm \gamma_i\}^{(\infty)}$ though $\{\bm\gamma_i\}^{(r)}\!>\!0$ in each EM iteration as suggested in Proposition \ref{thm.am_stationary}. This can be posed as such: for the $n^*$th entry of the $i^*$ vector $\bm\gamma_{i^*}$ and the set $\mathcal{M}$ that is an index set involving $[\bm \gamma_{i^*}^{(r)}]_{n^*}$, $\lim_{r\rightarrow \infty}[\bm d^{(r)}]_\mathcal{M}= 0$ is necessary and sufficient for $\lim_{r\rightarrow \infty}[\bm \gamma_{i^*}^{(r)}]_{n^*}=0$. We take $n^*\!=\!i^*\!=\!1$ and $\mathcal{M}\!=\!\{1,\cdots,N^{I-1}\}$, i.e., the indices involving $[\bm \gamma_1^{(r)}]_1$, without loss of generality. We note that $\mathcal{M}$ must be Kronecker-structured due to the Kronecker product of $\{\bm \gamma_i\}$.

% \noindent\emph{Sufficient}: The sufficient part is to show $\lim_{r\rightarrow \infty}[\bm d^{(\infty)}]_\mathcal{M}= 0$ leads to $[\bm \gamma_1^{(r)}]_1=0$. For any index $m^*\in\mathcal{M}$, the $r$th M-step leads to
% \begin{equation}\label{eq.lim_q}
%     [\bm \gamma^{(r+1)}]_{m^*} = \underset{[\bm \gamma]_{m^*}}{\min}\ \log[\bm \gamma]_{m^*} + [\bm d]_{m^*}[\bm \gamma]_{m^*}^{-1}.
% \end{equation}
% As $[\bm d]_{m^*}\rightarrow 0$, Problem~\eqref{eq.lim_q} reduces to $\min_{[\bm\gamma]_{m^*}}\log[\bm \gamma]_{m^*}$, solved by $[\bm\gamma]_{m^*} = 0$, indicating $[\bm\gamma^{(r+1)}]_\mathcal{M} = 0$. To spot $[\bm\gamma_1]_1= 0$ from $[\bm\gamma^{(r+1)}]_\mathcal{M} = 0$, we can devectorize vector $\bm\gamma^{(r+1)}$ into an $I$-dimension tensor $\mathcal{I}:=\circ_{i=I}^1 \bm \gamma_i$ where $\circ$ denotes outer product. We can see the tensor slice $[\mathcal{I}]_{1,:\cdots,:}$, i.e., all the entries related to $[\bm \gamma_1]_1$, is zero, which means $[\bm \gamma_1^{(r)}]_1=0$ is the only feasible solution due to the Kronecker structure constraint on set $\mathcal{M}$. 
% % This can be extended to the case where multiple hyperparameters are zero by identifying all-zero slices and their common hyperparameter since if it does not hold, Kronecker-structured $\mathcal{M}$ cannot exist.

% \noindent \emph{Necessary}: The necessary part is to show $\lim_{r\rightarrow \infty}[\bm \gamma_1^{(r)}]_1=0$ leads to $[\bm d^{(\infty)}]_\mathcal{M}= 0$. Vector $\bm d^{(r)}$ is defined in~\eqref{eq.compute_d}, where as $[\bm \gamma_1]_1\rightarrow 0$, $\diag([\bm \gamma^{(r)}]_\mathcal{M})^{-1}$ in $\bm \Sigma_{\bm x}^{-1}$ will go to infinity. Then the general idea is to separate the infinite part, ensure the rest is invertible, and study $\bm \Sigma_{\bm x}$ using the inversion lemma. We have
% \begin{align}
%     \bm \Sigma_{\bm x}^{-1} &= \sigma^{-2}\bm H^\mathsf{H}\bm H\!+\!\diag(\bm \gamma^{(r)})^{-1}\\
%     &=\underbrace{\sigma^{-2}\bm H^\mathsf{H}\bm H\!+\!\bm \Gamma_\mathsf{f}}_{\bm \Sigma_\mathsf{f}^{-1}}+\!\bm \Gamma_\mathsf{i},
% \end{align}
% where $\bm \Gamma_\mathsf{i}$ contains all the terms that go to infinity and has the closed-form $\bm \Gamma_\mathsf{i}\!:=\!\bm E\diag(([\bm \gamma_1]_{1}\otimes_{i=2}^I\bm \gamma_i)^{-1}\!-\!\bm 1_{N^{I-1}})\bm E^\mathsf{T}$ and we denote the rest as $\bm \Gamma_\mathsf{f}$. Here $\bm E=[\begin{smallmatrix}
%     \bm I_{N^{I-1}}&\bm 0
% \end{smallmatrix}]^\mathsf{T}\in\mathbb{R}^{N^I\times N^{I-1}}$ and subtracting $\bm 1_{N^{I-1}}$ in $\bm \Gamma_\mathsf{i}$ is to make $\bm \Gamma_\mathsf{f}$ invertible. Further, there is
% \begin{align*}\label{eq.limit_inverse}
%     &\lim_{r\rightarrow \infty}\bm \Sigma_{\bm x}\\
%     =&\lim_{r\rightarrow \infty}(\bm \Sigma_\mathsf{f}^{-1}+\!\bm E\diag(([\bm \gamma_1]_{1}\otimes_{i=2}^I\bm \gamma_i)^{-1}\!-\!\bm 1_{N^{I-1}})\bm E^\mathsf{T})^{-1}\\
%     =&\lim_{r\rightarrow \infty} \bm \Sigma_\mathsf{f}-\bm \Sigma_\mathsf{f}\bm E(\diag(([\bm \gamma_1]_{1}\otimes_{i=2}^I\bm \gamma_i)^{-1}\!-\!\bm 1_{N^{I-1}})^{-1}+\bm E^\mathsf{T}\bm \Sigma_\mathsf{f}\bm E)^{-1}\\
%     &\times\bm E^\mathsf{T}\bm \Sigma_\mathsf{f}\\
%     =&\bm \Sigma_\mathsf{f}-\bm \Sigma_\mathsf{f} \bm E (\bm E^\mathsf{T} \bm \Sigma_{\mathsf{f}}\bm E)^{-1}\bm E^\mathsf{T}\bm \Sigma_\mathsf{f}.\\
%     % &=\left[\begin{smallmatrix}
%     %     \bm 0 & \bm 0\\
%     %     \bm 0 & *
%     % \end{smallmatrix}\right], 
% \end{align*}
% If we partition $\bm \Sigma_\mathsf{f}=\left[\begin{smallmatrix}
%         \bm \Sigma_\mathsf{f1} & \bm \Sigma_\mathsf{f2}\\
%         \bm \Sigma_\mathsf{f2}^\mathsf{T} & *
%     \end{smallmatrix}\right]$,
% where $*$ is $(N-1)N^{I-1}\times(N-1)N^{I-1}$, then we can further simplify
% \begin{align}
%     \lim_{r\rightarrow \infty}\bm \Sigma_{\bm x}&=\bm \Sigma_\mathsf{f}-\bm \Sigma_\mathsf{f} \bm E (\bm E^\mathsf{T} \bm \Sigma_{\mathsf{f}}\bm E)^{-1}\bm E^\mathsf{T}\bm \Sigma_\mathsf{f}\\
%     &=\begin{bmatrix}
%         \bm \Sigma_\mathsf{f1} & \bm \Sigma_\mathsf{f2}\\
%         \bm \Sigma_\mathsf{f2}^\mathsf{T} & *
%     \end{bmatrix} - \begin{bmatrix}
%         \bm \Sigma_\mathsf{f1} \\
%         \bm \Sigma_\mathsf{f2}^\mathsf{T} 
%     \end{bmatrix} \bm \Sigma_\mathsf{f1}^{-1}\begin{bmatrix}
%         \bm \Sigma_\mathsf{f1} &
%         \bm \Sigma_\mathsf{f2}
%     \end{bmatrix}\\
%     &=\begin{bmatrix}
%         \bm \Sigma_\mathsf{f1} & \bm \Sigma_\mathsf{f2}\\
%         \bm \Sigma_\mathsf{f2}^\mathsf{T} & *
%     \end{bmatrix} - \begin{bmatrix}
%         \bm \Sigma_\mathsf{f1} & \bm \Sigma_\mathsf{f2}\\
%         \bm \Sigma_\mathsf{f2}^\mathsf{T} & \bm \Sigma_\mathsf{f2}^\mathsf{T}\bm \Sigma_\mathsf{f1}^{-1}\bm \Sigma_\mathsf{f2}
%     \end{bmatrix} \\
%     &= \begin{bmatrix}
%         \bm 0 & \bm 0\\
%         \bm 0 & *-\bm \Sigma_\mathsf{f2}^\mathsf{T}\bm \Sigma_\mathsf{f1}^{-1}\bm \Sigma_\mathsf{f2}
%     \end{bmatrix},
% \end{align}
% where $\bm \Sigma_\mathsf{f1}^{-1}$ always exists as $\bm \Sigma_{\mathsf{f}}$ is PD and for any non-zero vector $\bm x$ there is $\bm x^\mathsf{T}\bm \Sigma_\mathsf{f1}\bm x = [\bm x;\bm 0]^\mathsf{T}\bm \Sigma_\mathsf{f}[\bm x;\bm 0] > 0$. Thus, there is $\lim_{r\rightarrow \infty}[\bm d]_\mathcal{M}=0$. 

% To conclude, although Proposition \ref{thm.am_stationary} seems to evoke a contradiction between $\{\bm\gamma_i\}^{(r)}\!>\!0$ in each EM iteration and the expected sparsity of the hyperparameters, the convergent point $\{\bm \gamma_i\}^{(\infty)} \in \mathcal{C}\setminus\mathcal{C}_+$ can possess zero entries, upon the condition that $\bm d^{(r)}$ converges to the closure of $\{\bm d|\bm d > 0\}$, which is empirically observed.
% %\vspace{-0.2cm}
\subsection{Local minima of KroSBL cost function}
\label{sec:local_minima}
Having studied the convergence properties of the algorithm, we now look at the properties of the limit points. Unlike the previous section, the results of this section assume that the sparse vector is also Kronecker-structured. The first result of the section, Theorem~\ref{thm.local_minima_sparse}, proves that all local minima $\{\bm \gamma_i\}$ of the KroSBL cost function $\mathcal{L}$ in~\eqref{eq.cost_ml} are sparse. Subsequently, in Theorem~\ref{thm.no_local}, we derive an upper bound on the number of local minima of the KroSBL cost function.

\begin{thm}\label{thm.local_minima_sparse}
    In the noiseless setting, every local minimum of $\mathcal{L}$ is achieved at a sparse solution $\{\bm \gamma_i\}$, that is, $\|\bm \gamma_i\|_0 \leq {M_i}$ for $i=1,2,\cdots,I$, if the sparse vector is Kronecker-structured, i.e., $\bm x =\otimes_i^I\bm x_i$, for some $\bm x_i\in\mathbb{C}^{N}$.
\end{thm}
% {\color{red} what do you mean by a sparse solution? What level of sparsity?}
\begin{proof}
    See Appendix \ref{appe.sparse_local}.
\end{proof}

%While KroSBL generates a sparse solution, this sparsity can originate from two cases. First, some hyperparameter vectors are dense while others are sparse, leading to a sparse vector after the Kronecker product. Second, all hyperparameter vectors are sparse. Theorem 2 shows that the first case is infeasible. Now we shift to the upper bound for the number of local minima of $\mathcal{L}$.

Theorem \ref{thm.local_minima_sparse} 
%shows that the local minima of $\mathcal{L}$ are sparse. Further, it 
indicates that the local minimum is sparse, not because some hyperparameter vectors ($\bm\gamma_i$'s) are dense while others are sparse, leading to a sparse Kronecker product. Instead, it implies each $\bm\gamma_i$ generated by KroSBL is sparse, following our  Kronecker-structured support model. 

Now we discuss an upper bound for the number of local minima of $\mathcal{L}$. For this, we use the concept of unique representation property (URP). The matrix $\bm H$ is said to satisfy URP if any subset of ${\bar{M}}$ columns of $\bm H$ is linearly independent~\cite{wipf2004sparse}. If the dictionary $\bm H$ satisfies the URP, the number of local minima of $\mathcal{L}$ in \eqref{eq.cost_ml} without the Kronecker-structured support constraint $\mathcal{C}$ (the classic SBL algorithm) is~\cite{wipf2004sparse}
\begin{equation}
    \mathcal{N}_{\mathsf{SBL}} \leq \binom{N^I}{{\bar{M}}} - \sum_{p=1}^{P} \binom{N^I - D_{p}}{{\bar{M}}- D_{p}} + P\leq \binom{N^I}{{\bar{M}}},
\end{equation}
where $D_{p}$ is $\ell_0$-norm of the $p$th degenerate sparse solution of the SBL cost function, and $P$ is the number of sparse solution. When we impose the Kronecker-structured support constraint, the upper bound of the number of local minima decreases, as discussed~next.

\begin{thm}\label{thm.no_local}
Consider the model in~\eqref{eq.problem_basic} and assume that i) the noise variance $\sigma^2 = 0$, ii) $\bm H$ satisfies the URP, and iii) there exist $P_i$ degenerate sparse solutions $\bm x_{i,p}$, $p=1,2,\ldots,P_i$ such that $\bm y = (\otimes_{i=1}^I \bm H_i)(\otimes_{i=1}^I \bm x_{i,p})$ and 
    %the number of non-zero entries in $\bm x_{i,p} = D_{i,p} < N$.
    $\Vert\bm x_{i,p}\Vert_0 = D_{i,p} < {M_i}$. Then, the number of distinct local minima of the KroSBL cost $\mathcal{L}$ in $\mathcal{C}$, denoted as $\mathcal{N}$, satisfies
    \begin{equation}\label{eq.upper_local_minima}
        \mathcal{N} \leq \prod_{i=1}^I \left(  \binom{N}{{M_i}} -\sum_{p=1}^{P_i} \binom{N - D_{i,p}}{{M_i}- D_{i,p}} + P_i \right)\leq \prod_{i=1}^I\binom{N}{{M_i}}.
    \end{equation}
\end{thm}

\begin{proof}
    See Appendix \ref{appe.upper_bound}.
\end{proof}



%% Figure environment removed
%
%% Figure environment removed




Theorem \ref{thm.no_local} and the result for classical SBL uses the same assumption, $\bm H$ satisfies URP, to derive an upper bound for the distinct number of local minima. % when $\bm H$ satisfies URP. %Under the same assumption, the number of local minima of $\mathcal{L}$ of \eqref{} in the classic SBL algorithm is \cite{wipf2004sparse}
% \begin{equation}
%     \mathcal{N}_{\mathsf{SBL}} \leq \begin{pmatrix} M^I\\ N^I \end{pmatrix} -\sum_{p=1}^{P} \begin{pmatrix} M^I - D_{p}\\ N^I- D_{p} \end{pmatrix} + P,
% \end{equation}
% where $D_{p}$ is the number of non-zero entries of degenerate sparse solution of the SBL cost function. 
However, our result shows that $\mathcal{N}$ is dominated by $\prod_{i=1}^I\binom{N}{{M_i}}$ while $\mathcal{N}_{\mathsf{SBL}}$ is dominated by $\binom{N^I}{{\bar{M}}}>\prod_{i=1}^I\binom{N}{{M_i}}$. Thus, incorporating the Kronecker structure can greatly diminish the solution space, explaining the better reconstruction performance of the KroSBL.
% As established in \cite{wipf2004sparse}, all local minima of $\mathcal{L}$ are sparse. However, the fact that $\bm \gamma$ is sparse does not necessarily indicate that each $\bm \gamma_i$ is sparse. For example, one of $\bm \gamma_i$ could be dense while the rest only possess very few non-zeros. But our discussion clearly shows that incorporating the structural information eliminates such local minima. Then intuitively, the number of local minima is greatly reduced.

\section{Performance Evaluation}
\label{sec:numsimu}

We conduct numerical experiments to investigate the efficacy of our algorithms for sparse vector recovery. We evaluate the recovery performance of AM-KroSBL and SVD-KroSBL by comparing them with three benchmarking algorithms: the classic SBL (cSBL) \cite{wipf2004sparse}, KSBL \cite{chang2021sparse}, and KOMP \cite{caiafa2013computing}. %Codes are available here.

We choose $I=3$ i.e., $\bm H = \otimes_{i=1}^3\bm H_i$ and the sparse vector $\bm x = \otimes_{i=1}^3\bm x_i$ where $\bm x_i\in\mathbb{R}^{15\times 1}$. The entries of $\bm x_i\in\mathbb{R}^{15\times 1}$, $\bm H_1 \in \mathbb{R}^{M\times 15}$, $\bm H_2 \in \mathbb{R}^{12\times 15}$, and $\bm H_3 \in \mathbb{R}^{15\times 15}$ are drawn from $\mathcal{N}(0,1)$, where $M = \{6,8,10,12,14\}$, called measurement level, controls the total number of measurements $\bar{M}=180M$ (or the under-sampling ratio $\bar{M}/N^I=180M/3375)$. The sparsity level for each $\bm x_i$ is $S=\{2,3,4,5,6\}$, and the support is generated uniformly at random. The zero-mean additive white Gaussian measurement noise level is decided by the signal-to-noise ratio $\text{SNR~(dB)} = 10\log_{10}\mathbb{E}\{\|\bm H\bm x\|_2^2/\|\bm n\|_2^2\}$ and takes values from $\{5,10,15,20,25,30\}$. 


We use three metrics for the assessment: NMSE, support recovery rate (SRR), and run time. Here, we define 
\begin{align}
    \text{NMSE} &= \mathbb{E}\left\{\frac{\|\bm x-\hat{\bm x}\|_2}{\|\bm x\|_2}\right\}\\
    \text{SRR} &= \frac{|\supp(\hat{\bm x}) \cap \supp(\bm x) |}{|\supp(\hat{\bm x}) \cup \supp(\bm x) |},
\end{align}
where ${\bm x}$ is the ground truth and $\hat{\bm x}$ is the reconstructed vector. 
%The second is the support recovery rate $\text{SRR} = |\supp(\hat{\bm x}) \cap \supp(\bm x) |/\left(|\supp(\hat{\bm x}) - \supp(\bm x) | + |\supp(\bm x)|\right)$ \cite{wang2018alternative}, with $\supp(\cdot)$ representing the support of the argument vector. The third is computational time. We assess our algorithms against benchmarks with different values of SNR, $M$, and $K$. 
We set the maximum EM iteration to 150 EM for the SBL-based methods. We also implement the complexity reduction technique described in \cite{he2022structure} for AM- and SVD-KroSBL and the technique in \cite{chang2021sparse} for KSBL. The pruning is also included to prune small hyperparameters for the all SBL-based methods \cite{zhang2011clarify}. We average over a hundred independent realizations. Our observations from the results, summarized in Figs.~\ref{fig_con} to \ref{fig_ce} and Tables~\ref{tab:sparse_recovery} and \ref{tab.time_pc}, are as follows\footnote{
Our code is available \href{https://github.com/YanbinHe/JournalKroSBL.git}{here}. Also, see \href{https://github.com/Dingqinliu/Encryption_Matlab/blob/3fd2edaadf10b512810933465361cd2ee1af1337/encryption_based_on_CS_chaotic/Tensor_CS/Fig_8/tensor_OMPND.m}{GitHub link} for the KOMP code.}.
% In SVD-KroSBL and the classic SBL, we eliminate the hyperparameters that are smaller than $10^{-3}$ of the largest one while in AM-KroSBL this value is $10^{-2}$. We also eliminate the hyperparameters that are smaller than $10^{-4}$. The thresholds are chosen empirically. 
% We turn to the Oracle Least Squares (OLS) as the baseline, where $\supp(\bm x)$ is perfectly known.
%\vspace{-0.3cm}
\subsection{Convergence illustration}

% Figure environment removed

Fig. \ref{fig_con} demonstrates the convergence property of cSBL, KSBL, AM-, and SVD-KroSBL \emph{with and without pruning}. We include the convergence without pruning because our theoretical analysis does not account for pruning. First, we look at Fig. \ref{fig_con}. We note that our AM-KroSBL and SVD-KroSBL lead to lower NMSE because they incorporate the Kronecker structure, avoiding unwanted local minimum compared with cSBL. However, KSBL, despite incorporating the same prior knowledge, results in a higher NMSE than AM- and SVD-KroSBL. Interestingly, KSBL initially has a similar NMSE as AM-KroSBL, as both schemes solve~\eqref{eq.mstep} using the gradient-based iterative method. However, KSBL is trapped in a local minimum after a few iterations because it uses a loose approximation and does not constrain its hyperparameters $\{\bm \gamma_i\}$ in $\mathcal{C}$, i.e., does not normalize the hyperparameters. Our experiments show that the entries of some $\bm \gamma_i$'s of KSBL upon convergence become very small while others become very large. Although the Kronecker product of $\{\bm \gamma_i\}$ of both algorithms are the same initially, small $\bm \gamma_i$ values unstabilize KSBL. This numerical instability reduces the estimation accuracy and leads to a local minimum with high NMSE. 
%In fact, only cSBL, AM-, and SVD-KroSBL are able to converge in this case. KSBL outputs \texttt{NaN} and terminates. 
To mitigate this issue, pruning small components is useful, helping KSBL to converge numerically, as shown in Fig.~\ref{fig_con2}. Pruning also accelerates other schemes. However, NMSE performance is sensitive to the pruning threshold, which is only empirically optimized. A larger threshold leads to faster convergence but is at the risk of eliminating true components.
%\vspace{-0.3cm}
\subsection{Comparison with the state-of-the-art}

% Figure environment removed

%We assess the NMSE and SRR as functions of SNRs, $M$, and $K$ values. Fig.~\ref{fig.snr} shows that both AM- and SVD-KroSBL outperform the competing schemes, i.e., cSBL, KSBL, and KOMP, in terms of NMSE performance. In Fig. \ref{fig1.a}, we change SNRs while fixing the sparsity level $K=3$ and measurement level $M = 8$. 
Figs. \ref{fig1.a} and \ref{fig1.b} show that the performance of all the algorithms improves with SNR and the number of measurements $\bar{M}=180M$ (or equivalently under-sampling ratio $\bar{M}/N^I=180M/3375)$, as expected. Similarly, Fig. \ref{fig1.c} shows that increasing the sparsity level hinders the reconstruction of all schemes, as the number of measurements is unchanged. Further, our SVD-KroSBL outperforms all the other algorithms, both in terms of NMSE and SRR. The next best-performing algorithm is AM-KroSBL in most cases. The exception is in Fig. \ref{fig2.a}, where KSBL and cSBL have higher SRR than the AM-based when the SNR is low. This behavior is because the pruning step of the AM-based fails to eliminate small components outside the true support. However, the nonzero entries are recovered accurately, indicated by a low NMSE. Finally, KSBL is expected to perform better than cSBL. However, this is not true in the low-measurement regime due to its approximations, seen in  Fig. \ref{fig1.b}. %, where we fix $\text{SNR}=25\text{dB}$ and observe KSBL performs worse than cSBL in low measurement scenario. 
%From Fig. \ref{fig1.c}, we notice that increasing the sparsity level hinders the reconstruction of all schemes, as the number of measurements are unchanged. 
%But both the SVD- and AM-based still outperform the other schemes. Fig. \ref{fig2.a}, Fig. \ref{fig2.b}, and Fig. \ref{fig2.c} depict the SRR at different under-sampling ratios, i.e., $\bar{M}/N^I=180M/3375$. We can observe that SVD-based achieves the best recovery rate, seconded by the AM-based in most cases. 

%\subsection{Computation time}
Table \ref{tab:sparse_recovery} compares the run time of different algorithms. KOMP has the lowest run time due to its greedy nature but suffers from high NMSE and low SRR, as shown in Fig.~\ref{fig.snr}. SVD-KroSBL has the lowest run time among the remaining algorithms. It is faster as it is non-iterative and takes fewer EM iterations to converge (see Fig.~\ref{fig_con}), and also uses the complexity reduction technique \cite{he2022structure} in the E-step.% and pruning in the M-step step. 
We also observe that AM-KroSBL outperforms the KSBL and has a similar run time as cSBL. Although cSBL takes fewer EM iterations, each EM iteration of AM-KroSBL is faster than cSBL due to the complexity reduction technique \cite{he2022structure}. 
%the faster convergence of cSBL (see Fig. \ref{fig_con}) and the iterative M-step of AM-KroSBL. This behavior is because we implement the complexity reduction technique \cite{he2022structure}. Although the inference takes AM-KroSBL more EM iterations, each EM iteration is faster than cSBL. 
Further, unlike KSBL and cSBL, AM-KroSBL has an iterative M-step, but the inner loop converges quickly, making it faster than KSBL. %The complexity reduction technique \cite{he2022structure} also greatly reduces the computational burden in the E-step compared with the E-step of KSBL, explaining our observations.

\begin{table}[]
\caption{Computation time (in seconds) comparison with $\text{SNR}=25\text{dB}$ and measurement level $M=8$ (underlined text for \underline{the best} and boldface for \textbf{the second best})} %{color{red}Setting?}}
\scriptsize
\centering
\begin{tabular}{llllll}
\hline
%\multicolumn{6}{c}{Sparsity recovery: $\text{SNR}=25\text{dB}$, $M=8$}                                                                                                                                                                                      \\ \hline 
\multicolumn{1}{l|}{Sparsity level}                     & \multicolumn{1}{l|}{$S=2$}                       & \multicolumn{1}{l|}{$S=3$}                       & \multicolumn{1}{l|}{$S=4$}                       & \multicolumn{1}{l|}{$S=5$}                       & $S=6$                       \\ \hline
\hline
\multicolumn{1}{l|}{\multirow{1}{*}{AM-KroSBL}} & \multicolumn{1}{l|}{\multirow{1}{*}{14.068}} & \multicolumn{1}{l|}{\multirow{1}{*}{12.863}} & \multicolumn{1}{l|}{\multirow{1}{*}{11.215}} & \multicolumn{1}{l|}{\multirow{1}{*}{10.180}} & \multirow{1}{*}{10.070} \\ \hline
\multicolumn{1}{l|}{SVD-KroSBL}                 & \multicolumn{1}{l|}{\textbf{3.520}}          & \multicolumn{1}{l|}{\textbf{2.945}}          & \multicolumn{1}{l|}{\textbf{2.596}}          & \multicolumn{1}{l|}{\textbf{2.651}}          & \textbf{3.008}          \\ \hline
\multicolumn{1}{l|}{KSBL}                     & \multicolumn{1}{l|}{23.700}                  & \multicolumn{1}{l|}{22.858}                  & \multicolumn{1}{l|}{20.537}                  & \multicolumn{1}{l|}{19.413}                  & 18.632                  \\ \hline
\multicolumn{1}{l|}{cSBL}                       & \multicolumn{1}{l|}{12.837}                  & \multicolumn{1}{l|}{11.449}                  & \multicolumn{1}{l|}{10.510}                  & \multicolumn{1}{l|}{11.541}                  & 12.723                  \\ \hline
\multicolumn{1}{l|}{KOMP}                       & \multicolumn{1}{l|}{{\ul \textit{1.170}}}    & \multicolumn{1}{l|}{{\ul \textit{1.257}}}    & \multicolumn{1}{l|}{{\ul \textit{1.170}}}    & \multicolumn{1}{l|}{{\ul \textit{1.179}}}    & {\ul \textit{1.250}}    \\ \hline

\end{tabular}
\label{tab:sparse_recovery}
\vspace{0.2cm}
\end{table}


\subsection{Comparison of SVD-KroSBL and AM-KroSBL}\label{sec.svd_best}

We observe from Fig.\ref{fig.snr} that the SVD-KroSBL algorithm outperforms AM-KroSBL and cSBL with sufficient measurements regardless of its approximations. Here, we give the intuition behind the better performance of the SVD-KroSBL. In KroSBL, the M-step solves~\eqref{eq.mstep}. SVD-KroSBL approximates~\eqref{eq.mstep} by first identifying~\eqref{eq.mstep_svd} and then solving~\eqref{prob.bidecom}. Suppose $\bm d^{(r)} = \otimes_{i=1}^I \bm d_i^{(r)}$ for some nonnegative vectors $\bm d_i^{(r)}\in\mathbb{R}^{N\times 1}$. Then the optimal solution to \eqref{eq.mstep} is attained at $\check{\bm\gamma}_i=\bm d_i^{(r)}/\Vert\bm d_i^{(r)}\Vert$ for $i=1,2,\ldots,I-1$, and $\check{\bm\gamma}_I=\prod_{i=1}^{I-1}\Vert\bm d_i^{(r)}\Vert\bm d_I^{(r)}$. Since \eqref{eq.mstep_svd} attains the optimal value at $\check{\bm\gamma}=\bm d^{(r)}$ that satisfies the constraints of \eqref{eq.mstep}, $\check{\bm\gamma}=\bm d^{(r)}$ is also optimal for \eqref{eq.mstep}. Hence, the SVD-based is exact when $\bm d^{(r)}$ is Kronecker-structured. Further, we empirically verified the rank of rearranged/devectorized $\bm d^{(r)}$ when $I = 3$, where $\text{rank} = 1$ indicates $\bm d^{(r)} = \otimes_{i=1}^3 \bm d_i^{(r)}$. Without noise, the SVD-based method drives $\bm d^{(r)}$ to this structure within a few EM iterations and retains this structure, as shown in Fig. \ref{fig.rank}. Therefore, after a few EM iterations, the SVD-based method solves the M-step exactly and outperforms the iterative first-order optimization in the AM-KroSBL algorithm. Furthermore, Fig. \ref{fig.rank} also indicates that SVD-based converges to a Kronecker-structured $\bm d^{(r)}$ faster than AM-KroSBL and cSBL. Thus, SVD can be viewed as a stronger imposement of the Kronecker structure that accelerates EM compared with the other algorithms. %Due to this refinement and acceleration, SVD-based performs better than AM-based and cSBL given the limited number of iterations. 

% \begin{table*}[]
% \caption{Computational time for different schemes. \underline{The best}. \textbf{The second best}.}
% \scriptsize
% \begin{tabular}{llllllll|}
% \hline
% \multicolumn{8}{c|}{Sparsity recovery: $K=4$, $M=8$}                                                                                                   & \multicolumn{8}{c}{Channel estimation: $K_I = 6$}                                                                                                                                                                                                                                                                                                                                                                                                    \\ \hline
% \multicolumn{2}{|l|}{SNR}                                                & \multicolumn{1}{l|}{5dB}                                             & \multicolumn{1}{l|}{10dB}                                            & \multicolumn{1}{l|}{15dB}                                            & \multicolumn{1}{l|}{20dB}                                            & \multicolumn{1}{l|}{25dB}                                            & 30dB                                           & \multicolumn{2}{l|}{SNR}                                            & \multicolumn{1}{l|}{6dB}                                    & \multicolumn{1}{l|}{8dB}                                    & \multicolumn{1}{l|}{10dB}                                   & \multicolumn{1}{l|}{12dB}                                  & \multicolumn{1}{l|}{16dB}                                  & \multicolumn{1}{l|}{20dB}                                  \\ \hline
% \multicolumn{2}{l|}{{\color[HTML]{000000} }}                            & \multicolumn{1}{l|}{{\color[HTML]{000000} }}                         & \multicolumn{1}{l|}{{\color[HTML]{000000} }}                         & \multicolumn{1}{l|}{{\color[HTML]{000000} }}                         & \multicolumn{1}{l|}{{\color[HTML]{000000} }}                         & \multicolumn{1}{l|}{{\color[HTML]{000000} }}                         & {\color[HTML]{000000} }                        & \multicolumn{2}{l|}{{\color[HTML]{000000} PC-KroSBL $\beta = 0.5$}} & \multicolumn{1}{l|}{{\color[HTML]{000000} \textbf{22.478}}} & \multicolumn{1}{l|}{{\color[HTML]{000000} \textbf{14.979}}} & \multicolumn{1}{l|}{{\color[HTML]{000000} \textbf{12.281}}} & \multicolumn{1}{l|}{{\color[HTML]{000000} \textbf{9.785}}} & \multicolumn{1}{l|}{{\color[HTML]{000000} \textbf{7.592}}} & \multicolumn{1}{l|}{{\color[HTML]{000000} \textbf{7.167}}} \\ \cline{9-16} 
% \multicolumn{2}{l|}{\multirow{-2}{*}{{\color[HTML]{000000} AM-KroSBL}}} & \multicolumn{1}{l|}{\multirow{-2}{*}{{\color[HTML]{000000} 32.785}}} & \multicolumn{1}{l|}{\multirow{-2}{*}{{\color[HTML]{000000} 32.005}}} & \multicolumn{1}{l|}{\multirow{-2}{*}{{\color[HTML]{000000} 25.881}}} & \multicolumn{1}{l|}{\multirow{-2}{*}{{\color[HTML]{000000} 17.448}}} & \multicolumn{1}{l|}{\multirow{-2}{*}{{\color[HTML]{000000} 12.554}}} & \multirow{-2}{*}{{\color[HTML]{000000} 9.601}} & \multicolumn{2}{l|}{{\color[HTML]{000000} PC-KroSBL $\beta = 0.8$}} & \multicolumn{1}{l|}{{\color[HTML]{000000} {\ul \it 22.412}}}    & \multicolumn{1}{l|}{{\color[HTML]{000000} 17.515}}          & \multicolumn{1}{l|}{{\color[HTML]{000000} 15.937}}          & \multicolumn{1}{l|}{{\color[HTML]{000000} 14.027}}         & \multicolumn{1}{l|}{{\color[HTML]{000000} 10.363}}         & \multicolumn{1}{l|}{{\color[HTML]{000000} 9.18}}           \\ \hline
% \multicolumn{2}{l|}{{\color[HTML]{000000} SVD-KroSBL}}                  & \multicolumn{1}{l|}{{\color[HTML]{000000} \textbf{6.571}}}           & \multicolumn{1}{l|}{{\color[HTML]{000000} \textbf{5.094}}}           & \multicolumn{1}{l|}{{\color[HTML]{000000} \textbf{4.113}}}           & \multicolumn{1}{l|}{{\color[HTML]{000000} \textbf{3.796}}}           & \multicolumn{1}{l|}{{\color[HTML]{000000} \textbf{3.541}}}           & {\color[HTML]{000000} \textbf{3.510}}          & \multicolumn{2}{l|}{{\color[HTML]{000000} AM-KroSBL}}               & \multicolumn{1}{l|}{{\color[HTML]{000000} 38.784}}          & \multicolumn{1}{l|}{{\color[HTML]{000000} 23.819}}          & \multicolumn{1}{l|}{{\color[HTML]{000000} 15.460}}          & \multicolumn{1}{l|}{{\color[HTML]{000000} 12.399}}         & \multicolumn{1}{l|}{{\color[HTML]{000000} 11.593}}         & \multicolumn{1}{l|}{{\color[HTML]{000000} 12.571}}         \\ \hline
% \multicolumn{2}{l|}{{\color[HTML]{000000} KroSBL}}                      & \multicolumn{1}{l|}{{\color[HTML]{000000} 46.376}}                   & \multicolumn{1}{l|}{{\color[HTML]{000000} 42.599}}                   & \multicolumn{1}{l|}{{\color[HTML]{000000} 34.641}}                   & \multicolumn{1}{l|}{{\color[HTML]{000000} 29.910}}                   & \multicolumn{1}{l|}{{\color[HTML]{000000} 25.431}}                   & {\color[HTML]{000000} 22.011}                  & \multicolumn{2}{l|}{{\color[HTML]{000000} SVD-KroSBL}}              & \multicolumn{1}{l|}{{\color[HTML]{000000} 25.117}}          & \multicolumn{1}{l|}{{\color[HTML]{000000} {\ul \it 13.515}}}    & \multicolumn{1}{l|}{{\color[HTML]{000000} {\ul \it 9.233}}}     & \multicolumn{1}{l|}{{\color[HTML]{000000} {\ul \it 7.125}}}    & \multicolumn{1}{l|}{{\color[HTML]{000000} {\ul \it 6.034}}}    & \multicolumn{1}{l|}{{\color[HTML]{000000} {\ul \it 5.529}}}    \\ \hline
% \multicolumn{2}{l|}{{\color[HTML]{000000} cSBL}}                        & \multicolumn{1}{l|}{{\color[HTML]{000000} 51.991}}                   & \multicolumn{1}{l|}{{\color[HTML]{000000} 34.849}}                   & \multicolumn{1}{l|}{{\color[HTML]{000000} 24.445}}                   & \multicolumn{1}{l|}{{\color[HTML]{000000} 18.609}}                   & \multicolumn{1}{l|}{{\color[HTML]{000000} 18.858}}                   & {\color[HTML]{000000} 25.008}                  & \multicolumn{2}{l|}{{\color[HTML]{000000} PC-SBL}}                  & \multicolumn{1}{l|}{{\color[HTML]{000000} 124.591}}         & \multicolumn{1}{l|}{{\color[HTML]{000000} 119.326}}         & \multicolumn{1}{l|}{{\color[HTML]{000000} 110.246}}         & \multicolumn{1}{l|}{{\color[HTML]{000000} 88.043}}         & \multicolumn{1}{l|}{{\color[HTML]{000000} 69.286}}         & \multicolumn{1}{l|}{{\color[HTML]{000000} 68.920}}         \\ \hline
% \multicolumn{2}{l|}{{\color[HTML]{000000} KOMP}}                        & \multicolumn{1}{l|}{{\color[HTML]{000000} {\ul \it 2.677}}}              & \multicolumn{1}{l|}{{\color[HTML]{000000} {\ul \it 2.470}}}              & \multicolumn{1}{l|}{{\color[HTML]{000000} {\ul \it 2.729}}}              & \multicolumn{1}{l|}{{\color[HTML]{000000} {\ul \it 2.819}}}              & \multicolumn{1}{l|}{{\color[HTML]{000000} {\ul \it 2.830}}}              & {\color[HTML]{000000} {\ul \it 2.494}}             & \multicolumn{2}{l|}{{\color[HTML]{000000} cSBL}}                    & \multicolumn{1}{l|}{{\color[HTML]{000000} 59.753}}          & \multicolumn{1}{l|}{{\color[HTML]{000000} 35.953}}          & \multicolumn{1}{l|}{{\color[HTML]{000000} 25.067}}          & \multicolumn{1}{l|}{{\color[HTML]{000000} 21.400}}         & \multicolumn{1}{l|}{{\color[HTML]{000000} 15.922}}         & \multicolumn{1}{l|}{{\color[HTML]{000000} 12.846}}         \\ \hline
% \end{tabular}
% \label{tab_ct}
% \end{table*}

% Further, it can be seen that it takes more iterations for AM-based to converge to the Kronecker structure compared with cSBL, aligning with Fig. \ref{fig.convergence} in the noisy setting. Being Kronecker structured is not a necessary condition for good reconstruction performance, possessing low-rank property can already provide better estimations. That is why AM-based is still better than cSBL despite its slow convergence. 


% Figure environment removed

% % Figure environment removed
\subsection{IRS-aided channel estimation}
In this subsection, we present the observation from the results obtained when our PC-KroSBL is applied to the IRS-aided MIMO channel estimation, as discussed in Sec.~\ref{sec:channelesti}. We choose the number of BS antennas $R=16$, the number of MS antennas $T=6$, and the number of IRS elements $L=256$. Each entry of the IRS configurations $\{\bm \theta_k\}_{k=1}^{K_{\mathrm{I}}}$ is uniformly drawn from $\{-1/\sqrt{N}$ $,1/\sqrt{N}\}$ with $K_{\mathrm{I}}=6$ and $K_{\mathrm{P}}=3$. The number of grid angles is $N=18$, and all AoDs/AoAs are drawn from the grids. We assume the angle spreads over three grid points, leading to three consecutive non-zeros in sparse vectors $\bm g_{\mathrm{L}}$ and $\bm g_{\mathrm{R}}$. For this, we choose one grid point uniformly at random and add the selected grid and its two neighboring grids to the support. The channel gains $\beta_{{\mathrm{MS}},p}$ and $\beta_{{\mathrm{BS}},p}$ defined in Sec.~\ref{sec:channelesti} are drawn from $\mathcal{CN}(0,1)$~\cite{lin2021channel}. The performance of our PC-KroSBL is compared with PC-SBL \cite{fang2014pattern} and cSBL \cite{wipf2004sparse}. The pattern-coupled coefficient is $0.9$ in the PC-SBL algorithm. We choose $\beta=0.5$ and $\beta=0.8$. The performance metrics are NMSE, symbol error rate (SER), and run time. Here, channel estimation NMSE is given as
\begin{equation*}
\frac{1}{K_{\mathrm{I}}}\sum_{k = 1}^{K_{\mathrm{I}}}\!\!\frac{\|\bm H_\mathrm{BS} \diag (\bm \theta_{k}) \bm H_\mathrm{MS} - \tilde{\bm H}_\mathrm{BS} \diag (\bm \theta_{k}) \tilde{\bm H}_\mathrm{MS}\|_F^2}{\|\bm H_\mathrm{BS} \diag (\bm \theta_{k}) \bm H_\mathrm{MS}\|_F^2},
\end{equation*}
with $\tilde{\bm H}_\mathrm{BS} \diag (\bm \theta_{k}) \tilde{\bm H}_\mathrm{MS}$ being the reconstructed channel. SER is computed over data transmission containing $10^5$ uncoded QPSK symbols. The results are averaged over fifty independent channel realizations.

  % Figure environment removed

\begin{table}[h]
\centering
\caption{
Computation time (in seconds) comparison for the IRS-aided MIMO channel estimation with $K_{\mathrm{I}}=6$ and $K_{\mathrm{P}}=3$ (underlined text for \underline{the best} and boldface for \textbf{the second best})%Computation time. \underline{The best}. \textbf{The second best}.}%{\color{red}setting?
}
\scriptsize
\begin{tabular}{llllll}
\hline
%\multicolumn{6}{c}{IRS-aided MIMO channel estimation: $K_{\mathrm{I}}=6$, $K_{\mathrm{P}}=3$}                                                                                                                                                                                                      \\ \hline
\multicolumn{1}{l|}{SNR}           & \multicolumn{1}{l|}{4dB}                     & \multicolumn{1}{l|}{8dB}                    & \multicolumn{1}{l|}{12dB}                   & \multicolumn{1}{l|}{16dB}                   & 20dB                   \\ \hline \hline
\multicolumn{1}{l|}{PC-KroSBL $\beta = 0.8$} & \multicolumn{1}{l|}{{\ul \textit{32.735}}} & \multicolumn{1}{l|}{15.172}                & \multicolumn{1}{l|}{10.338}               & \multicolumn{1}{l|}{8.559}                & 8.139                \\ \hline
\multicolumn{1}{l|}{PC-KroSBL $\beta = 0.5$} & \multicolumn{1}{l|}{\textbf{36.281}}       & \multicolumn{1}{l|}{\textbf{14.832}}       & \multicolumn{1}{l|}{\textbf{9.391}}       & \multicolumn{1}{l|}{\textbf{7.704}}       & \textbf{7.223}       \\ \hline
\multicolumn{1}{l|}{AM-KroSBL}     & \multicolumn{1}{l|}{54.798}                & \multicolumn{1}{l|}{25.414}                & \multicolumn{1}{l|}{12.103}               & \multicolumn{1}{l|}{10.431}               & 11.312               \\ \hline
\multicolumn{1}{l|}{SVD-KroSBL}    & \multicolumn{1}{l|}{41.954}                & \multicolumn{1}{l|}{{\ul \textit{14.426}}} & \multicolumn{1}{l|}{{\ul \textit{7.472}}} & \multicolumn{1}{l|}{{\ul \textit{6.485}}} & {\ul \textit{6.035}} \\ \hline
\multicolumn{1}{l|}{PC-SBL}        & \multicolumn{1}{l|}{91.534}                & \multicolumn{1}{l|}{86.007}                & \multicolumn{1}{l|}{64.974}               & \multicolumn{1}{l|}{50.231}               & 47.629               \\ \hline
\multicolumn{1}{l|}{cSBL}          & \multicolumn{1}{l|}{84.592}                & \multicolumn{1}{l|}{26.948}                & \multicolumn{1}{l|}{14.638}               & \multicolumn{1}{l|}{11.579}               & 9.250                \\ \hline
\end{tabular}
\label{tab.time_pc}
\vspace{0.2cm}
\end{table}
We focus on the low measurement regime with the under-sampling ratio $\bar{M}/\bar{N} = RK/N^3 \approx 5\%$. Fig. \ref{fig_ce} shows that PC-KroSBL outperforms AM- and SVD-KroSBL that do not account for block sparsity in terms of NMSE and SER. Comparing the performance of PC-KroSBL for different values of $\beta$, we infer that $\beta=0.8$ achieves better accuracy and lower error than $\beta=0.5$ case.
%This behavior is evident since PC-KroSBL $\beta = 0.8$ consistently achieves lower NMSE and SER in all the cases. 
Another interesting observation is that AM-KroSBL has higher NMSE but lower SER than SVD-KroSBL when $\text{SNR} = \{4,8,12,16\}$. This is because SVD-KroSBL fails to identify correct AoAs and AoDs for some channel realizations, resulting in severe channel estimation errors. In such cases, SER is close to one, significantly affecting the overall SER performance of SVD-KroSBL. %For instance, if SVD-KroSBL fails for one channel realization, and all transmissions are consequently wrong, i.e., $\text{SER}=1$. It needs successful estimation for at least another $10^4$ channel realizations to balance this failure to achieve $\text{SER}\!=\!10^{-4}$ after averaging over all realizations. This explains why SVD-KroSBL has lower NMSE but higher SER since SVD-KroSBL fails occasionally.
However, the AM-KroSBL does not completely fail even in the low measurement scenario and works better with increasing SNR. Moreover, PC-KroSBL can consistently and accurately estimate the channel and ensure low SER with limited measurements. 

Further, PC-SBL fails in all SNR scenarios because it does not exploit the Kronecker structure and hence, it needs more measurements than KroSBL algorithms for a graceful recovery. Also, while both sparse vectors $\bm g_{\mathrm{L}}$ and $\bm g_{\mathrm{B}}$ exhibit block sparsity, PC-SBL can only exploit the block sparsity of the $\bm g_{\mathrm{L}}\otimes\bm g_{\mathrm{B}}$. In contrast, PC-KroSBL can utilize the block sparsity of both $\bm g_{\mathrm{L}}$ and $\bm g_{\mathrm{B}}$, which is an added advantage of our algorithm. Finally, unlike PC-SBL, the cSBL algorithm involves no approximation, because of which it performs better than PC-SBL in the low measurement regime. However, our KroSBL outperforms cSBL, in which no prior knowledge is incorporated.
% {\color{red} what about cSBL? }
 
Table \ref{tab.time_pc} includes the computation time of different schemes. PC-KroSBL has a comparable run time to SVD-KroSBL and is faster than PC-SBL and cSBL despite its iterative M-step (inner loop). This is due to the complexity reduction technique \cite{he2022structure} and the fast convergence of the inner loop. 

% % Figure environment removed

\section{Conclusion}

In this paper, we solved the Kronecker-structured linear inversion problem using the SBL framework with the EM procedure. To encourage the Kronecker-supported sparse vector, we designed a prior distribution parameterized by a Kronecker-structured hyperparameter vector and derived two Bayesian algorithms: an iterative AM-based and non-iterative SVD-based algorithm. The AM-based algorithm is theoretically guaranteed to converge to the stationary point of the SBL cost function, while the SVD-based algorithm is faster and has better reconstruction accuracy. We applied our algorithm to the IRS-aided channel estimation for MIMO systems and extended the framework to the block sparsity case. We also analyzed the local minima of the SBL cost function. The analysis of SVD-KroSBL and devising real-time sparse recovery algorithms exploiting the Kronecker structure with reduced complexity can be exciting topics for future work.


 \section*{Acknowledgments}
 We thank Dr. David Wipf, Jianzhe Liang, Changheng Li, and Yanyan Hu for insightful discussions on the theoretical analysis and Sofia-Eirini Kotti for helping us with the coding.

\appendices
\section{Proof of Proposition \ref{thm.am_stationary}}
\label{appe.am}
The proof uses Zangwill's Convergence Theorem A \cite{zangwill1969nonlinear} and the following proposition to arrive at the desired results:

\begin{proposition}\label{thm.coercive_open}
Let $f(\bm x): \mathcal{X}\rightarrow\mathbb{R}$ be a continuous function. If
    \begin{equation}\label{eq.coercive_open}
                f(\bm x)\rightarrow+\infty\ \text{as}\ \mathcal{X} \ni \bm x \rightarrow \mathrm{bd} (\mathcal{X})\ \text{or}\ \|\bm x\| \rightarrow +\infty,
    \end{equation}
    where $\mathrm{bd} (\mathcal{X})$ denotes the boundary of $\mathcal{X}$, then any sublevel set $\mathcal{S}:\{\bm x|f(\bm x)\leq c\}$ is compact.
\end{proposition}

\begin{proof}
% Similar statement can be found in . 
% {\color{red} There will be a natural question of whether the result is known or not. So mention that we adopted the idea from ``some reference''.}
This proof is adopted from~\cite{calafiore2014optimization}. A set is compact if it is closed and bounded~\cite{calafiore2014optimization}. We start with the proof of boundedness. Suppose $\mathcal{S}$ is unbounded, then there must exist a sequence $\{\bm x_n\} \subset \mathcal{S}$ with $\|\bm x_n\|\rightarrow +\infty$. Then, \eqref{eq.coercive_open} indicates $f(\bm x_n)\rightarrow +\infty$. Therefore, there exists an integer $n_0>0$ such that $f(\bm x_n) > c$, for $ n > n_0$, which is a contradiction to the assumption $\{\bm x_n\} \subset \mathcal{S}$. So $\mathcal{S}$ is bounded.


We next complete the proof by establishing the closedness of $\mathcal{S}$. Suppose $\mathcal{S}$ is not a closed set. Then, there exists at least one sequence $\{\bm x_n\in \mathcal{S}\}$ converging to $ \bar{\bm x}\notin\mathcal{S}$, which implies either $\bar{\bm x}\in \mathcal{X}$ with $f(\bar{\bm x}) > c$, or $\bar{\bm x} \in \mathrm{bd} (\mathcal{X})$. However, $\bar{\bm x} \in \mathrm{bd} (\mathcal{X})$ further implies that $ f(\bm x_n) \to +\infty$ from \eqref{eq.coercive_open}, which is a contradiction. If  $\bar{\bm x}\in \mathcal{X}$ with $f(\bar{\bm x}) > c$, then there exists a neighbourhood $\mathcal{V}_0$ of $\bar{\bm x}$ such that  $f(\bm x)>c$ for any $\bm x \in \mathcal{V}_0$. The existence of $\mathcal{V}_0$ is guaranteed because $f(\bm x)$ is continuous at $\bar{\bm x}$. However, because $\{\bm x_n\in \mathcal{S}\}$ converges to $ \bar{\bm x}\notin\mathcal{S}$, there exists an integer $n_0$ such that $\bm x_{n} \in \mathcal{V}_0$, for $n > n_0$. Hence, $f(\bm x_n) > c$, leading to a contradiction to the assumption that $\{\bm x_n\} \subset \mathcal{S}$.
% We next show that both these cases lead to a contradiction.
% \begin{enumerate}[leftmargin=*]
%     \item Suppose  $\bar{\bm x}\in \mathcal{X}$ with $f(\bar{\bm x}) > c$. Since $f(\bm x)$ is continuous at $\bar{\bm x}$ and $f(\bar{\bm x}) > c$, then there exists a neighbourhood $\mathcal{V}_0$ of $\bar{\bm x}$ such that  $f(\bm x)>c$ for any $\bm x \in \mathcal{V}_0$. However, because $\{\bm x_n\in \mathcal{S}\}$ converges to $ \bar{\bm x}\notin\mathcal{S}$, there exists an integer $n_0$ such that $\{\bm x_{n}\} \in \mathcal{V}_0$, for $n > n_0$. Hence, $f(\bm x_n) > c$, leading to a contradiction to the assumption that $\{\bm x_n\} \subseteq \mathcal{S}$.
%     \item Suppose that $\bar{\bm x} \in \mathrm{bd} (\mathcal{X})$. Then, $ f(\bm x_n) \to +\infty$ from \eqref{eq.coercive_open}. Therefore, there exists an integer $n_0>0$ such that $f(\bm x_n) > c$, for $ n > n_0$, which is a contradiction to the assumption $\{\bm x_n\} \subseteq \mathcal{S}$.
% \end{enumerate}
Thus, $\mathcal{S}$ contains all its limit points, proving that $\mathcal{S}$ is closed and the proof is complete. 
%Thus, set $\mathcal{S}$ is compact upon the closedness and boundedness.
\end{proof}

Next, we prove the proposition using the above results. We let the sequence $\{\bm\gamma_i\}^{(t)}|_{t=1}^\infty$ be generated by the AM in Algorithm \ref{al.AMKroSBL} with the starting point $\{\bm\gamma_i\}^{(1)} \in \mathcal{C}_+$, where we omit the EM index $r$ for brevity.  We also define the mapping from $\{\bm \gamma_i\}^{(t)}$ to $\{\bm \gamma_i\}^{(t+1)}$ in Algorithm \ref{al.AMKroSBL} as a function $M(\cdot)$, i.e., $M(\{\bm \gamma_i\}^{(t)})=\{\bm \gamma_i\}^{(t+1)}$. We prove the result using the Zangwill's convergence theorem~\cite{zangwill1969nonlinear}. Suppose the following conditions from the Zangwill's convergence theorem hold,
\begin{enumerate}
    \item If the sequence $\{\bm\gamma_i\}^{(t)}|_{t=1}^\infty$ is in a compact set $\mathcal{S} \subset \mathcal{C}_+$. \label{condition1}
    \item If $\{\bm\gamma_i\}^{(t)}$ is not a stationary point of $Q(\{\bm \gamma_i\})$, \label{condition2} %for \label{condition2} $\{\bm\gamma_i\}^{(t+1)} = M(\{\bm\gamma_i\}^{(t)})$, there is
    \begin{equation}
        Q(\{\bm\gamma_i\}^{(t)}) > Q(\{\bm\gamma_i\}^{(t+1)}).
    \end{equation}
    \item If $\{\bm\gamma_i\}^{(t)}$ is a stationary point of $Q(\{\bm \gamma_i\})$, the AM step terminates or %for $\{\bm\gamma_i\}^{(t+1)} = M(\{\bm\gamma_i\}^{(t)})$, there is 
    \label{condition3}
    \begin{equation}
        Q(\{\bm\gamma_i\}^{(t)}) \geq Q(\{\bm\gamma_i\}^{(t+1)}).
    \end{equation}
    \item Function $Q(\{\bm \gamma_i\})$ is continuous in $\{\bm \gamma_i\}$ and  $M(\cdot)$ is continuous at \label{condition4} $\{\bm\gamma_i\}^{(t)}$ if $\{\bm\gamma_i\}^{(t)}$ is not a stationary point.
    \end{enumerate}
Then, Zangwill's theorem \cite{zangwill1969nonlinear} guarantees that the AM algorithm stops at a stationary point of $Q(\{\bm \gamma_i\})$. 
%At the high level, the proof contains the following three parts:
% to verify the condition \ref{condition1} to \ref{condition4} for $\{\bm\gamma_i\}^{(t)}|_{t=0}^\infty$ and the AM algorithm.
Consequently, in the remainder of the proof, we verify Conditions~\ref{condition1}-\ref{condition4}.% to prove Proposition \ref{thm.am_stationary}. 

% \subsection{Non-increasing property of \texorpdfstring{$Q(\{\bm\gamma_i\}^{(t)})|_{t=1}^\infty$}{TEXT}}\label{sec.non_increase_Q}
% Sequence $Q(\{\bm\gamma_i\}^{(t)})|_{t=1}^\infty$ being non-increasing indicates
% \begin{equation}
%         Q(\{\bm \gamma_i\}^{(t)})\geq Q(\{\bm \gamma_i\}^{(t+1)}).
% \end{equation}

% \noindent We express $Q=\log|\diag(\bm\gamma)|+(\bm d^{(r)})^{\mathsf{T}}\bm\gamma^{-1}$ regarding $\bm \gamma_1$:
% \begin{equation}
% \begin{aligned}\label{eq.cost_single}
%    Q(\bm \gamma_1)  &=N^{I-1}\log |\diag(\bm \gamma_1)| + (\bm d^{(r)})^{\mathsf{T}}(\bm \gamma_1 \otimes (\otimes_{i=2}^I\bm \gamma_i))^{-1} + C\\
%    &= \sum_{n_1=1}^N N^{I-1}\log [\bm \gamma_1]_{n_1} + \frac{1}{[\bm \gamma_1]_{n_1}} (\otimes_{i=2}^I\bm \gamma_i^{-1})^{\mathsf{T}}\bm d_{n_1}^{(r)} + C
% \end{aligned}    
% \end{equation}
% where scalar $C$ is irrelevant to $\bm \gamma_1$. It can be seen that $\underset{\substack{\bm\gamma_1}}{\min}\ Q(\bm \gamma_1)$ is separable, i.e., is equivalent to solve for $n_1=1,2,\ldots,N$
% \begin{equation}
%     \underset{\substack{[\bm\gamma_1]_{n_1}}}{\min}\  N^{I-1}\log [\bm \gamma_1]_{n_1} \\+ \frac{1}{[\bm \gamma_1]_{n_1}} (\otimes_{i=2}^I\bm \gamma_i^{-1})^{\mathsf{T}}\bm d^{(r)}_{n_1},
% \end{equation}
% with $\bm d^{(r)}_{n_1} = [\bm d^{(r)}]_{(n_1-1)N^{I-1}+1:n_1N^{I-1}}$, which is minimized when $\tilde{\bm \gamma}_1 = N^{-I+1}(\bm I \otimes (\otimes_{i=2}^I\bm \gamma_i^{-1}))^{\mathsf{T}}\bm d^{(r)}$ as in~\eqref{eq.update} and applies to all $\{\bm \gamma_i\}$. Thus, \emph{within $r$th EM iteration}, during \emph{the $t$th iteration of AM} of Algorithm \ref{al.AMKroSBL}, we have
% \begin{equation*}
% % \label{eq.em_am_update}
%         Q(\{\bm \gamma_i\}^{(t)})\geq Q(\tilde{\bm \gamma}_1,\{\bm \gamma_i\}^{(t)}_{i=2:I})\geq Q(\{\tilde{\bm \gamma}_i\})\overset{(a)}{=}Q(\{\bm \gamma_i\}^{(t+1)}),
% \end{equation*}
% where $(a)$ is the projection. Thus, the sequence $Q(\{\bm\gamma_i\}^{(t)})|_{t=1}^\infty$ is non-increasing. 

%\subsection{Proof of Condition \ref{condition1}}\label{appe.am_compact}

% \begin{table}[t!]
% \centering
% \caption{Computation time for different schemes. \underline{The best}. \textbf{The second best}.}
% \scriptsize
% \begin{tabular}{llllll}
% \hline
% \multicolumn{6}{c}{IRS-aided communication system channel estimation}                                                                                                                                                                                             \\ \hline
% \multicolumn{1}{l|}{SNR}           & \multicolumn{1}{l|}{4dB}                     & \multicolumn{1}{l|}{8dB}                    & \multicolumn{1}{l|}{12dB}                   & \multicolumn{1}{l|}{16dB}                   & 20dB                   \\ \hline
% \multicolumn{1}{l|}{PC-KroSBL $\beta=0.8$} & \multicolumn{1}{l|}{{\ul \textit{31.420}}} & \multicolumn{1}{l|}{14.882}                & \multicolumn{1}{l|}{9.660}                & \multicolumn{1}{l|}{8.199}                & 7.618                \\ \hline
% \multicolumn{1}{l|}{PC-KroSBL $\beta=0.5$} & \multicolumn{1}{l|}{\textbf{34.998}}       & \multicolumn{1}{l|}{\textbf{12.903}}       & \multicolumn{1}{l|}{\textbf{7.230}}       & \multicolumn{1}{l|}{{\ul \textit{5.340}}} & {\ul \textit{4.982}} \\ \hline
% \multicolumn{1}{l|}{AM-KroSBL}     & \multicolumn{1}{l|}{53.775}                & \multicolumn{1}{l|}{23.485}                & \multicolumn{1}{l|}{10.613}               & \multicolumn{1}{l|}{8.189}                & 7.137                \\ \hline
% \multicolumn{1}{l|}{SVD-KroSBL}    & \multicolumn{1}{l|}{39.895}                & \multicolumn{1}{l|}{{\ul \textit{12.766}}} & \multicolumn{1}{l|}{{\ul \textit{6.262}}} & \multicolumn{1}{l|}{\textbf{5.728}}       & \textbf{5.918}       \\ \hline
% \multicolumn{1}{l|}{PC-SBL}        & \multicolumn{1}{l|}{89.051}                & \multicolumn{1}{l|}{87.394}                & \multicolumn{1}{l|}{67.615}               & \multicolumn{1}{l|}{42.992}               & 53.092               \\ \hline
% \multicolumn{1}{l|}{cSBL}          & \multicolumn{1}{l|}{98.800}                & \multicolumn{1}{l|}{28.215}                & \multicolumn{1}{l|}{14.589}               & \multicolumn{1}{l|}{10.935}               & 8.867                \\ \hline
% \end{tabular}
% \label{tab.time_pc}
% \end{table}


We begin with Condition~\ref{condition1}. From Lemma~\ref{thm.am_cost}, we deduce that $\{\bm\gamma_i\}^{(t)}|_{t=1}^\infty\subset \mathcal{S}$ where $\mathcal{S}$ is the sublevel set of $Q(\{\bm\gamma_i\}^{(0)})$. So to check Condition~\ref{condition1}, we prove that $\mathcal{S}$ is compact. To this end, we establish that $\mathcal{S}$ is compact using the following result.
%We first establish Proposition \ref{thm.coercive_open} and using which we then show that $Q(\{\bm \gamma_i\})$ satisfies Proposition \ref{thm.coercive_open}.
Invoking Proposition~\ref{thm.coercive_open}, it is enough to verify that $Q(\{\bm \gamma_i\})$ satisfies~\eqref{eq.coercive_open} on its domain $\mathcal{C}_+$ to ensure compactness of $\mathcal{S}$. %First, we check if the condition in~\eqref{eq.coercive_open} $Q\rightarrow +\infty$ when $\mathcal{C}_+ \ni \{\bm \gamma_i\} \rightarrow \mathrm{bd} (\mathcal{C}_+)\ $ and $ \|\{\bm \gamma_i\}\| \rightarrow +\infty$, where we define $\|\{\bm \gamma_i\}\| = (\sum_{i=1}^I\|\bm \gamma_i\|_2^2)^{1/2}$. 
For this, we notice that $Q(\{\bm \gamma_i\})$ in~\eqref{eq.qfunc} can be rewritten~as
\begin{align}\label{eq.coerciveT}
    Q(\{\bm \gamma_i\})&=\sum_{j=1}^{N^I}\log [\bm \gamma]_j + [\bm d^{(r)}]_j [\bm \gamma]_j^{-1}\\
    &= \log [\bm \gamma]_{j_*} + [\bm d^{(r)}]_{j_*} [\bm \gamma]_{j_*}^{-1}+\sum_{j\neq j_*}\log [\bm \gamma]_j + [\bm d^{(r)}]_j [\bm \gamma]_j^{-1},
    \end{align}
for any $j_*=1,2,\ldots,N^I$.  Since $\bm d^{(r)}>0$ and the minimum value of function $f(x)=\log x+a/x$ is attained at $x=a$ and $f(x)\geq f(a)=\log a+1$, for any $a> 0$, we get
    \begin{multline}
    Q(\{\bm \gamma_i\})\geq \log [\bm \gamma]_{j_*} + [\bm d^{(r)}]_{j_*} [\bm \gamma]_{j_*}^{-1}+\sum_{j\neq j_*}\left(\log [\bm d^{(r)}]_j + 1\right).
\end{multline} The assumption $\bm d^{(r)}>0$ also implies that the second term in the lower bound $\sum_{j\neq j_*}\left(\log [\bm d]_j + 1\right)$ is finite. Therefore, if  $[\bm \gamma]_{j_*}\to 0$ or $[\bm \gamma]_{j_*}\to +\infty$, the lower bound on $Q$ goes to $+\infty$, and so, $Q\rightarrow +\infty$. %it is enough to verify that there exists an index $j_*$ such that $\log [\bm \gamma]_{j_*} + [\bm d^{(r)}]_{j_*} [\bm \gamma]_{j_*}^{-1}\to+\infty$, which happens if  $[\bm \gamma]_{j_*}\to 0$ or $[\bm \gamma]_{j_*}\to +\infty$. 

Clearly, $Q\rightarrow +\infty$ when $ \|\{\bm \gamma_i\}\| \rightarrow +\infty$ because at least one entry $\bm\gamma_{j_*}$ of $\bm \gamma$ satisfies $[\bm \gamma]_{j_*}\to +\infty$. Similarly, when $\mathcal{C}_+ \ni \{\bm \gamma_i\} \rightarrow \mathrm{bd} (\mathcal{C}_+)\ $, there exists an index $i_*$ such that at least one entry of $\bm\gamma_{i_*}$ goes to zero. In that case, we can choose  $j_*$ such that  
\begin{equation}
    [\bm \gamma]_{j_*}=\min_l\;[\bm \gamma_{i_*}]_l\prod_{i\neq i_*}\|\bm \gamma_i\|_\infty.
\end{equation}
Then, $[\bm \gamma]_{j_*}\to 0$ if $\prod_{i\neq i_*}\|\bm \gamma_i\|_\infty$ is finite; otherwise $[\bm \gamma]_{j_*}\to \infty$. In both cases, $Q\rightarrow +\infty$. Hence, \eqref{eq.coercive_open} holds for $Q(\{\bm \gamma_i\})$, and due to Proposition \ref{thm.coercive_open}, $\mathcal{S}$ is compact. Thus, Condition~\ref{condition1} is established.

% We notice that $Q\rightarrow +\infty$ when $ \|\bm \gamma_I\| \rightarrow +\infty$. 
% where $\bm \gamma = \otimes_{i=1}^I \bm \gamma_i$. Also, 
% \begin{equation}
%         Q(\{\bm \gamma_i\})\geq \log [\bm \gamma]_j + [\bm d^{(r)}]_j [\bm \gamma]_j^{-1}+L,
%     \end{equation}
% We observe the following facts.


% \begin{enumerate}[label=F\arabic*,leftmargin=*]
%     \item We have $\log [\bm \gamma]_j + [\bm d^{(r)}]_j [\bm \gamma]_j^{-1}\geq\log[\bm d^{(r)}]_j+1$, and $\lim_{[\bm \gamma]_j \rightarrow 0\ \text{or}\ +\infty} \log [\bm \gamma]_j + [\bm d^{(r)}]_j [\bm \gamma]_j^{-1} = +\infty$.
%     \label{fact1}


%     \item As $\{\bm \gamma_i\}\in\mathcal{C}_+$ and the properties of $\ell_\infty$, $\ell_1$ and $\ell_2$ norm \cite{lang2012introduction}, we have for $i=1,2,\ldots,I-1$, there is $1>\|\bm \gamma_i\|_\infty\geq\frac{1}{\sqrt{N}}$ and $\|\bm \gamma_I\|_\infty\!\geq\!\frac{1}{\sqrt{N}}\!\|\bm \gamma_I\|_2$.
    
    
%     % As $\sqrt{N}\|\bm \gamma_i\|_\infty \geq \|\bm \gamma_i\|_2=1 \geq 1/\sqrt{N}\|\bm \gamma_i\|_1$ for $i=1,2,\ldots,I-1$ and the fact that , we conclude 
%     \label{fact2}

% \end{enumerate}
% % \vspace{-0.5cm}
% \begin{enumerate}[leftmargin=*]
%     \item The first condition of~\eqref{eq.coercive_open} is
%     $$Q\rightarrow +\infty\ \text{as}\ \|\{\bm \gamma_i\}\| \rightarrow +\infty,$$
%     where we define $\|\{\bm \gamma_i\}\| = (\sum_{i=1}^I\|\bm \gamma_i\|_2^2)^{1/2}=(\|\bm \gamma_I\|_2^2+I-1)^{1/2}$ which indicates $\|\bm \gamma_I\|_2 \rightarrow+\infty$. Besides, \eqref{eq.coerciveT} can be further rewritten as
%     \begin{equation}
%     \begin{aligned}
%         Q(\{\bm \gamma_i\})\geq \log [\bm \gamma]_j + [\bm d^{(r)}]_j [\bm \gamma]_j^{-1}+L,
%     \end{aligned}
%     \end{equation}
%     where $[\bm \gamma]_j = \prod_{i=1}^I\|\bm \gamma_i\|_\infty\geq (1/\sqrt{N})^I \|\bm \gamma_I\|_2 \rightarrow +\infty$ and $L$ represents the finite lower bound of the rest due to \ref{fact1}. Due to \ref{fact2}, we ensure $Q\rightarrow +\infty$. 

%     \item The second condition of~\eqref{eq.coercive_open} is 
%     $$Q\rightarrow+\infty\ \text{as}\ \mathcal{C}_+ \ni \{\bm \gamma_i\} \rightarrow \mathrm{bd} (\mathcal{C}_+),$$
%     meaning when at least one entry in $\{\bm \gamma_i\}$ approaches zero, the cost function value $Q\rightarrow+\infty$. There are two cases.

% \textbf{Case 1:} The entry goes to zero is in $\bm \gamma_I$ which is denoted as $\min(\bm \gamma_I)$. From~\eqref{eq.coerciveT} we have
% \begin{equation}
%     Q\geq \log [\bm \gamma]_j + [\bm d^{(r)}]_j [\bm \gamma]_j^{-1}+L\rightarrow+\infty.
% \end{equation}
% where $[\bm \gamma]_j = \prod_{i=1}^{I-1}\|\bm \gamma_i\|_\infty\min(\bm \gamma_I)< \min(\bm \gamma_I) \rightarrow 0$.

% \textbf{Case 2:} The entry is in $\bm \gamma_i$ for $i=1,2,\ldots,I-1$, e.g., $\min(\bm \gamma_1)$. From~\eqref{eq.coerciveT} we have
% \begin{equation}
%     Q\geq \log [\bm \gamma]_j + [\bm d^{(r)}]_j [\bm \gamma]_j^{-1}+L\rightarrow+\infty.
% \end{equation}
% where $[\bm \gamma]_j=\min(\bm \gamma_1)\prod_{i=2}^I\|\bm \gamma_i\|_\infty< C\min(\bm \gamma_1) \rightarrow 0$. Scalar $C < +\infty$ is the upper bound of $\|\bm \gamma_I\|_\infty$, otherwise we have $\|\bm \gamma_I\|_2 \rightarrow+\infty$.
% \end{enumerate}

% Thus, function $Q$ satisfies condition~\eqref{eq.coercive_open} and the sublevel set of $Q$ is compact as stated in Proposition \ref{thm.coercive_open}. Further, sequence $Q(\{\bm\gamma_i\}^{(t)})|_{t=1}^\infty$ being non-increasing ensures $\{\bm\gamma_i\}^{(t)}|_{t=1}^\infty\subset \mathcal{S}$ \cite[Theorem 1]{zhao2017unified}.

% \vspace{-0.5cm}
%\subsection{Proof of Conditions \ref{condition2} and \ref{condition3}}\label{appe.am_inequ}
We next examine Condition~\ref{condition2}. A stationary point  $\{\bm \gamma_i^\mathsf{s}\}\in \mathcal{C}_+$ of $Q$  satisfies 
\begin{equation}
\frac{\partial Q(\{\bm \gamma_i\}|\bm \gamma^{(r)})}{\partial \{\bm \gamma_i\}}\bigg|_{\{\bm \gamma_i^\mathsf{s}\}}
%=
% \begin{bmatrix}
% \frac{\partial Q}{\partial \bm \gamma_1}\\
% \vdots\\
% \frac{\partial Q}{\partial \bm \gamma_I}
% \end{bmatrix}_{\{\bm \gamma_i^\mathsf{s}\}}
=
\bm 0.
\end{equation}
If $\{\bm \gamma_i\}^{(t)} \in \mathcal{C}_+$ is not a stationary point of $Q(\{\bm \gamma_i\})$, there exists at least one index $i_*\in\{1,2,\ldots,I\}$ such that 
% \begin{equation}\label{eq.grad_neq}
%     \begin{bmatrix}
%     \bm \gamma_1^{(t)}\\\vdots\\\bm \gamma_I^{(t)}
%     \end{bmatrix}
%     \neq
%     N^{-I+1}
%     \begin{bmatrix}
%     [\bm I \otimes (\otimes_{i=2}^I \bm \gamma_i^{(t)})^{(-1)}]^\mathsf{T}\bm d^{(r)}\\\vdots\\
%     [(\otimes_{i=1}^{I-1} \bm \gamma_i^{(t)})^{(-1)} \otimes \bm I  ]^\mathsf{T}\bm d^{(r)}
%     \end{bmatrix}.
% \end{equation}
% Inequality~\eqref{eq.grad_neq} can be caused by 
\begin{align}\label{eq.gamma2_ine}
    \bm \gamma_{i_*}^{(t)}&\neq N^{-I+1}\left[(\otimes_{j=1}^{i_*-1}(\bm \gamma_j^{(t)})^{-1})\otimes \bm I_N \otimes (\otimes_{j=i_*+1}^I(\bm \gamma_j^{(t)})^{-1})\right]^\mathsf{T}\bm d^{(r)}\\
  \bm \gamma_i^{(t)} &= N^{-I+1}\big[(\otimes_{j=1}^{i-1}(\bm \gamma_j^{(t)})^{-1})\otimes \bm I_N \otimes (\otimes_{j=n+1}^I(\bm \gamma_j^{(t)})^{-1})\big]^\mathsf{T}\bm d^{(r)},\label{eq.stationary}
\end{align}
for $i=1,2,\ldots,i_*-1$. Therefore, from the AM update \eqref{eq.update}, 
\begin{equation}\label{eq.nochange_am}
    \tilde{\bm \gamma}_i = \bm \gamma_i^{(t)}, \text{ for}\; i=1,2,\ldots,i^{*}-1.
\end{equation}
Substituting this relation in \eqref{eq.update} with $i=i_*$, we obtain
\begin{equation}
    \tilde{\bm \gamma}_{i_*} = N^{-I+1}\big[(\otimes_{i=1}^{i_*-1}(\bm \gamma_i^{(t)})^{-1})\otimes \bm I_N \otimes (\otimes_{i=i_*+1}^I(\bm \gamma_i^{(t)})^{-1})\big]^\mathsf{T}\bm d^{(r)}.
\end{equation}   
%for any $m$ between $1$ and $I-1$ and the equality for $\bm \gamma_I^{(t)}$ always holds since
% \begin{equation*}
% \tilde{\bm \gamma}_I = [(\otimes_{i=1}^{I-1} \tilde{\bm \gamma}_i)^{-1} \otimes \bm I  ]^\mathsf{T}\bm d^{(r)}
% \Rightarrow \bm \gamma_I^{(t)} = [(\otimes_{i=1}^{I-1} \bm \gamma_i^{(t)})^{-1} \otimes \bm I  ]^\mathsf{T}\bm d^{(r)},
% \end{equation*}
% by~multiplying $\prod_{i=1}^{I-1}\|\tilde{\bm \gamma}_i\|_2$ on both sides. Further, for $n =1,2,\ldots,m-1$, there should be 
% \begin{equation}\label{eq.gamma1_equ}
% \bm \gamma_n^{(t)} = N^{-I+1}\big[(\otimes_{i=1}^{n-1}(\bm \gamma_i^{(t)})^{-1})\otimes \bm I \otimes (\otimes_{i=n+1}^I(\bm \gamma_i^{(t)})^{-1})\big]^\mathsf{T}\bm d^{(r)},
% \end{equation}
%since if for some $n < m$~\eqref{eq.gamma1_equ} does not hold, then this already guarantees $Q(\{\bm \gamma_i\}^{(t)})>Q(\{\bm \gamma_i\}^{(t+1)})$ with the same reason as we will discuss for~\eqref{eq.gamma2_ine} and it is unnecessary to discuss the inequality for any $m>n$, e.g., Eq.~\eqref{eq.gamma2_ine}. Now, we are going to detail why~\eqref{eq.gamma2_ine} ensures $Q(\{\bm \gamma_i\}^{(t)})>Q(\{\bm \gamma_i\}^{(t+1)})$.
%As in Algorithm \ref{al.AMKroSBL}, in order to obtain $\{\bm\gamma_i\}^{(t+1)} = M(\{\bm\gamma_i\}^{(t)})$, we need to first update $\{\bm \gamma_i\}$ sequentially as in~\eqref{eq.update}.
% \begin{equation}
%     \begin{aligned}\label{eq.update_rule}
%     \tilde{\bm \gamma}_i = N^{-(I-1)}\big[(\otimes_{l=1}^{i-1}(\tilde{\bm \gamma}_l)^{-1})\otimes \bm I \otimes (\otimes_{l=i+1}^I(\bm \gamma_l^{(t)})^{-1})\big]^\mathsf{T}\bm d^{(r)},
%     % \tilde{\bm \gamma}_1^{(t+1)} &= N^{-2}
%     % [\bm I \otimes (\bm \gamma_2^{(t)})^{-1} \otimes (\bm \gamma_3^{(t)})^{-1}]^\mathsf{T}\bm d^{(r)}\\
%     % \tilde{\bm \gamma}_2^{(t+1)} &= N^{-2} [(\tilde{\bm \gamma}_1^{(t+1)})^{-1} \otimes \bm I \otimes (\bm \gamma_3^{(t)})^{-1}]^\mathsf{T}\bm d^{(r)}\\
%     % \tilde{\bm \gamma}_3^{(t+1)} &= N^{-2} [(\tilde{\bm \gamma}_1^{(t+1)})^{-1} \otimes (\tilde{\bm \gamma}_2^{(t+1)})^{-1} \otimes \bm I  ]^\mathsf{T}\bm d^{(r)},
% \end{aligned}
% \end{equation}
% where $\tilde{\bm \gamma}_i$ is not necessarily to be unit norm.
% \begin{equation}
% \begin{aligned}
%     \bm \gamma_1^{(t+1)} &= \frac{\tilde{\bm \gamma}_1^{(t+1)}}{\|\tilde{\bm \gamma}_1^{(t+1)}\|_2},\\
%     \bm \gamma_2^{(t+1)} &= \frac{\tilde{\bm \gamma}_2^{(t+1)}}{\|\tilde{\bm \gamma}_2^{(t+1)}\|_2},\\
%     \bm \gamma_3^{(t+1)} &= \|\tilde{\bm \gamma}^{(t+1)}_1\|_2\|\tilde{\bm \gamma}^{(t+1)}_2\|_2\tilde{\bm \gamma}_3^{(t+1)}.
% \end{aligned}    
% \end{equation}
The AM update in \eqref{eq.update} also guarantees
\begin{equation}
    \tilde{\bm \gamma}_{i_*}= \arg\min_{\bm\gamma_{i_*}}Q\left(\{\tilde{\bm \gamma}_i\}_{i=1}^{i_*-1},\bm\gamma_{i_*},\{\bm \gamma_i^{(t)}\}_{i=i_*+1}^I\right)\neq  \bm \gamma_{i_*}^{(t)}.
\end{equation}
So using \eqref{eq.nochange_am} and the above relation, we conclude 
\begin{align}
    Q\left(\{\bm \gamma_i\}^{(t)}\right) &=Q\left(\{\tilde{\bm \gamma}_i\}_{i=1}^{i_*-1},\bm \gamma_{i_*}^{(t)},\{\bm \gamma_i^{(t)}\}_{i=i_*+1}^I\right) \\
    &> Q\left(\{\tilde{\bm \gamma}_i\}_{i=1}^{i_*-1},\tilde{\bm \gamma}_{i_*},\{\bm \gamma_i^{(t)}\}_{i=i_*+1}^I\right)  \geq Q(\{\bm \gamma_i\}^{(t+1)}),
\end{align}
using arguments similar to \eqref{eq.nonincrease}. Thus, Condition\ref{condition2} is verified.
% \begin{equation}
%  Q\left(\{\bm \gamma_i\}^{(t)}\right)\geq Q\left(\{\tilde{\bm \gamma}_i\}_{i=1}^{i_*},\{\bm \gamma_i^{(t)}\}_{i=i_*+1}^I\right)     \geq Q\left(\{\tilde{\bm \gamma}_i\})=Q(\{\bm \gamma_i\}^{(t+1)}\right). 
% \end{equation}
% According to the discussion in Appendix \ref{sec.non_increase_Q}, we have
% \begin{multline}\label{eq.cascaded_rela}
% Q(\{\bm \gamma_i\}^{(t)})
% \overset{\mathclap{(a)}}{\geq} Q(\tilde{\bm \gamma}_1,\{\bm \gamma_i\}^{(t)}_{i=2:I})
% \overset{\mathclap{(b)}}{\geq} Q(\{\tilde{\bm \gamma_i}\}_{i=1}^m,\{\bm \gamma_i\}^{(t)}_{i=m+1:I})\\
% \geq Q(\{\tilde{\bm \gamma_i}\})
% = Q(\{\bm \gamma_i\}^{(t+1)}).
% \end{multline}
% Here the $Q(\bm \gamma_1^{(t)},\bm \gamma_2^{(t)},\bm \gamma_3^{(t)})$ and $Q(\bm \gamma_1^{(t+1)},\bm \gamma_2^{(t+1)},\bm \gamma_3^{(t+1)})$ are the cost function after and before the projection, respectively. By doing so, all the inequalities in~\eqref{eq.cascaded_rela} are free of projection step, simplifying the analysis. 
% Eq.~\eqref{eq.gamma1_equ} and~\eqref{eq.gamma2_ine} ensure the strict inequality.
% Eq. \eqref{eq.gamma1_equ} infers that $\tilde{\bm \gamma}_n = \bm \gamma_n^{(t)}$ for $n =1,2,\ldots,m-1$ according to the updating rule~\eqref{eq.update} and thus all the inequalities between $(a)$ and $(b)$ become equality. With this, we proceed to 
% \begin{multline}
%         Q(\{\bm \gamma_i\}^{(t)})
%         =Q(\{\tilde{\bm \gamma_i}\}_{i=1}^{m-1},\{\bm \gamma_i\}^{(t)}_{i=m:I})\\
%         \overset{\mathclap{(b)}}{\geq} Q(\{\tilde{\bm \gamma_i}\}_{i=1}^m,\{\bm \gamma_i\}^{(t)}_{i=m+1:I}).
% \end{multline}
% \noindent Since
% \begin{align*}
%     \tilde{\bm \gamma}_m =& N^{-I+1}\big[(\otimes_{i=1}^{m-1}(\tilde{\bm \gamma}_i)^{-1})\otimes \bm I \otimes (\otimes_{i=m+1}^I(\bm \gamma_i^{(t)})^{-1})\big]^\mathsf{T}\bm d^{(r)}\\
%     =&N^{-I+1}\big[(\otimes_{i=1}^{m-1}(\bm \gamma_i^{(t)})^{-1})\otimes \bm I \otimes (\otimes_{i=m+1}^I(\bm \gamma_i^{(t)})^{-1})\big]^\mathsf{T}\bm d^{(r)}\\
%     \neq& \bm \gamma_m^{(t)}, \numberthis
% \end{align*}   
% thus, if we fix $\tilde{\bm \gamma}_i\ (\text{or }\bm \gamma_i^{(t)})$ for $i=1,2,\ldots,m-1$ and $\bm \gamma_i^{(t)}$ for $i=m+1,\ldots,I$, then $\tilde{\bm \gamma}_m$ attains the unique global minimum while $\bm \gamma_m^{(t)}$ not. Then $(b)$ becomes strict inequality. We arrive at $Q(\{\bm \gamma_i\}^{(t)}) > Q(\{\bm \gamma_i\}^{(t+1)})$. 
% In this case, if~\eqref{eq.gamma1_equ} does not hold, it is unnecessary to discuss~\eqref{eq.gamma2_ine} because~\eqref{eq.gamma1_equ} being violated already guarantees the strict inequality before $(b)$ with the same reason as we discuss~\eqref{eq.gamma2_ine}.

 Further, to check Condition~\ref{condition3}, we note that if $\{\bm \gamma_i\}^{(t)} \in \mathcal{C}_+$ is a stationary point, then it satisfies \eqref{eq.stationary} for $i=1,2,\ldots,I$. Then,  
% \begin{equation}
%     \begin{bmatrix}
%     \bm \gamma_1^\mathsf{s}\\\vdots\\\bm \gamma_I^\mathsf{s}
%     \end{bmatrix}
%     =
%     N^{-I+1}
%     \begin{bmatrix}
%     [\bm I \otimes (\otimes_{i=2}^I \bm \gamma_i^\mathsf{s})^{(-1)}]^\mathsf{T}\bm d^{(r)}\\\vdots\\
%     [(\otimes_{i=1}^{I-1} \bm \gamma_i^\mathsf{s})^{(-1)} \otimes \bm I  ]^\mathsf{T}\bm d^{(r)}
%     \end{bmatrix},
% \end{equation}
$M\left(\{\bm \gamma_i\}^{(t)}\right) = \{\bm \gamma_i\}^{(t)}$ and $Q\left(\{\bm \gamma_i\}^{(t)}\right) = Q\left(\{\bm \gamma_i\}^{(t+1)}\right)$. So, Condition~\ref{condition3} holds.

Finally, we note that $M(\cdot)$ and $Q$ involves the opertaions $\det(\cdot)$, $\log(\cdot)$, and $(\cdot)^{-1}$ that are continuous on $\mathcal{C}_+$. Hence,  $M(\cdot)$ and $Q(\{\bm \gamma_i\})$ are also continuous, satisfying Condition~\ref{condition4}. Thus, the proof is complete.


\section{Proof of Theorem \ref{thm.stationary}}
\label{appe.em}

Theorem \ref{thm.stationary} is proven using the convergence guarantees for the generalized EM (GEM)~\cite[Theorem 6]{wu1983convergence}. %We first define the generalized EM (GEM) algorithm as Theorem 6 mainly focuses on the GEM algorithm.
GEM is an iterative algorithm similar to EM where the numerically infeasible M-step is replaced with a point-to-set map such that for every iteration $r$,
% \begin{defi}\label{defi.GEM}
% \cite{dempster1977maximum} An EM algorithm is defined as a GEM algorithm if as the E-step and M-step proceed, the following inequality
\begin{equation}
    Q(\{\bm \gamma_i\}^{(r)}|\{\bm \gamma_i\}^{(r)}) \geq Q(\{\bm \gamma_i\}^{(r+1)}|\{\bm \gamma_i\}^{(r)}),
\end{equation}
always holds for $Q$ defined in~\eqref{eq.qfunc}.
%\end{defi}
%Definition \ref{defi.GEM} shows what differs the GEM algorithm from the EM algorithm is the global minimum is no longer required in the M-step for the GEM algorithm. 
Unlike EM, the GEM algorithm does not require achieving the global minimum for the M-step optimization problem. We recall from Lemma~\ref{thm.am_cost} 
%that the $r$th EM iteration satisfies
        % \begin{multline*}
        %     Q(\{\bm \gamma_i\}^{(r)}|\bm \gamma^{(r)})
        %     = Q(\{\bm \gamma_i\}^{(r,1)})\\
        %     \geq Q(\{\bm \gamma_i\}^{(r,\infty)})
        %     =Q(\{\bm \gamma_i\}^{(r+1)}|\bm \gamma^{(r)}),    
        % \end{multline*}
%where $(a)$ is the initialization of the AM algorithm,
    % by $\{\bm \gamma_i\}^{(t)}|_{t=0} = \{\bm \gamma_i\}^{(r-1)}$
%    where the last step gives the solution to the M-step.
    % is $\{\bm \gamma_i\}^{(r)} = \{\bm \gamma_i\}^{(t)}|_{t=\infty}$.
%Thus, $Q$ is non-increasing over the EM iterations;
AM-KroSBL is a GEM algorithm. Now,     
% The proof of Theorem \ref{thm.stationary} starts with rephrasing Theorem 6 \cite{wu1983convergence} in our case.
suppose the following conditions of \cite[Theorem 6]{wu1983convergence} holds,

\begin{enumerate}[leftmargin=*]
    \item the domain of $\mathcal{L}$ is a subset in $\bar{N}$-th dimensional Euclidean space $\mathbb{R}^{\bar{N}}$, \label{gem.c1}
    \item KroSBL cost function $\mathcal{L}$ is continuous in its domain and differentiable in the interior of the domain, \label{gem.c3}
    \item the sublevel set of $\mathcal{L}(\{\bm \gamma_i\}^{(1)})$ is compact for any initilization $\{\bm \gamma_i\}^{(1)}$ with $\mathcal{L}(\{\bm \gamma_i\}^{(1)}) < +\infty$, \label{gem.c2}
%    \item the AM-KroSBL relies on a GEM algorithm, \label{gem.c4}
    \item the sequence $\{\bm\gamma_i\}^{(r)}|_{r=1}^{\infty}$ generated by AM-KroSBL has the additional property $\nabla Q = \frac{\partial }{\partial \{\bm \gamma_i\}^{(r+1)}}Q(\{\bm \gamma_i\}^{(r+1)}|\bm \gamma^{(r)}) = 0$, \label{gem.c5}
    \item $\nabla Q$ is continuous in both $\{\bm\gamma_i\}^{(r+1)}$ and $\{\bm\gamma_i\}^{(r)}$. \label{gem.c6}
\end{enumerate}
Then, Theorem 6 in \cite{wu1983convergence} ensures that the sequence $\{\bm\gamma_i\}^{(r)}|_{r=1}^{\infty}$ generated by AM-KroSBL converges to the stationary points of $\mathcal{L}$. Here, Condition~\ref{gem.c1} is trivially satisfied because the domain of $\mathcal{L}$, i.e., $\mathcal{C}$, is a subset of the $\bar{N}$-dimensional Euclidean space. Similarly, $\mathcal{L}=\log |\bm \Sigma_{\bm y}| + \bm y^\mathsf{H} \bm \Sigma_{\bm y}^{-1} \bm y$ is a continuous function of $\{\bm \gamma_i\}$ because matrix inversion and determinant are continuous in its entries \cite[Theorems~5.19 and 5.20]{schott2016matrix}. Further, the derivative of $\mathcal{L}$ exists everywhere in $\mathcal{C}$, and thus, Condition~\ref{gem.c3} is also satisfied. So, in the remainder of the proof, we verify if Conditions~\ref{gem.c2}-\ref{gem.c6} to establish the convergence guarantee of AM-KroSBL.

% At the high level, the proof includes the following four steps:
% \begin{enumerate}[label=\Alph*,leftmargin=*]
%     \item Conditions \ref{gem.c1}-\ref{gem.c3} are basic requirements for $\mathcal{L}$. The domain of $\mathcal{L}$, i.e., $\mathcal{C}$, is a subset of the $\bar{N}$-th dimensional Euclidean space. The compactness of the sublevel set, the continuity, and the differentiablity are shown in Appendix \ref{appe.assvali}. 

%     \item Condition \ref{gem.c4} is as follows. According to Appendix \ref{appe.am_compact}, the $r$th EM iteration has
%         \begin{multline*}
%             Q(\{\bm \gamma_i\}^{(r)}|\bm \gamma^{(r)})
%             \overset{(a)}{=} Q(\{\bm \gamma_i\}^{(r,1)})\\
%             \geq Q(\{\bm \gamma_i\}^{(r,\infty)})
%             \overset{(b)}{=}Q(\{\bm \gamma_i\}^{(r+1)}|\bm \gamma^{(r)}),    
%         \end{multline*}
%     where $(a)$ is the initialization of the AM algorithm,
%     % by $\{\bm \gamma_i\}^{(t)}|_{t=0} = \{\bm \gamma_i\}^{(r-1)}$
%     and $(b)$ is the solution to the M-step.
%     % is $\{\bm \gamma_i\}^{(r)} = \{\bm \gamma_i\}^{(t)}|_{t=\infty}$.
%     Thus, $Q$ is non-increasing regarding EM iterations, and hence AM-KroSBL relies on a GEM algorithm.

%     \item Condition \ref{gem.c5} requires the point $\{\bm \gamma_i\}^{(r+1)}$ to be a stationary point of $Q(\{\bm \gamma_i\}^{(r+1)}|\bm \gamma^{(r)})$. This holds according to Proposition \ref{thm.am_stationary} if $\bm d^{(r)} > 0$. We will show $\bm d^{(r)} > 0$ in Appendix \ref{appe.positived}.

%     \item Fourth, the continuity of $\nabla Q$ is shown in Appendix \ref{appe.continuity}, corresponding to condition \ref{gem.c6}.
% \end{enumerate}

%\subsection{Condition validation}\label{appe.assvali}
% \noindent Here we present the high level guideline for the proof of Theorem \ref{thm.stationary} that contains four steps in total:
% \begin{enumerate}[label=\Alph*.]
%     \item First, validate the assumptions taken in \cite{wu1983convergence} in order to properly implement Proposition \ref{prop.thm1}.
%     \item Second, demonstrate that AM-KroSBL is a GEM algorithm.
%     \item Third, reveal the continuity of the gradient of $Q$ in both $\{\bm \gamma_1^{(r)},\bm \gamma_2^{(r)},\bm \gamma_3^{(r)}\}$ and $\{\bm \gamma_1^{(r-1)},\bm \gamma_2^{(r-1)},\bm \gamma_3^{(r-1)}\}$.
% \end{enumerate}
To verify Condition~Conditions~\ref{gem.c2}, we first show the compactness of the sublevel set of $\mathcal{L}$. Compactness is established via the coerciveness of the KroSBL cost function $\mathcal{L}$ since the sublevel sets of coercive functions are compact \cite{calafiore2014optimization}.
% \begin{defi}\label{def.2}
% Let $\mathcal{D}$ be a subset of $\mathbb{R}^n$. A function $f:\mathcal{D} \rightarrow \mathbb{R}$ is a coercive function if
% \begin{align}
%     \lim_{\|\bm x\|\to+\infty} f(\bm x) = +\infty.
% \end{align}
% \end{defi}
% \begin{proposition}\label{thm.coercive}
% Cost $\mathcal{L}$ is a coercive function of $\{\bm \gamma_i\}$ on~$\mathcal{C}$. The sublevel set of $\mathcal{L}(\{\bm \gamma_i\}^{(0)})$ is compact for any~$\mathcal{L}(\{\bm \gamma_i\}^{(0)}) < +\infty$.
% \end{proposition}
% \begin{proof}
By definition~\cite{fedorov2018structured}, function $\mathcal{L}$ is coercive if $ \lim_{\|\{\bm \gamma_i\}\|\to+\infty} \mathcal{L} = +\infty,$
% \begin{equation}
%     \lim_{\|\{\bm \gamma_i\}\|\to+\infty} \mathcal{L} = +\infty,
% \end{equation}
where $\|\{\bm \gamma_i\}\| = \left(\sum_{i=1}^I\|\bm \gamma_i\|_2^2\right)^{1/2}$. %In our case, $\|\bm \gamma_i\|_2=1$, for $i=1,2,\ldots,I-1$. Thus, $\mathcal{L}$ is coercive if $\lim_{\|\bm \gamma_I\|_2\to+\infty} \mathcal{L} = +\infty.$
% \begin{equation}
%     \lim_{\|\bm \gamma_I\|\to+\infty} \mathcal{L} = +\infty.
% \end{equation}
Further, since $\bm \Sigma_{\bm y} = \sigma^2 \bm I_{\bar{M}} + \bm H \bm \Gamma \bm H^\mathsf{H}$ is positive-definite (PD) when $\sigma^2 > 0$, we have
\begin{equation}
    \mathcal{L} = \log |\bm \Sigma_{\bm y}| + \bm y^\mathsf{H} \bm \Sigma_{\bm y}^{-1} \bm y
    >\log |\bm \Sigma_{\bm y}|
    =\sum_{j=1}^{\bar{M}} \log (\sigma^2 + \lambda_j),
\end{equation}
where $\lambda_j\!\geq\!0$ is the $j$th eigenvalue of $\bm H \bm \Gamma \bm H^\mathsf{H}$. So $\mathcal{L}$ is coercive~if
\begin{equation}\label{eq.sumloglimit}
    \lim_{\|\{\bm \gamma_i\}\|\to+\infty} \sum_{j=1}^{\bar{M}} \log (\sigma^2 + \lambda_j) = +\infty,
\end{equation}
which is true if at least one of the $\lambda_j$'s goes to infinity as $\|\{\bm \gamma_i\}\|$ goes to $+\infty$. Moreover, from the boundedness assumption on the norm of the dictionary columns, we derive
\begin{equation*}\label{eq.trace_lb_infty}
\sum_{j=1}^{\bar{M}} \lambda_j\!=\!\trace(\bm H \bm \Gamma \bm H^\mathsf{H})=\sum_{i=1}^{N^I}[\bm\gamma]_i\|[\bm H]_i\|_2^2\geq \epsilon^2\sum_{i=1}^{N^I}[\bm\gamma]_i\geq \epsilon^2\|\bm\gamma\|_{\infty}. %=  \epsilon^2\prod_{i=1}^I\|\bm\gamma_i\|_{\infty}\geq \frac{\epsilon^2}{N^{I/2}} \|\bm\gamma_I\|_{2},
\end{equation*}
%where the last step follows because $\|\bm\gamma_i\|_{\infty}\geq \frac{1}{\sqrt{N}}\|\bm\gamma_i\|_2 = \frac{1}{\sqrt{N}}$, for $i=1,2,\ldots,I-1$. 
So, $\lim_{\|\{\bm \gamma_i\}\|\to+\infty} \sum_{j=1}^{\bar{M}} \lambda_j=+\infty$, 
%where $(c)$ is due to the non-negativity of $\{\bm \gamma_i\}$ and $(d)$ is due to the assumption that each column of $\bm H$ has norm strictly greater than $\epsilon$, while $\bm H_\mathsf{m}$ is the column corresponding to $\max(\bm \gamma_i)$ for all $i$. Eq. \eqref{eq.trace_lb_infty} 
which means at least one eigenvalue goes to $+\infty$, proving \eqref{eq.sumloglimit}. 
Thus, KroSBL cost function $\mathcal{L}$ is a coercive function on $\mathcal{C}$, and its sublevel set of the starting point $\{\bm \gamma_i\}^{(1)}$ is compact. As a consequence, Condition~\ref{gem.c2} is true.
% \begin{equation}
%     \mathcal{S}_l=\left\{\{\bm \gamma_i\}\big|\mathcal{L}(\{\bm \gamma_i\})\leq \mathcal{L}(\{\bm \gamma_i\}^{(0)}) \right\},
% \end{equation}
% We would also like to show that why constraining $\mathcal{L}$ on $\mathcal{C}$ is necessary and beneficial to the analysis. Suppose $\mathcal{L}$ is defined not on $\mathcal{C}$ but on $\mathbb{R}^{3N}_+$, in order to push $\mathcal{L}$ to $+\infty$, we need to again have at least one eigenvalue to be $\infty$ when $(\sum_{j=1}^3\|\bm \gamma_j\|_2^2)^{1/2}\to+\infty$. It can be seen that $\mathcal{L}$ does not go to $+\infty$ on the path $\bm \gamma_1 = c \bm 1$, $\bm \gamma_2 = c^{-1/2} \bm 1$, and $\bm \gamma_3 = c^{-1/2}\bm 1$ when $\|\{\bm \gamma_1,\bm \gamma_2,\bm \gamma_3\}\|=(N(c^2+2c^{-1}))^{1/2}\to+\infty$ is caused by $c\rightarrow+\infty$, where $c$ is a positive scalar.
% \end{proof}

% \noindent 
% \vspace{-0.5cm}
% \subsection{Proof of GEM}\label{appe.em_gem}

% % The GEM algorithm is introduced when it is numerically infeasible to maximize $Q$ in the M-step \cite{dempster1977maximum}. Instead of solving M-step exactly, GEM allows operation that sorely improves $Q$. Therefore, we need to show that the $Q$ function is improved after each EM iteration (though not necessarily maximized). 


% According to Appendix \ref{appe.am_compact}, the $r$-th EM iteration has
% \begin{multline}
%     Q(\{\bm \gamma_i\}^{(r-1)}|\bm \gamma^{(r-1)})
%     \overset{(a)}{=} Q(\{\bm \gamma_i\}^{(t)})|_{t=0}\\
%     \geq Q(\{\bm \gamma_i\}^{(t)})|_{t=\infty}
%     \overset{(b)}{=}Q(\{\bm \gamma_i\}^{(r)}|\bm \gamma^{(r-1)}),    
% \end{multline}
% where $(a)$ the AM step is initialized as $\{\bm \gamma_i\}^{(t)}|_{t=0} = \{\bm \gamma_i\}^{(r-1)}$, and $(b)$ the solution to the M-step is $\{\bm \gamma_i\}^{(r)} = \{\bm \gamma_i\}^{(t)}|_{t=\infty}$. Thus, $Q$ is non-increasing regarding EM iterations, and hence AM-KroSBL relies on a GEM algorithm.
% and the resulting sequence $\{\bm \gamma_1^{(r)},\bm \gamma_2^{(r)},\bm \gamma_3^{(r)}\}$ is a GEM sequence.
% Further, the monotonicity, i.e., $\mathcal{L}(\{\bm \gamma_i\}^{(r)})$ $\geq$ $\mathcal{L}(\{\bm \gamma_i\}^{(r-1)})$ is guaranteed by the sequence $\{\bm \gamma_i\}^{(r)}$ generated by GEM algorithm \cite{wu1983convergence}, and we can obtain the byproduct that $\{\bm \gamma_i\}^{(r)}|_{r=0}^\infty \subset \mathcal{C}_l$ and admits at least one limit point since $\mathcal{S}_l$ is compact and $\mathcal{L}$ is continuous.

 
% % Figure environment removed

% % Figure environment removed
%\subsection{Proof of the Positiveness of \texorpdfstring{$\bm d$}{TEXT}}\label{appe.positived}

% \begin{proposition}\label{prop.hadamard}
% (\cite[Theorem 4.2. Hadamard's Inequality]{million2007hadamard}) Suppose $\bm A$ is positive semidefinite (PSD) of size $n$. Then $|\bm A| \leq [\bm A]_{11}\ldots[\bm A]_{nn}$.
% \end{proposition}

We next verify Condition~\ref{gem.c5}, which requires the point $\{\bm \gamma_i\}^{(r+1)}$ to be a stationary point of $Q(\{\bm \gamma_i\}^{(r+1)}|\bm \gamma^{(r)})$. According to Proposition \ref{thm.am_stationary}, it holds if $\bm d^{(r)} > 0$. So, we next show that $\bm d^{(r)} = \diag(\bm \Sigma_{\bm x}+\bm \mu_{\bm x}\bm \mu_{\bm x}^\mathsf{H}) > 0$. Since $\diag(\bm \mu_{\bm x}\bm \mu_{\bm x}^\mathsf{H})\geq  0$, it suffices to show that $\diag(\bm \Sigma_{\bm x})>0$. Also, since PD matrices have positive diagonal entries, from \eqref{eq.post_meva}, it is enough to verify that $\bm \Sigma_{\bm x}^{-1} = \sigma^{-2}\bm H^\mathsf{H}\bm H+\diag(\bm \gamma^{(r)})^{-1}$ is PD. So, $\bm d^{(r)} > 0$ if $\{\bm \gamma_i\}^{(r)} >0$ because $\sigma^{-2}\bm H^\mathsf{H}\bm H$ is PSD \cite{million2007hadamard}. From this observation, we prove the condition $\bm d^{(r)} > 0$ using induction. Since $\{\bm \gamma_i\}^{(1)} \in\mathcal{C}_+$, we have $\bm d^{(1)}>0$. Next, we assume that $\bm d^{(r)}>0$, for some $r>0$. From Proposition~\ref{thm.am_stationary}, $\{\bm \gamma_i\}^{(r+1)}$ generated by the AM algorithm is bounded away from $\mathrm{bd}(\mathcal{C}_+)$ when $\bm d^{(r)} > 0$. As a result, we conclude that $\{\bm \gamma_i\}^{(r+1)}>0$, which in turn implies that $\bm d^{(r+1)}>0$. Hence, Condition~\ref{gem.c5} holds in our case.

%We defined $\bm d^{(r)}$ in~\eqref{eq.compute_d} and would like to show $\diag(\bm \Sigma_{\bm x}+\bm \mu_{\bm x}\bm \mu_{\bm x}^\mathsf{H}) > 0$.
% First, we know that $(\cdot)^2$ is non-negative, and $\bm \Sigma_{\mathrm{x}}$ is a covariance matrix measuring the confidence of the estimates \cite{lei2022rethink}, thus having non-negative diagonal entries as well. The next is to show it is strictly positive.
%First, it can be seen that $\diag(\bm \mu_{\bm x}\bm \mu_{\bm x}^\mathsf{H})$ is non-negative. Suppose $\bm \Gamma^{(r)}$ is PD, i.e., $\{\bm \gamma_i\}^{(r)} >0$, then $(\bm \Gamma^{(r)})^{-1}$ is also PD. Given $\sigma^{-2}\bm H^\mathsf{H}\bm H$ is PSD, the matrix $\sigma^{-2}\bm H^\mathsf{H}\bm H+(\bm \Gamma^{(r)})^{-1}$ is guaranteed to be PD, hence its inverse matrix $\bm \Sigma_{\bm x}$. 
%Using \cite[Theorem 4.2. Hadamard's Inequality]{million2007hadamard}, we have $0 < |\bm\Sigma_{\bm x}| \leq \prod_{j=1}^{N^I} [\bm\Sigma_{\bm x}]_{jj}$. Covariance $\bm\Sigma_{\bm x}$ has non-negative diagonal entries $[\bm\Sigma_{\mathrm{x}}]_{jj}$ for $j=1,\ldots,\bar{N}$. The product of non-negative values being positive indicates all are positive. Thus, we have $\bm d^{(r)} > 0$. We have shown that starting with $\{\bm \gamma_i\}^{(r)} >0$ leads to $\bm d^{(r)} > 0$. 
%Also, since $\{\bm \gamma_i\}^{(r+1)}$ generated by the AM algorithm in Algorithm \ref{al.AMKroSBL} is bounded away from $\mathrm{bd}(\mathcal{C}_+)$ due to the compactness of the sublevel set $\mathcal{S}$ when $\bm d^{(r)} > 0$, we conclude $\{\bm \gamma_i\}^{(r+1)}>0$. We also assume that the AM-KroSBL algorithm is initialized with $\{\bm\gamma_i\}^{(1)} > 0$. As a result, as the AM-KroSBL proceeds, we can guarantee $\bm d^{(r)} > 0$ in each EM iteration.

% This seems to be contradict to the conclusion given in \cite{wipf2004sparse} where all the local minima should be sparse. But the thing is, with iterations proceeding, most of the entries in $\bm d$ are driven to zero but not exactly zero. For the AM step, the $\bm d$ is fixed and as we have shown in 


%\subsection{Continuity of the gradient of \texorpdfstring{$Q$}{TEXT}}\label{appe.continuity}
%Finally, we check Condition~\ref{gem.c6}.  Showing the gradient $\nabla Q = \frac{\partial Q(\{\bm \gamma_i\}|\bm \gamma^{(r-1)})}{\partial \{\bm \gamma_i\}}$ is a continuous function in both $\{\bm \gamma_i\}$ and $\{\bm \gamma_i\}^{(r-1)}$ is done by first revealing the dependency of $\nabla Q$ on $\{\bm \gamma_i\}$. Without loss of generality, the gradient towards $\bm \gamma_1$ is
Finally, we show Condition~\ref{gem.c6} by first computing the gradient of $Q$ with respect to $\bm \gamma_i$ as
\begin{multline}
    \frac{\partial }{\partial \bm \gamma_i}Q(\{\bm \gamma_i\}|\bm \gamma^{(r)})=-N^{I-1}\bm\gamma_i^{-1}+\diag(\bm\gamma_i)^{-2} \\\times\left[\left(\otimes_{l=1}^{i-1}(\bm \gamma_l)^{-1}\right)\otimes \bm I_N \otimes \left(\otimes_{l=i+1}^I(\bm \gamma_l^{(r,t)})^{-1}\right)\right]^\mathsf{T}\bm d^{(r)}.
\end{multline}
Here, operators $(\cdot)^{-1}$ and $(\cdot)^{-2}$ are continuous in $(0,+\infty)$. Thus, the gradient is continuous in $\{\bm \gamma_i\}$. Finally, in $\nabla Q$, only $\bm d^{(r)}$ depends on $\{\bm \gamma_i\}^{(r)}$ as in~\eqref{eq.post_meva}. Since the matrix inversion is continuous in its entries \cite[Theorem 5.20]{schott2016matrix}, $\bm \Sigma_{\bm x}$ and $\bm \mu_{\bm x}$ are continuous in $\{\bm \gamma_i\}^{(r)}$. Therefore, $\nabla Q$ is continuous in $\{\bm \gamma_i\}^{(r)}$. Thus, Condition~\ref{gem.c6} is established, and the proof is complete.
% With Proposition~\ref{prop.thm1}, this closes the discussion of the convergence property of AM-KroSBL and the proof of Theorem \ref{thm.stationary}.

\section{Proof of Theorem~\ref{thm.local_minima_sparse}}\label{appe.sparse_local}

To prove the sparsity of local minima, we start with a few supporting lemmas.
%we first construct a problem of minimizing a concave function constrained on a convex polytope, and show . Then, we use the equivalence between extreme points and basic feasible solutions.%, adhering to the outline in~\cite{wipf2004sparse}. 
%Although the analyses are conducted without being constrained in $\mathcal{C}_+$, they can be translated by properly scaling $\{\bm \gamma_i\}$. Therefore, the conclusions still hold under constraints. Several lemmas are established first.

\begin{lemma}\label{lmm.ml_separable}
$\log|\bm \Sigma_{\bm y}|$ is concave with respect to $\{\bm \gamma_i\}$ in the noiseless case, i.e., $\sigma^2=0$.
\end{lemma}

\begin{proof}
    When $\sigma^2 = 0$, $\bm \Sigma_{\bm y} = \bm H \bm \Gamma \bm H^\mathsf{H} = \otimes_{i=1}^I \bm H_i \bm \Gamma_i \bm H_i^\mathsf{H}$. We have
    \begin{equation}
        \log|\bm \Sigma_{\bm y}| = \log|\otimes_{i=1}^I \bm H_i \bm \Gamma_i \bm H_i^\mathsf{H}|=\sum_{i=1}^I\left({\prod_{j\neq i}M_j}\right)\log|\bm H_i \bm \Gamma_i \bm H_i^\mathsf{H}|.
    \end{equation}
Since $\bm H_i \bm \Gamma_i \bm H_i^\mathsf{H}$ is a PSD matrix and affine in $\bm \gamma_i$, and function $\log|\cdot|$ is a concave function in the space of PSD matrices, $\log|\bm H_i \bm \Gamma_i \bm H_i^\mathsf{H}|$ is concave in $\bm \gamma_i$. Thus, $\log|\bm \Sigma_{\bm y}|$ is concave because the sum of concave functions is also concave.
\end{proof}

\begin{lemma}\label{lmm.constant}
If $\{\bm \gamma_i\}$ satisfies $\bm b = \bm A (\otimes_{i=1}^I\bm \gamma_i)$, where 
    \begin{equation}\label{eq:ab_defn}
        \bm b = \bm y - \sigma^2\bm u;\ \ \ \bm A = \bm H\diag(\bm H^\mathsf{H}\bm u),
    \end{equation}
    and $\bm u$ is any fixed vector such that $\bm y^\mathsf{H}\bm u = C$, then $\bm y^\mathsf{H}\bm \Sigma_{\bm y}^{-1}\bm y$ is a constant $C$ for any value of $\sigma^2\geq 0$.
\end{lemma}

\begin{proof}
%Since $\bm \Sigma_{\bm y} = \sigma^2 \bm I_{\bar{M}} + \bm H \bm \Gamma \bm H^\mathsf{H}$, setting $\bm u = (\bm H \bm \Gamma \bm H^\mathsf{H} + \sigma^2\bm I_{\bar{M}})^{-1}\bm y$ establishes the result for the noisy case, i.e, $\sigma^2>0$. To analyze the noiseless case, 

We combine the relation $\bm b = \bm A (\otimes_{i=1}^I\bm \gamma_i)$ and \eqref{eq:ab_defn} to obtain
\begin{align}
\bm y &= \bm A (\otimes_{i=1}^I\bm \gamma_i) + \sigma^2\bm u =\bm H\diag(\bm H^\mathsf{H}\bm u) (\otimes_{i=1}^I\bm \gamma_i) + \sigma^2\bm u \\
&= \bm H \bm \Gamma \bm H^\mathsf{H} \bm u + \sigma^2\bm u = \bm \Sigma_{\bm y}\bm u,
\end{align}    
where $\bm \Gamma=\diag(\otimes_{i=1}^I\bm \gamma_i)$. Then, $\bm y^\mathsf{H} \bm \Sigma_{\bm y}^{-1} \bm y= \bm y^\mathsf{H}\bm u=C$, for any value $\sigma^2\geq 0$.


%Let the eigenvalue decomposition of $\bm H \bm \Gamma \bm H^\mathsf{H}$ be $\bm V\bm S\bm V^\mathsf{H}$ where $\bm S$ is not necessarily full rank. Then, in the noiseless case, we derive
% \begin{align*}
% \lim_{\sigma^2\rightarrow 0}\bm y^\mathsf{H} \bm \Sigma_{\bm y}^{-1} \bm y \notag\\
%         &\hspace{-2cm}=        \lim_{\sigma^2\rightarrow 0}\bm u^\mathsf{H} \bm \Sigma_{\bm y} \bm u = \bm y^\mathsf{H}\bm \\
%         &\hspace{-2cm}=\lim_{\sigma^2\rightarrow 0} \bm u^\mathsf{H}( \sigma^2\bm I_{\bar{M}}\!+\!\bm V\bm S\bm V^\mathsf{H})(\sigma^2 \bm I_{\bar{M}}\!+\!\bm V\bm S\bm V^\mathsf{H})^{-1} (\bm V\bm S\bm V^\mathsf{H}\!+\!\sigma^2\bm I_{\bar{M}}\textbf{})\bm u\\
%         &\hspace{-2cm}=\lim_{\sigma^2\rightarrow 0}\bm u^\mathsf{H}\bm V\bm S\bm V^\mathsf{H}(\sigma^2 \bm I_{\bar{M}} + \bm V\bm S\bm V^\mathsf{H})^{-1}\bm V\bm S\bm V^\mathsf{H} \bm u\\
%         &\hspace{-2cm}=\bm u^\mathsf{H}\bm V\bm S\bm V^\mathsf{H} \bm u=C,
%     \end{align*}
% where the last step is obtained by defining $\bm V\bm S\bm V^\mathsf{H}\bm u = \bm y$ when $\sigma^2 = 0$. This completes the proof.
\end{proof}

% Lemma \ref{lmm.constant} serves as an essential cornerstone for the proof of Theorem~\ref{thm.local_minima_sparse} as it establishes the relationship between the ML problem~\ref{eq.ml_problem} and a constrained concave optimization problem. Further, Lemma \ref{lmm.constant} holds for any $\bm u$. Later, we will choose a Kronecker-structured $\bm u=\otimes_{i=1}^I$


% \begin{lemma}\label{lmm.kron_unique}
%     Let $\bm x$ be the Kronecker product of two vectors, i.e., $\bm x = \bm x_1 \otimes \bm x_2$. The Kronecker decomposition of $\bm x$, i.e., decomposing $\bm x$ into two vectors so that the Kronecker product outputs $\bm x$, ends up with $\tilde{\bm x}_1$ and $\tilde{\bm x}_2$ where $\tilde{\bm x}_1 = \alpha\bm x_1$ and $\tilde{\bm x}_2 = \alpha^{-1}\bm x_2$ for $\alpha \neq 0$, i.e., is unique up to a non-zero scale.
% \end{lemma}

% \begin{proof}
%     Since $\bm x = \bm x_1 \otimes \bm x_2$, we can arrange $\bm x$ as a matrix $\bm X$ such that $\bm X=\bm x_2 \bm x_1^\mathsf{T}$ and rank-one. Then any non-zero column and appropriately scaled non-zero row can be a pair of solution.
    
%     % The leading left and right singular vectors are unique to a sign. Scaling this pair of singular vectors with any pair of coefficients produces a set of solution as long as the product of coefficients equals to the singular value, which completes the proof.
% \end{proof}

\begin{lemma}\label{lmm.kron_equations_separable}
    Consider the set of linear equations, with $\bm t_1,\bm t_2\neq \bm 0$,
    \begin{equation}\label{eq.linear_kron}
        \left(\bm \Phi_1 \otimes \bm \Phi_2 \right)\left(\bm w_1\otimes \bm w_2 \right) = \bm t_1 \otimes \bm t_2.
    \end{equation}
    Seeking $\bm w_1$ and $\bm w_2$ that satisfy~\eqref{eq.linear_kron} is equivalent to solving 
    \begin{equation}\label{eq.linear_kron_soln}
        \bm \Phi_1 \bm w_1 = \alpha\bm t_1;\ \bm \Phi_2 \bm w_2 = \alpha^{-1}\bm t_2,
    \end{equation}
    where $\alpha$ is any non-zero scalar.
\end{lemma}

\begin{proof}
    We rewrite~\eqref{eq.linear_kron} as $(\bm \Phi_1\bm w_1)\otimes(\bm \Phi_2\bm w_2)=\bm t_1\otimes\bm t_2$. Now, we can arrange this equation as $\left(\bm \Phi_2\bm w_2 \right)\left(\bm \Phi_1\bm w_1 \right)^{\mathsf{H}}=\bm t_2 \bm t_1^\mathsf{H}$, which is a rank-one matrix. Here, $\bm t_2 \bm t_1^\mathsf{H}$ has at least one nonzero column since $\bm t_1,\bm t_2\neq \bm 0$, and every column of $\bm t_2 \bm t_1^\mathsf{H}$ is a scaled version $\bm t_2$. Therefore, the solution to the system of equations is given by \eqref{eq.linear_kron_soln},
leading to the desired conclusion.
\end{proof}

With all the mentioned lemmas, we present the proof of Theorem~\ref{thm.local_minima_sparse}. First, we pose another optimization problem:
\begin{equation}\label{problem}
    \underset{\substack{\{\bm \gamma_i\}\geq 0}}{\min}\log|\bm \Sigma_{\bm y}| \;\text{s.t.} \;
    \bm A \left(\otimes_{i=1}^I\bm \gamma_i\right) = \bm b,
\end{equation}
where $\bm A = \bm H\diag(\bm H^\mathsf{H}\bm u)$ and $\bm b = \bm y - \sigma^2\bm u$. As Lemma \ref{lmm.constant}, the constraint $\bm A (\otimes_{i=1}^I\bm \gamma_i) = \bm b$ holds the second term of $\mathcal{L}$ constant, and we minimize the first term of $\mathcal{L}$, which is concave, over a bounded convex polytope.
% Every local minimum of~\eqref{problem} satisfies the following two conditions. First, it minimizes $\log|\bm \Sigma_{\bm y}|$. Second, it solves $\bm A \left(\otimes_{i=1}^I\bm \gamma_i\right) = \bm y$ for some given vector $\bm u$. The second fact can further lead to $\bm y^\mathsf{H}\bm \Sigma_{\bm y}^{-1}\bm y = \bm y^\mathsf{H}\bm u$ according to Lemma~\ref{lmm.constant}. Therefore, if we want the local minima of~\eqref{eq.ml_problem} and~\eqref{problem} to coincide, we only need to show that for 
Then, any local minimum of~\eqref{eq.ml_problem} denoted by $\{\bm \gamma_i^*\}$, must also be a local minimum of~\eqref{problem} with
\begin{equation}
    \bm y^\mathsf{H}\left(\sigma^2 \bm I_{\bar{M}} + \bm H \diag(\otimes_{i=1}^I \bm \gamma_i^*) \bm H^\mathsf{H}\right)^{-1}\bm y = C^* = \bm y^\mathsf{H}\bm u^*,
\end{equation}
as long as there exists a vector $\bm u^*$ satisfying $C^*\!=\!\bm y^\mathsf{H}\bm u^*$ to construct $\bm A$ and $\bm b$. A candidate for $\bm u^*$ in the noiseless setting~is
%In general, such $\bm u^*$ always exists. It is not necessary to be $\bm u^* = (\bm H \bm \Gamma^* \bm H^\mathsf{H} + \sigma^2\bm I)^{-1}\bm y$ but another candidate can be $\bm u^*=C^*/\|\bm y\|_2^2\bm y$ which also always exists and satisfies $C^* = \bm y^\mathsf{H}\bm u^*$. Without noise, we arrive at
\begin{equation}
    \bm u^* = C^*\frac{ \bm y}{\|\bm y\|_2^2} =C^*\frac{ \otimes_{i=1}^I\bm H_i \bm x_i}{\|\bm y\|_2^2},
\end{equation}
where we also use \eqref{eq.problem_basic}, \eqref{eq.separable_dict} and the assumption that $\bm x =\otimes_i^I\bm x_i$. Thus, we obtain 
\begin{equation*}
 \bm A =  \frac{C^*}{\|\bm y\|_2^2}  \bm H\diag(\bm H^\mathsf{H}\otimes_{i=1}^I\bm H_i \bm x_i)=\frac{C^*}{\|\bm y\|_2^2}  \otimes_{i=1}^I\bm H_i\diag(\bm H_i^\mathsf{H}\bm H_i \bm x_i).
\end{equation*}
Similarly, we have
\begin{equation}
    \bm b = \bm y = \otimes_i^I\bm H_i\bm x_i.
\end{equation}
Thus, using Lemma \ref{lmm.kron_equations_separable} with the scaling factor $\alpha=1$,  the constraint $\bm A (\otimes_{i=1}^I\bm \gamma_i)=\bm b$ can be written as 
% \begin{equation}\label{eq:KronConstr}
%     \left(\otimes_{i=1}^I \bm A_i \right) \left(\otimes_{i=1}^I\bm \gamma_i\right) = \otimes_{i=1}^I \bm y_i.
% \end{equation}
% Referring to Lemma \ref{lmm.kron_equations_separable}, the constraint \eqref{eq:KronConstr} can be rewritten as 
\begin{equation}
    \bm A_i \bm \gamma_i = \bm b_i,\forall i=1,2,\ldots,I,
\end{equation}
where $\bm A_i=\left(C^*/\|\bm y\|_2^2\right)^{1/I}  \bm H_i\diag(\bm H_i^\mathsf{H}\bm H_i \bm x_i)$, and $\bm b =\bm H_i\bm x_i$. %, and $\prod_{i=1}^I\alpha_i = 1$. We can choose $\alpha_i = 1$ as different $\alpha_i$ values only scale $\bm b_i$ or the solution $\bm \gamma_i$ differently but not the support. According to Lemma \ref{lmm.kron_equations_separable}, 
Now, the problem \eqref{problem} can be transformed into $I$ separated problems as
\begin{equation}\label{problem_2}
    \underset{\substack{\bm\gamma_i}}{\min}\ \log|\bm H_i \bm \Gamma_i \bm H_i^\mathsf{H}| \hspace{0.3cm} \text{s.t.} \
    \bm A_i \bm \gamma_i = \bm y_i, \bm \gamma_i \geq 0,
\end{equation}
for $i=1,2,\ldots,I$. Any local minimum of $\mathcal{L}$, e.g., $\{\bm \gamma_i\}^*$ is also a local minimum of \eqref{problem_2}. Furthermore, all local minima of \eqref{problem_2} are achieved at extreme points, which are also basic feasible solutions with at most ${M_i}$ non-zeros for each $\bm \gamma_i$ \cite{luenberger1984linear}. Thus, $\{\bm \gamma_i^*\}$ is sparse when noise is absent.


\section{Proof of Theorem~\ref{thm.no_local}}\label{appe.upper_bound}

The proof is based on the following lemma:
\begin{lemma}\label{prop.kron_urp}
    If $\bm H = \otimes_{i=1}^I \bm H_i$ satisfies URP, then $\bm H_i$ for all $i$ also satisfy URP.
\end{lemma}

\begin{proof}
    Suppose we choose any {$M_i$} columns of $\bm H_i$ indexed by $\mathcal{M}_i$, for $i=1,2,\ldots,I$. Then, the Kronecker product of the corresponding submatrices, given by $\otimes_{i=1}^I [\bm H_i]_{\mathcal{M}_i}\in\mathbb{C}^{\bar{M}\times\bar{M}}$, is a submatrix of $\bm H=\otimes_{i=1}^I \bm H_i$.  Since $\bm H$ satisfies URP, any subset of $\bar{M}$ columns of $\bm H$ are linearly independent, we get  
    \begin{equation}\label{eq.URP_rank}
        \bar{M}=\rank\big([\bm H]_{\otimes_{i=1}^I \mathcal{M}_i}\big) = \prod_{i=1}^I \rank\left([\bm H_i]_{\mathcal{M}_i}\right)\leq {\prod_{i=1}^I M_i}={\bar{M}}.
    \end{equation} The last step follows because $\rank\big([\bm H_i]_{\mathcal{M}_i}\big) \leq {M_i}$. So, \eqref{eq.URP_rank} holds if and only if $\rank\big([\bm H_i]_{\mathcal{M}_i}\big) = {M_i}$ for $i=1,2,\ldots,I$. Hence, any subset of ${M_i}$ columns of $\bm H_i$ is linearly independent, and $\bm H_i$ satisfies URP, for $i=1,2,\ldots,I$.
\end{proof}

Lemma \ref{prop.kron_urp} ensures that $\bm H_i$ satisfies the URP for all $i$. Then, we next show that for any index set $\mathcal{M}_i$ such that $|\mathcal{M}_i|={M_i}$, if we restrict the nonzero values of $\bm \gamma_i$ to the set $\mathcal{M}_i$, there can only be one minimum for $\mathcal{L}(\{[\bm \gamma_i]_{\mathcal{M}_i})$. For this, we note that \begin{align}
    \mathcal{L}(\{[\bm \gamma_i]_{\mathcal{M}_i}) & = \log \left|\otimes_{i=1}^I [\bm H_i]_{\mathcal{M}_i} [\bm \Gamma_i]_{\mathcal{M}_i} [\bm H_i]_{\mathcal{M}_i}^{\mathsf{H}} \right| \notag\\
    &\hspace{0.5cm}+ \bm y^\mathsf{H} \left( \otimes_{i=1}^I [\bm H_i]_{\mathcal{M}_i} [\bm \Gamma_i]_{\mathcal{M}_i} [\bm H_i]_{\mathcal{M}_i}^{\mathsf{H}}\right)^{-1} \bm y\\
   & = \sum_{i=1}^I \left({\prod_{j\neq i}M_j}\right) \log \left|[\bm H_i]_{\mathcal{M}_i} [\bm \Gamma_i]_{\mathcal{M}_i} [\bm H_i]_{\mathcal{M}_i}^{\mathsf{H}} \right|\notag\\
    &\hspace{0.5cm}+ \bm y^\mathsf{H} \otimes_{i=1}^I\left( \left([\bm H_i]_{\mathcal{M}_i}^{\mathsf{H}}\right)^{-1}  [\bm \Gamma_i]_{\mathcal{M}_i}^{-1}[\bm H_i]_{\mathcal{M}_i}^{-1} \right) \bm y.
    \label{eq.Costfnt_simpl}
\end{align}
Here, the second term can be simplified using the assumption, $\bm y = \left(\otimes_{i=1}^I[\bm H_i]_{\mathcal{M}_i}\right)\left(\otimes_{i=1}^I[\bm x_i]_{\mathcal{M}_i} \right) = \otimes_{i=1}^I([\bm H_i]_{\mathcal{M}_i}[\bm x_i]_{\mathcal{M}_i})$, as follows: 
\begin{multline}
  \bm y^\mathsf{H} \otimes_{i=1}^I\left(  [\bm H_i]_{\mathcal{M}_i} [\bm \Gamma_i]_{\mathcal{M}_i} [\bm H_i]_{\mathcal{M}_i}^{\mathsf{H}}\right)^{-1} \bm y\\= \otimes_{i=1}^I[\bm x_i]_{\mathcal{M}_i}^\mathsf{H}  [\bm \Gamma_i]_{\mathcal{M}_i}^{-1} [\bm x_i]_{\mathcal{M}_i} = \prod_{i=1}^I[\bm x_i]_{\mathcal{M}_i}^\mathsf{H}  [\bm \Gamma_i]_{\mathcal{M}_i}^{-1} [\bm x_i]_{\mathcal{M}_i}.
    \end{multline}
Therefore, from \eqref{eq.Costfnt_simpl}, we arrive at
\begin{multline}
  \mathcal{L}(\{[\bm \gamma_i]_{\mathcal{M}_i})= \sum_{i=1}^I {\prod_{j\neq i}M_j} \log \left|[\bm H_i]_{\mathcal{M}_i} [\bm H_i]_{\mathcal{M}_i}^{\mathsf{H}} \right|\\+  \sum_{i=1}^I {\prod_{j\neq i}M_j} \log \left|[\bm \Gamma_i]_{\mathcal{M}_i}  \right|+ \prod_{i=1}^I[\bm x_i]_{\mathcal{M}_i}^\mathsf{H}  [\bm \Gamma_i]_{\mathcal{M}_i}^{-1} [\bm x_i]_{\mathcal{M}_i}.
    \end{multline}
    Setting the derivative of the above function with respect to $\bm\gamma_i$ to zero gives
    \begin{equation}
         {\prod_{j\neq i}M_j} [\bm \gamma_i]_{\mathcal{M}_i}  =  [\bm x_i]_{\mathcal{M}_i}^2  \prod_{j\neq i}[\bm x_j]_{\mathcal{M}_j}^\mathsf{H}  [\bm \Gamma_j]_{\mathcal{M}_j}^{-1} [\bm x_j]_{\mathcal{M}_j},
    \end{equation}
    for $i=1,2,\ldots,I$. However, since $\{\bm \gamma_i\}\in\mathcal{C}$, we obtain the unique minimum of $ \mathcal{L}(\{[\bm \gamma_i]_{\mathcal{M}_i})$ at
    \begin{equation}
        [\bm \gamma_i]_{\mathcal{M}_i}=\begin{cases}
            \frac{[\bm x_i]_{\mathcal{M}_i}^2}{\|[\bm x_i]_{\mathcal{M}_i}^2\|}, & \text{if } i<I\\
            [\bm x_I]_{\mathcal{M}_I}^2  \prod_{j=1}^{I-1}\|[\bm x_j]_{\mathcal{M}_j}^2\|  & \text{if } i=I.
        \end{cases}
    \end{equation}

Therefore, every set of $\{\mathcal{M}_I\}_{i=1}^I$ corresponds to one unique local minimum, and counting for all possible index set combinations, we get $\mathcal{N}\leq \prod_{i=1}^I\binom{N}{{M_i}}$. Further, all index set combinations containing a degenerate solution share the same minimum~\cite{wipf2004sparse}. Accounting for such repetitions, we derive~\eqref{eq.upper_local_minima}. Thus, the proof is complete.

% \begin{proposition}\label{prop.unique_minima}
% We have the following noiseless system
% \begin{equation}\label{eq.kronecker_noiseless_sys}
%     \bm y = \left(\otimes_{i=1}^I\bm H_i\right)\left(\otimes_{i=1}^I\bm x_i \right),
% \end{equation}
% where $\bm H_i$s are assumed to satisfy the URP such that for all $i$, every subset of $M$ columns of $\bm H_i$, denoted as $[\bm H_i]_\mathcal{M}$, are linearly independent. Then for every subset of $M$ basis vectors with associated $[\bm \gamma_i]_{n_i}$ values, denoted $[\bm \Gamma_{i}]_\mathcal{M}$, there is at most one minimum of $\mathcal{L}$ regarding these $M$ bases.
% \end{proposition}

% \begin{proof}
% We take $\bm \gamma_1$ as an example. From~\eqref{eq.kronecker_noiseless_sys} we have $\bm y = \otimes_{i=1}^N \bm y_i$. Based on URP, we rewrite the $\mathcal{L}$ as
%     \begin{align}
%     \mathcal{L} &= \log \left|\otimes_{i=1}^I [\bm H_i]_\mathcal{M} [\bm \Gamma_i]_\mathcal{M} [\bm H_i]_\mathcal{M}^{\mathsf{H}} \right| \notag\\&\hspace{2.3cm}+ \bm y^\mathsf{H} \left( \otimes_{i=1}^I [\bm H_i]_\mathcal{M} [\bm \Gamma_i]_\mathcal{M} [\bm H_i]_\mathcal{M}^{\mathsf{H}}\right)^{-1} \bm y\\
%     &=N^{I-1} \left(\sum_{i=1}^I \log \left|[\bm H_i]_\mathcal{M} [\bm \Gamma_i]_\mathcal{M} [\bm H_i]_\mathcal{M}^{\mathsf{H}}\right|\right)\notag\\
%     &\hspace{2cm}+C_1 \bm y^\mathsf{H}_1 [\bm H_1]_\mathcal{M}^{-1} [\bm \Gamma_1]_\mathcal{M}^{-1} [\bm H_1]_\mathcal{M}^{\mathsf{H}-1}\bm y_1 + C_2\\
%     % &=N^2 \sum_{i=1}^N \log [\bm \gamma_1]_i + \otimes_{j=1}^3 \bm y^\mathsf{H}_j (\bm H_j^M \bm \Gamma_j^M \bm H_j^{M\mathsf{H}})^{-1}\bm y_j + C_1\\
%     % &=N^{I-1} \sum_{n_1=1}^N \log [\bm \gamma_1]_{n_1} + C_2 \bm y^\mathsf{H}_1 (\bm H_1^M)^{-1} (\bm \Gamma_1^M)^{-1} (\bm H_1^{M\mathsf{H}})^{-1}\bm y_1 + C_1\\
%     % &=N^{I-1} \sum_{n_1=1}^N \log [\bm \gamma_1]_{n_1} + C_2 \sum_{i=1}^N \frac{[\bm x_1]_i^2}{[\bm \gamma_1]_i} + C_1\\
%     &=\sum_{n_1=1}^N N^{I-1} \log [\bm \gamma_1]_{n_1} + C_1\frac{[\bm x_1]_{n_1}^2}{[\bm \gamma_1]_{n_1}} + C_2', \numberthis
%     \end{align}
% where $C_1$, $C_2$, and $C_2'$ are irrelevant scalars to $\bm \gamma_1$. We can see that the unique minimum is achieved at $\bm \gamma_1 = C_2N^{-I+1}|\bm x_1|^2$.
% \end{proof}

% %We present the relation between the URP of $\bm H$ and that of $\bm H_i$ for all $i$ to make a fair comparison.



% Lemma \ref{prop.kron_urp} points out that $\bm H_i$ for all $i$ also satisfy the URP. According to the discussion in \cite{wipf2004sparse}, the number of local minima for each hyperparameter $\bm \gamma_i$ is bounded by the term in the bracket of~\eqref{eq.upper_local_minima}. Combining local minima of different $\bm \gamma_i$s produces the local minima of $\mathcal{L}$ which gives~\eqref{eq.upper_local_minima}.

\bibliographystyle{IEEEtran}
\bibliography{bibfile}

% \newpage

% \section{Biography Section}
% If you have an EPS/PDF photo (graphicx package needed), extra braces are
%  needed around the contents of the optional argument to biography to prevent
%  the LaTeX parser from getting confused when it sees the complicated
%  $\backslash${\tt{includegraphics}} command within an optional argument. (You can create
%  your own custom macro containing the $\backslash${\tt{includegraphics}} command to make things
%  simpler here.)
 
% \vspace{11pt}

% \bf{If you include a photo:}\vspace{-33pt}
% \begin{IEEEbiography}[{% Figure removed}]{Michael Shell}
% Use $\backslash${\tt{begin\{IEEEbiography\}}} and then for the 1st argument use $\backslash${\tt{includegraphics}} to declare and link the author photo.
% Use the author name as the 3rd argument followed by the biography text.
% \end{IEEEbiography}

% \vspace{11pt}

% \bf{If you will not include a photo:}\vspace{-33pt}
% \begin{IEEEbiographynophoto}{John Doe}
% Use $\backslash${\tt{begin\{IEEEbiographynophoto\}}} and the author name as the argument followed by the biography text.
% \end{IEEEbiographynophoto}

\vfill

\end{document}

% \subsection{Proof of Eq. \texorpdfstring{\eqref{eq.separate_log_likeli}}{TEXT}}\label{appe.separable_loglikeli}
% We will show this, for example, by separating $\alpha^{-1} = [\bm \gamma_1]_1$ from other entries in $\bm \gamma_1$. We have
% \begin{equation}
% \begin{aligned}
%     \bm \Sigma_{\bm y} 
%     % &= \sigma^2 \bm I + \bm H \bm \Gamma \bm H^\mathsf{H}\\
%     &=\sigma^2 \bm I + (\bm H_1 \bm \Gamma_1 \bm H_1^\mathsf{H})\otimes (\otimes_{i=2}^I\bm H_i \bm \Gamma_i \bm H_i^\mathsf{H})\\
%     &=\sigma^2 \bm I + (\alpha^{-1}[\bm H_1]_1[\bm H_1]_1^\mathsf{H}+\bm H_{1^-} \bm \Gamma_{1^-} \bm H_{1^-}^\mathsf{H})\otimes(\otimes_{i=2}^I\bm H_i \bm \Gamma_i \bm H_i^\mathsf{H})\\
%     % &=\sigma^2 \bm I\! +\! [\bm \gamma_1]_{n_i}[\bm H_1]_{n_i}[\bm H_1]_{n_i}^\mathsf{H}\otimes (\bm H_2 \bm \Gamma_2 \bm H_2^\mathsf{H}) \otimes (\bm H_3 \bm \Gamma_3 \bm H_3^\mathsf{H})\!\\&+\!\bm H_{1n_i^-} \bm \Gamma_{1n_i^-} \bm H_{1n_i^-}^\mathsf{H}\otimes (\bm H_2 \bm \Gamma_2 \bm H_2^\mathsf{H}) \otimes (\bm H_3 \bm \Gamma_3 \bm H_3^\mathsf{H})\\
%     &=\sigma^2 \bm I + \alpha^{-1}\bm H_{\alpha} \bm \Gamma_{\alpha} \bm H_{\alpha}^\mathsf{H}+\bm H_{\alpha^-} \bm \Gamma_{\alpha^-} \bm H_{\alpha^-}^\mathsf{H}\\
%     &=\alpha\bm H_{\alpha} \bm \Gamma_{\alpha} \bm H_{\alpha}^\mathsf{H} + \bm \Sigma_{ \alpha^-},
% \end{aligned}    
% \end{equation}
% where $\bm H_{1^-}$ is a matrix excluding the first column, $\bm \Gamma_{1^-}$ is a diagonal matrix excluding the first diagonal entry, and $\bm \Sigma_{\alpha^-} = \sigma^2 \bm I + \bm H_{\alpha^-} \bm \Gamma_{\alpha^-} \bm H_{\alpha^-}^\mathsf{H}$ does not rely on $\alpha$. Further, by utilizing the matrix determinant lemma and inversion lemma \cite{zhang2017matrix}, we~have
% \begin{equation}
% \begin{aligned}
% |\bm \Sigma_{\bm y}| &= |\bm \Sigma_{\alpha^-}||\bm I_{N^{I-1}} + \alpha\bm \Gamma_\alpha^{1/2}\bm H_\alpha^\mathsf{H} \bm \Sigma_{\bm \alpha^-}^{-1} \bm H_\alpha \bm \Gamma_\alpha^{1/2}|\\
% \bm \Sigma_{\bm y}^{-1} &= \bm \Sigma_{\alpha^-}^{-1} - \bm \Sigma_{\alpha^-}^{-1} \bm H_\alpha\bm \Gamma_\alpha^{1/2} (\alpha^{-1}\bm I+\bm \Sigma_\mathsf{h})^{-1}\bm \Gamma_\alpha^{1/2}\bm H_\alpha^\mathsf{H}\bm \Sigma_{\alpha^-}^{-1},
% \end{aligned}
% \end{equation}
% where $\bm \Sigma_\mathsf{h}=\bm \Gamma_\alpha^{1/2}\bm H_\alpha^\mathsf{H} \bm \Sigma_{\alpha^-}^{-1} \bm H_\alpha \bm \Gamma_\alpha^{1/2}$. This gives 
% \begin{equation}
% \begin{aligned}
%     \mathcal{L}&=\log p(\bm y|\{\bm \gamma_i\}) \\&= -\log |\bm \Sigma_{\bm y}| - \bm y^\mathsf{H} (\bm \Sigma_{\bm y})^{-1} \bm y\\
%     &=-\log |\bm \Sigma_{\alpha^-}| - \log |\bm I_{N^{I-1}} + \alpha\bm \Sigma_\mathsf{h}| - \bm y^\mathsf{T}\bm \Sigma_{\alpha^-}^{-1}\bm y \\&+ \hat{\bm y}^\mathsf{H} (\alpha^{-1}\bm I+\bm \Sigma_\mathsf{h})^{-1}\hat{\bm y}\\
%     &=\mathcal{L}_{\alpha^-}-l(\alpha),
% \end{aligned}
% \end{equation}
% where $\hat{\bm y}=\bm \Gamma_\alpha^{1/2}\bm H_\alpha^\mathsf{H}\bm \Sigma_{\alpha^-}^{-1}\bm y$. $\mathcal{L}_{\alpha^-}$ does not depend on $\alpha$. 
% % {\appendix[Proof of the Zonklar Equations]
% % Use $\backslash${\tt{appendix}} if you have a single appendix:
% % Do not use $\backslash${\tt{section}} anymore after $\backslash${\tt{appendix}}, only $\backslash${\tt{section*}}.
% % If you have multiple appendixes use $\backslash${\tt{appendices}} then use $\backslash${\tt{section}} to start each appendix.
% % You must declare a $\backslash${\tt{section}} before using any $\backslash${\tt{subsection}} or using $\backslash${\tt{label}} ($\backslash${\tt{appendices}} by itself
% %  starts a section numbered zero.)}