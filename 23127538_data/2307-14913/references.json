{
  "2111-09543": {
    "title": "DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing",
    "authors": [
      "Pengcheng He",
      "Jianfeng Gao",
      "Weizhu Chen"
    ],
    "submission_date": "2021-11-18",
    "semantic_scholar_id": "972706306f85b1bfb40c7d35c796ad5174eb0c9c"
  },
  "2007-14062": {
    "title": "Big Bird: Transformers for Longer Sequences",
    "authors": [
      "M. Zaheer",
      "Guru Guruganesh",
      "Kumar Avinava Dubey",
      "J. Ainslie",
      "Chris Alberti",
      "Santiago Ontañón",
      "Philip Pham",
      "Anirudh Ravula",
      "Qifan Wang",
      "Li Yang",
      "Amr Ahmed"
    ],
    "submission_date": "2020-07-28",
    "semantic_scholar_id": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3"
  },
  "1907-11692": {
    "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
    "authors": [
      "Yinhan Liu",
      "Myle Ott",
      "Naman Goyal",
      "Jingfei Du",
      "Mandar Joshi",
      "Danqi Chen",
      "Omer Levy",
      "M. Lewis",
      "Luke Zettlemoyer",
      "Veselin Stoyanov"
    ],
    "submission_date": "2019-07-26",
    "semantic_scholar_id": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de"
  },
  "1810-04805": {
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "authors": [
      "Jacob Devlin",
      "Ming-Wei Chang",
      "Kenton Lee",
      "Kristina Toutanova"
    ],
    "submission_date": null,
    "semantic_scholar_id": "df2b0e26d0599ce3e70df8a9da02e51594e0e992"
  }
}