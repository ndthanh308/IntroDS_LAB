\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2022


% ready for submission
\usepackage{neurips_2023}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2022}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2022}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2022}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}
\usepackage{todonotes}
\usepackage{caption}
\usepackage{comment}
\usepackage{subcaption}
\usepackage{amsthm}
% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{booktabs}            % professional-quality tables
\usepackage{multirow}            % tabular cells spanning multiple rows
\usepackage{amsfonts}            % blackboard math symbols
\usepackage{graphicx}            % figures
\usepackage{duckuments}          % sample images
\usepackage{dsfont}
\usepackage{natbib}
\usepackage{scalerel}
% \usepackage[noend]{algpseudocode}
\usepackage[noend,ruled,vlined,noline]{algorithm2e}
% Set algorithm keyword formatting
\newcommand{\Var}{\textup}
\newcommand{\Comment}{\tcc*[r]}
\SetKwComment{tcc}{$\triangleright$~}{}
\SetCommentSty{normalfont}
\SetKwInput{KwRequire}{Require}
% \SetKwComment{Comment}{$\triangleright$\ }{}
% Set algorithm line numbers
% \SetNlSty{}{}{:}

\usepackage{wrapfig}

\setcitestyle{numbers}
% \usepackage{algorithmic}
\usepackage{etoolbox}\AtBeginEnvironment{algorithm}{\tiny}

\usepackage{streams}
\usepackage{stringdiagrams}
\usepackage{csquotes}
\usepackage{dirtytalk}

\newcommand{\stefano}[1]{\textcolor{green}{stefano: #1}}
\newcommand{\alberto}[1]{\textcolor{cyan}{Alberto: #1}}
% Attempt to make hyperref and algorithmic work together better:
% \newcommand{\theHalgorithm}{\arabic{algorithm}}
\newcommand{\mc}{\mathcal}
\newcommand{\B}{\mathbf}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}


\newcommand{\note}[1]{\textcolor{red}{Note: #1}}
\newcommand{\fg}[1]{\textcolor{blue}{F: #1}}
\newcommand{\ste}[1]{\textcolor{green}{Ste: #1}}


\title{Categorical Foundation of Explainable AI}



% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  David S.~Hippocampus\thanks{Use footnote for providing further information
    about author (webpage, alternative address)---\emph{not} for acknowledging
    funding agencies.} \\
  Department of Computer Science\\
  Cranberry-Lemon University\\
  Pittsburgh, PA 15213 \\
  \texttt{hippo@cs.cranberry-lemon.edu} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}


\begin{document}


\maketitle


\begin{abstract}
% Explainable AI (XAI) has become very popular by  studying the deployment of explaining techniques for black-boxes and interpretable AI models. 
% While the concept of ``explainability'' might seem intuitive, 
Explainable AI (XAI) aims to address the human need for safe and reliable AI systems. However, numerous surveys emphasize the absence of a sound mathematical formalization of key XAI notions---remarkably including the term ``\textit{explanation}'' which still lacks a precise definition. To bridge this gap, this paper presents the first mathematically rigorous definitions of key XAI notions and processes, using the well-funded formalism of Category theory. We show that our categorical framework allows to: (i) model existing learning schemes and architectures, (ii) formally define the term ``explanation'', (iii) establish a theoretical basis for XAI taxonomies, and (iv) analyze commonly overlooked aspects of explaining methods. As a consequence, our categorical framework promotes the ethical and secure deployment of AI technologies as it represents a significant step towards a sound theoretical foundation of explainable AI.
% In order to bridge this gap, this paper presents the first mathematically rigorous, unifying theory of explainable AI. By using the well-funded  apparatus of Category theory, we provide a theoretical framework allowing the first formalization of key XAI notions and processes. We then show that this categorical framework enables to
% As a result, this work provides guidance in navigating the field and strengthens the reputation of XAI, promoting the ethical and secure deployment of AI technologies. 
% In summary, the proposed foundation of XAI represents a valuable tool for framing future research and a useful guide for researchers entering the field.
% Using the formal and abstract language of category theory, we provide the first formal definitions for all essential terms in XAI. Based on these definitions, we propose a taxonomy of XAI methods, demonstrating how the introduced theory can be used to categorize the main classes of XAI systems currently studied in literature.
% These reviews also call for a sound and unifying formalism for XAI, which can help researchers navigate a rapidly growing body of knowledge and avoid the emergence of ill-posed questions. 
% we present what is, to the best of our knowledge, the first sound formal theory of XAI. 
% \mike{kindly consider a rephrase of the previous sentence, something like: 'To the authors knowledge this is the first attempt to formalize a unifying theory of XAI '}
% Employing the framework of category theory, and feedback monoidal categories in particular, we first provide formal definitions for all essential terms in explainable AI. Then we propose a taxonomy of the field following the proposed structure, showing how the introduced theory can be used to categorize the main classes of XAI systems currently studied in literature.
% % apparently irreconcilable approaches. 
% % We believe that the formalism we propose will be an invaluable tool to properly frame future research lines, and a precious guidance for new researchers approaching the field of explainable AI.
% In summary, the foundation of XAI proposed in this paper represents a significant tool to properly frame future research lines, and a precious guidance for new researchers approaching the field. 
% \mike{Kindly consider a rephrase without the 'In summary, we believe'. You already mentioned 'the best of authors' knowledge'.  Further words like: 'believe' will increase the uncertainty of the reader about the quality and contribution of your work! And you contribute something very good!!! :) Let them be sure about that :) }
%To this end, we use the category of Cartesian streams to propose the first formal definitions of the essential terminology in explainable AI to strengthen the reputation of the field. Based on our definitions we also propose the first formal taxonomy of explainable AI based on categorical structures and semantics in order to provide guidance in navigating the field. 
\end{abstract}


\section{Introduction}
% The rise of Artificial Intelligence (AI) poses ethical~\citep{duran2021afraid,lo2020ethical} and legal~\citep{wachter2017counterfactual, gdpr2017} concerns. Addressing these concerns is crucial for the deployment of such technologies in safety-critical domains which require accurate and trustworthy AI agents~\citep{rudin2019stop,shen2022trust}. 
% % Philosophical concerns turn into pressing needs in safety-critical domains which 
Explainable AI (XAI) has emerged as a critical research field to address ethical~\citep{duran2021afraid,lo2020ethical} and legal~\citep{wachter2017counterfactual, gdpr2017} concerns related to the safe deployment of AI technologies.
% Explainable AI (XAI) aims to address the human need for safe and trustworthy AI dictated by ethical~\citep{duran2021afraid,lo2020ethical} and legal~\citep{wachter2017counterfactual, gdpr2017} concerns. 
%
%
% through the design of interpretable AI models and algorithms able to explain uninterpretable AI models~\citep{arrieta2020explainable}. % A considerable number of surveys attempted to organize this fast-growing literature to pinpoint the main opportunities, challenges, and research directions~\citep{adadi2018peeking,Das2020OpportunitiesAC,arrieta2020explainable,dovsilovic2018explainable,tjoa2020survey,gunning2019xai,hoffman2018metrics,palacio2021xai}. 
% Despite the increasing interest in the domain, 
However, several domain surveys remark that key fundamental XAI notions
% at the very core of the idea of 
still lack a formal definition, and that the whole field is still missing a unifying and sound formalism~\citep{adadi2018peeking,palacio2021xai}.
% , despite a few recent review papers~\citep{Das2020OpportunitiesAC,palacio2021xai}. 
The notion
% of this gap is that the very 
of \textit{explanation} represents a pivotal example as it still lacks a proper mathematical formalization, as demonstrated by the attempts to define the concept in the current literature:
% {\small
% \begin{displayquote}
{\small\textit{``An explanation is an answer to a `}why?\textit{' question''~\citep{miller2019explanation}} or
\textit{``An explanation is additional meta information, generated by an external algorithm or by the machine learning model itself, to describe the feature importance or relevance of an input instance towards a particular output classification''~\citep{Das2020OpportunitiesAC}}}.
% or
% \textit{``An explanation is the process of describing one or more facts, such that it facilitates the understanding of aspects related to said facts''}~\citep{palacio2021xai}.
% \end{displayquote}
% }


We argue that the absence of a rigorous formalization of key explainable AI notions may significantly undermine progress in the field by leading to ill-posed questions, annoying clutter in taxonomies, and narrow perspectives on future research directions. In contrast, a sound mathematical definition would provide a solid foundation for XAI notions and taxonomies, leading to a more organized and efficient research field. This way, researchers could easily identify knowledge gaps and promising research directions---including the exploration of the (currently overlooked) theoretical side of explainable AI. 


% \textbf{Contributions.} We propose the first-ever mathematical formalization of fundamental XAI notions and processes (\Cref{sec:framework}). To this end, we use Category Theory as it represents a sound process-oriented mathematical formalism and, for this reason,  

\textbf{Contributions.} We propose a theoretical framework allowing a unified, comprehensive and rigorous formalization of foundational XAI notions and processes (\Cref{sec:framework}). 
To the best of the authors' knowledge, this is the first-ever work investigating such a research direction in the XAI field.
To this objective, we use Category theory as it represents a sound mathematical formalism centered around \emph{processes}, and for this reason, it is widely used in theoretical computer science~\citep{Abramsky2004ACS, Selinger2001ControlCA, stein2021, Swan2022, Turi1997TowardsAM}, and, more recently, in AI~\citep{aguinaldo2021graphical,cruttwell2022categorical, ong2022learnable, shiebler2021category, katsumata19}. 
In particular, we show that our categorical framework enables us to: model existing learning schemes and architectures (\Cref{sec:finding1}), formally define the term ``explanation'' (\Cref{sec:syExp}),  establish a theoretical basis for XAI taxonomies (\Cref{sec:tax}), and analyze commonly overlooked aspects of explaining methods (\Cref{sec:xai-semantics}).

% Specifically, in this paper, we formalize the notions of ``learning'' and ``explaining agents'' (Section~\ref{sec:learning-agent}), and provide the first formal definition of ``explanation" (Section~\ref{sec:syExp}). We then make a concrete use of our framework to generate the first theory-grounded taxonomy of XAI methods (Section~\ref{sec:tax}), highlighting concepts overlooked by current taxonomies. 
%Using our formalization we show that: (i)


% A categorical formalization provides several advantages, which are established principles in programming language semantics \citep{Abramsky2004ACS, Selinger2001ControlCA, Swan2022, Turi1997TowardsAM} and, for this reason, represents now a growing trend in the AI literature \citep{aguinaldo2021graphical,shiebler2021category,cruttwell2022categorical,ong2022learnable}. 

% Our approach provides several established advantages, such as abstraction, uniformity, and compositionality, which allow for a unified framework that can accommodate both learning and explanation tasks.

% % Similar informal definitions are also common for other key actors in the field.
% %This lack of solid mathematical foundations and coherence in the field not only undermines the soundness of current research, but also slows current and future research down by distracting researchers in explainable AI and by gathering skepticism from researchers in other fields.
% The absence of a rigorous formalization of key explainable AI notions may severely undermine this research field, as it could lead to ill-posed questions, induce re-discovery of the same ideas, and hinder the study of existing and future devised explainable/interpretable models from a more general perspective. On the contrary, mathematically defining foundational aspects of XAI paves the way to common theoretical advancements in the field in the form of theorems.
% % and make it difficult for new researchers to approach the domain.
% To fill this gap, we propose the first mathematical formalization of fundamental XAI notions by exploiting Category Theory. In facts, the formalization of AI concepts by using category theory has recently attracted the attention of several researchers and has become a growing trend in the literature \citep{aguinaldo2021graphical,shiebler2021category,cruttwell2022categorical,ong2022learnable}.
% However, to the best of our knowledge this is the first paper to use category theory, or any other mathematical theory, to formally define XAI notions.
% % Category theory is a mathematical framework, providing a sound and abstract formalism to study general structures and systems of structures.
% In any case, the categorical framework provides several well-known advantages, which are established principles in programming language semantics \citep{Abramsky2004ACS, Selinger2001ControlCA, Turi1997TowardsAM}. % categorical structures are sufficient to define fundamental XAI notions
% For instance, fundamental notions of AI and XAI, like learning and explaining agents, may be seen as special instances of categorical definitions (\emph{abstraction}), category theory provide a unified framework to accomodate both learning and explanation tasks 
% (\emph{uniformity}), and categorical structures  naturally enables compositional reasoning (\emph{compositionality}). 
% % In particular, as we will see in Sec. \ref{sec:framework}, we introduced Cartesian Streams as they allow to define learning (and explaining) agents in a unified way.



%%% FG direi non necessario:
% For this reason, many mathematical disciplines are affected by category theory, including algebra~\citep{eilenberg1945general}, geometry~\citep{bredon2012sheaf}, logic~\citep{johnstone2014topos}, 
% % programming semantics~\citep{jacobs2009categorical,abramsky2004categorical,uustalu2008comonadic}, 
% and more recently machine learning~\citep{cruttwell2022categorical,ong2022learnable}.



% \textbf{Problem statement and contributions.}
% Problem: Provide a formal and unifying theory of explainable AI. Contributions:
% \begin{itemize}
%     \item provided a formal and unifying theory of explainable AI
%     \item demonstrated how a unifying theory is concretely useful allowing to: (i) navigate the field with a shared language and notation, (ii) cluster/unify apparently different open problems, and (iii) identify research directions and opportunities previously unnoticed due to the mess
%     \item show how general theory allows better experimental design
%     \item experiments on concept robustness and link to model invarances
% \end{itemize}


% \textbf{Knowledge gap.}
% Informally explainable AI aims at solving the problem of explaining complex models decision processes to their users with the aim of increasing user trust in the models. However, the problem is ill-defined as the structure of these agents and concepts like ``explain'' or ``trust'' are not mathematically formalized.

% \todo{Give example of how bad the situation is with formalism. Give 2 SHORT current definitions of the same object:}
% \begin{quote}%[White box/interpretable model]
% \textit{Given the agents $f$ (model) and $g$ (user), the model $f$ is a white box for the user $g$ if the user can understand the cause of the decisions of $f$~\citep{miller2019explanation} or, equivalently, if the user can consistently predict the decisions of $f$~\citep{kim2016examples}.}
% \end{quote}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Comment if we need more space :
%The paper is organized as follows. Section \ref{sec:pre} briefly recalls 
% the most commonly used taxonomies to describe XAI systems and 
%some basics of category theory. In Section \ref{sec:framework} we introduce the formal definitions for XAI and Section \ref{sec:tax} reviews the existing literature following the proposed theory. Finally, Section \ref{sec:key} discusses the key findings of this paper and draws some conclusions. %For guidance, Table~\ref{tab:notation} reports the main references to the notation used in this paper.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% \section{Notation and basic definitions}
\section{Background on Category Theory}
\label{sec:pre}
Category theory is a mathematical framework that excels in formalizing processes---making this a natural formalism to analyze ``learning processes''.
% Processes are first-class entities in category theory and, by asking specific properties to these processes, we are able to describe learning agents.
This section introduce the basics of category theory required for our formalization of XAI notions, including: Monoidal categories and string diagrams we will use to represent the backbone structure of ``explaining agents'' (\Cref{sec:moncat}); Cartesian streams we will use to model the dynamic process of ``learning'' (\Cref{sec:cartstream}); signatures and institutions we will use to formally define what an ``explanation'' is (\Cref{sec:inst}). 
% and free categories, which we will use to define abstract XAI structures (\Cref{sec:freecat}).
% For further definitions and details we refer interested readers to the Appendix. 
% For guidance, Table~\ref{tab:notation} reports the main references to the notation used in this paper.
%%%
% In particular, we will use feedback monoidal categories as a syntax to model structures sharing some of the key properties of AI systems, being able to: observe inputs, provide outputs, and receive feedback dynamically. 
% Cartesian streams provide a semantics for these models. We will use the category of signatures to model the structure of ``explanations''.
% Further details can be found in the apendix.


\subsection{Monoidal Categories and String Diagrams}
\label{sec:moncat}
Intuitively a category is a collection of objects and morphisms satisfying specific composition rules. 
% Hence, category theory provides a suitable syntax to model compositional processes.
\begin{definition}[\citet{eilenberg1945general}]
    A \emph{category} \(\cat{C}\) consists of a class of \emph{objects} $\cat{C}^o$ and, for every \(X,Y \in \cat{C}^o\), a set of \emph{morphisms} $\hom(X,Y)$ with input type \(X\) and output type \(Y\). A morphism \(f \in \hom(X,Y)\) is written \(f \colon X \to Y\), and for all morphisms \(f \colon X \to Y\) and \(g \colon Y \to Z\) there is a \emph{composite} morphisms \(f \dcomp g \colon X \to Z\), with composition being associative. For each \(X \in \cat{C}^o\) there is an \emph{identity} morphism \(\id{X} \in \hom(X,X)\) that makes composition unital.
    % (\Cref{fig:category-string-diagrams}, bottom).
\end{definition}

Well-known examples are the category $\cat{Set}$, whose objects are \emph{sets} and morphisms are \emph{functions}, and the category $\cat{Vec}$, whose objects are \emph{vector spaces} and morphisms are \emph{linear maps}.
Different categories can also be connected using special operators called \emph{functors} i.e., mappings of objects and morphisms from one category to another (preserving compositions and identity morphisms).  
For instance, there is a functor $\mc{F}$ from $\cat{Vec}$ to $\cat{Set}$ that simply ignores the vector space structure. 
It associates to each vector space $\B V\in\cat{Vet}^o$ the set $\mc{F}(\B V)=V\in\cat{Set}^o$, and to each linear mapping $f\in\hom(\B V,\B W)$ in $\cat{Vet}$ the function $\mc{F}(f) = f \in\hom(V,W)$ in $\cat{Set}$.
% The category $\cat{Top}$ whose objects are topological spaces and morphisms are continuous functions.
% We can connect different categories using a special operator called \emph{functor} i.e., a mapping preserving compositions and identity morphisms between two categories $\cat{C}_1$ and $\cat{C}_2$, such that objects and morphisms of $\cat{C}_1$ are mapped into objects and morphisms of $\cat{C}_2$.

In this work we are mainly interested in \emph{monoidal categories} as they offer a sound formalism for processes with multiple inputs and outputs~\citep{Coecke2017,fritz2020}.
Monoidal categories~\citep{maclane78} are categories with additional structure, namely a monoidal product $\times$ and a neutral element, 
% $e$
enabling the composition of morphisms in parallel (cf. Appendix \ref{app:mon-cat}). Notably, monoidal categories allow for a graphical representation of processes using \emph{string diagrams}~\citep{joyal1991geometry}. String diagrams enable a more intuitive reasoning over equational theories, and we will use them throughout the paper to provide illustrative, yet formal, definitions of XAI notions. The Coherence Theorem for monoidal categories~\citep{maclane78} guarantees that string diagrams are a sound and complete syntax for monoidal categories. Thus all coherence equations for monoidal categories correspond to continuous deformations of string diagrams. For instance, given \(f \colon X \to Y\) and \(g \colon Y \to Z\), the morphisms \(f \dcomp g \colon X \to Z\) and \(\id{X}\) are represented as
% Morphisms of monoidal categories can be seen as processes with (possibly multiple) inputs and outputs~\citep{Coecke2017,fritz2020}, whose composition flow can be intuitively represented by string diagrams~\citep{joyal1991geometry}. 
% The Coherence Theorem for monoidal categories~\citep{maclane78} ensures that string diagrams are a sound and complete syntax for monoidal categories, and thus all coherence equations for monoidal categories correspond to continuous deformations of string diagrams. For instance, given \(f \colon X \to Y\) and \(g \colon Y \to Z\), the morphisms \(f \dcomp g \colon X \to Z\) and \(\id{X}\) are represented as
% \begin{wrapfigure}[1]{r}{0.50\textwidth}
% \vspace{-0.1cm}
% Figure removed;
% \end{wrapfigure}
%as\begin{center}% Figure removed\end{center}
%% Figure environment removed
the equation \(f \dcomp \id{Y} = f = \id{X} \dcomp f\) as
%% Figure environment removed
%\\
%\begin{center}
% Figure removed;
the morphism $h$ with multiple inputs $X_1,X_2,X_3$ and outputs $Y_1,Y_2$ (left), and the parallel composition of two morphisms \(f_1 \colon X_1 \to Y_1\) and \(f_2 \colon X_2 \to Y_2\) (right) can be represented as follows:
% \begin{wrapfigure}[3]{r}{0.31\textwidth}
% \vspace{-0.4cm}

% Figure environment removed
% \end{wrapfigure}
%% Figure environment removed
%\begin{center}
%\includegraphics[scale=0.50]%{Figure/esempio.pdf}   






\subsection{
% Feedback monoidal categories and 
Cartesian Streams}
\label{sec:cartstream}
% Symmetric monoidal categories are monoidal categories where is possible to swap objects in a parallel process, see the appendix for the definition.
A common process in machine learning involves the update of the parameters of a function, based on the \emph{feedback} of a loss function. To model this process, we can use \emph{feedback monoidal categories}.
% Sequential and parallel processes with multiple inputs and outputs can be suitably modeled by morphisms of (Cartesian\footnote{Roughly a symmetric monoidal category allows to switch objects, while a Cartesian monoidal category has also the possibility to copy and discard objects. % cartesian is always symmetric See Appendix \ref{app:symm-mon-cat} for more details.}) monoidal categories. However, the learning phase for AI models is often dynamic, observing inputs, producing outputs, and getting \emph{feedback} over and over again to adjust the parameters they depend on during training. For this reason, we introduce feedback monoidal categories, and particularly cartesian streams, to model this dynamic behaviour.
\begin{definition} [\citep{katis02,di2021canonical}]
A \emph{feedback monoidal category} is a symmetric (cf. Appendix~\ref{app:symm-mon-cat}) monoidal category \(\cat{C}\) endowed with a functor $\mc{F}:\cat{C} \rightarrow \cat{C}$, and an operation \(\fbk[S] \colon \hom (X \times \mc{F}(S), Y \times S) \to \hom (X,Y)\) for all \(X,Y,S\) in \(\cat{C}^o\), which satisfies a set of axioms (cf.  \Cref{app:feedback-cat}).
\end{definition}
 % To model this dynamic behavior, we can use the category of Cartesian streams~\citep{uustalu05,katsumata19}. 
% \fg{Questa cosa di sintassi e semantica non la capisco; Elena puoi chiarirci o provare a rifrasare? Io avrei detto semplicemente che tra le varie feedback monoidal categories, noi si considera i cartesian stream PERCHè (..ci piacciono? sono pratici? hanno un aggancio immediato con il learning? qualcosa del genere) e poi definizione}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Feedback monoidal categories are the \emph{syntax} for processes with feedback loops. Their \emph{semantics} can be given for instance by monoidal streams~\citep{monoidalStreams}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Among feedback monoidal categories (Theorem \ref{th:stream}), 
In learning processes, optimizing the loss function often involves a sequence of feedback iterations. 
Following~\citet{katsumata19}, we use \emph{Cartesian streams} (cf. \Cref{app:streams}) to model this kind of processes. 
Cartesian streams form a feedback \emph{Cartesian} monoidal category, i.e are equipped, for every object \(X\), with morphisms \(\cp_X \colon X \to X \times X\) and \(\discard_X \colon X \to e\) that make it possible to copy and discard objects (cf. Appendix~\ref{app:symm-mon-cat}). This makes them the ideal category to formalize (possibly infinite) streams of objects and morphisms.
\begin{definition}[\citep{katsumata19,uustalu2008comonadic}] 
Let $\cat{C}$ be a Cartesian category. We call $\Stream{\cat{C}}$ the category of \emph{Cartesian streams} over $\cat{C}$, whose objects \(\stream{X} = (X_0, X_1, \dots)\) are countable lists of objects in $\cat{C}$, and given \(\stream{X},\stream{Y}\in \Stream{\cat{C}}^o\), the set of \emph{morphisms} $\hom(\stream{X}, \stream{Y})$ is the set of all \(\stream{f} \colon \stream{X} \to \stream{Y}\), where \(\stream{f}\) is a family of morphisms in $\cat{C}$, \(f_n \colon X_0 \times \cdots \times X_n \to Y_n\), 
% indexed by natural numbers.
for $n\in\mathbb{N}$.
\end{definition}
%, we will simply write $X^{\mathbb{N}}$ in place of $\stream{X}$.
% \fg{Some details are missing? Where these $X_i,Y_i$ are taken from? and also these $f_n$? Are they all objects and morphisms in Set? O la costruzione può essere ottenuta per ogni data categoria in input coi suoi oggetti e morfismi? E per quello c'è il pedice in Stream\_set?}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In Cartesian streams, a morphism \(f_n\) represents a process that receives a new input \(X_n\) and produces an output \(Y_n\) at time step \(n\). We can compute the outputs until time \(n\) by combining \(f_0, \dots, f_n\) to get \(\tilde{f}_n \colon X_0 \times \cdots \times X_n \to Y_0 \times \cdots \times Y_n\) as follows:
\begin{wrapfigure}[3]{r}{0.4\textwidth}
\vspace{-0.3cm}
% Figure removed
\end{wrapfigure}
\begin{itemize}
\vspace{-0.3cm}
\item \(\tilde{f}_0 \defn f_0\)
\item \(\tilde{f}_{n+1} \defn (\id{X_{n+1}} \times \cp_{X_0 \times \cdots \times X_n}) \dcomp (f_{n+1} \times \tilde{f}_n)\)
\end{itemize}
%\[\cartesianStreamsCompositionFig\]
%\begin{center}
%\includegraphics[scale=0.50]%{Figure/Streams.pdf}\end{center}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We denote by $X^{\mathbb{N}}$ the object $\stream{X}\in\Stream{\cat{C}}^o$ such that $\stream{X}=(X,X,\ldots)$, for some $X\in \cat{C}^o$.
Notably, the following theorem demonstrates that Cartesian streams can model dynamic processes with feedback---such as learning processes, as we will show in ~\Cref{ex:mlp} and~\ref{ex:nas}. 
\begin{theorem}[\citep{monoidalStreams}]
\label{th:stream}
Cartesian streams 
% over a Cartesian category 
form a feedback Cartesian category.
\end{theorem}
% Cartesian streams allow to properly model the training of (almost) any machine learning system, like Multi-Layer Perceptrons~\ref{ex:mlp} and Recurrent Neural Networks~\ref{ex:nas},  
% considering them in a unified context thanks to their coherence theorem (\emph{coherence of string diagrams} \citep{}). 
% More details about Cartesian streams are provided in \Cref{app:streams}.

% In this setting \(\stream{X} = (X_0, X_1, \dots)\) and \(\stream{Y} = (Y_0, Y_1, \dots)\) have the meaning to encode the flow of the input and output sets of the process $\stream{f}$ during the training.


% \paragraph{Free Categories} 
% \textbf{Free Categories.} We generate ``abstract'' categories using the notion of \emph{free category}~\citep{maclane78}. Intuitively, a free category serves as a template for a  class of categories (e.g., feedback monoidals). To generate a free category, we just need to specify a set of objects and morphisms generators. Then we can realize ``concrete'' instances of a free category $\cat{F}$ using a functor from $\cat{F}$ to another category $\cat{C}$ that preserves the axioms of $\cat{F}$. If such a functor exists then $\cat{C}$ is of the same type of $\cat{F}$ (e.g., the image of a free feedback monoidal category via a feedback functor is a feedback monoidal category). 






\subsection{Institutions and Signatures}
\label{sec:inst}
To provide a formal definition of ``explanation'' (\Cref{sec:syExp}) we will rely on institution theory~\citep{goguen1992institutions}.  
Institution theory offers an ideal platform for formalizing explanations---whether expressed through symbolic languages or semantic-based models---as it enables a thorough analysis of both the structure (syntax) and meaning (semantics) of explanations across diverse languages~\citep{tarski1944semantic}, thus facilitating a deeper understanding of their nature.
% Intuitively, an institution encompasses the ``syntax'' and the ``semantics'' of any (formal) language. 
More rigorously, an institution $I$ consists of (i) a category \(\cat{Sign}_I\) whose objects are signatures (i.e. vocabularies of symbols), (ii) a functor $Sen: \cat{Sign}_I \mapsto \cat{Set}$ which provides sets of well-formed expressions ($\Sigma$-sentences) for each signature \(\Sigma\in\cat{Sign}_I^o\), and (iii) a functor $Mod: \cat{Sign}_I^{op} \mapsto \cat{Set}$\footnote{Given a category \(\cat{C}\), $\cat{C}^{op}$ denotes its \emph{opposite} category, formed by reversing its morphisms~\citep{maclane78}.} that assigns a semantic interpretation (i.e. a world) to the symbols in each signature~\citep{goguen2005concept}.
% This approach allows to formalize the concept of explanation exploiting a well-defined mathematical framework according to its intuitive meaning.  
% and serve as ``context'' or ``interpretant'' in the sense of classical logic~\citep{goguen2005concept}. 
% From this abstract vocabulary, institution theory defines abstract statements as sentences obtained from a vocabulary $\Sigma$~\citep{goguen1992institutions}.
%\begin{definition}[$\Sigma$-sentence~\citep{goguen2005concept}]
%Let $I$ be an institution and let $\Sigma$ be an object of $\cat{Sign}_I$. A $\Sigma$-\emph{sentence} is an element of \(Sen(\Sigma)\).
%\end{definition}
% However, we will show how institutions can also describe almost any form of ``explanation'' used in the literature.
% In the remaining of this section we provide few examples.
% \begin{example}
    A typical example of institution is First-Order Logic (FOL), where the category of signatures is given by sets of relations\footnote{We note that both constant and function symbols can be seen as special cases of relations.} as objects and arity-preserving functions as morphisms. Sentences and models are defined by standard FOL formulas and structures. 
% \end{example}



% The main motivation behind the use of institutions lies in their sentences and models, fundamental constituents of a logical theory.  By representing vocabularies of known forms of explanation as signatures, and consequently as objects within a fixed category, institutions possess the capacity to encode explanations within a logical framework, providing a definition of satisfaction (``truth'') invariant under vocabulary change (signature morphism).






% Other examples of institutions made in a similar fashion are Propositional logic, Modal logic, and many others. However, the notion of institution is expressive enough to encode most of the known forms of explanation used in the literature.


% \begin{example}
%     The institution of \emph{saliency  maps} can be defined as FOL, but with all the signatures' objects consisting of a single relation (e.g. binary in case of 2D-images). Intuitively, this relation corresponds to the saliency of each pixel in a certain picture. For instance, taking a signature $\Sigma=\{S\}$, this institution may includes well-formed $\Sigma$-sentences as: $\exists (x,y)\ S(x,y) \wedge S(x+1,y) \wedge S(x,y+1) \wedge S(x+1,y+1)$ or $S(a,b)$ where $a,b$ denote FOL constants. A model of the former sentence is given by a concrete saliency map that highlight a square of side 1 in a certain picture. 
%     %%%%OLD
%     % The institution of \emph{heatmaps} can be defined as FOL, where all the relations in the signature objects are binary. Intuitively, binary relations of heatmaps represent pixels with their positions and colors. Thus this institution includes well-formed $\Sigma$-sentences as the following: $\exists (x,y): P((x,y), Red) \wedge P((x+1,y), Red) \wedge P((x,y+1), Red) \wedge P((x+1,y+1), Red)$. This sentence is satisfied by the concrete heatmaps that highlight a square. 
% \end{example}
% %\fg{TODO: richeck esempio}

% The institution of \emph{heatmaps} can be defined in a similar way considering signatures' objects consisting of a singe binary relation representing position and color of each pixel in a given space.



% \begin{example}
%     The institution of \emph{formal language} $L$ has sets of letters as signature objects. Sentences are standard sets of strings well-formed according to the rules of $L$. The models of a set of sentences are sequences of strings obtained instantiating the symbols appearing in the sentences and giving an interpretation of them.
% \end{example}


% \begin{definition}[Signature~\citep{goguen1992institutions}]
% A \emph{signature} $\Sigma = (S_f, S_r, \text{ar})$ is a collection of:
% \begin{itemize}
%     \item a set of function symbols $S_f$
%     \item a set of relation symbols (or predicates) $S_r$
%     \item a morphism $\text{ar}: S_f \cup S_r \rightarrow \mathbb{N}$, which assigns a natural number called \textit{arity} to every function or relation symbol \alberto{The notation ``ar'' sucks, it is kinda incoherent with the rest. But I guess we are forced to use it because of previous papers, right?}
% \end{itemize}
% \end{definition}
% \stefano{I would not use the word morphism in this definition. Morphisms in mathemathics and in the paper have a specific meaning. I would use "function"}
%Signatures form a category $\mathsf{Sign}$ whose objects are signatures and whose morphisms $\phi: \Sigma \rightarrow \Sigma'$ are interpretations between signatures corresponding to a ``change of notation''~\citep{goguen1992institutions}.
%From this abstract vocabulary, institution theory defines abstract statements as sentences obtained from a vocabulary $\Sigma$~\citep{goguen1992institutions}.
%\begin{definition}[$\Sigma$-sentence~\citep{goguen2005concept}]
%There is a functor $Sen: \mathsf{Sign} \rightarrow \Set$ mapping each signature $\Sigma$ to the set of statements $Sen(\Sigma)$.
%A $\Sigma$-\emph{sentence} is an element of \(Sen(\Sigma)\).
%\end{definition}
% and each signature morphism $\sigma: \Sigma \rightarrow \Sigma'$ to the sentence translation map $Sen(\sigma): Sen(\Sigma) \rightarrow Sen(\Sigma')$.


%%%% moved in sec 3.4
% \citet{tarski1944semantic} and~\citet{goguen1992institutions} proved how the semantics of ``truth'' is invariant under change of signature. This means that we can safely use signature morphisms to %change ``notation'' 
% switch from one ``notation'' to another, inducing consistent syntactic changes in a $\Sigma$-sentence without impacting the ``meaning'' or the ``conclusion'' of the sentence~\citep{goguen1992institutions}. As a result, signature morphisms can translate a certain explanation between different signatures. 
%%%%%%%



% \begin{example}
% % A simple example of a $\Sigma$-sentence 
% A valid sentence in natural language is ``All men are mortal. Socrates is a man.
% Therefore, Socrates is mortal.''. Using a functor from natural language to first-order logic \elena{is there a functor from natural language to FOL? is natural language even a syntax in the mathematical sense?}, we can write the syllogism as ``$\forall x\ \text{man}(x) \rightarrow \text{mortal}(x)$, $\text{man}(Socrates) \Longrightarrow \text{mortal}(Socrates)$'', without changing its validity.
% \end{example}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



% % \subsection{Free Categories} \label{sec:freecat}
% In this work we will generate ``abstract'' categories using the notion of a \emph{free category}~\citep{maclane78}. Intuitively, a free category serves as a template for a  class of categories (e.g., feedback monoidal categories). To generate a free category, we just need to specify a set of objects' and morphisms' types. We can also realize ``concrete'' instances of a free category $\cat{F}$ using a functor from $\cat{F}$ to a category $\cat{C}$, that preserves the axioms of $\cat{F}$. If such a functor exists then $\cat{C}$ is of the same type of $\cat{F}$ (e.g., the image of a free feedback monoidal category via a feedback functor is a feedback monoidal category). 


%\elena{mention functorial semantics somewhere~\citep{lawvere1963functorial}, cite something for free constructions, for adjoints~\citep{kan1958adjoint}}
%A syntax is a way of reasoning abstractly about structures without the need to know the details of any given structure. In the same way a traditional syntax is defined by a set of symbols and some rules to combine them, the syntax of categories is

%Though all Section 4 we will make use of the notion of free category. 

%A syntax is a way of reasoning abstractly about structures without the need to know the details of any given structure. In the same way a traditional syntax is defined by a set of symbols and some rules to combine them, the syntax of categories is given by \emph{free categories}. 

% Before showing as several examples of existing (X)AI models can be defined in terms of category theory, we also provide a definition of what can be considered as a ``prototype" of an AI learning agent, and successively an explaining AI learning agent. 

%When reasoning abstractly about structures, we want to ensure that the reasoning carried out still holds in the concrete instantiations.
% Free categories is an advanced topic beyond the scope of this paper and we refer to \citep{maclane78} for an extensive discussion. 
%are defined by a set of generators for objects and for morphisms. The rules to combine them are given by the structure and axioms of of a specific categories and feedback monoidal categories, respectively.
%Syntaxes are a way of reasoning abstractly about structures without the need to know the details of any given structure.
%In the same way traditional syntaxes are defined by a set of symbols and some rules to combine them, free symmetric monoidal categories and free feedback monoidal categories are defined by a set of generators for objects and for morphisms.
%The rules to combine them are given by the structure and axioms of symmetric monoidal categories and feedback monoidal categories, respectively.
%When reasoning with a syntax, we want to ensure that the reasoning carried out still holds in the semantics.
%This is done by symmetric monoidal functors, in the case of symmetric monoidal categorie, and feedback functors, in the case of feedback monoidal categories.
%In fact, by definition of free symmetric monoidal (resp. feedback monoidal) category, once we fix the semantics of the generators, there exist a unique symmetric monoidal (feedback monoidal) functor to the semantics category.
%We will employ free feedback monoidal categories as syntax for learning agents, and take semantics in the feedback monoidal category \(\Stream{\Set}\) of cartesian streams.
%free symmetric monoidal categories and free feedback monoidal categories are defined by a set of generators for objects and for morphisms.
%The rules to combine them are given by the structure and axioms of symmetric monoidal categories and feedback monoidal categories, respectively.
%When reasoning with a syntax, we want to ensure that the reasoning carried out still holds in the semantics.
%This is done by functors for generic categories, and in particular by feedback functors, in the case of feedback monoidal categories.
%In fact, by definition of free symmetric monoidal (resp. feedback monoidal) category, once we fix the semantics of the generators, there exists a unique symmetric monoidal (feedback monoidal) functor to the semantics category. These functors can be considered as intepretations that instatantiete prototype categories into concrete ones.
%When reasoning with a syntax, we want to ensure that the reasoning carried out still holds in the semantics.
%This is done by functors for generic categories, and in particular by feedback functors, in the case of feedback monoidal categories.
%In fact, by definition of free symmetric monoidal (resp. feedback monoidal) category, once we fix the semantics of the generators, there exists a unique symmetric monoidal (feedback monoidal) functor to the semantics category. These functors can be considered as 
%In this paper, we will employ free feedback monoidal categories to define
% as syntax for 
%abstract learning agents, and the category of Cartesian streams \(\Stream{\Set}\) for concrete instance of these structures.

%As a free feedback monoidal category, cartesian streams %quotiented by observational equivalence
%are suitable to represent every other model that can compose sequentially (\emph{category}) and in parallel (\emph{monoidal category}), have a ‘backwards’ flow of corrections to the model (\emph{feedback category}), and considers equal two processes when they are 'intuitively' equal (\emph{coherence of string diagrams} \citep{}).

% \stefano{Proposta def free category: A \emph{free category} generated by a set of morphisms $\mathcal{G}$ is a category whose objects are source or target objects of generators in $\mathcal{G}$ and whose morphisms are the tuples of composable functions of $\mathcal{G}$. Intuitively, given a set of axioms for the composition, a free category generated by a set of morphisms is the most "general" category containing the generators and satisfying the chosen axioms.}



% \usepackage{graphicx}
%\begin{table}[!t]
%\centering
%\caption{Reference for notation. List of main operations, objects, morphisms, categories, and functors.}
%\label{tab:notation}
%\resizebox{\columnwidth}{!}{\small
%\begin{tabularx}{\columnwidth}{cX}
% {\small
%\toprule
%\multicolumn{1}{c}{\textbf{\textsc{Symbol}}} & \multicolumn{1}{l}{\textbf{\textsc{Description}}} \\ \midrule
% \multicolumn{2}{c}{\textbf{Operations}}  \\
%\multicolumn{2}{c}{\textbf{Objects}}  \\
%$X$ & \textit{Input}: the input type of a model. \\
%$Y$ & \textit{Output}: the output type of a model. \\
%$P$ & \textit{Parameter}: the type of a model state. \\
%$E$ & \textit{Explanation}: the output type of an explainer. \\
%\\
%\multicolumn{2}{c}{\textbf{Morphisms}}  \\
%$\id{Z}$ & \textit{Identity}: the identity operation on $Z$. \\
%$g$ & \textit{Model}: given an input $X$ and parameter $P$, returns an output $Y$. \\
%$\nabla_Y$ & \textit{Optimizer}: given a reference $Y$, a model output $Y$ and a parameter $P$, returns the updated parameter $P$. \\ 
%$\eta$ & \textit{Model explainer}: given an input $X'$ and a parameter $P$, returns an output $Y'$ and an explanation $E$. \\ 
% $\tau$ & Translator: given an explanation $\mathcal{E}$, returns an output prediction $Y$. \\
%\\
%\multicolumn{2}{c}{\textbf{Categories}}  \\
%\(\Stream{\Set}\) & \textit{Cartesian streams}: feedback monoidal category of Cartesian streams on $\Set$. \\
%\(\mathsf{Learn}\) & \textit{Category of learners}: free feedback monoidal category generated by the objects $X$, $Y$, $P$, and the morphisms $g$ and $\nabla$.\\
%\(\mathsf{XLearn}\) & \textit{Category of explainable learners}: free feedback monoidal category generated by the objects $X$, $Y$, $P$, $E$ and the morphisms $\eta$, $\nabla_Y$, and $\nabla_E$. \\
%\(\mathsf{Sign}_I\) & \textit{Category of signatures of $I$}: category of signatures of the institution $I$. \\
%\\
%\multicolumn{2}{c}{\textbf{Functors \& Operators}}  \\
%$\fbk[S]$ & \textit{Feedback}: the operation which brings an output back to the input. \\
%\(Sen\) & \textit{Sentence}: functor from the category of signatures $\mathsf{Sign}_I$ to $\Sigma$-sentences over $\Set$. \\
%\(Mod\) & \textit{Model}: functor from the category of signatures $\mathsf{Sign}_I^{op}$ to $\Sigma$-models over $\Set$. \\
%$T$ & \textit{Interpreter}: functor from the category of learners \(\mathsf{Learn}\) to Cartesian streams $\Stream{\Set}$ over $\Set$. \\
%\bottomrule
%\end{tabularx}%
%}
%\end{table}







\section{Categorical Framework of Explainable AI}
\label{sec:framework}



We use feedback monoidal categories and institutions to formalize fundamental (X)AI notions.
% learning agents and the notion of ``explanation". 
% Here we formalize XAI structures and \stefano{semantics?} semantics using feedback monoidal categories and the category signatures $\mathsf{Sign}$. 
To this end, we first introduce the definition of ``abstract explaining learning agent'' (Section~\ref{sec:learning-agent}) as a morphism in free feedback monoidal categories, and 
% generated by a \emph{model}, an \emph{optimiser} and an \emph{explainer} morphisms
%``learning agent'' (Section~\ref{sec:learning-agent}) and ``explaining learning agent'' (Section~\ref{sec:x-learning-agent}) as morphisms
%in free feedback monoidal categories
% generated by a \emph{model}, an \emph{optimiser} and an \emph{explainer} morphisms
then we describe a functor instantiating this concept in the concrete feedback monoidal category of \(\Stream{\Set}\) (Section~\ref{sec:interpretation}). Intuitively, a \emph{free} category serves as a template for a  class of categories (e.g., feedback monoidals). To generate a free category, we just need to specify a set of objects and morphisms generators. Then we can realize ``concrete'' instances of a free category $\cat{F}$ through a functor from $\cat{F}$ to another category $\cat{C}$ that preserves the axioms of $\cat{F}$ (cf. \Cref{app:free}). 


%If such a functor exists then $\cat{C}$ is of the same type of $\cat{F}$ (e.g., the image of a free feedback monoidal category via a feedback functor is a feedback monoidal category).







%\note{add free are templates for any LA (cf. \Cref{app:free})}
%Then we describe a functor translating these abstract concepts into concrete instances in the feedback monoidal category of \(\Stream{\Set}\) (Section~\ref{sec:interpretation}). 




% \subsection{Syntax of Learning Agents}
\subsection{Abstract Explaining Learning Agents}
\label{sec:learning-agent}

% Taking inspiration by related works like \citet{katsumata19} and \citet{cruttwell2022categorical} for gradient-based learning systems, and \citet{wilson2021reverse} for Boolean Circuits, 
We formalize the abstract concept of an explaining learning agent drawing inspiration from ~\citep{cruttwell2022categorical, katsumata19, wilson2021reverse}.
At a high level, learning can be characterized as an \emph{iterative process with feedback}. This process involves a function (known as \emph{model} or \emph{explainer} in a XAI method) which updates its internal states (e.g., a set of \emph{parameters}) guided by some feedback from the environment (often managed by an \emph{optimizer}).
Formally, we define an abstract explaining learning agent as a morphism in the free feedback Cartesian monoidal category $\mathsf{XLearn}$ generated by: 
\begin{itemize}
    \item the objects $X,Y, P$, and $E$ representing input, output, parameter, and explanation types;
    \item the \textit{model/explainer} morphism $\eta: X \times P \rightarrow Y \times E$ which produces predictions in $Y$ and explanations in $E$;
    \item the \textit{optimizer} morphism \(\nabla_Y \colon Y \times Y \times P \to P\), which produces updated parameters in \(P\) given supervisions in \(Y\), model/explainer predictions in \(Y\), and parameters in \(P\).   %through its explanations in $E$.
    %\item the two morphisms \(\nabla_Y \colon Y \times Y \times P \to P\) (as referred before), and $\nabla_E: E \times E \times P \rightarrow P$ to optimize the agent parameters through its explanations in $E$.
\end{itemize}
% \begin{wrapfigure}[3]{r}{0.45\textwidth}
% \vspace{-0.2cm}
% % Figure removed    
% \end{wrapfigure}
%\begin{itemize}
%    \item the three objects $X,Y$ and $P$ representing the input, output and parameter space of the agent, respectively;
%    \item a morphism  \(g \colon X \times P \to Y\) representing a model, which  produces outputs in \(Y\) given inputs in \(X\) and parameters in \(P\);
%    \item a morphism \(\nabla_Y \colon Y \times Y \times P \to P\) representing an optimizer, which produces updated parameters in \(P\) given supervisions in \(Y\), the model predictions in \(Y\) and the parameters in \(P\).
%\end{itemize}
% In order to specify the syntax of abstract learning agents we define the free category $\mathsf{Learn}$ of abstract models and optimizers.
%\begin{definition}
% [Abstract model and optimizer]
%We call $\mathsf{Learn}$ the free feedback Cartesian category generated by three objects \(X,Y\) and \(P\), and by two morphisms \(g \colon X \times P \to Y\) and  \(\nabla_Y \colon Y \times Y \times P \to P\).
%\[\learnModel \quad \learnOptimizer\]
%% Figure environment removed
%\centering{
%% Figure removed}
%\end{definition}
%\begin{remark}
%We use the same object's symbol whenever we require conceptually different inputs/outputs to belong to the same space.
%This is also what happen in classic machine learning, e.g. when an optimizer tries to match the predictions of a neural model and the labeled samples belonging being both represented as vectors in $\mathbb{R}^n$.  
%\end{remark}
%%%%%%%%%%
% Having defined the free category for abstract models and optimizers, we formalize an abstract learning agent as a particular morphism in $\mathsf{Learn}$.
%\begin{definition}
% [Abstract learning agent]
%An \emph{abstract learning agent} is the morphism in $\mathsf{Learn}$ given by the morphisms' composition  $\fbk[P]\left((\id{Y}\times\id{X}\times\nu_P);(\id{Y}\times g \times \id{P});\nabla_Y\right)$, which corresponds to the following string diagram:
% $(\id{Y} \times g) \dcomp \nabla)$:
% $\fbk[P]((\id{Y} \times g) \dcomp \nabla)$
%\[\learnCat\]
% \fg{I think the full dot should be before the split with $g$ and I'll represent somehow the $\fbk[P]$?} 
%\end{definition}
% \todo{do we need this proposition?}
% \begin{proposition}
% % \elena{Consider whether we want to do this, maybe with the Para construction?}
% Learning agents form a feedback monoidal category. 
% \end{proposition}
% \subsection{Syntax of Explaining Agents}
%\subsection{Abstract Explaining Agents}
%\label{sec:x-learning-agent}
%In a similar fashion, we may provide an informal identikit of what makes a learning agent explainable, or more specifically which could be the main features of an explainable/explaining AI model. The main difference with respect to a basic learning agent is that we expect an explaining agent to manipulate an extra object, called \textit{explanation}, which can be itself subject to optimization. Hence, we define an abstract explaining agent as a morphism in the free feedback monoidal category $\mathsf{XLearn}$ generated by:
%\begin{itemize}
%    \item the four objects $X,Y,P$ (as referred before), and $E$ representing the explanation space/type;
%    \item a morphism $\eta: X \times P \rightarrow Y \times E$ called \textit{explainer}, that produces both predictions in $Y$ and explanations in $E$;
%    \item the two morphisms \(\nabla_Y \colon Y \times Y \times P \to P\) (as referred before), and $\nabla_E: E \times E \times P \rightarrow P$ to optimize the agent parameters through its explanations in $E$.
%\end{itemize}
% \begin{definition}
 % [Abstract explainer and optimizer]
\begin{definition}
$\mathsf{XLearn}$ is the free feedback Cartesian category generated by the objects $X,Y,P$ and $E$, and by the morphisms $\eta: X \times P \rightarrow Y\times E$ and $\nabla_{Y}: Y \times Y \times P \rightarrow P$.
% % Figure environment removed
\end{definition}
% \begin{wrapfigure}[3]{r}{0.45\textwidth}
% \vspace{-0.2cm}
% % Figure removed    
% \end{wrapfigure}
% and
     %$\nabla_E: E \times E \times P \rightarrow P$.   
%\[\learnExplainer \quad \learnOptimizerX \quad \learnOptimizerExplanations \]
 % \end{definition}
 %\begin{center}
%% Figure removed\end{center}
% \begin{remark}
The introduction of these morphisms allows us to establish a formal definition of an abstract learning agent that produces explanations\footnote{It is worth noting the object symbols used here solely represent the type of an object (e.g., a specific vector space). Consequently, objects represented by the same symbol may still have a different content.}.
% This is also what happen in classic machine learning, e.g. when an optimizer tries to match the predictions of a neural model and the labeled samples belonging being both represented as vectors in $\mathbb{R}^n$.  
% \end{remark}
 %\begin{definition}
 % [Abstract explainer and optimizer]
     %We call $\mathsf{XLearn}$ the free feedback Cartesian category generated by four objects $X,Y,P$ and $E$, and by three morphisms $\eta: X \times P \rightarrow Y\times E$, $\nabla_{Y}: Y \times Y \times P \rightarrow P$ and
     %$\nabla_E: E \times E \times P \rightarrow P$.
%% Figure removed     
%\[\learnExplainer \quad \learnOptimizerX \quad \learnOptimizerExplanations \]
% \end{definition}
\begin{definition}%[Abstract explainable learning agent]
An \emph{abstract explaining learning agent} is the morphism in $\mathsf{XLearn}$ given by the morphisms' composition: 
% \begin{equation}
% \label{eq:AXLA}
% {\scriptstyle
    $\fbk[P]\left((\id{Y\times X}\times\nu_{P});(\id{Y}\times \eta \times \id{P}); (\id{Y\times Y}\times\discard_{E}) ;\nabla_{Y}\right)$
% }
% \end{equation}
% $\fbk[P]((\id{Y} \times \eta ) \dcomp \nabla)$:
%\[\xLearnCat\]
\begin{center}% Figure removed\end{center}


%or % by the composition 
%${\scriptstyle \fbk[P]\left((\id{E\times X}\times\nu_{P});(\id{E}\times \eta \times \id{P}); (\id{E}\times \swap{Y,E\times P});(\nabla_{E}\times \discard_Y)\right)}$
% $\fbk[P]((\id{E} \times \eta ) \dcomp \nabla_E)$:
%\[\xLearnCatExplanations\]
% \[\fbk[P]((\id{E} \times \eta ) \dcomp \nabla_E).\]
% \[\xLearnCat\]
\end{definition}
%\fg{Tenere solo def 3.5, mettere remark nell'istanziazione per il fatto che si può fare gradOutput e gradE. e anche il learning agent viene fori solo come concreto, con $E=\emptyset$}
% \todo{add definition of agreement?}
% \begin{proposition}
% % \elena{Consider whether we want to do this, maybe with the Para construction?}
% Explainable learning agents form a feedback monoidal category. 
% \end{proposition}
% Notice how in our formalism ``explaining'' is not necessarily a dynamic process as learning. In general, explaining is a static process if the explainer is detached from the translator. Thus, in our framework the primary function of the translator is to make the explaining process dynamic by mapping explanations $\mathcal{E}$ into outputs $Y$ which an optimizer $\nabla$ can process to update the parameters $P$.

% \begin{remark}
%     To be more general, we defined the free categories of $\mathsf{Learn}$ and $\mathsf{XLearn}$ by using different object types $X$, $Y$, $P$. 
%     However, we can always define a unified free category with an explainer working on the same input/output type of a learning agent (as often happens in the practice). To generate this category we can use the objects $X,Y,P,E$ and the morpisms $g:X\times P\rightarrow Y$, $\eta:X\times P\rightarrow Y\times E$, $\nabla_Y:Y\times Y\times P\rightarrow P$, $\nabla_E:E\times E\times P\rightarrow P$.
% \end{remark}



\subsection{Concrete Learning and Explaining Agents}
% \subsection{Semantics of Explainable AI}
\label{sec:interpretation}
The free category $\mathsf{XLearn}$ allows us to highlight the key features of explaining learning agents from an abstract perspective. However, we can instantiate explaining learning agents in ``concrete'' forms using a feedback functor from the free category $\mathsf{XLearn}$ to the category of Cartesian streams over \(\Set\), i.e. \(\Stream{\Set}\).
Using this functor, we can establish a mapping from our abstract construction to any conceivable concrete setting, involving diverse explainers (e.g., decision trees, logistic regression), input data types (e.g., images, text), outputs, parameters, or explanations. Achieving this mapping simply requires the definition of a specific functor we call \emph{translator}.
\begin{definition}
% [Translators] 
An \emph{agent translator} is a  feedback Cartesian functor  $\mc{T}: \mathsf{XLearn} \rightarrow \Stream{\Set}$.
\end{definition}
Among translators, we distinguish two significant classes: those that instantiate learning agents (\Cref{def:concrete-la}) and those that instantiate explainable learning agents (\Cref{def:xla}). Intuitively, a concrete learning agent is an instance of an abstract learning agent that does not provide any explanation while a concrete explaining learning agent does output an (non-empty) explanation.
%Roughly, translators take the abstract representation of learning/explaining problems, and then return a fixed input space, output space, model, explanation, and so forth. In particular, we are interested in the realization  of concrete learning agents by means of translator functors.  
% Agent translators instantiate the abstract representation of learning problems into a fixed input space, output space, model, explanation, and so forth. %In particular, we are interested in the realization  of concrete learning agents by means of translator functors.
% Within the broad class of translators, we can highlight two classes of functors: namely, those that instantiate learning agents and explainable learning agents.
%\begin{itemize}
%\item[]
%    \item A functor \textbf{$\mc{T}$} from $\mathsf{Learn}$ to \(\Stream{\Set}\) is called agent translator.
%    \item Given an institution $I$ and a signature $\Sigma\in\cat{Sign}_I$, a functor $\mathrm{T}_{\Sigma}$ from $\mathsf{XLearn}$ to \(\Stream{\Set}\), with $\mathrm{T}_{\Sigma}(E)= (Sen(\Sigma),Sen(\Sigma),\ldots,)$, is called an explaining agent translator.
%\end{itemize}
% In facts, we can map abstract objects and morphisms from $\mathsf{Learn}$ and $\mathsf{XLearn}$ to concrete streams of sets and functions. 
\begin{definition}\label{def:concrete-la}
% [Concrete learning agent]
Let $\mc{T}$ be an agent translator such that $\mc{T}(E) = \{*\}^{\mathbb{N}}$, where $\{*\}$ is a singleton set. % the one element set is the monoidal unit for \times (using the empty set would make everything collapse because \emptyset \times X = \emptyset)
A \textit{learning agent (LA)}, is the image $\mc{T}(\alpha)$ being $\alpha$ the abstract learning agent.
% ${\displaystyle \alpha = \fbk[P]\left((\id{Y}\times\id{X}\times\nu_P);(\id{Y}\times g \times \id{P});\nabla_Y\right)}$.
% \todo{not sure how to frame this one...}
% \textit{Human agent} A human agent is a concrete learning agent whose objects are in \(\Set\) and whose morphisms are functions in the human brain.
\end{definition}
The set $\{*\}^{\mathbb{N}}$ denotes the neutral element of the  monoidal product
in $\Stream{\Set}$
and conveys the absence of explanations. In this case, $\mc{T}(\eta)$ will be simply called \textit{model}.
and we will remove the explicit dependence on $\{*\}$ in the output space as $T(Y)\times\{*\}^{\mathbb{N}}\iso T(Y)$.
To instantiate explaining learning agents instead, we introduce two distinct types of translators: the semantic and the syntactic translator. This choice is motivated by the foundational elements of institution theory, namely sentences and models: Sentences correspond to well-formed \emph{syntactic} expressions, while models capture the \emph{semantic} interpretations of these sentences~\citep{goguen1992institutions, tarski1944semantic}. We refer to concrete instances of both syntactic and semantic explaining learning agents as ``explaining learning agents'' (XLA).
\begin{definition}
\label{def:xla}
% [Concrete learning agent]
Let $\mc{T}$ be an agent translator, $I$ an institution and $\alpha$ the abstract learning agent. The image $\mc{T}(\alpha)$ is said a \textit{syntactic explaining learning agent} if $\mc{T}(E) = Sen(\Sigma)^{\mathbb{N}}$ and a \textit{semantic explaining learning agent} if $\mc{T}(E) = Mod(\Sigma)^{\mathbb{N}}$, for some signature $\Sigma$ of $I$.
\end{definition}
% We will refer to both concrete syntactic and semantics explaining learning agents as \emph{explaining learning agents (XLA)}. In practice, a concrete learning agent is an instance of an abstract learning agent that does not provide any explanation while a concrete explaining learning agent does output an explanation. In a real-word scenario we are interested in those kinds of explanations which are ``understandable'' for a certain target. To capture this idea, we focus on syntactic and semantics explaining learning agents, which provide explanations using the formalism of institution theory. 
%This distinction allows to represent a broad class of learning process under the scope of a prev
%For instance, to represent a standard classification problem with $m$-classes, we may set up a translator $T$ such that $T(X)=\mathbb{R}^n$, $T(Y)=[0,1]^m$, $T(g)$ being an MLP, $T(P)=\mathbb{R}^p$ the space of the MLP weights and $T(\nabla_Y)$ the Adam optimizer. 
The high degree of generality in this formalization enables the definition of  any real-world learning setting and learning agent (to the author knowledge). Indeed, using Cartesian streams as the co-domain of translators, we can effectively model a wide range of learning use cases, including (but not limited to) those involving iterative feedback. To simplify the notation, in the following sections we use the shortcuts: $\mathcal{X} = \mc{T}(X)$, $\mathcal{Y} = \mc{T}(Y)$, $\mathcal{P} = \mc{T}(P)$, $\mathcal{E} = \mc{T}(E)$, $\hat{\eta}=\mc{T}(\eta)$  and $\hat{\nabla}_{\mathcal{Y}}=\mc{T}(\nabla_Y)$.  



\section{Impact on XAI and Key Findings}

% Finally, we formalize the notion of ``explanation'' as a $\Sigma$-theory, for some signature $\Sigma$ (Section~\ref{sec:syExp}). %and of ``understanding'' as a signature morphism within an institution (Section~\ref{sec:syExp}).


\subsection{Finding \#1: Our framework models existing learning schemes and architectures}\label{sec:finding1}
As a proof of concept, in the following examples we show how the proposed categorical framework makes it possible to capture the structure of some popular learning algorithms such as deep neural networks.
% Starting from the common representation of abstract learning agents, translator functors allow us to model different types of real-world learners, including gradient-based AI agents. 
% Indeed, the inherent generality of this formalization allows the definition of (almost) any typical learning setting and learning agent. 
% Classic learning algorithms, like deep neural networks or recurrent neural networks, generally rely on multiple-steps of backpropagation, possibly through time and applied to sequential samples. 
% For this reason we decided to rely on $\mathsf{Stream_{Set}}$, i.e. a feedback category of Cartesian streams, as codomain for translators. Indeed, $\mathsf{Stream_{Set}}$ is sufficiently general to accomodate all of these learning use cases, as we will see in the next examples.
% %%%
%%%


% Using the translator functor $\mc{T}$, we can also describe some of the most common learning paradigms in AI such as supervised and unsupervised learning.
%\fg{vedi come fare, se a elenco di framework tipo questi o a esempi secchi}
\begin{example}
%[classic supervised learning with streams]
\label{ex:mlp}
A classic multi-layer perceptron (MLP,~\citep{rumelhart1985learning}) classifier with Adam optimizer~\citep{kingma2014adam} can be seen as an instance of an abstract explaining learning agent whose translator functor is defined as follows (\Cref{fig:LA}):
    $\mc{X}= (\mathbb{R}^n)^{\mathbb{N}}$, $\mc{Y}=\left([0,1]^m\right)^{\mathbb{N}}$, $\hat{\eta}_i$ being an MLP for all $i$, $\mc{P}$ the space of the MLP parameters, e.g. $\mc{P}=(\mathbb{R}^p)^{\mathbb{N}}$, $\hat{\nabla}_{\mc{Y}_i}$ being e.g. the Adam optimizer, and $\mc{E} = \{*\}^{\mathbb{N}}$.
\end{example}
% Figure environment removed
In Example \ref{ex:mlp} we successfully model an MLP by constraining the components of the morphism $\eta$ to have constant values $\hat{\eta}_i = \text{MLP}$ (independent of the first $i-1$ inputs). However, by removing this constraint, we can instantiate a broader class of learning agents including e.g. recurrent neural networks~\citep{hochreiter1997long, hopfield1982neural} and transformers~\citep{vaswani2017attention}. In these scenarios, the neural functions become dependent on previous inputs, effectively capturing the input stream. Additionally, we can also model learning settings where a model's architecture changes over time, as in neural architecture search~\citep{elsken2019neural}.
\begin{example}
\label{ex:nas}
% [Neural Architecture Search]
A classical Neural Architecture Search algorithm~\citep{elsken2019neural} 
% with Adam optimizer 
can be seen as an instance of an abstract explaining learning agent whose translator functor is defined as follows:
    $\mc{X}= (\mathbb{R}^n)^{\mathbb{N}}$, $\mc{Y}=([0,1]^m)^{\mathbb{N}}$, $\hat{\eta}_i = \text{MLP}_i$ being a different neural architecture for every step, $\mc{P}$ the space of the MLPs parameters, e.g. $\mc{P}=(\mathbb{R}^p)^{\mathbb{N}}$, $\hat{\nabla}_{\mc{Y}_i}$ being the Adam optimizer, and $\mc{E} = \{*\}^{\mathbb{N}}$.
\end{example}
% \begin{example}[Learning Paradigms]
% \textit{Supervised Learning. } Supervised Learning is a concrete learning process whose morphisms are functions in \(\Set\), whose objects are in \(\Set\), and whose optimizer updates the parameters $P$ such that the output $Y$ is close to the input labels $Y$.
% \textit{Unsupervised Learning. } Unsupervised learning is a concrete learning process whose morphisms are functions in \(\Set\), whose objects are in \(\Set\), and whose input $Y$ is empty.
% \end{example}

% \begin{example}[Gradient-based Learning Agent]
% Using $\mc{T}$, we can also model the categorical definition of gradient-based learning proposed by~\citet{cruttwell2022categorical} as a special case:
% % , which includes some of the most studied objects in explainable AI i.e., neural networks~\citep{lecun2015deep}.
% A gradient-based learning agent is an AI agent whose optimizer generates parameters updates using the gradient of a differentiable loss function over the parameters.
% % of type $\mathcal{L}(Y,g(X))$ w.r.t. the current parameter states $P_t$ i.e., $P_{t+1} = P_{t} - \alpha \frac{\partial \mathcal{L}(Y,g(X))}{\partial P_t}$.
% \end{example}


%\begin{example}[Gradient-based Learning Agent]
    %This definition can be seen a special case of an instantiation of $\mathsf{Learn}$ through a translator $\mc{T}$, so as the category $\mathsf{ParaLens}$ they refer to is a special case of $\mathsf{Stream}_{\mathsf{Set}}$ category \todo{provide reference for a theorem, or change the sentence} describing only one step of the training. For what concern string diagrams representation, we notice that our diagram represent a generalization of the one used in~\citet{cruttwell2022categorical}, where both the optimizer and the loss function are included in the $\nabla_Y$ morphism. 
    % (see Fig. \ref{fig:gblavsala}).
    % % Figure environment removed

%\end{example}


%%% da adeguare a seconda dell'esempio
%As we can see from the previous examples, Cartesian streams offer a precise framework to represent the dynamic process of learning.
%Indeed, we can fully describe the learning process of a concrete agent which keeps updating its parameters until it eventually reaches a stationary state. 
%Given a learning agent, this process is represented by the image $T(P)$ through the translator functor $T$.
%A learning process is convergent if there exist $k$ such that the Cartesian stream has $\hat{\eta}_{n+1} = \hat{\eta}_n| (\mc{X}_n \times \mc{P}_n)\times \cdots \times (\mc{X}_0 \times \mc{P}_0)$, 
%$\mc{X}_{n+1} = \mc{X}_n$ and $\mc{P}_{n+1} = \mc{P}_n$ for $n > k$ \elena{\(\mc{P}_n\) è il tipo dell'output, non l'output, giusto? non riesco tanto a parsare quello che c'è scritto}.





%In the same way we can define concrete explaining learning agents. However in this case the mapping is carried out by an agent translator $T_{A}$, which maps the object $E$ for explanations into the set of sentences of a specific signature $\Sigma$ in a certain institution.


%\begin{definition}
% [Concrete explaining agent]
%Given an explaining agent translator $\mathrm{T}_{\Sigma}$ between $\mathsf{XLearn}$ and $\mathsf{Stream_{Set}}$, we call \textit{concrete explaining learning agent}, or simply \textit{explaining learning agent (XLA)}, the image $\mathrm{T}_{\Sigma}(\alpha)$ of the abstract explainable agent, where:
%${\scriptstyle \alpha=\fbk[P]\left((\id{Y\times X}\times\nu_{P});(\id{Y}\times \eta \times \id{P}); (\id{Y\times Y}\times\swap{E,P}) ;(\nabla_{Y}\times \discard_E)\right)}$ or 
%${\scriptstyle \alpha=\fbk[P]\left((\id{E\times X}\times\nu_{P});(\id{E}\times \eta \times \id{P}); (\id{E}\times \swap{Y,E\times P});(\nabla_{E}\times \discard_Y)\right)}$
% \[
% \begin{array}{l}
% \fbk[P]((\id{Y\times X}\times\nu_{P});(\id{Y}\times \eta \times \id{P});\\
% ;(\id{Y\times Y}\times\swap{E,P}) ;(\nabla_{Y}\times \discard_E))
% \end{array}
% \]
% \todo{not sure how to frame this one...}
% \textit{Human agent} A human agent is a concrete learning agent whose objects are in \(\Set\) and whose morphisms are functions in the human brain.
%\end{definition}


%\begin{remark}
The proposed formalism is general enough to encompass learning processes that produce explanations and optimize them indirectly through a standard optimizer $\hat{\nabla}_{\mathcal{Y}}$ applied to the explainer's predictions and reference labels of type $\mathcal{Y}$. However, we can also  optimize explanations directly by applying the optimizer to the explanations themselves, treating them as regular model outputs. To this end, we can choose a suitable translator functor $\mc{T}$ such that $\mc{T}(Y)$ represents the space of explanations. We also remark that not all (X)AI methods require parameter updating. In such cases, the parameter updating process can be represented by an identity morphism i.e., by setting $\mc{T}(\nabla_{Y})=\epsilon_{\mathcal{Y}}\times\epsilon_{\mathcal{Y}}\times\id{\mathcal{P}}$. 
% i.e. is the identity over the parameters space $\mc{T}(P)$.
%\end{remark}
%\stefano{controllare l'ultima frase}

% \begin{itemize}
%     \item we need now to go from abstract categories to concrete categories
%     \item first we define an interpreter as a functor mapping free cat in concrete cat
%     \item then we can start discussing: learning paradigms (supervised vs unsupervised), human vs AI learner, gradient-based learner, classifiers\&regressors, etc
% \end{itemize}

% \todo{The following are examples}

% \elena{All the following definitions should be given in terms of the shape of the semantics functor}

% \begin{definition}[Adversarial Learning]
% Adversarial learning is a learning process where the optimizer updates the parameters $P$ such that the predictions $B$ are far from the input supervisions $B$.
% \end{definition}

% Figure \ref{fig:learning_agent} formally describes the category of learning agents \(\Learn{\cat{Stream}}\) by means of its string diagrams. Figure \ref{fig:learning_agent_composition} formally describes the composition of learning agents in series and in parallel.
% The composition of learning agents is still a learning agent. The composition of models is still a model. A model can be a learning agent by itself (with its own internal model and optimizer).
% % Figure environment removed
% % Figure environment removed

% \todo{The following are examples}

% \elena{I am not sure what \(\Learn{\Stream{}}\) should be}

% \begin{example}[Learning Classifier]
% A learning classifier is a concrete learning agent where supervisions and predictions are objects $A,B \in [0,1]^k$.
% \end{example}


% From objects to concepts 

% \section{``Explain'' the Explainable AI Literature}


% Add:
% \begin{itemize}
%     \item agreement (concordant explanations): $\frac{\partial P}{\partial t} \rightarrow 0$ with $\nabla_E$
% \end{itemize}
% \mike{Please consider to include a small introduction to explain the difference of an explainable agent and explainable method, so the reader can know what will read in the section 3 and 4. Right now the manuscript is a bit confusing as you start immidiatly with literature review and terminology. As i was reading the manuscript I was expected to read about XAI methods and generalized terminology. In section 3 that you speak about the XAI agents, i was thinking about the XAI methods which you explain in 4 and this confused me a lot. Just include a small paragraph of 3 sentences that explain the XAI framework with the XAI agency, the method and the communication for the explanation before start the terminology (a scheme will be even better!)}


\subsection{Finding \#2: Our framework enables a formal definition of ``explanation''}
\label{sec:syExp}
% Current definitions consider explanations as ``answers to \textit{why?} questions''~\citep{miller2019explanation}. 
Our theoretical framework allows us to provide the first formal definition of the term ``explanation'', which embodies the very essence and purpose of explainable AI.
Our formalization goes beyond a mere definition of ``explanation'', as it highlights a natural distinction between different forms of explanations. Indeed, institution theory (and translator functors) allows a straightforward characterization of syntactic and semantic explanations. While both forms of explanations are prevalent in the current XAI literature, their distinctions are often overlooked, thus limiting a deep understanding of the true nature and intrinsic limitations of a given explanation.
\begin{definition}
% [Explanation]
    Given an institution $I$, an object $\Sigma$ of $\cat{Sign}_I$, and a concrete explainer $\hat{\eta}=\mc{T}(\eta):\mc{X}\times \mc{P}\rightarrow \mc{Y}\times \mc{E}$,
    % over the input $\mc{X}$ and parameters $\mc{P}$, 
    an \emph{explanation} $\mc{E} = \mc{T}(E)$ in a language $\Sigma$ is a set of $\Sigma$-sentences (\emph{syntactic} explanation) or a model of a set of $\Sigma$-sentences 
    % (i.e. a $\Sigma$-model) 
    (\emph{semantic} explanation). 
\end{definition}
%Thus explanations are concrete instances of abstract sentences (i.e $\Sigma$-sentences).
We immediately follow up our definition with a concrete example to make it more tangible.
\begin{example}
\label{ex:logsig}
    Let $I_{PL}$ be the institution of Propositional Logic and $\Sigma$ a signature of $I_{PL}$ such that $\{x_{\textit{flies}},x_{\textit{animal}},x_{\textit{plane}},x_{\textit{dark\_color}},\ldots\}\subseteq\Sigma$ and with the standard connectives of Boolean Logic, i.e. $\neg,\wedge,\vee,\rightarrow$. For instance, $\hat{\eta}$ could be an explainer aiming at predicting an output in $\mc{Y}=\{x_{\textit{plane}},x_{\textit{bird}},\ldots\}$ given an input in $\mc{X}$.    
    Then a syntactic explanation could consist of a $\Sigma$-sentence like 
    % $\{x_{\textit{plane}}\rightarrow p_{\textit{flies}}$, $p_{\textit{flies}} \wedge \neg p_{\textit{animal}}$, $
    $\varepsilon = x_{\textit{flies}}\wedge\neg x_{\textit{animal}}\rightarrow x_{\textit{plane}}$, a semantic explanation could be the truth-function of  $\varepsilon$.
    % while a model of $\varepsilon$ e.g., % Figure removed, is a possible semantic explanation. 
    % Having established the theoretical framework of learning agents, we can now proceed to formally define the fundamental notion of ``explanation''. 
% , properly of the XAI research field. %such as ``explanation''. %and ``understanding''.
% \paragraph{What is an explanation?}
% So far we getting started with explanations (i) from an abstract perspective as an output object produced by an explainer morphism $\eta$, depending on its input $X$ and/or parameters $P$, (ii) more concretely as the mapping of an explaining agent translator into a set of sentences or models in a certain signature. 
% We are now interested in analyzing the properties of this special object.
% According to the introduced definition, an explainable learning agent distinguishes from a learning agent as it also generates a special object $E$, called ``explanation''. 
% In our categorical framework, an explanation is an output of the explainer $\eta$, depending on its input $X'$ and/or parameters $P'$. %~\citep{Das2020OpportunitiesAC}.
% A typical explanation can thus describe a causal relationships between events. Humans typically disambiguate events in ``causes'' and ``effects'' such that whenever the ``causes'' occur, then the ``effects'' must follow~\citep{todo}. 
% In our formalism all objects  the learning agent parameters $P$, the data $X$ are the ``causes'' while the agent predictions $Y$ are the ``effects''.
% \todo{change notation for single explanation and explanation object type}
    % \} 
    % \fg{Qui siamo ancora a livello simbolico, vedi ex nella 3.4} 
    %Aspe domanda ma noi stiamo richiedendo che la nostra spiegazione E sia contemporaneamente una sigma-sentence e anche un oggetto della cat instanziata. Non mi suona molto bene la cosa
    % In the category of sets, consider the input sample $x = \{beak,wing,trunk\}$, the parameters $p = \{relevant, relevant, irrelevant\}$, then an explanation could be the sentence $(beak,relevant) \wedge (wing,relevant)$.
\end{example}
% \begin{example}
%         Let assume  $\Sigma$ to be a signature of Propositional Logic 
%     with $\{x\} = \textit{VAR}$, being $\textit{VAR}$ the set of propositional variables, a set of relations $R = \{Files(x), Animal(x), Plane(x), \dots\}$ and the standard connectives of Boolean Logic, i.e. $\neg,\wedge,\vee,\rightarrow$. For instance, $\hat{\eta}$ could be an explainer aiming at predicting an output in $\mc{Y}=\{\textit{plane}\}$ given an input in $\mc{X}$.    
%     Then an explanation could consist in a $\Sigma$ sentence, like 
%     % $\{\textit{Plane}(x) \rightarrow p_{\textit{flies}}$, $p_{\textit{flies}} \wedge \neg p_{\textit{animal}}$, $
%     $\varepsilon = Files(x) \wedge\neg Animal(x)\rightarrow Plane(x)$.
% \end{example}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%







% \begin{remark}
Our definition of explanation not only generalizes and formalizes existing definitions in the literature but also encompasses key aspects discussed by different researchers, such as: {\small\textit{``An explanation is an answer to a `}why?\textit{' question''~\citep{miller2019explanation}}, \textit{``An explanation is additional meta information, generated by an external algorithm or by the machine learning model itself, to describe the feature importance or relevance of an input instance towards a particular output classification''~\citep{Das2020OpportunitiesAC}} or \textit{``An explanation is the process of describing one or more facts, such that it facilitates the understanding of aspects related to said facts''}~\citep{palacio2021xai}}. In fact, our definition of explanation can provide (i) additional meta information to describe properties related to the explainer, or (ii)  the feature relevance of an input, or (iii) an answer to why an input instance leads to a specific output. Furthermore, \citet{tarski1944semantic} and~\citet{goguen1992institutions} proved how the semantics of ``truth'' is invariant under change of signature. This means that we can safely use signature morphisms to 
switch from one ``notation'' to another, inducing consistent syntactic changes in a $\Sigma$-sentence without conditioning the ``meaning'' or the ``conclusion'' of the sentence~\citep{goguen1992institutions}. As a result, signature morphisms can translate a certain explanation between different signatures, hence paving the way to study ``communication" as well as ``understanding" between different XLAs.  
% However, we leave an in depth investigation of these fundamental concepts to future work.
% \end{remark}
% As a simple example of a $\Sigma$-sentence in natural language is ``All men are mortal. Socrates is a man.
% Therefore, Socrates is mortal.''. 
% Using a functor from natural language to first-order logic, we can write the syllogism as ``$\forall x\ \text{man}(x) \rightarrow \text{mortal}(x)$, $\text{man}(Socrates) \Longrightarrow \text{mortal}(Socrates)$'', without changing its validity.
% \todo{PIANO: esempio della rilevanza+tarski e co + def comunicazione tra explainer? + esempio conversione di spiegazioni.. .e quindi understanding!}
%\begin{example}
%\label{ex:logrel}
%Let us assume a signature $\Sigma'$ with a vocabulary extending the one in Example \ref{ex:logsig} with two additional symbols $R=\{\textit{relevant}, \textit{irrelevant}\}$. We consider as sentences in this language, expressions of kind $(x_1:r_1,\ldots,x_n:r_n)$, where $x_i\in \textit{VAR}$ and $r_i\in R$ for $n\in\mathbb{N}$, $i=1,\ldots,n$, . Then an explanation for $\hat{\eta}$, as defined in Example \ref{ex:logsig}, could consist in a $\Sigma'$ sentence like $\varepsilon'=(x_{\textit{flies}}:\textit{relevant},x_{\textit{animal}}:\textit{relevant},x_{\textit{dark\_color}}:\textit{irrelevant})$.
%\end{example}
%\stefano{Non credo che l'esempio sia corretto, gli oggetti della signature della istituzione della logica proposizionale sono le variabile e relevant/irrelevant non lo sono}
 



%%%%%%work on it for the rebuttal
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% PIE IN CASO TAGLIA TUTTO DA QUA ALLA FINE DELLA SEZ
% \todo{TOCHECK!! and in case remove (I would say also the understanding part)}
% On the other hand, in our framework an explanation is tightly related to the specific inputs and outputs of an explainable agent, then it is fundamental for comparing explanations in different signatures, to define how two explainers  can ``communicate" between them.
% \begin{definition}[Explainers' communication]
% Given two explainer $\hat{\eta}_1:\mc{X}_1\times\mc{P}_1\rightarrow \mc{Y}_1\times\mc{E}_1$ and $\hat{\eta}_2:\mc{X}_2\times\mc{P}_2\rightarrow \mc{Y}_2\times\mc{E}_2$ with signatures $\Sigma_1$ and $\Sigma_2$ respectively, we say that $\hat{\eta}_1$ can communicate with $\hat{\eta}_2$ if it exists a function $\Phi:\mc{X}_1\times\mc{P}_1\times\mc{Y}_1\times\mc{E}_1\rightarrow \mc{E}_2$, such that $\Phi_{|\mc{E}_1}$ is a morphism between $Sen(\Sigma_1)$ and $Sen(\Sigma_2)$.
% \end{definition}
% \begin{example}
%     As an example of communication between explainable agents, and following Example \ref{ex:logsig} and \ref{ex:logrel}, we can consider $\Phi(x,p,y,\varepsilon)=(x_i:\textit{relevant},\ldots,x_j:\textit{irrelevant})$ if $x_i$s occur in $\varepsilon$ whereas $x_j$s do not.
% \end{example}
% \todo{To make more correct or delete it}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%






% \paragraph{What is understanding?}
% ``Understanding'' is another key notion, tightly connected to the concept of ``explanation'', which currently lacks a formalization in explainable AI. In this context, we are often interested in a specific type of understanding which~\citet{pritchard2009knowledge} refers to as \textit{understanding-why}. This form of understanding is often called \textit{explanatory understanding} and is ascribed in sentences that take the form ``I understand why Z'', where Z is an explanation (for example, ``I understand why the bread burnt as I left the oven on''). 
% % Notice how this informal description is tightly connected with the informal description of an explanation as ``an answer to a \textit{why?} question''~\citep{miller2019explanation}. 
% Using this intuition, we can formally define understanding as follows.
% % \todo{agreement vs understanding}
% % \todo{change to interpretation}
% %%%% TO FIX DEPENDING ON THE PREVIOUS PART
% \begin{definition}[Understanding]
%     An explainable learning agent providing explanations in a signature $\Sigma'$ can understand the explanation $E$ in the signature $\Sigma$ if and only if it exists at least one signature morphism $\phi: \Sigma \rightarrow \Sigma'$.
%     % from the signature $\Sigma$ of the explanation and the signature $\Sigma'$ of the agent.
% \end{definition}

% \stefano{Aggiungere uno o più esempi di traduzione e stressare il fatto che con il nostro framework le spiegazioni hanno dietro un potente background logico}

% \stefano{Con le istituzione la traduzione può essere fatta solo all'interno della singola istituzione altrimenti il concetto di verità non è preservato. Pensare ad un funtore tra istituzioni diverse? Preserva il concetto di verità? Da sviluppare più attentamente in una versione estesa sec me} \fg{immagino si possa fare tra "sotto"-instituzioni, tipo FOL e saliencymaps? l'idea del "funtore" tra istituzioni è anche figa, ma non avendo esempi di istituzioni molto diverse per ora lascerei perdere, e parlerei solo di quello che succede facendo riferimento a frammenti di stesse signatures al+}


% \begin{remark}
%     Notice that the existence of this morphism is not always guaranteed. This means that in some cases some learners, like even human observers, may not be able to understand certain AI explanations. This happens even among human beings talking in two different (natural) languages. In other situations, a partial morphism may exist allowing a form of partial understanding. This happens for example in translating natural languages to formal languages.
%     % Nevertheless, when two agents share the same ``language'', to allow for a kind of understanding to exist. \todo{add discussion on partial understanding e.g., from natural language to formal language}
% \end{remark}

% Another interesting research line in AI consists in studying multi-agents learning environments, with possibly the human in the loop.  
% According to our proposed formalization, for instance we can define  the notion of ``agreement'' between two explainers by co-optimizing their explanations until convergence, or evaluating their communicated explanations.
% % \begin{definition}[Agreement]
% %     Given two explainable learning agents $\hat{\eta}$ and $\hat{\eta}'$ providing explanations in a signature $\Sigma$ and $\Sigma'$ if and only if exists a communication function $\Phi$ between them, such that $\nabla_{E'}$ converges if provided $\mc{E}'$ and $\Phi(\mc{E})$ as inputs.
% % \end{definition}
% % \todo{a questo punto 1 explainer capisce l'altro quando tramite il morfismo glielo posso attaccare al $\nabla_E$ e mi da convergenza..?}



% % \todo{notice how interpretation depends on the meaning of the input $X$! Even decision trees and LR might not be interpretable if the input space is meaningless to a human observer}

\subsection{Finding \#3: Our framework provides a theoretical foundation for XAI taxonomies}
\label{sec:tax}
%\stefano{800 taxonomieeees}
Using 
% agent translators,
our categorical constructions, 
we can develop a theory-grounded taxonomy of XAI methods that goes beyond current ones and catalogues existing approaches in a systematic and rigorous manner. We recognize the importance of such a foundation due to the ongoing debates and challenges faced by current taxonomies in comprehensively encompassing all existing XAI methods. Indeed, existing approches in the XAI literature have only provided subjective viewpoints on the field, distinguishing methods according to controversial criteria such as: the scope/methodology/usage of explanations~\citep{Das2020OpportunitiesAC}; the how/what/why of explanations~\citep{palacio2021xai}; the relationship between explainers and systems being explained, e.g., intrinsic/post-hoc, model-specific/agnostic, local/global~\citep{molnar2020interpretable}; or the specific data types, such as images~\citep{kulkarni2022explainable}, graphs~\citep{li2022survey}, natural language~\citep{danilevsky2020survey}, and tabular data~\citep{di2022explainable}. However, these taxonomies lack a solid and grounded motivation and rely primarily on subjective preferences. As a result, they are unable to draw truly universal conclusions and provide a general understanding of the field. On the contrary, our taxonomy aims to fill this gap by providing a comprehensive classification of XAI methods that is grounded in our theoretical framework. As an example, we use the proposed categorical framework to explicitly describe the following macro-categories of XAI methods~\citep{Das2020OpportunitiesAC,molnar2020interpretable,palacio2021xai}: post-hoc and intrinsic methods, model-agnostic and model-specific methods, forward-based and backward-based methods, and even more recent approaches such as concept bottleneck models. 

% In the following, we show that our framework can effectively describe existing macro-classes of XAI methods (\Cref{sec:xai-structures}). Furthermore, we demonstrate that our framework offers valuable insights into overlooked aspects of explainable techniques, specifically the distinction between semantic and syntactic explanations (\Cref{sec:xai-semantics}).


% To this end, we first revise some of the most common  XAI methods (\Cref{sec:XAItax}) and discuss their connection with our  framework
% (\Cref{sec:xai-structures}). 
% Finally, we analyze common semantics of data and explanations (\Cref{sec:xai-semantics}). 
% has been removed for space reasons
% We then show how category theory proves useful to reason about XAI ``structures of structures'' (Section~\ref{sec:xai-structures-bis}).
% Finally, we briefly mention methods to evaluate XAI approaches  (Section~\ref{sec:xai-evaluation}).
% to generate a simple and sound categorical taxonomy of explainable AI.
% We will employ our taxonomy to describe XAI systems in terms of their categorical structure and semantics using the categorical framework developed in Section~\ref{sec:framework}. 
% The structure of our taxonomy is based on the three core elements described in Section~\ref{sec:framework} (ML models, explainers, and interpreters), while the semantics is based on the formal explanation semantics presented in Section~\ref{sec:framework}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \subsection{XAI Dimensions}
% \label{sec:XAItax}
% % Explainable AI (XAI) aims to satisfy the human need for accurate and trustworthy AI models~\citep{Das2020OpportunitiesAC}
% Seminal works in explainable AI include saliency maps~\citep{simonyan2013deep,selvaraju2017grad}, local surrogate models~\citep{ribeiro2016model}, rule-based models~\citep{breiman1984classification}, and more recently concept-based models~\citep{kim2018interpretability}. Some of these methods are so effective that their impact now deeply affects other research disciplines such as medicine~\citep{jimenez2020drug}, physics~\citep{schmidt2009distilling,cranmer2019learning}, and even pure mathematics~\citep{davies2021advancing}. 


% very often these taxonomies do not consider the whole research field, their notations clash with each other, and, even more importantly, they are not supported by a solid mathematical formalism, \alberto{preventing them from drawing truly universal conclusions. A POSSIBLE VERSION.}


% \subsection{Explainable AI}
% Dimensions of interest
% \begin{itemize}
%     \item Type of data
%     \begin{itemize}
%         \item images
%         \item tabular data
%         \item graph-structured data
%     \end{itemize}
%     \item Type of explanation
%     \begin{itemize}
%         \item concept-based: CBM, Rule-based, ...
%         \item Heat-map
%     \end{itemize}
%     \item Type of integration between Xer and Model
%     \begin{itemize}
%         \item Post-hoc
%         \item self-explainable
%         \item interpretable
%     \end{itemize}
%     \item Scope
%     \begin{itemize}
%         \item local
%         \item global
%     \end{itemize}
% \end{itemize}




% Being black or white box is user-dependent. White boxes are ``interpretable'' while black boxes require ``explanations'' to be interpreted.
% \begin{definition}[Explanation]
% Given the agents $f$ (model) and $g$ (user), an explanation is an answer to a why-question~\citep{miller2019explanation} or a simplified representation of the decision process of $f$ to meaningful concepts which are user-understandable and reasonable~\citep{Das2020OpportunitiesAC}.
% \end{definition}
% Informal qualities of good explanations:
% \begin{itemize}
%     \item short
%     \item language
%     \item fidelity
%     \item consistent with prior beliefs of the user
%     \item general (high support)
% \end{itemize}
% \begin{definition}[Explainable AI problem]
% Given the agents $f$ (model) and $g$ (user), build an agent $r$ (explainer) that explains the decisions of the model $f$ to the user $g$.
% \end{definition}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% \subsection{Grounding XAI Taxonomies on Cartesian Streams}
% \label{sec:xai-structures}

% We start the discussion with some of the main XAI structures according to existing 
% Here we show how the proposed theory can serve as a sound formalism for diverse XAI approaches. 

% As the main differences among these approaches do not concern the optimizer, we  will show for simplicity the portion of string diagram containing the XLA explainer.

% \mike{no example for local and global methods}
% The simplest explainable AI notions we can model with these structures are the key notions of ``interpretable'' and ``non-interpretable'' AI system.
% \begin{definition}[Non-interpretable system]
% A non-interpretable system is a model $f: X \times P \rightarrow Y$ which does not generate explanations $\mathcal{E}$.
% \end{definition}
% The peculiarity of non-interpretable systems is that their predictions $Y$ do not depend on explanations $\mathcal{E}$. Such systems are often opposed to interpretable and explainable systems:
% \begin{definition}[Interpretable system]
% An interpretable system is composition $\tau \circ \eta$ of an explainer $\eta\colon X \times P \rightarrow \mathcal{E}$ and an interpreter $\tau\colon \mathcal{E} \rightarrow Y$.
% \end{definition}
% \begin{definition}[Explainable system]
% An explainable system is composition $(\eta \circ f) \otimes f \colon X \times P \rightarrow \mathcal{E} \times Y$ of a non-interpretable model $f\colon X \times P \rightarrow Y$ and an explainer $\eta: f \rightarrow \mathcal{E}$.
% \end{definition}
% Notice how our categorization allows for a clear distinction between non-interpretable, interpretable, and explainable systems: (i) non-interpretable systems do not provide any sort of explanation $\mathcal{E}$, (ii) interpretable systems use explanations $\mathcal{E}$ to predict the task $Y$, while (iii) explainable systems do not use explanations $\mathcal{E}$ to predict the task $Y$. 

\paragraph{Post-hoc vs Intrinsic.}
XAI surveys currently distinguish between intrinsic and post-hoc explainers. Informally, the key difference is that intrinsic XAI methods evolve model parameters and explanations at the same time, whereas post-hoc methods extract explanations from pre-trained models~\citep{Das2020OpportunitiesAC, molnar2020interpretable}.

% \todo{change to calligraphic}
\textbf{Post-hoc explainer.}
Given a trained LA model $\hat{\mu}: \mathcal{X} \times \mathcal{P} \rightarrow \mathcal{Y}$, a post-hoc explainer is an XLA explainer $\hat{\eta}:\mc{X}'\times\mc{P}'\to\mc{Y}'\times\mc{E}$, such that $\mc{X}' = \mathcal{Y} \times \mathcal{X} \times \mathcal{P}$:
\begin{center}% Figure removed\end{center}

% \begin{wrapfigure}[2]{r}{0.25\textwidth}
% \vspace{-0.6cm}
   
% \end{wrapfigure}
% \begin{definition}
 % [Abstract explainer and optimizer]
 \textbf{Intrinsic explainer.} 
 An intrinsic explainer is an XLA explainer $\hat{\eta}$ whose input objects are parameters $\mc{P}$ and a set of entries of a database $\mathcal{X}$:
 
% Figure environment removed
%\begin{example}[Intrinsic explainer]
%\begin{center}% Figure removed\end{center}
%\[ \xaiIntrinsic \]
%\end{example}
%\[ \xaiPosthoc \]
%

Common intrinsic explainers are logic/rule-based~\citep{barbiero2023interpretable, breiman1984classification, ciravegna2023logic, friedman2008predictive, manhaeve2018deepproblog, schmidt2009distilling,yang2017scalable}, linear~\citep{doshi2015graph, hastie2017generalized,nelder1972generalized,santosa1986linear,tibshirani1996regression, verhulst1845resherches}, and prototype-based~\citep{fix1989discriminatory, kaufmann1987clustering} approaches; while well-known post-hoc explainers include saliency maps~\citep{selvaraju2017grad, simonyan2013deep}, surrogate models~\citep{lundberg2017unified,ribeiro2016should,ribeiro2018anchors}, and some concept-based approaches~\citep{espinosa2022concept,ghorbani2019interpretation,ghorbani2019towards,kim2016examples}.

% All the other explainable AI systems are just different flavours of the ones we described without any significant structural difference. This means that we can unify them all and study them using the same categorical structures we already defined. For example, in our taxonomy ``model-agnostic, model-specific, forward-based, and backward-based'' approaches~\citep{adadi2018peeking,Das2020OpportunitiesAC} all fall under the categorical structures of explainable systems without significant structural differences:
\paragraph{Model-Agnostic vs Model-Specifics}
Another common theme in XAI reviews is to differentiate model-agnostic and model-specific explainers. Intuitively, model-agnostic explainers extract explanations independently from the architecture and/or parameters of the model being explained, whereas model-specific explainers depend on the architecture and/or parameters of the model.

% \begin{wrapfigure}[3]{r}{0.30\textwidth}
% \vspace{-0.4cm}
% % Figure removed    
% \end{wrapfigure}
\textbf{Model-agnostic explainer. }
Given an LA model $\hat{\mu}: \mathcal{X} \times \mathcal{P} \rightarrow \mathcal{Y}$, a model-agnostic explainer is an XLA explainer $\hat{\eta}:\mc{X}'\times\mc{P}'\to\mc{Y}'\times\mc{E}$, such that $\mc{X}' = \mathcal{Y} \times \mathcal{X}$.

% Figure environment removed
% instantiated by a translator functor $\mc{T}$ such that $\mc{T}(X) = \mathcal{X} \times \mathcal{Y}$:
%\begin{center}% Figure removed\end{center}
%\[ \xaiModelAgnostic \]

% \begin{wrapfigure}[3]{r}{0.30\textwidth}
% \vspace{-0.3cm}
% % Figure removed    
% \end{wrapfigure}
\textbf{Model-specific explainer.} 
% Given an LA model $\hat{g}: \mathcal{X} \times \mathcal{P} \rightarrow \mathcal{Y}$, a model-specific explainer is an XLA explainer $\hat{\eta}: \mathcal{X} \times \mathcal{P} \rightarrow \mathcal{Y} \times \mathcal{E} $  where the input object of the explainer $\mathcal{X}$ contains the parameters $\mathcal{P}$ of the model $\hat{g}$. Thus a model-specific explainer is instantiated by a translator functor $\mc{T}$ such that $\mc{T}(X) = \mathcal{Y} \times \mathcal{X} \times \mathcal{P}$:
A model-specific explainer simply differentiates from a model-agnostic, as the XLA explainer $\hat{\eta}:\mc{X}'\times\mc{P}'\to\mc{Y}'\times\mc{E}$ has $\mc{X}' = \mc{Y}\times \mc{X} \times \mathcal{P}$.
%\begin{center}
\hspace*{2.5cm}  
% Figure removed  
%\end{center}
%\[ \xaiPosthoc \]based

Typical examples of model-agnostic explainers include surrogate models~\citep{lundberg2017unified,ribeiro2016should,ribeiro2018anchors} and some concept-based approaches~\citep{ghorbani2019interpretation,ghorbani2019towards,kim2016examples}. Among renowned model-specific explainers instead we can include all model-intrinsic explainers~\citep{molnar2020interpretable} and some post-hoc explainers such as saliency maps~\citep{selvaraju2017grad, simonyan2013deep} as they can only explain gradient-based systems.
% LIME~\citep{ribeiro2016should}, SHAP~\citep{}, and ... are typical examples of model-agnostic explainable systems.
% LENs~\citep{ciravegna2023logic}, ... are typical examples of model-specific explainable systems.

\paragraph{Forward vs Backward.}
Another relevant difference among XAI methods for gradient-based models is whether the explainer relies on the upcoming parameters~\citep{petsiuk2018rise,zeiler2014visualizing,zintgraf2017visualizing} or the gradient of the loss on the parameters in the learning optimizer~\citep{selvaraju2017grad,simonyan2013deep}.

% \begin{wrapfigure}[3]{r}{0.30\textwidth}
% \vspace{-0.5cm}
% % Figure removed    
% \end{wrapfigure}
\textbf{Forward-based explainer. }
Given a gradient-based LA model $\hat{\mu}: \mathcal{X} \times \mathcal{P} \rightarrow \mathcal{Y}$, a forward-based explainer is an XLA explainer $\hat{\eta}:\mc{X}'\times\mc{P}'\to\mc{Y}'\times\mc{E}$ %: \mathcal{X} \times \mathcal{P}' \rightarrow  \mathcal{Y}' \times \mathcal{E}$ 
with $\mc{X}'=\mc{X}''\times \mc{P}$.
% instantiated by a translator functur $T$ such that $T(\mathcal{X})$ contains the parameters space $\mathcal{P}'$ of the model $\hat{\eta}$:
\hspace*{3.7cm}  
% Figure removed
%\[ \xaiForward \]

% ... are typical examples of model-specific explainable systems.
\textbf{Backward-based explainer. }
Given a gradient-based LA model $\hat{\mu}: \mathcal{X} \times \mathcal{P} \rightarrow \mathcal{Y}$ and an optimizer $\hat{\nabla}_{\mc{Y}}:\mc{Y}\times\mc{Y}\times\mc{P}\rightarrow\mc{P}$, a backward-based explainer is an XLA explainer $\hat{\eta}: \mathcal{X}' \times \mathcal{P}' \rightarrow \mathcal{Y}' \times \mathcal{E}$ where,  $\mc{X}'=\mc{X}''\times h(\mc{P})$, being $h(\mathcal{P})=\frac{\partial\mathcal{L}(\mc{Y},\mc{Y})}{\partial \mc{P}}$ the gradient of the loss function $\mc{L}$ on the parameters $\mc{P}$.
% \[ \xaiBackward \]

% \begin{example}[Concept bottleneck models]
\textbf{A case study: Concept bottleneck models.}
 Concept bottleneck models~\citep{koh2020concept} are recent XAI architectures which first predict a set of human-understandable objects called ``concepts'' and then use these concepts to solve downstream classification tasks. Our framework allows to formally define even these advanced XAI structures as follows: A concept bottleneck model is an XLA such that $\hat{\eta}$ is composed of a concept predictor $\hat{\mu}: \mathcal{X} \times \mathcal{P} \rightarrow \mathcal{Y}$ and a task predictor $\hat{\eta}':\mc{Y}\times\mc{P}\to\mc{Y}\times\mc{E}$, $\mathcal{Y}$ is the set of classes and $\mc{P} = \mc{P}' \times \mc{P}''$ is the product of the parameter space of the two models.%is system composed of two learning agents: a concept encoder $\hat{\mu}: \mathcal{X} \times \mathcal{P} \rightarrow \mathcal{Y}$ and a task predictor $\hat{\eta}:\mc{X}'\times\mc{P}'\to\mc{Y}'\times\mc{E}$, such that $\mc{X}' = \mathcal{Y}$, where $\mathcal{Y}$ is a set of human-understandable symbols:
\begin{center}% Figure removed\end{center}

Overall these examples give a taste of the flexibility and expressive power of our categorical framework demonstrating how it can successfully encompass existing XAI approaches and taxonomies.
\paragraph{A note on data types and explanation semantics.}
While the taxonomy of XAI methods mostly depends on how explainer and model functions are combined, the specific semantics of explanations mostly depends on the content of the data objects. The semantics of data can be described in terms of the set of attributes used to characterize each sample and on the set of values each attribute can take. We usually refer to data objects as feature and label matrixes, corresponding to the input  and target objects $\mathcal{X}$ and $\mathcal{Y}$, respectively. The semantics of a feature matrix varies depending on the attributes which typically represent pixels in images~\citep{kulkarni2022explainable}, relations in graphs~\citep{li2022survey}, words in natural language~\citep{danilevsky2020survey}, or semantically-meaningful variables (such as ``temperature'', ``shape'', or ``color'') in tabular data~\citep{di2022explainable}. Notice how different data types do not change the architecture of an XLA. However, choosing a specific data type can lead to significantly different levels of human understanding. In facts, human understanding does not depend directly on the structure of the XAI system, but rather on the kind of provided explanation.
For example, \citet{kim2018interpretability} shows how humans prefer explanations based on meaningful, human-understandable ``concepts'' (such as ``temperature'', ``shape'', or ``color''), rather than explanations based on pixels. Similarly, several works show how humans do not reason in terms of low-level attributes, but rather in terms of high-level ideas~\citep{ghorbani2019interpretation, goguen2005concept}. 




% \end{example}

% Thus explanations based on such semantics might significantly improve human understanding~\citep{ghorbani2019interpretation}.
% However, as discussed in the next section, another fundamental aspect to take into account for this discussion is how the explanation is conveyed, namely if it is expressed by a symbolic language or just as the result of a semantics model.


% \subsection{Reasoning with structures of structures}
% \label{sec:xai-structures-bis}
% So far, we described the key differences of XAI methods simply in terms of the input object of the explainer. However, some XAI approaches affect not only the explainer, but the whole process of learning, thus representing an opportunity to study XAI ``structures of structures''. For instance, concept bottleneck models~\citep{koh2020concept} represent a good case study for our purpose.
% \begin{example}[Concept bottleneck model]
%     Given an AI model $\hat{g}: \mathcal{X} \times \mathcal{P} \rightarrow \mathcal{C}$, a concept bottleneck model is a XAI system where the explainer $\hat{\eta}: \mathcal{C} \times \mathcal{P}' \rightarrow  \mathcal{Y}\times \mathcal{E}$ takes the output object of the model $\mathcal{C}$ as input object: \todo{fix tikz}
% \end{example}
% % \stefano{Is XAI system defined?}
% \[ \xaiCBM \]
% % \paragraph{Agreement between explainers}
% % This example demonstrates how category theory allows to reason about structures of structures more easily freeing XAI researchers from contingent details. 
% % For instance, we can use our categorical structures to develop and invent more exotic structures which are yet to find a concrete implementation in explainable AI literature. 
% Another interesting research line in AI
% consists in studying multi-agents learning environments, with possibly the human in the loop.  
% % \todo{we can add multi-agent communication to model human-in-the-loop ML, or AI-informed discovery}
% According to our proposed formalization, for instance we can define  the notion of ``agreement'' between two explainers by co-optimizing their explanations until convergence.
% % using an exotic structure which is yet to find a concrete implementation in explainable AI literature. 
% \begin{example}[Agreement of explainers]
% Given a pair of explainers $\hat{\eta}$ and $\hat{\eta}'$, we can optimize the agreement of their explanations using the following structure:
% \todo{fix tikz}
% \[ \xaiAgreement \]
% \end{example}

\subsection{Finding \#4: Our framework emphasizes commonly overlooked aspects of explanations}
\label{sec:xai-semantics}
Current XAI taxonomies often neglect the distinction between syntactic and semantic approaches. On the contrary, our~\Cref{def:xla} provides a natural distinction between these two forms of XLAs, thus introducing a novel perspective to analyze XAI methods. On the one hand, syntactic approaches work on the structure of symbolic languages and operate on $\Sigma$-sentences. Notable examples of syntactic approaches are proof systems such as natural deduction~\citep{prawitz2006natural} and sequent calculi~\citep{takeuti2013proof} which are designed to operate on formal languages such as first-order logic.  On the other hand, semantic approaches provide explanations related to the meaning or interpretation of sentences as a model of a language. Most XAI techniques actually fall into this class of methods as semantic explanations establish a direct connection with specific problem domains~\citep{karasmanoglou2022heatmap, lundberg2017unified,ribeiro2016should, simonyan2013deep}. The following examples show the 
% concrete difference
relation
between a syntactic and a semantic techniques emphasizing connections between XAI methods that often slip unnoticed.
\begin{wrapfigure}[11]{r}{0.4\textwidth}
\vspace{-0.4cm}
% Figure removed
\caption{Example of saliency map.}
\label{fig:saliency}
\end{wrapfigure}
\begin{example}
    Saliency maps~\citep{simonyan2013deep} are classic examples of semantic XLAs. To formally describe this case study we first need to define its institution $I_{SM}$.
    $I_{SM}$ 
    can be defined as a fragment of first-order logic with all the signatures' objects consisting of a single predicate. 
    % (e.g. binary in case of 2D-images). 
    Intuitively, in a classification task this relation represents the saliency degree of each pixel in a certain picture for the class prediction. Sentences and models are defined as in FOL by using the provided signature.
    For instance, the model shown in \Cref{fig:saliency} is a semantic explanation for the task of image recognition. However, a syntactic explanation describing the semantics model can be easily expressed in $I_{SM}$. Let assume $\Sigma$ consist of $S$ the saliency predicate per pixel, and a constant $p_i$ for each $i$-th pixel. Assuming to collect the most ``salient" pixels in the figure in the set $\textit{SalPix}$, the sentence expressing the syntactic explanation may be:%Let assume $\Sigma=\{S,p_1,p_2,\ldots\}$ with $S$ the saliency predicate per pixel, and $p_i$ the constant for the $i$-th pixel. Assuming to collect the most ``salient" pixels in the figure in the set $\textit{SalPix}$, the sentence expressing may be explanation will be:
$\displaystyle\bigwedge_{p\in\textit{SalPix}}S(p)$.
% \begin{wrapfigure}
%    [3]{r}{0.25\textwidth}
% \vspace{-0.4cm}
% % Figure removed    
% \end{wrapfigure}
    % For instance, taking the signature $\Sigma=\{S^{(2)}\}$, this institution may includes well-formed $\Sigma$-sentences as: $\exists (x,y)\ S(x,y) \wedge S(x+1,y) \wedge S(x,y+1) \wedge S(x+1,y+1)$ or $S(a,b)$ where $a,b$ denote FOL constants.
    % A model of the former sentence is given by a concrete saliency map that highlight a square of side 1 in a certain picture. 
    %%%%OLD
    % The institution of \emph{heatmaps} can be defined as FOL, where all the relations in the signature objects are binary. Intuitively, binary relations of heatmaps represent pixels with their positions and colors. Thus this institution includes well-formed $\Sigma$-sentences as the following: $\exists (x,y): P((x,y), Red) \wedge P((x+1,y), Red) \wedge P((x,y+1), Red) \wedge P((x+1,y+1), Red)$. This sentence is satisfied by the concrete heatmaps that highlight a square. 
\end{example}
%\fg{TODO: richeck esempio}
\begin{example}[Feature importance]
Another classic example of semantic XLAs are Feature importance methods (?) ~\cite{}. Following a similar approach to saliency maps, we provide a rigorous formalization of this specific case study by introducing the institution $I_{FI}$. Also
    $I_{FI}$ 
    can be defined as a fragment of first-order logic with all the signatures' objects consisting of a single predicate and a constant $f_i$ for every feature. 
\end{example}
    % (e.g. binary in case of 2D-images). 

% The istitution of heatmaps constitutes a fragment of first-order logic, hence any syntactic explanation of a heatmap can be seen as a particular instance of a first-order logic sentence. 
% This simple but yet important consequence of a unified view of intuitively different types of explanation makes available the well-studied framework of first-order logic to heatmaps, allowing to reason about models of heatmaps, proving theorems and so forth, and  thus showing the importance of a theoretical unified approach for XAI.     

While the strong connection to a specific context may stimulate human intuition, it often limits the scope and robustness of semantic explanations, hindering human understanding in the long run as extensively discussed by~\citet{ghorbani2019interpretation,rudin2019stop}. On the contrary, symbolic languages such as first-order logic or natural language are preferable for conveying meaningful messages as explanations as suggested by~\citet{kahneman2011thinking,marcus2020next}. For this reason, a promising but often overlooked research direction consists in accurately lifting semantic explanations into a symbolic language in order to provide a syntactic explanation~\citep{breiman1984classification,ciravegna2023logic,costa2018automatic,guidotti2018local, letham2015interpretable, ribeiro2018anchors}. By recognizing the importance of this distinction, our definition can provide a suitable basis to gaining deeper insights into the limitations of different forms of explanations.





%%%%HEATMAPS
% The institution of \emph{heatmaps} can be defined in a similar way considering signatures' objects consisting of a singe binary relation representing position and color of each pixel in a given space.



% \subsection{Semantics of Explanations}
% \label{sec:xai-semantics}
% While the taxonomy of XAI models mostly depends on how explainer and model functions are combined, the semantics of explanations mostly depends 
% on the content of the data objects and the signature used to form sentences. 
% The semantics of the explanation depends 
% on two factors: the semantics of the data and the form of a valid $\Sigma$-sentence.
% While category theory formalizes explainable AI structures, the semantics of explanations mostly depends on the content of the objects transformed by the morphisms. From a categorical perspective, all the objects handled by AI models are ``formal concepts'' in \(\Set\), as we discussed in Section~\ref{sec:framework}. Given this general structure, we can still differentiate the semantics of data with regards to the semantics of explanations, as follows.

% \paragraph{Semantics of data} 
% The semantics of data forms the raw material for the semantics of explanations. We describe the semantics of data in terms of the set of attributes used to characterize each sample and on the set of values each attribute can take. We usually refer to data objects as feature and label matrixes, corresponding to input objects $\mathcal{X}$ and target $\mathcal{Y}$ respectively. The semantics of a feature matrix varies depending on the attributes which typically represent pixels in images~\citep{kulkarni2022explainable}, relations in graphs~\citep{li2022survey}, words in natural language~\citep{danilevsky2020survey}, or semantically-meaningful variables (such as ``temperature'', ``shape'', or ``color'') in tabular data~\citep{di2022explainable}. Notice how different data types do not change the architecture of an explainable AI system. However, choosing a specific data type can lead to significantly different levels of human understanding. In fact, human understanding does not depend directly on the structure of the explainable AI system, but rather on the existence and completeness of a proper signature morphism from the explanation to the human observer. For example, humans lean towards explanations whose semantics is based on meaningful, human-understandable ``concepts'' (such as ``temperature'', ``shape'', or ``color''), rather than explanations whose semantics is based on pixels, as pointed out in seminal works in concept learning~\citep{kim2018interpretability}. In fact several works show how humans do not reason in terms of low-level attributes like pixels, but rather in terms of high-level ideas~\citep{goguen2005concept,ghorbani2019interpretation}. Thus explanations based on such semantics might significantly improve human understanding~\citep{ghorbani2019interpretation}.

% \paragraph{Semantics of Explanations}
% % While data semantics supplies the raw material for $\Sigma$-sentences, the form of a sentence determines the . 
% The simplest form of explanation is a pure description of the most relevant inputs~\citep{Das2020OpportunitiesAC}. Seminal XAI methods typically provide this form of descriptions by showing the most relevant input attributes for the prediction of a given sample, as it happens in saliency maps~\citep{simonyan2013deep}, Concept Activation Vectors~\citep{kim2018interpretability}, and SHapley Additive exPlanations~\citep{lundberg2017unified} . A more advanced form of explanation describes specific combinations of attributes leading to specific predictions. This form of explanation is common in rule-based systems such as decision trees~\citep{breiman1984classification} and Generalized Additive Models~\citep{hastie2017generalized}. 
% Finally, following existing surveys~\citep{Das2020OpportunitiesAC}, we can further distinguish the semantics of explanations on whether they hold for a single sample (as in LIME~\citep{ribeiro2016should}) or for a more general group of samples (as in decision trees~\citep{breiman1984classification}).
%%%%%%%%%





% While this distinction may impact the confidence of human observers, it does not change the structure nor the semantics of explanations.

% \subsection{Explainable AI: Evaluation}
% \label{sec:xai-evaluation}
% Following our taxonomy, the evaluation of explainable AI algorithms may follow two main branches: structural and semantic. Structural evaluation measures the prediction performances of an explainable AI system. Typical gold-standards for such metrics can be the reference labels (accuracy) or the predictions of a non-interpretable models (fidelity). Structural evalutaion does not directly depend on human understanding, but rather on the efficiency of the algorithm in making accurate predictions. Semantic evaluation instead measures whether the semantics of the explanations matches the criteria for human understanding. This evaluation may require human tests, but also the development of metrics which can automatically provide feedback to explainable AI systems for a given class of users as shown by~\citep{yeh2020completeness,zarlenga2021quality}.

% Evaluation can be:
% \begin{itemize}
%     \item structural: how good are explainer predictions w.r.t. an end-to-end model?
%     \item semantic: how human-understandable the explanation is?
% \end{itemize}

% \section{Two Case Studies}
% \subsection{Local and global explainers}

% \subsection{Concept-based Models}


\section{Discussion}
\label{sec:key}

\paragraph{Significance and relations with other XAI foundational works.}
The explainable AI research field is growing at a considerable rate~\citep{minh2022explainable} and it now has a concrete impact on other research disciplines~\citep{cranmer2019learning,davies2021advancing,jimenez2020drug} as well as on the deployment of AI technologies~\citep{duran2021afraid,lo2020ethical,wachter2017counterfactual}. This rising interest in explainable AI increases the need for a sound foundation and taxonomy of the field~\citep{adadi2018peeking,palacio2021xai} allowing XAI researchers to keep up with the latest innovations and making it possible for external audiences to quickly navigate the field. Existing reviews and taxonomies significantly contribute in: (i) describing and clustering the key methods and trends in the XAI literature~\citep{molnar2020interpretable}, (ii) proposing the first qualitative definitions for the core XAI terminology~\citep{Das2020OpportunitiesAC}, (iii) relating XAI methodologies, aims, and terminology with other scientific disciplines~\citep{miller2019explanation}, and (iv) identifying the key knowledge gaps and research opportunities~\citep{Das2020OpportunitiesAC}. However, most of these works acknowledge the need for a more sound mathematical foundation and formalism to support the field. Our framewok arises to fill this gap. In particular our methodology formalizes key XAI notions for the first time, using the category of Cartesian streams and the category of signatures. Our work also draws from~\citet{katsumata19} who propose Cartesian streams to model gradient-based learning, and~\citet{cruttwell2022categorical} who model gradient-based learning using the category of lenses~\citep{riley2018categories}. 
The categorical formalisms of lenses and streams are closely related~\citep{monoidalStreams}. Intuitively, lenses can be used to encode one-stage processes, while streams can encode processes with time indexed by natural numbers, i.e. providing a more suitable description of the dynamic process of learning.
%Our approach is more general as it is able to capture reinforcement learning agents through the feedback structure of the category \(\Stream{\Set}\).
% However, our work models XAI objects and morphisms using Cartesian streams which generalize the category of lenses \todo{@Elena please check this}. 
However, our work opens to more general AI systems which are not necessarily gradient-based by generalizing the category of lenses with Cartesian streams.

\paragraph{Limitations}
% This paper presents the first formalization of key explainable AI notions and the first theory-grounded taxonomy of the field. 
As with any mathematical theory, our approach represents one of the possible formalizations of foundational notions of XAI. Specifically, in this work we face the challenging task of formalizing vague concepts such as ``explanation'', acknowledging the ongoing debate over their meaning, not only within the AI community but also in philosophy, epistemology, and psychology. Nonetheless, as described in \Cref{sec:framework}, our definitions offer a robust formalization of XAI notions while maintaining sufficient generality to encompass most of the relevant instantiations of AI concepts, thanks to the well-established mathematical framework of category theory \citep{Abramsky2004ACS, aguinaldo2021graphical, cruttwell2022categorical, ong2022learnable, Selinger2001ControlCA, shiebler2021category, Swan2022, Turi1997TowardsAM}. This formalization aims to serve as a stepping stone for XAI researchers to conduct further rigorous studies on these notions.


% \todo{for authors and reviewers of this paper: please populate this section with ideas and comments
% \begin{itemize}
%     \item Discuss the limitation of this approach? Like if some models or paradigm are not expressable here or if it is strenuous to do?
% \end{itemize}
% }

% \paragraph{Broader Impact}

\paragraph{Conclusion}
This work presents the first formal theory of explainable AI. In particular, we formalized key notions and processes that were still lacking a rigorous definition. We then show that our categorical framework enables us to: (i) model existing learning schemes and architectures, (ii) formally define the term ``explanation'', (iii) establish a theoretical basis for XAI taxonomies, and (iv) emphasize commonly overlooked aspects of XAI methods, like the comparison between syntactic and semantics explanations.
% We then use our definitions to propose the first theory-grounded taxonomy of the XAI literature. 
% \todo{add limitations here} \mike{no need of limitation again you already mention above} 
Through this work, we provide a first answer to the pressing need for a more sound foundation and formalism in XAI as advocated by the current literature~\citep{adadi2018peeking,palacio2021xai}. While our taxonomy provides guidance to navigate the field, our formalism strengthens the reputation of explainable AI encouraging a safe and ethical deployment of AI technologies.
We think that this work may contribute in paving the way for new research directions in XAI, including the exploration of previously overlooked theoretical side of explainable AI, and the mathematical definition of foundational XAI notions, like ``understandability" and ``trustworthiness".
% , e.g. quantitatively investigating the mutual understanding of two agents using different signatures, or even trying to agree on different explanations. 

% \section{Discussion and conclusion}

% \todo{fun facts}

% \begin{proposition}
% Human learners are explainable learning agents.
% \end{proposition}

% \begin{definition}[Explainable Machine Learning Agent]
% An explainable machine learning agent is a concrete category of \(\XLearn{\cat{Stream}}\) where objects are in the category of vectors \(\Vect\), morphisms are parametrized functions, and explanations are well-formed $\Sigma$-formulae.
% \end{definition}

% \begin{itemize}
%     \item data type / level of abstraction (raw features, concept-based)
%     \item structure of explanations (descriptions/feature attribution, rules)
%     \item scope (local/global)
% \end{itemize}


% \begin{example}[Local Explainer]
% A local explainer is an explainer model providing an explanation for a single element of the input.
% \end{example}
% \begin{example}[Global Explainer]
% A global explainer is an explainer model providing an explanation for a set of elements of the input.
% \end{example}


% Here we use our formal definitions to provide clear ``explainations'' of the explainable AI literature \citep{molnar2020interpretable}.


% The type of data deeply affects XAI algorithms as they often determine the semantics of the explanations which are often given in terms of the attributes of the input data.

% More specifically, explainable AI research focuses on four main dimensions of interest~\citep{Das2020OpportunitiesAC,tavares2020understanding}: data type, explanation structure, explanation scope, and model structure. 

% \subsection{Explainable AI}
% Dimensions of interest
% \begin{itemize}
%     \item Type of data
%     \begin{itemize}
%         \item images
%         \item tabular data
%         \item graph-structured data
%     \end{itemize}
%     \item Type of explanation
%     \begin{itemize}
%         \item concept-based: CBM, Rule-based, ...
%         \item Heat-map
%     \end{itemize}
%     \item Type of integration between Xer and Model
%     \begin{itemize}
%         \item Post-hoc
%         \item self-explainable
%         \item interpretable
%     \end{itemize}
%     \item Scope
%     \begin{itemize}
%         \item local
%         \item global
%     \end{itemize}
% \end{itemize}


% \subsection{Explainer}

% \todo{unify all examples in a discourse}

% \begin{example}[Local Explainer]
% A local explainer is an explainer model providing an explanation for a single element of the input.
% \end{example}
% \begin{example}[Global Explainer]
% A global explainer is an explainer model providing an explanation for a set of elements of the input.
% \end{example}

% \subsection{Explainable Learning Agents}
% \begin{example}[Surrogate Learning Agent]
% A surrogate learning agent is a explainable learning agent $\xi$ which minimizes the difference between its predictions and the predictions of a learning agent $s$.
% \end{example}
% As opposed to a self-explainable learning agent!

% \begin{example}[Post-Hoc Learning Agent]
% A post-hoc learning agent is a surrogate learning agent which updates its parameters once the parameters of $s$ are fixed.
% \end{example}

% \todo{THIS MIGHT BE A NEW THING!}
% \begin{example}[Synchronous Learning Agent]
% A synchronous learning agent is a surrogate learning agent which updates its parameters while $s$ updates its parameters.
% \end{example}

% \subsection{Types of Explanations}
% In practice, we will represent a concrete contexts using matrices where the rows are headed by object names, the columns are headed by attribute names, and the value of a cell represents the binary relation. In machine learning settings the usual context is known as ``feature matrix'' and is represented by the set $X \subseteq \mathbb{R}^{n \times d}$ where $n=|G|$ and $d=|M|$ representing the relation $I \subseteq G \times M$. 
% Notice how a formal concept can have different representations depending on its intent $M$. 
% This is why when the intent is less ``structured'' (e.g., pixels) concepts can be quite noisy. For instance, two images representing the concept ``house'' need to have the exact same pixels representing the ``house''. This is because the context of ``pixels'' is not translation or rotation invariant. 

% This is why in these contexts the original intent is usually transformed into more robust set of attributes where concepts are more stable (i.e., the formal concept is larger). The most common contexts in machine learning are:
% \begin{itemize}
%     \item $A \subseteq \{0,1\}^{n \times n}$ commonly known as adjacency matrix
%     \item $Y \subseteq \mathbb{R}^{n \times l}$ commonly known as the matrix of ``reference labels''
%     % \item $C \subseteq \mathbb{R}^{n \times k}$ commonly known as the matrix of ``ground truth attribute labels''
% \end{itemize}

% To provide a set of more intuitive examples of concepts in different contexts, Figure X shows a single instance of the formal concept with name ``house'':
% \begin{itemize}
%     \item input matrix: segment of image (subset of pixels)
%     \item adjacency matrix: motif of graph (subset of nodes and edges)
%     % \item attribute label matrix: a row with a subset of non-zero columns of the table corresponding to the attribute names ``(roof, floor, walls)''
%     \item label matrix: a rectangle with a non-zero elements of the table (subset of rows and columns)
% \end{itemize}

% All the following work in a similar way i.e., they give an importance to a concept (relation between an object and some of its attributes)
% \begin{itemize}
%     \item Feature Attribution Explanation
%     \item heatmap
%     \item rule-based
% \end{itemize}

% \subsection{Invariants and Explanation Robustness}
% Explanations based on inputs are noisy. Processors can enforce invariants (e.g., translation/rotation/scaling in CNNs, permutation equivariance in GNNs, etc.) generating spaces of robust representations where these invariants are certified~\citep{shen2022trust}. This motivated~\citet{kim2018interpretability} opening to provide robust explanations based on robust concepts respecting similar invariants as human concepts~\citep{kim2018interpretability}.

% \begin{definition}[Concept Robustness]
% The concept robustness is the size of the tuple $(T,\mathcal{M})$ i.e., the number of models and theories of the concept.
% \end{definition}

% \begin{theorem}[Processor invariants and concept robustness]
% Enforcing representation invariants increases concept robustness.
% \end{theorem}



% The \icmltitle you define below is probably too long as a header.

% \section{Unifying Open Problems and Research Opportunities}
% \begin{itemize}
%     \item accuracy-explainability trade-off~\citep{zarlenga2022concept}
%     \item divergence metrics
%     \item formal languages for interpreters
%     \item differentiable interpreters
%     \item interpreters as mapping from system 1 and system 2, learning and reasoning~\citep{kahneman2011thinking}
%     \item robustness to input perturbations (solution: concept learning)
%     \item concept impurity, information leakage, and concept dependency/independency
%     \item enforcing best invariances
%     \item concept representations
%     \item cost of supervised labels for concepts
% \end{itemize}

% \section{Experiments}

% \subsection{Concept robustness}
% we're defining concept robustness in terms of its "size/support" and the "size/support" of a concept depends on its invariants and the invariants can be forced by architectures. 
% So a simple experiment could be to evaluate concept robustness using the same dataset but enforcing different invariants with different architectures.
% We can use a clustering algorithm on the embedding of different architectures + silhouette/homogeneity score to measure concept robustness.
% We can also measure the robustness in the pixel space and compare it with the robustness in the embeddings.


% TODO: next works Active learning, Adversarial Learning, 

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}



\bibliographystyle{plainnat}
\bibliography{references}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Elements of Category Theory}\label{app:cat}
\subsection{Monoidal Categories}\label{app:mon-cat}
% \fg{TOCHECK SE TUTTO FILA}\\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% dalla sez 2
The process interpretation of monoidal categories~\citep{Coecke2017,fritz2020} sees morphisms in monoidal categories as modelling processes with multiple inputs and multiple outputs.
Monoidal categories also provide an intuitive syntax for them through string diagrams~\citep{joyal1991geometry}.
The coherence theorem for monoidal categories~\citep{maclane78} ensures that string diagrams are a sound and complete syntax for them and thus all coherence equations for monoidal categories correspond to continuous deformations of string diagrams. One of the main advantages of string diagrams is that they make reasoning with equational theories more intuitive.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\begin{definition}[\citet{eilenberg1945general}]
    A \emph{category} \(\cat{C}\) is given by a class of \emph{objects} $\cat{C}^o$ and, for every two objects \(X,Y \in \cat{C}^o\), a set of \emph{morphisms} $\hom(X,Y)$ with input type \(X\) and output type \(Y\). A morphism \(f \in \hom(X,Y)\) is written \(f \colon X \to Y\).
    For all morphisms \(f \colon X \to Y\) and morphisms \(g \colon Y \to Z\) there is a \emph{composite} morphisms \(f \dcomp g \colon X \to Z\).
    For each object \(X \in \cat{C}^o\) there is an \emph{identity} morphism \(\id{X} \in \hom(X,X)\), which represents the process that ``does nothing'' to the input and just returns it as it is.
    Composition needs to be associative, i.e. there is no ambiguity in writing \(f \dcomp g \dcomp h\), and unital, i.e. \(f \dcomp \id{Y} = f = \id{X} \dcomp f\) (\Cref{fig:category-string-diagrams-app}, bottom).
\end{definition}
% Figure environment removed




Monoidal categories~\citep{maclane78} are categories endowed with extra structure, a monoidal product and a monoidal unit, that allows morphisms to be composed \emph{in parallel}.
The monoidal product is a functor \(\tensor \colon \cat{C} \times \cat{C} \to \cat{C}\) that associates to two processes, \(f_1 \colon X_1 \to Y_1\) and \(f_2 \colon X_2 \to Y_2\), their parallel composition \(f_1 \tensor f_2 \colon X_1 \tensor X_2 \to Y_1 \tensor Y_2\) (\Cref{fig:string-diagrams-monoidal-cat-app}, center).
The monoidal unit is an object \(\monoidalunit \in \cat{C}^o\), which represents the ``absence of inputs or outputs'' and needs to satisfy \(X \tensor \monoidalunit \iso X \iso \monoidalunit \tensor X\), for each \(X\in \cat{C}^o\).
For this reason, this object is not drawn in string diagrams and a morphism \(s \colon \monoidalunit \to Y\), or \(t \colon X \to \monoidalunit\), is represented as a box with no inputs, or no outputs.
\[\statemorphismFig{} \quad \text{and} \quad \costatemorphismFig{}\]



\subsection{Cartesian and Symmetric Monoidal Categories}\label{app:symm-mon-cat}

A symmetric monoidal structure on a category is required to satisfy some coherence conditions \citep{maclane78}, which ensure that string diagrams are a sound and complete syntax for symmetric monoidal categories \citep{joyal1991geometry}. 
Like functors are mappings between categories that preserve their structure, \emph{symmetric monoidal functors} are mappings between symmetric monoidal categories that preserve the structure and axioms of symmetric monoidal categories.

A monoidal category is \emph{symmetric} if there is a morphism \(\swap{X,Y} \colon X \tensor Y \to Y \tensor X\), for any two objects \(X\) and \(Y\), called the \emph{symmetry}.
% Figure environment removed
The inverse of \(\swap{X,Y}\) is \(\swap{Y,X}\):
\[\swapinverseFig{X}{Y}\]
These morphisms need to be coherent with the monoidal structure, e.g.
\[\hexagoneqexampleFig{X}{Y}{Z}.\]
Moreover, the symmetries are a natural transformation, i.e. morphisms can be swapped,
\[\swapnaturalFig{}.\]



Some symmetric monoidal categories have additional structure that allows resources to be copied and discarded~\citep{fox76}.
These are called Cartesian categories.
It is customary to indicate with \(\times\) the monoidal product given by the cartesian structure and with \(e\) the corresponding monoidal unit.
Cartesian categories are equipped, for every object \(X\), with morphisms \(\cp_X \colon X \to X \times X\) and \(\discard_X \colon X \to e\):
\[\copyXFig{} \quad \text{and} \quad \discardXFig{}\]
These need to be natural transformations, i.e. morphisms can be copied and discarded,
\begin{align*}
    \copynaturalFig{}\\
    \discardnaturalFig{}
\end{align*}
be coherent with the monoidal structure
\begin{align*}
    \copycoherentFig{X}{Y}\\
    \discardcoherentFig{X}{Y}
\end{align*}
and satisfy the equations of a cocommutative comonoid.
\begin{align*}
    \copycoassociativeFig\\
    \copycounitalFig\\
    \copycocommutativeFig
\end{align*}




\subsection{Feedback Monoidal Categories}\label{app:feedback-cat}

\begin{definition}\label{def:fmc}
A feedback monoidal category is a symmetric monoidal category \(\cat{C}\) endowed with an endofunctor $F:\cat{C} \rightarrow \cat{C}$, and an operation \(\fbk[S] \colon \hom (X \times F(S), Y \times S) \to \hom (X,Y)\) for all objects \(X,Y,S\) in \(\cat{C}\),
which satisfies the following axioms:
\[
\small
\begin{array}{ll}
    \mbox{(Tightening)} & \mbox{\(\fbk[S]((g \times \id{FS}) \dcomp f \dcomp (h \times \id{S})) = g \dcomp \fbk[S](f) \dcomp h\);} \\
     \mbox{(Joining)} & \mbox{\(\fbk[S \times T](f) = \fbk[S](\fbk[T](f))\);} \\
     \mbox{(Vanishing)} & \mbox{\(\fbk[\monoidalunit](f) = f\);} \\
     \mbox{(Strength)} & \mbox{\(\fbk[S](g \times f) = g \times \fbk[S](f)\);} \\
     \mbox{(Sliding)} & \mbox{\(\fbk[T](f \dcomp (\id{Y} \times g)) = \fbk[S]((\id{X} \times g) \dcomp f)\)} .
\end{array}
\]
\end{definition}
Feedback monoidal functors are mappings between feedback monoidal categories that preserve the structure and axioms of feedback monoidal categories.

Feedback monoidal categories are the \emph{syntax} for processes with feedback loops.
When the monoidal structure of a feedback monoidal category is cartesian, we call it feedback cartesian category.
Their \emph{semantics} can be given by monoidal streams~\citep{monoidalStreams}.
In cartesian categories, these have an explicit description.
We refer to them as cartesian streams, but they have appeared in the literature multiple times under the name of ``stateful morphism sequences''~\citep{katsumata19} and ``causal stream functions''~\citep{uustalu05}.

\subsection{Cartesian Streams}\label{app:streams}
A \emph{cartesian stream} \(\stream{f} \colon \stream{X} \to \stream{Y}\), with \(\stream{X} = (X_0, X_1, \dots)\) and \(\stream{Y} = (Y_0, Y_1, \dots)\), is a family of functions \(f_n \colon X_n \times \cdots \times X_0 \to Y_n\) indexed by natural numbers.
Cartesian streams form a category \(\Stream{\Set}\).

Each \(f_n\) gives the output of the stream at time \(n\).
We can compute the outputs until time \(n\) by combining \(f_0, \dots, f_n\) to get \(\hat{f}_n \colon X_n \times \cdots \times X_0 \to Y_n \times \cdots \times Y_0\) as follows:
\begin{itemize}
\item \(\hat{f}_0 \defn f_0\)
\item \(\hat{f}_{n+1} \defn (\id{X_{n+1}} \times \cp_{X_n \times \cdots \times X_0}) \dcomp (f_{n+1} \times \hat{f}_n)\)
\end{itemize}
\[\cartesianStreamsCompositionFig\]
The composition of two cartesian streams \(\stream{f} \colon \stream{X} \to \stream{Y}\) and \(\stream{g} \colon \stream{Y} \to \stream{Z}\) has components \((\stream{f} \dcomp \stream{g})_n \defn \hat{f}_n \dcomp g_n\), and the identity stream has projections as components \((\id{\stream{X}})_n \defn \proj{X_n}\).

\subsection{Free Categories}
\label{app:free}
We generate ``abstract'' categories using the notion of \emph{free category}~\citep{maclane78}. Intuitively, a free category serves as a template for a  class of categories (e.g., feedback monoidals). To generate a free category, we just need to specify a set of objects and morphisms generators. Then we can realize ``concrete'' instances of a free category $\cat{F}$ using a functor from $\cat{F}$ to another category $\cat{C}$ that preserves the axioms of $\cat{F}$. If such a functor exists then $\cat{C}$ is of the same type of $\cat{F}$ (e.g., the image of a free feedback monoidal category via a feedback functor is a feedback monoidal category).

\subsection{Institutions}\label{app:institutions}

An \emph{institution} $I$ is constituted by:
\begin{itemize}
\item[(i)] a category
\(\cat{Sign}_I\) whose objects are signatures (i.e. vocabularies of symbols);
\item[(ii)] a functor $Sen: \cat{Sign}_I \mapsto \cat{Set}$ providing sets of well-formed expressions ($\Sigma$-sentences) for each signature \(\Sigma\in\cat{Sign}_I^o\);
\item[(iii)] a functor $Mod: \cat{Sign}_I^{op} \mapsto \cat{Set}$ providing semantic interpretations, i.e. worlds.
\end{itemize}
Furthermore, Satisfaction is then a parametrized relation $\models_{\Sigma}$ between $Mod(\Sigma)$ and $Sen(\Sigma)$, such that for all signature morphism $\rho: \Sigma \mapsto \Sigma'$, $\Sigma'$-model $M'$, and any $\Sigma$-sentence $e$, 
\begin{equation*}
    M' \models_{\Sigma} \rho(e) \text{ iff } \rho(M') \models_{\Sigma} e
\end{equation*}
where $\rho(e)$ abbreviates $Sen(\rho)(e)$ and $\rho(M')$ stands for $Mod(\rho)(e)$.





\end{document}
% % % % 