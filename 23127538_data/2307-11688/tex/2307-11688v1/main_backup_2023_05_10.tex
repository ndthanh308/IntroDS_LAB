\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2022


% ready for submission
\usepackage{neurips_2023}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2022}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2022}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2022}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}
\usepackage{todonotes}
\usepackage{caption}
\usepackage{comment}
\usepackage{subcaption}
\usepackage{amsthm}
% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{booktabs}            % professional-quality tables
\usepackage{multirow}            % tabular cells spanning multiple rows
\usepackage{amsfonts}            % blackboard math symbols
\usepackage{graphicx}            % figures
\usepackage{duckuments}          % sample images
\usepackage{dsfont}
\usepackage{natbib}
\usepackage{scalerel}
\usepackage[noend]{algpseudocode}
\usepackage{algorithm}

% \usepackage{algorithmic}
\usepackage{etoolbox}\AtBeginEnvironment{algorithmic}{\tiny}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
% \theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}


\newcommand{\note}[1]{\textcolor{red}{Note: #1}}
\newcommand{\fg}[1]{\textcolor{blue}{F: #1}}
\newcommand{\ste}[1]{\textcolor{green}{Ste: #1}}


\title{Interpretable Graph Networks Bridge Equational and Topological Conjectures in Universal Algebra}
% Interpretable Graph Networks Frame \\Evidences for Universal Algebra Conjectures
% Interpretable Graph Networks Bridge Equational and Topological Representations in Universal Algebra
% "Exploring the Relationship between Equational and Topological Properties in Universal Algebra with Interpretable Graph Networks"
% "Interpretable Graph Networks Reveal Connections between Equational and Topological Properties in Universal Algebra"
% "Bridging Equational and Topological Properties in Universal Algebra with Interpretable Graph Networks"
% "Interpretable Graph Networks Unveil Correlations between Equational and Topological Properties in Universal Algebra"
% "Connecting Equational and Topological Properties in Universal Algebra using Interpretable Graph Networks"
% "A Graph-Based Approach to Understanding the Relationship between Equational and Topological Properties in Universal Algebra"
% "Using Interpretable Graph Networks to Uncover the Links between Equational and Topological Properties in Universal Algebra"
% "Visualizing the Relationship between Equational and Topological Properties in Universal Algebra with Interpretable Graph Networks"
% "Investigating Equational and Topological Properties in Universal Algebra with Interpretable Graph Networks"
% "Interpretable Graph Networks: Revealing the Relationship between Equational and Topological Properties in Universal Algebra".
% "Exploring Universal Algebra with Interpretable Graph Networks"
% "Unveiling Algebraic Properties through Interpretable Graph Networks"
% "Using Interpretable Graph Networks to Understand Universal Algebra"
% "Graph Networks Shed Light on Algebraic Properties in Universal Algebra"
% "Interpretable Graph Networks: A Tool for Revealing Algebraic Properties in Universal Algebra"
% "Discovering Universal Algebraic Properties with Interpretable Graph Networks"
% "Interpretable Graph Networks Uncover Hidden Patterns in Universal Algebra"
% "Visualizing Algebraic Properties with Interpretable Graph Networks"
% "Insights into Universal Algebra through Interpretable Graph Networks"
% "Interpretable Graph Networks: A New Approach to Studying Universal Algebra"


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  David S.~Hippocampus\thanks{Use footnote for providing further information
    about author (webpage, alternative address)---\emph{not} for acknowledging
    funding agencies.} \\
  Department of Computer Science\\
  Cranberry-Lemon University\\
  Pittsburgh, PA 15213 \\
  \texttt{hippo@cs.cranberry-lemon.edu} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}


\begin{document}


\maketitle


\begin{abstract}
Recently, AI-assisted solutions have proven successful when applied to Mathematics and have opened new possibilities for exploring unsolved problems that have eluded traditional approaches for years or even centuries. Following this direction, in this paper we present a novel approach to relate equational properties of special algebraic structures to sub-portions of their topological representation. We provide theorem suggestions through AI alonside with Explainability (XAI) metrics that support the conjectures. In particular, we examine the distributive and modular properties of algebraic lattices, whose characterization is well-known in universal algebra. The architecture hereby presented is capable of recovering known subpatterns that identify these properties, while also producing new relevant candidates as theorem suggesters for mathematicians to investigate.
\fg{to change, this is the one of ICML}
\end{abstract}

\section{Introduction}




\begin{itemize}
    \item General definition of the problem:
    \fg{AI for mathematics: general goal (produce counter-example, introduce conjecture,....)}
\end{itemize}

Artificial Intelligence (AI) has been used in mathematics for several decades, and the field of computational mathematics has grown significantly as a result. Some of the earliest applications of AI in mathematics included the use of expert systems to solve algebraic and geometric problems, and the development of automated theorem provers that could mechanically prove mathematical theorems.
In recent years, there has been a surge of interest in the use of deep learning techniques, such as neural networks, for mathematical research. These techniques have been applied to a wide range of problems, from classifying mathematical objects to predicting the behavior of complex dynamical systems.
There has also been significant progress in the development of interactive proof assistants, which allow mathematicians to verify the correctness of their proofs using computer programs. These tools can help to reduce the likelihood of errors and increase confidence in the correctness of mathematical results.
Overall, the use of AI in mathematics has the potential to revolutionize the field by providing new insights, automating tedious tasks, and enabling mathematicians to explore new areas of research that were previously inaccessible.
%%%%

%%%%
AI has been utilized in mathematics for many years, leading to a significant growth in the field of computational mathematics. Early applications included expert systems for solving algebraic and geometric problems and automated theorem provers. More recently, deep learning techniques such as neural networks have been used for a wide range of mathematical problems. Additionally, interactive proof assistants have been developed to verify the correctness of mathematical proofs using computer programs. AI has the potential to revolutionize mathematics by providing new insights, automating tedious tasks, and enabling mathematicians to explore previously inaccessible areas of research.

\begin{itemize}
    \item Existing solutions and knowledge gaps
    \fg{Specific solutions: RL, diff equa, theorem proving, geometry-topological, group theory, ecc, THEN: collaboration with mathematicians (Davis etc..), Hence: us for UA:}
%Why UA?
The theory of universal algebra has applications in computer science, logic, and other branches of mathematics, such as topology and geometry. It provides a framework for studying the properties of algebraic structures in a systematic and general way, and for developing algorithms and software for manipulating these structures. 
Interestingly, several properties can be described by relying on the underlined graph representation of these algebraic structures, hence allowing the exploitation of well-suited AI-based models excelling on graph-structured data. In this regard, Graph Neural Networks (GNN) represent a good candidate for graph classifications, and recently some advances have been made introducing novel interpretable concept-based GNNs capable of automatically extract the relevant concepts for a classification task \cite{magister2021gcexplainer,passerini?}.
    
    \item Visual summary of the paper: problem, solution, results
    \fg{The idea sketced in general? Like, graph property -> gen. dataset -> train gnn classifier -> use gnn theorem suggester}

We introduce a new framework where human mathematicians can benefit from explainable AI agent as theorem suggester. In particular, exploiting interpretable GNNs to hightlight relevant portions of graphs responsible for the failure of an equational property. 

%%%%%%
We present a general methodology to semi-automate the process of generating conjectures for universal algebra. The methodology consists of a close collaboration between a human expert in universal algebra and an interpretable graph neural network (Figure \ref{fig:vis_abs}). The proposed workflow allows experts to convert algebraic properties expressed by (quasi-)equations into datasets suitable for machine learning. This newly created dataset is later be fed to an interpretable GNN, and it is possible to visualize concept-based explanations yielded by the interpretable model, providing feedback to the human expert. The expert can then refine the conjectures, tweak the dataset, and restart the loop.
 \fg{Add trying to prove the theorem suggested by the conjecture?}
%%%%%%%%%%%%%%%
    
    \item Our solution and contributions
    \fg{We investigate the properties x,y,.., by combining GNN+GCE, we expect results; to evaluate we realized the benchmark;}
    
    The main contribution of the paper are the following:
    \begin{itemize}
    \item We propose a novel methodology that  given a certain equational property builds a dataset of graph-based algebraic structures, and then train an interpretable GNN  to conjecture  which is the sub-portion of the graph responsible for the failure of such a property. 
    \item We introduce a new concept-based interpretable graph neural model trained to suggest new conjectures to solve universal algebra's open problems.
    
    \item We create and release\footnote{Hidden for double blind submission.} the first dataset of lattices in PyTorch labelled with different algebraic properties expressed by equations and quasi-equations.
    \ste{with a pytorch script?}
    \item We proposed the first benchmark in universal algebra for AI agents, and discuss a manifesto of a (possibly very large) class of problems in this field that can be experimented by our model.
        \item  Our method shows to be capable of verifying well-established results in UA and proposing new insightful conjectures.
    \end{itemize}
    \fg{to clean..}
\end{itemize}



% Figure environment removed








\section{Background}
\label{sec:back}

\subsection{Universal Algebra}
\label{sec:ua}
% \todo[@Stefano, @Francesco]

The field of Universal Algebra (UA) is a branch of mathematics concerning the study of algebraic structures from a general and abstract perspective. \emph{Algebraic structures} are typically represented as ordered pairs $\mathbf{A} = (A, F)$, consisting of a set $A$ and a collection of operations $F$ defined on that set. %The operations may include binary operations (such as addition or multiplication), unary operations (such as negation or inverse), nullary operations (constants), and so forth. 
UA aims to identify and explore common algebraic properties shared by various mathematical systems often express as set of equations. \emph{Varieties} are classes of algebraic structures sharing a common set of identities. By specifying these identities, varieties enable the classification and study of algebraic systems based on their common properties. %They form the foundation for exploring the fundamental principles and interrelationships within the field of Universal Algebra. 
Prominent instances of varieties that have been extensively studied across various academic fields encompass Groups, Rings, Boolean Algebras, Fields, and many others. We refer the reader to \cite{BurrisSanka} for foundational notions of UA.%Universal Algebra has applications in various areas of mathematics and beyond. It provides a foundation for understanding the structure and properties of algebraic systems in fields like computer science, physics, and engineering. It also serves as a fundamental tool in the study of mathematical logic, model theory, and formal languages.
%\subsection{Lattices} 
%\label{sec:latt}
A particularly relevant variety of algebras are Lattices, which are studied for their connection with logical structures as well as for their notable algebraic properties. 

\begin{definition}
 A \emph{lattice} $\mathbf{L}$ is an algebraic structure composed by a non-empty set $L$ and two binary operations $\vee$ and $\wedge$ satisfying the commutativity, associativity, idempotency, and absorption axioms, see Appendix \ref{def:lattice}. %following axioms and their duals obtained exchanging $\vee$ and $\wedge$:
 \begin{comment}
\begin{align*}
&x \vee y \approx y \vee x  &&\text{(commutativity)}
\\&x \vee (y \vee z) \approx (x \vee y)  &&\text{(associativity)}
\\&x \vee x \approx x  &&\text{(idempotency)}
\\&x \approx x \vee (x \wedge y)  &&\text{(absorption)}
\end{align*}
\end{comment}
\end{definition}
%class of lattice class of lattices is studied both for is connection with logical structures and for its algebraic properties. 

Essentially a lattice is a partially ordered set in which every pair of elements has a \emph{supremum} and an \emph{infimum}.  Lattices can be depicted through their \emph{Hasse diagrams} which are a graphical representation of a lattice. In an Hasse diagram, the elements of the lattice are represented by nodes, and the ordering relation between them is represented by directed edges. Specifically, if there is an edge from node $x$ to node $y$, then $x \leq y$ in the ordering of the lattice, see Figure \ref{fig:n5m3} for concrete examples of Hasse diagrams. Consequently, Hasse diagrams can be understood as graphs. A \emph{sublattice} $\mathbf{L}'$ of a lattice $\mathbf{L}$ is a lattice such that $L' \subseteq L$ and $\mathbf{L}'$ preserves the original order of $L$, i.e. for all $a, b \in L'$ then $a \leq b$ in $L'$ if and only if $a \leq b$ in $L$. Intuitively, a sublattice is a substructure of a lattice that preserves the essential structure of the original lattice. 

\ste{Pietro insert minifigures}

\begin{comment}
% Figure environment removed 

\stefano{Insert an example}
\end{comment}


%Another fundamental notion for algebraic structure are \emph{congruences}, namely the equivalence relations on $A$ compatible with the operations of $\mathbf{A}$. The set of all congruences of an algebra $\mathbf{A}$ forms a lattice with the order given by set-theretic inclusion and serves as a powerful tool in the study of universal algebra, enabling a deeper understanding of the interplay between congruences and the algebraic operations within a given structure.



%These structures appear as a fundamental aspect congruence lattices of a generic algebra $\mathbf{A}$. In summary, a congruence lattice is a lattice that represents the relationships and properties of \emph{congruences} on an algebraic structure, . It serves as a powerful tool in the study of universal algebra, enabling a deeper understanding of the interplay between congruences and the algebraic operations within a given structure.

%In fact, for every algebra $\mathbf{A}$ the set of its \emph{congruences}, namely the equivalence relations on $A$ compatible with the operations of $\mathbf{A}$, form a lattice structure. Congruence lattices are of interest  since they encode many relevant properties of the original algebras as structural properties of lattice. \fg{I'd rephrase for clarity}



%An interesting problem in algebra is the connection between the internal structure of an algebra and the identities which it satisfies. The study of varieties of algebras provides some insight into this problem.

%Important concepts often used to investigate such abstract problems are the so called congruence lattices of algebraic structures. Namely, those objects are graphs representing partially ordered sets such that every pair of elements has unique supremum and infimum determined by the underlying algebra. Several results in the literature witness the importance of this notion relatively to properties satisfied by algebraic structure, many of which can be described by omission or admission of certain subpatern in a graph. The genesis of this abstract branch of Algebra can be tracked to the pioneeristic work of Dedekind, which studied the classical problem of the admission of lattices as congruence sublattices of a given class of algebraic structures. In this paper our aim is to deal with these type of questions using tools provided by artificial intelligence. Intuitively these kind of problems reduce to identify graph subpatterns that witness structural properties of the whole graph and thus it lends itself to be attached from a neural prospective with architectures like Graph Neural Networks.

The field of lattice varieties emerged as a branch of inquiry stemming from the investigation of general varieties, a subject initially introduced by Garrett Birkhoff in the 1930s. Birkhoff's pioneering contributions provided the first notable advancements in this field, which was subsequently expanded upon by Alfred Tarski and, for congruence distributive varieties, by Bjarni Jonsson. The foundational work of Tarski and Jonsson played a significant role in establishing the groundwork for the development of a wide range of results concerning lattice varieties. Some significant varieties of lattices can be characterized through the omission of one or more lattices. Specifically, a variety $\mathcal{V}$ of lattices is said to \emph{omit} a lattice $\mathbf{L}$ if the latter cannot be identified as a sublattice of any lattice in $\mathcal{V}$. This exclusion criterion serves as a useful means of characterizing and classifying different types of lattice varieties.

\begin{definition}
    Let $\mathbf{L}$ be a lattice. Then $\mathbf{L}$ is \emph{modular} (\emph{distributive}) if it satisfies the following equation:
    \begin{align*}
        &x \leq y \rightarrow x \vee (y \wedge z) \approx y \wedge (x \vee z) &&\text{(modularity)}
        \\&x \vee (y \wedge z) \approx (x \vee y) \wedge (x \vee z) &&\text{(distributivity)}
    \end{align*}
\end{definition}

As we can see from Figure \ref{fig:n5m3} $\mathbf{N}_5$ is not modular taking the substitution $x = a, y= b, z= c$ and the same substitution shows that $\mathbf{M}_3$ is not distributive. The classes of distributive and modular lattices form two distinct varieties showing a classical examples of characterization of lattice variety through lattice omissions. 

\begin{theorem}[Dedekind]\label{the:n5}
Let $\mathcal{V}$ be a lattice variety. Then $\mathcal{V}$ is modular variety if and only if $\mathcal{V}$ omits $\mathbf{N}_5$.
\end{theorem}

\begin{theorem}[Birkhoff]\label{the:m3}
Let $\mathcal{V}$ be a lattice variety. Then $\mathcal{V}$ is distributive variety if and only if $\mathcal{V}$ omits $\mathbf{N}_5$ and $\mathbf{M}_3$.
\end{theorem}

Starting from these classic results, the investigation of lattice omissions and the structural characterizations of classes of lattices has evolved into a rich and extensively studied field. For a more in-depth exploration of this subject, we refer the reader to \cite{JipsenRose}.

\fg{todo: add theorem for the properties/varieties that can be described as lattices omission} \cite{whitman1941free}

We notice that similar characterizations of varieties in terms of lattices' omission have been produced for the lattice of congruences of a certain variety \fg{add reference or theorem} \cite{}. Hence, the method we describe in the next section could be further investigated in this direction. However, as it is not a trivial extension, we plan to extend our approach for this probelm in future work.

\begin{comment}
\subsection{Basics of Universal Algebra}
% \todo[@Stefano, @Francesco]


Universal Algebra (UA) is a branch of Mathematics that studies algebraic structures in a general and abstract way, without restricting to particular operations or sets of elements. In particular, it deals with the study of algebraic structures that are defined purely in terms of their operations and the axioms governing them.
%
The main focus of UA is to study algebraic structures that are common to many different areas of mathematics, such as groups, rings, fields, lattices, and Boolean algebras. It also deals with the study of the properties of algebraic structures that are preserved under various operations, such as homomorphisms and substructures.


\paragraph{Equational Properties and Lattices' Omission}

The field of General Algebra concerns the study of classes of algebraic structures that share relevant properties. An \emph{algebraic structure} is a pair $\mathbf{A} = (A, F)$ where $A$ is a nonempty set and $F$ is a set of operations on $A$. Famous examples of algebraic structure studied in various fields are Groups, Rings, Boolean Algebras, Fields, and so forth. A particularly relevant class of algebraic structure are Lattices which are studied both for their  connection with logical structures and for their algebraic properties. 

\begin{definition}
 A \emph{lattice} $\mathbf{L}$ is an algebraic structure composed of a non-empty set $L$ and two binary operations $\vee$ and $\wedge$ satisfying the following axioms and their duals obtained exchanging $\vee$ and $\wedge$:
\begin{align*}
&x \vee y \approx y \vee x  &&\text{(commutativity)}
\\&x \vee (y \vee z) \approx (x \vee y)  &&\text{(associativity)}
\\&x \vee x \approx x  &&\text{(idempotency)}
\\&x \approx x \vee (x \wedge y)  &&\text{(absorption)}
\end{align*}
\end{definition}
%class of lattice class of lattices is studied both for is connection with logical structures and for its algebraic properties. 

Essentially a lattice is a partially ordered set in which every pair of elements has a \emph{supremum} and an \emph{infimum}. These structures appear as congruence lattices of a generic algebra $\mathbf{A}$. In fact, for every algebra $\mathbf{A}$ the set of its \emph{congruences}, namely the equivalence relations on $A$ compatible with the operations of $\mathbf{A}$, form a lattice structure. Congruence lattices are of interest  since they encode many relevant properties of the original algebras as structural properties of lattice.

An interesting problem in algebra is the connection between the internal structure of an algebra and the identities which it satisfies. The study of varieties of algebras provides some insight into this problem.

%Important concepts often used to investigate such abstract problems are the so called congruence lattices of algebraic structures. Namely, those objects are graphs representing partially ordered sets such that every pair of elements has unique supremum and infimum determined by the underlying algebra. Several results in the literature witness the importance of this notion relatively to properties satisfied by algebraic structure, many of which can be described by omission or admission of certain subpatern in a graph. The genesis of this abstract branch of Algebra can be tracked to the pioneeristic work of Dedekind, which studied the classical problem of the admission of lattices as congruence sublattices of a given class of algebraic structures. In this paper our aim is to deal with these type of questions using tools provided by artificial intelligence. Intuitively these kind of problems reduce to identify graph subpatterns that witness structural properties of the whole graph and thus it lends itself to be attached from a neural prospective with architectures like Graph Neural Networks.


\paragraph{Relevant properties: Congruence Modularity and Distributivity} \label{sec:latt}
A fundamental problem in Universal Algebra is determining the properties of an algebraic variety $\mathcal{V}$ - a nonempty class of algebras of type $\sigma$, closed under homomorphic images, direct products, and subalgebras, (cfr. appendix \ref{aa}). Some varieties of lattices can be characterised through the omission of one or more lattices in the congruence lattice (cfr. appendix \ref{aa}) of its algebras.\\

% % Figure environment removed

\fg{Definition of Modularity and Distributivity to be given as equations}

\fg{To split definition from comments!}

\begin{definition} \textbf{Congruence Modularity} \\
In abstract algebra, a congruence relation is an equivalence relation on an algebraic structure such that algebraic operations done with equivalent elements will yield equivalent elements (compatibility). Every congruence relation has a corresponding quotient structure, whose elements are the equivalence classes (or congruence classes) for the relation. \cite{hungerford}\\
\end{definition}

\begin{definition} \textbf{Congruence Distributivity}\\
An algebra is congruence distributive if its lattice of congruence relations is a distributive lattice - i.e. is a lattice in which the operations of join and meet distribute over each other. \cite{hungerford}
Congruence distributivity has many structural consequences, the most important being JÃ³nsson's Lemma 1 \cite{jonson} which implies that a finitely generated congruence distributive variety is residually finite.\\
\end{definition}
\fg{I'd say explicitely that is the lattice of congruences of the variety to omit N5. It's enough a note}
\begin{theorem}\textbf{Congruence Modularity} \\
Let $\mathcal{V}$ be a variety. Then the following are equivalent \cite{proofbook}:
\begin{enumerate}
    \item $\mathcal{V}$ is congruence modular;
    \item $\mathcal{V}$ omits $N_5$;\\
\end{enumerate}
\label{theorem1}
\end{theorem}

\begin{theorem}\textbf{Congruence Distributivity}\\
Let $\mathcal{V}$ be a variety. Then the following are equivalent \cite{proofbook}:
\begin{enumerate}
    \item $\mathcal{V}$ is congruence distributive;
    \item $\mathcal{V}$ omits $N_5$ and $M_3$;\\
\end{enumerate}
\label{theorem2}
\end{theorem}


$N_5$ and $M_3$ are not the only lattices that affect algebraic properties. McKenzie et al. \cite{Mckenzie1970} for example discovered six structures that preclude semi-distributiveness.


\end{comment}

% \subsection{Graph Neural Networks}
% Graph Neural Networks (GNNs, ~\cite{scarselli2008graph}) are differentiable models designed to process relational data in the form of graphs. A graph can be defined as a tuple $G=(V,E)$ which comprises nodes $V = \{1, \dots, n\}$, the entities of a domain, and edges $E \subseteq \{1, \dots, n\} \times \{1, \dots, n\}$, the relations between pairs of nodes. Nodes (or edges) can be endowed with features $\mathbf{x}_i \in \mathbb{R}^d$, representing $d$ characteristics of each entity (or relation), and with $l$ ground truth task labels $y_i \in Y \subseteq \{0, 1\}^l$. A typical GNN $g$ learns a set of node embeddings $\mathbf{h}_i$ with a scheme known as message passing~\cite{gilmer2017neural}. 
% % Specifically, message passing aggregates for each node $i\in V$ local information shared by its neighboring nodes $N_i = \{k: (k,i) \in E \}$:
% % \begin{equation}
% %     \mathbf{h}_i = \sum_{k \in N_i} g(\textbf{m}_{ik}, \mathbf{x}_i) \qquad \textbf{m}_{ik} = \phi(\mathbf{x}_i, \mathbf{x}_k)
% % \end{equation}

% % where $\textbf{m}_{ik}$ is the aggregate of the feature vectors $\textbf{x}_i$ and $\textbf{x}_k$ of nodes $i$ and $k$, respectively, computed using a permutation invariant function $\phi$. 
% A readout function $f: H \rightarrow Y$ then processes the node embeddings to predict node labels $\hat{y}$. GNNs are trained via stochastic gradient descent minimizing the cross entropy loss between $\hat{y}_i$ and ground-truth $y_i$.

\subsection{Graph neural network explainability}


\section{Methods}

As recalled in the previous section, a variety of algebras corresponds to a class of algebras satisfying a common set of identities, and some lattice varieties are characterized by a certain lattice(s) omission (e.g. Theorem \ref{the:n5}-\ref{the:m3}). However, the problem of identifying which is the necessary lattice omission (if any exists!) that characterizes a lattice variety satisfying a certain equational property (like the distributive law) is very difficult. To 
% alleviate this burden
help figuring out which might be a suitable claim for a proof attempt (or simply to get novel insights),
we designed a framework in which an AI agent may support a human mathematician by suggesting as candidates some lattices whose omission is responsible for the satisfaction of a certain algebraic property. In this section we detail how the two main aspects of this process take place, i.e.
(i) the creation of a suitable dataset of lattices satisfying and violating one or more  algebraic properties under investigation (Section \ref{sec:latt_data})
% , the details on this generative process are provided in Section \ref{sec:benchmark}, 
(ii) the definition of the interpretable graph network architecture we used to produce concept-based explanations (Section \ref{sec:IGN}), 


\subsection{Generating Datasets of Lattices}
\label{sec:latt_data}

\begin{minipage}{.4\textwidth}
Our methodology offers a versatile means of exploring various algebraic properties. Specifically, this paper focuses on investigating properties that can be expressed as equations of the form "$\textit{term}_1\approx\textit{term}_2$" (e.g. distributivity) and quasi-equations of the form "if $\textit{equation}_1$ holds then $\textit{equation}_2$ holds" (e.g. semi-distributivity). However, our approach is not limited to these specific cases, as any property whose determinate validity can be verified on a finite lattice can be effectively examined using this methodology. 
\end{minipage}
\hspace{0.01\textwidth}
\begin{minipage}{0.58\textwidth}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[H]
\caption{Generation of distributive lattices}
\label{alg:dataset}
\begin{algorithmic}
\Require $n \geq 1$ \textit{\Comment{$n$: cardinality of  lattices}}
\State $\textit{Lattices} = []$
\State $\textit{AllFuncs}\gets \textit{genAllFunctions}(n)$
\textit{\Comment{all $2^{n\times n}$ binary functions on $\{1,\ldots,n\}^2$ represented with matrices of size $n\times n$}}
\For{$L\in \textit{AllFuncs}$}
        % \State $M \gets \textit{def\_LoE}(i,n)$ \Comment{$\textit{def\_LoE}$ defines the matrix corresponding to the $i$-th indicator function of a partial order $LoE$ on the $n\times n$ nodes.}
        % \Comment{To make this step faster, we only consider functions $LoE$: $LoE(i,j)=1$ only if $i\leq  j$.}
        \If{$\textit{isPartialOrder}(L)$} \textit{\Comment{$L$ should be reflexive, antisymmetric and transitive. Intuitively $L(i,j)=1$ means $i\leq_L j$}}
        % \State $L \gets \textit{trans\_clo}(M)$ \Comment{Transitive closure of M.}
        \If{$\textit{isLattice}(L)$} 
        \textit{\Comment{Any pair
       of nodes in $L$ must have a unique infimum and supremum}}
       \For{$i,j\leq n$}
       \State $\wedge_L[i,j]\gets \sup_{x\leq n} \{ x\leq_L i \mbox{ and } x\leq_L j\}$
\State $\vee_L[i,j]\gets \inf_{x\leq n}\{ i\leq_L x\mbox{ and } j\leq_L x\}$
\EndFor
        \If{$\textit{isDistributive}(L)$} \textit{\Comment{$L$ should satisfy the distributive law with $\wedge_L,\vee_L$}}
        \State $\textit{distributive} = ``True"$
        \Else 
        \State $\textit{distributive} = ``False"$
        \EndIf
        \State $\textit{Lattices}.append([L,\textit{distributive}])$
        \EndIf
        \EndIf
\EndFor
\end{algorithmic}
\end{algorithm}
\end{minipage}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Having said that, to build our dataset we first construct the whole set of lattices up to a certain node cardinality. Then we check the satisfaction of the algebraic properties under investigation for them all, and collect all the resulting labelled samples in a json file. This collection will be the input of the GNN model described in Section \ref{sec:IGN}. 
We notice that automatically checking the validity of a single equation on a medium-size lattice is not necessarily computationally prohibitive, e.g. it would require checking $n^3$ identities for a lattice with $n$ nodes, however the number of existing lattices explodes as $n$ increases. Even if it is difficult to determine exactly the number of lattices of a given size, to give an idea to an interested reader, it is known that there are at least 2,000,000 of non-isomorphic lattices with 10 elements \cite{berman2000counting}. 
For this reason, starting from a certain node cardinality on we only take into account a fixed small numbers of lattice samples per cardinality. 
% \fg{I don't know if saying what follows..}
Despite this may seem as negatively biasing the dataset towards small lattices, we notice that known lattice omissions often rely on lattices with few nodes. In addition, in the experiments section (Table \ref{tab:auc}) we deliberately investigate the generalization capacity of our model in the circumstance of training on just small-size lattices and then predicting on bigger ones. This is also interesting to appreciate the generalization capacity of the model that has learnt how to predict the satisfiability of equational properties on (possibly very) big graph structures without needing to explicitly checking them.
% \fg{Say something about numbering lattice nodes?..maybe unnecessary..}
% Roughly, lattices are special graphs whose relation among nodes is a partial order (e.g. reflexive, antisymmetric and transitive).
A naive sketch of the algorithm we used to generate the lattices' datasets is reported in Algorithm \ref{alg:dataset}\footnote{The datasets generator code will be made public in case of paper acceptance.}. For simplicity, we show how to generate all the lattices with a fixed number of nodes, and
we only take into account the distributive property. In the case the cardinality is too high (e.g. greater than 10), it is enough to upper bound the maximum number of generated lattice samples per cardinality.

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{algorithm}
% \caption{Dataset generation of distributive lattices with fixed cardinality}
% \label{alg:dataset}
% \begin{algorithmic}
% \Require $n \geq 1$ \textit{\Comment{$n$: cardinality of  lattices}}
% \State $\textit{Lattices} = []$
% \State $\textit{AllFuncs}\gets \textit{genAllFunctions}(n)$
% \textit{\Comment{$\textit{AllFuncs}$ being a list of $2^{n\times n}$ matrices of size $n\times n$, containing all the binary functions definable on $\{1,\ldots,n\}^2$.}}
% \For{$L\in \textit{AllFuncs}$}
%         % \State $M \gets \textit{def\_LoE}(i,n)$ \Comment{$\textit{def\_LoE}$ defines the matrix corresponding to the $i$-th indicator function of a partial order $LoE$ on the $n\times n$ nodes.}
%         % \Comment{To make this step faster, we only consider functions $LoE$: $LoE(i,j)=1$ only if $i\leq  j$.}
%         \If{$\textit{isPartialOrder}(L)$} \textit{\Comment{Checking if $L$ is reflexive, antisymmetric and transitive. Intuitively $L(i,j)=1$ means that the node $i$ is less or equal that $j$ wrt $L$ (i.e. $i\leq_L j$).}}
%         % \State $L \gets \textit{trans\_clo}(M)$ \Comment{Transitive closure of M.}
%         \If{$\textit{isLattice}(L)$} 
%         \textit{\Comment{Checking if for each pairs
%        of nodes in $L$, there exist uniquely their infimum and supremum.}}
%        \For{$i,j\leq n$}
%        \State $\wedge_L[i,j]\gets \sup_{x\leq n} \{ x\leq_L i \mbox{ and } x\leq_L j\}$
% \State $\vee_L[i,j]\gets \inf_{x\leq n}\{ i\leq_L x\mbox{ and } j\leq_L x\}$
% \EndFor
%         \If{$\textit{isDistributive}(L)$} \textit{\Comment{Checking if $L$ satisfies the distributive law with $\wedge_L,\vee_L$}}
%         \State $\textit{distributive} = ``True"$
%         \Else 
%         \State $\textit{distributive} = ``False"$
%         \EndIf
%         \State $\textit{Lattices}.append([L,\textit{distributive}])$
%         \EndIf
%         \EndIf
% \EndFor
% \end{algorithmic}
% \end{algorithm}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \fg{Motivating Example?}
% \begin{example}[Dataset]
% In this motivating example, we consider the problem of classifying lattices using a binary classification task. Our dataset is composed only of lattices of dimension up to 100 with supervision indicating if the lattice is modular or not, as well as if it is distributive or not. The goal of the classifier is to accurately predict whether a given lattice belongs to the variety of distributive or modular lattices.
% \end{example}






\subsection{Interpretable Graph Networks (iGNNs)}
\label{sec:IGN}

% Figure environment removed


In this section, we aim to design an interpretable graph network (iGNN, Figure~\ref{fig:ignn}) that satisfies the definition of "interpretability" commonly adopted in the literature~\citep{rudin2019stop}. According to this definition, an ML system is interpretable if and only if (1) its inputs are semantically meaningful, and (2) its model inference is simple for humans to understand. This definition covers ML systems that take tabular data sets or sets of concepts as inputs, and (piece-wise) linear models such as logistic regression or decision trees. To achieve this goal in GNNs, we introduce an interpretable graph layer that learns semantically meaningful concepts and give them as inputs to a simple linear classification layer. We then illustrate how this layer can be incorporated into existing architectures or used to create hierarchical iGNNs (HiGNNs), which consist of a sequence of interpretable graph layers.

\subsubsection{Interpretable Graph Layer}
The interpretable graph layer (Figure~\ref{fig:ignn}) serves three main functions: message passing, concept generation, and task predictions. The first step of the interpretable graph layer involves a standard message passing operation (Eq.~\ref{eq:node-concepts} right), which aggregates information from node neighbors. This operation enables the sharing and processing of relational information across nodes and it represents the basis of any GNN layer.

\paragraph{Node-level concepts}
Generating an interpretable concept space is the first step towards interpretability. According to common definitions~\citep{ghorbani2019towards,magister2021gcexplainer}, a relevant concept is a ``high-level human-understandable unit of information'' (e.g., a door color) which is shared by input samples and can thus be identified using clustering techniques. Message passing algorithms do cluster node embeddings based on the structure of node neighborhoods, as observed by~\citet{magister2021gcexplainer}. However, the real-valued large embedding representations $\mathbf{h}_i$ generated by message passing can be challenging for humans to interpret. To address this, we use a hard Gumbel-Softmax activation $\Theta: \mathbb{R}^q \mapsto \{0,1\}^q$, following~\citet{azzolin2022global}: 
\begin{equation} \label{eq:node-concepts}
\mathbf{c}_i = \Theta \big( \mathbf{h}_i \big) 
\qquad 
\mathbf{h}_i = \phi \Big(\mathbf{x}_i, \bigoplus_{j \in N_i} \psi(\mathbf{x}_i, \mathbf{x}_j)\Big)
\end{equation}
where $\psi$ and $\phi$ are learnable functions aggregating information from a node neighborhood and $\oplus$ is a permutation invariant aggregation function (such as sum or max). During the forward pass, the Gumbel-Softmax activation $\Theta$ produces a one-hot encoded representation of each node embedding. Since nodes sharing the same neighborhood have similar embeddings $\mathbf{h}_i$ due to message passing, they will also have the same one-hot vector $\mathbf{c}_i$ due to the Gumbel-Softmax, and vice versa. We can then interpret nodes having the same one-hot concept $\mathbf{c}_i$ as nodes having similar embeddings $\mathbf{h}_i$ and thus sharing a similar neighborhood. This way, by visualizing the subgraph that depicts the neighborhood, the learned concept representation becomes easily interpretable to humans (Figure~\ref{fig:ignn}), aiding in the understanding of the reasoning process of the network.


\begin{example}[Interpreting node-level concepts]
Consider the problem of classifying distributive lattices with a simplified data set including $N_5$ % Figure removed and $M_3$ % Figure removed only, and where each node has a constant feature $x_i = 1$. One layer of message passing will then generate only two types of node embeddings e.g., $\mathbf{h}_{II} = [0.2, -0.4, 0.3]$ for nodes with a 2-node neighborhood (e.g., % Figure removed), and $\mathbf{h}_{III} = [0.6, 0.2, -0.1]$ for nodes with a 3-node neighborhood (e.g., % Figure removed).
As a consequence, the Gumbel-Softmax can only generate two possible concept vectors e.g., $\mathbf{c}_{II} = [0, 0, 1]$ and $\mathbf{c}_{III} = [1, 0, 0]$. The one-to-one mapping between the structure of the neighborhood and the concept activation makes the learned concepts easily interpretable e.g., $\mathbf{c}_i = [1,0,0] \iff$ % Figure removed or $\mathbf{c}_j = [0,0,1] \iff$ % Figure removed. 
\end{example}

\paragraph{Graph-level concept embeddings (max/add pooling)}
To generate a graph-level concept space in the interpretable graph layer, we can utilize the node-level concept space produced by the Gumbel-Softmax. Normally, graph-level embeddings are generated by applying a permutation invariant aggregation function on node embeddings. However, in iGNNs we restrict the options to (piece-wise) linear permutation invariant functions in order to follow our interpretability requirements dictated by~\citep{rudin2019stop}. However, this restriction still includes common options such as max or sum pooling. Max pooling can easily be interpreted by taking the component-wise max over the one-hot encoded concept vectors $\mathbf{c}_i$. After max pooling, the graph-level concept vector has a value of $1$ at the $k$-th index if and only if at least one node activates the $k$-th concept i.e., $\exists i \in V, \mathbf{c}_{ik} = 1$. Similarly, we can interpret the output of a sum pooling: a graph-level concept vector takes a value $v \in \mathbb{N}$ at the $k$-th index after sum pooling if and only if there are exactly $v$ nodes activating the $k$-th concept i.e., $\exists i_0,\dots,i_v \in V, \mathbf{c}_{ik} = 1$.

\begin{example}[Interpreting graph-level concepts]
Following our example, let's use sum pooling to generate graph-level concepts. For an $N_5$ graph, we have 5 nodes with exactly the same 2-node neighborhood. Therefore, sum pooling generates a graph-level embedding $[0,0,5]$, which certifies that we have 5 nodes of the same type e.g., % Figure removed. For an $M_3$ graph, the top and bottom nodes have a 3-node neighborhood e.g., % Figure removed, while the middle nodes have a 2-node neighborhood e.g., % Figure removed. This means that sum pooling generates a graph-level embedding $[2, 0, 3]$, certifying that we have 2 nodes of type % Figure removed and 3 nodes of type  % Figure removed.
\end{example}

% \paragraph{Concept visualization}

\paragraph{Interpretable classifier}
To ensure prioritization of the most relevant concepts for the GNN classification task, a classifier is needed, as it will predict the task labels using the concept representations. However, using a black box classifier like a multi-layer perceptron (MLP, \cite{mlp}) would not be ideal as it could compromise the interpretability of our model. Instead, an interpretable linear classifier such as a single-layer network (or perceptron, \cite{slp_Karimboyevich_Nematullayevich_2022}) can be used. This allows for a completely interpretable model from the input to the classification head, as the input representations of the classifier are interpretable concepts and the classifier is a simple linear model which is intrinsically interpretable as discussed by \citet{rudin2019stop}. In fact, the weights of the perceptron can be used to identify which concepts are most relevant for the classification task. Hence, the resulting model can be used not only for classification, but also for interpretation and understanding of the problem at hand.

\subsubsection{Interpretable architectures}
The interpretable graph layer can be used to instantiate different types of interpretable GNNs (iGNNs). One approach is to plug this layer as the last message passing layer of a standard GNN architecture: 
\begin{align}
& \hat{y} = f\Big(\boxplus_{i \in V} \ \ \Big(\Theta \Big(\phi^{(L)} \Big(\mathbf{h}_i^{(L-1)}, \bigoplus_{j \in N_i} \psi^{(L)}(\mathbf{h}_i^{(L-1)}, \mathbf{h}_j^{(L-1)})\Big)\Big)\Big)\Big) \\
& \mathbf{h}_i^{(l)} = \phi^{(l)} \Big(\mathbf{h}_i^{(l-1)}, \bigoplus_{j \in N_i} \psi^{(l)}(\mathbf{h}_i^{(l-1)}, \mathbf{h}_j^{(l-1)})\Big)
\quad l = 1,\dots,L
\end{align}
where $f$ is an interpretable model, $\boxplus$ is an interpretable\footnote{Piece-wise linear function according to \cite{rudin2019stop} definition of interpretability.} permutation-invariant function (such as max or sum), $\Theta$ is a Gumbel-Softmax hard activation function, and $\mathbf{h}_i^0 = \mathbf{x}_i$. This way, we can interpret the first part of the network as a feature extractor generating latent representations that are useful for generating high-quality clusters from which concepts can be extracted. This approach is useful when we only care about the most complex neighborhoods/concepts. Another approach is to generate a hierarchical architecture where each GNN layer is interpretable:
\begin{equation}
\hat{y}^{(l)} = f\Big(\boxplus_{i \in V} \Big(\ \Theta \Big(\mathbf{h}_j^{(l)} \Big) \Big)\Big)
\qquad l = 1,\dots,L
\end{equation}
In this case, we can interpret every single layer of our model with concepts of increasing complexity. The concepts extracted from the first layer represent subgraphs representing the 1-hop neighborhood of a node, while at the second layer, they will represent 2-hop neighborhoods and so on. These hierarchical iGNNs can be useful to get insights into concepts with different granularities. By analyzing the concepts extracted at each layer, we can gain a better understanding of a GNN inference and the importance of different structures in the graph for the classification task at hand.

\subsubsection{Training}
When it comes to training interpretable GNNs (iGNNs), we can make use of the standard activation functions such as softmax or sigmoid, along with standard loss functions like cross-entropy. The choice of the activation and loss functions depends on the nature of the task to be performed. In the case of hierarchical iGNNs (HiGNNs), which involve a layered architecture with increasing levels of complexity, we can apply the loss function at each layer of the concept hierarchy. This ensures that each layer is doing its best to extract the most relevant concepts to solve the task at hand. The loss function for each layer can also be weighted differently to prioritize the formation of optimal concepts of a specific size, allowing the HiGNN to learn in a progressive and efficient way. By doing so, we can train the HiGNN in a progressive and effective manner, allowing us to uncover concepts with different granularities, and gain insights into how the model is performing at different levels of abstraction.


\section{Experiments}
\subsection{Research questions}
In this section we analyze the following research questions:
\begin{itemize}
    \item \textbf{Generalization} - How do GNNs generalize when trained to predict universal algebra's properties? Can GNNs strongly generalize i.e., can they perform well on large lattices when trained only with small lattices? Do interpretable GNNs generalize as well?
    \item \textbf{Interpretability} - Do concept-based explanations generated by interpretable GNNs empirically match universal algebra's conjectures? How can concept-based explanations suggest novel conjectures? Can we use interpretable GNNs to provide insights on mutual relationships between lattice properties?
    \item \textbf{Concept Quality} - Are the concepts identified by interpretable GNNs pure? Is the concept space complete?
\end{itemize}

\subsection{Setup}
\paragraph{Baselines}
For our comparative study, we evaluate the performance of interpretable GNNs (iGNNs) and their hierarchical version against equivalent GNN models (i.e., having the same hyperparameters such as number layers, training epochs, and learning rate). For vanilla GNNs we resort to common practice replacing the Gumbel-Softmax with a standard leaky relu activation. We exclude from our baselines prototype or concept-based GNNs pre-defining graph structures for explanations (such as protGNN or GLGExplainer), as for most datasets these structures are unknown. Appendix~\ref{app:baselines} covers implementation details. We show local and post-hoc explanations in Appendix~\ref{app:local-xai} as these techniques are not suitable for a global and exact understanding of a model inference.
% Similarly, we do not consider local explainability methods such as GNNExplainer or integrated gradients, as our focus is to understand the behavior of the GNN on a global scale. 
% We also avoid comparing against post-hoc methods, such as GLGExplainer or GCExplainer~\citep{magister2021gcexplainer}, since we are interested in obtaining the exact and precise reason for the GNN's predictions, without relying on surrogate models to approximate the decision-making mechanism.

\paragraph{Evaluation}
We use three quantitative metrics to assess a model generalization and interpretability. We use the area under the receiver operating characteristic (AUC ROC) curve to assess task generalization. We evaluate generalization under two different conditions: with independently and identically distributed train/test splits, and out-of-distribution by training on graphs up to eight nodes, while testing on graphs with more than eight nodes (``strong generalization''). We further assess generalization under binary and multilabel (classifying 5 properties of a lattice at the same time) classification conditions. To evaluate interpretability, we use standard metrics such as completeness and fidelity. Completeness\footnote{We assess the recall of the completeness as the datasets are very unbalanced towards the negative label.} assesses the quality of the concept space on a global scale using an interpretable model to map concepts to tasks, while fidelity measures the difference in predictions obtained with an interpretable surrogate model and the original model. Finally, we evaluate the meaningfulness of our concept-based explanations by visualizing and comparing the generated concepts with ground truth lattices ($M_3$ and $N_5$), whose omission is known to be significant for modular and distributive properties. All metrics in our evaluation, across all experiments, are computed on test sets using $X$ random seeds, and reported using the mean and $95\%$ confidence interval. 


\section{Key Findings}
% \subsection{Generalization}

% Figure environment removed

\subsection{Generalization}

\paragraph{iGNNs improve interpretability without sacrificing task accuracy (Figure~\ref{fig:comp-fidelity})}
Our experimental evaluation reveals that interpretable GNNs are able to strike a balance between completeness and fidelity, two crucial metrics that are used to assess generalization-interpretability trade-offs. The completeness score evaluates the quality of the concept space, while the fidelity score measures the accuracy of the predictions obtained with an interpretable classifier instead of a black-box. Our findings indicate that iGNNs are capable of achieving optimal fidelity scores, aligning with the existing literature which suggests that a model with fidelity score of $100\%$ is considered to be interpretable. In contrast, interpretable surrogate models of black-box GNNs exhibit lower fidelity scores. Moreover, we observe that the multilabel classification scenario, which requires models to learn a more varied and diverse set of concepts, is the most challenging and results in the lowest completeness scores on average. We also notice that the more challenging out-of-distribution scenario results in the lowest completeness and fidelity scores across all datasets. Overall, these results provide strong empirical evidence for iGNNs interpretability, demonstrate how concept spaces are highly informative to solve universal algebra's tasks, and show how iGNNs may improve GNNs' interpretability without sacrificing task accuracy.


\paragraph{GNNs generalize well on universal algebra's tasks (Table~\ref{tab:auc})}
Our experimental results demonstrate that GNNs exhibit strong generalization capabilities across all tasks in universal algebra. Specifically, we trained the models on graphs of size up to 8 and evaluated their performance on larger graphs, which presents a significantly more challenging task due to the shift in data distribution. Despite this difficulty, our findings reveal that the GNNs achieved an impressive mean accuracy of X\% on the larger graphs. This level of performance indicates that the models are highly robust and able to generalize effectively to previously unseen instances. 
We also observed that interpretable graph networks (iGNNs and HiGNNs) attain high task performance, often outperforming equivalent black-box GNNs, reinforcing the notion that interpretable models can be relied upon to provide reliable and interpretable predictions as suggested by~\citep{rudin2019stop}. Finally, GNNs performed well even in the challenging multilabel case, where they had to learn a wider and more diverse set of concepts and tasks (AUCROC was only ~X\% lower than in binary classification scenarios). Overall, these results highlight the potential of GNNs to tackle challenging problems in universal algebra and provide an effective tool to handle lattices that are difficult to analyze manually on pen and paper.


% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
\begin{table}[!t]
\centering
\caption{Generalization performance of graph neural models in solving universal algebra's tasks. Values represents the mean and the standard error of the mean of the area under the receiver operating curve (AUCROC, \%).}
\label{tab:auc}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcccccc}
\hline
 & \multicolumn{3}{c}{\textbf{weak generalization}} & \multicolumn{3}{c}{\textbf{strong generalization}} \\
\multicolumn{1}{c}{} & \textbf{GNN} & \textbf{iGNN} & \textbf{HiGNN} & \textbf{GNN} & \textbf{iGNN} & \textbf{HiGNN} \\ \hline
\textbf{Distributive} & $99.80 \pm 0.04$ & $99.56 \pm 0.12$ & $99.45 \pm 0.06$ & $99.51 \pm 0.20$ & $99.44 \pm 0.05$ & $99.42 \pm 0.04$ \\
\textbf{Join Semi Distributive} & $99.49 \pm 0.02$ & $98.31 \pm 0.15$ & $98.28 \pm 0.04$ & $98.77 \pm 0.15$ & $97.50 \pm 0.14$ & $97.48 \pm 0.14$ \\
\textbf{Meet Semi Distributive} & $99.52 \pm 0.04$ & $98.19 \pm 0.06$ & $98.25 \pm 0.08$ & $98.90 \pm 0.03$ & $97.18 \pm 0.14$ & $96.89 \pm 0.37$ \\
\textbf{Modular} & $99.77 \pm 0.02$ & $99.18 \pm 0.11$ & $99.35 \pm 0.09$ & $99.32 \pm 0.22$ & $99.21 \pm 0.14$ & $99.11 \pm 0.22$ \\
\textbf{Semi Distributive} & $99.66 \pm 0.03$ & $98.57 \pm 0.02$ & $98.50 \pm 0.06$ & $99.19 \pm 0.04$ & $97.28 \pm 0.48$ & $96.88 \pm 0.47$ \\
\textbf{Multi Label} & $99.60 \pm 0.02$ & $96.32 \pm 0.34$ & $95.98 \pm 0.50$ & $98.62 \pm 0.43$ & $95.29 \pm 0.55$ & $95.27 \pm 0.32$ \\ \hline
\end{tabular}%
}
\end{table}

% \subsection{Interpretability}
% \paragraph{Interpretable GNNs generate intuitive concept-based explanations}
% \missingfigure{\note{ADD EXAMPLES OF CONCEPT-BASED EXPLANATIONS}}

\subsection{Interpretability}
\paragraph{Concept-based explanations empirically validate universal algebra's conjectures (Figure~\ref{fig:explanations})}
We present empirical evidence to support the validity of theorems \ref{the:n5} and \ref{the:m3} by examining the concepts generated for modular and distributive tasks in a binary classification setup. Specifically, we investigate the presence of certain concepts when classifying modular and distributive lattices. For the modularity task, our results show that the lattice $N_5$ appears among non-modular concepts, but is never found in modular lattices, while the lattice $M_3$ appears only among modular concepts, which is consistent with theorem \ref{the:n5}. In the case of distributivity, we observe that both $M_3$ and $N_5$ are present among non-distributive concepts, and are never found in distributive lattices, which is also in line with theorem \ref{the:m3}. These findings provide the first large-scale empirical evidence for the validity of theorems \ref{the:n5} and \ref{the:m3}, and further demonstrate the effectiveness of graph neural networks in learning and analyzing lattice properties. Moreover, our results highlight how interpretable GNNs can not only learn the properties of universal algebra but also identify structures that are unique to one type of lattice (e.g., non-modular) and absent from another (e.g., modular), thus providing human-interpretable explanations for what the models learn.

% % Figure environment removed


\paragraph{Concept-based explanations may suggest novel conjectures (Figure~\ref{fig:explanations})}
Our results provide exciting opportunities for future research in universal algebra. With evidence that our method works (as shown for modular and distributive tasks), we can now use interpretable GNNs to gain insights into other universal algebra tasks that currently lack conjectures. For example, the meet-semi-distributive property currently lacks a conjecture identifying the (set of) lattice(s) whose omission makes a lattice meet-semi-distributive. Using concept visualizations, we can compare the concepts found for lattices that are not meet-semi-distributive against the concepts of lattices that are meet-semi-distributive. Interestingly, we observe $N_5$ but not $M_3$ among the concepts of meet-semi-distributive lattices, while we observe both $N_5$ and $M_3$ only in concepts that are not meet-semi-distributive. This observation suggests that $N_5$ is not a key lattice for meet-semi-distributive lattices, \emph{unlike distributive lattices}. These findings are significant because they demonstrate how analyzing concepts in interpretable GNNs can provide universal algebraists with a powerful and automatic tool to formulate new conjectures based on identifying simple lattices playing a role in specific properties. By leveraging the power of interpretable GNNs, we may uncover previously unknown connections between different properties and identify new patterns and structures that could lead to the development of new conjectures and theorems in universal algebra.

\ste{To review simidistributive example}


% Figure environment removed


% % Figure environment removed

% \paragraph{Interpretable GNNs provide insights on relationships among universal algebra's properties  (Figure~\ref{fig:multilabel})}
% Interpretable GNNs may represent a valuable tool for gaining insights into the relationships among different universal algebra properties. We show this in the multilabel scenario, where the model is trained to learn multiple properties simultaneously. Here the network must identify all the relevant concepts for all tasks at the same time, which makes this task more challenging compared to the binary classification setup. The resulting model provides valuable information on the similarities and differences among different properties based on the learnt concepts. To visualize this, we used a subset of concepts represented by colored circles in the embedding space of an interpretable GNN. The color of each circle corresponds to a unique string of labels shared by all samples in the cluster, and its size is proportional to the number of samples belonging to the concept. The sub-lattice corresponding to the input structure that activates the concept is also displayed on top of the circle. This approach allows us to have a global overview of the concept space and identify the relationships between different properties. For example, we observe that the concept of "non modular and non join semidistributive" might be closely related to the concept of being "non meet semi distributive and non join semidistributive". These concepts are close in the embedding space, and as a consequence, their corresponding lattices are also similar. Overall, these results show that interpretable GNNs offer a powerful tool for analyzing the relationships between universal algebra properties, explaining connections among properties in terms of concepts, which can then be leveraged by universal algebra specialists as a hint when developing new conjectures. 

% % Figure environment removed
% % Figure environment removed



% \subsection{Key Finding 3}

\section{Discussion}
\paragraph{Relations with the state of the art}


While general frameworks in which an AI system interacts with a human in a cycle of producing novel conjectures and refining them \cite{Davies2021}, in this paper we add a new strength in theorem suggesting, by means of sub-graph concept explanations. 

\paragraph{Limitations}
\ste{We are dealing with finite lattices and thus extrapolating explanation only using finite lattice can still produce insight about a given problem but with a possibly difficult "generalization"}
\ste{We are dealing with only explanations of structural type and thus properties that are not structural are not "detached" by our explainer, (change explainer to overcome this limitation)}
\paragraph{Conclusion}
\ste{future developments: 1- Mining structural properties characterizing meaningfull classes of graphs, e.g. Bulatov lemma.
2- Congruence lattices}



%\section*{References}

% References follow the acknowledgments. Use unnumbered first-level heading for
% the references. Any choice of citation style is acceptable as long as you are
% consistent. It is permissible to reduce the font size to \verb+small+ (9 point)
% when listing the references.
% Note that the Reference section does not count towards the page limit.
% \medskip

\bibliographystyle{apalike}
\bibliography{references}

\newpage

\appendix


\section{Algebra definitions}
\label{app:algebra}

\subsection{Formal defintions for Universal Algebra}
Universal algebra is the field of mathematics that studies algebraic structures, which are defined as a set $A$ along with its own collection of operations. An $n$-ary operation on $A$ is a function that takes $n$ elements of $A$ and returns a single element from the set. More formally \cite{BurrisSanka,Jonnson_1967,day_1969}:\\
\begin{definition}\textbf{N-ary function}
For $A$ non-empty set and $n$ nonnegative integer we define $A^0 = \{\emptyset\}$ and, for $n > 0$, $A^n$
is the set of n-tuples of elements from $A$. An $n$-ary operation (or function)
on $A$ is any function $f$ from $A^n$
to $A$; $n$ is the arity (or rank) of $f$. An operation $f$ on $A$ is called an n-ary operation if its arity  is $n$.\\
\end{definition}
\begin{definition}\textbf{Algebraic Structure} An algebra $ \mathcal{A}$ is a pair $ \langle A, F \rangle$ where $A$ is a non-empty set called universe and $F$ is a set of finitary operations on $A$.\\
\end{definition}

Apart from the operations on $A$, an algebra is further defined by axioms, that in the particular case of universal algebras are often of the form of identities. The collection of algebraic structures defined by equational laws are called varieties. \cite{HYLAND2007437}\\
\begin{definition}\textbf{Variety}
A nonempty class K of algebras of type $\mathcal{F}$ is called a variety if it is closed under subalgebras, homomorphic images, and direct products.
\end{definition}

\begin{definition}
\label{def:lattice}
 A \emph{lattice} $\mathbf{L}$ is an algebraic structure composed by a non-empty set $L$ and two binary operations $\vee$ and $\wedge$ satisfying the following axioms and their duals obtained exchanging $\vee$ and $\wedge$:
\begin{align*}
&x \vee y \approx y \vee x  &&\text{(commutativity)}
\\&x \vee (y \vee z) \approx (x \vee y)  &&\text{(associativity)}
\\&x \vee x \approx x  &&\text{(idempotency)}
\\&x \approx x \vee (x \wedge y)  &&\text{(absorption)}
\end{align*}
\end{definition}

% Figure environment removed

Congruence lattices of algebraic structures are partially ordered sets such that every pair of elements has unique supremum and infimum determined by the underlying algebra. This object is important relatively to algebraic structures' properties, many of which can be described by omission or admission of certain subpatterns in a graph.\\
\begin{definition}\textbf{Congruence Lattice}\label{cl}\\
For every algebra $\mathcal{A}$ on the set $A$, the identity relation on $A$, and $A \times A$ are trivial congruences. An algebra with no other congruences is called simple. Let $\mathrm{Con}(\mathcal{A})$ be the set of congruences on the algebra $\mathcal{A}$. Because congruences are closed under intersection, we can define a meet operation: $ \wedge : \mathrm{Con}(\mathcal{A}) \times \mathrm{Con}(\mathcal{A}) \to \mathrm{Con}(\mathcal{A})$ by simply taking the intersection of the congruences $E_1 \wedge E_2 = E_1\cap E_2$. Congruences are not closed under union, however we can define the closure operator of any binary relation $E$, with respect to a fixed algebra $\mathcal{A}$, such that it is a congruence, in the following way: $ \langle E \rangle_{\mathcal{A}} = \bigcap \{ F \in \mathrm{Con}(\mathcal{A}) \mid E \subseteq F \}$. Note that the closure of a binary relation is a congruence and thus depends on the operations in $\mathcal{A}$, not just on the carrier set. Now define $ \vee: \mathrm{Con}(\mathcal{A}) \times \mathrm{Con}(\mathcal{A}) \to \mathrm{Con}(\mathcal{A})$ as $E_1 \vee E_2 = \langle E_1\cup E_2 \rangle_{\mathcal{A}} $. For every algebra $\mathcal{A}$, $(\mathrm{Con}(\mathcal{A}), \wedge, \vee)$ with the two operations defined above forms a lattice, called the congruence lattice of $\mathcal{A}$.
\end{definition}


\begin{definition} \textbf{Subalgebra}
    Let $\mathbf{A}$ and $\mathbf{B}$ be two algebras of the same type. Then $\mathbf{B}$ is a \textit{subalgebra} of $\mathbf{A}$ if $B \subseteq A$ and every fundamental operation of $\mathbf{B}$ is the restriction of the corresponding operation of $\mathbf{A}$, i.e., for each function symbol $f$, $f^{\mathbf{B}}$ is $f^{\mathbf{A}}$ restricted to $\mathbf{B}$.
\end{definition}

\begin{definition} \textbf{Homomorphic image}
    Suppose $\mathbf{A}$ and $\mathbf{B}$ are two algebras of the same type $\mathcal{F}$. A mapping $\alpha : A \rightarrow B$ is called a \textit{homomorphism} from $\mathbf{A}$ to $\mathbf{B}$ if 
    $$
    \alpha f^{\mathbf{A}}(a_1, \dotsc ,a_n) = f^{\mathbf{B}}(\alpha a_1, \dotsc, \alpha a_n)
    $$
    for each n-ary $f$ in $\mathcal{F}$ and each sequence $a_1, \dotsc, a_n$ from $\mathbf{A}$. If, in addition, the mapping $\alpha$ is onto then $\mathbf{B}$ is said to be a homomorphic image of $\mathbf{A}$.
\end{definition}

\begin{definition}\textbf{Direct product}
    Let $\mathbf{A}_1$ and $\mathbf{A}_2$ be two algebras of the same type $\mathcal{F}$. We define the direct product $\mathbf{A}_1 \times \mathbf{A}_2$ to be the algebra whose universe is the set $A_1 \times A_2$, and such that for $f \in \mathcal{F}$ and $a_i \in A_1$, $a_i' \in A_2$, $1 \leq i \leq n$,

    $$
    f^{\mathbf{A}_1 \times \mathbf{A}_2}(\langle a_1, a_1' \rangle, \dotsc , \langle a_n, a_n') = \langle f^{\mathbf{A}_1}(a_1, \dotsc, a_n), f^{\mathbf{A}_2}(a_1', \dotsc, a_n') \rangle
    $$
\end{definition}


\section{Baselines' details}
\label{app:baselines}
In practice, we train all models using eight message passing layers and different embedding sizes ranging from 16 to 64. We train all models for 200 epochs with a learning rate of 0.001. For interpretable models, we set the Gumbel-Softmax temperature to the default value of 1 and the activation behavior to "hard," which generates one-hot encoded embeddings in the forward pass, but computes the gradients using the soft scores. For the hierarchical model, we set the internal loss weight to 0.1 (to score it roughly 10\% less w.r.t. the main loss). Overall, our selection of baselines aims at embracing a wide set of training setups and architectures to assess the effectiveness and versatility of GNNs for analyzing lattice properties in universal algebra. To demonstrate the robustness of our approach, we implemented different types of message passing layers, including graph convolution and GIN. 

\section{Generalization results details}
\label{app:generalization}



\section{Concept completeness and purity}
\label{app:completeness}
Our experimental results demonstrate that interpretable GNNs produce concepts with high completeness and purity, which are standard quantitative metrics used to evaluate the quality of concept-based approaches. Specifically, our approach achieves up to X\% completeness and Y purity, with an average score of Z and W, respectively. The lowest scores are obtained for the multilabel case, which is more challenging as models must learn a wider and more diverse set of concepts. Furthermore, the hierarchical structure of interpretable GNNs enables us to evaluate the quality of intermediate concepts layer by layer. This hierarchy provides insights into why we may need more layers, and it can be used as a valuable tool to find the optimal setup and tune the size of the architecture. Additionally, it can also be used to compare the quality of concepts at different layers of the network. Overall, these results quantitatively assess and validate the high quality of the concepts learned by the interpretable GNNs, highlighting the effectiveness of this approach for learning and analyzing complex algebraic structures.

% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
\begin{table}[!h]
\centering
\caption{Concept completeness scores of graph neural models in solving universal algebra's tasks. Values represents the mean and the standard error of the mean of the area under the receiver operating curve (AUCROC, \%).}
\label{tab:completenes}
\resizebox{.8\textwidth}{!}{%
\begin{tabular}{lcccc}
\hline
 & \multicolumn{2}{c}{\textbf{\textsc{weak completeness}}} & \multicolumn{2}{c}{\textbf{\textsc{strong completeness}}} \\
\multicolumn{1}{c}{\textbf{}} & \textbf{iGNN} & \textbf{HiGNN} & \textbf{iGNN} & \textbf{HiGNN} \\ \hline
\textbf{Distributive} & $77.30 \pm 0.20$ & $76.60 \pm 2.35$ & $78.19 \pm 1.71$ & $73.16 \pm 3.63$ \\
\textbf{Join Semi Distributive} & $85.20 \pm 0.86$ & $86.84 \pm 0.37$ & $81.08 \pm 0.18$ & $79.76 \pm 0.38$ \\
\textbf{Meet Semi Distributive} & $84.21 \pm 0.68$ & $84.34 \pm 1.08$ & $80.86 \pm 1.32$ & $79.68 \pm 0.22$ \\
\textbf{Modular} & $76.98 \pm 0.28$ & $73.77 \pm 3.14$ & $81.36 \pm 0.70$ & $77.61 \pm 2.37$ \\
\textbf{Semi Distributive} & $87.33 \pm 1.16$ & $85.62 \pm 0.27$ & $84.03 \pm 0.89$ & $82.26 \pm 0.21$ \\ \hline
\end{tabular}%
}
\end{table}

\note{ADD TABLE WITH CONCEPT PURITY}


\section{Local and post-hoc explanations}
\label{app:local-xai}

\section{Concepts}
\label{app:concepts-viz}

\subsection{Distributive}
% Figure environment removed
% Figure environment removed


\subsection{Modular}

% Figure environment removed
% Figure environment removed


\subsection{Join Semi Distributive}

% Figure environment removed
% Figure environment removed


\subsection{Meet Semi Distributive}

% Figure environment removed
% Figure environment removed


\subsection{Semi Distributive}

% Figure environment removed
% Figure environment removed




\end{document}


% \section{Submission of papers to NeurIPS 2022}


% Please read the instructions below carefully and follow them faithfully.


% \subsection{Style}


% Papers to be submitted to NeurIPS 2022 must be prepared according to the
% instructions presented here. Papers may only be up to {\bf nine} pages long,
% including figures. Additional pages \emph{containing only acknowledgments and
% references} are allowed. Papers that exceed the page limit will not be
% reviewed, or in any other way considered for presentation at the conference.


% The margins in 2022 are the same as those in 2007, which allow for $\sim$$15\%$
% more words in the paper compared to earlier years.


% Authors are required to use the NeurIPS \LaTeX{} style files obtainable at the
% NeurIPS website as indicated below. Please make sure you use the current files
% and not previous versions. Tweaking the style files may be grounds for
% rejection.


% \subsection{Retrieval of style files}


% The style files for NeurIPS and other conference information are available on
% the World Wide Web at
% \begin{center}
%   \url{http://www.neurips.cc/}
% \end{center}
% The file \verb+neurips_2022.pdf+ contains these instructions and illustrates the
% various formatting requirements your NeurIPS paper must satisfy.


% The only supported style file for NeurIPS 2022 is \verb+neurips_2022.sty+,
% rewritten for \LaTeXe{}.  \textbf{Previous style files for \LaTeX{} 2.09,
%   Microsoft Word, and RTF are no longer supported!}


% The \LaTeX{} style file contains three optional arguments: \verb+final+, which
% creates a camera-ready copy, \verb+preprint+, which creates a preprint for
% submission to, e.g., arXiv, and \verb+nonatbib+, which will not load the
% \verb+natbib+ package for you in case of package clash.


% \paragraph{Preprint option}
% If you wish to post a preprint of your work online, e.g., on arXiv, using the
% NeurIPS style, please use the \verb+preprint+ option. This will create a
% nonanonymized version of your work with the text ``Preprint. Work in progress.''
% in the footer. This version may be distributed as you see fit. Please \textbf{do
%   not} use the \verb+final+ option, which should \textbf{only} be used for
% papers accepted to NeurIPS.


% At submission time, please omit the \verb+final+ and \verb+preprint+
% options. This will anonymize your submission and add line numbers to aid
% review. Please do \emph{not} refer to these line numbers in your paper as they
% will be removed during generation of camera-ready copies.


% The file \verb+neurips_2022.tex+ may be used as a ``shell'' for writing your
% paper. All you have to do is replace the author, title, abstract, and text of
% the paper with your own.


% The formatting instructions contained in these style files are summarized in
% Sections \ref{gen_inst}, \ref{headings}, and \ref{others} below.


% \section{General formatting instructions}
% \label{gen_inst}


% The text must be confined within a rectangle 5.5~inches (33~picas) wide and
% 9~inches (54~picas) long. The left margin is 1.5~inch (9~picas).  Use 10~point
% type with a vertical spacing (leading) of 11~points.  Times New Roman is the
% preferred typeface throughout, and will be selected for you by default.
% Paragraphs are separated by \nicefrac{1}{2}~line space (5.5 points), with no
% indentation.


% The paper title should be 17~point, initial caps/lower case, bold, centered
% between two horizontal rules. The top rule should be 4~points thick and the
% bottom rule should be 1~point thick. Allow \nicefrac{1}{4}~inch space above and
% below the title to rules. All pages should start at 1~inch (6~picas) from the
% top of the page.


% For the final version, authors' names are set in boldface, and each name is
% centered above the corresponding address. The lead author's name is to be listed
% first (left-most), and the co-authors' names (if different address) are set to
% follow. If there is only one co-author, list both author and co-author side by
% side.


% Please pay special attention to the instructions in Section \ref{others}
% regarding figures, tables, acknowledgments, and references.


% \section{Headings: first level}
% \label{headings}


% All headings should be lower case (except for first word and proper nouns),
% flush left, and bold.


% First-level headings should be in 12-point type.


% \subsection{Headings: second level}


% Second-level headings should be in 10-point type.


% \subsubsection{Headings: third level}


% Third-level headings should be in 10-point type.


% \paragraph{Paragraphs}


% There is also a \verb+\paragraph+ command available, which sets the heading in
% bold, flush left, and inline with the text, with the heading followed by 1\,em
% of space.


% \section{Citations, figures, tables, references}
% \label{others}


% These instructions apply to everyone.


% \subsection{Citations within the text}


% The \verb+natbib+ package will be loaded for you by default.  Citations may be
% author/year or numeric, as long as you maintain internal consistency.  As to the
% format of the references themselves, any style is acceptable as long as it is
% used consistently.


% The documentation for \verb+natbib+ may be found at
% \begin{center}
%   \url{http://mirrors.ctan.org/macros/latex/contrib/natbib/natnotes.pdf}
% \end{center}
% Of note is the command \verb+\citet+, which produces citations appropriate for
% use in inline text.  For example,
% \begin{verbatim}
%    \citet{hasselmo} investigated\dots
% \end{verbatim}
% produces
% \begin{quote}
%   Hasselmo, et al.\ (1995) investigated\dots
% \end{quote}


% If you wish to load the \verb+natbib+ package with options, you may add the
% following before loading the \verb+neurips_2022+ package:
% \begin{verbatim}
%    \PassOptionsToPackage{options}{natbib}
% \end{verbatim}


% If \verb+natbib+ clashes with another package you load, you can add the optional
% argument \verb+nonatbib+ when loading the style file:
% \begin{verbatim}
%    \usepackage[nonatbib]{neurips_2022}
% \end{verbatim}


% As submission is double blind, refer to your own published work in the third
% person. That is, use ``In the previous work of Jones et al.\ [4],'' not ``In our
% previous work [4].'' If you cite your other papers that are not widely available
% (e.g., a journal paper under review), use anonymous author names in the
% citation, e.g., an author of the form ``A.\ Anonymous.''


% \subsection{Footnotes}


% Footnotes should be used sparingly.  If you do require a footnote, indicate
% footnotes with a number\footnote{Sample of the first footnote.} in the
% text. Place the footnotes at the bottom of the page on which they appear.
% Precede the footnote with a horizontal rule of 2~inches (12~picas).


% Note that footnotes are properly typeset \emph{after} punctuation
% marks.\footnote{As in this example.}


% \subsection{Figures}


% % Figure environment removed


% All artwork must be neat, clean, and legible. Lines should be dark enough for
% purposes of reproduction. The figure number and caption always appear after the
% figure. Place one line space before the figure caption and one line space after
% the figure. The figure caption should be lower case (except for first word and
% proper nouns); figures are numbered consecutively.


% You may use color figures.  However, it is best for the figure captions and the
% paper body to be legible if the paper is printed in either black/white or in
% color.


% \subsection{Tables}


% All tables must be centered, neat, clean and legible.  The table number and
% title always appear before the table.  See Table~\ref{sample-table}.


% Place one line space before the table title, one line space after the
% table title, and one line space after the table. The table title must
% be lower case (except for first word and proper nouns); tables are
% numbered consecutively.


% Note that publication-quality tables \emph{do not contain vertical rules.} We
% strongly suggest the use of the \verb+booktabs+ package, which allows for
% typesetting high-quality, professional tables:
% \begin{center}
%   \url{https://www.ctan.org/pkg/booktabs}
% \end{center}
% This package was used to typeset Table~\ref{sample-table}.


% \begin{table}
%   \caption{Sample table title}
%   \label{sample-table}
%   \centering
%   \begin{tabular}{lll}
%     \toprule
%     \multicolumn{2}{c}{Part}                   \\
%     \cmidrule(r){1-2}
%     Name     & Description     & Size ($\mu$m) \\
%     \midrule
%     Dendrite & Input terminal  & $\sim$100     \\
%     Axon     & Output terminal & $\sim$10      \\
%     Soma     & Cell body       & up to $10^6$  \\
%     \bottomrule
%   \end{tabular}
% \end{table}


% \section{Final instructions}


% Do not change any aspects of the formatting parameters in the style files.  In
% particular, do not modify the width or length of the rectangle the text should
% fit into, and do not change font sizes (except perhaps in the
% \textbf{References} section; see below). Please note that pages should be
% numbered.


% \section{Preparing PDF files}


% Please prepare submission files with paper size ``US Letter,'' and not, for
% example, ``A4.''


% Fonts were the main cause of problems in the past years. Your PDF file must only
% contain Type 1 or Embedded TrueType fonts. Here are a few instructions to
% achieve this.


% \begin{itemize}


% \item You should directly generate PDF files using \verb+pdflatex+.


% \item You can check which fonts a PDF files uses.  In Acrobat Reader, select the
%   menu Files$>$Document Properties$>$Fonts and select Show All Fonts. You can
%   also use the program \verb+pdffonts+ which comes with \verb+xpdf+ and is
%   available out-of-the-box on most Linux machines.


% \item The IEEE has recommendations for generating PDF files whose fonts are also
%   acceptable for NeurIPS. Please see
%   \url{http://www.emfield.org/icuwb2010/downloads/IEEE-PDF-SpecV32.pdf}


% \item \verb+xfig+ "patterned" shapes are implemented with bitmap fonts.  Use
%   "solid" shapes instead.


% \item The \verb+\bbold+ package almost always uses bitmap fonts.  You should use
%   the equivalent AMS Fonts:
% \begin{verbatim}
%    \usepackage{amsfonts}
% \end{verbatim}
% followed by, e.g., \verb+\mathbb{R}+, \verb+\mathbb{N}+, or \verb+\mathbb{C}+
% for $\mathbb{R}$, $\mathbb{N}$ or $\mathbb{C}$.  You can also use the following
% workaround for reals, natural and complex:
% \begin{verbatim}
%    \newcommand{\RR}{I\!\!R} %real numbers
%    \newcommand{\Nat}{I\!\!N} %natural numbers
%    \newcommand{\CC}{I\!\!\!\!C} %complex numbers
% \end{verbatim}
% Note that \verb+amsfonts+ is automatically loaded by the \verb+amssymb+ package.


% \end{itemize}


% If your file contains type 3 fonts or non embedded TrueType fonts, we will ask
% you to fix it.


% \subsection{Margins in \LaTeX{}}


% Most of the margin problems come from figures positioned by hand using
% \verb+\special+ or other commands. We suggest using the command
% \verb+\includegraphics+ from the \verb+graphicx+ package. Always specify the
% figure width as a multiple of the line width as in the example below:
% \begin{verbatim}
%    \usepackage[pdftex]{graphicx} ...
%    % Figure removed
% \end{verbatim}
% See Section 4.4 in the graphics bundle documentation
% (\url{http://mirrors.ctan.org/macros/latex/required/graphics/grfguide.pdf})


% A number of width problems arise when \LaTeX{} cannot properly hyphenate a
% line. Please give LaTeX hyphenation hints using the \verb+\-+ command when
% necessary.


% \begin{ack}
% Use unnumbered first level headings for the acknowledgments. All acknowledgments
% go at the end of the paper before the list of references. Moreover, you are required to declare
% funding (financial activities supporting the submitted work) and competing interests (related financial activities outside the submitted work).
% More information about this disclosure can be found at: \url{https://neurips.cc/Conferences/2022/PaperInformation/FundingDisclosure}.


% Do {\bf not} include this section in the anonymized submission, only in the final paper. You can use the \texttt{ack} environment provided in the style file to autmoatically hide this section in the anonymized submission.
% \end{ack}