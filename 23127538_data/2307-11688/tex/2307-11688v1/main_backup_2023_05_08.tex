\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2022


% ready for submission
\usepackage{neurips_2023}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2022}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2022}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2022}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}
\usepackage{todonotes}
\usepackage{caption}
\usepackage{comment}
\usepackage{subcaption}
\usepackage{amsthm}
% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{booktabs}            % professional-quality tables
\usepackage{multirow}            % tabular cells spanning multiple rows
\usepackage{amsfonts}            % blackboard math symbols
\usepackage{graphicx}            % figures
\usepackage{duckuments}          % sample images
\usepackage{dsfont}
\usepackage{natbib}
\usepackage{scalerel}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
% \theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}


\newcommand{\note}[1]{\textcolor{red}{Note: #1}}
\newcommand{\fg}[1]{\textcolor{blue}{F: #1}}
\newcommand{\ste}[1]{\textcolor{green}{Ste: #1}}


\title{Interpretable Graph Networks Frame Evidences\\for Universal Algebra Conjectures}
% Interpretable Graph Networks Bridge Equational and Topological Representations in Universal Algebra
% "Exploring the Relationship between Equational and Topological Properties in Universal Algebra with Interpretable Graph Networks"
% "Interpretable Graph Networks Reveal Connections between Equational and Topological Properties in Universal Algebra"
% "Bridging Equational and Topological Properties in Universal Algebra with Interpretable Graph Networks"
% "Interpretable Graph Networks Unveil Correlations between Equational and Topological Properties in Universal Algebra"
% "Connecting Equational and Topological Properties in Universal Algebra using Interpretable Graph Networks"
% "A Graph-Based Approach to Understanding the Relationship between Equational and Topological Properties in Universal Algebra"
% "Using Interpretable Graph Networks to Uncover the Links between Equational and Topological Properties in Universal Algebra"
% "Visualizing the Relationship between Equational and Topological Properties in Universal Algebra with Interpretable Graph Networks"
% "Investigating Equational and Topological Properties in Universal Algebra with Interpretable Graph Networks"
% "Interpretable Graph Networks: Revealing the Relationship between Equational and Topological Properties in Universal Algebra".
% "Exploring Universal Algebra with Interpretable Graph Networks"
% "Unveiling Algebraic Properties through Interpretable Graph Networks"
% "Using Interpretable Graph Networks to Understand Universal Algebra"
% "Graph Networks Shed Light on Algebraic Properties in Universal Algebra"
% "Interpretable Graph Networks: A Tool for Revealing Algebraic Properties in Universal Algebra"
% "Discovering Universal Algebraic Properties with Interpretable Graph Networks"
% "Interpretable Graph Networks Uncover Hidden Patterns in Universal Algebra"
% "Visualizing Algebraic Properties with Interpretable Graph Networks"
% "Insights into Universal Algebra through Interpretable Graph Networks"
% "Interpretable Graph Networks: A New Approach to Studying Universal Algebra"


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  David S.~Hippocampus\thanks{Use footnote for providing further information
    about author (webpage, alternative address)---\emph{not} for acknowledging
    funding agencies.} \\
  Department of Computer Science\\
  Cranberry-Lemon University\\
  Pittsburgh, PA 15213 \\
  \texttt{hippo@cs.cranberry-lemon.edu} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}


\begin{document}


\maketitle


\begin{abstract}
  Last thing to do
  %
This paper proposes an end-to-end differentiable model to validate and suggest new UA conjectures. 
\end{abstract}

\section{Introduction}
\begin{itemize}
    \item General definition of the problem:
    \fg{AI for mathematics: general goal (produce counter-example, introduce conjecture,....)}
    \item Existing solutions and knowledge gaps
    \fg{Specific solutions: RL, diff equa, theorem proving, geometry-topological, gropu theory, ecc, THEN: collaboration with mathematicians (Davis etc..), Hence: us for UA:}
%Why UA?
The theory of universal algebra has applications in computer science, logic, and other branches of mathematics, such as topology and geometry. It provides a framework for studying the properties of algebraic structures in a systematic and general way, and for developing algorithms and software for manipulating these structures. 
Interestingly, several properties can be described by relying on the underlined graph representation of these algebraic structures, hence allowing the exploitation of well-suited AI-based models excelling on graph-structured data. In this regard, Graph Neural Networks (GNN) represent a good candidate for graph classifications, and recently some advances have been made introducing novel interpretable concept-based GNNs capable of automatically extract the relevant concepts for a classification task \cite{magister2021gcexplainer,passerini?}.
    
    \item Visual summary of the paper: problem, solution, results
    \fg{The idea sketced in general? Like, graph property -> gen. dataset -> train gnn classifier -> use gnn theorem suggester}

We introduce a new framework where human mathematicians can benefit from explainable AI agent as theorem suggester. In particular, exploiting interpretable GNNs to hightlight relevant portions of graphs responsible for the failure of an equational property. 
    
    \item Our solution and contributions
    \fg{We investigate the properties x,y,.., by combining GNN+GCE, we expect results; to evaluate we realized the benchmark;}
    
    The main contribution of the paper are the following:
    \begin{itemize}
    \item We propose a novel methodology that  given a certain equational property builds a dataset of graph-based algebraic structures, and then train an interpretable GNN  to conjecture  which is the sub-portion of the graph responsible for the failure of such a property. 
    \item We introduce a new concept-based interpretable graph neural model trained to suggest new conjectures to solve universal algebra's open problems.
    
    \item We create and release\footnote{Hidden for double blind submission.} the first dataset of lattices in PyTorch labelled with different algebraic properties expressed by equations and quasi-equations.
    \item We proposed the first benchmark in universal algebra for AI agents, and discuss a manifesto of a (possibly very large) class of problems in this field that can be experimented by our model.
        \item  Our method shows to be capable of verifying well-established results in UA and proposing new insightful conjectures.
    \end{itemize}
    \fg{to clean..}
\end{itemize}



% Figure environment removed








\section{Background}
\label{sec:back}

\fg{non-algebra background to add}

\subsection{Universal Algebra}
\label{sec:ua}
% \todo[@Stefano, @Francesco]

The field of Universal Algebra (UA) is a branch of mathematics concerning the study of algebraic structures from a general and abstract perspective. \emph{Algebraic structures} are typically represented as ordered pairs $\mathbf{A} = (A, F)$, consisting of a set $A$ and a collection of operations $F$ defined on that set. The operations may include binary operations (such as addition or multiplication), unary operations (such as negation or inverse), nullary operations (constants), and so forth. UA aims to identify and explore common algebraic properties shared by various mathematical systems often express as set of equations. \emph{Varieties} are classes of algebraic structures sharing a common set of identities. By specifying these identities, varieties enable the classification and study of algebraic systems based on their common properties. %They form the foundation for exploring the fundamental principles and interrelationships within the field of Universal Algebra. 
Prominent instances of varieties that have been extensively studied across various academic fields encompass Groups, Rings, Boolean Algebras, Fields, and many others. We refer the reader to \cite{burrisSanka} for foundational notions of UA.%Universal Algebra has applications in various areas of mathematics and beyond. It provides a foundation for understanding the structure and properties of algebraic systems in fields like computer science, physics, and engineering. It also serves as a fundamental tool in the study of mathematical logic, model theory, and formal languages.
\subsection{Lattices} 
\label{sec:latt}

A particularly relevant variety of algebras are Lattices, which are studied for their connection with logical structures as well as for their notable algebraic properties. 

\begin{definition}
 A \emph{lattice} $\mathbf{L}$ is an algebraic structure composed by a non-empty set $L$ and two binary operations $\vee$ and $\wedge$ satisfying the following axioms and their duals obtained exchanging $\vee$ and $\wedge$:
\begin{align*}
&x \vee y \approx y \vee x  &&\text{(commutativity)}
\\&x \vee (y \vee z) \approx (x \vee y)  &&\text{(associativity)}
\\&x \vee x \approx x  &&\text{(idempotency)}
\\&x \approx x \vee (x \wedge y)  &&\text{(absorption)}
\end{align*}
\end{definition}
%class of lattice class of lattices is studied both for is connection with logical structures and for its algebraic properties. 

Essentially a lattice is a partially ordered set in which every pair of elements has a \emph{supremum} and an \emph{infimum}.  Lattices can be depicted through their \emph{Hasse diagrams} which are a graphical representation of a lattice. In an Hasse diagram, the elements of the lattice are represented by nodes, and the ordering relation between them is represented by directed edges. Specifically, if there is an edge from node $x$ to node $y$, then $x \leq y$ in the ordering of the lattice, see Figure \ref{fig:n5m3} for concrete examples of Hasse diagrams. Consequently, Hasse diagrams can be understood as graphs. A \emph{sublattice} $\mathbf{L}'$ of a lattice $\mathbf{L}$ is a lattice such that $L' \subseteq L$ and $\mathbf{L}'$ preserves the original order of $L$, i.e. for all $a, b \in L'$ then $a \leq b$ in $L'$ if and only if $a \leq b$ in $L$. Intuitively, a sublattice is a substructure of a lattice that preserves the essential structure of the original lattice.

% Figure environment removed

\begin{comment}
% Figure environment removed 

\stefano{Insert an example}
\end{comment}


%Another fundamental notion for algebraic structure are \emph{congruences}, namely the equivalence relations on $A$ compatible with the operations of $\mathbf{A}$. The set of all congruences of an algebra $\mathbf{A}$ forms a lattice with the order given by set-theretic inclusion and serves as a powerful tool in the study of universal algebra, enabling a deeper understanding of the interplay between congruences and the algebraic operations within a given structure.



%These structures appear as a fundamental aspect congruence lattices of a generic algebra $\mathbf{A}$. In summary, a congruence lattice is a lattice that represents the relationships and properties of \emph{congruences} on an algebraic structure, . It serves as a powerful tool in the study of universal algebra, enabling a deeper understanding of the interplay between congruences and the algebraic operations within a given structure.

%In fact, for every algebra $\mathbf{A}$ the set of its \emph{congruences}, namely the equivalence relations on $A$ compatible with the operations of $\mathbf{A}$, form a lattice structure. Congruence lattices are of interest  since they encode many relevant properties of the original algebras as structural properties of lattice. \fg{I'd rephrase for clarity}



%An interesting problem in algebra is the connection between the internal structure of an algebra and the identities which it satisfies. The study of varieties of algebras provides some insight into this problem.

%Important concepts often used to investigate such abstract problems are the so called congruence lattices of algebraic structures. Namely, those objects are graphs representing partially ordered sets such that every pair of elements has unique supremum and infimum determined by the underlying algebra. Several results in the literature witness the importance of this notion relatively to properties satisfied by algebraic structure, many of which can be described by omission or admission of certain subpatern in a graph. The genesis of this abstract branch of Algebra can be tracked to the pioneeristic work of Dedekind, which studied the classical problem of the admission of lattices as congruence sublattices of a given class of algebraic structures. In this paper our aim is to deal with these type of questions using tools provided by artificial intelligence. Intuitively these kind of problems reduce to identify graph subpatterns that witness structural properties of the whole graph and thus it lends itself to be attached from a neural prospective with architectures like Graph Neural Networks.

The field of lattice varieties emerged as a branch of inquiry stemming from the investigation of general varieties, a subject initially introduced by Garrett Birkhoff in the 1930s. Birkhoff's pioneering contributions provided the first notable advancements in this field, which was subsequently expanded upon by Alfred Tarski and, for congruence distributive varieties, by Bjarni Jonsson. The foundational work of Tarski and Jonsson played a significant role in establishing the groundwork for the development of a wide range of results concerning lattice varieties. Some significant varieties of lattices can be characterized through the omission of one or more lattices. Specifically, a variety $\mathcal{V}$ of lattices is said to \emph{omit} a lattice $\mathbf{L}$ if the latter cannot be identified as a sublattice of any lattice in $\mathcal{V}$. This exclusion criterion serves as a useful means of characterizing and classifying different types of lattice varieties.

\begin{definition}
    Let $\mathbf{L}$ be a lattice. Then $\mathbf{L}$ is \emph{modular} (\emph{distributive}) if it satisfies the following equation:
    \begin{align*}
        &x \leq y \rightarrow x \vee (y \wedge z) \approx y \wedge (x \vee z) &&\text{(modularity)}
        \\&x \vee (y \wedge z) \approx (x \vee y) \wedge (x \vee z) &&\text{(distributivity)}
    \end{align*}
\end{definition}

As we can see from Figure \ref{fig:n5m3} $\mathbf{N}_5$ is not modular taking the substitution $x = a, y= b, z= c$ and the same substitution shows that $\mathbf{M}_3$ is not distributive. The classes of distributive and modular lattices form two distinct varieties showing a classical examples of characterization of lattice variety through lattice omissions. 

\begin{theorem}[Dedekind]\label{the:n5}
Let $\mathcal{V}$ be a lattice variety. Then $\mathcal{V}$ is modular variety if and only if $\mathcal{V}$ omits $\mathbf{N}_5$.
\end{theorem}

\begin{theorem}[Birkhoff]\label{the:m3}
Let $\mathcal{V}$ be a lattice variety. Then $\mathcal{V}$ is distributive variety if and only if $\mathcal{V}$ omits $\mathbf{N}_5$ and $\mathbf{M}_3$.
\end{theorem}

Starting from these classic results, the investigation of lattice omissions and the structural characterizations of classes of lattices has evolved into a rich and extensively studied field. For a more in-depth exploration of this subject, we refer the reader to \cite{JipsenRose}.

\fg{todo: add theorem for the properties/varieties that can be described as lattices omission}

For the record, we only mention here that similar characterization of varieties in terms of lattices' omission have been produced for the lattice of congruences of a certain variety \fg{add reference or theorem} \cite{}. Hence, the method we describe in the next section could be further investigated in this direction. However, as it is not a trivial extension, we plan to extend our approach for this probelm in future work.

\begin{comment}
\subsection{Basics of Universal Algebra}
% \todo[@Stefano, @Francesco]


Universal Algebra (UA) is a branch of Mathematics that studies algebraic structures in a general and abstract way, without restricting to particular operations or sets of elements. In particular, it deals with the study of algebraic structures that are defined purely in terms of their operations and the axioms governing them.
%
The main focus of UA is to study algebraic structures that are common to many different areas of mathematics, such as groups, rings, fields, lattices, and Boolean algebras. It also deals with the study of the properties of algebraic structures that are preserved under various operations, such as homomorphisms and substructures.


\paragraph{Equational Properties and Lattices' Omission}

The field of General Algebra concerns the study of classes of algebraic structures that share relevant properties. An \emph{algebraic structure} is a pair $\mathbf{A} = (A, F)$ where $A$ is a nonempty set and $F$ is a set of operations on $A$. Famous examples of algebraic structure studied in various fields are Groups, Rings, Boolean Algebras, Fields, and so forth. A particularly relevant class of algebraic structure are Lattices which are studied both for their  connection with logical structures and for their algebraic properties. 

\begin{definition}
 A \emph{lattice} $\mathbf{L}$ is an algebraic structure composed of a non-empty set $L$ and two binary operations $\vee$ and $\wedge$ satisfying the following axioms and their duals obtained exchanging $\vee$ and $\wedge$:
\begin{align*}
&x \vee y \approx y \vee x  &&\text{(commutativity)}
\\&x \vee (y \vee z) \approx (x \vee y)  &&\text{(associativity)}
\\&x \vee x \approx x  &&\text{(idempotency)}
\\&x \approx x \vee (x \wedge y)  &&\text{(absorption)}
\end{align*}
\end{definition}
%class of lattice class of lattices is studied both for is connection with logical structures and for its algebraic properties. 

Essentially a lattice is a partially ordered set in which every pair of elements has a \emph{supremum} and an \emph{infimum}. These structures appear as congruence lattices of a generic algebra $\mathbf{A}$. In fact, for every algebra $\mathbf{A}$ the set of its \emph{congruences}, namely the equivalence relations on $A$ compatible with the operations of $\mathbf{A}$, form a lattice structure. Congruence lattices are of interest  since they encode many relevant properties of the original algebras as structural properties of lattice.

An interesting problem in algebra is the connection between the internal structure of an algebra and the identities which it satisfies. The study of varieties of algebras provides some insight into this problem.

%Important concepts often used to investigate such abstract problems are the so called congruence lattices of algebraic structures. Namely, those objects are graphs representing partially ordered sets such that every pair of elements has unique supremum and infimum determined by the underlying algebra. Several results in the literature witness the importance of this notion relatively to properties satisfied by algebraic structure, many of which can be described by omission or admission of certain subpatern in a graph. The genesis of this abstract branch of Algebra can be tracked to the pioneeristic work of Dedekind, which studied the classical problem of the admission of lattices as congruence sublattices of a given class of algebraic structures. In this paper our aim is to deal with these type of questions using tools provided by artificial intelligence. Intuitively these kind of problems reduce to identify graph subpatterns that witness structural properties of the whole graph and thus it lends itself to be attached from a neural prospective with architectures like Graph Neural Networks.


\paragraph{Relevant properties: Congruence Modularity and Distributivity} \label{sec:latt}
A fundamental problem in Universal Algebra is determining the properties of an algebraic variety $\mathcal{V}$ - a nonempty class of algebras of type $\sigma$, closed under homomorphic images, direct products, and subalgebras, (cfr. appendix \ref{aa}). Some varieties of lattices can be characterised through the omission of one or more lattices in the congruence lattice (cfr. appendix \ref{aa}) of its algebras.\\

% % Figure environment removed

\fg{Definition of Modularity and Distributivity to be given as equations}

\fg{To split definition from comments!}

\begin{definition} \textbf{Congruence Modularity} \\
In abstract algebra, a congruence relation is an equivalence relation on an algebraic structure such that algebraic operations done with equivalent elements will yield equivalent elements (compatibility). Every congruence relation has a corresponding quotient structure, whose elements are the equivalence classes (or congruence classes) for the relation. \cite{hungerford}\\
\end{definition}

\begin{definition} \textbf{Congruence Distributivity}\\
An algebra is congruence distributive if its lattice of congruence relations is a distributive lattice - i.e. is a lattice in which the operations of join and meet distribute over each other. \cite{hungerford}
Congruence distributivity has many structural consequences, the most important being JÃ³nsson's Lemma 1 \cite{jonson} which implies that a finitely generated congruence distributive variety is residually finite.\\
\end{definition}
\fg{I'd say explicitely that is the lattice of congruences of the variety to omit N5. It's enough a note}
\begin{theorem}\textbf{Congruence Modularity} \\
Let $\mathcal{V}$ be a variety. Then the following are equivalent \cite{proofbook}:
\begin{enumerate}
    \item $\mathcal{V}$ is congruence modular;
    \item $\mathcal{V}$ omits $N_5$;\\
\end{enumerate}
\label{theorem1}
\end{theorem}

\begin{theorem}\textbf{Congruence Distributivity}\\
Let $\mathcal{V}$ be a variety. Then the following are equivalent \cite{proofbook}:
\begin{enumerate}
    \item $\mathcal{V}$ is congruence distributive;
    \item $\mathcal{V}$ omits $N_5$ and $M_3$;\\
\end{enumerate}
\label{theorem2}
\end{theorem}


$N_5$ and $M_3$ are not the only lattices that affect algebraic properties. McKenzie et al. \cite{Mckenzie1970} for example discovered six structures that preclude semi-distributiveness.


\end{comment}

% \subsection{Graph Neural Networks}
% Graph Neural Networks (GNNs, ~\cite{scarselli2008graph}) are differentiable models designed to process relational data in the form of graphs. A graph can be defined as a tuple $G=(V,E)$ which comprises nodes $V = \{1, \dots, n\}$, the entities of a domain, and edges $E \subseteq \{1, \dots, n\} \times \{1, \dots, n\}$, the relations between pairs of nodes. Nodes (or edges) can be endowed with features $\mathbf{x}_i \in \mathbb{R}^d$, representing $d$ characteristics of each entity (or relation), and with $l$ ground truth task labels $y_i \in Y \subseteq \{0, 1\}^l$. A typical GNN $g$ learns a set of node embeddings $\mathbf{h}_i$ with a scheme known as message passing~\cite{gilmer2017neural}. 
% % Specifically, message passing aggregates for each node $i\in V$ local information shared by its neighboring nodes $N_i = \{k: (k,i) \in E \}$:
% % \begin{equation}
% %     \mathbf{h}_i = \sum_{k \in N_i} g(\textbf{m}_{ik}, \mathbf{x}_i) \qquad \textbf{m}_{ik} = \phi(\mathbf{x}_i, \mathbf{x}_k)
% % \end{equation}

% % where $\textbf{m}_{ik}$ is the aggregate of the feature vectors $\textbf{x}_i$ and $\textbf{x}_k$ of nodes $i$ and $k$, respectively, computed using a permutation invariant function $\phi$. 
% A readout function $f: H \rightarrow Y$ then processes the node embeddings to predict node labels $\hat{y}$. GNNs are trained via stochastic gradient descent minimizing the cross entropy loss between $\hat{y}_i$ and ground-truth $y_i$.

% \subsection{Concept-based explainability}


\section{Methods}


We present a general methodology to semi-automate the process of generating conjectures for universal algebra. The methodology consists of a close collaboration between a human expert in universal algebra and an interpretable graph neural network. The proposed workflow allows experts to convert algebraic properties expressed by (quasi-)equations into datasets suitable for machine learning. This newly created dataset is later be fed to an interpretable GNN, and it is possible to visualize concept-based explanations yielded by the interpretable model, providing feedback to the human expert. The expert can then refine the conjectures, tweak the dataset, and restart the loop. \fg{Add trying to prove the theorem suggested by the conjecture?}
In this section, we describe the human workflow and the generative algorithm to create the datasets (Section \ref{sec:benchmark}), followed by a description of the machine workflow (Section \ref{sec:IGN}) that outlines the interpretable graph network architecture, training process, and generation of concept-based explanations.


\subsection{Generating Datasets of Lattices}
\label{sec:benchmark}


As we recalled in Section \ref{sec:back}, a variety of algebras corresponds to a class of algebras satisfying a common set of identities, and some lattice varieties are characterized by a certain lattice(s) omission (e.g. Theorem \ref{the:n5}-\ref{the:m3}). However, the problem of identifying which is the necessary lattice omission (if any exists!) that characterizes a lattice variety satisfying a certain equational property (like the distributive law) is very difficult. To 
% alleviate this burden
help figuring out which might be a suitable claim for a proof attempt (or simply to get novel insights),
we designed a framework in which an AI agent may support a human mathematician by suggesting some lattices as candidates whose omission is responsible for the satisfaction of a certain algebraic property. The workflow takes place according to the following stepsprocess takes place

\paragraph{Generating a Dataset of Lattices}







% In order to test our model on specific properties, we built a suitable dataset of algebraic structures satisfying and violating the algebraic properties under investigation.

\begin{itemize}
    \item motivating example(s):
    \ste{What is a lattice of type $N_5$? To Rewrite}
\begin{example}[Dataset]
In this motivating example, we consider the problem of classifying lattices using a binary classification task. Our data set is composed only of lattices of type $N_5$ % Figure removed or $M_3$ % Figure removed, and each node $i \in V$ of every graph in the data set is assigned a constant feature $x_i = 1$. The task of the classifier is to distinguish between $N_5$ and $M_3$ lattices based on their graph structures.
\end{example}
    \item algorithm
\end{itemize}




\subsection{Interpretable Graph Networks (iGNNs)}
\label{sec:IGN}

% Figure environment removed


In this section, we describe an interpretable graph network (iGNN). We start by discussing an interpretable graph layer, which is the main component of an iGNN, and illustrate how this layer generates stable concept representations during training and makes interpretable task predictions. Next, we explain how this layer can be integrated into existing architectures or used to create hierarchical iGNNs (HiGNNs) that consist of a sequence of interpretable graph layers. Finally, we discuss the training of iGNNs and their main hyperparameters.

\subsubsection{Interpretable Graph Layer}
Here we describe an interpretable graph layer which is the fundamental building block of an iGNN. This layer serves three main functions: (i) it performs a message passing operation where it aggregates information from neighboring nodes to update the current node's feature representation, (ii) it generates interpretable concept spaces which capture meaningful patterns in the data, and (iii) it makes task predictions using concept activations through an interpretable model. We will start the description of this layer using a simple motivating example.




%\begin{example}[Data set]
%In this motivating example, we consider the problem of classifying lattices using a binary classification task. Our data set is composed only of lattices of type $N_5$ % Figure removed or $M_3$ % Figure removed, and each node $i \in V$ of every graph in the data set is assigned a constant feature $x_i = 1$. The task of the classifier is to distinguish between $N_5$ and $M_3$ lattices based on their graph structures.
%\end{example}

\paragraph{Message passing}
The first step of the interpretable graph layer involves a message passing operation, which aggregates information from node neighbors. This operation enables the sharing and processing of relational information across nodes and it is a crucial step in GNNs.
Specifically, message passing aggregates for each node $i\in V$ local information shared by its neighboring nodes $N_i = \{k: (k,i) \in E \}$:
\begin{equation}
    \mathbf{h}_i^{(1)} = \phi \Big(\mathbf{x}_i, \bigoplus_{j \in N_i} \psi(\mathbf{x}_i, \mathbf{x}_j)\Big)
    % f(\mathbf{x}_i) = \phi \Big(\mathbf{x}_i, \bigoplus_{j \in N_i} \psi(\mathbf{x}_i, \mathbf{x}_j)\Big)
\end{equation}
where $\psi$ and $\phi$ are learnable functions  aggregating information from a node neighborhood and $\bigoplus$ is a permutation invariant aggregation function (such as sum or max).
% The interpretable graph layer does not rely on any specific algorithm for message passing, such as graph convolution or graph attention, but the choice of algorithm may affect the level of expressivity of the layer. However, it is important to note that the interpretability of the layer is not affected by the algorithm used.

\begin{example}[Node embeddings]
Considering the dataset of the previous example. Assume we are using a single layer of graph convolution for message passing with an embedding size of 3. As all nodes contain exactly the same feature ($x_i=1 \ \forall i \in V$), then the output of graph convolution depends on the structure of the 1-hop neighborhood \textbf{only}. Notice that $N_5$ contains only 2-node neighborhoods e.g., % Figure removed, while $M_3$ lattices have both 2-node and 3-node neighborhoods e.g., % Figure removed and % Figure removed, respectively. Thanks to message passing, all nodes sharing the same neighborhood will share the same embedding representation. This means that we can only get two different embeddings as the output of this operation e.g., $\mathbf{h}_{II} = [0.2, -0.4, 0.3]$ for nodes with a 2-node neighborhood, and $\mathbf{h}_{III} = [0.6, 0.2, -0.1]$ for nodes with a 3-node neighborhood.
\end{example}

\paragraph{Node-level concepts (hard Gumbel-Softmax)}
The first step towards interpretability is to generate an interpretable concept space. It is possible to identify concepts in the embedding space of a neural network by looking for clusters. In fact, a cluster in the embedding space identifies a shared property of the input instances found by the network. Interestingly, we notice that message passing is already clustering nodes according to the structure of the neighborhood. However, the real-valued large embedding representations generated by message passing operations are not easy to interpret for humans. To make the a message passing layer more interpretable, we generate explicit cluster assignments withing the GNN architecture using a hard Gumbel-Softmax activation $\Theta$:




\begin{equation}
    \Theta = \begin{cases} 
    \mathbf{a}_i = \mathds{1}_{\scaleto{\arg \max_j [\textbf{s}_j + \log \mathbf{h}_{ij}]}{8pt}} \quad \textit{(hard activation for forward pass)}\\[10pt]
    \mathbf{a}_{ij} = \frac{\scaleto{\exp(\textbf{s}_j + \log \mathbf{h}_{ij}) / \tau}{8pt}}{\scaleto{\sum_{r=1}^q \exp(\textbf{s}_r + \log \mathbf{h}_{ir}) / \tau}{8.5pt}} \quad \textit{(soft activation for backward pass)}
    \end{cases}
\end{equation}
where $\mathds{1}_r$ is a one-hot encoding function returning a vector of the same size of $\mathbf{h}_i$ with the $r$-th entry equal to $1$, $s_1,\dots,s_r$ are i.i.d samples drawn from $\text{Gumbel}(0, 1)$, and $\tau$ is the softmax temperature. As $\tau$ approaches $0$, samples from the Gumbel-Softmax distribution become one-hot and the Gumbel-Softmax distribution becomes identical to a categorical distribution.
This activation generates a one-hot encoded representation of each node embedding in the forward pass (while it allows to backpropagate the gradients using soft scores). This allows humans to easily interpret each node embedding by looking at the non-zero index of the one-hot vector, which represents the cluster assignment. Moreover, since all of the nodes sharing the same neighborhood share the same embeddings (thanks to message passing), all such nodes will have the same one-hot vector. As a result, the neighborhood of all nodes with the same one-hot vector is the same for all such nodes. Considering this, the output of the hard Gumbel-Softmax is interpreted as a form of "\emph{latent concept label}" representing a specific subgraph. The simple visualization of the subgraph depicting the neighborhood makes the learnt concept representation immediately interpretable to humans, facilitating the understanding of the reasoning process of the network.

\begin{example}[Interpreting node-level concepts]
Recalling our running example, message passing generated two types of node embeddings i.e., $\mathbf{h}_{II} = [0.2, -0.4, 0.3]$ for nodes with a 2-node neighborhood, and $\mathbf{h}_{III} = [0.6, 0.2, -0.1]$ for nodes with a 3-node neighborhood. After applying the Gumbel-Softmax, we observe that there are only two types of activations produced---one for nodes with a 2-node neighborhood and one for nodes with a 3-node neighborhood e.g., $\mathbf{a}_{II} = [0, 0, 1]$ and $\mathbf{h}_{III} = [1, 0, 0]$. These activations can be easily interpreted as cluster or "latent concept labels" for the nodes. Specifically, all nodes with a non-zero 3rd index share the property of having a 2-node neighborhood, while all nodes with a non-zero 1st index share the property of having a 3-node neighborhood. Thus, there exists a precise one-to-one mapping between the structure of the neighborhood and the concept label, which makes the learned concept representation easily interpretable by humans who can easily visualize the subgraph representing the input neighborhood structure for each concept label e.g., $\mathbf{a}_i = [1,0,0] \iff$ % Figure removed or $\mathbf{a}_j = [0,0,1] \iff$ % Figure removed. 
\end{example}

\paragraph{Graph-level concept embeddings (max/add pooling)}
The second interpretable step of the interpretable graph layer is focused on generating a graph-level concept space. To achieve this goal, we can leverage the interpretable node-level concept space generated using the Gumbel-Softmax method. This step involves following the common procedures to aggregate node embeddings. It is worth noting that different aggregation functions can lead to different interpretations. Sum and max aggregations are typical interpretable aggregation functions:
\begin{align}
    & \mathbf{z} = \max_i \mathbf{a}_i \quad \in \{0,1\}^q  & \textit{(max pool for concept presence)}\\
    & \mathbf{z} = \sum_i \mathbf{a}_i \quad \in \mathbb{N}^q & \textit{(sum pool to count concepts)}
\end{align}
Max aggregation can easily be interpreted by taking the component-wise max over one-hot encoded vectors. In this case, after the max aggregation, a graph-level embedding vector takes a value of 1 at the i-th index  if and only if at least one node has the concept label \emph{i}. Similarly, we can interpret the output of a sum aggregation. In this case, a graph-level embedding vector takes a value of $X$ at the i-th index after the sum aggregation if and only if there are exactly $X$ nodes that have the concept label \emph{i}.\\

\begin{example}[Interpreting graph-level concepts]
Following our running example, let us consider a sum pool aggregation. For an $N_5$ graph, we have 5 nodes with exactly the same 2-node neighborhood. Therefore, a sum pool aggregation generates a graph-level embedding $z = [0,0,5]$, which certifies that we have 5 nodes of the same type e.g., % Figure removed. For an $M_3$ graph, the top and bottom nodes have a 3-node neighborhood e.g., % Figure removed, while the middle nodes have a 2-node neighborhood e.g., % Figure removed. This means that the sum pool aggregation generates a graph-level embedding $\mathbf{z} = [2, 0, 3]$, certifying that we have 2 nodes of type % Figure removed and 3 nodes of type  % Figure removed.
\end{example}

% \paragraph{Concept visualization}

\paragraph{Interpretable classifier}
To ensure prioritization of the most relevant concepts for the GNN classification task, a classifier is needed, as it will predict the task labels using the concept representations. However, using a black box classifier like a multi-layer perceptron (MLP, \cite{mlp}) would not be ideal as it could compromise the interpretability of our model. Instead, an interpretable linear classifier such as a single-layer network (or perceptron, \cite{slp_Karimboyevich_Nematullayevich_2022}) can be used. This allows for a completely interpretable model from the input to the classification head, as the input representations of the classifier are interpretable concepts and the classifier is a simple linear model which is intrinsically interpretable as discussed by \citet{rudin2019stop}. In fact, the weights of the perceptron can be used to identify which concepts are most relevant for the classification task. Hence, the resulting model can be used not only for classification, but also for interpretation and understanding of the problem at hand.

\subsubsection{Interpretable architectures}
The interpretable graph layer can be used to instantiate different types of interpretable GNNs (iGNNs). One approach is to plug this layer as the last message passing layer of a standard GNN architecture: 
\begin{align}
& \hat{y} = f\Big(\boxplus_{i \in V} \ \ \Theta \Big(\phi^{(L)} \Big(\mathbf{h}_i^{(L-1)}, \bigoplus_{j \in N_i} \psi^{(L)}(\mathbf{h}_i^{(L-1)}, \mathbf{h}_j^{(L-1)})\Big)\Big)\Big) \\
& \mathbf{h}_i^{(l)} = \phi^{(l)} \Big(\mathbf{h}_i^{(l-1)}, \bigoplus_{j \in N_i} \psi^{(l)}(\mathbf{h}_i^{(l-1)}, \mathbf{h}_j^{(l-1)})\Big)
\quad l = 1,\dots,L
\end{align}
where $f$ is an interpretable model, $\boxplus$ is an interpretable\footnote{Piece-wise linear function according to \cite{rudin2019stop} definition of interpretability.} permutation-invariant function (such as max or sum), $\Theta$ is a Gumbel-Softmax hard activation function, and $\mathbf{h}_i^0 = \mathbf{x}_i$. This way, we can interpret the first part of the network as a feature extractor generating latent representations that are useful for generating high-quality clusters from which concepts can be extracted. This approach is useful when we only care about the most complex neighborhoods/concepts. Another approach is to generate a hierarchical architecture where each GNN layer is interpretable:
\begin{equation}
\hat{y}^{(l)} = f\Big(\boxplus_{i \in V} \ \Theta \Big(\mathbf{h}_j^{(l)} \Big) \Big)
\qquad l = 1,\dots,L
\end{equation}
In this case, we can interpret every single layer of our model with concepts of increasing complexity. The concepts extracted from the first layer represent subgraphs representing the 1-hop neighborhood of a node, while at the second layer, they will represent 2-hop neighborhoods and so on. These hierarchical iGNNs can be useful to get insights into concepts with different granularities. By analyzing the concepts extracted at each layer, we can gain a better understanding of the information flow within the graph and the importance of different neighborhoods in the graph for the classification task at hand. This approach also allows us to identify the most important features in the graph for the classification task.

\subsubsection{Training}
When it comes to training interpretable GNNs (iGNNs), we can make use of the standard activation functions such as softmax or sigmoid, along with standard loss functions like cross-entropy. The choice of the activation and loss functions depends on the nature of the task to be performed. In the case of hierarchical iGNNs (HiGNNs), which involve a layered architecture with increasing levels of complexity, we can apply the loss function at each layer of the concept hierarchy. This ensures that each layer is doing its best to extract the most relevant concepts to solve the task at hand. The loss function for each layer can also be weighted differently to prioritize the formation of optimal concepts of a specific size, allowing the HiGNN to learn in a progressive and efficient way. By doing so, we can train the HiGNN in a progressive and effective manner, allowing us to uncover concepts with different granularities, and gain insights into how the model is performing at different levels of abstraction.


\section{Experiments}
\subsection{Research questions}
In this section we analyze the following research questions:
\begin{itemize}
    \item \textbf{Generalization} - How do GNNs generalize when trained to predict universal algebra's properties? Can GNNs strongly generalize i.e., can they perform well on large lattices when trained only with small lattices? Do interpretable GNNs generalize as well?
    \item \textbf{Interpretability} - Do concept-based explanations generated by interpretable GNNs empirically match universal algebra's conjectures? How can concept-based explanations suggest novel conjectures? Can we use interpretable GNNs to provide insights on mutual relationships between lattice properties?
    \item \textbf{Concept Quality} - Are the concepts identified by interpretable GNNs pure? Is the concept space complete?
\end{itemize}

\subsection{Setup}
\paragraph{Baselines}
For our comparative study, we evaluate the performance of interpretable GNNs (iGNNs) and their hierarchical version against equivalent GNN models (i.e., having the same hyperparameters such as number layers, training epochs, and learning rate). For vanilla GNNs we resort to common practice replacing the Gumbel-Softmax with a standard leaky relu activation function. To demonstrate the robustness of our approach, we implemented different types of message passing layers, including graph convolution and GIN. We do not consider suitable baselines prototype or concept-based GNNs which pre-define the structure of the concepts or prototypes used (such as protGNN or GLGExplainer), as for some of our datasets these structures are unknown. Similarly, we do not consider local explainability methods such as GNNExplainer or integrated gradients, as our focus is to understand the behavior of the GNN on a global scale. We also avoid comparing against post-hoc methods, such as GLGExplainer or GCExplainer~\citep{magister2021gcexplainer}, since we are interested in obtaining the exact and precise reason for the GNN's predictions, without relying on surrogate models to approximate the decision-making mechanism. In practice, we train all models using eight message passing layers and different embedding sizes ranging from 16 to 64. We train all models for 200 epochs with a learning rate of 0.001. For interpretable models, we set the Gumbel-Softmax temperature to the default value of 1 and the activation behavior to "hard," which generates one-hot encoded embeddings in the forward pass, but computes the gradients using the soft scores. For the hierarchical model, we set the internal loss weight to 0.1 (to score it roughly 10\% less w.r.t. the main loss). Overall, our selection of baselines aims at embracing a wide set of training setups and architectures to assess the effectiveness and versatility of GNNs for analyzing lattice properties in universal algebra.

\paragraph{Evaluation}
In our experimental evaluation, we compute a variety of quantitative metrics and employ our novel visualization techniques to support qualitative results. We start by assessing the generalization ability of our models using the area under the receiver operating characteristic (AUC ROC) curve. This enables us to illustrate the classification performance of the models as the discrimination threshold varies. We evaluate the AUC ROC under two different conditions. Firstly, we assess generalization under a standard scenario where the train and test data are independently and identically distributed (IID). Next, we assess "strong generalization" by computing the AUC ROC when the data is out-of-distribution (OOD). In this scenario, the training set consists of graphs up to eight nodes, while the test set comprises graphs with more than eight nodes, representing a significantly more challenging distribution shift from train to test.
We further assess generalization under two additional conditions: binary and multilabel classification. For the binary case, given a universal algebra's property, we train our models to classify lattices for which the property is either true or false. In the multilabel case, we train our models to classify lattices using all properties at the same time (i.e. modular, distributive, semidistributive, join semidistributive, and meet semidistributive). To quantitatively evaluate concepts, we use standard metrics such as completeness and purity. Completeness assesses the quality of the concept space on a global scale using a decision tree to map concepts to tasks, while purity measures the quality of a concept on a local scale by evaluating how similar the structures (subgraphs) of different samples activating the same concept are.
Finally, we evaluate the usefulness of our concept-based explanations by visualizing and comparing the generated concepts with ground truth lattices ($M_3$ and $N_5$), which are known to be significant for modular and distributive properties as stated by theorems \ref{the:n5} and \ref{the:m3}. We use similar visualizations for other properties to show relevant lattices that can then be used as hints and starting points to develop ad-hoc conjectures.
All metrics in our evaluation, across all experiments, are computed on test sets using $X$ random seeds, from which we compute a metric's mean and $95\%$ confidence interval. 


\section{Key Findings}
\subsection{Generalization}

\paragraph{GNNs weakly generalize on universal algebra's tasks (Table~\ref{tab:auc})}
Our experiments demonstrate that graph neural networks (GNNs) generalize exceptionally well across various tasks in universal algebra. We achieved an impressive mean AUCROC of X\% on the test set. Such a level of performance demonstrates that the models are remarkably robust and able to effectively generalize to previously unseen instances. We also observed that interpretable graph networks (iGNNs and HiGNNs) attain high task performance, with an AUCROC very close to black-box GNNs (2-3\% on average). This difference in performance could be attributed to additional constraints we imposed on these architectures and their loss functions. In particular, both the Boolean concept embeddings generated by the hard Gumbel-Softmax and the auxiliary losses computed at each layer in hierarchical models may affect the optimizer in navigating the loss landscape. Interestingly, GNNs performed well even in the challenging multilabel case, where they had to learn a wider and more diverse set of concepts and tasks (AUCROC was only ~X\% lower than in binary classification scenarios). Overall, these findings highlight the potential of GNNs as a powerful tool for learning universal algebra's properties.

% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
\begin{table}[!h]
\centering
\caption{Generalization performance of graph neural models in solving universal algebra's tasks. Values represents the mean and the standard error of the mean of the area under the receiver operating curve (AUCROC, \%).}
\label{tab:auc}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcccccc}
\hline
 & \multicolumn{3}{c}{\textbf{weak generalization}} & \multicolumn{3}{c}{\textbf{strong generalization}} \\
\multicolumn{1}{c}{} & \textbf{GNN} & \textbf{iGNN} & \textbf{HiGNN} & \textbf{GNN} & \textbf{iGNN} & \textbf{HiGNN} \\ \hline
\textbf{Distributive} & $99.80 \pm 0.04$ & $99.56 \pm 0.12$ & $99.45 \pm 0.06$ & $99.51 \pm 0.20$ & $99.44 \pm 0.05$ & $99.42 \pm 0.04$ \\
\textbf{Join Semi Distributive} & $99.49 \pm 0.02$ & $98.31 \pm 0.15$ & $98.28 \pm 0.04$ & $98.77 \pm 0.15$ & $97.50 \pm 0.14$ & $97.48 \pm 0.14$ \\
\textbf{Meet Semi Distributive} & $99.52 \pm 0.04$ & $98.19 \pm 0.06$ & $98.25 \pm 0.08$ & $98.90 \pm 0.03$ & $97.18 \pm 0.14$ & $96.89 \pm 0.37$ \\
\textbf{Modular} & $99.77 \pm 0.02$ & $99.18 \pm 0.11$ & $99.35 \pm 0.09$ & $99.32 \pm 0.22$ & $99.21 \pm 0.14$ & $99.11 \pm 0.22$ \\
\textbf{Semi Distributive} & $99.66 \pm 0.03$ & $98.57 \pm 0.02$ & $98.50 \pm 0.06$ & $99.19 \pm 0.04$ & $97.28 \pm 0.48$ & $96.88 \pm 0.47$ \\
\textbf{Multi Label} & $99.60 \pm 0.02$ & $96.32 \pm 0.34$ & $95.98 \pm 0.50$ & $98.62 \pm 0.43$ & $95.29 \pm 0.55$ & $95.27 \pm 0.32$ \\ \hline
\end{tabular}%
}
\end{table}

\paragraph{GNNs strongly generalize on universal algebra's tasks (Table~\ref{tab:auc})}
Our experimental results demonstrate that GNNs exhibit strong generalization capabilities across all tasks in universal algebra. Specifically, we trained the models on graphs of size up to 8 and evaluated their performance on larger graphs, which presents a significantly more challenging task due to the shift in data distribution. Despite this difficulty, our findings reveal that the GNNs achieved an impressive mean accuracy of X\% on the larger graphs. This level of performance indicates that the models are highly robust and able to generalize effectively to previously unseen instances. Remarkably, interpretable GNNs only showed minor differences in classification performance, reinforcing the notion that they can be relied upon to provide reliable and interpretable predictions. Overall, these results highlight the potential of GNNs to tackle challenging problems in universal algebra and provide an effective tool to handle lattices that are difficult to analyze manually on pen and paper.

\subsection{Interpretability}
% \paragraph{Interpretable GNNs generate intuitive concept-based explanations}
% \missingfigure{\note{ADD EXAMPLES OF CONCEPT-BASED EXPLANATIONS}}

\paragraph{Concept-based explanations empirically validate universal algebra's conjectures (Figure~\ref{fig:concept_modularity})}
We present empirical evidence to support the validity of theorems \ref{the:n5} and \ref{the:m3} by examining the concepts generated for modular and distributive tasks in a binary classification setup. Specifically, we investigate the presence of certain concepts when classifying modular and distributive lattices. For the modularity task, our results show that the lattice $N_5$ appears among non-modular concepts, but is never found in modular lattices, while the lattice $M_3$ appears only among modular concepts, which is consistent with theorem \ref{the:n5}. In the case of distributivity, we observe that both $M_3$ and $N_5$ are present among non-distributive concepts, and are never found in distributive lattices, which is also in line with theorem \ref{the:m3}. These findings provide the first large-scale empirical evidence for the validity of theorems \ref{the:n5} and \ref{the:m3}, and further demonstrate the effectiveness of graph neural networks in learning and analyzing lattice properties. Moreover, our results highlight how interpretable GNNs can not only learn the properties of universal algebra but also identify structures that are unique to one type of lattice (e.g., non-modular) and absent from another (e.g., modular), thus providing human-interpretable explanations for what the models learn.

% Figure environment removed


\paragraph{Concept-based explanations suggest novel conjectures (Figure~\ref{fig:concept_meet_semidist})}
Our results provide exciting opportunities for future research in universal algebra. With evidence that our method works (as shown for modular and distributive tasks), we can now use interpretable GNNs to gain insights into other universal algebra tasks that currently lack conjectures. For example, the meet-semi-distributive property currently lacks a conjecture identifying the (set of) lattice(s) whose omission makes a lattice meet-semi-distributive. Using concept visualizations, we can compare the concepts found for lattices that are not meet-semi-distributive against the concepts of lattices that are meet-semi-distributive. Interestingly, we observe $N_5$ but not $M_3$ among the concepts of meet-semi-distributive lattices, while we observe both $N_5$ and $M_3$ only in concepts that are not meet-semi-distributive. This observation suggests that $N_5$ is not a key lattice for meet-semi-distributive lattices, \emph{unlike distributive lattices}. These findings are significant because they demonstrate how analyzing concepts in interpretable GNNs can provide universal algebraists with a powerful and automatic tool to formulate new conjectures based on identifying simple lattices playing a role in specific properties. By leveraging the power of interpretable GNNs, we may uncover previously unknown connections between different properties and identify new patterns and structures that could lead to the development of new conjectures and theorems in universal algebra.

\ste{To review simidistributive example}

% Figure environment removed

\paragraph{Interpretable GNNs provide insights on relationships among universal algebra's properties  (Figure~\ref{fig:multilabel})}
Interpretable GNNs may represent a valuable tool for gaining insights into the relationships among different universal algebra properties. We show this in the multilabel scenario, where the model is trained to learn multiple properties simultaneously. Here the network must identify all the relevant concepts for all tasks at the same time, which makes this task more challenging compared to the binary classification setup. The resulting model provides valuable information on the similarities and differences among different properties based on the learnt concepts. To visualize this, we used a subset of concepts represented by colored circles in the embedding space of an interpretable GNN. The color of each circle corresponds to a unique string of labels shared by all samples in the cluster, and its size is proportional to the number of samples belonging to the concept. The sub-lattice corresponding to the input structure that activates the concept is also displayed on top of the circle. This approach allows us to have a global overview of the concept space and identify the relationships between different properties. For example, we observe that the concept of "non modular and non join semidistributive" might be closely related to the concept of being "non meet semi distributive and non join semidistributive". These concepts are close in the embedding space, and as a consequence, their corresponding lattices are also similar. Overall, these results show that interpretable GNNs offer a powerful tool for analyzing the relationships between universal algebra properties, explaining connections among properties in terms of concepts, which can then be leveraged by universal algebra specialists as a hint when developing new conjectures. 

% Figure environment removed

% Figure environment removed

% Figure environment removed



\paragraph{Interpretable GNNs generate pure and complete concept spaces (Table~\ref{tab:completenes} and~\ref{tab:purity})}
Our experimental results demonstrate that interpretable GNNs produce concepts with high completeness and purity, which are standard quantitative metrics used to evaluate the quality of concept-based approaches. Specifically, our approach achieves up to X\% completeness and Y purity, with an average score of Z and W, respectively. The lowest scores are obtained for the multilabel case, which is more challenging as models must learn a wider and more diverse set of concepts. Furthermore, the hierarchical structure of interpretable GNNs enables us to evaluate the quality of intermediate concepts layer by layer. This hierarchy provides insights into why we may need more layers, and it can be used as a valuable tool to find the optimal setup and tune the size of the architecture. Additionally, it can also be used to compare the quality of concepts at different layers of the network. Overall, these results quantitatively assess and validate the high quality of the concepts learned by the interpretable GNNs, highlighting the effectiveness of this approach for learning and analyzing complex algebraic structures.

% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
\begin{table}[!h]
\centering
\caption{Concept completeness scores of graph neural models in solving universal algebra's tasks. Values represents the mean and the standard error of the mean of the area under the receiver operating curve (AUCROC, \%).}
\label{tab:completenes}
\resizebox{.8\textwidth}{!}{%
\begin{tabular}{lcccc}
\hline
 & \multicolumn{2}{c}{\textbf{\textsc{weak completeness}}} & \multicolumn{2}{c}{\textbf{\textsc{strong completeness}}} \\
\multicolumn{1}{c}{\textbf{}} & \textbf{iGNN} & \textbf{HiGNN} & \textbf{iGNN} & \textbf{HiGNN} \\ \hline
\textbf{Distributive} & $77.30 \pm 0.20$ & $76.60 \pm 2.35$ & $78.19 \pm 1.71$ & $73.16 \pm 3.63$ \\
\textbf{Join Semi Distributive} & $85.20 \pm 0.86$ & $86.84 \pm 0.37$ & $81.08 \pm 0.18$ & $79.76 \pm 0.38$ \\
\textbf{Meet Semi Distributive} & $84.21 \pm 0.68$ & $84.34 \pm 1.08$ & $80.86 \pm 1.32$ & $79.68 \pm 0.22$ \\
\textbf{Modular} & $76.98 \pm 0.28$ & $73.77 \pm 3.14$ & $81.36 \pm 0.70$ & $77.61 \pm 2.37$ \\
\textbf{Semi Distributive} & $87.33 \pm 1.16$ & $85.62 \pm 0.27$ & $84.03 \pm 0.89$ & $82.26 \pm 0.21$ \\ \hline
\end{tabular}%
}
\end{table}

\note{ADD TABLE WITH CONCEPT PURITY}

% \subsection{Key Finding 3}

\section{Discussion}
\paragraph{Relations with the state of the art}


While general frameworks in which an AI system interacts with a human in a cycle of producing novel conjectures and refining them \cite{Davies2021}, in this paper we add a new strength in theorem suggesting, by means of sub-graph concept explanations. 

\paragraph{Limitations}

\paragraph{Conclusion}


\section*{References}

% References follow the acknowledgments. Use unnumbered first-level heading for
% the references. Any choice of citation style is acceptable as long as you are
% consistent. It is permissible to reduce the font size to \verb+small+ (9 point)
% when listing the references.
% Note that the Reference section does not count towards the page limit.
% \medskip

\bibliographystyle{apalike}
\bibliography{references}

\newpage

\appendix


\section{Algebra definitions}

\subsection{Formal defintions for Universal Algebra}
Universal algebra is the field of mathematics that studies algebraic structures, which are defined as a set $A$ along with its own collection of operations. An $n$-ary operation on $A$ is a function that takes $n$ elements of $A$ and returns a single element from the set. More formally \cite{BurrisSanka,Jonnson_1967,day_1969}:\\
\begin{definition}\textbf{N-ary function}
For $A$ non-empty set and $n$ nonnegative integer we define $A^0 = \{\emptyset\}$ and, for $n > 0$, $A^n$
is the set of n-tuples of elements from $A$. An $n$-ary operation (or function)
on $A$ is any function $f$ from $A^n$
to $A$; $n$ is the arity (or rank) of $f$. An operation $f$ on $A$ is called an n-ary operation if its arity  is $n$.\\
\end{definition}
\begin{definition}\textbf{Algebraic Structure} An algebra $ \mathcal{A}$ is a pair $ \langle A, F \rangle$ where $A$ is a non-empty set called universe and $F$ is a set of finitary operations on $A$.\\
\end{definition}

Apart from the operations on $A$, an algebra is further defined by axioms, that in the particular case of universal algebras are often of the form of identities. The collection of algebraic structures defined by equational laws are called varieties. \cite{HYLAND2007437}\\
\begin{definition}\textbf{Variety}
A nonempty class K of algebras of type $\mathcal{F}$ is called a variety if it is closed under subalgebras, homomorphic images, and direct products.
\end{definition}

Congruence lattices of algebraic structures are partially ordered sets such that every pair of elements has unique supremum and infimum determined by the underlying algebra. This object is important relatively to algebraic structures' properties, many of which can be described by omission or admission of certain subpatterns in a graph.\\
\begin{definition}\textbf{Congruence Lattice}\label{cl}\\
For every algebra $\mathcal{A}$ on the set $A$, the identity relation on $A$, and $A \times A$ are trivial congruences. An algebra with no other congruences is called simple. Let $\mathrm{Con}(\mathcal{A})$ be the set of congruences on the algebra $\mathcal{A}$. Because congruences are closed under intersection, we can define a meet operation: $ \wedge : \mathrm{Con}(\mathcal{A}) \times \mathrm{Con}(\mathcal{A}) \to \mathrm{Con}(\mathcal{A})$ by simply taking the intersection of the congruences $E_1 \wedge E_2 = E_1\cap E_2$. Congruences are not closed under union, however we can define the closure operator of any binary relation $E$, with respect to a fixed algebra $\mathcal{A}$, such that it is a congruence, in the following way: $ \langle E \rangle_{\mathcal{A}} = \bigcap \{ F \in \mathrm{Con}(\mathcal{A}) \mid E \subseteq F \}$. Note that the closure of a binary relation is a congruence and thus depends on the operations in $\mathcal{A}$, not just on the carrier set. Now define $ \vee: \mathrm{Con}(\mathcal{A}) \times \mathrm{Con}(\mathcal{A}) \to \mathrm{Con}(\mathcal{A})$ as $E_1 \vee E_2 = \langle E_1\cup E_2 \rangle_{\mathcal{A}} $. For every algebra $\mathcal{A}$, $(\mathrm{Con}(\mathcal{A}), \wedge, \vee)$ with the two operations defined above forms a lattice, called the congruence lattice of $\mathcal{A}$.
\end{definition}


\begin{definition} \textbf{Subalgebra}
    Let $\mathbf{A}$ and $\mathbf{B}$ be two algebras of the same type. Then $\mathbf{B}$ is a \textit{subalgebra} of $\mathbf{A}$ if $B \subseteq A$ and every fundamental operation of $\mathbf{B}$ is the restriction of the corresponding operation of $\mathbf{A}$, i.e., for each function symbol $f$, $f^{\mathbf{B}}$ is $f^{\mathbf{A}}$ restricted to $\mathbf{B}$.
\end{definition}

\begin{definition} \textbf{Homomorphic image}
    Suppose $\mathbf{A}$ and $\mathbf{B}$ are two algebras of the same type $\mathcal{F}$. A mapping $\alpha : A \rightarrow B$ is called a \textit{homomorphism} from $\mathbf{A}$ to $\mathbf{B}$ if 
    $$
    \alpha f^{\mathbf{A}}(a_1, \dotsc ,a_n) = f^{\mathbf{B}}(\alpha a_1, \dotsc, \alpha a_n)
    $$
    for each n-ary $f$ in $\mathcal{F}$ and each sequence $a_1, \dotsc, a_n$ from $\mathbf{A}$. If, in addition, the mapping $\alpha$ is onto then $\mathbf{B}$ is said to be a homomorphic image of $\mathbf{A}$.
\end{definition}

\begin{definition}\textbf{Direct product}
    Let $\mathbf{A}_1$ and $\mathbf{A}_2$ be two algebras of the same type $\mathcal{F}$. We define the direct product $\mathbf{A}_1 \times \mathbf{A}_2$ to be the algebra whose universe is the set $A_1 \times A_2$, and such that for $f \in \mathcal{F}$ and $a_i \in A_1$, $a_i' \in A_2$, $1 \leq i \leq n$,

    $$
    f^{\mathbf{A}_1 \times \mathbf{A}_2}(\langle a_1, a_1' \rangle, \dotsc , \langle a_n, a_n') = \langle f^{\mathbf{A}_1}(a_1, \dotsc, a_n), f^{\mathbf{A}_2}(a_1', \dotsc, a_n') \rangle
    $$
\end{definition}



\section{Concepts}

\subsection{Distributive}
% Figure environment removed
% Figure environment removed


\subsection{Modular}

% Figure environment removed
% Figure environment removed


\subsection{Join Semi Distributive}

% Figure environment removed
% Figure environment removed


\subsection{Meet Semi Distributive}

% Figure environment removed
% Figure environment removed


\subsection{Semi Distributive}

% Figure environment removed
% Figure environment removed




\end{document}


% \section{Submission of papers to NeurIPS 2022}


% Please read the instructions below carefully and follow them faithfully.


% \subsection{Style}


% Papers to be submitted to NeurIPS 2022 must be prepared according to the
% instructions presented here. Papers may only be up to {\bf nine} pages long,
% including figures. Additional pages \emph{containing only acknowledgments and
% references} are allowed. Papers that exceed the page limit will not be
% reviewed, or in any other way considered for presentation at the conference.


% The margins in 2022 are the same as those in 2007, which allow for $\sim$$15\%$
% more words in the paper compared to earlier years.


% Authors are required to use the NeurIPS \LaTeX{} style files obtainable at the
% NeurIPS website as indicated below. Please make sure you use the current files
% and not previous versions. Tweaking the style files may be grounds for
% rejection.


% \subsection{Retrieval of style files}


% The style files for NeurIPS and other conference information are available on
% the World Wide Web at
% \begin{center}
%   \url{http://www.neurips.cc/}
% \end{center}
% The file \verb+neurips_2022.pdf+ contains these instructions and illustrates the
% various formatting requirements your NeurIPS paper must satisfy.


% The only supported style file for NeurIPS 2022 is \verb+neurips_2022.sty+,
% rewritten for \LaTeXe{}.  \textbf{Previous style files for \LaTeX{} 2.09,
%   Microsoft Word, and RTF are no longer supported!}


% The \LaTeX{} style file contains three optional arguments: \verb+final+, which
% creates a camera-ready copy, \verb+preprint+, which creates a preprint for
% submission to, e.g., arXiv, and \verb+nonatbib+, which will not load the
% \verb+natbib+ package for you in case of package clash.


% \paragraph{Preprint option}
% If you wish to post a preprint of your work online, e.g., on arXiv, using the
% NeurIPS style, please use the \verb+preprint+ option. This will create a
% nonanonymized version of your work with the text ``Preprint. Work in progress.''
% in the footer. This version may be distributed as you see fit. Please \textbf{do
%   not} use the \verb+final+ option, which should \textbf{only} be used for
% papers accepted to NeurIPS.


% At submission time, please omit the \verb+final+ and \verb+preprint+
% options. This will anonymize your submission and add line numbers to aid
% review. Please do \emph{not} refer to these line numbers in your paper as they
% will be removed during generation of camera-ready copies.


% The file \verb+neurips_2022.tex+ may be used as a ``shell'' for writing your
% paper. All you have to do is replace the author, title, abstract, and text of
% the paper with your own.


% The formatting instructions contained in these style files are summarized in
% Sections \ref{gen_inst}, \ref{headings}, and \ref{others} below.


% \section{General formatting instructions}
% \label{gen_inst}


% The text must be confined within a rectangle 5.5~inches (33~picas) wide and
% 9~inches (54~picas) long. The left margin is 1.5~inch (9~picas).  Use 10~point
% type with a vertical spacing (leading) of 11~points.  Times New Roman is the
% preferred typeface throughout, and will be selected for you by default.
% Paragraphs are separated by \nicefrac{1}{2}~line space (5.5 points), with no
% indentation.


% The paper title should be 17~point, initial caps/lower case, bold, centered
% between two horizontal rules. The top rule should be 4~points thick and the
% bottom rule should be 1~point thick. Allow \nicefrac{1}{4}~inch space above and
% below the title to rules. All pages should start at 1~inch (6~picas) from the
% top of the page.


% For the final version, authors' names are set in boldface, and each name is
% centered above the corresponding address. The lead author's name is to be listed
% first (left-most), and the co-authors' names (if different address) are set to
% follow. If there is only one co-author, list both author and co-author side by
% side.


% Please pay special attention to the instructions in Section \ref{others}
% regarding figures, tables, acknowledgments, and references.


% \section{Headings: first level}
% \label{headings}


% All headings should be lower case (except for first word and proper nouns),
% flush left, and bold.


% First-level headings should be in 12-point type.


% \subsection{Headings: second level}


% Second-level headings should be in 10-point type.


% \subsubsection{Headings: third level}


% Third-level headings should be in 10-point type.


% \paragraph{Paragraphs}


% There is also a \verb+\paragraph+ command available, which sets the heading in
% bold, flush left, and inline with the text, with the heading followed by 1\,em
% of space.


% \section{Citations, figures, tables, references}
% \label{others}


% These instructions apply to everyone.


% \subsection{Citations within the text}


% The \verb+natbib+ package will be loaded for you by default.  Citations may be
% author/year or numeric, as long as you maintain internal consistency.  As to the
% format of the references themselves, any style is acceptable as long as it is
% used consistently.


% The documentation for \verb+natbib+ may be found at
% \begin{center}
%   \url{http://mirrors.ctan.org/macros/latex/contrib/natbib/natnotes.pdf}
% \end{center}
% Of note is the command \verb+\citet+, which produces citations appropriate for
% use in inline text.  For example,
% \begin{verbatim}
%    \citet{hasselmo} investigated\dots
% \end{verbatim}
% produces
% \begin{quote}
%   Hasselmo, et al.\ (1995) investigated\dots
% \end{quote}


% If you wish to load the \verb+natbib+ package with options, you may add the
% following before loading the \verb+neurips_2022+ package:
% \begin{verbatim}
%    \PassOptionsToPackage{options}{natbib}
% \end{verbatim}


% If \verb+natbib+ clashes with another package you load, you can add the optional
% argument \verb+nonatbib+ when loading the style file:
% \begin{verbatim}
%    \usepackage[nonatbib]{neurips_2022}
% \end{verbatim}


% As submission is double blind, refer to your own published work in the third
% person. That is, use ``In the previous work of Jones et al.\ [4],'' not ``In our
% previous work [4].'' If you cite your other papers that are not widely available
% (e.g., a journal paper under review), use anonymous author names in the
% citation, e.g., an author of the form ``A.\ Anonymous.''


% \subsection{Footnotes}


% Footnotes should be used sparingly.  If you do require a footnote, indicate
% footnotes with a number\footnote{Sample of the first footnote.} in the
% text. Place the footnotes at the bottom of the page on which they appear.
% Precede the footnote with a horizontal rule of 2~inches (12~picas).


% Note that footnotes are properly typeset \emph{after} punctuation
% marks.\footnote{As in this example.}


% \subsection{Figures}


% % Figure environment removed


% All artwork must be neat, clean, and legible. Lines should be dark enough for
% purposes of reproduction. The figure number and caption always appear after the
% figure. Place one line space before the figure caption and one line space after
% the figure. The figure caption should be lower case (except for first word and
% proper nouns); figures are numbered consecutively.


% You may use color figures.  However, it is best for the figure captions and the
% paper body to be legible if the paper is printed in either black/white or in
% color.


% \subsection{Tables}


% All tables must be centered, neat, clean and legible.  The table number and
% title always appear before the table.  See Table~\ref{sample-table}.


% Place one line space before the table title, one line space after the
% table title, and one line space after the table. The table title must
% be lower case (except for first word and proper nouns); tables are
% numbered consecutively.


% Note that publication-quality tables \emph{do not contain vertical rules.} We
% strongly suggest the use of the \verb+booktabs+ package, which allows for
% typesetting high-quality, professional tables:
% \begin{center}
%   \url{https://www.ctan.org/pkg/booktabs}
% \end{center}
% This package was used to typeset Table~\ref{sample-table}.


% \begin{table}
%   \caption{Sample table title}
%   \label{sample-table}
%   \centering
%   \begin{tabular}{lll}
%     \toprule
%     \multicolumn{2}{c}{Part}                   \\
%     \cmidrule(r){1-2}
%     Name     & Description     & Size ($\mu$m) \\
%     \midrule
%     Dendrite & Input terminal  & $\sim$100     \\
%     Axon     & Output terminal & $\sim$10      \\
%     Soma     & Cell body       & up to $10^6$  \\
%     \bottomrule
%   \end{tabular}
% \end{table}


% \section{Final instructions}


% Do not change any aspects of the formatting parameters in the style files.  In
% particular, do not modify the width or length of the rectangle the text should
% fit into, and do not change font sizes (except perhaps in the
% \textbf{References} section; see below). Please note that pages should be
% numbered.


% \section{Preparing PDF files}


% Please prepare submission files with paper size ``US Letter,'' and not, for
% example, ``A4.''


% Fonts were the main cause of problems in the past years. Your PDF file must only
% contain Type 1 or Embedded TrueType fonts. Here are a few instructions to
% achieve this.


% \begin{itemize}


% \item You should directly generate PDF files using \verb+pdflatex+.


% \item You can check which fonts a PDF files uses.  In Acrobat Reader, select the
%   menu Files$>$Document Properties$>$Fonts and select Show All Fonts. You can
%   also use the program \verb+pdffonts+ which comes with \verb+xpdf+ and is
%   available out-of-the-box on most Linux machines.


% \item The IEEE has recommendations for generating PDF files whose fonts are also
%   acceptable for NeurIPS. Please see
%   \url{http://www.emfield.org/icuwb2010/downloads/IEEE-PDF-SpecV32.pdf}


% \item \verb+xfig+ "patterned" shapes are implemented with bitmap fonts.  Use
%   "solid" shapes instead.


% \item The \verb+\bbold+ package almost always uses bitmap fonts.  You should use
%   the equivalent AMS Fonts:
% \begin{verbatim}
%    \usepackage{amsfonts}
% \end{verbatim}
% followed by, e.g., \verb+\mathbb{R}+, \verb+\mathbb{N}+, or \verb+\mathbb{C}+
% for $\mathbb{R}$, $\mathbb{N}$ or $\mathbb{C}$.  You can also use the following
% workaround for reals, natural and complex:
% \begin{verbatim}
%    \newcommand{\RR}{I\!\!R} %real numbers
%    \newcommand{\Nat}{I\!\!N} %natural numbers
%    \newcommand{\CC}{I\!\!\!\!C} %complex numbers
% \end{verbatim}
% Note that \verb+amsfonts+ is automatically loaded by the \verb+amssymb+ package.


% \end{itemize}


% If your file contains type 3 fonts or non embedded TrueType fonts, we will ask
% you to fix it.


% \subsection{Margins in \LaTeX{}}


% Most of the margin problems come from figures positioned by hand using
% \verb+\special+ or other commands. We suggest using the command
% \verb+\includegraphics+ from the \verb+graphicx+ package. Always specify the
% figure width as a multiple of the line width as in the example below:
% \begin{verbatim}
%    \usepackage[pdftex]{graphicx} ...
%    % Figure removed
% \end{verbatim}
% See Section 4.4 in the graphics bundle documentation
% (\url{http://mirrors.ctan.org/macros/latex/required/graphics/grfguide.pdf})


% A number of width problems arise when \LaTeX{} cannot properly hyphenate a
% line. Please give LaTeX hyphenation hints using the \verb+\-+ command when
% necessary.


% \begin{ack}
% Use unnumbered first level headings for the acknowledgments. All acknowledgments
% go at the end of the paper before the list of references. Moreover, you are required to declare
% funding (financial activities supporting the submitted work) and competing interests (related financial activities outside the submitted work).
% More information about this disclosure can be found at: \url{https://neurips.cc/Conferences/2022/PaperInformation/FundingDisclosure}.


% Do {\bf not} include this section in the anonymized submission, only in the final paper. You can use the \texttt{ack} environment provided in the style file to autmoatically hide this section in the anonymized submission.
% \end{ack}