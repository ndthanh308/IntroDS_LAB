\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2022


% ready for submission
% \usepackage{neurips_2023}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
    \usepackage[preprint]{neurips_2022}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2022}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2022}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}
\usepackage{todonotes}
\usepackage{caption}
\usepackage{comment}
\usepackage{subcaption}
\usepackage{amsthm}
% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{booktabs}            % professional-quality tables
\usepackage{multirow}            % tabular cells spanning multiple rows
\usepackage{amsfonts}            % blackboard math symbols
\usepackage{graphicx}            % figures
\usepackage{duckuments}          % sample images
\usepackage{dsfont}
\usepackage{natbib}
\usepackage{scalerel}
% \usepackage[noend]{algpseudocode}
\usepackage[noend,ruled,vlined,noline]{algorithm2e}
% Set algorithm keyword formatting
\newcommand{\Var}{\textup}
\newcommand{\Comment}{\tcc*[r]}
\SetKwComment{tcc}{$\triangleright$~}{}
\SetCommentSty{normalfont}
\SetKwInput{KwRequire}{Require}
% \SetKwComment{Comment}{$\triangleright$\ }{}
% Set algorithm line numbers
% \SetNlSty{}{}{:}

\usepackage{wrapfig}

\setcitestyle{numbers}
% \usepackage{algorithmic}
\usepackage{etoolbox}\AtBeginEnvironment{algorithm}{\tiny}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}


\newcommand{\note}[1]{\textcolor{red}{Note: #1}}
\newcommand{\fg}[1]{\textcolor{blue}{F: #1}}
\newcommand{\ste}[1]{\textcolor{green}{Ste: #1}}



\title{Interpretable Graph Networks\\Formulate Universal Algebra Conjectures}
% spark, devise, conceive, evoke, elaborate, infer, derive, formulate
% Interpretable Graph Networks Frame \\Evidences for Universal Algebra Conjectures
% Interpretable Graph Networks Bridge Equational and Topological Representations in Universal Algebra
% "Exploring the Relationship between Equational and Topological Properties in Universal Algebra with Interpretable Graph Networks"
% "Interpretable Graph Networks Reveal Connections between Equational and Topological Properties in Universal Algebra"
% "Bridging Equational and Topological Properties in Universal Algebra with Interpretable Graph Networks"
% "Interpretable Graph Networks Unveil Correlations between Equational and Topological Properties in Universal Algebra"
% "Connecting Equational and Topological Properties in Universal Algebra using Interpretable Graph Networks"
% "A Graph-Based Approach to Understanding the Relationship between Equational and Topological Properties in Universal Algebra"
% "Using Interpretable Graph Networks to Uncover the Links between Equational and Topological Properties in Universal Algebra"
% "Visualizing the Relationship between Equational and Topological Properties in Universal Algebra with Interpretable Graph Networks"
% "Investigating Equational and Topological Properties in Universal Algebra with Interpretable Graph Networks"
% "Interpretable Graph Networks: Revealing the Relationship between Equational and Topological Properties in Universal Algebra".
% "Exploring Universal Algebra with Interpretable Graph Networks"
% "Unveiling Algebraic Properties through Interpretable Graph Networks"
% "Using Interpretable Graph Networks to Understand Universal Algebra"
% "Graph Networks Shed Light on Algebraic Properties in Universal Algebra"
% "Interpretable Graph Networks: A Tool for Revealing Algebraic Properties in Universal Algebra"
% "Discovering Universal Algebraic Properties with Interpretable Graph Networks"
% "Interpretable Graph Networks Uncover Hidden Patterns in Universal Algebra"
% "Visualizing Algebraic Properties with Interpretable Graph Networks"
% "Insights into Universal Algebra through Interpretable Graph Networks"
% "Interpretable Graph Networks: A New Approach to Studying Universal Algebra"


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
Francesco Giannini$^*$\\
  Universit\`a di Siena, Italy\\
  \texttt{francesco.giannini@unisi.it} \\
  \And
  Stefano Fioravanti$^*$\\
  Universit\`a di Siena, Italy\\
  \texttt{stefano.fioravanti@unisi.it} \\
  \And
   Oguzhan Keskin\\
   University of Cambridge, UK\\
  \texttt{ok313@cam.ac.uk} \\
  \And
   Alisia Maria Lupidi\\
   University of Cambridge, UK\\
  \texttt{aml201@cam.ac.uk} \\
  \And
  Lucie Charlotte Magister \\
  University of Cambridge, UK\\
  \texttt{lcm67@cam.ac.uk} \\
  \And
  Pietro Li\'o \\
  University of Cambridge, UK\\
  \texttt{pl219@cam.ac.uk} \\
  \And
  Pietro Barbiero$^*$ \\
  University of Cambridge, UK\\
  \texttt{pb737@cam.ac.uk} \\}

  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\



\begin{document}


\maketitle


\begin{abstract}
The rise of Artificial Intelligence (AI) recently empowered researchers to investigate hard mathematical problems which eluded traditional approaches for decades. Yet, the use of AI in Universal Algebra (UA)---one of the fields laying the foundations of modern mathematics---is still completely unexplored. This work proposes the first use of AI to investigate UA's conjectures with an equivalent equational and topological characterization.
While topological representations would enable the analysis of such properties using graph neural networks, the limited transparency and brittle explainability of these models hinder their straightforward use to empirically validate existing conjectures or to formulate new ones.
To bridge these gaps, we propose a general algorithm generating AI-ready datasets based on UA's conjectures, and introduce a novel neural layer to build fully interpretable graph networks. The results of our experiments demonstrate that interpretable graph networks: (i) enhance interpretability without sacrificing task accuracy, (ii) strongly generalize when predicting universal algebra's properties, (iii) generate simple explanations that empirically validate existing conjectures, and (iv) identify subgraphs suggesting the formulation of novel conjectures. 
% Our findings demonstrate the potential of our methodology and open the doors of universal algebra to AI.

% Several problems in UA can be formalized in terms of topological properties of algebraic structures, and  are therefore suitable to be investigated by graph neural architectures. 
% In this regard, this paper proposes an approach that uses a newly devised interpretable GNN to relate equational properties of special algebraic structures to sub-portions of their graph representation. 
% %
%  Interpretable GNNs have the potential to offer explainability and transparency in the decision-making process of AI models. This is essential in mathematical research as it provides a way to understand how AI is aiding in the discovery of new theorems.
% such as the distributive and modular properties of algebraic lattices. 
% The architecture presented in this paper is capable of identifying known sub-patterns that characterize these properties while also suggesting new theorems for mathematicians to investigate. The use of Interpretable GNNs provides insight into how these suggestions are made and can aid in the development of new mathematical knowledge.
% Overall, this paper highlights the potential of AI for Mathematics, specifically in Universal Algebra, and emphasizes the importance of Interpretable GNNs in ensuring transparency and interpretability in AI-assisted mathematical research.
\end{abstract}

\begin{wrapfigure}[11]{r}{0.38\textwidth}
\vspace{-2.2cm}
    % Figure removed
    % % Figure removed
    \caption{{\small Interpretable graph networks support universal algebra research.}}
    \label{fig:vis_abs}
\end{wrapfigure}
\section{Introduction}
Universal Algebra (UA, \citep{BurrisSanka}) is one of the foundational fields of modern mathematics, yet the complexity of studying abstract algebraic structures hinders scientific progress and discourages many academics. Recently, the emergence of powerful AI technologies empowered researchers to investigate intricate mathematical problems which eluded traditional approaches for decades, leading to the solution of open problems (e.g., ~\cite{lample2019deep}) and discovery of new conjectures (e.g., ~\cite{Davies2021}). Yet, universal algebra currently remains an uninvestigated realm for AI, a completely uncharted territory with deep impact in all mathematical disciplines.

Universal algebra studies algebraic structures from an abstract perspective. Interestingly, several UA conjectures equivalently characterize algebraic properties using equations or graphs~\citep{JipsenRose}. In theory, studying UA properties as graphs would enable the use of powerful AI techniques, such as Graph Neural Networks (GNN,~\citep{scarselli2008graph}), which excel on graph-structured data. However, two factors currently limit scientific progress. First, the \textit{absence of benchmark datasets} suitable for machine learning prevents widespread application of AI to UA. Second, \textit{GNNs' opaque reasoning} obstructs human understanding of their decision process~\citep{rudin2019stop}. Compounding the issue of GNNs' limited transparency, GNN explainability methods mostly rely on brittle and untrustworthy local/post-hoc methods \citep{huang2022graphlime,luo2020parameterized,magister2021gcexplainer,rudin2019stop,ying2019gnnexplainer} or pre-defined subgraphs for explanations~\citep{azzolin2022global, vu2020pgm}, which are often unknown in UA. 
% Hence, the limited transparency and brittle explainability of GNNs hinder universal algebrists from using them to empirically validate existing conjectures or to formulate new ones.

\textbf{Contributions.}
In this work, we investigate universal algebra's conjectures through AI (Figure~\ref{fig:vis_abs}), venturing for the first time in this previously uncharted territory. Our work includes three significant contributions. First, we propose a novel algorithm that generates a dataset suitable for training AI models based on an UA equational conjecture. Second, we generate and release the first-ever universal algebra's dataset compatible with AI, which contains more than $29,000$ lattices and the labels of $5$ key properties i.e., modularity, distributivity, semi-distributivity, join semi-distributivity, and meet semi-distributivity. And third, we introduce a novel neural layer that makes GNNs fully interpretable, according to Rudin's \cite{rudin2019stop} notion of interpretability. The results of our experiments demonstrate that interpretable GNNs (iGNNs): (i) enhance GNN interpretability without sacrificing task accuracy, (ii) strongly generalize when trained to predict universal algebra's properties, (iii) generate simple concept-based explanations that empirically validate existing conjectures, and (iv) identify subgraphs which could be relevant for the formulation of novel conjectures. Our findings demonstrate the potential of our methodology and open the doors of universal algebra to AI.


% \begin{itemize}
%     \item General definition of the problem:
%     \fg{AI for mathematics: general goal (produce counter-example, introduce conjecture,....)}
% \end{itemize}

% Artificial Intelligence (AI) has been used in mathematics for several decades, and the field of computational mathematics has grown significantly as a result. Some of the earliest applications of AI in mathematics included the use of expert systems to solve algebraic and geometric problems, and the development of automated theorem provers that could mechanically prove mathematical theorems.
% In recent years, there has been a surge of interest in the use of deep learning techniques, such as neural networks, for mathematical research. These techniques have been applied to a wide range of problems, from classifying mathematical objects to predicting the behavior of complex dynamical systems.
% There has also been significant progress in the development of interactive proof assistants, which allow mathematicians to verify the correctness of their proofs using computer programs. These tools can help to reduce the likelihood of errors and increase confidence in the correctness of mathematical results.
% Overall, the use of AI in mathematics has the potential to revolutionize the field by providing new insights, automating tedious tasks, and enabling mathematicians to explore new areas of research that were previously inaccessible.
% %%%%

% %%%%
% AI has been utilized in mathematics for many years, leading to a significant growth in the field of computational mathematics. Early applications included expert systems for solving algebraic and geometric problems and automated theorem provers. More recently, deep learning techniques such as neural networks have been used for a wide range of mathematical problems. Additionally, interactive proof assistants have been developed to verify the correctness of mathematical proofs using computer programs. AI has the potential to revolutionize mathematics by providing new insights, automating tedious tasks, and enabling mathematicians to explore previously inaccessible areas of research.

% \begin{itemize}
%     \item Existing solutions and knowledge gaps
%     \fg{Specific solutions: RL, diff equa, theorem proving, geometry-topological, group theory, ecc, THEN: collaboration with mathematicians (Davis etc..), Hence: us for UA:}
% %Why UA?
% The theory of universal algebra has applications in computer science, logic, and other branches of mathematics, such as topology and geometry. It provides a framework for studying the properties of algebraic structures in a systematic and general way, and for developing algorithms and software for manipulating these structures. 
% Interestingly, several properties can be described by relying on the underlined graph representation of these algebraic structures, hence allowing the exploitation of well-suited AI-based models excelling on graph-structured data. In this regard, Graph Neural Networks (GNN) represent a good candidate for graph classifications, and recently some advances have been made introducing novel interpretable concept-based GNNs capable of automatically extract the relevant concepts for a classification task \cite{magister2021gcexplainer,passerini?}.
    
%     \item Visual summary of the paper: problem, solution, results
%     \fg{The idea sketced in general? Like, graph property -> gen. dataset -> train gnn classifier -> use gnn theorem suggester}

% We introduce a new framework where human mathematicians can benefit from explainable AI agent as theorem suggester. In particular, exploiting interpretable GNNs to hightlight relevant portions of graphs responsible for the failure of an equational property. 

% %%%%%%
% We present a general methodology to semi-automate the process of generating conjectures for universal algebra. The methodology consists of a close collaboration between a human expert in universal algebra and an interpretable graph neural network (Figure \ref{fig:vis_abs}). The proposed workflow allows experts to convert algebraic properties expressed by (quasi-)equations into datasets suitable for machine learning. This newly created dataset is later be fed to an interpretable GNN, and it is possible to visualize concept-based explanations yielded by the interpretable model, providing feedback to the human expert. The expert can then refine the conjectures, tweak the dataset, and restart the loop.
%  \fg{Add trying to prove the theorem suggested by the conjecture?}
% %%%%%%%%%%%%%%%
    
%     \item Our solution and contributions
%     \fg{We investigate the properties x,y,.., by combining GNN+GCE, we expect results; to evaluate we realized the benchmark;}
    
%     The main contribution of the paper are the following:
%     \begin{itemize}
%     \item We propose a novel methodology that  given a certain equational property builds a dataset of graph-based algebraic structures, and then train an interpretable GNN  to conjecture  which is the sub-portion of the graph responsible for the failure of such a property. 
%     \item We introduce a new concept-based interpretable graph neural model trained to suggest new conjectures to solve universal algebra's open problems.
    
%     \item We create and release\footnote{Hidden for double blind submission.} the first dataset of lattices in PyTorch labelled with different algebraic properties expressed by equations and quasi-equations.
%     \ste{with a pytorch script?}
%     \item We proposed the first benchmark in universal algebra for AI agents, and discuss a manifesto of a (possibly very large) class of problems in this field that can be experimented by our model.
%         \item  Our method shows to be capable of verifying well-established results in UA and proposing new insightful conjectures.
%     \end{itemize}
%     \fg{to clean..}
% \end{itemize}












\section{Background}
% and Problem Definition}
\label{sec:back}

% \subsection{Universal Algebra}
% \label{sec:ua}
% \todo[@Stefano, @Francesco]

Universal Algebra is a branch of mathematics studying general and abstract algebraic structures. \emph{Algebraic structures} are typically represented as ordered pairs $\mathbf{A} = (A, F)$, consisting of a non-empty set $A$ and a collection of operations $F$ defined on the set. %The operations may include binary operations (such as addition or multiplication), unary operations (such as negation or inverse), nullary operations (constants), and so forth. 
UA aims to identify algebraic properties (often in equational form) shared by various mathematical systems. In particular, \emph{varieties} are classes of algebraic structures sharing a common set of identities, which enable the study of algebraic systems based on their common properties. %They form the foundation for exploring the fundamental principles and interrelationships within the field of universal algebra. 
Prominent instances of varieties that have been extensively studied across various academic fields encompass Groups, Rings, Boolean Algebras, Fields, and many others. 
% We refer the reader to \cite{BurrisSanka} for foundational notions of UA.%universal algebra has applications in various areas of mathematics and beyond. It provides a foundation for understanding the structure and properties of algebraic systems in fields like computer science, physics, and engineering. It also serves as a fundamental tool in the study of mathematical logic, model theory, and formal languages.
%\subsection{Lattices} 
%\label{sec:latt}
A particularly relevant variety of algebras are Lattices (details in Appendix \ref{def:lattice}), which are often studied for their connection with logical structures.
\begin{definition} \label{def:back_lattice}
 A \emph{lattice} $\mathbf{L}$ is an algebraic structure composed by a non-empty set $L$ and two binary operations $\vee$ and $\wedge$, satisfying the commutativity, associativity, idempotency, and absorption axioms. %following axioms and their duals obtained exchanging $\vee$ and $\wedge$:
 \begin{comment}
\begin{align*}
&x \vee y \approx y \vee x  &&\text{(commutativity)}
\\&x \vee (y \vee z) \approx (x \vee y)  &&\text{(associativity)}
\\&x \vee x \approx x  &&\text{(idempotency)}
\\&x \approx x \vee (x \wedge y)  &&\text{(absorption)}
\end{align*}
\end{comment}
\end{definition}
%class of lattice class of lattices is studied both for is connection with logical structures and for its algebraic properties. 
\begin{wrapfigure}[8]{r}{0.25\textwidth}
    % Figure removed
    % Figure removed
    \caption{\small Hasse diagrams.}
    \label{fig:small_n5m3}
\end{wrapfigure}
Equivalently a lattice can be characterized as a partially ordered set in which every pair of elements has a \emph{supremum} and an \emph{infimum} (cf. Appendix \ref{def:lattice}). Lattices also have formal representations as graphs via \emph{Hasse diagrams} $(L,E)$ (e.g., Figure~\ref{fig:small_n5m3}), where each node $x \in L$ is a lattice element, and directed\footnote{The orientation of Hasse diagrams is always to be meant bottom-up, hence we will omit  arrows for simplicity.} edges $(x,y) \in E \subseteq L \times L$ represent the  ordering relation, such that if $(x,y) \in E$ then $x \leq_L y$ in the ordering of the lattice. A \emph{sublattice} $\mathbf{L}'$ of a lattice $\mathbf{L}$ is a lattice such that $L' \subseteq L$ and $\mathbf{L}'$ preserves the original order (the ``essential structure'') of $L$, i.e. for all $x, y \in L'$ then $x \leq_{L'} y$ if and only if $x \leq_L y$. The foundational work by \citet{birkhoff1935structure}, \citet{Dedekind1900}, and \citet{jonsson} played a significant role in discovering that some significant varieties of lattices can be characterized through the omission of one or more lattices. Specifically, a variety $\mathcal{V}$ of lattices is said to \emph{omit} a lattice $\mathbf{L}$ if it cannot be identified as a sublattice of any lattice in $\mathcal{V}$. A parallel line of work in UA characterizes lattices in terms of equational ("$\textit{term}_1\approx\textit{term}_2$") and quasi-equational ("if $\textit{equation}_1$ holds then $\textit{equation}_2$ holds") properties, such as distributivity and  modularity.
% This exclusion criterion serves as a useful means of characterizing and classifying different types of lattice varieties.
\begin{definition}
    Let $\mathbf{L}$ be a lattice. $\mathbf{L}$ is \emph{modular} if it satisfies $x \leq y \rightarrow x \vee (y \wedge z) \approx y \wedge (x \vee z)$; \emph{distributive} if it satisfies $x \vee (y \wedge z) \approx (x \vee y) \wedge (x \vee z)$.
    % \begin{align*}
    %     &x \leq y \rightarrow x \vee (y \wedge z) \approx y \wedge (x \vee z) &&\text{(modularity)}
    %     \\&x \vee (y \wedge z) \approx (x \vee y) \wedge (x \vee z) &&\text{(distributivity)}
    % \end{align*}
\end{definition}
For instance, as showed in Figure~\ref{fig:small_n5m3}, $\mathbf{N}_5$ is neither modular nor distributive- considering the substitution $x = a, y= c, z= b$. The same substitution shows that $\mathbf{M}_3$ is not distributive. The classes of distributive and modular lattices show classical examples of varieties that can equivalently be characterized using equations and lattice omissions, as illustrated by the following theorems. 
\begin{theorem}[\citet{Dedekind1900}]\label{the:n5}
A lattice variety $\mathcal{V}$ is modular if and only if $\mathcal{V}$ omits $\mathbf{N}_5$.
\end{theorem}
\begin{theorem}[\citet{birkhoff1935structure}]\label{the:m3}
A lattice variety $\mathcal{V}$ is distributive if and only if $\mathcal{V}$ omits $\mathbf{N}_5$ and $\mathbf{M}_3$.
\end{theorem}
Starting from these classic results, the investigation of lattice omissions and the structural characterizations of classes of lattices has evolved into a rich and extensively studied field~\citep{JipsenRose}, but it was never approached with advanced AI methods before. 
% In Section~\ref{sec:ex}, we investigate a wide range of relevant properties of lattice varieties including semidistributive quasi-equations\cite{JonssonRival} (cf. Appendix~\ref{def:semis}).
% , which are proved to be relevant for lattice omission  \cite{JonssonRival}.
%  add to related works at the end of the paper
% Several examples of structural properties that characterize congruence lattices of algebraic classes can be found in \cite{AgBaFi,KearnesKiss2013,Nation1974,whitman1941free}. Consequently, it would be beneficial to explore the potential of the methodology described in the next section in this direction. Nevertheless, extending our approach to this domain is a nontrivial task that requires further investigation, and we intend to pursue it in future research.

%\fg{todo: add theorem for the properties/varieties that can be described as lattices omission} \cite{whitman1941free}

%We notice that similar characterizations of varieties in terms of lattices' omission have been produced for the lattice of congruences of a certain variety \fg{add reference or theorem} \cite{}. Hence, the method we describe in the next section could be further investigated in this direction. However, as it is not a trivial extension, we plan to extend our approach for this probelm in future work.
\begin{comment}
\subsection{Basics of universal algebra}
% \todo[@Stefano, @Francesco]


universal algebra (UA) is a branch of Mathematics that studies algebraic structures in a general and abstract way, without restricting to particular operations or sets of elements. In particular, it deals with the study of algebraic structures that are defined purely in terms of their operations and the axioms governing them.
%
The main focus of UA is to study algebraic structures that are common to many different areas of mathematics, such as groups, rings, fields, lattices, and Boolean algebras. It also deals with the study of the properties of algebraic structures that are preserved under various operations, such as homomorphisms and substructures.


\paragraph{Equational Properties and Lattices' Omission}

The field of General Algebra concerns the study of classes of algebraic structures that share relevant properties. An \emph{algebraic structure} is a pair $\mathbf{A} = (A, F)$ where $A$ is a nonempty set and $F$ is a set of operations on $A$. Famous examples of algebraic structure studied in various fields are Groups, Rings, Boolean Algebras, Fields, and so forth. A particularly relevant class of algebraic structure are Lattices which are studied both for their  connection with logical structures and for their algebraic properties. 

\begin{definition}
 A \emph{lattice} $\mathbf{L}$ is an algebraic structure composed of a non-empty set $L$ and two binary operations $\vee$ and $\wedge$ satisfying the following axioms and their duals obtained exchanging $\vee$ and $\wedge$:
\begin{align*}
&x \vee y \approx y \vee x  &&\text{(commutativity)}
\\&x \vee (y \vee z) \approx (x \vee y)  &&\text{(associativity)}
\\&x \vee x \approx x  &&\text{(idempotency)}
\\&x \approx x \vee (x \wedge y)  &&\text{(absorption)}
\end{align*}
\end{definition}
%class of lattice class of lattices is studied both for is connection with logical structures and for its algebraic properties. 

Essentially a lattice is a partially ordered set in which every pair of elements has a \emph{supremum} and an \emph{infimum}. These structures appear as congruence lattices of a generic algebra $\mathbf{A}$. In fact, for every algebra $\mathbf{A}$ the set of its \emph{congruences}, namely the equivalence relations on $A$ compatible with the operations of $\mathbf{A}$, form a lattice structure. Congruence lattices are of interest  since they encode many relevant properties of the original algebras as structural properties of lattice.

An interesting problem in algebra is the connection between the internal structure of an algebra and the identities which it satisfies. The study of varieties of algebras provides some insight into this problem.

%Important concepts often used to investigate such abstract problems are the so called congruence lattices of algebraic structures. Namely, those objects are graphs representing partially ordered sets such that every pair of elements has unique supremum and infimum determined by the underlying algebra. Several results in the literature witness the importance of this notion relatively to properties satisfied by algebraic structure, many of which can be described by omission or admission of certain subpatern in a graph. The genesis of this abstract branch of Algebra can be tracked to the pioneeristic work of Dedekind, which studied the classical problem of the admission of lattices as congruence sublattices of a given class of algebraic structures. In this paper our aim is to deal with these type of questions using tools provided by artificial intelligence. Intuitively these kind of problems reduce to identify graph subpatterns that witness structural properties of the whole graph and thus it lends itself to be attached from a neural prospective with architectures like Graph Neural Networks.


\paragraph{Relevant properties: Congruence Modularity and Distributivity} \label{sec:latt}
A fundamental problem in universal algebra is determining the properties of an algebraic variety $\mathcal{V}$ - a nonempty class of algebras of type $\sigma$, closed under homomorphic images, direct products, and subalgebras, (cfr. appendix \ref{aa}). Some varieties of lattices can be characterised through the omission of one or more lattices in the congruence lattice (cfr. appendix \ref{aa}) of its algebras.\\

% % Figure environment removed

\fg{Definition of Modularity and Distributivity to be given as equations}

\fg{To split definition from comments!}

\begin{definition} \textbf{Congruence Modularity} \\
In abstract algebra, a congruence relation is an equivalence relation on an algebraic structure such that algebraic operations done with equivalent elements will yield equivalent elements (compatibility). Every congruence relation has a corresponding quotient structure, whose elements are the equivalence classes (or congruence classes) for the relation. \cite{hungerford}\\
\end{definition}

\begin{definition} \textbf{Congruence Distributivity}\\
An algebra is congruence distributive if its lattice of congruence relations is a distributive lattice - i.e. is a lattice in which the operations of join and meet distribute over each other. \cite{hungerford}
Congruence distributivity has many structural consequences, the most important being JÃ³nsson's Lemma 1 \cite{jonson} which implies that a finitely generated congruence distributive variety is residually finite.\\
\end{definition}
\fg{I'd say explicitely that is the lattice of congruences of the variety to omit N5. It's enough a note}
\begin{theorem}\textbf{Congruence Modularity} \\
Let $\mathcal{V}$ be a variety. Then the following are equivalent \cite{proofbook}:
\begin{enumerate}
    \item $\mathcal{V}$ is congruence modular;
    \item $\mathcal{V}$ omits $N_5$;\\
\end{enumerate}
\label{theorem1}
\end{theorem}

\begin{theorem}\textbf{Congruence Distributivity}\\
Let $\mathcal{V}$ be a variety. Then the following are equivalent \cite{proofbook}:
\begin{enumerate}
    \item $\mathcal{V}$ is congruence distributive;
    \item $\mathcal{V}$ omits $N_5$ and $M_3$;\\
\end{enumerate}
\label{theorem2}
\end{theorem}


$N_5$ and $M_3$ are not the only lattices that affect algebraic properties. McKenzie et al. \cite{Mckenzie1970} for example discovered six structures that preclude semi-distributiveness.
\end{comment}
% \subsection{Graph Neural Networks}
% Graph Neural Networks (GNNs, ~\cite{scarselli2008graph}) are differentiable models designed to process relational data in the form of graphs. A graph can be defined as a tuple $G=(V,E)$ which comprises nodes $V = \{1, \dots, n\}$, the entities of a domain, and edges $E \subseteq \{1, \dots, n\} \times \{1, \dots, n\}$, the relations between pairs of nodes. Nodes (or edges) can be endowed with features $\mathbf{x}_i \in \mathbb{R}^d$, representing $d$ characteristics of each entity (or relation), and with $l$ ground truth task labels $y_i \in Y \subseteq \{0, 1\}^l$. A typical GNN $g$ learns a set of node embeddings $\mathbf{h}_i$ with a scheme known as message passing~\cite{gilmer2017neural}. 
% % Specifically, message passing aggregates for each node $i\in V$ local information shared by its neighboring nodes $N_i = \{k: (k,i) \in E \}$:
% % \begin{equation}
% %     \mathbf{h}_i = \sum_{k \in N_i} g(\textbf{m}_{ik}, \mathbf{x}_i) \qquad \textbf{m}_{ik} = \phi(\mathbf{x}_i, \mathbf{x}_k)
% % \end{equation}

% % where $\textbf{m}_{ik}$ is the aggregate of the feature vectors $\textbf{x}_i$ and $\textbf{x}_k$ of nodes $i$ and $k$, respectively, computed using a permutation invariant function $\phi$. 
% A readout function $f: H \rightarrow Y$ then processes the node embeddings to predict node labels $\hat{y}$. GNNs are trained via stochastic gradient descent minimizing the cross entropy loss between $\hat{y}_i$ and ground-truth $y_i$.

\section{Methods}
\label{sec:methods}
The problem of characterizing lattice varieties through lattice omission is very challenging as it requires the analysis of large (potentially infinite) lattices~\citep{birkhoff1935structure,Dedekind1900,jonsson}. % on small latticesand it quickly becomes intractable for large lattices
To address this task, we propose the first AI-assisted framework supporting mathematicians in finding empirical evidences to validate existing conjectures and to suggest novel theorems. To this end, we propose a general algorithm (Section \ref{sec:latt_data}) allowing researchers in universal algebra to define a property of interest and generate a dataset suitable to train AI models. We then introduce interpretable graph networks (Section \ref{sec:IGN}) which can suggest candidate lattices whose omission is responsible for the satisfaction of the given algebraic property.


\subsection{A Tool to Generate Datasets of Lattice Varieties}
\label{sec:latt_data}
% \begin{minipage}{.46\textwidth}
\begin{wrapfigure}[11]{R}{0.5\textwidth}
\vspace{-0.5cm}
\begin{algorithm}[H]
\caption{\small Generate dataset of lattice varieties.}
\label{alg:dataset}
% \SetCustomAlgoRuledWidth{0.45\textwidth}  
% \begin{algorithmic}
    % \Require
\DontPrintSemicolon
\SetInd{0.1em}{1em}
\KwIn{$n \geq 1$, \emph{hasProperty}($\cdot$,$\cdot$,$\cdot$)\tcp*[f]{\textit{$n$: cardinality}}}
$\textit{Dataset} = []$\;
$\textit{AllFuncs}\gets \textit{genAllFuncs}(n)$
\tcp*[f]{\textit{binary functions as $n\times n$ matrices}}\;
\For(\tcp*[f]{\textit{$L(i,j)=1$ meaning $i\leq_L j$}}){$L\in \textit{AllFuncs}$}
{\If(\tcp*[f]{\textit{check if $\leq_L$ is refl., antisym. and trans.}}){$\textit{isPartialOrder}(L)$}{
        \If(\tcp*[f]{\textit{check if $L$ is a lattice
        % each node pair has unique $\inf$ and $\sup$
        }}){$\textit{isLattice}(L)$}{
       \For{$i,j\leq n$}{
       $\wedge_L[i,j]\gets \sup_{x\leq L} \{ x\leq_L i \mbox{ and } x\leq_L j\}$\;
$\vee_L[i,j]\gets \inf_{x\leq L}\{ i\leq_L x\mbox{ and } j\leq_L x\}$
}
\If(\tcp*[f]{\textit{check $\wedge_L$,$\vee_L$ properties}}){$\textit{hasProperty}(L,\wedge_L,\vee_L)$}
{$\textit{Dataset}.append([L,\text{True}])$}
\Else{$\textit{Dataset}.append([L,\text{False}])$}
}
}
}
% \end{algorithmic}
\end{algorithm}
% \end{minipage}
\end{wrapfigure}
We propose a general methodology to investigate any algebraic property whose validity can be verified on a finite lattice. In this work, we focus on properties that can be characterized via equations 
and quasi-equations. To train AI models, we propose a general dataset generator\footnote{The dataset generator code and the generated datasets will be made public in case of paper acceptance.} for lattice varieties (Algorithm \ref{alg:dataset}). The generator takes as input the number of nodes $n$ in the lattices  and a function to check whether a lattice satisfies a given property. We generate $2^{n\times n}$ matrices of size $n\times n$, containing all binary functions definable on $\{1,\ldots,n\}^2$, and filter only binary matrices representing partial orders\footnote{Our algorithm optimizes this step considering only reflexive and antisymmetric binary relations, and enforces transitivity with an easy fix-point calculation.}. Then, we verify that the partial ordered set $L$ is a lattice, by checking that any pair of nodes always has a unique infimum and supremum. This directly verifies that $\wedge_L$ and $\vee_L$ satisfy Definition~\ref{def:back_lattice}. Finally, we check whether the lattice satisfies the target property or not, and append it and the property label to our dataset. We remark that checking the validity of a single ternary equation on a medium-size lattice is not computationally prohibitive (i.e., it ``only'' requires checking $n^3$ identities), but the number of existing lattices increases exponentially as $n$ increases. For instance, it is known that there are at least 2,000,000 non-isomorphic lattices with $L=10$ elements~\cite{berman2000counting}. Therefore, we only sample a fixed number of lattices per cardinality starting from a certain node cardinality. While this may seem a strong bias, we notice that known and relevant lattice omissions often rely on lattices with few nodes~\cite{birkhoff1935structure, Dedekind1900}. To empirically verify that this is not a significant limitation, in our experiments we deliberately investigate the generalization capacity of GNNs when trained on small-size lattices and tested on larger ones. This way we can use GNNs to predict the satisfiability of equational properties on large graph structures without explicitly checking them. Using Algorithm~\ref{alg:dataset}, we generated the first large-scale AI-compatible datasets of lattices containing more than $29,000$ graphs and the labels of $5$ key properties of lattice (quasi-)varieties i.e., modularity, distributivity, semi-distributivity, join semi-distributivity, and meet semi-distributivity, whose definitions can be found in Appendix \ref{app:algebra}.


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{algorithm}
% \caption{Dataset generation of distributive lattices with fixed cardinality}
% \label{alg:dataset}
% \begin{algorithmic}
% \Require $n \geq 1$ \textit{\Comment{$n$: cardinality of  lattices}}
% \State $\textit{Lattices} = []$
% \State $\textit{AllFuncs}\gets \textit{genAllFunctions}(n)$
% \textit{\Comment{$\textit{AllFuncs}$ being a list of $2^{n\times n}$ matrices of size $n\times n$, containing all the binary functions definable on $\{1,\ldots,n\}^2$.}}
% \For{$L\in \textit{AllFuncs}$}
%         % \State $M \gets \textit{def\_LoE}(i,n)$ \Comment{$\textit{def\_LoE}$ defines the matrix corresponding to the $i$-th indicator function of a partial order $LoE$ on the $n\times n$ nodes.}
%         % \Comment{To make this step faster, we only consider functions $LoE$: $LoE(i,j)=1$ only if $i\leq  j$.}
%         \If{$\textit{isPartialOrder}(L)$} \textit{\Comment{Checking if $L$ is reflexive, antisymmetric and transitive. Intuitively $L(i,j)=1$ means that the node $i$ is less or equal that $j$ wrt $L$ (i.e. $i\leq_L j$).}}
%         % \State $L \gets \textit{trans\_clo}(M)$ \Comment{Transitive closure of M.}
%         \If{$\textit{isLattice}(L)$} 
%         \textit{\Comment{Checking if for each pairs
%        of nodes in $L$, there exist uniquely their infimum and supremum.}}
%        \For{$i,j\leq n$}
%        \State $\wedge_L[i,j]\gets \sup_{x\leq n} \{ x\leq_L i \mbox{ and } x\leq_L j\}$
% \State $\vee_L[i,j]\gets \inf_{x\leq n}\{ i\leq_L x\mbox{ and } j\leq_L x\}$
% \EndFor
%         \If{$\textit{isDistributive}(L)$} \textit{\Comment{Checking if $L$ satisfies the distributive law with $\wedge_L,\vee_L$}}
%         \State $\textit{distributive} = ``True"$
%         \Else 
%         \State $\textit{distributive} = ``False"$
%         \EndIf
%         \State $\textit{Lattices}.append([L,\textit{distributive}])$
%         \EndIf
%         \EndIf
% \EndFor
% \end{algorithmic}
% \end{algorithm}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \fg{Motivating Example?}
% \begin{example}[Dataset]
% In this motivating example, we consider the problem of classifying lattices using a binary classification task. Our dataset is composed only of lattices of dimension up to 100 with supervision indicating if the lattice is modular or not, as well as if it is distributive or not. The goal of the classifier is to accurately predict whether a given lattice belongs to the variety of distributive or modular lattices.
% \end{example}






\subsection{Interpretable Graph Networks (iGNNs)}
\label{sec:IGN}

In this section, we design an interpretable graph network (iGNN, Figure~\ref{fig:ignn}) that satisfies the notion of "interpretability" introduced by~\citet{rudin2019stop}. According to this definition, a machine learning (ML) system is interpretable if and only if (1) its inputs are semantically meaningful, and (2) its model inference is simple for humans to understand (e.g., sparse and/or symbolic). This definition covers ML systems that take tabular datasets or sets of concepts as inputs, and (piece-wise) linear models such as logistic regression or decision trees. To achieve this goal in GNNs, we introduce an interpretable graph layer that learns semantically meaningful concepts and uses them as inputs for a simple linear classification layer. We then show how this layer can be included into existing architectures or into hierarchical iGNNs, which consist of a sequence of interpretable graph layers.

\subsubsection{Interpretable Graph Layer}
The interpretable graph layer (Figure~\ref{fig:ignn}) serves three main functions: message passing, concept generation, and task predictions. The first step of the interpretable graph layer involves a standard message passing operation (Eq.~\ref{eq:node-concepts} right), which aggregates information from node neighbors. This operation enables to share and process relational information across nodes and it represents the basis of any GNN layer.


% Figure environment removed


\paragraph{Node-level concepts.}
An interpretable concept space is the first step towards interpretability. Following~\citet{ghorbani2019towards}, a relevant concept is a ``high-level human-understandable unit of information'' shared by input samples and thus identifiable with clustering techniques. Message passing algorithms do cluster node embeddings based on the structure of node neighborhoods, as observed by~\citet{magister2021gcexplainer}. However, the real-valued large embedding representations $\mathbf{h}_i \in \mathbb{R}^q, q\in \mathbb{N}$ generated by message passing can be challenging for humans to interpret. To address this, we use a hard Gumbel-Softmax activation $\Theta: \mathbb{R}^q \mapsto \{0,1\}^q$, following~\citet{azzolin2022global}: 
\begin{equation} \label{eq:node-concepts}
\mathbf{c}_i = \Theta \big( \mathbf{h}_i \big) 
\qquad 
\mathbf{h}_i = \phi \Big(\mathbf{x}_i, \bigoplus_{j \in N_i} \psi(\mathbf{x}_i, \mathbf{x}_j)\Big)
\end{equation}
where $\psi$ and $\phi$ are learnable functions aggregating information from a node neighborhood $N_i$, and $\oplus$ is a permutation invariant aggregation function (such as sum or mean). During the forward pass, the Gumbel-Softmax activation $\Theta$ produces a one-hot encoded representation of each node embedding. Since nodes sharing the same neighborhood have similar embeddings $\mathbf{h}_i$ due to message passing, they will also have the same one-hot vector $\mathbf{c}_i$ due to the Gumbel-Softmax, and vice versa - we can then interpret nodes having the same one-hot concept $\mathbf{c}_i$ as nodes having similar embeddings $\mathbf{h}_i$ and thus sharing a similar neighborhood. 
More formally, we can assign a semantic meaning to a reference concept $\gamma \in \{0,1\}^q$ by visualizing concept prototypes corresponding to the inverse images of a node concept vector. In practice, we can consider a subset of the input lattices $\Gamma$ corresponding to the node's ($p$-hop)  neighborhood covered by message passing:
\begin{align}
    \Gamma (\gamma, p) = \Big\{ \mathbf{L}^{\langle i,p \rangle} \mid i \in L \wedge \ \mathbf{L} \in \mathcal{D} \wedge \ \mathbf{c}_i = \gamma \Big\}
\end{align}
where $\mathcal{D}$ is the set of all training lattices, and $\mathbf{L}^{\langle i,p \rangle}$ is the graph corresponding to the $p$-hop neighborhood ($p \in \{1,\dots,|L|\}$) of the node $i \in L$, as suggested by~\citet{ghorbani2019towards,magister2021gcexplainer}. This way, by visualizing concept prototypes as subgraph neighborhoods, the meaning of the concept representation becomes easily interpretable to humans (Figure~\ref{fig:ignn}), aiding in the understanding of the reasoning process of the network.


\begin{example}[Interpreting node-level concepts]
\label{ex:nc}
Consider the problem of classifying distributive lattices with a simplified dataset including $\mathbf{N}_5$ % Figure removed and $\mathbf{M}_3$ % Figure removed only, and where each node has a constant feature $x_i = 1$. As these two lattices only have nodes with 2 or 3 neighbours, one layer of message passing will then generate only two types of node embeddings e.g., $\mathbf{h}_{II} = [0.2, -0.4, 0.3]$ for nodes with a 2-nodes neighborhood (e.g., % Figure removed), and $\mathbf{h}_{III} = [0.6, 0.2, -0.1]$ for nodes with a 3-nodes neighborhood (e.g., % Figure removed).
As a consequence, the Gumbel-Softmax will only generate two possible concept vectors e.g., $\mathbf{c}_{II} = [0, 0, 1]$ and $\mathbf{c}_{III} = [1, 0, 0]$. 
%
Hence, for instance the concept % Figure removed belongs to $\mathbf{c}_{II}$, while % Figure removed belongs to $\mathbf{c}_{III}$.
% The one-to-one mapping between the structure of the neighborhood and the concept activation makes the learned concepts easily interpretable e.g., $\mathbf{c}_i = [1,0,0] \iff$ % Figure removed or $\mathbf{c}_j = [0,0,1] \iff$ % Figure removed. 
\end{example}

\paragraph{Graph-level concept embeddings.}
To generate a graph-level concept space in the interpretable graph layer, we can utilize the node-level concept space produced by the Gumbel-Softmax. Normally, graph-level embeddings are generated by applying a permutation invariant aggregation function on node embeddings. However, in iGNNs we restrict the options to (piece-wise) linear permutation invariant functions in order to follow our interpretability requirements dictated by~\citet{rudin2019stop}. This restriction still includes common options such as max or sum pooling. Max pooling can easily be interpreted by taking the component-wise max over the one-hot encoded concept vectors $\mathbf{c}_i$. After max pooling, the graph-level concept vector has a value of $1$ at the $k$-th index if and only if at least one node activates the $k$-th concept i.e., $\exists i \in L, \mathbf{c}_{ik} = 1$. Similarly, we can interpret the output of a sum pooling: a graph-level concept vector takes a value $v \in \mathbb{N}$ at the $k$-th index after sum pooling if and only if there are exactly $v$ nodes activating the $k$-th concept i.e., $\exists i_0,\dots,i_v \in L, \mathbf{c}_{ik} = 1$.

\begin{example}[Interpreting graph-level concepts]
\label{ex:gc}
Following Example \ref{ex:nc}, let us use sum pooling to generate graph-level concepts. For an $\mathbf{N}_5$ graph, we have 5 nodes with exactly the same 2-node neighborhood. Therefore, sum pooling generates a graph-level embedding $[0,0,5]$, which certifies that we have 5 nodes of the same type e.g., % Figure removed. For an $\mathbf{M}_3$ graph, the top and bottom nodes have a 3-node neighborhood e.g., % Figure removed, while the middle nodes have a 2-node neighborhood e.g., % Figure removed. This means that sum pooling generates a graph-level embedding $[2, 0, 3]$, certifying that we have 2 nodes of type % Figure removed and 3 nodes of type  % Figure removed.
\end{example}

% \paragraph{Concept visualization}

\paragraph{Interpretable classifier.}
To prioritize the identification of relevant concepts, we use a classifier to predict the task labels using the concept representations. A black-box classifier like a multi-layer perceptron~\cite{mlp} would not be ideal as it could compromise the interpretability of our model, so  instead we use an interpretable linear classifier such as a single-layer network~\cite{slp_Karimboyevich_Nematullayevich_2022}. This allows for a completely interpretable and differentiable model from the input to the classification head, as the input representations of the classifier are interpretable concepts and the classifier is a simple linear model which is intrinsically interpretable as discussed by~\citet{rudin2019stop}. In fact, the weights of the perceptron can be used to identify which concepts are most relevant for the classification task. Hence, the resulting model can be used not only for classification, but also to interpret and understand the problem at hand.

\subsubsection{Interpretable architectures}
The interpretable graph layer can be used to instantiate different types of iGNNs. One approach is to plug this layer as the last message passing layer of a standard GNN architecture: 
\begin{align}
& \hat{y} = f\Big(\boxplus_{i \in K} \ \ \Big(\Theta \Big(\phi^{(K)} \Big(\mathbf{h}_i^{(K-1)}, \bigoplus_{j \in N_i} \psi^{(K)}(\mathbf{h}_i^{(K-1)}, \mathbf{h}_j^{(K-1)})\Big)\Big)\Big)\Big) \\
& \mathbf{h}_i^{(l)} = \phi^{(l)} \Big(\mathbf{h}_i^{(l-1)}, \bigoplus_{j \in N_i} \psi^{(l)}(\mathbf{h}_i^{(l-1)}, \mathbf{h}_j^{(l-1)})\Big)
\quad l = 1,\dots,K
\end{align}
where $f$ is an interpretable classifier (e.g., single-layer network), $\boxplus$ is an interpretable piece-wise linear and permutation-invariant function (such as max or sum), $\Theta$ is a Gumbel-Softmax hard activation function, and $\mathbf{h}_i^0 = \mathbf{x}_i$. In this way, we can interpret the first part of the network as a feature extractor generating well-clustered latent representations from which concepts can be extracted. This approach is useful when we only care about the most complex neighborhoods/concepts. Another approach is to generate a hierarchical transparent architecture where each GNN layer is interpretable:
\begin{equation}
\hat{y}^{(l)} = f\Big(\boxplus_{i \in K} \Big(\ \Theta \Big(\mathbf{h}_j^{(l)} \Big) \Big)\Big)
\qquad l = 1,\dots,K
\end{equation}
In this case, we can interpret every single layer of our model with concepts of increasing complexity. The concepts extracted from the first layer represent subgraphs corresponding to the $1$-hop neighborhood of a node, those extracted at the second layer will correspond to $2$-hop neighborhoods, and so on. These hierarchical iGNNs can be useful to get insights into concepts with different granularities. By analyzing the concepts extracted at each layer, we gain a better understanding of the GNN inference and of the importance of different (sub)graph structures for the classification task.

\subsubsection{Training}
The choice of the activation and loss functions iGNNs depends on the nature of the task at hand and does not affect their interpretability. For classification tasks, we use standard activation functions such as softmax or sigmoid, along with standard loss functions like cross-entropy. For hierarchical iGNNs (HiGNNs), we apply the loss function at each layer of the concept hierarchy, as their layered architecture enables intermediate supervisions. This ensures that each layer is doing its best to extract the most relevant concepts to solve the task. Internal losses can also be weighted differently to prioritize the formation of optimal concepts of a specific size, allowing the HiGNN to learn in a progressive and efficient way. 
% By doing so, we can train the HiGNN in a progressive and effective manner, allowing us to uncover concepts with different granularities, and gain insights into how the model is performing at different levels of abstraction.

% \subsection{}


\section{Experimental Analysis}
\label{sec:ex}
\subsection{Research questions}
In this section we analyze the following research questions:
\begin{itemize}
    \item \textbf{Generalization} - Can GNNs generalize when trained to predict universal algebra's properties? Can interpretable GNNs generalize as well?
    \item \textbf{Interpretability} - Do interpretable GNNs concepts empirically validate universal algebra's conjectures? How can concept-based explanations suggest novel conjectures? 
    % Can we use interpretable GNNs to provide insights on mutual relationships between lattice properties?
    % \item \textbf{Concept Quality} - Are the concepts identified by interpretable GNNs pure? Is the concept space complete?
\end{itemize}

\subsection{Setup}
\paragraph{Baselines.}
For our comparative study, we evaluate the performance of iGNNs and their hierarchical version against equivalent GNN models (i.e., having the same hyperparameters such as number layers, training epochs, and learning rate). For vanilla GNNs we resort to common practice replacing the Gumbel-Softmax with a standard leaky ReLU activation. We exclude from our main baselines prototype or concept-based GNNs pre-defining graph structures for explanations, as for most datasets these structures are unknown. Appendix~\ref{app:baselines} covers implementation details. We show more extensive results including local and post-hoc explanations in Appendix~\ref{app:local-xai}.
% Similarly, we do not consider local explainability methods such as GNNExplainer or integrated gradients, as our focus is to understand the behavior of the GNN on a global scale. 
% We also avoid comparing against post-hoc methods, such as GLGExplainer or GCExplainer~\citep{magister2021gcexplainer}, since we are interested in obtaining the exact and precise reason for the GNN's predictions, without relying on surrogate models to approximate the decision-making mechanism.

\paragraph{Evaluation.}
We employ three quantitative metrics to assess a model's generalization and interpretability. We use the Area Under the Receiver Operating Characteristic (AUC ROC) curve to assess task generalization. We evaluate generalization under two different conditions: with independently and identically distributed train/test splits, and out-of-distribution by training on graphs up to eight nodes, while testing on graphs with more than eight nodes (``strong generalization''). We further assess generalization under binary and multilabel settings (classifying 5 properties of a lattice at the same time). To evaluate interpretability, we use standard metrics such as completeness~\citep{yeh2020completeness} and fidelity~\citep{ribeiro2016should}. Completeness\footnote{We assess the recall of the completeness as the datasets are very unbalanced towards the negative label.} assesses the quality of the concept space on a global scale using an interpretable model to map concepts to tasks, while fidelity measures the difference in predictions obtained with an interpretable surrogate model and the original model. Finally, we evaluate the meaningfulness of our concept-based explanations by visualizing and comparing the generated concepts with ground truth lattices like e.g. $\mathbf{M}_3$ and $\mathbf{N}_5$, whose omission is known to be significant for modular and distributive properties. All metrics in our evaluation, across all experiments, are computed on test sets using $5$ random seeds, and reported using the mean and $95\%$ confidence interval. 


\section{Key Findings}
% \subsection{Generalization}

\subsection{Generalization}
\paragraph{iGNNs improve interpretability without sacrificing task accuracy (Figure~\ref{fig:comp-fidelity}).}
Our experimental evaluation reveals that interpretable GNNs are able to strike a balance between completeness and fidelity, two crucial metrics that are used to assess generalization-interpretability trade-offs~\citep{ribeiro2016should}. 
% The completeness score evaluates the quality of the concept space, while the fidelity score measures the accuracy of the predictions obtained with an interpretable classifier instead of a black-box. 
We observe that the multilabel classification scenario, which requires models to learn a more varied and diverse set of concepts, is the most challenging and results in the lowest completeness scores on average. We also notice that the more challenging out-of-distribution scenario results in the lowest completeness and fidelity scores across all datasets. More importantly, our findings indicate that iGNNs achieve optimal fidelity scores, as their classification layer consists of a simple linear function of the learnt concepts which is intrinsically interpretable~\citep{rudin2019stop}.
% , aligning with the existing literature which suggests that a model with fidelity score of $100\%$ is considered to be interpretable. 
On the contrary, interpretable surrogate models of black-box GNNs exhibit, as expected, lower fidelity scores, confirming analogous observations in the explainable AI literature~\citep{ribeiro2016should,rudin2019stop}. In practice, this discrepancy between the original black-box predictions and the predictions obtained with an interpretable surrogate model questions the actual usefulness of black-boxes when interpretable alternatives achieve similar results in solving the problem at hand, as extensively discussed by~\citet{rudin2019stop}. Overall, these results demonstrate how concept spaces are highly informative to solve universal algebra's tasks and how the interpretable graph layer may improve GNNs' interpretability without sacrificing task accuracy. We refer the reader to Appendix \ref{app:completeness} for detailed discussion on quantitative analysis of concept space obtained by iGNNs under different generalization settings with comparisons to their black-box counterparts. 

% Figure environment removed


\paragraph{GNNs strongly generalize on universal algebra's tasks (Figure~\ref{fig:strong-generalization}).}
Our experimental findings demonstrate the strong generalization capabilities of GNNs across the universal algebra tasks we designed. Indeed, we stress GNNs test generalization abilities by training the models on graphs of size up to $n$ (with $n$ ranging from $5$ to $8$), and evaluating their performance on much larger graphs of size up to $50$. We designed this challenging experiment in order to understand the limits and robustness of interpretable GNNs when facing a significant data distribution shift from training to test data. 
\begin{wrapfigure}[14]{r}{0.7\textwidth}
    \centering
    % Figure removed
    \caption{Strong generalization performance with respect to the maximum number of nodes used in training.}
    \label{fig:strong-generalization}
\end{wrapfigure}
Remarkably, iGNNs exhibit robust generalization abilities (similar to their black-box counterparts) when trained on graphs up to size $8$ and tested on larger graphs. This evidence confirms the hypothesis that interpretable models can deliver reliable and interpretable predictions, as suggested by \citet{rudin2019stop}. However, we observe that black-box GNNs slightly outperform iGNNs when trained on even smaller lattices.
We hypothesize that this is due to the more constrained architecture of iGNNs, which imposes tighter bounds on their expressiveness when compared to standard black-box GNNs.
Notably, training with graphs of size up to $5$ or $6$ significantly diminishes GNNs generalization in the tasks we designed. We hypothesize that this is due to the scarcity of non-distributive and non-modular lattices during training, but it may also suggest that some patterns of size $7$ and $8$ might be quite relevant to generalize to larger graphs. Unfortunately, running generalization experiments with $n \leq 4$ was not possible since all such lattices trivially omitted $\mathbf{N}_5$ and $\mathbf{M}_3$. It is worth mentioning that GNNs performed well even in the challenging multilabel case, where they had to learn a wider and more diverse set of concepts and tasks. In all experiments, we observe a plateau of the AUC ROC scores for $n=8$, thus suggesting that a training set including graphs of this size might be sufficient to learn the relevant patterns allowing the generalization to larger lattice varieties. For detailed numerical results across all tasks, we refer the reader to Table~\ref{tab:auc} in Appendix~\ref{app:generalization}. Overall, these results emphasize the potential of GNNs in addressing complex problems in universal algebra, providing an effective tool to handle lattices that are difficult to analyze manually with pen and paper.

% Our experimental results demonstrate that interpretable GNNs generalize as well as black-box GNN across universal algebra tasks.
% Our experimental results demonstrate that GNNs well generalize across all universal algebra's tasks we designed. Specifically, we trained the models on graphs of size up to 8 and evaluated their performance on larger graphs, which presents a significantly more challenging task due to the shift in data distribution.
% Despite this difficulty, our findings reveal that the GNNs achieved an impressive mean accuracy of X\% on the larger graphs. This level of performance indicates that the models are highly robust and able to generalize effectively to previously unseen instances. 
% We also observed that interpretable graph networks (iGNNs and HiGNNs) attain high task performance, often outperforming equivalent black-box GNNs, reinforcing the notion that interpretable models can be relied upon to provide reliable and interpretable predictions as suggested by~\citet{rudin2019stop}.
% Our experimental results demonstrate that GNNs generalize well across the universal algebra tasks we designed. Specifically, we trained the models on graphs of size up to $n$, where $n$ is between 5 and 8, and evaluated their performance on larger graphs, which presents a significantly more challenging task due to the shift in data dist distribution. Despite this difficulty, our findings reveal that iGNNs achieved strong generalization capabilites similar to their black-box counterpart when trained on graphs with size up to 8, reinforcing the notion that interpretable models can be relied upon to provide reliable and interpretable predictions as suggested by~\citet{rudin2019stop}. However, we also notice that when we train with even smaller lattices, black-box GNNs outperform iGNNs. We hypothesize that this is a result of a more constrained iGNN architecture which provides tighter bounds to their representation expressiveness than that of black-box GNNs. We note that training with graphs with a largest size of 5 and 6 greatly diminishes generalization strength due to the scarcity of non-distributive and non-modular lattices. In fact, it is not possible to run generalization experiments with $n \leq 4$ since all such lattices trivially omit $\mathbf{N}_5$ and $\mathbf{M}_3$.
% Finally, GNNs performed well even in the challenging multilabel case, where they had to learn a wider and more diverse set of concepts and tasks (AUCROC was competitive with binary classification scenarios for $n=8$). We refer the reader to Table \ref{tab:auc} in Appendix \ref{app:generalization} for numerical results over all tasks. Overall, these results highlight the potential of GNNs to tackle challenging problems in universal algebra and provide an effective tool to handle lattices that are difficult to analyze manually on pen and paper.



% \subsection{Interpretability}
% \paragraph{Interpretable GNNs generate intuitive concept-based explanations}
% \missingfigure{\note{ADD EXAMPLES OF CONCEPT-BASED EXPLANATIONS}}

\subsection{Interpretability}
\paragraph{Concept-based explanations empirically validate universal algebra's conjectures (Figure~\ref{fig:concept-ranking}).}
We present empirical evidence to support the validity of theorems \ref{the:n5} and \ref{the:m3} by examining the concepts generated for modular and distributive tasks. For this investigation we leverage the interpretable structure of iGNNs. Similarly to~\citet{ribeiro2016should}, we visualize in Figure~\ref{fig:concept-ranking} the weights of our trained linear classifier representing the relevance of each concept. We remark that the visualization is limited to the (top-$5$) most negative weights, as we are interested in those concepts that negatively affect the prediction of a property. In the same plot, we also show the prototype of each concept represented by the 2-hop neighborhood of a node activating the concept, following a similar procedure as ~\citet{ghorbani2019towards,magister2021gcexplainer,azzolin2022global}. Using this visualization, we investigate the presence of certain concepts when classifying modular and distributive lattices. 
\begin{wrapfigure}[14]{r}{0.65\textwidth}
    \centering
    \vspace{-4mm}
    % Figure removed
    % Figure removed
    \caption{Ranking of relevant clusters of lattices (x-axis) according to the interpretable GNN linear classifier weights (y-axis, the lower the more relevant the cluster). $\mathbf{N}_5$ is always the most important lattice to omit for modularity, while both $\mathbf{M}_3$ and $\mathbf{N}_5$ are relevant for distributivity, thus validating theorems~\ref{the:n5} and~\ref{the:m3}.}
    \label{fig:concept-ranking}
\end{wrapfigure}
For the modularity task, our results show that the lattice $\mathbf{N}_5$ appears among non-modular concepts, but is never found in modular lattices, while the lattice $\mathbf{M}_3$ appears among both modular and non-modular concepts, which is consistent with Theorem \ref{the:n5}. In the case of distributivity, we observe that both $\mathbf{M}_3$ and $\mathbf{N}_5$ are present among non-distributive concepts, and are never found in distributive lattices, which is also in line with Theorem \ref{the:m3}.
These findings provide a large-scale empirical evidence for the validity of theorems \ref{the:n5} and \ref{the:m3}, and further demonstrate the effectiveness of graph neural networks in learning and analyzing lattice properties. Overall, these results highlight how interpretable GNNs can not only learn the properties of universal algebra but also identify structures that are unique to one type of lattice (e.g., non-modular) and absent from another (e.g., modular), thus providing human-interpretable explanations for what the models learn.



\paragraph{Contrastive explanations highlight topological differences between properties of lattice varieties (Figure~\ref{fig:contrastive}).}
% With evidence that our method works (as shown for modular and distributive tasks), 
We leverage interpretable GNNs to analyze the key topological differences of classical lattice properties such as join and meet semi-distribuitivity characterized by relevant quasi-equations (cf. Appendix~\ref{def:semis}).  %gain insights into other universal algebra tasks that currently lack conjectures. %For example, the meet-semi-distributive property currently lacks a conjecture identifying the (set of) lattice(s) whose omission makes a lattice meet-semi-distributive. 
To this end, we visualize specific concept prototypes corresponding to lattices that are not meet semi-distributive against lattices that are meet semi-distributive.
\begin{wrapfigure}[8]{r}{0.65\textwidth}
    \centering
    \vspace{-3.5mm}
    % Figure removed
    % Figure removed
    % % Figure removed
    \caption{Contrastive explanations showing lattice varieties with a a pair of discording labels to highlight the key difference between join and meet semi-distributivity.}
    \label{fig:contrastive}
\end{wrapfigure}
We observe $\mathbf{N}_5$ but not $\mathbf{M}_3$ among the concepts of meet semi-distributive lattices, while we observe both $\mathbf{N}_5$ and $\mathbf{M}_3$ only in concepts that are not meet semi-distributive. This observation suggests that $\mathbf{N}_5$ is not a key lattice for meet semi-distributive lattices, \emph{unlike distributive lattices}. Furthermore, we find that the lattice pattern % Figure removed is relevant for non meet semidistributivity, while its dual % Figure removed is relevant for non join semidistributivity, thus empirically confirming the hypotheses of \citet{JonssonRival}. These findings are significant because they demonstrate how analyzing concepts in interpretable GNNs can provide universal algebraists with a powerful and automatic tool to formulate new conjectures based on identifying simple lattices that play a role in specific properties. By leveraging the power of interpretable GNNs, we may uncover previously unknown connections between different properties and identify new patterns and structures that could lead to the development of new conjectures and theorems in universal algebra, providing exciting opportunities for future research in universal algebra.




% \paragraph{Interpretable GNNs provide insights on relationships among universal algebra's properties  (Figure~\ref{fig:multilabel})}
% Interpretable GNNs may represent a valuable tool for gaining insights into the relationships among different universal algebra properties. We show this in the multilabel scenario, where the model is trained to learn multiple properties simultaneously. Here the network must identify all the relevant concepts for all tasks at the same time, which makes this task more challenging compared to the binary classification setup. The resulting model provides valuable information on the similarities and differences among different properties based on the learnt concepts. To visualize this, we used a subset of concepts represented by colored circles in the embedding space of an interpretable GNN. The color of each circle corresponds to a unique string of labels shared by all samples in the cluster, and its size is proportional to the number of samples belonging to the concept. The sub-lattice corresponding to the input structure that activates the concept is also displayed on top of the circle. This approach allows us to have a global overview of the concept space and identify the relationships between different properties. For example, we observe that the concept of "non modular and non join semidistributive" might be closely related to the concept of being "non meet semi distributive and non join semidistributive". These concepts are close in the embedding space, and as a consequence, their corresponding lattices are also similar. Overall, these results show that interpretable GNNs offer a powerful tool for analyzing the relationships between universal algebra properties, explaining connections among properties in terms of concepts, which can then be leveraged by universal algebra specialists as a hint when developing new conjectures. 

% % Figure environment removed
% % Figure environment removed



% \subsection{Key Finding 3}

\section{Discussion}


\paragraph{Relations with Graph Neural Network explainability.}
Graph Neural Networks (GNNs,\citep{scarselli2008graph}) process relational data generating node representations by combining the information of a node with that of its neighbors, thanks to a general learning paradigm known as message passing~\cite{gilmer2017neural}. A number of post-hoc explainability techniques have been proposed to explain the reasoning of GNNs. Inspired by vision approaches~\citep{simonyan2013deep,ribeiro2016should,ghorbani2019towards}, early explainability techniques focused on feature importance~\citep{pope2019explainability}, while subsequent works aimed to extract local explanations~\citep{ying2019gnnexplainer,luo2020parameterized,vu2020pgm} or global explanations using conceptual subgraphs by clustering the activation space~\citep{magister2021gcexplainer,zhang2022protgnn,magister2022encoding}. However, all these techniques either rely on pre-defined subgraphs for explanations (which are often unknown in UA) or provide post-hoc explanations which may be brittle and unfaithful as extensively demonstrated by~\citet{rudin2019stop}. On the contrary, our experiments show that iGNNs generate interpretable predictions according to~\citet{rudin2019stop} notion of interpretability via linear classifiers applied on sparse human-understandable concept representations.



\paragraph{Limitations.}

The approach proposed in this paper focuses on  universal algebra conjectures characterized both algebraically and topologically. Our methodology is limited to finite lattices, which may not capture all relevant information about infinite algebraic structures. However, the insights gained from finite-lattice explanations can still provide valuable information regarding a given problem (albeit with potentially limited generalization).
%
Moreover, our approach is restricted to topological properties on graphs, while non-structural properties 
% not captured by our approach 
may require the adoption of other kinds of (interpretable) models. 
% To address this limitation, we plan to explore different types of explainers in future research. By doing so, we aim to develop a more comprehensive understanding of the properties and structures of lattices and other algebraic structures.

% This study presents a novel approach that employs artificial intelligence (AI) to investigate conjectures in universal algebra, which are characterized both algebraically and topologically. 
% However, our methodology is limited to finite structures, which may not capture all relevant information about infinite structures. Nevertheless, the insights gained from our finite-lattice explanations can provide valuable information regarding a given problem, albeit with potentially limited generalization.
% %
% Moreover, our approach is restricted to structural properties, and non-structural properties may not be captured by our explainer. To address this limitation, we plan to explore different types of explainers in future research. By doing so, we aim to develop a more comprehensive understanding of the properties and structures of lattices and other algebraic structures.


\paragraph{Broader impact and perspectives.}
AI techniques are becoming increasingly popular for solving previously intractable mathematical problems and proposing new conjectures~\cite{lampledeep, loveland2016automated,Davies2021,he2022machine}. However, the use of modern AI methods in universal algebra was a novel and unexplored field until the development of the approach presented in this paper. To this end, our method uses interpretable graph networks to suggest graph structures that characterize relevant algebraic properties of lattices. With our approach, we empirically validated~\citet{Dedekind1900} and~\citet{birkhoff1935structure} theorems on distributive and modular lattices, by recovering relevant lattices. This approach can be readily extended---beyond equational properties determined by the omission of a sublattice in a variety~\cite{whitman1941free}---to any structural property of lattices, including the characterization of congruence lattices of algebraic varieties~\cite{AgBaFi,KearnesKiss2013,Nation1974,whitman1941free}. Our methodology can also be applied (beyond universal algebra) to investigate (almost) any mathematical property that can be topologically characterized on a graph, such as the classes of graphs/diagraphs with a fixed set of polymorphisms~\cite{Barto1,Barto,Olsak}. However, as universal algebra is a foundational branch of modern mathematics, any contribution to this field can already have significant implications in various mathematical disciplines.



\paragraph{Conclusion.}


This paper presents the first-ever AI-assisted approach to investigate equational and topological conjectures in the field of universal algebra. To this end, we present a novel algorithm to generate datasets suitable for AI models to study equational properties of lattice varieties. While topological representations would enable the use of graph neural networks, the limited transparency and brittle explainability of these models hinder their use in validating existing conjectures or proposing new ones. For this reason, we introduce a novel neural layer to build fully interpretable graph networks to analyze the generated datasets. The results of our experiments demonstrate that interpretable graph networks: enhance interpretability without sacrificing task accuracy, strongly generalize when predicting universal algebra's properties, generate simple explanations that empirically validate existing conjectures, and identify subgraphs suggesting the formulation of novel conjectures. These promising results demonstrate the potential of our methodology, opening the doors of universal algebra to AI with far-reaching impact across all mathematical disciplines.





%\section*{References}

% References follow the acknowledgments. Use unnumbered first-level heading for
% the references. Any choice of citation style is acceptable as long as you are
% consistent. It is permissible to reduce the font size to \verb+small+ (9 point)
% when listing the references.
% Note that the Reference section does not count towards the page limit.
% \medskip

\bibliographystyle{plainnat}
\bibliography{references}

\newpage

\appendix


\section{Algebra definitions}
\label{app:algebra}

\subsection{Formal defintions for Universal Algebra}
Universal algebra is the field of mathematics that studies algebraic structures, which are defined as a set $A$ along with its own collection of operations. An $n$-ary operation on $A$ is a function that takes $n$ elements of $A$ and returns a single element from the set. More formally \cite{BurrisSanka,day_1969,Jonnson_1967}:\\
\begin{definition}\textbf{N-ary function}
For $A$ non-empty set and $n$ nonnegative integer we define $A^0 = \{\emptyset\}$ and, for $n > 0$, $A^n$
is the set of n-tuples of elements from $A$. An $n$-ary operation (or function)
on $A$ is any function $f$ from $A^n$
to $A$; $n$ is the arity (or rank) of $f$. An operation $f$ on $A$ is called an n-ary operation if its arity  is $n$.\\
\end{definition}
\begin{definition}\textbf{Algebraic Structure} An algebra $ \mathcal{A}$ is a pair $ \langle A, F \rangle$ where $A$ is a non-empty set called universe and $F$ is a set of finitary operations on $A$.\\
\end{definition}

Apart from the operations on $A$, an algebra is further defined by axioms, that in the particular case of universal algebras are often of the form of identities. The collection of algebraic structures defined by equational laws are called varieties. \cite{HYLAND2007437}\\
\begin{definition}\textbf{Variety}
A nonempty class K of algebras of type $\mathcal{F}$ is called a variety if it is closed under subalgebras, homomorphic images, and direct products.
\end{definition}

\begin{definition}
\label{def:lattice}
 A \emph{lattice} $\mathbf{L}$ is an algebraic structure composed by a non-empty set $L$ and two binary operations $\vee$ and $\wedge$ satisfying the following axioms and their duals obtained exchanging $\vee$ and $\wedge$:
\begin{align*}
&x \vee y \approx y \vee x  &&\text{(commutativity)}
\\&x \vee (y \vee z) \approx (x \vee y)  &&\text{(associativity)}
\\&x \vee x \approx x  &&\text{(idempotency)}
\\&x \approx x \vee (x \wedge y)  &&\text{(absorption)}
\end{align*}
\end{definition}

\begin{theorem}[\cite{BurrisSanka}]
\label{th:pos-latt}
A partially ordered set $L$ is a lattice if and only if for every $a, b \in L$ both \emph{supremum} and \emph{infimum} of $\{a,b\}$ exist
(in $L$) with $a \vee b$ being the supremum and $a \wedge b$ the infimum.
\end{theorem}

\begin{definition}
\label{def:semis}
    Let $\mathbf{L}$ be a lattice. Then $\mathbf{L}$ is \emph{modular}(\emph{distributive}, $\vee$-\emph{semi-distributive}, $\wedge$-\emph{semi-distributive}) if it satisfies the following equation: 
     \begin{align*}
         &x \leq y \rightarrow x \vee (y \wedge z) \approx y \wedge (x \vee z) &&\text{(modularity)}
         \\&x \vee (y \wedge z) \approx (x \vee y) \wedge (x \vee z) &&\text{(distributivity)}
         \\&x \vee y \approx x \vee z \rightarrow  x \vee (y \wedge z) \approx x \vee y &&\text{($\vee$-semi-distributivity)}
         \\&x \wedge y \approx x \wedge z \rightarrow  x \wedge (y \vee z) \approx x \wedge y  &&\text{($\wedge$-semi-distributivity)}.
     \end{align*}
     Furthermore a lattice $\mathbf{L}$ is semi-distributive if is both $\vee$-semi-distributive and $\wedge$-semi-distributive
\end{definition}


% Figure environment removed

Congruence lattices of algebraic structures are partially ordered sets such that every pair of elements has unique supremum and infimum determined by the underlying algebra. This object is important relatively to algebraic structures' properties, many of which can be described by omission or admission of certain subpatterns in a graph.\\
\begin{definition}\textbf{Congruence Lattice}\label{cl}\\
For every algebra $\mathcal{A}$ on the set $A$, the identity relation on $A$, and $A \times A$ are trivial congruences. An algebra with no other congruences is called simple. Let $\mathrm{Con}(\mathcal{A})$ be the set of congruences on the algebra $\mathcal{A}$. Because congruences are closed under intersection, we can define a meet operation: $ \wedge : \mathrm{Con}(\mathcal{A}) \times \mathrm{Con}(\mathcal{A}) \to \mathrm{Con}(\mathcal{A})$ by simply taking the intersection of the congruences $E_1 \wedge E_2 = E_1\cap E_2$. Congruences are not closed under union, however we can define the closure operator of any binary relation $E$, with respect to a fixed algebra $\mathcal{A}$, such that it is a congruence, in the following way: $ \langle E \rangle_{\mathcal{A}} = \bigcap \{ F \in \mathrm{Con}(\mathcal{A}) \mid E \subseteq F \}$. Note that the closure of a binary relation is a congruence and thus depends on the operations in $\mathcal{A}$, not just on the carrier set. Now define $ \vee: \mathrm{Con}(\mathcal{A}) \times \mathrm{Con}(\mathcal{A}) \to \mathrm{Con}(\mathcal{A})$ as $E_1 \vee E_2 = \langle E_1\cup E_2 \rangle_{\mathcal{A}} $. For every algebra $\mathcal{A}$, $(\mathrm{Con}(\mathcal{A}), \wedge, \vee)$ with the two operations defined above forms a lattice, called the congruence lattice of $\mathcal{A}$.
\end{definition}


\begin{definition} \textbf{Subalgebra}
    Let $\mathbf{A}$ and $\mathbf{B}$ be two algebras of the same type. Then $\mathbf{B}$ is a \textit{subalgebra} of $\mathbf{A}$ if $B \subseteq A$ and every fundamental operation of $\mathbf{B}$ is the restriction of the corresponding operation of $\mathbf{A}$, i.e., for each function symbol $f$, $f^{\mathbf{B}}$ is $f^{\mathbf{A}}$ restricted to $\mathbf{B}$.
\end{definition}

\begin{definition} \textbf{Homomorphic image}
    Suppose $\mathbf{A}$ and $\mathbf{B}$ are two algebras of the same type $\mathcal{F}$. A mapping $\alpha : A \rightarrow B$ is called a \textit{homomorphism} from $\mathbf{A}$ to $\mathbf{B}$ if 
    $$
    \alpha f^{\mathbf{A}}(a_1, \dotsc ,a_n) = f^{\mathbf{B}}(\alpha a_1, \dotsc, \alpha a_n)
    $$
    for each n-ary $f$ in $\mathcal{F}$ and each sequence $a_1, \dotsc, a_n$ from $\mathbf{A}$. If, in addition, the mapping $\alpha$ is onto then $\mathbf{B}$ is said to be a homomorphic image of $\mathbf{A}$.
\end{definition}

\begin{definition}\textbf{Direct product}
    Let $\mathbf{A}_1$ and $\mathbf{A}_2$ be two algebras of the same type $\mathcal{F}$. We define the direct product $\mathbf{A}_1 \times \mathbf{A}_2$ to be the algebra whose universe is the set $A_1 \times A_2$, and such that for $f \in \mathcal{F}$ and $a_i \in A_1$, $a_i' \in A_2$, $1 \leq i \leq n$,

    $$
    f^{\mathbf{A}_1 \times \mathbf{A}_2}(\langle a_1, a_1' \rangle, \dotsc , \langle a_n, a_n') = \langle f^{\mathbf{A}_1}(a_1, \dotsc, a_n), f^{\mathbf{A}_2}(a_1', \dotsc, a_n') \rangle
    $$
\end{definition}


\section{Baselines' details}
\label{app:baselines}
In practice, we train all models using eight message passing layers and different embedding sizes ranging from $16$ to $64$. We train all models for $200$ epochs with a learning rate of $0.001$. For interpretable models, we set the Gumbel-Softmax temperature to the default value of 1 and the activation behavior to "hard," which generates one-hot encoded embeddings in the forward pass, but computes the gradients using the soft scores. For the hierarchical model, we set the internal loss weight to $0.1$ (to score it roughly $10\%$ less w.r.t. the main loss). Overall, our selection of baselines aims at embracing a wide set of training setups and architectures to assess the effectiveness and versatility of GNNs for analyzing lattice properties in universal algebra. To demonstrate the robustness of our approach, we implemented different types of message passing layers, including graph convolution and GIN. 

\section{Generalization results details}
\label{app:generalization}


% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
\begin{table}[!t]
\centering
\caption{Generalization performance of graph neural models in solving universal algebra's tasks. Values represents the mean and the standard error of the mean of the area under the receiver operating curve (AUCROC, \%).}
\label{tab:auc}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcccccc}
\hline
 & \multicolumn{3}{c}{\textbf{weak generalization}} & \multicolumn{3}{c}{\textbf{strong generalization}} \\
\multicolumn{1}{c}{} & \textbf{GNN} & \textbf{iGNN} & \textbf{HiGNN} & \textbf{GNN} & \textbf{iGNN} & \textbf{HiGNN} \\ \hline
\textbf{Distributive} & $99.80 \pm 0.04$ & $99.56 \pm 0.12$ & $99.45 \pm 0.06$ & $99.51 \pm 0.20$ & $99.44 \pm 0.05$ & $99.42 \pm 0.04$ \\
\textbf{Join Semi Distributive} & $99.49 \pm 0.02$ & $98.31 \pm 0.15$ & $98.28 \pm 0.04$ & $98.77 \pm 0.15$ & $97.50 \pm 0.14$ & $97.48 \pm 0.14$ \\
\textbf{Meet Semi Distributive} & $99.52 \pm 0.04$ & $98.19 \pm 0.06$ & $98.25 \pm 0.08$ & $98.90 \pm 0.03$ & $97.18 \pm 0.14$ & $96.89 \pm 0.37$ \\
\textbf{Modular} & $99.77 \pm 0.02$ & $99.18 \pm 0.11$ & $99.35 \pm 0.09$ & $99.32 \pm 0.22$ & $99.21 \pm 0.14$ & $99.11 \pm 0.22$ \\
\textbf{Semi Distributive} & $99.66 \pm 0.03$ & $98.57 \pm 0.02$ & $98.50 \pm 0.06$ & $99.19 \pm 0.04$ & $97.28 \pm 0.48$ & $96.88 \pm 0.47$ \\
\textbf{Multi Label} & $99.60 \pm 0.02$ & $96.32 \pm 0.34$ & $95.98 \pm 0.50$ & $98.62 \pm 0.43$ & $95.29 \pm 0.55$ & $95.27 \pm 0.32$ \\ \hline
\end{tabular}%
}
\end{table}

\section{Concept completeness and purity}
\label{app:completeness}
Our experimental results demonstrate that interpretable GNNs produce concepts with high completeness and purity, which are standard quantitative metrics used to evaluate the quality of concept-based approaches. Specifically, our approach achieves up to X\% completeness and Y purity, with an average score of Z and W, respectively. The lowest scores are obtained for the multilabel case, which is more challenging as models must learn a wider and more diverse set of concepts. Furthermore, the hierarchical structure of interpretable GNNs enables us to evaluate the quality of intermediate concepts layer by layer. This hierarchy provides insights into why we may need more layers, and it can be used as a valuable tool to find the optimal setup and tune the size of the architecture. Additionally, it can also be used to compare the quality of concepts at different layers of the network. Overall, these results quantitatively assess and validate the high quality of the concepts learned by the interpretable GNNs, highlighting the effectiveness of this approach for learning and analyzing complex algebraic structures.

% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
\begin{table}[!h]
\centering
\caption{Concept purity scores of graph neural models in solving universal algebra's tasks.}
\label{tab:purity}
\resizebox{.8\textwidth}{!}{%
\begin{tabular}{lcccccc}
\hline
 & \multicolumn{3}{c}{\textbf{\textsc{weak purity}}} & \multicolumn{3}{c}{\textbf{\textsc{strong purity}}} \\
\multicolumn{1}{c}{\textbf{}} & \textbf{GNN} & \textbf{iGNN} & \textbf{HiGNN} & \textbf{GNN} & \textbf{iGNN} & \textbf{HiGNN} \\ \hline
\textbf{Distributive} & $3.30 \pm 0.36$ & $3.64 \pm 0.30$ & $3.09 \pm 0.56$ & $3.29 \pm 0.38$ & $4.00 \pm 0.77$ & $4.15 \pm 0.67$ \\
\textbf{Join Semi Distributive} & $2.38 \pm 0.37$ & $3.96 \pm 0.51$ & $3.74 \pm 0.62$ & $3.45 \pm 0.34$ & $3.98 \pm 0.68$ & $4.29 \pm 0.61$ \\
\textbf{Meet Semi Distributive} & $3.24 \pm 0.63$ & $3.55 \pm 0.62$ & $3.39 \pm 0.29$ & $3.36 \pm 0.32$ & $4.25 \pm 0.39$ & $4.97 \pm 0.44$ \\
\textbf{Modular} & $3.10 \pm 0.35$ & $3.50 \pm 0.46$ & $4.44 \pm 0.56$ & $3.14 \pm 0.24$ & $3.19 \pm 1.01$ & $4.25 \pm 0.69$ \\
\textbf{Semi Distributive} & $2.84 \pm 0.51$ & $3.70 \pm 0.54$ & $4.11 \pm 0.46$ & $3.70 \pm 0.55$ & $3.92 \pm 0.28$ & $4.08 \pm 0.85$ \\ \hline
\end{tabular}%
}
\end{table}

% \note{ADD TABLE WITH CONCEPT PURITY}
\begin{table}[!h]
\centering
\caption{Concept completeness scores of graph neural models in solving universal algebra's tasks. Values represents the mean and the standard error of the mean of the area under the receiver operating curve (AUCROC, \%).}
\label{tab:completenes}
\resizebox{.8\textwidth}{!}{%
\begin{tabular}{lcccccc}
\hline
 & \multicolumn{2}{c}{\textbf{\textsc{weak completeness}}} & \multicolumn{2}{c}{\textbf{\textsc{strong completeness}}} \\
\multicolumn{1}{c}{\textbf{}} & \textbf{iGNN} & \textbf{HiGNN} & \textbf{iGNN} & \textbf{HiGNN} \\ \hline
\textbf{Distributive} & $77.30 \pm 0.20$ & $76.60 \pm 2.35$ & $78.19 \pm 1.71$ & $73.16 \pm 3.63$ \\
\textbf{Join Semi Distributive} & $85.20 \pm 0.86$ & $86.84 \pm 0.37$ & $81.08 \pm 0.18$ & $79.76 \pm 0.38$ \\
\textbf{Meet Semi Distributive} & $84.21 \pm 0.68$ & $84.34 \pm 1.08$ & $80.86 \pm 1.32$ & $79.68 \pm 0.22$ \\
\textbf{Modular} & $76.98 \pm 0.28$ & $73.77 \pm 3.14$ & $81.36 \pm 0.70$ & $77.61 \pm 2.37$ \\
\textbf{Semi Distributive} & $87.33 \pm 1.16$ & $85.62 \pm 0.27$ & $84.03 \pm 0.89$ & $82.26 \pm 0.21$ \\ \hline
\end{tabular}%
}
\end{table}


% \section{Local and post-hoc explanations}
% \label{app:local-xai}

% \section{Concepts}
% \label{app:concepts-viz}



% % Figure environment removed




% % Figure environment removed


\end{document}


% \section{Submission of papers to NeurIPS 2022}


% Please read the instructions below carefully and follow them faithfully.


% \subsection{Style}


% Papers to be submitted to NeurIPS 2022 must be prepared according to the
% instructions presented here. Papers may only be up to {\bf nine} pages long,
% including figures. Additional pages \emph{containing only acknowledgments and
% references} are allowed. Papers that exceed the page limit will not be
% reviewed, or in any other way considered for presentation at the conference.


% The margins in 2022 are the same as those in 2007, which allow for $\sim$$15\%$
% more words in the paper compared to earlier years.


% Authors are required to use the NeurIPS \LaTeX{} style files obtainable at the
% NeurIPS website as indicated below. Please make sure you use the current files
% and not previous versions. Tweaking the style files may be grounds for
% rejection.


% \subsection{Retrieval of style files}


% The style files for NeurIPS and other conference information are available on
% the World Wide Web at
% \begin{center}
%   \url{http://www.neurips.cc/}
% \end{center}
% The file \verb+neurips_2022.pdf+ contains these instructions and illustrates the
% various formatting requirements your NeurIPS paper must satisfy.


% The only supported style file for NeurIPS 2022 is \verb+neurips_2022.sty+,
% rewritten for \LaTeXe{}.  \textbf{Previous style files for \LaTeX{} 2.09,
%   Microsoft Word, and RTF are no longer supported!}


% The \LaTeX{} style file contains three optional arguments: \verb+final+, which
% creates a camera-ready copy, \verb+preprint+, which creates a preprint for
% submission to, e.g., arXiv, and \verb+nonatbib+, which will not load the
% \verb+natbib+ package for you in case of package clash.


% \paragraph{Preprint option}
% If you wish to post a preprint of your work online, e.g., on arXiv, using the
% NeurIPS style, please use the \verb+preprint+ option. This will create a
% nonanonymized version of your work with the text ``Preprint. Work in progress.''
% in the footer. This version may be distributed as you see fit. Please \textbf{do
%   not} use the \verb+final+ option, which should \textbf{only} be used for
% papers accepted to NeurIPS.


% At submission time, please omit the \verb+final+ and \verb+preprint+
% options. This will anonymize your submission and add line numbers to aid
% review. Please do \emph{not} refer to these line numbers in your paper as they
% will be removed during generation of camera-ready copies.


% The file \verb+neurips_2022.tex+ may be used as a ``shell'' for writing your
% paper. All you have to do is replace the author, title, abstract, and text of
% the paper with your own.


% The formatting instructions contained in these style files are summarized in
% Sections \ref{gen_inst}, \ref{headings}, and \ref{others} below.


% \section{General formatting instructions}
% \label{gen_inst}


% The text must be confined within a rectangle 5.5~inches (33~picas) wide and
% 9~inches (54~picas) long. The left margin is 1.5~inch (9~picas).  Use 10~point
% type with a vertical spacing (leading) of 11~points.  Times New Roman is the
% preferred typeface throughout, and will be selected for you by default.
% Paragraphs are separated by \nicefrac{1}{2}~line space (5.5 points), with no
% indentation.


% The paper title should be 17~point, initial caps/lower case, bold, centered
% between two horizontal rules. The top rule should be 4~points thick and the
% bottom rule should be 1~point thick. Allow \nicefrac{1}{4}~inch space above and
% below the title to rules. All pages should start at 1~inch (6~picas) from the
% top of the page.


% For the final version, authors' names are set in boldface, and each name is
% centered above the corresponding address. The lead author's name is to be listed
% first (left-most), and the co-authors' names (if different address) are set to
% follow. If there is only one co-author, list both author and co-author side by
% side.


% Please pay special attention to the instructions in Section \ref{others}
% regarding figures, tables, acknowledgments, and references.


% \section{Headings: first level}
% \label{headings}


% All headings should be lower case (except for first word and proper nouns),
% flush left, and bold.


% First-level headings should be in 12-point type.


% \subsection{Headings: second level}


% Second-level headings should be in 10-point type.


% \subsubsection{Headings: third level}


% Third-level headings should be in 10-point type.


% \paragraph{Paragraphs}


% There is also a \verb+\paragraph+ command available, which sets the heading in
% bold, flush left, and inline with the text, with the heading followed by 1\,em
% of space.


% \section{Citations, figures, tables, references}
% \label{others}


% These instructions apply to everyone.


% \subsection{Citations within the text}


% The \verb+natbib+ package will be loaded for you by default.  Citations may be
% author/year or numeric, as long as you maintain internal consistency.  As to the
% format of the references themselves, any style is acceptable as long as it is
% used consistently.


% The documentation for \verb+natbib+ may be found at
% \begin{center}
%   \url{http://mirrors.ctan.org/macros/latex/contrib/natbib/natnotes.pdf}
% \end{center}
% Of note is the command \verb+\citet+, which produces citations appropriate for
% use in inline text.  For example,
% \begin{verbatim}
%    \citet{hasselmo} investigated\dots
% \end{verbatim}
% produces
% \begin{quote}
%   Hasselmo, et al.\ (1995) investigated\dots
% \end{quote}


% If you wish to load the \verb+natbib+ package with options, you may add the
% following before loading the \verb+neurips_2022+ package:
% \begin{verbatim}
%    \PassOptionsToPackage{options}{natbib}
% \end{verbatim}


% If \verb+natbib+ clashes with another package you load, you can add the optional
% argument \verb+nonatbib+ when loading the style file:
% \begin{verbatim}
%    \usepackage[nonatbib]{neurips_2022}
% \end{verbatim}


% As submission is double blind, refer to your own published work in the third
% person. That is, use ``In the previous work of Jones et al.\ [4],'' not ``In our
% previous work [4].'' If you cite your other papers that are not widely available
% (e.g., a journal paper under review), use anonymous author names in the
% citation, e.g., an author of the form ``A.\ Anonymous.''


% \subsection{Footnotes}


% Footnotes should be used sparingly.  If you do require a footnote, indicate
% footnotes with a number\footnote{Sample of the first footnote.} in the
% text. Place the footnotes at the bottom of the page on which they appear.
% Precede the footnote with a horizontal rule of 2~inches (12~picas).


% Note that footnotes are properly typeset \emph{after} punctuation
% marks.\footnote{As in this example.}


% \subsection{Figures}


% % Figure environment removed


% All artwork must be neat, clean, and legible. Lines should be dark enough for
% purposes of reproduction. The figure number and caption always appear after the
% figure. Place one line space before the figure caption and one line space after
% the figure. The figure caption should be lower case (except for first word and
% proper nouns); figures are numbered consecutively.


% You may use color figures.  However, it is best for the figure captions and the
% paper body to be legible if the paper is printed in either black/white or in
% color.


% \subsection{Tables}


% All tables must be centered, neat, clean and legible.  The table number and
% title always appear before the table.  See Table~\ref{sample-table}.


% Place one line space before the table title, one line space after the
% table title, and one line space after the table. The table title must
% be lower case (except for first word and proper nouns); tables are
% numbered consecutively.


% Note that publication-quality tables \emph{do not contain vertical rules.} We
% strongly suggest the use of the \verb+booktabs+ package, which allows for
% typesetting high-quality, professional tables:
% \begin{center}
%   \url{https://www.ctan.org/pkg/booktabs}
% \end{center}
% This package was used to typeset Table~\ref{sample-table}.


% \begin{table}
%   \caption{Sample table title}
%   \label{sample-table}
%   \centering
%   \begin{tabular}{lll}
%     \toprule
%     \multicolumn{2}{c}{Part}                   \\
%     \cmidrule(r){1-2}
%     Name     & Description     & Size ($\mu$m) \\
%     \midrule
%     Dendrite & Input terminal  & $\sim$100     \\
%     Axon     & Output terminal & $\sim$10      \\
%     Soma     & Cell body       & up to $10^6$  \\
%     \bottomrule
%   \end{tabular}
% \end{table}


% \section{Final instructions}


% Do not change any aspects of the formatting parameters in the style files.  In
% particular, do not modify the width or length of the rectangle the text should
% fit into, and do not change font sizes (except perhaps in the
% \textbf{References} section; see below). Please note that pages should be
% numbered.


% \section{Preparing PDF files}


% Please prepare submission files with paper size ``US Letter,'' and not, for
% example, ``A4.''


% Fonts were the main cause of problems in the past years. Your PDF file must only
% contain Type 1 or Embedded TrueType fonts. Here are a few instructions to
% achieve this.


% \begin{itemize}


% \item You should directly generate PDF files using \verb+pdflatex+.


% \item You can check which fonts a PDF files uses.  In Acrobat Reader, select the
%   menu Files$>$Document Properties$>$Fonts and select Show All Fonts. You can
%   also use the program \verb+pdffonts+ which comes with \verb+xpdf+ and is
%   available out-of-the-box on most Linux machines.


% \item The IEEE has recommendations for generating PDF files whose fonts are also
%   acceptable for NeurIPS. Please see
%   \url{http://www.emfield.org/icuwb2010/downloads/IEEE-PDF-SpecV32.pdf}


% \item \verb+xfig+ "patterned" shapes are implemented with bitmap fonts.  Use
%   "solid" shapes instead.


% \item The \verb+\bbold+ package almost always uses bitmap fonts.  You should use
%   the equivalent AMS Fonts:
% \begin{verbatim}
%    \usepackage{amsfonts}
% \end{verbatim}
% followed by, e.g., \verb+\mathbb{R}+, \verb+\mathbb{N}+, or \verb+\mathbb{C}+
% for $\mathbb{R}$, $\mathbb{N}$ or $\mathbb{C}$.  You can also use the following
% workaround for reals, natural and complex:
% \begin{verbatim}
%    \newcommand{\RR}{I\!\!R} %real numbers
%    \newcommand{\Nat}{I\!\!N} %natural numbers
%    \newcommand{\CC}{I\!\!\!\!C} %complex numbers
% \end{verbatim}
% Note that \verb+amsfonts+ is automatically loaded by the \verb+amssymb+ package.


% \end{itemize}


% If your file contains type 3 fonts or non embedded TrueType fonts, we will ask
% you to fix it.


% \subsection{Margins in \LaTeX{}}


% Most of the margin problems come from figures positioned by hand using
% \verb+\special+ or other commands. We suggest using the command
% \verb+\includegraphics+ from the \verb+graphicx+ package. Always specify the
% figure width as a multiple of the line width as in the example below:
% \begin{verbatim}
%    \usepackage[pdftex]{graphicx} ...
%    % Figure removed
% \end{verbatim}
% See Section 4.4 in the graphics bundle documentation
% (\url{http://mirrors.ctan.org/macros/latex/required/graphics/grfguide.pdf})


% A number of width problems arise when \LaTeX{} cannot properly hyphenate a
% line. Please give LaTeX hyphenation hints using the \verb+\-+ command when
% necessary.


% \begin{ack}
% Use unnumbered first level headings for the acknowledgments. All acknowledgments
% go at the end of the paper before the list of references. Moreover, you are required to declare
% funding (financial activities supporting the submitted work) and competing interests (related financial activities outside the submitted work).
% More information about this disclosure can be found at: \url{https://neurips.cc/Conferences/2022/PaperInformation/FundingDisclosure}.


% Do {\bf not} include this section in the anonymized submission, only in the final paper. You can use the \texttt{ack} environment provided in the style file to autmoatically hide this section in the anonymized submission.
% \end{ack}