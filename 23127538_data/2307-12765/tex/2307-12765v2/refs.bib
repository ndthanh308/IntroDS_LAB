@inproceedings{social_network,
  title={What is Twitter, a social network or a news media?},
  author={Kwak, Haewoon and Lee, Changhyun and Park, Hosung and Moon, Sue},
  booktitle={Proceedings of the 19th international conference on World wide web},
  pages={591--600},
  year={2010}
}

@article{brain_science,
  title={Complex brain networks: graph theoretical analysis of structural and functional systems},
  author={Bullmore, Ed and Sporns, Olaf},
  journal={Nature reviews neuroscience},
  volume={10},
  number={3},
  pages={186--198},
  year={2009},
  publisher={Nature Publishing Group}
}

@article{HGNN1,
  title={Pathsim: Meta path-based top-k similarity search in heterogeneous information networks},
  author={Sun, Yizhou and Han, Jiawei and Yan, Xifeng and Yu, Philip S and Wu, Tianyi},
  journal={Proceedings of the VLDB Endowment},
  volume={4},
  number={11},
  pages={992--1003},
  year={2011},
  publisher={VLDB Endowment}
}

@article{HGNN2,
  title={Pathselclus: Integrating meta-path selection with user-guided object clustering in heterogeneous information networks},
  author={Sun, Yizhou and Norick, Brandon and Han, Jiawei and Yan, Xifeng and Yu, Philip S and Yu, Xiao},
  journal={ACM Transactions on Knowledge Discovery from Data (TKDD)},
  volume={7},
  number={3},
  pages={1--23},
  year={2013},
  publisher={ACM New York, NY, USA}
}

@article{understand_GCN,
  title={Characterizing and understanding GCNs on GPU},
  author={Yan, Mingyu and Chen, Zhaodong and Deng, Lei and others},
  journal={IEEE Computer Architecture Letters},
  volume={19},
  number={1},
  pages={22--25},
  year={2020},
  publisher={IEEE}
}

@inproceedings{HyGCN,
  title={Hygcn: A gcn accelerator with hybrid architecture},
  author={Yan, Mingyu and Deng, Lei and Hu, Xing and others},
  booktitle={2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)},
  pages={15--29},
  year={2020},
  organization={IEEE}
}

@article{previous3,
  title={Architectural implications of graph neural networks},
  author={Zhang, Zhihui and Leng, Jingwen and Ma, Lingxiao and Miao, Youshan and Li, Chao and Guo, Minyi},
  journal={IEEE Computer architecture letters},
  volume={19},
  number={1},
  pages={59--62},
  year={2020},
  publisher={IEEE}
}


@inproceedings{SeHGNN,
  title={Simple and Efficient Heterogeneous Graph Neural Network},
  author={Yang, Xiaocheng and Yan, Mingyu and Pan, Shirui and Ye, Xiaochun and Fan, Dongrui},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  year={2023}
}

@inproceedings{MAGNN,
  title={Magnn: Metapath aggregated graph neural network for heterogeneous graph embedding},
  author={Fu, Xinyu and Zhang, Jiani and Meng, Ziqiao and King, Irwin},
  booktitle={Proceedings of The Web Conference 2020},
  pages={2331--2341},
  year={2020}
}

@inproceedings{HAN,
  title={Heterogeneous graph attention network},
  author={Wang, Xiao and Ji, Houye and Shi, Chuan and others},
  booktitle={The world wide web conference},
  pages={2022--2032},
  year={2019}
}

@inproceedings{HetSANN,
  title={An attention-based graph neural network for heterogeneous structural learning},
  author={Hong, Huiting and Guo, Hantao and Lin, Yucheng and Yang, Xiaoqing and Li, Zang and Ye, Jieping},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  number={04},
  pages={4132--4139},
  year={2020}
}

@inproceedings{Simple-HGN,
  title={Are we really making much progress? Revisiting, benchmarking and refining heterogeneous graph neural networks},
  author={Lv, Qingsong and Ding, Ming and Liu, Qiang and others},
  booktitle={Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \& Data Mining},
  pages={1150--1160},
  year={2021}
}

@inproceedings{R-GCN,
  title={Modeling relational data with graph convolutional networks},
  author={Schlichtkrull, Michael and Kipf, Thomas N and Bloem, Peter and Berg, Rianne van den and Titov, Ivan and Welling, Max},
  booktitle={European semantic web conference},
  pages={593--607},
  year={2018},
  organization={Springer}
}

@inproceedings{RSHN,
  title={Relation structure-aware heterogeneous graph neural network},
  author={Zhu, Shichao and Zhou, Chuan and Pan, Shirui and Zhu, Xingquan and Wang, Bin},
  booktitle={2019 IEEE international conference on data mining (ICDM)},
  pages={1534--1539},
  year={2019},
  organization={IEEE}
}

@article{understand_HGNN,
  title={Characterizing and Understanding HGNNs on GPUs},
  author={Yan, Mingyu and Zou, Mo and Yang, Xiaocheng and others},
  journal={IEEE Computer Architecture Letters},
  volume={21},
  number={2},
  pages={69--72},
  year={2022},
  publisher={IEEE}
}

@article{proteomic,
  title={The principled design of large-scale recursive neural network architectures--dag-rnns and the protein structure prediction problem},
  author={Baldi, Pierre and Pollastri, Gianluca},
  journal={The Journal of Machine Learning Research},
  volume={4},
  pages={575--602},
  year={2003},
  publisher={JMLR. org}
}

@article{scene_description1,
  title={Graph-based generation of referring expressions},
  author={Krahmer, Emiel and Erk, Sebastiaan van and Verleg, Andr{\'e}},
  journal={Computational Linguistics},
  volume={29},
  number={1},
  pages={53--72},
  year={2003},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{scene_description2,
  title={A graphical representation of the state spaces of hierarchical level-of-detail scene descriptions},
  author={Mason, Ashton E. W. and Blake, Edwin H.},
  journal={IEEE Transactions on Visualization and Computer Graphics},
  volume={7},
  number={1},
  pages={70--75},
  year={2001},
  publisher={IEEE}
}

@inproceedings{NLP,
  title={Recursive neural networks applied to discourse representation theory},
  author={Bua, Antonella and Gori, Marco and Santini, Fabrizio},
  booktitle={International Conference on Artificial Neural Networks},
  pages={290--295},
  year={2002},
  organization={Springer}
}

@article{cpu_drawback,
  title={Eyeriss: A spatial architecture for energy-efficient dataflow for convolutional neural networks},
  author={Chen, Yu-Hsin and Emer, Joel and Sze, Vivienne},
  journal={ACM SIGARCH Computer Architecture News},
  volume={44},
  number={3},
  pages={367--379},
  year={2016},
  publisher={ACM New York, NY, USA}
}

@article{gpu_drawback,
  title={NVIDIA Tesla: A unified graphics and computing architecture},
  author={Lindholm, Erik and Nickolls, John and Oberman, Stuart and Montrym, John},
  journal={IEEE micro},
  volume={28},
  number={2},
  pages={39--55},
  year={2008},
  publisher={IEEE}
}

@misc{cublas,
author = “NVIDIA”,
title = “cublas”,
howpublished = “Website”,
year = {},
note = {\url{https://developer.nvidia.com/cublas}}
}

@misc{knowledge_network,
author = “Google”,
title = “Knowledge”,
howpublished = “Website”,
year = {},
note = {\url{https://www.google. com/intl/en_us/insidesearch/features/search/knowledge.html}}
}

@inproceedings{7pj,
  title={Highlights of the high-bandwidth memory (hbm) standard},
  author={O’Connor, Mike},
  booktitle={Memory forum workshop},
  volume={3},
  year={2014}
}

@article{ramulator,
  title={Ramulator: A fast and extensible DRAM simulator},
  author={Kim, Yoongu and Yang, Weikun and Mutlu, Onur},
  journal={IEEE Computer architecture letters},
  volume={15},
  number={1},
  pages={45--49},
  year={2015},
  publisher={IEEE}
}

@article{gnn,
  title={The graph neural network model},
  author={Scarselli, Franco and Gori, Marco and Tsoi, Ah Chung and Hagenbuchner, Markus and Monfardini, Gabriele},
  journal={IEEE transactions on neural networks},
  volume={20},
  number={1},
  pages={61--80},
  year={2008},
  publisher={IEEE}
}

@article{gnn1,
  title={How powerful are graph neural networks?},
  author={Xu, Keyulu and Hu, Weihua and Leskovec, Jure and Jegelka, Stefanie},
  journal={arXiv preprint arXiv:1810.00826},
  year={2018}
}

@article{gnn2,
  title={Graph neural networks: A review of methods and applications},
  author={Zhou, Jie and Cui, Ganqu and Hu, Shengding and others},
  journal={AI Open},
  volume={1},
  pages={57--81},
  year={2020},
  publisher={Elsevier}
}

@article{gnn3,
  title={Graph neural networks in recommender systems: a survey},
  author={Wu, Shiwen and Sun, Fei and Zhang, Wentao and Xie, Xu and Cui, Bin},
  journal={ACM Computing Surveys (CSUR)},
  year={2020},
  publisher={ACM New York, NY}
}

@inproceedings{HGNN3,
  title={Heterogeneous graph neural network},
  author={Zhang, Chuxu and Song, Dongjin and Huang, Chao and Swami, Ananthram and Chawla, Nitesh V},
  booktitle={Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery \& data mining},
  pages={793--803},
  year={2019}
}

@article{GraphMat,
 author = {Sundaram, Narayanan and Satish, Nadathur and Patwary, Md Mostofa Ali and others},
 title = {GraphMat: High Performance Graph Analytics Made Productive},
 journal = {Proc. VLDB Endow.},
 issue_date = {July 2015},
 volume = {8},
 number = {11},
 month = jul,
 year = {2015},
 issn = {2150-8097},
 pages = {1214--1225},
 numpages = {12},
 doi = {10.14778/2809974.2809983},
 acmid = {2809983},
 publisher = {VLDB Endowment},
} 

@inproceedings{Gunrock,
 author = {Wang, Yangzihao and Davidson, Andrew and Pan, Yuechao and Wu, Yuduo and Riffel, Andy and Owens, John D.},
 title = {Gunrock: A High-performance Graph Processing Library on the GPU},
 booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 series = {PPoPP '16},
 year = {2016},
 isbn = {978-1-4503-4092-2},
 location = {Barcelona, Spain},
 pages = {11:1--11:12},
 articleno = {11},
 numpages = {12},
 doi = {10.1145/2851141.2851145},
 acmid = {2851145},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings {TensorFlow,
author = {Mart{\'\i}n Abadi and Paul Barham and Jianmin Chen and others},
title = {TensorFlow: A System for Large-Scale Machine Learning},
booktitle = {12th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 16)},
year = {2016},
isbn = {978-1-931971-33-1},
address = {Savannah, GA},
pages = {265--283},
publisher = {{USENIX} Association},
month = nov,
}

@incollection{PyTorch,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and others},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},
}

@INPROCEEDINGS{awbgcn,
  author={Geng, Tong and Li, Ang and Shi, Runbin and others},
  booktitle={2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)}, 
  title={AWB-GCN: A Graph Convolutional Network Accelerator with Runtime Workload Rebalancing}, 
  year={2020},
  volume={},
  number={},
  pages={922-936},
  doi={10.1109/MICRO50266.2020.00079}
}


@inproceedings{igcn,
author = {Geng, Tong and Wu, Chunshu and Zhang, Yongan and others},
title = {I-GCN: A Graph Convolutional Network Accelerator with Runtime Locality Enhancement through Islandization},
year = {2021},
isbn = {9781450385572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3466752.3480113},
doi = {10.1145/3466752.3480113},
abstract = {Graph Convolutional Networks (GCNs) have drawn tremendous attention in the past three years. Compared with other deep learning modalities, high-performance hardware acceleration of GCNs is as critical but even more challenging. The hurdles arise from the poor data locality and redundant computation due to the large size, high sparsity, and irregular non-zero distribution of real-world graphs. In this paper we propose a novel hardware accelerator for GCN inference, called I-GCN, that significantly improves data locality and reduces unnecessary computation. The mechanism is a new online graph restructuring algorithm we refer to as islandization. The proposed algorithm finds clusters of nodes with strong internal but weak external connections. The islandization process yields two major benefits. First, by processing islands rather than individual nodes, there is better on-chip data reuse and fewer off-chip memory accesses. Second, there is less redundant computation as aggregation for common/shared neighbors in an island can be reused. The parallel search, identification, and leverage of graph islands are all handled purely in hardware at runtime working in an incremental pipeline. This is done without any preprocessing of the graph data or adjustment of the GCN model structure. Experimental results show that I-GCN can significantly reduce off-chip accesses and prune 38\% of aggregation operations, leading to performance speedups over CPUs, GPUs, the prior art GCN accelerators of 5549 \texttimes{}, 403 \texttimes{}, and 5.7 \texttimes{} on average, respectively.},
booktitle = {MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1051–1063},
numpages = {13},
keywords = {Graph Neural Network, High-Performance Computing, Data Locality, Hardware Accelerator, Machine Learning},
location = {Virtual Event, Greece},
series = {MICRO '21}
}

@inproceedings{TPU,
	location = {New York, {NY}, {USA}},
	title = {In-Datacenter Performance Analysis of a Tensor Processing Unit},
	isbn = {978-1-4503-4892-8},
	doi = {10.1145/3079856.3080246},
	series = {{ISCA} '17},
	shorttitle = {{TPU}},
	abstract = {Many architects believe that major improvements in cost-energy-performance must now come from domain-specific hardware. This paper evaluates a custom {ASIC}---called a Tensor Processing Unit ({TPU}) --- deployed in datacenters since 2015 that accelerates the inference phase of neural networks ({NN}). The heart of the {TPU} is a 65,536 8-bit {MAC} matrix multiply unit that offers a peak throughput of 92 {TeraOps}/second ({TOPS}) and a large (28 {MiB}) software-managed on-chip memory. The {TPU}'s deterministic execution model is a better match to the 99th-percentile response-time requirement of our {NN} applications than are the time-varying optimizations of {CPUs} and {GPUs} that help average throughput more than guaranteed latency. The lack of such features helps explain why, despite having myriad {MACs} and a big memory, the {TPU} is relatively small and low power. We compare the {TPU} to a server-class Intel Haswell {CPU} and an Nvidia K80 {GPU}, which are contemporaries deployed in the same datacenters. Our workload, written in the high-level {TensorFlow} framework, uses production {NN} applications ({MLPs}, {CNNs}, and {LSTMs}) that represent 95\% of our datacenters' {NN} inference demand. Despite low utilization for some applications, the {TPU} is on average about 15X -- 30X faster than its contemporary {GPU} or {CPU}, with {TOPS}/Watt about 30X -- 80X higher. Moreover, using the {CPU}'s {GDDR}5 memory in the {TPU} would triple achieved {TOPS} and raise {TOPS}/Watt to nearly 70X the {GPU} and 200X the {CPU}.},
	pages = {1--12},
	booktitle = {Proceedings of the 44th Annual International Symposium on Computer Architecture},
	publisher = {{ACM}},
	author = {Jouppi, Norman P. and Young, Cliff and Patil, Nishant and others},
	urldate = {2018-02-07},
	date = {2017},
	keywords = {accelerator, {CNN}, deep learning, {DNN}, domain-specific architecture, {GPU}, {LSTM}, {MLP}, neural network, {RNN}, {TensorFlow}, {TPU}},
	file = {ACM Full Text PDF:/Users/ymy/MyZone/Zotero/storage/PXGIZUPX/Jouppi et al. - 2017 - In-Datacenter Performance Analysis of a Tensor Pro.pdf:application/pdf}
}



@inproceedings{Graphicionado,
  title = {Graphicionado: {{A}} High-Performance and Energy-Efficient Accelerator for Graph Analytics},
  shorttitle = {Graphicionado},
  doi = {10.1109/MICRO.2016.7783759},
  abstract = {Graphs are one of the key data structures for many real-world computing applications and the importance of graph analytics is ever-growing. While existing software graph processing frameworks improve programmability of graph analytics, underlying general purpose processors still limit the performance and energy efficiency of graph analytics. We architect a domain-specific accelerator, Graphicionado, for high-performance, energy-efficient processing of graph analytics workloads. For efficient graph analytics processing, Graphicionado exploits not only data structure-centric datapath specialization, but also memory subsystem specialization, all the while taking advantage of the parallelism inherent in this domain. Graphicionado augments the vertex programming paradigm, allowing different graph analytics applications to be mapped to the same accelerator framework, while maintaining flexibility through a small set of reconfigurable blocks. This paper describes Graphicionado pipeline design choices in detail and gives insights on how Graphicionado combats application execution inefficiencies on general-purpose CPUs. Our results show that Graphicionado achieves a 1.76-6.54x speedup while consuming 50-100x less energy compared to a state-of-the-art software graph analytics processing framework executing 32 threads on a 16-core Haswell Xeon processor.},
  booktitle = {2016 49th {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}} ({{MICRO}})},
  author = {Ham, T. J. and Wu, L. and Sundaram, N. and Satish, N. and Martonosi, M.},
  month = oct,
  year = {2016},
  keywords = {16-core Haswell Xeon processor,Best,data structure-centric datapath specialization,data structures,domain-specific accelerator,energy conservation,general purpose processors,general-purpose CPU,graph graph processing,graph theory,Graphicionado,high-performance energy-efficient accelerator,Machine learning algorithms,Memory management,memory subsystem specialization,parallel processing,Pipelines,power aware computing,Programming,Readed,Software,Software algorithms,software graph analytics,System-on-chip,vertex programming},
  pages = {1--13},
  file = {/Users/ymy/MyZone/Zotero/storage/G3GFAXG2/graphicionado_slide.pdf;/Users/ymy/MyZone/Zotero/storage/PI78W67R/Ham et al. - 2016 - Graphicionado A high-performance and energy-effic.pdf;/Users/ymy/MyZone/Zotero/storage/6NPDM4QV/7783759.html},

}


% GCN
@inproceedings{GCN,
title={Semi-Supervised Classification with Graph Convolutional Networks},
author={Thomas N. Kipf and Max Welling},
booktitle={International Conference on Learning Representations, {ICLR} 2017},
year={2017},
url={https://openreview.net/forum?id=SJU4ayYgl}
}

% GAT
@article{GAT,
  title="{Graph Attention Networks}",
  author={Veli{\v{c}}kovi{\'{c}}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Li{\`{o}}, Pietro and Bengio, Yoshua},
  journal={International Conference on Learning Representations, {ICLR} 2018},
  year={2018},
  url={https://openreview.net/forum?id=rJXMpikCZ},
}

% HG survey
@article{HG_survey,
  title={A survey of heterogeneous information network analysis},
  author={Shi, Chuan and Li, Yitong and Zhang, Jiawei and Sun, Yizhou and Philip, S Yu},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  volume={29},
  number={1},
  pages={17--37},
  year={2016},
  publisher={IEEE}
}

%% metapaths
@article{sun2011pathsim,
  title={Pathsim: Meta path-based top-k similarity search in heterogeneous information networks},
  author={Sun, Yizhou and Han, Jiawei and Yan, Xifeng and Yu, Philip S and Wu, Tianyi},
  journal={Proceedings of the VLDB Endowment},
  volume={4},
  number={11},
  pages={992--1003},
  year={2011},
  publisher={VLDB Endowment}
}
@article{sun2012mining,
  title={Mining heterogeneous information networks: principles and methodologies},
  author={Sun, Yizhou and Han, Jiawei},
  journal={Synthesis Lectures on Data Mining and Knowledge Discovery},
  volume={3},
  number={2},
  pages={1--159},
  year={2012},
  publisher={Morgan \& Claypool Publishers}
}


@article{comprehensive_gnn_survey,
  title={A comprehensive survey on graph neural networks},
  author={Wu, Zonghan and Pan, Shirui and Chen, Fengwen and Long, Guodong and Zhang, Chengqi and Philip, S Yu},
  journal={IEEE transactions on neural networks and learning systems},
  volume={32},
  number={1},
  pages={4--24},
  year={2020},
  publisher={IEEE}
  }


@inproceedings{hgnn_survey_tangjie,
  title={Heterogeneous Network Representation Learning.},
  author={Dong, Yuxiao and Hu, Ziniu and Wang, Kuansan and Sun, Yizhou and Tang, Jie},
  booktitle={IJCAI},
  volume={20},
  pages={4861--4867},
  year={2020}
}

@article{hgnn_survey_hanjiawei,
  title={Heterogeneous network representation learning: A unified framework with survey and benchmark},
  author={Yang, Carl and Xiao, Yuxin and Zhang, Yu and Sun, Yizhou and Han, Jiawei},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  year={2020},
  publisher={IEEE}
}

@article{hgnn_survey_shiruipan,
  title={Graph Neural Networks for Graphs with Heterophily: A Survey},
  author={Zheng, Xin and Liu, Yixin and Pan, Shirui and Zhang, Miao and Jin, Di and Yu, Philip S},
  journal={arXiv preprint arXiv:2202.07082},
  year={2022}
}

@article{hgnn_survey_shichuan,
  title={A survey on heterogeneous graph embedding: methods, techniques, applications and sources},
  author={Wang, Xiao and Bo, Deyu and Shi, Chuan and Fan, Shaohua and Ye, Yanfang and Yu, Philip S},
  journal={arXiv preprint arXiv:2011.14867},
  year={2020}
}


%[CIKM 18] 支付宝恶意账号检测
@inproceedings{liu2018heterogeneous,
  title={Heterogeneous graph neural networks for malicious account detection},
  author={Liu, Ziqi and Chen, Chaochao and Yang, Xinxing and Zhou, Jun and Li, Xiaolong and Song, Le},
  booktitle={Proceedings of the 27th ACM international conference on information and knowledge management},
  pages={2077--2085},
  year={2018}
}

%[SIGIR 20] 信息检索
@inproceedings{mao2020item,
  title={Item tagging for information retrieval: a tripartite graph neural network based approach},
  author={Mao, Kelong and Xiao, Xi and Zhu, Jieming and Lu, Biao and Tang, Ruiming and He, Xiuqiang},
  booktitle={Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={2327--2336},
  year={2020}
}

%[KDD 20] 淘宝 电商搜索
@inproceedings{niu2020dual,
  title={A dual heterogeneous graph attention network to improve long-tail performance for shop search in e-commerce},
  author={Niu, Xichuan and Li, Bofang and Li, Chenliang and others},
  booktitle={Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={3405--3415},
  year={2020}
}

%Didi， real-time event prediction
@inproceedings{didi_real-time_event_prediction,
  title={Dynamic heterogeneous graph neural network for real-time event prediction},
  author={Luo, Wenjuan and Zhang, Han and Yang, Xiaodi and others},
  booktitle={Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery \& data mining},
  pages={3213--3223},
  year={2020}
}

%Amazon  product-recommendation
@Inproceedings{Zheng2021,
 author = {Liyuan Zheng and Zhen ZUO and Wenbo Wang and Chaosheng Dong and Michinari Momma and Yi Sun},
 title = {Heterogeneous graph neural networks with neighbor-SIM attention mechanism for substitute product recommendation},
 year = {2021},
 url = {https://www.amazon.science/publications/heterogeneous-graph-neural-networks-with-neighbor-sim-attention-mechanism-for-substitute-product-recommendation},
 booktitle = {DLG-AAAI 2021},
}

%Alibaba Bid Keyword Matching
@inproceedings{Alibaba_Bid_Keyword_Matching,
  title={Heterogeneous Graph Neural Networks for Large-Scale Bid Keyword Matching},
  author={Liu, Zongtao and Ma, Bin and Liu, Quan and Xu, Jian and Zheng, Bo},
  booktitle={Proceedings of the 30th ACM International Conference on Information \& Knowledge Management},
  pages={3976--3985},
  year={2021}
}

%[IKDE 22] 推荐系统
@article{li2022disentangled,
  title={Disentangled graph neural networks for session-based recommendation},
  author={Li, Ansong and Cheng, Zhiyong and Liu, Fan and Gao, Zan and Guan, Weili and Peng, Yuxin},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  year={2022},
  publisher={IEEE}
}

%[KDD 19] 推荐系统
@inproceedings{fan2019metapath,
  title={Metapath-guided heterogeneous graph neural network for intent recommendation},
  author={Fan, Shaohua and Zhu, Junxiong and Han, Xiaotian and others},
  booktitle={Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery \& data mining},
  pages={2478--2486},
  year={2019}
}

%[CIKM 21 short paper] 医疗数据分析
@inproceedings{luo2021imas,
  title={IMAS++ An Intelligent Medical Analysis System Enhanced with Deep Graph Neural Networks},
  author={Luo, Feng and Zhang, Yue and Wang, Xiaoli},
  booktitle={Proceedings of the 30th ACM International Conference on Information \& Knowledge Management},
  pages={4754--4758},
  year={2021}
}


%知识图谱推理
@inproceedings{CompGCN,
    title={Composition-based Multi-Relational Graph Convolutional Networks},
    author={Shikhar Vashishth and Soumya Sanyal and Vikram Nitin and Partha Talukdar},
    booktitle={International Conference on Learning Representations},
    year={2020},
    url={https://openreview.net/forum?id=BylA_C4tPr}
}


%知识图谱推理
@inproceedings{A2N,
  title={A2N: Attending to neighbors for knowledge graph inference},
  author={Bansal, Trapit and Juan, Da-Cheng and Ravi, Sujith and McCallum, Andrew},
  booktitle={Proceedings of the 57th annual meeting of the association for computational linguistics},
  pages={4387--4392},
  year={2019}
}

%知识图谱推理
@inproceedings{M2GNN,
  title={Mixed-curvature multi-relational graph neural network for knowledge graph completion},
  author={Wang, Shen and Wei, Xiaokai and Nogueira dos Santos, Cicero Nogueira and others},
  booktitle={Proceedings of the Web Conference 2021},
  pages={1761--1771},
  year={2021}
}


@inproceedings{R-GAT,
  title={Relational Graph Attention Network for Aspect-based Sentiment Analysis},
  author={Wang, Kai and Shen, Weizhou and Yang, Yunyi and Quan, Xiaojun and Wang, Rui},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={3229--3238},
  year={2020}
}

@inproceedings{PyG,
  title={Fast Graph Representation Learning with {PyTorch Geometric}},
  author={Fey, Matthias and Lenssen, Jan E.},
  booktitle={ICLR Workshop on Representation Learning on Graphs and Manifolds},
  year={2019},
}

@inproceedings{DGL,
  title={Deep graph library: Towards efficient and scalable deep learning on graphs},
  author={Wang, Minjie Yu},
  booktitle={ICLR workshop on representation learning on graphs and manifolds},
  year={2019}
}

@inproceedings{Tesseract,
  title={A scalable processing-in-memory accelerator for parallel graph processing},
  author={Ahn, Junwhan and Hong, Sungpack and Yoo, Sungjoo and Mutlu, Onur and Choi, Kiyoung},
  booktitle={Proceedings of the 42nd Annual International Symposium on Computer Architecture},
  pages={105--117},
  year={2015}
}

@inproceedings{GraphR,
  title={GraphR: Accelerating graph processing using ReRAM},
  author={Song, Linghao and Zhuo, Youwei and Qian, Xuehai and Li, Hai and Chen, Yiran},
  booktitle={2018 IEEE International Symposium on High Performance Computer Architecture (HPCA)},
  pages={531--543},
  year={2018},
  organization={IEEE}
}

@inproceedings{GraphDynS,
author = {Yan, Mingyu and Hu, Xing and Li, Shuangchen and others},
title = {Alleviating Irregularity in Graph Analytics Acceleration: A Hardware/Software Co-Design Approach},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358318},
doi = {10.1145/3352460.3358318},
abstract = {Graph analytics is an emerging application which extracts insights by processing large volumes of highly connected data, namely graphs. The parallel processing of graphs has been exploited at the algorithm level, which in turn incurs three irregularities onto computing and memory patterns that significantly hinder an efficient architecture design. Certain irregularities can be partially tackled by the prior domain-specific accelerator designs with well-designed scheduling of data access, while others remain unsolved.Unlike prior efforts, we fully alleviate these irregularities at their origin---the data-dependent program behavior. To achieve this goal, we propose GraphDynS, a hardware/software co-design with decoupled datapath and data-aware dynamic scheduling. Aware of data dependencies extracted from the decoupled datapath, GraphDynS can elaborately schedule the program on-the-fly to maximize parallelism. To extract data dependencies at runtime, we propose a new programming model in synergy with a microarchitecture design that supports datapath decoupling. Through data dependency information, we present several data-aware strategies to dynamically schedule workloads, data accesses, and computations. Overall, GraphDynS achieves 4.4\texttimes{} speedup and 11.6\texttimes{} less energy on average with half the memory bandwidth compared to a state-of-the-art GPGPU-based solution. Compared to a state-of-the-art graph analytics accelerator, GraphDynS also achieves 1.9\texttimes{} speedup and 1.8\texttimes{} less energy on average using the same memory bandwidth.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {615–628},
numpages = {14},
keywords = {graph analytics, accelerator, software and hardware co-design},
location = {Columbus, OH, USA},
series = {MICRO '52}
}

@INPROCEEDINGS{HGL,
  author={Gui, Yuntao and Wu, Yidi and Yang, Han and others},
  booktitle={SC22: International Conference for High Performance Computing, Networking, Storage and Analysis}, 
  title={HGL: Accelerating Heterogeneous GNN Training with Holistic Representation and Optimization}, 
  year={2022},
  volume={},
  number={},
  pages={1-15},
  doi={10.1109/SC41404.2022.00077}}


@inproceedings{Spam_review_detection,
  title={Spam review detection with graph convolutional networks},
  author={Li, Ao and Qin, Zhou and Liu, Runshi and Yang, Yiqun and Li, Dong},
  booktitle={Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
  pages={2703--2711},
  year={2019}
}



@INPROCEEDINGS{technology_scale, 
author={O. {Villa} and D. R. {Johnson} and M. {Oconnor} and others}, 
booktitle={SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis}, 
title={Scaling the Power Wall: A Path to Exascale}, 
year={2014}, 
volume={}, 
number={}, 
pages={830-841}, 
keywords={multiprocessing systems;parallel machines;performance evaluation;power aware computing;exascale system;HPC application;supercomputer development;ExaFlops;energy efficiency improvement;performance projection;Graphics processing units;Computer architecture;Bandwidth;Registers;Instruction sets;Supercomputers;Kernel}, 
doi={10.1109/SC.2014.73}, 
ISSN={2167-4337}, 
month={Nov},}

@online{CACTI,
	title = {CACTI},
	url = {http://www.hpl.hp.com/research/cacti/},
	urldate = {2018-09-11},
	file = {HP Labs \: CACTI:/Users/ymy/MyZone/Zotero/storage/C7WL9Y8H/cacti.html:text/html}
}


@inproceedings{ozdal_energy_2016,
  title = {Energy Efficient Architecture for Graph Analytics Accelerators},
  doi = {10.1109/ISCA.2016.24},
  eventtitle = {2016 {ACM}/{IEEE} 43rd Annual International Symposium on Computer Architecture ({ISCA})},
  pages = {166--177},
  booktitle = {2016 {ACM}/{IEEE} 43rd Annual International Symposium on Computer Architecture ({ISCA})},
  author = {Ozdal, M. M. and Yesil, S. and Kim, T. and others},
  date = {2016-06}
}

@article{EnGN,
author = {Liang, Shengwen and Wang, Ying and Liu, Cheng and others},
title = {EnGN: A High-Throughput and Energy-Efficient Accelerator for Large Graph Neural Networks},
year = {2021},
issue_date = {Sept. 2021},
publisher = {IEEE Computer Society},
address = {USA},
volume = {70},
number = {9},
issn = {0018-9340},
doi = {10.1109/TC.2020.3014632},
journal = {IEEE Trans. Comput.},
month = {sep},
pages = {1511–1525},
numpages = {15}
}

@INPROCEEDINGS{GCoD,
  author={You, Haoran and Geng, Tong and Zhang, Yongan and Li, Ang and Lin, Yingyan},
  booktitle={2022 IEEE International Symposium on High-Performance Computer Architecture (HPCA)}, 
  title={GCoD: Graph Convolutional Network Acceleration via Dedicated Algorithm and Accelerator Co-Design}, 
  year={2022},
  volume={},
  number={},
  pages={460-474},
  doi={10.1109/HPCA53966.2022.00041}}


@INPROCEEDINGS{ReGNN,
  author={Chen, Cen and Li, Kenli and Li, Yangfan and Zou, Xiaofeng},
  booktitle={2022 IEEE International Symposium on High-Performance Computer Architecture (HPCA)}, 
  title={ReGNN: A Redundancy-Eliminated Graph Neural Networks Accelerator}, 
  year={2022},
  volume={},
  number={},
  pages={429-443},
  doi={10.1109/HPCA53966.2022.00039}}

@inproceedings{ReGNN-HUAKE,
  author       = {Cong Liu and
                  Haikun Liu and
                  Hai Jin and others},
  editor       = {Rob Oshana},
  title        = {ReGNN: a ReRAM-based heterogeneous architecture for general graph
                  neural networks},
  booktitle    = {{DAC} '22: 59th {ACM/IEEE} Design Automation Conference, San Francisco,
                  California, USA, July 10 - 14, 2022},
  pages        = {469--474},
  publisher    = {{ACM}},
  year         = {2022},
  url          = {https://doi.org/10.1145/3489517.3530479},
  doi          = {10.1145/3489517.3530479},
  timestamp    = {Tue, 21 Mar 2023 07:57:35 +0100},
  biburl       = {https://dblp.org/rec/conf/dac/LiuL0LZDXL22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS {FlowGNN,
author = {R. Sarkar and S. Abi-Karam and Y. He and L. Sathidevi and C. Hao},
booktitle = {2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA)},
title = {FlowGNN: A Dataflow Architecture for Real-Time Workload-Agnostic Graph Neural Network Inference},
year = {2023},
volume = {},
issn = {},
pages = {1099-1112},
doi = {10.1109/HPCA56546.2023.10071015},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {mar}
}

@INPROCEEDINGS {SGCN,
author = {M. Yoo and J. Song and J. Lee and N. Kim and Y. Kim and J. Lee},
booktitle = {2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA)},
title = {SGCN: Exploiting Compressed-Sparse Features in Deep Graph Convolutional Network Accelerators},
year = {2023},
volume = {},
issn = {},
pages = {1-14},
doi = {10.1109/HPCA56546.2023.10071102},
url = {https://doi.ieeecomputersociety.org/10.1109/HPCA56546.2023.10071102},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {mar}
}

@ARTICLE{GraNDe,
  author={Yun, Sungmin and Kim, Byeongho and Park, Jaehyun and Nam, Hwayong and Ahn, Jung Ho and Lee, Eojin},
  journal={IEEE Computer Architecture Letters}, 
  title={GraNDe: Near-Data Processing Architecture With Adaptive Matrix Mapping for Graph Convolutional Networks}, 
  year={2022},
  volume={21},
  number={2},
  pages={45-48},
  doi={10.1109/LCA.2022.3182387}}

@ARTICLE{GRIP,
  author={Kiningham, Kevin and Levis, Philip and Ré, Christopher},
  journal={IEEE Transactions on Computers}, 
  title={GRIP: A Graph Neural Network Accelerator Architecture}, 
  year={2023},
  volume={72},
  number={4},
  pages={914-925},
  doi={10.1109/TC.2022.3197083}}

@inproceedings{GROW,
  author       = {Ranggi Hwang and
                  Minhoo Kang and
                  Jiwon Lee and
                  Dongyun Kam and
                  Youngjoo Lee and
                  Minsoo Rhu},
  title        = {{GROW:} {A} Row-Stationary Sparse-Dense {GEMM} Accelerator for Memory-Efficient
                  Graph Convolutional Neural Networks},
  booktitle    = {{IEEE} International Symposium on High-Performance Computer Architecture,
                  {HPCA} 2023, Montreal, QC, Canada, February 25 - March 1, 2023},
  pages        = {42--55},
  publisher    = {{IEEE}},
  year         = {2023},
  url          = {https://doi.org/10.1109/HPCA56546.2023.10070983},
  doi          = {10.1109/HPCA56546.2023.10070983},
  timestamp    = {Wed, 29 Mar 2023 11:07:46 +0200},
  biburl       = {https://dblp.org/rec/conf/hpca/HwangKLKLR23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{REFLIP-HUAKE,
  author       = {Yu Huang and
                  Long Zheng and
                  Pengcheng Yao and
                  others},
  title        = {Accelerating Graph Convolutional Networks Using Crossbar-based Processing-In-Memory
                  Architectures},
  booktitle    = {{IEEE} International Symposium on High-Performance Computer Architecture,
                  {HPCA} 2022, Seoul, South Korea, April 2-6, 2022},
  pages        = {1029--1042},
  publisher    = {{IEEE}},
  year         = {2022},
  url          = {https://doi.org/10.1109/HPCA53966.2022.00079},
  doi          = {10.1109/HPCA53966.2022.00079},
  timestamp    = {Mon, 23 May 2022 16:36:22 +0200},
  biburl       = {https://dblp.org/rec/conf/hpca/HuangZYWLJX22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{GCNAX,
  author       = {Jiajun Li and
                  Ahmed Louri and
                  Avinash Karanth and
                  Razvan C. Bunescu},
  title        = {{GCNAX:} {A} Flexible and Energy-efficient Accelerator for Graph Convolutional
                  Neural Networks},
  booktitle    = {{IEEE} International Symposium on High-Performance Computer Architecture,
                  {HPCA} 2021, Seoul, South Korea, February 27 - March 3, 2021},
  pages        = {775--788},
  publisher    = {{IEEE}},
  year         = {2021},
  url          = {https://doi.org/10.1109/HPCA51647.2021.00070},
  doi          = {10.1109/HPCA51647.2021.00070},
  timestamp    = {Tue, 08 Jun 2021 16:13:48 +0200},
  biburl       = {https://dblp.org/rec/conf/hpca/LiLKB21a.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Hyperscale,
author = {Li, Shuangchen and Niu, Dimin and Wang, Yuhao and others},
title = {Hyperscale FPGA-as-a-Service Architecture for Large-Scale Distributed Graph Neural Network},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527439},
doi = {10.1145/3470496.3527439},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {946–961},
numpages = {16},
keywords = {accelerator, graph neural network, FPGA-as-a-service},
location = {New York, New York},
series = {ISCA '22}
}


% knowledge graph
@inproceedings{oh2018knowledge,
  title={Knowledge graph completion by context-aware convolutional learning with multi-hop neighborhoods},
  author={Oh, Byungkook and Seo, Seungmin and Lee, Kyong-Ho},
  booktitle={Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
  pages={257--266},
  year={2018}
}
@article{socher2013reasoning,
  title={Reasoning with neural tensor networks for knowledge base completion},
  author={Socher, Richard and Chen, Danqi and Manning, Christopher D and Ng, Andrew},
  journal={Advances in neural information processing systems},
  volume={26},
  year={2013}
}
@inproceedings{zhang2019iteratively,
  title={Iteratively learning embeddings and rules for knowledge graph reasoning},
  author={Zhang, Wen and Paudel, Bibek and Wang, Liang and others},
  booktitle={The World Wide Web Conference},
  pages={2366--2377},
  year={2019}
}
@inproceedings{chen2017task,
  title={Task-guided and path-augmented heterogeneous network embedding for author identification},
  author={Chen, Ting and Sun, Yizhou},
  booktitle={Proceedings of the tenth ACM international conference on web search and data mining},
  pages={295--304},
  year={2017}
}


% recommendation 
@inproceedings{geng2015learning,
  title={Learning image and user features for recommendation in social networks},
  author={Geng, Xue and Zhang, Hanwang and Bian, Jingwen and Chua, Tat-Seng},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={4274--4282},
  year={2015}
}
@inproceedings{zhao2017meta,
  title={Meta-graph based recommendation fusion over heterogeneous information networks},
  author={Zhao, Huan and Yao, Quanming and Li, Jianda and Song, Yangqiu and Lee, Dik Lun},
  booktitle={Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining},
  pages={635--644},
  year={2017}
}
@inproceedings{hu2018leveraging,
  title={Leveraging meta-path based context for top-n recommendation with a neural co-attention model},
  author={Hu, Binbin and Shi, Chuan and Zhao, Wayne Xin and Yu, Philip S},
  booktitle={Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery \& data mining},
  pages={1531--1540},
  year={2018}
}





% social network
@inproceedings{yasunaga2019scisummnet,
  title={Scisummnet: A large annotated corpus and content-impact models for scientific paper summarization with citation networks},
  author={Yasunaga, Michihiro and Kasai, Jungo and Zhang, Rui and others},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  number={01},
  pages={7386--7393},
  year={2019}
}
@article{zhou2015cross,
  title={Cross-platform identification of anonymous identical users in multiple social media networks},
  author={Zhou, Xiaoping and Liang, Xun and Zhang, Haiyan and Ma, Yuefeng},
  journal={IEEE transactions on knowledge and data engineering},
  volume={28},
  number={2},
  pages={411--424},
  year={2015},
  publisher={IEEE}
}
@article{tajeuna2018modeling,
  title={Modeling and predicting community structure changes in time-evolving social networks},
  author={Tajeuna, Etienne Gael and Bouguessa, Mohamed and Wang, Shengrui},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  volume={31},
  number={6},
  pages={1166--1180},
  year={2018},
  publisher={IEEE}
}
@article{zheng2020clustering,
  title={Clustering social audiences in business information networks},
  author={Zheng, Yu and Hu, Ruiqi and Fung, Sai-fu and others},
  journal={Pattern Recognition},
  volume={100},
  pages={107126},
  year={2020},
  publisher={Elsevier}
}

\revise{
@ARTICLE{FPGAN,
  author={Yan, Weian and Tong, Weiqin and Zhi, Xiaoli},
  journal={IEEE Access}, 
  title={FPGAN: An FPGA Accelerator for Graph Attention Networks With Software and Hardware Co-Optimization}, 
  year={2020},
  volume={8},
  number={},
  pages={171608-171620},
  doi={10.1109/ACCESS.2020.3023946}}

@inproceedings{NTGAT,
author = {Hou, Wentao and Zhong, Kai and Zeng, Shulin and Dai, Guohao and Yang, Huazhong and Wang, Yu},
title = {NTGAT: A Graph Attention Network Accelerator with Runtime Node Tailoring},
year = {2023},
isbn = {9781450397834},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3566097.3567869},
doi = {10.1145/3566097.3567869},
abstract = {Graph Attention Network (GAT) has demonstrated better performance in many graph tasks than previous Graph Neural Networks (GNN). However, it involves graph attention operations with extra computing complexity. While a large amount of existing literature has researched GNN acceleration, few have focused on the attention mechanism in GAT. The graph attention mechanism makes the computation flow different. Therefore, previous GNN accelerators can not support GAT well. Besides, GAT distinguishes the importance of neighbors and makes it possible to reduce the workload through runtime tailoring. We present NTGAT, a software-hardware co-design approach to accelerate GAT with runtime node tailoring. Our work comprises both a runtime node tailoring algorithm and an accelerator design. We propose a pipeline sorting method and a hardware unit to support node tailoring during inference. The experiments show that our algorithm can reduce up to 86\% of aggregation workload while incurring slight accuracy loss (&lt;0.4\%). And the FPGA based accelerator can achieve up to 3.8\texttimes{} speedup and 4.98\texttimes{} energy efficiency comparing to the GPU baseline.},
booktitle = {Proceedings of the 28th Asia and South Pacific Design Automation Conference},
pages = {645–650},
numpages = {6},
keywords = {software-hardware codesign, graph attention network},
location = {Tokyo, Japan},
series = {ASPDAC '23}
}

@ARTICLE{FTW-GAT,
  author={He, Zerong and Tian, Teng and Wu, Qizhe and Jin, Xi},
  journal={IEEE Transactions on Circuits and Systems II: Express Briefs}, 
  title={FTW-GAT: An FPGA-Based Accelerator for Graph Attention Networks with Ternary Weights}, 
  year={2023},
  volume={},
  number={},
  pages={1-1},
  doi={10.1109/TCSII.2023.3280180}}

}


@INPROCEEDINGS{DNNBuilder,
  author={Zhang, Xiaofan and Wang, Junsong and Zhu, Chao and others},
  booktitle={2018 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)}, 
  title={DNNBuilder: an Automated Tool for Building High-Performance DNN Hardware Accelerators for FPGAs}, 
  year={2018},
  volume={},
  number={},
  pages={1-8},
  doi={10.1145/3240765.3240801}}


@article{workloadbalance,
author = {Blumofe, Robert D. and Leiserson, Charles E.},
title = {Scheduling Multithreaded Computations by Work Stealing},
year = {1999},
issue_date = {Sept. 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {5},
issn = {0004-5411},
url = {https://doi.org/10.1145/324133.324234},
doi = {10.1145/324133.324234},
abstract = {This paper studies the problem of efficiently schedulling fully strict (i.e., well-structured) multithreaded computations on parallel computers. A popular and practical method of scheduling this kind of dynamic MIMD-style computation is “work stealing,” in which processors needing work steal computational threads from other processors. In this paper, we give the first provably good work-stealing scheduler for multithreaded computations with dependencies.Specifically, our analysis shows that the expected time to execute a fully strict computation on P processors using our work-stealing scheduler is T1/P + O(T ∞ , where T1 is the minimum serial execution time of the multithreaded computation and (T ∞ is the minimum execution time with an infinite number of processors. Moreover, the space required by the execution is at most S1P, where S1 is the minimum serial space requirement. We also show that the expected total communication of the algorithm is at most O(PT ∞( 1 + nd)Smax), where Smax is the size of the largest activation record of any thread and nd is the maximum number of times that any thread synchronizes with its parent. This communication bound justifies the folk wisdom that work-stealing schedulers are more communication efficient than their work-sharing counterparts. All three of these bounds are existentially optimal to within a constant factor.},
journal = {J. ACM},
month = {sep},
pages = {720–748},
numpages = {29},
keywords = {multithreading, work stealing, critical-path length, randomized algorithm, thread scheduling, multiprocessor}
}

@inproceedings{PowerGraph,
author = {Joseph E. Gonzalez and Yucheng Low and Haijie Gu and Danny Bickson and Carlos Guestrin},
title = {{PowerGraph}: Distributed {Graph-Parallel} Computation on Natural Graphs},
booktitle = {10th USENIX Symposium on Operating Systems Design and Implementation (OSDI 12)},
year = {2012},
isbn = {978-1-931971-96-6},
address = {Hollywood, CA},
pages = {17--30},
url = {https://www.usenix.org/conference/osdi12/technical-sessions/presentation/gonzalez},
publisher = {USENIX Association},
month = oct,
}



@inproceedings{MetaNMP, 
author = {Chen, Dan and He, Haiheng and Jin, Hai and others}, title = {MetaNMP: Leveraging Cartesian-Like Product to Accelerate HGNNs with Near-Memory Processing}, year = {2023}, isbn = {9798400700958}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3579371.3589091}, doi = {10.1145/3579371.3589091}, abstract = {Heterogeneous graph neural networks (HGNNs) based on metapath exhibit powerful capturing of rich structural and semantic information in the heterogeneous graph. HGNNs are highly memory-bound and thus can be accelerated by near-memory processing. However, they also suffer from significant memory footprint (due to storing metapath instances as intermediate data) and severe redundant computation (when vertex features are aggregated among metapath instances). To address these issues, this paper proposes MetaNMP, the first DIMM-based near-memory processing HGNNs accelerator with reduced memory footprint and high performance. Specifically, we first propose a cartesian-like product paradigm to generate all metapath instances on the fly for heterogeneous graphs. In this way, metapath instances no longer need to be stored as intermediate data, avoiding significant memory consumption. We then design a data flow for aggregating vertex features on metapath instances, which aggregates vertex features along the direction of the metapath instances dispersed from the starting vertex to exploit shareable aggregation computations, eliminating most of the redundant computations. Finally, we integrate specialized hardware units in DIMM to accelerate HGNNs with near-memory processing, and introduce a broadcast mechanism for edge data and vertex features to mitigate the inter-DIMM communication. Our evaluation shows that MetaNMP achieves the memory space reduction of 51.9% on average and the performance improvement by 415.18\texttimes{} compared to NVIDIA Tesla V100 GPU.}, booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture}, articleno = {56}, numpages = {13}, keywords = {heterogeneous graph neural networks, near-memory processing, cartesian product}, location = {Orlando, FL, USA}, series = {ISCA '23} }


@article{GraphSage,
  title={Inductive representation learning on large graphs},
  author={Hamilton, Will and Ying, Zhitao and Leskovec, Jure},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{Comprehensive_Survey_GNN_Distributed_Training,
  title={A Comprehensive Survey on Distributed Training of Graph Neural Networks},
  author={Lin, Haiyang and Yan, Mingyu and Ye, Xiaochun and others},
  journal={arXiv preprint arXiv:2211.05368},
  year={2022}
}


@article{MultiGCN,
  title={Multi-Node Acceleration for Large-Scale GCNs},
  author={Sun, Gongjian and Yan, Mingyu and Wang, Duo and others},
  journal={IEEE Transactions on Computers},
  volume={71},
  number={12},
  pages={3140--3152},
  year={2022},
  publisher={IEEE}
}

@article{GCN_Bidirectional_Fusion,
  title={Hardware acceleration for gcns via bidirectional fusion},
  author={Li, Han and Yan, Mingyu and Yang, Xiaocheng and others},
  journal={IEEE Computer Architecture Letters},
  volume={20},
  number={1},
  pages={66--4},
  year={2021},
  publisher={IEEE}
}

@ARTICLE{understand_distributed_gnn,
  author={Lin, Haiyang and Yan, Mingyu and Yang, Xiaocheng and others},
  journal={IEEE Computer Architecture Letters}, 
  title={Characterizing and Understanding Distributed GNN Training on GPUs}, 
  year={2022},
  volume={21},
  number={1},
  pages={21-24},
  doi={10.1109/LCA.2022.3168067}}

@INPROCEEDINGS{GNNMark,
  author={Baruah, Trinayan and Shivdikar, Kaustubh and Dong, Shi and others},
  booktitle={2021 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)}, 
  title={GNNMark: A Benchmark Suite to Characterize Graph Neural Network Training on GPUs}, 
  year={2021},
  volume={},
  number={},
  pages={13-23},
  doi={10.1109/ISPASS51385.2021.00013}}

@INPROCEEDINGS{characterize_gnn_efficiency,
  author={Huang, Xin and Kim, Jongryool and Rees, Bradley and Lee, Chul-Ho},
  booktitle={2022 IEEE International Symposium on Workload Characterization (IISWC)}, 
  title={Characterizing the Efficiency of Graph Neural Network Frameworks with a Magnifying Glass}, 
  year={2022},
  volume={},
  number={},
  pages={160-170},
  doi={10.1109/IISWC55918.2022.00023}}

@inproceedings{understand_and_bridge,
author = {Huang, Kezhao and Zhai, Jidong and Zheng, Zhen and Yi, Youngmin and Shen, Xipeng},
title = {Understanding and Bridging the Gaps in Current GNN Performance Optimizations},
year = {2021},
isbn = {9781450382946},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437801.3441585},
doi = {10.1145/3437801.3441585},
abstract = {Graph Neural Network (GNN) has recently drawn a rapid increase of interest in many domains for its effectiveness in learning over graphs. Maximizing its performance is essential for many tasks, but remains preliminarily understood. In this work, we provide an in-depth examination of the state-of-the-art GNN frameworks, revealing five major gaps in the current frameworks in optimizing GNN performance, especially in handling the special complexities of GNN over traditional graph or DNN operations. Based on the insights, we put together a set of optimizations to fill the gaps. These optimizations leverage the state-of-the-art GPU optimization techniques and tailor them to the special properties of GNN. Experimental results show that these optimizations achieve 1.37\texttimes{}--15.5\texttimes{} performance improvement over the state-of-the-art frameworks on various GNN models.},
booktitle = {Proceedings of the 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {119–132},
numpages = {14},
keywords = {parallelism, performance optimizations, GNN},
location = {Virtual Event, Republic of Korea},
series = {PPoPP '21}
}

@misc{understand_gnn_computational_graph,
      title={Understanding GNN Computational Graph: A Coordinated Computation, IO, and Memory Perspective}, 
      author={Hengrui Zhang and Zhongming Yu and Guohao Dai and others},
      year={2021},
      eprint={2110.09524},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{analysis_of_bottlenecks,
title = {Empirical analysis of performance bottlenecks in graph neural network training and inference with GPUs},
journal = {Neurocomputing},
volume = {446},
pages = {165-191},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.03.015},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221003659},
author = {Zhaokang Wang and Yunpan Wang and Chunfeng Yuan and Rong Gu and Yihua Huang},
}

@INPROCEEDINGS{characterize_gnn_accelerators,
  author={Guirado, Robert and Jain, Akshay and Abadal, Sergi and Alarcón, Eduard},
  booktitle={2021 IEEE International Symposium on Circuits and Systems (ISCAS)}, 
  title={Characterizing the Communication Requirements of GNN Accelerators: A Model-Based Approach}, 
  year={2021},
  volume={},
  number={},
  pages={1-5},
  doi={10.1109/ISCAS51556.2021.9401612}}

@article{in_depth_concurrency_analysis,
  title={Parallel and distributed graph neural networks: An in-depth concurrency analysis},
  author={Besta, Maciej and Hoefler, Torsten},
  journal={arXiv preprint arXiv:2205.09702},
  year={2022}
}


@INPROCEEDINGS{GNN_dataflow_taxonomy,
  author={Garg, Raveesh and Qin, Eric and Muñoz-Matrínez, Francisco and Guirado, Robert and Jain, Akshay and Abadal, Sergi and Abellán, José L. and Acacio, Manuel E. and Alarcón, Eduard and Rajamanickam, Sivasankaran and Krishna, Tushar},
  booktitle={2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)}, 
  title={Understanding the Design-Space of Sparse/Dense Multiphase GNN dataflows on Spatial Accelerators}, 
  year={2022},
  volume={},
  number={},
  pages={571-582},
  doi={10.1109/IPDPS53621.2022.00062}}
