\section{Background}\label{sec:background}
In this section, we introduce the relevant concepts for HGNNs using Table~\ref{tb:notation}, Fig.~\ref{fig:HGNN}, Algorithm \ref{alg:Original_Programming_Model}, and Table~\ref{tab:HGNN_general_framework}.

\textbf{Heterogeneous Graph.} 
A HetG shown in Fig.~\ref{fig:HGNN}, is defined as $G=(V,E,\mathcal{T}^v,\mathcal{T}^e)$~\cite{SeHGNN,Simple-HGN} using notations in Table~\ref{tb:notation}, where $V$ is the set of vertices with a vertex type mapping function $\phi:V\rightarrow\mathcal{T}^v$, and $E$ is the set of edges with an edge type mapping function $\psi:E\rightarrow\mathcal{T}^e$.
Each vertex $v_i{\in}V$ is attached with a vertex type $c_v{=}\phi(v_i){\in}\mathcal{T}^v$. Each edge $e_{u,v}{\in}E\,$ is attached with a relation $r_{c_v,c_u}{=}\psi(e_{u,v}){\in}\mathcal{T}^e$, starting from the source vertex $u$ to the target vertex $v$.  A graph is heterogeneous when $|\mathcal{T}^v|+|\mathcal{T}^e|>2$, otherwise it is homogeneous.

% 移动到fig1，fig1加上metapath
\textbf{Semantic Graph.} The semantic graph is generated by different metapath. 
A metapath refers to a sequence of vertex types and edge types that captures specific semantic relationships between vertices. It provides a higher-level abstraction of the graph structure and enables the identification of meaningful paths and patterns within the graph. By defining and utilizing metapaths, we can gain insights into the complex relationships and dependencies present in HetGs.
%For example, each semantic graph in R-GCN~\cite{R-GCN} contains only one type of relations, such as author$\xrightarrow{\rm writes}$paper, paper$\xrightarrow{\rm cites}$paper (abbreviated as AP and PP in Fig.\ref{fig:HGNN}), 
For example in Fig.\ref{fig:HGNN}, a HetG is partitioned into several semantic graphs based on metapaths~\cite{sun2011pathsim,sun2012mining}, such as author$\rightarrow$paper$\rightarrow$author (abbreviated as APA) that represents a coauthor relationship.



\begin{table}[!t]
%\vspace{-8pt}
\centering
\caption{Notations and corresponding explanations.}
\label{tb:notation}
\resizebox{0.48\textwidth}{!}{
\tabcolsep=0.5pt
\begin{tabular}{cc|cc}
\toprule
Notation                & Explanation                           & Notation                  & Explanation       \\ \midrule
$G$                     & heterogeneous graph                   & $V$                       & vertex set \\
$E$                     & edge set                              & $\mathcal{T}^v$           & vertex type set \\
$\mathcal{T}^e$         & edge type set                         & $u,\,v$                   & vertex \\
e ($e_{u,v}$)          & edge (from $u$ to $v$)                & $G^{\mathcal{P}}$ & semantic graph \\
$r$, $\mathcal{P}$          & relation, metapath   & c ($c_v$)                 & vertex type \\
$h$                     & vertex or relation embedding                      & $\mathcal{N}_v$           & neighbor set of vertex $v$ \\
$W$                     & transformation weight matrix          & $h'$                      & projected vertex feature \\
$a$                     & attention vector                      & $||$                      & concatenation \\
$\alpha_{u,v}$          & attention importance    & $\theta (\theta^{\mathcal{P}}_{u,v})$ & attention coefficient \\
$z$                     & intermediate aggregation feature      & $b$                       & transformation bias \\
$\sigma$                & non-linear functions                  & $x$                       & original vertex feature \\

\bottomrule
\vspace{-15pt}
\end{tabular}}
\end{table}

\begin{algorithm}[!t]
    \SetAlgoLined
    \label{alg:Original_Programming_Model}
    \footnotesize
    \caption{\textbf{HGNN Programming Model}}
   % \centerline{{\underline{\color{blue} $\triangleleft \quad \textbf{Semantic Graph Build}$}}}
   % \For{each metapath $\mathcal{P}$}{ %\\{\color{blue} \Comment{Semantic Graph Build}}
    %    $G^{\mathcal{P}}$=Semantic Graph Build($G$,$\mathcal{P}$)\;
   % }
   % \centerline{{\underline{\color{blue} $\triangleleft \quad \textbf{Feature Projection}$}}}
    \For{each vertex $v$}{
        $h_{v}'$=Feature Projection($h_{v}$)\;
    }
   % \centerline{{\underline{\color{blue} $\triangleleft \quad \textbf{Neighbor Aggregation}$}}}
    \For{each semantic graph $G^{\mathcal{P}}$}{
        \For{each vertex pair ($u,v$)}{
            $z_{v}^\mathcal{P}$=Neighbor Aggregation($h_{u}'$)\;
        }
    }
  %  \centerline{{\underline{\color{blue} $\triangleleft \quad \textbf{Semantic Fusion}$}}}
    \For{each vertex $v$}{
        \For{each semantic graph $G^{\mathcal{P}}$}{
            $z_v$=Semantic Fusion($z_{v}^\mathcal{P}$)\;
        }
    }
    
\end{algorithm}

% ------------two column--------------
% Figure environment removed


\begin{table*}[!ht]
\small\centering
\caption{The illustration of representative HGNN models.}
\label{tab:HGNN_general_framework}
\resizebox{1.0\textwidth}{!}{
    \begin{tabular}{|c|ccc|c|}
    \hline
     &
      % HAN~\cite{HAN} &
      % R-GAT~\cite{R-GAT} &
      % R-GCN~\cite{R-GCN} &
      % S-HGN~\cite{Simple-HGN} \\ \hline
      \multicolumn{1}{c|}{HAN~\cite{HAN}} &
      \multicolumn{1}{c|}{R-GAT~\cite{R-GAT}} &
      R-GCN~\cite{R-GCN} &
      S-HGN~\cite{Simple-HGN} \\ \hline
      
    \begin{tabular}[c]{@{}c@{}}Feature\\ Projection\end{tabular} &
      \multicolumn{1}{c|}{${h}'_v=W^{c_v} {x}_v$} &
      \multicolumn{2}{c|}{${h}_v^r=W^{r} {x}_v$} &
      ${h}'_v=W^{c_v} {x}_v$ \\ \hline
    \begin{tabular}[c]{@{}c@{}}Neighbor\\ Aggregation\end{tabular} &
      \multicolumn{2}{c|}{\begin{tabular}[c]{@{}c@{}}$\theta^\mathcal{P}_{u,v}=\sigma({a}^T_{\mathcal{P}}\cdot [{h}'_u || {h}'_v])$,\\ $\alpha^\mathcal{P}_{u,v}=\frac{\exp(\theta^\mathcal{P}_{u,v})}{\sum_{k\in\mathcal{N}^\mathcal{P}_v}\exp(\theta^\mathcal{P}_{v,k})}, {z}^\mathcal{P}_v=\sigma(\sum_{u\in\mathcal{N}^\mathcal{P}_v}\alpha^\mathcal{P}_{u,v}{h}'_u)$\end{tabular}} &
      ${z}_v^r=\frac{1}{|\mathcal{N}_v^r|}\sum_{u\in\mathcal{N}_v^r}{h}^r_u$ &
      \begin{tabular}[c]{@{}c@{}}$\alpha_{u,v}=\frac{\exp \left(\sigma\left(\boldsymbol{a}^{T}\left[{h}'_{u}\left\|\boldsymbol{h}'_v\right\| \boldsymbol{W}_{r} \boldsymbol{h}_r\right]\right)\right)}{\sum_{k \in \mathcal{N}_{v}} \exp \left(\sigma\left(\boldsymbol{a}^{T}\left[{h}'_k\left\|{h}'_v\right\| W_{r} \boldsymbol{h}_r\right]\right)\right)}$,\\ ${h}_{v} =\sum_{u \in \mathcal{N}_{v}^r} \alpha_{u,v} {h}'_{u}$\end{tabular} \\ \hline
    \begin{tabular}[c]{@{}c@{}}Semantic\\ Fusion\end{tabular} &
      \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}$w_{\mathcal{P}}=\frac{1}{|V^{\mathcal{P}}|}\sum_{v\in V^{\mathcal{P}}}{q}^T\cdot\tanh({W}^{\mathcal{P}}{z}_v^\mathcal{P}+{b})$,\\ $\beta_{\mathcal{P}_i}=\frac{\exp(w_{P_i})}{\sum_{\mathcal{P}_j}\exp(w_{P_j})},\,\,{h}_v = \sum_{\mathcal{P}_i}\beta_{\mathcal{P}_i}{z}^{\mathcal{P}_i}_v$\end{tabular}} &
      \multicolumn{1}{c|}{${h}_v =\frac{1}{|\mathcal{P}|} \sum_{\mathcal{P}_i}{z}^{\mathcal{P}_i}_v$} &
      ${h}_v=\sum_r {z}_v^r+W^{c_v}{x}_v$ &
      --- \\ \hline
    \end{tabular}
}
\vspace{-10pt}

\end{table*}



\begin{comment}

\textbf{GNN.} 
GNN (e.g. GCNs~\cite{GCN}, GATs~\cite{GAT}) captures structural information in homoGs following a neighborhood aggregation scheme, where the representation of each vertex is computed by recursively aggregating the features of neighbor vertices. 
\revise{In general, each layer in a GNN model contains two major stages: \blackcircledempty{1} \textit{Feature Projection} (FP) stage transforms the feature vector of each vertex to a new one using a multi-layer perceptron (MLP); \blackcircledempty{2} \textit{Neighbor Aggregation} (NA) stage aggregates features from all its source neighbors.} 

    
%These stages are also named \textit{Combination} and \textit{Aggregation} in work~\cite{HyGCN}. 
Typically, the k-th layer/iteration of GNNs is formulated as
\begin{equation}
\begin{aligned}
  {\rm \textbf{FP}}: & \quad {h'}^{(k)}_v = W(h^{(k-1)}_v), \\
  {\rm \textbf{NA}}: & \quad h^{(k)}_v = \sum_{u\in {\mathcal{N}_v\bigcup{\{v\}}}} \hat{\alpha}^{(k)}_{u,v} {h'}^{(k)}_u,
\end{aligned}
\label{eq:gcn}
\end{equation}
where $W$ is the transformation matrix and $\hat{\alpha}_{u,v}$ represents importance of each neighbor vertex. 

The $\hat{\alpha}_{u,v}$ derives from the normalized adjacency matrix in GCNs and the attention module in GATs. 
%
For example, GATs leverage weighted aggregation by introducing an attention mechanism to encourage the model to focus on the most important parts of the neighbor vertices. The attention importance is calculated using the vertex features on both the source and target vertices, which are represented as (${k}$ is omitted for simplicity)
\begin{equation}
% \setlength{\abovedisplayskip}{2pt}
% \setlength{\belowdisplayskip}{2pt}
\hat{\alpha}_{u,v}=\frac{\exp{({\rm LeakyReLU}(a^T[Wh_v || Wh_u]))}}{\sum_{t\in \mathcal{N}_v} \exp{({\rm LeakyReLU}(a^T[Wh_v || Wh_t]))}}, \label{eq:attn}
\end{equation}
\end{comment}

\textbf{Heterogeneous Graph Neural Network.}
HGNN follows a neighborhood aggregation scheme and a semantic fusion scheme, where the final representation of each vertex is computed by recursively aggregating the feature vectors of its neighbor vertices in each semantic graph and fusing the aggregated results across all semantic graphs, as shown in Fig. \ref{fig:HGNN}.
%They first aggregate neighboring features of the same semantic and then fuse different semantics.
For example, HAN~\cite{HAN} aggregates structural information using the neighbor attention in each semantic graph and then fuses outputs from different semantic graphs using the semantic attention for each vertex.

To capture both the structural information and semantic information in HetGs, most prevalent HGNN models usually contain four major execution stages as shown in Fig. \ref{fig:HGNN}.
\blackcircledempty{1} Semantic Graph Build: The SGB stage builds semantic graphs for the following stages by partitioning the original HetG into a set of semantic graphs based on relations or predefined metapaths.
\blackcircledempty{2} Feature Projection: In the FP stage, the feature vector of each vertex is transformed to a new one using a MLP within each semantic graph.
\blackcircledempty{3} Neighbor Aggregation: The NA stage utilizes an attention mechanism~\cite{GAT} to perform a weighted sum aggregation of features from neighbors within each semantic graph.
\blackcircledempty{4} Semantic Fusion: The SF stage fuses the semantic information obtained from all semantic graphs with an attention mechanism~\cite{GAT}, aiming to combine the results of the NA stage across different semantic graphs for each vertex.
Algorithm \ref{alg:Original_Programming_Model} shows the programming model for HGNNs. 
Table~\ref{tab:HGNN_general_framework} presents the computation corresponding to each stage of four representative HGNN models.


% \blackcircledempty{1} Semantic Graph Build (SGB): \add{SGB stage partitions the original HetG into several semantic
% graphs based on relations or predefined metapaths;} \blackcircledempty{2} Feature Projection (FP): \add{FP stage transforms the feature vector of each
% vertex to a new one using a multi-layer perceptron (MLP) in
% each semantic graph;}
% \blackcircledempty{3} Neighbor Aggregation (NA): \add{NA stage
% maintains the most graph processing behaviors\cite{Graphicionado} and aggregates features from neighbors in each semantic graph;}
% and \blackcircledempty{4} Semantic Fusion (SF): \add{SF stage fuses semantic information revealed by all semantic graphs, i.e., fuses the results of the NA
% stage across different semantic graphs for each vertex;}





\begin{comment}
    
\textbf{HGNN.}
\revise{HGNN captures both the structural information and semantic information in HetGs following a neighborhood aggregation scheme and a semantic fusion scheme, where neighbors of different semantics are aggregated separately and then fused.} In general, most prevalent HGNNs usually contain four major execution stages, including \blackcircledempty{1} \textit{Semantic Graph Build} (SGB), \blackcircledempty{2} \textit{Feature Projection} (FP), \blackcircledempty{3} \textit{Neighbor Aggregation} (NA), and \blackcircledempty{4} \textit{Semantic Fusion} (SF), as shown in Fig.~\ref{fig:HGNN}. The SGB stage splits the original HetG into several semantic graphs based on relations or pre-defined metapaths. \revise{The FP and NA stages are just like those in GNNs but are executed independently in each semantic graph. The SF stage fuses the NA results of the same vertices across different semantic graphs.}

% HGNN 在算法库里面是如何实现的（PyG & DGL）

%Popular GPU-based graph learning software frameworks (e.g. PyG~\cite{PyG}, DGL~\cite{DGL}) construct an HGNN model as a composition of several GNN models. After the original HetG is split into several semantic graphs, each semantic graph is processed with a traditional GNN model. These traditional GNN models are executed one by one, and their outputs are fused to generate the final embedding for each vertex.

The implementation of HGNNs on GPU-based graph learning software frameworks (e.g. PyG~\cite{PyG}, DGL~\cite{DGL}) can be viewed as combinations of the SGB method, basic GNN modules for each semantic graph, and the final fusion method. 
For example, R-GCN~\cite{R-GCN} and R-GAT~\cite{R-GAT} spilt the original HetGs based on different relations and take the average of outputs of all semantic graphs as the final embeddings, while HAN~\cite{HAN} splits HetGs with different metapaths in the SGB stage and use a weighted-sum format with attentions in the SF stage. R-GCN takes a GCN as the basic GNN module in each semantic graph while R-GAT and HAN use a GAT. Unlike the above three methods, Simple-HGN (S-HGN)~\cite{Simple-HGN} takes a single multi-layer GAT network as the backbone and incorporates both vertex features and learnable edge-type embeddings to generate attention importance. \revise{Details of each model can be see at Table~\ref{tab:HGNN_general_framework}.}

% \todo{add table of HGNNs algorithm.}
\end{comment}
