\section{Experimental Results} \label{sec:experimental_results} 
In this section, we compare HiHGNN with the baselines and present the optimization analysis in detail. 
The last set of bars in the figures, labeled GM, shows the geometric mean across all HGNN models. 


\subsection{Overall Results} \label{sec:overall_results} 
% \todo{
%% 1. 一句话告诉大家为什么做实验，引出图
%% 2. 给出最直接的结论
%% 3. 给出支撑结论的技术点
%% 4. 给出实例，支撑技术点
%% 5. 对需要突出的insight进行解释描述

%% 1. energy部分减少
% }
\textbf{Speedup.} 
Fig.~\ref{fig:overall_speedup} shows the speedup of HiHGNN to GPU T4.
HiHGNN achieves an average speedup of 40.0$\times$ and 8.3$\times$ compared to GPU T4 and A100, respectively.
The performance improvement of HiHGNN is attributed to the well-designed hardware datapath for HGNN and the efficient exploitation of inter-semantic-graph parallelism and data reusability. First, the bound-aware stage-fusion programming model and its corresponding hardware datapath help greatly improve compute utilization and bandwidth utilization across all stages.
Second, an independency-aware parallelism exploitation optimization helps leverage more hardware resources to improve performance.
Third, the similarity-aware execution scheduling significantly reduces the random accesses to DRAM, resulting in further performance improvement.
The detailed analysis of the effects of these optimizations is presented in Section \ref{sec:evaluation_optimization}.


% Figure environment removed


\textbf{Compare with HyGCN-like Accelerator.}
As discussed in Section~\ref{sec:the_need_for_HGNN_accelerator}, existing HyGCN-like accelerators primarily cater to GCN models and lack support for the execution flow of HGNNs. To underscore the advantages of HiHGNN over HyGCN-like accelerators, we modify the execution flow of HiHGNN to align with that of HyGCN-like accelerators, denoting the modified version as HyHGNN. This adaptation involves na\"ive stage fusion, shifting the parallel granularity from the edge level to the stage level.
It is worth noting that the techniques introduced by HiHGNN, such as workload-aware scheduling and similarity-aware execution scheduling, are not incorporated into HyHGNN. 

Fig.~\ref{fig:compare_simple_stage_fusion} manifests that the HiHGNN yields a 6.1$\times$ improvement over HyHGNN. 
This improvement primarily stems from the dedicated datapath design tailored for HGNNs. Additionally, HiHGNN's superior parallelism, facilitated by stage-fusion programming and independency-aware parallel execution, contributes to the enhanced performance. Furthermore, the efficient data reuse mechanism of HiHGNN, specifically the similarity-aware execution scheduling, plays a crucial role in driving these performance gains.


% Figure environment removed


\textbf{Area and Power.} Table \ref{tb:chip_area_power} provides a detailed breakdown of the area and power of HiHGNN (except for DRAM), which are 21.36 $mm^2$ and 12.00 $W$, respectively. The buffers that include FP-Buf, NA-Buf, SF-Buf, and Att-Buf occupy 60.25\% area and consume 12.10\% power. \textit{SIMD Module} and \textit{Systolic Module} are the main computing units that produce 38.81\% and 83.61\% power, respectively. For the computation precision, we use a 32-bit integer that is enough to maintain the accuracy of HGNN inference.

\begin{table}[!t]
\vspace{-5pt}
 \caption{Characteristics of HiHGNN (TSMC 12 $nm$).} \label{tb:chip_area_power}
 \centering
 \renewcommand\arraystretch{1.0}
    \resizebox{0.46\textwidth}{!}{
\begin{tabular}{|l|l|r|r|r|r|}
\hline
\multicolumn{2}{|l|}{\begin{tabular}[c]{@{}c@{}}\textbf{Component or Block}\end{tabular}} & \begin{tabular}[r]{@{}c@{}}\textbf{Area ($mm^2$)}\end{tabular} & \textbf{\%} & \begin{tabular}[r]{@{}c@{}}\textbf{Power ($mW$)}\end{tabular}  & \textbf{\%} \\ \hline \hline
\multicolumn{2}{|l|}{{HiHGNN (4 Lanes)}}    &21.36 &100 &12001.87 &100    \\ \hline \hline
\multicolumn{6}{|c|}{\begin{tabular}[c]{@{}c@{}} \textbf{Breakdown by Functional Block} \end{tabular}} \\ \hline
\multicolumn{2}{|l|}{FP-Buf}     &1.80 &8.42 &202.90 &1.69    \\
\multicolumn{2}{|l|}{NA-Buf}   &10.70 &50.11 &1207.42 &10.06    \\
\multicolumn{2}{|l|}{SF-Buf}   &0.09 &0.41 &9.98 &0.08     \\
\multicolumn{2}{|l|}{Att-Buf}   &0.28 &1.31 &31.60 &0.26    \\
\multicolumn{2}{|l|}{Systolic Module}    &5.10 &23.88 &6758.40 &56.31   \\
\multicolumn{2}{|l|}{SIMD Module}          &3.19 &14.93 &3276.80 &27.30    \\
\multicolumn{2}{|l|}{Crossbar}          &0.09 &0.44 &440.82 &3.67    \\
\multicolumn{2}{|l|}{Others}          &0.11 &0.50 & 73.95 & 0.62    \\  \hline
\end{tabular}
}
%\vspace{-5pt}
% \todo{add RAB crossbar}
\end{table}


% Figure environment removed



\textbf{Energy and Its Breakdown.} Fig.~\ref{fig:overall_detailed_results} (a) shows that HiHGNN achieves the average energy reduction with 99.59\% and 99.74\% compared to GPU T4 and GPU A100, respectively. The energy consumption of all platforms includes the DRAM.
There are two factors mainly contributing to the overall reduction in energy consumption. First, the reduction of execution time leads to a decrease in energy consumption. Second, the exploitation of data reusability greatly reduces DRAM accesses, reducing energy consumption as well.

Fig.~\ref{fig:overall_detailed_results} (b) shows the breakdown of the energy consumption of different HGNNs models. 
First, the energy consumed by DRAM accesses occupies most of the energy consumption in all models. 
Second, the HAN model incurs a higher energy cost on its \textit{SIMD Module} and on-chip buffer than other models because the NA stage occupies the most execution time.
Third, both the R-GCN and R-GAT models consume a higher energy consumption in \textit{Systolic Module} compared to other models, due to the long execution time of their FP stage.
Fourth, the S-HGN model consumes more energy in the DRAM access, due to the requirement to load both raw features and the additional edge embeddings in the NA stage.


\textbf{Utilization of DRAM Bandwidth.}
Fig.~\ref{fig:overall_detailed_results} (c) shows the average utilization of the DRAM bandwidth of HiHGNN and others. 
HiHGNN reaches 2.1$\times$ of GPU T4 and 5.0$\times$ of GPU A100 in bandwidth utilization, respectively.
The bandwidth utilization for GPUs is limited by the random accesses to projected features, attention coefficients, and edge embeddings. However, those intermediate results are elaborately stored on on-chip buffers in HiHGNN, eliminating random DRAM access. It should be noted that, in the case of the HAN model, the intermediate results in the NA stage are mostly stored in NA-Buf, which significantly reduces the DRAM accesses while also reducing bandwidth utilization.



\textbf{Number of DRAM Accesses.} 
Fig.~\ref{fig:overall_detailed_results} (d) shows the total data access to the DRAM of HiHGNN and others during runtime, normalized to GPU T4. 
HiHGNN dramatically reduces DRAM accesses by 89\% and 80\% compared to GPU T4 and GPU A100 on average, respectively.
%
HiHGNN reduces the need for DRAM access by elaborately reusing intermediate results that are frequently accessed, such as projected features.
From the perspective of the model, the HAN model adopts the type-specific FP stage and attention-based NA stage, and is greatly benefited from the reuse of intermediate results. In addition, HAN also takes advantage of the similarity-aware execution scheduling to remove redundant data accesses at the semantic graph level, resulting in a further reduction of DRAM accesses.
On the contrary, the relation-specific FP stage of R-GCN dominates the execution, and significant DRAM accesses to raw features are inevitable.


\subsection{Effects of Proposed Optimizations} \label{sec:evaluation_optimization}

A detailed evaluation on the DBLP dataset is conducted to give more insights into the proposed optimizations. The optimizations include bound-aware stage fusion, independency-aware parallelism exploitation, and similarity-aware execution scheduling. 
We compare the following combinations to understand the effectiveness of them: 1) w/ and w/o bound-aware stage fusion; 2) the scalability of scale-up architecture as well as w/ and w/o workload-aware scheduling; 3) w/ and w/o similarity-aware execution scheduling.


\textbf{Effect of Bound-aware Stage Fusion.}
Fig.~\ref{fig:BPA_benefit} depicts the benefit brought by the bound-aware stage fusion. During execution, this technique reduces 35\% execution time on average.
For the R-GCN and R-GAT models, the time-consuming FP stage greatly benefits from the bound-aware stage fusion, which results in up to 50\% improvements in compute utilization and performance. However, for HAN, the NA stage dominates the entire processing time, making the bound-aware stage fusion a limited success.

% Figure environment removed


% \todo{Focus on multi-lane design and semantic graph parallel execution.
% %随着Lane的数目上升，并行执行的语义图个数更多，
% %1）性能实现了线性上升；2）部件利用率和带宽得到提升；3）负载均衡有效果
% }

\textbf{Effect of Independency-aware Parallel Execution.}
Fig.~\ref{fig:IPD_benefit} illustrates the scalability of HiHGNN and the effect of workload-aware scheduling. 
Fig.~\ref{fig:IPD_benefit} (a) depicts an approximately linear improvement in performance and compute utilization with the increasing number of lanes.
This is because the workload-aware scheduling efficiently enables multiple lanes to work together on a single semantic graph, ensuring the scalability of the entire architecture.
Fig.~\ref{fig:IPD_benefit} (b) gives an ablation study on the impact of workload-aware scheduling. The results demonstrate that this scheduling leads to a gradual improvement in compute utilization and performance with the increasing number of lanes.
Noticed, for the HAN model, the total workload consists of three semantic graphs. Therefore, when the number of lanes scales up to four, this scheduling allows for an additional idle lane to be utilized for processing, resulting in a significant improvement.

% Figure environment removed


% \todo{Speedup + Data Access Volume:
% %凸显 1）Speedup, 2）数据访存量下降
% }



\textbf{Effect of Similarity-aware Execution Scheduling.}
%Building upon the RAB has already achieved a reduction of the computation of projected features and coefficients by over 95\%, 
Fig.~\ref{fig:SES_benefit} (a) and (b) show the speedup and DRAM data access in the four-lane architecture of HiHGNN with the similarity-aware execution scheduling, respectively, compared to the random scheduling case. Note that the horizontal axis represents the ratio between the size of total projected vertex features and the size of the FP-Buf.

In detail, when the number of semantic graphs is four, the scheduling has limited impact on performance and DRAM data access because of the four-lane architecture. However, as the number of semantic graphs increases to eight and twelve, the scheduling demonstrates better performance improvement and more reduction of DRAM access. This improvement is attributed to the effective reuse of projected features from previous semantic graph by the subsequent one. Furthermore, the prepossessing overhead of this scheduling on CPU is negligible. For instance, on the DBLP dataset, it accounts for less than 0.1\% of the end-to-end execution time.

% \revise{
% Though the RAB has dramatically reduced the computation of projected features and coefficients by over 95\%.
% }
% Fig.~\ref{fig:SES_benefit} (a) and (b) respectively show the speedup and DRAM data access in the four-lane architecture of HiHGNN with the similarity-aware execution scheduling, compared to the random scheduling case. Note that the horizontal axis represents the ratio between the size of total projected vertex features and the size of the FP-Buf.
% %
% When the number of semantic graphs is four, the effect of this scheduling on performance and DRAM data access is limited. As the number of semantic graphs increases to eight and twelve, the effect of the scheduling exhibits a better performance improvement and a greater reduction in DRAM accesses. This is benefited from the efficient reuse of the projected features across all semantic graphs.
% \revise{What is more, the prepossessing overhead of this scheduling on CPU is negligible. For example, it is less than 0.1\% of end-to-end execution time for DBLP dataset.}




% Figure environment removed



\subsection{Results on FPGA Implementation} 

% Figure environment removed

To further validate the feasibility of our design, we implement and evaluate a small single-lane design of HiHGNN on the Xilinx FPGA Alveo U50 accelerator card using Xilinx Vitis Toolchains. Note that due to the limited arithmetic and bandwidth resources on FPGA board, our FPGA prototype implementation is only used to validate the implementability of the architecture and the effectiveness of the strategy.

Fig.~\ref{fig:FPGA} (a) and (b) respectively present the implementation layout and utilization ratios of various types of resources of FPGA. 
%Since lane-level parallelism is not available, semantic graphs are consecutively streamed into the accelerator. 
Fig.~\ref{fig:FPGA} (c) shows that HiHGNN achieves an average speedup of 1.23$\times$ among various datasets with the adoption of the similarity-aware execution scheduling. This result is consistent with the result in the previous subsection. 


