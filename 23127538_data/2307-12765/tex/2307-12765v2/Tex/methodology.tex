\section{Experimental Methodology}\label{sec:evaluation_methodology}
\textbf{Methodology.} The performance and energy of HiHGNN are evaluated using the following tools.

\textit{Cycle-accurate Simulator.} 
For the performance evaluation, a customized cycle-accurate simulator is designed and implemented to measure execution time in the number of cycles. This simulator models the microarchitectural behavior of each hardware module of HiHGNN. In addition, a detailed cycle-accurate on-chip memory model is implemented and integrated. This is also integrated with Ramulator~\cite{ramulator}, a cycle-accurate DRAM simulator, to simulate the cycle-accurate behavior of DRAM accesses to HBM.

\textit{CAD Tools.} 
For area, power, and critical path delay measurements, we implement an RTL version of each hardware module and synthesize it. We use the Synopsys Design Compiler with the TSMC 12 $nm$ standard VT library for the synthesis and estimate the power consumption using Synopsys PrimeTime PX. The slowest module has a critical path delay of 0.83 $ns$ including setup and hold time, putting HiHGNN comfortably at the 1 GHz clock frequency. 

\textit{Memory Measurements.} 
The access latency, energy, and area of the on-chip buffers are estimated using Cacti 6.5~\cite{CACTI}. We use four different scaling factors to convert them to 12 $nm$ technology, as depicted in work~\cite{technology_scale,ozdal_energy_2016} since Cacti only supports down to 32 $nm$ technology. 
The access latency and energy of HBM1.0 are simulated by Ramulator and estimated with 7 pJ/bit as in the work~\cite{7pj}, respectively.

\begin{table}[!t]
\vspace{-5pt}
\centering
\caption{Information of HetG datasets.} \label{tab:datasets}
\renewcommand\arraystretch{1.2}
\setlength\tabcolsep{2pt}%	
\resizebox{0.49\textwidth}{!}{
\begin{tabular}{ccccc}
\toprule
\textbf{Dataset} &
  \textbf{\#Vertex} &
  \textbf{\#Feature} &
  \textbf{\#Edge of Each Relation} &
  \textbf{Metapath} \\ \midrule
\multirow{4}{*}{IMDB} & movie (M): 4932 & M: 3489  & 
\  \multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}AM: 14779 MA: 14779\\ KM: 23610 MK: 23610\\ DM: 4932 MD: 4932\end{tabular}} &
\multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}{MDM}\\ {MAM}\\ {MKM}\end{tabular}}\\
 & director (D): 2393 & D: 3341 &     &  \\
 & actor (A): 6124    & A: 3341 &     &  \\
 & keyword (K): 7971  & K: --- &     &  \\ \midrule
\multirow{4}{*}{ACM} &
  paper (P): 3025 &
  P: 1902 &
  \multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}TP: 255619 PT: 255619\\ SP: 3025 PS: 3025\\ PP: 5343 -PP: 5343 \\ AP: 9949 PA: 9949\end{tabular}} &
  \multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}{PPSP}\\ {PSP}\\ {PPAP}\\ {PAP}\end{tabular}} \\
 & author (A): 5959   & A: 1902 &        &  \\
 & subject (S): 56   & S: 1902 &        &  \\
 & term (T): 1902     & T: --- &        &  \\ \midrule
\multirow{4}{*}{DBLP} & author (A): 4057 & A: 334 & 
\multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}AP: 19645 PA: 19645\\ VP: 14328 PV: 14328\\ TP: 85810 PT: 85810\end{tabular}} &
\multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}{APA}\\ {APTPA}\\ {APCPA}\end{tabular}} \\
 & paper (P): 14328   & P: 4231 &        &  \\
 & term (T): 7723     & T: 50   &        &  \\
 & venue (V): 20      & V: --- &        &  \\ \bottomrule
\end{tabular}
}
\scriptsize
%\vspace{-5pt}
\end{table}

\textbf{Benchmark Datasets and HGNN Models.} 
The benchmark datasets that we used are listed in Table~\ref{tab:datasets}.
The raw features of each vertex are provided by the dataset itself.
The scale of heterogeneous graph datasets differs from that of homogeneous graph datasets in terms of calculation methods. The size of homogeneous graph datasets is determined by the number of edges and vertices they exhibit, while the size of heterogeneous graph datasets is determined by the construction of semantic graphs by the metapaths. For instance, in the experiment, the three datasets include 92,760, 82,384, and 133,910 vertices, and 630,542, 6,309,372, and 24,669,366 edges, respectively.
Four popular HGNN models are used, including HAN~\cite{HAN}, R-GCN~\cite{R-GCN}, R-GAT~\cite{R-GAT}, and S-HGN~\cite{Simple-HGN}, which are widely adopted in the evaluation of algorithm community~\cite{Simple-HGN,HGL,hgnn_survey_hanjiawei}.

%For the computation precision, we use the 32-bit integer which is enough to maintain the accuracy of HGNN inference.

%\todo{
%在论文中找机会描述一下语义图的大小，凸显数据集处理过程中很大
%加一些引用，证明这些模型被大量用来对比和评估性能。
%需要一个Table或者描述，把模型的配置说清楚
%}

\textbf{Baseline Platforms.} To compare the performance of HiHGNN to the state-of-the-art work, all HGNN models are implemented using a state-of-the-art framework DGL 1.0.2 \cite{DGL} and evaluated on an NVIDIA GPU T4 and an NVIDIA GPU A100, using the NVIDIA Nsight Compute. All models are implemented with the same number of hidden units \{64\} and layers are \{1, 3, 3, 2\} for \{HAN, R-GAT, R-GCN, S-HGN\}, respectively.
In addition, we implement the above HGNN models in HiHGNN. All models are implemented with the same number of hidden units and layers used in GPUs. Table \ref{tb:platform} lists the configurations for the above implementations. 

It should be noted that utilizing our proposed optimizations in GPU platform may result in the loss of hardware-optimized benefits on GPUs, which could outweigh the potential gains. This is because: i) GPUs accelerate HGNNs using coarse-grained hardware-optimized operations (i.e., SpMM) which are sophisticatedly optimized in hardware level; ii) Our optimizations are based on decoupling coarse-grained operations into finer-grained ones.


\begin{table}[!t]
%\vspace{-10pt}
\centering
\caption{Platforms for HiHGNN and baselines.} \label{tb:platform}
\renewcommand\arraystretch{1.2}
\setlength\tabcolsep{2pt}%	
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{ccccll}
\toprule
 & \textbf{T4} & \textbf{A100}   & \multicolumn{3}{c}{\textbf{HiHGNN (4 Lanes)}}                                                                                                 \\ \midrule
\textbf{\begin{tabular}[c]{@{}c@{}}Peak\\ Performance\end{tabular}}  & \begin{tabular}[c]{@{}c@{}} 8.1 TFLOPS, \\  1.59 GHz \end{tabular}                 & \begin{tabular}[c]{@{}c@{}} 19.5 TFLOPS, \\  1.41 GHz \end{tabular}                & \multicolumn{3}{c}{\begin{tabular}[c]{@{}c@{}} 16.38 TOPS, \\  1.0 GHz \end{tabular}} \\ \midrule
\textbf{\begin{tabular}[c]{@{}c@{}}On-chip\\ Buffer\end{tabular}}  & \begin{tabular}[c]{@{}c@{}}L1 Cache 1.28 MB,\\ L2 Cache 4 MB \end{tabular} & \begin{tabular}[c]{@{}c@{}}L1 Cache 20 MB,\\ L2 Cache 40 MB \end{tabular} & \multicolumn{3}{c}{\begin{tabular}[c]{@{}c@{}}2.44 MB (FP-Buf),\\14.52 MB (NA-Buf),\\0.12 MB (SA-Buf),\\ 0.38 MB (Att-Buf)\end{tabular}} \\ \midrule
\textbf{\begin{tabular}[c]{@{}c@{}}Off-chip\\ Memory\end{tabular}} & \begin{tabular}[c]{@{}c@{}} 300 GB/s, \\ GDDR6\end{tabular}                                & \begin{tabular}[c]{@{}c@{}}1935 GB/s, \\ HBM2e\end{tabular}                                & \multicolumn{3}{c}{\begin{tabular}[c]{@{}c@{}}512 GB/s, \\ HBM1.0\end{tabular}}                                                                                                                            \\ \bottomrule
\end{tabular}
}
\scriptsize
\vspace{0.5pt}
\par Note: The compute units include 96 systolic arrays (each with 8$\times$8 MACs) and 128 8-way SIMD cores for each lane.
%Note: The GPU data in the table are all sourced from Nvidia's official documentation. 
\vspace{-5pt}
\end{table}

