
\section{Architecture Design}
% \todo{
%%%%注意：重点突出语义图之间，语义图内的肯定被GAT加速器利用了，容易导致创新性冲突
%%%%捋逻辑的重点是把关键要素给弄出来：标题、首小段、段落首句、图标题和内容、表标题和内容
%%%%缺乏的能力：凝练/拔高创新点；图表的清晰表现力；专业术语。
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%section标题下的描述应就是\subsection{Architecture Overview}
%%本章节先介绍architecture的概要，让大家对我们的架构设计和创新有个整体认识。然后，介绍用于挖掘语义图内和语义图间并行性）（需要在introduction章节给并行性起名字，Intra-semantic-graph Parallelism，Inter-semantic-graph Parallelism）的技术，其中包含xxx (Secection.xxx）。目的是让大家深入了解挖掘并行性的创新技术。最后，介绍用于挖掘语义图内和语义图间数据复用性（需要在introduction章节给并行性起名字，Intra-semantic-graph Data Reusablity，Inter-semantic-graph Data Reusablity）的技术，其中包含xxx (Secection.xxx）。目的是让大家深入了解挖掘数据复用性的创新技术。
%%%\subsection{Parallelism Exploitation: Bound-aware \\Stage Assembly and Dependency-aware \\Parallelism Disassembly}\label{sec:parallel_exploitation}
%%%
%%%\subsubsection{Parallel Programming Model}
% }
This section presents HiHGNN, a high-performance HGNN accelerator designed to exploit both the high-degree intra- and inter-semantic-graph parallelism and data reusability. We begin by introducing a stage-fusion programming model and a novel hardware datapath coordinating with it in Section \ref{sec:stage_fusion} that effectively enables the fusion and pipeline of stages with different execution bounds. Next, we design an independency-aware parallel execution for the inter-semantic-parallelism exploitation, which involves the scale-up and workload balancing optimizations in Section \ref{sec:parallel_exploitation}. Finally, we propose a similarity-aware execution scheduling to maximize the reuse of intermediate results across the processing of semantic graphs in Section \ref{sec:data_reusability}.

% \begin{algorithm}[!t]
%     \SetAlgoLined
%     % \caption{\textbf{Bound-aware Stage Fusion Algorithm}}
%     \caption{\textbf{Stage-Fusion Programming Model}}
%     \footnotesize
%     \label{alg:Programming Model}
%     \textbf{Input:} semantic graphs of G = \{$G^{\mathcal{P}_1}$, $G^{\mathcal{P}_2}$ ... $G^{\mathcal{P}_n}$\};\\
%     \textbf{Output:} the embedding $h_v$ of each vertex $v$.
    
%     \For{semantic graph $G^{\mathcal{P}}$ $\in$ G}{
%         \centerline{{\underline{\color{blue} $\triangleleft \quad \textbf{FP Stage}$}}}
%                 \For{each vertex v in FP\_task\_list}{
%                     $h_v'$=Feature\_Projection($W^{c_v}$, $h_v$)\;
%                     \eIf{v is source vertex}{
%                         $\theta_{*,v}^{\mathcal{P}}$=CCoefNA(${a_{\mathcal{P}}^T}_2$, $h_v'$)\;
%                     }
%                     {
%                         $\theta_{v,*}^{\mathcal{P}}$=CCoefNA(${a_{\mathcal{P}}^T}_1$, $h_v'$)\;
%                     }
%                 }
%         \centerline{{\underline{\color{blue} $\triangleleft \quad \textbf{NA Stage}$}}}
%                 \For{$e_{u,v}$ $\in$ $E^{\mathcal{P}}$}{
%                     % \tcp{Edge-Centric Scheduling}
%                     \eIf{$x_v$ and $x_u$ is projected}{
%                         $\alpha_{u,v}^{\mathcal{P}}$=CImpNA($\theta_{v,*}^{\mathcal{P}}$, $\theta_{*,u}^{\mathcal{P}}$) \;
%                         $\alpha_{v}^{\mathcal{P}}$$\gets$Accumulate($\alpha_{u,v}^{\mathcal{P}}$)\;
%                         $z_v^{\mathcal{P}}$$\gets$Aggregate($\alpha_{u,v}^{\mathcal{P}}$, $h_u'$)\;
%                         \If{all neighbors of $v$ have finished NA}{
%                             Send\_to\_LSF\_task\_list($v$);
%                         }
%                     }
%                     {
%                         Send\_to\_FP\_task\_list($v$,$u$);
%                     }
%                 }
%         \centerline{{\underline{\color{blue} $\triangleleft \quad \textbf{Local SF Stage}$}}}
%             \For{each vertex v in LSF\_task\_list}{
%                 % $z_v^{\mathcal{P}}$$\gets$EW-DIV($\alpha_{v}^{\mathcal{P}}$)\;
%                 % \tcp{NA and SA Fusion}
%                 $w_v^{\mathcal{P}}$=CImpLSF($z_v^{\mathcal{P}}$, $\alpha_{v}^{\mathcal{P}}$)\;
%                 $w_{\mathcal{P}}$$\gets$Accumulate($w_v^{\mathcal{P}}$)\;
%                 \If{all vertex in $G^{\mathcal{P}}$ have finished LSF}{
%                     Send\_to\_GSF\_task\_list($w_{\mathcal{P}}$);
%                 }
%             }
%         \centerline{{\underline{\color{blue} $\triangleleft \quad \textbf{Global SF Stage}$}}}
%             \For{each $w_{\mathcal{P}}$ in GSF\_task\_list}{
%                 % $w_{\mathcal{P}}$=$w_{\mathcal{P}}$/ $\lvert V^{\mathcal{P}} \rvert$\;
%                 $\beta_\mathcal{P}$=CImpGSF($w_{\mathcal{P}}$)\;
%                 $\beta_G$$\gets$Accumulate($\beta_\mathcal{P}$)\;
%                 \For{vertex v $\in$ $V^{\mathcal{P}}$}{
%                     $z_v$$\gets$Aggregate($\beta_{\mathcal{P}}$, $z_v^{\mathcal{P}}$)
%                 }
%             }
%     }
%     \centerline{{\underline{\color{blue} $\triangleleft \quad \textbf{Final Stage}$}}}
%     \For{vertex v $\in$ $V$}{
%         {$h_v$}{$\gets$}EW-DIV($z_v$,\,$\beta_G$)\;
%     }
%     % \vspace{-15pt}
% \end{algorithm}
%\vspace{-15pt}

\begin{algorithm}[!t]
    \SetAlgoLined
    % \caption{\textbf{Bound-aware Stage Fusion Algorithm}}
    \caption{\textbf{Stage-fusion Programming Model}}
    \footnotesize
    \label{alg:Programming Model}
    \textbf{Input:} semantic graphs of G = \{$G^{\mathcal{P}_1}$, $G^{\mathcal{P}_2}$ ... $G^{\mathcal{P}_n}$\};\\
    \textbf{Output:} the embedding $h_v$ of each vertex $v$;\\
    \textbf{Initial:} all task lists are empty. 
    
    \For{semantic graph $G^{\mathcal{P}}$ $\in$ G}{
        NA\_task\_list=$E^{\mathcal{P}}$;

        % \For{each task list \textbf{in parallel}}{
            \centerline{{\underline{\color{blue} $\triangleleft \quad \textbf{FP Stage}$}}}
                    \For{each vertex v in FP\_task\_list}{
                        $h_v'$=Feature\_Projection($W^{c_v}$, $h_v$)\;
                        $\theta_v^{\mathcal{P}}$=Compute\_Coefficient(${a_{\mathcal{P}}^T}$, $h_v'$);
                    }
            \centerline{{\underline{\color{blue} $\triangleleft \quad \textbf{NA Stage}$}}}
                    \For{each edge $e_{u,v}$ in NA\_task\_list}{
                        % \tcp{Edge-Centric Scheduling}
                        \eIf{$x_v$ and $x_u$ is projected}{
                            $z_v^{\mathcal{P}}$$\gets$Aggregate($\theta_u^{\mathcal{P}}$, $\theta_v^{\mathcal{P}}$, $h_u'$)\;
                            \If{all neighbors of $v$ have finished NA}{
                                Send\_to\_LSF\_task\_list($v$);
                            }
                        }
                        {
                            Send\_to\_FP\_task\_list($u$,$v$);
                        }
                    }
            \centerline{{\underline{\color{blue} $\triangleleft \quad \textbf{Local SF Stage}$}}}
                \For{each vertex v in LSF\_task\_list}{
                    $w_{\mathcal{P}}$$\gets$Compute\_and\_Accumulate($q^{T}$, $z_v^{\mathcal{P}}$)\;
                    \If{all vertex in $G^{\mathcal{P}}$ have finished LSF}{
                        Send\_to\_GSF\_task\_list($w_{\mathcal{P}}$);
                    }
                }
            \centerline{{\underline{\color{blue} $\triangleleft \quad \textbf{Global SF Stage}$}}}
                \For{each $w_{\mathcal{P}}$ in GSF\_task\_list}{
                    $\beta_G$$\gets$Compute\_and\_Accumulate($w_{\mathcal{P}}$)\;
                    \For{vertex v $\in$ $V^{\mathcal{P}}$}{
                        $z_v$$\gets$Aggregate($w_{\mathcal{P}}$, $z_v^{\mathcal{P}}$)
                    }
                }
        % }
    }
    \centerline{{\underline{\color{blue} $\triangleleft \quad \textbf{Final Stage}$}}}
    \For{vertex v $\in$ $V$}{
        {$h_v$}{$\gets$}EW-DIV($z_v$,\,$\beta_G$)\;
    }
    % \vspace{-15pt}
\end{algorithm}

% \subsection{Parallelism Exploitation: Bound-aware \\Stage Assembly and Dependency-aware \\Parallelism Disassembly}\label{sec:parallel_exploitation}
\subsection{Bound-aware Stage Fusion}\label{sec:stage_fusion}
As mentioned in Section \ref{sec:characterization}, different stages of HGNN exhibit diverse execution bounds, leading to unbalanced utilization across different hardware components and limited performance. Although previous efforts~\cite{DNNBuilder,HyGCN, GRIP, GCoD} have employed stage-fusion techniques to achieve better parallelism in traditional workloads, they lack awareness of execution bounds and fail to match the workflow of HGNNs. This inspires us to propose a bound-aware stage fusion methodology for HGNN acceleration, efficiently improving hardware utilization and exploiting inter-stage parallelism.

\subsubsection{Stage-fusion Programming Model} \label{sec:programming_model}
To fully utilize hardware resources and implement pipeline execution, we propose a novel programming model in which the original execution stages are decomposed and reorganized, allowing the fusion and pipeline of execution for stages with different execution bounds. This model is illustrated in Algorithm~\ref{alg:Programming Model}. Note that all the functions included are user-definable, maintaining good programming flexibility and adaptability for various HGNN models. All stages except for the final stage can be executed in parallel as long as their corresponding task lists are non-empty. Overall, this differs significantly from the sequential execution of stages shown in Algorithm~\ref{alg:Original_Programming_Model}.

We have made the following modifications and optimizations compared to the original programming model.
\blackcircledempty{1}
To achieve stage fusion, we break the barrier that separates the NA stage from the FP stage in the original programming model.
The original NA stage is split into two steps, i.e., the computation of attention coefficients (lines 8) and the rest of the process (line 12).
Then we integrate the former step with the FP stage, enabling directly forwarding the projected features for the computation of attention coefficients without waiting for all vertex features to be projected.
\blackcircledempty{2}
We remove the barrier between the NA and SF stages in most HGNNs.
The barrier exists because the HGNNs require all results from different semantic graphs to generate the semantic attention importance in the NA stage.
So, we decompose the SF stage into two stages, namely the Local SF (LSF) stage and the Global SF (GSF) stage. For a given semantic graph $G^{\mathcal{P}}$, the former mainly involves the intermediate computation of the semantic attention importance (line 21), which is executed once after all the neighbors of a target vertex in $G^{\mathcal{P}}$ are aggregated. 
While the latter generates the ultimate semantic attention importance of $G^{\mathcal{P}}$ (line 27) and performs the semantic aggregation of all the target vertices (lines 28-30). It is executed once for each semantic graph when all the target vertices have accomplished their neighbor aggregation processes.
In this way, the LSF stage can be fused into the execution of the NA stage (lines 13-15).

Noticed that in the stage-fusion programming model, we implement the fine-grained parallelism between the FP stage and the NA stage. At this point, if the FP stage is still being used for driving like in the original programming model, then every time a vertex is projected in the FP stage, the NA stage has to search for its neighbors in the edge list or adjacency matrix, which triggers a large number of random accesses and degrades the performance. To reduce the random accesses, we put all edges in the NA\_task\_list, which eliminates the random access for neighbor search. In this way, although the access to the feature becomes random, the feature itself has a larger dimension, thus the impact on overall bandwidth utilization is relatively small.



% Figure environment removed

% Figure environment removed

Fig.~\ref{fig:programming_model} illustrates the difference in the execution flow between the traditional one (Fig.~\ref{fig:programming_model} (a)) and our programming model (Fig.~\ref{fig:programming_model} (b)). By decomposing and fusing the execution flow, we can combine the execution of the compute-bound FP stage and the memory-bound NA stage. Besides, we can also fuse the compute-bound LSF stage with its preceding memory-bound NA stage to improve the utilization of compute and bandwidth resources simultaneously. Additionally, we pipeline the execution of each stage, enabling the different stages to run in parallel, thus resulting in performance improvement.


% \textbf{Dependency-aware Parallelism Disassembly.} \todo{
% %一句话说为什么这么做和key idea：为了利用更多执行部件挖掘细粒度的并行性，我们将Super-stage的粗粒度并行性进行感知依赖的细粒度拆解。
% %然后围绕着编程模型的修改，从执行流程上重点描述一下怎么拆解粗粒度并行性和挖掘细粒度并行性。可以通过一个图抽象编程模型的改变，从而更形象表征并行性的拆解和挖掘。
% %最好在2段话内描述完
% }

% \todo{
%%%一句话说一下这个hardware Datapath与Programing model关系，以及作用：In conjunction with the proposed programming model, we implement several hardware components to constitute hardware datapath in microarchitecture.
%%用架构图介绍hardware componentes（2段话）。
%%用datapath图介绍Datapath，重点体现出stage fusion的实现，以及细粒度并行性的挖掘（3段话）。
% }

\subsubsection{Implementation of Hardware Datapath}\label{sec:Hardware_datapath}
In conjunction with the proposed programming model, we implement several hardware components to constitute to hardware datapath in microarchitecture. We employ a hybrid architecture to minimize the design complexity of hardware components, as in work~\cite{TPU,HyGCN,SGCN,ReGNN}. This enables us to facilitate data path optimization and leverage parallelism as well as data reusability more effectively.

% \revise{The challenge lies in the complex data dependencies, unavoidable control overhead, and the requirement for more efficient storage management introduced by the more intricate execution flow of HGNN.}

% \todo{Supplement why we use hybrid architecture. Find the answer in rebuttal.}

\textbf{Hardware Components.} 
Throughout the execution of the HGNN models, computational workloads are mainly occupied by matrix-vector multiplication (MVM) and element-wise (EW) operations. To boost their execution efficiency, we design a dedicated module for each of them as depicted in Fig.~\ref{fig:hardware_components}. First, a flexible \textit{Systolic Module} is established for the MVM operations, which is based on the well-known systolic array design \cite{TPU}. Two execution modes, one for fine-grained matrix multiplication and the other for larger-scale matrix operation, are employed as suggested in work \cite{HyGCN}. Second, EW operations running through the whole processing of HGNNs are performed by the \textit{SIMD Module}. In addition, the \textit{Activation Module} is built to perform non-linear functions like \textit{LeakyRelu}, \textit{Elu}, and \textit{Softmax}. To coordinate workloads across various hardware components, we introduce a centralized \textit{Scheduler} equipped with a redundancy-aware bitmap (RAB) which is built by a set of two-port SRAM banks to support the reuse of intermediate results. The RAB will be further explained in Section~\ref{sec:basic_implementation_of_data_reuse}.

The high-bandwidth memory (HBM) stores information about the original semantic graphs, mainly including the adjacent information in the compressed sparse column (CSC) format and raw features stored continuously according to vertex categories. \textit{Feature Projection Buffer} (FP-Buf) and \textit{Neighbor Aggregation Buffer} (NA-Buf) aim to cache the projected features and the intermediate neighbor aggregated features respectively for data reuse. Similar to NA-Buf, \textit{Semantic Fusion Buffer} (SF-Buf) is used to hold the features that are aggregated from multiple semantic graphs for vertices. During the computation process of the attention mechanism, a series of parameters as well as intermediate results, are cached in the \textit{Attention Buffer} (Att-Buf). The \textit{Memory Access Controller} is used to schedule the data interactions between the on-chip buffers and HBM.

% With the support of these basic hardware components, diverse operations can be executed in a parallel manner, providing the hardware foundation for mining the parallelism in the execution process of the HGNN models. 


\textbf{Hardware Datapath.} The hardware datapath of our proposed stage-fusion programming model is built from the fundamental hardware components we stated above and represents the dataflow during execution. 

The main challenge in building the hardware datapath is to remove the pipeline stall. 
On one hand, the data access conflicts caused by fine-grained parallelism with edge-centric granularity increase the access latency, and on the other hand, the accumulation of \textit{Softmax} functions on the denominator also leads to pipeline stalls. For the former we can utilize multi-port SRAM to realize on-chip buffers, while for the latter we need to further decompose the execution flow of \textit{Softmax}. Take the weighted aggregation of NA stage as an example, the decomposition is as follows: 

%We define the stages outlined in Algorithm~\ref{alg:Programming Model} as coarse-grained execution stages, where each stage has a specific execution bound.
%
%\add{Pipeline stalling in hardware datapath is the main challenge we confront when further partition coarse-grained execution stages into fine-grained pipelines. On one hand, the data access conflicts caused by fine-grained parallelism with edge-centric granularity increase the access latency, and on the other hand, the accumulation of \textit{Softmax} functions on the denominator also leads to pipeline stalling. For the former we can utilize multi-port SRAM to realize on-chip buffers, while for the latter we need to further decompose the execution flow of \textit{Softmax}.} \revise{Take the weighted aggregation of NA stage as an example, the decomposition is as follows:

\begin{equation*}
\setlength{\abovedisplayskip}{2pt}
\setlength{\belowdisplayskip}{2pt}
    \begin{aligned}  \label{eq:softmax decompose}
        \displaystyle \sum_{u\in \mathcal{N}_v^{\mathcal{P}}} \alpha_{u,v}^{\mathcal{P}} \cdot h_u^{'}=\frac{\displaystyle \sum_{u\in \mathcal{N}_v^{\mathcal{P}}} exp(\theta_{u,v}^{\mathcal{P}}) \cdot h_u^{'}}{\displaystyle \sum_{k\in \mathcal{N}_v^{\mathcal{P}}} exp(\theta_{k,v}^{\mathcal{P}})}
    \end{aligned}
\end{equation*}

The weighted aggregation in the GSF stage also employs a similar approach. An example is illustrated in Fig.~\ref{fig:softmax_decompose}. Once the numerator is computed, it can be immediately used for aggregation and accumulated onto the denominator, avoiding pipeline stalls caused by waiting for the computation of \textit{Softmax} denominator.


% Figure environment removed

% \begin{equation}
%     \begin{aligned}
%         \label{eq5:normalization fuse}
%         \displaystyle \sum_{u\in \mathcal{N}_v^{\Phi}} \alpha_{u,v}^{\mathcal{P}} \cdot h_u^{'}=\displaystyle \sum_{u\in \mathcal{N}_v^{\Phi}} \frac{exp(\theta_{u,v}^{\mathcal{P}})}{\displaystyle \sum_{k\in \mathcal{N}_v^{\mathcal{P}}} exp(\theta_{v,k}^{\mathcal{P}})} \cdot h_u^{'}=\frac{\displaystyle \sum_{u\in \mathcal{N}_v^{\mathcal{P}}} exp(\theta_{u,v}^{\mathcal{P}}) \cdot h_u^{'}}{\displaystyle \sum_{k\in \mathcal{N}_v^{\mathcal{P}}} exp(\theta_{v,k}^{\mathcal{P}})}
%     \end{aligned}
% \end{equation}

% Figure environment removed


The resulting datapath is depicted in Fig.~\ref{fig:datapath}. For each edge $e_{u,v}$, the raw features of the source vertex $u$ and the target vertex $v$ are projected into the same vector space through a linear transformation function performed by the \textit{Systolic Module} (indicated by SYST in Fig.~\ref{fig:datapath}). A bitmap-based data reuse mechanism is employed here to remove computation redundancy. Since $h_u'$ and $h_v'$ are globally reusable while $\theta_{u,*}^{\mathcal{P}}$ and $\theta_{*,v}^{\mathcal{P}}$ ($\theta^{\mathcal{P}}_{u, *}$ for source vertex and $\theta^{\mathcal{P}}_{*, v}$ for target vertex) are only reusable within the same semantic graph, a total of three cases arise for the reuse mechanism, as indicated by the different colored lines in Fig.~\ref{fig:datapath}. More details are given in Section~\ref{sec:basic_implementation_of_data_reuse}. 

Unlike the traditional HGNN execution process, after obtaining $h'_{u}$ and $h'_{v}$, instead of writing them back and keeping them until aggregation in the NA stage, they are immediately sent to the \textit{Systolic Module} for the computation of $\theta_{u,*}^{\mathcal{P}}$ and $\theta_{*,v}^{\mathcal{P}}$ in NA stage, and the weighted aggregation operation from $u$ to $v$ is completed by the \textit{SIMD Module} afterward. 

For each target vertex $v$ in $G^{\mathcal{P}}$, once all the features of neighbors have been aggregated by the \textit{SIMD Module}, a partial computation of the semantic importance $w_{\mathcal{P}}$ contributed by $v$ denoted as $w_v^{\mathcal{P}}$ is carried out using the \textit{Systolic Module} instantly. Meanwhile, the value is accumulated in the ultimate result of $w_{\mathcal{P}}$. When all vertices in $G^{\mathcal{P}}$ finish the FP, NA, and LSF stages, the GSF stage first computes the ultimate semantic importance $w_{\mathcal{P}}$ of $G^{\mathcal{P}}$ by dividing it with the total number of vertices $V^{\mathcal{P}}$ and then aggregates the semantic features of all vertices in $G^{\mathcal{P}}$, which is similar to NA and is performed on \textit{SIMD Module}. Following the completion of all the aforementioned stages, the final embeddings $h$ are produced by dividing target vertices' features by an accumulated global weight $\beta_G$.




% By decomposing the centralized workload of the NA stage in the original programming model and distributing it into the FP stage for parallel execution, the huge demand for memory bandwidth in the NA stage is relieved in a stage fusion manner, which results in enhanced utilization of both compute and bandwidth resources as well as improved performance.

%\candelete{Note that in the FP stage, computation is mainly carried out by the \textit{Systolic Module}, whereas in the NA stage, the \textit{SIMD Module} is predominantly utilized. Regarding the SF stage, as described in Section \ref{sec:programming_model}, we have separated the compute-bound computation process for attention importance, i.e., LSF from the original model's SF stage, which mainly employs the \textit{Systolic Module}. In the GSF stage, the \textit{SIMD Module} is used for computation as it performs the actual semantic aggregation for each target vertex.}

% Figure environment removed

\subsection{Parallelism Exploitation: Independency-aware Parallel Execution}\label{sec:parallel_exploitation}
% \subsection{Independency-aware Parallelism Exploitation}\label{sec:parallel_exploitation}
In this section, we propose the independency-aware parallel execution to exploit the inter-semantic-graph parallelism by leveraging the independency among semantic graphs.

\subsubsection{Scale-up Optimization for Basic Architecture} \label{sec:multi-lane}
% \todo{
%%%一句话说一下为什么要优化Datapath： To exploit the parallelism between semantic graphs, we propose a scale-up optimization for hardware datapath.
%%具体说明怎么优化，其中首句说明优化核心（1段话）。
%%多个语义图被多个Datapath的并行执行例子（1段话）。
%%总结说明，通过针对并行编程模型的Hardware Datapath实现，以及对Hardware Datapath的scale-up优化，不仅能挖掘xxx并行，还能挖掘xxx并行性，大大提高部件利用率的同时，还能利用更多的部件提高性能（1段简单话）。
% }
To dig into the parallelism across semantic graphs, we provide the scale-up optimization for the above architecture. % based on the basic design.



We first give the motivation to exploit inter-semantic-graph parallelism. First, the processing of each semantic graph in the NA stage is independent.
Second, the NA stage dominates the total execution time of HGNNs, as demonstrated in Section~\ref{sec:motivation}.
% Third, Fig.~\ref{fig:relative_time} (a) and (b) demonstrate that the execution time of total execution and NA stage both increase as the number of semantic graphs increases. 
Third, the execution time of total execution and the NA stage both increase as the number of semantic graphs increases, as demonstrated in Fig.~\ref{fig:relative_time} (a) and (b). 
Therefore, it is vital to exploit the parallelism across semantic graphs.


To this end, a scale-up optimization for the basic architecture as shown in Fig.~\ref{fig:multi-lane} (a) is presented. 
%The basis for scaling up is the extension of the lanes, which consists of \textit{SIMD Module} and the corresponding buffers. Each lane independently processes a semantic graph.
The basis for scaling up is the extension of the lanes and each lane independently processes a semantic graph.
For simplicity, we refer to the basic design of HiHGNN as a single-lane architecture, while the scale-up design of HiHGNN is called a multi-lane architecture.
%
In the multi-lane architecture, the hardware components are generally the same as those in the single-lane one. The \textit{Systolic Module} is mainly responsible for MVM operations, while the \textit{SIMD Module} handles EW operations. To schedule the execution, we replace the original \textit{Scheduler} with \textit{Global Scheduler} which schedules the lane group with other modules. A scheduler named \textit{Local Scheduler} is used to handle the processing within the lane group.
An additional \textit{SIMD Module} outside to execute the GSF stage after synchronizing the results of different lanes.
Besides, to allocate the workload to each lane and achieve dynamic scheduling across them, we leverage crossbar switches with the number of ports being consistent with the number of lanes for data transmission, which is illustrated in detail in the next section.


 % Figure environment removed


\subsubsection{Workload Balance across Hardware Datapaths} \label{sec:workload_balance}
To make full use of compute units, we propose workload-aware scheduling to balance the workload across different lanes. 

With the scale-up design, the multi-lane architecture can leverage both stage-level parallelism and semantic-graph-level parallelism.
However, the workload imbalance across different semantic graphs is severe in real-world datasets. 
For example, the DBLP dataset consists of three semantic graphs, each containing 4057 vertices, but with vastly different numbers of edges, i.e., 7043571, 5000496, and 11113, respectively.
When the three workloads are assigned directly to the $Lane_1$, $Lane_2$, and $Lane_3$. $Lane_1$ and $Lane_2$ become overloaded, while $Lane_3$ is underloaded. 

Additionally, to avoid introducing extra overhead during the execution process, we require a low-latency and stable algorithm to balance the workloads.
Fortunately, unlike the situation in the previous work~\cite{workloadbalance}, there are no computational dependencies between different workloads. This advantageous characteristic allows us to adopt a low-cost strategy to achieve workload balance among lanes.

To fully utilize the compute resources, we propose workload-aware scheduling illustrated in Fig.~\ref{fig:multi-lane} (b) on a four-lane architecture.
We denote the initial workloads received by the global scheduler from three semantic graphs during a specific period as $W_1$, $W_2$, $W_3$, respectively, and the four lanes as $Lane_1$, $Lane_2$, $Lane_3$ and $Lane_4$. The allocation threshold for each lane is set to the maximum number of edges that the lane can process once. In this example, the threshold is set to three.
During the runtime, the \textit{Local Scheduler} first identifies each task list that exceeds the threshold and pushes the excess parts into a task list \textit{Overflow Workload} ($OW$) to prevent any blocking in the execution. 
Once none of the task lists exceeds the threshold, the \textit{Local Scheduler} assigns the task lists to their corresponding lanes, such as $W_1$ to $Lane_1$, except for the $OW$.
Finally, the \textit{Local Scheduler} assigns the workloads in the $OW$ to the lanes that have not reached the threshold. 
This scheduling approach ensures that there is no blocking in each lane and maximizes the utilization of the compute units.

For synchronization of intermediate results, the \textit{Local Scheduler} records the aggregation status of each vertex. When a vertex finishes NA, all lanes that have the partial aggregation results of this vertex send these results to the lane it is originally assigned to in the workloads dispatch.
As an example, in this case, the vertices executed on $L_4$ are transferred to $L_2$ and $L_3$ for the LSF stage. When all vertices within a semantic graph have been finished the LSF stage, the results are sent to an outside \textit{SIMD Module} for the GSF stage and then stored in SF-Buf. 
% \add{Note that workload-aware scheduling method focuses on workload distribution of each batch, and is independent of dataset size.}
% \todo{add stable}

