\section{Related Work}

% \todo{Revise it:
%一段GNN accelerator，最新的工作，根据PC添加，越多越好，但要归纳: stage-fusion(以前的方法如何实现stage-fusion，但是不行) / parallelism exploitation(别人已经挖掘了甚么parallelism，但是没有挖掘inter-semantic graph的parallelism)
%一段software framework for GNN和HGNN (一段话凸显我们采用的DGL平台好)
% }

Due to the outstanding learning capacity of GNNs on graph data, GNN accelerators have drawn great interest from the architecture community~\cite{HyGCN,awbgcn,igcn,GRIP,FlowGNN,EnGN,ReGNN,GROW,SGCN,REFLIP-HUAKE,ReGNN-HUAKE,GCNAX,Hyperscale,FPGAN,NTGAT,FTW-GAT,GCN_Bidirectional_Fusion,MultiGCN}.
%
One part of them~\cite{HyGCN,GCoD,GRIP} employs stage fusion to boost the overall performance. 
For example, \textit{HyGCN}~\cite{HyGCN} first uses two engines to respectively accelerate NA and FP stages, and then leverages inter-stage fusion to improve the overall performance. 
However, the workflow of HGNNs differs from that of GNNs, resulting in a different fusion methodology.
%
Another part of them~\cite{FlowGNN,EnGN,ReGNN} improves performance by exploiting multi-dimensional parallelism. For example, \textit{FlowGNN}~\cite{FlowGNN} uses a scalable dataflow architecture to generally support a wide range of GNN models and exploits multiple-level parallelism in terms of the processing of vertices and edges. 
Unfortunately, previous efforts are ill-suited to exploit the high-degree parallelism and data reusability which are inherent in HGNNs. 
%
In addition, most HGNNs leverage attention mechanisms to improve model accuracy. However, most GNN accelerators focus on graph convolutional network (GCN)~\cite{GCN} and GCN variants, failing to perform attention mechanisms. 
A few other efforts~\cite{NTGAT,FTW-GAT,FPGAN} using parameter quantization and model pruning that sacrifice accuracy to accelerate graph attention networks. Their optimizations are orthogonal to ours.


%GNN accelerators tailored to GNNs gain significant speedup and energy savings compared to GPUs~\cite{HyGCN,awbgcn}. However, they are unable to effectively handle the execution of HGNNs. On one hand, the accelerators designed for  graph convolutional network (GCN)~\cite{GCN} and GCN variants (e.g., GraphSage \cite{GraphSage}) target to the simple workflow of GCNs, failing to perform the attention mechanism~\cite{HyGCN,awbgcn,ReGNN} widely used by HGNNs. In addition, they lack the efficient scheduling of the execution flow and dataflow to handle the execution semantics and patterns of HGNNs. 
%
%On the other hand, existing graph attention network (GAT)~\cite{GAT} accelerators~\cite{NTGAT,FTW-GAT,FPGAN} employ model quantization and dynamic pruning operations to reduce the number of computations, which sacrifices model accuracy for performance improvement. Due to the multi-level aggregation in HGNNs, the model's accuracy loss can be amplified. Meanwhile,  they lack the ability to fully exploit abundant inter-semantic-graph-level parallelism in heterogeneous graphs and opportunities for data reuse during the execution process. 


%\highlight{A DIMM-based near-memory processing HGNNs accelerator, MetaNMP, is recently proposed to greatly reduce the memory footprint and redundant computation~\cite{MetaNMP}. It first uses a cartesian-like product paradigm to generate all metapath instances on the fly to reduce memory footprint. Secondly, it aggregates vertex features along the direction of the metapath instances dispersed from the starting vertex to exploit shareable aggregation computations, reducing the redundant computations. Finally, it introduces a broadcast mechanism for edge data and vertex features to mitigate the inter-DIMM communication.
%MetaNMP mainly focuses on the HGNN model which aggregates vertex features along the metapath in feature projection stage such as MAGNN~\cite{MAGNN}. However, such HGNN models are niche. In addition, based on the DIMM-based near-memory processing, MetaNMP cannot be adopted as a near term solution since this memory technology is still immature. In contrast, our work presents a practical solution by leveraging the off-the-shelf HBM. Furthermore, our work exploits the newfound parallelism and data reusability in HGNNs.}

The first DIMM-based near-memory processing HGNNs accelerator, MetaNMP, is recently proposed, greatly reducing memory footprint and eliminate redundant computations~\cite{MetaNMP}. 
The former is achieved by employing a cartesian-like product paradigm to generate all metapath instances on the fly. While for the latter, it aggregates vertex features along the dispersed metapath instances from the starting vertex, effectively utilizing shareable aggregation computations. 
%MetaNMP also introduces a broadcast mechanism to mitigate inter-DIMM communication for edge data and vertex features.
MetaNMP mainly focuses on the HGNN model which uses the intra-metapath aggregation~\cite{MAGNN} to aggregate vertex features along the metapath in feature projection stage. However, the intra-metapath aggregation hasn’t been widely used in the algorithm community after proposed by MAGNN~\cite{MAGNN} in 2020. 
Unlike MetaNMP, our work aims to exploit the inter-semantic-graph parallelism and data reusability in HGNNs, which is universal in HGNNs models.
In addition, based on the DIMM-based near-memory processing, MetaNMP cannot be adopted as a near term solution since this memory technology is still immature. In contrast, our work presents a practical solution by leveraging the off-the-shelf HBM.

%\add{A DIMM-based near-memory processing HGNNs accelerator, MetaNMP, is recently proposed to greatly reduce memory footprint and eliminate redundant computations~\cite{MetaNMP}. The former is achieved by employing a cartesian-like product paradigm for on-the-fly generation of all metapath instances. For the latter, it aggregates vertex features along the dispersed metapath instances from the starting vertex, utilizing shareable aggregation computations. MetaNMP also introduces a broadcast mechanism to mitigate inter-DIMM communication for edge data and vertex features. MetaNMP mainly focuses on the HGNN model which aggregates vertex features along the metapath such as MAGNN~\cite{MAGNN}. However, such HGNN models are very niche. In addition, MetaNMP cannot be adopted as a near term solution since the DIMM-based near-memory processing technology is still immature. In contrast, our work presents a practical solution by leveraging the off-the-shelf HBM. Furthermore, our work exploits the newfound parallelism and data reusability, which is universal in HGNNs models.}



A number of previous efforts have characterized the execution of HGNNs~\cite{understand_HGNN} and GNNs~\cite{understand_GCN,understand_distributed_gnn,GNNMark,understand_and_bridge,understand_gnn_computational_graph,analysis_of_bottlenecks,characterize_gnn_accelerators,in_depth_concurrency_analysis} in detail for the software and hardware optimizations. Our work~\cite{understand_HGNN} quantitatively characterizes the inference phase of HGNNs on GPUs, in which we reveal the performance bottleneck, execution pattern, and execution semantic of HGNNs. Our other work~\cite{understand_GCN,understand_distributed_gnn} characterizes the hybrid execution pattern of GCNs on GPUs and distributed GNN training on a multi-node GPU system. Additionally, GNNMark~\cite{GNNMark} introduces a unified evaluation framework that supports various GNN models and datasets, enabling performance bottleneck analysis and evaluation of system-level metrics such as scalability.

% Work~\cite{characterize_gnn_efficiency} provides an in-depth study of two mainstream GNN frameworks to analyze their performance in terms of runtime and the consumption of power and energy.

DGL~\cite{DGL} and PyG~\cite{PyG} are two popular GPU-based Python libraries used for the HGNN models implementation. For example, DGL provides many high-performance GPU kernels to perform various operations, such as the SpMMCsr kernel for the NA stage. However, GPU inherently suffers from low efficiency in the performing of irregular memory accesses, e.g., encountering massive unavailing replacements of features during the NA stage. 



%The main difference between DGL and PyG is that DGL is a cross-platform graph neural network library that supports multiple backend frameworks such as PyTorch, MXNet, and TensorFlow, while PyG is built explicitly on PyTorch to process graph data. Furthermore, DGL is designed to be more general-purpose and can be used for processing various types of graph data such as undirected graphs, directed graphs, heterogeneous graphs, etc., while PyG is more focused on specific tasks such as node classification, edge classification, graph classification, etc.



\begin{comment}
\textbf{GNN Accelerators.} Due to the outstanding performance of GNNs on non-Euclidean space data, GNN accelerators have drawn great interest from the architecture community. Some of the GNN accelerators~\cite{HyGCN}\cite{GCoD}\cite{GRIP} divide the GNN execution process into different stages and employ stage fusion to boost the overall performance. \textit{HyGCN}\cite{HyGCN} proposes a hybrid architecture that designs dedicated acceleration engines for aggregation and combination stages, and optimizes the overall system via an inter-engine pipeline for inter-stage fusion. Other works~\cite{FlowGNN}\cite{EnGN}\cite{ReGNN} achieve performance improvement through the exploitation of multi-dimensional parallelism. \textit{FlowGNN}\cite{FlowGNN} uses a scalable dataflow architecture to generally support a wide range of GNN models equipped with the exploitation of multiple-level parallelism involving vertices and edges. Although the above works have explored multi-dimensional parallelism within a processing graph for better performance, they have not fully exploited inter-semantic-graph parallelism which is inherent in HGNNs. Moreover, the execution semantics and execution flow of the sequential stages in HGNNs differs from those in GNNs, thus resulting in different fusion methodologies of compute, memory access, and dataflow.

\textbf{Software Framework for HGNN.}
Deep Graph Library (DGL)~~\cite{DGL} and PyTorch Geometric (PyG)~\cite{PyG} are two popular Python libraries used for the HGNN models implementation.
The main difference between DGL and PyG is that DGL is a cross-platform graph neural network library that supports multiple backend frameworks such as PyTorch, MXNet, and TensorFlow, while PyG is specifically built on PyTorch to process graph data. Furthermore, DGL is designed to be more general-purpose and can be used for processing various types of graph data such as undirected graphs, directed graphs, heterogeneous graphs, etc., while PyG is more focused on specific tasks such as node classification, edge classification, graph classification, etc.

\end{comment}
