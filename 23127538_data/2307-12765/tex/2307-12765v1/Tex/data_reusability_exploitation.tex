
\subsection{Data Reusability Exploitation: Similarity-aware Execution Scheduling} \label{sec:data_reusability}
Due to the fine-grained execution flow, it inevitably introduces a significant amount of redundant computations and DRAM accesses. 
To address this issue and eliminate redundant computations and DRAM accesses within and across the processing of semantic graphs, we initially employ a basic data-reuse technique.
Building on this foundation, we further propose a similarity-aware execution scheduling to exploit the inter-semantic-graph data reusability.

\subsubsection{Basic Mechanism of Data Reuse} \label{sec:basic_implementation_of_data_reuse}
To harvest the reusability of intermediate results both within and across semantic graphs, we implement a basic mechanism for data reuse using a bitmap.

We first give two findings to show the data-reuse opportunity. 
i) For the prevalent HGNN models, 
%\delete{the FP stage can be divided into two categories. The first category calculates the projected features within the processing of each semantic graph, which is called the relation-specific FP stage. The second category projects the raw features based on vertex types and is shared globally across all semantic graphs, which is called the type-specific FP stage. Both of them can reuse the projected features within semantic graphs and the latter one can further reuse across semantic graphs.}
the projected features can be reused within and across semantic graphs in FP stage due to the vertex-type-specific projection. ii) As illustrated in Algorithm~\ref{alg:Programming Model}, the attention importance used for the aggregation from the source vertex $u$ to the target vertex $v$ in the NA stage can be generated from $\theta^{\mathcal{P}}_{v, *}$ and $\theta^{\mathcal{P}}_{*, u}$, which are derived from $h'_v$ and $h'_u$ directly. 
% Since the attention vector $a^T$ is shared among all pairs of vertices in the same semantic graph for all attention-based HGNN models, the
These coefficients can be reused for all edges, avoiding recomputations for each edge.

% Figure environment removed

\begin{table}[!t]
    \centering
    \vspace{-10pt}
    \caption{The status of feature vector encoded by RAB.}
    \label{table:rab encoding}
    \resizebox{0.37\textwidth}{!}{
        \begin{tabular}{|c|ccc|}
        \hline
         \multirow{2}{*}{\diagbox{}{}}  & \multicolumn{3}{c|}{Status}                                                                         \\ \cline{2-4} 
                              & \multicolumn{1}{c|}{Projected} & \multicolumn{1}{c|}{Calculated $\theta^{\mathcal{P}}_{*, u}$} & Calculated $\theta^{\mathcal{P}}_{v, *}$ \\ \hline
        \multirow{5}{*}{Code} & \multicolumn{1}{c|}{0}         & \multicolumn{1}{c|}{0}                     & 0                     \\ \cline{2-4} 
                              & \multicolumn{1}{c|}{1}         & \multicolumn{1}{c|}{0}                     & 0                     \\ \cline{2-4} 
                              & \multicolumn{1}{c|}{1}         & \multicolumn{1}{c|}{1}                     & 0                     \\ \cline{2-4} 
                              & \multicolumn{1}{c|}{1}         & \multicolumn{1}{c|}{0}                     & 1                     \\ \cline{2-4} 
                              & \multicolumn{1}{c|}{1}         & \multicolumn{1}{c|}{1}                     & 1                     \\ \hline
        \end{tabular}
    }
    %\scriptsize
    \vspace{-5pt}
\end{table}



Based on these findings, we implement a bitmap called RAB to keep track of whether the computation for a given vertex has been performed or not. 
We record both the projected features and the attention coefficients to eliminate redundant computation within and across semantic graphs. 
However, since each vertex in every semantic graph has multiple status, providing separate bitmaps for each individual case will significantly increase overhead. Thus, we integrate the RAB into the \textit{Local Scheduler} and employ the binary encoding approach to assign three bits to each vertex, which are used to record its status.
These bits are indexed by the pair (type, index). The encoding of the bitmap is shown in Table~\ref{table:rab encoding}. 
%
Before starting the feature projection for a vertex $v$, the \textit{Global Scheduler} first checks the bitmap for its corresponding bits. If it is filled with full zeros, the vertex's current feature vector needs to be projected. Similarly, the middle and lowest bits indicate whether the attention coefficients in that semantic graph corresponding to the vertex have been generated.

% \delete{Since this process follows the FP stage closely, the projected feature of vertex $v$ from the FP stage can be immediately forwarded to the computation of attention coefficients $\theta^{\mathcal{P}}_{v, *}$ and $\theta^{\mathcal{P}}_{*, u}$, the bitmap can indicate both whether the vertex's feature has been projected and whether the attention coefficients have been generated.}

Meanwhile, a raw feature is solely accessed from DRAM for the first projection of a vertex in scenarios where RAB is utilized. Subsequently, the projected features and attention coefficients can be directly read from either DRAM or on-chip buffer for subsequent computations, thereby eliminating the need for repetitive access to raw features in DRAM. 





\subsubsection{Similarity-aware Execution Scheduling}
\label{sec:SES}
To further exploit the data reusability across semantic graphs, we propose a similarity-aware execution scheduling by adjusting the execution order of semantic graphs based on the similarity between semantic graphs.

For the prevalent HGNN models, each type of vertex is projected only once across all semantic graphs, and stored in the FP-Buf for reuse. 
Therefore, if there is a significant overlap (similarity) in vertex types between two consecutive semantic graphs, the latter semantic graph can reuse the projected feature left in the FP-Buf by the former semantic graph. This can help avoid duplicate DRAM accesses and improve performance.

To identify the reusability of data between semantic graphs, we represent each semantic graph as a vertex in a hypergraph, as shown in Fig.~\ref{fig:hypergraph} (a). In the hypergraph, each edge connects two semantic graphs that have at least one common type of vertex. This means that the projected features of the common vertices can be reused, promoting computational efficiency. To measure the similarity between two connected semantic graphs, we assign a weight to the edge that reflects their similarity as shown in Fig.~\ref{fig:hypergraph} (b). The weight is calculated using the following formula: $w_e=1 - \eta_e/\sum_{i \in E} \eta_i$, where \textit{E} is the set of all edges in the hypergraph and $\eta_e$ is the number of common vertices in the two semantic graphs connected by the edge \textit{e}. A lower weight value of $w_e$ indicates a higher similarity between the two semantic graphs, implying that more projected features can be reused.

%\delete{After the hypergraph is built, we transform the scheduling of the execution order of semantic graphs into finding a path that starts from a specific semantic graph and passes through all other semantic graphs exactly once.}
After the hypergraph is built, the scheduling of the execution order of semantic graphs is transformed to finding a path that starts from a specific semantic graph and passes through all other semantic graphs exactly once.
The objective is to maximize the similarity across all sequentially executed semantic graphs, thereby achieving more reuse of intermediate results. To accomplish this, we utilize the \textit{Shortest Hamilton Path} algorithm on the hypergraph to determine a specific order sequence of semantic graphs for execution.

There are still two challenges need to be addressed. First, not all hypergraphs created by semantic graphs are \textit{Hamiltonian graphs}, which means that \textit{Hamilton paths} are not always found. Second, since there is no inherent execution priority among semantic graphs, the beginning and ending points of the \textit{Hamilton path} are random. Therefore, as shown in Fig.~\ref{fig:hypergraph} (c), we first add extra edges to the hypergraph with a weight of 1, represented by gray dashed connections, which makes hypergraph a complete graph. Second, we add two virtual vertices to the hypergraph, which are connected to all other vertices with weights of zero, represented by the green dashed connections with virtual vertices. These virtual vertices are then used as the beginning and ending points of the \textit{Hamilton path}, respectively.
Finally, the execution orders of semantic graphs are produced by the shortest Hamilton path algorithm.
%ensures that the reusability of the projected features is maximized during the execution process. 

%Furthermore, to prevent the NA stage from getting blocked due to the preemption of FP-Buf by different lanes, the FP-Buf is dynamically divided into sections for each lane. The capacity of each section is determined by the average degree of the semantic graphs processed by the lane. While each lane can read from the entire buffer, it can only write to its designated section.

