\subsection{Random Forest of decision trees}

The machine learning was performed using a random forest regression model\cite{RFR_Kang,RFR_Dutschmann,RFR_Breiman2001}, as it is implemented in SciKitLearn \cite{scikit-learn}.
Random forests have been used for the prediction of several molecular properties. \cite{RFR_MOLS_Faber,faber_qml_lower_DFT,RFR_SOLUB_Palmer,RFR_Ward_mols}

We found that in our case random forest regression performs better than neural networks and kernel ridge regression models. In fact, random forests allow more flexible representations and usually produce accurate results even for small training set sizes.

Training a random forest is also fast since the creation of every individual decision tree can be performed in parallel. In general, more decision trees in the forest lead to better (until the model saturates), but more expensive predictions.

SciKitLearn default hyper-parameters were used: those impose no restrictions on the sizes of the leaves, on the depths trees, or on the number of features chosen to perform a split; each forest contained 100 decision trees (estimators), trained with bootstrapping.

We chose then 100 decision trees as a compromise between precision and computing time. In fact, the Mean Absolute Error (MAE) for 100 decision trees is higher than the MAE for 1000 decision trees by approximately 1\% (see SI), but the predictions are ten times faster.
