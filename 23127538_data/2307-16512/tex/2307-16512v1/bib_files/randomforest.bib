@article{RFR_Breiman2001,
author={Breiman, Leo},
year= {2001},
title={Random Forests},
journal={Machine Learning},
pages={5-32},
volume={45},
issue={1},
abstract={Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund & R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148â€“156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
url={https://doi.org/10.1023/A:1010933404324},
doi={10.1023/A:1010933404324}
}

@article{RFR_Kang,
author = {Kang, Beomchang and Seok, Chaok and Lee, Juyong},
title = {Prediction of Molecular Electronic Transitions Using Random Forests},
journal = {Journal of Chemical Information and Modeling},
volume = {60},
number = {12},
pages = {5984-5994},
year = {2020},
doi = {10.1021/acs.jcim.0c00698},
    note ={PMID: 33090804},
URL = { https://doi.org/10.1021/acs.jcim.0c00698},
        eprint = { https://doi.org/10.1021/acs.jcim.0c00698}
}


@Article{RFR_Dutschmann,
AUTHOR = {Dutschmann, Thomas-Martin and Baumann, Knut},
TITLE = {Evaluating High-Variance Leaves as Uncertainty Measure for Random Forest Regression},
JOURNAL = {Molecules},
VOLUME = {26},
YEAR = {2021},
NUMBER = {21},
ARTICLE-NUMBER = {6514},
URL = {https://www.mdpi.com/1420-3049/26/21/6514},
PubMedID = {34770921},
ISSN = {1420-3049},
ABSTRACT = {Uncertainty measures estimate the reliability of a predictive model. Especially in the field of molecular property prediction as part of drug design, model reliability is crucial. Besides other techniques, Random Forests have a long tradition in machine learning related to chemoinformatics and are widely used. Random Forests consist of an ensemble of individual regression models, namely, decision trees and, therefore, provide an uncertainty measure already by construction. Regarding the disagreement of single-model predictions, a narrower distribution of predictions is interpreted as a higher reliability. The standard deviation of the decision tree ensemble predictions is the default uncertainty measure for Random Forests. Due to the increasing application of machine learning in drug design, there is a constant search for novel uncertainty measures that, ideally, outperform classical uncertainty criteria. When analyzing Random Forests, it appears obvious to consider the variance of the dependent variables within each terminal decision tree leaf to obtain predictive uncertainties. Hereby, predictions that arise from more leaves of high variance are considered less reliable. Expectedly, the number of such high-variance leaves yields a reasonable uncertainty measure. Depending on the dataset, it can also outperform ensemble uncertainties. However, small-scale comparisons, i.e., considering only a few datasets, are insufficient, since they are more prone to chance correlations. Therefore, large-scale estimations are required to make general claims about the performance of uncertainty measures. On several chemoinformatic regression datasets, high-variance leaves are compared to the standard deviation of ensemble predictions. It turns out that high-variance leaf uncertainty is meaningful, not superior to the default ensemble standard deviation. A brief possible explanation is offered.},
DOI = {10.3390/molecules26216514}
}

@article{RFR_MOLS_Faber,
author = {Faber,Felix A.  and Christensen,Anders S.  and Huang,Bing  and von Lilienfeld,O. Anatole },
title = {Alchemical and structural distribution based representation for universal quantum machine learning},
journal = {The Journal of Chemical Physics},
volume = {148},
number = {24},
pages = {241717},
year = {2018},
doi = {10.1063/1.5020710},
URL = { https://doi.org/10.1063/1.5020710},
eprint = { https://doi.org/10.1063/1.5020710}
}

@article{RFR_Ward_mols,
  title = {Including crystal structure attributes in machine learning models of formation energies via Voronoi tessellations},
  author = {Ward, Logan and Liu, Ruoqian and Krishna, Amar and Hegde, Vinay I. and Agrawal, Ankit and Choudhary, Alok and Wolverton, Chris},
  journal = {Phys. Rev. B},
  volume = {96},
  issue = {2},
  pages = {024104},
  numpages = {12},
  year = {2017},
  month = {Jul},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevB.96.024104},
  url = {https://link.aps.org/doi/10.1103/PhysRevB.96.024104}
}

@article{RFR_SOLUB_Palmer,
author = {Palmer, David S. and O'Boyle, Noel M. and Glen, Robert C. and Mitchell, John B. O.},
title = {Random Forest Models To Predict Aqueous Solubility},
journal = {Journal of Chemical Information and Modeling},
volume = {47},
number = {1},
pages = {150-158},
year = {2007},
doi = {10.1021/ci060164k},
    note ={PMID: 17238260},
URL = { https://doi.org/10.1021/ci060164k},
eprint = { https://doi.org/10.1021/ci060164k}
}


