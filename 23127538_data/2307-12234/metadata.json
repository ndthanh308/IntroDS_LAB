{
  "title": "MARS: Exploiting Multi-Level Parallelism for DNN Workloads on Adaptive Multi-Accelerator Systems",
  "authors": [
    "Guan Shen",
    "Jieru Zhao",
    "Zeke Wang",
    "Zhe Lin",
    "Wenchao Ding",
    "Chentao Wu",
    "Quan Chen",
    "Minyi Guo"
  ],
  "submission_date": "2023-07-23T05:50:37+00:00",
  "revised_dates": [],
  "abstract": "Along with the fast evolution of deep neural networks, the hardware system is also developing rapidly. As a promising solution achieving high scalability and low manufacturing cost, multi-accelerator systems widely exist in data centers, cloud platforms, and SoCs. Thus, a challenging problem arises in multi-accelerator systems: selecting a proper combination of accelerators from available designs and searching for efficient DNN mapping strategies. To this end, we propose MARS, a novel mapping framework that can perform computation-aware accelerator selection, and apply communication-aware sharding strategies to maximize parallelism. Experimental results show that MARS can achieve 32.2% latency reduction on average for typical DNN workloads compared to the baseline, and 59.4% latency reduction on heterogeneous models compared to the corresponding state-of-the-art method.",
  "categories": [
    "cs.DC",
    "cs.AI",
    "cs.AR"
  ],
  "primary_category": "cs.DC",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.12234",
  "pdf_url": null,
  "comment": "Accepted by 60th DAC",
  "num_versions": null,
  "size_before_bytes": 933556,
  "size_after_bytes": 343433
}