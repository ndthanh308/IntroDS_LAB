\begin{thebibliography}{10}

\bibitem{he2016deep}
Kaiming He et~al.
\newblock Deep residual learning for image recognition.
\newblock In {\em CVPR}, pages 770--778, 2016.

\bibitem{vaswani2017attention}
Ashish Vaswani et~al.
\newblock Attention is all you need.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{cheng2016wide}
Heng-Tze Cheng et~al.
\newblock Wide \& deep learning for recommender systems.
\newblock In {\em Proceedings of the 1st workshop on deep learning for
  recommender systems}, pages 7--10, 2016.

\bibitem{fowers2018configurable}
Jeremy Fowers et~al.
\newblock A configurable cloud-scale dnn processor for real-time ai.
\newblock In {\em ISCA}, pages 1--14. IEEE, 2018.

\bibitem{zhang2019efficient}
Wentai Zhang et~al.
\newblock An efficient mapping approach to large-scale dnns on multi-fpga
  architectures.
\newblock In {\em DATE}, pages 1241--1244. IEEE, 2019.

\bibitem{kwon2021heterogeneous}
Hyoukjun Kwon et~al.
\newblock Heterogeneous dataflow accelerators for multi-dnn workloads.
\newblock In {\em HPCA}, pages 71--83. IEEE, 2021.

\bibitem{zhang2022h2h}
Xinyi Zhang et~al.
\newblock H2h: Heterogeneous model to heterogeneous system mapping with
  computation and communication awareness.
\newblock {\em DAC}, 2022.

\bibitem{peer2peer}
deeppat kristopk.
\newblock How to use the pcie peer-2-peer version 1.0, 2021.

\bibitem{rashidi2020astra}
Saeed Rashidi et~al.
\newblock Astra-sim: Enabling sw/hw co-design exploration for distributed dl
  training platforms.
\newblock In {\em ISPASS}, pages 81--92, 2020.

\bibitem{zheng2022alpa}
Lianmin Zheng et~al.
\newblock Alpa: Automating inter-and intra-operator parallelism for distributed
  deep learning.
\newblock {\em OSDI}, 2022.

\bibitem{tan2021nn}
Zhanhong Tan et~al.
\newblock Nn-baton: Dnn workload orchestration and chiplet granularity
  exploration for multichip accelerators.
\newblock In {\em ISCA}, pages 1013--1026. IEEE, 2021.

\bibitem{vasilache2018tensor}
Nicolas Vasilache et~al.
\newblock Tensor comprehensions: Framework-agnostic high-performance machine
  learning abstractions.
\newblock {\em arXiv preprint arXiv:1802.04730}, 2018.

\bibitem{kao2022magma}
Sheng-Chun Kao et~al.
\newblock Magma: An optimization framework for mapping multiple dnns on
  multiple accelerator cores.
\newblock In {\em HPCA}, pages 814--830. IEEE, 2022.

\bibitem{jiang2019achieving}
Weiwen Jiang et~al.
\newblock Achieving super-linear speedup across multi-fpga for real-time dnn
  inference.
\newblock {\em ACM TECS}, 18(5s):1--23, 2019.

\bibitem{wei2017automated}
Xuechao Wei et~al.
\newblock Automated systolic array architecture synthesis for high throughput
  cnn inference on fpgas.
\newblock In {\em DAC}, pages 1--6, 2017.

\bibitem{lu2017evaluating}
Liqiang Lu et~al.
\newblock Evaluating fast algorithms for convolutional neural networks on
  fpgas.
\newblock In {\em FCCM}, pages 101--108. IEEE, 2017.

\bibitem{zhang2020casia}
Shifeng Zhang et~al.
\newblock Casia-surf: A large-scale multi-modal benchmark for face
  anti-spoofing.
\newblock {\em IEEE TBIOM}, 2(2):182--193, 2020.

\bibitem{shen2019facebagnet}
Tao Shen et~al.
\newblock Facebagnet: Bag-of-local-features model for multi-modal face
  anti-spoofing.
\newblock In {\em Proceedings of CVPR Workshops}, 2019.

\end{thebibliography}
