% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{orr2023multi}
J.~Orr and A.~Dutta, ``Multi-agent deep reinforcement learning for multi-robot
  applications: A survey,'' \emph{Sensors}, vol.~23, no.~7, Apr. 2023.

\bibitem{guan2022efficient}
C.~Guan\emph{,~et~al.}, ``Efficient multi-agent communication via
  self-supervised information aggregation,'' in \emph{NeurIPS}, New Orleans,
  USA, Nov. 2022.

\bibitem{amirkhani2022consensus}
A.~Amirkhani and A.~H. Barshooi, ``Consensus in multi-agent systems: a
  review,'' \emph{Artificial Intelligence Review}, vol.~55, no.~5, pp.
  3897--3935, Jun. 2022.

\bibitem{yu2022surprising}
C.~Yu\emph{,~et~al.}, ``The surprising effectiveness of ppo in cooperative
  multi-agent games,'' in \emph{NeurIPS}, New Orleans, USA, Nov. 2022.

\bibitem{das2019tarmac}
A.~Das\emph{,~et~al.}, ``Tarmac: Targeted multi-agent communication,'' in
  \emph{ICML}, Long Beach, USA, Jun. 2019.

\bibitem{niu2021multi}
Y.~Niu\emph{,~et~al.}, ``Multi-agent graph-attention communication and
  teaming,'' in \emph{AAMAS}, Virtual, Online, May 2021.

\bibitem{wang2021tomc}
Y.~Wang\emph{,~et~al.}, ``Tom2c: Target-oriented multi-agent communication and
  cooperation with theory of mind,'' in \emph{ICLR}, Virtual, Online, Apr.
  2022.

\bibitem{yan2022relative}
Y.~Yan\emph{,~et~al.}, ``Relative distributed formation and obstacle avoidance
  with multi-agent reinforcement learning,'' in \emph{ICRA}, Philadelphia, USA,
  May 2022.

\bibitem{rusu2015policy}
A.~A. Rusu\emph{,~et~al.}, ``Policy distillation,'' in \emph{ICLR}, San Juan,
  USA, May 2016.

\bibitem{pan2022flexible}
C.~Pan\emph{,~et~al.}, ``Flexible formation control using hausdorff distance: A
  multi-agent reinforcement learning approach,'' in \emph{EUSIPCO}, Belgrade,
  Serbia, Aug. 2022.

\bibitem{lowe2017multi}
R.~Lowe\emph{,~et~al.}, ``Multi-agent actor-critic for mixed
  cooperative-competitive environments,'' in \emph{NeurIPS}, Long Beach, USA,
  Dec. 2017.

\bibitem{panerati2021learning}
J.~Panerati\emph{,~et~al.}, ``Learning to flyâ€”a gym environment with pybullet
  physics for reinforcement learning of multi-agent quadcopter control,'' in
  \emph{IROS}, Prague, Czech republic, Sep. 2021.

\bibitem{xiao2023stochastic}
B.~Xiao\emph{,~et~al.}, ``Stochastic graph neural network-based value
  decomposition for marl in internet of vehicles,'' in \emph{VTC}, Florence,
  Italy, Jun. 2023.

\bibitem{schulman2015high}
J.~Schulman\emph{,~et~al.}, ``High-dimensional continuous control using
  generalized advantage estimation,'' in \emph{ICLR}, San Juan, USA, May 2016.

\bibitem{vaswani2017attention}
A.~Vaswani\emph{,~et~al.}, ``Attention is all you need,'' in \emph{NeurIPS},
  Long Beach, USA, Dec. 2017.

\end{thebibliography}
