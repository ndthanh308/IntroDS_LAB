\section{Experimental Results}
\subsection{Experimental Settings}
\textbf{Data collection.} %Ethics approval for this study was provided by \emph{anonymized}. 
%Ethics approval for this study was granted by Anonymous. 
We collected 270 volumetric T1-weighted MRI and 267 thin-slice CT head scans with bony reconstruction performed in pediatric patients under routine scanning protocols\footnote{Ethics approval was granted by Southern Adelaide Clinical Human Research Ethics Committee}.
We targeted the age group from 6â€“24 months since pediatric patients are more susceptible to ionizing radiation and experience a greater cancer risk (up to 24\% increase) from radiation exposure~\cite{mathews2013cancer}. 
Furthermore, surgery for craniosynostosis, a birth defect in which the skull bones fuse too early, typically occurs during this age~\cite{zakhary2014surgical,governale2015craniosynostosis}. 
%. Geometric distortions in MR volumes were corrected using a 3D correction algorithm in the [Siemens Syngo console workstation]. All the MR volumes were N4 corrected and normalized by aligning the white matter peak identified by the fuzzy C-means. 
The scans were acquired by %MR/CT Philips and GE system	
Ingenia 3.0T MRI scanners and Philips Brilliance 64 CT scanners. We then resampled the volumetric scans to the same resolution of 1.0 $\times$ 1.0 $\times$ 1.0 $\text{mm}^3$. %The volumetric scans were resampled to the same resolution of 1.0 $\times$ 1.0 $\times$ 1.0 $\text{mm}^3$. %MRI and CT volumes were resampled to the same resolution of 1.0 $\times$ 1.0 $\times$ 1.0 $\text{mm}^3$. 
 %13 MRI-CT volumes from the same patients that were captured less than three months apart are registered using rigid registration algorithms with the Dipy Python library.%\footnote{\url{https://dipy.org/}.}.

The dataset comprises brain MR and CT volumes from 262 subjects. 13 MRI-CT volumes from the same patients that were captured less than three months apart are registered using rigid registration algorithms. 
 The dataset is divided into 249, 1 and 12 subjects for training, validating and testing set. Following~\cite{wolterink2017deep}, we conducted experiments on sagittal slices.  Each MR and CT volume consists of 180 to 200 slices, which are resized and padded to the size of $224 \times 224$. The intensity range of CT is clipped into [-1000, 2000]. All models are trained using the Adam optimizer for 100 epochs, with a learning rate of 0.0002 which linearly decays to zero over the last 50 epochs. We use a batch size of 16 and train on two NVIDIA RTX 3090 GPUs. %using PyTorch version 1.9.1. %The hyperparameters for each loss are fine-tuned to achieve optimal performance. %The Supplementary material describe the ablation studies on the hyperparameters.

\textbf{Evaluation metrics.} To provide a quantitative evaluation of methods, we compute the same standard performance metrics as in previous works~\cite{yang2020unsupervised,liu2021ct} including  mean absolute  error (MAE),  peak  signal-to-noise ratio (PSNR), and structural similarity (SSIM) between ground-truth and synthesized CT. \new{The scope of the paper centers on theoretical development; clinical evaluations such as dose calculation and treatment planning will be conducted in future work.}


\subsection{Results and Discussions}
\textbf{Comparisons with state-of-the-art.} We compare the performance of our proposed MaskGAN with existing state-of-the-art image synthesis methods, including CycleGAN~\cite{zhu2017unpaired}, AttentionGAN~\cite{tang2021attentiongan},  structure-constrained CycleGAN (sc-CycleGAN)~\cite{yang2020unsupervised} and shape-CycleGAN~\cite{ge2019unpaired}. Shape-CycleGAN requires annotated segmentation to train a separate U-Net. For a fair comparison, we implement shape-CycleGAN using our extracted coarse masks based on the authors' official code.%\footnote{\url{https://github.com/gyhandy/Unpaired_MR_to_CT_Image_Synthesis}.}. %We implement shape-CycleGAN following the official author's codebase\footnote{\url{https://github.com/gyhandy/Unpaired_MR_to_CT_Image_Synthesis}}.
%For shape-CycleGAN, following author's codebase\footnote{\href{https://github.com/gyhandy/Unpaired_MR_to_CT_Image_Synthesis}{https://github.com/gyhandy/Unpaired_MR_to_CT_Image_Synthesis}}, we first train the U-Net using our extracted coarse masks, freezes U-Net and trains the generators. %Among them, sc-CycleGAN and AttentionGAN are structural-based image synthesis methods which aim to preserve the structures of the image in the input domain. 
Note that CT-to-MRI synthesis is a secondary task supporting the primary MRI-to-CT synthesis task. As better MRI synthesis leads to improved CT synthesis, we also report the model's performance on MRI synthesis.

% \begin{table}[!h]
%     \caption{Quantitative comparison of different methods on the primary MRI-CT task and the secondary CT-MRI task. The results of two ablated versions of our proposed MaskGAN are also reported. $\pm$ standard deviation is reported over three evaluations.}
%     \label{tab:sota}
%     \centering
%     \resizebox{\columnwidth}{!}{%
%     \begin{tabular}{|c|c|c|c|c|c|c|}
%     \hline \textbf{Task} & \multicolumn{3}{c|}{Primary: MRI-to-CT} & \multicolumn{3}{c|}{Secondary: CT-to-MRI} \\\hline
%          \textbf{Method} & MAE $\downarrow$ & PSNR $\uparrow$ & SSIM $\uparrow$ & MAE $\downarrow$ & PSNR $\uparrow$ & SSIM $\uparrow$  \\\hline
%          CycleGAN~\cite{zhu2017unpaired} & 36.12 & 31.57& 46.17  & 37.21 & 30.59 & 43.73 \\
%     % MedGAN~\cite{armanious2020medgan} &  32.54 & 32.57 & 52.67 & 30.87 & 31.45 & 50.57\\
% AttentionGAN~\cite{tang2021attentiongan} & 27.25 & 32.88 & 54.57 & 30.47 & 30.15 & 52.66 \\
%          sc-CycleGAN~\cite{yang2020unsupervised} & 26.88& 32.97 & 55.08 & 28.43 & 30.47  & 52.64\\
%          shape-CycleGAN~\cite{ge2019unpaired} & 26.45  & 33.14  & 56.73 & 27.96& 31.65 & 53.88\\\hline
%         Our MaskGAN (w/o Shape) &  22.85 &  33.37 & 60.19& 24.58  & 32.43 & 57.35 \\
%         Our MaskGAN (w/o Mask) & 23.29 $\pm$ 0.22 &  33.81 $\pm$0.10 & 59.93 $\pm$ 0.11 & 25.11 $\pm$ 0.28 & 32.06 $\pm$ 0.09 & 56.88 $\pm$ 0.12 \\
%          Our MaskGAN & \textbf{21.78} & \textbf{34.75} & \textbf{64.27} & \textbf{21.66} & \textbf{32.55} & \textbf{58.32} \\\hline
%     \end{tabular}%
%     }
% \end{table}

% \begin{table}[!h]
%     \caption{Quantitative comparison of different methods on the primary MRI-CT task and the secondary CT-MRI task. The results of two ablated versions of our proposed MaskGAN are also reported. $\pm$ standard deviation is reported over five evaluations. The paired t-test is conducted between MaskGAN and a compared method at $p=0.05$. When the improvement of MaskGAN is statistically significant. The result of the compared method is underlined.}
%     \label{tab:sota}
%     \centering
%     \resizebox{\columnwidth}{!}{%
%     \begin{tabular}{@{}l|ccc||ccc@{}}
%     \toprule \multirow{ 2}{*}{\textbf{Methods}} & \multicolumn{3}{c}{\textbf{Primary: MRI-to-CT}} & \multicolumn{3}{c}{\textbf{Secondary: CT-to-MRI}} \\
%          & MAE $\downarrow$ & PSNR $\uparrow$ & SSIM $\uparrow$ & MAE $\downarrow$ & PSNR $\uparrow$ & SSIM $\uparrow$  \\\midrule
%          CycleGAN~\cite{zhu2017unpaired} & \underline{32.12 $\pm$ 0.31} &\underline{31.57 $\pm$ 0.12} &\underline{46.17 $\pm$ 0.20 } &\underline{34.21 $\pm$ 0.33} &\underline{30.59 $\pm$ 0.24} &\underline{43.73 $\pm$ 0.17} \\
%     % MedGAN~\cite{armanious2020medgan} & \underline{ 32.54 & \underline{32.57 & \underline{52.67 & \underline{30.87 & \underline{31.45 & \underline{50.57\\
% AttentionGAN~\cite{tang2021attentiongan} & \underline{26.25 $\pm$ 0.25} &\underline{32.88 $\pm$ 0.09} &\underline{54.57 $\pm$ 0.15} &\underline{28.47 $\pm$ 0.22} &\underline{30.15 $\pm$ 0.10} &\underline{53.66 $\pm$ 0.14} \\
%          sc-CycleGAN~\cite{yang2020unsupervised} & \underline{24.55 $\pm$ 0.24} &\underline{32.97 $\pm$ 0.07} &\underline{57.08 $\pm$ 0.11} &\underline{26.13 $\pm$ 0.15} &\underline{30.47 $\pm$ 0.07 } &\underline{54.14 $\pm$ 0.10} \\
%          shape-CycleGAN~\cite{ge2019unpaired} & \underline{24.30 $\pm$ 0.28} &\underline{33.14 $\pm$ 0.05} &\underline{57.73 $\pm$ 0.13} &\underline{25.96 $\pm$ 0.19} &\underline{31.65 $\pm$ 0.08} &\underline{54.88 $\pm$ 0.09} \\\hline
%         MaskGAN (w/o Shape) & \underline{ 23.45 $\pm$ 0.19} &\underline{ 34.02 $\pm$ 0.09} &\underline{60.19 $\pm$ 0.06} &\underline{22.58 $\pm$ 0.23} &\underline{32.43 $\pm$ 0.07} &\underline{57.35 $\pm$ 0.08} \\
%         MaskGAN (w/o Mask) & \underline{24.15 $\pm$ 0.22} &\underline{ 33.54 $\pm$0.10} &\underline{59.93 $\pm$ 0.11} &\underline{23.11 $\pm$ 0.28} &\underline{32.06 $\pm$ 0.09} &\underline{56.88 $\pm$ 0.12} \\
%          MaskGAN (Ours) & \textbf{22.78 $\pm$ 0.18}  & \textbf{34.75 $\pm$ 0.08} & \textbf{60.84 $\pm$ 0.10} & \textbf{22.66 $\pm$ 0.17} & \textbf{32.55 $\pm$ 0.06} & \textbf{58.32 $\pm$ 0.10} \\\hline
%     \end{tabular}%
%     }
% \end{table}
\begin{table}[!h]
    \caption{Quantitative comparison of different methods on the primary MRI-CT task and the secondary CT-MRI task. The results of an ablated version of our proposed MaskGAN are also reported. $\pm$ standard deviation is reported over five evaluations. The paired t-test is conducted between MaskGAN and a compared method at $p=0.05$. The improvement of MaskGAN over all compared methods is statistically significant.}
    \label{tab:sota}
    \centering
    \resizebox{\columnwidth}{!}{%
    \begin{tabular}{@{}l|ccc||ccc@{}}
    \toprule \multirow{ 2}{*}{\textbf{Methods}} & \multicolumn{3}{c}{\textbf{Primary: MRI-to-CT}} & \multicolumn{3}{c}{\textbf{Secondary: CT-to-MRI}} \\
         & MAE $\downarrow$ & PSNR $\uparrow$ & SSIM (\%) $\uparrow$ & MAE $\downarrow$ & PSNR $\uparrow$ & SSIM (\%) $\uparrow$  \\\midrule
         CycleGAN~\cite{zhu2017unpaired} & {32.12 $\pm$ 0.31} &{31.57 $\pm$ 0.12} &{46.17 $\pm$ 0.20 } &{34.21 $\pm$ 0.33} &{29.88 $\pm$ 0.24} &{45.73 $\pm$ 0.17} \\
    % MedGAN~\cite{armanious2020medgan} & { 32.54 & {32.57 & {52.67 & {30.87 & {31.45 & {50.57\\
AttentionGAN~\cite{tang2021attentiongan} & {28.25 $\pm$ 0.25} &{32.88 $\pm$ 0.09} &{53.57 $\pm$ 0.15} &{30.47 $\pm$ 0.22} &{30.15 $\pm$ 0.10} &{50.66 $\pm$ 0.14} \\
         sc-CycleGAN~\cite{yang2020unsupervised} & {24.55 $\pm$ 0.24} &{32.97 $\pm$ 0.07} &{57.08 $\pm$ 0.11} &{26.13 $\pm$ 0.15} &{31.22 $\pm$ 0.07 } &{54.14 $\pm$ 0.10} \\
         shape-CycleGAN~\cite{ge2019unpaired} & {24.30 $\pm$ 0.28} &{33.14 $\pm$ 0.05} &{57.73 $\pm$ 0.13} &{25.96 $\pm$ 0.19} &{31.69 $\pm$ 0.08} &{54.88 $\pm$ 0.09} \\\midrule
        MaskGAN (w/o Shape) & { 22.78 $\pm$ 0.19} &{ 34.02 $\pm$ 0.09} &{60.19 $\pm$ 0.06} &{23.58 $\pm$ 0.23} &{32.43 $\pm$ 0.07} &{57.35 $\pm$ 0.08} \\
        %MaskGAN (w/o Mask) & {24.15 $\pm$ 0.22} &{ 33.54 $\pm$0.10} &{59.93 $\pm$ 0.11} &{23.11 $\pm$ 0.28} &{32.06 $\pm$ 0.09} &{56.88 $\pm$ 0.12} \\
         MaskGAN (Ours) & \textbf{21.56 $\pm$ 0.18}  & \textbf{34.75 $\pm$ 0.08} & \textbf{61.25 $\pm$ 0.10} & \textbf{22.77 $\pm$ 0.17} & \textbf{32.55 $\pm$ 0.06} & \textbf{58.32 $\pm$ 0.10} \\\hline
    \end{tabular}%
    }
\end{table}

% Figure environment removed

Table~\ref{tab:sota} demonstrates that our proposed MaskGAN outperforms existing methods for statistical significance of $p=0.05$ in both tasks.  %Our method drastically decreases the MAE of CycleGAN by 44.17\%. Compared with structural-based methods, the proposed method improves the SSIM of sc-CycleGAN and AttentionGAN by 13.86\% and 14.95\%, respectively. Compared with the shape-aware method, Oour MaskGAN reduces MAE of shape-CycleGAN by 21.44\%. 
The method reduces the MAE of CycleGAN and AttentionGAN by 29.07\% and 19.36\%, respectively. %and significantly improves the SSIM of AttentionGAN by 14.11\%. 
%Compared with sc-CycleGAN, which is the current state-of-the-art method in unsupervised MR-CT synthesis, our MaskGAN drastically boosts the SSIM by 7.31\%. 
Furthermore, MaskGAN outperforms shape-CycleGAN, reducing its MAE by 11.28\%. Unlike shape-CycleGAN, which underperforms when trained with coarse segmentations, our method obtains consistently higher results. Fig.~\ref{fig:vis} shows the visual results of different methods. sc-CycleGAN produces artifacts (e.g., the eye socket in the first sample and the nasal cavity in the second sample), as it preserves pixel-wise correlations. In contrast, our proposed MaskGAN preserves shape-wise consistency and produces the smoothest synthetic CT. \new{Unlike adult datasets~\cite{yang2020unsupervised,ge2019unpaired}, pediatric datasets are easily misaligned due to children's rapid growth between scans. Under this challenging setting, unpaired image synthesis can have non-optimal visual results and SSIM scores. Yet, our MaskGAN achieves the highest quality, indicating its suitability for pediatric image synthesis.} %Compared with AttentionGAN, our MaskGAN better preserves the head shape and creates the most consistent MR-to-CT mapping. %In contrast, AttentionGAN and our proposed MaskGAN preserve shape-wise consistency, producing smoother CT outputs. Compared with AttentionGAN, our MaskGAN better delineates the head shape and creates the most consistent mapping from MRI to CT.


We perform an ablation study by removing the cycle shape consistency loss (w/o Shape).
%To validate the impact of the two proposed loss objectives (mask loss and cycle shape consistency loss), two ablated versions of MaskGAN are evaluated. %They are MaskGAN trained with CSC loss only, i.e., MaskGAN (w/o CSC) with mask loss only, i.e., MaskGAN (w/o Shape). %Note that AttentionGAN~\cite{tang2021attentiongan} is MaskGAN without mask and shape-consistency losses. 
Compared with shape-CycleGAN, MaskGAN using only a mask loss significantly reduces MAE by 6.26\%. %The background masks implicitly learned by AttentionGAN can be unsatisfactory. In contrast, our proposed mask loss explicitly supervises the attention learning process using the ground-truth mask. Hence, applying our mask loss significantly improves AttentionGAN in all metrics. Our MaskGAN combines both mask loss and shape-consistency loss to ensure the structural consistency when translating between MRI and CT domains. 
The combination of both mask and cycle shape consistency losses results in the largest improvement, demonstrating the complementary contributions of our two losses.%, further decreasing MAE of MaskGAN (w/o Shape) by 5.36\% and 3.44\% on primary and secondary tasks, respectively. 
%This demonstrates the complementary contributions of our two losses towards structure preservation across MR and CT domains. %sc-CycleGAN addresses the spurious mapping by comparing the pixel correlation in the MRI and that in the synthetic CT. However, pixel correlation in MRI in certain areas is different from that in CT. For example, brain tissues 


%\subsection{Ablation Study}
% \begin{table}[!h]
%     \caption{Ablation study on our proposed loss functions.}
%     \label{tab:abl}
%     \centering
%     \begin{tabular}{|P{2.5cm}|P{2.5cm}|P{1.5cm}|P{1.5cm}|P{1.5cm}|}
%          \hline \textbf{Mask} & \textbf{Shape consistency} & \textbf{MAE} $\downarrow$ & \textbf{PSNR} $\uparrow$ & \textbf{SSIM} $\uparrow$   \\\hline
%          & & 30.25 & 32.88 & 53.57\\\hline
%           \checkmark & &  23.85 &  32.37 & 58.19 \\\hline
%          & \checkmark & 24.29 &  32.81 & 57.93 \\\hline
%          \checkmark & \checkmark & \textbf{20.57} & \textbf{34.03} & \textbf{61.58} \\\hline
%     \end{tabular}
% \end{table}
% \textbf{Evolution of attention masks learned by MaskGAN.} To explore the efficiency of MaskGAN, we visualize how learned attention masks evolve over time. %Given that the total number of epochs is 100, 
% Fig.~\ref{fig:evo} shows the attention masks learned by MaskGAN at epoch 15 and epoch 60. At epoch 15, the model over-extends the foreground mask to the back of the head, thus generating inconsistent mappings.
% At epoch 60, our MaskGAN learns to correctly delineate the head under the supervision of the mask loss. 

% % Figure environment removed

\textbf{Robustness to error-prone coarse masks.}
%We investigate the robustness of MaskGAN to noisy ground-truth masks and 
We compare the performance of our approach with shape-CycleGAN~\cite{ge2019unpaired} using deformed masks that simulate human errors during annotation. %To alter object shapes, we employ random elastic deformation, which is a standard data augmentation technique in medical image segmentation~\cite{ronneberger2015u}. This technique applies random displacement vectors to objects. The level of distortion is controlled by the standard deviation of the normal distribution from which the vectors are sampled.
To alter object shapes, we employ random elastic deformation, a standard data augmentation technique~\cite{ronneberger2015u} that applies random displacement vectors to objects. The level of distortion is controlled by the standard deviation of the normal distribution from which the vectors are sampled.
Fig.~\ref{fig:deform} (Left) shows MAE of the two methods under increasing levels of distortion. MAE of shape-CycleGAN drastically increases as the masks become more distorted. Fig.~\ref{fig:deform} (Right) shows that our MaskGAN (d) better preserves the anatomy. %In contrast to shape-CycleGAN, which freezes the shape extractor, our technique jointly trains the shape and the content generators. Besides mask loss, the mask generator also receives supervision from the adversarial loss to remedy the impact of error-prone segmentation. %This joint training remedies the impact of error-prone segmentation. %Besdies mask loss, the mask generator also receives supervision from adversarial loss. Hence, the generator can learn to adjust the extracted shape to optimize the final output. %Jointly training shape and content generators as in our MaskGAN can remedy the impact of error-prone segmentation. %This highlights the robustness of MaskGAN, even when trained under error-prone masks.
 % Figure environment removed


% \textbf{Evolution of attention masks learned by MaskGAN.} To explore the efficiency of MaskGAN, we visualize how learned attention masks evolve over time. %Given that the total number of epochs is 100, 
% Fig.~\ref{fig:evo} shows the attention masks learned by MaskGAN at epoch 15 and epoch 60. At epoch 15, the model over-extends the foreground mask to the back of the head, thus generating inconsistent mappings. %Due to the lack of paired data, the model synthesizes CT results that look realistic, but unaligned with the input MRI. 
% At epoch 60, our MaskGAN learns to correctly delineate the head under the supervision of the proposed mask loss. The attention masks makes the model more explainable.
% \vspace{-1em}
% % Figure environment removed

% \textbf{Tolerance to noisy ground-truth masks.}
% To investigate the robustness of our MaskGAN to noisy ground-truth masks, we report the results of MaskGAN and shape-CycleGAN~\cite{ge2019unpaired} while adding noise to ground-truth masks.
% \begin{table}[]
%     \centering
%         \caption{MAE of shape-consistency based methods when injecting noise to the ground-truth masks.}
%     \label{tab:noise}
%     \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
%          \hline Noise & 1\% & 2\% & 5\% & 10\% & 15\% & 20\% & 25\% & 40\% \\\hline
%          ShapeGAN~\cite{ge2019unpaired} & \\\hline
%          MaskGAN (Ours) & \\\hline
%     \end{tabular}
% \end{table}


% \begin{table}[!h]
%     \caption{Ablation study on our proposed loss functions and different generator's architecture.}
%     \label{tab:abl}
%     \centering
%     \begin{tabular}{|c|c|c|c|c|c|c|}
%          \hline \textbf{Method} & \textbf{Mask loss} & \textbf{Shape loss} & \textbf{U-Net generator} & \textbf{MAE} $\downarrow$ & \textbf{PSNR} $\uparrow$ & \textbf{SSIM} $\uparrow$   \\\hline
%         MaskGAN & \checkmark & & & 23.85 &  32.37 & 58.19 \\\hline
%         MaskGAN & & \checkmark &  24.29 &  32.81 & 57.93 \\\hline
%          MaskGAN & \checkmark & \checkmark & \checkmark & \textbf{20.57} & \textbf{34.03} & \textbf{61.58} \\\hline
%     \end{tabular}
% \end{table}

