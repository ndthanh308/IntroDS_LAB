\section{Proposed Method}
%To resolve the inconsistent mappings, recent methods impose shape consistency between MR and synthetic CT by training an additional segmentation network~\cite{ge2019unpaired,emami2021sa}. However, they require a precise segmentation annotation, which is costly and time-consuming.
%Existing methods~\cite{zhang2022map,armanious2020medgan} employ CycleGAN~\cite{zhu2017unpaired} using the cycle consistency loss to tackle the unsupervised CT synthesis problem. However, this loss objective does not impose the structural consistency between the MRI and CT domains, leading to spurious mappings from MRI's background to CT's foreground. 
%A recent image synthesis method, called AttentionGAN~\cite{tang2021attentiongan}, implicitly learns a coarse foreground attention mask to synthesize discriminative foreground structures while leaving the background intact. However, the attention mask is learned indirectly via adversarial loss without direct mask supervision. Thus, the quality of learned masks might be poor.

%%%%%%%%%%%%%%%%
% Why MaskGAN does not need precise segmentation map?
%%%%%%%%%%%
%\subsection{MaskGAN architecture}
%The proposed MaskGAN learns the coarse foreground and background masks, outlining the shape of prominent anatomical structures. Our emphasis is on synthesizing CT contents in the segmented foreground structures, preserving the background untouched. Fig.~\ref{fig:maskGAN} shows the MaskGAN's architecture.
In this section, we first introduce the \textbf{MaskGAN} architecture, shown in Fig.~\ref{fig:maskGAN}, and then describe the three supervision losses we use for optimization.

% Figure environment removed

%Following an unsupervised learning paradigm, the proposed method learns to synthesize CT from MRI using two sets of \textit{unpaired} MR and CT images
\subsection{MaskGAN architecture} 
The network comprises two generators, each learning an MRI-CT and a CT-MRI translation. Our generator design has two branches, one for generating masks and the other for synthesizing the content in the masked regions. The mask branch learns $N$ attention masks $A_i$, where the first $N-1$ masks capture foreground (FG) structures and the last mask represents the background (BG). The content branch synthesizes $N-1$ outputs for the foreground structures, denoted as $C$. Each output, $C_i$, represents the synthetic content for the corresponding foreground region that is masked by the attention mask $A_i$. % $i$-th synthetic content $C_i$ is for the $i$-th foreground region masked by $A_i$. 

Intuitively, each channel $A_i$ in the mask tensor $A$ focuses on different anatomical structures in the medical image. For instance, one channel emphasizes on synthesizing the skull, while another focuses on the brain tissue. %Here, the attention mask tensor $A$ has a shape of $N \times H \times W$, where $N$ is the total number of channels, $H \times W$ denote the height and width of the input and output image. 
The last channel $A_N$ in A corresponds to the background and is applied to the original input to preserve the background contents. The final output is the sum of masked foreground contents and masked background input. Formally, the synthetic CT output generated from the input MRI $x$ is defined as
\begin{equation}
    O_{\text{CT}} = G_{\text{CT}}(x) = A_{\text{CT}}^N x + \sum_{i=1}^{N-1} A_{\text{CT}}^i C_{\text{CT}}^i.
\end{equation}
The synthetic MRI output from the CT scan $y$ is defined similarly based on the attention masks and the contents from the MR generator. %Fig.~\ref{fig:maskGAN} shows the overview of our MaskGAN with $N=3$ attention masks. 
The proposed network is trained using three training objectives described in the next sections.%: the standard CycleGAN loss, our proposed mask supervision, and shape consistency supervision.

\subsection{CycleGAN supervision}
\label{method:cyclegan}
The two generators, $G_{MR}$ and $G_{CT}$, map images from MRI domain ($X$) and CT domain ($Y$), respectively. Two discriminators, $D_{MR}$ and $D_{CT}$, are used to distinguish real from fake images in the MRI and CT domains. The adversarial loss for training the generators to produce synthetic CT images is defined as
\begin{equation}
    \resizebox{0.99\hsize}{!}{    
    $\mathcal{L}_{\text{CT}}(G_{\text{MR}}, D_{\text{CT}},x,y) = \mathbb{E}_{y \sim p_{\text{data}}(y)} \big[ \log D_{\text{CT}}(y) \big] +  \mathbb{E}_{x \sim p_{\text{data}}(x)} \big[ \log (1- D_{\text{CT}}(G_{\text{MR}}(x))) \big]$.
    }
\end{equation}
The adversarial loss $\mathcal{L}_{\text{MR}}$ for generating MRI images is defined in a similar manner.
% \begin{equation}
%     \resizebox{0.99\hsize}{!}{   
%     $L_{\text{adv}}(G_{\text{CT}}, D_{\text{MRI}},y,x) = \mathbb{E}_{x \sim p_{\text{data}}}(x) \big[ \log D_{\text{MRI}}(x) \big] +  \mathbb{E}_{x \sim p_{\text{data}}}(x) \big[ \log (1- D_{\text{CT}}(G_{\text{MR}}(x))) \big]$.
%     }
% \end{equation}
For unsupervised training, CycleGAN imposes the cycle consistency loss, which is formulated as follows
\begin{equation}
    \mathcal{L}_{\text{cycle}} = \mathbb{E}_{x \sim p_{\text{data}}}(x) \norm{x - G_{CT}(G_{MR}(x))} + \mathbb{E}_{y \sim p_{\text{data}}}(y) \norm{y - G_{MR}(G_{CT}(y))}.
\end{equation}
The CycleGAN's objective $\mathcal{L}_{\text{GAN}}$ is the combination of adversarial and cycle consistency loss.%: $L_{\text{GAN}} = \lambda L_{\text{cycle}} + L_{\text{MRI}} + L_{\text{CT}}$, 
% \begin{equation}
%     L_{\text{GAN}} = \lambda L_{\text{cycle}} + L_{\text{MRI}} + L_{\text{CT}},
% \end{equation}
%where $\lambda$ is a coefficient controlling the strength of cycle consistency loss.


%%%
\subsection{Mask and cycle shape consistency supervision}
\label{method:mask}
\textbf{Mask loss.} To reduce spurious mappings in the background regions, MaskGAN explicitly guides the mask generator to differentiate the foreground objects from the background using mask supervision. We extract the coarse mask $B$ using basic image processing operations.
Specifically, we design a simple but robust algorithm that works on both MRI and CT scans, with a binarization stage followed by a refinement step. 
\new{In the binarization stage, we normalize the intensity to the range [0, 1] and apply a binary threshold of 0.1, selected based on histogram inspection, to separate the foreground from the background. 
In the post-processing stage, we refine the binary image using morphological operations, specifically employing a binary opening operation to remove small artifacts. We perform connected component analysis~\cite{samet1988efficient} and keep the largest component as the foreground. Column 6 in Fig.~\ref{fig:vis} shows examples of extracted masks.} %Connected component analysis~\cite{samet1988efficient}, a well-established algorithm for blob extraction, is applied to capture the largest component in the binary image as the foreground structure. 
%After extracting the coarse mask, we set the \textit{soft} values of 0.1 and 0.9 for background and foreground, instead of the \textit{hard} values of 0 and 1. This regularizes the mask learning and prevents overfitting on the coarse mask. 
%Further details about the algorithm can be found in the supplementary materials. %We extract the coarse mask $B$ using the standard connected component analysis algorithm~\cite{samet1988efficient}, which is a simple and well-established technique for blob extraction. Fig.~\ref{fig:extract} shows the two-stage algorithm for mask extraction. The core idea is to extract the largest connected component as the foreground entity in the image.
%Further details about the algorithm can be found in the supplementary materials. %We extract the ground-truth background mask $B$ from the image using a robust two-stage mask extraction algorithm which consists of image pre-processing and connected component analysis stages as shown in Fig.~\ref{fig:extract}. Supplementary materials discuss the proposed algorithm in more details. %First, the input is pre-processed and converted to a binary mask. Then, we identify the largest connected component from the binary image, which represents the foreground object. Note that the mask can have small holes inside the foreground due to noise. The small background regions surrounded by foreground pixels are considered as `holes' if their area is less than a certain threshold. The label of pixels on identified holes are changed to foreground.

% % Figure environment removed

We introduce a novel mask supervision loss that penalizes the difference between the background mask $A_N$ learned from the input image and the ground-truth background mask $B$ in both MRI and CT domains. The mask loss for the attention generators is formulated as
\begin{equation}
    \mathcal{L}_{\text{mask}} = \mathbb{E}_{x \sim p_{\text{data}}(x)} \norm{A^{\text{MR}}_N - B^{\text{MR}}_N}_1 + \mathbb{E}_{y \sim p_{\text{data}}(y)} \norm{A_N^{\text{CT}} - B_N^{\text{CT}}}_1.
\end{equation}

%\textit{Discussion.} Previous segmentation-based methods~\cite{ge2019unpaired,zhang2018translating} only have a single content generator and enforce shape consistency on the \textit{synthetic contents} from the generator. In contrast, we decouple the learning into two generators and only impose consistency on the \textit{mask} produced by the mask generator, while giving the content generator more flexibility. This flexibility allows the content generator to adjust content when the learned mask is imperfect. Disentangling the generator into two branches for learning masks and contents separately makes our MaskGAN more robust to error-prone segmentation masks. 

\textit{Discussion.} Previous shape-aware methods~\cite{ge2019unpaired,zhang2018translating} use a pre-trained U-Net~\cite{ronneberger2015u} segmentation network to enforce shape consistency on the generator. U-Net is pre-trained in a separate stage and frozen when the generator is trained. Hence, any errors produced by the segmentation network cannot be corrected. In contrast, we jointly train the shape extractor, \textit{i.e.}, the mask generator, and the content generator end-to-end. Besides mask loss $\mathcal{L}_{\text{mask}}$, the mask generator also receives supervision from an adversarial loss $\mathcal{L}_{\text{GAN}}$ to adjust the extracted shape and optimize the final synthetic results. %This strategy allows the both generators to communicate and potentially adjust the extracted shape to optimize the final synthetic results. 
Moreover, in contrast to previous methods that train a separate shape extractor, our MaskGAN uses a shared encoder for mask and content generators, as illustrated in Fig.~\ref{fig:maskGAN}. Our design embeds the extracted shape knowledge into the content generator, thus improving the structural consistency of the synthetic contents.

% only have a single content generator and enforce shape consistency on the \textit{synthetic contents} from the generator. In contrast, we decouple the learning into two generators and only impose consistency on the \textit{mask} produced by the mask generator, while giving the content generator more flexibility. This flexibility allows the content generator to adjust content when the learned mask is imperfect. Disentangling the generator into two branches for learning masks and contents separately makes our MaskGAN more robust to error-prone segmentation masks. 

%Existing methods~\cite{ge2019unpaired,zhang2018translating} segment the output and enforces consistency with the ground-truth segmentation. The generator needs to strictly produce synthetic contents matching the ground truth, thus being susceptible to imperfect ground-truth segmentations. In contrast, ours segment the input and does not restrict . Thus, the content generator is given the flexibility to adjust and produce accurate contents within the imprecise mask segmentation
%Existing segmentation-based methods strictly enforce the consistent segmentation on the final synthetic CT. In contrast, we impose shape consistency on the mask, giving the content generator the flexibility to adjust and produce accurate contents, even when using imprecise mask segmentation. Existing methods~\cite{ge2019unpaired,zhang2018translating} strictly

%Our MaskGAN only utilizes a coarse segmentation mask as a guiding signal, without the need for costly and time-consuming precise segmentation. %This means you can enjoy high-quality synthetic CT images without sacrificing efficiency or accuracy.
%In contrast, we impose the shape consistency on the mask, while the synthetic CT content in the masked regions are flexibly generated. Even when the mask generator incorrectly segments some background region as foreground, the content generator still can produce correct background contents in the wrongly segmented foreground regions. Our MaskGAN only uses coarse segmentation masks as a hint without relying on expensive precise segmentation.

\textbf{Cycle shape consistency loss.}
Spurious mappings can occur when the anatomy is shifted during translation. To preserve structural consistency across domains, we introduce the cycle shape consistency (CSC) loss as our secondary contribution. 
%An image should have the same anatomical structures in MRI and CT domains. To preserve the structural consistency when translating across domains, we propose a cycle shape consistency (CSC) loss. 
Our loss penalizes the discrepancy between the background attention mask $A^{\text{MR}}_N$ learned from the input MRI image and the mask $\tilde{A}^{\text{CT}}_N$ learned from synthetic CT. Enforcing consistency in both domains, we formulate the shape consistency loss as
\begin{equation}
    \mathcal{L}_{\text{shape}} =  \mathbb{E}_{x \sim p_{\text{data}}(x)} \norm{A_N^{\text{MR}} - \tilde{A}_N^{\text{CT}}}_1 + \mathbb{E}_{y \sim p_{\text{data}}(y)} \norm{A_N^{\text{CT}} - \tilde{A}_N^{\text{MR}}}_1.
\end{equation}
%Applying shape consistency on each individual foreground channel $A_i$ between two domains may restrict the learning flexibility of the two generators. Thus, we only apply shape consistency loss on the background mask $A_N$. 
%Our strategy allows the MRI and CT generator to focus on translating different foreground areas. 
The final loss for MaskGAN is the sum of three loss objectives weighted by the corresponding loss coefficients: $\mathcal{L} =\mathcal{L}_{\text{GAN}} + \lambda_{\text{mask}}\mathcal{L}_{\text{mask}} + \lambda_{\text{shape}}\mathcal{L}_{\text{shape}}$.
% \begin{equation}
%     \mathcal{L} =\mathcal{L}_{\text{GAN}} + \lambda_{\text{mask}}\mathcal{L}_{\text{mask}} + \lambda_{\text{shape}}\mathcal{L}_{\text{shape}}
% \end{equation}
%MRI and CT images have different contrasts in certain body areas. For example, bones have high contrast in CT, while having low values in MRI. The foreground masks in the MRI image could be different from the CT image. Therefore, we only apply shape consistency loss on the background mask $A_N$ but not on each individual foreground mask $A_i$. Our strategy allows the MRI generator and CT generator to independently learn different and diverse foreground masks.

% \textbf{Discussion on learnable attention masks.} Instead of learning the background masks, one can directly use the ground-truth background mask. However, ground-truth background masks have many artifacts since traditional image processing techniques are susceptible to noise. Thus, our MaskGAN learns the attention masks 
