@article{code1,
	title={Article Title},
	author={A. Firstauthor and A. Secondauthor},
	journal={Journal name},
	volume={1},
	pages={1--2},
	year={2023},
	publisher={Publisher}
}


@article{cai_rop_2015,
  title={ROP: Matrix recovery via rank-one projections},
  author={Cai, T Tony and Zhang, Anru},
  journal={The Annals of Statistics},
  volume={43},
  number={1},
  pages={102--138},
  year={2015},
  publisher={Institute of Mathematical Statistics}
}

@inproceedings{lexa_compressive_2011,
	title = {Compressive power spectral density estimation},
	doi = {10.1109/ICASSP.2011.5947200},
	abstract = {In this paper, we consider power spectral density estimation of bandlimited, wide-sense stationary signals from sub-Nyquist sampled data. This problem has recently received attention from within the emerging field of cognitive radio for example, and solutions have been proposed that use ideas from compressed sensing and the theory of digital alias-free signal processing. Here we develop a compressed sensing based technique that employs multi-coset sampling and produces multi-resolution power spectral estimates at arbitrarily low average sampling rates. The technique applies to spectrally sparse and nonsparse signals alike, but we show that when the wide-sense stationary signal is spectrally sparse, compressed sensing is able to enhance the estimator. The estimator does not require signal reconstruction and can be directly obtained from a straightforward application of nonnegative least squares.},
	booktitle = {2011 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Lexa, Michael A. and Davies, Mike E. and Thompson, John S. and Nikolic, Janosch},
	month = may,
	year = {2011},
	note = {ISSN: 2379-190X},
	keywords = {Bandwidth, compressed sensing, Compressed sensing, Estimation, Least squares approximation, multi-coset sampling, nonnegative least squares, power spectral density estimation, Random processes, Signal processing},
	pages = {3884--3887},
	file = {IEEE Xplore Full Text PDF:/home/redelogne/Zotero/storage/XE7MQBW7/Lexa et al. - 2011 - Compressive power spectral density estimation.pdf:application/pdf;IEEE Xplore Abstract Record:/home/redelogne/Zotero/storage/C7LWBGX8/5947200.html:text/html},
}

@article{dirksen_covariance_2021,
	title = {Covariance estimation under one-bit quantization},
	url = {http://arxiv.org/abs/2104.01280},
	abstract = {We consider the classical problem of estimating the covariance matrix of a subgaussian distribution from i.i.d. samples in the novel context of coarse quantization, i.e., instead of having full knowledge of the samples, they are quantized to one or two bits per entry. This problem occurs naturally in signal processing applications. We introduce new estimators in two diﬀerent quantization scenarios and derive non-asymptotic estimation error bounds in terms of the operator norm. In the ﬁrst scenario we consider a simple, scale-invariant one-bit quantizer and derive an estimation result for the correlation matrix of a centered Gaussian distribution. In the second scenario, we add random dithering to the quantizer. In this case we can accurately estimate the full covariance matrix of a general subgaussian distribution by collecting two bits per entry of each sample. In both scenarios, our bounds apply to masked covariance estimation. We demonstrate the near-optimality of our error bounds by deriving corresponding (minimax) lower bounds and using numerical simulations.},
	language = {en},
	urldate = {2021-08-25},
	journal = {arXiv:2104.01280 [cs, math, stat]},
	author = {Dirksen, Sjoerd and Maly, Johannes and Rauhut, Holger},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.01280},
	keywords = {Computer Science - Information Theory, Mathematics - Statistics Theory},
	file = {Dirksen et al. - 2021 - Covariance estimation under one-bit quantization.pdf:/home/redelogne/Zotero/storage/QK8WFYUF/Dirksen et al. - 2021 - Covariance estimation under one-bit quantization.pdf:application/pdf},
}

@article{maly_new_2021,
	title = {New challenges in covariance estimation: multiple structures and coarse quantization},
	shorttitle = {New challenges in covariance estimation},
	url = {http://arxiv.org/abs/2106.06190},
	abstract = {In this self-contained chapter, we revisit a fundamental problem of multivariate statistics: estimating covariance matrices from ﬁnitely many independent samples. Based on massive Multiple-Input Multiple-Output (MIMO) systems we illustrate the necessity of leveraging structure and considering quantization of samples when estimating covariance matrices in practice. We then provide a selective survey of theoretical advances of the last decade focusing on the estimation of structured covariance matrices. This review is spiced up by some yet unpublished insights on how to beneﬁt from combined structural constraints. Finally, we summarize the ﬁndings of our recently published preprint “Covariance estimation under one-bit quantization” [15] to show how guaranteed covariance estimation is possible even under coarse quantization of the samples.},
	language = {en},
	urldate = {2021-08-25},
	journal = {arXiv:2106.06190 [math, stat]},
	author = {Maly, Johannes and Yang, Tianyu and Dirksen, Sjoerd and Rauhut, Holger and Caire, Giuseppe},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.06190},
	keywords = {Mathematics - Statistics Theory},
	file = {Maly et al. - 2021 - New challenges in covariance estimation multiple .pdf:/home/redelogne/Zotero/storage/PUV7NXQT/Maly et al. - 2021 - New challenges in covariance estimation multiple .pdf:application/pdf},
}

@article{chen_exact_2015,
  title={Exact and stable covariance estimation from quadratic sampling via convex programming},
  author={Chen, Yuxin and Chi, Yuejie and Goldsmith, Andrea J},
  journal={IEEE Transactions on Information Theory},
  volume={61},
  number={7},
  pages={4034--4059},
  year={2015},
  publisher={IEEE}
}

@article{eldar_sample_2019,
	title = {Sample {Efficient} {Toeplitz} {Covariance} {Estimation}},
	url = {http://arxiv.org/abs/1905.05643},
	abstract = {We study the sample complexity of estimating the covariance matrix T of a distribution D over d-dimensional vectors, under the assumption that T is Toeplitz. This assumption arises in many signal processing problems, where the covariance between any two measurements only depends on the time or distance between those measurements.1 We are interested in estimation strategies that may choose to view only a subset of entries in each vector sample x ∼ D, which often equates to reducing hardware and communication requirements in applications ranging from wireless signal processing to advanced imaging. Our goal is to minimize both 1) the number of vector samples drawn from D and 2) the number of entries accessed in each sample. We provide some of the ﬁrst non-asymptotic bounds on these sample complexity measures that exploit T ’s Toeplitz structure, and by doing so, signiﬁcantly improve on results for generic covariance matrices. These bounds follow from a novel analysis of classical and widely used estimation algorithms (along with some new variants), including methods based on selecting entries from each vector sample according to a so-called sparse ruler.},
	language = {en},
	urldate = {2021-08-25},
	journal = {arXiv:1905.05643 [cs, eess, math, stat]},
	author = {Eldar, Yonina C. and Li, Jerry and Musco, Cameron and Musco, Christopher},
	month = oct,
	year = {2019},
	note = {arXiv: 1905.05643},
	keywords = {Mathematics - Statistics Theory, Computer Science - Machine Learning, Computer Science - Data Structures and Algorithms, Electrical Engineering and Systems Science - Signal Processing},
	file = {Eldar et al. - 2019 - Sample Efficient Toeplitz Covariance Estimation.pdf:/home/redelogne/Zotero/storage/JHT4NHTU/Eldar et al. - 2019 - Sample Efficient Toeplitz Covariance Estimation.pdf:application/pdf},
}

@article{gribonval_blind_2011,
	title = {Blind calibration for compressed sensing by convex optimization},
	url = {http://arxiv.org/abs/1111.7248},
	abstract = {We consider the problem of calibrating a compressed sensing measurement system under the assumption that the decalibration consists in unknown gains on each measure. We focus on blind calibration, using measures performed on a few unknown (but sparse) signals. A naive formulation of this blind calibration problem, using 1 minimization, is reminiscent of blind source separation and dictionary learning, which are known to be highly non-convex and riddled with local minima. In the considered context, we show that in fact this formulation can be exactly expressed as a convex optimization problem, and can be solved using oﬀthe-shelf algorithms. Numerical simulations demonstrate the eﬀectiveness of the approach even for highly uncalibrated measures, when a suﬃcient number of (unknown, but sparse) calibrating signals is provided. We observe that the success/failure of the approach seems to obey sharp phase transitions.},
	language = {en},
	urldate = {2021-08-25},
	journal = {arXiv:1111.7248 [math, stat]},
	author = {Gribonval, Rémi and Chardon, Gilles and Daudet, Laurent},
	month = nov,
	year = {2011},
	note = {arXiv: 1111.7248},
	keywords = {Mathematics - Statistics Theory},
	file = {Gribonval et al. - 2011 - Blind calibration for compressed sensing by convex.pdf:/home/redelogne/Zotero/storage/QENHGAZR/Gribonval et al. - 2011 - Blind calibration for compressed sensing by convex.pdf:application/pdf},
}

@inproceedings{saade_random_2016,
  title={Random projections through multiple optical scattering: Approximating kernels at the speed of light},
  author={Saade, Alaa and Caltagirone, Francesco and Carron, Igor and Daudet, Laurent and Dr{\'e}meau, Ang{\'e}lique and Gigan, Sylvain and Krzakala, Florent},
  booktitle={2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={6215--6219},
  year={2016},
  organization={IEEE}
}


@article{rajaei_intensity-only_2016,
	title = {Intensity-only optical compressive imaging using a multiply scattering material and a double phase retrieval approach},
	url = {http://arxiv.org/abs/1510.01098},
	doi = {10.1109/ICASSP.2016.7472439},
	abstract = {In this paper, the problem of compressive imaging is addressed using natural randomization by means of a multiply scattering medium. To utilize the medium in this way, its corresponding transmission matrix must be estimated. To calibrate the imager, we use a digital micromirror device (DMD) as a simple, cheap, and high-resolution binary intensity modulator. We propose a phase retrieval algorithm which is well adapted to intensity-only measurements on the camera, and to the input binary intensity patterns, both to estimate the complex transmission matrix as well as image reconstruction. We demonstrate promising experimental results for the proposed algorithm using the MNIST dataset of handwritten digits as example images.},
	language = {en},
	urldate = {2021-08-25},
	journal = {2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
	author = {Rajaei, Boshra and Tramel, Eric W. and Gigan, Sylvain and Krzakala, Florent and Daudet, Laurent},
	month = mar,
	year = {2016},
	note = {arXiv: 1510.01098},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {4054--4058},
	file = {Rajaei et al. - 2016 - Intensity-only optical compressive imaging using a.pdf:/home/redelogne/Zotero/storage/G3X3C8X9/Rajaei et al. - 2016 - Intensity-only optical compressive imaging using a.pdf:application/pdf},
}

@article{liutkus2014imaging,
  title={Imaging with nature: Compressive imaging using a multiply scattering medium},
  author={Liutkus, Antoine and Martina, David and Popoff, S{\'e}bastien and Chardon, Gilles and Katz, Ori and Lerosey, Geoffroy and Gigan, Sylvain and Daudet, Laurent and Carron, Igor},
  journal={Scientific reports},
  volume={4},
  number={1},
  pages={1--7},
  year={2014},
  publisher={Nature Publishing Group}
}

@inproceedings{ohana_kernel_2020,
  title={Kernel computations from large-scale random features obtained by optical processing units},
  author={Ohana, Ruben and Wacker, Jonas and Dong, Jonathan and Marmin, S{\'e}bastien and Krzakala, Florent and Filippone, Maurizio and Daudet, Laurent},
  booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={9294--9298},
  year={2020},
  organization={IEEE}
}

@article{launay_light---loop_2020,
	title = {Light-in-the-loop: using a photonics co-processor for scalable training of neural networks},
	shorttitle = {Light-in-the-loop},
	url = {http://arxiv.org/abs/2006.01475},
	abstract = {As neural networks grow larger and more complex and data-hungry, training costs are skyrocketing. Especially when lifelong learning is necessary, such as in recommender systems or self-driving cars, this might soon become unsustainable. In this study, we present the ﬁrst optical co-processor able to accelerate the training phase of digitally-implemented neural networks. We rely on direct feedback alignment as an alternative to backpropagation, and perform the error projection step optically. Leveraging the optical random projections delivered by our coprocessor, we demonstrate its use to train a neural network for handwritten digits recognition.},
	language = {en},
	urldate = {2021-08-25},
	journal = {arXiv:2006.01475 [cs, eess, stat]},
	author = {Launay, Julien and Poli, Iacopo and Müller, Kilian and Carron, Igor and Daudet, Laurent and Krzakala, Florent and Gigan, Sylvain},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.01475},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, Computer Science - Emerging Technologies, Electrical Engineering and Systems Science - Image and Video Processing},
	annote = {Comment: 2 pages, 1 figure},
	file = {Launay et al. - 2020 - Light-in-the-loop using a photonics co-processor .pdf:/home/redelogne/Zotero/storage/NJEW6AQ8/Launay et al. - 2020 - Light-in-the-loop using a photonics co-processor .pdf:application/pdf},
}

@article{ghanem_fast_2020,
	title = {Fast {Graph} {Kernel} with {Optical} {Random} {Features}},
	url = {http://arxiv.org/abs/2010.08270},
	abstract = {The graphlet kernel is a classical method in graph classiﬁcation. It however suffers from a high computation cost due to the isomorphism test it includes. As a generic proxy, and in general at the cost of losing some information, this test can be efﬁciently replaced by a user-deﬁned mapping that computes various graph characteristics. In this paper, we propose to leverage kernel random features within the graphlet framework, and establish a theoretical link with a mean kernel metric. If this method can still be prohibitively costly for usual random features, we then incorporate optical random features that can be computed in constant time. Experiments show that the resulting algorithm is orders of magnitude faster that the graphlet kernel for the same, or better, accuracy.},
	language = {en},
	urldate = {2021-08-25},
	journal = {arXiv:2010.08270 [cs, stat]},
	author = {Ghanem, Hashem and Keriven, Nicolas and Tremblay, Nicolas},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.08270},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
	file = {Ghanem et al. - 2020 - Fast Graph Kernel with Optical Random Features.pdf:/home/redelogne/Zotero/storage/TLCH7C6H/Ghanem et al. - 2020 - Fast Graph Kernel with Optical Random Features.pdf:application/pdf},
}

@article{ohana_photonic_2021,
	title = {Photonic {Differential} {Privacy} with {Direct} {Feedback} {Alignment}},
	url = {http://arxiv.org/abs/2106.03645},
	abstract = {Optical Processing Units (OPUs) – low-power photonic chips dedicated to large scale random projections – have been used in previous work to train deep neural networks using Direct Feedback Alignment (DFA), an effective alternative to backpropagation. Here, we demonstrate how to leverage the intrinsic noise of optical random projections to build a differentially private DFA mechanism, making OPUs a solution of choice to provide a private-by-design training. We provide a theoretical analysis of our adaptive privacy mechanism, carefully measuring how the noise of optical random projections propagates in the process and gives rise to provable Differential Privacy. Finally, we conduct experiments demonstrating the ability of our learning procedure to achieve solid end-task performance.},
	language = {en},
	urldate = {2021-08-25},
	journal = {arXiv:2106.03645 [cs]},
	author = {Ohana, Ruben and Ruiz, Hamlet J. Medina and Launay, Julien and Cappelli, Alessandro and Poli, Iacopo and Ralaivola, Liva and Rakotomamonjy, Alain},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.03645},
	keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	file = {Ohana et al. - 2021 - Photonic Differential Privacy with Direct Feedback.pdf:/home/redelogne/Zotero/storage/AD79TMQS/Ohana et al. - 2021 - Photonic Differential Privacy with Direct Feedback.pdf:application/pdf},
}

@article{hesslow_photonic_2021,
	title = {Photonic co-processors in {HPC}: using {LightOn} {OPUs} for {Randomized} {Numerical} {Linear} {Algebra}},
	shorttitle = {Photonic co-processors in {HPC}},
	url = {http://arxiv.org/abs/2104.14429},
	abstract = {Randomized Numerical Linear Algebra (RandNLA) is a powerful class of methods, widely used in High Performance Computing (HPC). RandNLA provides approximate solutions to linear algebra functions applied to large signals, at reduced computational costs. However, the randomization step for dimensionality reduction may itself become the computational bottleneck on traditional hardware. Leveraging near constanttime linear random projections delivered by LightOn Optical Processing Units we show that randomization can be signiﬁcantly accelerated, at negligible precision loss, in a wide range of important RandNLA algorithms, such as RandSVD or trace estimators.},
	language = {en},
	urldate = {2021-08-25},
	journal = {arXiv:2104.14429 [cs, stat]},
	author = {Hesslow, Daniel and Cappelli, Alessandro and Carron, Igor and Daudet, Laurent and Lafargue, Raphaël and Müller, Kilian and Ohana, Ruben and Pariente, Gustave and Poli, Iacopo},
	month = may,
	year = {2021},
	note = {arXiv: 2104.14429},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
	annote = {Comment: Add "This project has received funding from the European Union's Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie grant agreement No 860830"},
	file = {Hesslow et al. - 2021 - Photonic co-processors in HPC using LightOn OPUs .pdf:/home/redelogne/Zotero/storage/GWF2BHIW/Hesslow et al. - 2021 - Photonic co-processors in HPC using LightOn OPUs .pdf:application/pdf},
}

@article{dremeau_reference-less_2015,
	title = {Reference-less measurement of the transmission matrix of a highly scattering material using a {DMD} and phase retrieval techniques},
	volume = {23},
	issn = {1094-4087},
	url = {http://arxiv.org/abs/1502.03324},
	doi = {10.1364/OE.23.011898},
	abstract = {This paper investigates experimental means of measuring the transmission matrix (TM) of a highly scattering medium, with the simplest optical setup. Spatial light modulation is performed by a digital micromirror device (DMD), allowing high rates and high pixel counts but only binary amplitude modulation. We used intensity measurement only, thus avoiding the need for a reference beam. Therefore, the phase of the TM has to be estimated through signal processing techniques of phase retrieval. Here, we compare four different phase retrieval principles on noisy experimental data. We validate our estimations of the TM on three criteria : quality of prediction, distribution of singular values, and quality of focusing. Results indicate that Bayesian phase retrieval algorithms with variational approaches provide a good tradeoff between the computational complexity and the precision of the estimates.},
	language = {en},
	number = {9},
	urldate = {2021-08-25},
	journal = {Optics Express},
	author = {Dremeau, Angelique and Liutkus, Antoine and Martina, David and Katz, Ori and Schulke, Christophe and Krzakala, Florent and Gigan, Sylvain and Daudet, Laurent},
	month = may,
	year = {2015},
	note = {arXiv: 1502.03324},
	keywords = {Physics - Optics},
	pages = {11898},
	file = {Dremeau et al. - 2015 - Reference-less measurement of the transmission mat.pdf:/home/redelogne/Zotero/storage/3RVVX49C/Dremeau et al. - 2015 - Reference-less measurement of the transmission mat.pdf:application/pdf;Dremeau et al. - 2015 - Reference-less measurement of the transmission mat.pdf:/home/redelogne/Zotero/storage/SJDZI95V/Dremeau et al. - 2015 - Reference-less measurement of the transmission mat.pdf:application/pdf},
}

@article{schellekens_breaking_2021,
	title = {Breaking the waves: asymmetric random periodic features for low-bitrate kernel machines},
	issn = {2049-8772},
	shorttitle = {Breaking the waves},
	url = {http://arxiv.org/abs/2004.06560},
	doi = {10.1093/imaiai/iaab008},
	abstract = {Many signal processing and machine learning applications are built from evaluating a kernel on pairs of signals, e.g., to assess the similarity of an incoming query to a database of known signals. This nonlinear evaluation can be simpliﬁed to a linear inner product of the random Fourier features of those signals: random projections followed by a periodic map, the complex exponential. It is known that a simple quantization of those features (corresponding to replacing the complex exponential by a diﬀerent periodic map that takes binary values, which is appealing for their transmission and storage), distorts the approximated kernel, which may be undesirable in practice. Our take-home message is that when the features of only one of the two signals are quantized, the original kernel is recovered without distortion; its practical interest appears in several cases where the kernel evaluations are asymmetric by nature, such as a client-server scheme.},
	language = {en},
	urldate = {2021-08-25},
	journal = {Information and Inference: A Journal of the IMA},
	author = {Schellekens, Vincent and Jacques, Laurent},
	month = may,
	year = {2021},
	note = {arXiv: 2004.06560},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Signal Processing},
	pages = {iaab008},
	file = {Schellekens and Jacques - 2021 - Breaking the waves asymmetric random periodic fea.pdf:/home/redelogne/Zotero/storage/7772ARNV/Schellekens and Jacques - 2021 - Breaking the waves asymmetric random periodic fea.pdf:application/pdf},
}

@article{kar_random_nodate,
	title = {Random {Feature} {Maps} for {Dot} {Product} {Kernels}},
	abstract = {Approximating non-linear kernels using feature maps has gained a lot of interest in recent years due to applications in reducing training and testing times of SVM classiﬁers and other kernel based learning algorithms. We extend this line of work and present low distortion embeddings for dot product kernels into linear Euclidean spaces. We base our results on a classical result in harmonic analysis characterizing all dot product kernels and use it to deﬁne randomized feature maps into explicit low dimensional Euclidean spaces in which the native dot product provides an approximation to the dot product kernel with high conﬁdence.},
	language = {en},
	author = {Kar, Purushottam and Karnick, Harish},
	pages = {9},
	file = {Kar and Karnick - Random Feature Maps for Dot Product Kernels.pdf:/home/redelogne/Zotero/storage/RMTEKYI4/Kar and Karnick - Random Feature Maps for Dot Product Kernels.pdf:application/pdf},
}

@article{kar_random_nodate-1,
	title = {Random {Feature} {Maps} for {Dot} {Product} {Kernels} {Supplementary} {Material}},
	abstract = {This document contains detailed proofs of theorems stated in the main article entitled Random Feature Maps for Dot Product Kernels.},
	language = {en},
	author = {Kar, Purushottam and Karnick, Harish},
	pages = {8},
	file = {Kar and Karnick - Random Feature Maps for Dot Product Kernels Supple.pdf:/home/redelogne/Zotero/storage/DNBQF3ZY/Kar and Karnick - Random Feature Maps for Dot Product Kernels Supple.pdf:application/pdf},
}

@article{li_one-sketch-for-all_nodate,
	title = {One-{Sketch}-for-{All}: {Non}-linear {Random} {Features} from {Compressed} {Linear} {Measurements}},
	language = {en},
	author = {Li, Xiaoyun and Li, Ping},
	pages = {11},
	file = {Li and Li - One-Sketch-for-All Non-linear Random Features fro.pdf:/home/redelogne/Zotero/storage/X48YMSQB/Li and Li - One-Sketch-for-All Non-linear Random Features fro.pdf:application/pdf},
}

@inproceedings{rahimi_random_nodate,
 author = {Rahimi, Ali and Recht, Benjamin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Random Features for Large-Scale Kernel Machines},
 volume = {20},
 year = {2007}
}




@article{choromanski_rethinking_2021,
	title = {Rethinking {Attention} with {Performers}},
	url = {http://arxiv.org/abs/2009.14794},
	abstract = {We introduce Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear (as opposed to quadratic) space and time complexity, without relying on any priors such as sparsity or low-rankness. To approximate softmax attentionkernels, Performers use a novel Fast Attention Via positive Orthogonal Random features approach (FAVOR+), which may be of independent interest for scalable kernel methods. FAVOR+ can also be used to efﬁciently model kernelizable attention mechanisms beyond softmax. This representational power is crucial to accurately compare softmax with other kernels for the ﬁrst time on large-scale tasks, beyond the reach of regular Transformers, and investigate optimal attention-kernels. Performers are linear architectures fully compatible with regular Transformers and with strong theoretical guarantees: unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence and low estimation variance. We tested Performers on a rich set of tasks stretching from pixel-prediction through text models to protein sequence modeling. We demonstrate competitive results with other examined efﬁcient sparse and dense attention methods, showcasing effectiveness of the novel attention-learning paradigm leveraged by Performers.},
	language = {en},
	urldate = {2021-08-25},
	journal = {arXiv:2009.14794 [cs, stat]},
	author = {Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared and Mohiuddin, Afroz and Kaiser, Lukasz and Belanger, David and Colwell, Lucy and Weller, Adrian},
	month = mar,
	year = {2021},
	note = {arXiv: 2009.14794},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: Published as a conference paper + oral presentation at ICLR 2021. 38 pages. See https://github.com/google-research/google-research/tree/master/protein\_lm for protein language model code, and https://github.com/google-research/google-research/tree/master/performer for Performer code. See https://ai.googleblog.com/2020/10/rethinking-attention-with-performers.html for Google AI Blog},
	file = {Choromanski et al. - 2021 - Rethinking Attention with Performers.pdf:/home/redelogne/Zotero/storage/DUGHCA9C/Choromanski et al. - 2021 - Rethinking Attention with Performers.pdf:application/pdf},
}

@article{zhang_sigma-delta_2021,
	title = {Sigma-{Delta} and {Distributed} {Noise}-{Shaping} {Quantization} {Methods} for {Random} {Fourier} {Features}},
	url = {http://arxiv.org/abs/2106.02614},
	abstract = {We propose the use of low bit-depth Sigma-Delta and distributed noise-shaping methods for quantizing the Random Fourier features (RFFs) associated with shiftinvariant kernels. We prove that our quantized RFFs – even in the case of 1-bit quantization – allow a high accuracy approximation of the underlying kernels, and the approximation error decays at least polynomially fast as the dimension of the RFFs increases. We also show that the quantized RFFs can be further compressed, yielding an excellent trade-off between memory use and accuracy. Namely, the approximation error now decays exponentially as a function of the bits used. Moreover, we empirically show by testing the performance of our methods on several machine learning tasks that our method compares favorably to other state of the art quantization methods in this context.},
	language = {en},
	urldate = {2021-08-25},
	journal = {arXiv:2106.02614 [cs, math, stat]},
	author = {Zhang, Jinjie and Cloninger, Alexander and Saab, Rayan},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.02614},
	keywords = {Computer Science - Information Theory, Statistics - Machine Learning, Computer Science - Machine Learning},
	file = {Zhang et al. - 2021 - Sigma-Delta and Distributed Noise-Shaping Quantiza.pdf:/home/redelogne/Zotero/storage/LXCCC6RY/Zhang et al. - 2021 - Sigma-Delta and Distributed Noise-Shaping Quantiza.pdf:application/pdf},
}

@article{baraniuk_simple_2008,
  title={A simple proof of the restricted isometry property for random matrices},
  author={Baraniuk, Richard and Davenport, Mark and DeVore, Ronald and Wakin, Michael},
  journal={Constructive Approximation},
  volume={28},
  number={3},
  pages={253--263},
  year={2008},
  publisher={Springer}
}



@article{cucker_mathematical_2001,
	title = {On the mathematical foundations of learning},
	volume = {39},
	issn = {0273-0979, 1088-9485},
	url = {https://www.ams.org/bull/2002-39-01/S0273-0979-01-00923-5/},
	doi = {10.1090/S0273-0979-01-00923-5},
	language = {en},
	number = {1},
	urldate = {2021-08-25},
	journal = {Bulletin of the American Mathematical Society},
	author = {Cucker, Felipe and Smale, Steve},
	month = oct,
	year = {2001},
	pages = {1--49},
	file = {Cucker and Smale - 2001 - On the mathematical foundations of learning.pdf:/home/redelogne/Zotero/storage/KLSWNYBA/Cucker and Smale - 2001 - On the mathematical foundations of learning.pdf:application/pdf},
}

@article{schaback_kernel_2006,
	title = {Kernel techniques: {From} machine learning to meshless methods},
	volume = {15},
	issn = {0962-4929, 1474-0508},
	shorttitle = {Kernel techniques},
	url = {https://www.cambridge.org/core/product/identifier/S0962492906270016/type/journal_article},
	doi = {10.1017/S0962492906270016},
	abstract = {Kernels are valuable tools in various fields of numerical analysis, including approximation, interpolation, meshless methods for solving partial differential equations, neural networks, and machine learning. This contribution explains why and how kernels are applied in these disciplines. It uncovers the links between them, in so far as they are related to kernel techniques. It addresses non-expert readers and focuses on practical guidelines for using kernels in applications.},
	language = {en},
	urldate = {2021-08-25},
	journal = {Acta Numerica},
	author = {Schaback, Robert and Wendland, Holger},
	month = may,
	year = {2006},
	pages = {543--639},
	file = {Schaback and Wendland - 2006 - Kernel techniques From machine learning to meshle.pdf:/home/redelogne/Zotero/storage/AWBE6YUE/Schaback and Wendland - 2006 - Kernel techniques From machine learning to meshle.pdf:application/pdf},
}

@inproceedings{hallac_toeplitz_2017,
	address = {Halifax NS Canada},
	title = {Toeplitz {Inverse} {Covariance}-{Based} {Clustering} of {Multivariate} {Time} {Series} {Data}},
	isbn = {978-1-4503-4887-4},
	url = {https://dl.acm.org/doi/10.1145/3097983.3098060},
	doi = {10.1145/3097983.3098060},
	abstract = {Subsequence clustering of multivariate time series is a useful tool for discovering repeated patterns in temporal data. Once these patterns have been discovered, seemingly complicated datasets can be interpreted as a temporal sequence of only a small number of states, or clusters. For example, raw sensor data from a fitness-tracking application can be expressed as a timeline of a select few actions (i.e., walking, sitting, running). However, discovering these patterns is challenging because it requires simultaneous segmentation and clustering of the time series. Furthermore, interpreting the resulting clusters is difficult, especially when the data is high-dimensional. Here we propose a new method of model-based clustering, which we call Toeplitz Inverse Covariance-based Clustering (TICC). Each cluster in the TICC method is defined by a correlation network, or Markov random field (MRF), characterizing the interdependencies between different observations in a typical subsequence of that cluster. Based on this graphical representation, TICC simultaneously segments and clusters the time series data. We solve the TICC problem through alternating minimization, using a variation of the expectation maximization (EM) algorithm. We derive closed-form solutions to efficiently solve the two resulting subproblems in a scalable way, through dynamic programming and the alternating direction method of multipliers (ADMM), respectively. We validate our approach by comparing TICC to several state-of-the-art baselines in a series of synthetic experiments, and we then demonstrate on an automobile sensor dataset how TICC can be used to learn interpretable clusters in real-world scenarios.},
	language = {en},
	urldate = {2021-08-26},
	booktitle = {Proceedings of the 23rd {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {ACM},
	author = {Hallac, David and Vare, Sagar and Boyd, Stephen and Leskovec, Jure},
	month = aug,
	year = {2017},
	pages = {215--223},
	file = {Hallac et al. - 2017 - Toeplitz Inverse Covariance-Based Clustering of Mu.pdf:/home/redelogne/Zotero/storage/XMLFWQ74/Hallac et al. - 2017 - Toeplitz Inverse Covariance-Based Clustering of Mu.pdf:application/pdf},
}



@article{dasgupta_elementary_2003,
	title = {An elementary proof of a theorem of {Johnson} and {Lindenstrauss}},
	volume = {22},
	issn = {1042-9832, 1098-2418},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/rsa.10073},
	doi = {10.1002/rsa.10073},
	language = {en},
	number = {1},
	urldate = {2021-10-04},
	journal = {Random Structures and Algorithms},
	author = {Dasgupta, Sanjoy and Gupta, Anupam},
	month = jan,
	year = {2003},
	pages = {60--65},
	file = {Dasgupta and Gupta - 2003 - An elementary proof of a theorem of Johnson and Li.pdf:/home/redelogne/Zotero/storage/W4RWZ9WW/Dasgupta and Gupta - 2003 - An elementary proof of a theorem of Johnson and Li.pdf:application/pdf},
}

@article{keriven_sketching_nodate,
	title = {Sketching for large-scale learning of mixture models},
	language = {fr},
	author = {Keriven, Nicolas},
	pages = {188},
	file = {Keriven - Sketching for large-scale learning of mixture mode.pdf:/home/redelogne/Zotero/storage/FIRWH8CH/Keriven - Sketching for large-scale learning of mixture mode.pdf:application/pdf},
}

@inproceedings{achlioptas_database-friendly_2001,
  title={Database-friendly random projections},
  author={Achlioptas, Dimitris},
  booktitle={Proceedings of the twentieth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems},
  pages={274--281},
  year={2001}
}



@article{gharan_lecture_nodate,
	title = {Lecture 7: {Schwartz}-{Zippel} {Lemma}, {Perfect} {Matching}},
	language = {en},
	author = {Gharan, Shayan Oveis},
	pages = {8},
	file = {Gharan - Lecture 7 Schwartz-Zippel Lemma, Perfect Matching.pdf:/home/redelogne/Zotero/storage/RJLU4XTA/Gharan - Lecture 7 Schwartz-Zippel Lemma, Perfect Matching.pdf:application/pdf},
}

@article{keriven_newma_2020,
	title = {{NEWMA}: a new method for scalable model-free online change-point detection},
	shorttitle = {{NEWMA}},
	url = {http://arxiv.org/abs/1805.08061},
	abstract = {We consider the problem of detecting abrupt changes in the distribution of a multi-dimensional time series, with limited computing power and memory. In this paper, we propose a new, simple method for model-free online change-point detection that relies only on fast and light recursive statistics, inspired by the classical Exponential Weighted Moving Average algorithm (EWMA). The proposed idea is to compute two EWMA statistics on the stream of data with diﬀerent forgetting factors, and to compare them. By doing so, we show that we implicitly compare recent samples with older ones, without the need to explicitly store them. Additionally, we leverage Random Features (RFs) to eﬃciently use the Maximum Mean Discrepancy as a distance between distributions, furthermore exploiting recent optical hardware to compute high-dimensional RFs in near constant time. We show that our method is signiﬁcantly faster than usual non-parametric methods for a given accuracy.},
	language = {en},
	urldate = {2021-10-18},
	journal = {arXiv:1805.08061 [cs, stat]},
	author = {Keriven, Nicolas and Garreau, Damien and Poli, Iacopo},
	month = apr,
	year = {2020},
	note = {arXiv: 1805.08061},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Keriven et al. - 2020 - NEWMA a new method for scalable model-free online.pdf:/home/redelogne/Zotero/storage/QV9BBIUG/Keriven et al. - 2020 - NEWMA a new method for scalable model-free online.pdf:application/pdf},
}

@article{gretton_kernel_2008,
	title = {A {Kernel} {Method} for the {Two}-{Sample} {Problem}},
	url = {http://arxiv.org/abs/0805.2368},
	abstract = {We propose a framework for analyzing and comparing distributions, allowing us to design statistical tests to determine if two samples are drawn from diﬀerent distributions. Our test statistic is the largest diﬀerence in expectations over functions in the unit ball of a reproducing kernel Hilbert space (RKHS). We present two tests based on large deviation bounds for the test statistic, while a third is based on the asymptotic distribution of this statistic. The test statistic can be computed in quadratic time, although eﬃcient linear time approximations are available. Several classical metrics on distributions are recovered when the function space used to compute the diﬀerence in expectations is allowed to be more general (eg. a Banach space). We apply our two-sample tests to a variety of problems, including attribute matching for databases using the Hungarian marriage method, where they perform strongly. Excellent performance is also obtained when comparing distributions over graphs, for which these are the ﬁrst such tests.},
	language = {en},
	urldate = {2021-10-18},
	journal = {arXiv:0805.2368 [cs]},
	author = {Gretton, Arthur and Borgwardt, Karsten and Rasch, Malte J. and Scholkopf, Bernhard and Smola, Alexander J.},
	month = may,
	year = {2008},
	note = {arXiv: 0805.2368},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, G.3, I.2.6},
	file = {Gretton et al. - 2008 - A Kernel Method for the Two-Sample Problem.pdf:/home/redelogne/Zotero/storage/J9NJQ8W4/Gretton et al. - 2008 - A Kernel Method for the Two-Sample Problem.pdf:application/pdf},
}


@article{salmon_2013,
	author = {Salmon, B.P. and Kleynhans, W. and Bergh, Frans and Olivier, Jan and Grobler, Trienko and Wessels, Konrad},
	year = {2013},
	month = {06},
	pages = {1079-1085},
	title = {Land Cover Change Detection Using the Internal Covariance Matrix of the Extended Kalman Filter Over Multiple Spectral Bands},
	volume = {6},
	journal = {Selected Topics in Applied Earth Observations and Remote Sensing, IEEE Journal of},
	doi = {10.1109/JSTARS.2013.2241023}
}

@article{ramsay_cov,author={K. Ramsay, S. Chenouri}, title={Robust multiple change-point detection for covariance matrices using data depth},year={2021},}


@article{SPWCM,
  title={Signal processing with compressive measurements},
  author={Davenport, Mark A and Boufounos, Petros T and Wakin, Michael B and Baraniuk, Richard G},
  journal={IEEE Journal of Selected topics in Signal processing},
  volume={4},
  number={2},
  pages={445--460},
  year={2010},
  publisher={IEEE}
}




@article{esann22, author = {Delogne, R. and Schellekens, V. and Jacques, L.}, year ={2022}, title={{ROP} inception: signal estimation with quadratic random sketching},
journal={European Symposium on Artificial Neural Networks}, url={https://arxiv.org/abs/2205.08225}
}

@article{icassp23, author = {Delogne R. and Schellekens V., Daudet L. and Jacques L.}, year ={2022}, title={Signal Processing with Quadratic Optical Sketches},
journal={Intarnational conference on acoustics and signal processing 2023}, url={https://arxiv.org/abs/2212.00660}
}


@article{phaselift, author = {E.J. Candès and T.  Strohmer and V. Voroninski}, year ={2013}, title={PhaseLift: Exact and Stable Signal Recovery from Magnitude Measurements via Convex Programming},
journal={Comm. Pure Appl. Math.}, pages={66: 1241-1274}
}

@article{jacques_spe, author = {L. Jacques and K. Degraux and C. De Vleeschouwer}, year ={2013}, title={Quantized Iterative Hard Thresholding: Bridging 1-bit and High-Resolution Quantized Compressed Sensing},
journal={Proceedings of International Conference on Sampling Theory and Applications}, pages={p.105-108}
}

@article{mnist, author = {L. Deng}, year ={2012}, title={The {MNIST} database of handwritten digit images for machine learning research [best of the web]},
journal={IEEE signal processing magazine}, pages={vol. 29, no 6, p. 141-142}
}


@article{ka03, author = {V. Kastrinaki, M. Zervakis,  K. Kalaitzakis},  title={A survey of video processing techniques for traffic applications},
journal={Image and vision computing}, pages={21(4), 359-381},
year={2003}
}

@misc{jacques_spe_bis,
  doi = {10.48550/ARXIV.1305.1786},
  url = {https://arxiv.org/abs/1305.1786},
  author = {Jacques, Laurent and Degraux, Kévin and De Vleeschouwer, Christophe},
  keywords = {Information Theory (cs.IT), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Quantized Iterative Hard Thresholding: Bridging 1-bit and High-Resolution Quantized Compressed Sensing},
  journal = {Proceedings of International Conference on Sampling Theory and Applications},
  pages={105-108},
  year = {2013},

}

@InProceedings{foucart_flavors,
author="Foucart, Simon",
editor="Fasshauer, Gregory E.
and Schumaker, Larry L.",
title="Flavors of Compressive Sensing",
booktitle="Approximation Theory XV: San Antonio 2016",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="61--104",
abstract="About a decade ago, a couple of groundbreaking articles revealed the possibility of faithfully recovering high-dimensional signals from some seemingly incomplete information about them. Perhaps more importantly, practical procedures to perform the recovery were also provided. These realizations had a tremendous impact in science and engineering. They gave rise to a field called `compressive sensing,' which is now in a mature state and whose foundations rely on an elegant mathematical theory. This survey presents an overview of the field, accentuating elements from approximation theory, but also highlighting connections with other disciplines that have enriched the theory, e.g., statistics, sampling theory, probability, optimization, metagenomics, graph theory, frame theory, and Banach space geometry.",
isbn="978-3-319-59912-0"
}
