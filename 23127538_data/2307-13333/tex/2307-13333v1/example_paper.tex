%%%%%%%% ICML 2023 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2023} with \usepackage[nohyperref]{icml2023} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage[accepted]{icml2023}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2023}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

\usepackage{multicol}
\usepackage{multirow}
%\usepackage{subcaption}
\usepackage{url}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Feature Importance Measurement based on Decision Tree Sampling}

\begin{document}

\twocolumn[
\icmltitle{Feature Importance Measurement based on Decision Tree Sampling}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2023
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
%\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Chao Huang}{a}
\icmlauthor{Diptesh Das}{a}
\icmlauthor{Koji Tsuda}{a,b,c}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{a}{Graduate School of Frontier Sciences, The University of Tokyo, 5-1-5 Kashiwanoha,
Kashiwa 277-8561, Japan}
\icmlaffiliation{b}{Center for Basic Research on Materials, National Institute for Materials Science, 1-1
Namiki, Tsukuba, Ibaraki 305-0044, Japan}
\icmlaffiliation{c}{RIKEN Center for Advanced Intelligence Project, 1-4-1 Nihonbashi, Chuo-ku, Tokyo
103-0027, Japan}


\icmlcorrespondingauthor{Koji Tsuda}{tsuda@k.u-tokyo.ac.jp}
%\icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Decision tree, Interpretability, Random Forest, SAT, Reliability, Trustworthy AI}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Random forest is effective for prediction tasks but the randomness of tree generation hinders interpretability in feature importance analysis. To address this, we proposed DT-Sampler, a SAT-based method for measuring feature importance in tree-based model. Our method has fewer parameters than random forest and provides higher interpretability and stability for the analysis in real-world problems. An implementation of DT-Sampler is available at \url{https://github.com/tsudalab/DT-sampler}.% \looseness=-1
\end{abstract}

\section{Introduction}
\label{Introduction}
% Feature importance in tree-based models always rely on random forest \cite{breiman2001random}, an algorithm that can generate a lot of decision trees by randomly choosing a  subset of the features and the training data. Even though random forest is a powerful tool for feature importance measurement, we think it faces three drawbacks.

%1. The random forest algorithm generates an ensemble of such decision trees, where each tree is constructed using a random subset of the features and a random subset of the training data. However, this is also heuristic and hard to be interpreted statistically.

%2. The interpretation of feature importance can be influenced by the randomness of decision tree generation. For instance, when using scikit-learn to build random forest and then measure the feature importance, the results are always influenced by the parameters such as $criterion$, $max\_leaf\_nodes$, $max\_depth$, $random\_state$ and so on. There are also different methods to calculate feature importance such as mean decrease impurity and permutation importance, which also lead to total different results sometimes.

%3. The methods commonly used to generate decision trees, e.g. ID3\cite{quinlan1986induction}, C4.5\cite{quinlan2014c4} and CART\cite{breiman1984classification}, are difficult to generate small trees since the process of selecting the optimal split at each node is based on heuristics rather than a purely mathematical or statistical approach.\\

Interpretable machine learning (ML) models are paramount for their seamless integration in high stake decision making problems e.g., medical diagnosis~\cite{lakkaraju2016interpretable,nemati2018interpretable,ahmad2018interpretable,das2019interpretable,das2022fast,das2022confidence,adadi2020explainable}, criminal justice~\cite{angelino2017learning,wang2022pursuit,liu2022fasterrisk}, etc. In medical diagnosis, specially in computer assisted diagnosis (CAD), model accuracy is important, but it is equally important for the doctor and the patient to know the features used in CAD modeling~\cite{goodman2017european,rudin2019stop}. There have been several established feature selection (FS) algorithms in ML literature, namely LASSO~\cite{tibshirani1996regression}, marginal screening (MS)~\cite{fan2008sure}, orthogonal matching pursuit (OMP)~\cite{pati1993orthogonal}, decision tree (DT) based, etc. Among them, DT-based FS have been widely studied due to their high interpretability~\cite{quinlan1986induction,quinlan2014c4,breiman1984classification}.\looseness=-1 

Constructing an accurate and a small size (hence, better interpretability) DT is a challenging problem, and has been an active area of research over last four decades. Most of the existing methods are ad hoc, and do not have explicit control over the size and accuracy of a DT. For e.g., there are greedy splitting-based~%\cite{rivest1987learning}
\cite{quinlan1986induction,quinlan2014c4,breiman1984classification}, Bayesian-based~\cite{denison1998bayesian,chipman1998bayesian,chipman2002bayesian,chipman2010bart,letham2015interpretable}, branch-and-bound methods~\cite{angelino2017learning} for DT construction.\looseness=-1 

Random forest (RF)\cite{breiman2001random} is a widely used ensemble decision tree FS method. While RF has shown improvements in prediction accuracy and mitigating overfitting risk, due to the heuristic algorithms of decision tree generation, it often faces challenges such as the preference for larger trees, lack of statistical interpretability, randomness in feature importance measurement due the influence of many parameters, etc. \looseness=-1 

%Most of these existing algorithms aim to find best (accuracy and size) DT by searching over an exponential search space of DT. Recent studies such as Bayesian methods~\cite{chipman2010bart} can reduce the search space, but produces sub-optimal DT.  The branch-and-bound method~\cite{angelino2017learning} can generate optimal DT by exploiting a number of theoretical bounds (e.g., an upper bound on the node size), but do not provide explicit control over accuracy or node size.
%
%

To handle those challenges, we propose a DT-based FS methods where we allow the user (e.g., a domain expert) to explicitly control the size and accuracy of a DT. We leverage a Boolean satisfiability(SAT)\cite{biere2009handbook} encoding of a DT and perform uniform sampling of the SAT space with user-specified accuracy and size.Our method is a tree ensemble FS that generates small-size and high-accuracy decision trees, and determines the feature importance based on its emergence probability (i.e., the probability of a feature appearing in the high accuracy space). \looseness=-1 
%

Through numerical experiments, we evaluated our proposed method using four real-world dataset. We demonstrated that our method is capable of producing comparable accuracy as RF, but with small size DTs. We also compared our encoding with existing SAT-based encoding and demonstrated that our encoding scheme generates less variables and computationally more efficient.
% This is an early demonstration of our ongoing research where we only considered classification problems. However, our method can also be extended to decision tree clustering. 
%  
As we can uniformly sample form a high accuracy space with a specific tree size, this will allow us to formulate a statistical hypothesis testing framework to judge the significance of selected feature in terms of $p$-value and confidence interval which we consider as a potential future work.\looseness=-1  


 
% ensemble based FS methods have shown better efficacy in practice as an ensemble of predictors can better capture the complexity of real-world data over a single predictor. In this paper we are interested in decision tree (DT) ensemble method of FS. 
 
 
%In this paper we are interested in decision tree (DT) ensemble method of FS as an ensemble of predictors can better capture the complexity of real-world data over a single predictor.


%\paragraph{Contribution:}

%\textbf{\underline{Squeeze the ``red colored" text into the method section.}}

%\textcolor{red}{
%Decision trees are widely recognized by their high interpretability in solving real-world problems. The smaller tree sizes are always expected for the better connection between models and the decision process in real world. However, constructing the decision trees with highest training accuracy and smallest size is a NP-hard problem and most commonly used algorithms for constructing decision trees rely on heuristics\cite{quinlan1986induction} \cite{quinlan2014c4} \cite{breiman1984classification}, with no guarantee of the size and accuracy. While they show great capabilities in prediction tasks, the large amount of parameters and tendency to generate bigger decision trees impede their interpretability. To address these problems, approaches based on constraint programming, especially Boolean satisfiability(SAT) have been extensively developed \cite{bessiere2009minimising}\cite{ijcai2018p189}\cite{ijcai2020p662}\cite{inbook_opt_tree}. Boolean satisfiability problem is the problem of determining whether there exists an assignment of Boolean values (true/false) to a given set of Boolean variables that satisfies given Boolean formulas. By encoding the decision trees to a SAT problem, it's possible to generate smaller decision trees with high accuracy compared to the ones generated by heuristic rules.}

%\textcolor{red}{
 %Random forest\cite{breiman2001random} is a widely used ensemble model based on decision tree models. While it has shown improvements in prediction accuracy and mitigating overfitting risk, random forest faces bigger challenges due to the heuristic algorithms of decision tree generation including the preference for larger trees, lack of statistical interpretability, feature importance measurement influenced by different parameters and etc. In order to solve these problems, we aim to develop a tree-based ensemble model that can: 1. generate small and high-accuracy decision trees; 2. have less parameters; provide high interpretability for feature importance measurement.}



%\textcolor{red}{
% Then, we need to answer a question: how to obtain an ensemble of decision trees without any heuristic rules?\\
% Our answer is by using SAT sampling. Currently, the well-known SAT samplers\cite{inbook}\cite{10.1145/3180155.3180248}\cite{10.5555/3020652.3020682} can generate satisfiable solutions uniformly or near-uniformly. By constructing an appropriate encoding for decision trees, we can utilize SAT samplers to obtain an ensemble of decision trees sampled uniformly from a specific space. The method enhances interpretability since we have precise knowledge of the origin of each decision tree, as opposed to generating them using heuristic rules employed by random forest.}

%\textcolor{red}{
%Our primary contributions lie in:\\
%1.We introduced a sampling-based method for measuring feature importance in tree-based models. This method requires less parameters and provides higher interpretability than random forest.  \\
%2.We developed a SAT-encoding technique to represent decision tree structures and incorporate constraints that enable the generation of decision trees with specific desired accuracy.}


%\paragraph{Notations:} For any natural number $n$, we use $[n]=\{1, \ldots, n\}$.
\section{Method}
%Before introducing the details of our work, we want to clarify our definition of feature importance. We don't use any heuristic method to calculate feature importance and just simply calculate feature importance by considering the probability that this feature appears in the high accuracy space. For example, if the emergence probability of feature 1 is higher than feature 2 in the decision tree space with accuracy $\ge 90\%$, then we can say feature 1 is more importance than feature 2 in this space.

\subsection{Feature importance using DT sampling}
To naively determine the feature importance using decision tree (DT) sampling, one can enumerate all possible DT and find all the DTs exceeding an accuracy threshold. However, this naive approach is computationally prohibitive as the DT search space grows exponentially with the size of DT, see Table~\ref{The time to get all of decision trees} in appendix for details. 
Therefore, we propose a SAT based encoding of DT that reduces the search space significantly and also improves the sampling efficiency. 
%decision tree (DT) sampling method to determine the feature importance. 
The flow diagram of our method is shown in Figure \ref{workflow}. First, we encode DTs with specific size (\#node) and accuracy (threshold) as a SAT problem represented in conjunctive normal form(CNF). Once the SAT encoding of DTs is constructed, any solution that satisfies all the constraints in the CNF file can be decoded into a valid decision tree of specific size and accuracy. Then, we utilize SAT sampling to generate multiple decision trees and calculate feature importance (emergence probability) based on the sampling results. \looseness=-1 


%We utilize SAT samplers to generate decision trees in the specific space constrained by SAT encoding and uniformly sample the decision trees whose accuracy is larger than a given threshold. As depicted in Figure \ref{workflow}, first, we encode the decision trees with specific size and accuracy as a SAT problem represented in Conjunctive Normal Form(CNF). Once the SAT encoding of the decision tree is constructed, any solution that satisfies all the constraints in the CNF file can be decoded into a decision tree of the specific size and accuracy. Then, we employ SAT sampling to generate multiple decision trees and calculate feature importance based on the sampling results. 


%#of the SAT encoding can correspond to a decision tree space of specific accuracy and size. Then, we apply SAT sampling in our CNF file

%In other words, a decision tree belongs to this decision tree space if and only if its size and accuracy on the training set meet the requirements specified by the SAT encoding.

% Figure environment removed

\subsection{SAT-based DT encoding}
% In this section, we introduce how to encode decision tree into a SAT problem. Our encoding method is mostly based on the method proposed in \cite{ijcai2018p189} but we adopted a new method to encode tree structure that accelerates the process of SAT sampling largely. 
Constructing decision trees with high accuracy and small size is an active area of research in the domain of constraint programming, and many Boolean satisfiability(SAT)-based encodings have been proposed in the literature~\cite{bessiere2009minimising,ijcai2018p189,ijcai2020p662,inbook_opt_tree}. To reduce the search space and enable fast sampling we proposed an efficient SAT-encoding of DT. 
%
Our encoding method is motivated by the method proposed in~\cite{ijcai2018p189}, but developed a new
 encoding (only branch node encoding) scheme that accelerates the process of SAT sampling significantly. We also introduced additional variables and constraints to make it possible to encode the DTs with any accuracy that you want. Encoding DTs with only branch nodes is non-trivial, the details of which have been provided below.  \looseness=-1 
%

\begin{table}[h]
  \small
  \caption{Description of propositional variables}
  \label{Description of propositional variables}
  \centering

\begin{tabular}{|c|c|}
    \hline
  Var  & Description of variables \\ \hline
 $vl_i$  & \scriptsize 1 iff branch node i has a left branch child, $ i \in [N]$ \\ 
   $vr_i$  & \scriptsize 1 iff branch node i has a right branch child, $ i \in [N]$ \\  

   $l_{ij}$ & \scriptsize 1 iff node i has node j as the left child, \scriptsize with $j\in Child(i)$  \\ 
   $r_{ij}$ & \scriptsize 1 iff node i has node j as the right child, \scriptsize with $j\in Child(i)$ \\ \hline

 $lc_i$  & \scriptsize 1 iff class of the left leaf child of node i is 1, $ i \in [N]$ \\ 
$rc_i$  & \scriptsize 1 iff class of the right leaf child of node i is 1, $ i \in [N]$ \\ 
   \hline

 $a_{rj}$ & \scriptsize 1 iff feature $f_r$ is assigned to node j,  $ r \in [K]$, $ j \in [N]$ \\ 
$u_{rj}$ & \tiny 1 iff feature $f_r$ is being discriminated against by node j,  $ r \in [K]$, $ j \in [N]$  \\ \hline

\end{tabular}
\end{table}

\paragraph{SAT variables and constraints:}
We consider the encoding of decision trees with 2N+1 nodes and the training data consists of M samples and K features. Binary decision tree with 2N+1 nodes comprises N branch nodes and N+1 leaf nodes. The base method~\cite{ijcai2018p189} sets the node ID sequentially as showed in Figure \ref{Node ID in SAT encoding}.a. Since it cannot distinguish between branch and leaf nodes by node IDs, branch and leaf nodes are assigned equivalent variables and are differentiated by additional constraints. In order to simplify it, we propose a method that only takes the branch nodes into consideration, viewing the leaf nodes as one of the properties of branch nodes as depicted in Figure \ref{Node ID in SAT encoding}.b. \looseness=-1 

% Figure environment removed



 All the variables required to encode a DT are shown in Table \ref{Description of propositional variables}, the subscript i and j represent the node ID (or index) while the subscript r denotes the feature ID(or index). For any natural number $n$, we use $[m:n]=\{m, m+1, \ldots, n-1, n\}$. The Function defined as  $Child(i)=[i+1: \min (2i+1, N)]$  can return possible node IDs of the children of $i^{th}$ node. \looseness=-1 
%
%
% Figure environment removed



%\paragraph{SAT Constraints:}
There are four types of branch nodes as depicted in Figure \ref{Four types of branch nodes}. We use $vl_i$ (resp.$vr_i$) variable to denote whether the $i^{th}$ node has a left (resp. right) branch child or not. With $i \in [1:N]$ and $C \in \{0,1\}$:\looseness=-1 
\begin{equation} \label{formula 1}
\begin{split}
&vl_i= C \implies \sum_{j \in Child(i)} l_{ij} = C\\
%&vl_i=0 \implies \sum_{j \in Child(i)} l_{ij} = 0, \\
&vr_i= C \implies \sum_{j \in Child(i)} r_{ij} = C, \\
%&vr_i=0 \implies \sum_{j \in Child(i)} r_{ij} = 0.\\
\end{split}
\end{equation}

Every branch node (except root) has exactly one parent: 
\begin{equation} \label{formula 2}
\sum_{i=\lfloor \frac{j}{2}\rfloor}^{j-1} (l_{ij} + r_{ij}) = 1, \; with \; j \in [2:N]
\end{equation}



The IDs of branch nodes are assigned according to level order of the tree. For example, as shown in Figure \ref{sequential}, if $l_{35}=1$, then $l_{26}$ or $r_{26}$ must be 0, because $6^{th}$ node cannot appear in front of $5^{th}$ node. With $i \in [1:N-1]$, $j \in Child(i)$: \looseness=-1 
\begin{equation} \label{formula 3}
\begin{split}
&l_{ij} \lor r_{ij}\implies \sum_{h=1}^{i-1} \sum_{k=j}^{k=N}(l_{hk} + r_{hk})=0\\
&r_{ij}\implies \sum_{k=j}^{k=N}l_{ik} + \sum_{k=j+1}^{k=N}r_{ik}=0\\
\end{split}
\end{equation}

%
\begin{table*}[t]
%\small
%\setlength{\tabcolsep}{0.2em}
\caption{Comparison of encoding size. $\#b$,$\#f$ denotes the number of training samples and the number of features respectively. $\#var$ denotes the number of variables used to build the encoding of the decision tree. $\#var\_cnf$,$\#cls\_cnf$ denotes the number of variables and the number of clauses in the Conjunctive Normal Form(CNF) file generated by tseitin transformation\cite{Tseitin1983} provided in z3-solver\cite{de2008z3}. $\#Ave. time$ denotes the time to generate 100 samples by unigen. We ran all the experiments (including the base encoding) on Intel(R) Xeon(R) CPU E3-1270 v6 @ 3.80GHz.}
\label{comparison of encoding size}
%
\centering
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
\centering
Dataset & \#b & \#f & \#n & thres.& \#var & \#var\_cnf & \#cls\_cnf & Ave. time  \\ \hline
%\multirow{2}{*}{weather} & 14 & 10 & 9 & 1.00 & 422/186 & 1312/458 & 7053/2657 & 1.18/0.03 \\ 
%            & 14 & 10 & 11 & 0.90 & 527/236 & 2030/977 & 12211/9472 & 89.14/45.13 \\ \hline

\multirow{2}{*}{mouse} & 50 & 15 & 13 & 1.00 & 896/406 & 3599/1274 & 20586/8397 & 31.76/0.25 \\ 
     & 50 & 15 & 11 & 0.90 & 747/336 & 3260/1990 & 22890/24689 & 1028.02/567.08 \\ \hline

\multirow{2}{*}{car} 
%& 30 & 21 & 11 & 1.00 & 1011/456 & 3739/1328 & 21468/8779 & 140.48/66.55 \\ 
     & 40 & 10 & 17 & 1.00 & 866/390 & 3956/1406 & 21625/9495 & 260.68/65.47 \\ 
     & 50 & 15 & 11 & 0.90 & 747/336 & 3436/2152 & 26218/33473 & 1722.26/310.99 \\ \hline

\multirow{2}{*}{breast} & 40 & 15 & 17 & 1.00 & 1206/550 & 5821/2041 & 32400/13663 & 96.34/2.32 \\ 
            & 50 & 12 & 13 & 0.90 & 740/334 & 3759/2198 & 24732/27869 & 407.32/125.81 \\ \hline

\multirow{2}{*}{heart} & 40 & 19 & 13 & 1.00 & 1104/502 & 4572/1590 & 26063/10697 & 79.87/11.62 \\ 
            & 50 & 10 & 13 & 0.90 & 636/286 & 3241/2108 & 21568/25552 & 1671.18/327.89 \\ \hline


\end{tabular}

\end{table*}
%


At any branch node, exactly one feature is assigned.
\begin{equation} \label{formula 4}
\sum_{r=1}^K a_{rj}=1,\;  with \; j \in [1:N]
\end{equation}

Variable $u_{rj}$ has the information of whether the $r^{th}$ feature is discriminated at any node on the path from the root to this node. If the $r^{th}$ feature has already been assigned to one of ancestors, then it should not be assigned again. With $r \in [1:K]$, $j \in [1:N]$:

\begin{equation} \label{formula 5}
\begin{split}    
&\bigwedge_{i=\lfloor \frac{j}{2}\rfloor}^{j-1}(u_{ri}\land (l_{ij} \lor r_{ij}) \implies \neg a_{rj}) \\
&u_{rj} \iff (a_{rj} \lor \bigvee_{i=\lfloor \frac{i}{2}\rfloor}^{j-1}(u_{ri}\land (l_{ij} \lor r_{ij})))
\end{split}
\end{equation}


The encoding given by Formula (\ref{formula 1})-(\ref{formula 5}) specify a space including all of valid decision trees of a given size but can't learn from the training data. To learn from the training data, we need to track if the $r^{th}$ feature was discriminated positively or negatively along the path from the root to $j^{th}$ node as proposed in \cite{ijcai2018p189}. We adopted the same strategy in our method. 

Furthermore, to sample decision trees with specific accuracy, we need to add an accuracy variable $w_t$, which is set to 1 if and only if the constraints required for correctly classifying the $i^{th}$ example are satisfied. Formula (\ref{formula 6}) is used to constrain the accuracy. For example, if the parameter $threshold=80\%$, then the decision trees must correctly classify $80\%$ samples at least. 
\begin{equation} \label{formula 6}
\frac{1}{M}\sum_{t=1}^{M} w_t >= threshold,\;with \;threshold \in [0,1]
\end{equation}

% Figure environment removed


\subsection{Decision Tree Sampling}
\label{decision tree sampling}
\paragraph{Sampling method:}

To obtain samples from the decision tree space, we employ two SAT samplers: QuickSampler \cite{10.1145/3180155.3180248} and UniGen3 \cite{inbook}. 
%
QuickSampler is a heuristic search algorithm that can generate large amounts of samples quickly. The algorithm starts with a random assignment and iteratively modifies the assignment by flipping the truth values of randomly selected variables. It is very efficient but the uniformity cannot be guaranteed. In contrast, UniGen3 is a more sophisticated algorithm for uniform SAT sampling with solid theoretical guarantees. It requires adding extra clauses to the encoding, which makes the sampling process computationally expensive. \looseness=-1 

\paragraph{Sampling set:}
%Unigen3 and Quicksampler allow users to assign a subset of all the variables as sampling set. If the sampling set contains Y variables, the size of the solution space will be $2^Y$. The samplers provide uniformity in this sampling set and the larger sampling size will slower the efficiency of sampling.

%In our case, we don't need to include some auxiliary variables in sampling set. For example, $u_{ij}$ is used to ensure that there are no repeated assigned features in any decision path but we don't need it during the decoding process. In addition, either the set $\{vl_i,vr_i\}$ or the set $\{l_{ij}, r_{ij}\}$ contains all the information needed to decode the tree structure as shown in Figure \ref{equivalence}, we only need to add one of them in the sampling set. In order to make the sampling process more efficient and guarantee the uniformity of sampling results, we select the smallest set of variables that can be used to decode a decision tree as the sampling set, which consists of $\{vl_i,vr_i,lc_i,rc_i,a_{rj}\}$. 

Unigen3 and Quicksampler allow users to assign a subset of all the variables as sampling set. If the sampling set contains Y variables, the size of the solution search space will be $2^Y$. The samplers provide uniformity within the sampling set and increasing Y may adversely affect the sampling efficiency. 
%
% Therefore, we identified some auxiliary variables in sampling set and removed them for improved efficiency. 
%
% In our case, we don't need to include the auxiliary variables in sampling set. 
Only part of valuables will be in the sampling set. For example, $u_{ij}$ is used to ensure that there are no repeated assigned features in any decision path but we don't need it during the decoding process. In addition, either the set $\{vl_i,vr_i\}$ or the set $\{l_{ij}, r_{ij}\}$ contains all the information needed to decode the tree structure, we only need to add one of them in the sampling set. Therefore, the smallest sampling set is $\{vl_i,vr_i,lc_i,rc_i,a_{rj}\}$. \looseness=-1 
%% Figure environment removed


%\subsubsection{Ground Truth}
%The ground truth of the sampling result is the distribution of all the decision trees in the whole space. Here we only consider the decision trees that has the same number of nodes and the distribution means the probability of decision trees with different accuracy appearing in the space. 
%

%\\
%We calculated the ground truth of the decision tree distribution when $\#node \leq 11$ on a subset of binarized UCI car dataset with 10 features. As showed in Table \ref{The time to get all of decision trees}, the computation time will increase rapidly along with the increment of size, which is also the reason why we need to do sampling instead of enumerating. Even though it is also reasonable to decide the class of every leaf node by greedy algorithm if you only care the tree with highest accuracy on a given structure and feature allocation, it's still time-consuming as long as the size of decision tree increases. 

%\begin{table}[h] 
%\small
%
%\caption{The time to get all of decision trees}
%\label{The time to get all of decision trees}
%\centering
%\begin{tabular}{|c|c|c|c|}
%    \hline
%    \#nodes & 
%    \begin{tabular}{c}
%    \#valid \\
%     structures
%    \end{tabular}& 
%    
%    \begin{tabular}{c}
%    \#decision trees\\ (Upper Bound)
%    \end{tabular}& 
%    Time\\ \hline
%
%3&1&$1\times 10^1$&fast \\ \hline
%5&2&$2\times 2^3 \times 10^2$&fast \\ \hline
%7&5&$5\times 2^4 \times 10^3$&fast \\ \hline
%9&14&$14\times 2^5 \times 10^4$&365s \\ \hline
%11&42&$42\times 2^6 \times 10^5$&6h approx. \\ \hline
%13&132&$132\times 2^7 \times 10^6$&377h est. \\ \hline
%15&429&$429\times 2^8 \times 10^7$&24,505h est. \\ \hline
%
%\end{tabular}
%\end{table}

\paragraph{Feature importance measurement:}
\cite{li2022self} measured the importance of elements in sequences based on the distribution under a qualification threshold. Inspired by this concept, we define feature importance as the contribution of each feature to a high accuracy space. Specifically, within a space consisting of decision trees surpassing a given threshold, the contributions can be evaluated based on the probability of each feature appearing in this space (we name it as emergence probability).
Since we sample decision trees from uniform distribution, we can estimate the probability by just counting how many times each feature appears.
%
Random forest often uses feature permutation or mean decrease in impunity to calculate feature importance. It's also possible to apply these approaches to our framework. \looseness=-1 
%
\section{Results}
%We ran all the experiments on Intel(R) Xeon(R) CPU E3-1270 v6 @ 3.80GHz. 
%
\paragraph{Comparison between DT-sampler and RF:}
We compared our method with RF on several real-world benchmark datasets\cite{Dua:2019}. As shown in Table \ref{rf vs ours}, our method can provide similar accuracy compared with random forest even if we only sample decision tree in small space. Relying on heuristic rules to build decision trees, random forest tends to generate larger decision trees. Besides, the randomness of tree generation makes it difficult to generate stable results for feature importance measurement (see Figure~\ref{rf problem} in appendix for details). In the appendix (see Figure ~\ref{stability}), we provide more detailed results on the stability of our method's feature importance measurements.\looseness=-1 
%
\begin{table}[h]
\small
\caption{Comparison of tree sizes and accuracy. Grid search on parameters = \{'max\_leaf\_nodes':[3,6,9,12,15,18,21,None]\} is utilized to run random forest and all the results are the average of three experiments on different subsets of the corresponding datasets, shown in the order of RF/ours. \#b,\#f denotes the number of training samples and the number of features respectively.}
\label{rf vs ours}
\setlength{\tabcolsep}{0.2em}

    \centering
  %\caption{Random Forest and our method}
  \begin{tabular}{|c|c|c|c|c|c|c|c|} 
    \hline \rm Dataset & \#b & \#f & thres.(\%) & \#node & training acc.(\%) & test acc.(\%)  \\ \hline
    mouse & 50 & 15 & 92.0 & 6.90/7 & 98.00/96.00 & 91.67/93.33   \\ \hline
    car & 100 & 15 & 92.0 & 16.93/11 & 98.00/93.00 & 88.57/88.32 \\  \hline
    % & 150 & 15 & 0.92 & 34.58/13 & 98.00\%/ 92.00\% & 93.47/90.75 \\ \hline
    
    breast & 150 & 15 &  81.6 & 19.00/11 & 83.33/80.89 & 73.49/74.54 \\ \hline

    heart & 170 & 19 & 81.0 & 23.00/15 & 89.21/84.50 & 83.93/81.36\\ \hline
   \end{tabular}

\end{table}

\paragraph{Comparison with existing SAT encodings:}
Our new encoding of tree structure reduces a large part of variables and constraints compared to the encoding method in \cite{ijcai2018p189}. The results on several benchmark datasets proved the acceleration in the process of SAT sampling as shown in Table \ref{comparison of encoding size}. \looseness=-1 

%We performed our experiments on five real-world benchmark datasets as shown in Table \ref{comparison of encoding size}. $\#b$,$\#f$ denotes the number of training samples and the number of features respectively. $\#var$ denotes the number variables used to build the encoding of the decision tree with 15 nodes while $\var_cnf$,$\#CLs_cnf$ denotes the number of variables and the number of clauses in the CNF file generated by tseitin transformation\cite{Tseitin1983} provided in z3-solver\cite{de2008z3}. $\#Ave. time$ denotes the average time of 5 experiments on the different subsets of corresponding datasets. $\#n$ denotes the average number of nodes of the optimal decision trees. 



%\subsection{Sampling results compared with ground truth}
%In this section we compare the sampling results generated by Unigen and Quicksampler with the ground truth. The ground truth is generated by enumerating all decision trees of 11 nodes on the subset of car dataset containing 30 samples and 10 features. As showed in Figure \ref{unigen}-\ref{ground truth}, Unigen can sample decision trees with guaranteed uniformity, while Quicksampler seems to have difficulty generating decision trees of high or low accuracy. Even though Quicksampler cannot guarantee the uniformity, we use it on the complicated cases that Unigen can't handle. Since the results of Quicksampler are largely influenced by the first solution generated by the SAT solver, we do samplings 3 times and combine all of sampling results to ensure better uniformity.


%% Figure environment removed

%\subsection{Drawbacks of Random Forest in feature importance measurement}
%We use different random\_state to generate 100 decision trees by random forest algorithm on breast-cancer dataset. Even though all of the parameters are the same, the distributions of decision trees are totally different as shown in Figure \ref{seed12_rf}.a and \ref{seed12_rf}.d. The randomness of decision tree generation doesn't have much influence on prediction but leads to the different results of feature importance measurement,  which is hard to give reliable explanation for real-world problems. 
%
%%We did  multiple experiments on a subset of car dataset. As shown in figure \ref{rf problem}, we always got %different results when changing the parameter of splitting criterion or adopting different feature importance %measurement approaches. This instability makes it hard to give a reliable explanation for real-world problems. 
%On the other hand, the feature importance of our method are always stable as shown in figure \ref{car results of decision tree sampling}.
%
%In addition, even in the same random forest model, the results are also quite different if we use different %methods to calculate feature importance. The heuristic d 


%We compared our method to Random Forest with the parameters selected by GridSearch in several datasets. Note %that our method can only deal with binary datasets, so each dataset is binarized firstly. Then due to the %complexity, we only used a subset of the dataset and performed feature selection using scikit-learn . $\#f_r$ %and $\#b$ denote the number of features and the number of samples of the training data, respectively. 

%Our method only has two parameters: threshold and \#node. The threshold depends on the specific occasions, say, %sometimes 99\% accuracy is requred but sometimes 80\% is enough. The selection of \#node is importance, when %\#node is small, there is less decision trees with high accuracy in the sampling space, and when \#node is big, %the calculation will become slow. Of course, it is also possible to apply model selection techniques to decide %these two parameters.

%% Figure environment removed


%\begin{table*}[t]
%    \centering
%  \caption{Random Forest and our method}
%  \begin{tabular}{|c|c|c|c|c|c|c|c|} 
%    \hline \rm Dataset & \#b & \#f & threshold & \#node_1 &\#node_2 & training acc. RFs/Ours & test acc. RFs/Ours \\ \hline
%    car & 100 & 10 & 0.90 & 16.27&13 & 93.00\% / 92.0\%  & 88.02\% / 85.4\% \\ \hline
%    mouse & 50 & 15 & 0.92 & 5.00&5 & 98.00\% / 96.00\% & 95.00\% / 95.00\%   \\ \hline
%    breast-cancer & 150 & 15 &  0.83 & 29.00&11 & 86.67\%/84.00\% & 73.23\% / 73.23\% \\ \hline
%    heart-disease & 170 & 19 & 0.80 & 29.00&15 & 90.59\%/86.47\% & 85.04\% / 82.68\%\\ \hline
%   \end{tabular}
%\end{table*}




%% Figure environment removed

\subsection{Interpretation}
%Our method offers a refined approach to measure feature importance, providing formal and straightforward interpretations. We consider a space comprising all decision trees with an accuracy $\ge$ a given threshold. By calculating the probability of each feature appearing in this space, we can determine the extent to which each feature contributes, thereby defining feature importance in a straightforward manner.
We define feature importance as its emergence probability in the high accuracy space as mentioned in \ref{decision tree sampling}. Parameter $threshold$ is used to describe what a high accuracy space means and its value depends on specific real-world scenarios and the desired level of strictness regarding accuracy requirements. To demonstrate our method, we utilize decision tree sampling on a subset of the breast-cancer dataset, which consists of 150 samples and 15 selected features (refer to Figure \ref{interpretation} in appendix). Initially, we set the threshold to 0, allowing for the random sampling of any decision tree. In this case, each feature is assigned to any branch node with equal probability, resulting in an emergence probability of $\frac{1}{15}$ for each feature. However, as we increase the threshold, the emergence probabilities of the features differ. Features with an emergence probability $\ge \frac{1}{15}$ are considered important.\looseness=-1 




%For instance, if we consider decision trees with an accuracy of at least 75\% as effective, the important features would be $\{f_{10}, f_7, f_9, f_{11}, f_{15}, f_{6}\}$. On the other hand, if we raise the accuracy requirement to a minimum of 83\% for each effective decision tree, the important features would change to $\{f_{15}, f_{10}, f_{11}, f_{7}, f_{6}\}$.




%% Figure environment removed



\section{Conclusion}
%We proposed an approach for decision tree sampling that can provide higher interpretability than random forest in feature importance measurement. With fewer parameters and exempted from the influence of random seeds, our method can always give stable results for feature importance measurement. Although decision tree sampling requires a larger amount of time than random forest, we expect that it would not be a problem along with the acceleration of SAT solver and the emergence of more efficient SAT sampler. 


% We proposed a method to compute feature importance using decision tree sampling. Random forest (RF) is a widely used feature selection method. However, due to the randomness in tree generation and over dependence on many parameters, it is hard to rely on RF-based feature selection. Our method provides a principled framework to compute feature importance based on tree accuracy and tree size. Although we demonstrated the efficacy (statistically and computationally) of the proposed method using five real world dataset, we want to highlight that this paper is an early demonstration of an ongoing research and there exists scope of improvement. Potential future research direction can be the development of a fast SAT solver using quantum annealing or QUBO-based solver, and or we can also develop a statistical hypothesis testing framework on top our proposed DT sampling method to judge the reliability of feature selection.


We proposed a method to measure feature importance using DT sampling and compared our method with random forest using four real-world datasets. Due to the randomness in tree generation and over-dependence on many parameters, RF-based feature selection generates unstable results. Our method provides a principled framework to measure feature importance based on sampling results from a high accuracy space with a clear threshold, which provides stable analysis results for real-world problems. Potential future research direction can be the development of a statistical hypothesis testing framework on top of our proposed DT sampling method to judge the reliability of feature selection and or the development of a fast SAT solver using quantum annealing or other QUBO-based solver.\looseness=-1 


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

\clearpage
\bibliography{example_paper}
\bibliographystyle{icml2023}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
%\onecolumn
\section{Appendix}

%You can have as much text here as you want. The main body must be at most $8$ pages long.
%For the final version, one more page can be added.
%If you want, you can use an appendix like this one, even using the one-column format.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Figure environment removed



\subsection{Ground Truth of sampling}
Generating all decision trees of size 2N+1 can be done by following procedures.

1. generate all possible binary trees of size N. The number of binary trees of a given size is the Catalan number $C_N$, which is given by the formula: $C_N = \frac{(2N)!}{((N+1)! \times N!)}$. \\
2. Add N+1 leaf nodes to each tree and assign decision values to the leaf nodes. Since every leaf node can be 0 or 1, N+1 leaf nodes will have $2^{N+1}$ different combinations. \\
3. Assign a feature to each branch node. Note that we always obey the rule that there is no repeated features in any path from root to any leaf.

The ground truth of the sampling result is the distribution of all the decision trees in the whole space. We calculated the ground truth of the decision tree distribution when $\#node \leq 11$ on a subset of binarized car dataset with 10 features. As showed in Table \ref{The time to get all of decision trees}, the computation time will increase rapidly along with the increment of size, which is also the reason why we need to do sampling instead of enumerating. Figure \ref{samgle_vs_true} shows the comparisons among the ground truth and the sampling results obtained by different samplers.


\begin{table}[h!] 
\small
\caption{The time to get all of decision trees. The time results of the two last rows are estimated and we didn't consider any multi-thread or multi-process strategies here. }
\label{The time to get all of decision trees}
\centering
\begin{tabular}{|c|c|c|c|}
    \hline
    \#nodes & 
    \begin{tabular}{c}
    \#valid \\
     structures
    \end{tabular}& 
    
    \begin{tabular}{c}
    \#decision trees\\ (Upper Bound)
    \end{tabular}& 
    Time\\ \hline

3&1&$1\times 10^1$&fast \\ \hline
5&2&$2\times 2^3 \times 10^2$&fast \\ \hline
7&5&$5\times 2^4 \times 10^3$&fast \\ \hline
9&14&$14\times 2^5 \times 10^4$&365s \\ \hline
11&42&$42\times 2^6 \times 10^5$&6h approx. \\ \hline
13&132&$132\times 2^7 \times 10^6$&377h est. \\ \hline
15&429&$429\times 2^8 \times 10^7$&24,505h est. \\ \hline

\end{tabular}



\end{table}

% Figure environment removed

\subsection{Interpretation}
The results of feature importance change with the definition of high accuracy space. As shown in Figure \ref{interpretation}, if we consider decision trees with an accuracy of at least 75\% as effective, the important features would be $\{f_{6}, f_7, f_9, f_{10}, f_{11}, f_{15}\}$. On the other hand, if we raise the accuracy requirement to a minimum of 83\% for each effective decision tree, the important features would change to $\{f_{6}, f_{7}, f_{10}, f_{11}, f_{15} \}$.

% Figure environment removed



\subsection{Comparisons of Feature Importance between DT-sampler and RF}
We did experiments on a subset with 100 samples of breast-cancer dataset to compare our method with random forest in feature importance measurement. Our method shows superior stability compared to Random Forest. In Figure \ref{rf problem}, we observe that when different random seeds or parameters are used, the distribution of decision trees generated by Random Forest consistently changes. This variability in tree generation directly impacts the feature importance results, leading to significant differences.

Furthermore, Random Forest tends to generate a large number of trees with low accuracy, making it unreliable to measure feature importance for real-world problems. In contrast, our method calculates feature importance based on decision trees sampled exclusively from a high accuracy space, which ensures the stability and interpretability of our results as depicted in Figure \ref{stability}.

% Figure environment removed

% Figure environment removed





\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
