In this section, we first describe our algorithm for $\cliquedet{k, \ell}$, and then analyze its running time in some interesting cases. 

Throughout this section, we use $g(k, \ell)$ to denote our algorithm's running time exponent on the number of $\ell$-cliques of $\cliquedet{k, \ell}$, i.e., our algorithm for $\cliquedet{k, \ell}$ runs in $\tO(\Delta_\ell^{g(k, \ell)})$ time. 

\subsection{General Detection Framework}
Now we describe a generic algorithm for $\cliquedet{k, \ell}$ for $k \ge 3$ (for $k=2$, we trivially list all edges in the graph, so $g(2, 1) = 2$) in Algorithm~\ref{alg:generic_detection}.

\begin{algorithm}
\caption{Generic $
\cliquedet{k, \ell}$ algorithm.}\label{alg:generic_detection}
\begin{algorithmic}
\item \textbf{Input:} Graph $G = (V, E)$ and the list $L$ of all $\ell$-cliques. 
\item \textbf{Output:} Output {\sc yes} if $G$ contains a $k$-cliques, and {\sc no} otherwise.
\item \textbf{The Algorithm:} 
\begin{itemize}
    \item Let integers $k \ge a \geq b \geq c \ge 1$ be such that $k = a + b + c$ (the algorithm chooses $a, b, c$ optimally). Then goal is then to bound the number of $d$-cliques for $d \in \{a, b, c\}$. 
    \begin{itemize}
        \item If $d \ge \ell$, we can use Lemma~\ref{lem:simple_list_ub} to upper bound the number of $d$-cliques with $S_d = \tilde{\Theta}(\Delta_\ell^{d/\ell})$, and add these $d$-cliques to a list $L_d$ in the same time. 
        \item If $d < \ell$, for every $d$-clique $K$ with $\Delta_
        \ell (K) \leq \Delta_\ell^{x_d}$ (for some parameter $x_d \in [0, 1]$ to be chosen), we check if $K$ is in a $k$-clique by recursively running $\cliquedet{k-d, \ell-d}$ in its neighbourhood. Then, let $L_d$ denote the set of remaining $d$-cliques. Then, $S_{d} := |L_d|  = \Theta(\Delta_\ell^{1-x_d})$. The running time of this step is 
        \begin{align*}
            \tO\left(\sum_{\substack{K: d\text{-clique}\\\Delta_\ell(K) \le \Delta_\ell^{x_d}}} \Delta_\ell(K)^{g(k-d, \ell - d)}\right) & \le \tO\left(\sum_{\substack{K: d\text{-clique}\\\Delta_\ell(K) \le \Delta_\ell^{x_d}}} \Delta_\ell(K) \cdot \Delta_\ell^{x_d (g(k-d, \ell - d) - 1)}\right)\\
            & \le \tO\left(\Delta_\ell^{1+x_d (g(k-d, \ell - d) - 1)}\right). 
        \end{align*}
    \end{itemize}
    \item Finally, we conduct a usual matrix multiplication of dimensions $S_a, S_b, S_c$ in time $\MM(S_a, S_b, S_c)$ as follows. If we find a $k$-clique, output {\sc yes}, otherwise we output {\sc no.}
    \begin{itemize}
        \item Create a matrix $X$ whose rows are indexed by $a$-cliques in $L_a$ and columns are indexed by  $b$-cliques in $L_b$. Set $A[K_a, K_b] = 1$ if  the nodes of $K_a$ and $K_b$ form an $(a+ b)$-clique, and $0$ otherwise.
        \item Create a matrix $Y$ whose rows are indexed by $b$-cliques in $L_b$ and columns are indexed by $c$-cliques in $L_c$, and set the entries similarly.
        \item Compute $Z = XY$. For each pair of  remaining $a$-clique $K_a$ and $c$-clique $K_c$ that form an $(a + c)$-clique, check if $Z[K_a, K_c] > 0$. If such an entry exists, output {\sc yes}. Otherwise, output {\sc no}.
    \end{itemize}
    
\end{itemize}
\end{algorithmic}
\end{algorithm}

The correctness of this algorithm is immediate. 
We also remark that the algorithm can  be used to count the number of $k$-cliques, by replacing all the recursive calls with the counting version of the algorithm, using the matrix multiplication to count the number of $k$-cliques in the remaining graph, and properly summing up and scaling the numbers. Clearly, the counting version of the algorithm will have the same running time. 

\subsection{Examples}
\label{sec:detection_examples}
Let us give some explicit examples to illustrate the algorithm. 

\paragraph{$\cliquedet{k,1}$.} The simplest example is $\cliquedet{k,1}$ for $k \ge 3$. Let $\lfloor k/3 \rfloor \leq c \leq b \leq a \leq \lceil k/3\rceil$ be integers such that $a+b+c = k$, which is one of the possible choices of $a, b, c$ for the algorithm.
Note that $c = \lfloor k/3 \rfloor, b =  \lceil (k-1)/3\rceil, a = \lceil k/3 \rceil$. Since $a, b, c \ge \ell = 1$, the algorithm would choose to use Lemma~\ref{lem:simple_list_ub} to bound the number of cliques of sizes $a, b, c$ as $n^a, n^b, n^c$ respectively. Thus, the running time of the algorithm is $\tO(n^{\omega(a, b, c)}) = \tO(n^{\beta(k)})$, matching the previous running time \cite{eisenbrand2004complexity}.

\paragraph{$\cliquedet{k,\ell}$ for $\ell \leq \lfloor k / 3\rfloor.$}
Similar as above, let $c = \lfloor k/3 \rfloor, b =  \lceil (k-1)/3\rceil, a = \lceil k/3 \rceil$ and the algorithm would choose to use Lemma~\ref{lem:simple_list_ub} to bound the number of cliques of sizes $a, b, c$. Thus, the running time of the algorithm is  $\tO(\Delta_\ell^{\omega(a/\ell, b/\ell, c/\ell)}) \leq \tO(\Delta_\ell^{\omega(\lceil k/3 \rceil,   \lceil (k-1)/3\rceil, \lfloor k/3 \rfloor)/\ell})$. 
This running time is optimal barring improvements for $\cliquedet{k, 1}$:


\begin{table}[ht]
    \centering
    \begin{tabular}{c|c|c|c|c|c|c|c|c|c|c}
         \backslashbox{$\ell$}{$k$} &  3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12\\
         \hline 
         1 & 2.373 & 3.252 & 4.089 & 4.746 & 5.594 & 6.401 & 7.119 & 7.952 & 8.751 & 9.492\\
         2 & 1.408 & 1.657 & 2.058 & 2.373 & 2.797 & 3.201 & 3.559 & 3.976 & 4.376 & 4.746\\
         3 & - & 1.248 & 1.422 & 1.669 & 1.918 & 2.151 & 2.373 & 2.651 & 2.917 & 3.164\\
         4 & - & - & 1.175 & 1.298 & 1.487 & 1.657 & 1.841 & 2.028 & 2.207 & 2.373\\
         5 & - & - & - & 1.130 & 1.233 & 1.378 & 1.504 & 1.660 & 1.811 & 1.954
    \end{tabular}
    \caption{Our $\cliquedet{k, \ell}$ exponent for various values of $k, \ell$ with the best current bound on $\omega$ \cite{alman2021refined} and rectangular matrix multiplication \cite{LU18}. See also \cite{van2019dynamic} for a way to bound $\omega(a, b, c)$ for arbitrary $a, b, c > 0$ from values of $\omega(1, x, 1)$. 
    The $(k, \ell)$th entry corresponds to the exponent $\alpha$ such that the runtime to detect a $k$-clique is $\tilde{O}(\Delta_\ell^\alpha)$ , where $\Delta_\ell$ is the number of $\ell$-cliques. 
    }
    \label{tab:det_exponent}
\end{table}

\begin{proposition}
Fix any positive integers $k \ge 3$ and $\ell \le \lfloor k/3\rfloor$, and let $\beta(k) = \omega(\lceil k/3 \rceil,   \lceil (k-1)/3\rceil, \lfloor k/3 \rfloor)$.
If $\cliquedet{k, 1}$ requires $n^{\beta(k) - o(1)}$ time, then
$\cliquedet{k, \ell}$ requires $\Delta_\ell^{\beta(k)/\ell-o(1)}$ time. 
\end{proposition}
\begin{proof}
Suppose for the sake of contradiction that $\cliquedet{k, \ell}$ has an  $O(\Delta_\ell^{\beta(k)/\ell-\eps}$) time algorithm $\mathcal{A}$ for some $\eps > 0$. Then given a $\cliquedet{k, 1}$ instance, we can first use Lemma~\ref{lem:simple_list_ub} to list all $\ell$-cliques in $O(n^\ell)$ time, and the number of $\ell$-cliques is bounded by $O(n^\ell)$. Then we can use $\mathcal{A}$ to solve the $\cliquedet{k, 1}$ instance in $O((n^\ell)^{\beta(k)/\ell-\eps})=n^{\beta(k)-\eps \ell}$ time, a contradiction.
\end{proof}



\begin{example}[$\cliquedet{3, 2}$]
\label{ex:clique-det-3-2}
\em
In this case, the algorithm can only choose $a=b=c=1$, and it would naturally choose $x_a=x_b=x_c$. The time it takes to bound the number of $1$-cliques (nodes) is $\tO(\Delta_2^{1+x_a (g(2, 1) - 1)}) = \tO(m^{1+x_a})$. Then we have $S_a, S_b, S_c \le \Theta(m^{1-x_a})$. Thus, the running time for the matrix multiplication of dimensions $S_a, S_b, S_c$ is $\tO(m^{(1-x_a)\omega})$. Overall, the running time is $\tO(m^{\frac{2\omega}{\omega+1}})$ by setting $x_a = \frac{\omega-1}{\omega+1}$. This is essentially Alon, Yuster and Zwick \cite{alon1997finding}'s triangle detection algorithm for sparse graphs. 
\end{example}

\begin{example}[$\cliquedet{4, 2}$]
\label{ex:clique-det-4-2}
\em
In this case, the algorithm can only choose $a=2, b=c=1$, and it would naturally choose $x_b=x_c$. The algorithm uses Lemma~\ref{lem:simple_list_ub} to (trivially) bound the number of edges as $m$. 
The time it takes to bound the number of nodes is $\tO(\Delta_2^{1+x_b (g(3, 1) - 1)}) = \tO(m^{1+x_b(\omega-1)})$. Then we have $S_a \le \Theta(m), S_b, S_c \le \Theta(m^{1-x_b})$. Thus, the running time for the matrix multiplication of dimensions $S_a, S_b, S_c$ is $\tO(m^{\omega(1, 1-x_b, 1-x_b)})$. The algorithm chooses $x_b$ so that $1+x_b(\omega-1) = \omega(1, 1-x_b, 1-x_b)$. If we simply bound $\omega(1, 1-x_b, 1-x_b)$ by $x_b + \omega(1-x_b)$, we can get $g(4, 2) \le \frac{\omega+1}{2}$ by setting $x_b = \frac{1}{2}$. For the current best bound of rectangular matrix multiplication \cite{LU18}, we can set $x_b = 0.478$ to get an upper bound $g(4, 2) \le 1.657$. As seen in Table~\ref{table:improved_det_4_5}, this is an improvement over the previous best algorithm of Eisenbrand and Grandoni \cite{eisenbrand2004complexity}. The key difference between our algorithm and \cite{eisenbrand2004complexity}'s algorithm is that, after they perform a similar first stage, they recursively call a $\cliquedet{4, 1}$ algorithm on graphs with $S_b$ nodes, losing the information that the graph has $S_a = m$ edges to begin with. We instead utilize this information with rectangular matrix multiplication to get a better running time. 

\end{example}

\begin{example}[$\cliquedet{5, 2}$]
\label{ex:clique-det-5-2}
\em
In this case, let the algorithm  choose $a=b=2, c=1$ (the choice $a=3, b=c=1$ gives a worse bound). The algorithm uses Lemma~\ref{lem:simple_list_ub} to (trivially) bound the number of edges as $m$. 
The time it takes to bound the number of nodes is $\tO(\Delta_2^{1+x_c (g(4, 1) - 1)}) = \tO(m^{1+x_c(\omega(1, 2, 1)-1)})$. Then we have $S_a,S_b \le \Theta(m), S_c \le \Theta(m^{1-x_c})$. Thus, the running time for the matrix multiplication of dimensions $S_a, S_b, S_c$ is $\tO(m^{\omega(1, 1, 1-x_c)})$. The algorithm chooses $x_c$ so that $1+x_c(\omega(1, 2, 1)-1) = \omega(1, 1, 1-x_c)$. If we simply bound $\omega(1, 2, 1)$ by $\omega + 1$ and $\omega(1, 1, 1-x_c)$ by $2x_c + (1-x_c)\omega$, we can get $g(5, 2) \le \frac{\omega+2}{2}$ by setting $x_c = \frac{1}{2}$. For the current best bound of rectangular matrix multiplication~\cite{LU18}, we can set $x_c = 0.4698$ to get an upper bound $g(5, 2) \le 2.058$. As seen in Table~\ref{table:improved_det_4_5}, this is an improvement over the previous best known algorithm of Eisenbrand and Grandoni \cite{eisenbrand2004complexity}.
\end{example}


\begin{example}[More Small Examples]
\label{ex:more-small-examples}
\em
See Tables~\ref{tab:detection_runtime} and \ref{tab:det_exponent} for more examples of the running times of our algorithm. These running times were obtained by finding the optimal values of $a, b, c$ using dynamic programming. 

From previous examples, one might wonder whether the algorithm always sets $a, b, c$ as close to $k/3$ as possible. The following example shows that it is not the case (for $\omega = 2$). 

In $\cliquedet{8, 4}$, if the algorithm chooses $a=4, b = c = 2$, then the running time is 
$$\tO\left(\Delta_4^{1+x_b(g(6, 2)-1)}+\Delta_4^{1+x_c(g(6, 2)-1)} + \Delta_4^{\omega(1, 1-x_b, 1-x_c)}\right).$$
By setting $x_b=x_c = \frac{1}{2}$, this running time is bounded by $\tO(\Delta_4^{3/2})$ when $\omega = 2$ (See Table~\ref{tab:det_exponent} for the value of $g(6, 2)$ when $\omega = 2$). 

However, if the algorithm chooses a more balanced choice $a=b=3, c = 2$, then the running time is $$\tO\left(\Delta_4^{1+x_a(g(5, 1)-1)}+\Delta_4^{1+x_b(g(5, 1)-1)}+\Delta_4^{1+x_c(g(6, 2)-1)} + \Delta_4^{\omega(1-x_a, 1-x_b, 1-x_c)}\right).$$
One optimal way to set the parameters when $\omega = 2$ is $x_a = x_b = \frac{1}{5}$ and $x_c = \frac{3}{5}$, which only gives an $\tO(\Delta_4^{8/5})$ running time when $\omega = 2$ (See Table~\ref{tab:det_exponent} for the values of $g(5, 1)$ and $g(6, 2)$ when $\omega = 2$). 
\end{example}

\subsection{Upper Bound for \texorpdfstring{$\cliquedet{k, k - h}$}{(k, k-h)-Clique-Detection}}
\label{sec:k-h_detect_bound}

In this section, we analyze the running time of our algorithm for $\cliquedet{k, k - h}$ for some constant $h=O(1)$. For convenience, let $e_h(k) = g(k, k - h)$.

We start with the following lemma.
\begin{lemma}
\label{lem:det_exponent_monotone}
For every $k > h$, $e_h(k + 1) \le e_h(k)$. 
\end{lemma}
\begin{proof}
We prove the statement by induction. We skip the base case $k = h+1$ as it works similarly as the induction step
(except for $h=1$, in which case $e_h(2) = 2$ and $e_h(3) = \frac{2\omega}{\omega+1} \le e_h(2)$, as the algorithm handles $\cliquedet{2, 1}$ specially).  Suppose the statement is already true for all smaller $k$. 

Let $\ell = k - h$ and $\ell' = k + 1 - h$. Suppose for $\cliquedet{k, \ell}$, the optimal parameters are $a, b, c, x_a, x_b, x_c, S_a, S_b, S_c$ ($x_d$ is relevant only if $d < \ell$ for $d \in \{a, b, c\}$).  Consider $\cliquedet{k+1, \ell+1}$ with parameters $a' = a+1, b' =b, c'=c$ and $x'_{a'}, x'_{b'}, x'_{c'}, S'_{a'}, S'_{b'}, S'_{c'}$ to be determined. Let $\Delta_\ell$ be the number of $\ell$-cliques in the $\cliquedet{k, \ell}$ instance and let $\Delta'_{\ell'}$ be the number of $(\ell+1)$-cliques in the $\cliquedet{k+1, \ell+1}$ instance. 

We first compare exponents related to $S_a$ and $S'_{a'}$.
\begin{itemize}
    \item If $a \ge \ell$. Then $S_a$ in $\cliquedet{k, \ell}$ is bounded by $\tO(\Delta_\ell^{a / \ell})$. In the $\cliquedet{k + 1, \ell + 1}$ algorithm, $S_{a'}'$ is bounded  by $\tO((\Delta'_{\ell'})^{(a+1)/(\ell+1)})$, a smaller exponent. 
    \item If $a < \ell$, the exponent of the running time for bounding $S_a$ in $\cliquedet{k, \ell}$ is $1+x_a (e_h(k-a) - 1)$, and $S_a$ is bounded by $\Delta_\ell^{1-x_a}$. Let $x'_{a'}$ be equal to $x_a$ in the algorithm for $\cliquedet{k + 1, \ell + 1}$. Then notice that the exponent for running time is $1+x'_{a'}(e_h(k + 1 - a') - 1) = 1+x_a(e_h(k-a)-1)$ and the bound on $S'_{a'}$ is $(\Delta'_{\ell'})^{1-x_a}$, both with same exponents as previous bounds. 
\end{itemize}
We then compare exponents related to $S_b$ and $S'_{b'}$. 
\begin{itemize}
    \item If $b > \ell$. Then $b' = b \ge \ell+1 = \ell'$. Then $S_b$ in $\cliquedet{k, \ell}$ is bounded by $\tO(\Delta_\ell^{b / \ell})$. In the $\cliquedet{k + 1, \ell + 1}$ algorithm, $S_{b'}'$ is bounded  by $\tO((\Delta'_{\ell'})^{b/(\ell+1)})$, a smaller exponent. 
    \item If $b = \ell$. In this case, $S_b = \tO(\Delta_\ell)$ and we will have $b' < \ell'$. Let $x'_{b'} = 0$ in $\cliquedet{k + 1, \ell + 1}$. Then $S'_{b'}$ is bounded by $\tO((\Delta'_{\ell'})^1)$, the same exponent as the bound of $S_b$. Also, the cost for having this bound is $\tO((\Delta'_{\ell'})^{1+x'_{b'}(e_h(k+1-b'))}) = \tO(\Delta'_{\ell'})$, so we can ignore the cost as it is near-linear time.
    \item If $b < \ell$, the exponent of the running time for bounding $S_b$ in $\cliquedet{k, \ell}$ is $1+x_b (e_h(k-b) - 1)$, and $S_b$ is bounded by $\Delta_\ell^{1-x_b}$. Let $x'_{b'}$ be equal to $x_b$ in the algorithm for $\cliquedet{k + 1, \ell + 1}$. Then notice that the exponent for running time is $1+x'_{b'}(e_h(k + 1 - b') - 1) = 1+x_b(e_h(k-b+1)-1)$. By the induction assumption, 
    $e_h(k-b+1) \le e_h(k-b)$, so $1+x_b(e_h(k-b+1)-1)$ is upper bounded by the running time exponent of the corresponding case in $\cliquedet{k, \ell}$. Note that this case does not happen in the base case $k=h+1$, as $b < \ell = 1$ can never happen, so we can safely apply the induction assumption. 
    The bound on $S'_{b'}$ is $(\Delta'_{\ell'})^{1-x_b}$,  with the same exponent as $S_b$ in $\cliquedet{k, \ell}$. 
\end{itemize}
The comparison of the exponents related to $S_c$ and $S'_{c'}$ works similarly. Thus, $e_h(k+1) \le e_h(k)$. 
\end{proof}

\begin{proposition}
\label{prop:eh_upper_bound}
$e_h(k) = 1+O\left(1/ k^{\log_{\frac{3}{2}}(\frac{\omega}{\omega-1})}\right)$. 
\end{proposition}
\begin{proof}
Let $\ell = k - h$. 
 Let $k_0 = 100h$. For all $k \le k_0$, $e_h(k) = O(1)$. 

For $k > k_0$, we choose $a, b, c$ in our $\cliquedet{k, k - h}$ algorithm so that $\lfloor k/3\rfloor = c \le b \le a = \lceil k/3\rceil$. Clearly, $a, b, c < \ell = k - h$. The running time of the algorithm is thus 
$$\tO\left(\Delta_\ell^{1+x_a  \cdot (e_h(k-a)-1)} 
+ \Delta_\ell^{1+x_b  \cdot (e_h(k-b)-1)} 
+ \Delta_\ell^{1+x_c  \cdot (e_h(k-c)-1)} 
+ MM\left(\Delta_\ell^{1-x_a}, \Delta_\ell^{1-x_b},\Delta_\ell^{1-x_c} \right)\right).$$
By Lemma~\ref{lem:det_exponent_monotone}, $e_h(k-c) \le e_h(k-b) \le e_h(k-a)$, so the running time is bounded by 
$$\tO\left(\Delta_\ell^{1+\max\{x_a, x_b, x_c\}  \cdot (e_h(k-a)-1)} 
+MM\left(\Delta_\ell^{1-x_a}, \Delta_\ell^{1-x_b},\Delta_\ell^{1-x_c} \right)\right).$$
Set $x_a = x_b = x_c = \frac{\omega -1}{\omega + e_h(k-a) - 1}$. The running time then becomes 
$$\tO\left(\Delta_\ell^{\frac{\omega \cdot e_h(k-a)}{\omega + e_h(k-a) - 1}}\right).$$
Thus,
$e_h(k) \le \frac{\omega \cdot e_h(k-a)}{\omega + e_h(k-a) - 1}$. Consequently, $$e_h(k) - 1 \le \frac{(\omega - 1) \cdot (e_h(k-a) - 1)}{\omega + e_h(k-a) - 1} \le \frac{\omega - 1}{\omega}  \cdot (e_h(k-a) - 1) = \frac{\omega - 1}{\omega}  \cdot (e_h(k-\lceil k/3\rceil) - 1).$$
Therefore $e_h(k) - 1 \le O\left(\left(\frac{\omega - 1}{\omega}\right)^{\log_{\frac{3}{2}} k}\right) = O\left(1/ k^{\log_{\frac{3}{2}}(\frac{\omega}{\omega-1})}\right)$.
\end{proof}

We also show that our choices of $a, b, c$ are not too far away from optimal, at least when $\omega = 2$. In the following proposition, recall $e_h(k)$ is the exponent of our algorithm, instead of the best exponent for $\cliquedet{k, k - h}$. 

\begin{proposition}
$e_h(k) = 1+\Omega\left(1/ k^{\log_{\frac{3}{2}}(2)}\right)$. 
\end{proposition}
\begin{proof}
Let $\ell = k - h$,  $\rho = \log_{\frac{3}{2}}(2)$, and $f_h(k) = \frac{1}{e_h(k) - 1}$. 
 Let $k_0 = 100h$. It is not difficult to see that for all $k \le k_0$, $f_h(k) \le M k^\rho - 1$ for some sufficiently large constant $M > 1$ because our algorithm does not achieve almost linear time, i.e., it always has $e_h(k) > 1$ and thus $f_h(k) < \infty$. 
 
 Let $k > k_0$, and let $a, b, c$ be the optimal choices for $\cliquedet{k, k - h}$. We will show by induction that $f_h(k) \le M k^\rho - 1$. 
 Consider two cases. 
 
 For the first case, assume $a < \ell$. Let $x_a, x_b, x_c$ be the optimal parameters for $\cliquedet{k, k - h}$, and if there are multiple choices, we choose one set of parameters with smallest $x_a+x_b+x_c$. 
 Then, the bound of our running time is (up to $\tO(1)$ factors) 
$$\Delta_\ell^{1+x_a  \cdot (e_h(k-a)-1)} 
+ \Delta_\ell^{1+x_b  \cdot (e_h(k-b)-1)} 
+ \Delta_\ell^{1+x_c  \cdot (e_h(k-c)-1)} 
+ MM\left(\Delta_\ell^{1-x_a}, \Delta_\ell^{1-x_b},\Delta_\ell^{1-x_c}\right).$$ 

Suppose $x_a > x_b$. By Lemma~\ref{lem:det_exponent_monotone}, $e_h(k-a) \ge e_h(k-b)$. Therefore, we can slightly increase $x_b$, and the running time of the algorithm will not be worse. This contradicts with the optimality of $x_a, x_b, x_c$ and minimality of $x_a+x_b+x_c$. Thus, we must have $x_a \le x_b$. Similarly, we have $x_b \le x_c$. 

Then we can lower bound $MM\left(\Delta_\ell^{1-x_a}, \Delta_\ell^{1-x_b},\Delta_\ell^{1-x_c}\right)$ by   $\Delta_\ell^{2-x_a-x_b}$.

The optimal way to balance $\Delta_\ell^{1+x_a  \cdot (e_h(k-a)-1)}, \Delta_\ell^{1+x_b  \cdot (e_h(k-b)-1)}, \Delta_\ell^{1+x_c  \cdot (e_h(k-c)-1)}$ and $ \Delta_\ell^{2-x_a-x_b}$ is to set 
$x_a = \frac{e_h(k-b)-1}{e_h(k-a)e_h(k-b)-1}$, $x_b = \frac{e_h(k-a)-1}{e_h(k-a)e_h(k-b)-1}$ and $x_c = \min\{1, \frac{(e_h(k-a)-1)(e_h(k-b)-1)}{(e_h(k-a)e_h(k-b)-1)(e_h(k-c)-1)}\}$, which gives \[e_h(k) \ge \frac{2e_h(k-a)e_h(k-b)-e_h(k-a)-e_h(k-b)}{e_h(k-a)e_h(k-b)-1}.\] Substituting $e_h$ by $f_h$ gives the following cleaner formula:
$$f_h(k) \le 1+f_h(k-a)+f_h(k-b).$$
As the algorithm chooses the optimal $a, b, c$, we have that 
$$f_h(k) \le \max_{\substack{1 \leq c \leq b \leq a \leq k\\ a + b + c = k}} \left\{ 1 + f_h(k-a) + f_h(k-b)\right\}.$$

By Lemma~\ref{lem:det_exponent_monotone}, $f_h(k-b)$ is nondecreasing when $b$ increases, so we can pick $b$ to be as large as possible for fixed $a$. Therefore, for fixed $a$, we choose  $c = \lfloor \frac{k-a}{2} \rfloor$ and $b = \lceil \frac{k-a}{2} \rceil$. Therefore, we can rewrite
$$f_h(k) \le \max_{ k/3  \leq a \leq k-2} \left\{1 + f_h(k-a) + f_h\left(\left\lfloor \frac{k+a}{2}\right\rfloor\right)\right\}.$$

By the induction assumption, $f_h(k') \le M (k')^\rho - 1$ for all $k'<k$.

Then,
\begin{align*}
    f_h(k)&\leq \max_{k/3 \le a \le k-2} \left\{1 + f_h(k-a) + f_h\left(\left\lfloor\frac{k+a}{2}\right\rfloor\right)\right\} \\
    & \leq \max_{0 \le p \le k/3}\left\{ 1 + M \left(\frac{2k}{3} - 2p\right)^\rho + M\left(\frac{2k}{3} + p\right)^\rho - 2 \right\}\\
    & \leq Mk^\rho \cdot \max_{0 \le p' \le 1/3} \left\{\left(\frac{2}{3}-2p'\right)^\rho + \left(\frac{2}{3}+p'\right)^\rho \right\} - 1\\
    &\le Mk^\rho - 1,
\end{align*}
which completes the induction step for this case. 

For the other case, assume $a \ge \ell$. Note that we must have $b, c < \ell$ as $2\ell > k$. Let $x_b, x_c$ be the optimal parameters. Similar as before, we can assume $x_b \le x_c$. 
 Then, the bound of our running time is (up to $\tO(1)$ factors) 
\begin{align*}
&\Delta_\ell^{1+x_b  \cdot (e_h(k-b)-1)} 
+ \Delta_\ell^{1+x_c  \cdot (e_h(k-c)-1)} 
+ MM\left(\Delta_\ell^{a/\ell}, \Delta_\ell^{1-x_b},\Delta_\ell^{1-x_c}\right)\\
\ge & \Delta_\ell^{1+x_b  \cdot (e_h(k-b)-1)} 
+ \Delta_\ell^{1+x_c  \cdot (e_h(k-c)-1)} 
+ \Delta_\ell^{a/\ell + 1 - x_b}
\end{align*}
The optimal way to balance is to set $x_b = \frac{a}{\ell e_h(k-b)}$ and $x_c = \min\{1, \frac{a(e_h(k-b)-1)}{\ell e_h(k-b)(e_h(k-c)-1)}\}$. 
This gives $e_h(k) \ge \frac{ae_h(k-b)-a}{\ell e_h(k-b)}+1$. Note that it is possible that $x_b > 1$ in this setting, but if that happens, $e_h(k) > e_h(k-b)$, which by Lemma~\ref{lem:det_exponent_monotone}, can never be optimal. In terms of $f_h$, this implies that $f_h(k) \le \frac{\ell  (f_h(k-b)+1)}{a}$. 
As the algorithm chooses the optimal $a, b, c$, we have that 
$$f_h(k) \le \max_{\substack{1 \leq c \leq b < \ell \leq a \leq k\\ a + b + c = k}}  \frac{\ell  (f_h(k-b)+1)}{a}.$$
By Lemma~\ref{lem:det_exponent_monotone}, $f_h(k-b)$ is nondecreasing when $b$ increases, so we can pick $b$ to be as large as possible for fixed $a$. Therefore, for fixed $a$, we choose  $c = \lfloor \frac{k-a}{2} \rfloor$ and $b = \lceil \frac{k-a}{2} \rceil$. Thus, we can rewrite
$$f_h(k) \le \max_{ \ell \leq a \leq k-2} \frac{\ell  (f_h\left(\left\lfloor \frac{k+a}{2}\right\rfloor\right)+1)}{a} \le \max_{ \ell \leq a \leq k-2}  \left\{f_h\left(\left\lfloor \frac{k+a}{2}\right\rfloor\right)+1\right\}.$$
By induction, it can be further upper bounded by 
$$\max_{ \ell \leq a \leq k-2}  \left\{M\left(\left\lfloor \frac{k+a}{2}\right\rfloor\right)^\rho -1 +1\right\} \le M(k-1)^\rho < Mk^\rho - 1,$$
as $M, \rho > 1$. This finishes the induction step for this case. 

Overall, we have shown that $f_h(k) \le M k^\rho - 1$ for all $k$, which implies $e_h(k) = 1+\Omega\left(1/ k^{\log_{\frac{3}{2}}(2)}\right)$. 
\end{proof}

\subsection{Upper Bound for \texorpdfstring{$\cliquedet{C\ell, \ell}$}{(Cl, l)-Clique-Detection}}\label{sec:Cl_l_detectionbound}
Define a sequence of functions $(f_i)_{i \ge 0}$ as follows:
$$f_i(C) = \frac{2^i \omega^{i+1} C}{3^{i+1}(\omega-1)^i+\left(3 (2^i - 3^i) (\omega-1)^i - 2^i (\omega-1)^i \omega + 2^i \omega^{i + 1}\right)C}. $$
The functions have the following recurrence relation, whose proof we omit as it is straightforward algebra. 
\begin{claim}
$f_0(C) = \frac{\omega C}{3}$ and $f_i(C) = \frac{\omega}{1+\frac{\omega - 1}{f_{i-1}\left(\frac{2C}{3-C}\right)}}$ for $i > 0$.
\end{claim}


Then we can express the running time of $\cliquedet{C\ell, \ell}$ for sufficiently large $\ell$ in terms of the functions $f_i$:
\begin{theorem}\label{thm:det_mult_upper_bound}
Let $C > 1$ be any constant such that $\frac{1}{C} \in \left(1-\left(\frac{2}{3}\right)^i, 1-\left(\frac{2}{3}\right)^{i+1} \right]$ for some constant integer $i \ge 0$. Then for any $\ell \ge 1$ and $C\ell \le  k \le (C+o_\ell(1)) \ell$, $g(k, \ell) \le f_i(C) + o_\ell(1)$. 
\end{theorem}
\begin{proof}
We prove by induction on $i$. 

When $i = 0$, $k \ge C\ell \ge 3\ell$. Therefore, we can apply the  $\cliquedet{k,\ell}$ example in Section~\ref{sec:detection_examples} for $\ell \leq \lfloor k / 3\rfloor$  to get $g(k, \ell) \le \omega(\lceil k/3 \rceil,   \lceil (k-1)/3\rceil, \lfloor k/3 \rfloor)/\ell$. This leads to 
\begin{align*}
    g(k, \ell) &\le \omega(k/3+1, k/3+1,k/3+1) / \ell \\
    & = \frac{(k/3+1)\omega}{\ell}\\
    & \le \frac{((C+o_\ell(1)) \ell / 3 + 1) \omega}{\ell}\\
    & \le \frac{\omega C}{3} + o_\ell(1) = f_0(C) + o_\ell(1).
\end{align*}

When $i > 0$, assume the claim is correct for $i-1$. Similar to the proof of Proposition~\ref{prop:eh_upper_bound}, we choose $a, b, c$ in our $\cliquedet{k, \ell}$ algorithm so that $\lfloor k/3\rfloor = c \le b \le a = \lceil k/3\rceil$. By the same analysis, the running time exponent can then be bounded by 
$\frac{\omega \cdot g(k-a, \ell - a)}{\omega + g(k-a, \ell - a) - 1}$. Let $C' = \frac{2C}{3-C}$. It is not difficult to verify that $\frac{1}{C'} \in \left(1-\left(\frac{2}{3}\right)^{i-1}, 1-\left(\frac{2}{3}\right)^{i} \right]$. 

Also, 
\begin{align*}
    \frac{k-a}{\ell - a} &\ge \frac{k - k/3}{\ell - k/3} \ge \frac{C \ell - (C\ell) / 3}{\ell - (C \ell) / 3} = \frac{2C}{3 - C} = C',
\end{align*}
and 
\begin{align*}
    \frac{k-a}{\ell - a} &\le \frac{k - (k/3 + 1)}{\ell - (k/3 + 1)} \le \frac{(C + o_\ell(1))\ell - ((C + o_\ell(1))\ell) / 3}{\ell - ((C + o_\ell(1)) \ell) / 3} = \frac{2C + o_\ell(1)}{3 - C - o_\ell(1)} = C' + o_\ell(1).
\end{align*}
Thus, $C'(\ell - a) \le k-a \le (C'+o_\ell(1))(\ell-a)$, so $g(k-a, \ell - a) \le f_{i-1}(C') + o_\ell(1)$ by induction. Therefore, the running time exponent of $\cliquedet{k, \ell}$ can be bounded by 
\begin{align*}
    \frac{\omega \cdot g(k-a, \ell - a)}{\omega + g(k-a, \ell - a) - 1} &= \frac{\omega}{1 + \frac{\omega - 1}{g(k-a, \ell - a)}}
    \le \frac{\omega}{1 + \frac{\omega - 1}{f_{i-1}(C') + o_\ell(1)}}
     \le \frac{\omega}{1 + \frac{\omega - 1}{f_{i-1}(\frac{2C}{3-C})}} +  o_\ell(1) = f_i(C) + o_\ell(1).
\end{align*}
\end{proof}

% Figure environment removed

In Figure~\ref{fig:detection_upper_bound}, we compare the bound obtained from Theorem~\ref{thm:det_mult_upper_bound} with the actual running time of Algorithm~\ref{alg:generic_detection} computed by dynamic programming for $3 \leq k \leq 200$.
In particular, for various values of $C$, we plot the exponent of $\cliquedet{k, \lfloor k/C\rfloor}$ against the upper bound obtained from Theorem~\ref{thm:det_mult_upper_bound} (without the $o_\ell(1)$ factor). Figure~\ref{fig:detection_upper_bound} shows that the estimates given by Theorem~\ref{thm:det_mult_upper_bound} are actually quite close to the actual exponents, and the values  indeed converge to our bound. 

