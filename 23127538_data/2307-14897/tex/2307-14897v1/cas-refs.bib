
@inproceedings{chen_simple_2020,
	title = {A {Simple} {Framework} for {Contrastive} {Learning} of {Visual} {Representations}},
	url = {https://proceedings.mlr.press/v119/chen20j.html},
	abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
	month = nov,
	year = {2020},
	pages = {1597--1607},
	annote = {ISSN: 2640-3498},
	file = {Full Text PDF:C\:\\Users\\asus\\Zotero\\storage\\HDUULEX2\\Chen et al. - 2020 - A Simple Framework for Contrastive Learning of Vis.pdf:application/pdf;Supplementary PDF:C\:\\Users\\asus\\Zotero\\storage\\5DRSUNIN\\Chen et al. - 2020 - A Simple Framework for Contrastive Learning of Vis.pdf:application/pdf},
}

@inproceedings{hendrycks_using_2019,
	title = {Using {Self}-{Supervised} {Learning} {Can} {Improve} {Model} {Robustness} and {Uncertainty}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/file/a2b15837edac15df90721968986f7f8e-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Hendrycks, Dan and Mazeika, Mantas and Kadavath, Saurav and Song, Dawn},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d' and Fox, E. and Garnett, R.},
	year = {2019},
	file = {Full Text:C\:\\Users\\asus\\Zotero\\storage\\2JNRUFVT\\Hendrycks et al. - 2019 - Using Self-Supervised Learning Can Improve Model R.pdf:application/pdf},
}

@inproceedings{zbontar_barlow_2021,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Barlow {Twins}: {Self}-{Supervised} {Learning} via {Redundancy} {Reduction}},
	volume = {139},
	url = {https://proceedings.mlr.press/v139/zbontar21a.html},
	abstract = {Self-supervised learning (SSL) is rapidly closing the gap with supervised methods on large computer vision benchmarks. A successful approach to SSL is to learn embeddings which are invariant to distortions of the input sample. However, a recurring issue with this approach is the existence of trivial constant solutions. Most current methods avoid such solutions by careful implementation details. We propose an objective function that naturally avoids collapse by measuring the cross-correlation matrix between the outputs of two identical networks fed with distorted versions of a sample, and making it as close to the identity matrix as possible. This causes the embedding vectors of distorted versions of a sample to be similar, while minimizing the redundancy between the components of these vectors. The method is called Barlow Twins, owing to neuroscientist H. Barlow’s redundancy-reduction principle applied to a pair of identical networks. Barlow Twins does not require large batches nor asymmetry between the network twins such as a predictor network, gradient stopping, or a moving average on the weight updates. Intriguingly it benefits from very high-dimensional output vectors. Barlow Twins outperforms previous methods on ImageNet for semi-supervised classification in the low-data regime, and is on par with current state of the art for ImageNet classification with a linear classifier head, and for transfer tasks of classification and object detection.},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Zbontar, Jure and Jing, Li and Misra, Ishan and LeCun, Yann and Deny, Stephane},
	editor = {Meila, Marina and Zhang, Tong},
	month = jul,
	year = {2021},
	pages = {12310--12320},
	file = {Full Text:C\:\\Users\\asus\\Zotero\\storage\\YW8DIE9E\\Zbontar et al. - 2021 - Barlow Twins Self-Supervised Learning via Redunda.pdf:application/pdf},
}

@article{liu_self-supervised_2023,
	title = {Self-{Supervised} {Learning}: {Generative} or {Contrastive}},
	volume = {35},
	issn = {1558-2191},
	shorttitle = {Self-{Supervised} {Learning}},
	doi = {10.1109/TKDE.2021.3090866},
	abstract = {Deep supervised learning has achieved great success in the last decade. However, its defects of heavy dependence on manual labels and vulnerability to attacks have driven people to find other paradigms. As an alternative, self-supervised learning (SSL) attracts many researchers for its soaring performance on representation learning in the last several years. Self-supervised representation learning leverages input data itself as supervision and benefits almost all types of downstream tasks. In this survey, we take a look into new self-supervised learning methods for representation in computer vision, natural language processing, and graph learning. We comprehensively review the existing empirical methods and summarize them into three main categories according to their objectives: generative, contrastive, and generative-contrastive (adversarial). We further collect related theoretical analysis on self-supervised learning to provide deeper thoughts on why self-supervised learning works. Finally, we briefly discuss open problems and future directions for self-supervised learning. An outline slide for the survey is provided\$ˆ1\$1.},
	number = {1},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Liu, Xiao and Zhang, Fanjin and Hou, Zhenyu and Mian, Li and Wang, Zhaoyu and Zhang, Jing and Tang, Jie},
	month = jan,
	year = {2023},
	keywords = {Computational modeling, Computer architecture, Context modeling, contrastive learning, Data models, deep learning, generative model, Predictive models, Self-supervised learning, Supervised learning, Task analysis},
	pages = {857--876},
	annote = {Conference Name: IEEE Transactions on Knowledge and Data Engineering},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\asus\\Zotero\\storage\\XJ9DQGS8\\9462394.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\asus\\Zotero\\storage\\AYYD8GT6\\Liu et al. - 2023 - Self-Supervised Learning Generative or Contrastiv.pdf:application/pdf},
}

@inproceedings{von_kugelgen_self-supervised_2021,
	title = {Self-{Supervised} {Learning} with {Data} {Augmentations} {Provably} {Isolates} {Content} from {Style}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/8929c70f8d710e412d38da624b21c3c8-Abstract.html},
	abstract = {Self-supervised representation learning has shown remarkable success in a number of domains. A common practice is to perform data augmentation via hand-crafted transformations intended to leave the semantics of the data invariant. We seek to understand the empirical success of this approach from a theoretical perspective. We formulate the augmentation process as a latent variable model by postulating a partition of the latent representation into a content component, which is assumed invariant to augmentation, and a style component, which is allowed to change. Unlike prior work on disentanglement and independent component analysis, we allow for both nontrivial statistical and causal dependencies in the latent space. We study the identifiability of the latent representation based on pairs of views of the observations and prove sufficient conditions that allow us to identify the invariant content partition up to an invertible mapping in both generative and discriminative settings. We find numerical simulations with dependent latent variables are consistent with our theory. Lastly, we introduce Causal3DIdent, a dataset of high-dimensional, visually complex images with rich causal dependencies, which we use to study the effect of data augmentations performed in practice.},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Von Kügelgen, Julius and Sharma, Yash and Gresele, Luigi and Brendel, Wieland and Schölkopf, Bernhard and Besserve, Michel and Locatello, Francesco},
	year = {2021},
	pages = {16451--16467},
	file = {Full Text PDF:C\:\\Users\\asus\\Zotero\\storage\\M97XU36F\\von Kügelgen et al. - 2021 - Self-Supervised Learning with Data Augmentations P.pdf:application/pdf},
}

@inproceedings{moon_tailoring_2022,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Tailoring {Self}-{Supervision} for {Supervised} {Learning}},
	isbn = {978-3-031-19806-9},
	doi = {10.1007/978-3-031-19806-9_20},
	abstract = {Recently, it is shown that deploying a proper self-supervision is a prospective way to enhance the performance of supervised learning. Yet, the benefits of self-supervision are not fully exploited as previous pretext tasks are specialized for unsupervised representation learning. To this end, we begin by presenting three desirable properties for such auxiliary tasks to assist the supervised objective. First, the tasks need to guide the model to learn rich features. Second, the transformations involved in the self-supervision should not significantly alter the training distribution. Third, the tasks are preferred to be light and generic for high applicability to prior arts. Subsequently, to show how existing pretext tasks can fulfill these and be tailored for supervised learning, we propose a simple auxiliary self-supervision task, predicting localizable rotation (LoRot). Our exhaustive experiments validate the merits of LoRot as a pretext task tailored for supervised learning in terms of robustness and generalization capability. Our code is available at https://github.com/wjun0830/Localizable-Rotation.},
	booktitle = {Computer {Vision} – {ECCV} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Moon, WonJun and Kim, Ji-Hwan and Heo, Jae-Pil},
	editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	note = {event-place: Cham},
	keywords = {Supervised learning, Auxiliary self-supervision, Pretext task},
	pages = {346--364},
	file = {Submitted Version:C\:\\Users\\asus\\Zotero\\storage\\9D3EFUAL\\Moon et al. - 2022 - Tailoring Self-Supervision for Supervised Learning.pdf:application/pdf;Submitted Version:C\:\\Users\\asus\\Zotero\\storage\\496Q6HM8\\Moon et al. - 2022 - Tailoring Self-Supervision for Supervised Learning.pdf:application/pdf},
}

@inproceedings{lee_self-supervised_2020,
	title = {Self-supervised {Label} {Augmentation} via {Input} {Transformations}},
	url = {https://proceedings.mlr.press/v119/lee20c.html},
	abstract = {Self-supervised learning, which learns by constructing artificial labels given only the input signals, has recently gained considerable attention for learning representations with unlabeled datasets, i.e., learning without any human-annotated supervision. In this paper, we show that such a technique can be used to significantly improve the model accuracy even under fully-labeled datasets. Our scheme trains the model to learn both original and self-supervised tasks, but is different from conventional multi-task learning frameworks that optimize the summation of their corresponding losses. Our main idea is to learn a single unified task with respect to the joint distribution of the original and self-supervised labels, i.e., we augment original labels via self-supervision. This simple, yet effective approach allows to train models easier by relaxing a certain invariant constraint during learning the original and self-supervised tasks simultaneously. It also enables an aggregated inference which combines the predictions from different augmentations to improve the prediction accuracy. Furthermore, we propose a novel knowledge transfer technique, which we refer to as self-distillation, that has the effect of the aggregated inference in a single (faster) inference. We demonstrate the large accuracy improvement and wide applicability of our framework on various fully-supervised settings, e.g., the few-shot and imbalanced classification scenarios.},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Lee, Hankook and Hwang, Sung Ju and Shin, Jinwoo},
	month = nov,
	year = {2020},
	pages = {5714--5724},
	annote = {ISSN: 2640-3498},
	file = {Full Text PDF:C\:\\Users\\asus\\Zotero\\storage\\IBCRHHER\\Lee et al. - 2020 - Self-supervised Label Augmentation via Input Trans.pdf:application/pdf;Supplementary PDF:C\:\\Users\\asus\\Zotero\\storage\\XP5C8IDA\\Lee et al. - 2020 - Self-supervised Label Augmentation via Input Trans.pdf:application/pdf},
}

@inproceedings{gidaris_unsupervised_2018,
	title = {Unsupervised {Representation} {Learning} by {Predicting} {Image} {Rotations}},
	url = {https://openreview.net/forum?id=S1v4N2l0-},
	abstract = {Over the last years, deep convolutional neural networks (ConvNets) have transformed the field of computer vision thanks to their unparalleled capacity to learn high level semantic image features. However, in order to successfully learn those features, they usually require massive amounts of manually labeled data, which is both expensive and impractical to scale. Therefore, unsupervised semantic feature learning, i.e., learning without requiring manual annotation effort, is of crucial importance in order to successfully harvest the vast amount of visual data that are available today. In our work we propose to learn image features by training ConvNets to recognize the 2d rotation that is applied to the image that it gets as input. We demonstrate both qualitatively and quantitatively that this apparently simple task actually provides a very powerful supervisory signal for semantic feature learning. We exhaustively evaluate our method in various unsupervised feature learning benchmarks and we exhibit in all of them state-of-the-art performance. Specifically, our results on those benchmarks demonstrate dramatic improvements w.r.t. prior state-of-the-art approaches in unsupervised representation learning and thus significantly close the gap with supervised feature learning. For instance, in PASCAL VOC 2007 detection task our unsupervised pre-trained AlexNet model achieves the state-of-the-art (among unsupervised methods) mAP of 54.4\%\$that is only 2.4 points lower from the supervised case. We get similarly striking results when we transfer our unsupervised learned features on various other tasks, such as ImageNet classification, PASCAL classification, PASCAL segmentation, and CIFAR-10 classification. The code and models of our paper will be published on: https://github.com/gidariss/FeatureLearningRotNet},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Gidaris, Spyros and Singh, Praveer and Komodakis, Nikos},
	month = feb,
	year = {2018},
	file = {Full Text PDF:C\:\\Users\\asus\\Zotero\\storage\\U2EB2K3Q\\Gidaris et al. - 2023 - Unsupervised Representation Learning by Predicting.pdf:application/pdf},
}

@inproceedings{noroozi_unsupervised_2016,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Unsupervised {Learning} of {Visual} {Representations} by {Solving} {Jigsaw} {Puzzles}},
	isbn = {978-3-319-46466-4},
	doi = {10.1007/978-3-319-46466-4_5},
	abstract = {We propose a novel unsupervised learning approach to build features suitable for object detection and classification. The features are pre-trained on a large dataset without human annotation and later transferred via fine-tuning on a different, smaller and labeled dataset. The pre-training consists of solving jigsaw puzzles of natural images. To facilitate the transfer of features to other tasks, we introduce the context-free network (CFN), a siamese-ennead convolutional neural network. The features correspond to the columns of the CFN and they process image tiles independently (i.e., free of context). The later layers of the CFN then use the features to identify their geometric arrangement. Our experimental evaluations show that the learned features capture semantically relevant content. We pre-train the CFN on the training set of the ILSVRC2012 dataset and transfer the features on the combined training and validation set of Pascal VOC 2007 for object detection (via fast RCNN) and classification. These features outperform all current unsupervised features with \$\$51.8{\textbackslash}textbackslash,{\textbackslash}textbackslash\%\$\$for detection and \$\$68.6{\textbackslash}textbackslash,{\textbackslash}textbackslash\%\$\$for classification, and reduce the gap with supervised learning (\$\$56.5{\textbackslash}textbackslash,{\textbackslash}textbackslash\%\$\$and \$\$78.2{\textbackslash}textbackslash,{\textbackslash}textbackslash\%\$\$respectively).},
	booktitle = {Computer {Vision} – {ECCV} 2016},
	publisher = {Springer International Publishing},
	author = {Noroozi, Mehdi and Favaro, Paolo},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	year = {2016},
	note = {event-place: Cham},
	keywords = {Self-supervised learning, Feature transfer, Image representation learning, Unsupervised learning},
	pages = {69--84},
	file = {Full Text PDF:C\:\\Users\\asus\\Zotero\\storage\\3CQNR7T2\\Noroozi and Favaro - 2016 - Unsupervised Learning of Visual Representations by.pdf:application/pdf},
}

@article{shorten_survey_2019,
	title = {A survey on {Image} {Data} {Augmentation} for {Deep} {Learning}},
	volume = {6},
	issn = {2196-1115},
	url = {https://doi.org/10.1186/s40537-019-0197-0},
	doi = {10.1186/s40537-019-0197-0},
	abstract = {Deep convolutional neural networks have performed remarkably well on many Computer Vision tasks. However, these networks are heavily reliant on big data to avoid overfitting. Overfitting refers to the phenomenon when a network learns a function with very high variance such as to perfectly model the training data. Unfortunately, many application domains do not have access to big data, such as medical image analysis. This survey focuses on Data Augmentation, a data-space solution to the problem of limited data. Data Augmentation encompasses a suite of techniques that enhance the size and quality of training datasets such that better Deep Learning models can be built using them. The image augmentation algorithms discussed in this survey include geometric transformations, color space augmentations, kernel filters, mixing images, random erasing, feature space augmentation, adversarial training, generative adversarial networks, neural style transfer, and meta-learning. The application of augmentation methods based on GANs are heavily covered in this survey. In addition to augmentation techniques, this paper will briefly discuss other characteristics of Data Augmentation such as test-time augmentation, resolution impact, final dataset size, and curriculum learning. This survey will present existing methods for Data Augmentation, promising developments, and meta-level decisions for implementing Data Augmentation. Readers will understand how Data Augmentation can improve the performance of their models and expand limited datasets to take advantage of the capabilities of big data.},
	number = {1},
	journal = {Journal of Big Data},
	author = {Shorten, Connor and Khoshgoftaar, Taghi M.},
	month = jul,
	year = {2019},
	keywords = {Big data, Data Augmentation, Deep Learning, GANs, Image data},
	pages = {60},
	file = {Full Text PDF:C\:\\Users\\asus\\Zotero\\storage\\DWEGQ95I\\Shorten and Khoshgoftaar - 2019 - A survey on Image Data Augmentation for Deep Learn.pdf:application/pdf;Snapshot:C\:\\Users\\asus\\Zotero\\storage\\3X7SN285\\s40537-019-0197-0.html:text/html},
}

@article{mumuni_data_2022,
	title = {Data augmentation: {A} comprehensive survey of modern approaches},
	volume = {16},
	issn = {2590-0056},
	shorttitle = {Data augmentation},
	url = {https://www.sciencedirect.com/science/article/pii/S2590005622000911},
	doi = {10.1016/j.array.2022.100258},
	abstract = {To ensure good performance, modern machine learning models typically require large amounts of quality annotated data. Meanwhile, the data collection and annotation processes are usually performed manually, and consume a lot of time and resources. The quality and representativeness of curated data for a given task is usually dictated by the natural availability of clean data in the particular domain as well as the level of expertise of developers involved. In many real-world application settings it is often not feasible to obtain sufficient training data. Currently, data augmentation is the most effective way of alleviating this problem. The main goal of data augmentation is to increase the volume, quality and diversity of training data. This paper presents an extensive and thorough review of data augmentation methods applicable in computer vision domains. The focus is on more recent and advanced data augmentation techniques. The surveyed methods include deeply learned augmentation strategies as well as feature-level and meta-learning-based data augmentation techniques. Data synthesis approaches based on realistic 3D graphics modeling, neural rendering, and generative adversarial networks are also covered. Different from previous surveys, we cover a more extensive array of modern techniques and applications. We also compare the performance of several state-of-the-art augmentation methods and present a rigorous discussion of the effectiveness of various techniques in different scenarios of use based on performance results on different datasets and tasks.},
	number = {100258},
	journal = {Array},
	author = {Mumuni, Alhassan and Mumuni, Fuseini},
	month = dec,
	year = {2022},
	keywords = {Computer vision, Generative adversarial network, Machine learning, Meta-learning, Review of data augmentation, Synthetic data},
	file = {Full Text:C\:\\Users\\asus\\Zotero\\storage\\T4W6RZYA\\Mumuni and Mumuni - 2022 - Data augmentation A comprehensive survey of moder.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\asus\\Zotero\\storage\\RGK8BCQM\\S2590005622000911.html:text/html},
}

@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature14539},
	doi = {10.1038/nature14539},
	abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
	number = {7553},
	journal = {Nature},
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	month = may,
	year = {2015},
	keywords = {Computer science, Mathematics and computing},
	pages = {436--444},
	annote = {Number: 7553 Publisher: Nature Publishing Group},
	file = {Full Text PDF:C\:\\Users\\asus\\Zotero\\storage\\KLCLHBVG\\LeCun et al. - 2015 - Deep learning.pdf:application/pdf},
}

@inproceedings{huang_densely_2017,
	title = {Densely {Connected} {Convolutional} {Networks}},
	url = {https://ieeexplore.ieee.org/document/8099726},
	doi = {10.1109/CVPR.2017.243},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Huang, Gao and Liu, Zhuang and van der Maaten, Laurens and Weinberger, Kilian Q.},
	year = {2017},
	note = {event-place: Honolulu, HI, USA},
	pages = {2261--2269},
	file = {Full Text PDF:C\:\\Users\\asus\\Zotero\\storage\\PKKVARZP\\Huang et al. - 2017 - Densely Connected Convolutional Networks.pdf:application/pdf},
}

@article{sun_fully_2018,
	title = {Fully {Convolutional} {Networks} for {Semantic} {Segmentation} of {Very} {High} {Resolution} {Remotely} {Sensed} {Images} {Combined} {With} {DSM}},
	volume = {15},
	issn = {1558-0571},
	doi = {10.1109/LGRS.2018.2795531},
	abstract = {Recently, approaches based on fully convolutional networks (FCN) have achieved state-of-the-art performance in the semantic segmentation of very high resolution (VHR) remotely sensed images. One central issue in this method is the loss of detailed information due to downsampling operations in FCN. To solve this problem, we introduce the maximum fusion strategy that effectively combines semantic information from deep layers and detailed information from shallow layers. Furthermore, this letter develops a powerful backend to enhance the result of FCN by leveraging the digital surface model, which provides height information for VHR images. The proposed semantic segmentation scheme has achieved an overall accuracy of 90.6\% on the ISPRS Vaihingen benchmark.},
	number = {3},
	journal = {IEEE Geoscience and Remote Sensing Letters},
	author = {Sun, Weiwei and Wang, Ruisheng},
	month = mar,
	year = {2018},
	keywords = {deep learning, Benchmark testing, Color, Convolution, Fully convolutional networks (FCN), Image resolution, Image segmentation, remote sensing, Remote sensing, semantic segmentation, Semantics, very high resolution (VHR)},
	pages = {474--478},
	annote = {Conference Name: IEEE Geoscience and Remote Sensing Letters},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\asus\\Zotero\\storage\\TYF9AXNB\\8281008.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\asus\\Zotero\\storage\\Q7FHCUTW\\Sun and Wang - 2018 - Fully Convolutional Networks for Semantic Segmenta.pdf:application/pdf},
}

@inproceedings{long_fully_2015,
	title = {Fully {Convolutional} {Networks} for {Semantic} {Segmentation}},
	url = {https://ieeexplore.ieee.org/document/7298965},
	doi = {10.1109/CVPR.2015.7298965},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
	year = {2015},
	note = {event-place: Boston, MA, USA},
	pages = {3431--3440},
	file = {Full Text PDF:C\:\\Users\\asus\\Zotero\\storage\\YXX2YE8M\\Long et al. - 2015 - Fully Convolutional Networks for Semantic Segmenta.pdf:application/pdf},
}

@inproceedings{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {https://aclanthology.org/N19-1423},
	doi = {10.18653/v1/N19-1423},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = jun,
	year = {2019},
	note = {event-place: Minneapolis, Minnesota},
	pages = {4171--4186},
	file = {Full Text PDF:C\:\\Users\\asus\\Zotero\\storage\\BQF6YB74\\Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf},
}

@article{zhang_deep_2018,
	title = {Deep learning for sentiment analysis: {A} survey},
	volume = {8},
	issn = {1942-4795},
	shorttitle = {Deep learning for sentiment analysis},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.1253},
	doi = {10.1002/widm.1253},
	abstract = {Deep learning has emerged as a powerful machine learning technique that learns multiple layers of representations or features of the data and produces state-of-the-art prediction results. Along with the success of deep learning in many application domains, deep learning is also used in sentiment analysis in recent years. This paper gives an overview of deep learning and then provides a comprehensive survey of its current applications in sentiment analysis. This article is categorized under: Fundamental Concepts of Data and Knowledge {\textbackslash}textgreater Data Concepts Algorithmic Development {\textbackslash}textgreater Text Mining},
	number = {4},
	journal = {WIREs Data Mining and Knowledge Discovery},
	author = {Zhang, Lei and Wang, Shuai and Liu, Bing},
	year = {2018},
	keywords = {deep learning, data mining, machine learning, natural language processing, neural network, opinion mining, sentiment analysis, survey},
	pages = {e1253},
	annote = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/widm.1253},
	file = {Snapshot:C\:\\Users\\asus\\Zotero\\storage\\R7WRB2C2\\widm.html:text/html;Submitted Version:C\:\\Users\\asus\\Zotero\\storage\\ST9J8T9U\\Zhang et al. - 2018 - Deep learning for sentiment analysis A survey.pdf:application/pdf},
}

@article{krizhevsky_learning_2009,
	title = {Learning multiple layers of features from tiny images},
	url = {https://www.cs.toronto.edu/~kriz/cifar.html},
	journal = {Master's thesis, Department of Computer Science, University of Toronto},
	author = {Krizhevsky, Alex},
	year = {2009},
	annote = {Publisher: Toronto, ON, Canada},
	file = {learning-features-2009-TR.pdf:C\:\\Users\\asus\\Downloads\\learning-features-2009-TR.pdf:application/pdf},
}

@inproceedings{chen_towards_2022,
	title = {Towards {Understanding} the {Mixture}-of-{Experts} {Layer} in {Deep} {Learning}},
	url = {https://openreview.net/forum?id=MaYzugDmQV},
	abstract = {The Mixture-of-Experts (MoE) layer, a sparsely-activated model controlled by a router, has achieved great success in deep learning. However, the understanding of such architecture remains elusive. In this paper, we formally study how the MoE layer improves the performance of neural network learning and why the mixture model will not collapse into a single model. Our empirical results suggest that the cluster structure of the underlying problem and the non-linearity of the expert are pivotal to the success of MoE. This motivates us to consider a challenging classification problem with intrinsic cluster structures. Theoretically, we proved that this problem is hard to solve by a single expert such as a two-layer convolutional neural network (CNN). Yet with the MoE layer with each expert being a two-layer CNN, the problem can be solved successfully. In particular, our theory shows that the router can learn the cluster-center features, which helps divide the input complex problem into simpler classification sub-problems that individual experts can conquer. To our knowledge, this is the first theoretical result toward formally understanding the mechanism of the MoE layer for deep learning.},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Chen, Zixiang and Deng, Yihe and Wu, Yue and Gu, Quanquan and Li, Yuanzhi},
	month = oct,
	year = {2022},
	file = {Full Text PDF:C\:\\Users\\asus\\Zotero\\storage\\HPX8APU2\\Chen et al. - 2022 - Towards Understanding the Mixture-of-Experts Layer.pdf:application/pdf},
}

@inproceedings{he_momentum_2020,
	title = {Momentum {Contrast} for {Unsupervised} {Visual} {Representation} {Learning}},
	url = {https://ieeexplore.ieee.org/document/9157636/},
	doi = {10.1109/CVPR42600.2020.00975},
	abstract = {We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
	month = jun,
	year = {2020},
	note = {event-place: Seattle, WA, USA},
	keywords = {Task analysis, Unsupervised learning, Buildings, Dictionaries, Loss measurement, Training, Visualization},
	pages = {9726--9735},
	annote = {ISSN: 2575-7075},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\asus\\Zotero\\storage\\76UJIHNJ\\9157636.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\asus\\Zotero\\storage\\4FYZAC65\\He et al. - 2020 - Momentum Contrast for Unsupervised Visual Represen.pdf:application/pdf},
}

@inproceedings{he_deep_2016,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {https://ieeexplore.ieee.org/document/7780459},
	doi = {10.1109/CVPR.2016.90},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = jun,
	year = {2016},
	note = {event-place: Las Vegas, NV, USA},
	keywords = {Image segmentation, Training, Visualization, Complexity theory, Degradation, Image recognition, Neural networks},
	pages = {770--778},
	annote = {ISSN: 1063-6919},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\asus\\Zotero\\storage\\BCBLECVQ\\stamp.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\asus\\Zotero\\storage\\REVT33AK\\He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf:application/pdf},
}

@inproceedings{chen_exploring_2021,
	title = {Exploring {Simple} {Siamese} {Representation} {Learning}},
	url = {https://ieeexplore.ieee.org/document/9578004},
	doi = {10.1109/CVPR46437.2021.01549},
	abstract = {Siamese networks have become a common structure in various recent models for unsupervised visual representation learning. These models maximize the similarity between two augmentations of one image, subject to certain conditions for avoiding collapsing solutions. In this paper, we report surprising empirical results that simple Siamese networks can learn meaningful representations even using none of the following: (i) negative sample pairs, (ii) large batches, (iii) momentum encoders. Our experiments show that collapsing solutions do exist for the loss and structure, but a stop-gradient operation plays an essential role in preventing collapsing. We provide a hypothesis on the implication of stop-gradient, and further show proof-of-concept experiments verifying it. Our "SimSiam" method achieves competitive results on ImageNet and downstream tasks. We hope this simple baseline will motivate people to rethink the roles of Siamese architectures for unsupervised representation learning. Code is made available.1},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Chen, Xinlei and He, Kaiming},
	month = jun,
	year = {2021},
	note = {event-place: Nashville, TN, USA},
	keywords = {Computer vision, Computational modeling, Computer architecture, Visualization, Codes, Shape, Tools},
	pages = {15745--15753},
	annote = {ISSN: 2575-7075},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\asus\\Zotero\\storage\\DEN2BKAC\\9578004.html:text/html;Submitted Version:C\:\\Users\\asus\\Zotero\\storage\\2EL6ZJ2A\\Chen and He - 2021 - Exploring Simple Siamese Representation Learning.pdf:application/pdf},
}

@inproceedings{doersch_unsupervised_2015,
	title = {Unsupervised {Visual} {Representation} {Learning} by {Context} {Prediction}},
	url = {https://ieeexplore.ieee.org/document/7410524},
	doi = {10.1109/ICCV.2015.167},
	abstract = {This work explores the use of spatial context as a source of free and plentiful supervisory signal for training a rich visual representation. Given only a large, unlabeled image collection, we extract random pairs of patches from each image and train a convolutional neural net to predict the position of the second patch relative to the first. We argue that doing well on this task requires the model to learn to recognize objects and their parts. We demonstrate that the feature representation learned using this within-image context indeed captures visual similarity across images. For example, this representation allows us to perform unsupervised visual discovery of objects like cats, people, and even birds from the Pascal VOC 2011 detection dataset. Furthermore, we show that the learned ConvNet can be used in the R-CNN framework [19] and provides a significant boost over a randomly-initialized ConvNet, resulting in state-of-the-art performance among algorithms which use only Pascal-provided training set annotations.},
	booktitle = {2015 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Doersch, Carl and Gupta, Abhinav and Efros, Alexei A.},
	month = dec,
	year = {2015},
	note = {event-place: Santiago, Chile},
	keywords = {Predictive models, Semantics, Training, Visualization, Context, Data mining, Image representation},
	pages = {1422--1430},
	annote = {ISSN: 2380-7504},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\asus\\Zotero\\storage\\GPCGZYRJ\\7410524.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\asus\\Zotero\\storage\\7SWRB3DH\\Doersch et al. - 2015 - Unsupervised Visual Representation Learning by Con.pdf:application/pdf},
}

@inproceedings{khosla_supervised_2020,
	title = {Supervised {Contrastive} {Learning}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/file/d89a66c7c80a29b1bdbab0f2a1a94af8-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Khosla, Prannay and Teterwak, Piotr and Wang, Chen and Sarna, Aaron and Tian, Yonglong and Isola, Phillip and Maschinot, Aaron and Liu, Ce and Krishnan, Dilip},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
	year = {2020},
	pages = {18661--18673},
	file = {Full Text:C\:\\Users\\asus\\Zotero\\storage\\CUUXMS4P\\Khosla et al. - 2020 - Supervised Contrastive Learning.pdf:application/pdf},
}

@inproceedings{shazeer_outrageously_2017,
	title = {Outrageously {Large} {Neural} {Networks}: {The} {Sparsely}-{Gated} {Mixture}-of-{Experts} {Layer}},
	shorttitle = {Outrageously {Large} {Neural} {Networks}},
	url = {https://openreview.net/forum?id=B1ckMDqlg},
	abstract = {The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Shazeer, Noam and Mirhoseini, *Azalia and Maziarz, *Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
	year = {2017},
	file = {Full Text PDF:C\:\\Users\\asus\\Zotero\\storage\\A7B4PWLP\\Shazeer et al. - 2022 - Outrageously Large Neural Networks The Sparsely-G.pdf:application/pdf},
}

@article{yudistira_gated_2017,
	title = {Gated spatio and temporal convolutional neural network for activity recognition: towards gated multimodal deep learning},
	volume = {2017},
	issn = {1687-5281},
	shorttitle = {Gated spatio and temporal convolutional neural network for activity recognition},
	url = {https://doi.org/10.1186/s13640-017-0235-9},
	doi = {10.1186/s13640-017-0235-9},
	abstract = {Human activity recognition requires both visual and temporal cues, making it challenging to integrate these important modalities. The usual schemes for integration are averaging and fixing the weights of both features for all samples. However, how much weight is needed for each sample and modality, is still an open question. A mixture of experts via a gating Convolutional Neural Network (CNN) is one promising architecture for adaptively weighting every sample within a dataset. In this paper, rather than just averaging or using fixed weights, we investigate how a natural associative cortex such as a network integrates expert networks to form a gating CNN scheme. Starting from Red Green Blue color model (RGB) values and optical flows, we show that with proper treatment, the gating CNN scheme works well, indicating future approaches to information integration in future activity recognition.},
	number = {1},
	journal = {EURASIP Journal on Image and Video Processing},
	author = {Yudistira, Novanto and Kurita, Takio},
	month = dec,
	year = {2017},
	keywords = {Deep learning, Action recognition, CNN, Gated network},
	pages = {85},
	file = {Full Text PDF:C\:\\Users\\asus\\Zotero\\storage\\GMTRATXC\\Yudistira and Kurita - 2017 - Gated spatio and temporal convolutional neural net.pdf:application/pdf;Snapshot:C\:\\Users\\asus\\Zotero\\storage\\27UDP47G\\s13640-017-0235-9.html:text/html},
}

@inproceedings{gross_hard_2017,
	title = {Hard {Mixtures} of {Experts} for {Large} {Scale} {Weakly} {Supervised} {Vision}},
	url = {https://ieeexplore.ieee.org/document/8100023},
	doi = {10.1109/CVPR.2017.540},
	abstract = {Training convolutional networks (CNNs) that fit on a single GPU with minibatch stochastic gradient descent has become effective in practice. However, there is still no effective method for training large networks that do not fit in the memory of a few GPU cards, or for parallelizing CNN training. In this work we show that a simple hard mixture of experts model can be efficiently trained to good effect on large scale hashtag (multilabel) prediction tasks. Mixture of experts models are not new [7, 3], but in the past, researchers have had to devise sophisticated methods to deal with data fragmentation. We show empirically that modern weakly supervised data sets are large enough to support naive partitioning schemes where each data point is assigned to a single expert. Because the experts are independent, training them in parallel is easy, and evaluation is cheap for the size of the model. Furthermore, we show that we can use a single decoding layer for all the experts, allowing a unified feature embedding space. We demonstrate that it is feasible (and in fact relatively painless) to train far larger models than could be practically trained with standard CNN architectures, and that the extra capacity can be well used on current datasets.},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Gross, Sam and Ranzato, Marc'Aurelio and Szlam, Arthur},
	month = jul,
	year = {2017},
	note = {event-place: Honolulu, HI, USA},
	keywords = {Data models, Predictive models, Training, Decoding, Logic gates, Standards},
	pages = {5085--5093},
	annote = {ISSN: 1063-6919},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\asus\\Zotero\\storage\\9SULXA3S\\authors.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\asus\\Zotero\\storage\\99LDWTRQ\\Gross et al. - 2017 - Hard Mixtures of Experts for Large Scale Weakly Su.pdf:application/pdf},
}

@article{fedus_switch_2022,
	title = {Switch {Transformers}: {Scaling} to {Trillion} {Parameter} {Models} with {Simple} and {Efficient} {Sparsity}},
	volume = {23},
	issn = {1533-7928},
	shorttitle = {Switch {Transformers}},
	url = {http://jmlr.org/papers/v23/21-0998.html},
	abstract = {In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) models defy this and instead select different parameters for each incoming example. The result is a sparsely-activated model—with an outrageous number of parameters—but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs, and training instability. We address these with the introduction of the Switch Transformer. We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs. Our proposed training techniques mitigate the instabilities, and we show large sparse models may be trained, for the first time, with lower precision (bfloat16) formats. We design models based off T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with the same computational resources. These improvements extend into multilingual settings where we measure gains over the mT5-Base version across all 101 languages. Finally, we advance the current scale of language models by pre-training up to trillion parameter models on the "Colossal Clean Crawled Corpus", and achieve a 4x speedup over the T5-XXL model.},
	number = {120},
	journal = {Journal of Machine Learning Research},
	author = {Fedus, William and Zoph, Barret and Shazeer, Noam},
	year = {2022},
	pages = {1--39},
	file = {Full Text PDF:C\:\\Users\\asus\\Zotero\\storage\\BQAVJNRT\\Fedus et al. - 2022 - Switch Transformers Scaling to Trillion Parameter.pdf:application/pdf;Source Code:C\:\\Users\\asus\\Zotero\\storage\\85LK4UDR\\moe.html:text/html},
}

@inproceedings{lewis_base_2021,
	title = {{BASE} {Layers}: {Simplifying} {Training} of {Large}, {Sparse} {Models}},
	shorttitle = {{BASE} {Layers}},
	url = {https://proceedings.mlr.press/v139/lewis21a.html},
	abstract = {We introduce a new balanced assignment of experts (BASE) layer for large language models that greatly simplifies existing high capacity sparse layers. Sparse layers can dramatically improve the efficiency of training and inference by routing each token to specialized expert modules that contain only a small fraction of the model parameters. However, it can be difficult to learn balanced routing functions that make full use of the available experts; existing approaches typically use routing heuristics or auxiliary expert-balancing loss functions. In contrast, we formulate token-to-expert allocation as a linear assignment problem, allowing an optimal assignment in which each expert receives an equal number of tokens. This optimal assignment scheme improves efficiency by guaranteeing balanced compute loads, and also simplifies training by not requiring any new hyperparameters or auxiliary losses. Code is publicly released.},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Lewis, Mike and Bhosale, Shruti and Dettmers, Tim and Goyal, Naman and Zettlemoyer, Luke},
	month = jul,
	year = {2021},
	pages = {6265--6274},
	annote = {ISSN: 2640-3498},
	file = {Full Text PDF:C\:\\Users\\asus\\Zotero\\storage\\SJCVYCY2\\Lewis et al. - 2021 - BASE Layers Simplifying Training of Large, Sparse.pdf:application/pdf},
}

@article{torralba_80_2008,
	title = {80 {Million} {Tiny} {Images}: {A} {Large} {Data} {Set} for {Nonparametric} {Object} and {Scene} {Recognition}},
	volume = {30},
	issn = {1939-3539},
	shorttitle = {80 {Million} {Tiny} {Images}},
	url = {https://ieeexplore.ieee.org/document/4531741},
	doi = {10.1109/TPAMI.2008.128},
	abstract = {With the advent of the Internet, billions of images are now freely available online and constitute a dense sampling of the visual world. Using a variety of non-parametric methods, we explore this world with the aid of a large dataset of 79,302,017 images collected from the Internet. Motivated by psychophysical results showing the remarkable tolerance of the human visual system to degradations in image resolution, the images in the dataset are stored as 32 x 32 color images. Each image is loosely labeled with one of the 75,062 non-abstract nouns in English, as listed in the Wordnet lexical database. Hence the image database gives a comprehensive coverage of all object categories and scenes. The semantic information from Wordnet can be used in conjunction with nearest-neighbor methods to perform object classification over a range of semantic levels minimizing the effects of labeling noise. For certain classes that are particularly prevalent in the dataset, such as people, we are able to demonstrate a recognition performance comparable to class-specific Viola-Jones style detectors.},
	number = {11},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Torralba, Antonio and Fergus, Rob and Freeman, William T.},
	month = nov,
	year = {2008},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Computer vision, Image resolution, Degradation, Image recognition, Humans, Image databases, Image sampling, Internet, large datasets, Layout, nearest-neighbor methods, Object recognition, Psychology, Visual system},
	pages = {1958--1970},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\asus\\Zotero\\storage\\GRAZ7YEG\\4531741.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\asus\\Zotero\\storage\\MLR2PBYS\\Torralba et al. - 2008 - 80 Million Tiny Images A Large Data Set for Nonpa.pdf:application/pdf;Submitted Version:C\:\\Users\\asus\\Zotero\\storage\\WF2S3JJE\\Torralba et al. - 2008 - 80 Million Tiny Images A Large Data Set for Nonpa.pdf:application/pdf},
}

@inproceedings{cao_learning_2019,
	title = {Learning {Imbalanced} {Datasets} with {Label}-{Distribution}-{Aware} {Margin} {Loss}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/621461af90cadfdaf0e8d4cc25129f91-Abstract.html},
	abstract = {Deep learning algorithms can fare poorly when the training dataset suffers from heavy class-imbalance but the testing criterion requires good generalization on less frequent classes. We design two novel methods to improve performance in such scenarios. First, we propose a theoretically-principled label-distribution-aware margin (LDAM) loss motivated by minimizing a margin-based generalization bound. This loss replaces the standard cross-entropy objective during training and can be applied with prior strategies for training with class-imbalance such as re-weighting or re-sampling. Second, we propose a simple, yet effective, training schedule that defers re-weighting until after the initial stage, allowing the model to learn an initial representation while avoiding some of the complications associated with re-weighting or re-sampling. We test our methods on several benchmark vision tasks including the real-world imbalanced dataset iNaturalist 2018. Our experiments show that either of these methods alone can already improve over existing techniques and their combination achieves even better performance gains.},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Cao, Kaidi and Wei, Colin and Gaidon, Adrien and Arechiga, Nikos and Ma, Tengyu},
	year = {2019},
	file = {Cao et al. - 2019 - Learning Imbalanced Datasets with Label-Distributi.pdf:C\:\\Users\\asus\\Zotero\\storage\\J3V9KXQ9\\Cao et al. - 2019 - Learning Imbalanced Datasets with Label-Distributi.pdf:application/pdf},
}

@article{rani_self-supervised_2023,
	title = {Self-supervised {Learning}: {A} {Succinct} {Review}},
	issn = {1886-1784},
	shorttitle = {Self-supervised {Learning}},
	url = {https://doi.org/10.1007/s11831-023-09884-2},
	doi = {10.1007/s11831-023-09884-2},
	abstract = {Machine learning has made significant advances in the field of image processing. The foundation of this success is supervised learning, which necessitates annotated labels generated by humans and hence learns from labelled data, whereas unsupervised learning learns from unlabeled data. Self-supervised learning (SSL) is a type of un-supervised learning that helps in the performance of downstream computer vision tasks such as object detection, image comprehension, image segmentation, and so on. It can develop generic artificial intelligence systems at a low cost using unstructured and unlabeled data. The authors of this review article have presented detailed literature on self-supervised learning as well as its applications in different domains. The primary goal of this review article is to demonstrate how images learn from their visual features using self-supervised approaches. The authors have also discussed various terms used in self-supervised learning as well as different types of learning, such as contrastive learning, transfer learning, and so on. This review article describes in detail the pipeline of self-supervised learning, including its two main phases: pretext and downstream tasks. The authors have shed light on various challenges encountered while working on self-supervised learning at the end of the article.},
	language = {en},
	urldate = {2023-03-26},
	journal = {Archives of Computational Methods in Engineering},
	author = {Rani, Veenu and Nabi, Syed Tufael and Kumar, Munish and Mittal, Ajay and Kumar, Krishan},
	month = jan,
	year = {2023},
	keywords = {Supervised learning, Machine learning, Contrastive learning, Self-supervised, Un-supervised learning},
	file = {Full Text PDF:C\:\\Users\\asus\\Zotero\\storage\\YQ4UW3AG\\Rani et al. - 2023 - Self-supervised Learning A Succinct Review.pdf:application/pdf},
}

@article{jing_self-supervised_2021,
	title = {Self-{Supervised} {Visual} {Feature} {Learning} {With} {Deep} {Neural} {Networks}: {A} {Survey}},
	volume = {43},
	issn = {1939-3539},
	shorttitle = {Self-{Supervised} {Visual} {Feature} {Learning} {With} {Deep} {Neural} {Networks}},
	doi = {10.1109/TPAMI.2020.2992393},
	abstract = {Large-scale labeled data are generally required to train deep neural networks in order to obtain better performance in visual feature learning from images or videos for computer vision applications. To avoid extensive cost of collecting and annotating large-scale datasets, as a subset of unsupervised learning methods, self-supervised learning methods are proposed to learn general image and video features from large-scale unlabeled data without using any human-annotated labels. This paper provides an extensive review of deep learning-based self-supervised general visual feature learning methods from images or videos. First, the motivation, general pipeline, and terminologies of this field are described. Then the common deep neural network architectures that used for self-supervised learning are summarized. Next, the schema and evaluation metrics of self-supervised learning methods are reviewed followed by the commonly used datasets for images, videos, audios, and 3D data, as well as the existing self-supervised visual feature learning methods. Finally, quantitative performance comparisons of the reviewed methods on benchmark datasets are summarized and discussed for both image and video feature learning. At last, this paper is concluded and lists a set of promising future directions for self-supervised visual feature learning.},
	number = {11},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Jing, Longlong and Tian, Yingli},
	month = nov,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {deep learning, Self-supervised learning, Task analysis, Training, Visualization, Annotations, convolutional neural network, Feature extraction, Learning systems, transfer learning, unsupervised learning, Videos},
	pages = {4037--4058},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\asus\\Zotero\\storage\\I8XRT2BS\\9086055.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\asus\\Zotero\\storage\\VSKLFFIX\\Jing and Tian - 2021 - Self-Supervised Visual Feature Learning With Deep .pdf:application/pdf},
}

@article{albelwi_survey_2022,
	title = {Survey on {Self}-{Supervised} {Learning}: {Auxiliary} {Pretext} {Tasks} and {Contrastive} {Learning} {Methods} in {Imaging}},
	volume = {24},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1099-4300},
	shorttitle = {Survey on {Self}-{Supervised} {Learning}},
	url = {https://www.mdpi.com/1099-4300/24/4/551},
	doi = {10.3390/e24040551},
	abstract = {Although deep learning algorithms have achieved significant progress in a variety of domains, they require costly annotations on huge datasets. Self-supervised learning (SSL) using unlabeled data has emerged as an alternative, as it eliminates manual annotation. To do this, SSL constructs feature representations using pretext tasks that operate without manual annotation, which allows models trained in these tasks to extract useful latent representations that later improve downstream tasks such as object classification and detection. The early methods of SSL are based on auxiliary pretext tasks as a way to learn representations using pseudo-labels, or labels that were created automatically based on the dataset’s attributes. Furthermore, contrastive learning has also performed well in learning representations via SSL. To succeed, it pushes positive samples closer together, and negative ones further apart, in the latent space. This paper provides a comprehensive literature review of the top-performing SSL methods using auxiliary pretext and contrastive learning techniques. It details the motivation for this research, a general pipeline of SSL, the terminologies of the field, and provides an examination of pretext tasks and self-supervised methods. It also examines how self-supervised methods compare to supervised ones, and then discusses both further considerations and ongoing challenges faced by SSL.},
	language = {en},
	number = {4},
	urldate = {2023-03-27},
	journal = {Entropy},
	author = {Albelwi, Saleh},
	month = apr,
	year = {2022},
	note = {Number: 4
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {contrastive learning, auxiliary pretext tasks, contrastive loss, data augmentation, downstream tasks, encoder, pretext tasks, self-supervised learning (SSL)},
	pages = {551},
	file = {Full Text PDF:C\:\\Users\\asus\\Zotero\\storage\\7AYJXSBD\\Albelwi - 2022 - Survey on Self-Supervised Learning Auxiliary Pret.pdf:application/pdf},
}

@article{ohri_review_2021,
	title = {Review on self-supervised image recognition using deep neural networks},
	volume = {224},
	issn = {0950-7051},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121003531},
	doi = {10.1016/j.knosys.2021.107090},
	abstract = {Deep learning has brought significant developments in image understanding tasks such as object detection, image classification, and image segmentation. But the success of image recognition largely relies on supervised learning that requires huge number of human-annotated labels. To avoid costly collection of labeled data and the domains where very few standard pre-trained models exist, self-supervised learning comes to our rescue. Self-supervised learning is a form of unsupervised learning that allows the network to learn rich visual features that help in performing downstream computer vision tasks such as image classification, object detection, and image segmentation. This paper provides a thorough review of self-supervised learning which has the potential to revolutionize the computer vision field using unlabeled data. First, the motivation of self-supervised learning is discussed, and other annotation efficient learning schemes. Then, the general pipeline for supervised learning and self-supervised learning is illustrated. Next, various handcrafted pretext tasks are explained that enable learning of visual features using unlabeled image dataset. The paper also highlights the recent breakthroughs in self-supervised learning using contrastive learning and clustering methods that are outperforming supervised learning. Finally, we have performance comparisons of self-supervised techniques on evaluation tasks such as image classification and detection. In the end, the paper is concluded with practical considerations and open challenges of image recognition tasks in self-supervised learning regime. From the onset of the review paper, the core focus is on visual feature learning from images using the self-supervised approaches.},
	language = {en},
	urldate = {2023-03-27},
	journal = {Knowledge-Based Systems},
	author = {Ohri, Kriti and Kumar, Mukesh},
	month = jul,
	year = {2021},
	keywords = {Deep learning, Self-supervised learning, Unsupervised learning, Contrastive learning, Convolutional neural network, Online clustering, Pretext tasks, Semi-supervised learning, Transfer learning},
	pages = {107090},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\asus\\Zotero\\storage\\UJRHLD2N\\Ohri and Kumar - 2021 - Review on self-supervised image recognition using .pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\asus\\Zotero\\storage\\FWFYPYKZ\\S0950705121003531.html:text/html},
}

@inproceedings{sohn_fixmatch_2020,
	title = {{FixMatch}: {Simplifying} {Semi}-{Supervised} {Learning} with {Consistency} and {Confidence}},
	volume = {33},
	shorttitle = {{FixMatch}},
	url = {https://proceedings.neurips.cc/paper/2020/hash/06964dce9addb1c5cb5d6e3d9838f733-Abstract.html},
	abstract = {Semi-supervised learning (SSL) provides an effective means of leveraging unlabeled data to improve a model’s performance. This domain has seen fast progress recently, at the cost of requiring more complex methods. In this paper we propose FixMatch, an algorithm that is a significant simplification of existing SSL methods. FixMatch first generates pseudo-labels using the model’s predictions on weakly-augmented unlabeled images. For a given image, the pseudo-label is only retained if the model produces a high-confidence prediction. The model is then trained to predict the pseudo-label when fed a strongly-augmented version of the same image. Despite its simplicity, we show that FixMatch achieves state-of-the-art performance across a variety of standard semi-supervised learning benchmarks, including 94.93\% accuracy on CIFAR-10 with 250 labels and 88.61\% accuracy with 40 – just 4 labels per class. We carry out an extensive ablation study to tease apart the experimental factors that are most important to FixMatch’s success. The code is available at https://github.com/google-research/fixmatch.},
	urldate = {2023-03-27},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Sohn, Kihyuk and Berthelot, David and Carlini, Nicholas and Zhang, Zizhao and Zhang, Han and Raffel, Colin A and Cubuk, Ekin Dogus and Kurakin, Alexey and Li, Chun-Liang},
	year = {2020},
	pages = {596--608},
	file = {arXiv Fulltext PDF:C\:\\Users\\asus\\Zotero\\storage\\5EQG835R\\Sohn et al. - 2020 - FixMatch Simplifying Semi-Supervised Learning wit.pdf:application/pdf;Full Text:C\:\\Users\\asus\\Zotero\\storage\\JKH4TQMK\\Sohn et al. - 2020 - FixMatch Simplifying Semi-Supervised Learning wit.pdf:application/pdf},
}

@misc{kaushik_part_2019,
	title = {Part 2: {Backpropagation} for {Convolution} with {Strides}},
	shorttitle = {Part 2},
	url = {https://medium.com/@mayank.utexas/backpropagation-for-convolution-with-strides-fb2f2efc4faa},
	abstract = {Loss gradient with respect to the filter (weight) tensor},
	language = {en},
	journal = {Medium},
	author = {Kaushik, Mayank},
	month = may,
	year = {2019},
	file = {Snapshot:C\:\\Users\\asus\\Zotero\\storage\\NZIKNTNU\\backpropagation-for-convolution-with-strides-fb2f2efc4faa.html:text/html},
}

@misc{kaushik_part_2019-1,
	title = {Part 1: {Backpropagation} for {Convolution} with {Strides}},
	shorttitle = {Part 1},
	url = {https://medium.com/@mayank.utexas/backpropagation-for-convolution-with-strides-8137e4fc2710},
	abstract = {Loss gradient with respect to the input},
	language = {en},
	journal = {Medium},
	author = {Kaushik, Mayank},
	month = may,
	year = {2019},
	file = {Snapshot:C\:\\Users\\asus\\Zotero\\storage\\HWQSPGFD\\backpropagation-for-convolution-with-strides-8137e4fc2710.html:text/html},
}

@inproceedings{le_tiny_2015,
	title = {Tiny {ImageNet} {Visual} {Recognition} {Challenge}},
	url = {https://www.semanticscholar.org/paper/Tiny-ImageNet-Visual-Recognition-Challenge-Le-Yang/384ce792cf2b2afbe001f2168bfe7d5e7804c736},
	abstract = {In this work, we investigate the effect of convolutional network depth, receptive field size, dropout layers, rectified activation unit type and dataset noise on its accuracy in Tiny-ImageNet Challenge settings. In order to make a thorough evaluation of the cause of the peformance improvement, we start with a basic 5 layer model with 5×5 convolutional receptive fields. We keep increasing network depth or reducing receptive field size, and continue applying modern techniques, such as PReLu and dropout, to the model. Our model achieves excellent performance even compared to state-of-the-art results, with 0.444 final error rate on the test set.},
	author = {Le, Ya and Yang, Xuan S.},
	year = {2015},
	annote = {[TLDR] This work investigates the effect of convolutional network depth, receptive field size, dropout layers, rectified activation unit type and dataset noise on its accuracy in Tiny-ImageNet Challenge settings and achieves excellent performance even compared to state-of-the-art results.},
	file = {Full Text PDF:C\:\\Users\\asus\\Zotero\\storage\\LK8JA5WL\\Le and Yang - 2015 - Tiny ImageNet Visual Recognition Challenge.pdf:application/pdf},
}

@inproceedings{yang_rethinking_2020,
	title = {Rethinking the {Value} of {Labels} for {Improving} {Class}-{Imbalanced} {Learning}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/e025b6279c1b88d3ec0eca6fcb6e6280-Abstract.html},
	abstract = {Real-world data often exhibits long-tailed distributions with heavy class imbalance, posing great challenges for deep recognition models. We identify a persisting dilemma on the value of labels in the context of imbalanced learning: on the one hand, supervision from labels typically leads to better results than its unsupervised counterparts; on the other hand, heavily imbalanced data naturally incurs ''label bias'' in the classifier, where the decision boundary can be drastically altered by the majority classes. In this work, we systematically investigate these two facets of labels. We demonstrate, theoretically and empirically, that class-imbalanced learning can significantly benefit in both semi-supervised and self-supervised manners. Specifically, we confirm that (1) positively, imbalanced labels are valuable: given more unlabeled data, the original labels can be leveraged with the extra data to reduce label bias in a semi-supervised manner, which greatly improves the final classifier; (2) negatively however, we argue that imbalanced labels are not useful always: classifiers that are first pre-trained in a self-supervised manner consistently outperform their corresponding baselines. Extensive experiments on large-scale imbalanced datasets verify our theoretically grounded strategies, showing superior performance over previous state-of-the-arts. Our intriguing findings highlight the need to rethink the usage of imbalanced labels in realistic long-tailed tasks. Code is available at https://github.com/YyzHarry/imbalanced-semi-self.},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Yang, Yuzhe and Xu, Zhi},
	year = {2020},
	pages = {19290--19301},
	file = {Full Text PDF:C\:\\Users\\asus\\Zotero\\storage\\D3GAPUNC\\Yang and Xu - 2020 - Rethinking the Value of Labels for Improving Class.pdf:application/pdf},
}

@inproceedings{loshchilov_sgdr_2023,
	title = {{SGDR}: {Stochastic} {Gradient} {Descent} with {Warm} {Restarts}},
	shorttitle = {{SGDR}},
	url = {https://openreview.net/forum?id=Skq89Scxx},
	abstract = {Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets, where we demonstrate new state-of-the-art results at 3.14{\textbackslash}\% and 16.21{\textbackslash}\%, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at {\textbackslash}{\textbackslash} {\textbackslash}url\{https://github.com/loshchil/SGDR\}},
	language = {en},
	author = {Loshchilov, Ilya and Hutter, Frank},
        booktitle={International Conference on Learning Representations},
	year = {2017},
	file = {Full Text PDF:C\:\\Users\\asus\\Zotero\\storage\\FIL3Q3L6\\Loshchilov and Hutter - 2023 - SGDR Stochastic Gradient Descent with Warm Restar.pdf:application/pdf},
}

@inproceedings{zagoruyko_wide_2016,
	address = {York, UK},
	title = {Wide {Residual} {Networks}},
	isbn = {978-1-901725-59-9},
	url = {http://www.bmva.org/bmvc/2016/papers/paper087/index.html},
	doi = {10.5244/C.30.87},
	language = {en},
	booktitle = {Procedings of the {British} {Machine} {Vision} {Conference} 2016},
	publisher = {British Machine Vision Association},
	author = {Zagoruyko, Sergey and Komodakis, Nikos},
	year = {2016},
	pages = {87.1--87.12},
	file = {Full Text PDF:C\:\\Users\\asus\\Zotero\\storage\\AH2NN3XS\\Zagoruyko and Komodakis - 2016 - Wide Residual Networks.pdf:application/pdf},
}

@inproceedings{madry_towards_2023,
	title = {Towards {Deep} {Learning} {Models} {Resistant} to {Adversarial} {Attacks}},
	url = {https://openreview.net/forum?id=rJzIBfZAb},
	abstract = {Recent work has demonstrated that neural networks are vulnerable to adversarial examples, i.e., inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against a well-defined class of adversaries. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest robustness against a first-order adversary as a natural security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models.},
	language = {en},
	author = {Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
        booktitle={International Conference on Learning Representations},
	year = {2018},
	file = {Full Text PDF:C\:\\Users\\asus\\Zotero\\storage\\WU5BSXIT\\Madry et al. - 2023 - Towards Deep Learning Models Resistant to Adversar.pdf:application/pdf},
}

@inproceedings{zhang_theoretically_2019,
	title = {Theoretically {Principled} {Trade}-off between {Robustness} and {Accuracy}},
	url = {https://proceedings.mlr.press/v97/zhang19p.html},
	abstract = {We identify a trade-off between robustness and accuracy that serves as a guiding principle in the design of defenses against adversarial examples. Although this problem has been widely studied empirically, much remains unknown concerning the theory underlying this trade-off. In this work, we decompose the prediction error for adversarial examples (robust error) as the sum of the natural (classification) error and boundary error, and provide a differentiable upper bound using the theory of classification-calibrated loss, which is shown to be the tightest possible upper bound uniform over all probability distributions and measurable predictors. Inspired by our theoretical analysis, we also design a new defense method, TRADES, to trade adversarial robustness off against accuracy. Our proposed algorithm performs well experimentally in real-world datasets. The methodology is the foundation of our entry to the NeurIPS 2018 Adversarial Vision Challenge in which we won the 1st place out of  2,000 submissions, surpassing the runner-up approach by 11.41\% in terms of mean L\_2 perturbation distance.},
	language = {en},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Zhang, Hongyang and Yu, Yaodong and Jiao, Jiantao and Xing, Eric and Ghaoui, Laurent El and Jordan, Michael},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {7472--7482},
	file = {Full Text PDF:C\:\\Users\\asus\\Zotero\\storage\\3UID36VH\\Zhang et al. - 2019 - Theoretically Principled Trade-off between Robustn.pdf:application/pdf;Supplementary PDF:C\:\\Users\\asus\\Zotero\\storage\\GV9B8TF5\\Zhang et al. - 2019 - Theoretically Principled Trade-off between Robustn.pdf:application/pdf},
}

@article{yang_survey_2022,
	title = {A {Survey} on {Deep} {Semi}-{Supervised} {Learning}},
	issn = {1558-2191},
	doi = {10.1109/TKDE.2022.3220219},
	abstract = {Deep semi-supervised learning is a fast-growing field with a range of practical applications. This paper provides a comprehensive survey on both fundamentals and recent advances in deep semi-supervised learning methods from perspectives of model design and unsupervised loss functions. We first present a taxonomy for deep semi-supervised learning that categorizes existing methods, including deep generative methods, consistency regularization methods, graph-based methods, pseudo-labeling methods, and hybrid methods. Then we provide a comprehensive review of 60 representative methods and offer a detailed comparison of these methods in terms of the type of losses, architecture differences, and test performance results. In addition to the progress in the past few years, we further discuss some shortcomings of existing methods and provide some tentative heuristic solutions for solving these open problems.},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Yang, Xiangli and Song, Zixing and King, Irwin and Xu, Zenglin},
	year = {2022},
	note = {Conference Name: IEEE Transactions on Knowledge and Data Engineering},
	keywords = {Deep learning, Data models, Supervised learning, Task analysis, Training, deep semi-supervised learning, semi-supervised learning, Semisupervised learning, Taxonomy},
	pages = {1--20},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\asus\\Zotero\\storage\\QMRW6UHP\\authors.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\asus\\Zotero\\storage\\V359RTZY\\Yang et al. - 2022 - A Survey on Deep Semi-Supervised Learning.pdf:application/pdf},
}

@article{maaten_visualizing_2008,
	title = {Visualizing {Data} using t-{SNE}},
	volume = {9},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v9/vandermaaten08a.html},
	abstract = {We present a new technique called "t-SNE" that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images ofobjects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large data sets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualizations produced by t-SNE are significantly better than those produced by the other techniques on almost all of the data sets.},
	number = {86},
	journal = {Journal of Machine Learning Research},
	author = {Maaten, Laurens van der and Hinton, Geoffrey},
	year = {2008},
	pages = {2579--2605},
	file = {Full Text PDF:C\:\\Users\\asus\\Zotero\\storage\\SLN8VSGU\\Maaten and Hinton - 2008 - Visualizing Data using t-SNE.pdf:application/pdf},
}

@article{selvaraju_grad-cam_2020,
	title = {Grad-{CAM}: {Visual} {Explanations} from {Deep} {Networks} via {Gradient}-{Based} {Localization}},
	volume = {128},
	issn = {1573-1405},
	shorttitle = {Grad-{CAM}},
	url = {https://doi.org/10.1007/s11263-019-01228-7},
	doi = {10.1007/s11263-019-01228-7},
	abstract = {We propose a technique for producing ‘visual explanations’ for decisions from a large class of Convolutional Neural Network (CNN)-based models, making them more transparent and explainable. Our approach—Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept (say ‘dog’ in a classification network or a sequence of words in captioning network) flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Unlike previous approaches, Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers (e.g.VGG), (2) CNNs used for structured outputs (e.g.captioning), (3) CNNs used in tasks with multi-modal inputs (e.g.visual question answering) or reinforcement learning, all without architectural changes or re-training. We combine Grad-CAM with existing fine-grained visualizations to create a high-resolution class-discriminative visualization, Guided Grad-CAM, and apply it to image classification, image captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into failure modes of these models (showing that seemingly unreasonable predictions have reasonable explanations), (b) outperform previous methods on the ILSVRC-15 weakly-supervised localization task, (c) are robust to adversarial perturbations, (d) are more faithful to the underlying model, and (e) help achieve model generalization by identifying dataset bias. For image captioning and VQA, our visualizations show that even non-attention based models learn to localize discriminative regions of input image. We devise a way to identify important neurons through Grad-CAM and combine it with neuron names (Bau et al. in Computer vision and pattern recognition, 2017) to provide textual explanations for model decisions. Finally, we design and conduct human studies to measure if Grad-CAM explanations help users establish appropriate trust in predictions from deep networks and show that Grad-CAM helps untrained users successfully discern a ‘stronger’ deep network from a ‘weaker’ one even when both make identical predictions. Our code is available at https://github.com/ramprs/grad-cam/, along with a demo on CloudCV (Agrawal et al., in: Mobile cloud visual media computing, pp 265–290. Springer, 2015) (http://gradcam.cloudcv.org) and a video at http://youtu.be/COjUB9Izk6E.},
	language = {en},
	number = {2},
	urldate = {2023-06-15},
	journal = {International Journal of Computer Vision},
	author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
	month = feb,
	year = {2020},
	keywords = {Explanations, Grad-CAM, Interpretability, Transparency, Visual explanations, Visualizations},
	pages = {336--359},
	file = {Full Text PDF:C\:\\Users\\asus\\Zotero\\storage\\Z6TII5LT\\Selvaraju et al. - 2020 - Grad-CAM Visual Explanations from Deep Networks v.pdf:application/pdf},
}

@incollection{avidan_automix_2022,
	address = {Cham},
	title = {{AutoMix}: {Unveiling} the {Power} of {Mixup} for {Stronger} {Classifiers}},
	volume = {13684},
	isbn = {978-3-031-20052-6 978-3-031-20053-3},
	shorttitle = {{AutoMix}},
	url = {https://link.springer.com/10.1007/978-3-031-20053-3_26},
	abstract = {Data mixing augmentation have proved to be effective for improving the generalization ability of deep neural networks. While early methods mix samples by hand-crafted policies (e.g., linear interpolation), recent methods utilize saliency information to match the mixed samples and labels via complex offline optimization. However, there arises a trade-off between precise mixing policies and optimization complexity. To address this challenge, we propose a novel automatic mixup (AutoMix) framework, where the mixup policy is parameterized and serves the ultimate classification goal directly. Specifically, AutoMix reformulates the mixup classification into two sub-tasks (i.e., mixed sample generation and mixup classification) with corresponding sub-networks and solves them in a bi-level optimization framework. For the generation, a learnable lightweight mixup generator, Mix Block, is designed to generate mixed samples by modeling patch-wise relationships under the direct supervision of the corresponding mixed labels. To prevent the degradation and instability of bi-level optimization, we further introduce a momentum pipeline to train AutoMix in an end-to-end manner. Extensive experiments on nine image benchmarks prove the superiority of AutoMix compared with state-of-the-arts in various classification scenarios and downstream tasks.},
	language = {en},
	urldate = {2023-06-23},
	booktitle = {Computer {Vision} – {ECCV} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Liu, Zicheng and Li, Siyuan and Wu, Di and Liu, Zihan and Chen, Zhiyuan and Wu, Lirong and Li, Stan Z.},
	editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	doi = {10.1007/978-3-031-20053-3_26},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {441--458},
	file = {Liu et al. - 2022 - AutoMix Unveiling the Power of Mixup for Stronger.pdf:C\:\\Users\\asus\\Zotero\\storage\\VYXTD85P\\Liu et al. - 2022 - AutoMix Unveiling the Power of Mixup for Stronger.pdf:application/pdf},
}

@inproceedings{rasmus_semi-supervised_2015,
	title = {Semi-supervised {Learning} with {Ladder} {Networks}},
	volume = {28},
	url = {https://proceedings.neurips.cc/paper_files/paper/2015/hash/378a063b8fdb1db941e34f4bde584c7d-Abstract.html},
	abstract = {We combine supervised learning with unsupervised learning in deep neural networks. The proposed model is trained to simultaneously minimize the sum of supervised and unsupervised cost functions by backpropagation, avoiding the need for layer-wise pre-training. Our work builds on top of the Ladder network proposed by Valpola (2015) which we extend by combining the model with supervision. We show that the resulting model reaches state-of-the-art performance in semi-supervised MNIST and CIFAR-10 classification in addition to permutation-invariant MNIST classification with all labels.},
	urldate = {2023-06-29},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Rasmus, Antti and Berglund, Mathias and Honkala, Mikko and Valpola, Harri and Raiko, Tapani},
	year = {2015},
	file = {Full Text:C\:\\Users\\asus\\Zotero\\storage\\DJ57KGLK\\Rasmus et al. - 2015 - Semi-supervised Learning with Ladder Networks.pdf:application/pdf;Full Text PDF:C\:\\Users\\asus\\Zotero\\storage\\2PCPD54S\\Rasmus et al. - 2015 - Semi-supervised Learning with Ladder Networks.pdf:application/pdf},
}

@article{lee_pseudo-label_2013,
	title = {Pseudo-{Label} : {The} {Simple} and {Efficient} {Semi}-{Supervised} {Learning} {Method} for {Deep} {Neural} {Networks}},
	journal = {ICML 2013 Workshop : Challenges in Representation Learning (WREPL)},
	author = {Lee, Dong-Hyun},
	month = jul,
	year = {2013},
}

@inproceedings{tarvainen_mean_2017,
	title = {Mean teachers are better role models: {Weight}-averaged consistency targets improve semi-supervised deep learning results},
	volume = {30},
	shorttitle = {Mean teachers are better role models},
	url = {https://proceedings.neurips.cc/paper/2017/hash/68053af2923e00204c3ca7c6a3150cf7-Abstract.html},
	abstract = {The recently proposed Temporal Ensembling has achieved state-of-the-art results in several semi-supervised learning benchmarks. It maintains an exponential moving average of label predictions on each training example, and penalizes predictions that are inconsistent with this target. However, because the targets change only once per epoch, Temporal Ensembling becomes unwieldy when learning large datasets. To overcome this problem, we propose Mean Teacher, a method that averages model weights instead of label predictions. As an additional benefit, Mean Teacher improves test accuracy and enables training with fewer labels than Temporal Ensembling. Without changing the network architecture, Mean Teacher achieves an error rate of 4.35\% on SVHN with 250 labels, outperforming Temporal Ensembling trained with 1000 labels. We also show that a good network architecture is crucial to performance. Combining Mean Teacher and Residual Networks, we improve the state of the art on CIFAR-10 with 4000 labels from 10.55\% to 6.28\%, and on ImageNet 2012 with 10\% of the labels from 35.24\% to 9.11\%.},
	urldate = {2023-06-29},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Tarvainen, Antti and Valpola, Harri},
	year = {2017},
	file = {Full Text PDF:C\:\\Users\\asus\\Zotero\\storage\\XH9GEU9W\\Tarvainen and Valpola - 2017 - Mean teachers are better role models Weight-avera.pdf:application/pdf},
}

@inproceedings{berthelot_mixmatch_2019,
	title = {{MixMatch}: {A} {Holistic} {Approach} to {Semi}-{Supervised} {Learning}},
	volume = {32},
	shorttitle = {{MixMatch}},
	url = {https://proceedings.neurips.cc/paper/2019/hash/1cd138d0499a68f4bb72bee04bbec2d7-Abstract.html},
	abstract = {Semi-supervised learning has proven to be a powerful paradigm for leveraging unlabeled data to mitigate the reliance on large labeled datasets.
In this work, we unify the current dominant approaches for semi-supervised learning to produce a new algorithm, MixMatch, that
guesses low-entropy labels for data-augmented unlabeled examples and mixes labeled and unlabeled data using MixUp.
MixMatch obtains state-of-the-art results by a large margin across many datasets and labeled data amounts. For example,
on CIFAR-10 with 250 labels, we reduce error rate by a factor of 4 (from 38\% to 11\%) and by a factor of 2 on STL-10.
We also demonstrate how MixMatch can help achieve a dramatically better accuracy-privacy trade-off for differential privacy.
Finally, we perform an ablation study to tease apart which components of MixMatch are most important for its success.
Code is attached.},
	urldate = {2023-06-29},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Berthelot, David and Carlini, Nicholas and Goodfellow, Ian and Papernot, Nicolas and Oliver, Avital and Raffel, Colin A},
	year = {2019},
	file = {Full Text PDF:C\:\\Users\\asus\\Zotero\\storage\\5UY25BF3\\Berthelot et al. - 2019 - MixMatch A Holistic Approach to Semi-Supervised L.pdf:application/pdf},
}

@article{yang_survey_2022-1,
	title = {A {Survey} on {Deep} {Semi}-{Supervised} {Learning}},
	issn = {1558-2191},
	doi = {10.1109/TKDE.2022.3220219},
	abstract = {Deep semi-supervised learning is a fast-growing field with a range of practical applications. This paper provides a comprehensive survey on both fundamentals and recent advances in deep semi-supervised learning methods from perspectives of model design and unsupervised loss functions. We first present a taxonomy for deep semi-supervised learning that categorizes existing methods, including deep generative methods, consistency regularization methods, graph-based methods, pseudo-labeling methods, and hybrid methods. Then we provide a comprehensive review of 60 representative methods and offer a detailed comparison of these methods in terms of the type of losses, architecture differences, and test performance results. In addition to the progress in the past few years, we further discuss some shortcomings of existing methods and provide some tentative heuristic solutions for solving these open problems.},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Yang, Xiangli and Song, Zixing and King, Irwin and Xu, Zenglin},
	year = {2022},
	note = {Conference Name: IEEE Transactions on Knowledge and Data Engineering},
	keywords = {Deep learning, Data models, Supervised learning, Task analysis, Training, deep semi-supervised learning, semi-supervised learning, Semisupervised learning, Taxonomy},
	pages = {1--20},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\asus\\Zotero\\storage\\KE5BJ7S7\\keywords.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\asus\\Zotero\\storage\\U9TUX5YK\\Yang et al. - 2022 - A Survey on Deep Semi-Supervised Learning.pdf:application/pdf;Submitted Version:C\:\\Users\\asus\\Zotero\\storage\\PPEDPI53\\Yang et al. - 2022 - A Survey on Deep Semi-Supervised Learning.pdf:application/pdf},
}

@article{van_engelen_survey_2020,
	title = {A survey on semi-supervised learning},
	volume = {109},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/s10994-019-05855-6},
	doi = {10.1007/s10994-019-05855-6},
	abstract = {Semi-supervised learning is the branch of machine learning concerned with using labelled as well as unlabelled data to perform certain learning tasks. Conceptually situated between supervised and unsupervised learning, it permits harnessing the large amounts of unlabelled data available in many use cases in combination with typically smaller sets of labelled data. In recent years, research in this area has followed the general trends observed in machine learning, with much attention directed at neural network-based models and generative learning. The literature on the topic has also expanded in volume and scope, now encompassing a broad spectrum of theory, algorithms and applications. However, no recent surveys exist to collect and organize this knowledge, impeding the ability of researchers and engineers alike to utilize it. Filling this void, we present an up-to-date overview of semi-supervised learning methods, covering earlier work as well as more recent advances. We focus primarily on semi-supervised classification, where the large majority of semi-supervised learning research takes place. Our survey aims to provide researchers and practitioners new to the field as well as more advanced readers with a solid understanding of the main approaches and algorithms developed over the past two decades, with an emphasis on the most prominent and currently relevant work. Furthermore, we propose a new taxonomy of semi-supervised classification algorithms, which sheds light on the different conceptual and methodological approaches for incorporating unlabelled data into the training process. Lastly, we show how the fundamental assumptions underlying most semi-supervised learning algorithms are closely connected to each other, and how they relate to the well-known semi-supervised clustering assumption.},
	language = {en},
	number = {2},
	urldate = {2023-07-01},
	journal = {Machine Learning},
	author = {van Engelen, Jesper E. and Hoos, Holger H.},
	month = feb,
	year = {2020},
	keywords = {Machine learning, Semi-supervised learning, Classification},
	pages = {373--440},
	file = {Full Text PDF:C\:\\Users\\asus\\Zotero\\storage\\LE9GRX2U\\van Engelen and Hoos - 2020 - A survey on semi-supervised learning.pdf:application/pdf},
}


@inproceedings{zhang_mixup_2018,
	title = {mixup: {Beyond} {Empirical} {Risk} {Minimization}},
	shorttitle = {mixup},
	url = {https://openreview.net/forum?id=r1Ddp1-Rb},
	abstract = {Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.},
	language = {en},
	urldate = {2023-07-03},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N. and Lopez-Paz, David},
	month = feb,
	year = {2018},
	file = {Full Text PDF:C\:\\Users\\asus\\Zotero\\storage\\46CHAXHZ\\Zhang et al. - 2018 - mixup Beyond Empirical Risk Minimization.pdf:application/pdf},
}

@inproceedings{yun_cutmix_2019,
	title = {{CutMix}: {Regularization} {Strategy} to {Train} {Strong} {Classifiers} {With} {Localizable} {Features}},
	shorttitle = {{CutMix}},
	doi = {10.1109/ICCV.2019.00612},
	abstract = {Regional dropout strategies have been proposed to enhance performance of convolutional neural network classifiers. They have proved to be effective for guiding the model to attend on less discriminative parts of objects (e.g. leg as opposed to head of a person), thereby letting the network generalize better and have better object localization capabilities. On the other hand, current methods for regional dropout removes informative pixels on training images by overlaying a patch of either black pixels or random noise. Such removal is not desirable because it suffers from information loss causing inefficiency in training. We therefore propose the CutMix augmentation strategy: patches are cut and pasted among training images where the ground truth labels are also mixed proportionally to the area of the patches. By making efficient use of training pixels and retaining the regularization effect of regional dropout, CutMix consistently outperforms state-of-the-art augmentation strategies on CIFAR and ImageNet classification tasks, as well as on ImageNet weakly-supervised localization task. Moreover, unlike previous augmentation methods, our CutMix-trained ImageNet classifier, when used as a pretrained model, results in consistent performance gain in Pascal detection and MS-COCO image captioning benchmarks. We also show that CutMix can improve the model robustness against input corruptions and its out-of distribution detection performance.},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Yun, Sangdoo and Han, Dongyoon and Chun, Sanghyuk and Oh, Seong Joon and Yoo, Youngjoon and Choe, Junsuk},
	month = oct,
	year = {2019},
	note = {ISSN: 2380-7504},
	keywords = {Computational modeling, Computer vision, Dogs, Object detection, Robustness, Task analysis, Training},
	pages = {6022--6031},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\asus\\Zotero\\storage\\QZ32HMJI\\9008296.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\asus\\Zotero\\storage\\K5HD6BSX\\Yun et al. - 2019 - CutMix Regularization Strategy to Train Strong Cl.pdf:application/pdf},
}

@inproceedings{lee_smoothmix_2020,
	title = {{SmoothMix}: a {Simple} {Yet} {Effective} {Data} {Augmentation} to {Train} {Robust} {Classifiers}},
	shorttitle = {{SmoothMix}},
	doi = {10.1109/CVPRW50498.2020.00386},
	abstract = {Data augmentation has been proven effective which, by preventing overfitting, not only enhances the performance of a deep neural network but also leads to a better generalization even with limited dataset. Recently introduced regional dropout based data augmentation strategies remove (or replace) some parts of an input image with a desideratum to make the network focus on less discriminative portions of an image, which results in an improved performance. However, such approaches usually possess' strong-edge' problem caused by an obvious change in the pixels at the positions where the image is manipulated. It may not only impact on the local convolution operation but can also provide clues for the network to latch on to, which do not align well with the fundamental philosophy of augmentation. In order to minimize such peculiarities, we introduce Smoothmix in which blending of images is done based on soft edges and the training labels are computed accordingly. Extensive analysis performed on CIFAR-10, CIFAR- 100 and ImageNet for image classification demonstrates state-of-the-art results. Furthermore, Smoothmix significantly increases the robustness of a network against image corruption which is validated by the experiments carried out on CIFAR-100-C \& ImageNet-C corruption datasets.},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
	author = {Lee, Jin-Ha and Zaheer, Muhammad Zaigham and Astrid, Marcella and Lee, Seung-Ik},
	month = jun,
	year = {2020},
	note = {ISSN: 2160-7516},
	keywords = {Image edge detection, Kernel, Predictive models, Robustness, Task analysis, Training, Transforms},
	pages = {3264--3274},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\asus\\Zotero\\storage\\7HI8FZLK\\Lee et al. - 2020 - SmoothMix a Simple Yet Effective Data Augmentatio.pdf:application/pdf},
}

@article{baek_gridmix_2021,
	title = {{GridMix}: {Strong} regularization through local context mapping},
	volume = {109},
	issn = {0031-3203},
	shorttitle = {{GridMix}},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320320303976},
	doi = {10.1016/j.patcog.2020.107594},
	abstract = {Recently developed regularization techniques improve the networks generalization by only considering the global context. Therefore, the network tends to focus on a few most discriminative subregions of an image for prediction accuracy, leading the network being sensitive to unseen or noisy data. To address this disadvantage, we introduce the concept of local context mapping by predicting patch-level labels and combine it with a method of local data augmentation by grid-based mixing, called GridMix. Through our analysis of intermediate representations, we show that our GridMix can effectively regularize the network model. Finally, our evaluation results indicate that GridMix outperforms state-of-the-art techniques in classification and adversarial robustness, and it achieves a comparable performance in weakly supervised object localization.},
	language = {en},
	urldate = {2023-07-03},
	journal = {Pattern Recognition},
	author = {Baek, Kyungjune and Bang, Duhyeon and Shim, Hyunjung},
	month = jan,
	year = {2021},
	keywords = {Data augmentation, Deep learning, Network regularization},
	pages = {107594},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\asus\\Zotero\\storage\\LK8JHVPA\\Baek et al. - 2021 - GridMix Strong regularization through local conte.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\asus\\Zotero\\storage\\M28HZM3X\\S0031320320303976.html:text/html},
}

