\vspace{-2mm}
\section{Preliminaries} \label{sec:pre}
%In this section, we first present a formal definition of PageRank. Then we formulate the problem of single-node PageRank computation. Moreover, we provide a brief introduction to Personalized PageRank, which is a variant of PageRank and always plays a vital role in PageRank computation. 
This section introduces several basic concepts that are frequently adopted in the single-node PageRank computation. 
Table~\ref{tbl:def-notation} shows the notations that are frequently used in this paper. 

\begin{table} [t]
%\vspace{-4mm}
\centering
\renewcommand{\arraystretch}{1.3}
\begin{small}
\tblcapup
\caption{\rev Table of notations.}\label{tbl:def-notation}
\vspace{-4mm}
%\resizebox{0.9\linewidth}{!}{%
\resizebox{1\linewidth}{!}{%
%\tblcapdown
%p{2.3in}
\begin{tabular} 
%{|l|p{2.3in}|} \hline
{|c@{\hspace{1mm}}|P{2.25in}|} \hline
{\bf Notation} &  {\bf Description}  \\ \hline
$G=(V,E)$ & undirected graph with vertex set $V$ and edge set $E$ \\ \hline
$n, m$ & the numbers of nodes and edges in $G$ \\ \hline
$N(u)$ & the adjacency list of node $u$\\ \hline
%$n_u=|N_{out}(u)|$ & the number of $u$'s out neighbors\\ \hline
$\A$ & the adjacency matrix of $G$\\ \hline
$d_u$ & the degree of node $u$ \\ \hline
$\davg$ & the average node degree of the graph\\ \hline
$\dmax$ & the maximum node degree of the graph\\ \hline
$\D$ & the diagonal degree matrix that $D_{uu}=d_u$\\ \hline
$\P=\A\D^{-1}$ & the transitional probability matrix\\ \hline
%$\A_{uv}$ & the edge weight of edge $(u,v)\in E$ \\ \hline
%$\|\A\|_1=\sum_{(u,v)\in E}\A_{uv}$ &  the total weights of all edges in $G$\\ \hline
%$N_{in}(u), N_{out}(u)$	& 	the in/out neightbor set of node $u$	\\ \hline
%$d_{in}(u), d_{out}(u)$ & the in/out degree of node $u$ \\ \hline
%$\mathbf{D}$ & the diagonal degree matrix of $G$\\ \hline
%$\bar{d}=\frac{1}{n}\cdot \sum_{u\in V}d_u$ & the average out-degree in graph $G$\\ \hline
%$\A_u^*=\max_{v\in N_{out}(u)}\A_{uv}$ & the maximum edge weight of all the edges connecting node $u$ and its out-neighbors\\ \hline
%$\vp_u$ & the probability distribution for the weighted sampling with replacement at node $u$ that $\vp_u(v)=\frac{\A_{uv}}{d(u)}$ for $\forall v\in N_{out}(u)$\\ \hline
%For the random walk based on weighted sampling, the walk currently at node $u$ needs tsample a out-neighbor of $u$ from the probability distribution $\vp_u$. Specifically, $\vp_u(v)=\frac{\A_{uv}}{d(u)}$ for $\forall v\in N_{out}(u)$, and $\|\vp_u\|_1=1$. \\ \hline
%$\C$, $\rho_v$, $\X$, $\mu(\C)$ & Given a set $\C$, in which every node $v\in \C$ has an associated probability $p_v\in [0,1]$, the coin flip sampling on set $\C$ aims to sample a subset $\X$ that $\forall v\in \C$ is independently sampled into $\X$ with probability $p_v$. Additionally, $\mu(\C)$ denotes the expected output size that $\mu(\C)=\sum_{v\in \C}p_v$. \\ \hline
%Given a set $N_{out}(u)$, the weighted sampling with coin flip samples a subset from $N_{out}(u)$ that each $v\in N_{out}(u)$ is independently selected with probability $\rho_v\in [0,1]$. \\ \hline
%$N_{i}(u), N_{o}(u)$	& 	the in-/out-neighbor set of node $u$	\\ \hline
%$d_u$ & the degree of node $u$ on undirected graph\\ \hline
%$d_{i}(u), d_{o}(u)$ & the in-/out-degree of node $u$ \\ \hline
%$a,b$ & the Laplacian parameters\\ \hline
%$\vec{x}$ & the graph signal vector in $\mathcal{R}^n$, $\left\| \vec{x} \right\|_{2}=1$ \\ \hline
%$w_i, Y_i$ & the $i$-th weight  and partial sum $Y_i=\sum_{k=i}^\infty w_k$\\ \hline
%$L_E = \sum_{i=0}^\infty i w_i $  & the average propagation length\\ \hline
%$\vec{w}$ & the weighted vector with $w_i$ as the $i_{th}$ entry\\ \hline
$\alpha$	& the teleport probability that an $\alpha$-discounted random walk terminates at each step \\ \hline
$\vpi(t),\epi(t)$ & the true and estimated PageRank of node $t$. \\ \hline
$\vpi_t,\epi_t$ & the true and estimated Personalized PageRank vectors with regard to node $t$. \\ \hline
%$\vp_u$ & the sampling probability vector of $\forall v\in N_{out}(u)$ in the random walk process, i.e., $\vp_u(v)=\frac{\A_{uv}}{d_u}$ \\ \hline
%$\vec{r}^{(i)},\hat{\vec{r}}^{(i)}$ & the true and estimated $i$-hop residue vectors in $\mathcal{R}^n$\\ \hline
%$\vec{q}^{(i)},\hat{\vec{q}}^{(i)}$ & the true and estimated $i$-hop reserve vectors in $\mathcal{R}^n$\\ \hline
%$\e_r,\delta$ & the relative error and threshold \\ \hline
%$\e_r, \delta, p_f$ & parameters of relative error, relative error threshold and failure probability \\ \hline
$c$ & constant relative error\\ \hline
$\tilde{O}$ & the Big-Oh natation ignoring the log factors \\ \hline
%$\pi_{\ell}(s,t), \epi_{\ell}(s,t)$	& 	the exact and estimated $\ell$-hop PPR value of node $t$ with respect to $s$. \\ \hline
%$\pi_{\ell}(s,t), r_{\ell}(s,t)$	& 	the reserve and residue of $t$ during $\ell$-hop PPR push from $s$.\\ \hline
%$X_{\ell}(u,v)$	&	the backward push's increments from node $u$ (in level $\ell$) to node v (in level $\ell+1$)\\ \hline
%$C_{\ell}(u,v)$	&	cost in the backward push from node $u$ (in level $\ell$) to node $v$	\\ \hline
%$r_s^f(u), r_t^b(u)$	&	the Node Income	of $u$ in forward / backward Push start from node $s / t$\\ \hline
%$r_s^f(u,v), r_t^b(u,v)$	&	the Edge Saving	of edge ($u,v$) in forward / backward Push start from node $s / t$\\ \hline
%$p_s^f(u,v), p_t^b(u,v)$	&	the Edge Expense of edge ($u,v$) in forward / backward Push start from node $s / t$\\ \hline			
%$c$          &   the decay factor in the definition of SimRank                   \\ \hline
%$\e, \e_{min}$         &   additive error parameter and error required for exactness ($\e_{min} = 10^{-7}$)          \\ \hline
% $\e_r$         &   the maximum relative error allowed in top-$k$ SimRank queries
%$P$, $D$   & the transition matrix and the diagonal correction matrix\\ \hline
%$\vec{\pi}_i, \vec{\pi}_i^\ell,$   & the Personalized PageRank and $\ell$-hop Personalized PageRank vectors of node $v_i$\\ \hline
%$ \vec{h}_i^\ell$   &  the $\ell$-hop Hitting Probability vector of $v_i$\\	\hline
%$\rf(s,t)$, $\pif(s,t)$ & The reserve and residue of $t$ from $s$ in the forward search \\
%\hline
%$\frsum$ & The sum of all nodes' residues during in the forward
%search from $s$\\
%\hline
%$h^{\l}(v_i, v_j)$ & the hitting probability (HP) from node $v_i$ to node $v_j$ at step $\l$ (see Section~\ref{sec:our-overview}) \\ \hline
\end{tabular}
}
\vspace{-4mm}
\end{small}
\end{table}

\subsection{PageRank}
%\header{\bf Single-Node PageRank. } 
Given an undirected and unweighted graph $G=(V,E)$ with $n$ nodes and $m$ edges, the PageRank vector $\vpi$ is an $n$-dimensional vector, which can be mathematically formulated as: 
\begin{align}~\label{eqn:def_pagerank}
\vpi=(1-\alpha)\A\D^{-1}\cdot \vpi +\frac{\alpha}{n}\cdot \bm{1}. 
\end{align}
Here $\A$ denotes the adjacency matrix of the graph, $\D$ is the diagonal degree matrix that $\D_{uu}=d_u$, $\bm{1}\in \mathbb{R}^n$ denotes an all-one vector, and $\alpha$ is a {\em constant} damping factor, which is strictly less than $1$ (i.e., $\alpha \in (0,1)$). For each node $t\in V$, we use $\vpi(t)$ to denote the PageRank value of node $t$. %We may also use $\vpi(t)$ to denote node $t$'s {\em single-node PageRank} of $t$ for simplicity. 
According to the definition formula given in Equation~\eqref{eqn:def_pagerank}, the PageRank value of node $t$ satisfies the following recurrence relation: 
%Specifically, for each node $t\in V$, 
%we can further derive: 
\begin{align}\label{eqn:ite_pagerank}
\vpi(t)=(1-\alpha)\sum_{u\in N(t)}\frac{\vpi(u)}{d_u}+\frac{\alpha}{n}, 
\end{align}
where $u$ is one of the neighbor of node $t$, and $d_u$ denotes the degree of node $u$. In particular, Equation~\eqref{eqn:ite_pagerank} also indicates a lower bound of any node's PageRank that $\vpi(t)\ge \frac{\alpha}{n}$ for each $t\in V$. 

\header{\bf $\alpha$-random walk. } 
By the definition formula of PageRank vector $\vpi$ given in Equation~\eqref{eqn:def_pagerank}, 
%Reviewing the recurrence relation for $\vpi(t)$ given in Equation~\eqref{eqn:ite_pagerank}, the PageRank vector $\vpi$ actually indicates the stationary distribution of the Markov chain that at each step: 
%the PageRank vector $\vpi$ actually corresponds to the stationary distribution of a random walk based Markov chain. Specifically 
we can further derive: 
%In the seminal paper of PageRank~\cite{page1999pagerank}, Page et al. also propose an alternative definition of the PageRank vector $\vpi$ using the stationary distribution of a Markov chain: 
\begin{align}\label{eqn:power_series}
\vpi=\left(\mathbf{I}-(1-\alpha)\A \D^{-1}\right)^{-1}\cdot \left(\frac{\alpha}{n}\cdot \bm{1}\right). 
\end{align} 
As pointed out in~\cite{lofgren2015PHDthesis}, Equation~\eqref{eqn:power_series} can be solved using a power series expansion~\cite{avrachenkov2007monte}: 
\begin{align}\label{eqn:ite_power_method}
\vpi=\sum_{i=0}^{\infty} \alpha(1-\alpha)^i\cdot (\A\D^{-1})^{i}\cdot \frac{1}{n}\cdot \bm{1}, 
\end{align}
where $\vpi$ corresponds to a random walk probability distribution. Specifically, %a random walk
%which actually indicates the probability distribution of a random walk 
%of length $L \sim G(\alpha)$~\cite{fogaras2005MC, jeh2003scaling, fogaras2003start}. Specifically, 
a random walk on the graph is a sequence of nodes $W=\{w_0, w_1, w_2, \ldots\}$ that the $i$-th step (i.e., the node $w_i$) in the walk is selected uniformly at random from the neighbor of node $w_{i-1}$. The PageRank value of node $t$ equals to the probability that a so called {\em $\alpha$-random walk} (or $\alpha$-discounted random walks in some literature)~\cite{wang2017fora, wang2020RBS} simulated from a uniformly selected source node $s$ terminates at node $t$. Note that in each step (e.g., currently at node $u$), an $\alpha$-random walk: 
\begin{itemize}
    \item with probability $(1-\alpha)$, select a neighbor $v$ uniformly at random from the adjacency list $N(u)$ of node $u$, and moves from $u$ to $v$; 
    \item with probability $\alpha$, terminates at the current node $u$. 
\end{itemize}
Therefore, the length $L$ of an $\alpha$-random walk is a geometrical random number following the geometric distribution $L\sim G(\alpha)$. The expectation of $L$ is therefore a constant that $\E\left[L\right]=\frac{1}{\alpha}$. 
%a random walk of length $L$ terminates at node $t$, where the source node $w_0$ is selected uniformly at random from the vertex set $V$ and the length of the walk $L$ is a geometrical random number generated according to the geometric distribution $G(\alpha)$. Such random walk is called as {\em $\alpha$-random walk} (or $\alpha$-discounted random walks in some literature)~\cite{wang2017fora, wang2020RBS}. %The damping factor $\alpha$ is a constant strictly less than $1$ (i.e., $\alpha \in (0,1)$). 

\header{\bf Problem Definition. } In this paper, we concern the problem of single-node PageRank computation. Specifically, given a target node $t$, a relative error parameter $\rela$, and a failure probability parameter $p_f$, we aim to derive a $(c,p_f)$ approximation of $\vpi(t)$, which is formally defined as follows. 
%Given an undirected graph $G=(V,E)$ with a target node $t\in V$, a teleport probability $\alpha \in (0,1)$, and two parameters $\rela \in [0,1]$ and $\pf \in [0,1]$, we aim to compute an $(\e_r, \pf)$ approximation of $\vpi(t)$, which is defined as below.
\begin{definition} [$(\rela,\pf)$-Approximation of Single-Node PageRank]\label{def:problem}
%\begin{definition} [Estimating Single-Node PageRank with Constant Relative Error]
%\label{def:def_relaerr}
%Given a target node $t$ on graph $G=(V,E)$, the task of estimating single-node PageRank with constant relative error $c\in (0,1)$ aims to compute an estimator $\epi(t)$ of the single-node PageRank $\vpi(t)$, such that 
Given a target node $t$ in the graph $G=(V,E)$, %an estimator $\epi(t)$ of the single-node PageRank $\vpi(t)$, two parameters $\rela\in (0,1)$ and $\pf\in (0,1)$, 
$\epi(t)$ is an $(\rela, \pf)$-approximation of the single-node PageRank $\vpi(t)$ if 
\vspace{-1mm}
\begin{equation}\nonumber %\label{eqn:def_maxerr}
\left|\epi(t) - \vpi(t) \right| \le \rela \cdot \vpi(t)
\end{equation}
%simultaneously hold with probability no less than $1-p_f$. 
holds with probability at least $1-\pf$. %, where $c$ and $p_f$ are both constants in $(0,1)$.  
\end{definition}

Note that in a line of research~\cite{bressan2018sublinear, lofgren2014FastPPR, wang2020RBS}, $\rela$ is set as a {\em constant} and thus is omitted in the Big-Oh notation. In this paper, we assume $\rela$ is a constant following this convention. Additionally, we assume $\pf$ is also a {\em constant} without loss of generality. 
%we allow a small failure probability $\pf$ for approximating the single-node PageRank $\vpi(t)$ within the desired accuracy. , we also  since 
It's worth mentioning that a constant failure probability $\pf$ can be easily reduced to arbitrarily small with only adding a log factor to the running time by utilizing the Median-of-Mean trick~\cite{charikar2002mediantrick}.


\subsection{Personalized PageRank}
%\subsection{Personalized PageRank}
%\header{\bf Personalized PageRank (PPR). }
Apart from PageRank, the seminal paper~\cite{page1999pagerank} also propose a variant of PageRank, called Personalized PageRank (PPR), to evaluate the {\em personalized} centrality of graph vertices with respect to a given source node. The definition formula of PPR is analogous to that of PageRank except for the initial distribution: % the $\frac{1}{n}\cdot \bm{1}$ term in Equation~\eqref{eqn:def_pagerank} is replaced with an one-hot vector $\bm{e}_s$: 
\begin{align}\label{eqn:def_ppr}
\vpi_s=(1-\alpha)\A \D^{-1}\cdot \vpi_s +\alpha \bm{e}_s. 
\end{align}
Specifically, $\vpi_s \in \mathcal{R}^{n}$ is called the single-source PPR vector, where $\vpi_s(t)$ denotes the PPR value of node $t$ with respect to node $s$. $\bm{e}_s$ is an one-hot vector that $\bm{e}_s(s)=1$ and $\bm{e}_s(u)=0$ if $u\neq s$. Analogously, by applying the power series expansion~\cite{avrachenkov2007monte}, we can derive: 
\begin{align}\label{eqn:ite_power_method_ppr}
\vpi_s=\sum_{\ell=0}^{\infty}\alpha (1-\alpha)^\ell \left(\A \D^{-1}\right)^{\ell}\cdot \bm{e}_s. 
\end{align}
Equation~\eqref{eqn:ite_power_method_ppr} provides a probabilistic interpretation on the PPR score. Specifically, the PPR value $\vpi_s(u)$ corresponds to the probability that an $\alpha$-random walk generated from node $s$ terminates at node $u$. Additionally, by comparing Equation~\eqref{eqn:ite_power_method_ppr} with Equation ~\eqref{eqn:ite_power_method}, we note that the PageRank score $\vpi(t)$ is actually an average over all $\vpi_u(t)$ for $\forall u\in V$: 
%the relation between PageRank and Personalized PageRank can be directly derived from Equation~\eqref{eqn:ite_power_method} and ~\eqref{eqn:ite_power_method_ppr}: 
%\begin{fact}\label{fact:PageRank_PPR}
\begin{align}\label{eqn:PageRank_PPR}
\vpi(t)=\frac{1}{n}\cdot\sum_{s\in V}\vpi_s(t). 
\end{align}
%\end{fact}
%Recall that the PageRank value of node $t$ can be interpreted as a probability that an $\alpha$-random walk simulated from a {\em random} node terminates at node $t$. In comparison, the PPR value of node $t$ w.r.t node $s$ corresponds to the probability that an $\alpha$-random walk starting from {\em node $s$} terminates at node $t$. And Equation~\eqref{eqn:PageRank_PPR} indicates that $\vpi(t)$ is an average over all $\vpi_s(t)$ for $\forall s\in V$. 
In particular, on undirected graphs, PPR vectors exhibit an underlying {\em reversibility property} that for any node-pair $(u,v)\in V^2$~\cite{lofgren2015BiPPRundirected}:   
%Furthermore, a crucial property held only by the PPR values on undirected graphs is: 
\begin{align}\label{eqn:birectional_ppr}
\vpi_u(v)\cdot d_u=\vpi_v(u) \cdot d_v. 
\end{align}

\header{\bf $\boldsymbol{\ell}$-hop PPR. } Given a source node $s$, a target node $t$ and an integer $\ell\ge 0$, the $\ell$-hop PPR $\vpi_s^{(\ell)}(t)$ corresponds to the probability that an $\alpha$-random walk generated from node $s$ terminates at node $t$ exactly in its $\ell$-th step. The $\ell$-hop PPR vector $\vpi_s^{(\ell)}$ is defined as below. 
%we further present the definition of $\ell$-hop PPR vector $\vpi_s^{(\ell)}$: 
\begin{align}\label{eqn:def_lhopppr}
\vpi^{(\ell)}_s=\alpha (1-\alpha)^\ell \cdot \left(\A \D^{-1}\right)^{\ell} \bm{e}_s. 
\end{align}
By Equation~\eqref{eqn:def_lhopppr} and Equation~\eqref{eqn:ite_power_method}, we can thus derive $\vpi_s=\sum_{\ell=0}^{\infty} \vpi_s^{(\ell)}$. %From the probabilistic aspect, the $\ell$-hop PPR of node $t$ w.r.t node $s$ equals the probability that an $\alpha$-random walk starting from node $s$ terminates at node $t$ exactly at the $\ell$-th step. 
Moreover, the $\ell$-hop PPR value $\vpi^{(\ell)}_s(u)$ admits the following recursive equation that for each node $v\in V$ and each integer $\ell\ge 1$:  
%\begin{fact}\label{fact:PPR_recur}
\begin{align}\label{eqn:PPR_recur}
\vpi_t^{(\ell+1)}(v)=\sum_{u\in N(v)}\frac{(1-\alpha)}{d_u}\cdot \vpi_t^{(\ell)}(u). 
\end{align}
%\end{fact}
Moreover, the $\ell$-hop PPR vector also exhibits the reversibility property on undirected graphs. More specifically, for every two nodes $u,v$ in an undirected $G$ and every $\ell\in \{0,1, \ldots \}$, we have: 
%\begin{fact}\label{fact:undirectedPPR}
\begin{align}\label{eqn:undirectedPPR}
\vspace{-2mm}
\vpi^{(\ell)}_s(t)\cdot d_s=\vpi^{(\ell)}_t(s)\cdot d_t. 
\end{align}
%\end{fact} 

%\header{\bf Evolving Set. } 



%\header{\bf Problem Definition. } 






%%% Local Variables:
%%% mode: latex
%%% TeX-master: "paper"
%%% End:

