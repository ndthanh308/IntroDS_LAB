\section{Theoretical Analysis}
\label{sec:analysis}
%In this section, we analyze the theoretical properties of the \setpush algorithm. First, we formally prove that the $\ell$-hop residue $\r^{(\ell)}_t(u)$ is an unbiased estimator of $\frac{1}{\alpha}\cdot \vpi^{(\ell)}_t(u)$, and the output $\epi(t)$ returned by Algorithm~\ref{alg:VBES} is an unbiased estimator of the truncated PageRank $\bpi(t)$. Then Section~\ref{subsec:variance} bounds the variance of $\epi(t)$. Finally, Section~\ref{subsec:totalcost} presents the expected time cost of the \setpush algorithm. 
In this section, we analyze the theoretical properties of our \setpush. 


\subsection{Correctness}\label{subsec:correctness}
Recall that we have presented some intuitions on $\E\left[\r^{(\ell)}_t(u)\right]=\frac{1}{\alpha}\cdot \vpi^{(\ell)}_t(u)$ and $\E\left[\epi(t)\right]=\bpi(t)$ in Section~\ref{subsec:highlevelidea}. The following Lemmas further provide formal proofs on these intuitions. %{\rev Due to the page limit, we defer the proofs of Lemma~\ref{lem:unbiasedness_er} and Lemma~\ref{lem:unbiasedness_ppr} to our Technical Report~\cite{TechnicalReport}. }

\begin{lemma}\label{lem:unbiasedness_er}
For each $\ell\in \{0,1,\ldots, L\}$ The residue vector $\er^{(\ell)}_t$ obtained in Algorithm~\ref{alg:VBES} is an unbiased estimator of $\frac{1}{\alpha}\cdot \vpi^{(\ell)}_t$, such that for each $v\in V$,   
\begin{align*}
\E \left[\er^{(\ell)}_t(v)\right]=\frac{1}{\alpha}\cdot \vpi^{(\ell)}_t(v).     
\end{align*}
\end{lemma}

\begin{proof}
Let $\incre^{(\ell+1)}(u,v)$ denote the increment of $\r^{(\ell+1)}_t(v)$ in the update procedure conducted at node $u$ with nonzero $\r^{(\ell)}_t(u)$. 
%probability mass transferred from node $u$ to node $v$ in the iteration of updating $\r_t^{(\ell+1)}$ based on $\r_t^{(\ell)}$. 
%in the $(\ell+1)$-th iteration of Algorithm~\ref{alg:VBES}. 
According to Algorithm~\ref{alg:VBES}, for each node $u\in V$ with nonzero $\er^{(\ell)}_t(u)$, $\incre^{(\ell+1)}(u,v)=\frac{1-\alpha}{d_u}\cdot \er^{(\ell)}_t(u)$ deterministically if $\frac{(1-\alpha)}{d_u}\cdot \er^{(\ell)}_t(u)\ge \th$. Otherwise, $\incre^{(\ell+1)}(u,v)=\th$ with probability $\frac{(1-\alpha)}{d_u \cdot \th}\cdot \er^{(\ell)}_t(u)$, or $0$ with probability $1-\frac{(1-\alpha)}{d_u \cdot \th}\cdot \er^{(\ell)}_t(u)$. As a consequence, the expectation of $\incre^{(\ell+1)}(u,v)$ equals $\th \cdot \frac{(1-\alpha)}{d_u \cdot \th}\cdot \er^{(\ell)}_t(u)=\frac{(1-\alpha)}{d_u}\cdot \er^{(\ell)}_t(u)$. More specifically, we have: 
\begin{align*}
\E \left[\incre^{(\ell+1)}(u,v) \mid \er^{(\ell)}_t\right]=\frac{(1-\alpha)}{d_u}\cdot \er^{(\ell)}_t(u). 
\end{align*}
where $\E \left[\incre^{(\ell+1)}(u,v) \mid \er^{(\ell)}_t\right]$ denotes the expectation of $\incre^{(\ell+1)}(u,v)$ conditioned on the fact that the $\ell$-hop residue $\r^{(\ell)}_t$ has been derived. Furthermore, since $\er^{(\ell+1)}_t(v)=\sum_{u\in N(v)}\incre^{(\ell+1)}(u,v)$, we can derive: 
\begin{align}\label{eqn:conditional_exp}
\E \left[\er^{(\ell+1)}_t(v)~\big|~\er^{(\ell)}_t\right]\hspace{-1mm}=\hspace{-3mm}\sum_{u\in N(v)}\hspace{-3mm}\E \left[\incre^{(\ell+1)}(u,v) ~\big|~\er^{(\ell)}_t\right]\hspace{-1mm}=\hspace{-3mm}\sum_{u\in N(v)}\hspace{-3mm}\frac{(1-\alpha)}{d_u}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\er^{(\ell)}_t(u)
\end{align}
by applying the linearity of expectation. Given the fact: $\E[\er^{(\ell+1)}(v)]=\E\left[ \E \left[\er^{(\ell+1)}(v)~\big|~\er^{(\ell)}_t\right]\right]$, we can further derive: 
\begin{align}\label{eqn:recur_er}
\E \left[\er^{(\ell+1)}(v)\right]=\sum_{u\in N(v)}\frac{(1-\alpha)}{d_u}\cdot \E\left[\er^{(\ell)}_t(u)\right].
\end{align}
Based on the recursive formula as shown in Equation~\eqref{eqn:recur_er}, we are able to prove Lemma~\ref{lem:unbiasedness_er} by mathematical induction. Specifically, the base case $\er^{(0)}_t=\bm{e}_s=\frac{1}{\alpha}\cdot \vpi^{(0)}_t$ holds by definition. For the inductive case, assuming that $\E\left[\er^{(\ell)}_t(u)\right]=\frac{\vpi^{(\ell)}_t(u)}{\alpha}$ holds for each $u\in V$ and some $\ell\in \{0,1,\ldots, L-1\}$. By Equation~\eqref{eqn:recur_er}, we have:   
\begin{align*}
\E\left[\er^{(\ell+1)}_t(v)\right]\hspace{-1mm}=\hspace{-1mm}\frac{1}{\alpha}\cdot \hspace{-2mm}\sum_{u\in N(v)}\hspace{-1mm}\frac{(1-\alpha)}{d_u}\cdot \vpi^{(\ell)}_t(u)=\frac{1}{\alpha}\cdot \vpi^{(\ell+1)}_t(v),  
\end{align*}
where we apply the fact that $\vpi^{(\ell+1)}_t(v)=\sum_{u\in N(v)}\frac{(1-\alpha)}{d_u}\cdot \vpi^{(\ell)}_t(u)$ as shown in Equation~\eqref{eqn:PPR_recur}. Consequently, the inductive case holds, and Lemma~\ref{lem:unbiasedness_er} follows. 
\end{proof}

Based on Lemma~\ref{lem:unbiasedness_er}, we are able to prove that Algorithm~\ref{alg:VBES} returns an unbiased estimator of the truncated PageRank $\bpi(t)$. 

\begin{lemma}\label{lem:unbiasedness_ppr}
%For each node $s\in V$, Algorithm~\ref{alg:VBES} computes an estimator $\epi_t(s)$ such that $\E[\epi_t(s)]=\bpi_t(s)$. 
Algorithm~\ref{alg:VBES} returns an unbiased estimator $\epi(t)$ of the truncated PageRank score of node $t$. Specifically, $\E[\epi(t)]=\bpi(t)$. 
\end{lemma}

\begin{proof}
Note that Algorithm~\ref{alg:VBES} computes $\epi(t)$ as: 
\begin{align*}
\epi(t)=\frac{1}{n}\cdot \sum_{s\in V}\sum_{\ell=0}^{L} \frac{d_t}{d_s}\cdot \alpha \cdot \er^{(\ell)}_t(s). 
\end{align*}
By applying the linearity of expectation, we can derive: 
\begin{align*}
\E\left[\epi(t)\right]=\frac{1}{n}\cdot \hspace{-1mm}\sum_{s\in V}\sum_{\ell=0}^{L} \frac{d_t}{d_s}\cdot \alpha \cdot \E\left[\er^{(\ell)}_t(s)\right]=\frac{1}{n}\cdot \hspace{-1mm}\sum_{s\in V}\sum_{\ell=0}^{L} \frac{d_t}{d_s}\cdot \vpi^{(\ell)}_t(s), 
\end{align*}
where we employ the expectation of $\er^{(\ell)}_t(s)$ derived in Lemma~\ref{lem:unbiasedness_er}. Furthermore, by Equation~\eqref{eqn:undirectedPPR}, we have $\frac{d_t}{d_s}\cdot \vpi^{(\ell)}_t(s)=\vpi^{(\ell)}_s(t)$. Thus,  we can derive: 
\begin{align*}
\E\left[\epi(t)\right]=\frac{1}{n}\cdot \sum_{s\in V}\sum_{\ell=0}^{L} \vpi^{(\ell)}_s(t)=\bpi(t), 
\end{align*}
which follows the lemma. 
\end{proof}

%\begin{theorem}[Unbiasedness]\label{thm:unbiasedness}
%Algorithm~\ref{alg:VBES} returns an unbiased estimator $\epi(t)$ for the PageRank of node $t$ that $\E[\epi(t)]=\vpi(t)$. 
%\end{theorem}
Up to now, we have proved that $\epi(t)$ is an unbiased estimator of the truncated PageRank $\vpi(t)$. Next, we shall bound the variance of $\epi(t)$  and utilize the following Chebyshev Inequality~\cite{mitzenmacher2017probability} to bound the failure probability for deriving a $(\frac{c}{2}, p_f)$-approximation of $\bpi(t)$. 

\begin{fact}[Chebyshev's Inequality~\cite{mitzenmacher2017probability}]\label{fact:chebyshev}
Let $X$ denote a random variable. For any real number $\e>0$, $\Pr\left\{\left|X-\E[X]\right|\ge \e\right\}\le \frac{\Var[X]}{\e^2}$. 
\end{fact}



\subsection{Variance Analysis}\label{subsec:variance}
We claim that the variance of $\epi(t)$ can be bounded by $\frac{L\theta d_t}{n}\cdot \vpi(t)$, which is formally demonstrated in Theorem~\ref{thm:variance}. 

\begin{theorem}[Variance]\label{thm:variance}
The variance of the estimator $\epi(t)$ returned by Algorithm~\ref{alg:VBES} can be bounded as $\Var[\epi(t)]\le \frac{L \theta d_t }{n}\cdot \vpi(t)$. 
\end{theorem}

To prove Theorem~\ref{thm:variance}, we need several technical lemmas. Specifically, in Lemma~\ref{lem:conditional_variance}, we bound the variance of $\er^{(\ell+1)}_t(v)$ conditioned on $\er^{(\ell)}_t$ that is derived in the $\ell$-th iteration. 

\begin{lemma}\label{lem:conditional_variance} 
For each node $v\in V$ and each $\ell \in \{0, 1, \ldots, L-1\}$, the variance of $\er^{(\ell+1)}_t(v)$ can be bounded as
\begin{align*}
\Var\left[\er^{(\ell+1)}_t(v)~\big|~ \er^{(\ell)}_t\right]\le \sum_{u\in N(v)}\th \cdot \frac{(1-\alpha)\cdot \er^{(\ell)}_t(u)}{d_u}, 
\end{align*}
where $\Var\left[\er^{(\ell+1)}_t(v)~\big|~ \er^{(\ell)}_t\right]$ denotes the variance of $\er^{(\ell+1)}_t(v)$ conditioned on the value of $\er^{(\ell)}_t$ that has been derived in the $\ell$-th iteration. 
\end{lemma}

\begin{proof}
Recall that in the proof of Lemma~\ref{lem:unbiasedness_er}, we use $\incre^{(\ell+1)}(u,v)$ to denote the increment of $\r_t^{(\ell+1)}(v)$ in the update operations conducted at node $u$ with nonzero $\r_t^{(\ell)}(u)$. 
%probability mass transferred from node $u$ to node $v$ in the $(\ell+1)$-th iteration of Algorithm~\ref{alg:VBES}. %and we have $\er^{(\ell+1)}(v)=\sum_{u\in N(v)}\incre^{(\ell+1)}(u,v)$. Thus, we bound the variance of 
For the deterministic case when $\frac{(1-\alpha)}{d_u}\cdot \er^{(\ell)}_t(u)\ge \th$, $\incre^{(\ell+1)}(u,v)$ is deterministically set as $\frac{(1-\alpha)}{d_u}\cdot \er^{(\ell)}_t(u)$, and thus there is no variance caused. For the randomized case when  $\frac{(1-\alpha)}{d_u}\cdot \er^{(\ell)}_t(u)<\th$, we set $\incre^{(\ell+1)}(u,v)$ as $\th$ with probability $\frac{(1-\alpha)}{d_u\cdot \th}\cdot \er^{(\ell)}_t(u)$, or as $0$ with probability $1-\frac{(1-\alpha)}{d_u\cdot \th}\cdot \er^{(\ell)}_t(u)$. Therefore, in the randomized case, the variance of $\incre^{(\ell+1)}(u,v)$ conditioned on the residue vector $\er^{(\ell)}_t$ that has been derived in previous iterations can be bounded as: 
\begin{equation*}
\begin{aligned}
&\Var\left[\left. \incre^{(\ell+1)}(u,v)~\right|~\er^{(\ell)}_t \right]\le \E\left[\left. \left(\incre^{(\ell+1)}(u,v)\right)^2~\right|~\er^{(\ell)}_t \right]\\
&= \th^2 \cdot \frac{(1-\alpha)}{d_u\cdot \th}\cdot \er^{(\ell)}_t(u)=\th\cdot \frac{(1-\alpha)}{d_u}\cdot \er^{(\ell)}_t(u). 
\end{aligned}
\end{equation*}
Since $\er^{(\ell+1)}_t(v)=\sum_{u\in N(v)}\incre^{(\ell+1)}(u,v)$, we can further derive: 
\begin{align*}
\Var\left[\left. \er^{(\ell+1)}_t(v)~\right|~\er^{(\ell)}_t \right]=\Var\left[\left. \sum_{u\in N(v)}\incre^{(\ell+1)}(u,v)~\right|~\er^{(\ell)}_t \right]. 
\end{align*}
Notably, for each $u\in N(v)$, $\incre^{(\ell+1)}(u,v)$ is independent with each other according to the sampling procedures as described in Section~\ref{alg:VBES}. Thus, we can further derive: 
\begin{equation*}
\begin{aligned}
\Var\left[\hspace{-0.5mm}\left. \er^{(\ell+1)}_t\hspace{-0.5mm}(v)\right|\er^{(\ell)}_t \hspace{-0.5mm}\right]\hspace{-1mm}=\hspace{-3mm}\sum_{u\in N(v)}\hspace{-3mm}\Var\hspace{-0.5mm}\left[\hspace{-0.5mm}\left. \incre^{(\ell+1)}\hspace{-0.5mm}(u,v)\right|\er^{(\ell)}_t \hspace{-0.5mm}\right]\hspace{-1mm}\le \hspace{-3mm}\sum_{u\in N(v)}\hspace{-3.5mm}\frac{\th \hspace{-0.5mm}\cdot \hspace{-0.5mm}(1\hspace{-0.5mm}-\hspace{-0.5mm}\alpha)}{d_u}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\er^{(\ell)}_t\hspace{-0.5mm}(u), 
\end{aligned}    
\end{equation*}
which follows the lemma. 
\end{proof}


In the second step, we prove: 
\begin{lemma}\label{lem:var_recur}
The variance of the estimator $\epi(t)$ obtained by Algorithm~\ref{alg:VBES} can be computed as: 
\begin{equation*}
\begin{aligned}
&\Var\left[\epi(t)\right]=\frac{\alpha^2}{n^2}\cdot \Var\left[\sum_{\ell=0}^L \sum_{s\in V}\frac{d_t}{d_s}\cdot \er^{(\ell)}_t(s)\right]\\
&=\hspace{-0.5mm}\frac{\alpha^2}{n^2}\hspace{-0.5mm}\cdot \sum_{\ell=0}^{L-2}\hspace{-0.5mm}\E\left[\Var\hspace{-0.5mm}\left[\left.\sum_{v\in V}\left(\sum_{s\in V}\frac{d_t}{d_s}\cdot \hspace{-1mm}\sum_{i=0}^{L-\ell-1}\hspace{-0.5mm}\frac{\vpi^{(i)}_v\hspace{-0.5mm}(s)}{\alpha}\right)\cdot \hspace{-0.5mm}\er^{(\ell+1)}_t(v)~\right|~\er^{(\ell)}_t\right]\right]. 
\end{aligned}    
\end{equation*}
\end{lemma}
To prove Lemma~\ref{lem:var_recur}, recall that $\epi(t)=\frac{1}{n}\sum_{s\in V}\frac{d_t}{d_s}\cdot \epi_t(s)$, and $\epi_t(s)=\sum_{\ell=0}^L \alpha \er^{(\ell)}_t(s)$ according to Algorithm~\ref{alg:VBES}. Thus, the variance of $\epi(t)$ derived by Algorithm~\ref{alg:VBES} can be computed as: 
\begin{align*}
\Var\hspace{-0.5mm}\left[\epi(t)\right]\hspace{-0.5mm}=\hspace{-0.5mm}\Var\left[\hspace{-0.5mm}\frac{1}{n}\hspace{-0.5mm}\cdot \hspace{-1mm}\sum_{s\in V}\hspace{-0.5mm}\frac{d_t}{d_s}\hspace{-0.5mm}\cdot \hspace{-1mm}\sum_{\ell=0}^{L}\hspace{-0.5mm}\alpha \hspace{-0.5mm}\cdot \hspace{-0.5mm}\er_t^{(\ell)}\hspace{-0.5mm}(s)\hspace{-0.5mm}\right]\hspace{-1mm}=\hspace{-0.5mm}\frac{\alpha^2}{n^2}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\Var\left[\sum_{s\in V}\hspace{-1mm}\frac{d_t}{d_s}\hspace{-1mm}\cdot \hspace{-1mm}\sum_{\ell=0}^{L}\hspace{-0.5mm}\er_t^{(\ell)}\hspace{-0.5mm}(s)\hspace{-0.5mm}\right], 
\end{align*}
%following the first equality in Lemma~\ref{lem:var_recur}. 
For the second equality in Lemma~\ref{lem:var_recur},  
the detailed proof is rather technical, and we defer it to the Appendix (i.e., Section~\ref{sec:appendix}) for readability. At a high level, we prove it by repeatedly applying the law of total variance. Details of the law of total variance are given as below. 

\begin{fact}[Law of Total Variance~\cite{weiss2005TotalVarianceLaw}]\label{fact:totalvar}
For two random variables $X$ and $Y$, the law of total variance states: 
\begin{align*}
\Var\left[Y\right]=\E\left[\Var \left[Y\mid X\right]\right]+\Var\left[\E\left[Y\mid X\right]\right]
\end{align*}
holds if the two variables $X$ and $Y$ are on the same probability space and the variance of $Y$ is finite. 
\end{fact}

Furthermore, we plug the variance bound derived in Lemma~\ref{lem:conditional_variance} into Lemma~\ref{lem:var_recur}, which follows Lemma~\ref{lem:var_partial}.  

\begin{lemma}\label{lem:var_partial}
For all $\ell\in [0, L]$, the residue vectors $\er^{(\ell)}$ obtained by Algorithm~\ref{alg:VBES} in the $\ell$-th iterations satisfy: 
\begin{equation*}
\begin{aligned}
\sum_{\ell=1}^{L-1}\hspace{-0.5mm}\E\hspace{-0.5mm}\left[\hspace{-0.5mm}\Var\hspace{-0.5mm}\left[\hspace{-0.5mm}\left.\sum_{v\in V}\hspace{-0.5mm}\left(\sum_{s\in V}\hspace{-0.8mm}\frac{d_t}{d_s}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\hspace{-0.5mm}\sum_{i=0}^{L-\ell}\hspace{-1.5mm}\frac{\vpi^{(i)}_v\hspace{-0.5mm}(s)}{\alpha}\hspace{-0.5mm}\right)\hspace{-0.5mm}\cdot \hspace{-0.5mm}\er^{(\ell)}_t\hspace{-0.5mm}(v)~\right|\er^{(\ell-1)}_t\hspace{-0.5mm}\right]\right]\hspace{-1mm}\le \hspace{-0.5mm}\frac{ L\th d_t \hspace{-0.5mm}\cdot \hspace{-0.5mm} n\vpi(t)}{\alpha^2}. 
\end{aligned}    
\end{equation*}
\end{lemma}

\begin{proof}
According to Algorithm~\ref{alg:VBES}, given the residue vector $\r_t^{(\ell)}$, the residue's increment $X^{(\ell+1)}(u,v)$ of node $u$ (formally defined in the proof of Lemma~\ref{lem:unbiasedness_er}) is independent with that of other nodes $w\in V$. Therefore, the variance expression given in Lemma~\ref{lem:var_recur} can be rewritten as: 
%Recall that in the $(\ell+1)$-th iteration of Algorithm~\ref{alg:VBES}, we set $\er^{(\ell+1)}(v)=\sum_{u\in N(v)}\incre^{(\ell+1)}(u,v)$. And the value of $\incre^{(\ell+1)}(u,v)$ is independent from other $\incre^{(\ell+1)}(x,y)$ for $\forall (x, y) \in E$ based on the derived residue vectors $\er^{(j)}_t$ for $j\in \{0, 1, \ldots, \ell\}$. Therefore, for each $v\in V$, $\er^{(\ell+1)}_t(v)$ is independent with each other, which follows: 
\begin{equation}\label{eqn:exp_sum}
\begin{aligned}
&\sum_{\ell=1}^{L-1}\E\left[\Var\left[\left.\sum_{v\in V}\left(\sum_{s\in V}\frac{d_t}{d_s}\cdot \sum_{i=0}^{L-\ell}\frac{\vpi^{(i)}_v(s)}{\alpha}\right)\cdot \er^{(\ell)}_t(v)~\right|\er^{(\ell-1)}_t\right]\right]\\
&=\sum_{\ell=1}^{L-1}\E\left[\sum_{v\in V}\Var\left[\left.\left(\sum_{s\in V}\frac{d_t}{d_s}\cdot \sum_{i=0}^{L-\ell}\frac{\vpi^{(i)}_v(s)}{\alpha}\right)\cdot \er^{(\ell)}_t(v)~\right|\er^{(\ell-1)}_t\right]\right]. 
\end{aligned}    
\end{equation}
Note that $\sum_{s\in V}\frac{d_t}{d_s}\cdot \sum_{i=1}^{L-\ell}\frac{\vpi^{(i)}_v(s)}{\alpha}$ is a deterministic probability mass rather than a random variable. Thus, we have: 
\begin{equation*}
\begin{aligned}
&\Var\left[\left.\left(\sum_{s\in V}\frac{d_t}{d_s}\cdot \sum_{i=0}^{L-\ell}\frac{\vpi^{(i)}_v(s)}{\alpha}\right)\cdot \er^{(\ell)}_t(v)~\right|\er^{(\ell-1)}_t\right]\\
&=\left(\sum_{s\in V}\frac{d_t}{d_s}\cdot \sum_{i=0}^{L-\ell}\frac{\vpi^{(i)}_v(s)}{\alpha}\right)^2 \cdot \Var\left[\left. \er^{(\ell)}_t(v)~\right|\er^{(\ell-1)}_t\right]
\end{aligned}    
\end{equation*}
In particular, the value of $\sum_{s\in V}\frac{d_t}{d_s}\cdot \sum_{i=1}^{L-\ell}\frac{\vpi^{(i)}_v(s)}{\alpha}$ can be upper bounded as: 
\begin{align*}
\sum_{s\in V}\frac{d_t}{d_s}\cdot \hspace{-2mm}\sum_{i=0}^{L-\ell}\hspace{-1mm}\frac{\vpi^{(i)}_v(s)}{\alpha}\le \frac{d_t}{\alpha}\cdot \hspace{-1mm}\sum_{s\in V}\sum_{i=0}^{L-\ell}\hspace{-1mm}\vpi^{(i)}_v(s) \le \frac{d_t}{\alpha}\cdot \hspace{-1mm} \sum_{s\in V}\vpi_v(s)=\frac{d_t}{\alpha}. 
\end{align*}
Plugging into Equation~\eqref{eqn:exp_sum}, we can further derive: 
\begin{equation*}
\begin{aligned}
&\sum_{\ell=0}^{L-1}\E\left[\sum_{v\in V}\Var\left[\left.\left(\sum_{s\in V}\frac{d_t}{d_s}\cdot \hspace{-1mm}\sum_{i=0}^{L-\ell}\frac{\vpi^{(i)}_v(s)}{\alpha}\right)\cdot \er^{(\ell)}_t(v)~\right|\er^{(\ell-1)}_t\right]\right]\\
&\le \hspace{-1mm}\frac{d_t}{\alpha}\cdot \hspace{-1mm}\sum_{\ell=0}^{L-1}\E\left[\sum_{v\in V}\left(\sum_{s\in V}\frac{d_t}{d_s}\cdot \hspace{-1mm}\sum_{i=0}^{L-\ell}\frac{\vpi^{(i)}_v(s)}{\alpha}\right)\cdot \Var\left[\left. \er^{(\ell)}_t(v)~\right|\er^{(\ell-1)}_t\right]\right]. 
\end{aligned}    
\end{equation*}
Recall that in Lemma~\ref{lem:conditional_variance}, we have already bounded the conditional variance: 
%\begin{align*}
$\Var\left[\left. \er^{(\ell)}_t(v)~\right|\er^{(\ell-1)}_t\right]\hspace{-1mm}\le \hspace{-0.5mm}\sum_{u\in N(v)}\hspace{-0.5mm}\frac{\th \cdot (1-\alpha)\cdot \er^{(\ell-1)}_t(u)}{d_u}$, 
%\end{align*}
Moreover, by Lemma~\ref{lem:unbiasedness_er} and Equation~\eqref{eqn:PPR_recur}, we have: 
\begin{equation*}
\begin{aligned}
&\E\left[\Var\left[\left. \er^{(\ell)}_t(v)~\right|\er^{(\ell-1)}_t\right]\right]\le \sum_{u\in N(v)}\frac{\th \cdot (1-\alpha)}{d_u}\cdot \E\left[\er^{(\ell-1)}_t(u)\right]\\
&=\sum_{u\in N(v)} \frac{\th \cdot (1-\alpha)}{\alpha \cdot d_u}\cdot \vpi^{(\ell-1)}_t(u)=\frac{\th}{\alpha}\cdot \vpi^{(\ell)}_t(v). 
\end{aligned}    
\end{equation*}
Therefore, it follows: 
\begin{equation*}
\begin{aligned}
&\sum_{\ell=1}^{L-1}\E\left[\sum_{v\in V}\Var\left[\left.\left(\sum_{s\in V}\frac{d_t}{d_s}\cdot \hspace{-1mm}\sum_{i=0}^{L-\ell}\frac{\vpi^{(i)}_v(s)}{\alpha}\right)\cdot \er^{(\ell)}_t(v)~\right|~\er^{(\ell-1)}_t\right]\right]\\
& \le \frac{d_t \cdot \th}{\alpha^2}\cdot \sum_{\ell=1}^{L-1} \sum_{s\in V}\frac{d_t}{d_s}\cdot \sum_{i=0}^{L-\ell}\sum_{v\in V} \frac{\vpi^{(i)}_v(s)}{\alpha} \cdot \vpi^{(\ell)}_t(v). 
%=\frac{d_t\cdot \th}{\alpha^2}\cdot \sum_{\ell=0}^{L-2} \sum_{s\in V}\frac{d_t}{d_s}\cdot \hspace{-2mm}\sum_{i=0}^{L-\ell-1}\vpi^{(\ell+i+1)}_t(s). 
\end{aligned}    
\end{equation*}
Note that $\sum_{v\in V}\frac{1}{\alpha}\cdot \vpi^{(i)}_v(s)\cdot \vpi^{(\ell)}_t(v)=\vpi^{(\ell+i)}_t(s)$. Moreover, 
\begin{align*}
\sum_{s\in V}\frac{d_t}{d_s}\cdot \sum_{i=0}^{L-\ell}\vpi^{(\ell+i)}_t(s)\le \sum_{s\in V}\frac{d_t}{d_s}\cdot \vpi_t(s)=\sum_{s\in V} \vpi_s(t)=n\vpi(t). 
\end{align*}
As a consequence, we can further derive: 
\begin{equation*}
\begin{aligned}
&\sum_{\ell=1}^{L-1}\E\left[\Var\left[\left.\sum_{v\in V}\left(\sum_{s\in V}\frac{d_t}{d_s}\cdot \hspace{-1mm}\sum_{i=0}^{L-\ell}\frac{\vpi^{(i)}_v(s)}{\alpha}\right)\cdot \er^{(\ell)}_t(v)~\right|\er^{(\ell-1)}_t\right]\right]\\
&\le \frac{d_t \cdot \th}{\alpha^2} \cdot \sum_{\ell=1}^{L-1} n\pi(t) \le \frac{1}{\alpha^2} \cdot L \th d_t \cdot n \vpi(t), 
\end{aligned}   
\end{equation*}
which follows the lemma. 
\end{proof}


Finally, by putting Lemma~\ref{lem:var_recur} and ~\ref{lem:var_partial} together, we can conclude that  
%\begin{align*}
$\Var\left[\epi(t)\right]\le \frac{L\cdot \th \cdot d_t}{n^2}\cdot n\epi(t)$, 
%\end{align*}
following Theorem~\ref{thm:variance}. %{\rev Detailed proofs of the above theorem and lemmas can be found in the Technical Report~\cite{TechnicalReport} due to the page limit of the main text. }


\subsection{Time Cost}\label{subsec:totalcost}
In the following, we analyze the expected time cost of the \setpush algorithm. Moreover, Theorem~\ref{thm:finalcost_analysis} provides the theoretical guarantees of the \setpush algorithm for achieving a $(\rela, \pf)$-approximation of the single-node PageRank. 

\begin{lemma}\label{lem:cost_theta}
The expected time cost of Algorithm~\ref{alg:VBES} can be bounded by ${\rev \frac{1}{\alpha \theta}=}~O\left(\frac{1}{\th}\right)$. 
\end{lemma} 

\begin{proof}
Let $Cost^{(\ell+1)}(u,v)$ denote the time cost of increasing $\r^{(\ell+1)}_t(v)$ during the update process conducted at node $u$ with nonzero $\r^{(\ell)}_t(u)$. 
%for pushing the probability mass from node $u$ to node $v$ in the $(\ell+1)$-th iteration of Algorithm~\ref{alg:VBES}. 
According to Algorithm~\ref{alg:VBES}, $Cost^{(\ell+1)}(u,v)=1$ holds deterministically if $\frac{(1-\alpha)}{d_u}\cdot \er^{(\ell)}_t(u)\ge \th$. On the other hand, if $\frac{(1-\alpha)}{d_u}\cdot \er^{(\ell)}_t(u)< \th$, $Cost^{(\ell+1)}(u,v)=1$ (i.e., pushing probability mass from node $u$ to $v$) holds with probability $\frac{(1-\alpha)}{d_u\cdot \th}\cdot \er^{(\ell)}_t(u)$, or $Cost^{(\ell+1)}(u,v)=0$ holds with probability $1-\frac{(1-\alpha)}{d_u\cdot \th}\cdot \er^{(\ell)}_t(u)$. Thus, given the $\ell$-hop residue vector $\r^{(\ell)}_t$, the expectation of $Cost^{(\ell+1)}(u,v)$ can be bounded as: 
%if $\frac{(1-\alpha)}{d_u}\cdot \er^{(\ell)}_t(u)< \th$, 
\begin{align*}
\E\left[Cost^{(\ell+1)}(u,v)~\big|~ \er^{(\ell)}_t\right]\le 1\cdot \frac{(1-\alpha)}{d_u\cdot \th}\cdot \er^{(\ell)}_t(u). 
\end{align*}
Furthermore, let $Cost^{(\ell+1)}$ denote the time cost of updating the $(\ell+1)$-hop residue vector $\r^{(\ell+1)}_t$ based on the $\ell$-hop residue vector $\r^{(\ell)}_t$. Then we have $Cost^{(\ell+1)}\hspace{-0.5mm}=\hspace{-0.5mm}\sum_{(u,v)\in E} Cost^{(\ell+1)}(u,v)$. It follows: 
\begin{equation*}
\begin{aligned}
\E\hspace{-0.5mm}\left[Cost^{(\ell+1)}~\big|~ \er^{(\ell)}_t\hspace{-0.5mm}\right]\hspace{-0.5mm}=\hspace{-3mm}\sum_{(u,v)\in E}\hspace{-2.5mm}\E\left[Cost^{(\ell+1)}(u,v)~\big|~ \er^{(\ell)}_t\hspace{-0.5mm}\right]\hspace{-0.5mm}=\hspace{-3mm}\sum_{(u,v)\in E}\hspace{-2mm}\frac{(1\hspace{-0.5mm}-\hspace{-0.5mm}\alpha)}{d_u \hspace{-0.5mm}\cdot \hspace{-0.5mm}\th}\hspace{-0.5mm}\cdot \er^{(\ell)}_t\hspace{-0.5mm}(u). 
%1\cdot \frac{(1-\alpha)}{d_u\cdot \th}\cdot \er^{(\ell)}_t(u). 
\end{aligned}    
\end{equation*}
By the property of expectation, we further have:  
\begin{equation*}
\begin{aligned}
&\E\left[Cost^{(\ell+1)}\right]=\E\left[\E\left[Cost^{(\ell+1)}~\big|~ \er^{(\ell)}_t\hspace{-0.5mm}\right]\right]=\hspace{-2mm}\sum_{(u,v)\in E}\hspace{-1mm}\frac{(1\hspace{-0.5mm}-\hspace{-0.5mm}\alpha)}{d_u \hspace{-0.5mm}\cdot \hspace{-0.5mm}\th}\hspace{-0.5mm}\cdot \E\left[\er^{(\ell)}_t\hspace{-0.5mm}(u)\right]\\
&=\frac{1}{\alpha\th}\cdot \sum_{v\in V}\sum_{u\in N(v)}\hspace{-2mm}\frac{(1\hspace{-0.5mm}-\hspace{-0.5mm}\alpha)}{d_u}\hspace{-0.5mm}\cdot \vpi^{(\ell)}_t\hspace{-0.5mm}(u)=\frac{1}{\alpha\th}\cdot \sum_{v\in V}\vpi^{(\ell+1)}_t(v),  
\end{aligned}
\end{equation*}
where we apply Lemma~\ref{lem:unbiasedness_er} in the third equality given above. We also apply Equation~\eqref{eqn:PPR_recur} in the last equality as shown above. Furthermore, let $Cost=\sum_{\ell=0}^{L-1}Cost^{(\ell+1)}$ denote the total time cost of Algorithm~\ref{alg:VBES}. Thus, we can derive: 
\begin{align*}
\E\left[Cost\right]=\hspace{-0.5mm}\sum_{\ell=0}^{L-1}\E\left[Cost^{(\ell+1)}\right]\hspace{-0.5mm}=\hspace{-0.5mm}\frac{1}{\alpha\th}\cdot \hspace{-1mm}\sum_{v\in V}\sum_{\ell=0}^{L-1}\vpi^{(\ell+1)}_t(v)\hspace{-0.5mm}\le \hspace{-0.5mm}\frac{1}{\th}=O\left(\frac{1}{\th}\right),  
\end{align*}
by applying $\sum_{\ell=0}^{L-1}\vpi^{(\ell+1)}_t(v)\le \vpi_t(v)$, and $\sum_{v\in V}\vpi^{(\ell+1)}_t(v)=\alpha$. Therefore, the lemma follows. 
\end{proof}

In the end, we employ the bound of variance $\Var\left[\epi(t)\right]$ derived in Theorem~\ref{thm:variance} to the Chebyshev's Inequality given in Fact~\ref{fact:chebyshev}, to derive an appropriate setting of the threshold $\theta$. %As shown in Theorem~\ref{thm:finalcost_analysis}, by setting $\theta$ roughly proportional to the value of $\max\left\{\frac{1}{d_t}, \frac{1}{\sqrt{m}}\right\}$, the \setpush algorithm achieves a $(\rela, \pf)$-approximation of $\vpi(t)$. Plugging the setting of $\theta$ to the results given by Lemma~\ref{lem:cost_theta}, the expected time cost can be further bounded by $\tilde{O}\left(\min\left\{d_t,\sqrt{m}\right\}\right)$.  


% Figure environment removed


% Figure environment removed



\begin{theorem}\label{thm:finalcost_analysis}
By setting $\theta=\max\left\{\frac{\alpha c^2}{12L\cdot d_t},\frac{\alpha c^2}{12L}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\sqrt{\frac{2(1-\alpha)}{m}}\right\}$, Algorithm~\ref{alg:VBES} returns a $(c,p_f)$-approximation $\epi(t)$ of $\vpi(t)$, such that $|\vpi(t)-\epi(t)|\le c\cdot \vpi(t)$ holds with constant probability. The expected time cost of Algorithm~\ref{alg:VBES} is bounded by 
\begin{align*}
{\rev 
\frac{12 \cdot \left(\log_{1-\alpha}\frac{c\alpha}{2n}\right)}{\alpha^2 c^2}\cdot \min \left\{d_t, \sqrt{\frac{m}{2(1-\alpha)}}\right\}
}
~=\tilde{O}\left(\min\left\{d_t,\sqrt{m}\right\}\right). 
\end{align*} 
%\begin{enumerate}
%\item If we set $\theta=\tilde{O}\left(\frac{1}{\sqrt{m}}\right)$, then the expected time cost of Algorithm~\ref{alg:VBES} is bounded by $\tilde{O}\left(\sqrt{m}\right)$.  
%\item If we set $\theta=\tilde{O}\left(\frac{1}{\sqrt{m}}\right)$, then the expected time cost of Algorithm~\ref{alg:VBES} is bounded by $\tilde{O}\left(\sqrt{m}\right)$. 
%\end{enumerate}
%\vspace{-1mm}
\end{theorem}

\begin{proof} %To prove Theorem~\ref{thm:finalcost_analysis}, we utilize the Chebyshev's Inequality as shown below to bound the value of $\th$. 
Recall that the variance of $\epi(t)$ obtained by Algorithm~\ref{alg:VBES} is bounded by $\frac{L\cdot \th \cdot d_t}{n}\cdot \epi(t)$ as shown in Theorem~\ref{thm:variance}. Plugging into the Chebyshev's Inequality, we can further derive: 
\begin{align*}
\Pr\left\{\epi(t)-\bpi(t)\ge \frac{c}{2} \cdot \vpi(t)\right\} \le \frac{4 \cdot \Var\left[\epi(t)\right]}{c^2 \cdot \left(\vpi(t)\right)^2}\le \frac{4L\th d_t}{c^2 \cdot n\vpi(t)}. 
\end{align*}
Thus, by setting $\th= \frac{c^2 \cdot p_f \cdot n\vpi(t)}{4Ld_t}$, $\epi(t)-\bpi(t)\le \frac{c}{2} \cdot \vpi(t)$ holds with probability at least $p_f$. In particular, we note $\frac{c^2 \cdot p_f \cdot n\vpi(t)}{4Ld_t}\ge \frac{\alpha c^2 \cdot p_f}{4Ld_t}$ based on the fact that $\vpi(t)\ge \frac{\alpha}{n}$ as illustrated in Equation~\eqref{eqn:ite_pagerank}. If we set $\theta=\frac{\alpha c^2\cdot p_f}{4Ld_t}$, then according to Lemma~\ref{lem:cost_theta}, the expected time cost of Algorithm~\ref{alg:VBES} can be bounded by %$O\left(\frac{1}{\th}\right)=O\left(\frac{12Ld_t}{\alpha c^2}\right)=\tilde{O}\left(d_t\right)$ with $L=\log_{1-\alpha}{\frac{c\alpha}{2n}}$ and the constants $\alpha$ and $c$. 
{\rev 
$\frac{1}{\alpha \theta}=\frac{4L d_t}{\alpha^2 c^2 \cdot p_f}=\tilde{O}\left(d_t\right)$, where $\alpha, c, p_f$ are all constants, and $L=\log_{1-\alpha}{\frac{c\alpha}{2n}}$ (see Section~\ref{subsec:trunc_pagerank} for the details of setting $L$). 
}
Moreover, as we shall prove below, $\frac{n \vpi(t)}{d_t}\hspace{-0.5mm}\ge \hspace{-0.5mm}\alpha  \hspace{-0.5mm} \cdot  \hspace{-0.5mm}\sqrt{\frac{2(1-\alpha)}{m}}$ holds for any $t\in V$. Thus, by setting $\th \hspace{-0.5mm}= \hspace{-0.5mm}\frac{\alpha c^2 \cdot p_f}{4L}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\sqrt{\frac{2(1-\alpha)}{m}}$, the expected time cost of Algorithm~\ref{alg:VBES} is bounded by 
%$O\left(\hspace{-0.5mm}\frac{1}{\th}\hspace{-0.5mm}\right)\hspace{-0.5mm}=\hspace{-0.5mm}O\left(\frac{12L}{\alpha c^2}\hspace{-0.5mm}\cdot \hspace{-1mm}\sqrt{\frac{m}{2(1-\alpha)}}\right)=\tilde{O}\left(\hspace{-0.5mm}\sqrt{m}\right)$. 
{\rev 
$\frac{1}{\alpha \theta}=\frac{4L}{\alpha^2 c^2 \cdot p_f}\cdot \hspace{-1mm}\sqrt{\frac{m}{2(1-\alpha)}}=\tilde{O}(\sqrt{m})$. 
}

Now we present the proof of $\frac{n \vpi(t)}{d_t}\hspace{-0.5mm}\ge \hspace{-0.5mm}\alpha \cdot \sqrt{\frac{2(1-\alpha)}{m}}$. By Equation~\eqref{eqn:ite_pagerank}, we have: 
\begin{equation}\label{eqn:sqrtm}
\begin{aligned}
\vpi(t)\ge (1-\alpha)\hspace{-2mm}\sum_{u\in N(t)}\hspace{-1mm}\frac{\vpi(u)}{d_u}+\frac{\alpha}{n} \ge (1-\alpha)\hspace{-1mm}\sum_{u\in N(t)}\frac{1}{d_u}\cdot \frac{\alpha}{n}+\frac{\alpha}{n}. %=\frac{\alpha}{n} \cdot \left(\sum_{u\in N(t)}\frac{(1-\alpha)}{d_u}\right). 
\end{aligned}    
\end{equation}
We note $\sum_{u\in N(t)}\hspace{-1mm}\frac{1}{d_u}\hspace{-0.5mm}\ge \hspace{-0.5mm}\frac{d_t^2}{2m}$ since
%\begin{align*}
$\left(\sum_{u\in N(t)}\frac{1}{d_u}\right)\cdot \left(\sum_{u\in N(t)}d_u\right)\ge \left(\sum_{u\in N(t)}1\right)^2=d_t^2$ 
%\end{align*}
holds by the Cauchy-Schwarz Inequality~\cite{steele2004cauchy}. Plugging into Inequality~\eqref{eqn:sqrtm}, we can further derive: 
\begin{align*}
\vpi(t)\hspace{-0.5mm}\ge \hspace{-0.5mm}\frac{\alpha}{n}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\left(\frac{(1-\alpha)d_t^2}{2m}\hspace{-0.5mm}+\hspace{-0.5mm}1\right)\hspace{-0.5mm}=\hspace{-0.5mm}\frac{\alpha d_t}{n}\hspace{-0.5mm}\cdot\hspace{-0.5mm} \left(\frac{(1-\alpha)d_t\hspace{-0.5mm}+\hspace{-0.5mm}\frac{2m}{d_t}}{2m}\right)\hspace{-0.5mm}\ge \hspace{-0.5mm}\frac{\alpha d_t}{n}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\sqrt{\frac{2(1\hspace{-0.5mm}-\hspace{-0.5mm}\alpha)}{m}}, 
\end{align*}
where we apply the fact that $(1-\alpha)d_t+\frac{2m}{d_t} \ge 2\cdot \sqrt{(1-\alpha)2m}$ by the AM-GM Inequality. Consequently, $\frac{n \vpi(t)}{d_t}\hspace{-0.5mm}\ge \hspace{-0.5mm}\alpha \cdot \sqrt{\frac{2(1-\alpha)}{m}}$ holds for each $t\in V$, and the theorem follows. 
\end{proof}


\begin{comment}
\begin{proof} %To prove Theorem~\ref{thm:finalcost_analysis}, we utilize the Chebyshev's Inequality as shown below to bound the value of $\th$. 
Recall that the variance of $\epi(t)$ obtained by Algorithm~\ref{alg:VBES} is bounded by $\frac{L\cdot \th \cdot d_t}{n^2}\cdot n\epi(t)$ given in Theorem~\ref{thm:variance}. By the Chebyshev's Inequality, we have: 
\begin{align*}
\Pr\left\{\epi(t)-\bpi(t)\ge \frac{c}{2} \cdot \vpi(t)\right\} \le \frac{4 \cdot \Var\left[\epi(t)\right]}{c^2 \cdot \left(\vpi(t)\right)^2}\le \frac{4L\th d_t}{c^2 \cdot n\vpi(t)}. 
\end{align*}
Thus, by setting $\th= \frac{c^2 \cdot n\vpi(t)}{12Ld_t}$, $\epi(t)-\bpi(t)\le \frac{c}{2} \cdot \vpi(t)$ holds with probability at least $\frac{1}{3}$. In particular, we note $\frac{c^2 \cdot n\vpi(t)}{12Ld_t}\ge \frac{\alpha c^2}{12Ld_t}$ based on the fact that $\vpi(t)\ge \frac{\alpha}{n}$ as illustrated in Equation~\eqref{eqn:ite_pagerank}. If we set $\theta=\frac{\alpha c^2}{12Ld_t}$, then the expected time cost of Algorithm~\ref{alg:VBES} can be bounded by $O\left(\frac{1}{\th}\right)=O\left(\frac{12Ld_t}{\alpha c^2}\right)=\tilde{O}\left(d_t\right)$ with $L=\log_{1-\alpha}{\frac{c\alpha}{2n}}$ and the constants $\alpha$ and $c$. Moreover, as we shall prove below, $\frac{n \vpi(t)}{d_t}\hspace{-0.5mm}\ge \hspace{-0.5mm}\alpha  \hspace{-0.5mm} \cdot  \hspace{-0.5mm}\sqrt{\frac{2(1-\alpha)}{m}}$ holds for any $t\in V$. Thus, by setting $\th \hspace{-0.5mm}= \hspace{-0.5mm}\frac{\alpha c^2}{12L}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\sqrt{\frac{2(1-\alpha)}{m}}$, the expected time cost of Algorithm~\ref{alg:VBES} is bounded by $O\left(\hspace{-0.5mm}\frac{1}{\th}\hspace{-0.5mm}\right)\hspace{-0.5mm}=\hspace{-0.5mm}O\left(\frac{12L}{\alpha c^2}\hspace{-0.5mm}\cdot \hspace{-1mm}\sqrt{\frac{m}{2(1-\alpha)}}\right)=\tilde{O}\left(\hspace{-0.5mm}\sqrt{m}\right)$. Now we present the proof of $\frac{n \vpi(t)}{d_t}\hspace{-0.5mm}\ge \hspace{-0.5mm}\alpha \cdot \sqrt{\frac{2(1-\alpha)}{m}}$. By Equation~\eqref{eqn:ite_pagerank}, we have: 
\begin{equation}\label{eqn:sqrtm}
\begin{aligned}
\vpi(t)\ge (1-\alpha)\hspace{-2mm}\sum_{u\in N(t)}\hspace{-1mm}\frac{\vpi(u)}{d_u}+\frac{\alpha}{n} \ge (1-\alpha)\hspace{-1mm}\sum_{u\in N(t)}\frac{1}{d_u}\cdot \frac{\alpha}{n}+\frac{\alpha}{n}. %=\frac{\alpha}{n} \cdot \left(\sum_{u\in N(t)}\frac{(1-\alpha)}{d_u}\right). 
\end{aligned}    
\end{equation}
We note $\sum_{u\in N(t)}\hspace{-1mm}\frac{1}{d_u}\hspace{-0.5mm}\ge \hspace{-0.5mm}\frac{d_t^2}{2m}$ since
%\begin{align*}
$\left(\sum_{u\in N(t)}\frac{1}{d_u}\right)\cdot \left(\sum_{u\in N(t)}d_u\right)\ge \left(\sum_{u\in N(t)}1\right)^2=d_t^2$ 
%\end{align*}
holds by the Cauchy-Schwarz Inequality~\cite{steele2004cauchy}. Plugging into Inequality~\eqref{eqn:sqrtm}, we can further derive: 
\begin{align*}
\vpi(t)\hspace{-0.5mm}\ge \hspace{-0.5mm}\frac{\alpha}{n}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\left(\frac{(1-\alpha)d_t^2}{2m}\hspace{-0.5mm}+\hspace{-0.5mm}1\right)\hspace{-0.5mm}=\hspace{-0.5mm}\frac{\alpha d_t}{n}\hspace{-0.5mm}\cdot\hspace{-0.5mm} \left(\frac{(1-\alpha)d_t\hspace{-0.5mm}+\hspace{-0.5mm}\frac{2m}{d_t}}{2m}\right)\hspace{-0.5mm}\ge \hspace{-0.5mm}\frac{\alpha d_t}{n}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\sqrt{\frac{2(1\hspace{-0.5mm}-\hspace{-0.5mm}\alpha)}{m}}, 
\end{align*}
where we apply the fact that $(1-\alpha)d_t+\frac{2m}{d_t} \ge 2\cdot \sqrt{(1-\alpha)2m}$ by the AM-GM Inequality. Consequently, $\frac{n \vpi(t)}{d_t}\hspace{-0.5mm}\ge \hspace{-0.5mm}\alpha \cdot \sqrt{\frac{2(1-\alpha)}{m}}$ holds for each $t\in V$, and the theorem follows. 
\end{proof}
\end{comment}
