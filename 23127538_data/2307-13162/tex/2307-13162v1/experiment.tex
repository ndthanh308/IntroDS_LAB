%\vspace{-3mm}
\section{Experiments} \label{sec:exp}
%\vspace{-1mm}

\begin{table}[t]
%\vspace{-5mm}
\centering
\caption{Datasets}
\vspace{-4mm}
\begin{small}
\begin{tabular}{|@{\hspace{+4mm}}l@{\hspace{+4mm}}|@{\hspace{+4mm}}r@{\hspace{+4mm}}|@{\hspace{+4mm}}r@{\hspace{+4mm}}|@{\hspace{+4mm}}r@{\hspace{+4mm}}|} \hline
{{\bf Dataset}}& {{\bf $\boldsymbol{n}$}} & {{\bf $\boldsymbol{m}$}} & {{\bf  $\boldsymbol{m/n}$}} \\ \hline
{Youtube(YT)} &{1,138,499} & {5,980,886} & {5.25}\\
{IndoChina (IC)} &{7,414,768} & {301,969,638} & {40.73}\\
{Orkut-Links (OL)} & {3,072,441} & {234,369,798} & {76.28}  \\ 
%{sx-stackoverflow}&6024271&36233450&6.01&{0.789697}\\
{Friendster (FR)} & {68,349,466} & {3,623,698,684} & {53.02} \\
\hline
\end{tabular}
\end{small}
\label{tbl:datasets}
%\tbldown
\vspace{-5mm}
\end{table}

This section presents the empirical results of \setpush. %{\sheperd The source code of \setpush has been made available at \url{https://github.com/wanghzccls/SetPush-code}}. 
All experiments are conducted on a machine with an Intel(R) Xeon(R) Gold 6126@2.60GHz CPU and 500GB memory with the Linux OS. We implement all algorithms in C++ compiled by g++ with the O3 optimization turned on. 

\header{\bf Datasets. } We use four large-scale real-world datasets in the experiments~\footnote{http://snap.stanford.edu/data}~\footnote{http://law.di.unimi.it/datasets.php}, including Youtube (YT), IndoChina (IC), Orkut-Links (OL) and Friendster (FR). 
%The four datasets are available at~\cite{snapnets,LWA}. 
The Youtube, Orkut-Links and Friendster datasets are all originated from social networks, where the nodes in the graph correspond to the users in the website, and edges indicates friendship between users. Additionally, the IndoChina is a web dataset for the country domains in Indochina. We summarize the statistics of all the datasets in Table~\ref{tbl:datasets}. 


{\rev 
\header{\bf Query Sets. } We generate two sets of query nodes, denoted as $Q_1$ and $Q_2$, in the experiments. First, for the $Q_1$ query set, we select $10$ nodes from the graph's vertex set $V$ uniformly at random. 
For the second query set $Q_2$, we select 10 query nodes from $V$ according to the node degree distribution. The larger the node's degree is, the more likely the node is selected into $Q_2$. 
Note that the PageRank distribution of a real-world network is experimentally observed to follow the power-law distribution~\cite{wei2018topppr, lofgren2016BiPPR,wei2019prsim, bahmani2010fast}, and the power-law exponent of the PageRank distribution is the same as that of the degree distribution of the network. 
%According to~\cite{wei2018topppr, lofgren2016BiPPR,wei2019prsim, bahmani2010fast}, the PageRank distribution of a scale-free network follows the same power-law exponent as the degree distribution. And most real-world networks are experimentally observed to be scale-free~\cite{lofgren2013personalized, barabasi2013network}. 
%for the graph structures derived from real-world datasets, the PageRank distribution and the degree distribution both follows the power law distribution with the same exponent. 
Therefore, by sampling query nodes according to the degree distribution, we are more likely to obtain the query nodes with relatively large PageRank scores. 
%is approximately the same as that according to the PageRank distribution. 
%We conduct experiments using the second query set to testify the performance of \setpush for computing large PageRank scores on the graph. 
%with uniform query node to evaluate the performance of \setpush for evaluating small PageRank nodes. And we complement the experiments with degree distributed query node to observe the results of all methods for dealing with the nodes whose PageRank are relatively large. 
}

%\header{\bf Ground Truths. }
%We use the power method (see Section~\ref{sec:related} for the introduction to power method) to compute the ground truths. To be more specific, we adopt the power method~\cite{page1999pagerank} to compute the PageRank values of these query nodes. Recall that the power method is an iterative method based on the definition formula of PageRank given in Equation~\eqref{eqn:def_pagerank}. In our experiments, we set the maximum iterations of power method as $100$ and fix the damping factor $\alpha$ as $0.2$. 


{\rev 
\header{\bf Parameters. } 
We compare our \setpush with five competitors: MC~\cite{fogaras2005MC}, LocalPush~\cite{lofgren2013personalized}, FastPPR~\cite{lofgren2014FastPPR}, RBS~\cite{wang2020RBS} and \sublinear~\cite{bressan2018sublinear}. Among them, MC is a Monte-Carlo method. LocalPush is a reverse exploration method. FastPPR~\cite{lofgren2014FastPPR}, RBS~\cite{wang2020RBS} and \sublinear~\cite{bressan2018sublinear} are all hybrid methods. 
%which correspond to the Monte-Carlo method, the reverse exploration method, and two hybrid methods, respectively. 
We set the parameters of these competitors strictly according to the theoretical analysis. Specifically, for the MC method~\cite{fogaras2005MC}, it has one parameter $n_r$, the number of $\alpha$-random walks. We set $n_r=\frac{\frac{2}{3}c+2}{c^2 \cdot \vpi(t)}\cdot \ln{\frac{1}{p_f}}$ according to the analysis. The LocalPush method~\cite{lofgren2013personalized} has one parameter $\e$, and we set $\e=\frac{c \alpha}{n}$. The FastPPR method has two parameters: the push threshold $r_{\max}$ and the number of random walks $n_r$. We set $r_{\max}=c\cdot \sqrt{\frac{\alpha d_t \cdot \log{(1/(1-\alpha))}}{n\cdot \log{(1/p_f)}\cdot \log{(n/\alpha)}}}$, and $n_r=\frac{45\log_{1-\alpha}{(c\alpha/2n)}}{c}\cdot \sqrt{\frac{n \cdot d_t \cdot \log{(1/(1-\alpha))}\log{(2/p_f)}}{\alpha\cdot \log{(n/\alpha)}}}$ according to FastPPR~\cite{lofgren2014FastPPR}. For RBS, recall that RBS can achieve the $\tilde{O}(n)$ time complexity by setting the threshold $\theta=\frac{\rela^2 \cdot \vpi(t)}{12\cdot \log_{1-\alpha}{(c \alpha /2n)}}$. However, we do not know the real value of $\vpi(t)$ in advance, which only can be in place of the lower bound $\frac{\alpha}{n}$ of $\vpi(t)$ according to Equation~\eqref{eqn:ite_pagerank}. Thus, in the experiments of RBS, we set $\theta=\frac{c^2 \alpha}{12n \cdot \log_{1-\alpha}{(c \alpha /2n)}}$. For the \sublinear method, it has two parameters: the number of random walks $n_r$, the number of subgraphs $k$, and the maximum iteration number $L$. We set $n_r\hspace{-0.5mm}=\hspace{-0.5mm}\min\left\{n^{\frac{2}{3}}\hspace{-1mm}\cdot \dmax^{1/3} \hspace{-1mm}\cdot \hspace{-0.5mm}\left(\ln{\frac{n}{p_f}}\right)^{\frac{1}{3}}\hspace{-2mm}\cdot \hspace{-0.5mm}\left(\ln{\frac{1}{p_f}}\right)^{\frac{1}{3}}\hspace{-2mm}\cdot c^{-\frac{4}{3}},n^{\frac{4}{5}}\davg^{\frac{1}{5}}\hspace{-1mm} \cdot \hspace{-0.5mm}\left(\ln{\frac{n}{p_f}}\right)^{\frac{1}{5}}\hspace{-2mm}\cdot \hspace{-0.5mm}\left(\ln{\frac{1}{p_f}}\right)^{\frac{2}{5}}\hspace{-2mm}\cdot c^{-\frac{6}{5}}\right\}$, 
%\begin{align*}
%\vspace{-4mm}
%n_r\hspace{-0.5mm}=\hspace{-0.5mm}\min\left\{\hspace{-0.5mm}n^{\frac{2}{3}}\hspace{-1mm}\cdot \dmax^{1/3} \hspace{-1mm}\cdot \hspace{-1mm}\left(\ln{\frac{n}{p_f}}\right)^{\frac{1}{3}}\hspace{-2mm}\cdot \hspace{-0.5mm}\left(\ln{\frac{1}{p_f}}\right)^{\frac{1}{3}}\hspace{-2mm}\cdot c^{-\frac{4}{3}}, n^{\frac{4}{5}}\davg^{\frac{1}{5}}\hspace{-1mm} \cdot \hspace{-1mm}\left(\ln{\frac{n}{p_f}}\right)^{\frac{1}{5}}\hspace{-2mm}\cdot \hspace{-0.5mm}\left(\ln{\frac{1}{p_f}}\right)^{\frac{2}{5}}\cdot \hspace{-2mm}c^{-\frac{6}{5}}\hspace{-0.5mm}\right\}, 
%\vspace{-2mm}
%\end{align*}
$k=\frac{n \cdot \log{1/p_f}}{c^2 \cdot n_r}$, and $L=\ln(c/n)$ following~\cite{bressan2018sublinear}. In all experiments, we set the failure probability $p_f=0.1$, the relative error parameter $c=0.1$, and the damping factor $\alpha=0.2$ unless otherwise specified. 



% Figure environment removed


% Figure environment removed

\header{\bf Average Overall Query Time. } 
We first compare the average overall query time of all methods over the $10$ query nodes in the $Q_1$ query sets. The experimental results are shown in Figure~\ref{fig:query_uniform_0.1} and Figure~\ref{fig:query_uniform_0.5}. Specifically, in Figure~\ref{fig:query_uniform_0.1}, we set the relative error parameter $c=0.1$. And in Figure~\ref{fig:query_uniform_0.5}, we set $c=0.5$. 
%with the relative error parameters $c$ set as $0.1$ and $0.5$, respectively. 
From Figure~\ref{fig:query_uniform_0.1} and Figure~\ref{fig:query_uniform_0.5}, we observe that our \setpush consistently outperforms other competitors, which demonstrates the superiority of our \setpush. %Additionally, we note that the Monte-Carlo method costs the largest query time among all methods. 
Moreover, we note that the MC method outperforms LocalPush in terms of query time. This implies that the complexity bound of LocalPush is not tight. Additionally, generating random numbers in each random walks of MC leads to some time overhead which influences the empirical performance of the MC method. In particular, in Figure~\ref{fig:query_uniform_0.1}, we omit the MC method on the FR dataset since the average query time of MC on the FR dataset exceeds one day. 

%This is because the time cost of MC scales linearly with the size of the graph, and the generation time of random number still costs some time overhead. Note that we omit the results of the LocalPush method and the Monte-Carlo method (MC) on Friendster (FR) since their query time both exceed one day. 

Moreover, in Figure~\ref{fig:query_degree_0.1} and Figure~\ref{fig:query_degree_0.5}, we present the average query time of each method over the query nodes in the query set $Q_2$. In particular, we omit the LocalPush method in both Figure~\ref{fig:query_degree_0.1} and Figure~\ref{fig:query_degree_0.5}, and the MC method in Figure~\ref{fig:query_degree_0.1} because the query time of these methods exceed one day. We note that our \setpush still consistently outperforms other competitors when $c=0.1$. When $c=0.5$, our \setpush improves other competitors (except the \sublinear method) by up to an order of magnitude on all datasets. However, on the YT and OL datasets, the \sublinear method slightly outperforms our \setpush. %We note that this only happens under relatively large $c$ (i.e., $c=0.5$) and the target node $t$ with high PageRank value. 
We attribute the superiority of \sublinear shown in Figure~\ref{fig:query_degree_0.5} to the blacklist trick adopted in the \sublinear method. 
%Specifically, the \sublinear~\cite{bressan2018sublinear} method includes a preprocessing phase, in which the \sublinear method samples $\ell=\min \left\{\frac{n^{2/3}\dmax^{1/3}}{c^{4/3}}, \frac{n^{4/5}d^{1/5}}{c^{6/5}}\right\}$ $\alpha$-random walks. For each $u\in V$, the \sublinear method computes $b(u)$, where $b(u)$ denotes the faction of times that the $\ell$ $\alpha$-random walks stops at node $u$. The \sublinear method defines a set $B$ called blacklist, which contains all the nodes $u\in V$ with $b(u)\ge \tilde{O}\left(\frac{1}{\ell}\right)$. During the query phase, if the given target node $t$ belongs to the blacklist $B$, the \sublinear method will stop early and return $\frac{1}{b(t)}$ as the approximation of $\vpi(t)$. Thus, for the target node $t$ with high PageRank value and large relative error $c$, the empirical running time cost may outperform our \setpush. 
However, such blacklist trick cannot improve the query time complexity of \sublinear in the worst case. %However, in practice, the \sublinear may performs well for some target node under large relative error. 
In Figure~\ref{fig:dt}, we show the query time changes of each method with increasing $d_t$ and fixed $c=0.1$. We observe our \setpush can consistently outperform \sublinear on all datasets. This demonstrates the superiority and robustness of our \setpush. On the other hand, the blacklist trick can be also adopted in our \setpush, which, however, is beyond the scope of this paper. We leave the question of how to leverage this trick for achieving a better time complexity result as our future work.


%We also plot the query time results of each method with the query node selected according to the degree distribution in Figure~\ref{fig:query_degree_0.1} and Figure~\ref{fig:query_degree_0.5}. We note that the superiority of \setpush over other competitors is a bit weakened compared to the results shown in Figure~\ref{fig:query_uniform_0.1} and Figure~\ref{fig:query_degree_0.5}. This, actually, concurs with our analysis since the expected time complexity is influenced by the value of $d_t$. Nonetheless, our \setpush still achieves the smallest query time among all the methods, except for \sublinear on OL. Meanwhile, we also observe that the \sublinear algorithm performs best among the competitors, which concurs with the theoretical analysis that the \sublinear method provides the best results before our \setpush. 



\header{\bf $\boldsymbol{d_t}$ v.s. Average Overall Query Time. } 
In Figure~\ref{fig:dt}, we present the experimental results on the trade-offs between $d_t$ and the average overal query time. We leverage such experiments to observe the query time changes of each method with increasing the value of $d_t$. Specifically, we partition the vertex set $V$ into five subsets $V_1, V_2, V_3, V_4, V_5$, such that the average node degrees $\davg_1, \davg_2, \davg_3, \davg_4, \davg_5$ of $V_1, V_2, V_3, V_4, V_5$ satisfy $\davg_1 \ge 100\davg$, $\davg_2 \in [10 \davg, 100 \davg)$, $\davg_3 \in [\davg, 10\davg)$, $\davg_4 \in [0.1 \davg, \davg)$, and $\davg_5 \in [0.01 \davg, 0.1\davg)$, respectively. In each subset (i.e., $V_1, \ldots, V_5$), we select five query nodes uniformly at random, and conduct each method to compute the five query nodes' PageRank. For each subset $V_1,\ldots, V_5$, we report the average running time of each method over the five query nodes with $c=0.1$ and $p_f=0.1$. We omit LocalPush and RBS on FR when $d_t/\davg\ge 10$ because the query time exceeds one day. From Figure~\ref{fig:dt}, we note that our \setpush consistently outperforms all baseline methods on all datasets for all query sets. In particular, for law-degree query nodes, our \setpush achieves $10\times \sim 1000\times$ improvements on the query time over existing methods. For high-degree query nodes, the superiority of \setpush is gradually weakened, but still exists. Additionally, from Figure~\ref{fig:dt}, we can derive several interesting observations: 
\begin{itemize}
    \item The query time of the Monte-Carlo method, RBS, and the \sublinear method nearly remain unchanged with the increment of $d_t$. This concurs with our analysis that the three methods do not include $d_t$ in their complexity results. 

    \item The query time of FastPPR and BiPPR increase slowly with the increment of $d_t$. This also concurs with the theoretical analysis since the time complexity of FastPPR and BiPPR is $\tilde{O}\left(n \cdot d_t\right)$, which has a logarithmic dependence on $d_t$. 

    \item Finally, we note that the query time of our \setpush and LocalPush grow linearly with $d_t$, which demonstrate the time complexities of the two methods are in linear of $d_t$. 
\end{itemize}

\vspace{-2mm}
\header{\bf Empirical Errors of \setpush. } 
In Figure~\ref{fig:epsr_exp}, we evaluate the empirical error of our \setpush. Specifically, we adopt the power method~\cite{page1999pagerank} with the iteration times $L=100$ to compute the ground truth of PageRank. %Recall that the power method is an iterative method based on the definition formula of PageRank given in Equation~\eqref{eqn:def_pagerank}. In our experiments, we set the maximum iterations of power method as $100$ and fix the dampling factor $\alpha$ as $0.2$. 
Furthermore, on each dataset, we fix the relative error parameter $c=0.1$ and run our \setpush for each query node in the set $Q_1$. Then we compute the empirical relative error $c_{emp}$ for each query node following $c_{emp}=\frac{|\epi(t)-\vpi(t)|}{\vpi(t)}$. We report the average of the values $\left(\frac{c_{emp}}{c}\right)$ over all query nodes in Figure~\ref{fig:epsr_exp}. Note that $\left(\frac{c_{emp}}{c}\right) \le 1$ implies that the empirical relative error of \setpush meets the requirement of the $(c,p_f)$-approximation of $\vpi(t)$. From Figure~\ref{fig:epsr_exp}, we observe that the empirical relative errors of \setpush on all datasets are consistently smaller than $c$. In particular, on the IC datasets, the empirical relative errors of \setpush are smaller than $c$ by up to one or two order of magnitude. This demonstrates the correctness and query efficiency of our \setpush. 
%We vary $\e_r$ from $0.1$ to $10$


}



\begin{comment}
% Figure environment removed
\end{comment}





%%% Local Variables:
%%% mode: latex
%%% TeX-master: "paper"
%%% End:
