\section{Appendix} \label{sec:appendix}
%\begin{proof}[Proof of Lemma~\ref{lem:conditional_variance}]

%\begin{proof}[Proof of Lemma~\ref{lem:unbiasedness_er}]
\subsection{\rev Proof of Lemma~\ref{lem:unbiasedness_er}}
Let $\incre^{(\ell+1)}(u,v)$ denote the increment of $\r^{(\ell+1)}_t(v)$ in the update procedure conducted at node $u$ with nonzero $\r^{(\ell)}_t(u)$. 
%probability mass transferred from node $u$ to node $v$ in the iteration of updating $\r_t^{(\ell+1)}$ based on $\r_t^{(\ell)}$. 
%in the $(\ell+1)$-th iteration of Algorithm~\ref{alg:VBES}. 
According to Algorithm~\ref{alg:VBES}, for each node $u\in V$ with nonzero $\er^{(\ell)}_t(u)$, $\incre^{(\ell+1)}(u,v)=\frac{1-\alpha}{d_u}\cdot \er^{(\ell)}_t(u)$ deterministically if $\frac{(1-\alpha)}{d_u}\cdot \er^{(\ell)}_t(u)\ge \th$. Otherwise, $\incre^{(\ell+1)}(u,v)=\th$ with probability $\frac{(1-\alpha)}{d_u \cdot \th}\cdot \er^{(\ell)}_t(u)$, or $0$ with probability $1-\frac{(1-\alpha)}{d_u \cdot \th}\cdot \er^{(\ell)}_t(u)$. As a consequence, the expectation of $\incre^{(\ell+1)}(u,v)$ equals $\th \cdot \frac{(1-\alpha)}{d_u \cdot \th}\cdot \er^{(\ell)}_t(u)=\frac{(1-\alpha)}{d_u}\cdot \er^{(\ell)}_t(u)$. More specifically, we have: 
\begin{align*}
\E \left[\incre^{(\ell+1)}(u,v) \mid \er^{(\ell)}_t\right]=\frac{(1-\alpha)}{d_u}\cdot \er^{(\ell)}_t(u). 
\end{align*}
where $\E \left[\incre^{(\ell+1)}(u,v) \mid \er^{(\ell)}_t\right]$ denotes the expectation of $\incre^{(\ell+1)}(u,v)$ conditioned on the fact that the $\ell$-hop residue $\r^{(\ell)}_t$ has been derived. Furthermore, since $\er^{(\ell+1)}_t(v)=\sum_{u\in N(v)}\incre^{(\ell+1)}(u,v)$, we can derive: 
\begin{align}\label{eqn:conditional_exp}
\E \left[\er^{(\ell+1)}_t(v)~\big|~\er^{(\ell)}_t\right]\hspace{-1mm}=\hspace{-3mm}\sum_{u\in N(v)}\hspace{-3mm}\E \left[\incre^{(\ell+1)}(u,v) ~\big|~\er^{(\ell)}_t\right]\hspace{-1mm}=\hspace{-3mm}\sum_{u\in N(v)}\hspace{-3mm}\frac{(1-\alpha)}{d_u}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\er^{(\ell)}_t(u)
\end{align}
by applying the linearity of expectation. Given the fact: $\E[\er^{(\ell+1)}(v)]=\E\left[ \E \left[\er^{(\ell+1)}(v)~\big|~\er^{(\ell)}_t\right]\right]$, we can further derive: 
\begin{align}\label{eqn:recur_er}
\E \left[\er^{(\ell+1)}(v)\right]=\sum_{u\in N(v)}\frac{(1-\alpha)}{d_u}\cdot \E\left[\er^{(\ell)}_t(u)\right].
\end{align}
Based on the recursive formula as shown in Equation~\eqref{eqn:recur_er}, we are able to prove Lemma~\ref{lem:unbiasedness_er} by mathematical induction. Specifically, the base case $\er^{(0)}_t=\bm{e}_s=\frac{1}{\alpha}\cdot \vpi^{(0)}_t$ holds by definition. For the inductive case, assuming that $\E\left[\er^{(\ell)}_t(u)\right]=\frac{\vpi^{(\ell)}_t(u)}{\alpha}$ holds for each $u\in V$ and some $\ell\in \{0,1,\ldots, L-1\}$. By Equation~\eqref{eqn:recur_er}, we have:   
\begin{align*}
\E\left[\er^{(\ell+1)}_t(v)\right]\hspace{-1mm}=\hspace{-1mm}\frac{1}{\alpha}\cdot \hspace{-2mm}\sum_{u\in N(v)}\hspace{-1mm}\frac{(1-\alpha)}{d_u}\cdot \vpi^{(\ell)}_t(u)=\frac{1}{\alpha}\cdot \vpi^{(\ell+1)}_t(v),  
\end{align*}
where we apply the fact that $\vpi^{(\ell+1)}_t(v)=\sum_{u\in N(v)}\frac{(1-\alpha)}{d_u}\cdot \vpi^{(\ell)}_t(u)$ as shown in Equation~\eqref{eqn:PPR_recur}. Consequently, the inductive case holds, and Lemma~\ref{lem:unbiasedness_er} follows. 
%\end{proof}

%\begin{proof} 
\subsection{Proof of Lemma~\ref{lem:unbiasedness_ppr}}
%\label{lem:unbiasedness_ppr}
%Recall that initially we set $\epi_t$ as $\alpha \bm{e}_s$. After the $(\ell+1)$-th iteration of Algorithm~\ref{alg:VBES}, we update $\epi_t$ by $\epi_t+\alpha \er^{(\ell+1)}_t$. Thus, for each $s\in V$, $\epi_t(s)=\sum_{\ell=0}^{L} \alpha \cdot \er^{(\ell)}_t(s)$. Furthermore, we compute $\epi(t)=\frac{1}{n}\cdot \sum_{s\in V}\frac{d_t}{d_s}\cdot \epi_t(s)$ according to Algorithm~\ref{alg:VBES}. 
According to Algorithm~\ref{alg:VBES}, the derived estimator $\epi(t)$ equals: 
\begin{align*}
\epi(t)=\frac{1}{n}\cdot \sum_{s\in V}\sum_{\ell=0}^{L} \frac{d_t}{d_s}\cdot \alpha \cdot \er^{(\ell)}_t(s). 
\end{align*}
By applying the linearity of expectation, we can derive: 
\begin{align*}
\E\left[\epi(t)\right]=\frac{1}{n}\cdot \hspace{-1mm}\sum_{s\in V}\sum_{\ell=0}^{L} \frac{d_t}{d_s}\cdot \alpha \cdot \E\left[\er^{(\ell)}_t(s)\right]=\frac{1}{n}\cdot \hspace{-1mm}\sum_{s\in V}\sum_{\ell=0}^{L} \frac{d_t}{d_s}\cdot \vpi^{(\ell)}_t(s), 
\end{align*}
where we employ the expectation of $\er^{(\ell)}_t(s)$ derived in Lemma~\ref{lem:unbiasedness_er}. Furthermore, by Equation~\eqref{eqn:undirectedPPR}, we have $\frac{d_t}{d_s}\cdot \vpi^{(\ell)}_t(s)=\vpi^{(\ell)}_s(t)$. Thus,  we can derive: 
\begin{align*}
\E\left[\epi(t)\right]=\frac{1}{n}\cdot \sum_{s\in V}\sum_{\ell=0}^{L} \vpi^{(\ell)}_s(t)=\bpi(t), 
\end{align*}
which follows the lemma. 
%\end{proof}


\subsection{\rev Proof of Lemma~\ref{lem:conditional_variance}}
Recall that in the proof of Lemma~\ref{lem:unbiasedness_er}, we use $\incre^{(\ell+1)}(u,v)$ to denote the increment of $\r_t^{(\ell+1)}(v)$ in the update operations conducted at node $u$ with nonzero $\r_t^{(\ell)}(u)$. 
%probability mass transferred from node $u$ to node $v$ in the $(\ell+1)$-th iteration of Algorithm~\ref{alg:VBES}. %and we have $\er^{(\ell+1)}(v)=\sum_{u\in N(v)}\incre^{(\ell+1)}(u,v)$. Thus, we bound the variance of 
For the deterministic case when $\frac{(1-\alpha)}{d_u}\cdot \er^{(\ell)}_t(u)\ge \th$, $\incre^{(\ell+1)}(u,v)$ is deterministically set as $\frac{(1-\alpha)}{d_u}\cdot \er^{(\ell)}_t(u)$, and thus there is no variance caused. For the randomized case when  $\frac{(1-\alpha)}{d_u}\cdot \er^{(\ell)}_t(u)<\th$, we set $\incre^{(\ell+1)}(u,v)$ as $\th$ with probability $\frac{(1-\alpha)}{d_u\cdot \th}\cdot \er^{(\ell)}_t(u)$, or as $0$ with probability $1-\frac{(1-\alpha)}{d_u\cdot \th}\cdot \er^{(\ell)}_t(u)$. Therefore, in the randomized case, the variance of $\incre^{(\ell+1)}(u,v)$ conditioned on the residue vector $\er^{(\ell)}_t$ that has been derived in previous iterations can be bounded as: 
\begin{equation*}
\begin{aligned}
&\Var\left[\left. \incre^{(\ell+1)}(u,v)~\right|~\er^{(\ell)}_t \right]\le \E\left[\left. \left(\incre^{(\ell+1)}(u,v)\right)^2~\right|~\er^{(\ell)}_t \right]\\
&= \th^2 \cdot \frac{(1-\alpha)}{d_u\cdot \th}\cdot \er^{(\ell)}_t(u)=\th\cdot \frac{(1-\alpha)}{d_u}\cdot \er^{(\ell)}_t(u). 
\end{aligned}
\end{equation*}
Since $\er^{(\ell+1)}_t(v)=\sum_{u\in N(v)}\incre^{(\ell+1)}(u,v)$, we can further derive: 
\begin{align*}
\Var\left[\left. \er^{(\ell+1)}_t(v)~\right|~\er^{(\ell)}_t \right]=\Var\left[\left. \sum_{u\in N(v)}\incre^{(\ell+1)}(u,v)~\right|~\er^{(\ell)}_t \right]. 
\end{align*}
Notably, for each $u\in N(v)$, $\incre^{(\ell+1)}(u,v)$ is independent with each other according to the sampling procedures as described in Section~\ref{alg:VBES}. Thus, we can further derive: 
\begin{equation*}
\begin{aligned}
\Var\left[\hspace{-0.5mm}\left. \er^{(\ell+1)}_t\hspace{-0.5mm}(v)\right|\er^{(\ell)}_t \hspace{-0.5mm}\right]\hspace{-1mm}=\hspace{-3mm}\sum_{u\in N(v)}\hspace{-3mm}\Var\hspace{-0.5mm}\left[\hspace{-0.5mm}\left. \incre^{(\ell+1)}\hspace{-0.5mm}(u,v)\right|\er^{(\ell)}_t \hspace{-0.5mm}\right]\hspace{-1mm}\le \hspace{-3mm}\sum_{u\in N(v)}\hspace{-3.5mm}\frac{\th \hspace{-0.5mm}\cdot \hspace{-0.5mm}(1\hspace{-0.5mm}-\hspace{-0.5mm}\alpha)}{d_u}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\er^{(\ell)}_t\hspace{-0.5mm}(u), 
\end{aligned}    
\end{equation*}
which follows the lemma. 
%\end{proof}



\subsection{Proof of Lemma~\ref{lem:var_recur}}
%We now present the complete proof of Lemma~\ref{lem:var_recur}. 
Recall that in Section~\ref{subsec:variance}, we have proved: 
\begin{align*}
\Var\left[\epi(t)\right]=\frac{\alpha^2}{n^2}\cdot \Var\left[\sum_{\ell=0}^L \sum_{s\in V}\frac{d_t}{d_s}\cdot \er^{(\ell)}_t(s)\right]
\end{align*}
In the following, we present the proof of: 
\begin{equation}\label{eqn:totalvar_final}
\begin{aligned}
&\Var\left[\sum_{\ell=0}^L \sum_{s\in V}\frac{d_t}{d_s}\cdot \er^{(\ell)}_t(s)\right]\\
&=\sum_{\ell=1}^{L-1}\E\left[\Var\left[\left.\sum_{v\in V}\left(\sum_{s\in V}\frac{d_t}{d_s}\cdot \sum_{i=0}^{L-\ell}\frac{\vpi^{(i)}_v(s)}{\alpha}\right)\cdot \er^{(\ell)}_t(v)~\right|~\er^{(\ell-1)}_t\right]\right].  
\end{aligned}    
\end{equation}
%we prove it by iteratively applying the law of total variance (Fact~\ref{fact:totalvar}). The formal proof is as follows. 
Specifically, by the law of total variance, we can derive 
\begin{equation}\label{eqn:total_var1}
\begin{aligned}
\Var\left[\sum_{\ell=0}^L \sum_{s\in V}\frac{d_t}{d_s}\cdot \er^{(\ell)}_t(s)\right]
&=\E\left[\Var\left[\sum_{\ell=0}^L \sum_{s\in V}\frac{d_t}{d_s}\cdot \er^{(\ell)}_t(s)~\Big|~ \er_t^{(L-1)}\right]\right]\\
&+\Var\left[\E\left[\sum_{\ell=0}^L \sum_{s\in V}\frac{d_t}{d_s}\cdot \er^{(\ell)}_t(s)~\Big|~ \er_t^{(L-1)}\right]\right]
\end{aligned}
\end{equation}
As we shall show in the following, the second term in the right hand side of Equation~\eqref{eqn:total_var1} can be iteratively rewritten as the sum of an expectation and, again, a variance expression. Thus, we can repeatedly adopt the law of total variance to further rewrite the new variance expression as the summation of an expectation and a variance. By repeating the above process, in the end, we will derive Equation~\eqref{eqn:totalvar_final}. Details are presented as below. 

As the first step, we note that the variance term given in the right hand side of Equation~\eqref{eqn:total_var1} can be rewritten as below by the linearity of expectation. 
\begin{equation}\label{eqn:total_var2}
\begin{aligned}
&\hspace{-0.5mm}\Var\hspace{-0.5mm}\left[\hspace{-0.5mm}\E\hspace{-0.5mm}\left[\hspace{-0.5mm}\sum_{\ell=0}^L \sum_{s\in V}\hspace{-0.5mm}\frac{d_t}{d_s}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\er^{(\ell)}_t\hspace{-0.5mm}(s)\Big|~\er_t^{(L-1)}\hspace{-0.5mm}\right]\hspace{-0.5mm}\right]
\hspace{-1mm}=\hspace{-0.5mm}\Var\hspace{-0.5mm}\left[\hspace{-0.5mm}\sum_{s\in V}\hspace{-0.5mm}\frac{d_t}{d_s}\hspace{-0.5mm}\cdot \hspace{-1mm}\sum_{\ell=0}^L  \hspace{-0.5mm}\E\hspace{-0.5mm}\left[\hspace{-0.5mm}\er^{(\ell)}_t\hspace{-0.5mm}(s)\Big|~\er_t^{(L-1)}\hspace{-0.5mm}\right]\hspace{-0.5mm}\right]\\
&=\Var\left[\sum_{s\in V}\frac{d_t}{d_s}\cdot \left(\E\left[\er^{(L)}_t(s)\Big|~\er_t^{(L-1)}\right]+\sum_{\ell=0}^{L-1}  \E\left[\er^{(\ell)}_t(s)\Big|~\er_t^{(L-1)}\right]\right)\right]
\end{aligned}
\end{equation}
We note that for every $\ell\in [0,L-1]$, 
$\E\left[\er^{(\ell)}_t(s)~\Big|~\er_t^{(L-1)}\right]= \er^{(\ell)}_t(s)$.  
And for $\E\left[\er^{(L)}_t(s)\Big|~\er_t^{(L-1)}\right]$, we have: 
\begin{align*}
\E\left[\er^{(L)}_t(s)~\Big|~\er_t^{(L-1)}\right]=\sum_{u\in N(s)}\frac{(1-\alpha)}{d_u}\cdot \r^{(\ell)}_t(u)
\end{align*}
according to Equation~\eqref{eqn:conditional_exp}. 
In particular, we note that $\frac{(1-\alpha)}{d_u}=\frac{\vpi^{(1)}_u(s)}{\alpha}$ holds for every $u\in N(s)$ according to the definition formula of the $\ell$-hop PPR as shown in Equation~\eqref{eqn:def_lhopppr}. Thus, we further have: 
\begin{align*}
\E\left[\er^{(L)}_t(s)\Big|~\er_t^{(L-1)}\right]=\sum_{u\in N(s)}\frac{\vpi^{(1)}_u(s)}{\alpha}\cdot \er^{(L-1)}_t(u)
\end{align*}
Plugging into Equation~\eqref{eqn:total_var2}, we can therefore derive: 
\begin{equation}\label{eqn:total_var3}
\begin{aligned}
&\Var\left[\E\left[\sum_{\ell=0}^L \sum_{s\in V}\frac{d_t}{d_s}\cdot \er^{(\ell)}_t(s)~\Big|~\er_t^{(L-1)}\right]\right]\\
&=\Var\left[\sum_{s\in V}\frac{d_t}{d_s}\cdot \left(\sum_{u\in N(s)}\hspace{-1mm}\frac{1}{\alpha}\cdot \vpi^{(1)}_u(s) \cdot \er^{(L-1)}_t(u)+\sum_{\ell=0}^{L-1}\er^{(\ell)}_t(s)\right)\right]
\end{aligned}
\end{equation}
In particular, by Equation~\eqref{eqn:PPR_recur}, we have the following fact: 
\begin{itemize}
    \item $\vpi^{(0)}_s(s)=\frac{1}{\alpha}$, and $\vpi^{(0)}_s(u)=0$ for every $u\neq s$, 
    \item $\vpi^{(1)}_s(u)=0$ for every $u \notin N(s)$, 
\end{itemize}
Equation~\eqref{eqn:total_var3} can be further expressed as: 
\begin{equation}\label{eqn:total_var_mid}
\begin{aligned}
&\Var\left[\E\left[\sum_{\ell=0}^L \sum_{s\in V}\frac{d_t}{d_s}\cdot \er^{(\ell)}_t(s)\Big|~\er_t^{(L-1)}\right]\right]\\
&=\Var\left[\sum_{s\in V}\frac{d_t}{d_s}\cdot \left(\sum_{i=0}^{1}\sum_{u\in V}\hspace{-1mm}\frac{1}{\alpha}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\vpi^{(i)}_u(s)\hspace{-0.5mm} \cdot \hspace{-0.5mm}\er^{(L-1)}_t(u)+\sum_{\ell=0}^{L-2}\er^{(\ell)}_t(s)\right)\right]. 
\end{aligned}
\end{equation}
Again, we apply the law of total variance (Fact~\ref{fact:totalvar}) to Equation~\eqref{eqn:total_var_mid}, which follows: 
\begin{equation}\label{eqn:total_var4}
\begin{aligned}
&\Var\left[\sum_{s\in V}\frac{d_t}{d_s}\cdot \left(\sum_{i=0}^{1}\sum_{u\in V}\hspace{-1mm}\frac{1}{\alpha}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\vpi^{(i)}_u(s)\hspace{-0.5mm} \cdot \hspace{-0.5mm}\er^{(L-1)}_t(u)+\sum_{\ell=0}^{L-2}\er^{(\ell)}_t\hspace{-0.5mm}(s)\right)\right]\\
&=\E\left[\Var\left[\sum_{s\in V}\frac{d_t}{d_s}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\left(\sum_{i=0}^{1}\sum_{u\in V}\hspace{-1mm}\frac{1}{\alpha}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\vpi^{(i)}_u(s)\hspace{-0.5mm} \cdot \hspace{-0.5mm}\er^{(L-1)}_t\hspace{-0.5mm}(u)+\hspace{-1mm}\sum_{\ell=0}^{L-2}\hspace{-0.5mm}\er^{(\ell)}_t\hspace{-0.5mm}(s)\hspace{-0.5mm}\right)\Big|~ \er^{(L-2)}_t\hspace{-0.5mm}\right]\hspace{-0.5mm}\right]\\
&+\Var\left[\E\left[\sum_{s\in V}\frac{d_t}{d_s}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\left(\sum_{i=0}^{1}\sum_{u\in V}\hspace{-1mm}\frac{1}{\alpha}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\vpi^{(i)}_u(s)\hspace{-0.5mm} \cdot \hspace{-0.5mm}\er^{(L-1)}_t\hspace{-0.5mm}(u)+\hspace{-1mm}\sum_{\ell=0}^{L-2}\hspace{-0.5mm}\er^{(\ell)}_t\hspace{-0.5mm}(s)\hspace{-0.5mm}\right)\Big|~\er^{(L-2)}_t\hspace{-0.5mm}\right]\hspace{-0.5mm}\right]. 
\end{aligned}
\end{equation}
%Now we successfully express the second term in the right hand side of Equation~\eqref{eqn:total_var1} as a sum of an expectation and a variance. Next, 
Repeating the above process, we can further rewrite the second term in the right hand side of Equation~\eqref{eqn:total_var4} as a summation of an expectation and a variance. Specifically, consider the right hand side of Equation~\eqref{eqn:total_var4}. By the linearity of expectation, we have: 
\begin{equation}\label{eqn:total_var5}
\begin{aligned}
&\hspace{-1mm}\Var\left[\E\left[\sum_{s\in V}\frac{d_t}{d_s}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\left(\sum_{i=0}^{1}\sum_{u\in V}\hspace{-1mm}\frac{1}{\alpha}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\vpi^{(i)}_u(s)\hspace{-0.5mm} \cdot \hspace{-0.5mm}\er^{(L-1)}_t\hspace{-0.5mm}(u)+\hspace{-1mm}\sum_{\ell=0}^{L-2}\hspace{-0.5mm}\er^{(\ell)}_t\hspace{-0.5mm}(s)\hspace{-0.5mm}\right)\Big|~\er^{(L-2)}_t\hspace{-0.5mm}\right]\hspace{-0.5mm}\right]\\
&\hspace{-2mm}=\hspace{-1mm}\Var\hspace{-0.5mm}\left[\hspace{-0.5mm}\sum_{s\in V}\hspace{-0.5mm}\frac{d_t}{d_s}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\left(\sum_{i=0}^{1}\hspace{-0.5mm}\sum_{u\in V}\hspace{-1.5mm}\frac{\vpi^{(i)}_u\hspace{-0.5mm}(s)}{\alpha}\hspace{-0.5mm} \cdot \hspace{-0.5mm}\E\left[\hspace{-0.5mm}\er^{(L-1)}_t\hspace{-0.5mm}(u)\Big|\er^{(L-2)}_t\hspace{-0.5mm}\right]\hspace{-1mm}+\hspace{-1.5mm}\sum_{\ell=0}^{L-2}\hspace{-0.5mm}\E\hspace{-0.5mm}\left[\hspace{-0.5mm}\er^{(\ell)}_t\hspace{-0.5mm}(s)\Big|\er^{(L-2)}_t\hspace{-0.5mm}\right]\hspace{-0.5mm}\right)\hspace{-0.5mm}\right]
\end{aligned}
\end{equation}
Analogously, we have: 
\begin{align*}
\sum_{\ell=0}^{L-2}\E\left[\er^{(\ell)}_t(s)\Big|~\er^{(L-2)}_t\right]=\sum_{\ell=0}^{L-2}\er^{(\ell)}_t(s), 
\end{align*}
and by Equation~\eqref{eqn:conditional_exp} and Equation~\eqref{eqn:def_lhopppr}:
\begin{align*}
\E\left[\er^{(L-1)}_t\hspace{-0.5mm}(u)~\big|~\er^{(L-2)}_t\hspace{-0.5mm}\right]\hspace{-1mm}=\hspace{-3mm}\sum_{w\in N(u)}\hspace{-4mm}\frac{(1-\alpha)}{d_w}\cdot \er^{(L-2)}_t\hspace{-0.5mm}(w)\hspace{-0.5mm}=\hspace{-4mm}\sum_{w\in N(u)}\hspace{-4mm}\frac{\vpi^{(1)}_w\hspace{-0.5mm}(u)}{\alpha}\cdot \er^{(L-2)}_t\hspace{-0.5mm}(w). 
\end{align*}
Plugging into Equation~\eqref{eqn:total_var5}, we can further derive: 
\begin{equation}\label{eqn:total_var6}
\begin{aligned}
&\Var\left[\E\left[\sum_{s\in V}\frac{d_t}{d_s}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\left(\sum_{i=0}^{1}\sum_{u\in V}\hspace{-1mm}\frac{1}{\alpha}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\vpi^{(i)}_u(s)\hspace{-0.5mm} \cdot \hspace{-0.5mm}\er^{(L-1)}_t\hspace{-0.5mm}(u)+\hspace{-1mm}\sum_{\ell=0}^{L-2}\hspace{-0.5mm}\er^{(\ell)}_t\hspace{-0.5mm}(s)\hspace{-0.5mm}\right)\Big|~\er^{(L-2)}_t\hspace{-0.5mm}\right]\hspace{-0.5mm}\right]\\
&=\hspace{-0.5mm}\Var\hspace{-0.5mm}\left[\hspace{-0.5mm}\sum_{s\in V}\hspace{-0.5mm}\frac{d_t}{d_s}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\left(\sum_{i=0}^{1}\sum_{u\in V}\hspace{-1mm}\sum_{w\in N(u)}\hspace{-3.5mm}\frac{1}{\alpha^2}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\vpi^{(i)}_u(s)\hspace{-0.5mm}\cdot \hspace{-0.5mm}\vpi^{(1)}_w\hspace{-0.5mm}(u) \hspace{-0.5mm}\cdot \hspace{-0.5mm}\er^{(L-2)}_t\hspace{-0.5mm}(w)\hspace{-1mm}+\hspace{-1.5mm}\sum_{\ell=0}^{L-2}\hspace{-0.5mm}\er^{(\ell)}_t\hspace{-0.5mm}(s)\hspace{-0.5mm}\right)\hspace{-0.5mm}\right]\hspace{-1mm}. 
\end{aligned}
\end{equation}
Note that by Equation~\eqref{eqn:PPR_recur} and Equation~\eqref{eqn:birectional_ppr}, we can derive the following fact: 
\begin{equation}\label{eqn:recur_PPR_reverse_mid}
\begin{aligned}
&\vpi^{(i+1)}_w(s)=\frac{d_s}{d_w}\cdot \vpi^{(i+1)}_s(w)=\frac{d_s}{d_w}\cdot \hspace{-2mm}\sum_{u\in N(w)}\hspace{-1mm}\frac{(1-\alpha)}{d_u}\vpi^{(i)}_s(u)\\
&=\sum_{u\in N(w)}\hspace{-1mm}\frac{(1-\alpha)}{d_w}\cdot \left(\frac{d_s}{d_u}\cdot \vpi^{(i)}_s(u)\right)=\sum_{u\in N(w)}\hspace{-1mm}\frac{(1-\alpha)}{d_w}\cdot \vpi^{(i)}_u(s). 
\end{aligned}    
\end{equation}
Meanwhile, Equation~\eqref{eqn:PPR_recur} also indicates the following properties of $\ell$-hop PPR: 
\begin{itemize}
\item $\vpi^{(0)}_w(w)=\alpha$; 
\item $\vpi^{(1)}_w(u)=\frac{(1-\alpha)}{d_w}\cdot \vpi^{(0)}_w(w)=\frac{\alpha\cdot (1-\alpha)}{d_w}$ for every $u \in N(w)$; 
\item $\vpi^{(1)}_w(u)=0$ for every $u \notin N(w)$. 
\end{itemize}
Therefore, the recursive relation shown in Equation~\eqref{eqn:recur_PPR_reverse_mid} can be further expressed as: 
\begin{align}\label{eqn:recur_PPR_reverse}
\vpi^{(i+1)}_w(s)=\sum_{u\in N(w)}\frac{1}{\alpha}\cdot \vpi^{(1)}_w(u)\cdot \vpi^{(i)}_u(s). 
\end{align}
Plugging Equation~\eqref{eqn:recur_PPR_reverse} into Equation~\eqref{eqn:total_var6}, we can derive: 
\begin{equation*}
\begin{aligned}
&\Var\left[\E\left[\sum_{s\in V}\frac{d_t}{d_s}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\left(\sum_{i=0}^{1}\sum_{u\in V}\hspace{-1mm}\frac{1}{\alpha}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\vpi^{(i)}_u(s)\hspace{-0.5mm} \cdot \hspace{-0.5mm}\er^{(L-1)}_t\hspace{-0.5mm}(u)+\hspace{-1mm}\sum_{\ell=0}^{L-2}\hspace{-0.5mm}\er^{(\ell)}_t\hspace{-0.5mm}(s)\hspace{-0.5mm}\right)\Big|~\er^{(L-2)}_t\hspace{-0.5mm}\right]\hspace{-0.5mm}\right]\\
&=\hspace{-0.5mm}\Var\hspace{-0.5mm}\left[\sum_{s\in V}\hspace{-0.5mm}\frac{d_t}{d_s}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\left(\sum_{i=0}^{1}\sum_{w\in V}\frac{1}{\alpha}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\vpi^{(i+1)}_w(s)\hspace{-0.5mm}\cdot \hspace{-0.5mm}\er^{(L-2)}_t\hspace{-0.5mm}(w)\hspace{-0.5mm}+\hspace{-0.5mm}\sum_{\ell=0}^{L-2}\hspace{-0.5mm}\er^{(\ell)}_t(s)\right)\right]. 
\end{aligned}
\end{equation*}
Since $\vpi^{(0)}_s(s)=1$ and $\vpi^{(0)}_w(s)=0$ for any $w\neq s$ as mentioned above, we can further derive: 
\begin{equation}\label{eqn:total_var7}
\begin{aligned}
&\Var\left[\E\left[\sum_{s\in V}\frac{d_t}{d_s}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\left(\sum_{i=0}^{1}\sum_{u\in V}\hspace{-1mm}\frac{1}{\alpha}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\vpi^{(i)}_u(s)\hspace{-0.5mm} \cdot \hspace{-0.5mm}\er^{(L-1)}_t\hspace{-0.5mm}(u)+\hspace{-1mm}\sum_{\ell=0}^{L-2}\hspace{-0.5mm}\er^{(\ell)}_t\hspace{-0.5mm}(s)\hspace{-0.5mm}\right)\Big|~\er^{(L-2)}_t\hspace{-0.5mm}\right]\hspace{-0.5mm}\right]\\
&=\hspace{-0.5mm}\Var\hspace{-0.5mm}\left[\sum_{s\in V}\hspace{-0.5mm}\frac{d_t}{d_s}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\left(\sum_{i=0}^{2}\sum_{w\in V}\frac{1}{\alpha}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\vpi^{(i)}_w(s)\hspace{-0.5mm}\cdot \hspace{-0.5mm}\er^{(L-2)}_t\hspace{-0.5mm}(w)\hspace{-0.5mm}+\hspace{-0.5mm}\sum_{\ell=0}^{L-3}\hspace{-0.5mm}\er^{(\ell)}_t(s)\right)\right]. 
\end{aligned}
\end{equation}
If we apply the law of total variance to Equation~\eqref{eqn:total_var7} one more times, we will have the sum of an expectation and a variance again. 
%Let us take a review to the processes for deriving Equation~\eqref{eqn:total_var_mid} and Equation~\eqref{eqn:total_var7}. %We note that by repeating the law of total variance once process of 
Repeatedly applying the law of total variance and rewriting the expression of variance, as a consequence, we can derive: 
\begin{equation}\label{eqn:total_var8}
\begin{aligned}
&\Var\left[\sum_{\ell=0}^L \sum_{s\in V}\frac{d_t}{d_s}\cdot \er^{(\ell)}_t(s)\right]\\
&\hspace{-2mm}=\hspace{-1.5mm}\sum_{\ell=1}^{L-1}\hspace{-0.5mm}\E\hspace{-0.5mm}\left[\hspace{-0.5mm}\Var\hspace{-0.5mm}\left[\sum_{s\in V}\hspace{-0.5mm}\frac{d_t}{d_s}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\left(\hspace{-0.5mm}\sum_{i=0}^{\ell}\hspace{-0.5mm}\sum_{w\in V}\hspace{-1mm}\frac{1}{\alpha}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\vpi^{(i)}_w(s)\hspace{-0.5mm}\cdot \hspace{-0.5mm}\er^{(L-\ell)}_t\hspace{-0.5mm}(w)\hspace{-0.5mm}+\hspace{-3mm}\sum_{j=0}^{L-\ell-1}\hspace{-2mm}\er^{(j)}_t(s)\hspace{-0.5mm}\right)\Big| \er^{(L-\ell-1)}_t\hspace{-0.5mm}\right]\hspace{-0.5mm}\right]\\
&+\Var\left[\E\left[\sum_{s\in V}\frac{d_t}{d_s}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\left(\sum_{i=0}^{L-1}\hspace{-0.5mm}\sum_{w\in V}\hspace{-0.5mm}\frac{1}{\alpha}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\vpi^{(i)}_w(s)\hspace{-0.5mm}\cdot \hspace{-0.5mm}\er^{(1)}_t\hspace{-0.5mm}(w)\hspace{-0.5mm}+\er^{(0)}_t(s)\hspace{-0.5mm}\right)~\Big|~ \er^{(0)}_t\right]\right]
\end{aligned}
\end{equation}
For the second term in the right side of Equation~\eqref{eqn:total_var8}, we have: 
\begin{equation}\label{eqn:total_var9}
\begin{aligned}
&\Var\left[\E\left[\sum_{s\in V}\frac{d_t}{d_s}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\left(\sum_{i=0}^{L-1}\hspace{-0.5mm}\sum_{w\in V}\hspace{-0.5mm}\frac{1}{\alpha}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\vpi^{(i)}_w(s)\hspace{-0.5mm}\cdot \hspace{-0.5mm}\er^{(1)}_t\hspace{-0.5mm}(w)\hspace{-0.5mm}+\er^{(0)}_t(s)\hspace{-0.5mm}\right)~\Big|~ \er^{(0)}_t\right]\right]\\
&\hspace{-0.5mm}=\hspace{-0.5mm}\Var\hspace{-0.5mm}\left[\sum_{s\in V}\frac{d_t}{d_s}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\left(\sum_{i=0}^{L-1}\hspace{-0.5mm}\sum_{w\in V}\hspace{-0.5mm}\frac{1}{\alpha}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\vpi^{(i)}_w(s)\hspace{-0.5mm}\cdot \hspace{-0.5mm}\E\left[\er^{(1)}_t\hspace{-0.5mm}(w)\Big|~ \er^{(0)}_t\hspace{-0.5mm}\right]\hspace{-1mm}+\hspace{-0.5mm}\E\left[\hspace{-0.5mm}\er^{(0)}_t(s)\Big|~ \er^{(0)}_t\right]\hspace{-0.5mm}\right)\right]
\end{aligned}
\end{equation}
by the linearity of expectation. In particular, we note that by Equation~\eqref{eqn:conditional_exp}, we can derive: 
\begin{align*}
\E\left[\er^{(1)}_t\hspace{-0.5mm}(w)\Big|~ \er^{(0)}_t\right]=\sum_{x\in N(w)}\frac{(1-\alpha)}{d_x}\cdot \er^{(0)}_t(x). 
\end{align*}
Therefore, Equation~\eqref{eqn:total_var9} actually bounds the variance of $\er^{(0)}_t$, and the randomness comes from the values of $\er^{(0)}_t$. However, according to Algorithm~\ref{alg:VBES}, $\er^{(0)}_t$ is deterministically set as $\er^{(0)}_t=\bm{e}_t$. As a consequence, we have: 
\begin{align*}
\Var\left[\E\left[\sum_{s\in V}\frac{d_t}{d_s}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\left(\sum_{i=0}^{L-1}\hspace{-0.5mm}\sum_{w\in V}\hspace{-0.5mm}\frac{1}{\alpha}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\vpi^{(i)}_w(s)\hspace{-0.5mm}\cdot \hspace{-0.5mm}\er^{(1)}_t\hspace{-0.5mm}(w)\hspace{-0.5mm}+\er^{(0)}_t(s)\hspace{-0.5mm}\right)~\Big|~ \er^{(0)}_t\right]\right]=0.  
\end{align*}
Plugging into Equation~\eqref{eqn:total_var8}, we can thus derive: 
\begin{equation*}
\begin{aligned}
&\Var\left[\sum_{\ell=0}^L \sum_{s\in V}\frac{d_t}{d_s}\cdot \r^{(\ell)}_t(s)\right]\\
&\hspace{-1mm}=\hspace{-1.5mm}\sum_{\ell=1}^{L-1}\hspace{-0.5mm}\E\hspace{-0.5mm}\left[\hspace{-0.5mm}\Var\hspace{-0.5mm}\left[\sum_{s\in V}\hspace{-0.5mm}\frac{d_t}{d_s}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\left(\hspace{-0.5mm}\sum_{i=0}^{\ell}\hspace{-0.5mm}\sum_{w\in V}\hspace{-1mm}\frac{1}{\alpha}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\vpi^{(i)}_w(s)\hspace{-0.5mm}\cdot \hspace{-0.5mm}\er^{(L-\ell)}_t\hspace{-0.5mm}(w)\hspace{-0.5mm}+\hspace{-3mm}\sum_{j=0}^{L-\ell-1}\hspace{-2mm}\er^{(j)}_t(s)\hspace{-0.5mm}\right)\Big| \er^{(L-\ell-1)}_t\hspace{-0.5mm}\right]\hspace{-0.5mm}\right]\\
&\hspace{-1mm}=\hspace{-1.5mm}\sum_{\ell=1}^{L-1}\hspace{-0.5mm}\E\hspace{-0.5mm}\left[\hspace{-0.5mm}\Var\hspace{-0.5mm}\left[\sum_{s\in V}\hspace{-0.5mm}\frac{d_t}{d_s}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\left(\hspace{-0.5mm}\sum_{i=0}^{\ell}\hspace{-0.5mm}\sum_{w\in V}\hspace{-1mm}\frac{1}{\alpha}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\vpi^{(i)}_w(s)\hspace{-0.5mm}\cdot \hspace{-0.5mm}\er^{(L-\ell)}_t\hspace{-0.5mm}(w)\hspace{-0.5mm}\right)\Big| \er^{(L-\ell-1)}_t\hspace{-0.5mm}\right]\hspace{-0.5mm}\right],  
\end{aligned}
\end{equation*}
%Furthermore, we note that
%the first term in the right side of Equation~\eqref{eqn:total_var8} is a conditional variance about the random variables $\er^{(L-\ell)}(w)$ and $\sum_{j=0}^{L-\ell-1}\er^{(j)}_t(s)$ given the fixed value of $\er^{(L-\ell-1)}_t$. Therefore, it measures the variance of $\er^{(L-\ell)}_t(w)$ computed in the $(L-\ell)$-th iteration of the \setpush algorithm. Note that 
%the \setpush algorithm, the value of $\er^{(L-\ell)}_t(w)$ is only determined by $\er^{(L-\ell)}_t$. Therefore, we have: 
%\begin{equation}\label{eqn:total_var10}
%\begin{aligned}
%&\sum_{\ell=1}^{L-1}\hspace{-0.5mm}\E\hspace{-0.5mm}\left[\hspace{-0.5mm}\Var\hspace{-0.5mm}\left[\sum_{s\in V}\hspace{-0.5mm}\frac{d_t}{d_s}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\left(\hspace{-0.5mm}\sum_{i=0}^{\ell}\hspace{-0.5mm}\sum_{w\in V}\hspace{-1mm}\frac{1}{\alpha}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\vpi^{(i)}_w(s)\hspace{-0.5mm}\cdot \hspace{-0.5mm}\er^{(L-\ell)}_t\hspace{-0.5mm}(w)\hspace{-0.5mm}+\hspace{-3mm}\sum_{j=0}^{L-\ell-1}\hspace{-2mm}\er^{(j)}_t(s)\hspace{-0.5mm}\right)\Big| \er^{(L-\ell-1)}_t\hspace{-0.5mm}\right]\hspace{-0.5mm}\right]\\
%&=\hspace{-1.5mm}\sum_{\ell=1}^{L-1}\hspace{-0.5mm}\E\hspace{-0.5mm}\left[\hspace{-0.5mm}\Var\hspace{-0.5mm}\left[\sum_{s\in V}\hspace{-0.5mm}\frac{d_t}{d_s}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\left(\hspace{-0.5mm}\sum_{i=0}^{\ell}\hspace{-0.5mm}\sum_{w\in V}\hspace{-1mm}\frac{1}{\alpha}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\vpi^{(i)}_w(s)\hspace{-0.5mm}\cdot \hspace{-0.5mm}\er^{(L-\ell)}_t\hspace{-0.5mm}(w)\hspace{-0.5mm}\right)\Big| \er^{(L-\ell-1)}_t\hspace{-0.5mm}\right]\hspace{-0.5mm}\right]\\
%&+\hspace{-1.5mm}\sum_{\ell=1}^{L-1}\hspace{-0.5mm}\E\hspace{-0.5mm}\left[\hspace{-0.5mm}\Var\hspace{-0.5mm}\left[\sum_{s\in V}\hspace{-0.5mm}\frac{d_t}{d_s}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\left(\sum_{j=0}^{L-\ell-1}\hspace{-2mm}\er^{(j)}_t(s)\hspace{-0.5mm}\right)\Big| \er^{(L-\ell-1)}_t\hspace{-0.5mm}\right]\hspace{-0.5mm}\right]. 
%\end{aligned}
%\end{equation}
%Moreover, we note that according to Algorithm~\ref{alg:VBES}, the value of $\er^{(j)}_t(s)$ is fixed after the $j$-th iteration ($j\in [0,L-\ell-1]$). Thus, we can derive: 
%\begin{align*}
%\hspace{-1.5mm}\sum_{\ell=1}^{L-1}\hspace{-0.5mm}\E\hspace{-0.5mm}\left[\hspace{-0.5mm}\Var\hspace{-0.5mm}\left[\sum_{s\in V}\hspace{-0.5mm}\frac{d_t}{d_s}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\left(\sum_{j=0}^{L-\ell-1}\hspace{-2mm}\er^{(j)}_t(s)\hspace{-0.5mm}\right)\Big| \er^{(L-\ell-1)}_t\hspace{-0.5mm}\right]\hspace{-0.5mm}\right]=0. 
%\end{align*}
%Consequently, we have: 
%\begin{equation*}
%\begin{aligned}
%&\Var\left[\sum_{\ell=0}^L \sum_{s\in V}\frac{d_t}{d_s}\cdot \er^{(\ell)}_t(s)\right]\\
%&=\sum_{\ell=1}^{L-1}\E\left[\hspace{-0.5mm}\Var\hspace{-0.5mm}\left[\sum_{s\in V}\hspace{-0.5mm}\frac{d_t}{d_s}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\left(\sum_{i=0}^{\ell}\hspace{-0.5mm}\sum_{w\in V}\hspace{-0.5mm}\frac{1}{\alpha}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\vpi^{(i)}_w(s)\hspace{-0.5mm}\cdot \hspace{-0.5mm}\er^{(L-\ell)}_t(w)\right)\Big| \er^{(L-\ell-1)}_t\right]\right]\\
%&=\sum_{\ell=0}^{L-2}\E\left[\hspace{-0.5mm}\Var\hspace{-0.5mm}\left[\sum_{s\in V}\hspace{-0.5mm}\frac{d_t}{d_s}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\left(\sum_{i=0}^{L-\ell-1}\hspace{-0.5mm}\sum_{w\in V}\hspace{-0.5mm}\frac{1}{\alpha}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\vpi^{(i)}_w(s)\cdot \er^{(\ell+1)}_t(w)\hspace{-0.5mm}\right)\Big| \er^{(\ell)}_t\right]\right], 
%\end{aligned}
%\end{equation*}
which follows the lemma. 




%\begin{proof}[Proof of Lemma~\ref{lem:var_partial}]
\subsection{\rev Proof of Lemma~\ref{lem:var_partial}}
Recall that in the $(\ell+1)$-th iteration of Algorithm~\ref{alg:VBES}, we set $\er^{(\ell+1)}(v)=\sum_{u\in N(v)}\incre^{(\ell+1)}(u,v)$. And the value of $\incre^{(\ell+1)}(u,v)$ is independent from other $\incre^{(\ell+1)}(x,y)$ for $\forall (x, y) \in E$ based on the derived residue vectors $\er^{(j)}_t$ for $j\in \{0, 1, \ldots, \ell\}$. Therefore, for each $v\in V$, $\er^{(\ell+1)}_t(v)$ is independent with each other, which follows: 
\begin{equation}\label{eqn:exp_sum}
\begin{aligned}
&\sum_{\ell=0}^{L-2}\E\left[\Var\left[\left.\sum_{v\in V}\left(\sum_{s\in V}\frac{d_t}{d_s}\cdot \hspace{-1mm}\sum_{i=0}^{L-\ell-1}\frac{\vpi^{(i)}_v(s)}{\alpha}\right)\cdot \er^{(\ell+1)}_t(v)~\right|\er^{(\ell)}_t\right]\right]\\
&=\sum_{\ell=0}^{L-2}\E\left[\sum_{v\in V}\Var\left[\left.\left(\sum_{s\in V}\frac{d_t}{d_s}\cdot \hspace{-1mm}\sum_{i=0}^{L-\ell-1}\frac{\vpi^{(i)}_v(s)}{\alpha}\right)\cdot \er^{(\ell+1)}_t(v)~\right|\er^{(\ell)}_t\right]\right]. 
\end{aligned}    
\end{equation}
Note that $\sum_{s\in V}\frac{d_t}{d_s}\cdot \hspace{-1mm}\sum_{i=0}^{L-\ell-1}\frac{\vpi^{(i)}_v(s)}{\alpha}$ is a parameter, instead of a random variable, which can be bounded as: 
\begin{align*}
\sum_{s\in V}\frac{d_t}{d_s}\cdot \hspace{-2mm}\sum_{i=0}^{L-\ell-1}\hspace{-1mm}\frac{\vpi^{(i)}_v(s)}{\alpha}\le \frac{d_t}{\alpha}\cdot \hspace{-1mm}\sum_{s\in V}\sum_{i=0}^{L-\ell-1}\hspace{-1mm}\vpi^{(i)}_v(s) \le \frac{d_t}{\alpha}\cdot \hspace{-1mm} \sum_{s\in V}\vpi_v(s)=\frac{d_t}{\alpha}. 
\end{align*}
Therefore, by plugging the upper bound of $\sum_{s\in V}\frac{d_t}{d_s}\cdot \sum_{i=0}^{L-\ell-1}\frac{\vpi^{(i)}_v(s)}{\alpha}$ into Equation~\eqref{eqn:exp_sum}, we can further derive: 
\begin{equation*}
\begin{aligned}
&\sum_{\ell=0}^{L-2}\E\left[\sum_{v\in V}\Var\left[\left.\left(\sum_{s\in V}\frac{d_t}{d_s}\cdot \hspace{-1mm}\sum_{i=0}^{L-\ell-1}\frac{\vpi^{(i)}_v(s)}{\alpha}\right)\cdot \er^{(\ell+1)}_t(v)~\right|\er^{(\ell)}_t\right]\right]\\
&\le \hspace{-1mm}\frac{d_t}{\alpha}\cdot \hspace{-1mm}\sum_{\ell=0}^{L-2}\E\left[\sum_{v\in V}\left(\sum_{s\in V}\frac{d_t}{d_s}\cdot \hspace{-1mm}\sum_{i=0}^{L-\ell-1}\frac{\vpi^{(i)}_v(s)}{\alpha}\right)\cdot \Var\left[\left. \er^{(\ell+1)}_t(v)~\right|\er^{(\ell)}_t\right]\right]. 
\end{aligned}    
\end{equation*}
By Lemma~\ref{lem:conditional_variance}, we have: 
%\begin{align*}
$\Var\left[\left. \er^{(\ell+1)}_t(v)~\right|\er^{(\ell)}_t\right]\hspace{-1mm}\le \hspace{-0.5mm}\sum_{u\in N(v)}\hspace{-0.5mm}\frac{\th \cdot (1-\alpha)\cdot \er^{(\ell)}_t(u)}{d_u}$, 
%\end{align*}
and thus, by Lemma~\ref{lem:unbiasedness_er} and Equation~\eqref{eqn:PPR_recur}, it follows: 
\begin{equation*}
\begin{aligned}
&\E\left[\Var\left[\left. \er^{(\ell+1)}_t(v)~\right|\er^{(\ell)}_t\right]\right]\le \sum_{u\in N(v)}\frac{\th \cdot (1-\alpha)}{d_u}\cdot \E\left[\er^{(\ell)}_t(u)\right]\\
&=\sum_{u\in N(v)} \frac{\th \cdot (1-\alpha)}{\alpha \cdot d_u}\cdot \vpi^{(\ell)}_t(u)=\frac{\th}{\alpha}\cdot \vpi^{(\ell+1)}_t(v). 
\end{aligned}    
\end{equation*}
Plugging the above inequality into Equation~\eqref{eqn:exp_sum}, we have: 
\begin{equation*}
\begin{aligned}
&\sum_{\ell=0}^{L-2}\E\left[\sum_{v\in V}\Var\left[\left.\left(\sum_{s\in V}\frac{d_t}{d_s}\cdot \hspace{-1mm}\sum_{i=0}^{L-\ell-1}\frac{\vpi^{(i)}_v(s)}{\alpha}\right)\cdot \er^{(\ell+1)}_t(v)~\right|\er^{(\ell)}_t\right]\right]\\
& \le \frac{d_t \cdot \th}{\alpha^2}\cdot \sum_{\ell=0}^{L-2} \sum_{s\in V}\frac{d_t}{d_s}\cdot \sum_{i=0}^{L-\ell-1}\sum_{v\in V} \frac{\vpi^{(i)}_v(s)}{\alpha} \cdot \vpi^{(\ell+1)}_t(v). 
%=\frac{d_t\cdot \th}{\alpha^2}\cdot \sum_{\ell=0}^{L-2} \sum_{s\in V}\frac{d_t}{d_s}\cdot \hspace{-2mm}\sum_{i=0}^{L-\ell-1}\vpi^{(\ell+i+1)}_t(s). 
\end{aligned}    
\end{equation*}
We note $\sum_{v\in V}\frac{1}{\alpha}\cdot \vpi^{(i)}_v(s)\cdot \vpi^{(\ell+1)}_t(v)=\vpi^{(\ell+1+i)}_t(s)$ and 
\begin{align*}
\sum_{s\in V}\frac{d_t}{d_s}\cdot \sum_{i=0}^{L-\ell-1}\vpi^{(\ell+1+i)}_t(s)\le \sum_{s\in V}\frac{d_t}{d_s}\cdot \vpi_t(s)=\sum_{s\in V} \vpi_s(t)=n\vpi(t). 
\end{align*}
As a consequence, we can further derive: 
\begin{equation*}
\begin{aligned}
&\sum_{\ell=0}^{L-2}\E\left[\Var\left[\left.\sum_{v\in V}\left(\sum_{s\in V}\frac{d_t}{d_s}\cdot \hspace{-1mm}\sum_{i=0}^{L-\ell-1}\frac{\vpi^{(i)}_v(s)}{\alpha}\right)\cdot \er^{(\ell+1)}_t(v)~\right|\er^{(\ell)}_t\right]\right]\\
&\le \frac{d_t \cdot \th}{\alpha^2} \cdot \sum_{\ell=0}^{L-2} n\pi(t) \le \frac{1}{\alpha^2} \cdot L \th d_t \cdot n \vpi(t), 
\end{aligned}   
\end{equation*}
following the lemma. 
%\end{proof}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "paper"
%%% End:
