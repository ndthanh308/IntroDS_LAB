\section{Theoretical Analysis}
\label{sec:analysis}
In this section, we analyze the theoretical properties of the \setpush algorithm. First, we formally prove that the $\ell$-hop residue $\r^{(\ell)}_t(u)$ is an unbiased estimator of $\frac{1}{\alpha}\cdot \vpi^{(\ell)}_t(u)$, and the output $\epi(t)$ returned by Algorithm~\ref{alg:VBES} is an unbiased estimator of the truncated PageRank $\bpi(t)$. Then Section~\ref{subsec:variance} bounds the variance of $\epi(t)$. Finally, Section~\ref{subsec:totalcost} presents the expected time cost of the \setpush algorithm. 


\subsection{Correctness}\label{subsec:correctness}
Recall that we have presented some intuitions on $\E\left[\r^{(\ell)}_t(u)\right]=\frac{1}{\alpha}\cdot \vpi^{(\ell)}_t(u)$ and $\E\left[\epi(t)\right]=\bpi(t)$ in Section~\ref{subsec:highlevelidea}. The following Lemmas further provide formal proofs on these intuitions. {\rev Due to the page limit, we defer the proofs of Lemma~\ref{lem:unbiasedness_er} and Lemma~\ref{lem:unbiasedness_ppr} to our Technical Report~\cite{TechnicalReport}. }

\begin{lemma}\label{lem:unbiasedness_er}
For each $\ell\in \{0,1,\ldots, L\}$ The residue vector $\er^{(\ell)}_t$ obtained in Algorithm~\ref{alg:VBES} is an unbiased estimator of $\frac{1}{\alpha}\cdot \vpi^{(\ell)}_t$, such that for each $v\in V$,   
\begin{align*}
\E \left[\er^{(\ell)}_t(v)\right]=\frac{1}{\alpha}\cdot \vpi^{(\ell)}_t(v).     
\end{align*}
\end{lemma}


Based on Lemma~\ref{lem:unbiasedness_er}, we are able to prove that Algorithm~\ref{alg:VBES} returns an unbiased estimator of the truncated PageRank $\bpi(t)$. 

\begin{lemma}\label{lem:unbiasedness_ppr}
%For each node $s\in V$, Algorithm~\ref{alg:VBES} computes an estimator $\epi_t(s)$ such that $\E[\epi_t(s)]=\bpi_t(s)$. 
Algorithm~\ref{alg:VBES} returns an unbiased estimator $\epi(t)$ of the truncated PageRank score of node $t$. Specifically, $\E[\epi(t)]=\bpi(t)$. 
\end{lemma}

%\begin{theorem}[Unbiasedness]\label{thm:unbiasedness}
%Algorithm~\ref{alg:VBES} returns an unbiased estimator $\epi(t)$ for the PageRank of node $t$ that $\E[\epi(t)]=\vpi(t)$. 
%\end{theorem}
Up to now, we have proved that $\epi(t)$ is an unbiased estimator of the truncated PageRank $\vpi(t)$. Next, we shall bound the variance of $\epi(t)$  and utilize the following Chebyshev Inequality~\cite{mitzenmacher2017probability} to bound the failure probability for deriving a $(\frac{c}{2}, p_f)$-approximation of $\bpi(t)$. 

\begin{fact}[Chebyshev's Inequality~\cite{mitzenmacher2017probability}]\label{fact:chebyshev}
Let $X$ denote a random variable. For any real number $\e>0$, $\Pr\left\{\left|X-\E[X]\right|\ge \e\right\}\le \frac{\Var[X]}{\e^2}$. 
\end{fact}



\subsection{Variance Analysis}\label{subsec:variance}
We claim that the variance of $\epi(t)$ can be bounded by $\frac{L\theta d_t}{n}\cdot \vpi(t)$, which is formally demonstrated in Theorem~\ref{thm:variance}. 

\begin{theorem}[Variance]\label{thm:variance}
The variance of the estimator $\epi(t)$ returned by Algorithm~\ref{alg:VBES} can be bounded as $\Var[\epi(t)]\le \frac{L \theta d_t }{n}\cdot \vpi(t)$. 
\end{theorem}

To prove Theorem~\ref{thm:variance}, we need several technical lemmas. Specifically, in Lemma~\ref{lem:conditional_variance}, we bound the variance of $\er^{(\ell+1)}_t(v)$ conditioned on $\er^{(\ell)}_t$ that is derived in the $\ell$-th iteration. 

\begin{lemma}\label{lem:conditional_variance} 
For each node $v\in V$ and each $\ell \in \{0, 1, \ldots, L-1\}$, the variance of $\er^{(\ell+1)}_t(v)$ can be bounded as
\begin{align*}
\Var\left[\er^{(\ell+1)}_t(v)~\big|~ \er^{(\ell)}_t\right]\le \sum_{u\in N(v)}\th \cdot \frac{(1-\alpha)\cdot \er^{(\ell)}_t(u)}{d_u}, 
\end{align*}
where $\Var\left[\er^{(\ell+1)}_t(v)~\big|~ \er^{(\ell)}_t\right]$ denotes the variance of $\er^{(\ell+1)}_t(v)$ conditioned on the value of $\er^{(\ell)}_t$ that has been derived in the $\ell$-th iteration. 
\end{lemma}

In the second step, we prove: 
\begin{lemma}\label{lem:var_recur}
The variance of the estimator $\epi(t)$ obtained by Algorithm~\ref{alg:VBES} can be computed as: 
\begin{equation*}
\begin{aligned}
&\Var\left[\epi(t)\right]=\frac{\alpha^2}{n^2}\cdot \Var\left[\sum_{\ell=0}^L \sum_{s\in V}\frac{d_t}{d_s}\cdot \er^{(\ell)}_t(s)\right]\\
&=\hspace{-0.5mm}\frac{\alpha^2}{n^2}\hspace{-0.5mm}\cdot \sum_{\ell=0}^{L-2}\hspace{-0.5mm}\E\left[\Var\hspace{-0.5mm}\left[\left.\sum_{v\in V}\left(\sum_{s\in V}\frac{d_t}{d_s}\cdot \hspace{-1mm}\sum_{i=0}^{L-\ell-1}\hspace{-0.5mm}\frac{\vpi^{(i)}_v\hspace{-0.5mm}(s)}{\alpha}\right)\cdot \hspace{-0.5mm}\er^{(\ell+1)}_t(v)~\right|~\er^{(\ell)}_t\right]\right]. 
\end{aligned}    
\end{equation*}
\end{lemma}
To prove Lemma~\ref{lem:var_recur}, recall that $\epi(t)=\frac{1}{n}\sum_{s\in V}\frac{d_t}{d_s}\cdot \epi_t(s)$, and $\epi_t(s)=\sum_{\ell=0}^L \alpha \er^{(\ell)}_t(s)$ according to Algorithm~\ref{alg:VBES}. Thus, the variance of $\epi(t)$ derived by Algorithm~\ref{alg:VBES} can be computed as: 
\begin{align*}
\Var\hspace{-0.5mm}\left[\epi(t)\right]\hspace{-0.5mm}=\hspace{-0.5mm}\Var\left[\hspace{-0.5mm}\frac{1}{n}\hspace{-0.5mm}\cdot \hspace{-1mm}\sum_{s\in V}\hspace{-0.5mm}\frac{d_t}{d_s}\hspace{-0.5mm}\cdot \hspace{-1mm}\sum_{\ell=0}^{L}\hspace{-0.5mm}\alpha \hspace{-0.5mm}\cdot \hspace{-0.5mm}\er_t^{(\ell)}\hspace{-0.5mm}(s)\hspace{-0.5mm}\right]\hspace{-1mm}=\hspace{-0.5mm}\frac{\alpha^2}{n^2}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\Var\left[\sum_{s\in V}\hspace{-1mm}\frac{d_t}{d_s}\hspace{-1mm}\cdot \hspace{-1mm}\sum_{\ell=0}^{L}\hspace{-0.5mm}\er_t^{(\ell)}\hspace{-0.5mm}(s)\hspace{-0.5mm}\right], 
\end{align*}
%following the first equality in Lemma~\ref{lem:var_recur}. 
For the second equality in Lemma~\ref{lem:var_recur},  
%the detailed proof is rather technical, and we defer it to the Technical Report~\cite{TechnicalReport} for readability. Intuitively, 
we can prove it by repeatedly applying the law of total variance. Details of the law of total variance are given as below. 

\begin{fact}[Law of Total Variance~\cite{weiss2005TotalVarianceLaw}]\label{fact:totalvar}
For two random variables $X$ and $Y$, the law of total variance states: 
\begin{align*}
\Var\left[Y\right]=\E\left[\Var \left[Y\mid X\right]\right]+\Var\left[\E\left[Y\mid X\right]\right]
\end{align*}
holds if the two variables $X$ and $Y$ are on the same probability space and the variance of $Y$ is finite. 
\end{fact}

Furthermore, we plug the variance bound derived in Lemma~\ref{lem:conditional_variance} into Lemma~\ref{lem:var_recur}, then Lemma~\ref{lem:var_partial} follows. 

\begin{lemma}\label{lem:var_partial}
For all $\ell\in [0, L]$, the residue vectors $\er^{(\ell)}$ obtained by Algorithm~\ref{alg:VBES} in the $\ell$-th iterations satisfy: 
\begin{equation*}
\begin{aligned}
\sum_{\ell=0}^{L-2}\hspace{-0.5mm}\E\hspace{-0.5mm}\left[\hspace{-0.5mm}\Var\hspace{-0.5mm}\left[\hspace{-0.5mm}\left.\sum_{v\in V}\hspace{-0.5mm}\left(\sum_{s\in V}\hspace{-0.8mm}\frac{d_t}{d_s}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\hspace{-0.5mm}\sum_{i=0}^{L-\ell-1}\hspace{-1.5mm}\frac{\vpi^{(i)}_v\hspace{-0.5mm}(s)}{\alpha}\hspace{-0.5mm}\right)\hspace{-0.5mm}\cdot \hspace{-0.5mm}\er^{(\ell+1)}_t\hspace{-0.5mm}(v)~\right|\er^{(\ell)}_t\hspace{-0.5mm}\right]\right]\hspace{-1mm}\le \hspace{-0.5mm}\frac{ L\th d_t \hspace{-0.5mm}\cdot \hspace{-0.5mm} n\vpi(t)}{\alpha^2}. 
\end{aligned}    
\end{equation*}
\end{lemma}


Finally, by putting Lemma~\ref{lem:var_recur} and ~\ref{lem:var_partial} together, we can conclude that  
%\begin{align*}
$\Var\left[\epi(t)\right]\le \frac{L\cdot \th \cdot d_t}{n^2}\cdot n\epi(t)$, 
%\end{align*}
and thus Theorem~\ref{thm:variance} follows. {\rev Detailed proofs of the above theorem and lemmas can be found in the Technical Report~\cite{TechnicalReport} due to the page limit of the main text. }


\subsection{Time Cost}\label{subsec:totalcost}
In the following, we analyze the expected time cost of the \setpush algorithm. Moreover, Theorem~\ref{thm:finalcost_analysis} provides the theoretical guarantees of the \setpush algorithm for achieving a $(\rela, \pf)$-approximation of the single-node PageRank. 

\begin{lemma}\label{lem:cost_theta}
The expected time cost of Algorithm~\ref{alg:VBES} can be bounded by ${\rev \frac{1}{\alpha \theta}=}~O\left(\frac{1}{\th}\right)$. 
\end{lemma} 

\begin{proof}
Let $Cost^{(\ell+1)}(u,v)$ denote the time cost for pushing the probability mass from node $u$ to node $v$ in the $(\ell+1)$-th iteration of Algorithm~\ref{alg:VBES}. Thus, $Cost^{(\ell+1)}(u,v)=1$ holds deterministically if $\frac{(1-\alpha)}{d_u}\cdot \er^{(\ell)}_t(u)\ge \th$. On the other hand, if $\frac{(1-\alpha)}{d_u}\cdot \er^{(\ell)}_t(u)< \th$, $Cost^{(\ell+1)}(u,v)=1$ (i.e., pushing probability mass from node $u$ to $v$) with probability $\frac{(1-\alpha)}{d_u\cdot \th}\cdot \er^{(\ell)}_t(u)$, and $Cost^{(\ell+1)}(u,v)=0$ with probability $1-\frac{(1-\alpha)}{d_u\cdot \th}\cdot \er^{(\ell)}_t(u)$. Thus, if $\frac{(1-\alpha)}{d_u}\cdot \er^{(\ell)}_t(u)< \th$, 
\begin{align*}
\E\left[Cost^{(\ell+1)}(u,v)~\big|~ \er^{(\ell)}_t\right]=1\cdot \frac{(1-\alpha)}{d_u\cdot \th}\cdot \er^{(\ell)}_t(u). 
\end{align*}
Let $Cost^{(\ell+1)}$ denote the time cost of the $(\ell+1)$-th iteration in Algorithm~\ref{alg:VBES}. Since $Cost^{(\ell+1)}\hspace{-0.5mm}=\hspace{-0.5mm}\sum_{(u,v)\in E} Cost^{(\ell+1)}(u,v)$, we can further derive: 
\begin{equation*}
\begin{aligned}
\E\hspace{-0.5mm}\left[Cost^{(\ell+1)}~\big|~ \er^{(\ell)}_t\hspace{-0.5mm}\right]\hspace{-0.5mm}=\hspace{-3mm}\sum_{(u,v)\in E}\hspace{-2.5mm}\E\left[Cost^{(\ell+1)}(u,v)~\big|~ \er^{(\ell)}_t\hspace{-0.5mm}\right]\hspace{-0.5mm}=\hspace{-3mm}\sum_{(u,v)\in E}\hspace{-2mm}\frac{(1\hspace{-0.5mm}-\hspace{-0.5mm}\alpha)}{d_u \hspace{-0.5mm}\cdot \hspace{-0.5mm}\th}\hspace{-0.5mm}\cdot \er^{(\ell)}_t\hspace{-0.5mm}(u). 
%1\cdot \frac{(1-\alpha)}{d_u\cdot \th}\cdot \er^{(\ell)}_t(u). 
\end{aligned}    
\end{equation*}
It follows: 
\begin{equation*}
\begin{aligned}
&\E\left[Cost^{(\ell+1)}\right]=\E\left[\E\left[Cost^{(\ell+1)}~\big|~ \er^{(\ell)}_t\hspace{-0.5mm}\right]\right]=\hspace{-2mm}\sum_{(u,v)\in E}\hspace{-1mm}\frac{(1\hspace{-0.5mm}-\hspace{-0.5mm}\alpha)}{d_u \hspace{-0.5mm}\cdot \hspace{-0.5mm}\th}\hspace{-0.5mm}\cdot \E\left[\er^{(\ell)}_t\hspace{-0.5mm}(u)\right]\\
&=\frac{1}{\alpha\th}\cdot \sum_{v\in V}\sum_{u\in N(v)}\hspace{-2mm}\frac{(1\hspace{-0.5mm}-\hspace{-0.5mm}\alpha)}{d_u}\hspace{-0.5mm}\cdot \vpi^{(\ell)}_t\hspace{-0.5mm}(u)=\frac{1}{\alpha\th}\cdot \sum_{v\in V}\vpi^{(\ell+1)}_t(v),  
\end{aligned}
\end{equation*}
where we apply Lemma~\ref{lem:unbiasedness_er} in the third equality, and Equation~\eqref{eqn:PPR_recur} in the last equality. Furthermore, let $Cost=\sum_{\ell=0}^{L-1}Cost^{(\ell+1)}$ denote the total time cost of Algorithm~\ref{alg:VBES}. Thus, we can derive: 
\begin{align*}
\E\left[Cost\right]=\hspace{-0.5mm}\sum_{\ell=0}^{L-1}\E\left[Cost^{(\ell+1)}\right]\hspace{-0.5mm}=\hspace{-0.5mm}\frac{1}{\alpha\th}\cdot \hspace{-1mm}\sum_{v\in V}\sum_{\ell=0}^{L-1}\vpi^{(\ell+1)}_t(v)\hspace{-0.5mm}\le \hspace{-0.5mm}\frac{1}{\alpha\th}=O\left(\frac{1}{\th}\right),  
\end{align*}
by applying $\sum_{\ell=0}^{L-1}\vpi^{(\ell+1)}_t(v)\le \vpi_t(v)$, and $\sum_{v\in V}\vpi^{(\ell+1)}_t(v)$. Therefore, the lemma follows. 
\end{proof}

At the end, we employ the bound of variance $\Var\left[\epi(t)\right]$ derived in Theorem~\ref{thm:variance} to the Chebyshev's Inequality given in Fact~\ref{fact:chebyshev}, and thus obtain the required setting of threshold $\theta$. As shown in Theorem~\ref{thm:finalcost_analysis}, by setting $\theta$ roughly proportional to the value of $\max\left\{\frac{1}{d_t}, \frac{1}{\sqrt{m}}\right\}$, the \setpush algorithm achieves a $(\rela, \pf)$-approximation of $\vpi(t)$. Plugging the setting of $\theta$ to the results given by Lemma~\ref{lem:cost_theta}, the expected time cost can be further bounded by $\tilde{O}\left(\min\left\{d_t,\sqrt{m}\right\}\right)$.  


% Figure environment removed


% Figure environment removed



\begin{theorem}\label{thm:finalcost_analysis}
By setting $\theta=\max\left\{\frac{\alpha c^2}{12L\cdot d_t},\frac{\alpha c^2}{12L}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\sqrt{\frac{2(1-\alpha)}{m}}\right\}$, Algorithm~\ref{alg:VBES} returns an estimator $\epi(t)$ of $\vpi(t)$, such that $|\vpi(t)-\epi(t)|\le c\cdot \vpi(t)$ holds with constant probability. The expected time cost of Algorithm~\ref{alg:VBES} is bounded by 
\begin{align*}
{\rev 
\frac{12 \cdot \left(\log_{1-\alpha}\frac{c\alpha}{2n}\right)}{\alpha^2 c^2}\cdot \min \left\{d_t, \sqrt{\frac{m}{2(1-\alpha)}}\right\}
}
~=\tilde{O}\left(\min\left\{d_t,\sqrt{m}\right\}\right). 
\end{align*} 
%\begin{enumerate}
%\item If we set $\theta=\tilde{O}\left(\frac{1}{\sqrt{m}}\right)$, then the expected time cost of Algorithm~\ref{alg:VBES} is bounded by $\tilde{O}\left(\sqrt{m}\right)$.  
%\item If we set $\theta=\tilde{O}\left(\frac{1}{\sqrt{m}}\right)$, then the expected time cost of Algorithm~\ref{alg:VBES} is bounded by $\tilde{O}\left(\sqrt{m}\right)$. 
%\end{enumerate}
%\vspace{-1mm}
\end{theorem}

\begin{proof} %To prove Theorem~\ref{thm:finalcost_analysis}, we utilize the Chebyshev's Inequality as shown below to bound the value of $\th$. 
Recall that the variance of $\epi(t)$ obtained by Algorithm~\ref{alg:VBES} is bounded by $\frac{L\cdot \th \cdot d_t}{n^2}\cdot n\epi(t)$ given in Theorem~\ref{thm:variance}. By the Chebyshev's Inequality, we have: 
\begin{align*}
\Pr\left\{\epi(t)-\bpi(t)\ge \frac{c}{2} \cdot \vpi(t)\right\} \le \frac{4 \cdot \Var\left[\epi(t)\right]}{c^2 \cdot \left(\vpi(t)\right)^2}\le \frac{4L\th d_t}{c^2 \cdot n\vpi(t)}. 
\end{align*}
Thus, by setting $\th= \frac{c^2 \cdot n\vpi(t)}{12Ld_t}$, $\epi(t)-\bpi(t)\le \frac{c}{2} \cdot \vpi(t)$ holds with probability at least $\frac{1}{3}$. In particular, we note $\frac{c^2 \cdot n\vpi(t)}{12Ld_t}\ge \frac{\alpha c^2}{12Ld_t}$ based on the fact that $\vpi(t)\ge \frac{\alpha}{n}$ as illustrated in Equation~\eqref{eqn:ite_pagerank}. If we set $\theta=\frac{\alpha c^2}{12Ld_t}$, then according to Lemma~\ref{lem:cost_theta}, the expected time cost of Algorithm~\ref{alg:VBES} can be bounded by %$O\left(\frac{1}{\th}\right)=O\left(\frac{12Ld_t}{\alpha c^2}\right)=\tilde{O}\left(d_t\right)$ with $L=\log_{1-\alpha}{\frac{c\alpha}{2n}}$ and the constants $\alpha$ and $c$. 
{\rev 
$\frac{1}{\alpha \theta}=\frac{12L d_t}{\alpha^2 c^2}=\tilde{O}\left(d_t\right)$ because $L=\log_{1-\alpha}{\frac{c\alpha}{2n}}=\tilde{O}(1)$ (see Section~\ref{subsec:trunc_pagerank} for the details of setting $L$) and $\alpha, c$ are constants. 
}
Moreover, as we shall prove below, $\frac{n \vpi(t)}{d_t}\hspace{-0.5mm}\ge \hspace{-0.5mm}\alpha  \hspace{-0.5mm} \cdot  \hspace{-0.5mm}\sqrt{\frac{2(1-\alpha)}{m}}$ holds for any $t\in V$. Thus, by setting $\th \hspace{-0.5mm}= \hspace{-0.5mm}\frac{\alpha c^2}{12L}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\sqrt{\frac{2(1-\alpha)}{m}}$, the expected time cost of Algorithm~\ref{alg:VBES} is bounded by 
%$O\left(\hspace{-0.5mm}\frac{1}{\th}\hspace{-0.5mm}\right)\hspace{-0.5mm}=\hspace{-0.5mm}O\left(\frac{12L}{\alpha c^2}\hspace{-0.5mm}\cdot \hspace{-1mm}\sqrt{\frac{m}{2(1-\alpha)}}\right)=\tilde{O}\left(\hspace{-0.5mm}\sqrt{m}\right)$. 
{\rev 
$\frac{1}{\alpha \theta}=\frac{12L}{\alpha^2 c^2}\hspace{-0.5mm}\cdot \hspace{-1mm}\sqrt{\frac{m}{2(1-\alpha)}}=\tilde{O}(\sqrt{m})$, where, analogously, $L=\log_{1-\alpha}{\frac{c\alpha}{2n}}=\tilde{O}(1)$ and $\alpha, c$ are constants. 
}

Now we present the proof of $\frac{n \vpi(t)}{d_t}\hspace{-0.5mm}\ge \hspace{-0.5mm}\alpha \cdot \sqrt{\frac{2(1-\alpha)}{m}}$. By Equation~\eqref{eqn:ite_pagerank}, we have: 
\begin{equation}\label{eqn:sqrtm}
\begin{aligned}
\vpi(t)\ge (1-\alpha)\hspace{-2mm}\sum_{u\in N(t)}\hspace{-1mm}\frac{\vpi(u)}{d_u}+\frac{\alpha}{n} \ge (1-\alpha)\hspace{-1mm}\sum_{u\in N(t)}\frac{1}{d_u}\cdot \frac{\alpha}{n}+\frac{\alpha}{n}. %=\frac{\alpha}{n} \cdot \left(\sum_{u\in N(t)}\frac{(1-\alpha)}{d_u}\right). 
\end{aligned}    
\end{equation}
We note $\sum_{u\in N(t)}\hspace{-1mm}\frac{1}{d_u}\hspace{-0.5mm}\ge \hspace{-0.5mm}\frac{d_t^2}{2m}$ since
%\begin{align*}
$\left(\sum_{u\in N(t)}\frac{1}{d_u}\right)\cdot \left(\sum_{u\in N(t)}d_u\right)\ge \left(\sum_{u\in N(t)}1\right)^2=d_t^2$ 
%\end{align*}
holds by the Cauchy-Schwarz Inequality~\cite{steele2004cauchy}. Plugging into Inequality~\eqref{eqn:sqrtm}, we can further derive: 
\begin{align*}
\vpi(t)\hspace{-0.5mm}\ge \hspace{-0.5mm}\frac{\alpha}{n}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\left(\frac{(1-\alpha)d_t^2}{2m}\hspace{-0.5mm}+\hspace{-0.5mm}1\right)\hspace{-0.5mm}=\hspace{-0.5mm}\frac{\alpha d_t}{n}\hspace{-0.5mm}\cdot\hspace{-0.5mm} \left(\frac{(1-\alpha)d_t\hspace{-0.5mm}+\hspace{-0.5mm}\frac{2m}{d_t}}{2m}\right)\hspace{-0.5mm}\ge \hspace{-0.5mm}\frac{\alpha d_t}{n}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\sqrt{\frac{2(1\hspace{-0.5mm}-\hspace{-0.5mm}\alpha)}{m}}, 
\end{align*}
where we apply the fact that $(1-\alpha)d_t+\frac{2m}{d_t} \ge 2\cdot \sqrt{(1-\alpha)2m}$ by the AM-GM Inequality. Consequently, $\frac{n \vpi(t)}{d_t}\hspace{-0.5mm}\ge \hspace{-0.5mm}\alpha \cdot \sqrt{\frac{2(1-\alpha)}{m}}$ holds for each $t\in V$, and the theorem follows. 
\end{proof}


\begin{comment}
\begin{proof} %To prove Theorem~\ref{thm:finalcost_analysis}, we utilize the Chebyshev's Inequality as shown below to bound the value of $\th$. 
Recall that the variance of $\epi(t)$ obtained by Algorithm~\ref{alg:VBES} is bounded by $\frac{L\cdot \th \cdot d_t}{n^2}\cdot n\epi(t)$ given in Theorem~\ref{thm:variance}. By the Chebyshev's Inequality, we have: 
\begin{align*}
\Pr\left\{\epi(t)-\bpi(t)\ge \frac{c}{2} \cdot \vpi(t)\right\} \le \frac{4 \cdot \Var\left[\epi(t)\right]}{c^2 \cdot \left(\vpi(t)\right)^2}\le \frac{4L\th d_t}{c^2 \cdot n\vpi(t)}. 
\end{align*}
Thus, by setting $\th= \frac{c^2 \cdot n\vpi(t)}{12Ld_t}$, $\epi(t)-\bpi(t)\le \frac{c}{2} \cdot \vpi(t)$ holds with probability at least $\frac{1}{3}$. In particular, we note $\frac{c^2 \cdot n\vpi(t)}{12Ld_t}\ge \frac{\alpha c^2}{12Ld_t}$ based on the fact that $\vpi(t)\ge \frac{\alpha}{n}$ as illustrated in Equation~\eqref{eqn:ite_pagerank}. If we set $\theta=\frac{\alpha c^2}{12Ld_t}$, then the expected time cost of Algorithm~\ref{alg:VBES} can be bounded by $O\left(\frac{1}{\th}\right)=O\left(\frac{12Ld_t}{\alpha c^2}\right)=\tilde{O}\left(d_t\right)$ with $L=\log_{1-\alpha}{\frac{c\alpha}{2n}}$ and the constants $\alpha$ and $c$. Moreover, as we shall prove below, $\frac{n \vpi(t)}{d_t}\hspace{-0.5mm}\ge \hspace{-0.5mm}\alpha  \hspace{-0.5mm} \cdot  \hspace{-0.5mm}\sqrt{\frac{2(1-\alpha)}{m}}$ holds for any $t\in V$. Thus, by setting $\th \hspace{-0.5mm}= \hspace{-0.5mm}\frac{\alpha c^2}{12L}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\sqrt{\frac{2(1-\alpha)}{m}}$, the expected time cost of Algorithm~\ref{alg:VBES} is bounded by $O\left(\hspace{-0.5mm}\frac{1}{\th}\hspace{-0.5mm}\right)\hspace{-0.5mm}=\hspace{-0.5mm}O\left(\frac{12L}{\alpha c^2}\hspace{-0.5mm}\cdot \hspace{-1mm}\sqrt{\frac{m}{2(1-\alpha)}}\right)=\tilde{O}\left(\hspace{-0.5mm}\sqrt{m}\right)$. Now we present the proof of $\frac{n \vpi(t)}{d_t}\hspace{-0.5mm}\ge \hspace{-0.5mm}\alpha \cdot \sqrt{\frac{2(1-\alpha)}{m}}$. By Equation~\eqref{eqn:ite_pagerank}, we have: 
\begin{equation}\label{eqn:sqrtm}
\begin{aligned}
\vpi(t)\ge (1-\alpha)\hspace{-2mm}\sum_{u\in N(t)}\hspace{-1mm}\frac{\vpi(u)}{d_u}+\frac{\alpha}{n} \ge (1-\alpha)\hspace{-1mm}\sum_{u\in N(t)}\frac{1}{d_u}\cdot \frac{\alpha}{n}+\frac{\alpha}{n}. %=\frac{\alpha}{n} \cdot \left(\sum_{u\in N(t)}\frac{(1-\alpha)}{d_u}\right). 
\end{aligned}    
\end{equation}
We note $\sum_{u\in N(t)}\hspace{-1mm}\frac{1}{d_u}\hspace{-0.5mm}\ge \hspace{-0.5mm}\frac{d_t^2}{2m}$ since
%\begin{align*}
$\left(\sum_{u\in N(t)}\frac{1}{d_u}\right)\cdot \left(\sum_{u\in N(t)}d_u\right)\ge \left(\sum_{u\in N(t)}1\right)^2=d_t^2$ 
%\end{align*}
holds by the Cauchy-Schwarz Inequality~\cite{steele2004cauchy}. Plugging into Inequality~\eqref{eqn:sqrtm}, we can further derive: 
\begin{align*}
\vpi(t)\hspace{-0.5mm}\ge \hspace{-0.5mm}\frac{\alpha}{n}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\left(\frac{(1-\alpha)d_t^2}{2m}\hspace{-0.5mm}+\hspace{-0.5mm}1\right)\hspace{-0.5mm}=\hspace{-0.5mm}\frac{\alpha d_t}{n}\hspace{-0.5mm}\cdot\hspace{-0.5mm} \left(\frac{(1-\alpha)d_t\hspace{-0.5mm}+\hspace{-0.5mm}\frac{2m}{d_t}}{2m}\right)\hspace{-0.5mm}\ge \hspace{-0.5mm}\frac{\alpha d_t}{n}\hspace{-0.5mm}\cdot \hspace{-0.5mm}\sqrt{\frac{2(1\hspace{-0.5mm}-\hspace{-0.5mm}\alpha)}{m}}, 
\end{align*}
where we apply the fact that $(1-\alpha)d_t+\frac{2m}{d_t} \ge 2\cdot \sqrt{(1-\alpha)2m}$ by the AM-GM Inequality. Consequently, $\frac{n \vpi(t)}{d_t}\hspace{-0.5mm}\ge \hspace{-0.5mm}\alpha \cdot \sqrt{\frac{2(1-\alpha)}{m}}$ holds for each $t\in V$, and the theorem follows. 
\end{proof}
\end{comment}
