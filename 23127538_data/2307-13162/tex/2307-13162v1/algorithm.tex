\vspace{-2mm}
\section{Algorithm}\label{sec:algorithm}
{\rev 
In this section, we present \setpush, an approximation algorithm for single-node PageRank on undirected graphs. Given an arbitrary target node $t$, the expected time complexity of \setpush to derive a $(c,p_f)$-approximation of $\vpi(t)$ can be bounded by %achieves 
$\tilde{O}\left(\min\left\{d_t, \sqrt{m}\right\}\right)$.  %with a given target node $t$.

Before introducing the details of the \setpush algorithm, we first analyze the reasons why existing methods are unable to achieve the $\tilde{O}\left(\min\left\{d_t, \sqrt{m}\right\}\right)$ time complexity for the single-node PageRank computation on undirected graphs. 
} 

\vspace{-2mm}
\subsection{Limitations of Existing Methods}\label{subsec:reasonswhy}
\vspace{-1mm}
\begin{itemize}
    \item 
    {\rev 
    For the Monte-Carlo method, it admits an $\Omega\left(\frac{1}{\vpi(t)}\right)$ lower bound for deriving a $(c, p_f)$-approximation of $\vpi(t)$ even on undirected graphs. Worse still, by the definition formula of PageRank, the initial probability distribution of simulating $\alpha$-random walks is $\frac{1}{n}\cdot \bm{1}=\left(\frac{1}{n}, \frac{1}{n}, \ldots, \frac{1}{n}\right)$. Therefore, in the worst-case scenario where $\vpi(t)=O\left(\frac{1}{n}\right)$ (e.g., the node $v$ in Figure~\ref{fig:high_level}), %a bad case for the Monte-Carlo method is to estimate the PageRank value of the node whose PageRank is approximately equal to $\frac{1}{n}$ (e.g., the node $v$ in Figure~\ref{fig:high_level}). This means 
    the Monte-Carlo method needs to simulate at least $\Omega\left(n\right)$ $\alpha$-random walks in order to hit node $t$ once. 
    }
    %As a consequence, the Monte-Carlo method incurs an $\Omega\left(n\right)$ time cost for estimating the PageRank of any node in the graph with constant relative error. 
    %{\rev 
    %As a result, the Monte-Carlo method incurs an $\Omega\left(n\right)$ time cost for deriving a $(c, p_f)$-approximation of $\vpi(t)$ even on undirected graphs. 
    %}
    
    \item %For the reverse exploration method, the backward push operation makes them subject to a time complexity lower bound of $O(d_t)$ for pushing the mass initially at $\r^b(t)$ to its $d_t$ neighbors. 
    {\rev 
    For the reverse exploration method, the time complexity is lower bounded by $O(d_t)$ since we at least need to 
    %the backward push operation introduces an $O(d_t)$ time cost for 
    reversely push the probability mass initially at node $t$ to all of its neighbors (i.e., the $d_t$ neighbors). 
    %makes them subject to a time complexity lower bound of $O(d_t)$ for pushing the mass initially at $\r^b(t)$ to its $d_t$ neighbors.   
    Consider the node $u$ in Figure~\ref{fig:high_level}, where the neighborhood size of $u$ is $O(n)$. When we use the reverse exploration method to estimate the PageRank of node $u$, the time cost has reached $\Omega(n)$ even only after the first backward push operation. 

    \item %For the hybrid method by employing the reverse exploration as a separate phase (e.g., FastPPR~\cite{lofgren2014FastPPR} and BiPPR~\cite{lofgren2016BiPPR}), the reverse exploration phase still costs $O(d_t)$ time after the first backward push operation, which becomes $O(n)$ in some bad cases (e.g., given node $u$ as the target node in Figure~\ref{fig:high_level}). 
    For the hybrid method, 
    %which combine Monte-Carlo sampling with the vanilla backward push operation in the algorithm (e.g., FastPPR~\cite{lofgren2014FastPPR} and BiPPR~\cite{lofgren2016BiPPR}), 
    the limitations introduced by the lower bounds for Monte-Carlo sampling (i.e., $O\left(1/\vpi(t)\right)$) and the vanilla backward push (i.e., $O\left(d_t\right)$) still exist. Two exceptions are the \sublinear method~\cite{bressan2018sublinear} and the RBS method~\cite{wang2020RBS}. 
    }
    \begin{itemize}
        \item 
        {\rev 
        The \sublinear method sets a blacklist to record all high-degree nodes in the graph. %the nodes with large degrees. 
        In the reverse exploration phase, the \sublinear method only performs the backward push operations from the nodes outsides the blacklist set. %thus achieves $\tilde{O}\left(\min\left\{\frac{m^{2/3}\dmax^{1/3}}{\bar{d}^{1/3}}, \frac{m^{4/5}}{\bar{d}^{3/5}} \right\}\right)$ expected time complexity. 
        However, the \sublinear method still includes the Monte-Carlo sampling phase to simulate $\alpha$-random walks from a uniformly selected source node. Tthe $\Omega \left(1/\vpi(t)\right)$ lower bound for Monte-Carlo sampling hinders the \sublinear method from achieving the $\tilde{O}\left(\min\left\{d_t, \sqrt{m}\right\}\right)$ time complexity for the single-node PageRank computation on undirected graphs. 
        %as shown in Table~\ref{tbl:comparison}, such complexity is still no better than $\tilde{O}\left(\min\left\{d_t, \sqrt{m}\right\}\right)$. % as illustrated in Section~\ref{sec:intro}. 

        \item For RBS, its major drawback comes from the sampling operation that RBS adopts in each backward push operation. Specifically, the sampling operation adopted in each backward push operation of RBS is non-independent. Consider the bad case scenario as shown in Figure~\ref{fig:related}. The residue increment $\frac{(1-\alpha)\r^b(v)}{d_u}$ of each neighbor $u\in N(t)$ is identical. 
        %As a result, the function of the threshold $\theta$ for determining the fraction of deterministic push is disabled. in this bad case scenario, the RBS method needs to cost $O(d_t)=O(n)$ to deterministically 
        As a result, for all neighbors $u\in N(t)$, the conditions to conduct a randomized push (i.e., $\frac{(1-\alpha)\r^b(v)}{d_u}\ge rand \cdot \theta$) are satisfied simultaneously, which, again, leads to the $O(d_t)=O(n)$ time cost in such bad case scenario. 
        %which causes large variance and can only achieve a $\tilde{O}\left(n\right)$ expected time cost at best for $(\rela, \pf)$-approximation of single-node PageRank. 
        }
    \end{itemize}
    %An exception is the \sublinear method proposed by Bressan et al. ~\cite{bressan2018sublinear}.The \sublinear method sets a blacklist to record all the nodes with large degrees. In the reverse exploration phase, the \sublinear method performs the backward push operations only from the nodes not in the blacklist set, and thus achieves $\tilde{O}\left(\min\left\{\frac{m^{2/3}\dmax^{1/3}}{\bar{d}^{1/2}}, \frac{m^{4/5}}{\bar{d}^{3/5}} \right\}\right)$ expected time complexity. Nonetheless, such complexity is still no better than $\tilde{O}\left(\min\left\{d_t, \sqrt{m}\right\}\right)$ as illustrated in Section~\ref{sec:intro}. 
    %\item 
%e\vspace{-1mm}
\end{itemize}
In the following, we shall describe our \setpush in details and explain the superiority of our \setpush over existing methods. Specifically, we first define a concept called {\em truncated PageRank} in Section~\ref{subsec:trunc_pagerank}. Our \setpush is based on a $\left(\frac{\rela}{2}, \pf\right)$-approximation of the truncated PageRank. After that, in Section~\ref{subsec:highlevelidea} and ~\ref{subsec:setpushalg}, we provide the high-level ideas and detailed algorithm structure of \setpush. 


%\vspace{-2mm}
\subsection{Truncated PageRank}\label{subsec:trunc_pagerank}
%\vspace{-1mm}
%\header{\bf Truncated PageRank. } 
Given a target node $t$ in an undirected graph $G=(V,E)$, a constant damping factor $\alpha\in (0,1)$, and a constant relative error $\rela$, we refer to $\bpi(t)$ as the {\em truncated PageRank} of node $t$ if %$\bpi(t)=\frac{1}{n}\cdot \sum_{s\in V}\sum_{\ell=0}^{L} \vpi^{(\ell)}_s(t)$, 
\begin{align}\label{eqn:def_trunc_pagerank}
\bpi(t)=\frac{1}{n}\cdot \sum_{s\in V}\sum_{\ell=0}^{L} \vpi^{(\ell)}_s(t), 
\end{align}
where $L=\log_{1-\alpha}\frac{\rela \alpha}{2n}=O\left(\log{n}\right)$. Analogously, we call the $n$-dimensional vector $\bpi=\frac{1}{n}\cdot \sum_{s\in V}\sum_{\ell=0}^{L} \vpi^{(\ell)}_s$ the truncated PageRank vector. By Equation~\eqref{eqn:ite_power_method_ppr}, Equation~\eqref{eqn:PageRank_PPR} and Equation~\eqref{eqn:def_lhopppr}, we can further derive: 
%We observe that the PageRank vector $\vpi$ can be expressed as the sum of the truncated PageRank vector and $\ell$-hop PPR for $\ell\in \{L+1, L+2, \ldots \}$ by utilizing :  
\begin{align*}
\vspace{-2mm}
\vpi=\frac{1}{n}\cdot \sum_{s\in V}\sum_{\ell=0}^{\infty}\vpi^{(\ell)}_s=\bpi+\frac{1}{n}\cdot \sum_{s\in V}\sum_{\ell=L+1}^{\infty}\alpha (1-\alpha)^\ell \cdot \left(\A \D^{-1}\right)^\ell \cdot \bm{e}_s. 
\end{align*}
Therefore, for every $t\in V$, we have: 
\begin{align*}
\vpi(t)=\bpi(t)+\frac{1}{n}\cdot \sum_{s\in V}\sum_{\ell=L+1}^{\infty}\alpha (1-\alpha)^\ell \cdot \bm{e}_t^{\top}\cdot \left(\A \D^{-1}\right)^\ell \cdot \bm{e}_s. 
\end{align*}
We note that for each $\ell\in \{0, 1, 2, \ldots\}$, $\left(\bm{e}_t^{\top}\cdot \left(\A \D^{-1}\right)^\ell \cdot \bm{e}_s\right) \in [0,1]$. Thus, we have $\frac{1}{n}\cdot \sum_{s\in V}\bm{e}_t^{\top}\cdot \left(\A \D^{-1}\right)^\ell \cdot \bm{e}_s \le 1$. As a consequence, we can derive $\vpi(t)\le \bpi(t)+\sum_{\ell=L+1}^{\infty} \alpha (1-\alpha)^\ell=\bpi(t)+(1-\alpha)^{L+1}$. 
%\begin{align*}
%\vpi(t)\le \bpi(t)+\sum_{\ell=L+1}^{\infty} \alpha (1-\alpha)^\ell=\bpi(t)+(1-\alpha)^{L+1}. 
%\end{align*}
Recall that $L=\log_{1-\alpha} \frac{\rela \alpha}{2n}$. Then it follows: 
\begin{align}\label{eqn:bpagerank_basic}
\vpi(t) \le \bpi(t)+ \frac{c}{2}\cdot \frac{\alpha}{n}\le \bpi(t)+ \frac{c}{2}\cdot \vpi(t), 
\end{align}
where we apply the lower bound of $\vpi(t)$ that $\vpi(t)\ge \frac{\alpha}{n}$ as shown in Equation~\eqref{eqn:ite_pagerank}. Furthermore, Lemma~\ref{lem:trunc_pagerank_approx} implies that deriving a $(\rela, \pf)$-approximation of $\vpi(t)$ can be achieved by deriving a $(\frac{c}{2}, p_f)$-approximation of $\bpi(t)$. 

\begin{lemma}\label{lem:trunc_pagerank_approx}
Given a target node $t$ in the graph $G=(V,E)$, $\epi(t)$ is a $(\rela, \pf)$-approximation of node $t$'s PageRank $\vpi(t)$ if 
\begin{align*}
|\epi(t)-\bpi(t)|\le \frac{\rela}{2}\cdot \vpi(t)
\end{align*}
holds with probability at least $1-\pf$. 
\end{lemma}

\begin{proof}
For each node $t\in V$, we observe: 
\begin{equation*}
\begin{aligned}
&\left|\epi(t)-\vpi(t)\right|=\left|\epi(t)-\bpi(t)+\bpi(t)-\vpi(t)\right|\\
&\le \left|\epi(t)-\bpi(t)\right|+\left|\bpi(t)-\vpi(t)\right|\le \left|\epi(t)-\bpi(t)\right|+\frac{c}{2}\cdot \vpi(t), 
\end{aligned}    
\end{equation*}
where we plugging Equation~\eqref{eqn:bpagerank_basic} into the last inequality. Thus, if $\left|\epi(t)\hspace{-0.5mm}-\hspace{-0.5mm}\bpi(t)\right|\hspace{-0.5mm}\le \hspace{-0.5mm}\frac{c}{2}\cdot \vpi(t)$ holds with probability at least $1-\pf$, $\epi(t)$ is a $(\rela, \pf)$-approximation of $\vpi(t)$, which follows the lemma. 
\end{proof}

%\header{\bf Independent Volume-biased Evolving Set Process. } 

\subsection{Key Idea of \setpush}\label{subsec:highlevelidea}
%\subsection{A primitive operation: \spush}
Given an undirected graph $G=(V,E)$ and a target node $t$, our \setpush computes a $(c,p_f)$-approximation of node $t$'s PageRank by deriving a $(c/2, p_f)$-approximation $\epi(t)$ of $\bpi(t)$ according to
\begin{align}\label{eqn:actual_eqn}
\epi(t)=\frac{1}{n}\cdot \sum_{s\in V} \sum_{\ell=0}^L \frac{d_t}{d_s}\cdot \epi_t^{(\ell)}(s), 
\end{align}
where $\epi_t^{(\ell)}(s)$ is a $(c/2, p_f/n)$-approximation of the $\ell$-hop PPR value $\vpi_t^{(\ell)}(s)$. To understand Equation~\eqref{eqn:actual_eqn}, recall that $\vpi_t^{(\ell)}(s)\cdot d_t=\vpi_s^{(\ell)}(t)\cdot d_s$ as shown in Equation~\eqref{eqn:undirectedPPR}. Thus, if for each $s\in V$, $\epi_t^{(\ell)}(s)$ is a $(c/2, p_f/n)$-approximation of the $\ell$-hop PPR value $\vpi_t^{(\ell)}(s)$, then $\frac{d_t}{d_s}\cdot \epi_t^{(\ell)}(s)$ is a $(c/2, p_f/n)$-approximation of $\epi_s^{(\ell)}(t)$. According to the definition formula of the truncated PageRank $\bpi(t)$ as shown in Equation~\eqref{eqn:def_trunc_pagerank}, $\epi(t)$ is therefore a $(c/2, p_f)$-approximation of $\vpi(t)$. 

To compute $\epi_t^{(\ell)}(s)$, we maintain a variable called {\em $\ell$-hop residue} $\r^{(\ell)}_t(u)$ for each node $u$ in $G$. Initially, we set $\r^{(\ell)}_t\hspace{-1mm}=\bm{0}$ for $\forall \ell \hspace{-0.5mm}\in \hspace{-0.5mm}\{1,2, \ldots, L\}$ and $\r^{(0)}_t\hspace{-1mm}=\hspace{-0.5mm}\bm{e}_t$, where $\bm{0}$ is an $n$-dimensional all zero vector. During the query phase, we repeatedly conduct the following steps to update $\r^{(\ell+1)}_t$ based on $\r^{(\ell)}_t$ by iterating $\ell$ from $0$ to $L\hspace{-0.5mm}-\hspace{-0.5mm}1$: 
\begin{itemize}
\item Pick a node $u$ with nonzero $\r^{(\ell)}_t(u)$; 
\item If $(1-\alpha)\cdot \r^{(\ell)}_t(u) \ge \theta \cdot d_u$, we uniformly distribute $(1-\alpha)\cdot \r^{(\ell)}_t(u)$ to the $(\ell+1)$-hop residue $\r_t^{(\ell+1)}(v)$ of each $v\in N(u)$. To be more specific, for $\forall v\in N(u)$, $\r^{(\ell+1)}_t(v)\gets \r^{(\ell+1)}_t(v)+\frac{(1-\alpha)}{d_u}\cdot \r^{(\ell)}_t(u)$. Note that $\theta \in (0,1)$ is a tunable threshold and we provide a detailed analysis to the choice of $\theta$ in Section~\ref{sec:analysis}. 
\item Otherwise, we independently select some neighbors of $u$, and only distribute the probability mass at $\r^{(\ell)}_t(u)$ to those sampled neighbors. Notably, for each $v\in N(u)$, the expectation of $\r_t^{(\ell+1)}(v)$'s increment is still guaranteed to be $\frac{(1-\alpha)}{d_u}\cdot \r^{(\ell)}_t(u)$. 
%the expected time cost is linear to the size of the sampled outcomes, while for each $v\in N(u)$, 
%the expected increment of $\r^{(\ell+1)}_t(v)$ still equals $\frac{(1-\alpha)}{d_u}\cdot \r^{(\ell)}_t(u)$. 
%each neighbor $v\in N(u)$ is sampled with probability $\frac{(1-\alpha)}{\theta \cdot d_u}\cdot \r^{(\ell)}_t(u)$. 
\end{itemize}
%Additionally, the expected increment of $\r^{(\ell+1)}_t(v)$ still equals $\frac{(1-\alpha)}{d_u}\cdot \r^{(\ell)}_t(u)$. 

After all the $L$ iterations have been processed, we return $\epi(t)=\frac{1}{n}\cdot \sum_{s\in V} \sum_{\ell=0}^L \frac{d_t}{d_s}\cdot \alpha \cdot  \r^{(\ell)}_t(s)$ as an estimator of $\vpi(t)$. 

As we shall demonstrate in Section~\ref{sec:analysis}, the $\ell$-hop residue vector $\r_t^{(\ell)}$ is an unbiased estimate of $\frac{1}{\alpha}\cdot \vpi_t^{(\ell)}$. In other words, $\E\left[\r_t^{(\ell)}(u)\right]=\frac{1}{\alpha}\cdot \vpi_t^{(\ell)}(u)$ holds for each $u\in V$. 
%To see the correctness of such update process, 
%The correctness of the above update process can be directly verified based on Equation~\eqref{eqn:PPR_recur}. 
%Specifically, we show that for each $u\in V$, $\r_t^{(\ell)}(u)$ is an unbiased estimate of $\frac{1}{\alpha}\cdot \vpi_t^{(\ell)}(u)$. Then by Equation~\eqref{eqn:undirectedPPR} and Equation~\eqref{eqn:def_trunc_pagerank}, the correctness of our \setpush follows. 
To see this, we observe that $\vpi^{(0)}_t=\alpha \cdot \bm{e}_t$ holds by definition. Recall that we set $\r^{(0)}_t=\bm{e}_t$ as mentioned above. Therefore, $\E\left[\r^{(\ell)}_t\right]=\frac{1}{\alpha}\cdot \vpi^{(\ell)}_t$ holds when $\ell=0$. Furthermore, let us assume $\r^{(\ell)}_t=\frac{1}{\alpha}\cdot \vpi^{(\ell)}_t$ holds for any $i \in [0,\ell]$. Then for each $v\in V$, the expectation of $\r^{(\ell+1)}_t(v)$ satisfies: 
\begin{align*}
\E\left[\r^{(\ell+1)}_t(v)\right]=\hspace{-2mm}\sum_{u\in N(v)}\hspace{-2mm}\frac{(1-\alpha)}{d_u}\cdot \E\left[\r^{(\ell)}_t(u)\right]=\hspace{-2mm}\sum_{u\in N(v)}\hspace{-2mm}\frac{(1-\alpha)}{d_u}\cdot \frac{\vpi^{(\ell)}_t(u)}{\alpha}. 
\end{align*}
By Equation~\eqref{eqn:PPR_recur}, we can therefore derive $\E\left[\r^{(\ell+1)}_t(v)\right]=\frac{1}{\alpha}\cdot \vpi^{(\ell+1)}_t(v)$. Consequently, for every $\ell\in \{1, \ldots, L\}$, $\E\left[\r^{(\ell)}_t\right]\hspace{-1mm}=\frac{1}{\alpha}\cdot \vpi^{(\ell)}_t$ holds by induction. The formal proof can be found in Section~\ref{sec:analysis}. 

Furthermore, it can be proved that $\epi(t)$ is also an unbiased estimator of the truncated PageRank $\bpi(t)$. Specifically, recall that $\epi(t)=\frac{1}{n}\cdot \sum_{s\in V}\sum_{\ell=0}^{L} \frac{d_t}{d_s}\cdot \alpha \cdot \er^{(\ell)}_t(s)$ according to Algorithm~\ref{alg:VBES}. By applying the linearity of expectation, we can thus derive 
\begin{align*}
\E\left[\epi(t)\right]=\frac{1}{n}\cdot \sum_{s\in V}\sum_{\ell=0}^{L} \frac{d_t}{d_s}\cdot \alpha \cdot \E \left[\r_t^{(\ell)}(s)\right]=\frac{1}{n}\cdot \hspace{-1mm}\sum_{s\in V}\sum_{\ell=0}^{L} \frac{d_t}{d_s}\cdot \vpi_t^{(\ell)}(s).     
\end{align*}
Recall that in Equation~\eqref{eqn:undirectedPPR}, we show that $\frac{d_t}{d_s}\cdot \vpi^{(\ell)}_t(s)=\vpi^{(\ell)}_s(t)$, following $\E\left[\epi(t)\right]=\frac{1}{n}\cdot \hspace{-1mm}\sum_{s\in V}\sum_{\ell=0}^{L} \vpi_s^{(\ell)}(t)=\bpi(t)$. 

%Recall that the $\ell$-hop PPR value $\vpi_t^{(\ell)}(u)$ corresponds to the probability that an $\alpha$-random walk starting from node $t$ terminates at node $u$ at the $\ell$-th step. 
%$\r^{(\ell)}(u)$ to record the probability mass that is to be propagated from node $u$ to its neighbors. Initially,
%Analogous to the reverse exploration method mentioned before, we also maintain a residue vector to record the probability mass to be pushed from the current node. 


{\rev 
\header{\bf Advantages of the Push Operation Adopted in \setpush. }
}
%Note that the $\ell$-hop residue $\r^{(\ell)}_t(u)$ defined above is similar in spirit to the one used in the backward push operations (see Section~\ref{subsec:reverse_exploration}), but differs in two crucial aspects as described below. 
{\rev 
Note that the $\ell$-hop residue $\r^{(\ell)}_t(u)$ defined above is similar in spirit to the one used in the vanilla backward push operation adopted in the reverse exploration method (see Section~\ref{subsec:reverse_exploration}), but differs in two crucial aspects as described below. 
\begin{itemize}
    \item To distribute the probability mass maintained at $\r^{(\ell)}_t(u)$, the backward push operation (except in RBS~\cite{wang2020RBS}) %merely adopted in all reverse exploration and hybrid  methods~\cite{andersen2007contribution, lofgren2013personalized, lofgren2014FastPPR, lofgren2016BiPPR, bressan2018sublinear} (except the RBS method~\cite{wang2020RBS}) 
    touches every neighbor $v$ of $u$ to update the residue of $v$, which costs $O(d_u)$ deterministically. In comparison, for the node $u$ with $(1-\alpha)\cdot \r^{(\ell)}_t(u)\le \theta \cdot d_u$, we only select some neighbors $v\in N(u)$ to update $\r^{(\ell+1)}_t(v)$. Therefore, the time cost of each update process is only proportional to the size of the sampled outcomes. By this means, we successfully avoid the $O(d_u)$ term of time complexity introduced by the vanilla backward push. 
    \item Compared to the RBS method, we independently sample the neighbors $v$ from $N(u)$ to update $\r^{(\ell+1)}_t(v)$. As a consequence, the increment of $\r^{(\ell+1)}_t(v)$ for each $v\in V$ is independent with each other. In contrast, the sampling technique adopted in the RBS method~\cite{wang2020RBS} is non-independent, resulting in either large variance or expensive time cost. For example, consider the graph shown in Figure~\ref{fig:related} with node $t$ as the given target node. For the RBS method, the sampling condition of each $u\in N(t)$ is satisfied simultaneously, which costs either $O(n)$ time or unbounded approximation error. Instead, in \setpush, we can independently some $u\in N(t)$ to update $\r^{(\ell)}_t(u)$. 
\end{itemize}
}


\subsection{The \setpush Algorithm}\label{subsec:setpushalg}
\begin{algorithm}[t]
%\caption{\localpush for $L$-hop Transition Probabilities}
\caption{The \setpush Algorithm}
\label{alg:VBES}
\BlankLine
%\KwIn{Graph $G=(V,E)$, source node $s\in V$, length of random walk $L$, relative error threshold $\delta$, failure probability $p_f$, the number of random walk $n_r$\\}
\KwIn{Undirected graph $G=(V,E)$, target node $t\in V$, constant damping factor $\alpha$, threshold $\theta$\\}
\KwOut{Estimator of $\vpi(t)$\\}
Initialize two $n$-dimensional vectors $\er^{(0)}_t \hspace{-1.5mm}\gets \hspace{-0.5mm}\bm{e}_t$ and $\epi_t \hspace{-0.5mm} \gets \hspace{-0.5mm} \alpha \bm{e}_t$\; 
$L \gets \log_{1-\alpha}\frac{c\alpha}{2n}$\;
\For{$\ell$ from $0$ to $L-1$}{
    Initialize an $n$-dimensional vector $\er^{(\ell+1)}_t \gets \bm{0}$\; 
    \For{each $u\in V$ with nonzero $\er^{(\ell)}_t(u)$}{
        \If{$(1-\alpha)\cdot \er^{(\ell)}_t(u)\ge \th\cdot d_u$}{
            \For{each $v\in N(u)$}{
                $\er^{(\ell+1)}_t(v)\gets \er^{(\ell+1)}_t(v)+\frac{(1-\alpha)}{d_u}\cdot \er^{(\ell)}_t(u)$\;
            }
        }
        \Else{
            Let $idx\gets 0$, and $p^*\gets \frac{(1-\alpha)\cdot \er^{(\ell)}_t(u)}{d_u\cdot \th}$\; 
            \While{true}{
                Generate a geometrical random $rg \sim G(p^*)$\; 
                $idx \gets idx +rg$\; 
                \If{$idx > d_u$}{
                    break\;
                }
                Let $v$ denote the $idx$-th node in $N(u)$\; 
                %random number $\rand$ according to $\rand \sim B\left(d_u, \frac{(1-\alpha)\cdot \er^{(\ell)}_t(u)}{d_u\cdot \th}\right)$, and select $\rand$ neighbors from $N(u)$ uniformly at random\;
                %\For{each selected neighbor $v\in N(u)$}{
                $\er^{(\ell+1)}_t(v)\gets \er^{(\ell+1)}_t(v)+\th$\;
                %}
            }
        }
    }
    Clear $\er^{(\ell)}_t$\; 
    $\epi_t \gets \epi_t+\alpha \cdot \er^{(\ell+1)}_t$\;
}
$\epi(t) \gets \frac{1}{n}\cdot \sum_{s\in V}\frac{d_t}{d_s}\cdot \epi_t(s)$\;
\Return $\epi(t)$ as an estimator of $\vpi(t)$;
\end{algorithm}

Algorithm~\ref{alg:VBES} illustrates the pseudocode of the \setpush algorithm. Consider an undirected graph $G=(V,E)$, a target node $t$, a constant damping factor $\alpha\in (0,1)$ and a threshold parameter $\theta \in (0,1)$. Initially, we set $\r^{(0)}_t=\bm{e}_t$ and iteratively conduct the update process as described in Section~\ref{subsec:highlevelidea} from $\ell=0$ to $L-1$, where $L=\log_{1-\alpha}\frac{\rela \alpha}{2n}$. In particular, for the node $u$ with $0<(1-\alpha)\cdot \r^{(\ell)}_t(u)\le \theta \cdot d_u$, we adopt a {\em geometric sampling operation} to independently select neighbors $v$ from $N(u)$. Specifically, we aim to independently sample every $v\in N(u)$ with probability $p^*=\frac{(1-\alpha)\cdot \r^{(\ell)}_t(u)}{d_u \cdot \th}$. For each sampled $v\in N(u)$, we increase the residue $\r_t^{(\ell+1)}(v)$ by $\th$. By this means, the expectation of $\r_t^{(\ell+1)}(v)$'s increment is still $\frac{(1-\alpha)}{d_u}\cdot \r^{(\ell)}_t(u)$. It's worth mentioning that we aim to complete the above described sampling process using the time $O(d_u \cdot p^*)$. In other words, we require the expected time of the sampling process is asymptotically the same to the expected size of the sampling outcomes (i.e., the expected number of $u$'s neighbors that are successfully sampled). To achieve this goal, we define a variable $idx$ for referring to the index of the neighbor $v$ in $N(u)$ that to be sampled. Initially, we set $idx$ as $0$. Moreover, we define a geometric random number $rg$, and repeatedly generate $rg$ according to the geometric distribution $G(p^*)$. According to~\cite{devroye2006nonuniform,bringmann2012efficient}, a geometric random number can be generated in $O(1)$ time. %(see Appendix for the formal proof)
%, where $rand()$ denotes a uniform random number in $[0,1]$. 
We repeatedly generate the geometric random number $rg \sim G(p^*)$, update $idx \gets idx+rg$ and increase the residue $\r_t^{(\ell+1)}(v)$ of the $idx$-th neighbor $v$ in $N(u)$ by $\th$, until $idx > d_u$. %As a result, the expected time cost of the sampling process can be bounded by $O(d_u\cdot p^*)$. 

To understand the sampling process mentioned above, recall that a geometric random number $rg\sim G(p^*)$ indicates the number of Bernoulli trials needed to get one success, where
%the binomial distribution $B(x,p)$ corresponds to the probability distribution of the number of successes in a sequence of $x$ independent Bernoulli trials. 
each Bernoulli trial has two Boolean-valued outcomes: success (with probability $p^*$) and failure (with probability $1-p^*$). Therefore, by generating $rg \sim G(p^*)$, we are able to derive the index of the first sampled node in $N(u)$, using only $O(1)$ time. We iteratively generate $rg \sim G(p^*)$ to derive the index of the next sampled node from the index of the last sampled neighbor (recorded by $idx$). By this means, we are able to independently select each neighbor $v$ from $N(u)$ with probability $p^*$ using only $O(d_u\cdot p^*)=O\left(\frac{(1-\alpha)\cdot \r_t^{(\ell)}(u)}{\theta}\right)$ time in expectation. By carefully setting the value of $\theta$ (see Section~\ref{sec:analysis} for details), the expected time cost of \setpush can be consequently bounded by $O\left(\min\left\{d_t, \sqrt{m}\right\}\right)$. 

Additionally, after the $\ell$-th iteration ($\forall \ell \in \{0,1, \ldots, L-1\}$), we clear the $\ell$-hop residue vector $\r_t^{(\ell)}$ to save memory. Finally, we return $\epi(t)=\frac{1}{n}\cdot \sum_{s\in V} \sum_{\ell=0}^L \frac{d_t}{d_s}\cdot \alpha \r^{(\ell)}_t(s)$ as the estimator of $\vpi(t)$. 


%{\rev 
%\subsection{Theoretical Improvements over Existing Methods}
%}

%\header{\bf Algorithm Descriptions. } 




%%% Local Variables:
%%% mode: latex
%%% TeX-master: "paper"
%%% End:
