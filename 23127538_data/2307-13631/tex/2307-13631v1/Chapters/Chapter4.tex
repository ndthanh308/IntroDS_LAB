% Chapter 1

\chapter{Question Classification in Biomedical Question Answering} % Main chapter title

\label{Chapter4} % For referencing the chapter elsewhere, use \ref{Chapter1}
\setcounter{secnumdepth}{4}
\minitoc
This chapter presents the methods we propose for question classification in biomedical QA, a key task that is studied and evaluated separately. Section~\ref{Chapter4.2} will be dedicated to our proposed method for question type classification in biomedical QA. We consecrate section~\ref{Chapter4.3} to our proposed question topic classification method in biomedical QA.

\section{Introduction}
\label{Chapter4.1}
As we have described previously in section~\ref{Chapter3_6_1}, question classification is usually the first component in QA pipeline as the first step towards developing biomedical QA systems is processing and classifying the question in order to identify the question type and therefore the answer type to produce. The need of biomedical QA systems to handle a variety of natural language questions and to extract appropriate natural language answers, has resulted in questions being classified along various dimensions, including question type, topic, user, resource, etc. The most common question classification in biomedical QA is question type, sometimes referred to as the expected answer format. The identification of the type of a given biomedical question type is useful in candidate answer extraction as it allows to a biomedical QA system to know in advance the expected answer format, and therefore to use the appropriate answer extraction technique. In other words, the identification of the expected answer format required by a natural language question is usually carried out based on the question type and its linguistic information. For instance, a question identified as yes/no requires one of the two answers ``yes'' or ``no''. In contrast, a factoid question expects a particular answer type, such as an entity name. In this context, the most recent taxonomy of biomedical questions that is created by the BioASQ challenge \citep{tsatsaronis2012bioasq} consists of four types of questions that may cover all kinds of potential questions:

\begin{enumerate}
  \item Yes/No questions: They require only one of the two possible answers: ``yes'' or ``no''. For example, ``Is calcium overload involved in the development of diabetic cardiomyopathy?'' is a yes/no question and the answer is ``yes''.
  \item Factoid questions: They require a particular entity name (e.g., of a disease, drug, or gene), a number, or a similar short expression as an answer. For example, ``Which enzyme is deficient in Krabbe disease?'' is a factoid question and the answer is a single entity name ``galactocerebrosidase''.
  \item List questions: They expect a list of entity names (e.g., a list of gene names,  list of drug names), numbers, or similar short expressions as an answer. For example, ``What are the effects of depleting protein km23–1 (DYNLRB1) in a cell?'' is a list question.
  \item Summary questions: They expect a summary or short passage in return. For example, the expected answer format for the question ``What is the function of the viral KP4 protein?'' should be short text summary.
\end{enumerate}


With a view to developing an automatic biomedical QA system, the types of biomedical questions should be automatically identified by the system. The task of a question type classifier is to assign a class label, a category name which specifies the type of question, to a given natural language question. This task is usually carried out by checking whether it contains auxiliary verbs or Wh-question particles.  However, the complexity of the natural language poses many issues and challenges to this task in the biomedical domain, as some question that appear, at first, to belong to certain type can result to be of a different one. For instance, the biomedical question ``Where is the protein CLIC1 localized?'', from the BioASQ training questions, is a factoid question. Although the way in which it is constructed, by starting with ``where'', may lead it to the wrong classification of it being a summary question.

Another important dimension of classifying questions in biomedical QA is topic, sometimes referred to as the semantic type of the expected answer, as previously discussed in section~\ref{Chapter3_6_1}. Question topic classification refers to the process by which one or more topics are automatically derived from the given biomedical questions. The answer type is the semantic type of the expected answer, and is useful in generating specific answer retrieval strategies. For example, the question ``What is the best way to catch up on the diphtheria-pertussistetanus vaccine (DPT) after a lapse in the schedule?'' from NLM's data set represents a pharmacological question, and a biomedical QA system can therefore use the Micromedex pharmacological database as the resource to extract the answers. Recently, various approaches have addressed clinical question topic classification. For instance, \citep{yu2008automatically} and \citep{Cao_2010} proposed a clinical question topic classification method based on the combination of words, stemming, bigrams, UMLS concepts and UMLS semantic types as features for machine-learning algorithms to automatically classify an ad hoc clinical question into general topics. However, the existing methods have not taken into account the syntactic dependency relations in questions. Therefore, this may impact negatively the performance of the clinical question topic classification system.

In this context, we propose in the first contribution of this thesis work two machine learning-based methods for question classification in biomedical QA. The first method aims at classifying biomedical questions into one of the four categories: (1) yes/no, (2) factoid, (3) list, and (4) summary. The second method automatically identifies general topics from ad hoc clinical questions.  In the remaining of the chapter, we go into details about proposed question classification methods in biomedical QA.


\section{Question Type Classification}
\label{Chapter4.2}

In this section we describe the machine learning-based method we propose for biomedical question type classification \citep{kdir15,Sarrouti_MIM_2017}. As stated previously, the purpose of this method is to classify biomedical questions into one of the four categories: yes/no, factoid, list and summary  questions.

\subsection{Method}
\label{Chapter4.2.1}
With a view to achieving the goal of classifying the biomedical questions into the aforementioned categories, we first extract the appropriate features from BioASQ questions using our handcrafted lexico-syntactic patterns and then feed them to a machine-learning algorithm to conduct the classification task. The flowchart of the proposed method, as shown in Figure~\ref{fig:qc}, is constructed through the following main steps: (1) features extraction from biomedical questions using the set of our handcrafted lexico-syntactic patterns, (2) learned classifiers, and (3) predicting the class label using the trained classifiers.

% Figure environment removed

\subsubsection{Machine-learning models}
\label{Chapter4.2.1.1}
We have experimented with several machine-learning algorithms including SVM, Naive Bayes, and Decision Tree. Since a question can be assigned to one of 4 classes (i.e., yes/no, factoid, list, and summary), a multi-class classification has been used in this study. We have found that SVM outperforms the others, followed by Naive Bayes. Therefore, we have decided to report the results of SVM. Mallet\footnote{Mallet: \url{http://mallet.cs.umass.edu/index.php}.} and libSVM\footnote{LibSVM: \url{https://www.csie.ntu.edu.tw/~cjlin/libsvm/}.} are the freely available platforms for machine-learning algorithms that have been used in this work. We have used Mallet for Naive Bayes and Decision Tree. For SVM, we have used LibSVM.
\subsubsection{Machine-learning features}
\label{Chapter4.2.1.2}

We explored different features for machine-learning systems including words, bigrams, part-of-speech (POS) tags and features obtained by our predefined handcrafted lexico-syntactic patterns.
We first have experimented with bag-of-words also known as unigrams (terms in the questions are extracted, all tokens which match stop words list\footnote{Stop words list: \url{http://www.textfixer.com/resources/common-english-words.txt}} were removed). Then, we have generated N-grams, proximity-based sequences of words obtained by sliding a window of size N over the biomedical question, and use them as features. In this work, the bigrams terms, i.e., N-grams with $N=2$, are used. We also experimented with part-of-speech tags as features for machine-learning algorithms. We have used the Stanford CoreNLP tools for tokenization and POS tagging, as research shows a good performance of the latter in the biomedical domain \citep{lin2007syntactic}. Finally, we
have used features that have been provided by our predefined handcrafted lexico-syntactic patterns. The patterns were derived by analyzing 810 BioASQ training questions. They were found by passing the questions one by one to the Stanford CoreNLP for tokenization and POS tagging so as to capture their syntactic structure. Indeed, we have extracted words and their POS tags from each of BioASQ training questions. We have found after analysing all these questions that the ones belonging to a particular class have some typical structure. In addition, we found that adding
WordNet synonyms led to an enhanced performance of question classification; we therefore have used WordNet to generate the synonyms of some words in some patterns (e.g., see patterns for factoid questions).

As already noted, in order to extract the best feature set, our predefined patterns were used. The patterns are regular expressions that are represented as a text string. For a given biomedical question, the appropriate pattern is selected from the set of patterns as follow: after preprocessing (tokenization and POS tagging) the biomedical question using Stanford CoreNLP, the pattern is set to the left end of the biomedical question, and matching process starts. After a mismatch is found, the pattern is shifted one place right and a new matching process starts, and so on. Below are the set of patterns that have been used for the four categories: yes/no, factoid, list, and summary. The ``+'', ``|'', and ``*'' signs indicate the concatenation, OR, and any terms, respectively. Note also that NN, JJ, VBZ, VBP, and TAG indicate noun, adjective, verb 3rd person singular present, verb non-3rd person singular present, and the obtained par-of-speech tag of the word, respectively.


\begin{enumerate}
  \item \textbf{Yes/No patterns}: We have proposed the following expression for yes/no questions.
\begin{enumerate}
\item {[Be verbs $\mid$  Modal verbs $\mid$ Auxiliary verbs] + [*] +?};  where be verbs = \{am, is, are, been, being, was, were\},
modal verbs=  \{can, could, shall, should, will, would, may, might\}, and auxiliary verbs=  \{do, did, does, have, had, has\}
\end{enumerate}
\item \textbf{Factoid patterns}: We have defined the following patterns for questions which can belong to factoid category.
\begin{enumerate}

\item {[What $\mid$ Which] + [VBZ] + [*] + [X] + [*] +?}; where X = \{number, name, indication, value, frequency, prevalence, or WordNet synonyms of these words\}
\item {[What $\mid$ Which] + [NN] + [*] +?}
\item {[What $\mid$ Which] + [does $\mid$ do] + [*] + [stand for $\mid$ bind to] +? }
\item {[Which] + [TAG] + [*]+?}
\item {[Where] + [*] + [NN] + [*] + [VBZ]+ [located] +?}
\item {[When] + [TAG] + [*] +?}
\item {[Why] + [TAG] + [*] +?}
\item {[How] + [TAG] + [*] +?}
\end{enumerate}

\item \textbf{List patterns}:  We have provided the following patterns for questions which can belong to list category.
\begin{enumerate}

 \item {[What $\mid$ Which] + [VBP] + [*] + [X] + [*] +?; where X = \{numbers, names, indications, values, frequencies, prevalence, or WordNet synonyms of these words\}}
\item {[What $\mid$ Which] + [VBP] + [*]+ [NN] + [*] +?}
\item {[Which] + [TAG] + [*] +?;}
\item {[Where] + [NN] + [*] + [NN] + [VBP] + [used] +? }
\item {[When] + [TAG] + [*] +?}
\item {[How] + [TAG] + [*] +?}
\item {[Why] + [TAG] + [*] +?}
\end{enumerate}
\item \textbf{Summary patterns}: For the biomedical questions which can belong to summary category,  we have used the following patterns.
\begin{enumerate}
%\itemsep-4pt
\item {[What $\mid$ Which] + [VBZ] + [.*] + [X]+ [.*] +?; where X = \{definition, role, aim, effect, influence, mechanism, treatment, or WordNet synonyms of these words\} }
\item {[What] + [VBZ] + [NN] + [*] +?}
\item {[What] + [does] + [NN] + [*] + [do] +?}
\item {[Define $\mid$ Explain $\mid$ WordNet synonyms] + [*] + [NN] +?}
\item {[Why] + [TAG] + [*] +?}
\item {[How] + [TAG] + [*] +?}
\end{enumerate}
\end{enumerate}
\par
The key idea behind using the features provided by our set of patterns, is that only some words (e.g., Wh-question particles) in the biomedical question commonly represent the question type. Consider for example the biomedical question ``What is the definition of autophagy?'' from BioASQ training questions. The features vector (v) of this question is simply the patterns (see the first pattern of summary questions) that represent the structure of question. This question can be represented as follows: v= {{(what, 1), (VBZ, 1), (definition, 1)} where the pair is in the form (feature, frequency). Table~\ref{tab:4.2.1.2.1} shows the different feature space of the previous question.

\begin{table}[!h]
\centering
\caption{The different feature spaces of the biomedical question ``What is the definition of
autophagy?''}
\label{tab:4.2.1.2.1}
\begin{tabular}{M{4.6cm}M{10.7cm}}
\hline\noalign{\smallskip}
Feature space & Features  \\
\noalign{\smallskip}\hline\noalign{\smallskip}
Unigram& 	{(What, 1) (is, 1) (the, 1) (definition, 1) (of, 1) (autophagy, 1) (?, 1)}\\
\cmidrule(l){1-2}
Bigram &	{(What-is, 1) (is-the, 1) (the-definition, 1) (definition-of, 1) (of-autophagy, 1) (autophagy-?, 1)}\\
\cmidrule(l){1-2}
Part-of-speech&	{(WP, 1) (VBZ, 1) (DT, 1) (NN, 1) (IN, 1) (NN, 1)}\\
\cmidrule(l){1-2}
Part-of-speech + unigram &	{{(What, 1) (is, 1) (the, 1) (definition, 1) (of, 1) (autophagy, 1) (?, 1)}, {(WP, 1) (VBZ, 1) (DT, 1) (NN, 1) (IN, 1) (NN, 1)}}\\
\cmidrule(l){1-2}
\textbf{Set of patterns}	& \textbf{{(what, 1) (VBZ, 1) (definition, 1)}}\\

\noalign{\smallskip}\hline
\end{tabular}
\end{table}

We denote that biomedical questions that do not match any defined patterns are presented with their unigrams and part-of-speech tags.


The motivation to find alternative features for machine learning algorithms is the fact that words by themselves cannot capture the structure of a biomedical question. Intuitively, using our handcrafted lexico-syntactic patterns can capture the syntactic view of a biomedical question better than the other methods.
\subsection{Experimental results}

To validate the efficiency of our biomedical question type classification system, several experiments have been carried out using
the BioASQ training dataset and five batches of testing datasets that have been previously described in section~\ref{Chapter3_7_1} of chapter~\ref{Chapter3}. We have used a training set of 810 questions-answers, where one of the four types of questions: yes/no, factoid, list, summary was assigned to each question, as shown in Table~\ref{tab:3_7_1.2}. We have also used five batches of testing datasets provided in BioASQ Task 3b. Each of testing datasets is approximately comprised of 100 biomedical questions, as shown in Table~\ref{tab:3_7_1.3}. As indicators of classification effectiveness accuracy, precision, recall and F1-measure defined in equation~\ref{eq:1qc}, equation~\ref{eq:2qc}, equation~\ref{eq:3qc}, equation~\ref{eq:4qc}, respectively,  were used (cf. section~\ref{Chapter3_7_2}, chapter~\ref{Chapter3}).

For a machine-learning algorithm, we have used the multi-class SVM classifier. As a matter of fact, the linear kernel has been used for the SVM classifier since it is often recommended for text classification. Thus, the results have shown that linear kernel outperforms the other kernels such as RBF kernel, tree kernel, and composite kernel. As we have trained the SVM with a linear kernel, we only have needed to optimize the C parameter. The best value of C parameter is 1.01 which was fixed after 5-fold cross-validation.

Additionally, we have explored different features, including unigrams, bigrams, part-of-speech tags, the combination of part-of-speech tags and unigrams, and our set of handcrafted lexico-syntactic patterns. Table~\ref{tab:4.2.1.2.4} shows the SVM results in five feature models for automatically classifying a biomedical question into a category in terms of accuracy. The results show that the best method was trained on features extracted by our predefined patterns, which led to an accuracy of 89.40\%.

\begin{table}[h!]
%\scriptsize
\centering
\caption[The obtained results using SVM on five batches of testing datasets to automatically assign a category to biomedical questions]{The obtained results using SVM on five batches of testing datasets to automatically assign a category to biomedical questions. We explored different features, including unigram, bigram, part-of-speech, part-of-speech + unigram, and our set of predefined patterns.}
\label{tab:4.2.1.2.4}
\begin{tabular}{M{4.4cm}M{7cm}M{3.6cm}}
\hline\noalign{\smallskip}
Data sets&	Feature models &Accuracy (\%) \\
\noalign{\smallskip}\hline\noalign{\smallskip}
\multirow{5}{*}{Five batches} & Unigram & 79.48\\[1.5pt]
             & Bigram &  	65.18\\[1.5pt]
             &Part-of-speech&	77.08\\[1.5pt]
             &Part-of-speech+unigram&	80.08\\[1.5pt]
             &\textbf{Set of patterns}&	\textbf{89.40}\\[1.5pt]
\noalign{\smallskip}\hline
\end{tabular}
\end{table}

Using unigrams as features, the overall accuracy was 79.48\%. The results are meant to be a benchmark. We have found that other features have an impact on the performance of biomedical question type classification. Bigrams decreased the performance to 65.18\% in terms of accuracy
(an absolute decrease of 14\% accuracy). When part-of-speech tags were used as features, the overall performance decreased to 77.08\% accuracy, although the decrease was not statistically significant (an absolute decrease of 2.4\% accuracy). The incorporation of part-of-speech tags and unigrams as features improved the performance to 80.08\% accuracy.


We then experimented with our set of handcrafted lexico-syntactic patterns so as to show how well a system can perform the classification task by combining our predefined patterns and a machine-learning algorithm. We found that using our patterns features' provider of SVM leads to the highest accuracy of 89.40\%. Table~\ref{tab:4.2.1.2.5} shows the detailed results for each question category in two feature models, i.e., unigrams and set of our handcrafted lexico-syntactic patterns, on five batches of testing datasets using the SVM classifier. In two feature models, the category ``summary'' has the lowest classification performance, and ``yes/no'' has the highest one.

\begin{table}[h!]
\centering
\caption[The detailed results for each question category in two feature models (unigram and set of patterns) by applying SVM classifier]{The detailed results for each question category in two feature models (unigram and set of patterns) by applying SVM classifier. P, R, F, A indicate precision, recall, f1-measure, and accuracy, respectively.}
\label{tab:4.2.1.2.5}
\begin{tabular}{p{2.6cm}p{4cm}p{2.1cm}p{1.1cm}p{1.1cm}p{1.1cm}p{1.3cm}}
\hline\noalign{\smallskip}
Data sets &	Feature models &Class &	P (\%)	&R (\%)	&F (\%)	&A (\%) \\
\noalign{\smallskip}\hline\noalign{\smallskip}
\multirow{9}{*}{Five Batches}& \multirow{4}{*}{\parbox{3cm}{Unigram:\\Baseline model}} & Yes/No	& 95.00&	98.59&	96.70&\multirow{5}{*}{79.48}\\
                        & &Factoid  &	76.36&	66.70&	70.96\\
                         && List&	72.81&	92.28&	81.31\\
                         && Summary&	72.02&	59.91&	64.80\\
 \cmidrule(l){2-6}
                        &\multirow{4}{*}{\parbox{4cm}{Set of patterns:\\Proposed method} }& Yes/No	& 95.55&	100.0&	97.41&\multirow{5}{*}{89.40}\\
                         & &Factoid&	92.92&	80.14&	85.91\\
                         & &List&	83.27&	94.22&	88.37\\
                         & &Summary&	84.02&	80.49&	81.89\\
\noalign{\smallskip}\hline
\end{tabular}
\end{table}


\subsection{Discussion}
Overall, as shown in Table~\ref{tab:4.2.1.2.4}, the best system for automatically assigning a category to a biomedical question was trained on features extracted by our handcrafted lexico-syntactic patterns, which led to an accuracy of 89.40\%. Our results also show that feature selection impacts the biomedical question type classification performance. Using bag-of-words as features, the performance for automatically assigning a category to a question was an accuracy of 79.48\%. Thus, this feature set was chosen as a competitive baseline since it is widely used as baseline system in many question classification studies \citep{li2002learning,li2006learning,Patrick_2012,roberts2014automatically}. Bigrams decreased the performance to 65.18\% accuracy and the decrease was statistically significant (p < 0.01, t-test). Using the bigrams feature set by themselves could not capture the structure of a biomedical question in the context of this study, since the diversity in each category increased when using this feature set. For instance, for the biomedical question ``Why are insulators necessary in gene therapy vectors?'', the bigram ``why are'' is noisy, as the two words are not syntactically connected. Such noisy bigrams have a significant impact on the performance of question classification. Part-of-speech tags, on the other hand, slightly decreased the performance to 77.08\% accuracy. However, the decline was not statistically meaningful. In contrast, the combination of unigrams and part-of-speech tags as features improved the accuracy slightly (the absolute increase of 0.34\% accuracy), although the increase was not statistically significant. Meanwhile, using features extracted by our handcrafted lexico-syntactic patterns for SVM achieved the highest accuracy of 89.40\%. It outperforms the baseline system with a large margin 9.92\% in terms of accuracy and the increase is statistically significant. Inconsistency in category assignment may be responsible for the relation between the training size and the category classification performance. Typically, there is a strong positive relation between the training size and the classification performance: the larger the training size is the better a classifier performs. The Pearson correlation coefficient between F1-measure and the number of training size of a category shows an R-value of 0.83 (strong positive
correlation), which means that high F1 measure goes with high number of training data of a category (which confirms our hypothesis). We can see from Table~\ref{tab:4.2.1.2.5} and Table~\ref{tab:3_7_1.2} that the best performing category, yes/no, has the largest number of question instances (237), and the worst
performing category summary has the least number of question instances (168). Besides, \cite{metzler2005analysis} noticed that the ambiguity of labeled data has an impact on the category assignment. Unluckily, we have found that some biomedical questions in BioASQ training dataset were ambiguous. For instance, the biomedical question ``Which are the mutational hotspots of the human KRAS oncogene?'' is labeled with ``summary'' while it is also labeled by ``list'' category. Another example is the question ``Which are the newly identified DNA nucleases that can be used to treat thalassemia?'' that is labeled with ``factoid'' while it is also labeled by “list”. Biomedical question type classification can improve or decrease the performance
of an automatic biomedical QA system, because the answers extraction is based on the expected answer format of the questions. For example, extracting the answer to the question ``Which enzyme is deficient in Krabbe disease?'', which is asking for a
biomedical entity name, is not the same as extracting the answer to ``Is calcium overload involved in the development of diabetic cardiomyopathy?'', which is looking for ``yes'' or ``no'' as an answer. So, the class that can be assigned to a biomedical question affects greatly all the other steps of the QA process and therefore it is of vital importance to assign it properly. A study presented by \cite{Moldovan_2003} showed that
more than 36\% of the errors in a QA system are directly due to the question classification. In addition, the biomedical question
classification system can also improve the performance of IR systems \citep{Cao_2010}, because the question category can be used to choose the search strategy when the question is reformed to a query over IR systems. This is the case of the question ``What is the
definition of autophagy?'' from BioASQ datasets, identifying that the question category is ``summary'', the searching template
for locating the answer can be for example ``autophagy is a ...'' or ``definition of autophagy is ...'', which are much better than
simply searching by question words.
\section{Question Topic Classification}

In this section, we propose a machine learning-based method for question topic classification in biomedical QA \citep{Sarrouti_IBRA_2017}. The aim of this method is to classify clinical questions into general topics defined by the U.S. NLM. These topics are: device, diagnosis, epidemiology, etiology, history, management, pharmacological, physical, finding, procedure, prognosis, test, treatment and prevention.

\label{Chapter4.3}
\subsection{Method}

With a view to achieving the goal of classifying natural language questions into general topics and improving the performance of question topic classification in biomedical QA, we propose to incorporate the syntactic dependency relations into other features used in \citep{yu2008automatically,Cao_2010} for machine-learning approaches. The incorporation of syntactic and semantic features including words, bigrams, stemming, UMLS Metathesaurus concepts and semantic types introduced by \cite{yu2008automatically} and \cite{Cao_2010} seem to be quite enough to represent the questions. However, this method doesn't give the expected results. Intuitively, the incorporation of syntactically related pairs into these features may provide the best description of questions. A syntactic dependency relation \citep{nastase2007using} is a pair of grammatically related words in a phrase: the main verbs in two connected clauses, a verb and each of its arguments, a noun and each of its modifiers. It describes the syntactic structure of a sentence by using a typed dependency to establish relationships among words in terms of head and dependents.

The flowchart of the proposed method, as shown in Figure~\ref{fig:qc2}, is constructed through the following main steps: (1) features extraction from biomedical questions, (2) learned classifiers, and (3) predicting the class label using the trained classifiers.
% Figure environment removed


\subsubsection{Machine-learning models}

In question classification, most of the existing methods have used SVM as it leads to the best results in comparison with other classifiers \citep{yu2005classifying,yu2008automatically,Cao_2010}. Accordingly, we have experimented with SVM and other machine-learning algorithms including Naive Bayes and Decision Tree. Since a question can be assigned to one or more general topics, a multi-label classification has been used in this study. We therefore developed a binary machine learning classifier for each of the predefined topics. We have found that SVM
outperforms the others, followed by Naïve Bayes. Therefore, we have decided to report and compare the results of both SVM and Naïve Bayes. Mallet and libSVM are the freely available platforms for machine-learning algorithms that have been used in this study. We have used Mallet for Naïve Bayes and Decision Tree. For SVM, we have used libSVM.

\subsubsection{Machine-learning features}
After preprocessing the clinical questions (terms in the questions were extracted and stop words were removed), we have explored bag-of-words as features. We then have applied \cite{Porter_1980} stemmer and \cite{Krovetz_1993} algorithms for term normalisation process and used them as additional features. Next, we have extracted the syntactic dependency relations between words in the question using the Stanford parser \citep{de2006generating}, and explored them as additional features. The overall process sequence is depicted in Table~\ref{tab:4.3.1}. After that, we have generated the N-grams, proximity-based sequences of words obtained by sliding a window of size N over the question, of each question and used them as additional features. In this work, the bigrams (N-grams with $N=2$) terms were used. Finally, we have mapped the terms in questions into the UMLS Metathesaurus in order to identify concepts and their semantic types, and explored them as additional features. To do so, we have used the MetaMap tool to identify appropriate UMLS Metathesaurus concepts and semantic types in questions. Table~\ref{tab:4.3.2} illustrates an example of mapping a sample clinical question to UMLS concepts and semantic types.

\begin{table}[!h]
\centering
\caption[Linguistic preprocessing of a sample clinical question]{Linguistic preprocessing of a sample clinical question, tokenisation and part-of-speech tagging is shown in ``Step 1'': ``WP'' corresponds to a Wh-pronoun, ``VBZ'' to a verb in the third person singular present, etc. The parse tree is shown in ``Step 2'' and includes the root of the tree (ROOT), the question phrase (WHNP), several noun phrases (NP), etc. The dependencies tree is shown in ``Step 3'': ``nsubj'' is the nominal subject relation, ``cop'' refers to the relation between a complement and the copular verb, etc.}
\label{tab:4.3.1}
\begin{tabular}{M{1.7cm}M{13.7cm}}
\hline\noalign{\smallskip}

Question&What is the dose of Zithromax for this 35-kilogram kid?\\
\cmidrule(l){1-2}

Step 1 & What/\textcolor{red}{WP} is/\textcolor{red}{VBZ} the/\textcolor{red}{DT} dose/\textcolor{red}{NN} of/\textcolor{red}{IN} Zithromax/\textcolor{red}{NNP} for/\textcolor{red}{IN} this/\textcolor{red}{DT} 35-kilogram/\textcolor{red}{JJ} kid/\textcolor{red}{NN} ?/\textcolor{red}{.}
\\
\cmidrule(l){1-2}
Step 2&
\resizebox{\linewidth}{!}{%
\Tree[.\textcolor{red}{ROOT}
  [.\textcolor{red}{.SBARQ}
    [.\textcolor{red}{WHNP} [.WP What ]]
    [.\textcolor{red}{SQ} [.VBZ is ]
      [.\textcolor{red}{NP}
        [.\textcolor{red}{NP} [.DT the ] [.NN dose ]]
        [.\textcolor{red}{PP} [.IN of ]
          [.\textcolor{red}{NP}
            [.\textcolor{red}{NP} [.NNP Zithromax ]]
            [.\textcolor{red}{PP} [.IN for ]
              [.\textcolor{red}{NP} [.DT this ] [.JJ 35-kilogram ] [.NN kid ]]]]]]]]]}
 \\
 \cmidrule(l){1-2}
 Step 3 &\textcolor{red}{root}(ROOT-0, What-1)
\textcolor{red}{cop}(What-1, is-2)
\textcolor{red}{det}(dose-4, the-3)
\textcolor{red}{nsubj}(What-1, dose-4)
\textcolor{red}{case}(Zithromax-6, of-5)
\textcolor{red}{nmod:of}(dose-4, Zithromax-6)
\textcolor{red}{case}(kid-10, for-7)
\textcolor{red}{det}(kid-10, this-8)
\textcolor{red}{amod}(kid-10, 35-kilogram-9)
\textcolor{red}{nmod:for}(Zithromax-6, kid-10)\\
\noalign{\smallskip}\hline
\end{tabular}
\end{table}

%\begin{longtable}{M{1.4cm}M{11.9cm}}
%\caption{}\\
%\hline\noalign{\smallskip}

%\endfirsthead
%\multicolumn{2}{c}%
%{\tablename\ \thetable\ -- \textit{Continued from previous page}} \\
%\hline\noalign{\smallskip}

%\noalign{\smallskip}\hline
%\endhead
%\hline \multicolumn{2}{r}{\textit{Continued on next page}} \\
%\endfoot
%\noalign{\smallskip}\hline
%\endlastfoot

%\end{longtable}

\begin{table}[!h]
\centering
\caption{Example of mapping the clinical question ``Mother is alcoholic and abuses tobacco. What are statistics regarding inheritance of tobacco abuse and relationship to social situation?'' to UMLS Metathesaurus concepts and semantic types. CUI and TUI indicate concept unique identifier and type unique identifier, respectively.}
\label{tab:4.3.2}
\begin{tabular}{M{4.1cm}M{2cm}M{6.4cm}M{2cm}}
\hline\noalign{\smallskip}
UMLS concepts & UMLS CUI & UMLS semantic types& UMLS TUI  \\
\noalign{\smallskip}\hline\noalign{\smallskip}
Mother (person)&C0026591& Family Group&T099\\
\cmidrule(l){1-4}
Tobacco&C0040329&Hazardous or Poisonous Substance\newline Organic Chemical&T131\newline T109\\
\cmidrule(l){1-4}
Tobacco Use Disorder&C0040336&Mental or Behavioral Dysfunction&T048\\\cmidrule(l){1-4}
Alcoholics&C0687725&Patient or Disabled Group&T101\\\cmidrule(l){1-4}
Statistics (publications)&C0600673&Intellectual Product&T170\\\cmidrule(l){1-4}
Drug abuse&C0013146&Mental or Behavioral Dysfunction&T048\\\cmidrule(l){1-4}
Concept Relationship&C1705630&Idea or Concept&T078\\\cmidrule(l){1-4}
Mode of inheritance&C1708511&Genetic Function&T045\\\cmidrule(l){1-4}
Social situation&C0748872&Social Behavior&T054\\
\noalign{\smallskip}\hline
\end{tabular}
\end{table}

\subsection{Experimental results}


\subsubsection{Results}
To validate the efficiency of the proposed method to question topic classification in biomedical QA, several experiments have been conducted using the set of 4654 clinical questions maintained by the U.S. NLM and previously described in section~\ref{Chapter3_7} of chapter~\ref{Chapter3}. As indicators of classification effectiveness, F1-score defined in equation~\ref{eq:1qc} is used (cf. section~\ref{Chapter3_7_2}, chapter~\ref{Chapter3}), where the recall is the number of correctly predicted clinical questions divided by the total number of annotated questions in the same category, and precision is the number of correctly predicted clinical questions divided by the total number of predicted questions in the same category. We randomly select negative data to repeat the classifications ten times. We then report the average F1-scores.

We have explored supervised machine-learning algorithms to automatically classify an ad hoc clinical question written in natural language into one or more topics predefined by NLM. We have used the freely available Mallet and libSVM for supervised machine learning systems. We have experimented with three machine-learning algorithms including Naïve Bayes, Decision Tree, and SVM. We have found that SVM outperforms other classifiers, followed by Naive Bayes. Therefore, we have decided to report the results of SVM and Naive Bayes.  Indeed, the linear kernel has been used for the SVM classifier since it is often recommended for text classification. Thus, the results have shown that linear kernel outperforms the other kernels such as RBF kernel, tree kernel, and composite kernel. On the other hand, we have also compared the performance of our proposed method with different common features for machine-learning algorithms, including bag-of-words, bag-of-stems, bag-of-bigrams, bag-of-syntactic dependency relations, bag-of-UMLS concepts, bag-of-UMLS semantic types and the combination of features used by \cite{yu2008automatically,Cao_2010}.


For evaluation, we have arranged that each classifier has a baseline of 50\%. In other words, each classifier is trained on the same number of positive and negative data. For example, when we trained a binary classifier of Pharmacological, we had 1594 questions that were assigned to this category (see Table~\ref{tab:4.3.5}). This set of 1594 questions represents the positive training data. To generate negative training data, we have randomly selected 1594 questions from the remaining categories.



Table~\ref{tab:4.3.6} and Table~\ref{tab:4.3.7} show the F1-score for each topic using Naïve Bayes and SVM classifiers, respectively. We have explored different combinations of features including bag-of-words (BOW), bag-of-bigrams (BOB), bag-of-stems (BOS) using both Porter and Krovetz stemmers, bag-of-biomedical named entities (BOBNE), bag-of-syntactic dependency relations (BOSDR), bag-of-UMLS concept and semantic types (BOCST), and our combination of features, the proposed method. The proposed combination of features consists of BOW, BOB, BOS$_{porter}$, BOSDR, and BOCST. The overall results show that the best system was trained on our combination of features, which led to F-scores of 77.18\% and 71.77\% using SVM and Naïve Bayes respectively. Table~\ref{tab:4.3.8} and Table~\ref{tab:4.3.9} show the comparison of the proposed method with other combination of features for classifying clinical questions into categories. Table~\ref{tab:4.3.8} shows the increasing performance of the proposed method in comparison with other representations using Naïve Bayes as a classifier, and Table~\ref{tab:4.3.9} shows the increasing performance using the SVM classifier. These results show that our method is more effective as compared with state-of-the-art methods and outperforms them by an average of 4.5\% F1-score using Naïve Bayes and 4.73\% F1-score using SVM.



Figure~\ref{fig:topics1} and Figure~\ref{fig:topics2} show the classification performance of topic assignment in terms of F1-score as a function of training size using Naive Bayes and SVM, respectively. In both systems, the topic pharmacology has the highest classification performance (79.53\% F1-score for Naive Bayes and 84.85\% F1-score for SVM), and history has the lowest classification performance (55.71\% F1-score for Naive Bayes and 68.75\% F1-score for SVM).



\begin{landscape} % <-- HERE
\centering
\begin{table}
\centering
\caption[The obtained results in terms of F1-score using Naïve Bayes to automatically assign topics to ad hoc clinical questions]{The obtained results in terms of F1-score using Naïve Bayes to automatically assign topics to ad hoc clinical questions. We explored different combinations of feature sets including bag-of-words (BOW), bag-of-bigrams (BOB), bag-of-stems (BOS) using both Porter and Krovetz stemmers, bag-of-biomedical named entities (BOBNE), BOW+bag-of-UMLS concept and semantic types (BOCST), BOW+bag-of-syntactic dependency relations (BOSDR), BOW+BOB+BOCTY+BOS$_{porter}$ used in \citep{yu2008automatically,Cao_2010}, BOW+BOB+BOCST+BOS$_{krovetz}$, and our proposed method, the combination of BOW+BOB+BOCST+BOSDR+BOS$_{porter}$.}
\label{tab:4.3.6}
\begin{tabular}{M{4.5cm}M{1.2cm}M{1.2cm}M{1.2cm}M{1.1cm}M{1.1cm}M{1.2cm}M{1.4cm}M{1.6cm}M{1.6cm}M{1.6cm}}
\hline\noalign{\smallskip}
\multirow{4}{*}{Topics}&\multicolumn{9}{c}{Features} \\\cmidrule(l){2-11}
&&&&&&&\multicolumn{2}{c}{\thead{\cite{yu2008automatically}\\ \cite{Cao_2010}}}&\multicolumn{2}{c}{\thead{Proposed method}}\\\cmidrule(l){8-9}\cmidrule(l){10-11}
 &\multirow{2}{*}{BOW}& \multicolumn{2}{c}{BOS}& \multirow{2}{*}{BOBNE} &\multirow{2}{*}{\thead{BOW+\\BOCST}}& \multirow{2}{*}{\thead{BOW+\\BOSDR}}& \multicolumn{2}{c}{\thead{BOW+BOB+BOCST+}}& \multicolumn{2}{c}{\thead{BOW+BOB+BOSDR+BOCST+}}\\
\cmidrule(l){3-4}\cmidrule(l){8-9}\cmidrule(l){10-11}
&  &Porter& Krovetz&&&&BOS$_P$& BOS$_K$&BOS$_P$& BOS$_K$    \\


\noalign{\smallskip}\hline\noalign{\smallskip}
Device&55.13\%&56.42\%&55.85\%&55.46\%&60.45\%&56.87\%&68.73\%&68.19\%&69.47\%&68.96\%\\
Diagnosis&69.38\%&70.43\%&68.53\%&67.45\%&71.15\%&71.54\%&72.20\%&71.17\%&73.98\%&72.08\%\\
Epidemiology&65.82\%&66.88\%&64.42\%&63.12\%&67.55\%&67.65\%&70.62\%&69.14\%&71.63\%&70.40\%\\
Etiology&71.22\%&73.03\%&72.00\%&67.05\%&74.08\%&72.60\%&75.38\%&74.17\%&76.54\%&75.51\%\\
History&46.40\%&47.72\%&47.07\%&44.57\%&52.34\%&48.35\%&54.16\%&53.50\%&55.71\%&55.02\%\\
Management&60.26\%&61.35\%&60.44\%&61.08\%&63.62\%&62.05\%&65.34\%&64.39\%&66.78\%&65.83\%\\
Pharmacological&74.86\%&75.35\%&74.96\%&71.75\%&77.26\%&76.67\%&78.45\%&77.80\%&79.53\%&78.89\%\\
Physical\&Finding&67.89\%&68.58\%&67.90\%&64.89\%&69.65\%&68.74\%&73.76\%&72.59\%&74.61\%&73.50\%\\
Procedure&67.36\%&69.03\%&67.80\%&66.43\%&71.06\%&69.01\%&72.38\%&71.13\%&74.04\%&72.77\%\\
Prognosis&69.44\%&70.42\%&69.24\%&64.10\%&71.94\%&70.46\%&72.46\%&70.89\%&73.52\%&72.07\%\\
Test&72.51\%&73.05\%&72.66\%&68.32\%&75.78\%&73.85\%&76.89\%&76.18\%&78.24\%&77.53\%\\
Treatment\&Prevention&63.30\%&64.67\%&63.92\%&59.11\%&65.82\%&64.25\%&66.31\%&64.69\%&67.16\%&65.53\%\\\cmidrule(l){1-11}
Average&65.30\%&66.41\%&65.40\%&62.78\%&68.39\%&66.84\%&70.56\%&69.49\%&71.77\%&70.67\%\\

\noalign{\smallskip}\hline
\end{tabular}
\end{table}

\end{landscape} % <-- HERE

\begin{landscape} % <-- HERE
\centering
\begin{table}
\centering
\caption[The obtained results in terms of F1-score using SVM to automatically assign topics to ad hoc clinical questions]{The obtained results in terms of F1-score using SVM to automatically assign topics to ad hoc clinical questions. We explored different combinations of feature sets including bag-of-words (BOW), bag-of-bigrams (BOB), bag-of-stems (BOS) using both Porter and Krovetz stemmers, bag-of-biomedical named entities (BOBNE), BOW+bag-of-UMLS concept and semantic types (BOCST), BOW+bag-of-syntactic dependency relations (BOSDR), BOW+BOB+BOCST+BOS$_{porter}$ used in \citep{yu2008automatically,Cao_2010}, BOW+BOB+BOCST+BOS$_{krovetz}$, and our proposed method, the combination of BOW+BOB+BOCST+BOSDR+BOS$_{porter}$.}
\label{tab:4.3.7}
\begin{tabular}{M{4.5cm}M{1.2cm}M{1.2cm}M{1.2cm}M{1.1cm}M{1.1cm}M{1.2cm}M{1.4cm}M{1.6cm}M{1.6cm}M{1.6cm}}
\hline\noalign{\smallskip}
 \multirow{4}{*}{Topics}&\multicolumn{9}{c}{Features} \\\cmidrule(l){2-11}
&&&&&&&\multicolumn{2}{c}{\thead{\cite{yu2008automatically}\\ \cite{Cao_2010}}}&\multicolumn{2}{c}{\thead{Proposed method}}\\\cmidrule(l){8-9}\cmidrule(l){10-11}
 &\multirow{2}{*}{BOW}& \multicolumn{2}{c}{BOS}& \multirow{2}{*}{BOBNE} &\multirow{2}{*}{\thead{BOW+\\BOCST}}& \multirow{2}{*}{\thead{BOW+\\BOSDR}}& \multicolumn{2}{c}{\thead{BOW+BOB+BOCST+}}& \multicolumn{2}{c}{\thead{BOW+BOB+BOSDR+BOCST+}}\\
\cmidrule(l){3-4}\cmidrule(l){8-9}\cmidrule(l){10-11}
&  &Porter& Krovetz&&&&BOS$_{porter}$& BOS$_{krovetz}$&BOS$_{porter}$& BOS$_{krovetz}$    \\

\noalign{\smallskip}\hline\noalign{\smallskip}
Device&57.89\%&58.23\%&57.69\%&56.31\%&65.60\%&60.10\%&74.01\%&73.50\%&74.99\%&74.44\%\\
Diagnosis&74.22\%&74.24\%&73.14\%&70.38\%&75.63\%&75.19\%&77.13\%&75.33\%&78.10\%&77.05\%\\
Epidemiology&71.15\%&70.58\%&68.10\%&54.96\%&72.03\%&71.97\%&74.74\%&72.31\%&75.93\%&73.53\%\\
Etiology&80.31\%&80.67\%&78.64\%&75.05\%&81.02\%&80.95\%&82.47\%&80.07\%&83.11\%&81.67\%\\
History&52.72\%&55.69\%&55.03\%&50.96\%&58.57\%&54.31\%&67.18\%&66.52\%&68.75\%&68.09\%\\
Management&69.70\%&69.48\%&68.51\%&65.07\%&70.13\%&70.02\%&71.07\%&70.16\%&71.49\%&70.10\%\\
Pharmacological&82.41\%&82.83\%&82.16\%&76.20\%&84.66\%&83.04\%&84.71\%&84.04\%&84.85\%&84.19\%\\
Physical\&Finding&72.10\%&72.27\%&71.08\%&70.62\%&76.09\%&73.14\%&78.82\%&77.01\%&79.35\%&78.38\%\\
Procedure&69.56\%&70.08\%&68.81\%&68.18\%&75.47\%&71.32\%&78.68\%&77.42\%&79.12\%&78.35\%\\
Prognosis&72.68\%&73.68\%&72.13\%&69.27\%&73.89\%&72.87\%&74.03\%&72.51\%&74.17\%&73.61\%\\
Test&79.97\%&80.14\%&76.40\%&75.02\%&81.15\%&80.52\%&83.22\%&82.48\%&83.64\%&81.90\%\\
Treatment\&Prevention&68.19\%&69.00\%&67.40\%&65.16\%&69.99\%&69.21\%&71.73\%&70.10\%&72.63\%&71.91\%\\\cmidrule(l){1-11}
Average&70.91\%&71.41\%&69.92\%&66.43\%&73.68\%&71.89\%&76.48\%&75.12\%&77.18\%&76.10\%\\

\noalign{\smallskip}\hline
\end{tabular}
\end{table}

\end{landscape} % <-- HERE

\begin{table}[h!]
\centering
\caption{Comparison between the proposed representation (the combination of various features: BOW+BOB+BOCST+BOSDR+BOS$_{porter}$) and state-of-the-art representations on 4654 natural language clinical questions using Naive Bayes as a classifier in terms of F-score.}
\label{tab:4.3.8}
\begin{tabular}{M{2cm}M{1.2cm}M{1.2cm}M{1.2cm}M{1.2cm}M{1.2cm}M{1.3cm}M{1.5cm}M{1.7cm}}
\hline\noalign{\smallskip}
\multirow{2}{*}{Features}& \multirow{2}{*}{BOW}& \multicolumn{2}{c}{BOS}& \multirow{2}{*}{BOBNE} &\multirow{2}{*}{\thead{BOW+\\BOCST}}& \multirow{2}{*}{\thead{BOW+\\BOSDR}}& \multicolumn{2}{c}{\thead{BOW+BOB+ \\ BOCST+}}\\
\cmidrule(l){3-4}\cmidrule(l){8-9}
&  &Porter& Krovetz&&&&BOS$_{porter}$& BOS$_{krovetz}$    \\

\noalign{\smallskip}\hline\noalign{\smallskip}
F-score&65.30\%&66.41\%&65.40\%&62.78\%&68.39\%&66.84\%&70.56\%&69.49\%\\
Increase\newline performance&+6.47\%&+5.36\%&+6.37\%&+8.99\%&+3.83\%&+4.93\%&+1.21\%&+2.28\%\\
\noalign{\smallskip}\hline
\end{tabular}
\end{table}

\begin{table}[h!]
\centering
\caption{Comparison between the proposed representation (the combination of various features: BOW+BOB+BOCST+BOSDR+BOS$_{porter}$) and state-of-the-art representations on 4654 natural language clinical questions using SVM as a classifier in terms of F-score.}
\label{tab:4.3.9}
\begin{tabular}{M{2cm}M{1.2cm}M{1.2cm}M{1.2cm}M{1.2cm}M{1.2cm}M{1.3cm}M{1.5cm}M{1.7cm}}
\hline\noalign{\smallskip}
\multirow{2}{*}{Features}& \multirow{2}{*}{BOW}& \multicolumn{2}{c}{BOS}& \multirow{2}{*}{BOBNE} &\multirow{2}{*}{\thead{BOW+\\BOCST}}& \multirow{2}{*}{\thead{BOW+\\BOSDR}}& \multicolumn{2}{c}{\thead{BOW+BOB+ \\ BOCST+}}\\
\cmidrule(l){3-4}\cmidrule(l){8-9}
&  &Porter& Krovetz&&&&BOS$_{porter}$& BOS$_{krovetz}$    \\
\noalign{\smallskip}\hline\noalign{\smallskip}
F-score&70.91\%&71.41\%&69.92\%&66.43\%&73.68\%&71.89\%&76.48\%&75.12\%\\
Increase\newline
performance&+6.26\%&+5.74\%&+7.26\%&+10.75\%&+3.5\%&+5.29\%&+0.7\%&+2.06\%\\
\noalign{\smallskip}\hline
\end{tabular}
\end{table}

% Figure environment removed

% Figure environment removed
\subsection{Discussion}

While question classification has been widely investigated, few approaches are currently able to efficiently classify natural language questions into general topics, in particular for complex questions. This is a challenging task, particularly in a more specific domain such as the clinical domain.

Our experiments presented in Table~\ref{tab:4.3.6} and Table~\ref{tab:4.3.7} show that our SVM-based approach is promising for question topic classification in the context of biomedical QA. Our overall results confirm the findings presented in \cite{yu2008automatically,Cao_2010}, where among the multiple classification systems, the SVM-based one yielded the best results. If we compare our proposed method with the latter, we use the syntactic dependency relations as discriminative features to represent natural language clinical questions. The results
show that our representation which consists of BOW, BOB, BOCST, BOSDR, and BOS$_{porter}$ as features, is more effective as compared with state-of-the-art representations. As shown in Table~\ref{tab:4.3.6} and Table~\ref{tab:4.3.7} the average performance for assigning a topic to a natural language question was a 77.18\% F1-score using SVM and 71.77\% F1-score using Naïve Bayes, as opposed to the baseline of 50\% obtained by random guessing.

Table~\ref{tab:4.3.8} shows the increasing performance of the proposed method in comparison with state-of-the-art methods using Naïve Bayes as a classifier, and Table~\ref{tab:4.3.9} shows the increasing performance using SVM as a classifier. As shown in Table~\ref{tab:4.3.8}, using the Naïve Bayes classifier, our proposed representation outperforms state-of-the-art representations and leads to the highest F1-score of 71.77\%. It outperforms BOW with 6.47\%, BOS$_{porter}$ with 5.36\%, BOS$_{krovetz}$ with 6.37\%, BOBNE with 8.99\%, the combination of BOW and BOCST with 3.83\%, the combination of BOW, BOB, BOS$_{porter}$, BOCST that are used by \cite{yu2008automatically,Cao_2010} with 1.21\%. In the case of using SVM as a classifier, as we can see from Table~\ref{tab:4.3.9}, our proposed method still achieves higher F-score of 77.18\% as compared with the other representations. It generally outperforms BOW with a large margin 6.26\%, BOS$_{porter}$ with 5.74\%, BOS$_{krovetz}$ with 7.26\%, and the combination of BOW and BOCST with 3.5\%. Similarly, our proposed method has a better F-score performance than the method presented by \cite{yu2008automatically,Cao_2010} with an increase performance of 0.7\% F1-score.


As shown in Figure~\ref{fig:topics1} and Figure~\ref{fig:topics2}, inconsistency in topic attribution may be responsible for the topic training size of each topic and the topic classification performance. Typically, the classification performance depends directly on the training size: the larger the training size is the better a classifier performs. However, The Pearson correlation coefficient
between classification performance and the number of training size of a category shows an R-value of 0.2265 using SVM and 0.2396 using Naïve Bayes (weak correlation), which means that the relationship between classification performance and the number of training size is weak. Although we can see clearly from Figure~\ref{fig:topics1} and Figure~\ref{fig:topics2} that the best performing category, pharmacological category (84.85\% F-score using SVM and 79.75\% F1-score using Naïve Bayes), has the biggest number of question instances (1594) and the
worst performing category, history category (68.75\% F1-score using SVM and 55.71\% F1-score using Naïve Bayes), has the smallest number of question instances (43). Management category also has the highest number of question instances (1403), but it did not perform well (71.49\% F-score using SVM and 66.78\% F1-score using Naïve Bayes). Etiology category, on the other side, performs well (83.11\% F1-score using SVM
and 76.54\% F-score using Naïve Bayes) even though the number of instances available for training is small (173). We believe that Etiology is an unambiguous category for assignment. However, on the same data set, \cite{Cao_2010} have shown that the classification performance of clinical questions does not correlate with the number of categories assigned to the question.

Therefore, despite the noisy data, the obtained results show that our proposed method which is based on the incorporation of syntactic dependency relations with words, Porter stemmer, bigrams, UMLS Methasaurus concepts and semantic types, achieves good performance compared with the current state-of-the-art methods for clinical question topic classification.




\section{Summary of the Chapter}

In this chapter we have described the methodologies we proposed for question types and topic classification in biomedical QA. They were all based on machine learning approaches.

In section~\ref{Chapter4.2} we have presented in details the method we proposed for biomedical question type classification. This method which aims at assigning one of the four categories: yes/no, factoid, list, and summary to a natural language question in the biomedical domain, is based on combining both handcrafted lexico-syntactic patterns and machine-learning approaches. We have used the set of our handcrafted lexico-syntactic patterns to extract appropriate features for machine learning algorithms. We have experimented with several commonly used machine-learning approaches for question classification, including Naïve Bayes, Decision Tree, and SVM, and the obtained results have shown that SVM performed the best on the dataset made available as a part of the BioASQ challenge. We have also conducted experiments with different feature sets and best results were obtained using our handcrafted lexico-syntactic patterns. The predefined patterns yielding the best results are also made available which encourage replication of results (cf. section~\ref{Chapter4.2.1.2}).


In section~\ref{Chapter4.3} we have explained in details the methodology we proposed to question topic classification for the purpose of supporting automatic retrieval of clinical answers. This method aims at assigning one or more general topics to clinical questions written in natural language. It is based on various features including words, word stems, bigrams, UMLS concepts and semantic types, and syntactic dependency relations between pair words. We have explored several machine learning algorithms such as Naïve Bayes, Decision Tree, and SVM, showing SVM achieved the best results for this task on the annotated data that is released by NLM. We have also conducted experiments with different feature sets and best results were obtained using our combination of features which consists of bag-of-words, bag-of-Porter stemmer stems, bag-of-bigrams, bag-of-UMLS concepts and semantic types, and bag-of-syntactic dependency relations.

