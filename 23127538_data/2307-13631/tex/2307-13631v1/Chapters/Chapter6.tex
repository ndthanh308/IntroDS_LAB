% Chapter 1

\chapter{Answer Extraction and End-to-End Biomedical Question Answering System SemBioNLQA} % Main chapter title

\label{Chapter6} % For referencing the chapter elsewhere, use \ref{Chapter1}
\setcounter{secnumdepth}{4}
\minitoc

This chapter presents the methods we propose for answer extraction in biomedical QA, a key task that is studied and evaluated separately. Section~\ref{Chapter6.2} will present the proposed methods for answer extraction in biomedical QA. We consecrate section~\ref{Chapter6.3} to our proposed biomedical QA system named SemBioNLQA.

\section{Introduction}
\label{Chapter6.1}

Answer extraction is usually the last component in a typical QA pipeline as the final step towards developing biomedical QA systems is processing the set of relevant passages so as to extract the final answers. Answer extraction is the most challenging task of a biomedical QA system since this is when the precise answer has to be extracted from the candidate answers retrieved and selected by the passage retrieval component. The output from the answer extraction component is a specific answer like ``Pthirus pubis'' to the biomedical question ``What is the cause of Phthiriasis Palpebrarum?''. In general, the appropriate answers to the users questions should be extracted according to the type of the given question. A biomedical question like ``Does the histidine-rich Ca-binding protein (HRC) interact with triadin?'' expects an answer of type ``yes'' or ``no''. A biomedical question like ``What is the role of edaravone in traumatic brain injury?'' expects an answer of type ``summary''. In this context, the most recent taxonomy of biomedical questions that is created by the BioASQ challenge \citep{tsatsaronis2012bioasq} consists of four types of questions which may cover all kinds of potential questions:

\begin{enumerate}
  \item Yes/No questions: They require only one of the two possible answers: ``yes'' or ``no''. For example, ``Is calcium overload involved in the development of diabetic cardiomyopathy?'' is a yes/no question and the answer is ``yes''.
  \item Factoid questions: They require a particular entity name (e.g., of a disease, drug, or gene), a number, or a similar short expression as an answer. For example, ``Which enzyme is deficient in Krabbe disease?'' is a factoid question and the answer is a single entity name ``galactocerebrosidase''.
  \item List questions: They expect a list of entity names (e.g., a list of gene names,  list of drug names), numbers, or similar short expressions as an answer. For example, ``What are the effects of depleting protein km23â€“1 (DYNLRB1) in a cell?'' is a list question.
  \item Summary questions: They expect a summary or short passage in return. For example, the expected answer format for the question ``What is the function of the viral KP4 protein?'' should be a short text summary.
\end{enumerate}

Since the launch of the biomedical QA track at the BioASQ challenge, theories and methods in biomedical QA continue to evolve to better meet the needs of users questions, thanks to the many editions of the BioASQ challenge. In Phase B, Task b of the BioASQ challenge, participants were asked to answer with the exact answers and the ideal answers (i.e, paragraph-sized summaries). Exact answers are only required in the case of yes/no, factoid, list, while ideal answers are expected to be returned for all biomedical questions. In this context, \cite{yang2015learning} have described a learning-based method for biomedical QA that returns only the exact answers for factoid and list questions. Similarly, \cite{peng2015fudan}} have developed a biomedical QA system that retrieves solely the exact answers for factoid and list questions. The system first used PubTator \citep{Wei_2013} for generating the candidate answers and then ranked them based on their frequency in relevant documents and snippets. \cite{choi2015snumedinfo}, on the other hand, has presented a biomedical QA system which retrieves only the ideal answers for each given question. Meanwhile, \cite{neves2015hpi} has proposed a biomedical QA system named HPI based on the IMDB database and its built-in text analysis features to generate both the exact and the ideal answers for biomedical questions. \cite{schulze2016hpi}, the winning team of the 2016 BioASQ challenge, have presented a biomedical QA system based on the LexRank algorithm \citep{erkan2004lexrank} to retrieve only the ideal answers to biomedical questions. Although the aforementioned systems have proven to be quite successful at answering biomedical questions, biomedical answer extraction still requires further efforts in order to improve its performance as the most of the aforementioned systems do not deal with all question and answer types. For instance, only few answer extraction methods for biomedical yes/no questions have been presented, compared to other question types such as factoid, list, and summary. The most recent yes/no biomedical answer extraction method is the one which was presented by \cite{neves2015hpi}. The author has made a decision on either the answer is ``yes'' or ``no'' based on the sentiment analysis predictions provided by the IMDB technology.


On the other hand, despite the importance of answering biomedical questions, until recently there are only few integral
biomedical QA systems such as the ones described in \citep{lee2006beyond,cruchet2009trust,gobeill2009question,Cao_2011,abacha2015means,Kraus_2017} that can retrieve answers to biomedical questions written in natural language. While these systems have proven to be quite successful at answering biomedical questions, they provide a limited amount of question and answer types (cf. Table~\ref{tab:3.1c}), for instance, most of them  \citep{lee2006beyond,cruchet2009trust,Cao_2011,Kraus_2017} only handle definition questions or returns solely short summaries as answers for all types of questions, and the other ones do not deal with yes/no questions which are one of the most complicated question types to answer as they are seeking for a clear ``yes'' or ``no'' answer. Furthermore, such systems still require further efforts in order to improve their performance in terms of precision to currently supported question and answer types.

In this thesis work, compared to the aforementioned systems, our ultimate goal is to develop a biomedical QA system that is able to accept a variety of natural language questions and to generate appropriate natural language answers. We will present in section~\ref{Chapter6.2} the methods we propose for the extraction of the answers to biomedical questions including yes/no questions, factoid questions, list questions, and summary questions. In section~\ref{Chapter6.3} we will present our fully automated system SemBioNLQA - Semantic Biomedical Natural Language Question Answering - which has the ability to handle the aforementioned questions that are commonly asked in the biomedical domain. SemBioNLQA is derived from our previously established methods in (1) question classification (2) document retrieval, (3) passage retrieval, and (4) answer extraction components.


\section{Answer Extraction}
\label{Chapter6.2}
\subsection{Methods}

As our goal is to develop a biomedical QA system which has the ability to deal with four types of questions (i.e., yes/no questions,
factoid questions, list questions, and summary questions), therefore, we developed novel answer extraction methods for each question type. The proposed biomedical QA system will be able to retrieve quickly user' information needs by returning exact answers (e.g., ``yes'', ``no'', a biomedical entity name, etc.) and ideal answers (i.e., paragraph-sized summaries of relevant information) for yes/no, factoid and list questions,
whereas it only provides the ideal answers for summary questions. In this section, we describe in details the methods we propose for each of the aforementioned questions \citep{Sarrouti_yes_2017,Sarrouti_bioasq_2017}.

\subsubsection{Yes/no questions}

Yes/no questions, these are questions that require either ``yes'' or ``no'' as exact answer like ``yes'' to the biomedical yes/no question ``Does the CTCF protein co-localize with cohesin?''. Even though there are only two possible answers, ``yes'' or ``no,'' such questions can be quite hard to answer due to the complicated sentiment analysis process of the candidate answer passages. In this thesis work, in order to answer yes/no questions, we first used the Stanford CoreNLP tools for tokenization and part-of-speech tagging one by one the $N$ retrieved candidate passages answers $(p_1, p_2, ... , p_{n})$. Then, each word of the candidate answer passages is assigned its SENTIWORDNET score. SENTIWORDNET \footnote{SENTIWORDNET: \url{http://sentiwordnet.isti.cnr.it/}} which is a lexical resource for sentiment analysis and opinion mining \citep{baccianella2010sentiwordnet}, assigns to each synset of WordNet three sentiment scores: ``positivity'', ``negativity'', ``objectivity''. Assuming that there are $k$ words (w) in a candidate answer passage $p$, the final sentiment score (SC) of the candidate answer passage is defined by the following equation:

\begin{equation}\label{eq:1}
SC \;(p)= \sum_{i=1}^{k} SentiWordNet \; (w_i)
\end{equation}

The key idea behind using the SentiWordNet 3.0 lexical resource is that it is the result of the automatic annotation of all the synsets of WordNet 3.0 according to the notions of ``positivity,'' ``negativity,'' and ``neutrality''. The WordNet which is a large lexical database of English, is comprised of 155,287 words and 117,659 synsets, also called synonyms \citep{Miller_1995}. Furthermore, \cite{marchand2013lvic} have shown that SentiWordNet 3.0 outperforms other sentiment lexicons in the determination of the polarity, such as Bing Liu's Opinion Lexicon \citep{Hu_2004} and MPQA Subjectivity Lexicon \citep{Wilson_2005}.


Finally, the decision to output ``yes'' or ``no'' depends on the number of positive or negative candidate answers: ``yes'' for a positive final sentiment candidate answers score and ``no'' for a negative one. Algorithm~\ref{alg:2} further illustrates how the proposed yes/no answer extraction method works.


\begin{algorithm}[h!]
\caption{Biomedical yes/no answer generator}
\label{alg:2}
\begin{algorithmic}[1]
%\algnewcommand\INPUT{\item[\textbf{Input:}]}%
\State $\textbf{Input} : {yes/no\; question \;Q\; and\; set\;of\;candidate\;answers\; P}$
\State $\textbf{Output} : {answer: \;``yes"\;or\; ``no"}$
\State $postive \leftarrow {0}$ \Comment{number of positive candidate answer passages}
\State $negative \leftarrow {0}$ \Comment{number of negative candidate answer passages}
\State $i \leftarrow {1}$
\Function{PreProcessing}{$p: condidate\;answer\;passage$}
\State $TOKEN [1...m]\leftarrow \Call{TokenizationAndPOSTagging}{$p$}$
\State \Return $TOKEN$
\EndFunction

\Do
  \State $W[1...m] \leftarrow  \Call{PreProcessing} {P[i]}$ \Comment{get a set of words and their POS tags of a candidate answer}
  \State $ score \leftarrow 0.0 $
  \State $j \leftarrow {1}$
  \Do
    \State $score \gets score+ \Call{SentiWordNet} {$W[j]$}$
    \State $j \gets j+1$

  \doWhile{($j\leq m$)} \Comment{m is the size of the set of words W}
   \If{($score \geq 0$)}
        \State $positive \gets positive+1$
   \Else
   \State $negative \gets negative+1$
   \EndIf
\State $i \gets i+1$
\doWhile{($i\leq np$)} \Comment{np is number of candidate answers}

\If{($positive \geq negative$)} \Comment{the decision for the answers ``yes'' or ``no'' is based on the number of positive and negative candidate answers}
        \State $output \gets ``yes"$
\Else
 \State $output \gets ``no"$
\EndIf
\end{algorithmic}
\end{algorithm}

Figure~\ref{fig:sc} shows an example of the whole process, i.e, tokenization, part-of-speech tagging, and sentiment score assignment for a candidate answer passage of the biomedical yes/no question ``Does the CTCF protein co-localize with cohesin?'' from the BioASQ training datasets.

% Figure environment removed

\subsubsection{Factoid questions}

Factoid questions are the questions that expect a particular entity name (e.g., of a disease, drug, or gene), a number, or a similar short expression as an answer like ``Cysteine'' to the biomedical factoid question ``Which amino acid residue appears mutated in most of the cases reported with  cadasil syndrome?''.

To achieve the goal of answering factoid questions in our proposed biomedical QA system, we have proposed a factoid answer extraction method based on UMLS metathesaurus, BioPortal synonyms and the term frequency metric. We first have mapped the $N$ candidate answers $(p_1, p_2, ... , p_{n})$ of a given biomedical factoid question to the UMLS metathesaurus (2016AA knowledge source) using the MetaMap program so as to extract the set of biomedical entity names $Es$. We then have ranked the obtained set of biomedical entity names based on the term frequency metric $TF(e_i, Es)$, the number of times entity name $e_i$ appeared in the set of biomedical entity names $Es$. We have explored several term weighting methods such as TFIDF and BM25, showing term frequency achieved the best result for this task. We speculate that the answers are located in the first and second candidate answers. Next, synonyms for each of the $T$ top-ranked entity names are retrieved using Web services from BioPortal\footnote{\url{http://data.bioontology.org/documentation}}. Finally, the $T$ top-ranked biomedical entity names and their $T$ top synonyms are displayed as answers, excluding entities also mentioned in the question. The idea behind excluding entities mentioned in the question is that after analysing the training set of questions and answers released by the BioASQ organizers, we found that the most entities that appear in questions are not part of the answers. For example, the answer of the factoid question ``What is the name of Bruton's tyrosine kinase inhibitor that can be used for treatment of chronic lymphocytic leukemia?'' which contains several entities (e.g., ``Chronic Lymphocytic Leukemia''), is ``Ibrutinib'' which is not part of the question entities. As described by the BioASQ challenge, a factoid question has one correct answer, but up to five candidate answers and their synonyms are allowed. Figure~\ref{fig:mapping} shows an example of the whole process, i.e, mapping to UMLS metathesaurus, and synonyms extraction for a candidate answer of the factoid question ``Which type of lung cancer is afatinib used for?''.

% Figure environment removed


\subsubsection{List questions}

List questions are the questions which require a list of entity names (e.g., a list of gene names,  list of drug names), numbers, or similar short expressions as an answer like ``bortezomib'', ``vincristine'', ``doxorubicin'', ``etoposide'', ``cisplatin'', ``fludarabine'', and ``SD-1029 Stat3 inhibitor'' to the biomedical question ``Which drugs have been found effective for the treatment of chordoma?''. The main difference between factoid and list questions is that the former require a single list of answers while the latter expect a list of lists of entity names, numbers, or similar short expressions. As it is shown by the BioASQ challenge, each entity may be accompanied by a list of synonyms. Therefore, a list of entities should be provided for each question by the proposed biomedical QA system. In other words, the exact answer is the same of factoid questions, but the interpretation is different for list questions: All $T$ top-ranked entities are considered part of the same answer for the list question, not as candidates. The proposed method used to answer list questions in our system is similar to the one described for factoid questions.


\subsubsection{Summary questions}

Summary questions are the questions which expect a summary or short passage in return like ``\emph{The histidine-rich Ca-binding protein (HRC), a 165 kDa sarcoplasmic reticulum (SR) protein, regulates SR Ca cycling during excitation contraction coupling.  HRC mutations or polymorphisms lead to cardiac dysfunction.  The Ser96Ala genetic variant of HRC is associated with life-threatening ventricular arrhythmias and sudden death in idiopathic dilated cardiomyopathy (DCM)}'' to the biomedical question ``What is the role of the histidine rich calcium binding protein (HRC) in cardiomyopathy?''. As summary questions do not require exact answers, therefore, they are simply answered in this thesis work by formulating short summaries, i.e., ideal answers, of relevant information. For the given biomedical questions, the ideal answers are formed by concatenating the two top-ranked candidate answers which were retrieved by the proposed passage retrieval approach (cf. section~\ref{Chapter5.3}). We first have forwarded abstracts of the $N$ relevant documents of a given biomedical question to Stanford CoreNLP sentence splitter so as to segment them into sentences. We then have preprocessed the obtained set of candidate answers including tokenization, removing stop words, and applying Porter' stemmer to extract stemmed words. Additionally, we have used the MetaMap program for mapping both biomedical questions and candidate passages to UMLS concepts in order to extract biomedical concepts. Moreover, the MetaMap word sense disambiguation system has been used to resolve ambiguities in the texts by identifying the meaning of ambiguous terms. Using stemmed words and UMLS concepts as features, we finally ranked the set of candidate answers using BM25 as retrieval model, and concatenated the two top-ranked candidate answer passages.

In particular, in addition to the \emph{exact answers} returned for previous questions, i.e., yes/no questions, factoid questions and list questions, we also provide \emph{ideal answers}. Therefore, our proposed biomedical QA system provides both \emph{exact answers} and the \emph{ideal answers} for yes/no, factoid and list questions, whereas it only provides \emph{ideal answers} for summary questions.

\subsection{Experimental results}

In order to assess the effectiveness of the methods we proposed for biomedical answer extraction and compare with the current state-of-the-art methods, we performed several experiments on large standard datasets provided by the BioASQ challenge. We have used the test sets of biomedical questions provided in BioASQ Task 3b 2015, BioASQ Task 4b 2016, and BioASQ Task 5b 2017 described in subsection~\ref{Chapter3_7_1}, section~\ref{Chapter3_7}, chapter~\ref{Chapter3}. Moreover, we have participated in the 2017 BioASQ challenge (BioASQ Task 5b 2017).

The BioASQ challenge in phase B of Task 3b/4b/5b provides the test set of biomedical questions along with their golden documents, golden snippets, and questions types, i.e., whether yes/no, factoid, list or summary in order to evaluate biomedical answer extraction approaches in their best way. Given biomedical question and its golden passages, each participating system may return an ideal answer, i.e., a paragraph-sized summary of relevant information. In the case of yes/no, factoid, and list questions, the systems may also return exact answers; for summary questions, no exact answers will be returned. Accordingly, we have relied on the gold-standard passages provided by the BioASQ challenge, instead of the ones retrieved by the system to evaluate our answer extraction methods.

As indicators of answer extraction effectiveness: Accuracy was used for exact answers of yes/questions; mean reciprocal rank (MRR) was used for exact answer of factoid questions; mean average precision, mean average recall, and mean average f1-measure were used for exact answers of list questions; ROUGE-2 and ROUGE-SU4 were used for ideal answers. These evaluation metrics are described in details in subsection~\ref{Chapter3_7_2}, section~\ref{Chapter3_7}, chapter~\ref{Chapter3}. Additionally, the BioASQ challenge have also developed an online evaluation system\footnote{BioASQ evaluation system: \url{http://participants-area.bioasq.org/oracle/}} that allows uploading JSON result files and obtaining evaluations results at any time. Table~\ref{tab:6.1} and Table~\ref{tab:6.2} show the experimental results of the
proposed answer extraction methods and comparison with the state-of-the-art studies presented in \citep{zhang2015fudan,neves2015hpi,choi2015snumedinfo,yang2015learning,schulze2016hpi} on five batches of testing datasets provided by the BioASQ challenge in 2015 and 2016, respectively. In addition, Table~\ref{tab:6.3} and Table~\ref{tab:6.4} present the results of our participation in Phase B, Task 5b of the 2017 BioASQ challenge using our biomedical answer extraction system. The values inside parameters indicate our current rank, the total number of submissions, and the total number of participated teams for the task. Our system name for submission was ``sarrouti''.



\begin{table}[h!]
\centering
\caption[The overall results of the proposed biomedical answer extraction methods and comparison with the current state-of-the-art
methods on five batches of testing datasets provided by BioASQ 3b 2015.]{The overall results of the proposed biomedical answer extraction methods and comparison with the current state-of-the-art methods on five batches of testing datasets provided by BioASQ 3b 2015. The ``-'' replace the scores of systems that did not evaluate on this batch or did not deal with this task, while ``nr'' indicated that the results are not reported for this evaluation measure. Acc, P, R, F, R-2, R-SU4 indicate accuracy, precision, recall, and f-measure, rouge-2, rouge-SU2, respectively.}
\label{tab:6.1}
\begin{tabular}{p{1.6cm}p{3.6cm}p{1.1cm}p{1cm}p{1cm}p{1cm}p{1cm}p{1cm}p{1.2cm}}
\hline\noalign{\smallskip}
 \multirow{3}{*}{Datasets} &\multirow{3}{*}{System name}& \multicolumn{5}{c}{Exact answers} &  \multicolumn{2}{c}{\multirow{2}{*}{Idial answers}}\\
\cmidrule(l){3-7}

 & & Yes/No & Factoid & \multicolumn{3}{c}{ List}  &\multicolumn{2}{c}{}   \\
 \cmidrule(l){3-3}\cmidrule(l){4-4} \cmidrule(l){5-7} \cmidrule(l){8-9}
  & &  Acc &  MRR& P& R &F1  & R-2& R-SU4\\


\noalign{\smallskip}\hline\noalign{\smallskip}
\multirow{4}{*}{Batch 1} &	Our system & 0.6970&	0.1692	& 0.1545&	0.2409&	0.1830& 0.2716&	0.2860\\
            &\cite{zhang2015fudan}&-&	0.1423	& nr & nr & 0.0756& -& -\\
            &\cite{neves2015hpi}&0.6667&	-	& 0.0292& 0.0603& 0.0364& 0.1884& 0.2008\\
            &\cite{choi2015snumedinfo}&-&	-	& -& -& -& -& 0.3071\\
\cmidrule(l){1-9}
\multirow{4}{*}{Batch 2} &	Our system &0.6250&	0.1776	& 0.1929&	0.2714&	0.2127& 0.3123&	0.3364\\
           &\cite{zhang2015fudan}&-&	0.0859	& nr&nr& 0.1160& -& -\\
            &\cite{neves2015hpi}&0.5625&	-	& 0.0714& 0.0161& 0.0262& 0.2026& 0.2227\\
            &\cite{choi2015snumedinfo}&-&	-	& -& -& -& -& 0.3710\\
            \cmidrule(l){1-9}
\multirow{4}{*}{Batch 3} &	Our system &0.8621&	0.1840	& 0.2353&	0.2927&	0.2524& 0.3879&	0.4078\\
            &\cite{zhang2015fudan}&-&	0.0846	& nr& nr& 0.1319& -& -\\

            &\cite{yang2015learning}&-&	0.1615&	0.0539&  0.6933 & 0.0969 & -& -\\

            &\cite{neves2015hpi}&0.6207&	-	& -& -& -& 0.1934& 0.2189\\
            &\cite{choi2015snumedinfo}&-&	-	& -& -& -& -& 0.3941\\
            \cmidrule(l){1-9}
\multirow{4}{*}{Batch 4} &	Our system &0.7600&	0.2960	& 0.2783&	0.2713&	0.2588& 0.3917&	0.4108\\
           &\cite{zhang2015fudan}&-&	0.2299	& nr& nr& 0.2192& -& -\\

           &\cite{yang2015learning}&-&	0.5155&	0.3836&  0.3480 & 0.3168 & -& -\\

            &\cite{neves2015hpi}&0.5600&	0.0345	& 0.1522& 0.0473& 0.0689& 0.2504& 0.2724\\
            &\cite{choi2015snumedinfo}&-&	-	& -& -& -& -& 0.3906\\
            \cmidrule(l){1-9}
\multirow{4}{*}{Batch 5} &	Our system &0.6071&	0.1568	& 0.0583& 0.0736& 0.0625& 0.3440&	0.3533\\
            &\cite{zhang2015fudan}&-&	0.2500	& nr& nr& 0.1340& -& -\\

            &\cite{yang2015learning}&-&	0.2727 &	0.1704&  0.2573 & 0.1875 & -& -\\

            &\cite{neves2015hpi}&0.3571&	0.0909	& 0.0625& 0.0292& 0.0397& 0.1694& 0.1790\\
            &\cite{choi2015snumedinfo}&-&	-	& -& -& -& -& 0.3665\\
\noalign{\smallskip}\hline
\end{tabular}
\end{table}


\begin{table}[h!]
\centering
\caption[The overall results of the proposed biomedical answer extraction methods and comparison with the current state-of-the-art
methods on five batches of testing datasets provided by BioASQ 4b 2016]{The overall results of the proposed biomedical answer extraction methods and comparison with the current state-of-the-art methods on five batches of testing datasets provided by BioASQ 4b 2016. The ``-'' replace the scores of systems that did not deal with this task, while ``nr'' indicated that the results are not reported for this evaluation measure. Acc, P, R, F, R-2, R-SU4 indicate accuracy, precision, recall, and f-measure, rouge-2, rouge-SU2, respectively.}
\label{tab:6.2}
\begin{tabular}{p{1.6cm}p{3.6cm}p{1.1cm}p{1cm}p{1cm}p{1cm}p{1cm}p{1cm}p{1.2cm}}
\hline\noalign{\smallskip}
 \multirow{3}{*}{Datasets} &\multirow{3}{*}{System name}& \multicolumn{5}{c}{Exact answers} &  \multicolumn{2}{c}{\multirow{2}{*}{Idial answers}}\\
\cmidrule(l){3-7}

 & & Yes/No & Factoid & \multicolumn{3}{c}{ List}  &\multicolumn{2}{c}{}   \\
 \cmidrule(l){3-3}\cmidrule(l){4-4} \cmidrule(l){5-7} \cmidrule(l){8-9}
  & &  Acc &  MRR& P& R &F1  & R-2& R-SU4\\


\noalign{\smallskip}\hline\noalign{\smallskip}
\multirow{2}{*}{Batch 1} &	Our system &0.8214&	0.0726	& 0.2182&	0.3939&	0.2756& 0.4772&	0.4918\\
            &\cite{schulze2016hpi}&-&	-	& -& -& -& nr& 0.2231\\

\cmidrule(l){1-9}
\multirow{2}{*}{Batch 2} &	Our system &0.8750&	0.1452	& 0.2381&	0.2505&	0.2349& 0.5021&	0.5115\\
             &\cite{schulze2016hpi}&-&	-	& -& -& -& nr&0.2240 \\

            \cmidrule(l){1-9}
\multirow{2}{*}{Batch 3} &	Our system &0.8400&	0.1218	& 0.2381&	0.3627&	0.2812& 0.4978&	0.5061\\
            &\cite{schulze2016hpi}&-&	-	& -& -& -& nr& 0.2559\\

            \cmidrule(l){1-9}
\multirow{2}{*}{Batch 4} &	Our system &0.8095&0.1129&	0.1467&	0.2231&	0.1702& 0.5192&	0.5231\\
            &\cite{schulze2016hpi}&-&	-	& -& -& -& nr& 0.2280\\

            \cmidrule(l){1-9}
\multirow{3}{*}{Batch 5} &	Our system &0.8148& 0.1136&	0.1900&	0.2353&	0.1963& 0.4979&	0.5027\\
            &\cite{schulze2016hpi}&-&	-	& -& -& -& nr& 0.3233\\


\noalign{\smallskip}\hline
\end{tabular}
\end{table}




\begin{table}[h!]
\centering
\caption[The obtained results of our participation in ``Exact Answers'', Phase B, Task 5b of the 2017 BioASQ challenge using the proposed answer extraction methods]{The obtained results of our participation in ``Exact Answers'', Phase B, Task 5b of the 2017 BioASQ challenge using the proposed answer extraction methods. The first value inside parameters indicates our current rank and the total number
of submissions for the task, while the second indicates our current rank and the total number of participated teams.}
\label{tab:6.3}
\begin{tabular}{p{2cm}p{2.6cm}p{2.6cm}p{2cm}p{2cm}p{2.5cm}}
\hline\noalign{\smallskip}
 \multirow{2}{*}{Datasets} &  Yes/No & Factoid & \multicolumn{3}{c}{ List}   \\
 \cmidrule(l){2-2}\cmidrule(l){3-3} \cmidrule(l){4-6}
  &   Accuracy &  MRR& Precision& Recall &F-measure \\


\noalign{\smallskip}\hline\noalign{\smallskip}
\multirow{2}{*}{Batch 1}  &0.7647&	0.2033 (5/15)\newline (3/9)&	0.1909&	0.2658&	0.2129 (3/15)\newline (2/9) \\

\cmidrule(l){1-6}
\multirow{2}{*}{Batch 2}  &0.7778& 0.0887 (10/21)\newline (5/9)&	0.2400&	0.3922&	0.2920 (6/21)\newline (2/9)\\

            \cmidrule(l){1-6}
\multirow{2}{*}{Batch 3}  &0.8387 (1/21)\newline (1/10)& 0.2212 (9/21)\newline (4/10)&	0.2000&	0.4151&	0.2640 (6/21)\newline (3/10)\\


            \cmidrule(l){1-6}
\multirow{2}{*}{Batch 4} &0.6207 (2/27)\newline (2/11)& 0.0970 (13/27)\newline (5/11)&	0.1077&	0.2013&	0.1369 (12/27)\newline (5/11)\\


            \cmidrule(l){1-6}
\multirow{3}{*}{Batch 5} &0.4615 & 0.2071 (9/25)\newline (3/11)&	0.2091&	0.3087&	0.2438 (11/25)\newline (6/11)\\



\noalign{\smallskip}\hline
\end{tabular}
\end{table}

\begin{table}[h!]
\centering
\caption[The obtained results of our participation in ``Ideal Answers'', Phase B, Task 5b of the 2017 BioASQ challenge using the proposed answer extraction methods]{The obtained results of our participation in ``Ideal Answers'', Phase B, Task 5b of the 2017 BioASQ challenge using the proposed answer extraction methods. The first value inside parameters indicates our current rank and the total number
of submissions for the task, while the second indicates our current rank and the total number
of participated teams.}
\label{tab:6.4}
\begin{tabular}{p{2.1cm}p{1.7cm}p{2.2cm}p{2.2cm}p{1.3cm}p{1.9cm}p{1.9cm}}
\hline\noalign{\smallskip}
 \multirow{2}{*}{Datasets} & \multicolumn{2}{c}{Automatic scores} &  \multicolumn{4}{c}{Manual scores}\\
\cmidrule(l){2-3}\cmidrule(l){4-7}

 & Rouge-2& Rouge-SU4 & Readability&  Recall& Precision& Repetition \\


\noalign{\smallskip}\hline\noalign{\smallskip}
\multirow{2}{*}{Batch 1}  &0.4943 \newline(4/15)\newline(2/9)&0.5108\newline (3/15)\newline(2/9)&3.65 \newline(3/15)\newline(2/9)&	4.42 \newline (3/15)\newline(2/9)&	3.90 \newline(3/15)\newline(2/9)&	3.89\newline (3/15)\newline(2/9)\\

\cmidrule(l){1-7}
\multirow{2}{*}{Batch 2}  & 0.4579 \newline(4/21)\newline (2/9)& 0.4583\newline (4/21)\newline (2/9)&3.68\newline (6/21)\newline (2/9)&	4.59 \newline (2/21)\newline (2/9)&	4.01\newline (7/21)\newline (2/9)&	3.91 \newline(7/21)\newline (2/9)\\

            \cmidrule(l){1-7}
\multirow{2}{*}{Batch 3}  & 0.5566 \newline(4/21)\newline (2/10)&	0.5656 \newline(4/21)\newline (2/10)&3.91\newline (6/21)\newline (2/10)&	4.64 \newline (1/21)\newline (2/10)&	4.07 \newline(7/21)\newline (2/10)&	4.00 \newline(7/21)\newline (2/10)\\


            \cmidrule(l){1-7}
\multirow{2}{*}{Batch 4} &0.5895 \newline(4/27)\newline (2/11)&	0.5832 \newline(4/27)\newline (2/11)&3.86\newline (7/27)\newline (3/11)&	4.51 \newline (6/27)\newline (3/11)&	4.02 (3/27)\newline (2/11)&	3.95 \newline(6/27)\newline (2/11)\\


            \cmidrule(l){1-7}
\multirow{3}{*}{Batch 5} & 0.5772 \newline(7/25)\newline (3/11)&	0.5756\newline (7/25)\newline (3/11)&3.82\newline (8/25)\newline (3/11)&	4.53 \newline(5/25)\newline (2/11)	&3.91\newline (7/25)\newline (3/11)&	3.90 \newline(7/25)\newline (2/11)\\



\noalign{\smallskip}\hline
\end{tabular}
\end{table}


The proposed answer extraction methods provide both the exact and ideal answer to biomedical questions. For yes/no questions, the exact answer (``yes'' or ``no'') is formed by using SentiWordNet, a sentiment lexicon. Each word of the relevant snippets is assigned its SentiWordNet score, and the decision to output ``yes'' or ``no'' depends on the number of positive or negative snippets. For factoid questions, the exact answer is produced by identifying the biomedical entities that occur in the given top relevant snippets of the question, and reporting the five most frequent biomedical entities and their synonyms of the top snippets, excluding entities also mentioned in the question. For list questions, the exact answer is produced in the same manner, except that the most frequent entities and their synonyms of the top relevant snippets are now returned as the single list that answers the question. Note that in contrast to the 2015 and 2016 BioASQ challenges, biomedical QA systems are no longer allowed to provide an own list of synonyms in the 2017 challenge. On the other hand, to generate ideal answers (summaries),
we have applied MetaMap to the relevant snippets and the question, in order to obtain the UMLS concepts they refer to. We then have ranked the snippets by their BM25 similarity to the question (using Porter stems and UMLS concepts as features), and return the top two most highly ranked snippets (concatenated) as the ideal answer.

From the overall results, it can be seen that in each of the BioASQ challenges, the proposed biomedical answer extraction methods were very competitive and performed well in all challenges ranking within the top tier teams. Moreover, our system was one of the winners\footnote{Fifth BioASQ challenge winners:\url{http://www.bioasq.org/participate/fifth-challenge-winners}} in the 2017 edition of the BioASQ challenge. 


%% Figure environment removed


\subsection{Discussion}

Although open-domain QA is a longstanding challenge widely studied over the last decades, few systems are currently able to handle a variety of natural language questions and to generate the appropriate answers.  In this thesis work, we have proposed answer extraction methods in biomedical QA to handle the kinds of yes/no questions, factoid questions, list questions, and summary questions. Our methods are able to provide exact answers and paragraph-sized ideal answers (summaries of relevant information) for yes/no, factoid and list questions, whereas they only retrieve ideal answers for summary questions.

The experimental results detailed in Table~\ref{tab:6.1} and Table~\ref{tab:6.2} have shown that the proposed biomedical answer extraction methods are more competitive as compared with the current state-of-the-art methods. As can be seen from Table~\ref{tab:6.1}, compared with the two methods presented in \citep{neves2015hpi,zhang2015fudan}, one based on the in-memory database and another using the PubTator tool, our methods significantly outperformed the aforementioned methods in extracting both the exact answers and the ideal answers for yes/no, factoid, list and summary biomedical questions. Moreover, the increased performance was statistically significant (the P-value is 7.6e-05, the result is significant at p < 0.01). In particular, it can be seen clearly from Table~\ref{tab:6.1} that in all batches of testing datasets, the proposed
yes/no answer extraction method achieves performance improvements over the state-of-the-art method
presented in \citep{neves2015hpi}. Compared with the latter, which employs the sentiment analysis predictions
provided by the IMDB database, the proposed method gives better results (an average improvement of 15.68\% in terms of accuracy). Moreover, the increase performance is statistically significant (the p-value is < 0.00001, the result is significant at p < 0.01).


Additionally, the proposed systems is also competitive compared with the system proposed in \citep{choi2015snumedinfo}, which dealt only with the ideal answers (0.2860 against 0.3071, 0.3364 against 0.3710, 0.4078 against 0.3941, 0.4108 against 0.3906, and 0.3533 against 0.3665 of Rouge-SU4 in batch 1, batch 2, batch 3, batch 4, and batch 5 respectively).

In the 2016 BioASQ challenge, as shown in Table~\ref{tab:6.2}, the proposed answer extraction system still achieves good performance compared with the 2016 winning system developed by \citep{schulze2016hpi} which dealt solely with the ideal answers of questions. The latter is based on the LexRank algorithm \citep{erkan2004lexrank}, but that solely used the named entities for the similarity function. The important thing to note here is that our system significantly outperforms the \citep{schulze2016hpi} system in all batches of testing datasets. The largest difference in ROUGE-SU4 between SemBioNLQA and the aforementioned system was 0.2951 (0.5231 - 0.2280 in batch 4) which clearly indicates that our method is not only effective but robust in extracting the ideal answers.

On the other hand, as part of our participation in Phase B, Task 5B of the 2017 BioASQ challenge, the proposed answer extraction methods performed well in the challenge ranking within the top tier teams as shown in Table~\ref{tab:6.3} and Table~\ref{tab:6.4}. More details on the results can be found in the official web site of BioASQ\footnote{BioASQ Task5b phase b\url{http://participants-area.bioasq.org/results/5b/phaseB/}}. A total of 16, 22, 21, 27, and 25 runs were submitted for batch 1, batch 2, batch 3, batch 4, batch 5 respectively. Note that many runs were submitted by the same teams. As shown, our team is ranked among the top three. In batch 1, it achieved the third and the fifth
position within the 15 participating systems in extracting the exact answers of list and factoid questions respectively. More specifically, our system obtained the second and the third position when considering results by teams, instead of each individual run. In batch 2, considering results by teams, our system obtained the second and the fourth position in extracting the exact answers of list and factoid questions respectively. It can also be seen that in batch 3 and batch 4, our system, achieved the first and the second position respectively for answering
yes/no questions. Because of a very skewed class distribution in other batches, we have not compared our yes/no answer extraction method with
the participant systems which always answering ``yes''. For the ideal answers, our system in terms of ROUGE-2 achieved the fourth position compared to the 15, 21, and 21 participating systems in batch 1, batch 2 and batch 3 respectively, while in terms of ROUGE-SU4, the proposed system obtained the third position in batch 1 and the fourth position in batch 2. Besides, considering results by teams, instead of each individual run, our systems achieved the second position in terms of ROUGE-2 and ROUGE-SU4 in batches 1-4, whereas it achieved the third position in batch 5. Based on the manual scores in terms of readability, recall, precision, and repetition calculated by the BioASQ experts for each participating systems, our system achieved the second position in batch 1, batch 2, batch 3, batch 5, while it achieved the third position in batch 4. Overall, our system was one of the fifth BioASQ challenge winners\footnote{BioASQ challenge winners: \url{http://www.bioasq.org/participate/fifth-challenge-winners}}. This proves that the proposed answer extraction system could effectively identify the ideal answers to a given biomedical question.

Although the proposed answer extraction system could effectively answer a variety of biomedical questions, we found that there are still some mistakes that the proposed system cannot fix. An example is that, when the question is ``Does HER2 under-expression lead to favorable response to trastuzumab?,'' (identifier 51542eacd24251bc05000084) from BioASQ training questions, a positive answer might be ``Trastuzumab is a monoclonal antibody targeted to the Her2 receptor and approved for treatment of Her2 positive breast cancer,'' the sentiment score considers this answer to be a positive as there are both ``positive'' and ``approved'' in the passage. However, this example should be counted as a ``no'' answer because trastuzumab is effective only in cancers where Her2 is over-expressed. Dealing with such problems requires more complex semantic analysis, and we may need to parse the passage to get a grammar tree. Nevertheless, parsing and semantic analysis will bring in new errors and make this challenge even more complicated. We also found that the current form of the proposed system was not able to provide answers to some questions especially for these which expect a number as answer instead of biomedical entities. For instance, the answers for the biomedical question ``What is the prevalence of short QT syndrome?'' (identifier 52fb78572059c6d71c000067)  and ``What is the number of protein coding genes in the human genome?'' (identifier 535d3c069a4572de6f000006) collected from BioASQ training questions,  are ``0.01\% -0.1\%'', ``Between 20,000 and 25,000'', respectively. Such questions seem to be quite complicate and need more specific information extraction methods.


\section{A Semantic Biomedical Question Answering System SemBioNLQA}
\label{Chapter6.3}
\subsection{Methods}

In this section, we present the development, generic architecture and integrated components of our fully automated system SemBioNLQA - Semantic Biomedical Natural Language Question Answering - which has the ability to handle the kinds of yes/no questions, factoid questions, list questions and summary questions that are commonly asked in the biomedical domain. Figure~\ref{fig:interface} presents the SemBioNLQA Web system.

% Figure environment removed

The SemBioNLQA system, which consists of question classification, document retrieval, passage retrieval and answer extraction components, takes natural language questions as input, and outputs both \emph{exact answers} and \emph{ideal answers} as results.  SemBioNLQA is able to accept a variety of natural language questions and to generate appropriate natural language answers by providing both exact and ideal answers. It provides exact answers of type ``yes'' or ``no'' for yes/no questions, biomedical named entities for factoid questions, and a list of biomedical named entities for list questions. In addition to exact answers for yes/no, factoid and list questions, SemBioNLQA also returns ideal answers, while it retrieves only the ideal answers for summary questions. SemBioNLQA is derived from our previously established methods  in (1) question classification (cf. section~\ref{Chapter4.2}, chapter~\ref{Chapter4})  (2) document retrieval  (cf. section~\ref{Chapter5.2}, chapter~\ref{Chapter5}), (3) passage retrieval (cf. section~\ref{Chapter5.3}, chapter~\ref{Chapter5}), and (4) answer extraction system (cf. section~\ref{Chapter6.2}) which was one of the winners in the 2017 BioASQ challenge. Indeed, we developed the SemBioNLQA system based on the integration of these methods and techniques. Figure~\ref{fig:QAA} shows the architecture of SemBioNLQA and its main components.


SemBioNLQA first takes as its input a natural language biomedical question and includes preprocessing of the question, identification of the question type and the expected answer format to be required based on handcrafted lexico-syntactic patterns and support vector machine, as well as building a query from the question using UMLS entities to be fed into our document retrieval system based on PubMed and UMLS similarity. A document retrieval system is used to retrieve documents satisfying the query from the MEDLINE database. Then, it extracts relevant passages from top-ranked documents based on the BM25 model, stemmed words and UMLS concepts. Finally, it generates and returns both ``exact'' (depending on the expected answer for each question type) and paragraph-sized ``ideal'' answers from these passages based on the UMLS metathesaurus, BioPortal synonyms, SENTIWORDNET, term frequency metric and BM25 model. According to the QA classification approach presented by \cite{athenikos2010biomedical}, the SemBioNLQA can be classified as semantics-based biomedical QA.



% Figure environment removed

\subsection{Experimental results}

In order to assess the effectiveness of the SemBioNLQA system and compare it with the current integral biomedical QA systems presented in \citep{gobeill2009question,Cao_2011,Kraus_2017}, we present two different evaluations: (1) a systematic/automatic evaluation on benchmark datasets provided by the BioASQ challenges, and (2) a manual evaluation in terms of quality of the answers using BioASQ training questions and answers.

\subsubsection{Systematic evaluation}
In this experiment, we present a systematic evaluation on real biomedical questions provided by the BioASQ challenge in its 2017 edition so as to compare with Olelo, the most current biomedical QA system. Fortunately, the Olelo system has participated in Task 5b, phase B of the 2017 BioASQ challenge which enabled us to compare with them as we also have participated in the challenge. Please note that in this experiment, both systems SemBioNLQA and Olelo relied only on the gold-standard passages provided by BioASQ, instead of the ones retrieved by the systems.

As indicators of answer extraction effectiveness: Accuracy was used for exact answers of yes/questions; mean reciprocal rank (MRR) was used for exact answer of factoid questions; mean average precision, mean average recall, and mean average f-measure were used for exact answers of list questions; ROUGE-2 and ROUGE-SU4 were used for ideal answers. These evaluation metrics are described in details in subsection~\ref{Chapter3_7_2}, section~\ref{Chapter3_7}, chapter~\ref{Chapter3}. Table~\ref{tab:6.3.1} shows the experimental results of SemBioNLQA and comparison with Olelo on five batches of testing datasets provided by the 2017 BioASQ organizers during our participation. Please also note that we do not compare with other BioASQ participants in this work since they have not published the integral systems yet. As we previously noted, the BioASQ challenges in phase B of Task b provide the test set of biomedical questions along with their golden documents, golden snippets, and questions types. Therefore, participants do not require a fully QA to participate in the challenge. More details on the results can be found in the BioASQ web site\footnote{\url{http://participants-area.bioasq.org/results/5b/phaseB/}}. Our system name for submission was ``sarrouti''.


\begin{table}[h!]
\centering
\caption[The overall results of SemBioNLQA and comparison with Olelo on five batches of testing datasets provided by BioASQ 5b 2017]{The overall results of SemBioNLQA and comparison with Olelo on five batches of testing datasets provided by BioASQ 5b 2017. The ``-'' and ``nr'' indicate that the system did not deal with this task and the results are not reported for this evaluation measure, respectively. Acc, P, R, F, R-2, R-SU4 indicate accuracy, precision, recall, and f-measure, rouge-2, rouge-SU2, respectively.}
\label{tab:6.3.1}
\begin{tabular}{p{1.6cm}p{3.6cm}p{1.1cm}p{1cm}p{1cm}p{1cm}p{1cm}p{1cm}p{1.2cm}}
\hline\noalign{\smallskip}
 \multirow{3}{*}{Datasets} &\multirow{3}{*}{System name}& \multicolumn{5}{c}{Exact answers} &  \multicolumn{2}{c}{\multirow{2}{*}{Idial answers}}\\
\cmidrule(l){3-7}

 & & Yes/No & Factoid & \multicolumn{3}{c}{ List}  &\multicolumn{2}{c}{}   \\
 \cmidrule(l){3-3}\cmidrule(l){4-4} \cmidrule(l){5-7} \cmidrule(l){8-9}
  & &  Acc &  MRR& P& R &F1  & R-2& R-SU4\\
\noalign{\smallskip}\hline\noalign{\smallskip}
\multirow{2}{*}{Batch 1} &	SemBioNLQA & 0.7647&	0.2033	& 0.1909& 0.2658& 0.2129&0.4943& 0.5108 \\
                         &	Olelo & -&	0.0400&nr&nr&	 0.0477 & 0.2958& 0.3243\\

\cmidrule(l){1-9}
\multirow{2}{*}{Batch 2} &	SemBioNLQA &0.7778& 0.0887&	0.2400&	0.3922&	0.2920& 0.4579& 0.4583\\
                         &	Olelo & -&	0.0323&nr&nr&	 0.0287& 0.2048& 0.2500\\

            \cmidrule(l){1-9}
\multirow{2}{*}{Batch 3} &	SemBioNLQA &0.8387& 0.2212&	0.2000&	0.4151&	0.2640 & 0.5566 &	0.5656 \\
                         &	Olelo & -&	 0.0192 &nr&nr&	 0.0549& 0.2891& 0.3262\\
            \cmidrule(l){1-9}
\multirow{2}{*}{Batch 4} &	SemBioNLQA &0.6207& 0.0970&	0.1077&	0.2013&	0.1369& 0.5895&	0.5832 \\
                         &	Olelo & -&	0.0513 &nr&nr&0.0513& 0.3460& 0.3516\\

            \cmidrule(l){1-9}
\multirow{2}{*}{Batch 5} &	SemBioNLQA &0.4615 & 0.2071&	0.2091&	0.3087&	0.2438& 0.5772 &	0.5756\\
                         &	Olelo & -&	-&nr&nr&	 0.0379& 0.2117& 0.2626\\

\noalign{\smallskip}\hline
\end{tabular}
\end{table}


In particular, we also report the overall end-to-end evaluation results of the SemBioNLQA system on BioASQ datasets provided by the challenge in 2015 and 2016 editions in order to demonstrate the effectiveness of the SemBioNLQA and also to make new comparisons easier. Table~\ref{tab:6.3.2} highlights the obtained results on BioASQ 3b 2015 and BioASQ 4b 2016 datasets. All answers returned by the SemBioNLQA system on either BioASQ 3b 2015 and BioASQ 4b 2016 datasets are available for download\footnote{\url{https://sites.google.com/site/mouradsarrouti/datasets}}. Concretely, this experiment aims to answer the following questions:

\begin{enumerate}
  \item Is SemBioNLQA able to achieve improvements over the existing biomedical QA systems?
  \item Do question classification, document retrieval and passage retrieval components have an impact on the overall performance of the SemBioNLQA system?
\end{enumerate}



A direct comparison with the systems presented in \citep{neves2015hpi, zhang2015fudan,schulze2016hpi,yang2015learning,choi2015snumedinfo} and evaluated on either the 2015 or 2016 BioASQ challenges is not simple since the authors used the test set of biomedical questions along with their golden documents, golden snippets, and questions types released by the BioASQ challenges. While in the end-to-end evaluation of SemBioNLQA, the relevant documents, relevant passages, and the question type for a given biomedical question are obtained using its document retrieval, passage retrieval, and question classification systems, respectively.
\begin{table}[h!]
\centering
\caption[The overall evaluation results of the SemBioNLQA system on five batches of biomedical questions provided by BioASQ 3b 2015 and BioASQ 4b 2016]{The overall evaluation results of the SemBioNLQA system on five batches of biomedical questions provided by BioASQ 3b 2015 and BioASQ 4b 2016. Acc, P, R, F, R-2, R-SU4 indicate accuracy, precision, recall, and f-measure, rouge-2, rouge-SU2, respectively.}
\label{tab:6.3.2}
\begin{tabular}{p{3.1cm}p{1.3cm}p{1.2cm}p{1.1cm}p{1.1cm}p{1.1cm}p{1.1cm}p{1.1cm}p{1.3cm}}
\hline\noalign{\smallskip}
 \multirow{3}{*}{Dataset} &\multirow{3}{*}{Batch}& \multicolumn{5}{c}{Exact answers} &  \multicolumn{2}{c}{\multirow{2}{*}{Idial answers}}\\
\cmidrule(l){3-7}

 & & Yes/No & Factoid & \multicolumn{3}{c}{ List}  &\multicolumn{2}{c}{}   \\
 \cmidrule(l){3-3}\cmidrule(l){4-4} \cmidrule(l){5-7} \cmidrule(l){8-9}
  & &  Acc &  MRR& P& R &F1  & R-2& R-SU4\\

\noalign{\smallskip}\hline\noalign{\smallskip}
\multirow{5}{*}{BioASQ 3b 2015} &	Batch 1 &0.7273&	0.0128	& 0.0364&	0.0682&	0.0462& 0.1039&	0.1397\\
            &	Batch 2 &0.6875&	0.0339	& 0.0714&	0.1232&	0.0891& 0.1044&	0.1413\\
            &	Batch 3 &0.8621&	0.0641	& 0.0353&	0.0368&	0.0352& 0.1269&	0.1557\\
            &	Batch 4 &0.6800&	0.0586	& 0.0696&	0.0880&	0.0751& 0.1467&	0.1702\\
            &	Batch 5 &0.6786&	0.0795	& 0.0083&	0.0208&	0.0119& 0.0915&	0.1205\\

\cmidrule(l){1-9}
\multirow{5}{*}{BioASQ 4b 2016} &	Batch 1 &0.8214&	0.0534	& 0.1091&	0.1545&	0.1268&0.1552&	0.1855\\
            &	Batch 2 &0.7188&	0.0495	& 0.0476&	0.0477&	0.0462& 0.1378&	0.1720\\
            &	Batch 3 &0.8800&	0.1186	& 0.0857&	0.1667&	0.1122& 0.1430&	0.1730\\
            &	Batch 4 &0.8095&	0.0253	& 0.0400&	0.0667&	0.0500& 0.1097&	0.1289\\
            &	Batch 5 &0.8519&	0.0687	& 0.0300&	0.0517&	0.0368& 0.1609&	0.1836\\

\noalign{\smallskip}\hline
\end{tabular}
\end{table}
As it has already been stated before, to a given biomedical question, SemBioNLQA first retrieves the $N$ top-ranked documents, then finds the $N$ top-ranked passages and finally applies the appropriate answer extraction according to the question type detected by the question classification module. In particular, we have decided to go with the $N=10$ top-ranked documents and $N=10$ top-ranked passages since only the 10 first ones from the resulting list are permitted for the test in the 2015 and 2016 BioASQ challenges. After that, for yes/no questions, the exact answer (``yes'' or ``no'') is formed by using SENTIWORDNET, a sentiment lexicon. Each word of the relevant snippets is assigned its SENTIWORDNET score, and the decision to output ``yes'' or ``no'' depends on the number of positive or negative snippets. For factoid questions, the exact answer is produced by identifying the biomedical entities that occur in the given top relevant snippets of the question, and reporting the five most frequent biomedical entities and their synonyms of the top snippets, excluding entities also mentioned in the question. For list questions, the exact answer is produced in the same manner, except that the most frequent entities and their synonyms of the top relevant snippets are now returned as the single list that answers the question. On the other hand, to generate ideal answers (summaries), we have applied MetaMap to the relevant snippets and question in order to obtain the UMLS concepts they refer to. We then have ranked the snippets by their BM25 similarity to the question (using Porter stems and UMLS concepts as features), and return the top two most highly ranked snippets (concatenated) as the ideal answer. Figure~\ref{fig:r1}, Figure~\ref{fig:r2} and Figure~\ref{fig:r3}  show the SemBioNLQA output for three biomedical questions which come from BioASQ training questions.


% Figure environment removed

% Figure environment removed

% Figure environment removed





\subsubsection{Manual evaluation}

In this experiment, we randomly selected 30 questions from the BioASQ training dataset and posed these to the four systems - AskHermes, EAGLi, Olelo and SemBioNLQA. This evaluation was carried out manually, and therefore, we needed to limit the number of questions and types. We decided to limit it to factoid, list and yes/no questions given that these types of answers are easier to check manually than summaries. This sequence of 30 questions, which are listed in Appendix A, contains 10 factoid questions, 11 list questions and 9 yes/no questions. In our evaluation, an answer is considered as correct if the first returned biomedical entity (for factoid questions), at least one of the first five returned biomedical entities (for list questions) or the Boolean value, i.e., ``yes'' or ``no'', (for yes/no questions) is correct. Indeed, we manually checked the results returned by each system to look for the correct standard answers as provided by the BioASQ challenge. Table~\ref{tab:6.3.4} presents and compares the results of the aforementioned systems and SemBioNLQA. All answers returned by the systems are available for download \footnote{\url{https://sites.google.com/site/mouradsarrouti/datasets}}.

\begin{table}[h!]
\centering
\caption{Comparison of the obtained results by SemBioNLQA, EAGLi, AskHERMES and Olelo in terms of number of recognized questions and correct answers}
\label{tab:6.3.4}
\begin{tabular}{M{5.5cm}M{4.9cm}M{4.7cm}}
\hline\noalign{\smallskip}
Systems& Number of recognized questions& Number of correct answers\\
\noalign{\smallskip}\hline\noalign{\smallskip}
EAGLi \citep{gobeill2009question}&7/30&3/30\\
AskHERMES \citep{Cao_2011}&12/30&2/30\\
Olelo \citep{Kraus_2017}&30/30&6/30\\
\textbf{SemBioNLQA}&\textbf{30/30}&\textbf{18}/\textbf{30}\\
\noalign{\smallskip}\hline
\end{tabular}
\end{table}

\subsection{Discussion}

In contrast to traditional IR systems which identify a simple list of relevant documents for the user's query usually expressed in terms of some keywords, QA systems aims at providing precise short answers to user questions written in natural language. It is the goal of such systems to move the burden of browsing and filtering the numerous results, which can be quite time consuming, from the information seekers to the computer. While open-domain QA has been widely studied, few integral systems such as the ones described in \citep{gobeill2009question,Cao_2011,Kraus_2017} are currently able to automatically answer questions from the ever-increasing volume of peer-reviewed scientific articles in the biomedical domain. In this work, we addressed shortcomings of these systems, such as limited usability and performance in terms of the precision for the currently supported question and answer types. As results, unlike these systems, our developed biomedical QA system SemBioNLQA has the ability to handle a large amount of questions and answers types such as yes/no, factoid, list and summary questions that may cover all types of questions. It returns exact answers in the form of ``yes'' or ``no'' for yes/no questions, biomedical entities for factoid questions and a list of biomedical entities for list questions. In addition to exact answers for the aforementioned questions types, SemBioNLQA also formulates and returns ideal answers. For summary questions, the system retrieves only ideal answers since such questions do not have precise answers.

The results of the systematic evaluation, detailed in Table~\ref{tab:6.3.1}, have shown that SemBioNLQA gets better results compared with Olelo, the most current biomedical QA system. The presented system significantly outperformed the Olelo system in extracting both exact and ideal answers for the currently supported questions. The important thing to note here is that our system significantly outperforms the Olelo system in all batches of testing datasets. There is a large difference in the results, a thing that clearly indicates that our system is not only effective but also robust.

From another side, as shown in Table~\ref{tab:6.3.4} which presents and compares the results of the manual evaluation of SemBioNLQA, EAGLi, AskHERMES and also Olelo in terms of number of recognized questions and correct answers, SemBioNLQA gets better results and succeeded at answering the majority of randomly selected BioASQ questions. We have manually analyzed the answers provided for the biomedical questions by each system. In contrast to SemBioNLQA, which has proven to be quite successful at extracting the exact answers depending on the expected answer for each question type, Olelo returned a summary as the answer for the most questions and AskHERMES returned a multiple sentence passage as answer for all questions. Indeed, SemBioNLQA was able to detect the questions type were of the factoid, list or yes/no  types, and thus generated exact answers depending on the expected answer for each question type. This indicates that its integration of our question classification method offers SemBioNLQA the ability to understand and correctly recognize the information needs of users. In contrast, even though Olelo was developed to handle with factoid, list and summary questions, it was not able to detect the types for given questions, and thus generated summaries for all questions, and therefore, the users have to read these summaries so as to find the precise answers. In particular, it only returns exact answers when both the headword and semantic types are detected, in addition to the candidate answers being of this same semantic type. On the other hand, as shown in Table~\ref{tab:6.3.4}, both SemBioNLQA and Olelo have succeeded in returning answers for all questions, while AskHERMES and EAGLi could not provide answers for the majority of the questions, instead, only the following messages ``Nothing found! Please refine your question'' in the former and ``EAGLi did not understand your question. Try a popular example, or go to the manual mode.'' in the latter.

On the other hand, it is clear from Table~\ref{tab:6.3.2} that the different components of the SemBioNLQA system have a significant impact on the answer extraction task and therefore on the overall performance of SemBioNLQA since if the set of retrieved documents, passages and the type of a given question are not identified correctly, further processing steps to extract the answers will inevitably fail too.  For instance, for the question ``What is the association of spermidine with $\alpha$-synuclein neurotoxicity?'' (identifier 56c073fcef6e394741000020) from the batch 1 of test  set of the 2016 BioASQ challenge, the returned type of question is ``summary'' whereas in the corpus the type of question is ``factoid''. Therefore, the SemBioNLQA system will inevitably fail to extract and output the correct answer since extracting the answer to a factoid question, which is asking for a biomedical entity, is not the same as extracting the answer to a summary question which is looking only for an ideal answer.


Overall, SemBioNLQA holds a number of advantages over the state-of-the-art systems. First, the integration of our question types classification method it offers have a clear advantage over Olelo in that it returns exact answers depending on the expected answer of each question type. Second, SemBioNLQA which is aimed to be able to accept a variety of natural language questions and to generate appropriate natural language answers, provides an unbeatable advantage over AskHERMES, EAGli and Olelo in that it handles with a large amount of questions types including yes/no, factoid, list and summary questions. Third, the systematic and manual evaluations results demonstrated that SemBioNLQA is more effective as compared with the aforementioned systems.

In summary, biomedical QA is a very challenging task since it accepts questions written in natural language and provides precise answers instead of only presenting potentially relevant documents by integrating various resources. Therefore, no current system can always perform well on the myriad questions that can be asked of it. SemBioNLQA provides a practical and competitive alternative to help users find exact and ideal answers.

\section{Summary of the Chapter}

Starting from the aim of answering a variety of natural language questions, we presented in this thesis work the proposed methods for the extraction of the answers to given biomedical questions including yes/no questions, factoid questions, list questions, and summary questions. These types of questions that are commonly asked in the biomedical domain, may cover all types of questions that can be posed by the users.

In section~\ref{Chapter6.2} we presented in details the answer extraction system that we proposed for extracting the answer for each of the aforementioned question types. The proposed system provides exact answers (e.g., ``yes'', ``no'', a biomedical entity name, etc.) and ideal answers (i.e., paragraph-sized summaries of relevant information) for yes/no, factoid and list questions, whereas it provides only the ideal answers for summary questions. Thanks to an evaluation on standard benchmarks provided by the 2015 and 2016 BioASQ challenges, we noted that SemBioNLQA achieved promising performances compared with the best existing methods. Moreover, as part of our participation in Phase B, Task 5b of the 2017 BioASQ challenge, our submission was placed within the top tier submissions out of all participants. Furthermore, our system was one of the fifth BioASQ challenge winners.


In section~\ref{Chapter6.3} we tackled a fully automatic QA system in the biomedical domain, SemBioNLQA, which has the ability to deal with four types of biomedical questions including yes/no questions, factoid questions, list questions, and summary questions. SemBioNLQA is currently able to provide exact answers and paragraph-sized ideal answers for yes/no, factoid and list questions, whereas it only retrieves ideal answers for summary questions. The system relied on (1) handcrafted lexico-syntactic patterns and a machine learning approach for question classification, (2) PubMed search engine and UMLS similarity for document retrieval, (3) the BM25 model, stemmed words and UMLS concepts for passage retrieval, and (4) UMLS metathesaurus, BioPortal synonyms, sentiment analysis and term frequency metric for answer retrieval. Compared with the existing biomedical QA systems, SemBioNLQA has the potential to deal with a large amount of question and answer types. Moreover, experimental evaluations performed
on biomedical questions and answers provided by the BioASQ challenge especially in 2017 (as part of our
participation), show that SemBioNLQA achieves good performances compared with the most current state-of-the-art system and allows a practical and competitive alternative to help information seekers find exact and ideal answers to their biomedical questions.

