% Chapter 1
%\addcontentsline{toc}{chapter}{Résumé de la Thèse en Français}
\chapter*{Conclusion and Future Work} % Main chapter title
\addcontentsline{toc}{chapter}{Conclusion and Future Work}
\label{Chapter7} % For referencing the chapter elsewhere, use \ref{Chapter1}
\markboth{Conclusion and Future Work}{Conclusion and Future Work}
%\setcounter{secnumdepth}{4}
%\minitoc

%\section*{Introduction}
%\label{Chapter7.1}
%\addcontentsline{toc}{section}{Introduction}



In this PhD thesis, we focused on investigating and improving question answering in the biomedical domain where several specific challenges are addressed, such as specialized lexicons and terminologies, the types of treated questions and the characteristics of targeted documents. This improvement concerns the different components of a QA system, i.e., (1) question analysis and classification, (2) document retrieval, (3) passage retrieval, and (4) answer extraction. We proposed several methods for enabling users (e.g., researchers and health care professionals) to find precise and short answers to their natural language questions from the incessantly growing information in the biomedical domain. In addition, we developed a semantic biomedical QA system named SemBioNLQA by integrating the different proposed methods. The experimental results on two well-known standard corpus provided by the BioASQ challenge and U.S. NLM  have shown the effectiveness of the proposed methods. Furthermore, a subsystem of our SemBioNLQA system was one of the 2017 BioASQ challenge winners.

In this chapter we present the main conclusions derived from our contributions, and perspectives for future work. In section \hyperref[Chapter7.2]{Summary and Contributions} we present a summary of this work and its resulting contributions. In section \hyperref[Chapter7.3]{Future Work} we provide the future research directions for expanding our work.

\section*{Contributions}
\label{Chapter7.2}
\addcontentsline{toc}{section}{Contributions}

First, we proposed a machine learning-based method to identify the type of a given biomedical question (i.e., yes/no questions, factoid questions, list questions, and summary questions) which enables to a biomedical QA system to use the appropriate answer extraction. Then, we proposed another machine learning-based method to assign one or more topics to given biomedical questions in order to determine the semantic type of the expected answer which is very useful in generating specific answer retrieval strategies. Next, we proposed a document retrieval method in biomedical QA to retrieve the relevant documents that are likely to contain the answers to biomedical questions from the MEDLINE database. After that, we proposed a passage retrieval method in biomedical QA to retrieve the relevant passages/snippets to given biomedical questions. Finally, we proposed specific answer extraction methods to extract both exact and ideal answers from the retrieved passages, and presented our fully automated system SemBioNLQA - Semantic Biomedical Natural Language Question Answering - which is aimed to accept a variety of natural language questions and to generate appropriate natural language answers by providing both exact and ideal answers.

We summarize and highlight the main contributions and findings of this thesis work, addressing the research goals described in the \hyperref[Chapter1]{General Introduction}.

\subsection*{A question type classification method in biomedical QA}

We proposed a machine learning-based method for question type classification in biomedical QA. This methods aims at automatically classifying biomedical natural language questions into one of the following four categories which specify the types of questions: yes/no questions, factoid questions, list questions and summary questions defined by the BioASQ challenge. The task of assigning one of the aforementioned categories to a given question in biomedical QA is very useful in candidate answer extraction as it allows to a biomedical QA system to know in advance the expected answer format, and therefore to use the appropriate answer extraction strategy. For example, the biomedical question ``Does nimotuzumab improve survival of glioblastoma patients?'' expects an answer of type ``yes'' or ``no'', therefore, a yes/no answer extraction method should be used in this case to extract the final answer .

In the proposed method, we first extracted appropriate features from biomedical questions using our predefined handcrafted lexico-syntactic patterns which were constructed by analysing the BioASQ training questions. We then have fed these features for a machine-learning system. Finally, for a given unlabeled question, the class label is predicted using the trained classifier. Experimental evaluations performed on large standard annotated datasets of biomedical questions, provided by the BioASQ challenge, demonstrated that our method exhibits significant improved performance when compared to baseline systems. The proposed method achieved a roughly 10-point increase over the best baseline in terms of accuracy. Moreover, the obtained results have shown that using handcrafted lexico-syntactic patterns as features' provider of SVM lead to the highest accuracy of 89.40\%. The predefined patterns yielding the best results are also made available which encourage replication of results.

\subsection*{A question topic classification method in biomedical QA}

We proposed a machine learning-based method for question topic classification in biomedical QA. This method aims at automatically assigning one or more general topics (e.g., pharmacological, treatment, test, etiology, etc.) to clinical questions written in natural language. The task of question topic classification or answer type recognition is to determine the answer type, the semantic type of the expected answer, which is very useful in generating specific answer retrieval strategies and choosing the best resource from which the answer should be extracted. If we know the semantic type of the expected answer for a given question, we can avoid looking at every passage or biomedical entity name in the entire suite of candidate answer passages for the answer.

The proposed method first extracts a set of syntactic and semantic features from the annotated questions, including words, word stems, bigrams, UMLS concepts and semantic types, and syntactic dependency relations between pair words. It then feed these features for a machine learning model. Finally, it assigns one or more topics to the given unlabeled question using the trained classifier. We have explored several machine learning algorithms such as Naïve Bayes, Decision Tree, and SVM, showing SVM achieved the best results for this task on the annotated data that is released by NLM. A set of experiments have shown that the proposed method is more effective as compared with the state-of-the-art methods. Furthermore, The proposed method significantly outperforms the current state-of-the-art methods by an average of 4.5\% in terms of F1-score.


\subsection*{A document retrieval method in biomedical QA}

We studied the problem of deciding if a PubMed document is relevant to a specific natural language biomedical question in the context of biomedical QA. We developed a document retrieval system in which we proposed a new document reranking system. The idea behind this proposal is that there are many cases where the search engine mistakenly returns irrelevant citations high in the set or relevant citations low in the set for a given question, which poses a real problem for biomedical QA systems since they usually extract the answers from the documents ranked in the top of the set.

In the proposed system which aims at retrieving relevant documents to given question in biomedical QA, we first constructed the query from the input question using UMLS concepts identified by the MetMap tool. We then retrieved a set of possibly relevant documents to the query using a typical IR system which searches the MEDLINE database. We finally reranked the retrieved set of documents promoting to the top the documents it considers the most relevant to the biomedical question based on the semantic similarity between the question and the titles of the returned documents. To compute the semantic similarity scores, we first mapped both the given biomedical question and the titles of its set of the possibly relevant to UMLS Metathesaurus using MetaMap to identify UMLS concepts. We then computed the semantic similarity scores between UMLS concepts of the question and each title of the returned documents using UMLS similarity which uses path length as a similarity measure. The conducted experiments demonstrated that the proposed document retrieval system is more effective as compared with current state-of-the-art systems which were ranked within the 10 top tier systems in the BioASQ challenge. Furthermore, the experimental results have shown that the proposed document reranking method is able to achieve a significant improvement with respect to the original document ranking provided by a typical IR system.


\subsection*{A passage retrieval method in biomedical QA}

We proposed a passage retrieval system in biomedical QA to retrieve the relevant passages/snippets that are likely to contain the answer for a given biomedical question. In a typical biomedical QA system, the highly relevant documents that do not prominently answer a biomedical question are not the ideal candidate answers for further processing. Therefore, passage retrieval remains one of the most important components of any biomedical QA system as it allows to extract the set of potential answer passages from  the retrieved set of documents which serve as answer candidates from which the biomedical QA system extracts the answers. We have shown that the overall performance of a biomedical QA system heavily depends on the effectiveness of the integrated passage retrieval system.


The proposed biomedical passage retrieval consists of two stages, namely (1) document retrieval and (2) passage retrieval. During the first stage, we have used the PubMed search engine to retrieve the top-ranked documents to a given query constructed from the question using UMLS concepts. We then reranked the retrieved set of documents promoting to the top the documents it considers most relevant to the biomedical question based on UMLS similarity. At the second stage, passage identification, we have first taken the abstracts from the top ranked documents retrieved at the first stage, and used Stanford CoreNLP's tokenizer to split these abstracts into sentences. Thus, we treated the obtained set of sentences as a set of candidate passages, where the passage length is similar to that of the sentence. We then re-ranked the set of candidate passages using the BM25 model as a scoring function, and stemmed words and UMLS as features for text passage representation. Experimental evaluations performed on large standard datasets, provided by the BioASQ challenge, have shown that the proposed method achieves good performances compared with the current state-of-the-art methods. Furthermore, the proposed method significantly outperforms the current state-of-the-art methods by an average of 6.84\% in terms of MAP. In particular, we have found that passage length has a great impact on passage retrieval performance. We also anticipated improved passage retrieval if consistent passage length can be achieved.


\subsection*{SemBioNLQA: A semantic biomedical QA system}

We developed a novel answer extraction system which has the ability to deal with four types of questions including yes/no questions, factoid questions, list questions, and summary questions. In this system, we proposed for each question type its specific answer extraction method. For yes/no questions, the exact answer (i.e., ``yes'' or ``no'') is formed by using SentiWordNet, a sentiment lexicon. Each word of the relevant passages is assigned its SentiWordNet score, and the decision to output ``yes'' or ``no'' depends on the number of positive or negative candidate answer passages. For factoid questions, the exact answer is produced by identifying the biomedical entities that occur in the given top relevant passages of the biomedical question, and reporting the five most frequent biomedical entities and their synonyms of the top passages, excluding entities also mentioned in the question. For list questions, the exact answer is produced in the same manner, except that the most frequent entities and their synonyms of the top relevant passages are now returned as the single list that answers the question. To generate ideal answers (i.e., short summaries), we have applied MetaMap to the relevant passages and the question, in order to obtain the UMLS concepts they refer to. We then have ranked the passages by their BM25 similarity to the question (using Porter stems and UMLS concepts as features), and return the top two most highly ranked snippets (concatenated) as the ideal answer. A set of experiments on BioASQ datasets have shown that the proposed system is more competitive as compared with current state-of-the-art systems. Furthermore, our system was one of the winners in the 2017 BioASQ challenge.

On the other hand, we presented the fourth contribution of this thesis work which develop a semantic biomedical QA system named SemBioNLQA. SemBioNLQA is aimed to accept a variety of natural language questions and to generate appropriate natural language answers by providing both exact and ideal answers. The SemBioNLQA system, which consists of question classification, document retrieval, passage retrieval and answer extraction components, takes natural language questions as input, and outputs both short exact answers and ideal answers as results. SemBioNLQA is derived from our previously established methods  in (1) question classification (2) document retrieval, (3) passage retrieval , and (4) answer extraction systems. Compared with the current state-of-the-art biomedical QA systems, SemBioNLQA, a fully automated system, has the potential to deal with a large amount of question and answer types. SemBioNLQA retrieves quickly users' information needs by returning exact answers (e.g., ``yes'', ``no'', a biomedical entity name, etc.) and ideal answers (i.e., paragraph-sized summaries of relevant information) for yes/no, factoid and list questions, whereas it provides only the ideal answers for summary questions. Furthermore, experimental evaluations performed on real biomedical questions and answers provided by the BioASQ challenge shows that SemBioNLQA is more effective as compared with the existing systems and allows a practical and competitive alternative to help information seekers find exact and ideal answers to their biomedical questions.



\section*{Future Work}
\label{Chapter7.3}
\addcontentsline{toc}{section}{Future Work}

This thesis work opens several directions for further perspectives at different levels. In the following, we will shed light on the main perspectives related to each of the four components of a QA system, i.e., (1) question analysis and classification, (2) document retrieval, (3) passage retrieval, and (4) answer extraction.

\subsection*{Question classification}


The first step towards answering a biomedical question is analyzing and classifying the question in order to determine the type of question and the type of answer to produce. It is a crucial step of a biomedical QA system as the performance of such system depends directly on the performance of its question classification component. At the level of this task:

\begin{itemize}% \itemsep-4pt
 \item We intend to extensively study the training questions we used in this thesis work in order to develop additional patterns for improving the question type classification performance.

 \item We would consider further improvements of our question type classification method and question topic applying deep learning models which have been emerging as state-of-the-art for text classification.
\end{itemize}

\subsection*{Document and passage retrieval}

Due to the immense and increasing volume of biomedical literature, not only document retrieval and passage retrieval systems are becoming important for end-users searching for relevant documents and passages to their queries, but are also very important in many other text mining applications, particularly in QA systems. The identification of relevant documents and passages that may contain the answer for given questions is a challenging and important step for a biomedical QA system. In this context, we identified the following directions for future work:

\begin{itemize}% \itemsep-4pt

\item We intend to apply deep learning models to estimate the probability of the document's relevance to the question. More recently, deep learning models have been used in many IR systems and applied to various types of text matching problems. Such models produce a more robust predictor compared to computing similarities between vector representations of the query and document. They have shown significant improvements over traditional IR factors \citep{mohan2017deep}. An overview of neural retrieval models can be found in \citep{zhang2016neural,mitra2017neural}.

\item We intend to improve the performance of biomedical passage retrieval by exploring synonyms, semantic relationships between UMLS concepts and the word embedding.

\item We found in this thesis work that passage length has a great impact on the overall performance of passage retrieval systems, therefore, we anticipate improved passage retrieval if consistent passage length can be achieved.

\item We also intend to explore deep learning models to estimate the probability of the passage's relevance to the question.

\end{itemize}



\subsection*{Answer extraction}


Answer extraction is considered as the most challenging task of a QA system as  it allows extracting the precise answer, then presenting it to the user posing the question. The answer depends directly on the type of the given question (e.g., ``yes'' or ``no'' for yes/no question, biomedical entity names for factoid questions, etc.). At the level of this task, based on this thesis work, we identified the following directions for future work:


\begin{itemize} %\itemsep-4pt

\item We intend further improvements regarding answering yes/no questions by proposing more complex semantic and sentiment analysis methods.

\item Participating in the 2017 BioASQ challenge also provided us with a renewed interest in deep learning models. However, deep learning models are usually applied for factoid-type QA. Such approaches require a large number of question-answer pairs for the training phase. One of the 2017 wining systems of the 2017 BioASQ challenge developed by \cite{wiese2017neural} was a purely deep learning system. This system focused only on answering factoid and list questions. In particular, we are currently studying the different ways for incorporating deep learning models into our biomedical QA system in order to further improve its effectiveness particularly in answering factoid and list questions.


\item We envisage improving the effectiveness of the proposed QA system by searching in structured data (e.g., databases, ontologies, RDF triples, and the LOD cloud)

\item We also envisage working with full text (e.g., from PubMed Central) to see if we can improve answer extraction performance with a focused look at the full text. MEDLINE which indexes more than 24 million references, contains a link to the free full text of the article archived in PubMed Central that contains 4.4 million articles.

\end{itemize}

