% Chapter 1

\chapter{Document and Passage Retrieval in Biomedical Question Answering} % Main chapter title

\label{Chapter5} % For referencing the chapter elsewhere, use \ref{Chapter1}
\setcounter{secnumdepth}{4}
\minitoc

This chapter presents the methods we propose for document and passage retrieval in biomedical QA, key tasks that are also studied and evaluated separately. Section~\ref{Chapter5.2} will be dedicated to our proposed method for document retrieval in biomedical QA. We consecrate section~\ref{Chapter5.3} to our proposed passage retrieval method in biomedical QA.

\section{Introduction}
\label{Chapter5.1}

Document retrieval and passage retrieval are the most important components of any biomedical QA system, as shown in section~\ref{Chapter3_6_2} and section~\ref{Chapter3_6_3} of the chapter~\ref{Chapter3}, respectively. Document retrieval aims at retrieving the set of relevant documents that are likely to contain the answer to a given query constructed from the question, and passage retrieval consists in extracting relevant passages or snippets from the retrieved documents which serve as candidate answers and the biomedical QA system extracts and generates the answers from them. Undoubtedly, the overall performance of a biomedical QA system heavily depends on the effectiveness of the integrated document retrieval and passage retrieval components: if the document retrieval module and the passage retrieval module of a biomedical QA system fail to find the most relevant documents and passages, respectively, for the potential biomedical question, further processing steps to extract the answers will inevitably fail too. In other words, the correct answers to posted natural language questions can be found only when they already exist in one of the retrieved documents and passages. Additionally, it has been proved that the performance of the document retrieval and passage retrieval modules significantly affect the performance of the whole system \citep{Monz_2003,Tellex_2003}.


In the biomedical domain, the issues of retrieving relevant documents and passages to a given natural language question over a sizable textual document collection have been the lively topics of research in recent years, especially since the launch of the biomedical QA track at the BioASQ challenge \citep{tsatsaronis2012bioasq}. Although the most recent document retrieval systems presented in \citep{balikas2014results} and passage retrieval systems described in \citep{neves2015hpi,yenalaiiith,ligeneric,yang2015learning} have proven to be quite successful at extracting passages in biomedical QA, document retrieval and passage retrieval still require further efforts in order to improve their performance. In such systems the tasks of document retrieval and passage retrieval often set a framework in which an existing biomedical IR system is used, and completely depended on its ranking of documents and passages. Indeed, there are many cases where the IR system mistakenly returns irrelevant citations or passages high in the set or relevant citations or passages low in the set. This problem is certainly a challenging issue as a biomedical QA system usually extracts the answers from the top-ranked documents and passages. We assume that if we solve this issue, we could enhance the performance of  document retrieval and passage retrieval engines in biomedical QA and therefore increase the number of correct documents and passages retrieved containing the appropriate answers to the given natural language question.

In the second contribution of this thesis work, we propose two new methods to enhance the performance of document retrieval and passage retrieval in biomedical QA. The first method aims at retrieving most relevant documents to a given query from the MEDLINE database. The second method identifies and extracts relevant passages from the abstracts of the retrieved documents based on Stanford CoreNLP sentence length as passage length, stemmed words UMLS concepts as features for a probabilistic IR model called BM25. In the remaining of the chapter, we go into details about these proposed methods.

\section{Document Retrieval}
\label{Chapter5.2}

\subsection{Method}
In this section, we present in details the method we propose for document retrieval method in biomedical QA \citep{Sarrouti_2016}. As previously stated, this method aims at retrieving a set of relevant documents from the MEDLINE database to a given biomedical question in biomedical QA. To achieve that, we first create the query from the input question to be fed into a typical IR system. We then re-rank the retrieved documents based on the semantic UMLS similarity \citep{mcinnes2009umls} between biomedical concepts of a given question and each title of the returned documents. The proposed method takes as input a natural language question and a set of biomedical articles retrieved by an existing biomedical search engine (PubMed/MEDLINE). It then re-ranks the retrieved articles promoting to the top of the set the ones it considers most relevant to the given question. The flowchart of the proposed method, as shown in Figure~\ref{fig:dr}, is constructed through the following main steps: (1) query formulation, (2) querying the MEDLINE database using a typical information retrieval, and (3) re-ranking the retrieved documents in which we propose a method based on UMLS similarity.


% Figure environment removed


\subsubsection{Query formulation}

Query formulation is the task of constructing from the natural language question a set of keywords or concepts that form a query that can be sent to an IR system. In this work, we construct the query to be fed into a domain-specific IR system that allows the use of ontological terms instead of keywords for a more precise and efficient retrieval of relevant documents. To do so, we first use the MetaMap tool \citep{aronson2001effective} for mapping terms of the question to UMLS Metathesaurus in order to identify biomedical entity names and then connect them with the ``AND'' operator. For example, the biomedical question ``Which hormone concentrations are altered in patients with the Allan Herndon Dudley syndrome`?'' might be reformulated as ``hormones AND allan herndon dudley syndrome (AHDS) AND patients AND Mental concentration''; the question ``Is imatinib an antidepressant drug?'' as ``antidepressive agents AND imatinib''.



\subsubsection{Querying the MEDLINE database}


At this stage, a typical search engine is used to return a set of biomedical documents from the MEDLINE database to a given query constructed by the query formulation task. A search engine takes as input a query and outputs a set of potentially relevant documents, assigning a relevance score
to each document. During this stage of this work, we use GoPubMed search by calling the BioASQ PubMed service\footnote{BioASQ PubMed service: \url{http://gopubmed.org/web/gopubmed/bioasq/pubmedmedline}}. We also use PubMed search engine by calling E-utilities\footnote{E-utilities: \url{https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?}} Web service from PubMed. Figure~\ref{fig:pubmed} shows the list of documents returned by PubMed for the question ``Is imatinib an antidepressant drug?''. The number in the tag ``id'' represents PubMed Identifier (PMID) of a document.


% Figure environment removed


\subsubsection{Documents reranking}

Documents reranking, the re-ranking of the retrieved documents, is the important step of the proposed document retrieval system. In the proposed method, we do not completely depend on GoPubMed ranking of documents. So we re-rank the returned documents $(d_1, d_2, ... , d_{n})$ based on the semantic similarity between a given question and the title of each document. The idea is to assign a new relevance score to each of the returned documents. To do so, we first map both the given biomedical question and the titles of its set of the possibly relevant documents returned by the search engine to UMLS Metathesaurus using the MetaMap program so as to identify UMLS concepts. We then compute the sum of the semantic similarity scores between UMLS concepts of a given question and each title of the returned documents using UMLS similarity package\footnote{UMLS similarity: \url{http://maraca.d.umn.edu/cgi-bin/umls\_similarity/umls\_similarity.cgi?version=yes}} \cite{mcinnes2009umls}. Path length has been used as similarity measure where the similarity score is inversely proportional to the number of nodes along the shortest path between the concepts in Mesh\footnote{Mesh: \url{https://www.nlm.nih.gov/mesh/}} ontology. Figure~\ref{fig:sim} shows an example of simantic similarity scores between the biomedical question ``Is Tuberous Sclerosis a genetic disease?'' and the title ``Tuberous sclerosis complex diagnosed from oral lesions.'' of PubMed document (PMID=24310804).

% Figure environment removed


We finally rerank the set of relevant documents based on the semantic similarity scores. The higher semantic score, the higher the document appears in the set. Based on these steps, we design the algorithm~\ref{alg:1} to retrieve the high informative documents for a given biomedical question from PubMed articles.


\begin{algorithm}[h!]
\caption{Biomedical document re-ranking}
\label{alg:1}
\begin{algorithmic}[1]
%\algnewcommand\INPUT{\item[\textbf{Input:}]}%
\State $\textbf{Input} \leftarrow {Question \;Q\; and\; List\;of\;Documents\; R}$
\State $\textbf{Output} \leftarrow {List \;of \;Top\;Documents\; N}$
\Function{RankDocuments}{Q, R}
\State $scores \leftarrow \{\}$
\State $i \leftarrow {1}$
\Do
    \State $T[i]\leftarrow  R[i].title $
    \State $scores[i] \leftarrow \Call{ComputeSimilarity}{$Q, T[i]$}$ \Comment{call function to compute the semantic similarity between the question and document title}
    \State $i \gets i+1$
\doWhile{$i\leq n$} \Comment{n is the size of R}
\State $SortDocuments(R, scores)$ \Comment{sort R according the score of each document}
\State $N [1...m]\leftarrow R [1...m]$ \Comment{keep only the m top documents}
\State \Return $N$
\EndFunction

\Function{ComputeSimilarity}{Q, docTitle}
\State $Question\_concepts\_CUI[1...l]\leftarrow \Call{MappingQuestionToUMLS}{$Q$}$ \Comment{we used MetaMap tool for mapping text to UMLS}
\State $DocTitle\_concepts\_CUI[1...k]\leftarrow \Call{MappingDocTitleToUMLS}{$docTitle$}$
\State $similarity \leftarrow 0$, $sumSimilarity \leftarrow 0$
\State $i \leftarrow 1$, $j \leftarrow 1$
\Do
    \State $QCUI\leftarrow  Question\_Concepts\_CUI[i] $
\Do
    \State $TCUI\leftarrow  docTitle\_Concepts\_CUI[j] $
    \State $similarity \leftarrow UMLS::Similarity(QCUI, TCUI) $
    \If{$similarity \neq -1$} \Comment{there is no relationship between two concepts}
        \State $sumSimilarity \leftarrow sumSimilarity+similarity$
      \EndIf
    \State $j \gets j+1$
\doWhile{$j\leq k$}
    \State $i \gets i+1$
\doWhile{$i\leq l$}

\State \Return $sumSimilarity$
\EndFunction
\end{algorithmic}
\end{algorithm}

\subsection{Experimental results}


In order to evaluate and test the effectiveness of the proposed document retrieval method and our re-ranking technique, as well as to compare the results with state-of-the-art methods, we carried out experiments on benchmark test data provided by the BioASQ challenge. We used the typical evaluation measures used in IR which are: mean precision, mean recall, mean F-measure and mean average precision (MAP) described in section~\ref{Chapter3_7_2} (cf. chapter~\ref{Chapter3}). MAP is the most important measure for evaluating and comparing IR systems as it emphasises early precision and strongly takes into account recall. The proposed method takes as input a biomedical question and its set of possibly relevant documents that was retrieved by the GoPubMed search engine, and it reranks the retrieved documents promoting to the top the documents it considers most relevant to the biomedical question. In particular, we retrieved up to the 200 top ranked documents using the GoPubMed search engine, then reranked them using the proposed algorithm~\ref{alg:1}, and finally kept only the 100 top-ranked documents. Indeed, we have decided to go with the 100 first documents since only the 100 first ones from the resulting list are permitted to be submitted for the test in BioASQ 2014.

Table~\ref{tab:5.2.1} presents the experimental results of the proposed biomedical document retrieval method and the ones of the best state-of-the-art systems presented in \citep{balikas2014results} which were ranked within the 10 top tier systems on the standard test data provided by the BioASQ challenge \citep{tsatsaronis2012bioasq}. The experimental results showed Mean Precision of 0.2331, Mean Recall of 0.3644, Mean F-measure of 0.2253, and MAP of 0.2758. As can be seen from Table~\ref{tab:5.2.1}, the obtained results from the proposed document retrieval method in biomedical QA have an absolute competitiveness with the top 10 state-of-the-art methods results.

\begin{table}[h!]
\centering

\caption{The experimental results of the proposed document retrieval system and state-of-the-art systems presented in \citep{balikas2014results}, which were ranked within the 10 top tier systems on batch 1 of BioASQ 2014.}
\label{tab:5.2.1}
\begin{tabular}{p{4cm}p{3cm}p{2.8cm}p{3cm}p{1.3cm}}
\hline \noalign{\smallskip}
System &Mean Precision &Mean Recall &Mean F-measure &MAP  \\
\noalign{\smallskip}\hline\noalign{\smallskip}
\textbf{Our System }&\textbf{0.2331} &\textbf{0.3644} &\textbf{0.2253} &\textbf{0.2758} \\
SNUMedinfo1 &0.0457& 0.5958 &0.0826& 0.2612 \\
SNUMedinfo3 &0.0457 &0.5947 &0.0826 &0.2587 \\
SNUMedinfo2 &0.0451 &0.5862 &0.0815 &0.2547\\
SNUMedinfo4 &0.0457 &0.5941 &0.0826 &0.2493\\
SNUMedinfo5 &0.0459 &0.5947 &0.0829 &0.2410\\
Top 100 Baseline &0.2274 &0.4342 &0.2280 &0.1911 \\
Top 50 Baseline &0.2290 &0.3998 &0.2296 &0.1888 \\
Main system &0.0413 &0.2625 &0.0678 &0.1168 \\
Biomedical Text Ming &0.2279 &0.2068 &0.1665 &0.1101  \\
Wishart-S2 &0.1040 &0.1210 &0.0793 &0.0591 \\

\noalign{\smallskip}\hline

\end{tabular}
\end{table}

We have experimented with and without our document reranking method. Table~\ref{tab:5.2.2} shows the increase performance of the proposed document reranking method in comparison with GoPubMed document ranking in biomedical QA. In particular, this evaluation aims to answer the following question:

 \begin{itemize}
 \item Is the proposed reranking technique able to achieve improvement with respect to the original document ranking provided by a typical IR system?
 \end{itemize}

\begin{table}[h!]
\centering

\caption{Comparison of our document reranking method and GoPubMed document ranking for biomedical QA on batch 1 of BioASQ 2014.}
\label{tab:5.2.2}
\begin{tabular}{M{7.2cm}M{1.7cm}M{1.6cm}M{2.2cm}M{1.5cm}}
\hline \noalign{\smallskip}
System &Mean \newline  Precision &Mean  \newline Recall &Mean \newline F-measure &MAP  \\
\noalign{\smallskip}\hline\noalign{\smallskip}
 GuPubMed document ranking &0.2253	&0.3111&	0.1913&	0.1439 \\
\textbf{Proposed document reranking method}&\textbf{0.2331} &\textbf{0.3644} &\textbf{0.2253} &\textbf{0.2758} \\\cmidrule(l){1-5}
Increase performance&+0.0078&+0.0533&+0.034 &+0.1319 \\


\noalign{\smallskip}\hline

\end{tabular}
\end{table}


As shown in Table~\ref{tab:5.2.2}, the proposed document reranking method significantly outperformed the GoPubMed document ranking for placing the most relevant documents at the top of the set to a given biomedical question in biomedical QA. In terms of MAP, the proposed system achieved 0.2758, whereas the GoPuMed search engine achieved 0.1439.

\subsection{Discussion}

A typical IR system takes as input a query and outputs a set of potentially citations, assigning a relevance score to each citation. The higher the score is, the higher the citation appears in the set. However, there are many cases where the search engine mistakenly returns irrelevant citations high in the set or relevant citations low in the set. This poses a real problem for biomedical QA systems which usually extract the answer from the first retrieved documents to a given biomedical question. Taking an example from BioASQ training questions, assume we have the biomedical question ``Which are the cellular targets of imatinib mesylate?'' (identifier 53188992b166e2b806000019), PubMed returns 1115 articles. The BioASQ training dataset indicates that only two are relevant to the question, those with PMID (PubMed IDentifier) 15887238 and 15844661. The first one is ranked 154th and the second one 237th. This means that the first 10 or even 100 articles do not answer the question. In this work, we studied the problem of deciding if a PubMed document is relevant to a specific biomedical question written in natural language. We developed a document retrieval engine in which we proposed a new reranking method to re-rank the retrieved set of documents and move the most relevant documents to the top of the set.

The conducted experiments presented in Table~\ref{tab:5.2.1}, clearly demonstrate that the proposed method is more effective as compared with current state-of-the-art systems which were ranked within the 10 top tier systems presented in \citep{balikas2014results} in the BioASQ challenge. For instance, in terms of MAP, the proposed system outperformed SNUMedinfo \cite{choi2014classification}, the best performing system in the challenge, which uses both the semantic concept-enriched dependence model and the sequential dependence model.


Additionally, it is important to notice that in terms of Mean Precision, Mean F-measure and MAP, the proposed system achieved performance improvements over the state-of-the-art systems. In terms of Precision@100 which shows how useful the retrieval results is, the proposed system which achieved 0.2331, is more effective as compared to the state-of-the-art systems at retrieving relevant documents among the 100 retrieved documents.  The high precision of the proposed document retrieval system means that our system retrieved more relevant documents than irrelevant ones.

On the other hand, we have experimented with and without our document reranking method, as shown in Table~\ref{tab:5.2.2}, in order to show if the proposed reranking technique is able to achieve improvement with respect to the original document ranking provided by a typical IR system. Effectively, the experimental results have shown that the proposed document reranking method outperformed the GoPubMed document ranking by a statistically significant margin (0.1319 of MAP). On the other hand, as can be seen from Table~\ref{tab:5.2.2}, there is not a significant difference between the results in terms of Precssion@100 of the proposed method and GoPubMed. This means that both systems have returned the same number of relevant documents to the given biomedical questions. However, in terms of MAP, the proposed reranking system significantly outperformed the GoPubMed document ranking (0.2758 against 0.1439 of MAP). This means that the proposed system was very successful to move the relevant documents at the top of the set of retrieved documents. We attribute this to the UMLS similarity used in this work to calculate the similarity scores between the biomedical questions and the titles of the retrieved documents.


Placing and moving the relevant documents at the top of the set of retrieved documents to given biomedical questions is very useful for biomedical QA systems as they usually extract the answers from the first top-ranked documents. Furthermore, it is much better if the relevant documents are shown first for the users since by returning irrelevant citations high in the list or relevant citations low in the list to a given query, make it unlikely for a user to read them or the users have to read them one by one until they finally find a relevant document. Therefore, from the results and analysis, we can draw a conclusion that the proposed document retrieval system may improve the overall performance of biomedical QA systems, and allows a practical and competitive alternative to help information seekers find quickly the relevant documents to their queries.

Although the set of PubMed citations is generally ranked by relevance, the top-ranked citations is probably not the answers to the given biomedical questions. This is because citations are not an ideal unit to rank with respect to the ultimate goals of a biomedical QA system. A highly relevant citation that does not prominently answer a biomedical question is not an appropriate candidate for further QA processing. Therefore, the next step is to identify and extract a set of candidate answer passages from the retrieved and selected documents. Accordingly, we will present in the following section~\ref{Chapter5.3} an efficient method to extract a set of relevant passages to a given biomedical question.





\section{Passage Retrieval}
\label{Chapter5.3}
\subsection{Method}

In this section, we present in details the proposed passage retrieval method in biomedical QA \citep{Sarrouti_2017}, which is comprised of two stages: document retrieval and passage retrieval. During the document retrieval stage, the query that is constructed from the user question is handed over to the developed document retrieval system, which is similar to our proposed document retrieval approach, presented in previous section~\ref{Chapter5.2}, in order to find relevant documents. Then, the proposed passage retrieval method extracts relevant passages from the abstracts of the retrieved documents in the passage retrieval stage. The flowchart of the proposed method is shown in Figure~\ref{fig:pr}.


% Figure environment removed

The input to the proposed biomedical passage retrieval method is a natural language question. A document retrieval stage uses the MetaMap tool \citep{aronson2001effective} to identify the biomedical entities from the given question, and then connect them ``+'' operator to construct the query. Next, it sends the query to PubMed search engine by calling E-utilities\footnote{E-utilities: \url{https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?}} web service from PubMed in order to retrieve the $R$ relevant documents. After that, it re-ranks the $R$ retrieved documents based on our proposed document re-ranking algorithm~\ref{alg:1}, and finally keeps only the $N$ top-ranked ones to be input for the next stage, passage extraction.

In the passage retrieval stage, once the $N$ top-ranked documents $(d_1, d_2, ... , d_{n})$ are retrieved by the document retrieval system for a given question, we take abstracts of all these documents and forward them one by one to Stanford CoreNLP \citep{Manning_2014} in order  to extract all sentences, i.e., a set of candidate passages. The passage length is similar to that of the Stanford CoreNLP sentence. Figure~\ref{fig:passages} shows a list of candidate passages extracted from the abstract of the PubMed document\footnote{\url{https://www.ncbi.nlm.nih.gov/pubmed/23044018}} using the Stanford CoreNLP tool.

% Figure environment removed


Then, we compute the similarity scores based on BM25 as retrieval model between the biomedical question and each of the candidate passages. Before that, terms in both the question and passages are extracted, all tokens which match stop words list\footnote{Stop words:  \url{http://www.textfixer.com/resources/common-english-words.txt}} are removed and stemmed words are extracted using Porter' stemmer \citep{Porter_1980}. In addition, we have used the MetaMap\footnote{MetaMap: \url{http://metamap.nlm.nih.gov/}} program \citep{aronson2001effective} for mapping the biomedical question and passages to UMLS Metathesaurus (2016AA knowledge source) in order to extract UMLS concepts, and we then used them as additional features. Moreover, the MetaMap word sense disambiguation system \citep{Aronson_2010} has been used to resolve ambiguities in the texts by identifying the meaning of ambiguous terms. Therefore, stemmed words and UMLS concepts were then used as index terms. Indeed, each of the candidate passages is represented by the weighted vector $p_i={(stem_1^i, stem_2^i, ..., stem_n^i, concept_1^i, concept_2^i, ..., concept_n^i)}$ which contains stemmed words and UMLS concepts as features.

As it is stated, the similarity between a candidate passage and a biomedical question is defined by the BM25 model. Indeed, the BM25 function scores each passage in a set of passages according to the passage's relevance to a particular biomedical question. Formally, let a biomedical question $Q$ with terms and concepts $Q= (q_1, q_2, ... , q_n)$, the BM25 score for a passage $p$ is given by:
\begin{equation}\label{eq:1}
BM25\;(p)= \sum_{i=1}^n IDF(q_i)\frac {f(q_i)*(k_1+1)}{f(q_i)+k_1*(1-b+b*|P|/p_{avg})}\
\end{equation}
where:
\begin{itemize}
  \item $f(q_i)$ is the number of times term $q_i$ occurs in passage $p$;
  \item |P| is the number of words and concepts in passage $p$;
  \item $p_{avg}$ is the average number of words and concepts per passage;
  \item $k_1$ and $b$ are free parameters for Okapi BM25 \citep{Robertson96okapiat}. $k_1$ is a positive parameter
that calibrates the passage term frequency scaling and b is a parameter $(0 \leq b \leq 1)$ that determines the passage length scaling. By default, $k_1$ and $b$ are set to 1.2, 0.75, respectively.
\end{itemize}
The first quantity in the sum is the inverse document frequency (IDF). For a set of passages with $N$ passages, inverse document frequency for term and concept $q_i$ is defined in the following equation:
\begin{equation}\label{eq:2}
IDF(q_i)= \log {\frac {N-N(q_i)+0.5}{N(q_i)+0.5}}\
\end{equation}
where $N(q_i)$ is the number of passages in the set of passages that contain term or concept $q_i$. Note that terms with a lower $IDF$ are ignored since such terms are not informative.

Finally, after finding BM25 scores for all passages, we sort them and take the $M$ top-ranked passages matching most with the biomedical question.



\subsection{Experimental results}


In order to demonstrate the effectiveness of the proposed biomedical passage retrieval method and compare the results with the state-of-the-art methods, we carry out a comprehensive set of experiments on benchmark datasets provided by the bioASQ challenge. We have used the five test sets provided in BioASQ Task 3b.  Each set of testing datasets contains approximately 100 biomedical questions. As indicators of retrieval effectiveness, mean precision, mean recall, mean F1-measure and MAP described in section~\ref{Chapter3_7_2} were used (cf. chapter~\ref{Chapter3}). As it has already been stated before, the proposed method first retrieves the $N$ top-ranked documents based on PubMed and UMLS similarity, and then retrieves the $N$ top-ranked passages from the abstracts of the retrieved documents to a given biomedical question. To do so, the $N$ top-ranked documents are segmented into sentences using the Stanford CoreNLP tool in order to make a set of candidate passages. The passage length is similar to the Stanford CoreNLP sentence one. Then, the candidate passages are ranked using stemmed words and UMLS concepts as features for the BM25 model. The best values of parameters b and k1 of BM25 are respectively 0.85 and 1.2 which were fixed after 5-fold cross-validation. Finally, we kept the $N$ top-ranked passages from the set of candidate passages. In particular, we have decided to go with the 10 first passages ($N=10$) since only the 10 first ones from the resulting list are permitted to be submitted for the test in BioASQ 2015. MAP@10 gives an indication of how the passage retrieval system performs for QA systems that are primarily interested in the first set of retrieved documents. Figure~\ref{fig:docssrel} and Figure~\ref{fig:passagesrel} show an example of the retrieved documents and 10 top-ranked passages, respectively, for the biomedical question ``What symptoms characterize the Muenke syndrome?'' (52bf1d3c03868f1b0600000d) from the BioASQ datasets.

% Figure environment removed



% Figure environment removed

Table~\ref{tab:5.3.1} presents the experimental results of the proposed biomedical passage retrieval method on five batches of testing datasets provided by the BioASQ challenge \citep{tsatsaronis2012bioasq}. In terms of MAP, our system achieves 0.1178, 0.1180, 0.1491, 0.1633, and 0.1437 in Batch 1, Batch 2, Batch 3, Batch 4, and Batch 5, respectively.




\begin{table}[h!]
\centering

\caption{
The overall results of the proposed method on five batches of testing datasets provided by the BioASQ challenge.}
\label{tab:5.3.1}
\begin{tabular}{p{2.3cm} p{3.2cm}p{3.2cm}p{3.8cm}p{1.6cm}}
\hline \noalign{\smallskip}
Batches  &Mean Precision &Mean Recall &Mean F1-measure &MAP  \\
\noalign{\smallskip}\hline\noalign{\smallskip}
{Batch 1} &0.1340&	0.1153&	0.1069&	0.1178 \\

{Batch 2} &0.1465	&0.1084&	0.1082&	0.1180\\

{Batch 3} &0.1688	&0.1018	&0.1138&	0.1491\\

{Batch 4}&        0.1928&	0.1254&	0.1321&	0.1633\\

{Batch 5}&       0.1816	&0.1401&	0.1370&	0.1437\\
\noalign{\smallskip}\hline

\end{tabular}
\end{table}

Table~\ref{tab:5.3.2}, on the other hand, shows a comparison in terms of MAP between the proposed biomedical passage retrieval method and the current state-of-the-art methods presented in \citep{neves2015hpi,yenalaiiith,ligeneric,yang2015learning} on five batches of testing datasets. The ``+'' sign and number in parentheses indicate the statistically significant improvements of the proposed method over the state-of-the-art methods. The conducted experiments clearly demonstrate that the proposed method outperformed the current state-of-the-art methods by a statistically significant margin. The proposed method outperforms the current state-of-the-art methods by an average of 6.84\% in terms of MAP.
\begin{table}[h!]
\centering

\caption[Comparison in terms of MAP of the proposed biomedical passage retrieval method with the state-of-the-art methods on five batches of testing datasets provided by the BioASQ challenge]{Comparison in terms of MAP of the proposed biomedical passage retrieval method with the state-of-the-art methods on five batches of testing datasets provided by the BioASQ challenge. The ``-'' replace the scores of systems that did not evaluated on some batches.}
\label{tab:5.3.2}
\begin{tabular}{M{4.2cm}M{1.9cm} M{1.9cm}M{1.9cm}M{1.9cm}M{1.9cm}}
\hline \noalign{\smallskip}
\multirow{2}{*}{Models}& \multicolumn{5}{c}{MAP@10} \\
\cmidrule(l){2-6}
& Batch 1 & Batch 2 &Batch 3 &Batch 4&Batch 5 \\
\noalign{\smallskip}\hline\noalign{\smallskip}
\textbf{Proposed Method}&\textbf{0.1178}&\textbf{0.1180}&\textbf{0.1491}&\textbf{0.1633}&\textbf{0.1437}\\\cmidrule(l){1-6}
\cite{neves2015hpi}&0.0347 (+8.31\%)& 0.0355 (+8.3\%)& 0.0452 (+10.39\%)& 0.0624 (+10.09\%)& 0.0572 (+8.65\%)\\\cmidrule(l){1-6}
\cite{yenalaiiith}&0.0545 (+6.33\%)& 0.0709 (+4.71\%)& -& 0.0913 (+7.2\%)& -\\\cmidrule(l){1-6}
\cite{ligeneric}&0.0570 (+6.08\%)& 0.0521 (+6.59\%)& 0.0832 (+6.59\%)& 0.0936 (+6.97\%)& 0.1201 (+2.36\%)\\\cmidrule(l){1-6}
\cite{yang2015learning}&-&-&0.0892 (+5.99\%)&0.0957 (+6.76\%)&0.1027 (+4.1\%)\\
\noalign{\smallskip}\hline

\end{tabular}
\end{table}

Moreover, we present in Table~\ref{tab:5.3.3} the obtained results of the different implementation variations of the BM25 model, i.e., with or without stemming or UMLS concepts, and the two passage lengths: OpenNLP\footnote{OpenNLP: \url{https://opennlp.apache.org/}} sentence length and Stanford CoreNLP\footnote{Stanford CoreNLP: \url{http://stanfordnlp.github.io/CoreNLP/}}  sentence length. Concretely, these experiments aim to answer the following questions:

\begin{enumerate}
  \item What is the effect of stemming and UMLS concepts on the performance of passage retrieval systems?
  \item What is the effect of the passage length on the overall performance of passage retrieval systems?
\end{enumerate}


\begin{table}[h!]
\caption{The obtained results of the different implementation variations of the BM25 model, i.e., with or without stemming or UMLS concepts, and the two passage lengths: OpenNLP sentence length and Stanford CoreNLP sentence length}
\label{tab:5.3.3}
\begin{tabular}{p{6.2cm}p{1.5cm} p{1.5cm}p{1.5cm}p{1.5cm}p{1.5cm}}
\hline \noalign{\smallskip}
\multirow{2}{*}{Models}& \multicolumn{5}{c}{MAP@10} \\
\cmidrule(l){2-6}
& Batch1 & Batch2 &Batch3 &Batch4&Batch5 \\
\noalign{\smallskip}\hline\noalign{\smallskip}
$BM25_{OpenNLP, stem}$&0.1000& 0.1118 &0.1212  & 0.1289 &0.1208  \\
$BM25_{StanfordCoreNLP, stem}$&0.1128 & 0.1106 & 0.1354& 0.1367& 0.1349\\
$BM25_{StanfordCoreNLP, UMLS}$&0.1051 & 0.1014 & 0.1197 & 0.1626 & 0.1266 \\
$BM25_{StanfordCoreNLP, stem+UMLS}$&0.1178 & 0.1180 & 0.1491 & 0.1633 & 0.1437\\
\noalign{\smallskip}\hline

\end{tabular}
\end{table}

Additionally, we have experimented with TFIDF metrics used in \citep{neves2015hpi} to ordering the candidate passages obtained by Stanford CoreNLP sentence length so as to confirm how well passage length and retrieval models impact the performance of the passage retrieval systems. Table~\ref{tab:5.3.4} presents the results of TFIDF metric and Stanford CoreNLP sentence/passage length. The results clearly demonstrate that using the same retrieval model used in \citep{neves2015hpi}, i.e., TFIDF metric, and Stanford CoreNLP passage length outperformed the passage retrieval system presented by Neves \cite{neves2015hpi}, which uses the built-in information retrieval features available in the IMDB by a statistically significant margin.




\begin{table}[h!]
\caption{The detailed results using TFIDF metrics used in \citep{neves2015hpi} and Stanford CoreNLP passage length on five batches of testing datasets provided by the BioASQ challenge.}
\label{tab:5.3.4}
\begin{tabular}{p{2.6cm} p{3.2cm}p{3.1cm}p{3.6cm}p{1.6cm}}
\hline \noalign{\smallskip}
Batches  &Mean Precision &Mean Recall &Mean F1-measure &MAP  \\
\noalign{\smallskip}\hline\noalign{\smallskip}
{Batch 1} &0.1135	&0.1105&	0.0972&	0.0908 \\

{Batch 2} &0.1383	&0.1021	&0.1015&	0.0963\\

{Batch 3} &0.1431&	0.1046&	0.1042&	0.1010\\

{Batch 4}&0.1607	&0.0992&	0.1062&	0.1210\\

{Batch 5}& 0.1235&	0.1042&	0.0952&	0.1014\\
\noalign{\smallskip}\hline

\end{tabular}
\end{table}


\subsection{Discussion}


Though passage retrieval has been widely investigated, few approaches are currently able to efficiently extract passages from large collections of documents. This is a challenging task in any QA system, particularly in the biomedical domain. The definition of a passage (also known as a snippet) is necessarily system dependent, but the typical units include sentences, paragraphs, and sections. In this work, our goal was to develop an efficient passage retrieval method in biomedical QA. In our method, the length of a passage is similar to that of the sentence. The proposed system retrieves top-ranked documents which are broken up into suitable snippets/passages and reranked. Experimental results presented in Table~\ref{tab:5.3.1} and Table~\ref{tab:5.3.2} show that the proposed method is more effective as compared with the state-of-the-art methods proposed in \citep{neves2015hpi,yenalaiiith,ligeneric,yang2015learning}. The proposed method significantly outperforms these methods by an average of 6.84\% in terms of MAP. Furthermore, it is important to notice that in all batches of testing datasets, the proposed biomedical passage retrieval method achieves performance improvements over the state-of-the-art methods. For instance, compared with the approach proposed in \citep{neves2015hpi}, which employs TFIDF weighting metrics to compute similarity between questions and the set of candidate passages obtained by the built-in information retrieval features available in the IMDB database, our method gives better results (an average improvement of 9.15\% in terms of MAP). The increased performance was statistically significant (the P-value is 0.0357, the result is significant at p < 0.05). Our method outperforms also the method based on some specific rules used to define passage length and the sequential dependence model as retrieval model proposed and evaluated in \citep{ligeneric}. It outperforms the latter by an average of 5.72\% in terms of MAP.

Passage length and retrieval models are two factors that impact the performance of passage retrieval systems \citep{levow2013uwcl}. After deep analyzing passages in the BioASQ training dataset, we found that the distribution of passage length is approximately similar to that of a sentence. Indeed, if the set of the candidate passages retrieved from relevant documents is not identified correctly, the retrieval model used to rank these candidate passages and select the best ranked ones will inevitably fail too. Therefore, we have experimented with two sentence splitter tools, i.e., two passage lengths, in order to show how well a system can perform the passage retrieval task from PubMed documents by using the Stanford CoreNLP sentence length. Table~\ref{tab:5.3.3} presents the effect of the two passage lengths, OpenNLP\footnote{OpenNLP: \url{https://opennlp.apache.org/}} sentence length and Stanford CoreNLP\footnote{Stanford CoreNLP: \url{http://stanfordnlp.github.io/CoreNLP/}} sentence length, on passage retrieval using the BM25 model with or without stemming or UMLS concepts. The results show that passage length has a great impact on passage retrieval using the BM25 model. The highest MAP is obtained using the Stanford CoreNLP sentence/passage length. Moreover, the results also show that features impact the passage retrieval performance. Using stemmed words as features for the BM25 model, the performance in terms of MAP was 0.1128, 0.1106, 0.1354, 0.1367, and 0.1349 in Batch 1, Batch 2, Batch 3, Batch 4, and Batch 5, respectively. When UMLS concepts were used as additional features, the overall performance in terms of MAP decreased to 0.1051, 0.1014, 0.1197, 0.1626, and 0.1266 in Batch 1, Batch 2, Batch 3, Batch 4, and Batch 5, respectively. In contrast, the proposed method, i.e., the incorporation of stemmed words and UMLS concepts as features improved the performance to the highest MAP in all batches of testing datasets. Moreover, the increased performance was statistically significant.

In summary, from the results and analysis we can draw a conclusion that our proposed biomedical passage retrieval method allows extracting passages in biomedical QA with high mean average precision. We attribute this to the Stanford CoreNLP passage length, stemmed words and UMLS concepts features used in this work, and the utilization of the BM25 model. The biomedical passage retrieval module plays a vital role in QA systems since it allows retrieving the top-ranked passages which serve as candidate answers and the biomedical QA system selects the answers from them. It has a significant impact on the overall performance of a biomedical QA system. Therefore, by improving the performance of passage retrieval, the overall performance of biomedical QA systems will inevitably improve too.

\section{Summary of the Chapter}


In this chapter we have presented the methods we propose for document and passage retrieval in biomedical QA which aim at retrieving a set of relevant documents and passages to given biomedical questions, respectively.


In section~\ref{Chapter5.2} we described in details the proposed document retrieval method in biomedical QA. The proposed method consists of three stages, namely, query formulation, querying the MEDLINE database, and document reranking. The query formulation stage uses MetaMap to identify UMLS concepts so as to form the query from the input biomedical question. In the second stage, querying the MEDLINE database, we have used a typical IR system which retrieves a set of possibly relevant PubMed documents to the query. As there are many cases where the IR system mistakenly returns irrelevant PubMed documents high in the set or relevant PubMed documents low in the set, we have developed in the last stage a document reranking system based on UMLS similarity to re-rank the retrieved set of documents and move the most relevant documents to the top of the set. Experimental evaluations performed on large standard dataset, provided by the BioASQ challenge, have shown that the proposed method is more effective as compared with the state-of-the-art methods.

In section~\ref{Chapter5.3} we described in details the proposed passage retrieval method in biomedical QA which employs the Stanford CoreNLP sentence boundary as a passage length, stemmed words and UMLS concepts as features for the BM25 model to retrieve relevant passages in biomedical QA. For a given biomedical question,  we first have used PubMed search engine to retrieve relevant documents and reranked them based on UMLS similarity between concepts of the biomedical question and each title of the retrieved documents. We then have taken the abstracts of top-ranked documents and used Stanford CoreNLP for sentence splitter to generate a set of candidate passages. The passage length is similar to that of the Stanford CoreNLP sentence. We finally have built a new ranking of the set of candidate passages and kept the $N$ top-ranked ones using stemmed words and UMLS concepts as features for the BM25 model. Experiments conducted on large standard datasets provided by the BioASQ challenge have demonstrated the effectiveness of our method. The evaluation results show that the proposed method can achieve 6.84\% improvement over the current state-of-the-art methods in terms of MAP. 