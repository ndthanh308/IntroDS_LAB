% Chapter 1
%\addcontentsline{toc}{chapter}{Résumé de la Thèse en Français}
%\setcounter{section}{0}

\chapter{Résumé détaillé de la thèse en Français} % Main chapter title

\label{Chapter8} % For referencing the chapter elsewhere, use \ref{Chapter1}
\setcounter{secnumdepth}{4}
\begin{spacing}{2}
\begin{flushleft}
{\raggedright \Huge\bfseries Contributions à l'amélioration des systèmes de questions-réponses en domaine biomédical}
\end{flushleft}
\vspace*{20pt}

\end{spacing}

\minitoc

\section{Introduction}
\label{Chapter8.1}

Au cours des dernières décennies, la recherche dans le domaine biomédical a suscité un intérêt croissant de la part de la communauté de recherche, reflétée par une croissance exponentielle de la littérature scientifique. En effet, des milliers d'articles sont publiés et ajoutés chaque année à la base de données MEDLINE. Par exemple, plus de 806 000 et 869 000 citations ont été ajoutées en 2015 et 2016 respectivement. MEDLINE est la première base de données de la littérature biomédicale développée par la bibliothèque nationale de médecine des États-Unis (NLM). Elle contient plus que 24 millions de références d'articles de journaux en sciences de la vie axés sur la biomédecine. Avec cette augmentation explosive de la quantité de connaissances scientifiques, l'absorption automatique de toutes les informations pertinentes est devenue un défi, même pour les experts dans ce domaine. Dans ce contexte, des recherches récentes telles que celles de \cite{hristovski2015biomedical}, ont souligné que les moteurs de recherche spécialisés (e.g., PubMed) qui donnent l'accès à la base de données MEDLINE sont largement utilisés. Pour une requête constituée essentiellement de mots-clés ou de concepts, ces moteurs retournent un ensemble de documents potentiellement pertinents et délèguent à l’utilisateur la tâche de trouver l'information recherchée. En effet, l'ensemble des documents récupérés représente une taille de réponse encore trop grande pour pouvoir identifier facilement l'information recherchée. De plus, l'utilisateur doit souvent faire face à la charge d'étude et au filtrage des documents renvoyés à leur requête afin de trouver l’information recherchée si elle existe dans ces documents. Cette démarche, souvent laborieuse, nécessite un effort supplémentaire de la part de l’utilisateur et entraîne une perte de temps considérable, sans pour autant avoir la garantie de trouver la réponse correcte. Dans ce sens, une étude d'évaluation présentée par \cite{ely2000taxonomy} a montré que les médecins consacraient en moyenne moins de deux minutes à chercher l'information pertinente pour répondre à des questions cliniques, bien que bon nombre de leurs questions restent sans réponse. Dans une autre étude réalisée par \cite{Hersh283}, au moins 30 minutes sont nécessaires en moyenne pour les étudiants en médecine et les infirmières praticiennes pour répondre aux questions cliniques en utilisant MEDLINE.

Pour faciliter, accélérer et améliorer la recherche d’information, des systèmes de questions-réponses (SQR) sont mis en œuvre. Contrairement aux systèmes de recherche d’information traditionnels qui retournent un ensemble de documents potentiellement pertinents à une requête donnée, les SQR visent à fournir des réponses directes et précises à partir d’une collection de documents, du Web ou d’une base de données à des questions posées en langue naturelle. En effet, les SQR nécessitent l’utilisation des méthodes complexes d’extraction d’information et de traitement automatique de la langue.

Typiquement, les systèmes de questions-réponses se composent de quatre composants comme le montre la figure~\ref{fig:QR}: (1) analyse et classification des questions, (2) sélection des documents pertinents, (3) recherche des passages pertinents, et (4) extraction des réponses.


% Figure environment removed

L’entrée d’un SQR est une question formulée en langue naturelle. Le composant \emph{d'analyse et classification des questions} exploite des techniques d’analyse linguistique et de classification des questions pour déterminer à la fois le type de la question posée au système et le type de la réponse qu’il doit générer. Souvent, il peut y avoir plus de processus impliqués dans cette phase, tels que la reconnaissance des entités nommées et l'extraction des relations sémantiques. Le résultat de ce composant est une ou plusieurs caractéristiques de la question à utiliser dans les phases ultérieures. Le SQR construit ensuite à partir de la question une requête qui constitue l'entrée de deuxième module de \emph{sélection des documents pertinents}. Celui-ci fait appel à un moteur de recherche afin de trouver un ensemble de documents pertinents à la requête. Ensuite, à partir de ces documents, le processus de \emph{recherche des passages} procède à l'extraction des passages pertinents susceptibles de contenir les réponses candidates. Enfin, le module de \emph{traitement des réponses} ordonne les réponses candidates selon un score calculé en fonction du type de la réponse attendue qui a été déterminé par le module \emph{d'analyse et classification des questions}. La réponse la mieux classée parmi la liste des réponses candidates constitue la sortie du système de questions-réponses. Cette réponse finale est souvent accompagnée du texte brut à partir duquel la réponse a été extraite.

Bien que la recherche automatique des réponses aux questions en domaine ouvert est un défi largement étudié \citep{Green_1961,Woods_1973,Katz_2002,Kaisser_2008}, le domaine biomédical nécessite encore des efforts supplémentaires. En effet, les systèmes de questions-réponses dans le domaine ouvert concernent des questions qui ne sont pas limitées à un domaine spécifique, alors que les systèmes de questions-réponses dans un domaine spécifique, tel que le domaine biomédical, se rapporte à un contexte particulier et des problèmes plus complexes pour le traitement des questions et des réponses. \cite{athenikos2010biomedical} décrivent les caractéristiques particulières des systèmes de questions-réponses en domaine biomédical comme suit:
\begin{itemize}
  \item Corpus textuel de grande taille.
  \item Terminologie très complexe spécifique au domaine.
  \item Ressources lexicales, terminologiques et ontologiques spécifiques au domaine.
  \item Outils et méthodes pour exploiter les informations sémantiques incorporées dans les ressources lexicales, terminologiques et ontologiques.
  \item Format et typologie des questions spécifiques au domaine.
\end{itemize}




À la lumière de toutes ces caractéristiques, plusieurs travaux se sont intéressés à la problématique de recherche automatique de réponses aux questions dans le domaine biomédical. Certains ont proposé des systèmes pour répondre uniquement aux questions de type résumé ou générer des résumés pour tous les types de questions (e.g., \cite{lee2006beyond,cruchet2009trust,gobeill2009question,Cao_2011,Kraus_2017}). D’autres travaux se sont penchés sur les questions factuelles (e.g., \cite{abacha2015means}). Un nombre limité de systèmes s’est intéressé au traitement de plusieurs types de questions (e.g., \cite{neves2015hpi,zhang2015fudan,yang2015learning}). Cependant, ces systèmes exigent aux utilisateurs la spécification préalable du type de la question. De plus, malgré l'importance des questions booléennes dans le domaine biomédical, peu d'études (e.g., \cite{neves2015hpi}) ont été réalisées par rapport aux autres types de questions, comme les questions factuelles et les questions de type résumé. Même s'il n'y a que deux réponses possibles, «oui» ou «non», de telles questions sont parfois difficiles à aborder puisque leur traitement nécessite généralement des techniques complexes tel que l’analyse de sentiments, etc. Les systèmes de questions-réponses devraient prendre en considération les différents types de questions et leurs réponses appropriées.

Ce travail de thèse s'inscrit dans le cadre des systèmes de questions-réponses dans le domaine biomédical où plusieurs défis spécifiques sont relevés. Nous nous intéressons à l'étude et l’amélioration des méthodes permettant la recherche des réponses précises à des questions biomédicales dans une base de documents biomédicaux en langue anglaise. L'amélioration de la performance des systèmes de questions-réponses dépend de celle de chacune de leurs composantes. Notre objectif est la proposition d’un nouveau système de questions-réponses capable de traiter une variété de types de questions, y compris les questions booléennes, les questions factuelles, les questions de type liste et les questions de type résumé qui sont largement abordées en domaine biomédical. Nous avons apporté quatre contributions dans le cadre de ce travail de thèse:



\begin{enumerate}
\item Dans la première contribution, nous avons proposé une méthode de classification des questions permettant de déterminer le type de la question formulée en langue naturelle \citep{kdir15,Sarrouti_MIM_2017}. Etant basée à la fois sur les patrons lexico-syntaxiques et l'apprentissage automatique, celle-ci est exploitée par le SQR dans la phase d'extraction de la réponse finale. Dans le but de déterminer le type sémantique de la réponse attendue (i.e., un ou plusieurs sujets), nous avons proposé une variante de cette méthode \citep{Sarrouti_IBRA_2017} qui s’appuie sur d’autres caractéristiques de type lexical, morpho-syntaxique et sémantique. Le type sémantique de la réponse attendue d'une question biomédicale représente le sujet de la question tel que: «pharmacologie», «analyse», «traitement», etc. L'objectif est de permettre la réduction du nombre de documents parcourus lors de la recherche des réponses candidates.

\item La deuxième contribution consiste à suggérer une méthode de recherche des documents pertinents susceptibles de contenir les réponses aux questions biomédicales à partir de la base de données MEDLINE \citep{Sarrouti_2016}. Nous avons également proposé une alternative permettant la recherche des passages (i.e., extraits des documents) pertinents à des questions biomédicales données \citep{Sarrouti_2017}.
\item La troisième contribution propose des méthodes d’extraction des réponses appropriées \citep{Sarrouti_yes_2017,Sarrouti_bioasq_2017} permettant d'extraire à la fois les réponses \emph{exactes} et \emph{idéales} aux différents types de questions biomédicales, à savoir (1) les questions booléennes, (2) les questions factuelles, (3) les questions de type liste, et (4) les questions de type résumé.
\item La quatrième contribution consiste à développer un nouveau système de questions-réponses pour le domaine biomédical. En effet, l'ensemble des méthodes proposées sont développées et intégrées au sein d'un système global de questions-réponses, appelé SemBioNLQA. Il accepte en entrée une variété de questions et retourne des réponses appropriées \emph{exactes} et \emph{idéales}.
\end{enumerate}





\section{Méthodes proposées}
\label{Chapter8.3}
Nous proposons dans ce travail de thèse un ensemble de méthodes que nous avons développées et intégrées dans un système de questions-réponses pour le domaine biomédical afin de permettre aux utilisateurs (e.g., chercheurs et professionnels de la santé) de trouver des réponses précises et directes à leurs questions exprimées en langue naturelle. Dans ce qui suit, nous résumons et soulignons les principales contributions et résultats de ce travail de thèse.


\subsection{Méthodes proposées pour la classification des questions biomédicales}

Dans le chapitre~\ref{Chapter4}, nous avons proposé deux méthodes de classification de questions biomédicales à base de l'apprentissage automatique pour déterminer le type de la question ainsi que la catégorie sémantique de la réponse attendue à une question biomédicale donnée.

\subsubsection{Identification des types des questions biomédicales}
\label{Chapter8.3.1}
Dans la section~\ref{Chapter4.2} du chapitre~\ref{Chapter4}, nous avons décrit notre méthode basée sur l'apprentissage automatique pour l'identification des types des questions biomédicales. Celle-ci vise à classifier les questions biomédicales données dans l'une des quatre catégories définies par le challenge BioASQ. Ces types sont: (1) questions booléennes, (2) questions factuelles, (3) questions de type liste, et (4) questions de type résumé.

\begin{itemize}
\item	Questions booléennes: nécessitent seulement une des deux réponses possibles: «oui» ou «non». Par exemple, «Is calcium overload involved in the development of diabetic cardiomyopathy?» est une question booléenne et sa réponse est «oui».
\item	Questions factuelles: nécessitent une entité nommée biomédicale (par exemple, le nom d'une maladie, d'un médicament ou d'un gène), un nombre ou une expression courte comme réponse. Par exemple, «Which enzyme is deficient in Krabbe disease?» est une question factuelle et la réponse attendue est une entité nommée biomédicale «galactocérébrosidase».
\item	Questions de type liste: le type de la réponse attendue pour ce type de questions est une liste d’entités nommées biomédicales (par exemple, une liste de noms de gènes, une liste de noms de médicaments), des nombres ou des expressions courtes. Par exemple, «What are the effects of depleting protein km23–1 (DYNLRB1) in a cell?» est une question de type liste.
\item	Questions de type résumé: attendent un résumé ou un passage court comme réponse. Par exemple, le format de la réponse attendue pour la question «What is the function of the viral KP4 protein?» devrait être un résumé.

\end{itemize}

Le but de l'identification des types des questions dans les systèmes de questions-réponses est de déterminer le type de la question posée au système, et donc d’identifier le type de la réponse attendue pour vérifier si la réponse doit être une entité nommée biomédicale, un résumé, oui ou non, etc. Pour produire la réponse à une question donnée, le système de questions-réponses devrait connaître à l'avance le type de la réponse attendue pour sélectionner une méthode spécifique pour l'extraction de la réponse. Par exemple, pour la question biomédicale «Does nimotuzumab improve survival of glioblastoma patients?» qui admet «oui» ou «non» comme réponse, une méthode spécifique à l’extraction des réponses aux questions booléennes doit être utilisée. En effet, l'identification des types de questions est une tâche très importante car elle peut fortement affecter positivement ou négativement les étapes de traitement ultérieures: si le type de question n'est pas identifié correctement, les étapes du traitement de la réponse échoueront inévitablement.


Dans la méthode proposée, comme le montre la figure~\ref{fig:QTC}, nous avons d'abord extrait les caractéristiques appropriées à partir des questions biomédicales en utilisant nos patrons lexico-syntaxiques prédéfinis et formulés manuellement. Ces patrons ont été construits en analysant les questions d’entrainement fournies par le challenge BioASQ. Nous avons élaboré ces patrons manuellement après l'étape de la tokenisation et l'étiquetage morpho-syntaxique des questions d’entrainement effectuées en utilisant Stanford CoreNLP afin de capturer leur structure syntaxique. En effet, nous avons extrait les mots et leurs classes morphosyntaxiques de chacune des questions d’entrainement BioASQ. Après avoir analysé toutes ces questions appartenant à une catégorie particulière, nous avons proposé les structures syntaxiques possibles. Un patron est une expression régulière décrivant un modèle de question. Par exemple, le patron «{[What $\mid$ Which] + [VBZ] + [*] + [X]+ [*] +?; où X = \{definition, role, aim, effect, influence, mechanism, treatment, ou les synonymes WordNet de ces mots\}}» est l'un des patrons qui permettent de représenter les questions de type résumé. L'idée principale derrière la sélection des caractéristiques employées dans les patrons est que seuls certains mots de la question biomédicale (par exemple, les pronoms interrogatifs) permettent de représenter généralement le type de question.
Considérons par exemple la question biomédicale «What is the definition of autophagy?». Le vecteur de caractéristiques ($v$) de cette question est simplement les patrons qui représentent sa structure syntaxique. Cette question peut être représentée par un ensemble de paires sous forme (caractéristique, fréquence) comme suit: $v= \{(what, 1), (VBZ, 1), (definition, 1)\}$. Pour une question biomédicale donnée, le patron approprié est sélectionné parmi l'ensemble des patrons comme suit: après prétraitement (la tokenisation et l'étiquetage morpho-syntaxique) de la question biomédicale en utilisant Stanford CoreNLP, le patron est apparié à la question biomédicale de gouche à droite. En cas d'échec, nous examinons le patron suivant et ainsi de suite. Nous avons ensuite entraîné un modèle d’apprentissage automatique sur les caractéristiques extraites à partir du corpus d’entraînement. En effet, nous avons exploré plusieurs algorithmes d'apprentissage automatique tels que l’arbre de décision, naïve bayésien et SVM (machine à vecteurs de support). Nous avons retenu le classificateur SVM à cause de sa performance épreuvée en catégorisation des textes. Comme une question peut être attribuée à l'un des quatre types de questions déjà définis, la classification multi-classe a été adoptée dans ce travail. Enfin, nous avons utilisé le modèle de classification appris à l'aide du corpus d'apprentissage pour classifier les questions biomédicales non étiquetées à partir d’autres corpus, dits corpus de test.

% Figure environment removed

Le tableau~\ref{tab:r1f} présente les principaux résultats obtenus sur le corpus standard de questions biomédicales fourni par BioASQ.

\begin{table}[h!]
\selectlanguage{french}

%\scriptsize
\centering
\caption[Résultats obtenus en utilisant SVM sur cinq lots de questions de tests pour attribuer automatiquement une catégorie aux questions biomédicales]{Résultats obtenus en utilisant SVM sur cinq lots de questions de tests pour attribuer automatiquement une catégorie aux questions biomédicales. Nous avons exploré différentes caractéristiques à savoir unigramme, bigramme, étiquetage morpho-syntaxique, étiquetage morpho-syntaxique + unigramme et notre ensemble de patrons lexico-syntaxiques.}
\label{tab:r1f}
\begin{tabular}{M{3cm}M{9cm}M{3cm}}
\hline\noalign{\smallskip}
Corpus de test&	Caractéristiques &Accuracy (\%) \\
\noalign{\smallskip}\hline\noalign{\smallskip}
\multirow{5}{*}{Cinq lots} & Unigramme & 79.48\\[1.5pt]
             & Bigramme &  	65.18\\[1.5pt]
             &Étiquetage morpho-syntaxique&	77.08\\[1.5pt]
             &Étiquetage morpho-syntaxique+unigramme&	80.08\\[1.5pt]
             &\textbf{Patrons lexico-syntaxiques proposés}&	\textbf{89.40}\\[1.5pt]
\noalign{\smallskip}\hline
\end{tabular}
\end{table}

Les résultats obtenus ont montré que la méthode proposée améliore significativement trois systèmes de l'état de l'art. La méthode proposée a atteint une augmentation d'environ 10 points par rapport au meilleur système de base en termes de précision. De plus, la précision obtenue est de 89.40\% qui représente des améliorations statistiquement significatives par rapport aux systèmes de base.

En général, la performance des approches fondées sur les modèles supervisés dépend à la fois de la présence d’un corpus d’apprentissage bien annoté et la sélection d’un ensemble pertinent d’attributs. Dans le corpus BioASQ utilisé, nous avons identifié certains problèmes d'ambiguïtés des questions. Par exemple, la question biomédicale «Which are the mutational hotspots of the human KRAS oncogene?» qui figure deux fois dans le corpus d'apprentissage est étiquetée avec la catégorie «résumé» et aussi avec la catégorie «liste». Ce problème impacte négativement la performance de la classification des types de questions.

\subsubsection{Identification des sujets des questions biomédicales}
\label{Chapter8.3.2}

Dans la section~\ref{Chapter4.3} du chapitre~\ref{Chapter4}, nous avons proposé une variante basée sur l’apprentissage automatique pour classifier des questions biomédicales en plusieurs sujets. Celle-ci vise à assigner automatiquement un ou plusieurs sujets appelés aussi catégories sémantiques (e.g., pharmacologique, thérapeutique, étiologique, etc.) à des questions biomédicales. La tâche de classification des questions en plusieurs sujets dans les systèmes de questions-réponses permet de déterminer le type sémantique de la réponse attendue. Cette information permet de réduire le nombre de documents parcourus lors de la recherche des réponses candidates et d'éviter d'examiner tous les passages ou entités nommées biomédicales dans la liste complète de réponses candidates, en se restreignant à une catégorie unique de réponses telle que traitement, test, etc. Par exemple, la question «What is the best way to catch up on the diphtheria pertussis tetanus vaccine (DPT) after a lapse in the schedule?» appartient à la catégorie pharmacologique, et par la suite le système de questions-réponses peut se limiter à la base de données pharmacologiques «Micromedex» comme ressource pour chercher la réponse.

Comme le montre la figure~\ref{fig:QSC}, la méthode proposée extrait d'abord un ensemble de caractéristiques lexicales, syntaxiques et sémantiques à partir des questions cliniques, y compris les mots bruts, les bigrammes, les racines des mots, les concepts biomédicaux et leurs types sémantiques, et les relations syntaxiques de dépendance. Nous avons utilisé Porter stemmer \citep{Porter_1980}, le parseur Stanford \citep{de2006generating} et MetaMap \citep{aronson2001effective} pour extraire respectivement les racines des mots, les relations de dépendance syntaxique, et les concepts biomédicaux et leurs types sémantiques. Ensuite, nous avons entraîné un modèle d’apprentissage automatique sur les caractéristiques extraites à partir du corpus d’entraînement. En ce qui concerne les modèles d'apprentissage automatique, nous avons exploré plusieurs classificateurs tels que SVM, naïve bayésien et les arbres de décision. Nous avons adopté la classification multi-étiquettes dans cette étude puisqu'une question peut correspondre à un ou plusieurs sujets. Nous avons développé un classificateur d'apprentissage automatique binaire pour chacun des sujets prédéfinis dans le corpus fourni par NLM. Ces sujets sont: \emph{device}, \emph{diagnosis}, \emph{epidemiology}, etiology, \emph{history}, \emph{management}, \emph{ pharmacological}, \emph{physical}, \emph{finding}, \emph{procedure}, \emph{prognosis}, \emph{test}, \emph{treatment} et \emph{prevention}. Nous avons opté pour le classificateur SVM avec le noyau linéaire puisqu'il assure de meilleures performances en catégorisation de textes par rapport aux autres classificateurs \citep{yu2005classifying,yu2008automatically,Cao_2010}. Enfin, pour classifier les questions cliniques non étiquetées, nous avons adopté le modèle de classification appris précédemment en utilisant le corpus des questions cliniques fournies par NLM.


% Figure environment removed


Pour tester notre méthode de classification des sujets de questions, nous avons mené plusieurs expérimentations sur le corpus standard de questions cliniques fourni par NLM. Ce corpus contient 4654 questions cliniques, recueillies auprès des experts en domaine de santé à travers les États-Unis. Les résultats obtenus, rapportés dans le tableau~\ref{tab:r2f}, montrent que la méthode proposée atteint une meilleure performance de 77.18 en termes de F1-score pour la classification des sujets de questions. Ces résultats prouvent que notre méthode est plus efficace que les méthodes actuelles présentées dans \citep{yu2008automatically,Cao_2010} et les surpasse d'une moyenne de F1-score de 4.5\% et 4.73\% respectivement en utilisant naïve bayésien et SVM. Les détails sur les expérimentations menées ainsi que les résultats obtenus sont présentés à la section~\ref{Chapter4.3} du chapitre~\ref{Chapter4}.


\begin{landscape} % <-- HERE
\centering
\begin{table}
\selectlanguage{french}
\centering
\caption[Résultats obtenus en utilisant SVM pour attribuer automatiquement des sujets à des questions cliniques]{Résultats obtenus en utilisant SVM pour attribuer automatiquement des sujets à des questions cliniques. Nous avons exploré différentes caractéristiques à savoir unigrame (BOW), bigramme (BOB), racines des mots (BOS) en utilisant Porter et Krovetz, entités nommées biomédicales (BOBNE), BOW+les concepts biomédicaux et leurs types sémantiques (BOCST), BOW+les relations syntaxiques de dépendance (BOSDR), BOW+BOB+BOCST+BOS$_{porter}$ utilisés dans \citep{yu2008automatically,Cao_2010}, BOW+BOB+BOCST+BOS$_{krovetz}$, et la représentation proposée BOW+BOB+BOCST+BOSDR+BOS$_{porter}$.}
\label{tab:r2f}
\begin{tabular}{M{4.5cm}M{1.2cm}M{1.2cm}M{1.2cm}M{1.1cm}M{1.1cm}M{1.2cm}M{1.4cm}M{1.6cm}M{1.6cm}M{1.6cm}}
\hline\noalign{\smallskip}
 \multirow{4}{*}{Topics}&\multicolumn{9}{c}{Features} \\\cmidrule(l){2-11}
&&&&&&&\multicolumn{2}{c}{\thead{\cite{yu2008automatically}\\ \cite{Cao_2010}}}&\multicolumn{2}{c}{\thead{Proposed method}}\\\cmidrule(l){8-9}\cmidrule(l){10-11}
 &\multirow{2}{*}{BOW}& \multicolumn{2}{c}{BOS}& \multirow{2}{*}{BOBNE} &\multirow{2}{*}{\thead{BOW+\\BOCST}}& \multirow{2}{*}{\thead{BOW+\\BOSDR}}& \multicolumn{2}{c}{\thead{BOW+BOB+BOCST+}}& \multicolumn{2}{c}{\thead{BOW+BOB+BOSDR+BOCST+}}\\
\cmidrule(l){3-4}\cmidrule(l){8-9}\cmidrule(l){10-11}
&  &Porter& Krovetz&&&&BOS$_{porter}$& BOS$_{krovetz}$&BOS$_{porter}$& BOS$_{krovetz}$    \\

\noalign{\smallskip}\hline\noalign{\smallskip}
Device&57.89\%&58.23\%&57.69\%&56.31\%&65.60\%&60.10\%&74.01\%&73.50\%&74.99\%&74.44\%\\
Diagnosis&74.22\%&74.24\%&73.14\%&70.38\%&75.63\%&75.19\%&77.13\%&75.33\%&78.10\%&77.05\%\\
Epidemiology&71.15\%&70.58\%&68.10\%&54.96\%&72.03\%&71.97\%&74.74\%&72.31\%&75.93\%&73.53\%\\
Etiology&80.31\%&80.67\%&78.64\%&75.05\%&81.02\%&80.95\%&82.47\%&80.07\%&83.11\%&81.67\%\\
History&52.72\%&55.69\%&55.03\%&50.96\%&58.57\%&54.31\%&67.18\%&66.52\%&68.75\%&68.09\%\\
Management&69.70\%&69.48\%&68.51\%&65.07\%&70.13\%&70.02\%&71.07\%&70.16\%&71.49\%&70.10\%\\
Pharmacological&82.41\%&82.83\%&82.16\%&76.20\%&84.66\%&83.04\%&84.71\%&84.04\%&84.85\%&84.19\%\\
Physical\&Finding&72.10\%&72.27\%&71.08\%&70.62\%&76.09\%&73.14\%&78.82\%&77.01\%&79.35\%&78.38\%\\
Procedure&69.56\%&70.08\%&68.81\%&68.18\%&75.47\%&71.32\%&78.68\%&77.42\%&79.12\%&78.35\%\\
Prognosis&72.68\%&73.68\%&72.13\%&69.27\%&73.89\%&72.87\%&74.03\%&72.51\%&74.17\%&73.61\%\\
Test&79.97\%&80.14\%&76.40\%&75.02\%&81.15\%&80.52\%&83.22\%&82.48\%&83.64\%&81.90\%\\
Treatment\&Prevention&68.19\%&69.00\%&67.40\%&65.16\%&69.99\%&69.21\%&71.73\%&70.10\%&72.63\%&71.91\%\\\cmidrule(l){1-11}
Average&70.91\%&71.41\%&69.92\%&66.43\%&73.68\%&71.89\%&76.48\%&75.12\%&77.18\%&76.10\%\\

\noalign{\smallskip}\hline
\end{tabular}
\end{table}

\end{landscape}


\subsection{Méthode proposée pour la sélection des documents pertinents}


Dans la section~\ref{Chapter5.2} du chapitre~\ref{Chapter5}, nous avons proposé une méthode de la recherche, à partir de la base de données MEDLINE, des documents susceptibles de contenir la réponse à une question biomédicale donnée.

La recherche de documents pertinents pour une requête se fait en utilisant un moteur de recherche existant. À une requête donnée, un moteur de recherche retourne un ensemble de documents, en attribuant un score d’appariement à chaque document. Plus le score est élevé, plus le document est pertinent. Cependant, dans certains cas le moteur de recherche renvoie des documents non pertinents à l'entête de la liste des documents retournés (avec un score élevé), ou aussi des documents pertinents mais à la fin de la liste (avec un score réduit). Ce qui se répercute négativement sur la performance des SQR puisque l'extraction des réponses se fait à partir des documents qui figurent à l'entête de la liste.

Pour remédier à ce problème, nous avons présenté une méthode de recherche des documents pertinents qui se base sur le reclassement de documents. La figure~\ref{fig:RD} illustre les différentes étapes de la méthode proposée.

% Figure environment removed

Notre méthode de recherche des documents pertinents à une question biomédicale est constituée de plusieurs étapes. D'abord, nous avons construit la requête à partir de la question en utilisant les concepts UMLS correspondant aux termes de la question extraits par l’outil MetaMap. Ensuite, nous procédons à la recherche des documents potentiellement pertinents à la requête, que nous nous avons construite, en utilisant le moteur de recherche GoPubMed qui accède à la base de données MEDLINE. Enfin, une technique de reclassement de documents est effectuée pour reclasser la liste des documents préalablement obtenue. Notre technique de reclassement repose sur la similarité sémantique entre la question et les titres des documents retournés dans le but d'attribuer un score pour chaque document. Pour calculer les scores de similarité sémantique, nous avons d’abord identifié les concepts biomédicaux contenus à la fois dans la question et les titres des documents déjà obtenus en utilisant l'outil MetaMap qui se réfère au metathesaurus l’UMLS. Ensuite, les scores entre les concepts de la question et ceux des titres des documents sont calculés sur la base de la distance de similarité sémantique qui correspond à la longueur du chemin (Path Length) dans l’ontologie Mesh.

Pour évaluer notre méthode, nous avons mené plusieurs expérimentations sur le corpus standard de questions biomédicales fourni par BioASQ. Comme mesures d'évaluation, nous avons utilisé le rappel, précision, F1-mesure et précision moyenne (MAP). Le tableau~\ref{tab:r4f} présente les résultats obtenus en comparaison aux principales méthodes de l'état de l'art. Nous pouvons ramarquer que notre méthode est plus efficace que les autres méthodes présentées dans \citep{balikas2014results} qui ont été classées parmi les 10 premiers systèmes lors du challenge BioASQ. Les détails concernant les expérimentations menées ainsi que les résultats obtenus sont présentés à la section~\ref{Chapter5.2} du chapitre~\ref{Chapter5}.

\begin{table}[h!]
\centering
\selectlanguage{french}
\caption{Comparaison des résultats expérimentaux du système de recherche de documents proposé et des systèmes de l'état de l'art présentés dans \citep{balikas2014results}.}
\label{tab:r4f}
\begin{tabular}{p{4cm}p{2.7cm}p{2.5cm}p{3.1cm}p{1.9cm}}
\hline \noalign{\smallskip}
Système &Précision &Rappel & F1-mesure &MAP  \\
\noalign{\smallskip}\hline\noalign{\smallskip}
\textbf{Notre système}&\textbf{0.2331} &\textbf{0.3644} &\textbf{0.2253} &\textbf{0.2758} \\
SNUMedinfo1 &0.0457& 0.5958 &0.0826& 0.2612 \\
SNUMedinfo3 &0.0457 &0.5947 &0.0826 &0.2587 \\
SNUMedinfo2 &0.0451 &0.5862 &0.0815 &0.2547\\
SNUMedinfo4 &0.0457 &0.5941 &0.0826 &0.2493\\
SNUMedinfo5 &0.0459 &0.5947 &0.0829 &0.2410\\
Top 100 Baseline &0.2274 &0.4342 &0.2280 &0.1911 \\
Top 50 Baseline &0.2290 &0.3998 &0.2296 &0.1888 \\
Main system &0.0413 &0.2625 &0.0678 &0.1168 \\
Biomedical Text Ming &0.2279 &0.2068 &0.1665 &0.1101  \\
Wishart-S2 &0.1040 &0.1210 &0.0793 &0.0591 \\

\noalign{\smallskip}\hline

\end{tabular}
\end{table}



\subsection{Méthode proposée pour la recherche des passages pertinents}
\label{Chapter8.3.3}

Dans la section~\ref{Chapter5.3} du chapitre~\ref{Chapter5}, nous avons proposé une méthode pour la recherche des passages susceptibles de contenir la réponse à une question biomédicale donnée. La définition d'un passage dépend nécessairement du système de recherche des passages, généralement un passage pourrait être une section, un paragraphe ou une phrase. En particulier, un système de recherche des passages peut être défini comme étant un type d’application spécifique de la recherche d’information qui permet de récupérer un ensemble de passages plutôt que de fournir un ensemble de documents. Son but principal est d’extraire et de retourner les passages les plus pertinents qui servent comme des réponses candidates à partir desquelles le système de questions-réponses extraira et sélectionnera la réponse.

Bien que la recherche des passages soit une tâche difficile, elle reste l'une des composantes les plus importantes pour le développement d'un système de questions-réponses. Cela s'explique par le fait que la performance globale d'un SQR dépend fortement de l'efficacité de son processus de recherche des passages. En effet, si le module de recherche des passages ne parvient pas à trouver au moins un passage pertinent pour une question donnée, l’étape de traitement ultérieure pour l’extraction de la réponse finale échouera inévitablement.

Pour sélectionner les passages pertinents dans un SQR, nous avons proposé une nouvelle méthode de recherche des passages. Comme le montre la figure~\ref{fig:RP}, notre méthode comporte deux étapes, : (1) la recherche des documents, et (2) l’extraction des passages. La recherche des documents pertinents à une question se fait conformément à notre module précédent de recherche des documents. Ensuite, les passages pertinents sont extraits à partir des résumés de ces documents les plus pertinents. Pour ce faire, nous avons segmenté les résumés des documents sélectionnés en des phrases en utilisant l’outil Stanford CoreNLP \citep{Manning_2014}. L'ensemble des phrases obtenues constitue la liste des passages candidats. La longueur des passages est similaire à celle des phrases. Les scores sont calculés pour tous les passages candidats en fonction de leur degré d’appariement (ou de pertinence) avec la question en se basant sur le modèle BM25 introduit par \cite{Robertson96okapiat}. Ce modèle est le plus connu en recherche d’information. Comme caractéristiques représentatives des passages, nous avons combiné les racines des termes établies en utilisant le stemmer Porter \citep{Porter_1980}, et les concepts biomédicaux établis en utilisant MetaMap qui se réfère au metathesaurus de l’UMLS (version 2016AA). De plus, les mots vides sont éliminés au préalable et la désambiguïsation sémantique des concepts avec MetaMap \citep{Aronson_2010} est appliquée lors de l'extraction des concepts UMLS. Enfin, après avoir calculé les scores de pertinence pour tous les passages candidats, nous avons retenu les passages ayant les scores les plus élevés constituant ainsi les passages les plus pertinents.


% Figure environment removed

Pour montrer l’efficacité de notre méthode, nous avons effectué un ensemble d’expérimentations sur un corpus d’évaluation standard fourni par le challenge BioASQ. Comme indicateurs d'efficacité, nous avons utilisé les mesures d'évaluation des systèmes de recherche d'information les plus connues à savoir le rappel, la précision, F1-mesure et la précision moyenne (MAP). Le tableau~\ref{tab:r5f} présente les résultats obtenus en termes de MAP en comparaison aux principaux systèmes de l'état de l'art. Ces résultats montrent clairement que la méthode proposée a abouti à des meilleures performances par rapport aux autres méthodes. De plus, notre méthode apporte une amélioration de la performance statistiquement significative qui atteint 6.84\% en moyenne par rapport aux autres méthodes de la littérature présentées dans \citep{neves2015hpi,yenalaiiith,ligeneric,yang2015learning}. D'autres expérimentations détaillées sont présentés en section~\ref{Chapter5.3} du chapitre~\ref{Chapter5}.

\begin{table}[h!]
\centering
\selectlanguage{french}
\caption[Comparaison en termes de MAP de la méthode de recherche des passages proposée avec celles de l'état de l'art sur cinq lots de jeux de données de test fournis par le challenge BioASQ]{Comparaison en termes de MAP de la méthode de recherche des passages proposée avec celles de l'état de l'art sur cinq lots de jeux de données de test fournis par le challenge BioASQ. Le signe «-» remplace les scores des systèmes qui n'ont pas été évalués sur certains lots.}
\label{tab:r5f}
\begin{tabular}{M{4.2cm}M{1.9cm} M{1.9cm}M{1.9cm}M{1.9cm}M{1.9cm}}
\hline \noalign{\smallskip}
\multirow{2}{*}{Méthodes}& \multicolumn{5}{c}{MAP@10} \\
\cmidrule(l){2-6}
& Lot 1 & Lot 2 &Lot 3 &Lot 4&Lot 5 \\
\noalign{\smallskip}\hline\noalign{\smallskip}
\textbf{Méthode proposée}&\textbf{0.1178}&\textbf{0.1180}&\textbf{0.1491}&\textbf{0.1633}&\textbf{0.1437}\\\cmidrule(l){1-6}
\cite{neves2015hpi}&0.0347 (+8.31\%)& 0.0355 (+8.3\%)& 0.0452 (+10.39\%)& 0.0624 (+10.09\%)& 0.0572 (+8.65\%)\\\cmidrule(l){1-6}
\cite{yenalaiiith}&0.0545 (+6.33\%)& 0.0709 (+4.71\%)& -& 0.0913 (+7.2\%)& -\\\cmidrule(l){1-6}
\cite{ligeneric}&0.0570 (+6.08\%)& 0.0521 (+6.59\%)& 0.0832 (+6.59\%)& 0.0936 (+6.97\%)& 0.1201 (+2.36\%)\\\cmidrule(l){1-6}
\cite{yang2015learning}&-&-&0.0892 (+5.99\%)&0.0957 (+6.76\%)&0.1027 (+4.1\%)\\
\noalign{\smallskip}\hline

\end{tabular}
\end{table}

\subsection{Méthodes proposées pour l'extraction de la réponse}
\label{Chapter8.3.4}

Dans la section~\ref{Chapter6.2} du chapitre~\ref{Chapter6}, nous avons proposé des nouvelles méthodes d'extraction des réponses. Celles-ci permettent d'extraire des réponses \emph{exactes} et \emph{idéales} (résumés) aux questions biomédicales exprimées en langage naturel.

L’extraction des réponses est la dernière étape pour répondre à une question visant à traiter l’ensemble des réponses candidates afin d’en extraire ou générer la (les) réponse (s) finale (s). Ce processus constitue l’étape la plus difficile d’un système de questions-réponses puisqu'il s'agit d'extraire la réponse finale qui devrait être une réponse courte et précise. A titre d'exemple, la question biomédicale «What is the cause of Phthiriasis Palpebrarum?» admet comme réponse «Pthirus pubis». En général, les réponses appropriées aux questions des utilisateurs devraient être extraites selon le type de la question donnée. Autrement dit, la nature de la réponse varie suivant le type de la question. Par exemple, une question biomédicale comme «Does the histidine-rich Ca-binding protein (HRC) interact with triadin?» s’attend à une réponse de type «oui» ou «non», alors qu'une question biomédicale comme «What is the role of edaravone in traumatic brain injury?» s’attend à une réponse de type «résumé».

Dans ce travail de thèse, notre but final est de développer un nouveau système de questions-réponses qui a la capacité de traiter quatre types de questions à savoir les questions booléennes, les questions factuelles, les questions de type liste et les questions de type résumé. Ces types de questions biomédicales devraient couvrir toute sorte de questions potentielles qui peuvent être posées par les utilisateurs. Dans la section~\ref{Chapter8.3.5}, nous avons développé des nouvelles méthodes d’extraction des réponses pour chaque type de question défini précédemment.

Pour les questions booléennes, nous nous sommes basés sur le lexique de sentiment SentiWordNet \citep{baccianella2010sentiwordnet} pour extraire la réponse exacte («oui» ou «non»). SENTIWORDNET qui est une ressource lexicale pour l'analyse des sentiments et l'exploration de l'opinion, assigne à chaque synset de WordNet trois scores de sentiment: «positivité», «négativité» ou «objectivité». L'idée clé derrière l'utilisation de la ressource lexicale SentiWordNet 3.0 est qu'elle est le résultat de l'annotation automatique de tous les synsets de WordNet 3.0 selon les notions de «positivité», «négativité» et «neutralité». WordNet, qui est une grande base de données lexicale, comprend 155 287 mots et 117 659 synsets, appelés également synonymes \citep{Miller_1995}. En effet, dans la méthode proposée, un score SentiWordNet est attribué à chaque mot des passages pertinents, et la décision de retourner «oui» ou «non» dépend du nombre de réponses candidates positives ou négatives. Pour les questions factuelles, la réponse précise est produite en identifiant les entités nommées biomédicales qui figurent dans les réponses candidates, et en rapportant les cinq entités nommées biomédicales les plus fréquentes et leurs synonymes. En effet, nous avons d'abord identifié les entités nommées biomédicales en utilisant MetaMap qui se réfère au metathesaurus de l’UMLS (version 2016AA). Nous avons ensuite classé l'ensemble obtenu des entités nommées biomédicales en utilisant la fréquence du terme. La fréquence d'une entité est simplement le nombre d'occurrences de cette entité dans la liste des entités considérée. Nous avons exploré plusieurs méthodes de pondération telles que TF-IDF et BM25, en montrant que la fréquence du terme a atteint le meilleur résultat pour cette tâche. Enfin, les entités nommées biomédicales les mieux classées et leurs synonymes sont considérés comme réponses à la question factuelle. En effet, nous avons exclu les entités nommées mentionnées dans la question. Les synonymes sont récupérés en utilisant les services Web de BioPortal \footnote{\url{http://data.bioontology.org/documentation}}. Comme décrit par le challenge BioASQ, une question factuelle a une seule réponse correcte, mais cinq réponses possibles au maximum et leurs synonymes sont autorisés. Pour les questions de type liste, la réponse exacte est produite de la même manière, sauf que les entités nommées les plus fréquentes et leurs synonymes les plus pertinents sont cette fois renvoyées sous forme de liste qui répond à la question. En effet, la réponse exacte est la même pour les questions factuelles, mais l'interprétation est différente pour les questions de type liste: toutes les entités les mieux classées sont considérées comme faisant partie de la même réponse à une question de type liste et non pas comme candidates. Pour générer des réponses idéales, nous avons d’abord appliqué MetaMap aux passages pertinents et à la question biomédicale afin d’obtenir les concepts biomédicaux auxquels ils se réfèrent. Nous avons ensuite classé ces passages par leur pertinence à la question en utilisant les racines des mots et les concepts biomédicaux comme termes d’index et le modèle BM25 pour calculer les scores d’appariements entre la question et les passages. Enfin, nous avons retourné les deux meilleurs passages (concaténés) comme réponse idéale. Les détails sur ces méthodes d'extraction des réponses sont présentés dans la section~\ref{Chapter6.2} du chapitre~\ref{Chapter6}.


Nous avons testé les méthodes d'extraction des réponses proposées dans le cadre du challenge BioASQ 2017. La tâche demandée consistait à extraire des réponses exactes et idéales aux questions biomédicales posées en langage naturel. Plusieurs systèmes ont participé
en proposant différentes approches. Nous avons également évalué nos méthodes sur un ensemble de questions fournies par BioASQ dans leurs éditions 2015 et 2016. Comme indicateurs d’efficacité de l'extraction des réponses: l'accuracy a été utilisée pour les réponses exactes des questions booléennes; la moyenne des réciproques du rang (MRR) a été utilisée pour les réponses exactes des questions factuelles; la précision, le rappel et F1-mesure moyenne ont été utilisés pour les réponses exactes des questions de type liste; ROUGE-2 et ROUGE-SU4 ont été utilisés pour des réponses idéales. De plus, pour ces dernières, les experts du challenge BioASQ ont évalué manuellement les réponses retournées par les systèmes participants en termes de la simplicité de leur lecture, de rappel, de précision et de répétition. Les tableaux~\ref{tab:ex1} et~\ref{tab:ex2} présentent les résultats obtenus lors de la participation au challenge BioASQ 2017. Ces résultats sont publiés par le site officiel du challenge BioASQ \footnote{\url{http://participants-area.bioasq.org/results/5b/phaseB/}}.

\begin{table}[h!]
\centering
\selectlanguage{french}
\caption[Résultats obtenus lors de notre participation dans la phase B «réponses exactes» de la tâche 5b du challenge BioASQ 2017 en utilisant les méthodes proposées l'extraction des réponses]{Résultats obtenus lors de notre participation dans la phase B «réponses exactes» de la tâche 5b du challenge BioASQ 2017 en utilisant les méthodes proposées l'extraction des réponses. Les deux premières valeurs mises entre parenthèses indiquent notre classement et le nombre total de soumissions, les deux seconds valeurs entre parenthèses représentent notre classement et le nombre total d'équipes participantes.}
\label{tab:ex1}
\begin{tabular}{p{3cm}p{2.2cm}p{2.2cm}p{2.1cm}p{2.1cm}p{2.1cm}}
\hline\noalign{\smallskip}
 \multirow{2}{*}{Jeux de données} &  Booléennes & Factuelle & \multicolumn{3}{c}{ Liste}   \\
 \cmidrule(l){2-2}\cmidrule(l){3-3} \cmidrule(l){4-6}
  &   Accuracy &  MRR& Précision& Rappel &F1-mesure \\
\noalign{\smallskip}\hline\noalign{\smallskip}
\multirow{2}{*}{Lot 1}  &0.7647&	0.2033 \newline(5/15)\newline (3/9)&	0.1909&	0.2658&	0.2129 (3/15)\newline (2/9) \\
\cmidrule(l){1-6}
\multirow{2}{*}{Lot 2}  &0.7778& 0.0887 \newline(10/21)\newline (5/9)&	0.2400&	0.3922&	0.2920 (6/21)\newline (2/9)\\
\cmidrule(l){1-6}
\multirow{2}{*}{Lot 3}  &0.8387 \newline(1/21)\newline (1/10)& 0.2212 \newline(9/21)\newline (4/10)&	0.2000&	0.4151&	0.2640 (6/21)\newline (3/10)\\
\cmidrule(l){1-6}
\multirow{2}{*}{Lot 4} &0.6207 \newline(2/27)\newline (2/11)& 0.0970 \newline(13/27)\newline (5/11)&	0.1077&	0.2013&	0.1369 (12/27)\newline (5/11)\\
\cmidrule(l){1-6}
\multirow{3}{*}{Lot 5} &0.4615 & 0.2071 \newline(9/25)\newline (3/11)&	0.2091&	0.3087&	0.2438 (11/25)\newline (6/11)\\



\noalign{\smallskip}\hline
\end{tabular}
\end{table}

\begin{table}[h!]
\centering
\selectlanguage{french}
\caption[Résultats obtenus lors de notre participation dans la phase B «Réponses idéales» de la tâche 5b du challenge BioASQ 2017 en utilisant les méthodes proposées pour l'extraction des réponses]{Résultats obtenus lors de notre participation dans la phase B «Réponses idéales» de la tâche 5b du challenge BioASQ 2017 en utilisant les méthodes proposées pour l'extraction des réponses. Les deux premières valeurs mises entre parenthèses indiquent notre classement et le nombre total de soumissions, les deux seconds valeurs entre parenthèses représentent notre classement et le nombre total d'équipes participantes.}
\label{tab:ex2}
\begin{tabular}{p{2.8cm}p{1.9cm}p{2.5cm}p{1.9cm}p{1.2cm}p{1.4cm}p{1.6cm}}
\hline\noalign{\smallskip}
 \multirow{2}{*}{Jeux de données} & \multicolumn{2}{c}{Scores automatiques} &  \multicolumn{4}{c}{Scores manuels}\\
\cmidrule(l){2-3}\cmidrule(l){4-7}
 & ROUGE-2& ROUGE-SU4 & Lisibilité&  Rappel& Précision& Répétition \\
\noalign{\smallskip}\hline\noalign{\smallskip}
\multirow{2}{*}{Lot 1}  &0.4943 \newline(4/15)\newline(2/9)&0.5108 \newline(3/15)\newline(2/9)&3.65 \newline(3/15)\newline(2/9)&	4.42 \newline (3/15)\newline(2/9)&	3.90 \newline(3/15)\newline(2/9)&	3.89 (3/15)\newline(2/9)\\
\cmidrule(l){1-7}
\multirow{2}{*}{Lot 2}  & 0.4579 \newline(4/21)\newline (2/9)& 0.4583\newline (4/21)\newline (2/9)&3.68\newline (6/21)\newline (2/9)&	4.59 (2/21)\newline (2/9)&	4.01 (7/21)\newline (2/9)&	3.91 (7/21)\newline (2/9)\\
 \cmidrule(l){1-7}
\multirow{2}{*}{Lot 3}  & 0.5566 \newline(4/21)\newline (2/10)&	0.5656 \newline(4/21)\newline (2/10)&3.91\newline (6/21)\newline (2/10)&	4.64 \newline (1/21)\newline (2/10)&	4.07 (7/21)\newline (2/10)&	4.00 (7/21)\newline (2/10)\\
 \cmidrule(l){1-7}
\multirow{2}{*}{Lot 4} &0.5895\newline (4/27)\newline (2/11)&	0.5832 \newline(4/27)\newline (2/11)&3.86\newline (7/27)\newline (3/11)&	4.51 \newline (6/27)\newline (3/11)&	4.02 \newline(3/27)\newline (2/11)&	3.95 (6/27)\newline (2/11)\\
\cmidrule(l){1-7}
\multirow{3}{*}{Lot 5} & 0.5772 (7/25)\newline (3/11)&	0.5756\newline (7/25)\newline (3/11)&3.82\newline (8/25)\newline (3/11)&	4.53 (5/25)\newline (2/11)	&3.91 (7/25)\newline (3/11)&	3.90 (7/25)\newline (2/11)\\

\noalign{\smallskip}\hline
\end{tabular}
\end{table}

Les résultats obtenus montrent que les méthodes d'extraction des réponses proposées sont plus compétitives par rapport à celles des systèmes participants. De plus, notre système a été l'un des systèmes gagnants\footnote{\url{http://www.bioasq.org/participate/fifth-challenge-winners}} lors du challenge BioASQ 2017. Les détails concernant les expérimentations menées ainsi que les résultats obtenus sur d'autres jeux de données sont présentés en section~\ref{Chapter6.2} du chapitre~\ref{Chapter6}.


\subsection{SemBioNLQA : un système de questions-réponses sémantique pour le domaine biomédical}
\label{Chapter8.3.5}

Nous avons développé dans ce travail de thèse un système intégrant l'ensemble des méthodes proposées permettant de répondre aux différents types des questions biomédicales posées en langage naturel. Selon la classification des systèmes de questions-réponses présentée par \cite{athenikos2010biomedical}, l’approche adoptée par notre système est une approche sémantique. Notre système de questions-réponses appelé SemBioNLQA est présentée en détail dans la section~\ref{Chapter6.3} du chapitre~\ref{Chapter6}.

Le système SemBioNLQA accepte en entrée une variété de questions posées en langage naturel et génère des réponses appropriées exactes et idéales. Il permet de traiter plusieurs types de questions dans le domaine biomédical: les questions booléennes, les questions factuelles, les questions de type liste et les questions de type résumé. Il fournit des réponses exactes «oui»/«non» pour les questions booléennes, les entités nommées biomédicales pour les questions factuelles et une liste d’entités nommées biomédicales pour les questions de type liste. De plus, il retourne également des réponses idéales pour les questions de type résumé.

Comme nous l'avons déjà mentionné, le système SemBioNLQA comporte quatre composants principaux: (1) analyse et classification des questions, (2) sélection des documents pertinents, (3) recherche des passages pertinents, et (4) extraction des réponses. Nous avons développé le système SemBioNLQA en intégrant les méthodes proposées pour ces quatre composantes. En effet, SemBioNLQA prend en entrée une question biomédicale exprimée en langage naturel et inclut le prétraitement de la question, l’identification du type de la question et le type de la réponse attendue en utilisant notre méthode proposée pour la classification des types des questions biomédicales, ainsi que la construction d’une requête. Ensuite, il se fonde sur nos méthodes de recherche des documents et des passages pour effectuer l’appariement requête-documents, et sélectionner les passages pertinents. Enfin, à partir des passages les mieux classés, SemBioNLQA génère et renvoie à la fois la réponse exacte et idéale en fonction du type de la question et la réponse attendue en utilisant notre composant d’extraction des réponses qui a été l’un des premiers vainqueurs lors de la compétition internationale BioASQ 2017.

Afin d’évaluer l’efficacité de notre système SemBioNLQA et de le comparer à d'autres SQR existants, nous avons effectué deux évaluations différentes: (1) une évaluation automatique sur un corpus d’évaluation standard fourni par le challenge BioASQ, et (2) une évaluation manuelle en terme de qualité des réponses en utilisant un ensemble de questions et leurs réponses à partir du corpus d’apprentissage fourni par le challenge BioASQ pour le comparer aux principaux systèmes existants tels que AskHermes \citep{Cao_2011}, EAGLi \citep{gobeill2009question} et Olelo \citep{Kraus_2017}. Dans l’évaluation automatique, nous avons utilisé l'accuracy comme indicateur d'efficacité pour les réponses exactes aux questions booléennes; la moyenne des réciproques du rang (MRR) a été utilisée pour les réponses exactes aux questions factuelles. De plus, la précision, le rappel et F1-measure moyenne ont été utilisés pour les réponses exactes aux questions de type listes; et ROUGE-2 et ROUGE-SU4 ont été utilisés pour des réponses idéales.

Pour l’évaluation manuelle, nous avons sélectionné au hasard une séquence de 30 questions de l’ensemble des questions d’apprentissage fourni par BioASQ et nous les avons soumis aux quatre systèmes à évaluer: AskHermes \citep{Cao_2011}, EAGLi \citep{gobeill2009question}, Olelo \citep{Kraus_2017} et SemBioNLQA. Cette séquence correspond à 10 questions factuelles, 11 questions de type liste, et 9 questions booléennes. Comme cette évaluation a été effectuée manuellement, nous avons dû limiter le nombre des questions. Pour cela, nous nous sommes limités aux questions factuelles, questions de type liste et les questions booléennes étant donné que les réponses exactes à ces types
de questions sont plus faciles à vérifier manuellement que les questions résumés. Lors cette évaluation, une réponse est considérée correcte si la première entité nommée biomédicale retournée est correcte pour les questions factuelles, au moins l’une des cinq premières entités nommées biomédicales retournées est correcte pour les questions de type liste, ou la valeur booléenne, soit «oui» ou «non», pour les questions booléennes est correcte. En effet, nous avons vérifié manuellement les résultats retournés par chaque système en les comparant aux réponses attendues fournies par le challenge BioASQ. Toutes les réponses retournées par les systèmes sont disponibles pour téléchargement\footnote{\url{https://sites.google.com/site/mouradsarrouti/datasets}}. Le tableau~\ref{tab:ex3} présente les résultats de notre système SemBioNLQA comparé aux trois systèmes de l'état de l'art.




\begin{table}[h!]
\centering
\selectlanguage{french}
\caption{Comparaison des résultats obtenus par notre système SemBioNLQA, EAGLi, AskHERMES et Olelo en terme du nombre de questions reconnues et de réponses correctes.}
\label{tab:ex3}
\begin{tabular}{M{5.5cm}M{4.9cm}M{4.7cm}}
\hline\noalign{\smallskip}
Systèmes& Nombre de questions reconnues&Nombre de réponses correctes\\
\noalign{\smallskip}\hline\noalign{\smallskip}
EAGLi \citep{gobeill2009question}&7/30&3/30\\
AskHERMES \citep{Cao_2011}&12/30&2/30\\
Olelo \citep{Kraus_2017}&30/30&6/30\\
SemBioNLQA&30/30&18/30\\
\noalign{\smallskip}\hline
\end{tabular}
\end{table}


Les résultats obtenus montrent que SemBioNLQA assure des meilleurs résultats par rapport aux systèmes évalués et permet de répondre correctement à la majorité des questions sélectionnées aléatoirement à partir du corpus BioASQ. Contrairement au système Olelo qui a retourné des résumés comme réponses à la plupart des questions, et AskHERMES qui a renvoyé des passages comme réponses à toutes les questions, notre système SemBioNLQA a réussi à fournir les réponses exactes en fonction de la réponse attendue pour chaque type de question. En effet, SemBioNLQA a pu identifier les types de questions données et généré des réponses exactes en fonction de la réponse attendue pour chaque type de question. Cela indique que l’intégration de notre méthode de classification des questions permet à SemBioNLQA de mieux analyser et de reconnaître correctement les questions posées par les utilisateurs. En revanche, même si Olelo a été développé pour traiter les questions factuelles, les questions de type liste et les questions de type résumé, il n’était pas capable d'identifier les types des questions, et donc il a généré des résumés pour tous les types de questions. Cela oblige les utilisateurs à lire ces résumés pour trouver les réponses précises.

Par ailleurs, les résultats obtenus ont montré que les différents composants de notre système SemBioNLQA améliorent significativement la tâche d’extraction des réponses et donc la performance globale du système SemBioNLQA. En effet, si l’ensemble des documents récupérés, les passages et le type d’une question donnée ne sont pas identifiés correctement, les étapes de traitement ultérieures pour extraire les réponses échoueront inévitablement. Par exemple, pour la question «What is the association of spermidine with $\alpha$-synuclein neurotoxicity?» du lot 1 du test du BioASQ 2016, le type de question retourné par le système est «résumé» alors que dans le corpus le type de cette question est «factuelle». Par conséquent, le système SemBioNLQA échouera inévitablement à extraire la bonne réponse puisque l’extraction de la réponse à une question factuelle qui nécessite une entité nommée biomédicale comme réponse, ne se base pas sur les mêmes techniques d'extraction des réponses aux questions de type résumé.

Selon les résultats obtenus, SemBioNLQA présente un certain nombre d’avantages par rapport aux systèmes existants tels que AskHERMES, EAGli et Olelo. Cela se manifeste en trois aspects : (1) l’intégration de notre méthode de classification des types de questions offre un net avantage sur Olelo en ce qui concerne les réponses exactes qui dépendent de chaque type de question, (2) SemBioNLQA offre un avantage par rapport aux systèmes AskHERMES, EAGli et Olelo qui réside dans le fait de traiter plusieurs types de questions: les questions booléennes, les questions factuelles, les questions de type liste et les questions de type résumé, et (3) la performance de notre système SemBioNLQA par rapport aux autres systèmes est démontré par les résultats obtenus à la fois par l’évaluation automatique et l’évaluation manuelle. Les détails sur les expérimentations menées ainsi que les résultats obtenus sont présentés en section~\ref{Chapter6.3} du chapitre~\ref{Chapter6}.

\section{Conclusion et Perspectives}
Dans ce travail de thèse, nous nous sommes penchés sur l’étude et l’amélioration des méthodes permettant la recherche automatique des réponses précises et idéales à des questions biomédicales dans une base de documents biomédicaux (en langue anglaise). Les améliorations que nous avons apportées concernent le traitement de plusieurs types de questions, la génération des réponses appropriées et la performance de SQR développé. En effet, nous avons proposé un ensemble de méthodes pour les systèmes de questions-réponses en domaine biomédical pour permettre aux utilisateurs (chercheurs et professionnels de la santé, etc.) la recherche des réponses précises à leurs questions formulées en langage naturel.

Notre première contribution consiste en la proposition de deux méthodes de classification des questions permettant de déterminer le type de la question et le type sémantique de la réponse attendue à une question biomédicale. La première est basée à la fois sur les patrons lexico-syntaxiques et l’apprentissage automatique pour déterminer la catégorie de la question posée par l'utilisateur. Celle-ci permet de classifier automatiquement les questions biomédicales dans l’une des quatre types de questions définies par le challenge BioASQ : (1) les questions booléennes, (2) les questions factuelles, (3) les questions de type liste, et (4) les questions de type résumé. La tâche d’assigner l’une des catégories précitées à une question donnée dans les SQR est très utile dans l’extraction de réponses car elle permet à un SQR d'identifier à l’avance le format de la réponse attendue. Les résultats expérimentaux ont démontré que notre méthode assure une amélioration significative en terme de performance par rapport à d'autres systèmes existants. La deuxième méthode qui constitue une variante de la première consiste à déterminer le type sémantique de la réponse attendue (i.e., un ou plusieurs sujets) pour réduire le nombre des documents examinés lors de la recherche des réponses candidates. Pour cela, nous nous somme basés sur d’autres caractéristiques de type lexical, morphosyntaxique et sémantique. L'ensemble d’expérimentations que nous avons menées a montré que notre méthode est plus efficace que les méthodes de l’état de l’art.

La deuxième contribution consiste à suggérer une méthode de recherche des documents pertinents susceptibles de contenir les réponses candidates à partir de la base de données MEDLINE. Notre méthode est basée sur le moteur de recherche GoPubMed spécialisé dans le domaine biomédical pour sélectionner la liste des documents pertinents, et la similarité sémantique entre la question et les titres des documents retournés. Le calcul de cette similarité sémantique est utilisé pour reclasser les documents obtenus en fonction de leur pertinence à la question. Ce processus est justifié par le fait que le moteur de recherche renvoie souvent les documents dans un ordre qui ne correspond pas à leur degré de pertinence vis-à-vis de la question alors que l'extraction des réponses candidates se fait à partir des documents les mieux classés. Les résultats expérimentaux ont montré que la méthode proposée pour la sélection des documents est plus efficace que les méthodes qui ont été classées parmi les 10 meilleures lors du challenge BioASQ. Nous avons également proposé une alternative permettant la recherche des passages (i.e. extraits de documents) pertinents aux questions biomédicales. Celle-ci est fondée sur le moteur de recherche PubMed, la similarité sémantique UMLS, les racines des mots, les concepts UMLS et le modèle BM25. Les résultats expérimentaux ont montré que la méthode proposée atteint de bonnes performances par rapport aux méthodes de l’état de l’art les plus récentes.

Notre troisième contribution propose des méthodes d’extraction des réponses finales permettant d’extraire à la fois les réponses exactes et idéales aux questions biomédicales. Dans ce travail de thèse, notre objectif final est de construire un système complet de questions-réponses en mesure de traiter quatre types de questions qui sont les questions booléennes, les questions factuelles, les questions de type liste et les questions de type résumé. Par conséquent, nous avons développé des nouvelles méthodes d’extraction des réponses pour chaque type de questions traitées. Celles-ci sont basées sur des approches statistiques et sémantiques. Les résultats obtenus ont montré l’efficacité des méthodes proposées. De plus, nous avons participé au challenge BioASQ 2017 avec un sous-système d'extraction des réponses qui a été classé parmi les premiers vainqueurs.

Notre dernière contribution dans le cadre de ce travail de thèse consiste à intégrer l'ensemble des méthodes proposées au sein d’un système global de questions-réponses, appelé SemBioNLQA. Celui-ci accepte en entrée une variété de questions et retourne des réponses appropriées exactes et idéales. Il fournit des réponses exactes «oui» ou «non» pour les questions booléennes, les entités nommées biomédicales pour les questions factuelles et une liste d’entités nommées biomédicales pour les questions de type liste. De plus, il retourne des réponses idéales pour ces types de questions précédentes et les questions de type résumé. Les résultats des expérimentations que nous avons effectuées sur une collection standard de questions biomédicales fournie par le challenge BioASQ ont montré que le système proposé est plus efficace que les systèmes existants.

Les contributions apportées dans ce travail de thèse permettent d'améliorer la recherche automatique des réponses à des questions exprimées en langue naturelle. De plus, elles ouvrent plusieurs directions pour des futurs travaux de recherche. Ces perspectives concernent différents volets:

\begin{itemize}
\item Pour améliorer davantage la performance du processus de classification des questions, des études lexicals, morpho-syntaxiques et sémantiques  plus approfondies sur les questions d’apprentissage utilisées au cours ce travail de thèse sont nécessaires.
\item D'autres améliorations de nos méthodes de classification des questions sont également possibles par l'usage des modèles d’apprentissage en profondeur \emph{deep learning} qui ont émergé dans la littérature en domaine de classification des textes.

\item Nous envisageons d'appliquer des modèles d’apprentissage en profondeur pour estimer la probabilité de pertinence du document par rapport à la question. Nous envisageons également d’explorer des modèles d’apprentissage en profondeur pour estimer la probabilité de la pertinence du passage à la question. Une vue d’ensemble des modèles apprentissage en profondeur peut être trouvée dans \citep{zhang2016neural,mitra2017neural}.

\item La participation au chellenge BioASQ 2017 nous a également motivé pour l'utilisation des modèles d’apprentissage profond dans la phase d’extraction de la réponse. Cependant, ces modèles sont généralement appliqués uniquement pour les SQR traitant des questions factuelles. De telles approches nécessitent un très grand nombre de paires questions-réponses pour la phase d’apprentissage. L’un des systèmes gagnants du challenge BioASQ 2017 développé par \cite{wiese2017neural} était un système d’apprentissage purement profond qui est restreint au traitement des questions factuelles et les questions de type liste. L'incorporation des modèles d’apprentissage profonds dans notre SQR permettrait d’améliorer son efficacité pour répondre aux questions factuelles et aux questions de type liste.

\item Nous envisageons étendre notre SQR pour les données structurées (par exemple, des bases de données, des ontologies, des triplets RDF, etc.) afin d'améliorer sa performance en terme de précision.

\item Au niveau de l'étape de l'extraction des passages, nous projetons de considérer le texte intégral des documents (par exemple, à partir de PubMed Central) au lieu de leurs résumés pour améliorer les performances des méthodes d’extraction de réponses. Nous pourrions exploiter la base de données MEDLINE qui contient plus de 24 millions de références complètes et PubMed Central qui archive 4.4 millions d’articles.
\end{itemize}







