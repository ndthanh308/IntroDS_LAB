% Chapter 1

\chapter{State-of-the-Art in Biomedical Question Answering} % Main chapter title

\label{Chapter3} % For referencing the chapter elsewhere, use \ref{Chapter1}
\setcounter{secnumdepth}{4}
\setcounter{minitocdepth}{2}
\minitoc

This chapter presents background information to contextualise the topics of interest of this thesis work. In section~\ref{Chapter3_1}, a brief introduction is provided to question answering and its typical pipeline. In section~\ref{Chapter3_4}, the main characteristics of question answering in the biomedical domain are shown. In section~\ref{Chapter3_5}, the main resources that can be exploited for QA in the biomedical domain are presented. In section~\ref{Chapter3_6}, we review the related work to question answering with a particular focus on the biomedical domain. In section~\ref{Chapter3_8}, a synthesis is presented. Finally, the evaluation metrics we use for the assessment of the proposed methods for question answering in the biomedical domain are described in section~\ref{Chapter3_7}.


%----------------------------------------------------------------------------------------

% Define some commands to keep the formatting separated from the content
%\newcommand{\keyword}[1]{\textbf{#1}}
%\newcommand{\tabhead}[1]{\textbf{#1}}
%\newcommand{\code}[1]{\texttt{#1}}
%\newcommand{\file}[1]{\texttt{\bfseries#1}}
%\newcommand{\option}[1]{\texttt{\itshape#1}}

%----------------------------------------------------------------------------------------
\section{Introduction}
\label{Chapter3_1}

The search for short specific answers to questions written in natural language is one of the major challenges in the field of information retrieval. The question answering issue has been addressed by the artificial intelligence community in the literature since the 1960s. Early experiments in this direction implemented question answering systems based on knowledge bases written manually by experts in their own field of interest, such as BASEBALL \citep{Green_1961}, LUNAR \citep{Woods_1973} that operate in open domain. The BASEBALL system answers questions posed about the dates, locations or results of Baseball games played in the American league over one season. LUNAR which is one of the first sciences question answering system, was designed to assist the geologic analysis of stones returned by the Apollo mission.

Question answering (QA), unlike traditional IR aims at directly providing the short, precise answers to questions asked by inquirers in natural language, by employing complex IE and NLP techniques, instead of returning a large number of documents that are potentially relevant \citep{voorhees1999trec}. QA systems aim at directly producing and providing precise answers rather than entire documents to users questions, by automatically analyzing thousands of articles, ideally in less than a few seconds. Such systems, which can help users locate useful information quickly, need linguistically and semantically processing of both users questions and data sources in order to extract the relevant information. In particular, question answering differs from traditional IR in three main aspects: (1) information needs (i.e., queries) are expressed  using natural language instead of a set of keywords; (2) results depend directly to what has been specifically requested, be it a precise answer (e.g., a named entity) or a short summary (3) answers are generated based on the integration of unstructured and structured data, e.g., textual documents and knowledge bases, respectively. While the intended answer is usually a piece of text, in few cases, the format of the answer may also be a multimedia information.


While the early research works on QA in the domain of artificial intelligence (AI) dates far back to the 1960s, several investigations involving QA within IR and IE communities have been made by the introduction of the QA Track in Text REtrieval Conference (TREC\footnote{TREC: \url{http://trec.nist.gov/}}) evaluations in 1999 \citep{voorhees1999trec}. Since then, methods and tools have been developed for generating and extracting answers for the three types of natural language questions supported by TREC fora, including, factoid questions, list questions, and definitional questions. Most research works in the field of QA, as fostered by TREC and other similar evaluation fora such as NII-NACSIS Test Collection for IR systems (NTCIR\footnote{NTCIR: \url{http://research.nii.ac.jp/ntcir/index-en.html}}) \citep{kando2002overview} and Cross Language Evaluation Forum (CLEF\footnote{CLEF: \url{http://clef.isti.cnr.it/}}) \citep{Magnini_2005}, have so far been focused on open-domain QA.


Due to the continuous, enormous growth of textual data especially in the form of scientific articles in the biomedical domain, recently, the field of QA systems in the biomedical domain has witnessed a growing interest among researchers. Such systems can quickly and reliably assimilate relevant information from a multitude of biomedical textual resources. Furthermore, question answering is a text mining task directed towards aiding researchers and health care professionals in managing the exponential growth of textual information in the biomedical domain.


Natural language QA systems are typically composed of four main components including (1) question analysis and classification, (2) information or document retrieval, (3) passage retrieval and (4) answer extraction, as shown in Figure~\ref{fig:QAP} \citep{HIRSCHMAN_2001}.


% Figure environment removed

The input to a QA system is a natural language question. Based on the linguistic and semantic  processing of the question, question analysis and classification identifies the type of question being posed to the system, and the type of the expected answer it should generate. Often, there may be more processes involved at this phase, such as named entity recognition and relation extraction. Output from this stage is one or more features of the question for use in subsequent stages. It then constructs a query from the input question to be fed into the third component, information retrieval. In the information retrieval component, the system inputs the query into a document retrieval engine so as to find a set of relevant documents satisfying the query. Out of the retrieved and selected documents, the passage retrieval component consists of retrieving relevant passages (text segments). The retrieved passages set may be narrowed down to a smaller set of most relevant passages/snippets of text, which constitute the input for the last component, answer extraction. Finally, in the answer extraction component, based on the type of the question and the expected answer format generated by the question analysis and classification component, the final answer(s) are extracted and generated from the candidate answers passages selected by the passage retrieval component. The output of the question answering system consists of the top ranked answer(s), associated with the raw text from which the answers were extracted.


The biomedical-domain QA is one of the more popular restricted-domains QA. The following section~\ref{Chapter3_4} provides the main characteristics that distinguish QA in the biomedical domain from open-domain QA.




\section{Characteristics of Question Answering in the Biomedical Domain}
\label{Chapter3_4}

Several characteristics distinguish the biomedical domain as an application domain for QA from general, open-domain and other restricted-domains QA. Based on the three factors discussed in the previous section, namely, the size of the data, domain-dependent context, and domain-specific resources, \citet{athenikos2010biomedical} point out in their review of biomedical QA the characteristic features of QA in the biomedical domain. These characteristics include:

\begin{enumerate}
  \item Large-sized corpora.
  \item Highly complex domain-specific terminology.
  \item Domain-specific lexical, terminological, and ontological resources.
  \item Tools and methods for exploiting the semantic information embedded in the above resources.
  \item Domain-specific format and typology of questions.
\end{enumerate}

First, biomedical QA is characterized and challenged by the large-sized of unstructured information especially in the form of scientific articles. Retrieving the accurate answers to potential questions from a multitude of biomedical sources tends to be quit complex. Second, the prominent use of highly complex domain-specific terminology is both an advantage and a challenge for biomedical QA. In general, the amount of terminological variations and synonyms make biomedical text mining relatively difficult. However, biomedical QA may take advantage of the particularity and limited scope of natural language questions that a domain-specific terminology gives. Third, domains that are specific, limited, and complex are most likely to have available resources such as methasaurus and ontologies that can be used in the QA process. These resources are generally developed to help biomedical domain users to retrieve information using text mining applications as well as categorize and group the biomedical knowledge. In addition to available resources, the tools and techniques required for employing the semantic information they contain enable for deep question and answer processing. Lastly, biomedical QA may benefit from the domain-specific formatting and typology of questions so as to use and develop answer processing techniques strategies for each specific question type.


Due to the importance and unique characteristics of QA in the biomedical domain, recently, several research works have been presented in the different stages of QA. These works which include the important methods and techniques are reviewed and discussed in the section~\ref{Chapter3_6}.

\section{Resources for Biomedical Question Answering}
\label{Chapter3_5}

While the biomedical domain poses a notable challenge for answering natural language questions, with highly complex domain-specific terminology and the enormous growth of textual information especially in the form of scientific articles and electronic health records, it also provides various resources that can be very beneficial for QA, as Zweigenbaum \citep{zweigenbaum2003question} also notes in his overview on QA in biomedicine. Here we review some of the well-known and relevant resources for QA in the biomedical domain.

\subsection{Corpora}

The primary resource for text-based QA in the biomedical domain is obviously text. In this context, MEDLINE was the first and still remains the primary resource for biomedical QA and in general for biomedical text mining. The MEDLINE\footnote{MEDLINE: \url{https://www.nlm.nih.gov/pubs/factsheets/medline.html}} database  maintained by the U.S. National Library of Medicine (NLM), and contains citations to journal articles in the life sciences with a particular focus on biomedicine. The 2016 MEDLINE contains over 24 million bibliographic references published from 1946 to the present in over 5,623 worldwide scientific journals. Indeed, hundreds of thousands of references are added to the database each year. For example, more than 806 thousand were added in 2016.

Perhaps more importantly, abstracts and sometimes full text of MEDLINE citations can be downloaded using the Entrez Programming Utilities\footnote{Entrez Programming Utilities (E-Utilities), Encyclopedia of Genetics, Genomics, Proteomics and Informatics: \url{https://doi.org/10.1007/978-1-4020-6754-9_5383}} for text mining purposes. For instance, PubMed\footnote{PubMed: \url{https://www.ncbi.nlm.nih.gov/pubmed/}}, a free service provided by the NLM under the U.S. National Institutes of Health (NIH) and accessible through the National Center for Biotechnology Information (NCBI), gives access to MEDLINE for searching abstracts of biomedical literature. The result of a PubMed search is a list of references (including authors, title, source, and often an abstract) to journal articles and an indication of free full-text availability. PubMed Central\footnote{PubMed Central: \url{https://www.ncbi.nlm.nih.gov/pmc/}} is a full-text archive of biomedical and life science articles, maintained by the NIH. Alternatively, subsets of MEDLINE documents can be obtained from the archives of individual research teams that share their annotated collections, and community-wide large-scale evaluations that use MEDLINE citations. For example, the OHSUMED\footnote{OHSUMED Test Collection. Available at: \url{http://trec.nist.gov/data/t9_filtering/}} collection contains all MEDLINE citations in 270 biomedical journals published over a five-year period (1987–1991) as well as a more recent collection provided in TREC Genomics\footnote{TREC genomics track data. Available at: \url{http://ir.ohsu.edu/genomics/data.html}} Track that contains ten years of MEDLINE documents (1994–2003).



Also available in the biomedical domain is topically-annotated collections of MEDLINE abstracts such as the GENIA corpus \citep{Kim_2003}, the earlier BioCreAtIve collections \citep{Hirschman_2005,Krallinger_2008,Islamaj_Dogan_2017}, and more recent set of the BioASQ challenge \citep{tsatsaronis2012bioasq}. The GENIA corpus which contains 1999 MEDLINE abstracts, is syntactically and semantically annotated for part-of-speech, coreference, biomedical concepts and events, cellular localization, and protein reactions. The BioCreAtIve data sets derived from the BioCreAtIve challenge which concerned with the extraction of biologically significant entities names and useful information (e.g., protein - functional term associations) from the literature. The BioASQ collections provided by the BioASQ challenges contains standard data sets for evaluating semantic indexing, question answering and information extraction systems in the biomedical domain. The BioASQ data sets are currently the most thoroughly annotated collection of MEDLINE abstracts and biomedical questions. The BioASQ challenges include biomedical text mining tasks relevant to text classification, information retrieval, question answering from texts and structured data, multi-document summarization and many other areas.

\subsection{Lexical, terminological, and ontological resources}

Due to the need of structuring highly complex domain specific terminology and of making it machine-understandable, text mining community in the biomedical domain has constructed and developed a set of lexical, terminological, and ontological resources. The Unified Medical Language System (UMLS\footnote{UMLS: \url{https://www.nlm.nih.gov/research/umls/}}) \citep{lindberg1993unified,Bodenreider_2004}, a repository of biomedical vocabularies that is maintained by NLM, is the most important resource. The UMLS includes three knowledge resources: the Mtathesaurus, the Semantic Network, and the SPECIALIST Lexicon. The UMLS Metathesaurus is a comprehensive, multilingual biomedical vocabulary thesaurus that contains information on biomedical concepts, their various names, and the hierarchical and transversal relations between them. The latest version of the Metathesaurus (UMLS 2017AB) contains approximately 3.64 million concepts and 13.9 million unique concept names from 201 different source vocabularies including dictionaries, terminologies, and ontologies. The Semantic Network contains information about the set of semantic types, or broad subject categories, and set of useful and important relationships, or semantic relations that may hold between these semantic types. The semantic types provides a consistent categorization of all concepts in the Metathesaurus by grouping these concepts according to the semantic types that have been assigned to them. The Semantic Network in the UMLS 2017AB version contains 127 semantic types and 54 semantic relationships. The SPECIALIST Lexicon provides the lexical, syntactic, and morphological information needed for the SPECIALIST NLP system. It contains commonly English words and biomedical vocabulary.

Medical Subject Headings (MeSH\footnote{MeSH: \url{https://www.nlm.nih.gov/mesh/}}), the controlled vocabulary thesaurus that is maintained and manually updated every year by NLM, consists of medical subject headings in a hierarchical tree which allows search at several levels of specificity. The latest version of the MeSH thesaurus (MeSH 2018) contains 28.939 MeSH descriptors, 116.909 total descriptor terms, and 244.154 supplementary concept records. Mesh is the widely used for the purpose of indexing journal citations for databases in the biomedical domain. For example, it is used by the U.S. NLM to index biomedical articles for the MEDLINE/PubMed database as well as for the cataloging of books and documents acquired by the library. Overall, the U.S. NLM provides over 297 knowledge sources\footnote{NLM resources: \url{https://eresources.nlm.nih.gov/nlm_eresources/}} and tools supporting biomedical text mining applications.

Systematized Nomenclature of Medicine Clinical Terms (SNOMED CT) \citep{stearns2001snomed}, the most comprehensive and precise clinical terminology available in the world that is originally maintained by the College of American Pathologists, aims at encoding the meanings that are used in clinical information and to support the efficient recording of clinical records. It was formed by the merger of two large health care reference terminologies, SNOMED Reference Terminology (SNOMED RT) and the U.K. National Health Service Clinical Terms, and it is accessible through the U.S. NLM and the National Cancer Institute (NCI). SNOMED CT is one of a suit of specified standards for use in U.S. healthcare information systems for the electronic exchange of clinical health information. The latest version of SNOMED CT released in January 2017 contains over 326.734 active concepts.

Gene Ontology\footnote{Gene Ontology: \url{http://www.geneontology.org/}} \citep{ashburner2000gene}, a controlled vocabularies of concepts used to define gene function, and semantic relationships between concepts, is an ontology designed to structure the description of genes and genes products common to all species. It classifies gene functions along three aspects: (1) molecular function, the molecular activities of a gene product, such as binding or catalysis; (2) cellular component, where a gene product is active; and (3) biological process, pathways and operations made up of the elemental activities of multiple gene products.

Other sets of interoperable ontologies in the biomedical domain are developed and maintained by collaborative effort in the Open Biological and Biomedical Ontology (OBO) Foundry\footnote{Open Biomedical Ontologies: \url{http://www.obofoundry.org/}} and the National Center for Biomedical Ontology (NCBO\footnote{National Center for Biomedical Ontology: \url{https://www.bioontology.org/}}). The set of NCBO ontologies are accessed through BioPortal\footnote{NCBO BioPortal: \url{http://bioportal.bioontology.org/}}. Other important centers that develop specialized resources for text mining applications in the biomedical domain include the U.K. National Centre for Text Mining (NaCTeM\footnote{NaCTeM: \url{http://www.nactem.ac.uk/}}) and the European Bioinformatics Institute (EMBL-EBI\footnote{EMBL-EBI: \url{https://www.ebi.ac.uk/}}).


\subsection{Supporting tools}


Besides the various available terminological, lexical and ontological resources in the biomedical domain that provide the lexical semantic information needed for QA and other text mining applications, the biomedical domain also has supporting tools that are specifically designed and developed to help exploit the semantic information contained in those aforementioned resources, such as biomedical entities and semantic relations, etc. Here, we define the most widely used tools to identify biomedical entities and semantic relationships among them in the text.

MetaMap \citep{aronson2001effective} that is based upon UMLS, is the most widely used tool by researchers working on biomedical text mining, including biomedical QA researchers for the named entity recognition task. MetaMap is highly configurable NLP application developed at NLM that maps terms in free biomedical text to UMLS Metathesaurus or, equivalently, identifies UMLS Metathesaurus concepts referred to in text. MetaMap provides a wide variety of configuration options to give researchers the opportunity to choose the best configuration for a given task. MetaMap, which is an open source tool and available for download\footnote{MetaMap: \url{https://metamap.nlm.nih.gov/}}, it requires a UMLS Terminology Services (UTS) account.

SemRep \citep{Rindflesch_2003} that is also relied on UMLS, is often used by researchers and practitioners to extract UMLS Semantic Network semantic relationships between UMLS concepts from sentences in biomedical text. SempRep\footnote{SemRep: \url{https://semrep.nlm.nih.gov/}} developed at the U.S. NLM to map sentences in free biomedical text to UMLS Semantic Network in order to extract three-part propositions, called semantic predications. Each predication consists of a subject argument, an object argument, and the semantic relation that binds them. The subject and object arguments of each predication are UMLS Metathesaurus concepts and their binding relationship is a UMLS Semantic Network semantic relationship.


\section{Related Work on Biomedical Question Answering}
\label{Chapter3_6}

Due to the unique characteristics of biomedical as an application domain for QA, recently researchers have proposed several specific methods and techniques in the different processing stages of QA system so as to produce more precise responses. As shown previously in section~\ref{Chapter3_1}, QA systems typically incorporate several components, including question classification, document retrieval, passage retrieval, and answer extraction each of which has to deal with specific challenges and issues. For each QA component, its basic challenges and approaches are outlined and recent and influential methods are reviewed in the following subsections.

\subsection{Question classification}
\label{Chapter3_6_1}
Question classification is usually the first component in QA pipeline as the first step towards developing biomedical QA systems is processing and classifying the question in order to identify the question type and therefore the answer type to produce. Several researchers in the biomedical domain have investigated question classification as a means of analyzing and filtering biomedical questions \citep{Simpson_2012}. In general, question classification may include several tasks since the questions may be classified along many dimensions: question type \citep{cruchet2008supervised,roberts2014automatically}, answer type \citep{McRoy_2016}, topic \citep{kobayashi2006representing, yu2008automatically,Cao_2010}, user \citep{Liu_2011,Roberts_2016}, answerability \citep{yu2005being}, resource \citep{roberts2016resource}, etc. Question classification essentially covers all these classification systems, and thus one could have multiple question classifiers to classify a single question along many dimensions. The most common classification in the biomedical domain is question type and answer type, sometimes referred to as the expected answer format and semantic answer type, respectively. For example, the question type and the semantic type of the expected answer for the biomedical question ``What is the treatment of acute myocarditis?'' are ``factoid'' and ``treatment'', respectively.

The purpose of question type classification in biomedical QA is to determine the type of question and therefore identify the expected answer format, to check whether the answer should be a biomedical entity name, a short summary, ``yes'' or ``no'', etc. Importantly, to produce the answer to a given question, the QA systems which deal with more than two types of questions, should know in advance the expected answer format that allows using specific answer extraction strategies. The identification of question types in biomedical QA systems is a very important task as it may strongly affect positively or negatively further processing stages: if the question type is not identified correctly, further QA processing stages will inevitably fail too. This task is usually carried out in open-domain QA systems by taking into account the ``Wh-'' particle and predefined patterns. However, the complexity of natural language creates many challenges to this task in the biomedical domain. As some questions that appear, at first, to belong to a certain type can result to be of a different one. For instance, the biomedical question ``Where is the protein CLIC1 localized?'', from the BioASQ training questions, is a factoid question. Although the way in which it is constructed, to start with ``where'', may lead it to be a summary question which is incorrect.

On the other hand, the answer type is the semantic type of the expected answer, and is useful in filtering out irrelevant answer candidates. The answer type has been called commonly topic or semantic type of the expected answer. The identification of answer type to a given question in biomedical QA, which is one of the most common task of question classification studied in the literature, has a critical impact on the overall performance of biomedical QA systems as is useful in choosing the best resource from which the answer should be extracted and candidate answer selection. For example, the question ``What is the best way to catch up on the diphtheria-pertussistetanus vaccine (DPT) after a lapse in the schedule?'' from National Library of Medicine's data set represents a pharmacological question, and the QA system may therefore identify the Micromedex pharmacological database as the resource to produce the answers.

The approaches for question classification may be classified in rule-based approach and machine learning-based approach. The former tries to
match the question with some predefined rules, while the latter can automatically classify questions into categories, by extracting some features from them. Many studies in open-domain QA have used rule-based approach \citep{Khoury_2011,Biswas_2014}. Otherwise, machine learning approaches have improved solution for open-domain question classification. One of the biggest advantages of machine learning is that one can focus on designing insightful features, and rely on learning process to efficiently cope with the features. For example, \citet{li2002learning} have presented a hierarchical classifier based on the sparse network of winnows for open-domain question classification. Two classifiers were involved in this work: the first one classifies questions into coarse categories; and the other one into fine categories. \citet{yu2005modified} have improved the bayesian model by applying the TFIDF measure to deal with the weight of words for Chinese question classification. \citet{xu2006syntactic} have employed affiliated ingredients as features of the learned model and used the results obtained by the syntactic analysis for extracting the question word. \citet{li2008classifying} have classified open-domain ``what'' type of questions into proper semantic categories using conditional random fields (CRFs). CRFs has been used to label all words in a question, and then choose the label of head noun as question category. \citet{Xu_2012} have introduced a question classification method based on the SVM classifier for QA system in the tourism domain. Other methodologies take benefit of both rule-based and machine-leaning approaches such as the work described in \citep{Hao_2017}.


Although many solutions have been proposed for question classification tasks in open-domain QA, only a few works have been completed in the biomedical domain which have attempted to define the information needs of physicians. For example, for searching the best available evidences supporting responses to clinical questions, the evidence-based medicine (EBM) paradigm recommends questions be organized according to the PICO (Problem or Patient/Population, Intervention, Comparison, Outcome) format \citep{richardson1995well}. In addition to the specific frame, taxonomies of biomedical questions in the EBM framework have also been developed.  \citet{Ely429} have proposed a taxonomy of generic types of clinical questions asked by doctors about patient care so as to help answer such questions. The taxonomy contains the 10 most frequent question topics (e.g., diagnosis, treatment, management, etc.) among 1396 collected clinical questions (e.g., ``What is the drug of choice for condition x?''). \citet{Bergus_2000} proposed a taxonomy of medical questions according to the PICO representation of questions and the categories of clinical tasks involved in the questions. \cite{Ely710} developed an evidence taxonomy to deal with obstacles be faced when attempting to answer physicians' questions with evidence. The authors have classified the collected 1101 questions from 103 family doctors into clinical vs. non-clinical. The clinical questions are divided into general vs. specific. The general questions can be classified into evidence vs. no evidence. The evidence questions are categorised into intervention vs. no intervention categories. \citet{jacquemart2003towards} have presented a taxonomy that consists of 8 broad semantic models (e.g., [which X]–(r)–[B]) for categorizing clinical questions in a French-language medical QA system. In this context, the recent taxonomy of biomedical questions which is created by the BioASQ challenge \citep{tsatsaronis2012bioasq} consists of four types of questions that may cover all kinds of potential questions: yes/no questions, factoid questions, list questions, and summary questions. \citet{seol2004scenario} identified four types of questions including treatment, diagnosis, etiology, and prognosis.
\citet{huang2006evaluation} presented a manual classification of clinical questions asked in natural language as a mean to examine the adequacy and suitability of the PICO framework. The authors have reaffirmed the usefulness of the PICO framework for structuring clinical questions, but also they found that it is less suitable for clinical questions that do not implicate therapy elements. \citet{niu2003answering,niu2004analysis,niu2005analysis,niu2006using} proposed a PICO-based question analysis approach within the EpoCare project. Their methods which aim at automatically answering questions from clinical evidence, extract potential answers using the PICO framework to structure both the question and answer texts. Similarly, \citet{Demner_Fushman_2006, Demner_Fushman_2007} used the PICO framework in their proposal approach to clinical QA. Their methods use the semantic unification of the PICO frame of the query and that of candidate answers. \citet{abacha2015means} developed a medical QA system named MEANS which consists of (1) corpora annotation, (2) question analysis and classification, and (3) answer search. The question classification task in the MEANS system aims at classifying the given medical questions into three types: definition, yes/no, and factoid so as to determine the type of the expected answers (e.g., ``yes'' or ``no'', biomedical entity name, etc.). Their method is based on a set of patterns that were manually constructed by analysing the 100 medical questions. \citet{yu2005classifying} performed machine learning approaches to classify medical questions based on  Ely et al.' s taxonomy into topics. They have shown that using the question taxonomy with the SVM classifier leads to the highest performance. \citet{cruchet2008supervised} described a question classification method to identify the answer types of questions in a bilingual French/English QA system which is adopted to health domain. The authors who studied a small number of English and French medical questions, used support vector machine (SVM) for classifying these questions into several categories (e.g. symptoms, prevention, evolution, treatment, etc.). \citet{Cao_2010} proposed a SVM classifier-based method to classify clinical questions asked by physicians in natural language into predefined general topics (e.g., diagnosis, management, pharmacological, treatment, etc.). The authors incorporated several features of clincal questions for the SVM classifier such as words, bigrams, stemming, UMLS concepts and semantic types. Since one question can be assigned to multiple topics, a binary SVM classifier was adopted for each of the 12 topics. \citet{Patrick_2012} proposed a question classification method for answering clinical questions applied to electronic patient notes. They first collected a set of clinical questions from staff in an Intensive Care Unit. They then designed a clinical question taxonomy for question and answering purposes. Finally, they built a multilayer classification model to classify the clinical questions. \citet{roberts2014automatically} introduced a multi-class SVM classifier-based method to automatically classify consumer health questions into semantic types (e.g., anatomy, cause, diagnosis, etc.) for the purpose of supporting automatic retrieval of medical answers from consumer health resources. More recently, deep learning models such as recurrent neural Network (RNN) and LSTM have been emerging as state-of-the-art for sequence modeling, particularly for text classification. Deep learning classification methods are multi-layer networks, which have been introduced for both feature extraction and classification tasks. The most well-known deep learning methods are the convolutional neural networks (CNN), which have known a great success since their introduction by \citep{lecun1989backpropagation}. Many deep learning models have been proposed such as RNN, recursive neural network and LSTM. Such models produce a more robust predictor than traditional machine learning methods. However, they  require a large number of training instances for the training phase.


Beyond these types of question classification in the biomedical domain, other approaches have studied (a) question answerability to separate answerable questions from unanswerable ones  \citep{yu2005classifying,yu2005being}, (b) resource identification to determine the resource type of biomedical questions \citep{roberts2016resource}, and (c)  relation extraction \citep{hristovski2015biomedical} using SemRep \citep{Rindflesch_2003}.

Although several question classification methods have been proposed in biomedical QA, question classification still requires further efforts in order to improve its performance. For instance, existing solutions for biomedical question classification have so far focused on extracting syntactic and semantic features from questions and using machine learning algorithms so as to classify questions into different topics. However, they do not take into account the syntactic dependency relations in questions. Intuitively, the incorporation of syntactically related pairs into other features may provide the best description and representation of questions. The motivation to find alternative features for machine-learning algorithms is the fact that words by themselves cannot capture the gist of a clinical question.

Another challenging issue in question classification is the identification of the types and formats of potential questions and intended answers, respectively. Note only current biomedical QA systems have limitations in terms of the types and formats of questions and answers that they can process, but also in most such systems which dealt with more than one type of questions, the users have to give or select manually the question type to each given question. As the ultimate goal of biomedical QA systems is to be able to deal with a variety of natural language questions and to generate appropriate natural language answers, biomedical question type classification is a necessary task needs so as to automatically identify the type of question and therefore to see whether the answer should be a biomedical entity name, a short summary, ``yes'' or ``no'', etc.

In the next chapter~\ref{Chapter4} we will present in details the proposed machine learning based methods for question classification in biomedical QA. The first method consists at identifying the type (i.e., yes/no, factoid, list and summary questions) of a given biomedical question in order to determine the expected answer format. It is based on our predefined set of handcrafted lexico-syntactic patterns and machine learning algorithms. The second method, which is based on lexical, syntactic and semantic features for machine learning algorithms, allows classifying questions into topics in order to filter out irrelevant answer candidates.


\subsection{Document retrieval}
\label{Chapter3_6_2}

Document retrieval is usually the second component in a typical QA pipeline as the second step towards answering a biomedical question posed in natural language is retrieving the set of textual documents that are likely to contain the answer. Retrieval of a set of relevant documents to a given query that is constructed from the question is usually carried out based on an existing IR system. The retrieved textual document set is often narrowed down to a smaller set of most relevant documents which constitutes the input for further QA processing steps. In particular, document retrieval is one of the significant components that serves as the building block of efforts since the correct answers can be only found when the set of textual documents from which the QA system extracts the answers is retrieved correctly \citep{Monz_2003,Collins_Thompson_2004,athenikos2010biomedical,neves2015question}.

While many efforts and investigations have been made in this direction in the open domain, initiated by the QA Track in TREC evaluations which takes place regularly every year since 1999 \citep{voorhees1999trec,roberts2002information,Monz_2003,gaizauskas2004information,Collins_Thompson_2004,voorhees2005trec,Teufel}, it still remains a real challenge in the biomedical domain due to a variety of reasons. The most basic obstacle is that there exists a vast amount of textual data especially in the form of scientific articles that is constantly and rapidly increasing as new scientific articles are published every day in the biomedical domain. Furthermore, not only these data are generally expressed in natural language, which makes its automated processing more difficult and complex, but also terminological variations and synonyms make document retrieval difficult in general for the biomedical domain. Thus, an efficient access to relevant information is a challenging task. Although biomedical text mining is challenged by a prominent use of domain-specific terminology, document retrieval may benefit from the specificity and limited scope of textual documents and the potential queries that a domain-specific terminology provides. Due to the specific characteristics of biomedical as an application domain for document retrieval, recently, most proposed systems in some way make use of domain-specific semantic knowledge, such as the MeSH thesaurus, for document retrieval. For example, the NLM indexers use the MeSH descriptors to index MEDLINE citations for PubMed search engine, a well-known information retrieval system in the biomedical domain which comprises more than 24 million citations for biomedical literature from MEDLINE, life science journals, and online books. \cite{lee2006beyond,Yu_2007} described a semantic-based approach for the development of a medical QA system named MedQA. The MedQA system is built upon four stages, namely, question analysis, information retrieval, answer extraction, and summarization techniques to automatically generate paragraph-level answers to definitional questions from the MEDLINE citations and World-Wide-Web. For information retrieval, the authors first applied LT CHUNK \citep{Mikheev_1996} to identify noun phrases from medical questions and used them as the query terms to retrieve relevant documents from the MEDLINE collection. They then used the tool LUCENE\footnote{LUCENE: \url{http://lucene.apache.org/core/}} \citep{goetz2000lucene} to index the MEDLINE documents and applied the vector-space model (VSM), a TFIDF based cosine similarity model for computing the relevance of each document to a query. To retrieve definitions from World-Wide-Web, the authors used Google and the TFIDF metric. \citet{Cao_2011} integrated the latest version of the probabilistic relevance model BM25 \citep{Robertson_2004} within their developed clinical QA system AskHERMES for the document retrieval stage, as it proved to be the best performing retrieval model for tasks such as those at the recent TREC. \cite{abacha2015means} applied NLP methods to process the source documents used to extract the answers to
natural language questions in their developed medical QA system named MEANS. The authors exploited NLP techniques, such as named entity recognition and relation extraction, and domain-specific semantic knowledge (e.g., UMLS Methasaurus and UMLS Semantic Network) to build RDF annotations of the source documents and SPARQL queries representing the users questions.

More recently, there has been a growing interest among researchers in developing new methods and techniques to improve the performance of document retrieval in biomedical QA  with the introduction of the biomedical QA Track at the BioASQ\footnote{BioASQ Challenge: \url{http://www.bioasq.org/}} challenge \citep{tsatsaronis2012bioasq} which takes place regularly since 2013. The BioASQ challenge is an EU-funded project aiming at fostering research and solutions on both the biomedical QA and large-scale online semantic indexing areas. The BioASQ challenge comprised three tasks: (1) Task a: on large-scale online biomedical semantic indexing, (2) Task b: on biomedical semantic QA, and (3) Task c: on funding information extraction from biomedical literature. The goal of Task b is to assess the performance of QA systems in different stages of the QA process. It is sub-divided into two phases: phase A and phase B. In phase A participants had to respond with biomedical concepts, relevant documents, relevant passages, and RDF triples. In phase B participants were asked to answer with exact answers and ideal answers (paragraph-sized summaries). \cite{weissenborn2013answering} developed a biomedical QA system which is composed of three main stages, namely, question analysis, document retrieval, and answer extraction to answer factoid questions. Documents relevant to potential questions are retrieved by making keyword queries to the GoPubMed search engine, which searches the MEDLINE biomedical citations database. Similarly, \cite{neves2014hpi} proposed a biomedical QA system which consists of two components, namely, question processing and document processing. Documents relevant to biomedical questions are retrieved by making queries to the GoPubMed search engine. Queries were constructed using the Stanford CoreNLP tools\footnote{Stanford CoreNLP package: \url{https://nlp.stanford.edu/software/corenlp.shtml}} for sentence splitting, tokenization, part-of-speech tagging and chunking. Indeed, queries was generated from the question based on both the terms and the chunks. The authors also performed query expansion using synonyms obtained using services from BioPortal. \cite{mao2014ncbi} used PubMed search functions for retrieving relevant documents to a given question in biomedical QA. Given a search query, the authors used PubMed results-ranking options such as by date or by relevance. \cite{choi2014classification} described a biomedical document retrieval approach based on semantic concept-enriched dependence model and sequential dependence model. The concept-enriched dependence model incorporates UMLS Methasaurus concepts identified using MetMap, while the sequential dependence model incorporates sequential query term dependence into the retrieval model. The authors used the Indri IR system \citep{strohman2005indri} and 2014 MEDLINE citations which composed of roughly 22 million citations.

Although previous document retrieval methods have proven to be quite successful at retrieving relevant documents in biomedical QA, document retrieval still require further efforts in order to improve its performance. One of the main observations that can be made about existing systems is that the task of document retrieval often set a framework in which an existing biomedical IR system is used, and completely depended on its ranking of documents. Indeed, there are many cases where the search engine mistakenly returns irrelevant citations high in the set or relevant citations low in the set. This problem is certainly a challenging issue as a biomedical QA system usually extracts the answers from the top-ranked documents.

In the chapter~\ref{Chapter5} we will present in details the proposed document retrieval method in biomedical QA.

\subsection{Passage retrieval}
\label{Chapter3_6_3}
Passage retrieval is usually the third component in a typical QA pipeline as the third step towards answering a biomedical question is processing the set of relevant documents so as to identify a set of text segments (also known as passages or snippets) that are likely to contain the answer. In this stage, which is known as passage or snippet retrieval, a set of passages are extracted from the retrieved and selected documents. In particular, a passage retrieval system may be defined as a specialized type of IR application that retrieves a set of passages/snippets rather than providing a whole ranked set of documents \citep{Buscaldi_2009}. Its main purpose in a QA system is to retrieve and return top-ranked passages which serve as answer candidates and the QA system extracts and selects the answers from them. Although a challenging task itself, passage retrieval remains one of the most important modules for the development of a QA system as the overall performance of a QA system heavily depends on the effectiveness of the integrated passage retrieval component: if a passage retrieval system fails to find any relevant passage for a given question, further processing steps to extract an answer will inevitably fail too. In this context, several studies such as the one reported in \citep{Otterbacher_2009}, highlighted that the correct answer to a given question can be found with high precision when it already exists in one of the retrieved passages.

Although passage retrieval in open-domain QA is a well-studied research area \citep{Clarke_2003,Otterbacher_2009,Ryu_2014,Saneifar_2014,Othman_2016}, it still remains a real challenge in biomedical QA. As described in section~\ref{Chapter3_4}, QA in the biomedical domain has its own characteristics such as the presence of large-sized corpora, complex technical terms, compound words, and domain-specific format and typology of questions. Due to the unique characteristics of biomedical as an application domain for passage retrieval in QA, recently, researchers have increasingly sought to incorporate and use lexical, terminological, and ontological
resources throughout their proposed methods. For example, \citet{Zhou_2007} examined the influence of incorporating deep semantic knowledge (e.g., information about concepts and relationships between concepts) in a biomedical IR system. The authors showed that appropriate use of domain specific knowledge yields about 23\% improvement over the best reported results in the Genomics Track of TREC 2006 of TREC \citep{voorheestrec}. \citet{Chen_2011} explored the hidden connection from MEDLINE documents based on a passage retrieval method. Their method first uses Mesh concepts retrieved from the sentence-level windows, and then ranks them by z-score, TFIDF (Term Frequency Inverse Document Frequency) and PMI (Pointwise Mutual Information). In the passage retrieval experiments, the authors showed that the TFIDF and PMI methods can achieve much better performance than those in the concept retrieval experiment. \citet{Cao_2011} built a clinical QA system named AskHERMES to perform robust semantic analysis on complex clinical questions. The AskHERMES system consists of three main components: question analysis, document/passage retrieval, and answer extraction. In the passage retrieval component, the authors first used the BM25 model for document retrieval, and then extracted relevant passages based on both word-level and word sequence-level similarity between a question and a sentence in the candidate answer passage.

With the introduction of biomedical QA Track at the BioASQ challenge \citep{tsatsaronis2012bioasq} in 2013, passage retrieval in biomedical QA has received much attention from NLP/IR researchers. \citet{lingeman2014umass} applied the sequential dependence model to consecutive text segments inside the document in order to create a ranking on the subdocument level. They used a granularity of 50 words, which are shifted through the document in increments of 25 words. \citet{yenalaiiith} proposed a biomedical passage retrieval method based on cosine similarity and existence test score between the question and each sentence in the retrieved documents. Their method first uses the PubMed search engine to find relevant documents and then ranks them based on cosine similarity and existence test score. Their system finally extracts the sentences from the abstracts of the 10 top relevant documents and keeps only the 10 top sentences matching most with the biomedical question after finding similarity scores for all sentences. \citet{ligeneric} presented a biomedical passage retrieval method to exactly locate the passages for the biomedical questions from the users. In their method, the expansion query based on word embedding and the sequential dependence model are used to retrieve relevant passages from the retrieved documents using. The authors first used predefined rules to identify the candidate passages and then applied the sequential dependence
model to rank them. \citet{yang2015learning} applied the LETOR model to score and rank the obtained candidate passages from the abstracts of the top-ranked documents retrieved by their document retrieval system. \cite{peng2015fudan} used statistical language model in order to retrieve relevant documents and then query keywords are extracted from the retrieved documents by giving extra credit to terms that appear close to the query keywords for passages retrieval. \cite{neves2015hpi} proposed a passage retrieval approach based on the in-memory database in biomedical QA. The system first extracts candidate passages from the relevant documents based on the built-in information retrieval features available in the IMDB, which uses approximated string similarity to match terms from the question to the words in the documents. Then, the system proceeds the ranked candidate passages and retrieves only the top-ranked passages using TFIDF metrics. \citet{Morid_2016} proposed a method for extracting clinically useful sentences from synthesized online clinical resources that represent the most clinically useful information for directly answering clinicians's information needs. The authors developed a Kernel-based Bayesian Network classification model based on different domain-specific feature types (e.g., UMLS concepts) extracted from sentences in a gold standard composed of 18 UpToDate documents.

Though the existing passage retrieval methods have proven to be quite successful at extracting passages in biomedical QA, passage retrieval still requires further efforts in order to improve its performance. The passage retrieval component in biomedical QA, compared to that of document retrieval can benefit even more from incorporation of domain-specific semantic knowledge.

In the chapter~\ref{Chapter6} we will present in details the proposed passage retrieval method in biomedical QA.


\subsection{Answer extraction}
\label{Chapter3_6_4}

Answer processing and extraction is the last component in a typical QA pipeline as the last step towards answering a biomedical question posed in natural language is extracting and generating the final answer and presenting it to the user posing the question. Answer extraction is considered the most challenging task of a QA system as this is when the precise answer has to be extracted from the candidates answers retrieved and selected by the passage retrieval component. The answer depends directly to the question type since extracting the answer for example to the factoid question (e.g., ``Which is the gene most commonly mutated in Tay-Sachs disease?''), which is asking for a biomedical named entity as an answer, is not the same as extracting the answer to a yes/no question (e.g., ``Is imatinib an antidepressant drug?''), which is looking for ``yes'' or ``no'' as an answer, for instance. In general, if a QA system deals with more than two types of questions, then the appropriate answer extraction technique, which extracts the final answer from the candidate answers, is selected and chosen according to the question type that was automatically identified by the question classification component. An answer extraction technique ranks the candidate answers according to the degree to which they match the expected answer type.

Considering both the various types of questions that may a QA system deals with and their different intended types of answers, answer extraction within the biomedical domain is a challenging task. In particular, most approaches to biomedical QA in some way make use of domain-specific semantic knowledge for answer extraction. In MedQA system \citep{lee2006beyond,Yu_2007} which dealt only with definitional questions (i.e., questions with the format of ``What is X?''), the answer extraction component which extracts definitional sentences from the retrieved documents relevant sentences, uses lexico-syntactic patterns and UMLS Methasaurus concepts (UMLS 2005AA). \cite{Cao_2011} described an answer presentation approach based on clustering technique in their developed clinical QA system AskHERMES which returns summaries as answers to all potential questions.
The authors grouped all relevant text passages into different topics based on clustering technique to locate relevant information
of interest before delving into more detail. Topics are assigned to each cluster in the AskHERMES system using content-bearing query terms and expanded terms from the UMLS Methasaurus. \cite{abacha2015means} presented an answer search methodology based on semantic search and query relaxation in their proposed semantic medical QA system, called MEANS. In the answer search phase, the authors first associated to each initial SPARQL query one or several less precise queries according to the number of expected answers. Then, the constructed SPARQL queries are executed in order to interrogate RDF triples generated on the document processing step. The authors exploited named entity recognition and relation extraction techniques, and domain-specific semantic knowledge, such as UMLS Methasaurus and UMLS Semantic Network, to build RDF annotations of the source documents and SPARQL queries representing the users questions.

With the introduction of biomedical QA Track at the BioASQ challenge \citep{tsatsaronis2012bioasq} in 2013, various answer extraction techniques have been recently presented for different types of questions.  The BioASQ challenge released benchmark datasets of biomedical questions in English, along with gold standard answers. There were four types of questions: yes/no, factoid, list, and summary questions. Yes/no questions, these are questions that require either ``yes'' or ``no'' answer. Factoid questions, these are questions that require a particular entity name (e.g., of a disease, drug, or gene), a number, or a similar short expression as an answer. List questions, these are questions that expect a list of entity names (e.g., a list of gene names, a list of drug names), numbers, or similar short expressions as an answer. Summary questions, these are questions that expect short summaries as answer. In phase B of Task b of the BioASQ challenge, participants were asked to answer with exact answers and ideal answers (paragraph-sized summaries). Exact answers are only required in the case of yes/no, factoid, list, while ideal answers are expected to be returned for questions. The released questions were accompanied by their types and the correct answers for the required elements (documents and passages) of the first phase. In this context, \cite{yang2015learning} described the development of a biomedical QA system named OAQA that returns only the exact answers for factoid and list questions based on learning-based approaches. The authors trained three supervised models and several features (e.g., lemma, the semantic type of each concept, etc.) using factoid and list questions of BioASQ training questions. The first model which is an answer type detection model aims at identifying the semantic answer type of a given question, the second assigns a score to each candidate answer while the third is a collective re-ranking model. Similarly, \cite{peng2015fudan} described a biomedical QA system that retrieves solely the exact answers for factoid and list questions. The system first used PubTator \citep{Wei_2013} for generating the candidate answers and then ranked them using term frequency metrics. \cite{choi2015snumedinfo} presented an answer extraction method based on keyword terms to generate the ideal answers for biomedical questions. In their method, candidate passages are ranked based on number of keywords and then combined to form the ideal answer. On the other hand, \cite{neves2015hpi} described answer extraction approaches for generating both exact and ideal answers to yes/no, factoid, list and summary questions from the gold-standard passages provided by the BioASQ challenge. Her approaches are notables for the use of the IMDB database and its built-in text analysis features. For yes/no questions, decision on either the answers ``yes'' or ``no'' was based on the sentiment analysis predictions provided by the IMDB. For factoid and list questions, the author extracted exact answers based on the annotations of noun phrases and topics provided by IMDB. For ideal answers, the author built summaries based on the the phrases which contain sentiments. In other work, \cite{schulze2016hpi} used an algorithm that is based on LexRank \citep{erkan2004lexrank} and named entities for the generation of summaries as answers to biomedical questions. The authors first built a sentence graph and then calculated cosine similarity of each sentence with each other sentences using named entity as dimension for the vector. They finally used a LexRank-based method to calculate the sentences ranking.


While these systems have proven to be quite successful at answering biomedical questions, they provide a limited amount of question and answer types, for instance, some of them \citep{Cao_2011} handle only with definition questions or returns solely short summaries as answers for all types of questions, and the most of the other ones do not deal with yes/no questions which are one of the most complicated question types to answer as they are seeking for a clear ``yes'' or ``no'' answer. In addition, the biomedical QA systems still requires further efforts in order to improve their performance in terms of precision to currently supported question and answer types. Furthermore, in spite of the importance of answering yes/no questions in the biomedical domain, we observed that only few studies have been presented compared to other types of questions, such as factoid and summary questions. Even though there are only two possible answers, ``yes'' or ``no,'' such questions can be quite hard to answer due to the complicated sentiment analysis process of the candidate answers.

In the chapter~\ref{Chapter7} we will present in details the proposed answer extraction methods in biomedical QA.





%Approaches to the various text mining tasks in the biomedical domain make extensive use of the resources described in this section and sometimes derive meta-resources for a specific task.
\subsection{Integral biomedical question answering systems}
\label{Chapter3_6_5}
While many efforts have been made in the biomedical QA area, only few integral QA systems, which can automatically retrieve answers to biomedical natural language questions, have been presented up to now. In this section, we survey and discuss the main integral biomedical QA systems. However, techniques and methods used in different stages of such systems have been detailed in the previous sections.

In this context, \cite{lee2006beyond,Yu_2007} designed, implemented, and evaluated a medical definitional QA system (MedQA) which is composed of five components including (1) question classification, (2) query generation, (3) document retrieval, (4) answer extraction, and (5) text summarization. In MedQA, at first, the question classification component automatically classifies medical questions into categories of the taxonomy created by \citep{Ely429} based on supervised machine-learning approaches for which specific answer search is developed. Next, the document retrieval component uses the query terms to retrieve relevant documents from either the Web documents using Google or the locally-indexed MEDLINE corpora using both Lucene to index the MEDLINE collection and the vector-space model for computing the relevance of a document to a query. Then, the answer extraction component identifies from the retrieved documents relevant sentences that answer the questions based on lexico-syntactic patterns. Finally, the text summarization component removes the redundant sentences and condenses the sentences into a coherent summary which is considered as answer. Although the MedQA system returns short summaries that could potentially answer medical questions, current MedQA's capacity is limited: it only provides answers to definitional questions.

\cite{cruchet2009trust} built a biomedical QA system called HONQA which extracts sentences from Health On the Net Foundation (HON) certified websites and provides them as answers for biomedical questions. HONQA is based on a learning approach for identifying the question type and semantic resources such as UMLS to guide the system, particularly in the choice of answers, but no details are presented in the publication. In its current form, it is not able to provide exact answers to other question types, for instance,  yes/no and factoid questions \citep{Bauer_2012}.

\cite{gobeill2009question} developed a biomedical QA system, EAGLi, which aims at extracting answers to biomedical questions from MEDLINE documents. Given a natural language question, EAGLi first analyzes the question in order to find the question type and to build the query based on a set of patterns. Then, it retrieves a set of relevant documents from MEDLINE using either PubMed or EasyIR, a local search engine in MEDLINE. Finally, the system extracts and computes a score for each of concepts expressed in the most relevant documents, and finally outputs a ranked list of candidate answers.  The current EAGLi's capacity is limited to Wh-type questions since it only covers the definitional and factoid questions.

\cite{Cao_2011} described a clinical QA system named AskHERMES that returns short summaries as answers of ad-hoc clinical questions expressed in natural language. AskHERMES was developed through the main following steps: (1) question analysis, (2) document retrieval, (3) passage retrieval, and (4) summarization and answer presentation. In the question analysis step, the authors have first classified clinical questions into general topics (e.g., device, diagnosis, epidemiology, etc.) based on SVM classifier to facilitate information retrieval and then identified keywords that capture the most important content of the question using conditional random fields. In the document retrieval step, the BM25 model was used to retrieve relevant documents. After that, they have extracted candidate passages based on dynamically generates passage boundaries and scored them based on both word-level and word sequence-level similarity in the passage retrieval step. Finally, the answer was generated based on structural clustering using content-bearing terms. The AskHERMES system returns passages (short texts) that could potentially answer all types of clinical questions. However, it returns a large number of results, which tends to defeat the intent of a QA system in reducing the amount of information that must be read. Moreover, the system supports only a single answer type in form of multiple sentence passages for all questions types \citep{Bauer_2012}.

\cite{abacha2015means} developed a semantic medical QA system called MEANS based on NLP techniques to process medical natural language questions and documents used to find answers, and semantic Web technologies at both representation and interrogation levels. MEANS is composed of three main phases: (1) corpora annotation, (2) question analysis and classification, and (3) answer search. The authors have applied NLP methods, named entity recognition and relation extraction so as to build RDF annotations of the source documents and SPARQL queries representing the users questions. They further have defined the MESA ontology  to represent the concepts and relations between them in order to construct SPARQL translations of natural language questions. To extract answers, the SPARQL queries were executed in order to interrogate RDF triples constructed in the corpus-annotation step. Even so, the authors have dealt with four questions types, they have focused on factoid and yes/no questions since more specific processes are still required to deal with complex questions (e.g. why, when).

\cite{hristovski2015biomedical} introduced a biomedical QA system, SemBT, based on semantic relations extracted from the biomedical literature. SemBT consists of three main processing steps: (1) preprocessing, (2) question processing, and (3) answer processing. During the preprocessing step, the authors first have extracted semantic relations using the SemRep natural language processing system from sentences retrieved from MEDLINE citations, and then stored them in a database. In the question processing step, the authors have constructed a query for searching in the database of the extracted semantic relations.  Finally, in the answer processing phase, they have presented the resulting semantic relations as answers in a top-down fashion, first semantic relations with aggregated occurrence frequency, then particular sentences from which the semantic relations are extracted. The SemBT system returns answers in the form of semantic relations and particular sentences from which the semantic relations are extracted. However, in its current implementation, the questions must be in the form Subject-Relation-Object, and hence, it does not allow asking questions in natural language format, for example, the natural language question ``What drugs can be used to treat diabetes?'' can be asked in SemBT as ``phsu treats diabetes'' where ``phsu'' stands for ``pharmacological substance'' and ``treats'' is the name of the semantic relation \citep{hristovski2015biomedical}.

More recently, \cite{Kraus_2017} developed the Olelo system for intuitive exploration of
biomedical literature. The Olelo system consists of three main modules: (1) question processing, (2) document/passage retrieval, and (3) answer processing. In Olelo, the question processing module is based on a system described in \citep{schulze2016hpi}. In the second module, Olelo first uses the tokens and the matched terms to formulate a query to the database so as to retrieve abstracts of the retrieved documents and then ranks the obtained abstracts according to the occurrence and importance of the searched tokens. Finally, an answer is returned to the user depending on the type of the question by the third module. Although Olelo has proven to be quite successful at answering biomedical questions, currently, Olelo supports only three question types including factoid questions, list questions and summary questions. Indeed, it does not support yes/no questions which are one of the most complicated questions to answer as they are seeking for a clear ``yes'' or ``no'' answer.


In this thesis work, our goal is to go beyond the previous biomedical QA systems and develop a QA system with the ability to automatically handle with a large amount of question types including yes/no questions, factoid questions, list questions and summary questions that are commonly asked in the biomedical domain \citep{tsatsaronis2012bioasq}. We propose a fully automated system SemBioNLQA - Semantic Biomedical Natural Language Question Answering - which has the ability to handle the kinds of yes/no questions, factoid questions, list questions and summary questions that are commonly asked in the biomedical domain. SemBioNLQA is derived from our previously established methods in (1) question classification, (2) document retrieval, (3) passage retrieval, and (4) answer extraction systems. We develop the SemBioNLQA system based on the integration of these methods and techniques. These proposed methods are presented in details in the following chapters. Table~\ref{tab:3.1c} summarizes the dimensions and characteristics of the aforementioned systems and SemBioNLQA.

\begin{table}[h!]
\centering
\caption[Question answering systems comparison matrix of features between the aforementioned systems and our proposed system SemBioNLQA]{Question answering systems comparison matrix of features between the aforementioned systems and our proposed system SemBioNLQA. The ``-'' indicates that the system did not include target question types classification.}
\label{tab:3.1c}
\begin{tabular}{M{4.1cm}M{4cm}M{2.9cm}M{3.5cm}}
\hline\noalign{\smallskip}
QA systems &Question format&Question types&Answer types \\

\noalign{\smallskip}\hline\noalign{\smallskip}
MedQA\newline \citep{lee2006beyond}&	Natural language& Definition & Summaries\\
\cmidrule(l){1-4}
HONQA\newline \citep{cruchet2009trust}&	Natural language&Definition,\newline factoid& Sentence \\
\cmidrule(l){1-4}
EAGLi\newline \citep{gobeill2009question}&	Natural language&Definition,\newline factoid& Multi-phrase passages and
a list of single Sentities \\
\cmidrule(l){1-4}
AskHERMES\newline \citep{Cao_2011} &	Natural language&-& Multiple sentence passages\\
\cmidrule(l){1-4}

MEANS\newline \citep{abacha2015means}&	Natural language	& Definition,\newline yes/no,\newline factoid&
Sentence, named entity, ``yes'' or ``no''  \\
\cmidrule(l){1-4}

SemBT\newline \citep{hristovski2015biomedical}&Subject-Relation-Object&-	& Semantic relations, sentences  \\
\cmidrule(l){1-4}

Olelo\newline \citep{Kraus_2017}&	Natural language	& factoid,\newline list,\newline summary& MeSH term, list of Mesh terms, short summaries \\

\cmidrule(l){1-4}

SemBioNLQA &	Natural language	& Yes/no,\newline factoid,\newline list,\newline summary& ``Yes'' or ``no'',\newline UMLS entity,\newline list of UMLS entity, short summaries \\
%\cmidrule(l){1-7}
\noalign{\smallskip}\hline
\end{tabular}
\end{table}

Compared with the aforementioned systems, SemBioNLQA is aimed to be able to accept a variety of natural language questions and to generate appropriate natural language answers by providing both exact and ideal answers. It provides exact answers ``yes'' or ``no'' for yes/no questions, biomedical named entities for factoid questions, and a list of biomedical named entities for list questions. In addition to exact answers for yes/no, factoid and list questions, SemBioNLQA also returns ideal answers, while it retrieves only ideal answers for summary questions.

\section{Synthesis and Positioning}
\label{Chapter3_8}

Although several methods have been proposed in biomedical QA over recent years, biomedical QA still requires further efforts in order to improve its performance. For instance, existing solutions for biomedical question classification have so far focused on extracting syntactic and semantic features from questions and using machine learning algorithms so as to classify questions into different topics. However, they do not take into account the syntactic dependency relations in questions. Intuitively, the incorporation of syntactically related pairs into other features may provide the best description and representation of questions. The motivation to find alternative features for machine-learning algorithms is the fact that words by themselves cannot capture the gist of a clinical question.

Another challenging issue in question classification is the identification of the types and formats of potential questions and intended answers, respectively. The BioASQ taxonomy of biomedical questions consists of four types of questions including yes/no questions, factoid questions, list questions and summary questions that may cover all kinds of potential questions. Note only current biomedical QA systems have limitations in terms of the types and formats of questions and answers that they can process, but also in most such systems which dealt with more than one type of questions, the users have to give or select manually the question type to each given question. As the ultimate goal of biomedical QA systems is to be able to deal with a variety of natural language questions and to generate appropriate natural language answers, biomedical question type classification is a necessary task needs so as to automatically identify the type of question and therefore to see whether the answer should be a biomedical entity name, a short summary, ``yes'' or ``no'', etc.

On the second level, one of the main observations that can be made about existing systems is that the task of document retrieval often set a framework in which an existing biomedical IR system is used, and completely depended on its ranking of documents. Indeed, there are many cases where the search engine mistakenly returns irrelevant citations high in the set or relevant citations low in the set. This problem is certainly a challenging issue as a biomedical QA system usually extracts the answers from the top-ranked documents. On the other hand, the passage retrieval stage, compared to that of document retrieval can benefit even more from incorporation of domain-specific semantic knowledge. Although the existing passage retrieval methods have proven to be quite successful at extracting passages in biomedical QA, passage retrieval still requires further efforts in order to improve its performance.

On the third level, although the importance of answering questions in the biomedical domain, until now there are only few integral systems such as the ones described in \citep{lee2006beyond,cruchet2009trust,gobeill2009question,Cao_2011,abacha2015means,Kraus_2017} that can retrieve answers to biomedical questions written in natural language. While these systems have proven to be quite successful at answering biomedical questions, they provide a limited amount of question and answer types, for instance, some of them \citep{lee2006beyond,cruchet2009trust,Cao_2011} handle only with definition questions or returns solely short summaries as answers for all types of questions, and the most of the other ones do not deal with yes/no questions which are one of the most complicated question types to answer as they are seeking for a clear ``yes'' or ``no'' answer. In addition, the biomedical QA systems still requires further efforts in order to improve their performance in terms of precision to currently supported question and answer types. Furthermore, in spite of the importance of answering yes/no questions in the biomedical domain, we observed that only few studies have been presented compared to other types of questions, such as factoid and summary questions. Even though there are only two possible answers, ``yes'' or ``no,'' such questions can be quite hard to answer due to the complicated sentiment analysis process of the candidate answers.


The biomedical QA approach we propose through this thesis work takes into account these aspects and implements innovative methods in question classification, document retrieval, passage retrieval and answer extraction components. We propose a fully automated biomedical QA system with the ability to handle the kinds of yes/no questions, factoid questions, list questions, and summary questions that are commonly asked in the biomedical domain through the use of NLP methods, machine-learning approaches, and UMLS Metathesaurus. The proposed system provides the exact answers (e.g., ``yes'', ``no'', a biomedical entity, etc.) and the ideal answers (i.e., paragraph-sized summaries of relevant information) for yes/no, factoid and list questions, while it retrieves only the ideal answers for summary questions.

In the next chapters (chapter~\ref{Chapter4}, chapter~\ref{Chapter5}, and chapter~\ref{Chapter6}), we will present in details:


\begin{itemize} %\itemsep-4pt

\item The proposed machine learning based methods for question classification in biomedical QA. The first method consists at identifying the type (i.e., yes/no, factoid, list and summary questions) of a given biomedical question in order to determine the expected answer format. It is based on our predefined set of handcrafted lexico-syntactic patterns and machine learning algorithms. The second method, which is based on lexical, syntactic and semantic features for machine learning algorithms, allows classifying questions into topics in order to filter out irrelevant answer candidates.

\item The proposed document retrieval method which retrieves relevant citations to a given biomedical question from the MEDLINE database. The proposed method first builds the query by extracting biomedical concepts, then uses a specialized IR system that gives access to the MEDLINE database to retrieve relevant documents, and finally ranks them based on a semantic similarity. We have also proposed an alternative based on a probabilistic IR model and biomedical concepts to retrieve and extract a set of relevant passages (i.e., snippets) from the retrieved documents to given biomedical questions.


\item The proposed answer extraction methods for extracting natural language answers from passages that potentially containing answers through the use of semantic knowledge, NLP techniques and statistical techniques. The first answer extraction method, based on a sentiment lexicon, aims at generating the exact answers to yes/no questions. The second method makes uses a biomedical metathesaurus to provide the exact answers suited for factoid and list questions which require with respectively a biomedical entity and a list of them as answers. The third method, aiming at retrieving the ideal answers (i.e., short summaries of relevant information) to biomedical questions, is based on a probabilistic IR model and biomedical concepts.

\item The developed biomedical QA system named SemBioNLQA which is aimed to be able to accept a variety of natural language questions and to generate appropriate answers by providing both exact and ideal answers.  SemBioNLQA, which is fully automatic system, includes innovative methods previously proposed in question classification, document retrieval, passage retrieval and answer extraction components. It is derived from our previously established contributions in each of the aforementioned components.


\end{itemize}




\section{Evaluation of Biomedical Question Answering Systems}
\label{Chapter3_7}

We introduce in this section the experimental setup that is used in this thesis work for comparing different proposed methods in biomedical QA. The experimental setup is purposed to be as uniform and consistent across the different proposed methods of the thesis work reported here, so as to facilitate the interpretation and discussion of the experimental results. This experimental setup furthermore aims at making easier the reproducibility of our experiments and the comparison with our results, avoiding particular experimental setup decisions which might provide unfair advantages to our proposed methods. In subsection~\ref{Chapter3_7_1} and subsection~\ref{Chapter3_7_2} the evaluation datasets and evaluation measures will be introduced, respectively.


\subsection{Evaluation datasets}
\label{Chapter3_7_1}

Several fora such as BioASQ challenges, QA4MRE Alzheimer Disease, and TREC Genomics Track have been organized to promote research and benchmarking on biomedical QA from textual data.


The BioASQ challenge\footnote{The BioASQ challenge: \url{http://www.bioasq.org/}} \citep{tsatsaronis2012bioasq} which started in 2013, within 2017 edition \citep{Nentidis_2017}, comprised three tasks:
\begin{itemize}
  \item Task 5a on Large-Scale Online Biomedical Semantic Indexing
  \item Task 5b on Biomedical Semantic Question Answering
  \item Task c on Funding Information Extraction From Biomedical Literature
\end{itemize}

BioASQ which was part of a EU-funded project, was developed to boost state of the art performance for biomedical QA.  In Task 5b, BioASQ has provided the benchmark datasets which contain development and test biomedical questions, in English, along with golden standard answers. The challenge, within each edition, the organizers released a training set of biomedical questions-answers pairs and test sets of questions. There were four types of questions: yes/no questions, factoid questions, list questions, and summary questions. The benchmark datasets have been created by the BioASQ team of biomedical experts. The goal of Task 5b is to assess the performance of QA systems in different stages of the QA process. It is sub-divided into two phases: phase A and phase B. In phase A participants had to respond with biomedical concepts, relevant documents, relevant passages, and RDF triples. In phase B participants were asked to answer with exact answers and ideal answers (paragraph-sized summaries). Exact answers are only required in the case of yes/no, factoid, list, while ideal answers are expected to be returned for questions. The released questions were accompanied by their types and the correct answers for the required elements (documents and passages) of the first phase. Each batch of questions was released every two weeks and participants had 24 hours to submit results.


The QA4MRE (Question Answering for Machine Reading Evaluation) for biomedical text about Alzheimer's Disease took place in two editions (2012 \citep{morante2012machine} and 2013 \citep{morante2013machine}) and aimed to boost development of solutions in machine reading. Participant systems were asked to extract and generate the answers to a set of questions. A set of full text documents were provided along with ten questions, each of which had five candidate answers.  The test set consists of four reading tests where each test contains one document and ten questions related to that document as well as five possible answers per question. Questions are in the form of multiple choice, a particular type of factoid questions that are less complex in comparison to typical factoid questions, as participant systems can use the possible answers to query for candidate answers and do not need to find the final answer but just choose the most likely one.

The TREC Genomics challenge which is part of TREC, took place in 2006 \citep{voorheestrec} and 2007 \citep{voorheestrec2007} and aimed to foster development of solutions in passage retrieval as part of QA systems with focus on particular biomedical entity types, such as diseases, mutations, genes, proteins, and pathways. The challenge organizers have provided a set of 162.259 full text documents collected from about 49 journals related to genomics. Participants were asked to retrieve relevant passages to a given topic question from a collection of 162.259 documents. A set of 28 and 36 topic questions was constructed in 2006 and 2007, respectively. Evaluation for each participant system was carried out manually by TREC experts who were provided with the top scoring 1000 passages for each topic question.


Table~\ref{tab:3_7_1.1} shows different datasets provided by the BioASQ challenge, the QA4MRE challenge and the TREC Genomics challenge.


\begin{table}[h!]
\centering
\caption{Comparing biomedical QA datasets provided by the BioASQ challenge, TREC Genomics and QA4MRE Alzheimer Disease}
\label{tab:3_7_1.1}
\begin{tabular}{p{5cm}p{4.1cm}p{1.2cm}p{1.1cm}p{0.6cm}p{1.7cm}}
\hline\noalign{\smallskip}
Dataset &Questions (Train+Test)&Yes/No&Factoid&List&Summary\\

\noalign{\smallskip}\hline\noalign{\smallskip}
TREC 2006 Genomics&28&&&&\multicolumn{1}{c}{\ding{51}}\\%\xmark
TREC 2007 Genomics&36&&&&\multicolumn{1}{c}{\ding{51}}\\
QA4MRE Alzheimer Disease&40&&\multicolumn{1}{c}{\ding{51}}& \multicolumn{1}{c}{\ding{51}}&\\
BioASQ 2013&29+282&\multicolumn{1}{c}{\ding{51}}&\multicolumn{1}{c}{\ding{51}}&\multicolumn{1}{c}{\ding{51}}&\multicolumn{1}{c}{\ding{51}}\\
BioASQ 2014&310+500&\multicolumn{1}{c}{\ding{51}}&\multicolumn{1}{c}{\ding{51}}&\multicolumn{1}{c}{\ding{51}}&\multicolumn{1}{c}{\ding{51}}\\
BioASQ 2015&810+500&\multicolumn{1}{c}{\ding{51}}&\multicolumn{1}{c}{\ding{51}}&\multicolumn{1}{c}{\ding{51}}&\multicolumn{1}{c}{\ding{51}}\\
BioASQ 2016&1307+500&\multicolumn{1}{c}{\ding{51}}&\multicolumn{1}{c}{\ding{51}}&\multicolumn{1}{c}{\ding{51}}&\multicolumn{1}{c}{\ding{51}}\\
BioASQ 2017&1799+500&\multicolumn{1}{c}{\ding{51}}&\multicolumn{1}{c}{\ding{51}}&\multicolumn{1}{c}{\ding{51}}&\multicolumn{1}{c}{\ding{51}}\\
\noalign{\smallskip}\hline
\end{tabular}
\end{table}


To evaluate the different proposed methods of the thesis work reported here, we have used the datasets provided by the BioASQ challenge since as shown in Table~\ref{tab:3_7_1.1} they include a variety of questions types and answers, and also a large number of training and test questions. For these reasons, recently the BioASQ datasets are the most widely used to test the effectiveness of different parts of biomedical QA systems \citep{balikas2014results,balikas2015results,krithara2016results,Nentidis_2017}. Table~\ref{tab:3_7_1.2} shows the BioASQ 3b training questions used in this thesis work. The dataset include 810 questions-answers where each question was assigned to one category (yes/no, factoid, list, or summary). Table~\ref{tab:3_7_1.3} shows the test sets used to evaluate and compare the proposed methods in biomedical QA with current state-of-the-art methods. Table~\ref{tab:3_7_1.4} presents some examples of biomedical questions and their categories from BioASQ training questions.


\begin{table}[!htbp]
\centering
\caption{The question types and the number of BioASQ training questions assigned to each category}
\label{tab:3_7_1.2}
\begin{tabular}{p{5.3cm}p{4.5cm}p{5.2cm}}
\hline\noalign{\smallskip}
Question type & \#Questions & Category percentage  \\
\noalign{\smallskip}\hline\noalign{\smallskip}
Yes/No	& 237 &	29.26\%\\
Factoid	& 192 &	23.70\% \\
List	& 213 & 26.30\% \\
Summary	& 168 &	20.74\% \\
\noalign{\smallskip}\hline
\end{tabular}
\end{table}

\begin{table}[h!]
\centering
\caption{Number of questions and their categories in test sets of biomedical questions provided in the 2015, 2016, and 2017 and BioASQ challenges}
\label{tab:3_7_1.3}
\begin{tabular}{p{4cm}p{1.6cm}p{1.3cm}p{1.3cm}p{0.8cm}p{2cm}p{2.3cm}}
\hline\noalign{\smallskip}
\multirow{2}{*}{BioASQ dataset} &\multirow{2}{*}{Batch}&  \multicolumn{4}{c}{Question type} &\multirow{2}{*}{\#Total}\\
\cmidrule(l){3-6} &&\#Yes/No	& \#Factoid& \#List& \#Summary&\\

\noalign{\smallskip}\hline\noalign{\smallskip}
\multirow{5}{*}{BioASQ Task 3b 2015}  & Batch 1&	33	&26 &22 &19& 100 \\
& Batch 2&16&32 &28 &24& 100 \\
& Batch 3&29&26 &17 &28& 100 \\
& Batch 4&25&29 &23 &20& 97 \\
& Batch 5&28&22 &24 &26& 100 \\
\cmidrule(l){1-7}
\multirow{5}{*}{BioASQ Task 4b 2016}  & Batch 1&	28	&39 &11 &22& 100 \\
& Batch 2&32&31 &21 &16& 100 \\
& Batch 3&25&26 &21 &28&  100\\
& Batch 4&21&31 &15 &33&  100\\
& Batch 5&27& 34& 20&16&  97\\
\cmidrule(l){1-7}
\multirow{5}{*}{BioASQ Task 5b 2017}  & Batch 1&	17	&25 &22 &36& 100 \\
& Batch 2&27&31 &15 &27& 100 \\
& Batch 3&31&26 &15 &28&  100\\
& Batch 4&29&33 &13 &25&  100\\
& Batch 5&26& 35& 22&17&  100\\
%\cmidrule(l){1-8}
\noalign{\smallskip}\hline
\end{tabular}
\end{table}

\begin{table}[h!]
\centering
\caption{Question categories with some examples of biomedical questions collected from BioASQ training dataset}
\label{tab:3_7_1.4}
\begin{tabular}{M{2cm}M{13.4cm}}
\hline\noalign{\smallskip}
Category  & \hspace{0.5cm}Sample questions   \\
\noalign{\smallskip}\hline\noalign{\smallskip}
Yes/No &\begin{minipage}[t]{\linewidth}
 \begin{itemize} [nosep,nolistsep]
            %\itemsep-4pt
           \item Does SCRIB deregulation promote cancer?
           \item Is CADASIL syndrome a hereditary disease?
           \item Can PLN mutations lead to dilated cardiomyopathy?
         \end{itemize} \end{minipage}\\
         \cmidrule(l){1-2}
Factoid & \begin{minipage}[t]{\linewidth}\begin{itemize}[nosep,nolistsep]
          % \itemsep-4pt
            \item Which gene is involved in CADASIL?
            \item What is the inheritance pattern of Hunter disease or mu-copolysaccharidosis II?
            \item How many genes are in the gene signature screened by MammaPrint?
          \end{itemize}\end{minipage}\\
          \cmidrule(l){1-2}
List &\begin{minipage}[t]{\linewidth}\begin{itemize}[nosep,nolistsep]
       % \itemsep-4pt
        \item Which are the clinical characteristics of TSC?
        \item Which proteins induce inhibition of LINE-1 and Alu retrotransposition?
        \item What is being measured with an accelerometer in back pain patients
      \end{itemize}\end{minipage}\\
       \cmidrule(l){1-2}
Summary & \begin{minipage}[t]{\linewidth}\begin{itemize}[nosep,nolistsep]
            %\itemsep-4pt
            \item What is the treatment of acute pericarditis?
            \item How does trimetazidine affect intracellular kinase signal-ing in the heart?
            \item Why does the prodrug amifostine (ethyol) create hypoxia?
          \end{itemize}\end{minipage}
\\
\noalign{\smallskip}\hline
\end{tabular}
\end{table}

In particular, in a part of this thesis work (cf. section~\ref{Chapter4.3} of chapter~\ref{Chapter4}), we have used the publicly available data set\footnote{Set of clinical questions available at: \url{http://clinques.nlm.nih.gov/about.html}} of clinical questions maintained by the U.S. NLM in order to evaluate the effectiveness of the method we propose for question topic classification in biomedical QA. This benchmark collection of questions has been widely used in the literature to test and evaluate question topic classification methods such as in \citep{yu2008automatically,Cao_2010}. All these questions have been manually labelled and released by the U.S. NLM. This data set contains 4654 ad hoc clinical questions, collected from healthcare providers across the USA in four studies \citep{Ely_1999,elyjhon,ely1997lifelong,D_Alessandro_2003}. There are a total of 12 categories and each question is assigned to one or more categories. Some examples of clinical questions and their categories are shown in Table~\ref{tab:4.3.3}. The typology of those questions is illustrated in Table~\ref{tab:4.3.4}. Table~\ref{tab:4.3.5} presents the 12 categories and the number of the clinical questions assigned to each one. 3559 questions, 386 questions, 700 questions, four questions, five questions
are assigned to one category, two categories, three categories, four categories, and five categories, respectively.

\begin{table}[!h]
\centering
\caption{Examples of clinical questions and their topics maintained by the U.S. National Library of Medicine}
\label{tab:4.3.3}
\begin{tabular}{M{11cm}M{4.5cm}}
\hline\noalign{\smallskip}
Question & Topics \\
\noalign{\smallskip}\hline\noalign{\smallskip}
Mother is alcoholic and abuses tobacco. What are statistics regarding inheritance of tobacco abuse and relationship to social
situation?	& Epidemiology\\\cmidrule(l){1-2}

Does she have any underlying inflammation of her kidneys? Creatinine approximately 1.0, 3+ albumin on urinalysis, just over
500 milligrams protein/24 hrs, normal intravenous pyelogram. & Management \newline Diagnosis\\\cmidrule(l){1-2}

Coronary angioplasty and stent placed last week. Started on Ticlid, looks like she’s allergic to it. She’s supposed to be on
Ticlid one more week. Obviously we've got to stop it. Do they want her on something else or just stop it? & Management \newline Treatment \&
Prevention \newline Pharmacological\\

\noalign{\smallskip}\hline
\end{tabular}
\end{table}


\begin{table}[!h]
\centering
\caption[Typology of 4654 clinical questions and their representatives. The first column represents generic question proportions]{Typology of 4654 clinical questions and their representatives. The first column represents generic question proportions. The second column represents number of each question types and their percentages. Respectively, question examples are in the last column \citep{Cao_2010}}
\label{tab:4.3.4}
\begin{tabular}{M{3cm}M{4.3cm}M{7.8cm}}
\hline\noalign{\smallskip}
Question type &\#Question (percentage)& Question example \\
\noalign{\smallskip}\hline\noalign{\smallskip}
What& 2231 (48\%)& What is Endolimax nana and should you treat it?\\
Which& 62 (1\%)& Which dose of Premarin is green?\\
Why& 134 (3\%)& Why is she having pelvic pain?\\
How& 697 (15\%)& How do you inject the bicipital tendon?\\
Can& 187 (4\%) &Can Lorabid cause headaches?\\
Do& 320 (7\%) &Do we need to do a spinal tap to rule out meningitis?\\
Others& 1023 (22\%)& Is this respiratory plan for an extubated child okay?\\
\noalign{\smallskip}\hline
\end{tabular}
\end{table}

\begin{table}[!htbp]
\centering
\caption[The topics of 4654 clinical questions, the number of the clinical questions assigned and the percentage of the total questions]{The topics of 4654 clinical questions, the number of the clinical questions assigned and the percentage of the total questions \citep{Cao_2010}}
\label{tab:4.3.5}
\begin{tabular}{M{5.2cm}M{4.8cm}M{5cm}}
\hline\noalign{\smallskip}
Topics &\#Questions &Questions percentage\\
\noalign{\smallskip}\hline\noalign{\smallskip}
Device& 37& 0.8\%\\
Diagnosis& 994& 21.4\%\\
Epidemiology &104& 2.2\%\\
Etiology &173& 3.7\%\\
History& 42& 0.9\%\\
Management& 1403& 30.1\%\\
Pharmacological& 1594& 34.3\%\\
Physical Finding& 271& 5.8\%\\
Procedure &122& 2.6\%\\
Prognosis &53& 1.1\%\\
Test &746& 16.0\%\\
Treatment \& Prevention& 868& 18.7\%\\
Unspecified& 0& 0\%\\
\noalign{\smallskip}\hline
\end{tabular}
\end{table}

\subsection{Evaluation measures}
\label{Chapter3_7_2}


Several indicators have been used in this thesis work to evaluate the effectiveness of the proposed methods in biomedical QA. As shown previously, a typical QA system consists of four stages including question classification, document retrieval, passage retrieval and answer extraction, that can be studied and evaluated independently. Therefore, we present in this subsection the typical evaluation measures used for each stage.
\subsubsection{Evaluation measures for question classification}

The accuracy metric has been widely used in order to evaluate question type classification methods \cite{khoury2011question,li2002learning,xu2006syntactic,loni2011survey}. Accuracy, as it is defined in equation~\ref{eq:1qc}, is the number of correct made predictions divided by the total number of made predictions.
\begin{equation}\label{eq:1qc}
Accuracy= \frac {No.\; of \;{Correctly} \;{Classifed} \;{Questions}}{Total\; No.\;of\; Tested\; Questions}
\end{equation}

Additionally, precision, recall, and F1-measure are widely used to evaluate the effectiveness of question type classification methods at classifying questions into each of the predefined question types (multi-class classification). For each question type, precision, recall, and F1-measure are computed using the standard equation~\ref{eq:2qc}, equation~\ref{eq:3qc}, and equation~\ref{eq:4qc}, respectively. Precision is defined as the number of true positive over the number of true positive plus the number of false positive. Recall is defined as the number of true positive over the number of true positive plus the number of false negative. In other words, precision is the fraction of correct classification for a certain category, whereas recall is the fraction of instances of a category that were correctly classified. F1-measure also know as F1-score is defined as the harmonic mean of precision and recall.

\begin{equation}\label{eq:2qc}
Precision= \frac {True\; Positive}{True\; Positive+False\; Positive}
\end{equation}

\begin{equation}\label{eq:3qc}
Recall= \frac {True\; Positive}{True\; Positive+False\; Negative}
\end{equation}
\begin{equation}\label{eq:4qc}
{F1-measure}={F1-score}= \frac {2*Precision*Recall}{Precision+Recall}
\end{equation}


\subsubsection{Evaluation measures for document and passage retrieval}

As indicators of retrieval effectiveness, mean precision, mean recall, mean F1-measure and mean average precision (MAP) were used. Given a set of golden items (documents and passages), and a set of items (documents and passages) returned by document and passage retrieval system (for a particular biomedical question in our case), precision, recall, and F1-measure are calculated using the previously defined equation~\ref{eq:2qc}, equation~\ref{eq:3qc}, and equation~\ref{eq:4qc}, respectively, where true positives is the number of returned items that are also present in the golden set, false positives is the number of returned items that are not present in the golden set, and false negatives is the number of items of the golden set that were not returned by the system. Given a set of questions $q_1,q_2, \ldots, q_n$, mean precision, mean recall, and mean F1-measure of the document and passage retrieval system is obtained by averaging its precision, recall, and F1-measure, respectively, for all the questions \citep{Balikas_2013}.

As precision, recall, and F1-measure do not take into account the order of the items returned by IR system for each question,  it is common in IR
to compute MAP of the returned set of items \citep{Balikas_2013}. MAP is obtained by averaging the average precision (AP) of over a set of questions, defined in the following equation~\ref{eq:5ir}:



\begin{equation}\label{eq:5ir}
MAP= \sum_{i=1}^{n} AP_i
\end{equation}

where $AP_i$ is the average precision of the set returned for question $q_i$, as defined in equation~\ref{eq:6ir}, where $|L|$ is the number of items in the set, $|L_R|$ is the number of relevant items (documents or passages), $P(r)$ is the precision when the returned set is treated as containing only its first $r$ items, and $rel(r)$ equals 1 if the r-th item of the set is in the golden list (i.e., if the r-th item is relevant) and 0 otherwise returned for all the questions \citep{Balikas_2013}.

\begin{equation}\label{eq:6ir}
AP=  \frac {\sum_{r=1}^{|L|} P(r)*rel(r)}{|L_R|}
\end{equation}



\subsubsection{Evaluation measures for answer extraction}

The performance of the proposed answer extraction methods are evaluated using the evaluation measures described by the BioASQ challenge \citep{tsatsaronis2012bioasq}. In the case of yes/no questions, the exact answers had to be either ``yes'' or ``no''. Therefore, accuracy
is the most used evaluation metric to evaluate responses of yes/no questions. Let $n$ be, the number of yes/no questions, and $k$ the number of correctly answered yes/no questions, accuracy is computed using the following equation~\ref{eq:2ae}:

\begin{equation}\label{eq:2ae}
Accuracy= \frac {k}{n}
\end{equation}

For factoid questions, mean reciprocal rank (MRR) is the main measure used to evaluate the exact answers returned by the system. Assuming that there are $n$ factoid questions, MRR is computed using the equation~\ref{eq:3ae} where $r_i$ is the topmost position that contains the
golden entity name (or one of its synonyms) in the returned list of possible responses.

\begin{equation}\label{eq:3ae}
MRR= \frac {1}{n} * \sum_{i=1}^{n}\frac {1}{r_i}
\end{equation}

On the other hand, to evaluate the exact answers of list questions, the mean average precision, mean average recall, and mean average
F-measure metrics, which are computed by averaging precision, recall, and F1-measure over the list questions are used. These measures (i.e., precision, recall, and F1-measure) are computed using the standard equations defined in (\ref{eq:2qc}), (\ref{eq:3qc}) and (\ref{eq:4qc}), where true positives is the number of possible answers that are included both in the returned and the golden list; false positives is the number of possible answers that are mentioned in the returned, but not in the golden list; and false negatives is the number of possible answers that are included in the golden, but not in the returned list. Indeed, mean average F1-measure is the the official score used by the BioASQ challenge to rank the participant systems for list questions.


Finally, the ideal answers of questions (all questions types: yes/no, factoid, list and summary), were automatically evaluated using ROUGE-2 and ROUGE-SU4. Basically, ROUGE counts the overlap between an automatically constructed summary by the system and a set of golden summaries manually constructed by humans. More details of these evaluations metrics appear in \citep{Balikas_2013}.




\section{Summary of the Chapter}
\label{Chapter3_9}

In this chapter, we have presented a state of the art on QA systems which aim at answering natural language questions from textual documents.  We have started this chapter by presenting an introduction to question answering. We have described in details the generic architecture of QA systems. We then have presented specificities and characteristics of QA in the biomedical domain, respectively. We have also presented the main resources that can be exploited for QA in the biomedical domain. Next, we have reviewed current research efforts directed toward QA in the biomedical domain. We have described in details techniques and methods that have been proposed for each component of a biomedical QA system, including question classification, document retrieval, passage retrieval and answer extraction. Finally, we have presented a synthesis of the different methods, and described the metrics used for the evaluation of the methods we proposed in this Phd thesis.

The following chapters (chapter~\ref{Chapter4}, chapter~\ref{Chapter5}, and chapter~\ref{Chapter6}) are dedicated to the presentation of our methods for question classification, document retrieval, passage retrieval and answer extraction in biomedical QA. 