% Chapter 1

\chapter*{General Introduction} % Main chapter title
\addcontentsline{toc}{chapter}{General Introduction}
\label{Chapter1} % For referencing the chapter elsewhere, use \ref{Chapter1}
\markboth{General Introduction}{General Introduction}
%----------------------------------------------------------------------------------------



% Define some commands to keep the formatting separated from the content
\newcommand{\keyword}[1]{\textbf{#1}}
\newcommand{\tabhead}[1]{\textbf{#1}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\file}[1]{\texttt{\bfseries#1}}
\newcommand{\option}[1]{\texttt{\itshape#1}}

%----------------------------------------------------------------------------------------


\section*{Motivation}

\addcontentsline{toc}{section}{Motivation}

Over the course of the last decades, research in the biomedical field has received a fast growing interest from the research community, reflected by an exponential growth in the peer-reviewed scientific literature. MEDLINE\footnote{MEDLINE: \url{https://www.nlm.nih.gov/pubs/factsheets/medline.html}}, the U.S. National Library of Medicine (NLM) bibliographic database, contains more than 24 million references to journal articles in life sciences with a concentration on biomedicine. As shown in Figure~\ref{fig:MEDLINEstatistics} which shows the total number of citations added to MEDLINE by fiscal year, hundreds of thousands of citations are added to the database each year. For instance, more than 806,000 and 869,000 citations were added in 2015 and 2016 respectively. These statistics come from the official site of the U.S. NLM \footnote{U.S. National Library of Medicine: \url{https://www.nlm.nih.gov/bsd/index_stats_comp.html\#footnote1)}}.

% Figure environment removed

Unsurprisingly, this explosive rise in the amount of written scientific knowledge has made it difficult to absorb all relevant information even for experts in their field of interest. In this sense, recent reports such as the one presented by
\cite{hristovski2015biomedical}, have highlighted that the most widely used are specialized and domain-specific classical information retrieval (IR) systems, also known as search engines, such as PubMed\footnote{PubMed: \url{https://www.ncbi.nlm.nih.gov/pubmed/}} which gives access to the MEDLINE database. To given queries, usually expressed in terms of some keywords or concepts, traditional IR systems return a large number of citations that are potentially relevant. Indeed, the set of the retrieved documents represents an answer size that is still too large to identify the precise information readily. Moreover, in traditional IR, the users have often to deal with the burden of studying and filtering the returned citations of their queries so as to find the precise information they were looking for.
In this context, an evaluation study presented by \citet{ely2000taxonomy} showed that physicians spent on average less than two minutes looking for information to answer clinical questions, although many of their questions remained unanswered. In another study by \citet{Hersh283}, at least 30 minutes are needed on average for medical and nurse practitioner students to answer clinical questions using MEDLINE.

Retrieving short, accurate answers to a given natural language question from the ever-increasing volume of biomedical literature is the key to creating high-quality systematic reviews that support the practice of evidence-based medicine \citep{Sarker_2016,Ji_2017} and improve the quality of patient care \citep{ely2002obstacles,Kopanitsa_2017,Kropf_2017}. However, \citet{elyjhon} have shown that physicians do not seek answers to many of their natural language questions, often suspecting a lack of usable information. The most commonly reported obstacles to the search of an answer were the doctors's doubt that an answer existed and also the failure of the selected database to provide an answer.

To reduce searching, filtering and browsing time and effort while maximizing usefulness of that scientific knowledge, more accurate systems are needed such as question answering systems \citep{Wren_2011}. Question answering (QA), unlike classical IR, aims at providing information seekers with precise and specific answers, by automatically analyzing thousands of articles based on information extraction (IE) and natural language processing (NLP) techniques, instead of providing a large number of citations that are potentially relevant for the natural language questions posed by the inquirers \citep{lee2006beyond,athenikos2010biomedical,Bauer_2012,neves2015question}. Furthermore, QA systems, which can help users locate useful information quickly, take questions expressed in natural language (e.g., why, how, when, etc.) and extract precise answers by linguistically and semantically processing both questions and data sources under consideration. For example, for the biomedical question ``What is the name for anorexia in gymnasts?'', a QA system would provide a particular name as an answer , i.e., Anorexia Athletica, for anorexia in gymnasts.

Due to the importance of QA systems in the biomedical domain, a great number of research groups in the areas of IR, NLP and artificial intelligence have dedicated great efforts in the development of new methodologies to automatically answer biomedical questions from the scientific
literature. This is the scope of this thesis work reported here.

\section*{Background}
\addcontentsline{toc}{section}{Background}

Question answering in the open domain, a longstanding challenge widely studied, has received much attention by the IR community, initiated by the QA Track in Text REtrieval Conference (TREC\footnote{TREC: \url{http://trec.nist.gov/}}) evaluations, which takes place regularly every year since 1999 \citep{voorhees1999trec}. Since then, various open-domain QA systems have been proposed and developed \citep{voorhees1999trec,roberts2002information,Monz_2003,gaizauskas2004information,Collins_Thompson_2004,voorhees2005trec,Teufel}. Nevertheless,
only few efforts have been made in the biomedical domain due to a variety of reasons and conditions. As has been extensively documented in recent survey on biomedical QA \citep{athenikos2010biomedical}, open-domain QA is concerned with the questions that are not restricted to any domain, while in restricted-domain QA such as the biomedical one, the application domain provides a context for the QA process. \cite{athenikos2010biomedical} have also summarised the main characteristics of QA in the biomedical domain in three points: (1) large-sized textual corpora, (2) highly complex domain specific terminology, and (3) domain-specific format and typology of questions. The most basic obstacle is that there exists a vast amount of textual data, especially in the form of scientific articles that is constantly and rapidly increasing as new scientific articles are published every day in the biomedical domain. Furthermore, not only these data are generally expressed in natural language, which makes its automated processing more difficult and complex, but also terminological variations and synonymy make question answering generally more difficult for the biomedical domain.


Recently, especially since the introduction of biomedical QA Track at the BioASQ\footnote{BioASQ challenge: \url{http://www.bioasq.org/}} challenge \citep{tsatsaronis2012bioasq}, biomedical QA systems have attracted an increasing level of interest by researchers \citep{balikas2014results,balikas2015results,krithara2016results,Nentidis_2017}. Despite this considerable progress, there is a general awareness that there are still many open challenges and controversial issues that affect the current state of biomedical QA systems. The current state of the existing biomedical QA systems does not deal with all types of questions and answers. Furthermore, until recently only few integral QA systems such as the ones presented in \citep{lee2006beyond,cruchet2009trust,gobeill2009question,Cao_2011,abacha2015means,Kraus_2017} can automatically retrieve answers to biomedical questions written in natural language. While such systems have proven to be quite successful at answering biomedical questions, they accept a limited amount of question types and, furthermore, they provide a limited amount of answer types. For instance, some of them \citep{lee2006beyond,cruchet2009trust,Cao_2011}  only handle definition questions or return solely short summaries as answers for all types of questions, and the most of the other ones do not deal with yes/no questions which are one of the most complicated question types to answer as they are seeking for a clear ``yes'' or ``no'' answer. Moreover, the biomedical QA systems still require further efforts in order to improve their performance in terms of precision to currently supported question and answer types.


\section*{Research Goals}
\addcontentsline{toc}{section}{Research Goals}

The aim of this thesis work is to propose new methods in biomedical QA so as to enable a user to find an accurate answer to his human natural language question. A typical QA system can be viewed as a pipeline composed of many components, including question classification, document retrieval, passage retrieval, and answer extraction each of which has to deal with specific challenges and issues \citep{HIRSCHMAN_2001,athenikos2010biomedical,neves2015question,abacha2015means}. We consider that the improvement of such fundamental dimensions of the usefulness of question answering has to take into account the problems lying in each of the aforementioned components. It is a matter of proposing, designing, and evaluating a system which is able to automatically identify the types of questions, relevant documents, relevant passages, and precise answers. To achieve this purpose, we have pursued the following research goals.

\begin{itemize}

 \item \textbf{Research Goal 1} is the proposal of sophisticated methods for improving QA performance in the biomedical domain through the combination of NLP/IR techniques, machine-learning methods, and domain-specific knowledge resources.

 \item \textbf{Research Goal 2} is to deal with all types of natural language questions and answers. As stated in the motivation section, the current state of the existing biomedical QA systems does not deal with all types of questions and answers.

 \item \textbf{Research Goal 3} is to develop a fully automated semantic biomedical QA system which is able to accept a variety of natural language questions and to generate appropriate answers by providing both exact and ideal answers.
\end{itemize}




\section*{Contributions}
\addcontentsline{toc}{section}{Contributions}


By achieving the goals noted above, this thesis work makes various contributions to the field of question answering for the biomedical domain. This thesis work proposes new methods to improve QA performance in the biomedical domain through the combination of NLP techniques, machine-learning methods, and domain-specific knowledge resources. The proposed system, as shown in Figure~\ref{fig:QAA1}, is composed of many components which are: (1) question classification and query reformulation, (2) document retrieval, (3) passage retrieval, and (4) answer extraction.

The first component receives the input entered by the user, i.e., a natural language question (e.g., what, why, where, etc.), and includes preprocessing of the question, identification of the question type and the expected answer format to be required, as well as building a query which is an input to document retrieval, the second component. A document retrieval system is used to retrieve documents satisfying the query. After that, top-ranked passages are extracted from top-ranked documents by the passage retrieval, the third component. The output of this component is a set of top-ranked passages which is used as a set of candidate answers and as input to the last component, answer extraction. In this component, the candidate answers are matched against the expected answer type generated by the first component and ranked by how well they satisfy the user question using an appropriate answer extraction method. Finally, the top-ranked candidate answers and the raw texts from which the answers were extracted are shown to the user.

% Figure environment removed


This thesis work mainly revolves around the following contributions:

The \textbf{first contribution} of this thesis work is the proposal of two machine learning based methods for question classification in biomedical QA. The first method \citep{kdir15,Sarrouti_MIM_2017} consists at identifying the type of a given biomedical question in order to determine the expected answer format. It is based on our predefined set of handcrafted lexico-syntactic patterns and machine learning algorithms. The second method \citep{Sarrouti_IBRA_2017}, which is based on lexical, syntactic and semantic features for machine learning algorithms, allows classifying questions into topics in order to filter out irrelevant answer candidates.

The \textbf{second contribution} of this thesis work is, at first, the proposal of a document retrieval method \citep{Sarrouti_2016} which retrieves relevant citations to a given biomedical question from the MEDLINE database. The proposed method first builds the query by extracting biomedical concepts, then uses a specialized IR system that gives access to the MEDLINE database to retrieve relevant documents, and finally ranks them based on a semantic similarity. We also propose an alternative \citep{Sarrouti_2017} based on a probabilistic IR model and biomedical concepts to retrieve and extract a set of relevant passages (i.e., snippets) from the retrieved documents to given biomedical questions.

The \textbf{third contribution} of this thesis work is the proposal of effective methods for answer extraction from passages that potentially contain answers through the use of semantic knowledge, NLP techniques and statistical techniques. The first answer extraction method \citep{Sarrouti_yes_2017}, based on a sentiment lexicon, aims at generating the exact answers to yes/no questions. The second method uses a biomedical metathesaurus to provide the exact answers suited for factoid and list questions which require with respectively a biomedical entity and a list of them as answers. The third method, aiming at retrieving the ideal answers (i.e., short summaries of relevant information) to biomedical questions, is based on a probabilistic IR model and biomedical concepts \citep{Sarrouti_bioasq_2017}.

The \textbf{fourth contribution} of this thesis work is the development of a fully automated semantic biomedical QA system named SemBioNLQA which is aimed to be able to accept a variety of natural language questions and to generate appropriate answers by providing both exact and ideal answers.  SemBioNLQA, which is a fully automatic system, includes innovative methods previously proposed in question classification, document retrieval, passage retrieval and answer extraction components. It is derived from our previously established contributions in each of the aforementioned components.

\section*{Structure}
\addcontentsline{toc}{section}{Structure}

The overview of this thesis is structured as follows:

\begin{itemize}

\item \textbf{Chapter~\ref{Chapter2}} presents general concepts of biomedical natural language processing and text mining. First, we describe some natural language processing tasks we use as constructing blocks for the new methods, such as sentence splitting, word tokenization, n-grams, part-of-speech tagging, stemming and lemmatization, and parsing. Then, we introduce biomedical text mining tasks, such as information retrieval, document classification, named entity recognition, relation or information extraction, document summarization and question answering. We also describe a broader classification framework involving QA systems based on different criteria, and important factors that distinguish restricted-domain QA from open-domain.



\item \textbf{Chapter~\ref{Chapter3}} describes the state-of-the-art on the topics of interest for this thesis work. First, we provide a brief introduction to question answering and its generic architecture. Next, we describe the main characteristics of QA in the biomedical domain, and the resources available for biomedical QA. Then, we present related work and discussion about the main QA approaches with a particular focus on the biomedical domain. Finally, we describe the setup of the experiments of our contributions. In particular, we describe in details the datasets and evaluations metrics used for the assessment of our contributions.


\item \textbf{Chapter~\ref{Chapter4}} describes in details the methods we propose for question classification in biomedical QA. We first describe our proposed method for biomedical question type classification. This method aims at classifying questions into one of the four categories: yes/no questions, factoid questions, list questions and summary questions. We then present our method for question topic classification. This method aims at automatically assigning one or more general topics (e.g., treatment, test, etc.) to clinical questions. Evaluation and results are also presented for each method.

\item \textbf{Chapter~\ref{Chapter5}} presents our contributions for the improvement of both document retrieval and passage retrieval in biomedical QA. We first describe our document retrieval system in which we propose a new document re-ranking method. We then describe our passage retrieval method which retrieves the relevant passages/snippets that are likely to contain the answer for a given biomedical question. Evaluation and results are also presented for each contribution.

\item \textbf{Chapter~\ref{Chapter6}} describes in details the methods we propose for the exact and ideal answer extraction. It also presents our semantic QA system, called SemBioNLQA. Evaluation and results are presented for each contribution as well.


\item \textbf{\hyperref[Chapter7]{\textcolor{black}{Conclusion and Future Work}}} offers the conclusion of this thesis work and indicates directions for future works which are either in progress or those we plan to carry out for the continuity of the systems that have been developed during this thesis work.

%\textbf{Chapter~\ref{Chapter7}}



\item \textbf{Appendix~\ref{AppendixA}} contains a list of BioASQ questions used for the manual evaluation.


\item \textbf{Appendix~\ref{Chapter8}} contains a detailed summary of this thesis in French.

\item \textbf{\hyperref[AppendixB]{\textcolor{black}{Publications of the Author}}} contains publications of the author that are related to this thesis work.

\end{itemize} 