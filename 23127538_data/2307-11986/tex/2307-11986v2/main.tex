%%
%% This is file `sample-manuscript.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `manuscript')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-manuscript.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command.
%%%% Small single column format, used for CIE, CSUR, DTRAP, JACM, JDIQ, JEA, JERIC, JETC, PACMCGIT, TAAS, TACCESS, TACO, TALG, TALLIP (formerly TALIP), TCPS, TDSCI, TEAC, TECS, TELO, THRI, TIIS, TIOT, TISSEC, TIST, TKDD, TMIS, TOCE, TOCHI, TOCL, TOCS, TOCT, TODAES, TODS, TOIS, TOIT, TOMACS, TOMM (formerly TOMCCAP), TOMPECS, TOMS, TOPC, TOPLAS, TOPS, TOS, TOSEM, TOSN, TQC, TRETS, TSAS, TSC, TSLP, TWEB.
% \documentclass[acmsmall]{acmart}

%%%% Large single column format, used for IMWUT, JOCCH, PACMPL, POMACS, TAP, PACMHCI
% \documentclass[acmlarge,screen]{acmart}

%%%% Large double column format, used for TOG
% \documentclass[acmtog, authorversion]{acmart}

%%%% Generic manuscript mode, required for submission
%%%% and peer review
\documentclass[manuscript]{acmart}
% \documentclass[manuscript,screen,review]{acmart}
%% Fonts used in the template cannot be substituted; margin 
%% adjustments are not allowed.
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
% \setcopyright{acmcopyright}
% \copyrightyear{2018}
% \acmYear{2018}
% \acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
\copyrightyear{2023}
\acmYear{2023}
\setcopyright{licensedusgovmixed}\acmConference[KDD '23]{Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}{August 6--10, 2023}{Long Beach, CA, USA}
\acmBooktitle{Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '23), August 6--10, 2023, Long Beach, CA, USA}
\acmPrice{15.00}
\acmDOI{10.1145/3580305.3599819} \acmISBN{979-8-4007-0103-0/23/08}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
% \acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

\def\eg{\textit{e.g.,~}}
\def\ie{\textit{i.e.,~}}
\def\etal{\textit{et al.}~}
\def\Vec#1{{\boldsymbol{#1}}}
\def\Mat#1{{\boldsymbol{#1}}}
\def\S{\mathbf{S}}
\def\s{\mathbf{s}}
\def\y{\mathbf{y}}
\def\L\mathcal{L}
\def\C{\mathbf{C}}
\def\D{\mathbf{D}}
\def\v{\mathbf{v}}
\def\V{\mathbf{V}}
\def\F{\mathbf{F}}
\def\Y{\mathbf{Y}}
\def\X{\mathbf{X}}
\def\x{\mathbf{x}}
\def\z{\mathbf{z}}
\def\l{\mathbf{l}}
\def\T{\mathbf{T}}
\def\p{\mathbf{p}}
\def\Q{\mathbf{Q}}
\def\M{\mathbf{M}}
\def\R{\mathbf{R}}
\def\H{\mathbf{H}}
\def\r{\mathbf{r}}
\def\g{\mathbf{g}}
\def\G{\mathbf{G}}
\def\w{\mathbf{w}}
\def\te{\mathbf{t}}
\def\t{\mathbf{t}}
\def\L{\mathcal{L}}
\def\E{\mathbf{E}}
\def\I{\mathbf{I}}
\def\y{\mathbf{y}}
\def\u{\mathbf{u}}
\def\w{\mathbf{w}}
\def\x{\mathbf{x}}
\def\P{\mathbf{P}}
\def\R{\mathbf{R}}
\def\z{\mathbf{z}}
\def\I{\mathbf{I}}
\def\u{\mathbf{u}}
\def\W{\mathbf{W}}
\def\F{\mathbf{F}} 
\def\o{\mathbf{o}} 
\def\q{\mathbf{q}} 
\def\c{\mathbf{c}} % commented out - was messing up rendering of author's names (Turkish cedilla mark) 
\def\f{\mathbf{f}}
\def\Z{\mathbf{Z}}
\def\Re{\mathbb{R}}
\def\0{\mathbf{0}}
\def\I{\mathbf{I}}
\def\Le{\mathbf{\Lambda}}
\def\G{{G}}
\def\E{E}
\def\A{\mathbf{A}}
\def\B{\mathbf{\Sigma}}
\def\U{\mathbf{U}}
\def\a{\mathbf{a}}
\def\de{\mathbf{d}}
\def\L{\mathcal{L}}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{ bbold }
\usepackage{caption}
\usepackage{subcaption}
\usepackage{balance} 

%%
%% end of the preamble, start of the body of the document source.
\settopmatter{printacmref=true}
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title[Expert Knowledge-Aware Medical Image Difference VQA]{Expert Knowledge-Aware Image Difference Graph Representation Learning for Difference-Aware Medical Visual Question Answering}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Xinyue Hu}
\affiliation{
    \institution{The University of Texas at Arlington}
    \streetaddress{701 S Nedderman Dr}
    \city{Arlington}
    \state{Texas}
    \country{USA}
    \postcode{76019}
}
\email{xxh4034@mavs.uta.edu}

\author{Lin Gu}
\affiliation{
    \institution{RIKEN}
    \city{Tokyo}
    \country{Japan}
}
\affiliation{
    \institution{The University of Tokyo}
    \city{Tokyo}
    \country{Japan}
}
\email{lin.gu@riken.jp}


\author{Qiyuan An}
\affiliation{
    \institution{The University of Texas at Arlington}
    \streetaddress{701 S Nedderman Dr}
    \city{Arlington}
    \state{Texas}
    \country{USA}
    \postcode{76019}
}
\email{qxa5560@mavs.uta.edu}

\author{Mengliang Zhang}
\affiliation{
    \institution{The University of Texas at Arlington}
    \streetaddress{701 S Nedderman Dr}
    \city{Arlington}
    \state{Texas}
    \country{USA}
    \postcode{76019}
}
\email{mxz3935@mavs.uta.edu}

\author{Liangchen Liu}
\affiliation{
    \institution{National Institutes of Health Clinical Center}
    \streetaddress{10 Center Dr}
    \city{Bethesda}
    \state{Maryland}
    \country{USA}
    \postcode{20892}
}
\email{liangchen.liu@nih.gov}

\author{Kazuma Kobayashi}
\affiliation{
    \institution{National Cancer Center Research Institute}
    \city{Tokyo}
    \country{Japan}
}
\email{kazumkob@ncc.go.jp}

\author{Tatsuya Harada}
\affiliation{
    \institution{The University of Tokyo}
    \city{Tokyo}
    \country{Japan}
}
\affiliation{
    \institution{RIKEN}
    \city{Tokyo}
    \country{Japan}
}
\email{harada@mi.t.u-tokyo.ac.jp}

\author{Ronald M. Summers}
\affiliation{
    \institution{National Institutes of Health Clinical Center}
    \streetaddress{10 Center Dr}
    \city{Bethesda}
    \state{Maryland}
    \country{USA}
    \postcode{20892}
}
\email{rsummers@mail.cc.nih.gov}

\author{Yingying Zhu}
\authornote{Corresponding author.}
\affiliation{
    \institution{The University of Texas at Arlington}
    \streetaddress{701 S Nedderman Dr}
    \city{Arlington}
    \state{Texas}
    \country{USA}
    \postcode{76019}
}
\email{yingying.zhu@uta.edu}


%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Xinyue Hu et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
To contribute to automating the medical vision-language model, we propose a novel Chest-Xray Difference Visual Question Answering (VQA) task. Given a pair of main and reference images, this task attempts to answer several questions on both diseases and, more importantly, the differences between them. 
This is consistent with the radiologist's diagnosis practice that compares the current image with the reference before concluding the report. We collect a new dataset, namely  MIMIC-Diff-VQA, including 700,703 QA pairs from 164,324 pairs of main and reference images.  Compared to existing medical VQA datasets, our questions are tailored to the Assessment-Diagnosis-Intervention-Evaluation treatment procedure used by clinical professionals.
Meanwhile, we also propose a novel expert knowledge-aware graph representation learning model to address this task. 
The proposed baseline model leverages expert knowledge such as anatomical structure prior, semantic, and spatial knowledge to construct a multi-relationship graph, representing the image differences between two images for the image difference VQA task. 
The dataset and code can be found at \url{https://github.com/Holipori/MIMIC-Diff-VQA}. 
We believe this work would further push forward the medical vision language model.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%

\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10002951.10003317.10003347.10003348</concept_id>
<concept_desc>Information systems~Question answering</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10010147.10010178.10010224.10010240.10010241</concept_id>
<concept_desc>Computing methodologies~Image representations</concept_desc>
<concept_significance>300</concept_significance>
</concept>
<concept>
<concept_id>10010405.10010444.10010087.10010096</concept_id>
<concept_desc>Applied computing~Imaging</concept_desc>
<concept_significance>300</concept_significance>
</concept>
<concept>
<concept_id>10011007.10011006.10011050.10011058</concept_id>
<concept_desc>Software and its engineering~Visual languages</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Information systems~Question answering}
\ccsdesc[300]{Computing methodologies~Image representations}
\ccsdesc[300]{Applied computing~Imaging}
\ccsdesc[300]{Software and its engineering~Visual languages}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{visual question answering, medical imaging, datasets}

%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.
% \begin{teaserfigure}
%   % Figure removed
%   \caption{Seattle Mariners at Spring Training, 2010.}
%   \Description{Enjoying the baseball game from the third-base
%   seats. Ichiro Suzuki preparing to bat.}
%   \label{fig:teaser}
% \end{teaserfigure}

\received{20 February 2007}
\received[revised]{12 March 2009}
\received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
\label{intro}


%The medical informatics community
% Several recent works focus on 


% Extracting text-mined labels from clinical notes to  train deep-learning models for medical image analysis


% with several datasets:
% %Chest X-ray is one of the most commonly accessible examinations for screening and diagnosis, leading to a tremendous number of radiology images associated with free-text and reports accumulated in hospitals and released online including
%  MIMIC \cite{johnson2019mimic}, NIH14 \cite{wang2017chestx} and Chexpert \cite{irvin2019chexpert}.

% Figure environment removed

 The medical informatics community has been working on feeding the data-hungry deep learning algorithms by fully exploiting hospital databases with invaluable loosely labeled imaging data. Among diverse attempts, Chest X-ray datasets such as  MIMIC \cite{johnson2019mimic}, NIH14 \cite{wang2017chestx} and Chexpert \cite{irvin2019chexpert} have received particular attention.  During this arduous journey on vision-language (VL) modality, the community either mines per-image common disease label (Fig.\ref{fig:fig1}. (b)) through Natural Language Processing (NLP) or endeavors on report generation (Fig.\ref{fig:fig1}. (c) generated from \cite{emnlp2021report}) or even answer certain pre-defined questions (Fig.\ref{fig:fig1}. (d)).
Despite significant progress achieved on these tasks, the heterogeneity, systemic biases, and subjective nature of the report still pose many technical challenges. For example, the automatically mined labels from reports in Fig.\ref{fig:fig1}. (b) is problematic because the rule-based approach that was not carefully designed did not process all uncertainties and negations well \cite{johnson2019mimic-jpg}. 
Training an automatic radiology report generation system to match the report appears to avoid the inevitable bias in the standard NLP-mined thoracic pathology labels. 
However, radiologists tend to write more obvious impressions with abstract logic. For example, as shown in Fig.\ref{fig:fig1}. (a), a radiology report excludes many diseases (either commonly diagnosed or intended by the physicians) using negation expressions, e.g., no, free of, without, \textit{etc.} However, the artificial report generator could hardly guess which disease is excluded by radiologists.  
% Figure environment removed
Instead of thoroughly generating all of the descriptions, VQA is more plausible as it only answers the specific question. As shown in Fig. \ref{fig:fig1}, the question could be raised strictly for "is there any pneumothorax in the image?" in the report while the answer is no doubt "No". However, the questions in the existing VQA dataset ImageCLEF-VQA-Med \cite{ImageCLEF-VQA-Med2021} concentrate on very few general ones, such as "is there something wrong in the image? what is the primary abnormality in this image?", lacking the specificity for the heterogeneity and subjective texture. It not only degrades VQA into classification but, more unexpectedly, provides little helpful information for clinics. While VQA-RAD \cite{VQA-RAD} has more heterogeneous questions covering 11 question types, its 315 images dataset is relatively too small.



%\textcolor{black}{Kobayashi San please briefly add some lines here}.




To bridge the aforementioned gap in the visual language model, we propose a novel medical image difference VQA task more consistent with radiologists' practice.
% When radiologists make a diagnosis, they usually compare the main one with a reference image to find their differences.
% Therefore, understanding the clinical meaning of what and where has changed on two images is an essential step in the arduous medical vision language journey.  
{
\color{black}When radiologists make diagnoses, they compare current and previous images of the same patients to check the disease's progress. Actual clinical practice follows a patient treatment process (assessment - diagnosis - intervention - evaluation) as shown in Fig.~\ref{fig:motivation}.
A baseline medical image is used as an assessment tool to diagnose a clinical problem, usually followed by therapeutic intervention. Then, another follow-up medical image is retaken to evaluate the effectiveness of the intervention in comparison with the past baseline.  In this framework, every medical image has its purpose of clarifying the doctor's clinical hypothesis depending on the unique clinical course (\textit{e.g.}, whether the pneumothorax is mitigated after therapeutic intervention). 
However, existing methods can not provide a straightforward answer to the clinical hypothesis since they do not compare the past and present images. 
Therefore, we present a chest X-ray image difference VQA dataset, MIMIC-Diff-VQA, to fulfill the need of the medical image difference task. Moreover, we propose a system that answers doctors' questions by comparing the current medical image (main) to a past visit medical image (reference). This allows us to build a diagnostic support system that realizes the inherently interactive nature of radiology reports in clinical practice.  
}

% Therefore, we present a chest X-ray image difference VQA dataset, MIMIC-Diff-VQA.  
MIMIC-Diff-VQA contains pairs of "main"(present) and "reference"(past) images from the same patient's radiology images at different times from MIMIC-CXR\cite{johnson2019mimic} (a large-scale public database of chest radiographs with 227,835 studies, each with a unique report and images).
The question and answer pairs are extracted from the MIMIC-CXR report for "main"  and "reference" images using an Extract-Check-Fix cycle. 
% Similar to \cite{VQA-Med,VQA-RAD,pathvqa}, we first collect sets of abnormality names and attributes. Then we extract the abnormality in the images and their corresponding attributes using regular expressions. Finally, we compare the abnormalities contained in the two images and ask questions based on the collected information. 
There are seven types of questions included in our dataset: 1. abnormality, 2. presence, 3. view, 4. location, 5. type, 6. level, and 7. difference. The MIMIC-Diff-VQA dataset comprises 700,703 QA pairs extracted from 164,324 image pairs. 
% \textcolor{black}{which has been filtered to only contains the images from the PA view and AP view, and the corresponding patient has more than one image.}
Particularly, \textit{difference} questions are pairs of inquiries that pertain to the clinical progress and changes in the "main" image as compared to the "reference" image, as shown in Fig.~\ref{fig:fig1}(e).

%Comparing the differences between two medical images are very challenging task due to the body pose, view variances and deformations as shown in Fig.~\ref{fig:fig1}. %Current state-of-art image difference model did not consider 

The current mainstream state-of-the-art image difference method only applies to synthetic images with small view variations,\cite{jhamtani2018learning,park2019robust} as shown in Fig.~\ref{fig:achitechture}.
However, real medical image difference comparing is a very challenging task. Even the images from the same patient show large variances in the orientation, scale, range, view, and nonrigid deformation, which are often more significant than the subtle differences caused by diseases as shown in Fig.~\ref{fig:achitechture}. 
Since the radiologists examine the anatomical structure to find the progression of diseases, similarly, we propose an expert knowledge-aware image difference graph representation learning model as shown in Fig.~\ref{fig:achitechture}. We extract the features from different anatomical structures (for example, left lower lung, and right upper lung) as nodes in the graph. 
%we proposed an anatomical structure-aware image difference graph model and compare the image differences in each normalized anatomical region (for example, left lower lung). Each anatomical structure is defined as a node in the graph and we use the pre-trained anatomical and disease region detection model to extract the feature of each node. 

\textcolor{black}{
Moreover, we construct three different relationships in the graph to encode expert knowledge: 1) Spatial relationship based on the spatial distance between different anatomical regions.
2) Semantic relationship based on the disease and anatomical structure relationship from knowledge graph~\cite{zhang2020radiology}.
3) Implicit relationship to model potential implicit relationship beside 1) and 2). 
The image-difference graph feature representation is constructed by simply subtracting the main image graph feature and the reference image graph feature. This graph difference feature is fed into LSTM networks with attention modules for answer generation\cite{toutanova2003feature}. 
}

% Moreover, we construct three different relationships in the graph to encode expert knowledge: 1) Spatial relationship based on the spatial distance between different anatomical regions, such as "left lower lung", "right costophrenic angle", etc. We design this graph based on the fact that radiologists prefer to determine the abnormalities based on particular anatomical structures. For example, "Minimal blunting of the left costophrenic angle also suggests a tiny left pleural effusion.";
% 2) Semantic relationship based on the disease and anatomical structure relationship from knowledge graph~\cite{zhang2020radiology}.
% We construct this graph because diseases from the same or nearby regions could affect each other's existence. For example, "the effusions remain moderate and still cause substantial bilateral areas of basilar atelectasis."; 
% 3) Implicit relationship to model potential implicit relationship beside 1) and 2). 
% The graph feature representation for each image is learned as a weighted summation of the graph feature from these three different relationships. The image-difference graph feature representation is constructed by simply subtracting the main image graph feature and the reference image graph feature. This graph difference feature is fed into LSTM networks with attention modules for answer generation\cite{toutanova2003feature}. 

\textbf{ Our contributions are summarized as:}
%\textcolor{black}{ADD how the post processing part for feature extraction here and explain why this post featue processing are important for this, and citep the ACL paper here.: HXY }
%We constructed two graphs on the main and reference images and subtracting
%Next, a difference feature is generated and is fed into the feature attention module along with the graph features to generate the final features of the images. 
%Finally, the final answer is generated by the answer generator, which consists of attention modules and LSTM networks that takes into account Part-Of-Speech~\cite{toutanova2003feature} information.
%\textcolor{black}{
%In the general image difference\cite{tu2021semantic}, their before and after image are overall similar and easy to localize the difference by directly apply the feature maps.
%However, even if two chest X-ray images are from the same patient, the images can be very different due to the different positions at the time of capture.
%Therefore, we construct each image feature with combined anatomical features and their corresponding disease features.
%The anatomical feature is constructed in a specific order of anatomical locations.
%By doing so, we ensure that the model knows what anatomical structures each feature represents.
%}




1)We propose the medical imaging difference visual question answering problem and construct the first large-scale medical image difference visual question answering dataset, MIMIC-Diff-VQA. This dataset comprises  \textcolor{black}{164,324} image pairs,  containing \textcolor{black}{700,703} question-answer pairs related to various attributes, including abnormality, presence, location, level, type, view, and difference. 

2) We propose an anatomical structure-aware image-difference model to extract the image-difference feature relevant to disease progression and interventions. We extracted features from anatomical structures and compared the changes in each structure to reduce the image differences caused by body pose, view, and nonrigid deformations of organs.  

3)  We develop a multi-relationship image-difference graph feature representation learning method to leverage the spatial relationship and semantic relationship (extracted from expert knowledge graph) to compute image-difference graph feature representation, generate answers and interpret how the answer is generated on different image regions.

%both disease findings and anatomical structures are defined as nodes. We also establish the spatial relation among these nodes.




\section{MIMIC-Diff-VQA dataset.}
We introduce our new MIMIC-Diff-VQA dataset for the medical imaging difference question-answering problem. 
{
\color{black}
The MIMIC-Diff-VQA dataset is constructed following an Extract-Check-Fix cycle to minimize errors.
% Please refer to Appendix.~\ref{sec:dataset_construct} for the details on how the dataset is constructed.
}
In MIMIC-Diff-VQA, each entry contains two different chest X-ray images from the same patient with a question-answer pair.
Our question design is extended from VQA-RAD, but with an additional "difference" question type.
Ultimately, the questions can be divided into seven types: 1) abnormality, 2) presence, 3) view, 4) location, 5) type, 6) level, and 7) difference. 
Tab.~\ref{tab:question} shows examples of the different question types.

\begin{table*}[h]
  \caption{Selected examples of the different question types. See the Appendix for the full list.}
  \label{tab:question}
  \centering
    \begin{tabular}{ll}
    \toprule
    % \hline
    Question type & Example                                             \\
    \midrule
    % \hline \\
    Abnormality   & what abnormality is seen in the left lung?          \\ 
    Presence      & is there evidence of atelectasis in this image?     \\ 
    View          & which view is this image taken?                     \\ 
    Location      & where in the image is the pleural effusion located? \\ 
    Type          & what type is the opacity?                           \\
    Level         & what level is the cardiomegaly?                     \\ 
    Difference    & what has changed compared to the reference image?   \\
    \bottomrule
    \end{tabular}
\end{table*}

The image pairs are selected from the MIMIC-CXR~\cite{johnson2019mimic} dataset, and each image in an image pair is from the same patient.
A total of \textcolor{black}{164,324} image pairs are selected from MIMIC-CXR, on which \textcolor{black}{700,703} questions are constructed.
We also balance the "yes" and "no" answers to avoid possible bias.
The statistics regarding each question type can be seen in Fig.~\ref{fig:statistics}.
The ratio between the training, validation and testing set is 8:1:1.







\subsection{MIMIC-Diff-VQA dataset construction}
\label{sec:dataset_construct}

% Figure environment removed



% First, we exclude the lateral view and select only the common PA or AP views for comparison.
\textcolor{black}{To ensure the availability of a second image for differential comparison, we excluded patients with only one radiology visit before constructing our dataset. The overall process of dataset construction involves three steps: collecting keywords, building the Intermediate KeyInfo dataset, and generating questions and answers.
}

\subsubsection{\textcolor{black}{Collecting keywords}}
\textcolor{black}{We follow an iterative approach to collect abnormality names and sets of important attributes, such as location, level, and type, from the  MIMIC-CXR dataset. We utilize ScispaCy~\cite{neumann2019scispacy}, a SpaCy model for biomedical text processing, to extract entities from random reports. Subsequently, we manually review all the extracted entities to identify common, frequently occurring keywords that \textcolor{black}{align with radiologists' interests} and add these to our lists of abnormality names and attribute words. We also record different variants of the same abnormality during this process.
The full lists of the selected abnormality names and the attribute words are available in Appendix.}

% We collect a set of abnormality names, as well as the sets of important attributes including location, level, and type, from the MIMIC-CXR dataset.
% The lists of abnormality names and the attribute words are collected by iteratively extracting entities from random reports using ScispaCy~\cite{neumann2019scispacy}, which is a SpaCy model for biomedical text processing. Then we manually go through all the extracted entities that haven't been added to the collection list and select the common keywords that appear frequently \textcolor{black}{and align with radiologists' interests}. Then we add these selected keywords to the collection lists of abnormality names and attributes. 
% During this process, different variants that represent the same abnormality are also recorded.
\subsubsection{\textcolor{black}{Intermediate KeyInfo dataset}}
The previous rule-based label extraction method was limited to a small set of disease-related labels, lacked important information such as complicated disease pathologies, levels, and location, and was prone to errors due to negations. To address these issues, we followed an Extract-Check-Fix cycle to customize the rule set for MIMIC-CXR, ensuring the quality of our dataset through extensive manual verification.

\textcolor{black}{
For each patient visit, we used regular expression rules to extract the abnormality names and their variants. Then, we detected attribute words near the identified abnormalities using these rules. Additionally, by going through the extracted entities, we manually selected the keywords/expressions that indicated negation information to locate the negative findings, i.e. cases where the abnormality did not exist. 
}

% For each study, we use regular expressions to extract the abnormality names as well as their variants then detect attribute words near these detected abnormalities. 
% (Here, "study" represents a single patient visit. Please refer to Section~\ref{datasets} for more context.)
% Meanwhile, by going through the extracted entities, we manually select the keywords/expressions that indicate negation information to localize the negative findings, i.e. cases where the abnormality does not exist.

\textcolor{black}{Next, to ensure the accuracy and completeness of the extracted information, we conducted both manual and automated checks using tools such as Part-of-Speech, ScispaCy entity detection, and MIMIC-CXR-JPG~\cite{johnson2019mimic-jpg} labels as references. These were used to identify any missing or potentially incorrect information that may have been extracted and refined the rules accordingly. We repeated the Extract-Check-Fix cycle until minimal errors were found.}


\textcolor{black}{
As a result, we have created the Key-Info dataset, consisting of individual study details. 
As shown in Fig.~\ref{fig:data_struct}, for each study, the Key-Info dataset includes information on all positive findings, their attributes, and negative findings.
The "posterior location" attribute represents the location information that appears after the abnormality keyword in a sentence.
}

% Figure environment removed



\subsubsection{Study pairing and question generation}
Once the intermediate Key-Info database is constructed, we can generate study pair questions accordingly.  The examples of each question type are shown in Tab.~\ref{tab:question}. 
Each image pair contains the main image and a reference image, which are extracted from different studies of the same patient. \textcolor{black}{The reference and main visits are chosen strictly based on the earlier visit as the "reference" and the later visit as the "main" image.} Among all the question types, the first six question types are for the main image only, and the \textit{difference} question is for both images.

\subsection{Dataset Validation}
{\color{black}
To further verify the reliability of our constructed dataset, 3 human verifiers were assigned 1700 random sampled question-answer pairs along with the reports and evaluated each sample by annotating "correct" or "incorrect". Finally, the correctness rate of the evaluation achieved 97.33\%, which is acceptable for training neural networks.
Tab.~\ref{tab:evaluation} shows the evaluation results of each verifier.}
It proves that our approach of constructing a dataset in an Extract-Check-Fix cycle works well in ensuring that the constructed dataset has minimum mistakes.


\begin{table}[h]
\color{black}
\caption{\color{black}Evaluation results by human verifiers}
\label{tab:evaluation}
\begin{center}
\begin{tabular}{llll}
\toprule
\multicolumn{1}{c}{\bf Verifier} & \multicolumn{1}{c}{\bf \# of examples} & \multicolumn{1}{c}{\bf \# of correctness} & \multicolumn{1}{c}{\bf Correctness rate} \\
\midrule 
Verifier 1                & 500                              & 475                                 & 95\%                   \\
Verifier 2                & 1000                             & 989                                 & 98.9\%                  \\
Verifier 3                & 200                              & 193                                 & 96.5\%                   \\ 
Total                     & 1700                             & 1657                                 & 97.4\%                  \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\section{Method}
\label{sec:method}
\subsection{Problem Statement}
Given an image pair $(\I_m, \I_r)$, consisting of the main image $\I_m$ and the reference image $\I_r$, and  a question $\q$, our goal is to obtain the answer $\a$ of the question $\q$ from image pair. In our design, the main and reference images are from the same patient. 
% We follow the diagnostic process of radiologists to compare the changes between the main and the reference images from the same patient and ask different types of questions to accurately localize the changes in the patient's two images and find the abnormalities.





\subsection{Anatomical Structure-Aware Graph Construction and Feature Learning }
\textcolor{black}{Within the language generation and vision research domain, the most related works to the medical image difference VQA task is image difference captioning \cite{qiu2021describing,oluwasanmi2019fully,yao2022image}, which is designed to identify object movements and changes within a spatial context such as a static or complex background. 
As shown in the left Fig.\ref{fig:achitechture}, the object changes and movements in general image difference captioning are relatively large or significant compared to the background, making the problem easier to solve. These works usually assume a stable background with simple changes in the structure, position, and texture of foreground objects, without significant scaling. 
}
% Figure environment removed
% As shown in the left Fig.\ref{fig:achitechture}, previous work on image difference question answers in the general image domain. They create paired synthetic images with identical backgrounds and only move or remove the simple objects from the background. 
% The feature of image difference was extracted by simply comparing the feature on the exact image coordinates.

\textcolor{black}{
However, the medical image difference is distinct from the general image difference. 
Changes caused by diseases are generally subtle, and the image position, pose, and scale can vary significantly even for the same patient due to the pose and nonrigid deformation. 
}
 \textcolor{black}{As a result, general image difference methods can have difficulty adapting to the medical image difference task.}
To better capture the subtle disease changes and eliminate the pose, orientation, and scale changes, we propose an anatomical structure-aware image difference graph learning solution. Specifically, we represent each anatomical structure as a node and then assess the image changes within each structure in a similar manner to that of radiologists.

\subsubsection{Anatomical Structure,  Disease Region Detection, and Question Encoding.}
To begin, we use a pre-trained Faster-RCNN on the Chest ImaGenome dataset \cite{ren2015faster,wu2021chest, goldberger2000physiobank} to extract the anatomical bounding boxes and their corresponding features $\f_{a}$ from the input images. Subsequently, we train a Faster-RCNN on the VinDr-CXR dataset \cite{pham2021chest} to detect diseases. Rather than directly detecting diseases on the given input images, we extract the features $\f_{d}$ from the same anatomical regions by utilizing the previously extracted anatomical bounding boxes. Following previous work \cite{regat,norcliffe2018learning}, we tokenized each question and answer and embedded them with Glove~(\cite{pennington2014glove}) embeddings. We then used a bidirectional RNN with GRU~\cite{cho2014learning} and self-attention to generate the question embedding $\q$.

% We first extract the anatomical bounding boxes and their features $\f_{a}$ from the input images using pre-trained Faster-RCNN on the MIMIC dataset \cite{ren2015faster,karargyris2020eye}.
% %The anatomical features are extracted in a specific order.
% Then, we train a Faster-RCNN on the VinDr dataset \cite{pham2021chest} to detect the diseases. 
% Instead of directly detecting diseases on the given input images, we extract the features $\f_d$ from the same anatomical regions using the extracted anatomical bounding boxes.
% The questions and answers are processed the same way as ~\cite{regat,norcliffe2018learning}. 
% % After detecting the anatomical and disease regions, we use ROI pooling to extract feature representation on the trained model respectively. 
% Each word is tokenized and embedded with Glove~(\cite{pennington2014glove}) embeddings.
% Then we use a bidirectional RNN with GRU~\cite{cho2014learning} and self-attention to generate the question embedding $\q$.


%We calculated the IOU between the anatomical structure and disease bounding box to determine the location of the diseases. 

%The order of the disease features are assigned according to the anatomical locations.
%By doing so, we ensure each feature from the main image and the reference image corresponds to each other.


%Thereafter, we feed question embedding, the anatomical features and disease features of both main image and reference image to the Multi-Modal Graph Module.


%\textbf{2. Disease Structure Detection and Feature Extraction}
% Figure environment removed

% % Figure environment removed


\subsection{Expert Knowledge-Aware Multi-Relationship Graph Module}

% Figure environment removed

After extracting the disease and anatomical structure, we construct an expert knowledge-aware image representation graph for the main and reference image. 
The multi-relationship graph is defined as $
\mathcal{G}=\{\V, \mathcal{E}_{sp}, \mathcal{E}_{se}, \mathcal{E}_{imp}\}$, {\color{black}where $\mathcal{E}_{sp}, \mathcal{E}_{se},$ and $ \mathcal{E}_{imp}$ represent the edge sets of spatial graph, semantic graph and implicit graph}, each vertex  $\mathbf{v}_i \in \V, i = 1, \cdots , 2N$ can be either anatomical node~$\mathbf{v}_k = [f_{a,k}\|\q] \in \mathbb{R}^{d_f + d_q}, f_{a,k} \in \f_a, \mathrm{for} \ k = 1, \dots, N$, or disease node~$\mathbf{v}_k=[f_{d,k}\|\q] \in \mathbb{R}^{d_f + d_q}, f_{d,k} \in \f_d,  \mathrm{for} \ k = 1, \dots, N$, representing anatomical structures or disease regions, respectively. 
Both types of nodes are embedded with a question feature as shown in Fig.~\ref{fig:achitechture}. 
$d_f$ is the dimension of the anatomical and disease features.
$d_q$ is the dimension of the question embedding.
$N$ represents the number of anatomical structures of one image. 
Since each disease feature is extracted from the same corresponding anatomical region,
the total number of the vertex is $2N$.

%\textbf{Graph Construction}

We construct three types of relationships in the graph for each image:
1) \textbf{spatial relationship}: 
We construct spatial relationships according to the radiologist's practice of identifying abnormalities based on specific anatomical structures. For example, an actual radiology report can state that "the effusions remain moderate and still
cause substantial bilateral areas of basilar atelectasis" 
% as shown in Fig.~\ref{fig:anno1}; "The central part of the lungs appears clear, suggesting no evidence of pulmonary edema." as shown in Fig.~\ref{fig:anno2}.
In our MIMIC-Diff-VQA dataset, we design questions to assess spatial relationships, such as "Where in the image is the pleural effusion located?" (see Table~\ref{tab:question}). 
Following previous work~\cite{yao2018exploring}, we include 11 types of spatial relations between detected bounding boxes, such as "left lower lung", "right costophrenic angle", etc. The 11 spatial relations includes \texttt{inside} (class1), \texttt{cover} (class2), \texttt{overlap} (class3), and 8 directional classes. Each class corresponds to a 45-degree of direction.
We define the edge between node i and the node j as $a_{ij} = c$, where c is the class of the relationship, $c = 1, 2, \cdots, K$, K is the number of spatial relationship classes, which equals to 11.
When $d_{ij} > t$, we set $\mathbf{a}_{ij}=0$, where $d_{ij}$ is the Euclidean distance between the center points of the bounding boxes corresponding to the node $i$ and node $j$, $t$ is the threshold.
The threshold $t$ is defined as $(l_x + l_y)/3$ by reasoning and imitating the data given by~\cite{regat}.
% When two ROIs are too far, we set their adjacency $\mathbf{a}_{ij}=0 $.  The congested blood can back up into the veins of the lungs.

2) \textbf{Semantic relationship}: 
To incorporate expert knowledge into our approach, we use two knowledge graphs: an anatomical knowledge graph modified from \cite{zhang2020radiology} and a label occurrence knowledge graph built by ourselves. Please refer to the Appendix for detailed information about these knowledge graphs.
% \textcolor{black}{In order to encode expert knowledge,} The semantic relationship is based on two knowledge graphs, including an anatomical knowledge graph modified from \cite{zhang2020radiology}, as shown in Fig.~\ref{fig:anaKG} \textcolor{black}{(the newly added nodes have been marked as red)}, and a label occurrence knowledge graph built by ourselves, as shown in Fig.~\ref{fig:coKG}. 
If two labels are linked by an edge in the knowledge graph, we connect the corresponding nodes in our semantic relationship graph. The knowledge graphs represent abstracted expert knowledge and relationships between diseases, which are essential for disease diagnosis since multiple diseases can interrelate during the progression of a particular disease. For example, Figure~\ref{fig:progression} shows the progression from cardiomegaly to edema and pleural effusion. Cardiomegaly, which refers to an enlarged heart, can result from heart dysfunction that causes blood congestion in the heart, eventually leading to its enlargement. The congested blood is pumped into the lungs' veins, increasing the pressure in the vessels and pushing fluid out of the lungs and into the pleural spaces, indicating the initial sign of pulmonary edema. At the same time, fluid accumulates between the layers of the pleura outside the lungs, resulting in pleural effusion, which can also cause compression atelectasis. If pulmonary edema progresses, widespread opacification will appear in the lungs, as stated in actual diagnostic reports such as "the effusions remain moderate and still cause substantial bilateral areas of basilar atelectasis" and "Bilateral basilar opacity can be seen, suggesting the presence of the bilateral or right-sided basilar atelectasis" (Figure~\ref{fig:anno2}).

% If there is an edge linking two labels in the Knowledge graph, we connect the nodes having these two labels in our semantic relationship graph.
% The knowledge graph represents abstracted expert knowledge and the relationships between diseases. These relationships play a crucial role in disease diagnosis.
% Multiple diseases could be interrelated with each other during the progress of a specific disease.
% For example, in Fig.~\ref{fig:progression}, a progression from cardiomegaly to edema and pleural effusion is shown.
% Cardiomegaly, which refers to an enlarged heart, can start with a heart dysfunction that causes congestion of blood in the heart, eventually leading to the heart's enlargement. The congested blood would be pumped up into the veins of the lungs.
% As the pressure of the vessels in the lungs increases, fluid is pushed out of the lungs and enters pleural spaces, causing the initial sign of pulmonary edema.
% Meanwhile, the fluid starts to build up between the layers of the pleura outside the lungs, \textit{i.e.} pleural effusion.
% Pleural effusion can also cause compression atelectasis.
% If pulmonary edema continues to progress, widespread opacification in the lung will appear.
% We can verify it in actual diagnostic reports. For example, "the effusions remain moderate and still cause substantial bilateral areas of basilar atelectasis"; "Bilateral basilar opacity can be seen, suggesting the presence of the bilateral or right-sided basilar atelectasis" as shown in Fig.~\ref{fig:anno2}.


3) \textbf{Implicit relationship}: \textcolor{black}{a fully connected graph is applied to find the implicit relationships that are not defined by the other two graphs (spatial and semantic graphs). This graph serves as a complement to the other two as it covers all possible relationships, although it is not specific to any one particular relationship.
% $\mathcal{E}_{sp}$, $\mathcal{E}_{se}$ and $\mathcal{E}_{imp}$
% are the set of the spatial, semantic, and implicit edges respectively and $N$ is the number of vertexes.  
Among these three types of relationships, spatial and semantic relationships can be categorized as explicit relationships. The implicit graph itself is categorized as the implicit relationship.}

\subsection{Relation-Aware Graph Attention Network}
\textcolor{black}{
we construct the multi-relationship graph for both main and reference images and use the relation-aware graph attention network (ReGAT) proposed by~\cite{regat} to learn the graph representation for each image. We then embed the image into the final latent feature, which is input into the answer generation module to generate the final answers. Please refer to Appendix for details of the calculation. 
}



\section{Experiments}
\label{exp}
\subsection{Datasets}
\label{datasets}
\textbf{MIMIC-CXR.}
The MIMIC-CXR dataset is a large publicly available dataset of chest radiographs with radiology reports, containing 377,110 images corresponding to 227,835 radiograph studies from 65,379 patients \cite{johnson2019mimic}. One patient may have multiple studies, each consisting of a radiology report and one or more images. Two primary sections of interest in reports are findings: a natural language description of the important aspects of the image, and an impression: a summary of the most immediately relevant findings. 
% Image labels describing findings are derived from either the impression section and the findings section, or the final section of the report \cite{johnson2019mimic-jpg}. Labels are determined by two methods, NegBio \cite{peng2018negbio} and CheXpert \cite{irvin2019chexpert}.
Our MIMIC-Diff-VQA is constructed based on the MIMIC-CXR dataset.

\textbf{Chest ImaGenome.} MIMIC-CXR has been added more annotations by \cite{wu2021chest} including the anatomical structure bounding boxes. This new dataset is named Chest ImaGenome Dataset.
We trained the Faster-RCNN to detect the anatomical structures on their gold standard dataset, which contains 26 anatomical structures.

\textbf{VinDr-CXR.}
The VinDr-CXR dataset consists of 18,000 images manually annotated by 17 experienced radiologists \cite{nguyen2020vindr}. Its images have 22 local labels of boxes surrounding abnormalities and six global labels of suspected diseases. We used it to train the pre-trained disease detection model. 



\subsection{Baselines}
\textcolor{black}{It is important to compare multiple baselines. However, we would like to emphasize that the image difference question and answer task is a novel problem even in the general computer vision domain. To date, no prior research has specifically addressed the "image difference question answering" problem. Only a few studies have focused on the general image difference caption task, such as MMCFormers~\cite{qiu2021describing} and IDCPCL~\cite{yao2022image}.  Therefore, our work serves as the first step in this new direction and provides a valuable contribution to the research community.}
\textcolor{black}{We chose baseline models from traditional medical VQA tasks and image difference captioning tasks to address both non-"Difference" and "Difference" queries. Below are the baseline models we have selected:}

% Since we are the first to propose this medical imaging difference VQA problem, we have to choose baseline models from the traditional medical VQA task and image difference captioning task, respectively, \textcolor{black}{to address both non-"Difference" and "Difference" questions. Below are the baseline models we have chosen:}

1.\textit{MMQ} is one of the recently proposed methods to perform the traditional medical VQA task with excellent results. MMQ adopts Model Agnostic Meta-Learning (MAML)~\cite{finn2017model} to handle the problem of the small size of the medical dataset. It also relieves the problem of the difference in visual concepts between general and medical images when finetuning. 

2.\textit{MCCFormers} is proposed to handle the image difference captioning task~\cite{qiu2021describing}. It achieved state-of-the-art performance on the CLEVR-Change dataset~\cite{park2019robust}, a famous image difference captioning dataset. MCCFormers used transformers to capture the region relationships among intra- and inter-image pairs.

{
\color{black}
3.\textit{Image Difference Captioning with Pre-training and Contrastive Learning (IDCPCL)~\cite{yao2022image}} is the state-of-the-art method performed on the general image difference captioning task. They use the pretraining technique to build the bridge between vision and language, allowing them to align large visual variance between image pairs and greatly improve the performance on the challenging image difference dataset, Birds-to-Words~\cite{forbes2019neural}.
}

\subsection{Results and Discussion.}
\label{result}
We implemented the experiments on the PyTorch platform.
We used an Adam optimizer with a learning rate of 0.0001 to train our model for 30,000 iterations at a batch size of 64.
The experiments are conducted on two GeForce RTX 3090 cards with 3 hours and 49 minutes of training time.
The bounding box feature dimension is 1024.
Each word is represented by a 600-dimensional feature vector including a 300-dimensional Glove~\cite{pennington2014glove} embedding.
We used BLEU \cite{papineni2002bleu}, \textcolor{black}{METEOR \cite{lavie2007meteor}, ROUGE\_L \cite{lin2004rouge}, CIDEr \cite{vedantam2015cider}}, which are popular metrics for evaluating the generated text, as the metric in our experiments.
% We used five popular metrics for evaluating the generated text, including BLEU \cite{papineni2002bleu}, METEOR \cite{lavie2007meteor}, ROUGE\_L \cite{lin2004rouge}, CIDEr \cite{vedantam2015cider}, and SPICE \cite{anderson2016spice}.
We obtain the results using Microsoft COCO Caption Evaluation~\cite{chen2015microsoft}.
For the comparison with MMQ, we use accuracy as the metric.


\subsubsection{Ablation Study.}
In Tab.~\ref{tab:metric}, we present the quantitative results of our ablation studies on the MIMIC-Diff-VQA dataset using different graph settings. Our method was tested with implicit graph-only, spatial graph-only, semantic graph-only, and the full model incorporating all three graphs.
As we can see, our full model achieves the best performance across most metrics compared to other graph settings.
Furthermore, in the Appendix, we illustrated the regions of interest (ROIs) of our model using different graphs to demonstrate the improved interpretability achieved by incorporating the spatial and semantic graphs. This is particularly useful in analyzing the location and relationship between abnormalities, providing crucial insights into the anatomical structure from a medical perspective.

\begin{table}[h]
\caption{Quantitative results of our model with different graph settings performed on the MIMIC-Diff-VQA dataset}
\label{tab:metric}
\begin{center}
\begin{tabular}{lllll}
\toprule
\bf Metrics  & \bf Implicit & \bf Spatial & \bf Semantic & \bf Full \\
% \hline\\
\midrule
Bleu-1  & \bf  0.626 & 0.617 & 0.623 & 0.624  \\
Bleu-2  & 0.540 & 0.532 & 0.540 & \bf 0.541  \\
Bleu-3  & 0.475 & 0.468 & \bf 0.477 & \bf 0.477  \\
Bleu-4  & 0.418 &  0.413 & 0.421 & \bf 0.422  \\
METEOR  & 0.333 & 0.337 &\bf  0.340 & 0.337   \\
ROUGE-L & 0.649 &  0.647 & 0.644 &\bf  0.645  \\
CIDEr   & \bf 1.911 &  1.896 & 1.898 & 1.893  \\
% SPICE   & \bf 0.245 & 0.240 & 0.240 & 0.242  \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

% \begin{table}[]
% \centering
% \caption{hhh}
% \begin{tabular}{llllllllll}
% \toprule
% Metric & SPICE & Bleu\_1 & Bleu\_2 & Bleu\_3 & Bleu\_4 & METEOR & ROUGE\_L & CIDEr & SPICE \\
% \midrule
% result & 0.213 & 0.573   & 0.495   & 0.438   & 0.388   & 0.31   & 0.656    & 1.801 & 0.213
% \bottomrule
% \end{tabular}
% \end{table}

\subsubsection{Comparison of accuracy.}
% We also compare our model with the current medical VQA model MMQ on our MIMIC-Diff-VQA dataset.
Due to the nature of MMQ being a classification model, MMQ cannot perform on our \textit{difference} question type because of the diversity of answers.
Also, given that the baseline model cannot take in two images simultaneously, we exclude the \textit{difference} type question from this comparison.
Therefore, we compare our method with MMQ only on the other six types of questions, including \textit{abnormality, presence, view, location, type}, and \textit{level}. These six types of questions have a limited number of answers.
% Since the baseline MMQ is a classification-based model, they used accuracy as their metric.
To compare with them, we use accuracy as the metric for comparison.
Please note that our method is still a text-generation model.
We count the predicted answer as a True answer only when the prediction is fully matched with the ground truth answer.

The comparison results are shown in Tab.~\ref{tab:acc}.
We have refined the comparison into open-ended question results and closed-ended question (with only 'yes' or 'no' answers) results.
It is clear that the current VQA model has difficulty handling our dataset because of the lack of focus on the key regions and the ability to find the relationships between anatomical structures and diseases.
Also, even after filtering out the \textit{difference} questions, there are still \textcolor{black}{9,252} possible answers in total. It is difficult for a classification model to localize the optimal answer from such a huge amount of candidates.

\begin{table}[h]
\caption{Accuracy comparison between our method and MMQ on non-"Difference" questions of the MIMIC-Diff-VQA dataset.}
\label{tab:acc}
\centering
\begin{tabular}{llll}
\toprule
 Question  & Open  & Closed & Total \\
\midrule
MMQ  & 40.5  & 74.2 & 54.7 \\
% MMQ  & 40.5  & 74.2 & 54.7 \\
% Ours & 32.43 & 59.31  & 79.24\\
\bf Ours & 36.6 & 84.9  & 60.2\\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Evidence and faithfulness}
\textcolor{black}{
In terms of the evidence aspect, our model is designed to enhance the diagnostic process for doctors. Firstly, it highlights the regions of an image indicative of diseases, allowing doctors to quickly and easily inspect and verify their thoughts. Secondly, it empowers doctors to inquire further about specific abnormalities, providing them with the necessary tools to inspect and understand where the information comes from.}

\textcolor{black}{
In terms of the faithfulness aspect, there is concern that the model may capture the distribution of the dataset, relying solely on language priors without comprehending the input image and medical knowledge. To assess this language prior issue, we performed another experiment by removing all images and only keeping the questions. As shown in Tab.~\ref{tab:prior}, the resulting predictions were significantly worse than those obtained using the original images.
}

\begin{table}[h]
\caption{Comparison results between our method using questions only and using both images and questions.}
\label{tab:prior}
\begin{center}
\begin{tabular}{llll}
\toprule
Metrics  & Questions only & Images + questions  \\
% \hline \\
\midrule
Bleu-1   & 0.51     & \bf 0.62 \\
Bleu-2   & 0.33     & \bf 0.54 \\
Bleu-3   & 0.18    & \bf 0.48 \\
Bleu-4   & 0.12     & \bf 0.42 \\
\color{black}METEOR   & \color{black}0.319    & \color{black}\bf 0.337 \\
\color{black}ROUGE\_L & \color{black}0.340      &\color{black} \bf 0.645 \\
\color{black}CIDEr    & \color{black}0          & \color{black}\bf 1.893 \\
% \color{black}SPICE    & -          & \bf 0.457 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\subsubsection{Comparison of quality of the text.}
To evaluate the generated answers in the "difference" question, we use metrics specifically designed for evaluating generated text, such as BLEU, METEOR, ROUGE\_L, and CIDEr. The comparison results between our method, MCCFormers, and IDCPCL are presented in Tab.~\ref{tab:mcc}. Our method outperforms MCCFormers in all metrics. Although IDCPCL performs better than MCCFormers, it is still not comparable to our method.

\textcolor{black}{Even though our method utilized the pre-training technique, the comparison is not unfair. The main objective of our pre-trained model is to utilize medical knowledge (read and compare the images in each anatomical structure) to construct graph models and capture subtle changes in images related to disease progression. Our model is specifically tailored for the task of medical image difference VQA and does not employ any general pre-trained strategies like contrastive learning in our framework.
}

\textcolor{black}{
The IDCPCL baseline model used contrastive learning and a combination of three pre-training tasks (Masked Language Modeling, Masked Visual Contrastive Learning, and Fine-grained Difference Aligning) to align images and text. This approach was found to be effective in improving image difference captioning on datasets with large changes and complex background variations. To adapt this approach for the medical image difference VQA task, we made modifications to the IDCPCL model and pre-trained the image and text feature extraction on medical images and clinical notes. Contrastive learning has shown superior performances compared to the conventional pre-trained Resnet classification model~\cite{khosla2020supervised}. Despite the complex pre-training tasks employed, our method significantly outperformed IDCPCL across almost all metrics and interpretability measures.
}

MCCFormers has inferior results compared to our method, as it struggles to differentiate between images. 
This is due to the generated answers of MCCFormers being almost identical and its failure to identify the differences between images. 
MCCFormers, a difference captioning method, compares patch to patch directly, which may work well in the simple CLVER dataset. However, in medical images, most of which are not aligned, the patch-to-patch method cannot accurately identify which region corresponds to a specific anatomical structure. Additionally, MCCFormers does not require medical knowledge graphs to find the relationships between different regions.

% IDCPCL, on the other hand, has the ability to align significant differences between images, enabling it to have higher results than MCCFormers. However, it still uses pre-trained patch-wise image features, which is not feasible in the medical domain where finer features are required.

\begin{table}[h]
\caption{Comparison results between our method and MCCFormers on \textit{difference} questions of the MIMIC-diff-VQA dataset}
\label{tab:mcc}
\begin{center}
\begin{tabular}{llll}
\toprule
\bf Metrics  & \bf  MCCFormers & \bf \color{black}IDCPCL & \bf Ours  \\
% \hline \\
\midrule
Bleu-1   & 0.214  & \color{black}0.614      & \bf 0.628 \\
Bleu-2   & 0.190  & \color{black}0.541     & \bf 0.553 \\
Bleu-3   & 0.170  & \color{black}0.474    & \bf 0.491 \\
Bleu-4   & 0.153  & \color{black}0.414    & \bf 0.434 \\
\color{black}METEOR   & \color{black}0.319    & 0.303      &  \bf  0.339\\
\color{black}ROUGE\_L & \color{black}0.340      & \bf 0.582 &    0.577\\
\color{black}CIDEr    & \color{black}0          & 0.703 &  \bf   1.027\\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\subsubsection{Disccussion}
\textcolor{black}{During the process of clinical reasoning using medical imaging studies, a significant amount of background knowledge is utilized to compare the baseline study (past) with the target study (present). However, modeling background clinical expert knowledge is not straightforward due to its implicitness, which necessitates inferring the best configuration of knowledge modeling based on multiple graphs, such as the implicit, spatial, and semantic graphs (see Figure 3). Therefore, we stand on the shoulder of \cite{regat} which constructs a multi-relationship graph for general image VQA.}

\textcolor{black}{Please note that our model differs fundamentally from the one presented in \cite{regat}. Their model is designed specifically for single-image VQA problems, while ours is for medical image difference VQA, which is a novel problem that involves two images. 
Additionally, our approach extracts anatomical structure-aware features. This involves computing and normalizing the image differences within each anatomical structure, ensuring relevance to disease progression, and invariance to changes in image pose, orientation, and scale. 
To develop our approach, we created an expert knowledge-aware graph that utilizes clinical knowledge. This graph follows the workflow of clinicians who read, compare, and diagnose diseases from medical images based on anatomical structures.
}
\textcolor{black}{Our model is unique in its approach of incorporating clinical knowledge into a multi-relationship graph learning framework, which has not been utilized in general VQA models. 
}

\subsection{Visualization.} Visualized results can be found in Appendix. 



\section{Conclusion}
\label{conclusion}
First, We propose a medical image difference VQA problem and collect a large-scale MIMIC-Diff-VQA dataset for this task, which is valuable to both the research and medical communities. Also, we design an anatomical structure-aware \textcolor{black}{feature learning approach and an expert knowledge-aware}  multi-relation image difference graph to extract image-difference features. We train an image difference VQA framework utilizing medical knowledge graphs and compare it to current state-of-the-art methods with improved performances. 
\textcolor{black}{However, there are still limitations to our dataset and method.}
Our constructed dataset currently only focuses on the common cases and ignores special ones, \textit{i.e.} cases where the same disease appears in more than two places. Our current Key-Info dataset can only take care of, at most, two locations of the same disease. 
\textcolor{black}{
Furthermore, there are specific cases where different abnormality names may be combined. For example, when examining edema, interstitial opacities are indicative of edema. Therefore, future work should focus on expanding the dataset to include more special cases.}
% Future work could be extending the dataset to consider more special cases.

\textcolor{black}{
It is worth noting that our model also brings some errors. Representative errors can be summarized into three types: 1, confusion between different presentation aspects of the same abnormality, such as atelectasis and lung opacity being mistaken for each other. 2, different names for the same type of abnormality, such as enlargement of the cardiac silhouette being misclassified as cardiomegaly. 3, the pre-trained backbone (Faster-RCNN) used for extracting image features may provide inaccurate features and lead to incorrect predictions, such as lung opacity being wrongly recognized for pleural effusion.}

\begin{acks}
\textcolor{black}{
This research received support from the JST Moonshot R\&D Grant Number JPMJMS2011 and the Japan Society for the Promotion of Science Grant Number 22K07681. Additionally, it was partially supported by the Intramural Research Program of the National Institutes of Health Clinical Center.
}

\end{acks}

\bibliographystyle{ACM-Reference-Format}
\balance
\bibliography{mybib}

\appendix
\section{Appendix for Visualizations, Related Work, MIMIC-Diff-VQA Dataset, and Our Method}
\label{sec:add_app}
For further information on the Visualizations, related work, MIMIC-Diff-VQA dataset, and our method, please refer to the additional appendix, available at \url{https://github.com/Holipori/KDD2023Appendix/blob/main/Appendix.pdf}.
% \clearpage

% \section{More details regarding Dataset and Method}
% \subsection{Related Work}
% \subsection{Image Difference Captioning.}
% The analysis of different images has been explored by a number of researchers in the general domain. 
% The exploration of Image Difference Captioning(IDC) can be split into three phases. 
% The beginning of the first phase is characterized by the "spot-the-diff" dataset~\cite{jhamtani2018learning}, which consists of different frames of the same video surveillance footage.
% This marks the first time that the IDC task has been proposed.
% In this phase, the researchers only focus on the pixel-level difference in the same view of the same scene. 
% ~\cite{jhamtani2018learning} use the clusters of differing pixels as a proxy for exposing object-level differences. 
% ~\cite{tan2019expressing,oluwasanmi2019fully} propose to employ encoder-decoder architecture with attention modules to find the relationship between two images.
% In the second phase, the challenge was upgraded by adding different view angles of the scenes.
% % to identify the difference between different views of the same scene. 
% This demands a higher requirement for the analysis of different regions between images. 
% The iconic dataset in this phase is  the CLEVR-change dataset~\cite{park2019robust}, which comprises pictures of a group of objects(cube, sphere, and cylinder) from different views.
% The attention mechanism is widely employed to address this challenge~\cite{park2019robust,shi2020finding,tu2021semantic,sunbidirectional,kim2021agnostic,qiu2021describing}.
% \cite{hosseinzadeh2021image} proposes to use an auxiliary task to enhance the primary task to generate the captions.
% \cite{liao2021scene} consider 3D information and adopt a scene graph to assist in localizing the changing objects.
% \cite{kim2021agnostic} also introduces a CLEVR-DC dataset, which is similar to CLEVR-change, but with a larger viewpoint change.
% % The game-changer has come to the third phase. 
% The image pairs show more fine-grained visual differences in the third phase. 
% The Birds-to-Words dataset~\cite{forbes2019neural} is composed of a variety of bird images, and each image pair is captioned by human observers.
% Since the species, posture, and background of the birds in each picture vary greatly, this desires a new method to solve the problem.
% \cite{forbes2019neural} proposed Neural Naturalist, a transformer-based model.
% {
% \color{black}
% \cite{yan2021l2c} learns to understand the semantic structures while comparing the images by leveraging image segmentation with
% a novel semantic pooling and using graph convolutional networks to perform reasoning.
% \cite{yao2022image} embrace the pre-training technique to align the visual difference and the text descriptions and achieve state-of-the-art performance. 
% We compared our method with theirs and outperformed them on our medical image difference dataset.
% }


% \subsection{Medical Visual Question Answering.}
% Medical visual question answering aims to answer clinical questions given medical images. Medical images span a wide spectrum of modalities, including CT/MRI imaging, histopathology images, angiography, characteristic imaging appearance, ultrasound, and radiographs \cite{VQA-Med,VQA-RAD,pathvqa}. Clinical questions mainly ask for modality, plane, organ system, and abnormality \cite{VQA-Med}. However, large and well-annotated medical VQA datasets are still in scarcity. Previous MED-VQA methods mostly employ a two-stage procedure: 1) extract visual features on medical images through a detection model like Faster-RCNN \cite{ren2015faster}, YOLO \cite{redmon2016you}, and extract question features via BERT \cite{devlin2018bert}; 2) attempt to aggregate visual and question features for predicting the final answer \cite{zhan2020medical,abacha2018nlm,zhou2018employing,shi2019deep,yan2019zhejiang}. \cite{VQA-RAD} deploys existing VQA models, i.e., the stacked attention network (SAN) \cite{yang2016stacked} and the multimodal compact bilinear pooling (MCB) \cite{fukui2016multimodal}, in general domains to solve MED-VQA. \cite{nguyen2019overcoming} proposes to mix enhanced visual features framework with different attention mechanisms such as bilinear attention network (BAN) \cite{kim2018bilinear} and SAN. \cite{zhan2020medical} proposes separate reasoning modules for different questions to improve the reasoning on medical questions. \cite{shi2019deep} integrates question categories and question topic distributions to assist answer prediction. \cite{yan2019zhejiang} improves the CNN feature extractor with global average pooling to boost classification. \cite{zhou2018employing} applies some image enhancement methods by reconstructing with small random rotations, offsets, scaling, and clipping to boost classification. However, the MED-VQA problem still lacks fine-grained annotations on images, massive diversity of medical data types, and medical reasoning skills from professions, and is thus far from practical.

% \subsection{Other related work.}
% {\color{black}In the general domain, NS-VQA~\cite{yi2018neural} proposed to extract regions of interest(ROIs) with predicted semantic labels and generate scene graphs based on the semantic labels using Mask-RCNN.
% However, NS-VQA focuses on leveraging pre-designed Python logical programs to process different questions and interpret(calculate) the answers. Its answer generation greatly relies on the quality of the object segmentation and labeling by pre-trained Mask-RCNN. Since NS-VQA only evaluated the performance on a simple dataset: CLVER, where all pictures have a single color background, each object has a fixed number of labels and the same label types. Thus, training Mask-RCNN to detect different objects on this dataset is easy to obtain an ideal performance.  

% \cite{liu2021contrastive} proposed to extract abnormality-related image features by constructing a pool of normal chest X-ray images and using contrastive learning to distill the contrastive features between abnormal and normal images to improve the report generation performance. 
% However, We focus on comparing the past and current visiting images from the same patient to track the subtle changes between the two visits. Our method is clinically driven and aims at helping the radiologist validate the hypothesis of what has changed after the intervention for each patient.
% }






% \subsection{MIMIC-Diff-VQA construction}
% % \label{sec:appen_dataset}
% \textcolor{black}{Tab.~\ref{tab:dis} presents the list of disease keywords, while Tab.~\ref{tab:attr} lists the attribute keywords. Tab.~\ref{tab:ques} provides a full list of questions for each question type.}
% \begin{table}[h]
% \caption{Applicable disease names}
% \label{tab:dis}
% \centering
% % \resizebox{\columnwidth}{!}{%
% \begin{tabular}{ll}
% \toprule
% id & Disease names                                                                                                            \\
% \midrule
% 0              & pleural effusion                      \\
% 1              & atelectasis                           \\
% 2              & cardiomegaly                          \\
% 3              & enlargement of the cardiac silhouette \\
% 4              & edema                                 \\
% 5              & hernia                                \\
% 6              & vascular congestion                   \\
% 7              & hilar congestion                      \\
% 8              & pneumothorax                          \\
% 9              & heart failure                         \\
% 10             & lung opacity                          \\
% 11             & pneumonia                             \\
% 12             & tortuosity of the descending aorta    \\
% 13             & scoliosis                             \\
% 14             & gastric distention                    \\
% 15             & hypoxemia                             \\
% 16             & hypertensive heart disease            \\
% 17             & hematoma                              \\
% 18             & tortuosity of the thoracic aorta      \\
% 19             & contusion                             \\
% 20             & emphysema                             \\
% 21             & granuloma                             \\
% 22             & calcification                         \\
% 23             & pleural thickening                    \\
% 24             & thymoma                               \\
% 25             & blunting of the costophrenic angle    \\
% 26             & consolidation                         \\
% 27             & fracture                              \\
% 28             & pneumomediastinum                     \\
% 29             & air collection          \\
% \bottomrule
% \end{tabular}%
% % }
% \end{table}

% % Please add the following required packages to your document preamble:
% % \usepackage{graphicx}

% \begin{table}[h]
% \caption{Attribute keywords for level, location(pre), location(post), and type.}
% \label{tab:attr}
% \centering
% \resizebox{\columnwidth}{!}{%
% \begin{tabular}{llll}
% \toprule
% \multicolumn{4}{c}{Attribute}     
% \\
% \midrule
% level              & location(pre) & location(post)                    & type         \\
% \hline
% moderate           & mid to lower  & the lower lobe                    & interstitial \\
% acute              & left          & the upper lobe                    & layering     \\
% mild               & right         & the middle lobe                   & dense        \\
% small              & retrocardiac  & the left lung base                & parenchymal  \\
% moderately         & pericardial   & the right lung base               & compressive  \\
% severe             & bibasilar     & the lung bases                    & obstructive  \\
% moderate to large  & bilateral     & the left base                     & linear       \\
% moderate to severe & basilar       & the right base                    & plate-like   \\
% mild to moderate   & apicolateral  & the right upper lung              & patchy       \\
% moderate to large  & basal         & the left upper lung               & ground-glass \\
% minimal            & left-sided    & the right middle lung             & calcified    \\
% mildly             & lobe          & the left middle lung              & scattered    \\
% subtle             & lung          & the right mid lung                & interstitial \\
% massive            & area          & the left mid lung                 & focal        \\
% minimally         & right-sided   & the right lower lung              & multifocal   \\
% increasing         & apical        & the left lower lung               & multi-focal  \\
% decreasing        & pleural       & the right upper lobe              &  loculated            \\
% minor              & upper         & the left upper lobe               &  hazy            \\
% trace            & lower         & the right middle lobe             &              \\
%                    & middle        & the left middle lobe              &              \\
%                    & mid           & the right mid lobe                &              \\
%                    & rib           & the left mid lobe                 &              \\
%                    &               & the right lower lobe              &              \\
%                    &               & the left lower lobe               &              \\
%                    &               & the left apical area              &              \\
%                    &               & the left apical region            &              \\
%                    &               & the right apical area             &              \\
%                    &               & the right apical region           &              \\
%                    &               & the apical region                 &              \\
%                    &               & the apical area                   &              \\
%                    &               & the right mid to lower lung       &              \\
%                    &               & the left mid to lower lung        &              \\
%                    &               & the medial right lung base        &              \\
%                    &               & the medial left lung base         &              \\
%                    &               & the upper lungs                   &              \\
%                    &               & the lower lungs                   &              \\
%                    &               & the upper lobes                   &              \\
%                    &               & the lower lobes                   &              \\
%                    &               & the right mid to lower hemithorax &              \\
%                    &               & the soft tissues                 &             \\
%                    &               & the right midlung                &             \\
%                    &               & the left midlung                &             \\
% \bottomrule
% \end{tabular}%
% }
% \end{table}

% \begin{table*}[h]
% \caption{Full list of examples for each question type.}
% \label{tab:ques}
% \centering
% \begin{tabular}{ll}
% \toprule
% Question type                & example                                               \\
% \midrule
% \multirow{4}{*}{Abnormality} & what abnormalities are seen in this image?            \\
%                              & what abnormalities are seen in the \textit{[location]}?               \\
%                              & is there evidence of any abnormalities in this image? \\
%                              & is this image normal?                                 \\ \hline
% \multirow{3}{*}{Presence}    & is there evidence of \textit{[abnormality]} in this image?               \\
%                              & is there \textit{[abnormality]}?                                         \\
%                              & is there \textit{[abnormality]} in the \textit{[location]}?                             \\ \hline
% \multirow{3}{*}{View}        & which view is this image taken?                       \\
%                              & is this PA view?                                      \\
%                              & is this AP view?                                      \\ \hline
% \multirow{4}{*}{Location}    & where in the image is the \textit{[abnormality]} located?                \\
%                              & where is the \textit{[abnormality]}?                                     \\
%                              & is the \textit{[abnormality]} located on the left side or right side?    \\
%                              & is the \textit{[abnormality]} in the \textit{[location]}?                               \\ \hline
% Level                        & what level is the \textit{[abnormality]}?                                \\ \hline
% Type                         & what type is the \textit{[abnormality]}?                                 \\ \hline
% \multirow{2}{*}{Difference}                    & what has changed compared to the reference image?    \\
%                         & what has changed in the \textit{[location]} area?    \\
% \bottomrule
% \end{tabular}
% \end{table*}

% \subsection{Anatomical structure detection}
% The Anatomical structure detection results are shown in Tab.~\ref{tab:ana-detection}.
% \textcolor{black}{Our model heavily relies on anatomical detection results, therefore, we conducted tests using our trained Faster-RCNN on both diffuse and non-diffuse diseases to assess the robustness of our detection system.}
% We select interstitial edema as the diffuse disease.
% Diffuse diseases accounted for 5 out of a total of 200 examples, and non-diffuse diseases accounted for 195 examples.



% \begin{table*}[]
% \centering
% \color{black}
% \caption{{\color{black}Anatomical structure detection results. Precision represents when the Intersection over Union(IoU) threshold is set to 0.5. }}
% \label{tab:ana-detection}
% \begin{tabular}{llll}
%                          % & all 200 examples     & 5 examples                            & 195 examples                      \\
% \bf Category                 & \bf Precision (IoU =0.5) & \bf Diffuse disease Precision & \bf Non-diffuse Precision \\
% \hline \\
% right  lung              & 97.561               & 100                                   & 97.569                            \\
% right  lower lung zone   & 88.774               & 100                                   & 88.72                             \\
% right costophrenic angle & 68.294               & 80.198                                & 68.178                            \\
% left upper lung zone     & 95.075               & 100                                   & 95.114                            \\
% left hilar structures    & 90.092               & 100                                   & 90.479                            \\
% left hemidiaphragm       & 76.314               & 72.277                                & 76.908                            \\
% left clavicle            & 83.859               & 100                                   & 83.808                            \\
% svc                      & 87.734               & 100                                   & 87.729                            \\
% right atrium             & 80.54                & 100                                   & 80.457                            \\
% right upper lung zone    & 95.55                & 100                                   & 95.562                            \\
% right hilar structures   & 92.887               & 100                                   & 92.877                            \\
% right hemidiaphragm      & 83.766               & 100                                   & 83.7                              \\
% left mid lung zone       & 87.251               & 100                                   & 87.774                            \\
% left apical zone         & 92.654               & 100                                   & 93.312                            \\
% trachea                  & 89.421               & 100                                   & 89.444                            \\
% aortic arch              & 90.951               & 100                                   & 90.957                            \\
% cardiac silhouette       & 90.643               & 100                                   & 90.812                            \\
% carina                   & 45.423               & 30.693                                & 45.821                            \\
% right mid lung zone      & 91.776               & 100                                   & 91.754                            \\
% right apical zone        & 93.352               & 100                                   & 93.354                            \\
% left lung                & 96.695               & 100                                   & 96.942                            \\
% left lower lung zone     & 82.534               & 100                                   & 83.01                             \\
% left costophrenic angle  & 63.95                & 80.198                                & 64.321                            \\
% right clavicle           & 87.384               & 100                                   & 87.393                            \\
% upper mediastinum        & 95.216               & 100                                   & 95.26                             \\
% cavoatrial junction      & 66.503               & 100                                   & 65.747                           
% \end{tabular}
% \end{table*}

% \subsection{Relation-Aware Graph Attention Network.}

% % Figure environment removed

% As shown in Fig.\ref{fig:graph}, we construct the multi-relationship graph for both main and reference images and use the relation-aware graph attention network (ReGAT) proposed by~\cite{regat} to learn the graph representation for each image, and embed the image into the final latent feature. In a relation-aware graph attention network, edge labels are embedded to calculate the attention weights between nodes. Please refer to Appendix.~\ref{sec:add_app} for details of the calculation. 
% For simplicity, we use $G_{spa}(\cdot), G_{sem}(\cdot)$, and $G_{imp}(\cdot)$ to represent the spatial graph module, the semantic graph module, and the implicit graph module, respectively.
% Given the input feature nodes $\V$ of each image, the final graph feature $\widetilde V$ can be represented as:
% \begin{equation}
%     \widetilde \V = GAP(G_{spa}(\V) + G_{sem}(\V) + G_{imp}(\V))
% \end{equation}
% where $GAP(\cdot)$ means the global average pooling.
% The image difference graph features $\widetilde \V^{diff}$ is constructed by subtracting the node feature and edge feature between the main and reference image: 
% \begin{equation}
% \mathbf{\widetilde v}_i^{diff} = \mathbf{\widetilde v}_i^{main}-\mathbf{\widetilde v}^{ref}_i, i = 1,\cdots,2N,
% \end{equation}
% where $\mathbf{\widetilde v}_i^{diff}, \mathbf{\widetilde v}_i^{main},\mathbf{\widetilde v}_i^{ref} \in \mathbb{R}^d$ represent the final feature for the $i$-th node of graphs. 
% Therefore, the final graph features $\mathbf{\widetilde V}^{diff}, \mathbf{\widetilde V}^{main}$, and $\mathbf{\widetilde V}^{ref} \in \mathbb{R}^{2N \times d}$  can be obtained.




% \subsection{Feature Attention and Answer Generation}
% Following previous work~\cite{tu2021semantic}, the generated main, reference, and difference features $\mathbf{\widetilde v}_i^{main}, \mathbf{\widetilde v}_i^{ref}, \mathbf{\widetilde v}_i^{diff}$ are applied with feature attention, then output the final feature vectors $\mathbf{l}_m$, $\mathbf{l}_r$, and $\mathbf{l}_{diff}$. For details of the calculation, please refer to Appendix ~\ref{sec:add_app}.
% Next, we use an Answer Generation module that is composed of LSTM networks and attention modules, similar to ~\cite{tu2021semantic}'s setting, to generate the final answer.
% For the calculation details, please also refer to Section.~\ref{sec:answer_gen}.
% \textcolor{black}{
% We use a generative language model because our questions often have a wide range of potential answers (e.g. the \textit{difference} type question). Due to the complicated disease relationships, our dataset has a large and varied pool of answer candidates (51040 answers). Training a simple one-hot encoding classification model for these complicated questions and answers is not practical. Language models, however, can capture the semantic relationship between questions and complicated answers to generate semantic meaningful answers.}
% % A simple classification model is not adequate for our task.



% \subsection{Relation-Aware Graph Attention Network}
% % \label{sec:ragan}

% % Figure environment removed

% For the implicit relationship, each updated node $\widetilde{\textbf{v}}_i \in \mathbb{R}^d$ in the final graph can be calculated as below:
% \begin{equation}
%     \widetilde{\textbf{v}}_i = \mathbf{W}^o \cdot (\|_{m=1}^M \sigma(\sum_{j\in \mathcal{N}_i} \alpha_{ij} \mathbf{W}^m \mathbf{v}_j))
% \end{equation}
% where $\mathcal{N}_i$ is the neighborhood set of the node $i$, $\mathbf{W}^m \in \mathbb{R}^{d \times (d_f + d_q)}$ is the projection matrix, $d$ is the dimension of the final node feature, $\sigma$ is the activation function, $\|_{m=1}^M$ represents concatenating the output of the $M$ attention heads, $\mathbf{W}^o \in \mathbb{R}^{d \times Md}$. 
% The attention weights $\alpha_{ij}$ between the node $i$ and node $j$ consider the similarity between node pairs and the relations between the corresponding region locations. The calculation for $\alpha_{ij}$ can be formulated as:

% \begin{equation}
%     \alpha_{ij} = \frac{\alpha_{ij}^b \cdot \exp{(\alpha_{ij}^v)}}{\sum_{j=1}^K \alpha_{ij}^b \cdot \exp{(\alpha_{ij}^v)}}
% \end{equation}
% \begin{equation}
%     \alpha_{ij}^v = (\mathbf{U}\mathbf{v}_i)^\top \cdot (\mathbf{V}\mathbf{v}_j)
% \end{equation}
% \begin{equation}
%     \alpha_{ij}^b = \max{(0, w\cdot f_b(\mathbf{b}_{ij}))}
% \end{equation}
% where $U, V \in  \mathbb{R}^{d \times (d_f + d_q)}$ are projection matrices. $\mathbf{b}_{ij}$ is the relative geometry feature between node $i$ and $j$, and can be calculated by $ [\log(\frac{|x_i-x_j|}{w_i}), \log(\frac{|y_i-y_j|}{h_i}), \log(\frac{w_j}{w_i}), \log(\frac{h_j}{h_i})]$, $f_b$ is a function that embeds the $4$-dimensional relative geometry feature into $d$-dimensional,$w\in \mathbb{R}^d$ is a vector that transforms the feature into a scalar weight.
% The bounding box coordinates, widths, and heights of the node $i$ and $j$ can be represented by $x_i, x_j, y_i, y_j, w_i, w_j, h_i,$ and $h_j$.

% Spatial and semantic graphs, which can also be called explicit graphs, can be seen as directed graphs. The updating rule considers the relation directions between node pairs and the labels of the edges. The formulation of a single attention head is shown below:

% \begin{equation}
%     \widetilde{\textbf{v}}_i = \sigma(\sum_{j\in \mathcal{N}_i} \alpha_{ij} \mathbf{W}_{dir(i,j)} \mathbf{v}_j + b_{lab(i,j)})
% \end{equation}
% \begin{equation}
%     \alpha_{ij} = \frac{\exp{((\mathbf{U}\mathbf{v}_i)^\top \cdot \mathbf{V}_{dir(i,j)} \mathbf{v}_j + c_{lab(i,j)})}}{\sum_{j\in \mathcal{N}_i}\exp{((\mathbf{U}\mathbf{v}_i)^\top \cdot \mathbf{V}_{dir(i,j)} \mathbf{v}_j + c_{lab(i,j)})}}
% \end{equation}

% where $dir(i,j)$ represents the direction goes from node $i$ to $j$, $lab(i,j)$ is the label assigned to the edge $(i,j)$, $W_{dir(i,j)}, V_{dir(i,j)}\in  \mathbb{R}^{d \times (d_f + d_q)}$ are projection matrices, $b_{lab(i,j)}, c_{lab(i,j)}\in  \mathbb{R}^{d}$ are bias terms.
% The multi-head attention can be calculated similarly by concatenating the output features and adding a projection matrix $\mathbf{W}^o \in \mathbb{R}^{d \times Md}$.

% \subsection{Feature Attention Module}
% % \label{app:feature_attention}
% The generated main image features $\mathbf{\widetilde V}_i^{main}$, reference image feature$\mathbf{\widetilde V}_i^{ref}$ and the difference feature$\mathbf{\widetilde V}_i^{diff}$ are then fed into the Feature Attention Module, which is similar to the two modules in~\cite{tu2021semantic} called Cross-semantic Relation Measuring block(CSRM) and Prior Knowledge-guided Change Localizer. In the Feature Attention module, we first calculate the  prior knowledge $C'_m$, and $C'_r$ for the main image and the reference image, respectively. Take $C'_m$ for example, the calculation process is shown below.
% \begin{equation}
%     C_m = \phi(\mathbf{\widetilde V}^{main}W^c_q + \mathbf{\widetilde V}^{main}W^c_v + b^c)
% \end{equation}
% \begin{equation}
%     A_m = \sigma(\mathbf{\widetilde V}^{main}W^a_q + \mathbf{\widetilde V}^{main}W^a_v + b^a)
% \end{equation}
% \begin{equation}
%     C'_m = A_m \odot C_m
% \end{equation}
% where $C_m\in \mathbb{R}^{2N \times d}$ is the "candidate change", $A_m\in \mathbb{R}^{2N \times d}$ is the "attention gate", $W^c_q, W^c_v, W^a_q, W^a_v \in \mathbb{R}^{d \times d}$, $b^c, b^a \in \mathbb{R}^d$, $\odot$ represents the element-wise multiplication, $\phi$ is the tanh function, $\sigma$ is the sigmoid function. $C'_r$ can be calculated similarly.



% Then, guided by prior knowledge, we calculate the attention weights $a_m$ and $a_r$ for the main image and the reference image, respectively. The formulations are shown below:
% \begin{equation}
%     a_m = \sigma(\text{FC}_2(\text{ReLU}(\text{FC}_1([\mathbf{\widetilde V}^{main}; \mathbf{\widetilde V}^{diff}; C'_m]))))
% \end{equation}
% \begin{equation}
%     a_r = \sigma(\text{FC}_2(\text{ReLU}(\text{FC}_1([\mathbf{\widetilde V}^{ref}; \mathbf{\widetilde V}^{diff}; C'_r]))))
% \end{equation}
% where $[;]$ represents the concatenation, $FC$ represents fully-connected layer, $\sigma$ represents the sigmoid function.

% After obtaining the attention weights $a_m\in \mathbb{R}^{2N}$ and $a_r\in \mathbb{R}^{2N}$, the final image feature vector $\mathbf{l}_m$ and $\mathbf{l}_r$ for the main image and the reference image can be calculated as follows:
% \begin{equation}
%     \mathbf{l}_m = \sum_{i=1}^{2N} a_{m_i}  \mathbf{\widetilde v}_i^{main}
% \end{equation}
% \begin{equation}
%     \mathbf{l}_r =  \sum_{i=1}^{2N} a_{r_i} \mathbf{\widetilde v}_i^{ref}
% \end{equation}
% where $a_m\in \mathbb{R}^{2N}$ and $a_r\in \mathbb{R}^{2N}$ are the attention weights.
% The difference vector is accordingly computed as:
% \begin{equation}
%     \mathbf{l}_{diff} = \mathbf{l}_m - \mathbf{l}_r 
% \end{equation}

% \subsection{Answer Generation}
% \label{sec:answer_gen}

% % Finally, we input $\mathbf{l}_m$, $\mathbf{l}_r$, and $\mathbf{l}_{diff}$ into the Answer Generator to obtain the final answer.
% % Same as ~\cite{tu2021semantic}'s setting, the answer is predicted by LSTM networks with attention modules.
% % The Part-Of-Speech (POS) information is also considered to help generate the answers.

% \textbf{Dynamic Feature Generation.}
% At each time step $t$, we first calculate the attention weights $\alpha_i^{(t)}$, which is for calculating the intermediate dynamic feature $l_{dyn}^{(t)}$ in the next step. The $\alpha_i{(t)}$ can be calculated as follows:


% \begin{equation}
%     v = \text{ReLU}(W_{a_1} [l_{bef}; l_{diff}; l_{aft}] + b_{a_1})
% \end{equation}
% \begin{equation}
%     u^{(t)} = [ v ; h_c^{(t-1)}]
% \end{equation}
% \begin{equation}
%     h_a^{(t)} = LSTM_a(h_a^{(t)} | u^{(t)}, h_a^{(0:t-1)})
% \end{equation}
% \begin{equation}
%     \alpha_i^{(t)} \sim Softmax(W_{a_2}h_a^{(t)} + b_{a_2})
% \end{equation}

% where $W_{a_1}, W_{a_2}, b_{a_1}, b_{a_2}$ are learnable parameters, $LSTM_a$ is a LSTM network used as attention weights generator, $h_a^{(t)}$ is the output of the $LSTM_a$ at the time step $t$, $h_c^{(t-1)}$ is the output of the answer generator $LSTM_c$ at the time step $t-1$, which will be explained in more detail later. 

% Then, the intermediate dynamic feature $l_{dyn}^{(t)}$ can then be calculated as follows:

% \begin{equation}
%     l_{dyn}^{(t)} = \sum_i \alpha_i^{(t)} l_i
% \end{equation}

% where $i \in (bef, diff, aft)$. 

% Before calculating the final dynamic feature $L_{dyn}^{(t)}$, POS feature $p^{(t)}$ needs to be obtained first. The POS feature is calculated from the hidden embedding of the answer $h_c^{(t-1)}$ from the last time step. The calculation can be formulated as below:

% \begin{equation}
%     h_p^{(t)} = \text{ReLU}(W_{p_1} h_c^{(t-1)} + b_{p_1})
% \end{equation}
% \begin{equation}
%     w_p^{(t)} = Softmax(W_{p_2} h_p^{(t)} + b_{p_2})
% \end{equation}
% \begin{equation}
%     p^{(t)} = E_p w_p^{(t)}
% \end{equation}

% where $W_{p_1}, W_{p_2}, b_{p_1}, b_{p_2}$ are learnable parameters, $E_p$ is a learnable POS embedding matrix.

% With the intermediate dynamic feature $l_{dyn}^{(t)}$ and the POS feature $p^{(t)}$, we can calculated the final dynamic feature $L_{dyn}^{(t)}$.

% \begin{equation}
%     \beta_t = \sigma(W_{c_2}(\text{ReLU}(W_{c_1}[p^{(t)}; h_c^{(t-1)}; l_{dyn}^{(t)}])))
% \end{equation}
% \begin{equation}
%     L_{dyn}^{(t)} = \beta_t \odot l_{dyn}^{(t)}
% \end{equation}

% where the range of $\beta_t$ is $[0,1]$, the value of it indicates how much the visual information will be used in the answer generation part.

% \textbf{Answer generator.}
% The answer is generated by an LSTM network word by word. The initial word at time step $0$ is the $<start>$ token.

% \begin{equation}
%     c^{(t)} = [E[w^{(t-1)}]; L_{dyn}^{(t)}]
% \end{equation}
% \begin{equation}
%     h_c^{(t)} = \text{LSTM}_c(h_c^{(t)} |c^{(t)} , h_c^{(0:t-1)})
% \end{equation}
% \begin{equation}
%     w^{(t)} \sim Softmax(W_c h_c^{(t)} + b_c)
% \end{equation}

% where $E$ is a word embedding layer, $E[w^{(t-1)}]$ is the word embedding for the word $w^{(t-1)}$, $W_c, b_c$ are learnable parameters.

% We adopt the generative language model because our questions have highly diverse answers. (e.g. the \textit{difference} type question).
% A simple classification model is not adequate for our task.

% {\color{black}
% \subsection{Other results}
% \label{sec:other}
% We evaluated our proposed multi-relationship graph for the general chest X-ray image classification-based VQA problem (14 diseases) and compared it to state-of-the-art method SYSU-HCP~\cite{gong2021sysu}, the best team in the ImageCLEF-VQA-Med 2021 task.
% As shown in Tab.~\ref{tab:class-vqa}, We use AUC as the metric because answering abnormality questions can be considered a multi-label classification problem.
% Our model achieved significant improvement compared to the state-of-the-art disease classification performance.

% We show the results of our model on each question type in Tab.~\ref{tab:ques-type-bleu}. It is worth noting that, Bleu 3 and Bleu 4 tend to have low scores. This is because the answers to most of the questions are short, except for the "Difference" questions.  For abnormality questions, 72\% of the answers have less than or equal to 2 words; for location questions, 79\% of the answers have less than or equal to 2 words; 93\% of level questions have one-word answers.

% \begin{table}[]
% \color{black}
% \centering
% \caption{\color{black}Results of classification-based VQA problem.}
% \label{tab:class-vqa}
% \begin{tabular}{lll}
% \textbf{Answer}            & \textbf{SYSU-HCP} & \textbf{Ours} \\
% \hline \\
% Pneumothorax               & 0.806             & 0.876         \\
% edema                      & 0.737             & 0.893         \\
% lung lesion                & 0.665             & 0.843         \\
% no                         & 0.537             & 0.951         \\
% lung opacity               & 0.605             & 0.859         \\
% ateletasis                 & 0.645             & 0.868         \\
% pleural other              & 0.858             & 0.845         \\
% support devices            & 0.769             & 0.924         \\
% pneumonia                  & 0.715             & 0.833         \\
% pleural effusion           & 0.796             & 0.938         \\
% enlarged cardiomediastinum & 0.725             & 0.828         \\
% yes                        & 0.545             & 0.944         \\
% consolidation              & 0.708             & 0.819         \\
% cardiomegaly               & 0.688             & 0.892         \\
% fracture                   & 0.664             & 0.871         \\
% total (micro)              & 0.792             & 0.934         \\
% total (macro)              & 0.697             & 0.879        
% \end{tabular}
% \end{table}

% \begin{table}[]
% \centering
% \color{black}
% \caption{\color{black}Results of each question type. "-" represents not applicable because no ground truth answer has enough words to trigger the corresponding Bleu metric. }
% \label{tab:ques-type-bleu}
% \begin{tabular}{lllll}
% \bf Question type & \bf Bleu 1 &\bf  Bleu 2 &\bf  Bleu 3 & \bf Bleu 4 \\
% \hline \\
% Abnormality   & 0.482  & 0.333  & 0.197  & 0.109  \\
% Presence      & 0.801  & -      & -      & -      \\
% View          & 0.948  & 0.941  & -      & -      \\
% Location      & 0.525  & 0.364  & 0.210  & 0.144  \\
% Level         & 0.496  & 0.101  & 0.068  & -        \\
% Difference    & 0.641  & 0.564  & 0.500  & 0.441 
% \end{tabular}
% \end{table}

% }




% \subsection{More visualization examples of our method}
% % \label{sec:vis}
% {
% \color{black} To prove the improvement of the interpretability of our model by adding the spatial and semantic graphs, we visualize the ROIs of our model using different graphs and demonstrate the predictions. As shown in Fig.~\ref{fig:vis_location}(b), our model using the only implicit graph missed the regions important for the question and failed to interpret the correct answer. In contrast, as shown in Fig.~\ref{fig:vis_location}(a), with the help of the spatial relationship graph, our model succeeded in finding the critical region and delivering the correct answer.

% Fig.~\ref{fig:vis_abn} demonstrates a similar scenario on an abnormality-type question. our model using only the implicit graph detected only one abnormality, atelectasis, missed pleural effusion, and lung opacity.
% However, with the help of the semantic relationship graph, which emphasizes the relationship between pleural effusion, atelectasis, and lung opacity, our full model detected all three abnormalities and provided the correct answer.

% % Figure environment removed
% % Figure environment removed
% }

% As shown in Fig.~\ref{fig:vis2}, when asking about pleural effusion, which is an abnormality that happens in the lower lung when there is excess fluid between the layers of the pleura outside the lungs, our method highlighted the corresponding regions (left lower lung). Also, by focusing on these regions, our method can accurately determine the change in the level of pleural effusion between the main and reference image. 
% In Fig.~\ref{fig:vis1}, our method also highlighted cardiac silhouette, this could be because of the strong semantic relationship between cardiomegaly and pleural effusion as mentioned in Section.~\ref{sec:method} and Fig.~\ref{fig:progression}. 

% % Figure environment removed
% % Figure environment removed



% % % Figure environment removed


\end{document}
\endinput
%%
%% End of file `sample-authordraft.tex'.
