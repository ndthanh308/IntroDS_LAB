\vspacebeforesection
\section{Evaluation}
\label{sec:evaluation}

In this section, we evaluate the cost of the attack and the effectiveness of our detection and mitigation methods.

\vspacebeforesection
\subsection{Setup}
\label{sec:evaluation_setup}

We perform our experiments in agreement with Protocol Labs on the live IPFS networks. All our attacks targeted only content that we created. No content provided by other users was affected.

In the evaluation, we use three types of DHT nodes: \textit{(i)} the malicious Sybil nodes, \textit{(ii)} a \provider node hosting the content that we created, and \textit{(iii)} a \downloader node that attempts to resolve the target CID and fetch its content. 
%
All these nodes were hosted on a single AWS $\mathsf{t3.xlarge}$ instance with $4$ vCPUs and $16$ GiB memory.
%
The attacker node is implemented as a custom DHT client using \texttt{libp2p}~\cite{libp2p_github}. Our mitigation and detection methods are also implemented on top of \texttt{libp2p}. 
Except for the experiment of \Cref{fig:eclipse_timeline}, the \provider sends the provider record (\ie, it provides the content) after the Sybils are launched so that we avoid waiting for $\twarmup=48$ hours for the attack to take effect.


%

\vspacebeforesection
\subsection{DHT Lookup Accuracy}

% Figure environment removed%

The DHT query $\Call{GetClosestPeers}{\key}$ is ideally expected to return the $k=20$ closest peer IDs to the queried $\key$.
\Cref{fig:20closest} shows the number of $\Call{GetClosestPeers}{\key}$ queries in which each peer close to the queried $\key$ is discovered. The x-axis shows the index of nodes ordered by their distance to the queried $\key$ ($1$ is the closest node). The y-axis shows the fraction of queries in which each node was discovered. The results are averaged over $100$ experiments. The set of peers obtained is compared with the true $20$ closest peers (`perfect routing') that were obtained from a network crawler. We observed that when different nodes perform this query, and when the same node does it multiple times, the set of $20$ peers received in response is not always the same. This effect is present in both versions of \texttt{libp2p} considered here but the query responses are more consistent in the newer version. To mitigate this effect, the attacker has to use a larger number of Sybils $e > 20$ for high attack effectiveness $\aeff \to 100\%$.

\vspacebeforesection
\subsection{Attack}


% Figure environment removed%

We follow by determining the number of Sybil identities $\numSybils$ required to achieve high attack effectiveness $\aeff$.
%
%
%
Since the DHT queries do not consistently return the same set of peers, \downloaders may discover some honest \resolvers when there are only $\numSybils = 20$ Sybils.
%
\Cref{fig:attack_success_rate} shows the success
%
rate of the attack as the number of Sybil peers varies.
%
%
%
%
Even when $\numSybils < 20$, the attack sometime succeeds because too few honest \resolvers are contacted, and they may be offline.
Involving more Sybil peers increases the chance of a successful attack but also
%
proportionally increases the cost for the attacker.
%
%
With $e=45$ Sybils, the attack succeeds with $99\%$ probability. We use this value for the rest of the experiments.
%
%
%

 % Figure environment removed%
%
%
When the Sybil nodes are placed before the target content is added to the network, the attack takes effect immediately ($\twarmup=0$). However, we also explore a more realistic timeline in which content has been provided before launching the attack. %
\Cref{fig:eclipse_timeline} presents the evolution of the attack effectiveness $\aeff$ over time. For each censored CID, we spawn $5$ \downloader nodes per hour.
%
We stop the experiment when all queries have been unsuccessful for at least $3$ hours. 
%
Immediately after starting the attack, the effectiveness reaches $\aeff = 30\%$. This is caused by a portion of the \downloaders not encountering any honest \resolvers on their path toward the attacked region. The effectiveness steadily increases over time. Multiple spikes (\eg after 12h) are caused by a portion of \resolvers (running older IPFS versions) dropping the \provider records. 
%
%
The attack takes full effect after $48$ hours, which is when \resolvers drop the \provider record as per the current IPFS version.
%
Based on this result, we assume a maximum warmup time $\twarmup=48$h in the cost calculations that follow.

%

% Figure environment removed

Finally, we explore the \emph{cost} of performing the attack.
The AWS instances running the Sybils cost $\coper=0.16\$$ per hour.
\Cref{fig:sybil_generation_time} presents the time and monetary cost of generating $\numSybils = 45$ Sybil identities for both cryptosystems present in IPFS. For readability, we omit the number of iterations $s$, as it is the same for both methods and proportional to the cost/time. The monetary cost $\cgen$ and the generation time increase linearly with the network size. In larger networks, the distance between the closest honest \resolver and the target CID decreases and the generation algorithm requires more iterations. EdDSA is significantly faster than RSA and the generation time remains below $12$s translating into $0.0005$\$, even for the largest evaluated network with $n=30,000$ nodes. Generating Sybils using RSA, while slower, is feasible even for moderately resourceful attackers.

The peak CPU utilization of $30\%$ occurred only during the generation of Sybil private keys. The maximum bandwidth utilization of the machine hosting $\numSybils=45$ Sybils was $4.67$ Mbps (both inbound and outbound) when no requests were made for the censored CID. Taking the cost of generating Sybil identities $\cgen=0.0005$\$, the longest warmup time $\twarmup=48$h, and the duration of the attack $\teff$ hours, the total cost of the attack using EdDSA is given by $\catt = \cgen  + (\twarmup + \teff) \times \coper  = 7.68 + \teff \times 0.16\$$.
%
%
%




%

\vspacebeforesection
\subsection{Detection}
% Figure environment removed

We follow by investigating our attack detection mechanism, which calculates the KL divergence between a model and empirical peer ID distributions, and flags a CID as under attack if the divergence is above a certain threshold $\threshold$.
In \Cref{fig:fp-fn}, we plot the false positive and false negative rates of the detection method for different choices of the detection threshold (the lower the threshold, the more false positives, but the fewer false negatives).
%
We see that increasing the number of Sybils makes the attack easier to detect, with fewer false negatives.
Each DHT node can choose its own detection threshold, based on its desired false positive and false negative rates.
However, it is reasonable for the default implementation to choose a threshold that favors fewer false negatives, thereby mitigating most attacks, at the cost of a small overhead of running the mitigation even when there is no attack.
In the following experiments, we choose a threshold of
$0.94$ which achieves $4.4\%$ false positives and $0.81\%$ false negatives
%
(circled in \Cref{fig:fp-fn}).
%
%
%

% Figure environment removed%

In \Cref{fig:kl-threshold}, we show the results of the detection method for $250$ experiments each with a different number of Sybil peers. Each point represents the result of a single experiment and the y-coordinate is the observed KL divergence value. The percentage of successful attacks (solid circles), where the provider record was not found by the \downloader, and the percentage of experiments that were flagged as attacks are indicated. If the number of Sybils launched by the attacker is decreased, more attempted attacks go undetected (false negatives), but we see that these attacks are not successful either.
%
This effect is also summarized in \Cref{fig:attack_success_rate}.

% Figure environment removed%

To run experiments for different network sizes $N$, we simulate a DHT network by generating $N$ random peer IDs. Requests for a CID are resolved by simply finding the $20$ closest peer IDs in this simulated network. Network size estimation and detection are performed using these simulated responses. In \Cref{fig:diff_netsizes}, we show that the KL divergence metric is robust to changes in the network size. This is because our detection method automatically calculates the model distribution based on the estimated network size.
Therefore, a fixed detection threshold can be used even as the network size changes.
Note that the KL divergence values in such a simulation tend to be lower than those measured on the real network because lookups in the real network do not always result in the correct $20$ closest peers.




%

\vspacebeforesection
\subsection{Mitigation}

We evaluate the performance and the overhead of using our mitigation mechanism in the live IPFS network. Mitigation of a censorship attack on $\cid$ is successful if its \downloaders successfully retrieve at least one valid provider record using a $\Call{FindProviders}{\cid}$ operation. Our mitigation mechanism uses region-based queries 
%
with a region containing $20$ peer IDs on average
for both $\Call{FindProviders}{\cid}$ and $\Call{Provide}{\cid}$ when an attack is detected. %

%

%
For different number of Sybils $\numSybils$, we launch censorship attacks on 50 different CIDs. During each attack, we launch $\numSybils$ Sybil peers, and after waiting for one minute, we provide the target CID from a separate DHT instance. We then test the reachability of the content from ten different \downloaders. %
As a baseline comparison, we also provide results for $\numSybils=0$ whenever appropriate. 

 % Figure environment removed

\para{Mitigation effectiveness} \Cref{fig:mit-success-sybils} illustrates the mitigation effectiveness $\meff$ for different numbers of Sybils $e$, and provides results without the mitigation for comparison. %
%
Our mechanism mitigates all of the detected attacks for all the evaluated Sybil numbers. Importantly, for $e=45$, the number of \downloaders receiving their content increases from only $0.44\%$ without the mitigation to $100\%$ when it is activated.


 % Figure environment removed



%
To better understand the region-based queries, \Cref{fig:mit-overhead-sybils} presents the average size of the following sets of peers: (i) \textit{contacted:} peers encountered by the \downloaders during \Call{FindProviders}{\cid}, (ii) \textit{updated:} \resolvers that obtain the provider record for $\cid$, and (iii) \textit{intersection:} non-Sybil peers in the intersection of the two aforementioned sets of peers. We omit unresponsive peers in both sets. The number of contacted peers increases with the increasing number of Sybils. Higher peer density in the target region causes additional lookups that reach honest peers located nearby. For the same reason, the number of updated peers increases as well.

The number of contacted peers is substantially higher than the number of updated peers, because the former includes not only just the peers within the target region but also peers encountered during the DHT walk toward that target region. More importantly, the average size of the non-Sybil intersection of contacted and updated peers oscillates around 30. This is higher than the expected value of 20 (\ie, the region size used by the mitigation mechanism) because the current network size estimation slightly underestimates the size of the IPFS network. Higher intersection size ensures successful mitigation even when multiple honest nodes in the region are offline.

% Figure environment removed

\para{Mitigation overhead} In \Cref{fig:mit-lookups-sybils}, we assess the overhead of the mitigation mechanism in terms of the number of DHT lookups involved in region-based queries. While the number of lookups is up to $9$ times higher than for an un-attacked network, the overhead increases sub-linearly with the number of Sybils, and is incurred only when an attack is detected.

We then measure the latency of the $\Call{Provide}{}$ as well as $\Call{FindProviders}{}$ operations. The average latencies of these operations are shown in \Cref{tab:latency} when under attack and otherwise, for both the default and the mitigation modes.
%
In general, $\Call{FindProviders}{}$ waits until it finds $20$ distinct providers, or it has contacted all \resolvers. This is followed in both the default and mitigation modes. However, the first \provider record is obtained much earlier. Although finding one \provider record may be enough in the optimistic case, the first \provider record that the \downloader obtains may not be correct (it may be old or may be sent by a malicious peer) and therefore the \downloader might need to wait longer to retrieve the content.
Even though the mitigation mode requires several DHT queries while the default mode uses only one, the latency of the mitigation mode is not much higher than the default mode. This is because the subsequent queries are much faster than the first query.

\begin{table}[t]
    %
    %
    \newcolumntype{G}{>{\raggedright\arraybackslash} m{0.2\linewidth} }
    \newcolumntype{H}{>{\raggedright\arraybackslash} m{0.15\linewidth} }
    \newcolumntype{I}{>{\raggedright\arraybackslash} m{0.227\linewidth} }
    \renewcommand{\arraystretch}{1.2}
    
    %
    \begin{tabular}{GHII}
    
    \toprule
    & & Attack (45 Sybils) & No attack (0 Sybils) \\
    \midrule
    \multirow{2}{*}{Provide} & Default & \textcolor{red}{---} & 15914 ms \\
    %
    & Mitigation & 24780 ms & 27099 ms \\
    \midrule
    \multirow{2}{*}{Find $20$ providers} & Default & \textcolor{red}{---} & 26483 ms \\
    %
    & Mitigation & 28930 ms & 27756 ms \\
    \midrule
    \multirow{2}{*}{Find $1$ providers} & Default & \textcolor{red}{---} & 647 ms \\
    %
    & Mitigation & 712 ms & 329 ms \\
    \bottomrule
    
    \end{tabular}
    %
    \caption{Average latency (milliseconds) of Provide and FindProviders operations with and without our mitigation, during attack and no attack. A red dash indicates that the operation was unsuccessful due to the attack. \vspaceaftercaption}
    \label{tab:latency}
\end{table}
   
\iffalse
    % Figure environment removed
\fi
    


%