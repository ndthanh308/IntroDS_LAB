%
%

%
%
%
%
%
%
%

\section{Artifact Appendix}
\label{sec:ae-appendix}

%
%
%
%
%
%
%

%
 
%

In this work, we implement a censorship attack on the IPFS network, a method to detect the attack, and a method to mitigate the attack.
Our artifact includes the implementations of these three components and experiments to measure their effectiveness, accuracy, and cost.
%

\smallskip
\noindent
Artifact Outline (key aspects):
%

{
\footnotesize
\dirtree{%
.1 /.
.2 README.md.
.2 common.
.3 go-libp2p-kad-dht \begin{minipage}[t]{5cm}
(detection \& mitigation)
\end{minipage}.
.3 go-libp2p-kad-dht-Sybil \begin{minipage}[t]{5cm}
(Sybil DHT implementation)
\end{minipage}.
.3 Sybil\_DHT\_Nodes \begin{minipage}[t]{5cm}
(Sybil node implementation)
\end{minipage}.
.3 kubo \begin{minipage}[t]{5cm}
(standard IPFS implementation)
\end{minipage}.
.2 experimentCombined \begin{minipage}[t]{5cm}
(main experiment \& results)
\end{minipage}.
.3 README.md.
.2 python \begin{minipage}[t]{5cm}
(plots \& other simulations)
\end{minipage}.
.3 README.md.
}
}

%
%
%
%
%

\subsection{Description \& Requirements}
\label{sec:ae-appendix-requirements}

%

%

%



\subsubsection{How to access}
\label{sec:ae-appendix-access}
%
%
%
The artifact is available online
%
at the link \url{\artifactlink}.
The detection and mitigation parts are also available on Github: \url{\githublink},
%
and
are scheduled to be deployed in the official release of \path{go-libp2p-kad-dht}.
%
Our mitigation will be deployed on a per-client basis (not network-wide) and therefore, the experiments in this artifact are expected to remain reproducible as they attack our own \providers and \downloaders that do not use the mitigation.

%

\subsubsection{Hardware dependencies}
\label{sec:ae-appendix-hardware}
%
%
Our experiments require a machine with a public IP address which must allow incoming TCP connections on several ports to allow other IPFS peers to connect to our Sybil peers.
%
%
Our experiments were run on a machine rented from AWS.
The recommended instance type is \verb|t3.xlarge| which has a 2nd generation Intel Xeon Scalable Processor (3.1 GHz) with 4 vCPUs, 16 GB memory, and 5 Gbps peak bandwidth.
While the compute and memory requirements of our experiments are modest, a high peak bandwidth is required to ensure good connectivity of the Sybils and good attack effectiveness.
%
Our artifact includes instructions in the README on how to run the experiments on any other machine with these requirements.

\subsubsection{Software dependencies} 
\label{sec:ae-appendix-software}
%
Recommended operating system: Ubuntu 22.04. Required software: Go v1.19.10, Python 3.10, gcc, Make.

\subsubsection{Benchmarks} 
%
None.

\subsection{Artifact Installation \& Configuration}
\label{sec:ae-appendix-installation}
%

%

For the artifact evaluation, the setup begins with logging in to our provided AWS machine via SSH using the instructions given in the README file. Detailed instructions for each of the following steps are given in the top-level README file.

Network setup: In firewall settings, incoming TCP connections on ports 4001, 5001, 63800-63850 must be enabled.

Install software requirements: Install Go, Python, gcc, and make. We provide a file requirements.txt to help install all required Python packages.

Build and initialize IPFS: We provide the source code of kubo, the IPFS client written in Go, in our artifact. After compiling the source code, an IPFS node must be initialized.
The IPFS node runs in the background during our experiments and is used to obtain information from the network such as the closest peers to the target CID.

\subsection{Experiment Workflow}
\label{sec:ae-appendix-workflow}

%

%

Our main experiment (E1) has two phases: data collection and plotting.

Data collection: In this phase, we run the attack for different numbers of Sybils, different target CIDs, and different \downloader clients. We collect measurements regarding the attack's success, detection results, and mitigation success each time. The code for this step along with instructions is given in the folder \path{experimentCombined/}.

Data processing and plotting: In this phase, we process the collected measurements and generate the plots in the paper (\Cref{fig:attack_success_rate,fig:fp-fn,fig:kl-threshold,fig:mit-success-sybils,fig:mit-lookups-sybils,fig:mit-overhead-sybils}). The code and instructions for this step are in the folder \path{python/}.

%

\subsection{Major Claims}
\label{sec:ae-appendix-claims}
%

%

%

%
%
%
%
%
%
%

%
%

\begin{itemize}
    \item (C1): Our censorship attack using $e=45$ Sybil nodes blocks $99\%$ of users' content requests. This is illustrated in \Cref{fig:attack_success_rate}.
    %
    %
    \item (C2): Our detection mechanism achieves a false negative rate of 0.81\% and a false positive rate of 4.4\% for our chosen detection threshold (\Cref{fig:fp-fn}). Most attacks that were not detected were also not successful in censoring the content, and the detection rate improves as the attack effectiveness increases (\Cref{fig:kl-threshold}). 
    \item (C3): Our mitigation leads to successful content discovery 100\% of the time (\Cref{fig:mit-success-sybils}) by sending \provider records to a constant number of honest peers even as the number of Sybils increases (\Cref{fig:mit-lookups-sybils}). Moreover, the overhead of our mitigation increases sub-linearly with the number of Sybils (\Cref{fig:mit-overhead-sybils}). Claims (C1), (C2), and (C3) are proven by experiment (E1).
    \item (C4): Our censorship attack incurs a low cost for the attacker. The required keys can be generated on commodity hardware within $20$ seconds (for EdDSA) or 2 hours (for RSA). This is proven by the experiment (E2) whose results are illustrated in \Cref{fig:sybil_generation_time}.
\end{itemize}

%

%
%
%
%

\subsection{Evaluation}
\label{sec:ae-appendix-evaluation}

%

%

%

%

%
%

%
%
%
%
%
%
%

\subsubsection{Experiment (E1)}
[Detection and Mitigation] [4 compute-hours]:
%
In a single run of this experiment, we do the following: We create a new file, compute its CID, generate Sybil identifiers based on the CID, and launch Sybil nodes. Then, we launch two DHT nodes, a \provider and a \downloader. The \provider \emph{provides} the file, the \downloader attempts to find \providers for the file and runs the detection. We record the success of the attack and the detection result. Then, the \provider provides the file with the mitigation enabled, the \downloader attempts to find \providers also with the mitigation enabled, and we record the mitigation success, the number of DHT lookups performed by the mitigation, and the number of honest and Sybil peers contacted and successfully updated with \provider records. This whole process is repeated for 
%
10 different files.
This experiment is then repeated for 0, 20, 30, 40, and 45 Sybils. Each run takes 4-5 minutes on average. Hence, we scale down the number of runs for each experiment from 100 to 10 and run it for fewer values of the number of Sybils so that the experiment completes within 4 hours.

%
%

\textit{[Preparation]}
%
None beyond \Cref{sec:ae-appendix-installation}.

\textit{[Execution]}
%
Change the working directory to \path{experimentCombined/}, then build and run the experiment as per instructions in \path{experimentCombined/README.md}. Running the experiment stores the results in the specified output directory.

\textit{[Results]}
%
The folder \path{python/} contains code to generate \Cref{fig:attack_success_rate,fig:fp-fn,fig:kl-threshold,fig:mit-success-sybils,fig:mit-lookups-sybils,fig:mit-overhead-sybils} from the experiment results. For reference, we also provide the results corresponding to the plots in the paper in \path{experimentCombined/detection_results} and \path{experimentCombined/mitigation_results}.
%


\subsubsection{Experiment (E2)}
[Keys Generation Time] [10 compute-hours]: This experiment generates the required Sybil keys for different sizes of the network and analyzes the generation time. 

%

%
%

\textit{[Execution]}
Run two sets of experiments measuring generation time for RSA and EDDSA:
\begin{lstlisting}[language=bash,basicstyle=\footnotesize]
cd python/
go run measureGenerateSybilKeys rsa > timing_rsa.csv
go run measureGenerateSybilKeys eddsa > timing_eddsa.csv
\end{lstlisting}
We provide our results files for reference (generating RSA keys takes a while).
\begin{lstlisting}[language=bash,basicstyle=\footnotesize]
./simulation_results/sample_rsa.csv
./simulation_results/sample_eddsa.csv
\end{lstlisting} 


\textit{[Results]}
The commands below will reproduce the graph in \Cref{fig:sybil_generation_time}.
\begin{lstlisting}[language=bash,basicstyle=\footnotesize]
python3 plot_key_generation_time.py
\end{lstlisting}

\subsection{Customization}
%
%
Experiment (E1) provides arguments to customize the number of Sybils, the number of clients, and the number of CIDs for which to run the experiment.

\subsection{Notes}
%

The key generation time is heavily hardware dependent. The result might thus differ from the ones shown in \Cref{fig:sybil_generation_time}.
The experiment time might be shortened by adjusting the number of tries per network size. However, this comes at the price of losing the precision.

In the artifact, we include the code and instructions for generating all other figures in the paper as well.

%

%
%
%
%