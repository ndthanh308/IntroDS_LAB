% Generated by IEEEtran.bst, version: 1.13 (2008/09/30)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{Rose1990}
R.~Rose and D.~Paul, ``{A hidden Markov model based keyword recognition
  system},'' in \emph{ICASSP}, 1990, pp. 129--132 vol.1.

\bibitem{Chen2014}
G.~Chen, C.~Parada, and G.~Heigold, ``{Small-footprint keyword spotting using
  deep neural networks},'' in \emph{ICASSP}, 2014.

\bibitem{lopez2021}
I.~L{\'o}pez-Espejo, Z.-H. Tan, J.~Hansen, and J.~Jensen, ``{Deep spoken
  keyword spotting: An overview},'' \emph{IEEE Access}, 2021.

\bibitem{Tang2018}
R.~Tang and J.~Lin, ``{Deep Residual Learning for Small-Foot-print Keyword
  Spotting},'' in \emph{ICASSP}, 2018, pp. 5484--5488.

\bibitem{kirkpatrick2017overcoming}
J.~Kirkpatrick, R.~Pascanu, N.~Rabinowitz, J.~Veness, G.~Desjardins, A.~A.
  Rusu, K.~Milan, J.~Quan, T.~Ramalho, A.~Grabska-Barwinska, D.~Hassabis,
  C.~Clopath, D.~Kumaran, and R.~Hadsell, ``{Overcoming catastrophic forgetting
  in neural networks},'' \emph{PNAS}, vol. 114, 2017.

\bibitem{rebuffi2017icarl}
S.-A. Rebuffi, A.~Kolesnikov, G.~Sperl, and C.~H. Lampert, ``{iCaRL:
  Incremental classifier and representation learning},'' in \emph{CVPR}, 2017.

\bibitem{pellegrini2021continual}
L.~Pellegrini, V.~Lomonaco, G.~Graffieti, and D.~Maltoni, ``Continual learning
  at the edge: Real-time training on smartphone devices,''
  \emph{arXiv:2105.13127}, 2021.

\bibitem{petit2023fetril}
G.~Petit, A.~Popescu, H.~Schindler, D.~Picard, and B.~Delezoide, ``Fetril:
  Feature translation for exemplar-free class-incremental learning,'' in
  \emph{WACV}, 2023, pp. 3911--3920.

\bibitem{mai2022}
Z.~Mai, R.~Li, J.~Jeong, D.~Quispe, H.~Kim, and S.~Sanner, ``{Online continual
  learning in image classification: An empirical survey},''
  \emph{Neurocomputing}, vol. 469, pp. 28--51, 2022.

\bibitem{hayes2022online}
T.~L. Hayes and C.~Kanan, ``{Online Continual Learning for Embedded Devices},''
  \emph{CoLLAs-2022}, 2022.

\bibitem{borghi2023challenges}
G.~Borghi, G.~Graffieti, and D.~Maltoni, ``On the challenges to learn from
  natural data streams,'' \emph{arXiv:2301.03495}, 2023.

\bibitem{xiao2022}
Y.~Xiao, N.~Hou, and E.~S. Chng, ``{Rainbow Keywords: Efficient Incremental
  Learning for Online Spoken Keyword Spotting},'' in \emph{Interspeech}, 2022,
  pp. 3764--3768.

\bibitem{yang2022online}
M.~Yang, I.~Lane, and S.~Watanabe, ``Online continual learning of end-to-end
  speech recognition models,'' \emph{Interspeech}, 2022.

\bibitem{huang2022}
Y.~Huang, N.~Hou, and N.~F. Chen, ``{Progressive continual learning for spoken
  keyword spotting},'' in \emph{ICASSP}, 2022.

\bibitem{zenke2017continual}
F.~Zenke, B.~Poole, and S.~Ganguli, ``{Continual learning through synaptic
  intelligence},'' in \emph{ICML}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR,
  2017.

\bibitem{chaudhry2018efficient}
A.~Chaudhry, M.~Ranzato, M.~Rohrbach, and M.~Elhoseiny, ``{Efficient lifelong
  learning with a-gem},'' \emph{ICLR}, 2019.

\bibitem{mai2021supervised}
Z.~Mai, R.~Li, H.~Kim, and S.~Sanner, ``{Supervised contrastive replay:
  Revisiting the nearest class mean classifier in online class-incremental
  continual learning},'' in \emph{CVPRW}, 2021.

\bibitem{hayes2020lifelong}
T.~L. Hayes and C.~Kanan, ``{Lifelong machine learning with deep streaming
  linear discriminant analysis},'' in \emph{CVPRW}, 2020.

\bibitem{w2v2}
A.~Baevski, Y.~Zhou, A.~Mohamed, and M.~Auli, ``{wav2vec 2.0: A Framework for
  Self-Supervised Learning of Speech Representations},'' in \emph{NeurIPS},
  vol.~33, 2020, pp. 12\,449--12\,460.

\bibitem{rouvier2021study}
M.~Rouvier, P.-M. Bousquet, and J.~Duret, ``{Study on the temporal pooling used
  in deep neural networks for speaker verification},'' in \emph{EUSIPCO}.\hskip
  1em plus 0.5em minus 0.4em\relax IEEE, 2021, pp. 501--505.

\bibitem{you2019multi}
L.~You, W.~Guo, L.~Dai, and J.~Du, ``{Multi-Task Learning with High-Order
  Statistics for X-vector based Text-Independent Speaker Verification},'' in
  \emph{Interspeech}, 2019.

\bibitem{dasgupta2007line}
S.~Dasgupta and D.~Hsu, ``{On-line estimation with the multivariate Gaussian
  distribution},'' in \emph{ICCLT}.\hskip 1em plus 0.5em minus 0.4em\relax
  Springer, 2007, pp. 278--292.

\bibitem{mensink2013distance}
T.~Mensink, J.~Verbeek, F.~Perronnin, and G.~Csurka, ``{Distance-based image
  classification: Generalizing to new classes at near-zero cost},''
  \emph{TPAMI}, vol.~35, no.~11, pp. 2624--2637, 2013.

\bibitem{ayub2020cognitively}
A.~Ayub and A.~R. Wagner, ``{Cognitively-inspired model for incremental
  learning using a few examples},'' in \emph{CVPRW}, 2020, pp. 222--223.

\bibitem{ayub2021f}
------, ``{F-SIOL-310: A Robotic Dataset and Benchmark for Few-Shot Incremental
  Object Learning},'' in \emph{ICRA}.\hskip 1em plus 0.5em minus 0.4em\relax
  IEEE, 2021.

\bibitem{anagnostopoulos2012online}
C.~Anagnostopoulos, D.~K. Tasoulis, N.~M. Adams, N.~G. Pavlidis, and D.~J.
  Hand, ``{Online linear and quadratic discriminant analysis with adaptive
  forgetting for streaming classification},'' \emph{SADM}, 2012.

\bibitem{welford1962note}
B.~Welford, ``{Note on a method for calculating corrected sums of squares and
  products},'' \emph{Technometrics}, vol.~4, no.~3, pp. 419--420, 1962.

\bibitem{castro2018end}
F.~M. Castro, M.~J. Mar{\'\i}n-Jim{\'e}nez, N.~Guil, C.~Schmid, and K.~Alahari,
  ``{End-to-end incremental learning},'' in \emph{ECCV}, 2018, pp. 233--248.

\bibitem{lecun1998gradient}
Y.~LeCun, L.~Bottou, Y.~Bengio, and P.~Haffner, ``{Gradient-based learning
  applied to document recognition},'' \emph{IEEE}, 1998.

\bibitem{riesenhuber1999hierarchical}
M.~Riesenhuber and T.~Poggio, ``{Hierarchical models of object recognition in
  cortex},'' \emph{Nature NN}, vol.~2, no.~11, pp. 1019--1025, 1999.

\bibitem{zhou2021mixed}
Q.~Zhou, Z.~Qu, and C.~Cao, ``{Mixed pooling and richer attention feature
  fusion for crack detection},'' \emph{PRL}, vol. 145, pp. 96--102, 2021.

\bibitem{zeiler2013stochastic}
M.~D. Zeiler and R.~Fergus, ``{Stochastic pooling for regularization of deep
  convolutional neural networks},'' \emph{ICLR}, 2013.

\bibitem{bera2020effect}
S.~Bera and V.~K. Shrivastava, ``{Effect of pooling strategy on convolutional
  neural network for classification of hyperspectral remote sensing images},''
  \emph{IET IP}, vol.~14, no.~3, pp. 480--486, 2020.

\bibitem{monteiro2020performance}
J.~Monteiro, M.~J. Alam, and T.~Falk, ``{On the performance of time-pooling
  strategies for end-to-end spoken language identification},'' in \emph{LREC},
  2020, pp. 3566--3572.

\bibitem{wang2021revisiting}
S.~Wang, Y.~Yang, Y.~Qian, and K.~Yu, ``{Revisiting the statistics pooling
  layer in deep speaker embedding learning},'' in \emph{ISCSLP}.\hskip 1em plus
  0.5em minus 0.4em\relax IEEE, 2021, pp. 1--5.

\bibitem{feng2011geometric}
J.~Feng, B.~Ni, Q.~Tian, and S.~Yan, ``{Geometric $l$p-norm feature pooling for
  image classification},'' in \emph{CVPR}, 2011.

\bibitem{li2018towards}
P.~Li, J.~Xie, Q.~Wang, and Z.~Gao, ``{Towards faster training of global
  covariance pooling networks by iterative matrix square root normalization},''
  in \emph{CVPR}, 2018, pp. 947--955.

\bibitem{gsc}
P.~Warden, ``{Speech commands: A dataset for limited-vocabulary speech
  recognition},'' \emph{arXiv:1804.03209}, 2018.

\bibitem{mswc}
M.~Mazumder, S.~Chitlangia, C.~Banbury, Y.~Kang, J.~M. Ciro, K.~Achorn,
  D.~Galvez, M.~Sabini, P.~Mattson, D.~Kanter \emph{et~al.}, ``{Multilingual
  Spoken Words Corpus},'' in \emph{NeurIPS}, 2021.

\bibitem{splits}
Splits are available at:\\ \url{https://github.com/umbertomichieli/TAP-SLDA}.

\bibitem{HuBERT}
W.-N. Hsu, B.~Bolte, Y.-H.~H. Tsai, K.~Lakhotia, R.~Salakhutdinov, and
  A.~Mohamed, ``{Hubert: Self-supervised speech representation learning by
  masked prediction of hidden units},'' \emph{TASLP}, vol.~29, pp. 3451--3460,
  2021.

\bibitem{emformer}
Y.~Shi, Y.~Wang, C.~Wu, C.-F. Yeh, J.~Chan, F.~Zhang, D.~Le, and M.~Seltzer,
  ``{Emformer: Efficient memory transformer based acoustic model for low
  latency streaming speech recognition},'' in \emph{ICASSP}, 2021, pp.
  6783--6787.

\bibitem{torchaudio}
``Torchaudio models,'' \url{https://pytorch.org/audio/stable/pipelines.html},
  {Accessed: 2023-03-03}.

\bibitem{panayotov2015librispeech}
V.~Panayotov, G.~Chen, D.~Povey, and S.~Khudanpur, ``{Librispeech: An ASR
  corpus based on public domain audio books},'' in \emph{ICASSP}.\hskip 1em
  plus 0.5em minus 0.4em\relax IEEE, 2015, pp. 5206--5210.

\bibitem{wang2021unispeech}
C.~Wang, Y.~Wu, Y.~Qian, K.~Kumatani, S.~Liu, F.~Wei, M.~Zeng, and X.~Huang,
  ``Unispeech: Unified speech representation learning with labeled and
  unlabeled data,'' in \emph{ICML}, 2021, pp. 10\,937--10\,947.

\end{thebibliography}
