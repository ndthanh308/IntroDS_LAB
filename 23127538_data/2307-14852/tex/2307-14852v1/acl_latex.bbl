\begin{thebibliography}{63}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi

\bibitem[{Aangenendt(2022)}]{aangenendt2022archives}
Gijs Aangenendt. 2022.
\newblock Archives in the digital age. the use of ai and machine learning in
  the swedish archival sector.
\newblock Master's thesis, Uppsala University.

\bibitem[{An et~al.(2017)An, Bai, Deng, Sun, Zhong, and Dong}]{an2017knowledge}
Xiaomi An, Wenlin Bai, Hepu Deng, Shuyang Sun, Wenrui Zhong, and Yu~Dong. 2017.
\newblock \href {https://doi.org/10.1108/JD-04-2016-0040} {A knowledge
  management framework for effective integration of national archives resources
  in {C}hina}.
\newblock \emph{Journal of Documentation}, 73(1):18--34.

\bibitem[{An et~al.(2014)An, Deng, and Zhang}]{an2014reinventing}
Xiaomi An, Hepu Deng, and Bin Zhang. 2014.
\newblock \href {https://doi.org/10.1080/01576895.2014.911673} {Reinventing the
  concept of the state archival fond in {C}hina}.
\newblock \emph{Archives and Manuscripts}, 42(2):146--150.

\bibitem[{Austin et~al.(2021)Austin, Odena, Nye, Bosma, Michalewski, Dohan,
  Jiang, Cai, Terry, Le, and Sutton}]{Austin2021ProgramSW}
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski,
  David Dohan, Ellen Jiang, Carrie~J. Cai, Michael Terry, Quoc~V. Le, and
  Charles Sutton. 2021.
\newblock Program synthesis with large language models.
\newblock \emph{ArXiv}, abs/2108.07732.

\bibitem[{Blanke and Wilson(2017)}]{blanke2017identifying}
Tobias Blanke and Jon Wilson. 2017.
\newblock \href {https://doi.org/10.1109/BigData.2017.8258172} {Identifying
  epochs in text archives}.
\newblock In \emph{2017 IEEE International Conference on Big Data (Big Data)},
  pages 2219--2224. IEEE.

\bibitem[{Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan,
  Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess,
  Clark, Berner, McCandlish, Radford, Sutskever, and
  Amodei}]{Brown2020LanguageMA}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
  Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
  Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T.~J.
  Henighan, Rewon Child, Aditya Ramesh, Daniel~M. Ziegler, Jeff Wu, Clemens
  Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott
  Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec
  Radford, Ilya Sutskever, and Dario Amodei. 2020.
\newblock Language models are few-shot learners.
\newblock \emph{ArXiv}, abs/2005.14165.

\bibitem[{Cao et~al.(2023)Cao, Li, Liu, Yan, Dai, Yu, and
  Sun}]{cao2023comprehensive}
Yihan Cao, Siyu Li, Yixin Liu, Zhiling Yan, Yutong Dai, Philip~S Yu, and Lichao
  Sun. 2023.
\newblock A comprehensive survey of {AI}-generated content ({AIGC}): A history
  of generative {AI} from gan to {C}hat{GPT}.
\newblock \emph{arXiv preprint arXiv:2303.04226}.

\bibitem[{Chen et~al.(2021)Chen, Tworek, Jun, Yuan
  et~al.}]{Chen2021EvaluatingLL}
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, et~al. 2021.
\newblock Evaluating large language models trained on code.
\newblock \emph{ArXiv}, abs/2107.03374.

\bibitem[{Chen et~al.(2022)Chen, Li, Smiley, Ma, Shah, and
  Wang}]{chen2022convfinqa}
Zhiyu Chen, Shiyang Li, Charese Smiley, Zhiqiang Ma, Sameena Shah, and
  William~Yang Wang. 2022.
\newblock \href {http://arxiv.org/abs/2210.03849} {Conv{F}in{QA}: Exploring the
  chain of numerical reasoning in conversational finance question answering}.

\bibitem[{Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma
  et~al.}]{Chowdhery2022PaLMSL}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, et~al. 2022.
\newblock {PaLM}: Scaling language modeling with pathways.
\newblock \emph{ArXiv}, abs/2204.02311.

\bibitem[{Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick,
  and Tafjord}]{Clark2018ThinkYH}
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa
  Schoenick, and Oyvind Tafjord. 2018.
\newblock Think you have solved question answering? try {ARC}, the {AI}2
  reasoning challenge.
\newblock \emph{ArXiv}, abs/1803.05457.

\bibitem[{Conneau and Kiela(2018)}]{conneau2018senteval}
Alexis Conneau and Douwe Kiela. 2018.
\newblock \href {http://arxiv.org/abs/1803.05449} {Senteval: An evaluation
  toolkit for universal sentence representations}.

\bibitem[{Cui et~al.(2020)Cui, Che, Liu, Qin, Wang, and
  Hu}]{cui-etal-2020-revisiting}
Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shijin Wang, and Guoping Hu.
  2020.
\newblock \href {https://www.aclweb.org/anthology/2020.findings-emnlp.58}
  {Revisiting pre-trained models for {C}hinese natural language processing}.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing: Findings}, pages 657--668, Online. Association
  for Computational Linguistics.

\bibitem[{Cui et~al.(2023)Cui, Yang, and Yao}]{cui2023efficient}
Yiming Cui, Ziqing Yang, and Xin Yao. 2023.
\newblock Efficient and effective text encoding for {C}hinese llama and alpaca.
\newblock \emph{arXiv preprint arXiv:2304.08177}.

\bibitem[{Du et~al.(2021)Du, Qian, Liu, Ding, Qiu, Yang, and
  Tang}]{Du2021GLMGL}
Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and
  Jie Tang. 2021.
\newblock {GLM}: General language model pretraining with autoregressive blank
  infilling.
\newblock In \emph{Annual Meeting of the Association for Computational
  Linguistics}.

\bibitem[{Fan et~al.(2023)Fan, Li, Ma, Lee, Yu, and Hemphill}]{Fan2023ABR}
Lizhou Fan, Lingyao Li, Zihui Ma, Sanggyu Lee, Huizi Yu, and Libby Hemphill.
  2023.
\newblock A bibliometric review of large language models research from 2017 to
  2023.
\newblock \emph{ArXiv}, abs/2304.02020.

\bibitem[{Henninger and Scifleet(2016)}]{henninger2016new}
Maureen Henninger and Paul Scifleet. 2016.
\newblock \href {https://doi.org/10.1108/JD-06-2015-0069} {How are the new
  documents of social networks shaping our cultural memory}.
\newblock \emph{Journal of Documentation}, 72(2):277--298.

\bibitem[{Hutchinson(2018)}]{hutchinson2018protecting}
Tim Hutchinson. 2018.
\newblock \href {https://doi.org/10.1109/BigData.2018.8621929} {Protecting
  privacy in the archives: supervised machine learning and born-digital
  records}.
\newblock In \emph{2018 IEEE International Conference on Big Data (Big Data)},
  pages 2696--2701. IEEE.

\bibitem[{Hutchinson(2020)}]{hutchinson2020natural}
Tim Hutchinson. 2020.
\newblock \href {https://doi.org/10.1108/RMJ-09-2019-0055} {Natural language
  processing and machine learning as practical toolsets for archival
  processing}.
\newblock \emph{Records Management Journal}, 30(2):155--174.

\bibitem[{Jin et~al.(2019)Jin, Dhingra, Liu, Cohen, and Lu}]{jin2019pubmedqa}
Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William~W. Cohen, and Xinghua Lu.
  2019.
\newblock \href {http://arxiv.org/abs/1909.06146} {{PubMedQA}: A dataset for
  biomedical research question answering}.

\bibitem[{Kwiatkowski et~al.(2019)Kwiatkowski, Palomaki, Redfield, Collins
  et~al.}]{Kwiatkowski2019NaturalQA}
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, et~al.
  2019.
\newblock \href {https://doi.org/10.1162/tacl_a_00276} {Natural questions: A
  benchmark for question answering research}.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  7:453--466.

\bibitem[{Lansdall-Welfare and Cristianini(2020)}]{lansdall2020history}
Thomas Lansdall-Welfare and Nello Cristianini. 2020.
\newblock \href {https://doi.org/10.1093/llc/fqy077} {History playground: a
  tool for discovering temporal trends in massive textual corpora}.
\newblock \emph{Digital Scholarship in the Humanities}, 35(2):328--341.

\bibitem[{Lee(2019)}]{lee2019machine}
Benjamin Charles~Germain Lee. 2019.
\newblock \href {https://doi.org/10.1093/llc/fqy063} {Machine learning,
  template matching, and the international tracing service digital archive:
  Automating the retrieval of death certificate reference cards from 40 million
  document scans}.
\newblock \emph{Digital Scholarship in the Humanities}, 34(3):513--535.

\bibitem[{Lewis et~al.(2020)Lewis, Liu, Goyal, Ghazvininejad, Mohamed, Levy,
  Stoyanov, and Zettlemoyer}]{lewis2020bart}
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,
  Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020.
\newblock {BART}: Denoising sequence-to-sequence pre-training for natural
  language generation, translation, and comprehension.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics}, pages 7871--7880.

\bibitem[{Li et~al.(2022)Li, Tomko, Vasardani, and
  Baldwin}]{Li2022MultiSpanQAAD}
Haonan Li, Martin Tomko, Maria Vasardani, and Timothy Baldwin. 2022.
\newblock Multi{S}pan{QA}: A dataset for multi-span question answering.
\newblock In \emph{North American Chapter of the Association for Computational
  Linguistics}.

\bibitem[{Li et~al.(2023{\natexlab{a}})Li, Zhang, Koto, Yang, Zhao, Gong, Duan,
  and Baldwin}]{li2023cmmlu}
Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan
  Duan, and Timothy Baldwin. 2023{\natexlab{a}}.
\newblock \href {http://arxiv.org/abs/2306.09212} {{CMMLU}: Measuring massive
  multitask language understanding in {C}hinese}.

\bibitem[{Li et~al.(2023{\natexlab{b}})Li, Zhang, Zhao, Yang, and
  Yang}]{li2023batgpt}
Zuchao Li, Shitou Zhang, Hai Zhao, Yifei Yang, and Dongjie Yang.
  2023{\natexlab{b}}.
\newblock \href {http://arxiv.org/abs/2307.00360} {Bat{GPT}: A bidirectional
  autoregessive talker from generative pre-trained transformer}.

\bibitem[{Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov}]{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.
\newblock {RoBERTa}: A robustly optimized {BERT} pretraining approach.
\newblock \emph{arXiv preprint arXiv:1907.11692}.

\bibitem[{Lu et~al.(2023)Lu, Liang, Xu, He, Geng, Han, Xin, Wu, and
  Xiao}]{Lu2023BBTFinCC}
Dakuan Lu, Jiaqing Liang, Yipei Xu, Qi~He, Yipeng Geng, Mengkun Han, Ying Xin,
  Hengkui Wu, and Yanghua Xiao. 2023.
\newblock {BBT-F}in: Comprehensive construction of {C}hinese financial domain
  pre-trained language model, corpus and benchmark.
\newblock \emph{ArXiv}, abs/2302.09432.

\bibitem[{Moss et~al.(2018)Moss, Thomas, and Gollins}]{moss2018reconfiguration}
Michael Moss, David Thomas, and Tim Gollins. 2018.
\newblock The reconfiguration of the archive as data to be mined.
\newblock \emph{Archivaria}, 86(86):118--151.

\bibitem[{Moss(1996)}]{moss1996dang}
William~W Moss. 1996.
\newblock \href {https://doi.org/10.1017/S0305741000044155} {Dang'an:
  contemporary {C}hinese archives}.
\newblock \emph{The China Quarterly}, 145:112--129.

\bibitem[{{National Archives Administration}(2023)}]{saac2022}
{National Archives Administration}. 2023.
\newblock Summary of basic information on national archives administration and
  archives in 2021 (part 2).
\newblock
  \url{https://www.saac.gov.cn/daj/zhdt/202208/b9e2f459b5b1452d8ae83d7f78f51769.shtml}.

\bibitem[{OpenAI(2023)}]{OpenAI2023GPT4TR}
OpenAI. 2023.
\newblock {GPT}-4 technical report.
\newblock \emph{ArXiv}, abs/2303.08774.

\bibitem[{Pahune and Chandrasekharan(2023)}]{Pahune2023SeveralCO}
Saurabh~A Pahune and Manoj Chandrasekharan. 2023.
\newblock Several categories of large language models ({LLM}s): A short survey.
\newblock \emph{International Journal for Research in Applied Science and
  Engineering Technology}.

\bibitem[{Pal et~al.(2022)Pal, Umapathi, and Sankarasubbu}]{pal2022medmcqa}
Ankit Pal, Logesh~Kumar Umapathi, and Malaikannan Sankarasubbu. 2022.
\newblock \href {http://arxiv.org/abs/2203.14371} {{MedMCQA}: A large-scale
  multi-subject multi-choice dataset for medical domain question answering}.

\bibitem[{Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu}]{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J Liu. 2020.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{The Journal of Machine Learning Research}, 21(1):5485--5551.

\bibitem[{Rajpurkar et~al.(2018)Rajpurkar, Jia, and
  Liang}]{Rajpurkar2018KnowWY}
Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018.
\newblock Know what you donâ€™t know: Unanswerable questions for squad.
\newblock In \emph{Annual Meeting of the Association for Computational
  Linguistics}.

\bibitem[{Roper(2003)}]{roper2003archives}
Michael Roper. 2003.
\newblock \href {https://doi.org/10.1108/00220410310499645} {Archives and the
  public good: Accountability and records in modern society}.
\newblock \emph{Journal of documentation}, 59(5):617--619.

\bibitem[{Sawada et~al.(2023)Sawada, Paleka, Havrilla, Tadepalli, Vidas,
  Kranias, Nay, Gupta, and Komatsuzaki}]{sawada2023arb}
Tomohiro Sawada, Daniel Paleka, Alexander Havrilla, Pranav Tadepalli, Paula
  Vidas, Alexander Kranias, John~J. Nay, Kshitij Gupta, and Aran Komatsuzaki.
  2023.
\newblock \href {http://arxiv.org/abs/2307.13692} {{ARB}: Advanced reasoning
  benchmark for large language models}.

\bibitem[{Scao et~al.(2022)Scao, Fan, Akiki, Pavlick, Ili'c
  et~al.}]{Scao2022BLOOMA1}
Teven~Le Scao, Angela Fan, Christopher Akiki, Elizabeth-Jane Pavlick, Suzana
  Ili'c, et~al. 2022.
\newblock {BLOOM}: A 176b-parameter open-access multilingual language model.
\newblock \emph{ArXiv}, abs/2211.05100.

\bibitem[{Shabou et~al.(2020)Shabou, Ti{\`e}che, Knafou, and
  Gaudinat}]{shabou2020algorithmic}
Basma~Makhlouf Shabou, Julien Ti{\`e}che, Julien Knafou, and Arnaud Gaudinat.
  2020.
\newblock \href {https://doi.org/10.1108/RMJ-09-2019-0049} {Algorithmic methods
  to explore the automation of the appraisal of structured and unstructured
  digital data}.
\newblock \emph{Records management journal}, 30(2):175--200.

\bibitem[{Shah et~al.(2022)Shah, Chawla, Eidnani, Shah, Du, Chava, Raman,
  Smiley, Chen, and Yang}]{Shah2022WhenFM}
Raj~Sanjay Shah, Kunal Chawla, Dheeraj Eidnani, Agam Shah, Wendi Du, Sudheer
  Chava, Natraj Raman, Charese Smiley, Jiaao Chen, and Diyi Yang. 2022.
\newblock When {FLUE} meets {FLANG}: Benchmarks and large pretrained language
  model for financial domain.
\newblock \emph{ArXiv}, abs/2211.00083.

\bibitem[{Shanahan(2022)}]{Shanahan2022TalkingAL}
Murray Shanahan. 2022.
\newblock Talking about large language models.
\newblock \emph{ArXiv}, abs/2212.03551.

\bibitem[{Shao et~al.(2021)Shao, Geng, Liu, Dai, Yan, Yang, Zhe, Bao, and
  Qiu}]{shao2021cpt}
Yunfan Shao, Zhichao Geng, Yitao Liu, Junqi Dai, Hang Yan, Fei Yang, Li~Zhe,
  Hujun Bao, and Xipeng Qiu. 2021.
\newblock {CPT}: A pre-trained unbalanced transformer for both {C}hinese
  language understanding and generation.
\newblock \emph{arXiv preprint arXiv:2109.05729}.

\bibitem[{Talmor et~al.(2019)Talmor, Herzig, Lourie, and
  Berant}]{Talmor2019CommonsenseQAAQ}
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019.
\newblock Commonsense{QA}: A question answering challenge targeting commonsense
  knowledge.
\newblock \emph{ArXiv}, abs/1811.00937.

\bibitem[{Taori et~al.(2023)Taori, Gulrajani, Zhang, Dubois, Li, Guestrin,
  Liang, and Hashimoto}]{alpaca}
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos
  Guestrin, Percy Liang, and Tatsunori~B. Hashimoto. 2023.
\newblock Stanford alpaca: An instruction-following {LLaMA} model.
\newblock \url{https://github.com/tatsu-lab/stanford_alpaca}.

\bibitem[{Taylor et~al.(2022)Taylor, Kardas, Cucurull, Scialom, Hartshorn,
  Saravia, Poulton, Kerkez, and Stojnic}]{Taylor2022GalacticaAL}
Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony~S.
  Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic.
  2022.
\newblock Galactica: A large language model for science.
\newblock \emph{ArXiv}, abs/2211.09085.

\bibitem[{Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet,
  Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, Rodriguez, Joulin,
  Grave, and Lample}]{Touvron2023LLaMAOA}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
  Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric
  Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and
  Guillaume Lample. 2023{\natexlab{a}}.
\newblock {LLaMA}: Open and efficient foundation language models.
\newblock \emph{ArXiv}, abs/2302.13971.

\bibitem[{Touvron et~al.(2023{\natexlab{b}})Touvron, Lavril, Izacard, Martinet,
  Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar
  et~al.}]{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
  Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric
  Hambro, Faisal Azhar, et~al. 2023{\natexlab{b}}.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}.

\bibitem[{Tseng et~al.(2015)Tseng, Lee, Chang, and
  Chen}]{tseng-etal-2015-introduction}
Yuen-Hsien Tseng, Lung-Hao Lee, Li-Ping Chang, and Hsin-Hsi Chen. 2015.
\newblock \href {https://doi.org/10.18653/v1/W15-3106} {Introduction to
  {SIGHAN} 2015 bake-off for {C}hinese spelling check}.
\newblock In \emph{Proceedings of the Eighth {SIGHAN} Workshop on {C}hinese
  Language Processing}, pages 32--37, Beijing, China. Association for
  Computational Linguistics.

\bibitem[{Wang et~al.(2020)Wang, Pruksachatkun, Nangia, Singh, Michael, Hill,
  Levy, and Bowman}]{wang2020superglue}
Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael,
  Felix Hill, Omer Levy, and Samuel~R. Bowman. 2020.
\newblock \href {http://arxiv.org/abs/1905.00537} {Super{GLUE}: A stickier
  benchmark for general-purpose language understanding systems}.

\bibitem[{Wang et~al.(2019{\natexlab{a}})Wang, Singh, Michael, Hill, Levy, and
  Bowman}]{wang2019glue}
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and
  Samuel~R. Bowman. 2019{\natexlab{a}}.
\newblock \href {http://arxiv.org/abs/1804.07461} {{GLUE}: A multi-task
  benchmark and analysis platform for natural language understanding}.

\bibitem[{Wang et~al.(2019{\natexlab{b}})Wang, Tay, and
  Zhong}]{wang-etal-2019-confusionset}
Dingmin Wang, Yi~Tay, and Li~Zhong. 2019{\natexlab{b}}.
\newblock \href {https://doi.org/10.18653/v1/P19-1578} {Confusionset-guided
  pointer networks for {C}hinese spelling check}.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association
  for Computational Linguistics}, pages 5780--5785, Florence, Italy.
  Association for Computational Linguistics.

\bibitem[{Wang(2023)}]{wang2023mediagpt}
Zhonghao Wang. 2023.
\newblock \href {http://arxiv.org/abs/2307.10930} {Media{GPT} : A large
  language model target {C}hinese media}.

\bibitem[{Williams(2002)}]{williams2002trusting}
Caroline Williams. 2002.
\newblock \href {https://doi.org/10.1108/jd.2002.58.1.136.14} {Trusting
  records: legal, historical and diplomatic perspectives}.
\newblock \emph{Journal of Documentation}, 58(1):136--139.

\bibitem[{Wu et~al.(2023{\natexlab{a}})Wu, Zhang, Zhang, Wang, and
  Xie}]{Wu2023PMCLLaMAFF}
Chaoyi Wu, Xiaoman Zhang, Ya~Zhang, Yanfeng Wang, and Weidi Xie.
  2023{\natexlab{a}}.
\newblock Pmc-llama: Further finetuning llama on medical papers.
\newblock \emph{ArXiv}, abs/2304.14454.

\bibitem[{Wu et~al.(2023{\natexlab{b}})Wu, Irsoy, Lu, Dabravolski, Dredze,
  Gehrmann, Kambadur, Rosenberg, and Mann}]{Wu2023BloombergGPTAL}
Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian
  Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann.
  2023{\natexlab{b}}.
\newblock Bloomberg{GPT}: A large language model for finance.
\newblock \emph{ArXiv}, abs/2303.17564.

\bibitem[{Xiao et~al.(2021)Xiao, Xu, and Liu}]{xiao2021security}
Qiuhui Xiao, Xiaotong Xu, and Panpan Liu. 2021.
\newblock \href {https://doi.org/10.1108/LHT-04-2019-0088} {Security status of
  electronic records preservation in central {C}hina: The survey results of 34
  archives in wuhan city}.
\newblock \emph{Library Hi Tech}, 39(1):22--36.

\bibitem[{Xie et~al.(2023)Xie, Han, Zhang, Lai, Peng, Lopez-Lira, and
  Huang}]{Xie2023PIXIUAL}
Qianqian Xie, Weiguang Han, Xiao Zhang, Yanzhao Lai, Min Peng, Alejandro
  Lopez-Lira, and Jimin Huang. 2023.
\newblock {PIXIU}: A large language model, instruction data and evaluation
  benchmark for finance.
\newblock \emph{ArXiv}, abs/2306.05443.

\bibitem[{Xiong et~al.(2023)Xiong, Wang, Zhu, Zhao, Liu, Huang, Wang, and
  Shen}]{Xiong2023DoctorGLMFY}
Honglin Xiong, Sheng Wang, Yitao Zhu, Zihao Zhao, Yuxiao Liu, Linlin Huang,
  Qian Wang, and Dinggang Shen. 2023.
\newblock Doctor{GLM}: Fine-tuning your {C}hinese doctor is not a herculean
  task.
\newblock \emph{ArXiv}, abs/2304.01097.

\bibitem[{Zeng et~al.(2022)Zeng, Liu, Du, Wang, Lai, Ding, Yang, Xu, Zheng,
  Xia, Tam, Ma, Xue, Zhai, Chen, Zhang, Dong, and Tang}]{Zeng2022GLM130BAO}
Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi
  Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng~Lam Tam, Zixuan Ma, Yufei Xue,
  Jidong Zhai, Wenguang Chen, P.~Zhang, Yuxiao Dong, and Jie Tang. 2022.
\newblock {GLM-130B}: An open bilingual pre-trained model.
\newblock \emph{ArXiv}, abs/2210.02414.

\bibitem[{Zhang et~al.(2021)Zhang, Zhang, Chen, Guo, Hua, Wang, and
  Zhou}]{zhang2021mengzi}
Zhuosheng Zhang, Hanqing Zhang, Keming Chen, Yuhang Guo, Jingyun Hua, Yulong
  Wang, and Ming Zhou. 2021.
\newblock Mengzi: Towards lightweight yet ingenious pre-trained models for
  {C}hinese.
\newblock \emph{arXiv preprint arXiv:2110.06696}.

\bibitem[{Zhao et~al.(2023)Zhao, Zhou, Li, Tang, Wang, Hou, Min, Zhang, Zhang,
  Dong, Du, Yang, Chen, Chen, Jiang, Ren, Li, Tang, Liu, Liu, Nie, and rong
  Wen}]{Zhao2023ASO}
Wayne~Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,
  Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang,
  Yushuo Chen, Z.~Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang
  Liu, Peiyu Liu, Jianyun Nie, and Ji~rong Wen. 2023.
\newblock A survey of large language models.
\newblock \emph{ArXiv}, abs/2303.18223.

\end{thebibliography}
