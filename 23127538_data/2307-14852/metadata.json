{
  "title": "ArcGPT: A Large Language Model Tailored for Real-world Archival Applications",
  "authors": [
    "Shitou Zhang",
    "Jingrui Hou",
    "Siyuan Peng",
    "Zuchao Li",
    "Qibiao Hu",
    "Ping Wang"
  ],
  "submission_date": "2023-07-27T13:31:45+00:00",
  "revised_dates": [],
  "abstract": "Archives play a crucial role in preserving information and knowledge, and the exponential growth of such data necessitates efficient and automated tools for managing and utilizing archive information resources. Archival applications involve managing massive data that are challenging to process and analyze. Although LLMs have made remarkable progress in diverse domains, there are no publicly available archives tailored LLM. Addressing this gap, we introduce ArcGPT, to our knowledge, the first general-purpose LLM tailored to the archival field. To enhance model performance on real-world archival tasks, ArcGPT has been pre-trained on massive and extensive archival domain data. Alongside ArcGPT, we release AMBLE, a benchmark comprising four real-world archival tasks. Evaluation on AMBLE shows that ArcGPT outperforms existing state-of-the-art models, marking a substantial step forward in effective archival data management. Ultimately, ArcGPT aims to better serve the archival community, aiding archivists in their crucial role of preserving and harnessing our collective information and knowledge.",
  "categories": [
    "cs.CL"
  ],
  "primary_category": "cs.CL",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.14852",
  "pdf_url": "https://arxiv.org/pdf/2307.14852v1",
  "comment": null,
  "num_versions": null,
  "size_before_bytes": 42398224,
  "size_after_bytes": 42273761
}