% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{radford2018improving}
A.~Radford, K.~Narasimhan, T.~Salimans, I.~Sutskever \emph{et~al.}, ``Improving
  language understanding by generative pre-training,'' 2018.

\bibitem{devlin2018bert}
J.~Devlin, M.-W. Chang, K.~Lee, and K.~Toutanova, ``Bert: Pre-training of deep
  bidirectional transformers for language understanding,'' \emph{arXiv preprint
  arXiv:1810.04805}, 2018.

\bibitem{ranathunga2023neural}
S.~Ranathunga, E.-S.~A. Lee, M.~Prifti~Skenduli, R.~Shekhar, M.~Alam, and
  R.~Kaur, ``Neural machine translation for low-resource languages: A survey,''
  \emph{ACM Computing Surveys}, vol.~55, no.~11, pp. 1--37, 2023.

\bibitem{bahdanau2014neural}
D.~Bahdanau, K.~Cho, and Y.~Bengio, ``Neural machine translation by jointly
  learning to align and translate,'' \emph{arXiv preprint arXiv:1409.0473},
  2014.

\bibitem{ccayir2021effect}
A.~N. {\c{C}}ay{\i}r and T.~S. Navruz, ``Effect of dataset size on deep
  learning in voice recognition,'' in \emph{2021 3rd International Congress on
  Human-Computer Interaction, Optimization and Robotic Applications
  (HORA)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2021, pp. 1--5.

\bibitem{ali2021voice}
A.~T. Ali, H.~S. Abdullah, and M.~N. Fadhil, ``Voice recognition system using
  machine learning techniques,'' \emph{Materials Today: Proceedings}, pp. 1--7,
  2021.

\bibitem{el2021automatic}
W.~S. El-Kassas, C.~R. Salama, A.~A. Rafea, and H.~K. Mohamed, ``Automatic text
  summarization: A comprehensive survey,'' \emph{Expert systems with
  applications}, vol. 165, p. 113679, 2021.

\bibitem{liu2019text}
Y.~Liu and M.~Lapata, ``Text summarization with pretrained encoders,''
  \emph{arXiv preprint arXiv:1908.08345}, 2019.

\bibitem{radford2019language}
A.~Radford, J.~Wu, R.~Child, D.~Luan, D.~Amodei, I.~Sutskever \emph{et~al.},
  ``Language models are unsupervised multitask learners,'' \emph{OpenAI blog},
  vol.~1, no.~8, p.~9, 2019.

\bibitem{brown2020language}
T.~Brown, B.~Mann, N.~Ryder, M.~Subbiah, J.~D. Kaplan, P.~Dhariwal,
  A.~Neelakantan, P.~Shyam, G.~Sastry, A.~Askell \emph{et~al.}, ``Language
  models are few-shot learners,'' \emph{Advances in neural information
  processing systems}, vol.~33, pp. 1877--1901, 2020.

\bibitem{roberts2020much}
A.~Roberts, C.~Raffel, and N.~Shazeer, ``How much knowledge can you pack into
  the parameters of a language model?'' \emph{arXiv preprint arXiv:2002.08910},
  2020.

\bibitem{chen2021evaluating}
M.~Chen, J.~Tworek, H.~Jun, Q.~Yuan, H.~P. d.~O. Pinto, J.~Kaplan, H.~Edwards,
  Y.~Burda, N.~Joseph, G.~Brockman \emph{et~al.}, ``Evaluating large language
  models trained on code,'' \emph{arXiv preprint arXiv:2107.03374}, 2021.

\bibitem{ouyang2022training}
L.~Ouyang, J.~Wu, X.~Jiang, D.~Almeida, C.~L. Wainwright, P.~Mishkin, C.~Zhang,
  S.~Agarwal, K.~Slama, A.~Ray \emph{et~al.}, ``Training language models to
  follow instructions with human feedback,'' \emph{arXiv preprint
  arXiv:2203.02155}, 2022.

\bibitem{fu2022gptroadmap}
\BIBentryALTinterwordspacing
H.~Fu, Yao;~Peng and T.~Khot, ``How does gpt obtain its ability? tracing
  emergent abilities of language models to their sources,'' \emph{Yao Fu’s
  Notion}, Dec 2022. [Online]. Available:
  \url{https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1}
\BIBentrySTDinterwordspacing

\bibitem{openai2023gpt4}
OpenAI, ``Gpt-4 technical report,'' 2023.

\bibitem{goldstein2023generative}
J.~A. Goldstein, G.~Sastry, M.~Musser, R.~DiResta, M.~Gentzel, and K.~Sedova,
  ``Generative language models and automated influence operations: Emerging
  threats and potential mitigations,'' \emph{arXiv preprint arXiv:2301.04246},
  2023.

\bibitem{taecharungroj2023can}
V.~Taecharungroj, ``“what can chatgpt do?” analyzing early reactions to the
  innovative ai chatbot on twitter,'' \emph{Big Data and Cognitive Computing},
  vol.~7, no.~1, p.~35, 2023.

\bibitem{kreps2023potential}
S.~Kreps and D.~L. Kriner, ``The potential impact of emerging technologies on
  democratic representation: Evidence from a field experiment,'' \emph{New
  Media and Society, Forthcoming}, 2023.

\bibitem{voelkelartificial}
J.~G. Voelkel, R.~Willer \emph{et~al.}, ``Artificial intelligence can persuade
  humans on political issues.''

\bibitem{lu2023bounding}
A.~Lu, H.~Zhang, Y.~Zhang, X.~Wang, and D.~Yang, ``Bounding the capabilities of
  large language models in open text generation with prompt constraints,''
  \emph{arXiv preprint arXiv:2302.09185}, 2023.

\bibitem{kaloudi2020ai}
N.~Kaloudi and J.~Li, ``The ai-based cyber threat landscape: A survey,''
  \emph{ACM Computing Surveys (CSUR)}, vol.~53, no.~1, pp. 1--34, 2020.

\bibitem{huang2020metapoison}
W.~R. Huang, J.~Geiping, L.~Fowl, G.~Taylor, and T.~Goldstein, ``Metapoison:
  Practical general-purpose clean-label data poisoning,'' \emph{Advances in
  Neural Information Processing Systems}, vol.~33, pp. 12\,080--12\,091, 2020.

\bibitem{kairouz2021advances}
P.~Kairouz, H.~B. McMahan, B.~Avent, A.~Bellet, M.~Bennis, A.~N. Bhagoji,
  K.~Bonawitz, Z.~Charles, G.~Cormode, R.~Cummings \emph{et~al.}, ``Advances
  and open problems in federated learning,'' \emph{Foundations and
  Trends{\textregistered} in Machine Learning}, vol.~14, no. 1--2, pp. 1--210,
  2021.

\bibitem{issa2023blockchain}
W.~Issa, N.~Moustafa, B.~Turnbull, N.~Sohrabi, and Z.~Tari, ``Blockchain-based
  federated learning for securing internet of things: A comprehensive survey,''
  \emph{ACM Computing Surveys}, vol.~55, no.~9, pp. 1--43, 2023.

\bibitem{wu2022puma}
G.~Wu, M.~Hashemi, and C.~Srinivasa, ``Puma: Performance unchanged model
  augmentation for training data removal,'' in \emph{Proceedings of the AAAI
  Conference on Artificial Intelligence}, vol.~36, no.~8, 2022, pp. 8675--8682.

\bibitem{cao2018efficient}
Y.~Cao, A.~F. Yu, A.~Aday, E.~Stahl, J.~Merwine, and J.~Yang, ``Efficient
  repair of polluted machine learning systems via causal unlearning,'' in
  \emph{Proceedings of the 2018 on Asia Conference on Computer and
  Communications Security}, 2018, pp. 735--747.

\bibitem{biggio2012poisoning}
B.~Biggio, B.~Nelson, and P.~Laskov, ``Poisoning attacks against support vector
  machines,'' \emph{arXiv preprint arXiv:1206.6389}, 2012.

\bibitem{saha2020hidden}
A.~Saha, A.~Subramanya, and H.~Pirsiavash, ``Hidden trigger backdoor attacks,''
  in \emph{Proceedings of the AAAI conference on artificial intelligence},
  vol.~34, no.~07, 2020, pp. 11\,957--11\,965.

\bibitem{rahman2018membership}
M.~A. Rahman, T.~Rahman, R.~Lagani{\`e}re, N.~Mohammed, and Y.~Wang,
  ``Membership inference attack against differentially private deep learning
  model.'' \emph{Trans. Data Priv.}, vol.~11, no.~1, pp. 61--79, 2018.

\bibitem{slaney1994auditory}
M.~Slaney, D.~Naar, and R.~Lyon, ``Auditory model inversion for sound
  separation,'' in \emph{Proceedings of ICASSP'94. IEEE International
  Conference on Acoustics, Speech and Signal Processing}, vol.~2.\hskip 1em
  plus 0.5em minus 0.4em\relax IEEE, 1994, pp. II--77.

\bibitem{lahoti2019ifair}
P.~Lahoti, K.~P. Gummadi, and G.~Weikum, ``ifair: Learning individually fair
  data representations for algorithmic decision making,'' in \emph{2019 ieee
  35th international conference on data engineering (icde)}.\hskip 1em plus
  0.5em minus 0.4em\relax IEEE, 2019, pp. 1334--1345.

\bibitem{borji2023categorical}
A.~Borji, ``A categorical archive of chatgpt failures,'' \emph{arXiv preprint
  arXiv:2302.03494}, 2023.

\bibitem{guo2023close}
B.~Guo, X.~Zhang, Z.~Wang, M.~Jiang, J.~Nie, Y.~Ding, J.~Yue, and Y.~Wu, ``How
  close is chatgpt to human experts? comparison corpus, evaluation, and
  detection,'' \emph{arXiv preprint arXiv:2301.07597}, 2023.

\bibitem{pu2022deepfake}
J.~Pu, Z.~Sarwar, S.~M. Abdullah, A.~Rehman, Y.~Kim, P.~Bhattacharya, M.~Javed,
  and B.~Viswanath, ``Deepfake text detection: Limitations and opportunities,''
  \emph{arXiv preprint arXiv:2210.09421}, 2022.

\bibitem{goodfellow2020generative}
I.~Goodfellow, J.~Pouget-Abadie, M.~Mirza, B.~Xu, D.~Warde-Farley, S.~Ozair,
  A.~Courville, and Y.~Bengio, ``Generative adversarial networks,''
  \emph{Communications of the ACM}, vol.~63, no.~11, pp. 139--144, 2020.

\end{thebibliography}
