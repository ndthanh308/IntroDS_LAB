
\section{Methods}

% Figure environment removed

\autoref{fig:fig3} presents the proposed CCA architecture, which is mostly similar to that of ViT \cite{VIT} because it was designed to embed specific attributes through a detailed analysis of the ViT architecture. Hence, CCA is easily applicable under the ViT architecture. Moreover, as described in \autoref{sec:eval}, it yields excellent performance. The proposed architectures comprise self-attention and CCA modules. The following sections explain these networks.
% 번역가: Hence, CCAN is easily applicable under the ViT architecture. 이부분은 ViT 기반 방식으로 설계된 모델이므로 제거가능한 문장이라고 코멘트함
% 번역가: Moreover, as described in \autoref{sec:eval}, it yields excellent performance. 이부분은 성능에 대한 내용으로 method 섹션에서 설명할 필요가 없어보인다고 코멘트함
% \vspace*{-1pt}
\subsection{Self Attention Networks}
\label{sec:ssn}
The self-attention module learns a common representation containing the information necessary for multi-space embedding. The self-attention modules are nearly identical to ViT \cite{VIT}. ViT divides the image into specific patch sizes and converts it into continuous patch tokens (\patch). Here, classification tokens (\cls) \cite{bert} are added to the input sequence. As self-attention in ViT is position-independent, position embeddings are added to each patch token for vision applications requiring position information. All tokens of ViT are forwarded through a stacked transformer encoder and used for classification using \cls of the last layer. The transformer encoder consists of feed-forward (FFN) and multi-headed self-attention (MSA) blocks in a continuous chain. FFN has two multi-layer perceptrons; layer normalization (LN) is applied at the beginning of the block, followed by residual shortcuts. The following equation is for the $l$-th transformer encoder. 

\begin{equation} 
\begin{split}
\label{eq:vit}
    \mathbf{x}_{0} & = \left[ \mathbf{x}_{\cls} ; \mathbf{x}_{\patch} \right] + \mathbf{x}_{\pos} \\
    \mathbf{x^\prime}_l & = \mathbf{x}_{l-1} + \mathtt{MSA}(\mathtt{LN}(\mathbf{x}_{l-1})) \\
    \mathbf{x}_l & = \mathbf{x^\prime}_l + \mathtt{FFN}(\mathtt{LN}(\mathbf{x^\prime}_l))
 \end{split}
\end{equation}

where $\mathbf{x}_{0}$ is inital ViT input. $\mathbf{x}_{\cls} \in \mathbb{R}^{1\times D}$, $\mathbf{x}_{\patch} \in \mathbb{R}^{N\times D}$ and $\mathbf{x}_{\pos} \in \mathbb{R}^{(1+N)\times D}$ are the classification, patch, and positional embedding, respectively. The output of the $L-1$ repeated encoder is used as input to the CCA module, as explained in \autoref{sec:cca}

% Figure environment removed

\subsection{Conditional Cross Attention Network}
\label{sec:cca}
In this study, the transformer must fuse the concept of attributes and mapped condition information for the network to learn. Drawing inspiration from Vaswani \etal \cite{Transformer_NIPS2017_Vaswani}, we propose CCA to enable learning in line with the transformer's self-attention mechanism. CCA uses a common representation obtained from the self-attention module and cross-attention of the mask according to the given condition to learn nonlinear embeddings that effectively express the semantic similarity based on the condition. Though existing techniques, such as CSN \cite{veit2017} and ASEN \cite{ma2020fine}, have applied condition information to the embedding, these methods are CNN-based rather than transformer-based. 
\vspace*{-10pt}
%
\paragraph{Conditional Token Embedding} 
% The concept of network switch based on the condition is required to embed multiple attributes under a single network
Some network switch based on the condition is needed to embed multiple attributes under a single network. 
In other words, attributes must be learned according to the condition. This study proposes two conditional token embedding methods, as shown in  \autoref{fig:fig3}.

First, Condition ${c}$ is converted into a one-hot vector form, after which conditional token embedding is performed, similar to that used in multi-modal studies such as DeViSE \cite{DeViSE}, which learns text and image information having the same meaning using heterogeneous data in the same space, as follows:

\begin{equation}
\label{eq:Conditionalembedding1}
   {\mathbf{q}_\mathbf{c}} = \mathtt{FC(onehot({c}))} %\; \cdot \Uparrow
   %{{q}_{c}} = \mathtt{FC(onehot(\mathbf{c}))} %\; \cdot \Uparrow
\end{equation}

where $\mathbf{q}_\mathbf{c} \in \mathbb{R}^{D \times 1 }$, ${c}$ is condition of size $K$.

Second is the CSN \cite{veit2017} technique, presented in \autoref{fig:fig2} (a). To express $K$ conditions, CSN applies a mask $\in \mathbb{R}^{K\times D}$ to one of the features and uses element-wise multiplication to fuse and embed two CNN features $\in \mathbb{R}^{D}$. This study uses this step only for conditional feature embedding without fusing the features. To this end, we initialize the mask $ \in \mathbb{R}^{K \times D}$ for all attributes. This mask can be expressed as a learnable lookup table. The conditional token embedding using the mask is expressed as follows:
% 번역가: 해당 second 단락이 이해하기 어려워 우리가 의도한대로 쓰였는지 확인이 꼭 필요하다고 합니다. 그렇지 않다면 좀 더 명확한 설명을 해주고, 여러 문장으로 나누는것을 고려해달라고 코멘트
\begin{equation}
\begin{array}{l}
{\mathbf{q}_\mathbf{c}} = \mathtt{FC}(\phi(\mathbf{M_{\theta}}{[c, :]}))% \; \cdot \Uparrow 
\label{eq:Conditionalembedding2}
\end{array} 
\end{equation}

where $\phi$ refers to ReLU, the activation function. Accordingly, the dimensions must be the same as the feature to apply self-attention. The result of $\mathbf{FC}$ in \autoref{eq:Conditionalembedding1} and \autoref{eq:Conditionalembedding2} is embedded while matching the dimension of $C$.
% 번역가: Accordingly, the dimensions must be the same 이부분에서 무엇이 동일해야하는지에 대한 차원이 명확하지 않고 이걸 명확히 말해달라고 코멘트

Finally, the result of both equations must equal the dimensions of the token embedding in \autoref{sec:ssn}. Therefore, the same vector $\mathbf{q}_\mathbf{c} \in \mathbb{R}^{D \times 1 }$ is repeated times to expand the result of both equations as follows:
% 번역가: repeated times 가 몇번인지 명시해달라고 코멘트 
% (N+1)만큼 동일한 벡터를 반복한다라고 썼는데 내용을 이해 못한것같아요..

\begin{equation}
\begin{array}{l}
{Q_c} = [\mathbf{q}_\mathbf{c} ; \mathbf{q}_\mathbf{c} ; ... ; \mathbf{q}_\mathbf{c}]
\label{eq:Conditionalembedding3}
\end{array} 
\end{equation}

\vspace*{-10pt}
%

\paragraph{Conditional Cross Attention}
Finally, the transformer architecture must effectively fuse the conditional token embedding vector $Q_c$, for which we use CCA.
The MSA process in \autoref{eq:vit} uses a self-attention mechanism with the vector query ($Q$), key ($K$), and Value ($V$) as input and is expressed as follows:
 
\begin{equation}
\label{eq:selfattention}
   \mathtt{Attention}{(Q_i, K_i, V_i)} = \mathtt{softmax}{(\frac{Q_{i}K_{i}^\top}{\sqrt{d}})V_i}
\end{equation}

These vectors generated from image $i$ can be expressed using ${K_{i}, Q_i, V_i} \in \mathbb{R}^{N \times D}$, consistent with the tokens mentioned above. The inner product of ${Q}$ and ${K}$ is calculated, which scales and normalizes with the softmax function to obtain weight N.

In contrast, though CCA is nearly identical to self-attention, Query, $Q_c$ in \autoref{eq:Conditionalembedding3}, is generated to have condition information. $K_i$ and $V_i$, which are the same as above, are input, and the cross-attention mechanism is applied to construct the final CCA as follows. 

\begin{equation}
\label{eq:crossattention}
   \mathtt{Attention}{(Q_c, K_i, V_i)} = \mathtt{softmax}{(\frac{Q_{c}K_{i}^\top}{\sqrt{d}})V_i}
\end{equation}


The cross-attention mechanism is nearly identical to general self-attention; except for the part of \autoref{eq:crossattention}, it is the same as \autoref{eq:vit}. 
% 번역가: eq6 의 어느부분을 제외하고 eq1과 동일하다는건지 이해가 어렵다 좀 더 명확한 설명을하고 여러 문장으로 나누는것을 고려해보는것을 코멘트
The output is the embedding values of \cls and \patch. In our proposed CCA, only \cls is used for the loss calculation. For the final output, FC and l2 normalization are applied to the embedding feature $x_{\cls} \in \mathbb{R}^{D}$ of \cls as follows:

\begin{equation}
\label{eq:finalcls}
    f^{final}= \mathtt{l{2}}(\mathtt{FC}{(x_{\cls})})
\end{equation}


Self-attention, explained in \autoref{sec:ssn}, executes the transformer encoder until step $1 \sim (L-1) $, while CCAN, explained in \autoref{sec:cca}, applies only to the final step $L$. In other words, during inference, as shown in \autoref{fig:fig1}, if step $1 \sim (L-1)$ is executed only once and the condition in the final step $L$ is changed and repeated, then several specific features can be obtained under various conditions. \autoref{fig:fig6} shows related experimental results. Eight attributes in the FashionAI dataset are attended in regions matched to each attribute. In addition, step $1 \sim (L-1) $ in the network model can apply the existing ViT-based pre-trained model without modification for learning.


\subsection{Triplet Loss with Conditions}
\label{sec:triplet}
We use triplet loss for learning specific attributes, different from the previous general triplet loss in that a conditioned triplet must be constructed. If a label with image $I$ and condition $c$ exists, then the Pair can be denoted as $(I, L_{c})$. When expanded to triplets, this is expressed as follows.

\begin{equation}
\label{eq:loss1}
    \mathcal{T}= \{((I^a,L^{a}_c), (I^+,L^{+}_c), (I^-,L^{-}_c)| c)\}
\end{equation}

where $a$ indicates the anchor, $+$ means that it has the same class in the same condition as the anchor, and $-$ means that it does not have the same class. Using negative samples with the same condition in triplet learning can be interpreted as a hard negative mining strategy. As shown in \cite{xuan2020hard}, randomly selected negatives are easily distinguished from anchors, enabling the model to learn only coarse features. However, for negative samples with the same condition, the model must distinguish more fine-grained differences. Hence, informative negative samples more suitable for specific-attributes learning are provided. The equation of triplet loss $\mathcal{L}$ is as follows.

\begin{equation}
\label{eq:loss2}
\begin{multlined}
\mathcal{L}(I^a, I^+, I^- , c) = \\ \max\{0, \textsc{Dist}(I^a,I^+|c) - \textsc{Dist}(I^a,I^-|c) + m\}
\end{multlined}
\end{equation}

$m$ uses a predefined margin, and $\textsc{Dist()}$ refers to cosine distance. In the Appendix, we present Algorithm \autoref{alg_train}, which outlines the pseudo-code of our proposed method. 

% \algrenewcommand{\algorithmiccomment}[1]{$\triangleright$ #1}
% \begin{algorithm}[t!]
% %\footnotesize
% \small
% \caption{Pseudo-Code for CCA Training}
% \label{alg_train}
% \begin{algorithmic}[1]
% \State \textbf{input:} 
% Image $\mathcal{I}$, Condition $c$ \\
% batch  $\mathcal{B}$, training epochs $K$, triplet set $\mathcal{T}$ \\
% Self Attention Block $SA$, Conditional Cross Attention $CCA$
% \For{$epoch= 1,...,K $}
%     \For{$\mathcal{B}=1,...,M \in \mathcal{T} $}
%         \State $Triplet(\mathcal{A}_c, \mathcal{P}_c, \mathcal{N}_{c})\gets \mathcal{B}$
%         \State $\mathcal{I}, c \gets \mathcal{A}_c, \mathcal{P}_c, \mathcal{N}_{c}$
%         %\State \# Patch, Position Encoding etc 
%         \State $\mathcal{Q}_i, \mathcal{K}_i, \mathcal{V}_i \gets Token\_{Embedding}(\mathcal{I})$
%         \For{$l=1,...,(\mathcal{L}-1) $}  
%             \State $ \mathcal{Q}_i, \mathcal{K}_i, \mathcal{V}_i \gets SA (\mathcal{Q}_i, \mathcal{K}_i, \mathcal{V}_i)$               
%         \EndFor    
%         \State \textbf{Last iteration} $ l = \mathcal{L} $ \textbf{do} 
%         \State \hspace{\algorithmicindent} $ \mathcal{Q}_c \gets Conditional\_Token\_Embedding(c) $
%         \State \hspace{\algorithmicindent} $ \cls \gets CCA (\mathcal{Q}_c, \mathcal{K}_i, \mathcal{V}_i)$
%         \State \hspace{\algorithmicindent} $ f \gets l2(FC(\cls))$
%         \State calculate $ f_a, f_p, f_n \gets Triplet(\mathcal{A}_c, \mathcal{P}_c, \mathcal{N}_{c})$
%         \State calculate triplet loss $\mathcal{L} (f_a, f_p, f_n | c)$
%         \State calculate gradients of $\nabla\mathcal{L}(\theta)$
%         \State $\theta \gets Adam(\nabla\mathcal{L}(\theta)$)
%     \EndFor
% \EndFor
% \end{algorithmic}
% \end{algorithm}