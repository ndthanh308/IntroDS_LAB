\subsection{Ablation Studies}
\label{sec:ablation}


\paragraph{SOTA models applied Transformer} 
The results of the existing CSN \cite{veit2017}, ASEN \cite{ma2020fine} models are obtained with the RestNet50 as the backbone. For a fair comparison, we apply the ViT backbone rather than CNN to these methods and present the experimental results. These models are indicated as TF-CSN and TF-ASEN, respectively. To apply this to CSN and ASEN, first, CSN must accept dimensions of size $\mathbb{R}^{D}$. Hence, it must be applied in \cls $ \in \mathbb{R}^{D}$. In contrast, ASEN must accept a CNN feature map of $\in \mathbb{R}^{W \times H \times D}$ dimensions. For ViT, it must be applied in \patch $\in \mathbb{R}^{N \times D}$, which can be applied because $N$ can be reshaped to ${W \times H}$. One peculiarity is that ASEN outperforms CSN based on CNN but not that based on ViT. Overall, our proposed CCA, with the same transformer base as TF-CSN and TF-ASEN, outperforms both models.

\vspace*{-10pt}
\paragraph{Consistent Performance}
We found that previous studies yielded different performance results for the datasets. For example, \autoref{tab:fashionAI}, CAMNet \cite{song2022} outperformed ASEN and CSN, whereas, in \autoref{tab:DARN} and \autoref{tab:DeepFashion}, there are no performance results. Similarly, in \autoref{tab:DARN}, M2Fashion \cite{Wan01} outperformed ASEN and CSN, whereas, in \autoref{tab:fashionAI} and \autoref{tab:DeepFashion}, there are no results. This suggests that the performance varies with the dataset. Accordingly, we applied the CAMNet study to the DARN dataset to reproduce it. In \autoref{tab:DARN}, the $\dagger$ symbol indicates our reproduced results. CAMNet model yielded lower performance than ASEN and CSN. Moreover, ASEN outperformed CSN based on CNN in the experimental results of TF-CSN and TF-ASEN when using the transformer. This is attributed to differences in learning according to the characteristics of each dataset. Thus, learning to form embeddings for objects with multiple attributes using a single network is very difficult. In contrast, our proposed CCA consistently yields high performance for all datasets. 
\vspace*{-10pt}
%
\paragraph{Type-1 \vs Type-2}
These results relate to \autoref{eq:Conditionalembedding1} and \autoref{eq:Conditionalembedding2} in \autoref{sec:cca}. \autoref{tab:fashionAI} presents the results for CCA (Type-1) and CCA (Type-2); CCA (Type-2) yielded +2.97\% higher performance than CCA (Type-1). In \autoref{tab:DARN} and \autoref{tab:DeepFashion}, CCA (Type-2) showed +1.31\% and +0.4\% higher performance, respectively. In \autoref{tab:zappos50k}, CCA (Type-1) yielded +0.42\% higher performance. CCA (Type-2) was slightly higher in the previous three benchmark sets, whereas CCA (Type-1) was slightly higher by 0.13\% in this dataset. However, CCA (Type-1) and CCAN (Type-2) outperformed all results of the previous studies and TF-CSN and TF-ASEN described above, achieving SOTA performance. 
