%%%%%%%%% ABSTRACT
\begin{abstract}
% Many studies on vision tasks have focused on effectively creating an embedding space (single-space embedding) for single-label prediction of an object within an image. However, most real objects have several specific attributes (e.g., shape, color, length), 
% each of these attributes is composed of various classes. 
% Thus, in order to apply the model in real-world scenarios, it is essential to possess the capability of distinguishing granular components of an object. Adopting the conventional approach to embed multiple specific attributes into a single network causes entanglement, in which fine-grained features of each attribute cannot be separately identified. 
% To address this problem, we propose a Conditional Cross-Attention Network that induces disentangled multi-space embeddings (or specific-attributes embeddings) for various specific attributes with only a single backbone. First, we employ a cross-attention mechanism to fuse and switch the information of condition (specific attributes) and demonstrate its effectiveness through a diverse visualization example. Second, we leverage the vision transformer for the first time to a fine-grained image retrieval task and present a simple but effective framework compared to existing methods. Unlike prior studies where the performance varied with the benchmark dataset, the proposed method achieved consistent state-of-the-art performance on the FashionAI, DARN, DeepFashion, and Zappos50K benchmark datasets.

Many studies in vision tasks have aimed to create effective embedding spaces for single-label object prediction within an image. However, in reality, most objects possess multiple specific attributes, such as shape, color, and length, with each attribute composed of various classes. To apply models in real-world scenarios, it is essential to be able to distinguish between the granular components of an object. Conventional approaches to embedding multiple specific attributes into a single network often result in entanglement, where fine-grained features of each attribute cannot be identified separately.
To address this problem, we propose a Conditional Cross-Attention Network that induces disentangled multi-space embeddings for various specific attributes with only a single backbone. Firstly, we employ a cross-attention mechanism to fuse and switch the information of conditions (specific attributes), and we demonstrate its effectiveness through a diverse visualization example. Secondly, we leverage the vision transformer for the first time to a fine-grained image retrieval task and present a simple yet effective framework compared to existing methods. Unlike previous studies where performance varied depending on the benchmark dataset, our proposed method achieved consistent state-of-the-art performance on the FashionAI, DARN, DeepFashion, and Zappos50K benchmark datasets.

\end{abstract}

% Please consider adding a few lines regarding the significance/implications of the research conducted to present a more complete abstract as per academic writing conventions.
% 연구의 중요성/의미에 대해 몇 줄 추가하는것을 고려해달라

% I would suggest adding 3–7 keywords as it is a general requirement for most journals. Please check the target journal guidelines. Keywords are salient points that highlight the key aspects of your study. They are used for indexing purposes and help in improving the discoverability of your article. Please avoid using abbreviations here.
% Consider using the following keywords: image retrieval; conditional cross-attention network; multi-space embedding; vision transformer.
% 대부분의 저널에서 키워드를 추가하므로 3~7개의 키워드를 추가할것을 제안한다. 지침을 참고해봐라
% 초록에 약어를 사용하지 말아라. 추천 키워드는 image retrieval, conditional cross-attention network, multi-space embedding, vision transformer 이다. 
