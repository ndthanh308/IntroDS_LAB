% Figure environment removed

\section{Introduction}

% Figure environment removed

ImageNet~\cite{imagenet21k} is a representative benchmark dataset to verify the visual feature learning effects of deep learning models in the vision domain. However, each image has only one label, which cannot fully explain the various features of real objects. For example, a car can be identified with various attributes such as category, color, and length, as in \autoref{fig:fig1}. As shown in \autoref{fig:fig1} (a), the general method of forming embeddings for objects’ various attributes involves constructing neural networks equal to the number of specific attributes, and creating multiple embeddings for vision tasks such as image classification \cite{ResNet_He_2016_CVPR, efficientnet_pmlr_tan_19, Hu01} and retrieval \cite{Kalantidis01, SCH01}. Unlike conventional methods, this study presents a technique that embeds various attributes into a single network. We refer to this technique as multi-space attribute-specific embedding \autoref{fig:fig1} (b).

Embedding space aims to encapsulate feature similarities by mapping similar features to close points and dissimilar ones to farther points. However, when the model attempts to learn multiple visual and semantic concepts simultaneously, the embedding space becomes complex, resulting in entanglement; thus, points corresponding to the same semantic concept can be mapped in different regions. Consequently, embedding multiple concepts in an image into a single network is very challenging. Although previous studies attempted to solve this problem using convolutional neural networks (CNNs) \cite{veit2017, ma2020fine, dong2021fine, SCH01}, they have required intricate frameworks, such as the incorporation of multiple attention modules or stages, in order to identify specific local regions that contain attribute information.
%the proposed vision transformer (ViT)-based conditional cross-attention network (CCA) is the first to solve this problem.

Recently, there has been an increase in research related to ViT \cite{VIT}, which outperforms existing CNN-based models in various vision tasks, such as image classification \cite{VIT}, retrieval \cite{dtop}, and detection \cite{detr}. In addition, research analyzing how ViT learns representations compared to CNN is underway \cite{raghu2021vision, park2022vision, naseer2021intriguing}. Raghu \etal \cite{raghu2021vision} demonstrated that the higher layers of ViT are superior in preserving spatial locality information, providing improved spatially discriminative representation than CNN. Some attributes of an object are more easily distinguished when focusing on specific local areas. So, we tailor the last layer of ViT to recognize specific attributes based on their spatial locality, which provides fine-grained information about a particular condition. \autoref{fig:fig2} summarizes the difference between existing CNN-based and proposed ViT-based methods. This study makes the following contributions:
\begin{enumerate}[itemsep=2pt, parsep=0pt, topsep=0pt]
    \item Entanglement occurs when embedding an object containing multiple attributes using a single network. The proposed CCA that applies a cross-attention mechanism can solve this problem by adequately fusing and switching between the different condition information (specific attributes) and images. 
        
    \item This is the first study to apply ViT to multi-space embedding-based image retrieval tasks. In addition, it is a simple and effective method that can be applied to the ViT architecture with only minor modification. Moreover, it improves memory efficiency by forming multi-space embeddings with only one ViT backbone rather than multiple backbones.
    
    \item Most prior studies showed good performance only on specific datasets. However, the proposed method yields consistently high performance on most datasets and effectively learns interpretable representations. Moreover, the proposed method achieved state-of-the-art (SOTA) performance on all relevant benchmark datasets compared to existing methods.
\end{enumerate}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% 번역해주신 분이 conditional cross-attention network 을 CCAN으로 번역해주심
% 일단 번역해주신 그대로 CCAN으로 번역해놓겠습니다. 