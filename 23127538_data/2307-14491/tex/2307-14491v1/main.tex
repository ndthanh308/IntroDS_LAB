%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command.

\documentclass[sigconf,screen]{acmart}
\usepackage{colortbl}


%% NOTE that a single column version may be required for 
%% submission and peer review. This can be done by changing
%% the \doucmentclass[...]{acmart} in this template to 
%% \documentclass[manuscript,screen]{acmart}
%% 
%% To ensure 100% compatibility, please check the white list of
%% approved LaTeX packages to be used with the Master Article Template at
%% https://www.acm.org/publications/taps/whitelist-of-latex-packages 
%% before creating your document. The white list page provides 
%% information on how to submit additional LaTeX packages for 
%% review and adoption.
%% Fonts used in the template cannot be substituted; margin 
%% adjustments are not allowed.
%%
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
% \setcopyright{acmcopyright}
% \copyrightyear{2018}
% \acmYear{2018}
% \acmDOI{XXXXXXX.XXXXXXX}


%% These commands are for a PROCEEDINGS abstract or paper.
% \acmConference[Conference acronym 'XX]{Make sure to enter the correct
%   conference title from your rights confirmation emai}{June 03--05,
%   2018}{Woodstock, NY}
%
%  Uncomment \acmBooktitle if th title of the proceedings is different
%  from ``Proceedings of ...''!
%
%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
% %  June 03--05, 2018, Woodstock, NY} 
% \acmPrice{15.00}
% \acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%\acmSubmissionID{2207}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
% The majority of ACM publications use numbered citations and
% references.  The command \citestyle{authoryear} switches to the
% "author year" style.
%
% If you are preparing content for an event
% sponsored by ACM SIGGRAPH, you must use the "author year" style of
% citations and references.
% Uncommenting
% the next command will enable that style.
%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Modality-Agnostic Audio-Visual Deepfake Detection}
%\subtitle{Audio-Visual Matched-Agnostic Deepfake Detection}
%\title[short title]{No Need for Deepfakes to Be Audio-Visual Mismatched: Audio-Visual Matched-Agnostic Deepfake Detection}
%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Cai Yu}
%\authornote{Both authors contributed equally to this research.}

%\orcid{1234-5678-9012}

\affiliation{%
  \institution{Institute of Information Engineering, Chinese Academy of Sciences}
  \city{Haidian}
  \state{Beijing}
  \country{China}
}
\email{ycai5@buffalo.edu}

\author{Peng Chen}
\affiliation{%
  \institution{RealAI}
  \city{Haidian}
  \state{Beijing}
  \country{China}}
\email{peng.chen@realai.ai}

\author{Jiahe Tian}
\affiliation{%
 \institution{Institute of Information Engineering, Chinese Academy of Sciences}
  \city{Haidian}
  \state{Beijing}
  \country{China}
}
\author{Jin Liu}
\affiliation{%
 \institution{Institute of Information Engineering, Chinese Academy of Sciences}
  \city{Haidian}
  \state{Beijing}
  \country{China}
}

\author{Jiao Dai}
\affiliation{%
 \institution{Institute of Information Engineering, Chinese Academy of Sciences}
  \city{Haidian}
  \state{Beijing}
  \country{China}}

\author{Xi Wang}
\affiliation{%
 \institution{Institute of Information Engineering, Chinese Academy of Sciences}
  \city{Haidian}
  \state{Beijing}
  \country{China}}
\author{Yesheng Chai}
\affiliation{%
 \institution{Institute of Information Engineering, Chinese Academy of Sciences}
  \city{Haidian}
  \state{Beijing}
  \country{China}}

\author{Jizhong Han}
\affiliation{%
 \institution{Institute of Information Engineering, Chinese Academy of Sciences}
  \city{Haidian}
  \state{Beijing}
  \country{China}}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Cai Yu, et al.}
\renewcommand\footnotetextcopyrightpermission[1]{}
\settopmatter{printacmref=False} %remove ACM reference format

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
As AI-generated content (AIGC) thrives, Deepfakes have expanded from single-modality falsification to cross-modal fake content creation, where either audio or visual components can be manipulated. While using two unimodal detectors can detect audio-visual deepfakes, cross-modal forgery clues could be overlooked. Existing multimodal deepfake detection methods typically establish correspondence between the audio and visual modalities for binary real/fake classification, and require the co-occurrence of both modalities. However, in real-world multi-modal applications, missing modality scenarios may occur where either modality is unavailable. In such cases, audio-visual detection methods are less practical than two independent unimodal methods. Consequently, the detector can not always obtain the number or type of manipulated modalities beforehand, necessitating a fake-modality-agnostic audio-visual detector. In this work, we propose a unified fake-modality-agnostic scenarios framework that enables the detection of multimodal deepfakes and handles missing modalities cases, no matter the manipulation hidden in audio, video, or even cross-modal forms. To enhance the modeling of cross-modal forgery clues, we choose audio-visual speech recognition (AVSR) as a preceding task, which effectively extracts speech correlation across modalities, which is difficult for deepfakes to reproduce. Additionally, we propose a dual-label detection approach that follows the structure of AVSR to support the independent detection of each modality. Extensive experiments show that our scheme not only outperforms other state-of-the-art binary detection methods across all three audio-visual datasets but also achieves satisfying performance on detection modality-agnostic audio/video fakes. Moreover, it even surpasses the joint use of two unimodal methods in the presence of missing modality cases.
\end{abstract}


\keywords{Deepfake Detection, Multimodal Learning}
\begin{teaserfigure}
  % Figure removed
  \caption{Modality-Agnostic Audio-Visual Deepfake Detection: The figure on the left shows three modality-agnostic detection scenarios that our detector supports. This detector enables independent detection of forgery in each modality and can handle scenarios where either modality is unavailable. The figure on the right illustrates the general scheme of our method, which leverages the speech correlation across modalities via the AVSR task to perform dual-label detection.}
  \Description{Enjoying the baseball game from the third-base
  seats. Ichiro Suzuki preparing to bat.}
  \label{fig:teaser}
\end{teaserfigure}

%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.

\received{20 February 2007}
\received[revised]{12 March 2009}
\received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\pagestyle{plain}
\maketitle

\section{Introduction}
% Deepfake is typically associated with the forgery of a person's visual facial content, such as expression and identity\cite{korshunova2017fast,thies2016face2face,siarohin2019first}. However, with the development of AIGC-related technologies, the connotation of deepfake has expanded to multiple modalities. Besides the recent advance in text-to-speech (TTS) and voice conversion (VC)~\cite{ping2018clarinet,kameoka2018non} lowering the barrier to synthesizing human speech, more recently, an emergence of talking face generation \cite{tan2021bioinspired,prajwal2020lip} can even achieve cross-modal manipulation by generating talking faces conditioned on given speeches. Deepfake has already covered forgery within individual modalities, and even extended to cross-modal manipulations. 

As AI-generated content (AIGC) technologies continue to advance, deepfakes are becoming increasingly complex and diverse in form. 
Beyond faking faces or voices related to a person's identity through audio deepfake~\cite{ping2018clarinet,kameoka2018non} or visual deepfake~\cite{korshunova2017fast,thies2016face2face,siarohin2019first} techniques within each modality, malicious attackers can combine these technologies to create multimodal forged content, where both audio and visuals can be fake. Moreover, cutting-edge deepfake methods like Wav2Lip ~\cite{prajwal2020lip}and VideoReTalking~\cite{cheng2022videoretalking} can even achieve cross-modal forgery by driving audio to generate precise lip-sync videos. Consequently, it is evident that deepfakes now encompass audio, visual, and even cross-modal forms. In real-life scenarios, deepfake content can appear in various forms across social media (audio, video, or audio-visual pairs), and a single detector may not always have access to the number or type of manipulated modalities beforehand. While employing two separate unimodal detectors, one for audio and the other for video, seem like a viable approach, the cross-modal forgery clues generated by techniques like Wav2Lip or VideoReTalking could be overlooked. Considering the significance of multimedia content security, it is crucial to develop a modality-agnostic detector that effectively defends against emerging multimodal deepfakes, irrespective of the number or type of modalities involved.


% % Figure environment removed

Nowadays, a few emerging audio-visual deepfake detection methods~\cite{chugh2020not, gu2020deepfake, mittal2020emotions} have achieved promising results. For example, EmoForen~\cite{mittal2020emotions} detects deepfakes via emotional inconsistency. These methods predict a single binary real/fake label for the entire video by evaluating the consistency of specific audio-visual attributes. Instead of focusing on discerning the authenticity of the content, their primary effort lies in assessing the degree of audio-visual match. More recent methods, such as JointAV~\cite{zhou2021joint} and AVoiD-DF~\cite{yang2023avoid}, attempt to leverage cross-attention to learn potential cross-modal dependence. However, more effective perceptual cues remain underutilized. Nevertheless, all the abovementioned methods necessitate the presence of audio and video in pairs. However, in the real world, deepfakes do not always appear in audio-visual pairs. There are detection scenarios where either the audio or visual component of the media is absent or unavailable. In such cases, when the number of modalities is agnostic, audio-visual detection methods are less practical than employing two independent unimodal detection methods. Despite this, audio-visual detection methods are crucial for modeling forgery clues across modalities. Consequently, a critical question arises: can there be an audio-visual detection framework that effectively utilizes cross-modal forgery clues while maintaining robust performance when faced with modality-agnostic scenarios?


% The reason for this is that the natural phonation of humans shares a consistent dynamic pattern between audio and visual modalities~\cite{campbell2008processing,shi2021learning}. In contrast, such high-level speech correlation is difficult for deepfakes to replicate due to their unnatural creation process. Therefore, we believe that AVSR is tailor-made for audio-visual deepfake detection and is also conducive to our dual-label framework.

In this work, we propose a unified framework for audio-visual deepfake detection that targets modality-agnostic scenarios.  First, to effectively capture cross-modal forgery clues, we observe that a consistent dynamic pattern should exist between audio and visual modalities for a natural phonation process, while such subtle speech-related correlation may be difficult to produce by deepfakes due to their unnatural creation process. Therefore, we introduce audio-visual speech recognition (AVSR) as a preceding task (as illustrated in Figure \ref{fig:teaser}). By pretraining on a large amount of real videos, the AVSR model learns to recognize spoken words from both audio and visual content, which is exceptionally suited for extracting natural speech correlations across modalities. By transferring this speech correlation into our framework, the modeling of cross-modal forgery clues can be enhanced. Customized for deepfake detection, a Modality Compensation Adapter is inserted into the AVSR model to alleviate modal competition. Second, to support the independent detection of each modality, we propose a dual-label detection approach where independent labels are assigned for each modality so that even if a modality is missing, the performance related to the other will not be affected. Based on this insight, A Dual-Label Classifier is designed, and established with a fake-composition detector to extract forgery intensity, and a temporal aggregation module to capture temporal inconsistency. The Dual-Label detection approach establishes a parallel structure between each modality, which aligns with the AVSR structure, thereby maximizing AVSR's advantages. The unified structure formed by the combination of AVSR and Dual-Label Classifier guarantees the ability to effectively combat modality-agnostic deepfakes.
The contributions of this work are summarized as follows:

\begin{itemize}
    \item We propose a unified framework capable of detecting audio-visual deepfakes, regardless of the number or type of modality involved. With a dual-label detection approach, this framework can not only specifically identify whether either the audio or visual modality (or both) has been manipulated, but also effectively handle scenarios where either modality is unavailable. Moreover, it outperforms using two separate unimodal models in combination.
    
    \item We introduce AVSR as a preceding task to extract high-level speech correlation across modalities. This practice has demonstrated the effectiveness of speech correlation for modeling cross-modal forgery clues. The leveraging of AVSR, together with Dual-Label Classfier leads to great performance on each single modality. We hope this approach provides a new perspective for future deepfake detection tasks to leverage speech correlation.
    
    %\item We propose a new perspective to detect multimodal deepfakes in a Dual-Label manner that can tell which modality has been manipulated. This can provide more concrete evidence in forensic applications. To address the secondary problem in our dual-label detection that visual modality dominates model decision, we propose to compensate audio modality and improve detection performance on both modalities.
    % \item We notice the issue that the model decision is often dominated by the visual modality. To address the issue, we propose to compensate for the audio modality which further improves detection performance on both modalities. 
   \item Extensive experiments are conducted to examine the impact of each designed component, including generalizability across datasets and robustness against missing modalities. The overall results demonstrate that our framework not only outperforms various state-of-the-art competitors in binary detection but also effectively detects modality-agnostic audio-visual deepfakes with strong performance.
\end{itemize}

% Figure environment removed

\section{Related Work}
There are extensive research works devoted to the detection of deepfakes in multimedia content. According to the modality involved, methods for deepfake detection can be broadly divided into three categories as follows:

\subsection{Visual Modality Deepfake Detection} 
These methods perform image-level or video-level real/fake prediction. Image-level methods focus on image or intra-frame artifacts. Face X-ray \cite{li2020face} and SBIs \cite{shiohara2022detecting} target forgery artifacts of blending boundaries by simulating deepfake images. PEL \cite{gu2022exploiting} and MaDD \cite{zhao2021multi} exploit fine-grained clues to detect face forgery. As to video-level methods, previous works leverage biometric signals to detect deepfake videos, such as facial action patterns \cite{agarwal2019protecting,sun2021improving} and eye blinking \cite{li2018eye}. Another branch of video-level works utilizes temporal modeling~\cite{gu2022delving,zhang2021detecting,gu2021spatiotemporal} for classification since manipulation in fake videos usually causes temporal inconsistency. Arguing that the above tangible clues may make models overfit to low-level manipulation-specific artifacts, LipForen \cite{haliassos2021lips} proposes to catch high-level semantic inconsistency via a lipreading network. In terms of harnessing high-level information, LipForen is most relevant to our work. However, unlike our proposal, the utility of audio has been neglected.

\subsection{Auditory Modality Deepfake Detection} 
Audio deepfake detection mainly counters the voice spoofing synthesized by TTS or VC techniques. There are some works \cite{martin2022vicomtech,xie2021siamese,wang2021investigating} applying pretrained speech representation models, such as wav2vec2 \cite{baevski2020wav2vec}, to detect spoofed audio. These works also render a high plausibility for us to introduce audio-visual speech recognition to detect multimodal deepfakes.

\subsection{Cross-Modal Deepfake Detection} 
There exist a few methods involving both visual and auditory modalities. However, most of them intrinsically measure the degree of audio-visual correspondence \cite{chugh2020not,mittal2020emotions,hosler2021deepfakes} or treat audio signals as reference \cite{haliassos2022leveraging} for video so that they can only assign a binary real/fake label for the entire video. For example, MDS
\cite{chugh2020not} measures the audio-visual dissonance in a video via the Modality Dissonance Score. AVCDetection \cite{gu2020deepfake} models synchronization patterns between two modalities. EmoForen~\cite{mittal2020emotions} and LSTM-based~\cite{hosler2021deepfakes} leverage emotion clues to discern fake videos. %Nevertheless, these approaches barely achieve synergistic detection of both visual and auditory manipulation. 
Nevertheless, these methods can only determine whether audio and visual components are consistent or not; they cannot specifically predict which modality has been manipulated. JointAV \cite{zhou2021joint} has made an attempt to handle the case that either one (or both) manipulation of the two modalities. However, they primarily exploit the attention mechanism to capture cross-modal dependencies of the audio-visual pairs, while the effect of high-level audio-visual relations such as speech correlation is underutilized. Moreover, none of the aforementioned methods have discussed how their multimodal detectors perform when encountering scenarios with missing modalities.




\section{Why is AVSR?}
In this section, we aim to provide a more comprehensive explanation of our motivation for introducing AVSR as the preceding task, focusing specifically on two aspects:
\subsection{Biological Perspective}
 Human perception of speech is involving both audition and vision. The cortical correlates of seen speech suggest that auditory processing of speech is affected by vision at both neurological and perceptual levels \cite{shi2021learning}. Biological studies have shown that for speech perception, the visual channel and auditory channel share similar dynamic patterns \cite{campbell2008processing}. Thus, we seek to leverage this kind of high-level audio-visual correlation, or, more precisely, audio-visual speech correlation \cite{almajai2007maximising}, to identify authentic/fake audio-visual media. The rationale is that the correlation between audio and visual speech features is an intrinsic characteristic of the natural phonation process, which can be absent in deepfake videos due to the artificial synthesis process. For example, the study of \cite{9151013} has discovered phoneme-viseme mismatching in deepfakes when "M", "B", and “P” is pronounced. As the feature quality of audio-visual correlation can be reflected by Audio-Visual Speech Recognition (AVSR)~\cite{almajai2007maximising}, we choose it as the preceding task in our framework to enhance the modeling of cross-modal forgery clues.

\subsection{Multimodal Applications Perspective}
In real-world multimodal applications, scenarios may arise where either the audio or visual component of the media is absent or unavailable~\cite{ma2021smil}. This also applies to deepfake detection. Consequently, for a unified audio-visual deepfake detector, a weak dependency pattern across modalities is more desirable, so that even if a specific modality is missing, the performance related to the other modality remains relatively unaffected. Consider the application scenario of AVSR, where the modalities are exactly weak-dependent. In comparison to multi-modal models, unimodal models such as Audio Speech Recognition (ASR)~\cite{baevski2020wav2vec} and Visual Speech Recognition (VSR)~\cite{7952625} can also achieve speech recognition. The existence of AVSR serves to complement the limitations of these two tasks, such as learning more robust speech representations by leveraging visual signals in noisy speech environments or using speech signals to assist lip-reading models when mouth shapes look similar. This gives the AVSR the ability that even in the absence of the other modality, this signal-to-text mapping within each modality can still be independently completed~\cite{afouras2018deep,shi2021learning}. This suggests that, compared to using separate ASR and VSR models, AVSR can not only model cross-modal speech-related information but also preserve the semantic information within each modality. This allows us to utilize AVSR for detecting deepfakes in scenarios where a modality is missing. As a result, AVSR is a natural choice for our modality-agnostic scenarios.
%（简练一点 jointAV 和AVOIDF 强dependence 不适合）


\section{Method}
Considering that, for a given video, the forgery possibility for each modality is independent, and either modality can be present or not, we argue that the traditional binary-classification approach is suboptimal and fails to defend against modality-agnostic scenarios. Ideally, we would like to synergistically identify the forged modality in the video, regardless of the number or type of modality. In light of this, we propose expanding the deepfake detection task by implementing a dual-label detection approach, \emph{i.e.}, addressing the following two questions in one task: (1) \emph{Is the face in the video forged?} (2) \emph{Is the voice in the video forged?}

% \begin{itemize}
%     \item \emph{Is the face in the video forged?}
%     \item \emph{Is the voice in the video forged?}
% \end{itemize}

The proposed framework consists two stages. At the first stage, an encoder-decoder architecture is pretrained on the task of AVSR to extract desired speech correlation. The second stage performs deepfake detection based on acquired speech knowledge. A detailed description of the proposed framework is illustrated in the following subsections.


\subsection{Stage 1: Audio-Visual Speech Recognition.}

Given a speech recognition dataset $D_{sr}=\left\{a_{t}^{i}, v_{t}^{i}, y_{t}^{i} \right\}_{i=1}^{N_{sr}}$ of size $N_{sr}$, where $t = 1,...,T$ denotes time step of input sequence length and $a_{t}^{i},v_{t}^{i}$ are the corresponding audio segment and video frame of each time step. For speech recognition task, each time step contains a ground-truth label (word or character) $y_{t}^{i}$. In the setting of this framework, the network produces frame-wise character probabilities $\tilde{y}_{t}^{i} \in\{1,...,C\}$, where $C$ is the character class.

Speech recognition is a sequence model, which comprises of two unimodal encoders $\theta_{a},\theta_{v}$, a joint decoder $\theta_{j}$ and a multi-class classifier $\theta_{c}$. It first extracts unimodal embedding through each encoder, referred to as, $e_{t}^a={\theta_{a}}(a_{t}),e_{t}^v={\theta_{v}}(v_{t})$. The final character prediction is obtained by passing the concatenation of two modal embeddings through decoder and the classifier: $\tilde{y}_{t} = \theta_c({\theta_{j}}\left(e_{t}^a, e_{t}^v\right))$. Following \cite{afouras2018deep}, the speech recognition network is trained with CTC loss \cite{graves2006connectionist}, $\frac{1}{N_{sr}} \sum_{i=1}^{N_{sr}} \mathcal{L}_{\mathrm{CTC}}\left(\tilde{y}_{t}^{i}, y_{t}^{i} ; \theta_{a}, \theta_{v}, \theta_{j},\theta_{c}\right)$.


\subsection{Stage 2: Dual-Label Deepfake Detection}


\subsubsection{Dual-Label Definition}
%Given a deepfake detection dataset $D_{df}=\left\{a_{t}^{j}, v_{t}^{j}, y_{m}^{j} \right\}_{j=1}^{N_{df}}$, where $y^{m} \in\{0,1\}$, $m \in\{a,v\}$,where $m$ is the modality (i.e. video $a$ or audio $v$),
 Given a deepfake detection dataset $D_{df}=\left\{a_{t}^{j}, v_{t}^{j}, y_{m}^{j} \right\}_{j=1}^{N_{df}}$, where $y_{m} \in\{0,1\}$ is a time-independent label for the entire video, and $m \in\{a,v\}$ denotes the modality dimension (\emph{i.e.}. audio $a$ or video $v$). Compared with a conventional binary prediction for the entire video, where $y^i=0$ for real or $y^i=1$ for fake, the dual-label is reflected in that each dimension of the label specifies the possibility of whether each modality has been forged. For example, $y^i=[0,1]$ denotes the audio is real and 
video is fake, and $y^i=[1,1]$ denotes that both audio and video are fake, or vice versa.

\subsubsection{Training Protocol}
The process of dual-label deepfake detection is illustrated in Figure \ref{fig:overview}a. 
The weights of encoders and the decoder are transferred from that ($\theta_{a}, \theta_{v}, \theta_{j}$) of speech recognition. A specially designed Modality Compensation Adapter  $\theta_{m}$ is inserted into the encoder-decoder and trained from scratch. To perform dual-label detection, we design a dual-label classifier at the end of the network. The whole detection framework is trained with binary cross-entropy loss, $\frac{1}{N_{df}} \sum_{j=1}^{N_{df}} \mathcal{L}_{\mathrm{BCE}}\left(\tilde{y}_{m}^{j}, y_{m}^{j} ; \theta_{a}, \theta_{v}, \theta_{m} ,  \theta_{j},\theta_{d}\right)$.

\subsubsection{Modality Compensation Adapter }
As demonstrated in the recent speech recognition literature, AVSR models can relate audio modality to lexical prediction more effortlessly than the visual modality \cite{shi2021learning,afouras2018deep,ma2021end}. This might cause the audio modality to dominate the model decisions. In terms of deepfake detection, we believe that the differences between reals and fakes may vary across modalities, which could lead to the model becoming overly reliant on a specific modality. To verify if there is a modality dominance issue in our task, we perform an exploratory baseline experiment of dual-label deepfake detection on randomly sampled videos from the DFDC dataset. As shown in Table~\ref{tab:oberving}, the performance of audio detection is significantly worse than that of video detection. Hence, it is reasonable to speculate that there appears to be some level of visual dominance in the model decisions. Although this can be alleviated by using more fake audio samples, we think this manner is palliative.

\begin{table}[hb]
\centering
\caption{An exploratory experiment on detection of each modality in dual-label manner.}
\begin{tabular}{l|l|l}
\hline
       & Recall & \multicolumn{1}{c}{F1} \\ \hline
Visual & 76.92 & 77.39                   \\ \hline
Audio  & 57.81 & 71.49                   \\ \hline
\end{tabular}

\label{tab:oberving}
\end{table}
To address above issue of modality dominance, we propose a Modality Compensation Adapter  (Figure \ref{fig:overview}b) to prevent over-reliance on a certain modality for the network. Here, we only formalize the compensation for audio modality below, and the same operation can be applied to the video counterpart.
\begin{equation} 
c_{t}=e_{t}^{a}+\theta_{c}\left(Norm_{l2}\left(e_{t}^{a}\right),Norm_{l2}\left(e_{t}^{v}\right)\right)
 \end{equation} 

 where $e_{t}^{v}$,$e_{t}^{a}$ are unimodal embeddings extracted by encoders and $c_{t}$ denotes the compensated embeddings. Here we compensate audio modality by concatenating $L2$ normalization of each unimodal embedding and passing them through a learnable residual $\theta_{c}$. This operation is utilized to compensate for the weaker modality. We adopt audio compensation for our final framework because, compared to video compensation, it yields more reasonable and preferable results that align with our speculation (as verified by the ablation study in Section 5.3). 

\subsubsection{Dual-Label Classifier}

To achieve the goal of synergistic detection of both visual and auditory manipulation, we design a dual-label classifier that is composed of a Fake Composition Detector (FCD) and a Temporal Aggregation Module (TAM). This design is implemented based on two considerations. First, in the speech recognition architecture, it performs frame-wise prediction for each time step, whereby only frame-level outputs are served. However, for deepfake detection, frame-level output is insufficient to capture temporal jitter between consecutive frames. Straightforward conduction of a temporal-pooling operation might average these subtle clues, thus an interactive video-level information aggregation is needed to capture temporal inconsistency. Second, the real/fake discrepancy varies from different modalities. To bridge the forgery intensity gap across modalities and provide a generalizable representation, we impose an extra supervision from the given label to denote the forgery intensity, \emph{i.e.}, the number of fake modalities in a given video. This practice is inspired by MlTr \cite{cheng2021mltr}, a successful work in multi-label classification because dual-label deepfake detection shares a similar concept with multi-label classification tasks in terms of the classification paradigm. In their work, they claim that this reinforced supervision can force the model to extract the common features of the same class and learn a more robust projection between features and labels, while our implementation aims at endowing a constraint of the same intensity to the audio or video manipulation in an explicit way. Specifically, despite their forgery intensity difference, in the case of only one modality has been manipulated in a certain video, whether it is visual, or audio, the intense label is assigned as 1. We believe that this design can serve a more generalizable representation, which is validated in the following experiment section.

Based on above consideration, we propose to implement these designs in our dual-label classifier via two transformer-like \cite{vaswani2017attention} modules. FCD serves for forgery intensity modeling and TAM for capturing temporal inconsistency. As shown in Figure \ref{fig:overview}c, leveraging the flexibility of transformer tokens, we prepend two randomly initialized tokens: a fake-aware token $\mathbf{{T o k e n}}_{\text {fake }}$, and a temporal class token $\mathbf{{T o k e n}}_{\text {time }} $, at the head of the sequence, respectively. Formally, re-formulating the compensated embeddings passed through the joint decoder as $\hat{c_{t}}$, we perform dual-label classification as follows:
\begin{equation}
\left.z_{\text {temp }}=FCD\left(\ { Concat(\mathbf{{T o k e n}}_{\text {fake }}; \hat{c_{1}} ; \hat{c_{2}} \ldots \hat{c_{T}}}\right)\right)\\
\end{equation}
\begin{equation}
\left.z=TAM\left(\ { Concat (\mathbf{{T o k e n}}_{\text {time }}  } ,z_{\text {temp }}\right)\right)\\
\end{equation}
\begin{equation}
\mathbf{\hat{Token}}_{\text {fake }},\mathbf{\hat{Token}}_{\text {time }} =z[0], z[1]
\end{equation}

where we re-formulate the final output of fake-aware token and temporal class token as $\mathbf{\hat{Token}}_{\text {fake }}$, $\mathbf{\hat{Token}}_{\text {time }}$. Both tokens are followed by multi-layer perception (MLP) to output the final prediction. The difference is that the temporal class token predicts specific forgery modalities and is optimized by the aforementioned binary cross-entropy loss, while the fake-aware token serves prediction confidence for the number of fake modalities ($\sum_{m \in{a,v}} y_{m}^{j}$). Denoting the prediction of fake-aware token as $p$, it is optimized by cross-entropy loss, $\frac{1}{N_{df}} \sum_{j=1}^{N_{df}} \mathcal{L}_{\mathrm{CE}}\left( p^{j}, \sum_{m \in{a,v}} y_{m}^{j}; \theta_{a}, \theta_{v}, \theta_{m} ,  \theta_{j},\theta_{d}\right)$.


Eventually, for the final framework with FCD module, the overall learning objective can be defined as:
\begin{equation}
%\mathcal{L}= \mathcal{L}_{BCE}+\lambda_{f} \mathcal{L}_{CE}
\mathcal{L}= \mathcal{L}_{BCE}+ \mathcal{L}_{CE}
\end{equation}
%where  $\lambda_{f}$ is a weight factor, which is set to 1.

\subsection {Pretraining of AVSR}
The model is pretrained based on the approach of DeepAVSR \cite{afouras2018deep}. We select the TM-CTC model trained on LRS2 dataset and use the publicly available, pretrained model here\footnote{\url{https://github.com/lordmartian/deep_avsr}}.
 %

\subsection {Preprocessing}
\subsubsection{Audio Stream}
The audio signals are all resampled at a 16kHz sample rate and applied Short Time Fourier Transform (STFT) to obtain 321-dimensional spectrograms, with a 40ms window and 10ms hop-length. Since the visual signal are sampled at 25 fps per video, each visual frame corresponds to 4 acoustic frames. Following AVSR, the acoustic frames are padded to make input length a multiple of 4. Every 4 acoustic frames are concatenated and passed through a 1D convolutional layer with stride 4 to align input length with visual frames.

\subsubsection{Video Stream} 
After sampling the video at 25fps, we perform face tracking using the S3FD face detector \cite{zhang2017s3fd} for the video datasets that are not face-centered. For face-centered video frames, we resize it into a $224\times224$ and crop out a $112 \times 112$ patch that covers mouth region. Following Deep-AVSR, given a video of $T$ frames, a spatio-temporal ResNet \cite{stafylakis2017combining} is applied. After spatial average-pooling, a feature vector of $T \times 512$ is generated for each video.

\section{Experiment}

\subsection{Setup}

\subsubsection{Implementation}
The overall framework is stacked by multiple transformer layers. The layer numbers of each module are specified as  [$\theta_{a}$:6; $\theta_{v}$:6; $\theta_{j}$:6; $\theta_{d-FCD}$:1; $\theta_{d-TAM}$:2]. We adopt modality dropout during training, where dropout is applied to mask the full features of one modality, and the loss for the corresponding labels is also masked. We train the model with batch size of 12 and Adam optimization with a learning rate of 1e-5. Every model is trained for 100 iterations in total.

% \begin{table}[h]
% \centering
% \begin{tabular}{|c|c|c|c|c|}
% \hline$\theta_{a}$ & $\theta_{v}$ & $\theta_{j}$ & $\theta_{d-FCD}$ & $\theta_{d-TAM}$ \\
% \hline6             & 6             & 6             & 1   & 2  \\
% \hline

% \end{tabular}
% \caption{The layer number of transformers in each module.}
% \label{table1}
% \end{table}
\subsubsection{Audio-Visual Deepfake Datasets}
We use DFDC dataset \cite{dolhansky2019deepfake} and FakeAVCeleb (FAV) dataset \cite{khalid2021fakeavceleb} since they  contain both falsified audio and video, which can meet our experiment requirements. Besides, we also use a recently-released Localised  Audio-Visual DeepFake
dataset (LAV-DF)~\cite{cai2022you}. Different from the above two datasets in which fake content is throughout the entire video/audio signal, manipulation in LAV-DF is driven by transcripts content where word/words are replaced with its/their antonym(s) to generate small fake segments of audio or video. 
DFDC dataset contains voice-over videos or conversational videos of two subjects. To create a dataset for our experiments, following prior works \cite{chugh2020not,mittal2020emotions}, we randomly sampled a subset of 18000 videos (85:15 train-test split) from DFDC and manually filter out the above types of videos. Notably, similar to \cite{hosler2021deepfakes}, the fake audio label is given by comparing audio tracks of fake videos with that of original videos which is achieved via hash on the audio file sequence.\footnote{\url{https://www.kaggle.com/datasets/basharallabadi/dfdc-video-audio-labels}}  %the labels are determined by comparing audio tracks of fake videos with that of original videos. This is achieved via hash on the audio file sequence.
As for FAV and LAV-DF, we directly use full data following the splits for train/test from the original datasets.

\subsubsection{Metrics}
The metric of our main experiments is the F1 score. Since there are varying degrees of class imbalances in each dataset, we use average per-class F1 (CF1) and overall F1 (OF1). Actually, OF1 is preferable for class imbalance since it weighs each sample equally. To take the class size into consideration, we also report weighted-averaged F1 (WF1) scores which can account for the contribution of each class by weighting the number of samples of that given class. The accuracy score (ACC) is also used to evaluate the performance of each single modality detection.


\subsection{Comparison with SOTA Methods}
There exist no approaches that tackle audio/video deepfake detection in a dual-label way. To demonstrate the superior performance of our method, we compared our framework with multiple state-of-the-art baselines of binary deepfake detection. These methods can be divided into two groups: 

(1) unimodal methods involving only visual modality: Capsule~\cite{nguyen2019capsule}, Headpose~\cite{yang2019exposing}, VLMLP, VA-Log~\cite{8638330}, Xception~\cite{rossler2019faceforensics++}, Meso-4~\cite{afchar2018mesonet}, FWA, DSP-FWA~\cite{li2019warping}, and LipForen~\cite{haliassos2021lips}. 

(2) multimodal methods involving both audio and visual modalities: EmoForen~\cite{mittal2020emotions}, BA-TFD~\cite{cai2022you}, VFD~\cite{cheng2022voice}, MDS~\cite{chugh2020not}, and JointAV~\cite{zhou2021joint}. 

We report the AUC scores in Table \ref{tab:BinaryClass}, our framework outperforms all the other methods on both DFDC and FAV datasets, which can be attributed to the utility of the cross-modal speech correlation. EmoForen\cite{mittal2020emotions} utilizes emotion correlation across modalities, which is not as effective as our speech correlation. Notably, LipForen \cite{haliassos2021lips} leverages lipreading features without taking advantage of the audio aspect, thereby only achieving limited performance.
%The manipulations can be present in either audio or visual or both,cai2022you,cheng2022voice,chugh2020not,zhou2021joint
% of the channels. are largely visual-based fakes but it is unclear if the video is fake or the audio, 

Besides traditional deepfake datasets, we also want to show how our methods perform on partially modified deepfakes because these subtle fake contents are more indistinguishable and imperceptible. As shown in Table \ref{tab:BinaryLAV}, the results on LAV-DF show that our method also outperforms the other compared methods which demonstrates our methods can also combat such novelty deepfakes well.

\begin{table}[h]
\small
\centering
\caption{Comparison of our method with other methods on DFDC and FAV datasets using the AUC metric. Results of some methods are cited from MDS and VFD, LipForen on DFDC is reproduced by ourselves since the authors did not report this metric on the dataset}
\begin{tabular}{c|c|c|c}
\hline Modal & Methods & DFDC & FAV \\
\hline V & Meso-4~\cite{afchar2018mesonet} (2018) & $75.30$ & $49.17$ \\ 
 V & Capsule~\cite{nguyen2019capsule} (2019) & $53.30$ & $76.19$ \\
 V & Headpose~\cite{yang2019exposing} (2019) & $55.90$ & $49.00$ \\
 %V & TwoStream & $61.40$ $67.00$\\
 V & VA-MLP~\cite{8638330} (2019)& $61.90$ & $67.00$\\
 V & VA-LogReg~\cite{8638330} (2019) & $66.20$ & $67.90$\\
 V & Xepction~\cite{rossler2019faceforensics++} (2019) & $72.20$ & $76.19$\\
 
 V & FWA~\cite{li2019warping} (2019) & $72.70$  & $-$\\
 V & DSP-FWA~\cite{li2019warping}  (2019) & $75.50$  & $-$\\
 V & LipForen~\cite{haliassos2021lips} (2020) & $77.59$  & $97.60$\\ 
\hline AV & EmoForen~\cite{mittal2020emotions} (2020) & $84.40$  & $-$ \\
 AV & BA-TFD~\cite{cai2022you} (2022)& $84.60$  & $-$ \\
 AV & VFD~\cite{cheng2022voice} (2022) & $85.13$ & $86.11$\\
 AV & MDS~\cite{chugh2020not} (2020) & $90.55$  & $-$\\
 AV & JointAV~\cite{zhou2021joint} (2021) & $96.29$  & $-$\\
 AV & AVoiD-DF~\cite{yang2023avoid} (2023) ) & $94.80$  & $89.20$\\
\hline AV & Ours & $\mathbf{98.13}$ & $\mathbf{99.99}$ \\
\hline
\end{tabular}

\label{tab:BinaryClass}
\end{table}

\begin{table}
\small
\centering
\caption {Comparison of our method with other methods on  Localised Audio-Visual DeepFake datasets using AUC metrics. Results of other methods are cited from LAV-DF~\protect\cite{cai2022you}.}
% \begin{tabular}
% {c|c|c|c|c}
% \hline \multicolumn{1}{c|}{ Dataset } & \multicolumn{4}{c}{ LAV-DF } \\
% \hline Model & MDS & EffViT~\cite{coccomini2022combining} & BA-TFD & Ours\\
% \hline AUC & 82.8 & 96.5 & 99.0 &$\mathbf{99.9}$ \\
% \hline
% \end{tabular}
\begin{tabular}
{c|c|c|c}
\hline   \multicolumn{4}{c}{ LAV-DF } \\
\hline MDS & EffiViT~\cite{coccomini2022combining} (2022)& BA-TFD (2022) & Ours\\
\hline   82.8 & 96.5 & 99.0 &$\mathbf{99.9}$ \\
\hline
\end{tabular}

\label{tab:BinaryLAV}
\end{table}
%


\begin{table}[t]

% \begin{tabular}{c|c|c|c|c|c}
% %\begin{tabular}{|l|l|l|l|l|l|l|ll}
% \cline{1-6}
% \hline Methods  & OF1    & CF1   & WF1   & VF1    & AF1       \\ \cline{1-6}
% \hline w/o AVSR & 77.52  & 75.60 & 76.78 & 78.07 & 70.19     \\ \cline{1-6}
% \hline w/  AVSR     & 91.89  & 88.34 & 91.66 & 92.69 & 83.96     \\ \cline{1-6}
% \end{tabular}
\caption{The Benefit of AVSR. }
\begin{tabular}{c|c|c|c|c|c|c}
\hline Dataset  &\multicolumn{3}{c|}{DFDC}  &\multicolumn{3}{c}{LAV}    \\  \cline{1-7}
\hline Methods  & OF1    & CF1   & WF1   & OF1    & CF1   &WF1     \\ \cline{1-7}
\hline w/o AVSR & 77.52  & 75.60 & 76.78 & 95.65 &95.66  &95.53   \\ \cline{1-7}
%\hline w/  AVSR     & 91.89  & 88.34 & 91.66 & 99.85 &99.85  &99.86   \\ \cline{1-7} 
% above line is origin paper 70 step
\hline w/  AVSR     & 90.24  & 87.80 & 90.24 & 99.85 &99.85  &99.86   \\ \cline{1-7}
\end{tabular}

\label{tab:AVSR}
\end{table}

\begin{table}[t]

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \begin{tabular}{c|c|c|c|c|c|c}
% %\hline \multirow{2}{*}{Methods} & \multicolumn{2}{c|}{FF++(LQ)}   & \multicolumn{2}{c|}{FF++(HQ)}   & Celeb-DF       & DFDC         \\



% \hline \multirow{2}{*}{ Modality Compensation } & \multicolumn{5}{|c|}{ DFDC } & FakeAVCeleb \\
% %\hline \multirow{2}{*}{ Modality Compensation } & \multicolumn{5}{|c|}{DFDC} & FakeAVCeleb \\
%  \cline { 2 - 7 } & OF1 & CF1 & VF1 & AF1 & WF 1 & OF1 \\
% \hline None & $88.96$ & $86.02$ & $89.62$ & $82.40$ & $88.57$ & $54.98$ \\
% \hline Video & $87.79$ & $84.41$ & $88.50$ & $80.17$ & $87.57$ & $60.02$ \\
% \hline Audio &\textbf{91.89} & \textbf{88.34} & \textbf{92.69} & \textbf{83.96} & \textbf{91.66} & \textbf{68.57} \\
% \hline
% \end{tabular}
\caption{Ablation results of different strategies of modality compensation.}
\begin{tabular}{c|c|c|c|c|c}
%\hline \multirow{2}{*}{Methods} & \multicolumn{2}{c|}{FF++(LQ)}   & \multicolumn{2}{c|}{FF++(HQ)}   & Celeb-DF       & DFDC         \\



%\hline \multirow{2}{*}{Compensation} & \multicolumn{5}{|c}{ DFDC }  \\
\hline {Compensation}  & OF1 & CF1 & VF1 & AF1 & WF 1  \\
%\hline \multirow{2}{*}{ Modality Compensation } & \multicolumn{5}{|c|}{DFDC} & FakeAVCeleb \\
 %\cline { 2 - 6 } & OF1 & CF1 & VF1 & AF1 & WF 1  \\
\hline None & $88.96$ & $86.02$ & $89.62$ & $82.40$ & $88.57$  \\
\hline Video & $87.79$ & $84.41$ & $88.50$ & $80.17$ & $87.57$ \\
%\hline Audio &\textbf{91.89} & \textbf{88.34} & \textbf{92.69} & \textbf{83.96} & \textbf{91.66}   \\
\hline Audio &\textbf{90.24} & \textbf{87.80} & \textbf{90.84} & \textbf{84.61} & \textbf{90.24}   \\
% above line is origin model 70 step
\hline
\end{tabular}


\label{tab:MCA}
\end{table}



\begin{table*}[t]
\centering
\caption{Generalization study on the influence of different components in Dual-Label Classifier. The CrossFAV denotes trained on DFDC and tested on FakeAVCeleb, and vice versa.}
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c}
\hline \multicolumn{2}{c|}{Methods} & \multicolumn{5}{c|}{ CrossFAV } & \multicolumn{5}{c}{ CrossDFDC } \\
\hline TAM & FCD & OF1 & CF1 & WF1 & VACC & AACC & OF1 & CF1 & WF1 & VACC & AACC \\
\hline - &- & $65.77$ & $54.07$ & $57.12$ & $74.73$ & $49.34$ & $55.89$ & $29.23$ & $53.00$ & $58.46$ & $94.65$ \\
\hline - &$\surd$ & $67.50$ & $55.53$ & $\mathbf{59.68}$ & $76.41$ & $\mathbf{49.55}$ & $58.28$ & $32.67$ & $55.77$ & $59.19$ & $94.61$ \\
\hline  $\surd$ &- & $61.03$ & $49.64$ & $52.61$ & $68.26$ & $48.61$ & $54.97$ & $32.57$ & $52.38$ & $59.15$ & $\mathbf{94.92}$ \\
\hline $\surd$ &$\surd$ & $\mathbf{68.57}$ & $\mathbf{55.83}$ & $58.77$ & $\mathbf{80.00}$ & $48.89$ & $\mathbf{58.57}$ & $\mathbf{3 6 . 9 7}$ & $\mathbf{5 6 . 7 5}$ & $\mathbf{6 1 . 4 2}$ & $94.07$ \\
\hline
\end{tabular}


\label{tab:generalize}
\end{table*} 


% \begin{table*}[t]
% \centering
% \begin{tabular}{c|c|c|c|c|c|c|c|c}
% \hline \multicolumn{1}{c|}{Methods} & \multicolumn{4}{c|}{ FAV } & \multicolumn{4}{c}{ DFDC } \\
% \hline TYPE & VACC & AACC & VF1 & AF1 & VACC & AACC & VF1 & AF1 \\
% \hline ASR-Aclass & $-$ & $99.92$ & $-$ & $99.93$ & $-$ & $98.15$ & $-$ & $81.95$ \\
%  VSR-Vclass & $99.62$ & $-$ & $99.80$ & $-$ & $88.96$ & $-$ & $88.26$ & $-$ \\
% \hline AVSR-Aclass & $-$ & $99.90$ & $-$ & $99.91$ & $-$ & $98.38$ & $-$ & $83.96$ \\
%  AVSR-Vclass & $99.71$ & $-$ & $99.85$ & $-$ & $\mathbf{93.15}$ & $-$ & $\mathbf{ 9 3 .0 7}$ & $-$ \\
% \hline AVSR-DualLabel & $\mathbf{9 9. 7 8}$ & $\mathbf{9 9. 9 2}$ & $\mathbf{9 9 .8 9}$ & $\mathbf{9 9.93}$ & $92.69$ & $\mathbf{9 8 .3 8}$ & $92.69$ & $\mathbf{8 3. 9 6}$ \\
% \hline
% \end{tabular}
% \begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c}
% \hline \multicolumn{2}{c|}{Involved} & \multicolumn{2}{c|}{Detectable} & \multicolumn{1}{c|}{Auxiliary} & \multicolumn{4}{c|}{ FAV } & \multicolumn{4}{c}{ DFDC } \\
% \hline A & V & A & V & Type & VACC & AACC & VF1 & AF1 & VACC & AACC & VF1 & AF1 \\
% \hline $\surd$ &- &$\surd$ &- & ASR & $-$ & $99.90$ & $-$ & $99.91$ & $-$ & $97.88$ & $-$ & $80.28$ \\
% - & $\surd$ &- &$\surd$ & VSR & $99.62$ & $-$ & $99.80$ & $-$ & $88.96$ & $-$ & $88.26$ & $-$ \\
% \hline  $\surd$ & $\surd$ &$\surd$ &- & AVSR & $-$ & $99.90$ & $-$ & $99.91$ & $-$ & $98.38$ & $-$ & $83.96$ \\
%  $\surd$ & $\surd$ &- &$\surd$ & AVSR & $99.71$ & $-$ & $99.85$ & $-$ & $\mathbf{93.15}$ & $-$ & $\mathbf{ 9 3 .0 7}$ & $-$ \\
%  $\surd$ & $\surd$ & $\surd$ &$\surd$ &AVSR & $\mathbf{9 9. 7 8}$ & $\mathbf{9 9. 9 2}$ & $\mathbf{9 9 .8 9}$ & $\mathbf{9 9.93}$ & $92.69$ & $\mathbf{9 8 .3 8}$ & $92.69$ & $\mathbf{8 3. 9 6}$ \\
% \hline
% \end{tabular}
% \begin{tabular}{c|c|c|c|c|c|c|c|c|c|c}
% \hline  \multicolumn{2}{c}{Available Modality} & \multicolumn{1}{|c|}{Auxiliary} & \multicolumn{4}{c|}{ FAV } & \multicolumn{4}{c}{ DFDC } \\
% \hline  Audio & Visual & Type & VACC & AACC & VF1 & AF1 & VACC & AACC & VF1 & AF1 \\
% \hline 
% \rowcolor{red!20}
% $\surd$ &- & ASR & $-$ & $99.90$ & $-$ & $99.91$ & $-$ & $97.88$ & $-$ & $80.28$ \\
% - &$\surd$ & VSR & $99.62$ & $-$ & $99.80$ & $-$ & $88.96$ & $-$ & $88.26$ & $-$ \\
% \hline 

% \rowcolor{green!20}
% $\surd$ &- & AVSR & $-$ & $\mathbf{99.95}$ & $-$ & $\mathbf{99.95}$ & $-$ & $98.27$ & $-$ & $81.33$ \\
% \rowcolor{pink!20}
% - &$\surd$ & AVSR & $99.65$ & $-$ & $99.81$ & $-$ & $90.50$ & $-$ & $89.96$ & $-$ \\
%   $\surd$ &$\surd$ &AVSR & $\mathbf{9 9. 7 0}$ & $9 9. 9 3$ & $\mathbf{9 9 .8 4}$ & $9 9.93$ & $\mathbf{91.23}$ & $\mathbf{9 8 .46}$ & $\mathbf{90.83}$ & $\mathbf{8 4.  61}$ \\
% \hline
% \end{tabular}
% \caption{Comparison of dual-label framework with audio-only detection and video-only detection. Rows 1 and 2 show results of unimodal-involved audio-only detection / video-only detection, respectively; rows 3 and 4 show results of multimodal-involved audio-only detection / video-only detection, respectively; the last row shows results of our dual-label detection framework.}
% \label{tab:mulitmodal}
% \end{table*} 


\begin{table*}[t]
\centering
\caption{Comparison of our dual-label framework with unimodal approaches and multi-classifier approaches under three test modality-agnostic cases: prediction with visual-only (shown in pink), audio-only (shown in green), or audio-visual input. The best results are highlighted in bold.}
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c}
%\rowcolors{1}{green!20}{yellow!50}
\hline  \multicolumn{2}{c}{Available Modality} & \multicolumn{1}{|c|}{Auxiliary} & \multicolumn{1}{|c|}{Classification} & \multicolumn{4}{c|}{ FAV } & \multicolumn{4}{c}{ DFDC } \\
\hline Audio & Visual &Type &Type & VACC & AACC & VF1 & AF1 & VACC & AACC & VF1 & AF1 \\
\hline$\sqrt{ }$ & - & ASR & Binary & - & 99.90 & - & 99.91 & - & 97.88 & - & 80.28 \\
- & $\sqrt{ }$ & VSR & Binary & 99.62 & - & 99.80 & - & 88.96 & - & 88.26 & - \\

%\rowcolor{green!15}
\hline $\sqrt{ }$ & - & AVSR & Dual-classifier & - & \cellcolor{green!15}97.16 & - & \cellcolor{green!15}97.35 & - & \cellcolor{green!15}97.54 & - & \cellcolor{green!15}66.67 \\

- & $\sqrt{ }$ & AVSR & Dual-classifier & \cellcolor{red!15}99.50 & - & \cellcolor{red!15}99.74 & - & \cellcolor{red!15}87.96 & - & \cellcolor{red!15}87.95 & - \\
$\sqrt{ }$ & $\sqrt{ }$ & AVSR & Dual-classifier & 99.64 & 99.88 & 99.81 & 99.88 & 90.07 & 98.34 & 89.33 & 83.14 \\

\hline$\sqrt{ }$ & - & AVSR & Triple-classifier & - &\cellcolor{green!15} 98.19 & - &\cellcolor{green!15} 98.24 & - &\cellcolor{green!15} 95.80 & - &\cellcolor{green!15} 59.77 \\

- & $\sqrt{ }$ & AVSR & Triple-classifier & \cellcolor{red!15}99.48 & - &\cellcolor{red!15} 99.72 & - & \cellcolor{red!15}87.58 & - & \cellcolor{red!15}87.97 & - \\
$\sqrt{ }$ & $\sqrt{ }$ & AVSR & Triple-classifier & 99.64 & \textbf{99.95} & 99.81 & \textbf{99.95} & 89.96 & 98.15 & 89.21 & 82.22 \\

\hline$\sqrt{ }$ & - & AVSR & Dual-Label & - & \cellcolor{green!15}\textbf{99.95} & - & \cellcolor{green!15}\textbf{99.95} & - &\cellcolor{green!15} 98.27 & - & \cellcolor{green!15}81.33 \\

- & $\sqrt{ }$ & AVSR & Dual-Label & \cellcolor{red!15}99.65 & - & \cellcolor{red!15}99.81 & - & \cellcolor{red!15}90.50 & - & \cellcolor{red!15}89.96 & - \\
$\sqrt{ }$ & $\sqrt{ }$ & AVSR & Dual-Label & \textbf{99.70} & 99.93 & \textbf{99.84} & 99.93 & \textbf{91.23} & \textbf{98.46} & \textbf{90.83} & \textbf{84.61} \\
\hline
\end{tabular}

\label{tab:multimodal}
\end{table*} 

\subsection{Ablation Study}
\subsubsection{Benefit of AVSR}
To demonstrate the effectiveness of audio-visual speech correlation for dual-label deepfake detection, we train a framework the same as our final framework from scratch without AVSR weights loaded for comparison. 

As shown in Table~\ref{tab:AVSR}, introducing AVSR task significantly improves the detection performance, which verifies that potential speech correlation across modalities is highly beneficial to synergistic audio-visual deepfake detection. The improvement is not only reflected in globally forged DFDC, but also in LAV-DF which is partially forged by modifying the word sentiment of the video content. Albeit LAV-DF proves that changing a few uttering words can even lead to a subtle forgery beyond the perception of humans, leveraging AVSR allows us to extract phoneme-level features frame by frame. The promising potential of these fine-grained semantic features will show more using value in real-life scenarios.




\subsubsection{Benefit of Modality Compensation Adapter }
In this section, we evaluate how the form of the modality compensation strategy affects the performance of dual-label deepfake detection on DFDC. We evaluate the variants of (1) the simple concatenation of two modalities with no compensation (\textbf{None}), as preceding speech recognition does; (2) audio modality compensated (\textbf{Audio}); (3) video modality compensated (\textbf{Video}). We also show the detection performance of each single modality (\textbf{AF1} for audio, and \textbf{VF1} for video).

As shown in Table~\ref{tab:MCA}, consistent with our speculation in section 3, adopting audio modality compensation yields the most desirable performance. Notably, compared with video compensation, audio compensation improves not only audio detection performance but also that of video. Since the influence of audio on model decision-making tends to be weaker, we believe a balance between audio and visual modalities is achieved by this mechanism and helps both modalities learn better features.


\subsubsection{Generalization effect of Sub-modules in Dual-Label Classifier} We expect the design in dual-label classifier to provide a more generalizable audio-visual representation. As shown in Table~\ref{tab:generalize}, we develop several variants and conduct a series of experiments of cross-dataset tests to investigate the influence of different components. 
We train our framework on both FakeAVCeleb and DFDC and mutually perform cross-dataset tests. To show the generalization performance of each label, accuracy scores specific to each modality, \emph{i.e.},\textbf{VACC} and \textbf{AACC}, are also reported.

The results demonstrate that each kind of absence of the proposed module can cause a generalization performance decline. Among them, the deletion of FCD leads to a significant drop which verifies the importance of bridging the forgery intensity gap across modalities. Generally, both performance on two datasets reaches their peaks when employing the overall dual-label classifier.


\subsection{Compared with Unimodal Approaches and  Multi-classifier Approaches}
It is natural to question two aspects when evaluating our dual-label audio-visual deepfake detection framework: (1) how it performs compared to unimodal approaches, and (2) how it performs compared to multi-classifier approaches. In this section, we provide specific experimental comparisons.

We first trained two separate unimodal models for audio and video modalities as baselines. To ensure fairness, we loaded the weights from the ASR and VSR models respectively. Additionally, to investigate the detection performance of the dual-label manner, we followed previous multimodal approaches and transformed our framework into two variants of multi-classifier methods: a dual-classifier (i.e., for audio and visual streams, respectively) as done in  MDS and AVoiD-DF, and a triple-classifier (i.e., for audio, visual, and entire video, respectively) as implemented in JointAV. As presented in Table~\ref{tab:multimodal}, these comparative experiments, along with our proposed method, constitute four groups of experiments. Under each group of experiments, we report three types of test results to examine the performance under modality-agnostic scenarios, i.e., after training is completed in a multimodal setting, the model performs prediction with visual-only (shown in pink), audio-only (shown in green), or audio-visual input during the testing phase.

It can be observed that no matter whether compared to unimodal approaches or classifier variants, our method performs best on all metrics for both datasets, particularly in cases where both audio and video are available. In fact, with the support of AVSR, under such a test case that is consistent with training conditions, the results produced by different classifier variants exhibit minor differences and all surpass unimodal methods. This highlights the advantage of AVSR compared to unimodal methods using ASR or VSR when it comes to modeling cross-modal forgery clues. 

In more challenging missing modality situations, our method further extends its lead over other variants by maintaining performance on par with audio-visual detection. When either modality is unavailable, the variation in our method's performance on the FAV dataset remains minimal, within decimal point ranges. However, both dual and triple classifiers experience a decline of over 1 percentage point in the audio-only input situation, and both audio-only and video-only detection underperforms their respective unimodal methods. This gap is more evident in datasets dominated by visual forgeries, such as DFDC, where the AF1 for dual classifiers drops from 83.14 to 66.67, with an even more significant decline for triple classifiers (82.22 to 59.77). This occurs because the FAV dataset contains duplicated video (audio) content according to different audio (video) labels (e.g., some videos in RealAFakeV share the same video content as those in FakeAFakeV), while DFDC does not.  For video-only detection on DFDC, our method continues to outperform the unimodal baseline (VF1 increases from 88.26 to 90.83), while other variants underperform the baseline. The combined results from both datasets show that our method consistently maintains its advantages over unimodal approaches in their respective modalities, regardless of whether the modalities are present in pairs or not.

We attribute the effectiveness of our method to the fact that, compared to multiple classifiers which create a branched structure, the dual-label approach results in a highly-multiplexed unified structure that more effectively preserves the forgery patterns extracted by AVSR. In essence, the dual-label technique optimizes the benefits of AVSR. Overall, the performance on audio-only, visual-only, and audio-visual detection collectively demonstrates that the proposed framework excels in all three fake-modality-agnostic scenarios. %It is worth noting that detection of two modalities at the same time is more challenging than that of only one modality. Overall, this experiment proves that our dual-label detection framework achieves synergistic audio-visual deepfake detection with outstanding performance.
% Figure environment removed


\subsection{Visualization}
To demonstrate the efficacy of each design, we visualize the feature cluster of four categories with regard to manipulated modalities, namely, "RealV+RealA", "RealV+FakeA", "FakeV+RealA", and "FakeV+FakeA". We employ t-SNE \cite{van2008visualizing} to visualize the feature distribution of test set from FAV. As shown in Figure~\ref{fig:tsnemap}, without finetuning on AVSR (Figure a), the model suffers from the most severe feature entanglement. After finetuning on AVSR (Figure b, c, d), the features exhibit better distinguishability. However, the absence of either TAM or FCD still leaves the distinguishability of video-only fakes (orange) not satisfactory enough. Collectively, our framework generates the most favorable feature distribution. As depicted in Figure~\ref{fig:tsnemap}d, the distributions of four categories are disentangled well. Notably, among the three fake distributions, the feature clusters of audio-only fakes (green) and video-only fakes (orange) are relatively close to that of the reals (blue), and the audio-visual fakes (red) are most distant from the reals as they possess more fake compositions, which is consistent with our description of forgery intensity. We believe this optimal feature distribution also reflects the contribution of FCD.
%visualization.
% Use \bibliography{yourbibfile} instead or the References section will not appear in your paper

\section{Conclusion}
This paper presents a unified, modality-agnostic framework capable of detecting audio-visual deepfakes, regardless of the number or type of modality involved. We designed a dual-label detection classifier to specifically identify whether either the audio or visual modality (or both) has been manipulated, while effectively handling scenarios with missing modalities. The AVSR is introduced as a preceding task to provide high-level speech correlations across modalities, enhancing the framework's ability to model cross-modal forgery clues. The integration of AVSR with the Dual-Label Classifier results in excellent performance for each individual modality. The overall component design leads our framework to not only outperform various state-of-the-art competitors but also effectively detect modality-agnostic audio-visual deepfakes with strong performance.

\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}







\end{document}
%\typeout{get arXiv to do 4 passes: Label(s) may have changed. Rerun}
\endinput
%%
%% End of file `sample-sigconf.tex'.
