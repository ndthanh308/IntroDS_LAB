% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{korshunova2017fast}
I.~Korshunova, W.~Shi, J.~Dambre, and L.~Theis, ``Fast face-swap using
  convolutional neural networks,'' in \emph{Proceedings of the IEEE
  international conference on computer vision}, 2017, pp. 3677--3685.

\bibitem{thies2016face2face}
J.~Thies, M.~Zollhofer, M.~Stamminger, C.~Theobalt, and M.~Nie{\ss}ner,
  ``Face2face: Real-time face capture and reenactment of rgb videos,'' in
  \emph{Proceedings of the IEEE conference on computer vision and pattern
  recognition}, 2016, pp. 2387--2395.

\bibitem{siarohin2019first}
A.~Siarohin, S.~Lathuili{\`e}re, S.~Tulyakov, E.~Ricci, and N.~Sebe, ``First
  order motion model for image animation,'' \emph{Advances in Neural
  Information Processing Systems}, vol.~32, 2019.

\bibitem{ping2018clarinet}
W.~Ping, K.~Peng, and J.~Chen, ``Clarinet: Parallel wave generation in
  end-to-end text-to-speech,'' \emph{arXiv preprint arXiv:1807.07281}, 2018.

\bibitem{kameoka2018non}
H.~Kameoka, T.~Kaneko, K.~Tanaka, and N.~S.-V. Hojo, ``Non-parallel
  many-to-many voice conversion using star generative adversarial networks,''
  in \emph{SLT Workshop}, 2018, pp. 18--21.

\bibitem{prajwal2020lip}
K.~Prajwal, R.~Mukhopadhyay, V.~P. Namboodiri, and C.~Jawahar, ``A lip sync
  expert is all you need for speech to lip generation in the wild,'' in
  \emph{MM}, 2020, pp. 484--492.

\bibitem{cheng2022videoretalking}
K.~Cheng, X.~Cun, Y.~Zhang, M.~Xia, F.~Yin, M.~Zhu, X.~Wang, J.~Wang, and
  N.~Wang, ``Videoretalking: Audio-based lip synchronization for talking head
  video editing in the wild,'' 2022.

\bibitem{chugh2020not}
K.~Chugh, P.~Gupta, A.~Dhall, and R.~Subramanian, ``Not made for each
  other-audio-visual dissonance-based deepfake detection and localization,'' in
  \emph{MM}, 2020, pp. 439--447.

\bibitem{gu2020deepfake}
Y.~Gu, X.~Zhao, C.~Gong, and X.~Yi, ``Deepfake video detection using
  audio-visual consistency,'' in \emph{International Workshop on Digital
  Watermarking}.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2020, pp.
  168--180.

\bibitem{mittal2020emotions}
T.~Mittal, U.~Bhattacharya, R.~Chandra, A.~Bera, and D.~Manocha, ``Emotions
  don't lie: An audio-visual deepfake detection method using affective cues,''
  in \emph{MM}, 2020, pp. 2823--2832.

\bibitem{zhou2021joint}
Y.~Zhou and S.-N. Lim, ``Joint audio-visual deepfake detection,'' in
  \emph{ICCV}, 2021, pp. 14\,800--14\,809.

\bibitem{yang2023avoid}
W.~Yang, X.~Zhou, Z.~Chen, B.~Guo, Z.~Ba, Z.~Xia, X.~Cao, and K.~Ren,
  ``Avoid-df: Audio-visual joint learning for detecting deepfake,'' \emph{IEEE
  Transactions on Information Forensics and Security}, vol.~18, pp. 2015--2029,
  2023.

\bibitem{cheng2022voice}
H.~Cheng, Y.~Guo, T.~Wang, Q.~Li, T.~Ye, and L.~Nie, ``Voice-face homogeneity
  tells deepfake,'' \emph{arXiv preprint arXiv:2203.02195}, 2022.

\bibitem{haliassos2022leveraging}
A.~Haliassos, R.~Mira, S.~Petridis, and M.~Pantic, ``Leveraging real talking
  faces via self-supervision for robust forgery detection,'' in \emph{CVPR},
  2022, pp. 14\,950--14\,962.

\bibitem{cai2022you}
Z.~Cai, K.~Stefanov, A.~Dhall, and M.~Hayat, ``Do you really mean that? content
  driven audio-visual deepfake dataset and multimodal method for temporal
  forgery localization,'' \emph{arXiv preprint arXiv:2204.06228}, 2022.

\bibitem{ilyas2023avfakenet}
H.~Ilyas, A.~Javed, and K.~M. Malik, ``Avfakenet: A unified end-to-end dense
  swin transformer deep learning model for audio--visual deepfakes detection,''
  \emph{Applied Soft Computing}, vol. 136, p. 110124, 2023.

\bibitem{feng2023self}
C.~Feng, Z.~Chen, and A.~Owens, ``Self-supervised video forensics by
  audio-visual anomaly detection,'' \emph{arXiv preprint arXiv:2301.01767},
  2023.

\bibitem{yu2023pvass}
Y.~Yu, X.~Liu, R.~Ni, S.~Yang, Y.~Zhao, and A.~C. Kot, ``Pvass-mdd: Predictive
  visual-audio alignment self-supervision for multimodal deepfake detection,''
  \emph{IEEE Transactions on Circuits and Systems for Video Technology}, 2023.

\bibitem{li2020face}
L.~Li, J.~Bao, T.~Zhang, H.~Yang, D.~Chen, F.~Wen, and B.~Guo, ``Face x-ray for
  more general face forgery detection,'' in \emph{CVPR}, 2020, pp. 5001--5010.

\bibitem{shiohara2022detecting}
K.~Shiohara and T.~Yamasaki, ``Detecting deepfakes with self-blended images,''
  in \emph{CVPR}, 2022, pp. 18\,720--18\,729.

\bibitem{gu2022exploiting}
Q.~Gu, S.~Chen, T.~Yao, Y.~Chen, S.~Ding, and R.~Yi, ``Exploiting fine-grained
  face forgery clues via progressive enhancement learning,'' in \emph{AAAI},
  vol.~36, no.~1, 2022, pp. 735--743.

\bibitem{yu2022focus}
C.~Yu, P.~Chen, J.~Dai, X.~Wang, W.~Zhang, J.~Liu, and J.~Han, ``Focus by
  prior: Deepfake detection based on prior-attention,'' in \emph{2022 IEEE
  International Conference on Multimedia and Expo (ICME)}.\hskip 1em plus 0.5em
  minus 0.4em\relax IEEE, 2022, pp. 1--6.

\bibitem{zhao2021multi}
H.~Zhao, W.~Zhou, D.~Chen, T.~Wei, W.~Zhang, and N.~Yu, ``Multi-attentional
  deepfake detection,'' in \emph{CVPR}, 2021, pp. 2185--2194.

\bibitem{agarwal2019protecting}
S.~Agarwal, H.~Farid, Y.~Gu, M.~He, K.~Nagano, and H.~Li, ``Protecting world
  leaders against deep fakes.'' in \emph{CVPR Workshops}, vol.~1, 2019, p.~38.

\bibitem{sun2021improving}
Z.~Sun, Y.~Han, Z.~Hua, N.~Ruan, and W.~Jia, ``Improving the efficiency and
  robustness of deepfakes detection through precise geometric features,'' in
  \emph{CVPR}, 2021, pp. 3609--3618.

\bibitem{li2018eye}
Y.~Li, M.-C. Chang, and S.~Lyu, ``In ictu oculi: Exposing ai created fake
  videos by detecting eye blinking,'' in \emph{WIFS}.\hskip 1em plus 0.5em
  minus 0.4em\relax IEEE, 2018, pp. 1--7.

\bibitem{gu2022delving}
\BIBentryALTinterwordspacing
Z.~Gu, Y.~Chen, T.~Yao, S.~Ding, J.~Li, and L.~Ma, ``Delving into the local:
  Dynamic inconsistency learning for deepfake video detection,'' in
  \emph{AAAI}.\hskip 1em plus 0.5em minus 0.4em\relax {AAAI} Press, 2022, pp.
  744--752. [Online]. Available:
  \url{https://ojs.aaai.org/index.php/AAAI/article/view/19955}
\BIBentrySTDinterwordspacing

\bibitem{zhang2021detecting}
D.~Zhang, C.~Li, F.~Lin, D.~Zeng, and S.~Ge, ``Detecting deepfake videos with
  temporal dropout 3dcnn.'' in \emph{IJCAI}, 2021, pp. 1288--1294.

\bibitem{gu2021spatiotemporal}
Z.~Gu, Y.~Chen, T.~Yao, S.~Ding, J.~Li, F.~Huang, and L.~Ma, ``Spatiotemporal
  inconsistency learning for deepfake video detection,'' in \emph{CVPR}, 2021,
  pp. 3473--3481.

\bibitem{haliassos2021lips}
A.~Haliassos, K.~Vougioukas, S.~Petridis, and M.~Pantic, ``Lips don't lie: A
  generalisable and robust approach to face forgery detection,'' in
  \emph{CVPR}, 2021, pp. 5039--5049.

\bibitem{martin2022vicomtech}
J.~M. Mart{\'\i}n-Do{\~n}as and A.~{\'A}lvarez, ``The vicomtech audio deepfake
  detection system based on wav2vec2 for the 2022 add challenge,'' in
  \emph{ICASSP}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2022, pp.
  9241--9245.

\bibitem{xie2021siamese}
Y.~Xie, Z.~Zhang, and Y.~Yang, ``Siamese network with wav2vec feature for
  spoofing speech detection.'' in \emph{Interspeech}, 2021, pp. 4269--4273.

\bibitem{wang2021investigating}
X.~Wang and J.~Yamagishi, ``Investigating self-supervised front ends for speech
  spoofing countermeasures,'' \emph{arXiv preprint arXiv:2111.07725}, 2021.

\bibitem{baevski2020wav2vec}
A.~Baevski, Y.~Zhou, A.~Mohamed, and M.~Auli, ``wav2vec 2.0: A framework for
  self-supervised learning of speech representations,'' \emph{NeurIPS},
  vol.~33, pp. 12\,449--12\,460, 2020.

\bibitem{pianese2022deepfake}
A.~Pianese, D.~Cozzolino, G.~Poggi, and L.~Verdoliva, ``Deepfake audio
  detection by speaker verification,'' in \emph{2022 IEEE International
  Workshop on Information Forensics and Security (WIFS)}.\hskip 1em plus 0.5em
  minus 0.4em\relax IEEE, 2022, pp. 1--6.

\bibitem{hosler2021deepfakes}
B.~Hosler, D.~Salvi, A.~Murray, F.~Antonacci, P.~Bestagini, S.~Tubaro, and
  M.~C. Stamm, ``Do deepfakes feel emotions? a semantic approach to detecting
  deepfakes via emotional inconsistencies,'' in \emph{CVPR}, 2021, pp.
  1013--1022.

\bibitem{shi2021learning}
B.~Shi, W.-N. Hsu, K.~Lakhotia, and A.~Mohamed, ``Learning audio-visual speech
  representation by masked multimodal cluster prediction,'' in \emph{ICLR},
  2021.

\bibitem{campbell2008processing}
R.~Campbell, ``The processing of audio-visual speech: empirical and neural
  bases,'' \emph{Philosophical Transactions of the Royal Society B: Biological
  Sciences}, vol. 363, no. 1493, pp. 1001--1010, 2008.

\bibitem{almajai2007maximising}
I.~Almajai and B.~Milner, ``Maximising audio-visual speech correlation.'' in
  \emph{AVSP}, 2007, p.~17.

\bibitem{9151013}
S.~Agarwal, H.~Farid, O.~Fried, and M.~Agrawala, ``Detecting deep-fake videos
  from phoneme-viseme mismatches,'' in \emph{CVPR Workshops}, 2020, pp.
  2814--2822.

\bibitem{ma2021smil}
M.~Ma, J.~Ren, L.~Zhao, S.~Tulyakov, C.~Wu, and X.~Peng, ``Smil: Multimodal
  learning with severely missing modality,'' in \emph{Proceedings of the AAAI
  Conference on Artificial Intelligence}, vol.~35, no.~3, 2021, pp. 2302--2310.

\bibitem{7952625}
S.~Petridis, Z.~Li, and M.~Pantic, ``End-to-end visual speech recognition with
  lstms,'' in \emph{2017 IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP)}, 2017, pp. 2592--2596.

\bibitem{afouras2018deep}
T.~Afouras, J.~S. Chung, A.~Senior, O.~Vinyals, and A.~Zisserman, ``Deep
  audio-visual speech recognition,'' \emph{TPAMI}, 2018.

\bibitem{8682566}
S.~Zhang, M.~Lei, B.~Ma, and L.~Xie, ``Robust audio-visual speech recognition
  using bimodal dfsmn with multi-condition training and dropout
  regularization,'' in \emph{ICASSP 2019 - 2019 IEEE International Conference
  on Acoustics, Speech and Signal Processing (ICASSP)}, 2019, pp. 6570--6574.

\bibitem{9004036}
T.~Makino, H.~Liao, Y.~Assael, B.~Shillingford, B.~Garcia, O.~Braga, and
  O.~Siohan, ``Recurrent neural network transducer for audio-visual speech
  recognition,'' in \emph{2019 IEEE Automatic Speech Recognition and
  Understanding Workshop (ASRU)}, 2019, pp. 905--912.

\bibitem{graves2006connectionist}
A.~Graves, S.~Fern{\'a}ndez, F.~Gomez, and J.~Schmidhuber, ``Connectionist
  temporal classification: labelling unsegmented sequence data with recurrent
  neural networks,'' in \emph{ICML}, 2006, pp. 369--376.

\bibitem{ma2021end}
P.~Ma, S.~Petridis, and M.~Pantic, ``End-to-end audio-visual speech recognition
  with conformers,'' in \emph{ICASSP}.\hskip 1em plus 0.5em minus 0.4em\relax
  IEEE, 2021, pp. 7613--7617.

\bibitem{dolhansky2019deepfake}
B.~Dolhansky, R.~Howes, B.~Pflaum, N.~Baram, and C.~C. Ferrer, ``The deepfake
  detection challenge (dfdc) preview dataset,'' \emph{arXiv preprint
  arXiv:1910.08854}, 2019.

\bibitem{cheng2021mltr}
X.~Cheng, H.~Lin, X.~Wu, F.~Yang, D.~Shen, Z.~Wang, N.~Shi, and H.~Liu, ``Mltr:
  Multi-label classification with transformer,'' \emph{arXiv preprint
  arXiv:2106.06195}, 2021.

\bibitem{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  {\L}.~Kaiser, and I.~Polosukhin, ``Attention is all you need,'' in
  \emph{NeurIPS}, 2017, pp. 5998--6008.

\bibitem{zhang2017s3fd}
S.~Zhang, X.~Zhu, Z.~Lei, H.~Shi, X.~Wang, and S.~Z. Li, ``S3fd: Single shot
  scale-invariant face detector,'' in \emph{ICCV}, 2017, pp. 192--201.

\bibitem{stafylakis2017combining}
T.~Stafylakis and G.~Tzimiropoulos, ``Combining residual networks with lstms
  for lipreading,'' \emph{Interspeech}, pp. 3652--3656, 2017.

\bibitem{son2017lip}
J.~Son~Chung, A.~Senior, O.~Vinyals, and A.~Zisserman, ``Lip reading sentences
  in the wild,'' in \emph{Proceedings of the IEEE conference on computer vision
  and pattern recognition}, 2017, pp. 6447--6456.

\bibitem{kingma2014adam}
D.~P. Kingma and J.~Ba, ``Adam: A method for stochastic optimization,''
  \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{khalid2021fakeavceleb}
\BIBentryALTinterwordspacing
H.~Khalid, S.~Tariq, M.~Kim, and S.~S. Woo, ``Fake{AVC}eleb: A novel
  audio-video multimodal deepfake dataset,'' in \emph{NeurIPS}, 2021. [Online].
  Available: \url{https://openreview.net/forum?id=TAXFsg6ZaOl}
\BIBentrySTDinterwordspacing

\bibitem{coccomini2022combining}
D.~A. Coccomini, N.~Messina, C.~Gennaro, and F.~Falchi, ``Combining
  efficientnet and vision transformers for video deepfake detection,'' in
  \emph{International Conference on Image Analysis and Processing}.\hskip 1em
  plus 0.5em minus 0.4em\relax Springer, 2022, pp. 219--229.

\bibitem{afchar2018mesonet}
D.~Afchar, V.~Nozick, J.~Yamagishi, and I.~Echizen, ``Mesonet: a compact facial
  video forgery detection network,'' in \emph{WIFS}.\hskip 1em plus 0.5em minus
  0.4em\relax IEEE, 2018, pp. 1--7.

\bibitem{nguyen2019capsule}
H.~H. Nguyen, J.~Yamagishi, and I.~Echizen, ``Capsule-forensics: Using capsule
  networks to detect forged images and videos,'' in \emph{ICASSP}.\hskip 1em
  plus 0.5em minus 0.4em\relax IEEE, 2019, pp. 2307--2311.

\bibitem{yang2019exposing}
X.~Yang, Y.~Li, and S.~Lyu, ``Exposing deep fakes using inconsistent head
  poses,'' in \emph{ICASSP}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2019,
  pp. 8261--8265.

\bibitem{8638330}
F.~Matern, C.~Riess, and M.~Stamminger, ``Exploiting visual artifacts to expose
  deepfakes and face manipulations,'' in \emph{WACV Workshops}, 2019, pp.
  83--92.

\bibitem{rossler2019faceforensics++}
A.~Rossler, D.~Cozzolino, L.~Verdoliva, C.~Riess, J.~Thies, and M.~Nie{\ss}ner,
  ``Faceforensics++: Learning to detect manipulated facial images,'' in
  \emph{ICCV}, 2019, pp. 1--11.

\bibitem{chen2021defakehop}
H.-S. Chen, M.~Rouhsedaghat, H.~Ghani, S.~Hu, S.~You, and C.-C.~J. Kuo,
  ``Defakehop: A light-weight high-performance deepfake detector,'' in
  \emph{2021 IEEE International conference on Multimedia and Expo
  (ICME)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2021, pp. 1--6.

\bibitem{wodajo2021deepfake}
D.~Wodajo and S.~Atnafu, ``Deepfake video detection using convolutional vision
  transformer,'' \emph{arXiv preprint arXiv:2102.11126}, 2021.

\bibitem{Chen_2022_CVPR}
L.~Chen, Y.~Zhang, Y.~Song, L.~Liu, and J.~Wang, ``Self-supervised learning of
  adversarial example: Towards good generalizations for deepfake detection,''
  in \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR)}, June 2022, pp. 18\,710--18\,719.

\bibitem{Wang2021ACS}
X.~Wang and J.~Yamagishi, ``A comparative study on recent neural spoofing
  countermeasures for synthetic speech detection,'' in \emph{Interspeech},
  2021, pp. 4259--4263.

\bibitem{tak2021end}
H.~Tak, J.-w. Jung, J.~Patino, M.~Kamble, M.~Todisco, and N.~Evans,
  ``End-to-end spectro-temporal graph attention networks for speaker
  verification anti-spoofing and speech deepfake detection,'' \emph{Automatic
  Speaker Verification and Spoofing Countermeasures Challenge}, 2021.

\bibitem{tak2021endrawnet2}
H.~Tak, J.~Patino, M.~Todisco, A.~Nautsch, N.~Evans, and A.~Larcher,
  ``End-to-end anti-spoofing with rawnet2,'' in \emph{ICASSP 2021-2021 IEEE
  International Conference on Acoustics, Speech and Signal Processing
  (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2021, pp. 6369--6373.

\bibitem{Chung2020InDO}
J.~S. Chung, J.~Huh, S.~Mun, M.~Lee, H.-S. Heo, S.~Choe, C.~Ham, S.-Y. Jung,
  B.-J. Lee, and I.~Han, ``In defence of metric learning for speaker
  recognition,'' in \emph{Interspeech}, 2020, pp. 2977--2981.

\bibitem{heo2020clova}
H.~S. Heo, B.-J. Lee, J.~Huh, and J.~S. Chung, ``Clova baseline system for the
  voxceleb speaker recognition challenge 2020,'' \emph{arXiv preprint
  arXiv:2009.14153}, 2020.

\bibitem{desplanques2020ecapa}
B.~Desplanques, J.~Thienpondt, and K.~Demuynck, ``Ecapa-tdnn: Emphasized
  channel attention, propagation and aggregation in tdnn based speaker
  verification,'' pp. 3830--3834, 2020.

\bibitem{Cozzolino_2023_CVPR}
D.~Cozzolino, A.~Pianese, M.~Nie{\ss}ner, and L.~Verdoliva, ``Audio-visual
  person-of-interest deepfake detection,'' in \emph{Proceedings of the IEEE/CVF
  Conference on Computer Vision and Pattern Recognition (CVPR) Workshops}, June
  2023, pp. 943--952.

\bibitem{van2008visualizing}
L.~Van~der Maaten and G.~Hinton, ``Visualizing data using t-sne.''
  \emph{Journal of machine learning research}, vol.~9, no.~11, 2008.

\end{thebibliography}
