{
  "title": "Multiple Instance Learning Framework with Masked Hard Instance Mining for Whole Slide Image Classification",
  "authors": [
    "Wenhao Tang",
    "Sheng Huang",
    "Xiaoxian Zhang",
    "Fengtao Zhou",
    "Yi Zhang",
    "Bo Liu"
  ],
  "submission_date": "2023-07-28T01:40:04+00:00",
  "revised_dates": [
    "2023-07-31T12:24:42+00:00",
    "2023-12-21T02:07:58+00:00"
  ],
  "abstract": "The whole slide image (WSI) classification is often formulated as a multiple instance learning (MIL) problem. Since the positive tissue is only a small fraction of the gigapixel WSI, existing MIL methods intuitively focus on identifying salient instances via attention mechanisms. However, this leads to a bias towards easy-to-classify instances while neglecting hard-to-classify instances. Some literature has revealed that hard examples are beneficial for modeling a discriminative boundary accurately. By applying such an idea at the instance level, we elaborate a novel MIL framework with masked hard instance mining (MHIM-MIL), which uses a Siamese structure (Teacher-Student) with a consistency constraint to explore the potential hard instances. With several instance masking strategies based on attention scores, MHIM-MIL employs a momentum teacher to implicitly mine hard instances for training the student model, which can be any attention-based MIL model. This counter-intuitive strategy essentially enables the student to learn a better discriminating boundary. Moreover, the student is used to update the teacher with an exponential moving average (EMA), which in turn identifies new hard instances for subsequent training iterations and stabilizes the optimization. Experimental results on the CAMELYON-16 and TCGA Lung Cancer datasets demonstrate that MHIM-MIL outperforms other latest methods in terms of performance and training cost. The code is available at: https://github.com/DearCaat/MHIM-MIL.",
  "categories": [
    "cs.CV",
    "cs.AI"
  ],
  "primary_category": "cs.CV",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.15254",
  "pdf_url": "https://arxiv.org/pdf/2307.15254v3",
  "comment": "Published on ICCV2023",
  "num_versions": null,
  "size_before_bytes": 23107690,
  "size_after_bytes": 3604572
}