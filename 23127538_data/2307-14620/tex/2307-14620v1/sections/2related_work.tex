\section{Related Work}

\noindent\textbf{3D Detection in Indoor Scene.} 
%3D detection develops various methods depending on its inputs. 
3D detection utilizes various methods depending on inputs, and has achieved great success on point cloud~\cite{nie2020rfd, imvotenet, votenet, qi2018frustum, zhang2020h3dnet} and voxel representations~\cite{yi2018gspn, hou2020revealnet, hou20193d,engelmann20203d, hou2021exploring}. 3D-SIS~\cite{hou20193d} uses anchors to predict 3D bounding boxes from voxels fused by color and geometric features. The widely-used VoteNet~\cite{qi2019deep} proposes hough voting to regresses bounding box parameters from point features. 
However, depth sensors are not always readily available on many devices due to its huge power consumption, such as on VR/AR headsets. 
To get rid of sensor depth, Panoptic3D~\cite{dahnert2021panoptic} operates on point clouds extracted from predicted depth. Cube R-CNN~\cite{brazil2022omni3d} directly regresses 3D bounding boxes from a single 2D image. 

Comparably, the multi-view approach is also not limited by depth sensors and is more accurate. However, the current state-of-the-art multi-view method~\cite{rukhovich2022imvoxelnet} fuses the image naively by duplicating pixel features along the ray, which does not incorporate a sufficient amount of geometric clues. To address this, we leverage NeRF to embed geometry into the volume for better 3D detection.

\noindent\textbf{3D Reconstruction with Neural Radiance Field.}
Neural Radiance Field (NeRF)~\cite{mildenhall2021nerf} is a groundbreaking 3D scene representation that emerged three years ago, and has proven to be powerful for reconstructing 3D geometry from multi-view images~\cite{mildenhall2021nerf,barron2021mip,yariv2021volume,zhao2022human,wang2022neuris}. Early works~\cite{mildenhall2021nerf,barron2021mip,li2022tava,peng2021animatable,yu2021plenoctrees} directly optimize for per-scene density fields using differentiable volumetric rendering~\cite{max1995optical}. 
Later, NeuS~\cite{wang2021neus} and VolSDF~\cite{yariv2021volume} improve the reconstruction quality by using SDF as a replacement of density as the geometry representation. Recently, Ref-NeRF~\cite{verbin2022ref} proposes to use reflective direction for better geometry of glossy objects. Aside from aforementioned per-scene optimization methods, there are also works that aim to learn a generalizable NeRF from multiple scenes, such as IBRNet~\cite{wang2021ibrnet} and MVS-NeRF~\cite{chen2021mvsnerf}, which predict the density and color at each 3D location conditioned on the pixel-aligned image features. Despite this amazing progress, all of these methods only focus on a single task -- either novel-view synthesis or surface reconstruction. In contrast, we propose a novel method that incorporates NeRF seamlessly to improve 3D detection.


\noindent\textbf{NeRF for Perception.} Being able to capture accurate geometry, NeRF has gradually been incorporated into perception tasks such as classification \cite{jeong2022perfception}, segmentation \cite{vora2021nesf,zhi2021place}, detection \cite{hu2022nerf}, instance segmentation \cite{KunduCVPR2022PNF}, and panoptic segmentation \cite{fu2022panoptic}. However, most of them \cite{jeong2022perfception,hu2022nerf,vora2021nesf} follow the pipeline of first-NeRF-then-perception, which not only creates extra cost for perception tasks but also does not sufficiently use volumetric renderings to benefit perception during training. Besides, \cite{zhi2021place,fu2022panoptic} demonstrate that NeRF can significantly improve label efficiency for semantic segmentation by ensuring multi-view consistency and geometry constraints. Our proposed NeRF-Det method incorporates NeRF to ensure multi-view consistency for 3D detection. Through joint end-to-end training for NeRF and detection, no extra overheads are introduced during inference.
