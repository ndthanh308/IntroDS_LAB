\section{Experimental Results}
\label{sec:results}
\input{tables/comparison_with_sota}
\input{tables/arkitscene}

\noindent\textbf{Implementation details.} Our detection branch mainly follows ImVoxelNet, including backbones, detection head, resolutions and training recipe \textit{etc}. Please refer to supplemental material for more details.

Our implementation is based on MMDetection3D~\cite{mmdet3d2020}. To the best of our knowledge, we are the first to implement NeRF in MMDetection3D. We are also the first to conduct NeRF-style novel view synthesis and depth estimation on the whole ScanNet dataset, while prior works only test on a small number of scenes \cite{xu2022point,wei2021nerfingmvs}. The code will be released for future research.

\subsection{Main results}
\input{tables/geometry_modeling}
\paragraph{Quantitative results.}
We compare \OURS{} with point-cloud based methods~\cite{wang2018sgpn,hou20193d,qi2019deep}, RGB-D based methods~\cite{hou20193d,he2017mask} and the state-of-the-art RGB-only method ImVoxelNet~\cite{rukhovich2022imvoxelnet} on ScanNet, as shown in Tab.~\ref{tab:main_result}. 

With ResNet50 as the image backbone, we observe that \texttt{NeRF-Det-R50-1x} outperforms \texttt{ImVoxelNet-R50-1x} by $2.0$ mAP. On top of that, NeRF-Det with depth supervision, denoted as \texttt{NeRF-Det-R50-1x*}, further improves the detection performance by $0.6$ mAP compared to only RGB supervision \texttt{NeRF-Det-R50-1x}.


We denote the total training iterations of ImVoxelNet from the official code as \texttt{1x} in the Tab.~\ref{tab:main_result}. Yet the \texttt{1x} setting only iterates through each scene roughly $36$ times, which is far from sufficient for optimizing the NeRF branch, which requires thousands of iterations to optimize one scene, as indicated in~\cite{mildenhall2021nerf,wang2021ibrnet,barron2021mip,li2022tava}. Thus, we further conduct experiments with \texttt{2x} training iterations to fully utilize the potential of NeRF, and we observe that \texttt{NeRF-Det-R50-2x} reaches $52.0$ mAP, surpassing ImVoxelNet by $3.6$ mAP under the same setting (\texttt{ImVoxelNet-R50-2x}). It is worth mentioning that we do not introduce any extra data/labels to get such an improvement. If we further use depth supervision to train the NeRF branch (\texttt{NeRF-Det-R50-2x*}), the detection branch is further improved by $1.3$ in mAP@.50 as shown in Tab.~\ref{tab:geometry-evaluation}. This validates better geometry modeling (via depth supervision) could be helpful to the 3D detection task. While NeRF-Det provides an efficient method to incorporate depth supervision during the training process, introducing depth supervision in ImVoxelNet is difficult.


Moreover, when substituting ResNet50 with ResNet101, we achieve $52.9$ mAP@.25 on ScanNet, which outperforms ImVoxelNet on the same setting over 3.9 points. With depth supervision, \texttt{NeRF-Det-R101-2x*} reduces the gap between RGB-based method ImVoxelNet~\cite{rukhovich2022imvoxelnet} and point-cloud based method VoteNet~\cite{qi2019deep} by half (from $10$ mAP  $\rightarrow{}$ $5$ mAP). Besides, we conduct experiments on the ARKitScenes (see Tab.~\ref{tab:main_result_arkit}). The 3.1 mAP improvement further demonstrates the effectiveness of our proposed method.


% Figure environment removed

% \vspace{-0.5cm}
\paragraph{Qualitative results.}
We visualize the predicted 3D bounding boxes from \texttt{NeRF-Det-R50-2x} on scene meshes, as shown in Fig.~\ref{fig:bbox_vis_results}. We observe that the proposed method gets accurate detection prediction even on the extremely dense scenes, \textit{e.g.}, the first row and the left of the third row. The chairs are crowded in the room, and some of which are inserted into tables and are heavily occluded. Yet our method is able to detect them accurately. On the other hand, NeRF-Det can also tackle multiple scales, as shown in the second row and the right side of the third row, in which there are variant objects with difference sizes, like garbage bins, chairs, tables, doors, windows, and sofas \textit{etc}.

\vspace{-0.5cm}
\paragraph{Analysis of scene geometry modeling.}
As stated in the method section, we mitigate the ambiguity of the volume representation by learning an opacity field. Furthermore, we explore different scene geometry modeling methods, from using depth map to cost volume \cite{sun2021neucon,yang2020cost}, in Tab.~\ref{tab:geometry-evaluation}. 

\noindent\textit{Using the Depth Map.} In this experiment, we assume we have access to depth maps during \textit{both training and inference}. When building the voxel feature grid, instead of scattering the features on all points along the ray, we only place features to a single voxel cell according to the depth maps. Intuitively, this leads to less ambiguity in the volume representation, so we should observe better performance for detection. As a proof of concept, we first use ground-truth depth that comes with the dataset. This serves as an upper-bound for NeRF-Det as it provides a perfect geometry modeling. It indeed achieves a high detection accuracy of 54.5 mAP@.25 and 28.2 mAP@.50 (see second row), improving the baseline by 6.1 mAP@.25 and 4.5 mAP@.40. But in practice we cannot get access to the ground-truth depth maps. Thus, we try instead to render depth maps using out-of-box geometry reconstruction from NeuralRecon \cite{sun2021neucon}.

The results are shown in the third row of Tab.~\ref{tab:geometry-evaluation}. We observe that the depth estimation from NeuralRecon significantly degenerates the detection performance by $2.3$ mAP@.50 as compared to plain ImVoxelNet, demonstrating that the estimation error of depth maps propagates that inaccuracy through the detection pipeline.

\noindent\textit{Cost Volume.} Next, we compare our method with cost-volume based methods~\cite{yang2020cost,park2022time}. A common way to compute cost volume is using covariance~\cite{yang2020cost} between source view and reference view. This is similar to our method which calculates the variance of multiple input views. Following~\cite{yang2020cost}, we first use several 3D convolution layers to encode the cost volume, then get the probability volume via sigmoid, and multiply it with the feature volume $V^{avg}$. The results are in the fourth row of Tab.~\ref{tab:geometry-evaluation}. We can see that the cost-volume based method improves ImVoxelNet by 0.9 mAP@.25 and 0.7 mAP@.50 respectively. It is noteworthy to mention that if we remove the NeRF branch, our method is very similar to a cost-volume-based method with the differences being: 1) we augment the variance in the cost volume by mean volume and color volume, 2) we use the MLP and opacity function instead of sigmoid to model the scene geometry. The result is shown in the fifth row of Tab.~\ref{tab:geometry-evaluation}. We observe that the result is very close to a cost-volume-based method and that both ours and the cost-volume method lead to improvement over ImVoxelNet. 
% This demonstrates that cost-volume based method can benefit to detection task via better geometry modeling.

In contrast to explicitly estimating the depth and cost volume, we leverage NeRF to estimate the opacity field for the scene geometry. As shown in the gray part of Tab.~\ref{tab:geometry-evaluation}, with the opacity field modeled using NeRF, the performance is significantly improved by +3.6 mAP@.25 and +2.5 mAP@.50 compared to the baseline. After using depth supervision, NeRF-Det is able to achieve larger improvement with +3.7 mAP@.50 (the last row). As shown in Tab.~\ref{tab:geometry-evaluation}, our method of using NeRF to model scene geometry is more effective than using predicted depth or cost volume.

\input{tables/nerfrpn-compare}

\paragraph{Comparison to NeRF-then-Det method.} We compare our proposed NeRF-Det, a joint NeRF-and-Det method, to NeRF-RPN \cite{hu2022nerf} which is a NeRF-then-Det method, as shown in Tab.~\ref{tab:compare_with_nerf-then-det}. We choose 4 scenes as validation set that are not included in both NeRF-RPN and ScanNet train set. We use the official code and provided pre-trained models of NeRF-RPN to evaluate AP. Experiments in the first group demonstrate that our proposed joint-NeRF-and-Det paradigm is more effective than first-NeRF-then-Det method NeRF-RPN, with much faster speed. 

The second group of Tab.~\ref{tab:compare_with_nerf-then-det} shows that directly using our model (NeRF-Det-R50-2x in Tab. \ref{tab:main_result}) has drastic improvements relative to NeRF-RPN. Although our model is trained on large-scale dataset, we emphasize that this is our advantages since it is hard to apply NeRF-RPN on the whole ScanNet ($\thicksim$1500 scenes) given the heavy overhead. 

% \vspace{-0.55cm}
\paragraph{Is NeRF branch able to learn scene geometry?} We hypothesize that the better detection performance comes from better geometry. To verify this, we perform novel view synthesis and depth estimation from the prediction of the NeRF branch. The underlying assumption is that if our model has correctly inferred the scene geometry, we should be able to synthesize RGB and depth views from novel camera positions. We first visualize some of the synthesized images and depth estimation in Fig.~\ref{fig:nerf_vis}. The image and depth map quality look reasonable and reflects the scene geometry well.

Quantitatively, we evaluate novel-view synthesis and depth estimation by following the protocol of IBRNet \cite{wang2021ibrnet}. We select 10 novel views and randomly sample 50 nearby source views. Then, we average the evaluation results of the 10 novel views for each scene, and finally report average results on all 312 scenes in the validation set, as shown in Tab.~\ref{tab:nerf_evaluate}. Although novel-view synthesis and depth estimation are not the main focus of this paper, we achieve an average of 20+ PSNR for novel view synthesis, without per-scene training. For depth estimation, we achieves an RMSE of 0.756 on all scenes. While this performance falls behind state-of-the-art depth estimation methods, we note that \cite{wei2021nerfingmvs} reported average RMSE of 1.0125 and 1.0901 using Colmap \cite{schoenberger2016sfm} and vanilla NeRF  \cite{mildenhall2021nerf} for depth estimation on selected scenes in ScanNet. This comparison verifies that our method can render reasonable depth. 


% Figure environment removed


\subsection{Ablation Study}
We conduct multiple ablation studies on how different components affect the performance of NeRF-Det, including different feature sampling strategies, whether to share G-MLP, different losses and different features feed into the G-MLP. Besides, although novel-view synthesis is not our focus in this paper, we also provide some analysis for the novel-view synthesis results coming out of the NeRF branch, and show how the NeRF branch is influenced by the detection branch during joint training. All experiments in the ablation studies are based on \texttt{NeRF-Det-R50-1x*}.

\input{tables/ablation_on_featuer_sample}
\vspace{-0.55cm}
\paragraph{Ablation on G-MLP and feature sampling strategy.} As indicated in Sec.~\ref{sec:nerf_branch}, the key ingredient in our pipeline is a shared G-MLP which enables the constraint of multi-view consistency to be propagated from NeRF branch to detection branch. We conduct an ablation study as shown in Tab. \ref{tab:feature_sample}. Without shared G-MLP, the performance drops drastically from 50.1 to 48.1 at mAP@.25, shown in the fifth row of Tab. \ref{tab:geometry-evaluation}. In this case, the influence of multi-view consistency is only propagated into image backbone, significantly limiting the improvement created by NeRF.

Moreover, as mentioned in Sec.~\ref{sec:nerf_branch}, we sample point features along the ray from the multi-view image features instead of the low-resolution volume features. This ablation is shown in Tab.~\ref{fig:feature_sampling}. We observe that with shared G-MLP, both approaches outperform the baseline ImVoxelNet, and sampling from image features yields better performance ($+0.7$ in mAP@0.25) than sampling from volume features. 
% sampling features from 3D volume still improves the performance (1.9 mAP@.25 compared to ImVoxelNet), yet the performance gain is not as large as sampling features onto ray from multi-view 2D features.
For the novel-view synthesis task using NeRF branch, sampling from image features achieves $20.51$ in PSNR comparing to $18.93$ with volume sampling.

The fact that the performance of the NeRF branch is proportional to the performance of the detection branch also indicates that better NeRF optimization could lead to better detection results. 
% This indicates better optimizing NeRF also benefits detection. 



\input{tables/ablation_study_no_nerf_related_loss}
\vspace{-0.5cm}
\paragraph{Ablation study on different loss.} We study how different losses work for the NeRF branch, as shown in Tab.~\ref{tab:ablation_loss}. It shows that with only photo-metric loss, the performance is closed to purely using depth supervision (third row) in term of mAP@.50, indicating that the multi-view RGB consistency already provides sufficient geometry cues to enable the NeRF branch to learn geometry. When using both photo-metric loss and depth loss, the performance can be further improved. When neither photo-metric loss nor depth loss is used (last row), the performance falls back to that of a cost-volume based method. The performance is dropped by 1.2 mAP@.25 and 0.5 mAP@.50, which demonstrates that the NeRF branch is more effective.

\input{tables/ablation_on_features}

\vspace{-0.5cm}
\paragraph{Ablation study on different features.} We then study how different features affect performance, as shown in Tab. \ref{tab:ablation_feature}. The experiment shows that introducing variance features improves performance significantly -- over 0.7 mAP@.25 and 0.6 mAP@.50 compared to using only average features, which demonstrates that variance features indeed provide a good geometry prior. Moreover, incorporating image features also improves performance, indicating that low-level color information also provides good geometry cues.

\input{tables/NeRF_evaluate}
\vspace{-0.5cm}
\paragraph{Ablation Study on detection branch affecting novel view synthesis.} We keep the target views and source views the same with and without the detection branch. Results are shown in Tab.~\ref{tab:nerf_evaluate}. While the NeRF branch significantly improves 3D detection by improving scene geometry modeling, the detection branch does not benefit the NeRF branch. In fact, disabling the detection branch brings a 0.43db improvement. We hypothesize that the detection branch is prone to erase low-level details which are needed for the NeRF, which we aim to address in our future work.