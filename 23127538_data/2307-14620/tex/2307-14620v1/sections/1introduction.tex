\section{Introduction}
\label{sec:intro}
In this paper, we focus on the task of indoor 3D object detection using posed RGB images. 3D object detection is a fundamental task for many computer vision applications such as robotics and AR/VR. The algorithm design depends on input sensors. In the past few years, most 3D detection works focus on both RGB images and depth measurements (depth images, point-clouds, \etc). While depth sensors are widely adopted in applications such as autonomous driving, they are not readily available in most AR/VR headsets and mobile phones due to cost, power dissipation, and form factor constraints. Excluding depth input, however, makes 3D object detection significantly more challenging, since we need to understand not only the semantics, but also the underlying scene geometry from RGB-only images. 

To mitigate the absence of geometry, one straightforward solution is to estimate depth. However, depth estimation itself is a challenging and open problem. For example, most monocular depth-estimation algorithms cannot provide accurate metric depth or multi-view consistency \cite{hoiem2005automatic,saxena2008make3d,karsch2014depth,Ranftl2022}. Multi-view depth-estimation algorithms can only estimate reliable depth in textured and non-occluded regions~\cite{CGV-052, seitz2006comparison}. 

Alternatively, ImVoxelNet \cite{rukhovich2022imvoxelnet} models the scene geometry implicitly by extracting features from 2D images and projecting them to build a 3D volume representation. However, such a geometric representation is intrinsically ambiguous and leads to inaccurate detection.


On the other hand, Neural Radiance Field (NeRF) \cite{mildenhall2021nerf,chen2021mvsnerf,chen2021mvsnerf} has been proven to be a powerful representation for geometry modeling. However, incorporating NeRF into the 3D detection pipeline is a complex undertaking for several reasons:
\begin{enumerate*}[label=(\roman*)]
\item Rendering a NeRF requires high-frequency sampling of the space to avoid aliasing issues \cite{mildenhall2021nerf}, which is challenging in the 3D detection pipeline due to limited resolution volume.
\item Traditional NeRFs are optimized on a per-scene basis, which is incompatible with our objective of image-based 3D detection due to the considerable latency involved.
\item NeRF makes full use of multi-view consistency to better learn geometry during training. However, a simple stitch of first-NeRF-then-perception \cite{vora2021nesf,hu2022nerf,jeong2022perfception} (i.e., reconstruction-then-detection) does not bring the advantage of multi-view consistency to the detection pipeline.
\end{enumerate*}


To mitigate the issue of ambiguous scene geometry, we propose \OURS{} to explicitly model scene geometry as an opacity field by jointly training a NeRF branch with the 3D detection pipeline. Specifically, we draw inspirations from \cite{wang2021ibrnet,yu2020pixelnerf} to project ray samples onto the image plane and extract features from the high-resolution image feature map, rather than from the low-resolution volumes, thereby overcoming the need for high-resolution volumes. To further enhance the generalizability of NeRF model to unseen scenes, we augment the image features with more priors as the input to the NeRF MLP, which leads to more distinguishable features for NeRF modeling. Unlike previous works that build a simple stitch of NeRF-then-perception, we connect the NeRF branch with the detection branch through a \textit{shared} MLP that predicts a density field, subtly allowing the gradient of NeRF branches to back-propagate to the image features and benefit the detection branch during training. We then take advantage of the uniform distribution of the volume and transform the density field into an opacity field and multiply it with the volume features. This reduces the weights of empty space in the volume feature. Then, the geometry-aware volume features are fed to the detection head for 3D bounding box regression. It is worth noting that during inference, the NeRF branch is removed, which minimizes the additional overhead to the original detector.


Our experiments show that by explicitly modeling the geometry as an opacity field, we can build a much better volume representation and thereby significantly improve 3D detection performance. Without using depth measurements for training, we improve the state-of-the-art by $3.9$ and $3.1$ mAP on the ScanNet and the ARKITScenes datasets, respectively. 
Optionally, if depth measurements are also available for training, we can further leverage depth to improve the performance, while our inference model still does not require depth sensors.
Finally, although novel-view synthesis and depth estimation are not our focus, our analysis reveals that our method can synthesize reasonable novel-view images and perform depth prediction without per-scene optimization, which validates that our 3D volume features can better represent scene geometry.
