\section{Experiments}
\subsection{Experimental Settings}
\paragraph{Comparison methods.}
\input{contents/comparison/comp.tex}
\input{tab/usr.tex}
We compare our proposed method with the following baseline methods using the Nvidia Dynamic Scenes Dataset~\cite{yoon2020novel}:
%(1) Gao~\etal. 我们将编辑的区域反投影到3D空间中，
(1) Gao~\etal~\cite{Gao2021DynNeRF} is a widely used method for modeling dynamic scenes. 
% 为了让他们支持编辑.我们做了以下修改：
To support editing, we make the following modifications:
%我们用scene flow传播到其他时刻的3D空间中，投影到2D上监督 the time varying dynamic NeRF.
we project 3D scene flows onto 2D space to render optical flows, and use optical flows to warp the edited content to other frames in the training video.
% we unproject the edited region to 3D space, and use scene flows to propagate the edited region to other frames.
After obtaining the edited training video, we use it to supervise the time-varying dynamic NeRF.
% this baseline 被记作 Gao~\etal~\cite{Gao2021DynNeRF}$^*$.
This baseline is abbreviated as ``SF+DynNeRF''.
% , we project the propagated region to 2D space and use it to supervise the time varying dynamic NeRF.
%(2) 光流+动态NeRF是另一个强力的baseline
(2) ``OF+DynNeRF'' is another strong baseline.
%我们将编辑在reference image上的区域用光流warp到其他图片上，再用编辑后的图片监督 the time varying dynamic NeRF。
We employ optical flows predicted by the RAFT~\cite{teed2020raft} to warp the edited content in the reference image to other frames in the training video 
and use the edited video to supervise the time-varying dynamic NeRF.
%我们选择 Gao~\etal~\cite{Gao2021DynNeRF}为这个baseline的dynamic NeRF。
We choose Gao~\etal~\cite{Gao2021DynNeRF} as the dynamic NeRF for this baseline.
This baseline is abbreviated as ``OF+DynNeRF''.
%(3) Nerfies. Nerfies是为单目视频设计的建模non-rigidly deforming scene的常用算法。他将non-rigidly deforming scene建模成一个标准空间和一组形变场。我们直接用编辑的图片监督Nerfies的表示标准空间的MLP。
(3) Nerfies~\cite{park2021nerfies} is a commonly used algorithm for modeling non-rigidly deforming scenes for monocular videos. 
It models the non-rigidly deforming scene as a canonical space and a set of deformation fields. 
To edit Nerfies, we finetune it using the single edited image.
To prevent overfitting, we freeze the parameters of the deformation fields and only train the MLP representing the canonical space.
%(4) Hypernerf. Hypernerf是nerfies的扩展工作，利用 a family of higher-dimensional spaces to handle topological variations. 我们直接用编辑的图片监督Hypernerf的表示标准空间的MLP。
(4) HyperNeRF~\cite{park2021hypernerf} 
extends Nerfies and adopts a family of higher-dimensional spaces to handle topological variations.
HyperNeRF is also finetuned on the single edited image for editing. We only train the MLP that represents the canonical space while freezing the parameters of other components.
% To edit HyperNeRF, we also finetune it on the single edited image.
% We also only train the MLP representing the canonical space and freeze the parameters of other components.

\paragraph{Datasets.}
% 我们在3个datasets上验证了我们的算法，以便于体现了我们算法可以广泛应用于各种类型的动态NeRF。
We validate our algorithm on three datasets to demonstrate its wide applicability to various types of dynamic scenes.
% (1) Dynamic Scene Dataset是一个包含9段视频的数据集，被广泛用于衡量dynamic view synthesis这个task的performance。
(1) The Nvidia Dynamic Scenes Dataset~\cite{yoon2020novel} is a dataset containing 9 videos, which is widely used to measure the performance of dynamic view synthesis.
% 我们使用Gao~\etal~\cite{Gao2021DynNeRF}处理过的数据进行比较实验，并且我们也把Gao~\etal作为我们编辑的dynamic NeRF。
We use the data processed by Gao~\etal~\cite{Gao2021DynNeRF} for comparison experiments.
To reconstruct the dynamic scenes in this dataset for editing, we choose the method proposed by Gao~\etal~\cite{Gao2021DynNeRF}.
% 20个用户在这些视频上面edit编辑他们想要的效果，然后给我们的算法和baseline打分。
Twenty users were asked to edit these videos and then provide ratings for our algorithm and baseline.
% Two aspects are considered for users：(1)temporal consistency (2)视觉效果。分数是1-5连续值, where 1 is worse, and 5 is best. 
We consider two aspects for user evaluations: (1) temporal consistency and (2) photo-realism. The scores are integers ranging from 1 to 5, where 1 represents lower quality and 5 represents the highest quality.
% (2) Nerfies-HyperNeRF-Conerf dataset 是 Nerfies,HyperNeRF, Conerf 使用的dataset。 
(2) The Nerfies-HyperNeRF-CoNeRF dataset is utilized by Nerfies~\cite{park2021nerfies}, HyperNeRF~\cite{park2021hypernerf}, and CoNeRF~\cite{kania2022conerf} for their respective research.
%因为他们的capture protocol相似，我们把它们个视作一个dataset。
Since their capture protocols are similar, we treat them as a single dataset. We reconstruct the dynamic scenes using HyperNeRFs on this dataset and apply our method for editing.
% (3) ZJU-mocap是一个被广泛用于测试为人体的新视图生成的数据集。Following以前的工作，我们使用八个同步相机作为训练视角。我们编辑neuralbody验证我们的算法可以被应用于复杂人体数据上。
(3) The ZJU-MoCap~\cite{shuai2022multinb, peng2021neural} is a dataset widely used to test the novel view synthesis for human bodies.
Following previous work~\cite{shuai2022multinb}, we use eight synchronized cameras as training views.
We adopt the Neural Body~\cite{peng2021neural} to reconstruct the dynamic human body and utilize our method to edit it.
%其中我们在Dynamic Scene Dataset展现我们和baseline的qualitative results和user study的结果.
% On the Nvidia Dynamic Scene Dataset, we show our qualitative results and user study results with baselines.
We present our qualitative results and user study results with baselines on the Nvidia Dynamic Scenes Dataset. 
%在Nerfies-HyperNeRF dataset和ZJU-mocap上面展示我们的qualitative results。
On the Nerfies-HyperNeRF-CoNeRF and ZJU-MoCap datasets, we show qualitative results of our method.
\paragraph{Implementation details.}
% 我们使用Adam优化器训练我们的网络with a learning rate of 5e-4.
We use the Adam optimizer to train our network with a learning rate of 5e-4.
% 我们的具体网络架构在补充材料中。
The specific network architecture is in the supplementary material.
% 对于Nvidia Dynamic Scene Dataset，我们的网络需要训练12小时在一张3090上面。
Our network needs to be trained for 12 hours on an NVIDIA RTX 3090 for the Nvidia Dynamic Scenes Dataset, and two NVIDIA RTX 3090 GPUs for the Nerfies-HyperNeRF-CoNeRF and ZJU-MoCap datasets.
% 对于Nerfies-Hypernerf-CoNeRF and ZJU-MoCap dataset，我们需要用2张3090训练12小时。
% $beta$ 和$gamma$的具体的参数选择在补充材料中。
The detailed parameter selection of $\beta$ and $\gamma$ is in the supplementary material.

\subsection{Experiment Results}
\paragraph{Comparisons.}
We present some comparison results for our qualitative evaluation in~\figref{fig:exp}.
% The edited reference images are shown in the first column, the original reference images are shown in the second column,
% and the results at different time steps at novel views are shown in the other columns.
% 我们的算法效果远远好于其他baseline，
Our algorithm performs much better than other baselines.
Using optical flows and scene flows to warp the edited content is prone to accumulate errors, resulting in poor temporal consistency.
Additionally, because ``SF+DynNeRF'' lacks a well-defined and smooth surface, 
%遭受blur和ghosting artifacts的困扰。
the rendered results of ``SF+DynNeRF'' and ``OF+DynNeRF'' at novel views suffer from blur and ghosting artifacts.
% Nerfies和Hypernerf的结果产生了严重的temporal不consistent的现象，由于无法找到准确的correspondence
The results of Nerfies and HyperNeRF also exhibit serious temporal inconsistency due to inaccurate correspondences.
% HyperNeRF容易overfit在single 编辑frame上，严重影响其他帧的效果
Additionally, HyperNeRF tends to overfit on a single edited frame, which seriously affects the results of other frames.
% user study的结果也显示我们的方法更受用户喜欢，在表格中证明。
The user study results also show that our method is  preferred by users, as shown in~\tabref{usr}.
\paragraph{More qualitative results.}
% 在fig中我们展示了Nerfies-HyperNeRF-CoNeRF dataset上面的qualitative results。
In~\figref{nerfies}, we show the qualitative results on the Nerfies-HyperNeRF-CoNeRF dataset.
% 在fig中我们展示了ZJU-mocap dataset上面的qualitative results。
In~\figref{zju}, we show the qualitative results of the ZJU-MoCap dataset.
\subsection{Ablation Studies}
% truck  Skating
\input{tab/ab_corr.tex}

\paragraph{Temporal consistency.}

% 1. PCK
% - s2d
% - lap
% - normalizing flow 
% - scene flow
% 我们 analyze how the proposed components in motion representation affect the temporal consistency.
We analyze how the proposed components in motion representation affect temporal consistency.
% reference image到其他frame的pixel-wise correspondence准确性来衡量temporal consistency，which可以被PCK和EPE这两个指标衡量。
We measure temporal consistency by evaluating pixel-wise correspondence accuracy between the reference image and other frames.
%
% We adopt PCK-T~\cite{truong2021learning} and EPE~\cite{ilg2017flownet} to measure the pixel-wise correspondence accuracy.
% ``T'' in ``PCK-T'' means a given pixel threshold, which is set to 1 and 2 in our experiments.
We adopt PCK-T~\cite{truong2021learning} and EPE~\cite{ilg2017flownet} to measure the pixel-wise correspondence accuracy.
In the context of PCK-T, the letter ``T'' represents a given pixel threshold, which we set to 1 and 2 in our experiments.
% 'Truck'这个scene 在 Nvidia Dynamic Scene Dataset里被我们选中用于这个实验，因为它包含了大面积无纹理平面区域，容易获得dense correspondence的ground thruth并且更具有挑战性。
The ``Truck'' scene in the Nvidia Dynamic Scenes Dataset is selected for this experiment because it contains large areas of textureless planar regions, 
which is easy to obtain the ground truth of dense correspondence and is more challenging.
% 我们通过标注每一帧卡车的平面的四个角点计算homography matrix，进而获得dense correspondence的ground truth。
By annotating the four corners of the truck plane in each frame, the homography matrix is able to be calculated, and the ground truth of dense correspondence can be obtained.
% 我们分别去掉了四个重要的component以验证他们的有效性，分别是feature-based photometric term， laplacian smooth term，scene flow fields 和 invertible networks。 

We remove two important regularizations to verify their effectiveness, including the feature-based photo-metric term and the Laplacian smooth term.
% the ablation experiments of removing feature-based photometric term, Laplacian smooth term, scene flow fields, and invertible networks分别被abbreviated为``w\o FP'', ``w\o Lap'', ``w\o SF'', and ``w\o Inv''。
The ablation experiments of removing the feature-based photo-metric term and Laplacian smooth term are abbreviated as ``Ours w/o FP'' and ``Ours w/o Lap'', respectively.
% 为了证明我们需要用scene flow fields把motion信息蒸馏给invertible networks,我们设计了一个名为``w/o SFF'' baseline.
To demonstrate that we need scene flow fields to distill the motion information to invertible networks, we design a baseline named ``Ours w/o SFF''.
% 这个baseline去掉了scene flow fields，为了监督invertible networks，我们直接将local surface投影到2D平面上，然后RAFT估计的optical flow约束相邻帧的local surface的位置。
This baseline removes the scene flow fields. 
To supervise the invertible networks, we directly project the local surface to a 2D plane and then utilize the optical flow estimated by RAFT to constrain the position of the local surface between adjacent frames.
% invertible networks还被forward warping MLP和backward warping MLP替代，以验证invertible networks的有效性。
In order to show the effectiveness of invertible networks, we propose a baseline abbreviated as ``Ours w/o Inv''.
In this baseline, the invertible networks are replaced by forward-warping MLP and backward-warping MLP, like Disentangled3d~\cite{tewari2022disentangled3d}.
% 为了展示直接用scene flows warp被编辑的区域会有累积误差，我们直接用scene flows warp被编辑区域,which 被取名为 ``SF warping''。
We design a baseline named ``SF warping'' which directly warps the edited region with scene flows to show that there will be cumulative errors.
% As shown in~\tabref{ab_corr},  ``w\o FP'' and ``w\o Lap'' have lower PCK and higher EPE than our method,

As shown in~\tabref{ab_corr},  ``Ours w/o FP'' and ``Ours w/o Lap'' have lower PCK and higher EPE than our method,
% which indicates that the regularizations added on the local surface can improve the temporal consistency.
which indicates that the regularization added on the local surface can improve the temporal consistency.
% the worse performance of ``w\o SFF'' demonstrates that the it is necessary to utilize scene flow fields to distill the motion information to invertible networks
The performance of ``Ours w/o SFF'' demonstrates that it is necessary to utilize scene flow fields to distill the motion information to invertible networks.
% 这是因为使用使用scene flow fields蒸馏信号比起2D supervision是一个更加直接的3D监督信号，
This is because using scene flow fields to distill the motion information is a more direct 3D supervision signal than 2D optical flow  constraints.
% 并且,following NSFF and Gao~\etal~\cite{Gao2021DynNeRF},在scene flow fields训练的时候，temporal photometric consistency和一些regularizations可以被添加帮助scene flow feilds去除噪声。
Additionally, following NSFF~\cite{li2020neural} and Gao~\etal~\cite{Gao2021DynNeRF}, temporal photo-metric consistency and some regularization can be added to help eliminate noise in scene flow fields during training.
% Because our method strictly satisfies cycle consistency, it is less likely to accumulate artifacts and therefore performs better.
Because our method strictly satisfies cycle consistency, it is less likely to accumulate errors and performs better than  ``Ours w/o Inv''.
Our method is better than ``SF warping'',
for we add the regularization on the local surface to eliminate the accumulated errors.
% \todo{more detailed reason?}





% \paragraph{和naive alternatives比较}
% \paragraph{Importance of 处理遮挡.}
\paragraph{Importance of handling occlusion relationship.}
% \todo{remove volume rendering}
% 我们的方法可以把local surface和given dynamic nerf结合起来,一起使用volume rendering公式渲染
\input{contents/occlusion/occ}
Our method combines the local surface and the given dynamic NeRF
and uses the volume rendering equation to render them to handle the occlusion relationship.
% 为了验证这个设计的有效性，我们直接用mesh renderer渲染local surface，然后直接和nerf的结果相加。
To demonstrate the significance of this design,
we directly render the local surface with the mesh renderer
and combine it with the rendered results of the dynamic NeRF.
We abbreviate this baseline as ``Ours w/o Occ''.
As shown in~\figref{occ}, if the occlusion relationship is not handled, the rendered results would be incorrect.