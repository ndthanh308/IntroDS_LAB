\begin{thebibliography}{10}

\bibitem{alamo2019gradient}
{\sc T.~Alamo, P.~Krupa, and D.~Limon}, {\em Gradient based restart {FISTA}},
  in 2019 IEEE 58th Conference on Decision and Control (CDC), IEEE, 2019,
  pp.~3936--3941.

\bibitem{alamo2022restart}
{\sc T.~Alamo, P.~Krupa, and D.~Limon}, {\em Restart of accelerated first order
  methods with linear convergence under a quadratic functional growth
  condition}, IEEE Transactions on Automatic Control,  (2022).

\bibitem{Alamo2019}
{\sc T.~Alamo, D.~Limon, and P.~Krupa}, {\em Restart {FISTA} with global linear
  convergence}, in 2019 18th European Control Conference (ECC), IEEE, 2019,
  pp.~1969--1974.

\bibitem{Attouch2018}
{\sc H.~Attouch and A.~Cabot}, {\em Convergence rates of inertial
  forward-backward algorithms}, SIAM Journal on Optimization, 28 (2018),
  pp.~849--874.

\bibitem{Attouch2016}
{\sc H.~Attouch and J.~Peypouquet}, {\em The rate of convergence of
  {N}esterov's accelerated forward-backward method is actually faster than
  $1/k^2$}, SIAM Journal on Optimization, 26 (2016), pp.~1824--1834.

\bibitem{aujol2020convergence}
{\sc J.-F. Aujol, C.~Dossal, and A.~Rondepierre}, {\em Convergence rates of the
  {H}eavy-{B}all method under the {\l}ojasiewicz property}, Mathematical
  Programming, 198 (2023), pp.~195--254.

\bibitem{aujol2021fista}
{\sc J.-F. Aujol, C.~Dossal, and A.~Rondepierre}, {\em {FISTA} is an automatic
  geometrically optimized algorithm for strongly convex functions},
  Mathematical Programming,  (2023).

\bibitem{FISTArestart}
{\sc J.-F. Aujol, C.~H. Dossal, H.~Labarri{\`e}re, and A.~Rondepierre}, {\em
  {FISTA restart using an automatic estimation of the growth parameter}}.
\newblock HAL preprint:
  \url{https://hal.archives-ouvertes.fr/hal-03153525v3/file/Adaptive\_restart.pdf}.

\bibitem{Beck2017}
{\sc A.~Beck}, {\em First-Order Methods in Optimization}, Society for
  Industrial and Applied Mathematics, Philadelphia, PA, 2017.

\bibitem{beck2009fast}
{\sc A.~Beck and M.~Teboulle}, {\em Fast gradient-based algorithms for
  constrained total variation image denoising and deblurring problems}, IEEE
  transactions on image processing, 18 (2009), pp.~2419--2434.

\bibitem{Beck2009}
{\sc A.~Beck and M.~Teboulle}, {\em A fast iterative shrinkage-thresholding
  algorithm for linear inverse problems}, SIAM journal on imaging sciences, 2
  (2009), pp.~183--202.

\bibitem{Bertero2018}
{\sc M.~Bertero, P.~Boccacci, and V.~Ruggiero}, {\em Inverse Imaging with
  {P}oisson {D}ata}, 2053-2563, IOP, 2018.

\bibitem{Bolte2017}
{\sc J.~Bolte, T.~P. Nguyen, J.~Peypouquet, and B.~W. Suter}, {\em From error
  bounds to the complexity of first-order descent methods for convex
  functions}, Mathematical Programming, 165 (2017), pp.~471--507.

\bibitem{Bonettini2018}
{\sc S.~Bonettini, S.~Rebegoldi, and V.~Ruggiero}, {\em Inertial variable
  metric techniques for the inexact forward--backward algorithm}, SIAM Journal
  on Scientific Computing, 40 (2018), pp.~A3180--A3210.

\bibitem{Calatroni-Chambolle-2019}
{\sc L.~Calatroni and A.~Chambolle}, {\em Backtracking strategies for
  accelerated descent methods with smooth composite objectives}, SIAM journal
  on optimization, 29 (2019), pp.~1772--1798.

\bibitem{CalatroniI3D2021}
{\sc L.~Calatroni, G.~Garrigos, L.~Rosasco, and S.~Villa}, {\em Accelerated
  iterative regularization via dual diagonal descent}, SIAM Journal on
  Optimization, 31 (2021), pp.~754--784.

\bibitem{ChambolleDossal2015}
{\sc A.~Chambolle and C.~Dossal}, {\em On the convergence of the iterates of
  the “fast iterative shrinkage/thresholding algorithm”}, Journal of
  Optimization theory and Applications, 166 (2015), pp.~968--982.

\bibitem{ChambollePock2016}
{\sc A.~Chambolle and T.~Pock}, {\em An introduction to continuous optimization
  for imaging}, Acta Numerica, 25 (2016), pp.~161--319.

\bibitem{CombettesWajs2005}
{\sc P.~L. Combettes and V.~R. Wajs}, {\em Signal recovery by proximal
  forward-backward splitting}, Multiscale Modeling \& Simulation, 4 (2005),
  pp.~1168--1200.

\bibitem{fercoq2019adaptive}
{\sc O.~Fercoq and Z.~Qu}, {\em Adaptive restart of accelerated gradient
  methods under local quadratic growth condition}, IMA Journal of Numerical
  Analysis, 39 (2019), pp.~2069--2095.

\bibitem{Ferocq2016}
{\sc Q.~Fercoq and Z.~Qu}, {\em Restarting accelerated gradient methods with a
  rough strong convexity estimate},  (2016).
\newblock arXiv preprint: \url{https://arxiv.org/pdf/1609.07358}.

\bibitem{florea2020generalized}
{\sc M.~I. Florea and S.~A. Vorobyov}, {\em A generalized accelerated composite
  gradient method: Uniting nesterov's fast gradient method and fista}, IEEE
  Transactions on Signal Processing, 68 (2020), pp.~3033--3048.

\bibitem{garrigos2017convergence}
{\sc G.~Garrigos, L.~Rosasco, and S.~Villa}, {\em Convergence of the
  forward-backward algorithm: beyond the worst-case with the help of geometry},
  Mathematical Programming, 198 (2023), pp.~937--996.

\bibitem{GoldsteinFASTA2014}
{\sc T.~Goldstein, C.~Studer, and R.~Baraniuk}, {\em A field guide to
  forward-backward splitting with a fasta implementation}, 2014,
  \url{https://arxiv.org/abs/1411.3406}.

\bibitem{gonzaga2013fine}
{\sc C.~C. Gonzaga and E.~W. Karas}, {\em Fine tuning nesterov’s steepest
  descent algorithm for differentiable convex programming}, Mathematical
  Programming, 138 (2013), pp.~141--166.

\bibitem{Harmany2012}
{\sc Z.~T. Harmany, R.~F. Marcia, and R.~M. Willett}, {\em This is
  {SPIRAL}-{TAP}: Sparse {P}oisson intensity reconstruction algorithms—theory
  and practice}, IEEE Transactions on Image Processing, 21 (2012),
  pp.~1084--1096.

\bibitem{Lazzaretti2021}
{\sc M.~Lazzaretti, S.~Rebegoldi, L.~Calatroni, and C.~Estatico}, {\em A scaled
  and adaptive {FISTA} algorithm for signal-dependent sparse image
  super-resolution problems}, in Scale Space and Variational Methods in
  Computer Vision, Cham, 2021, Springer International Publishing, pp.~242--253.

\bibitem{Liang2017}
{\sc J.~Liang, J.~Fadili, and G.~Peyr\'{e}}, {\em Activity identification and
  local linear convergence of forward--backward-type methods}, SIAM Journal on
  Optimization, 27 (2017), pp.~408--437.

\bibitem{Liang2022}
{\sc J.~Liang, T.~Luo, and C.-B. Sch\"{o}nlieb}, {\em Improving “fast
  iterative shrinkage-thresholding algorithm”: Faster, smarter, and
  greedier}, SIAM Journal on Scientific Computing, 44 (2022), pp.~A1069--A1091.

\bibitem{Lin2015}
{\sc Q.~Lin and L.~Xiao}, {\em An adaptive accelerated proximal gradient method
  and its homotopy continuation for sparse optimization}, Computational
  Optimization and Applications, 60 (2015), pp.~633--674.

\bibitem{Necoara2019}
{\sc I.~Necoara, Y.~Nesterov, and F.~Glineur}, {\em Linear convergence of first
  order methods for non-strongly convex optimization}, Mathematical
  Programming, 175 (2019), pp.~69--107.

\bibitem{nesterov27method}
{\sc Y.~Nesterov}, {\em A method of solving a convex programming problem with
  convergence rate $o (1/k^2)$}, in Sov. Math. Dokl, vol.~27.

\bibitem{Nesterov1983}
{\sc Y.~Nesterov}, {\em A method for solving the convex programming problem
  with convergence rate ${O}(1/k^2)$}, Soviet Mathematics Doklady, 269 (1983),
  pp.~543--547.

\bibitem{Nesterov2004}
{\sc Y.~Nesterov}, {\em Introductory Lectures on Convex Optimization}, Boston :
  Kluwer Academic Publishers, vol. 87~ed., 2004.

\bibitem{Nesterov2013}
{\sc Y.~Nesterov}, {\em Gradient methods for minimizing composite functions},
  Mathematical Programming, 140 (2013), pp.~125--161.

\bibitem{ochs2015ipiasco}
{\sc P.~Ochs, T.~Brox, and T.~Pock}, {\em ipiasco: Inertial proximal algorithm
  for strongly convex optimization}, Journal of Mathematical Imaging and
  Vision, 53 (2015), pp.~171--181.

\bibitem{Candes2015}
{\sc B.~O'Donoghue and E.~Cand{\`e}s}, {\em Adaptive restart for accelerated
  gradient schemes}, Foundations of Computational Mathematics, 15 (2015),
  pp.~715--732.

\bibitem{Polyak1964}
{\sc B.~T. Polyak}, {\em Some methods of speeding up the convergence of
  iteration methods}, {USSR}computational mathematics and mathematical physics,
  4 (1964), pp.~1--17.

\bibitem{RebegoldiCalatroni2022}
{\sc S.~Rebegoldi and L.~Calatroni}, {\em Scaled, inexact, and adaptive
  generalized {FISTA} for strongly convex optimization}, SIAM Journal on
  Optimization, 32 (2022), pp.~2428--2459.

\bibitem{roulet2020sharpness}
{\sc V.~Roulet and A.~d'Aspremont}, {\em Sharpness, restart, and acceleration},
  SIAM Journal on Optimization, 30 (2020), pp.~262--289.

\bibitem{Scheinberg2014}
{\sc K.~Scheinberg, D.~Goldfarb, and X.~Bai}, {\em Fast first-order methods for
  composite convex optimization with backtracking}, Foundations of
  Computational Mathematics, 14 (2014), pp.~389--417.

\bibitem{VillaSalzo2013}
{\sc S.~Villa, S.~Salzo, L.~Baldassarre, and A.~Verri}, {\em Accelerated and
  inexact forward-backward algorithms}, SIAM Journal on Optimization, 23
  (2013), pp.~1607--1633.

\bibitem{zhang2015restricted}
{\sc H.~Zhang and L.~Cheng}, {\em Restricted strong convexity and its
  applications to convergence analysis of gradient-type methods in convex
  optimization}, Optimization Letters, 9 (2015), pp.~961--979.

\end{thebibliography}
