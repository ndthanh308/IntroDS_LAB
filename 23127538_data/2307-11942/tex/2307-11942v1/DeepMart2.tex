%\usepackage{newtxmath}
%\usepackage{natbib}
%\usepackage{hyperref}       % hyperlinks
% simple URL typesetting
% professional-quality tables
% blackboard math symbols
% compact symbols for 1/2, etc.

\documentclass[mathpazo]{cicp}%
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{units}
\usepackage{subfigure,graphicx}
\usepackage{microtype}
\usepackage{zymacros}
\usepackage{changepage}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{caption}
\usepackage{xcolor}
\usepackage{subfig}
\usepackage{amssymb}
\usepackage{graphicx}%
\setcounter{MaxMatrixCols}{30}
%TCIDATA{OutputFilter=latex2.dll}
%TCIDATA{Version=4.10.0.2345}
%TCIDATA{LastRevised=Friday, July 21, 2023 17:38:28}
%TCIDATA{<META NAME="GraphicsSave" CONTENT="32">}
\DeclareMathOperator\supp{supp}
\makeatletter
\def\changemargin#1#2{\list{}{\rightmargin#2\leftmargin#1}\item[]}
\let\endchangemargin=\endlist
\@ifundefined{showcaptionsetup}{}{ \PassOptionsToPackage{caption=false}{subfig}}
\makeatother
\newcommand{\bs}[1]{\boldsymbol{#1}}
\def \ri {{\rm i}}
\graphicspath{{./figs/}}
\begin{document}

\title{DeepMartNet - A Martingale based Deep Neural Network learning algorithm for
Eigenvalue Problems in High Dimensions \footnote{Date: 7/21/2023, cai@smu.edu} }
\author{Wei Cai\affil{1}}

\address{
\affilnum{1}\ Dept. of Mathematics, Southern Methodist University, Dallas, TX 75275. \\
}
%\emails{{\tt bowang@hunnu.edu.cn} (Bo Wang), {\tt wenzhongz@smu.edu} (Wenzhong Zhang), {\tt cai@smu.edu} (W.~Cai). Date: December 26, 2022.} submitted to CiCP special issue on Machine Learning for Scientific Computing.}


\ams{35Q68, 65N99, 68T07, 76M99}

\begin{abstract}
In this paper, we propose a neural network learning algorithm for finding
eigenvalue and eigenfunction for elliptic operators in high dimensions using
the Martingale property in the stochastic representation for the eigenvalue
problem. A loss function based on the Martingale property can be used for
efficient optimization by sampling the stochastic processes associated with
the elliptic operators. The proposed algorithm can be used for Dirichlet,
Neumann, and Robin eigenvalue problems in bounded or unbounded domains.

\end{abstract}
\maketitle
\date{}



\section{Introduction}

Computing eigenvalue and/or eigenfunctions for elliptic operators is one of
the key tasks for many scientific computing problems, e.g., ground states and
band structure calculation in quantum systems. Neural networks have been
recently explored for such as task. FermitNet \cite{fermitnet} is one of
leading methods using anti-symmetrized neural network wavefunctions in
variational Monte Carlo calculation of eigenvalues. Recently, Han et al
\cite{han20} developed a diffusion Monte Carlo method using the connection
between stochastic process and solution of elliptic equation and the backward
Kolmogrov equation to build a loss function.

In this paper, we use the Martingale problem for the eigenvalue problems and a
loss function using fact that the expectation of a Martingale is constant,
thus among any time locations where the expectation can be approximated by
sampling stochastic processes associated with the elliptic operator.

\section{DeepMartNet - a Martingale based neural network}

In this paper, we will propose a neural network for computing eigenvalue for
elliptic operator in high dimensions as arising from quantum mechanics.

Consider the following eigenvalue problem%

\begin{align}
\mathcal{L}u+V(\mathbf{x})u  &  =\lambda u,\text{ \ }\mathbf{x}\in
\Omega\subset R^{d},\label{evP}\\
\Gamma(u)  &  =0,\text{ \ }\mathbf{x}\in\partial\Omega,\nonumber
\end{align}
where the boundary operator could be one of the following three cases,%

\[
\Gamma(u)=\left\{
\begin{array}
[c]{cc}%
u & \text{Dirichlet}\\
\frac{\partial u}{\partial n} & \text{Neumann}\\
\frac{\partial u}{\partial n}-cu & \text{Robin}%
\end{array}
\right.  ,
\]
a decay condition will be given at $\infty$ if $\Omega=R^{d},$ and the
differential operator $L$ is given as%

\begin{equation}
\mathcal{L}=\mu^{\top}\nabla+\frac{\sigma^{\top}\sigma}{2}\nabla^{\top}%
\nabla\label{gen}%
\end{equation}
and the vector $\mu\mathbf{\in}R^{d},$ matrix $\sigma_{d\times d}$ can be
associated with the drift and diffusion of the following stochastic Ito
process $X_{t}(\omega)$ $\mathbf{\in}$ $R^{d}$ with $L$ as its generator%

\begin{align}
d\mathbf{X}_{t}  &  =\mu dt\mathbf{+}\sigma\mathbf{\cdot}d\mathbf{B}%
_{t}\label{sde}\\
\mathbf{X}_{t}  &  =\mathbf{x}_{0}\in\Omega\nonumber
\end{align}
where $\mathbf{B}_{t}=(B_{t}^{1},\cdots,B_{t}^{d})^{\top}\mathbf{\in}R^{d}$ is
Brownian motion in $R^{d}.$

\subsection{Dirichlet Eigenvalue Problem}

By Ito formula, we have%

\begin{equation}
du(\mathbf{X}_{t})=\mathcal{L}u(\mathbf{X}_{t})dt+\sigma^{\top}\nabla
u(\mathbf{X}_{t})d\mathbf{B}_{t}, \label{ItoP}%
\end{equation}
i.e.,%

\begin{align}
u(\mathbf{X}_{t}) &  =u(x_{0})+\int_{0}^{t}\mathcal{L}u(\mathbf{X}_{s}%
)ds+\int_{0}^{t}\sigma^{\top}\nabla u(\mathbf{X}_{s})d\mathbf{B}%
_{s}\nonumber\\
&  =u(x_{0})+\int_{0}^{t}(\lambda-V(\mathbf{X}_{s}))u(\mathbf{X}_{s}%
)ds+\int_{0}^{t}\sigma(\mathbf{X}_{s})^{\top}\nabla u(\mathbf{X}%
_{s})d\mathbf{B}_{s}.\label{ItoForm}%
\end{align}


Due to the fact that the last Ito integral term in (\ref{ItoForm}) is a
Martingale \cite{klebaner} , therefore the following defines Martingale%

\begin{equation}
M_{t}=u(\mathbf{X}_{t})-u(\mathbf{x}_{0})-\int_{0}^{t}(\lambda-V(\mathbf{X}%
_{s}))u(\mathbf{X}_{s})ds,\label{Mart}%
\end{equation}
which will have a constant expectation for any $s<t,$%

\begin{equation}
E[M_{t}-M_{s}]=0. \label{cExp}%
\end{equation}


In the case of finite domain $\Omega$, $\tau_{\partial\Omega}$ is a stopping
time where $\tau_{\partial\Omega}$ is the first exit time of the process
$X_{t}$ outside $\Omega$, then $M_{t\wedge\tau_{\partial\Omega}}$ is still a
Martingale \cite{klebaner}, thus%

\begin{equation}
E[M_{t\wedge\tau_{\partial\Omega}}-M_{s\wedge\tau_{\partial\Omega}}]=0.
\label{cExp1}%
\end{equation}


\begin{remark}
We could define a different generator $\mathcal{L}$ by not including ${\mu}%
^{\top}\nabla$ in (\ref{gen}), then the Martingale in (\ref{Mart1}) will be
changed to
\end{remark}%

\begin{equation}
M_{t}^{\ast}=u(\mathbf{X}_{t})-u(\mathbf{x}_{0})-\int_{0}^{t}(\lambda
-\mu^{\top}(\mathbf{X}_{s})\nabla-V(\mathbf{X}_{s}))u(\mathbf{X}%
_{s})ds,\label{Mart1}%
\end{equation}
where the process $\mathbf{X}_{t}$ is given by $d\mathbf{X}_{t}=\sigma
\mathbf{\cdot}d\mathbf{B}_{t},$instead.

\begin{itemize}
\item \bigskip DeepMartNet for eigenvalue $\lambda$
\end{itemize}

Let $u_{\theta}(\mathbf{x})$ be a neural network which will approximate the
eigenfunction with $\theta$ denoting all the weight and bias parameters, for a
given time interval $[0,T]$, we define a partition%

\[
0=t_{1}<t_{2}<\cdots<t_{i}<t_{i+1}<\cdots<t_{N}=T,
\]
and $M$-discrete realizations of the Ito process using Euler-Maruyama scheme
with $M$-realizations of the Brownian motions $\mathbf{B}_{i}^{(m)}$, $0\leq
m\leq M$, $\mathbf{\ }$%

\begin{align*}
\mathbf{X}_{i+1}^{(m)}  &  =\mathbf{X}_{i}^{(m)}+\mu\mathbf{(X}_{i}%
^{(m)}\mathbf{)}\Delta t_{i}\mathbf{+}\sigma\mathbf{\mathbf{(X}_{i}%
^{(m)}\mathbf{)}\cdot}\Delta\mathbf{B}_{i}^{(m)},\\
\mathbf{X}_{0}^{(m)}  &  =\mathbf{x}_{0}%
\end{align*}
where $\Delta t_{i}=t_{i+1}-t_{i},$%
\[
\Delta\mathbf{B}_{i}^{(m)}=\mathbf{B}_{i+1}^{(m)}-\mathbf{B}_{i}^{(m)}.
\]
Then, the loss function $l(\theta,\lambda)$ for the eigenfunction neural
network $u_{\theta}(\mathbf{x})$ and the eigenvalue $\lambda$ can be defined
as follows based on the%
\begin{align}
l(\theta,\lambda)  &  =l_{\mathbf{x}_{0}}(\theta,\lambda)=\frac{1}{N}%
\sum_{i=0}^{N-1}\left(  E[M_{t_{i+1}}-M_{t_{i}}]\right)  ^{2}+\beta
l_{reg}(\theta)\nonumber\\
&  \simeq\frac{1}{N}\sum_{i=0}^{N-1}\left(  \frac{1}{M}\sum_{m=1}^{M}\left(
u_{\theta}(\mathbf{X}_{i+1}^{(m)})-u_{\theta}(\mathbf{X}_{i}^{(m)}%
)-(\lambda- V(\mathbf{X}_{i}^{(m)}))u_{\theta}(\mathbf{X}_{i}^{(m)})\Delta
t_{i}\right)  \right)  ^{2}+\beta l_{reg}(\theta), \label{loss}%
\end{align}
where the subscript in $l_{\mathbf{x}_{0}}$ indicates all the sampled paths of
the stochastic process starts from $\mathbf{x}_{0}$, and an regularization
term $l_{reg}(\theta)$ is added for specific needs to be discussed later.

The DeepMartNet approximation for the eigenvalue $\lambda\sim\lambda^{\ast}$
will be obtained by minimizing the loss function $l(\theta,\lambda)$ using
stochastic gradient decent,%

\begin{equation}
(\theta^{\ast},\lambda^{\ast})=\arg\min l(\theta,\lambda). \label{Minimiz}%
\end{equation}


\begin{remark}
(regularizer $l_{reg}(\theta)).$ due to non-uniqueness of the eigenvalues, we
will need to introduce a constrain if we intend to compute the lowest
eigen-value (ground state for quantum systems). The Rayleigh energy can be
used for this purpose for zero drift and constant diffusion coefficient%
\begin{equation}
l_{reg}(\theta)=\int_{\Omega}\left(  \nabla^{\top}u_{\theta}\frac
{{\sigma}^{\top}{\sigma}}{2}\nabla u_{\theta}+Vu_{\theta}%
^{2}\right)  dx+{\gamma}\left(  \int_{\Omega}u_{\theta}^{2}%
d\mathbf{x-}1\right)  ^{2},\label{Regul}%
\end{equation}
where 1-normalization factor for the eigenfunction is also included.
\end{remark}


\begin{itemize}
\item \bigskip DeepMartNet for eigenvalue $\lambda$ and eigenfunction $u$
\end{itemize}

As the loss function in (\ref{loss}) only involves paths $\mathbf{X}_{t}$
starting from a fixed point $\mathbf{x}_{0},$ it may not be able to explore
all the state space of the process, therefore the minimization problem in
(\ref{Minimiz}) is expected only to produce a good approximation for the
eigenvalue. To achieve a good approximation to the eigenfunction as well, we will
need to sample the paths of the process $\mathbf{X}_{t}$ from $K$- initial
point $x_{0}^{(k)},1\leq k\leq K,$and define a global loss function%

\begin{equation}
R(\theta,\lambda)=\frac{1}{K}\sum_{k=1}^{K}l_{\mathbf{x}_{0}^{(k)}}%
(\theta,\lambda), \label{loss1}%
\end{equation}
whose minimizer $(\theta^{\ast},\lambda^{\ast})$ is expected to approximate
both the eigenfunction and eigenvalue%

\[
u(x)\sim u_{\theta^{\ast}},\qquad \lambda\sim\lambda^{\ast},
\]
where%

\begin{equation}
(\theta^{\ast},\lambda^{\ast})=\arg\min l(\theta,\lambda). \label{Minimiz1}%
\end{equation}


\subsection{Neuman and Robin Eigenvalue Problem}

We will illustrate the idea for the Robin eigenvalue problem for the simple
case of Laplacian operator,%

\[
\mathcal{L}=\frac{1}{2}\Delta
\]


In probabilistic solutions for Neumann and Robin BVPs, reflecting Brownian
motion will be needed which will go through specular reflections upon hitting
the domain boundary, and a measure of such reflections, the local time of RBM,
will be needed. we will introduce the boundary local time $L(t)$ for
reflecting Brownian motion through a Skorohod problem.

(\textrm{\textbf{Skorohod problem):}} Assume $D$ is a bounded domain in
$R^{d}$ with a $C^{2}$ boundary. Let $f(t)$ be a (continuous) path in $R^{d}$
with $f(0)\in\bar{D}$. A pair $(\xi(t),L(t))$ is a solution to the Skorohod
problem $S(f;D)$ if the following conditions are satisfied:

\begin{enumerate}
\item $\xi$ is a path in $\bar{D}$;

\item $L(t)$ is a nondecreasing function which increases only when $\xi
\in\partial D$, namely,
\begin{equation}
L(t)=\int_{0}^{t}I_{\partial D}(\xi(s))L(ds), \label{sk1}%
\end{equation}


\item The Skorohod equation holds:
\begin{equation}
S(f;D):\qquad\ \xi(t)=f(t)-\int_{0}^{t}n(\xi(s))L(ds), \label{sk2}%
\end{equation}
where $n(x)$ stands for the outward unit normal vector at $x\in\partial D$.
\end{enumerate}

For our case that $f(t)=B_{t}$, the corresponding $\xi_{t}$ will be the
reflecting Brownian motion (RBM) $\mathbf{X}_{t}$. As the name suggests, a RBM
behaves like a BM as long as its path remains inside the domain $D$, but it
will be reflected back inwardly along the normal direction of the boundary
when the path attempts to pass through the boundary. The fact that
$\mathbf{X}_{t}$ is a diffusion process can be proven by using a martingale
formulation and showing that $\mathbf{X}_{t}$ is the solution to the
corresponding martingale problem with the Neumann boundary condition
\cite{hsu84} \cite{Pap1990}.

Due to the fact that RBM $\mathbf{X}_{t}$ is a semimartingale \cite{hsu84}
\cite{Pap1990}, for which the Ito formula \cite{klebaner} give the following%

\begin{equation}
u(\mathbf{X}_{t})=u(x_{0})-\int_{0}^{t}cu(\mathbf{X}_{s})dL(s)-\int_{0}%
^{t}(V(\mathbf{X}_{s})-\lambda)u(\mathbf{X}_{s})ds+\int_{0}^{t}\nabla
u(\mathbf{X}_{s})\cdot d\mathbf{B}_{s},\label{ItoForm1}%
\end{equation}
where an additional path integral term involving the local time $L(s)$ is
added compared with (\ref{ItoForm}). Again, the last term above being a
Martingale, we can define the following Martingale%

\begin{equation}
M_{t}=u(\mathbf{X}_{t})-u(x_{0})+\int_{0}^{t}cu(\mathbf{X}_{s})dL(s)+\int
_{0}^{t}(V(\mathbf{X}_{s})-\lambda)u(\mathbf{X}_{s})ds.\label{Mart2}%
\end{equation}


Using this Martingale, the DeepMartNet for the Drichlet eigevalue problem can
be carried out similarly for the Neumann and Robin eigenvalue problems. The
sampling of reflecting Brownian motion and the computation of local time
$L(t)$ can be found in \cite{ding2023}.

\section{Conclusion}

In this paper, we introduce a Martingale based neural network for finding the
eigenvalue and eigenfunction for general elliptic operators for general types
of boundary conditions. Future numerical experiments will be carried out to
access the efficiency and accuracy of the proposed algorithm, especially in
high dimensions.

\begin{thebibliography}{9}                                                                                                %


\bibitem {fermitnet}Pfau D, Spencer JS, Matthews AG, Foulkes WM. Ab initio
solution of the many-electron Schr\~{A}\P dinger equation with deep neural
networks. Physical Review Research. 2020 Sep 16;2(3):033429.

\bibitem {han20}Han J, Lu J, Zhou M. Solving high-dimensional eigenvalue
problems using deep neural networks: A diffusion Monte Carlo like approach.
Journal of Computational Physics. 2020 Dec 15;423:109792.

\bibitem {hsu84}(Elton) P. Hsu, Reflecting Brownian Motion, Boundary Local
Time and the Neumann Problem, Dissertation Abstracts International Part B:
Science and Engineering [DISS. ABST. INT. PT. B- SCI. ENG.], 45(6), 1984.

\bibitem {Pap1990}V.G. Papanicolaou, The probabilistic solution of the third
boundary value problem for second order elliptic equations, Probab. Theory
Relat. Fields 87 (1990) 27-77.

\bibitem {ding2023}Cuiyang Ding, Yijing Zhou, Wei Cai, Xuan Zeng, and Chanhao
Yan, A Path Integral Monte Carlo (PIMC) Method based on Feynman-Kac Formula
for Electrical Impedance Tomography, Journal of Computational Physics., 476
(2023) 111862. 121.

\bibitem {klebaner}Klebaner FC. Introduction to stochastic calculus with
applications. World Scientific Publishing Company; 2012 Mar 21.
\end{thebibliography}


\end{document}