\begin{thebibliography}{49}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[AlKhamissi et~al.(2022)AlKhamissi, Ladhak, Iyer, Stoyanov, Kozareva,
  Li, Fung, Mathias, Celikyilmaz, and Diab]{alkhamissi-etal-2022-token}
AlKhamissi, B., Ladhak, F., Iyer, S., Stoyanov, V., Kozareva, Z., Li, X., Fung,
  P., Mathias, L., Celikyilmaz, A., and Diab, M.
\newblock {T}o{K}en: Task decomposition and knowledge infusion for few-shot
  hate speech detection.
\newblock In \emph{Proceedings of the 2022 Conference on Empirical Methods in
  Natural Language Processing}, pp.\  2109--2120, Abu Dhabi, United Arab
  Emirates, December 2022. Association for Computational Linguistics.
\newblock URL \url{https://aclanthology.org/2022.emnlp-main.136}.

\bibitem[Anthropic(2023)]{anthropic2023claude}
Anthropic.
\newblock Introducing claude, 2023.
\newblock URL \url{https://www.anthropic.com/index/introducing-claude}.

\bibitem[Bai et~al.(2022)Bai, Jones, Ndousse, Askell, Chen, DasSarma, Drain,
  Fort, Ganguli, Henighan, Joseph, Kadavath, Kernion, Conerly, El-Showk,
  Elhage, Hatfield-Dodds, Hernandez, Hume, Johnston, Kravec, Lovitt, Nanda,
  Olsson, Amodei, Brown, Clark, McCandlish, Olah, Mann, and
  Kaplan]{bai2022training}
Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D.,
  Fort, S., Ganguli, D., Henighan, T., Joseph, N., Kadavath, S., Kernion, J.,
  Conerly, T., El-Showk, S., Elhage, N., Hatfield-Dodds, Z., Hernandez, D.,
  Hume, T., Johnston, S., Kravec, S., Lovitt, L., Nanda, N., Olsson, C.,
  Amodei, D., Brown, T., Clark, J., McCandlish, S., Olah, C., Mann, B., and
  Kaplan, J.
\newblock Training a helpful and harmless assistant with reinforcement learning
  from human feedback.
\newblock arXiv preprint arXiv:2204.05862, 2022.

\bibitem[Chen et~al.(2021)Chen, Tworek, Jun, Yuan, de~Oliveira~Pinto, Kaplan,
  Edwards, Burda, Joseph, Brockman, Ray, Puri, Krueger, Petrov, Khlaaf, Sastry,
  Mishkin, Chan, Gray, Ryder, Pavlov, Power, Kaiser, Bavarian, Winter, Tillet,
  Such, Cummings, Plappert, Chantzis, Barnes, Herbert-Voss, Guss, Nichol,
  Paino, Tezak, Tang, Babuschkin, Balaji, Jain, Saunders, Hesse, Carr, Leike,
  Achiam, Misra, Morikawa, Radford, Knight, Brundage, Murati, Mayer, Welinder,
  McGrew, Amodei, McCandlish, Sutskever, and Zaremba]{chen2021evaluating}
Chen, M., Tworek, J., Jun, H., Yuan, Q., de~Oliveira~Pinto, H.~P., Kaplan, J.,
  Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger,
  G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S.,
  Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter, C.,
  Tillet, P., Such, F.~P., Cummings, D., Plappert, M., Chantzis, F., Barnes,
  E., Herbert-Voss, A., Guss, W.~H., Nichol, A., Paino, A., Tezak, N., Tang,
  J., Babuschkin, I., Balaji, S., Jain, S., Saunders, W., Hesse, C., Carr,
  A.~N., Leike, J., Achiam, J., Misra, V., Morikawa, E., Radford, A., Knight,
  M., Brundage, M., Murati, M., Mayer, K., Welinder, P., McGrew, B., Amodei,
  D., McCandlish, S., Sutskever, I., and Zaremba, W.
\newblock Evaluating large language models trained on code, 2021.

\bibitem[Christiano et~al.(2018)Christiano, Shlegeris, and
  Amodei]{christiano2018supervising}
Christiano, P., Shlegeris, B., and Amodei, D.
\newblock Supervising strong learners by amplifying weak experts.
\newblock arXiv preprint arXiv:1810.08575, 2018.

\bibitem[Creswell et~al.(2023)Creswell, Shanahan, and
  Higgins]{creswell2023selectioninference}
Creswell, A., Shanahan, M., and Higgins, I.
\newblock Selection-inference: Exploiting large language models for
  interpretable logical reasoning.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=3Pf3Wg6o-A4}.

\bibitem[Doshi-Velez \& Kim(2017)Doshi-Velez and Kim]{doshi-velez-2017-towards}
Doshi-Velez, F. and Kim, B.
\newblock Towards a rigorous science of interpretable machine learning.
\newblock arXiv preprint arXiv:1702.08608, 2017.

\bibitem[Dua et~al.(2022)Dua, Gupta, Singh, and
  Gardner]{dua-etal-2022-successive}
Dua, D., Gupta, S., Singh, S., and Gardner, M.
\newblock Successive prompting for decomposing complex questions.
\newblock In \emph{Proceedings of the 2022 Conference on Empirical Methods in
  Natural Language Processing}, pp.\  1251--1265, Abu Dhabi, United Arab
  Emirates, December 2022. Association for Computational Linguistics.
\newblock URL \url{https://aclanthology.org/2022.emnlp-main.81}.

\bibitem[Ferrucci et~al.(2010)Ferrucci, Brown, Chu-Carroll, Fan, Gondek,
  Kalyanpur, Lally, Murdock, Nyberg, Prager, Schlaefer, and
  Welty]{Ferrucci2010BuildingWA}
Ferrucci, D.~A., Brown, E.~W., Chu-Carroll, J., Fan, J., Gondek, D., Kalyanpur,
  A., Lally, A., Murdock, J.~W., Nyberg, E., Prager, J.~M., Schlaefer, N., and
  Welty, C.
\newblock Building watson: An overview of the deepqa project.
\newblock \emph{AI Mag.}, 31:\penalty0 59--79, 2010.

\bibitem[Fu et~al.(2021)Fu, Wang, Zhang, Zhou, and
  Yan]{fu-etal-2021-decomposing-complex}
Fu, R., Wang, H., Zhang, X., Zhou, J., and Yan, Y.
\newblock Decomposing complex questions makes multi-hop {QA} easier and more
  interpretable.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  EMNLP 2021}, pp.\  169--180, Punta Cana, Dominican Republic, November 2021.
  Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.findings-emnlp.17}.
\newblock URL \url{https://aclanthology.org/2021.findings-emnlp.17}.

\bibitem[Gao(2023)]{gao2023shapley}
Gao, L.
\newblock Shapley value attribution in chain of thought.
\newblock
  \url{https://www.lesswrong.com/posts/FX5JmftqL2j6K8dn4/shapley-value-attribution-in-chain-of-thought},
  04 2023.

\bibitem[Geva et~al.(2021)Geva, Khashabi, Segal, Khot, Roth, and
  Berant]{geva2021strategyqa}
Geva, M., Khashabi, D., Segal, E., Khot, T., Roth, D., and Berant, J.
\newblock {Did Aristotle Use a Laptop? A Question Answering Benchmark with
  Implicit Reasoning Strategies}.
\newblock \emph{Transactions of the Association for Computational Linguistics
  (TACL)}, 2021.

\bibitem[Golovneva et~al.(2023)Golovneva, Chen, Poff, Corredor, Zettlemoyer,
  Fazel-Zarandi, and Celikyilmaz]{golovneva2023roscoe}
Golovneva, O., Chen, M.~P., Poff, S., Corredor, M., Zettlemoyer, L.,
  Fazel-Zarandi, M., and Celikyilmaz, A.
\newblock {ROSCOE}: A suite of metrics for scoring step-by-step reasoning.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=xYlJRpzZtsY}.

\bibitem[Guo et~al.(2022)Guo, Li, and Haffari]{guo-etal-2022-complex}
Guo, X.-Y., Li, Y.-F., and Haffari, G.
\newblock Complex reading comprehension through question decomposition.
\newblock In \emph{Proceedings of the The 20th Annual Workshop of the
  Australasian Language Technology Association}, pp.\  31--40, Adelaide,
  Australia, December 2022. Australasian Language Technology Association.
\newblock URL \url{https://aclanthology.org/2022.alta-1.5}.

\bibitem[Hase et~al.(2020)Hase, Zhang, Xie, and Bansal]{hase-etal-2020-leakage}
Hase, P., Zhang, S., Xie, H., and Bansal, M.
\newblock Leakage-adjusted simulatability: Can models generate non-trivial
  explanations of their behavior in natural language?
\newblock In \emph{Findings of the Association for Computational Linguistics:
  EMNLP 2020}, pp.\  4351--4367, Online, November 2020. Association for
  Computational Linguistics.
\newblock \doi{10.18653/v1/2020.findings-emnlp.390}.
\newblock URL \url{https://aclanthology.org/2020.findings-emnlp.390}.

\bibitem[Holtzman et~al.(2020)Holtzman, Buys, Du, Forbes, and
  Choi]{holtzman2020curious}
Holtzman, A., Buys, J., Du, L., Forbes, M., and Choi, Y.
\newblock The curious case of neural text degeneration.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=rygGQyrFvH}.

\bibitem[Jacovi \& Goldberg(2020)Jacovi and
  Goldberg]{jacovi-goldberg-2020-towards}
Jacovi, A. and Goldberg, Y.
\newblock Towards faithfully interpretable {NLP} systems: How should we define
  and evaluate faithfulness?
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics}, pp.\  4198--4205, Online, July 2020.
  Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.acl-main.386}.
\newblock URL \url{https://aclanthology.org/2020.acl-main.386}.

\bibitem[Jung et~al.(2022)Jung, Qin, Welleck, Brahman, Bhagavatula, Le~Bras,
  and Choi]{jung-etal-2022-maieutic}
Jung, J., Qin, L., Welleck, S., Brahman, F., Bhagavatula, C., Le~Bras, R., and
  Choi, Y.
\newblock Maieutic prompting: Logically consistent reasoning with recursive
  explanations.
\newblock In \emph{Proceedings of the 2022 Conference on Empirical Methods in
  Natural Language Processing}, pp.\  1266--1279, Abu Dhabi, United Arab
  Emirates, December 2022. Association for Computational Linguistics.
\newblock URL \url{https://aclanthology.org/2022.emnlp-main.82}.

\bibitem[Kojima et~al.(2022)Kojima, Gu, Reid, Matsuo, and
  Iwasawa]{kojima2022large}
Kojima, T., Gu, S.~S., Reid, M., Matsuo, Y., and Iwasawa, Y.
\newblock Large language models are zero-shot reasoners.
\newblock In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and
  Oh, A. (eds.), \emph{Advances in Neural Information Processing Systems},
  volume~35, pp.\  22199--22213. Curran Associates, Inc., 2022.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2022/file/8bb0d291acd4acf06ef112099c16f326-Paper-Conference.pdf}.

\bibitem[Lanham et~al.(2023)Lanham, Chen, Radhakrishnan, Steiner, Denison,
  Hernandez, Li, Durmus, Hubinger, Kernion, Lukosuite, Nguyen, Cheng, Joseph,
  Schiefer, Rausch, Larson, McCandlish, Kundu, Kadavath, Yang, Henighan,
  Maxwell, Telleen-Lawton, Hume, Hatfield-Dodds, Kaplan, Brauner, Bowman, and
  Perez]{lanham2023transparency}
Lanham, T., Chen, A., Radhakrishnan, A., Steiner, B., Denison, C., Hernandez,
  D., Li, D., Durmus, E., Hubinger, E., Kernion, J., Lukosuite, K., Nguyen, K.,
  Cheng, N., Joseph, N., Schiefer, N., Rausch, O., Larson, R., McCandlish, S.,
  Kundu, S., Kadavath, S., Yang, S., Henighan, T., Maxwell, T., Telleen-Lawton,
  T., Hume, T., Hatfield-Dodds, Z., Kaplan, J., Brauner, J., Bowman, S.~R., and
  Perez, E.
\newblock Measuring faithfulness in chain-of-thought reasoning.
\newblock arXiv preprint (released concurrently), 2023.

\bibitem[Li et~al.(2022)Li, Chen, Shen, Chen, Zhang, Li, Wang, Qian, Peng, Mao,
  Chen, and Yan]{li2022explanations}
Li, S., Chen, J., Shen, Y., Chen, Z., Zhang, X., Li, Z., Wang, H., Qian, J.,
  Peng, B., Mao, Y., Chen, W., and Yan, X.
\newblock Explanations from large language models make small reasoners better.
\newblock arXiv preprint arXiv:2210.06726, 2022.

\bibitem[Lightman et~al.(2023)Lightman, Kosaraju, Burda, Edwards, Baker, Lee,
  Leike, Schulman, Sutskever, and Cobbe]{lightman2023verify}
Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike,
  J., Schulman, J., Sutskever, I., and Cobbe, K.
\newblock Let's verify step by step.
\newblock arXiv preprint arXiv:2305.20050, 2023.

\bibitem[Lin et~al.(2022)Lin, Hilton, and Evans]{lin2022truthfulqa}
Lin, S., Hilton, J., and Evans, O.
\newblock {T}ruthful{QA}: Measuring how models mimic human falsehoods.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pp.\  3214--3252,
  Dublin, Ireland, May 2022. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.acl-long.229}.
\newblock URL \url{https://aclanthology.org/2022.acl-long.229}.

\bibitem[Lyu et~al.(2023)Lyu, Havaldar, Stein, Zhang, Rao, Wong, Apidianaki,
  and Callison-Burch]{lyu2023faithful}
Lyu, Q., Havaldar, S., Stein, A., Zhang, L., Rao, D., Wong, E., Apidianaki, M.,
  and Callison-Burch, C.
\newblock Faithful chain-of-thought reasoning.
\newblock arXiv preprint arXiv 2301.13379, 2023.

\bibitem[Madaan \& Yazdanbakhsh(2022)Madaan and Yazdanbakhsh]{madaan2022two}
Madaan, A. and Yazdanbakhsh, A.
\newblock Text and patterns: For effective chain of thought, it takes two to
  tango, 2022.
\newblock arXiv prepring arXiv:2209.07686.

\bibitem[Mihaylov et~al.(2018)Mihaylov, Clark, Khot, and
  Sabharwal]{OpenBookQA2018}
Mihaylov, T., Clark, P., Khot, T., and Sabharwal, A.
\newblock Can a suit of armor conduct electricity? a new dataset for open book
  question answering.
\newblock In \emph{EMNLP}, 2018.

\bibitem[Min et~al.(2019)Min, Zhong, Zettlemoyer, and Hajishirzi]{min2019multi}
Min, S., Zhong, V., Zettlemoyer, L., and Hajishirzi, H.
\newblock Multi-hop reading comprehension through question decomposition and
  rescoring.
\newblock arXiv preprint arXiv 1906.02916, 2019.

\bibitem[Nakano et~al.(2021{\natexlab{a}})Nakano, Hilton, Balaji, Wu, Ouyang,
  Kim, Hesse, Jain, Kosaraju, Saunders, Jiang, Cobbe, Eloundou, Krueger,
  Button, Knight, Chess, and Schulman]{nakano2021web}
Nakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim, C., Hesse, C.,
  Jain, S., Kosaraju, V., Saunders, W., Jiang, X., Cobbe, K., Eloundou, T.,
  Krueger, G., Button, K., Knight, M., Chess, B., and Schulman, J.
\newblock Webgpt: Browser-assisted question-answering with human feedback.
\newblock arXiv preprint arXiv:2112.09332, 2021{\natexlab{a}}.

\bibitem[Nakano et~al.(2021{\natexlab{b}})Nakano, Hilton, Balaji, Wu, Ouyang,
  Kim, Hesse, Jain, Kosaraju, Saunders, Jiang, Cobbe, Eloundou, Krueger,
  Button, Knight, Chess, and Schulman]{nakano2021webgpt}
Nakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim, C., Hesse, C.,
  Jain, S., Kosaraju, V., Saunders, W., Jiang, X., Cobbe, K., Eloundou, T.,
  Krueger, G., Button, K., Knight, M., Chess, B., and Schulman, J.
\newblock Webgpt: Browser-assisted question-answering with human feedback.
\newblock \emph{CoRR}, abs/2112.09332, 2021{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2112.09332}.

\bibitem[Nye et~al.(2021)Nye, Johan~Andreassen, Gur-Ari, Michalewski, Austin,
  Bieber, Dohan, Lewkowycz, Bosma, Luan, Sutton, and Odena]{nye2021work}
Nye, M., Johan~Andreassen, A., Gur-Ari, G., Michalewski, H., Austin, J.,
  Bieber, D., Dohan, D., Lewkowycz, A., Bosma, M., Luan, D., Sutton, C., and
  Odena, A.
\newblock Show your work: Scratchpads for intermediate computation with
  language models.
\newblock arXiv preprint arXiv:2112.00114, 2021.

\bibitem[Olah(2023)]{olah2023interp}
Olah, C.
\newblock Interpretability {D}reams, 2023.
\newblock URL
  \url{https://transformer-circuits.pub/2023/interpretability-dreams/index.html}.

\bibitem[Patel et~al.(2022)Patel, Mishra, Parmar, and
  Baral]{patel2022questiondecomp}
Patel, P., Mishra, S., Parmar, M., and Baral, C.
\newblock Is a question decomposition unit all we need?
\newblock arXiv preprint arXiv:2205.12538, 2022.

\bibitem[Perez et~al.(2020)Perez, Lewis, Yih, Cho, and
  Kiela]{perez2020unsupervised}
Perez, E., Lewis, P., Yih, W.-t., Cho, K., and Kiela, D.
\newblock Unsupervised question decomposition for question answering.
\newblock In \emph{Conference on Empirical Methods in Natural Language
  Processing}, 2020.

\bibitem[Perez et~al.(2022)Perez, Ringer, Lukošiūtė, Nguyen, Chen, Heiner,
  Pettit, Olsson, Kundu, Kadavath, Jones, Chen, Mann, Israel, Seethor,
  McKinnon, Olah, Yan, Amodei, Amodei, Drain, Li, Tran-Johnson, Khundadze,
  Kernion, Landis, Kerr, Mueller, Hyun, Landau, Ndousse, Goldberg, Lovitt,
  Lucas, Sellitto, Zhang, Kingsland, Elhage, Joseph, Mercado, DasSarma, Rausch,
  Larson, McCandlish, Johnston, Kravec, Showk, Lanham, Telleen-Lawton, Brown,
  Henighan, Hume, Bai, Hatfield-Dodds, Clark, Bowman, Askell, Grosse,
  Hernandez, Ganguli, Hubinger, Schiefer, and Kaplan]{perez2022model}
Perez, E., Ringer, S., Lukošiūtė, K., Nguyen, K., Chen, E., Heiner, S.,
  Pettit, C., Olsson, C., Kundu, S., Kadavath, S., Jones, A., Chen, A., Mann,
  B., Israel, B., Seethor, B., McKinnon, C., Olah, C., Yan, D., Amodei, D.,
  Amodei, D., Drain, D., Li, D., Tran-Johnson, E., Khundadze, G., Kernion, J.,
  Landis, J., Kerr, J., Mueller, J., Hyun, J., Landau, J., Ndousse, K.,
  Goldberg, L., Lovitt, L., Lucas, M., Sellitto, M., Zhang, M., Kingsland, N.,
  Elhage, N., Joseph, N., Mercado, N., DasSarma, N., Rausch, O., Larson, R.,
  McCandlish, S., Johnston, S., Kravec, S., Showk, S.~E., Lanham, T.,
  Telleen-Lawton, T., Brown, T., Henighan, T., Hume, T., Bai, Y.,
  Hatfield-Dodds, Z., Clark, J., Bowman, S.~R., Askell, A., Grosse, R.,
  Hernandez, D., Ganguli, D., Hubinger, E., Schiefer, N., and Kaplan, J.
\newblock Discovering language model behaviors with model-written evaluations.
\newblock arXiv preprint arXiv:2212.09251, 2022.

\bibitem[Reppert et~al.(2023)Reppert, Rachbach, George, Stebbing, Byun,
  Appleton, and Stuhlm{\"u}eller]{reppert2023iterated}
Reppert, J., Rachbach, B., George, C., Stebbing, L., Byun, J., Appleton, M.,
  and Stuhlm{\"u}eller, A.
\newblock Iterated decomposition: Improving science {Q\&A} by supervising
  reasoning processes.
\newblock arXiv preprint arXiv:2301.01751, 2023.

\bibitem[Reynolds \& McDonell(2021)Reynolds and McDonell]{reynolds2021prompt}
Reynolds, L. and McDonell, K.
\newblock Prompt programming for large language models: Beyond the few-shot
  paradigm.
\newblock arXiv preprint arXiv:2102.07350, 2021.

\bibitem[Rudin(2019)]{rudin2019stop}
Rudin, C.
\newblock Stop explaining black box machine learning models for high stakes
  decisions and use interpretable models instead.
\newblock \emph{Nature Machine Intelligence}, 1:\penalty0 206--215, 05 2019.
\newblock \doi{10.1038/s42256-019-0048-x}.

\bibitem[Snell et~al.(2022)Snell, Klein, and Zhong]{snell2022distilling}
Snell, C., Klein, D., and Zhong, R.
\newblock Learning by distilling context.
\newblock arXic preprint arXiv 2209.15189, 2022.

\bibitem[Stuhlm{\"u}eller(2018)]{sthulmueller2018factored}
Stuhlm{\"u}eller, A.
\newblock Factored cognition.
\newblock
  \url{https://www.alignmentforum.org/posts/DFkGStzvj3jgXibFG/factored-cognition},
  12 2018.
\newblock AI Alignment Forum.

\bibitem[Stuhlmüller et~al.(2022)Stuhlmüller, Reppert, and
  Stebbing]{stuhlmueller2022primer}
Stuhlmüller, A., Reppert, J., and Stebbing, L.
\newblock Factored cognition primer.
\newblock \url{https://primer.ought.org}, 2022.

\bibitem[Taylor et~al.(2022)Taylor, Kardas, Cucurull, Scialom, Hartshorn,
  Saravia, Poulton, Kerkez, and Stojnic]{taylor2022galactica}
Taylor, R., Kardas, M., Cucurull, G., Scialom, T., Hartshorn, A., Saravia, E.,
  Poulton, A., Kerkez, V., and Stojnic, R.
\newblock Galactica: A large language model for science, 2022.

\bibitem[Turpin et~al.(2023)Turpin, Michael, Perez, and
  Bowman]{turpin2023language}
Turpin, M., Michael, J., Perez, E., and Bowman, S.~R.
\newblock Language models don't always say what they think: Unfaithful
  explanations in chain-of-thought prompting.
\newblock arXiv preprint arXiv:2305.04388, 2023.

\bibitem[Uesato et~al.(2022)Uesato, Kushman, Kumar, Song, Siegel, Wang,
  Creswell, Irving, and Higgins]{uesato2022solving}
Uesato, J., Kushman, N., Kumar, R., Song, F., Siegel, N., Wang, L., Creswell,
  A., Irving, G., and Higgins, I.
\newblock Solving math word problems with process- and outcome-based feedback.
\newblock arXiv preprint arXiv:2211.14275, 2022.

\bibitem[Wang et~al.(2023)Wang, Xu, Lan, Hu, Lan, Lee, and Lee]{wang2023plan}
Wang, L., Xu, W., Lan, Y., Hu, Z., Lan, Y., Lee, R. K.-W., and Lee, E.-P.
\newblock Plan-and-solve prompting: Improving zero-shot chain-of-thought
  reasoning by large language models.
\newblock arXiv preprint arXiv:2305.04091, 2023.

\bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, ichter, Xia, Chi, Le,
  and Zhou]{wei2022cot}
Wei, J., Wang, X., Schuurmans, D., Bosma, M., ichter, b., Xia, F., Chi, E., Le,
  Q.~V., and Zhou, D.
\newblock Chain-of-thought prompting elicits reasoning in large language
  models.
\newblock In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and
  Oh, A. (eds.), \emph{Advances in Neural Information Processing Systems},
  volume~35, pp.\  24824--24837. Curran Associates, Inc., 2022.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf}.

\bibitem[Wiegreffe et~al.(2021)Wiegreffe, Marasovi{\'c}, and
  Smith]{wiegreffe-etal-2021-measuring}
Wiegreffe, S., Marasovi{\'c}, A., and Smith, N.~A.
\newblock {M}easuring association between labels and free-text rationales.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pp.\  10266--10284, Online and Punta Cana,
  Dominican Republic, November 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.emnlp-main.804}.
\newblock URL \url{https://aclanthology.org/2021.emnlp-main.804}.

\bibitem[Yang et~al.(2018)Yang, Qi, Zhang, Bengio, Cohen, Salakhutdinov, and
  Manning]{yang2018hotpotqa}
Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W.~W., Salakhutdinov, R., and
  Manning, C.~D.
\newblock {HotpotQA}: A dataset for diverse, explainable multi-hop question
  answering.
\newblock In \emph{Conference on Empirical Methods in Natural Language
  Processing ({EMNLP})}, 2018.

\bibitem[Ye \& Durrett(2022)Ye and Durrett]{ye2022the}
Ye, X. and Durrett, G.
\newblock The unreliability of explanations in few-shot prompting for textual
  reasoning.
\newblock In Oh, A.~H., Agarwal, A., Belgrave, D., and Cho, K. (eds.),
  \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=Bct2f8fRd8S}.

\bibitem[Zhou et~al.(2023)Zhou, Sch{\"a}rli, Hou, Wei, Scales, Wang,
  Schuurmans, Cui, Bousquet, Le, and Chi]{zhou2023least}
Zhou, D., Sch{\"a}rli, N., Hou, L., Wei, J., Scales, N., Wang, X., Schuurmans,
  D., Cui, C., Bousquet, O., Le, Q., and Chi, E.
\newblock Least-to-most prompting enables complex reasoning in large language
  models.
\newblock arXiv preprint arXiv:2205.10625, 2023.

\end{thebibliography}
