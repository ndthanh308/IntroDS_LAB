@unpublished{amodei2016,
author = {Amodei, Dario and Olah, Chris and Steinhardt, Jacob and Christiano, Paul and Schulman, John and Man\'e, Dan},
Note = {arXiv preprint arXiv:1606.06565},
Title = {Concrete Problems in {AI} Safety},
year = {2016}
}

@inproceedings{kojima2022large,
 author = {Kojima, Takeshi and Gu, Shixiang (Shane) and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {22199--22213},
 publisher = {Curran Associates, Inc.},
 title = {Large Language Models are Zero-Shot Reasoners},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/8bb0d291acd4acf06ef112099c16f326-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}

@unpublished{nye2021work,
Author = {Nye, Maxwell and Johan Andreassen, Anders and Gur-Ari, Guy and Michalewski, Henryk and Austin, Jacob and Bieber, David and Dohan, David and Lewkowycz, Aitor and Bosma, Maarten and Luan, David and Sutton, Charles and Odena, Augustus},
Note = {arXiv preprint arXiv:2112.00114},
Title = {Show Your Work: Scratchpads for Intermediate Computation with Language Models},
Year = {2021}
}

@unpublished{reynolds2021prompt,
Author = {Reynolds, Laria and McDonell, Kyle},
Note = {arXiv preprint arXiv:2102.07350},
Title = {Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm},
Year = {2021}
}

@unpublished{christiano2018supervising,
Author = {Christiano, Paul and Shlegeris, Buck and Amodei, Dario},
Note = {arXiv preprint arXiv:1810.08575},
Title = {Supervising strong learners by amplifying weak experts
},
Year = {2018}
}

@misc{sthulmueller2018factored,
Author={Stuhlm{\"u}eller, Andreas},
Note = {AI Alignment Forum},
Title = {Factored Cognition},
Year = {2018},
month = {12},
howpublished={\url{https://www.alignmentforum.org/posts/DFkGStzvj3jgXibFG/factored-cognition}}
}

@unpublished{lightman2023verify,
Author={Lightman, Hunter and Kosaraju, Vineet and Burda, Yura and Edwards, Harri and Baker, Bowen and Lee, Tee and Leike, Jan and Schulman, John and Sutskever, Ilya and Cobbe, Karl},
Note = {arXiv preprint arXiv:2305.20050 },
Title = {Let's Verify Step by Step},
Year = {2023}

}

@unpublished{nakano2021web,
Author = {Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christoper and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and Jiang, Xu and Cobbe, Karl and Eloundou, Tyna and Krueger, Gretchen and Button, Kevin and Knight, Matthew and Chess, Benjamin and Schulman, John},
title = {WebGPT: Browser-assisted question-answering with human feedback
},
note = {arXiv preprint arXiv:2112.09332},
year = {2021}
}

@inproceedings{yang2018hotpotqa,
  title={{HotpotQA}: A Dataset for Diverse, Explainable Multi-hop Question Answering},
  author={Yang, Zhilin and Qi, Peng and Zhang, Saizheng and Bengio, Yoshua and Cohen, William W. and Salakhutdinov, Ruslan and Manning, Christopher D.},
  booktitle={Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
  year={2018}
}

@unpublished{perez2022model,
author={Perez, Ethan and Ringer, Sam and Lukošiūtė, Kamilė and Nguyen, Karina and Chen, Edwin and Heiner, Scott and Pettit, Craig and Olsson, Catherine and Kundu, Sandipan and Kadavath, Saurav and Jones, Andy and Chen, Anna and Mann, Ben and Israel, Brian and Seethor, Brian and McKinnon, Cameron and Olah, Christopher and Yan, Da and Amodei, Daniela and Amodei, Dario and Drain, Dawn and Li, Dustin and Tran-Johnson, Eli and Khundadze, Guro and Kernion, Jackson and Landis, James and Kerr, Jamie and Mueller, Jared and Hyun, Jeeyoon and Landau, Joshua and Ndousse, Kamal and Goldberg, Landon and Lovitt, Liane and Lucas, Martin and Sellitto, Michael and Zhang, Miranda and Kingsland, Neerav and Elhage, Nelson and Joseph, Nicholas and Mercado, Noem{\`i} and DasSarma, Nova and Rausch, Oliver and Larson, Robin and McCandlish, Sam and Johnston, Scott and Kravec, Shauna and Showk, Sheer El and Lanham, Tamera and Telleen-Lawton, Timothy and Brown, Tom and Henighan, Tom and Hume, Tristan and Bai, Yuntao and Hatfield-Dodds, Zac and Clark, Jack and Bowman, Samuel R. and Askell, Amanda and Grosse, Roger and Hernandez, Danny and Ganguli, Deep and Hubinger, Evan and Schiefer, Nicholas and Kaplan, Jared},
title = {Discovering Language Model Behaviors with Model-Written Evaluations
},
year = {2022},
note = {arXiv preprint arXiv:2212.09251}
}

@inproceedings{OpenBookQA2018,
 title={Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering},
 author={Todor Mihaylov and Peter Clark and Tushar Khot and Ashish Sabharwal},
 booktitle={EMNLP},
 year={2018}
}

@unpublished{wang2023plan,
Author = {Wang, Lei and Xu, Wanyu and Lan, Yihuai and Hu, Zhiqiang and Lan, Yunshi and Lee, Roy Ka-Wei and Lee, Ee-Peng},
title = {Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models},
note = {arXiv preprint arXiv:2305.04091}, 
year = {2023}
}

@unpublished{zhou2022teaching,
Author = {Zhou, Hattie and Nova, Azade and Larochelle, Hugo and Courville, Aaron and Neyshabur, Behnam and Sedghi, Hanie},
title = {Teaching Algorithmic Reasoning via In-context Learning},
year = {2022},
note = {arXiv preprint arXiv:2211.09066}
}

@unpublished{zhou2023least,
Author = {Zhou, Denny and Sch{\"a}rli, Nathanael and Hou, Le and Wei, Jason and Scales, Nathan and Wang, Xuezhi and Schuurmans, Dale and Cui, Claire and Bousquet, Olivier and Le, Quoc and Chi, Ed},
title = {Least-to-Most Prompting Enables Complex Reasoning in Large Language Models
},
year={2023},
note = {arXiv preprint arXiv:2205.10625}
}

@unpublished{li2022explanations,
Author = {Li, Shiyang and Chen, Jianshu and Shen, Yelong and Chen, Zhiyu and Zhang, Xinlu and Li, Zekun and Wang, Hong and Qian, Jing and Peng, Baolin and Mao, Yi and Chen, Wenhu and Yan, Xifeng},
Note = {arXiv preprint arXiv:2210.06726},
Title = {Explanations from Large Language Models Make Small Reasoners Better},
Year = {2022}
}


@unpublished{wang2022rationaleaugmented,
Author = {Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Zhou, Denny},
Note = {arXiv preprint arXiv:2207.00747},
Title = {Rationale-Augmented Ensembles in Language Models},
Year = {2022}
}

@inproceedings{wei2022cot,
 author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and ichter, brian and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {24824--24837},
 publisher = {Curran Associates, Inc.},
 title = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
 volume = {35},
 year = {2022},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf},
}

@misc{christiano2020current,
author={Christiano, Paul},
year = {2020},
month = {04},
title = {Current work in {AI} alignment},
howpublished={\url{https://forum.effectivealtruism.org/posts/63stBTw3WAW6k45dY/paul-christiano-current-work-in-ai-alignment}},
note= {Effective Altruism Global}
}

@inproceedings{alkhamissi-etal-2022-token,
    title = "{T}o{K}en: Task Decomposition and Knowledge Infusion for Few-Shot Hate Speech Detection",
    author = "AlKhamissi, Badr  and
      Ladhak, Faisal  and
      Iyer, Srinivasan  and
      Stoyanov, Veselin  and
      Kozareva, Zornitsa  and
      Li, Xian  and
      Fung, Pascale  and
      Mathias, Lambert  and
      Celikyilmaz, Asli  and
      Diab, Mona",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.136",
    pages = "2109--2120",
    abstract = "Hate speech detection is complex; it relies on commonsense reasoning, knowledge of stereotypes, and an understanding of social nuance that differs from one culture to the next. It is also difficult to collect a large-scale hate speech annotated dataset. In this work, we frame this problem as a few-shot learning task, and show significant gains with decomposing the task into its {``}constituent{''} parts. In addition, we see that infusing knowledge from reasoning datasets (e.g. ATOMIC2020) improves the performance even further. Moreover, we observe that the trained models generalize to out-of-distribution datasets, showing the superiority of task decomposition and knowledge infusion compared to previously used methods. Concretely, our method outperforms the baseline by 17.83{\%} absolute gain in the 16-shot case.",
}

@inproceedings{perez2020unsupervised,
  title={Unsupervised Question Decomposition for Question Answering},
  author={Perez, Ethan and Lewis, Patrick and Yih, Wen-tau and Cho, Kyunghyun and Kiela, Douwe},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2020}
}

@unpublished{turpin2023language,
Author = {Turpin, Miles and Michael, Julian and Perez, Ethan and Bowman, Samuel R.},  
Note = {arXiv preprint arXiv:2305.04388},
Title = {Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting},
Year = {2023} 
}

@article{rudin2019stop,
author = {Rudin, Cynthia},
year = {2019},
month = {05},
pages = {206-215},
title = {Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead},
volume = {1},
journal = {Nature Machine Intelligence},
doi = {10.1038/s42256-019-0048-x}
}

@article{geva2021strategyqa,
  title = {{Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies}},
  author = {Geva, Mor and Khashabi, Daniel and Segal, Elad and Khot, Tushar and Roth, Dan and Berant, Jonathan},
  journal = {Transactions of the Association for Computational Linguistics (TACL)},
  year = {2021},
}

@unpublished{reppert2023iterated,
title = {Iterated Decomposition: Improving Science {Q\&A} by Supervising Reasoning Processes},
Note = {arXiv preprint arXiv:2301.01751},
year = {2023},
author = {Reppert, Justin and Rachbach, Ben and George, Charlie and Stebbing, Luke and Byun, Jungwon and Appleton, Maggie and Stuhlm{\"u}eller, Andreas}
}

@misc{stuhlmueller2022primer,
  author = {Stuhlmüller, Andreas and Reppert, Justin and Stebbing, Luke},
  title = {Factored Cognition Primer},
  year = 2022,
  howpublished = {\url{https://primer.ought.org}},
  urldate = {2022-12-06}
}

@misc{madaan2022two,
author = {Madaan, Aman and Yazdanbakhsh, Amir},
Note = {arXiv prepring arXiv:2209.07686},
Title={Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango
},
Year={2022}
}



@unpublished{patel2022questiondecomp,
title={Is a Question Decomposition Unit All We Need?},
author = {Patel, Pruthvi and Mishra, Swaroop and Parmar, Mihir and Baral, Chitta},
Note = {arXiv preprint arXiv:2205.12538},
year = {2022}
}

@unpublished{askell2021general,
Author = {Askell, Amanda and Bai, Yuntao and Chen, Anna and Drain, Dawn and Ganguli, Deep and Henighan, Tom and Jones, Andy and Joseph, Nicholas and Mann, Ben and DasSarma, Nova and Elhage, Nelson and Hatfield-Dodds, Zac and Hernandez, Danny and Kernion, Jackson and Ndousse, Kamal and Olsson, Catherine and Amodei, Dario and Brown, Tom and Clark, Jack and McCandlish, Sam and Olah, Chris and Kaplan, Jared},
Note = {arXiv preprint arXiv:2112.00861},
Title = {A General Language Assistant as a Laboratory for Alignment},
Year = {2021}
}

@unpublished{yao2023beyond,
Author = {Yao, Yao and Li, Zuchao and Zhao, Hai}, 
Note = {arXiv preprint arXiv:2305.16582},
Title = {Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in Large Language Models}, 
Year = {2023}
}

@unpublished{creswell2022faithful,
Author = {Creswell, Antonia and Shanahan, Murray},
Note = {arXiv preprint arXiv:2208.14271},
Title = {Faithful Reasoning Using Large Language Models},
Year = {2022}
}

@inproceedings{
creswell2023selectioninference,
title={Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning},
author={Antonia Creswell and Murray Shanahan and Irina Higgins},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=3Pf3Wg6o-A4}
}

@inproceedings{fu-etal-2021-decomposing-complex,
    title = "Decomposing Complex Questions Makes Multi-Hop {QA} Easier and More Interpretable",
    author = "Fu, Ruiliu  and
      Wang, Han  and
      Zhang, Xuejun  and
      Zhou, Jun  and
      Yan, Yonghong",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.17",
    doi = "10.18653/v1/2021.findings-emnlp.17",
    pages = "169--180",
    abstract = "Multi-hop QA requires the machine to answer complex questions through finding multiple clues and reasoning, and provide explanatory evidence to demonstrate the machine{'}s reasoning process. We propose Relation Extractor-Reader and Comparator (RERC), a three-stage framework based on complex question decomposition. The Relation Extractor decomposes the complex question, and then the Reader answers the sub-questions in turn, and finally the Comparator performs numerical comparison and summarizes all to get the final answer, where the entire process itself constitutes a complete reasoning evidence path. In the 2WikiMultiHopQA dataset, our RERC model has achieved the state-of-the-art performance, with a winning joint F1 score of 53.58 on the leaderboard. All indicators of our RERC are close to human performance, with only 1.95 behind the human level in F1 score of support fact. At the same time, the evidence path provided by our RERC framework has excellent readability and faithfulness.",
}

@inproceedings{dua-etal-2022-successive,
    title = "Successive Prompting for Decomposing Complex Questions",
    author = "Dua, Dheeru  and
      Gupta, Shivanshu  and
      Singh, Sameer  and
      Gardner, Matt",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.81",
    pages = "1251--1265",
    abstract = "Answering complex questions that require making latent decisions is a challenging task, especially when limited supervision is available. Recent works leverage the capabilities of large language models (LMs) to perform complex question answering in a few-shot setting by demonstrating how to output intermediate rationalizations while solving the complex question in a single pass. We introduce {``}Successive Prompting{''} where, we iteratively break down a complex task into a simple task, solve it, and then repeat the process until we get the final solution. Successive prompting decouples the supervision for decomposing complex questions from the supervision for answering simple questions, allowing us to (1) have multiple opportunities to query in-context examples at each reasoning step (2) learn question decomposition separately from question answering, including using synthetic data, and (3) use bespoke (fine-tuned) components for reasoning steps where a large LM does not perform well. The intermediate supervision is typically manually written, which can be expensive to collect. We introduce a way to generate synthetic dataset which can be used to bootstrap model{'}s ability to decompose and answer intermediate questions. Our best model (with successive prompting) achieves an improvement in F1 of {\textasciitilde}5{\%} when compared with a state-of-the-art model with synthetic augmentations and few-shot version of the DROP dataset.",
}

@unpublished{doshi-velez-2017-towards,
title = "Towards A Rigorous Science of Interpretable Machine Learning",
author = {Doshi-Velez, Finale and Kim, Been},
year = {2017},
Note = {arXiv preprint arXiv:1702.08608}
}

@inproceedings{guo-etal-2022-complex,
    title = "Complex Reading Comprehension Through Question Decomposition",
    author = "Guo, Xiao-Yu  and
      Li, Yuan-Fang  and
      Haffari, Gholamreza",
    booktitle = "Proceedings of the The 20th Annual Workshop of the Australasian Language Technology Association",
    month = dec,
    year = "2022",
    address = "Adelaide, Australia",
    publisher = "Australasian Language Technology Association",
    url = "https://aclanthology.org/2022.alta-1.5",
    pages = "31--40",
}

@inproceedings{hase-etal-2020-leakage,
    title = "Leakage-Adjusted Simulatability: Can Models Generate Non-Trivial Explanations of Their Behavior in Natural Language?",
    author = "Hase, Peter  and
      Zhang, Shiyue  and
      Xie, Harry  and
      Bansal, Mohit",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.390",
    doi = "10.18653/v1/2020.findings-emnlp.390",
    pages = "4351--4367",
    abstract = "Data collection for natural language (NL) understanding tasks has increasingly included human explanations alongside data points, allowing past works to introduce models that both perform a task and generate NL explanations for their outputs. Yet to date, model-generated explanations have been evaluated on the basis of surface-level similarities to human explanations, both through automatic metrics like BLEU and human evaluations. We argue that these evaluations are insufficient, since they fail to indicate whether explanations support actual model behavior (faithfulness), rather than simply match what a human would say (plausibility). In this work, we address the problem of evaluating explanations from the the model simulatability perspective. Our contributions are as follows: (1) We introduce a leakage-adjusted simulatability (LAS) metric for evaluating NL explanations, which measures how well explanations help an observer predict a model{'}s output, while controlling for how explanations can directly leak the output. We use a model as a proxy for a human observer, and validate this choice with two human subject experiments. (2) Using the CoS-E and e-SNLI datasets, we evaluate two existing generative graphical models and two new approaches; one rationalizing method we introduce achieves roughly human-level LAS scores. (3) Lastly, we frame explanation generation as a multi-agent game and optimize explanations for simulatability while penalizing label leakage, which can improve LAS scores.",
}

@inproceedings{
ye2022the,
title={The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning},
author={Xi Ye and Greg Durrett},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=Bct2f8fRd8S}
}

@inproceedings{
golovneva2023roscoe,
title={{ROSCOE}: A Suite of Metrics for Scoring Step-by-Step Reasoning},
author={Olga Golovneva and Moya Peng Chen and Spencer Poff and Martin Corredor and Luke Zettlemoyer and Maryam Fazel-Zarandi and Asli Celikyilmaz},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=xYlJRpzZtsY}
}

@inproceedings{jung-etal-2022-maieutic,
    title = "Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations",
    author = "Jung, Jaehun  and
      Qin, Lianhui  and
      Welleck, Sean  and
      Brahman, Faeze  and
      Bhagavatula, Chandra  and
      Le Bras, Ronan  and
      Choi, Yejin",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.82",
    pages = "1266--1279",
    abstract = "Pre-trained language models (LMs) struggle with consistent reasoning; recently, prompting LMs to generate explanations that self-guide the inference has emerged as a promising direction to amend this. However, these approaches are fundamentally bounded by the correctness of explanations, which themselves are often noisy and inconsistent. In this work, we develop Maieutic Prompting, which aims to infer a correct answer to a question even from the unreliable generations of LM. Maieutic Prompting induces a tree of explanations abductively (e.g. X is true, because ...) and recursively, then frames the inference as a satisfiability problem over these explanations and their logical relations. We test Maieutic Prompting for true/false QA on three challenging benchmarks that require complex commonsense reasoning. Maieutic Prompting achieves up to 20{\%} better accuracy than state-of-the-art prompting methods, and as a fully unsupervised approach, performs competitively with supervised models. We also show that Maieutic Prompting improves robustness in inference while providing interpretable rationales.",
}

@misc{gao2023shapley,
Author = {Gao, Leo},
title = {Shapley Value Attribution in Chain of Thought},
howpublished = {\url{https://www.lesswrong.com/posts/FX5JmftqL2j6K8dn4/shapley-value-attribution-in-chain-of-thought}},
month = {04},
year = {2023},
}

@inproceedings{wiegreffe-etal-2021-measuring,
    title = "{M}easuring Association Between Labels and Free-Text Rationales",
    author = "Wiegreffe, Sarah  and
      Marasovi{\'c}, Ana  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.804",
    doi = "10.18653/v1/2021.emnlp-main.804",
    pages = "10266--10284",
    abstract = "In interpretable NLP, we require faithful rationales that reflect the model{'}s decision-making process for an explained instance. While prior work focuses on extractive rationales (a subset of the input words), we investigate their less-studied counterpart: free-text natural language rationales. We demonstrate that *pipelines*, models for faithful rationalization on information-extraction style tasks, do not work as well on {``}reasoning{''} tasks requiring free-text rationales. We turn to models that *jointly* predict and rationalize, a class of widely used high-performance models for free-text rationalization. We investigate the extent to which the labels and rationales predicted by these models are associated, a necessary property of faithful explanation. Via two tests, *robustness equivalence* and *feature importance agreement*, we find that state-of-the-art T5-based joint models exhibit desirable properties for explaining commonsense question-answering and natural language inference, indicating their potential for producing faithful free-text rationales.",
}

@unpublished{uesato2022solving,
Author = {Uesato, Jonathan and Kushman, Nate and Kumar, Ramana and Song, Francis and Siegel, Noah and Wang, Lisa and Creswell, Antonia and Irving, Geoffrey and Higgins, Irina},
Title  = {Solving math word problems with process- and outcome-based feedback},
Note = {arXiv preprint arXiv:2211.14275},
year={2022}
}

@unpublished{wu2021recursive,
Author = {Wu, Jeff and Ouyang, Long and Ziegler, Daniel M. and Stiennon, Nisan and Lowe, Ryan and Leike, Jan and Christiano, Paul},
Note = {arXic preprint arXiv:2109.10862},
year={2021},
Title = {Recursively Summarizing Books with Human Feedback}
}



@unpublished{lanham2023transparency,
Author = {Lanham, Tamera and Chen, Anna and Radhakrishnan, Ansh and Steiner, Benoit and Denison, Carson and Hernandez, Danny and Li, Dustin and Durmus, Esin and Hubinger, Evan and Kernion, Jackson and Lukosuite, Kamile and Nguyen, Karina and Cheng, Newton and Joseph, Nicholas and Schiefer, Nicholas and Rausch, Oliver and Larson, Robin and McCandlish, Sam and Kundu, Sandipan and Kadavath, Saurav and Yang, Shannon and Henighan, Thomas and Maxwell, Timothy and Telleen-Lawton, Timothy and Hume, Tristan and Hatfield-Dodds, Zac and Kaplan, Jared and Brauner, Jan and Bowman, Samuel R. and Perez, Ethan},
Note = {arXiv preprint (released concurrently)},
Title = {Measuring Faithfulness in Chain-of-Thought Reasoning},
Year = {2023}
}

@unpublished{wiher2022decoding,
Author = {Wiher, Gian and Meister, Clara and Cotterell, Ryan},
Note = {arXiv preprint arXiv:2203.15721},
Title = {On Decoding Strategies for Neural Text Generators},
Year = {2022}
}

@misc{cotra2018ida,
Author = {Cotra, Ajeya},
Note = {ai-alignment.com},
Title = {Iterated Distillation and Amplification},
Year = {2018},
month = {03},
howpublished = {\url{https://ai-alignment.com/iterated-distillation-and-amplification-157debfd1616}}
}

@unpublished{bai2022training, 
Author = {Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and Joseph, Nicholas and Kadavath, Saurav and Kernion, Jackson and Conerly, Tom and El-Showk, Sheer and Elhage, Nelson and Hatfield-Dodds, Zac and Hernandez, Danny and Hume, Tristan and Johnston, Scott and Kravec, Shauna and Lovitt, Liane and Nanda, Neel and Olsson, Catherine and Amodei, Dario and Brown, Tom and Clark, Jack and McCandlish, Sam and Olah, Chris and Mann, Ben and Kaplan, Jared},  
Note = {arXiv preprint arXiv:2204.05862},
Title = {Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback},
Year = {2022},
}

@misc{olah2023interp,
Author = {Olah, Chris},
Title = {Interpretability {D}reams},
url = "https://transformer-circuits.pub/2023/interpretability-dreams/index.html",
  Year = {2023}
}

@misc{anthropic2023claude,
Author = {Anthropic},
Title = {Introducing Claude},
url = "https://www.anthropic.com/index/introducing-claude",
  Year = {2023}
}

@inproceedings{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Alec Radford and Jeff Wu and Rewon Child and David Luan and Dario Amodei and Ilya Sutskever},
  year={2019},
  url={https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf}
}

@inproceedings{lewis-etal-2020-bart,
    title = "{BART}: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
    author = "Lewis, Mike  and
      Liu, Yinhan  and
      Goyal, Naman  and
      Ghazvininejad, Marjan  and
      Mohamed, Abdelrahman  and
      Levy, Omer  and
      Stoyanov, Veselin  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.703",
    doi = "10.18653/v1/2020.acl-main.703",
    pages = "7871--7880",
    abstract = "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.",
}

@unpublished{lyu2023faithful,
Author = {Lyu, Qing and Havaldar, Shreya and Stein, Adam and Zhang, Li and Rao, Delip and Wong, Eric and Apidianaki, Marianna and Callison-Burch, Chris},
Note = {arXiv preprint arXiv 2301.13379},
Title = {Faithful Chain-of-Thought Reasoning},
Year = {2023}
}

@misc{lanham2022externalized,
Author = {Lanham, Tamera},
howpublished = {\url{https://www.alignmentforum.org/posts/FRRb6Gqem8k69ocbi/externalized-reasoning-oversight-a-research-direction-for}},
month = {08},
year = {2022},
Note = {AI Alignment Forum}
}

@inproceedings{jacovi-goldberg-2020-towards,
    title = "Towards Faithfully Interpretable {NLP} Systems: How Should We Define and Evaluate Faithfulness?",
    author = "Jacovi, Alon  and
      Goldberg, Yoav",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.386",
    doi = "10.18653/v1/2020.acl-main.386",
    pages = "4198--4205",
    abstract = "With the growing popularity of deep-learning based NLP models, comes a need for interpretable systems. But what is interpretability, and what constitutes a high-quality interpretation? In this opinion piece we reflect on the current state of interpretability evaluation research. We call for more clearly differentiating between different desired criteria an interpretation should satisfy, and focus on the faithfulness criteria. We survey the literature with respect to faithfulness evaluation, and arrange the current approaches around three assumptions, providing an explicit form to how faithfulness is {``}defined{''} by the community. We provide concrete guidelines on how evaluation of interpretation methods should and should not be conducted. Finally, we claim that the current binary definition for faithfulness sets a potentially unrealistic bar for being considered faithful. We call for discarding the binary notion of faithfulness in favor of a more graded one, which we believe will be of greater practical utility.",
}

@unpublished{min2019multi,
Author = {Min, Sewon and Zhong, Victor and Zettlemoyer, Luke and Hajishirzi, Hannaneh},
Note = {arXiv preprint arXiv 1906.02916},
Title = {Multi-hop Reading Comprehension through Question Decomposition and Rescoring},
Year = {2019}
}

@unpublished{snell2022distilling,
Author = {Snell, Charlie and Klein, Dan and Zhong, Ruiqi},
Note = {arXic preprint arXiv 2209.15189 },
Title = {Learning by Distilling Context},
Year = {2022}
}

@article{Ferrucci2010BuildingWA,
  title={Building Watson: An Overview of the DeepQA Project},
  author={David A. Ferrucci and Eric W. Brown and Jennifer Chu-Carroll and James Fan and David Gondek and Aditya Kalyanpur and Adam Lally and J. William Murdock and Eric Nyberg and John M. Prager and Nico Schlaefer and Chris Welty},
  journal={AI Mag.},
  year={2010},
  volume={31},
  pages={59-79}
}

@misc{stuhlmueller2022supervise,
howpublished={\url{https://ought.org/updates/2022-04-06-process}},
Year = {2022},
Note = {ought.org},
Title={Supervise Process, not Outcomes},
Author = {Stuhlm{\"u}eller, Andreas and Byun, Jungwon}
}

@inproceedings{lin2022truthfulqa,
    title = "{T}ruthful{QA}: Measuring How Models Mimic Human Falsehoods",
    author = "Lin, Stephanie  and
      Hilton, Jacob  and
      Evans, Owain",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.229",
    doi = "10.18653/v1/2022.acl-long.229",
    pages = "3214--3252",
    abstract = "We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58{\%} of questions, while human performance was 94{\%}. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.",
}

@unpublished{bowman2022measuring, 
Author = {Bowman, Samuel R. and Hyun, Jeeyoon and Perez, Ethan and Chen, Edwin and Pettit, Craig and Heiner, Scott and Lukošiūtė, Kamilė and Askell, Amanda and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and Olah, Christopher and Amodei, Daniela and Amodei, Dario and Drain, Dawn and Li, Dustin and Tran-Johnson, Eli and Kernion, Jackson and Kerr, Jamie and Mueller, Jared and Ladish, Jeffrey and Landau, Joshua and Ndousse, Kamal and Lovitt, Liane and Elhage, Nelson and Schiefer, Nicholas and Joseph, Nicholas and Mercado, Noemí and DasSarma, Nova and Larson, Robin and McCandlish, Sam and Kundu, Sandipan and Johnston, Scott and Kravec, Shauna and El Showk, Sheer and Fort, Stanislav and Telleen-Lawton, Timothy and Brown, Tom and Henighan, Tom and Hume, Tristan and Bai, Yuntao and Hatfield-Dodds, Zac and Mann, Ben and Kaplan, Jared},  
Note = {arXiv preprint arXiv:2211.03540},
Title = {Measuring Progress on Scalable Oversight for Large Language Models},
Year = {2022}
}

@inproceedings{holtzman2020curious,
title={The Curious Case of Neural Text Degeneration},
author={Ari Holtzman and Jan Buys and Li Du and Maxwell Forbes and Yejin Choi},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=rygGQyrFvH}
}


@misc{chen2021evaluating,
      title={Evaluating Large Language Models Trained on Code}, 
      author={Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde de Oliveira Pinto and Jared Kaplan and Harri Edwards and Yuri Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and Dave Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William Hebgen Guss and Alex Nichol and Alex Paino and Nikolas Tezak and Jie Tang and Igor Babuschkin and Suchir Balaji and Shantanu Jain and William Saunders and Christopher Hesse and Andrew N. Carr and Jan Leike and Josh Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
      year={2021},
      eprint={2107.03374},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{nakano2021webgpt,
  author       = {Reiichiro Nakano and
                  Jacob Hilton and
                  Suchir Balaji and
                  Jeff Wu and
                  Long Ouyang and
                  Christina Kim and
                  Christopher Hesse and
                  Shantanu Jain and
                  Vineet Kosaraju and
                  William Saunders and
                  Xu Jiang and
                  Karl Cobbe and
                  Tyna Eloundou and
                  Gretchen Krueger and
                  Kevin Button and
                  Matthew Knight and
                  Benjamin Chess and
                  John Schulman},
  title        = {WebGPT: Browser-assisted question-answering with human feedback},
  journal      = {CoRR},
  volume       = {abs/2112.09332},
  year         = {2021},
  url          = {https://arxiv.org/abs/2112.09332},
  eprinttype    = {arXiv},
  eprint       = {2112.09332},
  timestamp    = {Mon, 03 Jan 2022 15:45:35 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2112-09332.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{taylor2022galactica,
      title={Galactica: A Large Language Model for Science}, 
      author={Ross Taylor and Marcin Kardas and Guillem Cucurull and Thomas Scialom and Anthony Hartshorn and Elvis Saravia and Andrew Poulton and Viktor Kerkez and Robert Stojnic},
      year={2022},
      eprint={2211.09085},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}