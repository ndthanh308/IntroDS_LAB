\section*{Appendix B -- The Secondary Variational Problems}
\label{appendix:b}

In this appendix, we analyze the secondary variational problem (SVP) related to the peer review matching problem.
% 
After studying the properties of the solution when we add a small regularization term, we focus on the diversity functional introduced in \cite{DBLP:conf/ijcai/AhmedDF17}.
% 
In particular, we show how the diversity function they proposed does not directly describe the diversity, but rather an SVP induced by the entropy function.
% 
This allows us to give a theoretical explanation of the large Entropy Gain results presented in \cite{DBLP:conf/ijcai/AhmedDF17}.



\subsection*{Enhancing the solutions Via Second Variational Problem}
% 
Albeit maximizing a linear function has been proven to be a reliable way to find a meaningful peer review matching, determining a matching by only using an ILP model leads towards undesirable features.
% 
First, the solution is often non-unique, which means there is still room for improving the solution.
% 
Second, since the solutions to LP problems lay in the vertexes of a polytope, the maximizers of the objective function lack other appealing properties such as fairness, entropy, and diversity.
% 
For example, the maximum allocation described in Example \ref{ex:fairness} allocates all the best papers to one reviewer and leaves the other reviewer with the worst one.
% 
A classic method to deal with both these issues is to augment the objective function with a (usually convex) function as follows
% 
\begin{equation}
    \label{eq:intro_sect4}
    \langle W,X \rangle + \lambda C(X),
\end{equation}
where $C$ is a function that describes the property we are interested in and $\lambda\in \mathbb{R}$ is a parameter that scales the effect of the function $C$ over the optimization problem.
% 
In this appendix, we show that for every problem \eqref{eq:intro_sect4} there does exist a small enough parameter $\lambda$ for which the maximizers of the function \eqref{eq:intro_sect4} are the matchings that maximizes\footnote{or minimizes, depending on the sign of $\lambda$} the function $C$ over the set of matchings that maximize the quality $\langle W,X \rangle$. 
% 













% 
\begin{definition}
\label{def:model_epsilon_div}
Given a matching problem and a convex and bounded function $C$ over the set of feasible matching, we define the secondary variational problem induced by $C$ as
\begin{equation}
\label{eq:problem}
    \max_{X\in \mathcal{A}} \WWl (X) =\max_{X\in \mathcal{A}} \langle W, X\rangle + \lambda C(X),
\end{equation}
where $\mathcal{A}$ is the set of feasible points $X$ and $\lambda\in\mathbb{R}$. 
%
\end{definition}


% 
Depending on the value of $\lambda$ the solution $X_\lambda$ of \eqref{eq:problem} changes.
% 
First of all, depending on the sign of $\lambda$, the problem searches for the maximum or the minimum of $C$.
% 
Moreover, as the module of $\lambda$ grows, the solution $X_\lambda$ gets less optimal for the LP problem and approaches the maximum or minimum of $C$.
% 
For the sake of simplicity, in what follows, we assume $C$ to be convex and $\lambda$ to be negative, which is also the most common scenario.
% 


\begin{proposition}
\label{prop:2}
Let $W=(w_{i,j})_{i,j}$ be an edge weight matrix such that $\min_{i,j}w_{i,j}>0$ and let $C$ be a convex and bounded function.
% 
Then, there exists $\epsilon>0$ such that for every $\lambda\in(0,\epsilon)$ the solution to
\begin{equation}
    \label{eq:lambdasmall}
    \max_{X\in\mathcal{A}} \langle W , X \rangle -\lambda C(X)
\end{equation}
is a Maximum Edge-weighted Matching with respect to edge weight matrix $W$ that minimizes $C$.
\end{proposition}
% 

\begin{proof}
Let $\bar{X}\in\AA$ be a minimizer of the LP problem
\[
\max_{X\in \AA}\langle W,X \rangle.
\]
Toward a contradiction, assume that $\forall\lambda>0$ there exists a $X_\lambda$ such that
\[
\langle W,\bar{X} \rangle - \lambda C(\bar{X}) < \langle W,X_\lambda \rangle - \lambda C(X_\lambda).
\]
Let us now consider a sequence, namely $\lambda_n$, such that $\lambda_n>0$ for every $n$ and that converges monotonically to $0$.
% 
Let $X_n:=X_{\lambda_n}$ be the related sequence of solutions.
% 
Since $\AA$ is finite, we have that, up to a sub-sequence, $X_n$ converges to some element in $\AA$, namely $X^*$, moreover, starting from a given $N$, we have $X_{n}=X^*$ for every $n>N$.
% 
By assumption, we have that $\langle W,X^*\rangle<\langle W,\bar{X} \rangle$.
% 
Moreover, we have
\[
\langle W,\bar{X} \rangle -\lambda_n C(\bar{X}) <  \langle W,X^*\rangle -\lambda_n C(X^*)
\]
for every $n>N$.
% 
By manipulating the latter relation, we get
\[
0<\langle W,\bar{X}-X^* \rangle  <   \lambda_n (C(\bar{X})-C(X^*))
\]
which is a contradiction, since the left-hand side is positive and the right-hand side converges to zero since $C$ is bounded.
\end{proof}



% 
If the matrix $W$ has integer values, we can bound the value of $\epsilon$.
% 

\begin{proposition}
\label{prop:bound_on_eps}
If the matrix $W$ has integer entries and $C$ is a bounded and convex function, we have that $\epsilon$ is greater than $\frac{1}{\Delta C}$ where $\Delta C= \max_{\mathcal{A}} \, C - \min_{\mathcal{A}} \, C$ and $\mathcal{A}$ is the set of feasible binary matrices. 
% 
In particular, whenever $0<\lambda<\frac{1}{\Delta C}$, the solutions of \eqref{eq:lambdasmall} are also maximizers of the LP model defined in Problem \ref{pr:classic}.
\end{proposition}

\begin{proof}(\emph{Proposition \ref{prop:bound_on_eps}})
Let us consider an optimal solution $\bar{X}$ for the ILP model.
% 
Since $W$ has integer values, for any given $X\in \AA$ that is not optimal, we have $\langle W,\bar{X}-X \rangle \ge 1$, hence we have that if there exists $X\in \AA$ that solves \eqref{def:model_epsilon_div} and that does not maximize the quantity $\langle W,X \rangle$ over $\AA$, then we have
\[
1 \le \langle W,\bar{X}-X \rangle \le \lambda\Big( C(\bar{X})-C(X) \Big)\le \lambda \Delta C
\]
which is not possible whenever $\Delta C \le \frac{1}{\lambda}$.
\end{proof}




\subsection*{The SVP in the Bilevel Formulation}


% 
Due to the nature of the BP problems, we can add a convex penalizing function to both the ULp and the LLp, which leads us to the following BP problem.
% 

\begin{problem}
\label{problem:bilevel_2}
In the framework described in Section \ref{sect:bilevel}, consider the following problem
\begin{eqnarray}
\label{eq:bivel_prob_pen}
\arraycolsep=2pt\def\arraystretch{1.25}
\begin{array}{ll} 
\underset{Z,X\in \mathbb{B}_{nm}}\max &\;\;\; \langle W_E,X\rangle + \langle Y^*,X \rangle + a_0 C_E(X)\\
&{\rm where }\quad l_i \le\sum_{j\in [m]}X_{i,j}\le u_i, \\
&\quad\quad \sum_{i\in [n]}X_{i,j}\le U_j,\\
&\quad\quad\sum_{i\in [n]}Z_{i,j}=U_j+\phi_j,\quad \text{and}\\
&\quad\quad X \le E-Z+Y^*\\
    &Y^* \in\underset{ Y\in \mathbb{B}_{nm}}\argmin \; \langle W_R,Y\rangle + a_j C_j(Y)\\
    &\quad{\rm where}\quad \sum_{i\in [n]}Y_{i,j}=U_j \quad \text{and}\quad 0\le Y\le Z 
\end{array} 
\end{eqnarray}
% 
where $C_E$ is the penalizer of the ULp, $C_j$ are the penalizers of the LLp, and $\{a_k\}_{k=0,\dots,m}$ are parameters that scale the influence of the respective penalizers. 
% 
As for the previous model, the LLp is a component-wise minimization, that is $Y^*$ is the matrix whose columns minimize the quantity $\langle (W_R)_{:,j},Y_{:,j} \rangle + a_j C_j(Y_{:,j})$.
\end{problem}


% % 
% While adding a penalizer to the ULp does not alter the structure of the problem in a significant way, adding a penalizer to the LLp is slightly more delicate.
% % 
% However, as long as the penalizers $C_j$ with $j=1,\dots,n$ are convex functions and the related $a_j$ are negative, we can still use the KKT conditions to reformulate the BP problem as a single-level formulation.
% %  





\subsection*{Some Remarks on the Diversity Function}


% 
Finally, we study the diversity function.
% 
To the best of our knowledge, diversity was first introduced in the context of peer review matching in \cite{DBLP:conf/ijcai/AhmedDF17}.
% 
Given a bipartite graph $(A\cup B,A\times B)$, a function that evaluates the weights of the edges $W:A\times B\to [0,\infty)$, and a matching $X=\{X_{i,j}\}_{(i,j)\in A\times B}$, the authors define the diversity of $X$ as
\begin{equation}
    \label{eq:diversity_weight_app}
    F(X):=\sum_{i\in A}\sum_{k\in K}\Big(\sum_{j\in B_k}w_{i,j}X_{i,j}\Big)^2
\end{equation}
where $w_{i,j}=W(i,j)$ and $\BB:=\{B_k\}_{k\in K}$ is a partition of one of the sides of the bipartite graph, in this case, $B$.
% 
They then use a greedy heuristic method to minimize $F$ over the set of constraints we described in Section \ref{sec:basic_notions}.
% 
Through empirical experiments, they show that the minimizers of \eqref{eq:diversity_weight_app} are close to the classic optimums from a utility viewpoint.
%
However, their solution has a way higher level of entropy.\footnote{For the sake of convenience, the authors of \cite{DBLP:conf/ijcai/AhmedDF17} considered the minimization version of the ILP rather than the maximizing one we presented in the paper. In this case, the editor aims at minimizing the quantity $\sum_{i,j}w_{i,j}X_{i,j}$.}
% 
Although at first sight, it might look like this model has nothing to do with the SVP, there exists a deep relation between the problem considered in \cite{DBLP:conf/ijcai/AhmedDF17} and the SVP induced by the following entropy function
\begin{equation}
    \label{eq:entropy}
    C(X):=-\sum_{i\in [n]}\sum_{k\in [K]}\Big(\sum_{j\in B_k} w_{i,j}X_{i,j}\Big)\log \Big(\sum_{j\in B_k} w_{i,j}X_{i,j}\Big).
\end{equation}
% 
Moreover, we also show that the function $F$ defined in \eqref{eq:diversity_weight_app} is not related to the classic notion of diversity (as the next example shows) and propose a correct definition of the diversity function.
% 



% % Figure environment removed

% Figure environment removed

\begin{example}
\label{ex:1}
Let us consider the bipartite graph defined by $A=\{L1,L2,L3\}$ and $B=\{R1,R2,R3\}$ (see Figure \ref{fig:1}).
% 
Let us set $w_{i,j}=W(Li,Rj)$ where $i,j=1,2,3$ and let us assume that $w_{1,1}=w_{1,2}=0.1$ and $w_{1,3}=0.9$.
% 
Finally, we assume that the partition over $B$ is $B_1:=\{R1,R2\}$ and $B_2=\{R3\}$.
% 
Let us assume that every element of $A$ has to be matched to at least two items of $B$.
% 
It is then easy to see that the optimal allocation (the one that minimizes the functional in \eqref{eq:diversity_weight_app}) of $L1$ is to allocate it to $R1$ and $R2$ since $(0.1+0.1)^2<0.1^2+0.9^2$.
% 
However, this is counterintuitive since the diversity function should seek allocations that match elements of $A$ to as many elements of different clusters in $B$.
% 
It is easy to generalize the example to the case of generic positive weights.
% 
Finally, notice that if all the weights of the graph are the same (say, for example, equal to $1$), the notion of diversity is consistent with what has been proposed in \cite{DBLP:conf/ijcai/AhmedDF17}).
\end{example}
% 



% 
In particular, minimizing the functional \eqref{eq:diversity_weight_app} does not always lead to a diverse matching.
% 
Motivated by the latter example, we propose the following definition of the diversity functional.
% 


\begin{definition}
Given a bipartite graph $([n]\cup [m],[n]\times [m])$ and a partition $\BB:=\{B_k\}_{k\in K}$ over $[m]$, we define the diversity functional $\DD:\mathbb{B}_{nm}\to [0,+\infty)$ as
\begin{equation}
    \label{eq:diversity_eq_right}
    \DD(X)=\sum_{i\in [n]}\sum_{k\in [K]}\Big(\sum_{j\in B_k}X_{i,j}\Big)^2.
\end{equation}
\end{definition}

From now on, we refer to the function $F$ defined in \eqref{eq:diversity_weight_app} as the \textit{weighted diversity function}, while we refer to $\DD$ defined in \eqref{eq:diversity_eq_right} as the \textit{diversity function}.




\subsection*{A Generalized Class of Weighted Diversity Functions and the Relation with the Entropy}

% 
In this section, we study how the minimization of the weighted diversity function $F$ defined in \eqref{eq:diversity_weight_app} is related to an SVP of the linear problem $\min_{X\in \AA} \;\langle W,X \rangle$ induced by the entropy function \eqref{eq:entropy}.
% 
This allows us to justify the results found in \cite{DBLP:conf/ijcai/AhmedDF17}. 
% 
We start the discussion by introducing a continuous family of functions that generalizes the weighted diversity function $F$.
% 

\begin{definition}
Given a weight matrix $W$ and a parameter $p\ge 1$, we define the weighted $p$-diversity functional as
\[
F_p(X):=\sum_{i\in A}\sum_{k\in K}\Big( \sum_{j\in G_k}w_{i,j}X_{i,j} \Big)^p.
\]
\end{definition}

% 
Notice that $F_2$ is equal to the weighted diversity functional $F$, defined in \eqref{eq:diversity_weight_app}. 
% 
From the same argument used to prove Proposition \ref{prop:2}, we infer the following result.
% 

\begin{proposition}
\label{prop:1}
Given any matching $X$, we have that $\lim_{p\to 1}F_p(X)=\langle W,X\rangle$.
% 
Moreover, the function $p\to F_p(X)$ is continuous and, given any feasible set $\mathcal{A}$, there exists a $\Bar{p}\in(1,+\infty)$ such that, for every $1<p<\bar p$, every minimizer of $F_p$ over $\mathcal{A}$ is a minimizer of $X\to F_1(X)$ over $\mathcal{A}$.
\end{proposition}


% 
In particular, we have that the minimizers of $F_p$ for $p\approx 1$ also maximize the quality of the matching.
% 
This is in line with the results reported in \cite{DBLP:conf/ijcai/AhmedDF17}, as we have that, due to the continuity of the function $p\to F_p(X)$, the minimum of $F_2$ has to be not that different from an element of $\tilde{S}_1$.
% 
Let us denote with $\mathcal{S}_p$ the set of minimizers of $F_p$ over $\mathcal{S}$, from Proposition \ref{prop:1}, we infer that the set $\mathcal{S}_p$ converges to a set $\tilde{S}_1$ that is a subset of the minimizers of $F_1$.
% 
We now show that this process is related to an SVP and that the set $\tilde{S}_1$ is the subset of minimizers of $F_1$ that maximizes the entropic penalizing function introduced in \eqref{eq:entropy}.
% 
Indeed, we recall that, given $a>0$, it holds true the following approximation formula 
\[
a^{1+\epsilon}\approx a(1 + \epsilon\log(a)),
\]
where $\epsilon>0$ is a small parameter.
% 
The latter formula allows us to approximate $F_{1+\epsilon}(X)$ as it follows

\begin{align*}
    F_{1+\epsilon}&(X)= \sum_{i\in [n]}\sum_{k\in [K]}\Big(\sum_{j\in B_k}w_{i,j}X_{i,j} \Big)^{1+\epsilon}\\
    &\approx  \sum_{i\in [n]}\sum_{k\in [K]}\sum_{j\in B_k}w_{i,j}X_{i,j}\Big(1+\epsilon \log\big(\sum_{j\in B_k}w_{i,j}X_{i,j}\big) \Big)\\
    &=\langle W, X\rangle \\
    &\quad+\epsilon \sum_{i\in [n]}\sum_{k\in [K]}\Big(\sum_{j\in B_k}w_{i,j}X_{i,j}\Big) \log\Big(\sum_{j\in B_k}w_{i,j}X_{i,j}\Big).
\end{align*}

Therefore, the weighted diversity function becomes an SVP for the entropy of the weights allocated to the partition.
% 
From Proposition \ref{prop:2}, we infer the following result.
% 

\begin{proposition}
Let us consider a quality matrix $W$ such that $w_{i,j}>0$ for every $i\in [n]$ and $j\in [m]$.
% 
Then, for every $p\in [1,\tilde{p}]$, there exists a value $\Bar{p}>1$ for which every solution to the follwing problem
\begin{equation}
    \label{eq:min_p_small}
    \min_{X\in\mathcal{A}} F_p(X), 
\end{equation}
is a minimizer of $F_1$ that maximizes the entropy function \eqref{eq:entropy}. 
\end{proposition}
% 


% 
Once again, this result is in line with the results presented in \cite{DBLP:conf/ijcai/AhmedDF17}, where the authors show that minimizing $F_2$ instead of the classical linear function ($F_1$ according to our notation) leads to solutions that have a higher entropy while still retaining a high overall quality.
% 
Following this idea, by applying the algorithm to the $p$-diversity functional with $p\in (1,2)$, we should find a solution that has a better efficiency from a matching perspective and that does not alter the entropy gain described in \cite{DBLP:conf/ijcai/AhmedDF17} for the case $p=2$.
% 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




