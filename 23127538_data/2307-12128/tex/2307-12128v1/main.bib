@article{Pawar,
abstract = {The pervasive use of cameras at indoor and outdoor premises on account of recording the activities has resulted into deluge of long video data. Such surveillance videos are characterized by single or multiple entities (persons, objects) performing sequential/concurrent activities. It is often interesting to detect suspicious behavior of such entities in an automated manner without any intervention of human personnel, and to this end, anomalous activity detection from surveillance videos is an important research domain in Computer Vision. Detecting the anomalous activities from videos is very challenging due to equivocal nature of anomalies, context at which events took place, lack of ample size of anomalous ground truth training data and also other factors associated with variation in environment conditions, illumination conditions and working status of capturing cameras. Though automated visual surveillance is one of the highly sought-after research domains, use of deep learning techniques for anomalous activity detection is still in nascent stage. Deep learning models like convolution neural networks, auto-encoders, Long Short Term Memory network models have achieved remarkable performance on different domains like image classification , object detection, speech processing, and expediting towards achieving excellence in anomaly detection tasks. This paper aims at studying and analyzing deep learning techniques for video-based anomalous activity detection. As outcome of the study, the graphical taxonomy has been put forth based on kinds of anomalies, World Wide Web (2019) 22:571-601 https://doi. level of anomaly detection, and anomaly measurement for anomalous activity detection. The focus has been given on various anomaly detection frameworks having deep learning techniques as their core methodology. Deep learning approaches from both the perspectives of accuracy oriented anomaly detection and real-time processing oriented anomaly detection are compared. This paper also sheds light upon research issues and challenges, application domains, benchmarked datasets and future directions in the domain of deep learning based anomaly detection.},
author = {Pawar, Karishma and Attar, Vahida and Song, Jingkuan and Jiang, Shuqiang and Ricci, Elisa and Huang, Zi},
doi = {10.1007/s11280-018-0582-1},
file = {::},
keywords = {anomalous activity detection,anomaly modeling,computer vision,deep learning,real time detection,video surveillance},
title = {{Deep learning approaches for video-based anomalous activity detection}},
url = {https://doi.org/10.1007/s11280-018-0582-1}
}
@techreport{stewart2022overview,
  title={Overview of Motor Vehicle Crashes in 2020},
  author={Stewart, Timothy},
  year={2022}
}
@inproceedings{elsayed2021intrusion,
  title={Intrusion Detection System in Smart Home Network Using Bidirectional LSTM and Convolutional Neural Networks Hybrid Model},
  author={Elsayed, Nelly and Zaghloul, Zaghloul Saad and Azumah, Sylvia Worlali and Li, Chengcheng},
  booktitle={2021 IEEE International Midwest Symposium on Circuits and Systems (MWSCAS)},
  pages={55--58},
  year={2021},
  organization={IEEE}
}
@article{avazov2021fire,
  title={Fire Detection Method in Smart City Environments Using a Deep-Learning-Based Approach},
  author={Avazov, Kuldoshbay and Mukhiddinov, Mukhriddin and Makhmudov, Fazliddin and Cho, Young Im},
  journal={Electronics},
  volume={11},
  number={1},
  pages={73},
  year={2021},
  publisher={MDPI}
}
@article{romero2019convolutional,
  title={Convolutional models for the detection of firearms in surveillance videos},
  author={Romero, David and Salamea, Christian},
  journal={Applied Sciences},
  volume={9},
  number={15},
  pages={2965},
  year={2019},
  publisher={MDPI}
}
@ARTICLE{sensquare,
  author={Montori, Federico and Bedogni, Luca and Bononi, Luciano},
  journal={IEEE Internet of Things Journal}, 
  title={A Collaborative Internet of Things Architecture for Smart Cities and Environmental Monitoring}, 
  year={2018},
  volume={5},
  number={2},
  pages={592-605},
  doi={10.1109/JIOT.2017.2720855}}
@article{Nida2022,
abstract = {Classification models for human action recognition require robust features and large training sets for good generalization. However, data augmentation methods are employed for imbalanced training sets to achieve higher accuracy. These samples generated using data augmentation only reflect existing samples within the training set, their feature representations are less diverse and hence, contribute to less precise classification. This paper presents new data augmentation and action representation approaches to grow training sets. The proposed approach is based on two fundamental concepts: virtual video generation for augmentation and representation of the action videos through robust features. Virtual videos are generated from the motion history templates of action videos, which are convolved using a convolutional neural network, to generate deep features. Furthermore, by observing an objective function of the genetic algorithm, the spatiotemporal features of different samples are combined , to generate the representations of the virtual videos and then classified through an extreme learning machine classifier on MuHAVi-Uncut, iXMAS, and IAVID-1 datasets. K E Y W O R D S computer vision, evolutionary deep features augmentation, genetic algorithm, human action recognition, video augmentation 1 | INTRODUCTION Human action recognition (HAR) are predicted by visual and motion metaphors, either using traditional hand-crafted features with prior information (that is, dense tra-jectories [1], BoF [2], and holistic representation [3,4]) or deep learning models, such as two-stream network [5], 3D convolutional neural network (CNN) [6], and recurrent neural networks [7,8]. Despite their successes, deep learning models require large training sets. These two categories of HAR techniques potentially improve the accuracy of the learning model based on the quality and quantity of the training data. However, data skewness and irregularity degrade performance due to missing feature representation and class imbalance problems. Synthetic data generation and data sampling are effective measures for mitigating data irregularity. Computer vision research has focused on synthetic data generation by applying data augmentation techniques to grow the training sets for stable HAR models.},
author = {Nida, Nudrat and Muhammad, | and Yousaf, Haroon and Irtaza, | Aun and Velastin, Sergio A and Yousaf, Muhammad Haroon},
doi = {10.4218/ETRIJ.2019-0510},
file = {::},
issn = {2233-7326},
journal = {ETRI Journal},
keywords = {computer vision,evolutionary deep features augmentation,genetic algorithm,human action recognition,video augmentation},
month = {jan},
publisher = {John Wiley & Sons, Ltd},
title = {{Video augmentation technique for human action recognition using genetic algorithm}},
url = {https://onlinelibrary.wiley.com/doi/full/10.4218/etrij.2019-0510 https://onlinelibrary.wiley.com/doi/abs/10.4218/etrij.2019-0510 https://onlinelibrary.wiley.com/doi/10.4218/etrij.2019-0510},
year = {2022}
}
@article{Tanaka2020,
abstract = {This paper describes an image processing method that can facilitate skill learning in karate using recorded karate competition videos. The proposed method compensates for a partially filmed karate court in the input video image by generating a complete court image via a network-based generator. To evaluate our method, a player-focused video was augmented with complete competition field information. The augmented video would be useful for observing both players' actions as well as the player positioning within the entire competition court. In particular, a complete court image generator was developed based on a generative adversarial network (GAN) framework to perform this video augmentation.},
author = {Tanaka, Kazumoto},
doi = {10.1109/TALE48869.2020.9368331},
isbn = {9781728169422},
journal = {Proceedings of 2020 IEEE International Conference on Teaching, Assessment, and Learning for Engineering, TALE 2020},
keywords = {Generative adversarial network,Karate,Skill learning,Video augmentation},
month = {dec},
pages = {674--677},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Video augmentation method for the facilitation of skill learning in Karate}},
year = {2020}
}
@article{Yun,
abstract = {State-of-the-art video action classifiers often suffer from overfitting. They tend to be biased towards specific objects and scene cues, rather than the foreground action content, leading to sub-optimal generalization performances. Recent data augmentation strategies have been reported to address the overfitting problems in static image classifiers. Despite the effectiveness on the static image classifiers, data augmentation has rarely been studied for videos. For the first time in the field, we systematically analyze the efficacy of various data augmentation strategies on the video classification task. We then propose a powerful augmentation strategy VideoMix. VideoMix creates a new training video by inserting a video cuboid into another video. The ground truth labels are mixed proportionally to the number of voxels from each video. We show that VideoMix lets a model learn beyond the object and scene biases and extract more robust cues for action recognition. VideoMix consistently outperforms other augmentation baselines on Kinet-ics and the challenging Something-Something-V2 benchmarks. It also improves the weakly-supervised action local-ization performance on THUMOS'14. VideoMix pretrained models exhibit improved accuracies on the video detection task (AVA).},
archivePrefix = {arXiv},
arxivId = {2012.03457v1},
author = {Yun, Sangdoo and Oh, Seong Joon and Heo, Byeongho and Han, Dongyoon and Kim, Jinhyung and Lab, Naver Ai},
eprint = {2012.03457v1},
file = {::},
title = {{VideoMix: Rethinking Data Augmentation for Video Classification}}
}
@misc{Kopuklu,
author = {K{\"{o}}p{\"{u}}kl{\"{u}}, Okan},
title = {{vidaug: Docs, Tutorials, Reviews | Openbase}},
url = {https://openbase.com/python/vidaug},
urldate = {2021-10-14}
}
@article{Wang2015,
abstract = {Transfer learning is a vital technique that generalizes models trained for
one setting or task to other settings or tasks. For example in speech
recognition, an acoustic model trained for one language can be used to
recognize speech in another language, with little or no re-training data.
Transfer learning is closely related to multi-task learning (cross-lingual vs.
multilingual), and is traditionally studied in the name of `model adaptation'.
Recent advance in deep learning shows that transfer learning becomes much
easier and more effective with high-level abstract features learned by deep
models, and the `transfer' can be conducted not only between data distributions
and data types, but also between model structures (e.g., shallow nets and deep
nets) or even model types (e.g., Bayesian models and neural models). This
review paper summarizes some recent prominent research towards this direction,
particularly for speech and language processing. We also report some results
from our group and highlight the potential of this very interesting research
field.},
archivePrefix = {arXiv},
arxivId = {1511.06066},
author = {Wang, Dong and Zheng, Thomas Fang},
eprint = {1511.06066},
file = {::},
journal = {2015 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference, APSIPA ASC 2015},
keywords = {()},
month = {nov},
pages = {1225--1237},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Transfer Learning for Speech and Language Processing}},
url = {https://arxiv.org/abs/1511.06066v1},
year = {2015}
}
@article{Weiss2016,
abstract = {Machine learning and data mining techniques have been used in numerous real-world applications. An assumption of traditional machine learning methodologies is the training data and testing data are taken from the same domain, such that the input feature space and data distribution characteristics are the same. However, in some real-world machine learning scenarios, this assumption does not hold. There are cases where training data is expensive or difficult to collect. Therefore, there is a need to create high-performance learners trained with more easily obtained data from different domains. This methodology is referred to as transfer learning. This survey paper formally defines transfer learning, presents information on current solutions, and reviews applications applied to transfer learning. Lastly, there is information listed on software downloads for various transfer learning solutions and a discussion of possible future research work. The transfer learning solutions surveyed are independent of data size and can be applied to big data environments.},
author = {Weiss, Karl and Khoshgoftaar, Taghi M. and Wang, DingDing},
doi = {10.1186/S40537-016-0043-6},
file = {::},
issn = {2196-1115},
journal = {Journal of Big Data 2016 3:1},
keywords = {Communications Engineering,Computational Science and Engineering,Data Mining and Knowledge Discovery,Database Management,Information Storage and Retrieval,Mathematical Applications in Computer Science,Networks},
month = {may},
number = {1},
pages = {1--40},
publisher = {SpringerOpen},
title = {{A survey of transfer learning}},
url = {https://journalofbigdata.springeropen.com/articles/10.1186/s40537-016-0043-6},
volume = {3},
year = {2016}
}
@misc{CDC2021,
author = {CDC},
booktitle = {Centers for Disease Control and Prevention},
title = {{Data and Statistics for SIDS and SUID}},
url = {https://www.cdc.gov/sids/data.htm},
urldate = {2021-08-23},
year = {2021}
}
@article{Theses2018,
author = {Theses, Graduate and Pang, Jinyong},
file = {::},
title = {{Scholar Commons Human Activity Recognition Based on Transfer Learning}},
url = {https://scholarcommons.usf.edu/etd},
year = {2018}
}

@article{Tasnim2021,
abstract = {Human activity recognition has become a significant research trend in the fields of computer vision, image processing, and human–machine or human–object interaction due to cost-effectiveness, time management, rehabilitation, and the pandemic of diseases. Over the past years, several methods published for human action recognition using RGB (red, green, and blue), depth, and skeleton datasets. Most of the methods introduced for action classification using skeleton datasets are constrained in some perspectives including features representation, complexity, and performance. How-ever, there is still a challenging problem of providing an effective and efficient method for human action discrimination using a 3D skeleton dataset. There is a lot of room to map the 3D skeleton joint coordinates into spatio-temporal formats to reduce the complexity of the system, to provide a more accurate system to recognize human behaviors, and to improve the overall performance. In this paper, we suggest a spatio-temporal image formation (STIF) technique of 3D skeleton joints by capturing spatial information and temporal changes for action discrimination. We conduct transfer learning (pretrained models-MobileNetV2, DenseNet121, and ResNet18 trained with ImageNet dataset) to extract discriminative features and evaluate the proposed method with several fusion techniques. We mainly investigate the effect of three fusion methods such as element-wise average, multiplication, and maximization on the performance variation to human action recognition. Our deep learning-based method outperforms prior works using UTD-MHAD (University of Texas at Dallas multi-modal human action dataset) and MSR-Action3D (Microsoft action 3D), publicly available benchmark 3D skeleton datasets with STIF representation. We attain accuracies of approximately 98.93%, 99.65%, and 98.80% for UTD-MHAD and 96.00%, 98.75%, and 97.08% for MSR-Action3D skeleton datasets using MobileNetV2, DenseNet121, and ResNet18, respectively.},
author = {Tasnim, Nusrat and Islam, Mohammad Khairul and Baek, Joong Hwan},
doi = {10.3390/APP11062675},
file = {::},
journal = {Applied Sciences (Switzerland)},
keywords = {Deep learning,Fusion strategies,Human activity recognition,Spatio-temporal image formation,Transfer learning},
month = {mar},
number = {6},
publisher = {MDPI AG},
title = {{Deep learning based human activity recognition using spatio-temporal image formation of skeleton joints}},
volume = {11},
year = {2021}
}
@article{Lim2018,
abstract = {This paper presents a classifier ensemble for Facial Expression Recognition
(FER) based on models derived from transfer learning. The main experimentation
work is conducted for facial action unit detection using feature extraction and
fine-tuning convolutional neural networks (CNNs). Several classifiers for
extracted CNN codes such as Linear Discriminant Analysis (LDA), Support Vector
Machines (SVMs) and Long Short-Term Memory (LSTM) are compared and evaluated.
Multi-model ensembles are also used to further improve the performance. We have
found that VGG-Face and ResNet are the relatively optimal pre-trained models
for action unit recognition using feature extraction and the ensemble of
VGG-Net variants and ResNet achieves the best result.},
annote = {The researcers worked on fine tuning and features extraction for pretarined model for Facial expression recognition. VGG -Net and ResNet was used as the base model.
Trainning most deep neural network model requires a very high computational cost and using of expensive GPu for trainning models ranging from days to months. Transfer learning on the tweleve specific action classes considered in this research can exploit the learned features from sucessful networks
Facial Action coding systmem is onther systematic approach that describe facialmuscle movement using Action units},
archivePrefix = {arXiv},
arxivId = {1807.07556},
author = {Lim, Yen Khye and Liao, Zukang and Petridis, Stavros and Pantic, Maja},
eprint = {1807.07556},
file = {:C\:/Users/Adewo/Documents/Mendley Documents/Transfer Learning for Action Unit Recognition - Lim et al. - 2018.pdf:pdf},
month = {jul},
title = {{Transfer Learning for Action Unit Recognition}},
url = {https://arxiv-org.uc.idm.oclc.org/abs/1807.07556v1},
year = {2018}
}
@inproceedings{Iqbal2019,
abstract = {Temporal localization of actions in videos has been of increasing interest in recent years. However, most existing approaches rely on complex architectures that are either expensive to train, inefficient at inference time, or require thorough and careful architecture engineering. Classical action recognition on pre-segmented clips, on the other hand, benefits from sophisticated deep architectures that paved the way for highly reliable video clip classifiers. In this paper, we propose to use transfer learning to leverage the good results from action recognition for temporal localization. We apply a network that is inspired by the classical bag-of-words model for transfer learning and show that the resulting framewise class posteriors already provide good results without explicit temporal modeling. Further, we show that combining these features with a deep but simple convolutional network achieves state of the art results on two challenging action localization datasets.},
annote = {The researchers performed action localizations on pre selected frames by leveraving on transfer learning from existing model. The overaching goal was to simply the complex architectures, expensive computation cost and inefficient inferencing in existing methodologies. This paper was able to achieve good results with simple convolution network and features from esisting model.
Action recognition with single action class in a video stream is lacking behind impractical applications, however actional localization in untrimmed video is a more tedious taks as it involves developing architectures that can accurately set boundaries and train end to end models that can recognize action classes in an untrimeed video stream.
Current research trend in Action Recognition is focused on
1. claasical deep neural network with two stream architectures (RGB and Optial flow).
Trasferring features from pretrained model on small action classes provides significant improvement os performance of AR Models .
2. Other area of focus has been on temporal localization and segementation of actions in untrimmed video. 
Hiddent markov model has been used in capturing long range dependcies in frame wise action recognition [Hilde Kuehne, Ali Arslan, and Thomas Serre. The language of actions: Recovering the syntax and semantics of goal- directed human activities. In IEEE Conf. on Computer Vision and Pattern Recognition, 2014.
[15]]. Spatio temporal convolution and semi -hiden markov model was used in capturing multiple actions tranistions in untrimmed video[Colin Lea, Austin Reiter, Ren´e Vidal, and Gregory Hager. Segmental spatiotemporal CNNs for fine-grained action seg- mentation. In European Conf. on Computer Vision, 2016.].

Transfer learning is a commonly used technique to ex-ploit deep neural networks that have been trained on a huge amount of data from one domain in order to train a model on a new domain.

The researcher utiled the I3D network on temporarilly untrimmed video to localize all action classes instances in a video stream. They showcased using deep vanila TCN on fetaures extracted in transfer learning can yeild state of art result with a light weight model without multiple layers and gated convolutions.},
author = {Iqbal, Ahsan and Richard, Alexander and Gall, Juergen},
booktitle = {Proceedings - 2019 International Conference on Computer Vision Workshop, ICCVW 2019},
doi = {10.1109/ICCVW.2019.00191},
file = {:C\:/Users/Adewo/Documents/Mendley Documents/Enhancing Temporal Action Localization with Transfer Learning from Action Recognition.pdf:pdf},
isbn = {9781728150239},
keywords = {Action detection,Action recognition,Computer vision},
month = {oct},
pages = {1533--1540},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Enhancing temporal action localization with transfer learning from action recognition}},
year = {2019}
}
@article{Abdulazeem2021,
abstract = {Human action recognition techniques have gained significant attention among next-generation technologies due to their specific features and high capability to inspect video sequences to understand human actions. As a result, many fields have benefited from human action recognition techniques. Deep learning techniques played a primary role in many approaches to human action recognition. The new era of learning is spreading by transfer learning. Accordingly, this study&#x2019;s main objective is to propose a framework with three main phases for human action recognition. The phases are pre-training, preprocessing, and recognition. This framework presents a set of novel techniques that are three-fold as follows, (i) in the pre-training phase, a standard convolutional neural network is trained on a generic dataset to adjust weights; (ii) to perform the recognition process, this pre-trained model is then applied to the target dataset; and (iii) the recognition phase exploits convolutional neural network and long short-term memory to apply five different architectures. Three architectures are stand-alone and single-stream, while the other two are combinations between the first three in two-stream style. Experimental results show that the first three architectures recorded accuracies of 83.24%, 90.72%, and 90.85%, respectively. The last two architectures achieved accuracies of 93.48% and 94.87%, respectively. Moreover, The recorded results outperform other state-of-the-art models in the same field.},
annote = {The research presnets a three fold architecture for Human Action Recognition.
Based on their experimental results the two stream styled architectures had higher accuracy that the state of art model.
Human action recognition can be catgorized to Video based AR and Sensor Based AR.
Deep leaning has made progress in action recognition, however there are still existing research gap. modelling temporal information is the major cjhallenge of CNN architecture with a single stream with RGB images. The tw tream architectures introduces the modelling of spatial information in RGB images and Temporal information in stacked optical flow of Images.
Transfer leaning invloves extarcting information from similar domain to a new domain area by extracting features learned from a trained model. transfer leaning can be of three types:
1. Fixed Feature Extraction
2. Fine tzunning of fixed layers and weights
3. Pre Trained Architecture.
The researcher intoduced a framework consisting of pre trainning, data augmentation and recognition. Features were extracted using VGG16 and Densenet an Xception model. The framework utilized a Temopal CNN LSTM cell and classifcation average of Two Stream CNN and One Stream CNN for final classification layer.

****Borrow frame sampling write up from the Section iii DATA PRE-PROCESSING AND AUGMENTATION.

Experimental results show that the combined architectures
achieved higher accuracy than the self-paced architectures and superior to staet of art with 94.87% accuracy},
author = {Abdulazeem, Yousry and Balaha, Hossam Magdy and Bahgat, Waleed M. and Badawy, Mahmoud},
doi = {10.1109/ACCESS.2021.3086668},
file = {:C\:/Users/Adewo/Documents/Mendley Documents/Human Action Recognition based on Transfer Learning Approach - Abdulazeem et al. - 2021.pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {Computer architecture,Convolutional neural network (CNN),Deep learning,Feature extraction,Human action recognition (HAR),Long short-term memory (LSTM),Solid modeling,Spatiotemporal info,Three-dimensional displays,Training,Transfer learning,Transfer learning (TL)},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Human Action Recognition based on Transfer Learning Approach}},
year = {2021}
}
@article{celaya2019texting,
  title={“Texting \& Driving” detection using deep convolutional neural networks},
  author={Celaya-Padilla, Jos{\'e} Mar{\'\i}a and Galv{\'a}n-Tejada, Carlos Eric and Lozano-Aguilar, Joyce Selene Anaid and Zanella-Calzada, Laura Alejandra and Luna-Garc{\'\i}a, Huizilopoztli and Galv{\'a}n-Tejada, Jorge Issac and Gamboa-Rosales, Nadia Karina and Velez Rodriguez, Alberto and Gamboa-Rosales, Hamurabi},
  journal={Applied Sciences},
  volume={9},
  number={15},
  pages={2962},
  year={2019},
  publisher={MDPI}
}
@inproceedings{Kunze2017,
abstract = {End-to-end training of automated speech recognition (ASR) systems requires massive data and compute resources. We explore transfer learning based on model adaptation as an approach for training ASR models under constrained GPU memory, throughput and training data. We conduct several systematic experiments adapting a Wav2Letter convolutional neural network originally trained for English ASR to the German language. We show that this technique allows faster training on consumer-grade resources while requiring less training data in order to achieve the same accuracy, thereby lowering the cost of training ASR models in other languages. Model introspection revealed that small adaptations to the network's weights were sufficient for good performance, especially for inner layers.},
annote = {Training Action Recognition modles requires massive dataset and high computational resources . This paper explored transfer learning on speech recognition task.The researchers discovered inferring knowledge from existing model save time taken in trainning new mode l and reduction in cost while maintaining a very good accuracy with constarianed GPU.
The research applied transfer learning techniques and lower resource footprint ann adopting an end to end CNN based model trained on English Language to recognize German speech.
Labelled data for Specific Action Recognition task with adequate volume of data is empirically difficult. Specific AR task can benefit from the concept of transfer learning which invoves enhancing the performance of an existing model by extracting features learnt from previous domain and applying it to a specific AR task with limited data availability and compute resource.

METHODS OF TRANSFER LEARNING.-SUBHEAD
1. Heterogeneious Transfer learning: 
This involves trainning a base model on multiple dataset with different tasks simultaneously ((Wang and Zheng, 2015-Dong Wang and Thomas Fang Zheng. 2015. Trans- fer Learning for Speech and Language Processing. arXiv:1511.06066 [cs] http://arxiv.org/abs/1511.06066.), this reserchers proved that they achieve a very competitive results(Chen and Mak, 2015; Knill et al., 2014)-Multitask learning of deep neural networks for low- resource speech recognition. IEEE/ACMTrans. Audio, Speech & Language Processing 23(7):1172–1183. http://dx.doi.org/10.1109/TASLP.2015.2422573). However, (Heigold et al., 2013-Multilingual acoustic models using distributed deep neural networks. In International Con- ference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, pages 8619–8623.) research showcased tat there is still need for a large amount of data to achive a significant improvements in model performance.

2. model adaptation
This involves training model on one or more tasks afterwrds retrain the whole model layers or some part of the layers on a new task entirely diffeent from what the base model was trained on. The parameters/features leaned in the base model serves as the starting point for trainning on a new task.
in our approach we freezed part of the base model layers and focused on features that has similar features with our specific task to avoid over fitting or our model not leaning specific features from the base model.

LOOK AT THE MODEL ARCHITECTUE HERE WHEN DISCUSSING MY PAPER},
archivePrefix = {arXiv},
arxivId = {1706.00290},
author = {Kunze, Julius and Kirsch, Louis and Kurenkov, Ilia and Krug, Andreas and Johannsmeier, Jens and Stober, Sebastian},
doi = {10.18653/v1/w17-2620},
eprint = {1706.00290},
file = {:C\:/Users/Adewo/Documents/Mendley Documents/Transfer Learning for Speech Recognition on a Budget - Kunze et al. - 2017.pdf:pdf},
month = {jul},
pages = {168--177},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Transfer Learning for Speech Recognition on a Budget}},
year = {2017}
}
@article{Fortun2015,
abstract = {Optical flow estimation is one of the oldest and still most active research domains in computer vision. In 35 years, many methodological concepts have been introduced and have progressively improved performances, while opening the way to new challenges. In the last decade, the growing interest in evaluation benchmarks has stimulated a great amount of work. In this paper, we propose a survey of optical flow estimation classifying the main principles elaborated during this evolution, with a particular concern given to recent developments. It is conceived as a tutorial organizing in a comprehensive framework current approaches and practices. We give insights on the motivations, interests and limitations of modeling and optimization techniques, and we highlight similarities between methods to allow for a clear understanding of their behavior.},
annote = {Analysis of motion is a very common topic in computer vision whhich involves analyzing the movement of objects and their behavioural change which varies depending on the exact area of application. Common areas of interest includes tracking objects, deformation quantification, detecting abnormal behaviour/movements. 

Developing Action recognition model in specific application domain by interpreting video contents can revolutionalize the field of Human assisted A.I. In the medical context, deformation of cells can easily be spotted and analyzed using A.I tailored towards this specific use case by estimating and trainning models on larger samples. Most surgeries can be performed by human assisted by A.i thereby liiting the medical error and ensuring precision in medical proceures and diagnosis.

Another area of application is in a smart city where violence can easily be spotted to alert approriate enforcement agengcies with automated anyalysis of video contents in surveilance cameras.
Robotica and auto navigation has benefited from the use of A.R system for automatic guidance specifically in obstacle detection, accident prevention and lane departure assist.},
author = {Fortun, Denis and Bouthemy, Patrick and Kervrann, Charles},
doi = {10.1016/j.cviu.2015.02.008},
file = {:C\:/Users/Adewo/Documents/Mendley Documents/Optical flow modeling and computation A survey - Fortun, Bouthemy, Kervrann - 2015.pdf:pdf},
issn = {1090235X},
journal = {Computer Vision and Image Understanding},
keywords = {Feature matching,Motion estimation,Occlusions,Optical flow,Optimization,Parametric models,Regularization},
pages = {1--21},
title = {{Optical flow modeling and computation: A survey}},
url = {https://hal.inria.fr/hal-01104081v2},
volume = {134},
year = {2015}
}
@article{Authors1960,
abstract = {Action segmentation as a milestone towards building automatic systems to understand untrimmed videos has received considerable attention in the recent years. It is typically being modeled as a sequence labeling problem but contains intrinsic and sufficient differences than text parsing or speech processing. In this paper, we introduce a novel hybrid temporal convolutional and recurrent network (TricorNet), which has an encoder-decoder architecture: the encoder consists of a hierarchy of temporal convolutional kernels that capture the local motion changes of different actions; the decoder is a hierarchy of recurrent neural networks that are able to learn and memorize long-term action dependencies after the encoding stage. Our model is simple but extremely effective in terms of video sequence labeling. The experimental results on three public action segmentation datasets have shown that the proposed model achieves superior performance over the state of the art.},
author = {Authors, Anonymous},
number = {2015},
pages = {6215},
title = {{VIDEO ACTION SEGMENTATION WITH HYBRID TEM- PORAL NETWORKS}},
url = {https://openreview.net/forum?id=r1nzLmWAb},
year = {1960}
}
@inproceedings{Carreira2017,
abstract = {The paucity of videos in current action classification datasets (UCF-101 and HMDB-51) has made it difficult to identify good video architectures, as most methods obtain similar performance on existing small-scale benchmarks. This paper re-evaluates state-of-the-art architectures in light of the new Kinetics Human Action Video dataset. Kinetics has two orders of magnitude more data, with 400 human action classes and over 400 clips per class, and is collected from realistic, challenging YouTube videos. We provide an analysis on how current architectures fare on the task of action classification on this dataset and how much performance improves on the smaller benchmark datasets after pre-training on Kinetics. We also introduce a new Two-Stream Inflated 3D ConvNet (I3D) that is based on 2D ConvNet inflation: filters and pooling kernels of very deep image classification ConvNets are expanded into 3D, making it possible to learn seamless spatio-temporal feature extractors from video while leveraging successful ImageNet architecture designs and even their parameters. We show that, after pre-training on Kinetics, I3D models considerably improve upon the state-of-the-art in action classification, reaching 80.2% on HMDB-51 and 97.9% on UCF-101.},
annote = {The authors introduce a new Two-Stream Inflated 3D Con-
vNet (I3D) that is based on 2D ConvNet inflation.
The questio n postulated by the researcher is to find out if the train-
ing pf an action classification network on a sufficiently large dataset, will give a similar boost in performance when ap plied to a different temporal task or dataset. The


The results suggest that there is always a boost in performance by pre-training, but the ex- tent of the boost varies significantly with the type of archi- tecture. 
“Two-Stream Inflated 3D ConvNets” (I3D) builds on a pre trained image classification architectures and inflates their pooling kernel to 3D},
archivePrefix = {arXiv},
arxivId = {1705.07750},
author = {Carreira, Jo{\~{a}}o and Zisserman, Andrew},
booktitle = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
doi = {10.1109/CVPR.2017.502},
eprint = {1705.07750},
file = {:C\:/Users/Adewo/Documents/Mendley Documents/Quo Vadis, Action Recognition A New Model and the Kinetics Dataset - Carreira, Zisserman - 2017.pdf:pdf},
isbn = {9781538604571},
month = {may},
pages = {4724--4733},
title = {{Quo Vadis, action recognition? A new model and the kinetics dataset}},
url = {https://arxiv.org/abs/1705.07750},
volume = {2017-Janua},
year = {2017}
}
@techreport{Zhao,
abstract = {Detecting actions in untrimmed videos is an important yet challenging task. In this paper, we present the structured segment network (SSN), a novel framework which models the temporal structure of each action instance via a struc-tured temporal pyramid. On top of the pyramid, we further introduce a decomposed discriminative model comprising two classifiers, respectively for classifying actions and determining completeness. This allows the framework to effectively distinguish positive proposals from background or incomplete ones, thus leading to both accurate recognition and localization. These components are integrated into a unified network that can be efficiently trained in an end-to-end fashion. Additionally, a simple yet effective temporal action proposal scheme, dubbed temporal actionness grouping (TAG) is devised to generate high quality action proposals. On two challenging benchmarks, THUMOS14 and ActivityNet, our method remarkably outperforms previous state-of-the-art methods, demonstrating superior accuracy and strong adaptivity in handling actions with various temporal structures. 1},
annote = {The authors present a SSN structured segement framework for detecting actions in untrimmed video and also classifying actions.
They used the THUMOS 14 dataset and correclate their results with other benchmarks. Their methodology is able to propose regions in to localize action.
The model accepts an unput video and a set a temporal action proposal and outputs a set of predictcted activity category.
Code available in github},
archivePrefix = {arXiv},
arxivId = {1704.06228v2},
author = {Zhao, Yue and Xiong, Yuanjun and Wang, Limin and Wu, Zhirong and Tang, Xiaoou and Lin, Dahua},
eprint = {1704.06228v2},
file = {:C\:/Users/Adewo/Documents/Mendley Documents/Temporal Action Detection with Structured Segment Networks - Zhao et al. - Unknown.pdf:pdf},
title = {{Temporal Action Detection with Structured Segment Networks}},
url = {http://yjxiong.me/others/ssn}
}
@article{Wang2017,
abstract = {Deep convolutional networks have achieved great success for image recognition. However, for action recognition in videos, their advantage over traditional methods is not so evident. We present a general and flexible video-level framework for learning action models in videos. This method, called temporal segment network (TSN), aims to model long-range temporal structures with a new segment-based sampling and aggregation module. This unique design enables our TSN to efficiently learn action models by using the whole action videos. The learned models could be easily adapted for action recognition in both trimmed and untrimmed videos with simple average pooling and multi-scale temporal window integration, respectively. We also study a series of good practices for the instantiation of TSN framework given limited training samples. Our approach obtains the state-the-of-art performance on four challenging action recognition benchmarks: HMDB51 (71.0%), UCF101 (94.9%), THUMOS14 (80.1%), and ActivityNet v1.2 (89.6%). Using the proposed RGB difference for motion models, our method can still achieve competitive accuracy on UCF101 (91.0%) while running at 340 FPS. Furthermore, based on the temporal segment networks, we won the video classification track at the ActivityNet challenge 2016 among 24 teams, which demonstrates the effectiveness of TSN and the proposed good practices.},
annote = {Action recognition in videos has two crucial keypoint, Appearances and temporal dynamics.
ConvNets was introduced to solve the challenge of video based action recognition.

Application of ConvNets in video action recognition is affected by
1. Long range temporal structure(Method usually focus on appearances and shot term motions yp to 16 frames)
2.Previous model were bult on trimmed video whereas real live scenarion have to deal with untrimmed video. Dominatingbackground portion ay affect action recognition.
3. Lack of large volume training dataset and diversity

The proposed temporal segement network TSN is a flexible framework for learning action models in videos.},
archivePrefix = {arXiv},
arxivId = {1705.02953},
author = {Wang, Limin and Xiong, Yuanjun and Wang, Zhe and Qiao, Yu and Lin, Dahua and Tang, Xiaoou and {Van Gool}, Luc},
eprint = {1705.02953},
file = {:C\:/Users/Adewo/Documents/Mendley Documents/Temporal Segment Networks for Action Recognition in Videos - Wang et al. - 2017.pdf:pdf},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Action recognition,ConvNets,good practices,temporal modeling,temporal segment networks},
month = {may},
number = {11},
pages = {2740--2755},
publisher = {IEEE Computer Society},
title = {{Temporal Segment Networks for Action Recognition in Videos}},
url = {http://arxiv.org/abs/1705.02953},
volume = {41},
year = {2017}
}
@article{Lee2020,
abstract = {Weakly-supervised temporal action localization aims to detect intervals of action instances with only video-level action labels for training. A crucial challenge is to separate frames of action classes from remaining, denoted as background frames (i.e., frames not belonging to any action class). Previous methods attempt background modeling by either synthesizing pseudo background videos with static frames or introducing an auxiliary class for background. However, they overlook an essential fact that background frames could be dynamic and inconsistent. Accordingly, we cast the problem of identifying background frames as out-of-distribution detection and isolate it from conventional action classification. Beyond our base action localization network, we propose a module to estimate the probability of being background (i.e., uncertainty [20]), which allows us to learn uncertainty given only video-level labels via multiple instance learning. A background entropy loss is further designed to reject background frames by forcing them to have uniform probability distribution for action classes. Extensive experiments verify the effectiveness of our background modeling and show that our method significantly outperforms state-of-the-art methods on the standard benchmarks - THUMOS'14 and ActivityNet (1.2 and 1.3). Our code and the trained model are available at https://github.com/Pilhyeon/Background-Modeling-via-Uncertainty-Estimation.},
annote = {The main task adressed in this paper is localizing action and seperating background frames that does not contain action class. The model was tested on Thumos dataset and I3D.

The data used are untrimmed videos used to reject bakgroundframes as out of detection.

Method
weakly supervised learning was used to train a model for localizing acton and detecting action. 
Step one feature extraction: Voideo were split into multiframe non overlapping segements to mitigate videos with varied length.
Feature Embedding: Extracted features was fed into 1D convolution layer with Relu Activation
Segement Classification: Sgement level class score was predicted for the embedded features for action localization

Action score segregation: Video level action probability was aggregated with softmax function to score all segenments for each actions to build a video level class score

The model was optimized with three loses 
1.Action Clssification Losses
2. Uncertainty modelling loss
3. Background entropy loss

Full code availableon github repo},
archivePrefix = {arXiv},
arxivId = {2006.07006},
author = {Lee, Pilhyeon and Wang, Jinglu and Lu, Yan and Byun, Hyeran},
eprint = {2006.07006},
file = {::},
isbn = {2006.07006v1},
title = {{Background Modeling via Uncertainty Estimation for Weakly-supervised Action Localization}},
url = {https://github.com/Pilhyeon/Background-Modeling-via-Uncertainty-Estimation. http://arxiv.org/abs/2006.07006},
year = {2020}
}
@article{Soomro2014,
abstract = {The ability to analyze the actions which occur in a video is essential for automatic understanding of sports. Action localization and recognition in videos are two main research topics in this context. In this chapter, we provide a detailed study of the prominent methods devised for these two tasks which yield superior results for sports videos.We adopt UCF Sports, which is a dataset of realistic sports videos collected from broadcast television channels, as our evaluation benchmark. First, we present an overview of UCF Sports along with comprehensive statistics of the techniques tested on this dataset as well as the evolution of their performance over time. To provide further details about the existing action recognition methods in this area, we decompose the action recognition framework into three main steps of feature extraction, dictionary learning to represent a video, and classification; we overview several successful techniques for each of these steps. We also overview the problem of spatio-temporal localization of actions and argue that, in general, it manifests a more challenging problem compared to action recognition. We study several recent methods for action localizationwhich have shown promising results on sports videos. Finally, we discuss a number of forward-thinking insights drawn from overviewing the action recognition and localization methods. In particular, we argue that performing the recognition on temporally untrimmed videos and attempting to describe an action, instead of conducting a forced-choice classification, are essential for analyzing the human actions in a realistic environment.},
annote = {Action localization and action recognition in videos are gray aread and dosent necessarily mean the same thing.
The paper used the UCF sports dataset as a benchmark for their task.

Most action regonition reserach work utilized three step in vodeo classification. 
1. Features extraction,
2. dictionary learning and 
3. classification


The UCF sport dataset contains 10 actions, 150 clips (10fps) 

Action localization can be more challenging becausethe action class has to be correctly identified and also determine his spatio temporal location. only 15% of reseearch papers on UCF discussed action localization results. 

Experimental setup for action locatization used Leave one out (LOO). which trains dataset on 9 actions and leaves only one out.
This approach was critized with experimental backup that video may have similiar background and consequetially yeild higher acuracy due to high correlation between videos. 

For feature extraction: Features such at data pont carry distinguishing information about actions being performed this can be acheived in two ways:
1> Low level: detects interest points related perfomed action. using sparse keppoints such as corners, edges blobs etc
2> High level: captires specific further infomation such as pose, shape and contextual information.


Space-Time Interest Points (STIP) were developed to capture feautres in video . 3D STIP

The action localizatiom task can be achieved by using sliding window approach . This methods slides in the spatio temporal volume of the video and finds the subvolume with highest scores and assigns a bounding box},
author = {Soomro, Khurram and Zamir, Amir R},
doi = {10.1007/978-3-319-09396-3_9},
file = {:C\:/Users/Adewo/Documents/Mendley Documents/Computer Vision in Sports - Soomro et al. - 2014.pdf:pdf},
issn = {21916594},
journal = {Advances in Computer Vision and Pattern Recognition},
pages = {181--208},
title = {{Action recognition in realistic sports videos}},
url = {http://crcv.ucf.edu/data/UCF_Sports_Action.php.},
volume = {71},
year = {2014}
}
@article{Ren2017,
abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features - using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model [3], our detection system has a frame rate of 5 fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
annote = {Methodology:
The paper adopts a method of localizing objects in a given image and putting ina bounding box for a faster object detection.
It combines detecting object and a bulding box through a region proposal network.
The full code is available on github},
archivePrefix = {arXiv},
arxivId = {1506.01497},
author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
doi = {10.1109/TPAMI.2016.2577031},
eprint = {1506.01497},
file = {:C\:/Users/Adewo/Documents/Mendley Documents/Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks - Ren et al. - 2017.pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Object detection,convolutional neural network,region proposal},
number = {6},
pages = {1137--1149},
pmid = {27295650},
title = {{Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks}},
url = {http://image-net.org/challenges/LSVRC/2015/results},
volume = {39},
year = {2017}
}
@article{Li2020,
abstract = {This paper describes the AVA-Kinetics localized human actions video dataset. The dataset is collected by annotating videos from the Kinetics-700 dataset using the AVA annotation protocol, and extending the original AVA dataset with these new AVA annotated Kinetics clips. The dataset contains over 230k clips annotated with the 80 AVA action classes for each of the humans in key-frames. We describe the annotation process and provide statistics about the new dataset. We also include a baseline evaluation using the Video Action Transformer Network on the AVA-Kinetics dataset, demonstrating improved performance for action classification on the AVA test set. The dataset can be downloaded from https://research.google.com/ava/},
annote = {This paper desscribed the process of annotating the AVA and Kinetict dataset from video dataset.
This paper built on the AVA and kinetics &)) dataset by providingbhuman annotation and localization for the dataset.
The AVA dataset denseley annotates 80 visual action in 430 15 minute clips. Each person in the vidual has its own bounding box with appropriate labels for co occuring actions about 1.6m labels.

In the kinetics 700 dataset, there are about 600 clips for each action class and a total of 650k clips for all class of about 700 human actions.

In detecting the bounding boxes an algorithm Faster RCNN was used proposed by(Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks -Shaoqing Ren)
Human annotators were utilized in annotating missing boundry box and also providel labels manually for the bounding boxes detected by the F RCNN.},
archivePrefix = {arXiv},
arxivId = {2005.00214},
author = {Li, Ang and Thotakuri, Meghana and Ross, David A. and Carreira, Jo{\~{a}}o and Vostrikov, Alexander and Zisserman, Andrew},
eprint = {2005.00214},
file = {:C\:/Users/Adewo/Documents/Mendley Documents/The AVA-Kinetics Localized Human Actions Video Dataset - Li et al. - 2020(2).pdf:pdf},
month = {may},
title = {{The AVA-Kinetics Localized Human Actions Video Dataset}},
url = {http://arxiv.org/abs/2005.00214},
year = {2020}
}
@article{Kay2017,
abstract = {We describe the DeepMind Kinetics human action video dataset. The dataset contains 400 human action classes, with at least 400 video clips for each action. Each clip lasts around 10s and is taken from a different YouTube video. The actions are human focussed and cover a broad range of classes including human-object interactions such as playing instruments, as well as human-human interactions such as shaking hands. We describe the statistics of the dataset, how it was collected, and give some baseline performance figures for neural network architectures trained and tested for human action classification on this dataset. We also carry out a preliminary analysis of whether imbalance in the dataset leads to bias in the classifiers.},
annote = {This paper detailed the process of data collection for the Kinectics dataset 400.
This dataset contains about 400 human actions and 10 s each.
Data was collected from Youtube based on Matching results with Kinetcics action list.

The advantage of Kintetics over UCF101 is that the dataset was collected from various yourtube users and different lightning conditions unlike the UCF dataset that contains multiple clips of repeated action from a single video.

The dataset was collected from youtube and labeled manually using Ama-zon Mechanical Turkers (AMT) to label videos that contains human action.

We can leverage the baby waking up dataset from kinetics that contains 600 clips of baby waking up.},
archivePrefix = {arXiv},
arxivId = {1705.06950},
author = {Kay, Will and Carreira, Joao and Simonyan, Karen and Zhang, Brian and Hillier, Chloe and Vijayanarasimhan, Sudheendra and Viola, Fabio and Green, Tim and Back, Trevor and Natsev, Paul and Suleyman, Mustafa and Zisserman, Andrew},
eprint = {1705.06950},
file = {:C\:/Users/Adewo/Documents/Mendley Documents/The Kinetics Human Action Video Dataset - Kay et al. - 2017.pdf:pdf},
month = {may},
title = {{The Kinetics Human Action Video Dataset}},
url = {http://arxiv.org/abs/1705.06950},
year = {2017}
}
@techreport{Dwibedi,
abstract = {We present an approach for estimating the period with which an action is repeated in a video. The crux of the approach lies in constraining the period prediction module to use temporal self-similarity as an intermediate representation bottleneck that allows generalization to unseen repetitions in videos in the wild. We train this model, called RepNet, with a synthetic dataset that is generated from a large unlabeled video collection by sampling short clips of varying lengths and repeating them with different periods and counts. This combination of synthetic data and a powerful yet constrained model, allows us to predict periods in a class-agnostic fashion. Our model substantially exceeds the state of the art performance on existing period-icity (PERTUBE) and repetition counting (QUVA) benchmarks. We also collect a new challenging dataset called Countix (∼90 times larger than existing datasets) which captures the challenges of repetition counting in real-world videos. Project webpage: https://sites.google. com/view/repnet.},
annote = {This paper focused of determining the number of times an action is repeated in a video stream. Leveraging on the kinetics data set with actions of interest they selected the videos and manually labeled the datasets with the number of repeated actions. Multiple Clips from a single video with different number of repeated actions was utilized in this dataset.

in Conclusion this paper was able to predict the number of times an action was done in a dataset.
Model This paper for my dataset curation and Images},
archivePrefix = {arXiv},
arxivId = {2006.15418v1},
author = {Dwibedi, Debidatta and Aytar, Yusuf and Tompson, Jonathan and Sermanet, Pierre and Zisserman, Andrew},
eprint = {2006.15418v1},
file = {:C\:/Users/Adewo/Documents/Mendley Documents/Counting Out Time Class Agnostic Video Repetition Counting in the Wild - Dwibedi et al. - Unknown.pdf:pdf},
title = {{Counting Out Time: Class Agnostic Video Repetition Counting in the Wild}},
url = {https://sites.google.}
}
@inproceedings{Baradel2018,
abstract = {Human activity recognition is typically addressed by detecting key concepts like global and local motion, features related to object classes present in the scene, as well as features related to the global context. The next open challenges in activity recognition require a level of understanding that pushes beyond this and call for models with capabilities for fine distinction and detailed comprehension of interactions between actors and objects in a scene. We propose a model capable of learning to reason about semantically meaningful spatio-temporal interactions in videos. The key to our approach is a choice of performing this reasoning at the object level through the integration of state of the art object detection networks. This allows the model to learn detailed spatial interactions that exist at a semantic, object-interaction relevant level. We evaluate our method on three standard datasets (Twenty-BN Something-Something, VLOG and EPIC Kitchens) and achieve state of the art results on all of them. Finally, we show visualizations of the interactions learned by the model, which illustrate object classes and their interactions corresponding to different activity classes.},
archivePrefix = {arXiv},
arxivId = {1806.06157v3},
author = {Baradel, Fabien and Neverova, Natalia and Wolf, Christian and Mille, Julien and Mori, Greg},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-030-01261-8_7},
eprint = {1806.06157v3},
file = {:C\:/Users/Adewo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Object Level Visual Reasoning in Videos - Baradel et al. - Unknown.pdf:pdf},
isbn = {9783030012601},
issn = {16113349},
keywords = {Human-object interaction,Video understanding},
pages = {106--122},
title = {{Object level visual reasoning in videos}},
url = {https://fabienbaradel.github.io/eccv18_object_level_visual_reasoning/},
volume = {11217 LNCS},
year = {2018}
}
@article{Li2020a,
abstract = {With the success of deep learning in classifying short trimmed videos, more attention has been focused on temporally segmenting and classifying activities in long untrimmed videos. State-of-the-art approaches for action segmentation utilize several layers of temporal convolution and temporal pooling. Despite the capabilities of these approaches in capturing temporal dependencies, their predictions suffer from over-segmentation errors. In this paper, we propose a multi-stage architecture for the temporal action segmentation task that overcomes the limitations of the previous approaches. The first stage generates an initial prediction that is refined by the next ones. In each stage we stack several layers of dilated temporal convolutions covering a large receptive field with few parameters. While this architecture already performs well, lower layers still suffer from a small receptive field. To address this limitation, we propose a dual dilated layer that combines both large and small receptive fields. We further decouple the design of the first stage from the refining stages to address the different requirements of these stages. Extensive evaluation shows the effectiveness of the proposed model in capturing long-range dependencies and recognizing action segments. Our models achieve state-of-the-art results on three datasets: 50Salads, Georgia Tech Egocentric Activities (GTEA), and the Breakfast dataset.},
archivePrefix = {arXiv},
arxivId = {2006.09220},
author = {Li, Shijie and Farha, Yazan Abu and Liu, Yun and Cheng, Ming-Ming and Gall, Juergen},
eprint = {2006.09220},
file = {:C\:/Users/Adewo/Documents/Mendley Documents/MS-TCN Multi-Stage Temporal Convolutional Network for Action Segmentation - Li et al. - 2020.pdf:pdf},
keywords = {Index Terms-Temporal action segmentation,temporal convolutional network !},
month = {jun},
title = {{MS-TCN++: Multi-Stage Temporal Convolutional Network for Action Segmentation}},
url = {http://arxiv.org/abs/2006.09220},
year = {2020}
}
@article{Lea2016,
abstract = {The ability to identify and temporally segment fine-grained human actions throughout a video is crucial for robotics, surveillance, education, and beyond. Typical approaches decouple this problem by first extracting local spatiotemporal features from video frames and then feeding them into a temporal classifier that captures high-level temporal patterns. We introduce a new class of temporal models, which we call Temporal Convolutional Networks (TCNs), that use a hierarchy of temporal convolutions to perform fine-grained action segmentation or detection. Our Encoder-Decoder TCN uses pooling and upsampling to efficiently capture long-range temporal patterns whereas our Dilated TCN uses dilated convolutions. We show that TCNs are capable of capturing action compositions, segment durations, and long-range dependencies, and are over a magnitude faster to train than competing LSTM-based Recurrent Neural Networks. We apply these models to three challenging fine-grained datasets and show large improvements over the state of the art.},
archivePrefix = {arXiv},
arxivId = {1611.05267},
author = {Lea, Colin and Flynn, Michael D. and Vidal, Rene and Reiter, Austin and Hager, Gregory D.},
eprint = {1611.05267},
file = {:C\:/Users/Adewo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Temporal Convolutional Networks for Action Segmentation and Detection - Lea et al. - 2016.pdf:pdf},
journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
month = {nov},
pages = {1003--1012},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Temporal Convolutional Networks for Action Segmentation and Detection}},
url = {http://arxiv.org/abs/1611.05267},
volume = {2017-Janua},
year = {2016}
}
@inproceedings{Piergiovanni2019,
abstract = {In this paper, we propose a convolutional layer inspired by optical flow algorithms to learn motion representations. Our representation flow layer is a fully-differentiable layer designed to capture the 'flow' of any representation channel within a convolutional neural network for action recognition. Its parameters for iterative flow optimization are learned in an end-to-end fashion together with the other CNN model parameters, maximizing the action recognition performance. Furthermore, we newly introduce the concept of learning 'flow of flow' representations by stacking multiple representation flow layers. We conducted extensive experimental evaluations, confirming its advantages over previous recognition models using traditional optical flows in both computational speed and performance. The code is publicly available.},
annote = {The reseaserachers worked on developing representation flow . Representation flow is a differentiable layer designed to capture the flow of the represntation layer within a Convolutional Neural Network for AR.
Two Stream network with both RGB and optical flow has been successful in increasing accuracy of Action Recognition tasks how ever computing optical flow at each frame iteration is computationally expensive and also increase the number of parameters required for inferencing. This serve as a draw back in real time application of AR tasks beacuse of the numerious activities required with short period. The resesearches stacked multiple representation flow tagged Flow of Flow to eliminate the requirements for optical flow during inferencing.
In this research stacking multiple representation flow layers, the devloped model captures long temporal intervals, CNN was intoroduced to compute intermediate future maps and convolutions to reduce number of channels before the oprical flow algorithm.
The authors contributed the body of knowledge introducing a new differentiable CNN feature map for intermeddiate represntations which outperforms existing motion representation methods with increases speed and accuracy.
This paper proposed a new reepresentation optical representation(Flow of flow) in the CNN two streams design RGB + optical flow. The optical flow requires optimization and computational exensive.
The researchers deloved optical flow algorithm to learn motion representations with a CNN for AR tast. flow of Flow represntation was achieved by stacking multiple represntation flow layers. Utilizing optical flow requires mutiple optimization iterations in every frame thereby leading to increased trainning parameters.},
archivePrefix = {arXiv},
arxivId = {1810.01455},
author = {Piergiovanni, A J and Ryoo, Michael S},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2019.01018},
eprint = {1810.01455},
file = {:C\:/Users/Adewo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Representation Flow for Action Recognition - Piergiovanni, Ryoo - Unknown.pdf:pdf;:C\:/Users/Adewo/Documents/Mendley Documents/Representation flow for action recognition.pdf:pdf},
isbn = {9781728132938},
issn = {10636919},
keywords = {Action Recognition,Deep Learning,Representation Learning},
pages = {9937--9945},
title = {{Representation flow for action recognition}},
url = {https://piergiaj.github.io/rep-flow-site/},
volume = {2019-June},
year = {2019}
}
@techreport{Malekzadeh,
abstract = {Sensitive inferences and user re-identification are major threats to privacy when raw sensor data from wearable or portable devices are shared with cloud-assisted applications. To mitigate these threats, we propose mechanisms to transform sensor data before sharing them with applications running on users' devices. These transformations aim at eliminating patterns that can be used for user re-identification or for inferring potentially sensitive activities, while introducing a minor utility loss for the target application (or task). We show that, on gesture and activity recognition tasks, we can prevent inference of potentially sensitive activities while keeping the reduction in recognition accuracy of non-sensitive activities to less than 5 percentage points. We also show that we can reduce the accuracy of user re-identification and of the potential inference of gender to the level of a random guess, while keeping the accuracy of activity recognition comparable to that obtained on the original data.},
archivePrefix = {arXiv},
arxivId = {1911.05996v1},
author = {Malekzadeh, Mohammad and Clegg, Richard G and Cavallaro, Andrea and Haddadi, Hamed},
eprint = {1911.05996v1},
file = {:C\:/Users/Adewo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Privacy and Utility Preserving Sensor-Data Transformations 1 - Malekzadeh et al. - Unknown.pdf:pdf},
title = {{Privacy and Utility Preserving Sensor-Data Transformations 1}},
url = {https://github.com/mmalekzadeh/motion-sense}
}
@article{Ma2019,
abstract = {Recent two-stream deep Convolutional Neural Networks (ConvNets) have made significant progress in recognizing human actions in videos. Despite their success, methods extending the basic two-stream ConvNet have not systematically explored possible network architectures to further exploit spatiotemporal dynamics within video sequences. Further, such networks often use different baseline two-stream networks. Therefore, the differences and the distinguishing factors between various methods using Recurrent Neural Networks (RNN) or Convolutional Neural Networks on temporally-constructed feature vectors (Temporal-ConvNets) are unclear. In this work, we would like to answer the question: given the spatial and motion feature representations over time, what is the best way to exploit the temporal information? Toward this end, we first demonstrate a strong baseline two-stream ConvNet using ResNet-101. We use this baseline to thoroughly examine the use of both RNNs and Temporal-ConvNets for extracting spatiotemporal information. Building upon our experimental results, we then propose and investigate two different networks to further integrate spatiotemporal information: (1) Temporal Segment RNN and (2) Inception-style Temporal-ConvNet. We demonstrate that using both RNNs (with LSTMs) and Temporal-ConvNets on spatiotemporal feature matrices are able to exploit spatiotemporal dynamics to improve the overall performance. Our analysis identifies specific limitations for each method that could form the basis of future work. Our experimental results on UCF101 and HMDB51 datasets achieve comparable state-of-the-art performances, 94.1% and 69.0%, respectively, without requiring extensive temporal augmentation or end-to-end training.},
annote = {This paper utilize ConvNet and two-stream ConvNet to provide indepth analysis decision for both RNN and Tempral ConvNet.
The use of spatial RGB images have been reported to hve a good performance with a potential possibility of mor improved performance by stacking thr RGB difference in i a video stream.
using a pretrained network and fine tunning of parameters are helpful in temporal stream network.

Deeper LSTM cells for action recognitiion tasks have a very limited impprovement on classification performance in AR

The researchers proposed temporal segment RNN and 2) Inception-style Temporal-ConvNet improves overall performance of action recognition task by extracting and inetgration spation temporal information in video stream.

Donahue et al. [4] with ConvNets and RNNs utilized spatial features extracted from each time step into Recurrent Neural network with LSTM cells},
archivePrefix = {arXiv},
arxivId = {1703.10667},
author = {Ma, Chih Yao and Chen, Min Hung and Kira, Zsolt and AlRegib, Ghassan},
doi = {10.1016/j.image.2018.09.003},
eprint = {1703.10667},
file = {:C\:/Users/Adewo/Documents/Mendley Documents/TS-LSTM and Temporal-Inception Exploiting Spatiotemporal Dynamics for Activity Recognition - Ma et al. - Unknown.pdf:pdf},
issn = {09235965},
journal = {Signal Processing: Image Communication},
keywords = {Action recognition,Convolutional neural network,Recurrent neural network,Video understanding},
pages = {76--87},
title = {{TS-LSTM and temporal-inception: Exploiting spatiotemporal dynamics for activity recognition}},
volume = {71},
year = {2019}
}
@techreport{Zhou,
abstract = {Temporal relational reasoning, the ability to link meaningful transformations of objects or entities over time, is a fundamental property of intelligent species. In this paper, we introduce an effective and interpretable network module, the Temporal Relation Network (TRN), designed to learn and reason about temporal dependencies between video frames at multiple time scales. We evaluate TRN-equipped networks on activity recognition tasks using three recent video datasets-Something-Something, Jester, and Charades-which fundamentally depend on temporal relational reasoning. Our results demonstrate that the proposed TRN gives convolutional neural networks a remarkable capacity to discover temporal relations in videos. Through only sparsely sampled video frames, TRN-equipped networks can accurately predict human-object interactions in the Something-Something dataset and identify various human gestures on the Jester dataset with very competitive performance. TRN-equipped networks also outperform two-stream networks and 3D convolution networks in recognizing daily activities in the Charades dataset. Further analyses show that the models learn intuitive and interpretable visual common sense knowledge in videos 1 .},
annote = {Temporal ralational reasoning in video paper focus on inferening the relationship between transformation/temporal dependencies between two video frames. 
Human are able to intuitively deduce action that happend betwwen two obervations by decuctive reasoning of actions that occured between two states. However this is a challenge for DNN. The ambiguity of accurate description of activities in a timeframe is a major barrier in video recognition.

Approach: A TRN (Temporal Network relation) was proposed to enable temporal relational reasoning .This model can describe a temporal relation or possible relations between observations in video.
models was applied on benchmark dataset s (Something-Something, Jester ,and Charades).
A core computer vision challenge is the recognition of action in a video. The advent of Depp CNN has helped in siginificant performance in image classification.

The 3D cnn have limited sequences of frame fed into the network with is comutationally costly, the temporal raltion network adress the challenge by sampling individual frames to understand the causal relationship.

Previous work modeled temporal structure for action recognition using bag of words, atomor action frammar. 
The model perform better than the baseline with a wide margin},
archivePrefix = {arXiv},
arxivId = {1711.08496v2},
author = {Zhou, Bolei and Andonian, Alex and Oliva, Aude and Torralba, Antonio},
eprint = {1711.08496v2},
file = {:C\:/Users/Adewo/Documents/Mendley Documents/Temporal Relational Reasoning in Videos - Zhou et al. - Unknown.pdf:pdf},
title = {{Temporal Relational Reasoning in Videos}},
url = {http://relation.csail.mit.edu/.}
}
@techreport{Kalfaoglu,
abstract = {In this work, we combine 3D convolution with late temporal modeling for action recognition. For this aim, we replace the conventional Temporal Global Average Pooling (TGAP) layer at the end of 3D con-volutional architecture with the Bidirectional Encoder Representations from Transformers (BERT) layer in order to better utilize the temporal information with BERT's attention mechanism. We show that this replacement improves the performances of many popular 3D convolution architectures for action recognition, including ResNeXt, I3D, SlowFast and R(2+1)D. Moreover, we provide the-state-of-the-art results on both HMDB51 and UCF101 datasets with 85.10% and 98.69% top-1 accuracy, respectively. The code is publicly available 4 .},
annote = {Spatial Information captures actions in a static frames and temporal information is obtained by harnessing all spatial information over all frames. This paper proposed using a biderectonal encoder o the a 3D convolution architecture.
The TGAP(Temporal average Pooling) may not cover necessary information and effective sequences. Bert was introduced for better temporal modelling at the end of the architecture. BERT hs been sucessfuly in NKP tasks for fusing contextual information from both directions.

ADAMW optimizer was used with BERT for its ability to generalize instead of ADAM 

Dataset
$ datasets were used HMDB51 , UCF101 ,Kinetics- 400 and IG65M .
The data were in different clases of action.
Each Video has a classs it belongs to. 
The resesrcher stated the dataset are not publicly available but the codes used are available.
Kinetics and IG65M was user for pre trained weights. The pre-trained weights are btained from authors of architectures ResNeXTt and I3d.},
archivePrefix = {arXiv},
arxivId = {2008.01232v2},
author = {Kalfaoglu, M Esat and Kalkan, Sinan and Alatan, A Aydin},
eprint = {2008.01232v2},
file = {:C\:/Users/Adewo/Documents/Mendley Documents/Late Temporal Modeling in 3D CNN Architectures with BERT for Action Recognition - Kalfaoglu, Kalkan, Alatan - Unknown.pdf:pdf},
keywords = {3D Convolution,Action Recognition,BERT,Late Tem-poral Modeling,Temporal Attention},
title = {{Late Temporal Modeling in 3D CNN Architectures with BERT for Action Recognition}}
}
@inproceedings{Farha2019,
abstract = {Temporally locating and classifying action segments in long untrimmed videos is of particular interest to many applications like surveillance and robotics. While traditional approaches follow a two-step pipeline, by generating frame-wise probabilities and then feeding them to high-level temporal models, recent approaches use temporal convolutions to directly classify the video frames. In this paper, we introduce a multi-stage architecture for the temporal action segmentation task. Each stage features a set of dilated temporal convolutions to generate an initial prediction that is refined by the next one. This architecture is trained using a combination of a classification loss and a proposed smoothing loss that penalizes over-segmentation errors. Extensive evaluation shows the effectiveness of the proposed model in capturing long-range dependencies and recognizing action segments. Our model achieves state-of-the-art results on three challenging datasets: 50Salads, Georgia Tech Egocentric Activities (GTEA), and the Breakfast dataset.},
archivePrefix = {arXiv},
arxivId = {2006.09220},
author = {Farha, Yazan Abu and Gall, Jurgen},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2019.00369},
eprint = {2006.09220},
file = {:C\:/Users/Adewo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/MS-TCN Multi-Stage Temporal Convolutional Network for Action Segmentation - Farha, Gall - 2019.pdf:pdf;:C\:/Users/Adewo/Documents/Mendley Documents/1903.01945.pdf:pdf},
isbn = {9781728132938},
issn = {10636919},
keywords = {Action Recognition},
month = {mar},
pages = {3570--3579},
publisher = {IEEE Computer Society},
title = {{MS-TCN: Multi-stage temporal convolutional network for action segmentation}},
url = {http://arxiv.org/abs/1903.01945},
volume = {2019-June},
year = {2019}
}
@article{Montes2016,
abstract = {This thesis explore different approaches using Convolutional and Recurrent Neural Networks to classify and temporally localize activities on videos, furthermore an implementation to achieve it has been proposed. As the first step, features have been extracted from video frames using an state of the art 3D Convolutional Neural Network. This features are fed in a recurrent neural network that solves the activity classification and temporally location tasks in a simple and flexible way. Different architectures and configurations have been tested in order to achieve the best performance and learning of the video dataset provided. In addition it has been studied different kind of post processing over the trained network's output to achieve a better results on the temporally localization of activities on the videos. The results provided by the neural network developed in this thesis have been submitted to the ActivityNet Challenge 2016 of the CVPR, achieving competitive results using a simple and flexible architecture.},
annote = {This paper used features from Convolutional Neural Network (C3D) to train RNN.
The challenge addressed in this paper is that previous researchers focused on prediction in trimmed video. However, the researchers focused on recognizing activities from untrimmed video and also identify temporal segement where activity occured in video .

Approach: A 3D-CNN that exploits spatial and short temporal corellations and a RNN with long temporal correlations 

Future work: The paper proposed to tain models with vieos with different activity because the paper focused on single activity in each video.

Pastwork: Previous work used 2D CNN to explore spatial correlation between frmes in a video and agumented with optical flow to provide information on temporal correlation. 3D CNN have a better video classification , however LSTM are typr of RNN that exploit short temporal correlaton better in sequences. This study combines the 3D Cnn and RNN LSTM into a single framework. 

Method: Videos were splitted to 16 frames and resized to fit C3D input. Averafe Class probability was computed for the 16 frames to dtermine the class with activity or no activity. Only Class with activity probability iver a sopecified threshold are kept and labeled. 640 hours videos with 64 mi;llion frames was used. 50% for training},
archivePrefix = {arXiv},
arxivId = {1608.08128},
author = {Montes, Alberto and Salvador, Amaia and Pascual, Santiago and Giro-i-Nieto, Xavier},
eprint = {1608.08128},
file = {:C\:/Users/Adewo/Documents/Mendley Documents/Temporal Activity Detection in Untrimmed Videos with Recurrent Neural Networks - Montes et al. - Unknown.pdf:pdf},
title = {{Temporal Activity Detection in Untrimmed Videos with Recurrent Neural Networks}},
url = {https://github.com/imatge-upc/ http://arxiv.org/abs/1608.08128},
year = {2016}
}
@article{Authors1960a,
abstract = {Action segmentation as a milestone towards building automatic systems to understand untrimmed videos has received considerable attention in the recent years. It is typically being modeled as a sequence labeling problem but contains intrinsic and sufficient differences than text parsing or speech processing. In this paper, we introduce a novel hybrid temporal convolutional and recurrent network (TricorNet), which has an encoder-decoder architecture: the encoder consists of a hierarchy of temporal convolutional kernels that capture the local motion changes of different actions; the decoder is a hierarchy of recurrent neural networks that are able to learn and memorize long-term action dependencies after the encoding stage. Our model is simple but extremely effective in terms of video sequence labeling. The experimental results on three public action segmentation datasets have shown that the proposed model achieves superior performance over the state of the art.},
annote = {This paper proposes a hybrid temporal convolutional and recurrent network (TricorNet) in video sequence labelling.
Segementing action is a challenge in video understanding, the methodology can be used to label different actions in a given video based on time sequence.
Previous approach used Two-stream CNN and 3D ConvNets. Anothe deconvolution approach using hierarchical 1D temporal Convolution and Deconvolution network performed better in action segementation however, does not capture other long term actions in video due to its fixed length. This paper imporved on the previous work by using 1D convolutional kernels in encoding and Bi_LSTMs hierarchy of RNN in decoing to mitigate the drawback of capturing different actions in long term videos.

Method: TricorNet has temporal convolutional kernels for encoding layers LE and Bi-LSTMs for decoding layers LD with a spatial dropout in all the convolution layers.
The model was tested with a data that captures 25 people tryin to prepare mxed salad each time with 17 different actions and the video duration is 5-10 minutes.

The limitation noted in the paper is that the model overfits or get stucked if the layers K are more than 2.},
author = {Authors, Anonymous},
file = {:C\:/Users/Adewo/Documents/Mendley Documents/VIDEO ACTION SEGMENTATION WITH HYBRID TEM-PORAL NETWORKS - Unknown - Unknown.pdf:pdf},
number = {2015},
pages = {6215},
title = {{VIDEO ACTION SEGMENTATION WITH HYBRID TEM- PORAL NETWORKS}},
year = {1960}
}
@techreport{Zhang,
abstract = {Efficiently modeling dynamic motion information in videos is crucial for action recognition task. Most state-of-the-art methods heavily rely on dense optical flow as motion representation. Although combining optical flow with RGB frames as input can achieve excellent recognition performance, the optical flow extraction is very time-consuming. This undoubtably will count against real-time action recognition. In this paper, we shed light on fast action recognition by lifting the reliance on optical flow. Our motivation lies in the observation that small displacements of motion boundaries are the most critical ingredients for distinguishing actions, so we design a novel motion cue called Persistence of Appearance (PA). In contrast to optical flow, our PA focuses more on distilling the motion information at boundaries. Also, it is more efficient by only accumulating pixel-wise differences in feature space, instead of using exhaustive patch-wise search of all the possible motion vectors. Our PA is over 1000× faster (8196fps vs. 8fps) than conventional optical flow in terms of motion modeling speed. To further aggregate the short-term dynamics in PA to long-term dynamics, we also devise a global temporal fusion strategy called Various-timescale Aggregation Pooling (VAP) that can adaptively model long-range temporal relationships across various timescales. We finally incorporate the proposed PA and VAP to form a unified framework called Persistent Appearance Network (PAN) with strong temporal modeling ability. Extensive experiments on six challenging action recognition benchmarks verify that our PAN outperforms recent state-of-the-art methods at low FLOPs. Codes and models are available at: https://github.com/zhang-can/PAN-PyTorch.},
archivePrefix = {arXiv},
arxivId = {2008.03462v1},
author = {Zhang, Can and Zou, Yuexian and Chen, Guang and Gan, Lei},
eprint = {2008.03462v1},
file = {:C\:/Users/Adewo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/PAN Towards Fast Action Recognition via Learning Persistence of Appearance - Zhang et al. - Unknown.pdf:pdf},
keywords = {Index Terms-Fast Action Recognition,Motion Representa-tion,Persistence of Appearance,Persistent Appearance Network},
title = {{PAN: Towards Fast Action Recognition via Learning Persistence of Appearance}},
url = {https://github.com/zhang-can/PAN-PyTorch.}
}
@techreport{Sudhakaran,
abstract = {Deep 3D CNNs for video action recognition are designed to learn powerful representations in the joint spatio-temporal feature space. In practice however, because of the large number of parameters and computations involved, they may under-perform in the lack of sufficiently large datasets for training them at scale. In this paper we introduce spatial gating in spatial-temporal decomposition of 3D kernels. We implement this concept with Gate-Shift Module (GSM). GSM is lightweight and turns a 2D-CNN into a highly efficient spatio-temporal feature extrac-tor. With GSM plugged in, a 2D-CNN learns to adaptively route features through time and combine them, at almost no additional parameters and computational overhead. We perform an extensive evaluation of the proposed module to study its effectiveness in video action recognition, achieving state-of-the-art results on Something Something-V1 and Diving48 datasets, and obtaining competitive results on EPIC-Kitchens with far less model complexity.},
archivePrefix = {arXiv},
arxivId = {1912.00381v2},
author = {Sudhakaran, Swathikiran and Escalera, Sergio and Lanz, Oswald},
eprint = {1912.00381v2},
file = {:C\:/Users/Adewo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gate-Shift Networks for Video Action Recognition - Sudhakaran, Escalera, Lanz - Unknown.pdf:pdf},
title = {{Gate-Shift Networks for Video Action Recognition}}
}
@techreport{Ghadiyaram2019,
abstract = {Current fully-supervised video datasets consist of only a few hundred thousand videos and fewer than a thousand domain-specific labels. This hinders the progress towards advanced video architectures. This paper presents an in-depth study of using large volumes of web videos for pre-training video models for the task of action recognition. Our primary empirical finding is that pre-training at a very large scale (over 65 million videos), despite on noisy social-media videos and hashtags, substantially improves the state-of-the-art on three challenging public action recognition datasets. Further, we examine three questions in the construction of weakly-supervised video action datasets. First, given that actions involve interactions with objects, how should one construct a verb-object pre-training label space to benefit transfer learning the most? Second, frame-based models perform quite well on action recognition; is pre-training for good image features sufficient or is pre-training for spatio-temporal features valuable for optimal transfer learning? Finally, actions are generally less well-localized in long videos vs. short videos; since action labels are provided at a video level, how should one choose video clips for best performance, given some fixed budget of number or minutes of videos?},
annote = {Trainning a model on large scale web video with a lot of noise often yeild a better result. With image classification, trainning with large volume of noisy web images yeilded a better results, this results can not be ifered for action recognition in videos as the pose a unique challenge including :
1. Temporal noise and Label Noise due to inability to loacalize action labels
2. Action labelling in videos involving more than one verb object, i.e two action taking place e.g Climbing tree.
3 Temporal Dimensions Short vd long videos. General assumption is that short videos contain few frames that are more relevant.

This study examines the impact of short videos and long videos for pre trainning, data volume and model capacity and a suitable pr trainning video. It was discovered that increasing the data volume increased performance and also shorter videos is more beneficial as they provide succint localized action.

Methods: This paper utilized open source dataset from social media(Youtube, Flickr etc). The data utilized a weak supervision of video models by using the video hashtag as the video label. 
Videos are about 1 to 60 seconds long which provides enough temporal noise and labeled action.
Model was trained on 65M Videos , however, the data was seperated into chunks, Performance increase in terms of amount of train data.},
archivePrefix = {arXiv},
arxivId = {1905.00561v1},
author = {Ghadiyaram, Deepti and Feiszli, Matt and Tran, Du and Yan, Xueting and Wang, Heng and Mahajan, Dhruv and Ai, Facebook},
eprint = {1905.00561v1},
file = {:C\:/Users/Adewo/Documents/Mendley Documents/Large-scale weakly-supervised pre-training for video action recognition - Ghadiyaram et al. - 2019.pdf:pdf},
title = {{Large-scale weakly-supervised pre-training for video action recognition}},
url = {https://github.com/dutran/R2Plus1D},
year = {2019}
}
@techreport{Jordao,
abstract = {Human activity recognition based on wearable sensor data has been an attractive research topic due to its application in areas such as healthcare and smart environments. In this context, many works have presented remarkable results using accelerometer, gyroscope and magnetometer data to represent the activities categories. However, current studies do not consider important issues that lead to skewed results, making it hard to assess the quality of sensor-based human activity recognition and preventing a direct comparison of previous works. These issues include the samples generation processes and the validation protocols used. We emphasize that in other research areas, such as image classification and object detection, these issues are already well-defined, which brings more efforts towards the application. Inspired by this, we conduct an extensive set of experiments that analyze different sample generation processes and validation protocols to indicate the vulnerable points in human activity recognition based on wearable sensor data. For this purpose, we implement and evaluate several top-performance methods, ranging from handcrafted-based approaches to con-volutional neural networks. According to our study, most of the experimental evaluations that are currently employed are not adequate to perform the activity recognition in the context of wearable sensor data, in which the recognition accuracy drops considerably when compared to an appropriate evaluation approach. To the best of our knowledge, this is the first study that tackles essential issues that compromise the understanding of the performance in human activity recognition based on wearable sensor data. Index Terms-Human activity recognition, wearable sensor data, state-of-the-art benchmark.},
annote = {The research of Artur et al focused on idetifying and tackling issues with validiation protocols and perfomance in Human Activity Recognition based on werable sensor data. previous work in this field has demonstrated that recognition accuracy drops significantly when comparing evaluations currently employed evaluation in recognition tasks. Image Classification and object detction have been able to suceesfuly map out benchmark evaluation protocls and validation which has led to the continous expansion of its application . Adequate evalution techniques is critical for activity recognition before developing its area of application .

One of the major challenge in activity recognition is that many researchers propose their own datasets and evaluation criteria, it becomes difficult to ascertain the most adequate dataset and reults. Performance metrics also varies aross multiple research work . Developing a standardized evalutaion tecjhniques will lead to robust research in application of Action Recognition tasks.

Current methods allow a data samples to appear in both training and testing data which directly cause bias in real performance when evaluating on a new research work, the accuracy drops by at least 10 %

Extensive research has been done on Activity recognition in hand held and mobile devices, The work of Stise et al [21] investigted influence of heterogeneous devices on the final performance of the classifiers to to perform activity recognition. activities were classified using handcrafted features and employed popular classifiers such as nearest-neighbor, support vector machines and random forest. They noticed sampling instabilities occured across various devices},
archivePrefix = {arXiv},
arxivId = {1806.05226v3},
author = {Jordao, Artur and Nazare, Antonio Carlos and Sena, Jessica and {Robson Schwartz}, William},
eprint = {1806.05226v3},
file = {:C\:/Users/Adewo/Documents/Mendley Documents//Human Activity Recognition Based on Wearable Sensor Data A Standardization of the State-of-the-Art - Jordao et al. - Unknown.pdf:pdf},
title = {{Human Activity Recognition Based on Wearable Sensor Data: A Standardization of the State-of-the-Art}},
url = {http://www.sense.dcc.ufmg.br/activity-recognition-based-wearable-}
}
@inproceedings{Piergiovanni2018,
abstract = {In this paper, we introduce a challenging new dataset, MLB-YouTube, designed for fine-grained activity detection. The dataset contains two settings: segmented video classification as well as activity detection in continuous videos. We experimentally compare various recognition approaches capturing temporal structure in activity videos, by classifying segmented videos and extending those approaches to continuous videos. We also compare models on the extremely difficult task of predicting pitch speed and pitch type from broadcast baseball videos. We find that learning temporal structure is valuable for fine-grained activity recognition .},
annote = {The researchers created a uniques dataset MLB yotube for video classification and activity detection in a continuous video stream. they worked on tackling manually tracked gme statistics.
More recent research work utlized Two stream CNN for AR task with RGB frames and optical flow as input which required large data sets such Kinetics, Thumos and ActivityNet.
The research of NG etal [13]discovered using LSTM cells and max pooling on all the frames in a video stream had better performance Video AR than other methods which is in cotrast with the work of Piergiovanni et al with subsampling of important event in a video stream and classifying those frames yieled a better performance** Read the Papper**
[13]J. Y.-H. Ng, M. Hausknecht, S. Vijayanarasimhan, O. Vinyals, R. Monga, and G. Toderici. Beyond short snip- pets: Deep networks for video classification. In Proceed- ings ofthe IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 4694–4702. IEEE, 2015. 1, 3
[14] A. Piergiovanni, C. Fan, and M. S. Ryoo. Learning latent sub-events in activity videos using temporal attention filters.



This research annotated video based on 8 different action classes. 
Our dataset is quite challenging as it is created from publicly available video and other data sources that contains our action class(Kinetics, UCF 101 ) . Different action classes share a very similar motion and appearances based on the camera angle. Further,making the classification of this task a very difficult task.
The researcers experimented with segemented video recognition approach which ivolves providing frame by frame representation into the CNN algorith without consideration of the begin/end time of the entire video. This make AR task a littl bit easier. The videos was divided into intervals of various lengths and maxpooled features together to classify the video clip.
Activity detection in continuous videos is a more challenging problem. there objective is to classify each frame with the occurring activities and requires the model to learn to detect the start and end of activities.
their experimental results indicates that all the model performed better on per frames classification except LSTM and temporal convolution has the problem of overfitting based on large numbers of parameters.


***Model this paper for my paper.
Experiment with different variables including Inception
CNN
13d + LSTM
TWO stream etc},
archivePrefix = {arXiv},
arxivId = {1804.03247v1},
author = {Piergiovanni, A J and Ryoo, Michael S},
eprint = {1804.03247v1},
file = {:C\:/Users/Adewo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fine-grained Activity Recognition in Baseball Videos - Piergiovanni, Ryoo - Unknown.pdf:pdf;:C\:/Users/Adewo/Documents/Mendley Documents/1804.03247.pdf:pdf},
title = {{Fine-grained Activity Recognition in Baseball Videos}},
url = {https://github.com/piergiaj/mlb-youtube/},
year = {2018}
}
@inproceedings{lea2016segmental,
  title={Segmental spatiotemporal cnns for fine-grained action segmentation},
  author={Lea, Colin and Reiter, Austin and Vidal, Ren{\'e} and Hager, Gregory D},
  booktitle={European Conference on Computer Vision},
  pages={36--52},
  year={2016},
  organization={Springer}
}
@article{chattopadhyay2017grad,
  title={Grad-CAM: Improved visual explanations for deep convolutional networks},
  author={Chattopadhyay, Adiya and Sarkar, Anirban and Howlader, Prantik and Balasubramanian, Vineeth N},
  journal={arXiv: 1710.11063},
  year={2017}
}
}
@article{Cai2015,
abstract = {This study proposes an efficient anomalous behaviour detection framework using trajectory analysis. Such framework includes the trajectory pattern learning module and the online abnormal detection module. In the pattern learning module, a coarse-to-fine clustering strategy is utilised. Vehicle trajectories are coarsely grouped into coherent clusters according to the main flow direction (MFD) vectors followed by a three-stage filtering algorithm. Then a robust K-means clustering algorithm is used in each coarse cluster to get fine classification by which the outliers are distinguished. Finally, the hidden Markov model (HMM) is used to establish the path pattern within each cluster. In the online detection module, the new vehicle trajectory is compared against all the MFD distributions and the HMMs so that the coherence with common motion patterns can be evaluated. Besides that, a real-time abnormal detection method is proposed. The abnormal behaviour can be detected when happening. Experimental results illustrate that the detection rate of the proposed algorithm is close to the state-of-the-art abnormal event detection systems. In addition, the proposed system provides the lowest false detection rate among selected methods. It is suitable for intelligent surveillance applications.},
annote = {Autonmoous accident detection in intelligent transportation is impotrtant in tracking vehicles and identifying the anomalies in traffic pattern. The research of Yingfeng cai discussed detection of abnormal traffic flow using clusterig techniques on Main Flow direction vectors and a k mean dlustering algorithm to identify outliers that deviates from normal trajectortyy pattern or motion flow.
Other research work explored intelligent visual description of scences with connected image points using spation temporal dynamics in Hidden Markov Model [Morris, B.T., Trivedi, M.M.: ‘Trajectory learning for activity understanding: unsupervised, multilevel, and long-term adaptive approach', IEEE Trans. Pattern Anal. Mach. Intell., 2011, 33, (11), pp. 2287–230]},
author = {Cai, Yingfeng and Wang, Hai and Chen, Xiaobo and Jiang, Haobin},
doi = {10.1049/IET-ITS.2014.0238},
file = {:C\:/Users/Adewo/Documents/Mendley Documents/Trajectory-based anomalous behaviour detection for intelligent traffic surveillance - Cai et al. - 2015.pdf:pdf},
issn = {1751956X},
journal = {IET Intelligent Transport Systems},
month = {oct},
number = {8},
pages = {810--816},
publisher = {Institution of Engineering and Technology},
title = {{Trajectory-based anomalous behaviour detection for intelligent traffic surveillance}},
volume = {9},
year = {2015}
}
@article{Sun2019,
abstract = {Automatic vehicle classification is a fundamental task in intelligent transportation systems. Image-based vehicle classification is challenging due to occlusion, low-illumination, and scale change. This study proposes an innovative approach by combining texture and shape features into a complementary feature and using the complementary feature to train a compressive dictionary to improve accuracy and efficiency. In the feature combination, the scale-invariant feature transform descriptor is applied to extract the texture and shape features from the original vehicle images and their edge images, respectively. In the dictionary training, a compressive dictionary learning (DL) algorithm, called compressive K singular value decomposition (CKSVD) algorithm, is proposed to improve the dictionary training efficiency. The CKSVD algorithm divides the feature dictionary into several same-sized data blocks and then performs the DL in each data block based on a very sparse random projection matrix. On the feature combination and DL, the proposed approach employs the kernel sparse representation method to classify vehicles into four types: buses, trucks, vans, and sedans. The kernel sparse representation method enables the linearly inseparable classification in the combined feature space to be linearly separable. Experimental results show that the proposed approach can improve the accuracy and efficiency of vehicle classification.},
annote = {Vehicle detection is an important task in autonomous smart transportation as it plays significant role in the monitoring of traffic surveillance system, classification of vehicles and idetifying features for detecting features based on cluttered background, poor ilumination or weather condition in video stream},
author = {Sun, Wei and Zhang, Xiaorui and Shi, Shunshun and He, Xiaozheng},
doi = {10.1049/iet-its.2018.5316},
file = {:C\:/Users/Adewo/Documents/Mendley Documents/Vehicle classification approach based on the combined texture and shape features with a compressive DL - Sun et al. - 2019.pdf:pdf},
issn = {1751956X},
journal = {IET Intelligent Transport Systems},
month = {jul},
number = {7},
pages = {1069--1077},
publisher = {Institution of Engineering and Technology},
title = {{Vehicle classification approach based on the combined texture and shape features with a compressive DL}},
volume = {13},
year = {2019}
}
@article{Bhatti2019,
abstract = {Internet of Things-enabled Intelligent Transportation Systems (ITS) are gaining significant attention in academic literature and industry, and are seen as a solution to enhancing road safety in smart cities. Due to the ever increasing number of vehicles, a significant rise in the number of road accidents has been observed. Vehicles embedded with a plethora of sensors enable us to not only monitor the current situation of the vehicle and its surroundings but also facilitates the detection of incidents. Significant research, for example, has been conducted on accident rescue, particularly on the use of Information and Communication Technologies (ICT) for efficient and prompt rescue operations. The majority of such works provide sophisticated solutions that focus on reducing response times. However, such solutions can be expensive and are not available in all types of vehicles. Given this, we present a novel Internet of Things-based accident detection and reporting system for a smart city environment. The proposed approach aims to take advantage of advanced specifications of smartphones to design and develop a low-cost solution for enhanced transportation systems that is deployable in legacy vehicles. In this context, a customized Android application is developed to gather information regarding speed, gravitational force, pressure, sound, and location. The speed is a factor that is used to help improve the identification of accidents. It arises because of clear differences in environmental conditions (e.g., noise, deceleration rate) that arise in low speed collisions, versus higher speed collisions). The information acquired is further processed to detect road incidents. Furthermore, a navigation system is also developed to report the incident to the nearest hospital. The proposed approach is validated through simulations and comparison with a real data set of road accidents acquired from Road Safety Open Repository, and shows promising results in terms of accuracy.},
annote = {Incresease in the use of autonomous vehicles and inteeligent transportation system have been deployed to increase road safety and accident reduction in smart cities. Most teschniques adopted in prompt monitoring of road incidents are expensive and sophisticated.
The reports of WHO showcase that *** people die every year in road accident and ** people get ijured. Roac accident is identified as the ** leading cause of death in the US/World (Get Facts). Modern day technology such as surveillance camera, Global Positioning System (GPS), Edge AI and IOT can be beneficial in developing and deploying deep algorithms for dettecting and predicting accidents. This systems can also be used in alerting LEA and first reponders about road accidents to provide spontanous response to victims. 

Fizzah et al proposed Internet of Tings-Enabled Accident Detection for Smart City using low cost solution for enhanced intelligent transportation system.

Some architecture for Accident detectction are built on input from RFID and sensors that transmit information to the cloud for processing and has limited capacity and computational power. Automating the processing and detection of accidents using Edge AI will help eliminating security concerns and time requiments for processing data.


** Talk about smart phone based detection system discussed in this paper and Hardware Based System before talking about Computer vision.},
author = {Bhatti, Fizzah and Shah, Munam Ali and Maple, Carsten and {Ul Islam}, Saif},
doi = {10.3390/s19092071},
file = {:C\:/Users/Adewo/Documents/Mendley Documents/A novel internet of things-enabled accident detection and reporting system for smart city environments - Bhatti et al. - 2019.pdf:pdf},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {Accident detection,Intelligent transportation systems,Internet of Things,Smart cities},
month = {may},
number = {9},
pmid = {31058879},
publisher = {MDPI AG},
title = {{A novel internet of things-enabled accident detection and reporting system for smart city environments}},
volume = {19},
year = {2019}
}

@article{shorten2019survey,
  title={A survey on image data augmentation for deep learning},
  author={Shorten, Connor and Khoshgoftaar, Taghi M},
  journal={Journal of Big Data},
  volume={6},
  number={1},
  pages={1--48},
  year={2019},
  publisher={Springer}
}
@article{dorney2020preventing,
  title={Preventing injuries must be a priority to prevent disease in the twenty-first century},
  author={Dorney, Kate and Dodington, James M and Rees, Chris A and Farrell, Caitlin A and Hanson, Holly R and Lyons, Todd W and Lee, Lois K},
  journal={Pediatric research},
  volume={87},
  number={2},
  pages={282--292},
  year={2020},
  publisher={Nature Publishing Group}
}
@article{meyer2020covid,
  title={COVID lockdowns, social distancing, and fatal Car crashes: more deaths on Hobbesian highways?},
  author={Meyer, Marshall W},
  journal={Cambridge Journal of Evidence-Based Policing},
  volume={4},
  number={3},
  pages={238--259},
  year={2020},
  publisher={Springer}
}
@article{Li2020,
abstract = {This paper describes the AVA-Kinetics localized human actions video dataset. The dataset is collected by annotating videos from the Kinetics-700 dataset using the AVA annotation protocol, and extending the original AVA dataset with these new AVA annotated Kinetics clips. The dataset contains over 230k clips annotated with the 80 AVA action classes for each of the humans in key-frames. We describe the annotation process and provide statistics about the new dataset. We also include a baseline evaluation using the Video Action Transformer Network on the AVA-Kinetics dataset, demonstrating improved performance for action classification on the AVA test set. The dataset can be downloaded from https://research.google.com/ava/},
annote = {This paper desscribed the process of annotating the AVA and Kinetict dataset from video dataset.
This paper built on the AVA and kinetics &)) dataset by providingbhuman annotation and localization for the dataset.
The AVA dataset denseley annotates 80 visual action in 430 15 minute clips. Each person in the vidual has its own bounding box with appropriate labels for co occuring actions about 1.6m labels.

In the kinetics 700 dataset, there are about 600 clips for each action class and a total of 650k clips for all class of about 700 human actions.

In detecting the bounding boxes an algorithm Faster RCNN was used proposed by(Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks -Shaoqing Ren)
Human annotators were utilized in annotating missing boundry box and also providel labels manually for the bounding boxes detected by the F RCNN.},
archivePrefix = {arXiv},
arxivId = {2005.00214},
author = {Li, Ang and Thotakuri, Meghana and Ross, David A. and Carreira, Jo{\~{a}}o and Vostrikov, Alexander and Zisserman, Andrew},
eprint = {2005.00214},
file = {:C\:/Users/Adewo/Documents/Mendley Documents/The AVA-Kinetics Localized Human Actions Video Dataset - Li et al. - 2020(2).pdf:pdf},
month = {may},
title = {{The AVA-Kinetics Localized Human Actions Video Dataset}},
url = {http://arxiv.org/abs/2005.00214},
year = {2020}
}
@misc{Kopuklu,
author = {K{\"{o}}p{\"{u}}kl{\"{u}}, Okan},
title = {{vidaug: Docs, Tutorials, Reviews | Openbase}},
url = {https://openbase.com/python/vidaug},
urldate = {2021-10-14}
}

@mastersthesis{domenech2020sing,
  title={Sing Language Recognition-ASL Recognition with MediaPipe and Recurrent Neural Networks},
  author={Dom{\`e}nech L{\'o}pez, Antonio},
  type={{B.S.} thesis},
  year={2020},
  school={Universitat Polit{\`e}cnica de Catalunya}
}
@InProceedings{Newell,
author="Newell, Alejandro
and Yang, Kaiyu
and Deng, Jia",
editor="Leibe, Bastian
and Matas, Jiri
and Sebe, Nicu
and Welling, Max",
title="Stacked Hourglass Networks for Human Pose Estimation",
booktitle="Computer Vision -- ECCV 2016",
year="2016",
publisher="Springer International Publishing",
address="Cham",
pages="483--499",
abstract="This work introduces a novel convolutional network architecture for the task of human pose estimation. Features are processed across all scales and consolidated to best capture the various spatial relationships associated with the body. We show how repeated bottom-up, top-down processing used in conjunction with intermediate supervision is critical to improving the performance of the network. We refer to the architecture as a ``stacked hourglass'' network based on the successive steps of pooling and upsampling that are done to produce a final set of predictions. State-of-the-art results are achieved on the FLIC and MPII benchmarks outcompeting all recent methods.",
isbn="978-3-319-46484-8"
}

@article{Li2020a,
abstract = {With the success of deep learning in classifying short trimmed videos, more attention has been focused on temporally segmenting and classifying activities in long untrimmed videos. State-of-the-art approaches for action segmentation utilize several layers of temporal convolution and temporal pooling. Despite the capabilities of these approaches in capturing temporal dependencies, their predictions suffer from over-segmentation errors. In this paper, we propose a multi-stage architecture for the temporal action segmentation task that overcomes the limitations of the previous approaches. The first stage generates an initial prediction that is refined by the next ones. In each stage we stack several layers of dilated temporal convolutions covering a large receptive field with few parameters. While this architecture already performs well, lower layers still suffer from a small receptive field. To address this limitation, we propose a dual dilated layer that combines both large and small receptive fields. We further decouple the design of the first stage from the refining stages to address the different requirements of these stages. Extensive evaluation shows the effectiveness of the proposed model in capturing long-range dependencies and recognizing action segments. Our models achieve state-of-the-art results on three datasets: 50Salads, Georgia Tech Egocentric Activities (GTEA), and the Breakfast dataset.},
annote = {Action localization and action recognition in videos are gray aread and dosent necessarily mean the same thing.
The paper used the UCF sports dataset as a benchmark for their task.

Most action regonition reserach work utilized three step in vodeo classification. 
1. Features extraction,
2. dictionary learning and 
3. classification


The UCF sport dataset contains 10 actions, 150 clips (10fps) 

Action localization can be more challenging becausethe action class has to be correctly identified and also determine his spatio temporal location. only 15% of reseearch papers on UCF discussed action localization results. 

Experimental setup for action locatization used Leave one out (LOO). which trains dataset on 9 actions and leaves only one out.
This approach was critized with experimental backup that video may have similiar background and consequetially yeild higher acuracy due to high correlation between videos. 

For feature extraction: Features such at data pont carry distinguishing information about actions being performed this can be acheived in two ways:
1> Low level: detects interest points related perfomed action. using sparse keppoints such as corners, edges blobs etc
2> High level: captires specific further infomation such as pose, shape and contextual information.


Space-Time Interest Points (STIP) were developed to capture feautres in video . 3D STIP

The action localizatiom task can be achieved by using sliding window approach . This methods slides in the spatio temporal volume of the video and finds the subvolume with highest scores and assigns a bounding box},
archivePrefix = {arXiv},
arxivId = {2006.09220},
author = {Li, Shijie and Farha, Yazan Abu and Liu, Yun and Cheng, Ming-Ming and Gall, Juergen},
eprint = {2006.09220},
file = {:C\:/Users/Adewo/Documents/Mendley Documents/MS-TCN Multi-Stage Temporal Convolutional Network for Action Segmentation - Li et al. - 2020.pdf:pdf},
keywords = {Index Terms-Temporal action segmentation,temporal convolutional network !},
month = {jun},
title = {{MS-TCN++: Multi-Stage Temporal Convolutional Network for Action Segmentation}},
url = {http://arxiv.org/abs/2006.09220},
year = {2020}
}
@article{Lee2020,
abstract = {Weakly-supervised temporal action localization aims to detect intervals of action instances with only video-level action labels for training. A crucial challenge is to separate frames of action classes from remaining, denoted as background frames (i.e., frames not belonging to any action class). Previous methods attempt background modeling by either synthesizing pseudo background videos with static frames or introducing an auxiliary class for background. However, they overlook an essential fact that background frames could be dynamic and inconsistent. Accordingly, we cast the problem of identifying background frames as out-of-distribution detection and isolate it from conventional action classification. Beyond our base action localization network, we propose a module to estimate the probability of being background (i.e., uncertainty [20]), which allows us to learn uncertainty given only video-level labels via multiple instance learning. A background entropy loss is further designed to reject background frames by forcing them to have uniform probability distribution for action classes. Extensive experiments verify the effectiveness of our background modeling and show that our method significantly outperforms state-of-the-art methods on the standard benchmarks - THUMOS'14 and ActivityNet (1.2 and 1.3). Our code and the trained model are available at https://github.com/Pilhyeon/Background-Modeling-via-Uncertainty-Estimation.},
annote = {The main task adressed in this paper is localizing action and seperating background frames that does not contain action class. The model was tested on Thumos dataset and I3D.

The data used are untrimmed videos used to reject bakgroundframes as out of detection.

Method
weakly supervised learning was used to train a model for localizing acton and detecting action. 
Step one feature extraction: Voideo were split into multiframe non overlapping segements to mitigate videos with varied length.
Feature Embedding: Extracted features was fed into 1D convolution layer with Relu Activation
Segement Classification: Sgement level class score was predicted for the embedded features for action localization

Action score segregation: Video level action probability was aggregated with softmax function to score all segenments for each actions to build a video level class score

The model was optimized with three loses 
1.Action Clssification Losses
2. Uncertainty modelling loss
3. Background entropy loss

Full code availableon github repo},
archivePrefix = {arXiv},
arxivId = {2006.07006},
author = {Lee, Pilhyeon and Wang, Jinglu and Lu, Yan and Byun, Hyeran},
eprint = {2006.07006},
file = {::},
isbn = {2006.07006v1},
title = {{Background Modeling via Uncertainty Estimation for Weakly-supervised Action Localization}},
url = {https://github.com/Pilhyeon/Background-Modeling-via-Uncertainty-Estimation. http://arxiv.org/abs/2006.07006},
year = {2020}
}
@article{Ma2019,
abstract = {Recent two-stream deep Convolutional Neural Networks (ConvNets) have made significant progress in recognizing human actions in videos. Despite their success, methods extending the basic two-stream ConvNet have not systematically explored possible network architectures to further exploit spatiotemporal dynamics within video sequences. Further, such networks often use different baseline two-stream networks. Therefore, the differences and the distinguishing factors between various methods using Recurrent Neural Networks (RNN) or Convolutional Neural Networks on temporally-constructed feature vectors (Temporal-ConvNets) are unclear. In this work, we would like to answer the question: given the spatial and motion feature representations over time, what is the best way to exploit the temporal information? Toward this end, we first demonstrate a strong baseline two-stream ConvNet using ResNet-101. We use this baseline to thoroughly examine the use of both RNNs and Temporal-ConvNets for extracting spatiotemporal information. Building upon our experimental results, we then propose and investigate two different networks to further integrate spatiotemporal information: (1) Temporal Segment RNN and (2) Inception-style Temporal-ConvNet. We demonstrate that using both RNNs (with LSTMs) and Temporal-ConvNets on spatiotemporal feature matrices are able to exploit spatiotemporal dynamics to improve the overall performance. Our analysis identifies specific limitations for each method that could form the basis of future work. Our experimental results on UCF101 and HMDB51 datasets achieve comparable state-of-the-art performances, 94.1% and 69.0%, respectively, without requiring extensive temporal augmentation or end-to-end training.},
annote = {This paper utilize ConvNet and two-stream ConvNet to provide indepth analysis decision for both RNN and Tempral ConvNet.
The use of spatial RGB images have been reported to hve a good performance with a potential possibility of mor improved performance by stacking thr RGB difference in i a video stream.
using a pretrained network and fine tunning of parameters are helpful in temporal stream network.

Deeper LSTM cells for action recognitiion tasks have a very limited impprovement on classification performance in AR

The researchers proposed temporal segment RNN and 2) Inception-style Temporal-ConvNet improves overall performance of action recognition task by extracting and inetgration spation temporal information in video stream.

Donahue et al. [4] with ConvNets and RNNs utilized spatial features extracted from each time step into Recurrent Neural network with LSTM cells},
archivePrefix = {arXiv},
arxivId = {1703.10667},
author = {Ma, Chih Yao and Chen, Min Hung and Kira, Zsolt and AlRegib, Ghassan},
doi = {10.1016/j.image.2018.09.003},
eprint = {1703.10667},
file = {:C\:/Users/Adewo/Documents/Mendley Documents/TS-LSTM and Temporal-Inception Exploiting Spatiotemporal Dynamics for Activity Recognition - Ma et al. - Unknown.pdf:pdf},
issn = {09235965},
journal = {Signal Processing: Image Communication},
keywords = {Action recognition,Convolutional neural network,Recurrent neural network,Video understanding},
pages = {76--87},
title = {{TS-LSTM and temporal-inception: Exploiting spatiotemporal dynamics for activity recognition}},
volume = {71},
year = {2019}
}
@article{Montes},
annote = {% for training},
archivePrefix = {arXiv},
arxivId = {1608.08128},
author = {Montes, Alberto and Salvador, Amaia and Pascual, Santiago and Giro-i-Nieto, Xavier},
eprint = {1608.08128},
file = {:C\:/Users/Adewo/Documents/Mendley Documents/Temporal Activity Detection in Untrimmed Videos with Recurrent Neural Networks - Montes et al. - Unknown.pdf:pdf},
title = {{Temporal Activity Detection in Untrimmed Videos with Recurrent Neural Networks}},
url = {https://github.com/imatge-upc/ http://arxiv.org/abs/1608.08128},
year = {2016}
}

@inproceedings{chan2016anticipating,
  title={Anticipating accidents in dashcam videos},
  author={Chan, Fu-Hsiang and Chen, Yu-Ting and Xiang, Yu and Sun, Min},
  booktitle={Asian Conference on Computer Vision},
  pages={136--153},
  year={2016},
  organization={Springer}
}
@article{Robles-Serrano2021,
abstract = {According to worldwide statistics, traffic accidents are the cause of a high percentage of violent deaths. The time taken to send the medical response to the accident site is largely affected by the human factor and correlates with survival probability. Due to this and the wide use of video surveillance and intelligent traffic systems, an automated traffic accident detection approach becomes desirable for computer vision researchers. Nowadays, Deep Learning (DL)-based approaches have shown high performance in computer vision tasks that involve a complex features relationship. Therefore, this work develops an automated DL-based method capable of detecting traffic accidents on video. The proposed method assumes that traffic accident events are described by visual features occurring through a temporal way. Therefore, a visual features extraction phase, followed by a temporary pattern identification, compose the model architecture. The visual and temporal features are learned in the training phase through convolution and recurrent layers using built-from-scratch and public datasets. An accuracy of 98% is achieved in the detection of accidents in public traffic accident datasets, showing a high capacity in detection independent of the road structure.},
annote = {.},
author = {Robles-Serrano, Sergio and Sanchez-Torres, German and Branch-Bedoya, John},
doi = {10.3390/COMPUTERS10110148},
file = {:C\:/Users/Adewo/Documents/Mendley Documents/Automatic detection of traffic accidents from video using deep learning techniques - Robles-Serrano, Sanchez-Torres, Branch-Bedoya - 202.pdf:pdf},
issn = {2073431X},
journal = {Computers},
keywords = {Accident detection,Convolutional neural networks,Deep learning,Recurrent neural networks,Urban traffic accident},
month = {nov},
number = {11},
publisher = {MDPI},
title = {{Automatic detection of traffic accidents from video using deep learning techniques}},
volume = {10},
year = {2021}
}

@article{sherstinsky2020fundamentals,
  title={Fundamentals of recurrent neural network (RNN) and long short-term memory (LSTM) network},
  author={Sherstinsky, Alex},
  journal={Physica D: Nonlinear Phenomena},
  volume={404},
  pages={132306},
  year={2020},
  publisher={Elsevier}
}
@article{Saunier2007,
abstract = {Traffic safety analysis has often been undertaken with historical collision data. However, well-recognized availability and quality problems are associated with collision data. In addition, the use of collision records for safety analysis is reactive: a significant number of collisions has to be recorded before action is taken. Therefore, the observation of traffic conflicts has been advocated as a complementary approach in the analysis of traffic safety. However, incomplete conceptualization and the cost of training observers and collecting conflict data have been factors inhibiting extensive application of the traffic conflict technique. The goal of this research is to develop a method for automated analysis of road safety with video sensors to address the problem of dependency on the deteriorating collision data. The method automates the extraction of traffic conflicts from video sensor data. This method should address the main shortcomings of the traffic conflict technique. A comprehensive system is described for traffic conflict detection in video data. The system is composed of a feature-based vehicle tracking algorithm adapted for intersections and a traffic conflict detection method based on the clustering of vehicle trajectories. The clustering uses a K-means approach with hidden Markov models and a simple heuristic to find the number of clusters automatically. Traffic conflicts can then be detected by identifying and adapting pairs of models of conflicting trajectories. The technique is demonstrated on real-world video sequences of traffic conflicts.},
annote = {The analysis of road safety is often dependent on the availability of collision data in a partixcular location. This paper developed an automated technique in analysis of video data in road safety using sensors.
A feauture based vehicle tracking algorithm using clustering teching for detecting trajectories of car. K-means ethod was used with Hidden Markov Model as a method for detecting taffic conflicts such as opposite trajectories/traffic movement.,

The availability of sophisticated algorithms and traffic control system are incapable of effectively monitoring busy highways and},
author = {Saunier, Nicolas and Sayed, Tarek},
doi = {10.3141/2019-08},
file = {:C\:/Users/Adewo/Documents/Mendley Documents/Automated Analysis of Road Safety with Video Data - Saunier, Sayed - 2007.pdf:pdf},
isbn = {9780309104463},
issn = {03611981},
journal = {Transportation Research Record},
month = {jan},
number = {2019},
pages = {57--64},
publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
title = {{Automated analysis of road safety with video data}},
url = {https://journals-sagepub-com.uc.idm.oclc.org/doi/10.3141/2019-08},
year = {2007}
}

@article{lim2018,
  title={Transfer learning for action unit recognition},
  author={Lim, Yen Khye and Liao, Zukang and Petridis, Stavros and Pantic, Maja},
  journal={arXiv preprint arXiv:1807.07556},
  year={2018}
}

@inproceedings{lim2016speech,
  title={Speech emotion recognition using convolutional and recurrent neural networks},
  author={Lim, Wootaek and Jang, Daeyoung and Lee, Taejin},
  booktitle={2016 Asia-Pacific signal and information processing association annual summit and conference (APSIPA)},
  pages={1--4},
  year={2016},
  organization={IEEE}
}
@article{morris2011trajectory,
  title={Trajectory learning for activity understanding: Unsupervised, multilevel, and long-term adaptive approach},
  author={Morris, Brendan Tran and Trivedi, Mohan Manubhai},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={33},
  number={11},
  pages={2287--2301},
  year={2011},
  publisher={IEEE}
}

@article{colyar2007ngsim,
  title={Ngsim-us highway 101 dataset},
  author={Colyar, J and Halkias, J},
  journal={Federal Highway Administration (FHWA), Tech. Rep. FHWA-HRT-07-030, DC, USA},
  year={2007}
}
@article{wang2019apolloscape,
  title={The apolloscape open dataset for autonomous driving and its application},
  author={Wang, Peng and Huang, Xinyu and Cheng, Xinjing and Zhou, Dingfu and Geng, Qichuan and Yang, Ruigang},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  year={2019},
  publisher={IEEE}
}
@inproceedings{Hui2015,
abstract = {Recently, traffic accident detection is becoming one of the interesting fields due to its tremendous application potential in Intelligent Transportation Systems. In this paper, we present a vision-based real time traffic accident detection method. We intend to extract foreground and background from video shots using the Gaussian Mixture Model (GMM) to detect vehicles; afterwards, the detected vehicles are tracked based on the mean shift algorithm. Then the three traffic accident parameters including the changes of the vehicles position, acceleration, and the direction of the moving vehicles are gathered to make the final accident decision. The experimental results on real video demonstrate the efficiency and the applicability of the proposed approach.},
author = {Hui, Zu and Xie, Yaohua and Lu, Ma and Fu, Jiansheng},
booktitle = {Proceedings of the World Congress on Intelligent Control and Automation (WCICA)},
doi = {10.1109/WCICA.2014.7052859},
file = {:C\:/Users/Adewo/Documents/Mendley Documents/Vision-based real-time traffic accident detection - Hui et al. - 2015.pdf:pdf},
isbn = {9781479958252},
keywords = {Accident detection,Gaussian mixture model,Intelligent transportation systems,Mean shift},
mendeley-groups = {SLR},
month = {mar},
number = {March},
pages = {1035--1038},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Vision-based real-time traffic accident detection}},
volume = {2015-March},
year = {2015}
}
@inproceedings{Janardhan2022,
abstract = {Abnormal driving includes the distractions like talking/ using mobile while driving, operating radio, eating, doing certain action like reaching back while the vehicle in motion. All these activities are related to endangerment of life and accident are prone. With the help of AWGRD network model using deep learning fusions, the state of driver at a particular frame of time can be determined and warned with a level of accuracy. There are, many models are present which are used to predict the state of driver like WGD, WGRD, AWGRD out of which AWGRD is better because it takes super positions of other layers, so the accuracy level increases. Our model predicts the behavior of driver by taking the help of ten classes which are ordered according to their state of activity.},
author = {Janardhan, G. and Gattu, Nitya Reddy and Kaki, Prathyusha and Kallem, Maneesha and Manne, Srinidhi},
doi = {10.1109/iceca52323.2021.9675904},
file = {:C\:/Users/Adewo/Documents/Mendley Documents/Video-Based Abnormal Driving Detection - Janardhan et al. - 2022.pdf:pdf},
isbn = {9781665435246},
mendeley-groups = {SLR},
month = {jan},
pages = {1553--1557},
publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
title = {{Video-Based Abnormal Driving Detection}},
year = {2022}
}
@article{Niskanen2021,
abstract = {A 3D model is an interesting technology used to realize the smooth flow of traffic and plays an important role in autonomous driving. This paper proposes an efficient and new approach monitoring urban transportation and traffic movements using a recently developed 2D, pulsed time-of-flight (TOF) solid-state laser profilometer. To validate the approach, five different vehicle speeds were used in an experiment. An analysis of the results showed that the profilometer has great potential for creating a 3D model of a moving vehicle at all urban speeds, even though the resolution of the digital 3D imaging decreased as the speed increased, and precisely achieved the determination of the dimensions of the vehicle. The procedure is not restricted to vehicles; it also has potential for low-speed vehicles found on building and infrastructure constructions and worksites, such as excavators and bulldozers. In the future, the 3D images of a moving vehicle will be a key component of building information modeling (BIM) for monitoring the flow of road traffic.},
author = {Niskanen, Ilpo and Immonen, Matti and Hallman, Lauri and Yamamuchi, Genki and Mikkonen, Martti and Hashimoto, Takeshi and Nitta, Yasushi and Ker{\"{a}}nen, Pekka and Kostamovaara, Juha and Heikkil{\"{a}}, Rauno},
doi = {10.1016/j.autcon.2020.103429},
file = {:C\:/Users/Adewo/Documents/Mendley Documents/Time-of-flight sensor for getting shape model of automobiles toward digital 3D imaging approach of autonomous driving - Niskanen et al.pdf:pdf},
issn = {09265805},
journal = {Automation in Construction},
keywords = {3D model,Moving vehicle,Time of flight,Urban speeds,profilometer},
mendeley-groups = {SLR},
pages = {103429},
title = {{Time-of-flight sensor for getting shape model of automobiles toward digital 3D imaging approach of autonomous driving}},
url = {https://doi.org/10.1016/j.autcon.2020.103429},
volume = {121},
year = {2021}
}
@inproceedings{Bai2017,
abstract = {Deformable Part Models (DPM) has high detection accuracy in the image detection algorithm, while it is at the expense of complex calculation and slowly running speed. In order to improve the development of its application, the algorithms of the weighted PCA (MPCA) and fast level locating are also proposed, MPCA can reduce the model parameters by reducing the dimensions of HOG features and the fast level locating can locate the object's level in the feature pyramid fast. This paper also presents the optical accidents elements detection for the scene of the accident which can be used in varying environment. Our results show that the detection of the accidents elements obtained are sufficiently robust to the changing scene of accident and its speed increases significantly.},
author = {Bai, Baolin and Liu, Yusong and Zhang, Chunyan},
booktitle = {Proceedings - 2017 32nd Youth Academic Annual Conference of Chinese Association of Automation, YAC 2017},
doi = {10.1109/YAC.2017.7967574},
file = {:C\:/Users/Adewo/Documents/Mendley Documents/Accident elements detection based on improved DPM - Bai, Liu, Zhang - 2017.pdf:pdf},
isbn = {9781538629017},
keywords = {DPM,accident elements detection,fast level locating,the weighted PCA},
mendeley-groups = {SLR},
month = {jun},
pages = {1094--1097},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Accident elements detection based on improved DPM}},
year = {2017}
}
@inproceedings{Ijjina2019,
abstract = {Computer vision-based accident detection through video surveillance has become a beneficial but daunting task. In this paper, a neoteric framework for detection of road accidents is proposed. The proposed framework capitalizes on Mask R-CNN for accurate object detection followed by an efficient centroid based object tracking algorithm for surveillance footage. The probability of an accident is determined based on speed and trajectory anomalies in a vehicle after an overlap with other vehicles. The proposed framework provides a robust method to achieve a high Detection Rate and a low False Alarm Rate on general road-traffic CCTV surveillance footage. This framework was evaluated on diverse conditions such as broad daylight, low visibility, rain, hail, and snow using the proposed dataset. This framework was found effective and paves the way to the development of general-purpose vehicular accident detection algorithms in real-time.},
archivePrefix = {arXiv},
arxivId = {1911.10037},
author = {Ijjina, Earnest Paul and Chand, Dhananjai and Gupta, Savyasachi and Goutham, K.},
booktitle = {2019 10th International Conference on Computing, Communication and Networking Technologies, ICCCNT 2019},
doi = {10.1109/ICCCNT45670.2019.8944469},
eprint = {1911.10037},
file = {:C\:/Users/Adewo/Documents/Mendley Documents/Computer Vision-based Accident Detection in Traffic Surveillance - Ijjina et al. - 2019.pdf:pdf},
isbn = {9781538659069},
keywords = {Accident Detection,Centroid based Object Tracking,Mask R-CNN,Vehicular Collision},
mendeley-groups = {SLR},
month = {jul},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Computer Vision-based Accident Detection in Traffic Surveillance}},
year = {2019}
}
@inproceedings{Srinivasan2020,
abstract = {Road accidents are man-made cataclysmic phenomena and are not generally predictable. With increasing numbers of deaths due to accidents in the roadways, a smart and fast detection system for road accidents is the need of the hour. Often, precious few seconds after the accidents make the difference between life and death. To address this problem more efficiently, 'A Novel Approach for Road Accident Detection in CCTV Videos using DETR Algorithm' has been developed to aid in notifying hospitals and the local police at places where instant notification is seldom feasible. This paper presents a novel and efficient method for detecting road accidents with DETR (Detection Transformers) and Random Forest Classifier. Objects such as cars, bikes, people, etc. in the CCTV footage are detected using the DETR and the features are fed to a Random Forest Classifier for frame wise classification. Each frame of the video is classified as an accident frame or a non-accident frame. A total count of predicted accident frames from any 60 continuous frames of the video are considered using a sliding window technique before the final decision is made. Simulation results show that the proposed system achieves 78.2% detection rate in CCTV videos.},
author = {Srinivasan, Aparajith and Srikanth, Anirudh and Indrajit, Haresh and Narasimhan, Venkateswaran},
booktitle = {2020 International Conference on Intelligent Data Science Technologies and Applications, IDSTA 2020},
doi = {10.1109/IDSTA50958.2020.9263703},
file = {:C\:/Users/Adewo/Documents/Mendley Documents/A Novel Approach for Road Accident Detection using DETR Algorithm - Srinivasan et al. - 2020.pdf:pdf},
isbn = {9781728183763},
keywords = {CCTV Videos,Detection Transformer,Random Forest Classifier,Road Accident Detection,Sliding Window Technique},
mendeley-groups = {SLR},
month = {oct},
pages = {75--80},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{A Novel Approach for Road Accident Detection using DETR Algorithm}},
year = {2020}
}
@inproceedings{Vatti2018,
abstract = {The number of fatal and disabling road accident are increasing day by day and is a real public health challenge. Many times, in the road accidents, human lives will be lost due to delayed medical assistance. Hence road accident deaths are more prominent. There exist many accident prevention systems which can prevent the accidents to certain extent, but they do not have any facility to communicate to the relatives in case accident happens. In this paper, the authors made an attempt to develop a car accident detection and communication system which will inform the relatives, nearest hospitals and police along with the location of the accident.},
author = {Vatti, Nagarjuna R. and Vatti, Prasannalakshmi and Vatti, Rambabu and Garde, Chandrashekhar},
booktitle = {Proceedings of the 2018 International Conference on Current Trends towards Converging Technologies, ICCTCT 2018},
doi = {10.1109/ICCTCT.2018.8551179},
file = {:C\:/Users/Adewo/Documents/Mendley Documents/Smart Road Accident Detection and communication System - Vatti et al. - 2018.pdf:pdf},
isbn = {9781538637012},
keywords = {Accident,Accident Detection,Communication,GPS,GSM},
mendeley-groups = {SLR},
month = {nov},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Smart Road Accident Detection and communication System}},
year = {2018}
}
@inproceedings{You2020,
abstract = {We propose a brand new benchmark for analyzing causality in traffic accident videos by decomposing an accident into a pair of events, cause and effect. We collect videos containing traffic accident scenes and annotate cause and effect events for each accident with their temporal intervals and semantic labels; such annotations are not available in existing datasets for accident anticipation task. Our dataset has the following two advantages over the existing ones, which would facilitate practical research for causality analysis. First, the decomposition of an accident into cause and effect events provides atomic cues for reasoning on a complex environment and planning future actions. Second, the prediction of cause and effect in an accident makes a system more interpretable to humans, which mitigates the ambiguity of legal liabilities among agents engaged in the accident. Using the proposed dataset, we analyze accidents by localizing the temporal intervals of their causes and effects and classifying the semantic labels of the accidents. The dataset as well as the implementations of baseline models are available in the code repository (https://github.com/tackgeun/CausalityInTrafficAccident).},
author = {You, Tackgeun and Han, Bohyung},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-030-58571-6_32},
file = {:C\:/Users/Adewo/Documents/Mendley Documents/Traffic Accident Benchmark for Causality Recognition - You, Han - 2020.pdf:pdf},
isbn = {9783030585709},
issn = {16113349},
mendeley-groups = {SLR},
pages = {540--556},
publisher = {Springer Science and Business Media Deutschland GmbH},
title = {{Traffic Accident Benchmark for Causality Recognition}},
url = {https://link-springer-com.uc.idm.oclc.org/chapter/10.1007/978-3-030-58571-6_32},
volume = {12352 LNCS},
year = {2020}
}
@inproceedings{Beinarovica2019,
abstract = {The main objective of the transport operations is a safe transportation process. There are various methods for gaining this task. This paper is a part of a big research, aimed at the developed of autonomous electric drive, and discusses one of the aims of the research-collision probability calculation of transport vehicles team moving in the one area. The goal of the current research is to develop a computer model, which would help to detect the possible crossing point of transport vehicles team parties, moving in one area, and calculate the collision probability. This information can be used further for the collision probability minimization by use of the appropriate actions. Previously authors have developed and described algorithms for collision probability minimization tasks. Proposed algorithm was implemented in the computer model. Computer simulations prove the workability and advantages of the developed model.},
author = {Beinarovica, Anna and Gorobetz, Mikhail and Alps, Ivars and Levchenkov, Anatoly},
booktitle = {2019 IEEE 60th Annual International Scientific Conference on Power and Electrical Engineering of Riga Technical University, RTUCON 2019 - Proceedings},
doi = {10.1109/RTUCON48111.2019.8982372},
file = {:C\:/Users/Adewo/Documents/Mendley Documents/Modelling and Simulation of Transport Collision Probability Recognition Algorithm for Traffic Safety - Beinarovica et al. - 2019.pdf:pdf},
isbn = {9781728139425},
keywords = {Transport safety,anti-collision system,autonomous vehicle,collision probability,possible crossing point,vehicles team},
mendeley-groups = {SLR},
month = {oct},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Modelling and Simulation of Transport Collision Probability Recognition Algorithm for Traffic Safety}},
year = {2019}
}
@inproceedings{Yang2021,
abstract = {Vehicle detection in videos is a valuable but challenging technology in traffic monitoring. Due to the advantage of real-time detection, Single Shot MultiBox Detector (SSD) is often used to detect vehicles in images. However, the accuracy degradation caused by SSD is one of the significant problems in video vehicle detection. To address this problem in real time, this paper enhances the detection performance by improving the SSD and employing the relationship of inter-frame detections. We propose a feature-fused SSD detector and a Tracking-guided Detections Optimizing (TDO) strategy for fast and effective video vehicle detection. We introduce a lightweight feature fusion sub-network to the standard SSD network, which aggregate the deeper layer features into the shallower layer features to enhance the semantic information of the shallower layer features. At the post-processing stage of the feature-fused SSD, the non-maximum suppression (NMS) is replaced by the TDO strategy, which link vehicles of inter-frames by fast tracking algorithm. Thus the missed detections can be compensated by the propagated results, and the confidence of the final results can be optimized in the temporal. Our approach significantly improves the temporal consistency of the detection results with lower complexity computations. We evaluate the proposed method on two datasets. The experiments on our labeled highway dataset show that the mean average precision (mAP) of our method is 8.2% higher than that of the base detector. The runtime of our feature-fused SSD is 27.1 frames per second (fps), which is suitable for real-time detection. The experiments on the ImageNet VID dataset prove that the proposed method is comparable with the state-of-the-art detectors as well.},
author = {Yang, Yanni and Song, Huansheng and Sun, Shijie and Zhang, Wentao and Chen, Yan and Rakal, Lionel and Fang, Yong},
booktitle = {Journal of Real-Time Image Processing},
doi = {10.1007/s11554-021-01121-y},
file = {:C\:/Users/Adewo/Documents/Mendley Documents/A fast and effective video vehicle detection method leveraging feature fusion and proposal temporal link - Yang et al. - 2021.pdf:pdf},
issn = {18618219},
keywords = {Detections optimizing,Feature fusion,Object detection,SSD,Video vehicle detection},
mendeley-groups = {SLR},
month = {aug},
number = {4},
pages = {1261--1274},
publisher = {Springer Science and Business Media Deutschland GmbH},
title = {{A fast and effective video vehicle detection method leveraging feature fusion and proposal temporal link}},
url = {https://link-springer-com.uc.idm.oclc.org/article/10.1007/s11554-021-01121-y},
volume = {18},
year = {2021}
}
@inproceedings{elsayed2019reduced,
  title={Reduced-gate convolutional lstm architecture for next-frame video prediction using predictive coding},
  author={Elsayed, Nelly and Maida, Anthony S and Bayoumi, Magdy},
  booktitle={2019 international joint conference on neural networks (ijcnn)},
  pages={1--9},
  year={2019},
  organization={IEEE}
}
@article{Ali2022,
abstract = {The prediction of crowd flows is an important urban computing issue whose purpose is to predict the future number of incoming and outgoing people in regions. Measuring the complicated spatial–temporal dependencies with external factors, such as weather conditions and surrounding point-of-interest (POI) distribution is the most difficult aspect of predicting crowd flows movement. To overcome the above issue, this paper advises a unified dynamic deep spatio-temporal neural network model based on convolutional neural networks and long short-term memory, termed as (DHSTNet) to simultaneously predict crowd flows in every region of a city. The DHSTNet model is made up of four separate components: a recent, daily, weekly, and an external branch component. Our proposed approach simultaneously assigns various weights to different branches and integrates the four properties' outputs to generate final predictions. Moreover, to verify the generalization and scalability of the proposed model, we apply a Graph Convolutional Network (GCN) based on Long Short Term Memory (LSTM) with the previously published model, termed as GCN-DHSTNet; to capture the spatial patterns and short-term temporal features; and to illustrate its exceptional accomplishment in predicting the traffic crowd flows. The GCN-DHSTNet model not only depicts the spatio-temporal dependencies but also reveals the influence of different time granularity, which are recent, daily, weekly periodicity and external properties, respectively. Finally, a fully connected neural network is utilized to fuse the spatio-temporal features and external properties together. Using two different real-world traffic datasets, our evaluation suggests that the proposed GCN-DHSTNet method is approximately 7.9%–27.2% and 11.2%–11.9% better than the AAtt-DHSTNet method in terms of RMSE and MAPE metrics, respectively. Furthermore, AAtt-DHSTNet outperforms other state-of-the-art methods.},
author = {Ali, Ahmad and Zhu, Yanmin and Zakarya, Muhammad},
doi = {10.1016/j.neunet.2021.10.021},
file = {:C\:/Users/Adewo/Documents/Mendley Documents/Exploiting dynamic spatio-temporal graph convolutional neural networks for citywide traffic flows prediction - Ali, Zhu, Zakarya - 2022.pdf:pdf},
issn = {18792782},
journal = {Neural Networks},
keywords = {GCN,LSTM,Road safety,Spatial and temporal dependencies,Traffic flow prediction},
mendeley-groups = {SLR},
pages = {233--247},
pmid = {34773899},
title = {{Exploiting dynamic spatio-temporal graph convolutional neural networks for citywide traffic flows prediction}},
url = {https://doi.org/10.1016/j.neunet.2021.10.021},
volume = {145},
year = {2022}
}
@inproceedings{Kinage2019,
abstract = {With an increasing number of vehicles, the number of accidents is also increasing at an unprecedented rate. Each year, among the total number of deaths 1.24 million deaths occurred due to the vehicle accident. In India, the root causes of these accidents are due to the drunken driver, drowsiness, and badly designed speed breakers. There is no effective mechanism to prevent these root causes. Our proposed system provides an efficient, cost-effective and real-time solution to prevent vehicle accident. When reading goes beyond predefined threshold values, an alert gets generated and if a driver does not take some action in specified time then our proposed system will handle the situation by cutting the fuel supply. Our proposed system uses a microcontroller named Arduino along with MQ-3 sensor, infrared sensor, accelerometer, and webcam. Arduino is used to regulate all these sensors.},
author = {Kinage, Vivek and Patil, Piyush},
booktitle = {Proceedings of the 3rd International Conference on I-SMAC IoT in Social, Mobile, Analytics and Cloud, I-SMAC 2019},
doi = {10.1109/I-SMAC47947.2019.9032662},
file = {:C\:/Users/Adewo/Documents/Mendley Documents/IoT Based Intelligent System for Vehicle Accident Prevention and Detection at Real Time - Kinage, Patil - 2019.pdf:pdf},
isbn = {9781728143651},
keywords = {Accident,Arduino,Drowsiness,IoT,Sensors,Vehicle},
mendeley-groups = {SLR},
month = {dec},
pages = {409--413},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{IoT Based Intelligent System for Vehicle Accident Prevention and Detection at Real Time}},
year = {2019}
}
@article{Huang2020,
abstract = {Crash Detection is essential in providing timely information to traffic management centers and the public to reduce its adverse effects. Prediction of crash risk is vital for avoiding secondary crashes and safeguarding highway traffic. For many years, researchers have explored several techniques for early and precise detection of crashes to aid in traffic incident management. With recent advancements in data collection techniques, abundant real-time traffic data is available for use. Big data infrastructure and machine learning algorithms can utilize this data to provide suitable solutions for the highway traffic safety system. This paper explores the feasibility of using deep learning models to detect crash occurrence and predict crash risk. Volume, Speed and Sensor Occupancy data collected from roadside radar sensors along Interstate 235 in Des Moines, IA is used for this study. This real-world traffic data is used to design feature set for the deep learning models for crash detection and crash risk prediction. The results show that a deep model has better crash detection performance and similar crash prediction performance than state of the art shallow models. Additionally, a sensitivity analysis was conducted for crash risk prediction using data 1-minute, 5-minutes and 10-minutes prior to crash occurrence. It was observed that is hard to predict the crash risk of a traffic condition, 10 min prior to a crash.},
author = {Huang, Tingting and Wang, Shuo and Sharma, Anuj},
doi = {10.1016/j.aap.2019.105392},
file = {:C\:/Users/Adewo/Documents/Mendley Documents/Highway crash detection and risk estimation using deep learning - Huang, Wang, Sharma - 2020.pdf:pdf},
issn = {00014575},
journal = {Accident Analysis and Prevention},
keywords = {Crash detection,Crash prediction,Deep learning},
mendeley-groups = {SLR},
pmid = {31841865},
title = {{Highway crash detection and risk estimation using deep learning}},
url = {https://doi.org/10.1016/j.aap.2019.105392},
volume = {135},
year = {2020}
}
@incollection{Gupta2021,
abstract = {CCTV surveillance cameras are installed in the majority of roads and highways these days; therefore, it generates millions and millions of hours of data, thus captures a variety of real-world incidents. Road accidents are one of the most severe and fatal incidents, which disrupt the smooth flow of traffic as well leading to wastage of time and resources. Detection of accidents not only helps us to save the life of victims, but also helps in reducing traffic congestion. In this, we have proposed a framework for accident detection based on hierarchical recurrent neural network. The framework localizes and identifies the presence of road accidents in the captured video. The framework contains a time-distributed model which learns both the temporal and spatial features of the video, making the framework more efficient. The proposed approach is evaluated on the dataset, built by obtaining recorded road accident videos from YouTube. Results demonstrate the applicability of our approach performs, accident detection, and localization effectively.},
author = {Gupta, Garima and Singh, Ritwik and {Singh Patel}, Ashish and Ojha, Muneendra},
booktitle = {Advances in Intelligent Systems and Computing},
doi = {10.1007/978-981-15-5859-7_21},
file = {:C\:/Users/Adewo/Documents/Mendley Documents/Accident detection using time-distributed model in videos - Gupta et al. - 2021.pdf:pdf},
issn = {21945365},
keywords = {Accident,Accident detection,Deep learning,Long short-term memory networks,Neural networks,Time-distributed,Traffic},
mendeley-groups = {SLR},
pages = {214--223},
publisher = {Springer Science and Business Media Deutschland GmbH},
title = {{Accident detection using time-distributed model in videos}},
url = {https://link-springer-com.uc.idm.oclc.org/chapter/10.1007/978-981-15-5859-7_21},
volume = {1184},
year = {2021}
}
@article{Yu,
abstract = {Traffic accidents usually lead to severe human casualties and huge economic losses in real-world scenarios. Timely accurate prediction of traffic accidents has great potential to protect public safety and reduce economic losses. However, it is challenging to predict traffic accidents due to the complex causality of traffic accidents with multiple factors, including spatial correlations, temporal dynamic interactions and external influences in traffic-relevant heterogeneous data. To overcome the above issues, this paper proposes a novel Deep Spatio-Temporal Graph Convolutional Network, namely DSTGCN, to predict traffic accidents. The proposed model is composed of three components: the first component is the spatial learning layer which performs graph convolutional operations on spatial information to learn the correlations in space. The second component is the spatio-temporal learning layer which utilizes graph and standard convolutions to capture the dynamic variations in both spatial and temporal perspective. The third component is the embedding layer which aims to obtain meaningful and semantic representations of external information. To evaluate the proposed model, we collect large-scale real-world data, including accident records, citi-wide vehicle speeds, road networks, meteorological conditions, and Point-of-Interest distributions. Experimental results on real-world datasets demonstrate that DSTGCN outper-forms both classical and state-of-the-art methods.},
author = {Yu, Le and Du, Bowen and Hu, Xiao and Sun, Leilei and Han, Liangzhe and Lv, Weifeng},
doi = {10.1016/j.neucom.2020.09.043},
file = {:C\:/Users/Adewo/Documents/Mendley Documents/Deep spatio-temporal graph convolutional network for traffic accident prediction - Yu et al. - Unknown.pdf:pdf},
keywords = {Deep learning,Graph convolutional network,Spatio-temporal data,Traffic accident prediction},
mendeley-groups = {SLR},
title = {{Deep spatio-temporal graph convolutional network for traffic accident prediction}},
url = {https://doi.org/10.1016/j.neucom.2020.09.043}
}
@inproceedings{Bortnikov2020,
abstract = {Automatic recognition of road accidents in traffic videos can improve road safety. Smart cities can deploy accident recognition systems to promote urban traffic safety and efficiency. This work reviews existing approaches for automatic accident detection and highlights a number of challenges that make accident detection a difficult task. Furthermore, we propose to implement a 3D Convolutional Neural Network (CNN) based accident detection system. We customize a video game to generate road traffic video data in a variety of weather and lighting conditions. The generated data is preprocessed using optical flow method and injected with noise to focus only on motion and introduce further variations in the data, respectively. The resulting data is used to train the model, which was then tested on real-life traffic videos from YouTube. The experiments demonstrate that the performance of the proposed algorithm is comparable to that of the existing models, but unlike them, it is not dependent on a large volume of real-life video data for training and does not require manual tuning of any thresholds.},
author = {Bortnikov, Mikhail and Khan, Adil and Khattak, Asad Masood and Ahmad, Muhammad},
booktitle = {Advances in Intelligent Systems and Computing},
doi = {10.1007/978-3-030-17798-0_22},
file = {:C\:/Users/Adewo/Documents/Mendley Documents/Accident Recognition via 3D CNNs for Automated Traffic Monitoring in Smart Cities - Bortnikov et al. - 2020.pdf:pdf},
isbn = {9783030177973},
issn = {21945365},
keywords = {3D convolutional neural networks,Accident recognition,Computer vision,Deep learning,Machine learning},
mendeley-groups = {SLR},
pages = {256--264},
publisher = {Springer Verlag},
title = {{Accident Recognition via 3D CNNs for Automated Traffic Monitoring in Smart Cities}},
url = {https://link-springer-com.uc.idm.oclc.org/chapter/10.1007/978-3-030-17798-0_22},
volume = {944},
year = {2020}
}
@article{Xia2018,
abstract = {Most research on anomaly detection has focused on event that is different from its spatial-temporal neighboring events. It is still a significant challenge to detect anomalies that involve multiple normal events interacting in an unusual pattern. In this work, a novel unsupervised method based on sparse topic model was proposed to capture motion patterns and detect anomalies in traffic surveillance. scale-invariant feature transform (SIFT) flow was used to improve the dense trajectory in order to extract interest points and the corresponding descriptors with less interference. For the purpose of strengthening the relationship of interest points on the same trajectory, the fisher kernel method was applied to obtain the representation of trajectory which was quantized into visual word. Then the sparse topic model was proposed to explore the latent motion patterns and achieve a sparse representation for the video scene. Finally, two anomaly detection algorithms were compared based on video clip detection and visual word analysis respectively. Experiments were conducted on QMUL Junction dataset and AVSS dataset. The results demonstrated the superior efficiency of the proposed method.},
author = {min Xia, Li and jie Hu, Xiang and Wang, Jun},
doi = {10.1007/s11771-018-3910-9},
file = {:C\:/Users/Adewo/Documents/Mendley Documents/Anomaly detection in traffic surveillance with sparse topic model - Xia, Hu, Wang - 2018.pdf:pdf},
issn = {22275223},
journal = {Journal of Central South University},
keywords = {SIFT flow,dense trajectory,fisher kernel,motion pattern,sparse topic model},
mendeley-groups = {SLR},
month = {oct},
number = {9},
pages = {2245--2257},
publisher = {Springer},
title = {{Anomaly detection in traffic surveillance with sparse topic model}},
url = {https://link-springer-com.uc.idm.oclc.org/article/10.1007/s11771-018-3910-9},
volume = {25},
year = {2018}
}
@inproceedings{Bao2020,
abstract = {Traffic accident anticipation aims to predict accidents from dashcam videos as early as possible, which is critical to safety-guaranteed self-driving systems. With cluttered traffic scenes and limited visual cues, it is of great challenge to predict how long there will be an accident from early observed frames. Most existing approaches are developed to learn features of accident-relevant agents for accident anticipation, while ignoring the features of their spatial and temporal relations. Besides, current deterministic deep neural networks could be overconfident in false predictions, leading to high risk of traffic accidents caused by self-driving systems. In this paper, we propose an uncertainty-based accident anticipation model with spatio-temporal relational learning. It sequentially predicts the probability of traffic accident occurrence with dashcam videos. Specifically, we propose to take advantage of graph convolution and recurrent networks for relational feature learning, and leverage Bayesian neural networks to address the intrinsic variability of latent relational representations. The derived uncertainty-based ranking loss is found to significantly boost model performance by improving the quality of relational features. In addition, we collect a new Car Crash Dataset (CCD) for traffic accident anticipation which contains environmental attributes and accident reasons annotations. Experimental results on both public and the newly-compiled datasets show state-of-the-art performance of our model. Our code and CCD dataset are available at https://github.com/Cogito2012/UString.},
archivePrefix = {arXiv},
arxivId = {2008.00334},
author = {Bao, Wentao and Yu, Qi and Kong, Yu},
booktitle = {MM 2020 - Proceedings of the 28th ACM International Conference on Multimedia},
doi = {10.1145/3394171.3413827},
eprint = {2008.00334},
file = {:C\:/Users/Adewo/Documents/Mendley Documents/Uncertainty-based Traffic Accident Anticipation with Spatio-Temporal Relational Learning - Bao, Yu, Kong - 2020.pdf:pdf},
isbn = {9781450379885},
keywords = {Bayesian neural networks,accident anticipation,graph convolution},
mendeley-groups = {SLR},
month = {aug},
pages = {2682--2690},
publisher = {Association for Computing Machinery, Inc},
title = {{Uncertainty-based Traffic Accident Anticipation with Spatio-Temporal Relational Learning}},
url = {http://arxiv.org/abs/2008.00334 http://dx.doi.org/10.1145/3394171.3413827},
year = {2020}
}
@inproceedings{Reddy2021,
abstract = {This paper presents a model which imitates human in all ways of driving without human intervention be it recognizing the car states. It incorporates the car speed, position to track, and the surrounding environments based on a combination of Deep Q-Learning and YOLOv3 on the Raspberry Pi computer. Environmental learning is done using Deep Q-Learning. Data can be collected by using Raspberry pi and a front-facing camera. A DNN model with Deep Q-Learning is trained to get high performance in recognition and control tasks. Deep Q-Learning approximates the values by using Deep Neural Networks (DNN). The neural network takes the initial state as input returns the Q-value of all possible actions as an output. Deep Q-Learning addresses correlations between samples and non-stationary by experience replay and fixed Q-Targets. YOLOv3 with data augmentation is used to detect and recognize stationary and movable objects, traffic lights, and road signs. YOLOv3 has improved bounding box prediction, more accurate class predictions, and improved abilities at different scales and is much faster. It is easy to optimize since it works based on algorithms that use only one neural network to execute all the task components. YOLOv3, along with data augmentation, significantly increases the data diversity without actually collecting the new training data.},
author = {Reddy, D. Ramesh and Chella, Chandravathi and {Ravi Teja}, K. Bala and {Rose Baby}, Heera and Kodali, Prakash},
booktitle = {ICCISc 2021 - 2021 International Conference on Communication, Control and Information Sciences, Proceedings},
doi = {10.1109/ICCISc52257.2021.9484954},
file = {:C\:/Users/Adewo/Documents/Mendley Documents/Autonomous Vehicle Based on Deep Q-Learning and YOLOv3 with Data Augmentation - Reddy et al. - 2021.pdf:pdf},
isbn = {9781665412797},
keywords = {Data augmentation,Deep Q-Learning,Raspberry Pi computer,YOLOv3},
mendeley-groups = {SLR},
month = {jun},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Autonomous Vehicle Based on Deep Q-Learning and YOLOv3 with Data Augmentation}},
year = {2021}
}
@inproceedings{Fernandez-Llorca2020,
abstract = {In highway scenarios, an alert human driver will typically anticipate early cut-in and cut-out maneuvers of surrounding vehicles using only visual cues. An automated system must anticipate these situations at an early stage too, to increase the safety and the efficiency of its performance. To deal with lane-change recognition and prediction of surrounding vehicles, we pose the problem as an action recognition/prediction problem by stacking visual cues from video cameras. Two video action recognition approaches are analyzed: two-stream convolutional networks and spatiotemporal multiplier networks. Different sizes of the regions around the vehicles are analyzed, evaluating the importance of the interaction between vehicles and the context information in the performance. In addition, different prediction horizons are evaluated. The obtained results demonstrate the potential of these methodologies to serve as robust predictors of future lane-changes of surrounding vehicles in time horizons between 1 and 2 seconds.},
archivePrefix = {arXiv},
arxivId = {2008.10869},
author = {Fern{\'{a}}ndez-Llorca, David and Biparva, Mahdi and Izquierdo-Gonzalo, Rub{\'{e}}n and Tsotsos, John K.},
booktitle = {2020 IEEE 23rd International Conference on Intelligent Transportation Systems, ITSC 2020},
doi = {10.1109/ITSC45102.2020.9294326},
eprint = {2008.10869},
file = {:C\:/Users/Adewo/Documents/Mendley Documents/Two-Stream Networks for Lane-Change Prediction of Surrounding Vehicles - Fern{\'{a}}ndez-Llorca et al. - 2020.pdf:pdf},
isbn = {9781728141497},
mendeley-groups = {SLR},
month = {sep},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Two-Stream Networks for Lane-Change Prediction of Surrounding Vehicles}},
year = {2020}
}
@article{Cui2014,
abstract = {Automatically recognizing rear light signals of front vehicles can significantly improve driving safety by automatic alarm and taking actions proactively to prevent collisions and accidents. Much previous research only focuses on the detection of brake signals at night. In this paper, we propose a novel and robust framework to detect rear lights of vehicles and estimate their signal states at daytime. Comparing with existing state-of-the-art works, our experimental results show the high detection rate and robustness of our system in complicated light conditions.},
author = {Cui, Zhiyong and Yang, Shao Wen and Wang, Chenqi and Tsai, Hsin Mu},
doi = {10.1109/ITSC.2014.6958037},
file = {:C\:/Users/Adewo/Documents/Mendley Documents/On addressing driving inattentiveness Robust rear light status classification using Hierarchical Matching Pursuit - Cui et al. - 2014.pdf:pdf},
isbn = {9781479960781},
journal = {2014 17th IEEE International Conference on Intelligent Transportation Systems, ITSC 2014},
mendeley-groups = {SLR},
month = {nov},
pages = {2243--2244},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{On addressing driving inattentiveness: Robust rear light status classification using Hierarchical Matching Pursuit}},
year = {2014}
}
@inproceedings{Wang2020a,
abstract = {Traffic flow analysis, prediction and management are keystones for building smart cities in the new era. With the help of deep neural networks and big traffic data, we can better understand the latent patterns hidden in the complex transportation networks. The dynamic of the traffic flow on one road not only depends on the sequential patterns in the temporal dimension but also relies on other roads in the spatial dimension. Although there are existing works on predicting the future traffic flow, the majority of them have certain limitations on modeling spatial and temporal dependencies. In this paper, we propose a novel spatial temporal graph neural network for traffic flow prediction, which can comprehensively capture spatial and temporal patterns. In particular, the framework offers a learnable positional attention mechanism to effectively aggregate information from adjacent roads. Meanwhile, it provides a sequential component to model the traffic flow dynamics which can exploit both local and global temporal dependencies. Experimental results on various real traffic datasets demonstrate the effectiveness of the proposed framework.},
author = {Wang, Xiaoyang and Ma, Yao and Wang, Yiqi and Jin, Wei and Wang, Xin and Tang, Jiliang and Jia, Caiyan and Yu, Jian},
booktitle = {The Web Conference 2020 - Proceedings of the World Wide Web Conference, WWW 2020},
doi = {10.1145/3366423.3380186},
file = {:C\:/Users/Adewo/Documents/Mendley Documents/Traffic Flow Prediction via Spatial Temporal Graph Neural Network - Wang et al. - 2020.pdf:pdf},
isbn = {9781450370233},
keywords = {Dynamic,Graph Neural Networks,Recurrent Neural Network,Spatial Temporal Model,Traffic Prediction,Transformer},
mendeley-groups = {SLR},
month = {apr},
pages = {1082--1092},
publisher = {Association for Computing Machinery, Inc},
title = {{Traffic Flow Prediction via Spatial Temporal Graph Neural Network}},
url = {https://doi.org/10.1145/3366423.3380186},
volume = {11},
year = {2020}
}
@article{Patil2021,
abstract = {The rising mobility of vehicles has contributed to a considerable rise in the worldwide number of deaths and injuries associated with highways. To counter this, many researchers are working to enhance comfort and safety for society. In order to add this, authors have experimented at different times and situations to explore the essence of driver drowsiness. The primary objective of this work is to develop an intelligent system which will monitor the behavior of the driver in order to improve the overall safety for passengers as well as pedestrians. Various samples are observed cautiously with the aid of web camera images to identify the tiredness of drivers. Hence using an artificial neural network, we have built an algorithm for automatic sensing of driver's behaviors related to drowsiness. Moreover, the algorithm shows the corrective actions to avoid any incidence and improves the safety of travelers. The consequences of the proposed method are discussed, as well as steps that could further increase the accuracy of detection to enhance safety.},
author = {Patil, Lalit and Khairnar, Hrishikesh},
doi = {10.1145/3478586.3478590},
file = {:C\:/Users/Adewo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Enhancing Safety of Electric Vehicle Drivers through Vision Oriented Monitoring System Vision Oriented Monitoring System - Patil, Khairn.pdf:pdf},
isbn = {9781450389716},
journal = {ACM International Conference Proceeding Series},
keywords = {Automobile vehicles,Driver state monitoring,Driver's Drowsiness,Road Safety},
mendeley-groups = {SLR},
month = {jun},
publisher = {Association for Computing Machinery},
title = {{Enhancing Safety of Electric Vehicle Drivers through Vision Oriented Monitoring System: Vision Oriented Monitoring System}},
url = {https://doi.org/10.1145/3478586.3478590},
year = {2021}
}
@article{Riaz2022,
abstract = {Anomaly anticipation in traffic scenarios is one of the primary challenges in action recognition. It is believed that greater accuracy can be obtained by the use of semantic details and motion information along with the input frames. Most state-of-the art models extract semantic details and pre-defined optical flow from RGB frames and combine them using deep neural networks. Many previous models failed to extract motion information from pre-processed optical flow. Our study shows that optical flow provides better detection of objects in video streaming, which is an essential feature in further accident prediction. Additional to this issue, we propose a model that utilizes the recurrent neural network which instantaneously propagates predictive coding errors across layers and time steps. By assessing over time the representations from the pre-trained action recognition model from a given video, the use of pre-processed optical flows as input is redundant. Based on the final predictive score, we show the effectiveness of our proposed model on three different types of anomaly classes as Speeding Vehicle, Vehicle Accident, and Close Merging Vehicle from the state-of-the-art KITTI, D2City and HTA datasets.},
author = {Riaz, Waqar and Chenqiang, Gao and Azeem, Abdullah and Saifullah and Bux, Jamshaid Allah and Ullah, Asif},
doi = {10.3390/rs14030447},
file = {:C\:/Users/Adewo/Documents/Mendley Documents/Traffic Anomaly Prediction System Using Predictive Network - Riaz et al. - 2022.pdf:pdf},
issn = {20724292},
journal = {Remote Sensing},
keywords = {Anomaly anticipation,Feature extraction,Optical flow,Predictive Network},
mendeley-groups = {SLR},
month = {jan},
number = {3},
pages = {447},
publisher = {Multidisciplinary Digital Publishing Institute},
title = {{Traffic Anomaly Prediction System Using Predictive Network}},
url = {https://www.mdpi.com/2072-4292/14/3/447/htm https://www.mdpi.com/2072-4292/14/3/447},
volume = {14},
year = {2022}
}
@inproceedings{Sotelo2021,
abstract = {Self-driving cars have experienced a booming development in the latest years, having achieved a certain degree of maturity. Their scene recognition capabilities have improved in an impressive manner, especially thanks to the development of Deep Learning techniques and the availability of immense amount of data contained in well-organized public datasets. But still, self-driving cars exhibit limited ability to deal with certain types of situations that do not pose a great challenge to human drivers, such as entering a congested round-about, dealing with cyclists, or giving way to a vehicle that is aggressively merging onto the highway from a ramp lane. All these tasks require the development of advanced prediction capabilities in order to provide the most likely trajectories for all traffic agents around the ego-car, namely vehicles and vulnerable road users, in a given time horizon. This talk will present some innovative solutions for efficient motion prediction in the context of autonomous driving. Predicting traffic participants trajectories is of major importance in autonomous driving applications, since it allows the controller to plan ahead the motion of the vehicle, avoiding collisions and making better driving decisions. In this work, we aim at socially-aware and socially-consistent vehicles and VRUs trajectory forecasting and interaction understanding. Accurately forecasting the motion of surrounding agents is an extremely complex and challenging task, considering that many factors can affect the future trajectory of an object. First of all, the variety and complexity of road scenes is immense and traffic scene dynamics can be extremely different among different, or even similar, scenarios. Therefore, one major challenge of developing prediction methods is to find comprehensive and generic representations for all common scenarios that can be encountered in the real world. Moreover, although deep learning based models have shown incredible forecasting abilities, it would be desirable to retain structural information and explainability, instead of relying on the black-box nature of these models. Motion forecasting must be socially-aware, i.e., it must consider the past trajectories and intentions of surrounding agents. However, one major open question in the field of motion forecasting is how to model such interactions among traffic agents. Understanding how the ego-vehicle actions might influence other actors' behaviors is essential for safe and comfortable motion planning of self-driving vehicles. Additionally, these predictions must be consistent among vehicles and non-overlapping. This can only be achieved by deeply understanding the scene dynamics and the essence of interactions among traffic participants. On the other side, most deep learning based models used for trajectory forecasting operate on data of a fixed size and a fixed spatial organization, which impedes to obtain a general representation for inputs and outputs such that they can be flexible to the number and type of agent as well as transferable under different scenarios.},
author = {Sotelo, Miguel Angel},
doi = {10.1109/icarsc52212.2021.9429787},
file = {:C\:/Users/Adewo/Documents/Mendley Documents/Advanced Motion Prediction for Self-Driving Cars - Sotelo - 2021.pdf:pdf},
mendeley-groups = {SLR},
month = {may},
pages = {1--2},
publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
title = {{Advanced Motion Prediction for Self-Driving Cars}},
year = {2021}
}
@inproceedings{Alkandari2015,
abstract = {Traffic congestion is a problem that requires extremely complex and iterative solutions. Our work demonstrates the main Map and the theory of the proposed Hybrid system, which based on fuzzy logic technique for optimally controlling the traffic flow and accident detection in cities. The proposed system was developed into a two-layered: the Architecture layer and the Application layer. The focus of this paper is to discuss the Application layer represented by the algorithm and the methods used in the in the Intelligent Traffic light control system. The system was divided into three sub systems: The proposed Dynamic Webster with dynamic Cycle Time algorithm, Accident Detection System using fuzzy logic theory and Action System depending on Accident Detection System. This paper explain the main parts of the system with a great details showed in flow charts.},
author = {Alkandari, Abdulrahman Abdullah and Aljandal, Meshari},
booktitle = {2015 2nd International Conference on Computing Technology and Information Management, ICCTIM 2015},
doi = {10.1109/ICCTIM.2015.7224594},
file = {:C\:/Users/Adewo/Documents/Mendley Documents/Theory_of_dynamic_fuzzy_logic_traffic_light_integrated_system_with_accident_detection_and_action.pdf:pdf},
isbn = {9781479962112},
keywords = {Dynamic Webster Dynamic Cycle Time (DWDC),Fuzzy Logic theory,Fuzzy Logic with Accident Detection,Intelligent traffic control system},
mendeley-groups = {SLR},
month = {aug},
pages = {62--68},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Theory of dynamic fuzzy logic traffic light integrated system with accident detection and action}},
year = {2015}
}
@article{Pawlus2013,
abstract = {Vehicle crash test is the most direct and common method to assess vehicle crashworthiness. Visual inspection and obtained measurements, such as car acceleration, are used, e.g. to examine impact severity of an occupant or to assess overall car safety. However, those experiments are complex, time-consuming, and expensive. We propose a method to reproduce car kinematics during a collision using nonlinear autoregressive (NAR) model which parameters are estimated by the use of feedforward neural network. NAR model presented in this study is derived from the more general one-nonlinear autoregressive with moving average (NARMA). Suitability of autoregressive systems for data-based modeling was confirmed by application of neural networks with a NAR model to experimental data-measurements of vehicle acceleration during a crash test. This model allows us to predict the kinematic responses (acceleration, velocity, and displacement) of a given car during a collision. The major advantage of this approach is that those plots can be obtained without additional teaching of a network. {\textcopyright} 2012 Elsevier Inc. All rights reserved.},
author = {Pawlus, Witold and Karimi, Hamid Reza and Robbersmyr, Kjell G},
doi = {10.1016/j.ins.2012.03.013},
file = {:C\:/Users/Adewo/Documents/Mendley Documents/Data-based modeling of vehicle collisions by nonlinear autoregressive.pdf:pdf},
issn = {00200255},
journal = {Information Sciences},
keywords = {Data-based modeling,Feedforward neural network,Nonlinear autoregressive model,Vehicle crash},
mendeley-groups = {SLR},
pages = {65--79},
title = {{Data-based modeling of vehicle collisions by nonlinear autoregressive model and feedforward neural network}},
url = {http://dx.doi.org/10.1016/j.ins.2012.03.013},
volume = {235},
year = {2013}
}
@article{Wang2020,
abstract = {Spatiotemporal safety forecasting has various applications in the neuroscience, climate and transportation domains. It is challenging due to (1) the complex spatial dependency on networks, (2) non-linear temporal dynamics with changing conditions and (3) the inherent difficulty of long-term forecasting. To address these challenges, a safety prediction model called the Spatial-Temporal Mixed Attention Graph-based Convolution model (STMAG) is proposed. Specifically, STMAG captures spatial dependency using graph convolutional networks (GCN), and temporal dependency using the sequence-to-sequence (Seq2Seq) architecture with the mixed attention mechanisms. A case study on the implementation of this model in traffic safety prediction is given as an example. Traffic safety forecasting is one canonical example of such a learning task, which is also a crucial problem to improving transportation and public safety. A number of detailed features (such as vehicle type, braking state, whether changing lanes or not) and exogenous variables (such as weather, time and road condition) are extracted from our big datasets. Finally, we conduct extensive experiments to evaluate the STMAG framework on real-world large-scale road network traffic datasets. Extensive experiments on our dataset show that the STMAG framework makes reasonably accurate predictions and significantly improves the prediction accuracy over baseline approaches.},
author = {Wang, Jingjuan and Chen, Qingkui and Gong, Huilin},
doi = {10.1016/j.ins.2020.03.040},
file = {:C\:/Users/Adewo/Documents/Mendley Documents/STMAG A spatial-temporal mixed attention graph-based convolution model for multi-data flow safety prediction.pdf:pdf},
issn = {00200255},
journal = {Information Sciences},
keywords = {Attention mechanism,Deep learning neural network,Graph convolutional network (GCN),Safety prediction,Seq2Seq model},
mendeley-groups = {SLR},
pages = {16--36},
title = {{STMAG: A spatial-temporal mixed attention graph-based convolution model for multi-data flow safety prediction}},
url = {https://doi.org/10.1016/j.ins.2020.03.040},
volume = {525},
year = {2020}
}
@article{Yao2022,
abstract = {Video anomaly detection has been extensively studied for static cameras but is much more challenging in egocentric driving videos where the scenes are extremely dynamic. This paper proposes an unsupervised method for anomaly detection based on future object localization. The idea is to predict locations of traffic participants short time steps into the future, and then monitor the accuracy and consistency of these predictions as evidence of an anomaly: inconsistent predictions tend to indicate that an anomaly has or is about to occur. To evaluate our method, we introduce a new large-scale benchmark dataset called Detection of Traffic Anomaly (DoTA) containing 4,677 videos with temporal, spatial, and categorical annotations. We also propose a new evaluation metric, which we call spatial-temporal area under curve (STAUC), and show that it captures how well a model detects both temporal and spatial locations of anomalous events (unlike existing metrics which focus only on temporal localization). Experimental results show that our method outperforms state-of-the-art methods on DoTA in terms of both metrics. In addition, we use the rich categorical annotations in DoTA to benchmark video action detection and online action detection methods. The DoTA dataset has been made available at: https://github.com/MoonBlvd/Detection-of-Traffic-Anomaly.},
annote = {Although newer methods
R(2+1)D and SlowFast achieve higher average accuracy, all candidates suffer from low accuracy on DoTA, indicating that traffic anomaly classification is challenging. First, dis- tant anomalies and occluded objects have low visibility and thus are hard to classify. For},
author = {Yao, Yu and Wang, Xizi and Xu, Mingze and Pu, Zelin and Wang, Yuchen and Atkins, Ella and Crandall, David},
doi = {10.1109/TPAMI.2022.3150763},
file = {:C\:/Users/Adewo/Documents/Mendley Documents/DoTA_Unsupervised_Detection_of_Traffic_Anomaly_in_Driving_Videos.pdf:pdf},
issn = {19393539},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Accidents,Annotations,Anomaly detection,Benchmark testing,Cameras,Future object localization,Measurement,Traffic accident detection,Video action recognition,Video anomaly detection,Videos},
mendeley-groups = {SLR},
publisher = {IEEE Computer Society},
title = {{DoTA: Unsupervised Detection of Traffic Anomaly in Driving Videos}},
year = {2022}
}
@inproceedings{Tang2017,
abstract = {In this paper, an efficient unsupervised model is proposed to recognize simple actions and complex activities in traffic scenes which is named mixture LDA model. Under this framework, we use hierarchical Bayesian models are to describe three important components in traffic video: basic visual features, simple actions, and complex activities. This model adopts an unsupervised way to learn how to recognize traffic video. Moving pixels can be divided into different simple actions and short video clips can be divided into different complex activities in a long traffic video sequence, then we can achieve the purpose of recognizing different activities in the surveillance video.},
author = {Tang, Xiaowei and Huang, Xin Lin and Sun, Si Yue and Dong, Hang and Zhang, Xin and Gao, Yu and Liu, Nan},
booktitle = {Lecture Notes of the Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering, LNICST},
doi = {10.1007/978-3-319-52730-7_36},
file = {:C\:/Users/Adewo/Documents/Mendley Documents/Intelligent recognition of traffic video based on mixture LDA model - Tang et al. - 2017.pdf:pdf},
isbn = {9783319527291},
issn = {18678211},
keywords = {Bayesian model,Mixture LDA model,Traffic video identification},
mendeley-groups = {SLR},
pages = {356--363},
publisher = {Springer Verlag},
title = {{Intelligent recognition of traffic video based on mixture LDA model}},
volume = {183},
year = {2017}
}

@inproceedings{izquierdo2019prevention,
  title={The prevention dataset: a novel benchmark for prediction of vehicles intentions},
  author={Izquierdo, Rub{\'e}n and Quintanar, A and Parra, Ignacio and Fern{\'a}ndez-Llorca, D and Sotelo, MA},
  booktitle={2019 IEEE Intelligent Transportation Systems Conference (ITSC)},
  pages={3114--3121},
  year={2019},
  organization={IEEE}
}
@ARTICLE{zhao-pku,  author={Zhao, Huijing and Wang, Chao and Lin, Yuping and Guillemard, Franck and Geronimi, Stéphane and Aioun, François},  journal={IEEE Transactions on Intelligent Transportation Systems},   title={On-Road Vehicle Trajectory Collection and Scene-Based Lane Change Analysis: Part I},   year={2017},  volume={18},  number={1},  pages={192-205},  doi={10.1109/TITS.2016.2571726}}
@article{halkias2006ngsim,
  title={NGSIM interstate 80 freeway dataset},
  author={Halkias, John and Colyar, James},
  journal={US Federal Highway Administration, FHWA-HRT-06-137, Washington, DC, USA},
  year={2006}
}
@article{zhan2019interaction,
  title={Interaction dataset: An international, adversarial and cooperative motion dataset in interactive driving scenarios with semantic maps},
  author={Zhan, Wei and Sun, Liting and Wang, Di and Shi, Haojie and Clausse, Aubrey and Naumann, Maximilian and Kummerle, Julius and Konigshof, Hendrik and Stiller, Christoph and de La Fortelle, Arnaud and others},
  journal={arXiv preprint arXiv:1910.03088},
  year={2019}
}
@inproceedings{krajewski2018highd,
  title={The highd dataset: A drone dataset of naturalistic vehicle trajectories on german highways for validation of highly automated driving systems},
  author={Krajewski, Robert and Bock, Julian and Kloeker, Laurent and Eckstein, Lutz},
  booktitle={2018 21st International Conference on Intelligent Transportation Systems (ITSC)},
  pages={2118--2125},
  year={2018},
  organization={IEEE}
}
@article{zhang2018gaan,
  title={Gaan: Gated attention networks for learning on large and spatiotemporal graphs},
  author={Zhang, Jiani and Shi, Xingjian and Xie, Junyuan and Ma, Hao and King, Irwin and Yeung, Dit-Yan},
  journal={arXiv preprint arXiv:1803.07294},
  year={2018}
}
@article{zhao2019t,
  title={T-gcn: A temporal graph convolutional network for traffic prediction},
  author={Zhao, Ling and Song, Yujiao and Zhang, Chao and Liu, Yu and Wang, Pu and Lin, Tao and Deng, Min and Li, Haifeng},
  journal={IEEE Transactions on Intelligent Transportation Systems},
  volume={21},
  number={9},
  pages={3848--3858},
  year={2019},
  publisher={IEEE}
  }

@article{smola2004tutorial,
  title={A tutorial on support vector regression},
  author={Smola, Alex J and Sch{\"o}lkopf, Bernhard},
  journal={Statistics and computing},
  volume={14},
  number={3},
  pages={199--222},
  year={2004},
  publisher={Springer}
}
@article{singh2018deep,
  title={Deep spatio-temporal representation for detection of road accidents using stacked autoencoder},
  author={Singh, Dinesh and Mohan, Chalavadi Krishna},
  journal={IEEE Transactions on Intelligent Transportation Systems},
  volume={20},
  number={3},
  pages={879--887},
  year={2018},
  publisher={IEEE}
}
@article{gu2016twitter,
  title={From Twitter to detector: Real-time traffic incident detection using social media data},
  author={Gu, Yiming and Qian, Zhen Sean and Chen, Feng},
  journal={Transportation research part C: emerging technologies},
  volume={67},
  pages={321--342},
  year={2016},
  publisher={Elsevier}
}
@article{Salas2017TrafficED,
  title={Traffic event detection framework using social media},
  author={A. Salas and Panagiotis Georgakis and Christopher Nwagboso and Ahmad Ammari and I. Petalas},
  journal={2017 IEEE International Conference on Smart Grid and Smart Cities (ICSGSC)},
  year={2017},
  pages={303-307}
}
@article{rashidi2017exploring,
  title={Exploring the capacity of social media data for modelling travel behaviour: Opportunities and challenges},
  author={Rashidi, Taha H and Abbasi, Alireza and Maghrebi, Mojtaba and Hasan, Samiul and Waller, Travis S},
  journal={Transportation Research Part C: Emerging Technologies},
  volume={75},
  pages={197--211},
  year={2017},
  publisher={Elsevier}
}
@article{xu2019traffic,
  title={Traffic Event Detection Using Twitter Data Based on Association Rules},
  author={Xu, Shishuo and Li, Songnian and Wen, Richard and Huang, Wei},
  journal={ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
  volume={4},
  pages={543--547},
  year={2019},
  publisher={Copernicus GmbH}
}
@article{arvin2019instantaneous,
  title={How instantaneous driving behavior contributes to crashes at intersections: extracting useful information from connected vehicle message data},
  author={Arvin, Ramin and Kamrani, Mohsen and Khattak, Asad J},
  journal={Accident Analysis \& Prevention},
  volume={127},
  pages={118--133},
  year={2019},
  publisher={Elsevier}
}
@article{xu2018sensing,
  title={Sensing and detecting traffic events using geosocial media data: A review},
  author={Xu, Shishuo and Li, Songnian and Wen, Richard},
  journal={Computers, Environment and Urban Systems},
  volume={72},
  pages={146--160},
  year={2018},
  publisher={Elsevier}
}
@article{adewopo2022deep,
  title={Deep Learning Algorithm for Threat Detection in Hackers Forum (Deep Web)},
  author={Adewopo, Victor and Gonen, Bilal and Elsayed, Nelly and Ozer, Murat and Elsayed, Zaghloul Saad},
  journal={arXiv preprint arXiv:2202.01448},
  year={2022}
}
@inproceedings{stisen2015smart,
  title={Smart devices are different: Assessing and mitigatingmobile sensing heterogeneities for activity recognition},
  author={Stisen, Allan and Blunck, Henrik and Bhattacharya, Sourav and Prentow, Thor Siiger and Kj{\ae}rgaard, Mikkel Baun and Dey, Anind and Sonne, Tobias and Jensen, Mads M{\o}ller},
  booktitle={Proceedings of the 13th ACM conference on embedded networked sensor systems},
  pages={127--140},
  year={2015}
}
@article{joshua1990estimating,
  title={Estimating truck accident rate and involvements using linear and Poisson regression models},
  author={Joshua, Sarath C and Garber, Nicholas J},
  journal={Transportation planning and Technology},
  volume={15},
  number={1},
  pages={41--58},
  year={1990},
  publisher={Taylor \& Francis}
}
@article{Huang2019,
abstract = {.},
annote = {In.},
archivePrefix = {arXiv},
arxivId = {1901.01138},
author = {Huang, Xiaohui and He, Pan and Rangarajan, Anand and Ranka, Sanjay},
doi = {10.1145/3373647},
eprint = {1901.01138},
file = {:C\:/Users/Adewo/Documents/Mendley Documents/Intelligent Intersection Two-Stream Convolutional Networks for Real-time Near Accident Detection in Traffic Video - Huang et al. - 2019.pdf:pdf},
issn = {23740361},
journal = {ACM Transactions on Spatial Algorithms and Systems},
keywords = {ITS,Intelligent transportation systems,convolutional neural networks,multipleobject tracking,near-accident detection},
month = {jan},
number = {2},
pages = {23},
publisher = {Association for Computing Machinery},
title = {{Intelligent Intersection: Two-Stream Convolutional Networks for Real-time Near Accident Detection in Traffic Video}},
url = {https://arxiv.org/abs/1901.01138v1},
volume = {6},
year = {2019}
}

@inproceedings{Piergiovanni2019,
abstract = {In this paper, we propose a convolutional layer inspired by optical flow algorithms to learn motion representations. Our representation flow layer is a fully-differentiable layer designed to capture the 'flow' of any representation channel within a convolutional neural network for action recognition. Its parameters for iterative flow optimization are learned in an end-to-end fashion together with the other CNN model parameters, maximizing the action recognition performance. Furthermore, we newly introduce the concept of learning 'flow of flow' representations by stacking multiple representation flow layers. We conducted extensive experimental evaluations, confirming its advantages over previous recognition models using traditional optical flows in both computational speed and performance. The code is publicly available.},
annote = {The reseaserachers worked on developing representation flow . Representation flow is a differentiable layer designed to capture the flow of the represntation layer within a Convolutional Neural Network for AR.
Two Stream network with both RGB and optical flow has been successful in increasing accuracy of Action Recognition tasks how ever computing optical flow at each frame iteration is computationally expensive and also increase the number of parameters required for inferencing. This serve as a draw back in real time application of AR tasks beacuse of the numerious activities required with short period. The resesearches stacked multiple representation flow tagged Flow of Flow to eliminate the requirements for optical flow during inferencing.
In this research stacking multiple representation flow layers, the devloped model captures long temporal intervals, CNN was intoroduced to compute intermediate future maps and convolutions to reduce number of channels before the oprical flow algorithm.
The authors contributed the body of knowledge introducing a new differentiable CNN feature map for intermeddiate represntations which outperforms existing motion representation methods with increases speed and accuracy.
This paper proposed a new reepresentation optical representation(Flow of flow) in the CNN two streams design RGB + optical flow. The optical flow requires optimization and computational exensive.
The researchers deloved optical flow algorithm to learn motion representations with a CNN for AR tast. flow of Flow represntation was achieved by stacking multiple represntation flow layers. Utilizing optical flow requires mutiple optimization iterations in every frame thereby leading to increased trainning parameters.},
archivePrefix = {arXiv},
arxivId = {1810.01455},
author = {Piergiovanni, A J and Ryoo, Michael S},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2019.01018},
eprint = {1810.01455},
file = {:C\:/Users/Adewo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Representation Flow for Action Recognition - Piergiovanni, Ryoo - Unknown.pdf:pdf;:C\:/Users/Adewo/Documents/Mendley Documents/Representation flow for action recognition.pdf:pdf},
isbn = {9781728132938},
issn = {10636919},
keywords = {Action Recognition,Deep Learning,Representation Learning},
pages = {9937--9945},
title = {{Representation flow for action recognition}},
url = {https://piergiaj.github.io/rep-flow-site/},
volume = {2019-June},
year = {2019}
}

@misc{page2021prisma,
  title={The PRISMA 2020 statement: An updated guideline for reporting systematic reviews [Internet]. Vol. 372, The BMJ},
  author={Page, MJ and McKenzie, JE and Bossuyt, PM and Boutron, I and Hoffmann, TC and Mulrow, CD and others},
  year={2021},
  publisher={BMJ Publishing Group}
}
@book{gough2017introduction,
  title={An introduction to systematic reviews},
  author={Gough, David and Oliver, Sandy and Thomas, James},
  year={2017},
  publisher={Sage}
}
@inproceedings{azumah2021deep,
  title={A deep lstm based approach for intrusion detection iot devices network in smart home},
  author={Azumah, Sylvia Worlali and Elsayed, Nelly and Adewopo, Victor and Zaghloul, Zaghloul Saad and Li, Chengcheng},
  booktitle={2021 IEEE 7th World Forum on Internet of Things (WF-IoT)},
  pages={836--841},
  year={2021},
  organization={IEEE}
}
@ARTICLE{Roboticsfrontier,
  
AUTHOR={Vrigkas, Michalis and Nikou, Christophoros and Kakadiaris, Ioannis A.},   
	 
TITLE={A Review of Human Activity Recognition Methods},      
	
JOURNAL={Frontiers in Robotics and AI},      
	
VOLUME={2},      
	
YEAR={2015},      
	  
URL={https://www.frontiersin.org/article/10.3389/frobt.2015.00028},       
	
DOI={10.3389/frobt.2015.00028},      
	
ISSN={2296-9144},   
   
ABSTRACT={Recognizing human activities from video sequences or still images is a challenging task due to problems, such as background clutter, partial occlusion, changes in scale, viewpoint, lighting, and appearance. Many applications, including video surveillance systems, human-computer interaction, and robotics for human behavior characterization, require a multiple activity recognition system. In this work, we provide a detailed review of recent and state-of-the-art research advances in the field of human activity classification. We propose a categorization of human activity methodologies and discuss their advantages and limitations. In particular, we divide human activity classification methods into two large categories according to whether they use data from different modalities or not. Then, each of these categories is further analyzed into sub-categories, which reflect how they model human activities and what type of activities they are interested in. Moreover, we provide a comprehensive analysis of the existing, publicly available human activity classification datasets and examine the requirements for an ideal human activity recognition dataset. Finally, we report the characteristics of future research directions and present some open issues on human activity recognition.}
}
@article{greff2016lstm,
  title={LSTM: A search space odyssey},
  author={Greff, Klaus and Srivastava, Rupesh K and Koutn{\'\i}k, Jan and Steunebrink, Bas R and Schmidhuber, J{\"u}rgen},
  journal={IEEE transactions on neural networks and learning systems},
  volume={28},
  number={10},
  pages={2222--2232},
  year={2016},
  publisher={IEEE}
}
@misc{holt2011mendeley,
  title={Mendeley: Creating communities of scholarly inquiry through research collaboration},
  author={Holt Zaugg, Richard E. West, Isaku Tateishi, Daniel L. Randall},
  year={2011},
  publisher={Springer}
}
@article{zaugg3610creating,
  title={Creating communities of scholarly inquiry through research collaboration},
  author={Zaugg, Holt and West, Richard E and Tateishi, Isaku and Randall, DL2011Mendeley},
  journal={TechTrends55132--3610.1007/s11528-011-0467-y}
}
@article{dirk2018archives,
  title={Archives and Airtable: Using Cloud-based Tools for Archival Survey and Workflow Management},
  author={Dirk, Katherine and Maddox, Jessica},
  year={2018}
}
@article{niglas2007media,
  title={Media Review: Microsoft Office Excel Spreadsheet Software.},
  author={Niglas, Katrin},
  journal={Journal of Mixed Methods Research},
  volume={1},
  number={3},
  pages={297--299},
  year={2007},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}
@article{wright2007write,
  title={How to write a systematic review.},
  author={Wright, Rick W and Brand, Richard A and Dunn, Warren and Spindler, Kurt P},
  journal={Clinical Orthopaedics and Related Research (1976-2007)},
  volume={455},
  pages={23--29},
  year={2007},
  publisher={LWW}
}
@article{harris2014write,
  title={How to write a systematic review},
  author={Harris, Joshua D and Quatman, Carmen E and Manring, MM and Siston, Robert A and Flanigan, David C},
  journal={The American journal of sports medicine},
  volume={42},
  number={11},
  pages={2761--2768},
  year={2014},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}
@article{kattenborn2021review,
  title={Review on Convolutional Neural Networks (CNN) in vegetation remote sensing},
  author={Kattenborn, Teja and Leitloff, Jens and Schiefer, Felix and Hinz, Stefan},
  journal={ISPRS Journal of Photogrammetry and Remote Sensing},
  volume={173},
  pages={24--49},
  year={2021},
  publisher={Elsevier}
}
@article{elsayed2020reduced,
  title={Reduced-gate convolutional long short-term memory using predictive coding for spatiotemporal prediction},
  author={Elsayed, Nelly and Maida, Anthony S and Bayoumi, Magdy},
  journal={Computational Intelligence},
  volume={36},
  number={3},
  pages={910--939},
  year={2020},
  publisher={Wiley Online Library}
}
@article{rodriguez2020shedding,
  title={Shedding light on people action recognition in social robotics by means of common spatial patterns},
  author={Rodr{\'\i}guez-Moreno, Itsaso and Mart{\'\i}nez-Otzeta, Jos{\'e} Mar{\'\i}a and Goienetxea, Izaro and Rodriguez-Rodriguez, Igor and Sierra, Basilio},
  journal={Sensors},
  volume={20},
  number={8},
  pages={2436},
  year={2020},
  publisher={Multidisciplinary Digital Publishing Institute}
}
@article{kruger2007meaning,
  title={The meaning of action: A review on action recognition and mapping},
  author={Kr{\"u}ger, Volker and Kragic, Danica and Ude, Ale{\v{s}} and Geib, Christopher},
  journal={Advanced robotics},
  volume={21},
  number={13},
  pages={1473--1501},
  year={2007},
  publisher={Taylor \& Francis}
}
@article{khan2014semantic,
  title={Semantic pyramids for gender and action recognition},
  author={Khan, Fahad Shahbaz and Van De Weijer, Joost and Anwer, Rao Muhammad and Felsberg, Michael and Gatta, Carlo},
  journal={IEEE transactions on image processing},
  volume={23},
  number={8},
  pages={3633--3645},
  year={2014},
  publisher={IEEE}
}
@inproceedings{yan2019research,
  title={Research on Human-Machine Task Collaboration Based on Action Recognition},
  author={Yan, Jihong and Yan, Shenyi and Zhao, Lizhong and Wang, Zipeng and Liang, Yun},
  booktitle={2019 IEEE International Conference on Smart Manufacturing, Industrial \& Logistics Engineering (SMILE)},
  pages={117--121},
  year={2019},
  organization={IEEE}
}
@inproceedings{kamthe2018suspicious,
  title={Suspicious activity recognition in video surveillance system},
  author={Kamthe, UM and Patil, CG},
  booktitle={2018 Fourth international conference on computing communication control and automation (ICCUBEA)},
  pages={1--6},
  year={2018},
  organization={IEEE}
}
@inproceedings{dhulekar2017human,
  title={Human action recognition: an overview},
  author={Dhulekar, Pravin and Gandhe, ST and Chitte, Harshada and Pardeshi, Komal},
  booktitle={Proceedings of the international conference on data engineering and communication technology},
  pages={481--488},
  year={2017},
  organization={Springer}
}
@article{al2019multimedia,
  title={Multimedia-oriented action recognition in Smart City-based IoT using multilayer perceptron},
  author={al Zamil, Mohammed GH and Samarah, Samer and Rawashdeh, Majdi and Karime, Ali and Hossain, M Shamim},
  journal={Multimedia Tools and Applications},
  volume={78},
  number={21},
  pages={30315--30329},
  year={2019},
  publisher={Springer}
}
@inproceedings{ren2002human,
  title={Human action recognition in smart classroom},
  author={Ren, Haibing and Xu, Guangyou},
  booktitle={Proceedings of fifth IEEE international conference on automatic face gesture recognition},
  pages={417--422},
  year={2002},
  organization={IEEE}
}
@inproceedings{davar2011domain,
  title={Domain adaptation in the context of sport video action recognition},
  author={Davar, N Faraji and de Campos, Teofilo and Windridge, David and Kittler, Josef and Christmas, William},
  booktitle={Domain Adaptation Workshop, in conjunction with NIPS},
  year={2011}
}
@article{zhou2020human,
  title={Human action recognition toward massive-scale sport sceneries based on deep multi-model feature fusion},
  author={Zhou, Ersan and Zhang, Heqing},
  journal={Signal Processing: Image Communication},
  volume={84},
  pages={115802},
  year={2020},
  publisher={Elsevier}
}
@article{ahmidi2017dataset,
  title={A dataset and benchmarks for segmentation and recognition of gestures in robotic surgery},
  author={Ahmidi, Narges and Tao, Lingling and Sefati, Shahin and Gao, Yixin and Lea, Colin and Haro, Benjamin Bejar and Zappella, Luca and Khudanpur, Sanjeev and Vidal, Ren{\'e} and Hager, Gregory D},
  journal={IEEE Transactions on Biomedical Engineering},
  volume={64},
  number={9},
  pages={2025--2041},
  year={2017},
  publisher={IEEE}
}
@inproceedings{sharghi2020automatic,
  title={Automatic operating room surgical activity recognition for robot-assisted surgery},
  author={Sharghi, Aidean and Haugerud, Helene and Oh, Daniel and Mohareri, Omid},
  booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},
  pages={385--395},
  year={2020},
  organization={Springer}
}
@inproceedings{raja2011joint,
  title={Joint pose estimation and action recognition in image graphs},
  author={Raja, Kumar and Laptev, Ivan and P{\'e}rez, Patrick and Oisel, Lionel},
  booktitle={2011 18th IEEE International Conference on Image Processing},
  pages={25--28},
  year={2011},
  organization={IEEE}
}
@inproceedings{cheron2015p,
  title={P-cnn: Pose-based cnn features for action recognition},
  author={Ch{\'e}ron, Guilhem and Laptev, Ivan and Schmid, Cordelia},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={3218--3226},
  year={2015}
}
@inproceedings{xiaohan2015joint,
  title={Joint action recognition and pose estimation from video},
  author={Xiaohan Nie, Bruce and Xiong, Caiming and Zhu, Song-Chun},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={1293--1301},
  year={2015}
}
@inproceedings{yao2011does,
  title={Does human action recognition benefit from pose estimation?''},
  author={Yao, Angela and Gall, Juergen and Fanelli, Gabriele and Van Gool, Luc},
  booktitle={Proceedings of the 22nd British machine vision conference-BMVC 2011},
  year={2011},
  organization={BMV press}
}
@inproceedings{jhuang2013towards,
  title={Towards understanding action recognition},
  author={Jhuang, Hueihan and Gall, Juergen and Zuffi, Silvia and Schmid, Cordelia and Black, Michael J},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={3192--3199},
  year={2013}
}
@inproceedings{sevilla2018integration,
  title={On the integration of optical flow and action recognition},
  author={Sevilla-Lara, Laura and Liao, Yiyi and G{\"u}ney, Fatma and Jampani, Varun and Geiger, Andreas and Black, Michael J},
  booktitle={German conference on pattern recognition},
  pages={281--297},
  year={2018},
  organization={Springer}
}
@inproceedings{lv2007single,
  title={Single view human action recognition using key pose matching and viterbi path searching},
  author={Lv, Fengjun and Nevatia, Ramakant},
  booktitle={2007 IEEE Conference on computer vision and pattern recognition},
  pages={1--8},
  year={2007},
  organization={IEEE}
}
@article{gedamu2021arbitrary,
  title={Arbitrary-view human action recognition via novel-view action generation},
  author={Gedamu, Kumie and Ji, Yanli and Yang, Yang and Gao, LingLing and Shen, Heng Tao},
  journal={Pattern Recognition},
  volume={118},
  pages={108043},
  year={2021},
  publisher={Elsevier}
}

@inproceedings{gabrielli2019action,
  title={Action recognition to estimate Activities of Daily Living (ADL) of elderly people},
  author={Gabrielli, Matteo and Leo, Pietro and Renzi, Fabrizio and Bergamaschi, Sonia},
  booktitle={2019 IEEE 23rd International Symposium on Consumer Technologies (ISCT)},
  pages={261--264},
  year={2019},
  organization={IEEE}
}
@article{muhammad2021human,
  title={Human action recognition using attention based LSTM network with dilated CNN features},
  author={Muhammad, Khan and Ullah, Amin and Imran, Ali Shariq and Sajjad, Muhammad and Kiran, Mustafa Servet and Sannino, Giovanna and de Albuquerque, Victor Hugo C and others},
  journal={Future Generation Computer Systems},
  volume={125},
  pages={820--830},
  year={2021},
  publisher={Elsevier}
}
@article{bo2021skeleton,
  title={Skeleton-based violation action recognition method for safety supervision in the operation field of distribution network based on graph convolutional network},
  author={Bo, Wang and Fuqi, Ma and Rong, Jia and Peng, Luo and Xuzhu, Dong},
  journal={CSEE Journal of Power and Energy Systems},
  year={2021},
  publisher={CSEE}
}
@article{al2020review,
  title={A review on computer vision-based methods for human action recognition},
  author={Al-Faris, Mahmoud and Chiverton, John and Ndzi, David and Ahmed, Ahmed Isam},
  journal={Journal of imaging},
  volume={6},
  number={6},
  pages={46},
  year={2020},
  publisher={Multidisciplinary Digital Publishing Institute}
}
@inproceedings{nowozin2014optimal,
  title={Optimal decisions from probabilistic models: the intersection-over-union case},
  author={Nowozin, Sebastian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={548--555},
  year={2014}
}
@inproceedings{elsayed2018empirical,
  title={Empirical activation function effects on unsupervised convolutional lstm learning},
  author={Elsayed, Nelly and Maida, Anthony S and Bayoumi, Magdy},
  booktitle={2018 IEEE 30th International Conference on Tools with Artificial Intelligence (ICTAI)},
  pages={336--343},
  year={2018},
  organization={IEEE}
}
@article{shi2015convolutional,
  title={Convolutional LSTM network: A machine learning approach for precipitation nowcasting},
  author={Shi, Xingjian and Chen, Zhourong and Wang, Hao and Yeung, Dit-Yan and Wong, Wai-Kin and Woo, Wang-chun},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}
@inproceedings{kuehne2014language,
  title={The language of actions: Recovering the syntax and semantics of goal-directed human activities},
  author={Kuehne, Hilde and Arslan, Ali and Serre, Thomas},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={780--787},
  year={2014}
}
@book{gleick2011information,
  title={The information: A history, a theory, a flood},
  author={Gleick, James},
  year={2011},
  publisher={Vintage}
}
@misc{marr_2021, title={How much data do we create every day? the mind-blowing stats everyone should read}, url={https://www.forbes.com/sites/bernardmarr/2018/05/21/how-much-data-do-we-create-every-day-the-mind-blowing-stats-everyone-should-read/?sh=7f37bdf260ba}, journal={Forbes}, publisher={Forbes Magazine}, author={Marr, Bernard}, year={2021}, month={Dec}} 
@article{vopson2020information,
  title={The information catastrophe},
  author={Vopson, Melvin M},
  journal={AIP Advances},
  volume={10},
  number={8},
  pages={085014},
  year={2020},
  publisher={AIP Publishing LLC}
}
@article{wang2020vision,
  title={A vision-based video crash detection framework for mixed traffic flow environment considering low-visibility condition},
  author={Wang, Chen and Dai, Yulu and Zhou, Wei and Geng, Yifei},
  journal={Journal of advanced transportation},
  volume={2020},
  year={2020},
  publisher={Hindawi}
}
@article{kang2017t,
  title={T-cnn: Tubelets with convolutional neural networks for object detection from videos},
  author={Kang, Kai and Li, Hongsheng and Yan, Junjie and Zeng, Xingyu and Yang, Bin and Xiao, Tong and Zhang, Cong and Wang, Zhe and Wang, Ruohui and Wang, Xiaogang and others},
  journal={IEEE Transactions on Circuits and Systems for Video Technology},
  volume={28},
  number={10},
  pages={2896--2907},
  year={2017},
  publisher={IEEE}
}
@inproceedings{zhu2018towards,
  title={Towards high performance video object detection},
  author={Zhu, Xizhou and Dai, Jifeng and Yuan, Lu and Wei, Yichen},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={7210--7218},
  year={2018}
}
@article{dawkins1982replicators,
  title={Replicators and vehicles},
  author={Dawkins, Richard and others},
  journal={Current problems in sociobiology},
  volume={45},
  pages={64},
  year={1982},
  publisher={Cambridge, MA}
}
@inproceedings{said2021framework,
  title={A Framework for the Discipline of Information Technology},
  author={Said, Hazem and Zidar, Michael and Varlioglu, Said and Itodo, Cornelius},
  booktitle={Proceedings of the 22st Annual Conference on Information Technology Education},
  pages={53--54},
  year={2021}
}
@misc{roztocki2019role,
  title={The role of information and communication technologies in socioeconomic development: towards a multi-dimensional framework},
  author={Roztocki, Narcyz and Soja, Piotr and Weistroffer, Heinz Roland},
  journal={Information Technology for Development},
  volume={25},
  number={2},
  pages={171--183},
  year={2019},
  publisher={Taylor \& Francis}
}
@article{parsa2019,
  title={Real-time accident detection: coping with imbalanced data},
  author={Parsa, Amir Bahador and Taghipour, Homa and Derrible, Sybil and Mohammadian, Abolfazl Kouros},
  journal={Accident Analysis \& Prevention},
  volume={129},
  pages={202--210},
  year={2019},
  publisher={Elsevier}
}
@ARTICLE{Ji,  author={Kamijo, S. and Matsushita, Y. and Ikeuchi, K. and Sakauchi, M.},  journal={IEEE Transactions on Intelligent Transportation Systems},   title={Traffic monitoring and accident detection at intersections},   year={2000},  volume={1},  number={2},  pages={108-118},  doi={10.1109/6979.880968}}
@article{Ji2013,
abstract = {We consider the automated recognition of human actions in surveillance videos. Most current methods build classifiers based on complex handcrafted features computed from the raw inputs. Convolutional neural networks (CNNs) are a type of deep model that can act directly on the raw inputs. However, such models are currently limited to handling 2D inputs. In this paper, we develop a novel 3D CNN model for action recognition. This model extracts features from both the spatial and the temporal dimensions by performing 3D convolutions, thereby capturing the motion information encoded in multiple adjacent frames. The developed model generates multiple channels of information from the input frames, and the final feature representation combines information from all channels. To further boost the performance, we propose regularizing the outputs with high-level features and combining the predictions of a variety of different models. We apply the developed models to recognize human actions in the real-world environment of airport surveillance videos, and they achieve superior performance in comparison to baseline methods. {\textcopyright} 1979-2012 IEEE.},
annote = {This paper worked on extracting temporal and spatial information from multiple adjacent framesthrough 3D convolutions. Human action recognition specifically in domain of behaviour analysos and customer attribute is a challenging task based on occlusion, camera angle and clutter backgrounds.
This research proposed visiual object recogniton using approriate CNN regularization has higher performance compared with conventional pattern recognition paradigm with handcrafted features.
The researchers observed that the performance differences be-tween 3D CNN and other methods tend to be larger when the number of positive training samples is small.

In video analysj spatial information is captured in the convolution stage to capture ftemporal features and spatial dimensions by convolving a 3D kernel to the cube formed by stacking multiple contiguous frames together thereby capturing spatial information through connected multiple frames from previous convolution layers.
The experiment was based on three action classes CELLTOEAR , OBJECTPUT and POINTING. The precision reported on this classes ar 64%,67% and 82% repectively.},
author = {Ji, Shuiwang and Xu, Wei and Yang, Ming and Yu, Kai},
doi = {10.1109/TPAMI.2012.59},
file = {:C\:/Users/Adewo/Documents/Mendley Documents/icml2010_3dcnn.pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {3D convolution,Deep learning,action recognition,convolutional neural networks,model combination},
number = {1},
pages = {221--231},
pmid = {22392705},
title = {{3D Convolutional neural networks for human action recognition}},
url = {http://www-nlpir.nist.gov/projects/trecvid/},
volume = {35},
year = {2013}
}
%#_________________________
@misc{nhtsa, title={Newly released estimates show traffic fatalities reached a 16-year high in 2021}, url={https://www.nhtsa.gov/press-releases/early-estimate-2021-traffic-fatalities}, journal={NHTSA}, publisher={NHTSA}, author={Media, NHTSA}, year={2022}, month={May}} 
@article{chang2020global,
  title={Global road traffic injury statistics: Challenges, mechanisms and solutions},
  author={Chang, Fang-Rong and Huang, He-Lai and Schwebel, David C and Chan, Alan HS and Hu, Guo-Qing},
  journal={Chinese journal of traumatology},
  volume={23},
  number={4},
  pages={216--218},
  year={2020},
  publisher={Elsevier}
  
}
@article{fors2009night,
  title={Night-time traffic in urban areas: A literature review on road user aspects},
  author={Fors, Carina and Lundkvist, Sven-Olof},
  year={2009},
  publisher={Statens v{\"a}g-och transportforskningsinstitut}
}
@misc{mohammed2019review,
  title={A review of the traffic accidents and related practices worldwide. Open Transport. J. 13, 65--83},
  author={Mohammed, AA and Ambak, K and Mosa, AM and Syamsunur, D},
  year={2019}
}
@article{vecino2022saving,
  title={Saving lives through road safety risk factor interventions: global and national estimates},
  author={Vecino-Ortiz, Andres I and Nagarajan, Madhuram and Elaraby, Sarah and Guzman-Tordecilla, Deivis Nicolas and Paichadze, Nino and Hyder, Adnan A},
  journal={The Lancet},
  year={2022},
  publisher={Elsevier}
}
@phdthesis{sauerzapf2012road,
  title={Road traffic crash fatalities: An examination of national fatality rates and factors associated with the variation in fatality rates between nations with reference to the World Health Organisation Decade of Action for Road Safety 2011-2020},
  author={Sauerzapf, Violet Anne},
  year={2012},
  school={University of East Anglia}
}
@article{Adewopo2022ReviewOA,
  title={Review on Action Recognition for Accident Detection in Smart City Transportation Systems},
  author={Victor Adewopo and Nelly Elsayed and Zag Elsayed and Murat Ozer and Ahmed Abdelgawad and Magdy Bayoumi},
  journal={ArXiv},
  year={2022},
  volume={abs/2208.09588}
}
@article{banstola2017cost,
  title={Cost-effectiveness of interventions to prevent road traffic injuries in low-and middle-income countries: A literature review},
  author={Banstola, Amrit and Mytton, Julie},
  journal={Traffic injury prevention},
  volume={18},
  number={4},
  pages={357--362},
  year={2017},
  publisher={Taylor \& Francis}
}
@article{bhalla2019building,
  title={Building road safety institutions in low-and middle-income countries: the case of Argentina},
  author={Bhalla, Kavi and Shotten, Marc},
  journal={Health Systems \& Reform},
  volume={5},
  number={2},
  pages={121--133},
  year={2019},
  publisher={Taylor \& Francis}
}
@article{mohammed2019review,
  title={A review of traffic accidents and related practices worldwide},
  author={Mohammed, Ali Ahmed and Ambak, Kamarudin and Mosa, Ahmed Mancy and Syamsunur, Deprizon},
  journal={The Open Transportation Journal},
  volume={13},
  number={1},
  year={2019}
}
@INPROCEEDINGS{Khalil,
  author={Khalil, Usman and Nasir, Adnan and Khan, S.M. and Javid, T. and Raza, S.A. and Siddiqui, A.},
  booktitle={2018 IEEE 21st International Multi-Topic Conference (INMIC)}, 
  title={Automatic Road Accident Detection Using Ultrasonic Sensor}, 
  year={2018},
  volume={},
  number={},
  pages={206-212},
  doi={10.1109/INMIC.2018.8595541}}
  @misc{NSC,
title = {{NSC Preliminary Estimates: 21,340 People Died in Traffic Crashes in First Half of 2022 - National Safety Council}},
url = {https://www.nsc.org/newsroom/nsc-preliminary-estimates-21340-people-died-in-tra},
urldate = {2023-03-06}
}
@article{green2004human,
  title={Human error in road accidents},
  author={Green, Marc and Senders, John},
  journal={Visual Expert},
  year={2004}
}

@Article{Mateen,
AUTHOR = {Mateen, Abdul and Hanif, Muhammad Zahid and Khatri, Narayan and Lee, Sihyung and Nam, Seung Yeob},
TITLE = {Smart Roads for Autonomous Accident Detection and Warnings},
JOURNAL = {Sensors},
VOLUME = {22},
YEAR = {2022},
NUMBER = {6},
ARTICLE-NUMBER = {2077},
URL = {https://www.mdpi.com/1424-8220/22/6/2077},
PubMedID = {35336248},
ISSN = {1424-8220},
ABSTRACT = {An increasing number of vehicles on the roads increases the risk of accidents. In bad weather (e.g., heavy rainfall, strong winds, storms, and fog), this risk almost doubles due to bad visibility as well as road conditions. If an accident happens, especially in bad weather, it is important to inform approaching vehicles about it. Otherwise, there might be another accident, i.e., a multiple-vehicle collision (MVC). If the Emergency Operations Center (EOC) is not informed in a timely fashion about the incident, fatalities might increase because they do not receive immediate first aid. Detecting humans or animals would undoubtedly provide us with a better answer for reducing human fatalities in traffic accidents. In this research, an accident alert light and sound (AALS) system is proposed for auto accident detection and alerts with all types of vehicles. No changes are required in non-equipped vehicles (nEVs) and EVs because the system is installed on the roadside. The idea behind this research is to make smart roads (SRs) instead of equipping each vehicle with a separate system. Wireless communication is needed only when an accident is detected. This study is based on different sensors that are used to build SRs to detect accidents. Pre-saved locations are used to reduce the time needed to find the accident&rsquo;s location without the help of a global positioning system (GPS). Additionally, the proposed framework for the AALS also reduces the risk of MVCs.},
DOI = {10.3390/s22062077}
}
@article{liu2019exploring,
  title={Exploring factors affecting the severity of night-time vehicle accidents under low illumination conditions},
  author={Liu, Jing and Li, Jingyu and Wang, Kun and Zhao, Jianyou and Cong, Haozhe and He, Ping},
  journal={Advances in Mechanical Engineering},
  volume={11},
  number={4},
  pages={1687814019840940},
  year={2019},
  publisher={SAGE Publications Sage UK: London, England}
}
@INPROCEEDINGS{Syedul,
  author={Syedul Amin, Md. and Jalil, Jubayer and Reaz, M. B. I.},
  booktitle={2012 International Conference on Informatics, Electronics & Vision (ICIEV)}, 
  title={Accident detection and reporting system using GPS, GPRS and GSM technology}, 
  year={2012},
  volume={},
  number={},
  pages={640-643},
  doi={10.1109/ICIEV.2012.6317382}}

  @article{white2011wreckwatch,
  title={Wreckwatch: Automatic traffic accident detection and notification with smartphones},
  author={White, Jules and Thompson, Chris and Turner, Hamilton and Dougherty, Brian and Schmidt, Douglas C},
  journal={Mobile Networks and Applications},
  volume={16},
  pages={285--303},
  year={2011},
  publisher={Springer}
}
@article{zadobrischi2022intelligent,
  title={Intelligent traffic monitoring through heterogeneous and autonomous networks dedicated to traffic automation},
  author={Zadobrischi, Eduard},
  journal={Sensors},
  volume={22},
  number={20},
  pages={7861},
  year={2022},
  publisher={MDPI}
}
@inproceedings{Heinonen2007PedestrianIA,
  title={Pedestrian Injuries and Fatalities},
  author={Justin A. Heinonen and John E. Eck},
  year={2007}
}
@article{tyndall2021pedestrian,
  title={Pedestrian deaths and large vehicles},
  author={Tyndall, Justin},
  journal={Economics of Transportation},
  volume={26},
  pages={100219},
  year={2021},
  publisher={Elsevier}
}
@inproceedings{Hs2007AnalysesOR,
  title={Analyses of Rear-End Crashes and Near-Crashes in the 100-Car Naturalistic Driving Study to Support Rear-Signaling Countermeasure Development},
  author={Dot Hs},
  year={2007}
}
@article{eboli2020factors,
  title={Factors influencing accident severity: an analysis by road accident type},
  author={Eboli, Laura and Forciniti, Carmen and Mazzulla, Gabriella},
  journal={Transportation research procedia},
  volume={47},
  pages={449--456},
  year={2020},
  publisher={Elsevier}
}
@article{kukkala2018advanced,
  title={Advanced driver-assistance systems: A path toward autonomous vehicles},
  author={Kukkala, Vipin Kumar and Tunnell, Jordan and Pasricha, Sudeep and Bradley, Thomas},
  journal={IEEE Consumer Electronics Magazine},
  volume={7},
  number={5},
  pages={18--25},
  year={2018},
  publisher={IEEE}
}
@article{jirovsky2015classification,
  title={CLASSIFICATION OF ROAD ACCIDENTS FROM THE PERSPECTIVE OF VEHICLE SAFETY SYSTEMS},
  author={JIROVSK{\'Y}, V{\'A}CLAV},
  journal={Middle Eur. Constr. Des. Cars J. Czech Tech. Univ},
  volume={13},
  number={2},
  pages={1--9},
  year={2015}
}
@misc{mohammed2019review,
  title={A review of the traffic accidents and related practices worldwide. Open Transport. J. 13, 65--83},
  author={Mohammed, AA and Ambak, K and Mosa, AM and Syamsunur, D},
  year={2019}
}
@article{sheoreysensor,
  title={Sensor Based Front End Collision Avoidance System for Highways},
  author={Sheorey, Tanuja and Shrivas, Nikhil Vivek}
}
@article{ma2022adaptive,
  title={An Adaptive Multi-Staged Forward Collision Warning System Using a Light Gradient Boosting Machine},
  author={Ma, Jun and Li, Jiateng and Gong, Zaiyan and Huang, Hongwei},
  journal={Information},
  volume={13},
  number={10},
  pages={483},
  year={2022},
  publisher={MDPI}
}
@techreport{nolan2001frontal,
  title={Frontal offset deformable barrier crash testing and its effect on vehicle stiffness},
  author={Nolan, Joseph M and Lund, Adrian K},
  year={2001},
  institution={SAE Technical Paper}
}
@article{marchant2022determine,
  title={To determine if changing to white light street lamps improves road safety: A multilevel longitudinal analysis of road traffic collisions during the relighting of Leeds, a UK city},
  author={Marchant, Paul Richard and Norman, Paul D},
  journal={Applied Spatial Analysis and Policy},
  pages={1--26},
  year={2022},
  publisher={Springer}
}

@inproceedings{zhao2017temporal,
  title={Temporal action detection with structured segment networks},
  author={Zhao, Yue and Xiong, Yuanjun and Wang, Limin and Wu, Zhirong and Tang, Xiaoou and Lin, Dahua},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2914--2923},
  year={2017}
}
@article{yu2021deep,
  title={Deep spatio-temporal graph convolutional network for traffic accident prediction},
  author={Yu, Le and Du, Bowen and Hu, Xiao and Sun, Leilei and Han, Liangzhe and Lv, Weifeng},
  journal={Neurocomputing},
  volume={423},
  pages={135--147},
  year={2021},
  publisher={Elsevier}
}
@inproceedings{bridger2012lighting,
  title={Lighting the way to road safety: a policy blindspot?},
  author={Bridger, Godfrey and King, Bryan},
  booktitle={Australasian Road Safety Research Policing Education Conference, 2012, Wellington, New Zealand},
  year={2012}
}
@article{huang2020intelligent,
  title={Intelligent intersection: Two-stream convolutional networks for real-time near-accident detection in traffic video},
  author={Huang, Xiaohui and He, Pan and Rangarajan, Anand and Ranka, Sanjay},
  journal={ACM Transactions on Spatial Algorithms and Systems (TSAS)},
  volume={6},
  number={2},
  pages={1--28},
  year={2020},
  publisher={ACM New York, NY, USA}
}
@article{alharbi2023performance,
  title={Performance Appraisal of Urban Street-Lighting System: Drivers’ Opinion-Based Fuzzy Synthetic Evaluation},
  author={Alharbi, Fawaz and Almoshaogeh, Meshal I and Ibrahim, Anwar H and Haider, Husnain and Elmadina, Abd Elaziz M and Alfallaj, Ibrahim},
  journal={Applied Sciences},
  volume={13},
  number={5},
  pages={3333},
  year={2023},
  publisher={MDPI}
}
@article{SMAILOVIC2023281,
title = {A REVIEW OF FACTORS ASSOCIATED WITH DRIVING UNDER THE INFLUENCE OF ALCOHOL},
journal = {Transportation Research Procedia},
volume = {69},
pages = {281-288},
year = {2023},
note = {AIIT 3rd International Conference on Transport Infrastructure and Systems (TIS ROMA 2022), 15th-16th September 2022, Rome, Italy},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2023.02.173},
url = {https://www.sciencedirect.com/science/article/pii/S2352146523001837},
author = {Emir Smailović and Dalibor Pešić and Boris Antić and Nenad Marković},
keywords = {DUI, prediction, drinking alcohol, meta-analysis, driver behaviour},
abstract = {Research into factors related to drink-driving is one of the challenges the researchers face in their scientific work. Creating conditions that would serve for prediction of driving under the influence of alcohol (DUI) is one of the goals of such research studies. A meta-analysis of DUI-related factors has been performed in this study. The first stage of the analysis includes the search of the ScienceDirect database. Two phrases have been used for the search: "alcohol and crash" and "DUI and crash". The research from the mentioned journal that is included in the meta-analysis, had to satisfy the condition in which the association between one of the factors and driving under the influence has been presented. The following are the three ways in which DUI has been defined: 1) self-reported behaviour, 2) testing performed by police officers, and 3) involvement in a road accident while being under the influence of alcohol. The second stage has seen the creation of a database of the strength of association between one of the factors and DUI, taken from selected papers in the database. All the factors that could be related to alcohol have been observed and included the following: driving style, characteristics of the driver, time and place of driving, characteristics of the vehicle, etc. Following the search of more than 1,000 studies in this field, an odds ratio has been created to associate the factors with DUI. The odds ratio of association (OR) represents the strength of association between a particular factor and DUI. By applying the mentioned methodology, a conclusion on the association between the factors and DUI has been drawn and may be used for prediction and as a typical example of DUI drivers in road traffic. When it comes to driver behaviour, the most important predictors of DUI are excessive speed, driving at night-time, failing to wear a seat belt, previous DUI experience, the presence of passengers in a car. As for the location, the greatest odds ratio associated with a DUI driver concerns driving an older car on a two-lane road in the urban area. The results of this study can be used for further development of the legislation related to drink-driving and consequently for improvement of the road safety situation in a territory.}
}
@article{HighwayTrafficSafetyAdministration2016,
abstract = {Based on the Non-Traffic Surveillance (NTS) system, an estimated average of 2,449 people were killed each year in non-traffic motor vehicle crashes during the 5-year period from 2016 to 2020. About a third (35\%) of those people killed were nonoccupants such as pedestrians and bicyclists. Additionally, on average, an estimated 86,920 people were injured in these crashes each year, again of which about a third (33\%) were nonoccupants. In this 5-year period, 2020 has the lowest number of people injured in non-traffic crashes and the highest number of people killed in non-traffic crashes-a phenomenon also observed in other National Highway Traffic Safety Administration traffic crash data systems, the Fatality Analysis Reporting System (FARS) and the Crash Report Sampling System (CRSS), possibly because of the COVID-19 pandemic and its related lockdown.},
author = {{Highway Traffic Safety Administration}, National and {Department of Transportation}, Us},
file = {::},
keywords = {2016,2020,Administration,Crashes,Department,Fatality,Highway,Injury,Non-Traffic,Safety,Statistics,Surveillance,Traffic,Transportation,TransportationNational,U.S.},
title = {{Crash Stats: Non-Traffic Surveillance: Fatality and Injury Statistics in Non-Traffic Crashes, 2016 to 2020 (Revised)}},
year = {2016}
}
@article{lin2022environmental,
  title={Environmental factors associated with severe motorcycle crash injury in university neighborhoods: a multicenter study in Taiwan},
  author={Lin, Heng-Yu and Li, Jian-Sing and Pai, Chih-Wei and Chien, Wu-Chien and Huang, Wen-Cheng and Hsu, Chin-Wang and Wu, Chia-Chieh and Yu, Shih-Hsiang and Chiu, Wen-Ta and Lam, Carlos},
  journal={International journal of environmental research and public health},
  volume={19},
  number={16},
  pages={10274},
  year={2022},
  publisher={MDPI}
}
@techreport{Bicyclist,
abstract = {This safety research report updates our understanding of bicyclist safety in the United States by examining the prevalence and risk factors of bicycle crashes involving motor vehicles on US roadways and assessing the most applicable countermeasures. To conduct this research, the National Transportation Safety Board (NTSB) used the following combination of quantitative and qualitative methods: reviewing countermeasures and research literature; analyzing crash and injury data; and interviewing national, state, and local traffic safety stakeholders. The NTSB identified three bicyclist safety issue areas in this report: (1) improving roadway infrastructure for bicyclists, (2) enhancing conspicuity, and (3) mitigating head injury. Other safety issues that repeatedly emerged during stakeholder interview sessions are also discussed. As a result of this safety research report, the NTSB makes new recommendations to the Intelligent Transportation Systems Joint Program Office; the National Highway Traffic Safety Administration (NHTSA); the Federal Highway Administration; the US Consumer Product Safety Commission; the 50 states, the District of Columbia, and the Commonwealth of Puerto Rico; and the American Association of State Highway and Transportation Officials. The NTSB also reiterates 10 recommendations to NHTSA.},
file = {::},
pages = {1--73},
title = {{Bicyclist Safety on US Roadways: Crash Risks and Countermeasures National Transportation Safety Board}},
url = {https://www.ntsb.gov/safety/safety-studies/Documents/SS1901.pdf},
year = {2019}
}
