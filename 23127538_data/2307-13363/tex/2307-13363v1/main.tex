% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[]{EMNLP2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{3DRP-Net: 3D Relative Position-aware Network for 3D Visual Grounding}
% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate "row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Zehan Wang\thanks{Equal contribution} \hspace{1em} Haifeng Huang\footnotemark[1] \hspace{1em} Yang Zhao \hspace{1em} Linjun Li \hspace{1em} Xize Cheng \\ \textbf{Yichen Zhu} \hspace{1em} \textbf{Aoxiong Yin} \hspace{1em} \textbf{Zhou Zhao}\thanks{Corresponding author}\\
Zhejiang University\\
\textit{ \small \{wangzehan01, huanghaifeng, zhaozhou\}@zju.edu.cn}
}

\begin{document}
\maketitle
\begin{abstract}
3D visual grounding aims to localize the target object in a 3D point cloud by a free-form language description. Typically, the sentences describing the target object tend to provide information about its relative relation between other objects and its position within the whole scene. In this work, we propose a relation-aware one-stage framework, named \textbf{3D} \textbf{R}elative \textbf{P}osition-aware \textbf{Net}work (3DRP-Net), which can effectively capture the relative spatial relationships between objects and enhance object attributes. Specifically, 1) we propose a \textbf{3D} \textbf{R}elative \textbf{P}osition \textbf{M}ulti-head \textbf{A}ttention (3DRP-MA) module to analyze relative relations from different directions in the context of object pairs, which helps the model to focus on the specific object relations mentioned in the sentence. 2) We designed a soft-labeling strategy to alleviate the spatial ambiguity caused by redundant points, which further stabilizes and enhances the learning process through a constant and discriminative distribution. Extensive experiments conducted on three benchmarks (i.e., ScanRefer and Nr3D/Sr3D) demonstrate that our method outperforms all the state-of-the-art methods in general. The source code will be released on GitHub.
\end{abstract}

\section{Introduction}
\label{sec:intro}
Visual grounding aims to localize the desired objects based on the given natural language description. With the rapid development and wide applications of 3D vision~\cite{ xia2018gibson, savva2019habitat, zhu2020vision, wang2019reinforced} in recent years, 3D visual grounding task has received more and more attention. Compared to the well-studied 2D visual grounding~\cite{yang2019fast,kamath2021mdetr,yang2022improving,li2021referring,deng2021transvg, plummer2015flickr30k, kazemzadeh2014referitgame}, the input sparse point clouds in the 3D visual grounding task are more irregular and more complex in terms of spatial positional relationships, which makes it much more challenging to locate the target object.

In the field of 3D visual grounding, the previous methods can be mainly categorized into two groups: the two-stage approaches~\cite{chen2020scanrefer, achlioptas2020referit3d, zhao20213dvg, yuan2021instancerefer, huang2022multi, cai20223djcg, huang2021text, wang2023distilling} and the one-stage approaches~\cite{luo20223d}. The former ones follow the detection-and-rank paradigm, and thanks to the flexibility of this architecture, they mainly explore the benefits of different object relation modeling methods for discriminating the target object. The latter fuse visual-text features to predict the bounding boxes of the target objects directly, and enhance the object attribute representation by removing the unreliable proposal generation phase.

% Figure environment removed

However, these two methods still have limitations. For two-stage methods, the model performance is highly dependent on the quality of the object proposals.  However, due to the sparsity and irregularity of the input 3D point cloud, sparse proposals may leave out the target object, while dense proposals will bring redundant computational costs and make the matching stage too complicated to distinguish the target object. As for the one-stage methods, although the existing approach~\cite{luo20223d} achieves better performance, they can not capture the relative spatial relationships between objects, which makes it often fail in samples that rely on relative relation reasoning. As shown in Fig.\ref{fig:3dvg}, the majority of sentences in 3D visual grounding contain relative spatial relation descriptions. Furthermore, due to the spatial complexity of the 3D scene, there are various relative position-related descriptions from different orientations. To further illustrate that relative position is a general and fundamental issue in 3D visual grounding tasks, we analyze the frequency of relative position words in ScanRefer and Nr3D/Sr3D, and the results show that at least $90\%$ of the sentences describe the relative position of objects, and most of them contain multiple spatial relations. Detailed statistics can be found in supplementary materials.

To alleviate above problems, we propose a one-stage 3D visual grounding framework, named \textbf{3D} \textbf{R}elative \textbf{P}osition-aware \textbf{Net}work (3DRP-Net). Our 3DRP-Net combines and enhances the advantages of the two-stage approaches for relations modeling and the one-stage approaches for proposal-free detection while avoiding the shortcomings of both methods. For the relations modeling, we devise a novel \textbf{3D} \textbf{R}elative \textbf{P}osition \textbf{M}ulti-head \textbf{A}ttention (3DRP-MA) module, which can capture object relations along multiple directions and fully consider the interaction between the relative position and object pairs which is ignored in previous two-stage methods~\cite{yuan2021instancerefer, zhao20213dvg, huang2021text}.

Specifically, we first extract features from the point cloud and description, and select key points. Then, the language and visual features interacted while considering the relative relations between objects. For the relation modeling, We introduce learnable relative position encoding in different heads of the multi-head attention to capture object pair relations from different orientations. Moreover, in sentences, the relative relations between objects are usually described as \textit{"Object 1-Relation-Object 2"}, such as "tv is on the tv cabinet" and "curtain is hanging on the window" in Fig.\ref{fig:3dvg}. The relation is meaningful only in the context of object pairs, thus our relative position encoding would interact with the object pairs' feature, to better capture and focus on the mentioned relations. 

Besides, as discussed in~\cite{qi2019deep}, point clouds only capture surface of object, and the 3D object centers are likely to be far away from any point. To accurately reflect the location of objects and learn comprehensive object relation knowledge, we sample multiple key points of each object. However, redundant key points may lead to ambiguity. To achieve disambiguation while promoting a more stable and discriminative learning process, we propose a soft-labeling strategy that uses a constant and discriminative distribution as the target label instead of relying on unstable and polarized hard-label or IoU scores.

%In the training phase, our model performs dense bounding box prediction on the target object for higher accuracy. The commonly used one-hot or multi-hot labels may lead to an unstable learning phase and low discriminative ability, so we design a soft-labeling strategy to replace the hard-labeling approach, which further improves the robustness and performance of our 3DRP-Net.

Our main contributions can be summarized as follows:
\begin{itemize}
	\item[$\bullet$]We propose a novel single-stage 3D visual grounding model, called \textbf{3D} \textbf{R}elative \textbf{P}osition-aware \textbf{Net}work (3DRP-Net), which for the first time captures relative position relationships in the context of object pairs for better spatial relation reasoning.
	
	\item[$\bullet$]We design a \textbf{3D} \textbf{R}elative \textbf{P}osition \textbf{M}ulti-head \textbf{A}ttention (3DRP-MA) module for simultaneously modeling spatial relations from different orientations of 3D space. Besides, we devise a soft-labeling strategy to alleviate the ambiguity while further enhancing the discriminative ability of the optimal key point and stabilizing the learning process.
	
	\item[$\bullet$]Extensive experiments demonstrate the effectiveness of our method. Our 3DRP-Net achieves state-of-the-art performance on three mainstream benchmark datasets ScanRefer, Nr3D, and Sr3D.
\end{itemize}

% Figure environment removed


\section{Related Work}
\subsection{3D Visual Grounding}
Recent works in 3D visual grounding can be summarized in two categories: two-stage and one-stage methods. We briefly review them in the following. 

\noindent \textbf{Two-stage Methods.}  Two-stage approaches follow the detection-and-rank scheme. In the first stage, 3D object proposals are generated by a pre-trained 3D object detector~\cite{chen2020scanrefer} or with the ground truth~\cite{achlioptas2020referit3d}. In the second stage, the best matching proposals would be selected by leveraging the language description. Advanced two-stage methods achieve good performance by better modeling the relationships among objects. Referit3D~\cite{achlioptas2020referit3d} and TGNN~\cite{huang2021text} make use of the graph neural network~\cite{scarselli2008graph} to model the relationships between objects. 3DVG-Transformer~\cite{zhao20213dvg} utilize attention mechanisms~\cite{vaswani2017attention} to enable interactions between proposals, and the similarity matrix can be adjusted based on the relative Euclidean distances between each pair of proposals.

\noindent \textbf{One-stage Methods.} One-stage approaches avoid the unstable and time-consuming object proposals generation stage under the detection-and-rank paradigm. The visual features extracted by the backbone are directly and densely fused with the language features, and the fused features are leveraged to predict the bounding boxes and referring scores. 3D-SPS~\cite{luo20223d} first addresses the 3D visual grounding problem by one-stage strategy. It firstly filters out the key points of language-relevant objects and processes inter-model interaction to progressively down-sample the key points. 

Our work utilizes the advanced one-stage framework and introduces a novel relative relation module to effectively capture the intricate relations between objects, enabling our model to achieve superior performance.

\subsection{Position Encoding in Attention}
The attention mechanism is the primary component of transformer~\cite{vaswani2017attention}. Since the attention mechanism is order-independent, information about the position should be injected for each token. In general, there are two mainstream encoding methods: absolute and relative position encoding.

\noindent \textbf{Absolute Position Encoding.} The original transformer~\cite{vaswani2017attention} considers the absolute positions, and the encodings are generated based on the sinusoids of varying frequency. Recent 3D object detection studies also use absolute position encodings. In Group-free~\cite{liu2021group}, the encodings are learned by the center and size of the predicted bounding box, while the Fourier function is used in 3DETR~\cite{misra2021end}.

\noindent \textbf{Relative Position Encoding.} Recently, some advanced works in natural language processing~\cite{he2020deberta, raffel2020exploring, shaw2018self} and image understanding~\cite{liu2021swin, hu2019local, hu2018relation} generate position encoding based on the relative distance between tokens. Relative relation representations are important for tasks where the relative ordering or distance matters.

Our method extends relative position encoding to 3D Euclidean space and enhances relative relation reasoning ability in 3D visual grounding.

\section{Method}
This section introduces the proposed 3D Relative Position-aware Network (3DRP-Net) for 3D visual grounding. In Sec.\ref{sec:overview}, we present an overview of our method. In Sec.\ref{sec:3DRP-MA}, we dive into the technical details of the 3D Relative Position Multi-head Attention (3DRP-MA) module and how to comprehensively and efficiently exploit the spatial position relations in the context of object pairs. In Sec.\ref{soft-label} and Sec.\ref{loss}, we introduce our soft-labeling strategy and the training objective function of our method.

\subsection{Overview}
\label{sec:overview}
The 3D visual grounding task aims to find the object most relevant to a given textual query. So there are two inputs in the 3D visual grounding task. One is the 3D point cloud which is represented by the 3D coordinates and auxiliary features (RGB values and normal vectors in our setting) of $N$ points. Another input is a free-form natural language description with $L$ words.

The overall architecture of our 3DRP-Net is illustrated in Fig.\ref{fig:3drp-net}. \textit{Firstly}, we adopt the pre-trained PointNet++~\cite{qi2017pointnet++} to sample $S$ seed points and $K$ key points from the input 3d point cloud and extract the $C$-dimensional enriched points feature. For the language description input, by using a pre-trained language encoder~\cite{radford2021learning}, we encode the $L$-length sentences to $D$-dimensional word features. \textit{Secondly}, a stack of transformer layers are applied for multimodal fusion. The features of key points are accordingly interacted with language and seed points to group the scene and language information for detection and localization. Our new 3D relative position multi-head attention in each layer enables the model to understand vital relative relations among objects in the context of each object pair. \textit{Eventually}, we use two standard multi-layer perceptrons to regress the bounding box and predict the referring confidence score based on the feature of each key point. As shown in Fig.\ref{fig:3drp-net}, in the training phase, we generate the target labels of referring scores based on the IoUs of the predicted boxes. During inference, we only select the key point with the highest referring score to regress the target bounding box.


\subsection{3D Relative Position Multi-head Attention}
\label{sec:3DRP-MA}
%% Figure environment removed
%\label{sec:3DRP-MA} 


When describing an object in 3D space, relations between objects are essential to distinguish objects in the same class. Given the spatial complexity of 3D space and the potentially misleading similar relative positions between different object pairs, a precise and thorough comprehension of the relative position relationships is crucial for 3D visual grounding. However, existing 3D visual grounding methods fail to effectively address complex spatial reasoning challenges, thereby compromising their performance. To address this limitation, we propose a novel 3D relative position multi-head attention to model object relations in the context of corresponding object pairs within an advanced one-stage framework.

\subsubsection{Relative Position Attention}
\label{RP-attention}
Before detailing our relative position attention, we briefly review the original attention mechanism in~\cite{vaswani2017attention}. Given an input sequence $x = \{x_1, ..., x_n\}$ of $n$ elements where $x_i \in \mathbb{R}^{d_x}$, and the output sequence $z = \{z_1, ..., z_n\}$ with the same length where $z_i \in \mathbb{R}^{d_z}$. Taking single-head attention, the output can be formulated as:
\begin{equation}
	q_i = x_i W^Q, \  k_j = x_j W^K, \  v_i = x_i W^V
\end{equation}
\begin{equation}
	a_{i,j} = \frac{q_i {k_j}^T}{\sqrt{d}}, \  z_i = \sum_{j=1}^{n}\frac{exp(a_{i,j})}{\sum_{k=1}^{n} exp(a_{i,k})} v_j
\end{equation}
where $W_Q, W_K, W_V \in \mathbb{R}^{d_x\times d_z}$ represents the projection matrices, $a_{i,j}$ is the attention weight from element $i$ to $j$.

Based on the original attention mechanism, we propose a novel relative position attention that incorporates relative position encoding between elements. Since the semantic meaning of a relative relation \textit{"Object 1-Relation-Object 2"} is also highly dependent on the object pairs involved, it is essential for the position encoding to fully interact with object features in order to accurately capture the specific relative relations mentioned in the description. To this end, the attention weight $a_{i,j}$ in our proposed relative position attention is calculated as follows:
\begin{equation}
	a_{i,j} = \frac{q_i k_j^T + q_i {r^k_{p(d_{ij})}}^T + r^q_{p(d_{ji})} k_j^T}{\sqrt{3d}}
\end{equation}
where $d_{ij}$ represents the relative distance from element $i$ to element $j$, while $d_{ji}$ is the opposite. $p(d) \in [0,2k)$ is an index function that maps continuous distance to discrete value, as detailed in Eq.\ref{eq:index}. $r^k_{p(\cdot)}, r^q_{p(\cdot)} \in \mathbb{R}^{(2k+1) \times d_z}$ is the learnable relative position encoding. Considering a typical object relation expression \textit{"Object 1-Relation-Object 2"}, our attention weight can be understood as a sum of three attention scores on object pairs and relation: \textit{Object 1-to-Object 2}, \textit{Object 1-to-Relation}, and \textit{Relation-to-Object 2}.

\subsubsection{Piecewise Index Function}
\label{index-function}
The points in the 3D point cloud are unevenly distributed in a Euclidean space, and the relative distances are continuous. To enhance the relative spatial information and reduce computation costs, we propose to map the continuous 3D relative distances into discrete integers in a finite set. Inspired by~\cite{wu2021rethinking}, we use the following piecewise index function:
\begin{equation}
	\resizebox{\hsize}{!}{$
		p(d) = \begin{cases}
			[d], & |d| \leqq \alpha \\
			sign(d) \times min(k, [\alpha+\frac{ln(|d|/\alpha)}{ln(\beta/\alpha)}(k-\alpha) ]), & |d| > \alpha \\
		\end{cases}$
	}
	\label{eq:index}
\end{equation}
where $[\cdot]$ is a round operation, $sign(\cdot)$ represents the sign of a number, i.e., returning 1 for positive input, -1 for negative, and 0 for otherwise. 

Eq.\ref{eq:index} performs a fine mapping in the $\alpha$ range. The further over $\alpha$, the coarser it is, and distances beyond $\beta$ would be mapped to the same value. In the 3D understanding field, many studies~\cite{zhao2021point, misra2021end} have demonstrated that neighboring points are much more important than the further ones. Therefore, mapping from continuous space to discrete values by Eq.\ref{eq:index} would not lead to much semantic information loss while significantly reducing computational costs.

\subsubsection{Multi-head Attention for 3D Position}
\label{xyzd}
Till now, our relative position attention module can handle the interaction between object features and relative position information in continuous space. However, points in 3D space have much more complicated spatial relations than pixels in 2D images or words in 1D sentences. As shown in Table \ref{ablation2}, relying on a single relative distance metric leads to insufficient and partial capture of inter-object relations. This makes it difficult to distinguish the target object when multiple spatial relations are described in the language expression. Therefore, we attempt to capture object relations from multiple directions. Specifically, we encode the relative distances under x, y, z coordinates, and the Euclidean metric, denoted as $D_x$, $D_y$, $D_z$, and $D_e$, respectively. These four relative position metrics represent most of object relations in the language description (e.g., $D_x$ for "left, right", $D_y$ for "front, behind", $D_z$ for "top, bottom",  $D_e$ for "near, far"). Based on the architecture of multi-head attention, each relative position encoding is injected into the relative position attention module of each head. Such a 3DRP-MA allows the model to jointly attend to information from different relative relations in 3D space.


%\subsection{Dual-decoder for 3D Visual Grounding}
%\label{dual-decoder}
%With the 3DRP-MA module to capture relative relations in 3D space, we design a dual-decoder to model intra- and inter-modality context. As shown in Fig.\ref{fig:3drp-net}, the object features selected by the sampling module are copied and fed into the textual and visual decoder. 
%
%In the textual decoder, the original self-attention blocks are replaced by our 3DRP-MA, which can model the relative relations between objects while considering the language description. In the visual decoder, both the self-attention and cross-attention blocks are replaced, which can enhance the global position information of objects (e.g., "in the corner", "next to the wall") by interacting with points of the whole scene. The input features of the visual decoder of each layer, except the first layer, will be added to the input features of the textual decoder for accurate and well-considered localization. These three complementary 3DRP-MA blocks can jointly capture multi-level visual context to help the final localization.


% Figure environment removed

\subsection{Soft-labeling Strategy}
\label{soft-label}
Due to the object center are often not contained in the given point clouds, we select multiple key points for each object to better reflect its location. Therefore, as shown in Fig.\ref{fig:soft-label}, there will be lots of accurately predicted boxes achieving high Intersection over Union (IoU) of target object. Previous methods~\cite{chen2020scanrefer, zhao20213dvg, luo20223d} use one-hot or multi-hot labels to supervise the referring score. The key points whose predicted box has the top $N_s$ highest IoU are set to 1, and others are set to 0, which can encourage the model to select the most high-IoU proposals. However, the simple hard-labeling strategy results in two problems: Firstly, proposals with similar and high IoUs may be labeled differently as 1 and 0, which can cause an unstable training phase. Secondly, it becomes difficult to distinguish between optimal and sub-optimal proposals, affecting the model's ability to accurately identify the most accurate proposal.

To tackle these issues, we introduce a soft-labeling strategy to smooth the label distribution and encourage the model to effectively distinguish the optimal proposal. To be specific, the soft-labeling function can be calculated as follow:
\begin{equation}
	\hat{s}_i = exp{(-\frac{i^2}{2 \sigma^2} + 1)}
	\label{soft-equation}
\end{equation}
where $i\in\{ 0, ..., N_s\}$ represents the $i$-th highest IoU. We set $\sigma$ as $[N_s/3]$ to control the smoothness of the distributions. The target label of the keypoint whose predicted box's IoU is $i$-th highest and greater than 0.25 is set to $\hat{s}_i$, and others are set to 0.

Although this strategy is simple, its role is to do more as one stroke, and the insight it provides is non-trivial.

\textit{For discriminative ability}, the soft-labels enhance the difference between the optimal and sub-optimal proposals, which enforces the model to accurately identify the best key point for regressing detection box. In contrast, when hard-labels or IoU scores are used as the target labels, there is little difference between optimal and sub-optimal proposals from the perspective of learning objectives. 
\textit{For stability}, compared to hard-labels, our soft-labels can cover a broader range of accurate proposals with a smoother label distribution, and excluding the proposals with low IoU further stabilizes the learning process. Additionally, compared to directly using IoU scores, the constant distribution in soft-labels provides a more stable loss across different samples. For example, if we have two samples with vastly different target objects, such as a large bed and a small chair, the bed sample would have significantly more key points selected, resulting in more proposals of the target object. Using IoU scores as labels would ultimately lead to a much larger loss for the bed sample than the chair sample, which is clearly unreasonable.

\begin{table*}[t]
	\centering
	\caption{Comparisons with state-of-the-art methods on \textit{ScanRefer}. We highlight the best performance in \textbf{bold}.}
	\label{scanrefer}
	\resizebox{0.9\textwidth}{!}{
		\begin{tabular}{cc|c|cc|cc|cc}
			\toprule
			\multicolumn{2}{l|}{\multirow{2}{*}{Methods}} & \multirow{2}{*}{Extra} & \multicolumn{2}{c|}{Unique} & \multicolumn{2}{c|}{Multiple} & \multicolumn{2}{c}{Overall} \\
			\multicolumn{2}{l|}{} &  & \multicolumn{1}{l}{Acc@0.25} & \multicolumn{1}{l|}{Acc@0.5} & \multicolumn{1}{l}{Acc@0.25} & \multicolumn{1}{l|}{Acc@0.5} & \multicolumn{1}{l}{Acc@0.25} & \multicolumn{1}{l}{Acc@0.5} \\ \midrule
			\specialrule{0em}{1.5pt}{1.5pt}
			\midrule
			\multirow{7}{*}{\textit{Two-stage:}} & ScanRefer & - & 67.64 & 46.19 & 32.06 & 21.26 & 38.97 & 26.10 \\
			& TGNN & - & 68.61 & 56.80 & 29.84 & 23.18 & 37.37 & 29.70 \\
			& InstanceRefer & - & 77.45 & 66.83 & 31.27 & 24.77 & 40.23 & 32.93 \\
			& SAT & 2D assist & 73.21 & 50.83 & 37.64 & 25.16 & 44.54 & 30.14 \\
			& 3DVG-Transformer & - & 77.16 & 58.47 & 38.38 & 28.70 & 45.90 & 34.47 \\
			& MVT & - & 77.67 & 66.45 & 31.92 & 25.26 & 40.80 & 33.26 \\
			& 3DJCG & Scan2Cap & 78.75 & 61.30 & 40.13 & 30.08 & 47.62 & 36.14 \\ 
            & ViL3DRel & - & 81.58 & \textbf{68.62} &40.30& 30.71& 47.94& 37.73 \\\hline
			\specialrule{0em}{1.5pt}{1.5pt}
			\multirow{2}{*}{\textit{One-stage:}} & 3D-SPS & - & 81.63 & 64.77 & 39.48 & 29.61 & 47.65 & 36.43 \\
			& \textbf{3DRP-Net (Ours)} & - & \textbf{83.13} & 67.74 & \textbf{42.14} & \textbf{31.95} & \textbf{50.10} & \textbf{38.90}
			\\ \bottomrule
	\end{tabular}}
\end{table*}


\begin{table*}[t]
	\centering
	\renewcommand{\arraystretch}{1}
	\caption{Comparisons with state-of-the-art methods on \textit{Nr3D} and \textit{Sr3D}. We highlight the best performance in \textbf{bold}.}
	\label{referit3d}
	\resizebox{0.9\textwidth}{!}{\begin{tabular}{c|ccccc|ccccc}
		\toprule
		\multirow{2}{*}{Method} & \multicolumn{5}{c|}{Nr3D} & \multicolumn{5}{c}{Sr3D} \\ \cline{2-11} 
		\multirow{2}{*}{} & \multicolumn{5}{c|}{} & \multicolumn{5}{c}{} \\ [-2.0ex]
		& Easy & Hard & \begin{tabular}[c]{@{}c@{}}View\\ Dep\end{tabular} & \begin{tabular}[c]{@{}c@{}}View\\ Indep\end{tabular} & Overall & Easy & Hard & \begin{tabular}[c]{@{}c@{}}View\\ Dep\end{tabular} & \begin{tabular}[c]{@{}c@{}}View\\ Indep\end{tabular} & Overall \\ \midrule
		\specialrule{0em}{1.5pt}{1.5pt}
		\midrule
		ReferIt3DNet & 43.6 & 27.9 & 32.5 & 37.1 & 35.6 & 44.7 & 31.5 & 39.2 & 40.8 & 40.8 \\
		InstanceRefer & 46.0 & 31.8 & 34.5 & 41.9 & 38.8 & 51.1 & 40.5 & 45.4 & 48.1 & 48.0 \\
		3DVG-Transformer & 48.5 & 34.8 & 34.8 & 43.7 & 40.8 & 54.2 & 44.9 & 44.6 & 51.7 & 51.4 \\
		LanguageRefer & 51.0 & 36.6 & 41.7 & 45.0 & 43.9 & 58.9 & 49.3 & 49.2 & 56.3 & 56.0 \\
		SAT & 56.3 & 42.4 & 46.9 & 50.4 & 49.2 & 61.2 & 50.0 & 49.2 & 58.3 & 57.9 \\
		3D-SPS & 58.1 & 45.1 & 48.0 & 53.2 & 51.5 & 65.4 & 56.2 & 49.2 & 63.2 & 62.6 \\
		MVT & 61.3 & 49.1 & 54.3 & 55.4 & 55.1 & 66.9 & 58.8 & 58.4 & 64.7 & 64.5 \\ 
            ViL3DRel & 70.2 &57.4& 62.0& 64.5 & 64.4 & 74.9 & 67.9& 63.8& 73.2 & 72.8 \\\hline
		\specialrule{0em}{1.5pt}{1.5pt}
		\textbf{3DRP-Net(Ours)} & \textbf{71.4} & \textbf{59.7} & \textbf{64.2} & \textbf{65.2} & \textbf{65.9}  &  \textbf{75.6} & \textbf{69.5} & \textbf{65.5} & \textbf{74.9} & \textbf{74.1} \\ \bottomrule
	\end{tabular}}
\end{table*}

\subsection{Training and Inference}
\label{loss}
We apply a multi-task loss function to train our 3DRP-Net in an end-to-end manner. 

\noindent \textbf{Referring Loss.} The Referring loss $L_{ref}$ is calculated between the target labels $\hat{S}$ discussed in Sec.\ref{soft-label} and predicted referring scores $S$ of $K$ keypoints with focal loss~\cite{lin2017focal}.


\noindent \textbf{Keypoints Sampling Loss.} Following the loss used in~\cite{luo20223d}, we apply the key points sampling loss $L_{ks}$ to make sure the selected key points are relevant to any object whose category is mentioned in the description.

\noindent \textbf{Detection Loss.} To supervise the predicted bounding boxes, we use the detection loss $L_{det}$ as an auxiliary loss. Following~\cite{luo20223d}, the $L_{det}$ consists of semantic classification loss, objectness binary classification loss, center offset regression loss and bounding box regression loss.

\noindent \textbf{Language Classification Loss.} Similar to~\cite{chen2020scanrefer}, We introduce the language classification loss $L_{text}$ to enhance language encoder.

Finally, the overall loss function in the training process can be summarized as
\begin{equation}
	L=\alpha_1 L_{ref}+\alpha_2 L_{ks}+\alpha_3 L_{det}+\alpha_4 L_{text}
\end{equation}
where the balancing factors $\alpha_1$, $\alpha_2$, $\alpha_3$, $\alpha_4$ are set default as 0.05, 0.8, 5, 0.1, respectively, and the $L_{ref}$ and $L_{det}$ are applied on all decoder stages following the setting in~\cite{qi2019deep}.

\section{Experiment}
\subsection{Datasets and Metrics}
\noindent \textbf{ScanRefer.} The ScanRefer dataset~\cite{chen2020scanrefer} annotates 800 scenes with 51,583 language descriptions based on ScanNet dataset~\cite{dai2017scannet}. Following the ScanRefer benchmark, we split the train/val/test set with 36,655, 9,508, and 5,410 samples, respectively. 

%The data can be divided into "Unique" and "Multiple" categories, depending on whether the target object is the only object of its class in the scene. To comprehensively evaluate our method, the accuracy on both "Unique" and "Multiple" subsets is also reported.

\noindent \textbf{Nr3D/Sr3D.} The Nr3D and Sr3D are two sub-datasets in ReferIt3D~\cite{achlioptas2020referit3d}. They are also annotated on the indoor 3D scene dataset Scannet~\cite{dai2017scannet}. Nr3D contains  41,503 human utterances collected by ReferItGame, and Sr3D contains 83,572 synthetic descriptions generated based on a "target-spatial relationship-anchor object" template. 

%The Referit3D also has different subsets. The "Easy" and "Hard" splits depend on whether there are more than two distractors in the scene. Meanwhile, the "View-dep." and "View-indep." subsets are determined by whether the referring expression is dependent on the camera view or not.

\noindent \textbf{Evaluation Metric.} For ScanRefer~\cite{chen2020scanrefer}, following previous work, we use Acc@\emph{m}IoU as the evaluation metric, where $\emph{m} \in \{0.25, 0.5\}$. This metric represents the ratio of the predicted bounding boxes whose Intersection over Union (IoU) with the ground-truth (GT) bounding boxes is larger than \emph{m}. For Sr3D and Nr3D~\cite{achlioptas2020referit3d}, the ground truth bounding boxes are available, and the model only needs to identify the described object from all the bounding boxes. Therefore, the evaluation metric of these two datasets is accuracy, \emph{i.e.}, the percentage of the correctly selected target object.

%\noindent \textbf{Implementation Details.} We adopt the pre-trained PointNet++\cite{qi2017pointnet++} and the language encoder in \cite{radford2021learning} to extract the features from point clouds and language descriptions, respectively, while the rest of the network is trained from scratch. We set the dimension $d$ in all transformer layers to 384. The layer number of the dual-decoder is set to 5. Our model is trained in an end-to-end manner with the AdamW\cite{kingma2014adam} optimizer and a batch size of 15 for 36 epochs. The initial learning rates of all transformer layers and the rest of the model are set to $1e-4$ and $1e-3$, and we use the cosine learning rate decay strategy to schedule the learning rates. The seed point number $M$ and keypoint number $M_{0}$ are set to 1024 and 256. For the soft-labeling strategy, the label number $N_s$ and distribute threshold $\theta$ are assigned as 24 and 0.95, respectively. In the piecewise index function, we set the $\alpha : \beta : \gamma = 1:2:4$, and the $\beta$ is assigned as 20. When calculating the relative position index, the coordinates of all points are linearly scaled to $[0,100]$.

\begin{table*}[t]
	\centering
	\caption{Ablation studies on relation position encoding and different relation modeling modules. None-Rel/One-Rel/Multi-Rel represent subsets that contain zero/one/multiple relation descriptions in the original Multiple set of ScanRefer, and the relative percentage improvements compared to the different settings are marked in \textcolor{green}{green}.}
	\renewcommand{\arraystretch}{1}
	\label{ablation1}
	\resizebox{\textwidth}{!}{\begin{tabular}{ccccccccc}
		\toprule
		Row & $D_e$ & $D_{xyz}$ & \begin{tabular}[c]{@{}c@{}}Rel Module\end{tabular} & Overall & Multiple & None-Rel & One-Rel & Multi-Rel \\ \midrule
		1 & \checkmark & \checkmark & 3DVG-Transformer & 36.85 & 30.16 & 34.89\textcolor{green}{(+2.95\%)} & 32.51\textcolor{green}{(+5.51\%)} & 28.03\textcolor{green}{(+6.60\%)} \\
		2 & \checkmark & \checkmark & 3DJCG & 36.43 & 29.62 & 35.51\textcolor{green}{(+1.15\%)} & 31.87\textcolor{green}{(+7.62\%)} & 27.35\textcolor{green}{(+9.25\%)} \\ \hline
		\specialrule{0em}{1.5pt}{1.5pt}
		3 & $\times$ & $\times$ & 3DRP-MA & 32.74 & 26.39 & 34.18\textcolor{green}{(+5.09\%)} & 28.39\textcolor{green}{(+20.82\%)} & 23.94\textcolor{green}{(+24.81\%)} \\
		4 & \checkmark & $\times$ & 3DRP-MA & 36.43 & 30.26 & 35.47\textcolor{green}{(+1.27\%)} & 32.54\textcolor{green}{(+5.41\%)} & 28.10\textcolor{green}{(+6.33\%)} \\
		5 & $\times$ & \checkmark & 3DRP-MA & 37.13 & 30.56 & 35.30\textcolor{green}{(+1.76\%)} & 32.87\textcolor{green}{(+4.35\%)} & 28.46\textcolor{green}{(+4.99\%)} \\ \hline
		\specialrule{0em}{1.5pt}{1.5pt}
		6 & \checkmark & \checkmark & 3DRP-MA & \textbf{38.90} & \textbf{31.91} & \textbf{35.92} & \textbf{34.30} & \textbf{29.88} \\ \bottomrule
	\end{tabular}}
\end{table*}

\begin{table}[t]

	\centering
	\tabcolsep=1mm
	\caption{Ablation studies on 3DRP-MA in each transformer layer and pair-aware relation attention.}
	\label{ablation2}
	\resizebox{\linewidth}{!}{\begin{tabular}{ccc|ccccc}
		\toprule
		Row & O1-R & R-O2 & $SA_1$ & $CA$ & $SA_2$ & Acc@0.25 & Acc@0.5 \\ \midrule
		1 & $\times$ & \checkmark & \checkmark & \checkmark & \checkmark & 48.83 & 38.46 \\
		
		2 & \checkmark & $\times$ & \checkmark & \checkmark & \checkmark & 48.30 & 37.56 \\ \hline
		\specialrule{0em}{1.5pt}{1.5pt}
		3 & \checkmark & \checkmark & \checkmark & $\times$ & $\times$ & 46.70 & 36.10 \\
		4 & \checkmark & \checkmark & \checkmark & \checkmark & $\times$ & 48.72 & 37.59 \\
		5 & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \textbf{50.10} & \textbf{38.90} \\ \bottomrule
	\end{tabular}}
\end{table}


\subsection{Quantitative Comparison}
\label{performance}

We compare our 3DRP-Net with other state-of-the-art methods on these three 3D visual grounding benchmarks.

\noindent \textbf{ScanRefer.} Table \ref{scanrefer} shows the performance on ScanRefer. 3DRP-Net outperforms the best two-stage method by $+4.20$ at Acc@0.25 and $+4.40$ at Acc@0.5 and exceeds the best one-stage method by $+2.45$ at Acc@0.25 and $+2.47$ at Acc@0.5. Even when compared to 3DJCG, which utilizes an extra Scan2Cap~\cite{chen2021scan2cap} dataset to assist its training, our 3DRP-Net still shows superiority in all metrics. Specifically, for the "Multiple" subset, 3DRP-Net achieves $+2.66$ and $+2.34$ gains when compared with the advanced one-stage model in terms of Acc@0.25 and Acc@0.5, which validates the proposed 3DRP-MA module is powerful for modeling complex relative position relations in 3D space and significantly contributes to distinguishing the described target object from multiple interfering objects.

\noindent \textbf{Nr3D/Sr3D.} Note that the task of Nr3D/Sr3D is different from ScanRefer, which aims to identify the described target object from all the given ground-truth bounding boxes. Therefore, the soft-labeling strategy and the keypoint sampling module are removed. We only verify the effectiveness of 3DRP-MA on these two datasets. Besides, the data augmentation methods in ViL3DRel~\cite{chen2022language} are also used in our training phase for a fair comparison. The accuracy of our method, together with other state-of-the-art methods, is reported in Table \ref{referit3d}. 3DRP-Net achieves the overall accuracy of $65.9\%$ and $74.1\%$ on Nr3D and Sr3D, respectively, which outperforms all existing methods by a large margin. In the more challenging "Hard" subset, 3DRP-Net significantly improves the accuracy by $+2.3\%$ in Nr3D and $+1.6\%$ in Sr3D, again demonstrating our method is beneficial for distinguishing objects by capturing the relative spatial relations.

\subsection{Ablation Study}
We conduct ablation studies to investigate the contribution of each component. All the ablation study results are reported on the ScanRefer validation set.

\noindent \textbf{Relation Modeling Module.} We compared our proposed 3DRP-MA with the relation modules in other 3D visual grounding methods. For fair comparisons, we also introduce distances in x, y, z coordinates and Euclidean space to other relation modules. The results are provided in Table \ref{ablation1}, comparing rows 1, 2 and 6, our 3DRP-MA is far superior to the relation modules in 3DVG-Trans and 3DJCG, and the performance improvement mainly comes from the subsets that rely on relative relationship reasoning for localization, namely the "One-Rel" and "Multi-Rel" subsets.

\noindent \textbf{Relative Position Encoding.} In Sec.\ref{xyzd}, we discuss the complexity of relative relations in 3D space and propose four relative position encodings based on relative distance in x,y,z coordinates ($D_{xyz}$), and the Euclidean metric ($D_e$), respectively. From Table \ref{ablation1}, both $D_{xyz}$ and $D_e$ can bring significant improvement for subsets that require relative relation reasoning. Row 6 demonstrates that considering relative relations from multiple directions further helps capture comprehensive and sufficient object relations and distinguish the target object from multiple distractors.

\noindent \textbf{Pair-aware relation attention.} The typical description of a spatial relation can be expressed as \textit{"Object 1-Relation-Object 2"}. Our pair-aware relation attention can be considered as the sum of two scores: \textit{Object 1-to-Relation} (O1-R) and \textit{Relation-to-Object 2} (R-O2). To further verify the superiority of capturing the relation in the context of an object pair, we ablate the two scores, and the results are illustrated in Table \ref{ablation2}. From rows 1, 2 and 5, both O1-R and R-O2 terms benefit the 3D visual grounding task by capturing the relative relations, and the joint use of O1-R and R-O2 provides a more comprehensive understanding of spatial relation description and leads to the best performance.

\noindent \textbf{3DRP-MA in each layer.} We study the effect of each 3DRP-MA module in the transformer layer. $SA_{1}$, $CA$ and $SA_{2}$ respectively denote whether to replace the self-attention before interacting with seed points, the cross-attention for key points and seed points, and the self-attention before interacting with language. Row 3 to 5 in Table \ref{ablation2} add each 3DRP-MA in turns and the performance is gradually improved to 50.10\% and 38.90\%.

\noindent \textbf{Soft-labeling Strategy.} Table \ref{ablation3} presents the performance of different labeling strategies. In hard-labeling, $N_s$ represents the number of key points whose IoU is in the top $N_s$ and greater than 0.25, which are labeled as 1. In soft-labeling, $N_s$ is a hyperparameter in Eq.\ref{soft-equation}, which controls the number of soft labels. To further demonstrate that our proposed strategy improves stability and discrimination, we also use IoUs score as a label. The "original" setting directly uses IoUs as a label, while the "linear" setting stretches IoUs linearly to the range of 0 to 1 to enhance discrimination. Compared to hard-labeling and IoUs methods, our soft-labeling strategy improves discrimination and stability. Using the "original" IoUs method lacks discrimination power and stability due to the unbalanced loss on different samples. Even using linear scaling to enhance discrimination power, this instability cannot be eliminated. Our method alleviates these problems with a discriminative constant distribution and shows comprehensive superiority in Table \ref{ablation3}.

\begin{table}[t]
	\centering
	\caption{Ablation studies on the labeling strategies.}
	\label{ablation3}
	\scalebox{1}{
		\resizebox{0.8\linewidth}{!}{\begin{tabular}{cccc}
			\toprule
			Strategy & $N_s$ & Acc@0.25 & Acc@0.5 \\ \midrule
			\multirow{2}{*}{IoUs} & Original & 48.20 & 38.06 \\
			& Linear & 48.82 & 37.50 \\ \hline
			\specialrule{0em}{1.5pt}{1.5pt}
			\multirow{3}{*}{Hard} & 1 & 47.36 & 37.25 \\
			& 4 & 47.29 & 37.68 \\ 
			& 8 & 47.30 & 37.26 \\\hline
			\specialrule{0em}{1.5pt}{1.5pt}
			\multirow{3}{*}{Soft} & 12 & 49.13 & 38.46 \\
			& 24 & \textbf{50.10} & \textbf{38.90} \\
			& 36 & 49.64 & 38.55 \\ \bottomrule
		\end{tabular}}
	}
\end{table}

\section{Conclusion}
In this paper, we propose a relation-aware one-stage model for 3D visual grounding, referred to as 3D Relative Position-aware Network (3DRP-Net). 3DRP-Net contains novel 3DRP-MA modules to exploit complex 3D relative relations within point clouds. Besides, we devise a soft-labeling strategy to achieve disambiguation while promoting a stable and discriminative learning process. Comprehensive experiments reveal that our 3DRP-Net outperforms other methods.

\section{Limitations}
The datasets of 3D visual grounding task are all stem from the original ScanNet dataset which brings generalization to other scene types into question. More diverse benchmarks are important for the further development of the field of 3D visual grounding.
% Entries for the entire Anthology, followed by custom entries
\bibliography{main}
\bibliographystyle{acl_natbib}

\newpage
\appendix


% Figure environment removed


% Figure environment removed

\newpage

\section{Qualitative Analysis}
In this section, we provide some visualization results in ScanRefer~\cite{chen2020scanrefer} for qualitative analysis.

\subsection{Analysis on Success Cases}
To better understand our 3DRP-Net, we visualize some success cases and comparisons with the other one-stage method~\cite{luo20223d} in Figure ~\ref{fig:visualize_right}. From (a,b,c), both 3D-SPS~\cite{luo20223d} and our 3DRP-Net accurately locate the target object when the description does not involve too many relative position relations and there are not many interfering objects in the scene. However, as shown in (d,e,f), when the relative position relation between objects is necessary for distinguishing the target object from multiple objects of the same category, the previous one-stage method 3D-SPS is often confused by distractors. By modeling the relative position in 3D space, our 3DRP-Net is able to fully leverage the relative position descriptions in the sentence for reasoning, which bring more precise localization.

% Figure environment removed

\subsection{Analysis on Failure Cases}

To conduct a comprehensive qualitative evaluation, we further elaborate on the failure cases and discuss them in detail. These reasons for our 3DRP-Net prediction errors can be roughly summarized into three categories:

\begin{itemize}
	\item[$\bullet$] \textbf{Ambiguous annotations.} Due to the complexity and irregularity of 3D scenes, ambiguous descriptions are difficult to be completely avoided in 3D visual grounding datasets. There may be multiple objects in a scene that match the description, but only one of them is considered correct by the annotation. As shown in the cases (1,2,3) of Figure \ref{fig:visualize_wrong}, both ground-truth objects and our predicted objects semantically match the natural language descriptions, but according to the ground truth box annotations, our predictions are completely wrong.

	\item[$\bullet$] \textbf{Challenging target object.} In 3D point clouds, some objects are inherently difficult to identify because of obscured or missing surfaces. In case 4 of Figure \ref{fig:visualize_wrong}, the described target object is a \underline{\textit{\textbf{cabinet}}}, but the point cloud in the ground truth box is seriously missing, which makes it very difficult to identify the \underline{\textit{\textbf{cabinet}}} in the scene.

	\item[$\bullet$] \textbf{Challenging auxiliary objects.} 3D visual grounding task often requires the relations between the target object and auxiliary objects to assist the localization. The challenging auxiliary objects may result in an incorrect prediction. As shown in case 5 of Figure \ref{fig:visualize_wrong}, the target \underline{\textit{\textbf{table}}} is on "the left of the bed", but the left and right side of a bed are difficult to distinguish, which requires identifying the direction of the bed according to the position of pillows. This reasoning process is too complex for our model, and our prediction actually found the table on the right side of a bed. In case 6, the auxiliary object is "chair of the cubicles", which is challenging for the model to recognize.
	
\end{itemize}

% Figure environment removed

\section{Statistics of Relative Position Words}
\label{statistics}
To further illustrate that relative position relation is a general and fundamental issue in 3D visual grounding task, we count some common words representing relative spatial relations in three 3D visual grounding datasets (i.e., ScanRefer~\cite{chen2020scanrefer}, Nr3D~\cite{achlioptas2020referit3d} and Sr3D~\cite{achlioptas2020referit3d}) in Figure \ref{fig:statistics4} and \ref{fig:statistics1}. From Figure \ref{fig:statistics4}, in ScanRefer, at least 97\% descriptions contain relative position relations, and more than 63\% sentences use multiple relative position relations to indicate the target object. Besides, about 90\% sentences utilize the relative position words in Nr3D, and almost all the samples in Sr3D require relative position relations between objects for localization. As shown in Figure \ref{fig:statistics1}, in ScanRefer and Nr3D, which collected human utterances as descriptions, most of the commonly used relative position words appear in the sentences. This further demonstrates the importance of modeling relative position relations from different perspectives.

\section{Implementation Details.} We adopt the pre-trained PointNet++~\cite{qi2017pointnet++} and the language encoder in CLIP~\cite{radford2021learning} to extract the features from point clouds and language descriptions, respectively, while the rest of the network is trained from scratch. We set the dimension $d$ in all transformer layers to 384. The layer number of the transformer is set to 4. Our model is trained in an end-to-end manner with the AdamW~\cite{kingma2014adam} optimizer and a batch size of 15 for 36 epochs. The initial learning rates of all transformer layers and the rest of the model are set to $1e-4$ and $1e-3$, and we use the cosine learning rate decay strategy to schedule the learning rates. The seed point number $M$ and keypoint number $M_{0}$ are set to 1024 and 256. For the soft-labeling strategy, the label number $N_s$ is assigned as 24. In the piecewise index function, we set the $\alpha : \beta : \gamma = 1:2:4$, and the $\beta$ is assigned as 20. When calculating the relative position index, the coordinates of all points are linearly scaled to $[0,100]$. 

In the ablation study, we further divided the "Multiple" set of ScanRefer into "Non-Rel/One-Rel/Multi-Rel" subsets according to the number of relational descriptions in the sentences. Specifically, we follow the statistical method in Sec.~\ref{statistics} to count some common words representing relative spatial relations.

\section{Prior Methods for Comparison}
In order to validate the effectiveness of the proposed 3DRP-Net, Sec. \ref{performance} comprehensively compare it to many previous state-of-the-art methods: 1) ReferIt3DNet~\cite{achlioptas2020referit3d}
2) ScanRefer~\cite{chen2020scanrefer}; 3) TGNN~\cite{huang2021text}; 4) InstanceRefer~\cite{yuan2021instancerefer};
5) LanguageRefer~\cite{roh2022languagerefer}; 6) SAT~\cite{yang2021sat}; 7) 3DVG-Trans~\cite{zhao20213dvg}; 8) MVT~\cite{huang2022multi}; 9) 
3D-SPS~\cite{luo20223d}; 10) 3DJCG~\cite{cai20223djcg}; 11) ViL3DRel~\cite{chen2022language}

\end{document}
