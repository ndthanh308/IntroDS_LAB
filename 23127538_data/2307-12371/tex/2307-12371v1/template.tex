\documentclass{article}



\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}		% Can be removed after putting your text content
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}


% Packages added by myself
\usepackage{comment}
\usepackage{arydshln}
\usepackage{soul}
\usepackage{enumitem}

\title{Evaluating Emotional Nuances in Dialogue Summarization}

%\date{September 9, 1985}	% Here you can change the date presented in the paper title
\date{} 					% Or removing it

\author{ 
	{Yongxin ZHOU, Fabien RINGEVAL, and François PORTET} \\
Univ. Grenoble Alpes, CNRS, Inria, Grenoble INP, LIG, 38000 Grenoble, France \\
prenom.nom@univ-grenoble-alpes.fr \\
}

% Uncomment to override  the `A preprint' in the header
%\renewcommand{\headeright}{Technical Report}
%\renewcommand{\undertitle}{Technical Report}
%\renewcommand{\shorttitle}{\textit{arXiv} Template}
%\renewcommand{\shorttitle}{}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={Evaluating Emotional Nuances in Dialogue Summarization},
%pdfsubject={q-bio.NC, q-bio.QM},
pdfauthor={Yongxin ZHOU, Fabien RINGEVAL, and François PORTET},
pdfkeywords={Dialogue summarization, Affective computing, Evaluation},
}

\begin{document}
\maketitle

\begin{abstract}
Automatic dialogue summarization is a well-established task that aims to identify the most important content from human conversations to create a short textual summary. Despite recent progress in the field, we show that most of the research has focused on summarizing the factual information, leaving aside the affective content, which can yet convey useful information to analyse, monitor, or support human interactions.
In this paper, we propose and evaluate a set of measures $PEmo$, to quantify how much emotion is preserved in dialog summaries. Results show that, summarization models of the state-of-the-art do not preserve well the emotional content in the summaries. We also show that by reducing the training set to only emotional dialogues, the emotional content is better preserved in the generated summaries, while conserving the most salient factual information.
\end{abstract}


% keywords can be removed
\keywords{Dialogue summarization \and Affective computing \and Evaluation}

\section{Introduction}
\label{intro}
Automatic dialogue summarization has been widely studied and applied to various domains, including meeting \citep{AMICorpus, zhong-etal-2021-qmsum}, chat \citep{gliwa-etal-2019-samsum}, email thread \citep{zhang-etal-2021-emailsum}, media interview \citep{zhu-etal-2021-mediasum}, customer service \citep{feigenblat-etal-2021-tweetsumm-dialog, macary-etal-2020-allosat, favre-etal-2015-call, lin-etal-2021-csds} and medical dialogue \citep{song-etal-2020-summarizing}, etc. 
However, most research has focused on summarizing factual information, leaving aside affective content.

Affective content has been the target of a few summarization tasks such as opinion summarization \citep{wang-ling-2016-neural}. However, opinion is only a subset of affective expressions and such task mainly focuses on non-dialogue texts reviews. In dialogues, factual information and subjective content are often intertwined. While it is essential to summarize the most pertinent factual information, the subjective content can also provide valuable insights, for example, improve customer service interactions and better support patients in the context of e-healthcare. Therefore, it is important to extract and synthesize this content when summarizing dialogues, as it can offer numerous benefits. 

Even though summarizing the affective part of dialogues could be highly valuable in many applications including health care, call centers, meetings or collaborative work, it has been understudied, with only one study focusing on this topic \citep{Roman2008}. 
For instance, in collaborative interactions scenarios, participants usually take turns expressing their opinion, whether they agree or disagree with each other’s proposals, and what they think. 
In the customer service domain, call center telephone conversations are often used for monitoring purpose and to improve service quality. Many calls contain emotional information that are deemed important to report \citep{Roman2008}. 

However, it is not unusual that when summarization tasks are designed, summarization guidelines often focus on facts and goals and leave human summarizers without concrete way to deal with emotional content \citep{Roman2008}. A recent counter-example is the DialogSum dataset \citep{chen-etal-2021-dialogsum}, where annotators were required to pay extra attention to several different aspects including \textit{Emotions}. This shows that the community acknowledges more and more that it is an important piece of information. 

In the context of dialogue summarization, a pertinent question arises: how can we reliably measure the ability of generative models to capture the emotional information of the input dialogue and reflect it in the generated summaries, such as the performance score accurately reflects the desired outcome?
The current automatic evaluation methods for dialogue summarization mostly rely on $n$-gram comparisons and embedding distances between generated and reference summaries. 
Some studies proposed new metrics to evaluate faithfulness in dialogue summarization \citep{wang-etal-2022-analyzing}.
However, these metrics have been designed for factual correctness, and do not focus on evaluating the relevance of the emotional content, which yet plays a key role in many aspects of human interactions. 

In this paper we make several contributions to the field of dialogue summarization, which are outlined below:
\begin{itemize}[noitemsep]
    \item We emphasise the importance of emotions for dialogue summarization, especially in the context of customer service and health care.
    \item We propose $PEmo$, a set of measures that can evaluate how much emotion is preserved in dialogue summaries.
    \item We built several systems running sentiment analysis (SA) at the word-level on dialogues and evaluated how much emotion is preserved in summaries using  $PEmo$.
    \item We exploited the DialogSum Challenge framework to provide a reliable set of data and state-of-the-art methods for the automatic generation of summaries, and analyzed the affective content using 
    $PEmo$.
\end{itemize}

Results show that by filtering the dialogues according to emotion, we can significantly increase the preservation of both positive and negative emotions in summaries, while preserving the factual information. 

\section{Related Work}

In the context of describing the human state of mind, several terms are usually employed, such as affect, feeling, emotion, sentiment, and opinion, which are sometimes used interchangeably despite existing differences between them \citep{6797872}. 
In our study, we consider emotional nuances as sentiments expressed by interlocutors in conversations, and the term \textit{sentiment} or \textit{emotional information} is used throughout the paper.

\subsection{Sentiment in Dialogue Summarization} 

Reporting the emotional states of interlocutors in dialogue summarization is important in several cases. For instance, it can improve the customer experience by finding out whether the customer feedback is positive or negative \citep{macary-etal-2020-allosat, favre-etal-2015-call}. 
In the context of health care, it is crucial to know how patients feel during human interactions such as clinical meetings, or human-machine interactions such as digital therapies \citep{10.1007/978-3-030-80285-1_55}.

As we mentioned earlier, factual information and subjective content are often intertwined in dialogues, and while it is important to summarize the most relevant factual information, the subjective content can also provide key information. A study by \citet{Roman2008} revealed that whenever a dialogue contains an extreme emotion, this behavior is reported in human written dialogue summary. The study also shows that the emotional reporting varies considerably depending on the summarizer’s viewpoint, and that size constraints have no impact on the emotional content reported in the summaries. In addition to this empirical evidence, there are some theoretical arguments in favor of the presence of emotions in dialogue summarization. 
For instance, \citet{tuggener-etal-2021-summarizing} mapped dialogue types (categorization of dialogue types according to \citealp{Walton1995-WALCID}) to summary items,
and \textit{Emotions} was explicitly mapped and emphasized as one of the summary items along with the following dialogue types: \textit{Deliberation}, \textit{Information seeking}, and \textit{Eristics}. 

Despite theoretical and empirical support for the inclusion of emotional information in dialogue summaries, the inclusion of emotions as a summary item is not a common practice when designing dialogue datasets; only one data set -- the Call Centre Conversation Summarization (CCCS) task \citep{favre-etal-2015-call} -- exploited \textit{Emotions} as a summary item, among the data sets that have been listed in a recent survey \citep{tuggener-etal-2021-summarizing}. 
It seems thus that research on emotions in dialogue summarization suffers a lack of resources. We argue that this lack of development is mainly due to two reasons: 1) the fact that current corpora did not consider emotions in their guideline for writing reference summary; and 2) that there is, to the best of our knowledge, no automatic measure to assess the emotional aspect of a summary with respect to its original source. 

\subsection{Dialogue Summarization Corpora and Emotion}\label{subsec:ds_datasets}

Despite dialogue summarization being a well-established task, the formulation of summarization tasks has not reached a consensus in the linguistic and the Natural Language Processing (NLP) communities, which has prevented from reaching a mutually agreed-upon definition of what a dialogue summary should look like \citep{guo-etal-2022-questioning}. %, there is no standard definition of high quality summaries. %
In order to evaluate to which extent corpora used for dialogue summarization considered emotional information in their summary, we performed a short overview of the guideline of several major dialogue summarization datasets, which have been widely used in NLP research in English, Chinese and French. This is summarized in Table~\ref{tab1:DS_datasets}. For each corpus, we can see that the used summary criteria are different. For some corpora, only the data has been made available, the annotation guidelines being rarely accessible.

As it can be seen, some corpora do not reveal the criteria used for 
% , such as the AMI meeting corpus \cite{AMICorpus} 
reference summaries. However, most of the corpora disclose their objectives to write reference summaries as for the AMI meeting corpus \citep{AMICorpus}, SAMSum \citep{gliwa-etal-2019-samsum} (written online conversation), TWEETSUMM \citep{feigenblat-etal-2021-tweetsumm-dialog}, which is focused on customer service, QMSum \citep{zhong-etal-2021-qmsum}, a query-based multi-domain meeting summarization dataset,
and DialogSum \citep{chen-etal-2021-dialogsum}, which is a real-life scenario dialogue summarization dataset. We can notice that amongst all those corpora, annotators were explicitly instructed to describe important emotions related to events in the reference summary only in the DialogSum data set.

The fact that the annotation is not explicitly tasked with processing emotion does not prevent the reference summary from containing emotion, to a certain extent.
%we argue that this is only a limited amount. 
We checked this by manually analyzing the 212 annotated synopses of the 100 dialogues taken from the RATP DECODA corpus test set \citep{favre-etal-2015-call}. We found that although annotators were not explicitly instructed to indicate customer satisfaction in the synopsis, some annotators did mention customer feelings, but this only occurred in a few cases, namely 4\% of the synopses. 

\begin{table*}[h]
\centering
\small
\begin{tabular}{p{2.5cm}p{2.3cm}ccp{6.2cm}}
\hline
\textbf{Name} & \textbf{Domain} & \textbf{Language} & \textbf{Guideline} & \textbf{Reference Summaries Criteria} \\ 
 &  &  & \textbf{Available} &  \\ 
        \hline
        AMI \citep{AMICorpus} & Meeting & English & Yes & 
        Abstractive summaries should have the following structure: abstract, decisions, problems/issues, actions.
        Extractive summaries: identify extracts from the transcript which jointly convey the correct kind of information about the meeting to fit the required purpose.
        The instructions do not mention emotion. \\
        \hline
        RATP-DECODA \citep{favre-etal-2015-call} & Telephone Customer Service & French & No & We contacted the authors and obtained their summary definition, there is no mention of emotion. \\ 
        \hline
        SAMSum \citep{gliwa-etal-2019-samsum} & Chat & English & Yes & (1) Be rather short, (2) extract important pieces of information, (3) include names of interlocutors, (4) be written in the third person. The  instructions do not mention emotion.\\
        \hline
        MEDIASUM \citep{zhu-etal-2021-mediasum} & Media Interview & English & No & The reference summaries were downloaded from text descriptions of the input documents (interviews) available on the web.
        \\
        \hline
        TWEETSUMM \citep{feigenblat-etal-2021-tweetsumm-dialog} & Customer Service & English & Yes & Extractive summary: highlight the most salient sentences in the dialog. Abstractive summaries: one sentence summarizing what the customer conveyed and a second sentence summarizing what the agent responded. The instructions do not mention emotion. \\
        \hline
        QMSum \citep{zhong-etal-2021-qmsum} & Multi-domain Meeting & English & Yes & The annotation process consists of three stages: topic segmentation, query generation, and query-based summarization. The  instructions do not mention emotion.  \\
        \hline
        CSDS \citep{lin-etal-2021-csds} & Customer Service & Chinese & No & There are three different summaries for each dialogue: an overall summary and two role-oriented summaries (user and agent). Emotion is not mentioned.
        \\
        \hline
        DIALOGSUM \citep{chen-etal-2021-dialogsum} & Spoken & English & Yes & Convey the most salient information; Be brief; Preserve important named entities within the conversation; Be written from an observer perspective; Be written in formal language. Pay extra attention to the following aspects: Tense Consistency, Discourse Relation, \textbf{Emotion} and Intent Identification. \\ 
        
        \hline
\end{tabular}
\caption{Major datasets for dialogue summarization with their summaries criteria. DialogSum is the only one to include emotion in the guideline.}
\label{tab1:DS_datasets}
\end{table*}


\subsection{Evaluation of Emotion in Dialogue Summarization} 

Most evaluation of summarization tasks still relies on $n$-gram base measure such as ROUGE \citep{lin-2004-rouge}. The F1 scores of ROUGE-1, ROUGE-2 and ROUGE-L are mainly reported, which measure word overlap, bi-gram overlap and longest common sequence between generated summaries and references. 

Other measures such as BERTScore embeddings \citep{Zhang*2020BERTScore:} have also emerged to provide a more subtle evaluation of similarities by taking context and semantic proximity into account. 

Recent works \citep{huang-etal-2020-achieved, 10.1162/tacl_a_00373} have already pointed out that these metrics do not  correlate equally with all kinds of human judgments. However, we are not aware of any automatic metric measuring emotion adequacy with sources in dialogue summarization or other linguistic summarization task.

\section{Measuring the Emotional Content of Summaries}

In this section, we present our method for measuring the emotional content of summaries. 
Our hypotheses rely on the following: 
\begin{itemize}[noitemsep]
    \item It is possible to build a system that can automatically evaluate the presence of emotions in texts with sufficient reliability, whether it is measured through dimensional polarities, or categories, and at the word level or sentence level.
    \item Ideally, the summaries should contain the same proportion, or respectively the same polarity, of emotions as the input source texts. Therefore, a system summarizing a document could be evaluated by measuring how far the emotion proportion, or respectively polarity, of the generated summary is from the emotion proportion, or respectively the emotion polarity, of the input text. In case of several documents, a unique measure can be derived. 
    \item Since emotional and factual content might be interleaved, the measure might correlate with other context-based measures but cannot replace them.   
\end{itemize}


\subsection{The PEmo measure}

In order to investigate whether the emotional aspects of the input dialogues are preserved in the corresponding summaries, we assess the variability of emotions in both the input dialogue and their corresponding summaries.
Since our hypothesis is that the emotional content of the input should be preserved in the output summary to a comparable extent, we propose to compute a ratio for the input and output emotional content.

There are different resolutions in which emotions can be calculated (document, paragraph, sentences, words). As a summary can be very short we assume that the word-level is the most adequate. 

After labeling positive and negative words in each sentence using word-level Sentiment Analysis (SA) models, the number of positive and negative words and the total number of words in the input dialogue and corresponding summaries can be counted. We then calculate the proportion of emotionally charged words in the whole dialogue and in the summary, respectively. The formula used for this calculation is as follows:

\begin{equation}
PEmo = (PosN+NegN) / TotalN\label{PEmo}
\end{equation}

In eq.~\ref{PEmo}, \textit{PEmo} represents the proportion of emotionally charged words in the given texts. We use \textit{PEmoDial}, and \textit{PEmoSumm} to represent the \textit{PEmo} in the input dialogue, and reference summaries (or generated summaries), respectively. Furthermore, we can also compute PEmo$_{P}$ (resp. PEmo$_{N}$) to denote the proportion of emotionally charged positive words -- Pos$_{N}$ -- only (resp. negative words -- Neg$_{N}$ -- only) in the given texts.

\subsection{The CorrPEmo measure}

To examine whether the emotional aspects presented in the input dialogues are also present in the output summaries, we first calculate \textit{PEmoDial} and \textit{PEmoSumm} for each dialogue-summary pair. 

Then to explore the potential correlation between \textit{PEmoDial} and \textit{PEmoSumm} in various splits of dialogue summarization datasets, we compute and report Spearman’s rank correlation coefficient ($\rho$), which assesses the monotonic relationships between two variables \citep{Zar2005SpearmanRC}.

\subsection{Experimental design} %DialogSum

We intend to show the effect of the measure empirically. Hence the first step of the method is to select a corpus with reference texts containing some emotional items. Out of all the corpora mentioned in Section \ref{subsec:ds_datasets}, we selected DialogSum \citep{chen-etal-2021-dialogsum}, which is composed of social conversations that are often emotionally charged. As a reminder, annotators were explicitly instructed to describe important emotions related to events in the summary. In addition, DialogSum was used in a challenge, in which several teams participated and presented their results \citep{chen-etal-2022-dialogsum}. 

We then computed \textit{PosN} and \textit{NegN} at the word-by-word level, we used two approaches: a dictionary-based classifier and a state-of-the-art model based on BERT \citep{devlin-etal-2019-bert}. These two models were evaluated on a separate corpus and used to evaluate to which extent DialogSum contains emotions in its documents. 

Finally, using \textit{PEmo} and standard measures, we evaluated to which extent the state-of-the-art models handle emotions with DialogSum. We then proposed a method to select the training target by eliminating documents without emotional content in the input dialogue and/or summary, and trained models on this filtered data to evaluate whether emotion handling can be improved.

\section{Measuring Emotional Content of Reference Summaries}

Initially, we adopted the \textit{opinion\_lexicon} \citep{10.1145/1014052.1014073} dictionary as the simplest approach for Sentiment Analysis (SA). This dictionary consists of two lists of positive and negative words; any word that is not positive or negative is thus labeled as neutral. 
However, this dictionary-based approach has some limitations, as the polarity of some words may vary depending on their context (e.g., the word ``kind"), and such differences cannot be distinguished by this dictionary-based approach. To overcome this limitation, we next explored contextual SA at the word level and considered training a SA model for this purpose.\footnote{In NLP, a common practice is to use a tokenizer to segment text into smaller units called tokens. These tokens can be words, characters or subwords. Since \textit{token} is a specific technical term, we use it to refer to \textit{word} when addressing SA models.}

\subsection{Training Word-level Sentiment Analysis Models}

\begin{table*}[htbp]
\centering
\small
\begin{tabular}{lllll}
\hline
 & overall\_accuracy & precision & recall & f1 \\
\hline
token-dict.   & 88.82            & 73.61          & 60.96        & 65.64    \\
\hline
BERT-SST3    & 97.87 ($\pm$0.06)           & 94.43 ($\pm$0.43)           & 94.07 ($\pm$0.38)        & 94.24 ($\pm$0.15)    \\
\hline
BERT-DS-SST3 & 97.96 ($\pm$0.04)            & 94.53 ($\pm$0.17)           & 94.39 ($\pm$0.20)        & 94.46 ($\pm$0.10)    \\
\hline
\multicolumn{5}{l}{$^{\mathrm{a}}$We report macro results for precision, recall and f1.}
\end{tabular}
\caption{Performances in terms of accuracy, precision, recall, f1 (\%) on the test set of the SST-3 dataset, for different models: \textit{token-dict.}, \textit{BERT-SST3} and \textit{BERT-DS-SST3}. Statistics are given in the following format: mean (standard deviation), based on three runs.}
\label{tab:results_token_classification_sst}
\end{table*}


\subsubsection{Corpus: Stanford Sentiment Treebank (SST)} \label{sst_corpus}

The SST dataset~\citep{socher-etal-2013-recursive} is the first corpus that provides fully labeled parse trees, enabling a complete analysis of the compositional effects of sentiment in language. 
This dataset has been extensively studied for binary single sentence sentiment classification (positive/negative) and fine-grained sentiment classification (five classes). 
Given the complete parse tree annotations, it presents an opportunity to adapt it for word level SA. To the best of our knowledge, this is the only dataset available for word level sentiment classification.

The SST dataset includes fine-grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences. 
It is partitioned with 8544 training examples, 1101 validation examples and 2210 test examples.

In the following, we only focus on studying word level polarity. Hence, we pre-processed the original SST that was annotated with a 5-point Likert scale (``very negative," ``negative," ``neutral," ``positive," and ``very positive") into a 3-point Likert scale by simply merging "very negative" into "negative" and "very positive" into "positive"; this will be referred to as \textbf{SST3} in this paper. 

\subsubsection{Word-level Sentiment Analysis Models}

We built three specific models to perform word-level SA.

\paragraph{token-dict.}

The dictionary-based classifier based on \textit{opinion\_lexicon} \citep{10.1145/1014052.1014073}, it relies on a list of positive and negative opinion or sentiment words in English (about 6,800 words).

\paragraph{BERT-SST3}

For the second model, we fine-tuned BERT \citep{devlin-etal-2019-bert} 
using the preprocessed SST3 dataset, with a learning rate of $5\mathrm{e}{-05}$ for 3 epochs. We choose the model with the lowest validation loss to report results on the test set and for further use.

\paragraph{BERT-DS-SST3}

As the SST dataset is composed of movie reviews and is not specific to conversational setting, we used domain adaptation to familiarize our model with dialogue-specific characteristics. To do so, we automatically annotated the DialogSum training partition using the \textit{token-dict.} model.
We then fine-tuned BERT on this annotated corpus with a learning rate of $2\mathrm{e}{-05}$ for 5 epochs, selecting the model with the lowest validation loss.

Next, we further fine-tuned the selected model on the training set of SST3, using a learning rate of $5\mathrm{e}{-05}$ for 3 epochs. We select the model with the lowest validation loss to evaluate on the test set and for future use. Both the training and prediction were performed on a NVIDIA Quadro RTX 6000 GPU.

\subsubsection{Word-level Sentiment Analysis Models' Performance}

Table~\ref{tab:results_token_classification_sst} presents the results of the models on the SST3 test set.
We evaluate token classification performance using common metrics such as token-level precision, recall, and F1 score. Due to an imbalance in the number of neutral token labels in preprocessed SST3 dataset, we report ``macro" results for these metrics when evaluating hypotheses.

The lexicon-based dictionary (\textit{token-dict.}) shows poor performance on the SST token classification task, particularly in terms of precision, recall, and F1 scores. While \textit{BERT-DS-SST3} performs similarly to \textit{BERT-SST3}. It seems that domain adaptation has not decreased the performance on SST3 and, on the contrary, has stabilized its performance with a lower standard deviation than that of BERT-SST3.

\subsection{Emotional Representation of DialogSum}

In what follows, we focus on comparing and evaluating the emotional representation in input dialogues and reference summaries from the DialogSum dataset. We employ \textit{BERT-DS-SST3} to calculate the \textit{PEmo} of each dialogue and its corresponding reference summary. We begin by presenting the distribution of \textit{PEmoDial} and \textit{PEmoSumm} for the training and validation sets. In the test set, each dialogue has three reference summaries, and we analyze the correlation of \textit{PEmoSumm} against \textit{PEmoDial} for these three summaries.  

\subsubsection{Training and Validation Sets}

Box plots of the distributions of \textit{PEmoDial} versus \textit{PEmoSumm} for the DialogSum training and validation sets are shown in Fig.~\ref{fig:PEmo_BERT_DS_SST3}. The figure includes two versions of the distributions: \textit{Full}, which comprises all samples, and \textit{Filtered}, which removes samples with \textit{PEmoDial} and/or \textit{PEmoSumm} values equal to zero (cf. Table~\ref{tab: statistic_DialogSum} for the statistics). We provide the \textit{Filtered} version to address the potential impact of zero values on reported results. 

Considering all samples, the median of \textit{PEmoSumm} is lower than that of \textit{PEmoDial}, indicating that there may be an under-representation of affective states in the reference summaries of the train and dev partitions, even though emotions are indicated in the corpus annotation guidelines. For the \textit{Filtered} distribution on the training and validation sets, the median of \textit{PEmoSumm} is similar to that of \textit{PEmoDial}. However, for both versions, the distributions outside of the quartiles of the box plots are more varied for \textit{PEmoSumm} than for \textit{PEmoDial}.

% Figure environment removed

\subsubsection{Test set}

For each dialogue in the test set, there are three summaries written and checked by different annotators. When there are multiple annotations for the same sample, inter-annotator agreement is often analyzed for quality control. 
The three summaries were compared and their pair-wise ROUGE scores were therefore calculated in \citet{chen-etal-2022-dialogsum}.

Removing the samples for which \textit{PEmoDial} is zero, the Spearman correlation of \textit{PEmoDial} versus \textit{PEmoSumm} among the different reference summaries is 0.28, 0.23 and 0.21, respectively. For all these Spearman’s rank correlation coefficient ($\rho$), the corresponding $p<.001$, suggesting that annotators' use of emotional charged words may differ from each other.

\section{Assessing Emotion Handling of Summarization Models}
\subsection{Filtering Methodology}

In order to investigate the emotion handling by the state-of-the-art models and to carefully select the training target by eliminating the pairs without emotional content in the input dialogue or the summary, we filtered the DialogSum dataset using the word-level SA method mentioned earlier: 
\textit{BERT-DS-SST3}. 

Detailed statistics for the DialogSum dataset are provided in Table~\ref{tab: statistic_DialogSum}, and \textit{Full set} represents its raw statistics. According to \citet{chen-etal-2021-dialogsum}, the final DialogSum dataset contains 13,460 dialogues, which are divided into training (12,460), validation (500) and test (500) sets. 

\begin{table*}
\centering
\small
\begin{tabular}{lllll}
\hline
Part.  &  \textbf{DialogSum} & \textbf{Filtered} by & \multicolumn{2}{c}{\bf With zero value}     \\
           &                     & \textbf{BERT-DS-SST3} & Dial. & Sum. \\
\hline
train & 12460 & 9687 (77.5\%) & 43 & 2757  \\ 
dev & 500 & 391 (78.2\%)  & 2 & 108 \\
test & 500 & 499 (99.8\%)  & 1 & 0\\
\hline
\end{tabular}
\caption{\label{tab: statistic_DialogSum}  Statistics for DialogSum dataset and the \textit{Filtered} version where documents without emotion were filtered out. For the Filtered corpus both input document and reference summaries without emotional content according to the BERT-DS-SST3 model were removed. However, for the test partition only the input documents were filtered out. Percentage of data kept is shown in parentheses (\%).}
\end{table*}

\subsection{Experimental Setup}

Following the state-of-the-art models \citep{chen-etal-2021-dialogsum},
we fine-tuned the BART-Large model \citep{lewis-etal-2020-bart} on the full set of DialogSum and on the filtered datasets. 
We also trained the model on a corpus of the same size as the filtered dataset, but whose instances were randomly sampled from the full DialogSum dataset.
The hyperparameters setting was 
learning rate of $5\mathrm{e}{-05}$ for 15 epochs. Experiments were performed on the NVIDIA Quadro RTX 6000 GPU and took about 2.5 hours for each run.

\subsection{Evaluation Metrics}

In addition to ROUGE \citep{lin-2004-rouge} and BERTScore \citep{Zhang*2020BERTScore:}\footnote{Following \citet{chen-etal-2022-dialogsum}, we use RoBERTa \citep{Liu2019RoBERTaAR} large as the backbone to compute BERTScore and the precision scores are reported.}, we propose a new set of measures to assess the relevance of a summary with respect to the emotional charge (proportion - PEmo), and its polarity (PEmo$_{P}$ / PEmo$_{N}$). 
The evaluation methods from emotional perspectives are as follows:

\textbf{CorrPEmo} is our proposed \textit{PEmo} evaluation method from the proportional perspective. To examine whether the emotional aspects presented in the input dialogues are also present in the output summaries, we compute the correlation \textit{PEmoDial} vs. \textit{PEmoSumm} and propose it as a new metric. 
We use \textit{BERT-DS-SST3} as the backbone to compute them and report Spearman’s rank correlation coefficient ($\rho$). 
We removed all samples whose \textit{PEmoDial} values are equal to zero.  

\textbf{CorrPEmo$_{P}$} and \textbf{CorrPEmo$_{N}$} are proposed from the polarity perspective. We examine whether the positive (resp. negative) emotional aspects presented in the input dialogues are also present in the output summaries. 

For the above measures from emotional perspectives, samples with zero \textit{PEmoDial} (or \textit{PEmo$_{P}$Dial} / \textit{PEmo$_{N}$Dial}) values are removed, to account for the potential impact of zero values on the reported correlation. 

\subsection{Qualitative Results}
% {\color[HTML]{FF0000} -}
\begin{table*}[htbp]
\centering
\small
% \scriptsize	
\begin{tabular}{p{4cm}|cccc|ccc}
\hline
\textbf{Model} & \textbf{R1} & \textbf{R2} & \textbf{RL} & \textbf{BERTScore}  & \textbf{CorrPEmo} & \textbf{CorrPEmo$_P$}  & \textbf{CorrPEmo$_N$}\\
test set & \multicolumn{4}{c|}{\emph{Full DialogSum test set}} & \multicolumn{3}{c}{\emph{Filtered test set}}\\
\# samples & \multicolumn{4}{c|}{500} & 499 & 491 & 419\\

\hline
\textit{State-of-the-art models} & \multicolumn{7}{c}{}\\
\hline
\textbf{BART$_{Large}$} \citep{chen-etal-2021-dialogsum} & 47.28  & 21.18  & 44.83   & - &  -   & - & - \\
\hline
\textbf{GoodBai} \citep{chen-etal-2022-dialogsum} & {\bf 47.61} & {\bf 21.66} & 45.48  & {\bf 92.72} & 0.36  & 0.34  & 0.40 \\
\textbf{UoT} \citep{lundberg-etal-2022-dialogue} & 47.29 & 21.65   & {\bf 45.92}  & 92.26 &    0.36     & 0.36  & 0.38 \\
\textbf{IITP-CUNI} \citep{bhattacharjee-etal-2022-multi} & 47.26  & 21.18  & 45.17   & 92.70 &  0.35  & 0.31 & 0.40 \\
\textbf{TCS\_WITM} \citep{chauhan-etal-2022-tcs} & 47.02  & 21.20   & 44.90  & 90.13  & 0.36 &  {\bf 0.37} & 0.43 \\ %0.38/0.37/0.35 &  0.39/0.38/0.38 & 0.48/0.51/0.50
\hdashline
\textbf{Human} \citep{chen-etal-2022-dialogsum} & 53.35  & 26.72 & 50.84  & 92.63  & - &  -   & - \\
\hline
\multicolumn{8}{l}{\textit{Our Results$^{\mathrm{c}}$}} \\
\hline
\textbf{baseline (BART$_{Large}$})   & 47.36      & 21.23   & 44.88   &  91.42    &  0.35 &   0.32    & 0.40 \\
 \textbf{baseline\_sub-sampled}  & 46.94   & 20.52  & 44.43     & 91.29 &  0.35 & 0.35 & 0.41 \\
\textbf{baseline\_Filtered}  & 45.78 & 19.69  & 43.21  & 90.83 &   \textbf{0.44} & {\bf 0.37} & {\bf 0.45} \\
\hline
\end{tabular}
\caption{\label{tab: dialogsum_results} Results of the different teams of the DialogSum challenge \cite{chen-etal-2022-dialogsum} compared to our baseline BART-Large model fine-tuned on the full (\textbf{baseline (BART$_{Large}$})) and filtered corpus (\textbf{baseline\_Filtered}) of the DialogSum dataset. The \textbf{baseline\_sub-sampled} model has been trained on a corpus of the same size as the Filtered dataset but whose instances have been randomly sampled from the the full DialogSum dataset. The \textit{CorrPEmo} results of state-of-the-art models are obtained by evaluating the output files provided by the participants of the challenge. }
\end{table*}

The results of the fine-tuned BART-Large model on different versions of the DialogSum dataset are presented in Table~\ref{tab: dialogsum_results}. We compare our results with previous results reported in DialogSum dataset paper \citep{chen-etal-2021-dialogsum}, and with results of several teams in the challenge \citep{chen-etal-2022-dialogsum, lundberg-etal-2022-dialogue, bhattacharjee-etal-2022-multi, chauhan-etal-2022-tcs}.
The predictions of the different systems were obtained from the corresponding authors, respectively. \textit{Human} results are those computed by \cite{chen-etal-2022-dialogsum} obtained by averaging each human annotator scores against others. 

As reported in the results table, the GoodBai model \citep{chen-etal-2022-dialogsum} provides the highest ROUGE and BERT scores very close to the other teams and slightly better than the BART$_{Large}$ model (1st line) provided as reference to the challenge. Our baseline model \textbf{baseline (BART$_{Large}$}) shows similar performances as the BART$_{Large}$ model (1st line) uses as reference in the challenge. When trained on the Filtered dataset, the \textbf{baseline\_Filtered} model exhibits a decrease of almost 1.5 points on all the ROUGE and BERT scores. However, when looking at the \textit{CorrPEmo} measures, the \textbf{baseline\_Filtered} model provides the best correlation of emotional content between dialogues and summaries (0.44) far from the state-of-the-art models (0.36 at most).

Looking at CorrPEmo$_P$ and CorrPEmo$_N$,  the \textbf{baseline\_Filtered} model also provides the best correlation of emotional content in terms of polarity between dialogues and summaries (0.37 and 0.45), while the difference on CorrPEmo$_P$ is slight and TCS\_WITM \citep{chauhan-etal-2022-tcs} also reached 0.37.

The \textbf{baseline\_sub-sampled} showed a decrease in ROUGE and BERTScore compared to the \textbf{baseline (BART$_{Large}$}) with a reduced number of training samples, while the \textit{CorrPEmo} measures were similar. 
Results show that by filtering the dialogues according to emotion (\textbf{baseline\_Filtered}), we can significantly increase the preservation of both positive and negative emotions in summaries, while preserving the factual information.

\section{Conclusion}

In dialogue summarization, the most important content almost always focuses on factual information, leaving aside the affective content of the interaction. We argue that emotional information is important content to report in dialogue summaries. In order to measure emotional content in dialogues and in summaries, we train SA models at the word level. We conduct a corpus-based analysis on the DialogSum corpus, in which dataset annotators were explicitly instructed to include emotion when writing reference summaries, we show that emotion is omitted to some extent in reference summaries in dialogue datasets. 

We then propose a new set of measures to evaluate the relevance of a summary based on the emotional load (proportion) and its polarity (PEmo$_{P}$ / PEmo$_{N}$).
Using this measure, we show that the summarization model often exhibits a mismatch between the emotional content of the input dialogue and the summary. We also show that by carefully selecting the training target, we can decrease this mismatch. This method provides a more comprehensive measure of dialogue summarization performance.


In this study, we chose the DialogSum corpus for analysis because it explicitly considers emotions in its annotation guidelines. In the future, we will extend our method and conduct large-scale analyses on various dialogue summarization datasets.
In addition, we plan to use LLMs such as GPT-3, controlling the prompt with and without instructions on emotional aspects, to validate the effectiveness of the proposed metric.

\section*{Limitations}

Our measure is still gross and focuses on perspectives of proportion and polarity. It does not distinguish, for example, whether we are reporting anger or sadness with the same distribution as in the dialogue. For this reason, we will look at other measures that might account for this.
Furthermore, the method currently only works for English.

\section*{Ethics Statement}
The DialogSum corpus we used in this study is composed of resources freely available online without copyright constraint for academic use. According to the authors, the annotators had degrees in English Linguistics or Applied Linguistics. They received a salary of around
9.5 dollars per hour and took this annotation as a part-time job. We chose this corpus because it is the only dialogue summarization corpus we found that mentions emotions in its annotation guidelines, but we also acknowledge that the corpus consisting of social conversations may differ from other kinds of  conversations such as in the medical domain or customer service that may have specific characteristics.

We should also emphasize that the $PEmo$ measure depends on a word-level sentiment analysis models which might not be available or biased if trained on a dataset different from the one $PEmo$ is applied to. 
While our experiments focused on increasing the similarity of proportion of emotion in the input and output texts, we did not perform a human evaluation of the outputs that might have provided more fine grained analyses. The standard automatic measures suggest that the summaries generated by the different models are similar but we recognize that the model learned on the filtered corpus might generate degraded outputs. Furthermore, the focus of the measure on the polarity is a crude evaluation of emotion that cannot account for subtle difference between the input text and the generated summaries as most of the other automatic measures.

\bibliographystyle{unsrtnat}
\bibliography{references, anthology}  %%% Uncomment this line and comment out the ``thebibliography'' section below to use the external .bib file (using bibtex) .


%%% Uncomment this section and comment out the \bibliography{references} line above to use inline references.
% \begin{thebibliography}{1}

% 	\bibitem{kour2014real}
% 	George Kour and Raid Saabne.
% 	\newblock Real-time segmentation of on-line handwritten arabic script.
% 	\newblock In {\em Frontiers in Handwriting Recognition (ICFHR), 2014 14th
% 			International Conference on}, pages 417--422. IEEE, 2014.

% 	\bibitem{kour2014fast}
% 	George Kour and Raid Saabne.
% 	\newblock Fast classification of handwritten on-line arabic characters.
% 	\newblock In {\em Soft Computing and Pattern Recognition (SoCPaR), 2014 6th
% 			International Conference of}, pages 312--318. IEEE, 2014.

% 	\bibitem{hadash2018estimate}
% 	Guy Hadash, Einat Kermany, Boaz Carmeli, Ofer Lavi, George Kour, and Alon
% 	Jacovi.
% 	\newblock Estimate and replace: A novel approach to integrating deep neural
% 	networks with existing applications.
% 	\newblock {\em arXiv preprint arXiv:1804.09028}, 2018.

% \end{thebibliography}


\end{document}
