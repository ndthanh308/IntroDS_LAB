
IEEEfull.bib
V1.12 (2007/01/11)
Copyright (c) 2002-2007 by Michael Shell
See: http://www.michaelshell.org/
for current contact information.

BibTeX bibliography string definitions of the FULL titles of
IEEE journals and magazines and online publications.

This file is designed for bibliography styles that require 
full-length titles and is not for use in bibliographies that
abbreviate titles.

Support sites:
http://www.michaelshell.org/tex/ieeetran/
http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
and/or
http://www.ieee.org/

Special thanks to Laura Hyslop and ken Rawson of IEEE for their help
in obtaining the information needed to compile this file. Also,
Volker Kuhlmann and Moritz Borgmann kindly provided some corrections
and additions.

*************************************************************************
Legal Notice:
This code is offered as-is without any warranty either expressed or
mosnetimplied; without even the implied warranty of MERCHANTABILITY or
FITNESS FOR A PARTICULAR PURPOSE! 
User assumes all risk.
In no event shall IEEE or any contributor to this code be liable for
any damages or losses, including, but not limited to, incidental,
consequential, or any other damages, resulting from the use or misuse
of any information contained here.

All comments are the opinions of their respective authors and are not
necessarily endorsed by the IEEE.

This work is distributed under the LaTeX Project Public License (LPPL)
( http://www.latex-project.org/ ) version 1.3, and may be freely used,
distributed and modified. A copy of the LPPL, version 1.3, is included
in the base LaTeX documentation of all distributions of LaTeX released
2003/12/01 or later.
Retain all contribution notices and credits.
** Modified files should be clearly indicated as such, including  **
** renaming them and changing author support contact information. **

File list of work: IEEEabrv.bib, IEEEfull.bib, IEEEexample.bib,
                   IEEEtran.bst, IEEEtranS.bst, IEEEtranSA.bst,
                   IEEEtranN.bst, IEEEtranSN.bst, IEEEtran_bst_HOWTO.pdf
*************************************************************************


USAGE:

\bibliographystyle{mybstfile}
\bibliography{IEEEfull,mybibfile}

where the IEEE titles in the .bib database entries use the strings
defined here. e.g.,


   journal = IEEE_J_AC,


to yield "{IEEE} Transactions on Automatic Control"


WARNING: IEEE uses abbreviated journal titles in their bibliographies!
Because this file provides the full titles, you should NOT use this file
for work that is to be submitted to the IEEE.

For IEEE work, you should use the abbreviated titles provided in the
companion file, IEEEabrv.bib.


** NOTES **

 1. Journals have been grouped according to subject in order to make it
    easier to locate and extract the definitions for related journals - 
    as most works use references that are confined to a single topic.
    Magazines are listed in straight alphabetical order.

 2. String names are closely based on IEEE's own internal acronyms.

 3. Older, out-of-print IEEE titles are included (but not including titles
    dating prior to IEEE's formation from the IRE and AIEE in 1963).






IEEE Journals 



aerospace and military
@STRING{IEEE_J_AES        = "{IEEE} Transactions on Aerospace and Electronic Systems"}
@STRING{IEEE_J_ANE        = "{IEEE} Transactions on Aerospace and Navigational Electronics"}
@STRING{IEEE_J_ANNE       = "{IEEE} Transactions on Aeronautical and Navigational Electronics"}
@STRING{IEEE_J_AS         = "{IEEE} Transactions on Aerospace"}
@STRING{IEEE_J_AIRE       = "{IEEE} Transactions on Airborne Electronics"}
@STRING{IEEE_J_MIL        = "{IEEE} Transactions on Military Electronics"}



autos, transportation and vehicles (non-aerospace)
@STRING{IEEE_J_ITS        = "{IEEE} Transactions on Intelligent Transportation Systems"}
@STRING{IEEE_J_VT         = "{IEEE} Transactions on Vehicular Technology"}
@STRING{IEEE_J_VC         = "{IEEE} Transactions on Vehicular Communications"}



circuits, signals, systems, audio and controls
@STRING{IEEE_J_SPL        = "{IEEE} Signal Processing Letters"}
@STRING{IEEE_J_ASSP       = "{IEEE} Transactions on Acoustics, Speech, and Signal Processing"}
@STRING{IEEE_J_AU         = "{IEEE} Transactions on Audio"}
@STRING{IEEE_J_AUEA       = "{IEEE} Transactions on Audio and Electroacoustics"}
@STRING{IEEE_J_AC         = "{IEEE} Transactions on Automatic Control"}
@STRING{IEEE_J_CAS        = "{IEEE} Transactions on Circuits and Systems"}
@STRING{IEEE_J_CASVT      = "{IEEE} Transactions on Circuits and Systems for Video Technology"}
@STRING{IEEE_J_CASI       = "{IEEE} Transactions on Circuits and Systems---Part {I}: Fundamental Theory and Applications"}
@STRING{IEEE_J_CASII      = "{IEEE} Transactions on Circuits and Systems---Part {II}: Analog and Digital Signal Processing"}
in 2004 CASI and CASII renamed part title to CASI_RP and CASII_EB, respectively.
@STRING{IEEE_J_CASI_RP    = "{IEEE} Transactions on Circuits and Systems---Part {I}: Regular Papers"}
@STRING{IEEE_J_CASII_EB   = "{IEEE} Transactions on Circuits and Systems---Part {II}: Express Briefs"}
@STRING{IEEE_J_CT         = "{IEEE} Transactions on Circuit Theory"}
@STRING{IEEE_J_CST        = "{IEEE} Transactions on Control Systems Technology"}
@STRING{IEEE_J_SP         = "{IEEE} Transactions on Signal Processing"}
@STRING{IEEE_J_SU         = "{IEEE} Transactions on Sonics and Ultrasonics"}
@STRING{IEEE_J_SAP        = "{IEEE} Transactions on Speech and Audio Processing"}
@STRING{IEEE_J_UE         = "{IEEE} Transactions on Ultrasonics Engineering"}
@STRING{IEEE_J_UFFC       = "{IEEE} Transactions on Ultrasonics, Ferroelectrics, and Frequency Control"}



communications
@STRING{IEEE_J_COML       = "{IEEE} Communications Letters"}
@STRING{IEEE_J_JSAC       = "{IEEE} Journal on Selected Areas in Communications"}
@STRING{IEEE_J_COM        = "{IEEE} Transactions on Communications"}
@STRING{IEEE_J_COMT       = "{IEEE} Transactions on Communication Technology"}
@STRING{IEEE_J_WCOM       = "{IEEE} Transactions on Wireless Communications"}



components, packaging and manufacturing
@STRING{IEEE_J_ADVP       = "{IEEE} Transactions on Advanced Packaging"}
@STRING{IEEE_J_CHMT       = "{IEEE} Transactions on Components, Hybrids and Manufacturing Technology"}
@STRING{IEEE_J_CPMTA      = "{IEEE} Transactions on Components, Packaging and Manufacturing Technology---Part {A}"}
@STRING{IEEE_J_CPMTB      = "{IEEE} Transactions on Components, Packaging and Manufacturing Technology---Part {B}: Advanced Packaging"}
@STRING{IEEE_J_CPMTC      = "{IEEE} Transactions on Components, Packaging and Manufacturing Technology---Part {C}: Manufacturing"}
@STRING{IEEE_J_CAPT       = "{IEEE} Transactions on Components and Packaging Technology"}
@STRING{IEEE_J_CAPTS      = "{IEEE} Transactions on Components and Packaging Technologies"}
@STRING{IEEE_J_CPART      = "{IEEE} Transactions on Component Parts"}
@STRING{IEEE_J_EPM        = "{IEEE} Transactions on Electronics Packaging Manufacturing"}
@STRING{IEEE_J_MFT        = "{IEEE} Transactions on Manufacturing Technology"}
@STRING{IEEE_J_PHP        = "{IEEE} Transactions on Parts, Hybrids and Packaging"}
@STRING{IEEE_J_PMP        = "{IEEE} Transactions on Parts, Materials and Packaging"}



CAD
@STRING{IEEE_J_TCAD       = "{IEEE} Journal on Technology in Computer Aided Design"}
@STRING{IEEE_J_CAD        = "{IEEE} Transactions on Computer-Aided Design of Integrated Circuits and Systems"}



coding, data, information, knowledge
@STRING{IEEE_J_IT         = "{IEEE} Transactions on Information Theory"}
@STRING{IEEE_J_KDE        = "{IEEE} Transactions on Knowledge and Data Engineering"}



computers, computation, networking and software
@STRING{IEEE_J_C          = "{IEEE} Transactions on Computers"}
@STRING{IEEE_J_CAL        = "{IEEE} Computer Architecture Letters"}
@STRING{IEEE_J_DSC        = "{IEEE} Transactions on Dependable and Secure Computing"}
@STRING{IEEE_J_ECOMP      = "{IEEE} Transactions on Electronic Computers"}
@STRING{IEEE_J_EVC        = "{IEEE} Transactions on Evolutionary Computation"}
@STRING{IEEE_J_FUZZ       = "{IEEE} Transactions on Fuzzy Systems"}
@STRING{IEEE_J_IFS        = "{IEEE} Transactions on Information Forensics and Security"}
@STRING{IEEE_J_MC         = "{IEEE} Transactions on Mobile Computing"}
@STRING{IEEE_J_NET        = "{IEEE/ACM} Transactions on Networking"}
@STRING{IEEE_J_NN         = "{IEEE} Transactions on Neural Networks"}
@STRING{IEEE_J_PDS        = "{IEEE} Transactions on Parallel and Distributed Systems"}
@STRING{IEEE_J_SE         = "{IEEE} Transactions on Software Engineering"}



computer graphics, imaging, and multimedia
@STRING{IEEE_J_JDT        = "{IEEE/OSA} Journal of Display Technology"}
@STRING{IEEE_J_IP         = "{IEEE} Transactions on Image Processing"}
@STRING{IEEE_J_MM         = "{IEEE} Transactions on Multimedia"}
@STRING{IEEE_J_VCG        = "{IEEE} Transactions on Visualization and Computer Graphics"}



cybernetics, ergonomics, robots, man-machine, and automation
@STRING{IEEE_J_ASE        = "{IEEE} Transactions on Automation Science and Engineering"}
@STRING{IEEE_J_JRA        = "{IEEE} Journal of Robotics and Automation"}
@STRING{IEEE_J_HFE        = "{IEEE} Transactions on Human Factors in Electronics"}
@STRING{IEEE_J_MMS        = "{IEEE} Transactions on Man-Machine Systems"}
@STRING{IEEE_J_PAMI       = "{IEEE} Transactions on Pattern Analysis and Machine Intelligence"}
in 1989 JRA became RA
in August 2004, RA split into ASE and RO
@STRING{IEEE_J_RA         = "{IEEE} Transactions on Robotics and Automation"}
@STRING{IEEE_J_RO         = "{IEEE} Transactions on Robotics"}
@STRING{IEEE_J_SMC        = "{IEEE} Transactions on Systems, Man, and Cybernetics"}
@STRING{IEEE_J_SMCA       = "{IEEE} Transactions on Systems, Man, and Cybernetics---Part {A}: Systems and Humans"}
@STRING{IEEE_J_SMCB       = "{IEEE} Transactions on Systems, Man, and Cybernetics---Part {B}: Cybernetics"}
@STRING{IEEE_J_SMCC       = "{IEEE} Transactions on Systems, Man, and Cybernetics---Part {C}: Applications and Reviews"}
@STRING{IEEE_J_SSC        = "{IEEE} Transactions on Systems Science and Cybernetics"}



earth, wind, fire and water
@STRING{IEEE_J_GE         = "{IEEE} Transactions on Geoscience Electronics"}
@STRING{IEEE_J_GRS        = "{IEEE} Transactions on Geoscience and Remote Sensing"}
@STRING{IEEE_J_GRSL       = "{IEEE} Geoscience and Remote Sensing Letters"}
@STRING{IEEE_J_OE         = "{IEEE} Journal of Oceanic Engineering"}



education, engineering, history, IEEE, professional
@STRING{IEEE_J_CJECE      = "Canadian Journal of Electrical and Computer Engineering"}
@STRING{IEEE_J_PROC       = "Proceedings of the {IEEE}"}
@STRING{IEEE_J_EDU        = "{IEEE} Transactions on Education"}
@STRING{IEEE_J_EM         = "{IEEE} Transactions on Engineering Management"}
@STRING{IEEE_J_EWS        = "{IEEE} Transactions on Engineering Writing and Speech"}
@STRING{IEEE_J_PC         = "{IEEE} Transactions on Professional Communication"}



electromagnetics, antennas, EMI, magnetics and microwave
@STRING{IEEE_J_AWPL       = "{IEEE} Antennas and Wireless Propagation Letters"}
@STRING{IEEE_J_MGWL       = "{IEEE} Microwave and Guided Wave Letters"}
@STRING{IEEE_J_MWCL       = "{IEEE} Microwave and Wireless Components Letters"}
@STRING{IEEE_J_AP         = "{IEEE} Transactions on Antennas and Propagation"}
@STRING{IEEE_J_EMC        = "{IEEE} Transactions on Electromagnetic Compatibility"}
@STRING{IEEE_J_MAG        = "{IEEE} Transactions on Magnetics"}
@STRING{IEEE_J_MTT        = "{IEEE} Transactions on Microwave Theory and Techniques"}
@STRING{IEEE_J_RFI        = "{IEEE} Transactions on Radio Frequency Interference"}
@STRING{IEEE_J_TJMJ       = "{IEEE} Translation Journal on Magnetics in Japan"}



energy and power
@STRING{IEEE_J_EC         = "{IEEE} Transactions on Energy Conversion"}
@STRING{IEEE_J_PEL        = "{IEEE} Power Electronics Letters"}
@STRING{IEEE_J_PWRAS      = "{IEEE} Transactions on Power Apparatus and Systems"}
@STRING{IEEE_J_PWRD       = "{IEEE} Transactions on Power Delivery"}
@STRING{IEEE_J_PWRE       = "{IEEE} Transactions on Power Electronics"}
@STRING{IEEE_J_PWRS       = "{IEEE} Transactions on Power Systems"}



industrial, commercial and consumer
@STRING{IEEE_J_APPIND     = "{IEEE} Transactions on Applications and Industry"}
@STRING{IEEE_J_BC         = "{IEEE} Transactions on Broadcasting"}
@STRING{IEEE_J_BCTV       = "{IEEE} Transactions on Broadcast and Television Receivers"}
@STRING{IEEE_J_CE         = "{IEEE} Transactions on Consumer Electronics"}
@STRING{IEEE_J_IE         = "{IEEE} Transactions on Industrial Electronics"}
@STRING{IEEE_J_IECI       = "{IEEE} Transactions on Industrial Electronics and Control Instrumentation"}
@STRING{IEEE_J_IA         = "{IEEE} Transactions on Industry Applications"}
@STRING{IEEE_J_IGA        = "{IEEE} Transactions on Industry and General Applications"}
@STRING{IEEE_J_IINF       = "{IEEE} Transactions on Industrial Informatics"}
@STRING{IEEE_J_PSE        = "{IEEE} Journal of Product Safety Engineering"}



instrumentation and measurement
@STRING{IEEE_J_IM         = "{IEEE} Transactions on Instrumentation and Measurement"}



insulation and materials
@STRING{IEEE_J_JEM        = "{IEEE/TMS} Journal of Electronic Materials"}
@STRING{IEEE_J_DEI        = "{IEEE} Transactions on Dielectrics and Electrical Insulation"}
@STRING{IEEE_J_EI         = "{IEEE} Transactions on Electrical Insulation"}



mechanical
@STRING{IEEE_J_MECH       = "{IEEE/ASME} Transactions on Mechatronics"}
@STRING{IEEE_J_MEMS       = "{IEEE/ASME} Journal of Microelectromechanical Systems"}



medical and biological
@STRING{IEEE_J_BME        = "{IEEE} Transactions on Biomedical Engineering"}
Note: The B-ME journal later dropped the hyphen and became the BME.
@STRING{IEEE_J_B-ME       = "{IEEE} Transactions on Bio-Medical Engineering"}
@STRING{IEEE_J_BMELC      = "{IEEE} Transactions on Bio-Medical Electronics"}
@STRING{IEEE_J_CBB        = "{IEEE/ACM} Transactions on Computational Biology and Bioinformatics"}
@STRING{IEEE_J_ITBM       = "{IEEE} Transactions on Information Technology in Biomedicine"}
@STRING{IEEE_J_ME         = "{IEEE} Transactions on Medical Electronics"}
@STRING{IEEE_J_MI         = "{IEEE} Transactions on Medical Imaging"}
@STRING{IEEE_J_NB         = "{IEEE} Transactions on NanoBioscience"}
@STRING{IEEE_J_NSRE       = "{IEEE} Transactions on Neural Systems and Rehabilitation Engineering"}
@STRING{IEEE_J_RE         = "{IEEE} Transactions on Rehabilitation Engineering"}



optics, lightwave and photonics
@STRING{IEEE_J_PTL        = "{IEEE} Photonics Technology Letters"}
@STRING{IEEE_J_JLT        = "{IEEE/OSA} Journal of Lightwave Technology"}



physics, electrons, nanotechnology, nuclear and quantum electronics
@STRING{IEEE_J_EDL        = "{IEEE} Electron Device Letters"}
@STRING{IEEE_J_JQE        = "{IEEE} Journal of Quantum Electronics"}
@STRING{IEEE_J_JSTQE      = "{IEEE} Journal of Selected Topics in Quantum Electronics"}
@STRING{IEEE_J_ED         = "{IEEE} Transactions on Electron Devices"}
@STRING{IEEE_J_NANO       = "{IEEE} Transactions on Nanotechnology"}
@STRING{IEEE_J_NS         = "{IEEE} Transactions on Nuclear Science"}
@STRING{IEEE_J_PS         = "{IEEE} Transactions on Plasma Science"}



reliability
@STRING{IEEE_J_DMR        = "{IEEE} Transactions on Device and Materials Reliability"}
@STRING{IEEE_J_R          = "{IEEE} Transactions on Reliability"}



semiconductors, superconductors, electrochemical and solid state
@STRING{IEEE_J_ESSL       = "{IEEE/ECS} Electrochemical and Solid-State Letters"}
@STRING{IEEE_J_JSSC       = "{IEEE} Journal of Solid-State Circuits"}
@STRING{IEEE_J_ASC        = "{IEEE} Transactions on Applied Superconductivity"}
@STRING{IEEE_J_SM         = "{IEEE} Transactions on Semiconductor Manufacturing"}



sensors
@STRING{IEEE_J_SENSOR     = "{IEEE} Sensors Journal"}



VLSI
@STRING{IEEE_J_VLSI       = "{IEEE} Transactions on Very Large Scale Integration ({VLSI}) Systems"}






IEEE Magazines



@STRING{IEEE_M_AES        = "{IEEE} Aerospace and Electronics Systems Magazine"}
@STRING{IEEE_M_HIST       = "{IEEE} Annals of the History of Computing"}
@STRING{IEEE_M_AP         = "{IEEE} Antennas and Propagation Magazine"}
@STRING{IEEE_M_ASSP       = "{IEEE} {ASSP} Magazine"}
@STRING{IEEE_M_CD         = "{IEEE} Circuits and Devices Magazine"}
@STRING{IEEE_M_CAS        = "{IEEE} Circuits and Systems Magazine"}
@STRING{IEEE_M_COM        = "{IEEE} Communications Magazine"}
@STRING{IEEE_M_COMSOC     = "{IEEE} Communications Society Magazine"}
@STRING{IEEE_M_CIM        = "{IEEE} Computational Intelligence Magazine"}
CSEM changed to CSE in 1999
@STRING{IEEE_M_CSE        = "{IEEE} Computing in Science and Engineering"}
@STRING{IEEE_M_CSEM       = "{IEEE} Computational Science and Engineering Magazine"}
@STRING{IEEE_M_C          = "{IEEE} Computer"}
@STRING{IEEE_M_CAP        = "{IEEE} Computer Applications in Power"}
@STRING{IEEE_M_CGA        = "{IEEE} Computer Graphics and Applications"}
@STRING{IEEE_M_CONC       = "{IEEE} Concurrency"}
@STRING{IEEE_M_CS         = "{IEEE} Control Systems Magazine"}
@STRING{IEEE_M_DTC        = "{IEEE} Design and Test of Computers"}
@STRING{IEEE_M_EI         = "{IEEE} Electrical Insulation Magazine"}
@STRING{IEEE_M_ETR        = "{IEEE} ElectroTechnology Review"}
@STRING{IEEE_M_EMB        = "{IEEE} Engineering in Medicine and Biology Magazine"}
@STRING{IEEE_M_EMR        = "{IEEE} Engineering Management Review"}
@STRING{IEEE_M_EXP        = "{IEEE} Expert"}
@STRING{IEEE_M_IA         = "{IEEE} Industry Applications Magazine"}
@STRING{IEEE_M_IM         = "{IEEE} Instrumentation and Measurement Magazine"}
@STRING{IEEE_M_IS         = "{IEEE} Intelligent Systems"}
@STRING{IEEE_M_IC         = "{IEEE} Internet Computing"}
@STRING{IEEE_M_ITP        = "{IEEE} {IT} Professional"}
@STRING{IEEE_M_MICRO      = "{IEEE} Micro"}
@STRING{IEEE_M_MW         = "{IEEE} Microwave Magazine"}
@STRING{IEEE_M_MM         = "{IEEE} Multimedia"}
@STRING{IEEE_M_NET        = "{IEEE} Network"}
@STRING{IEEE_M_PCOM       = "{IEEE} Personal Communications Magazine"}
@STRING{IEEE_M_POT        = "{IEEE} Potentials"}
CAP and PER merged to form PE in 2003
@STRING{IEEE_M_PE         = "{IEEE} Power and Energy Magazine"}
@STRING{IEEE_M_PER        = "{IEEE} Power Engineering Review"}
@STRING{IEEE_M_PVC        = "{IEEE} Pervasive Computing"}
@STRING{IEEE_M_RA         = "{IEEE} Robotics and Automation Magazine"}
@STRING{IEEE_M_SAP        = "{IEEE} Security and Privacy"}
@STRING{IEEE_M_SP         = "{IEEE} Signal Processing Magazine"}
@STRING{IEEE_M_S          = "{IEEE} Software"}
@STRING{IEEE_M_SPECT      = "{IEEE} Spectrum"}
@STRING{IEEE_M_TS         = "{IEEE} Technology and Society Magazine"}
@STRING{IEEE_M_VT         = "{IEEE} Vehicular Technology Magazine"}
@STRING{IEEE_M_WC         = "{IEEE} Wireless Communications Magazine"}
@STRING{IEEE_M_TODAY      = "Today's Engineer"}






IEEE Online Publications 



@STRING{IEEE_O_CSTO        = "{IEEE} Communications Surveys and Tutorials"}
@STRING{IEEE_O_DSO         = "{IEEE} Distributed Systems Online"}
@book{NG10,
author = {Naylor, Patrick A. and Gaubitch, Nikolay D.},
title = {Speech Dereverberation},
year = {2010},
isbn = {1849960550},
publisher = {Springer Publishing},
edition = {1st},
abstract = {Speech dereverberation is a signal processing technique of key importance for successful hands-free speech acquisition in applications of telecommunications and automatic speech recognition. Over the last few years, speech dereverberation has become a hot research topic driven by consumer demand, the availability of terminals based on Skype which encourage hands-free operation and the development of promising signal processing algorithms. Speech Dereverberation gathers together an overview, a mathematical formulation of the problem and the state-of-the-art solutions for dereverberation. Speech Dereverberation presents the most important current approaches to the problem of reverberation. It begins by providing a focused and digestible review of the relevant topics in room acoustics and also describes key performance measures for dereverberation. The algorithms are then explained together with relevant mathematical analysis and supporting examples that enable the reader to see the relative strengths and weaknesses of the various techniques, as well as giving a clear understanding of the open questions still to be addressed in this topic. Techniques rooted in speech enhancement are included, in addition to a substantial treatment of multichannel blind acoustic system identification and inversion. The TRINICON framework is shown in the context of dereverberation to be a powerful generalization of the signal processing for a important range of analysis and enhancement techniques. Speech Dereverberation offers the reader an overview of the subject area, as well as an in-depth text on the advanced signal processing involved. The book benefits the reader by providing such a wealth of information in one place, defines the current state of the art and, lastly, encourages further work on this topic by offering open research questions to exercise the curiosity of the reader. It is suitable for students at masters and doctoral level, as well as established researchers.}
}



@INPROCEEDINGS{ShiHain21,
  author={Shi, Yanpei and Hain, Thomas},
  booktitle={2021 IEEE Spoken Language Technology Workshop (SLT)}, 
 title={{S}upervised {S}peaker {E}mbedding {D}e-{M}ixing in {T}wo-{S}peaker {E}nvironment}, 
  year={2021},
  address   = {Online},
  volume={},
  number={},
  pages={758-765},
  doi={10.1109/SLT48900.2021.9383580}
}
@INPROCEEDINGS{7952118,  author={Luo, Yi and Chen, Zhuo and Hershey, John R. and Le Roux, Jonathan and Mesgarani, Nima},  booktitle={ICASSP 2017},   title={Deep clustering and conventional networks for music separation: Stronger together},   year={2017},  volume={},  number={},  pages={61-65},  doi={10.1109/ICASSP.2017.7952118}}
@article{Moritz2017,
title = {Multi-Channel Speech Enhancement and Amplitude Modulation Analysis for Noise Robust Automatic Speech Recognition},
journal = {Computer Speech \& Language},
volume = {46},
pages = {558-573},
year = {2017},
month={November},
issn = {0885-2308},
doi = {10.1016/j.csl.2016.11.004},
author = {N. Moritz and K. Adilo{\u{g}}lu and J. Anem{\"{u}}ller and S. Goetze and B. Kollmeier},
keywords = {Speech enhancement, Non-negative matrix factorization, Feature extraction, Modulation frequency analysis, CHiME, Amplitude modulation filter bank},
abstract = {The paper describes a system for automatic speech recognition (ASR) that is benchmarked with data of the 3rd CHiME challenge, a dataset comprising distant microphone recordings of noisy acoustic scenes in public environments. The proposed ASR system employs various methods to increase recognition accuracy and noise robustness. Two different multi-channel speech enhancement techniques are used to eliminate interfering sounds in the audio stream. One speech enhancement method aims at separating the target speaker's voice from background sources based on non-negative matrix factorization (NMF) using variational Bayesian (VB) inference to estimate NMF parameters. The second technique is based on a time-varying minimum variance distortionless response (MVDR) beamformer that uses spatial information to suppress sound signals not arriving from a desired direction. Prior to speech enhancement, a microphone channel failure detector is applied that is based on cross-comparing channels using a modulation-spectral representation of the speech signal. ASR feature extraction employs the amplitude modulation filter bank (AMFB) that implicates prior information of speech to analyze its temporal dynamics. AMFBs outperform the commonly used frame splicing technique of filter bank features in conjunction with a deep neural network (DNN) based ASR system, which denotes an equivalent data-driven approach to extract modulation-spectral information. In addition, features are speaker adapted, a recurrent neural network (RNN) is employed for language modeling, and hypotheses of different ASR systems are combined to further enhance the recognition accuracy. The proposed ASR system achieves an absolute word error rate (WER) of 5.67\% on the real evaluation test data, which is 0.16\% lower compared to the best score reported within the 3rd CHiME challenge.}
}
@inbook{Benesty_Source_Separation_2000,
author = {Benesty, Jacob},
title = {An Introduction to Blind Source Separation of Speech Signals},
year = {2000},
isbn = {0792378148},
publisher = {Kluwer Academic Publishers},
address = {USA},
booktitle = {Acoustic Signal Processing for Telecommunication},
pages = {321–330},
numpages = {10},
doi={10.1109/SLT48900.2021.9383580}
}
@inproceedings{reddy2021Interspeech,
  title={Interspeech 2021 Deep Noise Suppression Challenge},
  author={Reddy, Chandan KA and Dubey, Harishchandra and Koishida, Kazuhito and Nair, Arun and Gopal, Vishak and Cutler, Ross and Braun, Sebastian and Gamper, Hannes and Aichner, Robert and Srinivasan, Sriram},
  booktitle={Interspeech},
  address   = {Brno, Czech Republic},
  year={2021},
  month = {Sep},
}

@ARTICLE{cauchi2019QualityLSTM,
  author={Cauchi, Benjamin and Siedenburg, Kai and Santos, João F. and Falk, Tiago H. and Doclo, Simon and Goetze, Stefan},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 
  title={{Non-Intrusive Speech Quality Prediction Using Modulation Energies and LSTM-Network}}, 
  year={2019},
  volume={27},
  number={7},
  pages={1151-1163},
  doi={10.1109/TASLP.2019.2912123}}

@INPROCEEDINGS{Avila2016Quality,
  author={Avila, Anderson and Cauchi, Benjamin and Goetze, Stefan and Doclo, Simon and Falk, Tiago},
  booktitle={2016 IEEE International Workshop on Acoustic Signal Enhancement (IWAENC)}, 
  title={Performance comparison of intrusive and non-intrusive instrumental quality measures for enhanced speech}, 
  year={2016},
  pages={1-5},
  doi={10.1109/IWAENC.2016.7602907}}

@INPROCEEDINGS{Braun2021lossFunctions,
  author={Braun, Sebastian and Tashev, Ivan},
  booktitle={2021 44th International Conference on Telecommunications and Signal Processing (TSP)}, 
  title={A consolidated view of loss functions for supervised deep learning-based speech enhancement}, 
  year={2021},
  pages={72-76},
  doi={10.1109/TSP52935.2021.9522648}}


@inproceedings{cauchi2016spectrallynmf, 
author = {Cauchi, B and Gerkmann, T and Doclo, S and Naylor, PA and Goetze, S}, 
booktitle = {Proc. AES 60th Conference on Dereverberation and Reverberation of Audio, Music, and Speech}, 
title = {Spectrally and spatially informed noise suppression using beamforming and convolutive {NMF}}, 
  address   = {Leuven, Belgium},
year = {2016}, 
month = {January},
} 
@ARTICLE{Cauchi_REVERB_2015,
  author = {Cauchi, B. and Kodrasi, I. and Rehr, R. and Gerlach, S. and Jukic, A. and Gerkmann, T. and Doclo, S. and Goetze, S.},
  title = {{C}ombination of {MVDR} beamforming and single-channel spectral processing for enhancing noisy and reverberant speech},
  journal = {EURASIP Journal on Advances in Signal Processing},
  year = {2015},
  volume = {2015},
  number = {1},
  eid = {61},
  abstract = {This paper presents a system aiming at joint dereverberation and noise reduction by applying a combination of a beamformer with a single-channel spectral enhancement scheme. First, a minimum variance distortionless	response beamformer with an online estimated noise coherence matrix is used to suppress noise and reverberation. The output of this beamformer is then processed by a single-channel spectral enhancement scheme,	based on statistical room acoustics, minimum statistics, and temporal cepstrum smoothing, to suppress residual noise and reverberation. The evaluation is conducted using the REVERB challenge corpus, designed to evaluate speech enhancement algorithms in the presence of both reverberation and noise. The proposed system is evaluated using instrumental speech quality measures, the performance of an automatic speech recognition system, and a subjective evaluation of the speech quality based on a MUSHRA test. The performance achieved by beamforming, single-channel spectral enhancement, and their combination are compared, and experimental 	results show that the proposed system is effective in suppressing both reverberation and noise while improving the speech quality. The achieved improvements are particularly significant in conditions with high reverberation times.},
  doi = {10.1186/s13634-015-0242-x},
publisher = {Springer International Publishing},
}
@book{B2018,
abstract = {We present our first results in applications of recurrent neural networks to Russian. The problem of re-scoring of equiprobable hypotheses has been solved. We train several recurrent neural networks on a lemmatized news corpus to mitigate the problem of data sparseness. We also make use of morphological information to make the predictions more accurate. Finally we train the Ranking SVM model and show that combination of recurrent neural networks and morphological information gives better results than 5-gram model with Knesser-Ney discounting.},
annote = {Chapter 177},
author = {B, Ingo Siegert and B, Oliver Jokisch and Lotz, Alicia Flores and Trojahn, Franziska and Meszaros, Martin and Maruschke, Michael},
doi = {10.1007/978-3-319-99579-3},
file = {:home/will/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/B et al. - 2018 - Speech and Computer.pdf:pdf},
isbn = {978-3-319-99578-6},
keywords = {music coding,opus,spectral features,surround sound},
pages = {65--75},
publisher = {Springer International Publishing},
title = {{Speech and Computer}},
volume = {11096},
year = {2018}
}
@INPROCEEDINGS{openrir,
  author={Ko, Tom and Peddinti, Vijayaditya and Povey, Daniel and Seltzer, Michael L. and Khudanpur, Sanjeev},
  booktitle={ICASSP 2017}, 
  title={A study on data augmentation of reverberant speech for robust speech recognition}, 
  year={2017},
  volume={},
  number={},
  pages={5220-5224},
  doi={10.1109/ICASSP.2017.7953152}}
@inproceedings{WHAMR,
    title     = {{WHAMR}!: Noisy and Reverberant Single-Channel Speech Separation},
    author    = {Maciejewski, M. and Wichern, G. and Le Roux, J.},
    booktitle = {ICASSP 2020},
    year      = {2020},
    month     = {May}
}
@article{reverbtasnet,
   abstract = {We investigate the recently proposed Time-domain Audio Separation Network ({{TasNet}}) in the task of real-time single-channel speech dereverberation. Unlike systems that take time-frequency representation of the audio as input, {TasNet} learns an adaptive front-end in replacement of the time-frequency representation by a time-domain convolutional non-negative au-toencoder. We show that by formulating the dereverberation problem as a denoising problem where the direct path is separated from the reverberations, a {TasNet} denoising autoencoder can outperform a deep LSTM baseline on log-power magnitude spectrogram input in both causal and non-causal settings. We further show that adjusting the stride size in the convolutional autoencoder helps both the dereverberation and separation performance .},
   author = {Yi Luo and Nima Mesgarani},
   doi = {10.21437/Interspeech.2018-2290},
   keywords = {Index Terms: speech dereverberation,deep learning,speech separation,time-domain},
   title = {Real-time Single-channel Dereverberation and Separation with Time-domain Audio Separation Network},
   journal={Interspeech 2018},
   year={2018}
}

@inproceedings{MOSnet,
  author={Chen-Chou Lo and Szu-Wei Fu and Wen-Chin Huang and Xin Wang and Junichi Yamagishi and Yu Tsao and Hsin-Min Wang},
  title={{MOSNet: Deep Learning-Based Objective Assessment for Voice Conversion}},
  year=2019,
  booktitle={Proc. Interspeech 2019},
  pages={1541--1545},
  doi={10.21437/Interspeech.2019-2003},
}
@INPROCEEDINGS{close2023perceive,
  author={Close, George and Ravenscroft, William and Hain, Thomas and Goetze, Stefan},
  booktitle={ICASSP 2023}, 
  title={Perceive and Predict: Self-Supervised Speech Representation Based Loss Functions for Speech Enhancement}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  doi={10.1109/ICASSP49357.2023.10095666}}

@inproceedings{close22_interspeech,
  author={George Close and Samuel Hollands and Thomas Hain and Stefan Goetze},
  title={{Non-intrusive Speech Intelligibility Metric Prediction for Hearing Impaired Individuals for the Clarity Prediction Challenge 1}},
  year=2022,
  booktitle={Proc. Interspeech 2022},
  pages={3483--3487},
  doi={10.21437/Interspeech.2022-10182}
}

@inproceedings{close2023PAMGAN,
author={George Close and Thomas Hain and Stefan Goetze},
  title={{PAMGAN+/-: Improving Phase-Aware Speech Enhancement Performance via Expanded Discriminator Training}},
  year=2023,
  booktitle={AES Convention Europe 2023},
}
@ARTICLE{CGMM,  
author={T. {Higuchi} and N. {Ito} and S. {Araki} and T. {Yoshioka} and M. {Delcroix} and T. {Nakatani}},
journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},   
title={Online {MVDR} Beamformer Based on Complex Gaussian Mixture Model With Spatial Prior for Noise Robust ASR},   
year={2017},  
volume={25},  
number={4}, 
pages={780-793}, 
doi={10.1109/TASLP.2017.2665341}}

@techreport{Wang,
abstract = {With growing concerns about the safety and robustness of neural networks, a number of researchers have successfully applied abstract interpretation with numerical domains to verify properties of neural networks. We present a theoretical result that demonstrates the power of numerical domains, namely, the simple interval domain, for analysis of neural networks. Our main theorem, which we call the abstract universal approximation (AUA) theorem, generalizes the recent result by Baader et al. (2020) for ReLU networks to a rich class of neural networks. The classical universal approximation theorem says that, given function f , for any desired precision, there is a neural network that can approximate f. The AUA theorem states that for any function f , there exists a neural network whose abstract interpretation is an arbitrarily close approximation of the collecting semantics of f. Further, the network may be constructed using any well-behaved activation function-sigmoid, tanh, parametric ReLU, ELU, and more-making our result quite general. The implication of the AUA theorem is that there exist provably correct neural networks: Suppose, for instance, that there is an ideal robust image classifier represented as function f. The AUA theorem tells us that there exists a neural network that approximates f and for which we can automatically construct proofs of robustness using the interval abstract domain. Our work sheds light on the existence of provably correct neural networks, using arbitrary activation functions, and establishes intriguing connections between well-known theoretical properties of neural networks and abstract interpretation using numerical domains.},
archivePrefix = {arXiv},
arxivId = {2007.06093v3},
author = {Wang, Zi and Albarghouthi, Aws and Jha, Somesh},
eprint = {2007.06093v3},
file = {:home/will/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Albarghouthi, Jha - Unknown - Abstract Universal Approximation for Neural Networks.pdf:pdf},
title = {{Abstract Universal Approximation for Neural Networks}}
}

@misc{PESQ_standard, 
title = {Perceptual evaluation of speech quality ({PESQ}), and objective method for end-to-end speech quality assessment of narrowband telephone networks and speech codecs. ITU-T Recommendation P. 862},
author={Yi Hu and Philipos C. Loizou},
year={2000},
}
@misc{ioffe2015batch,
      title={Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}, 
      author={Sergey Ioffe and Christian Szegedy},
      year={2015},
      eprint={1502.03167},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{mimospeech,
abstract = {Recently, the end-to-end approach has proven its efficacy in monaural multi-speaker speech recognition. However, high word error rates (WERs) still prevent these systems from being used in practical applications. On the other hand, the spatial information in multi-channel signals has proven helpful in far-field speech recognition tasks. In this work, we propose a novel neural sequence-to-sequence (seq2seq) architecture, MIMO-Speech, which extends the original seq2seq to deal with multi-channel input and multi-channel output so that it can fully model multi-channel multi-speaker speech separation and recognition. MIMO-Speech is a fully neural end-to-end framework, which is optimized only via an ASR criterion. It is comprised of: 1) a monaural masking network, 2) a multi-source neural beamformer, and 3) a multi-output speech recognition model. With this processing, the input overlapped speech is directly mapped to text sequences. We further adopted a curriculum learning strategy, making the best use of the training set to improve the performance. The experiments on the spatialized wsj1-2mix corpus show that our model can achieve more than 60{\%} WER reduction compared to the single-channel system with high quality enhanced signals (SI-{SDR} = 23.1 dB) obtained by the above separation function.},
archivePrefix = {arXiv},
arxivId = {1910.06522},
author = {Chang, Xuankai and Zhang, Wangyou and Qian, Yanmin and Roux, Jonathan Le and Watanabe, Shinji},
eprint = {1910.06522},
file = {:home/will/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chang et al. - 2019 - MIMO-SPEECH End-to-End Multi-Channel Multi-Speaker Speech Recognition.pdf:pdf},
journal = {ASRU 2019},
keywords = {Overlapped speech recognition,curriculum learning,end-To-end,neural beamforming,speech separation},
month = {October},
pages = {237--244},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{MIMO-SPEECH: End-to-End Multi-Channel Multi-Speaker Speech Recognition}},
year = {2019}
}
@techreport{Povey,
abstract = {We describe the design of Kaldi, a free, open-source toolkit for speech recognition research. Kaldi provides a speech recognition system based on finite-state transducers (using the freely available OpenFst), together with detailed documentation and scripts for building complete recognition systems. Kaldi is written is C++, and the core library supports modeling of arbitrary phonetic-context sizes, acoustic modeling with subspace Gaussian mixture models (SGMM) as well as standard Gaussian mixture models, together with all commonly used linear and affine transforms. Kaldi is released under the Apache License v2.0, which is highly nonrestrictive, making it suitable for a wide community of users.},
author = {Povey, Daniel and Ghoshal, Arnab and Boulianne, Gilles and Burget, Luk{\'{a}}{\v{s}} and Glembek, Ondřej and Goel, Nagendra and Hannemann, Mirko and Motl{\'{i}}{\v{c}}ek, Petr and Qian, Yanmin and Schwarz, Petr and Silovsk´y, Jan Silovsk´y and Stemmer, Georg and Vesely, Karel Vesely},
file = {:home/will/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Povey et al. - Unknown - The Kaldi Speech Recognition Toolkit(2).pdf:pdf},
title = {{The Kaldi Speech Recognition Toolkit}},
}
@INPROCEEDINGS{LeRoux,  author={Roux, J. Le and Wisdom, S. and Erdogan, H. and Hershey, J. R.},  booktitle={ICASSP 2019},   title={{SDR – Half-baked or Well Done?}},   year={2019}, month={May},  volume={},  number={},  doi={10.1109/ICASSP.2019.8683855}}

@INPROCEEDINGS{tinysep,
  title={Tiny-Sepformer: A Tiny Time-Domain Transformer Network for Speech Separation},
  author={Jian Luo and Jianzong Wang and Ning Cheng and Edward Xiao and Xulong Zhang and Jing Xiao},
  booktitle={Interspeech 2022},
  year={2022}
}

@misc{yang2019improved,
      title={Improved Speech Separation with Time-and-Frequency Cross-domain Joint Embedding and Clustering}, 
      author={Gene-Ping Yang and Chao-I Tuan and Hung-Yi Lee and Lin-shan Lee},
      year={2019},
      eprint={1904.07845},
      archivePrefix={arXiv},
      primaryClass={cs.SD}
}

@article{10.1109/TASLP.2017.2726762,
author = {Kolbaek, Morten and Yu, Dong and Tan, Zheng-Hua and Jensen, Jesper and Kolbaek, Morten and Dong Yu and Zheng-Hua Tan and Jensen, Jesper},
title = {Multitalker Speech Separation With Utterance-Level Permutation Invariant Training of Deep Recurrent Neural Networks},
year = {2017},
issue_date = {October 2017},
publisher = {IEEE Press},
volume = {25},
number = {10},
issn = {2329-9290},
doi = {10.1109/TASLP.2017.2726762},
abstract = {In this paper, we propose the utterance-level permutation invariant training uPIT technique. uPIT is a practically applicable, end-to-end, deep-learning-based solution for speaker independent multitalker speech separation. Specifically, uPIT extends the recently proposed permutation invariant training PIT technique with an utterance-level cost function, hence eliminating the need for solving an additional permutation problem during inference, which is otherwise required by frame-level PIT. We achieve this using recurrent neural networks RNNs that, during training, minimize the utterance-level separation error, hence forcing separated frames belonging to the same speaker to be aligned to the same output stream. In practice, this allows RNNs, trained with uPIT, to separate multitalker mixed speech without any prior knowledge of signal duration, number of speakers, speaker identity, or gender. We evaluated uPIT on the WSJ0 and Danish two- and three-talker mixed-speech separation tasks and found that uPIT outperforms techniques based on nonnegative matrix factorization and computational auditory scene analysis, and compares favorably with deep clustering, and the deep attractor network. Furthermore, we found that models trained with uPIT generalize well to unseen speakers and languages. Finally, we found that a single model, trained with uPIT, can handle both two-speaker, and three-speaker speech mixtures.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {oct},
pages = {1901–1913},
numpages = {13}
}

@article{Kadolu2020AnES,
  title={An Empirical Study of {Conv-{TasNet}}},
  author={Berkan Kadıoğlu and Michael Horgan and Xiaoyu Liu and Jordi Pons and Dan Darcy and Vivek Kumar},
  journal={ICASSP 2020},
  year={2020},
  month=May,
  pages={7264-7268}
}
@article{selfattn,
author = {Lin, Zhouhan and Feng, Minwei and Dos Santos, Cicero and Yu, Mo and Xiang, Bing and Zhou, Bowen and Bengio, Y.},
year = {2017},
month = {03},
pages = {},
title = {A Structured Self-attentive Sentence Embedding}
}
@article{Chen2018,
abstract = {This paper describes a new baseline system for automatic speech recognition (ASR) in the CHiME-4 challenge to promote the development of noisy ASR in speech processing communities by providing 1) state-of-the-art system with a simplified single system comparable to the complicated top systems in the challenge, 2) publicly available and reproducible recipe through the main repository in the Kaldi speech recognition toolkit. The proposed system adopts generalized eigenvalue beamforming with bidirectional long short-term memory (LSTM) mask estimation. We also propose to use a time delay neural network (TDNN) based on the lattice-free version of the maximum mutual information (LF-MMI) trained with augmented all six microphones plus the enhanced data after beamforming. Finally, we use a LSTM language model for lattice and n-best re-scoring. The final system achieved 2.74$\backslash${\%} WER for the real test set in the 6-channel track, which corresponds to the 2nd place in the challenge. In addition, the proposed baseline recipe includes four different speech enhancement measures, short-time objective intelligibility measure ({STOI}), extended {STOI} (e{STOI}), perceptual evaluation of speech quality ({PESQ}) and speech distortion ratio ({SDR}) for the simulation test set. Thus, the recipe also provides an experimental platform for speech enhancement studies with these performance measures.},
archivePrefix = {arXiv},
arxivId = {1803.10109},
author = {Chen, Szu-Jui and Subramanian, Aswin Shanmugam and Xu, Hainan and Watanabe, Shinji},
eprint = {1803.10109},
file = {:home/will/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - 2018 - Building state-of-the-art distant speech recognition using the CHiME-4 challenge with a setup of speech enhancement.pdf:pdf},
journal = {Proceedings of the Annual Conference of the International Speech Communication Association, Interspeech},
keywords = {LSTM language modeling,Lattice-free MMI,Mask-based beamforming,Noise robustness,Speech recognition},
month = {March},
pages = {1571--1575},
publisher = {International Speech Communication Association},
title = {{Building state-of-the-art distant speech recognition using the CHiME-4 challenge with a setup of speech enhancement baseline}},
volume = {2018-September},
year = {2018}
}
@INPROCEEDINGS{irmammodel,  author={Y. {Xu} and C. {Weng} and L. {Hui} and J. {Liu} and M. {Yu} and D. {Su} and D. {Yu}},  booktitle={ICASSP 2019},   title={Joint Training of Complex Ratio Mask Based Beamformer and Acoustic Model for Noise Robust Asr},   year={2019},  volume={},  number={},  pages={6745-6749},  doi={10.1109/ICASSP.2019.8682576}}
@techreport{Chiu2018,
abstract = {In this paper we document our experiences with developing speech recognition for medical transcription-a system that automatically transcribes doctor-patient conversations. Towards this goal, we built a system along two different method-ological lines-a Connectionist Temporal Classification (CTC) phoneme based model and a Listen Attend and Spell (LAS) grapheme based model. To train these models we used a corpus of anonymized conversations representing approximately 14,000 hours of speech. Because of noisy transcripts and alignments in the corpus, a significant amount of effort was invested in data cleaning issues. We describe a two-stage strategy we followed for segmenting the data. The data cleanup and development of a matched language model was essential to the success of the CTC based models. The LAS based models, however were found to be resilient to alignment and transcript noise and did not require the use of language models. CTC models were able to achieve a word error rate of 20.1{\%}, and the LAS models were able to achieve 18.3{\%}. Our analysis shows that both models perform well on important medical utterances and therefore can be practical for transcribing medical conversations.},
archivePrefix = {arXiv},
arxivId = {1711.07274v2},
author = {Chiu, Chung-Cheng and Tripathi, Anshuman and Chou, Katherine and Co, Chris and Jaitly, Navdeep and Jaunzeikare, Diana and Kannan, Anjuli and Nguyen, Patrick and Sak, Hasim and Sankar, Ananth and Tansuwan, Justin and Wan, Nathan and Wu, Yonghui and Zhang, Xuedong},
eprint = {1711.07274v2},
file = {:home/will/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chiu et al. - 2018 - Speech recognition for medical conversations.pdf:pdf},
keywords = {CTC,Index Terms: medical transcription,conversational transcrip-tion,end-to-end attention models},
title = {{Speech recognition for medical conversations}},
year = {2018}
}
@inproceedings{Edwards2017,
abstract = {We present a speech recognition system for the medical domain whose architecture is based on a state-of-the-art stack trained on over 270 h of medical speech data and 30 million tokens of text from clinical episodes. Despite the acoustic challenges and linguistic complexity of the domain, we were able to reduce the system's word error rate to below 16{\%} in a realistic clinical use case. To further benchmark our system, we determined the human word error rate on a corpus covering a wide variety of speakers, working with multiple medical transcriptionists, and found that our speech recognition system performs on a par with humans.},
author = {Edwards, Erik and Salloum, Wael and Finley, Greg P. and Fone, James and Cardiff, Greg and Miller, Mark and Suendermann-Oeft, David},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-66429-3_51},
file = {:home/will/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Edwards et al. - 2017 - Medical speech recognition Reaching parity with humans.pdf:pdf},
isbn = {9783319664286},
issn = {16113349},
keywords = {Human word error rate,Medical speech recognition,Parity},
pages = {512--524},
publisher = {Springer Verlag},
title = {{Medical speech recognition: Reaching parity with humans}},
volume = {10458 LNAI},
year = {2017}
}
@techreport{Athineos,
abstract = {Current speech recognition systems uniformly employ short-time spectral analysis, usually over windows of 10-30 ms, as the basis for their acoustic representations. Any detail below this timescale is lost, and even temporal structure above this level is usually only weakly represented in the form of deltas etc. We address this limitation by proposing a novel representation of the temporal envelope in different frequency bands by exploring the dual of conventional linear prediction (LPC) when applied in the transform domain. With this technique of frequency-domain linear prediction (FDLP), the 'poles' of the model describe temporal , rather than spectral, peaks. By using analysis windows on the order of hundreds of milliseconds, the procedure automatically decides how to distribute poles to best model the temporal structure within the window. While this approach offers many possibilities for novel speech features, we experiment with one particular form, an index describing the 'sharpness' of individual poles within a window, and show a large relative word error rate improvement from 4.97\% to 3.81\% in a recognizer trained on general conversational telephone speech and tested on a small-vocabulary spontaneous numbers task. We analyze this improvement in terms of the confusion matrices and suggest how the newly-modeled fine temporal structure may be helping.},
author = {Athineos, Marios and Labrosa, Daniel P W Ellis},
file = {:C\:/Users/will9/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Athineos, Labrosa - Unknown - Frequency-Domain Linear Prediction For Temporal Features(2).pdf:pdf},
title = {{Frequency-Domain Linear Prediction For Temporal Features}},
url = {http://www.ee.}
}

@inproceedings{Purushothaman2020,
  author={A. Purushothaman and A. Sreeram and R. Kumar and S. Ganapathy},
  title={{Deep Learning Based Dereverberation of Temporal Envelopes for Robust Speech Recognition}},
  year=2020,
  month=oct,
  booktitle={Interspeech 2020},
  doi={10.21437/Interspeech.2020-2283},
  url={http://dx.doi.org/10.21437/Interspeech.2020-2283}
}


@article{Jia,
year = {2022},
archivePrefix = {arXiv},
arxivId = {2201.00480v1},
author = {Jia, Xupeng and Li, Dongmei},
eprint = {2201.00480v1},
eprinttype = {arXiv},
title = {{TFCN: Temporal-Frequential Convolutional Network for Single-Channel Speech Enhancement}},
primaryClass={eess.AS},
note={arXiv:2201.00480v1}
}


@article{Erdogan2016,
abstract = {Recent studies on multi-microphone speech databases indicate that it is beneficial to perform beamforming to improve speech recognition accuracies, especially when there is a high level of background noise. Minimum variance distortionless response ({MVDR}) beamforming is an important beamforming method that performs quite well for speech recognition purposes especially if the steering vector is known. However, steering the beamformer to focus on speech in unknown acoustic conditions remains a challenging problem. In this study, we use single-channel speech enhancement deep networks to form masks that can be used for noise spatial covariance estimation, which steers the {MVDR} beamforming toward the speech. We analyze how mask prediction affects performance and also discuss various ways to use masks to obtain the speech and noise spatial co-variance estimates in a reliable way. We show that using a single mask across microphones for covariance prediction with minima-limited post-masking yields the best result in terms of signal-level quality measures and speech recognition word error rates in a mismatched training condition.},
author = {Erdogan, Hakan and Hershey, John and Watanabe, Shinji and Mandel, Michael and {Le Roux}, Jonathan},
doi = {10.21437/Interspeech.2016-552},
file = {:home/will/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Erdogan et al. - 2016 - Improved {MVDR} beamforming using single-channel mask prediction networks.pdf:pdf},
keywords = {[Electronic Manuscript]},
title = {{Improved {MVDR} beamforming using single-channel mask prediction networks}},
url = {http://dx.doi.org/10.21437/Interspeech.2016-552},
year = {2016}
}
@inproceedings{Ernst2018,
archivePrefix = {arXiv},
arxivId = {1803.08243},
author = {Ernst, Ori and Chazan, Shlomo E. and Gannot, Sharon and Goldberger, Jacob},
booktitle = {European Signal Processing Conference},
doi = {10.23919/EUSIPCO.2018.8553141},
eprint = {1803.08243},
isbn = {9789082797015},
issn = {22195491},
title = {{Speech dereverberation using fully convolutional networks}},
volume = {2018-Septe},
year = {2018}
}
@techreport{Kanda2018,
abstract = {This paper presents Hitachi and JHU's efforts on developing CHiME-5 system to recognize dinner party speeches recorded by multiple microphone arrays. We newly developed (1) the way to apply multiple data augmentation methods, (2) residual bidirectional long short-term memory, (3) 4-ch acoustic models, (4) multiple-array combination methods, (5) hypothesis dedu-plication method, and (6) speaker adaptation technique of neu-ral beamformer. As the results, our best system in category B achieved 52.38{\%} of word error rates (WERs) for development set, which corresponded to 35{\%} of relative WER reduction from the state-of-the-art baseline. Our best system also achieved 48.20{\%} of WER for evaluation set, which was the 2nd best result in the CHiME-5 competition.},
author = {Kanda, Naoyuki and Ikeshita, Rintaro and Horiguchi, Shota and Fujita, Yusuke and Nagamatsu, Kenji and Wang, Xiaofei and Manohar, Vimal and {Yalta Soplin}, Nelson Enrique and Maciejewski, Matthew and Chen, Szu-Jui and Subramanian, Aswin Shanmugam and Li, Ruizhi and Wang, Zhiqi and Naradowsky, Jason and Garcia-Perera, L. Paola and Sell, Gregory},
doi = {10.21437/chime.2018-2},
file = {:home/will/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kanda et al. - 2018 - The HitachiJHU CHiME-5 system Advances in speech recognition for everyday home environments using multiple microph.pdf:pdf},
pages = {6--10},
title = {{The Hitachi/JHU CHiME-5 system: Advances in speech recognition for everyday home environments using multiple microphone arrays}},
year = {2018}
}

@inproceedings{Finley2018,
abstract = {Training models for speech recognition usually requires accurate word-level transcription of available speech data. For the domain of medical dictations, it is common to have “semi-literal” transcripts available: large numbers of speech files along with their associated formatted episode report, whose content only partially overlaps with the spoken content of the dictation. We present a semi-supervised method for generating acoustic training data by decoding dictations with an existing recognizer, confirming which sections are correct by using the associated report, and repurposing these audio sections for training a new acoustic model. The effectiveness of this method is demonstrated in two applications: first, to adapt a model to new speakers, resulting in a 19.7{\%} reduction in relative word errors for these speakers; and second, to supplement an already diverse and robust acoustic model with a large quantity of additional data (from already known voices), leading to a 5.0{\%} relative error reduction on a large test set of over one thousand speakers.},
author = {Finley, Greg P. and Edwards, Erik and Salloum, Wael and Robinson, Amanda and Sadoughi, Najmeh and Axtmann, Nico and Korenevsky, Maxim and Brenndoerfer, Michael and Miller, Mark and Suendermann-Oeft, David},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-99579-3_19},
file = {:home/will/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Finley et al. - 2018 - Semi-Supervised Acoustic Model Retraining for Medical ASR.pdf:pdf},
isbn = {9783319995786},
issn = {16113349},
keywords = {ASR,Acoustic modeling,Medical dictation,Medical speech recognition},
month = {sep},
pages = {177--187},
publisher = {Springer Verlag},
title = {{Semi-Supervised Acoustic Model Retraining for Medical ASR}},
volume = {11096 LNAI},
year = {2018}
}

@misc{resepformer,
  doi = {10.48550/ARXIV.2206.09507},
  url = {https://arxiv.org/abs/2206.09507},
  author = {Subakan, Cem and Ravanelli, Mirco and Cornell, Samuele and Lepoutre, Frédéric and Grondin, François},
  keywords = {Audio and Speech Processing (eess.AS), Machine Learning (cs.LG), Sound (cs.SD), Signal Processing (eess.SP), FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Resource-Efficient Separation Transformer},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@INPROCEEDINGS{deformconv,  author={Dai, Jifeng and Qi, Haozhi and Xiong, Yuwen and Li, Yi and Zhang, Guodong and Hu, Han and Wei, Yichen},  booktitle={ICCV},   title={Deformable Convolutional Networks},   year={2017},  volume={},  number={},  pages={764-773},  doi={10.1109/ICCV.2017.89}}
@ARTICLE{1ddconv,  author={Bhagya, D. and Suchetha, M.},  journal={IEEE Sensors Journal},   title={A 1-D Deformable Convolutional Neural Network for the Quantitative Analysis of Capnographic Sensor},   year={2021},  volume={21},  number={5},  pages={6672-6678},  doi={10.1109/JSEN.2020.3042989}}

@inproceedings{wdtcn,
AUTHOR={Ravenscroft, William and Goetze, Stefan and Hain, Thomas},
booktitle={IWAENC 2022},
title={Utterance Weighted Multi-Dilation Temporal Convolutional Networks for Monaural Speech Dereverberation},
year={2022}, month={Sep},  volume={},  number={},  pages={},
}

@techreport{Du5,
abstract = {This report describes our submission to the fifth CHiME Challenge. The main technical points of our system include the deep learning based speech enhancement and separation, training data augmentation via different versions of the official training data, SNR-based array selection, front-end model fusion, acoustic model fusion, and language model fusion. Tested on the development test set, our best system for single-array track using official LM has yielded a 37.7{\%} WER relative reduction over the results given by official baseline system. 1. System Overview CHiME-5[1] challenge features a single-array track and a multiple-array track, and we participate both of them. A unified framework of training process is given in Figure 1. As we can see, it contains several main parts including deep-learning based speech saparation (SS Model), speech enhancement (SE Model), multi-channel based WPE denoising, beamforming and acoustic model training. For the front-end, we fisrt conduct data simulation to augment data size by estimating impulse responses between binaural data and far-field data. Meanwhile , we apply a conventional multi-channel noise reduction using log-spectral amplitude [2] which is based on generalized weighted prediction error (GWPE) [3] and independent vector analysis (IVA) [4]. With the denoised data, we can build the following speech enhancement model and speech separation model which are both based on deep-learning techniques. After all, each method of these frond-end techniques can provide processed data of official original training data, add increase the diversity of original data. Using the final augmented data, five types of acoustic model are trained as the back-end system. The testing phase can be divided into two scenarios: single-array track and multiple-array track. For each track, two separate rankings will be produced: Rank A compares systems which are based on conventional acoustic modeling and using the supplied official language model, while Rank B has no such limitations. Speaking of single-track, the same conventional multi-channel preprocessing described above is first conducted. Then, speaker-dependent SS models are trained with the de-noised data for each speaker among the test set. The outputs of SS model and SE model, are integrated together to provide necessary initialization information for beamforming. After that, the beamformed speech is sent to back-end acoustic models for recognition. What's more, several acoustic models are fused at the state-level. However, for Rank B, the first-pass decoding is performed with the HMM and 3-gram to generate the lattice as the hypotheses, which are served for the second-pass decoding with a LSTM-based LM. In multiple-array track, we first use the SE Model to estimate the signal to noise ratio (SNR) for each array, two arrays with maximum SNRs are selected. The rest procedures are almost the same with single-arrary track, conventional multi-channel preprocessing is first used. Then, speaker-dependent SS model are trained with the denoised data for each speaker among the test set. The outputs of SS model and SE model are integrated together to provide necessary initialization information for beamforming. The beamformed speech is sent to back-end acoustic model for recognition. Multiple acoustic models of both two selected arrays are fused at the state-level. For Rank B, the first-pass decoding is performed with the HMM and 3-gram to generate the lattice as the hypotheses, which are served for the second-pass decoding with a LSTM-based LM. More details will be introduced in the following subsections. Figure 1: An illustration of unified training stage, including front-end processing, data augmentation and acoustic model-ing. 2. Main contributions First of all, due to rules defined by official[1], systems are allowed to exploit knowledge of the utterance start and end time, the utterance speaker label and the speaker location label. It's allowed to use binaural data and far-field data in the training set.},
author = {Du, Jun and Gao, Tian and Sun, Lei and Ma, Feng and Fang, Yi and Liu, Di-Yuan and Zhang, Qiang and Zhang, Xiang and Wang, Hai-Kun and Pan, Jia and Gao, Jian-Qing and Lee, Chin-Hui and Chen, Jing-Dong},
file = {:home/will/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Du et al. - Unknown - The USTC-iFlytek Systems for CHiME-5 Challenge.pdf:pdf},
title = {{The USTC-iFlytek Systems for CHiME-5 Challenge}}
}
@techreport{Kitza,
abstract = {This paper describes the systems for the single-array track and the multiple-array track of the 5th CHiME Challenge. The final system is a combination of multiple systems, using Confusion Network Combination (CNC). The different systems presented here are utilizing different front-ends and training sets for a Bidirectional Long Short-Term Memory (BLSTM) Acoustic Model (AM). The front-end was replaced by enhancements provided by Paderborn University [1]. The back-end has been implemented using RASR [2] and RETURNN [3]. Additionally, a system combination including the hypothesis word graphs from the system of the submission [1] has been performed, which results in the final best system.},
author = {Kitza, M. and Michel, W. and Boeddeker, C. and Heitkaemper, J. and Menne, T. and Schl{\"{u}}ter, R. and Ney, He. and Schmalenstroeer, J. and Drude, L. and Heymann, J. and Haeb-Umbach, R.},
file = {:home/will/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kitza et al. - Unknown - The RWTHUPB System Combination for the CHiME 2018 Workshop.pdf:pdf},
title = {{The RWTH/UPB System Combination for the CHiME 2018 Workshop}}
}

@techreport{Menne,
abstract = {This paper describes automatic speech recognition (ASR) systems developed jointly by RWTH, UPB and FORTH for the 1ch, 2ch and 6ch track of the 4th CHiME Challenge. In the 2ch and 6ch tracks the final system output is obtained by a Confusion Network Combination (CNC) of multiple systems. The Acoustic Model (AM) is a deep neural network based on Bidi-rectional Long Short-Term Memory (BLSTM) units. The systems differ by front ends and training sets used for the acoustic training. The model for the 1ch track is trained without any preprocessing. For each front end we trained and evaluated individual acoustic models. We compare the ASR performance of different beamforming approaches: a conventional superdirec-tive beamformer [1] and an {MVDR} beamformer as in [2], where the steering vector is estimated based on [3]. Furthermore we evaluated a BLSTM supported Generalized Eigenvalue beam-former using NN-{GEV} [4]. The back end is implemented using RWTH's open-source toolkits RASR [5], RETURNN [6] and rwthlm [7]. We rescore lattices with a Long Short-Term Memory (LSTM) based language model. The overall best results are obtained by a system combination that includes the lattices from the system of UPB's submission [8]. Our final submission scored second in each of the three tracks of the 4th CHiME Challenge. 1. Background This paper describes ASR systems for the 1ch, 2ch and 6ch tracks of the 4th CHiME Challenge. In contrast to the provided baseline system [9] the back end has been replaced completely and is described in Section 2.2. Furthermore we developed additional systems using different front ends. The front ends are described in Section 2.1. All experimental results presented in this work (Sections 3 and 4) are obtained with the official training set following the rules of the CHiME challenge. 2. Contributions 2.1. Front ends In addition to the baseline (BL) front end we developed three other front ends that utilize different beamformers. The final enhanced signal at the output of each beamformer is given by: Z Z Z(k, l) = w w w(k, l) H X X X(k, l) (1) where k, l denote the frequency index and time-frame, respectively , w w w(k, l) is the M × 1 vector of beamformer filter coefficients for a given front end, X X X(k, l) is the M × 1 vector of microphone array signals in the Short-time Fourier Transform (STFT) domain, and M denotes the number of microphones. We also improved the microphone failure detection mechanism of [2], so as to better identify corrupted microphones. The enhanced microphone failure detection was used in the front ends described in Sections 2.1.2 {\&} 2.1.3. 2.1.1. Microphone failure detection Our microphone failure detection mechanism is based on measuring the consistency of the energies (calculated in each time frame) between the microphone signals. To do that, we construct M time-series em(l), m = 1,. .. , M , each one containing the energy of the signal for l = 1,. .. , L frames, where L denotes the total number of frames in the utterance. Then, for each microphone m, the average correlation coefficient r AV m between em(l) and en(l) for n = m is calculated. A microphone is considered to have failed if r AV m is less than a threshold $\delta$, which was set empirically to 0.8. These microphones, in addition to the microphones which are considered to have failed by the system of [2], are excluded from further processing. 2.1.2. {MVDR} beamformer with steering vector estimation This front end (MV) utilizes a minimum variance distortionless response ({MVDR}) beamformer with diagonal loading, similar to the one in [2]. The filter coefficients are calculated as: w w w {MVDR} (k, l) = R R Rn(k) + diag(|X X X(k, l)| 2) −1 d d d(k) d d d(k) H [R R Rn(k) + diag(|X X X(k, l)| 2)] −1 d d d(k) (2) where = 10 −3 is the diagonal loading term, d d d(k) is the steering vector, R R Rn(k) is the spatial correlation matrix of noise, and diag(x x x) denotes the conversion of vector x x x to a diagonal matrix. For the estimation of the unknown quantities d d d(k) and R R Rn(k) we use the method of [3], which does not require knowledge of the array geometry or the speaker location. We assume},
author = {Menne, Tobias and Heymann, Jahn and Alexandridis, Anastasios and Irie, Kazuki and Zeyer, Albert and Kitza, Markus and Golik, Pavel and Kulikov, Ilia and Drude, Lukas and Schl{\"{u}}ter, Ralf and Ney, Hermann and Haeb-Umbach, Reinhold and Mouchtaris, Athanasios},
file = {:home/will/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Menne et al. - Unknown - The RWTHUPBFORTH System Combination for the 4th CHiME Challenge Evaluation.pdf:pdf},
title = {{The RWTH/UPB/FORTH System Combination for the 4th CHiME Challenge Evaluation}}
}
@INPROCEEDINGS{Goetze2,  author={S. {Goetze} and A. {Warzybok} and I. {Kodrasi} and J. O. {Jungmann} and B. {Cauchi} and J. {Rennies} and E. A. P. {Habets} and A. {Mertins} and T. {Gerkmann} and S. {Doclo} and B. {Kollmeier}},  booktitle={IWAENC 2014},   title={A study on speech quality and speech intelligibility measures for quality assessment of single-channel dereverberation algorithms}, address   = {Antibes, France},  year={2014}, month=September, volume={},  number={},  pages={233-237},  doi={10.1109/IWAENC.2014.6954293}}
@INPROCEEDINGS{yoshioka3,  author={T. {Yoshioka} and N. {Ito} and M. {Delcroix} and A. {Ogawa} and K. {Kinoshita} and M. {Fujimoto} and C. {Yu} and W. J. {Fabian} and M. {Espi} and T. {Higuchi} and S. {Araki} and T. {Nakatani}},  booktitle={2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)},   title={The {NTT CHiME-3} system: Advances in speech enhancement and recognition for mobile multi-microphone devices},   year={2015},  volume={},  number={},  pages={436-443},  doi={10.1109/ASRU.2015.7404828}}
@article{Frost1972,
abstract = {A constrained least mean-squares algorithm has been derived which is capable of adjusting an array of sensors in real time to respond to a signal coming from a desired direction while discriminating against noises coming from other directions. Analysis and computer simulations confirm that the algorithm is able to iteratively adapt variable weights on the taps of the sensor array to minimize noise power in the array output. A set of linear equality constraints on the weights maintains a chosen frequency characteristic for the array in the direction of interest. The array problem would be a classical constrained least-mean-squares problem except that the signal and noise statistics are assumed unknown a priori. A geometrical presentation shows that the algorithm is able to maintain the constraints and prevent the accumulation of quantization errors in a digital implementation. Copyright {\textcopyright} 1972 by The Institute of Electrical and Electronics Engineers, Inc.},
author = {Frost, Otis Lamont},
doi = {10.1109/PROC.1972.8817},
issn = {15582256},
journal = {Proceedings of the IEEE},
number = {8},
pages = {926--935},
title = {{An Algorithm for Linearly Constrained Adaptive Array Processing}},
volume = {60},
year = {1972}
}
@ARTICLE{WPE2,  author={T. {Yoshioka} and T. {Nakatani}},  journal={IEEE Transactions on Audio, Speech, and Language Processing},   title={Generalization of Multi-Channel Linear Prediction Methods for Blind MIMO Impulse Response Shortening},   year={2012},  volume={20},  number={10},  pages={2707-2720},  doi={10.1109/TASL.2012.2210879}}
@Inbook{WPE,
author="Yoshioka, Takuya
and Nakatani, Tomohiro
and Kinoshita, Keisuke
and Miyoshi, Masato",
editor="Cohen, Israel
and Benesty, Jacob
and Gannot, Sharon",
title="Speech Dereverberation and Denoising Based on Time Varying Speech Model and Autoregressive Reverberation Model",
bookTitle="Speech Processing in Modern Communication: Challenges and Perspectives",
year="2010",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="151--182",
abstract="Speech dereverberation and denoising have been important problems for decades in the speech processing field. As regards to denoising, a model-based approach has been intensively studied and many practical methods have been developed. In contrast, research on dereverberation has been relatively limited. It is in very recent years that studies on a model-based approach to dereverberation have made rapid progress. This chapter reviews a model-based dereverberation method developed by the authors. This dereverberation method is effectively combined with a traditional denoising technique, specifically a multichannel Wiener filter. This combined method is derived by solving a dereverberation and denoising problem with a modelbased approach. The combined dereverberation and denoising method as well as the original dereverberation method are developed by using a multichannel autoregressive model of room acoustics and a time-varying power spectrum model of clean speech signals.",
isbn="978-3-642-11130-3",
doi="10.1007/978-3-642-11130-3_6",
url="https://doi.org/10.1007/978-3-642-11130-3\_6"
}

@ARTICLE{pfeifenberger2019,  author={L. {Pfeifenberger} and M. {Zöhrer} and F. {Pernkopf}},  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},   title={Eigenvector-Based Speech Mask Estimation for Multi-Channel Speech Enhancement},   year={2019},  volume={27},  number={12},  pages={2162-2172},  doi={10.1109/TASLP.2019.2941592}}
@misc{pfeifenberger2020,
      title={Resource-Efficient Speech Mask Estimation for Multi-Channel Speech Enhancement}, 
      author={Lukas Pfeifenberger and Matthias Zöhrer and Günther Schindler and Wolfgang Roth and Holger Fröning and Franz Pernkopf},
      year={2020},
      eprint={2007.11477},
      archivePrefix={arXiv},
      primaryClass={eess.AS}
}
@article{goetze2006a,			  		  author={goetze, stefan and  kammeyer, karl-dirk and  mildner, volker},					  journal={journal of the audio engineering society}, 					  title={a psychoacoustic noise reduction approach for stereo hands-free systems}, 					  year={2006},					  volume={},					  number={},					  pages={},					  doi={},					  month={May},}						
@inproceedings{Fiscus1997,
abstract = {This paper describes a system developed at NIST to produce a composite Automatic Speech Recognition (ASR) system output when the outputs of multiple ASR systems are available, and for which, in many cases, the composite ASR output has lower error rate than any of the individual systems. The system implements a `voting' or rescoring process to reconcile differences in ASR system outputs. We refer to this system as the NIST Recognizer Output Voting Error Reduction (ROVER) system. As additional knowledge sources are added to an ASR system, (e.g., acoustic and language models), error rates are typically decreased. This paper describes a post-recognition process which models the output generated by multiple ASR systems as independent knowledge sources that can be combined and used to generate an output with reduced error rate. To accomplish this, the outputs of multiple of ASR systems are combined into a single, minimal cost word transition network (WTN) via iterative applications of dynamic programming (DP) alignments. The resulting network is searched by an automatic rescoring or `voting' process that selects an output sequence with the lowest score.},
author = {Fiscus, Jonathan G.},
booktitle = {IEEE Workshop on Automatic Speech Recognition and Understanding Proceedings},
doi = {10.1109/asru.1997.659110},
pages = {347--354},
publisher = {IEEE},
title = {{Post-processing system to yield reduced word error rates: Recognizer Output Voting Error Reduction (ROVER)}},
year = {1997}
}
@ARTICLE{dynadnn,  author={Han, Y. and Huang, G. and Song, S. and Yang, L. and Wang, H. and Wang, Y.},  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},   title={Dynamic Neural Networks: A Survey},   year={2021},  volume={},  number={},  pages={1-1},  doi={10.1109/TPAMI.2021.3117837}}

@inproceedings{dynaconv,
author = {Chen, Y. and Dai, X. and Liu, M. and Chen, D. and Yuan, L and Liu, Z.},
title = {Dynamic Convolution: Attention over Convolution Kernels},
booktitle = {CVPR 2020},
year = {2020},
month = {June},
abstract = {Light-weight convolutional neural networks (CNNs) suffer performance degradation as their low computational budgets constrain both the depth (number of convolution layers) and width (number of channels) of CNN, resulting limited representation capability. To address this issue, we present dynamic convolution, a new design that increases model complexity without increasing the network depth or width. Instead of using a single convolution kernel per layer, dynamic convolution aggregates multiple parallel convolution kernels dynamically based upon their attentions, which are input dependent. Assembling multiple kernels is not only computational efficient due to the small kernel size, but also has more representation power since these kernels are aggregated in a non-linear way via attention. By simply using dynamic convolution for the state-of-the-art architecture MobilenetV3-Small, the top-1 accuracy on ImageNet classification is boosted by 2.3\% with only 4\% additional FLOPs and 2.9 AP gain is achieved on COCO keypoint detection.},
url = {https://www.microsoft.com/en-us/research/publication/dynamic-convolution-attention-over-convolution-kernels/},
}
@ARTICLE{Mehrez10,  author={M. {Souden} and J. {Benesty} and S. {Affes}},  journal={IEEE Transactions on Audio, Speech, and Language Processing},   title={On Optimal Frequency-Domain Multichannel Linear Filtering for Noise Reduction},   year={2010},  volume={18},  number={2},  pages={260-276},}

@INPROCEEDINGS{prelu,  author={He, K. and Zhang, X. and Ren, S. and Sun, J.},  booktitle={ICCV 2015},   title={Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification},   year={2015},  volume={},  number={},  month=dec,  doi={10.1109/ICCV.2015.123}}

@article{atttasnet,
AUTHOR={Ravenscroft, W. and Goetze, S. and Hain, T.},   
TITLE={{Att-TasNet: Attending to Encodings in Time-Domain Audio Speech Separation of Noisy, Reverberant Speech Mixtures}},      
JOURNAL={Frontiers in Signal Processing},      
YEAR={2022},      
URL={https://www.frontiersin.org/articles/10.3389/frsip.2022.856968},       
DOI={10.3389/frsip.2022.856968},      
ABSTRACT={Separation of speech mixtures in noisy and reverberant environments remains a challenging task for state-of-the-art speech separation systems. Time-domain audio speech separation networks (TasNets) are among the most commonly used network architectures for this task. TasNet models have demonstrated strong performance on typical speech separation baselines where speech is not contaminated with noise. When additive or convolutive noise is present, performance of speech separation degrades significantly. TasNets are typically constructed of an encoder network, a mask estimation network and a decoder network. The design of these networks puts the majority of the onus for enhancing the signal on the mask estimation network when used without any pre-processing of the input data or post processing of the separation network output data. Use of multihead attention (MHA) is proposed in this work as an additional layer in the encoder and decoder to help the separation network attend to encoded features that are relevant to the target speakers and conversely suppress noisy disturbances in the encoded features. As shown in this work, incorporating MHA mechanisms into the encoder network in particular leads to a consistent performance improvement across numerous quality and intelligibility metrics on a variety of acoustic conditions using the WHAMR corpus, a data-set of noisy reverberant speech mixtures. The use of MHA is also investigated in the decoder network where it is demonstrated that smaller performance improvements are consistently gained within specific model configurations. The best performing MHA models yield a mean 0.6 dB scale invariant signal-to-distortion (SISDR) improvement on noisy reverberant mixtures over a baseline 1D convolution encoder. A mean 1 dB SISDR improvement is observed on clean speech mixtures.},
publisher={Frontiers}
}

@ARTICLE{Haeb-Umbach2019,  author={Haeb-Umbach, R. and Watanabe, S. and Nakatani, T. and Bacchiani, M. and Hoffmeister, B. and Seltzer, M. L. and Zen, H. and Souden, M.},  journal={IEEE Signal Processing Magazine},   title={Speech Processing for Digital Home Assistants: Combining Signal Processing With Deep-Learning Techniques},   year={2019},  volume={36},  number={6},  pages={111-124},  doi={10.1109/MSP.2019.2918706}}

@INPROCEEDINGS{8578843,  author={Hu, J. and Shen, L. and Sun, G.},  booktitle={CVPR 2018},   title={Squeeze-and-Excitation Networks},   year={2018},  volume={},  number={},  month=jun,  doi={10.1109/CVPR.2018.00745}}

@inproceedings{Hain3,
  author={T. Hain and L. Burget and J. Dines and P. N. Garner and A. El Hannani and M. Huijbregts and M. Karafiát and M. Lincoln and V. Wan},
  title={{The AMIDA 2009 meeting transcription system}},
  year=2010,
  booktitle={Interspeech 2010},
  month=sep,
  doi={10.21437/Interspeech.2010-130}
}

@inproceedings{IWAENCbestpaper,
  title={Monaural source separation: From anechoic to reverberant environments},
  author={Cord-Landwehr, Tobias and Boeddeker, Christoph and von Neumann, Thilo and Zorila, Catalin and Doddipatla, Rama and Haeb-Umbach, Reinhold},
  booktitle={17th International Workshop on Acoustic Signal Enhancement (IWAENC 2022)},
  month= september,
  year={2022}
}





@InProceedings{furcanext,
author="Zhang, Liwen
and Shi, Ziqiang
and Han, Jiqing
and Shi, Anyan
and Ma, Ding",
editor="Ro, Yong Man
and Cheng, Wen-Huang
and Kim, Junmo
and Chu, Wei-Ta
and Cui, Peng
and Choi, Jung-Woo
and Hu, Min-Chun
and De Neve, Wesley",
title="FurcaNeXt: End-to-End Monaural Speech Separation with Dynamic Gated Dilated Temporal Convolutional Networks",
booktitle="MultiMedia Modeling",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="653--665",
abstract="Deep dilated temporal convolutional networks (TCN) have been proved to be very effective in sequence modeling. In this paper we propose several improvements of TCN for end-to-end approach to monaural speech separation, which consists of (1) multi-scale dynamic weighted gated TCN with a pyramidal structure (FurcaPy), (2) gated TCN with intra-parallel convolutional components (FurcaPa), (3) weight-shared multi-scale gated TCN (FurcaSh) and (4) dilated TCN with gated subtractive-convolutional component (FurcaSu). All these networks take the mixed utterance of two speakers and maps it to two separated utterances, where each utterance contains only one speaker's voice. For the objective, we propose to train the networks by directly optimizing utterance-level signal-to-distortion ratio (SDR) in a permutation invariant training (PIT) style. Our experiments on the public WSJ0-2mix data corpus result in 18.4 dB SDR improvement, which shows our proposed networks can lead to performance improvement on the speaker separation task.",
isbn="978-3-030-37731-1"
}

@InProceedings{Hain2,
author="Hain, Thomas
and Burget, Lukas
and Dines, John
and Garau, Giulia
and Karafiat, Martin
and van Leeuwen, David
and Lincoln, Mike
and Wan, Vincent",
editor="Stiefelhagen, Rainer
and Bowers, Rachel
and Fiscus, Jonathan",
title="The 2007 {AMI(DA)} {System for Meeting Transcription}",
booktitle="Multimodal Technologies for Perception of Humans",
year="2008",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="414--428",
abstract="Meeting transcription is one of the main tasks for large vocabulary automatic speech recognition (ASR) and is supported by several large international projects in the area. The conversational nature, the difficult acoustics, and the necessity of high quality speech transcripts for higher level processing make ASR of meeting recordings an interesting challenge. This paper describes the development and system architecture of the 2007 AMIDA meeting transcription system, the third of such systems developed in a collaboration of six research sites. Different variants of the system participated in all speech to text transcription tasks of the 2007 NIST RT evaluations and showed very competitive performance. The best result was obtained on close-talking microphone data where a final word error rate of 24.9{\%} was obtained.",
isbn="978-3-540-68585-2"
}
@ARTICLE{58337,  author={P. J. {Werbos}},  journal={Proceedings of the IEEE},   title={Backpropagation through time: what it does and how to do it},   year={1990},  volume={78},  number={10},  pages={1550-1560},  doi={10.1109/5.58337}}
@techreport{Glorot,
abstract = {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random ini-tialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training , with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new ini-tialization scheme that brings substantially faster convergence.},
author = {Glorot, Xavier and Bengio, Yoshua},
file = {:home/will/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Glorot, Bengio - Unknown - Understanding the difficulty of training deep feedforward neural networks.pdf:pdf},
title = {{Understanding the difficulty of training deep feedforward neural networks}},
url = {http://www.iro.umontreal.}
}
@ARTICLE{GSC1,  author={L. {Griffiths} and C. {Jim}},  journal={IEEE Transactions on Antennas and Propagation},   title={An alternative approach to linearly constrained adaptive beamforming},   year={1982},  volume={30},  number={1},  pages={27-34},  doi={10.1109/TAP.1982.1142739}}
@article{hintonRMS, author={G. Hinton, N. Srivastava, K. Swersky}, title={Neural Networks for Machine Learning, Lecture 6a, Overview of mini-batch gradient descent}}

@INPROCEEDINGS{BLSTMgraves,  author={A. {Graves} and J. {Schmidhuber}},  booktitle={Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005.},   title={Framewise phoneme classification with bidirectional LSTM networks},   year={2005},  volume={4},  number={},  pages={2047-2052 vol. 4},  doi={10.1109/IJCNN.2005.1556215}}

@ARTICLE{{GEV}1,  author={E. {Warsitz} and R. {Haeb-Umbach}},  journal={IEEE Transactions on Audio, Speech, and Language Processing},   title={Blind Acoustic Beamforming Based on Generalized Eigenvalue Decomposition},   year={2007},  volume={15},  number={5},  pages={1529-1539},  doi={10.1109/TASL.2007.898454}}

@inproceedings{Hain,
author = {Hain, Thomas and Burget, Lukas and Dines, John and Garau, Giulia and Karafiat, Martin and Lincoln, Mike and Vepa, Jithendra and Wan, Vincent},
title = {The {AMI} {Meeting} {Transcription} {System}: {Progress and Performance}},
year = {2006},
isbn = {3540692673},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
doi = {10.1007/11965152_37},
abstract = {We present the AMI 2006 system for the transcription of speech in meetings. The system was jointly developed by multiple sites on the basis of the 2005 system for participation in the NIST RT'05 evaluations. The paper describes major developments such as improvements in automatic segmentation, cross-domain model adaptation, inclusion of MLP based features, improvements in decoding, language modelling and vocal tract length normalisation, the use of a new decoder, and a new system architecture. This is followed by a comprehensive description of the final system and its performance in the NIST RT'06s evaluations. In comparison to the previous year word error rate results on the individual headset microphone task were reduced by 20\% relative.},
booktitle = {Proceedings of the Third International Conference on Machine Learning for Multimodal Interaction},
pages = {419–431},
numpages = {13},
location = {Bethesda, MD},
series = {MLMI'06}
}

@techreport{Tzinis,
abstract = {In this paper, we present an efficient neural network for end-to-end general purpose audio source separation. Specifically, the backbone structure of this convolutional network is the SUccessive DOwn-sampling and Resampling of Multi-Resolution Features (SuDoRM-RF) as well as their aggregation which is performed through simple one-dimensional convolutions. In this way, we are able to obtain high quality audio source separation with limited number of floating point operations, memory requirements, number of parameters and latency. Our experiments on both speech and environmental sound separation datasets show that SuDoRM-RF performs comparably and even surpasses various state-of-the-art approaches with significantly higher computational resource requirements.},
archivePrefix = {arXiv},
arxivId = {2007.06833v1},
author = {Tzinis, Efthymios and Wang, Zhepei and Smaragdis, Paris},
eprint = {2007.06833v1},
file = {:home/will/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tzinis, Wang, Smaragdis - Unknown - SUDO RM-RF EFFICIENT NETWORKS FOR UNIVERSAL AUDIO SOURCE SEPARATION.pdf:pdf},
keywords = {Index Terms-Audio source separation,deep learning,low-cost neural net-works},
title = {{SUDO RM-RF: EFFICIENT NETWORKS FOR UNIVERSAL AUDIO SOURCE SEPARATION}},
url = {https://github.com/etzinis/sudo}
}

@INPROCEEDINGS{rfield,
AUTHOR={Ravenscroft, W. and Goetze, S. and Hain, T.},
booktitle={EUSIPCO 2022},
title={{Receptive Field Analysis of Temporal Convolutional Networks for Monaural Speech Dereverberation}},
year={2022}, 
month=aug,
volume={},  number={},  pages={}}

﻿@Article{sudormrf,
author={Tzinis, Efthymios
and Wang, Zhepei
and Jiang, Xilin
and Smaragdis, Paris},
title={Compute and Memory Efficient Universal Sound Source Separation},
journal={Journal of Signal Processing Systems},
year={2022},
month={Feb},
day={01},
volume={94},
number={2},
pages={245-259},
abstract={Recent progress in audio source separation led by deep learning has enabled many neural network models to provide robust solutions to this fundamental estimation problem. In this study, we provide a family of efficient neural network architectures for general purpose audio source separation while focusing on multiple computational aspects that hinder the application of neural networks in real-world scenarios. The backbone structure of this convolutional network is the SUccessive DOwnsampling and Resampling of Multi-Resolution Features (SuDoRM-RF) as well as their aggregation which is performed through simple one-dimensional convolutions. This mechanism enables our models to obtain high fidelity signal separation in a wide variety of settings where a variable number of sources are present and with limited computational resources (e.g. floating point operations, memory footprint, number of parameters and latency). Our experiments show that SuDoRM-RF models perform comparably and even surpass several state-of-the-art benchmarks with significantly higher computational resource requirements. The causal variation of SuDoRM-RF is able to obtain competitive performance in real-time speech separation of around 10dB scale-invariant signal-to-distortion ratio improvement (SI-SDRi) while remaining up to 20 times faster than real-time on a laptop device.},
issn={1939-8115},
doi={10.1007/s11265-021-01683-x},
url={https://doi.org/10.1007/s11265-021-01683-x}
}

@article{Bi2022MultiScaleCN,
  title={Multi-Scale Convolutional Network for Space-Based ADS-B Signal Separation with Single Antenna},
  author={Yan Bi and Chuankun Li},
  journal={Applied Sciences},
  year={2022}
}

@INPROCEEDINGS{close2022,
AUTHOR={Close, G. and Hain, T. and Goetze, S.},
booktitle={EUSIPCO 2022},
title={{MetricGAN+/-: Increasing Robustness of Noise Reduction on Unseen Data}},
month={Aug.},
address      = {Belgrade, Serbia},
year={2022}, }

@INPROCEEDINGS{9882231,
  author={Jin, Rong and Ablimit, Mijit and Hamdulla, Askar},
  booktitle={2022 3rd International Conference on Pattern Recognition and Machine Learning (PRML)}, 
  title={Speech Separation and Emotion Recognition for Multi-speaker Scenarios}, 
  year={2022},
  volume={},
  number={},
  pages={280-284},
  doi={10.1109/PRML56267.2022.9882231}}

@INPROCEEDINGS{speakerbeam2,  author={Delcroix, Marc and Ochiai, Tsubasa and Zmolikova, Katerina and Kinoshita, Keisuke and Tawara, Naohiro and Nakatani, Tomohiro and Araki, Shoko},  booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},   title={Improving Speaker Discrimination of Target Speech Extraction With Time-Domain Speakerbeam},   year={2020},  volume={},  number={},  pages={691-695},  doi={10.1109/ICASSP40776.2020.9054683}}

@INPROCEEDINGS{dsconv,  author={Chollet, François},  booktitle={CVPR 2017},   title={Xception: Deep Learning with Depthwise Separable Convolutions},   year={2017},  volume={},  number={},  pages={1800-1807},  doi={10.1109/CVPR.2017.195}}

@article{dprnn,
abstract = {Recent studies in deep learning-based speech separation have proven the
superiority of time-domain approaches to conventional time-frequency-based
methods. Unlike the time-frequency domain approaches, the time-domain
separation systems often receive input sequences consisting of a huge number of
time steps, which introduces challenges for modeling extremely long sequences.
Conventional recurrent neural networks (RNNs) are not effective for modeling
such long sequences due to optimization difficulties, while one-dimensional
convolutional neural networks (1-D CNNs) cannot perform utterance-level
sequence modeling when its receptive field is smaller than the sequence length.
In this paper, we propose dual-path recurrent neural network (DPRNN), a simple
yet effective method for organizing RNN layers in a deep structure to model
extremely long sequences. DPRNN splits the long sequential input into smaller
chunks and applies intra- and inter-chunk operations iteratively, where the
input length can be made proportional to the square root of the original
sequence length in each operation. Experiments show that by replacing 1-D CNN
with DPRNN and apply sample-level modeling in the time-domain audio separation
network ({TasNet}), a new state-of-the-art performance on WSJ0-2mix is achieved
with a 20 times smaller model than the previous best system.},
archivePrefix = {arXiv},
arxivId = {1910.06379},
author = {Luo, Yi and Chen, Zhuo and Yoshioka, Takuya},
doi = {10.1109/ICASSP40776.2020.9054266},
eprint = {1910.06379},
file = {:home/will/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Luo, Chen, Yoshioka - 2019 - Dual-path RNN efficient long sequence modeling for time-domain single-channel speech separation.pdf:pdf},
isbn = {9781509066315},
issn = {15206149},
journal = {ICASSP 2020},
keywords = {Speech separation,deep learning,recurrent neural networks,time domain},
month = {oct},
pages = {46--50},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Dual-path {RNN}: efficient long sequence modeling for time-domain single-channel speech separation}},
url = {https://arxiv.org/abs/1910.06379v2},
volume = {2020-May},
year = {2019}
}
@book{sokal1958statistical,
  title={A Statistical Method for Evaluating Systematic Relationships},
  author={Sokal, R.R. and Michener, C.D. and University of Kansas},
  series={University of Kansas science bulletin},
  url={https://books.google.co.uk/books?id=o1BlHAAACAAJ},
  year={1958},
  publisher={University of Kansas}
}
@inproceedings{sepformer,
author = {Subakan, Cem and Ravanelli, Mirco and Cornell, Samuele and Bronzi, Mirko and Zhong, Jianyuan},
year = {2021},
month = {06},
pages = {21-25},
title = {Attention Is All You Need In Speech Separation},
booktitle = {Interspeech 2021},
doi = {10.1109/ICASSP39728.2021.9413901}
}

@inproceedings{Bahdanau2014,
  author    = {Dzmitry Bahdanau and
               Kyunghyun Cho and
               Yoshua Bengio},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Neural Machine Translation by Jointly Learning to Align and Translate},
  booktitle = {ICLR 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  month={May},
  year      = {2015},
  url       = {http://arxiv.org/abs/1409.0473},
  timestamp = {Wed, 17 Jul 2019 10:40:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/BahdanauCB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Stoller2018,
abstract = {Models for audio source separation usually operate on the magnitude spectrum, which ignores phase information and makes separation performance dependant on hyper-parameters for the spectral front-end. Therefore, we investigate end-to-end source separation in the time-domain, which allows modelling phase information and avoids fixed spectral transformations. Due to high sampling rates for audio, employing a long temporal input context on the sample level is difficult, but required for high quality separation results because of long-range temporal correlations. In this context, we propose the Wave-U-Net, an adaptation of the U-Net to the one-dimensional time domain, which repeatedly resamples feature maps to compute and combine features at different time scales. We introduce further architectural improvements, including an output layer that enforces source additivity, an upsampling technique and a context-aware prediction framework to reduce output artifacts. Experiments for singing voice separation indicate that our architecture yields a performance comparable to a state-of-the-art spectrogram-based U-Net architecture, given the same data. Finally, we reveal a problem with outliers in the currently used {SDR} evaluation metrics and suggest reporting rank-based statistics to alleviate this problem.},
archivePrefix = {arXiv},
arxivId = {1806.03185},
author = {Stoller, Daniel and Ewert, Sebastian and Dixon, Simon},
eprint = {1806.03185},
file = {:home/will/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Stoller, Ewert, Dixon - 2018 - Wave-U-Net A Multi-Scale Neural Network for End-to-End Audio Source Separation.pdf:pdf},
journal = {Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018},
month = {June},
pages = {334--340},
publisher = {International Society for Music Information Retrieval},
title = {{Wave-U-Net: A Multi-Scale Neural Network for End-to-End Audio Source Separation}},
url = {http://arxiv.org/abs/1806.03185},
year = {2018}
}

@inproceedings{Katharopoulos2020,
    author = {Katharopoulos, A. and Vyas, A. and Pappas, N. and Fleuret, F.},
    title = {Transformers are {RNN}s: Fast Autoregressive Transformers with Linear Attention},
    booktitle = {ICML 2020},
    year = {2020}
}

@ARTICLE{8369155,
  author={Wang, DeLiang and Chen, Jitong},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 
  title={Supervised Speech Separation Based on Deep Learning: An Overview}, 
  year={2018},
  volume={26},
  number={10},
  pages={1702-1726},
  doi={10.1109/TASLP.2018.2842159}}
  
@INPROCEEDINGS{pyroomacoustics,  author={Scheibler, R. and Bezzam, E. and Dokmanić, I.},  booktitle={ICASSP 2018},   title={Pyroomacoustics: A Python Package for Audio Room Simulation and Array Processing Algorithms},   year={2018},  volume={},  number={}, month=apr,  doi={10.1109/ICASSP.2018.8461310}}

@inproceedings{li2020espnet,
  title={{ESPnet-SE}: End-to-End Speech Enhancement and Separation Toolkit Designed for {ASR} Integration},
  author={Chenda Li and Jing Shi and Wangyou Zhang and Aswin Shanmugam Subramanian and Xuankai Chang and Naoyuki Kamo and Moto Hira and Tomoki Hayashi and Christoph Boeddeker and Zhuo Chen and Shinji Watanabe},
  booktitle={Proceedings of IEEE Spoken Language Technology Workshop (SLT)},
  pages={785--792},
  year={2021},
  organization={IEEE},
}
@inproceedings{watanabe2018espnet,
  author={Shinji Watanabe and Takaaki Hori and Shigeki Karita and Tomoki Hayashi and Jiro Nishitoba and Yuya Unno and Nelson {Enrique Yalta Soplin} and Jahn Heymann and Matthew Wiesner and Nanxin Chen and Adithya Renduchintala and Tsubasa Ochiai},
  title={{ESPnet}: End-to-End Speech Processing Toolkit},
  year={2018},
  booktitle={Interspeech 2018},
  pages={2207--2211},
  doi={10.21437/Interspeech.2018-1456},
  url={http://dx.doi.org/10.21437/Interspeech.2018-1456}
}

@InProceedings{tcn,
author="Lea, Colin
and Vidal, Ren{\'e}
and Reiter, Austin
and Hager, Gregory D.",
editor="Hua, Gang
and J{\'e}gou, Herv{\'e}",
title="Temporal Convolutional Networks: A Unified Approach to Action Segmentation",
booktitle="Computer Vision -- ECCV 2016 Workshops",
year="2016",
publisher="Springer International Publishing",
address="Cham",
pages="47--54",
abstract="The dominant paradigm for video-based action segmentation is composed of two steps: first, compute low-level features for each frame using Dense Trajectories or a Convolutional Neural Network to encode local spatiotemporal information, and second, input these features into a classifier such as a Recurrent Neural Network (RNN) that captures high-level temporal relationships. While often effective, this decoupling requires specifying two separate models, each with their own complexities, and prevents capturing more nuanced long-range spatiotemporal relationships. We propose a unified approach, as demonstrated by our Temporal Convolutional Network (TCN), that hierarchically captures relationships at low-, intermediate-, and high-level time-scales. Our model achieves superior or competitive performance using video or sensor data on three public action segmentation datasets and can be trained in a fraction of the time it takes to train an RNN.",
isbn="978-3-319-49409-8"
}
@inproceedings{dptnet,
author = {Chen, Jingjing and Mao, Qirong and Liu, Dong},
year = {2020},
month = {10},
pages = {2642-2646},
title = {Dual-Path Transformer Network: Direct Context-Aware Modeling for End-to-End Monaural Speech Separation},
doi = {10.21437/Interspeech.2020-2205}
}

@INPROCEEDINGS{7404837,  author={J. {Barker} and R. {Marxer} and E. {Vincent} and S. {Watanabe}},  booktitle={2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)},   title={The third ‘CHiME’ speech separation and recognition challenge: Dataset, task and baselines},   year={2015},  volume={},  number={},  pages={504-511},  doi={10.1109/ASRU.2015.7404837}}
@article{BARKER2017605,
title = "The third ‘CHiME’ speech separation and recognition challenge: Analysis and outcomes",
journal = "Computer Speech \& Language",
volume = "46",
pages = "605 - 626",
year = "2017",
issn = "0885-2308",
doi = "https://doi.org/10.1016/j.csl.2016.10.005",
url = "http://www.sciencedirect.com/science/article/pii/S088523081630122X",
author = "Jon Barker and Ricard Marxer and Emmanuel Vincent and Shinji Watanabe",
keywords = "Noise-robust ASR, Microphone array, ‘CHiME’ challenge",
abstract = "This paper presents the design and outcomes of the CHiME-3 challenge, the first open speech recognition evaluation designed to target the increasingly relevant multichannel, mobile-device speech recognition scenario. The paper serves two purposes. First, it provides a definitive reference for the challenge, including full descriptions of the task design, data capture and baseline systems along with a description and evaluation of the 26 systems that were submitted. The best systems re-engineered every stage of the baseline resulting in reductions in word error rate from 33.4\% to as low as 5.8\%. By comparing across systems, techniques that are essential for strong performance are identified. Second, the paper considers the problem of drawing conclusions from evaluations that use speech directly recorded in noisy environments. The degree of challenge presented by the resulting material is hard to control and hard to fully characterise. We attempt to dissect the various ‘axes of difficulty’ by correlating various estimated signal properties with typical system performance on a per session and per utterance basis. We find strong evidence of a dependence on signal-to-noise ratio and channel quality. Systems are less sensitive to variations in the degree of speaker motion. The paper concludes by discussing the outcomes of CHiME-3 in relation to the design of future mobile speech recognition evaluations."
}

@article{Schmidt,
abstract = {We apply machine learning techniques to the problem of separating multiple speech sources from a single microphone recording. The method of choice is a sparse non-negative matrix factorization algorithm, which in an unsupervised manner can learn sparse representations of the data. This is applied to the learning of person-alized dictionaries from a speech corpus, which in turn are used to separate the audio stream into its components. We show that computational savings can be achieved by segmenting the training data on a phoneme level. To split the data, a conventional speech recognizer is used. The performance of the unsupervised and supervised adaptation schemes result in significant improvements in terms of the target-to-masker ratio. Index Terms: Single-channel source separation, sparse non-negative matrix factorization.},
author = {Schmidt, Mikkel N and Olsson, Rasmus K},
file = {:home/will/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schmidt, Olsson - Unknown - Single-Channel Speech Separation using Sparse Non-Negative Matrix Factorization.pdf:pdf},
title = {{Single-Channel Speech Separation using Sparse Non-Negative Matrix Factorization}},
year={2006},
booktitle={Interspeech 2006},
month={sep}
}

@INPROCEEDINGS{Roux2015,  author={Le Roux, Jonathan and Hershey, John R. and Weninger, Felix},  booktitle={ICASSP 2015},   title={Deep NMF for speech separation},   year={2015},  volume={},  number={},  pages={66-70},  doi={10.1109/ICASSP.2015.7177933}}

@article{doi:10.1121/1.381172,
author = {Parsons, Thomas W},
doi = {10.1121/1.381172},
journal = {The Journal of the Acoustical Society of America},
number = {4},
pages = {911--918},
title = {{Separation of speech from interfering speech by means of harmonic selection}},
url = {https://doi.org/10.1121/1.381172},
volume = {60},
year = {1976}
}

@proceedings{49414,
title	= {Conformer: Convolution-augmented Transformer for Speech Recognition},
editor	= {Anmol Gulati and Chung-Cheng Chiu and James Qin and Jiahui Yu and Niki Parmar and Ruoming Pang and Shibo Wang and Wei Han and Yonghui Wu and Yu Zhang and Zhengdong Zhang},
year	= {2020}
}


@inproceedings{Heymann2016,
abstract = {We present a new beamformer front-end for Automatic Speech Recognition and apply it to the 3rd-CHiME Speech Separation and Recognition Challenge. Without any further modification of the back-end, we achieve a 53{\%} relative reduction of the word error rate over the best baseline enhancement system for the relevant test data set. Our approach leverages the power of a bi-directional Long Short-Term Memory network to robustly estimate soft masks for a subsequent beamforming step. The utilized Generalized Eigenvalue beamforming operation with an optional Blind Analytic Normalization does not rely on a Direction-of-Arrival estimate and can cope with multi-path sound propagation, while at the same time only introducing very limited speech distortions. Our quite simple setup exploits the possibilities provided by simulated training data while still being able to generalize well to the fairly different real data. Finally, combining our front-end with data augmentation and another language model nearly yields a 64 {\%} reduction of the word error rate on the real data test set.},
author = {Heymann, Jahn and Drude, Lukas and Chinaev, Aleksej and Haeb-Umbach, Reinhold},
booktitle = {ASRU 2015},
doi = {10.1109/ASRU.2015.7404829},
file = {:home/will/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Heymann et al. - 2016 - BLSTM supported {GEV} beamformer front-end for the 3RD CHiME challenge.pdf:pdf},
isbn = {9781479972913},
keywords = {Beamforming,Feature Enhancement,Neural Networks,Robust Speech Recognition},
month = {February},
pages = {444--451},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{BLSTM supported {GEV} beamformer front-end for the 3RD CHiME challenge}},
year = {2016}
}
@InProceedings{Drude2018NaraWPE,
  Title     = {{NARA-WPE}: A Python package for weighted prediction error dereverberation in {Numpy} and {Tensorflow} for online and offline processing},
  Author    = {Drude, Lukas and Heymann, Jahn and Boeddeker, Christoph and Haeb-Umbach, Reinhold},
  Booktitle = {13. ITG Fachtagung Sprachkommunikation (ITG 2018)},
  Year      = {2018},
  Month     = {Oct},
}
@book{HeymannWPE,
author = {Heymann, Jahn and Drude, Lukas and Haeb-Umbach, Reinhold and Kinoshita, Keisuke and Nakatani, Tomohiro},
file = {:home/will/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Heymann et al. - Unknown - JOINT OPTIMIZATION OF NEURAL NETWORK-BASED WPE DEREVERBERATION AND ACOUSTIC MODEL FOR ROBUST ONLINE ASR.pdf:pdf},
isbn = {9781538646588},
keywords = {Index Terms-dereverberation,joint optimization,robust ASR,speech enhancement},
title = {{Joint Optimization of Neural Network-based WPE Dereverberation and Acoustic Model for Robust Online ASR}},
url = {https://www.github.com/fgnt/nara{\_}wpe},
year = {2019}
}

@inproceedings{Higuchi2016,
abstract = {This paper considers acoustic beamforming for noise robust automatic speech recognition (ASR). A beamformer attenuates background noise by enhancing sound components coming from a direction specified by a steering vector. Hence, accurate steering vector estimation is paramount for successful noise reduction. Recently, a beamforming approach was proposed that employs time-frequency masks. In the speech recognition system we submitted to the CHiME-3 Challenge, we employed a new form of this approach that uses a speech spectral model based on a complex Gaussian mixture model (CGMM) to estimate the time-frequency masks and the steering vector without providing technical details. This paper elaborates on this technique and examines its effectiveness for ASR. Experimental results show that the CGMM-based approach outperforms a recently proposed mask estimator based on a Watson mixture model. In addition, the CGMM-based approach is extended to an online speech enhancement scenario, which allows this technique to be used in an online recognition setup. This online version reduces the CHiME-3 evaluation error rate from 15.60{\%} to 8.47{\%}, which is a comparable improvement to that obtained by batch processing.},
author = {Higuchi, Takuya and Ito, Nobutaka and Yoshioka, Takuya and Nakatani, Tomohiro},
booktitle = {ICASSP 2016},
doi = {10.1109/ICASSP.2016.7472671},
isbn = {9781479999880},
issn = {15206149},
keywords = {CHiME-3,Noise robust speech recognition,beamforming,speech enhancement},
month = {May},
pages = {5210--5214},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Robust {MVDR} beamforming using time-frequency masks for online/offline ASR in noise}},
volume = {2016-May},
year = {2016}
}

@article{Hochreiter1997,
abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient-based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
author = {Hochreiter, Sepp and Schmidhuber, J{\"{u}}rgen},
doi = {10.1162/neco.1997.9.8.1735},
issn = {08997667},
journal = {Neural Computation},
month = {November},
number = {8},
pages = {1735--1780},
pmid = {9377276},
publisher = {MIT Press Journals},
title = {{Long Short-Term Memory}},
volume = {9},
year = {1997}
}
@misc{Johnson2014,
abstract = {Background: To undertake a systematic review of existing literature relating to speech recognition technology and its application within health care. Methods: A systematic review of existing literature from 2000 was undertaken. Inclusion criteria were: all papers that referred to speech recognition (SR) in health care settings, used by health professionals (allied health, medicine, nursing, technical or support staff), with an evaluation or patient or staff outcomes. Experimental and non-experimental designs were considered. Six databases (Ebscohost including CINAHL, EMBASE, MEDLINE including the Cochrane Database of Systematic Reviews, OVID Technologies, PreMED-LINE, PsycINFO) were searched by a qualified health librarian trained in systematic review searches initially capturing 1,730 references. Fourteen studies met the inclusion criteria and were retained. Results: The heterogeneity of the studies made comparative analysis and synthesis of the data challenging resulting in a narrative presentation of the results. SR, although not as accurate as human transcription, does deliver reduced turnaround times for reporting and cost-effective reporting, although equivocal evidence of improved workflow processes. Conclusions: SR systems have substantial benefits and should be considered in light of the cost and selection of the SR system, training requirements, length of the transcription task, potential use of macros and templates, the presence of accented voices or experienced and in-experienced typists, and workflow patterns.},
annote = {https://link-springer-com.sheffield.idm.oclc.org/content/pdf/10.1186/1472-6947-14-94.pdf},
author = {Johnson, Maree and Lapkin, Samuel and Long, Vanessa and Sanchez, Paula and Suominen, Hanna and Basilakis, Jim and Dawson, Linda},
booktitle = {BMC Medical Informatics and Decision Making},
doi = {10.1186/1472-6947-14-94},
file = {:home/will/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Johnson et al. - 2014 - A systematic review of speech recognition technology in health care.pdf:pdf},
issn = {14726947},
keywords = {Health professionals,Human transcriptions,Interactive voice response systems,Nursing,Speech recognition,Systematic review},
month = {oct},
number = {1},
pages = {1--14},
publisher = {BioMed Central Ltd.},
title = {{A systematic review of speech recognition technology in health care}},
volume = {14},
year = {2014}
}

@inproceedings{dnnwpe,
  author={K. Kinoshita and M. Delcroix and H. Kwon and T. Mori and T. Nakatani},
  title={{Neural Network-Based Spectrum Estimation for Online WPE Dereverberation}},
  year=2017,
  booktitle={Interspeech 2017},
  month=aug,
  doi={10.21437/Interspeech.2017-733}
}

@INPROCEEDINGS{8553141,  author={Ernst, O. and Chazan, S. E. and Gannot, S. and Goldberger, J.},  booktitle={EUSIPCO 2018},   title={Speech Dereverberation Using Fully Convolutional Networks}, month=sep,   year={2018},  volume={},  number={},   doi={10.23919/EUSIPCO.2018.8553141}}

@techreport{Liu2016,
author = {Liu, Yulan},
file = {:home/will/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu - 2016 - Distant Speech Recognition of Natural Spontaneous Multi-party Conversations.pdf:pdf},
title = {{Distant Speech Recognition of Natural Spontaneous Multi-party Conversations}},
year = {2016}
}

@techreport{Mohri,
abstract = {We survey the use of weighted finite-state transducers (WFSTs) in speech recognition. We show that WFSTs provide a common and natural representation for HMM models, context-dependency, pronunciation dictionaries , grammars, and alternative recognition outputs. Furthermore, general transducer operations combine these representations flexibly and efficiently. Weighted determinization and minimization algorithms optimize their time and space requirements, and a weight pushing algorithm distributes the weights along the paths of a weighted transducer optimally for speech recognition. As an example, we describe a North American Business News (NAB) recognition system built using these techniques that combines the HMMs, full crossword triphones, a lexicon of forty thousand words, and a large trigram grammar into a single weighted transducer that is only somewhat larger than the trigram word grammar and that runs NAB in real-time on a very simple decoder. In another example, we show that the same techniques can be used to optimize lattices for second-pass recognition. In a third example, we show how general automata operations can be used to assemble lattices from different recognizers to improve recognition performance.},
author = {Mohri, Mehryar and Pereira, Fernando and Riley, Michael},
file = {:home/will/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mohri, Pereira, Riley - Unknown - Weighted Finite-State Transducers in Speech Recognition.pdf:pdf},
title = {{Weighted Finite-State Transducers in Speech Recognition}}
}
@techreport{Fox,
abstract = {Recognition of speech in natural environments is a challenging task, even more so if this involves conversations between several speakers. Work on meeting recognition has addressed some of the significant challenges, mostly targeting formal, business style meetings where people are mostly in a static position in a room. Only limited data is available that contains high quality near and far field data from real interactions between participants. In this paper we present a new corpus for research on speech recognition, speaker tracking and diarisation, based on recordings of native speakers of English playing a table-top wargame. The Sheffield Wargames Corpus comprises 7 hours of data from 10 recording sessions, obtained from 96 microphones , 3 video cameras and, most importantly, 3D location data provided by a sensor tracking system. The corpus represents a unique resource, that provides for the first time location tracks (1.3Hz) of speakers that are constantly moving and talking. The corpus is available for research purposes, and includes annotated development and evaluation test sets. Baseline results for close-talking and far field sets are included in this paper.},
author = {Fox, Charles and Liu, Yulan and Zwyssig, Erich and Hain, Thomas},
file = {::},
title = {{The Sheffield Wargames Corpus}},
url = {http://www.cstr.inf.ed.ac.uk/research/}
}

@ARTICLE{5547558,  author={Nakatani, Tomohiro and Yoshioka, Takuya and Kinoshita, Keisuke and Miyoshi, Masato and Juang, Biing-Hwang},  journal={IEEE Transactions on Audio, Speech, and Language Processing},   title={Speech Dereverberation Based on Variance-Normalized Delayed Linear Prediction},   year={2010},  volume={18},  number={7},  pages={1717-1731},  doi={10.1109/TASL.2010.2052251}}

@techreport{Ochiai2017,
abstract = {This paper proposes a unified architecture for end-to-end automatic speech recognition (ASR) to encompass microphone-array signal processing such as a state-of-the-art neural beamformer within the end-to-end framework. Recently, the end-to-end ASR paradigm has attracted great research interest as an alternative to conventional hybrid paradigms with deep neural networks and hidden Markov models. Using this novel paradigm, we simplify ASR architecture by integrating such ASR components as acoustic, phonetic, and language models with a single neural network and optimize the overall components for the end-to-end ASR objective: generating a correct label sequence. Although most existing end-to-end frameworks have mainly focused on ASR in clean environments, our aim is to build more realistic end-to-end systems in noisy environments. To handle such challenging noisy ASR tasks, we study multi-channel end-to-end ASR architecture, which directly converts multichannel speech signal to text through speech enhancement. This architecture allows speech enhancement and ASR components to be jointly optimized to improve the end-to-end ASR objective and leads to an end-to-end framework that works well in the presence of strong background noise. We elaborate the effectiveness of our proposed method on the multichannel ASR benchmarks in noisy environments (CHiME-4 and AMI). The experimental results show that our proposed multichannel end-to-end system obtained performance gains over the conventional end-to-end baseline with enhanced inputs from a delay-and-sum beamformer (i.e., BeamformIT) in terms of character error rate. In addition, further analysis shows that our neural beamformer, which is optimized only with the end-to-end ASR objective, successfully learned a noise suppression function. Abstract-This paper proposes a unified architecture for end-to-end automatic speech recognition (ASR) to encompass microphone-array signal processing such as a state-of-the-art neural beamformer within the end-to-end framework. Recently, the end-to-end ASR paradigm has attracted great research interest as an alternative to conventional hybrid paradigms with deep neural networks and hidden Markov models. Using this novel paradigm, we simplify ASR architecture by integrating such ASR components as acoustic, phonetic, and language models with a single neural network and optimize the overall components for the end-to-end ASR objective: generating a correct label sequence. Although most existing end-to-end frameworks have mainly focused on ASR in clean environments, our aim is to build more realistic end-to-end systems in noisy environments. To handle such challenging noisy ASR tasks, we study multichannel end-to-end ASR architecture, which directly converts multichan-nel speech signal to text through speech enhancement. This architecture allows speech enhancement and ASR components to be jointly optimized to improve the end-to-end ASR objective and leads to an end-to-end framework that works well in the presence of strong background noise. We elaborate the effectiveness of our proposed method on the multichannel ASR benchmarks in noisy environments (CHiME-4 and AMI). The experimental results show that our proposed multichannel end-to-end system obtained performance gains over the conventional end-to-end baseline with enhanced inputs from a delay-and-sum beamformer (i.e., BeamformIT) in terms of character error rate. In addition, further analysis shows that our neural beamformer, which is optimized only with the end-to-end ASR objective, successfully learned a noise suppression function. Index Terms-multichannel end-to-end ASR, neural beam-former, encoder-decoder network.},
author = {Ochiai, Tsubasa and Member, Student and Watanabe, Shinji and Member, Senior and Hori, Takaaki and Hershey, John R and Xiao, Xiong},
file = {:home/will/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ochiai et al. - 2017 - Unified Architecture for Multichannel End-to-End Speech Recognition with Neural Beamforming IEEE Journal of Selec.pdf:pdf},
title = {{Unified Architecture for Multichannel End-to-End Speech Recognition with Neural Beamforming IEEE Journal of Selected Topics in Signal Processing Unified Architecture for Multichannel End-to-End Speech Recognition with Neural Beamforming}},
url = {http://www.merl.com},
year = {2017}
}
@inproceedings{Ochiai2017a,
abstract = {The field of speech recognition is in the midst of a paradigm shift: end-to-end neural networks are challenging the dominance of hidden Markov models as a core technology. Using an attention mechanism in a recurrent encoder-decoder architecture solves the dynamic time alignment problem, allowing joint end-to-end training of the acoustic and language modeling components. In this paper we extend the end-to-end framework to cncompass microphone array signal processing for noise suppression and speech enhancement within the acoustic encoding network. This allows the beamforming components to be optimized jointly within the recognition architecture to improve the end-to-end speech recognition objective. Experiments on the noisy speech benchmarks (CHiME-4 and AMI) show that our multichannel end-to-end system outperformed the attention-based baseline with input from a conventional adaptive beamformer.},
archivePrefix = {arXiv},
arxivId = {1703.04783},
author = {Ochiai, Tsubasa and Watanabe, Shinji and Hori, Takaaki and Hershey, John R.},
booktitle = {34th International Conference on Machine Learning, ICML 2017},
eprint = {1703.04783},
file = {:home/will/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ochiai et al. - 2017 - Multichannel end-to-end speech recognition.pdf:pdf},
isbn = {9781510855144},
month = {March},
pages = {4033--4042},
publisher = {International Machine Learning Society (IMLS)},
title = {{Multichannel end-to-end speech recognition}},
url = {https://arxiv.org/abs/1703.04783v1},
volume = {6},
year = {2017}
}
@article{Salloum2017,
author = {Salloum, Wael and Edwards, Erik and Ghaffarzadegan, Shabnam and Suendermann-Oeft, David and Miller, Mark},
journal = {undefined},
title = {{Crowdsourced Continuous Improvement of Medical Speech Recognition}},
year = {2017}
}
@article{Seltzer2004,
abstract = {Speech recognition performance degrades significantly in distant-talking environments, where the speech signals can be severely distorted by additive noise and reverberation. In such environments, the use of microphone arrays has been proposed as a means of improving the quality of captured speech signals. Currently, microphone-array-based speech recognition is performed in two independent stages: array processing and then recognition. Array processing algorithms, designed for signal enhancement, are applied in order to reduce the distortion in the speech waveform prior to feature extraction and recognition. This approach assumes that improving the quality of the speech wave-form will necessarily result in improved recognition performance and ignores the manner in which speech recognition systems operate. In this paper a new approach to microphone-array processing is proposed in which the goal of the array processing is not to generate an enhanced output waveform but rather to generate a sequence of features which maximizes the likelihood of generating the correct hypothesis. In this approach, called likelihood-maximizing beamforming, information from the speech recognition system itself is used to optimize a filter-and-sum beamformer. Speech recognition experiments performed in a real distant-talking environment confirm the efficacy of the proposed approach. Index Terms-Adaptive filtering, beamforming, distant-talking environments, microphone array processing, robust speech recognition .},
author = {Seltzer, Michael L and Raj, Bhiksha and Stern, Richard M},
doi = {10.1109/TSA.2004.832988},
file = {:home/will/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Seltzer, Raj, Stern - 2004 - Likelihood-Maximizing Beamforming for Robust Hands-Free Speech Recognition.pdf:pdf},
journal = {IEEE TRANSACTIONS ON SPEECH AND AUDIO PROCESSING},
number = {5},
title = {{Likelihood-Maximizing Beamforming for Robust Hands-Free Speech Recognition}},
volume = {12},
year = {2004}
}
@inproceedings{PB2018CHiME5,
  author    = {Boeddeker, Christoph and Heitkaemper, Jens and Schmalenstroeer, Joerg and Drude, Lukas and Heymann, Jahn and Haeb-Umbach, Reinhold},
  title     = {{Front-End Processing for the CHiME-5 Dinner Party Scenario}},
  year      = {2018},
  booktitle = {CHiME5 Workshop},
}
@article{VANCOMPERNOLLE1990433,
title = "Speech recognition in noisy environments with the aid of microphone arrays",
journal = "Speech Communication",
volume = "9",
number = "5",
pages = "433 - 442",
year = "1990",
note = "Neuropeech '89",
issn = "0167-6393",
doi = "https://doi.org/10.1016/0167-6393(90)90019-6",
url = "http://www.sciencedirect.com/science/article/pii/0167639390900196",
author = "Dirk {Van Compernolle} and Weiye Ma and Fei Xie and Marc {Van Diest}",
keywords = "Speech recognition, microphone array processing, noisy environments",
abstract = "This paper presents a microphone array adaptive beamformer with a dual function. The noise enhanced output is suited to transmission as well as to use as input to speech recognition systems. The areas of use envisaged are the car. the factory floor and noisy offices. The underlying structure is a steered Griffiths-Jim beamformer, with an added speech detection switch for the selective adaptation of both sections. This beamformer is effective in suppressing both stationary and non-stationary interference and is therefore a preprocessor for a wider range of speech recognition applications than any single channel noise suppression scheme could handle. Experiments were performed in a reverberant room with a 4-microphone array. Typical SNR improvements for communication purposes range from 4 to 12 dB. The effective SNR improvement for speech recognition purposes ranges from 4 to 8 dB.
Zusammenfassung
Dieser Beitrag beschreibt eine gerichtete Anordnung mehrerer Mikrofone, die eine doppelte Funktion besitzt. Das Ausgangssignal - mit verbessertem Störabstand - eignet sich einerseits zu Übertragungszwecken, andererseits als Eingangssignal für ein Spracherkennungssystem. Das System ist für Umfeldbedingungen gedacht, wie man sie im Fahrzeug, im Produktionsbereich einer Fabrik oder in einem geräuscherfüllten Büroraum antrifft. Die Grundstruktur ist ein gesteuerter Griffiths-Jim-Strahlenformer mit einem Zusatzschalter zur Sprachdetektion, der zur Selektion eines der beiden Einsatzbereiche benutzt wird. Dieser Strahlenformer verfügt über eine hohe Leistungsfähigkeit bei der Ausschaltung sowohl gleichbleibender als auch wechselnder Interferenzen und ist auf diese Weise jedenfalls als Präprozessor für eine breitere Palette von Einsätzen im Bereich der Spracherkennung zu verwenden als jedes einkanalige Geräuschunterdrükkungssystem. Versuche wurden mit einer Anordnung von 4 Mikrophonen wurden in einem Hallraum durchgeführt. Typische Werte für die Verbesserung des Störabstandes beim Einsatz zu Übertragungszwecken liegen zwischen 4 und 12 dB. Für die Spracherkennung liegen die effektiven Verbesserungen des Signal-Geräuschabstandes im Bereich von 4 bis 8 dB.
Résumé
Nous présentons un formateur de faisceau à double fonction applicable dans le cas d'un système de reconnaissance La sortie à niveau de bruit diminué convient aussi bien pour la transmission que pour l'entrée d'un système de reconnaissance de la parole. Les conditions ambiantes visées sont la voiture, l'atelier ou un bureau bruyant. La structure sous-jacente est constituée par un formateur de faisceau Griffiths-Jim dirigé, avec interrupteur supplémentaire de détection de la parole à utiliser pour l'adaptation sélective des deux fonctions. Ce formateur de faisceau est efficace pour la suppression des interférences stationnaires et des interférences non-stationnaires et constitue de la sorte une unité de pré-traitement destinée à une gamme d'applications de reconnaissance de la parole plus large que celles pourrait traiter un système de suppression de bruit de fond à canal unique. Des expériences ont été réalisées dans une chambre à écho équipée d'un réseau à 4 microphones. Les améliorations typiques en matière de rapports signal/bruit à des fins de communication se situent entre 4 et 12 dB. L'amélioration effective du rapport signal/bruit à des fins de reconnaissance de la parole se situe entre 4 et 8 dB."
}
@ARTICLE{CoxMVDR,  author={H. {Cox} and R. {Zeskind} and M. {Owen}},  journal={IEEE Transactions on Acoustics, Speech, and Signal Processing},   title={Robust adaptive beamforming},   year={1987},  volume={35},  number={10},  pages={1365-1376}}

@incollection{Shagoury2010,
abstract = {This chapter discusses speech recognition's (SR) proven ability to enhance the quality of patient care by increasing the speed of the medical documentation-process. The author addresses the history of medical dictation and its evolution to SR, along with the vast technical improvements to speech technologies over the past 30 years. Using real-world examples, this work richly demonstrates how the use of SR techno-logy directly affects improved productivity in hospitals, significant cost reductions, and overall quality improvements in the physician's ability to deliver optimal healthcare. The chapter also recognizes that beyond the core application of speech technologies to hospitals and primary care practitioners, SR is a core tool within the diagnostics field of healthcare, with broad adoption levels within the radiology department. In presenting these findings, the author examines natural language processing and most excitingly, the next generation of SR. After reading this chapter, the reader will become familiar with the high price of traditional medical transcription vis-{\`{a}}-vis the benefits of incorporating SR as part of the everyday clinical documentation workflow. {\textcopyright} 2010 Springer Science+Business Media, LLC.},
author = {Shagoury, John},
booktitle = {Advances in Speech Recognition: Mobile Environments, Call Centers and Clinics},
doi = {10.1007/978-1-4419-5951-5_11},
isbn = {9781441959508},
keywords = {Electronic health records (EHR),Front-end and background speech recognition,Medical transcription,Natural language processing (NLP),PACS,Quality of patient care,RIS,Radiology reports,Record-keeping errors,Speech-enabled electronic medical records (EMR) sy},
pages = {247--273},
publisher = {Springer US},
title = {{Dr. multi-task: Using speech to build up electronic medical records while caring for patients}},
year = {2010}
}

@article{ba2016layer,
  author    = {Lei Jimmy Ba and
               Jamie Ryan Kiros and
               Geoffrey E. Hinton},
  title     = {Layer Normalization},
  year      = {2016},
  eprinttype = {arXiv},
  eprint    = {1607.06450},
  timestamp = {Tue, 23 Jul 2019 17:33:23 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/BaKH16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  archivePrefix={arXiv},
  primaryClass={eess.AS},
  note={arXiv:2106.04624}
}

@INPROCEEDINGS{srmr,  author={Santos, J. F. and Senoussaoui, M. and Falk, T. H.},  booktitle={IWAENC 2014},   title={{An improved non-intrusive intelligibility metric for noisy and reverberant speech}},   year={2014},  volume={},  month=sep, number={},   doi={10.1109/IWAENC.2014.6953337}}

@ARTICLE{estoi,  author={Jensen, J. and Taal, C. H.},  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},   title={An Algorithm for Predicting the Intelligibility of Speech Masked by Modulated Noise Maskers},   year={2016},  volume={24},  number={11},  pages={2009-2022},  doi={10.1109/TASLP.2016.2585878}}

@Inbook{Bitzer2001,
author="Bitzer, Joerg
and Simmer, K. Uwe",
editor="Brandstein, Michael
and Ward, Darren",
title="Superdirective Microphone Arrays",
bookTitle="Microphone Arrays: Signal Processing Techniques and Applications",
year="2001",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="19--38",
abstract="This chapter gives an overview of so-called superdirective beamformers, which can be derived by applying the minimum variance distortionless response ({MVDR}) principle to theoretically well-defined noise fields, as for example the diffuse noise field. We show that all relevant performance measures for beamformer designs are functions of the coherence matrix of the noise field. Additionally, we present unconstrained and constrained {MVDR}-solutions using modified coherence functions. Solutions for different choices of the optimization criterion are given including a new solution to optimize the front-to-back ratio. Finally, we present a comparison of superdirective beamformers to gradient microphones and an alternative generalized sidelobe canceler (GSC) implementation of the superdirective beamformer.",
isbn="978-3-662-04619-7",
doi="10.1007/978-3-662-04619-7_2",
url="https://doi.org/10.1007/978-3-662-04619-7_2"
}

@inproceedings{Snyder2018,
abstract = {In this paper, we use data augmentation to improve performance of deep neural network (DNN) embeddings for speaker recognition. The DNN, which is trained to discriminate between speakers, maps variable-length utterances to fixed-dimensional embeddings that we call x-vectors. Prior studies have found that embeddings leverage large-scale training datasets better than i-vectors. However, it can be challenging to collect substantial quantities of labeled data for training. We use data augmentation, consisting of added noise and reverberation, as an inexpensive method to multiply the amount of training data and improve robustness. The x-vectors are compared with i-vector baselines on Speakers in the Wild and NIST SRE 2016 Cantonese. We find that while augmentation is beneficial in the PLDA classifier, it is not helpful in the i-vector extractor. However, the x-vector DNN effectively exploits data augmentation, due to its supervised training. As a result, the x-vectors achieve superior performance on the evaluation datasets.},
author = {Snyder, David and Garcia-Romero, Daniel and Sell, Gregory and Povey, Daniel and Khudanpur, Sanjeev},
booktitle = {ICASSP 2018},
doi = {10.1109/ICASSP.2018.8461375},
file = {:home/will/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Snyder et al. - 2018 - X-Vectors Robust DNN Embeddings for Speaker Recognition.pdf:pdf},
isbn = {9781538646588},
issn = {15206149},
keywords = {Data augmentation,Deep neural networks,Speaker recognition,X-vectors},
month = {sep},
pages = {5329--5333},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{X-Vectors: Robust DNN Embeddings for Speaker Recognition}},
volume = {2018-April},
year = {2018}
}
@INPROCEEDINGS{WSJCAM0,  author={T. {Robinson} and J. {Fransen} and D. {Pye} and J. {Foote} and S. {Renals}},  booktitle={1995 International Conference on Acoustics, Speech, and Signal Processing},   title={{WSJCAM0}: a British English speech corpus for large vocabulary continuous speech recognition},   year={1995},  volume={1},  number={},  pages={81-84 vol.1},  doi={10.1109/ICASSP.1995.479278}}
@article{REVERB,
abstract = {In recent years, substantial progress has been made in the field of reverberant speech signal processing, including both single-and multichannel dereverberation techniques and automatic speech recognition (ASR) techniques that are robust to reverberation. In this paper, we describe the REVERB challenge, which is an evaluation campaign that was designed to evaluate such speech enhancement (SE) and ASR techniques to reveal the state-of-the-art techniques and obtain new insights regarding potential future research directions. Even though most existing benchmark tasks and challenges for distant speech processing focus on the noise robustness issue and sometimes only on a single-channel scenario, a particular novelty of the REVERB challenge is that it is carefully designed to test robustness against reverberation, based on both real, single-channel, and multichannel recordings. This challenge attracted 27 papers, which represent 25 systems specifically designed for SE purposes and 49 systems specifically designed for ASR purposes. This paper describes the problems dealt within the challenge, provides an overview of the submitted systems, and scrutinizes them to clarify what current processing strategies appear effective in reverberant speech processing.},
author = {Kinoshita, K. and Delcroix, M. and Gannot, Sharon and Habets, E. A. P. and Haeb-Umbach, R. and Kellermann, W. and Leutnant, V. and Maas, R. and Nakatani, T. and Raj, B. and Sehr, A. and Yoshioka, T.},
doi = {10.1186/s13634-016-0306-6},
journal = {EURASIP Journal on Advances in Signal Processing},
keywords = {Automatic speech recognition,Dereverberation,Evaluation campaign,REVERB challenge,Reverberation},
pages = {7},
title = {{A summary of the REVERB challenge: state-of-the-art and remaining challenges in reverberant speech processing research}},
volume = {2016},
year = {2011}
}
@techreport{Warsitz,
abstract = {Broadband adaptive beamformers, which use a narrowband SNR-maximization optimization criterion for noise reduction, typically cause distortions of the desired speech signal at the beam-former output. In this paper two methods are investigated to control the speech distortion by comparing the eigenvector beam-former with a maximum likelihood beamformer: One is an analytic solution for the ideal case of absence of reverberation and the other one is a statistically motivated approach. We use the recently introduced gradient-ascent algorithm for adaptive principal eigenvector beamforming and then normalize the filter coefficients by the proposed distortion control methods. Experimental results in terms of the achievable SNR gain and a perceptual speech quality measure are given for the normalized eigenvector beamformer and are compared to standard beamforming methods .},
author = {Warsitz, Ernst and Haeb-Umbach, Reinhold},
title = {CONTROLLING SPEECH DISTORTION IN ADAPTIVE FREQUENCY-DOMAIN PRINCIPAL EIGENVECTOR BEAMFORMING}
}

@article{Vorobyov2013,
abstract = {Robustness is typically understood as an ability of adaptive beamforming algorithm to achieve high performance in the situations with imperfect, incomplete, or erroneous knowledge about the source, propagation media, and antenna array. It is also desired to achieve high performance with as little as possible prior information. In the last decade, several fruitful principles to minimum variance distortionless response ({MVDR}) robust adaptive beamforming (RAB) design have been developed and successfully applied to solve a number of problems in a wide range of applications. Such principles of {MVDR} RAB design are summarized here in a single paper. Prof. Gershman has actively participated in the development and applications of a number of such {MVDR} RAB design principles.},
author = {Vorobyov, Sergiy A and Gershman, Alex B},
doi = {10.1016/j.sigpro.2012.10.021},
file = {::},
journal = {Signal Processing},
keywords = {Convex optimization,Robust adaptive beamforming},
pages = {3264--3277},
title = {{Principles of minimum variance robust adaptive beamforming design}},
url = {http://dx.doi.org/10.1016/j.sigpro.2012.10.021},
volume = {93},
year = {2013}
}

@INPROCEEDINGS{WSJAV,  author={M. {Lincoln} and I. {McCowan} and J. {Vepa} and H. K. {Maganti}},  booktitle={IEEE Workshop on Automatic Speech Recognition and Understanding, 2005.},   title={The multi-channel Wall Street Journal audio visual corpus ({MC-WSJ-AV}): specification and initial experiments},   year={2005},  volume={},  number={},  pages={357-362},  doi={10.1109/ASRU.2005.1566470}}
@ARTICLE{ivector,  author={N. {Dehak} and P. J. {Kenny} and R. {Dehak} and P. {Dumouchel} and P. {Ouellet}},  journal={IEEE Transactions on Audio, Speech, and Language Processing},   title={Front-End Factor Analysis for Speaker Verification},   year={2011},  volume={19},  number={4},  pages={788-798},  doi={10.1109/TASL.2010.2064307}}
@techreport{Medennikov,
abstract = {This paper describes the STC system for the CHiME-6 Challenge aimed at multi-microphone multi-speaker speech recognition and diarization in a dinner party scenario. The system for Track 1 utilizes soft-activity based Guided Source Separation (GSS) front-end and a combination of advanced acoustic modeling techniques, including GSS-based training data augmentation , multi-stride and multi-stream self-attention layers, statistics layer and spectral augmentation, as well as lattice-level fusion of acoustic models. The system showed WER of 33.53{\%}/35.79{\%} on the development/evaluation data. For Track 2, we proposed a novel Target-Speaker Voice Activity Detection (TS-VAD) approach, which directly solves the diarization problem and allows performing GSS on top of the diarized segments. Our TS-VAD is based on i-vector speaker embeddings, which are initially estimated using a strong x-vector diarization system with spectral clustering. This approach allowed to achieve DER of 37.30{\%}/41.40{\%}, JER of 36.11{\%}/39.73{\%}, and WER of 41.56{\%}/44.49{\%} using acoustic models from the Track 1 system. Additionally, lattice rescoring with a neural language model was applied for Ranking B and provided WER reduction to 30.96{\%}/33.91{\%} in Track 1 and 39.56{\%}/42.67{\%} in Track 2. Index Terms: automatic speech recognition, speaker diariza-tion, guided source separation, target-speaker VAD, CHiME-6 1. Track 1: Speech recognition only 1.1. Front-end Track 1 conditions allow the participants to use the information about the speaker boundaries for each utterance. So it is possible to use Guided Source Separation (GSS) [1, 2], which was developed during the CHiME-5 Challenge [3] and later [4, 5] allowed to improve the recognition accuracy significantly. The STC system uses the combination of the Weighted Prediction Error (WPE) dereverberation method [6], GSS and the Minimum Variance Distortionless Response ({MVDR}) beamform-ing [7] adopted from the baseline system. As noted in [5], the use of the refined utterance boundaries obtained after the first-pass decoding can provide an additional WER improvement. By default, per-frame speaker activities induced from hard label information are multiplied by the spectral masks after each iteration of GSS. We supposed that using soft-activity labels can improve the masks estimates. Soft-activities can be extracted from the first-pass decoding lattices. However, we found that better results can be obtained using speaker activity probabilities estimated by a special model. A more detailed description of such models is given in Section 2.2. The basic {MVDR}-beamforming procedure included in the pb chime 1 package uses spectral masks obtained from GSS. After a thorough analysis of this procedure, we found several ways to improve the accuracy slightly. The first one is a diagonal reg-ularization of noise spatial covariance matrices. The second one is excluding one-third of all microphones with worst Envelope Variance [8] scores from the beamforming. 1.2. Back-end As demonstrated in [4], using GSS-enhanced data in training improves ASR results significantly. Following this, we trained AM on a dataset consisting of worn microphones recordings and data obtained using four versions of GSS with various settings (microphone set, context length, number of iterations). We also used the room simulation, speed and volume perturbation included in the baseline recipe. Our basic AM consists of 9-layer Convolutional Neural Network (CNN) [9] with residual connections, followed by 8-layer Factorized Time-Delay Neural Network (TDNN-F) [10]. The network takes an 80-dimensional log Mel filterbank or Gammatone filterbank [11] feature vectors as an input. Mean and standard deviation statistics computed by the "stats" layer are used as additional input channels, and a SpecAugment [12] layer is applied for spectral perturbation. Speaker embeddings are also used to provide a speaker-aware training. We obtained the best results when using i-vectors [13] as speaker embed-dings, however, models with x-vectors [14, 15]) were also included in an ensemble. We also observed a noticeable improvement after adding multi-stride and multi-stream self-attention layers [16, 17] into the model. All the models were trained according to the Lattice Free Maximum Mutual Information (LF-MMI) [18] criterion and fine-tuned for one more epoch of state-level Minimum Bayes Risk (sMBR) [19] training. Finally, we performed lattice fusion followed by MBR decoding [20] to combine recognition results from different models and different versions of GSS. As part of Ranking B, the regularized Long Short-Term Memory (LSTM) LM [21] on Byte Pair Encoding (BPE) [22] text decomposition was applied for lattices rescoring [23] prior to fusion. This provided an additional WER reduction. Recognition results are presented in Table 1. Dev WER{\%} Eval WER{\%} Kaldi baseline 51.76 51.29 Best single AM 36.82 38.59 Fusion 33.53 35.79 Lattice rescoring + Fusion 30.96 33.91 Table 1: ASR results for Track 1 1 https://github.com/fgnt/pb{\_}chime5},
author = {Medennikov, Ivan and Korenevsky, Maxim and Prisyach, Tatiana and Khokhlov, Yuri and Korenevskaya, Mariya and Sorokin, Ivan and Timofeeva, Tatiana and Mitrofanov, Anton and Andrusenko, Andrei and Podluzhny, Ivan and Laptev, Aleksandr and Romanenko, Aleksei},
file = {:home/will/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Medennikov et al. - Unknown - The STC System for the CHiME-6 Challenge.pdf:pdf},
title = {{The STC System for the CHiME-6 Challenge}},
url = {https://github.com/fgnt/pb{\_}chime5}
}

@article{Du,
abstract = {This technical report describes our submission to the 6th CHiME Challenge. The submitted systems for CHiME-6 cover both the multiple-array speech recognition track and multiple-array diarization and recognition track. For each track, the results corresponded to Category A and Category B are reported. The main technique points of our submission include the deep learning based iterative speech separation, training data augmentation via different versions of the official training data, SNR-based array selection, front-end model fusion, acoustic model fusion. Index Terms: speech recognition, human-computer interaction , computational paralinguistics 1. System Overview For CHiME-6, we participate both the multiple-array speech recognition track and multiple-array diarization and recognition track. The overall system flowchart is given in Figure 1. In CHiME-4 and CHiME-5, we proposed the iterative mask estimation and two-stage speech separation methods, and both methods combined the neural network based mask estimation and the CGMM-based method in [1, 2]. In CHiME-6, the iterative beamforming speech separation model (IBF-SS model) is proposed for the multiple-array speech recognition track and multiple-array diarization and recognition track. In CHiME-4, an information fusion framework with multi-channel feature concatenation was proposed in [3]. In CHiME-6, the technique of the multi-feature concatenation is also utilized, but the features containing multi-channel spatial characteristics are firstly applied in our acoustic model. For Rank B, the first-pass decoding is performed with the HMM and 3-gram to generate the lattice as the hypotheses, which are served for the second-pass decoding with a simple RNN-based language model (LM). In the following sections, we will give the detailed description on the multiple-array speech recognition track and multiple-array diarization and recognition track, respectively. 2. Multiple-Array Speech Recognition Track First of all, due to rules defined by official, systems are allowed to exploit knowledge of the utterance start and end time, the utterance speaker label and the speaker location label. It's allowed to use binaural data and far-field data in the training set. 2.1. Speech separation model training In order the make our speech separation model can utilize the spatial information of multi-channel data, the four beam-formed outputs of CGMM is utilized for our iterative beam-forming speech separation model (IBF-SS model). And the 257-dimensional log-power spectrograms (LPS) feature for four speakers and the corresponding phase features are concatena-tion as the input feature. For model architecture in both stages, we utilize a two layer Bi-directional long short-term memory (BLSTM) as the speech separation model, each direction with 512 cells. The Pytorch is used for training. After separation stage, the resulting waveforms can be directly sent to back-end acoustic models, or provide only masks to the beamforming. 2.2. Data simulation For the acoustic model training data, a certain amount of far-field data are simulated by using the worn data and real far-filed data. The impulse responses are estimated through pairs of worn data and far-field data. With estimated impulse responses and worn data, simulated far-field data can be generated. 2.3. Acoustic model (AM) All the acoustic models are trained using lattice-free maximum mutual information (LF-MMI) training with kaldi tool. For the single-input acoustic model, the input feature is the 40-dimensional MFCC. And for the multi-feature acoustic models, the various features containing multi-channel spatial characteristics are concatenated into 840-dimensional features as the input features. 2.3.1. AM with single feature},
author = {Du, Jun and Tu, Yan-Hui and Sun, Lei and Chai, Li and Tang, Xin and He, Mao-Kui and Ma, Feng and Pan, Jia and Gao, Jian-Qing and Liu, Dan and Lee, Chin-Hui and Chen, Jing-Dong},
doi = {10.1016/j},
file = {:home/will/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Du et al. - Unknown - The USTC-NELSLIP Systems for CHiME-6 Challenge2. AM with multi-feature concatenation Resnet-CNN-TDNNF Hybrid CN(2).pdf:pdf},
title = {{The USTC-NELSLIP Systems for CHiME-6 Challenge}},
url = {https://doi.org/10.1016/j.}
}

@inproceedings{Swietojanski2013,
abstract = {We investigate the application of deep neural network (DNN)-hidden Markov model (HMM) hybrid acoustic models for far-field speech recognition of meetings recorded using microphone arrays. We show that the hybrid models achieve significantly better accuracy than conventional systems based on Gaussian mixture models (GMMs). We observe up to 8{\%} absolute word error rate (WER) reduction from a discriminatively trained GMM baseline when using a single distant microphone, and between 4-6{\%} absolute WER reduction when using beamforming on various combinations of array channels. By training the networks on audio from multiple channels, we find the networks can recover significant part of accuracy difference between the single distant microphone and beamformed configurations. Finally, we show that the accuracy of a network recognising speech from a single distant microphone can approach that of a multi-microphone setup by training with data from other microphones. {\textcopyright} 2013 IEEE.},
author = {Swietojanski, Pawel and Ghoshal, Arnab and Renals, Steve},
booktitle = {ASRU 2013},
doi = {10.1109/ASRU.2013.6707744},
isbn = {9781479927562},
keywords = {Beamforming,Deep Neural Networks,Distant Speech Recognition,Meeting recognition,Microphone Arrays},
pages = {285--290},
title = {{Hybrid acoustic models for distant and multichannel large vocabulary speech recognition}},
year = {2013}
}
@misc{watanabe2020chime6,
      title={CHiME-6 Challenge:Tackling Multispeaker Speech Recognition for Unsegmented Recordings}, 
      author={Shinji Watanabe and Michael Mandel and Jon Barker and Emmanuel Vincent and Ashish Arora and Xuankai Chang and Sanjeev Khudanpur and Vimal Manohar and Daniel Povey and Desh Raj and David Snyder and Aswin Shanmugam Subramanian and Jan Trmal and Bar Ben Yair and Christoph Boeddeker and Zhaoheng Ni and Yusuke Fujita and Shota Horiguchi and Naoyuki Kanda and Takuya Yoshioka and Neville Ryant},
      year={2020},
      eprint={2004.09249},
      archivePrefix={arXiv},
      primaryClass={cs.SD}
}
@article{Waibel1989,
abstract = {In this paper we present a Time-Delay Neural Network (TDNN) approach to phoneme recognition which is characterized by two important properties. 1) Using a 3 layer arrangement of simple computing units, a hierarchy can be constructed that allows for the formation of arbitrary nonlinear decision surfaces. The TDNN learns these decision surfaces automatically using error backpropagation [1]. 2) The time-delay arrangement enables the network to discover acoustic-phonetic features and the temporal relationships between them independent of position in time and hence not blurred by temporal shifts in the input. As a recognition task, the speaker-dependent recognition of the phonemes “B,” “D,” and “G” in varying phonetic contexts was chosen. For comparison, several discrete Hidden Markov Models (HMM) were trained to perform the same task. Performance evaluation over 1946 testing tokens from three speakers showed that the TDNN achieves a recognition rate of 98.5 percent correct while the rate obtained by the best of our HMM's was only 93.7 percent. Closer inspection reveals that the network “invented” welLKnown acoustic-phonetic features (e.g., F2-rise, F2-fall, vowel-onset) as useful abstractions. It also developed alternate internal representations to link different acoustic realizations to the same concept. {\textcopyright} 1989 IEEE},
author = {Waibel, Alexander and Hanazawa, Toshiyuki and Hinton, Geoffrey and Shikano, Kiyohiro and Lang, Kevin J.},
doi = {10.1109/29.21701},
issn = {00963518},
journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
number = {3},
pages = {328--339},
title = {{Phoneme Recognition Using Time-Delay Neural Networks}},
volume = {37},
year = {1989}
}
@article{Williamson,
author = {Williamson, DS and transactions on Audio, DL Wang - IEEE/ACM and undefined 2017},
journal = {ieeexplore.ieee.org},
title = {{Time-frequency masking in the complex domain for speech dereverberation and denoising}},
url = {https://ieeexplore.ieee.org/abstract/document/7906509/}
}
@inproceedings{Yoshioka2016,
abstract = {CHiME-3 is a research community challenge organised in 2015 to evaluate speech recognition systems for mobile multi-microphone devices used in noisy daily environments. This paper describes NTT's CHiME-3 system, which integrates advanced speech enhancement and recognition techniques. Newly developed techniques include the use of spectral masks for acoustic beam-steering vector estimation and acoustic modelling with deep convolutional neural networks based on the «network in network» concept. In addition to these improvements, our system has several key differences from the official baseline system. The differences include multi-microphone training, dereverberation, and cross adaptation of neural networks with different architectures. The impacts that these techniques have on recognition performance are investigated. By combining these advanced techniques, our system achieves a 3.45{\%} development error rate and a 5.83{\%} evaluation error rate. Three simpler systems are also developed to perform evaluations with constrained set-ups.},
author = {Yoshioka, Takuya and Ito, Nobutaka and Delcroix, Marc and Ogawa, Atsunori and Kinoshita, Keisuke and Fujimoto, Masakiyo and Yu, Chengzhu and Fabian, Wojciech J. and Espi, Miquel and Higuchi, Takuya and Araki, Shoko and Nakatani, Tomohiro},
booktitle = {ASRU 2015},
doi = {10.1109/ASRU.2015.7404828},
isbn = {9781479972913},
keywords = {'CHiME' challenge,automatic speech recognition,speech enhancement},
month = {February},
pages = {436--443},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{The NTT CHiME-3 system: Advances in speech enhancement and recognition for mobile multi-microphone devices}},
year = {2016}
}
@ARTICLE{Frost72,
  author={O. L. {Frost}},
  journal={Proceedings of the IEEE}, 
  title={An algorithm for linearly constrained adaptive array processing}, 
  year={1972},
  volume={60},
  number={8},
  pages={926-935},
  doi={10.1109/PROC.1972.8817}}
@misc{NIST09,
author = {NIST},
title = {The {NIST} Rich Transcription 2009 ({RT}’09) Evaluation},
year = {2009},
note = {\url{http://www.itl.nist.gov/iad/mig/tests/rt/2009/docs/rt09-meeting-eval-plan-v2.pdf}}
}

@misc{NISTRT,
author = {NIST},
title = {{Rich} {Transcription} {Evaluation}},
year = {2020},
note = {\url{https://www.nist.gov/itl/iad/mig/rich-transcription-evaluation}}
}

@article{ocy179,
    author = {Blackley, Suzanne V and Huynh, Jessica and Wang, Liqin and Korach, Zfania and Zhou, Li},
    title = "{Speech recognition for clinical documentation from 1990 to 2018: a systematic review}",
    journal = {Journal of the American Medical Informatics Association},
    volume = {26},
    number = {4},
    pages = {324-338},
    year = {2019},
    month = {02},
    abstract = "{The study sought to review recent literature regarding use of speech recognition (SR) technology for clinical documentation and to understand the impact of SR on document accuracy, provider efficiency, institutional cost, and more.We searched 10 scientific and medical literature databases to find articles about clinician use of SR for documentation published between January 1, 1990, and October 15, 2018. We annotated included articles with their research topic(s), medical domain(s), and SR system(s) evaluated and analyzed the results.One hundred twenty-two articles were included. Forty-eight (39.3\\%) involved the radiology department exclusively and 10 (8.2\\%) involved emergency medicine; 10 (8.2\\%) mentioned multiple departments. Forty-eight (39.3\\%) articles studied productivity; 20 (16.4\\%) studied the effect of SR on documentation time, with mixed findings. Decreased turnaround time was reported in all 19 (15.6\\%) studies in which it was evaluated. Twenty-nine (23.8\\%) studies conducted error analyses, though various evaluation metrics were used. Reported percentage of documents with errors ranged from 4.8\\% to 71\\%; reported word error rates ranged from 7.4\\% to 38.7\\%. Seven (5.7\\%) studies assessed documentation-associated costs; 5 reported decreases and 2 reported increases. Many studies (44.3\\%) used products by Nuance Communications. Other vendors included IBM (9.0\\%) and Philips (6.6\\%); 7 (5.7\\%) used self-developed systems.Despite widespread use of SR for clinical documentation, research on this topic remains largely heterogeneous, often using different evaluation metrics with mixed findings. Further, that SR-assisted documentation has become increasingly common in clinical settings beyond radiology warrants further investigation of its use and effectiveness in these settings.}",
    issn = {1527-974X},
    doi = {10.1093/jamia/ocy179},
    url = {https://doi.org/10.1093/jamia/ocy179},
    eprint = {https://academic.oup.com/jamia/article-pdf/26/4/324/28015443/ocy179.pdf},
}

@article{BFIT1,
  author = {X. Anguera and C. Wooters and J. Hernando},
  title = {Acoustic beamforming for speaker diarization of meetings},
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  year = {2007},
  volume = {15},
  pages = {2011-2021},
  number = {7},
  month = {September}
}
@misc{BFIT2,
author = {Xavier Anguera},
  title = {{PhD Thesis: Robust Speaker Diarization for Meetings}},
  school = {Universitat Politecnica de Catalonia},
  year = {2006}
}

@ARTICLE{convtasnet,  author={Y. {Luo} and N. {Mesgarani}},  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},   title={{Conv-{TasNet}}: Surpassing Ideal Time–Frequency Magnitude Masking for Speech Separation},   year={2019},  volume={27},  number={8},  pages={1256-1266},  doi={10.1109/TASLP.2019.2915167}}

@INPROCEEDINGS{tasnet,  author={Y. {Luo} and N. {Mesgarani}},  booktitle={ICASSP 2018},   title={{TasNet}: Time-Domain Audio Separation Network for Real-Time, Single-Channel Speech Separation},   year={2018},  volume={},  number={},  pages={696-700},  doi={10.1109/ICASSP.2018.8462116}}

@INPROCEEDINGS{beamtasnet,  author={T. {Ochiai} and M. {Delcroix} and R. {Ikeshita} and K. {Kinoshita} and T. {Nakatani} and S. {Araki}},  booktitle={ICASSP 2020},   title={Beam-{TasNet}: Time-domain Audio Separation Network Meets Frequency-domain Beamformer},   year={2020},  month=May, volume={},  number={},  pages={6384-6388},  doi={10.1109/ICASSP40776.2020.9053575}}

@misc{koyama2020exploring,
      title={Exploring the Best Loss Function for DNN-Based Low-latency Speech Enhancement with Temporal Convolutional Networks}, 
      author={Yuichiro Koyama and Tyler Vuong and Stefan Uhlich and Bhiksha Raj},
      year={2020},
      eprint={2005.11611},
      archivePrefix={arXiv},
      primaryClass={eess.AS}
}

@INPROCEEDINGS{Kinoshita1,  author={K. {Kinoshita} and T. {Ochiai} and M. {Delcroix} and T. {Nakatani}},  booktitle={ICASSP 2020},   title={Improving Noise Robust Automatic Speech Recognition with Single-Channel Time-Domain Enhancement Network},   year={2020}, month=May, volume={},  number={},  pages={7009-7013},  doi={10.1109/ICASSP40776.2020.9053266}}

@INPROCEEDINGS{Erdogan,  author={S. {Sonning} and C. {Schüldt} and H. {Erdogan} and S. {Wisdom}},  booktitle={ICASSP 2020},   title={Performance Study of a Convolutional Time-Domain Audio Separation Network for Real-Time Speech Denoising},   year={2020}, month=May, volume={},  number={},  pages={831-835},  doi={10.1109/ICASSP40776.2020.9053846}}

@techreport{Rao,
author = {Rao, Wei and Xie, Lei and Wang, Yannan and Yu, Tao and Watanabe, Shinji and Tan, Zheng-Hua and Bu, Hui and Shang, Shidong},
file = {:home/will/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rao et al. - Unknown - ConferencingSpeech 2021 Challenge Evaluation Plan(2).pdf:pdf},
title = {{ConferencingSpeech 2021 Challenge Evaluation Plan}},
url = {https://tea-lab.qq.com/conferencingspeech-2021}
}

@article{Chen2020,
abstract = {Acoustic echo cannot be entirely removed by linear adaptive filters due to the nonlinear relationship between the echo and far-end signal. Usually a post processing module is required to further suppress the echo. In this paper, we propose a residual echo suppression method based on the modification of fully convo-lutional time-domain audio separation network ({Conv-{TasNet}}). Both the residual signal of the linear acoustic echo cancellation system, and the output of the adaptive filter are adopted to form multiple streams for the {Conv-{TasNet}}, resulting in more effective echo suppression while keeping a lower latency of the whole system. Simulation results validate the efficacy of the proposed method in both single-talk and double-talk situations.},
author = {Chen, Hongsheng and Xiang, Teng and Chen, Kai and Lu, Jing},
doi = {10.21437/Interspeech.2020-2234},
title = {{Nonlinear Residual Echo Suppression Based on Multi-stream {Conv-{TasNet}}}},
url = {http://dx.doi.org/10.21437/Interspeech.2020-2234},
year = {2020}
}


@techreport{Peddinti,
author = {Peddinti, Vijayaditya and Chen, Guoguo and Povey, Daniel and Khudanpur, Sanjeev},
file = {:C\:/Users/will9/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Peddinti et al. - Unknown - Reverberation robust acoustic modeling using i-vectors with time delay neural networks.pdf:pdf},
keywords = {Index Terms: far field speech recognition,reverberation,time delay neural networks},
title = {{Reverberation robust acoustic modeling using i-vectors with time delay neural networks}}
}

@techreport{Han,
author = {Han, Kyu J and Pan, Jing and Krishna, Venkata and Tadala, Naveen and Ma, Tao and Povey, Dan},
file = {:C\:/Users/will9/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Han et al. - Unknown - Multistream CNN for Robust Acoustic Modeling.pdf:pdf},
keywords = {Index Terms: Multistream CNN,LibriSpeech,contact center applications,robust acoustic modeling,speech recognition},
title = {{Multistream CNN for Robust Acoustic Modeling}},
url = {https://github.com/danpovey/pocolm.}
}
@article{Zhang2020OnEM,
  title={On End-to-end Multi-channel Time Domain Speech Separation in Reverberant Environments},
  author={Jisi Zhang and Catalin Zorila and Rama Doddipatla and Jon Barker},
  journal={ICASSP 2020},
  year={2020},
  month=May,
  pages={6389-6393}
}
@inproceedings{Noe2020,
abstract = {Convolutional Neural Networks (CNN) have been used in Automatic Speech Recognition (ASR) to learn representations directly from the raw signal instead of hand-crafted acoustic features, providing a richer and lossless input signal. Recent researches propose to inject prior acoustic knowledge to the first convolutional layer by integrating the shape of the impulse responses in order to increase both the interpretability of the learnt acoustic model, and its performances. We propose to combine the complex Gabor filter with complex-valued deep neural networks to replace usual CNN weights kernels, to fully take advantage of its optimal time-frequency resolution and of the complex domain. The conducted experiments on the TIMIT phoneme recognition task shows that the proposed approach reaches top-of-the-line performances while remaining interpretable.},
archivePrefix = {arXiv},
arxivId = {2002.04569},
author = {Noe, Paul Gauthier and Parcollet, Titouan and Morchid, Mohamed},
booktitle = {ICASSP 2020},
doi = {10.1109/ICASSP40776.2020.9054220},
eprint = {2002.04569},
isbn = {9781509066315},
issn = {15206149},
keywords = {Ga-bor filters,SincNet,complex neural networks,speech recognition},
month = {May},
pages = {7724--7728},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{CGCNN: Complex Gabor Convolutional Neural Network on Raw Speech}},
volume = {2020-May},
year = {2020}
}
@inproceedings{Park2018,
abstract = {Convolutional neural networks (CNNs) have been shown to improve classification tasks such as automatic speech recognition (ASR). Furthermore, the CNN with very deep architecture lowered the word error rate (WER) in reverberant and noisy environments. However, DNN-based ASR systems still perform poorly in unseen reverberant conditions. In this paper, we use the weighted prediction error (WPE)-based preprocessing for dereverberation. In our experiments on the ASR task of the REVERB Challenge 2014, the WPE-based processing with eight channels reduced the WER by 20\% for the real-condition data using CNN acoustic models with 10 layers.},
author = {Park, Sunchan and Jeong, Yongwon and Kim, Min Sik and Kim, Hyung Soon},
booktitle = {ICEIC 2018},
doi = {10.23919/ELINFOCOM.2018.8330593},
isbn = {9781538647547},
keywords = {Convolutional neural network,Dereverberation,Reverberant speech recognition,Weighted prediction error},
month = {April},
pages = {1--2},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Linear prediction-based dereverberation with very deep convolutional neural networks for reverberant speech recognition}},
volume = {2018-January},
year = {2018}
}
@inproceedings{Huang2015,
abstract = {Despite the fact that several sites have reported the effectiveness of convolutional neural networks (CNNs) on some tasks, there is no deep analysis regarding why CNNs perform well and in which case we should see CNNs' advantage. In the light of this, this paper aims to provide some detailed analysis of CNNs. By visualizing the localized filters learned in the convolutional layer, we show that edge detectors in varying directions can be automatically learned. We then identify four domains we think CNNs can consistently provide advantages over fully-connected deep neural networks (DNNs): channel-mismatched training-test conditions, noise robustness, distant speech recognition, and low-footprint models. For distant speech recognition, a CNN trained on 1000 hours of Kinect distant speech data obtains relative 4\% word error rate reduction (WERR) over a DNN of a similar size. To our knowledge, this is the largest corpus so far reported in the literature for CNNs to show its effectiveness. Lastly, we establish that the CNN structure combined with maxout units is the most effective model under small-sizing constraints for the purpose of deploying small-footprint models to devices. This setup gives relative 9.3\% WERR from DNNs with sigmoid units.},
author = {Huang, Jui Ting and Li, Jinyu and Gong, Yifan},
booktitle = {ICASSP 2015},
doi = {10.1109/ICASSP.2015.7178920},
isbn = {9781467369978},
issn = {15206149},
keywords = {Convolutional neural networks,DNN,low footprint models,maxout units},
month = {August},
pages = {4989--4993},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{An analysis of convolutional neural networks for speech recognition}},
volume = {2015-August},
year = {2015}
}
@techreport{Haeb-Umbach,
abstract = {The machine recognition of speech spoken at a distance from the microphones, known as far-field automatic speech recognition (ASR), has received a significant increase of attention in science and industry, which caused or was caused by an equally significant improvement in recognition accuracy. Meanwhile it has entered the consumer market with digital home assistants with a spoken language interface being its most prominent application. Speech recorded at a distance is affected by various acoustic distortions and, consequently, quite different processing pipelines have emerged compared to ASR for close-talk speech. A signal enhancement front-end for dereverberation, source separation and acoustic beamforming is employed to clean up the speech, and the back-end ASR engine is robustified by multi-condition training and adaptation. We will also describe the so-called end-to-end approach to ASR, which is a new promising architecture that has recently been extended to the far-field scenario. This tutorial article gives an account of the algorithms used to enable accurate speech recognition from a distance, and it will be seen that, although deep learning has a significant share in the technological breakthroughs, a clever combination with traditional signal processing can lead to surprisingly effective solutions.},
archivePrefix = {arXiv},
arxivId = {2009.09395v1},
author = {Haeb-Umbach, Reinhold and Heymann, Jahn and Drude, Lukas and Watanabe, Shinji and Delcroix, Marc and Nakatani, Tomohiro},
eprint = {2009.09395v1},
file = {:C\:/Users/will9/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Haeb-Umbach et al. - Unknown - Far-Field Automatic Speech Recognition.pdf:pdf},
keywords = {Index Terms-Automatic speech recognition,acoustic beamforming,dereverberation,end-to-end speech recognition,speech en-hancement},
title = {{Far-Field Automatic Speech Recognition}}
}
@techreport{Athineos,
author = {Athineos, Marios and Labrosa, Daniel P W Ellis},
file = {:C\:/Users/will9/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Athineos, Labrosa - Unknown - Frequency-Domain Linear Prediction For Temporal Features(2).pdf:pdf},
title = {{Frequency-Domain Linear Prediction For Temporal Features}},
url = {http://www.ee.}
}
@INPROCEEDINGS{8683634,  author={Pandey, A. and Wang, D.},  booktitle={ICASSP 2019},   title={{TCNN: Temporal Convolutional Neural Network for Real-time Speech Enhancement in the Time Domain}},   year={2019},  volume={},  number={}, month=may,  doi={10.1109/ICASSP.2019.8683634}}

@ARTICLE{9095210,  author={Zhao, Y. and Wang, D. and Xu, B. and Zhang, T.},  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},   title={{Monaural Speech Dereverberation Using Temporal Convolutional Networks With Self Attention}},   year={2020},  volume={28},  number={},  pages={1598-1607},  doi={10.1109/TASLP.2020.2995273}}

@article{Qian2017,
abstract = {Although great progresses have been made in automatic speech recognition (ASR), significant performance degradation is still observed when recognizing multi-talker mixed speech. In this paper, we propose and evaluate several architectures to address this problem under the assumption that only a single channel of mixed signal is available. Our technique extends permutation invariant training (PIT) by introducing the front-end feature separation module with the minimum mean square error (MSE) criterion and the back-end recognition module with the minimum cross entropy (CE) criterion. More specifically, during training we compute the average MSE or CE over the whole utterance for each possible utterance-level output-target assignment, pick the one with the minimum MSE or CE, and optimize for that assignment. This strategy elegantly solves the label permutation problem observed in the deep learning based multi-talker mixed speech separation and recognition systems. The proposed architectures are evaluated and compared on an artificially mixed AMI dataset with both two- and three-talker mixed speech. The experimental results indicate that our proposed architectures can cut the word error rate (WER) by 45.0\% and 25.0\% relatively against the state-of-the-art single-talker speech recognition system across all speakers when their energies are comparable, for two- and three-talker mixed speech, respectively. To our knowledge, this is the first work on the multi-talker mixed speech recognition on the challenging speaker-independent spontaneous large vocabulary continuous speech task.},
archivePrefix = {arXiv},
arxivId = {1707.06527},
author = {Qian, Yanmin and Chang, Xuankai and Yu, Dong},
eprint = {1707.06527},
file = {:C\:/Users/will9/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Qian, Chang, Yu - 2017 - Single-Channel Multi-talker Speech Recognition with Permutation Invariant Training.pdf:pdf},
journal = {Speech Communication},
keywords = {Feature separation,Joint-optimization,Multi-talker mixed speech recognition,Permutation invariant training},
month = {July},
pages = {1--11},
publisher = {Elsevier B.V.},
title = {{Single-Channel Multi-talker Speech Recognition with Permutation Invariant Training}},
url = {http://arxiv.org/abs/1707.06527},
volume = {104},
year = {2017}
}

@ARTICLE{speakerbeam,  author={Žmolíková, Kateřina and Delcroix, Marc and Kinoshita, Keisuke and Ochiai, Tsubasa and Nakatani, Tomohiro and Burget, Lukáš and Černocký, Jan},  journal={IEEE Journal of Selected Topics in Signal Processing},   title={SpeakerBeam: Speaker Aware Neural Network for Target Speaker Extraction in Speech Mixtures},   year={2019},  volume={13},  number={4},  pages={800-814},  doi={10.1109/JSTSP.2019.2922820}}

@techreport{Liu2014,
abstract = {This paper presents an investigation of far field speech recognition using beamforming and channel concatenation in the context of Deep Neural Network (DNN) based feature extraction. While speech enhancement with beamforming is attractive, the algorithms are typically signal-based with no information about the special properties of speech. A simple alternative to beamforming is con-catenating multiple channel features. Results presented in this paper indicate that channel concatenation gives similar or better results. On average the DNN front-end yields a 25\% relative reduction in Word Error Rate (WER). Further experiments aim at including relevant information in training adapted DNN features. Augmenting the standard DNN input with the bottleneck feature from a Speaker Aware Deep Neural Network (SADNN) shows a general advantage over the standard DNN based recognition system, and yields additional improvements for far field speech recognition.},
author = {Liu, Y and Zhang, P and Hain, T},
file = {:C\:/Users/will9/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu, Zhang, Hain - 2014 - This is a repository copy of Using neural network front-ends on far field multiple microphones based speech re.pdf:pdf},
keywords = {Index Terms-speech recognition,beamforming,deep neural networks,multiple microphone},
pages = {5542--5546},
publisher = {IEEE},
title = {USING NEURAL NETWORK FRONT-ENDS ON FAR FIELD MULTIPLE MICROPHONES BASED SPEECH RECOGNITION},
url = {http://eprints.whiterose.ac.uk/78166/},
year = {2014}
}
@article{Gales2007,
abstract = {Hidden Markov Models (HMMs) provide a simple and effective framework for modelling time-varying spectral vector sequences. As a consequence , almost all present day large vocabulary continuous speech recognition (LVCSR) systems are based on HMMs. Whereas the basic principles underlying HMM-based LVCSR are rather straightforward, the approximations and simplifying assumptions involved in a direct implementation of these principles would result in a system which has poor accuracy and unacceptable sensitivity to changes in operating environment. Thus, the practical application of HMMs in modern systems involves considerable sophistication. The aim of this review is first to present the core architecture of a HMM-based LVCSR system and then describe the various refinements which are needed to achieve state-of-the-art performance. These},
author = {Gales, Mark and Young, Steve},
doi = {10.1561/2000000004},
file = {:C\:/Users/will9/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gales, Young - 2007 - The Application of Hidden Markov Models in Speech Recognition.pdf:pdf},
journal = {Foundations and Trends R in Signal Processing},
number = {3},
pages = {195--304},
title = {{The Application of Hidden Markov Models in Speech Recognition}},
volume = {1},
year = {2007}
}
@techreport{Zhang2018,
abstract = {Eliminating the negative effect of non-stationary environmental noise is a long-standing research topic for automatic speech recognition that stills remains an important challenge. Data-driven supervised approaches, including ones based on deep neural networks, have recently emerged as potential alternatives to traditional unsupervised approaches and with sufficient training , can alleviate the shortcomings of the unsupervised methods in various real-life acoustic environments. In this light, we review recently developed, representative deep learning approaches for tackling non-stationary additive and convolutional degradation of speech with the aim of providing guidelines for those involved in the development of environmentally robust speech recognition systems. We separately discuss single-and multi-channel techniques developed for the front-end and back-end of speech recognition systems, as well as joint front-end and back-end training frameworks.},
archivePrefix = {arXiv},
arxivId = {1705.10874v3},
author = {Zhang, Zixing and Geiger, J{\"{u}}rgen and Pohjalainen, Jouni and {El-Desoky Mousa}, Amr and Schuller, Bj{\"{o}}rn},
eprint = {1705.10874v3},
file = {:C\:/Users/will9/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - 2018 - Deep Learning for Environmentally Robust Speech Recognition An Overview of Recent Developments.pdf:pdf},
keywords = {Index Terms-deep learning,multi-channel speech denoising,neural networks,non-stationary noise,robust speech recognition},
title = {{Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments}},
year = {2018}
}
@INPROCEEDINGS{Weninger14,
  author={Weninger, Felix and Watanabe, Shinji and Tachioka, Yuuki and Schuller, Björn},
  booktitle={ICASSP 2014}, 
  title={Deep recurrent de-noising auto-encoder and blind de-reverberation for reverberated speech recognition}, 
  year={2014},
  volume={},
  number={},
  pages={4623-4627},
  doi={10.1109/ICASSP.2014.6854478}}

@ARTICLE{Yoshioka12,  author={Yoshioka, Takuya and Sehr, Armin and Delcroix, Marc and Kinoshita, Keisuke and Maas, Roland and Nakatani, Tomohiro and Kellermann, Walter},  journal={IEEE Signal Processing Magazine},   title={Making Machines Understand Us in Reverberant Rooms: Robustness Against Reverberation for Automatic Speech Recognition},   year={2012},  volume={29},  number={6},  pages={114-126},  doi={10.1109/MSP.2012.2205029}}
@techreport{Weng,
abstract = {In this work, we propose recurrent deep neural networks (DNNs) for robust automatic speech recognition (ASR). Full recurrent connections are added to certain hidden layer of a conventional feedforward DNN and allow the model to capture the temporal dependency in deep representations. A new backpropagation through time (BPTT) algorithm is introduced to make the minibatch stochastic gradient descent (SGD) on the proposed recurrent DNNs more efficient and effective. We evaluate the proposed recurrent DNN architecture under the hybrid setup on both the 2 nd CHiME challenge (track 2) and Aurora-4 tasks. Experimental results on the CHiME challenge data show that the proposed system can obtain consistent 7\% relative WER improvements over the DNN systems, achieving state-of-the-art performance without front-end preprocessing, speaker adap-tive training or multiple decoding passes. For the experiments on Aurora-4, the proposed system achieves 4\% relative WER improvement over a strong DNN baseline system.},
author = {Weng, Chao and Yu, Dong and Watanabe, Shinji and Juang, Biing-Hwang},
file = {:C\:/Users/will9/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Weng et al. - Unknown - RECURRENT DEEP NEURAL NETWORKS FOR ROBUST SPEECH RECOGNITION.pdf:pdf},
keywords = {Aurora-4,CHiME,Index Terms-DNN,RNN,robust ASR},
title = {RECURRENT DEEP NEURAL NETWORKS FOR ROBUST SPEECH RECOGNITION}
}
@techreport{Sainath,
abstract = {Learning an acoustic model directly from the raw waveform has been an active area of research. However, waveform-based models have not yet matched the performance of log-mel trained neural networks. We will show that raw wave-form features match the performance of log-mel filterbank energies when used with a state-of-the-art CLDNN acoustic model trained on over 2,000 hours of speech. Specifically, we will show the benefit of the CLDNN, namely the time convolution layer in reducing temporal variations, the frequency convolution layer for preserving locality and reducing frequency variations, as well as the LSTM layers for temporal modeling. In addition, by stacking raw waveform features with log-mel features, we achieve a 3\% relative reduction in word error rate.},
author = {Sainath, Tara N and Weiss, Ron J and Senior, Andrew and Wilson, Kevin W},
file = {:C\:/Users/will9/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sainath et al. - Unknown - Learning the Speech Front-end With Raw Waveform CLDNNs.pdf:pdf},
title = {{Learning the Speech Front-end With Raw Waveform CLDNNs}}
}

@techreport{Hoshen,
abstract = {Standard deep neural network-based acoustic models for automatic speech recognition (ASR) rely on hand-engineered input features, typically log-mel filterbank magnitudes. In this paper, we describe a convolutional neural network-deep neural network (CNN-DNN) acoustic model which takes raw multichannel waveforms as input, i.e. without any preceding feature extraction, and learns a similar feature representation through supervised training. By operating directly in the time domain, the network is able to take advantage of the signal's fine time structure that is discarded when computing filterbank magnitude features. This structure is especially useful when analyzing multichannel inputs, where timing differences between input channels can be used to localize a signal in space. The first convolutional layer of the proposed model naturally learns a filterbank that is selective in both frequency and direction of arrival, i.e. a bank of bandpass beamformers with an auditory-like frequency scale. When trained on data corrupted with noise coming from different spatial locations, the network learns to filter them out by steering nulls in the directions corresponding to the noise sources. Experiments on a simulated multichannel dataset show that the proposed acoustic model outperforms a DNN that uses log-mel filterbank magnitude features under noisy and reverberant conditions.},
author = {Hoshen, Yedid and Weiss, Ron J and Wilson, Kevin W},
file = {:C\:/Users/will9/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hoshen, Weiss, Wilson - Unknown - SPEECH ACOUSTIC MODELING FROM RAW MULTICHANNEL WAVEFORMS.pdf:pdf},
keywords = {Index Terms-Automatic speech recognition,acoustic model-ing,beamforming,convolutional neural networks},
title = {SPEECH ACOUSTIC MODELING FROM RAW MULTICHANNEL WAVEFORMS}
}

@inproceedings{Kim17,
title	= {Generation of large-scale simulated utterances in virtual rooms to train deep-neural networks for far-field speech recognition in Google Home},
author	= {Chanwoo Kim and Ananya Misra and Kean Chin and Thad Hughes and Arun Narayanan and Tara Sainath and Michiel Bacchiani},
year	= {2017},
URL	= {http://www.isca-speech.org/archive/Interspeech_2017/pdfs/1510.PDF},
pages	= {379-383}
}

@article{Zhang2020,
abstract = {Despite successful applications of end-to-end approaches in multi-channel speech recognition, the performance still degrades severely when the speech is corrupted by reverberation. In this paper, we integrate the dereverberation module into the end-to-end multi-channel speech recognition system and explore two different frontend architectures. First, a multi-source mask-based weighted prediction error (WPE) module is incorporated in the frontend for dereverberation. Second, another novel frontend architecture is proposed, which extends the weighted power minimization distortionless response (WPD) convolutional beamformer to perform simultaneous separation and dereverberation. We derive a new formulation from the original WPD, which can handle multi-source input, and replace eigenvalue decomposition with the matrix inverse operation to make the back-propagation algorithm more stable. The above two architectures are optimized in a fully end-to-end manner, only using the speech recognition criterion. Experiments on both spatialized wsj1-2mix corpus and REVERB show that our proposed model outperformed the conventional methods in reverberant scenarios.},
archivePrefix = {arXiv},
arxivId = {2005.10479},
author = {Zhang, Wangyou and Subramanian, Aswin Shanmugam and Chang, Xuankai and Watanabe, Shinji and Qian, Yanmin},
eprint = {2005.10479},
file = {:C\:/Users/will9/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - 2020 - End-to-End Far-Field Speech Recognition with Unified Dereverberation and Beamforming.pdf:pdf},
journal = {Proceedings of the Annual Conference of the International Speech Communication Association, Interspeech},
keywords = {Dereverberation,Neural beamforming,Overlapped speech recognition,Speech separation,WPD},
month = {May},
pages = {324--328},
publisher = {International Speech Communication Association},
title = {{End-to-End Far-Field Speech Recognition with Unified Dereverberation and Beamforming}},
url = {http://arxiv.org/abs/2005.10479},
volume = {2020-October},
year = {2020}
}

@article{Zhang2021,
abstract = {Recently, the end-to-end approach has been successfully applied to multi-speaker speech separation and recognition in both single-channel and multichannel conditions. However, severe performance degradation is still observed in the reverberant and noisy scenarios, and there is still a large performance gap between anechoic and reverberant conditions. In this work, we focus on the multichannel multi-speaker reverberant condition, and propose to extend our previous framework for end-to-end dereverberation, beamforming, and speech recognition with improved numerical stability and advanced frontend subnetworks including voice activity detection like masks. The techniques significantly stabilize the end-to-end training process. The experiments on the spatialized wsj1-2mix corpus show that the proposed system achieves about 35\% WER relative reduction compared to our conventional multi-channel E2E ASR system, and also obtains decent speech dereverberation and separation performance ({SDR}=12.5 dB) in the reverberant multi-speaker condition while trained only with the ASR criterion.},
archivePrefix = {arXiv},
arxivId = {2102.11525},
author = {Zhang, Wangyou and Boeddeker, Christoph and Watanabe, Shinji and Nakatani, Tomohiro and Delcroix, Marc and Kinoshita, Keisuke and Ochiai, Tsubasa and Kamo, Naoyuki and Haeb-Umbach, Reinhold and Qian, Yanmin},
eprint = {2102.11525},
file = {:C\:/Users/will9/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - 2021 - End-to-End Dereverberation, Beamforming, and Speech Recognition with Improved Numerical Stability and Advanced Fro.pdf:pdf},
keywords = {Index Terms-Neural beamformer,cocktail party problem,dereverberation,overlapped speech recog-nition,speech separation},
month = {February},
title = {{End-to-End Dereverberation, Beamforming, and Speech Recognition with Improved Numerical Stability and Advanced Frontend}},
url = {http://arxiv.org/abs/2102.11525},
year = {2021}
}
@article{Boeddeker2019,
abstract = {We previously proposed an optimal (in the maximum likelihood sense) convolutional beamformer that can perform simultaneous denoising and dereverberation, and showed its superiority over the widely used cascade of a WPE dereverberation filter and a conventional MPDR beamformer. However, it has not been fully investigated which components in the convolutional beamformer yield such superiority. To this end, this paper presents a new derivation of the convolutional beamformer that allows us to factorize it into a WPE dereverberation filter, and a special type of a (non-convolutional) beamformer, referred to as a wMPDR beamformer, without loss of optimality. With experiments, we show that the superiority of the convolutional beamformer in fact comes from its wMPDR part.},
archivePrefix = {arXiv},
arxivId = {1910.13707},
author = {Boeddeker, Christoph and Nakatani, Tomohiro and Kinoshita, Keisuke and Haeb-Umbach, Reinhold},
eprint = {1910.13707},
file = {:C\:/Users/will9/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Boeddeker et al. - 2019 - Jointly optimal dereverberation and beamforming.pdf:pdf},
journal = {ICASSP 2019},
keywords = {Dereverberation,beamforming,speech enhancement},
month = {Oct},
pages = {216--220},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Jointly optimal dereverberation and beamforming}},
url = {http://arxiv.org/abs/1910.13707},
volume = {2020-May},
year = {2019}
}

@techreport{Kinoshita2020,
archivePrefix = {arXiv},
arxivId = {2003.03998v1},
author = {Kinoshita, Keisuke and Ochiai, Tsubasa and Delcroix, Marc and Nakatani, Tomohiro},
eprint = {2003.03998v1},
file = {:C\:/Users/will9/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kinoshita et al. - 2020 - IMPROVING NOISE ROBUST AUTOMATIC SPEECH RECOGNITION WITH SINGLE-CHANNEL TIME-DOMAIN ENHANCEMENT NETWORK(2).pdf:pdf},
keywords = {Index Terms-Single-channel speech enhancement,robust ASR,time-domain network},
title = {IMPROVING NOISE ROBUST AUTOMATIC SPEECH RECOGNITION WITH SINGLE-CHANNEL TIME-DOMAIN ENHANCEMENT NETWORK},
year = {2020}
}

@article{Isik,
archivePrefix = {arXiv},
arxivId = {1607.02173v1},
author = {Isik, Yusuf and {Le Roux}, Jonathan and Chen, Zhuo and Watanabe, Shinji and Hershey, John R},
journal = {ICASSP 2016},
eprint = {1607.02173v1},
month = {September},
year = {2016},
pages = {545-549},
file = {:home/will/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Isik et al. - Unknown - Single-Channel Multi-Speaker Separation using Deep Clustering.pdf:pdf},
keywords = {Index Terms: single-channel speech separation,deep learning,embedding},
booktitle = {{Single-Channel Multi-Speaker Separation using Deep Clustering}},
doi = {10.21437/Interspeech.2016-1176}
}
@article{BRADLEY2011713,
abstract = {ISO 3382-1 describes a number of objective room acoustics parameters that are generally accepted as useful for rating some specific aspects of concert hall sound fields. They include measures of decay times, energy ratios, measures of sound strength and several quantities related to the spatial aspects of sound fields. In most cases there are details of the measures, or their application, that raise questions. In general, there has not been a lot of practical research to explore how best to develop and use these objective measures to evaluate conditions in concert halls. For some well established measures such as Early Decay Time (EDT), we are not really sure how best to calculate their values. For other measures such as energy ratios, modifications are often proposed but without the support of subjective evaluations of the proposed changes. In other cases, such as measures of spatial impression, two approaches have been suggested, but their relative merits are not well understood. It is easy to propose ever more complex measures, but it is much more difficult to demonstrate their general utility. On the other hand, some commonly described characteristics do not have accepted related objective measures. Many more important and more general problems relate to the need, for design criteria in terms of each quantity, and for an improved understanding of just noticeable differences for each measure. This paper discusses each measure illustrating particular problems with measurements in various halls.},
author = {Bradley, J S},
doi = {https://doi.org/10.1016/j.apacoust.2011.04.004},
issn = {0003-682X},
journal = {Applied Acoustics},
keywords = { Concert hall acoustics, Objective measures,Room acoustics},
number = {10},
pages = {713--720},
title = {{Review of objective room acoustics measures and future needs}},
url = {https://www.sciencedirect.com/science/article/pii/S0003682X1100096X},
volume = {72},
year = {2011}
}
@Inbook{Habets2010,
author="Habets, E. A. P.",
editor="Naylor, Patrick A.
and Gaubitch, Nikolay D.",
title="Speech Dereverberation Using Statistical Reverberation Models",
bookTitle="Speech Dereverberation",
year="2010",
publisher="Springer London",
address="London",
pages="57--93",
abstract="In speech communication systems, such as voice-controlled systems, hands-free mobile telephones and hearing aids, the received microphone signals are degraded by room reverberation, ambient noise and other interferences. This signal degradation can decrease the fidelity and intelligibility of speech and the word recognition rate of automatic speech recognition systems.",
isbn="978-1-84996-056-4",
doi="10.1007/978-1-84996-056-4_3",
url="https://doi.org/10.1007/978-1-84996-056-4_3"
}
@INPROCEEDINGS{8553562,  author={Badeau, Roland},  booktitle={EUSIPCO 2018},   title={Unified Stochastic Reverberation Modeling},   year={2018},  volume={},  number={},  pages={2175-2179},  doi={10.23919/EUSIPCO.2018.8553562}}
@ARTICLE{1161990,  author={Oppenheim, A. and Schafer, R. and Stockham, T.},  journal={IEEE Transactions on Audio and Electroacoustics},   title={Nonlinear filtering of multiplied and convolved signals},   year={1968},  volume={16},  number={3},  pages={437-466},  doi={10.1109/TAU.1968.1161990}}
@misc{habets2007single,
  title={Single- and multi-microphone speech dereverberation using spectral enhancement},
  author={Habets, Emanu{\"e}l Anco Peter},
  volume={68},
  number={04},
  year={2007},
  doi = "10.6100/IR627677",
    language = "English",
    isbn = "978-90-386-1544-8",
    publisher = "Technische Universiteit Eindhoven",
    school = "Electrical Engineering"
}
@inproceedings{WHAM,
    title     = {{WHAM}!: Extending Speech Separation to Noisy Environments},
    author    = {Wichern, Gordon and Antognini, Joe and Flynn, Michael and Zhu,
                 Licheng Richard and McQuinn, Emmett and Crow,
                 Dwight and Manilow, Ethan and Le Roux, Jonathan},
    booktitle = {Interspeech 2019},
    year      = {2019},
    month     = sep
}

@techreport{Chen,
abstract = {Time-domain audio separation network ({{TasNet}}) has achieved remarkable performance in blind source separation (BSS). Classic multi-channel speech processing framework employs signal estimation and beamforming. For example, Beam-{{TasNet}} links multi-channel convolutional {{TasNet}} (MC-{{Conv-{{TasNet}}}}) with minimum variance distortionless response ({MVDR}) beamforming, which leverages the strong modelling ability of data-driven {MC}-{{Conv-{{TasNet}}}} and boosts the performance of beamforming with an accurate estimation of speech statistics. Such integration can be viewed as a directed acyclic graph by accepting multi-channel input and generating multi-source output. In this letter, we design a "multi-channel input, multi-channel multi-source output" (MIMMO) speech separation system entitled "Beam-Guided {{TasNet}}", where {MC}-{Conv-{{{TasNet}}}} and {MVDR} can interact and promote each other more compactly under a directed cyclic flow. Specifically, the first stage uses Beam-{{{TasNet}}} to generate estimated single-speaker signals, which favours the separation in the second stage. The proposed framework facilitates iterative signal refinement with the guide of beamforming and seeks to reach the upper bound of the {MVDR}-based methods. Experimental results on the spatialized WSJ0-2MIX demonstrate that the Beam-Guided {{TasNet}} has achieved an {SDR} of 20.7 dB, which exceeded the baseline Beam-{{TasNet}} by 4.2 dB under the same model size and narrowed the gap with the oracle signal-based {MVDR} to 2.9 dB.},
archivePrefix = {arXiv},
arxivId = {2102.02998v3},
author = {Chen, Hangting and Zhang, Pengyuan},
eprint = {2102.02998v3},
file = {:C\:/Users/will9/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen, Zhang - Unknown - Beam-Guided {{TasNet}} An Iterative Speech Separation Framework with Multi-Channel Output.pdf:pdf},
keywords = {Index Terms-Speech separation,{MVDR},multi-channel speech pro-cessing,time-domain network},
title = {{Beam-Guided {{TasNet}}: An Iterative Speech Separation Framework with Multi-Channel Output}},
year = 2019
}
@web_page{WSJ0,
   title = {CSR-I (WSJ0) Complete - Linguistic Data Consortium},
   url = {https://catalog.ldc.upenn.edu/LDC93S6A},
   year = 2021,
   month = nov
}

@INPROCEEDINGS{Hershey2016mar,  author={Hershey, John R. and Chen, Zhuo and Le Roux, Jonathan and Watanabe, Shinji},  booktitle={ICASSP 2016},   title={Deep clustering: Discriminative embeddings for segmentation and separation},   month =mar, year={2016},  volume={},  number={},  pages={31-35},  doi={10.1109/ICASSP.2016.7471631},
url = {https://www.merl.com/publications/TR2016-003}}

@article{Gu2019,
abstract = {The end-to-end approach for single-channel speech separation has been studied recently and shown promising results. This paper extended the previous approach and proposed a new end-to-end model for multi-channel speech separation. The primary contributions of this work include 1) an integrated waveform-in waveform-out separation system in a single neural network architecture. 2) We reformulate the traditional short time Fourier transform (STFT) and inter-channel phase difference (IPD) as a function of time-domain convolution with a special kernel. 3) We further relaxed those fixed kernels to be learnable, so that the entire architecture becomes purely data-driven and can be trained from end-to-end. We demonstrate on the WSJ0 far-field speech separation task that, with the benefit of learnable spatial features, our proposed end-to-end multi-channel model significantly improved the performance of previous end-to-end single-channel method and traditional multi-channel methods.},
archivePrefix = {arXiv},
arxivId = {1905.06286},
author = {Gu, Rongzhi and Wu, Jian and Zhang, Shi-Xiong and Chen, Lianwu and Xu, Yong and Yu, Meng and Su, Dan and Zou, Yuexian and Yu, Dong},
eprint = {1905.06286},
file = {:C\:/Users/will9/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gu et al. - 2019 - End-to-End Multi-Channel Speech Separation.pdf:pdf},
journal = {arXiv},
keywords = {Index Terms: end-to-end,multi-channel speech separation,spatial embedding,time-domain},
month = {May},
publisher = {arXiv},
title = {{End-to-End Multi-Channel Speech Separation}},
url = {http://arxiv.org/abs/1905.06286},
year = {2019}
}

@article{Zhao2021UNetBasedMS,
  title={{UNet++-Based Multi-Channel Speech Dereverberation and Distant Speech Recognition}},
  author={T. Zhao and Y. Zhao and S. Wang and M. Han},
  journal={ISCSLP 2021},
  year={2021},
  month=jan
}

@inproceedings{Wang2021TeCANetTA,
  title={{TeCANet: Temporal-Contextual Attention Network for Environment-Aware Speech Dereverberation}},
  author={H. Wang and B. Wu and L. Chen and M. Yu and J. Yu and Y. Xu and S. Zhang and C. Weng and D. Su and D. Yu},
  booktitle={Interspeech 2021},
  month=sep,
  year={2021}
}

@article{Wang2021ConvolutivePF,
  title={{Convolutive Prediction for Monaural Speech Dereverberation and Noisy-Reverberant Speaker Separation}},
  author={Z. Wang and G. Wichern and J. Le Roux},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  year={2021},
  volume={29},
  pages={3476-3490}
}

@inproceedings{su20b_interspeech,
  author={J. Su and Z. Jin and A. Finkelstein},
  title={{HiFi-GAN: High-Fidelity Denoising and Dereverberation Based on Speech Deep Features in Adversarial Networks}},
  year=2020,
  month=oct,
  booktitle={Interspeech 2020},
  doi={10.21437/Interspeech.2020-2143}
}


@article{Fu2021DESNetAM,
  title={{DESNet: A Multi-Channel Network for Simultaneous Speech Dereverberation, Enhancement and Separation}},
  author={Y. Fu and J. Wu and Y. Hu and M. Xing and L. Xie},
  journal={SLT 2021},
  year={2021},
  month=jan
}

@article{Wang2020DeepLB,
  title={{Deep Learning Based Target Cancellation for Speech Dereverberation}},
  author={Z. Wang and D. Wang},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  year={2020},
  volume={28},
  pages={941-950}
}


@ARTICLE{6887314,  author={Wang, Yuxuan and Narayanan, Arun and Wang, DeLiang},  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},   title={On Training Targets for Supervised Speech Separation},   year={2014},  volume={22},  number={12},  pages={1849-1858},  doi={10.1109/TASLP.2014.2352935}}

@inproceedings{Stoller,
  crossref = {DBLP:conf/ismir/2018},
  author    = {Daniel Stoller and
               Sebastian Ewert and
               Simon Dixon},
  editor    = {Emilia G{\'{o}}mez and
               Xiao Hu and
               Eric Humphrey and
               Emmanouil Benetos},
  title     = {Wave-U-Net: {A} Multi-Scale Neural Network for End-to-End Audio Source
               Separation},
  booktitle = {Proceedings of the 19th International Society for Music Information
               Retrieval Conference, {ISMIR} 2018, Paris, France, September 23-27,
               2018},
  pages     = {334--340},
  year      = {2018},
  url       = {http://ismir2018.ircam.fr/doc/pdfs/205\_Paper.pdf},
  timestamp = {Thu, 12 Mar 2020 11:32:43 +0100},
  biburl    = {https://dblp.org/rec/conf/ismir/StollerED18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@INPROCEEDINGS{Ochiai2020,  author={Ochiai, Tsubasa and Delcroix, Marc and Ikeshita, Rintaro and Kinoshita, Keisuke and Nakatani, Tomohiro and Araki, Shoko},  booktitle={ICASSP 2020},   title={Beam-{{TasNet}}: Time-domain Audio Separation Network Meets Frequency-domain Beamformer},   year={2020}, month=May,  volume={},  number={},  pages={6384-6388},  doi={10.1109/ICASSP40776.2020.9053575}}
@misc{librimix,
      title={LibriMix: An Open-Source Dataset for Generalizable Speech Separation}, 
      author={Joris Cosentino and Manuel Pariente and Samuele Cornell and Antoine Deleforge and Emmanuel Vincent},
      year={2020},
      eprint={2005.11262},
      archivePrefix={arXiv},
      primaryClass={eess.AS}
}
@inproceedings{deng20b_Interspeech,
  author={Chengyun Deng and Yi Zhang and Shiqian Ma and Yongtao Sha and Hui Song and Xiangang Li},
  title={{Conv-TasSAN: Separative Adversarial Network Based on {Conv-{{TasNet}}}}},
  month={May},
  year=2020,
  booktitle={Interspeech 2020},
  pages={2647--2651},
  doi={10.21437/Interspeech.2020-2371}
}

@techreport{fasnet,
abstract = {Beamforming has been extensively investigated for multi-channel audio processing tasks. Recently, learning-based beamforming methods, sometimes called neural beamformers, have achieved significant improvements in both signal quality (e.g. signal-to-noise ratio (SNR)) and speech recognition (e.g. word error rate (WER)). Such systems are generally non-causal and require a large context for robust estimation of inter-channel features, which is impractical in applications requiring low-latency responses. In this paper, we propose filter-and-sum network (FaSNet), a time-domain, filter-based beamforming approach suitable for low-latency scenarios. FaSNet has a two-stage system design that first learns frame-level time-domain adaptive beamforming filters for a selected reference channel, and then calculate the filters for all remaining channels. The filtered outputs at all channels are summed to generate the final output. Experiments show that despite its small model size, FaSNet is able to outperform several traditional oracle beamformers with respect to scale-invariant signal-to-noise ratio (SI-SNR) in rever-berant speech enhancement and separation tasks. Moreover, when trained with a frequency-domain objective function on the CHiME-3 dataset, FaSNet achieves 14.3\% relative word error rate reduction (RWERR) compared with the baseline model. These results show the efficacy of FaSNet particularly in reverberant and noisy signal conditions.},
archivePrefix = {arXiv},
arxivId = {1909.13387v2},
author = {Luo, Yi and Han, Cong and Mesgarani, Nima and Ceolini, Enea and Liu, Shih-Chii},
eprint = {1909.13387v2},
file = {:C\:/Users/will9/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Luo et al. - Unknown - FASNET LOW-LATENCY ADAPTIVE BEAMFORMING FOR MULTI-MICROPHONE AUDIO PROCESSING.pdf:pdf},
keywords = {Index Terms-Beamforming,audio processing,deep learning,low-latency,multi-channel},
title = {FASNET: LOW-LATENCY ADAPTIVE BEAMFORMING FOR MULTI-MICROPHONE AUDIO PROCESSING}
}
@techreport{Ko,
abstract = {Data augmentation is a common strategy adopted to increase the quantity of training data, avoid overfitting and improve ro-bustness of the models. In this paper, we investigate audio-level speech augmentation methods which directly process the raw signal. The method we particularly recommend is to change the speed of the audio signal, producing 3 versions of the original signal with speed factors of 0.9, 1.0 and 1.1. The proposed technique has a low implementation cost, making it easy to adopt. We present results on 4 different LVCSR tasks with training data ranging from 100 hours to 1000 hours, to examine the effectiveness of audio augmentation in a variety of data scenarios. An average relative improvement of 4.3\% was observed across the 4 tasks.},
author = {Ko, Tom and Peddinti, Vijayaditya and Povey, Daniel and Khudanpur, Sanjeev},
file = {:C\:/Users/will9/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ko et al. - Unknown - Audio Augmentation for Speech Recognition.pdf:pdf},
keywords = {Index Terms: speech recognition,data augmentation,deep neural network},
title = {{Audio Augmentation for Speech Recognition}},
url = {http://www.isip.piconepress.com/}
}

@ARTICLE{XMM+15,
  author = {Xiong, F. and Meyer, B.T. and Moritz, N. and Rehr, R. and Anem{\"{u}}ller,
	J. and Gerkmann, T. and Doclo, S. and Goetze, S.},
  title = {Front-end technologies for robust {ASR} in reverberant environments
	- spectral enhancement-based dereverberation and auditory modulation
	filterbank features},
  journal = {EURASIP Journal on Advances in Signal Processing},
  year = {2015},
  volume = {2015},
  number = {1},
  eid = {70},
  abstract = {This paper presents extended techniques aiming at the improvement
	of automatic speech recognition (ASR) in single-channel scenarios
	in the context of the REVERB (REverberant Voice Enhancement and Recognition
	Benchmark) challenge. The focus is laid on the development and analysis
	of ASR front-end technologies covering speech enhancement and feature
	extraction. Speech enhancement is performed using a joint noise reduction
	and dereverberation system in the spectral domain based on estimates
	of the noise and late reverberation power spectral densities (PSDs).
	To obtain reliable estimates of the PSDs—even in acoustic conditions
	with positive direct-to-reverberation energy ratios (DRRs)—we adopt
	the statistical model of the room impulse response explicitly incorporating
	DRRs, as well in combination with a novel proposed joint estimator
	for the reverberation time T60 and the DRR. The feature extraction
	approach is inspired by processing strategies of the auditory system,
	where an amplitude modulation filterbank is applied to extract the
	temporal modulation information. These techniques were shown to improve
	the REVERB baseline in our previous work. Here, we investigate if
	similar improvements are obtained when using a state-of-the-art ASR
	framework, and to what extent the results depend on the specific
	architecture of the back-end. Apart from conventional Gaussian mixture
	model (GMM)-hidden Markov model (HMM) back-ends, we consider subspace
	GMM (SGMM)-HMMs as well as deep neural networks in a hybrid system.
	The speech enhancement algorithm is found to be helpful in almost
	all conditions, with the exception of deep learning systems in matched
	training-test conditions. The auditory feature type improves the
	baseline for all system architectures. The relative word error rate
	reduction achieved by combining our front-end techniques with current
	back-ends is 52.7\% on average with the REVERB evaluation test set
	compared to our original REVERB result.},
  doi = {10.1186/s13634-015-0256-4},
  file = {:2015_Xiong_XMM+15_Front-end_technologies_for_robust_ASR_in_reverberant_environments_art_10.1186_s13634-015-0256-4.pdf:PDF},
  keywords = {Automatic speech recognition; Dereverberation; Auditory modulation
	filterbank; Deep neural network; REVERB challenge},
  language = {English},
  publisher = {Springer International Publishing},
  url = {http://dx.doi.org/10.1186/s13634-015-0256-4}
}

@ARTICLE{selosses,  author={Kolbæk, Morten and Tan, Zheng-Hua and Jensen, Søren Holdt and Jensen, Jesper},  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},   title={On Loss Functions for Supervised Monaural Time-Domain Speech Enhancement},   year={2020},  volume={28},  number={},  pages={825-838},  doi={10.1109/TASLP.2020.2968738}}

@InProceedings{barker_asru2015,
  Title                    = {The third {CH}i{ME} speech separation and recognition challenge: dataset, task and baselines},
  Author                   = {J. Barker and R. Marxer and E. Vincent and S. Watanabe},
  Booktitle                = {Proc. IEEE ASRU 2015},
  Year                     = {2015},
  Address                  = {Scottsdale, Arizona, USA},
  Pages                    = {504-511}
}

@ARTICLE{6932438,  author={Xu, Yong and Du, Jun and Dai, Li-Rong and Lee, Chin-Hui},  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},   title={A Regression Approach to Speech Enhancement Based on Deep Neural Networks},   year={2015},  volume={23},  number={1},  pages={7-19},  doi={10.1109/TASLP.2014.2364452}}

@misc{defossez2020real,
      title={Real Time Speech Enhancement in the Waveform Domain}, 
      author={Alexandre Defossez and Gabriel Synnaeve and Yossi Adi},
      year={2020},
      eprint={2006.12847},
      archivePrefix={arXiv},
      primaryClass={eess.AS}
}

@misc{pascual2017segan,
      title={SEGAN: Speech Enhancement Generative Adversarial Network}, 
      author={Santiago Pascual and Antonio Bonafonte and Joan Serrà},
      year={2017},
      eprint={1703.09452},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{kingma2017adam,
  title={Adam: A Method for Stochastic Optimization},
  author={Diederik P. Kingma and Jimmy Ba},
  journal={CoRR},
  year={2014},
  volume={abs/1412.6980}
}
@article{composite,
author = {Lin, Zhibin and Zhou, Lu and Qiu, Xiaojun},
year = {2019},
month = {02},
pages = {144-148},
title = {A composite objective measure on subjective evaluation of speech enhancement algorithms},
volume = {145},
journal = {Applied Acoustics},
doi = {10.1016/j.apacoust.2018.10.002}
}
@inproceedings{gc-mg-minus,
  doi = {10.48550/ARXIV.2203.12369},
  author = {Close, George and Hain, Thomas and Goetze, Stefan},
  keywords = {Sound (cs.SD), Machine Learning (cs.LG), Audio and Speech Processing (eess.AS), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},
  title = {{M}etric{GAN}+/-: {I}ncreasing {R}obustness of {N}oise {R}eduction on {U}nseen {D}ata},
  booktitle={EUSIPCO 2022},
  pages        = {},
  month        = {Aug.},
  address      = {Belgrade, Serbia},
  year = {2022},
  copyright = {Creative Commons Attribution 4.0 International},
}
@InProceedings{s_e_BLSTM,
author="Weninger, Felix
and Erdogan, Hakan
and Watanabe, Shinji
and Vincent, Emmanuel
and Le Roux, Jonathan
and Hershey, John R.
and Schuller, Bj{\"o}rn",
editor="Vincent, Emmanuel
and Yeredor, Arie
and Koldovsk{\'y}, Zbyn{\v{e}}k
and Tichavsk{\'y}, Petr",
title="Speech Enhancement with LSTM Recurrent Neural Networks and its Application to Noise-Robust ASR",
booktitle="Latent Variable Analysis and Signal Separation",
year="2015",
publisher="Springer International Publishing",
address="Cham",
pages="91--99",
abstract="We evaluate some recent developments in recurrent neural network (RNN) based speech enhancement in the light of noise-robust automatic speech recognition (ASR). The proposed framework is based on Long Short-Term Memory (LSTM) RNNs which are discriminatively trained according to an optimal speech reconstruction objective. We demonstrate that LSTM speech enhancement, even when used `na{\"i}vely' as front-end processing, delivers competitive results on the CHiME-2 speech recognition task. Furthermore, simple, feature-level fusion based extensions to the framework are proposed to improve the integration with the ASR back-end. These yield a best result of 13.76 {\%} average word error rate, which is, to our knowledge, the best score to date.",
isbn="978-3-319-22482-4"
}

@ARTICLE{stoi_loss,  author={Fu, Szu-Wei and Wang, Tao-Wei and Tsao, Yu and Lu, Xugang and Kawai, Hisashi},  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},   title={End-to-End Waveform Utterance Enhancement for Direct Evaluation Metrics Optimization by Fully Convolutional Neural Networks},   year={2018},  volume={26},  number={9},  pages={1570-1584},  doi={10.1109/TASLP.2018.2821903}}

@misc{fu2018qualitynet,
      title={Quality-Net: An End-to-End Non-intrusive Speech Quality Assessment Model based on BLSTM}, 
      author={Szu-Wei Fu and Yu Tsao and Hsin-Te Hwang and Hsin-Min Wang},
      year={2018},
      eprint={1808.05344},
      archivePrefix={arXiv},
      primaryClass={cs.SD}
}


@article{TIMIT,
author = {Garofolo, J. and Lamel, Lori and Fisher, W. and Fiscus, Jonathan and Pallett, D.},
year = {1993},
month = {01},
pages = {27403},
title = {DARPA TIMIT acoustic-phonetic continous speech corpus CD-ROM. NIST speech disc 1-1.1},
volume = {93},
journal = {NASA STI/Recon Technical Report N}
}


@article{DAEenhancementEXAMPLE,
author = {lu, Xugang and Tsao, Yu and Matsuda, Shigeki and Hori, C.},
year = {2013},
month = {01},
pages = {436-440},
title = {Speech enhancement based on deep denoising Auto-Encoder},
journal = {Proc. Interspeech}
}




@INPROCEEDINGS{7945915,  author={Kounovsky, Tomas and Malek, Jiri},  booktitle={2017 IEEE International Workshop of Electronics, Control, Measurement, Signals and their Application to Mechatronics (ECMSM)},   title={Single channel speech enhancement using convolutional neural network},   year={2017},  volume={},  number={},  pages={1-5},  doi={10.1109/ECMSM.2017.7945915}}
@ARTICLE{6887314,  author={Wang, Yuxuan and Narayanan, Arun and Wang, DeLiang},  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},   title={On Training Targets for Supervised Speech Separation},   year={2014},  volume={22},  number={12},  pages={1849-1858},  doi={10.1109/TASLP.2014.2352935}}
 @article{Davis_ieeetassp1980,
 author               = {S.B. Davis and P. Mermelstein},
 journal              = {IEEE Trans. Acoust. Speech Signal Process.},
 number               = {4},
 pages                = {357--366},
 title                = {Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences},
 volume               = {28},
 year                 = {1980},
 }
 
@book{10.5555/1203100,
author = {H\"{a}nsler, Eberhard and Schmidt, Gerhard},
title = {Topics in Acoustic Echo and Noise Control: Selected Methods for the Cancellation of Acoustical Echoes, the Reduction of Background Noise, and Speech Processing (Signals and Communication Technology)},
year = {2006},
isbn = {354033212X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg}
}

@misc{musan2015,
  author = {David Snyder and Guoguo Chen and Daniel Povey},
  title = {{MUSAN}: {A} {M}usic, {S}peech, and {N}oise {C}orpus},
  year = {2015},
  eprint = {1510.08484},
  note = {arXiv:1510.08484v1}
}


@inproceedings{Liu2019,
  author={Bin Liu and Shuai Nie and Shan Liang and Wenju Liu and Meng Yu and Lianwu Chen and Shouye Peng and Changliang Li},
  title={{Jointly Adversarial Enhancement Training for Robust End-to-End Speech Recognition}},
  year=2019,
  booktitle={Proc. Interspeech 2019},
  pages={491--495},
  doi={10.21437/Interspeech.2019-1242},
  url={http://dx.doi.org/10.21437/Interspeech.2019-1242}
}
@INPROCEEDINGS{7178964,  author={Panayotov, Vassil and Chen, Guoguo and Povey, Daniel and Khudanpur, Sanjeev},  booktitle={ICASSP 2015},   title={Librispeech: An ASR corpus based on public domain audio books},   year={2015},  volume={},  number={},  pages={5206-5210},  doi={10.1109/ICASSP.2015.7178964}}

@inproceedings{aishell_2017,
  title={AIShell-1: An Open-Source Mandarin Speech Corpus and A Speech Recognition Baseline},
  author={Hui Bu Jiayu Du Xingyu Na Bengu Wu Hao Zheng},
  booktitle={Oriental COCOSDA 2017},
  pages={Submitted},
  year={2017}
}


@inproceedings{meta_phone_aware_se,
  author={Or Tal and Moshe Mandel and Felix Kreuk and Yossi Adi},
  title={{A Systematic Comparison of Phonetic Aware Techniques for Speech Enhancement}},
  year=2022,
  booktitle={Proc. Interspeech 2022},
  doi={10.21437/Interspeech.2022-695}
}

@inproceedings{nisqa_pretrained_ss,
	doi = {10.21437/interspeech.2022-10147},
  
	url = {https://doi.org/10.21437%2Finterspeech.2022-10147},
  
	year = 2022,
	month = {sep},
  
	publisher = {{ISCA}
},
  
	author = {Bastiaan Tamm and Helena Balabin and Rik Vandenberghe and Hugo Van hamme},
  
	title = {Pre-trained Speech Representations as Feature Extractors for Speech Quality Assessment in Online Conferencing Applications},
  
	booktitle = {Interspeech 2022}
}


@inproceedings{lee2022textless,
    title = "Textless Speech-to-Speech Translation on Real Data",
    author = "Lee, Ann  and
      Gong, Hongyu  and
      Duquenne, Paul-Ambroise  and
      Schwenk, Holger  and
      Chen, Peng-Jen  and
      Wang, Changhan  and
      Popuri, Sravya  and
      Adi, Yossi  and
      Pino, Juan  and
      Gu, Jiatao  and
      Hsu, Wei-Ning",
    booktitle = "ACL",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    doi = "10.18653/v1/2022.naacl-main.63",
    abstract = "We present a textless speech-to-speech translation (S2ST) system that can translate speech from one language into another language and can be built without the need of any text data. Different from existing work in the literature, we tackle the challenge in modeling multi-speaker target speech and train the systems with real-world S2ST data. The key to our approach is a self-supervised unit-based speech normalization technique, which finetunes a pre-trained speech encoder with paired audios from multiple speakers and a single reference speaker to reduce the variations due to accents, while preserving the lexical content. With only 10 minutes of paired data for speech normalization, we obtain on average 3.2 BLEU gain when training the S2ST model on the VoxPopuli S2ST dataset, compared to a baseline trained on un-normalized speech target. We also incorporate automatically mined S2ST data and show an additional 2.0 BLEU gain. To our knowledge, we are the first to establish a textless S2ST technique that can be trained with real-world data and works for multiple language pairs.",
}



@inproceedings{voxpopuli,
    title = "{V}ox{P}opuli: A Large-Scale Multilingual Speech Corpus for Representation Learning, Semi-Supervised Learning and Interpretation",
    author = "Wang, Changhan  and
      Riviere, Morgane  and
      Lee, Ann  and
      Wu, Anne  and
      Talnikar, Chaitanya  and
      Haziza, Daniel  and
      Williamson, Mary  and
      Pino, Juan  and
      Dupoux, Emmanuel",
    booktitle = "ACL Proceedings",
    month = aug,
    year = "2021",
    publisher = "Association for Computational Linguistics",
}
@inproceedings{NISQA,
	doi = {10.21437/interspeech.2021-299},
  
	url = {https://doi.org/10.21437%2Finterspeech.2021-299},
  
	year = 2021,
	month = {aug},
  
	publisher = {{ISCA}
},
  
	author = {Gabriel Mittag and Babak Naderi and Assmaa Chehadi and Sebastian Möller},
  
	title = {{NISQA}: A Deep {CNN}-Self-Attention Model for Multidimensional Speech Quality Prediction with Crowdsourced Datasets},
  
	booktitle = {Interspeech 2021}
}


@ARTICLE{stoi,  author={Taal, Cees H. and Hendriks, Richard C. and Heusdens, Richard and Jensen, Jesper},  journal={IEEE Transactions on Audio, Speech, and Language Processing},   title={An Algorithm for Intelligibility Prediction of Time–Frequency Weighted Noisy Speech},   year={2011},  volume={19},  number={7},    doi={10.1109/TASL.2011.2114881}}



@INPROCEEDINGS{pesq,  
author={Rix, A.W. and Beerends, J.G. and Hollier, M.P. and Hekstra, A.P.},  
booktitle={ICASSP 2001},   
title={Perceptual evaluation of speech quality ({PESQ})-A new method for speech quality assessment of telephone networks and codecs},   year={2001},  volume={2},  number={},  pages={749-752 vol.2},  doi={10.1109/ICASSP.2001.941023}}


@INPROCEEDINGS{5495677,  author={Gerkmann, Timo and Krawczyk, Martin and Martin, Rainer},  booktitle={ICASSP 2010},   title={Speech presence probability estimation based on temporal cepstrum smoothing},   year={2010},  volume={},  number={},  pages={4254-4257},  doi={10.1109/ICASSP.2010.5495677}}

@INPROCEEDINGS{
         Povey_ASRU2011,
         author = {Povey, Daniel and Ghoshal, Arnab and Boulianne, Gilles and Burget, Lukas and Glembek, Ondrej and Goel, Nagendra and Hannemann, Mirko and Motlicek, Petr and Qian, Yanmin and Schwarz, Petr and Silovsky, Jan and Stemmer, Georg and Vesely, Karel},
       keywords = {ASR, Automatic Speech Recognition, GMM, HTK, SGMM},
          month = dec,
          title = {The Kaldi Speech Recognition Toolkit},
      booktitle = {IEEE 2011 Workshop on Automatic Speech Recognition and Understanding},
           year = {2011},
      publisher = {IEEE Signal Processing Society},
       location = {Hilton Waikoloa Village, Big Island, Hawaii, US},
           note = {IEEE Catalog No.: CFP11SRW-USB},
}
@ARTICLE{Gerkmann2012_NoisePowerEst,
  author={Gerkmann, Timo and Hendriks, Richard C.},
  journal={IEEE Transactions on Audio, Speech, and Language Processing}, 
  title={Unbiased MMSE-Based Noise Power Estimation With Low Complexity and Low Tracking Delay}, 
  year={2012},
  volume={20},
  number={4},
  pages={1383-1393},
  doi={10.1109/TASL.2011.2180896}}
@misc{matlabNR,
author = {Richard Hendriks (2021)},
title  = {Algorithm for Noise reduction for speech enhancement (https://www.mathworks.com/matlabcentral/fileexchange/46171-algorithm-for-noise-reduction-for-speech-enhancement), MATLAB Central File Exchange.},
data = {Retrieved August 12, 2021.}
}
@article{Bell2020,
abstract = {We present a structured overview of adaptation algorithms for neural network-based speech recognition, considering both hybrid hidden Markov model / neural network systems and end-to-end neural network systems, with a focus on speaker adaptation, domain adaptation, and accent adaptation. The overview characterizes adaptation algorithms as based on embeddings, model parameter adaptation, or data augmentation. We present a meta-analysis of the performance of speech recognition adaptation algorithms, based on relative error rate reductions as reported in the literature.},
archivePrefix = {arXiv},
arxivId = {2008.06580},
author = {Bell, Peter and Fainberg, Joachim and Klejch, Ondrej and Li, Jinyu and Renals, Steve and Swietojanski, Pawel},
doi = {10.1109/ojsp.2020.3045349},
eprint = {2008.06580},
file = {:home/george/Papers/Organised/Bell et al. - 2020 - Adaptation Algorithms for Neura. - 2020 - Adaptation Algorithms for Neural Network-Based Speech Recognition An Overview: - 2020 - Adaptation Algorithms for Neural Network-Based Speech Recognition An Overview},
journal = {IEEE Open Journal of Signal Processing},
pages = {33--66},
title = {{Adaptation Algorithms for Neural Network-Based Speech Recognition: An Overview}},
volume = {2},
year = {2020}
}

@inproceedings{wav2vec,
 author = {Baevski, Alexei and Zhou, Yuhao and Mohamed, Abdelrahman and Auli, Michael},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {12449--12460},
 title = {wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations},
 volume = {33},
 year = {2020}
}

@standard{itu-p56,
title = {Objective measurement of active speech level},
number = {P.56},
institution = {International Telecoms Union (ITU)},
address = {ITU-T, Geneva, Switzerland},
year = {1993}
}
@inproceedings{commonvoice:2020,
  author = {Ardila, R. and Branson, M. and Davis, K. and Henretty, M. and Kohler, M. and Meyer, J. and Morais, R. and Saunders, L. and Tyers, F. M. and Weber, G.},
  title = {Common Voice: A Massively-Multilingual Speech Corpus},
  booktitle = {Proc.~Conf.~on Language Resources and Evaluation},
  year = 2020
}
@inproceedings{xlsr,
  author={Arun Babu and Changhan Wang and Andros Tjandra and Kushal Lakhotia and Qiantong Xu and Naman Goyal and Kritika Singh and Patrick {von Platen} and Yatharth Saraf and Juan Pino and Alexei Baevski and Alexis Conneau and Michael Auli},
  title={{XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale}},
  year=2022,
  booktitle={Proc. Interspeech 2022},
  pages={2278--2282},
  doi={10.21437/Interspeech.2022-143}
}

@inproceedings{superb,
  author={Shu-wen Yang and Po-Han Chi and Yung-Sung Chuang and Cheng-I Jeff Lai and Kushal Lakhotia and Yist Y. Lin and Andy T. Liu and Jiatong Shi and Xuankai Chang and Guan-Ting Lin and Tzu-Hsien Huang and Wei-Cheng Tseng and Ko-tik Lee and Da-Rong Liu and Zili Huang and Shuyan Dong and Shang-Wen Li and Shinji Watanabe and Abdelrahman Mohamed and Hung-yi Lee},
  title={{SUPERB: Speech Processing Universal PERformance Benchmark}},
  year=2021,
  booktitle={Proc. Interspeech 2021},
  doi={10.21437/Interspeech.2021-1775}
}

@inproceedings{transformer,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Attention is All you Need},
 volume = {30},
 year = {2017}
}

@inproceedings{bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proc. of ACL 2019",
    year = "2019",
    address = "Minneapolis",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@misc{chang2021multichannel,
      title={Multi-Channel Transformer Transducer for Speech Recognition}, 
      author={Feng-Ju Chang and Martin Radfar and Athanasios Mouchtaris and Maurizio Omologo},
      year={2021},
      eprint={2108.12953},
      archivePrefix={arXiv},
      primaryClass={eess.AS}
}
@inproceedings{10.3115/1075527.1075614,
author = {Paul, Douglas B. and Baker, Janet M.},
title = {The Design for the Wall Street Journal-Based CSR Corpus},
year = {1992},
isbn = {1558602720},
publisher = {Association for Computational Linguistics},
address = {USA},
url = {https://doi.org/10.3115/1075527.1075614},
doi = {10.3115/1075527.1075614},
abstract = {The DARPA Spoken Language System (SLS) community has long taken a leadership position
in designing, implementing, and globally distributing significant speech corpora widely
used for advancing speech recognition research. The Wall Street Journal (WSJ) CSR
Corpus described here is the newest addition to this valuable set of resources. In
contrast to previous corpora, the WSJ corpus will provide DARPA its first general-purpose
English, large vocabulary, natural language, high perplexity, corpus containing significant
quantities of both speech data (400 hrs.) and text data (47M words), thereby providing
a means to integrate speech recognition and natural language processing in application
domains with high potential practical value. This paper presents the motivating goals,
acoustic data design, text processing steps, lexicons, and testing paradigms incorporated
into the multi-faceted WSJ CSR Corpus.},
booktitle = {Proceedings of the Workshop on Speech and Natural Language},
pages = {357–362},
numpages = {6},
location = {Harriman, New York},
series = {HLT '91}
}
@misc{parvathala_kodukula_andhavarapu_2021, title={Neural Comb Filtering using Sliding Window Attention Network for Speech Enhancement}, url={https://www.techrxiv.org/articles/preprint/Neural_Comb_Filtering_using_Sliding_Window_Attention_Network_for_Speech_Enhancement/15051972/1}, DOI={10.36227/techrxiv.15051972.v1}, abstractNote={
In this paper, we demonstrate the significance of restoring harmonics of the fundamental frequency (pitch) in deep neural network (DNN) based speech enhancement. We propose a sliding-window attention network to regress the spectral magnitude mask (SMM) from the noisy speech signal. Even though the network parameters can be estimated by minimizing the mask loss, it does not restore the pitch harmonics, especially at higher frequencies. In this paper, we propose to restore the pitch harmonics in the spectral domain by minimizing cepstral loss around the pitch peak. The network parameters are estimated using a combination of the mask loss and cepstral loss. The proposed network architecture functions like an adaptive comb filter on voiced segments, and emphasizes the pitch harmonics in the speech spectrum. The proposed approach achieves comparable performance with the state-of-the-art methods with much lesser computational complexity.
}, publisher={TechRxiv}, author={Parvathala, Venkatesh and Kodukula, Sri Rama Murty and Andhavarapu, Siva Ganesh}, year={2021}, month={Jul} } 
@inproceedings{xue21_interspeech,
  author={Cheng Xue and Weilong Huang and Weiguang Chen and Jinwei Feng},
  title={{Real-Time Multi-Channel Speech Enhancement Based on Neural Network Masking with Attention Model}},
  year=2021,
  booktitle={Proc. Interspeech 2021},
  pages={1862--1866},
  doi={10.21437/Interspeech.2021-2266}
}
@misc{li2021embedding,
      title={Embedding and Beamforming: All-neural Causal Beamformer for Multichannel Speech Enhancement}, 
      author={Andong Li and Wenzhe Liu and Chengshi Zheng and Xiaodong Li},
      year={2021},
      eprint={2109.00265},
      archivePrefix={arXiv},
      primaryClass={cs.SD}
}
@inproceedings{fu2019metricgan,
      title={MetricGAN: Generative Adversarial Networks based Black-box Metric Scores Optimization for Speech Enhancement}, 
      author={Szu-Wei Fu and Chien-Feng Liao and Yu Tsao and Shou-De Lin},
      year={2019},
      booktitle={Proc. Interspeech 2019},
      eprint={1905.04874},
      archivePrefix={arXiv},
      primaryClass={cs.SD}
}



@misc{hubert,
  doi = {10.48550/ARXIV.2106.07447},
  

  
  author = {Hsu, Wei-Ning and Bolte, Benjamin and {Hubert Tsai}, Yao-Hung and Lakhotia, Kushal and Salakhutdinov, Ruslan and Mohamed, Abdelrahman},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Audio and Speech Processing (eess.AS), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},
  
  title = {HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{NRbook,
author = {Hendriks, Richard and Gerkmann, Timo and Jensen, Jesper},
year = {2013},
month = {01},
pages = {1-80},
title = {DFT-Domain Based Single-Microphone Noise Reduction for Speech Enhancement: A Survey of the State of the Art},
volume = {9},
journal = {Synthesis Lectures on Speech and Audio Processing},
doi = {10.2200/S00473ED1V01Y201301SAP011}
}
@article{bagchi2018spectral,
      title={Spectral feature mapping with mimic loss for robust speech recognition}, 
      author={Deblin Bagchi and Peter Plantinga and Adam Stiff and Eric Fosler-Lussier},
      year={2018},
      eprint={1803.09816},
      archivePrefix={arXiv},
      primaryClass={cs.SD}
}
@misc{reddy2021dnsmos,
      title={DNSMOS: A Non-Intrusive Perceptual Objective Speech Quality metric to evaluate Noise Suppressors}, 
      author={Chandan K A Reddy and Vishak Gopal and Ross Cutler},
      year={2021},
      eprint={2010.15258},
      archivePrefix={arXiv},
      primaryClass={cs.SD}
}
@article{fu2021metricganu,
      title={MetricGAN-U: Unsupervised speech enhancement/ dereverberation based only on noisy/ reverberated speech}, 
      author={Szu-Wei Fu and Cheng Yu and Kuo-Hsuan Hung and Mirco Ravanelli and Yu Tsao},
      year={2021},
      eprint={2110.05866},
      archivePrefix={arXiv},
      primaryClass={cs.SD}
}
@inproceedings{reddy2021interspeech,
  title={INTERSPEECH 2021 Deep Noise Suppression Challenge},
  author={Reddy, Chandan KA and Dubey, Harishchandra and Koishida, Kazuhito and Nair, Arun and Gopal, Vishak and Cutler, Ross and Braun, Sebastian and Gamper, Hannes and Aichner, Robert and Srinivasan, Sriram},
  booktitle={INTERSPEECH},
  year={2021}
}


@article{chai2018acousticsguided,
      title={Acoustics-guided evaluation (AGE): a new measure for estimating performance of speech enhancement algorithms for robust ASR}, 
      author={Li Chai and Jun Du and Chin-Hui Lee},
      year={2018},
      eprint={1811.11517},
      archivePrefix={arXiv},
      primaryClass={eess.AS}
}

@InProceedings{lex_sim,
author="Bella, G{\'a}bor
and Batsuren, Khuyagbaatar
and Giunchiglia, Fausto",
editor="Ek{\v{s}}tein, Kamil
and P{\'a}rtl, Franti{\v{s}}ek
and Konop{\'i}k, Miloslav",
title="A Database and Visualization of the Similarity of Contemporary Lexicons",
booktitle="Text, Speech, and Dialogue",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="95--104",
abstract="Lexical similarity data, quantifying the ``proximity'' of languages based on the similarity of their lexicons, has been increasingly used to estimate the cross-lingual reusability of language resources, for tasks such as bilingual lexicon induction or cross-lingual transfer. Existing similarity data, however, originates from the field of comparative linguistics, computed from very small expert-curated vocabularies that are not supposed to be representative of modern lexicons. We explore a different, fully automated approach to lexical similarity computation, based on an existing 8-million-entry cognate database created from online lexicons orders of magnitude larger than the word lists typically used in linguistics. We compare our results to earlier efforts, and automatically produce intuitive visualizations that have traditionally been hand-crafted. With a new, freely available database of over 27 thousand language pairs over 331 languages, we hope to provide more relevant data to cross-lingual NLP applications, as well as material for the synchronic study of contemporary lexicons.",
isbn="978-3-030-83527-9"
}


@ARTICLE{FFASRHaebUmbach,
  author={Haeb-Umbach, Reinhold and Heymann, Jahn and Drude, Lukas and Watanabe, Shinji and Delcroix, Marc and Nakatani, Tomohiro},
  journal={Proceedings of the IEEE}, 
  title={Far-Field Automatic Speech Recognition}, 
  year={2021},
  volume={109},
  number={2},
  pages={124-148},
  doi={10.1109/JPROC.2020.3018668}}
  
  @misc{li2021multimetric,
      title={Multi-Metric Optimization using Generative Adversarial Networks for Near-End Speech Intelligibility Enhancement}, 
      author={Haoyu Li and Junichi Yamagishi},
      year={2021},
      eprint={2104.08499},
      archivePrefix={arXiv},
      primaryClass={eess.AS}
}
@INPROCEEDINGS{dolcoMVDRenhance,
  author={Tammen, Marvin and Doclo, Simon},
  booktitle={ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Deep Multi-Frame MVDR Filtering for Single-Microphone Speech Enhancement}, 
  year={2021},
  volume={},
  number={},
  pages={8443-8447},
  doi={10.1109/ICASSP39728.2021.9413775}}
@misc{fu2021metricgan,
      title={MetricGAN+: An Improved Version of MetricGAN for Speech Enhancement}, 
      author={Szu-Wei Fu and Cheng Yu and Tsun-An Hsieh and Peter Plantinga and Mirco Ravanelli and Xugang Lu and Yu Tsao},
      year={2021},
      eprint={2104.03538},
      archivePrefix={arXiv},
      primaryClass={cs.SD}
}
@inproceedings{ValentiniBotinhao2016InvestigatingRS,
  title={{Investigating RNN-based speech enhancement methods for noise-robust Text-to-Speech}},
  author={Cassia Valentini-Botinhao and X. Wang and Shinji Takaki and J. Yamagishi},
  booktitle={ISCA Speech Synthesis Workshop},
  year={2016}
}
@article{wavLM,
author = {Chen, Sanyuan and Wang, Chengyi and Chen, Zhengyang and Wu, Yu and Liu, Shujie and Li, Jinyu and Kanda, Naoyuki and Yoshioka, Takuya and Xiao, Xiong and Wu, Jian and Zhou, Long and Ren, Shuo and Qian, Yanmin and Qian, Yao and Zeng, Michael and Yu, Xiangzhan and Wei, Furu},
year = {2022},
month = {10},
pages = {1-14},
title = {WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing},
volume = {16},
journal = {IEEE J.~Sel.~Topics in Signal Processing},
doi = {10.1109/JSTSP.2022.3188113}
}

  @ARTICLE{5547575,  author={Falk, Tiago H. and Zheng, Chenxi and Chan, Wai-Yip},  journal={IEEE Transactions on Audio, Speech, and Language Processing},   title={A Non-Intrusive Quality and Intelligibility Measure of Reverberant and Dereverberated Speech},   year={2010},  volume={18},  number={7},  pages={1766-1774},  doi={10.1109/TASL.2010.2052247}}
  
  @article{DBLP:journals/corr/abs-1808-05344,
  author    = {Szu{-}Wei Fu and
               Yu Tsao and
               Hsin{-}Te Hwang and
               Hsin{-}Min Wang},
  title     = {Quality-Net: An End-to-End Non-intrusive Speech Quality Assessment
               Model based on {BLSTM}},
  journal   = {CoRR},
  volume    = {abs/1808.05344},
  year      = {2018},
  url       = {http://arxiv.org/abs/1808.05344},
  archivePrefix = {arXiv},
  eprint    = {1808.05344},
  timestamp = {Fri, 20 Dec 2019 16:00:29 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1808-05344.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@misc{vb-demand,
title = {Noisy speech database for training speech enhancement algorithms and TTS models},
year = {2017},
author={Valentini-Botinhao, Cassia},
url={https://doi.org/10.7488/ds/2117}
}

@misc{demand,
  author       = {Thiemann, Joachim and
                  Ito, Nobutaka and
                  Vincent, Emmanuel},
  title        = {{DEMAND: a collection of multi-channel recordings 
                   of acoustic noise in diverse environments}},
  month        = jun,
  year         = 2013,
  publisher    = {Zenodo},
  version      = {1.0},
  doi          = {10.5281/zenodo.1227121},
}

@misc{cs2022quality,
  doi = {10.48550/ARXIV.2203.16032},
  
  url = {https://arxiv.org/abs/2203.16032},
  
  author = {Yi, Gaoxiong and Xiao, Wei and Xiao, Yiming and Naderi, Babak and Möller, Sebastian and Wardah, Wafaa and Mittag, Gabriel and Cutler, Ross and Zhang, Zhuohuang and Williamson, Donald S. and Chen, Fei and Yang, Fuzheng and Shang, Shidong},
  
  keywords = {Sound (cs.SD), Audio and Speech Processing (eess.AS), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},
  
  title = {ConferencingSpeech 2022 Challenge: Non-intrusive Objective Speech Quality Assessment (NISQA) Challenge for Online Conferencing Applications},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{user_pref_se_mos,
author = {Akihiko Sugiyama, Osamu Shimada, Toshiyuki Nomura},
title = {USER PREFERENCE BETWEEN RESIDUAL NOISE AND SPEECH DISTORTION IN SPEECH ENHANCEMENT},
year = {2022},
booktitle = {IWAENC 2022}
}


@misc{speechbrain,
  title={{SpeechBrain}: A General-Purpose Speech Toolkit},
  author={Mirco Ravanelli and Titouan Parcollet and Peter Plantinga and Aku Rouhe and Samuele Cornell and Loren Lugosch and Cem Subakan and Nauman Dawalatabad and Abdelwahab Heba and Jianyuan Zhong and Ju-Chieh Chou and Sung-Lin Yeh and Szu-Wei Fu and Chien-Feng Liao and Elena Rastorgueva and François Grondin and William Aris and Hwidong Na and Yan Gao and Renato De Mori and Yoshua Bengio},
  year={2021},
  eprint={2106.04624},
  archivePrefix={arXiv},
  primaryClass={eess.AS},
  note={arXiv:2106.04624}
}
@inproceedings{perceptual_quality_phone_fort,
  doi = {10.48550/ARXIV.2010.15174},
  booktitle = {Interspeech'21},
  author = {Hsieh, Tsun-An and Yu, Cheng and Fu, Szu-Wei and Lu, Xugang and Tsao, Yu},
  keywords = {Sound (cs.SD), Machine Learning (cs.LG), Audio and Speech Processing (eess.AS), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},
  title = {Improving Perceptual Quality by Phone-Fortified Perceptual Loss using Wasserstein Distance for Speech Enhancement},
  year = {2021},
}

@misc{kawanaka2020stable,
      title={Stable Training of DNN for Speech Enhancement based on Perceptually-Motivated Black-Box Cost Function}, 
      author={Masaki Kawanaka and Yuma Koizumi and Ryoichi Miyazaki and Kohei Yatabe},
      year={2020},
      eprint={2002.05879},
      archivePrefix={arXiv},
      primaryClass={eess.AS}
}

@book{Loizou13SpeechEnhancementBook,
author = {Loizou, Philipos C.},
title = {Speech Enhancement: Theory and Practice},
year = {2013},
isbn = {1466504218},
publisher = {CRC Press, Inc.},
address = {USA},
edition = {2nd},
abstract = {With the proliferation of mobile devices and hearing devices, including hearing aids and cochlear implants, there is a growing and pressing need to design algorithms that can improve speech intelligibility without sacrificing quality. Responding to this need, Speech Enhancement: Theory and Practice, Second Edition introduces readers to the basic problems of speech enhancement and the various algorithms proposed to solve these problems. Updated and expanded, this second edition of the bestselling textbook broadens its scope to include evaluation measures and enhancement algorithms aimed at improving speech intelligibility. Fundamentals, Algorithms, Evaluation, and Future Steps Organized into four parts, the book begins with a review of the fundamentals needed to understand and design better speech enhancement algorithms. The second part describes all the major enhancement algorithms and, because these require an estimate of the noise spectrum, also covers noise estimation algorithms. The third part of the book looks at the measures used to assess the performance, in terms of speech quality and intelligibility, of speech enhancement methods. It also evaluates and compares several of the algorithms. The fourth part presents binary mask algorithms for improving speech intelligibility under ideal conditions. In addition, it suggests steps that can be taken to realize the full potential of these algorithms under realistic conditions. Whats New in This Edition Updates in every chapter A new chapter on objective speech intelligibility measures A new chapter on algorithms for improving speech intelligibility Real-world noise recordings (on accompanying CD) MATLAB code for the implementation of intelligibility measures (on accompanying CD) MATLAB and C/C++ code for the implementation of algorithms to improve speech intelligibility (on accompanying CD) Valuable Insights from a Pioneer in Speech Enhancement Clear and concise, this book explores how human listeners compensate for acoustic noise in noisy environments. Written by a pioneer in speech enhancement and noise reduction in cochlear implants, it is an essential resource for anyone who wants to implement or incorporate the latest speech enhancement algorithms to improve the quality and intelligibility of speech degraded by noise. Includes a CD with Code and Recordings The accompanying CD provides MATLAB implementations of representative speech enhancement algorithms as well as speech and noise databases for the evaluation of enhancement algorithms.}
}

@ARTICLE{MCSignalEnhancementDocloEtAl,
  author={Doclo, Simon and Kellermann, Walter and Makino, Shoji and Nordholm, Sven Erik},
  journal={IEEE Signal Processing Magazine}, 
  title={Multichannel Signal Enhancement Algorithms for Assisted Listening Devices: Exploiting spatial diversity using multiple microphones}, 
  year={2015},
  volume={32},
  number={2},
  pages={18-30},
  doi={10.1109/MSP.2014.2366780}}

--
EOF
