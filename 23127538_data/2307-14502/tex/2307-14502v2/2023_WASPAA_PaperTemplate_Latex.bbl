\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\def\UrlFont{\rmfamily}
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand\BIBentrySTDinterwordspacing{\spaceskip=0pt\relax}
\providecommand\BIBentryALTinterwordstretchfactor{4}
\providecommand\BIBentryALTinterwordspacing{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand\BIBforeignlanguage[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}

\bibitem{Loizou13SpeechEnhancementBook}
P.~C. Loizou, \emph{Speech Enhancement: Theory and Practice}, 2nd~ed.\hskip 1em
  plus 0.5em minus 0.4em\relax USA: CRC Press, Inc., 2013.

\bibitem{MCSignalEnhancementDocloEtAl}
S.~Doclo, W.~Kellermann, S.~Makino, and S.~E. Nordholm, ``Multichannel signal
  enhancement algorithms for assisted listening devices: Exploiting spatial
  diversity using multiple microphones,'' \emph{IEEE Signal Processing
  Magazine}, vol.~32, no.~2, 2015.

\bibitem{Moritz2017}
N.~Moritz, K.~Adilo{\u{g}}lu, J.~Anem{\"{u}}ller, S.~Goetze, and B.~Kollmeier,
  ``Multi-channel speech enhancement and amplitude modulation analysis for
  noise robust automatic speech recognition,'' \emph{Computer Speech \&
  Language}, vol.~46, November 2017.

\bibitem{FFASRHaebUmbach}
R.~Haeb-Umbach, J.~Heymann, L.~Drude, S.~Watanabe, M.~Delcroix, and
  T.~Nakatani, ``Far-field automatic speech recognition,'' \emph{Proceedings of
  the IEEE}, vol. 109, no.~2, 2021.

\bibitem{fu2021metricgan}
S.-W. Fu, C.~Yu, T.-A. Hsieh, P.~Plantinga, M.~Ravanelli, X.~Lu, and Y.~Tsao,
  ``Metricgan+: An improved version of metricgan for speech enhancement,''
  2021.

\bibitem{WHAMR}
M.~Maciejewski, G.~Wichern, and J.~Le~Roux, ``{WHAMR}!: Noisy and reverberant
  single-channel speech separation,'' in \emph{ICASSP 2020}, May 2020.

\bibitem{mimospeech}
X.~Chang, W.~Zhang, Y.~Qian, J.~L. Roux, and S.~Watanabe, ``{MIMO-SPEECH:
  End-to-End Multi-Channel Multi-Speaker Speech Recognition},'' \emph{ASRU
  2019}, October 2019.

\bibitem{close2023PAMGAN}
G.~Close, T.~Hain, and S.~Goetze, ``{PAMGAN+/-: Improving Phase-Aware Speech
  Enhancement Performance via Expanded Discriminator Training},'' in \emph{AES
  Convention Europe 2023}, 2023.

\bibitem{PESQ}
A.~Rix, J.~Beerends, M.~Hollier, and A.~Hekstra, ``Perceptual evaluation of
  speech quality ({PESQ})-a new method for speech quality assessment of
  telephone networks and codecs,'' in \emph{ICASSP 2001}, vol.~2, 2001.

\bibitem{STOI}
C.~H. Taal, R.~C. Hendriks, R.~Heusdens, and J.~Jensen, ``An algorithm for
  intelligibility prediction of time–frequency weighted noisy speech,''
  \emph{IEEE Transactions on Audio, Speech, and Language Processing}, vol.~19,
  no.~7, 2011.

\bibitem{LeRoux}
J.~L. Roux, S.~Wisdom, H.~Erdogan, and J.~R. Hershey, ``{SDR – Half-baked or
  Well Done?}'' in \emph{ICASSP 2019}, May 2019.

\bibitem{Avila2016Quality}
A.~Avila, B.~Cauchi, S.~Goetze, S.~Doclo, and T.~Falk, ``Performance comparison
  of intrusive and non-intrusive instrumental quality measures for enhanced
  speech,'' in \emph{2016 IEEE International Workshop on Acoustic Signal
  Enhancement (IWAENC)}, 2016.

\bibitem{close22_interspeech}
G.~Close, S.~Hollands, T.~Hain, and S.~Goetze, ``{Non-intrusive Speech
  Intelligibility Metric Prediction for Hearing Impaired Individuals for the
  Clarity Prediction Challenge 1},'' in \emph{Proc. Interspeech 2022}, 2022.

\bibitem{cauchi2019QualityLSTM}
B.~Cauchi, K.~Siedenburg, J.~F. Santos, T.~H. Falk, S.~Doclo, and S.~Goetze,
  ``{Non-Intrusive Speech Quality Prediction Using Modulation Energies and
  LSTM-Network},'' \emph{IEEE/ACM Transactions on Audio, Speech, and Language
  Processing}, vol.~27, no.~7, 2019.

\bibitem{reddy2021dnsmos}
C.~K.~A. Reddy, V.~Gopal, and R.~Cutler, ``Dnsmos: A non-intrusive perceptual
  objective speech quality metric to evaluate noise suppressors,'' 2021.

\bibitem{MOSnet}
C.-C. Lo, S.-W. Fu, W.-C. Huang, X.~Wang, J.~Yamagishi, Y.~Tsao, and H.-M.
  Wang, ``{MOSNet: Deep Learning-Based Objective Assessment for Voice
  Conversion},'' in \emph{Proc. Interspeech 2019}, 2019.

\bibitem{wav2vec}
A.~Baevski, Y.~Zhou, A.~Mohamed, and M.~Auli, ``wav2vec 2.0: A framework for
  self-supervised learning of speech representations,'' in \emph{Advances in
  Neural Information Processing Systems}, vol.~33, 2020.

\bibitem{hubert}
W.-N. Hsu, B.~Bolte, Y.-H. {Hubert Tsai}, K.~Lakhotia, R.~Salakhutdinov, and
  A.~Mohamed, ``Hubert: Self-supervised speech representation learning by
  masked prediction of hidden units,'' 2021.

\bibitem{superb}
S.~wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I.~J. Lai, K.~Lakhotia, Y.~Y. Lin,
  A.~T. Liu, J.~Shi, X.~Chang, G.-T. Lin, T.-H. Huang, W.-C. Tseng, K.~tik Lee,
  D.-R. Liu, Z.~Huang, S.~Dong, S.-W. Li, S.~Watanabe, A.~Mohamed, and
  H.~yi~Lee, ``{SUPERB: Speech Processing Universal PERformance Benchmark},''
  in \emph{Proc. Interspeech 2021}, 2021.

\bibitem{close2023perceive}
G.~Close, W.~Ravenscroft, T.~Hain, and S.~Goetze, ``Perceive and predict:
  Self-supervised speech representation based loss functions for speech
  enhancement,'' in \emph{ICASSP 2023}, 2023.

\bibitem{meta_phone_aware_se}
O.~Tal, M.~Mandel, F.~Kreuk, and Y.~Adi, ``{A Systematic Comparison of Phonetic
  Aware Techniques for Speech Enhancement},'' in \emph{Proc. Interspeech 2022},
  2022.

\bibitem{perceptual_quality_phone_fort}
T.-A. Hsieh, C.~Yu, S.-W. Fu, X.~Lu, and Y.~Tsao, ``Improving perceptual
  quality by phone-fortified perceptual loss using wasserstein distance for
  speech enhancement,'' in \emph{Interspeech'21}, 2021.

\bibitem{transformer}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez, L.~u.
  Kaiser, and I.~Polosukhin, ``Attention is all you need,'' in \emph{Advances
  in Neural Information Processing Systems}, vol.~30, 2017.

\bibitem{bert}
J.~Devlin, M.-W. Chang, K.~Lee, and K.~Toutanova, ``{BERT}: Pre-training of
  deep bidirectional transformers for language understanding,'' in \emph{Proc.
  of ACL 2019}, Minneapolis, 2019.

\bibitem{7178964}
V.~Panayotov, G.~Chen, D.~Povey, and S.~Khudanpur, ``Librispeech: An asr corpus
  based on public domain audio books,'' in \emph{ICASSP 2015}, 2015.

\bibitem{lee2022textless}
A.~Lee, H.~Gong, P.-A. Duquenne, H.~Schwenk, P.-J. Chen, C.~Wang, S.~Popuri,
  Y.~Adi, J.~Pino, J.~Gu, and W.-N. Hsu, ``Textless speech-to-speech
  translation on real data,'' in \emph{ACL}, Seattle, United States, July 2022.

\bibitem{voxpopuli}
C.~Wang, M.~Riviere, A.~Lee, A.~Wu, C.~Talnikar, D.~Haziza, M.~Williamson,
  J.~Pino, and E.~Dupoux, ``{V}ox{P}opuli: A large-scale multilingual speech
  corpus for representation learning, semi-supervised learning and
  interpretation,'' in \emph{ACL Proceedings}.\hskip 1em plus 0.5em minus
  0.4em\relax Association for Computational Linguistics, Aug. 2021.

\bibitem{xlsr}
A.~Babu, C.~Wang, A.~Tjandra, K.~Lakhotia, Q.~Xu, N.~Goyal, K.~Singh, P.~{von
  Platen}, Y.~Saraf, J.~Pino, A.~Baevski, A.~Conneau, and M.~Auli, ``{XLS-R:
  Self-supervised Cross-lingual Speech Representation Learning at Scale},'' in
  \emph{Proc. Interspeech 2022}, 2022.

\bibitem{ValentiniBotinhao2016InvestigatingRS}
C.~Valentini-Botinhao, X.~Wang, S.~Takaki, and J.~Yamagishi, ``{Investigating
  RNN-based speech enhancement methods for noise-robust Text-to-Speech},'' in
  \emph{ISCA Speech Synthesis Workshop}, 2016.

\bibitem{demand}
J.~Thiemann, N.~Ito, and E.~Vincent, ``{DEMAND: a collection of multi-channel
  recordings of acoustic noise in diverse environments},'' June 2013.

\bibitem{itu-p56}
\emph{Objective measurement of active speech level}, International Telecoms
  Union (ITU) Std. P.56, 1993.

\bibitem{commonvoice:2020}
R.~Ardila, M.~Branson, K.~Davis, M.~Henretty, M.~Kohler, J.~Meyer, R.~Morais,
  L.~Saunders, F.~M. Tyers, and G.~Weber, ``Common voice: A
  massively-multilingual speech corpus,'' in \emph{Proc.~Conf.~on Language
  Resources and Evaluation}, 2020.

\bibitem{speechbrain}
M.~Ravanelli, T.~Parcollet, P.~Plantinga, A.~Rouhe, S.~Cornell, L.~Lugosch,
  C.~Subakan, N.~Dawalatabad, A.~Heba, J.~Zhong, J.-C. Chou, S.-L. Yeh, S.-W.
  Fu, C.-F. Liao, E.~Rastorgueva, F.~Grondin, W.~Aris, H.~Na, Y.~Gao, R.~D.
  Mori, and Y.~Bengio, ``{SpeechBrain}: A general-purpose speech toolkit,''
  2021, arXiv:2106.04624.

\bibitem{kingma2017adam}
D.~P. Kingma and J.~Ba, ``Adam: A method for stochastic optimization,''
  \emph{CoRR}, vol. abs/1412.6980, 2014.

\bibitem{close2022}
G.~Close, T.~Hain, and S.~Goetze, ``{MetricGAN+/-: Increasing Robustness of
  Noise Reduction on Unseen Data},'' in \emph{EUSIPCO 2022}, Belgrade, Serbia,
  Aug. 2022.

\bibitem{composite}
Z.~Lin, L.~Zhou, and X.~Qiu, ``A composite objective measure on subjective
  evaluation of speech enhancement algorithms,'' \emph{Applied Acoustics}, vol.
  145, 02 2019.

\bibitem{lex_sim}
G.~Bella, K.~Batsuren, and F.~Giunchiglia, ``A database and visualization of
  the similarity of contemporary lexicons,'' in \emph{Text, Speech, and
  Dialogue}, K.~Ek{\v{s}}tein, F.~P{\'a}rtl, and M.~Konop{\'i}k, Eds.\hskip 1em
  plus 0.5em minus 0.4em\relax Cham: Springer International Publishing, 2021.

\bibitem{wavLM}
S.~Chen, C.~Wang, Z.~Chen, Y.~Wu, S.~Liu, J.~Li, N.~Kanda, T.~Yoshioka,
  X.~Xiao, J.~Wu, L.~Zhou, S.~Ren, Y.~Qian, Y.~Qian, M.~Zeng, X.~Yu, and
  F.~Wei, ``Wavlm: Large-scale self-supervised pre-training for full stack
  speech processing,'' \emph{IEEE J.~Sel.~Topics in Signal Processing},
  vol.~16, 10 2022.

\end{thebibliography}
