\section{Related  Work}\label{sec:related}
\vspace{-0.25em}

\noindent\textbf{Motion Datasets.}
For many years, the Middlebury dataset~\cite{middlebury} was the primary benchmark for stereo and motion estimation methods. This dataset contains a mix of synthetic and real data, with high-quality annotations, but is a very small dataset by today's standards ($<100$ frames). The MPI Sintel dataset~\cite{sintel} provided a large step forward in visual and motion diversity, by extracting $1064$ frames from a movie animated in Blender~\cite{blender}, including lighting variation, shadows, specular reflections, complex materials, and atmospheric effects. Our dataset is similar to Sintel, but is orders of magnitude larger, both in overall frame count and in the length of the video clips, and is also far more realistic, making use of rendering advancements in Blender. 

The KITTI dataset~\cite{kitti} provides stereo and flow annotations for real-world driving scenes. Real-world annotation is difficult, and therefore approximated: the authors use LiDAR combined with egomotion information to estimate motion in the static parts of the scene, and then fit 3D models to the cars to estimate the motion of car pixels. We opt for synthetic data generation to avoid these approximations and to ensure perfect fine-grained ground truth. 

A series of synthetic datasets have been introduced specifically for training neural nets for motion estimation: FlyingChairs \cite{flownet}, FlyingThings3D \cite{mayer2016large}, FlyingThings++ \cite{harley2022particle}, AutoFlow~\cite{sun2021autoflow}, and Kubric~\cite{greff2022kubric}. These datasets consist of random objects moving in random directions on random backgrounds, yielding unrealistic but extremely diverse data. Of these, only FlyingThings++~\cite{harley2022particle} and Kubric-MOVi-E~\cite{greff2022kubric} provide multi-frame trajectories (as opposed to 2-frame motion). Our dataset has similar motivations, in terms of enabling generalization via diversity, but is targeted toward longer-range tracking---across thousands of frames, instead of merely dozens. Our dataset also includes humans, which interact with each other and with the scene, which we hope will give advantage to methods that use high-level contextual cues (such as scene layout), in addition to the low-level motion and appearance signals.

The recently released TAP-Vid benchmark~\cite{doersch2022tap} aligns well with our work: it argues for the importance of fine-grained multi-frame tracking, and suggests a train/test pipeline where training happens in synthetic data (Kubric-MOVi-E~\cite{greff2022kubric} and RGB-stacking~\cite{lee2021beyond}), and testing happens in real data, which consists of manually annotated point tracks for videos in Kinetics~\cite{kay2017kinetics} and DAVIS~\cite{davis2017}. We show that by training in our new richer synthetic data, we improve performance on the TAP-Vid test set. The ``test'' split of our dataset also covers a gap in TAP-Vid by providing accurate annotations \textit{during occlusions}, while TAP-Vid only provides annotations during visibility. 

There is also a long line of work which trains directly on unlabelled data, using a variety of auxiliary objectives to encourage tracking to emerge~\cite{yu2016back,vondrick2018tracking,wang2019learning,jabri2020walk,bian2022learning}.
An advantage of these works is that they need not worry about a sim-to-real gap, because they train directly on real video. On the other hand, current rendering tools deliver such high photo-realism that the risk of a sim-to-real gap may be much smaller than seen in years past, making synthetic supervision increasingly viable~\cite{Dosovitskiy17,wang2020tartanair}. 

\vspace{1mm}\noindent\textbf{Motion Understanding.}
Early motion estimation methods cast point tracking as an optimization problem defined on handcrafted features \cite{Horn:1981,lucas1981iterative,tomasi1991detection,brox_densepoint,Brox2011LargeDO}, and these techniques continue to drive structure-from-motion \cite{bregler_recover,kong_deepnrsfm,novotny2019c3dpo} and simultaneous localization and mapping systems \cite{taketomi2017visual}. Given the success of neural networks in other computer vision tasks, researchers now typically train deep neural nets to solve the task in a feedforward manner~\cite{flownet,flownet2}, or mix feedforward and iterative inference~\cite{sun2018pwc,teed2020raft}. 

While most early work focuses on estimating optical flow (the motion field that links two consecutive frames), there has recently been a push to estimate fine-grained correspondences across multiple frames. PIPs~\cite{harley2022particle} estimates 8-frame trajectories for pixels, using a learned iterative inference procedure that considers match costs and an implicit temporal prior, considering all 8 timesteps jointly with a powerful MLP-Mixer~\cite{tolstikhin2021mlp}. These 8-frame trajectories can be chained across time to produce longer-range tracks, but these longer tracks are more susceptible to drift, and slow to compute. TAP-Net~\cite{doersch2022tap} estimates correspondences for pixels by taking the argmax of frame-by-frame cost maps, which are computed efficiently using time-shifted convolutions~\cite{lin2019tsm}. 
Empirically, TAP-Net outperforms PIPs when there are long occlusions or hard cuts in the video, likely because the 8-frame temporal window in PIPs is incapable of resolving occlusions that exceed this window, and because hard cuts are inconsistent with the learned prior~\cite{doersch2022tap}. 

\input{figs/fig_dataset}
In this work, we extend PIPs by eliminating its hard 8-frame constraint, allowing it to take much wider temporal context into account. We achieve this by replacing the MLP-Mixer component (in which some parameters were tied to the size of the temporal window), with a deep 1D convolutional network (in which fixed-length kernels are applied to arbitrary temporal spans). We show that our model, trained from scratch in PointOdyssey, 
outperforms both PIPs and TAP-Net. Additionally, we retain a key advantage of PIPs over TAP-Net, which is the ability to produce reasonable estimates \textit{during occlusions}, by tracking multiple timesteps jointly instead of frame-by-frame.
