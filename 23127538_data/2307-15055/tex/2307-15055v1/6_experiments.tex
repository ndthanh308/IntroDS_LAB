\section{Experiments}
\vspace{-0.25em}

In this section we explain our experimental setup and results. We recommend watching the supplementary video for better visualization of our dataset and results. 

\subsection{Experimental setup}
\vspace{-0.25em}

\noindent\textbf{Baselines.} We benchmark point trackers, PIPs~\cite{harley2022particle}, TAP-Net~\cite{doersch2022tap}, our proposed PIPs+ and PIPS++, an optical flow method 
RAFT~\cite{teed2020raft} (estimating the flow between consecutive pairs of frames and chaining the flows to form trajectories), and a strong feature-matching method, DINO~\cite{caron2021emerging}. 

\input{tables/acc_table2}
\vspace{0.5em}\noindent\textbf{Implementation details.} We use the official code of PIPs~\cite{harley2022particle}, RAFT~\cite{teed2020raft}, and DINO~\cite{teed2020raft}, and reimplement TAP-Net~\cite{doersch2022tap} in PyTorch. For RAFT and DINO, we use the pretrained weights for evaluation. We train and test PIPs and TAP-Net on our dataset, using 4-8 A5000 GPUs in parallel. In addition to evaluating on the PointOdyssey test set, we evaluate on TAP-Vid-DAVIS~\cite{doersch2022tap} and CroHD~\cite{sundararaman2021tracking}, which are real-video evaluation benchmarks. TAP-Vid-Davis mostly consists of videos of animals and humans, with sparse tracks annotated on foreground and background points; CroHD consists of surveillance-like recordings of crowds (\eg, in train stations), with tracks annotated on all human heads. We leave out TAP-Vid-Kinetics~\cite{doersch2022tap}, as it contains hard cuts, while our focus is on continuous video. 

\subsection{Evaluation}
\vspace{-0.25em}

\noindent\textbf{Evaluation metrics.} We report the average position accuracy $\delta_\textrm{avg}$ as proposed in TAP-Vid~\cite{doersch2022tap}. This measures the percentage of tracks within a threshold distance to ground truth, averaged over thresholds $\{1,2,4,8,16\}$, defined in a normalized resolution of $256 \times 256$.  
We use Median Trajectory Error (MTE) to measure the distance between the estimated tracks and ground truth tracks. While Harley et al.~\cite{harley2022particle} reported average trajectory error (ATE) using a \textit{mean}, we find the median more informative, as it is less sensitive to outliers. 
We also measure a ``Survival" rate, which we define as: the average number of frames until tracking failure, and report this as a ratio of video length. Failure is when L2 distance exceeds 50 pixels in the normalized $256\times256$ resolution. 

\vspace{0.5em}\noindent\textbf{Quantitative results.} We compile our results in~\cref{tab:exp}. 
First, inspecting results across rows (\ie, comparing methods), we can see that PIPs+ and PIPs++ achieve the best results among all methods, demonstrating the effectiveness of the wide temporal awareness. The narrow gap between PIPs+ and PIPs++ suggests that the multi-template strategy has only a modest effect, but is helpful on average. 
The results also demonstrate that prior methods perform better on real-world datasets when they are re-trained (from scratch) in our dataset. An exception here is TAP-Net~\cite{doersch2022tap}, where model trained by the authors (on Kubric) performs best; this is likely due to our smaller compute budget. All of our models are trained on 4-8 GPUs (c.f. 64 TPU-v3 cores in the original TAP-Net). 
Inspecting the results across columns (\ie, comparing datasets), we observe that PointOdyssey appears to be a more challenging benchmark than TAP-Vid-DAVIS~\cite{doersch2022tap} and CroHD~\cite{sundararaman2021tracking}.
We can also observe that the ranking of methods appears consistent among PointOdyssey and the two real-world datasets, suggesting a correlation between progress on PointOdyssey and progress on videos in the wild. \cref{fig:survival} plots survival rate over time in PointOdyssey, revealing that all methods struggle to keep tracks ``alive'' over long durations, but the PIPs models degrade more slowly than the rest. 

\vspace{0.5em}\noindent\textbf{Qualitative results.} We show qualitative results in~\cref{fig:result}. Our method generates more-stable point trajectories compared to PIPs~\cite{harley2022particle} and other baselines. Please see the supplementary for video visualizations. We find that all methods have difficulty with targets which are close to boundaries (\eg, targets on thin objects are the hardest).

\input{figs/figs_results}
