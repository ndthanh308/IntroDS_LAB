\clearpage
%\noindent{\textbf{\Large{Supplementary Material}}}
\appendix
\section*{Supplementary Material}


\section{Implementation Details}

\noindent\textbf{Training Loss.}
We train PIPs+ and PIPs++ using the weighted $L_1$ distance between the estimated trajectory and the ground truth trajectory across iterative updates as proposed by Harley et al.~\cite{harley2022particle}. For any query point $p_n$, we compute the loss as follows:
\begin{equation}
    \mathcal{L}_n = \sum_{k=1}^K \left( \gamma^{K-k}\frac{1}{T}\sum_{t=1}^T||p^k_{t,n}-p^*_{t,n}||_1 \right)\,,
\end{equation}
where $p^k_{t,n}$ denotes the estimated position at timestep $t$, from iteration $k$, and $p^*_{t,n}$ is the ground truth. We set $\gamma=0.8$ in our experiment. The full training loss is obtained by averaging the per-point loss across all the N query points:
\begin{equation}
    \mathcal{L} = \frac{1}{N}\sum_{n=1}^N\mathcal{L}_n\,.
    \label{eq: loss}
\end{equation}
The loss is applied even when the target is occluded, which asks the model to estimate the track during visibility gaps. Note that PIPs~\cite{harley2022particle} is also trained with a visibility classification loss, to predict whether a point is occluded, along with a score loss that supervises the similarity score map to peak at the correct locations to help the network converge faster. We find that those losses can be omitted without harming tracking performance, and therefore we only use the $\mathcal{L}$ loss presented in~\cref{eq: loss}.

\vspace{0.5em}\noindent\textbf{Test Time Trajectory Chaining.}
In order to track with PIPs for more than 8 frames, Harley et al.~\cite{harley2022particle} link multiple 8-frame predictions. The linking strategy works per-point: after tracking for 8 frames and estimating visibility on each frame, the tracker is re-initialized on the last timestep whose visibility exceeds a threshold. While PIPs runs quickly within 8-frame clips, this chaining strategy leads to an overall FPS of 3.6 (at $720\times1080$ on an Nvidia V100 GPU). 
Although PIPs++ does not in principle require a linking strategy, our GPU memory constraints necessitate one. 
We use a very simple strategy: we predict 36 timesteps at a time, in sliding-window fashion, with \textit{no overlap} between the windows. Since there is no visibility check, this is very fast, leading to an overall FPS of 55.2. The model can be run with larger or smaller windows, but in a small grid search we found that 36 works best, possibly because this was also the sequence length used at training time. 

\section{Additional Visualizations of PointOdyssey}

Sample animated characters from the dataset are shown in ~\cref{fig:supp_chara}. 
By retargeting motion data to these characters, we are able to generate a wide range of interacting sequences, as illustrated in \cref{fig:supp_motion}. 

Samples of our re-built motion capture environments are shown in \cref{fig:supp_scene}. 

~\cref{fig:supp_seq} shows sample images pixel trajectories from the dataset. 

We recommend watching the supplementary video for additional visualizations. 

\input{figs/figs_supp_character}

\section{Additional Results}

We show the performance of PIPs~\cite{harley2022particle} and our PIPs++ method on real-world data in~\cref{fig:supp_results}. While PIPs can track visible points effectively, it struggles with occlusions. Trajectories from PIPs++ are on average less sensitive to occlusions. 

\input{figs/figs_motions}
\input{figs/figs_supp_scene}
\input{figs/figs_supp_seq}
\input{figs/figs_supp_results}
