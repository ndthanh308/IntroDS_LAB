\vspace{-0.5em}
\section{Introduction}
\vspace{-0.25em}
In a variety of computer vision tasks, large-scale annotated datasets have provided a highway for the development of accurate models. In this paper, we aim to provide such a highway for the task of \textit{fine-grained long-range tracking}.  
The goal of fine-grained long-range tracking is: given any pixel coordinate in any frame of a video, track the corresponding world surface point for as long as possible. 


While there exist multiple generations of datasets targeting fine-grained short-range tracking (\ie, optical flow) \cite{middlebury,sintel,mayer2016large}, and annually updated datasets targeting several forms of coarse-grained long-range tracking (\ie, single-object tracking~\cite{fan2019lasot}, multi-object tracking~\cite{MOTChallenge2015}, video object segmentation~\cite{pont20172017}), there are only a handful of works at the intersection of fine-grained \textit{and} long-range tracking. 

\input{tables/comparison_table.tex}
Harley~\etal~\cite{harley2022particle} and Doersch~\etal~\cite{doersch2022tap}, train fine-grained trackers on unrealistic synthetic data (FlyingThings++~\cite{mayer2016large,harley2022particle} and Kubric-MOVi-E~\cite{greff2022kubric}), consisting of random objects moving in random directions on random backgrounds, and test on real-world videos with sparse human-provided annotations (BADJA~\cite{badja} and TAP-Vid~\cite{doersch2022tap}). 
While it is interesting that generalization to real video emerges from these models, the use of such simplistic training data precludes the learning of long-range temporal context, and scene-level semantic awareness. We argue that long-range point tracking should not be treated as an extension of optical flow, where naturalism might indeed be discarded without ill effect~\cite{sun2021autoflow}. Pixels in real video may move somewhat unpredictably, but they take a journey which reflects a variety of modellable factors, including camera shake, object-level motions and deformations, and multi-object relationships such as physical and social interactions. Realizing the grand scope of this problem, both in our data and in our methods, is critical for progress. 

We propose \textit{PointOdyssey}, a large-scale synthetic dataset for the training and evaluation of long-term fine-grained tracking. Our dataset aims to provide the complexity, diversity, and naturalism of real-world video, with pixel-perfect annotation only possible in simulation. 
Besides the length of our videos, the key aspects differentiating our work from prior synthetic datasets are (1) we use motions, scene layouts, and camera trajectories mined from real-world videos and motion captures (as opposed to being random or hand-designed), and (2) we use domain randomization on a wider range of scene attributes, including environment maps, lighting, human and animal bodies, camera trajectories, and materials (similar to Shen~\etal~\cite{shen2021igibson}). Thanks to progress in the availability of high-quality assets and rendering tools, 
we are also able to deliver better photo-realism than possible in years past. 

The motion profiles in our data come from large-scale motion-capture datasets of humans and animals~\cite{mahmood2019amass,li20214dcomplete}. We use these captures to drive humanoids and animals in outdoor scenes, producing realistic long-range trajectories. In outdoor scenes, we pair these actors with 3D assets randomly scattered on the ground plane, which react to the actors according to physics (\eg, being kicked away as the feet collide with the objects). To produce realistic indoor scenes, we use motion captures of indoor scenes~\cite{zheng2022gimo, zhang2022egobody}, and \textit{manually replicate} the capture environments in our simulator, allowing us to re-render the exact motions and interactions, and preserve their scene-aware nature. 
Finally, we import camera trajectories computed from real video~\cite{li2019learning}, and attach additional cameras to the synthetic humans' heads, giving challenging multi-view data of the scenes. 
Our capture-driven approach is in contrast to the mostly random motion patterns used in Kubric~\cite{greff2022kubric} and FlyingThings~\cite{mayer2016large}.
We hope that our data will encourage the development of tracking methods which use scene-level cues to provide strong priors on tracking, pushing past the tradition of relying entirely on bottom-up cues such as feature-matching. 

Our data's visual diversity stems from a large set of simulated assets: $42$ humanoid shapes with artist-made textures, $7$ animals, $1$K+ object/background textures, $1$K+ objects, $20$ unique 3D scenes, and $50$ environment maps. 
We randomize the scene lighting to achieve a wide range of dark and bright scenes. We also render dynamic fog and smoke effects into our scenes, introducing a form of partial occlusion entirely missing from FlyingThings and Kubric. 

PointOdyssey unlocks a variety of new challenges, one of them being: how to use long-range temporal context. Since prior datasets have only included short videos for training ($<30$ frames, see Table~\ref{tab:stat}), existing models only exploit similarly short temporal context. For example, the current state-of-the-art method Persistent Independent Particles (PIPs)~\cite{harley2022particle}, uses an 8-frame temporal window when tracking. As a step toward leveraging \textit{arbitrarily long} temporal context, we propose some modifications to PIPs~\cite{harley2022particle}, greatly widening its 8-frame temporal window, and incorporating a template-update mechanism. Experimental results show that our method achieves higher tracking accuracy than all existing methods, both on the PointOdyssey test set and on real-world benchmarks.

In summary, the main contribution of this paper is \textit{PointOdyssey}, a large-scale synthetic dataset for long-term point tracking, which aims to reflect the challenges---and opportunities---of real-world fine-grained tracking. The dataset, and the code for the simulation engine, are available at: \url{https://pointodyssey.com}