\section{Limitations}
\vspace{-0.25em}

PointOdyssey currently lacks large outdoor scenes where the camera travels a large distance, which is, for example, a frequent scenario in driving data~\cite{Dosovitskiy17}. It would be interesting to explore long-range agent-scene and agent-agent interactions in that context. We also note that our human and animal motion profiles are limited by our base datasets, and this constraint could be lifted with the help of recent generative models~\cite{rempe2021humor, yi2022mime}.
\input{figs/figs_sur}
While the focus in this paper is on point tracking, our dataset connects to a wide range of applications which we have not yet explored, such as 3D scene flow estimation, novel view synthesis in dynamic scenes (which would be especially challenging with PointOdyssey's atmospheric effects), human and animal pose estimation, and ego-centric vision. 
Finally, while PIPs++ takes a step toward modelling longer-range temporal priors, it is still a fairly low-level tracker, relying entirely on appearance-matching cues and a temporal prior. This leaves open the challenge of leveraging scene-level and semantic cues for tracking, where we expect PointOdyssey's training data will be especially valuable.

\section{Conclusion}
\vspace{-0.25em}
PointOdyssey is a large-scale synthetic dataset, and data generator, for long-term point tracking. The data is diverse and naturalistic, making it an ideal resource for training general-purpose fine-grained trackers. We demonstrate its usefulness through a new tracker called PIPs++, which leverages long-term temporal context and outperforms state-of-the-art. PointOdyssey also opens opportunities for developing trackers which utilize scene-level and semantic cues, though we have not explored this yet. We hope our work will also be useful beyond point tracking, enabling work in 3D and 4D scene analysis, and higher-level video understanding. 

\vspace{0.5em}\noindent{\textbf{Acknowledgments.}}
The authors thank Andrew Zisserman for feedback on an early version of the title. This work was supported by the Toyota Research Institute under the University 2.0 program, ARL grant W911NF-21-2-0104, and a Vannevar Bush Faculty Fellowship. 