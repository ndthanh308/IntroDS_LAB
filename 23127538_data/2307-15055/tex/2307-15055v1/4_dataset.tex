\section{PointOdyssey Dataset}
\vspace{-0.25em}
A sample from our dataset is shown in~\cref{fig:teaser}, and an overview of our data generation pipeline is shown in \cref{fig:dataset}. To generate complex but realistic long-range motion, we use humanoids, robots, and animals, driven by motion capture data~\cite{mahmood2019amass, li20214dcomplete, zheng2022gimo, zhang2022egobody}. This allows us to render long-term dynamic sequences that incorporate long-range interactions between the deformable characters and the 3D environments. We maximize the diversity of the dataset by randomizing the scenes with various materials, textures, and lighting. To add further visual complexity, we introduce random noise to the scene volume density to create changing fog and smoke, which act as a natural occluder and have a significant impact on the appearance and visibility of the scene. This section summarizes our data collection process.

\subsection{Long-Term Motion Data}
\vspace{-0.25em}

\noindent\textbf{Deformable Characters.} We collect 42 open-sourced artist-designed humans and robots from BlenderKit~\cite{blenderkit}, Mixamo~\cite{Mixamo}, and TurboSquid~\cite{turbosquid}, along with 7 animals from DeformingThings4D~\cite{li20214dcomplete}. These assets provide high-poly meshes, along with photorealistic materials and textures, and are rigged to enable animation. 

\vspace{0.5em}\noindent\textbf{Motion Retargeting.} To animate the humanoid characters, we use real-world human motion data~\cite{mahmood2019amass, zheng2022gimo, zhang2022egobody}. We retarget the source motions represented as SMPL-X~\cite{SMPL-X:2019} sequences to target characters, using the motion retargeting algorithm from the Rokoko Toolkit~\cite{Rokoko}. Defining $S_{Rig}$ as the rig of the SMPL-X human model and $T_{Rig}$ as the rig of the target character with $z$-axis up in the resting body pose, we equalize the scale between two rigs as:
\begin{equation}
s=\frac{Z_{max}(T_{Rig})-Z_{min}(T_{Rig})}{Z_{max}(S_{Rig})-Z_{min}(S_{Rig})}\,,
\end{equation}
where $s$ is then applied to the source rig as $S'_{Rig}=S_{Rig}/s$.
Defining $B_i(p_i)$ as a bone in a rig parameterized by $p_i$, (\eg, head and tail location and rotation) we align the source rig with the target rig, setting $p_i^{S^*}=p_j^{T}$, where $p_i^{S^*}$ is the parameter of the bone $B_i$ in the source rig, and $p_j^{T}$ denotes the parameter of the corresponding bone in the target rig, using the bone mapping between the two rigs. Using the aligned source rig $S^*_{Rig}$, we copy the animation to the target rig. 

\input{figs/figs_rebuilt}
For animals, since the motions in DeformingThings4D~\cite{li20214dcomplete} are already bound to the meshes, we do not need a retargeting process.

%\vspace{-0.5em}
\subsection{3D Environment Context}
\vspace{-0.25em}
Our dataset contains outdoor scenes, which involve randomized but physically coherent agent-object and object-object interactions, and indoor scenes, which involve realistic agent-scene and agent-agent interactions.

\vspace{0.5em}\noindent\textbf{Outdoor Scenes.} Similar to Kubric~\cite{greff2022kubric}, we populate outdoor scenes with random rigid objects from GSO~\cite{downs2022google} and PartNet~\cite{mo2019partnet}. We animate our deformable characters to move around in these scenes, and treat these characters as passive objects with infinite mass, causing the scattered rigid objects to react as though being kicked. We also apply random forces to the rigid objects at random timesteps, to create difficult near-random motion trajectories, with realistic physical collisions. We use HDR environment textures collected from PolyHaven\cite{Polyhaven} mapped into a dome-like region~\cite{LilySurfaceScraper} to simulate natural backgrounds.

\vspace{0.5em}\noindent\textbf{Indoor Scenes.}
We manually build twenty 3D indoor scenes to replicate specific 3D environments from our motion capture datasets~\cite{zheng2022gimo, zhang2022egobody}, matching the scene layouts and furniture as closely as possible, sourcing furniture assets from Blenderkit~\cite{blenderkit} and 3D-FRONT~\cite{fu20213d, fu20213dfuture}. 
We then use motion capture data from these same scenes to animate our characters in the environments, yielding collision-free and naturalistic motion, as shown in~\cref{fig:rebuilt}. We note that unlike the outdoor scenes and unlike prior work, these motions reflect true affordances of the 3D environments. 

\input{figs/figs_random}

\subsection{Camera Motion}
\vspace{-0.25em}
For outdoor scenes, we drive the camera using trajectories extracted from YouTube videos via structure-from-motion~\cite{li2019learning}. 
For indoor scenes, we manually create cinematic camera trajectories consisting of orbits, swoops, and zooms, as well as render ego-centric videos by attaching cameras to the heads of the virtual subjects. Similar to real-world egocentric video~\cite{damen2020epic}, our synthetic ego-centric views yield particularly challenging motion trajectories. 

\subsection{Scene Randomization}
\vspace{-0.25em}
We add diveristy by randomizing our synthetic scenes, in steps similar to iGibson~\cite{shen2021igibson}. For indoor scenes, we randomize the texture of floors, walls, and ceilings, by sampling from $80$ high-quality materials from BlenderKit~\cite{blenderkit}, and randomize the lighting. For outdoor scenes, we randomize the textures of objects by sampling from $1000$ texture maps from GSO~\cite{downs2022google}; we randomize the appearance of the animals by sampling from $24$ high-fidelity fur materials from Blenderkit; we randomize the background by sampling from $50$ $4$K-resolution HDR images from PolyHaven~\cite{Polyhaven}. We additionally generate fog and smoke by adding procedural atmospheric effects to the scene volume. As shown in~\cref{fig:random}, these scene randomization steps add diversity and difficulty to the data.

\subsection{Annotation Generation}
\vspace{-0.25em}
We generate point trajectories by exporting tracked 2D and 3D coordinates of random foreground and background vertices. We additionally compute visibility annotations, by comparing the depth of the tracked points to the rendered depth values at the projected coordinates. As shown in~\cref{fig:anno}, we also export depth, normals, instance segmentation, camera extrinsics, and camera intrinsics. While our focus is on point tracking, we hope these extra annotations will support a wide set of applications.

\input{figs/figs_anno}

\subsection{Statistics}
\vspace{-0.25em}
Our dataset consists of $43$ outdoor scenes and $61$ indoor scenes, totaling $216$K $540\times960$ images at $30$ FPS. The data was rendered in ~$2600$ GPU hours using the Cycles engine in Blender. We divide the dataset into $166$K frames for training, $24$K frames for validation, and $26$K frames for testing. \cref{tab:stat} summarizes key statistics of our dataset compared to related works. 

