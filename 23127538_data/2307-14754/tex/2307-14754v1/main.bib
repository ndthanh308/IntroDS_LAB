@misc{CCPA,
title = {California Consumer Privacy Act (CCPA)},
author = "CA DoJ",
url = {https://oag.ca.gov/privacy/ccpa},
year = {2018}
}

@inproceedings{beutel2019fairness,
  title={Fairness in recommendation ranking through pairwise comparisons},
  author={Beutel, Alex and Chen, Jilin and Doshi, Tulsee and Qian, Hai and Wei, Li and Wu, Yi and Heldt, Lukasz and Zhao, Zhe and Hong, Lichan and Chi, Ed H and others},
  booktitle={Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery \& data mining},
  pages={2212--2220},
  year={2019}
}

@article{wang2023survey,
  title={A survey on the fairness of recommender systems},
  author={Wang, Yifan and Ma, Weizhi and Zhang, Min and Liu, Yiqun and Ma, Shaoping},
  journal={ACM Transactions on Information Systems},
  volume={41},
  number={3},
  pages={1--43},
  year={2023},
  publisher={ACM New York, NY}
}

@article{costonCharacterizingFairnessSet,
	title = {Characterizing {Fairness} {Over} the {Set} of {Good} {Models} {Under} {Selective} {Labels}},
	abstract = {Algorithmic risk assessments are used to inform decisions in a wide variety of high-stakes settings. Often multiple predictive models deliver similar overall performance but differ markedly in their predictions for individual cases, an empirical phenomenon known as the “Rashomon Effect.” These models may have different properties over various groups, and therefore have different predictive fairness properties. We develop a framework for characterizing predictive fairness properties over the set of models that deliver similar overall performance, or “the set of good models.” Our framework addresses the empirically relevant challenge of selectively labelled data in the setting where the selection decision and outcome are unconfounded given the observed data features. Our framework can be used to 1) audit for predictive bias; or 2) replace an existing model with one that has better fairness properties. We illustrate these use cases on a recidivism prediction task and a real-world credit-scoring task.},
	language = {en},
	author = {Coston, Amanda and Rambachan, Ashesh and Chouldechova, Alexandra},
	pages = {12},
	file = {coston_characterizing fairness over the set of good models under selective labels.pdf:/Volumes/GoogleDrive/My Drive/Zotero DB/FlavioHima/coston_characterizing fairness over the set of good models under selective labels.pdf:application/pdf;costonCharacterizingFairnessSet-zotero.md:/Users/alexo/Zotero/mdnotesfiles/costonCharacterizingFairnessSet-zotero.md:text/plain},
}

@inproceedings{semenovaExistenceSimplerMachine2022,
	title = {On the {Existence} of {Simpler} {Machine} {Learning} {Models} {V4}},
	url = {http://arxiv.org/abs/1908.01755},
	doi = {10.1145/3531146.3533232},
	abstract = {It is almost always easier to find an accurate-but-complex model than an accurate-yet-simple model. Finding optimal, sparse, accurate models of various forms (linear models with integer coefficients, decision sets, rule lists, decision trees) is generally NP-hard. We often do not know whether the search for a simpler model will be worthwhile, and thus we do not go to the trouble of searching for one. In this work, we ask an important practical question: can accurate-yet-simple models be proven to exist, or shown likely to exist, before explicitly searching for them? We hypothesize that there is an important reason that simple-yet-accurate models often do exist. This hypothesis is that the size of the Rashomon set is often large, where the Rashomon set is the set of almost-equally-accurate models from a function class. If the Rashomon set is large, it contains numerous accurate models, and perhaps at least one of them is the simple model we desire. In this work, we formally present the Rashomon ratio as a new gauge of simplicity for a learning problem, depending on a function class and a data set. The Rashomon ratio is the ratio of the volume of the set of accurate models to the volume of the hypothesis space, and it is different from standard complexity measures from statistical learning theory. Insight from studying the Rashomon ratio provides an easy way to check whether a simpler model might exist for a problem before finding it, namely whether several different machine learning methods achieve similar performance on the data. In that sense, the Rashomon ratio is a powerful tool for understanding why and when an accurate-yet-simple model might exist. If, as we hypothesize in this work, many real-world data sets admit large Rashomon sets, the implications are vast: it means that simple or interpretable models may often be used for high-stakes decisions without losing accuracy.},
	language = {en},
	urldate = {2022-09-09},
	booktitle = {2022 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	author = {Semenova, Lesia and Rudin, Cynthia and Parr, Ronald},
	month = jun,
	year = {2022},
	note = {arXiv:1908.01755 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {1827--1858},
	file = {semenova_2022_on the existence of simpler machine learning models v4.pdf:/Users/alexo/Library/CloudStorage/GoogleDrive-alex.oesterling@gmail.com/My Drive/Zotero DB/FlavioHima/semenova_2022_on the existence of simpler machine learning models v4.pdf:application/pdf},
}

@misc{kleinbergSimplicityCreatesInequity2019,
	title = {Simplicity {Creates} {Inequity}: {Implications} for {Fairness}, {Stereotypes}, and {Interpretability}},
	shorttitle = {Simplicity {Creates} {Inequity}},
	url = {http://arxiv.org/abs/1809.04578},
	abstract = {Algorithms are increasingly used to aid, or in some cases supplant, human decision-making, particularly for decisions that hinge on predictions. As a result, two additional features in addition to prediction quality have generated interest: (i) to facilitate human interaction and understanding with these algorithms, we desire prediction functions that are in some fashion simple or interpretable; and (ii) because they inﬂuence consequential decisions, we also want them to produce equitable allocations. We develop a formal model to explore the relationship between the demands of simplicity and equity. Although the two concepts appear to be motivated by qualitatively distinct goals, we show a fundamental inconsistency between them. Speciﬁcally, we formalize a general framework for producing simple prediction functions, and in this framework we establish two basic results. First, every simple prediction function is strictly improvable: there exists a more complex prediction function that is both strictly more eﬃcient and also strictly more equitable. Put another way, using a simple prediction function both reduces utility for disadvantaged groups and reduces overall welfare relative to other options. Second, we show that simple prediction functions necessarily create incentives to use information about individuals’ membership in a disadvantaged group — incentives that weren’t present before simpliﬁcation, and that work against these individuals. Thus, simplicity transforms disadvantage into bias against the disadvantaged group. Our results are not only about algorithms but about any process that produces simple models, and as such they connect to the psychology of stereotypes and to an earlier economics literature on statistical discrimination.},
	language = {en},
	urldate = {2022-09-09},
	publisher = {arXiv},
	author = {Kleinberg, Jon and Mullainathan, Sendhil},
	month = jun,
	year = {2019},
	note = {arXiv:1809.04578 [cs, stat]},
	keywords = {Computer Science - Computers and Society, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Social and Information Networks, Computer Science - Data Structures and Algorithms},
	file = {kleinberg_2019_simplicity creates inequity - implications for fairness, stereotypes, and.pdf:/Volumes/GoogleDrive/My Drive/Zotero DB/FlavioHima/kleinberg_2019_simplicity creates inequity - implications for fairness, stereotypes, and.pdf:application/pdf},
}

@article{rolfRepresentationMattersAssessing,
	title = {Representation {Matters}: {Assessing} the {Importance} of {Subgroup} {Allocations} in {Training} {Data}},
	abstract = {Collecting more diverse and representative training data is often touted as a remedy for the disparate performance of machine learning predictors across subpopulations. However, a precise framework for understanding how dataset properties like diversity affect learning outcomes is largely lacking. By casting data collection as part of the learning process, we demonstrate that diverse representation in training data is key not only to increasing subgroup performances, but also to achieving population-level objectives. Our analysis and experiments describe how dataset compositions inﬂuence performance and provide constructive results for using trends in existing data, alongside domain knowledge, to help guide intentional, objective-aware dataset design.},
	language = {en},
	author = {Rolf, Esther and Worledge, Theodora and Recht, Benjamin and Jordan, Michael I},
	pages = {12},
	file = {rolf_representation matters - assessing the importance of subgroup allocations in.pdf:/Users/alexo/Library/CloudStorage/GoogleDrive-alex.oesterling@gmail.com/My Drive/Zotero DB/FlavioHima/rolf_representation matters - assessing the importance of subgroup allocations in2.pdf:application/pdf;rolfRepresentationMattersAssessing-zotero.md:/Users/alexo/Zotero/mdnotesfiles/rolfRepresentationMattersAssessing-zotero.md:text/plain},
}

@misc{bourtouleMachineUnlearning2020,
	title = {Machine {Unlearning}},
	url = {http://arxiv.org/abs/1912.03817},
	abstract = {Once users have shared their data online, it is generally difﬁcult for them to revoke access and ask for the data to be deleted. Machine learning (ML) exacerbates this problem because any model trained with said data may have memorized it, putting users at risk of a successful privacy attack exposing their information. Yet, having models unlearn is notoriously difﬁcult. We introduce SISA training, a framework that expedites the unlearning process by strategically limiting the inﬂuence of a data point in the training procedure. While our framework is applicable to any learning algorithm, it is designed to achieve the largest improvements for stateful algorithms like stochastic gradient descent for deep neural networks. SISA training reduces the computational overhead associated with unlearning, even in the worst-case setting where unlearning requests are made uniformly across the training set. In some cases, the service provider may have a prior on the distribution of unlearning requests that will be issued by users. We may take this prior into account to partition and order data accordingly, and further decrease overhead from unlearning.},
	language = {en},
	urldate = {2022-09-09},
	publisher = {arXiv},
	author = {Bourtoule, Lucas and Chandrasekaran, Varun and Choquette-Choo, Christopher A. and Jia, Hengrui and Travers, Adelin and Zhang, Baiwu and Lie, David and Papernot, Nicolas},
	month = dec,
	year = {2020},
	note = {arXiv:1912.03817 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
	file = {bourtoule_2020_machine unlearning.pdf:/Users/alexo/Library/CloudStorage/GoogleDrive-alex.oesterling@gmail.com/My Drive/Zotero DB/FlavioHima/bourtoule_2020_machine unlearning.pdf:application/pdf;bourtouleMachineUnlearning2020 - Annotations.md:/Users/alexo/Zotero/mdnotesfiles/bourtouleMachineUnlearning2020 - Annotations.md:text/plain;bourtouleMachineUnlearning2020 - Comment Published in IEEE S&P 2021.md:/Users/alexo/Zotero/mdnotesfiles/bourtouleMachineUnlearning2020 - Comment Published in IEEE S&P 2021.md:text/plain;bourtouleMachineUnlearning2020-zotero.md:/Users/alexo/Zotero/mdnotesfiles/bourtouleMachineUnlearning2020-zotero.md:text/plain;bourtouleMachineUnlearning2020.md:/Users/alexo/Zotero/mdnotesfiles/bourtouleMachineUnlearning2020.md:text/plain},
}

@misc{marxPredictiveMultiplicityClassification2020,
	title = {Predictive {Multiplicity} in {Classification}},
	url = {http://arxiv.org/abs/1909.06677},
	abstract = {In the context of machine learning, a prediction problem exhibits multiplicity if there exist several “good" models that attain identical or near-identical performance (i.e., accuracy, AUC, etc.). In this paper, we study the effects of multiplicity in human-facing applications, such as credit scoring and recidivism prediction. We introduce a specific notion of multiplicity – predictive multiplicity – to describe the existence of good models with conflicting predictions. Unlike existing notions of multiplicity (e.g., the Rashomon effect), predictive multiplicity reflects irreconcilable differences in the predictions of models with comparable performance, and presents new challenges for common practices such as model selection and local explanation. We propose measures to evaluate the predictive multiplicity in classification problems. We present integer programming methods to compute these measures for a given datasets by solving empirical risk minimization problems with discrete constraints. We demonstrate how these tools can inform stakeholders on a large collection of recidivism prediction problems. Our results show that real-world prediction problems often admit many good models that output wildly conflicting predictions, and support the need to report predictive multiplicity in model development.},
	language = {en},
	urldate = {2022-09-20},
	publisher = {arXiv},
	author = {Marx, Charles T. and Calmon, Flavio du Pin and Ustun, Berk},
	month = sep,
	year = {2020},
	note = {arXiv:1909.06677 [cs, stat]},
	keywords = {Computer Science - Computers and Society, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {marx_2020_predictive multiplicity in classification.pdf:/Volumes/GoogleDrive/My Drive/Zotero DB/FlavioHima/marx_2020_predictive multiplicity in classification.pdf:application/pdf;marxPredictiveMultiplicityClassification2020-zotero.md:/Users/alexo/Zotero/mdnotesfiles/marxPredictiveMultiplicityClassification2020-zotero.md:text/plain},
}

@misc{kohUnderstandingBlackboxPredictions2020,
	title = {Understanding {Black}-box {Predictions} via {Influence} {Functions}},
	url = {http://arxiv.org/abs/1703.04730},
	abstract = {How can we explain the predictions of a blackbox model? In this paper, we use inﬂuence functions — a classic technique from robust statistics — to trace a model’s prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction. To scale up inﬂuence functions to modern machine learning settings, we develop a simple, efﬁcient implementation that requires only oracle access to gradients and Hessian-vector products. We show that even on non-convex and non-differentiable models where the theory breaks down, approximations to inﬂuence functions can still provide valuable information. On linear models and convolutional neural networks, we demonstrate that inﬂuence functions are useful for multiple purposes: understanding model behavior, debugging models, detecting dataset errors, and even creating visuallyindistinguishable training-set attacks.},
	language = {en},
	urldate = {2022-09-20},
	publisher = {arXiv},
	author = {Koh, Pang Wei and Liang, Percy},
	month = dec,
	year = {2020},
	note = {arXiv:1703.04730 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {koh_2020_understanding black-box predictions via influence functions.pdf:/Users/alexo/Library/CloudStorage/GoogleDrive-alex.oesterling@gmail.com/My Drive/Zotero DB/FlavioHima/koh_2020_understanding black-box predictions via influence functions.pdf:application/pdf},
}

@misc{angelopoulosGentleIntroductionConformal2022,
	title = {A {Gentle} {Introduction} to {Conformal} {Prediction} and {Distribution}-{Free} {Uncertainty} {Quantification}},
	url = {http://arxiv.org/abs/2107.07511},
	abstract = {Black-box machine learning models are now routinely used in high-risk settings, like medical diagnostics, which demand uncertainty quantiﬁcation to avoid consequential model failures. Conformal prediction is a user-friendly paradigm for creating statistically rigorous uncertainty sets/intervals for the predictions of such models. Critically, the sets are valid in a distribution-free sense: they possess explicit, non-asymptotic guarantees even without distributional assumptions or model assumptions. One can use conformal prediction with any pre-trained model, such as a neural network, to produce sets that are guaranteed to contain the ground truth with a user-speciﬁed probability, such as 90\%. It is easy-tounderstand, easy-to-use, and general, applying naturally to problems arising in the ﬁelds of computer vision, natural language processing, deep reinforcement learning, and so on.},
	language = {en},
	urldate = {2022-09-20},
	publisher = {arXiv},
	author = {Angelopoulos, Anastasios N. and Bates, Stephen},
	month = sep,
	year = {2022},
	note = {arXiv:2107.07511 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Mathematics - Statistics Theory, Statistics - Methodology},
	file = {angelopoulos_2022_a gentle introduction to conformal prediction and distribution-free uncertainty.pdf:/Volumes/GoogleDrive/My Drive/Zotero DB/FlavioHima/angelopoulos_2022_a gentle introduction to conformal prediction and distribution-free uncertainty.pdf:application/pdf;angelopoulosGentleIntroductionConformal2022-zotero.md:/Users/alexo/Zotero/mdnotesfiles/angelopoulosGentleIntroductionConformal2022-zotero.md:text/plain},
}

@article{fisherAllModelsAre2021,
	title = {All {Models} are {Wrong}, but {Many} are {Useful}: {Learning} a {Variable}’s {Importance} by {Studying} an {Entire} {Class} of {Prediction} {Models} {Simultaneously}},
	abstract = {Variable importance (VI) tools describe how much covariates contribute to a prediction model’s accuracy. However, important variables for one well-performing model (for example, a linear model f (x) = xT β with a fixed coefficient vector β) may be unimportant for another model. In this paper, we propose model class reliance (MCR) as the range of VI values across all wellperforming model in a prespecified class. Thus, MCR gives a more comprehensive description of importance by accounting for the fact that many prediction models, possibly of different parametric forms, may fit the data well. In the process of deriving MCR, we show several informative results for permutation-based VI estimates, based on the VI measures used in Random Forests. Specifically, we derive connections between permutation importance estimates for a single prediction model, U-statistics, conditional variable importance, conditional causal effects, and linear model coefficients. We then give probabilistic bounds for MCR, using a novel, generalizable technique. We apply MCR to a public data set of Broward County criminal records to study the reliance of recidivism prediction models on sex and race. In this application, MCR can be used to help inform VI for unknown, proprietary models.},
	language = {en},
	author = {Fisher, Aaron and Rudin, Cynthia and Dominici, Francesca},
	year = {2021},
	pages = {88},
	file = {Fisher et al. - 2021 - All Models are Wrong, but Many are Useful Learnin.pdf:/Users/alexo/Zotero/storage/9JU29ZJ9/Fisher et al. - 2021 - All Models are Wrong, but Many are Useful Learnin.pdf:application/pdf},
}

@inproceedings{semenovaExistenceSimplerMachine2022b,
	title = {On the {Existence} of {Simpler} {Machine} {Learning} {Models} {V3}},
	url = {http://arxiv.org/abs/1908.01755},
	doi = {10.1145/3531146.3533232},
	abstract = {The Rashomon effect occurs when many different explanations exist for the same phenomenon. In machine learning, Leo Breiman used this term to characterize problems where many accurate-but-different models exist to describe the same data. In this work, we study how the Rashomon effect can be useful for understanding the relationship between training and test performance, and the possibility that simple-yet-accurate models exist for many problems. We consider the Rashomon set—the set of almost-equally-accurate models for a given problem—and study its properties and the types of models it could contain. We present the Rashomon ratio as a new measure related to simplicity of model classes, which is the ratio of the volume of the set of accurate models to the volume of the hypothesis space; the Rashomon ratio is different from standard complexity measures from statistical learning theory. For a hierarchy of hypothesis spaces, the Rashomon ratio can help modelers to navigate the trade-off between simplicity and accuracy. In particular, we ﬁnd empirically that a plot of empirical risk vs. Rashomon ratio forms a characteristic Γ-shaped Rashomon curve, whose elbow seems to be a reliable model selection criterion. When the Rashomon set is large, models that are accurate—but that also have various other useful properties—may often be obtained. These models might obey various constraints such as interpretability, fairness, or monotonicity.},
	language = {en},
	urldate = {2022-09-20},
	booktitle = {2022 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	author = {Semenova, Lesia and Rudin, Cynthia and Parr, Ronald},
	month = jun,
	year = {2022},
	note = {arXiv:1908.01755 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {1827--1858},
	file = {semenova_2022_on the existence of simpler machine learning models v3.pdf:/Users/alexo/Library/CloudStorage/GoogleDrive-alex.oesterling@gmail.com/My Drive/Zotero DB/FlavioHima/semenova_2022_on the existence of simpler machine learning models v3.pdf:application/pdf;semenovaExistenceSimplerMachine2022b-zotero.md:/Users/alexo/Zotero/mdnotesfiles/semenovaExistenceSimplerMachine2022b-zotero.md:text/plain},
}

@article{sekhariRememberWhatYou,
	title = {Remember {What} {You} {Want} to {Forget}: {Algorithms} for {Machine} {Unlearning}},
	abstract = {We study the problem of unlearning datapoints from a learnt model. The learner ﬁrst receives a dataset S drawn i.i.d. from an unknown distribution, and outputs a model wb that performs well on unseen samples from the same distribution. However, at some point in the future, any training datapoint z 2 S can request to be unlearned, thus prompting the learner to modify its output model while still ensuring the same accuracy guarantees. We initiate a rigorous study of generalization in machine unlearning, where the goal is to perform well on previously unseen datapoints. Our focus is on both computational and storage complexity.},
	language = {en},
	author = {Sekhari, Ayush and Acharya, Jayadev and Suresh, Ananda Theertha and Kamath, Gautam},
	pages = {12},
	file = {sekhari_remember what you want to forget - algorithms for machine unlearning.pdf:/Users/alexo/Library/CloudStorage/GoogleDrive-alex.oesterling@gmail.com/My Drive/Zotero DB/FlavioHima/sekhari_remember what you want to forget - algorithms for machine unlearning.pdf:application/pdf},
}

@inproceedings{chenWhenMachineUnlearning2021,
	title = {When {Machine} {Unlearning} {Jeopardizes} {Privacy}},
	url = {http://arxiv.org/abs/2005.02205},
	doi = {10.1145/3460120.3484756},
	abstract = {The right to be forgotten states that a data owner has the right to erase their data from an entity storing it. In the context of machine learning (ML), the right to be forgotten requires an ML model owner to remove the data owner's data from the training set used to build the ML model, a process known as machine unlearning. While originally designed to protect the privacy of the data owner, we argue that machine unlearning may leave some imprint of the data in the ML model and thus create unintended privacy risks. In this paper, we perform the first study on investigating the unintended information leakage caused by machine unlearning. We propose a novel membership inference attack that leverages the different outputs of an ML model's two versions to infer whether a target sample is part of the training set of the original model but out of the training set of the corresponding unlearned model. Our experiments demonstrate that the proposed membership inference attack achieves strong performance. More importantly, we show that our attack in multiple cases outperforms the classical membership inference attack on the original ML model, which indicates that machine unlearning can have counterproductive effects on privacy. We notice that the privacy degradation is especially significant for well-generalized ML models where classical membership inference does not perform well. We further investigate four mechanisms to mitigate the newly discovered privacy risks and show that releasing the predicted label only, temperature scaling, and differential privacy are effective. We believe that our results can help improve privacy protection in practical implementations of machine unlearning. Our code is available at https://github.com/MinChen00/UnlearningLeaks.},
	language = {en},
	urldate = {2022-09-24},
	booktitle = {Proceedings of the 2021 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	author = {Chen, Min and Zhang, Zhikun and Wang, Tianhao and Backes, Michael and Humbert, Mathias and Zhang, Yang},
	month = nov,
	year = {2021},
	note = {arXiv:2005.02205 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Cryptography and Security},
	pages = {896--911},
	file = {Chen et al. - 2021 - When Machine Unlearning Jeopardizes Privacy.pdf:/Users/alexo/Zotero/storage/EYPH7JQM/Chen et al. - 2021 - When Machine Unlearning Jeopardizes Privacy.pdf:application/pdf},
}

@misc{guptaAdaptiveMachineUnlearning2021,
	title = {Adaptive {Machine} {Unlearning}},
	url = {http://arxiv.org/abs/2106.04378},
	abstract = {Data deletion algorithms aim to remove the inﬂuence of deleted data points from trained models at a cheaper computational cost than fully retraining those models. However, for sequences of deletions, most prior work in the non-convex setting gives valid guarantees only for sequences that are chosen independently of the models that are published. If people choose to delete their data as a function of the published models (because they don’t like what the models reveal about them, for example), then the update sequence is adaptive. In this paper, we give a general reduction from deletion guarantees against adaptive sequences to deletion guarantees against non-adaptive sequences, using diﬀerential privacy and its connection to max information. Combined with ideas from prior work which give guarantees for non-adaptive deletion sequences, this leads to extremely ﬂexible algorithms able to handle arbitrary model classes and training methodologies, giving strong provable deletion guarantees for adaptive deletion sequences. We show in theory how prior work for non-convex models fails against adaptive deletion sequences, and use this intuition to design a practical attack against the SISA algorithm of Bourtoule et al. [2021] on CIFAR-10, MNIST, Fashion-MNIST.},
	language = {en},
	urldate = {2022-09-24},
	publisher = {arXiv},
	author = {Gupta, Varun and Jung, Christopher and Neel, Seth and Roth, Aaron and Sharifi-Malvajerdi, Saeed and Waites, Chris},
	month = jun,
	year = {2021},
	note = {arXiv:2106.04378 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Gupta et al. - 2021 - Adaptive Machine Unlearning.pdf:/Users/alexo/Zotero/storage/GDTI7ZV5/Gupta et al. - 2021 - Adaptive Machine Unlearning.pdf:application/pdf},
}

@article{maLearnForgetMachine2022,
	title = {Learn to {Forget}: {Machine} {Unlearning} {Via} {Neuron} {Masking}},
	issn = {1545-5971, 1941-0018, 2160-9209},
	shorttitle = {Learn to {Forget}},
	url = {https://ieeexplore.ieee.org/document/9844865/},
	doi = {10.1109/TDSC.2022.3194884},
	abstract = {Nowadays, machine learning models, especially neural networks, have became prevalent in many real-world applications. These models are trained based on a one-way trip from user data: as long as users contribute their data, there is no way to withdraw. To this end, machine unlearning becomes a popular research topic, which allows the model trainer to unlearn unexpected data from a trained machine learning model. In this paper, we propose the ﬁrst uniform metric called forgetting rate to measure the effectiveness of a machine unlearning method. It is based on the concept of membership inference and describes the transformation rate of the eliminated data from “memorized” to “unknown” after conducting unlearning. We also propose a novel unlearning method called Forsaken. It is superior to previous work in either utility or efﬁciency (when achieving the same forgetting rate). We benchmark Forsaken with eight standard datasets to evaluate its performance. The experimental results show that it can achieve more than 90\% forgetting rate on average and only causeless than 5\% accuracy loss.},
	language = {en},
	urldate = {2022-10-18},
	journal = {IEEE Transactions on Dependable and Secure Computing},
	author = {Ma, Zhuo and Liu, Yang and Liu, Ximeng and Liu, Jian and Ma, Jianfeng and Ren, Kui},
	year = {2022},
	pages = {1--14},
	file = {Ma et al. - 2022 - Learn to Forget Machine Unlearning Via Neuron Mas.pdf:/Users/alexo/Zotero/storage/LIIIUQQG/Ma et al. - 2022 - Learn to Forget Machine Unlearning Via Neuron Mas.pdf:application/pdf},
}

@misc{chundawatZeroShotMachineUnlearning2022,
	title = {Zero-{Shot} {Machine} {Unlearning}},
	url = {http://arxiv.org/abs/2201.05629},
	abstract = {Modern privacy regulations grant citizens the right to be forgotten by products, services and companies. In case of machine learning (ML) applications, this necessitates deletion of data not only from storage archives but also from ML models. Due to an increasing need for regulatory compliance required for ML applications, machine unlearning is becoming an emerging research problem. The right to be forgotten requests come in the form of removal of a certain set or class of data from the already trained ML model. Practical considerations preclude retraining of the model from scratch minus the deleted data. The few existing studies use either the whole training data, or a subset of training data, or some metadata stored during training to update the model weights for unlearning. However, strict regulatory compliance requires time-bound deletion of data. Thus, in many cases, no data related to the training process or training samples may be accessible even for the unlearning purpose. We therefore ask the question: is it possible to achieve unlearning with zero training samples? In this paper, we introduce the novel problem of zero-shot machine unlearning that caters for the extreme but practical scenario where zero original data samples are available for use. We then propose two novel solutions for zero-shot machine unlearning based on (a) error minimizing-maximizing noise and (b) gated knowledge transfer. These methods remove the information of the forget data from the model while maintaining the model efﬁcacy on the retain data. The zero-shot approach offers good protection against the model inversion attacks and membership inference attacks. We introduce a new evaluation metric, Anamnesis Index (AIN) to effectively measure the quality of the unlearning method. The experiments show promising results for unlearning in deep learning models on benchmark vision data-sets. The source code will be made publicly available.},
	language = {en},
	urldate = {2022-10-18},
	publisher = {arXiv},
	author = {Chundawat, Vikram S. and Tarun, Ayush K. and Mandal, Murari and Kankanhalli, Mohan},
	month = jul,
	year = {2022},
	note = {arXiv:2201.05629 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Chundawat et al. - 2022 - Zero-Shot Machine Unlearning.pdf:/Users/alexo/Zotero/storage/GMLWBS6A/Chundawat et al. - 2022 - Zero-Shot Machine Unlearning.pdf:application/pdf},
}

@misc{guoCertifiedDataRemoval2020,
	title = {Certified {Data} {Removal} from {Machine} {Learning} {Models}},
	url = {http://arxiv.org/abs/1911.03030},
	abstract = {Good data stewardship requires removal of data at the request of the data’s owner. This raises the question if and how a trained machine-learning model, which implicitly stores information about its training data, should be affected by such a removal request. Is it possible to “remove” data from a machine-learning model? We study this problem by deﬁning certiﬁed removal: a very strong theoretical guarantee that a model from which data is removed cannot be distinguished from a model that never observed the data to begin with. We develop a certiﬁed-removal mechanism for linear classiﬁers and empirically study learning settings in which this mechanism is practical.},
	language = {en},
	urldate = {2022-10-27},
	publisher = {arXiv},
	author = {Guo, Chuan and Goldstein, Tom and Hannun, Awni and van der Maaten, Laurens},
	month = aug,
	year = {2020},
	note = {arXiv:1911.03030 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {guo_2020_certified data removal from machine learning models.pdf:/Users/alexo/Library/CloudStorage/GoogleDrive-alex.oesterling@gmail.com/My Drive/Zotero DB/FlavioHima/guo_2020_certified data removal from machine learning models.pdf:application/pdf},
}

@article{izzoApproximateDataDeletion,
	title = {Approximate {Data} {Deletion} from {Machine} {Learning} {Models}},
	abstract = {Deleting data from a trained machine learning (ML) model is a critical task in many applications. For example, we may want to remove the inﬂuence of training points that might be out of date or outliers. Regulations such as EU’s General Data Protection Regulation also stipulate that individuals can request to have their data deleted. The naive approach to data deletion is to retrain the ML model on the remaining data, but this is too time consuming. In this work, we propose a new approximate deletion method for linear and logistic models whose computational cost is linear in the the feature dimension d and independent of the number of training data n. This is a signiﬁcant gain over all existing methods, which all have superlinear time dependence on the dimension. We also develop a new feature-injection test to evaluate the thoroughness of data deletion from ML models.},
	language = {en},
	author = {Izzo, Zachary and Smart, Mary Anne and Chaudhuri, Kamalika and Zou, James},
	pages = {10},
	file = {Izzo et al. - Approximate Data Deletion from Machine Learning Mo.pdf:/Users/alexo/Zotero/storage/DDMTBDL8/Izzo et al. - Approximate Data Deletion from Machine Learning Mo.pdf:application/pdf},
}

@inproceedings{daiFairnessExplanationQuality2022a,
	title = {Fairness via {Explanation} {Quality}: {Evaluating} {Disparities} in the {Quality} of {Post} hoc {Explanations}},
	shorttitle = {Fairness via {Explanation} {Quality}},
	url = {http://arxiv.org/abs/2205.07277},
	doi = {10.1145/3514094.3534159},
	abstract = {As post hoc explanation methods are increasingly being leveraged to explain complex models in high-stakes settings, it becomes critical to ensure that the quality of the resulting explanations is consistently high across all subgroups of a population. For instance, it should not be the case that explanations associated with instances belonging to, e.g., women, are less accurate than those associated with other genders. In this work, we initiate the study of identifying group-based disparities in explanation quality. To this end, we first outline several key properties that contribute to explanation quality—namely, fidelity (accuracy), stability, consistency, and sparsity—and discuss why and how disparities in these properties can be particularly problematic. We then propose an evaluation framework which can quantitatively measure disparities in the quality of explanations. Using this framework, we carry out an empirical analysis with three datasets, six post hoc explanation methods, and different model classes to understand if and when group-based disparities in explanation quality arise. Our results indicate that such disparities are more likely to occur when the models being explained are complex and non-linear. We also observe that certain post hoc explanation methods (e.g., Integrated Gradients, SHAP) are more likely to exhibit disparities. Our work sheds light on previously unexplored ways in which explanation methods may introduce unfairness in real world decision making.},
	language = {en},
	urldate = {2022-12-16},
	booktitle = {Proceedings of the 2022 {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	author = {Dai, Jessica and Upadhyay, Sohini and Aivodji, Ulrich and Bach, Stephen H. and Lakkaraju, Himabindu},
	month = jul,
	year = {2022},
	note = {arXiv:2205.07277 [cs]},
	keywords = {Computer Science - Machine Learning},
	pages = {203--214},
	file = {Dai et al. - 2022 - Fairness via Explanation Quality Evaluating Dispa.pdf:/Users/alexo/Zotero/storage/XV7BMNX9/Dai et al. - 2022 - Fairness via Explanation Quality Evaluating Dispa.pdf:application/pdf},
}

@misc{neelDescenttoDeleteGradientBasedMethods2020a,
  title = {Descent-to-{{Delete}}: {{Gradient-Based Methods}} for {{Machine Unlearning}}},
  shorttitle = {Descent-to-{{Delete}}},
  author = {Neel, Seth and Roth, Aaron and {Sharifi-Malvajerdi}, Saeed},
  year = {2020},
  month = jul,
  number = {arXiv:2007.02923},
  eprint = {2007.02923},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {We study the data deletion problem for convex models. By leveraging techniques from convex optimization and reservoir sampling, we give the first data deletion algorithms that are able to handle an arbitrarily long sequence of adversarial updates while promising both per-deletion run-time and steadystate error that do not grow with the length of the update sequence. We also introduce several new conceptual distinctions: for example, we can ask that after a deletion, the entire state maintained by the optimization algorithm is statistically indistinguishable from the state that would have resulted had we retrained, or we can ask for the weaker condition that only the observable output is statistically indistinguishable from the observable output that would have resulted from retraining. We are able to give more efficient deletion algorithms under this weaker deletion criterion.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/alexo/Zotero/storage/QKGCLK97/Neel et al. - 2020 - Descent-to-Delete Gradient-Based Methods for Mach.pdf}
}

@misc{lowyStochasticOptimizationFramework2022,
  title = {A {{Stochastic Optimization Framework}} for {{Fair Risk Minimization}}},
  author = {Lowy, Andrew and Baharlouei, Sina and Pavan, Rakesh and Razaviyayn, Meisam and Beirami, Ahmad},
  year = {2022},
  month = sep,
  number = {arXiv:2102.12586},
  eprint = {2102.12586},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  publisher = {{arXiv}},
  abstract = {Despite the success of large-scale empirical risk minimization (ERM) at achieving high accuracy across a variety of machine learning tasks, fair ERM is hindered by the incompatibility of fairness constraints with stochastic optimization. We consider the problem of fair classification with discrete sensitive attributes and potentially large models and data sets, requiring stochastic solvers. Existing in-processing fairness algorithms are either impractical in the large-scale setting because they require large batches of data at each iteration or they are not guaranteed to converge. In this paper, we develop the first stochastic in-processing fairness algorithm with guaranteed convergence. For demographic parity, equalized odds, and equal opportunity notions of fairness, we provide slight variations of our algorithm\textendash called FERMI\textendash and prove that each of these variations converges in stochastic optimization with any batch size. Empirically, we show that FERMI is amenable to stochastic solvers with multiple (non-binary) sensitive attributes and non-binary targets, performing well even with minibatch size as small as one. Extensive experiments show that FERMI achieves the most favorable tradeoffs between fairness violation and test accuracy across all tested setups compared with state-of-the-art baselines for demographic parity, equalized odds, equal opportunity. These benefits are especially significant with small batch sizes and for non-binary classification with large number of sensitive attributes, making FERMI a practical fairness algorithm for large-scale problems.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Theory,Computer Science - Machine Learning},
  file = {/Users/alexo/Library/CloudStorage/GoogleDrive-alex.oesterling@gmail.com/My Drive/Zotero DB/Concepts/Fairness/lowy_2022_a stochastic optimization framework for fair risk minimization.pdf;/Users/alexo/Zotero/storage/3F72HUTM/2102.12586 (1).pdf}
}

@inproceedings{caoMakingSystemsForget2015,
  title = {Towards {{Making Systems Forget}} with {{Machine Unlearning}}},
  booktitle = {2015 {{IEEE Symposium}} on {{Security}} and {{Privacy}}},
  author = {Cao, Yinzhi and Yang, Junfeng},
  year = {2015},
  month = may,
  pages = {463--480},
  publisher = {{IEEE}},
  address = {{San Jose, CA}},
  doi = {10.1109/SP.2015.35},
  abstract = {Today's systems produce a rapidly exploding amount of data, and the data further derives more data, forming a complex data propagation network that we call the data's lineage. There are many reasons that users want systems to forget certain data including its lineage. From a privacy perspective, users who become concerned with new privacy risks of a system often want the system to forget their data and lineage. From a security perspective, if an attacker pollutes an anomaly detector by injecting manually crafted data into the training data set, the detector must forget the injected data to regain security. From a usability perspective, a user can remove noise and incorrect entries so that a recommendation engine gives useful recommendations. Therefore, we envision forgetting systems, capable of forgetting certain data and their lineages, completely and quickly.},
  isbn = {978-1-4673-6949-7},
  langid = {english},
  file = {/Users/alexo/Zotero/storage/SQDGNGEL/Cao and Yang - 2015 - Towards Making Systems Forget with Machine Unlearn.pdf}
}

@misc{ginartMakingAIForget2019,
  title = {Making {{AI Forget You}}: {{Data Deletion}} in {{Machine Learning}}},
  shorttitle = {Making {{AI Forget You}}},
  author = {Ginart, Antonio and Guan, Melody Y. and Valiant, Gregory and Zou, James},
  year = {2019},
  month = nov,
  number = {arXiv:1907.05012},
  eprint = {arXiv:1907.05012},
  publisher = {{arXiv}},
  abstract = {Intense recent discussions have focused on how to provide individuals with control over when their data can and cannot be used \textemdash{} the EU's Right To Be Forgotten regulation is an example of this effort. In this paper we initiate a framework studying what to do when it is no longer permissible to deploy models derivative from specific user data. In particular, we formulate the problem of efficiently deleting individual data points from trained machine learning models. For many standard ML models, the only way to completely remove an individual's data is to retrain the whole model from scratch on the remaining data, which is often not computationally practical. We investigate algorithmic principles that enable efficient data deletion in ML. For the specific setting of k-means clustering, we propose two provably efficient deletion algorithms which achieve an average of over 100\texttimes{} improvement in deletion efficiency across 6 datasets, while producing clusters of comparable statistical quality to a canonical k-means++ baseline.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/alexo/Zotero/storage/EYTDL2AL/Ginart et al. - 2019 - Making AI Forget You Data Deletion in Machine Lea.pdf}
}

@inproceedings{di2022hidden,
  title={Hidden Poison: Machine unlearning enables camouflaged poisoning attacks},
  author={Di, Jimmy Z and Douglas, Jack and Acharya, Jayadev and Kamath, Gautam and Sekhari, Ayush},
  booktitle={NeurIPS ML Safety Workshop},
  year={2022}
}

@article{gupta2021adaptive,
  title={Adaptive machine unlearning},
  author={Gupta, Varun and Jung, Christopher and Neel, Seth and Roth, Aaron and Sharifi-Malvajerdi, Saeed and Waites, Chris},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={16319--16330},
  year={2021}
}

@inproceedings{shokri2017membership,
  title={Membership inference attacks against machine learning models},
  author={Shokri, Reza and Stronati, Marco and Song, Congzheng and Shmatikov, Vitaly},
  booktitle={2017 IEEE symposium on security and privacy (SP)},
  pages={3--18},
  year={2017},
  organization={IEEE}
}

@online{GDPR,
    title = {Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (General Data Protection Regulation)},
    url = {https://eur-lex.europa.eu/eli/reg/2016/679},
    author = {EU},
    date = {2016}
}

@article{cauwenberghs2000incremental,
  title={Incremental and decremental support vector machine learning},
  author={Cauwenberghs, Gert and Poggio, Tomaso},
  journal={Advances in neural information processing systems},
  volume={13},
  year={2000}
}

@article{karasuyama2010multiple,
  title={Multiple incremental decremental learning of support vector machines},
  author={Karasuyama, Masayuki and Takeuchi, Ichiro},
  journal={IEEE Transactions on Neural Networks},
  volume={21},
  number={7},
  pages={1048--1059},
  year={2010},
  publisher={IEEE}
}

@article{chaudhuri2011differentially,
  title={Differentially private empirical risk minimization.},
  author={Chaudhuri, Kamalika and Monteleoni, Claire and Sarwate, Anand D},
  journal={Journal of Machine Learning Research},
  volume={12},
  number={3},
  year={2011}
}

@article{hardt2016equality,
  title={Equality of opportunity in supervised learning},
  author={Hardt, Moritz and Price, Eric and Srebro, Nati},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@inproceedings{agarwal2018reductions,
  title={A reductions approach to fair classification},
  author={Agarwal, Alekh and Beygelzimer, Alina and Dud{\'\i}k, Miroslav and Langford, John and Wallach, Hanna},
  booktitle={International Conference on Machine Learning},
  pages={60--69},
  year={2018},
  organization={PMLR}
}

@article{berk2017convex,
  title={A convex framework for fair regression},
  author={Berk, Richard and Heidari, Hoda and Jabbari, Shahin and Joseph, Matthew and Kearns, Michael and Morgenstern, Jamie and Neel, Seth and Roth, Aaron},
  journal={arXiv preprint arXiv:1706.02409},
  year={2017}
}

@article{alghamdi2022beyond,
  title={Beyond adult and COMPAS: Fair multi-class prediction via information projection},
  author={Alghamdi, Wael and Hsu, Hsiang and Jeong, Haewon and Wang, Hao and Michalak, Peter and Asoodeh, Shahab and Calmon, Flavio},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={38747--38760},
  year={2022}
}

@incollection{angwin2016machine,
  title={Machine bias},
  author={Angwin, Julia and Larson, Jeff and Mattu, Surya and Kirchner, Lauren},
  booktitle={Ethics of data and analytics},
  pages={254--264},
  year={2016},
  publisher={Auerbach Publications}
}

@misc{asuncion2007uci,
  title={UCI machine learning repository},
  author={Asuncion, Arthur and Newman, David},
  year={2007},
  publisher={Irvine, CA, USA}
}

@article{ingels2011high,
  title={High School Longitudinal Study of 2009 (HSLS: 09): Base-Year Data File Documentation. NCES 2011-328.},
  author={Ingels, Steven J and Pratt, Daniel J and Herget, Deborah R and Burns, Laura J and Dever, Jill A and Ottem, Randolph and Rogers, James E and Jin, Ying and Leinwand, Steve},
  journal={National Center for Education Statistics},
  year={2011},
  publisher={ERIC}
}

@inproceedings{jeong2022fairness,
  title={Fairness without imputation: A decision tree approach for fair prediction with missing values},
  author={Jeong, Haewon and Wang, Hao and Calmon, Flavio P},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  number={9},
  pages={9558--9566},
  year={2022}
}

@inproceedings{jiang2019degenerate,
  title={Degenerate feedback loops in recommender systems},
  author={Jiang, Ray and Chiappa, Silvia and Lattimore, Tor and Gy{\"o}rgy, Andr{\'a}s and Kohli, Pushmeet},
  booktitle={Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
  pages={383--390},
  year={2019}
}

@inproceedings{kochno,
  title={No Matter How You Slice It: Machine Unlearning with SISA Comes at the Expense of Minority Classes},
  author={Koch, Korbinian and Soll, Marcus},
  booktitle={First IEEE Conference on Secure and Trustworthy Machine Learning}
}


@InProceedings{pmlr-v89-giordano19a,
  title = 	 {A Swiss Army Infinitesimal Jackknife},
  author =       {Giordano, Ryan and Stephenson, William and Liu, Runjing and Jordan, Michael and Broderick, Tamara},
  booktitle = 	 {Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics},
  pages = 	 {1139--1147},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Sugiyama, Masashi},
  volume = 	 {89},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {16--18 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v89/giordano19a/giordano19a.pdf},
  url = 	 {https://proceedings.mlr.press/v89/giordano19a.html},
  abstract = 	 {The error or variability of machine learning algorithms is often assessed by repeatedly refitting a model with different weighted versions of the observed data. The ubiquitous tools of cross-validation (CV) and the bootstrap are examples of this technique. These methods are powerful in large part due to their model agnosticism but can be slow to run on modern, large data sets due to the need to repeatedly re-fit the model. In this work, we use a linear approximation to the dependence of the fitting procedure on the weights, producing results that can be faster than repeated re-fitting by an order of magnitude. This linear approximation is sometimes known as the "infinitesimal jackknife" in the statistics literature, where it is mostly used as a theoretical tool to prove asymptotic results. We provide explicit finite-sample error bounds for the infinitesimal jackknife in terms of a small number of simple, verifiable assumptions. Our results apply whether the weights and data are stochastic or deterministic, and so can be used as a tool for proving the accuracy of the infinitesimal jackknife on a wide variety of problems. As a corollary, we state mild regularity conditions under which our approximation consistently estimates true leave k-out cross-validation for any fixed k. These theoretical results, together with modern automatic differentiation software, support the application of the infinitesimal jackknife to a wide variety of practical problems in machine learning, providing a "Swiss Army infinitesimal jackknife." We demonstrate the accuracy of our methods on a range of simulated and real datasets.}
}

@article{zhang2023forgotten,
  title={To Be Forgotten or To Be Fair: Unveiling Fairness Implications of Machine Unlearning Methods},
  author={Zhang, Dawen and Pan, Shidong and Hoang, Thong and Xing, Zhenchang and Staples, Mark and Xu, Xiwei and Yao, Lina and Lu, Qinghua and Zhu, Liming},
  journal={arXiv preprint arXiv:2302.03350},
  year={2023}
}

@inproceedings{dwork2012fairness,
  title={Fairness through awareness},
  author={Dwork, Cynthia and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Zemel, Richard},
  booktitle={Proceedings of the 3rd innovations in theoretical computer science conference},
  pages={214--226},
  year={2012}
}

@inproceedings{hebert2018multicalibration,
  title={Multicalibration: Calibration for the (computationally-identifiable) masses},
  author={H{\'e}bert-Johnson, Ursula and Kim, Michael and Reingold, Omer and Rothblum, Guy},
  booktitle={International Conference on Machine Learning},
  pages={1939--1948},
  year={2018},
  organization={PMLR}
}

@article{globus2022multicalibrated,
  title={Multicalibrated regression for downstream fairness},
  author={Globus-Harris, Ira and Gupta, Varun and Jung, Christopher and Kearns, Michael and Morgenstern, Jamie and Roth, Aaron},
  journal={arXiv preprint arXiv:2209.07312},
  year={2022}
}

@article{deng2023happymap,
  title={HappyMap: A Generalized Multi-calibration Method},
  author={Deng, Zhun and Dwork, Cynthia and Zhang, Linjun},
  journal={arXiv preprint arXiv:2303.04379},
  year={2023}
}

@inproceedings{kearns2018preventing,
  title={Preventing fairness gerrymandering: Auditing and learning for subgroup fairness},
  author={Kearns, Michael and Neel, Seth and Roth, Aaron and Wu, Zhiwei Steven},
  booktitle={International conference on machine learning},
  pages={2564--2572},
  year={2018},
  organization={PMLR}
}

@inproceedings{calders2009building,
  title={Building classifiers with independency constraints},
  author={Calders, Toon and Kamiran, Faisal and Pechenizkiy, Mykola},
  booktitle={2009 IEEE international conference on data mining workshops},
  pages={13--18},
  year={2009},
  organization={IEEE}
}

@article{zliobaite2015relation,
  title={On the relation between accuracy and fairness in binary classification},
  author={Zliobaite, Indre},
  journal={arXiv preprint arXiv:1505.05723},
  year={2015}
}

@inproceedings{zafar2017fairness,
  title={Fairness beyond disparate treatment \& disparate impact: Learning classification without disparate mistreatment},
  author={Zafar, Muhammad Bilal and Valera, Isabel and Gomez Rodriguez, Manuel and Gummadi, Krishna P},
  booktitle={Proceedings of the 26th international conference on world wide web},
  pages={1171--1180},
  year={2017}
}

@inproceedings{feldman2015certifying,
  title={Certifying and removing disparate impact},
  author={Feldman, Michael and Friedler, Sorelle A and Moeller, John and Scheidegger, Carlos and Venkatasubramanian, Suresh},
  booktitle={proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining},
  pages={259--268},
  year={2015}
}

@inproceedings{martinez2020minimax,
  title={Minimax pareto fairness: A multi objective perspective},
  author={Martinez, Natalia and Bertran, Martin and Sapiro, Guillermo},
  booktitle={International Conference on Machine Learning},
  pages={6755--6764},
  year={2020},
  organization={PMLR}
}

@article{krishna2023towards,
  title={Towards Bridging the Gaps between the Right to Explanation and the Right to be Forgotten},
  author={Krishna, Satyapriya and Ma, Jiaqi and Lakkaraju, Himabindu},
  journal={arXiv preprint arXiv:2302.04288},
  year={2023}
}

@inproceedings{cummings2019compatibility,
  title={On the compatibility of privacy and fairness},
  author={Cummings, Rachel and Gupta, Varun and Kimpara, Dhamma and Morgenstern, Jamie},
  booktitle={Adjunct Publication of the 27th Conference on User Modeling, Adaptation and Personalization},
  pages={309--315},
  year={2019}
}

@article{bagdasaryan2019differential,
  title={Differential privacy has disparate impact on model accuracy},
  author={Bagdasaryan, Eugene and Poursaeed, Omid and Shmatikov, Vitaly},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{esipova2022disparate,
  title={Disparate Impact in Differential Privacy from Gradient Misalignment},
  author={Esipova, Maria S and Ghomi, Atiyeh Ashari and Luo, Yaqiao and Cresswell, Jesse C},
  journal={arXiv preprint arXiv:2206.07737},
  year={2022}
}

@techreport{aibillofrights,
  title       = "Blueprint for an {AI} {B}ill of {R}ights",
  author      = "White House OSTP",
  institution = "The White House",
  address     = "Washington, DC",
  year        = 2022,
  month       = oct
}

@article{corbett2018measure,
  title={The measure and mismeasure of fairness: A critical review of fair machine learning},
  author={Corbett-Davies, Sam and Goel, Sharad},
  journal={arXiv preprint arXiv:1808.00023},
  year={2018}
}

@inproceedings{chouldechova2018case,
  title={A case study of algorithm-assisted decision making in child maltreatment hotline screening decisions},
  author={Chouldechova, Alexandra and Benavides-Prado, Diana and Fialko, Oleksandr and Vaithianathan, Rhema},
  booktitle={Conference on Fairness, Accountability and Transparency},
  pages={134--148},
  year={2018},
  organization={PMLR}
}

@article{chen2021algorithm,
  title={Algorithm fairness in ai for medicine and healthcare},
  author={Chen, Richard J and Chen, Tiffany Y and Lipkova, Jana and Wang, Judy J and Williamson, Drew FK and Lu, Ming Y and Sahai, Sharifa and Mahmood, Faisal},
  journal={arXiv preprint arXiv:2110.00603},
  year={2021}
}

@article{calmon2017optimized,
  title={Optimized pre-processing for discrimination prevention},
  author={Calmon, Flavio and Wei, Dennis and Vinzamuri, Bhanukiran and Natesan Ramamurthy, Karthikeyan and Varshney, Kush R},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}