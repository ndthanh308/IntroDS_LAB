Let $D = \{(x_1, y_1, s_1), \dots, (x_n, y_n, s_n)\} \in \mathcal{D}$ be a training dataset with size $n$, where $\mathcal{D}$ is the set of all possible datasets. For $i=1,2,\ldots, n$, $x_i \in \mathcal{X} \subseteq \mathbb{R}^d$ is a feature vector; $y_i \in \mathcal{Y} = \{0, 1\}$ is a binary label; and $s_i \in \mathcal{S} = \{a, b\}$ is a binary sensitive attribute. A learning algorithm is represented as a mapping $A: \mathcal{D} \rightarrow \mathcal{H}$ that maps a dataset $D\in \mathcal{D}$ to a model $A(D)$ in a hypothesis space $\mathcal{H}$. 

\paragraph{Unlearning.} Given set of samples to be deleted $R \subseteq D$, the goal of unlearning is to efficiently find a model that resembles the model retrained on $D\setminus R$ from scratch. We define  an unlearning mechanism as a mapping $M: \mathcal{H} \times \mathcal{D} \times \mathcal{D} \rightarrow \mathcal{H}$ that maps a learned model $A(D)$, the original dataset $D$, and a subset $R\subseteq D$ of data points to be deleted, to a new unlearned model $M(A(D), D, R)$. Ideally, the unlearned model $M(A(D), D, R)$ is statistically indistinguishable from $A(D\setminus R)$~\citep{guoCertifiedDataRemoval2020,neelDescenttoDeleteGradientBasedMethods2020a}. When both the learning algorithm $A$ and the unlearning mechanism $M$ are randomized, i.e., their outputs produce a probability distribution over the hypothesis $\mathcal{H}$, the notion of statistical indistinguishability can be formalized as follows.

\begin{definition}[$(\epsilon, \delta)$-Statistical Indistinguishability~\cite{guoCertifiedDataRemoval2020, neelDescenttoDeleteGradientBasedMethods2020a}.]\label{unlearning_definition}

For given $\epsilon, \delta > 0$, an unlearning mechanism $M$ achieves \emph{$(\epsilon, \delta)$-Statistical Indistinguishability} with respect to $A$, if for all $ \mathcal{M} \subseteq \mathcal{H}, D\in \mathcal{D}, R \subseteq D$ and $|R| = 1$, we have
\begin{align*}
    \Pr(M(A(D), D, R) \in \mathcal{M}) \leq e^\epsilon \Pr(A(D \setminus R) \in \mathcal{M}) + \delta, \\
    \Pr(A(D \setminus R) \in \mathcal{M}) \leq e^\epsilon \Pr(M(A(D), D, R) \in \mathcal{M}) + \delta.
\end{align*}
\end{definition}
Note that this definition is based on deletion of one data point ($|R| = 1$) and our theoretical analysis focuses on this simple case. However, it is possible to directly extend our theory to a batch of deletions with $|R| \le m$ for some $m < n$ using the results by \citet{guoCertifiedDataRemoval2020}.

\paragraph{Fairness.} We focus on a popular notion of group fairness, \emph{Equalized Odds}~\citep{hardt2016equality}, and present results for other popular metrics such as \emph{Demographic Parity}, \emph{Equality of Opportunity}, and \emph{Subgroup Accuracy} in the supplemental material. Consider a data point $(X, Y, S)$ randomly drawn from the data distribution, where $X\in \mathcal{X}, Y\in \mathcal{Y}$, and $S \in \mathcal{S}$. Note that $X$, $Y$, and $S$ are random variables. Equalized Odds is then defined as follows.
\begin{definition}[Equalized Odds~\citep{hardt2016equality}.] \label{equalizedodds}
    A model $h: \mathcal{X} \rightarrow \mathcal{Y}$ satisfies \emph{Equalized Odds} with respect to the sensitive attribute $S$ and the outcome $Y$ if the model prediction $h(X)$ and $S$ are independent conditional on $Y$. More formally, for all $y \in \{0, 1\}, \Pr(h(X) = 1 \mid S = a, Y = y) = \Pr(h(X) = 1 \mid S = b, Y = y).$
    % \begin{align*}
    %     \Pr(h(X) = 1 \mid S = a, Y = y) = 
    %     \Pr(h(X) = 1 \mid S = b, Y = y).
    % \end{align*}
\end{definition} 

In practice, when exact Equalized Odds is not achieved, we can use \emph{Absolute Equalized Odds Difference}, the absolute difference for both false positive rates and true positive rates between the two subgroups, as a quantitative measure for unfairness. The Absolute Equalized Odds Difference (AEOD) is formally defined as:
\begin{align}
    \frac{1}{2}\sum_{y\in \{0, 1\}}\big|\Pr(h(X) = 1 \mid S = a, Y=y) - \nonumber \\
    \Pr(h(X) = 1 \mid S = b, Y=y)\big|. \label{eq:aeod}
\end{align} 

%%%%%%%%%%%%%%%%%%%%%%%% Alex's original version %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \paragraph{Notation} 
% Let $\mathcal{D} = \{(x_1, y_1), \dots, (x_n, y_n)\}$ be a training dataset and let $A := (\mathcal{X} \times \mathcal{Y})^n \rightarrow \mathcal{H}$ be a learning algorithm that maps a dataset with data on the support of $\mathcal{X} \in \mathbb{R}^d$ and labels on the support of $\mathcal{Y} \in \{0, 1\}$ to a hypothesis space of models $\mathcal{H}$. % For simplicity we pack samples and their corresponding labels $x_i$, and $y_i$ into matrices $X$ and $Y$ respectively. 
% Generally this algorithm seeks to minimize an empirical estimate of a loss function $\mathcal{L}$. We consider the class of linear models measured over an empirical risk defined as follows: \alex{in my proof I multiply through by n, should I do that here as well?}:
% \begin{align}
%     \mathcal{L}_{erm}(\theta,\mathcal{D}) = \frac{1}{n}\sum_{i = 1}^{n} \ell(\theta,x_i, y_i) + \frac{\lambda}{2}||\theta||_2^2 \label{vanillaloss}
% \end{align}
% where $\ell(\theta, x, y)$ is a convex loss function.
% \alex{considering I introduce fairness and then unlearning in my methods section, I could swap the order here to match that}
% \paragraph{Unlearning} 
% The goal of unlearning is to remove a sample or multiple samples, $R \in \mathcal{D}$ and find a new model $h^* \in \mathcal{H}$ that is approximately equal to retraining algorithm $A$ on $\mathcal{D}\setminus R$. We define such an unlearning mechanism as the mapping $M : \mathcal{H} \times \mathcal{D} \times \mathcal{R \subset D} \rightarrow \mathcal{H}$ that takes in a trained model, a dataset and unlearning requests from that dataset, and outputs a new unlearned model. When unlearning, we have a full dataset, $\mathcal{D}$, a set of requests, $\mathcal{R} \subset \mathcal{D}$, and a remainder dataset $\mathcal{D}'$ such that $\mathcal{D} = \mathcal{R} + \mathcal{D}'$. In general, we assume $|\mathcal{R}| \ll |\mathcal{D}|$. The goal of unlearning is to show that the output of $M(A(
% \mathcal{D}), \mathcal{D}, \mathcal{R})$ well-approximates $A(\mathcal{D}\setminus \mathcal{R})$. We define this approximation as a statistical indistinguishability, and formalize unlearning as follows.
% \begin{definition}[$(\epsilon, \delta)$-Statistical Indistinguishability]\label{unlearning_definition}
% For all $\delta > 0$, for all $ h \subseteq \mathcal{H}, \mathcal{D} \subset \mathcal{Z}, R \in \mathcal{D}$, we have unlearned with algorithm $M$ if the following hold:
% \begin{align*}
%     P(M(A(\mathcal{D}), \mathcal{D}, \mathcal{R}) \in h) \leq e^\epsilon P(A(\mathcal{D} \setminus \mathcal{R}) \in h) + \delta \\
%     P(A(\mathcal{D} \setminus \mathcal{R}) \in h) \leq e^\epsilon P(M(A(\mathcal{D}), \mathcal{D}, \mathcal{R}) \in h) + \delta
% \end{align*}
% \end{definition}

% This definition draws directly from $(\epsilon, \delta)$-indistinguishability found in privacy literature \cite{guoCertifiedDataRemoval2020}, \cite{neelDescenttoDeleteGradientBasedMethods2020a}. To achieve this definition, a privacy mechanism is generally applied which randomizes the algorithm outputs. In this work, we use objective perturbation \cite{chaudhuri2011differentially} to achieve $(\epsilon, \delta)$-unlearning.

% \alex{cut, briefly touch in methods section}

% The following unlearning algorithm proposed by Guo et al. approximates the optimum achieved by retraining over a convex loss with a Newton step \cite{guoCertifiedDataRemoval2020}. Let $\theta^* = \arg\min_{\theta} \mathcal{L}(\theta, \mathcal{D}) $ be the fully-trained parameters of a model, and $\mathcal{R} \subset \mathcal{D}$ be the set of data to be unlearned. Then, unlearning over the normal empirical risk minimization loss $\mathcal{L}$ can be achieved by a Newton step. \alex{Should I label this, and if so, should I label it as Guo et al., or Newton's Method, or something else like "Algorithm 1"?}

% \begin{align}
%     M(\theta^*,\mathcal{D}, \mathcal{R}) := \theta^* + H_{\theta^*}^{-1}\Delta
% \end{align}

% where $\Delta = \sum_{i \in \mathcal{R}}\nabla \ell(h_\theta^*(x_i), y_i) + |\mathcal{R}|\lambda \theta^*$ is related to the loss gradient over the unlearned samples and $H_{\theta^*} = \nabla^2 \mathcal{L}(\theta^*, \mathcal{D}')$ is the Hessian over $\mathcal{D}'$ at $\theta^*$. The authors show that under certain assumptions, the loss gradient over the remainder dataset $\mathcal{D}'$ at unlearned model $\theta^- := M(\theta^*, \mathcal{D}, \mathcal{R})$ is bounded. They then show that by adding noise to the loss function during training (also known as objective perturbation \cite{chaudhuri2011differentially}), they can guarantee $(\epsilon, \delta)$ unlearnability for algorithm M.

% By Theorem 1 in \cite{guoCertifiedDataRemoval2020}, if $\ell$ is $\gamma$-lipschitz and $||\nabla\ell(h_\theta(x), y)||_2 \leq C$, then we know $||\nabla\mathcal{L}(\theta^-, \mathcal{D}')||_2 \leq \frac{4\gamma C^2}{\lambda^2(n-1)}$ is bounded. \color{red} How do I cite a theorem from another paper? Also, they prove only for the case $|\mathcal{R}| 
%  = 1$, which is why the bound has $(n-1)$ on the bottom but my math above allows for $|R|$ of any size. Should I change the bound to be correct despite not being their Theorem 1 exactly?\color{black}.

% To achieve unlearning, we add noise to our loss function during training, in a method known as objective perturbation \cite{chaudhuri2011differentially}. By drawing noise from a gaussian with variance proportional to the gradient bound on our unlearned model $\theta^-$, we can guarantee $(\epsilon, \delta)$ unlearnability for algorithm M according to Theorem 3 in \cite{guoCertifiedDataRemoval2020}.

% \paragraph{Fairness}

% We consider a setting where our feature vector $x_i$ contains sensitive group attribute information. Let $s_i \in \{a, b\}$ be a binary value representing subgroup membership for sample $i$ in our dataset. Our goal is to achieve fairness in model performance for each group. One such definition of fair treatment is Equalized Odds as proposed by \citet{hardt2016equality}.
% \begin{definition}[Equalized Odds:] \label{equalizedodds}
%     A classifier $h: \mathcal{X} \rightarrow \mathcal{Y}$ satisfies equalized odds if its predictions are conditionally independent of the protected attribute and given label. More formally, let binary group attribute $s$ take values in $\{a, b\}$. Then, for random variables $X$, $Y$, and $S$:
%     \begin{align}
%         P(h(X) = \hat{y} | S = a, Y = y) = P(h(X) = \hat{y} | S = b, Y = y)
%     \end{align}
% \end{definition}
% In other words, the outputs of our classifier should be independent of the true label and the sensitive group attribute of each sample. 