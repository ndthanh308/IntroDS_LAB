In this work, we show that fairness and unlearning cannot be achieved simultaneously with existing solutions, and we resolve this issue by proposing a fair unlearning algorithm. We prove our method is unlearnable and bound its fairness guarantees. Then, we evaluate our method on three real-world datasets and show that our method well approximates retraining with a fair loss function in terms of both accuracy and fairness. Further, we highlight worst case scenarios where unlearning requests disproportionately come from certain subgroups. Our fair unlearning method allows practitioners to achieve multiple data protection goals at the same time (fairness and unlearning) and satisfy regulatory principles. Furthermore, we hope this paper sparks discussion surrounding other auxiliary goals that may need be compliant with unlearning such as robustness and interpretability, or other definitions of fairness usch as multicalibration and multiaccuracy.