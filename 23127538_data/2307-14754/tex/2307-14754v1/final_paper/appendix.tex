\appendix
\section{Appendix}
\paragraph{Limitations.} Our work does have a few limitations; as with other state of the art unlearning techniques, it requires a convex loss function to guarantee unlearning and thus uses a convex fairness regularizer rather than other state of the art fairness techniques. However, these limitations do not prevent our contributions from being both practical and encouraging for practitioners and researchers in the fields of algorithmic fairness and machine unlearning. Furthermore, there are many ways to operationalize fairness, and as the most popular group fairness metrics can be written as linear constraints that are therefore convex, there is room to explore future fair unlearning methods within the space of convex optimization problems. In addition, there are fairness definitions beyond group fairness that our method does not aim to satisfy. While we achieve a narrow definition of fairness, it is important for practitioners to consider their overall fairness goals and what sort of definition is desirable for their specific use cases.

% \paragraph{Broader Impact.} As one of the first papers at the intersection of machine unlearning and algorithmic fairness, our paper has impact in multiple areas of data protection and trustworthy ML. We combine various tools from each to create a method that can protect users from harm across multiple axes of discrimination and data protection. As data protection policy becomes more robust with the rise of AI, and organizations seek to comply with this public policy, our method will enable practitioners to protect the various rights of their consumers. Our method will facilitate the fair treatment of users from unlearnable systems while ensuring they are correctly removed from models. However, our method does aim to comply with a narrow definition of group fairness, and so still has the potential to cause harm to individuals from other fairness perspectives such as multicalibration and individual fairness. As with many trustworthy AI solutions, it is important to understand the context in which a system is deployed to truly understand the risks and safety goals one seeks to achieve.

\subsection{Proof of Theorem \ref{thm:eps_delta}}

We begin by introducing two lemmas.

Recall the definition of our full loss function,
\begin{align}
    \mathcal{L}(\theta, D) = \frac{1}{n}\sum_{i=1}^n \ell(\theta, x_i, y_i) + \frac{\lambda}{2n}||\theta||_2^2& + \gamma\frac{1}{|N|}\sum_{i,j,k,l \in N}\ell_\textrm{fair}(\theta, \{(x_m, y_m)\}_{m \in \{i,j,k,l\}}), \nonumber \\
    \ell_\textrm{fair}(\theta, \{(x_m, y_m)\}_{m \in \{i,j,k,l\}}) &= \mathbbm{1}[y_i = y_j](\langle x_i, \theta\rangle  - \langle x_j, \theta\rangle)\mathbbm{1}[y_k = y_l](\langle x_k, \theta\rangle  - \langle x_l, \theta\rangle) \nonumber \\
    &=\mathbbm{1}[y_i = y_j]\mathbbm{1}[y_k = y_l]\left(\theta^T (x_i^T x_k - x_i^T x_l - x_j^T x_k + x_j^T x_l)\theta\right). \nonumber
\end{align}
For our proof of Lemma \ref{lem:bounded_gradient}, we multiply by the entire objective by $n$:
\begin{align}
    \mathcal{L}(\theta, D) = \sum_{i=1}^n \ell(\theta, x_i, y_i) + \frac{\lambda}{2}||\theta||_2^2 + \gamma\frac{n}{|N|}\sum_{i,j,k,l \in N}\ell_\textrm{fair}(\theta, \{(x_m, y_m)\}_{m \in \{i,j,k,l\}}).
\end{align}
Let $\theta^- := \theta^*+H_{\theta^*}^{-1}\Delta$ be the update step of our unlearing algorithm, where $\theta^*$ is our optimum when training on the entire dataset, $H_{\theta^*}^{-1} := (\nabla^2 \mathcal{L}(\theta^*; D'))^{-1}$ is the inverse hessian of the loss at $\theta^*$ evaluated over the remaining dataset defined as $D'$.Without loss of generality assume $R$ contains one element, $x_n$.
Let $G_a := \{i : s_i = a\}$, $G_b := \{i : s_i = b\}$ be sets of indices indicating subgroup membership for each sample, and that $x_n$ has group attribute $a$, or $n \in G_a$. Define $C = \{n\} \times G_b \times \{n\} \times G_b$.
Then, define our unlearning step $\Delta$ as the following:
\begin{align}
    &\Delta := \ell'(\theta^*, x_n, y_n) + \lambda \theta^*  + \gamma\left(\frac{n}{|N|} - \frac{n-1}{|N\setminus C|}\right)\sum_{(i,j,k,l)\in N \setminus C}\ell_\textrm{fair}'(\theta, \{(x_m, y_m)\}_{m \in \{i,j,k,l\}})\nonumber\\
    &+\gamma\frac{n}{|N|} \sum_{(i,j,k,l) \in C}\ell_\textrm{fair}'(\theta, \{(x_m, y_m)\}_{m \in \{i,j,k,l\}}). \label{eqn:delta}
\end{align}

\begin{lemma} \label{lem:bounded_gradient}
Assume the ERM loss $\ell$ is $\psi$-Lipschitz in its second derivative ($\ell''$ is $\psi$-Lipschitz), and bounded in its first derivative by a constant $||\ell'(\theta, x, y)||_2 \leq g$. Suppose the data is bounded such that for all $i \in n$,  $||x_i||_2 \leq 1$. Then the gradient of our unlearned model on the remaining dataset is bounded by:

\begin{align}
    ||\nabla \mathcal{L}(\theta^-; D')||_2 \leq  \frac{\psi}{\lambda^2(n-1)}\left(2g + ||\theta^*||_2\left|\left|\frac{8(n-1)}{n_a^2} \right|\right|_2\right)^2.
\end{align}
\end{lemma}

\textit{Proof:} This proof largely follows the structure of the proof of Theorem 1 in~\citet{guoCertifiedDataRemoval2020}, but differs in the addition of our fairness loss term which introduces additional terms into the bound.

Let $G(\theta) = \nabla L(\theta; D')$ be the gradient at the \textit{remaining} dataset $D'$. We want to bound the norm of the gradient at $\theta = \theta^-$, as we know for convex loss, the optimum when retraining over the \textit{remainder} dataset $D'$ is zero. Keeping the norm of the unlearned weights close to zero ensures we have achieved unlearning.

We substitute the expression for the unlearned parameter and do a first-order Taylor approximation, which states there exists an $\eta \in [0,1]$ such that the following holds:

\begin{align*}
    G(\theta^-) &= G(\theta^* + H_{\theta^*}^{-1}\Delta), \\
    &= G(\theta^*) + \nabla G(\theta^* + \eta H_{\theta^*}^{-1} \Delta)H_{\theta^*}^{-1}\Delta. 
\end{align*}

Recall that G is the gradient of the loss, so here $\nabla G$ is the Hessian evaluated at this $\eta$ perturbed value. Let $H_{\theta_\eta}$ be the Hessian evaluated at the point $\theta_\eta = \theta^* + \eta H_{\theta^*}^{-1}\Delta$. Then we have

\begin{align}
    &= G(\theta^*) + H_{\theta_\eta}H_{\theta^*}^{-1}\Delta, \nonumber \\
    &= G(\theta^*) + \Delta + H_{\theta_\eta}H_{\theta^*}^{-1}\Delta - \Delta.
\end{align}

By our choice of $\Delta$, $G(\theta^*) + \Delta = 0$.

\begin{align}
    G(\theta^*) &= \nabla \mathcal{L}(\theta^*; D'), \nonumber \\
    &= \sum_{i=1}^{n-1} \ell'(\theta^*, x_i, y_i) + (n-1)\lambda\theta^* + \gamma\frac{n-1}{|N\setminus C|}\sum_{i,j,k,l\in N \setminus C}\ell_\textrm{fair}'(\theta, \{(x_m, y_m)\}_{m \in \{i,j,k,l\}}). \label{eqn:G}
\end{align}

% Recall our definition of $\Delta$.

% \begin{align*}
%     &\Delta := \ell'(\theta^*, x_n, y_n) + \lambda \theta^*  + \gamma\left(\frac{n}{|N|} - \frac{n-1}{|N\setminus C|}\right)\sum_{(i,j,k,l)\in N \setminus C}\ell_\textrm{fair}'(\theta, \{(x_m, y_m)\}_{m \in \{i,j,k,l\}})\nonumber\\
%     &+\gamma\frac{n}{|N|} \sum_{(i,j,k,l) \in C}\ell_\textrm{fair}'(\theta, \{(x_m, y_m)\}_{m \in \{i,j,k,l\}}). 
%     % \Delta := \sum_{i =  n-|R|+1}^n\ell'(\theta^*, x_i, y_i) + |R|\lambda \theta^* + 2\theta^*\left( (\frac{1}{n_a^2n_b^2} - \frac{1}{n_a'^2n_b'^2})AA^T+\frac{1}{n_a^2n_b^2} (2AB^T + BB^T)\right)
% \end{align*}


Combining \eqref{eqn:delta} and \eqref{eqn:G}, we see $G(\theta^*) + \Delta$ equals 

\begin{align}
    G(\theta^*) + \Delta &= \sum_{i=1}^{n} \ell'(\theta^*, x_i, y_i) + n\lambda\theta^* + \gamma\frac{n}{|N|}\sum_{i,j,k,l \in C}\ell_\textrm{fair}'(\theta, \{(x_m, y_m)\}_{m \in \{i,j,k,l\}}), \label{eqn:G_plus_delta} \\
    &= \nabla \mathcal{L}(\theta^*; D),  \nonumber \\
    &= 0. \nonumber
\end{align}

We see in~\eqref{eqn:G_plus_delta} that $G(\theta^*) + \Delta = \nabla \mathcal{L}(\theta^*; D)$. Then, $\nabla \mathcal{L}(\theta^*; D) = 0$ because our loss function is convex and our optimum occurs at $\theta^*$. 

Returning to the proof of Lemma \ref{lem:bounded_gradient}, we see:

\begin{align}
    G(\theta^-) &= G(\theta^*) + \Delta + H_{\theta_\eta}H_{\theta^*}^{-1}\Delta - \Delta, \nonumber \\
    &= 0 + H_{\theta_\eta}H_{\theta^*}^{-1}\Delta - \Delta, \nonumber \\
    &= H_{\theta_\eta}H_{\theta^*}^{-1}\Delta - H_{\theta^*}H_{\theta^*}^{-1}\Delta, \nonumber\\
    &= (H_{\theta_\eta} - H_{\theta^*})H_{\theta^*}^{-1}\Delta. \label{eqn:simplified_unlearned_gradient}
\end{align}

We want to bound the norm of~\eqref{eqn:simplified_unlearned_gradient}.

\begin{align*}
    ||G(\theta^-)||_2 &= ||(H_{\theta_\eta} - H_{\theta^*})H_{\theta^*}^{-1}\Delta||_2 \\
    &\leq ||H_{\theta_\eta} - H_{\theta^*}||_2||H_{\theta^*}^{-1}\Delta||_2
\end{align*}

First, we bound the left term. Recall we define $H$ as the Hessian evaluated over the remainder dataset $D'$. Also note that $\ell''_\textrm{fair}$ is constant in $\theta$ so $\ell_\textrm{fair}''(\theta_\eta, \{(x_m, y_m)\}_{m \in \{i,j,k,l\}}) - \ell_\textrm{fair}''(\theta^*, \{(x_m, y_m)\}_{m \in \{i,j,k,l\}}) = 0$. Then we have

\begin{align}
    ||H_{\theta_\eta} - H_{\theta^*}||_2 &= ||\sum_{i=1}^{n-1} \nabla^2\ell(\theta_\eta, x_i, y_i) + \gamma\frac{n-1}{|N\setminus C|}\sum_{i,j,k,l\in N \setminus C}\ell_\textrm{fair}''(\theta_\eta, \{(x_m, y_m)\}_{m \in \{i,j,k,l\}}) \nonumber\\ 
     - \sum_{i=1}^{n-1}& \nabla^2\ell(\theta^*, x_i, y_i) - \gamma\frac{n-1}{|N\setminus C|}\sum_{i,j,k,l\in N \setminus C}\ell_\textrm{fair}''(\theta^*, \{(x_m, y_m)\}_{m \in \{i,j,k,l\}})||_2, \nonumber \\
    &\leq \sum_{i=1}^{n-1} || \nabla^2 \ell (\theta_\eta, x_i, y_i)-\nabla^2 \ell(\theta^*,x_i,y_i)||_2, \nonumber \\
    &= \sum_{i=1}^{n-1}|| x_i^T(\ell'' (\theta_\eta, x_i, y_i)-\ell''(\theta^*,x_i,y_i)) x_i||_2, \label{eqn:chainrule}  \\
    &\leq \sum_{i=1}^{n-1}||\ell'' (\theta_\eta, x_i, y_i)-\ell''(\theta^*, x_i,y_i)||_2||x_i||_2^2, \nonumber \\
    &\leq \sum_{i=1}^{n-1}\psi ||\theta_\eta - \theta^* ||_2||x_i||_2^2, \label{eqn:lipschitz} \\ 
    &\leq \sum_{i=1}^{n-1}\psi ||\theta_\eta - \theta^* ||_2, \label{eqn:x_simplify} \\
    &= (n-1)\psi ||\theta_\eta - \theta^* ||_2, \nonumber \\
    &= (n-1)\psi ||\theta^* + \eta H_{\theta^*}^{-1}\Delta - \theta^*||_2, \nonumber \\
    &= (n-1)\psi ||\eta H_{\theta^*}^{-1}\Delta||_2, \nonumber \\
    &\leq  (n-1)\psi ||H_{\theta^*}^{-1}\Delta||_2,
\end{align}

where \eqref{eqn:chainrule} comes from chain rule, \eqref{eqn:lipschitz} comes from our assumption $\ell''$ is $\psi$-Lipschitz, and \eqref{eqn:x_simplify} comes from our assumption that $||x_i||_2 \leq 1$.

Thus, we have:

\begin{align}
    ||G(\theta^-)||_2 &\leq ||H_{\theta_\eta} - H_{\theta^*}||_2||H_{\theta^*}^{-1}\Delta||_2, \nonumber \\
    &\leq (n-1)\psi ||H_{\theta^*}^{-1}\Delta||_2^2
\end{align}

We bound the quantity $||H_{\theta^*}^{-1}\Delta||_2 $. Remember we are evaluating our Hessians and gradients over the remainder dataset, $D'$. With a convex $\ell$ and $\ell_\textrm{fair}$ where $\nabla^2\ell, \nabla^2\ell_\textrm{fair}$ are positive semi-definite and regularization of strength $\frac{n\lambda}{2}$, our loss function is $n\lambda$-strongly convex. Thus, our Hessian over the remainder dataset (of size $n-1$) is bounded:
\begin{align}
    ||H_{\theta}||_2 \geq (n-1)\lambda \nonumber
\end{align}
And so is its inverse 
\begin{align}
    ||H_{\theta}^{-1}||_2 \leq \frac{1}{(n-1)\lambda} \label{hessinv}
\end{align}

Finally, we bound $\Delta$. Recall its definition:

\begin{align}
        % \Delta := \sum_{i =  n-|R|+1}^n\ell'(\theta^*, x_i, y_i) + |R|\lambda \theta^* + 2\theta^*\left( (\frac{1}{n_a^2n_b^2} - \frac{1}{n_a'^2n_b'^2})AA^T+\frac{1}{n_a^2n_b^2} (2AB^T + BB^T)\right)
    &\Delta := \ell'(\theta^*, x_n, y_n) + \lambda \theta^*  + \gamma\left(\frac{n}{|N|} - \frac{n-1}{|N\setminus C|}\right)\sum_{(i,j,k,l)\in N \setminus C}\ell_\textrm{fair}'(\theta, \{(x_m, y_m)\}_{m \in \{i,j,k,l\}})\nonumber\\
    &+\gamma\frac{n}{|N|} \sum_{(i,j,k,l) \in C}\ell_\textrm{fair}'(\theta, \{(x_m, y_m)\}_{m \in \{i,j,k,l\}}).
\end{align}


% \color{red} this is repetitive because in the proof for batched requests we actually bound the sum of losses here not the single piece. \color{black}

% \begin{align}
%     % \left|\left|\sum_{i =  n-|R|+1}^n\ell'(\theta^*, x_i, y_i)\right|\right|_2 \leq |R|C \label{term1}
%     \left|\left|\ell'(\theta^*, x_i, y_i)\right|\right|_2 \leq C \label{term1}
% \end{align}

We bound $\theta^*$ to bound our second term. Recall that the gradient over the full dataset at $\theta^*$ equals 0. 

\begin{align}
    0 &= \nabla \mathcal{L}(\theta^*, D) = \sum_{i=1}^{n} \ell'(\theta^*, x_i, y_i) + n\lambda\theta^* + \gamma\frac{n}{|N|} \sum_{i,j,k,l \in N}\ell_\textrm{fair}'(\theta^*, \{(x_m, y_m)\}_{m \in \{i,j,k,l\}}) \label{eqn:stationary_condition}
\end{align}

We expand $\ell'_\textrm{fair}$ and see it is linear in $\theta$. Denote this coefficient $F(i,j,k,l)$.

\begin{align}
    \ell_\textrm{fair}' =2\cdot\mathbbm{1}[y_i = y_j]\mathbbm{1}[y_k = y_l] (x_i^T x_k - x_i^T x_l - x_j^T x_k + x_j^T x_l)\theta
\end{align}

We can bound $\theta^*$ by taking the stationary point condition~\eqref{eqn:stationary_condition} and solving for $\theta^*$. Using the assumption the ERM loss is bounded $||\ell'(\theta, x_i, y_i)||_2 \leq g$, we see:

\begin{align}
    \theta^* &= \frac{-\sum_{i=1}^{n} \ell'(\theta^*, x_i, y_i)}{n\lambda + \gamma\frac{n}{|N|} \sum_{i,j,k,l \in N}F(i,j,k,l)}, \nonumber\\
    ||\theta^*||_2 &= \left|\left| \frac{\sum_{i=1}^{n} \ell'(\theta^*, x_i, y_i)}{n\lambda + \gamma\frac{n}{|N|} \sum_{i,j,k,l \in N}F(i,j,k,l)} \right|\right|_2, \nonumber\\
    ||\theta^*||_2 &\leq ng \left|\left|\frac{1}{n\lambda + \gamma\frac{n}{|N|} \sum_{i,j,k,l \in N}F(i,j,k,l)}\right|\right|_2, \nonumber \\
    ||\theta^*||_2 &\leq g \left|\left|\frac{1}{\lambda + \gamma\frac{1}{|N|} \sum_{i,j,k,l \in N}F(i,j,k,l)}\right|\right|_2  \label{thetanorm}
\end{align}

% To bound the denominator of the fraction, note $(A+B)(A+B)^T$ has a minimum of 0. Thus, the maximum value $\theta^*$ can take on is

% \begin{align}
%     ||\theta^*||_2 &\leq \frac{C}{\lambda} \label{term2}
% \end{align}

Now, we can bound the norm of $\Delta$. We use the triangle inequality as well as the fact that the terms $n, n_a, n_b > 1$ to simplify.

\begin{align}
    % &\Delta := \ell'(\theta^*, x_n, y_n) + \lambda \theta^*  + \gamma\left(\frac{n}{|N|} - \frac{n-1}{|N\setminus C|}\right)\sum_{(i,j,k,l)\in N \setminus C}\ell_\textrm{fair}'(\theta, \{(x_m, y_m)\}_{m \in \{i,j,k,l\}})\nonumber\\
    % &+\gamma\frac{n}{|N|} \sum_{(i,j,k,l) \in C}\ell_\textrm{fair}'(\theta, \{(x_m, y_m)\}_{m \in \{i,j,k,l\}}), \nonumber \\
    ||\Delta||_2 &= \left|\left|\ell'(\theta^*, x_n, y_n) + \theta^*\left[\lambda + \gamma\left( (\frac{n}{|N|} - \frac{n-1}{|N\setminus C|})\sum_{i,j,k,l\in N \setminus C}F(i,j,k,l)+\frac{n}{|N|} \sum_{i,j,k,l \in C}F(i,j,k,l) \right)\right]\right|\right|_2, \nonumber \\
    % &\leq ||\ell'(\theta^*, x_n, y_n)||_2 \left|\left|\theta^*\left[\lambda+ \gamma\left( (\frac{n}{|N|} - \frac{n-1}{|N\setminus C|})AA^T+\frac{n}{n_a^2n_b^2} (2AB^T + BB^T)\right)\right]\right|\right|_2 \\
    &\leq g + ||\theta^*||_2\left|\left|\lambda + \gamma\left( (\frac{n}{|N|} - \frac{n-1}{|N\setminus C|})\sum_{i,j,k,l\in N \setminus C}F(i,j,k,l)+\frac{n}{|N|} \sum_{i,j,k,l \in C}F(i,j,k,l) \right)\right|\right|_2, \nonumber \\
    &\leq g + ||\theta^*||_2\left|\left|\lambda + \gamma\left( (\frac{n}{|N|} - \frac{n-1}{|N|})\sum_{i,j,k,l\in N \setminus C}F(i,j,k,l)+\frac{n}{|N|} \sum_{i,j,k,l \in C}F(i,j,k,l) \right)\right|\right|_2, \nonumber \\
    &= g + ||\theta^*||_2\left|\left|\lambda + \gamma\left(\frac{1}{|N|}\sum_{i,j,k,l\in N \setminus C}F(i,j,k,l)+\frac{n}{|N|} \sum_{i,j,k,l \in C}F(i,j,k,l) \right)\right|\right|_2, \nonumber \\
    &= g + ||\theta^*||_2\left|\left|\lambda + \gamma\left(\frac{1}{|N|}\sum_{i,j,k,l\in N}F(i,j,k,l)+\frac{n-1}{|N|} \sum_{i,j,k,l \in C}F(i,j,k,l) \right)\right|\right|_2, \nonumber \\
    &\leq g + ||\theta^*||_2\left|\left|\lambda + \gamma\frac{1}{|N|}\sum_{i,j,k,l\in N}F(i,j,k,l)\right|\right|_2+||\theta^*||_2\left|\left|\frac{n-1}{|N|} \sum_{i,j,k,l \in C}F(i,j,k,l) \right|\right|_2
    % &= \left| \left|2\theta^*\left( -\frac{1}{(n_a-1)^2n_b'^2}AA^T+\frac{1}{n_a^2n_b^2} (AA^T + 2AB^T + BB^T)\right) \right| \right|_2\\
    % &= \left| \left|2\theta^*\left( -\frac{1}{(n_a-1)^2n_b'^2}AA^T+\frac{1}{n_a^2n_b^2} (A+B)(A+B)^T\right) \right| \right|_2\\
    % &\leq 2||\theta^*||_2 \left| \left|\left( -\frac{1}{(n_a-1)^2n_b'^2}AA^T+\frac{1}{n_a^2n_b^2} (A+B)(A+B)^T\right) \right| \right|_2 \\
    % &\leq 2||\theta^*||_2 \left(\frac{1}{(n_a-1)^2n_b'^2}\left| \left|AA^T\right|\right|_2+\frac{1}{n_a^2n_b^2} \left|\left|(A+B)(A+B)^T \right| \right|_2\right) \\
    % &\leq 2||\theta^*||_2 \left(\frac{1}{(n_a-1)^2n_b'^2}\left| \left|A\right|\right|^2_2+\frac{1}{n_a^2n_b^2} \left|\left|A+B \right| \right|^2_2\right)
\end{align}

We can plug in our solution for $||\theta^*||_2$ from \eqref{thetanorm} and we see the second term equals $g$.

\begin{align}
    ||\Delta||_2 &\leq 2g + ||\theta^*||_2\left|\left|\frac{n-1}{|N|} \sum_{i,j,k,l \in C}F(i,j,k,l) \right|\right|_2 \label{eqn:deltabound}
\end{align}

Next we analyze the final term. Recall our assumption $||x_i||_2 \leq 1$ and the fact that $|C| = |G_b|^2 = n_b^2$ and $|N| = |G_b|^2|G_a|^2 = n_a^2n_b^2$.
% Under the assumption that $n_a/n_b \approx \Theta(1)$, or that the subgroups are relatively balanced, we can bound the order of our third term. 

% Observe that $||\theta^*||_2$ is $O(1).$ Define $R$ as follows, and bound using the triangle inequality:

\begin{align}
    ||\theta^*||_2\left|\left|\frac{n-1}{|N|} \sum_{i,j,k,l \in C}F(i,j,k,l) \right|\right|_2 &= ||\theta^*||_2\left|\left|\frac{n-1}{|N|} \sum_{i,j,k,l \in C}2\cdot\mathbbm{1}[y_i = y_j]\mathbbm{1}[y_k = y_l] (x_i^T x_k - x_i^T x_l - x_j^T x_k + x_j^T x_l) \right|\right|_2, \nonumber \\
    &\leq ||\theta^*||_2\left|\left|\frac{8|C|(n-1)}{|N|} \right|\right|_2, \nonumber \\
    &= ||\theta^*||_2\left|\left|\frac{8|C|n-1}{|N|} \right|\right|_2, \nonumber \\
    &= ||\theta^*||_2\left|\left|\frac{8(n-1)}{n_a^2} \right|\right|_2.
\end{align}

Under the assumption that the subgroups are relatively balanced ($n_a, n_b \approx O(n)$) this term vanishes from \eqref{eqn:deltabound} as $n$ increases. From Equation \eqref{hessinv} we have $||H_{\theta}^{-1}||_2 \leq \frac{1}{(n-1)\lambda}$ and thus we combine \eqref{hessinv}, \eqref{eqn:deltabound} to show our gradient is $O(1/n)$. The final form of our gradient norm is the following.

\begin{align}
    ||\mathcal{L}(\theta^-, D')||_2 = ||G(\theta^-)||_2 &\leq (n-1)\psi ||H_{\theta^*}^{-1}\Delta||_2^2, \nonumber \\ 
    &\leq (n-1)\psi ||H_{\theta^*}^{-1}||^2_2||\Delta||_2^2, \nonumber \\
    &\leq (n-1)\psi \frac{1}{(n-1)^2\lambda^2}(2g + ||\theta^*||_2\left|\left|\frac{8(n-1)}{n_a^2} \right|\right|_2)^2, \nonumber \\
    &= \frac{\psi}{\lambda^2(n-1)}\left(2g + ||\theta^*||_2\left|\left|\frac{8(n-1)}{n_a^2} \right|\right|_2\right)^2. && \square
\end{align}

\begin{lemma}[\citet{guoCertifiedDataRemoval2020}] \label{guo_noise_bound}
    Let A be the learning algorithm that returns the unique optimum of the perturbed loss $\mathcal{L}(\theta, D) + \textbf{b}^T\theta$, and let M be the an unlearning mechanism that outputs $\theta^-$. Suppose that $||\nabla\mathcal{L}(\theta^-, D')||_2 \leq \epsilon'$ for some $\epsilon' > 0$. Then, we have the following guarantees for M:

    \begin{enumerate}
        \item If \textbf{b} is drawn from a distribution with density $p(\textbf{b}) \propto \exp{-\frac{\epsilon}{\epsilon'}||\textbf{b}||_2}$, then M is $\epsilon$-unlearnable for A;
        \item If $\textbf{b} \sim N(0, k\epsilon'/\epsilon)^d$ with k $>$ 0, then M is $(\epsilon, \delta)$-unlearnable for A with $\delta = 1.5\exp(-k^2/2)$
    \end{enumerate}
\end{lemma}

% \subsubsection{Proof Statement:}

\textit{Proof of Theorem~\ref{thm:eps_delta}:}

By combining Lemmas~\ref{lem:bounded_gradient} and \ref{guo_noise_bound}, we show that the loss gradient over the remaining dataset, evaluated at the unlearned parameter $\theta^-$ is bounded, and we can directly plug this into Lemma~\ref{guo_noise_bound} to achieve $(\delta, \epsilon)$-unlearning for some $\delta, \epsilon > 0$. \hfill $\square$

\subsection{Proof of Theorem~\ref{thm:fairnesG_bound}}

We begin with a useful corollary of Lemma \ref{lem:bounded_gradient}:

\begin{corollary}[$||\theta^-_{D'} - \theta^*_{D'}||_2$ is bounded.] \label{corr1}
Recall $\mathcal{L}(\theta, D')$ is $(n-1)\lambda$-strongly convex. Then, by strong convexity,
\begin{align}
    (n-1)\lambda||\theta_{D'}^- - \theta_{D'}^*||_2^2 &\leq (\nabla \mathcal{L}(\theta_{D'}^-, \mathcal{D'}) - \nabla\mathcal{L}(\theta_{D'}^*, \mathcal{D'})^T(\theta_{D'}^- - \theta_{D'}^*), \nonumber \\
    &= (\nabla \mathcal{L}(\theta_{D'}^-, \mathcal{D'}))^T(\theta_{D'}^- - \theta_{D'}^*), \nonumber \\
    &\leq ||\nabla\mathcal{L}(\theta_{D'}^-, \mathcal{D'})||_2 ||\theta_{D'}^- - \theta_{D'}^*||_2, \nonumber \\
    (n-1)\lambda||\theta_{D'}^- - \theta_{D'}^*||_2 &\leq  \frac{\psi}{\lambda^2(n-1)}(2g + ||\theta^*||_2\left|\left|\frac{8(n-1)}{n_a^2} \right|\right|_2)^2, \nonumber \\
    ||\theta_{D'}^- - \theta_{D'}^*||_2 &\leq \frac{\psi}{\lambda^3(n-1)^2}(2g + ||\theta^*||_2\left|\left|\frac{8(n-1)}{n_a^2} \right|\right|_2)^2
\end{align}
\end{corollary}

Next, we introduce the lemmas we will use in this proof.

\begin{lemma}[Claim B.1 in \citet{neelDescenttoDeleteGradientBasedMethods2020a}] \label{neellemma}
Suppose $\mathcal{L} : \Theta \rightarrow \mathbb{R}$ is $m$-strongly convex and let $\theta^* = \arg \min_{\theta \in \Theta} \mathcal{L}(\theta)$. Then, we have that for any $\theta \in \Theta$, $\mathcal{L}(\theta) \geq \mathcal{L}(\theta^*) + \frac{m}{2}||\theta-\theta^*||_2^2$.
\end{lemma}

Recall our definition of $\ell$, and $\ell_\textrm{fair}$. In addition we introduce a new term $\ell_\textrm{BCE}$ which includes $\ell$ and the $\ell_2$ regularization term.

\begin{align}
    \ell_\textrm{BCE} &= \ell(\theta, x, y) + \frac{2\lambda}{n}||\theta||_2^2 \\
    \ell_\textrm{fair}(\theta, \{(x_m, y_m)\}_{m \in \{i,j,k,l\}}) &= \mathbbm{1}[y_i = y_j](\theta^Tx_i - \theta^Tx_j)\mathbbm{1}[y_k = y_l](\theta^Tx_k - \theta^Tx_l).
\end{align}

\begin{lemma}[$||\theta_{D}^* - \theta_{D'}^-||_2$ is bounded.] \label{boundedness}
Assume both $\ell_\textrm{BCE}$ and $\ell_\textrm{fair}$ are $L$-Lipschitz. Suppose that for all $x \in \mathcal{X} \in \mathbb{R}^d$,  $||x_i||_2 \leq 1$. Then,
    \begin{align}
       ||\theta_{D}^* - \theta_{D'}^-||_2 &\leq ||\theta_{D}^* - \theta_{D'}^*||_2 + ||\theta_{D'}^* - \theta_{D'}^-||_2 \\
       &\leq \frac{L(\frac{1}{n} + \gamma \frac{|C|}{|N|})+\sqrt{L(\frac{1}{n} + \gamma \frac{|C|}{|N|})^2+2n\lambda\frac{2}{n_a}(1+\frac{\lambda}{2})||\thetaretrain||_2}}{n\lambda} \nonumber \\
        &+\frac{\psi}{\lambda^3(n-1)^2}(2g + ||\theta^*||_2\left|\left|\frac{8(n-1)}{n_a^2} \right|\right|_2)^2
    \end{align}
\end{lemma}

\textit{Proof:} We first bound $||\theta_{D}^* - \theta_{D'}^-||_2 \leq ||\theta_{D}^* - \theta_{D'}^*||_2 + ||\theta_{D'}^* - \theta_{D'}^-||_2$ using the triangle inequality. From Corollary \ref{corr1} we know the second term in this equation is bounded. We now bound $||\theta_{D}^* - \theta_{D'}^*||_2$ largely following the proof of Lemma 2.12 in \citet{neelDescenttoDeleteGradientBasedMethods2020a}.

WLOG, assume again that we unlearn the last datapoint with index n, and that $n \in G_a$. Let the $N := G_a \times G_b \times G_a \times G_b$ and $C := \{n\} \times G_b \times \{n\} \times G_b$ be the Cartesian products representing the the full set of indices for computing $\ell_\textrm{fair}$ and the removed indices respectively. Then,
% \vspace{-100pt}
\begin{align}
    \mathcal{L}(\thetaretrain, D)& = \frac{1}{n}\sum_{i=1}^n \ell_\textrm{BCE}(\thetaretrain, x_i, y_i) + \gamma\frac{1}{|N|} \sum_{i,j,k,l \in N} \ell_\textrm{fair}(\thetaretrain, \{(x_m, y_m)\}_{m \in \{i,j,k,l\}}), \nonumber \\
    =& \left[ \frac{(n-1)}{n(n-1)}\sum_{i=1}^{n-1} \ell_\textrm{BCE}(\thetaretrain, x_i, y_i) + \frac{1}{n}\ell_\textrm{BCE}(\thetaretrain, x_n, y_n)\right] \nonumber \\
        &+ \gamma\Bigg[\frac{|N\setminus C|}{|N||N\setminus C|}\sum_{i,j,k,l\in N\setminus C}\ell_\textrm{fair}(\thetaretrain, \{(x_m, y_m)\}_{m \in \{i,j,k,l\}})\nonumber\\ 
        &+ \frac{1}{|N|}\sum_{i,j,k,l \in C} \ell_\textrm{fair}(\thetaretrain, \{(x_m, y_m)\}_{m \in \{i,j,k,l\}})\Bigg], \nonumber \\
    =& \left[ \frac{(n-1)}{n(n-1)}\sum_{i=1}^{n-1} \ell_\textrm{BCE}(\thetaretrain, x_i, y_i) + \frac{1}{n}\ell_\textrm{BCE}(\thetaretrain, x_n, y_n)\right] \nonumber \\
        &+ \gamma\Bigg[\frac{|N\setminus C|}{|N||N\setminus C|}\sum_{i,j,k,l\in N\setminus C}\ell_\textrm{fair}(\thetaretrain, \{(x_m, y_m)\}_{m \in \{i,j,k,l\}}) \nonumber \\ 
        &+ \frac{1}{|N|}\sum_{i,j,k,l \in C} \ell_\textrm{fair}(\thetaretrain, \{(x_m, y_m)\}_{m \in \{i,j,k,l\}})\Bigg] \nonumber \\
        &+ (\frac{|N\setminus C|}{|N|}-\frac{|N\setminus C|}{|N|})\frac{1}{n-1}\sum_{i=1}^{n-1} \ell_\textrm{BCE}(\thetaretrain, x_i, y_i), \nonumber \\
    =& \frac{|N\setminus C|}{|N|}\Bigg[ \frac{1}{n-1}\sum_{i=1}^{n-1} \ell_\textrm{BCE}(\thetaretrain, x_i, y_i) \nonumber \\
    &+ \gamma\frac{1}{|N\setminus C|}\sum_{i,j,k,l\in N\setminus C}\ell_\textrm{fair}(\thetaretrain, \{(x_m, y_m)\}_{m \in \{i,j,k,l\}})\Bigg] \nonumber \\
        &+ \frac{1}{n}\ell_\textrm{BCE}(\thetaretrain, x_n, y_n) + \gamma\frac{1}{|N|}\sum_{i,j,k,l \in C} \ell_\textrm{fair}(\thetaretrain, \{(x_m, y_m)\}_{m \in \{i,j,k,l\}}) \nonumber \\
        &+ (\frac{n-1}{n}-\frac{|N\setminus C|}{|N|})\frac{1}{n-1}\sum_{i=1}^{n-1} \ell_\textrm{BCE}(\thetaretrain, x_i, y_i). 
\end{align} \\ 
Observe this first quantity is equal to $\mathcal{L}(\thetaretrain, D')$. We know by convexity that $\thetaretrain$ is the minimizing solution for this quantity, so this is a lower bound on any other value for $\theta$ such as $\thetafull$.
\begin{align}
    =& \frac{|N\setminus C|}{|N|}\mathcal{L}(\thetaretrain, D') \nonumber \\
        &+ \frac{1}{n}\ell_\textrm{BCE}(\thetaretrain, x_n, y_n)+\gamma\frac{1}{|N|}\sum_{i,j,k,l \in C} \ell_\textrm{fair}(\thetaretrain, \{(x_m, y_m)\}_{m \in \{i,j,k,l\}}) \nonumber \\
        &+(\frac{n-1}{n}-\frac{|N\setminus C|}{|N|})\frac{1}{n-1}\sum_{i=1}^{n-1} \ell_\textrm{BCE}(\thetaretrain, x_i, y_i) \nonumber \displaybreak \\
    \leq& \frac{|N\setminus C|}{|N|}\mathcal{L}(\thetafull, D') \nonumber \\
        &+ \frac{1}{n}\ell_\textrm{BCE}(\thetaretrain, x_n, y_n)+\gamma\frac{1}{|N|}\sum_{i,j,k,l \in C} \ell_\textrm{fair}(\thetaretrain, \{(x_m, y_m)\}_{m \in \{i,j,k,l\}}) \nonumber \\
        &+ (\frac{n-1}{n}-\frac{|N\setminus C|}{|N|})\frac{1}{n-1}\sum_{i=1}^{n-1} \ell_\textrm{BCE}(\thetaretrain, x_i, y_i) \nonumber \\
    =& \frac{|N\setminus C|}{|N|}\left[\frac{1}{n-1} \sum_{i=1}^{n-1} \ell_\textrm{BCE}(\thetafull, x_i, y_i) + \gamma\frac{1}{|N\setminus C|}\ell_\textrm{fair}(\thetafull, \{(x_m, y_m)\}_{m \in \{i,j,k,l\}}) \right] \nonumber \\
        &+ \frac{1}{n}\ell_\textrm{BCE}(\thetaretrain, x_n, y_n)+\gamma\frac{1}{|N|}\sum_{i,j,k,l \in C} \ell_\textrm{fair}(\thetaretrain, \{(x_m, y_m)\}_{m \in \{i,j,k,l\}}) \nonumber \\
        &+ (\frac{n-1}{n}-\frac{|N\setminus C|}{|N|})\frac{1}{n-1}\sum_{i=1}^{n-1} \ell_\textrm{BCE}(\thetaretrain, x_i, y_i) \nonumber \\
    =& \frac{|N\setminus C|}{|N|}\frac{1}{n-1}\sum_{i=1}^{n-1} \ell_\textrm{BCE}(\thetafull, x_i, y_i) + \gamma\frac{1}{|N|}\sum_{i,j,k,l\in N\setminus C}\ell_\textrm{fair}(\thetafull, \{(x_m, y_m)\}_{m \in \{i,j,k,l\}}) \nonumber \\
        &+ \frac{1}{n}\ell_\textrm{BCE}(\thetaretrain, x_n, y_n)+\gamma\frac{1}{|N|}\sum_{i,j,k,l \in C} \ell_\textrm{fair}(\thetaretrain, \{(x_m, y_m)\}_{m \in \{i,j,k,l\}}) \nonumber \\
        &+ (\frac{n-1}{n}-\frac{|N\setminus C|}{|N|})\frac{1}{n-1}\sum_{i=1}^{n-1} \ell_\textrm{BCE}(\thetaretrain, x_i, y_i)
\end{align}

Let $|G_a| = n_a$, and $|G_b| = n_b$. We show that when $n_a \geq \sqrt{n}$, then $\frac{n-1}{n} \leq \frac{|N\setminus C|}{|N|}$. Observe $|N| = n_a^2 n_b^2, |C| = n_b^2$, and that $|N\setminus C| = (n_a-1)^2n_b^2$. We claim the following:

\begin{align}
    \frac{n-1}{n} &\geq \frac{|N\setminus C|}{|N|}, \nonumber \\
    \frac{n-1}{n} &\geq \frac{(n_a-1)^2n_b^2}{n_a^2n_b^2}, \nonumber \\
    \frac{n-1}{n} &\geq \frac{n_a^2-2n_a+1}{n_a^2}, \nonumber \\ 
    1-\frac{1}{n} &\geq 1 - \frac{2}{n_a} + \frac{1}{n_a^2}. \label{eqn:fraction_ineq}
    % \frac{2}{n_a} - \frac{1}{n_a^2} &\geq \frac{1}{n} \\
    % n &\geq \frac{n_a^2}{2n_a-1} > \frac{n_a^2}{2n-1} \\
    % 2n^2 - n &\geq n_a^2
\end{align}

Observe that $1 \leq n_a \leq n$ always. Then, \eqref{eqn:fraction_ineq} is always true. We plug this in to the first term in our equation:

\begin{align}
    =&  \frac{|N\setminus C|}{|N|}\frac{1}{n-1}\sum_{i=1}^{n-1} \ell_\textrm{BCE}(\thetafull, x_i, y_i) + \gamma\frac{1}{|N|}\sum_{i,j,k,l\in N\setminus C}\ell_\textrm{fair}(\thetafull, \{(x_m, y_m)\}_{m \in \{i,j,k,l\}}) \nonumber \\
        &+ \frac{1}{n}\ell_\textrm{BCE}(\thetaretrain, x_n, y_n)+\gamma\frac{1}{|N|}\sum_{i,j,k,l \in C} \ell_\textrm{fair}(\thetaretrain, \{(x_m, y_m)\}_{m \in \{i,j,k,l\}}) \nonumber \\
        &+ (\frac{n-1}{n}-\frac{|N\setminus C|}{|N|})\frac{1}{n-1}\sum_{i=1}^{n-1} \ell_\textrm{BCE}(\thetaretrain, x_i, y_i), \nonumber \displaybreak \\
    \leq& \frac{n-1}{n}\frac{1}{n-1}\sum_{i=1}^{n-1} \ell_\textrm{BCE}(\thetafull, x_i, y_i) + \gamma\frac{1}{|N|}\sum_{i,j,k,l\in N\setminus C}\ell_\textrm{fair}(\thetafull, \{(x_m, y_m)\}_{m \in \{i,j,k,l\}}) \nonumber \\
        &+ \frac{1}{n}\ell_\textrm{BCE}(\thetaretrain, x_n, y_n)+\gamma\frac{1}{|N|}\sum_{i,j,k,l \in C} \ell_\textrm{fair}(\thetaretrain, \{(x_m, y_m)\}_{m \in \{i,j,k,l\}}) \nonumber \\
        &+ (\frac{n-1}{n}-\frac{|N\setminus C|}{|N|})\frac{1}{n-1}\sum_{i=1}^{n-1} \ell_\textrm{BCE}(\thetaretrain, x_i, y_i), \nonumber \\
    =& \frac{1}{n}\sum_{i=1}^{n-1} \ell_\textrm{BCE}(\thetafull, x_i, y_i) + \gamma\frac{1}{|N|}\sum_{i,j,k,l\in N\setminus C}\ell_\textrm{fair}(\thetafull, \{(x_m, y_m)\}_{m \in \{i,j,k,l\}}) \nonumber \\
        &+ \frac{1}{n}\ell_\textrm{BCE}(\thetaretrain, x_n, y_n)+\gamma\frac{1}{|N|}\sum_{i,j,k,l \in C} \ell_\textrm{fair}(\thetaretrain, \{(x_m, y_m)\}_{m \in \{i,j,k,l\}}) \nonumber \\
        &+ (\frac{n-1}{n}-\frac{|N\setminus C|}{|N|})\frac{1}{n-1}\sum_{i=1}^{n-1} \ell_\textrm{BCE}(\thetaretrain, x_i, y_i), \nonumber \\
    =& \left[\frac{1}{n}\sum_{i=1}^{n} \ell_\textrm{BCE}(\thetafull, x_i, y_i) - \frac{1}{n} \ell_\textrm{BCE}(\thetafull, x_n, y_n)\right]\nonumber \\
        &+ \left[\gamma\frac{1}{|N|}\sum_{i,j,k,l\in N}\ell_\textrm{fair}(\thetafull, \{(x_m, y_m)\}_{m \in \{i,j,k,l\}}) -\gamma\frac{1}{|N|}\sum_{i,j,k,l\in C}\ell_\textrm{fair}(\thetafull, \{(x_m, y_m)\}_{m \in \{i,j,k,l\}})\right] \nonumber \\
        &+ \frac{1}{n}\ell_\textrm{BCE}(\thetaretrain, x_n, y_n)+\gamma\frac{1}{|N|}\sum_{i,j,k,l \in C} \ell_\textrm{fair}(\thetaretrain, \{(x_m, y_m)\}_{m \in \{i,j,k,l\}}) \nonumber \\
        &+ (\frac{n-1}{n}-\frac{|N\setminus C|}{|N|})\frac{1}{n-1}\sum_{i=1}^{n-1} \ell_\textrm{BCE}(\thetaretrain, x_i, y_i), \nonumber \\
    =& \mathcal{L}(\thetafull, D) + \frac{1}{n}\left(\ell_\textrm{BCE}(\thetaretrain, x_n, y_n)-\ell_\textrm{BCE}(\thetafull, x_n, y_n)\right) \nonumber\\
        &+ \gamma\frac{1}{|N|}\left(\sum_{i,j,k,l \in C} \ell_\textrm{fair}(\thetafull, \{(x_m, y_m)\}_{m \in \{i,j,k,l\}})-\sum_{i,j,k,l \in C} \ell_\textrm{fair}(\thetaretrain, \{(x_m, y_m)\}_{m \in \{i,j,k,l\}})\right) \nonumber\\
        &+(\frac{n-1}{n}-\frac{|N\setminus C|}{|N|})\frac{1}{n-1}\sum_{i=1}^{n-1} \ell_\textrm{BCE}(\thetaretrain, x_i, y_i). \label{eqn:prelipschitz}
\end{align}

By our assumptions of lipschitzness, we can bound the differences for $\ell_\textrm{BCE}$ and $\ell_\textrm{fair}$ in \eqref{eqn:prelipschitz}.

\begin{align}
    % =& \mathcal{L}(\thetafull, D) + \frac{1}{n}(\ell_\textrm{BCE}(\thetaretrain, x_n, y_n)-\ell_\textrm{BCE}(\thetafull, x_n, y_n)) \nonumber\\
    %     &+ \gamma\frac{1}{|N|}(\sum_{i,j,k,l \in C} \ell_\textrm{fair}(\thetafull, \{(x_m, y_m)\}_{m \in \{i,j,k,l\}})-\sum_{i,j,k,l \in C} \ell_\textrm{fair}(\thetaretrain, \{(x_m, y_m)\}_{m \in \{i,j,k,l\}})) \nonumber\\
    %     &+(\frac{n-1}{n}-\frac{|N\setminus C|}{|N|})\frac{1}{n-1}\sum_{i=1}^{n-1} \ell_\textrm{BCE}(\thetaretrain, x_i, y_i)\\
    \leq& \mathcal{L}(\thetafull, D) + \frac{L}{n}||\thetaretrain - \thetafull||_2 + \gamma \frac{L|C|}{|N|}||\thetaretrain - \thetafull||_2 \nonumber\\
        &+(\frac{n-1}{n}-\frac{|N\setminus C|}{|N|})\frac{1}{n-1}\sum_{i=1}^{n-1} \ell_\textrm{BCE}(\thetaretrain, x_i, y_i), \nonumber \\
    =& \mathcal{L}(\thetafull, D) + L(\frac{1}{n} + \gamma \frac{|C|}{|N|})||\thetaretrain - \thetafull||_2 \nonumber \\
    &+ (\frac{n-1}{n}-\frac{|N\setminus C|}{|N|})\frac{1}{n-1}\sum_{i=1}^{n-1} \ell_\textrm{BCE}(\thetaretrain, x_i, y_i). \label{preremainderbound}
\end{align}

We examine the final term and show it is vanishingly small. Recall our assumption $||x_i||_2 \leq 1$. Then we can bound $\ell_\textrm{BCE}(\theta, x_i, y_i) = \ell(\theta, x_i, y_i) + \frac{\lambda}{2n}||\theta||_2 \leq (1 + \frac{\lambda}{2})||\theta||_2$.

\begin{align*}
    (\frac{n-1}{n}-\frac{|N\setminus C|}{|N|})\frac{1}{n-1}\sum_{i=1}^{n-1} \ell_\textrm{BCE}(\thetaretrain, x_i, y_i)
    \leq& (\frac{n-1}{n}-\frac{|N\setminus C|}{|N|})||\thetaretrain||_2, \\
    =& (\frac{n-1}{n}-\frac{|N\setminus C|}{|N|})(1 + \frac{\lambda}{2})||\thetaretrain||_2, \\
    =& (\frac{n-1}{n}-\frac{(n_a-1)^2n_b^2}{n_a^2n_b^2})(1 + \frac{\lambda}{2})||\thetaretrain||_2, \\
    =& (\frac{n-1}{n}-\frac{(n_a-1)^2}{n_a^2})(1 + \frac{\lambda}{2})||\thetaretrain||_2, \\
    =& (\frac{n-1}{n}-\frac{(n_a^2-2n_a+1)}{n_a^2})(1 + \frac{\lambda}{2})||\thetaretrain||_2, \\
    =& (\frac{n_a^2(n-1)-(n_a^2-2n_a+1)n}{nn_a^2})(1 + \frac{\lambda}{2})||\thetaretrain||_2, \\
    =& \frac{-n_a^2+2nn_a-n}{nn_a^2}(1 + \frac{\lambda}{2})||\thetaretrain||_2, \\
    \leq& \frac{2}{n_a}(1+\frac{\lambda}{2})||\thetaretrain||_2.
\end{align*}

Again, we assume $n_a, n_b \approx O(n)$, so this term is $O(1/n)$. Substituting this back in to Equation~\eqref{preremainderbound}, we have

\begin{align}
    \mathcal{L}(\thetaretrain, D) \leq \mathcal{L}(\thetafull, D) + L(\frac{1}{n} + \gamma \frac{|C|}{|N|})||\thetaretrain - \thetafull||_2 + \frac{2}{n_a}(1+\frac{\lambda}{2})||\thetaretrain||_2
\end{align}

Combining this with Lemma~\ref{neellemma}, we have:

\begin{align}
    \frac{n\lambda}{2}||\thetaretrain-\thetafull||_2^2 = L(\frac{1}{n} + \gamma \frac{|C|}{|N|})||\thetaretrain - \thetafull||_2 + \frac{2}{n_a}(1+\frac{\lambda}{2})||\thetaretrain||_2
\end{align}

Using the quadratic formula, and that $n, \lambda, n_1 > 0$, we know there is only one valid solution:

\begin{align}
    ||\thetaretrain-\thetafull||_2 = \frac{L(\frac{1}{n} + \gamma \frac{|C|}{|N|})+\sqrt{L(\frac{1}{n} + \gamma \frac{|C|}{|N|})^2+2n\lambda\frac{2}{n_a}(1+\frac{\lambda}{2})||\thetaretrain||_2}}{n\lambda}
\end{align}

Returning to our triangle equality, we have:

\begin{align}
    ||\theta_{D}^* - \theta_{D'}^-||_2 &\leq ||\theta_{D}^* - \theta_{D'}^*||_2 + ||\theta_{D'}^* - \theta_{D'}^-||_2  \nonumber\\
    &\leq \frac{L(\frac{1}{n} + \gamma \frac{|C|}{|N|})+\sqrt{L(\frac{1}{n} + \gamma \frac{|C|}{|N|})^2+2n\lambda\frac{2}{n_a}(1+\frac{\lambda}{2})||\thetaretrain||_2}}{n\lambda} \nonumber \\
        &+\frac{\psi}{\lambda^3(n-1)^2}(2g + ||\theta^*||_2\left|\left|\frac{8(n-1)}{n_a^2} \right|\right|_2)^2. && \square
\end{align}

% \textit{Proof Sketch:} We begin by bounding the region of disagreement between the two classifiers. Then, under the assumption that the conditional probability measures $P(\cdot \mid S=a, Y=y), P(\cdot \mid S=a, Y=y)$ are relatively well-behaved (their probability is not overly concentrated in any region), we can bound the difference in these measures for $\thetafull$, $\thetaunlearn$. 

\begin{lemma}[The interior angle between $\thetafull$ and $\thetaunlearn$ is bounded.]\label{boundedangle}
    Let $||\thetafull - \thetaunlearn||_2 \leq \kappa$. Define the angle between $\thetafull$ and $\thetaunlearn$ as $\phi$. Then,
    \begin{align}
        \phi &\leq \arccos\left(\frac{||\thetafull||_2^2 + ||\thetaunlearn||_2^2 - \kappa^2}{2||\thetafull||_2||\thetaunlearn||_2}\right), \nonumber \\
        &\leq \arccos\left(\frac{||\thetafull||_2^2 + (||\thetafull||_2 - \kappa)^2 - \kappa^2}{2||\thetafull||_2(||\thetafull||_2+\kappa)}\right)\nonumber\\
        &= \arccos\left(\frac{||\thetafull||_2 - \kappa}{||\thetafull||_2+\kappa}\right)
    \end{align}
\end{lemma}

The first inequality follows directly from applying the cosine angle formula to the triangle described by $||\thetafull||_2$, $||\thetaunlearn||_2$, and $||\thetafull-\thetaunlearn||_2$ and the fact that $||\thetafull-\thetaunlearn||_2 \leq \kappa$. Then, we use the triangle inequality to show 
\begin{align}
    ||\thetaunlearn||_2 \leq ||\thetafull||_2 + ||\thetaunlearn-\thetafull||_2 \leq ||\thetafull||_2 + \kappa, \nonumber \\
    ||\thetaunlearn||_2 \geq ||\thetafull||_2 - ||\thetafull-\thetaunlearn||_2 \geq ||\thetafull||_2 - \kappa. \nonumber
\end{align}
This lemma implies that as $\kappa$ approaches $0$, the angle $\phi$ approaches zero as well.

\begin{lemma}[Volume of internal segment of a d-sphere]\label{boundedvolume}
    Let $S$ be a $d$-sphere with radius 1 and volume $V = \frac{\pi^{d/2}}{\Gamma(\frac{d}{2}+1)}$. Then, let $\theta_1$, $\theta_2$ be normal vectors defining two hyperplanes such that their intersection lies tangent to the surface of $S$ with internal angle $\phi$ such that the angle bisector to $\phi$ is parallel to the diameter of $S$. Then, the volume of the region of intersection $B$ between the half spaces defined by the hyperplanes at $\phi$ within $S$ equals
    \begin{align}
        B &= V - 2\int_{\mu}^{1}\frac{\pi^{\frac{d-1}{2}}}{\Gamma(\frac{d+1}{2})}(1-y^2)^{d-1} dy, \nonumber \\
        \mu &= \frac{1}{2}\sin{\frac{\phi}{2}}. \nonumber
    \end{align}
\end{lemma}

We take the full volume $V$ and subtract off the volume of the spherical segment defined as the integral of the volume for a $d-1$-sphere over various radius values along the hyperplane to the surface of the sphere. Note that as $\phi \rightarrow 0, \mu \rightarrow 0$, and therefore $B \rightarrow 0$ as the right term approaches $V$.

\textit{Proof of Theorem~\ref{thm:fairnesG_bound}:}

Let $\thetafull$ have a mean absolute equalized odds upper bounded by $\alpha$, or

\begin{align}
    AEOD(\thetafull) \leq \alpha.
\end{align}

This implies

\begin{align}
    |\Pr(h_{\thetafull}(X) = 1 \mid S=a,Y=1)-\Pr(h_{\thetafull}(X) = 1 \mid S=b,Y=1)| \leq \alpha, \\
    |\Pr(h_{\thetafull}(X) = 1 \mid S=a,Y=0)-\Pr(h_{\thetafull}(X) = 1 \mid S=b,Y=0)| \leq \alpha. 
\end{align}

Define $A_{\thetafull} = \{x : h_{\thetafull}(x) = 1\}, A_{\thetaunlearn} = \{x : h_{\thetaunlearn}(x) = 1\}$. Then,

\begin{align}
    |\Pr(A_{\thetafull} \mid S=a,Y=1)-\Pr(A_{\thetafull} \mid S=b, Y=1)| \leq \alpha, \\
    |\Pr(A_{\thetafull} \mid S=a,Y=0)-\Pr(A_{\thetafull} \mid S=b,Y=0)| \leq \alpha. 
\end{align}

Now, define the regions of agreement as $A = A_{\thetafull} \cap A_{\thetaunlearn}, B = A_{\thetafull}^C \cap A_{\thetaunlearn}^C$ and the regions of disagreement as $C = A_{\thetafull}^C \cap A_{\thetaunlearn}, D = A_{\thetafull} \cap A_{\thetaunlearn}^C$. These events are disjoint, so we can decompose our conditional probability measures in to sums over these events and express $\Pr(A_{\thetaunlearn}\mid S,Y)$ in terms of $\Pr(A_{\thetafull}\mid S,Y)$

\begin{align}
    \Pr(A_{\thetafull}\mid S,Y) = \Pr(A\mid S,Y) + \Pr(D\mid S,Y), \nonumber\\
    \Pr(A_{\thetaunlearn}\mid S,Y) = \Pr(A\mid S,Y) + \Pr(C\mid S,Y), \nonumber\\
    \Pr(A_{\thetaunlearn}\mid S,Y) = \Pr(A_{\thetafull}\mid S,Y) + \Pr(C\mid S,Y) - \Pr(D\mid S,Y).
\end{align}

Then, we can bound the absolute difference:

\begin{align}
    |\Pr(A_{\thetaunlearn}&\mid S=a,Y=1) - \Pr(A_{\thetaunlearn}\mid S=b,Y=1)| \nonumber\\
    =& |\Pr(A_{\thetafull}\mid S=a,Y=1)-\Pr(A_{\thetafull}\mid S=b,Y=1) \nonumber\\
    &+\Pr(C \mid, S=a, Y=1) - \Pr(C \mid, S=b, Y=1) \nonumber\\
    &- \Pr(D \mid, S=a, Y=1) + \Pr(D \mid, S=b, Y=1)|, \nonumber\\
    \leq& |\Pr(A_{\thetafull}\mid S=a,Y=1)-\Pr(A_{\thetafull}\mid S=b,Y=1) \nonumber\\
    &+ |\Pr(C \mid, S=a, Y=1) - \Pr(C \mid, S=b, Y=1)| \nonumber\\
    &+ |\Pr(D \mid, S=b, Y=1) - \Pr(D \mid, S=a, Y=1)|, \nonumber\\
    \leq& \alpha + |\Pr(C \mid, S=a, Y=1) - \Pr(C \mid, S=b, Y=1)| \nonumber\\
    &+ |\Pr(D \mid, S=b, Y=1) - \Pr(D \mid, S=a, Y=1)|. \label{basebound}
\end{align}

Next, we relate the probability measures $\Pr(\cdot \mid S, Y)$ to the volume of their event spaces to establish a bound on the second and third terms in Equation~\ref{basebound}. We assume for all events $x \subseteq \mathcal{X}$, $\Pr(x | S, Y) \leq c \frac{|x|}{|\mathcal{X}|}$, or in other words, that our probability mass is relatively well distributed up to a constant. Then, computing $|C|$ and $|D|$ will provide upper bounds on their conditional likelihoods.

Recall our assumption that $||x_i||_2 \leq 1$. This means all of our data lies within a unit $d$-sphere we call $S$, which defines our event space $\mathcal{X}$ and has volume $V = |\mathcal{X}| = \frac{\pi^{d/2}}{\Gamma(\frac{d}{2}+1)}$. We can define events $A_{\thetafull}$ and $A_{\thetaunlearn}$ the halfspaces defined by $\thetafull$ and $\thetaunlearn$ intersected with $S$. Then, $A, B, C, \text{ and} D$ partition $S$ into four disjoint volumes. We seek to bound the volume of $C$ and $D$. First observe that the largest volume for $C + D$ occurs when $\thetafull, \thetaunlearn$ intersect tangent to $S$ and create a region where the angle between $\thetafull$ and $\thetaunlearn$ is bisected by a line parallel to the diameter of $S$. This creates a region where one of $C,D$ is empty and the other draws a large slice through $S$. Define the angle between $\thetafull$ and $\thetaunlearn$ as $\phi$. Without loss of generality, assume $|D| = 0$. Then, by Lemmas~\ref{boundedangle} and~\ref{boundedvolume}, if $||\thetafull - \thetaunlearn||_2 \leq d$, then $\phi$ is bounded and thus $|C|$ is bounded.

\begin{align}
    |C| &= V - 2\int_{\mu}^{1}\frac{\pi^{\frac{d-1}{2}}}{\Gamma(\frac{d+1}{2})}(1-y^2)^{d-1} dy,
    \\
    \phi &\leq \arccos\left(\frac{||\thetafull||_2 - \kappa}{||\thetafull||_2+\kappa}\right),
\end{align}
where
\begin{align}
    \mu &= \frac{1}{2}\sin{\frac{\phi}{2}}, \nonumber \\
    &= \frac{1}{2}\sqrt{\frac{1-\cos{\phi}}{2}}, \nonumber \\
    &\leq \frac{\sqrt{\kappa}}{2}.
\end{align}
by the double angle formula for cosine.

Returning to Equation~\ref{basebound}, we have

\begin{align}
    |\Pr(A_{\thetaunlearn}&\mid S=a,Y=1) - \Pr(A_{\thetaunlearn}\mid S=b,Y=1)| \nonumber \\
    &\leq \alpha + |\Pr(C \mid, S=a, Y=1) - \Pr(C \mid, S=b, Y=1)| \nonumber\\
    &+ |\Pr(D \mid, S=b, Y=1) - \Pr(D \mid, S=a, Y=1)|, \nonumber \\
    &\leq \alpha + \max(\Pr(C \mid, S=a, Y=1), \Pr(C \mid, S=b, Y=1)) \nonumber \\
    &+ \max(\Pr(D \mid, S=b, Y=1), \Pr(D \mid, S=a, Y=1)), \nonumber \\
    &\leq \alpha + c\frac{|C|}{|\mathcal{X}|} + c\frac{|D|}{|\mathcal{X}|}, \nonumber \\
    &\leq \alpha + c\frac{V- 2\int_{\mu}^{1}\frac{\pi^{\frac{d-1}{2}}}{\Gamma(\frac{d+1}{2})}(1-y^2)^{d-1} dy}{V}, \nonumber \\
    &= \alpha + c\left(1- \frac{2\int_{\mu}^{1}\frac{\pi^{\frac{d-1}{2}}}{\Gamma(\frac{d+1}{2})}(1-y^2)^{d-1} dy}{V}\right),
\end{align}

where 

\begin{align}
    \mu \leq \frac{\sqrt{\kappa}}{2}. \nonumber
\end{align}

By a symmetric argument, we also see

\begin{align}
    |\Pr(A_{\thetaunlearn}&\mid S=a,Y=0) - \Pr(A_{\thetaunlearn}\mid S=b,Y=0)| \leq \alpha + c\left(1- \frac{2\int_{\mu}^{1}\frac{\pi^{\frac{d-1}{2}}}{\Gamma(\frac{d+1}{2})}(1-y^2)^{d-1} dy}{V}\right).
\end{align}

And thus we have that the absolute mean equalized odds of $\thetaunlearn$ is bounded:

\begin{align}
    \frac{1}{2}\big[|\Pr(A_{\thetaunlearn}&\mid S=a,Y=1) - \Pr(A_{\thetaunlearn}\mid S=b,Y=1)| \nonumber \\
    &+ |\Pr(A_{\thetaunlearn}\mid S=a,Y=1) - \Pr(A_{\thetaunlearn}\mid S=b,Y=1)|\big], \nonumber \\
    &\leq \alpha + c\left(1- \frac{2\int_{\mu}^{1}\frac{\pi^{\frac{d-1}{2}}}{\Gamma(\frac{d+1}{2})}(1-y^2)^{d-1} dy}{V}\right), 
\end{align}

where

\begin{align}
    h \leq \frac{\sqrt{\kappa}}{2}. \nonumber
\end{align}

% We can view the decision regions of our classifiers as half spaces, and the regions of disagreement as intersections of half spaces with angle $\phi$ extending from the point of intersection between $\thetafull$ and $\thetaunlearn$. Furthermore, with our assumption $||x_i||_2 \leq 1$, these regions are bounded by a unit $d$-sphere. The maximal region of disagreement within the unit $d$-sphere occur when the hyperplanes tangent to $\thetafull$ and $\thetaunlearn$ intersect on the surface of the sphere, and are aligned such that the internal angle bisector of their intersection is parallel to the diameter of the $d$-sphere.

\subsection{Incompatibility of Existing Unlearning Methods}
\label{sec:incompatible}

Existing unlearning methods require a decomposition of the loss term as $\mathcal{L}(\theta) = \frac{1}{n} \sum_{i=1}^n \ell(\theta)$. For example, the most similar method to Fair Unlearning comes from~\citet{guoCertifiedDataRemoval2020}, and their unlearning step involves a second order approximation over $\Delta = \lambda \thetafull + \nabla\ell(\thetafull, x_n, y_n)$ to unlearn datapoint $(x_n, y_n)$. Similarly to our proof of Lemma~\ref{lem:bounded_gradient}, adding $\Delta$ with the gradient of $\thetaunlearn$ over the remaining dataset $\mathcal{L}(\thetaunlearn, D')$ results in a cancellation that enables the rest of their proof to bound the gradient. However, in their case $\Delta$ is directly corresponds to the gradient over sample $(x_n, y_n)$ only because of the decomposability of their loss term. This behavior makes all unlearning over decomposable loss functions the same: compute a Newton step $H^{-1}\Delta$ where $H$ is the hessian over $D'$ and $\Delta$ is the gradient over $R$. In our case our $\Delta$ is more complex because the fairness loss involves the interactions between each sample and samples in the opposite subgroup. Thus, removing one point requires removing a set of terms from the fairness loss, and thus $\Delta$ must be a more complex term to account for this rather than simply the gradient over $(x_n, y_n)$.

\subsection{Test Accuracy when unlearning at random.}

\begin{table}[h]
\centering
\caption{Test accuracy when unlearning from minority subgroups.}
\label{tab:unlearning from minority}
\resizebox{\linewidth}{!}{%
\begin{tabular}{lcccccc} 
\toprule
                                   & \multicolumn{2}{c}{COMPAS}            & \multicolumn{2}{c}{Adult}             & \multicolumn{2}{c}{HSLS}               \\
Method                             & 5\% Unlearned       & 10\% Unlearned      & 5\% Unlearned       & 10\% Unlearned      & 5\% Unlearned       & 10\% Unlearned       \\ 
\midrule
Full Training (BCE)                & $0.652 \pm 0.000$ & $0.652 \pm 0.000$ & $0.821 \pm 0.000$ & $0.821 \pm 0.000$ & $0.729 \pm 0.000$ & $0.729 \pm 0.000$  \\
Retraining (BCE)                   & $0.651 \pm 0.002$ & $0.652 \pm 0.001$ & $0.821 \pm 0.000$ & $0.822 \pm 0.000$ & $0.729 \pm 0.001$ & $0.730 \pm 0.001$  \\
Newton (\citet{guoCertifiedDataRemoval2020}) & $0.651 \pm 0.002$ & $0.652 \pm 0.001$ & $0.821 \pm 0.000$ & $0.823 \pm 0.000$ & $0.729 \pm 0.001$ & $0.730 \pm 0.002$  \\
SISA (\citet{bourtouleMachineUnlearning2020})   & $0.653 \pm 0.001$ & $0.654 \pm 0.002$ & $0.822 \pm 0.000$ & $0.823 \pm 0.000$ & $0.728 \pm 0.001$ & $0.728 \pm 0.001$  \\
PRU (\citet{izzoApproximateDataDeletion})    & $0.661 \pm 0.007$ & $0.666 \pm 0.007$ & $0.819 \pm 0.004$ & $0.815 \pm 0.035$ & $0.720 \pm 0.004$ & $0.724 \pm 0.003$  \\ 
\midrule
Full Training (Fair)               & $0.653 \pm 0.000$ & $0.653 \pm 0.000$ & $0.812 \pm 0.000$ & $0.812 \pm 0.000$ & $0.723 \pm 0.000$ & $0.723 \pm 0.000$  \\
Retraining (Fair)                  & $0.653 \pm 0.001$ & $0.661 \pm 0.005$ & $0.813 \pm 0.000$ & $0.816 \pm 0.001$ & $0.713 \pm 0.002$ & $0.713 \pm 0.002$  \\
Fair Unlearning (Ours)             & $0.652 \pm 0.001$ & $0.661 \pm 0.004$ & $0.814 \pm 0.000$ & $0.816 \pm 0.002$ & $0.724 \pm 0.001$ & $0.721 \pm 0.001$  \\
\bottomrule
\end{tabular}
}
\end{table}

\begin{table}[h]
\centering
\caption{Test accuracy when unlearning from majority subgroups.}
\label{tab:unlearning from majority}
\resizebox{\linewidth}{!}{%
\begin{tabular}{lcccccc} 
\toprule
                                      & \multicolumn{2}{c}{COMPAS}            & \multicolumn{2}{c}{Adult}             & \multicolumn{2}{c}{HSLS}               \\
Method                                & 5\% Unlearned       & 20\% Unlearned      & 5\% Unlearned       & 20\% Unlearned      & 5\% Unlearned       & 20\% Unlearned       \\ 
\midrule
Full Training (BCE)                & $0.654 \pm 0.000$ & $0.654 \pm 0.000$ & $0.814 \pm 0.000$ & $0.814 \pm 0.000$ & $0.738 \pm 0.000$ & $0.738 \pm 0.000$  \\
Retraining (BCE)            & $0.654 \pm 0.001$ & $0.652 \pm 0.002$ & $0.814 \pm 0.000$ & $0.814 \pm 0.000$ & $0.736 \pm 0.000$ & $0.736 \pm 0.002$  \\
Newton (\citet{guoCertifiedDataRemoval2020})  & $0.654 \pm 0.001$ & $0.652 \pm 0.002$ & $0.814 \pm 0.000$ & $0.814 \pm 0.001$ & $0.736 \pm 0.000$ & $0.735 \pm 0.002$  \\
SISA (\citet{bourtouleMachineUnlearning2020}) & $0.654 \pm 0.002$ & $0.653 \pm 0.004$ & $0.814 \pm 0.000$ & $0.815 \pm 0.001$ & $0.734 \pm 0.001$ & $0.733 \pm 0.002$  \\
PRU (\citet{izzoApproximateDataDeletion})     & $0.696 \pm 0.006$ & $0.688 \pm 0.003$ & $0.812 \pm 0.001$ & $0.811 \pm 0.002$ & $0.723 \pm 0.012$ & $0.731 \pm 0.004$  \\ 
\midrule
Full Training (Fair)                   & $0.666 \pm 0.000$ & $0.666 \pm 0.000$ & $0.815 \pm 0.000$ & $0.815 \pm 0.000$ & $0.725 \pm 0.000$ & $0.725 \pm 0.000$  \\
Retraining (Fair)               & $0.663 \pm 0.004$ & $0.653 \pm 0.002$ & $0.815 \pm 0.000$ & $0.813 \pm 0.000$ & $0.724 \pm 0.001$ & $0.725 \pm 0.002$  \\
Fair Unlearning (Ours)                            & $0.663 \pm 0.004$ & $0.653 \pm 0.003$ & $0.815 \pm 0.000$ & $0.814 \pm 0.001$ & $0.724 \pm 0.001$ & $0.725 \pm 0.002$  \\
\bottomrule
\end{tabular}
}
\end{table}

\subsection{Figures for various fairness metrics.}

Below we report figures for Demographic Parity, Equality of Opportunity, and Subgroup Accuracy. These results reflect the results shown in the main body of the paper, with the exception of Subgroup Accuracy which has more variable results, and sometimes worse results for fair methods. However, this is because our fairness regularizer optimizes for group fairness metrics such as Equalized Odds, Equality of Opportunity, and Demographic Parity which are metrics based on individual true and false positive rates rather than the raw test accuracy of each subgroup. 

% Figure environment removed

% Figure environment removed

% Figure environment removed