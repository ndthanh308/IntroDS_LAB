%%%%%%%%%%%%%% Alex's original version %%%%%%%%%%%%%%

% Recent technology policies such as the European Union's General Data Protection Regulation (GDPR) have stipulated the "Right to be Forgotten," which protects a consumer's right to request their data be deleted from private databases and models. The most straightforward approach to the data deletion from models, or machine unlearning, is to retrain the model without the removed data. However, it is often impractical to retrain a model for each removal request due computational cost. Consequently, a body of work on \emph{machine unlearning} has emerged in recent years, focusing on developing efficient approximations to retraining. While these methods are accurate, they are designed to approximate retraining over simple ERM loss functions. However, it is unclear whether they maintain performance when a model is optimized to protect for other user rights, such as the "Right to Fair Treatment." In this work, we propose a novel unlearning algorithm that preserves fairness while remaining provably unlearnable. We provide theoretical guarantees which demonstrate that the proposed algorithm unlearns while also mitigating disparity an report extensive evaluation on real-world datasets which demonstrate the efficacy of our approach. \\

%%%%%%%%%%%%%% Alex's original version %%%%%%%%%%%%%%

% \hima{Here is a rephrasing of the abstract to tidy it up and make it more reviewer friendly (writing it to save time as we are close to deadline -- feel free to use pieces or whole): "Right to be Forgotten is one of the key principles outlined by regulatory frameworks such as the General Data Protection Regulation (GDPR). This principle allows individuals to request their personal data and information to be deleted from the databases and models of organizations. To operationalize this principle in practice, a slew of techniques commonly referred to as \emph{machine unlearning} methods have been proposed in recent literature. While these methods provide efficient alternatives to retraining models from scratch in the face of data deletion requests, it is unclear how they impact other key properties critical to real-world applications such as fairness as they do not explicitly account for these properties. In this work, we address the aforementioned gaps by proposing the first \emph{fair machine unlearning} method that can provably and efficiently unlearn data instances while preserving group fairness. We also derive theoretical results which demonstrate that our method can provably unlearn data instances without exacerbating inequity. Extensive experimentation with real-world datasets demonstrates the efficacy of our method at unlearning data instnaces while preserving fairness. 
% %they do not explicitly account for other key properties critical to real-world applications such as fairness. Furthermore, there is little to no work that explores how "
% }
% The \emph{Right to be Forgotten}, as prescribed by regulatory policies such as the General Data Protection Regulation (GDPR), allows data subjects to request the deletion of their data from private databases and models. In response, several \emph{machine unlearning} methods have been developed to efficiently approximate retraining for models originally trained on \hima{models optimized for empirical risk minimization? accuracy based objective functions is sounding a bit weird either say models optimized solely for predictive accuracy or say models trained by minimizing empirical risk.} accuracy-based objective functions. However, it is unclear whether such unlearning methods are good approximations for auxiliary optimization goals such as fairness. In this work, we first show that existing unlearning methods are not applicable to fair machine learning objectives. To the best of our knowledge, we propose the first fair machine unlearning method that can provably and efficiently unlearn requested data points while preserving group fairness. Then we prove theoretically that our method satisfies unlearning without exacerbating disparities. Finally, we conduct extensive experiments on real-world datasets that verify the efficacy of our approach and show that our method efficiently approximates retraining in a variety of settings. \hima{i dont think we have any experiments on "efficient approximation of retraining". So, we should not make that claim!}

% \emph{Machine unlearning}, the problem of efficiently erasing the influence of certain training data points on the machine learning model, has become an important research topic.
% As corporate collection and usage of personal data continues to grow, 
% it is unclear whether these organizations will curate said data to protect consumers. 
% there is an increased risk that organizations will misuse data and violate the rights of consumers.
% it is unclear whether organizations curate their data with the public's data interests in mind. 
% Further, increasing media coverage and awareness of data-driven AI systems have established a consumer consciousness surrounding data protection. 
As public consciousness regarding the collection and use of personal information by corporations grows, it is of increasing importance that consumers be active participants in the curation of corporate datasets. In light of this, data governance frameworks such as the General Data Protection Regulation (GDPR) have outlined the \emph{right to be forgotten} as a key principle allowing individuals to request that their personal data be deleted from the databases and models used by organizations. To achieve forgetting in practice, several \emph{machine unlearning} methods have been proposed to address the computational inefficiencies of retraining a model from scratch with each unlearning request. 
%While these methods provide efficient alternatives to retraining models from scratch in the face of data deletion requests, 
While efficient online alternatives to retraining, it is unclear how these methods impact other properties critical to real-world applications, such as fairness.
%as they do not explicitly account for these properties. 
In this work, we propose the first \emph{fair machine unlearning} method that can provably and efficiently unlearn data instances while preserving group fairness. We derive theoretical results which demonstrate that our method can provably unlearn data instances while maintaining fairness objectives. Extensive experimentation with real-world datasets highlight the efficacy of our method at unlearning data instances while preserving fairness. 



% However, it is unclear whether such unlearning methods are applicable when a model is calibrated to safeguard other user rights, such as the \emph{Right to Fair Treatment}.
%%%%%%%%%%%%%%%%%% Cut sentences from Jiaqi's Draft %%%%%%%%%%%%%%%%%%%%

% \hima{I think we should not talk about privacy in the abstract or intro of this work much (its ok to discuss nuances and connections with privacy in methods/theory/experiments sections). Let's focus on unlearning and its potential trade-offs with model fairness which have not been investigated. 

% Abstract should have the following:
% 1. Start with right to be forgotten which requires data to be deleted both from databases as well as models per customer requests.
% 2. Since it is impossible to retrain several models each time a data deletion request comes in, the literature on unlearning started which focuses on making small gradient updates which guarantee forgetfulness. 
% 3. However, it is unclear if such approximations preserve key properties such as fairness relative to complete retraining. This question has not been explored before. 
% 4. In this work, we investigate if unlearning algorithms exacerbate unfairness. To this end, we carry out extensive empirical analysis with SOTA unlearning algorithms to find that these algorithms in fact result in more unfair predictions than complete retraining. 
% 5. Further, we propose an unlearning algorithm which preserves fairness <a few details about this algorithm>. We provide theoretical guarantees which demonstrate that the proposed algorithm not only unlearns effectively but also preserves fairness relative to total retraining.
% 6. Extensive empirical evaluation with multiple synthetic and real-world datasets demonstrates the efficacy of our approach. 
% }

 
% The field of unlearning seeks to develop efficient approximations to retraining while theoretically guaranteeing no information about the removed sample remains. We show that these approximations can come at the cost of fair treatment, highlighting a tension between \hima{highlighting a tension between unlearning and model fairness} various user rights and the challenge of implementing unlearning in a practical manner. We then provide a solution that approximates retraining fairly with theoretical guarantees.

% One such protection is the ``right to be forgotten,"  which allows end users to ask companies to erase their personal data from databases. 
% Over the past decade, ML models have been deployed in several real world applications including healthcare, law, and policy. Due to this immense popularity, there have been regulations which ensure that these models and algorithms benefit end users and do not cause unintended consequences. To this end, GDPR and AI bill of rights put forth the right to be forgotten, which allows end users to ask companies to erase their information from their databases and models. While this right is beneficial to end users, it can become logistically challenging to implement for companies because each request to erasure may lead to retraining several models. The field of unlearning addresses the operational challenge and proposes several algorithms to make incremental updates which can guarantee the erasure of individuals data from the models. As industries in the healthcare, financial, and tech sector begin implementing unlearning in compliance with state policy, it is important to characterize the behavior of unlearning. However, there is little to no work that examines the properties of these algorithms e.g., do these algorithms create or exacerbate disparities in outcomes for certain minority groups? In this work, we make the first attempt at analyzing if unlearning algorithms increase inequity in outcomes. To this end, we propose an evaluation framework which enables us to evaluate if any given unlearning algorithm is increasing inequity in outcomes for minority groups. We then apply this framework to different models and unlearning algorithms to demonstrate that unlearning algorithms may in fact exacerbate inequity in outcomes under certain conditions. We formalize the conditions under which this happens. By doing so, our work not only creates a framework that ensures checks and balances on unlearning algorithms, but also paves the way to develop novel unlearning algorithms which do not suffer from the aforementioned problems.