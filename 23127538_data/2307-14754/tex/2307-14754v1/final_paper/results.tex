% \hima{In this section, we describe our experimental evaluation in detail. First, we simulate the scenario where data instances are deleted at random, and evaluate the effectiveness of our method in this setting. Second, we consider the scenario where instances being deleted belong to specific subgroups, and evaluate our method in this context. }

In this section, we describe our experimental results in detail. First, we evaluate the effectiveness of fair unlearning in terms of both fairness and accuracy in the scenario where data points are deleted at random. Second, we consider more extreme settings for unlearning where data points are deleted disproportionately from a specific subgroup.

% In the following experiments, we explore the efficacy of our fair unlearning method on three real-world datasets. We show that our method preserves fairness while remaining an accurate and efficient approximation to retraining. In addition, we explore worst-case scenarios for unlearning and highlight cases where naively applying previous methods can exacerbate disparities.

\subsection{Experimental Setup}

We evaluate the proposed fair unlearning method on three real-world datasets using a logistic regression model. We simulate three types of data deletion requests and compare various unlearning methods with the brute-force retraining approach in terms of both model accuracy and fairness. We also report the metrics of the original model trained on the full training set as a reference. For all results we repeat our experiments five times and report mean and standard deviation. Note that the effectiveness of erasing the influence of requested data points is guaranteed by our theory rather than evaluated empirically, as is common in prior unlearning literature~\citep{guoCertifiedDataRemoval2020,neelDescenttoDeleteGradientBasedMethods2020a}.

\paragraph{Datasets.} We consider three datasets commonly used in the machine learning fairness literature. 1) \emph{COMPAS}~\citep{angwin2016machine} is a classic fairness benchmark with the task of predicting a criminal defendant's likelihood of recidivism. 2) \emph{Adult} from the UCI Machine Learning Repository~\citep{asuncion2007uci} is a large scale dataset with the task of predicting income bracket. 3) High School Longitudinal Study (\emph{HSLS}) is a dataset containing survey responses from high school students and parents with the task of predicting academic year~\citep{ingels2011high,jeong2022fairness}. For all datasets, we use the individual's race as the sensitive group identity.%, which is commonly studied in the machine learning fairness literature and is also legally protected by regulatory policies such as the Blueprint for AI Bill of Rights~\citep{aibillofrights}.

\paragraph{Data Deletion Settings.} We simulate data deletions in three settings: 1) \emph{random}, where data deletion requests come randomly from the training set independent of subgroup membership; 2) \emph{deletion from the minority group}, where data deletion requests come randomly from the minority group of the training set; 3) \emph{deletion from the majority group}, where data deletion requests come randomly from the majority group of the training set. 
% In all data deletion settings, we vary the number of deletion requests between $1\%$, $5\%$, $10\%$, $15\%$, and $20\%$ of the total training set size.

\paragraph{Evaluation Metrics.} We report experimental results on two metrics, 1) \emph{Test Accuracy} and 2) \emph{Absolute Equalized Odds Difference} (AEOD) as defined in Eq.~(\ref{eq:aeod}), with additional results for Demographic Parity, Equality of Opportunity, and Statistical Parity in the appendix. 



% We use a logistic regression model trained in Pytorch on three datasets commonly used in the fairness literature spanning various real world modalities. First we explore COMPAS, a classic fairness benchmark with the task of predicting a criminal defendant's likelihood of recidivism \cite{angwin2016machine}. This dataset contains sensitive attributes such as a user's race. Next, we explore UCI's Adult dataset, which is a large scale dataset with the task of predicting income bracket, while containing sensitive features such as race \alex{should I include sex if I dont evaluate on it}\cite{asuncion2007uci}. While we recognize the limitations of these datasets \alex{Citation}, both of these are commonly found in fairness and allow us to easily benchmark and position our work in the literature. Finally, we explore the High School Longitudinal Study (HSLS), a dataset in the education sphere containing survey data from students that underrepresents certain ethnic groups \cite{ingels2011high}, \cite{jeong2022fairness}. We acknowledge that there are a variety of sensitive group identities and their intersections, but we focus on race here as it is commonly studied and protected by policy such as the AI Bill of Rights~\citep{aibillofrights}.

% We choose to focus on race as our subgroup attribute 

% When unlearning, we limit our requests to 1\%, 5\%, 10\%, 15\%, and 20\% of the total train set size. In real-world settings, retraining at larger amounts of unlearning requests is more practical because of potential distribution shifts, privacy leakage, and performance concerns. We report results on three metrics: \\
% \textbf{(1) Test Accuracy:} We report test accuracy to ensure methods are not causing a drop in performance during unlearning. \\
% % \textbf{(2) Statistical Parity:} We compare subgroup performance by reporting absolute subgroup accuracy difference, or $|P(h(X) = y | S = a) - P(h(X) = y | S = b)|$ measured over empirical probability estimates on the test set. \\
% \textbf{(2) Absolute Equalized Odds Difference:} We measure equalized odds as defined in Definition \ref{equalizedodds} by taking the following quantity: 

% \begin{align}
%     \frac{1}{2}[|P(h(X) = 1 | Y=1, S = a)-P(h(X) = 1 | Y=1, S = b)| +... \nonumber \\ ...+ |P(h(X) = 1 | Y=0, S = a)-P(h(X) = 1 | Y=0, S = b)|]
% \end{align} 

% measured over empirical probability estimates on the test set. This is the average absolute difference for both false positive rates and true positive rates.

% % Figure environment removed

\paragraph{Baseline Methods.} 
We compare our method with four baselines designed for unlearning with BCE loss (Eq.~(\ref{bceloss})). In addition to brute-force retraining with BCE loss (named as \textbf{Retraining (BCE)}), we consider three unlearning methods from existing literature. The three methods are 1) \textbf{Newton}~\citep{guoCertifiedDataRemoval2020} a second-order approach that takes a single Newton step to approximate retraining; 2) \textbf{PRU}~\citep{izzoApproximateDataDeletion} which uses a statistical technique called the residual method to efficiently approximate the Hessian used in the Newton step; and 3) \textbf{SISA}~\citep{bourtouleMachineUnlearning2020} a method that splits the dataset into shards and trains a model on each shard, while unlearning by retraining each model separately to speed up the process. In addition, we compare our \textbf{Fair Unlearning} method designed for unlearning with fair loss (Eq.~(\ref{fairloss})) to brute-force retraining with fair loss (named as \textbf{Retraining (Fair)}). Finally, as references, we also evaluate models trained on the full training set with BCE loss and fair loss, respectively named as \textbf{Full Training (BCE)} and \textbf{Full Training (Fair)}. Note that the goal of all machine unlearning methods is to approximate retraining.

% We divide our unlearning methods into two groups. The first group is applicable to the BCE loss (Eq.~(\ref{bceloss})) while the second group is applicable to the fair loss (Eq.~(\ref{fairloss})). The first group consists of three existing unlearning methods in the literature as well as the brute-force retraining over the BCE loss (named as \textbf{Retraining (BCE)}). The three unlearning methods are 1) \textbf{Newton}~\citep{guoCertifiedDataRemoval2020} a second-order approach that takes a single Newton step to approximate retraining; 2) \textbf{PRU}~\citep{izzoApproximateDataDeletion} which uses a statistical method called the residual method to efficiently approximate the Hessian used in Newton steps; and 3) \textbf{SISA}~\citep{bourtouleMachineUnlearning2020} a method that splits the dataset into shards and trains a model on each shard, while unlearning by retraining each model separately to speed up the process. The second group consists of our proposed \textbf{Fair Unlearning} method and the brute-force retraining over the fair loss (named as \textbf{Retraining (Fair)}). As references, we also evaluate models trained on the full training set with BCE loss and fair loss, respectively named as \textbf{Full Training (BCE)} and \textbf{Full Training (Fair)}.



% \alex{Probably will cut influence from the graphs as I dont mention it in the main body (and its not actually any specific method in the literature)}

% We begin by training a logistic regression classifier on the entire dataset, denoted by \texttt{fully\_trained}. Then, we benchmark various unlearning methods with the naive and inefficient approach to unlearning, retraining, denoted as \texttt{naive\_retraining}. We then implement various benchmark methods from unlearning literature to explore the potential disparities they introduce. We select representative methods from three distinct approaches, labeled as \texttt{sharding} from Bourtoule et al.'s SISA algorithm \cite{bourtouleMachineUnlearning2020}, \texttt{newton} from Guo et al.'s second order Newton Ralphson approximation \cite{guoCertifiedDataRemoval2020}, and finally \texttt{residual} from Izzo et al.'s projective residual update method \cite{izzoApproximateDataDeletion}. We report the absolute equalized odds difference for these baselines in Figure \ref{fig:baselines}. 

% We observe that most unlearning methods have similar levels of unfairness to retraining, while in certain cases the \texttt{residual} algorithm and \texttt{sharding} algorithm exhibit increased disparity. Overall, however, we see that all of these methods are more unfair than a model trained with a fair loss function, demonstrating the need for our method to allow for both fairness and unlearning. \alex{I need to work on this last sentence. Its our main pitch in this paper, "our method allows for both fairness and unlearning".}



% \begin{table}[h]
%   \caption{Test accuracy when unlearning at random}
%   \label{tab:unlearning at random}
%   \centering
%   \begin{tabular}{lcccccc}
%     \toprule
%     & \multicolumn{2}{c}{COMPAS} & \multicolumn{2}{c}{Adult} & \multicolumn{2}{c}{HSLS} \\
%     % \cmidrule(r){1-2}
%     Method & 5\% Removal & 20\% Removal & 5\% Removal & 20\% Removal & 5\% Removal & 20\% Removal \\
%     \midrule
%     Full Dataset (Vanilla) & $0.652 \pm 0.000$ & $0.652 \pm 0.000 $
%         & $0.817 \pm 0.000$ & $0.816 \pm 0.000$ 
%         & $0.724 \pm 0.000$ & $0.724 \pm 0.000$\\
%     Naive Retraining (Vanilla) & $0.652 \pm 0.001$ & $0.654 \pm 0.003$
%         & $0.817 \pm 0.000$ & $0.817 \pm 0.001$
%         & $0.724 \pm 0.002$ & $0.724 \pm 0.002$\\
%     Newton (\citet{guoCertifiedDataRemoval2020}) & $0.652 \pm 0.001$ & $0.654 \pm 0.003$
%         & $0.817 \pm 0.000$ & $0.817 \pm 0.001$
%         & $0.724 \pm 0.002$ & $0.724 \pm 0.002$\\
%     Full Dataset (Fair) & $0.647 \pm 0.000$ & $0.647 \pm 0.000$
%         & $0.819 \pm 0.000$ & $0.819 \pm 0.000$
%         & $0.713 \pm 0.000$ & $0.713 \pm 0.000$\\
%     Naive Retraining (Fair) & $0.646 \pm 0.002$ & $0.642 \pm 0.003$ 
%         & $0.819 \pm 0.000$ & $0.819 \pm 0.001$
%         & $0.713 \pm 0.002$ & $0.713 \pm 0.002$\\
%     FMU (Ours) & $0.646 \pm 0.002$ & $0.642 \pm 0.003$ 
%         & $0.819 \pm 0.000$ & $0.819 \pm 0.001$
%         & $0.713 \pm 0.002$ & $0.713 \pm 0.002$\\
%     \bottomrule
%   \end{tabular}
% \end{table}

% \subsection{Unlearning at Random}

% \alex{Throughout my results, should I describe the Newton method as "newton" or "Guo et al.'s method"}

% Figure environment removed

\subsection{Experimental Results on Fairness}

\paragraph{Deletion at Random.}
We evaluate our method in terms of fairness when unlearning at random from our training dataset. For each of our datasets, and for both BCE and fair loss, we train a base logistic regression model on the full training dataset. Then, we randomly select $1\%$, $5\%$, $10\%$, $15\%$, and $20\%$ of our training set to unlearn. We compare the AOED of our method to various baselines. Results are reported in Figure~\ref{fig:unlearning at random}. Note that methods computed with BCE loss are shown with solid lines whereas methods computed with fair loss are shown with dashed lines.

% Figure environment removed

Our \textbf{Fair Unlearning }method (green) well-approximates the fairness performance of brute-force retraining with fair loss. In addition, we see that across all levels of unlearning, our method outperforms all baselines by a significant margin in AEOD for COMPAS, with similar results for Adult and HSLS. Note that \textbf{Newton} is the best approximator for \textbf{Retraining (BCE)}, oftentimes exactly covering the retraining curve, and \textbf{Fair Unlearning} similarly replicates the performance of \textbf{Retraining (Fair)}. Further, we see that certain existing methods such as \textbf{PRU} and \textbf{SISA} are highly variable and can lead to greater disparity than retraining. Notably, across all datasets, our method outperforms baselines in preserving fairness.

% After exploring baseline methods, we evaluate our method in terms of fairness and accuracy. We compare with the \textbf{Newton} method proposed by \citet{guoCertifiedDataRemoval2020} because this method also uses a second order approximation to achieve unlearning and is the most similar to ours. We begin in the most simple setting, where unlearning requests are distributed uniformly across various subgroups. For each of our datasets, we train a base logistic regression model on the full dataset with and without our fairness loss term. Then, we randomly select 1\%, 5\%, 10\%, 15\%, and 20\% of our training set, retraining on the remaining data and unlearning the removed samples for each removal size. We then compare the accuracy of unlearning approximation to retraining for both vanilla and fair loss functions in Table \ref{tab:unlearning at random} as well as the relative disparities between vanilla and fair training in Figure. We also report the relative disparities between vanilla and fair methods across all datasets in Figure \ref{fig:unlearning at random}.  

% Our method, shown in red in Figure \ref{fig:unlearning at random}, well-approximates the fairness performance of retraining. We observe that retraining with a fair loss function has similar performance in terms of accuracy yet drastically decreases unfairness when compared with retraining with a vanilla loss function. We also see that over a vanilla loss function, the \texttt{newton} method well approximates retraining in terms of both accuracy and disparity.



% \subsection{Unlearning from Subgroups}


% Figure environment removed
\paragraph{Deletion from Subgroups.}

\begin{table*}[h!]
\centering
\caption{Test accuracy when unlearning at random.}
\label{tab:unlearning at random}
\resizebox{\linewidth}{!}{%
\begin{tabular}{lcccccc} 
\toprule
                                      & \multicolumn{2}{c}{COMPAS}             & \multicolumn{2}{c}{Adult}             & \multicolumn{2}{c}{HSLS}               \\
Method                                & 5\% Unlearned       & 20\% Unlearned       & 5\% Unlearned       & 20\% Unlearned      & 5\% Unlearned       & 20\% Unlearned       \\ 
\midrule
Full Training (BCE)                & $0.652 \pm 0.000$ & $0.652 \pm 0.000 $ & $0.817 \pm 0.000$ & $0.816 \pm 0.000$ & $0.724 \pm 0.000$ & $0.724 \pm 0.000$  \\
Retraining (BCE)            & $0.652 \pm 0.001$ & $0.654 \pm 0.003$  & $0.817 \pm 0.000$ & $0.817 \pm 0.001$ & $0.724 \pm 0.002$ & $0.724 \pm 0.002$  \\
Newton (\citet{guoCertifiedDataRemoval2020})  & $0.652 \pm 0.001$ & $0.654 \pm 0.003$  & $0.817 \pm 0.000$ & $0.817 \pm 0.001$ & $0.724 \pm 0.002$ & $0.724 \pm 0.002$  \\
SISA (\citet{bourtouleMachineUnlearning2020}) & $0.653 \pm 0.001$ & $0.656 \pm 0.004$  & $0.817 \pm 0.000$ & $0.817 \pm 0.001$ & $0.725 \pm 0.001$ & $0.724 \pm 0.003$  \\
PRU (\citet{izzoApproximateDataDeletion})     & $0.688 \pm 0.005$ & $0.691 \pm 0.001$  & $0.810 \pm 0.003$ & $0.810 \pm 0.001$ & $0.726 \pm 0.004$ & $0.729 \pm 0.000$  \\ 
\midrule
Full Training (Fair)                   & $0.647 \pm 0.000$ & $0.647 \pm 0.000$  & $0.819 \pm 0.000$ & $0.819 \pm 0.000$ & $0.713 \pm 0.000$ & $0.713 \pm 0.000$  \\
Retraining (Fair)               & $0.646 \pm 0.002$ & $0.642 \pm 0.003$  & $0.819 \pm 0.000$ & $0.819 \pm 0.001$ & $0.713 \pm 0.002$ & $0.713 \pm 0.002$  \\
Fair Unlearning (Ours)                            & $0.646 \pm 0.002$ & $0.642 \pm 0.003$  & $0.819 \pm 0.000$ & $0.819 \pm 0.001$ & $0.713 \pm 0.002$ & $0.713 \pm 0.002$  \\
\bottomrule
\end{tabular}
}
\end{table*}
If unlearning at random preserves relative subgroup distribution in data, the worst case unlearning distribution for fairness is when unlearning requests are concentrated on specific subgroups. Consider a model that performs poorly on some group of people, resulting in increased distrust and eventual data deletion requests from that subgroup. Subgroup deletion creates a negative feedback loop where data deletion results in degradation in performance for that subgroup, motivating more members of said subgroup to request unlearning. To explore this, we unlearn only from minority groups (Fig. \ref{fig:unlearning from minority subgroups}) and majority groups (Fig. \ref{fig:unlearning from majority}). Similar to our experiments with random unlearning, we train a base model and compare all baselines across various levels of unlearning. Note that when unlearning from minority groups, we only unlearn up to $10\%$ of the dataset as the minority subgroup has (by definition) less representation in the dataset. Too many deletions from the minority group would destroy the stability of the learned models. For example, in the Adult dataset, 10\% deletion from the minority group already leads to a significant increase in the variance of the model, as indicated by the error bars.

We see that when unlearning from both minority and majority subgroups, our \textbf{Fair Unlearning} method approximates \textbf{Retraining (Fair)} and thus maintains the best fairness performance throughout unlearning. When unlearning from minority subgroups, for both COMPAS and HSLS, our method perfectly approximates fair retraining and largely outperforms other baselines in terms of AEOD. In Adult, we see that both fair retraining and unlearning have high standard deviation at $10\%$ unlearning. This is due to the highly imbalanced subgroup distribution in Adult, where approximately $\approx 10\%$ of the data represents the minority group. We do not see a similar increase in standard deviation for baseline methods trained with BCE loss because those methods do not differentiate between subgroups when optimizing, whereas in the fair setting this error can be attributed primarily to the fair loss term which requires well-represented subgroups for accurate estimation. This behavior highlights a problem that even when unlearning a small proportion of the total dataset, unlearning requests may completely destroy accurate estimates of subgroup distributions, resulting in downstream disparity. In majority subgroups, we see consistent performance by our method across all datasets, with significant improvements in terms of AEOD. Finally, similar to the results for random unlearning, \textbf{PRU} display high error bands and atypical AEOD scores when compared to other baselines, while \textbf{Newton} remains accurate at approximating \textbf{Retraining (BCE)}.



% \begin{table}
% \centering
% \caption{Test accuracy when unlearning from minority subgroups.}
% \label{tab:unlearning from minority}
% \resizebox{\linewidth}{!}{%
% \begin{tabular}{lcccccc} 
% \toprule
%                                    & \multicolumn{2}{c}{COMPAS}            & \multicolumn{2}{c}{Adult}             & \multicolumn{2}{c}{HSLS}               \\
% Method                             & 5\% Unlearned       & 10\% Unlearned      & 5\% Unlearned       & 10\% Unlearned      & 5\% Unlearned       & 10\% Unlearned       \\ 
% \midrule
% Full Training (BCE)                & $0.652 \pm 0.000$ & $0.652 \pm 0.000$ & $0.821 \pm 0.000$ & $0.821 \pm 0.000$ & $0.729 \pm 0.000$ & $0.729 \pm 0.000$  \\
% Retraining (BCE)                   & $0.651 \pm 0.002$ & $0.652 \pm 0.001$ & $0.821 \pm 0.000$ & $0.822 \pm 0.000$ & $0.729 \pm 0.001$ & $0.730 \pm 0.001$  \\
% Newton (\citet{guoCertifiedDataRemoval2020}) & $0.651 \pm 0.002$ & $0.652 \pm 0.001$ & $0.821 \pm 0.000$ & $0.823 \pm 0.000$ & $0.729 \pm 0.001$ & $0.730 \pm 0.002$  \\
% SISA (\citet{bourtouleMachineUnlearning2020})   & $0.653 \pm 0.001$ & $0.654 \pm 0.002$ & $0.822 \pm 0.000$ & $0.823 \pm 0.000$ & $0.728 \pm 0.001$ & $0.728 \pm 0.001$  \\
% PRU (\citet{izzoApproximateDataDeletion})    & $0.661 \pm 0.007$ & $0.666 \pm 0.007$ & $0.819 \pm 0.004$ & $0.815 \pm 0.035$ & $0.720 \pm 0.004$ & $0.724 \pm 0.003$  \\ 
% \midrule
% Full Training (Fair)               & $0.653 \pm 0.000$ & $0.653 \pm 0.000$ & $0.812 \pm 0.000$ & $0.812 \pm 0.000$ & $0.723 \pm 0.000$ & $0.723 \pm 0.000$  \\
% Retraining (Fair)                  & $0.653 \pm 0.001$ & $0.661 \pm 0.005$ & $0.813 \pm 0.000$ & $0.816 \pm 0.001$ & $0.713 \pm 0.002$ & $0.713 \pm 0.002$  \\
% Fair Unlearning (Ours)             & $0.652 \pm 0.001$ & $0.661 \pm 0.004$ & $0.814 \pm 0.000$ & $0.816 \pm 0.002$ & $0.724 \pm 0.001$ & $0.721 \pm 0.001$  \\
% \bottomrule
% \end{tabular}
% }
% \end{table}

% \begin{table}
% \centering
% \caption{Test accuracy when unlearning from majority subgroups.}
% \label{tab:unlearning from majority}
% \resizebox{\linewidth}{!}{%
% \begin{tabular}{lcccccc} 
% \toprule
%                                       & \multicolumn{2}{c}{COMPAS}            & \multicolumn{2}{c}{Adult}             & \multicolumn{2}{c}{HSLS}               \\
% Method                                & 5\% Unlearned       & 20\% Unlearned      & 5\% Unlearned       & 20\% Unlearned      & 5\% Unlearned       & 20\% Unlearned       \\ 
% \midrule
% Full Training (BCE)                & $0.654 \pm 0.000$ & $0.654 \pm 0.000$ & $0.814 \pm 0.000$ & $0.814 \pm 0.000$ & $0.738 \pm 0.000$ & $0.738 \pm 0.000$  \\
% Retraining (BCE)            & $0.654 \pm 0.001$ & $0.652 \pm 0.002$ & $0.814 \pm 0.000$ & $0.814 \pm 0.000$ & $0.736 \pm 0.000$ & $0.736 \pm 0.002$  \\
% Newton (\citet{guoCertifiedDataRemoval2020})  & $0.654 \pm 0.001$ & $0.652 \pm 0.002$ & $0.814 \pm 0.000$ & $0.814 \pm 0.001$ & $0.736 \pm 0.000$ & $0.735 \pm 0.002$  \\
% SISA (\citet{bourtouleMachineUnlearning2020}) & $0.654 \pm 0.002$ & $0.653 \pm 0.004$ & $0.814 \pm 0.000$ & $0.815 \pm 0.001$ & $0.734 \pm 0.001$ & $0.733 \pm 0.002$  \\
% PRU (\citet{izzoApproximateDataDeletion})     & $0.696 \pm 0.006$ & $0.688 \pm 0.003$ & $0.812 \pm 0.001$ & $0.811 \pm 0.002$ & $0.723 \pm 0.012$ & $0.731 \pm 0.004$  \\ 
% \midrule
% Full Training (Fair)                   & $0.666 \pm 0.000$ & $0.666 \pm 0.000$ & $0.815 \pm 0.000$ & $0.815 \pm 0.000$ & $0.725 \pm 0.000$ & $0.725 \pm 0.000$  \\
% Retraining (Fair)               & $0.663 \pm 0.004$ & $0.653 \pm 0.002$ & $0.815 \pm 0.000$ & $0.813 \pm 0.000$ & $0.724 \pm 0.001$ & $0.725 \pm 0.002$  \\
% Fair Unlearning (Ours)                            & $0.663 \pm 0.004$ & $0.653 \pm 0.003$ & $0.815 \pm 0.000$ & $0.814 \pm 0.001$ & $0.724 \pm 0.001$ & $0.725 \pm 0.002$  \\
% \bottomrule
% \end{tabular}
% }
% \end{table}
% \begin{table}[t]
%   \caption{Test accuracy when unlearning from majority subgroups.}
%   \label{tab:unlearning from majority}
%   \centering
%   \begin{tabular}{lcccccc}
%     \toprule
%     & \multicolumn{2}{c}{COMPAS} & \multicolumn{2}{c}{Adult} & \multicolumn{2}{c}{HSLS} \\
%     % \cmidrule(r){1-2}
%     Method & 5\% Removal & 20\% Removal & 5\% Removal & 20\% Removal & 5\% Removal & 20\% Removal \\
%     \midrule
%     Full Dataset (Vanilla) & $0.654 \pm 0.000$ & $0.654 \pm 0.000$
%         & $0.814 \pm 0.000$ & $0.814 \pm 0.000$
%         & $0.738 \pm 0.000$ & $0.738 \pm 0.000$ \\
%     Naive Retraining (Vanilla) & $0.654 \pm 0.001$ & $0.652 \pm 0.002$
%         & $0.814 \pm 0.000$ & $0.814 \pm 0.000$
%         & $0.736 \pm 0.000$ & $0.736 \pm 0.002$\\
%     Newton (\citet{guoCertifiedDataRemoval2020}) & $0.654 \pm 0.001$ & $0.652 \pm 0.002$
%         & $0.814 \pm 0.000$ & $0.814 \pm 0.001$
%         & $0.736 \pm 0.000$ & $0.735 \pm 0.002$\\
%     SISA (\citet{bourtouleMachineUnlearning2020}) & $0.654 \pm 0.002$ & $0.653 \pm 0.004$
%         & $0.814 \pm 0.000$ & $0.815 \pm 0.001$
%         & $0.734 \pm 0.001$ & $0.733 \pm 0.002$\\
%     PRU (\citet{izzoApproximateDataDeletion}) & $0.696 \pm 0.006$ & $0.688 \pm 0.003$
%         & $0.812 \pm 0.001$ & $0.811 \pm 0.002$
%         & $0.723 \pm 0.012$ & $0.731 \pm 0.004$\\
%     \midrule
%     Full Dataset (Fair) & $0.666 \pm 0.000$ & $0.666 \pm 0.000$
%         & $0.815 \pm 0.000$ & $0.815 \pm 0.000$
%         & $0.725 \pm 0.000$ & $0.725 \pm 0.000$\\
%     Naive Retraining (Fair) & $0.663 \pm 0.004$ & $0.653 \pm 0.002$ 
%         & $0.815 \pm 0.000$ & $0.813 \pm 0.000$
%         & $0.724 \pm 0.001$ & $0.725 \pm 0.002$\\
%     FUN (Ours) & $0.663 \pm 0.004$ & $0.653 \pm 0.003$ 
%         & $0.815 \pm 0.000$ & $0.814 \pm 0.001$
%         & $0.724 \pm 0.001$ & $0.725 \pm 0.002$\\
%     \bottomrule
%   \end{tabular}
% \end{table}

\subsection{Experimental Results on Accuracy}
\paragraph{Deletion at Random.} Finally, we validate the accuracy of \textbf{Fair Unlearning} to ensure that increased fairness does not come at a cost to performance. Trivially, AEOD can be achieved by a random guessing classifier despite its poor performance. We report test accuracy at $5\%$ and $20\%$ of samples unlearned across our various baselines and datasets in Table~\ref{tab:unlearning at random}. We see that while achieving significant improvements in AEOD, our method and brute-force retraining maintain performance and can even improve in some cases in terms of accuracy. Further, all approximations are highly precise, with very small standard deviations. 
\paragraph{Deletion from Subgroups.} Similarly, we validate the accuracy of our proposed method when unlearning from the majority and minority subgroup (tables in appendix). Our results show that even in extreme unlearning settings such as biased request distributions, our method remains accurate in addition to preserving fairness. It is interesting to note that we again observe a higher variance in \textbf{PRU} even in terms of accuracy, likely due to the authors usage of a residual approximation method to achieve even greater speedup~\citep{izzoApproximateDataDeletion}.