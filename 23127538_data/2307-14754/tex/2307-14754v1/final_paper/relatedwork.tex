% * previous work has focused on accurate approximation while being faster than retraining
% * navigation of tradeoff between accuracy (of approx) and runtime
% * however, these rarely consider the actual performance of these approximations
\paragraph{Unlearning.}

The naive approach to machine unlearning is to retrain a model from scratch with each data deletion request. Retraining is not a feasible alternative for companies with large models or limited resources. Thus, the primary objective of machine unlearning is to provide efficient approximations to retraining while minimizing computational costs. Early approaches in security and privacy attempt to achieve exact removal, where an unlearned model is identical to retraining, but are limited in model class~\citep{caoMakingSystemsForget2015, ginartMakingAIForget2019}. \citet{bourtouleMachineUnlearning2020} propose SISA, a more flexible approach to exact unlearning that "shards" a dataset, dividing it and training an ensemble of models where each can be retrained separately. SISA is unique in that it can be applied to any model that can be ensembled, allowing for the implementation of some fairness interventions. However, the ensembling aggregation scheme can result in decreased accuracy, and recent work has shown that sharding data poses a risk to fairness as well~\citep{kochno}. More recent approaches formalize approximate removal, where an unlearned model is statistically indistinguishable from retraining. These methods leverage convex loss functions over linear models to guarantee unlearning with a Newton step~\citep{guoCertifiedDataRemoval2020, sekhariRememberWhatYou, izzoApproximateDataDeletion} or gradient-based approach~\citep{neelDescenttoDeleteGradientBasedMethods2020a}. 
%Finally, \citet{neelDescenttoDeleteGradientBasedMethods2020a} generalize previous work to models with convex loss and use a gradient-based approach rather than a second order approximation to achieve unlearning.

% Prior research has focused on developing theoretical guarantees for approximation, leveraging statistical and computational techniques to accelerate runtimes. Although these studies navigate the trade-offs between runtime and approximation accuracy, they often neglect to consider the downstream consequences of unlearning.

% While the literature in machine unlearning is relatively new, past work in online learning has explored incrementally adding and removing samples from models such as SVMs \cite{cauwenberghs2000incremental}, \cite{karasuyama2010multiple}. Cao and Yang first introduced the term machine unlearning, presenting a deterministic method to decompose learning algorithms for easier data removal \cite{caoMakingSystemsForget2015}. However, these early methods focused on exact removal of training samples, limiting their scope in terms of unlearnable model types. Ginart et al. later extended these principles to k-means clustering algorithms and introduce the notion of statistical indistinguishability to the development of unlearning algorithms, which provides greater flexibility in terms of approximation techniques and unlearnable algorithms \cite{ginartMakingAIForget2019}.

% The SISA algorithm marked the first step toward approximate unlearning in trained models such as stochastic gradient descent-based deep learning models, physically splitting data into ''shards," training separate models on each shard, and aggregating their predictions \cite{bourtouleMachineUnlearning2020}. The authors guarantee unlearning while improving runtimes by only retraining the single model that saw a piece of data. However, the ensembling aggregation scheme can result in decreased accuracy, and recent work has shown that it poses a risk to fairness as well \cite{kochno}.

% Guo et al. took a different approach, formalizing unlearning from a differential privacy perspective and developing an unlearning method using a Newton step \cite{guoCertifiedDataRemoval2020}. They achieve efficient approximation with theoretical guarantees by adding noise during the training process and estimate the unlearned model parameters using the gradient influence \cite{kohUnderstandingBlackboxPredictions2020} of the samples to be removed. Sekhari et al. also use a second-order approximation with added noise to achieve efficient and accurate unlearning \cite{sekhariRememberWhatYou}. Izzo et al. proposed the Projective Residual Update (PRU) method, which uses a second-order residual estimation to efficiently infer the outputs of a retrained model, and then reverse-engineers the parameters of this retrained model from the inferred outputs \cite{izzoApproximateDataDeletion}. These second-order methods are limited to linear models with analytic solutions and easily computed Hessian matrices. Finally, \cite{neelDescenttoDeleteGradientBasedMethods2020a} generalizes previous work to models with convex loss and uses a gradient-based approach that avoids the computation of a Hessian. 

% The first step towards approximate unlearning in trained models came from the SISA algorithm \cite{bourtouleMachineUnlearning2020}. The authors physically split their data into "shards," train multiple models on each shard and ensemble them. This method guarantees unlearning while achieving speedup proportional to the number of shards but sacrifices accuracy due to the ensembling process. 

% Guo et al. \cite{guoCertifiedDataRemoval2020} formalize a definition of unlearning from a differntial privacy viewpoint, considering the statistical $(\epsilon, \delta)$ indistinguishability of a retrained and unlearned model. They then go on to provide an unlearning method using a Newton step based on the gradient influence of the point to be unlearned. By adding noise to their training procedure, they ensure that their unlearning procedure is statistically indistinguishable from the retraining process. \cite{izzoApproximateDataDeletion} compares this Newton-based approach to their proposed Projective Residual Update (PRU), which uses a residual method to infer the unlearned model's outputs, and reverse-engineers the unlearned model's parameters. However, these two methods are limited in scope to linear models where the solutions are analytic and the hessian can be easily approximated. 

\paragraph{Fairness.}

There are a multitude of definitions for fairness in machine learning, such as individual fairness, multicalibration or multiaccuracy, and group fairness. Individual fairness~\cite{dwork2012fairness} posits that "similar individuals should be treated similarly" by a model. 
% \hima{below sentences do not describe clearly what is group fairness. It would be great to say common notions of fairness are individual fairness, group fairness; this is what each of those mean. }
On the other hand, recent work has focused on multicalibration and multiaccuracy~\cite{hebert2018multicalibration, kearns2018preventing, deng2023happymap}, where predictions are required to be calibrated across subpopulations. These subpopulation definitions can be highly expressive, containing many intersectional identities from protected groups. In this work, however, we focus on the most commonly studied form of fairness, group fairness, which seeks to balance certain statistical metrics across predefined subgroups.

Group fairness literature has proposed various definitions of fairness, but the three most common definitions are Demographic Parity~\citep{zafar2017fairness, feldman2015certifying, zliobaite2015relation, calders2009building}, Equalized Odds, and Equality of Opportunity~\cite{hardt2016equality}. Each of these definitions seeks to ensure a different statistic regarding the data is balanced between subgroups. In addition, there are generally three approaches to achieving group fairness: preprocessing, in-processing, and postprocessing. Preprocessing algorithms attempt to correct the underlying dataset to ensure the resultant trained model is fair~\citep{calmon2017optimized}. In-processing algorithms occur during the training process by modifying conventional empirical risk minimization objectives to have fairness constraints~\citep{lowyStochasticOptimizationFramework2022, berk2017convex, agarwal2018reductions, martinez2020minimax}. Finally, post-processing algorithms modify the classifier predictions to ensure fair treatment across groups based on estimations from the training data~\citep{alghamdi2022beyond, hardt2016equality}. In this work we focus on in-processing algorithms because they allow for a single update over a modified fairness objective rather than requiring an additional operation before or after each unlearning request.

\paragraph{Intersections.} Despite advancements in machine unlearning, the literature still lacks sufficient consideration of the downstream impacts of unlearning methods. While recent papers have explored the compatibility of the right to be forgotten with the right to explanation~\citep{krishna2023towards}, there is little work at the intersection of unlearning and fairness. The most similar works to our own come from privacy literature. Here, a thread of work has shown the incompatibility of group fairness with privacy~\citep{esipova2022disparate, bagdasaryan2019differential, cummings2019compatibility} but these incompatibilities arise due to privacy-specific methods, such as gradient clipping and differences in neighboring datasets. In unlearning literature, recent empirical studies have shown that unlearning can increase disparity \cite{zhang2023forgotten}, and other works have demonstrated the incompatibility of fairness and unlearning for the SISA algorithm \cite{kochno}, but no work has provided a method to achieve both in practice. In this paper, we propose the first method which achieves fairness and unlearning with theoretical guarantees.

%%%%%%%%%% Removed discussion of why all approaches fail for unlearning after our meeting on Friday


% While all approaches are valid the perspective of a fairness practitioner, in-processing is the most translatable to the task of unlearning. \alex{I could talk about how preprocessing doesnt make sense because corporations will always want to use all of their data to maximize performance but I don't know if it is completely relevant} More specifically, pre-processing requires control over the training dataset, but with unlearning we have no control over which users request data deletion. In the case of post-processing, fair modification of a classifier requires an additional optimization procedure after the training process. Similar to the cost of naively retraining, recalibrating model predictions to be fair after every unlearning request can also be costly. Similarly, the state of the art for in-procesing algorithms is generally accepted to be the reductions approach proposed by Agarwal et al. \cite{agarwal2018reductions}. However, this approach requires an iterative training of multiple classifiers, which is again impractical in the unlearning setting. For these reasons, we use a convex fairness regularizer proposed by Berk et al. which we can easily approximate with unlearning guarantees \cite{berk2017convex}.

% \begin{table}
% \centering
% \caption{Visualization of Existing Unlearning and Fairness Capabilities}
% \label{tab:intro_table}
% \begin{tabular}{llcc} 
% \toprule
%   &     & \multicolumn{2}{c}{Unlearning}                    \\ 
%   &     & \multicolumn{1}{c}{Yes} & \multicolumn{1}{c}{No}  \\
% \midrule
% \multirow{2}{*}{Fairness} & Yes & Fair Unlearning (Ours) & Fair Methods~\citep{agarwal2018reductions, berk2017convex, dwork2012fairness, hardt2016equality, jeong2022fairness} \\
%   & No  & Existing Unlearning Methods~\citep{guoCertifiedDataRemoval2020, izzoApproximateDataDeletion, sekhariRememberWhatYou, bourtouleMachineUnlearning2020, neelDescenttoDeleteGradientBasedMethods2020a} & Basic ML\\
% \bottomrule
% \end{tabular}
% \end{table}