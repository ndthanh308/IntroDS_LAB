% \hima{General length recommendations: Abstract, intro, related work -- all in 2 to 3 pages; Methods and theory section: 3 to 3.5 pages; Remaining is experiments: at least 2 to 2.5 pages should be experiments}
Machine learning applications, ranging from recommender systems to credit scoring, frequently involve training sophisticated models using large quantities of personal data. As a means to prevent misuse of such data, policymakers have proposed new principles of data governance to protect user data from misuse and harm. Recent regulatory policies have included provisions\footnote{For example, Article 17 in the European Union's General Data Protection Regulation (GDPR)~\citep{GDPR} or Section~1798.105 in the California Consumer Privacy Act (CCPA)~\citep{CCPA}.} that allow data subjects to request the deletion of their information from databases and models used by organizations. These policies have given rise to a general data governance principle known as the \emph{right to be forgotten}. However, achieving this principle in practice is non-trivial; for instance, the naive approach of retraining a model from scratch each time a deletion request occurs is computationally prohibitive. As a result, a variety of \emph{machine unlearning} (or simply \emph{unlearning}) methods have been developed to effectively approximate the retraining process in an online manner with reduced computational costs~\citep{guoCertifiedDataRemoval2020,bourtouleMachineUnlearning2020,izzoApproximateDataDeletion,neelDescenttoDeleteGradientBasedMethods2020a}.

% \hima{some more high level notes: I think this introduction might benefit from some clear examples in para 2 and 3 which do the following: 1. highlight why fairness is critical in applications this paper cares about (para 2). 2. In the process of unlearning, how could unfairness be caused? (para 3). While we are trying to get there, i think the above points are not coming out very saliently.}

% \hima{high level comment about this part -- this paragraph needs to convey how important fairness is; following point sequence might be helpful in highlighting its importance. 1. In the aforementioned critical applications such as loan approval or recruitment or recommender systems, fairness is an important criteria. 2. with an example, describe why; for example if a predictor is unfair, one gender or race may get terrible outcomes. 3. This basically means we need to ensure fairness alongside unlearning. }
% \hima{wonder if it would be better to frame this argument as -- in critical real-world applications, it is also important to ensure other key properties such as fairness hold; as opposed to in setting where unlearning is important, other properties are also important. }

%it is important to ensure that other key properties hold such as the fair treatment of individuals.

In critical real-world applications that use personal data, consumers may also require their data not be used in the unfair treatment of others. For example, if a social media recommender system systematically under-ranks posts from a particular demographic group, it could substantially suppress the public voices of that group~\citep{beutel2019fairness, jiang2019degenerate}. Similarly, if a machine learning model used for credit scoring exhibits bias, it could unjustly deny loans to certain demographic groups~\citep{corbett2018measure}. In medicine, the fair allocation of healthcare services is also critical to prevent the exacerbation of systemic racism and improve the quality of care for marginalized groups~\citep{chen2021algorithm}. Protection against algorithmic discrimination has been spelled out in various regulatory documents~\cite{aibillofrights,GDPR,CCPA}, but it is unclear whether unlearning and fair treatment can coexist.
% such as recommender systems that may create echo chambers and lead to social biases~\citep{jiang2019degenerate}, and loan approval or job recruitment that will make high-stakes decisions~\jiaqi{cite}. 
% \hima{by calling fairness as an "auxiliary optimization objective", we may be downplaying its importance and our goal is to do the opposite. }

% Protection against algorithmic discrimination is a critical desideratum of models used in applications of consequence and as such, has been spelled out in the AI Bill of Rights~\citep{aibillofrights} and other AI policies~\cite{GDPR, CCPA}.

%it has been a common practice to incorporate fairness regularizers into the objective functions of machine learning models in these contexts~\citep{beutel2019fairness}. 

To adhere to the above mentioned regulatory principles, there is a concurrent need for practitioners to satisfy the right to be forgotten while still preventing discrimination in deployed settings. However, most existing machine unlearning methods are not directly compatible with popular fairness interventions. Specifically, most efficient unlearning methods~\citep{guoCertifiedDataRemoval2020,neelDescenttoDeleteGradientBasedMethods2020a, izzoApproximateDataDeletion} are restricted to unlearn training objectives that are convex and can be decomposed into sums of loss functions on individual training data points. On the other hand, existing fairness interventions generally have nonconvex objectives \cite{lowyStochasticOptimizationFramework2022} or involve pairwise or group comparisons that make unlearning a datapoint nontrivial due to each sample's entangled influence on all other datapoints in the objective function\footnote{See Appendix~\ref{sec:incompatible} for a detailed explanation.}~\citep{berk2017convex, beutel2019fairness}. As such, current unlearning methods are incompatible with fairness objectives, highlighting a need from real-world practitioners for a method that achieves fairness and unlearning simultaneously.

%%%%%%%%%%%%%% Old Paragraph 3


% \flavio{Careful here. There is literature in DP+fairness. }Despite the concurrent need to satisfy both the right to be forgotten and fairness in practical applications, there has been little to no prior research exploring methods to simultaneously achieve both. Machine unlearning methods  methods are not directly compatible with existing fairness interventions.
% % \hima{"are not applicable to fair machine learning objectives" might be a bit of a stretch; we can probably just say Existing unlearning methods are primarily designed to account for predictive accuracy but not fairness. Furthermore, it may not be feasible to trivially apply existing unlearning algorithms to loss functions that optimize for both predictive accuracy as well as fairness due to .."}
% % Existing unlearning methods are primarily designed to account for predictive accuracy without fairness requirements. 
% % Furthermore, it may not be feasible to naively apply existing unlearning methods to machine learning models trained with fairness regularizers.
% Specifically, most efficient unlearning methods~\citep{guoCertifiedDataRemoval2020,neelDescenttoDeleteGradientBasedMethods2020a, izzoApproximateDataDeletion} are restricted to unlearn training objectives that can be written as the average of loss functions on individual training data points. However, fairness interventions often involve pairwise or group comparisons which make unlearning a datapoint nontrivial due to its entangled influence on all other datapoints in the objective function\footnote{See Appendix~\ref{sec:incompatible} for a detailed explanation.}~\citep{berk2017convex, beutel2019fairness}.
% % Fairness regularizers, however, are typically obtained by comparing data points, which entangles two or more data points in the regularization functions and prevents straightforward applications of the aforementioned unlearning methods\footnote{See Appendix~\ref{sec:incompatible} for a detailed explanation.}. 
% While some unlearning methods permit unlearning over any objective~\citep{bourtouleMachineUnlearning2020} by splitting the training data into multiple shards and train multiple models, this ensembling can be inefficient and less performant. Furthermore, a recent study has shown that sharding the training data leads to disproportional negative effect to minority groups of the training data and is thus inappropriate for fair machine learning applications~\citep{kochno}. Therefore, there is a signifiant gap between the existing unlearning methods and the real-world applications where fairness is a crucial consideration.

%%%%%%%%%%%%%%%%% Old Paragraph 3


% \hima{what is "this critical gap"? last sentence in our previous paragraph should end with the gap to make it clear what the gap is.}
To fill in this critical gap in data governance and fair treatment, we propose the first \emph{fair unlearning} method
% , \hima{what does FAUNA expand to? we might want to say that} \funabbrv, 
that can effectively unlearn requested data deletions over a fair machine learning objective. Our fair unlearning approach takes a convex fairness regularizer with pairwise comparisons and unrolls these comparisons into an unlearnable form while ensuring the preservation of theoretical guarantees on unlearning. We theoretically demonstrate that our method can achieve a common notion of unlearning, \emph{statistical indistinguishability}~\citep{guoCertifiedDataRemoval2020,neelDescenttoDeleteGradientBasedMethods2020a}, while preserving \emph{equality of odds}~\citep{hardt2016equality}, a popular group fairness metric. Furthermore, we empirically evaluate the proposed method on a variety of real-world datasets and verify its effectiveness across various data deletion request settings, such as deletion at random and deletion from minority and majority subgroups. We show that our method well approximates retraining over fair machine learning objectives in terms of both accuracy and fairness. 
% \hima{next sentence -- are we showing empirical results for this?}
% Finally, we observe that naively applying existing unlearning methods to a fair objective function can disproportionately impact fairness when unlearning over minority versus majority groups across multiple datasets.

% \hima{the contributions below should be expanded a bit more? currently they look rather terse.}
% Our main contributions are the following:
% \begin{itemize}
% \item \hima{We formulate the problem of fair unlearning i.e., how to achieve unlearning while preserving fairness.}
%     \item We propose \funabbrv, an algorithm for unlearning over fair loss functions.
%     \item We prove \funabbrv is unlearnable and preserves fairness when compared with retraining.
%     \item We evaluate our algorithm on three real-world datasets commonly found in fairness literature spanning domains of criminal law, education, and finance.
% \end{itemize}

Our main contributions include:
\begin{itemize}
    \item We formulate the problem of fair unlearning, i.e., how to unlearn requested data points in scenarios where enforcement of fairness is crucial.
    \item We introduce the first fair unlearning method that can provably unlearn requested data points while preserving popular notions of fairness.
    \item We empirically verify the effectiveness of the proposed method through extensive experiments on a variety of real-world datasets and across different settings of data deletion requests.
\end{itemize}


%%%%%%%%%%%%%% Alex's original version %%%%%%%%%%%%%%

% Over the past decade, machine learning (ML) models have become prevalent in real world applications including healthcare, finance, and law. The usage of ML in these high-stakes settings has motivated regulation through policies such as the GDPR \cite{GDPR} and AI Bill of Rights to protect consumers and prevent the violation of their rights to data privacy, fair treatment, and more. One such right, the "right to be forgotten," mandates that users be able to request their personal data be removed from corporate use. While deleting data from databases is trivial, user information may persist in the weights of a deployed model that was trained on user data. \alex{The following is getting into privacy a little, should I remove?} Adversaries can use techniques such as membership inference attacks \cite{shokri2017membership} to extract training data with black box access to a deployed model. These vulnerabilities highlight a need for organizations to be able to remove or unlearn data from their models in addition to removing it from their databases.

% In practice, data deletion from machine learning models, or "machine unlearning," can be naively achieved by retraining without the samples to be forgotten. However, in many domains where ML is deployed at scale with large amounts of data, it is infeasible to retrain for every unlearning request. This problem has motivated a new body of literature, called machine unlearning, which seeks to make efficient approximations of a retrained model while guaranteeing data removal. 

% In addition to the ``right to be forgotten", consumer data protection regulation also advocates for the ``right to fair treatment," ensuring that people of different identities, such as race, gender, and socioeconomic status, are treated equally by algorithms as well. Fairness is a secondary goal to performance, and often is achieved in practice with modified optimization procedures such as by adding fair regularizers during training or applying fair postprocessing to the predictions of a model.

% \alex{Think about \textit{why} there is little work at the intersection, maybe include}

% When developing algorithms that protect the right to be forgotten and the right to fair treatment, it is important that protecting one right does not come at the cost to another. However, there is little work at the intersection of these topics and little knowledge of whether achieving both these rights can coincide in practice. Past unlearning literature has only focused on approximating vanilla loss functions without additional optimization constraints, and no work has explored whether the efficient approximations to retraining proposed by modern unlearning literature are also good approximations when evaluated by secondary goals such as fairness of a model.

% While existing work is more efficient than retraining and still protects a user's ``right to be forgotten," these approximations are not practical if they cannot satisfy other user protections. In this work we show that in the vanilla loss setting, unlearning algorithms can violate the right to fair treatment. We propose an algorithm for unlearning over a fair loss function, and theoretically prove its ability to unlearn. We then empirically evaluate this method on a variety of real-world datasets and show that with careful algorithm design, fairness and unlearning are compatible.

% This problem has motivated a new body of literature on more efficient approaches to data removal. Recent works in the field of unlearning draw on methods in security and privacy to guarantee unlearning while using efficient approximations of a retrained model.

% Recent work in unlearning has explored two main paths. The first use intelligent design to reduce computation cost by isolating data into "shards," training different models on each shard, and aggregating them, so that retraining only has to occur on one model with the shard containing the deleted point \cite{bourtouleMachineUnlearning2020}. The second set of approaches have focused on a differential privacy inspired notion of approximate deletion, providing statistical guarantees on the difference between a naively retrained model and a model unlearned using an efficient mechanism \cite{guoCertifiedDataRemoval2020} \cite{izzoApproximateDataDeletion}. These mechanisms focus on models with convex loss functions, using gradient-based approximations to create new parameters that are indistinguishable from the output of a retrained algorithm. 

%%%%%%%%%%%%%% Alex's original version %%%%%%%%%%%%%%


