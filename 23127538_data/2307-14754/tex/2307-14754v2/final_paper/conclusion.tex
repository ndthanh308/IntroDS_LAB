In this work we show that existing methods fail to accommodate models with fairness considerations, and we resolve this issue by proposing the first fair unlearning algorithm. We prove our method is unlearnable and bound its fairness guarantees. Then, we evaluate our method on three real-world datasets and show that our method well approximates retraining with a fair loss function in terms of both accuracy and fairness. Further, we highlight worst case scenarios where unlearning requests disproportionately come from certain subgroups. Our fair unlearning method allows practitioners to achieve multiple data protection goals at the same time (fairness and unlearning) and satisfy regulatory principles. Furthermore, we hope this work sparks discussion surrounding other auxiliary goals that may need be compliant with unlearning such as robustness and interpretability, or other definitions of fairness such as multicalibration and multiaccuracy.