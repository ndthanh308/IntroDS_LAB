\paragraph{Unlearning.}

The naive approach to machine unlearning is to retrain a model from scratch with each data deletion request. However, retraining is not feasible for companies with many large models or organizations with limited resources. Thus, the primary objective of machine unlearning is to provide efficient approximations to retraining. Early approaches in security and privacy attempt to achieve exact removal, where an unlearned model is identical to retraining, but are limited in model class~\citep{caoMakingSystemsForget2015, ginartMakingAIForget2019}. \citet{bourtouleMachineUnlearning2020} propose SISA, a flexible approach to exact unlearning that ``shards" a dataset, dividing it and training an ensemble of models where each can be retrained separately. More recent approaches propose approximate removal, requiring the unlearned model to be ``close" to the output of retraining. Some approximate removal methods focus on improving efficiency~\citep{wuDeltaGradRapidRetraining2020} and others try to preserve performance~\citep{wuPUMAPerformanceUnchanged2022}. While these methods apply to a large class of models, they have no formal guarantees on data removal. A second group of approximate approaches provide theoretical guarantees on the statistical indistinguishability of unlearned and retrained models. These noise-based methods leverage convex loss functions to guarantee unlearning with gradient updates~\citep{neelDescenttoDeleteGradientBasedMethods2020a} and Hessian methods~\citep{guoCertifiedDataRemoval2020, sekhariRememberWhatYou, izzoApproximateDataDeletion}. We augment this second set of approximate methods to simultaneously provide strong guarantees on data protection and preserve fairness performance while targeting a common class of models.

\paragraph{Fairness.}

There are a multitude of definitions for fairness in machine learning, such as individual fairness, multicalibration or multiaccuracy, and group fairness. Individual fairness~\citep{dwork2012fairness} posits that ``similar individuals should be treated similarly" by a model. 
On the other hand, recent work has focused on multicalibration and multiaccuracy~\citep{hebert2018multicalibration, kearns2018preventing, deng2023happymap}, where predictions are required to be calibrated across subpopulations. These subpopulation definitions can be highly expressive, containing many intersectional identities from protected groups. In this work, however, we focus on the most commonly studied form of fairness, group fairness, which seeks to balance certain statistical metrics across predefined subgroups. Group fairness literature has proposed various definitions of fairness, but the three most common definitions are Demographic Parity~\citep{zafar2017fairness, feldman2015certifying, zliobaite2015relation, calders2009building}, Equalized Odds, and Equality of Opportunity~\citep{hardt2016equality}. To achieve these definitions, there are generally three approaches to achieving group fairness: \emph{preprocessing} which attempts to correct dataset imbalance to ensure fairness~\citep{calmon2017optimized}, \emph{in-processing} which occurs during training by modifying traditional empirical risk minimization objectives to include fairness constraints~\citep{lowyStochasticOptimizationFramework2022, berk2017convex, agarwal2018reductions, martinez2020minimax}, and \emph{postprocessing} which modifies predictions to ensure fair treatment~\citep{alghamdi2022beyond, hardt2016equality}. 
In this work we focus on in-processing algorithms because they simply modify an objective to account for fairness rather than requiring an additional operation before or after each unlearning request which would also have to be made unlearnable.

\paragraph{Intersections.} Despite advancements in machine unlearning, the literature still lacks sufficient consideration of the downstream impacts of unlearning methods. While recent papers have explored the compatibility of the right to be forgotten with the right to explanation~\citep{krishna2023towards}, there is little work at the intersection of unlearning and fairness. In privacy literature, a thread of work has shown the incompatibility of group fairness with privacy~\citep{esipova2022disparate, bagdasaryan2019differential, cummings2019compatibility} but these incompatibilities arise due to privacy-specific methods, such as gradient clipping and differences in neighboring datasets. Fairness literature has studied the related problem of the influence of training data on fairness~\citep{wang2022understanding}, but does not provide any methods for unlearning. In unlearning literature, recent empirical studies have shown that unlearning can increase disparity~\citep{zhang2023forgotten}, other works have demonstrated the incompatibility of fairness and unlearning for the SISA algorithm \citep{kochno}, and one work~\citep{wang2023inductive} has provided a method to achieve removal and fairness but uses a sharding and retraining algorithm over fairness-corrected graph data for GNNs. In this paper, we propose the first efficient method which achieves fairness while being provably unlearnable without requiring retraining.