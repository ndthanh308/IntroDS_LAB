\subsection{Proof of Theorem \ref{thm:eps_delta}}\label{sec:thm1_proof}

We begin by introducing two lemmas.

Recall the definition of our full loss function,
\begin{align}
    \mathcal{L}(\theta, D) = \frac{1}{n}\sum_{i=1}^n \ell(\theta, x_i, y_i) + \frac{\lambda}{2n}||\theta||_2^2& + \gamma\frac{1}{|N|}\sum_{i,j,k,l \in N}\ell_\textrm{fair}(\theta, \{(x_f, y_f)\}_{f \in \{i,j,k,l\}}), \nonumber \\
    \ell_\textrm{fair}(\theta, \{(x_f, y_f)\}_{f \in \{i,j,k,l\}}) &= \mathbbm{1}[y_i = y_j](\langle x_i, \theta\rangle  - \langle x_j, \theta\rangle)\mathbbm{1}[y_k = y_l](\langle x_k, \theta\rangle  - \langle x_l, \theta\rangle) \nonumber \\
    &=\mathbbm{1}[y_i = y_j]\mathbbm{1}[y_k = y_l]\left(\theta^T (x_i^T x_k - x_i^T x_l - x_j^T x_k + x_j^T x_l)\theta\right). \nonumber
\end{align}
For our proof of Lemma \ref{lem:bounded_gradient}, we multiply by the entire objective by $n$:
\begin{align}
    \mathcal{L}(\theta, D) = \sum_{i=1}^n \ell(\theta, x_i, y_i) + \frac{\lambda}{2}||\theta||_2^2 + \gamma\frac{n}{|N|}\sum_{i,j,k,l \in N}\ell_\textrm{fair}(\theta, \{(x_f, y_f)\}_{f \in \{i,j,k,l\}}).
\end{align}
Let $\thetaunlearn := \thetafull+H_{\thetafull}^{-1}\Delta$ be the update step of our unlearing algorithm, where $\thetafull$ is our optimum when training on the entire dataset, $H_{\thetafull}^{-1} := (\nabla^2 \mathcal{L}(\thetafull; D'))^{-1}$ is the inverse hessian of the loss at $\thetafull$ evaluated over the remaining dataset defined as $D' = D \setminus R$. 
Let $R$ be our unlearning requests, and without loss of generality assume these are the last $m$ elements of $D$, $\{x_{n-m}, ... , x_n\}$. Let $G_a := \{i : s_i = a\}$, $G_b := \{i : s_i = b\}$ be sets of indices indicating subgroup membership for each sample. Define $r_a := \{i \in G_a : x_i \in R\}$, $r_b := \{i \in G_b : x_i \in R\}$, and $d'_a, d'_b$ similarly for $x_i \in D'$. Recall $n = |D|$, and let $n_a = |G_a|, n_b = |G_b|$ and similarly let $m = |R|, m_a = |r_a|, m_b = |r_b|$. 

Let $N = G_a \times G_b \times G_a \times G_b$, and let $C_{D'} = d'_a \times d'_b \times d'_a \times d'_b.$ Then,

Then, define our unlearning step $\Delta$ as the following:
\begin{align}
    &\Delta := \sum_{i=m}^{n}\ell'(\thetafull, x_i, y_i) + m\lambda \thetafull+\gamma\frac{n}{|N|} \sum_{(i,j,k,l) \in N}\ell_\textrm{fair}'(\thetafull, \{(x_f, y_f)\}_{f \in \{i,j,k,l\}})  \nonumber\\
    &-\gamma\frac{n-m}{|C_{D'}|}\sum_{(i,j,k,l)\in C_{D'}}\ell_\textrm{fair}'(\thetafull, \{(x_f, y_f)\}_{f \in \{i,j,k,l\}}). \label{eqn:delta}
\end{align}
\begin{lemma} \label{lem:bounded_gradient}
Assume the ERM loss $\ell$ is $\psi$-Lipschitz in its second derivative ($\ell''$ is $\psi$-Lipschitz), and bounded in its first derivative by a constant $||\ell'(\theta, x, y)||_2 \leq g$. Suppose the data is bounded such that for all $i \in n$,  $||x_i||_2 \leq 1$. Then the gradient of our unlearned model on the remaining dataset is bounded by:
\begin{align}
    ||\nabla \mathcal{L}(\thetaunlearn; D')||_2 \leq  \frac{\psi}{\lambda^2(n-m)}\left(2mg + ||\thetafull||_2\left|\left|\frac{8(m_a^2n_b^2 + m_b^2n_a^2 - m_a^2m_b^2)(n-m)}{(n_a^2n_b^2)} \right|\right|_2\right)^2.
\end{align}
\end{lemma}
\textit{Proof:} This proof largely follows the structure of the proof of Theorem 1 in~\citet{guoCertifiedDataRemoval2020}, but differs in the addition of our fairness loss term which introduces additional terms into the bound.

Let $G(\theta) = \nabla L(\theta; D')$ be the gradient at the \textit{remaining} dataset $D' = D \setminus R$. We want to bound the norm of the gradient at $\theta = \thetaunlearn$, as we know for convex loss, the optimum when retraining over the \textit{remainder} dataset $D'$ is zero. Keeping the norm of the unlearned weights close to zero ensures we have achieved unlearning.

We substitute the expression for the unlearned parameter and do a first-order Taylor approximation, which states there exists an $\eta \in [0,1]$ such that the following holds:
\begin{align*}
    G(\thetaunlearn) &= G(\thetafull + H_{\thetafull}^{-1}\Delta), \\
    &= G(\thetafull) + \nabla G(\thetafull + \eta H_{\thetafull}^{-1} \Delta)H_{\thetafull}^{-1}\Delta. 
\end{align*}
Recall that G is the gradient of the loss, so here $\nabla G$ is the Hessian evaluated at this $\eta$ perturbed value. Let $H_{\theta_\eta}$ be the Hessian evaluated at the point $\theta_\eta = \thetafull + \eta H_{\thetafull}^{-1}\Delta$. Then we have
\begin{align}
    &= G(\thetafull) + H_{\theta_\eta}H_{\thetafull}^{-1}\Delta, \nonumber \\
    &= G(\thetafull) + \Delta + H_{\theta_\eta}H_{\thetafull}^{-1}\Delta - \Delta.
\end{align}
By our choice of $\Delta$, $G(\thetafull) + \Delta = 0$.
\begin{align}
    G(\thetafull) &= \nabla \mathcal{L}(\thetafull; D'), \nonumber \\
    &= \sum_{i=1}^{n-m} \ell'(\thetafull, x_i, y_i) + (n-m)\lambda\thetafull \nonumber \\
    &+\gamma\frac{n-m}{|C_{D'}|}\sum_{(i,j,k,l)\in C_{D'}}\ell_\textrm{fair}'(\theta, \{(x_f, y_f)\}_{f \in \{i,j,k,l\}}). \label{eqn:G}
\end{align}
Combining \eqref{eqn:delta} and \eqref{eqn:G}, we see $G(\thetafull) + \Delta$ equals 0. 
\begin{align}
    G(\thetafull) + \Delta &= \sum_{i=1}^{n} \ell'(\thetafull, x_i, y_i) + n\lambda\thetafull + \gamma\frac{n}{|N|}\sum_{i,j,k,l \in N}\ell_\textrm{fair}'(\theta, \{(x_f, y_f)\}_{f \in \{i,j,k,l\}}), \label{eqn:G_plus_delta} \\
    &= \nabla \mathcal{L}(\thetafull; D),  \nonumber \\
    &= 0. \nonumber
\end{align}
We see in~\eqref{eqn:G_plus_delta} that $G(\thetafull) + \Delta = \nabla \mathcal{L}(\thetafull; D)$. Then, $\nabla \mathcal{L}(\thetafull; D) = 0$ because our loss function is convex and our optimum occurs at $\thetafull$. 

Returning to the proof of Lemma \ref{lem:bounded_gradient}, we have:
\begin{align}
    G(\thetaunlearn) &= G(\thetafull) + \Delta + H_{\theta_\eta}H_{\thetafull}^{-1}\Delta - \Delta, \nonumber \\
    &= 0 + H_{\theta_\eta}H_{\thetafull}^{-1}\Delta - \Delta, \nonumber \\
    &= H_{\theta_\eta}H_{\thetafull}^{-1}\Delta - H_{\thetafull}H_{\thetafull}^{-1}\Delta, \nonumber\\
    &= (H_{\theta_\eta} - H_{\thetafull})H_{\thetafull}^{-1}\Delta. \label{eqn:simplified_unlearned_gradient}
\end{align}
We want to bound the norm of~\eqref{eqn:simplified_unlearned_gradient}.
\begin{align*}
    ||G(\thetaunlearn)||_2 &= ||(H_{\theta_\eta} - H_{\thetafull})H_{\thetafull}^{-1}\Delta||_2 \\
    &\leq ||H_{\theta_\eta} - H_{\thetafull}||_2||H_{\thetafull}^{-1}\Delta||_2
\end{align*}
First, we bound the left term. Recall we define $H$ as the Hessian evaluated over the remainder dataset $D'$. Also note that $\ell''_\textrm{fair}$ and $\frac{\lambda}{2}||\theta||_2^2$ are constant in $\theta$ so $\ell_\textrm{fair}''(\theta_\eta, \{(x_f, y_f)\}_{f \in \{i,j,k,l\}}) - \ell_\textrm{fair}''(\thetafull, \{(x_f, y_f)\}_{f \in \{i,j,k,l\}}) = 0$. Then we have
\begin{align}
    ||H_{\theta_\eta} - H_{\thetafull}||_2 &= ||\sum_{i=1}^{n-m} \nabla^2\ell(\theta_\eta, x_i, y_i) + \lambda(n-m) + \gamma\frac{n-m}{|C_{D'}|}\sum_{i,j,k,l\in C_{D'}}\ell_\textrm{fair}''(\theta_\eta, \{(x_f, y_f)\}_{f \in \{i,j,k,l\}}) \nonumber\\ 
     - \sum_{i=1}^{n-m}& \nabla^2\ell(\thetafull, x_i, y_i) - \lambda(n-m) - \gamma\frac{n-m}{|C_{D'}|}\sum_{i,j,k,l\in C_{D'}}\ell_\textrm{fair}''(\thetafull, \{(x_f, y_f)\}_{f \in \{i,j,k,l\}})||_2, \nonumber \\
    &\leq \sum_{i=1}^{n-m} || \nabla^2 \ell (\theta_\eta, x_i, y_i)-\nabla^2 \ell(\thetafull,x_i,y_i)||_2, \nonumber \\
    &= \sum_{i=1}^{n-m}|| x_i^T(\ell'' (\theta_\eta, x_i, y_i)-\ell''(\thetafull,x_i,y_i)) x_i||_2, \label{eqn:chainrule}  \\
    &\leq \sum_{i=1}^{n-m}||\ell'' (\theta_\eta, x_i, y_i)-\ell''(\thetafull, x_i,y_i)||_2||x_i||_2^2, \nonumber \\
    &\leq \sum_{i=1}^{n-m}\psi ||\theta_\eta - \thetafull ||_2||x_i||_2^2, \label{eqn:lipschitz} \\ 
    &\leq \sum_{i=1}^{n-m}\psi ||\theta_\eta - \thetafull ||_2, \label{eqn:x_simplify} \\
    &= (n-m)\psi ||\theta_\eta - \thetafull ||_2, \nonumber \\
    &= (n-m)\psi ||\thetafull + \eta H_{\thetafull}^{-1}\Delta - \thetafull||_2, \nonumber \\
    &= (n-m)\psi ||\eta H_{\thetafull}^{-1}\Delta||_2, \nonumber \\
    &\leq  (n-m)\psi ||H_{\thetafull}^{-1}\Delta||_2,
\end{align}
where \eqref{eqn:chainrule} comes from chain rule, \eqref{eqn:lipschitz} comes from our assumption $\ell''$ is $\psi$-Lipschitz, and \eqref{eqn:x_simplify} comes from our assumption that $||x_i||_2 \leq 1$.

Thus, we have:
\begin{align}
    ||G(\thetaunlearn)||_2 &\leq ||H_{\theta_\eta} - H_{\thetafull}||_2||H_{\thetafull}^{-1}\Delta||_2, \nonumber \\
    &\leq (n-m)\psi ||H_{\thetafull}^{-1}\Delta||_2^2
\end{align}
We bound the quantity $||H_{\thetafull}^{-1}\Delta||_2 $. Remember we are evaluating our Hessians and gradients over the remainder dataset, $D'$. With a convex $\ell$ and $\ell_\textrm{fair}$ where $\nabla^2\ell, \nabla^2\ell_\textrm{fair}$ are positive semi-definite and regularization of strength $\frac{n\lambda}{2}$, our loss function is $n\lambda$-strongly convex. Thus, our Hessian over the remainder dataset (of size $n-m$) is bounded:
\begin{align}
    ||H_{\theta}||_2 \geq (n-m)\lambda \nonumber
\end{align}
And so is its inverse 
\begin{align}
    ||H_{\theta}^{-1}||_2 \leq \frac{1}{(n-m)\lambda} \label{hessinv}
\end{align}
Finally, we bound $\Delta$. Recall its definition:
\begin{align}
    &\Delta := \sum_{i=m}^{n}\ell'(\thetafull, x_i, y_i) + m\lambda \thetafull+\gamma\frac{n}{|N|} \sum_{(i,j,k,l) \in N}\ell_\textrm{fair}'(\thetafull, \{(x_f, y_f)\}_{f \in \{i,j,k,l\}})  \nonumber\\
    &-\gamma\frac{n-m}{|C_{D'}|}\sum_{(i,j,k,l)\in C_{D'}}\ell_\textrm{fair}'(\thetafull, \{(x_f, y_f)\}_{f \in \{i,j,k,l\}}).
\end{align}
We bound $\thetafull$ to bound our second term. Recall that the gradient over the full dataset at $\thetafull$ equals 0. 
\begin{align}
    0 &= \nabla \mathcal{L}(\thetafull, D) = \sum_{i=1}^{n} \ell'(\thetafull, x_i, y_i) + n\lambda\thetafull + \gamma\frac{n}{|N|} \sum_{i,j,k,l \in N}\ell_\textrm{fair}'(\thetafull, \{(x_f, y_f)\}_{f \in \{i,j,k,l\}}) \label{eqn:stationary_condition}
\end{align}
We expand $\ell'_\textrm{fair}$ and see it is linear in $\theta$. Denote this coefficient $F(i,j,k,l)$.
\begin{align}
    \ell_\textrm{fair}'(\thetafull, \{(x_f, y_f)\}_{f \in \{i,j,k,l\}})  &= 2\cdot\mathbbm{1}[y_i = y_j]\mathbbm{1}[y_k = y_l] (x_i^T x_k - x_i^T x_l - x_j^T x_k + x_j^T x_l)\thetafull \nonumber \\
    &= F(i,j,k,l)\thetafull
\end{align}
We can bound $\thetafull$ by taking the stationary point condition~\eqref{eqn:stationary_condition} and solving for $\thetafull$. Using the assumption the ERM loss is bounded $||\ell'(\theta, x_i, y_i)||_2 \leq g$, we see:
\begin{align}
    \thetafull &= \frac{-\sum_{i=1}^{n} \ell'(\thetafull, x_i, y_i)}{n\lambda + \gamma\frac{n}{|N|} \sum_{i,j,k,l \in N}F(i,j,k,l)}, \nonumber\\
    ||\thetafull||_2 &= \left|\left| \frac{\sum_{i=1}^{n} \ell'(\thetafull, x_i, y_i)}{n\lambda + \gamma\frac{n}{|N|} \sum_{i,j,k,l \in N}F(i,j,k,l)} \right|\right|_2, \nonumber\\
    ||\thetafull||_2 &\leq ng \left|\left|\frac{1}{n\lambda + \gamma\frac{n}{|N|} \sum_{i,j,k,l \in N}F(i,j,k,l)}\right|\right|_2, \nonumber \\
    ||\thetafull||_2 &\leq g \left|\left|\frac{1}{\lambda + \gamma\frac{1}{|N|} \sum_{i,j,k,l \in N}F(i,j,k,l)}\right|\right|_2  \label{thetanorm}
\end{align}
Now, we can bound the norm of $\Delta$. We use the triangle inequality as well as the fact that $|C_{D'}| < |N|$, and the terms $n > m > 0$ to simplify.
\begin{align}
    ||\Delta||_2 &= \left|\left|\sum_{i=m}^{n}\ell'(\thetafull, x_i, y_i) + \thetafull\left[m\lambda + \gamma\left( \frac{n}{|N|} \sum_{i,j,k,l \in N}F(i,j,k,l) - \frac{n-m}{|C_{D'}|}\sum_{i,j,k,l\in C_{D'}}F(i,j,k,l) \right)\right]\right|\right|_2, \nonumber \\
    &\leq mg + ||\thetafull||_2\left|\left|m\lambda + \gamma\left(\frac{n}{|N|} \sum_{i,j,k,l \in N}F(i,j,k,l) - \frac{n-m}{|C_{D'}|}\sum_{i,j,k,l\in C_{D'}}F(i,j,k,l) \right)\right|\right|_2, \nonumber \\
    &\leq mg + ||\thetafull||_2\left|\left|m\lambda + \gamma\left(\frac{n}{|N|} \sum_{i,j,k,l \in N}F(i,j,k,l) - \frac{n-m}{|N|}\sum_{i,j,k,l\in C_{D'}}F(i,j,k,l)\right)\right|\right|_2, \nonumber \\
    &= mg + ||\thetafull||_2\left|\left|m\lambda + \gamma\left(\frac{n}{|N|}\sum_{i,j,k,l\in N \setminus C_{D'}}F(i,j,k,l)+\frac{m}{|N|} \sum_{i,j,k,l \in C_{D'}}F(i,j,k,l) \right)\right|\right|_2, \nonumber \\
    &= gm + ||\thetafull||_2\left|\left|m\lambda + \gamma\left(\frac{m}{|N|} \sum_{i,j,k,l \in N}F(i,j,k,l) + \frac{n-m}{|N|}\sum_{i,j,k,l\in N \setminus C_{D'}}F(i,j,k,l)\right)\right|\right|_2, \nonumber \\
    &\leq mg + ||\thetafull||_2\left|\left|m\lambda + \gamma\frac{m}{|N|} \sum_{i,j,k,l \in N}F(i,j,k,l)\right|\right|_2+||\thetafull||_2\left|\left|\frac{n-m}{|N|}\sum_{i,j,k,l\in N \setminus C_{D'}}F(i,j,k,l)\right|\right|_2
\end{align}
We can plug in our solution for $||\thetafull||_2$ from \eqref{thetanorm} and we see the second term equals $g$.
\begin{align}
    ||\Delta||_2 &\leq 2mg + ||\thetafull||_2\left|\left|\frac{n-m}{|N|}\sum_{i,j,k,l\in N \setminus C_{D'}}F(i,j,k,l)\right|\right|_2 \label{eqn:deltabound}
\end{align}
Next we analyze the final term. Let, $m_a, m_b$ be the number of removals from subgroups a and b respectively and recall that $n_a, n_b$ is the total number of elements in subgroups a and b respectively. Recall our assumption $||x_i||_2 \leq 1$. Note that $N \setminus C_{D'}$ is the union of two sets; first, the set of removed points from subgroup a ($r_a$) and their interactions with all points in subgroup b ($G_b$), and second the set of removed points from subgroup b ($r_b$) and their interactions with all points in subgroup a ($G_a$). Then, we can compute the cardinality $|N \setminus C_{D'}| = m_a^2n_b^2 + m_b^2n_a^2 - m_a^2m_b^2.$ Note that $|N| = n_a^2n_b^2$. 
\begin{align}
    &||\thetafull||_2\left|\left|\frac{n-m}{|N|}\sum_{i,j,k,l\in N \setminus C_{D'}}F(i,j,k,l) \right|\right|_2 \nonumber\\
    &= ||\thetafull||_2\left|\left|\frac{n-m}{|N|} \sum_{i,j,k,l \in N \setminus C_{D'}}2\cdot\mathbbm{1}[y_i = y_j]\mathbbm{1}[y_k = y_l] (x_i^T x_k - x_i^T x_l - x_j^T x_k + x_j^T x_l) \right|\right|_2, \nonumber \\
    &\leq ||\thetafull||_2\left|\left|\frac{8|N \setminus C_{D'}|(n-m)}{|N|} \right|\right|_2, \nonumber \\
    &= ||\thetafull||_2\left|\left|\frac{8(m_a^2n_b^2 + m_b^2n_a^2 - m_a^2m_b^2)(n-m)}{(n_a^2n_b^2)} \right|\right|_2. \label{eqn:delta_rhs}
\end{align}
Combining \eqref{eqn:deltabound} and \eqref{eqn:delta_rhs}, 
\begin{align}
    ||\Delta||_2 &\leq 2mg + ||\thetafull||_2\left|\left|\frac{8(m_a^2n_b^2 + m_b^2n_a^2 - m_a^2m_b^2)(n-m)}{(n_a^2n_b^2)} \right|\right|_2.
\end{align}
The final form of our gradient norm $||\mathcal{L}(\thetaunlearn, D')||_2 = ||G(\thetaunlearn)||_2$ is the following.
\begin{align}
    ||G(\thetaunlearn)||_2 &\leq (n-m)\psi ||H_{\thetafull}^{-1}\Delta||_2^2, \nonumber \\ 
    &\leq (n-m)\psi ||H_{\thetafull}^{-1}||^2_2||\Delta||_2^2, \nonumber \\
    &\leq (n-m)\psi \frac{1}{(n-m)^2\lambda^2}(2mg + ||\thetafull||_2\left|\left|\frac{8(m_a^2n_b^2 + m_b^2n_a^2 - m_a^2m_b^2)(n-m)}{(n_a^2n_b^2)} \right|\right|_2)^2, \nonumber \\
    &= \frac{\psi}{\lambda^2(n-m)}\left(2mg + ||\thetafull||_2\left|\left|\frac{8(m_a^2n_b^2 + m_b^2n_a^2 - m_a^2m_b^2)(n-m)}{(n_a^2n_b^2)} \right|\right|_2\right)^2. \label{eqn:final_gradient} && \square
\end{align}
Assume the number of samples in subgroups a and b $(n_a, n_b)$ are relatively balanced ($O(n)$), and the number of unlearned samples $(m_a, m_b)$ are sufficiently low, such that $m_a = O(\sqrt{n_a}), m_b = O(\sqrt{n_b})$. Under these assumptions, we see that the term in the parenthesis is $O(1)$, so~\eqref{eqn:final_gradient} is $O(1/n).$ When we let $m=1$, and without loss of generality $m_a = 1, m_b = 0$, we exactly recover Thm. \ref{thm:eps_delta} in the main paper.
\begin{lemma}[\citet{guoCertifiedDataRemoval2020}] \label{guo_noise_bound}
    Let A be the learning algorithm that returns the unique optimum of the perturbed loss $\mathcal{L}(\theta, D) + \textbf{b}^T\theta$, and let M be the an unlearning mechanism that outputs $\thetaunlearn$. Suppose that $||\nabla\mathcal{L}(\thetaunlearn, D')||_2 \leq \epsilon'$ for some $\epsilon' > 0$. Then, we have the following guarantees for M:

    \begin{enumerate}
        \item If \textbf{b} is drawn from a distribution with density $p(\textbf{b}) \propto \exp{-\frac{\epsilon}{\epsilon'}||\textbf{b}||_2}$, then M is $\epsilon$-unlearnable for A;
        \item If $\textbf{b} \sim N(0, k\epsilon'/\epsilon)^d$ with $k > 0$, then M is $(\epsilon, \delta)$-unlearnable for A with $\delta = 1.5\exp(-k^2/2)$
    \end{enumerate}
\end{lemma}
\subsubsection{Proof of Theorem~\ref{thm:eps_delta}:}
By combining Lemmas~\ref{lem:bounded_gradient} and \ref{guo_noise_bound}, we show that the loss gradient over the remaining dataset, evaluated at the unlearned parameter $\thetaunlearn$ is bounded, and we can directly plug this into Lemma~\ref{guo_noise_bound} to achieve $(\delta, \epsilon)$-unlearning for some $\delta, \epsilon > 0$. \hfill $\square$
\subsection{Data-Dependent Bound}\label{sec:datadependentbound} From Eqn.~\eqref{eqn:simplified_unlearned_gradient}, we are computing a bound on the norm of the following:
\begin{align}
    (H_{\theta_\eta} - H_{\thetafull})H_{\thetafull}^{-1}\Delta. \label{eqn:datadependentbound}
\end{align}
The Hessian for $\mathcal{L}(\theta, D')$ is 
\begin{align*}
    X^{T}W_\theta X + \lambda(n-m) + \gamma\frac{n-m}{|C_{D'}|}\sum_{i,j,k,l\in C_{D'}}\ell_\textrm{fair}''(\theta, \{(x_f, y_f)\}_{f \in \{i,j,k,l\}})
\end{align*}
Where $X$ is the data matrix of samples in $D'$, and $W_\theta$ is a diagonal matrix where $W_{ii} = \ell''(\theta, x_i, y_i)$. Then, Eqn.~\eqref{eqn:datadependentbound} becomes
\begin{align}
    (X^T(W_{\theta_\eta} - W_{\thetafull})X)H_{\thetafull}^{-1}\Delta,
\end{align}
which by \citep{guoCertifiedDataRemoval2020}, Corollary 1, shows that 
\begin{align}
    ||\nabla \mathcal{L}(\thetafull; D')||_2 = ||(X^T(W_{\theta_\eta} - W_{\thetafull})X)H_{\thetafull}^{-1}\Delta||_2 \leq \gamma ||X||_2||H^{-1}_{\thetafull}\Delta||_2||XH^{-1}_{\thetafull}\Delta||_2.
\end{align}




\subsection{Proof of Theorem~\ref{thm:fairnesG_bound}}

We begin with a useful corollary of Lemma \ref{lem:bounded_gradient}:

\begin{corollary}[$||\thetaunlearn - \thetaretrain||_2$ is bounded.] \label{corr1}
Recall $\mathcal{L}(\theta, D')$ is $(n-m)\lambda$-strongly convex. Then, by strong convexity,
\begin{align}
    (n-m)\lambda||\thetaunlearn - \thetaretrain||_2^2 &\leq (\nabla \mathcal{L}(\thetaunlearn, \mathcal{D'}) - \nabla\mathcal{L}(\thetaretrain, \mathcal{D'})^T(\thetaunlearn - \thetaretrain), \nonumber \\
    &= (\nabla \mathcal{L}(\thetaunlearn, \mathcal{D'}))^T(\thetaunlearn - \thetaretrain), \nonumber \\
    &\leq ||\nabla\mathcal{L}(\thetaunlearn, \mathcal{D'})||_2 ||\thetaunlearn - \thetaretrain||_2, \nonumber \\
    (n-m)\lambda||\thetaunlearn - \thetaretrain||_2 &\leq  \frac{\psi}{\lambda^2(n-m)}\left(2mg + ||\thetafull||_2\left|\left|\frac{8(m_a^2n_b^2 + m_b^2n_a^2 - m_a^2m_b^2)(n-m)}{(n_a^2n_b^2)} \right|\right|_2\right)^2, \nonumber \\
    ||\thetaunlearn - \thetaretrain||_2 &\leq \frac{\psi}{\lambda^3(n-m)^2}\left(2mg + ||\thetafull||_2\left|\left|\frac{8(m_a^2n_b^2 + m_b^2n_a^2 - m_a^2m_b^2)(n-m)}{(n_a^2n_b^2)} \right|\right|_2\right)^2
\end{align}
\end{corollary}
Next, we introduce the lemmas we will use in this proof.
\begin{lemma}[The interior angle between $\thetaretrain$ and $\thetaunlearn$ is bounded.]\label{boundedangle}
    Let $||\thetaretrain - \thetaunlearn||_2 \leq \kappa$. Define the angle between $\thetaretrain$ and $\thetaunlearn$ as $\phi$. Then,
    \begin{align}
        \phi &\leq \arccos\left(\frac{||\thetaretrain||_2^2 + ||\thetaunlearn||_2^2 - \kappa^2}{2||\thetaretrain||_2||\thetaunlearn||_2}\right), \nonumber \\
        &\leq \arccos\left(\frac{||\thetaretrain||_2^2 + (||\thetaretrain||_2 - \kappa)^2 - \kappa^2}{2||\thetaretrain||_2(||\thetaretrain||_2+\kappa)}\right)\nonumber\\
        &= \arccos\left(\frac{||\thetaretrain||_2 - \kappa}{||\thetaretrain||_2+\kappa}\right)
    \end{align}
\end{lemma}

The first inequality follows directly from applying the cosine angle formula to the triangle described by $\thetaretrain$, $\thetaunlearn$, and $\thetaretrain-\thetaunlearn$ and the fact that $||\thetaretrain-\thetaunlearn||_2 \leq \kappa$. Then, we use the triangle inequality to show 
\begin{align}
    ||\thetaunlearn||_2 \leq ||\thetaretrain||_2 + ||\thetaunlearn-\thetaretrain||_2 \leq ||\thetaretrain||_2 + \kappa, \nonumber \\
    ||\thetaunlearn||_2 \geq ||\thetaretrain||_2 - ||\thetaretrain-\thetaunlearn||_2 \geq ||\thetaretrain||_2 - \kappa. \nonumber
\end{align}
This lemma implies that as $\kappa$ approaches $0$, the angle $\phi$ approaches zero as well.

\begin{lemma}[Volume of internal segment of a d-sphere]\label{boundedvolume}
    Let $S$ be a $d$-sphere with radius 1 and volume $V = \frac{\pi^{d/2}}{\Gamma(\frac{d}{2}+1)}$. Then, let $\theta_1$, $\theta_2$ be normal vectors defining two hyperplanes such that their intersection lies tangent to the surface of $S$ with internal angle $\phi$ such that the angle bisector to $\phi$ also bisects $S$. Then, the volume of the region of intersection $B$ between the half spaces defined by the hyperplanes at $\phi$ within $S$ equals
    \begin{align}
        B &= V - 2\int_{\mu}^{1}\frac{\pi^{\frac{d-1}{2}}}{\Gamma(\frac{d+1}{2})}(1-y^2)^{d-1} dy, \nonumber \\
        \mu &= \frac{1}{2}\sin{\frac{\phi}{2}}. \nonumber
    \end{align}
\end{lemma}

We take the full volume $V$ and subtract off the volume of the spherical segment defined as the integral of the volume for a $d-1$-sphere over various radius values along the hyperplane to the surface of the sphere. Note that as $\phi \rightarrow 0, \mu \rightarrow 0$, and therefore $B \rightarrow 0$ as the right term approaches $V$.
\subsubsection{Proof of Theorem~\ref{thm:fairnesG_bound}:}
Let $\thetaretrain$ have a mean absolute equalized odds upper bounded by $\alpha$, or
\begin{align}
    AEOD(\thetaretrain) \leq \alpha.
\end{align}
This implies
\begin{align}
    |P(h_{\thetaretrain}(X) = 1 \mid S=a,Y=1)-P(h_{\thetaretrain}(X) = 1 \mid S=b,Y=1)| \leq \alpha, \\
    |P(h_{\thetaretrain}(X) = 1 \mid S=a,Y=0)-P(h_{\thetaretrain}(X) = 1 \mid S=b,Y=0)| \leq \alpha. 
\end{align}
Define $A_{\thetaretrain} = \{x : h_{\thetaretrain}(x) = 1\}, A_{\thetaunlearn} = \{x : h_{\thetaunlearn}(x) = 1\}$. Then,
\begin{align}
    |P(A_{\thetaretrain} \mid S=a,Y=1)-P(A_{\thetaretrain} \mid S=b, Y=1)| \leq \alpha, \\
    |P(A_{\thetaretrain} \mid S=a,Y=0)-P(A_{\thetaretrain} \mid S=b,Y=0)| \leq \alpha. 
\end{align}
Now, define the regions of agreement as $A = A_{\thetaretrain} \cap A_{\thetaunlearn}, B = A_{\thetaretrain}^C \cap A_{\thetaunlearn}^C$ and the regions of disagreement as $C = A_{\thetaretrain}^C \cap A_{\thetaunlearn}, D = A_{\thetaretrain} \cap A_{\thetaunlearn}^C$. These events are disjoint, so we can decompose our conditional probability measures in to sums over these events and express $P(A_{\thetaunlearn}\mid S,Y)$ in terms of $P(A_{\thetaretrain}\mid S,Y)$
\begin{align}
    P(A_{\thetaretrain}\mid S,Y) = P(A\mid S,Y) + P(D\mid S,Y), \nonumber\\
    P(A_{\thetaunlearn}\mid S,Y) = P(A\mid S,Y) + P(C\mid S,Y), \nonumber\\
    P(A_{\thetaunlearn}\mid S,Y) = P(A_{\thetaretrain}\mid S,Y) + P(C\mid S,Y) - P(D\mid S,Y).
\end{align}
Then, we can bound the absolute difference:
\begin{align}
    |P(A_{\thetaunlearn}&\mid S=a,Y=1) - P(A_{\thetaunlearn}\mid S=b,Y=1)| \nonumber\\
    =& |P(A_{\thetaretrain}\mid S=a,Y=1)-P(A_{\thetaretrain}\mid S=b,Y=1) \nonumber\\
    &+P(C \mid, S=a, Y=1) - P(C \mid, S=b, Y=1) \nonumber\\
    &- P(D \mid, S=a, Y=1) + P(D \mid, S=b, Y=1)|, \nonumber\\
    \leq& |P(A_{\thetaretrain}\mid S=a,Y=1)-P(A_{\thetaretrain}\mid S=b,Y=1) \nonumber\\
    &+ |P(C \mid, S=a, Y=1) - P(C \mid, S=b, Y=1)| \nonumber\\
    &+ |P(D \mid, S=b, Y=1) - P(D \mid, S=a, Y=1)|, \nonumber\\
    \leq& \alpha + |P(C \mid, S=a, Y=1) - P(C \mid, S=b, Y=1)| \nonumber\\
    &+ |P(D \mid, S=b, Y=1) - P(D \mid, S=a, Y=1)|. \label{basebound}
\end{align}
Next, we relate the probability measures $P(\cdot \mid S, Y)$ to the size of their event spaces to establish a bound on the second and third terms in Equation~\ref{basebound}. We assume for all events $x \subseteq \mathcal{X}$, $P(x | S, Y) \leq c \frac{|x|}{|\mathcal{X}|}$, or in other words, that our probability mass is relatively well distributed up to a constant. Then, computing $|C|$ and $|D|$ will provide upper bounds on their conditional likelihoods.

Recall our assumption that $||x_i||_2 \leq 1$. This means all of our data lies within a unit $d$-sphere we call $S$, which defines our event space $\mathcal{X}$ and has volume $V = |\mathcal{X}| = \frac{\pi^{d/2}}{\Gamma(\frac{d}{2}+1)}$. We can define events $A_{\thetaretrain}$ and $A_{\thetaunlearn}$ the halfspaces defined by $\thetaretrain$ and $\thetaunlearn$ intersected with $S$. Then, $A, B, C, \text{ and } D$ partition $S$ into four disjoint volumes. We seek to bound the volume of $C$ and $D$. First observe that the largest volume for $C + D$ occurs when $\thetaretrain, \thetaunlearn$ intersect tangent to $S$ and create a region where the angle between $\thetaretrain$ and $\thetaunlearn$ is bisected by a hyperplane that also bisects $S$. This creates a region where one of $C,D$ is empty and the other draws a large slice through $S$. Define the angle between $\thetaretrain$ and $\thetaunlearn$ as $\phi$. Without loss of generality, assume $|D| = 0$. Then, by Lemmas~\ref{boundedangle} and~\ref{boundedvolume}, if $||\thetaretrain - \thetaunlearn||_2 \leq d$, then $\phi$ is bounded and thus $|C|$ is bounded.
\begin{align}
    |C| &= V - 2\int_{\mu}^{1}\frac{\pi^{\frac{d-1}{2}}}{\Gamma(\frac{d+1}{2})}(1-y^2)^{d-1} dy,
    \\
    \phi &\leq \arccos\left(\frac{||\thetaretrain||_2 - \kappa}{||\thetaretrain||_2+\kappa}\right),
\end{align}
where
\begin{align}
    \mu &= \frac{1}{2}\sin{\frac{\phi}{2}}, \nonumber \\
    &= \frac{1}{2}\sqrt{\frac{1-\cos{\phi}}{2}}, \nonumber \\
    &\leq \frac{\sqrt{\kappa}}{2}.
\end{align}
by the double angle formula for cosine.

Returning to Equation~\ref{basebound}, we have
\begin{align}
    |P(A_{\thetaunlearn}&\mid S=a,Y=1) - P(A_{\thetaunlearn}\mid S=b,Y=1)| \nonumber \\
    &\leq \alpha + |P(C \mid, S=a, Y=1) - P(C \mid, S=b, Y=1)| \nonumber\\
    &+ |P(D \mid, S=b, Y=1) - P(D \mid, S=a, Y=1)|, \nonumber \\
    &\leq \alpha + \max(P(C \mid, S=a, Y=1), P(C \mid, S=b, Y=1)) \nonumber \\
    &+ \max(P(D \mid, S=b, Y=1), P(D \mid, S=a, Y=1)), \nonumber \\
    &\leq \alpha + c\frac{|C|}{|\mathcal{X}|} + c\frac{|D|}{|\mathcal{X}|}, \nonumber \\
    &= \alpha + c\frac{V- 2\int_{\mu}^{1}\frac{\pi^{\frac{d-1}{2}}}{\Gamma(\frac{d+1}{2})}(1-y^2)^{d-1} dy}{V}, \nonumber \\
    &= \alpha + c\left(1- \frac{2\int_{\mu}^{1}\frac{\pi^{\frac{d-1}{2}}}{\Gamma(\frac{d+1}{2})}(1-y^2)^{d-1} dy}{V}\right),
\end{align}
where 
\begin{align}
    \mu \leq \frac{\sqrt{\kappa}}{2}. \nonumber
\end{align}
By a symmetric argument, we also see
\begin{align}
    |P(A_{\thetaunlearn}&\mid S=a,Y=0) - P(A_{\thetaunlearn}\mid S=b,Y=0)| \leq \alpha + c\left(1- \frac{2\int_{\mu}^{1}\frac{\pi^{\frac{d-1}{2}}}{\Gamma(\frac{d+1}{2})}(1-y^2)^{d-1} dy}{V}\right).
\end{align}
And thus we have that the absolute mean equalized odds of $\thetaunlearn$ is bounded:
\begin{align}
    \frac{1}{2}\big[|P(A_{\thetaunlearn}&\mid S=a,Y=1) - P(A_{\thetaunlearn}\mid S=b,Y=1)| \nonumber \\
    &+ |P(A_{\thetaunlearn}\mid S=a,Y=1) - P(A_{\thetaunlearn}\mid S=b,Y=1)|\big], \nonumber \\
    &\leq \alpha + c\left(1- \frac{2\int_{\mu}^{1}\frac{\pi^{\frac{d-1}{2}}}{\Gamma(\frac{d+1}{2})}(1-y^2)^{d-1} dy}{V}\right), 
\end{align}
where
\begin{align}
    h \leq \frac{\sqrt{\kappa}}{2}. \tag*{$\square$} \nonumber
\end{align}
\subsection{Incompatibility of existing unlearning methods.}
\label{sec:incompatible}

Existing unlearning methods require a decomposition of the loss term as $\mathcal{L}(\theta) = \frac{1}{n} \sum_{i=1}^n \ell(\theta)$. For example, the most similar method to Fair Unlearning comes from~\citet{guoCertifiedDataRemoval2020}, and their unlearning step involves a second order approximation over $\Delta = \lambda \thetafull + \nabla\ell(\thetafull, x_n, y_n)$ to unlearn datapoint $(x_n, y_n)$. Similarly to our proof of Lemma~\ref{lem:bounded_gradient}, adding $\Delta$ with the gradient of $\thetaunlearn$ over the remaining dataset $\mathcal{L}(\thetaunlearn, D')$ results in a cancellation that enables the rest of their proof to bound the gradient. However, in their case $\Delta$ is directly corresponds to the gradient over sample $(x_n, y_n)$ only because of the decomposability of their loss term. This behavior makes all unlearning over decomposable loss functions the same: compute a Newton step $H^{-1}\Delta$ where $H$ is the hessian over $D'$ and $\Delta$ is the gradient over $R$. In our case our $\Delta$ is more complex because the fairness loss involves the interactions between each sample and samples in the opposite subgroup. Thus, removing one point requires removing a set of terms from the fairness loss, and thus $\Delta$ must be a more complex term to account for this rather than simply the gradient over $(x_n, y_n)$.

\subsection{Extension of fairness penalty to other definitions of group fairness.} Recall the definition of Equalized Odds:
\begin{align}
    \Pr(h_\theta(X) = 1 \mid S = a, Y = y) = \Pr(h_\theta(X) = 1 \mid S = b, Y = y).
\end{align}
In contrast, Demographic Parity does not condition on the true label, $Y$.
\begin{align}
    \Pr(h_\theta(X) = 1 \mid S = a) = \Pr(h_\theta(X) = 1 \mid S = b).
\end{align}
Thus, to achieve this, we remove the indicator variable in our loss function.
\begin{align}
    &\mathcal{L}_\textrm{Demo. Par.}(\theta, D) := \left(\frac{1}{n_an_b} \sum_{i \in G_a}\sum_{j \in G_b}(\langle x_i,\theta\rangle - \langle x_j, \theta\rangle) \right)^2.
\end{align}
Equality of opportunity conditions only on true positive labels, $Y = 1$. 
\begin{align}
    \Pr(h_\theta(X) = 1 \mid S = a, Y=1) = \Pr(h_\theta(X) = 1 \mid S = b, Y=1).
\end{align}
Thus, to achieve this, we modify the indicator variable in our loss function to only consider true positives.
\begin{align}
    &\mathcal{L}_\textrm{Eq. Opp.}(\theta, D) := \left(\frac{1}{n_an_b} \sum_{i \in G_a}\sum_{j \in G_b} \mathbbm{1}[y_i = y_j = 1](\langle x_i,\theta\rangle - \langle x_j, \theta\rangle) \right)^2.
\end{align}
\subsection{Experiment details.} \label{sec:hyperparameters} We run all of our experiments on a 32GB Tesla V100 GPU, but using tabular data and small models, these results can be replicated with weaker hardware. When training, we use $\sigma = 1$ for our objective perturbation and compute $\epsilon, \delta$ bounds with $\delta = 0.0001$. We report the following hyperparameters for our final results in the main paper:
\begin{table}[h!]
    \centering
    \begin{tabular}{ccc}
    \toprule
         & $\ell_2$ Penalty ($\lambda$) & Fairness Penalty ($\gamma$) \\
    \midrule
       COMPAS  & $1\mathrm{e}-4$ & $1\mathrm{e}1$\\
       Adult  & $1\mathrm{e}-4$ & $1\mathrm{e}0$\\
       HSLS  & $1\mathrm{e}-2$ & $1\mathrm{e}1$\\
    \bottomrule
    \end{tabular}
    \caption{Hyperparameters for experiments in paper.}
    \label{tab:hyperparameters}
\end{table}
\subsection{Runtimes.} \label{sec:runtimes}
Below we include runtime comparisons (Table \ref{tab:runtimes}) for our method with naive retraining as well as precomputation times for COMPAS, Adult, and HSLS (Table \ref{tab:precomputation_times}). Our method achieves significant speedup over retraining a model.
\begin{table*}[h!]
\centering
\caption{Runtime Comparisons for Proposed Method vs Retraining.}
\label{tab:runtimes}
\resizebox{\linewidth}{!}{
\begin{tabular}{lcccccc} 
\toprule
& \multicolumn{2}{c}{COMPAS}             & \multicolumn{2}{c}{Adult}             & \multicolumn{2}{c}{HSLS}               \\
Method                                & 5\% Unlearned       & 20\% Unlearned       & 5\% Unlearned       & 20\% Unlearned      & 5\% Unlearned       & 20\% Unlearned       \\ 
\midrule
Retraining               & $3.500 \pm 0.340$ s & $3.180 \pm 0.142$ s & $36.796 \pm 1.090$ s & $31.175 \pm 1.040$ s & $14.963 \pm 0.396$ s& $12.362 \pm 0.795$ s \\
Fair Unlearning (Ours)                            & $0.170 \pm 0.010$ s & $0.164 \pm 0.003$ s& $8.800 \pm 0.511$ s & $8.483 \pm 0.938$ s & $6.014 \pm 0.249$ s & $5.704 \pm 0.015$ s  \\
\bottomrule
\end{tabular}
}
\end{table*}

\begin{table*}[h!]
\centering
\caption{Precomputation Runtime Comparisons for Hessian Inversion.}
\label{tab:precomputation_times}
\begin{tabular}{lccc} 
\toprule
& COMPAS & Adult & HSLS \\
Precomputation Time: & $0.182 \pm 0.006$s & $6.210 \pm 0.041$s & $4.964 \pm 0.117$s\\
\bottomrule
\end{tabular}
\end{table*}

\newpage

\subsection{Test accuracy when unlearning.}\label{appendix:accuracy}
In Tables \ref{tab:unlearning at random}, \ref{tab:unlearning from minority}, and \ref{tab:unlearning from majority}, we report test accuracy at various amounts of unlearning with standard deviation.
\begin{table*}[h!]
\centering
\caption{Test accuracy when unlearning at random.}
\label{tab:unlearning at random}
\resizebox{\linewidth}{!}{%
\begin{tabular}{lcccccc} 
\toprule
                                      & \multicolumn{2}{c}{COMPAS}             & \multicolumn{2}{c}{Adult}             & \multicolumn{2}{c}{HSLS}               \\
Method                                & 5\% Unlearned       & 20\% Unlearned       & 5\% Unlearned       & 20\% Unlearned      & 5\% Unlearned       & 20\% Unlearned       \\ 
\midrule
Full Training (BCE)                & $0.652 \pm 0.000$ & $0.652 \pm 0.000 $ & $0.817 \pm 0.000$ & $0.816 \pm 0.000$ & $0.724 \pm 0.000$ & $0.724 \pm 0.000$  \\
Retraining (BCE)            & $0.652 \pm 0.001$ & $0.654 \pm 0.003$  & $0.817 \pm 0.000$ & $0.817 \pm 0.001$ & $0.724 \pm 0.002$ & $0.724 \pm 0.002$  \\
Newton (\citet{guoCertifiedDataRemoval2020})  & $0.652 \pm 0.001$ & $0.654 \pm 0.003$  & $0.817 \pm 0.000$ & $0.817 \pm 0.001$ & $0.724 \pm 0.002$ & $0.724 \pm 0.002$  \\
SISA (\citet{bourtouleMachineUnlearning2020}) & $0.653 \pm 0.001$ & $0.656 \pm 0.004$  & $0.817 \pm 0.000$ & $0.817 \pm 0.001$ & $0.725 \pm 0.001$ & $0.724 \pm 0.003$  \\
PRU (\citet{izzoApproximateDataDeletion})     & $0.688 \pm 0.005$ & $0.691 \pm 0.001$  & $0.810 \pm 0.003$ & $0.810 \pm 0.001$ & $0.726 \pm 0.004$ & $0.729 \pm 0.000$  \\ 
\midrule
Full Training (Fair)                   & $0.647 \pm 0.000$ & $0.647 \pm 0.000$  & $0.819 \pm 0.000$ & $0.819 \pm 0.000$ & $0.713 \pm 0.000$ & $0.713 \pm 0.000$  \\
Retraining (Fair)               & $0.646 \pm 0.002$ & $0.642 \pm 0.003$  & $0.819 \pm 0.000$ & $0.819 \pm 0.001$ & $0.713 \pm 0.002$ & $0.713 \pm 0.002$  \\
Fair Unlearning (Ours)                            & $0.646 \pm 0.002$ & $0.642 \pm 0.003$  & $0.819 \pm 0.000$ & $0.819 \pm 0.001$ & $0.713 \pm 0.002$ & $0.713 \pm 0.002$  \\
\bottomrule
\end{tabular}
}
\end{table*}

\begin{table}[h!]
\centering
\caption{Test accuracy when unlearning from minority subgroups.}
\label{tab:unlearning from minority}
\resizebox{\linewidth}{!}{%
\begin{tabular}{lcccccc} 
\toprule
                                   & \multicolumn{2}{c}{COMPAS}            & \multicolumn{2}{c}{Adult}             & \multicolumn{2}{c}{HSLS}               \\
Method                             & 5\% Unlearned       & 10\% Unlearned      & 5\% Unlearned       & 10\% Unlearned      & 5\% Unlearned       & 10\% Unlearned       \\ 
\midrule
Full Training (BCE)                & $0.652 \pm 0.000$ & $0.652 \pm 0.000$ & $0.821 \pm 0.000$ & $0.821 \pm 0.000$ & $0.729 \pm 0.000$ & $0.729 \pm 0.000$  \\
Retraining (BCE)                   & $0.651 \pm 0.002$ & $0.652 \pm 0.001$ & $0.821 \pm 0.000$ & $0.822 \pm 0.000$ & $0.729 \pm 0.001$ & $0.730 \pm 0.001$  \\
Newton (\citet{guoCertifiedDataRemoval2020}) & $0.651 \pm 0.002$ & $0.652 \pm 0.001$ & $0.821 \pm 0.000$ & $0.823 \pm 0.000$ & $0.729 \pm 0.001$ & $0.730 \pm 0.002$  \\
SISA (\citet{bourtouleMachineUnlearning2020})   & $0.653 \pm 0.001$ & $0.654 \pm 0.002$ & $0.822 \pm 0.000$ & $0.823 \pm 0.000$ & $0.728 \pm 0.001$ & $0.728 \pm 0.001$  \\
PRU (\citet{izzoApproximateDataDeletion})    & $0.661 \pm 0.007$ & $0.666 \pm 0.007$ & $0.819 \pm 0.004$ & $0.815 \pm 0.035$ & $0.720 \pm 0.004$ & $0.724 \pm 0.003$  \\ 
\midrule
Full Training (Fair)               & $0.653 \pm 0.000$ & $0.653 \pm 0.000$ & $0.812 \pm 0.000$ & $0.812 \pm 0.000$ & $0.723 \pm 0.000$ & $0.723 \pm 0.000$  \\
Retraining (Fair)                  & $0.653 \pm 0.001$ & $0.661 \pm 0.005$ & $0.813 \pm 0.000$ & $0.816 \pm 0.001$ & $0.713 \pm 0.002$ & $0.713 \pm 0.002$  \\
Fair Unlearning (Ours)             & $0.652 \pm 0.001$ & $0.661 \pm 0.004$ & $0.814 \pm 0.000$ & $0.816 \pm 0.002$ & $0.724 \pm 0.001$ & $0.721 \pm 0.001$  \\
\bottomrule
\end{tabular}
}
\end{table}

\begin{table}[h!]
\centering
\caption{Test accuracy when unlearning from majority subgroups.}
\label{tab:unlearning from majority}
\resizebox{\linewidth}{!}{%
\begin{tabular}{lcccccc} 
\toprule
                                      & \multicolumn{2}{c}{COMPAS}            & \multicolumn{2}{c}{Adult}             & \multicolumn{2}{c}{HSLS}               \\
Method                                & 5\% Unlearned       & 20\% Unlearned      & 5\% Unlearned       & 20\% Unlearned      & 5\% Unlearned       & 20\% Unlearned       \\ 
\midrule
Full Training (BCE)                & $0.654 \pm 0.000$ & $0.654 \pm 0.000$ & $0.814 \pm 0.000$ & $0.814 \pm 0.000$ & $0.738 \pm 0.000$ & $0.738 \pm 0.000$  \\
Retraining (BCE)            & $0.654 \pm 0.001$ & $0.652 \pm 0.002$ & $0.814 \pm 0.000$ & $0.814 \pm 0.000$ & $0.736 \pm 0.000$ & $0.736 \pm 0.002$  \\
Newton (\citet{guoCertifiedDataRemoval2020})  & $0.654 \pm 0.001$ & $0.652 \pm 0.002$ & $0.814 \pm 0.000$ & $0.814 \pm 0.001$ & $0.736 \pm 0.000$ & $0.735 \pm 0.002$  \\
SISA (\citet{bourtouleMachineUnlearning2020}) & $0.654 \pm 0.002$ & $0.653 \pm 0.004$ & $0.814 \pm 0.000$ & $0.815 \pm 0.001$ & $0.734 \pm 0.001$ & $0.733 \pm 0.002$  \\
PRU (\citet{izzoApproximateDataDeletion})     & $0.696 \pm 0.006$ & $0.688 \pm 0.003$ & $0.812 \pm 0.001$ & $0.811 \pm 0.002$ & $0.723 \pm 0.012$ & $0.731 \pm 0.004$  \\ 
\midrule
Full Training (Fair)                   & $0.666 \pm 0.000$ & $0.666 \pm 0.000$ & $0.815 \pm 0.000$ & $0.815 \pm 0.000$ & $0.725 \pm 0.000$ & $0.725 \pm 0.000$  \\
Retraining (Fair)               & $0.663 \pm 0.004$ & $0.653 \pm 0.002$ & $0.815 \pm 0.000$ & $0.813 \pm 0.000$ & $0.724 \pm 0.001$ & $0.725 \pm 0.002$  \\
Fair Unlearning (Ours)                            & $0.663 \pm 0.004$ & $0.653 \pm 0.003$ & $0.815 \pm 0.000$ & $0.814 \pm 0.001$ & $0.724 \pm 0.001$ & $0.725 \pm 0.002$  \\
\bottomrule
\end{tabular}
}
\end{table}

\newpage 
% \subsection{Experiments with Deep Networks.} FIXME, add plots.

\subsection{Fairness-Accuracy Tradeoffs.}

Below we report the impact of the fairness regularizer on the fairness and accuracy performance of the model without unlearning. We see that the fairness regularizer has a tangible impact and controls a tradeoff between the fairness and accuracy of the model. This motivates the selection of our fairness penalty, $\gamma$, as detailed in \ref{sec:hyperparameters} where we see a good balance between fairness and accuracy performance.

% Figure environment removed

\subsection{Figures for various fairness metrics.}

Below we report figures for Demographic Parity, Equality of Opportunity, and Subgroup Accuracy. These results reflect the results shown in the main body of the paper, with the exception of Subgroup Accuracy which has more variable results, and sometimes worse results for fair methods. However, this is because our fairness regularizer optimizes for group fairness metrics such as Equalized Odds, Equality of Opportunity, and Demographic Parity which are metrics based on individual true and false positive rates rather than the raw test accuracy of each subgroup. 

% Figure environment removed

% Figure environment removed
\newpage

% Figure environment removed