In this section, we evaluate the effectiveness of fair unlearning in terms of both fairness and accuracy in the scenario where data points are deleted at random. Next, we consider more extreme settings for unlearning where data points are deleted disproportionately from a specific subgroup.

\subsection{Experimental Setup}

We evaluate the proposed fair unlearning method on three real-world datasets using a logistic regression model. However, note that previous work has shown combining an unlearnable linear layer with a DP-SGD trained neural network preserves unlearning guarantees~\citep{guoCertifiedDataRemoval2020}. We simulate three types of data deletion requests and compare various unlearning methods with retraining. We also report the metrics of the original model trained on the full training set as a reference. For all results we repeat our experiments five times and report mean and standard deviation. Note that the effectiveness of erasing the influence of requested data points is guaranteed by our theory rather than evaluated empirically, as is common in prior unlearning literature~\citep{guoCertifiedDataRemoval2020,neelDescenttoDeleteGradientBasedMethods2020a}.

\paragraph{Datasets.} We consider three datasets commonly used in the machine learning fairness literature. 1) \emph{COMPAS}~\citep{angwin2016machine} is a classic fairness benchmark with the task of predicting a criminal defendant's likelihood of recidivism. 2) \emph{Adult} from the UCI Machine Learning Repository~\citep{asuncion2007uci} is a large scale dataset with the task of predicting income bracket. 3) High School Longitudinal Study (\emph{HSLS}) is a dataset containing survey responses from high school students and parents with the task of predicting academic year~\citep{ingels2011high,jeong2022fairness}. For all datasets, we use the individual's race as the sensitive group identity.

\paragraph{Data Deletion Settings.} We simulate data deletions in three settings: 1) \emph{random deletion}, where data deletion requests come randomly from the training set independent of subgroup membership; 2) \emph{minority deletion}, where data deletion requests come randomly from the minority group of the training set; 3) \emph{majority deletion}, where data deletion requests come randomly from the majority group of the training set. 
\paragraph{Evaluation Metrics.} We report experimental results on \emph{Absolute Equalized Odds Difference} (AEOD) as defined in Eq.~(\ref{eq:aeod}), with results for Demographic Parity, Equality of Opportunity, and Statistical Parity in the Appendix. In addition, we explore the tradeoffs between $\epsilon$ and $\delta$ which control different strengths of unlearning. Finally, we report runtime, test accuracy, and fairness-accuracy tradeoffs for $\gamma$ in the Appendix.

% Figure environment removed
\vspace{-10pt}
% Figure environment removed

\paragraph{Baseline Methods.} 
We compare our method with four baselines designed for unlearning with BCE loss (Eq.~(\ref{bceloss})). In addition to brute-force retraining with BCE loss (named as \textbf{Retraining (BCE)}), we consider three unlearning methods from existing literature. The three methods are 1) \textbf{Newton}~\citep{guoCertifiedDataRemoval2020} a second-order approach that takes a single Newton step to approximate retraining; 2) \textbf{PRU}~\citep{izzoApproximateDataDeletion} which uses a statistical technique called the residual method to efficiently approximate the Hessian used in the Newton step; and 3) \textbf{SISA}~\citep{bourtouleMachineUnlearning2020} a method that splits the dataset into shards and trains a model on each shard, while unlearning by retraining each model separately to speed up the process. In addition, we compare our \textbf{Fair Unlearning} method designed for unlearning with fair loss (Eq.~(\ref{fairloss})) to brute-force retraining with fair loss (named as \textbf{Retraining (Fair)}). Finally, as references, we also evaluate models trained on the full training set with BCE loss and fair loss, respectively named as \textbf{Full Training (BCE)} and \textbf{Full Training (Fair)}. Note that the goal of all machine unlearning methods is to approximate retraining.

\paragraph{Runtime Complexity.} We compare the runtimes of our unlearning method and retraining over various removal sizes and report results in the Appendix. We also report precomputation times for Hessian inversion. Overall we see a $2\times$ to $20\times$ speedup with our method over retraining.

\paragraph{Unlearning Leakage.} We report the unlearning leakage of our method for various levels of noise variance using the bound shown in Thm.~\ref{thm:eps_delta}. For small datasets, \citet{guoCertifiedDataRemoval2020} propose a tighter data dependent bound which we adapt to fair unlearning in Appendix \ref{sec:datadependentbound}. We pick the lower value of $\epsilon$ between the two bounds for our reported results. For various magnitudes of $\sigma$, we fix $\delta = 0.0001$ and compute the accuracy of the model unlearned with our method in addition to the leakage, $\epsilon$. Values are reported in Fig.~\ref{fig:unlearning leakage}. We can achieve realistic values of $\epsilon$ with reasonable amounts of noise while preserving performance.

\subsection{Experimental Results on Fairness}
% Figure environment removed
\paragraph{Deletion at Random.}
We evaluate our method in terms of fairness when unlearning at random from our training dataset. For each of our datasets, and for both BCE and fair loss, we train a base logistic regression model on the full training dataset. Then, we randomly select $1\%$, $5\%$, $10\%$, $15\%$, and $20\%$ of our training set to unlearn. We compare the AOED of our method to various baselines. Results are reported in Figure~\ref{fig:unlearning at random}. Note that methods computed with BCE loss are shown with dashed lines whereas methods computed with fair loss are shown with solid lines.

As predicted by our theoretical results, our \textbf{Fair Unlearning} method (green) well-approximates the fairness performance of retraining with fair loss. In addition, we see that across all levels of unlearning, our method outperforms all baselines by a significant margin in AEOD for COMPAS, with similar results for Adult and HSLS. In the full statement of Thm. \ref{thm:eps_delta} for unlearning $m$ samples (see Appendix \ref{sec:thm1_proof}), we show that the unlearning bound scales with $m$. We similarly observe that the precision of our method decreases as we unlearn more samples. Note that \textbf{Newton} is the best approximator for \textbf{Retraining (BCE)}, oftentimes exactly covering the retraining curve, and \textbf{Fair Unlearning} similarly replicates the performance of \textbf{Retraining (Fair)}. Further, we see that certain existing methods such as \textbf{PRU} and \textbf{SISA} are highly variable and can lead to greater disparity than retraining. Notably, across all datasets, our method outperforms baselines in preserving fairness.

\paragraph{Deletion from Subgroups.}

If unlearning at random preserves relative subgroup distribution in data, the worst case unlearning distribution for fairness is when unlearning requests are concentrated on specific subgroups. Subgroup deletion can also create a negative feedback loop where data deletion results in degradation in performance for that subgroup, motivating more members of said subgroup to request unlearning. To explore this, we unlearn only from minority groups and majority groups (Fig. \ref{fig:unlearning from minority/majority subgroups}). Note that when unlearning from minority groups, we only unlearn up to $10\%$ of the dataset as the minority subgroup has (by definition) less representation in the dataset. Too many deletions from the minority group would destroy the stability of the learned models. For example, in the Adult dataset, which has 90\% majority and 10\% minority data, 10\% deletion from the minority group is the maximum possible and already drastically reduces model performance.

We see that when unlearning from both minority and majority subgroups, our \textbf{Fair Unlearning} method approximates \textbf{Retraining (Fair)} and thus maintains the best fairness performance throughout unlearning. When unlearning from minority subgroups, for both COMPAS and HSLS, our method perfectly approximates fair retraining and largely outperforms other baselines in terms of AEOD. In Adult, note that vanilla unlearning methods improve in fairness despite unlearning from the minority subgroup. This is not unexpected as models can generalize better on subsets of a dataset, especially if the removed data are outliers, and the same can occur with fairness performance. However, we highlight that our method always achieves better AEOD than baselines. This indicates that even if removing a portion of data happens to help fairness, we can better improve fairness performance by explicitly accommodating fairness constraints. We also see that both fair retraining and unlearning have high standard deviation at $10\%$ unlearning due to its imbalanced subgroup distribution. We do not see a similar change for baseline methods trained with BCE loss because those methods do not consider subgroups in their loss, whereas in the fair setting this error can be attributed primarily to the fair loss term which requires well-represented subgroups for accurate estimation. This behavior highlights a problem in unlearning settings that even when unlearning a small proportion of the total dataset, unlearning requests may completely destroy accurate estimates of subgroup distributions, resulting in downstream disparity when not considering fairness. 

In majority subgroups, we see consistent performance by our method across all datasets, with significant improvements in terms of AEOD. Finally, similar to the results for random unlearning, \textbf{PRU} display high error bands and atypical AEOD scores when compared to other baselines, while \textbf{Newton} remains accurate at approximating \textbf{Retraining (BCE)}.

\subsection{Experimental Results on Accuracy}
Finally, we validate the accuracy of \textbf{Fair Unlearning} to ensure that increased fairness does not come at a cost to performance. Trivially, AEOD can be optimized by a random guessing classifier despite its poor performance. We report test accuracy at $5\%$ and $20\%$ of samples unlearned across our various baselines and datasets in the supplemental material. We see that while achieving significant improvements in fairness, our method and brute-force retraining maintain performance in terms of accuracy and can even improve in some cases when unlearning at random and from subgroups. 