Machine learning applications, ranging from recommender systems to credit scoring, frequently involve training sophisticated models using large quantities of personal data. As a means to prevent misuse of such information, recent regulatory policies have included provisions\footnote{For example, Article 17 in the European Union's General Data Protection Regulation (GDPR)~\citep{GDPR} or Section~1798.105 in the California Consumer Privacy Act (CCPA)~\citep{CCPA}.} that allow data subjects to delete their information from databases and models used by organizations. These policies have given rise to a general regulatory principle known as the \emph{right to be forgotten}. However, achieving this principle in practice is non-trivial; for instance, the naive approach of retraining a model from scratch each time a deletion request occurs is computationally prohibitive. As a result, a variety of \emph{machine unlearning} (or simply \emph{unlearning}) methods have been developed to effectively approximate the retraining process with reduced computational costs~\citep{guoCertifiedDataRemoval2020,bourtouleMachineUnlearning2020,izzoApproximateDataDeletion,neelDescenttoDeleteGradientBasedMethods2020a}.

In critical real-world applications that use personal data we often seek to achieve other key properties, such as algorithmic fairness, in addition to unlearning. For example, if a social media recommender system systematically under-ranks posts from a particular demographic group, it could substantially suppress the public voices of that group~\citep{beutel2019fairness, jiang2019degenerate}. Similarly, if a machine learning model used for credit scoring exhibits bias, it could unjustly deny loans to certain demographic groups~\citep{corbett2018measure}. In medicine, the fair allocation of healthcare services is also critical to prevent the exacerbation of systemic racism and improve the quality of care for marginalized groups~\citep{chen2021algorithm}. Both protection against algorithmic discrimination and the right to be forgotten have been spelled out in various regulatory documents~\citep{aibillofrights,GDPR,CCPA}, highlighting a need to achieve them simultaneously in practice.

To adhere to the above mentioned regulatory principles, practitioners must satisfy the right to be forgotten while still preventing discrimination in deployed settings. However, most prior work in unlearning only considers models optimized for predictive performance, and fails to consider additional objectives. In fact, we find that most existing machine unlearning methods are not compatible with common fairness interventions. Specifically, popular unlearning methods~\citep{guoCertifiedDataRemoval2020,neelDescenttoDeleteGradientBasedMethods2020a, izzoApproximateDataDeletion} provide theoretical analysis over training objectives that are convex and can be decomposed into sums of losses on individual training data points. On the other hand, existing fairness interventions generally have nonconvex objectives \citep{lowyStochasticOptimizationFramework2022} or involve pairwise or group comparisons between samples that make unlearning a datapoint nontrivial due to each instance's entangled influence on all other samples in the objective function\footnote{See Appendix~\ref{sec:incompatible} for a detailed explanation.}~\citep{berk2017convex, beutel2019fairness}. As such, current unlearning methods cannot be naively combined with fairness objectives, highlighting a need from real-world practitioners for a method that achieves fairness and unlearning simultaneously.

To fill in this critical gap, we formalize the problem of fair unlearning, and propose the first \emph{fair unlearning} method
that can provably unlearn over a fairness-constrained objective. Our fair unlearning approach takes a convex fairness regularizer with pairwise comparisons and unrolls these comparisons into an unlearnable form while ensuring the preservation of theoretical guarantees on unlearning. We provide theoretical results that our method can achieve a common notion of unlearning, \emph{statistical indistinguishability}~\citep{guoCertifiedDataRemoval2020,neelDescenttoDeleteGradientBasedMethods2020a}, while preserving a measure of \emph{equality of odds}~\citep{hardt2016equality}, a popular group fairness metric. Furthermore, we empirically evaluate the proposed method on a variety of real-world datasets and verify its effectiveness across various data deletion request settings, such as deletion at random and deletion from minority and majority subgroups. We show that our method well approximates retraining over fair machine learning objectives in terms of both accuracy and fairness. 

Our main contributions include:
\begin{itemize}
    \item We formalize the problem of fair unlearning, i.e., how to develop models we can train fairly and unlearn from.
    \item We introduce the first fair unlearning method that can provably unlearn requested data points while preserving popular notions of fairness.
    \item We provide theoretical analysis at the intersection of fairness and unlearning that can be applied to a broad set of unlearning methods.
    \item We empirically verify the effectiveness of the proposed method through extensive experiments on a variety of real-world datasets and across different settings of data deletion requests.
\end{itemize}