
\documentclass{article} % For LaTeX2e
\usepackage{colm2024_conference}
\input{math_commands.tex}
\usepackage{wrapfig}
\usepackage[colorlinks=true,citecolor=.]{hyperref}
\usepackage{url}
\usepackage{inconsolata}
\usepackage{eurosym}
\usepackage{todonotes} 
\usepackage{subcaption}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage{array}
\usepackage{longtable}
\usepackage{makecell}
\usepackage{xspace}
\usepackage{xcolor,colortbl}
\usepackage{tcolorbox}
\usepackage{arydshln}
\usepackage{adjustbox}
\newcommand{\dline}{\hdashline[0.5pt/1pt]}
\usepackage{tikz}
\usepackage[tikz]{bclogo}
\usepackage{pgfplots}
\pgfplotsset{width=1.0\columnwidth}
\usepackage{multirow}

\usepackage{makecell}
\usepackage{pifont}
\usepackage{bbm}
\usepackage{rotating}
\usepackage{tablefootnote}
\usepackage{soul}
\usepackage{booktabs}
\usepackage{tikz}
\usepackage[tikz]{bclogo}
\usepackage{pgfplotstable}
\usepackage{mdframed}
\usepackage{graphicx}
% \usepackage{ulem}

\newcommand{\yuchen}[1]{\textcolor{red}{[Yuchen: #1]}} 
\newcommand{\lorahub}{LoraHub\xspace}
\newcommand{\flan}[0]{FLAN-T5\xspace}
\newcommand\scale[1]{{\fontfamily{mathtt}\selectfont {#1}}\xspace}
\newcommand{\icon}{\raisebox{-1pt}{% Figure removed}\xspace}

\newenvironment{rebuttal}
{
    \color{blue} % Set the text color to red
}
{
    \color{blue} % Reset the text color to black when the environment ends
}



\newmdenv[
  backgroundcolor=purple!10,
  skipabove=1em,
  skipbelow=0em,
  leftline=true,
  topline=false,
  bottomline=false,
  rightline=false,
  linecolor=purple!88,
  linewidth=4pt
]{githubquote}



\title{\icon\lorahub{}: Efficient Cross-Task Generalization via Dynamic LoRA Composition}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \colmfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.
\newcommand*{\affaddr}[1]{#1}
\newcommand*{\affmark}[1][*]{\textsuperscript{#1}}
\newcommand*{\email}[1]{\tt{#1}}

\author{
\makecell{Chengsong Huang\affmark[\textdagger\S]{\thanks{The first three authors contributed equally to this work. Correspondence to Qian Liu at \href{mailto:liuqian@sea.com}{\texttt{liuqian@sea.com}}.}}~~, Qian Liu\affmark[\textdagger]$^*$, Bill Yuchen Lin\affmark[$\lozenge$]$^*$, Tianyu Pang\affmark[\textdagger], Chao Du\affmark[\textdagger], Min Lin\affmark[\textdagger]}
\\
\centerline{\affaddr{\affmark[\textdagger]Sea AI Lab, Singapore}}\\
\centerline{\affaddr{\affmark[\S]Washington University in St. Louis, MO, USA}}\\
\centerline{\affaddr{\affmark[$\lozenge$]Allen Institute for AI, Seattle, WA, USA}}
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\colmfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
\input{0_abs}
\end{abstract}


\section{Introduction}


\input{1_intro}

\input{2_problem}


\section{Methodology}

\input{3_method}

\section{Experimental Results}
\input{4_evaluation}


\section{Experimental Analysis}\label{sec:analysis}
\input{5_analysis}


\section{Related work}

\input{6_relatedwork}


\section{Conclusion}

In this work, we have introduced \lorahub, a strategic framework for composing LoRA modules trained on diverse tasks in order to achieve adaptable performance on new tasks. Our approach enables the fluid combination of multiple LoRA modules using just a few examples from a novel task, without requiring additional model parameters or human expertise. The empirical results on the BBH benchmark demonstrate that \lorahub can effectively match the performance of in-context learning in few-shot scenarios, removing the need for in-context examples during inference.
Overall, our work shows the promise of strategic LoRA composability for rapidly adapting LLMs to diverse tasks. 
By fostering reuse and combination of LoRA modules, we can work towards more general and adaptable LLMs while minimizing training costs.

\section*{Reproducibility Statement}
The authors have made great efforts to ensure the reproducibility of the empirical results reported in this paper. Firstly, the experiment settings, evaluation metrics, and datasets were described in detail in Section 4.1.
Secondly, the codes and script for reproduce the result will be opensource after accepted.
Second, the source code implementing the proposed method and experiments will be made publicly available at upon acceptance of the paper.
Third, pre-trained LoRA modules from this work along with their configuration files and weights will be shared. These allow reproduction without retraining the LoRA modules, enabling quick testing and verification.



\bibliography{colm2024_conference}
\bibliographystyle{colm2024_conference}

\newpage
\appendix

\input{7_appendix}

\end{document}
