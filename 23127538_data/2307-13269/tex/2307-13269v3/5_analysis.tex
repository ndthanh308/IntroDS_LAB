In this section, we thoroughly examine the characteristics of our proposed method and uncover several insightful findings. If not specified, we use \flan{}-large for all analysis.


\begin{githubquote}
    Does composing LoRA modules extend beyond the single module's benefits?
\end{githubquote}

\begin{wraptable}{t}{0.6\textwidth} 
  \centering
  \caption{The average performance of various methods across all tasks in the benchmark BBH.}
  \begin{tabular}{ccc}
    \toprule
   LoRA Retrieval & \lorahub$_{\rm avg}$ &  \lorahub$_{\rm best}$ \\
    \midrule
    31.7 & 34.7 & 41.2 \\
    \bottomrule
  \end{tabular}
  \label{tab:baseline_compare}
\end{wraptable}

We acknowledge the investigation of cross-task performance in prior work~\citep{Jang2023ExploringTB}, which delved into the capabilities of LoRA and proposed a novel method centered around LoRA module retrieval.
In order to ensure a fair comparison, we conducted an experiment where we designed a LoRA retrieval mechanism based on the loss derived from few-shot examples.
Specifically, we ranked all LoRA module candidates according to this loss and evaluated the best candidate on the test set of the unseen task.
As depicted in Table~\ref{tab:baseline_compare}, the performance of LoRA retrieval is notably impressive, positioning it as a strong baseline. However, in comparison to LoraHub, the performance of LoRA retrieval is relatively less favorable

\begin{githubquote}
    How effective is the gradient-free optimization method?
\end{githubquote}


To assess the effectiveness of our gradient-free optimization method in correctly identifying the most suitable LoRA module for a given downstream task, we carried out an empirical study using the WikiTableQuestions~\citep{wtq} (WTQ) dataset.
We strategically included a LoRA module that was specifically trained on the WTQ dataset into our pool of LoRA candidate modules, which originally stemmed from tasks exclusive to the Flan Collection. Subsequently, we designated WTQ as the targeted downstream task and computed the weights consistent with the methods employed in \lorahub learning. As an end result, the WTQ-specific LoRA module was awarded the highest weight, exemplifying the algorithm's success in recognizing it as the most relevant.
Moreover, the combined LoRA module demonstrated marginal superiority over the WTQ LoRA module. 
This underscores the claim that the gradient-free optimization method has the ability to proficiently select the optimal upstream LoRA module for an unseen task.


\begin{githubquote}
    Can \lorahub work well on non-instruction-tuning models?
\end{githubquote}

In previous investigations, we primarily focused on models with zero-shot capabilities that were trained with instruction tuning. However, for models like T5 without zero-shot abilities, where training has a larger effect on parameters, it was unclear if \lorahub{} could still effectively manage and improve them. Our experiments show that although these models perform worse than \flan{}, \lorahub{} learning can still enable them to effectively generlize to unseen tasks. See Appendix~\ref{sec:t5_appendix} for more details. 


\begin{githubquote}
    Will the rank of LoRA modules impact the performance of \lorahub learning?
\end{githubquote}

The parameter rank plays a crucial role in the LoRA framework, directly influencing the number of trainable parameters utilized during LoRA tuning. This prompts an intriguing question: does the variation in rank values influence the outcomes observed within the LoraHub learning? Our analysis indicates that, for FLAN-T5, the choice of rank has minimal impact. However, for T5, it still exerts some influence. Empirical findings reveal that, in comparison to rank values of $4$ or $64$, a rank value of $16$ consistently demonstrates superior performance across different runs, both in terms of average and optimal values. Additional results are available in Appendix~\ref{sec:t5_appendix}.    


\begin{githubquote}
    Does more LoRA modules lead to better results?
\end{githubquote}

In our main experiments, we randomly selected $20$ LoRA modules for \lorahub{} learning. Therefore, we conducted experiments to investigate the effect of using different numbers of LoRA modules. The results demonstrate that as we increased the number of LoRA modules, the variance in performance increased. However, the maximum achievable performance also improved. More analysis on the variance and the detailed results can be found in Appendix~\ref{sec:number_of_lora}.


\begin{githubquote}
    How much computational resource can be saved?
\end{githubquote}
We follow to the memory test settings from the LoRA-FA~\citep{Zhang2023LoRAFAML} study for an accurate benchmark. In this context, full fine-tuning required about 40GB of memory, whereas LoRA fine-tuning used around 34GB. Remarkably, LoraHub only utilized about 5GB of memory, illustrating its efficiency due to the inference-only mode, which eliminates the need for storing gradients and optimization states.