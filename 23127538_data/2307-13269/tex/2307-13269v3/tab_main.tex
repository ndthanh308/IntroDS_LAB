\begin{table}[t]
\centering
\small

\caption{Experimental results of zero-shot learning (Zero), few-shot in-context learning (ICL), IA3 fine-tuning (IA3), LoRA tuning (LoRA), full fine-tuning (FFT) and our proposed few-shot \lorahub{} learning (\lorahub{}) on the BBH benchmark with \flan{}-large as the base LLM. We denote algorithmic tasks with the superscript $\S$ following previous work~\citep{DBLP:journals/corr/abs-2303-17564}. Note that we employ three runs, each leveraging different $5$-shot examples per task, as demonstrations for all few-shot methods. The average performance of all methods is reported below, and the best performance of each few-shot method can be found in the Appendix~\ref{sec:maximum_appendix}.}
\label{tab:performance}
\begin{tabular}{lccccccc}
\toprule
Task & Zero & ICL$_{\rm avg}$ & IA3$_{\rm avg}$ & LoRA$_{\rm avg}$ & FFT$_{\rm avg}$ & \lorahub{}$_{\rm avg}$ \\
\midrule
Boolean Expressions & 54.0 & 59.6 & 56.2 & 56.0 & 62.2 & 55.5 \\
Causal Judgement & 57.5 & 59.4 & 60.2 & 55.6 & 57.5 & 54.3 \\
Date Understanding & 15.3 & 20.4 & 20.0 & 35.8 & 59.3 & 32.9 \\
Disambiguation & 0.0 & 69.1 & 0.0 & 68.0 & 68.2 & 45.2 \\
Dyck Languages & 1.3 & 0.9 & 4.2 & 22.2 & 19.5 & 1.0 \\
Formal Fallacies & 51.3 & 55.3 & 51.5 & 53.6 & 54.0 & 52.8 \\
Geometric Shapes & 6.7 & 19.6 & 14.7 & 24 & 31.1 & 7.4 \\
Hyperbaton & 6.7 & 71.8 & 49.3 & 55.3 & 77.3 & 62.8 \\
\begin{tabular}[c]{@{}l@{}}Logical Deduction$^\S$ \\ {\small ~~~~~~~~~~~~~(five objects)}\end{tabular}  & 21.3 & 39.1 & 32.7 & 40.0 & 42.2 & 36.1 \\
\begin{tabular}[c]{@{}l@{}}Logical Deduction$^\S$ \\ {\small ~~~~~~~~~~~~~(seven objects)}\end{tabular} & 12.7 & 40.7 & 33.8 & 37.3 & 44.9 & 36.8 \\
\begin{tabular}[c]{@{}l@{}}Logical Deduction$^\S$ \\ {\small ~~~~~~~~~~~~~(three objects)}\end{tabular} & 0.0 & 51.6 & 8.5 & 53.6 & 52.9 & 45.7 \\
Movie Recommendation & 62.7 & 55.8 & 61.8 & 51.5 & 66.0 & 55.3 \\
Multistep Arithmetic & 0.7 & 0.7 & 0.7 & 0.2 & 0.0 & 0.4 \\
Navigate & 47.3 & 45.3 & 46.2 & 48.0 & 48.0 & 47.1 \\
Object Counting & 34.7 & 32.4 & 35.1 & 38.7 & 35.6 & 33.7 \\
Penguins in a Table & 43.5 & 41.3 & 45.0 & 36.2 & 31.9 & 35.9 \\
Reasoning about Colored Objects & 32.0 & 40.2 & 40.7 & 39.6 & 37.6 & 40.0 \\
Ruin Names & 23.3 & 19.3 & 24.4 & 37.8 & 61.3 & 24.4 \\
Salient Translation Error Detection & 37.3 & 47.3 & 37.1 & 16.0 & 16.2 & 36.0 \\
Snarks & 50.0 & 54.2 & 53.9 & 55.6 & 66.7 & 56.9 \\
Sports Understanding & 56.0 & 54.7 & 55.1 & 56.5 & 54.0 & 56.7 \\
Temporal Sequences & 16.7 & 25.1 & 18.2 & 25.1 & 37.8 & 18.2 \\
\begin{tabular}[c]{@{}l@{}}Tracking Shuffled Objects$^\S$ \\ {\small ~~~~~~~~~~~~~\quad\quad\quad(five objects)}\end{tabular} & 12.0 & 12.0 & 12.0 & 13.8 & 16.9 & 12.3 \\
\begin{tabular}[c]{@{}l@{}}Tracking Shuffled Objects$^\S$ \\ {\small ~~~~~~~~~~~~~\quad\quad\quad(seven objects)}\end{tabular} & 6.7 & 6.7 & 6.7 & 10.0 & 9.8 & 7.7 \\
\begin{tabular}[c]{@{}l@{}}Tracking Shuffled Objects$^\S$ \\ {\small ~~~~~~~~~~~~~\quad\quad\quad(three objects)}\end{tabular} & 24.7 & 31.1 & 30.7 & 30.9 & 32.0 & 29.2 \\
Web of Lies & 54.0 & 53.8 & 54.2 & 52.7 & 48.2 & 50.1 \\
Word Sorting & 1.3 & 0.5 & 1.3 & 4.9 & 4.9 & 1.1 \\
\midrule
Avg Performance Per Task & 27.0 & 37.3 & 31.6 & 37.7 & 42.1 & 34.7 \\
Avg Tokens Per Example & 111.6 & 597.8 & 111.6 & 111.6 & 111.6 & 111.6 \\
Gradient-based Training & No & No & Yes & Yes & Yes & No \\
\bottomrule
\end{tabular}
\end{table}
