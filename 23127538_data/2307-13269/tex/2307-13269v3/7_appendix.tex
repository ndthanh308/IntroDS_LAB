\section{More Analysis}


\begin{githubquote}
    Which LoRA modules are most effective for BBH tasks? 
\end{githubquote}

We hypothesized that the amalgamation of LoRA modules could incorporate skills and insights from a variety of specific tasks. To evaluate this, we examined the extent of influence a single LoRA module had amongst all tasks from the BBH benchmark. We measured the impact of each isolated task by calculating the average absolute weight. The top five modules, presented in Table~\ref{tab:useful}, were found to have substantial influence, as indicated by their maximum average weights, which suggested that they were notably more effective in cross-task transfer.
Remarkably, a common feature among these top five modules was their association with tasks requiring reading comprehension and reasoning skillsâ€”attributes indicative of higher cognitive complexity.
However, it is worth noting that none of the modules exhibited consistent improvement across all BBH tasks, as reflected in their average performance on all BBH tasks, which did not show a significant improvement compared to the original FLAN-T5-large, except for the Rank 2.
The results underscore the advantages of composing diverse modules in LoraHub.


\begin{table}[bt]
    \centering
    \small
    \caption{The top five beneficial LoRA modules for BBH tasks and their associated upstream tasks, the average weight values and the average performance on all BBH tasks.}
    \label{tab:useful}
    \begin{tabular}{llllll}
    \toprule
        Rank & Dataset: Task & Weight & Perf & Task Description \\ \midrule
        1 & WIQA: Last Process & 0.72  & 28.1 & \makecell[l]{Identifying the last step of a given process.} \\
        2 & RACE: Is this the Right Answer & 0.68 & 30.8 & \makecell[l]{Determining if given answer is correct.} \\ 
        3 & WIQA: First Process & 0.63 & 28.1 & \makecell[l]{Identifying the first step of a given process.} \\ 
        4 & AdversarialQA: BiDAF & 0.61 & 25.1 & \makecell[l]{Answering question created by an \\adversarial  model-in-the-loop.} \\ 
        5 & WebQuestions: What is the Answer & 0.58 & 27.0 & \makecell[l]{Answering question based on information \\extracted from the web.} \\ \bottomrule
    \end{tabular}
    \label{tab:top}
\end{table}


\begin{githubquote}
    How effective is the gradient-free optimization method?
\end{githubquote}


To assess the effectiveness of our gradient-free optimization method in correctly identifying the most suitable LoRA module for a given downstream task, we carried out an empirical study using the WikiTableQuestions~\citep{wtq} (WTQ) dataset.
We strategically included a LoRA module that was specifically trained on the WTQ dataset into our pool of LoRA candidate modules, which originally stemmed from tasks exclusive to the Flan Collection. Subsequently, we designated WTQ as the targeted downstream task and computed the weights consistent with the methods employed in \lorahub learning. As an end result, the WTQ-specific LoRA module was awarded the highest weight, exemplifying the algorithm's success in recognizing it as the most relevant.
Moreover, the combined LoRA module demonstrated marginal superiority over the WTQ LoRA module. 
This underscores the claim that the gradient-free optimization method has the ability to proficiently select the optimal upstream LoRA module for an unseen task.




\section{Result of Best Results}\label{sec:maximum_appendix}

As shown in Table~\ref{tab:max_perf}, compared to gradient-based parameter-efficient training methods like LoRA and IA3, our approach demonstrates superior performance in terms of best results over experimental runs. While it exhibits a noticeable lag behind the fully fine-tuning (FFT) method, which updates all parameters during training, this observation suggests that our proposed method has a promising upper limit. We anticipate that future research efforts can contribute to accelerating the optimization speed and further enhancing the efficacy of our approach.

\begin{table}[htbp]
\centering
\small
\caption{Experimental results of several few-shot methods, including in-context learning (ICL), IA3 fine-tuning (IA3), LoRA tuning (LoRA), full fine-tuning (FFT) and our \lorahub{} learning (\lorahub{}) on the BBH benchmark with \flan{}-large as the base LLM. We denote algorithmic tasks with the superscript $\S$ following previous work~\citep{DBLP:journals/corr/abs-2303-17564}. Note that we use $5$ examples per task as the demonstration for all methods. The best (\textit{best}) performance is reported as the maximum value obtained across three runs.}
\label{tab:max_perf}
\begin{tabular}{lccccccc}
\toprule
Task & ICL$_{\rm best}$ & IA3$_{\rm best}$ & LoRA$_{\rm best}$ & FFT$_{\rm best}$ & \lorahub{}$_{\rm best}$ \\
\midrule
Boolean Expressions & 62.7 & 58.0 & 60.7 & 65.3 & 60.7 \\ 
Causal Judgement & 59.8 & 62.1 & 57.5 & 60.9 & 63.2 \\ 
Date Understanding & 21.3 & 20.7 & 40.7 & 67.3 & 45.3 \\ 
Disambiguation & 69.3 & 0.0 & 68.7 & 70.7 & 68.0 \\ 
Dyck Languages & 2.0 & 4.7 & 25.3 & 33.3 & 2.7 \\ 
Formal Fallacies & 59.3 & 52.0 & 56.7 & 56.0 & 59.3 \\ 
Geometric Shapes & 20.0 & 15.3 & 28.7 & 39.3 & 18.7 \\ 
Hyperbaton & 72.7 & 49.3 & 57.3 & 82.0 & 72.7 \\ 
\begin{tabular}[c]{@{}l@{}}Logical Deduction$^\S$ \\ {\small ~~~~~~~~~~~~~(five objects)}\end{tabular} & 39.3 & 32.7 & 41.3 & 43.3 & 40.0 \\ 
\begin{tabular}[c]{@{}l@{}}Logical Deduction$^\S$ \\ {\small ~~~~~~~~~~~~~(seven objects)}\end{tabular} & 42.0 & 34.0 & 42.7 & 46.0 & 46.0 \\ 
\begin{tabular}[c]{@{}l@{}}Logical Deduction$^\S$ \\ {\small ~~~~~~~~~~~~~(three objects)}\end{tabular} & 52.7 & 8.7 & 56.7 & 60.7 & 52.7 \\ 
Movie Recommendation & 56.7 & 62.0 & 64.5 & 70.7 & 62.0 \\ 
Multistep Arithmetic & 0.7 & 0.7 & 0.7 & 0.0 & 1.3 \\ 
Navigate & 46.7 & 47.3 & 50.7 & 50.0 & 51.3 \\ 
Object Counting & 34.7 & 35.3 & 42.0 & 38.0 & 36.7 \\ 
Penguins in a Table & 43.5 & 45.7 & 41.3 & 37.0 & 47.8 \\ 
Reasoning about Colored Objects & 41.3 & 41.3 & 40.7 & 38.7 & 44.7 \\ 
Ruin Names & 20.7 & 25.3 & 42.0 & 66.0 & 28.7 \\ 
Salient Translation Error Detection & 48.0 & 37.3 & 17.3 & 21.3 & 42.7 \\ 
Snarks & 55.1 & 56.4 & 59.0 & 69.2 & 61.5 \\ 
Sports Understanding & 56.7 & 55.3 & 58.7 & 58.7 & 62.7 \\ 
Temporal Sequences & 26.7 & 18.7 & 31.3 & 48.7 & 21.3 \\
\begin{tabular}[c]{@{}l@{}}Tracking Shuffled Objects$^\S$ \\ {\small ~~~~~~~~~~~~~\quad\quad\quad(five objects)}\end{tabular}  & 12.0 & 12.0 & 16.0 & 20.0 & 16.7 \\ 
\begin{tabular}[c]{@{}l@{}}Tracking Shuffled Objects$^\S$ \\ {\small ~~~~~~~~~~~~~\quad\quad\quad(seven objects)}\end{tabular} & 6.7 & 6.7 & 12.0 & 10.0 & 15.3 \\ 
\begin{tabular}[c]{@{}l@{}}Tracking Shuffled Objects$^\S$ \\ {\small ~~~~~~~~~~~~~\quad\quad\quad(three objects)}\end{tabular} & 31.3 & 30.7 & 32.0 & 36.0 & 31.3 \\ 
Web of Lies & 54.0 & 54.7 & 55.3 & 54.0 & 57.3 \\ 
Word Sorting & 0.7 & 1.3 & 5.3 & 6.0 & 1.3 \\ \midrule
Best Performance (Average) & 38.4 & 32.1 & 40.9 & 46.2 & 41.2 \\ 
\bottomrule
\end{tabular}
\end{table}

\newpage
\section{Result of non-instrcution-tuned models}\label{sec:t5_appendix}

\begin{table}[htbp]
    \centering
    \caption{Comparsion among different ranks for few-shot \lorahub{} learning with the backbone T5-large~\citep{t5_paper} on the BBH benchmark. Note that the T5-large model achieved $0.0$\% on all tasks under the zero-shot setting except \textit{Dyck Languages}, where it scored $0.67$\%.}
\begin{tabular}{lc>{\columncolor{lightgray}}cc>{\columncolor{lightgray}}cc>{\columncolor{lightgray}}c}
\toprule
Task~$\downarrow$ \ \ \ \ Rank~$\rightarrow$ & 4$_{\rm avg}$ & 4$_{\rm best}$ & 16$_{\rm avg}$ & 16$_{\rm best}$ & 64$_{\rm avg}$ & 64$_{\rm best}$\\
\midrule
Boolean Expressions & 52.13 & 57.33 & 50.67 & \textbf{58.00} & 47.47 & 58.00\\
Causal Judgement & 52.41 & \textbf{55.17} & 49.66 & 54.02 & 50.80 & 54.02\\
Date Understanding & 0.40 & 2.00 & 14.40 & \textbf{29.33} & 4.53 & 10.00\\
Disambiguation& 10.00 & 31.33 & 26.93 & \textbf{42.00} & 1.73 & 4.67\\
Dyck Languages & 0.40 & 0.67 & 0.40 & 0.67 & 0.40 & \textbf{2.00}\\
Formal Fallacies & 48.40 & \textbf{54.00} & 46.93 & 51.33 & 46.93 & 50.00\\
Geometric Shapes & 0.00 & 0.00 & 6.53 & \textbf{32.67} & 1.47 & 7.33\\
Hyperbaton & 30.13 & 50.00 & 39.07 & \textbf{57.33} & 32.93 & 48.00\\
\begin{tabular}[c]{@{}l@{}}Logical Deduction$^\S$ \\ {\small ~~~~~~~~~~~~~(five objects)}\end{tabular}& 5.20 & 14.67 & 8.80 & \textbf{19.33} & 1.33 & 6.67\\

\begin{tabular}[c]{@{}l@{}}Logical Deduction$^\S$ \\ {\small ~~~~~~~~~~~~~(seven objects)}\end{tabular} & 6.40 & 17.33 & 9.33 & \textbf{19.33} & 3.47 & 16.00\\

\begin{tabular}[c]{@{}l@{}}Logical Deduction$^\S$ \\ {\small ~~~~~~~~~~~~~(three objects)}\end{tabular}& 14.40 & 32.00 & 21.73 & \textbf{34.67} & 6.93 & 15.33\\

Movie Recommendation & 7.07 & 18.67 & 7.87 & \textbf{22.00} & 1.20 & 6.00\\

Multistep Arithmetic two & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00\\

Navigate & 49.60 & 54.67 & 52.27 & \textbf{56.67} & 49.87 & 52.00\\

Object Counting & 7.20 & 18.00 & 16.00 & 21.33 & 13.73 & \textbf{26.67}\\

Penguins in a Table & 6.52 & 13.04 & 10.43 & \textbf{17.39} & 0.43 & 2.17\\

Reasoning about Colored Objects & 6.27 & 10.00 & 5.07 & \textbf{16.67} & 0.53 & 2.67\\

Ruin Names & 7.73 & 13.33 & 13.20 & \textbf{28.00} & 5.73 & 15.33\\

Salient Translation Error Detection & 0.00 & 0.00 & 1.73 & \textbf{8.67} & 0.00 & 0.00\\

Snarks & 21.28 & 42.31 & 49.49 & \textbf{60.26} & 16.15 & 38.46\\

Sports Understanding & 46.53 & \textbf{58.67} & 46.80 & 58.67 & 46.53 & 58.67\\

Temporal Sequences & 3.07 & 13.33 & 6.53 & \textbf{26.67} & 2.40 & 12.00\\

\begin{tabular}[c]{@{}l@{}}Tracking Shuffled Objects$^\S$ \\ {\small ~~~~~~~~~~~~~\quad\quad\quad(five objects)}\end{tabular}  & 5.20 & \textbf{14.00} & 4.13 & 9.33 & 0.13 & 0.67\\

\begin{tabular}[c]{@{}l@{}}Tracking Shuffled Objects$^\S$ \\ {\small ~~~~~~~~~~~~~\quad\quad\quad(seven objects)}\end{tabular}  & 2.67 & 10.00 & 2.80 & \textbf{14.00} & 3.20 & 8.00\\

\begin{tabular}[c]{@{}l@{}}Tracking Shuffled Objects$^\S$ \\ {\small ~~~~~~~~~~~~~\quad\quad\quad(three objects)}\end{tabular} & 3.73 & 17.33 & 16.27 & \textbf{34.67} & 5.87 & 26.67\\

Web of Lies & 48.53 & 54.00 & 54.00 & 56.00 & 54.67 & \textbf{57.33}\\

Word Sorting & 0.40 & \textbf{0.67} & 0.13 & 0.67 & 0.00 & 0.00\\
\midrule
Average Performance per Task & 16.14 & 24.17 & 20.78 & \textbf{30.73} & 14.76 & 21.43\\
\bottomrule
\end{tabular}
    \label{tab:t5_results}
\end{table}

\newpage

\section{Result of larger model}

\begin{table}[htbp]
\centering
\caption{ Experimental results of zero-shot learning (Zero) and our few-shot \lorahub{} learning (\lorahub{}) on the BBH benchmark with \flan{}-xl as the base LLM. Note that we use $5$ examples per task as the demonstration for both ICL and \lorahub{}. The average (\textit{avg}) performance of \lorahub{} is computed over $5$ runs with different random seeds, while the best (\textit{best}) performance is reported as the maximum value obtained across these runs. We can see the trend of the results are similar to \flan{}-large.}
\begin{tabular}{lcc>{\columncolor{lightgray}}c}
\toprule
Task & Zero & \lorahub$_{\rm avg}$ & \lorahub$_{\rm best}$ \\
\midrule
Boolean Expressions & 52.0 & 58.7 & 63.3 \\
Causal Judgement & 62.1 & 53.8 & 59.8 \\
Date Understanding & 38.0 & 37.6 & 38.0 \\
Disambiguation Qa & 0.0 & 20.5 & 54.7 \\
Dyck Languages & 1.3 & 0.9 & 2.0 \\
Formal Fallacies & 56.0 & 56.0 & 56.0 \\
Geometric Shapes & 8.7 & 17.5 & 28.0 \\
Hyperbaton & 45.3 & 53.5 & 56.7 \\
\begin{tabular}[c]{@{}l@{}}Logical Deduction$^\S$ \\ {\small ~~~~~~~~~~~~~(five objects)}\end{tabular} & 1.3 & 42.7 & 48.7 \\
\begin{tabular}[c]{@{}l@{}}Logical Deduction$^\S$ \\ {\small ~~~~~~~~~~~~~(seven objects)}\end{tabular} & 8.7 & 44.3 & 50.0 \\
\begin{tabular}[c]{@{}l@{}}Logical Deduction$^\S$ \\ {\small ~~~~~~~~~~~~~(three objects)}\end{tabular} & 0.7 & 56.4 & 61.3 \\
Movie Recommendation & 2.0 & 62.8 & 66.0 \\
Multistep Arithmetic Two & 0.0 & 0.4 & 0.7 \\
Navigate & 50.7 & 50.7 & 50.7 \\
Object Counting & 39.3 & 40.7 & 48.0 \\
Penguins In A Table & 17.4 & 40.9 & 45.7 \\
Reasoning About Colored Objects & 46.7 & 47.3 & 50.7 \\    
Ruin Names & 18.0 & 35.6 & 44.7 \\
Salient Translation Error Detection & 44.7 & 45.1 & 48.7 \\
Snarks & 60.3 & 60.8 & 61.5 \\
Sports Understanding & 56.7 & 51.3 & 53.3 \\
Temporal Sequences & 21.3 & 21.5 & 22.0 \\
\begin{tabular}[c]{@{}l@{}}Tracking Shuffled Objects$^\S$ \\ {\small ~~~~~~~~~~~~~\quad\quad\quad(five objects)}\end{tabular}  & 3.3 & 9.9 & 13.3 \\
\begin{tabular}[c]{@{}l@{}}Tracking Shuffled Objects$^\S$ \\ {\small ~~~~~~~~~~~~~\quad\quad\quad(seven objects)}\end{tabular}  & 5.3 & 7.3 & 8.7 \\
\begin{tabular}[c]{@{}l@{}}Tracking Shuffled Objects$^\S$ \\ {\small ~~~~~~~~~~~~~\quad\quad\quad(three objects)}\end{tabular}  & 7.3 & 21.7 & 31.3 \\
Web Of Lies & 54.7 & 47.1 & 48.7 \\
Word Sorting & 1.3 & 1.5 & 2.0 \\ \midrule

Average Performance per Task & 25.8 & 36.5 & 41.3 \\
\bottomrule
\end{tabular}
\end{table}

\newpage

\section{Improving the Robustness of \lorahub{}}

In order to enhance the robustness of \lorahub{}, we explored a straightforward approach in the selection of LoRA module candidates. Specifically, we first identified $20$ LoRA module candidates with the lowest loss on the few-shot examples. Our findings indicate a slight improvement in overall performance after applying the pre-filtering startegy. Since the primary instability in our approach arises from the selection of LoRA candidates. This method involves choosing a fixed set of LoRA candidates to ensure the stability of our approach.

\begin{table}[htbp]
    \centering
    \caption{The experimental results of loss-based pre-filtering.}

    \begin{tabular}{lccc}
        \toprule
        Task & \lorahub{}$_{\rm avg}$  & \lorahub{}$_{\rm filter}$ \\
        \midrule
        Boolean Expressions & 55.5 & 60.00 \\
        Causal Judgement & 54.3 & 52.9 \\
        Date Understanding & 32.9  & 33.3 \\
        Disambiguation & 45.2 & 62.7 \\
        Dyck Languages & 1.0 & 0.0 \\
        Formal Fallacies & 52.8 & 54.0 \\
        Geometric Shapes & 7.4 & 4.0 \\
        Hyperbaton & 62.8 & 64.0 \\
        \begin{tabular}[c]{@{}l@{}}Logical Deduction$^\S$ \\ {\small ~~~~~~~~~~~~~(five objects)}\end{tabular} & 36.1  & 37.3 \\
        \begin{tabular}[c]{@{}l@{}}Logical Deduction$^\S$ \\ {\small ~~~~~~~~~~~~~(seven objects)}\end{tabular} & 36.8 & 22.0 \\
        \begin{tabular}[c]{@{}l@{}}Logical Deduction$^\S$ \\ {\small ~~~~~~~~~~~~~(three objects)}\end{tabular} & 45.7 & 56.0 \\
        Movie Recommendation & 55.3  & 68.0 \\
        Multistep Arithmetic & 0.4 & 0.7 \\
        Navigate & 47.1 & 49.3 \\
        Object Counting & 33.7 & 38.7 \\
        Penguins in a Table & 35.9 & 37.0 \\
        Reasoning about Colored Objects & 40.0 & 33.3 \\
        Ruin Names & 24.4 & 22.0 \\
        Salient Translation Error Detection & 36.0 & 24.0 \\
        Snarks & 56.9 & 52.66 \\
        Sports Understanding & 56.7 & 58.0 \\
        Temporal Sequences & 18.2 & 27.3 \\
        \begin{tabular}[c]{@{}l@{}}Tracking Shuffled Objects$^\S$ \\ {\small ~~~~~~~~~~~~~\quad\quad\quad(five objects)}\end{tabular} & 12.3 & 11.3 \\
        \begin{tabular}[c]{@{}l@{}}Tracking Shuffled Objects$^\S$ \\ {\small ~~~~~~~~~~~~~\quad\quad\quad(seven objects)}\end{tabular} & 7.7 & 8.0 \\
        \begin{tabular}[c]{@{}l@{}}Tracking Shuffled Objects$^\S$ \\ {\small ~~~~~~~~~~~~~\quad\quad\quad(three objects)}\end{tabular} & 29.2 & 32.7 \\
        Web of Lies & 50.1 & 46.0 \\
        Word Sorting & 1.1 & 1.3 \\ \midrule
        Avg Performance Per Task & 34.7 & 35.4 \\
        \bottomrule
    \end{tabular}
    \label{tab:lorahub_robustness}
\end{table}


\newpage
\section{Performance on General Important Task}

In our research, we have identified specific LoRA modules that exhibit significant impact when integrated into merged LoRAs. Our focus lies in assessing the performance of the top five task-related LoRAs on the BBH benchmark. The results indicate that these top LoRAs perform similarly or even worse than zero-shot in most cases. Only one of them stands out as significantly better than zero-shot. However, it's worth noting that this performance is not as impressive as Lorahub. These findings support the idea that the merging process can improve overall performance.

\begin{table}[htbp]
\centering
\small
\caption{Detailed experimental results of top five LoRA modules shown in Table~\ref{tab:top} on BBH tasks.}
\label{tab:best_bbh_perf}
\begin{tabular}{lccccccc}
\toprule
Task & WIQA: Last & RACE: Right & WIQA: First & ADQA & WebQA \\
\midrule
Boolean Expressions & 52.67 & 58.00 & 52.67 & 54.67 & 53.33 \\ 
Causal Judgement & 55.17 & 63.22 & 55.17 & 57.47 & 57.47 \\ 
Date Understanding & 17.33 & 19.33 & 17.33 & 16.67 & 15.33 \\ 
Disambiguation & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\ 
Dyck Languages & 0.67 & 0.67 & 0.67 & 1.33 & 1.33 \\ 
Formal Fallacies & 51.33 & 51.33 & 51.33 & 51.33 & 51.33 \\ 
Geometric Shapes & 8.00 & 13.33 & 8.00 & 6.67 & 7.33 \\ 
Hyperbaton & 16.67 & 44.00 & 16.67 & 1.33 & 6.00 \\ 
\begin{tabular}[c]{@{}l@{}}Logical Deduction$^\S$ \\ {\small ~~~~~~~~~~~~~(five objects)}\end{tabular}& 23.33 & 28.00 & 23.33 & 19.33 & 20.67 \\ 
\begin{tabular}[c]{@{}l@{}}Logical Deduction$^\S$ \\ {\small ~~~~~~~~~~~~~(seven objects)}\end{tabular}& 22.00 & 26.00 & 22.00 & 10.67 & 12.00 \\ 
\begin{tabular}[c]{@{}l@{}}Logical Deduction$^\S$ \\ {\small ~~~~~~~~~~~~~(three objects)}\end{tabular}& 0.67 & 9.33 & 0.67 & 0.00 & 0.00 \\ 
Movie Recommendation & 63.33 & 62.67 & 63.33 & 56.67 & 63.33 \\ 
Multistep Arithmetic & 0.67 & 0.67 & 0.67 & 0.67 & 0.67 \\ 
Navigate & 47.33 & 50.00 & 47.33 & 47.33 & 47.33 \\ 
Object Counting & 34.67 & 34.00 & 34.67 & 35.33 & 35.33 \\ 
Penguins in a Table & 45.65 & 41.30 & 45.65 & 39.13 & 43.48 \\ 
Reasoning about Colored Objects & 40.00 & 37.33 & 40.00 & 31.33 & 30.67 \\ 
Ruin Names & 22.00 & 21.33 & 22.00 & 17.33 & 22.67 \\ 
Salient Translation Error Detection & 36.67 & 34.67 & 36.67 & 32.67 & 37.33 \\ 
Snarks & 52.56 & 55.13 & 52.56 & 47.44 & 52.56 \\ 
Sports Understanding & 56.00 & 58.67 & 56.00 & 55.33 & 55.33 \\ 
Temporal Sequences & 16.67 & 17.33 & 16.67 & 12.67 & 17.33 \\ 
\begin{tabular}[c]{@{}l@{}}Tracking Shuffled Objects$^\S$ \\ {\small ~~~~~~~~~~~~~\quad\quad\quad(five objects)}\end{tabular} & 12.00 & 12.00 & 12.00 & 10.67 & 12.00 \\ 
\begin{tabular}[c]{@{}l@{}}Tracking Shuffled Objects$^\S$ \\ {\small ~~~~~~~~~~~~~\quad\quad\quad(seven objects)}\end{tabular} & 6.67 & 6.67 & 6.67 & 6.67 & 6.67 \\ 
\begin{tabular}[c]{@{}l@{}}Tracking Shuffled Objects$^\S$ \\ {\small ~~~~~~~~~~~~~\quad\quad\quad(three objects)}\end{tabular} & 20.67 & 30.67 & 20.67 & 10.67 & 25.33 \\ 
Web of Lies & 54.67 & 54.00 & 54.67 & 54.00 & 54.00 \\ 
Word Sorting & 1.33 & 1.33 & 1.33 & 1.33 & 1.33 \\ 
\midrule
Avg Performance per Task & 28.10 & 30.78 & 28.10 & 25.14 & 27.04 \\
$\Delta$ FLAN-T5-large & 1.10 & 3.78 & 1.10 & -1.86 & 0.04\\
\bottomrule
\end{tabular}
\end{table}




\newpage
\section{Implementation details}
We implemented LoRA tuning using the Huggingface PEFT library~\citep{peft}, with the rank being set as $16$. The gradient-free method was implemented using the open-source Nevergrad optimization library~\citep{nevergrad}, with a constraint that the absolute value of LoRA weights should not exceed $1.5$. Originally, all coefficients of LoRA modules were set at zero.

In our standard settings, we set the maximum number of iterations $K$ as $40$.
The same $5$ examples were used during our \lorahub{} learning and the few-shot in-context learning. The hyperparameter $\alpha$ is set as $0.05$. Regarding the hyperparameters for training candidate LoRA modules, we maintained consistency across all modules, setting the batch size at $64$, the learning rate at $1e-4$, and the number of training epochs at $10$.

\section{Influence of Number of LoRA modules}\label{sec:number_of_lora}


% Figure environment removed

As shown in Figure~\ref{fig:number_lora}, with an increase in the number of LoRA module candidates, there is a corresponding increase in the performance variance.
Based on our in-depth analysis, the primary source of variance is not related to gradient-free optimization algorithms but rather associated with the LoRA candidate modules. 
In other words, once the candidates are determined, random seeds have minimal impact on the final performance.
Hence, we posit that the observed instability primarily arises from the inherent challenge of balancing the quantity and quality of the LoRA module candidates.    

\section{The Impact of Threshold}

In this section, we omitted the threshold in our implementation, and the results are summarized in Table \ref{tab:table_threshold}.
Our observations indicate that the removal of the threshold had minimal impact on the majority of tasks, underscoring the robustness of the gradient-free optimization algorithm itself in most cases.
The algorithm efficiently identified reasonable ranges even without specific upper and lower bounds.
However, three tasks, namely Date Understanding, Disambiguation and Hyperbaton, exhibited notable effects. The resulting performance decline led to an average decrease of 1.2\% compared to the setting with threshold. This highlights the significance of establishing a reasonable threshold to mitigate extreme scenarios.

\begin{table}[htbp]
    \centering
    \small
    \caption{The comparsion between \lorahub{} and \lorahub{} without threshold.}
    \begin{tabular}{lccc}
        \toprule
        Task & \lorahub{}$_{\rm avg}$ with threshold  &  \lorahub{}$_{\rm avg}$ without threshold \\
        \midrule
        Boolean Expressions & 55.5 & 54.0 \\
        Causal Judgement & 54.3 & 54.8 \\
        Date Understanding & 32.9  & 17.7 \\
        Disambiguation & 45.2 & 40.6 \\
        Dyck Languages & 1.0 & 1.1 \\
        Formal Fallacies & 52.8 & 51.7 \\
        Geometric Shapes & 7.4 & 6.7 \\
        Hyperbaton & 62.8 & 55.5 \\
        \begin{tabular}[c]{@{}l@{}}Logical Deduction$^\S$ \\ {\small ~~~~~~~~~~~~~(five objects)}\end{tabular} & 36.1  & 36.5 \\
        \begin{tabular}[c]{@{}l@{}}Logical Deduction$^\S$ \\ {\small ~~~~~~~~~~~~~(seven objects)}\end{tabular} & 36.8 & 35.6 \\
        \begin{tabular}[c]{@{}l@{}}Logical Deduction$^\S$ \\ {\small ~~~~~~~~~~~~~(three objects)}\end{tabular} & 45.7 & 49.9 \\
        Movie Recommendation & 55.3  & 59.3 \\
        Multistep Arithmetic & 0.4 & 0.7 \\
        Navigate & 47.1 & 47.6 \\
        Object Counting & 33.7 & 34.7 \\
        Penguins in a Table & 35.9 & 33.8 \\
        Reasoning about Colored Objects & 40.0 & 37.9 \\
        Ruin Names & 24.4 & 24.0 \\
        Salient Translation Error Detection & 36.0 & 37.1 \\
        Snarks & 56.9 & 51.6 \\
        Sports Understanding & 56.7 & 55.9 \\
        Temporal Sequences & 18.2 & 16.7 \\
        \begin{tabular}[c]{@{}l@{}}Tracking Shuffled Objects$^\S$ \\ {\small ~~~~~~~~~~~~~\quad\quad\quad(five objects)}\end{tabular} & 12.3 & 12.3 \\
        \begin{tabular}[c]{@{}l@{}}Tracking Shuffled Objects$^\S$ \\ {\small ~~~~~~~~~~~~~\quad\quad\quad(seven objects)}\end{tabular} & 7.7 & 8.5 \\
        \begin{tabular}[c]{@{}l@{}}Tracking Shuffled Objects$^\S$ \\ {\small ~~~~~~~~~~~~~\quad\quad\quad(three objects)}\end{tabular} & 29.2 & 29.8 \\
        Web of Lies & 50.1 & 50.3 \\
        Word Sorting & 1.1 & 1.3 \\ \midrule
        Avg Performance Per Task & 34.7 & 33.5 \\
        \bottomrule
    \end{tabular}
    \label{tab:table_threshold}
\end{table}