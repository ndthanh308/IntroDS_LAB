In this section, we provide an overview of our proposed method. We then explain the LoRA tuning procedure in detail. Last, we introduce the procedure of our \lorahub{} learning, which consists of the \textsc{Compose} stage and the \textsc{Adapt} stage.

\subsection{Method Overview}

As depicted in Figure~\ref{fig:pipeline}, we initially train LoRA modules on a variety of upstream tasks. Specifically, for $N$ distinct upstream tasks, we separately train $N$ LoRA modules, each represented as $m_i$ for task $\mathcal{T}_i\in \mathbb{T}$. Subsequently, for a new task $\mathcal{T}'\notin \mathbb{T}$, such as Boolean Expressions represented in Figure~\ref{fig:pipeline}, its examples $Q$ are utilized to steer the \lorahub learning process.
The \lorahub learning encapsulates two main phases: the \textsc{Compose} phase and the \textsc{Adapt} phase.
In the \textsc{Compose} phase, all available LoRA modules are combined into a single integrated module $\hat{m}$, using $\{w_1, w_2, \dots, w_N\}$ as coefficients.
Each $w_i$ is a scalar value that can take on positive or negative values, and the combination can be done in different ways.
During the \textsc{Adapt} phase, the combined LoRA module $\hat{m}$ is amalgamated with the LLM $M_{\theta}$, and its performance on few-shot examples from the new task $\mathcal{T}'$ is assessed. A gradient-free algorithm is subsequently deployed to update $w$, enhancing $\hat{m}$'s performance (e.g., loss) on the few-shot examples $Q$.
Finally, after iterating through $K$ steps, the optimum performing LoRA module is applied to the LLM $M_\theta$, yielding the final LLM $M_\phi=\operatorname{LoRA}(M_\theta, \hat{m})$. This serves as an effectively adjusted model for the unseen task $\mathcal{T}'$, which will then be deployed and not updated anymore.

% Figure environment removed

\subsection{LoRA tuning on upstream tasks}

LoRA effectively minimizes the number of trainable parameters through the process of decomposing the attention weight matrix update of the LLM, denoted as $W_0 \in R^{d\times k}$, into low-rank matrices. In more specific terms, LoRA exhibits the updated weight matrix in the form $W_0 + \delta W = W_0 + AB$, where $A \in \mathbb{R}^{ d\times r}$ and $B \in \mathbb{R}^{ r\times k}$ are trainable low-rank matrices with rank $r$, a dimension significantly smaller than those of $d$ and $k$. In this context, the product $AB$ defines the LoRA module $m$, as previously elaborated. By leveraging the low-rank decomposition, LoRA substantially reduces the number of trainable parameters needed to adapt the weights of LLMs duriing fine-tuning.

\subsection{\textsc{Compose}: Element-wise composition of LoRA modules}

Within the \textsc{Compose} stage, we implement an element-wise method to combine LoRA modules.
This process integrates the corresponding parameters of the LoRA modules, requiring the modules being combined to have the same rank $r$ to properly align the structures.
Given that $m_i = A_iB_i$, the combined LoRA module $\hat{m}$ can be obtained by:
\begin{equation}
    \hat{m} = (w_1A_1+w_2A_2+\dots+w_NA_N)(w_1B_1+w_2B_2+\dots+w_NB_N)\textrm{.}
\end{equation}

Notbly, as we show in Sec.~\ref{sec:analysis}, combining too many LoRA modules at once can expand the search space exponentially, which may destabilize the \lorahub learning process and prevent optimal performance.
To mitigate this, we employ random selection to prune the candidate space, and more advanced pre-filtering algorithms could be explored in the future.

\subsection{\textsc{Adapt}: Weight optimization via gradient-free methods}

During the \textsc{Adapt} stage, our goal is to modify the coefficients $w$ to boost the model's performace on the examples from an unseen task.
One might think of using gradient descent to optimize $w$, following standard backpropagation methods.
However, this approach demands constructing a hypernetwork for all LoRA modules, similar to differentiable architecture search methods~\citep{nas_hyper}. Constructing these hypernetworks demands for substantial GPU memory and time, posing a challenge. Given that $w$ consists of a relatively small number of parameters, we opted for gradient-free methods for optimization instead of gradient descent.

Inspired by previous work~\citep{blackbox_tuning}, we utilize a black-box optimization technique to find the optimal $w$. The optimization process is steered by the cross-entropy loss, setting the goal to locate the best set $\{w_1, w_2, \dots, w_N\}$ that reduces the loss $L$ on the few-shot examples $Q$. Furthermore, we incorporate L1 regularization to penalize the sum of the absolute values of $w$, helping to prevent obtaining extreme values. Consequently, the final objective of \lorahub is to minimize $L + \alpha \cdot \sum_{i=1}^N |w_i|$, where $\alpha$ serves as a hyperparameter.

In terms of the gradient-free method, we leverage Shiwa, a combinatorial optimization approach~\citep{NGOpt}. Shiwa offers a variety of algorithms and chooses the most suitable optimization algorithm for different circumstances. In most of the forthcoming experimental setups, we primarily employ the Covariance Matrix Adaptive Evolution Strategies (CMA-ES)~\citep{Hansen1996AdaptingAN}. CMA-ES, as a stochastic and population-based optimization algorithm, offers versatility in addressing a broad spectrum of optimization challenges.
It dynamically adjusts a search distribution, which is defined by a covariance matrix. During each iteration, CMA-ES systematically updates both the mean and covariance of this distribution to optimize the target function. In our application, we employ this algorithm to mold the search space for $w$. Ultimately, we use it to identify the optimal $w$ by evaluating their performance on the few-shot examples from an unseen task.