 
 

% Figure environment removed


As depicted in Figure~\ref{fig:pipeline}, we initially train LoRA modules on a variety of upstream tasks. Specifically, for $N$ distinct upstream tasks, we separately train $N$ LoRA modules, each represented as $m_i$ for task $\mathcal{T}_i\in \mathbb{T}$. Subsequently, for a novel task $\mathcal{T}'\notin \mathbb{T}$, such as Boolean Expressions represented in Figure~\ref{fig:pipeline}, its examples $Q$ are utilized to steer the \lorahub learning process. 

The \lorahub learning encapsulates two main phases: the \textsc{Compose} phase and the \textsc{Adapt} phase. In the \textsc{Compose} phase, all available LoRA modules are synthesized into a single module $\hat{m}$, using $\{w_1, w_2, \dots, w_N\}$ coefficients, represented as $\hat{m}=\sum_{i=1}^{N}w_i\times m_i$. Here, $w_i$ is a scalar weight that can assume positive or negative values. 
During the \textsc{Adapt} phase, the assembled LoRA module $\hat{m}$ is amalgamated with the LLM $M_{\theta}$, and its performance on few-shot examples from the new task $\mathcal{T}'$ is assessed. A gradient-free algorithm is subsequently deployed to update $w$, enhancing $\hat{m}$'s performance (e.g., loss) on the few-shot examples $Q$.

Finally, after iterating through $K$ steps, the optimum performing LoRA module is applied to the LLM $M_\theta$, yielding the final LLM $M_\phi=\operatorname{LoRA}(M_\theta, \hat{m})$. This serves as an effectively adjusted model for the unseen task $\mathcal{T}'$, which will then be deployed and not updated anymore.
 
\subsection{LoRA Tuning on Upstream Tasks}



LoRA effectively minimizes the count of trainable parameters through the process of decomposing the attention weight matrix update of the LLM, denoted as $W_0 \in R^{d\times k}$, into low-rank matrices. In more specific terms, LoRA exhibits the updated weight matrix in the form $W_0 + \delta W = W_0 + AB$, where $A \in \mathbb{R}^{ d\times r}$ and $B \in \mathbb{R}^{ r\times k}$ are novel low-rank matrices with rank $r$, a dimension significantly smaller than those of $d$ and $k$. In this context, the product $AB$ defines the LoRA module $m$, as previously elaborated. By leveraging this low-rank decomposition, LoRA imposes limitations that substantially reduce the number of trainable parameters necessary for adjusting the LLM weights.

 


\subsection{\textsc{Compose}: Element-wise Composition of LoRA Modules}

Within the \textsc{Compose} phase, we implement an element-wise method for composing multiple LoRA modules. This process integrates the corresponding parameters of the LoRA modules, necessitating that the modules being combined maintain an identical rank $r$ in order to align the structures effectively. Given that $m_i = A_iB_i$, the combined LoRA module, denoted by $\hat{m}$, can be represented as:
\begin{equation}
    \hat{m} = (w_1A_1+w_2A_2+\dots+w_NA_N)(w_1B_1+w_2B_2+\dots+w_NB_N)\textrm{.}
\end{equation}

It is vital to acknowledge that the composition of an excessive number of LoRA modules concurrently may increase the weights to be searched for, potentially destabilizing the \lorahub learning process and consequently inhibiting optimal performance. To mitigate this, we implement a pre-filtering procedure for the candidate LoRA modules before composition, similar to the pre-ranking stage utilized in search engines. In our experiments, we employ random selection to minimize the candidate space and further exploration of enhanced pre-filtering algorithms is regarded as future work. This concept is further elaborated in Section~\ref{sec:limitation}.





\subsection{\textsc{Adapt}: Weight Optimization via Gradient-free Methods}

During the \textsc{Adapt} stage, our task is to fine-tune the coefficients $w$ in order to boost the model's competency on a limited set of examples from a previously unseen task. One might think of using gradient descent for optimizing $w$, following standard backpropagation methods. However, this approach demands constructing a hypernet for all LoRA modules, reminiscent of differentiable architecture search methodologies~\citep{nas_hyper}. This requirement for substantial GPU memory and time poses a challenge. Given that $w$ consists of a relatively small number of parameters, we opted for gradient-free methods for optimization instead of gradient descent.

We utilize a black-box optimization technique following previous works ~\citep{blackbox_tuning} to find the optimal weights. The optimization process is steered by the cross-entropy loss, setting the goal to locate the best weight set $\{w_1, w_2, \dots, w_N\}$ that reduces the loss $L$ on the validation set $Q$. Furthermore, we incorporate L1 regularization to penalize the sum of the absolute values of all the $w$s, helping to prevent obtaining extreme values. Consequently, the final objective of \lorahub is to minimize $L + \alpha \cdot \sum_{i=1}^N |w_i|$, where $\alpha$ serves as a hyperparameter.

In terms of the gradient-free methodology, we leverage Shiwa, a combinatorial optimization approach \citep{NGOpt}. Shiwa offers a variety of algorithms and chooses the most suitable optimization algorithm for different circumstances. In most of the forthcoming experimental setups, we primarily employ the Covariance Matrix Adaptive Evolution Strategies (CMA-ES)~\citep{Hansen1996AdaptingAN}. Being a stochastic, derivative-free, population-based optimization algorithm, CMA-ES proves to be suitable for a wide range of optimization tasks. It adaptively updates a search distribution, characterized by a covariance matrix. In each iteration, CMA-ES refreshes the mean and covariance of the distribution toward target function optimization. For our case, we deploy this algorithm to shape the search space of $w$, and eventually select the best weights based on their performance on the few-shot examples from the unseen task.