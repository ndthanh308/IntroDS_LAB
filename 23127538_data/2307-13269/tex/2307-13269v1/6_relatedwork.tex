




\paragraph{Model Merging}

Our method substantially draws on the concept of LoRA module composition, and thus, aligns with the significant thread of research in model merging. This research focus is broadly categorized based on the ultimate objectives of model merging.

 

The first category focuses on merging entire models, and the goal is to combine individually trained models to approximate the performance benefits of model ensembling or multi-task learning.
Prior works such as \cite{Matena2021MergingMW} and \cite{jin2023dataless} operated under the assumption of shared model architectures. \cite{Matena2021MergingMW} amalgamates models by approximating Gaussian posterior distributions garnered from Fisher information, while \cite{jin2023dataless} merges models steered by weights that minimize the differences in prediction.
Another approach is merging models with different architectures.
For instance, \cite{ainsworth2023git} configures weights of different models prior to their merger. Following this objective, \cite{stoica2023zipit} merges models operating on varying tasks by identifying common features, without requiring additional training.
Unlike these works, our work focuses on merging models to enable cross-task generalization.

The second category most closely aligns with our research, stemming from a shared motivation of module composition. Various scholars have made advances in this line of research: \cite{Kingetsu2021NeuralNM} decomposes and recomposes modules on the basis of their functionality;
\cite{Ilharco2022EditingMW} proposes modulating model behavior using task vectors;
\cite{Lv2023ParameterefficientWE} amalgamates parameter-efficient modules weighted according to task similarity;
\cite{Zhang2023ComposingPM} crafts modules by employing specific arithmetic operations; 
\cite{sun2023multitask} improves few-shot performance of unseen tasks by multi-task pre-training of prompts;
\cite{Chronopoulou2023AdapterSoupWA} averages adapter weights intended for transfer;
\cite{Ponti2023CombiningPM} focuses on jointly learning adapters and a routing function that allocates skills to each task;
and \cite{Muqeeth2023SoftMO} concentrates on amalgamating experts in mixture of experts models;
However, these methods generally necessitate multi-task training or human prior on module selection for the downstream task.
In contrast, our method does not impose any special training requirements and simply employs vanilla LoRA tuning. 
Additionally, the module selection for downstream tasks is entirely data-driven without human prior knowledge.
This design gives the advantage of easily adding new LoRA modules for reuse, allowing our method to flexibly scale up the number of potential LoRA module candidates in the future.



 


\paragraph{Mixture of Experts}
The Mixture of Experts (MoE) is an ensemble method, often visualized as a collection of sub-modules, or 'experts', each specializing in processing different types of input data. Each expert in this system is controlled by a unique gating network, activated based on the distinct nature of the input data. For every token in these input sequences, this network identifies and engages the most suitable experts to process the data. As a result, the performance is superior compared to relying on a single, generic model for all types of input. This technique has proven instrumental in numerous domains, such as natural language processing and computer vision~\citep{Jacobs1991AdaptiveMO,Shazeer2017OutrageouslyLN, Du2021GLaMES,zhang2022skillnetnlu}. Our methodology displays similarities to MoE, wherein upstream-trained LoRA modules can be aligned with MoE's expert design. A noteworthy distinguishing factor is that our approach mechanism does not require any specialized manipulation of LoRAs during training while facilitating dynamic LoRA module assembly at any scale, each pre-tuned to different tasks. In contrast, MoE mandates a predetermined count of experts during both the training and testing phases. Recent studies on the interrelation between MoE and instruction tuning have demonstrated that the simultaneous application of both approaches enhances the effectiveness of each individually~\citep{shen2023mixtureofexperts}.

\paragraph{Cross-Task Generalization}
Recent advancements like CrossFit~\citep{ye-etal-2021-crossfit}, ExT5~\citep{aribandi2022ext}, FLAN~\citep{Wei2021FinetunedLM}, T0~\citep{sanh2021t0}, InstructGPT~\citep{InstructGPT}, and ReCross~\citep{recross} have been striving to foster a vastly multi-task model's generalization across different tasks, very much aligned with the objectives of our research. Among this cohort, the connections of CrossFit and ReCross with \lorahub are particularly noteworthy. 
The CrossFit framework \citep{ye-etal-2021-crossfit} mandates a minimal number of labeled examples of the target task for few-shot fine-tuning. However, its limitation lies in the application of task names as hard prefixes in templates, posing challenges in the task's generalization. On the other hand, while ReCross mitigates the need for labels in few-shot examples for retrieval, it necessitates a fine-tuning process using the retrieved data. This procedure appears time-consuming when compared to \lorahub's approach. Through the deployment of few-shot labeled examples and a gradient-free optimization process, \lorahub facilitates an iterative update of weights to compose the LoRA modules. The resultant method is more efficient and cost-effective relative to previous work. Overall, \lorahub offers a more practical and viable solution to the optimization process.


