





Significant progress in natural language processing  has been largely fueled by large-scale pretrained language models (LLMs) such as OpenAI GPT~\citep{gpt3}, Flan-T5~\citep{flan}, and LLaMA~\citep{touvron2023llama}. These models demonstrate top-tier performance across multiple NLP tasks. However, their enormous parameter size presents issues regarding computational efficiency and memory usage during fine-tuning. To mitigate these challenges, Low-Rank Adaptation (LoRA)~\citep{hu2022lora} has emerged as an efficient fine-tuning technique~\citep{lester-etal-2021-power,junxian_unified_2022,input_tuning}. By reducing memory demands and computational costs, it speeds up LLM training. LoRA achieves this by freezing the base model parameters (that is, an LLM) and training a lightweight, ancillary module, which regularly delivers high performance on target tasks.






While prior research has targeted the efficiency enhancement facilitated by LoRA, there is a dearth of investigation into the inherent modularity and composability of LoRA modules. Typically, previous methods train LoRA modules to specialize in individual tasks and domains. Yet, the intrinsic modularity of LoRA modules presents an intriguing research question: \textit{Is it feasible to compose LoRA modules for efficiently generalizing towards unseen tasks?}

In this paper, we tap into the potential of LoRA modularity for broad task generalization, going beyond single-task training to meticulously compose LoRA modules for malleable performance on unknown tasks. Crucially, our method enables an automatic assembling of LoRA modules, eliminating dependency on manual design or human expertise. With just a handful of examples from unencountered tasks (e.g., 5), our approach can autonomously orchestrate compatible LoRA modules without human intrusion. We do not presuppose which LoRA modules trained on specific tasks can be combined, offering flexibility in amalgamating any modules provided they conform to the specification (e.g., using the same LLM). As our approach leverages several available LoRA modules, we refer to it as \lorahub and denote our learning method as \textbf{\lorahub learning}.





To validate the efficiency of our proposed methods, we test our approaches using the widely recognized BBH benchmark with Flan-T5~\citep{flan} serving as the base LLM. The results underline the effectiveness of the LoRA module composition for unfamiliar tasks through a few-shot \lorahub learning process. Remarkably, our methodology achieves a score that closely matches the performance of few-shot, \textit{in-context} learning. Additionally, our method substantially reduces the inference cost compared to in-context learning, eliminating the requirement of examples as inputs for the LLM. 
Our learning procedure is also notable for its computational efficiency, using a \textit{gradient-free} approach to obtain the coefficients of LoRA modules and requiring only a handful of inference steps for unseen tasks. 
For instance, when applied to the BBH, our methodology can deliver superior performance in less than a minute using a single A100.

Importantly, \lorahub learning can feasibly be accomplished with a CPU-only machine, given that it merely requires proficiency to process LLM inference. 
With its versatility and robust performance, 
our work lays the foundation for the genesis of a platform, where users could effortlessly share, access, and apply trained LoRA modules to new tasks in this arena. 
Through such a platform, we envision the cultivation of a repository of reusable LoRA modules encompassing a myriad of capabilities. It also sets the stage for cooperative AI development, enabling the community to enrich the LLM's capabilities collectively via dynamic LoRA composition. The potential to share and reuse modules ensures the optimal application of resources across various tasks.











