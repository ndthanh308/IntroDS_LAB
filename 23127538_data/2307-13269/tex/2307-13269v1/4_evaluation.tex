In this section, we detail our principal experiments. We begin with an overview of the experimental setup and implementation intricacies. Following this, we share our findings and offer a succinct interpretation.

\subsection{Experimental Framework}

\paragraph{Large Language Model} We employ Flan-T5~\citep{flan} as our chosen Large Language Model (LLM). This series of models, characterized by similar structures and varying sizes, exhibits exemplary zero-shot and few-shot capabilities commensurate with the model size. Our investigation particularly targets the Flan-T5 large model.

\paragraph{Candidate LoRA Modules} Our methodology requires a compendium of LoRA modules trained on preceding tasks. For parity with Flan, we adopt the tasks utilized to instruct Flan-T5, thereby incorporating nearly $200$ distinct tasks and their corresponding instructions~\footnote{We accessed these publicly available tasks via \href{https://huggingface.co/datasets/conceptofmind/FLAN\_2022}{\texttt{huggingface.co/datasets/conceptofmind/FLAN\_2022}}.}. Following this, we created several LoRA modules as possible candidates~\footnote{These LoRA modules can be accessed at \href{https://huggingface.co/models?search=lorahub}{\texttt{huggingface.co/models?search=lorahub}}.}.
For the pre-filtering process, during each experimental sequence, we randomly select $20$ LoRA modules for potential considerations. 

\input{tab_main}





\paragraph{Dataset and Evaluation}

Our method is evaluated using the Big-Bench Hard (BBH) benchmark, a well-established standard that consists of multiple-choice questions from a variety of domains. We employ 27 tasks from this benchmark, following the challenging stipulations for language models outlined by previous researchers. Throughout all tasks, we employ Exact Match (EM) as our evaluation metric.

\subsection{Implementation Details}
We implemented LoRA tuning using the Huggingface PEFT library~\citep{peft}, keeping the default LoRA tuning hyperparameter at $r=16$. The gradient-free method was implemented using the open-source Nevergrad optimization library~\citep{nevergrad}, imposing a constraint that the absolute value of LoRA weights should not exceed $1.5$. At the outset, all LoRA modules were set at zero weights.

In our standard settings, we permitted a maximum of $40$ attempts to calculate the loss on the samples, denoted by the maximum step size $K$. Five examples were used during optimization, the same number as used in the few-shot in-context learning scenario. And the hyperparameter $\alpha$ is set as $0.05$. Regarding the hyperparameters for training candidate LoRA modules, we maintained consistency across all modules, setting the batch size at 64, the learning rate at $1e-4$, and the number of training epochs at 10.

\subsection{Main Results} 

Our experimental data shown in Table \ref{tab:performance} shows the superior efficacy of our method in comparison to zero-shot learning while closely resembling the performance of in-context learning (ICL) in few-shot scenarios. This observation is based on an average of five separate experimental runs. Importantly, our model utilizes an equivalent number of tokens as the zero-shot method, notably fewer than the count used by ICL. Albeit occasional performance fluctuations, our method consistently outperforms zero-shot learning in most instances. Where our method truly stands out is its ability to surpass ICL at optimum performance, but with less token usage. In the era of LLMs, the input length is directly proportional to the inference cost, and thus \lorahub's ability to economize on input tokens while approaching the peak performance grows increasingly significant.
