
\paragraph{Large Language Models} We assume that a large language model $M_\theta$ is based on Transformer architecture~\citep{transformer_paper} and has been pre-trained on a large-scale natural language corpus. The model architecture can be either encoder-decoder~\citep{t5_paper} or decoder-only~\citep{gpt3}. Also, $M_\theta$ could also have been fine-tuned with a large set of instruction-following datasets such as Flan Colleciton~\citep{longpre2023flan} and PromptSource~\citep{bach2022promptsource}.

\paragraph{Cross-Task Generalization} Assume we have $N$ distinct \textit{upstream tasks}, referred to as $\mathbb{T}=\{\mathcal{T}_1, ..., \mathcal{T}_N\}$. In practical situations, it is typical for users to desire an LLM to execute novel tasks that it has not encountered beforeâ€” an ability widely known as \textit{cross-task generalization}. 

Generally, cross-task generalization falls into two categories: zero-shot learning~\citep{mishra-etal-2022-cross,t0_paper,flan,chatgpt_paper, recross}, which necessitates no labeled examples of the new task, and few-shot learning~\citep{ye-etal-2021-crossfit,min-etal-2022-metaicl} which demands a handful of labeled examples. 
Our paper principally concentrates on the latter, wherein for an unseen target task $\mathcal{T}' \notin \mathbb{T}$, users can only furnish a limited set of labeled examples, $Q$. 
Our aim is to modify the model $M_\theta$ to accustom itself to task $\mathcal{T}'$ using only $Q$. An intuitive method would be to directly fine-tune the weights of $M_\theta$ based on $Q$, yielding an updated model $M_\phi$ with enhanced performance on $\mathcal{T}'$. However, this approach is inefficient, time-consuming, and unstable when $Q$ is small.

\paragraph{LoRA Tuning}  
LoRA~\citep{hu2022lora}, a parameter-efficient fine-tuning method, facilitates the adaptation of LLMs using a small-scale external module, eliminating the need for fine-tuning the entire model. LoRA tuning involves preserving the model weights while incorporating \textit{trainable} low-rank decomposition matrices in each layer as an adapter module. 
Compared to the full LLM, this module possesses significantly fewer trainable parameters, paving the way for rapid and robust adaptation using minimal examples. 
As such, LoRA tuning presents a resource-efficient technique to quickly adapt LLMs for novel tasks with restricted training data. 
Nevertheless, its performance often dwindles with extremely constrained examples (e.g. $5$ input-output pairs), as our experiments later illustrate. Hence, traditional LoRA methods primarily concentrate on training and testing within the same tasks, rather than venturing into few-shot cross-task generalization.


