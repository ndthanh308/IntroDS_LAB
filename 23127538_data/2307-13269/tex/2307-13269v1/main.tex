
\documentclass{article} %
\usepackage{iclr2023_conference,times}

\input{math_commands.tex}

\usepackage[colorlinks=true,citecolor=.]{hyperref}
\usepackage{url}
\usepackage{inconsolata}
\usepackage{eurosym}
\usepackage{todonotes} 
\usepackage{subcaption}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage{array}
\usepackage{longtable}
\usepackage{makecell}
\usepackage{xspace}
\usepackage{xcolor,colortbl}
\usepackage{tcolorbox}
\usepackage{arydshln}
\usepackage{adjustbox}
\newcommand{\dline}{\hdashline[0.5pt/1pt]}
\usepackage{tikz}
\usepackage[tikz]{bclogo}
\usepackage{pgfplots}
\pgfplotsset{width=1.0\columnwidth}
\usepackage{multirow}

\usepackage{makecell}
\usepackage{pifont}
\usepackage{bbm}
\usepackage{rotating}
\usepackage{tablefootnote}
\usepackage{soul}
\usepackage{booktabs}
\usepackage{tikz}
\usepackage[tikz]{bclogo}
\usepackage{pgfplotstable}
\usepackage{mdframed}
\usepackage{graphicx}

\newcommand{\yuchen}[1]{\textcolor{red}{[Yuchen: #1]}} 
\newcommand{\lorahub}{LoraHub\xspace}
\newcommand{\flan}[0]{FLAN-T5\xspace}
\newcommand\scale[1]{{\fontfamily{mathtt}\selectfont {#1}}\xspace}
\newcommand{\icon}{\raisebox{-1pt}{% Figure removed}\xspace}


\newmdenv[
  backgroundcolor=purple!10,
  skipabove=1em,
  skipbelow=0em,
  leftline=true,
  topline=false,
  bottomline=false,
  rightline=false,
  linecolor=purple!88,
  linewidth=4pt
]{githubquote}


\title{\icon \lorahub{}: Efficient Cross-Task Generalization via Dynamic LoRA  Composition}



\newcommand*{\affaddr}[1]{#1}
\newcommand*{\affmark}[1][*]{\textsuperscript{#1}}
\newcommand*{\email}[1]{\tt{#1}}

\author{
\makecell{Chengsong Huang\affmark[\textdagger\S]{\thanks{The first three authors contributed equally to this work. Correspondence to Qian Liu at \href{mailto:liuqian@sea.com}{\texttt{liuqian@sea.com}}.}}~~, Qian Liu\affmark[\textdagger]$^*$, Bill Yuchen Lin\affmark[$\lozenge$]$^*$, Tianyu Pang\affmark[\textdagger], Chao Du\affmark[\textdagger], Min Lin\affmark[\textdagger]}
\\
\centerline{\affaddr{\affmark[\textdagger]Sea AI Lab, Singapore}}\\
\centerline{\affaddr{\affmark[\S]Washington University in St. Louis, MO, USA}}\\
\centerline{\affaddr{\affmark[$\lozenge$]Allen Institute for AI, Seattle, WA, USA}}
}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy %
\begin{document}


\maketitle

\begin{abstract}
\input{0_abs}
\end{abstract}

% Figure environment removed


\section{Introduction}

\input{1_intro}



\section{Problem Statement}

\input{2_problem}

\section{Methodology}
\input{3_method}


   










\section{Evaluation}
\input{4_evaluation}


\section{Analysis}
\input{5_analysis}



















\section{Related Work}


\input{6_relatedwork}

\section{Conclusion}


In this work, we have introduced \lorahub, a strategic framework for composing LoRA modules trained on diverse tasks in order to achieve adaptable performance on new tasks. Our approach enables the fluid combination of multiple LoRA modules using just a few examples from a novel task, without requiring additional model parameters or human expertise. The empirical results on the BBH benchmark demonstrate that \lorahub can effectively match the performance of in-context learning in few-shot scenarios, removing the need for in-context examples during inference.
Overall, our work shows the promise of strategic LoRA composability for rapidly adapting LLMs to diverse tasks. 
By fostering reuse and combination of LoRA modules, we can work towards more general and adaptable LLMs while minimizing training costs.
 
 




\section{Limitations \& Future Work}\label{sec:limitation}

\paragraph{Pre-Filtering of LoRA Module Candidates}
While our method is successful in identifying and weighting relevant aspects from seen tasks to enhance unseen task performance, relying entirely on the model to perform this search can lead to increased computational demands and potentially unstable results. Incorporating a pre-filtering step to select only pertinent LoRA modules could expedite and refine performance. Identifying an effective selection strategy warrants further study.

\paragraph{Method Applicability to Decoder-Only Models}
All experiments for this study were executed using the encoder-decoder architecture. We aspire to extrapolate this method to decoder-only models such as GPT~\citep{gpt3}, aiming to determine its applicability in such contexts.

\paragraph{Exploring Superior Optimization Methods}
The use of a genetic algorithm for optimization in this study raises the question of whether better optimization approaches exist that could provide superior gradient-free optimization with limited examples. Although the current method has shown adequate performance, there is still room for improvement.

\bibliography{iclr2023_conference}
\bibliographystyle{iclr2023_conference}


\end{document}
