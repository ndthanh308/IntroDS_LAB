\begin{thebibliography}{45}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ainsworth et~al.(2023)Ainsworth, Hayase, and
  Srinivasa]{ainsworth2023git}
Samuel Ainsworth, Jonathan Hayase, and Siddhartha Srinivasa.
\newblock Git re-basin: Merging models modulo permutation symmetries.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=CQsmMYmlP5T}.

\bibitem[An et~al.(2022)An, Li, Lin, Liu, Chen, Fu, Chen, Zheng, and
  Lou]{input_tuning}
Shengnan An, Yifei Li, Zeqi Lin, Qian Liu, Bei Chen, Qiang Fu, Weizhu Chen,
  Nanning Zheng, and Jian{-}Guang Lou.
\newblock Input-tuning: Adapting unfamiliar inputs to frozen pretrained models.
\newblock \emph{ArXiv preprint}, abs/2203.03131, 2022.
\newblock URL \url{https://arxiv.org/abs/2203.03131}.

\bibitem[Aribandi et~al.(2022)Aribandi, Tay, Schuster, Rao, Zheng, Mehta,
  Zhuang, Tran, Bahri, Ni, Gupta, Hui, Ruder, and Metzler]{aribandi2022ext}
Vamsi Aribandi, Yi~Tay, Tal Schuster, Jinfeng Rao, Huaixiu~Steven Zheng,
  Sanket~Vaibhav Mehta, Honglei Zhuang, Vinh~Q. Tran, Dara Bahri, Jianmo Ni,
  Jai~Prakash Gupta, Kai Hui, Sebastian Ruder, and Donald Metzler.
\newblock Ext5: Towards extreme multi-task scaling for transfer learning.
\newblock In \emph{The Tenth International Conference on Learning
  Representations, {ICLR} 2022, Virtual Event, April 25-29, 2022}.
  OpenReview.net, 2022.
\newblock URL \url{https://openreview.net/forum?id=Vzh1BFUCiIX}.

\bibitem[Bach et~al.(2022)Bach, Sanh, Yong, Webson, Raffel, Nayak, Sharma, Kim,
  Bari, Fevry, Alyafeai, Dey, Santilli, Sun, Ben-david, Xu, Chhablani, Wang,
  Fries, Al-shaibani, Sharma, Thakker, Almubarak, Tang, Radev, Jiang, and
  Rush]{bach2022promptsource}
Stephen Bach, Victor Sanh, Zheng~Xin Yong, Albert Webson, Colin Raffel,
  Nihal~V. Nayak, Abheesht Sharma, Taewoon Kim, M~Saiful Bari, Thibault Fevry,
  Zaid Alyafeai, Manan Dey, Andrea Santilli, Zhiqing Sun, Srulik Ben-david,
  Canwen Xu, Gunjan Chhablani, Han Wang, Jason Fries, Maged Al-shaibani, Shanya
  Sharma, Urmish Thakker, Khalid Almubarak, Xiangru Tang, Dragomir Radev, Mike
  Tian-jian Jiang, and Alexander Rush.
\newblock {P}rompt{S}ource: An integrated development environment and
  repository for natural language prompts.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics: System Demonstrations}, pp.\  93--104, Dublin,
  Ireland, 2022. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.acl-demo.9}.
\newblock URL \url{https://aclanthology.org/2022.acl-demo.9}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert{-}Voss, Krueger,
  Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin,
  Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{gpt3}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
  Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
  Askell, Sandhini Agarwal, Ariel Herbert{-}Voss, Gretchen Krueger, Tom
  Henighan, Rewon Child, Aditya Ramesh, Daniel~M. Ziegler, Jeffrey Wu, Clemens
  Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott
  Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec
  Radford, Ilya Sutskever, and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell,
  Maria{-}Florina Balcan, and Hsuan{-}Tien Lin (eds.), \emph{Advances in Neural
  Information Processing Systems 33: Annual Conference on Neural Information
  Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual}, 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html}.

\bibitem[Chronopoulou et~al.(2023)Chronopoulou, Peters, Fraser, and
  Dodge]{Chronopoulou2023AdapterSoupWA}
Alexandra Chronopoulou, Matthew Peters, Alexander Fraser, and Jesse Dodge.
\newblock {A}dapter{S}oup: Weight averaging to improve generalization of
  pretrained language models.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  EACL 2023}, pp.\  2054--2063, Dubrovnik, Croatia, 2023. Association for
  Computational Linguistics.
\newblock URL \url{https://aclanthology.org/2023.findings-eacl.153}.

\bibitem[Chung et~al.(2022)Chung, Hou, Longpre, Zoph, Tay, Fedus, Li, Wang,
  Dehghani, Brahma, Webson, Gu, Dai, Suzgun, Chen, Chowdhery, Valter, Narang,
  Mishra, Yu, Zhao, Huang, Dai, Yu, Petrov, hsin Chi, Dean, Devlin, Roberts,
  Zhou, Le, and Wei]{flan}
Hyung~Won Chung, Le~Hou, S.~Longpre, Barret Zoph, Yi~Tay, William Fedus, Eric
  Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson,
  Shixiang~Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha
  Chowdhery, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams~Wei Yu, Vincent
  Zhao, Yanping Huang, Andrew~M. Dai, Hongkun Yu, Slav Petrov, Ed~Huai hsin
  Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc~V. Le, and Jason
  Wei.
\newblock Scaling instruction-finetuned language models.
\newblock \emph{ArXiv preprint}, abs/2210.11416, 2022.
\newblock URL \url{https://arxiv.org/abs/2210.11416}.

\bibitem[Du et~al.(2022)Du, Huang, Dai, Tong, Lepikhin, Xu, Krikun, Zhou, Yu,
  Firat, Zoph, Fedus, Bosma, Zhou, Wang, Wang, Webster, Pellat, Robinson,
  Meier{-}Hellstern, Duke, Dixon, Zhang, Le, Wu, Chen, and Cui]{Du2021GLaMES}
Nan Du, Yanping Huang, Andrew~M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong
  Xu, Maxim Krikun, Yanqi Zhou, Adams~Wei Yu, Orhan Firat, Barret Zoph, Liam
  Fedus, Maarten~P. Bosma, Zongwei Zhou, Tao Wang, Yu~Emma Wang, Kellie
  Webster, Marie Pellat, Kevin Robinson, Kathleen~S. Meier{-}Hellstern, Toju
  Duke, Lucas Dixon, Kun Zhang, Quoc~V. Le, Yonghui Wu, Zhifeng Chen, and
  Claire Cui.
\newblock Glam: Efficient scaling of language models with mixture-of-experts.
\newblock In Kamalika Chaudhuri, Stefanie Jegelka, Le~Song, Csaba
  Szepesv{\'{a}}ri, Gang Niu, and Sivan Sabato (eds.), \emph{International
  Conference on Machine Learning, {ICML} 2022, 17-23 July 2022, Baltimore,
  Maryland, {USA}}, volume 162 of \emph{Proceedings of Machine Learning
  Research}, pp.\  5547--5569. {PMLR}, 2022.
\newblock URL \url{https://proceedings.mlr.press/v162/du22c.html}.

\bibitem[Hansen \& Ostermeier(1996)Hansen and Ostermeier]{Hansen1996AdaptingAN}
Nikolaus Hansen and Andreas Ostermeier.
\newblock Adapting arbitrary normal mutation distributions in evolution
  strategies: the covariance matrix adaptation.
\newblock \emph{Proceedings of IEEE International Conference on Evolutionary
  Computation}, pp.\  312--317, 1996.

\bibitem[He et~al.(2022)He, Zhou, Ma, Berg{-}Kirkpatrick, and
  Neubig]{junxian_unified_2022}
Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg{-}Kirkpatrick, and Graham
  Neubig.
\newblock Towards a unified view of parameter-efficient transfer learning.
\newblock In \emph{The Tenth International Conference on Learning
  Representations, {ICLR} 2022, Virtual Event, April 25-29, 2022}.
  OpenReview.net, 2022.
\newblock URL \url{https://openreview.net/forum?id=0RDcd5Axok}.

\bibitem[Hu et~al.(2022)Hu, Shen, Wallis, Allen{-}Zhu, Li, Wang, Wang, and
  Chen]{hu2022lora}
Edward~J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen{-}Zhu, Yuanzhi Li,
  Shean Wang, Lu~Wang, and Weizhu Chen.
\newblock Lora: Low-rank adaptation of large language models.
\newblock In \emph{The Tenth International Conference on Learning
  Representations, {ICLR} 2022, Virtual Event, April 25-29, 2022}.
  OpenReview.net, 2022.
\newblock URL \url{https://openreview.net/forum?id=nZeVKeeFYf9}.

\bibitem[Ilharco et~al.(2022)Ilharco, Ribeiro, Wortsman, Gururangan, Schmidt,
  Hajishirzi, and Farhadi]{Ilharco2022EditingMW}
Gabriel Ilharco, Marco~Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan,
  Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi.
\newblock Editing models with task arithmetic.
\newblock \emph{ArXiv preprint}, abs/2212.04089, 2022.
\newblock URL \url{https://arxiv.org/abs/2212.04089}.

\bibitem[Jacobs et~al.(1991)Jacobs, Jordan, Nowlan, and
  Hinton]{Jacobs1991AdaptiveMO}
Robert~A. Jacobs, Michael~I. Jordan, Steven~J. Nowlan, and Geoffrey~E. Hinton.
\newblock Adaptive mixtures of local experts.
\newblock \emph{Neural Computation}, 3:\penalty0 79--87, 1991.
\newblock URL
  \url{https://direct.mit.edu/neco/article-abstract/3/1/79/5560/Adaptive-Mixtures-of-Local-Experts?redirectedFrom=fulltext}.

\bibitem[Jin et~al.(2023)Jin, Ren, Preotiuc-Pietro, and Cheng]{jin2023dataless}
Xisen Jin, Xiang Ren, Daniel Preotiuc-Pietro, and Pengxiang Cheng.
\newblock Dataless knowledge fusion by merging weights of language models.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=FCnohuR6AnM}.

\bibitem[Kingetsu et~al.(2021)Kingetsu, Kobayashi, and
  Suzuki]{Kingetsu2021NeuralNM}
Hiroaki Kingetsu, Kenichi Kobayashi, and Taiji Suzuki.
\newblock Neural network module decomposition and recomposition.
\newblock \emph{ArXiv preprint}, abs/2112.13208, 2021.
\newblock URL \url{https://arxiv.org/abs/2112.13208}.

\bibitem[Lester et~al.(2021)Lester, Al-Rfou, and
  Constant]{lester-etal-2021-power}
Brian Lester, Rami Al-Rfou, and Noah Constant.
\newblock The power of scale for parameter-efficient prompt tuning.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pp.\  3045--3059, Online and Punta Cana,
  Dominican Republic, 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.emnlp-main.243}.
\newblock URL \url{https://aclanthology.org/2021.emnlp-main.243}.

\bibitem[Lin et~al.(2022)Lin, Tan, Miller, Tian, and Ren]{recross}
Bill~Yuchen Lin, Kangmin Tan, Chris Miller, Beiwen Tian, and Xiang Ren.
\newblock Unsupervised cross-task generalization via retrieval augmentation.
\newblock In \emph{NeurIPS}, 2022.
\newblock URL
  \url{http://papers.nips.cc/paper\_files/paper/2022/hash/8a0d3ae989a382ce6e50312bc35bf7e1-Abstract-Conference.html}.

\bibitem[Liu et~al.(2020)Liu, Moreau, Preuss, Rozi{\`e}re, Rapin, Teytaud, and
  Teytaud]{NGOpt}
Jialin Liu, A.~Moreau, Mike Preuss, Baptiste Rozi{\`e}re, J{\'e}r{\'e}my Rapin,
  Fabien Teytaud, and Olivier Teytaud.
\newblock Versatile black-box optimization.
\newblock \emph{Proceedings of the 2020 Genetic and Evolutionary Computation
  Conference}, 2020.

\bibitem[Longpre et~al.(2023)Longpre, Hou, Vu, Webson, Chung, Tay, Zhou, Le,
  Zoph, Wei, and Roberts]{longpre2023flan}
Shayne Longpre, Le~Hou, Tu~Vu, Albert Webson, Hyung~Won Chung, Yi~Tay, Denny
  Zhou, Quoc~V. Le, Barret Zoph, Jason Wei, and Adam Roberts.
\newblock The flan collection: Designing data and methods for effective
  instruction tuning, 2023.

\bibitem[Lv et~al.(2023)Lv, Ding, Qin, Liu, and
  Sun]{Lv2023ParameterefficientWE}
Xingtai Lv, Ning Ding, Yujia Qin, Zhiyuan Liu, and Maosong Sun.
\newblock Parameter-efficient weight ensembling facilitates task-level
  knowledge transfer.
\newblock In \emph{Annual Meeting of the Association for Computational
  Linguistics}, 2023.

\bibitem[Mangrulkar et~al.(2022)Mangrulkar, Gugger, Debut, Belkada, and
  Paul]{peft}
Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, and Sayak
  Paul.
\newblock Peft: State-of-the-art parameter-efficient fine-tuning methods.
\newblock \url{https://github.com/huggingface/peft}, 2022.

\bibitem[Matena \& Raffel(2021)Matena and Raffel]{Matena2021MergingMW}
Michael Matena and Colin Raffel.
\newblock Merging models with fisher-weighted averaging.
\newblock \emph{ArXiv preprint}, abs/2111.09832, 2021.
\newblock URL \url{https://arxiv.org/abs/2111.09832}.

\bibitem[Min et~al.(2022)Min, Lewis, Zettlemoyer, and
  Hajishirzi]{min-etal-2022-metaicl}
Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi.
\newblock {M}eta{ICL}: Learning to learn in context.
\newblock In \emph{Proceedings of the 2022 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pp.\  2791--2809, Seattle, United States, 2022. Association
  for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.naacl-main.201}.
\newblock URL \url{https://aclanthology.org/2022.naacl-main.201}.

\bibitem[Mishra et~al.(2022)Mishra, Khashabi, Baral, and
  Hajishirzi]{mishra-etal-2022-cross}
Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi.
\newblock Cross-task generalization via natural language crowdsourcing
  instructions.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pp.\  3470--3487,
  Dublin, Ireland, 2022. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.acl-long.244}.
\newblock URL \url{https://aclanthology.org/2022.acl-long.244}.

\bibitem[Muqeeth et~al.(2023)Muqeeth, Liu, and Raffel]{Muqeeth2023SoftMO}
Mohammed Muqeeth, Haokun Liu, and Colin Raffel.
\newblock Soft merging of experts with adaptive routing.
\newblock \emph{ArXiv preprint}, abs/2306.03745, 2023.
\newblock URL \url{https://arxiv.org/abs/2306.03745}.

\bibitem[OpenAI(2022)]{chatgpt_paper}
OpenAI.
\newblock {ChatGPT}.
\newblock 2022.
\newblock URL \url{https://openai.com/blog/chatgpt}.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin,
  Zhang, Agarwal, Slama, Ray, Schulman, Hilton, Kelton, Miller, Simens, Askell,
  Welinder, Christiano, Leike, and Lowe]{InstructGPT}
Long Ouyang, Jeff Wu, Xu~Jiang, Diogo Almeida, Carroll~L. Wainwright, Pamela
  Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John
  Schulman, Jacob Hilton, Fraser Kelton, Luke~E. Miller, Maddie Simens, Amanda
  Askell, Peter Welinder, Paul~Francis Christiano, Jan Leike, and Ryan~J. Lowe.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{ArXiv preprint}, abs/2203.02155, 2022.
\newblock URL \url{https://arxiv.org/abs/2203.02155}.

\bibitem[Pasupat \& Liang(2015)Pasupat and Liang]{wtq}
Panupong Pasupat and Percy Liang.
\newblock Compositional semantic parsing on semi-structured tables.
\newblock In \emph{Proceedings of the 53rd Annual Meeting of the Association
  for Computational Linguistics and the 7th International Joint Conference on
  Natural Language Processing (Volume 1: Long Papers)}, pp.\  1470--1480,
  Beijing, China, 2015. Association for Computational Linguistics.
\newblock \doi{10.3115/v1/P15-1142}.
\newblock URL \url{https://aclanthology.org/P15-1142}.

\bibitem[Ponti et~al.(2023)Ponti, Sordoni, Bengio, and
  Reddy]{Ponti2023CombiningPM}
Edoardo~Maria Ponti, Alessandro Sordoni, Yoshua Bengio, and Siva Reddy.
\newblock Combining parameter-efficient modules for task-level generalisation.
\newblock In \emph{Proceedings of the 17th Conference of the European Chapter
  of the Association for Computational Linguistics}, pp.\  687--702, Dubrovnik,
  Croatia, 2023. Association for Computational Linguistics.
\newblock URL \url{https://aclanthology.org/2023.eacl-main.49}.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{t5_paper}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J. Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{J. Mach. Learn. Res.}, 21:\penalty0 140:1--140:67, 2020.
\newblock URL \url{http://jmlr.org/papers/v21/20-074.html}.

\bibitem[Rapin \& Teytaud(2018)Rapin and Teytaud]{nevergrad}
J.~Rapin and O.~Teytaud.
\newblock {Nevergrad - A gradient-free optimization platform}.
\newblock \url{https://GitHub.com/FacebookResearch/Nevergrad}, 2018.

\bibitem[Sanh et~al.(2022{\natexlab{a}})Sanh, Webson, Raffel, Bach, Sutawika,
  Alyafeai, Chaffin, Stiegler, Raja, Dey, Bari, Xu, Thakker, Sharma, Szczechla,
  Kim, Chhablani, Nayak, Datta, Chang, Jiang, Wang, Manica, Shen, Yong, Pandey,
  Bawden, Wang, Neeraj, Rozen, Sharma, Santilli, F{\'{e}}vry, Fries, Teehan,
  Scao, Biderman, Gao, Wolf, and Rush]{t0_paper}
Victor Sanh, Albert Webson, Colin Raffel, Stephen~H. Bach, Lintang Sutawika,
  Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey,
  M~Saiful Bari, Canwen Xu, Urmish Thakker, Shanya~Sharma Sharma, Eliza
  Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal~V. Nayak, Debajyoti Datta,
  Jonathan Chang, Mike~Tian{-}Jian Jiang, Han Wang, Matteo Manica, Sheng Shen,
  Zheng~Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj,
  Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault F{\'{e}}vry, Jason~Alan
  Fries, Ryan Teehan, Teven~Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and
  Alexander~M. Rush.
\newblock Multitask prompted training enables zero-shot task generalization.
\newblock In \emph{The Tenth International Conference on Learning
  Representations, {ICLR} 2022, Virtual Event, April 25-29, 2022}.
  OpenReview.net, 2022{\natexlab{a}}.
\newblock URL \url{https://openreview.net/forum?id=9Vrb9D0WI4}.

\bibitem[Sanh et~al.(2022{\natexlab{b}})Sanh, Webson, Raffel, Bach, Sutawika,
  Alyafeai, Chaffin, Stiegler, Raja, Dey, Bari, Xu, Thakker, Sharma, Szczechla,
  Kim, Chhablani, Nayak, Datta, Chang, Jiang, Wang, Manica, Shen, Yong, Pandey,
  Bawden, Wang, Neeraj, Rozen, Sharma, Santilli, F{\'{e}}vry, Fries, Teehan,
  Scao, Biderman, Gao, Wolf, and Rush]{sanh2021t0}
Victor Sanh, Albert Webson, Colin Raffel, Stephen~H. Bach, Lintang Sutawika,
  Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey,
  M~Saiful Bari, Canwen Xu, Urmish Thakker, Shanya~Sharma Sharma, Eliza
  Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal~V. Nayak, Debajyoti Datta,
  Jonathan Chang, Mike~Tian{-}Jian Jiang, Han Wang, Matteo Manica, Sheng Shen,
  Zheng~Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj,
  Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault F{\'{e}}vry, Jason~Alan
  Fries, Ryan Teehan, Teven~Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and
  Alexander~M. Rush.
\newblock Multitask prompted training enables zero-shot task generalization.
\newblock In \emph{The Tenth International Conference on Learning
  Representations, {ICLR} 2022, Virtual Event, April 25-29, 2022}.
  OpenReview.net, 2022{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=9Vrb9D0WI4}.

\bibitem[Shazeer et~al.(2017)Shazeer, Mirhoseini, Maziarz, Davis, Le, Hinton,
  and Dean]{Shazeer2017OutrageouslyLN}
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc~V. Le,
  Geoffrey~E. Hinton, and Jeff Dean.
\newblock Outrageously large neural networks: The sparsely-gated
  mixture-of-experts layer.
\newblock In \emph{5th International Conference on Learning Representations,
  {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track
  Proceedings}. OpenReview.net, 2017.
\newblock URL \url{https://openreview.net/forum?id=B1ckMDqlg}.

\bibitem[Shen et~al.(2023)Shen, Hou, Zhou, Du, Longpre, Wei, Chung, Zoph,
  Fedus, Chen, Vu, Wu, Chen, Webson, Li, Zhao, Yu, Keutzer, Darrell, and
  Zhou]{shen2023mixtureofexperts}
Sheng Shen, Le~Hou, Yanqi Zhou, Nan Du, Shayne Longpre, Jason Wei, Hyung~Won
  Chung, Barret Zoph, William Fedus, Xinyun Chen, Tu~Vu, Yuexin Wu, Wuyang
  Chen, Albert Webson, Yunxuan Li, Vincent Zhao, Hongkun Yu, Kurt Keutzer,
  Trevor Darrell, and Denny Zhou.
\newblock Mixture-of-experts meets instruction tuning:a winning combination for
  large language models, 2023.

\bibitem[Stoica et~al.(2023)Stoica, Bolya, Bjorner, Hearn, and
  Hoffman]{stoica2023zipit}
George Stoica, Daniel Bolya, Jakob Bjorner, Taylor Hearn, and Judy Hoffman.
\newblock Zipit! merging models from different tasks without training.
\newblock \emph{arXiv}, 2023.

\bibitem[Sun et~al.(2022)Sun, Shao, Qian, Huang, and Qiu]{blackbox_tuning}
Tianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing Huang, and Xipeng Qiu.
\newblock Black-box tuning for language-model-as-a-service.
\newblock In Kamalika Chaudhuri, Stefanie Jegelka, Le~Song, Csaba
  Szepesv{\'{a}}ri, Gang Niu, and Sivan Sabato (eds.), \emph{International
  Conference on Machine Learning, {ICML} 2022, 17-23 July 2022, Baltimore,
  Maryland, {USA}}, volume 162 of \emph{Proceedings of Machine Learning
  Research}, pp.\  20841--20855. {PMLR}, 2022.
\newblock URL \url{https://proceedings.mlr.press/v162/sun22e.html}.

\bibitem[Sun et~al.(2023)Sun, He, Zhu, Qiu, and Huang]{sun2023multitask}
Tianxiang Sun, Zhengfu He, Qin Zhu, Xipeng Qiu, and Xuanjing Huang.
\newblock Multitask pre-training of modular prompt for {C}hinese few-shot
  learning.
\newblock In \emph{Proceedings of the 61st Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pp.\  11156--11172,
  Toronto, Canada, July 2023. Association for Computational Linguistics.
\newblock URL \url{https://aclanthology.org/2023.acl-long.625}.

\bibitem[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux,
  Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, Rodriguez, Joulin, Grave, and
  Lample]{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
  Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric
  Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and
  Guillaume Lample.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{ArXiv preprint}, abs/2302.13971, 2023.
\newblock URL \url{https://arxiv.org/abs/2302.13971}.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{transformer_paper}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna~M. Wallach,
  Rob Fergus, S.~V.~N. Vishwanathan, and Roman Garnett (eds.), \emph{Advances
  in Neural Information Processing Systems 30: Annual Conference on Neural
  Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA,
  {USA}}, pp.\  5998--6008, 2017.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html}.

\bibitem[Wei et~al.(2022)Wei, Bosma, Zhao, Guu, Yu, Lester, Du, Dai, and
  Le]{Wei2021FinetunedLM}
Jason Wei, Maarten Bosma, Vincent~Y. Zhao, Kelvin Guu, Adams~Wei Yu, Brian
  Lester, Nan Du, Andrew~M. Dai, and Quoc~V. Le.
\newblock Finetuned language models are zero-shot learners.
\newblock In \emph{The Tenth International Conference on Learning
  Representations, {ICLR} 2022, Virtual Event, April 25-29, 2022}.
  OpenReview.net, 2022.
\newblock URL \url{https://openreview.net/forum?id=gEZrGCozdqR}.

\bibitem[Ye et~al.(2021)Ye, Lin, and Ren]{ye-etal-2021-crossfit}
Qinyuan Ye, Bill~Yuchen Lin, and Xiang Ren.
\newblock {C}ross{F}it: A few-shot learning challenge for cross-task
  generalization in {NLP}.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pp.\  7163--7189, Online and Punta Cana,
  Dominican Republic, 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.emnlp-main.572}.
\newblock URL \url{https://aclanthology.org/2021.emnlp-main.572}.

\bibitem[Zhang et~al.(2019)Zhang, Ren, and Urtasun]{nas_hyper}
Chris Zhang, Mengye Ren, and Raquel Urtasun.
\newblock Graph hypernetworks for neural architecture search.
\newblock In \emph{7th International Conference on Learning Representations,
  {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019}. OpenReview.net, 2019.
\newblock URL \url{https://openreview.net/forum?id=rkgW0oA9FX}.

\bibitem[Zhang et~al.(2022)Zhang, Tang, Dai, Zhou, Wu, and
  Shi]{zhang2022skillnetnlu}
Fan Zhang, Duyu Tang, Yong Dai, Cong Zhou, Shuangzhi Wu, and Shuming Shi.
\newblock Skillnet-nlu: A sparsely activated model for general-purpose natural
  language understanding, 2022.

\bibitem[Zhang et~al.(2023)Zhang, Chen, Liu, and He]{Zhang2023ComposingPM}
Jinghan Zhang, Shiqi Chen, Junteng Liu, and Junxian He.
\newblock Composing parameter-efficient modules with arithmetic operations.
\newblock \emph{ArXiv preprint}, abs/2306.14870, 2023.
\newblock URL \url{https://arxiv.org/abs/2306.14870}.

\end{thebibliography}
