 

In this section, we present a comprehensive exploration of our proposed method's attributes and reveal some unexpected insights. All analytical experiments are performed using Flan-T5 large. We respond to several research queries to further our investigations into this method.

\begin{githubquote} 
    How does \lorahub learning compare with LoRA tuning and conventional fine-tuning?
\end{githubquote}

% Figure environment removed
 


As demonstrated in Table~\ref{tab:performance}, we assess the efficacy of our \lorahub{} method in conjunction with the few-shot ICL technique. Both deliver substantial results in the few-shot context, eliminating the requirement for gradient data. However, a pertinent question emerges: How does \lorahub, operating without the need for gradients, compare to gradient-dependent methods such as LoRA tuning and full fine-tuning? To address this, we present a comparison of our approach, full fine-tuning (FFT), and LoRA tuning (LoRA) in Figure~\ref{fig:compare2LoRA}, drawing upon three typical tasks from the BBH benchmark: binary classification on \textit{Boolean Expressions}, multi-class on \textit{Date Understanding}, and text generation for \textit{Object Counting}. The comparison reveals that the relative effectiveness of our method varies across different tasks when pitted against those reliant on gradients.

All three methods exhibit volatility with very few examples, occasionally witnessing a dip in performance despite an increase in sample number. Despite such challenges associated with scanty data, our framework manages to deliver matching or even superior results compared to its gradient-based counterparts. If provided with more than $20$ instances, \lorahub's performance aligns with that of gradient-focused training, but only in simpler tasks such as binary classification (i.e., Boolean Expressions). Our method shows significant improvements on complex multi-class classification and generation tasks, albeit there remains a measurable performance gap when compared to traditional tuning methods given an adequate number of examples.




\begin{githubquote}
    Which LoRA modules are most effective for BBH tasks? 
\end{githubquote}

We hypothesized that the amalgamation of LoRA modules could incorporate skills and insights from a variety of specific tasks. To evaluate this, we examined the extent of influence a single LoRA module had amongst the 27 tasks from the Big-Bench Hard (BBH) benchmark. We measured the impact of each isolated task by calculating the average absolute weight. The top five tasks, denoted in Table~\ref{tab:useful}, were found to have substantial influence, as indicated by their maximum average weights, which suggested that they were notably more effective in cross-task transfer. Interestingly, their weight values insinuate that these tasks were significantly improved in sequential tasks. A unifying characteristic amongst these top five tasks was their inherent requirement of reading comprehension and reasoningâ€”skills of higher complexity.

\begin{table}[bt]
    \centering
    \small
    \caption{The top five useful upstream tasks for BBH tasks.}
    \label{tab:useful}
    \begin{tabular}{llll}
    \toprule
        Rank & Task Name & Usefulness & Task Description \\ \midrule
        1 & wiqa\_last\_process & 0.719 & \makecell[l]{Identifying the last step of a given process.} \\ 
        2 & race\_middle\_Is\_this\_the\_right\_answer & 0.681 & \makecell[l]{Determining if given answer is correct.} \\ 
        3 & wiqa\_final\_process & 0.626 & \makecell[l]{Identifying the final step of a given process.} \\ 
        4 & adversarial\_qa\_dbidaf\_based\_on & 0.611 & \makecell[l]{Answering question created\\ by an adversarial model-in-the-loop.} \\ 
        5 & web\_questions\_whats\_the\_answer & 0.583 & \makecell[l]{Answering question based on \\information extracted from the web.} \\ \bottomrule 
    \end{tabular}
\end{table}

 





\begin{githubquote}
    Evaluation of the gradient-free optimization method's efficacy.
\end{githubquote}


To assess the precision of our gradient-free optimization method in correctly identifying the most suitable LoRA module for a given downstream task, we carried out an empirical study using the WikiTableQuestions~\citep{wtq} (WTQ) dataset.
We strategically included a LoRA module that was specifically trained on the WTQ dataset into our pool of LoRA candidate modules, which originally stemmed from tasks exclusive to the Flan Collection. Subsequently, we designated WTQ as the targeted downstream task and computed the weights consistent with the methods employed in \lorahub learning. As an end result, the WTQ-specific LoRA module was awarded the highest weight, exemplifying the algorithm's success in recognizing it as the most relevant.
Moreover, the compounded LoRA module demonstrated marginal superiority over the WTQ LoRA module. This underscores the claim that our proposed method has the ability to proficiently select the optimal upstream LoRA for an unexplored task.

 