@inproceedings{ilharco2023editing,
    author = {Gabriel Ilharco and Marco Tulio Ribeiro and Mitchell Wortsman and Ludwig Schmidt and Hannaneh Hajishirzi and Ali Farhadi},
    booktitle = {The Eleventh International Conference on Learning Representations },
    title = {Editing models with task arithmetic},
    year = {2023}
}

@article{DBLP:journals/corr/abs-2303-17564,
  author       = {Shijie Wu and
                  Ozan Irsoy and
                  Steven Lu and
                  Vadim Dabravolski and
                  Mark Dredze and
                  Sebastian Gehrmann and
                  Prabhanjan Kambadur and
                  David S. Rosenberg and
                  Gideon Mann},
  title        = {BloombergGPT: {A} Large Language Model for Finance},
  journal      = {CoRR},
  volume       = {abs/2303.17564},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2303.17564},
  doi          = {10.48550/arXiv.2303.17564},
  eprinttype    = {arXiv},
  eprint       = {2303.17564},
  timestamp    = {Mon, 17 Apr 2023 10:34:50 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2303-17564.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{Du2021GLaMES,
    author = {Nan Du and
Yanping Huang and
Andrew M. Dai and
Simon Tong and
Dmitry Lepikhin and
Yuanzhong Xu and
Maxim Krikun and
Yanqi Zhou and
Adams Wei Yu and
Orhan Firat and
Barret Zoph and
Liam Fedus and
Maarten P. Bosma and
Zongwei Zhou and
Tao Wang and
Yu Emma Wang and
Kellie Webster and
Marie Pellat and
Kevin Robinson and
Kathleen S. Meier{-}Hellstern and
Toju Duke and
Lucas Dixon and
Kun Zhang and
Quoc V. Le and
Yonghui Wu and
Zhifeng Chen and
Claire Cui},
    booktitle = {International Conference on Machine Learning, {ICML} 2022, 17-23 July
2022, Baltimore, Maryland, {USA}},
    editor = {Kamalika Chaudhuri and
Stefanie Jegelka and
Le Song and
Csaba Szepesv{\'{a}}ri and
Gang Niu and
Sivan Sabato},
    series = {Proceedings of Machine Learning Research},
    timestamp = {Tue, 12 Jul 2022 01:00:00 +0200},
    title = {GLaM: Efficient Scaling of Language Models with Mixture-of-Experts},
    year = {2022}
}

@article{Jacobs1991AdaptiveMO,
    author = {Robert A. Jacobs and Michael I. Jordan and Steven J. Nowlan and Geoffrey E. Hinton},
    journal = {Neural Computation},
    title = {Adaptive Mixtures of Local Experts},
    year = {1991}
}

@inproceedings{Shazeer2017OutrageouslyLN,
    author = {Noam Shazeer and
Azalia Mirhoseini and
Krzysztof Maziarz and
Andy Davis and
Quoc V. Le and
Geoffrey E. Hinton and
Jeff Dean},
    booktitle = {Proc. of ICLR},
    timestamp = {Thu, 25 Jul 2019 01:00:00 +0200},
    title = {Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts
Layer},
    year = {2017}
}

@article{stoica2023zipit,
    author = {Stoica, George and Bolya, Daniel and Bjorner, Jakob and Hearn, Taylor and Hoffman, Judy},
    journal = {arXiv},
    title = {ZipIt! Merging Models from Different Tasks without Training},
    year = {2023}
}

@inproceedings{ainsworth2023git,
    author = {Samuel Ainsworth and Jonathan Hayase and Siddhartha Srinivasa},
    booktitle = {The Eleventh International Conference on Learning Representations },
    title = {Git Re-Basin: Merging Models modulo Permutation Symmetries},
    year = {2023}
}

@inproceedings{jin2023dataless,
    author = {Xisen Jin and Xiang Ren and Daniel Preotiuc-Pietro and Pengxiang Cheng},
    booktitle = {The Eleventh International Conference on Learning Representations},
    title = {Dataless Knowledge Fusion by Merging Weights of Language Models},
    year = {2023}
}

@article{NGOpt,
    author = {Jialin Liu and A. Moreau and Mike Preuss and Baptiste Rozi{\`e}re and J{\'e}r{\'e}my Rapin and Fabien Teytaud and Olivier Teytaud},
    journal = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference},
    title = {Versatile black-box optimization},
    year = {2020}
}

@inproceedings{sun2023multitask,
    abstract = {Prompt tuning is a parameter-efficient approach to adapting pre-trained language models to downstream tasks. Although prompt tuning has been shown to match the performance of full model tuning when training data is sufficient, it tends to struggle in few-shot learning settings. In this paper, we present Multi-task Pre-trained Modular Prompt (MP2) to boost prompt tuning for few-shot learning. MP2 is a set of combinable prompts pre-trained on 38 Chinese tasks. On downstream tasks, the pre-trained prompts are selectively activated and combined, leading to strong compositional generalization to unseen tasks. To bridge the gap between pre-training and fine-tuning, we formulate upstream and downstream tasks into a unified machine reading comprehension task. Extensive experiments under two learning paradigms, i.e., gradient descent and black-box tuning, show that MP2 significantly outperforms prompt tuning, full model tuning, and prior prompt pre-training methods in few-shot settings. In addition, we demonstrate that MP2 can achieve surprisingly fast and strong adaptation to downstream tasks by merely learning 8 parameters to combine the pre-trained modular prompts.},
    author = {Sun, Tianxiang  and
He, Zhengfu  and
Zhu, Qin  and
Qiu, Xipeng  and
Huang, Xuanjing},
    booktitle = {Proc. of ACL},
    title = {Multitask Pre-training of Modular Prompt for {C}hinese Few-Shot Learning},
    year = {2023}
}

@article{flan,
    author = {Hyung Won Chung and Le Hou and S. Longpre and Barret Zoph and Yi Tay and William Fedus and Eric Li and Xuezhi Wang and Mostafa Dehghani and Siddhartha Brahma and Albert Webson and Shixiang Shane Gu and Zhuyun Dai and Mirac Suzgun and Xinyun Chen and Aakanksha Chowdhery and Dasha Valter and Sharan Narang and Gaurav Mishra and Adams Wei Yu and Vincent Zhao and Yanping Huang and Andrew M. Dai and Hongkun Yu and Slav Petrov and Ed Huai-hsin Chi and Jeff Dean and Jacob Devlin and Adam Roberts and Denny Zhou and Quoc V. Le and Jason Wei},
    journal = {ArXiv preprint},
    title = {Scaling Instruction-Finetuned Language Models},
    year = {2022}
}

@misc{peft,
    author = {Sourab Mangrulkar and Sylvain Gugger and Lysandre Debut and Younes Belkada and Sayak Paul},
    howpublished = {\url{https://github.com/huggingface/peft}},
    title = {PEFT: State-of-the-art Parameter-Efficient Fine-Tuning methods},
    year = {2022}
}

@misc{nevergrad,
    author = {J. Rapin and O. Teytaud},
    howpublished = {\url{https://GitHub.com/FacebookResearch/Nevergrad}},
    journal = {GitHub repository},
    title = {{Nevergrad - A gradient-free optimization platform}},
    year = {2018}
}

@article{bbh,
    author = {BIG-bench authors},
    issn = {2835-8856},
    journal = {Transactions on Machine Learning Research},
    note = {},
    title = {Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models},
    year = {2023}
}

@inproceedings{LIME,
    author = {Yuhuai Wu and
Markus N. Rabe and
Wenda Li and
Jimmy Ba and
Roger B. Grosse and
Christian Szegedy},
    booktitle = {Proc. of ICML},
    editor = {Marina Meila and
Tong Zhang},
    series = {Proceedings of Machine Learning Research},
    timestamp = {Wed, 25 Aug 2021 01:00:00 +0200},
    title = {{LIME:} Learning Inductive Bias for Primitives of Mathematical Reasoning},
    year = {2021}
}

@inproceedings{wtq,
    author = {Pasupat, Panupong  and
Liang, Percy},
    booktitle = {Proc. of ACL},
    title = {Compositional Semantic Parsing on Semi-Structured Tables},
    year = {2015}
}

@inproceedings{hu2022lora,
    author = {Edward J. Hu and
Yelong Shen and
Phillip Wallis and
Zeyuan Allen{-}Zhu and
Yuanzhi Li and
Shean Wang and
Lu Wang and
Weizhu Chen},
    booktitle = {Proc. of ICLR},
    timestamp = {Sat, 20 Aug 2022 01:00:00 +0200},
    title = {LoRA: Low-Rank Adaptation of Large Language Models},
    year = {2022}
}

@article{Muqeeth2023SoftMO,
    author = {Mohammed Muqeeth and Haokun Liu and Colin Raffel},
    journal = {ArXiv preprint},
    title = {Soft Merging of Experts with Adaptive Routing},
    year = {2023}
}

@article{Hansen1996AdaptingAN,
    author = {Nikolaus Hansen and Andreas Ostermeier},
    journal = {Proceedings of IEEE International Conference on Evolutionary Computation},
    title = {Adapting arbitrary normal mutation distributions in evolution strategies: the covariance matrix adaptation},
    year = {1996}
}

@article{touvron2023llama,
    author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
    journal = {ArXiv preprint},
    title = {LLaMA: Open and Efficient Foundation Language Models},
    year = {2023}
}

@inproceedings{gpt3,
    author = {Tom B. Brown and
Benjamin Mann and
Nick Ryder and
Melanie Subbiah and
Jared Kaplan and
Prafulla Dhariwal and
Arvind Neelakantan and
Pranav Shyam and
Girish Sastry and
Amanda Askell and
Sandhini Agarwal and
Ariel Herbert{-}Voss and
Gretchen Krueger and
Tom Henighan and
Rewon Child and
Aditya Ramesh and
Daniel M. Ziegler and
Jeffrey Wu and
Clemens Winter and
Christopher Hesse and
Mark Chen and
Eric Sigler and
Mateusz Litwin and
Scott Gray and
Benjamin Chess and
Jack Clark and
Christopher Berner and
Sam McCandlish and
Alec Radford and
Ilya Sutskever and
Dario Amodei},
    booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
on Neural Information Processing Systems 2020, NeurIPS 2020, December
6-12, 2020, virtual},
    editor = {Hugo Larochelle and
Marc'Aurelio Ranzato and
Raia Hadsell and
Maria{-}Florina Balcan and
Hsuan{-}Tien Lin},
    timestamp = {Tue, 19 Jan 2021 00:00:00 +0100},
    title = {Language Models are Few-Shot Learners},
    year = {2020}
}

@article{input_tuning,
    author = {Shengnan An and
Yifei Li and
Zeqi Lin and
Qian Liu and
Bei Chen and
Qiang Fu and
Weizhu Chen and
Nanning Zheng and
Jian{-}Guang Lou},
    journal = {ArXiv preprint},
    title = {Input-Tuning: Adapting Unfamiliar Inputs to Frozen Pretrained Models},
    year = {2022}
}

@inproceedings{lester-etal-2021-power,
    author = {Lester, Brian  and
Al-Rfou, Rami  and
Constant, Noah},
    booktitle = {Proc. of EMNLP},
    title = {The Power of Scale for Parameter-Efficient Prompt Tuning},
    year = {2021}
}

@inproceedings{lv-etal-2023-parameter,
    abstract = {Recent studies show that large-scale pre-trained language models could be efficaciously adapted to particular tasks in a parameter-efficient manner. The trained lightweight set of parameters, such as adapters, can be easily stored and shared as a capability equipped with the corresponding models. Owning many lightweight parameters, we focus on transferring them between tasks to acquire an improvement in performance of new tasks, the key point of which is to obtain the similarity between tasks. In this paper, we explore 5 parameter-efficient weight ensembling methods to achieve such transferability and verify the effectiveness of them. These methods extract the information of datasets and trained lightweight parameters from different perspectives to obtain the similarity between tasks, and weight the existing lightweight parameters according to the comparability to acquire a suitable module for the initialization of new tasks. We apply them to three parameter-efficient tuning methods and test them on a wide set of downstream tasks. Experimental results show that our methods show an improvement of 5{\%}{\textasciitilde}8{\%} over baselines and could largely facilitate task-level knowledge transfer.},
    author = {Lv, Xingtai  and
Ding, Ning  and
Qin, Yujia  and
Liu, Zhiyuan  and
Sun, Maosong},
    booktitle = {Proc. of ACL},
    title = {Parameter-efficient Weight Ensembling Facilitates Task-level Knowledge Transfer},
    year = {2023}
}

@article{Matena2021MergingMW,
    author = {Michael Matena and Colin Raffel},
    journal = {ArXiv preprint},
    title = {Merging Models with Fisher-Weighted Averaging},
    year = {2021}
}

@inproceedings{Wortsman2022ModelSA,
    author = {Mitchell Wortsman and
Gabriel Ilharco and
Samir Yitzhak Gadre and
Rebecca Roelofs and
Raphael Gontijo Lopes and
Ari S. Morcos and
Hongseok Namkoong and
Ali Farhadi and
Yair Carmon and
Simon Kornblith and
Ludwig Schmidt},
    booktitle = {International Conference on Machine Learning, {ICML} 2022, 17-23 July
2022, Baltimore, Maryland, {USA}},
    editor = {Kamalika Chaudhuri and
Stefanie Jegelka and
Le Song and
Csaba Szepesv{\'{a}}ri and
Gang Niu and
Sivan Sabato},
    series = {Proceedings of Machine Learning Research},
    timestamp = {Thu, 02 Feb 2023 00:00:00 +0100},
    title = {Model soups: averaging weights of multiple fine-tuned models improves
accuracy without increasing inference time},
    year = {2022}
}

@inproceedings{Lv2023ParameterefficientWE,
    author = {Xingtai Lv and Ning Ding and Yujia Qin and Zhiyuan Liu and Maosong Sun},
    booktitle = {Annual Meeting of the Association for Computational Linguistics},
    title = {Parameter-efficient Weight Ensembling Facilitates Task-level Knowledge Transfer},
    year = {2023}
}

@article{Zhang2023ComposingPM,
    author = {Jinghan Zhang and Shiqi Chen and Junteng Liu and Junxian He},
    journal = {ArXiv preprint},
    title = {Composing Parameter-Efficient Modules with Arithmetic Operations},
    year = {2023}
}

@inproceedings{Chronopoulou2023AdapterSoupWA,
    author = {Chronopoulou, Alexandra  and
Peters, Matthew  and
Fraser, Alexander  and
Dodge, Jesse},
    booktitle = {Findings of the Association for Computational Linguistics: EACL 2023},
    title = {{A}dapter{S}oup: Weight Averaging to Improve Generalization of Pretrained Language Models},
    year = {2023}
}

@inproceedings{Ponti2023CombiningPM,
    author = {Ponti, Edoardo Maria  and
Sordoni, Alessandro  and
Bengio, Yoshua  and
Reddy, Siva},
    booktitle = {Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics},
    title = {Combining Parameter-efficient Modules for Task-level Generalisation},
    year = {2023}
}

@article{Kingetsu2021NeuralNM,
    author = {Hiroaki Kingetsu and Kenichi Kobayashi and Taiji Suzuki},
    journal = {ArXiv preprint},
    title = {Neural Network Module Decomposition and Recomposition},
    year = {2021}
}


@inproceedings{junxian_unified_2022,
    author = {Junxian He and
Chunting Zhou and
Xuezhe Ma and
Taylor Berg{-}Kirkpatrick and
Graham Neubig},
    booktitle = {Proc. of ICLR},
    timestamp = {Sat, 20 Aug 2022 01:00:00 +0200},
    title = {Towards a Unified View of Parameter-Efficient Transfer Learning},
    year = {2022}
}

@misc{zhang2022skillnetnlu,
    archiveprefix = {arXiv},
    author = {Fan Zhang and Duyu Tang and Yong Dai and Cong Zhou and Shuangzhi Wu and Shuming Shi},
    eprint = {2203.03312},
    primaryclass = {cs.CL},
    title = {SkillNet-NLU: A Sparsely Activated Model for General-Purpose Natural Language Understanding},
    year = {2022}
}

@misc{shen2023mixtureofexperts,
    archiveprefix = {arXiv},
    author = {Sheng Shen and Le Hou and Yanqi Zhou and Nan Du and Shayne Longpre and Jason Wei and Hyung Won Chung and Barret Zoph and William Fedus and Xinyun Chen and Tu Vu and Yuexin Wu and Wuyang Chen and Albert Webson and Yunxuan Li and Vincent Zhao and Hongkun Yu and Kurt Keutzer and Trevor Darrell and Denny Zhou},
    eprint = {2305.14705},
    primaryclass = {cs.CL},
    title = {Mixture-of-Experts Meets Instruction Tuning:A Winning Combination for Large Language Models},
    year = {2023}
}

@misc{longpre2023flan,
    archiveprefix = {arXiv},
    author = {Shayne Longpre and Le Hou and Tu Vu and Albert Webson and Hyung Won Chung and Yi Tay and Denny Zhou and Quoc V. Le and Barret Zoph and Jason Wei and Adam Roberts},
    eprint = {2301.13688},
    primaryclass = {cs.AI},
    title = {The Flan Collection: Designing Data and Methods for Effective Instruction Tuning},
    year = {2023}
}

@inproceedings{bach2022promptsource,
    author = {Bach, Stephen  and
Sanh, Victor  and
Yong, Zheng Xin  and
Webson, Albert  and
Raffel, Colin  and
Nayak, Nihal V.  and
Sharma, Abheesht  and
Kim, Taewoon  and
Bari, M Saiful  and
Fevry, Thibault  and
Alyafeai, Zaid  and
Dey, Manan  and
Santilli, Andrea  and
Sun, Zhiqing  and
Ben-david, Srulik  and
Xu, Canwen  and
Chhablani, Gunjan  and
Wang, Han  and
Fries, Jason  and
Al-shaibani, Maged  and
Sharma, Shanya  and
Thakker, Urmish  and
Almubarak, Khalid  and
Tang, Xiangru  and
Radev, Dragomir  and
Jiang, Mike Tian-jian  and
Rush, Alexander},
    booktitle = {Proc. of ACL},
    title = {{P}rompt{S}ource: An Integrated Development Environment and Repository for Natural Language Prompts},
    year = {2022}
}

@misc{hao2022structured,
    archiveprefix = {arXiv},
    author = {Yaru Hao and Yutao Sun and Li Dong and Zhixiong Han and Yuxian Gu and Furu Wei},
    eprint = {2212.06713},
    primaryclass = {cs.CL},
    title = {Structured Prompting: Scaling In-Context Learning to 1,000 Examples},
    year = {2022}
}

@article{t5_paper,
    author = {Colin Raffel and
Noam Shazeer and
Adam Roberts and
Katherine Lee and
Sharan Narang and
Michael Matena and
Yanqi Zhou and
Wei Li and
Peter J. Liu},
    journal = {J. Mach. Learn. Res.},
    timestamp = {Fri, 05 Feb 2021 00:00:00 +0100},
    title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text
Transformer},
    year = {2020}
}

@inproceedings{transformer_paper,
    author = {Ashish Vaswani and
Noam Shazeer and
Niki Parmar and
Jakob Uszkoreit and
Llion Jones and
Aidan N. Gomez and
Lukasz Kaiser and
Illia Polosukhin},
    booktitle = {Advances in Neural Information Processing Systems 30: Annual Conference
on Neural Information Processing Systems 2017, December 4-9, 2017,
Long Beach, CA, {USA}},
    editor = {Isabelle Guyon and
Ulrike von Luxburg and
Samy Bengio and
Hanna M. Wallach and
Rob Fergus and
S. V. N. Vishwanathan and
Roman Garnett},
    timestamp = {Thu, 21 Jan 2021 00:00:00 +0100},
    title = {Attention is All you Need},
    year = {2017}
}

@inproceedings{blackbox_tuning,
    author = {Tianxiang Sun and
Yunfan Shao and
Hong Qian and
Xuanjing Huang and
Xipeng Qiu},
    booktitle = {International Conference on Machine Learning, {ICML} 2022, 17-23 July
2022, Baltimore, Maryland, {USA}},
    editor = {Kamalika Chaudhuri and
Stefanie Jegelka and
Le Song and
Csaba Szepesv{\'{a}}ri and
Gang Niu and
Sivan Sabato},
    series = {Proceedings of Machine Learning Research},
    timestamp = {Thu, 23 Mar 2023 00:00:00 +0100},
    title = {Black-Box Tuning for Language-Model-as-a-Service},
    year = {2022}
}

@inproceedings{mishra-etal-2022-cross,
    author = {Mishra, Swaroop  and
Khashabi, Daniel  and
Baral, Chitta  and
Hajishirzi, Hannaneh},
    booktitle = {Proc. of ACL},
    title = {Cross-Task Generalization via Natural Language Crowdsourcing Instructions},
    year = {2022}
}

@inproceedings{t0_paper,
    author = {Victor Sanh and
Albert Webson and
Colin Raffel and
Stephen H. Bach and
Lintang Sutawika and
Zaid Alyafeai and
Antoine Chaffin and
Arnaud Stiegler and
Arun Raja and
Manan Dey and
M Saiful Bari and
Canwen Xu and
Urmish Thakker and
Shanya Sharma Sharma and
Eliza Szczechla and
Taewoon Kim and
Gunjan Chhablani and
Nihal V. Nayak and
Debajyoti Datta and
Jonathan Chang and
Mike Tian{-}Jian Jiang and
Han Wang and
Matteo Manica and
Sheng Shen and
Zheng Xin Yong and
Harshit Pandey and
Rachel Bawden and
Thomas Wang and
Trishala Neeraj and
Jos Rozen and
Abheesht Sharma and
Andrea Santilli and
Thibault F{\'{e}}vry and
Jason Alan Fries and
Ryan Teehan and
Teven Le Scao and
Stella Biderman and
Leo Gao and
Thomas Wolf and
Alexander M. Rush},
    booktitle = {Proc. of ICLR},
    timestamp = {Tue, 24 Jan 2023 00:00:00 +0100},
    title = {Multitask Prompted Training Enables Zero-Shot Task Generalization},
    year = {2022}
}

@inproceedings{recross,
    author = {Bill Yuchen Lin and
Kangmin Tan and
Chris Miller and
Beiwen Tian and
Xiang Ren},
    booktitle = {NeurIPS},
    timestamp = {Thu, 11 May 2023 17:08:21 +0200},
    title = {Unsupervised Cross-Task Generalization via Retrieval Augmentation},
    year = {2022}
}

@article{InstructGPT,
    author = {Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke E. Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Francis Christiano and Jan Leike and Ryan J. Lowe},
    journal = {ArXiv preprint},
    title = {Training language models to follow instructions with human feedback},
    year = {2022}
}


@inproceedings{aribandi2022ext,
    author = {Vamsi Aribandi and
Yi Tay and
Tal Schuster and
Jinfeng Rao and
Huaixiu Steven Zheng and
Sanket Vaibhav Mehta and
Honglei Zhuang and
Vinh Q. Tran and
Dara Bahri and
Jianmo Ni and
Jai Prakash Gupta and
Kai Hui and
Sebastian Ruder and
Donald Metzler},
    booktitle = {Proc. of ICLR},
    timestamp = {Sat, 20 Aug 2022 01:00:00 +0200},
    title = {ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning},
    year = {2022}
}

@inproceedings{Wei2021FinetunedLM,
    author = {Jason Wei and
Maarten Bosma and
Vincent Y. Zhao and
Kelvin Guu and
Adams Wei Yu and
Brian Lester and
Nan Du and
Andrew M. Dai and
Quoc V. Le},
    booktitle = {Proc. of ICLR},
    timestamp = {Sat, 20 Aug 2022 01:00:00 +0200},
    title = {Finetuned Language Models are Zero-Shot Learners},
    year = {2022}
}

@inproceedings{ye-etal-2021-crossfit,
    author = {Ye, Qinyuan  and
Lin, Bill Yuchen  and
Ren, Xiang},
    booktitle = {Proc. of EMNLP},
    title = {{C}ross{F}it: A Few-shot Learning Challenge for Cross-task Generalization in {NLP}},
    year = {2021}
}
@article{chatgpt_paper,
    author = {OpenAI},
    title = {{ChatGPT}},
    url = {https://openai.com/blog/chatgpt},
    year = {2022}
}


@inproceedings{min-etal-2022-metaicl,
    author = {Min, Sewon  and
Lewis, Mike  and
Zettlemoyer, Luke  and
Hajishirzi, Hannaneh},
    booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
    title = {{M}eta{ICL}: Learning to Learn In Context},
    year = {2022}
}

@inproceedings{nas_hyper,
    author = {Chris Zhang and
Mengye Ren and
Raquel Urtasun},
    booktitle = {Proc. of ICLR},
    timestamp = {Thu, 25 Jul 2019 01:00:00 +0200},
    title = {Graph HyperNetworks for Neural Architecture Search},
    year = {2019}
}

@inproceedings{wang-etal-2022-adamix,
    author = {Wang, Yaqing  and
Agarwal, Sahaj  and
Mukherjee, Subhabrata  and
Liu, Xiaodong  and
Gao, Jing  and
Awadallah, Ahmed Hassan  and
Gao, Jianfeng},
    booktitle = {Proc. of EMNLP},
    title = {{A}da{M}ix: Mixture-of-Adaptations for Parameter-efficient Model Tuning},
    year = {2022}
}

@article{Gema2023ParameterEfficientFO,
    author = {Aryo Pradipta Gema and Luke Daines and Pasquale Minervini and Beatrice Alex},
    journal = {ArXiv preprint},
    title = {Parameter-Efficient Fine-Tuning of LLaMA for the Clinical Domain},
    year = {2023}
}

@misc{WinNT,
    author = {crumb},
    howpublished = {\url{https://crumbly.medium.com/llama-2-molora-f5f909434711}},
    title = {Llama-2, Mixutre of LoRA},
    year = {2023}
}

@article{Liu2022FewShotPF,
  title={Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning},
  author={Haokun Liu and Derek Tam and Mohammed Muqeeth and Jay Mohta and Tenghao Huang and Mohit Bansal and Colin Raffel},
  journal={ArXiv},
  year={2022},
  volume={abs/2205.05638},
  url={https://api.semanticscholar.org/CorpusID:248693283}
}

% long-context
@article{wangchunshu2023efficent,
  author       = {Wangchunshu Zhou and
                  Yuchen Eleanor Jiang and
                  Ryan Cotterell and
                  Mrinmaya Sachan},
  title        = {Efficient Prompting via Dynamic In-Context Learning},
  journal      = {CoRR},
  volume       = {abs/2305.11170},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2305.11170},
  doi          = {10.48550/ARXIV.2305.11170},
  eprinttype    = {arXiv},
  eprint       = {2305.11170},
  timestamp    = {Thu, 25 May 2023 15:41:47 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2305-11170.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{ge2023incontext,
  author       = {Tao Ge and
                  Jing Hu and
                  Xun Wang and
                  Si{-}Qing Chen and
                  Furu Wei},
  title        = {In-context Autoencoder for Context Compression in a Large Language
                  Model},
  journal      = {CoRR},
  volume       = {abs/2307.06945},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2307.06945},
  doi          = {10.48550/ARXIV.2307.06945},
  eprinttype    = {arXiv},
  eprint       = {2307.06945},
  timestamp    = {Mon, 06 Nov 2023 15:17:28 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2307-06945.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{alexis2023adapting,
  author       = {Alexis Chevalier and
                  Alexander Wettig and
                  Anirudh Ajith and
                  Danqi Chen},
  title        = {Adapting Language Models to Compress Contexts},
  journal      = {CoRR},
  volume       = {abs/2305.14788},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2305.14788},
  doi          = {10.48550/ARXIV.2305.14788},
  eprinttype    = {arXiv},
  eprint       = {2305.14788},
  timestamp    = {Tue, 06 Jun 2023 18:10:43 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2305-14788.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{jiang-etal-2023-llmlingua,
    title = "LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models",
    author = "Huiqiang Jiang and Qianhui Wu and Chin-Yew Lin and Yuqing Yang and Lili Qiu",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    publisher = "Association for Computational Linguistics",
    url = "https://arxiv.org/abs/2310.05736",
}

@inproceedings{li-etal-2023-compressing,
    title = "Compressing Context to Enhance Inference Efficiency of Large Language Models",
    author = "Yucheng Li and Bo Dong and Chenghua Lin and Frank Guerin",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    publisher = "Association for Computational Linguistics",
    url = "https://arxiv.org/abs/2310.06201",
}

@article{jiang-etal-2023-longllm,
  author       = {Huiqiang Jiang and
                  Qianhui Wu and
                  Xufang Luo and
                  Dongsheng Li and
                  Chin{-}Yew Lin and
                  Yuqing Yang and
                  Lili Qiu},
  title        = {LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios
                  via Prompt Compression},
  journal      = {CoRR},
  volume       = {abs/2310.06839},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2310.06839},
  doi          = {10.48550/ARXIV.2310.06839},
  eprinttype    = {arXiv},
  eprint       = {2310.06839},
  timestamp    = {Tue, 24 Oct 2023 14:46:18 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2310-06839.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Jang2023ExploringTB,
  title={Exploring the Benefits of Training Expert Language Models over Instruction Tuning},
  author={Joel Jang and Seungone Kim and Seonghyeon Ye and Doyoung Kim and Lajanugen Logeswaran and Moontae Lee and Kyungjae Lee and Minjoon Seo},
  booktitle={International Conference on Machine Learning},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:256627673}
}

@inproceedings{DBLP:conf/icml/WuWGLZSL23,
  author       = {Chengyue Wu and
                  Teng Wang and
                  Yixiao Ge and
                  Zeyu Lu and
                  Ruisong Zhou and
                  Ying Shan and
                  Ping Luo},
  editor       = {Andreas Krause and
                  Emma Brunskill and
                  Kyunghyun Cho and
                  Barbara Engelhardt and
                  Sivan Sabato and
                  Jonathan Scarlett},
  title        = {{\(\pi\)}-Tuning: Transferring Multimodal Foundation Models with Optimal
                  Multi-task Interpolation},
  booktitle    = {International Conference on Machine Learning, {ICML} 2023, 23-29 July
                  2023, Honolulu, Hawaii, {USA}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {202},
  pages        = {37713--37727},
  publisher    = {{PMLR}},
  year         = {2023},
  url          = {https://proceedings.mlr.press/v202/wu23t.html},
  timestamp    = {Mon, 28 Aug 2023 17:23:09 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/WuWGLZSL23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{
yadav2023tiesmerging,
title={{TIES}-Merging: Resolving Interference When Merging Models},
author={Prateek Yadav and Derek Tam and Leshem Choshen and Colin Raffel and Mohit Bansal},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=xtaX3WyCj1}
}