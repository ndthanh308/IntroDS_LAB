% Figure environment removed


Recent progress in natural language processing (NLP) has been largely fueled by large language models (LLMs) such as OpenAI GPT~\citep{gpt3}, \flan~\citep{flan}, and LLaMA~\citep{touvron2023llama}. These models demonstrate top-tier performance across different NLP tasks. However, their enormous parameter size presents issues regarding computational efficiency and memory usage during fine-tuning. To mitigate these challenges, Low-Rank Adaptation (LoRA)~\citep{hu2022lora} has emerged as a parameter-efficient fine-tuning technique~\citep{lester-etal-2021-power,junxian_unified_2022,input_tuning}. By reducing memory demands and computational costs, it speeds up LLM training. LoRA achieves this by freezing the base model parameters (that is, an LLM) and training a lightweight module, which regularly delivers high performance on target tasks.

While prior research has targeted the efficiency enhancement facilitated by LoRA, there is a dearth of investigation into the inherent modularity and composability of LoRA modules. Typically, previous methods train LoRA modules to specialize in individual tasks. Yet, the intrinsic modularity of LoRA modules presents an intriguing research question: \textit{\textbf{Would it be possible to compose LoRA modules to generalize to novel tasks in an efficient manner?}}
In this paper, we tap into the potential of LoRA modularity for broad task generalization, going beyond single-task training to meticulously compose LoRA modules for malleable performance on unknown tasks. Crucially, our method enables an automatic assembling of LoRA modules, eliminating dependency on manual design or human expertise. With just a handful of examples from new tasks (e.g., 5), our approach can autonomously compose compatible LoRA modules without human intrusion. 
We do not make assumptions about which LoRA modules trained on particular tasks can be combined, allowing for flexibility in amalgamating any modules as long as they conform to the specification (e.g., using the same LLM).
As our approach leverages several available LoRA modules, we refer to it as \lorahub and denote our learning method as \textbf{\lorahub learning}.


To validate the efficiency of our proposed methods, we test our approaches using the widely recognized BBH benchmark with \flan~\citep{flan} serving as the base LLM. The results underline the effectiveness of the LoRA module composition for unfamiliar tasks through a few-shot \lorahub learning process.
Notably, our methodology achieves an average performance that closely matches that of few-shot in-context learning, while demonstrating a superior upper bound, particularly when using different demonstration examples.
Additionally, our method substantially reduces the inference cost compared to in-context learning, eliminating the requirement of examples as inputs for the LLM.
With fewer tokens per example during inference, our method significantly reduces computational overhead and enables faster responses.
It aligns with a broader research trend, where recent studies are actively exploring approaches to reduce the number of input tokens~\citep{wangchunshu2023efficent,ge2023incontext,alexis2023adapting,jiang-etal-2023-llmlingua,li-etal-2023-compressing,jiang-etal-2023-longllm}.
Our learning procedure is also notable for its computational efficiency, using a \textit{gradient-free} approach to obtain the coefficients of LoRA modules and requiring only a handful of inference steps for unseen tasks.
For example, when applied to a new task in BBH, our methodology can deliver superior performance in less than a minute using a single A100 card.

Importantly, \lorahub learning can feasibly be accomplished with a CPU-only machine, requiring proficiency solely for processing LLM inference. In our pursuit to democratize artificial intelligence, we are taking an important step forward by envisioning the establishment of the LoRA platform. The platform would serve as a marketplace where users can seamlessly share and access well-trained LoRA modules for diverse applications.
LoRA providers have the flexibility to freely share or sell their modules on the platform without compromising data privacy.
Users, equipped with CPU capability, can leverage trained LoRA modules contributed by others through automated distribution and composition algorithms.
This platform not only cultivates a repository of reusable LoRA modules with a myriad of capabilities but also sets the stage for cooperative AI development.
It empowers the community to collectively enrich the LLM's capabilities through dynamic LoRA composition.
