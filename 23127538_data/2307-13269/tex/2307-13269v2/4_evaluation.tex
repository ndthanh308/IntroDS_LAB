In this section, we provide details on our main experiments. First, we give an overview of the experimental setup and implementation details. Next, we present our findings along with the results.

\subsection{Experimental setup}

\paragraph{Large Language Model} In our main experiments, we employ \flan{}~\citep{flan}, particularly \flan{}-large, as the base LLM. 
The model has shown impressive abilities to perform zero-shot and few-shot learning.

\paragraph{Candidate LoRA Modules} Our methodology requires a compendium of LoRA modules trained on preceding tasks. For parity with FLAN, we adopt the tasks utilized to instruct \flan{}, thereby incorporating nearly $200$ distinct tasks and their corresponding instructions~\footnote{We released used the dataset at \href{https://huggingface.co/datasets/lorahub/flanv2}{\texttt{huggingface.co/datasets/lorahub/flanv2}}.}. Following this, we trained several LoRA modules as potential candidates.
During each experimental sequence, we randomly select $20$ LoRA modules from them as the candidate for our \lorahub{} learning. 

\paragraph{Dataset and evaluation}

Our method is evaluated using the Big-Bench Hard (BBH) benchmark, a well-established standard that consists of multiple-choice questions from a variety of domains.
The benchmark consists of $27$ different tasks, which are regarded to be challenging for language models.
For all tasks, we employ the exact match (EM) as our evaluation metric.

\input{tab_main}

\paragraph{Baseline Setup}

To enhance the demonstration of our method's performance, we expanded our comparisons beyond the zero-shot and in-context learning settings. We specifically chose three representative gradient-based methods for comparison: full fine-tuning (FFT), LoRA tuning (LoRA)~\citep{hu2022lora}, and IA3 fine-tuning (IA3)~\citep{Liu2022FewShotPF}.
For all gradient-based methods, for a fair comparsion, we train for $40$ epochs on the same three runs of $5$ examples employed in our methods.
In the case of FFT, a learning rate of 3e-5 is employed, whereas for IA3 and LoRA, we adopt a learning rate of 2e-4.
We report the performance of each method on the test set at the end of training (averaged over three runs) without any model selection to avoid potential selection bias.

\subsection{Main results} 

As shown in Table~\ref{tab:performance}, our experimental results demonstarte the superior efficacy of our method in comparison to zero-shot learning while closely resembling the performance of in-context learning (ICL) in few-shot scenarios. This observation is derived from an average performance of three runs, each leveraging different few-shot examples.
Importantly, our model utilizes an equivalent number of tokens as the zero-shot method, notably fewer than the count used by ICL. 
Although occasional performance fluctuations, our method consistently outperforms zero-shot learning in most tasks.
In the era of LLMs, the input length is directly proportional to the inference cost, and thus \lorahub's ability to economize on input tokens while approaching the peak performance grows increasingly significant.
Moreover, as shown in Appendix Table~\ref{tab:max_perf}, the upper bound performance of our method across these runs can surpass ICL on $18$ tasks, demonstrating its potential for future development.

Even when compared to certain gradient-based optimization methods, our approach consistently demonstrates competitive performance. For example, as depicted in Table~\ref{tab:performance}, our method exhibits a notable improvement of $3.1\%$ on average in contrast to the promising IA3 method. Nevertheless, we acknowledge that our approach still falls behind LoRA tuning and full fine-tuning, especially in tasks that exhibit significant deviation from the upstream task. Taking Dyck Languages as an example, both \lorahub and ICL achieve only an average performance of nearly $1.0\%$ on these tasks, while LoRA and FFT methods showcase impressive results with only $5$ examples.

\subsection{Discussion}

LoraHub addresses the challenge of reducing inference costs by eliminating the need for processing additional tokens, resulting in a noticeable reduction in overall inference expenses. However, it introduces an inherent cost during the \textsc{Adapt} stage, necessitating extra inference steps, such as the $40$ steps employed in our experiments. This introduces a trade-off between choosing the ICL approach and LoraHub, with the decision typically hinging on the nature of the situation.

For one-time ad-hoc tasks, the ICL approach should be more pragmatic due to LoraHub's additional inference step costs. In such scenarios, where immediate, single-use solutions are preferred, the simplicity and efficiency of ICL might outweigh the benefits of potential savings offered by LoraHub. Conversely, for recurring or similar tasks, LoraHub emerges as a compelling option. Despite the added inference step cost, LoraHub's ability to efficiently handle repetitive tasks, often occurring thousands of times, while concurrently reducing overall expenses, positions it as a viable option in such kind of situations.

In summary, our intention is not to replace ICL, but to present LoraHub as a complementary strategy with performance-efficiency trade-offs. Thus, we encourage a careful consideration of specific use cases and requirements when choosing between ICL and LoraHub, recognizing that the optimal solution may vary based on the nature and frequency of the tasks at hand.
