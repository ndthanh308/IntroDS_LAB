
\documentclass{article} % For LaTeX2e
\usepackage{iclr2024_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}
\usepackage{wrapfig}
\usepackage[colorlinks=true,citecolor=.]{hyperref}
\usepackage{url}
\usepackage{inconsolata}
\usepackage{eurosym}
\usepackage{todonotes} 
\usepackage{subcaption}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage{array}
\usepackage{longtable}
\usepackage{makecell}
\usepackage{xspace}
\usepackage{xcolor,colortbl}
\usepackage{tcolorbox}
\usepackage{arydshln}
\usepackage{adjustbox}
\newcommand{\dline}{\hdashline[0.5pt/1pt]}
\usepackage{tikz}
\usepackage[tikz]{bclogo}
\usepackage{pgfplots}
\pgfplotsset{width=1.0\columnwidth}
\usepackage{multirow}

\usepackage{makecell}
\usepackage{pifont}
\usepackage{bbm}
\usepackage{rotating}
\usepackage{tablefootnote}
\usepackage{soul}
\usepackage{booktabs}
\usepackage{tikz}
\usepackage[tikz]{bclogo}
\usepackage{pgfplotstable}
\usepackage{mdframed}
\usepackage{graphicx}
% \usepackage{ulem}

\newcommand{\yuchen}[1]{\textcolor{red}{[Yuchen: #1]}} 
\newcommand{\lorahub}{LoraHub\xspace}
\newcommand{\flan}[0]{FLAN-T5\xspace}
\newcommand\scale[1]{{\fontfamily{mathtt}\selectfont {#1}}\xspace}
\newcommand{\icon}{\raisebox{-1pt}{% Figure removed}\xspace}

\newenvironment{rebuttal}
{
    \color{blue} % Set the text color to red
}
{
    \color{blue} % Reset the text color to black when the environment ends
}



\newmdenv[
  backgroundcolor=purple!10,
  skipabove=1em,
  skipbelow=0em,
  leftline=true,
  topline=false,
  bottomline=false,
  rightline=false,
  linecolor=purple!88,
  linewidth=4pt
]{githubquote}


\title{\icon \lorahub{}: Efficient Cross-Task Generalization via Dynamic LoRA Composition}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.
\newcommand*{\affaddr}[1]{#1}
\newcommand*{\affmark}[1][*]{\textsuperscript{#1}}
\newcommand*{\email}[1]{\tt{#1}}

\author{
\makecell{Chengsong Huang\affmark[\textdagger\S]{\thanks{The first three authors contributed equally to this work. Correspondence to Qian Liu at \href{mailto:liuqian@sea.com}{\texttt{liuqian@sea.com}}.}}~~, Qian Liu\affmark[\textdagger]$^*$, Bill Yuchen Lin\affmark[$\lozenge$]$^*$, Tianyu Pang\affmark[\textdagger], Chao Du\affmark[\textdagger], Min Lin\affmark[\textdagger]}
\\
\centerline{\affaddr{\affmark[\textdagger]Sea AI Lab, Singapore}}\\
\centerline{\affaddr{\affmark[\S]Washington University in St. Louis, MO, USA}}\\
\centerline{\affaddr{\affmark[$\lozenge$]Allen Institute for AI, Seattle, WA, USA}}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
\input{0_abs}
\end{abstract}


\section{Introduction}


\input{1_intro}

\input{2_problem}


\section{Methodology}

\input{3_method}

\section{Experimental Results}
\input{4_evaluation}


\section{Experimental Analysis}\label{sec:analysis}
\input{5_analysis}


\section{Related work}

\input{6_relatedwork}


\section{Conclusion}

In this work, we have introduced \lorahub, a strategic framework for composing LoRA modules trained on diverse tasks in order to achieve adaptable performance on new tasks. Our approach enables the fluid combination of multiple LoRA modules using just a few examples from a novel task, without requiring additional model parameters or human expertise. The empirical results on the BBH benchmark demonstrate that \lorahub can effectively match the performance of in-context learning in few-shot scenarios, removing the need for in-context examples during inference.
Overall, our work shows the promise of strategic LoRA composability for rapidly adapting LLMs to diverse tasks. 
By fostering reuse and combination of LoRA modules, we can work towards more general and adaptable LLMs while minimizing training costs.

\section{Limitations \& future work}\label{sec:limitation}

\paragraph{Pre-Filtering of LoRA Module Candidates}
While our method is successful in identifying and weighting relevant aspects from seen tasks to enhance unseen task performance, relying entirely on the model to perform this search can lead to increased computational demands and potentially unstable results. Incorporating a pre-filtering step to select only pertinent LoRA modules could expedite and refine performance. Identifying an effective selection strategy warrants further study.

\paragraph{Method Applicability to Decoder-Only Models}
All experiments for this study were executed using the encoder-decoder architecture. We aspire to extrapolate this method to decoder-only models such as GPT~\citep{gpt3}, aiming to determine its applicability in such contexts.

\paragraph{Exploring Superior Optimization Methods}
The use of a genetic algorithm for optimization in this study raises the question of whether better optimization approaches exist that could provide superior gradient-free optimization with limited examples. Although the current method has shown adequate performance, there is still room for improvement.

% \section*{Reproducibility Statement}
% The authors have made great efforts to ensure the reproducibility of the empirical results reported in this paper. Firstly, the experiment settings, evaluation metrics, and datasets were described in detail in Section 4.1.
% Secondly, the codes and script for reproduce the result will be opensource after accepted.
% Second, the source code implementing the proposed method and experiments will be made publicly available at upon acceptance of the paper.
% Third, pre-trained LoRA modules from this work along with their configuration files and weights will be shared. These allow reproduction without retraining the LoRA modules, enabling quick testing and verification.


\bibliography{iclr2024_conference}
\bibliographystyle{iclr2024_conference}
\newpage
\appendix

\input{7_appendix}

\end{document}
