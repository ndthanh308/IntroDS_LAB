\section{Problem Statement}

\textbf{Large Language Models}\;\; We assume that a large language model $M_\theta$ is based on Transformer architecture~\citep{transformer_paper} and has been pre-trained on a large-scale text corpus. The model architecture can be either encoder-decoder~\citep{t5_paper} or decoder-only~\citep{gpt3}. Also, $M_\theta$ could also have been fine-tuned with a large set of instruction-following datasets such as Flan Colleciton~\citep{longpre2023flan} and PromptSource~\citep{bach2022promptsource}.

\textbf{Cross-Task Generalization}\;\; In real-world situations, users often desire an LLM to perform novel tasks that it has not encountered before â€” an ability widely known as \textit{cross-task generalization}.
Generally, cross-task generalization falls into two categories: zero-shot learning~\citep{mishra-etal-2022-cross,t0_paper,flan,chatgpt_paper, recross}, which necessitates no labeled examples of the new task, and few-shot learning~\citep{ye-etal-2021-crossfit,min-etal-2022-metaicl} which demands a handful of labeled examples.
Assume we have $N$ distinct \textit{upstream tasks} that the LLM has been trained on, denoted as $\mathbb{T}=\{\mathcal{T}_1, ..., \mathcal{T}_N\}$.
Our paper primarily focuses on the latter category, where for an unseen target task $\mathcal{T}' \notin \mathbb{T}$, users can only provide a limited set of labeled examples, $Q$.
Our aim is to modify the model $M_\theta$ to adapt it to task $\mathcal{T}'$ using only $Q$.
An intuitive method would be to fine-tune the weights of $M_\theta$ based on $Q$, yielding an updated model $M_\phi$ with enhanced performance on $\mathcal{T}'$. However, this approach is inefficient, time-consuming, and unstable when $Q$ is small.

\textbf{LoRA Tuning}\;\; LoRA~\citep{hu2022lora}, a parameter-efficient fine-tuning method, facilitates the adaptation of LLMs using lightweight modules, eliminating the need for fine-tuning the entire weights.
LoRA tuning involves keeping the original model weights frozen while introducing trainable low-rank decomposition matrices as adapter modules into each layer of the model.
Compared to the base LLM, this module possesses significantly fewer trainable parameters, paving the way for rapid adaptation using minimal examples.
As such, LoRA tuning presents a resource-efficient technique to quickly adapt LLMs for new tasks with restricted training data.
However, traditional LoRA methods primarily concentrate on training and testing within the same tasks~\citep{Gema2023ParameterEfficientFO}, rather than venturing into few-shot cross-task generalization.
