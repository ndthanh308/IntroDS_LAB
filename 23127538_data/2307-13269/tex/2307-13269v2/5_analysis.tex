In this section, we thoroughly examine the characteristics of our proposed method and uncover several insightful findings. If not specified, we use \flan{}-large for all analysis.

\begin{githubquote}
    Which LoRA modules are most effective for BBH tasks? 
\end{githubquote}

We hypothesized that the amalgamation of LoRA modules could incorporate skills and insights from a variety of specific tasks. To evaluate this, we examined the extent of influence a single LoRA module had amongst all tasks from the BBH benchmark. We measured the impact of each isolated task by calculating the average absolute weight. The top five modules, presented in Table~\ref{tab:useful}, were found to have substantial influence, as indicated by their maximum average weights, which suggested that they were notably more effective in cross-task transfer.
Remarkably, a common feature among these top five modules was their association with tasks requiring reading comprehension and reasoning skillsâ€”attributes indicative of higher cognitive complexity.
However, it is worth noting that none of the modules exhibited consistent improvement across all BBH tasks, as reflected in their average performance on all BBH tasks, which did not show a significant improvement compared to the original FLAN-T5-large, except for the Rank 2.
The results underscore the advantages of composing diverse modules in LoraHub.


\begin{table}[bt]
    \centering
    \small
    \caption{The top five beneficial LoRA modules for BBH tasks and their associated upstream tasks, the average weight values and the average performance on all BBH tasks.}
    \label{tab:useful}
    \begin{tabular}{llllll}
    \toprule
        Rank & Dataset: Task & Weight & Perf & Task Description \\ \midrule
        1 & WIQA: Last Process & 0.72  & 28.1 & \makecell[l]{Identifying the last step of a given process.} \\
        2 & RACE: Is this the Right Answer & 0.68 & 30.8 & \makecell[l]{Determining if given answer is correct.} \\ 
        3 & WIQA: First Process & 0.63 & 28.1 & \makecell[l]{Identifying the first step of a given process.} \\ 
        4 & AdversarialQA: BiDAF & 0.61 & 25.1 & \makecell[l]{Answering question created by an \\adversarial  model-in-the-loop.} \\ 
        5 & WebQuestions: What is the Answer & 0.58 & 27.0 & \makecell[l]{Answering question based on information \\extracted from the web.} \\ \bottomrule
    \end{tabular}
    \label{tab:top}
    \vspace{-5mm}
\end{table}


\begin{githubquote}
    How effective is the gradient-free optimization method?
\end{githubquote}


To assess the effectiveness of our gradient-free optimization method in correctly identifying the most suitable LoRA module for a given downstream task, we carried out an empirical study using the WikiTableQuestions~\citep{wtq} (WTQ) dataset.
We strategically included a LoRA module that was specifically trained on the WTQ dataset into our pool of LoRA candidate modules, which originally stemmed from tasks exclusive to the Flan Collection. Subsequently, we designated WTQ as the targeted downstream task and computed the weights consistent with the methods employed in \lorahub learning. As an end result, the WTQ-specific LoRA module was awarded the highest weight, exemplifying the algorithm's success in recognizing it as the most relevant.
Moreover, the combined LoRA module demonstrated marginal superiority over the WTQ LoRA module. 
This underscores the claim that the gradient-free optimization method has the ability to proficiently select the optimal upstream LoRA module for an unseen task.


\begin{githubquote}
    Can \lorahub work well on non-instruction-tuning models?
\end{githubquote}

In previous investigations, we primarily focused on models with zero-shot capabilities that were trained with instruction tuning. However, for models like T5 without zero-shot abilities, where training has a larger effect on parameters, it was unclear if \lorahub{} could still effectively manage and improve them. Our experiments show that although these models perform worse than \flan{}, \lorahub{} learning can still enable them to effectively generlize to unseen tasks. See Appendix~\ref{sec:t5_appendix} for more details. 


\begin{githubquote}
    Will the rank of LoRA modules impact the performance of \lorahub learning?
\end{githubquote}

The parameter rank plays a crucial role in the LoRA framework, directly influencing the number of trainable parameters utilized during LoRA tuning. This prompts an intriguing question: does the variation in rank values influence the outcomes observed within the LoraHub learning? Our analysis indicates that, for FLAN-T5, the choice of rank has minimal impact. However, for T5, it still exerts some influence. Empirical findings reveal that, in comparison to rank values of $4$ or $64$, a rank value of $16$ consistently demonstrates superior performance across different runs, both in terms of average and optimal values. Additional results are available in Appendix~\ref{sec:t5_appendix}.    


\begin{githubquote}
    Does more LoRA modules lead to better results?
\end{githubquote}

In our main experiments, we randomly selected $20$ LoRA modules for \lorahub{} learning. Therefore, we conducted experiments to investigate the effect of using different numbers of LoRA modules. The results demonstrate that as we increased the number of LoRA modules, the variance in performance increased. However, the maximum achievable performance also improved. More analysis on the variance and the detailed results can be found in Appendix~\ref{sec:number_of_lora}.


\begin{githubquote}
    Does composing LoRA modules extend beyond the single module's benefits?
\end{githubquote}

\begin{table}[t]
  \centering
  \caption{The average performance of various methods across all tasks in the benchmark BBH.}
  \begin{tabular}{ccc}
    \toprule
   LoRA Retrieval & \lorahub$_{\rm avg}$ &  \lorahub$_{\rm best}$ \\
    \midrule
    31.7 & 34.7 & 41.2 \\
    \bottomrule
  \end{tabular}
  \label{tab:baseline_compare}
\end{table}

We acknowledge the investigation of cross-task performance in prior work~\citep{Jang2023ExploringTB}, which delved into the capabilities of LoRA and proposed a novel method centered around LoRA module retrieval.
In order to ensure a fair comparison, we conducted an experiment where we designed a LoRA retrieval mechanism based on the loss derived from few-shot examples.
Specifically, we ranked all LoRA module candidates according to this loss and evaluated the best candidate on the test set of the unseen task.
As depicted in Table~\ref{tab:baseline_compare}, the performance of LoRA retrieval is notably impressive, positioning it as a strong baseline. However, in comparison to LoraHub, the performance of LoRA retrieval is relatively less favorable
