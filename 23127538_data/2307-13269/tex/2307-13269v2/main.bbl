\begin{thebibliography}{58}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ainsworth et~al.(2023)Ainsworth, Hayase, and Srinivasa]{ainsworth2023git}
Samuel Ainsworth, Jonathan Hayase, and Siddhartha Srinivasa.
\newblock Git re-basin: Merging models modulo permutation symmetries.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.

\bibitem[An et~al.(2022)An, Li, Lin, Liu, Chen, Fu, Chen, Zheng, and Lou]{input_tuning}
Shengnan An, Yifei Li, Zeqi Lin, Qian Liu, Bei Chen, Qiang Fu, Weizhu Chen, Nanning Zheng, and Jian{-}Guang Lou.
\newblock Input-tuning: Adapting unfamiliar inputs to frozen pretrained models.
\newblock \emph{ArXiv preprint}, 2022.

\bibitem[Aribandi et~al.(2022)Aribandi, Tay, Schuster, Rao, Zheng, Mehta, Zhuang, Tran, Bahri, Ni, Gupta, Hui, Ruder, and Metzler]{aribandi2022ext}
Vamsi Aribandi, Yi~Tay, Tal Schuster, Jinfeng Rao, Huaixiu~Steven Zheng, Sanket~Vaibhav Mehta, Honglei Zhuang, Vinh~Q. Tran, Dara Bahri, Jianmo Ni, Jai~Prakash Gupta, Kai Hui, Sebastian Ruder, and Donald Metzler.
\newblock Ext5: Towards extreme multi-task scaling for transfer learning.
\newblock In \emph{Proc. of ICLR}, 2022.

\bibitem[Bach et~al.(2022)Bach, Sanh, Yong, Webson, Raffel, Nayak, Sharma, Kim, Bari, Fevry, Alyafeai, Dey, Santilli, Sun, Ben-david, Xu, Chhablani, Wang, Fries, Al-shaibani, Sharma, Thakker, Almubarak, Tang, Radev, Jiang, and Rush]{bach2022promptsource}
Stephen Bach, Victor Sanh, Zheng~Xin Yong, Albert Webson, Colin Raffel, Nihal~V. Nayak, Abheesht Sharma, Taewoon Kim, M~Saiful Bari, Thibault Fevry, Zaid Alyafeai, Manan Dey, Andrea Santilli, Zhiqing Sun, Srulik Ben-david, Canwen Xu, Gunjan Chhablani, Han Wang, Jason Fries, Maged Al-shaibani, Shanya Sharma, Urmish Thakker, Khalid Almubarak, Xiangru Tang, Dragomir Radev, Mike Tian-jian Jiang, and Alexander Rush.
\newblock {P}rompt{S}ource: An integrated development environment and repository for natural language prompts.
\newblock In \emph{Proc. of ACL}, 2022.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert{-}Voss, Krueger, Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{gpt3}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert{-}Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel~M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria{-}Florina Balcan, and Hsuan{-}Tien Lin (eds.), \emph{Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual}, 2020.

\bibitem[Chevalier et~al.(2023)Chevalier, Wettig, Ajith, and Chen]{alexis2023adapting}
Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen.
\newblock Adapting language models to compress contexts.
\newblock \emph{CoRR}, abs/2305.14788, 2023.
\newblock \doi{10.48550/ARXIV.2305.14788}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2305.14788}.

\bibitem[Chronopoulou et~al.(2023)Chronopoulou, Peters, Fraser, and Dodge]{Chronopoulou2023AdapterSoupWA}
Alexandra Chronopoulou, Matthew Peters, Alexander Fraser, and Jesse Dodge.
\newblock {A}dapter{S}oup: Weight averaging to improve generalization of pretrained language models.
\newblock In \emph{Findings of the Association for Computational Linguistics: EACL 2023}, 2023.

\bibitem[Chung et~al.(2022)Chung, Hou, Longpre, Zoph, Tay, Fedus, Li, Wang, Dehghani, Brahma, Webson, Gu, Dai, Suzgun, Chen, Chowdhery, Valter, Narang, Mishra, Yu, Zhao, Huang, Dai, Yu, Petrov, hsin Chi, Dean, Devlin, Roberts, Zhou, Le, and Wei]{flan}
Hyung~Won Chung, Le~Hou, S.~Longpre, Barret Zoph, Yi~Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang~Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams~Wei Yu, Vincent Zhao, Yanping Huang, Andrew~M. Dai, Hongkun Yu, Slav Petrov, Ed~Huai hsin Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc~V. Le, and Jason Wei.
\newblock Scaling instruction-finetuned language models.
\newblock \emph{ArXiv preprint}, 2022.

\bibitem[crumb(2023)]{WinNT}
crumb.
\newblock Llama-2, mixutre of lora.
\newblock \url{https://crumbly.medium.com/llama-2-molora-f5f909434711}, 2023.

\bibitem[Du et~al.(2022)Du, Huang, Dai, Tong, Lepikhin, Xu, Krikun, Zhou, Yu, Firat, Zoph, Fedus, Bosma, Zhou, Wang, Wang, Webster, Pellat, Robinson, Meier{-}Hellstern, Duke, Dixon, Zhang, Le, Wu, Chen, and Cui]{Du2021GLaMES}
Nan Du, Yanping Huang, Andrew~M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams~Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten~P. Bosma, Zongwei Zhou, Tao Wang, Yu~Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathleen~S. Meier{-}Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc~V. Le, Yonghui Wu, Zhifeng Chen, and Claire Cui.
\newblock Glam: Efficient scaling of language models with mixture-of-experts.
\newblock In Kamalika Chaudhuri, Stefanie Jegelka, Le~Song, Csaba Szepesv{\'{a}}ri, Gang Niu, and Sivan Sabato (eds.), \emph{International Conference on Machine Learning, {ICML} 2022, 17-23 July 2022, Baltimore, Maryland, {USA}}, Proceedings of Machine Learning Research, 2022.

\bibitem[Ge et~al.(2023)Ge, Hu, Wang, Chen, and Wei]{ge2023incontext}
Tao Ge, Jing Hu, Xun Wang, Si{-}Qing Chen, and Furu Wei.
\newblock In-context autoencoder for context compression in a large language model.
\newblock \emph{CoRR}, abs/2307.06945, 2023.
\newblock \doi{10.48550/ARXIV.2307.06945}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2307.06945}.

\bibitem[Gema et~al.(2023)Gema, Daines, Minervini, and Alex]{Gema2023ParameterEfficientFO}
Aryo~Pradipta Gema, Luke Daines, Pasquale Minervini, and Beatrice Alex.
\newblock Parameter-efficient fine-tuning of llama for the clinical domain.
\newblock \emph{ArXiv preprint}, 2023.

\bibitem[Hansen \& Ostermeier(1996)Hansen and Ostermeier]{Hansen1996AdaptingAN}
Nikolaus Hansen and Andreas Ostermeier.
\newblock Adapting arbitrary normal mutation distributions in evolution strategies: the covariance matrix adaptation.
\newblock \emph{Proceedings of IEEE International Conference on Evolutionary Computation}, 1996.

\bibitem[He et~al.(2022)He, Zhou, Ma, Berg{-}Kirkpatrick, and Neubig]{junxian_unified_2022}
Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg{-}Kirkpatrick, and Graham Neubig.
\newblock Towards a unified view of parameter-efficient transfer learning.
\newblock In \emph{Proc. of ICLR}, 2022.

\bibitem[Hu et~al.(2022)Hu, Shen, Wallis, Allen{-}Zhu, Li, Wang, Wang, and Chen]{hu2022lora}
Edward~J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen{-}Zhu, Yuanzhi Li, Shean Wang, Lu~Wang, and Weizhu Chen.
\newblock Lora: Low-rank adaptation of large language models.
\newblock In \emph{Proc. of ICLR}, 2022.

\bibitem[Ilharco et~al.(2023)Ilharco, Ribeiro, Wortsman, Schmidt, Hajishirzi, and Farhadi]{ilharco2023editing}
Gabriel Ilharco, Marco~Tulio Ribeiro, Mitchell Wortsman, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi.
\newblock Editing models with task arithmetic.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.

\bibitem[Jacobs et~al.(1991)Jacobs, Jordan, Nowlan, and Hinton]{Jacobs1991AdaptiveMO}
Robert~A. Jacobs, Michael~I. Jordan, Steven~J. Nowlan, and Geoffrey~E. Hinton.
\newblock Adaptive mixtures of local experts.
\newblock \emph{Neural Computation}, 1991.

\bibitem[Jang et~al.(2023)Jang, Kim, Ye, Kim, Logeswaran, Lee, Lee, and Seo]{Jang2023ExploringTB}
Joel Jang, Seungone Kim, Seonghyeon Ye, Doyoung Kim, Lajanugen Logeswaran, Moontae Lee, Kyungjae Lee, and Minjoon Seo.
\newblock Exploring the benefits of training expert language models over instruction tuning.
\newblock In \emph{International Conference on Machine Learning}, 2023.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:256627673}.

\bibitem[Jiang et~al.(2023{\natexlab{a}})Jiang, Wu, Lin, Yang, and Qiu]{jiang-etal-2023-llmlingua}
Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu.
\newblock Llmlingua: Compressing prompts for accelerated inference of large language models.
\newblock In \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}. Association for Computational Linguistics, December 2023{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2310.05736}.

\bibitem[Jiang et~al.(2023{\natexlab{b}})Jiang, Wu, Luo, Li, Lin, Yang, and Qiu]{jiang-etal-2023-longllm}
Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin{-}Yew Lin, Yuqing Yang, and Lili Qiu.
\newblock Longllmlingua: Accelerating and enhancing llms in long context scenarios via prompt compression.
\newblock \emph{CoRR}, abs/2310.06839, 2023{\natexlab{b}}.
\newblock \doi{10.48550/ARXIV.2310.06839}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2310.06839}.

\bibitem[Jin et~al.(2023)Jin, Ren, Preotiuc-Pietro, and Cheng]{jin2023dataless}
Xisen Jin, Xiang Ren, Daniel Preotiuc-Pietro, and Pengxiang Cheng.
\newblock Dataless knowledge fusion by merging weights of language models.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.

\bibitem[Kingetsu et~al.(2021)Kingetsu, Kobayashi, and Suzuki]{Kingetsu2021NeuralNM}
Hiroaki Kingetsu, Kenichi Kobayashi, and Taiji Suzuki.
\newblock Neural network module decomposition and recomposition.
\newblock \emph{ArXiv preprint}, 2021.

\bibitem[Lester et~al.(2021)Lester, Al-Rfou, and Constant]{lester-etal-2021-power}
Brian Lester, Rami Al-Rfou, and Noah Constant.
\newblock The power of scale for parameter-efficient prompt tuning.
\newblock In \emph{Proc. of EMNLP}, 2021.

\bibitem[Li et~al.(2023)Li, Dong, Lin, and Guerin]{li-etal-2023-compressing}
Yucheng Li, Bo~Dong, Chenghua Lin, and Frank Guerin.
\newblock Compressing context to enhance inference efficiency of large language models.
\newblock In \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}. Association for Computational Linguistics, December 2023.
\newblock URL \url{https://arxiv.org/abs/2310.06201}.

\bibitem[Lin et~al.(2022)Lin, Tan, Miller, Tian, and Ren]{recross}
Bill~Yuchen Lin, Kangmin Tan, Chris Miller, Beiwen Tian, and Xiang Ren.
\newblock Unsupervised cross-task generalization via retrieval augmentation.
\newblock In \emph{NeurIPS}, 2022.

\bibitem[Liu et~al.(2022)Liu, Tam, Muqeeth, Mohta, Huang, Bansal, and Raffel]{Liu2022FewShotPF}
Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin Raffel.
\newblock Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning.
\newblock \emph{ArXiv}, abs/2205.05638, 2022.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:248693283}.

\bibitem[Liu et~al.(2020)Liu, Moreau, Preuss, Rozi{\`e}re, Rapin, Teytaud, and Teytaud]{NGOpt}
Jialin Liu, A.~Moreau, Mike Preuss, Baptiste Rozi{\`e}re, J{\'e}r{\'e}my Rapin, Fabien Teytaud, and Olivier Teytaud.
\newblock Versatile black-box optimization.
\newblock \emph{Proceedings of the 2020 Genetic and Evolutionary Computation Conference}, 2020.

\bibitem[Longpre et~al.(2023)Longpre, Hou, Vu, Webson, Chung, Tay, Zhou, Le, Zoph, Wei, and Roberts]{longpre2023flan}
Shayne Longpre, Le~Hou, Tu~Vu, Albert Webson, Hyung~Won Chung, Yi~Tay, Denny Zhou, Quoc~V. Le, Barret Zoph, Jason Wei, and Adam Roberts.
\newblock The flan collection: Designing data and methods for effective instruction tuning, 2023.

\bibitem[Lv et~al.(2023)Lv, Ding, Qin, Liu, and Sun]{Lv2023ParameterefficientWE}
Xingtai Lv, Ning Ding, Yujia Qin, Zhiyuan Liu, and Maosong Sun.
\newblock Parameter-efficient weight ensembling facilitates task-level knowledge transfer.
\newblock In \emph{Annual Meeting of the Association for Computational Linguistics}, 2023.

\bibitem[Mangrulkar et~al.(2022)Mangrulkar, Gugger, Debut, Belkada, and Paul]{peft}
Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, and Sayak Paul.
\newblock Peft: State-of-the-art parameter-efficient fine-tuning methods.
\newblock \url{https://github.com/huggingface/peft}, 2022.

\bibitem[Matena \& Raffel(2021)Matena and Raffel]{Matena2021MergingMW}
Michael Matena and Colin Raffel.
\newblock Merging models with fisher-weighted averaging.
\newblock \emph{ArXiv preprint}, 2021.

\bibitem[Min et~al.(2022)Min, Lewis, Zettlemoyer, and Hajishirzi]{min-etal-2022-metaicl}
Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi.
\newblock {M}eta{ICL}: Learning to learn in context.
\newblock In \emph{Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, 2022.

\bibitem[Mishra et~al.(2022)Mishra, Khashabi, Baral, and Hajishirzi]{mishra-etal-2022-cross}
Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi.
\newblock Cross-task generalization via natural language crowdsourcing instructions.
\newblock In \emph{Proc. of ACL}, 2022.

\bibitem[Muqeeth et~al.(2023)Muqeeth, Liu, and Raffel]{Muqeeth2023SoftMO}
Mohammed Muqeeth, Haokun Liu, and Colin Raffel.
\newblock Soft merging of experts with adaptive routing.
\newblock \emph{ArXiv preprint}, 2023.

\bibitem[OpenAI(2022)]{chatgpt_paper}
OpenAI.
\newblock {ChatGPT}.
\newblock 2022.
\newblock URL \url{https://openai.com/blog/chatgpt}.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray, Schulman, Hilton, Kelton, Miller, Simens, Askell, Welinder, Christiano, Leike, and Lowe]{InstructGPT}
Long Ouyang, Jeff Wu, Xu~Jiang, Diogo Almeida, Carroll~L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke~E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul~Francis Christiano, Jan Leike, and Ryan~J. Lowe.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{ArXiv preprint}, 2022.

\bibitem[Pasupat \& Liang(2015)Pasupat and Liang]{wtq}
Panupong Pasupat and Percy Liang.
\newblock Compositional semantic parsing on semi-structured tables.
\newblock In \emph{Proc. of ACL}, 2015.

\bibitem[Ponti et~al.(2023)Ponti, Sordoni, Bengio, and Reddy]{Ponti2023CombiningPM}
Edoardo~Maria Ponti, Alessandro Sordoni, Yoshua Bengio, and Siva Reddy.
\newblock Combining parameter-efficient modules for task-level generalisation.
\newblock In \emph{Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics}, 2023.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu]{t5_paper}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J. Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock \emph{J. Mach. Learn. Res.}, 2020.

\bibitem[Rapin \& Teytaud(2018)Rapin and Teytaud]{nevergrad}
J.~Rapin and O.~Teytaud.
\newblock {Nevergrad - A gradient-free optimization platform}.
\newblock \url{https://GitHub.com/FacebookResearch/Nevergrad}, 2018.

\bibitem[Sanh et~al.(2022)Sanh, Webson, Raffel, Bach, Sutawika, Alyafeai, Chaffin, Stiegler, Raja, Dey, Bari, Xu, Thakker, Sharma, Szczechla, Kim, Chhablani, Nayak, Datta, Chang, Jiang, Wang, Manica, Shen, Yong, Pandey, Bawden, Wang, Neeraj, Rozen, Sharma, Santilli, F{\'{e}}vry, Fries, Teehan, Scao, Biderman, Gao, Wolf, and Rush]{t0_paper}
Victor Sanh, Albert Webson, Colin Raffel, Stephen~H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M~Saiful Bari, Canwen Xu, Urmish Thakker, Shanya~Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal~V. Nayak, Debajyoti Datta, Jonathan Chang, Mike~Tian{-}Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng~Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault F{\'{e}}vry, Jason~Alan Fries, Ryan Teehan, Teven~Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander~M. Rush.
\newblock Multitask prompted training enables zero-shot task generalization.
\newblock In \emph{Proc. of ICLR}, 2022.

\bibitem[Shazeer et~al.(2017)Shazeer, Mirhoseini, Maziarz, Davis, Le, Hinton, and Dean]{Shazeer2017OutrageouslyLN}
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc~V. Le, Geoffrey~E. Hinton, and Jeff Dean.
\newblock Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.
\newblock In \emph{Proc. of ICLR}, 2017.

\bibitem[Shen et~al.(2023)Shen, Hou, Zhou, Du, Longpre, Wei, Chung, Zoph, Fedus, Chen, Vu, Wu, Chen, Webson, Li, Zhao, Yu, Keutzer, Darrell, and Zhou]{shen2023mixtureofexperts}
Sheng Shen, Le~Hou, Yanqi Zhou, Nan Du, Shayne Longpre, Jason Wei, Hyung~Won Chung, Barret Zoph, William Fedus, Xinyun Chen, Tu~Vu, Yuexin Wu, Wuyang Chen, Albert Webson, Yunxuan Li, Vincent Zhao, Hongkun Yu, Kurt Keutzer, Trevor Darrell, and Denny Zhou.
\newblock Mixture-of-experts meets instruction tuning:a winning combination for large language models, 2023.

\bibitem[Stoica et~al.(2023)Stoica, Bolya, Bjorner, Hearn, and Hoffman]{stoica2023zipit}
George Stoica, Daniel Bolya, Jakob Bjorner, Taylor Hearn, and Judy Hoffman.
\newblock Zipit! merging models from different tasks without training.
\newblock \emph{arXiv}, 2023.

\bibitem[Sun et~al.(2022)Sun, Shao, Qian, Huang, and Qiu]{blackbox_tuning}
Tianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing Huang, and Xipeng Qiu.
\newblock Black-box tuning for language-model-as-a-service.
\newblock In Kamalika Chaudhuri, Stefanie Jegelka, Le~Song, Csaba Szepesv{\'{a}}ri, Gang Niu, and Sivan Sabato (eds.), \emph{International Conference on Machine Learning, {ICML} 2022, 17-23 July 2022, Baltimore, Maryland, {USA}}, Proceedings of Machine Learning Research, 2022.

\bibitem[Sun et~al.(2023)Sun, He, Zhu, Qiu, and Huang]{sun2023multitask}
Tianxiang Sun, Zhengfu He, Qin Zhu, Xipeng Qiu, and Xuanjing Huang.
\newblock Multitask pre-training of modular prompt for {C}hinese few-shot learning.
\newblock In \emph{Proc. of ACL}, 2023.

\bibitem[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, Rodriguez, Joulin, Grave, and Lample]{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{ArXiv preprint}, 2023.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{transformer_paper}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna~M. Wallach, Rob Fergus, S.~V.~N. Vishwanathan, and Roman Garnett (eds.), \emph{Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, {USA}}, 2017.

\bibitem[Wang et~al.(2022)Wang, Agarwal, Mukherjee, Liu, Gao, Awadallah, and Gao]{wang-etal-2022-adamix}
Yaqing Wang, Sahaj Agarwal, Subhabrata Mukherjee, Xiaodong Liu, Jing Gao, Ahmed~Hassan Awadallah, and Jianfeng Gao.
\newblock {A}da{M}ix: Mixture-of-adaptations for parameter-efficient model tuning.
\newblock In \emph{Proc. of EMNLP}, 2022.

\bibitem[Wei et~al.(2022)Wei, Bosma, Zhao, Guu, Yu, Lester, Du, Dai, and Le]{Wei2021FinetunedLM}
Jason Wei, Maarten Bosma, Vincent~Y. Zhao, Kelvin Guu, Adams~Wei Yu, Brian Lester, Nan Du, Andrew~M. Dai, and Quoc~V. Le.
\newblock Finetuned language models are zero-shot learners.
\newblock In \emph{Proc. of ICLR}, 2022.

\bibitem[Wu et~al.(2023{\natexlab{a}})Wu, Wang, Ge, Lu, Zhou, Shan, and Luo]{DBLP:conf/icml/WuWGLZSL23}
Chengyue Wu, Teng Wang, Yixiao Ge, Zeyu Lu, Ruisong Zhou, Ying Shan, and Ping Luo.
\newblock {\(\pi\)}-tuning: Transferring multimodal foundation models with optimal multi-task interpolation.
\newblock In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), \emph{International Conference on Machine Learning, {ICML} 2023, 23-29 July 2023, Honolulu, Hawaii, {USA}}, volume 202 of \emph{Proceedings of Machine Learning Research}, pp.\  37713--37727. {PMLR}, 2023{\natexlab{a}}.
\newblock URL \url{https://proceedings.mlr.press/v202/wu23t.html}.

\bibitem[Wu et~al.(2023{\natexlab{b}})Wu, Irsoy, Lu, Dabravolski, Dredze, Gehrmann, Kambadur, Rosenberg, and Mann]{DBLP:journals/corr/abs-2303-17564}
Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David~S. Rosenberg, and Gideon Mann.
\newblock Bloomberggpt: {A} large language model for finance.
\newblock \emph{CoRR}, abs/2303.17564, 2023{\natexlab{b}}.
\newblock \doi{10.48550/arXiv.2303.17564}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2303.17564}.

\bibitem[Yadav et~al.(2023)Yadav, Tam, Choshen, Raffel, and Bansal]{yadav2023tiesmerging}
Prateek Yadav, Derek Tam, Leshem Choshen, Colin Raffel, and Mohit Bansal.
\newblock {TIES}-merging: Resolving interference when merging models.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}, 2023.
\newblock URL \url{https://openreview.net/forum?id=xtaX3WyCj1}.

\bibitem[Ye et~al.(2021)Ye, Lin, and Ren]{ye-etal-2021-crossfit}
Qinyuan Ye, Bill~Yuchen Lin, and Xiang Ren.
\newblock {C}ross{F}it: A few-shot learning challenge for cross-task generalization in {NLP}.
\newblock In \emph{Proc. of EMNLP}, 2021.

\bibitem[Zhang et~al.(2019)Zhang, Ren, and Urtasun]{nas_hyper}
Chris Zhang, Mengye Ren, and Raquel Urtasun.
\newblock Graph hypernetworks for neural architecture search.
\newblock In \emph{Proc. of ICLR}, 2019.

\bibitem[Zhang et~al.(2022)Zhang, Tang, Dai, Zhou, Wu, and Shi]{zhang2022skillnetnlu}
Fan Zhang, Duyu Tang, Yong Dai, Cong Zhou, Shuangzhi Wu, and Shuming Shi.
\newblock Skillnet-nlu: A sparsely activated model for general-purpose natural language understanding, 2022.

\bibitem[Zhang et~al.(2023)Zhang, Chen, Liu, and He]{Zhang2023ComposingPM}
Jinghan Zhang, Shiqi Chen, Junteng Liu, and Junxian He.
\newblock Composing parameter-efficient modules with arithmetic operations.
\newblock \emph{ArXiv preprint}, 2023.

\bibitem[Zhou et~al.(2023)Zhou, Jiang, Cotterell, and Sachan]{wangchunshu2023efficent}
Wangchunshu Zhou, Yuchen~Eleanor Jiang, Ryan Cotterell, and Mrinmaya Sachan.
\newblock Efficient prompting via dynamic in-context learning.
\newblock \emph{CoRR}, abs/2305.11170, 2023.
\newblock \doi{10.48550/ARXIV.2305.11170}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2305.11170}.

\end{thebibliography}
