\begin{thebibliography}{10}

\bibitem{olive1977rule}
J.~Olive, ``Rule synthesis of speech from dyadic units,'' in {\em IEEE
  International Conference on Acoustics, Speech and Signal Processing
  (ICASSP)}, vol.~2, pp.~568--570, IEEE, 1977.

\bibitem{hunt1996unit}
A.~J. Hunt and A.~W. Black, ``Unit selection in a concatenative speech
  synthesis system using a large speech database,'' in {\em IEEE International
  Conference on Acoustics, Speech and Signal Processing (ICASSP)}, vol.~1,
  pp.~373--376, IEEE, 1996.

\bibitem{campbell1997prosody}
N.~Campbell and A.~W. Black, ``Prosody and the selection of source units for
  concatenative synthesis,'' in {\em Progress in Speech Synthesis},
  pp.~279--292, Springer, 1997.

\bibitem{wang2000corpus}
R.-H. Wang, Z.~Ma, W.~Li, and D.~Zhu, ``A corpus-based {Chinese} speech
  synthesis with contextual dependent unit selection,'' in {\em International
  Conference on Spoken Language Processing (ICSLP)}, pp.~391--394, ISCA, 2000.

\bibitem{black1997automatically}
A.~W. Black and P.~A. Taylor, ``Automatically clustering similar units for unit
  selection in speech synthesis,'' in {\em European Conference on Speech
  Communication and Technology (EUROSPEECH)}, pp.~601--604, ISCA, 1997.

\bibitem{yoshimura1999simultaneous}
T.~Yoshimura, K.~Tokuda, T.~Masuko, T.~Kobayashi, and T.~Kitamura,
  ``Simultaneous modeling of spectrum, pitch and duration in {HMM}-based speech
  synthesis,'' in {\em European Conference on Speech Communication and
  Technology (EUROSPEECH)}, pp.~2347--2350, ISCA, 1999.

\bibitem{tokuda2000speech}
K.~Tokuda, T.~Yoshimura, T.~Masuko, T.~Kobayashi, and T.~Kitamura, ``Speech
  parameter generation algorithms for {HMM}-based speech synthesis,'' in {\em
  IEEE International Conference on Acoustics, Speech and Signal Processing
  (ICASSP)}, vol.~3, pp.~1315--1318, IEEE, 2000.

\bibitem{zen2009statistical}
H.~Zen, K.~Tokuda, and A.~W. Black, ``Statistical parametric speech
  synthesis,'' {\em Speech Communication}, vol.~51, pp.~1039--1064, 2009.

\bibitem{tokuda2013speech}
K.~Tokuda, Y.~Nankaku, T.~Toda, H.~Zen, J.~Yamagishi, and K.~Oura, ``Speech
  synthesis based on hidden markov models,'' {\em Proceedings of the IEEE},
  vol.~101, pp.~1234--1252, 2013.

\bibitem{ze2013statistical}
H.~Ze, A.~Senior, and M.~Schuster, ``Statistical parametric speech synthesis
  using deep neural networks,'' in {\em IEEE International Conference on
  Acoustics, Speech and Signal Processing (ICASSP)}, pp.~7962--7966, IEEE,
  2013.

\bibitem{sotelo2017char2wav}
J.~Sotelo, S.~Mehri, K.~Kumar, J.~F. Santos, K.~Kastner, A.~Courville, and
  Y.~Bengio, ``Char2wav: End-to-end speech synthesis,'' in {\em International
  Conference on Learning Representations (ICLR)}, 2017.

\bibitem{tacotron}
Y.~Wang, R.~Skerry-Ryan, D.~Stanton, Y.~Wu, R.~J. Weiss, N.~Jaitly, Z.~Yang,
  Y.~Xiao, Z.~Chen, S.~Bengio, {\em et~al.}, ``Tacotron: Towards end-to-end
  speech synthesis,'' in {\em Annual Conference of the International Speech
  Communication Association (INTERSPEECH)}, pp.~4006--4010, ISCA, 2017.

\bibitem{tacotron2}
J.~Shen, R.~Pang, R.~J. Weiss, M.~Schuster, N.~Jaitly, Z.~Yang, Z.~Chen,
  Y.~Zhang, Y.~Wang, R.~Skerrv-Ryan, {\em et~al.}, ``Natural {TTS} synthesis by
  conditioning wavenet on {MEL} spectrogram predictions,'' in {\em IEEE
  International Conference on Acoustics, Speech and Signal Processing
  (ICASSP)}, pp.~4779--4783, IEEE, 2018.

\bibitem{deepvoice3}
W.~Ping, K.~Peng, A.~Gibiansky, S.~O. Arik, A.~Kannan, S.~Narang, J.~Raiman,
  and J.~Miller, ``Deep voice 3: Scaling text-to-speech with convolutional
  sequence learning,'' in {\em International Conference on Learning
  Representations (ICLR)}, 2018.

\bibitem{li2019neural}
N.~Li, S.~Liu, Y.~Liu, S.~Zhao, and M.~Liu, ``Neural speech synthesis with
  transformer network,'' in {\em AAAI Conference on Artificial Intelligence},
  pp.~6706--6713, AAAI Press, 2019.

\bibitem{Yu2020DurIANDI}
C.~Yu, H.~Lu, N.~Hu, M.~Yu, C.~Weng, K.~Xu, P.~Liu, D.~Tuo, S.~Kang, G.~Lei,
  D.~Su, and D.~Yu, ``Durian: Duration informed attention network for speech
  synthesis,'' in {\em Annual Conference of the International Speech
  Communication Association (INTERSPEECH)}, pp.~2027--2031, ISCA, 2020.

\bibitem{fastspeech2}
Y.~Ren, C.~Hu, X.~Tan, T.~Qin, S.~Zhao, Z.~Zhao, and T.-Y. Liu, ``Fastspeech 2:
  Fast and high-quality end-to-end text to speech,'' in {\em International
  Conference on Learning Representations (ICLR)}, 2021.

\bibitem{reference}
R.~Skerry-Ryan, E.~Battenberg, Y.~Xiao, Y.~Wang, D.~Stanton, J.~Shor, R.~Weiss,
  R.~Clark, and R.~A. Saurous, ``Towards end-to-end prosody transfer for
  expressive speech synthesis with tacotron,'' in {\em International Conference
  on Machine Learning (ICML)}, vol.~80, pp.~4693--4702, PMLR, 2018.

\bibitem{gst}
Y.~Wang, D.~Stanton, Y.~Zhang, R.-S. Ryan, E.~Battenberg, J.~Shor, Y.~Xiao,
  Y.~Jia, F.~Ren, and R.~A. Saurous, ``Style tokens: Unsupervised style
  modeling, control and transfer in end-to-end speech synthesis,'' in {\em
  International Conference on Machine Learning (ICML)}, vol.~80,
  pp.~5180--5189, PMLR, 2018.

\bibitem{VAEGST}
Y.-J. Zhang, S.~Pan, L.~He, and Z.-H. Ling, ``Learning latent representations
  for style control and transfer in end-to-end speech synthesis,'' in {\em IEEE
  International Conference on Acoustics, Speech and Signal Processing
  (ICASSP)}, pp.~6945--6949, IEEE, 2019.

\bibitem{an2019learning}
X.~An, Y.~Wang, S.~Yang, Z.~Ma, and L.~Xie, ``Learning hierarchical
  representations for expressive speaking style in end-to-end speech
  synthesis,'' in {\em IEEE Automatic Speech Recognition and Understanding
  Workshop (ASRU)}, pp.~184--191, IEEE, 2019.

\bibitem{wu2019end}
P.~Wu, Z.~Ling, L.~Liu, Y.~Jiang, H.~Wu, and L.~Dai, ``End-to-end emotional
  speech synthesis using style tokens and semi-supervised training,'' in {\em
  Asia-Pacific Signal and Information Processing Association Annual Summit and
  Conference (APSIPA ASC)}, pp.~623--627, IEEE, 2019.

\bibitem{hsu2018hierarchical}
W.-N. Hsu, Y.~Zhang, R.~J. Weiss, H.~Zen, Y.~Wu, Y.~Wang, Y.~Cao, Y.~Jia,
  Z.~Chen, J.~Shen, {\em et~al.}, ``Hierarchical generative modeling for
  controllable speech synthesis,'' in {\em International Conference on Learning
  Representations (ICLR)}, 2018.

\bibitem{multiscale}
X.~Li, C.~Song, J.~Li, Z.~Wu, J.~Jia, and H.~Meng, ``Towards multi-scale style
  control for expressive speech synthesis,'' in {\em Annual Conference of the
  International Speech Communication Association (INTERSPEECH)},
  pp.~4673--4677, {ISCA}, 2021.

\bibitem{yi2022prosodyspeech}
Y.~Yi, L.~He, S.~Pan, X.~Wang, and Y.~Xiao, ``Prosodyspeech: Towards advanced
  prosody model for neural text-to-speech,'' in {\em IEEE International
  Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pp.~7582--7586, IEEE, 2022.

\bibitem{lee2019robust}
Y.~Lee and T.~Kim, ``Robust and fine-grained prosody control of end-to-end
  speech synthesis,'' in {\em IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}, pp.~5911--5915, IEEE, 2019.

\bibitem{klimkov2019fine}
V.~Klimkov, S.~Ronanki, J.~Rohnke, and T.~Drugman, ``Fine-grained robust
  prosody transfer for single-speaker neural text-to-speech,'' in {\em Annual
  Conference of the International Speech Communication Association
  (INTERSPEECH)}, pp.~4440--4444, ISCA, 2019.

\bibitem{TPGST}
D.~Stanton, Y.~Wang, and R.~Skerry-Ryan, ``Predicting expressive speaking style
  from text in end-to-end speech synthesis,'' in {\em IEEE Spoken Language
  Technology Workshop (SLT)}, pp.~595--602, IEEE, 2018.

\bibitem{bert}
J.~Devlin, M.-W. Chang, K.~Lee, and K.~Toutanova, ``{BERT:} pre-training of
  deep bidirectional transformers for language understanding,'' in {\em Annual
  Conference of the North American Chapter of the Association for Computational
  Linguistics: Human Language Technologies (NAACL HLT)}, pp.~4171--4186,
  Association for Computational Linguistics, 2019.

\bibitem{berttacotron}
T.~Hayashi, S.~Watanabe, T.~Toda, K.~Takeda, S.~Toshniwal, and K.~Livescu,
  ``Pre-trained text embeddings for enhanced text-to-speech synthesis,'' in
  {\em Annual Conference of the International Speech Communication Association
  (INTERSPEECH)}, pp.~4430--4434, ISCA, 2019.

\bibitem{bertemb}
Y.~Xiao, L.~He, H.~Ming, and F.~K. Soong, ``Improving prosody with linguistic
  and bert derived features in multi-speaker based {Mandarin} chinese neural
  {TTS},'' in {\em IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP)}, pp.~6704--6708, IEEE, 2020.

\bibitem{fang2019towards}
W.~Fang, Y.-A. Chung, and J.~Glass, ``Towards transfer learning for end-to-end
  speech synthesis from deep pre-trained language models,'' {\em arXiv preprint
  arXiv:1906.07307}, 2019.

\bibitem{wsv}
Y.-J. Zhang and Z.-H. Ling, ``Extracting and predicting word-level style
  variations for speech synthesis,'' {\em IEEE/ACM Transactions on Audio,
  Speech, and Language Processing}, vol.~29, pp.~1582--1593, 2021.

\bibitem{ren2022prosospeech}
Y.~Ren, M.~Lei, Z.~Huang, S.~Zhang, Q.~Chen, Z.~Yan, and Z.~Zhao,
  ``Prosospeech: Enhancing prosody with quantized vector pre-training in
  text-to-speech,'' in {\em IEEE International Conference on Acoustics, Speech
  and Signal Processing (ICASSP)}, pp.~7577--7581, IEEE, 2022.

\bibitem{liu22m_interspeech}
Z.~Liu, N.~Wu, Y.~Zhang, and Z.~Ling, ``Integrating discrete word-level style
  variations into non-autoregressive acoustic models for speech synthesis,'' in
  {\em Annual Conference of the International Speech Communication Association
  (INTERSPEECH)}, pp.~5508--5512, ISCA, 2022.

\bibitem{tan2020fine}
D.~Tan and T.~Lee, ``Fine-grained style modeling, transfer and prediction in
  text-to-speech synthesis via phone-level content-style disentanglement,'' in
  {\em Annual Conference of the International Speech Communication Association
  (INTERSPEECH)}, pp.~4683--4687, ISCA, 2021.

\bibitem{du2021rich}
C.~Du and K.~Yu, ``Rich prosody diversity modelling with phone-level mixture
  density network,'' in {\em Annual Conference of the International Speech
  Communication Association (INTERSPEECH)}, pp.~3136--3140, ISCA, 2021.

\bibitem{survey}
X.~Tan, T.~Qin, F.~Soong, and T.-Y. Liu, ``A survey on neural speech
  synthesis,'' {\em arXiv preprint arXiv:2106.15561}, 2021.

\bibitem{longformevaluationg}
R.~Clark, H.~Silen, T.~Kenter, and R.~Leith, ``Evaluating long-form
  text-to-speech: Comparing the ratings of sentences and paragraphs,'' in {\em
  ISCA Speech Synthesis Workshop (SSW10)}, pp.~99--104, ISCA, 2019.

\bibitem{wu2022self}
Y.~Wu, X.~Wang, S.~Zhang, L.~He, R.~Song, and J.-Y. Nie, ``Self-supervised
  context-aware style representation for expressive speech synthesis,'' in {\em
  Annual Conference of the International Speech Communication Association
  (INTERSPEECH)}, pp.~5503--5507, ISCA, 2022.

\bibitem{nakata11audiobook}
W.~Nakata, T.~Koriyama, S.~Takamichi, N.~Tanji, Y.~Ijima, R.~Masumura, and
  H.~Saruwatari, ``Audiobook speech synthesis conditioned by cross-sentence
  context-aware word embeddings,'' in {\em ISCA Speech Synthesis Workshop
  (SSW11)}, pp.~211--215, ISCA, 2021.

\bibitem{crossutterance}
G.~Xu, W.~Song, Z.~Zhang, C.~Zhang, X.~He, and B.~Zhou, ``Improving prosody
  modelling with cross-utterance bert embeddings for end-to-end speech
  synthesis,'' in {\em IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP)}, pp.~6079--6083, IEEE, 2021.

\bibitem{li2022cross}
Y.~Li, C.~Yu, G.~Sun, H.~Jiang, F.~Sun, W.~Zu, Y.~Wen, Y.~Yang, and J.~Wang,
  ``Cross-utterance conditioned vae for non-autoregressive text-to-speech,'' in
  {\em Annual Meeting of the Association for Computational Linguistics (ACL)},
  pp.~391--400, Association for Computational Linguistics, 2022.

\bibitem{xue2022paratts}
L.~Xue, F.~K. Soong, S.~Zhang, and L.~Xie, ``Paratts: Learning linguistic and
  prosodic cross-sentence information in paragraph-based {TTS},'' {\em IEEE/ACM
  Transactions on Audio, Speech, and Language Processing}, vol.~30,
  pp.~2854--2864, 2022.

\bibitem{selkirk1986derived}
E.~Selkirk, ``On derived domains in sentence phonology,'' {\em Phonology},
  vol.~3, pp.~371--405, 1986.

\bibitem{liberman1977stress}
M.~Liberman and A.~Prince, ``On stress and linguistic rhythm,'' {\em Linguistic
  Inquiry}, vol.~8, pp.~249--336, 1977.

\bibitem{tseng2005fluent}
C.-y. Tseng, S.-h. Pin, Y.~Lee, H.-m. Wang, and Y.-c. Chen, ``Fluent speech
  prosody: Framework and modeling,'' {\em Speech Communication}, vol.~46,
  pp.~284--309, 2005.

\bibitem{lei2021fine}
Y.~Lei, S.~Yang, and L.~Xie, ``Fine-grained emotion strength transfer, control
  and prediction for emotional speech synthesis,'' in {\em IEEE Spoken Language
  Technology Workshop (SLT)}, pp.~423--430, IEEE, 2021.

\bibitem{msemotts}
Y.~Lei, S.~Yang, X.~Wang, and L.~Xie, ``{MsEmoTTS}: Multi-scale emotion
  transfer, prediction, and control for emotional speech synthesis,'' {\em
  IEEE/ACM Transactions on Audio, Speech, and Language Processing}, vol.~30,
  pp.~853--864, 2022.

\bibitem{proposed}
S.~Lei, Y.~Zhou, L.~Chen, Z.~Wu, S.~Kang, and H.~Meng, ``Towards expressive
  speaking style modelling with hierarchical context information for {Mandarin}
  speech synthesis,'' in {\em IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}, pp.~7922--7926, IEEE, 2022.

\bibitem{lei22c_interspeech}
S.~Lei, Y.~Zhou, L.~Chen, J.~Hu, Z.~Wu, S.~Kang, and H.~Meng, ``Towards
  multi-scale speaking style modelling with hierarchical context information
  for {Mandarin} speech synthesis,'' in {\em Annual Conference of the
  International Speech Communication Association (INTERSPEECH)},
  pp.~5523--5527, ISCA, 2022.

\bibitem{VAE}
D.~P. Kingma and M.~Welling, ``Auto-encoding variational bayes,'' in {\em
  International Conference on Learning Representations (ICLR)}, 2014.

\bibitem{oplustil2020using}
P.~Oplustil-Gallegos and S.~King, ``Using previous acoustic context to improve
  text-to-speech synthesis,'' {\em arXiv preprint arXiv:2012.03763}, 2020.

\bibitem{gallegos2021comparing}
P.~O. Gallegos, J.~O'Mahony, and S.~King, ``Comparing acoustic and textual
  representations of previous linguistic context for improving
  text-to-speech,'' in {\em ISCA Speech Synthesis Workshop (SSW11)},
  pp.~205--210, ISCA, 2021.

\bibitem{gru}
J.~Chung, C.~Gulcehre, K.~Cho, and Y.~Bengio, ``Empirical evaluation of gated
  recurrent neural networks on sequence modeling,'' in {\em Annual Conference
  on Neural Information Processing Systems (NeurIPS) Workshop on Deep Learning
  and Representation Learning}, 2014.

\bibitem{han}
Z.~Yang, D.~Yang, C.~Dyer, X.~He, A.~Smola, and E.~Hovy, ``Hierarchical
  attention networks for document classification,'' in {\em Conference of the
  North American Chapter of the Association for Computational Linguistics:
  Human Language Technologies (NAACL HLT)}, pp.~1480--1489, Association for
  Computational Linguistics, 2016.

\bibitem{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  {\L}.~Kaiser, and I.~Polosukhin, ``Attention is all you need,'' in {\em
  Annual Conference on Neural Information Processing Systems (NeurIPS)},
  pp.~5998--6008, 2017.

\bibitem{fastpitch}
A.~{\L}a{\'n}cucki, ``Fastpitch: Parallel text-to-speech with pitch
  prediction,'' in {\em IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP)}, pp.~6588--6592, IEEE, 2021.

\bibitem{kong2020hifi}
J.~Kong, J.~Kim, and J.~Bae, ``Hifi-gan: Generative adversarial networks for
  efficient and high fidelity speech synthesis,'' {\em Annual Conference on
  Neural Information Processing Systems (NeurIPS)}, pp.~17022--17033, 2020.

\bibitem{kingma2014adam}
D.~P. Kingma and J.~Ba, ``Adam: A method for stochastic optimization,'' in {\em
  International Conference on Learning Representations (ICLR)}, 2015.

\bibitem{xlnet}
Z.~Yang, Z.~Dai, Y.~Yang, J.~Carbonell, R.~R. Salakhutdinov, and Q.~V. Le,
  ``Xlnet: Generalized autoregressive pretraining for language understanding,''
  {\em Annual Conference on Neural Information Processing Systems (NeurIPS)},
  pp.~5754--5764, 2019.

\end{thebibliography}
