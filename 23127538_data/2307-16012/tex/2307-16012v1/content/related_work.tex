\section{Related Work} \label{sec:relatedwork}
\subsection{Reference Audio based Style Modeling}
The reference audio based style modeling aims to generate speech with the style that is transferred from a given audio.
In \cite{reference}, an unsupervised reference encoder is utilized to extract sentence-level global style embedding from given audio.
Multiple works have been conducted to study more effective global style representations.
Specifically, the global style could be represented by utilizing a fixed number of learnable global style tokens (GST) \cite{gst}, or by incorporating the variational autoencoder \cite{VAE} to achieve better style disentanglement \cite{VAEGST}, or by using several GST layers with residual connections to learn hierarchical embeddings implicitly \cite{an2019learning}.
In addition, \cite{wu2019end} further introduces an explicit relationship between style tokens and emotion categories in order to enhance the interpretability of the learned style tokens.

Instead of modeling the global style of speech in a sentence level, some researches focus on local prosodic characteristics \cite{lee2019robust, yi2022prosodyspeech, klimkov2019fine, tan2020fine}.
\cite{lee2019robust, yi2022prosodyspeech} introduce the idea of utilizing an additional reference attention mechanism to align the extracted style embedding sequence with the phoneme sequence.
Other studies, such as \cite{klimkov2019fine, tan2020fine}, achieve the same goal based on the forced-alignment technology.
Moreover, a recent study considering the multi-scale nature of speech style \cite{multiscale} also draws our attention.
It proposes a multi-scale reference encoder to extract both the global-scale sentence-level and the local-scale phoneme-level style features from the given reference speech. 


In this paper, the proposed method also learns the style representations at different levels. 
In addition to the sentence-level and subword-level, we also consider the global-level that presents the overall style of a wider range of speech containing adjacent sentences.
Furthermore, MSStyleTTS can directly predict the multi-scale style representations from the context without reference audio.
Besides, the residual style embeddings is used to represent style variations at each level rather than using the extracted style embeddings directly, which facilitates the subsequent prediction task.


\subsection{Text Predicted Style Modeling}
It is important to note that during inference stage, auxiliary inputs, such as manually-determined reference audio or token weights, are required for all of the aforementioned researches \cite{reference, gst, VAEGST,wu2019end, an2019learning,lee2019robust, yi2022prosodyspeech, klimkov2019fine, tan2020fine,multiscale}.
To prevent these manual interventions at inference stage, the text-predicted global style token (TP-GST) model \cite{TPGST} extends the GST \cite{gst} by modeling style embedding or style token weights from the input phoneme sequences.
Thus, it is possible to synthesize stylistic speech conditioned on the predicted style embedding during the inference stage without the use of manually-determined audio.
Considering that the style and semantic information of sentences are closely related to each other, the PLMs, such as BERT \cite{bert}, have been used to provide richer semantic information for expressive speech synthesis \cite{berttacotron, bertemb, fang2019towards}. 
In these works, the semantic representations derived from pre-trained BERT are added to the acoustic model as additional inputs according to attention mechanism or forced-alignment technology.
Several previous works \cite{wsv, liu22m_interspeech,ren2022prosospeech, tan2020fine, du2021rich} attempt to model fine-grained style from text in order to control local prosodic variations in speech.
The word-level style variation (WSV) \cite{wsv} designs an unsupervisedly-learned fine-grained representations to describe local style properties.
Multiple studies combine word-level style modeling with the non-autoregressive acoustic model to improve the efficiency of speech synthesis \cite{liu22m_interspeech, ren2022prosospeech}, and other studies further model the finer-grained style, such as at phoneme level \cite{tan2020fine, du2021rich}.

Our work is distinct from the above in two aspects: 
(i) previous models only focus on mono-scale style information while our work takes into consideration style information at difference levels beyond the sentence level;
(ii) other models utilize the semantic information of only the current sentence to improve pronunciation and expressiveness of the synthesized speech,
while we make use of the context information containing multiple adjacent sentences.
To our best knowledge, there is currently no research conducting multi-scale style modeling from the context for expressive speech synthesis.


\subsection{Context-aware Style Modeling}
Recently, some efforts have been made to consider a wider range of contextual information to improve the performance of expressive speech synthesis.
\cite{nakata11audiobook} obtains context-aware text representations of the current sentence by feeding both the neighboring and current sentences to BERT.
Moreover, some recent researches \cite{crossutterance, li2022cross} have further utilized cross-utterance information obtained from neighboring sentences to improve the prosody.
Apart from the contextual text information, it has been reported that contextual speech information can also lead to improvement of expressiveness \cite{oplustil2020using, gallegos2021comparing}.
In addition to synthesize speech at the sentence-level, \cite{xue2022paratts} has also implemented paragraph-based speech synthesis by inputting phoneme sequence of the entire paragraph. 

Above methods use context information as additional inputs to implicitly learn the prosody or style of speech.
Different from this implicit way, our proposed method utilizes a multi-scale style extractor to extract style representations at the three levels from ground-truth speech and explicitly guide the style prediction.
In addition to the contextual semantic information, the hierarchical structural relationships in context are also considered in our method.
Then, with the help of two style predictors, style embeddings of each level are generated for single sentence or whole paragraph.
Moreover, compared with ParaTTS \cite{xue2022paratts} that considers phoneme sequence only, our proposed MSStyleTTS further considers the semantic and structural information in the context and models the style at different levels in the paragraph in an explicit way.
