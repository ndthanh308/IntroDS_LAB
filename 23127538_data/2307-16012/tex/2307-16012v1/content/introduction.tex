\section{Introduction}
\label{sec:introduction}
\IEEEPARstart{T}{ext-to-speech} (TTS), which aims to generate natural and intelligible speech from text, has widespread applications in human-computer interaction.
The traditional TTS methods include concatenative synthesis \cite{olive1977rule, hunt1996unit, campbell1997prosody, wang2000corpus, black1997automatically} and statistical parametric speech synthesis \cite{yoshimura1999simultaneous, tokuda2000speech, zen2009statistical, tokuda2013speech, ze2013statistical}.
With the rapid development of deep learning, neural network based TTS models \cite{sotelo2017char2wav, tacotron, tacotron2, deepvoice3,li2019neural, Yu2020DurIANDI, fastspeech2} are devised to generate spectrograms directly from phoneme sequences, which brings great progress in synthesizing high-quality speech with a neutral style.
However, there is still a clear gap between synthesized speeches and human recordings in terms of expressiveness, which hinders the advancement of speech synthesis technology.
One important factor is that there are complex and multi-scale stylistic variations in human recordings, affected by multiple factors (including contextual information, speakerâ€™s intention, etc.), causing difficulties for TTS models to learn directly from phoneme sequences.

To model the style,
one general approach is style transfer TTS \cite{reference, gst, VAEGST, an2019learning, wu2019end,hsu2018hierarchical, multiscale, yi2022prosodyspeech}.
It is designed to generate speech with the same style as the given reference audio.
In \cite{reference}, a reference encoder is trained to obtain the style embedding at global level from the given speech.
The global style token (GST) and their derivatives further utilize several learnable style tokens to represent the global style, and perform well on the style transfer at sentence-level \cite{gst, VAEGST, wu2019end, an2019learning}.
Other recent proposed methods seek to control the local prosodic characteristics by considering fine-grained style representations \cite{lee2019robust, yi2022prosodyspeech, klimkov2019fine}.

Compared with style transfer TTS that require auxiliary inputs in inference, the approach that directly predicts style from text is more practical and flexible.
The text-predicted global style token (TP-GST) model \cite{TPGST} firstly introduces the idea of predicting style embedding or style token weights from input phoneme sequence.
Speeches with more pitch and energy variations than baseline Tacotron \cite{tacotron} can be generated in this way.
Considering that style and semantic information of utterance are closely related, the text embeddings derived from pre-trained language models (PLMs), e.g., Bidirectional Encoder Representations from Transformer (BERT) \cite{bert}, have been incorporated to TTS models to improve the expressiveness of the synthesized speech \cite{berttacotron, bertemb, fang2019towards}.
Fine-grained style representations to model local prosodic variations in speech, such as word level \cite{wsv, ren2022prosospeech, liu22m_interspeech} and phoneme level \cite{tan2020fine, du2021rich}, are also considered in some works.

However, the above text-predicted methods have two major limitations.
First, these approaches only take into account the information of the current sentence to be synthesized.
For the same sentence, these models lack the ability to capture the diverse speech variations (e.g., intonation, stress and emotion) that may be affected by its neighboring sentences.
It leads to the lack of coherence between adjacent sentences and accurate expression of synthesized speech.
This is against the human perception that the speech style of the current utterance should be primarily influenced by the context \cite{survey, longformevaluationg, wu2022self}.
In this regard, it has shown that considering a wider range of context is helpful for expressive speech synthesis \cite{nakata11audiobook, crossutterance, li2022cross,xue2022paratts}.
Second, these methods only focus on modeling the mono-scale style representations of human recordings. 
However, the style expressiveness of human recordings varies from coarse to fine granularity \cite{selkirk1986derived, liberman1977stress, tseng2005fluent}.
It shall be regarded as a composition of multi-scale acoustic factors, among which global scale is typically observed as timbre and emotion and is tended to be consistent throughout the entire utterance.
The other is the local scale, which is closer to the prosody variation, including the stress, pause, pitch, and other acoustic features of speech.
These different scales of style work together to produce rich expressiveness in speech.
Towards this end, some recent studies on similar tasks, like emotional speech synthesis \cite{lei2021fine, msemotts} and style transfer \cite{multiscale}, attempt to model stylistic variations at multiple scales.
But these approaches require auxiliary inputs, such as emotion labels and reference speech, and only consider sentence in isolation.
To the best of our knowledge, there is currently no work investigating multi-scale speaking style modeling by considering only text information.

In this paper, we propose MSStyleTTS, a novel multi-scale style modeling method that models styles at global-level, sentence-level, and subword-level\footnote{For Chinese, one Chinese character corresponds to one subword, and one subword is composed of at least one phoneme.} from hierarchical context information for expressive speech synthesis. 
It contains a multi-scale style extractor, a multi-scale style predictor, and an acoustic model based on FastSpeech 2 \cite{fastspeech2}.
The acoustic model synthesized speech of the current sentence with the multi-scale style embedding provided by the extractor or the predictor.
The extractor is used to extract style embeddings at the above three levels from the corresponding ranges of mel-spectrograms, respectively.
It is jointly trained with the acoustic model to learn the multi-scale style representations in an unsupervised way and then explicitly guide the training of the predictor.
Besides, we represent the style variations at different levels as residuals to reduce the overlap of style information between different levels, which make the subsequent prediction task easier.
The predictor generates the style embeddings at these three levels by utilizing a wider range of contextual information.
By utilizing the hierarchical context encoder, the predictor considers both structural relationships in context and the contextual information given by the pre-trained BERT \cite{bert}.
Evaluations on both in-domain and out-of-domain audiobook datasets demonstrate that the naturalness and expressiveness of speeches generated by MSStyleTTS are significantly better than those of baseline FastSpeech 2 \cite{fastspeech2}, WSV* \cite{wsv}, and HCE \cite{proposed} models.
The speech samples generated by our proposed model are available online
\footnote{Speech samples: \href{https://thuhcsi.github.io/TASLP-MSStyleTTS}{https://thuhcsi.github.io/TASLP-MSStyleTTS}}.

Our preliminary work has been presented in \cite{lei22c_interspeech}.
In this study,
the preliminary model is extended with an autoregressive style predictor to effectively capture the multi-scale speaking style and maintain coherence between sentences, which shows the effect on improving paragraph-based speech synthesizing.
We conduct extensive subjective evaluations and ablation studies to demonstrate the effectiveness of the techniques employed in our model.
Moreover, we further analyze how the expressiveness of synthesized speech is influenced by different ranges of context information and different levels of style representations utilized in MSStyleTTS, which has never been studied in previous work.
Overall, the main contributions of this paper are summarized as follows:
\begin{itemize}
    \item This paper presents MSStyleTTS, to our knowledge, which is the first attempt to model the multi-scale style from the context. It significantly improves the expressiveness and coherence of generated speech for both single-sentence and multi-sentence test.
    \item To learn the style representations, we propose a multi-scale style extractor that extracts style embeddings at subword-level, sentence-level and global-level from the corresponding ranges of mel-spectrogram and represents the style variations at different levels as residuals.
    \item For style prediction, we propose a hierarchical context encoder and two hierarchical style predictors to explore the structural relationship in context and predict styles at different levels with residual connections. Moreover, an autoregressive style predictor is designed for achieving paragraph-based synthesis.
    \item Evaluations on both in-domain and out-of-domain datasets show that MSStyleTTS achieves the goal of synthesizing speech with rich expressiveness.
    Extensive ablation studies demonstrate the effectiveness of techniques employed in MSStyleTTS, especially for the context information usage and multi-scale style representation.
\end{itemize}

The rest of the paper is organized as follows. 
We introduce related work in Section \ref{sec:relatedwork},
followed by details of our proposed approach in Section \ref{sec:model}.
Experimental results and ablation studies are presented in Section \ref{sec:exp}.
Section \ref{sec:conclution} gives conclusion of our work.