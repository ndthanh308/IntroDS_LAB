\section{Experiments} \label{sec:exp}
\subsection{Experimental Setup}
In our experiments, an internal single-speaker audiobook corpus is adopted to train and evaluate all the models\footnote{Implemented based on: \href{https://github.com/ming024/FastSpeech2}{https://github.com/ming024/FastSpeech2}\label{fn_fs2}}.
This corpus contains around 87 chapters of a fiction novel recorded by a professional Chinese male speaker, about 30 hours in total.
This audiobook corpus is suitable for expressive speech synthesis, as the styles vary between utterances, and the prosody characteristics fluctuate significantly within each utterance, such as pitch, energy, and speed.
The corpus is divided into 14,558 utterances, of which 200 are used for testing, 456 for validation, and the remainder for training.
To further test the performance of the models outside the training corpus domain, we select 30 sentences with its context from other different audiobooks as the evaluation dataset for the out-of-domain experiments.

All the text sequence is transformed to the phoneme sequence and provided as input to the acoustic model.
The 80-dimensional mel-spectrograms are extracted from the raw waveform with frame size 1,200, hop size 240, and sampling rate 24kHz, and are used as the target output.
Silences at the beginning and end of each utterance are trimmed.
We force-align the audio and phoneme sequence by an automatic speech recognition tool to obtain phoneme boundaries and durations.
Meanwhile, we use the Python library of PyWORLD\footnote{\href{https://github.com/JeremyCCHsu/Python-Wrapper-for-World-Vocoder}{https://github.com/JeremyCCHsu/Python-Wrapper-for-World-Vocoder}} to extract the frame-level pitch and energy values, and then derive the pitch and energy values at phone-level by averaging frame-level values based on the forced-aligned phoneme boundaries.

In our implementation, the sentences considered in the extractor and predictor are made up of the current sentence, its two past sentences, and its two future sentences.
We conduct an open-source pre-trained Chinese BERT-base model released by Google\footnote{\href{https://github.com/google-research/bert}{https://github.com/google-research/bert}} to extract the semantic information.
Its parameters are fixed when training the predictor.
The model is trained using a single NVIDIA V100 GPU with a batch size of 16 for 220k steps.
For MSStyleTTS, 180k iterations are required for the first train step, with 60k for each of the global-level, sentence-level, and subword-level modules, then 20k for the second train step and 20k for the final train step.
We utilize Adam optimizer\cite{kingma2014adam} with $\beta_1=0.9$, $\beta_2=0.98$, $\epsilon=10^{-9}$ and employ the warm-up strategy before 4,000 iterations.
Moreover, a well-trained HiFi-GAN \cite{kong2020hifi} model is utilized as the vocoder to transform the mel-spectrogram into the waveform, which is pre-trained on a large standard corpus and is adapted with 200k iterations on our audiobook corpus.

\subsection{Baseline Methods}
In order to test the performance of our proposed multi-scale model, we choose three baselines for comparison.
All baselines and proposed models only use text input at the inference stage, and share the same input context sequence and phoneme sequences.
The details of the three baseline models are described as follows:

\textbf{FastSpeech 2}
An open-source implementation\textsuperscript{\ref{fn_fs2}} of FastSpeech 2 \cite{fastspeech2}.
The model architecture is consistent with that of the acoustic model in our MSStyleTTS.


\textbf{WSV*}
To build this baseline model, the Word-Level Style Variations (WSVs)\cite{wsv} approach with several modifications is followed, which is referred as WSV*.
To ensure a fair comparison, FastSpeech 2 is employed as the acoustic model in our implementation, rather than the Tacotron 2 \cite{tacotron2} in the original version of WSV \cite{wsv}.
Moreover, we input the same context as MSStyleTTS into BERT, and then utilize an extra bidirectional GRU to consider the context information.
Thus it is also a context-aware method and models style at the word level.

\textbf{HCE} 
We conduct the Hierarchical Context Encoder (HCE) \cite{proposed} model which models the style at the global level from the context.
For a fair comparison, instead of XLNet \cite{xlnet} which is used in the original version of HCE, BERT has been used to obtain semantic information. 
The structure and hyperparameters of the pre-trained BERT model, acoustic model, and hierarchical context encoder are the same as those in MSStyleTTS.

\subsection{Subjective Evaluation}
\begin{table*}[h]
  \caption{The MOS test scores of different models with 95\% confidence intervals.}
  \label{tab:mos}
  \centering
  \begin{tabular}{c|ccc} 
    \hline
    \textbf{Models} & \multicolumn{2}{c}{\textbf{S-MOS}} & \textbf{M-MOS}  \\
    \textbf{} & \textbf{In-domain} & \textbf{Out-of-domain} & \textbf{}  \\
    \hline
    Ground Truth & $4.665\pm0.074$ & - & $4.560\pm0.071$ ~~~               \\
    FastSpeech 2 & $3.573\pm0.094$& $3.569\pm0.057$& $3.669\pm0.072$  ~~~  \\
    WSV* & $3.681\pm0.080$& $3.579\pm0.059$& $3.664\pm0.068$ ~~~ \\
    HCE & $3.785\pm0.084$& $3.681\pm0.065$& $3.682\pm0.075$ ~~~  \\
    \hline
    MSStyleTTS & $\mathbf{4.058\pm0.074}$ & $\mathbf{3.997\pm0.061}$ & $3.758\pm0.069$~~~ \\
    MSStyleTTS (AR) & - & - & $\mathbf{3.867\pm0.072}$~~~ \\
    \hline
  \end{tabular}
\end{table*}
% Figure environment removed
Two mean opinion score (MOS) tests are applied to evaluate the naturalness and expressiveness of the synthesized speech: 1) Single-sentence MOS (S-MOS): evaluate the synthesized speech of single sentence; 2) Multi-sentence MOS (M-MOS): evaluate the entire long-form speech which combines the synthesized speeches of multiple sentences in order.
A group of 25 listening subjects who are native Mandarin speakers are recruited to rate the generated speeches on a scale from 1 to 5 with 1 point interval.
They also score the audio reconstructed from the ground truth mel-spectrogram by HiFiGAN vocoder.
Additionally, we conduct the ABX preference test between our proposed MSStyleTTS and each of the three baselines to further analyze the perceived quality of the synthesized speech.
The same 25 listeners are asked to give their preferences between a pair of speeches synthesized by different models.
\subsubsection{S-MOS results}
The results are shown in Table \ref{tab:mos}.
For the in-domain dataset, it is observed that FastSpeech 2 has achieved the lowest MOS, with a significant gap compared to Ground Truth, suggesting that it is difficult to directly predict the rich and complex prosodic variations only from phonetic descriptions.
By considering context information, the other three models (WSV*, HCE, and MSStyleTTS) all perform better.
Our MSStyleTTS achieves the highest score of $4.058$, exceeding WSV* that just considers the fine-grained style representation by $0.377$ and HCE that narrowly considers the coarse-grained style representation by $0.273$.
This demonstrates that while each mono-scale baseline has different degrees of improvement in the expressiveness of the synthesized speech, the overall performance can be significant enhanced by merging them into a comprehensive multi-scale style model. 
For the out-of-domain dataset, we observe a decrease in scores for each model compared with the in-domain dataset, and MSStyleTTS still receives the highest score.
It indicates that our proposed model successfully learns to model different levels of style representations with robustness and generalization.

\subsubsection{M-MOS results}
The last column in Table \ref{tab:mos} shows the results of M-MOS.
We observe that speech synthesized by FastSpeech 2 has relatively smooth style transitions among sentences but lacks of expressiveness, while the speech synthesized by WSV* and HCE has rich expressiveness but varies greatly from one sentence to the next, both of which result in poor performance in the paragraph-based synthesis.
Benefiting from modelling the style representation beyond the sentence level, our proposed model achieves a higher score of $3.758$. 
Moreover, with the help of autoregressive style predictor, the M-MOS score of the proposed MSStyleTTS (AR) gains further improvement compared to the MSStyleTTS.
This demonstrates that in paragraph-based synthesis, the style coherence of the synthesized speech is important for the perceived quality.

\subsubsection{ABX test results}
The preference test results are presented in Fig.\ref{fig:abx}, which demonstrate that our proposed MSStyleTTS model significantly outperforms all the baselines.
With the additional context semantic information, MSStyleTTS gets an extra preference ($54.8\%$) over FastSpeech 2.
In addition, our MSStyleTTS is $38\%$ more preferred than the WSV*, indicating that modeling the coarse-grained style can synthesize speech with richer expressiveness.
Compared with HCE, the advantage of MSStyleTTS is that fine-grained style representation is able to control the local style characteristics, such as intonation and stress pattern, leading to a higher preference of $26\%$ than HCE.
\begin{table}[!tb]
  \caption{Objective evaluation results for MCD, F0 RMSE, Energy RMSE and Duration MSE of different models on the test set.}
  \label{tab:objective}
  \centering
  \begin{tabular}{ccccc} %l@{}l  r r
    \hline
     &\textbf{FastSpeech 2} & \textbf{WSV*} & \textbf{HCE} & \textbf{MSStyleTTS}\\
    % \textbf{Model} &\textbf{MCD} & \textbf{F0 RMSE} & \textbf{Energy RMSE} & \textbf{Duration MSE}\\
    \hline
    MCD  & $5.066$ & $5.062$ & $5.031$ & \textbf{4.979}~~  \\
    F0 RMSE & $65.266$ & $64.807$ & $63.683$& \textbf{62.544}~~ \\
    Energy RMSE  & $5.162$ & $5.221$ & $5.045$ & \textbf{4.926}~~  \\
    Duration MSE  & $0.218$ & $0.205$ & $0.209$ & \textbf{0.201}~~  \\
    \hline
  \end{tabular}
\end{table}
\subsection{Objective Evaluation}
To measure the prosody and naturalness of synthesized speech objectively, we calculate mel-cepstrum distortion (MCD), the root mean square error (RMSE) of F0 and energy, and the MSE of duration as the metrics of objective evaluation following \cite{fastspeech2, xue2022paratts}.
Since the lengths of the predicted and ground truth mel-spectrograms may be different, we first apply dynamic time warping (DTW) to derive the alignment relationships between the two mel-spectrograms.
Then, we compute the minimum MCD by aligning the two mel-spectrograms.
Following the alignment relationships, the F0 and energy sequences extracted from the synthesized speech are also aligned towards ground truth.
For the duration, we directly calculate the MSE between the predicted phoneme duration and the ground truth one.

The objective evaluation results are presented in Table \ref{tab:objective}.
The model with fine-grained style modeling (WSV*) or coarse-grained style modeling (HCE) outperforms FastSpeech 2 in most metrics, and our MSStyleTTS is the best.
From the results, it is observed that even though the fine-grained style is more closely associated with the prosody, WSV* does not show better performance than HCE in terms of the RMSE of F0 and energy.
Possible reason is that the hierarchical structure of context is ignored.
Better performance is achieved by MSStyleTTS with the multi-scale style modeling framework, demonstrating the advantages of modeling style at different levels.
It restores more accurate prosody characteristics such as pitch, energy, and duration for expressive speech synthesis.

% Figure environment removed

\subsection{Case Studies}
To further explore the impact of the multi-scale style modeling framework on the expressiveness and prosody of synthesized speech, two case studies are conducted to compare our MSStyleTTS with two mono-scale baselines, respectively.
The ground truth speeches are also provided as references.
Fig.\ref{fig:casestudy} shows the mel-spectrograms and pitch contours of ground truth speeches and speeches synthesized by different models for two test utterances.

It is observed that the speech synthesized by HCE contains larger pitch fluctuation than our MSStyleTTS.
Nevertheless, due to the absence of fine-grained style, it is difficult to control the local style variations of generated speech, resulting in a significant difference in the intonation trend compared with ground truth.
The intonation trend of the speech synthesized by WSV* is similar to MSStyleTTS but has a higher overall pitch value that is inconsistent with ground truth.
Compared with these two mono-scale baselines, the proposed multi-scale MSStyleTTS generates speech that is more similar to ground truth in terms of both the overall pitch value and local style variations, such as the intonation trend and stress patterns.


\begin{table*}[t]
  \caption{Objective evaluation results of the proposed extractor and the extractor without residual strategy on the test set. Here, ``extracted style" and ``predicted style" stand for synthesized speech with the multi-scale style embedding provided by the extractor and predictor respectively. ``-residual strategy" stand for removing the residual embeddings used in the extractor.}
  \label{tab:objextractor}
  \centering
  \begin{tabular}{lcccc} %l@{}l  r r
    \hline
     \textbf{Models} &\textbf{MCD} & \textbf{F0 RMSE} & \textbf{Energy RMSE} & \textbf{Duration MSE}\\
    % \textbf{Model} &\textbf{MCD} & \textbf{F0 RMSE} & \textbf{Energy RMSE} & \textbf{Duration MSE}\\
    \hline
    Proposed (extracted style)  & $4.710$ & $44.457$ & $3.237$ & $0.098$~~  \\
    \quad -residual strategy (extracted style) & $4.723$ & $44.416$ & $3.116$& $0.097$~~ \\
    Proposed (predicted style) & $4.979$ & $62.544$ & $4.926$ & $0.201$~~  \\
    \quad -residual strategy (predicted style) & $5.013$ & $62.956$ & $4.993$ & $0.203$~~  \\
    \hline
  \end{tabular}
\end{table*}

\begin{table}[h]
  \caption{Results of the CMOS test between MSStyleTTS and the model without using residuals to represent style variations.}
  \label{tab:cmosextractor}
  \centering
  \begin{tabular}{l|c} %l@{}l  r r
    \hline
    \textbf{Models} &\textbf{CMOS} \\
    \hline
    MSStyleTTS & $0$ ~~~ \\
    \quad -residual strategy & $-0.516$ ~~~ \\
    \hline
  \end{tabular}
\end{table}

\begin{table}[h]
  \caption{Results of the CMOS test between MSStyleTTS and the model without using the knowledge distillation strategy to train the predictor.}
  \label{tab:cmostrain}
  \centering
  \begin{tabular}{l|cc} %l@{}l  r r
    \hline
    \textbf{Models} &\multicolumn{2}{c}{\textbf{CMOS}} \\
    \textbf{} & \textbf{In-domain} & \textbf{Out-of-domain} \\
    \hline
    MSStyleTTS & $0$ & $0$ ~~~ \\
    \quad -knowledge distillation & $-0.620$ & $-0.469$ ~~~ \\
    \hline
  \end{tabular}
\end{table}
\subsection{Ablation Studies}
We conduct ablation studies to demonstrate the effectiveness of several techniques used in our proposed MSStyleTTS, including using knowledge distillation strategy to train the predictor,  using residuals to represent style variations for style extraction, utilizing context information in the predictor, using multi-scale style predictor for style prediction, and designing multi-scale style representations.

\subsubsection{The Effect of Using Knowledge Distillation Strategy to Train the Predictor}
As introduced in Section \ref{sec:modeltraining}, our proposed model is trained with a knowledge distillation strategy in three steps.
In this part, we conduct ablation studies by removing this strategy.
That is, the multi-scale style predictor is jointly trained with the acoustic model directly.
Table \ref{tab:cmostrain} presents the results.
We can see that using knowledge distillation strategy can gain signiÔ¨Åcant improvements in both in-domain and out-of-domain datasets.
It demonstrates that the predictor can learn better style representations of different levels by utilizing a pre-trained extractor as teacher model.

\subsubsection{The Effect of Using Residuals to Represent Style Variations}
As shown in Fig.\ref{fig:extractor}, we represent the style variations at different levels as residuals.
To verify the effectiveness of this representation method, we build a new extractor without residual strategy.
Firstly, we synthesize speeches with style embeddings extracted from ground-truth speech by two extractors, which is also regarded as the upper bound performance of our proposed technique.
The objective evaluation results are presented in the first two rows of Table \ref{tab:objextractor}.
As can be seen, the style embeddings extracted by two extractors have comparable performance, indicating that using residual strategy does not affect the power of style extraction.

Considering the purpose of extracting style embedding is to guide the training of the predictor rather than style transfer, we further trained two style predictors by utilizing these two style embeddings as target.
The last two rows in Table \ref{tab:objextractor} show the objective evaluation results of the generated speeches based on the predicted style embedding.
We can see that removing the residual strategy performs worse in terms of all evaluation metrics, indicating that using residuals to represent style variations is beneficial for style prediction.

The comparison mean opinion score (CMOS) test further compares perceived quality of the generated speeches.
The listening subjects are asked to rate the comparative degree between two compared models on a scale from -3 to 3 with 1 point interval, where lower rate indicates that the first model performs better than the second model and higher rate indicates that the second model performs better than the first model.
As shown in Table \ref{tab:cmosextractor}, the neglect of residual strategy results in $-0.516$ CMOS.
These results indicate that our proposed style extraction approach can effectively obtain multi-scale style information from the speech and facilitate the subsequent prediction task by reducing the interference or overlap between different levels of styles.

\subsubsection{Comparisons of Utilizing Different Ranges of Context Information in Predictor}
\begin{table}[t]
  \caption{Objective evaluation results of the proposed MSStyleTTS on the test set when using different ranges of context in predictor. Here, ``L" stands for the number of sentences considered in both the past and future context. ``L=0" means only considering the text of the current sentence.} 
  \label{tab:objcontext}
  \centering
  \begin{tabular}{ccccc} %l@{}l  r r
    \hline
    L &\textbf{MCD} & \textbf{F0 RMSE} & \textbf{Energy RMSE} & \textbf{Duration MSE}\\
    % \textbf{Model} &\textbf{MCD} & \textbf{F0 RMSE} & \textbf{Energy RMSE} & \textbf{Duration MSE}\\
    \hline
    0  & $5.041$ & $64.410$ & $5.155$ & $0.207$~~  \\
    1 & $5.016$ & $62.704$ & $5.108$& $0.207$~~ \\
    2 & \textbf{4.979} & \textbf{62.544} & \textbf{4.926} & \textbf{0.201}~~  \\
    3 & $5.048$ & $63.130$ & $5.082$ & $0.206$~~  \\
    4 & $5.121$ & $63.277$ & $5.143$ & $0.210$~~  \\
    \hline
  \end{tabular}
\end{table}
% Figure environment removed
% Figure environment removed
\begin{table*}[t]
  \caption{Objective evaluation results of ablation studies for the multi-scale style predictor and the style representations at different levels. Here, ``-residuals connections" stand for removing the residuals connections in the predictor, ``-global-level style" stand for ignoring the global-level style modeling, and ``-sentence-level style" stand for further ignoring the sentence-level style modeling on top of ignoring global-level style.}
  \label{tab:objpredictor}
  \centering
  \begin{tabular}{lccccc} %l@{}l  r r
    \hline
     \textbf{Models} &\textbf{MCD} & \textbf{F0 RMSE} & \textbf{Energy RMSE} & \textbf{Duration MSE} & \textbf{Style loss} \\
    % \textbf{Model} &\textbf{MCD} & \textbf{F0 RMSE} & \textbf{Energy RMSE} & \textbf{Duration MSE}\\
    \hline
    MSStyleTTS & $4.979$ & $62.544$ & $4.926$ & $0.201$ & $1.459$ ~~  \\
    \quad -residuals connections & $5.012$ & $62.350$ & $4.968$ & $0.203$ & $1.491$ ~~  \\
    \quad -global-level style & $5.037$ & $63.148$ & $4.975$ & $0.205$ & -~~  \\
    \qquad -sentence-level style & $5.181$ & $64.833$ & $5.201$ & $0.214$ & -~~  \\
    \hline
  \end{tabular}
\end{table*}
\begin{table}[h]
  \caption{Result of the ABX preference test between MSStyleTTS and the model without using residual connections in the predictor (-residual connections). N/P denotes ``no preference" and $p$ means the $p$-value of $t$-test between two models.}
  \label{tab:abxpredictor}
  \centering
  \begin{tabular}{cccc} %l@{}l  r r
    \hline
    \textbf{MSStyleTTS} & \textbf{-residual connections} & \textbf{N/P} &\textbf{$p$} \\
    \hline
    $52.6$ & $26.7$ & $20.7$ & \textless 0.001 ~~ \\
    \hline
  \end{tabular}
\end{table}
As introduced in Section \ref{sec:modelpredictor}, we predict multi-scale style from the context information that is composed of the current sentence, $L$ past sentences and $L$ future sentences.
By increasing or decreasing the number of sentences considered in the predictor, this experiment explores how the range of context influences the performance of the style modeling.
The objective evaluation results are shown in Table \ref{tab:objcontext}.
The model achieves the best performance across all evaluation metrics when $L = 2$ (i.e., the number of sentences considered in the context is 5).

Fig.\ref{fig:mos} further presents the MOS test results when different ranges of the context are used as input of the predictor.
First, we can see that all models perform well in terms of expressiveness, except the model considering only the text of the current sentence ($L=0$) and the model considering too many sentences in the context ($L=4$).
Second, the highest MOS result is obtained when the range of the context is 5 sentences ($L=2$) in comparison test, which is consistent with the objective evaluation results shown in Table \ref{tab:objcontext}.
As the number of sentences considered in the context decreases, the MOS of the synthesized speech decreases significantly, especially it shows the lowest MOS of $3.793$ when $L=0$. 
That is, the context information in the adjacent sentences is crucial for expressive speech synthesis, and increasing the range of context information can improve speech prosody.
However, we find that the expressiveness of the synthesized speech decreases with increasing contextual range when the number of sentences considered in the context is greater than 5.
It illustrates that considering the extra-long range of context information does not improve the perceived quality of the synthesized speech.
We suppose the reason is that the context with a long distance to the current sentence is not relevant for determining the style of the current.

To verify this hypothesis and visualize the contributions to style modeling of difference sentences that are of different distances to the current utterance, we randomly select 20 examples in the test set and plot the attention weights of the inter-sentence level attention module in the multi-scale style predictor when 9 sentences are considered in the context.
The result is shown in Fig.\ref{fig:alignment}.
As expected, although the input of the multi-scale style predictor contains 4 sentences in both the past and future context, the attention weights of the inter-sentence network tends to focus mainly on the past 2 sentences, the current sentence and the future 2 sentences (i.e., the sentence positions from 2 to 6).
That is, when sentences more than 5 are considered in the context, the extra context information does not sufficiently contribute to the style modeling, but increases the complexity of the modeling and affects the expressiveness of the synthesized speech.
This supports our hypothesis and explains why we only consider the past 2 sentences, the current sentence and the future 2 sentences in our implementation.

\subsubsection{The Effect of Multi-Scale Style Predictor}
The multi-scale style predictor consists of a hierarchical context encoder and a hierarchical style predictor.
Hierarchical context encoder is indispensable in the proposed multi-scale framework to extract the context information at different levels, and its effectiveness has been demonstrated in our previous work \cite{proposed}.
Therefore, we only evaluate the performance of the proposed hierarchical style predictor.
An ablation model has been built by removing the residual connections in this module.
The first two rows in Table \ref{tab:objpredictor} show the objective evaluation results of MSStyleTTS and the ablation model.
As can be seen, removing the residual connections causes worse MCD, RMSE of energy and MSE of duration.
This indicates the advantages of the residual connections in the hierarchical style predictor, which explicitly provides information about coarser-grained style when predicting the finer-grained style.
In addition, to measure the accuracy of style prediction, we calculate the MSE of style embedding between the predicted and the ground truth.
The results are shown in the last column in Table \ref{tab:objpredictor}.
With the residual connections in the predictor, MSStyleTTS has improved in style prediction, compared with the ablation model.
The ABX preference test is conducted between MSStyleTTS and the model without using residual connections in the hierarchical style predictor.
The results and the corresponding $p$-value are shown in Table \ref{tab:abxpredictor}.
The preference rate of our MSStyleTTS exceeds the ablation model by $25.9\%$, which demonstrates the effectiveness of our proposed hierarchical style predictor.

\subsubsection{Comparisons Between Global-Level, Sentence-Level and Subword-Level Style Representation} \label{sec:style}
\begin{table}[t]
  \caption{Results of the CMOS tests among the proposed model, the model without global-level style modeling and the model without both global-level and sentence-level style modeling.}
  \label{tab:cmos}
  \centering
  \begin{tabular}{l|c} %l@{}l  r r
    \hline
    \textbf{Models} &\textbf{CMOS} \\
    \hline
    MSStyleTTS & $0$ ~~~ \\
    \quad -global-level style & $-0.428$ ~~~ \\
    \qquad -sentence-level style & $-0.640$ ~~~ \\
    \hline
  \end{tabular}
\end{table}
In previous experiments, we have adopted two baseline models that consider mono-scale style modeling, including HCE for global-level style modeling and WSV* for word-level style modeling.
Comparison with these two models illustrates the advantage of multi-scale style modeling.
Although these two baseline models use the same context information and acoustic model as our proposed MSStyleTTS, they differ in extracting style representation and predicting style representation.

To further demonstrate the effectiveness of the global-level, sentence-level, and subword-level style representation, we first build an ablation model by removing the global-level style extraction module and prediction module.
The objective evaluation result is shown in the third row of Table \ref{tab:objpredictor}.
Our MSStyleTTS significantly outperforms the model without global-level style modeling, which indicates the effectiveness of modeling the global-level style variations for expressive speech synthesis.
Second, a new ablation model is built by further removing the sentence-level modules, that is, only modeling the subword-level style from context.
The last row in Table \ref{tab:objpredictor} shows the objective evaluation result of the new ablation model.
Comparing the results of these two ablation models, we can see that neglecting the sentence-level style modeling leads to a further decrease in all the metrics.

Table \ref{tab:cmos} shows the results of CMOS tests among our MSStyleTTS and the two ablation models mentioned above.
It can be seen that neglecting the global-level style affect the perceived quality of synthesized speech, while further neglecting the sentence-level style degrades the perceived quality of synthesized speech even more significantly.
These subjective evaluation results are consistent with the objective ones shown in Table \ref{tab:objpredictor} and again verify that in addition to fine-grained style modeling (e.g., word-level), coarse-grained style modeling (e.g., sentence-level and global-level) contributes equally to the perceived quality of the synthesized speech.  

% Figure environment removed
\begin{table}[t]\scriptsize%\footnotesize%
  \caption{Objective evaluation on emotion classifiers of using style representation at different levels. ``Global-level" and ``Sentence-level" stand for using global-level or sentence-level style representation to train emotion classifier, respectively, ``Both" stand for using both the global-level and sentence-level style representation.}
  \label{tab:emotion}
  \centering
  \begin{tabular}{cccc}
    \hline
    \textbf{Granularity} & \textbf{Overall accuracy (\%)} & \textbf{F1 Weight (\%)} &\textbf{Kappa value} \\
    \hline
    Global-level & $82.77$ & $76.12$ & $0.0773$ ~~ \\
    Sentence-level & $81.55$ & $75.16$ & $0.0606$ ~~ \\
    Both & $83.54$ & $77.90$ & $0.1644$ ~~ \\
    \hline
  \end{tabular}
\end{table} 
To analyze what each level of style representation learns, we conduct three case studies to synthesize speeches without global-level, sentence-level, and subword-level style representations, respectively.
The style representation extracted from the ground truth speech is used in this evaluation.
We plot the mel-spectrograms and pitch contours of these speeches in Fig.\ref{fig:ablationcasestudy}, the ground truth speeches and speeches synthesized with all three levels of style representation are provided for comparison. 
It can be seen that removing the global-level or sentence-level style representation results in a higher overall pitch value,
which is inconsistent with the ground truth speech.
It indicates the ability of global-level and sentence-level style representation
to control the overall prosody of the sentence.
Moreover, the word enclosed by the red box is emphasized through a higher pitch value in the ground truth speech of test case 3.
However, the pitch contour of the word synthesized without subword-level style representation is much flatter than that of the ground truth and of the speech synthesized with all three levels of style representations.
It demonstrates that subword-level style representation can learn the prosody variations such as stress.

We have performed an emotion classification task on different levels of style representation to further analyze the effect of global-level and sentence-level style.
Table \ref{tab:emotion} shows the classification results.
As can be seen, the global-level style representation contains richer emotion-related information than the sentence-level style representation.
When utilizing both global-level and sentence-level style representation, the classifier has achieved the best results in terms of all evaluation metrics.
We conjecture that the possible reason may be the sentence-level style representation acts as supplement of the global-level one, which is more relevant to emotion strength and sentence-level prosody pattern.
\subsection{Discussion}
With the multi-scale style extractor and the multi-scale style predictor, our proposed method can synthesize speech with rich expressiveness by modeling multi-scale style from a wider range of context information. 

In the current method, the different levels of style representations are obtained from the ground truth speech in an unsupervised way.
Although the experiments in Section \ref{sec:style} illustrates that coarse-grained style representations contain emotion-related information, they do not perform well on the emotion classification task.
For future work, training the style extractor in a self-supervised manner is an alternative way to establish the connection between emotion and coarse-grained style representation without explicit emotion labels.
As for style prediction, it is worth exploring how to learn more efficient style representations with large amounts of speech data in an unsupervised manner.

It is worth noting that comparing the first and third rows in Table \ref{tab:objextractor}, there is still a large gap between the speeches synthesized based on the extracted style representations and the predicted style representations in terms of all evaluation metrics. 
This indicates that it is still challenging to model complex style variations in the real-world corpus.
In future work, we will explore a new model structure to further model speaking style by considering more contextual information.