\begin{abstract}
Expressive speech synthesis is crucial for many human-computer interaction scenarios, such as audiobooks, podcasts, and voice assistants.
Previous works focus on predicting the style embeddings at one single scale from the information within the current sentence.
Whereas, context information in neighboring sentences and multi-scale nature of style in human speech are neglected, making it challenging to convert multi-sentence text into natural and expressive speech.
In this paper, we propose MSStyleTTS, a style modeling method for expressive speech synthesis, to capture and predict styles at different levels from a wider range of context rather than a sentence.
Two sub-modules, including multi-scale style extractor and multi-scale style predictor, are trained together with a FastSpeech 2 based acoustic model.
The predictor is designed to explore the hierarchical context information by considering structural relationships in context and predict style embeddings at global-level, sentence-level and subword-level.
The extractor extracts multi-scale style embedding from the ground-truth speech and explicitly guides the style prediction.
Evaluations on both in-domain and out-of-domain audiobook datasets demonstrate that the proposed method significantly outperforms the three baselines.
In addition, we conduct the analysis of the context information and multi-scale style representations that have never been discussed before.

\end{abstract}

\begin{IEEEkeywords}
text-to-speech, expressive speech synthesis, multi-scale, style modeling, hierarchical.
\end{IEEEkeywords}