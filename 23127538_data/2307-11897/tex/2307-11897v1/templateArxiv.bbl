\begin{thebibliography}{56}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alipov et~al.(2021)Alipov, Simmons-Edler, Putintsev, Kalinin, and
  Vetrov]{alipov2021towards}
Vyacheslav Alipov, Riley Simmons-Edler, Nikita Putintsev, Pavel Kalinin, and
  Dmitry Vetrov.
\newblock Towards practical credit assignment for deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2106.04499}, 2021.

\bibitem[Anand and Precup(2021)]{anand2021preferential}
Nishanth Anand and Doina Precup.
\newblock {P}referential {T}emporal {D}ifference {L}earning.
\newblock In \emph{International Conference on Machine Learning}, pages
  286--296. PMLR, 2021.

\bibitem[Andrychowicz et~al.(2017)Andrychowicz, Wolski, Ray, Schneider, Fong,
  Welinder, McGrew, Tobin, Pieter~Abbeel, and
  Zaremba]{andrychowicz2017hindsight}
Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong,
  Peter Welinder, Bob McGrew, Josh Tobin, OpenAI Pieter~Abbeel, and Wojciech
  Zaremba.
\newblock Hindsight experience replay.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[Arjona-Medina et~al.(2019)Arjona-Medina, Gillhofer, Widrich,
  Unterthiner, Brandstetter, and Hochreiter]{arjona2019rudder}
Jose~A Arjona-Medina, Michael Gillhofer, Michael Widrich, Thomas Unterthiner,
  Johannes Brandstetter, and Sepp Hochreiter.
\newblock {RUDDER}: {R}eturn decomposition for delayed rewards.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Arumugam et~al.(2021)Arumugam, Henderson, and
  Bacon]{arumugam2021information}
Dilip Arumugam, Peter Henderson, and Pierre-Luc Bacon.
\newblock {A}n {I}nformation-{T}heoretic {P}erspective on {C}redit {A}ssignment
  in {R}einforcement {L}earning.
\newblock \emph{arXiv preprint arXiv:2103.06224}, 2021.

\bibitem[Baird~III(1993)]{baird1993advantage}
Leemon~C. Baird~III.
\newblock {A}dvantage {U}pdating.
\newblock Technical report, Wright Laboratory, Wright-Patterson Air Force Base,
  1993.

\bibitem[Bellemare et~al.(2017)Bellemare, Dabney, and
  Munos]{10.5555/3305381.3305428}
Marc~G. Bellemare, Will Dabney, and R\'{e}mi Munos.
\newblock A distributional perspective on reinforcement learning.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning - Volume 70}, ICML'17, page 449–458. JMLR.org, 2017.

\bibitem[Bellman(1957)]{bellman1957markovian}
Richard Bellman.
\newblock A {M}arkovian {D}ecision {P}rocess.
\newblock \emph{Journal of Mathematics and Mechanics}, pages 679--684, 1957.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{brockman2016openai}
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman,
  Jie Tang, and Wojciech Zaremba.
\newblock Openai gym, 2016.

\bibitem[Buesing et~al.(2018)Buesing, Weber, Zwols, Racaniere, Guez, Lespiau,
  and Heess]{buesing2018woulda}
Lars Buesing, Theophane Weber, Yori Zwols, Sebastien Racaniere, Arthur Guez,
  Jean-Baptiste Lespiau, and Nicolas Heess.
\newblock Woulda, coulda, shoulda: Counterfactually-guided policy search, 2018.

\bibitem[Chatterji et~al.(2021)Chatterji, Pacchiano, Bartlett, and
  Jordan]{chatterji2021theory}
Niladri Chatterji, Aldo Pacchiano, Peter Bartlett, and Michael Jordan.
\newblock On the theory of reinforcement learning with once-per-episode
  feedback.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 3401--3412, 2021.

\bibitem[Chelu et~al.(2022)Chelu, Borsa, Precup, and van
  Hasselt]{chelu2022selective}
Veronica Chelu, Diana Borsa, Doina Precup, and Hado van Hasselt.
\newblock Selective credit assignment.
\newblock \emph{arXiv preprint arXiv:2202.09699}, 2022.

\bibitem[Choi et~al.(2022)Choi, Meng, Song, and Ermon]{choi2022density}
Kristy Choi, Chenlin Meng, Yang Song, and Stefano Ermon.
\newblock Density ratio estimation via infinitesimal classification.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 2552--2573. PMLR, 2022.

\bibitem[Dai et~al.(2020)Dai, Nachum, Chow, Li, Szepesv{\'a}ri, and
  Schuurmans]{dai2020coindice}
Bo~Dai, Ofir Nachum, Yinlam Chow, Lihong Li, Csaba Szepesv{\'a}ri, and Dale
  Schuurmans.
\newblock {CoinDICE}: {O}ff-policy confidence interval estimation.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 9398--9411, 2020.

\bibitem[Gangwani et~al.(2020)Gangwani, Zhou, and Peng]{gangwani2020learning}
Tanmay Gangwani, Yuan Zhou, and Jian Peng.
\newblock Learning guidance rewards with trajectory-space smoothing, 2020.

\bibitem[Gelada and Bellemare(2019)]{gelada2019off}
Carles Gelada and Marc~G Bellemare.
\newblock Off-policy deep reinforcement learning by bootstrapping the covariate
  shift.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pages 3647--3655, 2019.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and
  Levine]{haarnoja2018soft}
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor, 2018.

\bibitem[Hallak and Mannor(2017)]{hallak2017consistent}
Assaf Hallak and Shie Mannor.
\newblock Consistent on-line off-policy evaluation.
\newblock In \emph{International Conference on Machine Learning}, pages
  1372--1383. PMLR, 2017.

\bibitem[Hallak et~al.(2016)Hallak, Tamar, Munos, and
  Mannor]{hallak2016generalized}
Assaf Hallak, Aviv Tamar, R{\'e}mi Munos, and Shie Mannor.
\newblock Generalized emphatic temporal difference learning: {B}ias-variance
  analysis.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~30, 2016.

\bibitem[Harutyunyan et~al.(2019)Harutyunyan, Dabney, Mesnard, Gheshlaghi~Azar,
  Piot, Heess, van Hasselt, Wayne, Singh, Precup, and
  Munos]{NEURIPS2019_195f1538}
Anna Harutyunyan, Will Dabney, Thomas Mesnard, Mohammad Gheshlaghi~Azar, Bilal
  Piot, Nicolas Heess, Hado~P van Hasselt, Gregory Wayne, Satinder Singh, Doina
  Precup, and Remi Munos.
\newblock Hindsight credit assignment.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2019/file/195f15384c2a79cedf293e4a847ce85c-Paper.pdf}.

\bibitem[Hill et~al.(2018)Hill, Raffin, Ernestus, Gleave, Kanervisto, Traore,
  Dhariwal, Hesse, Klimov, Nichol, Plappert, Radford, Schulman, Sidor, and
  Wu]{stable-baselines}
Ashley Hill, Antonin Raffin, Maximilian Ernestus, Adam Gleave, Anssi
  Kanervisto, Rene Traore, Prafulla Dhariwal, Christopher Hesse, Oleg Klimov,
  Alex Nichol, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor,
  and Yuhuai Wu.
\newblock Stable baselines.
\newblock \url{https://github.com/hill-a/stable-baselines}, 2018.

\bibitem[Hung et~al.(2019)Hung, Lillicrap, Abramson, Wu, Mirza, Carnevale,
  Ahuja, and Wayne]{hung2019optimizing}
Chia-Chun Hung, Timothy Lillicrap, Josh Abramson, Yan Wu, Mehdi Mirza, Federico
  Carnevale, Arun Ahuja, and Greg Wayne.
\newblock Optimizing agent behavior over long time scales by transporting
  value.
\newblock \emph{Nature communications}, 10\penalty0 (1):\penalty0 5223, 2019.

\bibitem[Jiang and Li(2016)]{jiang2016doubly}
Nan Jiang and Lihong Li.
\newblock Doubly robust off-policy value evaluation for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  652--661. PMLR, 2016.

\bibitem[Kaelbling et~al.(1998)Kaelbling, Littman, and
  Cassandra]{kaelbling1998planning}
Leslie~Pack Kaelbling, Michael~L Littman, and Anthony~R Cassandra.
\newblock {P}lanning and {A}cting in {P}artially {O}bservable {S}tochastic
  {D}omains.
\newblock \emph{Artificial Intelligence}, 101\penalty0 (1-2):\penalty0 99--134,
  1998.

\bibitem[Klopf(1972)]{klopf1972brain}
A~Harry Klopf.
\newblock \emph{Brain {F}unction and {A}daptive {S}ystems: {A} Heterostatic
  {T}heory}.
\newblock Air Force Cambridge Research Laboratories, Air Force Systems Command,
  United States Air Force, 1972.

\bibitem[Konda and Tsitsiklis(1999)]{konda1999actor}
Vijay Konda and John Tsitsiklis.
\newblock {A}ctor-{C}ritic {A}lgorithms.
\newblock \emph{Advances in Neural Information Processing Systems}, 12, 1999.

\bibitem[Lambert et~al.(2022)Lambert, Castricato, von Werra, and
  Havrilla]{huggingface2022rlhf}
Nathan Lambert, Louis Castricato, Leandro von Werra, and Alex Havrilla.
\newblock Illustrating {R}einforcement {L}earning from {H}uman {F}eedback
  ({RLHF}), 2022.
\newblock URL \url{https://huggingface.co/blog/rlhf}.

\bibitem[Liu et~al.(2018)Liu, Li, Tang, and Zhou]{liu2018breaking}
Qiang Liu, Lihong Li, Ziyang Tang, and Dengyong Zhou.
\newblock Breaking the curse of horizon: {I}nfinite-horizon off-policy
  estimation.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Liu et~al.(2019)Liu, Swaminathan, Agarwal, and Brunskill]{liu2019off}
Yao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill.
\newblock Off-policy policy gradient with state distribution correction.
\newblock \emph{arXiv preprint arXiv:1904.08473}, 2019.

\bibitem[Liu et~al.(2020)Liu, Bacon, and Brunskill]{liu2020understanding}
Yao Liu, Pierre-Luc Bacon, and Emma Brunskill.
\newblock Understanding the curse of horizon in off-policy evaluation via
  conditional importance sampling.
\newblock In \emph{International Conference on Machine Learning}, pages
  6184--6193. PMLR, 2020.

\bibitem[Mesnard et~al.(2021)Mesnard, Weber, Viola, Thakoor, Saade,
  Harutyunyan, Dabney, Stepleton, Heess, Guez, Éric Moulines, Hutter, Buesing,
  and Munos]{mesnard2021counterfactual}
Thomas Mesnard, Théophane Weber, Fabio Viola, Shantanu Thakoor, Alaa Saade,
  Anna Harutyunyan, Will Dabney, Tom Stepleton, Nicolas Heess, Arthur Guez,
  Éric Moulines, Marcus Hutter, Lars Buesing, and Rémi Munos.
\newblock Counterfactual credit assignment in model-free reinforcement
  learning, 2021.

\bibitem[Minsky(1961)]{minsky1961steps}
Marvin Minsky.
\newblock Steps toward {A}rtificial {I}ntelligence.
\newblock \emph{Proceedings of the IRE}, 49\penalty0 (1):\penalty0 8--30, 1961.

\bibitem[Mnih et~al.(2013)Mnih, Kavukcuoglu, Silver, Graves, Antonoglou,
  Wierstra, and Riedmiller]{mnih2013playing}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis
  Antonoglou, Daan Wierstra, and Martin Riedmiller.
\newblock Playing atari with deep reinforcement learning, 2013.

\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,
  Silver, and Kavukcuoglu]{mnih2016asynchronous}
Volodymyr Mnih, Adria~Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy
  Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.
\newblock Asynchronous methods for deep reinforcement learning.
\newblock In \emph{International conference on machine learning}, pages
  1928--1937. PMLR, 2016.

\bibitem[Nachum et~al.(2019)Nachum, Chow, Dai, and Li]{nachum2019dualdice}
Ofir Nachum, Yinlam Chow, Bo~Dai, and Lihong Li.
\newblock {DualDICE}: {B}ehavior-{A}gnostic {E}stimation of {D}iscounted
  {S}tationary {D}istribution {C}orrections.
\newblock \emph{NeurIPS}, 2019.

\bibitem[Pacchiano et~al.(2023)Pacchiano, Saha, and Lee]{pacchiano2023dueling}
Aldo Pacchiano, Aadirupa Saha, and Jonathan Lee.
\newblock {D}ueling {RL}: {R}einforcement {L}earning with {T}rajectory
  {P}references.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 6263--6289. PMLR, 2023.

\bibitem[Precup et~al.(2000)Precup, Sutton, and Singh]{precup2000eligibility}
Doina Precup, Richard~S Sutton, and Satinder~P Singh.
\newblock {E}ligibility {T}races for {O}ff-{P}olicy {P}olicy {E}valuation.
\newblock In \emph{Proceedings of the Seventeenth International Conference on
  Machine Learning}, pages 759--766, 2000.

\bibitem[Puterman(1994)]{Puterman94}
Martin~L. Puterman.
\newblock \emph{{M}arkov {D}ecision {P}rocesses---{D}iscrete {S}tochastic
  {D}ynamic {P}rogramming}.
\newblock John Wiley \& Sons, Inc., New York, NY, 1994.

\bibitem[Ren et~al.(2022)Ren, Guo, Zhou, and Peng]{ren2022learning}
Zhizhou Ren, Ruihan Guo, Yuan Zhou, and Jian Peng.
\newblock Learning long-term reward redistribution via randomized return
  decomposition, 2022.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{schulman2017proximal}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Schulman et~al.(2018)Schulman, Moritz, Levine, Jordan, and
  Abbeel]{schulman2018highdimensional}
John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter
  Abbeel.
\newblock High-dimensional continuous control using generalized advantage
  estimation, 2018.

\bibitem[Singh and Sutton(1996)]{singh1996reinforcement}
Satinder~P Singh and Richard~S Sutton.
\newblock Reinforcement learning with replacing eligibility traces.
\newblock \emph{Machine learning}, 22\penalty0 (1-3):\penalty0 123--158, 1996.

\bibitem[Sutton(1984)]{sutton1984temporal}
Richard~S. Sutton.
\newblock \emph{{T}emporal {C}redit {A}ssignment in {R}einforcement
  {L}earning}.
\newblock PhD thesis, University of Massachusetts Amherst, 1984.

\bibitem[Sutton(1988)]{sutton1988learning}
Richard~S Sutton.
\newblock Learning to {P}redict by the {M}ethods of {T}emporal {D}ifferences.
\newblock \emph{Machine Learning}, 3\penalty0 (1):\penalty0 9--44, 1988.

\bibitem[Sutton and Barto(1998)]{sutton1998introduction}
Richard~S Sutton and Andrew~G Barto.
\newblock \emph{{I}ntroduction to {R}einforcement {L}earning}.
\newblock MIT Press, 1998.

\bibitem[Sutton et~al.(1999{\natexlab{a}})Sutton, McAllester, Singh, and
  Mansour]{sutton1999policy}
Richard~S Sutton, David McAllester, Satinder Singh, and Yishay Mansour.
\newblock {P}olicy {G}radient {M}ethods for {R}einforcement {L}earning with
  {F}unction {A}pproximation.
\newblock \emph{Advances in Neural Information Processing Systems}, 12,
  1999{\natexlab{a}}.

\bibitem[Sutton et~al.(1999{\natexlab{b}})Sutton, Precup, and
  Singh]{sutton1999between}
Richard~S Sutton, Doina Precup, and Satinder Singh.
\newblock Between {MDP}s and semi-{MDPs}: {A} framework for temporal
  abstraction in reinforcement learning.
\newblock \emph{Artificial Intelligence}, 112\penalty0 (1-2):\penalty0
  181--211, 1999{\natexlab{b}}.

\bibitem[Sutton et~al.(2016)Sutton, Mahmood, and White]{sutton2016emphatic}
Richard~S Sutton, A~Rupam Mahmood, and Martha White.
\newblock An emphatic approach to the problem of off-policy temporal-difference
  learning.
\newblock \emph{The Journal of Machine Learning Research}, 17\penalty0
  (1):\penalty0 2603--2631, 2016.

\bibitem[Thomas and Brunskill(2016)]{thomas2016data}
Philip Thomas and Emma Brunskill.
\newblock Data-efficient off-policy policy evaluation for reinforcement
  learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  2139--2148. PMLR, 2016.

\bibitem[Todorov et~al.(2012)Todorov, Erez, and Tassa]{6386109}
Emanuel Todorov, Tom Erez, and Yuval Tassa.
\newblock Mujoco: A physics engine for model-based control.
\newblock In \emph{2012 IEEE/RSJ International Conference on Intelligent Robots
  and Systems}, pages 5026--5033, 2012.
\newblock \doi{10.1109/IROS.2012.6386109}.

\bibitem[Uehara et~al.(2020)Uehara, Huang, and Jiang]{uehara2020minimax}
Masatoshi Uehara, Jiawei Huang, and Nan Jiang.
\newblock Minimax weight and {$Q$}-function learning for off-policy evaluation.
\newblock In \emph{International Conference on Machine Learning}, pages
  9659--9668. PMLR, 2020.

\bibitem[van Hasselt et~al.(2021)van Hasselt, Madjiheurem, Hessel, Silver,
  Barreto, and Borsa]{van2021expected}
Hado van Hasselt, Sephora Madjiheurem, Matteo Hessel, David Silver, Andr{\'e}
  Barreto, and Diana Borsa.
\newblock Expected eligibility traces.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~35, pages 9997--10005, 2021.

\bibitem[Williams(1992)]{williams1992simple}
Ronald~J Williams.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock \emph{Machine Learning}, 8\penalty0 (3):\penalty0 229--256, 1992.

\bibitem[Yu(2015)]{yu2015convergence}
Huizhen Yu.
\newblock On convergence of emphatic temporal-difference learning.
\newblock In \emph{Conference on Learning Theory}, pages 1724--1751. PMLR,
  2015.

\bibitem[Zhang et~al.(2020{\natexlab{a}})Zhang, Dai, Li, and
  Schuurmans]{zhang2020gendice}
Ruiyi Zhang, Bo~Dai, Lihong Li, and Dale Schuurmans.
\newblock {GenDICE}: {G}eneralized {O}ffline {E}stimation of {S}tationary
  {V}alues.
\newblock In \emph{International Conference on Learning Representations},
  2020{\natexlab{a}}.

\bibitem[Zhang et~al.(2020{\natexlab{b}})Zhang, Liu, and
  Whiteson]{zhang2020gradientdice}
Shangtong Zhang, Bo~Liu, and Shimon Whiteson.
\newblock {GradientDICE}: {R}ethinking generalized offline estimation of
  stationary values.
\newblock In \emph{International Conference on Machine Learning}, pages
  11194--11203. PMLR, 2020{\natexlab{b}}.

\end{thebibliography}
