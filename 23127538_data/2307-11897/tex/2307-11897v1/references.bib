@misc{brockman2016openai,
      title={OpenAI Gym}, 
      author={Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba},
      year={2016},
      eprint={1606.01540},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{schulman2018highdimensional,
      title={High-Dimensional Continuous Control Using Generalized Advantage Estimation}, 
      author={John Schulman and Philipp Moritz and Sergey Levine and Michael Jordan and Pieter Abbeel},
      year={2018},
      eprint={1506.02438},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{ren2022learning,
      title={Learning Long-Term Reward Redistribution via Randomized Return Decomposition}, 
      author={Zhizhou Ren and Ruihan Guo and Yuan Zhou and Jian Peng},
      year={2022},
      eprint={2111.13485},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{gangwani2020learning,
      title={Learning Guidance Rewards with Trajectory-space Smoothing}, 
      author={Tanmay Gangwani and Yuan Zhou and Jian Peng},
      year={2020},
      eprint={2010.12718},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@INPROCEEDINGS{6386109,
  author={Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots and Systems}, 
  title={MuJoCo: A physics engine for model-based control}, 
  year={2012},
  volume={},
  number={},
  pages={5026-5033},
  doi={10.1109/IROS.2012.6386109}}

  @inproceedings{NEURIPS2019_195f1538,
 author = {Harutyunyan, Anna and Dabney, Will and Mesnard, Thomas and Gheshlaghi Azar, Mohammad and Piot, Bilal and Heess, Nicolas and van Hasselt, Hado P and Wayne, Gregory and Singh, Satinder and Precup, Doina and Munos, Remi},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Hindsight Credit Assignment},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/195f15384c2a79cedf293e4a847ce85c-Paper.pdf},
 volume = {32},
 year = {2019}
}

@misc{mnih2013playing,
      title={Playing Atari with Deep Reinforcement Learning}, 
      author={Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Alex Graves and Ioannis Antonoglou and Daan Wierstra and Martin Riedmiller},
      year={2013},
      eprint={1312.5602},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{haarnoja2018soft,
      title={Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor}, 
      author={Tuomas Haarnoja and Aurick Zhou and Pieter Abbeel and Sergey Levine},
      year={2018},
      eprint={1801.01290},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{mesnard2021counterfactual,
      title={Counterfactual Credit Assignment in Model-Free Reinforcement Learning}, 
      author={Thomas Mesnard and Théophane Weber and Fabio Viola and Shantanu Thakoor and Alaa Saade and Anna Harutyunyan and Will Dabney and Tom Stepleton and Nicolas Heess and Arthur Guez and Éric Moulines and Marcus Hutter and Lars Buesing and Rémi Munos},
      year={2021},
      eprint={2011.09464},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{buesing2018woulda,
      title={Woulda, Coulda, Shoulda: Counterfactually-Guided Policy Search}, 
      author={Lars Buesing and Theophane Weber and Yori Zwols and Sebastien Racaniere and Arthur Guez and Jean-Baptiste Lespiau and Nicolas Heess},
      year={2018},
      eprint={1811.06272},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{nachum2019dualdice,
  title={{DualDICE}: {B}ehavior-{A}gnostic {E}stimation of {D}iscounted {S}tationary
  {D}istribution {C}orrections},
  author={Nachum, Ofir and Chow, Yinlam and Dai, Bo and Li, Lihong},
  journal={NeurIPS},
  year={2019}
}



@article{ziegler2019fine,
  title={Fine-tuning language models from human preferences},
  author={Ziegler, Daniel M and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey},
  journal={arXiv preprint arXiv:1909.08593},
  year={2019}
}

@article{stiennon2020learning,
  title={Learning to summarize with human feedback},
  author={Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={3008--3021},
  year={2020}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={arXiv preprint arXiv:2203.02155},
  year={2022}
}

@misc{huggingface2022rlhf,
    author={Lambert, Nathan and Castricato, Louis and von Werra, Leandro and Havrilla, Alex},
    title={Illustrating {R}einforcement {L}earning from {H}uman {F}eedback ({RLHF}) },
    url={https://huggingface.co/blog/rlhf},
    year={2022}
}

@article{abel2017agent,
  title={Agent-agnostic human-in-the-loop reinforcement learning},
  author={Abel, David and Salvatier, John and Stuhlm{\"u}ller, Andreas and Evans, Owain},
  journal={arXiv preprint arXiv:1701.04079},
  year={2017}
}

@article{griffith2013policy,
  title={Policy shaping: {I}ntegrating human feedback with reinforcement learning},
  author={Griffith, Shane and Subramanian, Kaushik and Scholz, Jonathan and Isbell, Charles L and Thomaz, Andrea L},
  journal={Advances in Neural Information Processing Systems},
  volume={26},
  year={2013}
}

@inproceedings{ross2011reduction,
  title={A reduction of imitation learning and structured prediction to no-regret online learning},
  author={Ross, St{\'e}phane and Gordon, Geoffrey and Bagnell, Drew},
  booktitle={Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
  pages={627--635},
  year={2011},
  organization={JMLR Workshop and Conference Proceedings}
}

@misc{stable-baselines,
  author = {Hill, Ashley and Raffin, Antonin and Ernestus, Maximilian and Gleave, Adam and Kanervisto, Anssi and Traore, Rene and Dhariwal, Prafulla and Hesse, Christopher and Klimov, Oleg and Nichol, Alex and Plappert, Matthias and Radford, Alec and Schulman, John and Sidor, Szymon and Wu, Yuhuai},
  title = {Stable Baselines},
  year = {2018},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/hill-a/stable-baselines}},
}

@article{ross2014reinforcement,
  title={Reinforcement and imitation learning via interactive no-regret learning},
  author={Ross, Stephane and Bagnell, J Andrew},
  journal={arXiv preprint arXiv:1406.5979},
  year={2014}
}

@inproceedings{knox2008tamer,
  title={{TAMER}: {T}raining an agent manually via evaluative reinforcement},
  author={Knox, W Bradley and Stone, Peter},
  booktitle={2008 7th IEEE International Conference on Development and Learning},
  pages={292--297},
  year={2008},
  organization={IEEE}
}

@misc{sutton2023alberta,
      title={The Alberta Plan for AI Research}, 
      author={Richard S. Sutton and Michael Bowling and Patrick M. Pilarski},
      year={2023},
      eprint={2208.11173},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}
@inproceedings{Isbell2001ASR,
  title={A social reinforcement learning agent},
  author={Charles Lee Isbell and Christian R. Shelton and Michael Kearns and Satinder Singh and Peter Stone},
  booktitle={International Conference on Autonomous Agents},
  year={2001}
}

@inproceedings{warnell2018deep,
  title={Deep {TAMER}: {I}nteractive agent shaping in high-dimensional state spaces},
  author={Warnell, Garrett and Waytowich, Nicholas and Lawhern, Vernon and Stone, Peter},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={32},
  year={2018}
}

@inproceedings{macglashan2017interactive,
  title={Interactive learning from policy-dependent human feedback},
  author={MacGlashan, James and Ho, Mark K and Loftin, Robert and Peng, Bei and Wang, Guan and Roberts, David L and Taylor, Matthew E and Littman, Michael L},
  booktitle={International Conference on Machine Learning},
  pages={2285--2294},
  year={2017},
  organization={PMLR}
}

@article{arumugam2019deep,
  title={Deep reinforcement learning from policy-dependent human feedback},
  author={Arumugam, Dilip and Lee, Jun Ki and Saskin, Sophie and Littman, Michael L},
  journal={arXiv preprint arXiv:1902.04257},
  year={2019}
}

@inproceedings{pilarski2011online,
  title={Online human training of a myoelectric prosthesis controller via actor-critic reinforcement learning},
  author={Pilarski, Patrick M and Dawson, Michael R and Degris, Thomas and Fahimi, Farbod and Carey, Jason P and Sutton, Richard S},
  booktitle={2011 IEEE International Conference on Rehabilitation Robotics},
  pages={1--7},
  year={2011},
  organization={IEEE}
}

@phdthesis{knox2012learning,
title={Learning from Human-Generated Reward},
author={William Bradley Knox},
school={The University of Texas at Austin},
year={2012}
}

@article{wilson2012bayesian,
  title={A {B}ayesian approach for policy learning from trajectory preference queries},
  author={Wilson, Aaron and Fern, Alan and Tadepalli, Prasad},
  journal={Advances in Neural Information Processing Systems},
  volume={25},
  year={2012}
}

@inproceedings{akrour2011preference,
  title={Preference-based policy learning},
  author={Akrour, Riad and Schoenauer, Marc and Sebag, Mich{\`e}le},
  booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  pages={12--27},
  year={2011},
  organization={Springer}
}

@inproceedings{thomaz2006reinforcement,
  title={Reinforcement learning with human teachers: {E}vidence of feedback and guidance with implications for learning performance},
  author={Thomaz, Andrea Lockerd and Breazeal, Cynthia},
  booktitle={AAAI},
  volume={6},
  pages={1000--1005},
  year={2006},
  organization={Boston, MA}
}

@inproceedings{akrour2012april,
  title={{APRIL}: {A}ctive preference learning-based reinforcement learning},
  author={Akrour, Riad and Schoenauer, Marc and Sebag, Mich{\`e}le},
  booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  pages={116--131},
  year={2012},
  organization={Springer}
}

@article{furnkranz2012preference,
  title={Preference-based reinforcement learning: {A} formal framework and a policy iteration algorithm},
  author={F{\"u}rnkranz, Johannes and H{\"u}llermeier, Eyke and Cheng, Weiwei and Park, Sang-Hyeun},
  journal={Machine learning},
  volume={89},
  number={1},
  pages={123--156},
  year={2012},
  publisher={Springer}
}

@inproceedings{akrour2014programming,
  title={Programming by feedback},
  author={Akrour, Riad and Schoenauer, Marc and Sebag, Mich{\`e}le and Souplet, Jean-Christophe},
  booktitle={International Conference on Machine Learning},
  pages={1503--1511},
  year={2014},
  organization={JMLR. org}
}

@inproceedings{wirth2016model,
  title={Model-free preference-based reinforcement learning},
  author={Wirth, Christian and F{\"u}rnkranz, Johannes and Neumann, Gerhard},
  booktitle={Thirtieth AAAI Conference on Artificial Intelligence},
  year={2016}
}

@inproceedings{el2016score,
  title={Score-based inverse reinforcement learning},
  author={El Asri, Layla and Piot, Bilal and Geist, Matthieu and Laroche, Romain and Pietquin, Olivier},
  booktitle={International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2016)},
  year={2016}
}

@article{christiano2017deep,
  title={Deep reinforcement learning from human preferences},
  author={Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@article{ibarz2018reward,
  title={Reward learning from human preferences and demonstrations in atari},
  author={Ibarz, Borja and Leike, Jan and Pohlen, Tobias and Irving, Geoffrey and Legg, Shane and Amodei, Dario},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@article{leike2018scalable,
  title={Scalable agent alignment via reward modeling: a research direction},
  author={Leike, Jan and Krueger, David and Everitt, Tom and Martic, Miljan and Maini, Vishal and Legg, Shane},
  journal={arXiv preprint arXiv:1811.07871},
  year={2018}
}

@inproceedings{pacchiano2023dueling,
  title={{D}ueling {RL}: {R}einforcement {L}earning with {T}rajectory {P}references},
  author={Pacchiano, Aldo and Saha, Aadirupa and Lee, Jonathan},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={6263--6289},
  year={2023},
  organization={PMLR}
}

@article{chatterji2021theory,
  title={On the theory of reinforcement learning with once-per-episode feedback},
  author={Chatterji, Niladri and Pacchiano, Aldo and Bartlett, Peter and Jordan, Michael},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={3401--3412},
  year={2021}
}

@article{liu2018breaking,
  title={Breaking the curse of horizon: {I}nfinite-horizon off-policy estimation},
  author={Liu, Qiang and Li, Lihong and Tang, Ziyang and Zhou, Dengyong},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@article{liu2019off,
  title={Off-policy policy gradient with state distribution correction},
  author={Liu, Yao and Swaminathan, Adith and Agarwal, Alekh and Brunskill, Emma},
  journal={arXiv preprint arXiv:1904.08473},
  year={2019}
}

@inproceedings{hallak2017consistent,
  title={Consistent on-line off-policy evaluation},
  author={Hallak, Assaf and Mannor, Shie},
  booktitle={International Conference on Machine Learning},
  pages={1372--1383},
  year={2017},
  organization={PMLR}
}

@inproceedings{gelada2019off,
  title={Off-policy deep reinforcement learning by bootstrapping the covariate shift},
  author={Gelada, Carles and Bellemare, Marc G},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  pages={3647--3655},
  year={2019}
}

@inproceedings{liu2020understanding,
  title={Understanding the curse of horizon in off-policy evaluation via conditional importance sampling},
  author={Liu, Yao and Bacon, Pierre-Luc and Brunskill, Emma},
  booktitle={International Conference on Machine Learning},
  pages={6184--6193},
  year={2020},
  organization={PMLR}
}

@inproceedings{uehara2020minimax,
  title={Minimax weight and {$Q$}-function learning for off-policy evaluation},
  author={Uehara, Masatoshi and Huang, Jiawei and Jiang, Nan},
  booktitle={International Conference on Machine Learning},
  pages={9659--9668},
  year={2020},
  organization={PMLR}
}

@inproceedings{zhang2020gendice,
  title={{GenDICE}: {G}eneralized {O}ffline {E}stimation of {S}tationary {V}alues},
  author={Zhang, Ruiyi and Dai, Bo and Li, Lihong and Schuurmans, Dale},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@inproceedings{zhang2020gradientdice,
  title={{GradientDICE}: {R}ethinking generalized offline estimation of stationary values},
  author={Zhang, Shangtong and Liu, Bo and Whiteson, Shimon},
  booktitle={International Conference on Machine Learning},
  pages={11194--11203},
  year={2020},
  organization={PMLR}
}

@article{dai2020coindice,
  title={{CoinDICE}: {O}ff-policy confidence interval estimation},
  author={Dai, Bo and Nachum, Ofir and Chow, Yinlam and Li, Lihong and Szepesv{\'a}ri, Csaba and Schuurmans, Dale},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={9398--9411},
  year={2020}
}

@article{minsky1961steps,
  title={Steps toward {A}rtificial {I}ntelligence},
  author={Minsky, Marvin},
  journal={Proceedings of the IRE},
  volume={49},
  number={1},
  pages={8--30},
  year={1961},
  publisher={IEEE}
}

@book{sutton1998introduction,
  title={{I}ntroduction to {R}einforcement {L}earning},
  author={Sutton, Richard S and Barto, Andrew G},
  year={1998},
  publisher={MIT Press}
}

@article{sutton1988learning,
  title={Learning to {P}redict by the {M}ethods of {T}emporal {D}ifferences},
  author={Sutton, Richard S},
  journal={Machine Learning},
  volume={3},
  number={1},
  pages={9--44},
  year={1988},
  publisher={Springer}
}

@phdthesis{sutton1984temporal,
  title={{T}emporal {C}redit {A}ssignment in {R}einforcement {L}earning},
  author={Sutton, Richard S.},
  year={1984},
  school={University of Massachusetts Amherst}
}

@article{williams1992simple,
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={Williams, Ronald J},
  journal={Machine Learning},
  volume={8},
  number={3},
  pages={229--256},
  year={1992},
  publisher={Springer}
}

@inproceedings{mnih2016asynchronous,
  title={Asynchronous methods for deep reinforcement learning},
  author={Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  booktitle={International conference on machine learning},
  pages={1928--1937},
  year={2016},
  organization={PMLR}
}

@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@article{sutton1999policy,
  title={{P}olicy {G}radient {M}ethods for {R}einforcement {L}earning with {F}unction {A}pproximation},
  author={Sutton, Richard S and McAllester, David and Singh, Satinder and Mansour, Yishay},
  journal={Advances in Neural Information Processing Systems},
  volume={12},
  year={1999}
}

@article{konda1999actor,
  title={{A}ctor-{C}ritic {A}lgorithms},
  author={Konda, Vijay and Tsitsiklis, John},
  journal={Advances in Neural Information Processing Systems},
  volume={12},
  year={1999}
}

@article{bellman1957markovian,
  title={A {M}arkovian {D}ecision {P}rocess},
  author={Bellman, Richard},
  journal={Journal of Mathematics and Mechanics},
  pages={679--684},
  year={1957},
  publisher={JSTOR}
}

@book{Puterman94,
        author={Martin L. Puterman},
        title={{M}arkov {D}ecision {P}rocesses---{D}iscrete {S}tochastic
		{D}ynamic {P}rogramming},
        publisher = "John Wiley \& Sons, Inc.",
        address={New York, NY},
        year={1994}
}

@inproceedings{10.5555/3305381.3305428,
author = {Bellemare, Marc G. and Dabney, Will and Munos, R\'{e}mi},
title = {A Distributional Perspective on Reinforcement Learning},
year = {2017},
publisher = {JMLR.org},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {449–458},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{van2021expected,
  title={Expected eligibility traces},
  author={van Hasselt, Hado and Madjiheurem, Sephora and Hessel, Matteo and Silver, David and Barreto, Andr{\'e} and Borsa, Diana},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  pages={9997--10005},
  year={2021}
}

@book{klopf1972brain,
  title={Brain {F}unction and {A}daptive {S}ystems: {A} Heterostatic {T}heory},
  author={Klopf, A Harry},
  year={1972},
  publisher={Air Force Cambridge Research Laboratories, Air Force Systems Command, United States Air Force}
}

@article{singh1996reinforcement,
  title={Reinforcement learning with replacing eligibility traces},
  author={Singh, Satinder P and Sutton, Richard S},
  journal={Machine learning},
  volume={22},
  number={1-3},
  pages={123--158},
  year={1996},
  publisher={Springer}
}

@article{bellemare2013arcade,
  title={{T}he {A}rcade {L}earning {E}nvironment: {A}n evaluation platform for general agents},
  author={Bellemare, Marc G and Naddaf, Yavar and Veness, Joel and Bowling, Michael},
  journal={Journal of Artificial Intelligence Research},
  volume={47},
  number={1},
  pages={253--279},
  year={2013},
  publisher={AI Access Foundation}
}


@inproceedings{precup2000eligibility,
  title={{E}ligibility {T}races for {O}ff-{P}olicy {P}olicy {E}valuation},
  author={Precup, Doina and Sutton, Richard S and Singh, Satinder P},
  booktitle={Proceedings of the Seventeenth International Conference on Machine Learning},
  pages={759--766},
  year={2000}
}

@inproceedings{jiang2016doubly,
  title={Doubly robust off-policy value evaluation for reinforcement learning},
  author={Jiang, Nan and Li, Lihong},
  booktitle={International Conference on Machine Learning},
  pages={652--661},
  year={2016},
  organization={PMLR}
}

@inproceedings{thomas2016data,
  title={Data-efficient off-policy policy evaluation for reinforcement learning},
  author={Thomas, Philip and Brunskill, Emma},
  booktitle={International Conference on Machine Learning},
  pages={2139--2148},
  year={2016},
  organization={PMLR}
}

@article{arumugam2021information,
  title={{A}n {I}nformation-{T}heoretic {P}erspective on {C}redit {A}ssignment in {R}einforcement {L}earning},
  author={Arumugam, Dilip and Henderson, Peter and Bacon, Pierre-Luc},
  journal={arXiv preprint arXiv:2103.06224},
  year={2021}
}

@article{andrychowicz2017hindsight,
  title={Hindsight experience replay},
  author={Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Pieter Abbeel, OpenAI and Zaremba, Wojciech},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@techreport{baird1993advantage,
  title={{A}dvantage {U}pdating},
  author={Baird III, Leemon C.},
  year={1993},
  institution={Wright Laboratory, Wright-Patterson Air Force Base}
}

@inproceedings{yu2015convergence,
  title={On convergence of emphatic temporal-difference learning},
  author={Yu, Huizhen},
  booktitle={Conference on Learning Theory},
  pages={1724--1751},
  year={2015},
  organization={PMLR}
}

@article{sutton2016emphatic,
  title={An emphatic approach to the problem of off-policy temporal-difference learning},
  author={Sutton, Richard S and Mahmood, A Rupam and White, Martha},
  journal={The Journal of Machine Learning Research},
  volume={17},
  number={1},
  pages={2603--2631},
  year={2016},
  publisher={JMLR. org}
}

@inproceedings{anand2021preferential,
  title={{P}referential {T}emporal {D}ifference {L}earning},
  author={Anand, Nishanth and Precup, Doina},
  booktitle={International Conference on Machine Learning},
  pages={286--296},
  year={2021},
  organization={PMLR}
}

@inproceedings{hallak2016generalized,
  title={Generalized emphatic temporal difference learning: {B}ias-variance analysis},
  author={Hallak, Assaf and Tamar, Aviv and Munos, R{\'e}mi and Mannor, Shie},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={30},
  number={1},
  year={2016}
}

@inproceedings{khetarpal2020options,
  title={Options of interest: {T}emporal abstraction with interest functions},
  author={Khetarpal, Khimya and Klissarov, Martin and Chevalier-Boisvert, Maxime and Bacon, Pierre-Luc and Precup, Doina},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  pages={4444--4451},
  year={2020}
}

@article{sutton1999between,
  title={Between {MDP}s and semi-{MDPs}: {A} framework for temporal abstraction in reinforcement learning},
  author={Sutton, Richard S and Precup, Doina and Singh, Satinder},
  journal={Artificial Intelligence},
  volume={112},
  number={1-2},
  pages={181--211},
  year={1999},
  publisher={Elsevier}
}

@article{chelu2022selective,
  title={Selective credit assignment},
  author={Chelu, Veronica and Borsa, Diana and Precup, Doina and van Hasselt, Hado},
  journal={arXiv preprint arXiv:2202.09699},
  year={2022}
}

@article{alipov2021towards,
  title={Towards practical credit assignment for deep reinforcement learning},
  author={Alipov, Vyacheslav and Simmons-Edler, Riley and Putintsev, Nikita and Kalinin, Pavel and Vetrov, Dmitry},
  journal={arXiv preprint arXiv:2106.04499},
  year={2021}
}

@article{hung2019optimizing,
  title={Optimizing agent behavior over long time scales by transporting value},
  author={Hung, Chia-Chun and Lillicrap, Timothy and Abramson, Josh and Wu, Yan and Mirza, Mehdi and Carnevale, Federico and Ahuja, Arun and Wayne, Greg},
  journal={Nature communications},
  volume={10},
  number={1},
  pages={5223},
  year={2019},
  publisher={Nature Publishing Group UK London}
}

@article{arjona2019rudder,
  title={{RUDDER}: {R}eturn decomposition for delayed rewards},
  author={Arjona-Medina, Jose A and Gillhofer, Michael and Widrich, Michael and Unterthiner, Thomas and Brandstetter, Johannes and Hochreiter, Sepp},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{choi2022density,
  title={Density ratio estimation via infinitesimal classification},
  author={Choi, Kristy and Meng, Chenlin and Song, Yang and Ermon, Stefano},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={2552--2573},
  year={2022},
  organization={PMLR}
}

@article{kaelbling1998planning,
  title={{P}lanning and {A}cting in {P}artially {O}bservable {S}tochastic {D}omains},
  author={Kaelbling, Leslie Pack and Littman, Michael L and Cassandra, Anthony R},
  journal={Artificial Intelligence},
  volume={101},
  number={1-2},
  pages={99--134},
  year={1998},
  publisher={Elsevier}
}
