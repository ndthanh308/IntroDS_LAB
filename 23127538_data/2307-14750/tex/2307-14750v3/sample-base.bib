

% Journals

% First the Full Name is given, then the abbreviation used in the AMS Math
% Reviews, with an indication if it could not be found there.
% Note the 2nd overwrites the 1st, so swap them if you want the full name.

 %{AMS}
 @String{AMSTrans = "American Mathematical Society Translations" }
 @String{AMSTrans = "Amer. Math. Soc. Transl." }
 @String{BullAMS = "Bulletin of the American Mathematical Society" }
 @String{BullAMS = "Bull. Amer. Math. Soc." }
 @String{ProcAMS = "Proceedings of the American Mathematical Society" }
 @String{ProcAMS = "Proc. Amer. Math. Soc." }
 @String{TransAMS = "Transactions of the American Mathematical Society" }
 @String{TransAMS = "Trans. Amer. Math. Soc." }

 %ACM
 @String{CACM = "Communications of the {ACM}" }
 @String{CACM = "Commun. {ACM}" }
 @String{CompServ = "Comput. Surveys" }
 @String{JACM = "J. ACM" }
 @String{ACMMathSoft = "{ACM} Transactions on Mathematical Software" }
 @String{ACMMathSoft = "{ACM} Trans. Math. Software" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newsletter" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newslett." }

 @String{AmerSocio = "American Journal of Sociology" }
 @String{AmerStatAssoc = "Journal of the American Statistical Association" }
 @String{AmerStatAssoc = "J. Amer. Statist. Assoc." }
 @String{ApplMathComp = "Applied Mathematics and Computation" }
 @String{ApplMathComp = "Appl. Math. Comput." }
 @String{AmerMathMonthly = "American Mathematical Monthly" }
 @String{AmerMathMonthly = "Amer. Math. Monthly" }
 @String{BIT = "{BIT}" }
 @String{BritStatPsych = "British Journal of Mathematical and Statistical
          Psychology" }
 @String{BritStatPsych = "Brit. J. Math. Statist. Psych." }
 @String{CanMathBull = "Canadian Mathematical Bulletin" }
 @String{CanMathBull = "Canad. Math. Bull." }
 @String{CompApplMath = "Journal of Computational and Applied Mathematics" }
 @String{CompApplMath = "J. Comput. Appl. Math." }
 @String{CompPhys = "Journal of Computational Physics" }
 @String{CompPhys = "J. Comput. Phys." }
 @String{CompStruct = "Computers and Structures" }
 @String{CompStruct = "Comput. \& Structures" }
 @String{CompJour = "The Computer Journal" }
 @String{CompJour = "Comput. J." }
 @String{CompSysSci = "Journal of Computer and System Sciences" }
 @String{CompSysSci = "J. Comput. System Sci." }
 @String{Computing = "Computing" }
 @String{ContempMath = "Contemporary Mathematics" }
 @String{ContempMath = "Contemp. Math." }
 @String{Crelle = "Crelle's Journal" }
 @String{GiornaleMath = "Giornale di Mathematiche" }
 @String{GiornaleMath = "Giorn. Mat." } % didn't find in AMS MR., ibid.

 %IEEE
 @String{Computer = "{IEEE} Computer" }
 @String{IEEETransComp = "{IEEE} Transactions on Computers" }
 @String{IEEETransComp = "{IEEE} Trans. Comput." }
 @String{IEEETransAC = "{IEEE} Transactions on Automatic Control" }
 @String{IEEETransAC = "{IEEE} Trans. Automat. Control" }
 @String{IEEESpec = "{IEEE} Spectrum" } % didn't find in AMS MR
 @String{ProcIEEE = "Proceedings of the {IEEE}" }
 @String{ProcIEEE = "Proc. {IEEE}" } % didn't find in AMS MR
 @String{IEEETransAeroElec = "{IEEE} Transactions on Aerospace and Electronic
     Systems" }
 @String{IEEETransAeroElec = "{IEEE} Trans. Aerospace Electron. Systems" }

 @String{IMANumerAna = "{IMA} Journal of Numerical Analysis" }
 @String{IMANumerAna = "{IMA} J. Numer. Anal." }
 @String{InfProcLet = "Information Processing Letters" }
 @String{InfProcLet = "Inform. Process. Lett." }
 @String{InstMathApp = "Journal of the Institute of Mathematics and
     its Applications" }
 @String{InstMathApp = "J. Inst. Math. Appl." }
 @String{IntControl = "International Journal of Control" }
 @String{IntControl = "Internat. J. Control" }
 @String{IntNumerEng = "International Journal for Numerical Methods in
     Engineering" }
 @String{IntNumerEng = "Internat. J. Numer. Methods Engrg." }
 @String{IntSuper = "International Journal of Supercomputing Applications" }
 @String{IntSuper = "Internat. J. Supercomputing Applic." } % didn't find
%% in AMS MR
 @String{Kibernetika = "Kibernetika" }
 @String{JResNatBurStand = "Journal of Research of the National Bureau
     of Standards" }
 @String{JResNatBurStand = "J. Res. Nat. Bur. Standards" }
 @String{LinAlgApp = "Linear Algebra and its Applications" }
 @String{LinAlgApp = "Linear Algebra Appl." }
 @String{MathAnaAppl = "Journal of Mathematical Analysis and Applications" }
 @String{MathAnaAppl = "J. Math. Anal. Appl." }
 @String{MathAnnalen = "Mathematische Annalen" }
 @String{MathAnnalen = "Math. Ann." }
 @String{MathPhys = "Journal of Mathematical Physics" }
 @String{MathPhys = "J. Math. Phys." }
 @String{MathComp = "Mathematics of Computation" }
 @String{MathComp = "Math. Comp." }
 @String{MathScand = "Mathematica Scandinavica" }
 @String{MathScand = "Math. Scand." }
 @String{TablesAidsComp = "Mathematical Tables and Other Aids to Computation" }
 @String{TablesAidsComp = "Math. Tables Aids Comput." }
 @String{NumerMath = "Numerische Mathematik" }
 @String{NumerMath = "Numer. Math." }
 @String{PacificMath = "Pacific Journal of Mathematics" }
 @String{PacificMath = "Pacific J. Math." }
 @String{ParDistComp = "Journal of Parallel and Distributed Computing" }
 @String{ParDistComp = "J. Parallel and Distrib. Comput." } % didn't find
%% in AMS MR
 @String{ParComputing = "Parallel Computing" }
 @String{ParComputing = "Parallel Comput." }
 @String{PhilMag = "Philosophical Magazine" }
 @String{PhilMag = "Philos. Mag." }
 @String{ProcNAS = "Proceedings of the National Academy of Sciences
                    of the USA" }
 @String{ProcNAS = "Proc. Nat. Acad. Sci. U. S. A." }
 @String{Psychometrika = "Psychometrika" }
 @String{QuartMath = "Quarterly Journal of Mathematics, Oxford, Series (2)" }
 @String{QuartMath = "Quart. J. Math. Oxford Ser. (2)" }
 @String{QuartApplMath = "Quarterly of Applied Mathematics" }
 @String{QuartApplMath = "Quart. Appl. Math." }
 @String{RevueInstStat = "Review of the International Statisical Institute" }
 @String{RevueInstStat = "Rev. Inst. Internat. Statist." }

 %SIAM
 @String{JSIAM = "Journal of the Society for Industrial and Applied
     Mathematics" }
 @String{JSIAM = "J. Soc. Indust. Appl. Math." }
 @String{JSIAMB = "Journal of the Society for Industrial and Applied
     Mathematics, Series B, Numerical Analysis" }
 @String{JSIAMB = "J. Soc. Indust. Appl. Math. Ser. B Numer. Anal." }
 @String{SIAMAlgMeth = "{SIAM} Journal on Algebraic and Discrete Methods" }
 @String{SIAMAlgMeth = "{SIAM} J. Algebraic Discrete Methods" }
 @String{SIAMAppMath = "{SIAM} Journal on Applied Mathematics" }
 @String{SIAMAppMath = "{SIAM} J. Appl. Math." }
 @String{SIAMComp = "{SIAM} Journal on Computing" }
 @String{SIAMComp = "{SIAM} J. Comput." }
 @String{SIAMMatrix = "{SIAM} Journal on Matrix Analysis and Applications" }
 @String{SIAMMatrix = "{SIAM} J. Matrix Anal. Appl." }
 @String{SIAMNumAnal = "{SIAM} Journal on Numerical Analysis" }
 @String{SIAMNumAnal = "{SIAM} J. Numer. Anal." }
 @String{SIAMReview = "{SIAM} Review" }
 @String{SIAMReview = "{SIAM} Rev." }
 @String{SIAMSciStat = "{SIAM} Journal on Scientific and Statistical
     Computing" }
 @String{SIAMSciStat = "{SIAM} J. Sci. Statist. Comput." }

 @String{SoftPracExp = "Software Practice and Experience" }
 @String{SoftPracExp = "Software Prac. Experience" } % didn't find in AMS MR
 @String{StatScience = "Statistical Science" }
 @String{StatScience = "Statist. Sci." }
 @String{Techno = "Technometrics" }
 @String{USSRCompMathPhys = "{USSR} Computational Mathematics and Mathematical
     Physics" }
 @String{USSRCompMathPhys = "{U. S. S. R.} Comput. Math. and Math. Phys." }
 @String{VLSICompSys = "Journal of {VLSI} and Computer Systems" }
 @String{VLSICompSys = "J. {VLSI} Comput. Syst." }
 @String{ZAngewMathMech = "Zeitschrift fur Angewandte Mathematik und
     Mechanik" }
 @String{ZAngewMathMech = "Z. Angew. Math. Mech." }
 @String{ZAngewMathPhys = "Zeitschrift fur Angewandte Mathematik und Physik" }
 @String{ZAngewMathPhys = "Z. Angew. Math. Phys." }

% Publishers % ================================================= |

 @String{Academic = "Academic Press" }
 @String{ACMPress = "{ACM} Press" }
 @String{AdamHilger = "Adam Hilger" }
 @String{AddisonWesley = "Addison-Wesley" }
 @String{AllynBacon = "Allyn and Bacon" }
 @String{AMS = "American Mathematical Society" }
 @String{Birkhauser = "Birkha{\"u}ser" }
 @String{CambridgePress = "Cambridge University Press" }
 @String{Chelsea = "Chelsea" }
 @String{ClaredonPress = "Claredon Press" }
 @String{DoverPub = "Dover Publications" }
 @String{Eyolles = "Eyolles" }
 @String{HoltRinehartWinston = "Holt, Rinehart and Winston" }
 @String{Interscience = "Interscience" }
 @String{JohnsHopkinsPress = "The Johns Hopkins University Press" }
 @String{JohnWileySons = "John Wiley and Sons" }
 @String{Macmillan = "Macmillan" }
 @String{MathWorks = "The Math Works Inc." }
 @String{McGrawHill = "McGraw-Hill" }
 @String{NatBurStd = "National Bureau of Standards" }
 @String{NorthHolland = "North-Holland" }
 @String{OxfordPress = "Oxford University Press" }  %address Oxford or London?
 @String{PergamonPress = "Pergamon Press" }
 @String{PlenumPress = "Plenum Press" }
 @String{PrenticeHall = "Prentice-Hall" }
 @String{SIAMPub = "{SIAM} Publications" }
 @String{Springer = "Springer-Verlag" }
 @String{TexasPress = "University of Texas Press" }
 @String{VanNostrand = "Van Nostrand" }
 @String{WHFreeman = "W. H. Freeman and Co." }

%Entries

@inproceedings{li2020oscar,
  title={Oscar: Object-semantics aligned pre-training for vision-language tasks},
  author={Li, Xiujun and Yin, Xi and Li, Chunyuan and Zhang, Pengchuan and Hu, Xiaowei and Zhang, Lei and Wang, Lijuan and Hu, Houdong and Dong, Li and Wei, Furu and others},
  booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part XXX 16},
  pages={121--137},
  year={2020},
  organization={Springer}
}

@inproceedings{wang2022ofa,
  title={Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework},
  author={Wang, Peng and Yang, An and Men, Rui and Lin, Junyang and Bai, Shuai and Li, Zhikang and Ma, Jianxin and Zhou, Chang and Zhou, Jingren and Yang, Hongxia},
  booktitle={International Conference on Machine Learning},
  pages={23318--23340},
  year={2022},
  organization={PMLR}
}

@article{wang2021simvlm,
  title={Simvlm: Simple visual language model pretraining with weak supervision},
  author={Wang, Zirui and Yu, Jiahui and Yu, Adams Wei and Dai, Zihang and Tsvetkov, Yulia and Cao, Yuan},
  journal={arXiv preprint arXiv:2108.10904},
  year={2021}
}

@misc{kakaobrain2022coyo-700m,
  title         = {COYO-700M: Image-Text Pair Dataset},
  author        = {Byeon, Minwoo and Park, Beomhee and Kim, Haecheon and Lee, Sungjun and Baek, Woonhyuk and Kim, Saehoon},
  year          = {2022},
  howpublished  = {\url{https://github.com/kakaobrain/coyo-dataset}},
}

@inproceedings{li2022blip,
  title={Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation},
  author={Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
  booktitle={International Conference on Machine Learning},
  pages={12888--12900},
  year={2022},
  organization={PMLR}
}


@inproceedings{feng2019unsupervised,
  title={Unsupervised image captioning},
  author={Feng, Yang and Ma, Lin and Liu, Wei and Luo, Jiebo},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={4125--4134},
  year={2019}
}

@inproceedings{cornia2020meshed,
  title={Meshed-memory transformer for image captioning},
  author={Cornia, Marcella and Stefanini, Matteo and Baraldi, Lorenzo and Cucchiara, Rita},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10578--10587},
  year={2020}
}

@inproceedings{luo2021dual,
  title={Dual-level collaborative transformer for image captioning},
  author={Luo, Yunpeng and Ji, Jiayi and Sun, Xiaoshuai and Cao, Liujuan and Wu, Yongjian and Huang, Feiyue and Lin, Chia-Wen and Ji, Rongrong},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={3},
  pages={2286--2293},
  year={2021}
}

@inproceedings{kuo2022beyond,
  title={Beyond a Pre-Trained Object Detector: Cross-Modal Textual and Visual Context for Image Captioning},
  author={Kuo, Chia-Wen and Kira, Zsolt},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={17969--17979},
  year={2022}
}

@inproceedings{laina2019towards,
  title={Towards unsupervised image captioning with shared multimodal embeddings},
  author={Laina, Iro and Rupprecht, Christian and Navab, Nassir},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={7414--7424},
  year={2019}
}

@inproceedings{lin2014microsoft,
  title={Microsoft coco: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle={European Conference on Computer Vision},
  pages={740--755},
  year={2014},
  organization={Springer}
}

@inproceedings{sharma2018conceptual,
  title={Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning},
  author={Sharma, Piyush and Ding, Nan and Goodman, Sebastian and Soricut, Radu},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2556--2565},
  year={2018}
}

@inproceedings{plummer2015flickr30k,
  title={Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models},
  author={Plummer, Bryan A and Wang, Liwei and Cervantes, Chris M and Caicedo, Juan C and Hockenmaier, Julia and Lazebnik, Svetlana},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={2641--2649},
  year={2015}
}

@article{kim2019image,
  title={Image captioning with very scarce supervised data: Adversarial semi-supervised learning approach},
  author={Kim, Dong-Jin and Choi, Jinsoo and Oh, Tae-Hyun and Kweon, In So},
  journal={arXiv preprint arXiv:1909.02201},
  year={2019}
}

@article{chen2015microsoft,
  title={Microsoft coco captions: Data collection and evaluation server},
  author={Chen, Xinlei and Fang, Hao and Lin, Tsung-Yi and Vedantam, Ramakrishna and Gupta, Saurabh and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  journal={arXiv preprint arXiv:1504.00325},
  year={2015}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International Conference on Machine Learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@article{lewis2019bart,
  title={Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension},
  author={Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1910.13461},
  year={2019}
}

@article{reimers2019sentence,
  title={Sentence-BERT: Sentence embeddings using siamese bert-networks},
  author={Reimers, Nils and Gurevych, Iryna},
  journal={arXiv preprint arXiv:1908.10084},
  year={2019}
}




@inproceedings{fang2022injecting,
  title={Injecting semantic concepts into end-to-end image captioning},
  author={Fang, Zhiyuan and Wang, Jianfeng and Hu, Xiaowei and Liang, Lin and Gan, Zhe and Wang, Lijuan and Yang, Yezhou and Liu, Zicheng},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={18009--18019},
  year={2022}
}


@inproceedings{wu2022difnet,
  title={DIFNet: Boosting Visual Information Flow for Image Captioning},
  author={Wu, Mingrui and Zhang, Xuying and Sun, Xiaoshuai and Zhou, Yiyi and Chen, Chao and Gu, Jiaxin and Sun, Xing and Ji, Rongrong},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={18020--18029},
  year={2022}
}

@article{cho2022fine,
  title={Fine-grained image captioning with clip reward},
  author={Cho, Jaemin and Yoon, Seunghyun and Kale, Ajinkya and Dernoncourt, Franck and Bui, Trung and Bansal, Mohit},
  journal={arXiv preprint arXiv:2205.13115},
  year={2022}
}

@inproceedings{rennie2017self,
  title={Self-critical sequence training for image captioning},
  author={Rennie, Steven J and Marcheret, Etienne and Mroueh, Youssef and Ross, Jerret and Goel, Vaibhava},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={7008--7024},
  year={2017}
}

@inproceedings{vinyals2015show,
  title={Show and tell: A neural image caption generator},
  author={Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={3156--3164},
  year={2015}
}

@inproceedings{xu2015show,
  title={Show, attend and tell: Neural image caption generation with visual attention},
  author={Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhudinov, Ruslan and Zemel, Rich and Bengio, Yoshua},
  booktitle={International Conference on Machine Learning},
  pages={2048--2057},
  year={2015},
  organization={PMLR}
}

@article{lu2017exploring,
  title={Exploring models and data for remote sensing image caption generation},
  author={Lu, Xiaoqiang and Wang, Binqiang and Zheng, Xiangtao and Li, Xuelong},
  journal={IEEE Transactions on Geoscience and Remote Sensing},
  volume={56},
  number={4},
  pages={2183--2195},
  year={2017},
  publisher={IEEE}
}

@article{meng2021object,
  title={Object-Centric Unsupervised Image Captioning},
  author={Meng, Zihang and Yang, David and Cao, Xuefei and Shah, Ashish and Lim, Ser-Nam},
  journal={arXiv preprint arXiv:2112.00969},
  year={2021}
}

@article{guo2020recurrent,
  title={Recurrent relational memory network for unsupervised image captioning},
  author={Guo, Dan and Wang, Yang and Song, Peipei and Wang, Meng},
  journal={arXiv preprint arXiv:2006.13611},
  year={2020}
}

@inproceedings{zhou2021triple,
  title={Triple sequence generative adversarial nets for unsupervised image captioning},
  author={Zhou, Yucheng and Tao, Wei and Zhang, Wenqiang},
  booktitle={ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={7598--7602},
  year={2021},
  organization={IEEE}
}

@inproceedings{gu2018unpaired,
  title={Unpaired image captioning by language pivoting},
  author={Gu, Jiuxiang and Joty, Shafiq and Cai, Jianfei and Wang, Gang},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={503--519},
  year={2018}
}

@article{ben2021unpaired,
  title={Unpaired image captioning with semantic-constrained self-learning},
  author={Ben, Huixia and Pan, Yingwei and Li, Yehao and Yao, Ting and Hong, Richang and Wang, Meng and Mei, Tao},
  journal={IEEE Transactions on Multimedia},
  volume={24},
  pages={904--916},
  year={2021},
  publisher={IEEE}
}

@inproceedings{gu2019unpaired,
  title={Unpaired image captioning via scene graph alignments},
  author={Gu, Jiuxiang and Joty, Shafiq and Cai, Jianfei and Zhao, Handong and Yang, Xu and Wang, Gang},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={10323--10332},
  year={2019}
}

@article{zhu2022unpaired,
  title={Unpaired Image Captioning by Image-level Weakly-Supervised Visual Concept Recognition},
  author={Zhu, Peipei and Wang, Xiao and Luo, Yong and Sun, Zhenglong and Zheng, Wei-Shi and Wang, Yaowei and Chen, Changwen},
  journal={arXiv preprint arXiv:2203.03195},
  year={2022}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}
@article{honda2021removing,
  title={Removing word-level spurious alignment between images and pseudo-captions in unsupervised image captioning},
  author={Honda, Ukyo and Ushiku, Yoshitaka and Hashimoto, Atsushi and Watanabe, Taro and Matsumoto, Yuji},
  journal={arXiv preprint arXiv:2104.13872},
  year={2021}
}

@article{krishna2017visual,
  title={Visual genome: Connecting language and vision using crowdsourced dense image annotations},
  author={Krishna, Ranjay and Zhu, Yuke and Groth, Oliver and Johnson, Justin and Hata, Kenji and Kravitz, Joshua and Chen, Stephanie and Kalantidis, Yannis and Li, Li-Jia and Shamma, David A and others},
  journal={International Journal of Computer Vision},
  volume={123},
  number={1},
  pages={32--73},
  year={2017},
  publisher={Springer}
}

@misc{Shutter_2022,
    year = {2022},
    title = {Shutterstock is an American provider of stock photography, stock footage, stock music, and editing tools},
    howpublished = {\url{https://www.shutterstock.com/}}    
}


@article{jiang2018difnet,
  title={Difnet: Semantic segmentation by diffusion networks},
  author={Jiang, Peng and Gu, Fanglin and Wang, Yunhai and Tu, Changhe and Chen, Baoquan},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@inproceedings{karpathy2015deep,
  title={Deep visual-semantic alignments for generating image descriptions},
  author={Karpathy, Andrej and Fei-Fei, Li},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={3128--3137},
  year={2015}
}

@inproceedings{papineni2002bleu,
  title={Bleu: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
  pages={311--318},
  year={2002}
}

@inproceedings{banerjee2005meteor,
  title={METEOR: An automatic metric for MT evaluation with improved correlation with human judgments},
  author={Banerjee, Satanjeev and Lavie, Alon},
  booktitle={Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization},
  pages={65--72},
  year={2005}
}

@inproceedings{lin2004rouge,
  title={Rouge: A package for automatic evaluation of summaries},
  author={Lin, Chin-Yew},
  booktitle={Text Summarization Branches Out},
  pages={74--81},
  year={2004}
}

@inproceedings{vedantam2015cider,
  title={Cider: Consensus-based image description evaluation},
  author={Vedantam, Ramakrishna and Lawrence Zitnick, C and Parikh, Devi},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={4566--4575},
  year={2015}
}

@inproceedings{anderson2016spice,
  title={Spice: Semantic propositional image caption evaluation},
  author={Anderson, Peter and Fernando, Basura and Johnson, Mark and Gould, Stephen},
  booktitle={European Conference on Computer Vision},
  pages={382--398},
  year={2016},
  organization={Springer}
}

@inproceedings{chen2022visualgpt,
  title={Visualgpt: Data-efficient adaptation of pretrained language models for image captioning},
  author={Chen, Jun and Guo, Han and Yi, Kai and Li, Boyang and Elhoseiny, Mohamed},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={18030--18040},
  year={2022}
}

@inproceedings{souza2020bertimbau,
  title={BERTimbau: pretrained BERT models for Brazilian Portuguese},
  author={Souza, F{\'a}bio and Nogueira, Rodrigo and Lotufo, Roberto},
  booktitle={Brazilian Conference on Intelligent Systems},
  pages={403--417},
  year={2020},
  organization={Springer}
}

@article{lin2021pretrained,
  title={Pretrained transformers for text ranking: Bert and beyond},
  author={Lin, Jimmy and Nogueira, Rodrigo and Yates, Andrew},
  journal={Synthesis Lectures on Human Language Technologies},
  volume={14},
  number={4},
  pages={1--325},
  year={2021},
  publisher={Morgan \& Claypool Publishers}
}

@article{lehman2021does,
  title={Does BERT Pretrained on Clinical Notes Reveal Sensitive Data?},
  author={Lehman, Eric and Jain, Sarthak and Pichotta, Karl and Goldberg, Yoav and Wallace, Byron C},
  journal={arXiv preprint arXiv:2104.07762},
  year={2021}
}

@article{budzianowski2019hello,
  title={Hello, it's GPT-2--how can I help you? towards the use of pretrained language models for task-oriented dialogue systems},
  author={Budzianowski, Pawe{\l} and Vuli{\'c}, Ivan},
  journal={arXiv preprint arXiv:1907.05774},
  year={2019}
}

@article{lee2020patent,
  title={Patent claim generation by fine-tuning OpenAI GPT-2},
  author={Lee, Jieh-Sheng and Hsiang, Jieh},
  journal={World Patent Information},
  volume={62},
  pages={101983},
  year={2020},
  publisher={Elsevier}
}

@inproceedings{liu2022video,
  title={Video swin transformer},
  author={Liu, Ze and Ning, Jia and Cao, Yue and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Hu, Han},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={3202--3211},
  year={2022}
}

@article{cao2021swin,
  title={Swin-unet: Unet-like pure transformer for medical image segmentation},
  author={Cao, Hu and Wang, Yueyue and Chen, Joy and Jiang, Dongsheng and Zhang, Xiaopeng and Tian, Qi and Wang, Manning},
  journal={arXiv preprint arXiv:2105.05537},
  year={2021}
}

@inproceedings{liang2021swinir,
  title={Swinir: Image restoration using swin transformer},
  author={Liang, Jingyun and Cao, Jiezhang and Sun, Guolei and Zhang, Kai and Van Gool, Luc and Timofte, Radu},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={1833--1844},
  year={2021}
}



@inproceedings{zhang2018deep,
  title={Deep mutual learning},
  author={Zhang, Ying and Xiang, Tao and Hospedales, Timothy M and Lu, Huchuan},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={4320--4328},
  year={2018}
}

@article{tarvainen2017mean,
  title={Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results},
  author={Tarvainen, Antti and Valpola, Harri},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@inproceedings{chen2021self,
  title={Self-distillation for few-shot image captioning},
  author={Chen, Xianyu and Jiang, Ming and Zhao, Qi},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={545--555},
  year={2021}
}

@inproceedings{jain2021perturb,
  title={Perturb, Predict \& Paraphrase: Semi-Supervised Learning using Noisy Student for Image Captioning.},
  author={Jain, Arjit and Samala, Pranay Reddy and Jyothi, Preethi and Mittal, Deepak and Singh, Maneesh Kumar},
  booktitle={IJCAI},
  pages={758--764},
  year={2021}
}

@inproceedings{li2019entangled,
  title={Entangled transformer for image captioning},
  author={Li, Guang and Zhu, Linchao and Liu, Ping and Yang, Yi},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={8928--8937},
  year={2019}
}

@inproceedings{baldrati2022effective,
  title={Effective Conditioned and Composed Image Retrieval Combining CLIP-Based Features},
  author={Baldrati, Alberto and Bertini, Marco and Uricchio, Tiberio and Del Bimbo, Alberto},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={21466--21474},
  year={2022}
}


@inproceedings{zhou2022extract,
  title={Extract free dense labels from clip},
  author={Zhou, Chong and Loy, Chen Change and Dai, Bo},
  booktitle={European Conference on Computer Vision},
  pages={696--712},
  year={2022},
  organization={Springer}
}

@inproceedings{meng2022object,
  title={Object-Centric Unsupervised Image Captioning},
  author={Meng, Zihang and Yang, David and Cao, Xuefei and Shah, Ashish and Lim, Ser-Nam},
  booktitle={European Conference on Computer Vision},
  pages={219--235},
  year={2022},
  organization={Springer}
}

@article{oord2018representation,
  title={Representation learning with contrastive predictive coding},
  author={Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1807.03748},
  year={2018}
}

@inproceedings{zhong2022regionclip,
  title={Regionclip: Region-based language-image pretraining},
  author={Zhong, Yiwu and Yang, Jianwei and Zhang, Pengchuan and Li, Chunyuan and Codella, Noel and Li, Liunian Harold and Zhou, Luowei and Dai, Xiyang and Yuan, Lu and Li, Yin and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={16793--16803},
  year={2022}
}

@article{alayrac2022flamingo,
  title={Flamingo: a visual language model for few-shot learning},
  author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={23716--23736},
  year={2022}
}

@article{yang2023re,
  title={Re-vilm: Retrieval-augmented visual language model for zero and few-shot image captioning},
  author={Yang, Zhuolin and Ping, Wei and Liu, Zihan and Korthikanti, Vijay and Nie, Weili and Huang, De-An and Fan, Linxi and Yu, Zhiding and Lan, Shiyi and Li, Bo and others},
  journal={arXiv preprint arXiv:2302.04858},
  year={2023}
}

@article{li2023blip,
  title={Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
  author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  journal={arXiv preprint arXiv:2301.12597},
  year={2023}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{kumar2022imagecaptioning,
  title={The Illustrated Image Captioning using transformers. ankur3107. github. io (2022)},
  author={Kumar, Ankur},
  journal={URL: https://ankur3107. github. io/blogs/the-illustrated-image-captioning-using-transformers},
  year={2022}
}

@article{xie2022visual,
  title={Visual clues: Bridging vision and language foundations for image paragraph captioning},
  author={Xie, Yujia and Zhou, Luowei and Dai, Xiyang and Yuan, Lu and Bach, Nguyen and Liu, Ce and Zeng, Michael},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={17287--17300},
  year={2022}
}

@inproceedings{brock2021high,
  title={High-performance large-scale image recognition without normalization},
  author={Brock, Andy and De, Soham and Smith, Samuel L and Simonyan, Karen},
  booktitle={International Conference on Machine Learning},
  pages={1059--1071},
  year={2021},
  organization={PMLR}
}

@article{hoffmann2022training,
  title={Training compute-optimal large language models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}

@article{lin2022retrieval,
  title={Retrieval augmented visual question answering with outside knowledge},
  author={Lin, Weizhe and Byrne, Bill},
  journal={arXiv preprint arXiv:2210.03809},
  year={2022}
}

@inproceedings{qu2021passage,
  title={Passage retrieval for outside-knowledge visual question answering},
  author={Qu, Chen and Zamani, Hamed and Yang, Liu and Croft, W Bruce and Learned-Miller, Erik},
  booktitle={Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={1753--1757},
  year={2021}
}

@article{chen2022re,
  title={Re-imagen: Retrieval-augmented text-to-image generator},
  author={Chen, Wenhu and Hu, Hexiang and Saharia, Chitwan and Cohen, William W},
  journal={arXiv preprint arXiv:2209.14491},
  year={2022}
}

@article{zhu2023prompt,
  title={Prompt-based learning for unpaired image captioning},
  author={Zhu, Peipei and Wang, Xiao and Zhu, Lin and Sun, Zhenglong and Zheng, Wei-Shi and Wang, Yaowei and Chen, Changwen},
  journal={IEEE Transactions on Multimedia},
  year={2023},
  publisher={IEEE}
}

@inproceedings{hu2023reveal,
  title={Reveal: Retrieval-augmented visual-language pre-training with multi-source multimodal knowledge memory},
  author={Hu, Ziniu and Iscen, Ahmet and Sun, Chen and Wang, Zirui and Chang, Kai-Wei and Sun, Yizhou and Schmid, Cordelia and Ross, David A and Fathi, Alireza},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={23369--23379},
  year={2023}
}

@inproceedings{ramos2023smallcap,
  title={SmallCap: lightweight image captioning prompted with retrieval augmentation},
  author={Ramos, Rita and Martins, Bruno and Elliott, Desmond and Kementchedjhieva, Yova},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={2840--2849},
  year={2023}
}

@article{ramos2023retrieval,
  title={Retrieval-augmented image captioning},
  author={Ramos, Rita and Elliott, Desmond and Martins, Bruno},
  journal={arXiv preprint arXiv:2302.08268},
  year={2023}
}

@inproceedings{changpinyo2021conceptual,
  title={Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts},
  author={Changpinyo, Soravit and Sharma, Piyush and Ding, Nan and Soricut, Radu},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={3558--3568},
  year={2021}
}

@inproceedings{lu2017knowing,
  title={Knowing when to look: Adaptive attention via a visual sentinel for image captioning},
  author={Lu, Jiasen and Xiong, Caiming and Parikh, Devi and Socher, Richard},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={375--383},
  year={2017}
}

@article{liu2021exploring,
  title={Exploring semantic relationships for unpaired image captioning},
  author={Liu, Fenglin and Gao, Meng and Zhang, Tianhao and Zou, Yuexian},
  journal={arXiv preprint arXiv:2106.10658},
  year={2021}
}

@inproceedings{zhang2022look,
  title={Look Twice as Much as You Say: Scene Graph Contrastive Learning for Self-Supervised Image Caption Generation},
  author={Zhang, Chunhui and Huang, Chao and Li, Youhuan and Zhang, Xiangliang and Ye, Yanfang and Zhang, Chuxu},
  booktitle={Proceedings of the 31st ACM International Conference on Information \& Knowledge Management},
  pages={2519--2528},
  year={2022}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={5485--5551},
  year={2020},
  publisher={JMLRORG}
}

@software{openchatkit,
  title = {{OpenChatKit: An Open Toolkit and Base Model for Dialogue-style Applications}},
  author = {Together Computer},
  url ={https://github.com/togethercomputer/OpenChatKit},
  year = {2023},
  version = {0.15},
}

@misc{openai,
  title={GPT-3.5: Language Models are Few-Shot Learners},
  author={OpenAI},
  year={2023},
  howpublished={\url{https://openai.com/gpt-3.5/}},
  note={Accessed: YYYY-MM-DD}
}

@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{deng2023conica,
  title={CONICA: A Contrastive Image Captioning Framework with Robust Similarity Learning},
  author={Deng, Lin and Zhong, Yuzhong and Wang, Maoning and Zhang, Jianwei},
  booktitle={Proceedings of the 31st ACM International Conference on Multimedia},
  pages={5109--5119},
  year={2023}
}

@inproceedings{cheng2023beyond,
  title={Beyond generic: Enhancing image captioning with real-world knowledge using vision-language pre-training model},
  author={Cheng, Kanzhi and Song, Wenpo and Ma, Zheng and Zhu, Wenhao and Zhu, Zixuan and Zhang, Jianbing},
  booktitle={Proceedings of the 31st ACM International Conference on Multimedia},
  pages={5038--5047},
  year={2023}
}

@inproceedings{zhang2023improving,
  title={Improving Image Captioning through Visual and Semantic Mutual Promotion},
  author={Zhang, Jing and Xie, Yingshuai and Liu, Xiaoqiang},
  booktitle={Proceedings of the 31st ACM International Conference on Multimedia},
  pages={4716--4724},
  year={2023}
}

@inproceedings{wang2023cropcap,
  title={CropCap: Embedding Visual Cross-Partition Dependency for Image Captioning},
  author={Wang, Bo and Zhang, Zhao and Zhao, Suiyi and Zhang, Haijun and Hong, Richang and Wang, Meng},
  booktitle={Proceedings of the 31st ACM International Conference on Multimedia},
  pages={1750--1758},
  year={2023}
}

@article{liu2024visual,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal={Advances in neural information processing systems},
  volume={36},
  year={2024}
}

@article{chiang2023vicuna,
  title={Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt quality},
  author={Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E and others},
  journal={See https://vicuna. lmsys. org (accessed 14 April 2023)},
  volume={2},
  number={3},
  pages={6},
  year={2023}
}

@article{zhu2023minigpt,
  title={Minigpt-4: Enhancing vision-language understanding with advanced large language models},
  author={Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed},
  journal={arXiv preprint arXiv:2304.10592},
  year={2023}
}

@inproceedings{liu2021swin,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={10012--10022},
  year={2021}
}

@inproceedings{yu2023cgt,
  title={CgT-GAN: CLIP-guided Text GAN for Image Captioning},
  author={Yu, Jiarui and Li, Haoran and Hao, Yanbin and Zhu, Bin and Xu, Tong and He, Xiangnan},
  booktitle={Proceedings of the 31st ACM International Conference on Multimedia},
  pages={2252--2263},
  year={2023}
}

@inproceedings{xu2023zero,
  title={Zero-TextCap: Zero-shot Framework for Text-based Image Captioning},
  author={Xu, Dongsheng and Zhao, Wenye and Cai, Yi and Huang, Qingbao},
  booktitle={Proceedings of the 31st ACM International Conference on Multimedia},
  pages={4949--4957},
  year={2023}
}

@article{hessel2021clipscore,
  title={Clipscore: A reference-free evaluation metric for image captioning},
  author={Hessel, Jack and Holtzman, Ari and Forbes, Maxwell and Bras, Ronan Le and Choi, Yejin},
  journal={arXiv preprint arXiv:2104.08718},
  year={2021}
}

@article{nukrai2022text,
  title={Text-only training for image captioning using noise-injected clip},
  author={Nukrai, David and Mokady, Ron and Globerson, Amir},
  journal={arXiv preprint arXiv:2211.00575},
  year={2022}
}

@article{li2023decap,
  title={Decap: Decoding clip latents for zero-shot captioning via text-only training},
  author={Li, Wei and Zhu, Linchao and Wen, Longyin and Yang, Yi},
  journal={arXiv preprint arXiv:2303.03032},
  year={2023}
}




@article{chen2023minigpt,
  title={Minigpt-v2: large language model as a unified interface for vision-language multi-task learning},
  author={Chen, Jun and Zhu, Deyao and Shen, Xiaoqian and Li, Xiang and Liu, Zechun and Zhang, Pengchuan and Krishnamoorthi, Raghuraman and Chandra, Vikas and Xiong, Yunyang and Elhoseiny, Mohamed},
  journal={arXiv preprint arXiv:2310.09478},
  year={2023}
}

@article{liu2023improved,
  title={Improved baselines with visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
  journal={arXiv preprint arXiv:2310.03744},
  year={2023}
}

@inproceedings{tang2021clip4caption,
  title={Clip4caption: Clip for video caption},
  author={Tang, Mingkang and Wang, Zhanyu and Liu, Zhenhua and Rao, Fengyun and Li, Dian and Li, Xiu},
  booktitle={Proceedings of the 29th ACM International Conference on Multimedia},
  pages={4858--4862},
  year={2021}
}

@inproceedings{guo2019aligning,
  title={Aligning linguistic words and visual semantic units for image captioning},
  author={Guo, Longteng and Liu, Jing and Tang, Jinhui and Li, Jiangwei and Luo, Wei and Lu, Hanqing},
  booktitle={Proceedings of the 27th ACM international conference on multimedia},
  pages={765--773},
  year={2019}
}

@inproceedings{li2023multi,
  title={Multi-modal Domain Adaptation for Text Visual Question Answering Tasks},
  author={Li, Zhiyuan and Liu, Dongnan and Cai, Weidong},
  booktitle={2023 International Conference on Digital Image Computing: Techniques and Applications (DICTA)},
  pages={371--378},
  year={2023},
  organization={IEEE}
}

@article{li2024enhancing,
  title={Enhancing Advanced Visual Reasoning Ability of Large Language Models},
  author={Li, Zhiyuan and Liu, Dongnan and Zhang, Chaoyi and Wang, Heng and Xue, Tengfei and Cai, Weidong},
  journal={arXiv preprint arXiv:2409.13980},
  year={2024}
}

@article{li2024multimodal,
  title={Multimodal Causal Reasoning Benchmark: Challenging Vision Large Language Models to Infer Causal Links Between Siamese Images},
  author={Li, Zhiyuan and Wang, Heng and Liu, Dongnan and Zhang, Chaoyi and Ma, Ao and Long, Jieting and Cai, Weidong},
  journal={arXiv preprint arXiv:2408.08105},
  year={2024}
}