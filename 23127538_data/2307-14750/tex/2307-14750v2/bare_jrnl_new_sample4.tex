\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage[para,online,flushleft]{threeparttable}
\usepackage{colortbl}
\usepackage{float}
\usepackage[table]{xcolor}

\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021

\begin{document}

\title{Exploring Annotation-free Image Captioning with Retrieval-augmented Pseudo Sentence Generation}

\author{Zhiyuan Li,
Dongnan Liu, Heng Wang, Chaoyi Zhang, and Weidong Cai

\thanks{Z. Li, D. Liu, H. Wang, C. Zhang and W. Cai are with the School of Computer Science, University of Sydney, Sydney 2008, Australia (email: zhli0736@uni.sydney.edu.au;   dongnan.liu@sydney.edu.au;  
 hwan9147@uni.sydney.edu.au;  
 czha5168@uni.sydney.edu.au;
tom.cai@sydney.edu.au).}% <-this % stops a space
}

% The paper headers
\markboth{}%
{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

% \IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

\maketitle

\begin{abstract}
Training an image captioner without annotated image-sentence pairs has gained traction in recent years. Previous approaches can be categorized into two strategies: crawling sentences from mismatching corpora and aligning them with the given images as pseudo annotations, or pre-training the captioner using external image-text pairs. However, the aligning setting seems to reach its performance limit due to the quality problem of pairs, and pre-training requires significant computational resources. To address these challenges, we propose a new strategy ``LPM + retrieval-augmented learning" where the prior knowledge from large pre-trained models (LPMs) is leveraged as supervision, and a retrieval process is integrated to further reinforce its effectiveness. Specifically, we introduce \textbf{R}etrieval-\textbf{a}ugmented \textbf{P}seudo \textbf{S}entence \textbf{G}eneration (RaPSG), which adopts an efficient approach to retrieve highly relevant short region descriptions from the mismatching corpora and use them to generate a variety of pseudo sentences with distinct representations as well as high quality via LPMs. In addition, a fluency filter and a CLIP-guided training objective are further introduced to facilitate model optimization. Experimental results demonstrate that our method surpasses the SOTA pre-training model (Flamingo3B) by achieving a CIDEr score of 78.1 (+5.1) while utilizing only 0.3$\%$ of its trainable parameters (1.3B VS 33M). Importantly, our approach eliminates the need of computationally expensive pre-training processes on external datasets (e.g., the requirement of 312M image-text pairs for Flamingo3B). We further show that with a simple extension, the generated pseudo sentences can be deployed as weak supervision to boost the $1\%$ semi-supervised image caption benchmark up to 93.4 CIDEr score (+8.9) which showcases the versatility and effectiveness of our approach. Code is available at: https://github.com/Zhiyuan-Li-John/RaPSG.
\end{abstract}

\begin{IEEEkeywords}
Image Captioning, Large Pre-trained Models, Retrieval-augmented Models, Zero-shot Learning, Unsupervised Learning.
\end{IEEEkeywords}

\section{Introduction}
\label{intro}
\IEEEPARstart{I}{mage} captioning is one of the fundamental vision-language (VL) tasks, which generates natural textual descriptions for images. The advent of Transformer-based models~\cite{cornia2020meshed,hu2022tsfnet,ji2022multi,wang2022text,kuo2022beyond,luo2021dual} has brought significant advancements in performance. However, the performance of traditional full-supervised methods is restricted by the availability of paired image-text data. Recent breakthroughs in image captioning~\cite{li2020oscar,wang2022ofa,wang2021simvlm} have illustrated the power of pre-training large-scale VL models using external datasets. However, its practical applications are constrained
by the massive resource requirements and time consumption (e.g. SimVLM$_{huge}$~\cite{wang2021simvlm} has 1.4B trainable parameters and requires 1.8B image-text pairs for pre-training). Additionally, these pre-training methods are optimized with the data crawled from the website~\cite{kakaobrain2022coyo}, and it inevitably contains some noisy image-text pairs, as shown in Figure~\ref{fig1} (a), which makes the pre-training process suffer from poor sample efficiency (large-scale dataset with noisy image-text pairs is a suboptimal source of supervision~\cite{li2022blip}).
% Figure environment removed

To alleviate these problems, transferring prior knowledge from frozen LPMs to downstream VL tasks has become the new trend. One notable architecture is Flamingo~\cite{alayrac2022flamingo}, which proposes an auxiliary trainable module as a ``bridge" to connect the LPM with a vision encoder. Different from~\cite{wang2021simvlm}, Flamingo keeps the pre-trained models frozen to reduce computational cost and counteract the issue of catastrophic forgetting. Following this idea, Re-ViLM~\cite{yang2023re} builds the model based on Flamingo but adds the retrieved captions (CLIP-based~\cite{radford2021learning}) from the external database as augmentation. Similarly, BLIP2~\cite{li2023blip} introduces an efficient pre-training strategy that bootstraps language-image pre-training with frozen image encoders and large language models. However, despite the advancements, all these methods still rely on billions of external image-text pairs (e.g., 129M for BLIP2) for ``bridge" learning and remain susceptible to the challenge of the noisy image-text pairs problem.

In this paper, we propose an efficient framework called RaPSG (Retrieval-augmented Pseudo Sentence Generation) that leverages frozen LPMs to generate high-quality pseudo sentences without the need of external image-text pairs for optimization. Specifically, 
a retrieval-based pipeline is designed to generate multiple sentences for each target image, as shown in Figure~\ref{fig1} (b). To address the challenge of noisy image-text pairs and improve the quality of generated pseudo sentences, we propose a refinement strategy based on a ranking of high-relevance region descriptions. For each target image, we employ the pre-trained model CLIP~\cite{radford2021learning} to retrieve the top-$k$ most correlated region descriptions from the Visual Genome (VG) dataset~\cite{krishna2017visual} (We eliminate the overlapping parts between COCO and VG). However, as shown in Figure~\ref{fig1} (b), these fetched region descriptions lack certain modified words compared to regular sentences, which results in a semantic information deficiency. To overcome this problem, we further group region descriptions into multiple comprehensive and distinct long sentences using summarization LPMs (e.g., BART~\cite{lewis2019bart} and LLaMA~\cite{touvron2023llama}). After obtaining such high-quality pseudo sentences, we then introduce a self-supervised framework to facilitate the retrieval-augmented captioner, optimized with the original images and corresponding generated pseudo sentences as supervisions. In addition, we design two mechanisms to improve the plain pseudo-labeling strategy. Firstly, a fluency filter is proposed to effectively filter out imperfect descriptions for mitigating the impact of noisy image-text pairs. Additionally, a CLIP-guided optimization strategy is proposed to encourage the model's understanding of the relationship between images and text which can offset the lack of external image-text pairs.

To demonstrate the capability of our RaPSG approach, we evaluate its performance on the MSCOCO~\cite{chen2015microsoft} and Flickr30k~\cite{plummer2015flickr30k} benchmarks. To the best of our knowledge, we are making an early attempt to explore a new benchmark setting ``LPMs + retrieval-augmented self-supervised learning" on image captioning tasks without labels which is efficient and could provide a new direction for the research community. There are two comparable settings that we can contrast with our RaPSG approach: zero-shot and weakly-supervised image captioning. Compared with the existing zero-shot image captioning methods by tuning LPMs (SimVLM~\cite{wang2021simvlm}, Flamingo~\cite{alayrac2022flamingo} and Re-ViLM~\cite{yang2023re}), we introduce a more efficient learning process that involves self-supervised training with the generated pseudo sentences, rather than relying on a large-scale external dataset. The experiment results show that our approach has 
outperformed the SOTA model Flamingo3B on zero-shot settings with 78.1 (+5.1) CIDEr score. Additionally, we conduct a comprehensive comparison of our method with weakly-supervised image captioning models, encompassing unsupervised, unpaired, and weakly-supervised settings. This is because both our approach and these models assume the absence of grounded image-text pairs and instead propose optimizing the models using pseudo pairs. According to the experimental results, our method consistently achieves SOTA performance compared to these models. Notably, our proposed RaPSG even surpasses unpaired setting models that require real caption annotations for images, which indicates the effectiveness and efficiency of our method. To further explore the potency of our method, we decide to test the quality of generated pseudo sentences where we treat them as weak supervision for the semi-supervised image captioning benchmarks where only $1\%$ data has labels~\cite{li2020oscar}. It presents that our method achieves 93.4 (+8.9) CIDEr score over SOTA performance.
 
Our contributions can be summarized as below:
\begin{itemize}
\item We propose an inference-only method to transfer the knowledge from frozen LLMs by generating a variety of distinct pseudo sentences for each target image.
\item A fluency filter and CLIP-guided training are further introduced to strengthen the retrieval-augmented learning of the captioner for better prediction.
\item Experiments demonstrate our proposed method can achieve SOTA performance on both zero-shot and weakly-supervised image captioning settings.
\item Our method also outperforms the SOTA $1\%$ semi-supervised image captioning models.
\end{itemize}

% Figure environment removed

\section{Related Work}
\subsection{Large-scale Pre-trained Models in Image Captioning}
The power of large-scale pre-trained models (LPMs) has been demonstrated for VL tasks (OSCAR~\cite{li2020oscar}, OFA~\cite{wang2022ofa}, SimVLM~\cite{wang2021simvlm}). In recent years, the appearance of a series of high-performance LPMs (e.g. ViT~\cite{dosovitskiy2020image}, GPT~\cite{radford2019language}, CLIP~\cite{radford2021learning}) has widely extended the possibility of directly getting prior knowledge. Kuo et al.~\cite{kuo2022beyond} used pre-trained CLIP to mine missing attributes and relationships as auxiliary inputs in a fully supervised captioning task. Cho et al.~\cite{cho2022fine} used CLIP to build a CLIP score replacing the traditional cross-entropy loss, which can avoid reference in strength learning. Chen et al.~\cite{chen2022visualgpt} leveraged the linguistic knowledge from a large pre-trained language model to overcome the limited annotated data in the semi-supervised setting of image captioning tasks. Kumar~\cite{kumar2022imagecaptioning} augmented the pre-trained GPT2 with ViT encoder and directly fine-tuned the model on downstream tasks. Xie et al.~\cite{xie2022visual} used visual clues to bridge large pre-trained vision foundation models and language models for image paragraph captioning. Additionally, some works start to explore leveraging from the frozen LPMs. Flamingo~\cite{alayrac2022flamingo} builds a trainable architecture to bridge NFNet~\cite{brock2021high} and  Chinchilla models~\cite{hoffmann2022training}, which can efficiently accept arbitrarily interleaved visual data and text as input and generate text in an open-ended manner.  BLIP2~\cite{li2023blip} bridges the modality gap with a lightweight querying Transformer and is more efficient in the pre-training strategy. However, all these methods still need pre-training on large-scale datasets for model optimization. 

\subsection{Retrieval-augmented Models with LPMs}
Retrieval-augmented methods have been widely applied in VL tasks in recent years, which are more efficient compared with pre-training strategies. In visual question answering, retrieving the outside knowledge for question answering has become the new trend~\cite{lin2022retrieval,qu2021passage}. In text-to-image generation, Chen et al.~\cite{chen2022re} propose a generative model that uses retrieved information to produce high-fidelity and faithful images for uncommon entities.

In this work, we apply the retrieval-augmented method for image captioning. Currently, few works~\cite{zhu2023prompt,hu2022reveal,yang2023re, ramos2023smallcap,ramos2023retrieval} apply a similar idea with LPMs for the same task. Zhu et al.~\cite{zhu2023prompt} used pre-trained CLIP to extract the semantic prompt for more accurate caption prediction under the adversarial learning framework. Re-ViLM~\cite{yang2023re} builds upon the Flamingo but supports using CLIP to retrieve relevant knowledge from the external database. REVEAL~\cite{hu2022reveal} learns to encode world knowledge into a large-scale memory and
to retrieve information from it to answer knowledge-intensive queries. Smallcap~\cite{ramos2023smallcap} and EXTRA~\cite{ramos2023retrieval} apply retrieval sentences as extra information in fully-supervised image captioning tasks. Compared with their methods, our approach gets knowledge from high-quality generated pseudo sentences and is more data-efficient, which avoids using unpaired human annotation~\cite{zhu2023prompt} or large-scale image-text corpus for pre-training~\cite{hu2022reveal,yang2023re}.


\section{Method}
In this section, we introduce our proposed framework RaPSG, whose overview is shown in Figure~\ref{fig2}. The retrieval-augmented pseudo sentence efficient generation (Retrieval-augmented PSG module) is proposed to learn knowledge from the LPMs (Section~\ref{pseudo}). To reduce the appearance of unnatural pseudo sentences, we innovatively design a fluency filter (Section~\ref{fluency}). Finally, the self-supervised training with generated pseudo image-text pairs is guided by a CLIP-based contrastive loss to improve the prediction accuracy (Section~\ref{contrastive}).


\subsection{Retrieval-augmented PSG Module}
\label{pseudo}
Before introducing our proposed RaPSG algorithm, we first revisit the concept of fully supervised image captioning. Given an image $I$ with human-annotated descriptions set $D=\{S_1,...,S_H\}$ where $H$ is the number of annotated sentences, the captioning model aims to generate a prediction sentence $P$ mimicking the annotated sentences. Denote $S_h$ as a reference sentence in annotation set $D$ consisting of $N$ words: $S_h=\{w_1^h,w_2^h,...,w_N^h\}$. Similarly, we denote prediction sentence $P=\{w_1,w_2,...,w_N\}$. In previous pre-training strategies, the crawled sentence $S_h'$ replaces the real human-annotated sentence $S_h$ from external large-scale dataset~\cite{kakaobrain2022coyo,changpinyo2021conceptual}. The pre-training process of image captioning can be formulated as:
\vspace{-0.25cm}
 \begin{equation}
\begin{aligned}
   &\text{min} \sum_{h=1}^{H=1} \mathop{l}(P,S_h') = \{l_1',...,l_N'\}\\
   &=\{f(w_1,w_1'),...,f(w_N,w_N')\},
  \label{eq1}
 \end{aligned}
\end{equation}
 where $l$ is the negative log likelihood function, $H$ is the number of descriptions per image, $w_n$ is the word in the prediction sentence $P$, and $w_1'$ is the word in crawled sentence $S_h'$. $f$ is the comparison function between prediction word $w_n$ and crawled word $w_1'$. Compared with real-human annotation, such pre-training strategies incur the poor sample efficiency challenge~\cite{li2022blip}. The crawled sentence $S_h'$ inevitably contains some irrelevant words $w_n'$ or phrases $\{w_n',...,w_{n+t}'\}$. The presence of these irrelevant parts explicitly impacts the prediction accuracy because the captioning model is mimicking an inaccurate sentence~\cite{honda2021removing}.


To address the above challenge, we propose RaPSG, a two-stage retrieval-augmented pseudo sentence generation method. It leverages the prior knowledge stored in LPMs to 
generate high-quality pseudo sentences, which serve as effective training supervision. Specifically, our method is based on the text processing capabilities from different aspects of LPMs. These include region-level matching using CLIP, global-level summarization through BART, and LLaMA for further enhancement. By incorporating these aspects, our method ensures the quality of pseudo sentences through comprehensive and robust text processing capabilities within RaPSG.


% In the first stage, we utilize the summarization capability of LPMs (BART) to condense short high-relevant region descriptions into pseudo sentences (shown in Figure~\ref{fig3}). This enables us to capture the essential information from regions and generate concise and informative sentences. In the second stage, we take advantage of the expressive power of the large-scale generative model (Llama) to further refine and enhance the previous pseudo sentences (shown in Figure~\ref{fig4}). This enables us to produce more fluent and contextually appropriate sentences for supervision.

% Figure environment removed

In the first stage, we focus on utilizing the summarization capability of LPMs (BART) to condense short high-relevant region descriptions into pseudo sentences (shown in Figure~\ref{fig3}). This enables us to capture the essential information from regions and generate concise and informative sentences. To begin, we retrieve local-level region descriptions that match the provided images. For this purpose, we employ a small-scale public dataset that comprises  region descriptions (VG~\cite{krishna2017visual}), some of whose images are sourced from the MSCOCO dataset~\cite{chen2015microsoft}. To mitigate the influence of overlapping between COCO and VG, we refrain from directly employing the region descriptions from VG.  Instead, we adopt the approach proposed by \cite{kuo2022beyond}, which involves mining attributes and relationships to obtain region descriptions and effectively eliminate the potential overlapping between COCO and VG. After preparing region descriptions, we utilize the pre-trained CLIP~\cite{radford2021learning} model to extract cross-modal features. The CLIP has two branches: CLIP-I and CLIP-T, which can serve as $\mathcal{E}_I$ and $\mathcal{E}_T$ for images and texts, respectively. Then, we use the cosine similarity function as our $F_s$ to compute the matching score of each region description for the given image and rank them in descending order. It is defined as: 
\begin{equation}
\begin{aligned}
   S(R_m)=F_s(\mathcal{E}_I(I), \mathcal{E}_T(R_m)),
  \label{eq2}
 \end{aligned}
\end{equation}
where $\mathcal{E}_I$ and $\mathcal{E}_T$ are the frozen feature extractors in the CLIP for images and sentences, respectively. $F_s$ is the function calculating the similarity between the cross-modality features. Next, the top-$k$ most relevant descriptions are selected according to the score for subsequent steps. However, the region descriptions (shown in Figure~\ref{fig1} (b)) are short of modifying phases compared with normal sentences. According to previous work~\cite{feng2019unsupervised}, the concepts contain too little semantic information would cause image captioning training failure.

To cope with the absence of information, we propose to refine these local-level descriptions by grouping them for global-level descriptions with summarization-based LPMs. In the set of top-$k$ retrieved region descriptions for a given image, the top $1/4$ descriptions will be first picked up as a group according to the matching scores. Then, the grouped descriptions will 
eliminate the repeated words and be summarized into a single sentence by leveraging the text summarization ability of the model BART~\cite{lewis2019bart}. The generated pseudo sentences can prevent irrelevant information from appearing since words in region descriptions share a high confidence score. Instead of deprecating the rest descriptions with less confidence, we also propose to generate summarizations for them to leverage the diverse meaning within them, which enhances the pseudo sentences at the quantity level.  Specifically, a similarity score is calculated between each of the rest region descriptions and the first pseudo sentence. Next, these descriptions are grouped into several comprehensive summarization sentences based on scores (i.e., the top 1/3 for a group, the middle 1/3 for the second, and the last 1/3 for the third). In this way, descriptions sharing more similarities would be grouped together to avoid arranging too many objects in a single sentence generation process. The issue of grouping complex objects together will be discussed in Section~\ref{fluency}. According to this setting, our method can generate four high-quality pseudo sentences per image in the first stage.

% Figure environment removed

In the second stage, our main objective is to distil the crucial information from the preceding four sentences and utilize it to generate more appropriate pseudo sentences. To achieve this, we propose refining the sentences by leveraging the expressive power of other LPMs, specifically large-size generative models (LLaMA~\cite{touvron2023llama}). This enables us to produce sentences that are more fluent and contextually appropriate for supervision. Initially, we compare the four pseudo sentences with the given image, treating them as image-text pairs for training the captioner (as shown in the top part of Figure~\ref{fig4}). This process establishes a reconnection between the sentences and the visual content, enabling the captioner to generate predictions that possess a more accurate understanding across both the image and text domains (This process is different from CLIP-guided self-supervised training). Subsequently, we freeze the trained image captioner as a predictor and generate the prediction sentence based on the given image. However, since the captioner is supervised by the previous four sentences, it may tend to learn similar information among the four, potentially resulting in a lack of specific details within the context.

To address this limitation, we propose incorporating a large-size generative model, LLaMA-7B, to generate pseudo sentences with more detailed information. In our approach, we refine the sentences by using the predictions from the frozen captioner as well as the top-$k$ most relevant region descriptions. By combining these elements, LLaMA learns the core ideas from the predictions and incorporates the detailed information from the region descriptions. This integration enables us to generate superior pseudo sentences that encompass a greater level of detail. Consequently, we obtain a more appropriate sentence as our fifth output. With these two stages completed, we successfully generate a set of five pseudo sentences that are ready for further use.

% Figure environment removed

\subsection{Fluency Filter}
\label{fluency}
The fluency filter is designed to sift the generated sentences to remove the negative influence from the low-quality pseudo captions. For each given image, the filter carefully selects the best sentence among five pseudo sentences to ensure a precise match with the image. Figure~\ref{fig5} shows a comparison of two generated pseudo sentences from BART based on two groups of region descriptions in the first stage of the RaPSG module. The first case shows that the model successfully comprehends the relationship between the skateboard and the trick in the inference process. By contrast, the second sentence does not capture the important information to describe the image because the model recognizes the metallic-element different from the skateboard. Due to the limited discernment of LPMs, they might encounter confusion when varying appellations are referring to the same object within the region descriptions. This may lead to the generated sentence fragmenting into two or more semantic parts, impacting its coherence and accuracy. 

We propose to filter out the low-quality pseudo sentences via CIDEr metric~\cite{vedantam2015cider} (an image description evaluation based on human preference)
 because these low-quality pseudo sentences are also made up of highly relevant phrases but in an unnatural arrangement and can deceive the common evaluation methods. Since real annotations are unavailable, we use the model's predictions as references. To this end, we propose that the five pseudo sentences are examined by the CIDEr metric, and the one graded the highest is chosen as follows:
 \begin{equation}
\begin{aligned}
    D(h) &= \mathop{argmax}_{h=1}^5 CIDEr(S_h',P)\\
    & = \mathop{argmax}_{h=1}^5 \sum_{n=1}^{N}\mathcal{W}_{n}CIDEr_n(S_h',P),
  \label{eq3}
 \end{aligned}
\end{equation}
 where $S_h'$ is the $h$-th pseudo sentence among five, $P$ is the model prediction sentence, and $\mathcal{W}_n$ is the uniform weight for $n$-grams.  
 
\subsection{CLIP-Guided Training}
\label{contrastive}
The CLIP guidance module is proposed to encourage the sentence prediction to semantically match image content in CLIP embedding space as we abandon pre-training on external large-scale datasets. CLIP~\cite{radford2021learning} is a multimodal encoder trained on a large scale of image-text data and can be used to measure the similarity between a given text and an image. 

The InfoNCE~\cite{oord2018representation} is employed to reduce cross-modal information loss. The frozen image encoder CLIP-I and text encoder CLIP-T are used to embed a dozen original images and corresponding predictions into a representation space, respectively. Then the pairwise affinities are computed based on the encoded features. The learning process can be formulated as minimizing the contrastive information loss:
\begin{equation}
\begin{aligned}
    &L_I =-\text{log}\frac{\text{exp}(q\cdot k^+ / \tau)}{\text{exp}(q\cdot k^+ / \tau)+\sum\limits_{k^-}\text{exp}(q\cdot k^- / \tau)},\\
  \label{eq5}
 \end{aligned}
 \vspace{-0.3cm}
\end{equation}
where $q$ is a feature representation for an image (anchor) extracted from the CLIP-I. $k^+$ is the text feature for this image (positive key), and $k^-$ are text features for other images from the same batch in the training process (negative key). Both of them are generated by CLIP-T. $\tau$ is the temperature hyper-parameter.

\section{Experiments}
\subsection{Experiments Setting}
\paragraph{Datasets} We choose MSCOCO dataset~\cite{chen2015microsoft} and Flickr30k dataset~\cite{plummer2015flickr30k} with Karpathy~\cite{karpathy2015deep} split as our test benchmark. The MSCOCO images are divided into three parts: 113k images for training, 5k images for validation, and the remaining 5k images for testing. The Flickr30k images are divided into three parts: 29k images for training, 1k images for validation, and the remaining 1k images for testing. 

\begin{table*}[t]
\centering
\caption{The comparison of our approach with SOTA zero-shot models (directly from the paper) on MSCOCO and Flickr30k benchmark. We denote different captions (i.e., CTX, M$^2$, DIFNet, and DLCT) inside the brackets. Pseudo Sents. represents the generated pseudo sentences from the RaPSG module. Flamingo$_{large}^*$~\cite{yang2023re} is the model re-implemented on public available dataset since no official implementation of Flamingo is available which is based on large-scale in-house datasets and LPMs.}
\label{tab1}
\scalebox{0.9}{
\begin{threeparttable}
\begin{tabular}{lcc|cccc|cc}
    \toprule[0.5mm]
    \multirow{2}{*}{Model} & \multirow{2}{*}{Trainable params.} & \multirow{2}{*}{External dataset} & \multicolumn{4}{c}{MSCOCO Karpathy test} & \multicolumn{2}{c}{Flickr30k Karpathy test} \\
     & & & BLEU-4 & METEROR & CIDEr & SPICE & CIDEr & SPICE\\
    \midrule[0.3mm]
    SimVLM$_{base}$~\cite{wang2021simvlm} & - & \multirow{3}{*}{1.8B}  & 9.5 & 11.5 & 24.0 & 7.5 & - & - \\
    SimVLM$_{large}$~\cite{wang2021simvlm} & - &   & 10.5 & 12.0 & 24.9 & 8.3 & - & - \\
    SimVLM$_{huge}$~\cite{wang2021simvlm} &1.4B &   & 11.2 & 14.7 & 32.2 & 8.5 & - & - \\
    \midrule[0.3mm]
    Re-ViLM$_{base}$~\cite{yang2023re} & 158M & \multirow{3}{*}{762M} & 17.0 & - & 51.2 & - & 45.2 & 9.2\\
    Re-ViLM$_{medium}$~\cite{yang2023re} & 347M &  & 17.9 & - & 53.6 & - & 52.0 & 9.8 \\
    Re-ViLM$_{large}$~\cite{yang2023re} & 806M &  & 18.6 & - & 60.8 & - & 52.1 & 10.0 \\
    \midrule[0.3mm]
    Flamingo$_{large}^*$~\cite{yang2023re} & 489M & 762M & 16.5 & - & 49.2 & - & 46.4 & 9.4 \\
    Flamingo3B~\cite{alayrac2022flamingo} & 1.3B & \multirow{2}{*}{312M} & - & - & 73.0 & - & 60.6 & -\\
    Flamingo80B~\cite{alayrac2022flamingo} & 10B &  & - & - & 84.3 & - & 75.4 & -\\
    \midrule[0.3mm]
    Our Pseudo Sents. & 0 & \multirow{5}{*}{0.45M}  & 8.8 & 18.0 & 39.3 & 13.3 & 21.2 & 9.3\\
    Ours (w/ CTX) & 40M &   & 18.3 & 21.2  & 72.4 & 14.1 & 53.3 & 10.7\\
    Ours (w/ M$^2$) & 38M &  & 18.9 & 20.9  & 75.3 & 14.7 & 56.8 & 11.2\\
    Ours (w/ DLCT) & 63M &  & 19.4 & 21.1  & 75.9 & 14.5 & 58.4 & 11.5\\
    Ours (w/ DIFNet) & 33M &  & 19.3 & 21.4  & 78.1 & 14.9 & 59.1 & 11.8\\
    \bottomrule[0.5mm]
\end{tabular}
\end{threeparttable}}
\end{table*}

\begin{table*}[t]
  \centering
  \caption{The comparison of our method and other evaluation without fully supervision on MSCOCO benchmark.}
  \label{tab2}
  \scalebox{0.9}{
  \begin{tabular}{llcccccccc}
    \toprule[0.4mm]
    Category & Method & BLEU-1 & BLEU-4 & METEROR & ROUGE & CIDEr & SPICE \\
    \midrule[0.2mm]
    \multirow{3}{*}{Unsupervised Setting}&SME-GAN~\cite{laina2019towards} & - & 6.5 & 12.9 & 35.1 &  22.7 & 7.4 \\
    &UC-GAN~\cite{feng2019unsupervised} & 41.0 & 5.6 & 12.4 & 28.7 & 28.6 & 8.1\\
    &TSGAN~\cite{zhou2021triple} & 46.2 & 6.9 & 13.0 & 32.3 & 28.9 & 8.3\\
    &R$^2$M~\cite{guo2020recurrent} & 51.2 & 8.3 & 14.0 & 35.0 & 29.3 & 9.6\\
    &RWLSA~\cite{honda2021removing} & 50.2 & 6.8 & 14.1 & 34.8 & 32.9 & 8.8 \\
    \midrule[0.2mm]
    \multirow{4}{*}{Unpaired Setting} & Graph-Align~\cite{lu2017knowing} & 67.1 & 21.5 & 20.9 & 47.2 & 69.5 & 15.0 \\ 
    &SCS~\cite{ben2021unpaired} & 67.1 & 22.8 & 21.4 & 47.7 & 74.7 & 15.1 \\
    &Fine-Grained SRE~\cite{liu2021exploring} & 67.8 & 21.8 & 22.1 & 48.4 & 75.7 & 16.1 \\
    & PL-UIC~\cite{zhu2023prompt} & - & 25.0 & 22.6 & 49.4 & 77.9 & 15.2\\
    \midrule[0.2mm]
    \multirow{2}{*}{Weakly-supervised Setting}
    &SGCL~\cite{zhang2022look} &63.6 & 20.2 & 20.0 & 47.9 & 55.0 & 13.5  \\
    &WS-UIC~\cite{zhu2022unpaired} & -&21.5 & 20.1 & 45.8 & 65.7 & 13.6 \\
    \midrule[0.2mm]
    \multirow{4}{*}{LPMs + Retrieval-augmented PSG}
    & Ours (w/ CTX) & 67.0 & 18.3 & 21.2 & 44.9 & 72.4 & 14.1 \\
    & Ours (w/ M$^2$) & 67.5 & 18.9 & 20.9 & 45.5 & 75.3 & 14.7 \\
    & Ours (w/ DLCT) & 69.5 & 19.4 & 21.1 & 45.6 & 75.9 & 14.5 \\
    & Ours (w/ DIFNet) & 70.5 & 19.3 & 21.4 & 46.0 & 78.1 & 14.9 \\
    \bottomrule[0.4mm]
  \end{tabular}}
\end{table*}

\paragraph{Image caption backbones}Our approach is versatile for different image captioning models. To validate it, we incorporate our proposed framework with several SOTA captioners, including:
\begin{itemize}
    \item M$^2$ Transformer~\cite{cornia2020meshed}: it uses a mesh-like connectivity mechanism at the decoding stage to exploit multi-level features of images.  
    \item CTX Transformer~\cite{kuo2022beyond}: it adds an auxiliary input to represent missing information, which overcomes the drawback of captioning model conditioned only on the object detector's outputs.
    \item DLCT Transformer~\cite{luo2021dual}: it introduces a dual-level collaborative Transformer to utilize the complementary advantage between region features and grid features to improve captioning results.
    \item DIFNet Transformer~\cite{wu2022difnet}: it proposes a dual information flow network taking the segmented COCO images as another visual information to enhance the contribution of visual information for captioning.
\end{itemize}

\subsection{Comparison Against Zero-shot Evaluation}
According to the explanation in Section~\ref{intro},
we choose to compare our RaPSG approach with the zero-shot models on MSCOCO and Flickr30k benchmark since both of them are built up on LPMs. The results presented in Table~\ref{tab1} demonstrate the effectiveness of our method on the MSCOCO benchmark, surpassing the performance of SimVLM~\cite{wang2021simvlm}, Re-ViLM~\cite{yang2023re}, and Flamingo3B~\cite{alayrac2022flamingo} (note that we do not compare with REVEAL~\cite{hu2022reveal} as no official result of zero-shot setting is available). As indicated in Table~\ref{tab1}, previous approaches rely on pre-training with a large number of external image-text pairs and demand a considerable number of trainable parameters. For instance, Flamingo3B is pre-trained on 312M external image-text pairs, whereas our model only requires 0.45M generated pseudo sentences for the learning process, which is more data-efficient.

Additionally, Table~\ref{tab1} includes the comparison results on the Flicker30k benchmark, which demonstrates the robustness of our method across different datasets. It achieves comparable performance to SOTA models while maintaining efficiency in terms of model trainable parameters (e.g. $6.7\%$ of Flamingo, and $4\%$ of Re-ViLM).

\subsection{Comparsion Against Weakly-supervised Supervision}
Except the zero-shot setting, we also compare our method with other models that operate without full supervision, including unsupervised~\cite{laina2019towards,feng2019unsupervised,zhou2021triple,guo2020recurrent,honda2021removing}, unpaired\cite{lu2017knowing,ben2021unpaired,liu2021exploring,zhu2023prompt}, and weakly-supervised\cite{zhang2022look,zhu2022unpaired} approaches. Unsupervised and weakly-supervised methods retrieve sentences from mismatching corpora, while unpaired methods use the original corpora but each sentence does not pair with the corresponding images.
The experiments indicate that our method surpasses these data-efficient methods by utilizing the generated pseudo sentences instead of fetching complete sentences.


Table~\ref{tab2} provides a comprehensive comparison on the MSCOCO benchmark. The results highlight the effectiveness of our approach, as it outperforms all other types of settings, including unsupervised, unpaired, and weakly-supervised attempts. It is significant to note that our method even surpasses unpaired setting models that employ real images and real annotations but operate in an unpaired setting. This suggests that generating pseudo sentences hold greater potential than retrieving complete sentences.

% Figure environment removed
\begin{table}[t]
  \centering
  \caption{The comparison of different LPMs working on region descriptions in Retrieval-augmented PSG module on MSCOCO benchmark. Summ. Model represents the summarization models. Gener, Model represents the large pre-trained generative models.}
  \label{tab3}
  \scalebox{0.9}{
  \begin{tabular}{ccccccccccc}
    \toprule[0.4mm]
    Stage & Kind & Method & B1 & B4 & M & C  \\
    \midrule[0.2mm]
    - & Region descs. & CLIP  & 10.6 & 1.5 & 9.8& 18.3 \\
    \midrule[0.2mm]
    \multirow{3}{*}{One} & \multirow{3}{*}{Summ. Model} & T5~\cite{raffel2020exploring} & 35.3 & 3.8 & 13.3 & 19.5 \\
    & & GPT2~\cite{radford2019language} & 38.7 & 5.0 & 12.4 & 23.5\\
    & & BART\cite{lewis2019bart} & 45.9 & 6.4 & 15.9 & 37.2\\
    \midrule[0.2mm]
    \multirow{3}{*}{Two} & \multirow{3}{*}{Gener. Model} & ChatGPT~\cite{openai} & 38.1 & 4.5 & 15.8 & 29.5 \\
    & & Openchatkit~\cite{openchatkit} & 44.5 & 9.6 & 14.1 & 36.3\\
    & & LLaMA-7B~\cite{touvron2023llama} & 48.1 & 8.8 & 18.0 & 39.3\\
    \bottomrule[0.4mm]
  \end{tabular}}
\end{table}

\vspace{-0.2cm}
\subsection{Ablation Study}
In this section, extensive experiments have been conducted to indicate how our proposed method improves the quality of pseudo sentences. Additionally, the contribution of each designed objective has been quantified in this part. Finally, the paper's core concept of selecting region descriptions instead of complete sentences has been explained.

\begin{table}[h]
  \centering
  \caption{Different performance of top-k setting with the model BART on MSCOCO dataset.}
  \label{tab4}
  \scalebox{0.9}{
  \begin{tabular}{lccccccccc}
    \toprule[0.4mm]
    Metric & k=4 & k=8 & k=12 & k=16 & k=20 & k=24  \\
    \midrule[0.2mm]
    B4 & 16.73 & 16.42 & 16.65 & 16.78 & 16.71 & 16.58\\
    M & 19.51 & 19.80 & 19.82 & 19.76 & 19.74 & 19.66\\
    C & 67.81 & 69.05 & 69.01 & 69.14 & 69.03 & 68.79\\
    \bottomrule[0.4mm]
  \end{tabular}}
  \vspace{-0.3cm}
\end{table}

\begin{table}[h]
  \centering
  \caption{The detailed information of five generated pseudo sentences on MSCOCO benchmark}
  \label{tab5}
  \scalebox{0.9}{
  \begin{tabular}{lccccccccc}
    \toprule[0.4mm]
    Kind & B1 & B4 & M & R & C & S \\
    \midrule[0.2mm]
    1$_{st}$ Pseu. Sent. &45.1 & 5.9 & 15.6 & 30
    .0 & 34.7 & 10.1\\
    2$_{nd}$ Pseu. Sent. & 45.9 & 6.4 &15.9 & 30.7 & 37.2 & 10.4 \\
    3$_{rd}$ Pseu. Sent. & 44.6 & 5.5 & 15.5 & 29.5 & 34.2 & 9.9\\
    4$_{th}$ Pseu. Sent. & 41.6 & 4.7 & 14.4 & 27.6 & 28.9 & 8.8\\
    5$_{th}$ Pseu. Sent. & 48.1 & 8.8 & 18.0 & 33.8 & 39.3 & 13.3\\
    \bottomrule[0.4mm]
  \end{tabular}}
  \vspace{-0.3cm}
\end{table}


\paragraph{Quality}We investigate here how the quality of the generated pseudo sentence influences the overall performance. In the initial stage, 
Figure~\ref{fig6} displays the quality of pseudo sentences generated with different numbers of region descriptions per group on MSCOCO dataset.
Table~\ref{tab3} shows the comparison of generated pseudo sentences' quality based on different models, including T5~\cite{raffel2020exploring}, GPT2~\cite{radford2019language}, BART~\cite{lewis2019bart}, ChatGPT~\cite{openai}, and so on. Table~\ref{tab4} presents the different choices of top-k on MSCOCO dataset.
According to the above results, we choose to fetch the top-16 region descriptions from the VG dataset and use BART to summarize each semantic group with 4 descriptions in the first stage. In the second stage, we test the performance of different large-scale generative models on refining sentences as shown in Table~\ref{tab3}. According to the comparison, we decide to use the LLaMA-7B to enhance the prediction. Lastly, Table~\ref{tab5} provides a comprehensive comparison of the quality of the five generated pseudo sentences.





\paragraph{Contribution}We also investigate the contribution of each designed module, as shown in Table~\ref{tab6}. The RaPSG module is the key component to improving the model performance. The fluency filter is designed to filter out the unnatural sentences among pseudo sentence generation and leave the best one matching the given image. It can also significantly improve the prediction accuracy. Figure~\ref{fig7} shows two cases where the fluency filter picked up the best pseudo sentence based on its CIDEr score. We introduce CLIP guidance in the retrieval-augmented learning process, which drives the predicted caption to be semantically consistent with the given image by shrinking the cross-modal distance in the feature embedding
space. For comparison, we replace this module with Cho's CLIP reward~\cite{cho2022fine} and it shows our CLIP guidance yields better results.

\begin{table}[t]
  \centering
  \caption{Ablation study of different proposed modules conducted on the DLCT. ``RD'' represents the original region descriptions. ``PS'' means the generated pseudo sentences. ``D'' is the DIFNet model. ``\cite{cho2022fine}" represents training with Cho's CLIP reward instead of our CLIP guidance module.}
  \label{tab6}
  \scalebox{0.9}{
  \begin{tabular}{lccccccccc}
    \toprule[0.4mm]
    Module & B1 & B4 & M & R & C & S \\
    \midrule[0.2mm]
    RD &10.6 & 1.5 & 9.8 & 19.3 & 18.3 &7.5\\
    PS & 48.1 & 8.8 &18.0 & 33.8 & 39.3 & 13.3 \\
    PS+FF & 59.4 & 15.2 & 20.2 & 39.6 & 56.0 & 13.9 \\
    D+PS & 67.9 & 16.9 & 20.3 & 43.9 & 70.2 & 13.7\\
    D+PS+FF & 70.3 & 19.1 & 21.1 & 45.9 & 76.9 & 14.7\\
    D+PS+FF+~\cite{cho2022fine} & 67.6 & 17.0 & 19.9 & 44.4 & 69.4 & 13.3\\
    D+PS+FF+CG & 70.5 & 19.3 & 21.4 & 46.0 & 78.1 & 14.9\\
    \bottomrule[0.4mm]
  \end{tabular}}
  \vspace{-0.3cm}
\end{table}

% Figure environment removed

\paragraph{Region Descriptions VS Complete Sentences}
In the experiments of our main paper, we choose the region descriptions from the Visual Genome dataset (VG)~\cite{krishna2017visual} as the text source. In this section, we discuss whether retrieving whole sentences from corpora instead of region descriptions will be a better choice. We pick up another popular corpus, named Google Concept Caption (GCC)~\cite{sharma2018conceptual}, to verify our selection. GCC is a popular text source for general scenes with whole sentences descriptions, which is used in most unsupervised image captioning works~\cite{laina2019towards,guo2020recurrent,honda2021removing,zhou2021triple}. For a fair comparison, we use the pre-trained CLIP~\cite{radford2021learning} to fetch individual top-$16$ most relevant descriptions from two corpora and then generate pseudo sentences, respectively.

\begin{table}[t]
  \centering
  \caption{The comparison result of how corpora affect the generated pseudo sentence quality.}
  \label{tab8}
  \scalebox{0.9}{
  \begin{tabular}{clcccccccc}
    \toprule[0.4mm]
    Source& Kind & B1 & B4 & M & R & C & S \\
    \midrule[0.2mm]
    \multirow{2}{*}{VG} & Region Desc. & 10.6 & 1.5 & 9.8 & 19.3 & 18.3 & 7.5 \\
    & Pseu. Sent. & 48.1 & 8.8 & 18.0 & 33.8 & 39.3 & 13.3 \\
    \midrule[0.2mm]
    \multirow{2}{*}{GCC} & Whole Sent. & 36.6 & 4.7 & 13.1 & 26.9 & 24.9 & 8.8 \\
    & Pseu. Sent. & 35.3 & 4.5 & 14.0 & 27.0 & 21.1 & 8.9 \\
    \bottomrule[0.4mm]
  \end{tabular}}
  \vspace{-0.2cm}
\end{table}

% Figure environment removed

% Figure environment removed

According to the results shown in Table~\ref{tab8}, it is obvious that VG-based pseudo sentences present better performance than GCC-based ones on all the metrics. Figure~\ref{fig8} displays an example of two generated pseudo sentences and a real human annotation. We can observe that the descriptions from the GCC dataset have many words or phrases mismatched with the given image. This low-relevance information cannot be distinguished from the valuable information by the LPMs, which misleads the sentence meaning in the generation process. Additionally, containing too much irrelevant information could make the sentence abnormally long, which would further drop the performance of sentence predictions under all metrics~\cite{feng2019unsupervised}.

\begin{table}[h]
\centering
\caption{Performance comparison with SOTA LPM-based method on $1\%$ semi-supervised MSCOCO captioning task.}
  \label{tab9}
\scalebox{1.0}{
  \begin{tabular}{lcccccccccc}
    \toprule[0.5mm]
    Model & B1  & B4 & M & R & C & S  \\
    \midrule[0.3mm]
    Self Distillation~\cite{chen2021self}  &67.9  & 25.0 & 21.7 & 49.3 & 73.0 & 14.5\\
    OSCAR~\cite{li2020oscar}  & 67.2  & 23.3 & 22.5 & 49.1 & 78.4 & -\\
    VisualGPT~\cite{chen2022visualgpt}  & 69.5  & 25.6 & 22.6 & 49.6 & 80.9 & -\\
    P$^3$~\cite{jain2021perturb} &68.8  & 27.5 & 23.4 & 51.0 & 84.5 & 16.1\\
    \midrule[0.3mm]
    M$^2$~\cite{cornia2020meshed} &67.4 &22.8 &21.4 &48.2 & 70.9 &14.8\\
    M$^2$+ours &  68.7   & 23.3 & 22.1 & 49.5 & 83.1 & 15.8\\

    DLCT~\cite{luo2021dual} &68.0  & 24.4 & 21.3 & 48.8 &74.2 &14.3\\
    DLCT+ours &  72.4  & 27.1 & 23.1 & 51.5 & 90.8 & 16.5\\

    CTX~\cite{kuo2022beyond} & 71.6  &26.4 &23.2 &50.8 &85.4 &16.7\\
    CTX+ours &  72.4   & 27.7 & 23.5 & 51.5 & 90.8 & 17.1\\
    
    DIFNet~\cite{wu2022difnet} & 70.8  & 24.9 & 22.2 & 49.7 & 81.3 & 15.3 \\
    DIFNet+ours & 73.5  & 27.7  & 23.1 & 51.8 & 93.4 & 16.7 \\
    \bottomrule[0.5mm]
  \end{tabular}}
  \vspace{-0.3cm}
\end{table}


\subsection{Extension on Semi-supervised Image Captioning Benchmarks}
Since our approach works well in zero-shot and  unsupervised settings, we also test whether it can deal with the data scarcity problem in a semi-supervised setting where only partial images have the corresponding text annotations, while other images are unlabeled. Specifically, we follow the existing semi-supervised image captioning benchmark~\cite{chen2021self}. The proposed RaPSG is firstly optimized on the $99\%$ images without caption labels. Then the model is further finetuned on the rest of $1\%$ labeled data. We repeat the experiments under 3 different selections of the $1\%$ labeled samples and calculate the average performance as output.

As shown in Table~\ref{tab9}, compared with current approaches, our approach achieves a performance gain with 93.4 (+8.9) CIDEr score. It means our generated pseudo sentences can remit the demand for annotation in semi-supervised captioning tasks. We also note the remarkable few-shot capability of Flamingo on image captioning. Specifically, the Flamingo fine-tuned with 32 cases can outperform our approach. However, considering Flamingo has 400X more trainable parameters and 312M external image-text pairs, our method is acceptable in specific scenarios where efficiency is highly demanded.


\subsection{Qualitative Results} 
To highlight the captioning ability of our approach, we present some qualitative results of our generated pseudo sentences and predictions in Figure~\ref{fig9}. It can be noticed that our captioning predictions have better quality under all metrics compared with region descriptions and pseudo sentences. However, some of the examples do not make sense from a human view though their CIDEr scores are fine (e.g., ``\textit{kite flying in a park is fun.}"). The correct sentence would be like ``\textit{flying a kite in a park is fun.}". One potential reason could be the pre-trained text summarization BART cannot deal with a batch of similar objects in the composition process. It tends to disarrange complex relationships among different region descriptions.



\section{Conclusion}
In this work, we make the first attempt to present a novel retrieval-augmented pseudo sentence generation method, named RaPSG, for image captioning. It can leverage the prior knowledge from the frozen LPMs with a data-efficient method. The generated sentences can avoid the appearance of irrelevant words or phrases in pseudo sentences and keep the diversity of pseudo references, which is attributed to the innovative combination of ranking, grouping, and summarization in our design. In addition, we design a fluency filter to sift the generated sentences and a CLIP guidance module to make the predicted captions semantically consistent with the given image. The idea of ``LPMs + retrieval-augmented learning" could be a new strategy to replace large-scale pre-training due to the computationally efficient. In our extensive experiments, we show a comparable result with SOTA zero-shot models and achieve SOTA performance on semi-supervised image captioning benchmark. In the future, we are interested in improving our proposed method on image captioning with few-shot labeled cases. We also plan to improve the grammar of our generated pseudo sentences to make them more natural for humans. \vspace{-0.2cm}


\begin{thebibliography}{1}
\bibliographystyle{IEEEtran}

\bibitem{cornia2020meshed}
M. Cornia, M. Stefanini, L. Baraldi, and R. Cucchiara, ``Meshed-memory transformer for image captioning," in {\it{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}}, 2020, pp. 10578-10587.

\bibitem{hu2022tsfnet}
H. Nannan, Y. Ming, C. Fan, F. Feng, and B. Lyu, ``TSFNet: Triple-steam image captioning," {\it{IEEE Transactions on Multimedia}}, 2022.

\bibitem{ji2022multi}
J. Ji, X. Huang, X. Sun, Y. Zhou, G. Luo, L. Cao, J. Liu, L. Shao, and R. Ji, ``Multi-Branch Distance-Sensitive Self-Attention Network for Image Captioning," {\it{IEEE Transactions on Multimedia}}, 2022. 

\bibitem{wang2022text}
D. Wang, Z. Hu, Y. Zhou, R. Hong, and M. Wang, ``A text-guided generation and refinement model for image captioning," {\it{IEEE Transactions on Multimedia}}, 2022.

\bibitem{kuo2022beyond}
C. Kuo, and Z. Kira, ``Beyond a pre-trained object detector: Cross-modal textual and visual context for image captioning," in {\it{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}}, 2022, pp. 17969-17979.

\bibitem{luo2021dual}
Y. Luo, J. Ji, X. Sun, L. Cao, Y. Wu, F. Huang, C. Lin, and R. Ji, ``Dual-level collaborative transformer for image captioning," in {\it{Proceedings of the AAAI conference on Artificial Intelligence}}, vol. 35, no. 3, 2021, pp. 2286-2293.

\bibitem{li2020oscar}
X. Li, X. Yin, C. Li, P. Zhang, X. Hu, L. Zhang, L. Wang, H. Hu, L. Dong, F. Wei {\it{et al.}}, ``Oscar: Object-semantic aligned pre-training for vision-language tasks," in {\it{Proceedings of the European Conference on Computer Vision}}. Springer, 2020, pp. 121-137.

\bibitem{wang2022ofa}
P. Wang, A. Yang, R. Men, J. Lin, S. Bai, Z. Li, J. Ma, C. Zhou, J. Zhou, and H. Yang, ``Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework," {\it{arXiv preprint arXiv: 2202.03052}}, 2022.

\bibitem{wang2021simvlm}
Z. Wang, J. Yu, A. Yu, Z. Dai, Y. Tsvetkov, and Y. Cao, ``Simvlm: Simple visual language model pretraining with weak supervision," {\it{arXiv preprint arXiv:2108.10904}}, 2021.

\bibitem{kakaobrain2022coyo}
M. Byeon, B. Park, H. Kim, S. Lee, W. Baek, and S. Kim, {\it{Coyo-700m: Image-text pair dataset.}} [Online]. Available: https://github.com/kakaobrain/coyo-dataset.

\bibitem{li2022blip}
J. Li, D. Li, C. Xiong, and S. Hoi, ``Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation," in {\it{International Conference on Machine Learning}}. PMLR, 2022, pp. 12888-12900.

\bibitem{krishna2017visual}
R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen, Y. Kalantidis, L. Li, D. Shamma {\it{et al}}., ``Visual genome: Connecting language and vision using crowdsourced dense image annotations," {\it{International Journal of Computer Vision}}. 123, pp. 32-73.

\bibitem{alayrac2022flamingo}
J. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds {\it{et al}}., ``Flamingo: a visual language model for few-shot learning," {\it{arXiv preprint arXiv:2204.14198}}, 2022.

\bibitem{yang2023re}
Z. Yang, W. Ping, Z. Liu, V. Korthikanti, W. Nie, D. Huang, L. Fan, Z. Yu, S. Lan, B. Li {\it{et al}}., ``Re-vilm: Retrieval-augmented visual language model for zero and few-shot image captioning," {\it{arXiv preprint arXiv:2302.04858}}, 2023.

\bibitem{radford2021learning}
A. Radford, J. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark {\it{et al}}., ``Learning transferable visual models from natural language supervision," in {\it{International Conference on Machine Learning}}. PMLR, 2021, pp. 8748-8763.

\bibitem{li2023blip}
J. Li, D. Li, S. Savarese, and S. Hoi, ``Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models," {\it{arXiv preprint arXiv:2301.12597}}, 2023.


\bibitem{lewis2019bart}
M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, and L. Zettlemoyer, ``Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension," {\it{arXiv preprint arXiv:1910.13461}}, 2019.

\bibitem{touvron2023llama}
H. Touvron, T. Lavril, G. Izacard, X. Martinet, M. Lachaux, T. Lacroix {\it{et al}}., ``Llama: Open and efficient foundation language models," {\it{arXiv preprint arXiv:2302.13971}}, 2023.

\bibitem{sharma2018conceptual}
P. Sharma, N. Ding, S. Goodman, and R. Soricut, ``Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning," in {\it{Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}}, 2018, pp. 2556--2565.

\bibitem{feng2019unsupervised}
Y. Feng, L. Ma, W.liu, and J. Luo, ``Unsupervised image captioning," in {\it{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}}, 2019, pp. 4125-4134.

\bibitem{laina2019towards}
I. Laina, C. Rupprecht, and N. Navab, ``Towards unsupervised image captioning with shared multimodal embeddings," in {\it{Proceedings of the IEEE/CVF International Conference on Computer Vision}}, 2019, pp. 7414-7424.

\bibitem{zhu2023prompt}
P. Zhu, X. Wang, L. Zhu, Z. Sun, W. Zheng, Y. Wang, and C. Chen, ``Prompt-based learning for unpaired image captioning," {\it{IEEE Transactions on Multimedia}}, 2023.

\bibitem{dosovitskiy2020image}
A. Dosovitskiy, L. Beyer, A. Kolensnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani {\it{et al}}., ``An image is worth 16x16 words: Transformers for image recognition at scale," {\it{arXiv preprint arXiv:2010.11929}}, 2020.

\bibitem{radford2019language}
A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, ``Language models are unsupervised multitask learners," {\it{OpenAI blog}} 1, no. 8, 2019.

\bibitem{cho2022fine}
J. Cho, S. Yoon, A. Kale, F. Dernoncourt, T. Bui, and M. Bansal, ``Fine-grained image captioning with clip reward," {\it{arXiv preprint
arXiv:2205.13115}}, 2022.

\bibitem{chen2022visualgpt}
J. Chen, H. Guo, K. Yi, B. Li, and M. Elhoseiny, ``Visualgpt: Data-efficient adaptation of pretrained language models for image captioning," in {\it{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}}, 2022, pp. 18030--18040.

\bibitem{kumar2022imagecaptioning}
A. Kumar, ``The Illustrated Image Captioning using transformers," {\it{ankur3107.github.io}}, 2022.

\bibitem{xie2022visual}
Y. Xie, L. Zhou, X. Dai, L. Yuan, N. Bach, C. Liu, and M. Zeng, ``Visual clues: bridging vision and language foundations for image paragraph captioning," {\it{arXiv preprint arXiv:2206.01843}}, 2022.

\bibitem{brock2021high}
A. Brock, S. De, S. Smith, and K. Simonyan, ``High-performance large-scale image recognition without normalization," in {\it{International Conference on Machine Learning}}, 2021, pp. 1059--1071.

\bibitem{hoffmann2022training}
J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford {\it{et al}}., ``Training compute-optimal large language models," {\it{arXiv preprint arXiv:2203.15556}}, 2022.

\bibitem{lin2022retrieval}
W. Lin, and B. Byrne, ``Retrieval Augmented Visual Question Answering with Outside Knowledge," {\it{arXiv preprint arXiv:2210.03809}}, 2022.

\bibitem{qu2021passage}
C. Qu, H. Zamani, L. Yang, W. Croft, and E. Learned-Miller, ``Passage retrieval for outside-knowledge visual question answering," in {\it{Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval}}, 2021, pp. 1753--1757.

\bibitem{chen2022re}
W. Chen, H. Hu, C. Saharia, and W. Cohen, ``Re-imagen: Retrieval-augmented text-to-image generator," {\it{arXiv preprint arXiv:2209.14491}}, 2022.

\bibitem{hu2022reveal}
Z. Hu, A. Iscen, C. Sun, Z. Wang, K. Chang, Y. Sun, C. Schmid, D. Ross, and A. Fathi, ``REVEAL: Retrieval-Augmented Visual-Language Pre-Training with Multi-Source Multimodal Knowledge Memory," {\it{arXiv preprint arXiv:2212.05221}}, 2022.

\bibitem{ramos2023smallcap}
R. Rita, M. Bruno, E. Desmond, and K. Yova, ``SmallCap: lightweight image captioning prompted with retrieval augmentation," {\it{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}}, 2023, pp. 2840--2849.

\bibitem{ramos2023retrieval}
R. Rita, E. Desmond, and M. Bruno, ``Retrieval-augmented image captioning," {\it{arXiv preprint arXiv:2302.08268}}, 2023.

\bibitem{changpinyo2021conceptual}
S. Changpinyo, P. Sharma, N. Ding, and R. Soricut, ``Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts," in {\it{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}}, 2021, pp. 3558--3568.

\bibitem{reimers2019sentence}
N. Reimers, and I. Gurevych, ``Sentence-BERT: Sentence embeddings using siamese bert-networks," {\it{arXiv preprint arXiv:1908.10084}}, 2019.

\bibitem{vedantam2015cider}
R. Vedantam, Z. Lawrence, and D. Parikh, ``Cider: Consensus-based image description evaluation," in {\it{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}}, 2015, pp. 4566--4575.

\bibitem{oord2018representation}
A. Oord, Y. Li, and O. Vinyals, ``Representation learning with contrastive predictive coding," {\it{arXiv preprint arXiv:1807.03748}}, 2018.

\bibitem{chen2015microsoft}
X. Chen, H. Fang, T. Lin, R. Vedantam, S. Gupta, P. Doll{\'a}r, and C. Zitnick, ``Microsoft coco captions: Data collection and evaluation server," {\it{arXiv preprint arXiv:1504.00325}}, 2015.

\bibitem{plummer2015flickr30k}
B. Plummer, L. Wang, C. Cervantes, J. Caicedo, J. Hockenmaier, and S. Lazebnik, ``Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models," in {\it{Proceedings of the IEEE international conference on computer vision}}, 2015, 2641--2649.

\bibitem{karpathy2015deep}
A. Karpathy, and F. Li, ``Deep visual-semantic alignments for generating image descriptions," in {\it{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}}, 2015, pp. 3128--3137.

\bibitem{wu2022difnet}
M. Wu, X. Zhang, X. Sun, Y. Zhou, C. Chen, J. Gu, X. Sun, and R. Ji, ``DIFNet: Boosting Visual Information Flow for Image Captioning," in {\it{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}}, 2022, pp. 18020--18029.

\bibitem{zhou2020unified}
L. Zhou, H. Palangi, L. Zhang, H. Hu, J. Corso, and J. Gao, ``Unified vision-language pre-training for image captioning and vqa," in {\it{Proceedings of the AAAI conference on artificial intelligence}}, 2020, pp. 13041--13049.

\bibitem{zhou2021triple}
Y. Zhou, W. Tao, and W. Zhang, ``Triple sequence generative adversarial nets for unsupervised image captioning," in {\it{ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}}, 2021, pp. 7598--7602.

\bibitem{guo2020recurrent}
D. Guo, Y. Wang, P. Song, and M. Wang, ``Recurrent relational memory network for unsupervised image captioning," {\it{arXiv preprint arXiv:2006.13611}}, 2020.

\bibitem{honda2021removing}
U. Honda, Y. Ushiku, A. Hashimoto, T. Watanabe, and Y. Matsumoto, ``Removing word-level spurious alignment between images and pseudo-captions in unsupervised image captioning," {\it{arXiv preprint arXiv:2104.13872}}, 2021.

\bibitem{lu2017knowing}
J. Lu, C. Xiong, D. Parikh, and R. Socher, ``Knowing when to look: Adaptive attention via a visual sentinel for image captioning," in {\it{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}}, 2017, pp. 375--383.

\bibitem{ben2021unpaired}
H. Ben, Y. Pan, Y. Li, T. Yao, R. Hong, M. Wang, and T. Mei, ``Unpaired image captioning with semantic-constrained self-learning," {\it{IEEE Transactions on Multimedia}}, 2021, 24, pp. 904--916.

\bibitem{liu2021exploring}
F. Liu, M. Gao, T. Zhang, and Y. Zou, ``Exploring semantic relationships for unpaired image captioning," {\it{arXiv preprint arXiv:2106.10658}}, 2021.

\bibitem{zhang2022look}
C. Zhang, C. Huang, Y. Li, X. Zhang, Y. Ye, and C. Zhang, ``Look Twice as Much as You Say: Scene Graph Contrastive Learning for Self-Supervised Image Caption Generation," in {\it{Proceedings of the 31st ACM International Conference on Information \& Knowledge Management}}, 2022, pp. 2519--2528.

\bibitem{zhu2022unpaired}
P. Zhu, X. Wang, Y. Luo, Z. Sun, W. Zheng, Y. Wang, and C. Chen, ``Unpaired Image Captioning by Image-level Weakly-Supervised Visual Concept Recognition," {\it{arXiv preprint arXiv:2203.03195}}, 2022.

\bibitem{raffel2020exploring}
C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, P. Liu, ``Exploring the limits of transfer learning with a unified text-to-text transformer," {\it{The Journal of Machine Learning Research}}, 2020, 21, pp. 5485--5551.

\bibitem{openai}
OpenAI. (2023). ChatGPT (Feb 13 version) [Large language model]. https://chat.openai.com

\bibitem{openchatkit}
Together Computer, ``OpenChatKit: An Open Toolkit and Base Model for Dialogue-style Applications," https://github.com/togethercomputer/OpenChatKit, 2023.


\bibitem{chen2021self}
X. Chen, M. Jiang, and Q. Zhao, ``Self-distillation for few-shot image captioning," in {\it{Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision}}, 2021, pps. 545--555.

\bibitem{jain2021perturb}
A. Jain, P. Samala, P. Jyothi, D. Mittal, and M. Singh, ``Perturb, Predict \& Paraphrase: Semi-Supervised Learning using Noisy Student for Image Captioning.", {\it{IJCAI}}, 2021, pps. 758--764.



\end{thebibliography}


% \newpage

% \section{Biography Section}
% If you have an EPS/PDF photo (graphicx package needed), extra braces are
%  needed around the contents of the optional argument to biography to prevent
%  the LaTeX parser from getting confused when it sees the complicated
%  $\backslash${\tt{includegraphics}} command within an optional argument. (You can create
%  your own custom macro containing the $\backslash${\tt{includegraphics}} command to make things
%  simpler here.)
 
% \vspace{11pt}

% \bf{If you include a photo:}\vspace{-33pt}
% \begin{IEEEbiography}[{% Figure removed}]{Michael Shell}
% Use $\backslash${\tt{begin\{IEEEbiography\}}} and then for the 1st argument use $\backslash${\tt{includegraphics}} to declare and link the author photo.
% Use the author name as the 3rd argument followed by the biography text.
% \end{IEEEbiography}

% \vspace{11pt}

% \bf{If you will not include a photo:}\vspace{-33pt}
% \begin{IEEEbiographynophoto}{John Doe}
% Use $\backslash${\tt{begin\{IEEEbiographynophoto\}}} and the author name as the argument followed by the biography text.
% \end{IEEEbiographynophoto}




\vfill

\end{document}


