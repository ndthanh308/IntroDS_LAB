
\documentclass{article} %
\usepackage{iclr2023_conference,times}

\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\renewcommand{\topfraction}{0.75}
\renewcommand{\textfraction}{0.15}

\input{header}
\input{commands}



\title{\centering Benchmarking Offline Reinforcement\\ Learning on Real-Robot Hardware}



\usepackage{authblk}
\author[1]{\bf Nico Gürtler}
\author[1]{\bf Sebastian Blaes}
\author[1]{\bf Pavel Kolev}
\author[1]{\bf Felix Widmaier}
\author[2]{\bf Manuel Wüthrich}
\author[3]{\authorcr\bf Stefan Bauer} %
\author[1]{\bf Bernhard Schölkopf\,}
\author[1]{\bf Georg Martius}
\affil[1]{Max Planck Institute for Intelligent Systems\thanks{Correspondence to \texttt{nico.guertler@tuebingen.mpg.de}}}
\affil[2]{Harvard University}
\affil[3]{KTH Stockholm}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy %
\begin{document}


\maketitle

\begin{abstract}
Learning policies from previously recorded data is a promising direction for real-world robotics tasks, as online learning is often infeasible.
Dexterous manipulation in particular remains an open problem in its general form. The combination of offline reinforcement learning with large diverse datasets, however, has the potential to lead to a breakthrough in this challenging domain analogously to the rapid progress made in supervised learning in recent years.
To coordinate the efforts of the research community toward tackling this problem,
we propose a benchmark including:
i) a large collection of data for offline learning from a dexterous manipulation platform
on two tasks, obtained with capable RL agents trained in simulation;
ii) the option to execute learned policies on a real-world robotic system
and a simulation for efficient debugging.
We evaluate prominent open-sourced offline reinforcement learning algorithms on the datasets
and provide a reproducible experimental setup for offline reinforcement learning on real systems.
Visit \url{https://sites.google.com/view/benchmarking-offline-rl-real} for more details.
\end{abstract}

\section{Introduction}

Reinforcement learning (RL)~\citep{Sutton1998-ql} holds great potential for robotic manipulation and other real-world decision-making problems as it can solve tasks autonomously by learning from interactions with the environment.
When data can be collected during learning, RL in combination with high-capacity function approximators can solve challenging high-dimensional problems~\citep{Mnih2015-rj,Lillicrap2015:ddpg,silver2017mastering,berner2019dota}.
However, in many cases online learning is not feasible because collecting a large amount of experience with a partially trained policy is either prohibitively expensive or unsafe~\citep{Dulac2020empiricalRL}.
Examples include autonomous driving, where suboptimal policies can lead to accidents,
robotic applications where the hardware is likely to get damaged without additional safety mechanisms,
and collaborative robotic scenarios where humans are at risk of being harmed.


Offline reinforcement learning (offline RL or batch RL) \citep{lange2012batch} tackles this problem by learning a policy from prerecorded data generated by experts or handcrafted controllers respecting the system's constraints.
Independently of how the data is collected, it is essential to make the best possible use of it and to
design algorithms that improve performance with the increase of available data.
This property has led to unexpected generalization in computer vision~\citep{Krizhevsky2012-qg,he2016deep,redmon2016you} and natural language tasks~\citep{floridi2020gpt,devlin2018bert} when massive datasets are employed.
With the motivation to learn similarly capable decision-making systems from data, the field of offline RL has gained considerable attention.
Progress is currently measured by benchmarking algorithms on simulated domains, both in terms of data collection and evaluation.

Yet, real-world data differs from simulated data qualitatively and quantitatively in several aspects~\citep{Dulac2020empiricalRL}.
First, observations are noisy and sometimes faulty.
Second, real-world systems introduce delays in the sensor readings and often have different sampling rates for different modalities.
Third, the action execution can also be delayed and can get quantized by low-level hardware constraints.
Fourth, real-world environments are rarely stationary.
For instance, in autonomous robotics, battery voltages might drop, leading to reduced motor torques for the same control command. Similarly, thermal effects change sensor readings and motor responses.
Abrasion changes friction behavior and dust particles can change object appearances and sensing in general.
Fifth, contacts are crucial for robotic manipulation but are only insufficiently modeled in current physics simulations, in particular for soft materials.
Lastly, physical robots have individual variations.

Since real-world data is different from simulated data, it is important to put offline RL algorithms to the test on real systems.
We propose challenging robotic manipulation datasets recorded on real robots for two
 tasks: object pushing and object lifting with reorientation on the \Trifinger platform \citep{trifinger}.
To study the differences between real and simulated environments, %
 we also provide datasets collected in simulation.
Our benchmark of state-of-the-art offline RL algorithms on these datasets reveals that they are able to solve the moderately difficult pushing task while their performance on the more challenging lifting task leaves room for improvement.
In particular, there is a much larger gap between the performance of the expert policy and offline-learned policies on the real system compared to the simulated system. This underpins the importance of real-world benchmarks for offline RL.
We furthermore study the impact of adding suboptimal trajectories to expert data and find that all algorithms are `distracted' by them, \ie, their success rate drops significantly. This identifies an important open challenge for the offline RL community: robustness to suboptimal trajectories.\looseness-1

Importantly, a cluster of TriFinger robots is set up for evaluation of offline-learned policies for which remote access can be requested for research purposes. With our dataset and evaluation platform, we therefore aim to provide a breeding ground for future offline RL algorithms.

\section{The TriFinger Platform} \label{sec:platform}
\vspace{-1em}
% Figure environment removed

We use a robot cluster
that was initially developed and build for the \emph{Real Robot Challenge} in 2020 and 2021 \citep{rrc2020_and_2021}. The robots that constitute the cluster are an industrial-grade adaptation of a robotic platform called \Trifinger, an open-source hardware and software design introduced in \citet{trifinger}, see \figref{fig:trifinger}.%
The robots have three arms mounted at a 120 degrees radially symmetric arrangement with 3 degrees of freedom (DoF) each.
The arms are actuated by outrunner brushless motors with a 1:9 belt-drive, yielding high agility, low friction, and good force feedback (details on the actuator modules can be found in \citet{Grimminger2020-tl}).
Pressure sensors inside the elastic fingertips provide basic tactile feedback.
The working area, where objects can be manipulated, is encapsulated by a high barrier to ensure that the object stays inside the arena even during aggressive motions. This is essential for operation without human supervision.
The robot is inside a closed housing that is well lit by top-mounted LED panels making the images taken by three high-speed global shutter cameras consistent.
The cameras are distributed between the arms to ensure objects are always seen by any camera. We study dexterous manipulation of a cube whose pose is estimated by a visual tracking system with 10\,Hz.


To abstract away the low-level details, we developed a software with a simple Gym \citep{brockman2016openai} interface in Python that can interact with  the robots  at a maximal rate of 1\,kHz in position control or torque control mode (see \citep{trifinger} for details).
We use a control frequency of 50\,Hz and torque control for this work.
We have a custom object tracking tool to provide position and orientation of a single colored cube in the environment, allowing algorithms to work without visual input.\looseness-1

On top of this interface, we have developed a submission system that allows users to submit \emph{jobs} to a cluster of these robots for unattended remote execution.
This setup was specifically adapted for ease of use in the offline RL setting and was extensively tested.
We will provide researchers with access to the robot cluster, which will allow them to evaluate the policies they trained on the dataset proposed herein.
To ease development and study the fundamental differences between simulated and real world data,
we provide a corresponding simulated environment using PyBullet \citep{coumans2016pybullet}. %

To summarize, the hardware and software have the following three key properties:
i) physically capable of dexterous manipulation;
ii) robust enough for running and evaluating learning methods, and
iii) easy to use (robot hardware and simulator) and integrated in existing code frameworks.


\section{The \Trifinger Datasets} \label{sec:dataset}
We consider two tasks that involve a colored cube: pushing the cube to a target location and lifting the cube to a desired location and orientation.
To create behavioral datasets for these tasks on the \Trifinger platform, we need expert policies,
 which we obtain using reinforcement learning in a parallel simulation environment with domain randomization.
\Figref{fig:overview} visualizes the entire procedure -- from training to data collection to offline learning. In this section, we describe the tasks and the data collection while \sec{sec:benchmarking} is dedicated to benchmarking offline RL algorithms on the collected data.

\subsection{Dexterous Manipulation Tasks}
% Figure environment removed


We consider two tasks on the TriFinger platform that require dexterous manipulation of a cube:

\paragraph{Push} The goal is to move the cube to a target location. This task does not require the agent to align the orientation of the cube; the reward is based only on the desired and the achieved position.

\paragraph{Lift} The cube has to be picked up and moved to a target pose in the air which includes position and orientation. This requires flipping the cube on the ground, obtaining a stable grasp, lifting it to a target location and rotating it to match the target orientation.

Following prior work \citep{Hwangbo2019-ln, Allshire21} we define the reward by applying a logistic kernel $k(x)=\left(b+2\right)\left(\exp(a\lVert x\rVert) + b + \exp(-a\lVert x\rVert)\right)^{-1}$
to the difference between desired and achieved position (for the Push task) or the desired and achieved corner points of the cube (for the Lift task). The parameters $a$ and $b$ control the length scale over which the reward decays and how sensitive it is for small distances $x$, respectively. This yields a smooth, dense and bounded reward, see \supp{sec:reward} for details. We define success in the pushing task as reaching the goal position with a tolerance of 2\,cm. For the lifting task we additionally require to not deviate more than 22 degrees from the goal orientation.

We note that pushing is more challenging than it may appear due to inelastic collisions between the soft fingertips and the cube which are not modeled in rigid body physics simulators. Furthermore, the performance of policies can be quite sensitive to the value of sliding friction. Lifting is, however, even more challenging as flipping the cube based on a noisy object-pose with a low refresh rate is error-prone, learning the right sequence of behaviors requires long-term credit assignment and dropping the cube often results in loosing all progress in an episode.

\subsection{Training Expert Policies}\label{sec:training_expert_policies}

We train expert policies with online RL in simulation which we then use for data collection on the real system (\figref{fig:overview}).
We build on prior work which achieved sim-to-real transfer for a dexterous manipulation task on the TriFinger platform \citep{Allshire21}.
This approach replicates the real system in a fast, GPU-accelerated rigid body physics simulator \citep{makoviychuk2021isaac} and trains with an optimized implementation \citep{rl-games2022} of Proximal Policy Optimization \citep{schulman2017proximal} with a high number of parallel actors. %
We furthermore adopt their choice of a control frequency of 50\,Hz and use torque control to enable direct control of the fingers.
The sensor input is the proprioceptive sensor information and the object pose.
In order to obtain policies that are robust enough to work on the real system, domain randomization \citep{tobin2017domain,mandlekar2017adversarially,peng2018sim} is applied: for each episode, physics parameters like friction coefficients are sampled from a distribution that is likely to contain the  parameters of the real environment. Additionally, noise is added to the observations and actions to account for sensor noise and the stochasticity of the real robot. Furthermore, random forces are applied to the cube to obtain a policy that is robust against perturbations, as in \citep{andrychowicz2020learning}.
The object and goal poses are represented by keypoints, i.e., the Cartesian coordinates of the corners of the cube. This choice was empirically shown to accelerate training compared to separately encoding position and orientation in Cartesian coordinates and a quaternion~\citep{Allshire21}.


To improve the robustness of the trained policies across the different robot instances and against other real-world effects like tracking errors due to accumulating dust, we modified the implementation of \citet{Allshire21} in several ways. While the original code correctly models the 10\,Hz refresh rate of the object pose estimate, we found it beneficial to also simulate the delay between the time when the camera images are captured and when they are provided to the agent. This delay typically ranges between 100\,ms and 200\,ms. We furthermore fully randomize the initial orientation of the cube and use a hand-crafted convex decomposition of the barrier collision mesh to avoid artifacts of the automatic decomposition, to which the policy can overfit.
For the pushing task we penalized rapid changes in the cube position and orientation as sliding and flipping the cube transfers less well to the real system than moving it in a slow and controlled manner.
We observed that policies trained with RL in simulation tend to output oscillatory actions, which does not transfer well to the real system and causes stronger wear effects~\citep{mysore2021regularizing}.
To avoid this, we penalized changing the action which led to smoother movements and better performance. For the Lift task we additionally consider a policy which was trained with an Exponential Moving Average on the actions as we observed that vibrations on the real robot can lead to slipping and dropping (see \figref{fig:reset-drops} (b)). These vibrations might be caused by elastic deformations of the robot hardware and the complex contact dynamics between the soft fingertips and the cube that are not modeled in simulation.
As also observed in \citet{wang2022dexterous}, the performance on the real system varies significantly with training seeds. We evaluated 20 seeds on the real system and used the best one for data collection. More details on training in simulation can be found in the \supp{subsec:training_expert_policies_sim}.


\subsection{Data Collection} %
\label{sec:data-collection}

% Figure environment removed

We collected data for the pushing and lifting tasks both in a PyBullet \citep{coumans2016pybullet} simulation~\citep{trifinger-simulation} and on the real system, as shown in \figref{fig:overview}.
To ensure that the data collection procedures are identical, we run the same code with the simulator backend and with the real-robot backend on six TriFinger platforms (\figref{fig:trifinger}).
We observed that policies learned in simulation struggle with retrieving the cube from the barrier on the real robot\footnote{Possibly because the object tracking performance is slightly worse at the barrier or the dynamics of the outstretched fingers aligns less well between simulation and reality.}.
To alleviate this problem, we employ a predefined procedure to push the cube away from the barrier between episodes.
The resulting starting positions together with the target positions are visualized in \figref{fig:dataset-positions}.
During data collection, self-tests are performed at regular intervals to ensure that all robots are fully functional.


For the dataset to be useful to the community, it should be possible to evaluate offline-learned policies on the real robots. As machine learning models can be computationally demanding, we wait for a fixed time interval between receiving a new observation and starting to apply the action based on this observation. This time budget is allocated for running the policy without deviating from the control frequency used for data collection. We choose 10\,ms for the Push task and 2\,ms for the Lift task as we found training with bigger delays difficult. Note that our expert policies run in less than 1\,ms.

We provide as much information about the system as possible in the observations. The robot state is captured by joint angles, angular velocities, recorded torques, fingertip forces, fingertip positions, fingertip velocities (both obtained via forward kinematics), and the ID of the robot. The object pose is represented by a position, a quaternion encoding orientation, the keypoints, the delay of the camera images for tracking and the tracking confidence. The observation additionally contains the last action, which is applied during the fixed delay between observation and action. The desired and achieved goal are also included in the observation and contain either a position for the Push task or keypoints for the Lift task.
Some observations provide redundant information; however, we believe this simplifies working with the datasets, as we provide user-friendly Python code that implements filtering as well as automatically converting observations to a flat array. Moreover, we additionally publish a version of each dataset with camera images from three viewpoints. We believe that directly learning from these image datasets is an exciting challenge for the community\footnote{At the time of writing we cannot benchmark on these datasets since the robot cluster does not provide GPU-access at the moment. This will likely change in the near future.} and that they are moreover valuable in their own right as a large collection of robotic manipulation video footage.

For each task we consider pure expert data (\DExp),
mixed data recorded with a range of training checkpoints (\DCheckpoints),
a combination of 50\% expert trajectories and 50\% trajectories recorded with a weaker policy with additive Gaussian noise on the actions (\DMixed) and the 50\% expert data in \DMixed (\DHalfExp).
We run the same policies in simulation and on the real system.
An exemplary expert behavior for the Lift task is shown in \figref{fig:expert:lift}.
Episodes last for 15\,s for the Push task and 30\,s for the Lift task as reorienting, grasping and lifting the cube requires more time. For the Push task, we collect 16\,h of interaction for each dataset, corresponding to 3840 episodes and 2.8 million transitions. For the more demanding Lift task, we collect 20\,h of robot interactions corresponding to 2400 episodes and 3.6 million transitions.
The \DHalfExp datasets contain the expert data that is included in the corresponding \DMixed datasets to isolate the effects of reducing the amount of expert data and adding suboptimal trajectories.
The average success rates are provided next to the offline learning results in \tabref{tab:results:push} and \ref{tab:results:lift} (see \emph{data} column).
In \supp{sup:datasets}, we give a detailed analysis along with a specification (Table~\ref{tab:overview-datasets}) and statistics (Table~\ref{tab:details:datasets})
of the offline RL datasets.











% Figure environment removed

\section{Benchmarking Offline Reinforcement Learning Algorithms}
\label{sec:benchmarking}


We benchmark offline RL algorithms on pairs of simulated and real datasets and study the impact of data quality on their performance. We limit our evaluation to the best algorithms provided in the open-source library d3rlpy~\citep{SI21} to keep the required robot time for the evaluation of the trained policies manageable.
Namely, we benchmark the following algorithms:
BC~\citep{BS95, Pomerleau91, RossGB11, TorabiWS18}, %
CRR~\citep{NZMSRSSGHF20}, AWAC~\citep{NDGL20}, CQL~\citep{KumarZTL20}, and
IQL~\citep{KNL21}. We report results for two sets of hyperparameters: The default values (except for CQL for which we performed a grid search on Push-\DPushSimExp as the default parameters did not learn) and the results of a grid search on Lift-\DLiftSimMix (marked by $^\dagger$). Details about the hyperparamters and their optimization are in \supp{sup:hyperparameters}. We think that the performance at the default hyperparameters is highly relevant for offline RL on real data as optimizing the hyperparameters without a simulator is often infeasible.

\subsection{Results}\label{sec:results}

We train with five different seeds for each algorithm and evaluate with a fixed set of randomly sampled goals. Details about the policy evaluation on the simulated and real set-up can be found in Appendix \ref{sup:policy_evaluation}. We report success rates in the main text and returns in Appendix \ref{sup:results}.

\begin{table}
  \centering
  \vspace{-.5em}
  \caption{{\bf Pushing: Success rate on the \DPush Datasets.}
  `data' denotes the mean over the dataset. Average and standard deviation over five training seeds. A star $^*$ indicates significance w.r.t.\ all other methods using Welch's $t$-test with $p<0.05$.
  }\label{tab:results:push}\vspace{-.5em}
  \begin{adjustbox}{max width=0.9\textwidth}
  \begin{tabular}{l|c|ccccc}
    \toprule
    Push-Datasets           & data      & BC             & CRR                    & AWAC               & CQL        &   IQL\\
    \midrule
    \DPushSimExp & 0.95 & \( 0.83 \pm 0.02 \) & \( \bf 0.94 \pm 0.04 \) & \( 0.92 \pm 0.03 \) & \( 0.03 \pm 0.01 \) & \( 0.88 \pm 0.04 \) \\
\DPushSimHalfExp & 0.95 & \( 0.71 \pm 0.05 \) & \( \bf 0.79 \pm 0.05 \) & \( \bf 0.79 \pm 0.02 \) & \( 0.05 \pm 0.02 \) & \( 0.70 \pm 0.06 \) \\
\DPushSimMix & 0.53 & \( 0.53 \pm 0.09 \) & \( \bf 0.88 \pm 0.03 \) & \( 0.83 \pm 0.05 \) & \( 0.17 \pm 0.03 \) & \( 0.66 \pm 0.14 \) \\
\DPushSimCheckpoints & 0.76 & \( 0.53 \pm 0.04 \) & \( 0.09 \pm 0.10 \) & \( \bf 0.84 \pm 0.06 \)$^{*}$\!\!\! & \( 0.02 \pm 0.01 \) & \( 0.69 \pm 0.07 \) \\

    \midrule
    \DPushRealExp     & 0.92 & \( 0.74 \pm 0.05 \) & \( \bf 0.87 \pm 0.07 \) & \( 0.80 \pm 0.03 \) & \( 0.54 \pm 0.13 \) & \( 0.75 \pm 0.08 \)\\
    \DPushRealHalfExp & 0.92 & \( 0.66 \pm 0.08 \) & \( \bf 0.78 \pm 0.04 \) & \( 0.76 \pm 0.10 \) & \( 0.48 \pm 0.08 \) & \( 0.70 \pm 0.08 \)\\
    \DPushRealMix     & 0.51 & \( 0.48 \pm 0.10\)  & \( \bf 0.84 \pm 0.06\)$^{*}$\!\!\!  & \( 0.69 \pm 0.06\)  & \( 0.14 \pm 0.04\)  & \( 0.68 \pm 0.05\) \\
    \DPushRealCheckpoints & 0.49 & \(0.29\pm0.06\) & \(0.30\pm0.06\) & \(0.61\pm0.09\) & \(0.02\pm0.02\) & \(\bf 0.66\pm0.08\) \\
    \bottomrule
  \end{tabular}
  \end{adjustbox}
\end{table}

The benchmarking results for the Push task are summarized in \tabref{tab:results:push}.  Most of the offline RL algorithms perform well but the performance on the real data is generally worse. An exception to this pattern is CQL which performs poorly on the simulated Push task and gains some performance on the real data, perhaps due to the broader data distribution of the stochastic real-world environment. As expected BC performs well on the expert datasets but cannot exceed the dataset success rate on the \DMixed data, unlike CRR, AWAC, and IQL. On the expert data, CRR and AWAC match the performance of the expert policy in simulation but fall slightly behind on the real data. Interestingly, the performance of CRR is not negatively impacted by weak trajectories (\eg 84\%  on \DPushRealMix compared to 78\% on \DPushRealHalfExp, where the latter only contains the expert data portion). We provide insights into how the behavior policy compares to an offline-trained policy for the Push task in \figref{fig:offline_rl_vs_expert} (b) and (c).

The more challenging Lift task separates the algorithms more clearly as summarized in \tabref{tab:results:lift}. CQL does not reach a non-zero success rate at all, despite our best efforts to optimize the hyperparameters (see \supp{sup:hyperparameters}). This is in line with the results reported in \citet{Mandlekar2021WhatMattersRL} where CQL failed to learn on the datasets corresponding to more complex manipulation tasks. \citet{Kumar2021workflowRL} furthermore discusses the sensitivity of CQL to the choice of hyperparameters, especially on robotic data. While the best algorithms come close to matching the performance of the expert on Lift-\DLiftSimExp, they fall short of reaching the dataset success rate on the real-robot data (Lift-\DLiftRealExp).
On the \DLiftRealSmoothExp dataset the success rates are, on average, slightly lower, probably due to the non-Markovian behavior policy. The returns are generally higher, however, likely due to the smoothed expert reaching higher returns (see \tabref{tab:app:results:lift_return}).

On the Lift-\DMixed datasets all algorithms perform significantly worse than on the expert data. This effect is most pronounced on the real robot data and calls the ability of the algorithms to make good use of the 50\% expert trajectories into question. To verify that the performance drop is not due to only half of the expert data being available, we also train solely on the expert trajectories contained in the \DMixed dataset. We refer to this dataset as \DHalfExp. We find that the resulting policy performs significantly better, ruling out that the performance drop on the Lift-\DMixed dataset is exclusively caused by a lack of data. \figref{fig:lifting-sim} shows that this is true for the simulated Lift datasets as well. We conclude that the benchmarked offline RL algorithms still exhibit BC-like behavior, i.e., they are distracted by suboptimal data. Their poor performance on the \DCheckpoints datasets further underpins their reliance on imitation of a consistent expert as all algorithms struggle to learn from the data collected by a range of training checkpoints (see tables \ref{tab:results:push} and \ref{tab:results:lift} and learning curves in section \ref{sup:learning-curves}).\looseness-1

\begin{table}
  \centering
  \vspace{-1em}
  \caption{{\bf Lifting: Success rate on the \DLift Datasets.} `data' denotes the mean over the dataset. Average and standard deviation over five training seeds. Experiments with hyperparameters optimized on \DLiftSimMix are marked with a $^\dagger$. Stars $^*$ and $^{**}$ indicate significance w.r.t. second best Welch's $t$-test with $p<0.05$ and $p<0.01$, respectively.}\label{tab:results:lift}\vspace{-.5em}
  \begin{adjustbox}{max width=0.9\textwidth}
  \begin{tabular}{l|c|ccccc}
    \toprule
    Lift-Datasets         & data &       BC          &     CRR         &       AWAC      &       CQL        &     IQL      \\
    \midrule
    \DLiftSimExp & 0.87 & \( 0.64 \pm 0.00 \) & \( \bf 0.80 \pm 0.03 \) & \( 0.75 \pm 0.04 \) & \( 0.00 \pm 0.00 \) & \( 0.47 \pm 0.06 \) \\
    \DLiftSimHalfExp & 0.88 & \( 0.64 \pm 0.02 \) & \( \bf 0.78 \pm 0.02 \)$^{*}$\!\!\! & \( 0.69 \pm 0.05 \) & \( 0.00 \pm 0.00 \) & \( 0.04 \pm 0.01 \) \\
    \DLiftSimMix & 0.5 & \( 0.16 \pm 0.04 \) & \( 0.43 \pm 0.35 \) & \( \bf 0.55 \pm 0.09 \) & \( 0.00 \pm 0.00 \) & \( 0.24 \pm 0.05 \) \\
    \DLiftSimCheckpoints & 0.68 & \( 0.01 \pm 0.01 \) & \( 0.19 \pm 0.06 \) & \( \bf 0.41 \pm 0.09 \)$^{**}$\!\!\!\!\! & \( 0.00 \pm 0.00 \) & \( 0.00 \pm 0.00\) \\
    \midrule
    \DLiftSimExp{}$^\dagger$ & 0.87 & \( 0.80 \pm 0.04 \) & \( \bf 0.84 \pm 0.01 \) & \( \bf 0.84 \pm 0.01 \) & \( 0.01 \pm 0.01 \) & \( 0.80 \pm 0.03 \) \\
    \DLiftSimHalfExp{}$^\dagger$ & 0.88 & \( 0.67 \pm 0.09 \) & \( 0.75 \pm 0.08 \) & \( \bf 0.82 \pm 0.02 \) & \( 0.00 \pm 0.01 \) & \( 0.75 \pm 0.05 \) \\
    \DLiftSimMix{}$^\dagger$ & 0.5 & \( 0.47 \pm 0.03 \) & \( \bf 0.69 \pm 0.04 \) & \( 0.54 \pm 0.03 \) & \( 0.00 \pm 0.00 \) & \( 0.64 \pm 0.05 \) \\
    \DLiftSimCheckpoints{}$^\dagger$ & 0.68 & \( 0.01 \pm 0.00 \) & \( 0.20 \pm 0.07 \) & \( \bf 0.32 \pm 0.04 \)$^{*}$\!\!\! & \( 0.00 \pm 0.00\) & \( 0.01 \pm 0.01 \) \\
    \midrule
    \DLiftRealSmoothExp & 0.64 & \(0.26\pm0.09\) & \(\bf 0.52\pm0.12\)$^{*}$\!\!  & \(0.34\pm0.04\) & \(0.00\pm0.00\) & \(0.24\pm0.03\) \\
    \DLiftRealExp     & 0.66 & \( 0.27 \pm 0.09 \) & \( \bf 0.57 \pm 0.07 \)$^{**}$\!\!\!\!\!   & \( 0.24 \pm 0.04 \) & \( 0.00 \pm 0.00 \) & \( 0.29 \pm 0.09 \)\\
    \DLiftRealHalfExp & 0.68 & \(0.15\pm0.01\) & \(\bf 0.38\pm0.08\)$^{**}$\!\!\!\!\!   & \(0.12\pm0.08\) & \(0.00\pm0.00\) & \(0.12\pm0.06\) \\
    \DLiftRealMix     & 0.40  & \( 0.02 \pm 0.04 \) & \( \bf 0.17 \pm 0.09 \) & \( 0.03 \pm 0.06 \) & \( 0.00 \pm 0.00 \) & \( 0.11 \pm 0.13 \)\\
    \DLiftRealCheckpoints & 0.42 & \(0.00\pm0.01\) & \(\bf 0.18\pm0.07\)$^{**}$\!\!\!\!\! & \(0.02\pm0.01\) & \(0.00\pm0.00\) & \(0.03\pm0.02\) \\
    \midrule
    \DLiftRealExp{}$^\dagger$ & 0.66 & \(0.28\pm0.04\) & \(\bf 0.54\pm0.09\) & \(0.31\pm0.04\) & \(0.00\pm0.00\) & \(0.48\pm0.07\) \\
    \DLiftRealHalfExp{}$^\dagger$ & 0.68 & \(0.25\pm0.07\) & \(\bf 0.37\pm0.09\) & \(0.36\pm0.06\) & \(0.01\pm0.01\) & \(0.30\pm0.06\)\\
    \DLiftRealMix{}$^\dagger$ & 0.40  &\(0.09\pm0.04\) & \(\bf 0.29\pm0.07\)$^{*}$\!\! & \(0.12\pm0.06\) & \(0.00\pm0.00\) & \(0.15\pm0.03\) \\
    \DLiftRealCheckpoints{}$^\dagger$ & 0.42 & \(0.00\pm0.00\) & \(\bf 0.18\pm0.04\)$^{***}$\!\!\!\!\!\!\! & \(0.01\pm0.01\) & \(0.00\pm0.00\) & \(0.02\pm0.01\) \\
    \bottomrule
  \end{tabular}
  \end{adjustbox}
\end{table}

% Figure environment removed

The Lift datasets, in particular those recorded on the real robots, generally contain more useful sub-trajectories than apparent from the success rate, which is defined as achieving the goal pose \textit{at the end} of an episode. For example, during 87\% of all episodes in the Lift-\DLiftRealExp dataset the goal pose is achieved \textit{at some point} but the best algorithm CRR achieves a success rate of only 57\%. This suggests that these datasets have an untapped potential for trajectory stitching. As the observations are furthermore noisy and do not contain estimates of the velocity of the cube, training recurrent policies could potentially lead to an increase in performance.

To investigate the impact of noise on the performance of offline RL policies and the expert, we collected datasets in simulation for up to twice the noise amplitudes measured on the real system.  \figref{fig:offline_rl_vs_expert} (a) shows that the performance of the expert and the offline RL policies degrades only slowly when increasing the noise scale, ruling out that noise is the sole explanation for the performance gap between simulated and real system. As delays in the observation and action execution are already implemented in the simulated environment, we conclude that other factors like more complex contact dynamics and elastic deformations of the fingertips and robot limbs are likely causing the larger performance gap between data and learned policies on the real robots.

To test how well the policies learned from the datasets generalize over instances of the robot hardware, we evaluated on a hold-out robot which was not used for data collection. We did not see a significant difference in performance (see \supp{sup:holdout-robot} for details on this experiment) suggesting that the datasets cover enough variations in the robot hardware to enable generalization to unseen robots.

In summary, CRR and AWAC generally perform best on the proposed datasets with IQL also being competitive after hyperparameter optimization. The performance gap between expert and offline RL is in general bigger on the real system, perhaps due to more challenging dynamics.





% Figure environment removed

\vspace*{-.5em}
\section{Related Work} \vspace*{-.5em}
\label{sec:related}

\paragraph{Offline RL:} The goal of offline RL \citep{Levine2020SurveyRL, Prudencio2022SurveyRL} is to learn effective policies without resorting to an online interaction
by leveraging large and diverse datasets covering a sufficient amount of expert transitions. This approach is particularly interesting if interactions with the environment are either prohibitively costly
or even dangerous.

Offline RL faces a fundamental challenge, known as the Distributional Shift (DS) problem, originating from two
sources. There is a distribution mismatch
in training, as we use the behavioral data for training, but the learned policy would create a different state-visitation distribution.
The second problem is that during policy improvement,
 the learned policy requires an evaluation of the Q-function
 on unseen actions (out of distribution).
An over-estimation of the Q-value on these out-of-distribution samples leads to learning of a suboptimal policy.

Several algorithmic schemes were proposed for addressing the DS problem:
i) constraining the learned policy to be close to the behavior data~\citep{FujimotoMP19, KFSTL19, ZhangKP21, KostrikovFTN21};
ii) enforcing conservative estimates of future rewards~\citep{KumarZTL20, YuKRRLF21, CXJA22}; and
iii) model-based methods that estimate the uncertainty via ensembles~\citep{JannerFZL19, KidambiRNJ20}.

Additionally, other approaches include: implicitly tackling the DS problem via (advantage-weighted) variants of behavioral cloning~\citep{NDGL20, NZMSRSSGHF20, ChenZWWWR20, FujimotoG21} or even completely bypassing it either
by removing the off-policy evaluation and performing a constrained policy improvement
using an on-policy Q estimate of the behavior policy \mbox{\citep{BrandfonbrenerW21}}, or
by approximating the policy improvement step implicitly by learning on-data state value function~\citep{KNL21}.
An orthogonal research line considers combining importance sampling and off-policy techniques~\citep{NDKCL19, NachumCD019, ZhangD0S20, XZLY21}, and recently \citep{ChenLRLGLASM21, JannerLL21} investigated learning an optimal trajectory distribution via transformer architectures.

The simplest strategy for learning from previously collected data is behavioral cloning (BC), which is fitting a policy to the data directly.
Offline RL can outperform BC:
i) on long-horizon tasks with mixed expert data, \eg, trajectories collected
via an expert and a noisy-expert; and
ii) with expert or near expert data, when there is a mismatch between the initial and the deployment state distribution.

\vspace*{-.1em}
\paragraph{RL for dexterous manipulation:}
Reinforcement learning was recently successfully applied to dexterous manipulation on real hardware \citep{OpenAI2018-gn,OpenAI2019-gt,Allshire21, wang2022dexterous}. These results rely on training with online RL in simulation, however, and are consequently limited by the fidelity of the simulator.  While domain randomization \citep{tobin2017domain,mandlekar2017adversarially,peng2018sim} can account for a mismatch between simulation and reality in terms of physics parameters, it cannot compensate oversimplified dynamics. The challenges of real-world environments have been recognized and partly modeled in simulated environments \citep{Dulac2020empiricalRL}. To overcome the sim-to-real gap entirely, however, data from real-world interactions is still required, in particular for robotics problems involving contacts.

\vspace*{-.1em}
\paragraph{Offline RL datasets:}
While offline RL datasets with data from simulated environments, like D4RL~\citep{fu2020d4rl} and RL Unplugged~\citep{gulcehre2020rl}, have propelled the field forward, the lack of real-world robotics data has been recognized \citep{behnke2006robot,Bonsignorio2015-lg, Calli2015-zj, Amigoni2015-gh, Murali2019-rg}. Complex contact dynamics, soft deformable fingertips and vibrations are particularly relevant for robotic manipulation but are not modeled sufficiently well in simulators used by the RL community \citep{todorov2012mujoco,makoviychuk2021isaac,brax2021github}. Recently, three small real-world datasets with human demonstrations for a robot arm with a gripper (using operational space control) have been proposed \citep{Mandlekar2021WhatMattersRL}. Two of them require only basic lifting and dropping while the third, more challenging task could not be solved with the available amount of data. For more challenging low-level physical manipulation, a dataset suitable for offline RL is still missing. We therefore provide the first real-robot dataset for dexterous manipulation which is sufficiently large for offline RL (one order of magnitude more data on real robots than prior work \citep{Mandlekar2021WhatMattersRL}) and for which learned policies can easily be evaluated remotely on a real-robot platform.

\vspace*{-.1em}
\paragraph{Affordable open-source platforms:}
Our hardware platform is open source.
Other affordable robotic open-source platforms are, for instance, a manipulator \citep{yang2019replab},
a simple robotic hand and quadruped \citep{ahn2019robel}.
Since it is hard to set up and maintain such platforms,
we provide access to our real platform upon request,
and hope that this will bring the field forward.

\vspace*{-.1em}
\paragraph{Remote benchmarks:}
For mobile robotics, \citet{pickem2017robotarium} propose the Robotarium, a remotely accessible swarm robotics research platform, and \citet{kumar2019offworld} offer OffWorld gym consisting of two navigation tasks with a wheeled robot.
Similarly, Duckietown \citep{paull2017duckietown} hosts the AI Driving Olympics \citep{AI-Driving-olympics}. %


\section{Conclusion}\vspace*{-.5em}
\label{sec:conclusion}

We present benchmark datasets for robotic manipulation
 that are intended to help improving the state-of-the-art in offline reinforcement learning.
To record datasets, we trained capable policies using online learning in simulation with domain randomization.
Our analysis and evaluation on two tasks, Push and Lift, show that offline RL algorithms still leave room for improvement on data from real robotic platforms. We identified two factors that could translate into increased performance on our datasets: trajectory stitching and robustness to non-expert trajectories.
Further, our analysis indicates that noise and delay alone cannot explain the larger gap between dataset and offline RL performance on real systems, underpinning the importance of real-robot benchmarks.

We invite the offline RL community to train their algorithms with the new datasets and test the empirical performance of the latest offline RL algorithms, \eg \citep{KostrikovFTN21, XZLY21, Kumar2021workflowRL, CXJA22}, on real-robot hardware.


\subsubsection*{Author Contributions}
N.G., S.Bl., P.K., M.W., S.Ba., B.S., and G.M. conceived the idea, methods and experiments.
G.M. initiated the project, M.W., S.Ba. and B.S. conceived the robotic platform. F.W. implemented the low-level robot control and parts of the submission system. 
N.G. trained the expert policies and created the datasets.
N.G., S.Bl., and P.K. conducted the offline RL experiments, collected the results and analyzed them under the supervision of G.M.
N.G. ran all experiments on the real systems. 
N.G. and F.W. wrote the software for downloading and accessing the datasets.
N.G., S.Bl., P.K., F.W. and G.M. drafted the manuscript, and all authors revised it. %

\subsubsection*{Acknowledgments}
We are grateful for the help of Thomas Steinbrenner in repairing and maintaining the robot cluster. Moreover, feedback by Arthur Allshire on training expert policies in simulation and by Huanbo Sun on domain randomization proved valuable. We acknowledge the support from the German Federal Ministry of Education and Research (BMBF) through the Tübingen AI Center (FKZ: 01IS18039B).
Georg Martius is a member of the Machine Learning Cluster of Excellence, EXC number 2064/1 – Project number 390727645.
Pavel Kolev was supported by the Cyber Valley Research Fund and the Volkswagen Stiftung (No 98 571).
We thank the anonymous reviewers for comments which helped improve the presentation of the paper.

\subsection*{Reproducibility Statement}
We publish the datasets we propose as benchmarks (sections \ref{sec:data-collection} and \ref{sup:datasets}) and provide access to the cluster of real \Trifinger platforms (section \ref{sec:platform}) we used for data collection. Submissions to the cluster do not require any robotics experience and can be made in the form of a Python implementation of a RL policy (section \ref{sec:code}). A simulated version of the \Trifinger platform~\citep{trifinger-simulation} and a low-cost hardware variant are furthermore publicly available as open source \citep{trifinger}.
For offline RL training we moreover use open-source software~\citep{SI21}. Finally, we describe our hyperparameter optimization in detail and provide the resulting hyperparameters in section \ref{sup:hyperparameters}.

\bibliography{references}
\bibliographystyle{iclr2023_conference}
\clearpage

\appendix
\input{appendix.tex}

\end{document}
