\renewcommand{\topfraction}{0.85}
\renewcommand{\bottomfraction}{0.5}
\renewcommand{\floatpagefraction}{0.95}
\renewcommand{\textfraction}{0.05}
\renewcommand{\thetable}{S\arabic{table}}
\renewcommand{\thefigure}{S\arabic{figure}}
\renewcommand{\theequation}{S\arabic{equation}}

\setcounter{figure}{0}
\setcounter{table}{0}
\setcounter{equation}{0}

\section{Additional Results}\label{sup:results}

This section contains additional experimental results. In particular values for the average returns achieved by the algorithms are reported. These were omitted from the main text as the success rates are easier to interpret.

\subsection{Returns for the Push Task}

We report the returns achieved on the Push task in \tabref{tab:results:push_returns_updated}. 
Note that for the simulated datasets the best performing algorithm in terms of return is not necessarily the same as the best performing one in terms of success rates (see   \tabref{tab:results:push}). For instance, AWAC has the highest average return on \DPushSimExp while CRR achieves a higher success rate. This has two reasons. First, the success is measured only at the final step of the episode, while the return is computed as the cumulative reward over time. Second, the return is dense as it is computed from the distance between cube and target, while the success rate is a sparse signal. 

\begin{table}[ht]
  \centering
  \caption{{\bf Push: Returns on the \DPush Datasets.}
  `data' denotes the mean over the dataset. Average and standard deviation over five training seeds.
  }\label{tab:results:push_returns_updated}
  \begin{adjustbox}{max width=\textwidth}
  \begin{tabular}{l|c|cccccc}
    \toprule
    Push-Datasets           & data      & BC             & CRR          &  AWAC               & CQL        &   IQL\\
    \midrule
    \DPushSimExp & 674 & \( 585 \pm 19 \) & \( 636 \pm 20 \) & \( \bf 657 \pm 14 \) & \( 184 \pm 23 \) & \( 631 \pm 18 \) \\
\DPushSimHalfExp & 674 & \( 535 \pm 18 \) & \( 576 \pm 17 \) & \( \bf 586 \pm 14 \) & \( 226 \pm 12 \) & \( 565 \pm 22 \) \\
\DPushSimMix & 512 & \( 460 \pm 50 \) & \( \bf 613 \pm 17 \) & \( 603 \pm 23 \) & \( 311 \pm 29 \) & \( 543 \pm 75 \) \\
\DPushSimCheckpoints & 583 & \( 460 \pm 26 \) & \( 205 \pm 100 \)\!\!\! & \( \bf 636 \pm 20 \)$^{*}$\!\!\! & \( 138 \pm 11 \) & \( 588 \pm 25 \) \\
    \midrule
    \DPushRealExp     & 660 & \( 563 \pm 34\)  & \( \bf 638 \pm 16\)  & \( 624 \pm 17\)  & \( 514 \pm 37\)  & \( 592 \pm 29\) \\
    \DPushRealHalfExp & 660 & \( 546 \pm 29\)  & \( \bf 627 \pm 21\)  & \( 587 \pm 46\)  & \( 471 \pm 26\)  & \( 573 \pm 23\) \\
    \DPushRealMix      & 429 & \( 387 \pm 45\)  & \( \bf 622 \pm 41\)$^{*}$\!\!\!  & \( 568 \pm 29\)  & \( 346 \pm 68\)  & \( 555 \pm 16\)  \\
    \DPushRealCheckpoints & 419 & \(335\pm23\) & \(373\pm41\)\! & \(569\pm24\) & \(206\pm18\) & \(\bf 600\pm30\) \\
    \bottomrule
  \end{tabular}
  \end{adjustbox}
\end{table}

\subsection{Returns for the Lift Task} %

In this section, we report the returns achieved on the Lift task (\tabref{tab:app:results:lift_return}). Interestingly, although IQL achieves a significantly higher return on the \DLiftRealMix dataset than CRR, its final success rate is lower. An analysis of the rollout statistics reveals that IQL often moves the cube close to the goal pose but not close enough to satisfy the success criteria (defined in section \ref{sec:dataset}). CRR, on the other hand, on average deviates further from the goal pose than IQL but has a bigger fraction of rollouts in which it satisfies the success criteria. In summary IQL does better on average on this dataset but lacks precision in matching the goal pose.


\begin{table}[ht]
  \centering
  \caption{{\bf Lift: Returns on the \DLift Datasets.}
  `data' denotes the mean over the dataset. Average and standard deviation over five training seeds.
  }\label{tab:app:results:lift_return}
  \begin{adjustbox}{max width=\textwidth}
  \begin{tabular}{l|c|cccccc}
    \toprule
    Lift-Datasets           & data      & BC             & CRR          &  AWAC               & CQL        &   IQL\\
    \midrule
    \DLiftSimExp & 1334 & \( 1129 \pm 56 \) & \!\( 1246 \pm 10 \) & \( \bf 1280 \pm 20 \)$^{*}$ & \( 163 \pm 14 \) & \( 1133 \pm 41 \) \\
    \DLiftSimHalfExp & 1337 & \( 1112 \pm 39 \) & \!\!\!\!\!\( \bf 1231 \pm 9 \) & \!\!\( 1211 \pm 19 \) & \( 154 \pm 13 \) & \( 744 \pm 40 \)\!\!\! \\
    \DLiftSimMix & 1133 & \( 791 \pm 43 \)\!\!\! & \( 727 \pm 447 \)\!\!\!\!\! & \!\!\!\( \bf 1103 \pm 62 \) & \( 164 \pm 15 \) & \( 943 \pm 56 \)\!\!\! \\
    \DLiftSimCheckpoints & 1173 & \( 409 \pm 9 \) & \( 604 \pm 79 \)\!\! & \( \bf 931 \pm 75 \)$^{***}$\!\!\!\!\!\!\!\! & \!\!\!\( 161 \pm 8 \) & \( 572 \pm 24 \)\!\!\! \\
    \midrule
   \DLiftSimExp{}$^\dagger$ & 1334 & \( 1274 \pm 17 \) & \( 1245 \pm 26 \) & \( \bf 1319 \pm 11 \)$^{*}$\! & \( 399 \pm 56 \) & \( 1286 \pm 20 \) \\
    \DLiftSimHalfExp{}$^\dagger$ & 1337 & \( 1191 \pm 33 \) & \( 1153 \pm 50 \) & \( \bf 1303 \pm 14 \)$^{*}$\! & \( 439 \pm 26 \) & \( 1229 \pm 45 \) \\
    \DLiftSimMix{}$^\dagger$ & 1133 & \( 1040 \pm 22 \) & \( 1087 \pm 49 \) & \!\( 1120 \pm 23 \) & \( 467 \pm 34 \) & \( \bf 1172 \pm 34 \)$^{*}$\!\!\! \\
    \DLiftSimCheckpoints{}$^\dagger$ & 1173 & \( 411 \pm 35 \)\!\!\! & \( 593 \pm 93 \)\!\!\! & \( \bf 862 \pm 53 \)$^{**}$\!\!\!\!\!\!\! & \!\!\!\( 151 \pm 9 \) & \( 564 \pm 31 \)\!\!\!\! \\
    \midrule
    \DLiftRealSmoothExp & 1206 & \(915\pm36\) & \!\!\!\!\(\bf 1059\pm54\) & \!\!\!\(1031\pm42\) & \(143\pm33\) & \!\!\!\(1002\pm19\)\\
    \DLiftRealExp     & 1064 & \( 711 \pm 92 \) & \!\( \bf 1014 \pm 62 \)$^{*}$ & \( 820 \pm 50 \) & \( 283 \pm 38 \) & \( 901 \pm 43 \)\\
    \DLiftRealHalfExp & 1064 & \(553\pm77\) & \(\bf 837\pm58\) & \(613\pm107\)\!\!\! & \!\!\!\(200\pm7\) & \(759\pm101\)\!\!\! \\
    \DLiftRealMix     & 851  & \( 346 \pm 21 \) & \( 633 \pm 59 \) & \( 397 \pm 71 \) & \( 298 \pm 16 \) & \( \bf 827 \pm 105 \)$^{*}$\!\!\!\!\!\!\\
   \DLiftRealCheckpoints & 862 & \(272\pm56\) & \(\bf 631\pm62\) & \(346\pm64\) & \!\!\!\(207\pm5\) & \(608\pm30\) \\
   \midrule
   \DLiftRealExp{}$^\dagger$ & 1064 & \(676\pm47\) & \(889\pm39\) & \(747\pm44\) & \(289\pm33\) & \(\bf 899\pm40\)\\
   \DLiftRealHalfExp{}$^\dagger$ & 1064 & \(702\pm83\) & \(813\pm57\) & \(797\pm54\) & \(312\pm65\) & \(\bf 855\pm51\)\\
   \DLiftRealMix{}$^\dagger$ & 851 & \(437\pm47\) & \(\bf 707\pm20\)$^{*}$\!\!\! & \(481\pm54\) & \(269\pm33\) & \(574\pm67\) \\
   \DLiftRealCheckpoints{}$^\dagger$ & 862 & \(288\pm73\) & \(\bf 634\pm36\) & \(361\pm81\) & \(192\pm28\) & \(596\pm33\) \\
   \bottomrule
  \end{tabular}
  \end{adjustbox}
\end{table}

\subsection{Evaluation on the Hold-Out Robot}
\label{sup:holdout-robot}

To quantify how well the policies obtained with offline RL generalize to unseen hardware, we evaluate them on a holdout robot which was not used for data collection. \tabref{tab:results:holdout} shows the results for \DPush-\DPushRealExp and \DLift-\DPushRealExp. The performance of the algorithms on the hold-out robot is within the performance margin of the other robots, suggesting that there is no significant difference between the different robots.

\begin{table}[tbh]
  \centering
  \caption{{\bf Evaluation on hold-out robot.} Success rate on the Real-Expert datasets.}\label{tab:results:holdout}
  \begin{adjustbox}{max width=\textwidth}
  \begin{tabular}{l|ccccccc}
    \toprule
    Dataset & BC & CRR & AWAC & CQL & IQL\\
    \midrule
    Push-\DPushRealExp  & \( 0.80 \pm 0.04 \) & \(\bf 0.91 \pm 0.08 \) & \( 0.84 \pm 0.06 \) & \( 0.61 \pm 0.05 \) & \( 0.83 \pm 0.09 \)  \\
    Lift-\DLiftRealExp & \( 0.29 \pm 0.04 \) & \(\bf 0.64 \pm 0.05 \)$^{**}$\!\!\!\!\! & \( 0.31 \pm 0.08 \) & \( 0.00 \pm 0.00 \) & \( 0.24 \pm 0.07 \) \\
    \bottomrule
  \end{tabular}
  \end{adjustbox}
\end{table}

\subsection{Impact of noise on performance}
\label{sup:noise-scale}

As mentioned in section \ref{sec:results}, we studied the impact of noise on the performance of the expert and the considered offline RL algorithms by recording a sequence of datasets with increasing noise scale in simulation. A relative noise scale of 1 corresponds to the variance measured on the actions and observations of the real system. \figref{fig:noise_scale} shows the success rate and return as a function of the noise scale up to a value of 2. As the performance of the expert and the offline RL policies degrades only slowly when increasing noise, we conclude that the stochasticity of the real system cannot be the only reason for the performance gap between the simulated and the real system. As delays in the observation and action execution are already implemented in the simulated environment, we hypothesize that other factors like more complex contact dynamics and elastic deformations of the fingertips and robot limbs are likely causing the larger performance gap between data and learned policies on the real robots. 

AWAC and CRR perform consistently over a wide range of noise scales with a slight decrease in performance for high relative noise scales (probably due to a large variance of the estimated object pose). BC and IQL seem to struggle with the narrow data distribution generated by a deterministic environment but improve with increasing stochasticity. While the performance of BC drops significantly again for large noise scales, IQL becomes competitive in this regime.



% Figure environment removed

\subsection{Bar plots}
\label{sup:bar-plots}

\figref{fig:bars-push} and \figref{fig:bars-lift} show bar plots that summarize the performance of the algorithms on the two tasks in the simulated and real environment. Before averaging results from different datasets, we \emph{normalized} the algorithm performance by the dataset performance, i.e., reaching the average success rate or return of the dataset corresponds to a value of 1. 

While AWAC (and IQL on the real data) can exceed the behavior policy on average on the Push task, all algorithms fall short of matching the dataset performance on the challenging Lift datasets. While success rates on the Lift-Real datasets are particularly low, the returns indicate that CRR and IQL significantly outperform BC.

Since the hyperparameter optimization was done on the Lift-\DLiftSimMix dataset, it has the biggest impact on the performance on the Lift-Sim datasets. IQL in particular improves considerably there. The increase in performance on the Lift-Real datasets is considerably smaller, however. This suggests that optimizing the hyperparameters offline RL algorithms in simulation may have limited benefits on real environments.

% Figure environment removed
% Figure environment removed


\clearpage
\newpage
\subsection{Learning Curves} %
\label{sup:learning-curves}

In this section, we provide learning curves for the offline RL algorithms on all datasets. Since the evaluation of all training checkpoints on the real robots is prohibitively expensive and time-consuming, we evaluate the checkpoints learned on the \textbf{real data} in the \textbf{simulated environment}. This gives an over-optimistic estimate of the learning performance on the real robots.

For the challenging Lift-\DLiftRealMix task we performed a grid search for each algorithm (BC, CRR, AWAC, CQL, IQL) and selected the best hyperparameters. We present the grid search in \tabref{tab:grid-search-procedure}, the corresponding optimal hyperparameters in \tabref{tab:optimal-hyperparams:lift-sim-new-mixed}, and the default hyperparameters in \tabref{tab:default-hyperparams}.
We note that due to the poor performance of CQL, we expanded our grid search specifically for this algorithm (on the Push-\DPushSimExp data) and selected the corresponding optimal hyperparameters as its default parameters (see Figure~\ref{fig:histogram-CQL-400}). Our newly performed gridsearch, as mentioned above, was unfortunately not leading to improvements for CQL. 
Similar difficulties are reported in~\cite{Kumar2021workflowRL} and are tackled via case distinction and appropriate regularization techniques. It would be interesting to test the preceding two algorithmic techniques on our datasets, once their implementation is integrated in the \textsc{d3rlpy} library~\citep{SI21}.

We proceed by presenting the learning curves for the offline RL algorithms on the datasets: 
for the Lift task in section~\ref{subsec:lift-datasets} and for the Push task in section~\ref{subsec:push-datasets}. Shaded areas indicate the interval between the 0.25 and 0.75 quantiles.





\newpage
\subsubsection{Lift Datasets}\label{subsec:lift-datasets}


% Figure environment removed

\newpage




% Figure environment removed

\newpage

\subsubsection{Push Datasets}\label{subsec:push-datasets}


Here, we use the \textbf{default} hyperparameters in Table~\ref{tab:default-hyperparams}, where for the CQL algorithm we selected hyperparameters optimized by a grid search (see Figure~\ref{fig:histogram-CQL-400} for histograms). The \textit{success rates} and \textit{returns} are evaluated in the simulated environment.
% Figure environment removed

% Figure environment removed


\newpage




\clearpage

\section{Datasets}
\label{sup:datasets}

In this section we provide additional details on the datasets we collected. Section \ref{sec:app-data-collection} summarizes the data collection process, section \ref{sec:success} contains a discussion of the different notions of success we report, section \ref{sec:reward} is dedicated to the reward and the terminals, and section \ref{sec:dataset_analysis} provides a detailed analysis of the statistics of the datasets.

\subsection{Data Collection}\label{sec:app-data-collection}

Data is collected by running jobs on a cluster of TriFinger platforms without human intervention. Before the recording of each dataset the platforms were cleaned from particle dust to ensure consistent object tracking performance. Each job consists of the following steps:

\begin{enumerate}
    \item Move fingers to initial state and sample new goal.
    \item Run one episode/rollout with the selected policy and store transitions.
    \item Move the cube away from the barrier with a pre-recorded trajectory of the fingers.
    \item Repeat from 1. unless the desired number of episodes per job is reached.
    \item Do a self-check including cameras, pose estimation and joints.
    \item Approximately reset the cube to the center of the arena.
\end{enumerate}

The number of episodes collected per job is eight for the Push task (\SI{15}{s} per episode) and six for the Lift task (\SI{30}{s} per episode). If a self-check fails, the corresponding robot is deactivated and the maintainers are alerted via mail. Finally, the data from all episodes is combined in a  single HDF5 file following the conventions of D4RL~\citep{fu2020d4rl}.

The expert policies are obtained after the training in Isaac Gym (see section \ref{sec:training_expert_policies} and section \ref{subsec:training_expert_policies_sim}). The weak policies are training checkpoints at $210\cdot 10^6$ training steps for pushing and $288\cdot 10^6$ training steps for lifting. Gaussian noise is added to the actions with an amplitude of \SI{0.2}{Nm} and an update frequency of 8 for pushing and \SI{0.04}{Nm} with an update frequency of 4 for lifting.

The \DCheckpoints datasets are collected with policies from a range of training checkpoints where the total number of jobs was distributed as uniformly as possible over all training checkpoints. For pushing 22 checkpoints up to $668\cdot 10^6$ training steps were used while for lifting 59 training checkpoints up to $1721\cdot 10^6$ training steps were considered. \figref{fig:checkpoint_datasets} shows the return and success rate of these checkpoints on the real TriFinger cluster.

% Figure environment removed


\subsection{Three Notions of Success}\label{sec:success} %

We consider dexterous manipulation tasks which require the agent to match a goal position (for the Push task) or a goal pose (for the Lift task) with a tracked cube. The tolerance for goal achievement is \SI{2}{cm} for the position and 22 degrees for the orientation. We define \emph{momentary success} as achieving the goal at a single point in time, i.e., in an individual time step. This notion of success is rather weak, however, as achieving the goal pose for a short amount of time is significantly easier than maintaining it. For the pushing task in particular, it is much more likely to move through the goal by accident than to stabilize the cube after having reached it. When lifting the cube, it is  quite challenging to maintain a stable grasp due to the variance of the object pose estimate which introduces a considerable amount of noise to the observations. We therefore define \emph{success} as achieving the goal at the end of the episode which is only likely to happen when maintaining the goal pose for an extended period of time.

From the perspective of offline RL, however, it is highly relevant whether parts of the trajectories in the datasets lead to success, even if it is short-lived. As offline RL algorithms can, in principle, combine information from several trajectories, even trajectories that do not end with goal achievement may contain enough information to learn a successful policy. We therefore also report \emph{transient success} which indicates whether the goal has been achieved at any time during the episode and \emph{mean momentary success} which corresponds to the fraction of time steps during which the goal has been achieved.

\subsection{Reward and Terminals}\label{sec:reward} %

As already mentioned in section \ref{sec:dataset} of the main text, we use a logistic kernel \cite{Hwangbo2019-ln, Allshire21}
\begin{equation}
k(x)=\frac{b+2}{\exp(a\lVert x\rVert) + b + \exp(-a\lVert x\rVert)}
\end{equation}
to define the reward where $\lVert \cdot \rVert$ denotes the Euclidean norm. For the Push task, we apply it to the difference between the achieved and the desired cube position. Since we also want to take the orientation of the cube into account for the Lift task, we apply $k$ separately to the differences between the desired and achieved corner points (or keypoints) of the cube and average over the results \cite{Allshire21}. We use $a=30$ and $b=2$. See \figref{fig:reward} for a visualization of the reward function.

% Figure environment removed

By default, the terminals in the dataset (a Boolean indicating whether a terminal state has been reached) are never set. We chose this default setting to avoid problems due to state aliasing: Episodes last for a finite number of time steps $H$, and the RL objective is to maximize the discounted return (or cumulative reward)
\begin{equation}
    J=\sum_{t=0}^{H-1} \gamma^t r_t \ ,
\end{equation}
where $\gamma$ denotes the discounting factor and $r_t$ the reward in step $t$. A Markov state therefore has to keep track of how much time remains until the episode ends. The observation, on the other hand, does not contain this information.  This omission is a deliberate choice as we want to obtain a policy which is independent of the time step. This makes it impossible, however, to accurately estimate the value based on the observation if $\gamma$ is too large. Intuitively, without access to the time step $t$, the agent cannot know whether a cube far away from the goal corresponds to a large expected return to go because it is the beginning of an episode or a small expected return to go because there is no time left to move the cube and accumulate reward. See \citet{pardo2018time} for a more detailed discussion of issues arising from training with a finite horizon.

A practical solution to this problem is to not set the terminals and choose a gamma which is appropriate for the time scale of the task (for offline learning we chose $\gamma=0.99$). This choice hides the episodic nature of the task from the agent which results in good performance while avoiding a dependence of the policy on time. It may, however, sacrifice optimality in some corner cases like dropping the cube close to the end of the episode when there is not enough time to flip the cube over before lifting it again. Nevertheless, as we provide a flag to set the terminals at the episode ends and as it is straight forward to augment the observation with the intra-episode time step, the datasets are also suitable for experiments with time-dependent policies.





\subsection{Dataset Analysis}\label{sec:dataset_analysis}%

\paragraph{Overview:} We provide an overview of the various dataset types and their properties in \tabref{tab:overview-datasets}. 

\begin{table}
    \centering
    \caption{Overview of the \Trifinger offline RL datasets.}
    \label{tab:overview-datasets}
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{l|l|cccc}
    \toprule
    task & dataset & overall duration [h] & \#episodes & \#transitions [$10^6$] & episode length [s]\\
    \midrule
    \multirow{6}{*}{Push-} 
    & \DPushSimExp & 16 & 3840 & 2.8 & 15  \\
    & \DPushSimHalfExp & 8 & 1920 & 1.4 & 15  \\
    & \DPushSimMix & 16 & 3840 & 2.8 & 15  \\
    & \DPushSimCheckpoints & 16 & 3840 & 2.8 & 15  \\
    & \DPushRealExp  & 16 & 3840 & 2.8 & 15  \\
    & \DPushRealHalfExp & 8 & 1920 & 1.4 & 15  \\
    & \DPushRealMix & 16 & 3840 & 2.8 & 15  \\
    & \DPushRealCheckpoints & 16 & 3840 & 2.8 & 15  \\
    \midrule
    \multirow{8}{*}{Lift-} 
    & \DLiftSimExp & 20 & 2400 & 3.6 & 30  \\
    & \DLiftSimHalfExp & 10 & 1200 & 1.8 & 30  \\
    & \DLiftSimMix & 20 & 2400 & 3.6 & 30  \\
    & \DLiftSimCheckpoints & 20 & 2400 & 3.6 & 30  \\
    & \DLiftRealSmoothExp & 20 & 2400 & 3.6 & 30  \\
    & \DLiftRealExp & 20 & 2400 & 3.6 & 30  \\
    & \DLiftRealHalfExp & 10 & 1200 & 1.8 & 30  \\
    & \DLiftRealMix & 20 & 2400 & 3.6 & 30  \\
    & \DLiftRealCheckpoints & 20 & 2400 & 3.6 & 30  \\
    \bottomrule
    \end{tabular}
    \end{adjustbox}
\end{table}

\paragraph{Statistics:} \tabref{tab:details:datasets} summarizes the statistics of the \Trifinger datasets. While the success rate and the mean return constitute a limit to what can be achieved with pure imitation learning, the high numbers for the transient success rate (fraction of episodes in which the goal was achieved at least in one time step but not necessarily at the end of the episode) indicate that offline RL can potentially outperform the behavior policy significantly on these datasets.

\begin{table}
    \centering
    \caption{Statistics of the proposed \Trifinger offline RL datasets: \DPush and \DLift. Definitions of the success rates and the reward can be found in section \ref{sec:success} and section \ref{sec:reward}, respectively.}
    \label{tab:details:datasets}

    \begin{tabular}{l|cccc}
    \toprule
    \DPush & success rate & \makecell{mean momentary\\ success rate} & \makecell{transient\\ success rate} & mean return \\
    \midrule
    \DPushSimExp & 0.95 & 0.87 & 0.95 & 674\\
    \DPushSimHalfExp & 0.94 & 0.86 & 0.94 & 667\\
    \DPushSimMix & 0.53 & 0.48 & 0.86 & 512\\
    \DPushSimCheckpoints & 0.76 & 0.68 & 0.77 & 583\\
    \midrule
    \DPushRealExp & 0.92 & 0.78 & 0.98 & 660\\
    \DPushRealHalfExp & 0.92 & 0.78 & 0.98 & 660\\
    \DPushRealMix & 0.51 & 0.43 & 0.72 & 429\\
    \DPushRealCheckpoints & 0.49 & 0.40 & 0.63 & 419\\
    \bottomrule
    \toprule
    \DLift & success rate & \makecell{mean momentary\\ success rate} & \makecell{transient\\ success rate}  & mean return \\
    \midrule
    \DLiftSimExp & 0.87 & 0.77 & 0.97 & 1334\\
    \DLiftSimHalfExp & 0.88 & 0.78 & 0.98 & 1337\\
    \DLiftSimMix & 0.50 & 0.44 & 0.93 & 1133\\
    \DLiftSimCheckpoints & 0.68 & 0.60 & 0.84 & 1173\\
    \midrule
    \DLiftRealSmoothExp & 0.64 & 0.53 & 0.82 & 1206\\
    \DLiftRealExp & 0.67 & 0.52 & 0.87 & 1064\\
    \DLiftRealHalfExp & 0.68 & 0.52 & 0.86 & 1064\\
    \DLiftRealMix & 0.40 & 0.30 & 0.66 & 851\\
    \DLiftRealCheckpoints & 0.42 & 0.32 & 0.65 & 862\\
    \bottomrule
    \end{tabular}
\end{table}

\paragraph{Impact of variations between robots:} \figref{fig:success_rates_robots} compares the success rates of the expert policies on the individual robots used for data collection. While the performance differences between the robot instances are significant, the expert policies perform reasonably well on all of them, achieving at least 80\% success rate on the Push task and 60\% on the Lift task on all robots.

% Figure environment removed

\paragraph{Distribution of returns:} \figref{fig:returns_hist} shows the distribution of the episode returns for the datasets collected on the real system. It reveals that the expert policy on the Push task performed more consistently on the real robot than its counterpart for the Lift task. Qualitatively, this can be attributed to either (i) not being able to flip the cube to the approximately correct orientation, (ii) failing to establish a stable grasp on the cube, or (iii) dropping it after already having lifted it. Flipping the cube on the ground might fail due to incomplete modeling of interactions between the fingertips and the cube in the rigid body physics simulator or because it is sensitive to the value of sliding friction, the noise on the object pose estimate makes it difficult to maintain a stable grasp after having lifted the object.

% Figure environment removed

\paragraph{Initial cube position and goal distributions:}
\figref{fig:dataset-positions} visualizes the distribution of the initial cube position (blue) and the goal position (red) for the Push and Lift tasks. The goal positions are sampled on the ground for the Push task and in the air for the Lift task. The distribution of the initial cube position results from the reset procedure which removes the cube from the boundaries by moving the fingers along a pre-recorded trajectory.

% Figure environment removed

\paragraph{Action statistics:} 
\figref{fig:actions_push_real_expert} shows the distribution of actions of the expert policy that recorded \DPush-\DPushRealExp and of a policy learned from this data by AWAC. 

% Figure environment removed


\paragraph{Reset procedure:} As mentioned in section \ref{sec:data-collection} of the main text, we removed the cube from the barrier between episodes because we observed that the expert policies we trained in simulation struggled with retrieving the cube from the barrier. \figref{fig:distance-to-center} shows a histogram of the distance between the center of the cube and the arena center. While the reset procedure is sufficiently stochastic to randomize the initial cube position (see \figref{fig:dataset-positions}), it clearly removes the cube from the barrier in the majority of all resets. More precisely, only in 3\% of all resets the cube center was within 6 cm of the boundary (the cube has a width of 6.5 cm).

% Figure environment removed

\paragraph{Dropping frequency -- pure feedforward vs. smoothed:} One reason for the lower success rate on the Lift task as compared to the Push task is frequent dropping of the cube. This can be the result of vibrations of the fingers which can occur when applying policies trained in simulation on the real robots or of shaking caused by the noisy pose estimate for the cube. We found that applying a low-pass filter, more precisely an Exponential Moving Average, helps with both problems. \figref{fig:dropping-frequency} shows the frequency at which the cube is dropped over the course of a lifting episode. Dropping the cube is defined as reaching the height of the goal pose up to a tolerance of \SI{2}{cm} at some point during the episode and then dropping the cube flat on the ground (up to a tolerance of \SI{1}{cm}). Smoothing the actions before applying them clearly helps with avoiding dropping, in particular later in the episode. Note, however, that the smoothing has to be applied already during training in simulation for the policy to adapt to it. Despite the action smoothing the dropping rate does not reach zero. This can be partly attributed to unstable grasps that lead to slipping on the real robot.


\subsection{Using the Datasets}\label{sec:code} %

We provide an easy-to-use Python package that is compatible with a popular collection of offline RL datasets \cite{fu2020d4rl}. To use the datasets, it is sufficient to clone the repository, instantiate the desired environment and call a method that returns the dataset. The correct dataset is then downloaded automatically. We also provide options for filtering the observations and for converting them to flat arrays.

The following example code demonstrates how (parts of) a dataset can be loaded:
\begin{minted}[bgcolor=codebg]{python}
import gymnasium as gym

import numpy as np

import trifinger_rl_datasets


env = gym.make(
    "trifinger-cube-push-real-expert-v0",
    disable_env_checker=True,
    visualization=False,
)

# load data at timesteps specified in array
dataset_part = env.get_dataset(indices=np.array([42, 1000, 5000]))

# load data corresponding to a range of timesteps
dataset_part = env.get_dataset(rng=(1000, 2000))

# load the whole dataset
dataset = env.get_dataset()
\end{minted}

For installation instructions and further details we refer to the repository of the Python package at \url{https://github.com/rr-learning/trifinger_rl_datasets}.

\subsection{Submitting a policy to the robot cluster}

Access to the robot cluster can be requested at \url{https://webdav.tuebingen.mpg.de/trifinger/}. The policy has to be implemented in a GitHub repository following a fixed interface. We recommend adapting the example package available at \url{https://github.com/rr-learning/trifinger-rl-example}.

\clearpage
\section{Training}\label{sup:training}

\subsection{Training Expert Policies in Simulation}\label{subsec:training_expert_policies_sim} %

In addition to the modifications mentioned in section \ref{sec:training_expert_policies}, we also ported training from Isaac Gym 2 to the current version Isaac Gym 3 \citep{makoviychuk2021isaac}. We furthermore increased the torque range from $[\SI{-0.36}{Nm}, \SI{0.36}{Nm}]$ to $[\SI{-0.397}{Nm}, \SI{0.397}{Nm}]$ to make sure the fingers are strong enough to lift the cube when supporting it from below. To make the training environment resemble the data collection setting on the real robots, we furthermore implemented an adjustable delay between when an observation is received and when the action based on this observation is applied for the first time. The success rates and returns reached after training are shown in \tabref{tab:isaac-performance}.

\begin{table}
    \centering
    \caption{Performance of the expert policies in the Isaac Gym environment. The numbers for the success rate and return are not directly comparable to those in the PyBullet simulator and on the real robot for two reasons: (i) In Isaac Gym Lift episodes last for \SI{15}{s} instead of \SI{30}{s}, and (ii) the return in the Isaac Gym environment is contains auxiliary reward terms as described in \citet{Allshire21} and section \ref{sec:training_expert_policies}.}
    \label{tab:isaac-performance}
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{l|ccc}
    \toprule
     & success rate & return & \#training steps \\
    \midrule
    Push & 0.99  & 629 &   $0.83\cdot 10^9$\\
    Lift & 0.87 & 589 & $1.72\cdot 10^9$\\
    \bottomrule
    \end{tabular}
    \end{adjustbox}
\end{table}

Note that the success rates we give for the lifting task on the real robot are not directly comparable to the ones reported in \citet{Allshire21} as our lifting task is more challenging. While \citet{Allshire21} evaluate success after \SI{60}{s}\footnote{Personal communication}, we evaluate after \SI{30}{s}. As policies usually need several attempts to flip the cube to roughly the correct orientation and for picking it up, the success rate after \SI{30}{s} is lower in general. We chose the shorter episode length because it is, in principle, sufficient to solve the task and because we wanted to avoid episodes which consist, to a large part, of the cube being held in place. Moreover, unlike \citet{Allshire21}, we do not push the cube to the center of the arena before each episode but only remove it from the barrier.

\subsection{Training with Offline RL on the Datasets} %

We use the implementations of BC, CRR, AWAC, CQL and IQL provided by the open-source library \textsc{d3rlpy}~\citep{SI21}. The code is available at \url{https://github.com/takuseno/d3rlpy} and the documentation can be found at \url{https://d3rlpy.readthedocs.io/}. For our experiments we used versions 1.1.0 and 1.1.1 of \textsc{d3rlpy}. The used hyperparameters and the performed optimization are discussed in the next section.

\subsection{Hyperparameters}\label{sup:hyperparameters}
We performed a grid search over hyperparameters for all algorithms as documented in \tabref{tab:grid-search-procedure}. 
The hyperparameter setting with the highest performance in terms of final average return on Lift-\DLiftSimMix was selected, as listed in \tabref{tab:optimal-hyperparams:lift-sim-new-mixed}. In the paper, the results with optimized parameters are marked with a $^\dagger$.
Otherwise, the default parameters were used, as listed in \tabref{tab:default-hyperparams}. 

The rest of the section contains a detailed analysis of the grid search.


\begin{table}[htp]
  \centering
  \caption{{\bf Hyperparameter grid search (on dataset Lift-\DLiftSimMix)}. Each algorithm is trained with the same two seeds (sampled uniformly at random).}\label{tab:grid-search-procedure}
  \begin{adjustbox}{max width=.9\textwidth}
  \begin{tabular}{l|c}
    \toprule
    Algorithms & Parameters\\
    \midrule
    AWAC & \multicolumn{1}{p{12cm}}{ \{(actor\_learning\_rate=R, critic\_learning\_rate=R) : R$\in$\{1.5E-4, 3.0E-4, 6.0E-4\}\}; \newline
                                    batch\_size$\in\{256, 512\}$; lam$\in\{0.3, 1.0, 3.0\}$ }\\\\

    BC & \multicolumn{1}{p{12.5cm}}{learning\_rate=\{1.5E-4; 3.0E-4; 6.0E-4\}; batch\_size$\in\{256, 512\}$ }\\\\

    CRR & \multicolumn{1}{p{12.5cm}}{ \{(actor\_learning\_rate=R, critic\_learning\_rate=R) : R$\in$\{1.5E-4, 3.0E-4, 6.0E-4\}\}; \newline batch\_size$\in\{256, 512\}$; beta$\in\{0.25,1.0,4.0\}$ }\\\\

    CQL & \multicolumn{1}{p{12.5cm}}{ \{(actor\_learning\_rate=R, critic\_learning\_rate=3.0*R, initial\_alpha=T[1], \newline
                                    alpha\_learning\_rate=\{R if T[0] else 0\}, temp\_learning\_rate=R, alpha\_threshold=T[2]) : \newline
                                    R$\in$\{5.0E-5, 1.0E-4\}, T$\in$\{[true, 1.0, 1.0], [true, 1.0, 5.0], [true, 1.0, 10.0], \newline
                                    [false, 0.3, 10.0], [false, 1.0, 10.0], [false, 3.0, 10.0]\} \}; \newline
                                    conservative\_weight$\in$\{2.5, 5.0, 10.0, 20.0\} }\\\\

    IQL & \multicolumn{1}{p{12.5cm}}{ \{(actor\_learning\_rate=R, critic\_learning\_rate=R) : R$\in$\{1.5E-4, 3.0E-4, 6.0E-4\}\}; \newline
                                    batch\_size$\in\{256, 512\}$; expectile$\in\{0.7,0.8,0.9\}$; weight\_temp$\in\{3.0, 10.0\}$ }\\
    \bottomrule
  \end{tabular}
  \end{adjustbox}
\end{table}


\begin{table}[htp]
  \centering
  \caption{{\bf Optimized Hyperparameters using grid search} \tabref{tab:grid-search-procedure}.}\label{tab:optimal-hyperparams:lift-sim-new-mixed}
  \begin{adjustbox}{max width=.9\textwidth}
  \begin{tabular}{l|c}
    \toprule
    Algorithms & Parameters\\
    \midrule
    AWAC$^{\dagger}$ & \multicolumn{1}{p{12cm}}{ actor\_learning = critic\_learning\_rate = 0.00015; batch\_size=256; lam=3.0}\\\\

    BC$^{\dagger}$ & \multicolumn{1}{p{12.5cm}}{learning\_rate=0.00015; batch\_size=512 }\\\\

    CRR$^{\dagger}$ & \multicolumn{1}{p{12.5cm}}{ actor\_learning = critic\_learning\_rate = 0.00015; batch\_size=256; beta=1.0}\\\\

    CQL$^{\dagger}$ & \multicolumn{1}{p{12.5cm}}{ actor\_learning\_rate=0.0001; critic\_learning\_rate=0.0003; initial\_alpha=1.0; \newline
    conservative\_weight=20.0; alpha\_learning\_rate=0.0; action\_scaler: minmax } \\\\

    IQL$^{\dagger}$ & \multicolumn{1}{p{12.5cm}}{ actor\_learning\_rate = critic\_learning\_rate= 0.00015; batch\_size=256; \newline expectile=0.9; weight\_temp=3.0 }\\
    \bottomrule
  \end{tabular}
  \end{adjustbox}
\end{table}


\begin{table}[htp]
  \centering
  \caption{{\bf Default hyperparameters}. All algorithms except BC (with batch\_size=100 and without a critic) have batch\_size=256 and n\_critics=2.
  Note that due to bad performance we optimized CQL's parameters on Push-\DPushSimExp with an extensive grid search as shown in \figref{fig:histogram-CQL-400}. For all other algorithms, we used the default values of the implementation.}
  \label{tab:default-hyperparams}
  \begin{adjustbox}{max width=.9\textwidth}
  \begin{tabular}{l|c}
    \toprule
    Algorithms & Parameters\\
    \midrule
    AWAC & \multicolumn{1}{p{12cm}}{ actor\_learning = critic\_learning\_rate = 0.0003; batch\_size=1024; lam=1.0 }\\\\

    BC & \multicolumn{1}{p{12.5cm}}{learning\_rate=0.001; batch\_size=100 }\\\\

    CRR & \multicolumn{1}{p{12.5cm}}{ actor\_learning = critic\_learning\_rate = 0.0003; batch\_size=256; beta=1.0}\\\\

    CQL & \multicolumn{1}{p{12.5cm}}{ actor\_learning\_rate=0.0001; critic\_learning\_rate=0.0003; initial\_alpha=1.0; \newline
    conservative\_weight=20.0; alpha\_learning\_rate=0.0; action\_scaler: minmax } \\\\

    IQL & \multicolumn{1}{p{12.5cm}}{ actor\_learning\_rate = critic\_learning\_rate= 0.0003; batch\_size=256; \newline expectile=0.7; weight\_temp=3.0 }\\
    \bottomrule
  \end{tabular}
  \end{adjustbox}
\end{table}

\clearpage
In \figref{fig:histogram-gridsearch-lift} we present the returns and success rates for each of the hyperparameter settings for the Lift task in simulation. We see that the different algorithms have very different sensitivity to their hyperparameters. For CRR a large fraction of parameters leads to good results. For AWAC the sensitivity is a bit higher. IQL seems to degrade more gracefully with changed parameters. For CQL we were unable to find good hyperparameters, despite running 48 configurations.
The performance of the individual runs over training time are shown in \figref{fig:gridseach-timeevo-lift}. 

% Figure environment removed


On the Push task, we ran an even larger hyperparameter scan with 405 configurations for CQL, as presented in \figref{fig:histogram-CQL-400}. 
Even for this much simpler task, we see that the majority of parameters yield low success rates.
In addition, we note that the training became quite unstable for alpha\_lr > 0.0 and due to this, we conducted our main grid-search with alpha\_lr = 0.0.
Then, we chose the best hyperparameter configuration as the default setting for CQL.

% Figure environment removed


% Figure environment removed


\newpage



\section{Policy Evaluation}
\label{sup:policy_evaluation}

\subsection{Evaluation on the Real System}
\label{sup:eval_real}

On the real platforms, we report numbers for 48 trials for the Push task and 36 trials for the Lift task, each for 5 training seeds. Each algorithm and seed is evaluated on \( 5 \) to \( 6 \) robots with the same set of \( 6 \) (for the Lift task) to \( 8 \) (for the Push task) goals per robot (computed from the robot ID). The success rate and return of each algorithm and seed is computed from the resulting trials. The mean and standard deviations of the success rates and returns of each algorithm is computed across seeds.

\subsection{Evaluation in the Simulator}
\label{sup:eval_sim}

In the simulated environment, we perform 100 testing episodes per final policy for 5 independent training runs.





