{
  "title": "Towards Video Anomaly Retrieval from Video Anomaly Detection: New Benchmarks and Model",
  "authors": [
    "Peng Wu",
    "Jing Liu",
    "Xiangteng He",
    "Yuxin Peng",
    "Peng Wang",
    "Yanning Zhang"
  ],
  "submission_date": "2023-07-24T06:22:37+00:00",
  "revised_dates": [
    "2024-02-29T01:28:34+00:00"
  ],
  "abstract": "Video anomaly detection (VAD) has been paid increasing attention due to its potential applications, its current dominant tasks focus on online detecting anomalies% at the frame level, which can be roughly interpreted as the binary or multiple event classification. However, such a setup that builds relationships between complicated anomalous events and single labels, e.g., ``vandalism'', is superficial, since single labels are deficient to characterize anomalous events. In reality, users tend to search a specific video rather than a series of approximate videos. Therefore, retrieving anomalous events using detailed descriptions is practical and positive but few researches focus on this. In this context, we propose a novel task called Video Anomaly Retrieval (VAR), which aims to pragmatically retrieve relevant anomalous videos by cross-modalities, e.g., language descriptions and synchronous audios. Unlike the current video retrieval where videos are assumed to be temporally well-trimmed with short duration, VAR is devised to retrieve long untrimmed videos which may be partially relevant to the given query. To achieve this, we present two large-scale VAR benchmarks, UCFCrime-AR and XDViolence-AR, constructed on top of prevalent anomaly datasets. Meanwhile, we design a model called Anomaly-Led Alignment Network (ALAN) for VAR. In ALAN, we propose an anomaly-led sampling to focus on key segments in long untrimmed videos. Then, we introduce an efficient pretext task to enhance semantic associations between video-text fine-grained representations. Besides, we leverage two complementary alignments to further match cross-modal contents. Experimental results on two benchmarks reveal the challenges of VAR task and also demonstrate the advantages of our tailored method. Captions are publicly released at https://github.com/Roc-Ng/VAR.",
  "categories": [
    "cs.CV",
    "cs.AI"
  ],
  "primary_category": "cs.CV",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.12545",
  "pdf_url": null,
  "comment": "This work has been accepted to the IEEE TIP. Copyright has been transferred",
  "num_versions": null,
  "size_before_bytes": 12347060,
  "size_after_bytes": 273307
}