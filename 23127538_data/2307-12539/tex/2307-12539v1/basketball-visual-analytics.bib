
@article{losadaBKVizBasketballVisual2016,
	title = {{BKViz}: {A} {Basketball} {Visual} {Analysis} {Tool}},
	volume = {36},
	issn = {0272-1716, 1558-1756},
	shorttitle = {{BKViz}},
	url = {https://ieeexplore.ieee.org/document/7750529/},
	doi = {10.1109/MCG.2016.124},
	language = {en},
	number = {6},
	urldate = {2020-05-15},
	journal = {IEEE {CG\&A}},
	author = {Losada, Antonio G. and Theron, Roberto and Benito, Alejandro},
	month = nov,
	year = {2016},
	keywords = {Basketball},
	pages = {58--68},
	file = {Losada et al. - 2016 - BKViz A Basketball Visual Analysis Tool.pdf:/Users/ticalin/Zotero/storage/GGY85A23/Losada et al. - 2016 - BKViz A Basketball Visual Analysis Tool.pdf:application/pdf},
}

@article{steinBringItPitch2018,
	title = {Bring {It} to the {Pitch}: {Combining} {Video} and {Movement} {Data} to {Enhance} {Team} {Sport} {Analysis}},
	volume = {24},
	issn = {1077-2626},
	shorttitle = {Bring {It} to the {Pitch}},
	url = {http://ieeexplore.ieee.org/document/8019849/},
	doi = {10.1109/TVCG.2017.2745181},
	abstract = {Analysts in professional team sport regularly perform analysis to gain strategic and tactical insights into player and team behavior. Goals of team sport analysis regularly include identiﬁcation of weaknesses of opposing teams, or assessing performance and improvement potential of a coached team. Current analysis workﬂows are typically based on the analysis of team videos. Also, analysts can rely on techniques from Information Visualization, to depict e.g., player or ball trajectories. However, video analysis is typically a time-consuming process, where the analyst needs to memorize and annotate scenes. In contrast, visualization typically relies on an abstract data model, often using abstract visual mappings, and is not directly linked to the observed movement context anymore. We propose a visual analytics system that tightly integrates team sport video recordings with abstract visualization of underlying trajectory data. We apply appropriate computer vision techniques to extract trajectory data from video input. Furthermore, we apply advanced trajectory and movement analysis techniques to derive relevant team sport analytic measures for region, event and player analysis in the case of soccer analysis. Our system seamlessly integrates video and visualization modalities, enabling analysts to draw on the advantages of both analysis forms. Several expert studies conducted with team sport analysts indicate the effectiveness of our integrated approach.},
	language = {en},
	number = {1},
	urldate = {2020-05-15},
	journal = {IEEE {TVCG}},
	author = {Stein, Manuel and Janetzko, Halldor and Lamprecht, Andreas and Breitkreutz, Thorsten and Zimmermann, Philipp and Goldlucke, Bastian and Schreck, Tobias and Andrienko, Gennady and Grossniklaus, Michael and Keim, Daniel A.},
	month = jan,
	year = {2018},
	keywords = {Soccer, Video Overlay, Computer Vision},
	pages = {13--22},
	file = {Stein et al. - 2018 - Bring It to the Pitch Combining Video and Movemen.pdf:/Users/ticalin/Zotero/storage/ZCDYQ5M3/Stein et al. - 2018 - Bring It to the Pitch Combining Video and Movemen.pdf:application/pdf},
}

@inproceedings{stein2018,
  author    = {Manuel Stein and
               Thorsten Breitkreutz and
               Johannes H{\"{a}}ussler and
               Daniel Seebacher and
               Christoph Niederberger and
               Tobias Schreck and
               Michael Grossniklaus and
               Daniel A. Keim and
               Halldor Janetzko},
  title     = {{Revealing the Invisible: Visual Analytics and Explanatory Storytelling
               for Advanced Team Sport Analysis}},
  booktitle = {Proc. of BDVA},
  pages     = {1--9},
  publisher = {{IEEE}},
  year      = {2018}
}

@article{DBLP:journals/cga/SteinJBSSGCK16,
  author    = {Manuel Stein and
               Halldor Janetzko and
               Thorsten Breitkreutz and
               Daniel Seebacher and
               Tobias Schreck and
               Michael Grossniklaus and
               Iain D. Couzin and
               Daniel A. Keim},
  title     = {{Director's Cut: Analysis and Annotation of Soccer Matches}},
  journal   = {{IEEE} CG\&A},
  volume    = {36},
  number    = {5},
  pages     = {50--60},
  year      = {2016},
  doi       = {10.1109/MCG.2016.102}
}

@article{soltaniAugmentedRealityTools2020,
	title = {Augmented reality tools for sports education and training},
	volume = {155},
	issn = {03601315},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0360131520301226},
	doi = {10.1016/j.compedu.2020.103923},
	abstract = {Augmented reality (AR) provides additional information to the reality of sportspeople, and might offer supplementary advantages compared to other technologies. The goals of this study were to characterize and understand the benefits of AR in sports education and training. We reviewed Pubmed, Scopus, Web of Science, and SportDiscus databases, and discussed the results according to their role in sport (practitioner, spectator, and customer). Our results showed that different AR approaches might be used for learning and providing feedback. New rules could be introduced for reducing the gap between players with different experience levels. Additional information could also be added to improve the audience experience. We also explored the limitations of current AR systems and their efficacy in training, and provided suggestions for designing training scenarios.},
	language = {en},
	urldate = {2020-06-08},
	journal = {Computers \& Education},
	author = {Soltani, Pooya and Morice, Antoine H.P.},
	month = oct,
	year = {2020},
	pages = {103923},
	file = {Soltani and Morice - 2020 - Augmented reality tools for sports education and t.pdf:/Users/ticalin/Zotero/storage/KNYMTFRM/Soltani and Morice - 2020 - Augmented reality tools for sports education and t.pdf:application/pdf},
}

@article{andrienkoConstructingSpacesTimes2019a,
	title = {Constructing {Spaces} and {Times} for {Tactical} {Analysis} in {Football}},
	issn = {1941-0506},
	doi = {10.1109/TVCG.2019.2952129},
	abstract = {A possible objective in analyzing trajectories of multiple simultaneously moving objects, such as football players during a game, is to extract and understand the general patterns of coordinated movement in different classes of situations as they develop. For achieving this objective, we propose an approach that includes a combination of query techniques for flexible selection of episodes of situation development, a method for dynamic aggregation of data from selected groups of episodes, and a data structure for representing the aggregates that enables their exploration and use in further analysis. The aggregation, which is meant to abstract general movement patterns, involves construction of new time-homomorphic reference systems owing to iterative application of aggregation operators to a sequence of data selections. As similar patterns may occur at different spatial locations, we also propose constructing new spatial reference systems for aligning and matching movements irrespective of their absolute locations. The approach was tested in application to tracking data from two Bundesliga games of the 2018/2019 season. It enabled detection of interesting and meaningful general patterns of team behaviors in three classes of situations defined by football experts. The experts found the approach and the underlying concepts worth implementing in tools for football analysts.},
	journal = {IEEE {TVCG}},
	author = {Andrienko, Gennady and Andrienko, Natalia and Anzer, Gabriel and Bauer, Pascal and Budziak, Guido and Fuchs, Georg and Hecker, Dirk and Weber, Hendrik and Wrobel, Stefan},
	year = {2019},
	note = {Conference Name: IEEE Transactions on Visualization and Computer Graphics},
	keywords = {Data mining, Data visualization, Aggregates, Companies, coordinated movement, football, Games, movement data, soccer, sport analytics, Sports, Trajectory, Visual analytics},
	pages = {1--1},
	file = {IEEE Xplore Abstract Record:/Users/ticalin/Zotero/storage/68E4T959/8894420.html:text/html;Andrienko et al_2019_Constructing Spaces and Times for Tactical Analysis in Football.pdf:/Users/ticalin/Zotero/storage/RXBKKTDN/Andrienko et al_2019_Constructing Spaces and Times for Tactical Analysis in Football.pdf:application/pdf},
}

@article{andrienkoVisualAnalysisPressure2017,
	title = {Visual analysis of pressure in football},
	volume = {31},
	issn = {1384-5810, 1573-756X},
	url = {http://link.springer.com/10.1007/s10618-017-0513-2},
	doi = {10.1007/s10618-017-0513-2},
	language = {en},
	number = {6},
	urldate = {2020-06-10},
	journal = {Data Mining and Knowledge Discovery},
	author = {Andrienko, Gennady and Andrienko, Natalia and Budziak, Guido and Dykes, Jason and Fuchs, Georg and von Landesberger, Tatiana and Weber, Hendrik},
	month = nov,
	year = {2017},
	pages = {1793--1839},
	file = {Andrienko et al. - 2017 - Visual analysis of pressure in football.pdf:/Users/ticalin/Zotero/storage/8PE6HQT3/Andrienko et al. - 2017 - Visual analysis of pressure in football.pdf:application/pdf},
}

@inproceedings{goldsberryCourtVisionNewVisual2012,
	title = {{CourtVision}: {New} {Visual} and {Spatial} {Analytics} for the {NBA}},
	abstract = {This paper investigates spatial and visual analytics as means to enhance basketball expertise. We introduce CourtVision, a new ensemble of analytical techniques designed to quantify, visualize, and communicate spatial aspects of NBA performance with unprecedented precision and clarity. We propose a new way to quantify the shooting range of NBA players and present original methods that measure, chart, and reveal differences in NBA players’ shooting abilities. We conduct a case study, which applies these methods to 1) inspect spatially aware shot site performances for every player in the NBA, and 2) to determine which players exhibit the most potent spatial shooting behaviors. We present evidence that Steve Nash and Ray Allen have the best shooting range in the NBA. We conclude by suggesting that visual and spatial analysis represent vital new methodologies for NBA analysts.},
	language = {en},
	author = {Goldsberry, Kirk},
	year = {2012},
	pages = {1--7},
	file = {Goldsberry - 2012 - CourtVision New Visual and Spatial Analytics for .pdf:/Users/ticalin/Zotero/storage/TTEIRHU6/Goldsberry - 2012 - CourtVision New Visual and Spatial Analytics for .pdf:application/pdf},
}

@inproceedings{theronVisualAnalysisTimeMotion2010,
	address = {Berlin, Heidelberg},
	title = {Visual {Analysis} of {Time}-{Motion} in {Basketball} {Games}},
	volume = {6133},
	isbn = {978-3-642-13543-9 978-3-642-13544-6},
	url = {http://link.springer.com/10.1007/978-3-642-13544-6_19},
	doi = {10.1007/978-3-642-13544-6_19},
	abstract = {This work aims to facilitate the task of basketball coaches, by means of the visualization and analysis of the players’ movements in the court. This is possible thanks to the use of Global Positioning System (GPS) devices that generate data of the position of the player, almost in real time. The main objective of our proposal consists on the tracking, both statistics and kinematics, of a basketball player due to the physical activity developed during a match. The comparison of the data from several players or between two teams also will improve the performance and tactical capacity of players and trainers.},
	language = {en},
	urldate = {2020-06-10},
	booktitle = {Smart {Graphics}},
	publisher = {Springer Berlin Heidelberg},
	author = {Therón, Roberto and Casares, Laura},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Taylor, Robyn and Boulanger, Pierre and Krüger, Antonio and Olivier, Patrick},
	year = {2010},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {196--207},
	file = {Therón and Casares - 2010 - Visual Analysis of Time-Motion in Basketball Games.pdf:/Users/ticalin/Zotero/storage/EDW9WWBJ/Therón and Casares - 2010 - Visual Analysis of Time-Motion in Basketball Games.pdf:application/pdf},
}

@inproceedings{malikCohortComparisonEvent2015,
	address = {Atlanta, Georgia, USA},
	title = {Cohort {Comparison} of {Event} {Sequences} with {Balanced} {Integration} of {Visual} {Analytics} and {Statistics}},
	isbn = {978-1-4503-3306-1},
	url = {http://dl.acm.org/citation.cfm?doid=2678025.2701407},
	doi = {10.1145/2678025.2701407},
	abstract = {Finding the differences and similarities between two datasets is a common analytics task. With temporal event sequence data, this task is complex because of the many ways single events and event sequences can differ between the two datasets (or cohorts) of records: the structure of the event sequences (e.g., event order, co-occurring events, or event frequencies), the attributes of events and records (e.g., patient gender), or metrics about the timestamps themselves (e.g., event duration). In exploratory analyses, running statistical tests to cover all cases is time-consuming and determining which results are signiﬁcant becomes cumbersome. Current analytics tools for comparing groups of event sequences emphasize a purely statistical or purely visual approach for comparison. This paper presents a taxonomy of metrics for comparing cohorts of temporal event sequences, showing that the problem-space is bounded. We also present a visual analytics tool, CoCo (for “Cohort Comparison”), which implements balanced integration of automated statistics with an intelligent user interface to guide users to signiﬁcant, distinguishing features between the cohorts. Lastly, we describe two early case studies: the ﬁrst with a research team studying medical team performance in the emergency department and the second with pharmacy researchers.},
	language = {en},
	urldate = {2020-06-10},
	booktitle = {Proceedings of the 20th {International} {Conference} on {Intelligent} {User} {Interfaces} - {IUI} '15},
	publisher = {ACM Press},
	author = {Malik, Sana and Du, Fan and Monroe, Megan and Onukwugha, Eberechukwu and Plaisant, Catherine and Shneiderman, Ben},
	year = {2015},
	pages = {38--49},
	file = {Malik et al. - 2015 - Cohort Comparison of Event Sequences with Balanced.pdf:/Users/ticalin/Zotero/storage/K9AIV6PS/Malik et al. - 2015 - Cohort Comparison of Event Sequences with Balanced.pdf:application/pdf},
}

@inproceedings{pallotAugmentedSportExploring2013,
	address = {Laval, France},
	title = {Augmented sport: exploring collective user experience},
	isbn = {978-1-4503-1875-4},
	shorttitle = {Augmented sport},
	url = {http://dl.acm.org/citation.cfm?doid=2466816.2466821},
	doi = {10.1145/2466816.2466821},
	language = {en},
	urldate = {2020-06-22},
	booktitle = {Proceedings of the {Virtual} {Reality} {International} {Conference} on {Laval} {Virtual} - {VRIC} '13},
	publisher = {ACM Press},
	author = {Pallot, Marc and Eynard, Remy and Poussard, Benjamin and Christmann, Olivier and Richir, Simon},
	year = {2013},
	pages = {1},
	file = {Pallot et al_2013_Augmented sport.pdf:/Users/ticalin/Zotero/storage/3WJ99HLV/Pallot et al_2013_Augmented sport.pdf:application/pdf},
}

@inproceedings{zhiGameBotVisualizationaugmentedChatbot2020,
	address = {Honolulu HI USA},
	title = {{GameBot}: {A} {Visualization}-augmented {Chatbot} for {Sports} {Game}},
	isbn = {978-1-4503-6819-3},
	shorttitle = {{GameBot}},
	url = {https://dl.acm.org/doi/10.1145/3334480.3382794},
	doi = {10.1145/3334480.3382794},
	abstract = {The major sports leagues, including The National Basketball Association (NBA) and the EPL (the English Premier League), are adopting conversational systems (chatbots) as an innovative outlet to deliver game information and engage fans. However, current sports chatbots only provide scores and game highlight videos, which are often inadequate for statistical data related requests. We present GameBot, an interactive chatbot for sports fans to explore game statistical data. GameBot features (1) the direct answers to user’s stats-related questions, and (2) the use of data visualizations as supporting context for sports fans’ stats-related questions.},
	language = {en},
	urldate = {2021-09-24},
	booktitle = {Extended {Abstracts} of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Zhi, Qiyu and Metoyer, Ronald},
	month = apr,
	year = {2020},
	pages = {1--7},
	file = {Zhi and Metoyer - 2020 - GameBot A Visualization-augmented Chatbot for Spo.pdf:/Users/ticalin/Zotero/storage/I5QZQFBJ/Zhi and Metoyer - 2020 - GameBot A Visualization-augmented Chatbot for Spo.pdf:application/pdf},
}

@inproceedings{metoyerCouplingStoryVisualization2018,
	address = {Tokyo Japan},
	title = {Coupling {Story} to {Visualization}: {Using} {Textual} {Analysis} as a {Bridge} {Between} {Data} and {Interpretation}},
	isbn = {978-1-4503-4945-1},
	shorttitle = {Coupling {Story} to {Visualization}},
	url = {https://dl.acm.org/doi/10.1145/3172944.3173007},
	doi = {10.1145/3172944.3173007},
	abstract = {Online writers and journalism media are increasingly combining visualization (and other multimedia content) with narrative text to create narrative visualizations. Often, however, the two elements are presented independently of one another. We propose an approach to automatically integrate text and visualization elements. We begin with a writer’s narrative that presumably can be supported with visual data evidence. We leverage natural language processing, quantitative narrative analysis, and information visualization to (1) automatically extract narrative components (who, what, when, where) from data-rich stories, and (2) integrate the supporting data evidence with the text to develop a narrative visualization. We also employ bidirectional interaction from text to visualization and visualization to text to support reader exploration in both directions. We demonstrate the approach with a case study in the data-rich ﬁeld of sports journalism.},
	language = {en},
	urldate = {2021-09-24},
	booktitle = {23rd {International} {Conference} on {Intelligent} {User} {Interfaces}},
	publisher = {ACM},
	author = {Metoyer, Ronald and Zhi, Qiyu and Janczuk, Bart and Scheirer, Walter},
	month = mar,
	year = {2018},
	pages = {503--507},
	file = {Metoyer et al. - 2018 - Coupling Story to Visualization Using Textual Ana.pdf:/Users/ticalin/Zotero/storage/CNE8JBNP/Metoyer et al. - 2018 - Coupling Story to Visualization Using Textual Ana.pdf:application/pdf},
}

@article{perinSoccerStoriesKickoffVisual2013,
	title = {{SoccerStories}: {A} {Kick}-off for {Visual} {Soccer} {Analysis}},
	volume = {19},
	issn = {1077-2626},
	shorttitle = {{SoccerStories}},
	url = {http://ieeexplore.ieee.org/document/6634087/},
	doi = {10.1109/TVCG.2013.192},
	abstract = {This article presents SoccerStories, a visualization interface to support analysts in exploring soccer data and communicating interesting insights. Currently, most analyses on such data relate to statistics on individual players or teams. However, soccer analysts we collaborated with consider that quantitative analysis alone does not convey the right picture of the game, as context, player positions and phases of player actions are the most relevant aspects. We designed SoccerStories to support the current practice of soccer analysts and to enrich it, both in the analysis and communication stages. Our system provides an overview+detail interface of game phases, and their aggregation into a series of connected visualizations, each visualization being tailored for actions such as a series of passes or a goal attempt. To evaluate our tool, we ran two qualitative user studies on recent games using SoccerStories with data from one of the world’s leading live sports data providers. The ﬁrst study resulted in a series of four articles on soccer tactics, by a tactics analyst, who said he would not have been able to write these otherwise. The second study consisted in an exploratory follow-up to investigate design alternatives for embedding soccer phases into word-sized graphics. For both experiments, we received a very enthusiastic feedback and participants consider further use of SoccerStories to enhance their current workﬂow.},
	language = {en},
	number = {12},
	urldate = {2021-10-05},
	journal = {IEEE {TVCG}},
	author = {Perin, Charles and Vuillemot, Romain and Fekete, Jean-Daniel},
	month = dec,
	year = {2013},
	pages = {2506--2515},
	file = {Perin et al. - 2013 - SoccerStories A Kick-off for Visual Soccer Analys.pdf:/Users/ticalin/Zotero/storage/2CDBPZ3T/Perin et al. - 2013 - SoccerStories A Kick-off for Visual Soccer Analys.pdf:application/pdf},
}

@article{sisnerosExpandingPlusMinusVisual,
	title = {Expanding {Plus}-{Minus} for {Visual} and {Statistical} {Analysis} of {NBA} {Box}-{Score} {Data}},
	abstract = {In this work we present an augmentation of the plus-minus statistic as well as a new visual platform for exploring our derived values. Speciﬁcally, we apply the concept of measuring impact via differentials to all box score statistics and expand the focus of analysis from a player to a team. That is, on a per-game basis for a stat, we are concerned only with how many more or less than an opponent a team accumulates of that stat. We consider traditional plus-minus numbers at the team level as a measure of the quality of a win/loss for a team; this creates several interesting opportunities for evaluating the impacts of player accomplishments numerically at the team level. We will detail PluMP, the plus-minus plot, and provide illuminating examples found in 2012-2013 NBA box score data. Further, we will provide a representative example of more general analysis that follows directly from our paradigm.},
	language = {en},
	author = {Sisneros, Robert and Moer, Mark Van},
	pages = {8},
	file = {Sisneros and Moer - Expanding Plus-Minus for Visual and Statistical An.pdf:/Users/ticalin/Zotero/storage/HQC9DEK3/Sisneros and Moer - Expanding Plus-Minus for Visual and Statistical An.pdf:application/pdf},
}

@article{beshaiBucketsBasketballShot,
	title = {Buckets: {Basketball} {Shot} {Visualization}},
	abstract = {Basketball fans are often interested in learning more about their favourite players and most tools today provide long tables of numbers to support these users. However, basketball is a spatial game and can beneﬁt from visualizations to simplify and clarify the communication of a given player’s performance. I created Buckets to ﬁll this role. Buckets supports viewing details about individual players, comparing multiple players, and exploring league trends through a variety of interactive charts designed to be used by fans and casual analysts. The focus is on shooting performance and includes three novel chart uses: a ﬁlterable shot chart, a simpliﬁed distance-from-hoop chart, and a left-right court position partitioned chart. Buckets was shared online as an informal evaluation and was positively received.},
	language = {en},
	author = {Beshai, Peter},
	pages = {19},
	file = {Beshai - Buckets Basketball Shot Visualization.pdf:/Users/ticalin/Zotero/storage/VS7GGPV8/Beshai - Buckets Basketball Shot Visualization.pdf:application/pdf},
}

@article{luceyHowGetOpen,
	title = {“{How} to {Get} an {Open} {Shot}”: {Analyzing} {Team} {Movement} in},
	abstract = {In this paper, we use ball and player tracking data from STATS SportsVU from the 2012-2013 NBA season to analyze offensive and defensive formations of teams. We move beyond current analysis that uses only play-by-play event-driven statistics (i.e., rebounds, shots) and look at the spatiotemporal changes in a team’s formation. A major concern, which also gives a clue to unlocking this problem, is that of permutations caused by the constant movement and interchanging of positions by players. In this paper, we use a method that represents a team via “role” which is immune to the problem of permutations. We demonstrate the utility of our approach by analyzing all the plays that resulted in a 3point shot attempt in the 2012-2013 NBA season. We analyzed close to 20,000 shots and found that when a player is “open” the shooting percentage is around 40\%, compared to a “pressured” shot which is close to 32\%. There is nothing groundbreaking behind this finding (i.e., putting more defensive pressure on the shooter reduces shooting percentages) but finding how teams get shooters open is. Using our method, we show that the amount of defensive role-swaps are predictive of getting an open-shot and this measure can be used to measure the defensive effectiveness of a team. Additionally, our role representation allows for large-scale retrieval of plays by using the tracking data as the input query rather than a text label - this “video Google” approach allows for quick and accurate play retrieval.},
	language = {en},
	author = {Lucey, Patrick and Bialkowski, Alina and Carr, Peter and Yue, Yisong and Matthews, Iain and Research, Disney},
	pages = {8},
	file = {Lucey et al. - “How to Get an Open Shot” Analyzing Team Movement.pdf:/Users/ticalin/Zotero/storage/YPFJN6D3/Lucey et al. - “How to Get an Open Shot” Analyzing Team Movement.pdf:application/pdf},
}

@article{duSurveyCompetitiveSports2021,
	title = {A survey of competitive sports data visualization and visual analysis},
	volume = {24},
	issn = {1343-8875, 1875-8975},
	url = {https://link.springer.com/10.1007/s12650-020-00687-2},
	doi = {10.1007/s12650-020-00687-2},
	abstract = {Competitive sports data visualization is an increasingly important research direction in the ﬁeld of information visualization. It is also an important basis for studying human behavioral pattern and activity habits. In this paper, we provide a taxonomy of sports data visualization and summarize the state-of-the-art research from four aspects of data types, main tasks and visualization techniques and visual analysis. Speciﬁcally, we ﬁrst put sports data into two categories: spatiotemporal information and statistical information. Then, we propose three main tasks for competitive sports data visualization: feature presentation, feature comparison and feature prediction. Furthermore, we classify competitive sports data visualization techniques based on data characteristics into ﬁve categories: high-dimensional data visualization, timeseries visualization, graph (network) visualization, glyph visualization and other visualization, and we analyze the relationship between major tasks and visualization techniques. We also introduce visual analysis research work of competitive sports, propose the features and limitations of competitive sports data, summarize multimedia visualization in competitive sports and ﬁnally discuss visual analysis evaluation. In this survey, we attempt to help readers to ﬁnd appropriate techniques for different data types and different tasks. Our paper also intends to provide guidelines and references for future researchers when they study human behavior and moving patterns.},
	language = {en},
	number = {1},
	urldate = {2021-10-05},
	journal = {Journal of Visualization},
	author = {Du, Meng and Yuan, Xiaoru},
	month = feb,
	year = {2021},
	pages = {47--67},
	file = {Du and Yuan - 2021 - A survey of competitive sports data visualization .pdf:/Users/ticalin/Zotero/storage/TGN7AJXN/Du and Yuan - 2021 - A survey of competitive sports data visualization .pdf:application/pdf},
}

@article{bressaWhatSituationSituated2021,
	title = {What's the {Situation} with {Situated} {Visualization}? {A} {Survey} and {Perspectives} on {Situatedness}},
	issn = {1941-0506},
	shorttitle = {What's the {Situation} with {Situated} {Visualization}?},
	doi = {10.1109/TVCG.2021.3114835},
	abstract = {Situated visualization is an emerging concept within visualization, in which data is visualized in situ, where it is relevant to people. The concept has gained interest from multiple research communities, including visualization, human-computer interaction (HCI) and augmented reality. This has led to a range of explorations and applications of the concept, however, this early work has focused on the operational aspect of situatedness leading to inconsistent adoption of the concept and terminology. First, we contribute a literature survey in which we analyze 44 papers that explicitly use the term “situated visualization” to provide an overview of the research area, how it defines situated visualization, common application areas and technology used, as well as type of data and type of visualizations. Our survey shows that research on situated visualization has focused on technology-centric approaches that foreground a spatial understanding of situatedness. Secondly, we contribute five perspectives on situatedness (space, time, place, activity, and community) that together expand on the prevalent notion of situatedness in the corpus. We draw from six case studies and prior theoretical developments in HCI. Each perspective develops a generative way of looking at and working with situatedness in design and research. We outline future directions, including considering technology, material and aesthetics, leveraging the perspectives for design, and methods for stronger engagement with target audiences. We conclude with opportunities to consolidate situated visualization research.},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Bressa, Nathalie and Korsgaard, Henrik and Tabard, Aurelien and Houben, Steven and Vermeulen, Jo},
	year = {2021},
	note = {Conference Name: IEEE Transactions on Visualization and Computer Graphics},
	pages = {1--1},
	file = {IEEE Xplore Full Text PDF:/Users/ticalin/Zotero/storage/L6VET48G/Bressa et al. - 2021 - What's the Situation with Situated Visualization .pdf:application/pdf;IEEE Xplore Abstract Record:/Users/ticalin/Zotero/storage/8XKDVIFE/9552238.html:text/html},
}

@inproceedings{whiteSiteLensSituatedVisualization2009,
	address = {Boston MA USA},
	title = {{SiteLens}: situated visualization techniques for urban site visits},
	isbn = {978-1-60558-246-7},
	shorttitle = {{SiteLens}},
	url = {https://dl.acm.org/doi/10.1145/1518701.1518871},
	doi = {10.1145/1518701.1518871},
	abstract = {Urban designers and urban planners often conduct site visits prior to a design activity to search for patterns or better understand existing conditions. We introduce SiteLens, an experimental system and set of techniques for supporting site visits by visualizing relevant virtual data directly in the context of the physical site, which we call situated visualization. We address alternative visualization representations and techniques for data collection, curation, discovery, comparison, manipulation, and provenance. A real use scenario is presented and two iterations of evaluation with faculty and students from the Columbia University Graduate School of Architecture, Planning and Preservation provide directions and insight for further investigation.},
	language = {en},
	urldate = {2021-12-13},
	booktitle = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {White, Sean and Feiner, Steven},
	month = apr,
	year = {2009},
	pages = {1117--1120},
	file = {White and Feiner - 2009 - SiteLens situated visualization techniques for ur.pdf:/Users/ticalin/Zotero/storage/ZGQVXE4C/White and Feiner - 2009 - SiteLens situated visualization techniques for ur.pdf:application/pdf},
}

@article{dourishWhatWeTalk2004,
	title = {What we talk about when we talk about context},
	volume = {8},
	issn = {1617-4909, 1617-4917},
	url = {http://link.springer.com/10.1007/s00779-003-0253-8},
	doi = {10.1007/s00779-003-0253-8},
	abstract = {The emergence of ubiquitous computing as a new design paradigm poses significant challenges for HCI and interaction design. Traditionally, human-computer interaction has taken place within a constrained and well-understood domain of experience – single users sitting at desks and interacting with conventionally-designed computers employing screens, keyboards and mice for interaction. New opportunities have engendered considerable interest in “context-aware computing” –computational systems that can sense and respond to aspects of the settings in which they are used. However, considerable confusion surrounds the notion of “context” – what it means, what it includes, and what role it plays in interactive systems. This paper suggests that the representational stance implied by conventional interpretations of “context” misinterprets the role of context in everyday human activity, and proposes an alternative model that suggests different directions for design.},
	language = {en},
	number = {1},
	urldate = {2021-12-13},
	journal = {Personal and Ubiquitous Computing},
	author = {Dourish, Paul},
	month = feb,
	year = {2004},
	pages = {19--30},
	file = {Dourish - 2004 - What we talk about when we talk about context.pdf:/Users/ticalin/Zotero/storage/C3NL6M2U/Dourish - 2004 - What we talk about when we talk about context.pdf:application/pdf},
}

@article{tsaiFeasibilityStudyVirtual2020,
	title = {Feasibility {Study} on {Virtual} {Reality} {Based} {Basketball} {Tactic} {Training}},
	issn = {1941-0506},
	doi = {10.1109/TVCG.2020.3046326},
	abstract = {In this work, a VR-based basketball training system comprising a standalone VR device and a tablet is proposed. The system is intended to improve the ability of players to understand offensive tactics and to enable them to perform these tactics correctly. We compare the training effectiveness of various degrees of immersion fidelity, including a conventional basketball tactic board, a 2D monitor, and virtual reality. A multi-camera-based human tracking system is developed and built around a real-world basketball court to record and analyze the running trajectory of each player during tactical execution. The accuracy of the running path and hesitation time at each tactical step is evaluated for each participant. Furthermore, we assess several subjective measurements, including simulator sickness, presence, and sport imagery ability, to carry out a more comprehensive exploration of the feasibility of the proposed VR framework for basketball tactics training. The results indicate that the proposed system is useful for learning complex tactics. Also, high VR immersion training benefits athletes in terms of abilities related to strategic imagery.},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Tsai, Wan-Lun and Pan, Tse-Yu and Hu, Min-Chun},
	year = {2020},
	note = {Conference Name: IEEE Transactions on Visualization and Computer Graphics},
	pages = {1--1},
}

@article{tsaiFeasibilityStudyVirtual2020a,
	title = {Feasibility {Study} on {Virtual} {Reality} {Based} {Basketball} {Tactic} {Training}},
	issn = {1941-0506},
	doi = {10.1109/TVCG.2020.3046326},
	abstract = {In this work, a VR-based basketball training system comprising a standalone VR device and a tablet is proposed. The system is intended to improve the ability of players to understand offensive tactics and to enable them to perform these tactics correctly. We compare the training effectiveness of various degrees of immersion fidelity, including a conventional basketball tactic board, a 2D monitor, and virtual reality. A multi-camera-based human tracking system is developed and built around a real-world basketball court to record and analyze the running trajectory of each player during tactical execution. The accuracy of the running path and hesitation time at each tactical step is evaluated for each participant. Furthermore, we assess several subjective measurements, including simulator sickness, presence, and sport imagery ability, to carry out a more comprehensive exploration of the feasibility of the proposed VR framework for basketball tactics training. The results indicate that the proposed system is useful for learning complex tactics. Also, high VR immersion training benefits athletes in terms of abilities related to strategic imagery.},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Tsai, Wan-Lun and Pan, Tse-Yu and Hu, Min-Chun},
	year = {2020},
	note = {Conference Name: IEEE Transactions on Visualization and Computer Graphics},
	pages = {1--1},
}

@inproceedings{hongloWhoKickedBall2021,
	title = {Who kicked the ball? {Situated} {Visualization} in {On}-{Site} {Sports} {Spectating}},
	shorttitle = {Who kicked the ball?},
	doi = {10.1109/VRW52623.2021.00130},
	abstract = {With the recent technological advancements in sports broadcasting, viewers that follow a sports game through broadcast media or online are presented with an enriched experience that includes additional content such as statistics and graphics that help to follow a game. In contrast, spectators at live sporting events often miss out on this additional content.In this paper, we explore the opportunities of using situated visualization to enrich on-site sports spectating. We developed two novel situated visualization approaches for on-site sports spectating: (1) situated broadcast-styled visualization which mimics television broadcasts and (2) situated infographics which places visual elements into the environment.},
	booktitle = {2021 {IEEE} {Conference} on {Virtual} {Reality} and {3D} {User} {Interfaces} {Abstracts} and {Workshops} ({VRW})},
	author = {Hong Lo, Wei and Zollmann, Stefanie and Regenbrecht, Holger},
	month = mar,
	year = {2021},
	pages = {496--497},
	file = {Hong Lo et al. - 2021 - Who kicked the ball Situated Visualization in On-.pdf:/Users/ticalin/Zotero/storage/E7YMNTQH/Hong Lo et al. - 2021 - Who kicked the ball Situated Visualization in On-.pdf:application/pdf},
}

@inproceedings{zollmannARSpectatorExploringAugmented2019,
	address = {Brisbane, QLD, Australia},
	title = {{ARSpectator}: {Exploring} {Augmented} {Reality} for {Sport} {Events}},
	isbn = {978-1-4503-6945-9},
	shorttitle = {{ARSpectator}},
	url = {http://dl.acm.org/citation.cfm?doid=3355088.3365162},
	doi = {10.1145/3355088.3365162},
	abstract = {Augmented Reality (AR) has gained a lot of interests recently and has been used for various applications. Most of these applications are however limited to small indoor environments. Despite the wide range of large scale application areas that could highly benefit from AR usage, until now there are rarely AR applications that target such environments. In this work, we discuss how AR can be used to enhance the experience of on-site spectators at live sport events. We investigate the challenges that come with applying AR for such a large scale environment and explore state-of-the-art technology and its suitability for an on-site AR spectator experience. We also present a concept design and explore the options to implement AR applications inside large scale environments.},
	language = {en},
	urldate = {2020-05-15},
	booktitle = {{SIGGRAPH} {Asia} 2019 {Technical} {Briefs} on   - {SA} '19},
	publisher = {ACM Press},
	author = {Zollmann, Stefanie and Langlotz, Tobias and Loos, Moritz and Lo, Wei Hong and Baker, Lewis},
	year = {2019},
	keywords = {AR Registration, Fan Experience},
	pages = {75--78},
	file = {Zollmann et al. - 2019 - ARSpectator Exploring Augmented Reality for Sport.pdf:/Users/ticalin/Zotero/storage/LCF4AY2I/Zollmann et al. - 2019 - ARSpectator Exploring Augmented Reality for Sport.pdf:application/pdf},
}

@article{grubertPervasiveAugmentedReality2017,
	title = {Towards {Pervasive} {Augmented} {Reality}: {Context}-{Awareness} in {Augmented} {Reality}},
	volume = {23},
	issn = {1941-0506},
	shorttitle = {Towards {Pervasive} {Augmented} {Reality}},
	doi = {10.1109/TVCG.2016.2543720},
	abstract = {Augmented Reality is a technique that enables users to interact with their physical environment through the overlay of digital information. While being researched for decades, more recently, Augmented Reality moved out of the research labs and into the field. While most of the applications are used sporadically and for one particular task only, current and future scenarios will provide a continuous and multi-purpose user experience. Therefore, in this paper, we present the concept of Pervasive Augmented Reality, aiming to provide such an experience by sensing the user's current context and adapting the AR system based on the changing requirements and constraints. We present a taxonomy for Pervasive Augmented Reality and context-aware Augmented Reality, which classifies context sources and context targets relevant for implementing such a context-aware, continuous Augmented Reality experience. We further summarize existing approaches that contribute towards Pervasive Augmented Reality. Based our taxonomy and survey, we identify challenges for future research directions in Pervasive Augmented Reality.},
	number = {6},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Grubert, Jens and Langlotz, Tobias and Zollmann, Stefanie and Regenbrecht, Holger},
	month = jun,
	year = {2017},
	note = {Conference Name: IEEE Transactions on Visualization and Computer Graphics},
	pages = {1706--1724},
	file = {IEEE Xplore Full Text PDF:/Users/ticalin/Zotero/storage/FDJD7I88/Grubert et al. - 2017 - Towards Pervasive Augmented Reality Context-Aware.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/ticalin/Zotero/storage/3FQ2YW5E/7435333.html:text/html},
}

@article{chenGameFlowNarrativeVisualization2016,
	title = {{GameFlow}: {Narrative} {Visualization} of {NBA} {Basketball} {Games}},
	volume = {18},
	issn = {1520-9210, 1941-0077},
	shorttitle = {{GameFlow}},
	url = {http://ieeexplore.ieee.org/document/7577822/},
	doi = {10.1109/TMM.2016.2614221},
	abstract = {Although basketball games have received broad attention, the forms of game reports and webcast are purely content-based cross-media: texts, videos, snapshots, and performance ﬁgures. Analytical narrations of games that seek to compose a complete game from heterogeneous datasets are challenging for general media producers because such a composition is time-consuming and heavily depends on domain experts. In particular, an appropriate analytical commentary of basketball games requires two factors, namely, rich context and domain knowledge, which includes game events, player locations, player proﬁles, and team proﬁles, among others. This type of analytical commentary elicits a timely and effective basketball game data visualization made up of different sources of media. Existing visualizations of basketball games mainly proﬁle a particular aspect of the game. Therefore, this paper presents an expressive visualization scheme that comprehensively illustrates NBA games with three levels of details: a season level, a game level, and a session level. We reorganize a basketball game as a sequence of sessions to depict the game states and heated confrontations. We design and implement a live system that integrates multimedia NBA datasets: play-by-play text data, box score data, game video data, and action area data. We demonstrate the effectiveness of this scheme with case studies and user feedbacks.},
	language = {en},
	number = {11},
	urldate = {2020-05-15},
	journal = {IEEE Transactions on Multimedia},
	author = {Chen, Wei and Lao, Tianyi and Xia, Jing and Huang, Xinxin and Zhu, Biao and Hu, Wanqi and Guan, Huihua},
	month = nov,
	year = {2016},
	keywords = {Visual Analytics, Basketball, Game Narrator},
	pages = {2247--2256},
	file = {Chen et al. - 2016 - GameFlow Narrative Visualization of NBA Basketbal.pdf:/Users/ticalin/Zotero/storage/4K2KAB4Q/Chen et al. - 2016 - GameFlow Narrative Visualization of NBA Basketbal.pdf:application/pdf},
}

@inproceedings{zhiGameViewsUnderstandingSupporting2019,
	address = {Glasgow, Scotland Uk},
	title = {{GameViews}: {Understanding} and {Supporting} {Data}-driven {Sports} {Storytelling}},
	isbn = {978-1-4503-5970-2},
	shorttitle = {{GameViews}},
	url = {http://dl.acm.org/citation.cfm?doid=3290605.3300499},
	doi = {10.1145/3290605.3300499},
	abstract = {Various stakeholders in the sports domain rely on the analysis and presentation of sports data to derive insights. In particular, sportswriters construct game stories using statistical information; fans share their viewpoints based on the real-time stats while watching the game. In this paper, we explore how these stakeholders construct data-driven sports stories. We began by observing a sportswriter, then analyzed published sports stories, and characterized 1500 fan comments about particular sporting events. We found that their story needs were similar in some respects while quite different in others. Based on the findings, we implemented two exploratory prototypes: GameViews-Writers for sportswriters to quickly extract key game information and GameViews-Fans to support a real-time data-driven gameviewing experience for fans. We report insights from two user studies conducted with four professional sportswriters and eight sports fans, respectively. We discuss the results of these studies and present several avenues for future work.},
	language = {en},
	urldate = {2020-07-16},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}  - {CHI} '19},
	publisher = {ACM Press},
	author = {Zhi, Qiyu and Lin, Suwen and Talkad Sukumar, Poorna and Metoyer, Ronald},
	year = {2019},
	pages = {1--13},
	file = {Zhi et al. - 2019 - GameViews Understanding and Supporting Data-drive.pdf:/Users/ticalin/Zotero/storage/69X5XTY3/Zhi et al. - 2019 - GameViews Understanding and Supporting Data-drive.pdf:application/pdf},
}

@article{chenAugmentingSportsVideos2022b,
	title = {Augmenting {Sports} {Videos} with {VisCommentator}},
	volume = {28},
	issn = {1077-2626, 1941-0506, 2160-9306},
	url = {https://ieeexplore.ieee.org/document/9552848/},
	doi = {10.1109/TVCG.2021.3114806},
	language = {en},
	number = {1},
	urldate = {2022-01-18},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Chen, Zhutian and Ye, Shuainan and Chu, Xiangtong and Xia, Haijun and Zhang, Hui and Qu, Huamin and Wu, Yingcai},
	month = jan,
	year = {2022},
	pages = {824--834},
	file = {Chen et al. - 2022 - Augmenting Sports Videos with VisCommentator.pdf:/Users/ticalin/Zotero/storage/FVAAULBQ/Chen et al. - 2022 - Augmenting Sports Videos with VisCommentator.pdf:application/pdf},
}

@article{chuTIVEEVisualExploration2022,
	title = {{TIVEE}: {Visual} {Exploration} and {Explanation} of {Badminton} {Tactics} in {Immersive} {Visualizations}},
	volume = {28},
	issn = {1077-2626, 1941-0506, 2160-9306},
	shorttitle = {{TIVEE}},
	url = {https://ieeexplore.ieee.org/document/9557225/},
	doi = {10.1109/TVCG.2021.3114861},
	language = {en},
	number = {1},
	urldate = {2022-01-19},
	journal = {{IEEE} TVCG},
	author = {Chu, Xiangtong and Xie, Xiao and Ye, Shuainan and Lu, Haolin and Xiao, Hongguang and Yuan, Zeqing and Chen, Zhutian and Zhang, Hui and Wu, Yingcai},
	month = jan,
	year = {2022},
	pages = {118--128},
	file = {Chu et al. - 2022 - TIVEE Visual Exploration and Explanation of Badmi.pdf:/Users/ticalin/Zotero/storage/9LYRK9WK/Chu et al. - 2022 - TIVEE Visual Exploration and Explanation of Badmi.pdf:application/pdf},
}

@inproceedings{diefenbachHedonicHumancomputerInteraction2014,
	address = {Vancouver BC Canada},
	title = {The 'hedonic' in human-computer interaction: history, contributions, and future research directions},
	isbn = {978-1-4503-2902-6},
	shorttitle = {The 'hedonic' in human-computer interaction},
	url = {https://dl.acm.org/doi/10.1145/2598510.2598549},
	doi = {10.1145/2598510.2598549},
	abstract = {Over the recent years, the notion of a non-instrumental, hedonic quality of interactive products received growing interest. Based on a review of 151 publications, we summarize more than ten years research on the hedonic to provide an overview of definitions, assessment tools, antecedents, consequences, and correlates. We highlight a number of contributions, such as introducing experiential value to the practice of technology design and a better prediction of overall quality judgments and product acceptance. In addition, we suggest a number of areas for future research, such as providing richer, more nuanced models and tools for quantitative and qualitative analysis, more research on the consequences of using hedonic products and a better understanding of when the hedonic plays a role and when not.},
	language = {en},
	urldate = {2022-01-20},
	booktitle = {Proceedings of the 2014 conference on {Designing} interactive systems},
	publisher = {ACM},
	author = {Diefenbach, Sarah and Kolb, Nina and Hassenzahl, Marc},
	month = jun,
	year = {2014},
	pages = {305--314},
	file = {Diefenbach et al. - 2014 - The 'hedonic' in human-computer interaction histo.pdf:/Users/ticalin/Zotero/storage/5KBDCEZ4/Diefenbach et al. - 2014 - The 'hedonic' in human-computer interaction histo.pdf:application/pdf},
}

@inproceedings{jetterVREverythingPossible2020,
	address = {Honolulu HI USA},
	title = {"{In} {VR}, everything is possible!": {Sketching} and {Simulating} {Spatially}-{Aware} {Interactive} {Spaces} in {Virtual} {Reality}},
	isbn = {978-1-4503-6708-0},
	shorttitle = {"{In} {VR}, everything is possible!"},
	url = {https://dl.acm.org/doi/10.1145/3313831.3376652},
	doi = {10.1145/3313831.3376652},
	abstract = {We propose using virtual reality (VR) as a design tool for sketching and simulating spatially-aware interactive spaces. Using VR, designers can quickly experience their envisioned spaces and interactions by simulating technologies such as motion tracking, multiple networked devices, or unusual form factors such as spherical touchscreens or bezel-less display tiles. Design ideas can be rapidly iterated without restrictions by the number, size, or shape and availability of devices or sensors in the lab. To understand the potentials and challenges of designing in VR, we conducted a user study with 12 interaction designers. As their tool, they used a custom-built virtual design environment with ﬁnger tracking and physics simulations for natural interactions with virtual devices and objects. Our study identiﬁed the designers’ experience of space in relation to their own bodies and playful design explorations as key opportunities. Key challenges were the complexities of building a usable yet versatile VR-based "World Editor".},
	language = {en},
	urldate = {2022-01-27},
	booktitle = {Proceedings of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Jetter, Hans-Christian and Rädle, Roman and Feuchtner, Tiare and Anthes, Christoph and Friedl, Judith and Klokmose, Clemens Nylandsted},
	month = apr,
	year = {2020},
	pages = {1--16},
	file = {Jetter et al. - 2020 - In VR, everything is possible! Sketching and Si.pdf:/Users/ticalin/Zotero/storage/9XK9XYTA/Jetter et al. - 2020 - In VR, everything is possible! Sketching and Si.pdf:application/pdf},
}

@article{weissRevisitedComparisonEmpirical2021,
	title = {Revisited: {Comparison} of {Empirical} {Methods} to {Evaluate} {Visualizations} {Supporting} {Crafting} and {Assembly} {Purposes}},
	volume = {27},
	issn = {1941-0506},
	shorttitle = {Revisited},
	doi = {10.1109/TVCG.2020.3030400},
	abstract = {Ubiquitous, situated, and physical visualizations create entirely new possibilities for tasks contextualized in the real world, such as doctors inserting needles. During the development of situated visualizations, evaluating visualizations is a core requirement. However, performing such evaluations is intrinsically hard as the real scenarios are safety-critical or expensive to test. To overcome these issues, researchers and practitioners adapt classical approaches from ubiquitous computing and use surrogate empirical methods such as Augmented Reality (AR), Virtual Reality (VR) prototypes, or merely online demonstrations. This approach's primary assumption is that meaningful insights can also be gained from different, usually cheaper and less cumbersome empirical methods. Nevertheless, recent efforts in the Human-Computer Interaction (HCI) community have found evidence against this assumption, which would impede the use of surrogate empirical methods. Currently, these insights rely on a single investigation of four interactive objects. The goal of this work is to investigate if these prior findings also hold for situated visualizations. Therefore, we first created a scenario where situated visualizations support users in do-it-yourself (DIY) tasks such as crafting and assembly. We then set up five empirical study methods to evaluate the four tasks using an online survey, as well as VR, AR, laboratory, and in-situ studies. Using this study design, we conducted a new study with 60 participants. Our results show that the situated visualizations we investigated in this study are not prone to the same dependency on the empirical method, as found in previous work. Our study provides the first evidence that analyzing situated visualizations through different empirical (surrogate) methods might lead to comparable results.},
	number = {2},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Weiß, Maximilian and Angerbauer, Katrin and Voit, Alexandra and Schwarzl, Magdalena and Sedlmair, Michael and Mayer, Sven},
	month = feb,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Visualization and Computer Graphics},
	pages = {1204--1213},
	file = {IEEE Xplore Full Text PDF:/Users/ticalin/Zotero/storage/7X2R82HS/Weiß et al. - 2021 - Revisited Comparison of Empirical Methods to Eval.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/ticalin/Zotero/storage/DEFHAA9N/9225008.html:text/html},
}

@inproceedings{linUnderstandingSituatedAR2021,
	address = {Yokohama Japan},
	title = {Towards an {Understanding} of {Situated} {AR} {Visualization} for {Basketball} {Free}-{Throw} {Training}},
	isbn = {978-1-4503-8096-6},
	url = {https://dl.acm.org/doi/10.1145/3411764.3445649},
	doi = {10.1145/3411764.3445649},
	abstract = {We present an observational study to compare co-located and situated real-time visualizations in basketball free-throw training. Our goal is to understand the advantages and concerns of applying immersive visualization to real-world skill-based sports training and to provide insights for designing AR sports training systems. We design both a situated 3D visualization on a head-mounted display and a 2D visualization on a co-located display to provide immediate visual feedback on a player’s shot performance. Using a within-subject study design with experienced basketball shooters, we characterize user goals, report on qualitative training experiences, and compare the quantitative training results. Our results show that real-time visual feedback helps athletes refne subsequent shots. Shooters in our study achieve greater angle consistency with our visual feedback. Furthermore, AR visualization promotes an increased focus on body form in athletes. Finally, we present suggestions for the design of future sports AR studies.},
	language = {en},
	urldate = {2022-02-01},
	booktitle = {Proceedings of the 2021 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Lin, Tica and Singh, Rishi and Yang, Yalong and Nobre, Carolina and Beyer, Johanna and Smith, Maurice A. and Pfister, Hanspeter},
	month = may,
	year = {2021},
	pages = {1--13},
	file = {Lin et al. - 2021 - Towards an Understanding of Situated AR Visualizat.pdf:/Users/ticalin/Zotero/storage/9UR738HG/Lin et al. - 2021 - Towards an Understanding of Situated AR Visualizat.pdf:application/pdf},
}

@article{walnyUnderstandingPenTouch2012,
	title = {Understanding {Pen} and {Touch} {Interaction} for {Data} {Exploration} on {Interactive} {Whiteboards}},
	volume = {18},
	issn = {1941-0506},
	doi = {10.1109/TVCG.2012.275},
	abstract = {Current interfaces for common information visualizations such as bar graphs, line graphs, and scatterplots usually make use of the WIMP (Windows, Icons, Menus and a Pointer) interface paradigm with its frequently discussed problems of multiple levels of indirection via cascading menus, dialog boxes, and control panels. Recent advances in interface capabilities such as the availability of pen and touch interaction challenge us to re-think this and investigate more direct access to both the visualizations and the data they portray. We conducted a Wizard of Oz study to explore applying pen and touch interaction to the creation of information visualization interfaces on interactive whiteboards without implementing a plethora of recognizers. Our wizard acted as a robust and flexible pen and touch recognizer, giving participants maximum freedom in how they interacted with the system. Based on our qualitative analysis of the interactions our participants used, we discuss our insights about pen and touch interactions in the context of learnability and the interplay between pen and touch gestures. We conclude with suggestions for designing pen and touch enabled interactive visualization interfaces.},
	number = {12},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Walny, Jagoda and Lee, Bongshin and Johns, Paul and Henry Riche, Nathalie and Carpendale, Sheelagh},
	month = dec,
	year = {2012},
	note = {Conference Name: IEEE Transactions on Visualization and Computer Graphics},
	pages = {2779--2788},
	file = {IEEE Xplore Full Text PDF:/Users/ticalin/Zotero/storage/456HM9FR/Walny et al. - 2012 - Understanding Pen and Touch Interaction for Data E.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/ticalin/Zotero/storage/FTFS3PUL/6327284.html:text/html},
}

@inproceedings{ledoEvaluationStrategiesHCI2018,
	address = {Montreal QC Canada},
	title = {Evaluation {Strategies} for {HCI} {Toolkit} {Research}},
	isbn = {978-1-4503-5620-6},
	url = {https://dl.acm.org/doi/10.1145/3173574.3173610},
	doi = {10.1145/3173574.3173610},
	abstract = {Toolkit research plays an important role in the field of HCI, as it can heavily influence both the design and implementation of interactive systems. For publication, the HCI community typically expects toolkit research to include an evaluation component. The problem is that toolkit evaluation is challenging, as it is often unclear what ‘evaluating’ a toolkit means and what methods are appropriate. To address this problem, we analyzed 68 published toolkit papers. From our analysis, we provide an overview of, reflection on, and discussion of evaluation methods for toolkit contributions. We identify and discuss the value of four toolkit evaluation strategies, including the associated techniques that each employs. We offer a categorization of evaluation strategies for toolkit researchers, along with a discussion of the value, potential limitations, and trade-offs associated with each strategy.},
	language = {en},
	urldate = {2022-02-13},
	booktitle = {Proceedings of the 2018 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Ledo, David and Houben, Steven and Vermeulen, Jo and Marquardt, Nicolai and Oehlberg, Lora and Greenberg, Saul},
	month = apr,
	year = {2018},
	pages = {1--17},
	file = {Ledo et al. - 2018 - Evaluation Strategies for HCI Toolkit Research.pdf:/Users/ticalin/Zotero/storage/N7ZJNHLY/Ledo et al. - 2018 - Evaluation Strategies for HCI Toolkit Research.pdf:application/pdf},
}

@article{obrienDevelopmentEvaluationSurvey2010,
	title = {The development and evaluation of a survey to measure user engagement},
	volume = {61},
	issn = {1532-2890},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/asi.21229},
	doi = {10.1002/asi.21229},
	abstract = {Facilitating engaging user experiences is essential in the design of interactive systems. To accomplish this, it is necessary to understand the composition of this construct and how to evaluate it. B...},
	language = {en},
	number = {1},
	urldate = {2022-02-24},
	journal = {Journal of the American Society for Information Science and Technology},
	author = {O'Brien, Heather L. and Toms, Elaine G.},
	month = jan,
	year = {2010},
	note = {Publisher: John Wiley \& Sons, Ltd},
	pages = {50--69},
	file = {Accepted Version:/Users/ticalin/Zotero/storage/GXVAWN9R/O'Brien and Toms - 2010 - The development and evaluation of a survey to meas.pdf:application/pdf;Snapshot:/Users/ticalin/Zotero/storage/FEITPRCP/asi.html:text/html},
}

@article{scaifeExternalCognitionHow1996,
	title = {External cognition: how do graphical representations work?},
	volume = {45},
	issn = {10715819},
	shorttitle = {External cognition},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1071581996900488},
	doi = {10.1006/ijhc.1996.0048},
	language = {en},
	number = {2},
	urldate = {2022-02-28},
	journal = {International Journal of Human-Computer Studies},
	author = {Scaife, Mike and Rogers, Yvonne},
	month = aug,
	year = {1996},
	pages = {185--213},
	file = {Scaife and Rogers - 1996 - External cognition how do graphical representatio.pdf:/Users/ticalin/Zotero/storage/GHA7LHWU/Scaife and Rogers - 1996 - External cognition how do graphical representatio.pdf:application/pdf},
}

@article{obrienDevelopmentEvaluationSurvey2010a,
	title = {The development and evaluation of a survey to measure user engagement},
	volume = {61},
	issn = {1532-2890},
	url = {http://onlinelibrary.wiley.com/doi/abs/10.1002/asi.21229},
	doi = {10.1002/asi.21229},
	abstract = {Facilitating engaging user experiences is essential in the design of interactive systems. To accomplish this, it is necessary to understand the composition of this construct and how to evaluate it. Building on previous work that posited a theory of engagement and identified a core set of attributes that operationalized this construct, we constructed and evaluated a multidimensional scale to measure user engagement. In this paper we describe the development of the scale, as well as two large-scale studies (N=440 and N=802) that were undertaken to assess its reliability and validity in online shopping environments. In the first we used Reliability Analysis and Exploratory Factor Analysis to identify six attributes of engagement: Perceived Usability, Aesthetics, Focused Attention, Felt Involvement, Novelty, and Endurability. In the second we tested the validity of and relationships among those attributes using Structural Equation Modeling. The result of this research is a multidimensional scale that may be used to test the engagement of software applications. In addition, findings indicate that attributes of engagement are highly intertwined, a complex interplay of user-system interaction variables. Notably, Perceived Usability played a mediating role in the relationship between Endurability and Novelty, Aesthetics, Felt Involvement, and Focused Attention.},
	language = {en},
	number = {1},
	urldate = {2022-02-28},
	journal = {Journal of the American Society for Information Science and Technology},
	author = {O'Brien, Heather L. and Toms, Elaine G.},
	year = {2010},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/asi.21229},
	pages = {50--69},
	file = {Snapshot:/Users/ticalin/Zotero/storage/73M6JW2N/asi.html:text/html;Full Text PDF:/Users/ticalin/Zotero/storage/LXPSMJTB/O'Brien and Toms - 2010 - The development and evaluation of a survey to meas.pdf:application/pdf},
}


@article{seidlBhostgustersRealtimeInteractive,
  title = {Bhostgusters: {{Realtime Interactive Play Sketching}} with {{Synthesized NBA Defenses}}},
  author = {Seidl, Thomas and Cherukumudi, Aditya and Hartnett, Andrew and Carr, Peter and Lucey, Patrick},
  pages = {13},
  langid = {english},
  keywords = {Basketball,Interactive retrieval},
  file = {/Users/ticalin/Zotero/storage/REF8JBFM/Seidl et al. - Bhostgusters Realtime Interactive Play Sketching .pdf}
}

@article{dow2005wizard,
  title={Wizard of Oz support throughout an iterative design process},
  author={Dow, Steven and MacIntyre, Blair and Lee, Jaemin and Oezbek, Christopher and Bolter, Jay David and Gandy, Maribeth},
  journal={IEEE Pervasive Computing},
  volume={4},
  number={4},
  pages={18--26},
  year={2005},
  publisher={IEEE}
}

@article{lo2022stats,
  title={Stats on-site—Sports spectator experience through situated visualizations},
  author={Lo, Wei Hong and Zollmann, Stefanie and Regenbrecht, Holger},
  journal={Computers \& Graphics},
  volume={102},
  pages={99--111},
  year={2022},
  publisher={Elsevier}
}

@article{tian2019use,
  title={Use of machine learning to automate the identification of basketball strategies using whole team player tracking data},
  author={Tian, Changjia and De Silva, Varuna and Caine, Michael and Swanson, Steve},
  journal={Applied Sciences},
  volume={10},
  number={1},
  pages={24},
  year={2019},
  publisher={MDPI}
}

@inproceedings{mcintyre2016recognizing,
  title={Recognizing and analyzing ball screen defense in the nba},
  author={McIntyre, Avery and Brooks, Joel and Guttag, John and Wiens, Jenna},
  booktitle={Proceedings of the MIT Sloan Sports Analytics Conference, Boston, MA, USA},
  pages={11--12},
  year={2016}
}

@phdthesis{nistala2018using,
  title={Using deep learning to understand patterns of player movement in basketball},
  author={Nistala, Akhil},
  year={2018},
  school={Massachusetts Institute of Technology}
}

% website below 
 @misc{miro, author ={{Miro}}, 
 url = {https://miro.com/}, 
 howpublished = {\url{https://miro.com/}},
 year = 2022} 
 
 
 @online{courtvision, author = {{CourtVision}},  title={}, url={https://www.clipperscourtvision.com/}, year = 2020}
 
 @book{hartson2012ux,
  title={The UX Book: Process and guidelines for ensuring a quality user experience},
  author={Hartson, Rex and Pyla, Pardha S},
  year={2012},
  publisher={Elsevier}
}

@misc{sportvu, title={Papers with code - NBA sportvu dataset}, howpublished = {\url{https://paperswithcode.com/dataset/nba-sportvu}}, author={SportVU}
 } 
 
 @misc{clipperscourtvision, title={Clippers CourtVision}, howpublished = {\url{https://www.clipperscourtvision.com/}}, author={CourtVision}
 } 
 
 @misc{nbaleaguepass, title={NBA League Pass}, howpublished = {\url{https://support.watch.nba.com/hc/en-us/articles/115000585974-NBA-League-Pass}}, author={NBA}
 } 
 
@misc{kagglenba, title={NBA Play-by-Play Data 2015-2021}, author = {Kaggle}, 
 howpublished = {\url{https://www.kaggle.com/schmadam97/nba-playbyplay-data-20182019}}
 }
 
@misc{espn2015, title={Cavaliers vs. warriors - play-by-play - December 25, 2015},  howpublished ={\url{https://www.espn.com/nba/playbyplay/_/gameId/400828325}}, journal={ESPN}, publisher={ESPN Internet Ventures}, author={ESPN}} 
  
@misc{nbashotcharts,
   title={NBA Shot Charts},howpublished={\url{http://nbashotcharts.com/}}} 
   
@misc{basketballref,
   title={Basketball-Reference},howpublished={\url{https://www.basketball-reference.com/}}}
   
@misc{nbastats,
   title={NBA Stats},howpublished={\url{https://www.nba.com/stats/}}}    
   
@misc{espnnba,
   title={ESPN - NBA },howpublished={\url{https://www.espn.com/nba/}}} 
   
@misc{nba,
   title={The Offical Site of NBA },howpublished={\url{https://www.nba.com/}}} 
   
@misc{reddit,
   title={Reddit - NBA },howpublished={\url{https://www.reddit.com/r/nba/}}} 
   
@misc{secondspectrum,
   title={Second Spectrum },howpublished={\url{https://www.secondspectrum.com/}}} 
   
   
@misc{harrypotter,
   title={Omnioculars | Harry Potter Wiki },howpublished={\url{https://harrypotter.fandom.com/wiki/Omnioculars/}}}    
@misc{dota,
   title={DOTA 2 },howpublished={\url{https://www.dota2.com/}}}     
@misc{lol,
   title={League of Legends },howpublished={\url{https://www.leagueoflegends.com/}}}     
@misc{nba2k,
   title={NBA 2K},
   howpublished={\url{https://nba.2k.com/}}}        
   
@online{76ers,
  author = {Marcus Hayes},
  title = {Analytics-driven Sixers ride the numbers to NBA playoffs},
  year = 2018,
  url = {https://www.inquirer.com/philly/sports/sixers/sixers-76ers-philadelphia-analytics-process-numbers-nba-playoffs-miami-heat-brett-brown-bryan-colangelo-20180418.html},
  urldate = {2022-03-13}
}

@article{o2010development,
  title={The development and evaluation of a survey to measure user engagement},
  author={O'Brien, Heather L and Toms, Elaine G},
  journal={Journal of the American Society for Information Science and Technology},
  volume={61},
  number={1},
  pages={50--69},
  year={2010},
  publisher={Wiley Online Library}
}

@article{brehmer2013multi,
  title={{A Multi-level Typology of Abstract Visualization Tasks}},
  author={Brehmer, Matthew and Munzner, Tamara},
  journal={IEEE {TVCG}},
  volume={19},
  number={12},
  pages={2376--2385},
  year={2013},
  publisher={IEEE}
}

@inproceedings{lucey2014get,
  title={How to get an open shot: Analyzing team movement in basketball using tracking data},
  author={Lucey, Patrick and Bialkowski, Alina and Carr, Peter and Yue, Yisong and Matthews, Iain},
  booktitle={Proceedings of the 8th annual MIT SLOAN sports analytics conference},
  year={2014},
  organization={Citeseer}
}

@article{dahlback1993wizard,
  title={Wizard of Oz studies—why and how},
  author={Dahlb{\"a}ck, Nils and J{\"o}nsson, Arne and Ahrenberg, Lars},
  journal={Knowledge-based systems},
  volume={6},
  number={4},
  pages={258--266},
  year={1993},
  publisher={Elsevier}
}
@article{palmeiro2018interaction,
  title={Interaction between pedestrians and automated vehicles: A Wizard of Oz experiment},
  author={Palmeiro, Ana Rodr{\'\i}guez and van der Kint, Sander and Vissers, Luuk and Farah, Haneen and de Winter, Joost CF and Hagenzieker, Marjan},
  journal={Transportation research part F: traffic psychology and behaviour},
  volume={58},
  pages={1005--1020},
  year={2018},
  publisher={Elsevier}
}
@article{budzianowski2018multiwoz,
  title={MultiWOZ--A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling},
  author={Budzianowski, Pawe{\l} and Wen, Tsung-Hsien and Tseng, Bo-Hsiang and Casanueva, Inigo and Ultes, Stefan and Ramadan, Osman and Ga{\v{s}}i{\'c}, Milica},
  journal={arXiv preprint arXiv:1810.00278},
  year={2018}
}
@inproceedings{browne2019wizard,
  title={Wizard of OZ prototyping for machine learning experiences},
  author={Browne, Jacob T},
  booktitle={Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems},
  pages={1--6},
  year={2019}
}

@inproceedings{ens2021grand,
  title={Grand challenges in immersive analytics},
  author={Ens, Barrett and Bach, Benjamin and Cordeil, Maxime and Engelke, Ulrich and Serrano, Marcos and Willett, Wesley and Prouzeau, Arnaud and Anthes, Christoph and B{\"u}schel, Wolfgang and Dunne, Cody and others},
  booktitle={Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
  pages={1--17},
  year={2021}
}

@book{marriott2018immersive,
  title={Immersive analytics},
  author={Marriott, Kim and Schreiber, Falk and Dwyer, Tim and Klein, Karsten and Riche, Nathalie Henry and Itoh, Takayuki and Stuerzlinger, Wolfgang and Thomas, Bruce H},
  volume={11190},
  year={2018},
  publisher={Springer}
}

@inproceedings{DBLP:conf/chi/FuS22,
  author    = {Yu Fu and
               John T. Stasko},
  title     = {Supporting Data-Driven Basketball Journalism through Interactive Visualization},
  booktitle = {Proc. of CHI},
  pages     = {598:1--598:17},
  publisher = {{ACM}},
  year      = {2022},
  url       = {https://doi.org/10.1145/3491102.3502078},
  doi       = {10.1145/3491102.3502078},
  timestamp = {Fri, 29 Apr 2022 17:07:24 +0200},
  biburl    = {https://dblp.org/rec/conf/chi/FuS22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}