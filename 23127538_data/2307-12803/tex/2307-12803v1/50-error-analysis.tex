\section{Error Analysis}
\label{sec:error-analysis}
\noindent\fbox{%
\parbox{0.98\linewidth}{%
\fontsize{9.5}{11.5}\selectfont
\textbf{RQ3.} What are the errors and failure modes of unguided and guided methods for radiology report summarization?
}}

\subsection{Evaluation Setup}
Inspired by the Multidimensional Quality Metrics framework for evaluation of machine translation systems (\citealp{Lommel:2014:MQM}), we conduct a span-based error annotation.
We task annotators to highlight erroneous text spans and to classify them according to an error taxonomy.
As a starting point, we use the taxonomy proposed by \citet{Yu:2022:medRxiv}.
Based on two pilot runs, we extended this taxonomy from initially 6 to 11 fine-grained error categories (see \cref{tab:error-analysis-results}) and developed a definition and examples for each. %\footnote{We provide full annotation guidelines in~\cref{sec:appendix-annotation-guidelines}.}
Following \citet{Yu:2022:medRxiv}, we opt for a reference-based evaluation.
We want to understand how the system generated summary differs from the clinician summary both in content and correctness of the presented facts.
Therefore, our errors can be grouped into additions (spans in the candidate), omissions (spans in the reference), and binary choices for the correctness of presented facts.
Further, we ask annotators to flag any additional errors they encounter as a free-form answer.
We provide full annotation guidelines in~\cref{sec:appendix-annotation-guidelines}.

\paragraph{Materials.}
We randomly select 100 reports from the official test set of MIMIC-CXR which is stratified to cover both frequent and less frequent inputs/clinical observations~\cite{Johnson:2019:arXiv}.
For each input, we generate four candidate summaries using BertAbs (representative of unguided systems), GSum w/ Thresholding (representative of systems with domain-agnostic guidance), and WGSum/WGSum+CL (representative of systems with domain-specific guidance).
We present the reference summary and all candidates (in random order) at once to annotators to reduce effort and ensure consistent annotation of similar summaries.
Each set of summaries is completed by three annotators resulting in 1,200 error annotations (100~reports \texttimes\ 4~candidates \texttimes\ 3~annotators). We form a ``gold standard'' from the triple annotation by majority voting (example aggregation in \cref{sec:appendix-span-based-aggregation}).

\paragraph{Annotators.}
To account for the domain knowledge necessary for the annotation task, we hired 6 senior medical students in their fifth year of training. All annotators are fluent in English. We compensated annotators with 10.5â‚¬ per hour (standard rate for student assistants in Germany). The annotation took 23.1 hours (avg. 4.6 min/sample), plus additional time for pilot rounds and discussions.

\input{53-tab-error-analysis}

\subsection{Results (RQ3)}
We report aggregated error counts and example annotations in \cref{tab:error-analysis-results}.\footnote{
To measure inter-annotator agreement (IAA), we calculate $F_1$ for span-annotations~\cite{Deleger:2012:AMIA} and Krippendorffs' Alpha for binary judgments~\cite{Krippendorff:1970:ALPHA}. Aggregated IAA: 1. Omissions: 0.61, 2. Additions: 0.60, 3. Incorrect Location: 0.25, and 4. Incorrect Severity: 0.41. IAA by error category for span-level annotations in~\cref{sec:appendix-iaa}.
}

Overall, we find that the prevalence of errors is comparable across the investigated methods, and that only 14--22\% of generated summaries are error-free.
The most common errors are omissions and additions of findings, which indicates that the models struggle to select relevant content (1a. 43--52\%; 2a. 44--57\%).
Compared with unguided summarization, there is a slight trend that guided methods reduce the risk of omissions, while only WGSum+CL succeeds at doing this without sacrificing precision.
Even though additions are common, they rarely contradict the reference (2e. 0--3\%).
Similarly, when both the reference and candidate present the same findings, errors related to their clinical correctness are rare (3. 5--8\%; 4. 6--9\%).

A surprising finding is the common omission and addition of clinicians' communications (1d. 16--20\%; 2d. 3--8\%).
By manual inspection (examples in \cref{tab:error-analysis-results}), we find that these are specific actions that a clinician performed after the examination such as informing colleagues about the findings, or recommending additional analysis.
Additions of this kind have likely no grounding in the underlying report.
To successfully generate such statements, models would require additional context information or guidance from a user.

\subsection{Discussion}
\label{sec:error-analysis-discussion}
Overall, our error analysis reveals that the key differences between model-generated impressions and radiologists' impressions relate to content selection (i.e., a tension between completeness/recall and relevance/precision).
We offer two hypotheses to explain the models' difficulties in this area.

First, there may be latent factors that explain which findings are included in the impression.
Among those factors could be patient demographics, the radiograph, prior exams and the clinical question.
Typically, this information is available to radiologists through the electronic health records, and is partly documented in the background section of radiology reports.
Early work explored using the background section as guidance~\cite{Zhang:2018:LOUHI}, but more recent work commonly excluded it in pre-processing~\cite{Sotudeh:2020:ACL,Hu:2021:ACL,Hu:2022:ACL}.
We present evaluation results when including the background and observe an overall improvement in almost all metrics for abstractive methods (\cref{sec:appendix-background-experiment}).
This improvement indicates that (i) additional context supports content selection, and (ii) it could be useful to explicitly model the background in guided summarization.

Second, we anecdotally observed a substantial degree of duplication in the MIMIC-CXR corpus, where reports with identical findings have different impressions (examples in \cref{sec:appendix-duplication-examples}).\footnote{11.9\% of the 122,500 MIMIC-CXR training reports have a findings section occurring more than once. Among those reports are only 1036 distinct impressions.}
This may lead to corpus-level inconsistencies preventing models to reliably learn the selection of findings.
We note that there can be numerous reasons for these duplication induced inconsistencies, including the presence of latent factors (see above) and remaining subjectivity/uncertainty in radiologists' assessments.
We leave the investigation of this aspect of data quality and potential effects of training data deduplication for future work.

\subsection{Limitations}
We note two limitations of this error analysis.

First, the analysis is based on comparing candidate impressions with reference impressions.
In the absence of the full clinical context, we argue that this is the most reliable benchmark for completeness and relevance of summaries.
However, we recognize that we cannot draw any conclusions about the factuality of additions with respect to the full report.
To give a first factuality estimate, we conducted a post-hoc analysis with RadNLI~\cite{Miura:2021:NAACL}.
Let $x_i$ be a sentence in report $\bm{x} = (x_1,\ldots,x_n)$, and $s$ be an addition span.
If RadNLI predicts a contradiction for any $(x_i, s)$ pair, we label this span as contradicting and neutral/entailed otherwise.
We find that between 23.4\% (BertAbs) and 29.3\% (GSum w/ Thresholding) of additions are contradicting, indicating that factuality is another challenge for current models (details in \cref{sec:appendix-radnli}).

Second, the sample size was driven by time and resource constraints ($N=100$).
To estimate representativeness of this sample, we compare descriptive statistics of the sample with those of the whole test set (length, novelty, compression), and observe that these largely agree (see \cref{sec:appendix-replication-error-analysis}).
While we believe that this sample is sufficient to support the qualitative conclusions about the failure modes of current methods, a larger study is warranted when the goal is to quantitatively compare the efficacy of different methods.
