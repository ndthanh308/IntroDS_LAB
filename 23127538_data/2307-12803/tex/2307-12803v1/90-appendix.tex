\section{Detailed Analysis}
\label{sec:appendix-analysis}
To support replication, this section provides supplementary analysis on the results of the main part.

\subsection{Target Impression Length Distribution and Evaluation by Target Length}
\label{sec:appendix-target-summary-length}
We demonstrated in the main part that variable-length guidance helps to adapt to varying target lengths.
To better interpret this result, we plot the length distribution of target summaries and the ROUGE-1 score by target-length interval in \cref{fig:target-summary-length}.
It can be observed that the length distribution has a long tail with a peak around 4-5 tokens.
Impressions of this length are standard phrasings to indicate that no abnormalities were found (e.g., \emph{``No evidence of acute findings''}).

% Figure environment removed

\subsection{Evaluating GSum in an Oracle Setting}
\label{sec:appendix-gsum-oracle-experiment}
As a supplement to the oracle experiment in \cref{sec:results-gsum-fixed}, we provide all metrics for the three inference settings of GSum in \cref{tab:appendix-gsum-oracle}: (i) automatic fixed-length guidance (i.e., extracted from BertExt with $k=1$), (ii) automatic variable-length guidance but with an oracle length (i.e., BertExt with $k = |\text{OracleExt}(\bm{x}, \bm{y})|$), and (iii) oracle guidance (i.e., $\bm{g} = \text{OracleExt}(\bm{x}, \bm{y})$).

\input{91-tab-gsum-oracle}

\subsection{BertExt: Evaluating Fixed-length Settings}
\label{sec:appendx-fixed-k-testing}
To evaluate if larger values for $k$ in the fixed-summary length setting would improve the effectiveness of BertExt, we generate summaries for all settings of $k = \{1,...,5\}$.
Analogously, we provide these summaries as guidance signal to GSum.
\cref{tab:appendix-fixed-k-testing} reports the results of this experiment.
While we find that larger settings of $k$ lead to an increase in recall, we see an equally strong drop in precision, both on BertExt and GSum which demonstrates the necessity of variable-length extractive guidance.

\input{92-tab-eval-fixed}

\subsection{Evaluating Guidance Length Prediction}
\label{sec:appendix-oracle-approx}
To predict the length of OracleExt in the variable-length guidance setting, we employ a logistic regression classifier and a BERT-based classifier (cf. \cref{sec:experimental-setup}).
Detailed evaluation results for both classification models are given in \cref{tab:appendix-oracle-approx}.

\input{93-tab-oracle-approx-eval}

\subsection{Including the Background Section}
\label{sec:appendix-background-experiment}
To understand to what extent the background section carries important information for summarizing findings to impression, we prepend it to the findings section and retrain all models.
It can be observed that this change improves most abstractive methods on both datasets (\cref{fig:background-experiment}).
For extractive methods results stay largely on par or get worse, indicating that these models do not effectively integrate the background information.

% Figure environment removed

\subsection{Examples of Duplicated Findings and Impressions}
\label{sec:appendix-duplication-examples}
We anecdotally observed a large degree of duplication within MIMIC-CXR which may cause corpus-level inconsistencies (see discussion in~\cref{sec:error-analysis-discussion}).
This section further quantifies the degree of duplication and provides several examples.
Throughout, we only consider instances of \emph{exact} duplication.
Of the 122,500 training reports in MIMIC-CXR, we find that 11.9\% have a findings section occurring more than once.
We present examples of duplicate findings with \emph{different} impressions in~\cref{tab:duplicates}.
In addition, we calculate a \emph{label entropy} over the probabilities that each impression occurs for a given finding.
We posit that duplicate finding-impression pairs may negatively impact model training in two ways.
First, for findings with a high label entropy, the training loss cannot not stabilize (i.e., it is not clear which impression the model should favor).
Second, for findings with a low label entropy, the model may learn a kind of ``majority vote,'' which in turn may render models not sensitive enough to generate useful summaries for slightly different findings.
We leave further investigation of report duplication to future work.

\subsection{Factuality of Additions}
\label{sec:appendix-radnli}
As discussed in~\cref{sec:error-analysis-discussion}, we use RadNLI~\cite{Miura:2021:NAACL} to get a first estimate for the factuality of additions marked by annotators in the error analysis.
RadNLI obtained an accuracy of 77.8\% on a test set of 480 manually labeled sentence pairs in MIMIC-CXR~\cite{Miura:2021:NAACL}, which we consider sufficient for an initial exploration of the factuality of additions.
\cref{tab:radnli-results} presents a breakdown of the RadNLI predictions for all addition spans and models.
It can be seen that the majority of additions is either neutral to the findings section, or entailed by it.
Yet, between 23.4\% and 29.3\% of additions contradict at least one findings sentence, indicating that factuality of radiology report summarization methods can also further be improved.

\subsection{Error Analysis: Responses to \emph{Other} Category}
We analyze the annotators' comments from the \emph{other} error category, and categorize these errors into two-level hierarchy using a bottom-up approach. Our categorization alongside definitions, examples and counts is shown in \cref{tab:app:other}.

\section{Replication Details for Modeling}
\label{sec:appendix-replication-models}
We report hyperparameters of the summarization models in \cref{tab:hyperparameters-summarization}, and for models that predict the length of OracleExt (\textsc{lr-approx}/\textsc{bert-approx}) in \cref{tab:hyperparameters-oracle-approx}.
All models were trained on NVIDIA RTX A6000 GPUs with 48GB of memory.

\input{95-tab-radnli}

\section{Replication Details for Error Analysis}
\label{sec:appendix-replication-error-analysis}

\paragraph{Sample statistics.}
For inclusion in the error analysis, samples were drawn uniformly at random from the official test set of MIMIC-CXR. We compare statistics of the sample with those of the full test set in~\cref{tab:error-analysis-sample-statistics}.

\input{98-tab-sample-stats}

\paragraph{Aggregating span-based annotations.}
\label{sec:appendix-span-based-aggregation}
From the three annotations we form a ``gold standard'' as follows: for binary questions we take a majority vote.
For span-based annotations, we first group (partially) overlapping spans, and then take a majority vote within each group.
We provide an example for the majority voting of span-based annotations below. A1, A2, A3, denote annotators, and \texttt{[--eX--]} denotes an error of category X.

\begin{small}
\begin{verbatim}
Tokens:  a   b  c   d   e   f   g   h
A1    : [-e1-]  [-----e2----]
A2    : [-e1-]  [-e1-] [-e2-]
A3    : [-e1-]                 [--e1--]
---------------------------------------
Group :    1          2           3
---------------------------------------
Vote  : [-e1-]         [-e2-]
\end{verbatim}
\end{small}

\paragraph{Inter-annotator agreement (IAA).}
\label{sec:appendix-iaa}
We calculate $F_1$ for span-annotations (\citet{Deleger:2012:AMIA}, categories 1 and 2), and Krippendorffs' Alpha~\cite{Krippendorff:1970:ALPHA} for binary judgments (categories 3 and 4) and report the IAA by category in~\cref{tab:iaa}.
\input{96-tab-params-summarization}
\input{96-tab-params-oracle-approx}
\input{97-tab-iaa}
\input{94-tab-duplication-examples}
\input{96-tab-other-responses}
\input{99-annotation-guidelines}
