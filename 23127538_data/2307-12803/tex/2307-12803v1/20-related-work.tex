\section{Related Work}

\paragraph{Guided text summarization.}
To address shortcomings in content selection and factuality of neural abstractive summarization methods~\cite{Rush:2015:EMNLP,Nallapati:2016:CoNLL}, guided methods aim to control the content of summaries through carefully selected guidance signals such as keywords~\cite{Li:2018:NAACL}, sentences~\cite{Chen:2018:ACL}, entities~\cite{Fan:2018:ACLws,Narayan:2021:TACL}, templates~\cite{Cao:2018:ACL} and prompts~\cite{He:2022:EMNLP}.

In the radiology domain, \citet{Zhang:2018:LOUHI} proposed to guide generation with the background section of reports using a dual-encoder model.
With a similar architecture, subsequent work explored the use of salient ontology terms~\cite{MacAvaney:2019:SIGIR,Sotudeh:2020:ACL}.
Besides text-based guidance, \citet{Hu:2021:ACL,Hu:2022:ACL} propose a graph-guided decoder which attends both to the report text and to a word-graph of clinical entities.
In contrast, we explore extractive summaries as guidance signal~\cite{Liu:2019:EMNLP,Dou:2021:NAACL}.
Crucially, this guidance signal can be extracted without any domain-specific resources such as ontologies and clinical entity taggers.
To this end, our method is similar to the approach by~\citet{Zhu:2023:arXiv}, which guides summarization with reference summaries from the training set, selected at random or by an oracle. Similar to our approach, this guidance signal can vary in length.

\paragraph{Alternatives to guided summarization.}
Several parallel research lines aim to enhance radiology report summarization with a different methodological focus.
First, several studies optimize factual consistency through reinforcement learning~\cite{Zhang:2020:ACL,Delbrouck:2022:ML4H} or post-hoc reranking~\cite{Xie:2023:arXiv}.
Second,~\citet{Karn:2022:ACL} devise an extract-then-abstract pipeline with multi-agent reinforcement learning.
Last, recent work explores domain-adaptation techniques for pre-trained language models to better accommodate radiology reports~\cite{Cai:2021:TransMultimedia,VanVeen:2023:ACL-ws}.
Our work is orthogonal to these efforts and future work could investigate how to combine them with guided summarization.

\paragraph{Evaluation of radiology report summarization.}
Evaluating text summarization systems is a long standing issue.
Since automatic metrics have a limited correlation with human judgment~\cite{Fabbri:2021:TACL}, manual evaluation is still regarded as the gold standard.
For the task of radiology report summarization, most manual evaluations focus on coarse criteria such as accuracy, completeness, conciseness, and readability~\cite{Zhang:2018:LOUHI,Hu:2022:ACL,Cai:2021:TransMultimedia}.
Yet, these evaluations only provide limited insights into directions for improvement.
To support the interpretation of automatic and manual evaluations, and to understand the pitfalls of current methods, we conduct an error analysis~\cite{Van-Miltenburg:2021:INLG}.
In this line of work,~\citet{Yu:2022:medRxiv} evaluated the ability of automatic metrics to capture six fine-grained errors of radiograph-to-impression models.
We extend this taxonomy in our error analysis.
