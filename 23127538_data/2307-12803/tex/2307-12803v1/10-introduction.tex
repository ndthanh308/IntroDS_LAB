\section{Introduction}
The radiology report is an important tool for radiologists to communicate examination results with other clinicians.
Typically, these reports contain three sections: the background section describing the exam and patient context, the findings section providing a detailed description of observations, and the impression section, which concisely summarizes the key findings~\cite{Kahn:2009:Radiology}.
In the clinical process, the impression is of high importance as it informs further treatments.
However, writing the impression can be time-consuming and error-prone, which is why automatic text summarization systems can substantially improve the quality of clinical reporting~\cite{Gershanik:2011:AMIA}.

From a summarization perspective, this task involves both an \emph{extractive} component, where important findings are copied verbatim into the summary, and an \emph{abstractive} component, forming those findings into a concise conclusion taking into account the full report (example in \cref{fig:example}).
Although abstractive methods generate fluent and relevant summaries, they are prone to hallucinations and their output is difficult to control~\cite{Maynez:2020:ACL,Kryscinski:2020:EMNLP,Huang:2020:EMNLP}.
Therefore, current methods for radiology report summarization employ \emph{guided text summarization} to control the summary content through carefully selected guidance signals such as salient ontology terms~\cite{Sotudeh:2020:ACL}, facts~\cite{Zhang:2020:ACL}, and clinical entities~\cite{Hu:2022:ACL}.

\input{10-example}
% Figure environment removed

While summary quality has improved steadily, two key issues have received little attention.
First, the success of current methods heavily relies on the availability and quality of the domain-specific guidance extractors (i.e., ontologies, fact extractors, clinical entity taggers).
As these resources are expensive to develop, and as they are only available for a few languages and clinical domains, it is of great interest to investigate to what extent we can use domain-agnostic guidance signals to make guided radiology report summarization methods more easily adoptable.
Second, while we do see improvements in automatic metrics and human assessments of coarse quality criteria such as fluency, correctness and completeness, we lack a good understanding and quantification of the errors and failure modes of current methods. We address the two issues as follows (overview in~\cref{fig:overview}).

\paragraph{Contribution 1: a variable-length extractive guidance signal (\cref{sec:method,sec:results-technical-evaluation}).}
Motivated by the observation that summaries have a large extractive component, we investigate \emph{extractive summaries} as guidance~\cite{Dou:2021:NAACL}.
We identify that the \emph{length of the extractive summaries} is critical for the effectiveness of this guidance signal.
Intuitively, generating longer summaries requires more guidance than shorter ones. % (e.g., if there are no abnormalities to report).
Therefore, we outline two approaches to adapt the guidance length to each report: (i) a classifier that predicts a suitable length, and (ii) a threshold-based method.
This variable-length guidance signal improves upon unguided summarization, is competitive with recent domain-guided methods, and cheap to adopt as it does not require any domain-specific resources.

\paragraph{Contribution 2: an error analysis (\cref{sec:error-analysis}).}
We conduct an error analysis of unguided, guided and domain-guided methods to identify avenues for improvements of all methods.
We ask domain experts to identify errors in the outputs of four systems and to characterize them along 11 fine-grained categories.
This analysis uncovers three aspects: (1) despite guidance, a significant portion of candidates shows problems in content selection, (2) some content selection decisions are likely only explained by latent factors, (3) there are some dataset-level issues in MIMIC-CXR, including duplicate findings with different impressions, and impression segments without any grounding in the reports.

We make experiment code, full pre-processing pipeline for the datasets, and 1,200 expert assessments of model outputs publicly available.\footnote{\rurl{github.com/jantrienes/inlg2023-radsum}} % for future research.
