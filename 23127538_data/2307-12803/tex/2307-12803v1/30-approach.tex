\section{Method}
\label{sec:method}
We formulate the task of summarizing radiology reports as follows.
Given the findings section of a report, represented as a sequence of tokens $\bm{x} = (x_1, x_2,\ldots, x_N)$, the goal is to generate an impression $\bm{y} = (y_1,y_2,\ldots,y_M)$ that accurately summarizes the most significant findings.
The guided summarization framework extends this problem setting with an additional input signal $\bm{g} = (g_1, g_2,\ldots, g_L)$ which aims to improve the quality of generated summaries by indicating salient information in $\bm{x}$.

\subsection{Model and Extractive Guidance}
\label{sec:method-extractive-guidance}
As a concrete implementation of the guided summarization framework, we adopt GSum~\cite{Dou:2021:NAACL}.
This sequence-to-sequence model extends a transformer-based architecture for abstractive text summarization~\cite{Liu:2019:EMNLP} with an additional encoder for the guidance signal $\bm{g}$.
To create a guidance-aware representation of the input, the decoder first attends to the encoded representation of $\bm{g}$, and afterwards to the whole input document $\bm{x}$ using cross-attention~\cite{Vaswani:2017:NIPS}.
The authors demonstrate that GSum is effective at controlling the content of summaries, leading to good results on several non-medical datasets.

\paragraph{Extractive guidance.}
While $\bm{g}$ can take any form, \citet{Dou:2021:NAACL} found the output of an extractive summarization to be highly effective.
Intuitively, this guidance signal informs the model about which input sentences should be highlighted in a summary.
An important implementation detail of GSum is the mechanism to obtain the extractive sentences.
\citet{Dou:2021:NAACL} distinguish between the \textbf{oracle} setting and the \textbf{automatic} setting.
In the oracle setting, the guidance sentences are greedily picked from $\bm{x}$ such that they maximize \textsc{rouge} with respect to $\bm{y}$~\cite{Nallapati:2017:AAAI}.
In the automatic setting, this oracle is approximated by an extractive summarization method (BertExt,~\citealp{Liu:2019:EMNLP}).
The training labels for BertExt are derived using the same greedy matching, thus BertExt can be considered an approximation of the oracle guidance.
Selecting the guidance signal from BertExt follows a top-$k$ strategy: scoring all sentences for relevance and selecting the highest scoring sentences until a predefined length threshold is reached~\cite{Nallapati:2017:AAAI,Liu:2019:EMNLP}.
Following \citet{Dou:2021:NAACL}, the oracle signal is used during training of GSum.
During inference, we state explicitly whether we use the oracle or automatic variant.

\subsection{Variable-length Extractive Guidance}
\label{sec:method-variable-length-guidance}
We empirically find that extracting a fixed-length summary with the top-$k$ approach has a negative impact on the effectiveness of GSum (\cref{sec:results-gsum-fixed}).
To address this problem, we propose two methods to select a variable-length extractive guidance signal from BertExt.
Formally, for a given document $\bm{x}$ and its sequence of sentences $(s_1,\ldots,s_N)$, with $s_{i}$ being the $i$-th sentence in $\bm{x}$, these methods have to select $L<N$ sentences as guidance $\bm{g}$.

\paragraph{Method 1: predicting oracle length.}
As described in \cref{sec:method-extractive-guidance}, BertExt is trained to assign a label $y \in \{0, 1\}$ to each sentence $s_{i}$. The predicted probability $p(y = 1|s_{i})$ indicates if $s_{i}$ should be included in the summary.
The ground-truth labels are derived from an extractive oracle $f_{\text{oracle}}(\bm{x}, \bm{y})$ which greedily selects a subset of sentences of length $[0,3]$ that maximizes \textsc{rouge} against the gold summary $\bm{y}$~\cite{Liu:2019:EMNLP}.
Instead of taking a fixed number of sentences with highest probability (top-$k$), we train a sequence-classification model to predict the length of the extractive oracle $f_{\text{approx}}(\bm{x}) = L \in [0,3]$, and select the top-$L$ sentences as guidance signal.

\paragraph{Method 2: threshold calibration.}
Instead of considering the full ranked list of sentences, this method constrains selection with a threshold-based approach inspired by \citet{Jia:2021:AAAI}.
Recall that $p(y = 1|s_{i})$ denotes the probability that BertExt assigns to the positive class.
We then select the set of sentences that exceed a probability threshold $T$ as guidance signal:
%
\begin{equation*}
  \bm{g} = \{s_{i} \in \bm{x} | p(y=1|s_{i}) \ge T \}.
\end{equation*}
%
We optimize $T \in [0,1]$ on a validation set to maximize \textsc{rouge}-1.
