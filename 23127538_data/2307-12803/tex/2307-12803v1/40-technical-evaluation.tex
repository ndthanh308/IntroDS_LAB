\section{Technical Evaluation}
\label{sec:results-technical-evaluation}

\noindent\fbox{%
\parbox{0.98\linewidth}{%
\fontsize{9.5}{11.5}\selectfont
\textbf{RQ1.} To what extent are extractive summaries an effective guidance signal for radiology report summarization?\\[0.5em]
\textbf{RQ2.} How does adapting the extractive guidance length to each report impact the overall quality of summaries?
}}

\subsection{Experimental Setup}
\label{sec:experimental-setup}

\paragraph{Datasets.}
\label{sec:datasets}
We use two public datasets of English chest x-ray reports: \textbf{MIMIC-CXR}~\cite{Johnson:2019:SciData} and \textbf{OpenI}~\cite{Demner-Fushman:2015:JAMIA}.
Consistent with prior work~\cite{Zhang:2018:LOUHI,Sotudeh:2020:ACL,Hu:2022:ACL}, we retain reports with exactly one findings and one impression section, where both have an acceptable length ($\ge 10$ tokens in findings, $\ge 2$ tokens in impression), and we discard the background section.\footnote{%
  To compare the relative utility of guidance signals, including the background section is not necessary. For completeness, we report results with background section in \cref{sec:error-analysis-discussion}.
}
Following \citet{Hu:2022:ACL}, we use the official training, validation and test splits of MIMIC-CXR and a random split with a 70/10/20 ratio for OpenI.
We use \textsc{spacy} for tokenization and \textsc{nltk} for sentence segmentation.\footnote{\rurl{spacy.io} and \rurl{nltk.org}}
\cref{tab:datasets} reports the dataset statistics.

\input{41-tab-datasets}

\paragraph{Baselines.}
We compare with three categories of baselines: (1) unguided methods, (2) vanilla GSum with fixed-length extractive guidance~\cite{Dou:2021:NAACL}, and (3) domain-specific guided methods.
Regarding unguided methods, we use \textbf{OracleExt}~\cite{Nallapati:2017:AAAI} which greedily selects sentences from the findings that maximize \textsc{rouge} with respect to the impression.
Furthermore, we use \textbf{BertExt} and \textbf{BertAbs}~\cite{Liu:2019:EMNLP} which are extractive/abstractive transformer-based models initialized with pre-trained \textsc{bert}~\cite{Devlin:2019:NAACL}.
Regarding domain-specific methods, we compare with \textbf{WGSum}~\cite{Hu:2021:ACL} which employs a graph-guided decoder to attend to a graph of clinical entities extracted with Stanza~\cite{Zhang:2021:JAMIA}, and with
\textbf{WGSum+CL}~\cite{Hu:2022:ACL} which refines this guidance signal through contrastive learning.

\paragraph{Automatic evaluation metrics.}
We evaluate the quality of generated impressions with \textsc{rouge} $F_1$~\cite{Lin:2004:WS} to measure unigram and bigram overlap as a proxy for relevance (\mbox{R-1}, \mbox{R-2}) and the longest common subsequence as a proxy for fluency (\mbox{R-L}).
In addition, we report BERTScore as a measure of soft-alignment~\cite{Zhang:2020:ICLR}.
As factual correctness is critical, we also calculate a factuality $F_1$~\cite[Fact.]{Zhang:2020:ACL,Hu:2022:ACL}.
This metric is based on a rule-based fact-extraction method, CheXpert~\cite{Irvin:2019:AAAI}, which labels the status (present, absent, uncertain) of 14 radiological observations.
By applying this procedure to both the reference and candidate summary, we can calculate a precision/recall of facts.

\paragraph{Implementation and hyperparameters.}
For all summarization models, we use the hyperparameters and code of the original papers.
Below, we focus on deviations from those settings and report all hyperparameters in \cref{sec:appendix-replication-models}.

For BertExt, BertAbs and GSum, we make three adaptations: (i) the summary length of BertExt is set to the average number of sentences selected by OracleExt, rounded to the nearest integer,\footnote{On MIMIC-CXR and OpenI $|\text{OracleExt}(\bm{x}, \bm{y})| \approx 1$.} (ii) we reduce the training steps to 20,000 to account for the smaller datasets, and (iii) to address an exploding gradient problem, we reduce the initial learning rate by a factor of 10.
For final testing, we take the checkpoint with lowest validation loss on MIMIC-CXR. On OpenI, we found the loss to be unstable, so opted to select models by validation R-1.

Regarding the guidance-length prediction models (Method 1 in \cref{sec:method-variable-length-guidance}), we experiment with two classifiers.
First, a multinomial logistic regression classifier with unigram bag-of-words features (\textsc{lr-approx}).
Second, as this model may be too simplistic to accurately predict the guidance length, we implement a transformer-based classifier (\textsc{bert-approx}) on top of DistilBERT~\cite{Sanh:2019:DistilBERT}.

\subsection{Fixed-length Guidance (RQ1)}
\label{sec:results-gsum-fixed}
We first aim to understand if extractive summaries can be a useful guidance signal for radiology report summarization.
To this end, we compare BertAbs (i.e., unguided) with GSum in its default configuration (Part 1 in \cref{tab:results}).

\input{42-tab-results}

We find that \textbf{GSum with fixed-length extractive guidance~\cite{Dou:2021:NAACL} does not generalize to the radiology domain.}
Compared with BertAbs, effectiveness decreases by 4.5\% and 3.2\% in R-1 for MIMIC-CXR and OpenI, respectively.
This is surprising as GSum is highly effective on multiple non-medical summarization benchmarks under the same experimental conditions~\cite{Dou:2021:NAACL}.
Our hypothesis is that highly varying summary lengths make the standard fixed-length guidance in GSum ineffective on this data.\footnote{\cref{sec:appendix-target-summary-length} gives the length distribution of targets.}
We empirically verify this hypothesis in the following experiments.

\paragraph{Comparing oracle and automatic guidance.}
To get an upper-bound estimate for extractive guidance signals, we analyze GSum in an unrealistic oracle setting.
Recall from \cref{sec:method-extractive-guidance} that during training of GSum, the guidance signal is extracted by OracleExt, whereas during inference guidance is extracted by BertExt with a summary length fixed to $k=1$ across all reports.
If we instead also use OracleExt as guidance extractor during inference, we see a substantial increase in all metrics (R-1 46.3\textrightarrow58.8 on MIMIC, and R-1 60.1\textrightarrow68.8 on OpenI, all metrics in Appendix \cref{tab:appendix-gsum-oracle}).
\textbf{This oracle experiment demonstrates (i) that GSum learned to rely on guidance, and (ii) that extractive summaries can be a highly effective guidance signal if selected in the right way.}

Given that GSum is effective when we use the oracle guidance (OracleExt), it is important to understand how this guidance signal differs from the automatically extracted guidance (BertExt).
We find that a characterizing difference between the two guidance signals is the length of the resulting summaries.
OracleExt produces summaries with 0/1/2/3 sentences for 2/52/32/14\% of the MIMIC-CXR reports, and for 15/67/14/3\% of the OpenI reports.
This implies that a guidance signal with a length of $k=1$ is too short for 46\% of the MIMIC-CXR reports, whereas on OpenI it is too short for 17\% and too long for 15\%.

\subsection{Variable-length Guidance (RQ2)}
\label{sec:results-variable-length-guidance}
We next evaluate the utility of our proposed variable-length extractive guidance signal (Part 2 of \cref{tab:results}). We make several observations.

First, we find that variable-length extractive guidance substantially improves the effectiveness of GSum.
On MIMIC-CXR, our adaptation is also better than unguided summarization (BertAbs).
In particular, we observe a large increase in factuality, which is critical in the clinical domain.
While we see a similar improvement of GSum on OpenI, this guided summarization model does not improve over BertAbs.
One potential reason is that OpenI is more abstractive than MIMIC-CXR, as indicated by the high degree of novelty (\cref{tab:datasets}) and the relatively low scores of the extractive methods (BertExt, OracleExt in \cref{tab:results}).
This corroborates the findings by \citet{Dou:2021:NAACL}, where GSum was less effective on more abstractive datasets.
For future work, it would be interesting to study the interplay between the degree of abstraction, and the utility of extractive guidance signals.

Second, regarding the different strategies to obtain variable-length extractive summaries, we cannot conclude that one is superior over another.
The classifier-based approaches (LR-Approx, BERT-Approx), and the thresholding-based approach (Thresholding) lead to similar results when the extracted guidance is used downstream in GSum.
For each guidance extraction strategy, we calculate the ROUGE scores of the guidance signal with respect to the gold summaries.
From \cref{tab:results-bertex}, we see that all strategies have the desired effect of increasing content recall, with a smaller sacrifice in precision.

\input{43-tab-bertext}

Third, to better understand how guidance influences the quality of summaries, we plot the \mbox{R-1} scores across different target summary lengths (\cref{fig:results-rouge-by-target}).
We find that variable-length guidance improves the quality of longer summaries, while for shorter targets, extractive guidance is not beneficial.
By manual inspection, we find that short targets are standard phrasings of negative results (e.g., \emph{``No evidence of acute findings''}), whereas longer targets have a higher extractive component by reiterating particular findings.
In practice, it could be interesting to combine unguided and guided methods by letting the radiologist decide whether a long or short summary should be generated.

% Figure environment removed

\paragraph{Comparison with domain-specific guided summarization (WGSum, WGSum+CL).}
Lastly, compared with the domain-specific guided methods (Part 3 of \cref{tab:results}), we find on MIMIC-CXR that GSum with variable-length extractive guidance is just as effective as WGSum and WGSum+CL which use a graph of clinical entities.
On OpenI, our approach improves over WGSum, but is slightly worse than WGSum+CL.

\paragraph{Summary of RQ1/RQ2.}
Overall our results show that extractive summaries are a promising guidance signal for clinical reports without requiring any domain-specific resources.
We envision that this makes it easier to adopt guided summarization in other clinical domains and languages, for which domain-specific resources like ontologies and clinical NER models are not widely available.
