\begin{table*}[t]
\centering
\small
\resizebox{\textwidth}{!}{
\begin{tabular}{llllll}
\toprule
\textbf{Parameter} & \textbf{BertExt} & \textbf{BertAbs} & \textbf{GSum} & \textbf{WGSum} & \textbf{WGSum+CL} \\
\midrule
Training Steps (MIMIC) & 20,000 & 20,000 & 20,000 & 50,000 & 100,000 \\
Training Steps (OpenI) & 20,000 & 20,000 & 20,000 & 20,000 & 20,000 \\
LR (Encoder) & $2\mathrm{e}{-3}$ & $2\mathrm{e}{-4}$ & $2\mathrm{e}{-4}$ & $5\mathrm{e}{-2}$ & $2\mathrm{e}{-4}$ \\
LR (Decoder) & n/a & $2\mathrm{e}{-2}$ & $2\mathrm{e}{-2}$ & $5\mathrm{e}{-2}$ & $5\mathrm{e}{-2}$ \\
Warmup (Encoder) & 10,000 & 20,000 & 20,000 & 8000 & 10,000 \\
Warmup (Decoder) & n/a & 10,000 & 10,000 & 8000 & 7000 \\
Dropout & 0.1 & 0.2 & 0.2 & 0.1 & 0.2 \\
Checkpoint freq. (MIMIC) & 1000 & 2000 & 2000 & 2000 & 2000 \\
Checkpoint freq. (OpenI) & 1000 & 2000 & 2000 & 200 & 200 \\
Decoding & n/a & Beam search & Beam search & Beam search & Beam search \\
Prediction length & n/a & $\ge$ 5 tokens & $\ge$ 5 tokens & $\ge$ 5 tokens & $\ge$ 5 tokens \\
Training GPUs & 3 & 5 & 5 & 4 & 3 \\
Inference GPUs & 1 & 1 & 1 & 1 & 1 \\
Base model & bert-base-uncased & bert-base-uncased & bert-base-uncased & None & \begin{tabular}[x]{@{}l@{}}dmis-lab/biobert-\\base-cased-v1.1\end{tabular} \\
Parameters & 120,512,513 & 180,222,522 & 205,433,914 & 82,260,794 & 221,600,069 \\
\bottomrule
\end{tabular}
}
\caption{Hyperparameters of BertExt/BertAbs~\cite{Liu:2019:EMNLP}, GSum~\cite{Dou:2021:NAACL}, WGSum~\cite{Hu:2021:ACL} and WGSum+CL~\cite{Hu:2022:ACL}. Training steps, warmup and learning rates were adapted as described in \cref{sec:experimental-setup}. Remaining parameters kept as in the original publications.}
\label{tab:hyperparameters-summarization}
\end{table*}
