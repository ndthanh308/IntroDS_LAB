\section{Quality criteria and evaluation}
\label{sec:quality_criteria}
As stated earlier, one of the objectives of this research is to ask the question:  Is the quality of the composite greater than or equal to the sum total of the quality of the individual segments? In other words, the individual segments in the composite should not appear unconnected but should work together as a whole in meeting the artist's intent and quality goals. 

In this section, we lay out the quality criteria, and evaluation approach and discuss the results of our implementations.

\subsection{Quality criteria}
We find that the present methods of evaluating the image quality of a generated image are not sufficient for our purposes. For example, methods such as FID, Inception Score, Precision, and Recall \cite{FID_NIPS, IS_NIPS, sajjadi2018assessing, borji2022pros} are traditionally used for measuring the quality and diversity of generated images, but only with respect to a large set of reference images. Further, they do not evaluate some key properties of concern to us such as conformity of the generated images to the provided inputs, the harmonization achieved when forming images from sub-scenes, and the overall aesthetic and technical quality of the generated images. These properties are key to holistically evaluating the Composite Diffusion approach. To this end, we propose the following set of quality criteria:

\textbf{1. CF:} \textit{Content Fidelity:} 
 The purpose of the text prompts is to provide a natural language description of what needs to be generated in a particular region of the image. The purpose of the control conditions is to specify objects or visual elements within a sub-scene. This parameter measures how well the generated image represents the textual prompts (or control conditions)  used to describe the sub-scene.

\textbf{2. SF:} \textit{Spatial Layout Fidelity:} The purpose of the spatial layout is to provide spatial location guidance to various elements of the image. This parameter measures how well the parts of the generated image conform to the boundaries of specified segments or sub-scenes. 

\textbf{3. BH:} \textit{Blending and Harmony:}
When we compose an image out of its parts, it is important that the different regions blend together well and we do not get abrupt transitions between any two regions.
Also, it is important that the image as a whole appears harmonious, i.e., the contents, textures, colors, etc. of different regions form a unified whole. This parameter measures the smoothness of the transitions between the boundaries of the segments, and the harmony among different segments of the image. 

\textbf{4. QT:} \textit{Technical Quality:}
The presence of noise and unwanted artifacts that can appear in the image generations can be distracting and may reduce the visual quality of the generated image. This parameter measures how clean the image is from the unwanted noise, color degradation, and other unpleasant artifacts like lines, patches, and ghosting appearing on the mask boundaries or other regions of the image.

\textbf{5. QA:} \textit{Aesthetics Quality:} 
 Aesthetics refers to the visual appeal of an image. Though subjective in nature, this property plays a great part in the acceptability or consumption of the image by the viewers or the users.  This parameter measures the visual appeal of the generated image to the viewer. 

\subsection{Evaluation approach}
\label{sec:evaluation}
   In this section, we provide details about our evaluation approach. We first provide information on the baselines used for comparison and then information on the methods used for evaluation such as user studies, automated evaluations, and artist's consultation and feedback. 
   
We deploy the following two baselines for comparison with our methods:
\begin{itemize}
\item \textbf{Baseline 1 (B1)} - \textit{Text to Image:}  This is the base diffusion model that takes only text prompts as the input. Since this input is unimodal, the spatial information is provided solely through natural language descriptions. 
    
\item \textbf{Baseline 2 (B2)} - \textit{Serial Inpainting:}  As indicated in the section \ref{sec:rw_inpainting}, we should be able to achieve a composite generation by serially applying inpainting to an appropriate background image and generating one segment at a time.

\end{itemize}
A sample of images from different algorithms is shown in Figure \ref{fig:image-compare}. We have implemented our algorithms using Stable Diffusion 1.5 \cite{stable_diffusion}as our base diffusion model, and Controlnets 1.1 \cite{zhang2023adding} as our base for implementing controls. The implementation details for our algorithms and two baselines are available in Appendix \ref{sec:experimental_setup}, \ref{sec:serial_inpainting}, \&  \ref{sec:our_method_implementation}.

% Figure environment removed

We measure the performance of our approach against the two baselines using the above-mentioned quality criteria. Specifically, we perform four different kinds of evaluations:

\textbf {(i)} \textit{Human evaluations:}
We created a survey where users were shown the input segment layout and textual descriptions and the corresponding generated image. The users were then asked to rate the image on a scale of 1 to 5 for the five different quality criteria. We utilized social outreach and Amazon MTurk to conduct the surveys and used two different sets of participants: (i) a set of General Population (GP) comprised of people from diverse backgrounds, and (ii) a set of Artists and Designers (AD) comprised of people with specific background and skills in art and design field. 

We found the current methods of automated metrics  \cite{FID_NIPS, IS_NIPS, sajjadi2018assessing, borji2022pros} inadequate for evaluating the particular quality requirements of Composite Diffusion. Hence, we consider and improvise a few automated methods that can give us the closest measure of these qualities. We adopt CLIP-based similarity \cite{clip} to measure content(text) fidelity and spatial layout fidelity. We use Gaussian noise as an indicator of technical degradation in generation and estimate it \cite{chen2015efficient} to measure the technical quality of the generated image. For aesthetic quality evaluation,  we use a CLIP-based aesthetic scoring model \cite{laion_aesthetic} that was trained on -  a dataset of 4000 AI-generated images and their corresponding human-annotated aesthetic scores. ImageReward \cite{xu2023imagereward} is a text-image human preference reward model trained on human preference ranking of over 100,000 images; we utilize this model  to estimate human preference for a comparison set of generated images. 

Additionally, we also do \textbf{(iii)} a qualitative visual comparison of images (e.g., Figures \ref{fig:relevant_comparison},  and  \ref{fig:image-compare}), and \textbf{(iv)} an informal validation by consulting with an artist. We refer readers to Appendix \ref{sec:survey_results}, \ref{sec:automated_evaluation_methods_details}, and \ref{appendix_sec:artworks} for more details on the human and automated evaluation methods. 

% Figure environment removed

% Figure environment removed

\subsection{Results and discussion}
\label{sec:results}
In this section, we summarize the results from the different types of evaluations and provide our analysis for each quality criterion.

\subsubsection{Content Fidelity} In both types of human evaluations, GP and AD, Composite Diffusion(CD) scores are higher than the two baselines. Composite Diffusion also gets a higher score for content fidelity on automated evaluation methods. 

\textbf{Our take:} This can be attributed to the rich textual descriptions used for describing each image segment, resulting in an overall increase in semantic information and control in the generation process. One can argue that similar rich textual descriptions are also available for the serial inpainting method (B2). However, B2 might get several limitations: (i) There is a dependency on the initial background image that massively influences the inpainting process, (ii) There is a sequential generation of the segments, which would mean that the segments that are generated earlier are not aware of the full context of the image. (iii) The content in textual prompts may sometimes be missed as the the prompts for  inpainting apply to the whole scene than a sub-scene generation.

\begin{table}[t!]
\centering
\caption{Automated evaluation results. The best performing algorithm in a category is marked in bold}
\label{tab:automated_result}
\begin{small}
\begin{tabular}{|c|c|c|c|}
\hline
                          & \textbf{B1} & \textbf{B2} & \textbf{Ours} \\ \hline
Content Fidelity $\uparrow$    & 0.2301          & 0.2485          & \textbf{0.2554}          \\ \hline
Spatial Layout Fidelity $\uparrow$ & 0.2395          & 0.2632          & \textbf{0.2735}          \\ \hline
Blending \& Harmony $\downarrow$         & 6903            & \textbf{725}             & {7404}          \\ \hline
Technical Quality $\downarrow$     & 1.34            & 2.6859          & \textbf{1.2438}        \\ \hline
Aesthetic Quality $\uparrow$   & 6.3448          & 5.5069          & \textbf{6.3492}          \\ \hline
Human Preference $\downarrow$ & 3               & 2               & \textbf{1}               \\ \hline
\end{tabular}
\end{small}
\end{table}

\subsubsection{Spatial Fidelity} This is a key parameter for our evaluation. All three types of evaluation methods - Human evaluation GP and AD, and automated methods - reveal a superior performance of Composite Diffusion.  

\textbf{Our take:}This is on expected lines. Text-to-Image (B1) provides no explicit control over the spatial layout apart from using natural language to describe the relative position of objects in a scene.  B2 could have spatial fidelity better than B1 and equivalent to Composite Diffusion. It does show an improvement over B1 in human evaluation-AD and automated methods. Its lower spacial conformance compared to Composite Diffusion can be attributed to the same reasons that we discussed in the previous section.

\subsubsection{Blending and Harmony} Human-GP evaluation rates our method as the best, while Human-AD evaluation and automated methods give an edge to the serial inpainting method. 

\textbf{Our take:} Text-to-Image (B1) generates one holistic image, and we expect it to produce a well-harmonized image. This higher rating for the serial-inpainting method could be due to the particular implementation of inpainting that we use in our project. This inpainting implementation (RunwayML SD 1.5 \cite{stable_diffusion})is especially fine-tuned to provide seamless filling of a masked region by direct inference similar to text-to-image generation. Further, in Composite Diffusion, the blending and harmonization are affected by the chosen scaffolding value, as shown in Appendix table  \ref{tab:kappa_ablation}.

\subsubsection{Technical Quality} Human evaluation-GP gives our method a better score, while Human evaluation-AP gives a slight edge to the other methods. The automated evaluation method considers only one aspect of technical quality, viz., the presence of noise; our algorithm shows lesser noise artifacts.

\textbf{Our Take:} Both serial-inpainting and Composite Diffusion build upon the base model B1. Any derivative approach risks losing the technical quality while attempting to introduce control. Hence, we expect the best-performing methods to maintain the technical quality displayed by B1. However, repeated application of inpainting to cover all the segments in B2 may amplify any noisy artifact introduced in the early stages. We also observed that for Composite Diffusion, if the segment masks do not have well-demarcated boundaries, we might get unwanted artifacts in the generated composites. 

\subsubsection{Aesthetical Quality} Human evaluation-GP gives Composite Diffusion method a clear edge over baseline methods, while Human evaluation-AP results show a comparable performance. The automated evaluation methods rate our method higher than the serial inpainting and only marginally higher than the text-to-image baseline. 

\textbf{Our take:} These results indicate that our approach does not cause any loss of aesthetic quality but may even enhance it.   The good performance of Composite Diffusion in aesthetic evaluation can be due to the enhanced detail and nuance with both textual and spatial controls. The lack of global context of all the segments in serial inpainting and the dependence on an appropriate background image put it at a slight disadvantage. Aesthetics is a subjective criterion that can be positively influenced by having more meaningful generations and better placements of visual elements. Hence, combining segment layouts and content conditioning in Composite Diffusion may lead to compositions with more visually pleasing signals. 

We further did a qualitative validation with an external artist. We requested the artist to specify her intent in the form of freehand drawings with labeled descriptions. We manually converted the artist's intent to bimodal input of segment layout and textual descriptions suitable for our model.   We then created artwork through Composite Diffusion and asked the artist to evaluate them qualitatively. The feedback was largely positive and encouraging. The artist's inputs, the generated artwork, and the artist's feedback are available in the Appendix section \ref{appendix_sec:artworks}. 

 We also present a qualitative visual comparison of our generated outputs with the baselines and other related approaches in Figures \ref{fig:image-compare} and \ref{fig:relevant_comparison} respectively. Summarizing the results of multiple modes of evaluation, we can affirm that our Composite Diffusion methods perform holistically and well across all the different quality criteria.
