\section{Our experimental setup}
\label{sec:experimental_setup}
As stated earlier, in this work, we aim to generate a composite image guided entirely  by free-form segments and corresponding natural textual prompts (with optional additional control conditions).  In this section, we summarize our choice of base setup, provide a running example to help explain the working of different algorithms, and provide implementation details of the base setup.

% Figure environment removed

\subsection{Running example} To explain the different algorithms, we will use a common running example. The artist's input is primarily bimodal: free-form segment layout and corresponding natural language descriptions as shown in Figure \ref{fig:running-example}. As a first step common to all the algorithms, the segment layout is converted to segment masks as one-hot encoding vectors where `$0$' represents the absence of pixel information, and `$1$' indicates the presence of image pixels. To standardize the outputs of the generative process, all the initial inputs (noise samples, segment layouts, masks, reference, and background images) and the generated images in this paper are of 512x512 pixel dimensions. Additionally, in the case of latent diffusion setup, we downsize the masks, encode the reference images, and sample the noise into 64x64 pixels corresponding to the latent space dimensions of the model. 

\subsection{Implementation details }
We choose open-domain diffusion model architecture, namely \textit{Stable Diffusion} \cite{stable_diffusion}, to serve as base architectures for our composite diffusion methods. Table \ref{tab:setup_summary} provides a summary of the features of the base setup. 
The diffusion model has a U-Net backbone with a cross-attention mechanism, trained to support conditional diffusion. We use the pre-trained text-to-image diffusion model (Version 1.5) that is developed by researchers and engineers from CompVis, Stability AI, RunwayML, and LAION and is trained on 512x512 images from a subset of the LAION-5B dataset. A frozen CLIP ViT-L/14 text encoder is used to condition the model on text prompts. For scheduling the diffusion steps and  sampling the outputs, we use DDIM \cite{song2020denoising}. 


% Table
\begin{table}[ht!]%
\caption{Summary of features of  the base setup}
\label{tab:setup_summary}
\begin{minipage}{\columnwidth}
\begin{center}
\begin{tabular}{ll}
  \toprule
  Feature & Setup  \\ 
  \midrule
  Diffusion Space   & Latent\\ 
  Conditionality & Conditional\\
  Guidance      & Classifier-free\\
  Model Size  & $\approx$ 850 million\\
  Open Domain Models      & StabilityAI \\
  Sampling Method  & DDIM \\
  \bottomrule
\end{tabular}
\end{center}
\bigskip\centering
\end{minipage}
\end{table}

% Figure environment removed

\begin{algorithm}[ht!]
\caption{Text-to-Image generation in the base setup}
\label{alg:setup-B}
\textbf{Input}
Target text description $d$,\\
Initial image, $x_T \sim \mathcal{N}(0, \textbf{I})$, Number of diffusion steps = $k$.

\textbf{Output:}  An output image, $x_{0}$, which is sufficiently grounded to input $d$.

$z_T \leftarrow \mathcal{E}(x_T)$, \tcp*{Encode into latent space} 
$d_z \leftarrow \mathcal{C}(d)$ \tcp*{Create CLIP text encoding} 

    \For {\textbf{all} $t$ \textbf{from} $k$ to $1$} {
        $z_{t-1} \leftarrow Denoise(z_t , d_z)$ \tcp*{Denoise using text-condition and DDIM}   
    }
  
\Return  $x_{0} \leftarrow  \mathcal{D} (z_{0})$ \tcp*{Final Image}
\end{algorithm}

We describe image generation through this setup in the next  section.

\subsection{Text-to-Image generation in the base setup}
In this setup (refer to Figure \ref{fig:setup-B}), a pixel-level image ($x$) is first encoded  into a  lower-dimensional latent-space representation with the help of a variational autoencoder(VAE) ($\mathcal{E}(x)\rightarrow  z$). The diffusion process then operates in this latent space. 
This setup uses a conditional\footnote {In practice, the model is trained to act as a both conditional and unconditional model. An empty text prompt is used for unconditional generation along with the input text prompt for conditional generation. The two results are then combined to generate a better quality denoised image. Refer to section \ref{sec:background_cfg}.} diffusion model which is pre-trained on natural text using CLIP encoding.  For a generation,  the model takes CLIP encoding of the  natural text ($\mathcal{C}(d) \rightarrow d_{CLIP} $) as the conditioning input and directly infers a denoised sample $z_t$ without the help of an external classifier  (classifier free guidance) \cite{ho2022classifier}. Mathematically, we use equation \ref{eq:cond-infer} for generating the additive noise $\hat{\epsilon}_{\theta}$ at timestep $t$, and use equation \ref{eq:ddim_sample} for generating $z_t$ from $\hat{\epsilon}_{\theta}$ via DDIM sampling. After the diffusion process is over, the resultant latent $z_0$ is decoded back to pixel-space ($\mathcal{D}(z_0) \rightarrow  x_0$).

 As stated earlier, spatial information cannot be adequately described through only text conditioning. In the next section, we extend the existing in-painting methods to support Composite Diffusion. However, we shall see that these methods do not fully satisfy our quality desiderata which leads us to the development of our approach for Composite Diffusion as described in the main paper.

 
