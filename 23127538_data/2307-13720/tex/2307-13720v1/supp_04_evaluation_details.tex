\section{Human evaluation and survey details}
\label{sec:survey_results}

During the course of the project, we conducted a set
of three different surveys. A preliminary and a revised
survey were conducted on the general population, and a separate survey was conducted on artists and designers. In these surveys,
we evaluated the generated outputs of text-to-image
generation, serial inpainting generation methods, and
our composite generation methods. The text-to-image
generations serve as the first baseline (\textbf{B1}) and serial inpainting generations serve as the second baseline (\textbf{B2})
for comparison.

% Figure environment removed

\subsection{Survey design}
The survey design involved the following parts:

\subsubsection{Method for choosing survey samples} 
We designed five different input sets for the survey. The free-form segment layouts and corresponding text descriptions were chosen to bring in a variety of scenarios for the input cases. Refer to Fig. \ref{fig:survey-segment-layouts} for the inputs and to Fig. \ref{fig:image-compare} for a sample of generated images for the survey. Since we also wanted to compare our generations with the base model generations, and text-to-image model only allows a single text prompt input, we manually crafted the prompts for the first base case. This was done by: (i) creating a text description that best tries to capture the essence of different segment prompts, (ii) concatenating the different segment descriptions into a single text description. The best of these two generations were taken as the representative pictures for base models. For selecting the samples from the different algorithms we followed the following protocol. Since the underlying models are of different architecture (for example, the serial inpainting method uses a specialized inpainting model and requires a background image), we generated 3 images using random seeds for each algorithm and for each set of inputs. We then chose the best representatives (1 out of 3) from each algorithm for the survey samples. 

% Figure environment removed

\subsubsection{Survey questions:} In each of the surveys, the survey takers were presented with a Google form on the web containing anonymized and randomly sorted images generated from these three algorithms with corresponding inputs. The respondents were asked to rate each of these images on five quality parameters. We explained each quality parameter and  asked a corresponding quality question as listed below:

\begin{enumerate}
  \item \textit{Text Fidelity:} How closely does the image match the text prompts? 
  \item \textit{Mask Fidelity:}  How closely does the image match the mask shapes?
  \item \textit{Blending \& Harmony:} How well do the segments blend together and how harmonious is the overall image?
  \item \textit{Technical Quality:} How would you rate the overall technical quality of the image?  
  \item \textit{Aesthetic Quality:} How would you rate the overall aesthetic quality of the image?
\end{enumerate}

The respondents were asked to rate a generated image for a given quality parameter on a scale of 1 to 5 (semantic differential scales). We also provided a rough rating 
guideline for these parameters. Refer to Fig.\ref{fig:survey_interface} for a snapshot of the web survey.

\subsection{Survey execution:}
The details of the execution of  the three surveys are as follows:

\subsubsection{Phase 1: Preliminary survey:} We conducted this survey on a diverse set of 14 respondents who were spread across age (20-80), gender, and profession.  Our experience with the first survey gave us vital feedback on how to design the survey more effectively. For example, many surveyors said that they found it tough to take the survey as it was lengthy. There were a total of 75 rating questions that needed to be answered. So there was some fatigue due to the cognitive load. The first survey was organized in the following manner: Each set of inputs was a separate page and contained all five quality questions. On each page, the respondents were presented with 3 pics from 3 different algorithms(anonymized and randomly sorted) and were asked to rate each of the pictures on five quality parameters. We also received feedback that all the guidance information was on the front page,  and they had to revisit it several times to understand the rating guidelines and the meaning of each quality parameter. Further, some users told us that `aesthetics' influenced their rating of the other qualities; They tended to rate an image with higher aesthetics higher for other qualities as well.

\subsubsection{Phase 2: Revised survey}
We built upon this feedback, and without changing the content, restructured the survey to make it more modular for our final assessment. We also found the guidelines in \cite{bylinskii2022towards} relevant and followed them to fine-tune the survey organization. The two major changes were: (1) Each quality parameter was made into a separate survey. This was done to help the surveyors focus on one quality parameter at a time. (2) We provided guidelines for the score rating on each of the survey pages as a ready reference. 

The survey was further divided into two sets of Surveyors representing different sets of professional skills. 

\begin{itemize}

\item \textbf{Survey population: Artists and Designers (AD)}: We conducted this survey during the final phase of our project. We used the same set of images as used in the preliminary survey to collect responses from artists and designers. We took the help of Amazon M-Turk for collecting responses for this survey.  There was no restriction on whether a person took all 5 surveys or only a subset of them. There were a total of 20 respondents for each of the five surveys (where one survey was comprised of a distinct quality parameter). 

\item \textbf{Survey population: General (GP)}: 
We conducted this survey simultaneously with the above survey.  The participants in this survey were chosen from a larger general population that also included professionals such as engineers and software developer. In this case, 22 respondents completed all the five survey sets, while 48 completed at least one set.

\end{itemize}

\begin{table}[t!]
\resizebox{\columnwidth}{!}{
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Method}         & \textbf{B1} & \textbf{B2} & \textbf{Ours} \\ \hline
Content Fidelity $\uparrow$         & 3.81±1.0    & 3.22±1.07   & 3.92±0.97     \\ \hline
Spatial Layout Fidelity $\uparrow$ & 3.21±1.08   & 3.14±1.15   & 3.62±0.97     \\ \hline
Blending \& Harmony $\uparrow$     & 3.87±0.96   & 4.02±0.99   & 3.86±1.03     \\ \hline
Technical Quality $\uparrow$        & 3.85±1.02   & 3.75±1.15   & 3.6±0.98      \\ \hline
Aesthetic Quality $\uparrow$       & 3.55±0.92   & 3.55±1.0    & 3.52±0.99     \\ \hline
\end{tabular}}
\caption{Results of the survey conducted on Artists and Designers}
\label{tab:artists_designers}
\end{table}



\begin{table}[t!]
\resizebox{\columnwidth}{!}{
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Method}         & \textbf{B1} & \textbf{B2} & \textbf{Ours} \\ \hline
Content Fidelity $\uparrow$         & 2.8±1.28   & 2.38±1.13   & 3.12±1.45     \\ \hline
Spatial Layout Fidelity $\uparrow$ & 2.19±1.11   & 2.99±1.44   & 3.82±1.08     \\ \hline
Blending \& Harmony $\uparrow$     & 3.47±1.07   & 2.94±1.22   & 3.62±1.14     \\ \hline
Technical Quality $\uparrow$        & 3.33±1.11   & 2.78±1.16   & 3.39±1.14     \\ \hline
Aesthetic Quality $\uparrow$       & 3.16±1.19   & 2.66±1.26   & 3.36±1.28     \\ \hline
\end{tabular}}
\caption{Results of the survey conducted on General Population.}
\label{tab:general_population}
\end{table}

\subsection{Survey Results}
Table \ref{tab:artists_designers} presents the results of the survey for the artists and designers population, and Fig. \ref{fig:evaluation-ad} presents a graphical representation of the same for easy comparison. Since the set of images and the survey questions were the same across the two phases of the survey, we consolidate the results of general population responses. Table \ref{tab:general_population} presents the consolidated results of the survey of the general population, and Fig. \ref{fig:evaluation-gp} gives a graphical representation of the same.

\begin{table*}[htbp!]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
\textbf{Kappa} & \textbf{Content Fidelity $\uparrow$} & \textbf{Spatial Layout Fidelity $\uparrow$} & \textbf{Technical Quality $\uparrow$} & \textbf{Human Preference $\downarrow$} & \textbf{Aesthetic Score$\uparrow$} & \textbf{Blending \& Harmony $\downarrow$} \\ \hline
0              & 0.2634                 & 0.278                     & 1.2612                & 3                         & 6.1809                  & 5321              \\ \hline
20             & 0.2629                 & 0.278                     & 1.2079                & 3                         & 6.1487                  & 6137              \\ \hline
40             & 0.2596                 & 0.2726                    & 1.6987                & 3                         & 6.296                   & 8078              \\ \hline
60             & 0.2627                 & 0.2757                    & 1.4186                & 4                         & 6.2565                  & 6827              \\ \hline
80             & 0.2594                 & 0.2744                    & 1.3123                & 4                         & 5.9693                  & 7235              \\ \hline
100            & 0.2579                 & 0.2773                    & 1.7702                & 3                         & 6.0798                  & 7699              \\ \hline
\end{tabular}}
\caption{Automated Method evaluation across different scaffolding factor $\kappa$ values. We observe that the general trend is that Blending \& Harmony (lower is better) progressively gets slightly worse as we move from  lower to higher $\kappa$, while the other factors remain quite similar across different $\kappa$ values.}
\label{tab:kappa_ablation}
\end{table*}

\section{Automated evaluation methods}
\label{sec:automated_evaluation_methods_details}
We find that the present methods of automated quality comparisons such as FID and IS aren't well suited for the given quality criteria. In the section below we discuss a few of the methods that are widely used in measuring the capabilities of generative models, point out their drawbacks,  and then detail our methods for automated evaluation. 

\subsection{Current approaches for automated evaluation} 
Inception score (IS), Fréchet inception distance (FID), precision and recall are some of the commonly used metrics for assessing the quality of synthetically generated images \cite{FID_NIPS, IS_NIPS, sajjadi2018assessing, borji2022pros}. IS score jointly measures the diversity and quality of generated images. FID measures the similarity between the distribution of real images and generated images. Metrics like precision and recall \cite{sajjadi2018assessing} separately capture the quality and diversity  aspects of the generator. Precision is an indicator of how much the generated images are similar to the real ones, and recall measures how good the generator is in synthesizing all the instances of the training data set \cite{borji2022pros}.

These approaches have some drawbacks to our requirement of assessing the quality of Composite Diffusion generations:  (i) These approaches require a large set of reference images to produce a statistically significant score. The distribution of the training set images is not relevant to us. We need datasets that have - an input set of sub-scene layouts along with textual descriptions of those sub-scenes, and a corresponding set of reference images., (ii) Even if we had the facility of a relevant large dataset, these methods assume that the reference images provide the highest benchmark for quality and diversity. This might not be always true as the generated images can exceed the quality of reference images and have a variety that is different from the reference set., and (iii) These methods don't measure the quality with the granularity as described in the quality criteria that we use in this paper.


\subsection{Our approach for automated evaluation}

We devise the following automated methods to evaluate the generated images based on our quality criteria. 

\textbf{Content Fidelity $\uparrow$:} The objective here is to obtain a measure of how similar the image is to each of the artist's intended content, and in this case, we use the textual descriptions as content. We compute the cosine similarity between the CLIP embeddings of the image and the CLIP embeddings of each segment's description. We then take the mean of these similarity scores. Here \textit{a greater score indicates greater content fidelity}.

\textbf{Spatial-layout Fidelity $\uparrow$:} The objective here is to measure how accurately we generate a segment's content. We use masking to isolate a segment from the image. We find the CLIP similarity score between the masked image and that segment's description. We do this for all the segments and then take the mean of these scores. Here \textit{a greater score indicates greater spatial-layout fidelity}.

\textbf{Technical Quality $\downarrow$:} The goal here is to measure if there are any degradations or the presence of unwanted artifacts in the generated images. It is difficult to define all types of degradations in an image. We consider the presence of noise as a vital form of degradation. We estimate the Gaussian noise level in the image by using the method described in \cite{chen2015efficient}. Here \textit{a lower score indicates greater technical quality}.

\textbf{Aesthetics $\uparrow$:} We use the aesthetic quality estimator from \cite{laion_aesthetic} to get an estimate of the aesthetic quality of the image. This method uses a linear layer on top of the CLIP embedding model and is trained on 4000 samples to estimate if an image is looking good or not. Here \textit{a greater score indicates greater perceived aesthetics}.

\textbf{Blending \& Harmony $\downarrow$:} We detect the presence of edges around the segment boundaries as a measure of better blending. Hence \textit{a lower value in this case indicates better blending}. 

\textbf{Human Preference $\downarrow$:} To additionally estimate the human preference we rank the images generated by the different algorithms using ImageReward\cite{xu2023imagereward}. This method uses a data-driven approach to score human preferences for a set of images. Here \textit{a greater score indicates lower preference}.

\subsection{Limitations of automated evaluation methods}
As stated in the main paper, these measures are the initial attempts and may give only a ballpark estimation of the qualities under consideration. Content Fidelity and Spatial-layout metrics are only as good as the capability underlying the image-text model - OpenAI's CLIP model \cite{clip}. Technical quality should give an overall measure of technical aberrations like color degradation, unwanted line artifacts, etc. However, we limit ourselves to only measuring the overall noise levels. Aesthetics is a highly subjective aspect of image quality and the CLIP aesthetic model \cite{laion_aesthetic}, though effective, has been trained on a relatively small-sized dataset. Blending \& Harmony in our case is limited to measuring the presence of edges around the boundaries of a segment. Measuring harmony in images is a challenging problem as one needs to also consider the positioning, scale, and coloring of the elements and segments in the context of the overall image. Human preference scoring utilizes ImageReward \cite{xu2023imagereward}, which models the ranking that humans would assign to a group of images.  Although this method performs better than CLIP and BLIP in this aspect, it lacks the explainability of why one image is ranked higher over the other.

Finding better, more precise, and holistic machine-assisted methods for measuring the qualities presented in this paper is an opportunity for future research. 


\subsection{Benchmark dataset}
A notable challenge in the automated evaluation of the composite diffusion method is the lack of benchmark datasets. Currently, there do not exist any datasets which consist of segment (or sub-scene) layouts with rich textual descriptions for each segment. Creation of such a dataset is non-trivial using automated methods and requires expensive annotation \cite{Avrahami_2023_CVPR}. 

We handcraft a dataset containing 100 images where we segment each representative image into sub-scenes and manually annotate each sub-scene with a relevant textual description. This enables us to build a benchmark dataset for composite image generation with sufficiently high-quality data. We use this dataset to generate images using our baseline and Composite Diffusion methods. We  use the automated methods described above to get the automated evaluation results (Table \ref{tab:kappa_ablation}). 
 

