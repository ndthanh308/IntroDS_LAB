\section{Our method: details and features}
\label{sec:our_method_implementation}
In the main paper, we presented a generic algorithm that is applicable to any diffusion model that supports \textit{conditional generation with classifier-free implicit guidance}. Here, we present the implementation details and elaborate on a few downstream applications of Composite Diffusion. 

\subsection{Implementation details of the main algorithm}
 In the previous Appendix section \ref{sec:experimental_setup}, we detailed the actual base model which we use as the example implementation of Composite Diffusion. Since the base setup operates in latent diffusion space, to implement our main Composite Diffusion algorithm in this setup, we have to do two additional steps: \textbf{(i)} Prepare the input for latent diffusion by decoding all the image latents through a VAE to 64x64 latent pixel space, \textbf{(ii)} After the Composite Diffusion process (refer to Fig. \ref{fig:our-algo} for the details of typical steps),  use a VAE decoder to decode the outputs of the latent diffusion model into the 512x512 pixel space. Since the VAE encoding maintains the spatial information, we either directly use a 64x64 pixel segment layout, or  downsize the resulting masks to 64x64 pixel image space.

% Figure environment removed

As mentioned in the main paper, for supporting additional control conditions in Composite Diffusion, we use the Stable Diffusion v1.5 compatible implementation of ControlNet \cite{zhang2023adding}. ControlNet is implemented as a parallel U-Net whose weights are copied from the main architecture, but which can be trained on particular control conditions \cite{zhang2023adding} such as canny edge, lineart, scribbles, semantic segmentations, and open poses. 

In our implementation, for supporting \textit{control conditions} in segments, we first prepare a control input for every segment. The controls  that we experimented with included lineart, open-pose, and scribble. Each segment has a separate control input that is designed to be formed in a 512x512 image space but only in the region that is specific to that segment.   Each control input is then passed through an encoding processor that creates a control condition that is embedded along with the text conditioning. ControlNets convert image-based conditions to 64 Ã— 64 feature space to match the convolution size: 
$c_f = \mathcal{E}(c_i)$  (refer to equation 9 of \cite{zhang2023adding}), 
where $c_i$ is the image space condition, and $c_f$ is the corresponding converted feature map. 

Another important aspect is to use a ControlNet model that is particularly trained for the type of control input specified for a segment. However, as shown in the main paper and also illustrated in Fig. \ref{fig:teaser}, more than one type of ControlNets can be deployed for different segments for achieving Composite Diffusion.

\subsection{Example runs}
With reference to the running example shown in the main paper, we present the different stages of the evolution of a composite image using Serial Inpainting and our Composite Diffusion algorithms. Refer to Figures \ref{fig:gen-steps}, \ref{fig:ex-run-si-a}, \ref{fig:ex-run-si-b}, \ref{fig:ex-run-si-c}, and \ref{fig:ex-run-cd}. To standardize our depiction, we run each algorithm for a total of 50 diffusion steps using DDIM as the underlying sampling method. The figures show every alternate DDIM step. 

% Figure environment removed


