\section{Related work}
\label{sec:related_work}
In this section, we discuss the approaches that are related to our work from multiple perspectives.

\subsection{Text-to-Image generative models} The field of text-to-image generation has recently seen rapid advancements, driven primarily by the evolution of powerful neural network architectures. Approaches like DALLÂ·E \cite{dalle} and VQ-GAN \cite{vqgan} proposed a two-stage method for image generation. These methods employ a discrete variational auto-encoder (VAE) to acquire comprehensive semantic representations, followed by a transformer architecture to autoregressively model text and image tokens. Subsequently, diffusion-based approaches, such as Guided Diffusion \cite{openai-guided-diff} \cite{diffusion_models_dhariwal}, have showcased superior image sample quality compared to previous GAN-based techniques. Dalle-2 \cite{dalle2} and Imagen \cite{imagegen} perform the diffusion process in the pixel-image space while Latent Diffusion Models such as Stable Diffusion \cite{stable_diffusion} perform the diffusion process in a more computationally suitable latent space.  However, in all these cases, relying on single descriptions to depict complex scenes restricts the level of control users possess over the generation process.

% Figure environment removed

\subsection{Spatial control models} Some past works on image generation have employed segments for spatial control but were limited to domain-specific segments. For example, GauGAN \cite{gaugan} introduced spatially-adaptive normalization to incorporate semantic segments to generate high-resolution images. PoE-GAN \cite{poe_gan} utilized the product of experts method to integrate semantic segments and a global text prompt to enhance the controllability of image generation. However, both approaches rely on GAN architectures and are constrained to specific domains with a fixed segment vocabulary. Make-A-Scene \cite{make_a_scene} utilized an optional set of dense segmentation maps, along with a global text prompt, to aid in the spatial controllability of generation. VQ-GAN \cite{vqgan} can be trained to use semantic segments as inputs for image generation. No-Token-Left-Behind \cite{paiss2022token} employed explainability-based methods to implement spatial conditioning in VQ-GAN; they propose a method that conditions a text-to-image model on spatial locations using an optimization approach. The approaches discussed above are also limited by training only on a fixed set of dense segments.

\subsection{Inpainting} 
\label{sec:rw_inpainting}
The work that comes closest to our approach in diffusion models is in-painting. Almost all the popular models \cite{dalle2}, \cite{imagegen},  \cite{stable_diffusion} support some form of inpainting. The goal of inpainting is to modify a portion in an image specified by a segment-mask (and optional accompanying textual description) while retaining the information outside the segment. Some of the approaches for inpainting in the recent past include repaint \cite{repaint}, blended-diffusion\cite{avrahami2022blended}, and latent-blended diffusion \cite{latent_blended}. RunwayML \cite{stable_diffusion} devises a specialized model for in-painting in Stable Diffusion, by modifying the architecture of the UNet model to include special masked inputs.  As we show in later this paper, one can conceive of an approach for Composite Diffusion using inpainting, where we can perform inpainting for each segment in a serial manner (refer to Appendix  \ref{sec:serial_inpainting}). However, as we explain in this paper, a simple extension of localized in-painting methods for multi-segment composites presents some drawbacks. 

\subsection{Other diffusion-based composition methods}
Some works look at the composition or editing of images through a different lens. These include prompt-to-prompt editing \cite{hertz2022prompt, mokady2022null}, composing scenes through composable prompts \cite{liu2022compositional}, and methods for personalization of subjects in a generative model \cite{ruiz2022dreambooth}.  Composable Diffusion \cite{Composable-Diffusion} takes a structured approach to generate images where separate diffusion models generate distinct components of an image. As a result, they can generate more complex imagery than seen during the training. Composed GLIDE \cite{liu2022compositional} is a composable diffusion implementation that builds upon the GLIDE model \cite{glide} and utilizes compositional operators to combine textual operations. Dreambooth \cite{ruiz2022dreambooth} allows the personalization of subjects in a text-to-image diffusion model through fine-tuning. The learned subjects can be put in totally new contexts such as scenes, poses, and lighting conditions. Prompt-to-prompt editing techniques \cite{couairon2022diffedit, hertz2022prompt, mokady2022null} exploit the information in cross-attention layers of a diffusion model by pinpointing areas that spatially correspond to particular words in a prompt. These areas can then be modified according to the change of the words in the prompt. 
Our method is complementary to these advances. We concentrate specifically on composing the spatial segments specified via a spatial layout. So, in principle, our methods can be supplemented with these capabilities (and vice versa). 

\subsection{Spatial layout and natural text-based models }
In this section, we discuss three related concurrent works: SpaText \cite{Avrahami_2023_CVPR}, eDiffi \cite{balaji2023ediffi}, and Multi-diffusion \cite{bar2023multidiffusion}.  All these works provide some method of creating images from spatially free-form layouts with natural text descriptions. 

SpaText \cite{Avrahami_2023_CVPR} achieves spatial control by training the model to be space-sensitive by additional CLIP-based spatial-textual representation. The approach requires the creation of a training dataset and extensive model training, both of which are costly. Their layout schemes differ slightly from ours as they are guided towards creating outlines of the objects, whereas we focus on specifying the sub-scene. 

eDiffi \cite{balaji2023ediffi} proposes a method called paint-with-words which exploits the cross-attention mechanism of U-Net in the diffusion model to specify the spatial positioning of objects. Specifically, it associates certain phrases in the global text prompt with particular regions by manipulating the cross-attention matrix. Similar to our work, they do not require pre-training for a segment-based generation. However, they must create an explicit control for the objects in the text description for spatial control. We use the inherent capability of U-net's cross-attention layers to guide the relevant image into the segments through step-inpainting and other techniques.

Multi-diffusion \cite{bar2023multidiffusion} proposes a mechanism for controlling the image generation in a region by providing the abstraction of an optimization loss between an ideal output by a single diffusion generator and multiple diffusion processes that generate different parts of an image. It also provides an application of this abstraction to segment layout and natural-text-based image generation. This approach has some similarities to ours in that they also build their segment generation by step-wise inpainting. They also use bootstrapping to anchor the image and then use the later stages for blending. However, our approach is more generic, has a wider scope, and is more detailed.
For example, we don't restrict the step composition to a particular method. Our scaffolding stage has a much wider significance as our principal goal is to create segments independent of each other, and the goal of the harmonization stage is to create segments in the context of each other. We provide alternative means of handling both the scaffolding and harmonization stages. 

Further,  in comparison to all the above approaches, we achieve \textit{additional control over the orientation and placement of objects within a segment} through reference images and control conditions specific to the segment.










