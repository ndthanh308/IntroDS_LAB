\section{Introduction}
\label{sec:introduction}

% Figure environment removed

Recent advances in diffusion models \cite{diffusion_models_dhariwal}, such as Dalle-2 \cite{dalle}, Imagen \cite{imagegen}, and Stable Diffusion \cite{stable_diffusion} have enabled artists to generate vivid imagery by describing their envisioned scenes with natural language phrases.
However, it is cumbersome and occasionally even impossible to specify spatial information or sub-scenes within an image solely by text descriptions. Consequently, artists have limited or no direct control over the layout, placement, orientation, and properties of the individual objects within a scene. These creative controls are indispensable for artists seeking to express their creativity \cite{solidarnyh2023} and are crucial in various content creation domains, including illustration generation, graphic design, and advertisement production. Frameworks like Controlnets \cite{zhang2023adding} offer exciting new capabilities by training parallel conditioning networks within diffusion models to support numerous control conditions. Nevertheless, as we show in this paper, creating a complex scene solely based on control conditions can still be challenging.  As a result, achieving the desired imagery may require several hours of labor or maybe only be partially attainable through pure text-driven or control-condition-driven techniques.



To overcome these challenges, we propose \textbf{Composite-Diffusion}  as a method for creating composite images by combining spatially distributed segments or sub-scenes. These segments are generated and harmonized through independent diffusion processes to produce a final composite image. The \textit{artistic intent} in Composite Diffusion is conveyed through the following two means:

\textbf{(i) Spatial Intent:} Artists can flexibly arrange sub-scenes using a free-form spatial layout. A unique color identifies each sub-scene.

\textbf{(ii) Content intent:}Artists can specify the desired content within each sub-scene through text descriptions. They can augment this information by using examples images and other control methods such as scribbles, line drawings, pose indicators, etc.

We believe, and our initial experience has shown, that this approach offers a powerful and intuitive method for visual artists to stipulate their artwork.

This paper seeks to answer two primary research questions: First, how can native diffusion models facilitate composite creation using the diverse input modalities we described above? Second, how do we assess the quality of images produced using Composite Diffusion methods? Our paper \textbf{contributes} in the following novel ways:

\textbf{1.}We present a comprehensive, modular, and flexible method for creating composite images, where the individual segments (or sub-scenes) can be influenced not only by textual descriptions, but also by various control modalities such as line art, scribbles, human pose, canny images, and reference images. The method also enables the simultaneous use of different control conditions for different segments. 

\textbf{2.} Recognizing the inadequacy of existing image quality metrics such as FID (Frechet Inception Distance) and Inception Scores \cite{FID_NIPS, IS_NIPS} for evaluating the quality of composite images, we introduce a new set of quality criteria. While principally relying  on human evaluations for quality assessments,  we also develop new methods of automated evaluations suitable for these quality criteria.

We rigorously evaluate our methods using various techniques including quantitative user evaluations, automated assessments, artist consultations, and qualitative visual comparisons with alternative approaches. In the following sections, we delve into related work (Section \ref{sec:related_work}), detail our method (Section \ref{sec:our_method}), and discuss the evaluation and implications of our approach (Section \ref{sec:quality_criteria}, and \ref{sec:conclusion}).

