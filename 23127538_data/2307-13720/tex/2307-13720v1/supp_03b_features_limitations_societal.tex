\subsection{Personalization at a scale}
\label{sec:personalizing_at_scale}
One of the motivations for composite image generation is to produce a controlled variety of outputs. This is to enable customization and personalization at a scale. Our Composite Diffusion models help to achieve variations through: (i) variation in the initial noise sample, (ii) variation in free-form segment layout, (iii) variation through segment content, and (iv) variation through fine-tuned models. 

\subsubsection{Variation through Noise} This is applicable to all the generative diffusion models. The initial noise sample massively influences  the final generated image.  This initial noise can be supplied by a purely random sample of noise or by an appropriately noised (\textit{q-sampled}) reference image. Composite Diffusion further allows us to keep these initial noise variations particular to a segment. This method, however, only gives more variety but not any control over the composite image generations.

\subsubsection {Variation through segment layout}
 We can introduce controlled variation in the spatial arrangement of elements or regions of an image by changing the segment layout while keeping the segment descriptions constant. Refer to figure \ref{fig:church-gens} for an illustration where we introduce two different layouts for any given set of segment descriptions.

\subsubsection {Variation through text descriptions}
Alternatively, we can keep the segment layout constant, and change the description of the segments (through text or control conditions) to bring controlled variation in the content of the segments. Refer to figure \ref{fig:church-gens} for an illustration where each of the three columns represents a different set of segment descriptions for any of the segment layouts.

% Figure environment removed

% Figure environment removed

\subsubsection{Specialized fine-tuned models }
The base diffusion models can be further fine-tuned on specialized data sets to produce domain-specialized image generations. For example, a number of fine-tuned implementations of Stable Diffusion are available in the public domain \cite{FinetunedSD}. This aspect can be extremely useful when creating artwork customized for different sets of consumers. One of the advantages of our composite methods is that as long as the fine-tuning does not disturb the base-model architecture, \textit{our methods allow a direct {plug-and-play} with the fine-tuned models}. 

Figure \ref{fig:fine_tuned_gens} gives an illustration of using 10 different public domain fine-tuned models with our main Composite Diffusion algorithm for generating specific-styled artwork. The only code change required for achieving these results was the change of reference to the fine-tuned model and the addition of style specification in the text prompts.

In the following sections, we discuss some of the limitations of our approach and provide a brief discussion on the possible societal impact of this work.

\subsection{Limitations}
\label{sec:limitations}
Though our method is very flexible and effective in a variety of domains and composition scenarios, we do encounter some limitations which we discuss below:

\textit{Granularity of sub-scenes:} The segment sizes in the segment layout are limited by the diffusion image space. So, as the size of the segment grows smaller, it becomes difficult to support sub-scenes. Our experience has shown that it is best to restrict the segment layout to 2-5 sub-scenes. Some of this is due to the particular model that we use in implementation. Since Stable Diffusion is a latent space diffusion model \cite{stable_diffusion}, the effective size for segment layout is only 64x64 pixels. If we were operating directly in the pixel space, we would have considerably more flexibility because of 8 fold increase in the segment-layout size of 512x512 pixels.

\textit{Shape conformance:} In the only text-only conditioning case,  our algorithms do perform quite well on mask shape conformance. However, total shape adherence to an object only through the segment layout is sometimes difficult. Moreover, in the text-only condition case, while generating an image within a segment the whole latent is in play. The effectiveness of a generation within the segment is influenced by how well the scaffolding image is conducive as well as non-interfering to the image requirements of the segment. This creates some dependency on the choice of scaffolding image. Further, extending the scaffolding stage improves the conformance of objects to mask shapes but there is a trade-off with the overall harmony of the image. 

So in the case where strict object conformance is required, we recommend using the control condition inputs as specified in our algorithm, though this might reduce the diversity of the images that text-only conditioning can produce.

\textit{Training and model limitations:}The quality and variety in generated object configurations are heavily influenced by the variety that the model encounters in the training data. So, as a result, not all object specifications are equal in creating quality artifacts. Although we have tested the models and methods on different kinds of compositions, based on our limited usage we cannot claim that model will equally work well for all domains. For example, we find that it works very well on closeup faces of human beings but the faces may get a bit distorted when we generate a full-length picture of a person or a group of people.

\subsection{Societal impact} 
\label{sec:societal_impact}
Recent rapid advancements in generative models have been so stunning that they have left many people in society (and in particular, the artists)  both worried and excited at the same time. On one hand, these tools, especially when they are getting increasingly democratized and accessible, give artists an enabling tool to create powerful work in lesser time. On the other hand, traditional artists are concerned about losing the business critical for their livelihood to amateurs \cite{artist_impact_wired}. Also, since these models pick off artistic styles easily from a few examples, the affected artists, who take years to build their portfolio and style, might feel shortchanged. Also, there is a concern that AI art maybe be treated at the same level and hence compete with traditional art. 

We feel that generative AI technology is as disruptive as photography was to realistic paintings. Our work, in particular, is based on Generative Models that can add to the consequences. However, since our motivation is to help artists improve their workflow and create images that self-express them, this modality of art may also have a very positive impact on their art and art processes. With confidence tempered with caution, we believe that it should be a welcome addition to an artist's toolkit. 


% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed