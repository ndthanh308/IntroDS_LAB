
\section{Composite Diffusion through serial inpainting} 
\label{sec:serial_inpainting}

Inpainting is the means of filling in missing portions or restoring the damaged parts of an image. It has been traditionally used to restore damaged photographs and paintings and (or) to edit and replace certain parts or objects in digital images\cite{bertalmio2000image}.  Diffusion models have been quite effective in inpainting tasks. A portion of the image, that needs to be edited, is marked out with the help of a mask, and then the content of the masked portion is generated through a diffusion model - in the context of the rest of the image, and sometimes with the additional help of a text prompt \cite{repaint, avrahami2022blended, latent_blended}. 

An obvious question is: Can we serially  (or repeatedly ) apply inpainting to achieve Composite Diffusion?  In the following section, we develop our implementation for serial inpainting and discuss issues that arise with respect to Composite Diffusion achieved through these means. The implementation also serves as the baseline for comparing our main Composite Diffusion algorithms (Algo. \ref{alg:scaff} and Algo. \ref{alg:harmon}). 

% described in the main paper.

% Figure environment removed

\subsection{Serial Inpainting - algorithm and implementation}
\label{sec:impl_serial_inpainting}

The method essentially involves successive application of the in-painting method for each segment of the layout. We start with an initial background image ($I_{bg}$) and repeatedly apply the in-painting process to generate segments specified in the free-form segment layout and text descriptions (refer to Algo. \ref{alg:serial-inpaint} for details). The method is further explained in Fig. \ref{fig:serial-inpaint} with the help of the running example. 

\begin{algorithm}
\caption{Serial Inpainting for composite creation }
\label{alg:serial-inpaint}

\textbf{Input:} Set of segment masks $m^i \in M$, set of segment descriptions $d^i \in D$, background image $I_{bg}$,
initial image, $x_T \sim \mathcal{N}(0, \textbf{I})$

\textbf{Output:}  An output image, $x_{comp}$, which is sufficiently grounded to  the inputs of segment layout and segment descriptions. \\

$z_T \leftarrow \mathcal{E}(x_T)$, \tcp*{Encode into latent space} 
$\forall i, m^i_z \leftarrow Downsample(m^i)$ \tcp*{Downsample all masks to latent space}
$\forall i, d^i_z \leftarrow \mathcal{C}_{CLIP}(d^i)$ \tcp*{Generate CLIP encoding for all text descriptions}


\For{\textbf{all segments} $i$  \textbf{from} $1$ to $n$}  
    {
    
    $z_{bg}^{masked} \leftarrow \mathcal{E}(I_{bg} \odot (1-m^i))$ \tcp*{Encode masked background image}
    $z_{bg} \leftarrow Inpaint(z_T, z_{bg}^{{masked}}, m^i_z,d^i_z)$\tcp*{Inpaint the segment}  
    $I_{bg} \leftarrow \mathcal{D}  (z_{bg})$ \tcp*{Decode the latent to get the new reference image}
    } 
    
\Return  $x^{comp} \leftarrow I_{bg}$ \tcp*{Final composite}
\end{algorithm}


We base our implementation upon the specialized in-painting method developed by RunwayML for Stable Diffusion \cite{stable_diffusion}. This in-painting method extends the U-net architecture described in the previous section to include additional input of a masked image.  It has 5 additional input channels (4 for the encoded masked image and 1 for the mask itself) and a checkpoint model which is fine-tuned for in-painting. 

% Figure environment removed

\subsection{Issues in Composite Diffusion via serial inpainting}
The method is capable of building good composite images. However, there are a few issues. One of the main issues with the serial inpainting methods for Composite Diffusion is the \textit{dependence on an initial background image}. Since this method is based on inpainting, the segment formation cannot start from scratch. So a suitable background image has to be either picked from a collection or generated anew. If we generate it anew, there is no guarantee that the segments will get the proper context for development. This calls for a careful selection from multiple generations. Also because a new segment will be generated in the context of the underlying image, this sometimes leads to undesirable consequences. Further, if any noise artifacts or other technical aberrations get introduced in the early part of the generation, their effect might get amplified in the repeated inpainting process. Some other issues might arise because of a specific inpainting implementation. For example, in the method of inpainting that we used (RunwayML Inpainting 1.5), the mask text inputs were occasionally  missed and sometimes the content of the segments gets duplicated. Refer to Fig. \ref{fig:issues-inpainting} for visual examples of some of these issues.
 
All these issues motivated the need to develop our methods, as described in the main paper, to support Composite Diffusion. We compare our algorithms against these two baselines of (i) basic text-to-image algorithms, and (ii) serial inpainting algorithms. The results of these comparisons are presented in the main paper with some more details available in the later sections of this Appendix.
