\section{Background for methods}
\label{sec:background}
In this section, we provide an overview of diffusion-based generative models and diffusion guidance mechanisms that serve as the foundational blocks of the methods in this paper. The reader is referred to \cite{dieleman2022guidance, ho2020denoising, weng2021diffusion} for any further details and mathematical derivations. 

\subsection{Diffusion models(DM)} In the context of image generation, DMs are a type of generative model with two diffusion processes: (i) a \textit{ forward diffusion process}, where we define a Markov chain by gradually adding a small amount of random noise to the image at each time step, and (ii)a \textit{reverse diffusion process}, where the model learns to generate the desired image, starting from a random noise sample. 

\subsubsection{Forward diffusion process}
\label{sec:forward_diff}
Given a real distribution $q(\mathbf{x})$, we sample an image $\mathbf{x}_0$ from it  ($\mathbf{x}_0 \sim q(\mathbf{x})$). We gradually add Gaussian noise to it with a variance schedule $\{\beta_t \in (0, 1)\}_{t=1}^T$ over $T$ steps to get progressively noisier versions of the image $\mathbf{x}_1, \dots, \mathbf{x}_T$. The conditional distribution at each time step $t$ with respect to its previous timestep $t-1$ is given by the diffusion kernel:

\begin{equation}
q(\mathbf{x}_{1:T} ) = q(\mathbf{x}_0)\prod^T_{t=1} q(\mathbf{x}_t \vert \mathbf{x}_{t-1}) 
\end{equation}

The features in $\mathbf{x}_0$ are gradually lost as step $t$ becomes larger. When $T$ is sufficiently large, $T \to \infty$, then $\mathbf{x}_T$ approximates an isotropic Gaussian distribution. 

\textbf{Q-sampling:} An interesting property of the forward diffusion process is that we can also sample $\mathbf{x}_t$  directly from $\mathbf{x}_0$ in the closed form. If we let  $\alpha_t =  1 - \beta_t$, $\bar{\alpha_t} = \prod^t_{s=1} \alpha_s$, we get:

\begin{equation}
q(\mathbf{x}_t \vert \mathbf{x}_0) = \mathcal{N}(\mathbf{x}_t; \sqrt{\bar{\alpha}_t} \mathbf{x}_0, (1 - \bar{\alpha}_t)\mathbf{I}) 
\end{equation}

Further, for $\bm{\epsilon} \sim \mathcal{N} (\mathbf{0}, \mathbf{I})$, $\mathbf{x}_t$ can be expressed as a linear combination of $\mathbf{x}_0$ and $\bm{\epsilon}$:

\begin{equation}
\mathbf{x}_t = \sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t}{\bm{\epsilon}}
\end{equation}

We utilize this property in many of our algorithms and refer to it as: \textit{`q-sampling'}.

\subsubsection{Reverse diffusion process}
 Here we reverse the Markovian process and, instead, we sample from $q(\mathbf{x}_{t-1} \vert \mathbf{x}_t)$.  By repeating this process, we should be able to recreate the true sample (image), starting from the pure noise $\mathbf{x}_T \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$. If $\beta_t$ is sufficiently small, $q(\mathbf{x}_{t-1} \vert \mathbf{x}_t)$ too will be an isotropic Gaussian distribution. However, it is not straightforward to estimate $q(\mathbf{x}_{t-1} \vert \mathbf{x}_t)$ in closed form. We, therefore, train a model $p_\theta$ to approximate the conditional probabilities that are required to run the reverse diffusion process. 

\begin{equation}
    p_\theta(\mathbf{x}_{t-1} \vert \mathbf{x}_t) = \mathcal{N}(\mathbf{x}_{t-1}; \bm{\mu}_\theta(\mathbf{x}_t, t), \bm{\Sigma}_\theta(\mathbf{x}_t, t))
\end{equation}
where $\bm{\mu}_\theta$ and $\bm{\Sigma}_\theta$ are the predicted mean and variance of the conditional Gaussian distribution. In the earlier  implementations $\bm{\Sigma}_\theta (x_t, t)$ was kept constant \cite{ho2020denoising}, but later it was shown that it is preferable to learn it through a neural network that interpolates between the upper and lower bounds for the fixed covariance \cite{diffusion_models_dhariwal}.

The reverse distribution is:
\begin{equation}
    p_\theta(\mathbf{x}_{0:T}) = p(\mathbf{x}_T) \prod^T_{t=1} p_\theta(\mathbf{x}_{t-1} \vert \mathbf{x}_t)
\end{equation}
Instead of directly inferring the image through $\bm{\mu}_\theta(x_t, t)$), it might be more convenient to predict the noise ($\bm{\epsilon}_\theta(x_t, t)$) added to the initial noisy sample ( $\mathbf{x}_t$)  to obtain  the denoised sample ($\mathbf{x}_{t-1}$) \cite{ho2020denoising}. Then, $\bm{\mu}_\theta(\mathbf{x}_t, t)$ can be derived as follows:

\begin{equation}
\bm{\mu}_\theta(x_t, t)
=  \frac{1}{\sqrt{\alpha_t}} \Big( \mathbf{x}_t - \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}} \bm{\epsilon}_\theta(\mathbf{x}_t, t) \Big)
\end{equation}

\textbf{Sampling:} Mostly, a U-Net neural architecture \cite{unet} is used to predict the denoising amount at each step. A scheduler samples the output from this model. Together with the knowledge of time step $t$, and the input noisy sample $\mathbf{x}_t$, it generates a denoised sample $\mathbf{x}_t$. For sampling through Denoising Diffusion Probabilistic Model (DDPM) \cite{ho2020denoising}, denoised sample is obtained through the following computation:

\begin{equation}
\mathbf{x}_{t-1} =  \frac{1}{\sqrt{\alpha_t}} \Big( \mathbf{x}_t - \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}} \bm{\epsilon}_\theta(\mathbf{x}_t, t) \Big) + \sigma_t \bm{\epsilon}
\end{equation}

where $\bm{\Sigma}_\theta(\mathbf{x}_t, t) = \sigma^2_{t} \mathbf{I}$ , and  $\bm{\epsilon} \sim \mathcal{N} (\mathbf{0}, \mathbf{I}) $ is a random sample from the standard Gaussian distribution. 

To achieve optimal results for image quality and speed-ups, besides DDPM, various sampling methods, such as DDIM, LDMS, PNDM, and LMSD \cite{samplers_sd_andrew, samplers_sd_agata} can be employed.

We use DDIM (Denoising Diffusion Implicit Models) as the common method of sampling for all the algorithms discussed in this paper. Using DDIM, we sample $\mathbf{x}_{t-1}$ from $\mathbf{x}_t$ and $\mathbf{x}_0$  via the following equation \cite{song2020denoising}:

\begin{equation}
\label{eq:ddim_sample}
    \mathbf{x}_{t-1} = \sqrt{\bar{\alpha}_{t-1}} \mathbf{x}_0 + \sqrt{1-\bar{\alpha}_{t-1} -\sigma^2_t}  \bm{\epsilon}_\theta(\mathbf{x}_t, t) + \sigma_t \bm{\epsilon}
\end{equation}

Using DDIM sampling, we can produce samples that are comparable to DDPM samples in image quality, while using only a small subset of DDPM timesteps (e.g., $50$ as opposed to $1000$).

\subsubsection{Latent diffusion models(LDM)} 
We can further increase the efficiency of the generative process by running the diffusion process in latent space that is lower-dimensional than but perceptually equivalent to pixel space. Performing diffusion in lower dimensional space provides massive advantages in terms of reduced computational complexity. For this, we first downsample the images into a lower-dimensional latent space and then upsample the results from the diffusion process into the pixel space. For example, the latent diffusion model described in  \cite{stable_diffusion} uses a suitably trained variational autoencoder to encode an RGB pixel-space image ($\mathbf{x} \in \mathbb{R}^{H \times W \times 3}$) into a latent-space representation ($\mathbf{\mathbf{z}} = \mathcal{E}(\mathbf{x})$, $\mathbf{z}\in \mathbb{R}^{h\times w \times c}$  ), where $f=H/h=W/w$ describes the downsampling factor. The diffusion model in the latent space operates similarly to the pixel-space diffusion model described in the previous sections, except that it utilizes a latent space time-conditioned U-Net architecture. The output of the diffusion process ($\tilde {\mathbf{z}}$) is decoded back to the pixel-space ($\tilde {\mathbf{x}} = \mathcal{D}(\tilde{\mathbf{z}})$). 

\subsection{Diffusion guidance}
An unconditional diffusion model, with mean $\mu_{\theta}(x_t)$ and variance $\Sigma_\theta(x_t)$  usually predicts a score function $\nabla_{x_t} \mathrm{log}\, p(x_t)$ which additively perturbs it and pushes it in the direction of the gradient. In conditional models, we try to model conditional distribution $\nabla_{x_t}\mathrm{log}\, p(x_t|y)$, where $y$ can be any conditional input such as class label and free-text. This term, however, can be derived to be a combination of unconditional and conditional terms\cite{dieleman2022guidance}: 
$$\nabla_{x_t}\mathrm{log}\, p(x_t|y) = \nabla_{x_t}\mathrm{log}\, p(x_t) + \nabla_{x_t} \mathrm{log}\, p(y|x_t) $$

\subsubsection{Classifier driven guidance}
We can obtain $\mathrm{log}\, p(y|x_t)$ from an external classifier that can predict a target $y$ from a high-dimension input like an image $x$. A guidance scale $s$ can further amplify the conditioning guidance.
$$\nabla_{x_t}\mathrm{log}\, p_s(x_t|y) = \nabla_{x_t}\mathrm{log}\, p(x_t) + s.\nabla_{x_t} \mathrm{log}\, p(y|x_t) $$
$s$ affects the quality and diversity of samples.
\subsubsection{CLIP driven guidance}
Contrastive Languageâ€“Image Pre-training (CLIP) is a neural network that can learn visual concepts from natural language supervision \cite{clip}.  %CLIP model consists of a text and image encoder which are pre-trained together for the task of image-text matching. 
The pre-trained encoders from the CLIP model can be used to obtain semantic image and text embeddings which can be used to score how closely an image and a text prompt are semantically related. 

Similar to a classifier, we can use the gradient of the dot product of the image and caption encodings ( $f(x_t)$  and $g(c)$) with respect to the image to guide the diffusion process  \cite{clipglass, glide, style_clip}.

$$\hat{\mu}_\theta(x_t|c) = \mu_\theta(x_t|c) + s \cdot \Sigma_\theta(x_t|c)\nabla_{x_t} (f(x_t) \cdot g(c))$$

To perform a simple classifier-guided diffusion, Dhariwal and Nichol\cite{diffusion_models_dhariwal} use a classifier that is pre-trained on noisy images to guide the image generation. However, training a CLIP model from scratch on noisy images may not be always feasible or practical. To mitigate this problem we can estimate a clean image $ \hat{x_0}$ from a noisy latent $x_t$ by using the following equation.  

\begin{equation}
\hat{x_0} =  \frac{{x}_t}{\sqrt{\bar{\alpha}_t}} - \frac{\sqrt{1 - \bar{\alpha}_t}\epsilon_\theta(x_t, t)}{\sqrt{ \bar{\alpha}_t}}    
\end{equation}

We can then use this projected clean image $\hat{x}_0$ at each state of diffusion step $t$ for comparing with the target text.
Now, a CLIP-based loss $L_{CLIP}$ may be defined as the cosine distance (or some similar distance measure) between the CLIP embedding of the text prompt ($d$) and the embedding of the estimated clean image $\hat{x}_0$:
 $${L}_{CLIP}(x, d) = D_c(CLIP_{img}(\hat{x}_0), CLIP_{txt}(d)) $$

\subsubsection{Classifier-free guidance}
\label{sec:background_cfg}
Classifier-guided mechanisms face a few challenges, such as: (i) may not be robust enough in dealing with noised samples in the diffusion process,(ii) not all the information in $x$ is relevant for predicting $y$, which may cause adversarial guidance, (iii) do not work well for predicting complex $y$ like `text'.  The classifier-free guidance \cite{ho2022classifier} helps overcome this and also utilizes the knowledge gained by a pure generative model. A conditional generative model is trained to act as both conditional and unconditional (by dropping out the conditional signal by 10-20\% during the training phase). The above equation (section 3.3.1) can be reinterpreted as \cite{dieleman2022guidance, glide}:

\begin{multline}
\nabla_{x_t}\mathrm{log}\, p_s(x_t|y) = \nabla_{x_t}\mathrm{log}\, p(x_t)  \\ + s.(\nabla_{x_t} \mathrm{log}\, p(x_t| y) - \nabla_{x_t}\mathrm{log}\, p(x_t)) 
\end{multline}

For $s=0$, we get an unconditional model, for $s=1$, we get a conditional model, and for $s>1$ we strengthen the conditioning signal. The above equation can be expressed in terms of noise estimates at diffusion timestep $t$, as follows:

\begin{equation}
\label{eq:cond-infer}
    \hat{\epsilon}_{\theta}(x_t|c) = \epsilon_{\theta}(x_t|\emptyset) + s \cdot (\epsilon_{\theta}(x_t|c) - \epsilon_{\theta}(x_t|\emptyset))
\end{equation}

where $c$ is the text caption representing the conditional input, and $\emptyset$ is an empty sequence or a null set representing unconditional output. Our DDIM sampling for conditioned models will utilize these estimates.
