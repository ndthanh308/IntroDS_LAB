\section{Conclusion}
\label{sec:conclusion}
In this paper, we introduced composite generation as a method for generating an image by composing from its constituent \textit{sub-scenes}. The method vastly enhances the capabilities of text-to-image models. It enables a new mode for the artists to create their art by specifying (i) \textit{spatial intent} using free-form sub-scene layout, and (ii)  \textit{content intent} for the sub-scenes using natural language descriptions and different forms of control conditions such as scribbles, line art, and human pose. 

To provide artists with better affordances, we propose that the spatial layout should be viewed as a coarse-grained layout for \textit{sub-scenes} rather than an outline of individual fine-grained objects. For a finer level of control within a sub-scene, it is best to apply sub-scene-specific control conditions. We strongly feel that this arrangement is intuitive for artists and easy to use for novices.  

We implemented composite generation in the context of diffusion models and called it \textit{Composite Diffusion}. We showed that the model generates quality images while adhering to the spatial and semantic constraints imposed by the input modalities of free-form segments, natural text, and other control conditions. Our methods do not require any retraining of models or change in  the core architecture of the pre-trained models. Further, they work seamlessly with any fine-tuning of the base generative model.

We recommend modularizing the process of composite diffusion into two stages: scaffolding and harmonizing. With this separation of concerns, researchers can independently develop and improve the respective stages in the future. We observe that diffusion processes are inherently harmonizing in nature and we can achieve a more natural blending and harmonization of an image by exploiting this property than through other external means. 

We also highlighted the need for better \textit{quality criteria} for generative image generations. We devised one such \textit{quality criteria} suitable for evaluating the results of Composite Diffusion in this paper.  To evaluate using these criteria, we conducted both human evaluations and automated evaluations. Although the automated evaluation methods for Composite Diffusion are limited in their scope and are in an early stage of development,  we nevertheless found an interesting positive correlation between human evaluations and automated evaluations. 

We make an essential observation about benchmarking: The strength of the base model heavily influences the quality of generated composite. Base model strength, in turn, depends upon the parameter strength, architecture, and quality of the training data of the base model. Hence, any evaluation of the quality of generated composite images should be in relation to the base model image quality. For example, in this paper, we use Stable Diffusion v1.5 as the standard base for all types of generations, viz., text-to-image, repeated inpainting, and composite diffusion generations.

Finally, we demonstrated that our approach achieves greater spatial, semantic, and creative control in comparison to the baselines and other approaches. This gives us confidence that with careful application, the holistic quality of an image generated through Composite Diffusion would indeed be greater than or equal to ($>=$) the sum of the quality of its constituent parts. 

\subsection{Future work}
We discuss some of the interesting future research problems and possibilities related to this work. 

We implemented Composite Diffusion in the context of Stable Diffusion \cite{stable_diffusion}. It would instructive to explore the application of Composite Diffusion in the context of different architectures like Dalle-E \cite{dalle2},  Imagen \cite{imagegen},  or other open sources models such as Deep Flyod \cite{deep_floyd}. Since the definition of Composite Generation (with input modality as defined in this paper) is generic, it can also be applied to other generative models, such as GANs or any future visual generative models. 

In this work, we have experimented with only two sampling methods - DDPM  \cite{ho2020denoising} and  DDIM \cite{song2020denoising}; all the generations in this paper use DDIM. It would be interesting to study the impact of different sampling methods, such as Euler, DPM, LMS, etc. \cite{samplers_sd_andrew, samplers_sd_agata},  on the Composite Diffusion. 

For evaluation purposes, we faced the challenge of a relevant dataset for Composite Diffusion. There are no ready data sets that provide \textit{ free-form sub-scene layout} along with the \textit{natural-language descriptions} of those sub-scenes. We handcrafted a 100-image data set of sub-scene layouts and associated sub-scene captions. The input dataset and the associated generated images helped us benchmark and evaluate different composite generation methods.   By doing multiple generations for each set of inputs, we can effectively enhance the size of the data set for evaluation. We strongly feel that this dataset should be augmented further for size and diversity - through community help or automatic means. A larger data set, curated on the above lines, will be extremely useful for benchmarking and future work.
