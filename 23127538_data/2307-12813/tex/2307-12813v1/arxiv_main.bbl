\begin{thebibliography}{10}

\bibitem{arbelle2021detector}
A.~Arbelle, S.~Doveh, A.~Alfassy, J.~Shtok, G.~Lev, E.~Schwartz, H.~Kuehne,
  H.~B. Levi, P.~Sattigeri, R.~Panda, et~al.
\newblock Detector-free weakly supervised grounding by separation.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 1801--1812, 2021.

\bibitem{cai2022bigdetection}
L.~Cai, Z.~Zhang, Y.~Zhu, L.~Zhang, M.~Li, and X.~Xue.
\newblock Bigdetection: A large-scale benchmark for improved object detector
  pre-training.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 4777--4787, 2022.

\bibitem{cao2022correspondence}
M.~Cao, J.~Jiang, L.~Chen, and Y.~Zou.
\newblock Correspondence matters for video referring expression comprehension.
\newblock In {\em Proceedings of the 30th ACM International Conference on
  Multimedia}, pages 4967--4976, 2022.

\bibitem{carion2020end}
N.~Carion, F.~Massa, G.~Synnaeve, N.~Usunier, A.~Kirillov, and S.~Zagoruyko.
\newblock End-to-end object detection with transformers.
\newblock In {\em Computer Vision--ECCV 2020: 16th European Conference,
  Glasgow, UK, August 23--28, 2020, Proceedings, Part I 16}, pages 213--229.
  Springer, 2020.

\bibitem{chen2020uniter}
Y.-C. Chen, L.~Li, L.~Yu, A.~El~Kholy, F.~Ahmed, Z.~Gan, Y.~Cheng, and J.~Liu.
\newblock Uniter: Universal image-text representation learning.
\newblock In {\em Computer Vision--ECCV 2020: 16th European Conference,
  Glasgow, UK, August 23--28, 2020, Proceedings, Part XXX}, pages 104--120.
  Springer, 2020.

\bibitem{deng2021transvg}
J.~Deng, Z.~Yang, T.~Chen, W.~Zhou, and H.~Li.
\newblock Transvg: End-to-end visual grounding with transformers.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 1769--1779, 2021.

\bibitem{dosovitskiy2020image}
A.~Dosovitskiy, L.~Beyer, A.~Kolesnikov, D.~Weissenborn, X.~Zhai,
  T.~Unterthiner, M.~Dehghani, M.~Minderer, G.~Heigold, S.~Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock {\em arXiv preprint arXiv:2010.11929}, 2020.

\bibitem{du2022visual}
Y.~Du, Z.~Fu, Q.~Liu, and Y.~Wang.
\newblock Visual grounding with transformers.
\newblock In {\em 2022 IEEE International Conference on Multimedia and Expo
  (ICME)}, pages 1--6. IEEE, 2022.

\bibitem{Everingham2010}
M.~Everingham, L.~Van~Gool, C.~K. Williams, J.~Winn, and A.~Zisserman.
\newblock The pascal visual object classes (voc) challenge.
\newblock {\em International journal of computer vision}, 2010.

\bibitem{ghiasi2022scaling}
G.~Ghiasi, X.~Gu, Y.~Cui, and T.-Y. Lin.
\newblock Scaling open-vocabulary image segmentation with image-level labels.
\newblock In {\em Computer Vision--ECCV 2022: 17th European Conference, Tel
  Aviv, Israel, October 23--27, 2022, Proceedings, Part XXXVI}, pages 540--557.
  Springer, 2022.

\bibitem{gu2021open}
X.~Gu, T.-Y. Lin, W.~Kuo, and Y.~Cui.
\newblock Open-vocabulary object detection via vision and language knowledge
  distillation.
\newblock {\em arXiv preprint arXiv:2104.13921}, 2021.

\bibitem{gupta2019lvis}
A.~Gupta, P.~Dollar, and R.~Girshick.
\newblock Lvis: A dataset for large vocabulary instance segmentation.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and
  pattern recognition}, pages 5356--5364, 2019.

\bibitem{hu2017modeling}
R.~Hu, M.~Rohrbach, J.~Andreas, T.~Darrell, and K.~Saenko.
\newblock Modeling relationships in referential expressions with compositional
  modular networks.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 1115--1124, 2017.

\bibitem{kamath2021mdetr}
A.~Kamath, M.~Singh, Y.~LeCun, G.~Synnaeve, I.~Misra, and N.~Carion.
\newblock Mdetr-modulated detection for end-to-end multi-modal understanding.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 1780--1790, 2021.

\bibitem{kazemzadeh2014referitgame}
S.~Kazemzadeh, V.~Ordonez, M.~Matten, and T.~Berg.
\newblock Referitgame: Referring to objects in photographs of natural scenes.
\newblock In {\em EMNLP}, pages 787--798, 2014.

\bibitem{Krasin2017}
I.~Krasin, T.~Lin, T.~Duerig, P.~Kr\"{a}henb\"{u}hl, A.~Gupta, C.~Burgess, and
  V.~Ferrari.
\newblock {OpenImages}: A public dataset for large-scale multi-label and
  multi-class image classification.
\newblock {\em Dataset available from https://github.com/openimages}, 2017.

\bibitem{krishna2017visualgenome}
R.~Krishna, Y.~Zhu, O.~Groth, J.~Johnson, K.~Hata, J.~Kravitz, S.~Chen,
  Y.~Kalantidis, L.-J. Li, D.~A. Shamma, et~al.
\newblock Visual {G}enome: Connecting language and vision using crowdsourced
  dense image annotations.
\newblock {\em IJCV}, 123:32--73, 2017.

\bibitem{li2022elevater}
C.~Li, H.~Liu, L.~Li, P.~Zhang, J.~Aneja, J.~Yang, P.~Jin, H.~Hu, Z.~Liu, Y.~J.
  Lee, et~al.
\newblock Elevater: A benchmark and toolkit for evaluating language-augmented
  visual models.
\newblock {\em Advances in Neural Information Processing Systems},
  35:9287--9301, 2022.

\bibitem{li2021bottom}
L.~Li, Y.~Bu, and Y.~Cai.
\newblock Bottom-up and bidirectional alignment for referring expression
  comprehension.
\newblock In {\em Proceedings of the 29th ACM International Conference on
  Multimedia}, pages 5167--5175, 2021.

\bibitem{li2022grounded}
L.~H. Li, P.~Zhang, H.~Zhang, J.~Yang, C.~Li, Y.~Zhong, L.~Wang, L.~Yuan,
  L.~Zhang, J.-N. Hwang, et~al.
\newblock Grounded language-image pre-training.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 10965--10975, 2022.

\bibitem{li2021referring}
M.~Li and L.~Sigal.
\newblock Referring transformer: A one-step approach to multi-task visual
  grounding.
\newblock {\em Advances in neural information processing systems},
  34:19652--19664, 2021.

\bibitem{li2022cross}
Q.~Li, Y.~Zhang, S.~Sun, J.~Wu, X.~Zhao, and M.~Tan.
\newblock Cross-modality synergy network for referring expression comprehension
  and segmentation.
\newblock {\em Neurocomputing}, 467:99--114, 2022.

\bibitem{Lin2014}
T.~Lin, M.~Maire, S.~J. Belongie, L.~D. Bourdev, R.~B. Girshick, J.~Hays,
  P.~Perona, D.~Ramanan, P.~Doll{\'{a}}r, and C.~L. Zitnick.
\newblock {Microsoft COCO:} {C}ommon {O}bjects in {C}ontext.
\newblock {\em arXiv preprint arXiv:1405.0312}, 2014.

\bibitem{lin2014microsoft}
T.-Y. Lin, M.~Maire, S.~Belongie, J.~Hays, P.~Perona, D.~Ramanan,
  P.~Doll{\'a}r, and C.~L. Zitnick.
\newblock Microsoft {COCO}: Common objects in context.
\newblock In {\em ECCV}, pages 740--755. Springer, 2014.

\bibitem{liu2023polyformer}
J.~Liu, H.~Ding, Z.~Cai, Y.~Zhang, R.~K. Satzoda, V.~Mahadevan, and
  R.~Manmatha.
\newblock Polyformer: Referring image segmentation as sequential polygon
  generation.
\newblock In {\em CVPR}, 2023.

\bibitem{liu2017referring}
J.~Liu, L.~Wang, and M.-H. Yang.
\newblock Referring expression generation and comprehension via attributes.
\newblock In {\em Proceedings of the IEEE International Conference on Computer
  Vision}, pages 4856--4864, 2017.

\bibitem{liu2023groundingdino}
S.~Liu, Z.~Zeng, T.~Ren, F.~Li, H.~Zhang, J.~Yang, C.~Li, J.~Yang, H.~Su,
  J.~Zhu, et~al.
\newblock Grounding dino: Marrying dino with grounded pre-training for open-set
  object detection.
\newblock {\em arXiv preprint arXiv:2303.05499}, 2023.

\bibitem{liu2021swin}
Z.~Liu, Y.~Lin, Y.~Cao, H.~Hu, Y.~Wei, Z.~Zhang, S.~Lin, and B.~Guo.
\newblock Swin transformer: Hierarchical vision transformer using shifted
  windows.
\newblock In {\em Proceedings of the IEEE/CVF international conference on
  computer vision}, pages 10012--10022, 2021.

\bibitem{liu2022convnet}
Z.~Liu, H.~Mao, C.-Y. Wu, C.~Feichtenhofer, T.~Darrell, and S.~Xie.
\newblock A convnet for the 2020s.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 11976--11986, 2022.

\bibitem{lu2022unified}
J.~Lu, C.~Clark, R.~Zellers, R.~Mottaghi, and A.~Kembhavi.
\newblock Unified-io: A unified model for vision, language, and multi-modal
  tasks.
\newblock {\em arXiv preprint arXiv:2206.08916}, 2022.

\bibitem{luo2020multi}
G.~Luo, Y.~Zhou, X.~Sun, L.~Cao, C.~Wu, C.~Deng, and R.~Ji.
\newblock Multi-task collaborative network for joint referring expression
  comprehension and segmentation.
\newblock In {\em Proceedings of the IEEE/CVF Conference on computer vision and
  pattern recognition}, pages 10034--10043, 2020.

\bibitem{mao2016generation}
J.~Mao, J.~Huang, A.~Toshev, O.~Camburu, A.~L. Yuille, and K.~Murphy.
\newblock Generation and comprehension of unambiguous object descriptions.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 11--20, 2016.

\bibitem{minderer2022simpleOWLViT}
M.~Minderer, A.~Gritsenko, A.~Stone, M.~Neumann, D.~Weissenborn,
  A.~Dosovitskiy, A.~Mahendran, A.~Arnab, M.~Dehghani, Z.~Shen, et~al.
\newblock Simple open-vocabulary object detection with vision transformers.
\newblock {\em arXiv preprint arXiv:2205.06230}, 2022.

\bibitem{ordonez2011im2text}
V.~Ordonez, G.~Kulkarni, and T.~Berg.
\newblock Im2text: Describing images using 1 million captioned photographs.
\newblock {\em Advances in neural information processing systems}, 24, 2011.

\bibitem{plummer2015flickr30k}
B.~A. Plummer, L.~Wang, C.~M. Cervantes, J.~C. Caicedo, J.~Hockenmaier, and
  S.~Lazebnik.
\newblock Flickr30k entities: Collecting region-to-phrase correspondences for
  richer image-to-sentence models.
\newblock In {\em Proceedings of the IEEE international conference on computer
  vision}, pages 2641--2649, 2015.

\bibitem{radford2021learning}
A.~Radford, J.~W. Kim, C.~Hallacy, A.~Ramesh, G.~Goh, S.~Agarwal, G.~Sastry,
  A.~Askell, P.~Mishkin, J.~Clark, et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In {\em International conference on machine learning}, pages
  8748--8763. PMLR, 2021.

\bibitem{shao2019objects365}
S.~Shao, Z.~Li, T.~Zhang, C.~Peng, G.~Yu, X.~Zhang, J.~Li, and J.~Sun.
\newblock Objects365: A large-scale, high-quality dataset for object detection.
\newblock In {\em Proceedings of the IEEE/CVF international conference on
  computer vision}, pages 8430--8439, 2019.

\bibitem{sharma2018conceptual}
P.~Sharma, N.~Ding, S.~Goodman, and R.~Soricut.
\newblock Conceptual captions: A cleaned, hypernymed, image alt-text dataset
  for automatic image captioning.
\newblock In {\em Proceedings of the 56th Annual Meeting of the Association for
  Computational Linguistics (Volume 1: Long Papers)}, pages 2556--2565, 2018.

\bibitem{song2021co}
S.~Song, X.~Lin, J.~Liu, Z.~Guo, and S.-F. Chang.
\newblock Co-grounding networks with semantic attention for referring
  expression comprehension in videos.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 1346--1355, 2021.

\bibitem{subramanian2022reclip}
S.~Subramanian, W.~Merrill, T.~Darrell, M.~Gardner, S.~Singh, and A.~Rohrbach.
\newblock Reclip: A strong zero-shot baseline for referring expression
  comprehension.
\newblock {\em arXiv preprint arXiv:2204.05991}, 2022.

\bibitem{sun2022proposal}
M.~Sun, W.~Suo, P.~Wang, Y.~Zhang, and Q.~Wu.
\newblock A proposal-free one-stage framework for referring expression
  comprehension and generation via dense cross-attention.
\newblock {\em IEEE Transactions on Multimedia}, 2022.

\bibitem{thomee2016yfcc100m}
B.~Thomee, D.~A. Shamma, G.~Friedland, B.~Elizalde, K.~Ni, D.~Poland, D.~Borth,
  and L.-J. Li.
\newblock Yfcc100m: The new data in multimedia research.
\newblock {\em Communications of the ACM}, 59(2):64--73, 2016.

\bibitem{wang2023v3det}
J.~Wang, P.~Zhang, T.~Chu, Y.~Cao, Y.~Zhou, T.~Wu, B.~Wang, C.~He, and D.~Lin.
\newblock V3det: Vast vocabulary visual detection dataset.
\newblock {\em arXiv preprint arXiv:2304.03752}, 2023.

\bibitem{wang2021improving}
L.~Wang, J.~Huang, Y.~Li, K.~Xu, Z.~Yang, and D.~Yu.
\newblock Improving weakly supervised visual grounding by contrastive knowledge
  distillation.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and
  pattern recognition}, pages 14090--14100, 2021.

\bibitem{wang2022ofa}
P.~Wang, A.~Yang, R.~Men, J.~Lin, S.~Bai, Z.~Li, J.~Ma, C.~Zhou, J.~Zhou, and
  H.~Yang.
\newblock Ofa: Unifying architectures, tasks, and modalities through a simple
  sequence-to-sequence learning framework.
\newblock In {\em International Conference on Machine Learning}, pages
  23318--23340. PMLR, 2022.

\bibitem{wang2022unifying}
P.~Wang, A.~Yang, R.~Men, J.~Lin, S.~Bai, Z.~Li, J.~Ma, C.~Zhou, J.~Zhou, and
  H.~Yang.
\newblock Unifying architectures, tasks, and modalities through a simple
  sequence-to-sequence learning framework.
\newblock {\em arXiv preprint arXiv:2202.03052}, 2022.

\bibitem{wu2020phrasecut}
C.~Wu, Z.~Lin, S.~Cohen, T.~Bui, and S.~Maji.
\newblock Phrasecut: Language-based image segmentation in the wild.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 10216--10225, 2020.

\bibitem{wu2023cora}
X.~Wu, F.~Zhu, R.~Zhao, and H.~Li.
\newblock Cora: Adapting clip for open-vocabulary detection with region
  prompting and anchor pre-matching.
\newblock {\em arXiv preprint arXiv:2303.13076}, 2023.

\bibitem{wu2023GRES}
Y.~Wu, Z.~Zhang, C.~Xie, F.~Zhu, and R.~Zhao.
\newblock Advancing referring expression segmentation beyond single image.
\newblock {\em arXiv preprint arxiv:2305.12452}, 2023.

\bibitem{yan2023universal}
B.~Yan, Y.~Jiang, J.~Wu, D.~Wang, P.~Luo, Z.~Yuan, and H.~Lu.
\newblock Universal instance perception as object discovery and retrieval.
\newblock {\em arXiv preprint arXiv:2303.06674}, 2023.

\bibitem{yao2022detclip}
L.~Yao, J.~Han, Y.~Wen, X.~Liang, D.~Xu, W.~Zhang, Z.~Li, C.~Xu, and H.~Xu.
\newblock Detclip: Dictionary-enriched visual-concept paralleled pre-training
  for open-world detection.
\newblock {\em arXiv preprint arXiv:2209.09407}, 2022.

\bibitem{yu2016modeling}
L.~Yu, P.~Poirson, S.~Yang, A.~C. Berg, and T.~L. Berg.
\newblock Modeling context in referring expressions.
\newblock In {\em Computer Vision--ECCV 2016: 14th European Conference,
  Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14},
  pages 69--85. Springer, 2016.

\bibitem{zang2022open}
Y.~Zang, W.~Li, K.~Zhou, C.~Huang, and C.~C. Loy.
\newblock Open-vocabulary detr with conditional matching.
\newblock In {\em Computer Vision--ECCV 2022: 17th European Conference, Tel
  Aviv, Israel, October 23--27, 2022, Proceedings, Part IX}, pages 106--122.
  Springer, 2022.

\bibitem{zareian2021open}
A.~Zareian, K.~D. Rosa, D.~H. Hu, and S.-F. Chang.
\newblock Open-vocabulary object detection using captions.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 14393--14402, 2021.

\bibitem{zhang2022dino}
H.~Zhang, F.~Li, S.~Liu, L.~Zhang, H.~Su, J.~Zhu, L.~M. Ni, and H.-Y. Shum.
\newblock Dino: Detr with improved denoising anchor boxes for end-to-end object
  detection.
\newblock {\em arXiv preprint arXiv:2203.03605}, 2022.

\bibitem{zhang2022glipv2}
H.~Zhang, P.~Zhang, X.~Hu, Y.-C. Chen, L.~Li, X.~Dai, L.~Wang, L.~Yuan, J.-N.
  Hwang, and J.~Gao.
\newblock Glipv2: Unifying localization and vision-language understanding.
\newblock {\em Advances in Neural Information Processing Systems},
  35:36067--36080, 2022.

\bibitem{zhong2022regionclip}
Y.~Zhong, J.~Yang, P.~Zhang, C.~Li, N.~Codella, L.~H. Li, L.~Zhou, X.~Dai,
  L.~Yuan, Y.~Li, et~al.
\newblock Regionclip: Region-based language-image pretraining.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 16793--16803, 2022.

\bibitem{zhou2022detecting}
X.~Zhou, R.~Girdhar, A.~Joulin, P.~Kr{\"a}henb{\"u}hl, and I.~Misra.
\newblock Detecting twenty-thousand classes using image-level supervision.
\newblock In {\em Computer Vision--ECCV 2022: 17th European Conference, Tel
  Aviv, Israel, October 23--27, 2022, Proceedings, Part IX}, pages 350--368.
  Springer, 2022.

\bibitem{zhou2021real}
Y.~Zhou, R.~Ji, G.~Luo, X.~Sun, J.~Su, X.~Ding, C.-W. Lin, and Q.~Tian.
\newblock A real-time global inference network for one-stage referring
  expression comprehension.
\newblock {\em IEEE Transactions on Neural Networks and Learning Systems},
  2021.

\bibitem{zhu2022seqtr}
C.~Zhu, Y.~Zhou, Y.~Shen, G.~Luo, X.~Pan, M.~Lin, C.~Chen, L.~Cao, X.~Sun, and
  R.~Ji.
\newblock Seqtr: A simple yet universal network for visual grounding.
\newblock In {\em Computer Vision--ECCV 2022: 17th European Conference, Tel
  Aviv, Israel, October 23--27, 2022, Proceedings, Part XXXV}, pages 598--615.
  Springer, 2022.

\end{thebibliography}
