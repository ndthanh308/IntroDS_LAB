\section{Dataset}
\label{sec:dataset}
\vspace{-5pt}
\subsection{Dataset highlight}

% \input{figures/examples}
\input{figures/highlight}

The proposed dataset is re-annotated on GRD~\cite{wu2023GRES}, a dataset for RES~\cite{zhu2022seqtr,liu2023polyformer}. As briefly introduced in \cref{sec:introduction}, it contains 3 major characteristics. In \cref{fig:highlight} we show some examples from previous datasets and \ddd{} to highlight them. Here we elaborate on them with a few other characteristics:

The first is \textit{complete annotation}.
For REC, the instances referred by one description are only annotated on a few images. For other images without the annotation of this description, it is unknown whether the corresponding instance exists or not.
That is to say, their annotations are not complete.
Contrarily, as shown by \cref{fig:highlight1}, in \ddd{}, the objects referred in all images by any description are annotated, so are the negative samples, like traditional object detection datasets.

The second is \textit{unrestricted language description}. As shown in \cref{fig:highlight2}, unlike (open vocabulary) object detection that retrieves objects with category names, we retrieve object with language expressions, which is rather flexible.
As is shown in \cref{fig:len_refs}, the lengths of descriptions in \ddd{} vary a lot. The shortest descriptions have 1 or 2 words, where the DOD task downgrades to OVD, while the longest may have 15 or more words, resulting in rather complex language expressions.

The third is \textit{absence expression}. Current datasets with language description, like RefCOCO series for REC, usually describe objects possessing certain features.
They usually focus on the ability to discover the existence of concepts, but neglecting their their absence.
Noticing the missing of ability to verify such ability, we also annotate objects lacking a certain attribute.
\cref{fig:highlight3} shows a example with presence description and another with absence description from \ddd{}.
Such absence description takes about one quater of the references in this dataset. This is a first for existing benchmarks.

The forth is \textit{instance-level annotation}, a charasteric not holded by GRD as it is intended for RES.

The fifth is \textit{one description can refer to multiple instances in a image}, as shown in \cref{fig:nbox_per_img_ref}. This is not true for REC datasets. If we regard the category names in OVD as references, than OD datasets does have this feature.

In summary, the proposed dataset differs from the REC dataset primarily in terms of characteristics 1st, 3rd, and 5th. In contrast, when compared to the OD datasets, the proposed dataset showcases disparities in the 2nd and 3rd characteristics, and when compared with GRD, in the 2nd, 3rd, 4th, and 5th characteristics.
We refer the readers for the \supp{} for more information about the characteristics of \ddd{} and more examples.

\input{figures/lengths_nums}

\subsection{Annotation process}

We take GRD as the source of images, together with its original annotations.
Originally, it is divided into many groups, each group with several given references, and positive and negative samples are only annotated inside each group.
We further annotate these images with the following 3 steps:

\noindent \textbf{Adding instance-level annotation.}
GRD is meant for RES so each reference corresponds to a semantic mask across one image. For the DOD task that requires recognition and localization of each instances, we annotate each instance referred by a description with a individual bounding box (and an instance mask).
This is the basic step to make this dataset suitable for localization tasks.

\noindent \textbf{Adding complete annotations.}
Beyond the intra-group annotation in GRD, we further annotate the positive and negative samples for each reference across the whole dataset, by leveraging CLIP~\cite{radford2021learning} to select from all descriptions the possible candidates for each image based on image-text similarity. With the dataset-wise complete annotation, the division of groups are not necessary for evaluation, but only a way to organize the references by scenarios.
With this annotation step, this dataset is suitable for detection task. Besides, the number of positive and negative samples are greatly increased.

\noindent \textbf{Adding annotations for absence expressions.}
We further designed many absence descriptions based on the scenarios in the dataset, besides the traditional presence expression in GRD. We annotate the instances in the images across the whole dataset with these absence expressions.
This step enhances the proposed benchmark's difficulty level while enabling evaluation of existing models' ability to comprehend the absences of concepts.

\subsection{Dataset statistics}

\noindent \textbf{GRD statistics.}
% {'nsent': 316, 'nanno': 18343, 'nimg': 10634, 'ngroup': 106, 'num_img_sent': 9238, 'num_anti_img_sent': 22454, 'num_anno_sent': 13734, 'num_anti_anno_sent': 40901, 'avg_sent_len': 5.550632911392405}
% {'nsent': 422, 'nanno': 18343, 'nimg': 10634, 'ngroup': 106, 'num_img_sent': 9238, 'num_anti_img_sent': 22454, 'num_anno_sent': 13734, 'num_anti_anno_sent': 40901, 'avg_sent_len': 5.890995260663507}
It has 10,578 images collected online, divided into 106 groups.
Each group has around 100 images and 3 expressions referring to segmentation masks in this group, resulting in 316 references, 9,323 positive image-text pairs and 22,201 negative pairs.
Note that it only annotates positive and negative samples inside each group, i.e., the annotation completeness is only \textbf{group-level}, so a reference will not be verified outside its group.
The expressions have an average length of 5.9 words.
We refer the reader to the original paper for specific statistics of GRD.

\noindent \textbf{Statistics of the proposed \ddd{}.}
The proposed dataset has 10,578 images, all from GRD. It has 422 well-designed expressions, including 316 expressions from GRD and 106 absence expressions we added (one for each scenario).
The instance-level annotation results in 18,514 boxes.

% intra-group:
% {'nsent': 316, 'nanno': 18514, 'nimg': 10578, 'ngroup': 106, 'num_img_sent': 9323, 'num_anti_img_sent': 22201, 'num_anno_sent': 13917, 'num_anti_anno_sent': 41231, 'avg_sent_len': 5.987341772151899}
% {'nsent': 422, 'nanno': 18514, 'nimg': 10578, 'ngroup': 106, 'num_img_sent': 13035, 'num_anti_img_sent': 29067, 'num_anno_sent': 20279, 'num_anti_anno_sent': 53383, 'avg_sent_len': 6.31042654028436}
% inter-group:
% {'nsent': 316, 'nanno': 18514, 'nimg': 10578, 'ngroup': 106, 'num_img_sent': 9323, 'num_anti_img_sent': 22201, 'num_anno_sent': 16480, 'num_anti_anno_sent': 5833944, 'avg_sent_len': 5.987341772151899}
% {'nsent': 422, 'nanno': 18514, 'nimg': 10578, 'ngroup': 106, 'num_img_sent': 13035, 'num_anti_img_sent': 29067, 'num_anno_sent': 24282, 'num_anti_anno_sent': 7788626, 'avg_sent_len': 6.31042654028436}
Due to the effort in \textit{complete annotation}, for a reference, each image in the dataset is annotated for possible positive and negative samples, i.e., the annotation completeness is \textbf{dataset-level}.
Thus, there are 24,282 positive object-text pairs and 7,788,626 negative pairs, orders of magnitude larger than GRD.
Among them, those with images and texts from the same scenario are probably more difficult, which includes 20,279 positive and 53,383 negative pairs.
The average length of expressions is 6.3 words, due to the relative longer absence expressions.
More statistics and examples are available in \supp{}.

% \input{figures/evaluation}

\subsection{Evaluation metrics}

Each description in the proposed dataset corresponds to a category. For all metrics, we perform the evaluation on each reference first.
We use FULL, PRES and ABS to denote a metric separately evaluated on all descriptions, presence descriptions only and absence descriptions only. If not noted explicitly, the FULL setting is adopted.
The evaluation metrics we proposed for the \ddd{} include:

\noindent \textbf{Intra-scenario mAP.}
For this metric, we perform evaluation on each image with only the descriptions from to the scenario the image belongs to. A predicted instance is considered positive if its description is the same with a GT instance and the IoU is above a certain threshold.
The final metric is the mean average precision (mAP) averaged on different IoU thresholds (0.5 $\sim$ 0.95), following previous object detection datasets like COCO.
This is used as the default metric in our experimental settings.

\noindent \textbf{Inter-scenario mAP.}
This metric is similar to the intra-scenario mAP described above, except that for each image, we detect the possible instances with all 422 reference. This is aligned with the common mAP in object detection datasets.

% \noindent \textbf{Recall.}
% In REC datasets like RefCOCO~\cite{yu2016modeling,mao2016generation}, each image will have one and only one prediction for each reference, and the standard metric is accuracy (which is precision and also recall). This is not suitable for DOD, which is essentially a detection task. We adopt the average recall metric in COCO API for some analyses, but it does not necessarily correspond to the effectiveness of a method.

% The evaluation process for this metrics are illustrated in \cref{fig:evaluation}.
