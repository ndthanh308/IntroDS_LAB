\section{Conclusion and Limitation}
\label{sec:conclusion}

% In this paper, we present a more flexible and realistic setting for object understanding, called Described Object Detection, which encompasses both object detection and referring expression comprehension tasks. For this, we propose a benchmark more challenging than existing benchmarks, with unrestricted language reference, complete annotation, and absence expressions. We evaluate several state-of-the-art methods from previous tasks on this dataset, and propose a new baseline outperforming them for future research.
% This work contributes to the development of object understanding and provides a new benchmark for evaluating models in this area.

In this paper, we bring the Described Object Detection (DOD) task to the foreground. 
For this task, we introduce a dataset called \ddd{}, which annotates described objects without omission and features flexible language expressions, whether long or short, complex or simple. 
Our evaluation of SOTA methods from REC or OVD on \ddd{} reveals challenges faced by REC, OVD, and bi-functional approaches.
Based on these observations, we propose a baseline that largely improves REC methods for DOD task.
We believe that these datasets and findings contribute to advancing the understanding and development of DOD methods, facilitating future research in this area.

% \noindent \textbf{Limitation and future work.}
% Future work can focus on further improving the performance of models on this benchmark by adapting their task formulation and reusing their training data, and exploring other related tasks in object understanding.

\noindent \textbf{Limitation and broader impact.}
This work establishes the research foundation for the DOD task. Compared to traditional detection algorithms, the DOD model has a lower customization threshold, enabling users to specify the detection target using language. This may lead to potential abuse, such as detecting people's privacy. Moreover, a significant amount of GPU computing was utilized in model development and algorithm evaluation, resulting in carbon emissions.
