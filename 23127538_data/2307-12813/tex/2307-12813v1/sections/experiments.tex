\section{Experimental Analyses}
\label{sec:experiments}

\subsection{Comparison of baselines on our metrics}
\label{subsec:experimental_comp}

\input{tables/comparison}

We make comparisons on the baselines introduced in \cref{sec:baselines}, mainly with the intra-scenario setting.
Unless explicitly noted, this is the default setting, instead of the more difficult inter-scenario.

\noindent \textbf{Existing SOTAs are not sufficient for DOD, and bi-functional ones are better than others.}
% \noindent \textbf{Intra-scenario evaluation.}
As shown in \cref{tab:comparison}, existing methods, though obtaining state-of-the-art performance on their original benchmarks, do not achieve very strong performance on this benchmark.
Among them, recent bi-functional methods~\cite{liu2023groundingdino,yan2023universal} are obviously better than others, and currently OVD methods are better than REC.
The inferiority of REC methods is likely due to its impractical setting described in \cref{sec:introduction}, i.e., predict one and only one instance for each reference. We will discuss more on this later.

\noindent \textbf{Rejecting irrelevant references are difficult, which REC are naturally incapable of.}
% \noindent \textbf{Inter-scenario evaluation.}
Different from intra-scenario evaluation, the inter-scenario setting evaluates all reference in the dataset for each image. As descriptions from other scenarios are likely not relevant to the images semantically, this requires more the ability to reject irrelevant references for a image. This is aligned with the evaluation in standard detection tasks.
From \cref{tab:comparison}, we can see that OFA, a REC method, achieves almost zero on this setting. This is caused by its predicting a region for every description, which brings a large number of FPs when the candidate references are a lot. This indicates how important it is to empower such REC methods with the ability to reject false positives.
We find that none of the verified methods achieve a good performance under inter-scenario setting, which implies that existing methods are far from being capable of DOD, and shows how challenging a benchmark \ddd{} is.

\noindent \textbf{The proposed baseline outperforms others.}
The proposed baseline is based on OFA, but our improvements lift up the performance by a significant amount. It outperforms all these existing methods on intra-scenario setting, and surpasses them with larger margin under inter-scenario. This may indicate the proposed baseline has a stronger ability to reject irrelevant references.
Still, the proposed method is far from perfect and can merely serve as a baseline for future works.

\input{figures/score_distribution}

\subsection{Further analysis}

\noindent \textbf{Absence descriptions are more difficult for most methods.}
% \noindent \textbf{Difference over presence expressions and absence expressions.}
As can be seen from \cref{tab:comparison}, the performance of baseline methods over PRES (presence descriptions) is generally better than that over ABS (absence descriptions).
This implies a possibility that current methods do not truly distinguish between presence/absence of attributes in a language description.
% \Todo{add fig.}

\noindent \textbf{REC methods fail to provide good confidence scores.}
% \noindent \textbf{Confidence score distribution.}
For the baselines evaluated above, we visualize the distribution of their scores on TPs and FPs (in regardless of categories) to see their ability to perform classification and provide good confidence scores.
As shown in \cref{fig:score_distribution}, confidence scores from OFA are not distinguishable between TP and FP. This is partially due to its seq2seq framework that do not produce confidence score directly, and partially due to the grounding formulation of REC, which only locates the image region most similar to the text description, without distinguishing between postive and negative samples.
With the task decomposition step to augment the ability of binary classification, the proposed baseline shows a TP score distribution and a FP distribution that are rather different from each other, which can provide relatively good classification results. As this step does not change the model framework nor the training datasets, this improvement is attributed to a more suitable taks formulation.

\input{tables/analysis_ref_num}

\noindent \textbf{Multi-instance detection are difficult for methods other than OVD.}
% \noindent \textbf{Images with multiple targets.}
For each image, the proposed dataset can have zero to multiple instances \textit{for one description}.
To see how current methods handle images with various number of instances, we perform evaluation under three different split settings:
\textbf{no-instance}, where for each description, the evaluation is only performed on images with no referred instance;
\textbf{one-instance}, where the evaluation is performed on those with one referred instance;
and \textbf{multi-instance}, performed on those with multiple referred instances.
From \cref{tab:analysis_box_num}, we find that OVD methods, though not competitive over the whole dataset or the images with few instances, outperform other methods when there are many instances referred by the description. For OWL-ViT, the performance does not decrease much when number of instances increases, which is very distinct from other methods.
Other methods, either REC or bi-functional, suffer greatly from the multi-instance situations.
We can draw a conclusion that OVD methods are stronger for multi-target detection, while REC and current bi-functional ones are less robust to such situations.

\noindent \textbf{REC or bi-functional methods lack the ability to reject negative instances.}
% \noindent \textbf{Ability to reject negative instances.}
In the \textbf{no-instance} column in \cref{tab:analysis_box_num}, we do not report mAP as there is no GT for the corresponding reference and AP is thus not applicable. Instead, predictions on such images are FPs and we use the ratio of images where FPs are produced versus the total number of no-instance images for one reference as the metric, namely False Postive Per Category (FPPC). We report the FPPC averaged over all reference.
We find that the majority of the baselines are unable to determine whether a image contains a referred target or not, and still produce predictions.
This is natural for REC methods. The bi-functional methods are trained and inferred with the formulation of REC task so they also suffer from this. 
Only the OVD method and the proposed baseline are able to reject such negative image-text pair.

\input{tables/analysis_ref_length}

\noindent \textbf{OVD methods suffer from long descriptions greatly while others do not.}
% \noindent \textbf{References with different lengths.}
We partition the references according to their lengths and then evaluate on these partitions. The results are shown in \cref{tab:analysis_ref_length}. Short, middle, long and very long corresponds to references with $1\sim3$, $4\sim6$, $7\sim9$, and more than 9 words. For short descriptions with at most 3 words, which is very close to OVD setting (with 1 or 2 words), OVD or bi-functional methods obtain similar performance. However, as the length of references increases, the performance of OVD methods decrease fast, while REC and joint resolution methods suffer less from this. We can see that OVD methods are sensitive to long references, as expected, while other two types do not.

% \input{tables/recall}

% \noindent \textbf{Compared with REC, OVD methods tend to make predictions only when it's certain.}
% % \noindent \textbf{Analysis on recalls.}
% The recalls of different methods are shown in \cref{tab:recall}. OVD methods are obviously bad at recall, which means that it tends to produce a result when it is quite certain. Grounding-DINO, though performs not as good as the proposed baseline in terms of mAPs, obtains the best recall. This indicates that it tends to produce more detection results.

% \Todo{do we need to consider inference strategy (one ref at a time vs all ref at a time? No.}

% \input{figures/qualitative}

% \noindent \textbf{Qualitative analysis.}
% We visualize the results of different baseline methods on the proposed benchmark. This is shown in \cref{fig:qualitative}. We can see that the proposed method handles such images better.

\input{tables/method_ablation}

\subsection{Ablation on the proposed baseline}

\noindent \textbf{Method components.}
In \cref{tab:method_ablation}, we perform ablation on the proposed improvements in our baseline, step-by-step from OFA to the final OFA-DOD, to see how these improvements affect the performance.
\textbf{Granularity decomposition} (GD) makes the method more suitable for localization task.
It disentangle tasks of global or local granularity by handling them with 2 separated branch, and improves the performance significantly.
\textbf{Reconstructed data} (RD) uniforms REC and OD data into the same form, and prepares multi-instance samples.
This results in a noisy but unified format for training on data from both task. It provides the multi-instance samples from detection data and the long description comprehension from REC data. It improves the performance significantly too.
\textbf{Task decomposition} (TD) is proposed to help rejecting false references.
It breaks down the DOD task into a REC step and a VQA step, the performance on the proposed benchmark is greatly improved.

\noindent \textbf{Training tasks.}
As OFA and the proposed OFA-DOD baseline are both multi-task multi-modal method, we also perform a drop-one-out ablation on the training data we used. This is shown in \cref{tab:ablation_tasks}.
\textbf{Detection} data provides samples for localization, especially multi-instance situation.
This part of data also helps the model, possibly because it provides more samples for the model to learn to localize objects.
\textbf{I2T} is important for generalization and zero-shot evaluation on \ddd{}.
I2T denotes image-to-text tasks like image captioning and visual question answering. It helps the generalization of the proposed baseline by image-to-text learning. We can see that it greatly affects the performance.
\textbf{MLM} is important for generalization on this dataset, but actually not.
Removing the MLM task has no significant effect on the performance. Therefore, we surmise that the generalization ability on \ddd{} comes from I2T tasks.
