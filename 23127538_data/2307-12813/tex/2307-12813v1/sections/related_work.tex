\section{Related Work}
\vspace{-5pt}
\label{sec:related_work}
\subsection{Relevant datasets and benchmarks}
\vspace{-5pt}
\noindent \textbf{Object detection datasets.}
% Object detection is a mature task in computer vision and many datasets have been proposed for this task.
% PASCAL VOC, MSCOCO, LVIS, Object365, OpenImage, ODinW, BigDetection, V3Det.
A variety of datasets have been proposed for object detection. Some have become standard benchmarks, like PASCAL VOC~\cite{Everingham2010} and MSCOCO~\cite{Lin2014}; some are more frequently used for pretraining, like Object365\cite{shao2019objects365}, OpenImages\cite{Krasin2017} and BigDetection\cite{cai2022bigdetection}. A few works focused on special settings, like LVIS~\cite{gupta2019lvis} for long-tailed detection and ODinW~\cite{li2022elevater} for zero-shot evaluation in the wild.
Recently, V3Det~\cite{wang2023v3det} is proposed to facilitate object detection with a extremely large vocabulary. These datasets are frequently used in OVD too.
% A variety of datasets have been proposed for object detection. A popular line of datasets are curated as benchmark for general-purpose, including PASCAL VOC\cite{Everingham2010}, MSCOCO\cite{Lin2014}, Object365\cite{shao2019objects365}, OpenImage\cite{Krasin2017}, and BigDetection\cite{cai2022bigdetection}.
% Another line of datasets are proposed for specialized scenes, including V3Det\cite{wang2023v3det}, LVIS\cite{Gupta2019} for long-tail data scenes;  ODinW\cite{li2022elevater} to evaluate model transferring scenes; Wider Face~\cite{yang2016wider}, Pedestrian Detection~\cite{dollar2011pedestrian}, and PTD~\cite{andriluka2008people} for human-centric scenes; and Cityscapes~\cite{cordts2016cityscapes}, KITTI~\cite{geiger2012we}, and Mapillary~\cite{neuhold2017mapillary} for driving scenes.
% , which can be summarized into three categories. 
% including PASCAL VOC, MSCOCO, LVIS, Object365, OpenImage, ODinW, BigDetection, and V3Det.
% (1) Benchmark Dataset: PASCAL VOC\cite{Everingham2010}, MSCOCO\cite{Lin2014}.
% (2) Large-Scale Dataset: Object365\cite{shao2019objects365}, OpenImage\cite{Krasin2017}, BigDetection\cite{cai2022bigdetection}.
% (3) Specialized Dataset: ODinW\cite{li2022elevater}, V3Det\cite{wang2023v3det}, LVIS\cite{Gupta2019}.
% These object detection datasets have evolved significantly, both addressing various challenges for specific cases and pushing the boundaries of algorithmic advancements.
% From benchmark datasets like PASCAL VOC and MS COCO to specialized datasets like KITTI and Visual Genome, each dataset contributes to the progress of object detection research by providing unique characteristics, diverse annotations, and real-world contexts for algorithm evaluation and development. 

\noindent \textbf{Referring expression comprehension datasets.}
% Referece expression comprehension is a multi-modal task.
% RefCOCO, RefCOCOg, RefCOCO+. RefClef. Visual Genome. PhraseCut.
% \Todo{notice that phraseCut refers to multiple instances per image.}
% Referring expression comprehension (REC) aims to localize an object in the given image described by a language expression.
Several datasets have been introduced to evaluate the performance of REC methods, including RefClef~\cite{kazemzadeh2014referitgame}, RefCOCO~\cite{yu2016modeling}, RefCOCO+~\cite{yu2016modeling}, RefCOCOg (G-Ref)~\cite{mao2016generation}, Visual Genome~\cite{krishna2017visualgenome}, and PhraseCut~\cite{wu2020phrasecut}.
RefClef, RefCOCO, and RefCOCO+ are collected interactively in ReferitGame~\cite{kazemzadeh2014referitgame}, thus the given expressions are more concise and less flowery. 
% A characteristic of RefCOCO+ is that location words are banned in its expressions, which also makes it more challenging. 
% Among them, RefCOCO+ bans location words in expressions, making it more challenging.
RefCOCOg is collected non-interactively, resulting in more complex expressions, often full sentences instead of phrases.
Visual Genome focuses on the annotations of visual relationships.
% One of the key strengths of Visual Genome lies in its extensive annotations of visual relationships, which enables researchers to investigate complex scene understanding, contextual reasoning, and the interplay between objects in an image.
% PhraseCut, collected on top of the Visual Genome, where each image is accompanied by a set of annotated phrases that describe the objects present in the scene. These phrases may include object attributes, relationships, or other linguistic descriptions that provide a more comprehensive understanding of the visual content.

\subsection{Current methods}

\noindent \textbf{Open-vocabulary object detection methods.}
% Traditional object detection methods.
% Open vocabulary detection.
% Models integrating other similar tasks: 
% RegionCLIP~\cite{zhong2022regionclip}: adopt the pretrained visual encoder in CLIP and finetune on image-text pairs in CC3M.
% Detic. use clip embeddings to encode category names for OVD.
% GLIP and GLIP v2: phrase localization on flick30k.
% Object detection is a computer vision task that allows to identify and locate objects in an image or video.
% Object detection models have been traditionally formulated for closed-vocabulary settings, which can be divided as ``one-stage”~\cite{} and
% ``two-stage”~\cite{} formulations. 
% However, this vanilla object detection is limited to the fixed object category defined in advance, which greatly limits the practicability of existing methods.
Open-vocabulary detection is gaining attention recently. It aims at detecting arbitrary classes with language for generalization while only trained on limited classes.
The first, OVR-CNN~\cite{zareian2021open}, introduces image-caption pairs to pretrain the visual encoder to improve its zero-shot generalization.
With~\cite{radford2021learning} proposed, Detic~\cite{zhou2022detecting}, DetCLIP~\cite{yao2022detclip}, RegionCLIP~\cite{zhong2022regionclip}, and OV-DETR~\cite{zang2022open} upgrade the image and language embedding pre-trained from CLIP.
ViLD~\cite{gu2021open} further distills knowledge from CLIP to inherit the semantics of language for recognizing novel class.
GLIP~\cite{li2022grounded,zhang2022glipv2} formulates object detection as a phrase grounding~\cite{plummer2015flickr30k} problem and leverages additional phrase grounding data to help vision-language alignment.

% It shows that such a formulation can even achieve stronger performance on fully-supervised detection benchmarks.

\noindent \textbf{Referring expression comprehension methods.}
REC is natively a vision-language task.
Existing works~\cite{deng2021transvg,sun2022proposal,subramanian2022reclip,li2022cross,zhou2021real,li2021bottom,song2021co,zhu2022seqtr,li2021referring,liu2023polyformer,lu2022unified,wang2022unifying} can be divided into three categories.
(1) Specialist models tailored for REC.
Previously, two-staged works~\cite{hu2017modeling,yu2016modeling} reformulate this as a ranking task to select the best-matching region from candidates regions. More recently, one-stage approaches~\cite{zhou2021real,subramanian2022reclip,sun2022proposal,arbelle2021detector} have been proposed to speedup the inference process.
(2) Multi-task models~\cite{zhu2022seqtr,li2021referring,liu2023polyformer}.
They usually design a unified formulation for a few closely related tasks.
For example, SeqTR~\cite{zhu2022seqtr} unifies REC and RES as a point prediction problem and obtains promising results.
% RefTR~\cite{li2021referring} proposes a simple one-stage multi-task framework for REC and RES, which is capable of simultaneous language grounding at both a bounding box and segmentation level.
% MCN~\cite{luo2020multi} novelly proposes Consistency Energy Maximization to enable REC and RES to focus on similar visual regions by maximizing the consistency energy between these two tasks.
% PolyFormer~\cite{liu2023polyformer} formulates REC and RES as a sequence-to-sequence prediction problem, which naturally fuses multi-modal features together as input and generate a sequence of polygon vertices and bounding box corner points.
(3) Multi-modal pre-training models~\cite{chen2020uniter,lu2022unified,wang2022ofa}.
% Recent progress in multi-modal understanding has been mainly powered by pre-training large transformer models to learn generic multi-modal representations from enormous amounts of aligned image-text data, then fine-tuning them on downstream tasks. 
% A prevalent paradigm is to extract visual and textual features independently and then use the attention mechanism of the transformers to learn an alignment between the two.
Unified-IO~\cite{lu2022unified} and OFA~\cite{wang2022unifying} proposes a unified sequence-to-sequence framework that can handle a variety of vision, language, and multi-modal tasks. Currently, OFA holds the SOTA among REC methods.

% These methods can be
% divided into single stream [6, 24, 65, 22] and two-stream
% [47, 28, 29, 46] architectures depending on whether the text
% and images are processed by a single combined transformer
% or two separate transformers followed by some cross attention layers. For both these types, the prevalent approach is
% to extract visual and textual features independently and then
% use the attention mechanism of the transformers to learn an
% alignment between the two


% from CNN-RNN
% model, modular network to complex graph-based model. 


% Specialist models.
% TransVG.
% RefTR. SeqTR. PolyFormer. REC + RES.
% Pretrained models.
% OFA. UNITER. Unified-IO. multi-task pretrain + single-task fine-tuning.


\noindent \textbf{Bi-functional models for REC and OVD.}
% Pretrain + Finetune.
% M-DETR. pretrain on detection data. finetune and eval on RefCOCO.
% Grounding-DINO. UNINEXT. train on multi-task data including detection and REC. direct evaluation on each task.
% \Todo{explain why these methods have not solve this problem.}
% OVD (or OD) and REC can be unified as instance perception tasks, which aims at finding certain objects specified by some queries such as category names, language expressions, and target annotations. Therefore, a lot of recent work attempts to combine OD and REC tasks, considering the instance perception tasks as ``text-conditioned object detection".
% Specifically, the OD task is used to pre-train a powerful object detector, and then the language modality is introduced to fine-tune a vision-language understanding specialized model (e.g. REC task). 
Some recent works try to handle tasks like OVD and REC together in a model. They usually reformulate the training on these tasks, so that one model can learn on datasets from both tasks. However, the inference for each task is still different and independent from each other.
MDETR~\cite{kamath2021mdetr} is derived from DETR~\cite{carion2020end} framework. It is pretrained on OD and image-text tasks, then fine-tuned and evaluated on REC.
More recently, Grounding-DINO~\cite{liu2023groundingdino} proposes to extend a closed-set detector DINO~\cite{zhang2022dino} by performing vision-language fusion at multiple phases and evaluated in REC datasets, while UNINEXT~\cite{yan2023universal} reformulates diverse image and video tasks into a unified object discovery and retrieval paradigm.
For these models, though knowledge from OVD and REC are shared through pretraining, they are still treated as two different tasks.
% The main challenge in this task is how to transfer the image-level representations of the
% image-text backbone to detection despite the scarcity of localized annotations for
% rare classes. Making efficient use of the image-text pre-training is crucial since
% it allows for scaling without the need for expensive human annotations.
% The key solution of openset object detection is introducing language to a closed-set
% detector for open-set concept generalization.
% but this complete field has been split into multiple independent subtasks.
