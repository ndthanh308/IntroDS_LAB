\section{Introduction}
\label{sec:introduction}
Detecting objects of interest within a scene using language is a pivotal area of focus. This field encompasses two key tasks: Open-Vocabulary object Detection (OVD)~\cite{zhong2022regionclip,zhou2022detecting,yao2022detclip,ghiasi2022scaling,gu2021open,li2022grounded,zang2022open,zareian2021open,carion2020end,liu2023groundingdino,minderer2022simpleOWLViT} and Referring Expression Comprehension (REC)~\cite{du2022visual,deng2021transvg,arbelle2021detector,wang2021improving,sun2022proposal,subramanian2022reclip,cao2022correspondence,li2022cross,zhou2021real,li2021bottom,song2021co,zhu2022seqtr,li2021referring,luo2020multi,liu2023polyformer,lu2022unified,wang2022unifying,chen2020uniter,hu2017modeling,liu2017referring,yu2016modeling}.
We present an intuitive illustration of these two settings in \cref{fig:teaser}.
The first task, OVD, expands the scope of object detection to any given short category name. However, these setting neglect the instances described by intricate descriptions.
The second is REC.
It focuses on locating the spatial positions of objects described by linguistic expressions and assumes the target object must appear in the given image. However, in real-world scenarios, if describing objects that do not exist in the image, the REC algorithm will also output false-positive bounding boxes.
Recent advancements have witnessed the joint training of bi-functional models, such as Grounding-DINO~\cite{liu2023groundingdino} and UNINEXT~\cite{yan2023universal}, which involve both OVD and REC data.
Notwithstanding, these models still rely on separate training procedures and inference strategies for OVD and REC, and evaluate on these two tasks of independently.

\input{figures/teaser}

As shown in \cref{fig:teaser}, a more practical detection algorithm should be able to detect any described category, whether long or short, complex or simple, while discard predictions in images where targets are absent.
In order to address this significant yet often overlooked scenario, we propose the concept of \textbf{Described Object Detection (DOD)}.
Note that this setting is a superset of OVD and REC. When the language expression is limited to a short category name, it becomes OVD. When we limit the images to detect objects known to be present in the images beforehand, it downgrades to REC.

Can the existing SOTA algorithms of the community support DOD tasks? 
To address this inquiry, this paper establishes the research foundation of DOD tasks by constructing a dataset, scrutinizing relevant methodologies, analyzing the relevant methods, and exploring improvement space.

\textbf{Datasets \& Benchmarks.}
For the DOD task, we built a new dataset called the \textbf{Description Detection Dataset} (\ddd{}, /dikju:b/), comprising 422 well-designed descriptions and 24,282 positive object-description pairs. 
Compared to the former OVD or REC datasets, as depicted in (\cref{fig:highlight}), it has three noteworthy characteristics:
1) All objects referred by descriptions are annotated across the entire dataset, making \ddd{} a detection-style dataset similar to COCO~\cite{lin2014microsoft}, rather than a REC dataset.
2) Instances in this dataset are annotated with flexible and free-form language expressions, which can be short or long, simple or complex, unlike the OVD dataset.
3) We included numerous absence descriptions, such as ``a person without a safety helmet'', addressing a widely-needed yet overlooked detection requirement. This is summarized in \cref{tab:dataset_highlight}.
In \ddd{}, we evaluate three types of SOTA methods, namely OWL-ViT~\cite{minderer2022simpleOWLViT}/CORA~\cite{wu2023cora}(OVD), OFA (REC)~\cite{wang2022ofa}, and UNINEXT~\cite{yan2023universal}/Grounding-DINO~\cite{liu2023groundingdino} (bi-functional methods). This serves as a reference baseline for the community.

\textbf{Findings \& Improvements.} 
We analyze the SOTA OVD~\cite{minderer2022simpleOWLViT,wu2023cora}, REC~\cite{wang2022ofa}, and bi-functional methods~\cite{yan2023universal,liu2023groundingdino} from multiple perspectives through experiments on \ddd{}.
Below are the interesting findings that may offer insights for future research.
1) Existing REC methods obtain weak performance on this benchmark. They fail to provide good confidence scores for DOD, lacks the ability to reject negative instances, and suffers in multi-target scenarios. The essential reason is their task formulation, which performs grounding, \textit{i.e.}, matching between text and a image region, rather than detection, and does not attempt to distinguish positive and negative instances.
2) OVD methods outperform REC methods when directly applied to the DOD task. They show promise in handling multiple instances and rejecting negative ones. However, their performance is hindered by lengthy descriptions due to constraints in their training data.
3) Bi-functional methods, although superior to uni-functional ones, still face challenges similar to REC methods. They struggle to reject negative instances and handle multiple instances effectively. At times, OVD models surpass them, indicating that they have not fully benefited from both REC and OVD approaches. As a result, they are not ready to deal with DOD for now.
4) We propose a baseline that greatly improves the REC method it is based on, and outperforms current SOTAs. It abilities to handle multiple targets and reject negative instances are improved much by simply reconstructing training data to multi-target form, and adding a binary classification sub-task. 
Although there is still room for improvement, this baseline serves as a starting point, and the proposed enhancements are relevant for future DOD research.


% With the development of multi-modal learning, the gap between these object understanding tasks are narrowed rapidly with several advances: the first is the surging of open-vocabulary detection (OVD), which extends the object detection task to detect not only a pre-defined categories, but also other objects with category names; the second is some methods that handles tasks like OVD and REC together, like grounding-DINO and UNINEXT, using language expression as the bridge. These methods are trained jointly on the data of different tasks, but still handles 
% Despite these advances, there still lacks a benchmark to evaluate the effectiveness of these models.

% We propose a general setting for object understanding, namely \texttt{omni-expression detection}, together with a benchmark. In this setting, models are required to locate all instances corresponding to language description from the whole dataset. 
% This setting is more flexible and realistic compared to previous tasks. Compared with category-guided detection like OD or OVD, we retrieve objects based on a language expression with versatile length, rather than a short category name; compared with language-guided grounding like REC, we do not make the unrealistic assumption on the existance or number of instance in the image. Note that this setting is a superset of both REC and OD (OVD).
% When we limit the language expression to be a short category name, it downgrades to object detection; when we limit the images to detect for a language expression from the whole set to only the images with one and only one referred instance, it downgrades to REC.

% A benchmark is proposed for this setting. It has three characterics that worth highlighting:

% The first is \textit{unrestricted language reference}. Instances in this dataset are annotated with flexible and free-form language expressions that can be short or long, simple or complex. This is different from object detection datasets that annotate objects with category names.

% The second is \textit{complete annotation}. For any reference, the objects referred in all images are annotated, so an image can contain zero to multiple instances for a reference, like previous detection datasets.
% Instead, for REC, the objects referred by one description are only annotated on a few images. For other images without this reference, it is unknown whether the corresponding instances exist or not.
% That is to say, the annotations in REC are far from complete.

% The third is \textit{absence expression}. We notice that current datasets with description as references, like RefCOCO for REC, usually describe objects \textbf{possessing} certain features. We also annotate objects \textbf{lacking} some features. Such absence expressions take up approximately $\frac{1}{4}$ of the references defined in this dataset. This is a first for object understanding datasets.

\input{tables/dataset_highlight}

% The dataset is built by re-annotation on a previous dataset GRD, which is proposed for segmentation tasks like referring expression segmentation (RES). Our effort for extension over GRD covers the latter 2 characteristics, i.e., complete annotation across the dataset and absence expressions of objects. Besides, GRD itself only contains semantic-level annotation, and we further provide \textit{instance-level annotation} on it.

% Our dataset for omni-expression detection, build on the generalization of two different types of object understanding tasks (language-guided grounding and category-guided detection), provides a benchmark to verify the capabilities of models to discover and locate complex events. It is very challenging.

% We verify multiple state-of-the-art methods for REC, OVD or their mixture on the proposed benchmark, and propose a new method improved upon OFA. It outperforms existing methods, and can serve as a baseline for future works on language-guided detection.

% The contribution of this work includes:
% \begin{itemize}
%     \item Based on the limitation of existing object understanding tasks, we propose a more realistic and flexible setting called omni-expression detection;
%     \item We proposed a benchmark for evaluation under this setting, which has 4 characteristics that differs from previous benchmarks;
%     \Todo{rewrite this.}
%     \item We evaluate SOTA methods of different object understanding tasks on the proposed dataset, together with a improved model that outperforms them and serve as a proposed baseline.
% \end{itemize}
