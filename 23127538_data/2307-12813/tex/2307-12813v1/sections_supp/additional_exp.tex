\section{More experimental results}
\label{sec:additional_exp}

\input{tables/comparison_supp}
\input{tables/bifunctional_thres}
\input{figures/vis_rejection}

\subsection{Additional results}
\noindent \textbf{More complete comparison between baselines.}
In \cref{tab:comparison_supp} we show a more complete comparison of the evaluated baselines on \ddd{} with different metrics. Results on average recalls are added.
In REC datasets like RefCOCO~\cite{yu2016modeling,mao2016generation}, the standard metric is accuracy (which equals to precision and also recall in REC setting). This is not suitable for DOD, which is essentially a detection task. Here we also report the average recall metric in COCO API, but it does not necessarily correspond to the effectiveness of a method for DOD, which requires rejecting negative instances while REC does not.

As shown in \cref{tab:comparison_supp}, REC methods are bad at recall, possibly because it can only predict one instance for one description, no matter how many instances actually exists in GT.
OVD methods are also bad at this metric though they produce a dozen of output (see \cref{fig:vis_absence,fig:vis_rejection}. This may partially explains its low mAP.
The bi-functional methods and the DOD one are all strong on this metric.
Grounding-DINO, though performs not as good as the proposed OFA-DOD baseline in terms of mAPs, obtains the best recall. This indicates that it tends to produce more detection results.

\noindent \textbf{Inference of bi-functional methods.}
As discussed in Section 5.1 of the main paper, Bi-functional methods obtains a 100\% No-instance FPPC and fails to reject negative images on \ddd{}. This is due to the inference strategy that is based on REC. It is possible to apply other inference strategy for them.

We verify the effect of inference strategy on these 2 bi-functional methods, with No-instance FPPC and overall FULL mAP, and make comparison with the proposed baseline, in \cref{tab:binfunctional_thres}. As shown in the table, we try to apply a threshold to filter certain low-score predictions, similar to the post-processing steps in OVD~\cite{minderer2022simpleOWLViT}.
With this inference strategy, we observe that the increase of score threshold does lower the No-instance FPPC significantly, but at the cost of overall mAP. Therefore, we apply the REC-based inference strategy for these bi-functional methods by default.

Furthermore, we find that when the score threshold is quite high (0.7 for Grounding-DINO and 0.8 for UNINEXT), they reach a FPPC similar to the proposed baseline but with much less overall mAP (15.7 mAP for UniNext and 13.6 mAP for Grounding-DINO, while ours 21.6 mAP). Therefore, it might be fair to say that the proposed baseline achieves a better balance between the ability to reject negative images and the overall detection capability.

% \Todo{mAP (or other overlap metric) to show the difference w/ or wo/ the absence word like "no".}

\input{figures/vis_absence}

\subsection{Visual comparisons}

% \noindent \textbf{Images with multiple instances.}

\noindent \textbf{Rejecting negative samples.}
As shown in \cref{fig:vis_rejection}, we visualized two descriptions and two images with no corresponding GT instance. An ideal DOD method should refrain from predicting instances.

OWL-ViT~\cite{minderer2022simpleOWLViT}, the OVD method, predicts multiple instances on these images, some of which overlap with each other. Such redundant predictions are not suitable for this setting.

OFA~\cite{wang2022ofa}, the REC method, always predicts an instance for one reference, making it highly prone to mistakes in such negative images.

Grounding-DINO~\cite{liu2023groundingdino}, the bi-functional method, correctly locates the ``hot air balloon'' and ``dog'' but fails to capture features related to ``with words'' and ``clothed'' in the language description.

The proposed baseline for DOD successfully rejects one negative image but fails with the other one. This possibly implies that it may perform better on such challenges compared to previous methods, but is still far from being strong.

\noindent \textbf{Absence or presence descriptions.}
In \cref{fig:vis_absence}, we present the detection results for two pairs of descriptions, each with one absence description and its exact counterpart presence description. We visualize the GT (Ground Truth) and also predictions from 4 representative methods.

In the first pair, \textit{butterfly \textcolor{red}{not} stopping on flowers}, the GT exists for the absence description, but not for the corresponding presence counterpart. We observe that previous methods are not sensitive to the distinction between presence and absence, leading to similar results for both descriptions. However, the proposed baseline stands as an exception by correctly predicting the bounding box for the absence description and successfully rejecting the presence one. This could be attributed to the language comprehension ability of OFA, as it is trained on multiple text-related tasks.

In the second pair, \textit{santa claus \textcolor{red}{without} bags}, most methods also yield similar results for both descriptions. Although OFA produces noticeably different bounding boxes for two descriptions, the one corresponding to the absence description is overly large, while the one for the presence description results in a negative prediction. Unfortunately, the proposed baseline incorrectly rejects the predictions for this case.
