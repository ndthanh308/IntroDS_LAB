\section{The Proposed Baseline}
\label{sec:method_details}

As stated in Section 4.2 of our paper, we choose OFA as the foundation for the proposed baseline. We make 3 improvements on it, and the details are stated below:

\subsection{Granularity Decomposition}

The aim of this adjustment is to enhance the suitability of the baseline for localization tasks such as OVD, REC, and DOD.
The original OFA~\cite{wang2022ofa} consists of a multi-modal encoder and a decoder.
For each task, whether it involves image-only, text-only, or image-text inputs, an image (which can be omitted) and a text prompt are fed into the multi-modal encoder to predict the output as a text sequence. 
All task processes are forced to co-exist in one encoder and one decoder.

To achieve this decomposition, we divide the pretraining tasks of OFA into two different granularities: global tasks for language modeling-related tasks like IC, VQA, MLM, etc., and local tasks for region localization-related tasks such as object detection and REC.
We add an extra decoder alongside the original one, which also takes input from the encoder. The two decoders handle global and local tasks independently, thereby avoiding mutual interference.

This improvement effectively resolves conflicts between different tasks and enhances the capability of the model for localization tasks.

\subsection{Reconstructed Data}

This improvement is to benefit detection with multiple target instances.
For OFA, REC is performed with one image and one text prompt (question prefix concatenated with one description) as input, and a bounding box sequence with 4 coordinate tokens as output. The input sequence has the form:

\begin{center}
    \textcolor{blue}{Which region does the text [REF1] describe?} \textcolor{green}{[IMG1]},
\end{center}

where \textcolor{blue}{[REF1]} is a description annotated for the image, and \textcolor{green}{[IMG1]} is the image token sequence.

Originally, each input example in REC is a image-text-box pair, where one reference is annotated with one bounding box for one image.
We reconstruct the data of REC by 2 steps:
First, we grouping the descriptions belonging to one image, and each reconstructed input example is a combination of one image, $N$ positive descriptions, and $N$ boxes, where $N$ is a integer equal to or larger than 1.
Second, for each image, we sample some descriptions from other images as the negative description.
With the prepared data, we change the input as:

\begin{center}
    \textcolor{blue}{Which of these options are in the image? Choose from options: [REF1] [REF2] [REF3] ...}
    \textcolor{green}{[IMG1]},
\end{center}

where \textcolor{blue}{[REF1] [REF2] [REF3]} are positive or negative randomly sampled.
The output is to predict a series of multiple boxes, each followed by its corresponding descriptions in the input.
This results in a unified data format for OD and REC. For OD, the negative descriptions are negative class names.
The reformulated data are noisy, as they are not initially prepared for DOD, and a sampled negative description is not necessarily negative due to the image-level annotation completeness of REC. Still, we find such reconstructed data helpful.

\subsection{Task Decomposition}

This step aims to enhance the baseline's capability to discern false positives. In addition to training on REC (to locate a region based on a reference), we leverage the multi-task nature of OFA by introducing an additional VQA task. This task involves determining whether a predicted region and a description match with each other and can be viewed as a binary classification problem.
The input for this VQA task is:

\begin{center}
    \textcolor{blue}{Does the region [BOX1] describes [REF1]?}
    \textcolor{green}{IMG1},
\end{center}

where \textcolor{blue}{[BOX1]} is the bounding box coordinate tokens corresponds to the description. For training, the box and the reference are either from a GT text-box pair, or the GT box is shifted (as negative sample), or the box and the reference are from different text-box pairs (as negative sample, too).
The output of this task is a text sequence \textcolor{blue}{yes} for positive samples and \textcolor{blue}{no} for negative samples.
This step is responsible for rejecting possible false positives.
