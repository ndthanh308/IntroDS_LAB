\section{Evaluating Existing Baselines}
\label{sec:eval_baselines}

In Section 4.1 of the paper we evaluate several representative and SOTA methods for OVD~\cite{minderer2022simpleOWLViT,wu2023cora}, REC~\cite{wang2022ofa} and bi-functional methods~\cite{liu2023groundingdino,yan2023universal} on the proposed \ddd{} for the DOD task.
Here we introduce these methods and describe how we adapt them to the DOD task and evaluate them on \ddd{}. Notably, the images in \ddd{} do not overlap with the training data of these existing baselines and our proposed baseline, so all the comparisons are actually conducted under zero-shot setting, and is relatively fair.

\noindent \textbf{OFA.}
OFA is the SOTA REC method. It is proposed as a general-purpose vision-language model, with ability to performing various tasks like image captioning (IC), visual question answering (VQA), referring expression comprehension (REC), etc. adopts data from various tasks for pretraining, including mask-language modeling (MLM), IC, VQA, REC, OD, etc.
Notably, through pretrained on object detection datasets~\cite{lin2014microsoft,gupta2019lvis}, it is not evaluated on these tasks at all. We find that a pretrained OFA model merely achieves 9.6 mAP on COCO~\cite{lin2014microsoft} benchmark, which is too far from modern object detectors. This is also the reason we do not include it into bi-functional models.

OFA can be evaluated on a downstream task either after pretraining or after fine-tuning on the specfic dataset.
On REC datasets, it is already strong with only pretraining and achieves SOTA performance after fine-tuning on REC only.
As the images in \ddd{} do not overlap with those in REC datasets, we use the pretrained model of OFA rather than the one fine-tuned on REC data, for better generalization ability.
The official checkpoints are used as the model to evaluate on \ddd{}. Model checkpoints of multiple sizes are available and we use the largest two, namely OFA-base and OFA-large.

For REC task, OFA takes in a pair of one image and one sentence, and predicts a sequence of 4 coordinates, which forms a bounding box.
For DOD, we apply a similar inference strategy. For a image and the candidate descriptions (for intra-scenario setting, only a few descriptions in that scenario; for inter-scenario setting, all the descriptions in the dataset), each description and the image form a input image-text pair and predicts a detected instance (bounding box) that will be saved as the result.
As OFA predicts token sequences of box coordinates and no classification scores, we use the average of the classification score on the 4 coordinate tokens as the confidence score for each detected instance. No further processing is applied.

\noindent \textbf{OWL-ViT.}
OWL-ViT~\cite{minderer2022simpleOWLViT} and CORA~\cite{wu2023cora} are the SOTA OVD methods.
OWL-ViT also adopts a pretraining and fine-tuning strategy for training. It is pretrained with image-text contrastive learning, similar to CLIP~\cite{radford2021learning} and then transferred to OVD with simple modification and fine-tuning on standard detection datasets. For evaluation on \ddd{}, we use the model fine-tuned on detection datasets without other training.
Model checkpoints with ViT-base~\cite{dosovitskiy2020image} and ViT-large backbones are available.

For OVD, OWL-ViT takes in some text sequences and one image, and predicts a lot of instances consisting of bounding boxes, class labels as well as classification scores. The text sequences are category names like ``giraffe'' ``car'', etc. The detected instances with a score less than threshold 0.1 are filtered.
For the proposed DOD, we apply a similar inference strategy. The input text is the candidate descriptions, and the output instances are filtered by the same threshold 0.1. No other modifications or post-process are applied.

\noindent \textbf{CORA.}
CORA~\cite{wu2023cora} is a DETR~\cite{kamath2021mdetr} style method that adapts CLIP~\cite{radford2021learning} to OVD. It takes CLIP as the pretrained model and fine-tune the modified framework on detection datasets~\cite{lin2014microsoft,gupta2019lvis}.

The inference of CORA on OVD is performed as a matching between image region features and category name embeddings encoded by CLIP text encoder. For inference on DOD, we adopt the same strategy. We only replace the input images with the images from \ddd{} and the category names with the candidate descriptions. Other details follow the settings in CORA for OVD.

\noindent \textbf{Grounding-DINO.}
The bi-functional Grounding-DINO~\cite{liu2023groundingdino} extends a close-set object detector DINO~\cite{zhang2022dino} to open-set object detection. It is pretrained on vast object detection~\cite{lin2014microsoft,gupta2019lvis,Krasin2017,shao2019objects365} and image captioning data~\cite{sharma2018conceptual,thomee2016yfcc100m,ordonez2011im2text}.However, this model is not competitive on REC, and a further fine-tuning on REC data~\cite{yu2016modeling,mao2016generation}is required to achieve a strong performance.
Official model checkpoints with Swin-tiny~\cite{liu2021swin} and Swin-base backbones are available.

It produces a lot of detected instances for one image-text input, and filters some instances with a threshold hyper-parameter. For the inference on REC, given an image-reference pair, it merely keeps the one and only instance with the largest score.
We follow its inference process on REC task for the proposed DOD. We will dig more into the specific inference strategy and hyper-parameters in the additional experiments in \cref{sec:additional_exp}. 

\noindent \textbf{UNINEXT.}
UNINEXT~\cite{yan2023universal} stands as another bi-functional method, reformulating a diverse array of tasks, such as object detection, REC, video-based tracking, image and video segmentation tasks, into a unified multi-task framework that excels in instance prediction and retrieval. This innovative approach involves three stages of pre-training without any single-task fine-tuning. In the first stage, training is performed with Object365~\cite{shao2019objects365}, followed by the second stage with REC data and COCO, and finally, the third stage with extensive data from video tasks.

For evaluation on \ddd{}, we utilize the UNINEXT models trained in the second stage, which only utilizes image data and is relatively fair for comparison. Model checkpoints featuring ConvNeXt-large~\cite{liu2022convnet} and ViT-huge backbones are available, and these are the ones we employ for evaluation.

For each task it is pretrained on, UNINEXT designs an individual inference strategy.
For the DOD task, we adopt an inference strategy similar to REC. To delve deeper into the specific inference strategy and hyper-parameters, we conduct additional experiments akin to the those taken in Grounding-DINO~\cite{liu2023groundingdino} in \cref{sec:additional_exp}.
