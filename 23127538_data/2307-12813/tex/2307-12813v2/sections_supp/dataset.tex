\section{Dataset Details}
\label{sec:dataset_details}

\subsection{More examples}

In the Section 3.1 of the main paper, we introduce the characteristics of the proposed \ddd{} dataset, and elaborate the 3 major ones in Fig. 2 with some examples. Here we provide more examples to supplement this part.

\input{figures/examples_completeness}

\noindent \textbf{Complete annotation.}
The first characteristic of \ddd{} is the dataset-level complete and thorough annotations, setting it apart from REC datasets~\cite{yu2016modeling,mao2016generation}. In \ddd{}, every image is annotated for possible positive and negative instances, as demonstrated in \cref{fig:examples_completeness}. This figure includes several images with positive instance labels (first row) and several images with negative instance labels (second row) for each of the four descriptions. Such comprehensive annotation makes the proposed dataset well-suited for detection tasks.

In comparison, REC datasets like RefCOCO~\cite{yu2016modeling,mao2016generation} only annotate several positive instances in a few images for each description, leaving all the other images without annotations for that particular description; thus, their annotation completeness is limited to the image-level. On the other hand, GRD~\cite{wu2023gres} annotates a description for a group of images while dividing the entire set into multiple groups, resulting in an annotation completeness at the group-level.

\input{figures/examples_freeness}

\noindent \textbf{Unrestricted description.}
The categories in \ddd{} encompass more than just simple object names, such as \texttt{cat}, \texttt{dog} and \texttt{bird} found in typical object detection datasets~\cite{lin2014microsoft,gupta2019lvis,shao2019objects365}. As illustrated in \cref{fig:examples_freeness}, the descriptions are expressed in unrestricted natural language. The longer and more complex descriptions resemble references found in REC datasets~\cite{yu2016modeling,mao2016generation,kazemzadeh2014referitgame}. For instance, a description like \texttt{a fisher who stands on the shore and whose lower body is not submerged by water} comprises 16 words and encompasses multiple attributes like \texttt{fisher}, \texttt{stands on the shore} and \texttt{lower body is not submerged by water}. These attributes are semantically abstract and visually diverse. On the other hand, the shorter and simpler descriptions can be similar to the category names in OD datasets, such as \texttt{backpack}, \texttt{swing bench} and \texttt{a sailboat}.
This illustrates that the descriptions of objects in \ddd{} are free-form and unrestricted, covering a wide range of description types present in both REC and OD datasets.

\input{figures/examples_absence}

\noindent \textbf{Absence description.}
To the best of our knowledge, the proposed dataset is the first annotated dataset specifically designed to address absence descriptions. Examples with annotations for both presence and absence descriptions from our dataset (\ddd{}) are illustrated in \cref{fig:examples_absence}. For visualization purposes, we have selected some absence descriptions that have contradictory presence descriptions. The absence descriptions and the corresponding presence descriptions differ primarily in the existence of key attributes. For instance, the first presence description emphasizes black/white boards \textit{with} words written, while the first absence description focuses on those \textit{without} words.

It is important to note that in certain cases, some images contain both absence and presence descriptions. For example, in the first example image of the second presence-absence pair, both dogs led by ropes and not led by ropes coexist. Such instances pose significant challenges, as they require the DOD model to comprehend the absence of concepts in a language description and to discern the subtle differences among instances within an image.

\noindent \textbf{Other characteristics for instance annotations.}
Examples in \cref{fig:examples_completeness,fig:examples_freeness,fig:examples_absence} all illustrate some additional characteristics of \ddd{}:

(1) Instance-level annotation, where each instance is individually labeled.
(2) One description can refer to multiple instances in an image.
(3) Each instance is annotated with both bounding boxes and masks. As a result, the proposed dataset is not limited to the Described Object Detection setting focused on in this work but can also support a similar task, producing instance segmentation masks rather than object detection bounding boxes.

\subsection{More statistics}

The proposed dataset contains a total of 10,578 images, 18,514 boxes (including instance masks), and 422 well-designed descriptions. These descriptions comprise 316 presence descriptions and 106 absence descriptions.

Regarding the inter-scenario setting, considering all 422 descriptions, there are 24,282 positive object-text pairs and 7,788,626 negative pairs. When considering only positive descriptions, there are 16,480 positive pairs and 5,833,944 negative pairs.

For the intra-scenario setting (where candidate descriptions for an image only come from the same scenario), there are 20,279 positive pairs and 53,383 negative pairs. For the subset with only positive descriptions, there are 13,917 positive pairs and 41,231 negative pairs.

The average expression length in the dataset is 6.3 words.

\input{figures/lengths_nums_supp}

In \cref{fig:lengths_nums_supp}, two additional histograms demonstrate the distribution of the number of positive descriptions and the number of positive instances within a single image in the dataset. This visualization highlights the complexity of the proposed dataset, with frequent occurrences of multiple references and many instances within one image.


\noindent \textbf{Absence descriptions.}
To the best of our knowledge, the proposed \ddd{} benchmark is the first to investigate the capability of models to comprehend the absence of certain features and attributes and distinguish between absence and presence. This unique focus on absence-related comprehension sets it apart from previous benchmarks with description annotation (e.g., datasets like RefCOCO~\cite{yu2016modeling,mao2016generation} for REC and RES tasks). Notably, RefCOCO contains an extremely small and neglectable number of instances with absence descriptions. In contrast, the \ddd{} dataset comprises 106 absence expressions out of a total of 422 descriptions, approximately 25\%, and 7,802 positive annotated instances. This significant inclusion of absence-related expressions contributes to a vital and distinguishing characteristic of our proposed benchmark.

\noindent \textbf{Category overlapping with previous datasets.}
The proposed dataset can be regarded as an OVD benchmark (but with longer references rather than category names), if we take classes and references in previous OVD/REC datasets as \textit{base} classes, and the classes in \ddd{} as \textit{novel}.
Categories in \ddd{} has very little overlap with previous datasets. Here we try to quantify the minimal overlap between \textit{base} (OVD datasets like COCO/LVIS and REC datasets like RefCOCO/+/g) and \textit{novel} ($D^3$).
For comparison with OVD datasets, we used ChatGPT to generate synonyms from category names in those datasets and then match them against references in $D^3$. The overlapping percentage is 0.4\% for COCO and 0.9\% for LVIS.
For COCO, which have less categories, we also perform manual check and calculation, resulting in 0.7\% overlap with $D^3$.
For REC datasets, we apply a threshold on the sentence similarity calculated via HuggingFace's \texttt{bert-base-cased-finetuned-mrpc} model. The calculated overlaps of $D^3$ with RefCOCO/+/g is 0.0\%, 0.2\% and 0.7\%, separately.
Thus, novel classes ($D^3$) overlap <1\% with base classes (from OVD \& REC datasets).

\subsection{Annotation process}

\input{figures/anno_process}

The data source of \ddd{] is 106 groups from GRD~\cite{wu2023gres}, with about 100 images crawled from \href{https://www.flickr.com/}{Flickr} and 3\textasciitilde 4 designed refs for each group. Each group belongs to a different scenario and the overlapping between refs from different groups are small (i.e, a ref for one group are not frequent (but possible) to appear in the images from another group). Now we have 10000+ images and 300+ refs.

A diagram illustrating the annotation process of \ddd{} is presented in \cref{fig:annotation_process}. Here we describe the details of the annotation steps as below:

\begin{enumerate}
    \item \textsc{Manual} Adding absence refs: design 1\textasciitilde 2 absence refs based on the images for each group and add them to the corresponding groups. Now we have 400+ refs.
    \item \textsc{Automatic} Selecting possible positive refs: for each image, select \textit{all the refs} (4\textasciitilde 6) from the group it belongs to, and also the other 105 groups (top-$n$ refs out of 400+ refs, by CLIP similarity between the image and each description). Now for each image, we have $n+4$\textasciitilde $n+6$ candidate refs and all the other refs are filtered out. $n$ is set as 40 initially.
    \item \textsc{Manual} Verification: randomly choose 5 groups of images, and check if there are any positive refs that should not be filtered out. If so, increase $n$ to cover that ref and go back to step 2.
    \item \textsc{Manual} Human annotation: annotation by trained annotators on all images. The annotation of boxes (and instance masks) are instance-level, dataset-wise complete, and includes absence refs.
    \item \textsc{Manual} Quality check: this includes 3 small steps:
    \begin{enumerate}
        \item Discarding some images (unsuitable for annotation, e.g., ambiguity) or categories from the dataset. About 8\% samples are discarded.
        \item Quality check on 100\% samples. For each group, if image with error is more than 2\%, it is returned for re-annotation. Otherwise the errors are fixed and this group passes this step.
        \item Final check on 5\% samples. For each group, if there are image with error, it is returned, otherwise it is accepted.
    \end{enumerate}
\end{enumerate}
