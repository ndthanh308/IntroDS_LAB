\section{Dataset}
\label{sec:dataset}
\vspace{-5pt}
\subsection{Dataset highlight}

% \input{figures/examples}
\input{figures/highlight}

The proposed dataset is re-annotated on GRD~\cite{wu2023gres}, a dataset for RES~\cite{zhu2022seqtr,liu2023polyformer}. As briefly introduced in \cref{sec:introduction}, it contains three major characteristics. In \cref{fig:highlight}, we show some examples from previous datasets and \ddd{} to highlight them. Here we elaborate on them with a few other characteristics:

The first is \textit{complete annotation}. For REC, the instances referred to by one description are only annotated in a few images. For other images without the annotation of this description, it is unknown whether the corresponding instance exists or not. That is to say, their annotations are not complete. Contrarily, as shown in \cref{fig:highlight1}, in \ddd{}, the objects referred to in all images by any description are annotated, as are the negative samples, like traditional object detection datasets.

The second is \textit{unrestricted language description}. As shown in \cref{fig:highlight2}, unlike (open vocabulary) object detection that retrieves objects with category names, we retrieve objects with language expressions, which is rather flexible. As is shown in \cref{fig:len_refs}, the lengths of descriptions in \ddd{} vary a lot. The shortest descriptions have one or two words, where the DOD task downgrades to OVD, while the longest may have 15 or more words, resulting in rather complex language expressions.

The third is \textit{absence expression}. Current datasets with language description, like RefCOCO series for REC, usually describe objects with certain features. They usually focus on the ability to discover the existence of concepts but neglect their absence. Noticing the missing ability to verify such capability, we also annotate objects lacking a certain attribute. \cref{fig:highlight3} shows an example with presence description and another with absence description from \ddd{}. Such absence description makes up about one quarter of the references in this dataset. This is a first for existing benchmarks.

The fourth is \textit{instance-level annotation}, a characteristic not held by GRD as it is intended for RES.

The fifth is \textit{one description can refer to multiple instances} in an image, as in \cref{fig:nbox_per_img_ref}. This is not true for REC datasets. If we regard category names as references, then OD datasets do have this feature.

In summary, the proposed dataset differs from the REC dataset primarily in terms of characteristics 1st, 3rd, and 5th. In contrast, when compared to OD datasets, the proposed dataset showcases disparities in the 2nd and 3rd characteristics, and when compared with GRD, in the 2nd, 3rd, 4th, and 5th characteristics. We refer the readers to the \supp{} for more information about the characteristics of \ddd{} and more examples.


\input{figures/lengths_nums}

\subsection{Annotation process}

We utilize the GRD dataset~\cite{wu2023gres} as the source for images, along with its original annotations. Originally, it is divided into multiple groups, each containing several references, with positive and negative samples annotated only within each group. We extend the annotations in three aspects:

\noindent \textbf{Adding instance-level annotation.}
GRD is designed for RES, where each reference corresponds to one semantic mask across one image. However, for the DOD task, which requires the recognition and localization of individual instances, we annotate each instance referred to by a description with an individual bounding box (along with an instance mask). This is the basic step to adapt the dataset for instance localization.

\noindent \textbf{Adding complete annotations.}
In addition to the intra-group annotation in GRD, we further annotate the positive and negative samples for each reference across the entire dataset. With complete dataset-wise annotations, the division into groups becomes unnecessary for evaluation, serving only as a means to organize references by scenarios. This enhancement makes the dataset suitable for detection tasks, significantly increasing the number of positive and negative samples.

Note that we use the complete annotation similar to COCO~\cite{lin2014microsoft}, i.e., explicit positive and negative certificates for all categories on all images, rather than federated annotation~\cite{gupta2019lvis,Krasin2017}. This allows using mAP (mean Average Precision) as the evaluation metric, which is elaborated in \cref{sec:eval_metric}.

\noindent \textbf{Adding annotations for absence expressions.}
We have designed many absence descriptions based on the scenarios within the dataset, in addition to the traditional presence expressions in GRD. We annotate the instances in the images across the entire dataset with these absence expressions.
This step increases the difficulty level of the proposed benchmark and enables the evaluation of existing models' ability to comprehend the absence of concepts.

We present a concise overview of the overall annotation process here.
We organize groups of images and references (both for presence and absence). For each image, the references in its group are used. References from other groups may also appear, but with lower probability.
We employ CLIP~\cite{radford2021learning} to select a large number of candidates from these references in other groups.
We manually check and adjust the hyper-parameters to make sure that such CLIP filtering usually do not miss positive refs.
Subsequently, annotators select the positive references from these candidates (rather than from all references in the dataset) and add bounding boxes to the images.
For more detailed information regarding the annotation process, please refer to \supp{}.


\subsection{Dataset statistics}

\noindent \textbf{GRD statistics.}
% {'nsent': 316, 'nanno': 18343, 'nimg': 10634, 'ngroup': 106, 'num_img_sent': 9238, 'num_anti_img_sent': 22454, 'num_anno_sent': 13734, 'num_anti_anno_sent': 40901, 'avg_sent_len': 5.550632911392405}
% {'nsent': 422, 'nanno': 18343, 'nimg': 10634, 'ngroup': 106, 'num_img_sent': 9238, 'num_anti_img_sent': 22454, 'num_anno_sent': 13734, 'num_anti_anno_sent': 40901, 'avg_sent_len': 5.890995260663507}
It has 10,578 images collected online, divided into 106 groups.
Each group has around 100 images and 3 expressions referring to segmentation masks in this group, resulting in 316 references, 9,323 positive image-text pairs and 22,201 negative pairs.
Note that it only annotates positive and negative samples inside each group, i.e., the annotation completeness is only \textbf{group-level}, so a reference will not be verified outside its group.
The expressions have an average length of 5.9 words.
We refer the reader to the original paper for specific statistics of GRD.

\noindent \textbf{\ddd{} statistics.}
The proposed \ddd{} has 10,578 images, all from GRD. It has 422 well-designed expressions, including 316 expressions from GRD and 106 absence expressions we added (one for each scenario).
The instance-level annotation results in 18,514 boxes.

% intra-group:
% {'nsent': 316, 'nanno': 18514, 'nimg': 10578, 'ngroup': 106, 'num_img_sent': 9323, 'num_anti_img_sent': 22201, 'num_anno_sent': 13917, 'num_anti_anno_sent': 41231, 'avg_sent_len': 5.987341772151899}
% {'nsent': 422, 'nanno': 18514, 'nimg': 10578, 'ngroup': 106, 'num_img_sent': 13035, 'num_anti_img_sent': 29067, 'num_anno_sent': 20279, 'num_anti_anno_sent': 53383, 'avg_sent_len': 6.31042654028436}
% inter-group:
% {'nsent': 316, 'nanno': 18514, 'nimg': 10578, 'ngroup': 106, 'num_img_sent': 9323, 'num_anti_img_sent': 22201, 'num_anno_sent': 16480, 'num_anti_anno_sent': 5833944, 'avg_sent_len': 5.987341772151899}
% {'nsent': 422, 'nanno': 18514, 'nimg': 10578, 'ngroup': 106, 'num_img_sent': 13035, 'num_anti_img_sent': 29067, 'num_anno_sent': 24282, 'num_anti_anno_sent': 7788626, 'avg_sent_len': 6.31042654028436}
Due to the effort in \textit{complete annotation}, for a reference, each image in the dataset is annotated for possible positive and negative samples, i.e., the annotation completeness is \textbf{dataset-level}.
Thus, there are 24,282 positive object-text pairs and 7,788,626 negative pairs, orders of magnitude larger than GRD.
Among them, those with images and texts from the same scenario are probably more difficult, which includes 20,279 positive and 53,383 negative pairs.
The average length of expressions is 6.3 words, due to the relative longer absence expressions.
More statistics and examples of \ddd{} are available in \supp{}.

% \input{figures/evaluation}

\subsection{Evaluation metrics}
\label{sec:eval_metric}

The classification of instances in \ddd{} is \textbf{multi-label}. Each description corresponds to a category. Naturally, there can be relationships between categories, such as parent-child hierarchies, synonyms, and partial overlap. When designing categories, we intentionally reduce parent-child or synonym relationships to ensure greater diversity and challenge. However, there exists partial overlap between categories. Therefore, in \ddd{}, one instance may correspond to multiple descriptions, and the classification in \ddd{} is multi-label~\cite{gupta2019lvis} rather than single-label~\cite{lin2014microsoft}, making it suitable for categories with relationships. An effective detector should assign all relevant positive categories (e.g., \texttt{dog not led by rope outside} and \texttt{clothed dog} for a clothed dog not led by rope outside) for an instance.

We use \textbf{standard mAP} for evaluation. Given the multi-label setting and the exhaustive annotation (all positive and negative labels are known for an instance) of \ddd{}, category relationships will not affect the evaluation, so we can use consistent evaluation for each category across all images.
We describe the evaluation process here.
For inference, an instance predicted with category A and B is regarded as an instance for category A and an instance for B.
The AP for each category is computed as follows: Predictions for each category across all images are sorted by score in descending order, and those with a ground truth IoU exceeding a threshold are counted as TP (and the ground truth is marked as taken), while the rest are counted as false positives. With these TP and FP instances, we calculate the precision, recall, and AP. The mAP is calculated by averaging the AP across all categories.

We use \textsl{FULL}, \textsl{PRES}, and \textsl{ABS} to denote evaluation on all descriptions, presence descriptions only, and absence descriptions only. If not noted explicitly, the \textsl{FULL} setting is adopted.
The specific metrics for \ddd{} include:
\textit{Intra-scenario mAP:}
For this metric, we perform evaluation on each image with only the descriptions from the image's scenario. The final metric is the mAP averaged on different IoU thresholds from 0.5 to 0.95, following COCO~\cite{lin2014microsoft}. This is used as the default metric in our experimental settings.
\textit{Inter-scenario mAP:}
It is similar to the intra-scenario mAP described above, except that for each image, we detect the possible instances with all 422 references. This is aligned with the common mAP in object detection datasets~\cite{lin2014microsoft} and is much more challenging than the intra-scenario mAP.

% \noindent \textbf{Recall.}
% In REC datasets like RefCOCO~\cite{yu2016modeling,mao2016generation}, each image will have one and only one prediction for each reference, and the standard metric is accuracy (which is precision and also recall). This is not suitable for DOD, which is essentially a detection task. We adopt the average recall metric in COCO API for some analyses, but it does not necessarily correspond to the effectiveness of a method.

% The evaluation process for this metrics are illustrated in \cref{fig:evaluation}.
