\section{Related Work}
\vspace{-5pt}
\label{sec:related_work}
\subsection{Relevant datasets and benchmarks}
\vspace{-5pt}
\noindent \textbf{Object detection datasets.}
% Object detection is a mature task in computer vision and many datasets have been proposed for this task.
% PASCAL VOC, MSCOCO, LVIS, Object365, OpenImage, ODinW, BigDetection, V3Det.
A variety of datasets have been proposed for object detection. Some have become standard benchmarks, like PASCAL VOC~\cite{Everingham2010} and COCO~\cite{lin2014microsoft}; while others are more frequently used for pretraining~\cite{shao2019objects365,Krasin2017,cai2022bigdetection}. A few works have focused on special settings, such as LVIS~\cite{gupta2019lvis} for long-tailed detection and ODinW~\cite{li2022elevater} for zero-shot evaluation in the wild.
Recently, V3Det~\cite{wang2023v3det}facilitates object detection with an extremely large vocabulary.
Some are re-splitted and frequently used in OVD as well, like COCO and LVIS.
As explained in \cref{sec:introduction}, these datasets are all annotated with simple category labels rather than flexible language expressions like \ddd{}.
% A variety of datasets have been proposed for object detection. A popular line of datasets are curated as benchmark for general-purpose, including PASCAL VOC\cite{Everingham2010}, MSCOCO\cite{Lin2014}, Object365\cite{shao2019objects365}, OpenImage\cite{Krasin2017}, and BigDetection\cite{cai2022bigdetection}.
% Another line of datasets are proposed for specialized scenes, including V3Det\cite{wang2023v3det}, LVIS\cite{Gupta2019} for long-tail data scenes;  ODinW\cite{li2022elevater} to evaluate model transferring scenes; Wider Face~\cite{yang2016wider}, Pedestrian Detection~\cite{dollar2011pedestrian}, and PTD~\cite{andriluka2008people} for human-centric scenes; and Cityscapes~\cite{cordts2016cityscapes}, KITTI~\cite{geiger2012we}, and Mapillary~\cite{neuhold2017mapillary} for driving scenes.
% , which can be summarized into three categories. 
% including PASCAL VOC, MSCOCO, LVIS, Object365, OpenImage, ODinW, BigDetection, and V3Det.
% (1) Benchmark Dataset: PASCAL VOC\cite{Everingham2010}, MSCOCO\cite{Lin2014}.
% (2) Large-Scale Dataset: Object365\cite{shao2019objects365}, OpenImage\cite{Krasin2017}, BigDetection\cite{cai2022bigdetection}.
% (3) Specialized Dataset: ODinW\cite{li2022elevater}, V3Det\cite{wang2023v3det}, LVIS\cite{Gupta2019}.
% These object detection datasets have evolved significantly, both addressing various challenges for specific cases and pushing the boundaries of algorithmic advancements.
% From benchmark datasets like PASCAL VOC and MS COCO to specialized datasets like KITTI and Visual Genome, each dataset contributes to the progress of object detection research by providing unique characteristics, diverse annotations, and real-world contexts for algorithm evaluation and development. 

\noindent \textbf{Referring expression comprehension datasets.}
% Referece expression comprehension is a multi-modal task.
% RefCOCO, RefCOCOg, RefCOCO+. RefClef. Visual Genome. PhraseCut.
% \Todo{notice that phraseCut refers to multiple instances per image.}
% Referring expression comprehension (REC) aims to localize an object in the given image described by a language expression.
Several datasets have been introduced to evaluate REC methods, including RefClef~\cite{kazemzadeh2014referitgame}, RefCOCO~\cite{yu2016modeling}, RefCOCO+~\cite{yu2016modeling}, RefCOCOg~\cite{mao2016generation}, Visual Genome~\cite{krishna2017visualgenome}, and PhraseCut~\cite{wu2020phrasecut}.
Some~\cite{kazemzadeh2014referitgame,yu2016modeling} are collected interactively, and the expressions are more concise and less diverse.
% A characteristic of RefCOCO+ is that location words are banned in its expressions, which also makes it more challenging.
% Among them, RefCOCO+ bans location words in expressions, making it more challenging.
RefCOCOg is collected non-interactively, resulting in more complex expressions.
Comparatively, Visual Genome focuses on visual relationships.
All these datasets only annotate a few positive images for each category and leave other images unknown, which makes them unsuitable for the detection task.
% One of the key strengths of Visual Genome lies in its extensive annotations of visual relationships, which enables researchers to investigate complex scene understanding, contextual reasoning, and the interplay between objects in an image.
% PhraseCut, collected on top of the Visual Genome, where each image is accompanied by a set of annotated phrases that describe the objects present in the scene. These phrases may include object attributes, relationships, or other linguistic descriptions that provide a more comprehensive understanding of the visual content.

\noindent \textbf{Other related tasks and datasets.}
% A few similar tasks or benchmarks were proposed, but still very different from DOD.
% The Phrase Detection task~\cite{plummer2020phrasedet} lacks explicit negative certificates as negative instances are not labeled, and it actually is not a detection task. Additionally, its references are simply phrases.
% In DOD, we ensure that positive instances are annotated exhaustively and all the other references are reliably negative labels, and allow references to be words, phrases, or even sentences.
% The Cops-Ref benchmark~\cite{chen2020copsref} focuses on assessing the grounding capability of REC methods in difficult negative regions with related and distracting targets. It provides explicit negative certificates for only a small set of images, but not across the whole dataset like \ddd{}.
% The zero-shot grounding task~\cite{sadhu2019zeroshotgrounding} focuses on locating concepts not in the training set. However, it still assumes the presence of objects described by query phrases in images, just like REC. In contrast, DOD aims to detect objects described by flexible expressions throughout each image in the dataset. Thus, there can be zero, one or multiple objects referred in an image. More specifically, zero-shot grounding task assumes the existence of objects, locates one target only in a image, with a short phrase, while DOD makes no assumption about whether the referred target exists, and locates zero to multiple targets, with varied language description from short category names, to phrases, and long descriptions.
% They have different focuses and zero-shot grounding is conceptually close to a subset of DOD.
Several related tasks and benchmarks exist, but they differ significantly from DOD.
Phrase Detection~\cite{plummer2020phrasedet} lacks explicit negative labels as negative instances are unlabeled, and does not constitute a true detection task. Additionally, its references are simply phrases.
In contrast, DOD ensures exhaustive annotation of positive and negative labels, and its references can be words, phrases, or sentences.
Cops-Ref benchmark~\cite{chen2020copsref} focuses on evaluating the grounding capability of REC methods in difficult negative regions with related and distracting targets. It provides explicit negative certificates for only a limited set of images.
In \ddd{}, negative certificates are available across the entire dataset.
Zero-shot grounding~\cite{sadhu2019zeroshotgrounding} centers on locating concepts not in the training set. It assumes the existence of the object referred by a reference in a image, and locates a single target per image, with a short phrase, while DOD makes no assumptions about the existence of the target, and locates zero to multiple targets, with varied expressions.


\subsection{Current methods}

\noindent \textbf{Open-vocabulary object detection methods.}
% Traditional object detection methods.
% Open vocabulary detection.
% Models integrating other similar tasks: 
% RegionCLIP~\cite{zhong2022regionclip}: adopt the pretrained visual encoder in CLIP and finetune on image-text pairs in CC3M.
% Detic. use clip embeddings to encode category names for OVD.
% GLIP and GLIP v2: phrase localization on flick30k.
% Object detection is a computer vision task that allows to identify and locate objects in an image or video.
% Object detection models have been traditionally formulated for closed-vocabulary settings, which can be divided as ``one-stage”~\cite{} and
% ``two-stage”~\cite{} formulations. 
% However, this vanilla object detection is limited to the fixed object category defined in advance, which greatly limits the practicability of existing methods.
Open-vocabulary detection is currently receiving increased attention. It aims to detect arbitrary classes using language for generalization, even when trained on a limited set of classes.
The first approach, OVR-CNN~\cite{zareian2021open}, utilizes image-caption pairs for pretraining the visual encoder to enhance its zero-shot generalization capabilities. With the introduction of CLIP~\cite{radford2021learning}, models such as Detic~\cite{zhou2022detecting}, DetCLIP~\cite{yao2022detclip}, RegionCLIP~\cite{zhong2022regionclip}, and OV-DETR~\cite{zang2022open} have further advanced image and language embeddings pretrained using CLIP.
ViLD~\cite{gu2022openvocabulary} further distills knowledge from CLIP to inherit language semantics for recognizing novel classes.
GLIP~\cite{li2022grounded,zhang2022glipv2} formulates object detection as a phrase grounding problem~\cite{plummer2015flickr30k} and utilizes additional phrase grounding data to facilitate vision-language alignment.


% It shows that such a formulation can even achieve stronger performance on fully-supervised detection benchmarks.

\noindent \textbf{Referring expression comprehension methods.}
% REC is natively a vision-language task.
Existing works~\cite{deng2021transvg,song2021co,li2021referring,liu2023polyformer,wang2022ofa} can be divided into three categories.
(1) Specialist models tailored for REC.
Previously, two-staged works~\cite{hu2017modeling,yu2016modeling} reformulate this as a ranking task.
More recently, one-stage approaches~\cite{zhou2021real,subramanian2022reclip,arbelle2021detector} speed up the inference process.
(2) Multi-task models~\cite{zhu2022seqtr,li2021referring,liu2023polyformer}.
They usually design a unified formulation for a few closely related tasks.
For example, SeqTR~\cite{zhu2022seqtr} unifies REC and RES as a point prediction problem.
% RefTR~\cite{li2021referring} proposes a simple one-stage multi-task framework for REC and RES, which is capable of simultaneous language grounding at both a bounding box and segmentation level.
% MCN~\cite{luo2020multi} novelly proposes Consistency Energy Maximization to enable REC and RES to focus on similar visual regions by maximizing the consistency energy between these two tasks.
% PolyFormer~\cite{liu2023polyformer} formulates REC and RES as a sequence-to-sequence prediction problem, which naturally fuses multi-modal features together as input and generate a sequence of polygon vertices and bounding box corner points.
(3) Multi-modal pre-training models~\cite{chen2020uniter,lu2022unified,wang2022ofa}.
% Recent progress in multi-modal understanding has been mainly powered by pre-training large transformer models to learn generic multi-modal representations from enormous amounts of aligned image-text data, then fine-tuning them on downstream tasks. 
% A prevalent paradigm is to extract visual and textual features independently and then use the attention mechanism of the transformers to learn an alignment between the two.
Unified-IO~\cite{lu2022unified} and OFA~\cite{wang2022ofa} propose unified sequence-to-sequence frameworks that can handle a variety of vision, language, and multi-modal tasks. Currently, OFA holds the SOTA among REC methods.

% These methods can be
% divided into single stream [6, 24, 65, 22] and two-stream
% [47, 28, 29, 46] architectures depending on whether the text
% and images are processed by a single combined transformer
% or two separate transformers followed by some cross attention layers. For both these types, the prevalent approach is
% to extract visual and textual features independently and then
% use the attention mechanism of the transformers to learn an
% alignment between the two

% Specialist models.
% TransVG.
% RefTR. SeqTR. PolyFormer. REC + RES.
% Pretrained models.
% OFA. UNITER. Unified-IO. multi-task pretrain + single-task fine-tuning.


\noindent \textbf{Bi-functional models for REC and OVD/OD.}
% Pretrain + Finetune.
% M-DETR. pretrain on detection data. finetune and eval on RefCOCO.
% Grounding-DINO. UNINEXT. train on multi-task data including detection and REC. direct evaluation on each task.
% \Todo{explain why these methods have not solve this problem.}
% OVD (or OD) and REC can be unified as instance perception tasks, which aims at finding certain objects specified by some queries such as category names, language expressions, and target annotations. Therefore, a lot of recent work attempts to combine OD and REC tasks, considering the instance perception tasks as ``text-conditioned object detection".
% Specifically, the OD task is used to pre-train a powerful object detector, and then the language modality is introduced to fine-tune a vision-language understanding specialized model (e.g. REC task). 
Some recent works~\cite{kamath2021mdetr,dou2022fiber,kuo2022findit,liu2023groundingdino,yan2023universal} aim to handle tasks such as OVD (or OD) and REC concurrently within a single model. They typically restructure the training approach for these tasks, enabling a single model to learn from datasets related to both tasks. However, the inference process for each task remains distinct and independent of the other.
% MDETR~\cite{kamath2021mdetr}, which is derived from the DETR~\cite{carion2020end} framework, undergoes pretraining on OD and image-text tasks, followed by fine-tuning and evaluation on REC.
FIBER~\cite{dou2022fiber} employs a two-stage pretraining strategy, separately utilizing image-text and image-text-box data to enhance data efficiency.
More recently, Grounding-DINO~\cite{liu2023groundingdino} extends a closed-set detector by performing vision-language fusion at multiple stages and evaluating its performance on REC datasets.
UNINEXT~\cite{yan2023universal} reformulates various image and video tasks into a unified object discovery and retrieval paradigm.
Despite these models sharing knowledge between detection and REC through pretraining, they are still treated as distinct tasks in these bi-functional models.

Methods with potential for DOD are continuously emerging and we will update them in \href{https://github.com/Charles-Xie/awesome-described-object-detection}{this list}.
% The main challenge in this task is how to transfer the image-level representations of the
% image-text backbone to detection despite the scarcity of localized annotations for
% rare classes. Making efficient use of the image-text pre-training is crucial since
% it allows for scaling without the need for expensive human annotations.
% The key solution of openset object detection is introducing language to a closed-set
% detector for open-set concept generalization.
% but this complete field has been split into multiple independent subtasks.
