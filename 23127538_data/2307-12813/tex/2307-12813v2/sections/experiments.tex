\section{Experimental Analyses}
\label{sec:experiments}

\subsection{Comparison of baselines on our metrics}
\label{subsec:experimental_comp}

\input{tables/comparison}

We make comparisons on the baselines introduced in \cref{sec:baselines}, mainly with the intra-scenario setting.
Unless explicitly noted, this is the default setting, instead of the more difficult inter-scenario.

\noindent \textbf{Existing SOTAs are insufficient for DOD, and bi-functional models outperform others.}
As demonstrated in \cref{tab:comparison}, existing methods, while achieving SOTA performance on their original benchmarks, fall short in delivering strong performance on \ddd{}.
Among them, recent bi-functional methods~\cite{liu2023groundingdino,yan2023universal} are notably superior to others, and currently, OVD methods outperform REC.
The inferiority of REC methods is likely due to their impractical setting described in \cref{sec:introduction}, which involves predicting one and only one instance for each reference. We will delve into this further.

\noindent \textbf{Rejecting irrelevant references are difficult, which REC are naturally incapable of.}
In contrast to intra-scenario evaluation, the inter-scenario setting assesses all references in the dataset for each image. Since references from other scenarios are likely not semantically relevant to the images, this necessitates the ability to reject irrelevant references for an image. This aligns with the evaluation in standard detection tasks.
From \cref{tab:comparison}, it is evident that OFA, a REC method, almost completely fails in this setting. This is caused by its prediction of a region for every reference, resulting in a large number of false positives when there are numerous candidate references. This underscores the importance of empowering REC methods with the ability to reject false positives.
We find that none of the verified methods achieve good performance under the inter-scenario setting, indicating that existing methods are far from being capable of DOD. This highlights the challenge of \ddd{}

\noindent \textbf{The proposed baseline outperforms existing methods.}
The proposed baseline is based on OFA, but our improvements significantly enhance its performance. It outperforms all existing methods in the intra-scenario setting and surpasses them by a wider margin in the inter-scenario setting. This may suggest that the proposed baseline has a stronger ability to reject irrelevant references.
Nonetheless, the proposed method is far from perfect and can only serve as a baseline for future research.

\input{figures/score_distribution}

\subsection{Further analysis}

\noindent \textbf{Absence descriptions are more difficult for most methods.}
As shown in \cref{tab:comparison}, the performance of baseline methods on \textsl{PRES} (presence descriptions) is consistently superior to that on \textsl{ABS} (absence descriptions). This suggests that existing methods may not effectively differentiate between the presence and absence of attributes in a language description.


\noindent \textbf{REC methods fail to provide good confidence scores.}
We visualized the score distributions from baselines for TPs and FPs, to assess their capabilities in classification and confidence estimation.
As in \cref{fig:score_distribution}, the confidence scores from OFA do not exhibit a clear distinction between TP and FP cases. This can be attributed in part to the seq2seq framework in OFA, which does not directly yield confidence scores, and in part to the grounding formulation of REC, which identifies the image region most similar to the text description without distinguishing between positive and negative.

With a task decomposition step to enhance binary classification performance, our OFA-DOD demonstrates a significant disparity between TP and FP score distributions, yielding more reliable classification results. Note that this improvement does not necessitate modifications to the model framework or training datasets; rather, it is attributed to a more appropriate task formulation.

\input{tables/analysis_ref_num}

\noindent \textbf{Multi-instance detection is challenging for methods other than OVD.}
For each image, \ddd{} can have zero to multiple instances \textbf{for a single description}. To assess how current methods handle varying numbers of instances, we conducted evaluations under three different settings: \textbf{no-instance}, where for a reference, evaluations are limited to images without any referred instance; \textbf{one-instance}, for images with a single instance; and \textbf{multi-instance}, for images with multiple instances. As shown in \cref{tab:analysis_box_num}, OVD methods outperform others when multiple instances are referred by the description, although they may not be as competitive on the entire dataset or images with few instances. Notably, OWL-ViT maintains consistent performance even as the number of instances increases, which sets it apart from other methods. In contrast, REC and current bi-functional methods struggle in multi-instance scenarios. This highlights the strength of OVD methods in multi-target detection, while REC and current bi-functional approaches are less robust in such situations.

\noindent \textbf{REC and bi-functional methods lack the ability to reject negative instances.} In the \textbf{no-instance} column of \cref{tab:analysis_box_num}, we do not report mAP since there are no positive instances in GT for the corresponding reference, making AP inapplicable. Predictions on such images are FPs, so we measure the ratio of images where FPs are produced to the total number of no-instance images for a given reference, namely False Positives Per Category (FPPC). We report the average FPPC over all references.
We observe that most baselines are incapable of determining whether an image contains the referred target or not, yet they still produce predictions. This behavior is expected for REC methods. Bi-functional methods, trained and inferred with the REC task formulation, also exhibit this issue. Only the OVD method and our proposed baseline can effectively reject such negative image-text pairs.

\input{tables/analysis_ref_length}

\noindent \textbf{OVD methods suffer from long descriptions greatly while others do not.}
We partition the references according to their lengths and then evaluate on these partitions. The results are shown in \cref{tab:analysis_ref_length}, where \textit{short}, \textit{middle}, \textit{long} and \textit{very long} corresponding to references with 1\textasciitilde 3, 4\textasciitilde 6, 7\textasciitilde 9, and more than 9 words.
For \textit{short} descriptions, which is close to OVD setting, OVD and bi-functional methods obtain similar performance. However, as the length of references increases, the performance of OVD methods decrease fast, while REC and bi-functional methods suffer less from this. We can see that OVD methods are sensitive to long references, as expected, while other two types do not.

More experiments and additional \textbf{qualitative results} are available in \supp{}.

% \input{tables/recall}

% \noindent \textbf{Compared with REC, OVD methods tend to make predictions only when it's certain.}
% % \noindent \textbf{Analysis on recalls.}
% The recalls of different methods are shown in \cref{tab:recall}. OVD methods are obviously bad at recall, which means that it tends to produce a result when it is quite certain. Grounding-DINO, though performs not as good as the proposed baseline in terms of mAPs, obtains the best recall. This indicates that it tends to produce more detection results.

% \Todo{do we need to consider inference strategy (one ref at a time vs all ref at a time? No.}

% \input{figures/qualitative}

% \noindent \textbf{Qualitative analysis.}
% We visualize the results of different baseline methods on the proposed benchmark. This is shown in \cref{fig:qualitative}. We can see that the proposed method handles such images better.

\input{tables/method_ablation}

\subsection{Ablation on the proposed baseline}

\noindent \textbf{Method components.}
In \cref{tab:method_ablation}, we perform ablation on the proposed improvements in our baseline, step-by-step from OFA to OFA-DOD, to see how they affect the performance.
Granularity decomposition (GD) makes the method more suitable for localization task.
It disentangle tasks of global or local granularity by handling them with 2 separated branch.
Reconstructed data (RD) uniforms REC and OD data into the same form, and prepares multi-instance samples with both short and long references.
Task decomposition (TD) is proposed to help rejecting FPs.
It breaks down the DOD task into a REC step followed by a VQA step.
All three of them improve the performance obviously.

\noindent \textbf{Training tasks.}
We also perform a drop-one-out ablation on the multi-modal multi-task training data, in \cref{tab:ablation_tasks}.
\textbf{Detection} data provides samples for localization, especially multi-instance situation.
It is instinctively important for learning to localize, and indeed matters for performance.
\textbf{I2T} (image-to-text, like image captioning and visual question answering) often helps the generalization and zero-shot performance of multi-modal methods. We find that it does affect the zero-shot performance on \ddd{} greatly.
\textbf{MLM} is theoretically important for language understanding and generalization. However, we find it actually is not.
Removing the MLM task has no significant effect on the performance.
We surmise that the generalization ability of OFA-DOD on \ddd{} mainly comes from I2T.
