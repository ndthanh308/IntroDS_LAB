\section{Introduction}
\label{sec:introduction}

Detecting objects of interest within a scene using language is a pivotal area of focus. This field encompasses two key tasks: Open-Vocabulary object Detection (OVD)~\cite{ghiasi2022scaling,gu2022openvocabulary,li2022grounded,minderer2022simpleOWLViT,zang2022open,zareian2021open} and Referring Expression Comprehension (REC)~\cite{li2021referring,luo2020multi,liu2023polyformer,yu2016modeling,zhou2021real}.
We present an intuitive illustration of these two settings in \cref{fig:teaser}.
The first task, OVD, expands the scope of object detection (OD) to any given short category name. However, these settings neglect the instances described by intricate descriptions.
The second task, REC, focuses on spatially locating one target described by an expression and assumes the target must exist in the image. However, in real-world scenarios, if the described objects do not exist in the image, REC algorithms output false-positive results.
Recent advancements have witnessed the joint training of bi-functional models, such as Grounding-DINO~\cite{liu2023groundingdino} and UNINEXT~\cite{yan2023universal}, which involve both OVD and REC data.
Notwithstanding, these models still rely on separate training procedures and inference strategies for OVD and REC, and evaluate these two tasks independently.

\input{figures/teaser}

As shown in \cref{fig:teaser}, a more practical detection algorithm should be able to detect any described category, whether long or short, complex or simple, while discarding predictions in images where targets are absent.
In order to address this significant yet often overlooked scenario, we propose the concept of \textbf{Described Object Detection (DOD)}. Note that this setting is a superset of OVD and REC.
When the language expression is limited to a short category name, it becomes OVD. When we limit the images to detect objects known to be present in the images beforehand, it downgrades to REC.

Can the existing SOTA algorithms of the community support DOD tasks?
To address this inquiry, this paper establishes the research foundation of DOD tasks by constructing a dataset, scrutinizing relevant methodologies, analyzing the relevant methods, and exploring improvement space.

\textbf{Motivation \& real-world application of DOD.}
% OVD can only perform detection based on categories, where the detection targets are limited to certain ``object classes'' rather than ``objects with specific attributes/relationships''. This approach lacks an understanding of contextual information within images and cannot leverage language to precisely control detection targets and requirements. This inflexibility prevents it from meeting specific application demands.
% REC, while capable of comprehending longer object descriptions with attributes or relationships, assumes the existence of such objects in the image. In cases where the described object doesn't exist, REC lacks the ability to reject or filter, leading to false positive errors. This issue poses a significant problem for practical applications and limits its direct usability.
% Consider a practical scenario, such as detecting ``individuals without helmets'' in a construction site using camera data. An OVD method can detect objects like ``helmets'' and ``people'', but can't determine the relationship between people and helmets, rendering it unsuitable for direct application. On the other hand, the REC method produces localization results in any image, but often generates false positives, making it impractical. The current solutions involve breaking down the process, first detecting ``people'' and ``helmets'', then training a separate model to determine the relationship between them, or determining presence first, followed by the REC method for localization. This approach requires multiple specialized models, tailored for each scenario, which is far from practical and is inefficient in terms of development.
OVD is limited to categorical detection, focusing on \textit{classes} rather than specific attributes or relationships. It lacks detailed contextual understanding and cannot adapt to precise detection requirements from language.
REC comprehend longer descriptions for attributes or relationships, but assumes the existence of one target in the image. This leads to false positives when the target is absent, limiting its practical usability.
Consider detecting \texttt{individuals without helmets} on a construction site using camera data: OVD can detect \texttt{helmets} and \texttt{people} but not determine their relationship. REC locate one region in any image and generate false positives frequently. Existing solutions involve using separate models for object detection then relationship classification, or REC after image classification, both resulting in inefficiency.

% Hence, there is a significant demand for detection based on language descriptions: a model with strong generalization capabilities, capable of determining whether the described object exists in the image and localizing it based on arbitrary language descriptions. This is where our proposed DOD task comes in. The introduced DOD task has various practical applications, including:
% Urban security, like detecting ``individuals without helmets'' in construction sites, ``dog outside without leash'' in communities, ``clothes hung outdoors'' on a street, ``overloaded vehicles'' and ``fallen trees on roadsides'' on the road, etc;
% Network security, where sensitive images containing bloodshed or violence need to be detected within a massive image dataset;
% (Fine-grained) photo album retrieval based on language (descriptions, keywords, etc.);
% Retrieval and filtering of web image data;
% Detection of specific events in autonomous driving, such as ``pedestrians crossing the road''.
Hence, there is a demand for language-based object detection: a model with strong generalization capabilities that can verify the existence of described objects in images and localize them based on arbitrary expressions. The proposed DOD task addresses this need and finds practical applications in:
urban security, detecting
% \texttt{individuals without helmets} in construction sites,
\texttt{dogs without leashes} in communities, \texttt{clothes hung outdoors} on streets, \texttt{overloaded vehicles}, and \texttt{fallen trees on roadsides};
network security, like identifying sensitive images with violence or bloodshed within large datasets;
(fine-grained) photo album retrieval based on descriptions or keywords;
retrieval and filtering of web image data;
specific event detection in autonomous driving, such as \texttt{pedestrians crossing the road}.


\textbf{Dataset \& benchmark.}
% For the DOD task, we built a new dataset called \textbf{Description Detection Dataset} (\ddd{}, /dikju:b/), comprising 422 well-designed descriptions and 24,282 positive object-description pairs. 
% Compared to the former OVD or REC datasets, as depicted in (\cref{fig:highlight}), it has three noteworthy characteristics:
% 1) All objects referred by descriptions are annotated across the entire dataset, making \ddd{} a detection-style dataset similar to COCO~\cite{lin2014microsoft}, rather than a REC dataset.
% 2) Instances in this dataset are annotated with flexible and free-form language expressions, which can be short or long, simple or complex, unlike the OVD dataset.
% 3) We included numerous absence descriptions, such as ``a person without a safety helmet'', addressing a widely-needed yet overlooked detection requirement. This is summarized in \cref{tab:dataset_highlight}.
% On \ddd{}, we evaluate three types of SOTA methods, namely OWL-ViT~\cite{minderer2022simpleOWLViT}/CORA~\cite{wu2023cora}(OVD), OFA (REC)~\cite{wang2022ofa}, and UNINEXT~\cite{yan2023universal}/Grounding-DINO~\cite{liu2023groundingdino} (bi-functional methods). This serves as a reference for the community.
For DOD, we introduce the \textbf{Description Detection Dataset} (\ddd{}, /dikju:b/), an evaluation-only benchmark containing 422 descriptions and 24,282 positive object-description pairs. Unlike previous OVD or REC datasets (see \cref{fig:highlight}), \ddd{} stands out in three key aspects (see \cref{tab:dataset_highlight}):
1) \textit{Complete annotation}: All descriptions refer to objects annotated throughout the dataset, making \ddd{} a detection-style dataset akin to COCO~\cite{lin2014microsoft}.
2) \textit{Unrestricted description}: Annotations in \ddd{} include diverse and flexible language expressions, varying in length and complexity.
3) \textit{Absence expression}: We include descriptions regarding absence of concepts, such as \texttt{a person \textit{without} a safety helmet}, addressing an often-overlooked detection requirement.
The details of \ddd{} is elaborated in \cref{sec:dataset}.
We evaluate state-of-the-art methods on \ddd{}: OWL-ViT~\cite{minderer2022simpleOWLViT}/CORA~\cite{wu2023cora} (OVD), OFA (REC)~\cite{wang2022ofa}, and UNINEXT~\cite{yan2023universal}/Grounding-DINO~\cite{liu2023groundingdino} (bi-functional) to provide a reference for the community. This benchmark may serve as a starting point for the DOD task.

\textbf{Findings \& improvements.} 
% We analyze the SOTA OVD~\cite{minderer2022simpleOWLViT,wu2023cora}, REC~\cite{wang2022ofa}, and bi-functional methods~\cite{yan2023universal,liu2023groundingdino} from multiple perspectives through experiments on \ddd{}.
% Below are the interesting findings that may offer insights for future research.
% 1) Existing REC methods obtain weak performance on this benchmark. They fail to provide good confidence scores for DOD, lacks the ability to reject negative instances, and suffers in multi-target scenarios. The essential reason is their task formulation, which performs grounding, \textit{i.e.}, matching between text and a image region, rather than detection, and does not attempt to distinguish positive and negative instances.
% 2) OVD methods outperform REC methods when directly applied to the DOD task. They show promise in handling multiple instances and rejecting negative ones. However, their performance is hindered by lengthy descriptions due to constraints in their training data.
% 3) Bi-functional methods, although superior to uni-functional ones, still face challenges similar to REC methods. They struggle to reject negative instances and handle multiple instances effectively. At times, OVD models surpass them, indicating that they have not fully benefited from both REC and OVD approaches. As a result, they are not ready to deal with DOD for now.
% 4) We propose a baseline that greatly improves the REC method it is based on, and outperforms current SOTAs. It abilities to handle multiple targets and reject negative instances are improved much by simply reconstructing training data to multi-target form, and adding a binary classification sub-task. 
% Although there is still room for improvement, this baseline serves as a starting point, and the proposed enhancements are relevant for future DOD research.
The experimental analysis for different methods on \ddd{} yields some findings for future research (see \cref{sec:experiments}):
1) Existing REC methods perform poorly, lacking confidence scores and the ability to reject negatives, and struggling with multi-target situations. This is due to their task formulation of grounding, i.e., matching between text and image region and not distinguishing positive and negatives.
2) OVD methods excel REC ones on DOD, though lengthy descriptions, which is not available in their training data, limit their performance.
3) Bi-functional methods, while superior to REC and OVD ones, share similar challenges with REC methods. Sometimes they are surpassed by OVD models, indicating they have not fully benefited from REC and OVD.
Based on these findings, we propose \textbf{a baseline OFA-DOD} that greatly improves a REC method, and outperforms current SOTAs. Its abilities to handle multiple targets and reject negative instances are improved by simple data reconstruction and an auxiliary sub-task. It is still far from a strong DOD method, but may provide some insights for research in the future.

% With the development of multi-modal learning, the gap between these object understanding tasks are narrowed rapidly with several advances: the first is the surging of open-vocabulary detection (OVD), which extends the object detection task to detect not only a pre-defined categories, but also other objects with category names; the second is some methods that handles tasks like OVD and REC together, like grounding-DINO and UNINEXT, using language expression as the bridge. These methods are trained jointly on the data of different tasks, but still handles 
% Despite these advances, there still lacks a benchmark to evaluate the effectiveness of these models.

% We propose a general setting for object understanding, namely \texttt{omni-expression detection}, together with a benchmark. In this setting, models are required to locate all instances corresponding to language description from the whole dataset. 
% This setting is more flexible and realistic compared to previous tasks. Compared with category-guided detection like OD or OVD, we retrieve objects based on a language expression with versatile length, rather than a short category name; compared with language-guided grounding like REC, we do not make the unrealistic assumption on the existance or number of instance in the image. Note that this setting is a superset of both REC and OD (OVD).
% When we limit the language expression to be a short category name, it downgrades to object detection; when we limit the images to detect for a language expression from the whole set to only the images with one and only one referred instance, it downgrades to REC.

% A benchmark is proposed for this setting. It has three characterics that worth highlighting:

% The first is \textit{unrestricted language reference}. Instances in this dataset are annotated with flexible and free-form language expressions that can be short or long, simple or complex. This is different from object detection datasets that annotate objects with category names.

% The second is \textit{complete annotation}. For any reference, the objects referred in all images are annotated, so an image can contain zero to multiple instances for a reference, like previous detection datasets.
% Instead, for REC, the objects referred by one description are only annotated on a few images. For other images without this reference, it is unknown whether the corresponding instances exist or not.
% That is to say, the annotations in REC are far from complete.

% The third is \textit{absence expression}. We notice that current datasets with description as references, like RefCOCO for REC, usually describe objects \textbf{possessing} certain features. We also annotate objects \textbf{lacking} some features. Such absence expressions take up approximately $\frac{1}{4}$ of the references defined in this dataset. This is a first for object understanding datasets.

\input{tables/dataset_highlight}

% The dataset is built by re-annotation on a previous dataset GRD, which is proposed for segmentation tasks like referring expression segmentation (RES). Our effort for extension over GRD covers the latter 2 characteristics, i.e., complete annotation across the dataset and absence expressions of objects. Besides, GRD itself only contains semantic-level annotation, and we further provide \textit{instance-level annotation} on it.

% Our dataset for omni-expression detection, build on the generalization of two different types of object understanding tasks (language-guided grounding and category-guided detection), provides a benchmark to verify the capabilities of models to discover and locate complex events. It is very challenging.

% We verify multiple state-of-the-art methods for REC, OVD or their mixture on the proposed benchmark, and propose a new method improved upon OFA. It outperforms existing methods, and can serve as a baseline for future works on language-guided detection.

% The contribution of this work includes:
% \begin{itemize}
%     \item Based on the limitation of existing object understanding tasks, we propose a more realistic and flexible setting called omni-expression detection;
%     \item We proposed a benchmark for evaluation under this setting, which has 4 characteristics that differs from previous benchmarks;
%     \Todo{rewrite this.}
%     \item We evaluate SOTA methods of different object understanding tasks on the proposed dataset, together with a improved model that outperforms them and serve as a proposed baseline.
% \end{itemize}
