\section{Related Work}

\begin{comment}
\subsection{Definitions of Toxicity}
According to the definition coined by Google's Jigsaw project, toxicity is ``rude, disrespectful, or unreasonable language that is likely to make someone leave a discussion"~\cite{Jigsaw}. Toxic behavior can range from excessive use of profanity to outright hate speech~\cite{DecisionLab}. A study from Pew Research Center listed several forms of online harassment, such as offensive name-calling, intentional efforts to embarrass someone, physical threats, stalking, sexual harassment, and harassment over a sustained period of time~\cite{PewResearch}. Other forms of toxicity include insults and mockery~\cite{Anderson}, trolling~\cite{trolling}, and cyberbullying~\cite{cyberbullying}, etc. 


\subsection{Toxicity in Online Communities}
Social media and online gaming communities are rife with online toxicity. Several factors contribute to online toxicity, including user anonymity, context collapse, and online disinhibition effect~\cite{Suler_2004}.

Different lexicons and annotated datasets have been used to study toxicity and abusive language on social media platforms \cite{waseem2016you,waseem2016hateful}. 
For instance, \cite{davidson2017automated} present a dataset of tweets containing hateful and offensive language. They leveraged a classic machine learning model using features such as TF-IDF, part-of-speech tags, and sentiment to detect hateful content. Similarly, \cite{founta2018large} leveraged Twitter to do a more fine-grained analysis of toxic language using offensive, abusive, hateful speech, aggressive, cyberbullying, spam, and normal categories. 
State-of-the-art approaches such as deep learning \cite{sarwar2022unsupervised,agrawal2018deep,arango2019hate} and transformer-based models \cite{isaksen2020using} have been used in toxicity detection. More specifically, \cite{isaksen2020using} utilized Bidirectional Encoder Representations from Transformers (BERT) to analyze tweets labeled as hateful, offensive, or normal and found pre-trained language models quite effective in predicting the hateful instances. 
Furthermore, \cite{murthy2019visualizing} used a combination of qualitative and quantitative approaches to study antagonistic racialized discourses on YouTube. \cite{obadimu2019identifying} also utilized YouTube and Perspective API to  examine toxicity in posts on pro- and anti-NATO channels. \cite{tellakat2020understanding} used Hatebase.org dictionary, a large online repository of hate speech, as well as LIWC to investigate inter- vs. intra-group toxicity on Reddit.

Recent research showed that the majority of work and models used for toxicity detection ``encode'' biases against marginalized groups \cite{waseem2016you,zhou2021challenges,hutchinson2020social}. For instance, \cite{sap-etal-2022-annotators} found strong associations between toxicity rating and identities and beliefs of human coders. Consequently, these biases are embedded in off-the-shelf models such as Perspective API, which are vastly used in this domain. \cite{chong2022understanding} compared toxicity triggers in western and eastern contexts and found that conversations coming from the former group focus on politics, money, and power while the latter ones discuss social or public issues, like school and mental health.  

In addition to social media, due to its competitive nature, online gaming community also suffer from toxic behavior of the gamers, since they use offensive language and toxic behavior as a means to release anger and disappointment. A recent study found that 74\% of US players report harassment, 65\% of which were severe\footnote{
\url{https://www.adl.org/free-to-play}}. 
Prior research in this domain is mostly focused on detecting cyberbullying \cite{kwak2015exploring} and sexist behaviors \cite{ekiciler2014bullying}.
Previous studies have conducted data annotation using pre-defined lexicon categories. \cite{martens2015toxicity} used top n-gram to manually label toxic expressions in chat communication of Multiplayer Online Games. \cite{weld2021conda} created CONDA, a dataset of conversations from the chat logs, to study in-game toxic language using a semantic-level toxicity framework. 
\cite{reid2022bad} leveraged in-game verbal communication and game metadata to detect toxic communication. Moreover, they used logistic regression and features from LIWC to detect the intensity of toxic language (high vs low toxicity). Toxic behavior analysis among teammates and opponents shows that players experience more hostility from teammates compared to their opponents \cite{shen2020viral}. Neto et al. found more positive conversations in the opponent groups compared to allies. The results also showed that positive interaction between players are vital, as respectful communication between teammates has impact on better performance overall \cite{neto2017studying}.
\end{comment}

\subsection{Study of Morality in SE}
Moral foundation theory has not been studied in the context of software engineering to the best of our knowledge as we did our literature review, but morality in terms of ethics has been studied in previous works.

Gotterbarn et. al. \cite{gotterbarnMoral1995} give a broad overview of ethics in computer technology, and present a model for software engineering ethics.
Logsdon et. al. \cite{logsdonSoftware1994} studied the relationship between moral judgment and software piracy through the analysis of 350 survey questionaries, and based on their results, they did not find a strong connection between them. Still, they explained that "software copying is perceived as an issue of low moral intensity".
\cite{borregueroFathoming2015} studied "D-Index", which is their idea of the Developers' Virtues Index, to understand the social and technical skills recipe that drives OSS communities to success. Based on their results, at least one professional in OSS communities has a D-Index above a certain threshold.
\cite{ylenProfessional2017} investigated organizational practices and virtues in software developers’ work, and how a work organization can create space for agency by supporting professional virtues. The practices that they identified that enable professional agency were democracy, experimentation, self-directed development, and independent project team.
Another work that analyzes software quality through the lens of ethics and morality is Peslak et. al. \cite{peslakImproving2004}. In this work, the author believes that poor software quality presents an ethical issue for society, and explores philosophical ethical theories, such as Aristotelian virtue ethics and Humean virtue ethics, to address the issue of poor software qualities.
Gogoll et. al. \cite{gogollEthics2021} investigate the usefulness of Codes of Conducts in handling ethical problems. They argue that these Codes of Conducts are barely able to provide normative orientation in software development and they cannot replace ethical deliberation.
% Leach et. al. \cite{leachFreedom2009} argue that the moral imagination observable in the interaction between the human imagination and technology can be understood with reference to its emergence around certain methods of technical production.
\cite{mitchellIncorporating2022} argues that there is a lack of tool and process support for ethical deliberation in software engineering, and they explored the state of ethical practices through interviews conducted with software professionals. Based on the results, while there is an awareness among practitioners, much more work needs to be done.
\cite{boochMorality2008} poses the question that "is there a moral dimension to developing software?" and "Should software architects have a professional code of ethics?". They argue that software development indeed has a moral dimension to it because our civilization runs on software. Communities have developed technology that has changed the way individuals live, businesses operate, etc.
Siponen et. al. \cite{siponenNew2012} developed a model that explains the effects of neutralization techniques on software piracy intentions. They hypothesize that moral beliefs negatively affect the intention to commit software piracy. Offenders do not refrain from piracy because of fear, but rather because they evaluate the act as morally wrong.
Huang et. al. \cite{huangLeaving2021} introduced the notation of Open Source Software for Societal Good (OSS4SG). Through semi-structured interviews, they found the motivations and challenges of OSS4SG communities. For example, OSS4SG contributors focus less on benefiting themselves by filling their resumes with tech skills and are more interested in leaving their mark on society.

\subsection{Toxicity in Open Source}
%Examples: 
Similar to other online forums, OSS communication channels are not free of toxic content. For example, Linux Kernel Mailing List is infamous for incivil communications, often involving personal attacks and disrespectful comments~\cite{incivility, heated_discussions} such as, \textit{``[Person’s name], SHUT THE F**K UP! ... your whole email was so horribly wrong, and the patch that broke things was so obviously crap. Fix your f*cking compliance tool, because it is obviously broken. And fix your approach to kernel programming"}. %Linus Torvalds, the creator of Linux, apologized for his own rude behavior after being hostile for years towards other Linux developers \footnote{https://arstechnica.com/gadgets/2018/09/linus-torvalds-apologizes-for-years-of-being-a-jerk-takes-time-off-to-learn-empathy/}. 
Toxic behaviors like this have been responsible for causing stress and burnout~\cite{Raman2020}, reduced developer motivation and productivity~\cite{Bosu2013ImpactOP}, leading to team attrition~\cite{6614728}.

%Prevention: 
Systems to manage toxic comments have been in use for several popular OSS platforms, e.g., GitHub. On GitHub, the system is primarily manual, allowing a moderator to, e.g., lock the discussion on an issue, close an issue, delete or hide comments, or block individual users from commenting. Miller et al. studied different aspects of toxicity on GitHub, including how developers react to the current moderation mechanism, noting that the current system does not always resolve the problem and that a significant amount of burden continues to be placed on the project maintainers~\cite{Miller2022}. Therefore automated interventions to detect and flag toxic conversations in OSS are necessary.

%Challenges
In spite of the presence of several state-of-the-art techniques for toxicity detection in blogs and tweets~\cite{gunasekara-nejadgholi-2018-review, bhat-etal-2021-say-yes}, applying these tools directly to the SE-related text is not effective due to several reasons~\cite{Sarker2020ABS}. For instance, tweets are shorter than code review comments and rarely contain code samples or other technical terms relevant to SE. Handling this problem often requires bespoke approaches (e.g., training data, lexicons) that are domain-specific~\cite{9793879}. %In fact, toxicity detection performance degrades significantly when trained on one communication channel and evaluated on another (e.g, trained on Github issue comments and tested on Gitter chat messages)~\cite{Sarker2020ABS}.

%Auto-Detection: 
Towards domain-specific detection of toxic content, \cite{Raman2020} proposed a machine-learning based technique to detect toxic issue comments on GitHub. Their model performed best when using only two features, the Stanford Politeness score and Google Perspective API. 
This model was found to be not generalizable across other types of developer communications, such as code reviews and chats~\cite{Sarker2020ABS}. 
Another study detected offensive language (only swearing and profanity) on Stack Overflow, GitHub, and chats by using the Perspective API and regular expressions ~\cite{Cheriyan21} as features.

While detecting toxicity in SE has only recently started getting attention, researchers have long studied other negative interactions, such as pushback~\cite{pushback}, conflict~\cite{Elliott:2003}, impoliteness~\cite{p31, p131}, anger~\cite{anger}, and other negative  emotions~\cite{Chatterjee2022DataAugmentation, Chatterjee21, tone21} in textual software artifacts.

Current approaches in toxicity detection are not generalizable in terms that they miss classifying toxic/biased words that are salient and  cultural or domain-specific (e.g., in SE context) and do not show up in off-the-shelf lexicons and datasets. Our work builds upon the previous works in this domain and investigates the change in the model's effectiveness when using additional features based on psycholinguistic scores and moral values. % We leverage the dataset curated by \cite{Raman2020} to 








