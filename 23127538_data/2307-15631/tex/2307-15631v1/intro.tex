% \noindent
% \textbf{abstract}

% \noindent
% %% Text of abstract
% Studies have shown that toxic behavior can cause contributors to leave, and hinder newcomers' (especially from underrepresented communities) participation in Open Source Software (OSS) projects. Thus, detection of toxic language plays a crucial role in OSS collaboration and inclusivity. 
% Off-the-shelf toxicity detectors are ineffective when applied to OSS communications, due to the distinct nature of toxicity observed in these channels (e.g., entitlement and arrogance are more frequently observed on GitHub than on Reddit or Twitter). 
% In this paper, we investigate a machine learning-based approach for automatic detection of toxic communications in OSS. We leverage psycholinguistic lexicons, and Moral Foundations Theory to analyze toxicity in two types of OSS communication channels; issue comments and code reviews. Our evaluation indicates that our approach can achieve a significant performance improvement (up to 7\% increase in F1 score) over the existing domain-specific toxicity detector. The results show that utilizing moral values is more effective than linguistic cues, resulting in 67.50\% F1-measure in identifying toxic instances in code review data and 64.83\% in issue comments. 


\section{Introduction}
% Structure:
% - The importance of OSS and OSS research
% - The challenges of OSS, mainly how communications can be unhealthy in online platforms
% - How morality can help us better understand communications in OSS and help us with the challenges
% - Our goal (summarize our contributions)
% summarizing take away me

%Open source software (OSS) projects have become crucial for the digital advancement of society, with over 90\% of companies leveraging open source~\cite{GithubOcto2022}. However, the challenge is that many new OSS projects fail, and longstanding ones are abandoned~\cite{coelho2017why}. OSS projects often rely on volunteers, making it important to protect and encourage newcomer contributions while also ensuring that established contributors remain engaged~\cite{7476635}. Research shows that toxic interactions and uncivil language serve as a barrier to contribution, leading to negative emotions and isolation, which can result in contributors abandoning projects~\cite{Igor15, qiu19, 9402044}. Therefore, to foster a strong and growing open-source ecosystem, it is essential to mitigate toxic conversations and promote a positive and inclusive community~\cite{zanetti2013riseandfall}.  

Open source software (OSS) projects have become crucial for the digital advancement of society, with over 90\% of companies leveraging open source~\cite{GithubOcto2022}. However, many new and longstanding OSS projects fail, in part due to toxic interactions and uncivil language, which serve as a barrier to contribution and lead to negative emotions and isolation~\cite{Igor15, qiu19, 9402044}. %To foster a strong and growing open-source ecosystem, it is essential to mitigate toxic conversations and promote a positive and inclusive community~\cite{zanetti2013riseandfall}.
Thus, understanding toxicity in OSS has gained increasing attention in recent years~\cite{ cheriyanTowards2021, Sarker2020ABS, toxicr, ferreiraIncivility2022, ferreiraHeat2022, egelmanPredicting2020}. 
Uncivil features such as name-calling, frustration, and impatience were found in more than half of non-technical emails in the Linux Kernel mailing list~\cite{ferreiraSTFU2021}, swearing and personal offense were identified as the main reasons for offense in several communities~\cite{cheriyanTowards2021}, and GitHub issue threads contain various types of toxic behaviors~\cite{Miller2022}. 
Although researchers outside the Software Engineering (SE) domain have designed several techniques to automatically detect toxicity in online forums~\cite{shethDefining2021, maityOpinion2018, pascualFerratoxicity2021}, directly applying those tools to SE is ineffective due to several factors, including the unique nature of SE text~\cite{Raman2020, Sarker2020ABS, Chatterjee2022DataAugmentation}. 
%For instance, those tools can incorrectly identify commonly-used neutral words in SE (e.g., `kill' refers to terminating a program) as toxic. 
More recently, machine learning-based tools have shown promise in detecting toxicity when trained on domain-specific data~\cite{toxicr}, however it remains crucial to address the fundamental issue of their design lacking an understanding of the unique nature and triggers of toxicity in OSS~\cite{Miller2022,9793879}.

%More than half of non-technical emails in the Linux Kernel mailing list include uncivil features such as name-calling, frustration, and impatience~\cite{ferreiraSTFU2021}. Swearing and personal offense were identified as the main reasons for offense in GitHub, Gitter, Stack Overflow, and Slack communities~\cite{cheriyanTowards2021}. Toxicity in GitHub issue threads consist of insulting, entitled, unprofessional, arrogant, and trolling behaviors~\cite{Miller2022}.


%by creating and maintaining key projects while also promoting a positive and inclusive community~\cite{zanetti2013riseandfall}.
%Open Source Software (OSS) projects are societal goods that have increased the speed of digital advancement. OSS has inserted itself into our technological infrastructure's fabric. Based on GitHub's Octoverse report in 2022, GitHub alone had 413 million open source contributions, and overall, more than 90\% of companies leverage open source \cite{GithubOcto2022}. Therefore, it is in society's interest to foster a strong and growing OSS ecosystem by ensuring key projects are maintained and new projects are successfully created. However, most of the new OSS projects fail, and many longstanding projects are abandoned by developers, even though they may still support key societal missions~\cite{coelho2017why}. The challenge is that OSS projects often rely on the work of volunteers who contribute their time in order to improve their own software development skills and to be part of a community~\cite{7476635}.

%Previous research has shown that uncivil language and toxic interaction serve as a barrier to newcomer contribution in open source software projects~\cite{Igor15, qiu19}. Therefore, to ensure OSS projects are successful, it is important to protect and encourage the participation of contributors, both newcomers as well as those that have established a track record of contributions to a project~\cite{zanetti2013riseandfall}. A common reason contributors abandon projects is because of social and emotional factors~\cite{9402044}, e.g., because they experience negative emotions about the project's direction, have a negative experience with other participants through a toxic conversation, or feel isolated and ignored by other participants.


%Researchers have conducted studies to examine the prevalence and characteristics of uncivil language in these communities, with some finding that frustration, name-calling, and impatience are among the most common forms of toxic behavior~\cite{ferreiraSTFU2021}. Other research has categorized toxicity into various types of behavior such as insulting, entitled, unprofessional, arrogant, and trolling~\cite{Miller2022}. 
%Toxicity detection tools trained on other domains (e.g., Google Perspective API) may not be effective when directly applied to software engineering (SE) corpora due to the unique nature of SE text, which often includes technical jargon, code snippets, and discussions on complex software architecture.

%. However,  to improve the tools we need to look beyond explicit signals.


%\textcolor{red}{Recent studies have tried to characterize and understand the toxicity observed in OSS and Software Engineering (SE) communities. Ferrira et al.  \cite{ferreiraSTFU2021} found that more than half of the non-technical emails in the Linux Kernel Mailing List include uncivil features, with frustration, name-calling, and impatience being the most frequent ones. Cheriayn et al. \cite{cheriyanTowards2021} found that swearing in GitHub and Gitter, and personal offense in Stack Overflow and Slack are the main reasons for offense in these communities. By analyzing GitHub issue threads, Milller et al.\cite{Miller2022} categorized toxicity into Insulting, Entitled, Unprofessional, Arrogant, and Trolling comments, while analyzing the targets, triggers, and authors of toxic comments.} 

%\textcolor{red}{Towards preventing online abuse and toxicity, researchers have designed tools and techniques to detect and mitigate toxicity in various fields ~\cite{cyberbullying, offensive}. However, automatically detecting toxic content in OSS is a challenging task~\cite{ferreiraIncivility2022}. %In previous studies, Ferrira et. al. \cite{ferreiraIncivility2022} found that for detecting incivility in SE texts, BERT performs better than the classical machine learning classifiers, but still is not the optimal solution. In addition,  Directly applying toxicity detection tools trained on other domains (e.g., Google Perspective API) to software engineering (SE) corpora is ineffective due to the distinct nature of SE text~\cite{Raman2020, Sarker2020ABS}. These tools can incorrectly identify commonly-used words in the SE context (e.g., `bug' means software defect, `kill' refers to terminating a program) as toxic, and vice-versa. To solve this issue, using and training SE domain-specific tools and techniques have shown promising results in toxicity detection in tools such as ToxiCR \cite{toxicr}.}  %In the development of ToxiCR, only two steps (out of eight) of the preprocessing steps are SE-specific. Therefore, there is always room for improvement in making these tools more domain-adapted, and more accurate.

Language is an indispensable tool to convey information between people, and analyzing it could play a significant role in understanding the challenges faced in OSS communities ~\cite{saneiImpacts2021, ferreiraSentiment2019, Sajadi2023}. Linguistic styles and word choices can help in analyzing and understanding people's thoughts, opinions, and feelings~\cite{triandis1989self,boyd2022development,rezapour2019enhancing, Chatterjee21}. 
People's values and personal norms affect their (spontaneous) attitude, decision-making process, and what they perceive as good or bad, and moral or immoral \cite{haidt2001emotional,rezapour2021incorporating}. Previous research in psychology has addressed the connection between moral principles and hate and showed that morality is a key feature of hatred; hate is connected to core moral beliefs and higher levels of moral emotions (e.g., contempt, anger, disgust)~\cite{morality_hate_2018,morality_hate_2021}. 
Considering the importance of effective communication, we believe that analyzing moral values as exhibited in OSS texts can provide valuable insights into the underlying beliefs and values that influence communication styles in this community, especially in spreading toxicity. 
%Moral Foundation Theory is our approach to understanding SE communications better. 

In this paper, we aim to investigate the moral principles exhibited in GitHub and explore their relationship with toxicity. %explore moral principles as exhibited in the SE context to get a better understanding of moral values exhibited in OSS communications, and to analyze their co-occurrence with toxicity. 
To that end, we leverage Moral Foundation Theory (MFT), a social psychological theory that assumes individual judgments are influenced by emotional and cognitive appraisals, referred to as intuitions or foundations~\cite{graham2013moral,haidt2004intuitive}. 
%MFT categorizes people's moral reactions and behavior into five foundations or principles which are each further characterized by two opposing values as virtues and vices, such as Care/Harm\cite{graham2013moral,haidt2004intuitive}. 
%Fairness/Cheating, Authority/Subversion, Loyalty/Betrayal, and Purity/Degradation . 
We explore a dataset of 100 toxic issue threads labeled with their representative natures (e.g., `Insulting', `Arrogant')~\cite{Miller2022}, to analyze moral principles and how they are exhibited in the context of OSS project communications. We map the observed moralities in each thread to their corresponding natures of toxicity to understand the relationship and co-occurrence. 
Our findings reveal that toxic issue threads exhibit moral principles, with each moral principle being associated with at least one type of toxicity. This suggests that toxic comments are associated with moral principles, and that the Moral Foundations Theory (MFT) may be useful in detecting toxicity. 
To apply MFT to the domain of SE, it would be necessary to adapt it, and we believe that doing so has the potential to enhance our understanding and detection of toxicity in SE communications.
%Based on our results, moral principles are exhibited in toxic issue threads, and we can identify moral principles in toxic SE communications. Every moral principle is associated with at least one type of toxicity, suggesting that toxic comments are linked to moral principles, and MFT has the potential to be used in toxicity detection. By adapting the Moral Foundations Theory to the domain of SE, we believe there is a potential to apply this theory to better understand and detect toxicity in SE communications.
% XX YY



%However, the role of moral values in the textual analysis of communications in OSS is still unexplored.



\begin{comment}
Open Source Software (OSS) projects are a societal good. 
OSS projects have increased the speed of digital advancement, and therefore a technology that relies on OSS is pervasive, e.g., OSS underpins nuclear submarines, industrial marketplaces, and mobile phones. 
It is in society's interest to foster a strong and growing OSS ecosystem by ensuring key projects are maintained and new projects are successfully created.
However, most new OSS projects fail and many longstanding projects are abandoned by developers, even though they may still be supporting key societal missions~\cite{coelho2017why}.
The challenge is that OSS projects often rely on the work of volunteers who contribute their time in order to improve their own software development skills and to be part of a community~\cite{7476635}. %In fact, McDonald and Goggins note that the growth and participation of these community volunteers are the primary measure of OSS project health~\cite{mcdonald2013performance}. 

Previous research has shown that uncivil language serves as a barrier to newcomer contribution in open source software projects~\cite{Igor15, qiu19}. Therefore, to ensure OSS projects are successful, it is important to protect and encourage the participation of contributors, both newcomers as well as those that have established a track record of contributions to a project~\cite{zanetti2013riseandfall}. A common reason contributors abandon projects is because of social and emotional factors~\cite{9402044}, e.g., because they experience negative emotions about the direction of the project, have a negative experience with other participants through a toxic conversation (as shown in Figure \ref{OSSToxicExample}), or feel isolated and ignored by other participants. Certain OSS projects publish codes of conduct to describe acceptable behavior by participants. However, monitoring adherence to these policies remains a challenge~\cite{tourani2018code}. Toxicity also jeopardizes diversity and inclusion, since it can particularly affect the participation of women and other underrepresented communities \cite{oss_women, oss_diversity}. 
If project maintainers and participants were able to proactively detect and prevent toxic communications in OSS projects, it would lead to more inclusive and sustainable OSS.

% Figure environment removed

There are several challenges in automatically detecting toxic content in OSS. Directly applying toxicity detection tools trained on other domains (e.g., Google Perspective API) to software engineering (SE) corpora is ineffective due to the distinct nature of SE text~\cite{Raman2020, Sarker2020ABS}. These tools can incorrectly identify commonly-used words in the SE context (e.g., `bug' means software defect, `kill' refers to terminating a program) as toxic and vice-versa. 
%Handling this problem often requires bespoke approaches (e.g., training data, lexicons) that are domain-specific. In fact, toxicity detection performance degrades significantly when trained on one communication channel and evaluated on another (e.g, trained on Github issue comments and tested on Gitter chat messages)~\cite{Sarker2020ABS}.
%Studies using general-purpose toxicity detectors in SE text have shown relatively poor results~\cite{Raman2020, Sarker2020ABS}.
%To that end, researchers have designed machine learning-based techniques for automatically detecting toxicity in open source discussions~\cite{Raman2020, Sarker2020ABS}. 
%WHY IS THIS PROBLEM DIFFICULT? Talk about the differences in nuances of language. 
Additionally, toxicity is exhibited differently in OSS communications than in other online forums. A recent qualitative study~\cite{Miller2022} found that, in contrast to Twitter or Reddit where toxicity is often accompanied by hate speech and offensive cursing, toxicity on GitHub tends to use milder language. Thus the nature of toxicity exhibited in OSS is more nuanced. Common examples of toxicity in open source (e.g., GitHub issue comments) include insults resulting from technical disagreements and entitled, demanding, and arrogant comments from project users. 
%These new findings provide us the opportunity to build effective OSS toxicity detectors by leveraging the linguistic cues. 
However, existing automatic toxicity detectors have not yet been effective in identifying ``covert toxicity''~\cite{Lees2021CapturingCT} such as sarcasm, cynicism, insult, and entitlement in SE communications~\cite{9793879}. 

Towards preventing online abuse and toxicity, researchers in the Natural Language Processing (NLP) community have designed techniques to automatically identify cyber-bullying~\cite{cyberbullying}, offensive language~\cite{offensive}, and hate speech~\cite{Vigna2017HateMH, Chaudhary2021CounteringOH, Silva2016AnalyzingTT}.
For instance, (psycho)linguistic markers using dictionary categories from Linguistic Inquiry and Word Count (LIWC) \cite{LIWC} were found helpful in the analysis of toxic language in various communities~\cite{an2021predicting,mathur2018did,silva2020data,salminen2020developing,jahan2020team,reid2022bad}. As shown in previous studies, linguistic styles and word choices can help in analyzing and understanding people's thoughts, opinions, and feelings~\cite{triandis1989self,boyd2022development,rezapour2019enhancing}. %seraj2021language,

Moreover, Moral Foundations Theory (MFT) assumes that individual judgements are influenced by emotional and cognitive appraisals, referred to as intuitions or foundations \cite{graham2013moral,haidt2004intuitive}. People's values and personal norms affect their (spontaneous) attitude, decision-making process, and what they perceive as good or bad, and moral or immoral \cite{haidt2001emotional,rezapour2021incorporating}. 
MFT categorizes people's moral reactions and behavior into five foundations or principles which are each further characterized by two opposing values as virtues (good) and vices (bad); care/harm, fairness/cheating, authority/subversion, loyalty/betrayal, and purity/degradation \cite{graham2013moral,haidt2004intuitive}. 
Previous research in psychology has addressed the connection between moral foundations and hate and showed that morality is a key feature of hatred; hate is connected to core moral beliefs and higher levels of moral emotions (e.g., contempt, anger, and disgust)~\cite{morality_hate_2018,morality_hate_2021}. 
However, %to the best of our knowledge, 
the role of moral values in %spreading 
the analysis of toxicity in OSS is still underexplored. 

In this paper, we leverage psycholinguistic lexicons (LIWC~\cite{boyd2022development}) and Moral Foundation Theory (MFT~\cite{graham2013moral,haidt2004intuitive}) to analyze toxicity in OSS communication channels. To ensure generalizability, we detect and quantify expressions of moral values as well as psychological markers in two types of software-related artifacts: (a) GitHub issue comments~[dataset by \cite{Raman2020}], and (b) Gerrit code review comments [extended dataset by \cite{Sarker2020ABS}], to investigate the following research questions:

\begin{itemize}
 %\item \textbf{RQ1: What additional insights can we gain when  psycholinguistic cues are added to the baseline of toxicity detection models in OSS?}
 \item \textbf{RQ1: How does the performance of toxicity models change when psycholinguistic cues are added as features?}
 We found that using psycholinguistic features, we can achieve a slight improvement (by $\sim2\%$ in $F1_1$) in performance from baseline in identifying toxic instances across all classifiers.
% Results from LIWC. â€” Limitations of the current tools (e.g., sarcasm or cynicism detection)
 \item \textbf{RQ2: How does the performance of toxicity models change when moral values are added as features?}
 Adding morality on top of psycholinguistic features resulted in a significant jump in the performance of all classifiers ($\sim 2-7\%$ in $F1_1$).
 \item \textbf{RQ3: What types of SE texts are difficult to automatically detect as toxic using our techniques?}
 We conduct qualitative error analysis to answer this question. Our observations provide several insights and potential areas of improvements to support future work in toxicity detection, e.g., using domain-specific dictionary, understanding the context of the discussion, etc. 
 
\end{itemize}

%We report precision, recall, F-measure, AUC, and MCC and conduct qualitative error analysis. Our results indicate that post hoc quality conversations can be identified with precision of 0.XX, recall of 0.XX, F-measure of 0.XX, ROC of XX, and MCC of 0.XX.

\end{comment}
