\section{Introduction}
% Figure environment removed
Recently entity alignment (EA) has attracted wide attention as a crucial task for aggregating knowledge graphs (KGs) from diverse data sources.
Multi-modal information, particularly visual images, serves as a vital supplement for entities. 
% We notice that existing work in multi-modal entity alignment (MMEA) primarily focuses on fusing multi-modal entity features for intra-modal or inter-modal alignment while neglecting the universal phenomenon of missing visual modalities.
However, achieving visual modality completeness always proves challenging for automatically constructed KGs both on the Internet and domain-specific KGs.
 For instance, in the DBP15K datasets \cite{DBLP:conf/semweb/SunHL17} for EA, only a portion of the entities have attached images (e.g., $67.58\%$ in DBP15K$_{JA\text{-}EN}$ \cite{DBLP:conf/aaai/0001CRC21}). This incompleteness is inherent to the DBpedia KG \cite{DBLP:journals/semweb/LehmannIJJKMHMK15}, as not every entity possesses an associated image. 
Furthermore, the intrinsic ambiguity of visual images also impacts the alignment quality.
% since entities can have various visual representations.
As illustrated  in Figure \ref{fig:Introcase}, the movie \emph{THOR} can be represented by a snapshot of the movie (star) poster or an image of the movie title itself. While individuals familiar with the Marvel universe can effortlessly  associate these patterns, machines struggle to discern significant visual feature association without the aid of external technologies like OCR and linking knowledge bases \cite{DBLP:conf/semweb/0007CGPYC21}, posing challenges for alignment tasks.
This phenomenon primarily arises from the abstraction of single-modal content, e.g., country-related images could be either national flags, landmarks or maps.

In this paper, we deliver an in-depth analysis of potential missing visual modality for MMEA.
To achieve this, we propose the {MMEA-UMVM} dataset, which contains seven separate datasets with a total of 97 splits, each with distinct degrees of visual modality incompleteness, and benchmark several latest MMEA models. 
 To ensure a comprehensive comparison, our dataset encompasses bilingual, monolingual, as well as normal and high-degree KG variations, with standard (non-iterative) and iterative training paradigms to evaluate the model performance.  
The robustness of the models against ambiguous images is discussed by comparing their performance under complete visual modality.

In our analysis, we identify two critical phenomena: {{(\rmnum{1})}} Models may succumb to overfitting noise during training, thereby affecting overall performance. {{(\rmnum{2})}} Models exhibit performance oscillations or even declines at high  missing modality rates, indicating that sometimes the additional multi-modal data negatively impacts EA and leads to even worse results than when no visual modality information is used.
These findings provide new insights for further exploration in this field. 
Building upon these observations, we propose our model UMAEA,  which alleviates those shortcomings of other models via introducing multi-scale modality hybrid and circularly missing modality imagination.
Experiments prove that our model can consistently achieve SOTA results across all benchmark splits with limited parameters and runtime, which supports our perspectives.
