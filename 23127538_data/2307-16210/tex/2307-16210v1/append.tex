\clearpage

\appendix
\section{Appendix}
\begin{table}[!htbp]
    \centering
    \vspace{-0.1cm}
    % \footnotesize
    \caption{Statistics for original datasets, where ``EA pairs'' refers to the pre-aligned entity pairs. Note that not all entities have the associated images or the equivalent counterparts in the other KG. For dataset \{ EN-FR-15K, EN-DE-15K, D-W-15K-V1, and D-W-15K-V2 \} in Multi-OpenEA, we omit the ``15K'' suffix to unify the description throughout this paper.}
    \label{tab:dataset}
    \vspace{-3pt}
    \renewcommand\arraystretch{1.0}
    \resizebox{1.\linewidth}{!}{
    \begin{tabular}{@{}l|c|cccccccc@{}}
        \toprule
        \makebox[2.5cm][c]{Dataset} & KG & \# Ent. & \# Rel. & \# Attr. & \# Rel. Triples & \# Attr. Triples & \# Image & \# EA pairs \\
        \midrule
        \multirow{2}*{DBP15K$_{ZH\text{-}EN}$} & ZH {\footnotesize (Chinese)} & 19,388 & 1,701 & 8,111 & 70,414 & 248,035 & 15,912 & \multirow{2}*{15,000} \\
        & EN {\footnotesize (English)} & 19,572 & 1,323 & 7,173 & 95,142 & 343,218 & 14,125 \\
        \midrule
        \multirow{2}*{DBP15K$_{JA\text{-}EN}$} & JA {\footnotesize (Japanese)} & 19,814 & 1,299 & 5,882 & 77,214 & 248,991 & 12,739 & \multirow{2}*{15,000} \\
        & EN {\footnotesize (English)} & 19,780 & 1,153 & 6,066 & 93,484 & 320,616 & 13,741 \\
        \midrule
        \multirow{2}*{DBP15K$_{FR\text{-}EN}$} & FR {\footnotesize (French)} & 19,661 & 903 & 4,547 & 105,998 & 273,825 & 14,174 & \multirow{2}*{15,000} \\
        & EN {\footnotesize (English)} & 19,993 & 1,208 & 6,422 & 115,722 & 351,094 & 13,858 \\
        \midrule
        \multirow{2}*{OpenEA$_{EN\text{-}FR}$} & EN {\footnotesize (English)} & 15,000 & 267 & 308 & 47,334 & 73,121 & 15,000 & \multirow{2}*{15,000} \\
        & FR {\footnotesize (French)} & 15,000 & 210 & 404 & 40,864 & 67,167 & 15,000 \\
        \midrule
        \multirow{2}*{OpenEA$_{EN\text{-}DE}$} & EN {\footnotesize (English)} & 15,000 & 215 & 286 & 47,676 & 83,755 & 15,000 & \multirow{2}*{15,000} \\
        & DE (German) & 15,000 & 131 & 194 & 50,419 & 156,150 & 15,000 \\
                \midrule
        \multirow{2}*{OpenEA$_{D\text{-}W\text{-}V1}$} & DBpedia & 15,000 & 248 & 342 & 38,265 & 68,258 & 15,000 & \multirow{2}*{15,000} \\
        & Wikidata & 15,000 & 169 & 649 & 42,746 & 138,246 & 15,000 \\
        \midrule
        \multirow{2}*{OpenEA$_{D\text{-}W\text{-}V2}$} & DBpedia & 15,000 & 167 & 175 & 73,983 & 66,813 & 15,000 & \multirow{2}*{15,000} \\
        & Wikidata & 15,000 & 121 & 457 & 83,365 & 175,686 & 15,000 \\
        \bottomrule
    \end{tabular}
    }
    \vspace{-0.6cm}
\end{table}

\begin{table}[!htbp]
  \centering
\centering
%\footnotesize
{
\vspace{-2pt}
\caption{The proportion $R_{img}$ of entities containing images for each dataset in our setting, with ``\texttt{STD}'' refers to the standard $R_{img}$ in raw datasets.
}
\label{tab:split}
}
\vspace{-2pt}
\resizebox{1.0\linewidth}{!}{
\begin{tabular}{@{}l|l}
\hline
 & \\ [-2ex]
\makebox[2.5cm][c]{Dataset} & \makebox[14cm][c]{$R_{img}$} \\
 & \\ [-2ex]
\hline
DBP15K$_{ZH\text{-}EN}$
&\texttt{\small ~0.05, 0.1, 0.15, 0.2, 0.3, 0.4, 0.45, 0.5, 0.55, 0.6, 0.7, 0.75, 0.7829~(STD)}\\
& \\ [-2ex]
\hline
 & \\ [-2ex]
DBP15K$_{JA\text{-}EN}$
&\texttt{\small ~0.05, 0.1, 0.15, 0.2, 0.3, 0.4, 0.45, 0.5, 0.55, 0.6, 0.7, 0.7032~(STD)}\\
& \\ [-2ex]
\hline
 & \\ [-2ex]
DBP15K$_{FR\text{-}EN}$
&\texttt{\small ~0.05, 0.1, 0.15, 0.2, 0.3, 0.4, 0.45, 0.5, 0.55, 0.6, 0.6758  (STD) }\\
& \\ [-2ex]
\hline
 & \\ [-2ex]
OpenEA$_{EN\text{-}FR}$
&\texttt{\small ~0.05, 0.1, 0.15, 0.2, 0.3, 0.4, 0.45, 0.5, 0.55, 0.6, 0.7, 0.8, 0.9, 0.95, 1.0~(STD)}\\ 
& \\ [-2ex]
\hline
 & \\ [-2ex]
OpenEA$_{EN\text{-}DE}$
&\texttt{\small ~0.05, 0.1, 0.15, 0.2, 0.3, 0.4, 0.45, 0.5, 0.55, 0.6, 0.7, 0.8, 0.9, 0.95, 1.0~(STD)}\\ 
& \\ [-2ex]
\hline
 & \\ [-2ex]
OpenEA$_{D\text{-}W\text{-}V1}$
&\texttt{\small ~0.05, 0.1, 0.15, 0.2, 0.3, 0.4, 0.45, 0.5, 0.55, 0.6, 0.7, 0.8, 0.9, 0.95, 1.0~(STD)}\\ 
& \\ [-2ex]
\hline
 & \\ [-2ex]
OpenEA$_{D\text{-}W\text{-}V2}$
&\texttt{\small ~0.05, 0.1, 0.15, 0.2, 0.3, 0.4, 0.45, 0.5, 0.55, 0.6, 0.7, 0.8, 0.9, 0.95, 1.0~(STD)}\\ 
[-2ex] \\
\hline
\end{tabular}}
\vspace{-0.6cm}
\end{table}

\subsection{Dataset Statistics}
Our detailed dataset statistics are presented in Table \ref{tab:dataset}.
A set of pre-aligned entity pairs is offered for guidance,  
which is proportionally split into a training set (seed alignments $\mathcal{S}$) and a testing set $\mathcal{S}_{te}$ based on the given seed alignment ratio ($R_{sa}$).
Notably,   each entity in the four Multi-OpenEA benchmark \cite{DBLP:journals/corr/abs-2302-08774} is initially associated with three images obtained from the Google search engine. In this study, we select the highest-ranked image, which is the first one, to serve as the visual information for the entity.
 The details for 97 data splits are contained in Table \ref{tab:split}, and the complete data for benchmark is accessible at {\color{blue}\url{https://github.com/zjukg/UMAEA}}.

% \vspace{-0.4cm}
\subsection{Supplementary for Experiments}
% Figure environment removed
\subsubsection{{Experiment Settings.}}
Those attribute triples $<$\textit{entity}, \textit{attribute}, \textit{value}$>$ in KGs  have been researched in many previous EA works \cite{DBLP:conf/aaai/TrisedyaQZ19,DBLP:conf/emnlp/LiuCPLC20,DBLP:conf/ijcai/Tang0C00L20,DBLP:conf/kdd/ChenL00WYC22,DBLP:conf/icde/ZhongZFD22}.
Nevertheless, in order to focus on our key subject,
we do not utilize the contents of \textit{value} parts in this work which are mainly string formats like specific date, land area or coordinate position.
Furthermore, in order to concentrate on uncertainly missing visual modality, we exclude surface-related information such as the name of entity, relation, and attribute. Our approach primarily utilizes information derived from the type of entity and relationship, as well as structure of the graph and the image data, which is inherited from previous works \cite{DBLP:conf/aaai/0001CRC21,DBLP:conf/kdd/ChenL00WYC22,DBLP:conf/coling/LinZWSW022}. Each entity is associated with multiple attributes and either $0$ or $1$ image. We achieve this association through id/index sharing, following previous works \cite{DBLP:conf/kdd/ChenL00WYC22,DBLP:journals/corr/abs-2302-08774,DBLP:conf/coling/LinZWSW022,DBLP:conf/aaai/0001CRC21}, rather than explicitly defining triples. For example, Wang et al. \cite{DBLP:journals/bdr/WangWQZ20} incorporate images as entities through the introduction of a specific \textit{Imageof} relation, allowing for a more formal structure and organization of the KG.

 Regarding the loss trade-off for multi-task learning, we attempted to use the Automatic Weighted Loss (AWL) technique \cite{DBLP:conf/cvpr/KendallGC18} to dynamically assign weights to different training objectives. However, we found that directly summing the losses after scaling resulted in similar performance ($\pm$ 0.3$\%$ in hit$@$1) compared to using AWL. Hence, we omitted this empirical study in the paper.

Regarding $R_{img}^2$ for MCLEA, as mentioned before, the adverse effect gradually recovers and gains benefits as $R_{img}$ rises to a certain level $R_{img}^2$. Here, $R_{img}^2$ represents the minimum observed $R_{img}$ at which the model's performance surpasses that without visual information ($R_{img}$=0). For MCLEA, we calculate as follows:: 
$[0.7(\text{ZH-EN})+0.7(\text{FR-EN})+0.6(\text{JA-EN})+0.55(\text{D-W-v1})+0.7(\text{D-W-v2})+0.6(\text{EN-DE})+0.6(\text{EN-FR})]/7\times100\%=63.6\%$


\vspace{-0.3cm}
\subsubsection{{Additional Experiments.}}
In this section, we provide the remaining benchmark results. 
As a supplement to Figure 5, we offer a performance comparison of models for DBP15K$_{JA-EN}$ and DBP15K$_{FR-EN}$ under different testing sets, as shown in Figure \ref{fig:appcase}, which is consistent to DBP15K$_{ZH-EN}$.

Table \ref{tab:appdbp} and Table \ref{tab:appoea} present the model performance when they are applied to typical EA tasks excluding the influence of visual modality, which obviates the need for the CMMI module during training. 
The results show that our model achieved superior performance in non-multimodal EA tasks, indicating that UMAEA can even effectively mitigate the impact of information imbalance issues arising from attribute, relation, and graph structure during model training. 
Furthermore, we provide the performance curves under the Hit$@$1 metric, as illustrated in Figure \ref{fig:appline}, where the general trend in performance change closely resembles that observed under the MRR metric (Figure \ref{fig:line}).

\begin{table}[!htbp]
    \centering
%    \footnotesize
	\tabcolsep=0.3cm
	\vspace{-0.2cm}
    \renewcommand\arraystretch{1.0}
    \caption{Non-iterative (Non-iter.) and iterative (Iter.) results on three standard DPB15K \cite{DBLP:conf/semweb/SunHL17} datasets with $R_{sa}=0.3$ without the visual modality ($R_{img}=0$). 
    }
    % \vspace{-1pt}
    \resizebox{0.84\linewidth}{!}{
    \begin{tabular}{@{}l|l|ccc|ccc|ccc}
        \toprule
        & \multirow{2}*{\makebox[2cm][c]{Models}} & \multicolumn{3}{c|}{DBP15K$_{ZH-EN}$} & \multicolumn{3}{c|}{DBP15K$_{JA-EN}$} & \multicolumn{3}{c}{DBP15K$_{FR-EN}$} \\
        & & {\scriptsize H@1} & {\scriptsize H@10} & {\scriptsize MRR} & {\scriptsize H@1} & {\scriptsize H@10} & {\scriptsize MRR} & {\scriptsize H@1} & {\scriptsize H@10} & {\scriptsize MRR} \\
        \midrule
        \parbox[t]{2mm}{\multirow{5}{*}{\rotatebox[origin=c]{90}{Non-iter.}}} 
        & MSNEA {\footnotesize {\cite{DBLP:conf/kdd/ChenL00WYC22}}} & .503 & .795 & .602 & .395 & .715 & .504 & .472 & .820 & .593 \\
        & EVA {\footnotesize \cite{DBLP:conf/aaai/0001CRC21}} &
        {.629} & {.882} & {.719} & {.627} & {.879} & {.714} & {.626} & {.896} & {.722} \\
        & MCLEA {\footnotesize {\cite{DBLP:conf/coling/LinZWSW022}}}  &
        {.672} & {.907} & {.756} & {.663} & {.904} & {.751} & {.679} & {.923} & {.769} \\
         & MEAformer {\footnotesize {\cite{chen2023meaformer}}}  &
        \underline{.708} & \underline{.925} & \underline{.787} & \underline{.699} & \underline{.934} & \underline{.785} & \underline{.722} & \underline{.947} & \underline{.805} \\
        & \CC\textbf{UMAEA}  &
        \CC\textbf{.718} & \CC\textbf{.930} & \CC\textbf{.797} & \CC\textbf{.723} & \CC\textbf{.941} & \CC\textbf{.803} & \CC\textbf{.748} & \CC\textbf{.956} & \CC\textbf{.826} \\
        \midrule
        \parbox[t]{2mm}{\multirow{5}{*}{\rotatebox[origin=c]{90}{Iter.}}} 
        & MSNEA {\footnotesize {\cite{DBLP:conf/kdd/ChenL00WYC22}}} & .545 & .850 & .648 & .451 & .788 & .567 & .531 & .872 & .648 \\
        & EVA {\footnotesize \cite{DBLP:conf/aaai/0001CRC21}} &
        {.696} & {.907} & {.774} & {.695} & {.908} & {.772} & {.708} & {.930} & {.790} \\
        & MCLEA {\footnotesize {\cite{DBLP:conf/coling/LinZWSW022}}}  &
        {.749} & {.933} & {.817} & {.752} & {.935} & {.821} & {.779} & {.955} & {.847} \\
        & MEAformer {\footnotesize {\cite{chen2023meaformer}}}  &
        \underline{.775} & \underline{.940} & \underline{.837} & \underline{.761} & \underline{.950} & \underline{.831} & \underline{.785} & \underline{.963} & \underline{.852} \\
        & \CC\textbf{UMAEA}  &
        \CC\textbf{.793} & \CC\textbf{.952} & \CC\textbf{.852} & \CC\textbf{.794} & \CC\textbf{.960} & \CC\textbf{.857} & \CC\textbf{.820} & \CC\textbf{.976} & \CC\textbf{.880} \\
        \bottomrule
    \end{tabular}
    }
    \label{tab:appdbp}
	\vspace{-0.3cm}
\end{table}

\begin{table}[!htbp]
    \centering
%    \footnotesize
	\tabcolsep=0.3cm
    \renewcommand\arraystretch{1.0}
%    \vspace{-0.2cm}
    \caption{Non-iterative (Non-iter.) and iterative (Iter.) results on four standard OpenEA \cite{DBLP:journals/pvldb/SunZHWCAL20} datasets  with $R_{sa}=0.2$ without the visual modality ($R_{img}=0$).  
    }
    \vspace{-1pt}
    \resizebox{0.98\linewidth}{!}{
    \begin{tabular}{@{}l|l|ccc|ccc|ccc|ccc}
        \toprule
        & \multirow{2}*{\makebox[2cm][c]{Models}} & \multicolumn{3}{c|}{OpenEA$_{EN-FR}$} & \multicolumn{3}{c|}{OpenEA$_{EN-DE}$} & \multicolumn{3}{c|}{OpenEA$_{D-W-V1}$} & \multicolumn{3}{c}{OpenEA$_{D-W-V2}$} \\
        & & {\scriptsize H@1} & {\scriptsize H@10} & {\scriptsize MRR} & {\scriptsize H@1} & {\scriptsize H@10} & {\scriptsize MRR} & {\scriptsize H@1} & {\scriptsize H@10} & {\scriptsize MRR} & {\scriptsize H@1} & {\scriptsize H@10} & {\scriptsize MRR} \\
        \midrule
        \parbox[t]{2mm}{\multirow{5}{*}{\rotatebox[origin=c]{90}{Non-iter.}}} 
        & MSNEA {\footnotesize {\cite{DBLP:conf/kdd/ChenL00WYC22}}} 
        & .260 & .506 & .341 & .334 & .572 & .413 & .332 & .545 & .404 & .612 & .840 & .689 \\
        & EVA {\footnotesize \cite{DBLP:conf/aaai/0001CRC21}} 
        & {.525} & {.827} & {.631} & {.721} & {.918} & {.790} & {.579} & {.809} & {.662} & .775 & .952 & .839 \\
        & MCLEA {\footnotesize {\cite{DBLP:conf/coling/LinZWSW022}}}  
        & {.571} & {.862} & {.675} & {.737} & {.921} & {.803} & {.620} & {.848} & {.704} & {.816} & {.972} & {.874} \\
        & MEAformer {\footnotesize {\cite{chen2023meaformer}}}  
        & \underline{.604} & \underline{.895} & \underline{.708} & \underline{.754} & \underline{.937} & \underline{.818} & \underline{.645} & \underline{.878} & \underline{.729} & \underline{.839} & \underline{.982} & \underline{.892} \\
        & \CC\textbf{UMAEA}  
        & \CC\textbf{.608} & \CC\textbf{.897} & \CC\textbf{.711} & \CC\textbf{.763} & \CC\textbf{.942} & \CC\textbf{.826} & \CC\textbf{.653} & \CC\textbf{.883} & \CC\textbf{.738} & \CC\textbf{.840} & \CC\textbf{.982} & \CC\textbf{.892} \\
        \midrule
        \parbox[t]{2mm}{\multirow{5}{*}{\rotatebox[origin=c]{90}{Iter.}}}
        & MSNEA {\footnotesize {\cite{DBLP:conf/kdd/ChenL00WYC22}}} 
        & .294 & .580 & .391 & .385 & .621 & .463 & .417 & .655 & .500 & .657 & .864 & .726 \\
        & EVA {\footnotesize \cite{DBLP:conf/aaai/0001CRC21}} 
        & {.602} & {.873} & {.699} & {.770} & {.936} & {.829} & {.658} & {.861} & {.734} & .848 & .980 & .899 \\
        & MCLEA {\footnotesize {\cite{DBLP:conf/coling/LinZWSW022}}}  
        & {.646} & {.899} & {.739} & {.790} & {.946} & {.846} & {.696} & {.896} & {.772} & {.881} & {.984} & {.922} \\
        & MEAformer {\footnotesize {\cite{chen2023meaformer}}}  
        & \underline{.656} & \underline{.916} & \underline{.749} & \underline{.793} & \underline{.950} & \underline{.848} & \underline{.703} & \underline{.889} & \underline{.772} & \textbf{.884} & \underline{.988} & \underline{.923} \\
        & \CC\textbf{UMAEA}  
        & \CC\textbf{.670} & \CC\textbf{.921} & \CC\textbf{.763} & \CC\textbf{.801} & \CC\textbf{958} & \CC\textbf{.857} & \CC\textbf{.715} & \CC\textbf{.910} & \CC\textbf{.789} & \CC{.882} & \CC\textbf{.993} & \CC\textbf{.925} \\
        \bottomrule
    \end{tabular}
    }
    \label{tab:appoea}
    \vspace{-0.8cm}
\end{table}

 % Figure environment removed

\vspace{-0.3cm}
\subsubsection{\textbf{Baseline Analysis.}}
We attribute the lower performance of translation based methods (e.g., MSNEA) to their reliance on semantics assumptions, which limits their ability to capture the complex structural information among entities for alignment.

Some works \cite{DBLP:conf/ijcai/WuLF0Y019,DBLP:conf/cikm/YangWZQWHH21} hold that 
the structural information plays an important role in the EA task. 
% which makes GNN-based models generally perform better. 
By performing graph convolution over an entity’s neighbors, GCNs can incorporate more structural characteristics of knowledge graphs, while the translation assumption in translation-based models focuses more on the relationship among heads, tails and relations.



\subsection{Model Details}
We reproduce EVA \cite{DBLP:conf/aaai/0001CRC21}, MSNEA \cite{DBLP:conf/kdd/ChenL00WYC22}, MCLEA \cite{DBLP:conf/coling/LinZWSW022} and MEAformer \cite{chen2023meaformer} based on their source code \footnote{\color{blue} \url{https://github.com/cambridgeltl/eva}}$^{,}$\footnote{{\color{blue} \url{https://github.com/lzxlin/MCLEA}}}$^{,}$\footnote{\color{blue} \url{https://github.com/liyichen-cly/MSNEA}}$^{,}$\footnote{\color{blue} \url{https://github.com/zjukg/MEAformer}} with their
original model pipelines unchanged but unifying hyper-parameters.
 Yuan et al. \cite{yuan2023multi} consider the inter-modal effects and mitigate the impact of weak modalities, while Hama et al. \cite{DBLP:journals/access/HamaM23} quantify the importance of modality by embedding the entities into the probability distribution.
 Guo et al. \cite{DBLP:journals/corr/abs-2305-14651} propose the GEEA framework with the mutual variational autoencoder (M-VAE) to mutually encode/decode entities between source and target KGs for both entity alignment and
entity synthesis.
Given that their methods have different goals than ours and were recently published, we did not perform direct comparisons with them in our experiments.

\subsection{Metric Details}
\subsubsection{\textbf{Hits$@$N}} describes the fraction of true aligned 
 target entities that appear in the first N entities of the sorted rank list:
\begin{equation}
  \vspace{-2pt}
    \operatorname{Hits} @ \text{N}=\frac{1}{|\mathcal{S}_{te}|} \sum_{i=1}^{|\mathcal{S}_{te}|} \mathbb{I}[{\text {rank}_i} \leqslant \text{N}]\, ,
\end{equation}
where  ${{\text{rank}}_{i}}$ refers to the rank position of the first correct mapping for the i-th query entities and $\mathbb{I}=1$ if ${\text {rank}_i} \leqslant N$ and 0 otherwise.
$\mathcal{S}_{te}$ refers to the testing alignment set.
\subsubsection{\textbf{MRR}} (Mean Reciprocal Ranking $\uparrow$) is a statistic measure for evaluating many algorithms that produces a list of possible responses to a sample of queries, ordered by probability of correctness. 
In the field of EA, the reciprocal rank of a query entity (i.e., an entity from the source KG) response is the multiplicative inverse of the rank of the first correct alignment entity in the target KG.
MRR is the average of the reciprocal ranks of results for a sample of candidate alignment entities:
\begin{equation}
    \vspace{-2pt}
    \mathbf{MRR}=\frac{1}{|\mathcal{S}_{te}|} \sum_{i=1}^{|\mathcal{S}_{te}|} \frac{1}{\text {rank}_i} \,.
\end{equation}
\subsubsection{\textbf{MR}} (Mean Rank $\downarrow$) computes the arithmetic mean over all individual ranks which is similar to MRR:
\begin{equation}
    % \vspace{-2pt}
    \mathbf{MR}=\frac{1}{|\mathcal{S}_{te}|} \sum_{i=1}^{|\mathcal{S}_{te}|} {\text {rank}_i} \,. 
\end{equation}
Note that MR is sensitive to any model performance changes, not only changes that occur below a certain cutoff and therefore reflects the average performance.

\subsection{Future Work \& Discussion}
Knowledge Graphs (KGs) have been empirically validated to provide substantial benefits in a multitude of downstream applications. They serve as significant sources of knowledge supplementation and data augmentation for diverse tasks including, but not limited to, Question Answering \cite{DBLP:conf/semweb/0007CGPYC21,DBLP:conf/jist/0007HCGFP0Z22}, Zero-shot Learning \cite{DBLP:journals/pieee/ChenGCPHZHC23,DBLP:conf/ijcai/ChenG0HPC21,chen2023duet,DBLP:conf/www/GengC0PYYJC21}, and AI4Science \cite{fang2023knowledge,DBLP:conf/aaai/FangZYZD0Q0FC22}.

Despite these advancements, the application of Multi-modal Knowledge Graphs (MMKGs) to such tasks remains relatively unexplored. One plausible reason for this gap is the inherent uncertainty, ambiguity, and occasional missing phenomena associated with various modalities in MMKGs, a challenge particularly prominent within the visual modality, as examined in this paper.

Our objective with this research is to stimulate further academic discourse and exploration in the direction of Multi-modal Entity Alignment (MMEA). We anticipate more scholarly endeavors focusing on MMKG-driven downstream tasks, and we eagerly look forward to the comprehensive understanding and exploitation of the untapped potential of multi-modal KGs within the Semantic Web community.

Moreover, there remain opportunities for future research related to this work, such as evaluating our techniques in the context of incompleteness in other modalities (e.g., attribute), and investigating effective techniques to  utilize more detailed visual contents for MMEA. There is  potential for enhancing UMAEA’s efficiency, we also view this as a direction for future research which has not been explored in depth.

