\subsection{Experiment Setup}
To guarantee a fair assessment, we use a total of seven MMEA datasets derived from three major categories ({bilingual}, {monolingual}, and {high-degree}), with two representative  pre-trained visual encoders (ResNet-152 \cite{DBLP:conf/cvpr/HeZRS16} and CLIP \cite{DBLP:conf/icml/RadfordKHRGASAM21}), and evaluated the performance of {four} models under two distinct settings ({standard} (non-iterative) and {iterative}). In this research, we intentionally set aside the surface modality (literal information) to focus on understanding the effects of absent visual modality on model performance.

\subsubsection{Datasets.}
DBP15K \cite{DBLP:conf/semweb/SunHL17} contains three  datasets ($R_{sa}=0.3$) built from the multilingual versions of DBpedia, including DBP15K$_{ZH\text{-}EN}$, DBP15K$_{JA\text{-}EN}$ and DBP15K$_{FR\text{-}EN}$. 
% Each of them contains about $400$K triples and $15$K pre-aligned entity pairs. 
We adopt their multi-model variants \cite{DBLP:conf/aaai/0001CRC21} with entity-matched images attached.
Besides, four Multi-OpenEA datasets ($R_{sa}=0.2$) \cite{DBLP:journals/corr/abs-2302-08774} are used, which are the multi-modal variants of the OpenEA benchmarks \cite{DBLP:journals/pvldb/SunZHWCAL20} with entity images achieved by searching the entity names through the Google search engine. We include two bilingual datasets \{ EN-FR-15K, EN-DE-15K \} and two monolingual datasets \{ D-W-15K-V1, D-W-15K-V2 \}, where V1 and V2 denote two versions with distinct average relation degrees.
To create our \textbf{MMEA-UMVM} (uncertainly missing visual modality) datasets, we perform random image dropping on MMEA datasets. Specifically, we randomly discard entity images to achieve varying degrees of visual modality missing, ranging from 0.05 to the maximum $R_{img}$ of the raw datasets with a step of 0.05 or 0.1. 
Finally, we get a total number of 97 data split. 
See appendix \footnote{The appendix is attached with the arXiv version of this paper.} for more details. 

%\textbf{\textit{(\rmnum{1})}} {\emph{Bilingual}}: DBP15K \cite{DBLP:conf/semweb/SunHL17} contains three  datasets built from the multilingual versions of DBpedia: DBP15K$_{ZH\text{-}EN}$, DBP15K$_{JA\text{-}EN}$ and DBP15K$_{FR\text{-}EN}$. Each of them contains about $400$K triples and $15$K pre-aligned entity pairs with 30\% of them as the seed alignments. We adopt its multi-model variant \cite{DBLP:conf/aaai/0001CRC21} with entity-matched images attached.
%% which are standard datasets in MMEA community.
%\textbf{\textit{(\rmnum{2})}} {\emph{Monolingual}}: We select FB15K-DB15K (FBDB15K) and FB15K-YAGO15K (FBYG15K) in MMKG \cite{DBLP:conf/esws/LiuLGNOR19} with three data splits which have 20\%, 50\%, and 80\% of pre-aligned EA pairs as the seed alignments, respectively.
%% \end{itemize}

\subsubsection{Iterative Training.}
Following Lin et al. \cite{DBLP:conf/coling/LinZWSW022}, we adopt a probation technique for iterative training. The probation can be viewed as a buffering mechanism, which maintains a temporary cache to store cross-graph mutual nearest entity pairs from the testing set.
Concretely, every $K_e$ (where $K_e = 5$) epochs, we propose cross-KG entity pairs that are mutual nearest neighbors in the vector space and add them to a candidate list $\mathcal{N}^{cd}$. 
Furthermore, an entity pair in $\mathcal{N}^{cd}$ will be added into the training set if it remains a mutual nearest neighbour for $K_s$ ($=$ $10$) consecutive rounds.

\subsubsection{Baselines.}
Six prominent EA algorithms proposed in recent years are selected as our baseline comparisons, excluding the surface information for a parallel evaluation.
%Besides, 
We further collect 3 latest MMEA methods as the strong baselines, including EVA \cite{DBLP:conf/aaai/0001CRC21}, MSNEA \cite{DBLP:conf/kdd/ChenL00WYC22}, and MCLEA \cite{DBLP:conf/coling/LinZWSW022}. Particularly, we reproduce them with their original pipelines unchanged in our benchmark.
\input{exp-tab-1}

\subsubsection{Implementation Details.}\label{sec:detail}
To ensure fairness, we consistently reproduce or implement all methods with the following settings:
{{(\rmnum{1})}} The hidden layer dimensions $d$ for all networks are unified into 300.  
The total epochs for baselines are set to 500 with an optional iterative training strategy applied for another 500 epochs, following \cite{DBLP:conf/coling/LinZWSW022}.
Training strategies including cosine warm-up schedule ($15\%$ steps for LR warm-up), early stopping, and gradient accumulation are adopted. The AdamW optimizer ($\beta_1=0.9$, $\beta_2=0.999$) is used, with a fixed batch size of 3500. 
{{(\rmnum{2})}} To demonstrate model stability, following \cite{DBLP:conf/ksem/ChenLWXWC20,DBLP:conf/coling/LinZWSW022}, the vision encoders $Enc_{v}$ are set to ResNet-152 \cite{DBLP:conf/cvpr/HeZRS16} on DBP15K where the vision feature dimension $d_v$ is $2048$, and set to CLIP \cite{DBLP:conf/icml/RadfordKHRGASAM21} on Multi-OpenEA with $d_v=512$.
{{(\rmnum{3})}} An alignment editing method is employed to reduce the error accumulation \cite{DBLP:conf/ijcai/SunHZQ18}.
{{(\rmnum{4})}} Following Yang et al. \cite{DBLP:conf/emnlp/YangZSLLS19}, Bag-of-Words (BoW) is selected for encoding relations ($x^r$) and attributes ($x^a$)  as fixed-length (i.e., $d_r=d_a=1000$) vectors. Specially, we firstly sort relations/attributes across KGs by frequencies in descending order. At rank $d_r$/$d_a$, we truncated or padded the list to discard the long-tail relations/attributes and obtain fixed-length all-zero vectors $x^r$ and $x^a$. For entity $e_i$: if it includes any of the top-k attributes, the corresponding position in $x_i^a$ is set to 1; if a relation of $e_i$ is among the top-k, the corresponding position in $x_i^r$ is incremented by 1.


In our UMAEA model, $\tau$ is set to 0.1 which determines how much attention the contrast loss pays to difficult negative samples. Besides, the head number $N_h$ in MHCA is set to $1$, and the training epochs are set to \{250, 50, 100\} for stage 1, 2-1, 2-2, respectively. Despite potential performance variations resulting from parameter searching, our focus remained on achieving broad applicability rather than fine-tuning for specific datasets.
During iterative training, the pipeline is repeated; but the expansion of the training set occurs exclusively in stage 1.
For MSNEA, we eliminate the attribute values for input consistency, and extend MSNEA with iterative training capability. All experiments are conducted on RTX 3090Ti GPUs.
