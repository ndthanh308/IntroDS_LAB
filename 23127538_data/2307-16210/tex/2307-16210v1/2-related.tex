\section{Related Work}
Entity Alignment (EA) \cite{tkde/Sun,DBLP:journals/corr/abs-2305-14651} is the task of identifying equivalent entities across multiple knowledge graphs (KGs), which can facilitate knowledge integration.

\subsubsection{Typical Entity Alignment} methods mainly rely on the relational, attribute, and surface (or literal) features of KG entity for alignment. 
Specifically, symbol logic-based technologies are used \cite{DBLP:conf/semweb/Jimenez-RuizG11,DBLP:journals/pvldb/SuchanekAS11,DBLP:conf/ijcai/QiZCCXZZ21} to constrain the EA process via manually defined prior rules (e.g., logical reasoning and lexical matching).
Embedding-based methods \cite{tkde/Sun} eschew the ad-hoc heuristics of logic-based approaches, employing learned embedding space similarity measures for rapid alignment decisions.
Among these, {GNN-based EA models} \cite{DBLP:conf/emnlp/LiCHSLC19,DBLP:conf/aaai/SunW0CDZQ20,DBLP:conf/emnlp/LiuCPLC20,DBLP:conf/acl/WuLFWZ20,DBLP:conf/kdd/GaoLW0W022,DBLP:conf/semweb/WangCLSJHH22} emphasize local and global structural KG characteristics, primarily utilizing graph neural networks (GNNs) for neighborhood entity feature aggregation.
While {{translation-based EA methods}} \cite{DBLP:conf/ijcai/ZhangSHCGQ19,DBLP:conf/semweb/SunHHCGQ19,DBLP:conf/wsdm/XinSH0022,DBLP:conf/ijcai/CaiMZJ22,DBLP:journals/corr/abs-2304-04389} 
use techniques like TransE \cite{DBLP:conf/nips/BordesUGWY13} 
to capture the pairwise information from relational triples, positing that relations can be modeled as straightforward translations in the vector space. 

\subsubsection{Multi-modal Entity Alignment} (MMEA) normally leverages visual modality as supplementary information to enhance EA, with each entity accompanied by a related image.
Specifically, 
Chen et al. \cite{DBLP:conf/ksem/ChenLWXWC20} propose to combine knowledge representations from different modalities, minimizing the distance between holistic embeddings of aligned entities.
Liu et al. \cite{DBLP:conf/aaai/0001CRC21} use a learnable attention weighting scheme to assign varying importance to each modality. 
Chen et al. \cite{DBLP:conf/kdd/ChenL00WYC22} incorporate visual features to guide relational feature learning while weighting valuable attributes for alignment. 
Lin et al. \cite{DBLP:conf/coling/LinZWSW022} further improve intra-modal learning with contrastive learning. 
Shi et al. \cite{DBLP:journals/dase/WangSYZLZ23} filter out mismatched images with pre-defined ontologies and an image type classifier.
Chen et al. \cite{chen2023meaformer} dynamically predict the mutual modality weights for entity-level modality fusion and alignment.

% and apply KL divergence to reduce the modality gap between joint and uni-modal embeddings.
These approaches substantiate that visual information indeed contributes positively to EA.
However, we notice that all of them are based on two ideal assumptions: {{(\rmnum{1})}} Entities and images have a one-to-one correspondence, meaning that a single image sufficiently encapsulates and conveys all the information about an entity. {{(\rmnum{2})}} Images are always available, implying that an entity consistently possesses a corresponding image.

% Figure environment removed

In real-world KGs, the noise is an inherent issue. Even for the standard MMEA datasets \cite{DBLP:conf/semweb/SunHL17,DBLP:conf/esws/LiuLGNOR19,DBLP:conf/ksem/ChenLWXWC20,DBLP:conf/aaai/0001CRC21}, they are hard to satisfy those two ideal conditions mentioned above. 
Consequently, we focus on two more pragmatic and demanding issues: {{(\rmnum{1})}} {In MMKGs, entity images might be missing uncertainly, implying a varying degree of image absence.}  {{(\rmnum{2})}} {In MMKGs, images of the entities could be uncertainly ambiguous, suggesting that a single entity might have heterogeneous visual representations. }
To tackle these challenges, we present a benchmark consisting of seven datasets on which extensive experiments are conducted, and introduce our model UMAEA against these problems.

\subsubsection{Incomplete Multi-modal Learning}
aims to tackle classification or reconstruction tasks, like multi-modal emotion recognition \cite{DBLP:conf/acl/ZhaoLJ20} and cross-modal retrieval \cite{DBLP:conf/mm/JingLZLYH20},  by leveraging information from available modalities when one modality is missing (e.g,  a tweet may only have images or text content).
% Jing et al. \cite{DBLP:conf/mm/JingLZLYH20} introduce dual-aligned variational autoencoders for this purpose, while Ma et al. \cite{DBLP:conf/aaai/MaRZTWP21} use meta-learning with modality priors from complete datasets.
In multi-modal alignment tasks, missing modality significantly impacts the performance as the symmetry of paired multi-modal data leads to noise accumulation when it is uncertain which side has modality incompleteness, 
further hindering model training. 
Prior MMEA studies \cite{DBLP:conf/aaai/0001CRC21,DBLP:conf/kdd/ChenL00WYC22,DBLP:conf/coling/LinZWSW022,chen2023meaformer} calculate mean and variance from available visual features, enabling random generation of those incomplete features using a normal distribution. In this paper, we develop an adaptive method for optimal training under the conditions with uncertainly missing or noisy visual modality, meanwhile providing a comprehensive benchmark.


