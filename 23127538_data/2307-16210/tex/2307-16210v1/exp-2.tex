\subsubsection{Complete Modality.}
We also evaluate our model on the standard multi-modal DBP15K \cite{DBLP:conf/aaai/0001CRC21} dataset, achieving satisfactory results with or without the visual modality (w/o IMG), as shown in Table \ref{tab:std-1}. It is noteworthy that the DBP15K dataset only has part of the entities with images attached (e.g., $78.29\%$ in DBP15K$_{ZH\text{-}EN}$, $70.32\%$ in DBP15K$_{FR\text{-}EN}$, and $67.58\%$ in DBP15K$_{JA\text{-}EN}$), which is inherent to the DBPedia database.
To further showcase our method's adaptability, in Table \ref{tab:std-2}, we evaluate it on the standard Multi-OpenEA dataset with $100\%$ image data attached, demonstrating that our method can be superior in the (MM)EA task against the potentially ambiguous modality information.
% since we achieve the best performance with full images or without images.
% where UMAEA continues to achieves SOTA performance.
\input{exp-tab-2}




