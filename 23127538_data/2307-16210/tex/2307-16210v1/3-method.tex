
\section{Method}
\subsection{Preliminaries}
We define a MMKG as a five-tuple $\mathcal{G}$$=$$\{\mathcal{E}, \mathcal{R}, \mathcal{A}, \mathcal{V}, \mathcal{T}\}$, where
$\mathcal{E}, \mathcal{R}, \mathcal{A}$ and $\mathcal{V}$ denote the sets of entities, relations, attributes, and images, respectively. $\mathcal{T}$ $\subseteq$ $\mathcal{E} \times \mathcal{R} \times\mathcal{E}$ is the set of relation triples.
Given two MMKGs $\mathcal{G}_1$ $=$ $\{\mathcal{E}_1, \mathcal{R}_1, \mathcal{A}_1, \mathcal{V}_1, \mathcal{T}_1\}$ and $\mathcal{G}_2$ $=$ $\{\mathcal{E}_2, \mathcal{R}_2, \mathcal{A}_2, \mathcal{V}_2, \mathcal{T}_2\}$, MMEA aims to discern each entity pair ($e^1_i$, $e^2_i$), $e^1_i \in \mathcal{E}_1$, $e^2_i \in \mathcal{E}_2$ where $e^1_i$ and $e^2_i$ correspond to an identical real-world entity $e_i$.
%To enhance clarity in this section, unless specifically required in statements or formulas, we will omit the superscript symbol representing the source KG of an entity by default in the text.
For clarity, we omit the superscript symbol denoting the source KG of an entity in our context, except when explicitly required in statements or formulas.
A set of pre-aligned entity pairs is provided, which is proportionally divided into a training set (i.e., seed alignments $\mathcal{S}$) and a testing set $\mathcal{S}_{te}$ based on a given seed alignment ratio ($R_{sa}$). 
We denote $\mathcal{M}=\{g, r, a, v\}$ as the set of available modalities.
Commonly, 
in typical KG datasets for MMEA, each entity is associated with multiple attributes and  $0$ or $1$ image, and the proportion ($R_{img}$) of entities containing images is uncertain (e.g., 67.58$\%$ in DBP15K$_{JA\text{-}EN}$ \cite{DBLP:conf/aaai/0001CRC21}). 
In this study, in order to facilitate a comprehensive evaluation, dataset MMEA-UMAM is proposed where we  define $R_{img}$ as a controlled variable for  benchmarking.


\subsection{Multi-modal Knowledge Embedding} 
\subsubsection{Graph Structure Embedding.} 
Let $x_i^g \in \mathbb{R}^d$ represent the  randomly initialized graph embedding of entity $e_i$ where $d$ is the predetermined hidden dimension. 
We employ the Graph Attention Network (GAT) \cite{DBLP:conf/iclr/VelickovicCCRLB18} with two attention heads and two layers to capture the structural information of $\mathcal{G}$, equipped with a diagonal weight matrix \cite{DBLP:journals/corr/YangYHGD14a} $\bm{W}_g \in \mathbb{R}^{d \times d}$ for linear transformation. We define
%\begin{equation} \label{eq:map}
 $h_i^g = GAT(\bm{W}_g, \bm{M}_g; x_i^g)\,$,
%\end{equation}
where $\bm{M}_g$ denotes to the graph adjacency matrix. 

\subsubsection{Relation, Attribute, and Visual Embedding.} 
To mitigate the information contamination arising from blending relation / attribute representations in GNN-like networks \cite{DBLP:conf/aaai/0001CRC21}, we employ separate fully connected layers, parameterized by $\bm{W}_m \in \mathbb{R}^{d_m \times d}$, 
% to transform features $x^m$ 
for embedding space harmonization via
%\begin{equation} \label{eq:map}
 $h_i^m$ $=$ $FC_m(\bm{W}_m, x_i^m)\,$,
%\end{equation}
where $m \in \{r, a, v\}$ and $r$, $a$, $v$, represent relation, attribute, visual modalities, respectively. 
Furthermore,  $x_i^m \in \mathbb{R}^{d_m}$ denotes the input feature of entity $e_i$ for the corresponding modality $m$.
We follow Yang et al. \cite{DBLP:conf/emnlp/YangZSLLS19} to use the bag-of-words features for relation ($x^r$) and attribute ($x^a$) representations (see Section \ref{sec:detail} for details).
While for the visual modality, we employ a pre-trained (frozen) visual model as the encoder ($Enc_v$) to obtain the visual embeddings $x^v_i$ for each available image of the entity $e_i$.
% where the final layer output before logits serves as the image feature.
%\begin{equation} \label{map}
%x^v_i = Enc_v(v_i)\,.
%\end{equation}
For entities without image data, we generate random image features using a normal distribution parameterised by the mean and standard deviation of other available images \cite{DBLP:conf/aaai/0001CRC21,DBLP:conf/kdd/ChenL00WYC22,DBLP:conf/coling/LinZWSW022,chen2023meaformer}.  

\subsection{Multi-scale Modality Hybrid}
This section describes the detailed architecture of the multi-scale modality hybrid for aligning multi-modal entities between MMKGs. The model comprises three modality alignment modules operating at different scales, each associated with a training objective as depicted in Figure \ref{fig:model}.

\subsubsection{Global Modality Integration} (GMI) emphasizes global alignment for each multi-modal entity pair, where the multi-modal embeddings for an entity are first concatenated and then aligned using a learnable global weight, allowing the model to adaptively learn the relative quality of each modality across two MMKGs. 
Let $w_m$ be the global weight for modality $m$. We formulate the GMI joint embedding ${h}^{GMI}_i$ for entity $e_i$ as: 
\begin{align} \label{eq:cat}
    {h}^{GMI}_i = \bigoplus\nolimits_{m \in \mathcal{M}}[w_m{h}_i^m]\, ,
%    Fusion(h_i, w, \{r, a, v, s\}) \,, \\ 
%    Fusion(h, w, \mathcal{M}) & = \bigoplus\nolimits_{m \in \mathcal{M}}[w_m{h}_i^m]\,,
\end{align} 
where $\bigoplus$ refers to the vector concatenation operation.
To enhance model's sensitivity to feature differences between unaligned entities, we introduce a unified entity alignment contrastive learning framework, inspired by Lin et al. \cite{DBLP:conf/coling/LinZWSW022}, to consolidate the training objectives of the modules. For each entity pair ($e_i^1$,$e_i^2$) in $\mathcal{S}$, we define $\mathcal{N}^{ng}_i$ $=$ $\{e^1_j|\forall e^1_j \in \mathcal{E}_1, j \neq i\}$ $\cup$ $\{e^2_j|\forall e^2_j \in \mathcal{E}_2, j \neq i\}$ as its negative entity set.
%And we further involve the in-batch negative sampling strategy \cite{DBLP:conf/icml/ChenK0H20} to limit the sampling scope of $\mathcal{N}^{ng}_i$ within the mini-batch for efficiency.
To improve efficiency, we adopt the in-batch negative sampling strategy \cite{DBLP:conf/icml/ChenK0H20}, restricting the sampling scope of  $\mathcal{N}^{ng}_i$ to the mini-batch $\mathcal{B}$.
Concretely,
we define the alignment probability distribution as follows:
\begin{equation}\label{eq:macl}
    p_m(e^1_i, e^2_i) =  \frac{\gamma_m(e^1_i, e^2_i)}{\gamma_m(e^1_i, e^2_i) + \sum\nolimits_{e_j \in \mathcal{N}^{ng}_i}\gamma_m(e^1_i, e_j)} \, ,
\end{equation}
where $\gamma_m(e_i, e_j)$ $=$ $\exp({h^{m\top}_{i}}{h^m_{j}}/\tau)$ and $\tau$ represents the temperature hyper-parameter.
To account for the alignment direction of entity pairs in \eqref{eq:macl}, we establish a bi-directional alignment objective as:
\begin{equation}\label{eq:clloss}
    \mathcal{L}_m = - \mathbb{E}_{i \in \mathcal{B}}\, \log[\,p_m(e^1_i, e^2_i)+p_m(e^2_i, e^1_i)\,]/2 \,,
\end{equation}
where $m$ denotes a modality or an embedding type. We denote the training objective as $\mathcal{L}_{GMI}$ when the GMI join embedding is used, i.e., $\gamma_{GMI}(e_i, e_j)$ is set to $\exp({h^{{GMI}\top}_{i}}{h^{GMI}_{j}}/\tau)$.

We note that the global adaptive weighting allows the model to capitalize on high-quality modalities while minimizing the impact of low-quality modalities, such as the redundant information within attributes / relations, and noise within images. Concurrently, it ensures the preservation of valuable information to a certain extent, ultimately contributing to the stability of the alignment process.

\subsubsection{Entity-level Modality Alignment} aims to perform instance-level modality weighting and alignment, utilizing minimum cross-KG  confidence measures from seed alignments to constrain the modality alignment objectives.
It allows the model to dynamically assign lower training weights to missing or ambiguous modality information, thereby reducing the risk of encoder misdirection arising from uncertainties. % during the entity alignment process.
To achieve this, we follow Chen et al. \cite{chen2023meaformer} to adapt the vanilla Transformer \cite{DBLP:conf/nips/VaswaniSPUJGKP17} for two types of sub-layers:  
the multi-head cross-modal attention (MHCA) block and the fully connected feed-forward networks (FFN).

Specifically, MHCA operates its attention function across $N_h$ parallel heads. The $i$-th head is parameterized by modally shared matrices $\bm{W}_q^{(i)}$, $\bm{W}_k^{(i)}$, $\bm{W}_v^{(i)}$ $\in \mathbb{R}^{d \times d_h}$, transforming the multi-modal input $h^m$ into modal-aware query ${Q}^{(i)}_m$, key ${K}^{(i)}_m$, and value ${V}^{(i)}_m$ in $\mathbb{R}^{d_h}$ ($d_h=d/N_h$):
\begin{equation}
{Q}^{(i)}_m, {K}^{(i)}_m, {V}^{(i)}_m =h^m \bm{W}_q^{(i)}, h^m \bm{W}_k^{(i)}, h^m \bm{W}_v^{(i)} \,.\\
\end{equation}
MHCA generates the following output for a given feature of modality $m$:
\begin{align}
       \operatorname{MHCA}(h^m) & =\bigoplus\nolimits_{i=1}^{N_h}\operatorname{head}_i^m \cdot \bm{W}{o} \,, \\
		\operatorname{head}_{\mathrm{i}}^m & = \sum\nolimits_{j \in \mathcal{M}} \beta^{(i)}_{mj}{V}^{(i)}_j \,,
\end{align}
where $\bm{W}_{o}$ $\in \mathbb{R}^{d \times d}$. The attention weight ($\beta_{m j}$)
 % ($\beta^{(i)}_{mj}$) 
 between an entity's modality $m$ and $j$  in each head  is calculated as:
\begin{equation}
    \beta_{m j}=\frac{\exp (Q_m^\top K_j / \sqrt{d_h} )}{\sum_{i \in \mathcal{M}} \exp (Q_m^\top K_i / \sqrt{d_h})}\,.
\end{equation}
% is set in MHCA.
Besides, layer normalization (LN) and residual connection (RC) are incorporated to stabilize training:
\begin{equation}
\hat{h}^m = LayerNorm(\operatorname{MHCA}(h^m) + h^m) \,.
\end{equation}

The FFN consists of two linear transformation layers and a ReLU activation function with LN and RC applied afterwards:
\begin{align}
\operatorname{FFN}(\hat{h}^m) & = ReLU(\hat{h}^m\bm{W}_{1} + b_{1})\bm{W}_{2} +b_{2} \,, \\
\label{eq:hidden}
\hat{h}^m & \gets LayerNorm(\operatorname{FFN}(\hat{h}^m) + \hat{h}^m) \,,
\end{align}
where $\bm{W}_{1}$ $\in \mathbb{R}^{d \times d_{in}}$ and $\bm{W}_{2}$ $\in \mathbb{R}^{d_{in} \times d}$.
Notably, 
we define the entity-level confidence $\tilde{w}^m$ for each modality $m$ as:
\begin{equation}
    \tilde{w}^m = \frac{\exp(\sum\nolimits_{j \in \mathcal{M}} \sum\nolimits_{i=0}^{N_h}  \beta^{(i)}_{mj}/\sqrt{|\mathcal{M}| \times N_h})}{\sum\nolimits_{k \in \mathcal{M}}\exp(\sum\nolimits_{j \in \mathcal{M}} \sum\nolimits_{i=0}^{N_h}  \beta^{(i)}_{kj}\sqrt{|\mathcal{M}| \times N_h})}\,,
\end{equation}
which captures crucial inter-modal interface information and adaptively adjusts model's cross-KG alignment confidence for different modalities from each entity.
To facilitate learning these dynamic confidences and incorporating them into the training process, we devise two distinct training objectives: $\mathcal{L}_{ECIA}$ and $\mathcal{L}_{IIR}$.
%\begin{equation}
%    \mathcal{L}_{EMA} = ,
%\end{equation}
The first objective is \emph{explicit confidence-augmented intra-modal alignment} (ECIA), while the second is \emph{implicit inter-modal refinement} (IIR), which will be discussed in the following subsection.
For the ECIA, we design the following training target which is the variation of 
 Equation \eqref{eq:clloss}:
\begin{align}
    \mathcal{L}_{ECIA} & = \sum\nolimits_{m \in \mathcal{M}}\widetilde{\mathcal{L}}_m\,\, , \\
    \widetilde{\mathcal{L}}_m = - \mathbb{E}_{i \in \mathcal{B}}\,\log[\,\phi_m(e^1_i, e^2_i& )*(p_m(e^1_i, e^2_i)+p_m(e^2_i, e^1_i))\,]/2 \,.
\end{align}
Considering the symmetric nature of EA and the varying quality of aligned entities and their modality features within each KG, we employ the minimum confidence value to minimize errors. For example, $e^1_i$ may possess high-quality image data while $e^2_i$ lacks image information, as illustrated in Figure \ref{fig:Introcase}. In such cases, using the original objective for feature alignment will inadvertently align meaningful features with random noise, thereby disrupting the encoder training process.
To mitigate this issue, we define $\phi_m(e^1_i, e^2_i)$ as the minimum confidence value for entities $e^1_i$ and $e^2_i$ in modality $m$, calculated by
%\begin{equation}
	$\phi_m(e_i, e_j) = Min(\tilde{w}_i^m, \tilde{w}_j^m)\,$.
%\end{equation}

\subsubsection{Late Modality Refinement} \label{sec:iir}
 leverages the transformer layer outputs to 
 % construct a fused embedding with a joint alignment objective. This approach 
 further enhance the entity-level adaptive modality alignment through an \emph{implicit inter-modal refinement} (IIR) objective, enabling the refinement of attention scores by directly aligning the output hidden states.
Concretely, we define the hidden state embedding of modality $m$ for entity $e_i$ as $\hat{h}^m$, following  Equation \eqref{eq:hidden}. We define:
\begin{equation}
 \mathcal{L}_{IIR} = \sum\nolimits_{m \in \mathcal{M}}\widehat{\mathcal{L}}_m\,,
\end{equation}
where $\widehat{\mathcal{L}}_m$ is also a variant of $\mathcal{L}_m$,  as illustrated in Equation \eqref{eq:clloss}, with only the following modification: 
%\begin{equation}
$\widehat{\gamma}_m(e_i, e_j) = \exp(\hat{h}^{m\top}_{i}{\hat{h}^m_{j}}/\tau)$.
%\end{equation}

As depicted in Figure \ref{fig:model}, we designate the entire process so far as the first stage of our (main) model, with the training objective formulated as:
\begin{equation}
\mathcal{L}_{1} = \mathcal{L}_{GMI} + \mathcal{L}_{ECIA} + \mathcal{L}_{IIR} \,.
\end{equation}



%\subsection{Modality Aware Contrastive Learning.}

\subsection{Circularly Missing Modality Imagination.}
Note that our primary target of the first stage is to alleviate the impact of modality noise and incompleteness on the alignment process throughout training. Conversely,  the second stage draws inspiration from VAE \cite{DBLP:journals/corr/KingmaW13,DBLP:conf/nips/SohnLY15} and CycleGAN \cite{DBLP:conf/iccv/ZhuPIE17}, which accentuates generative modeling and unsupervised domain translation. Expanding upon these ideas, we develop our circularly missing modality imagination (CMMI) module, aiming to enable the model to proactively complete missing modality information. 

To reach our goal, we develop a variational multi-modal autoencoder framework, allowing the hidden layer output between the encoder $MLP_{Enc}$ and decoder $MLP_{Dec}$ (parameterized by $\bm{W}_{Enc} \in \mathbb{R}^{3d \times 2d}$ and $\bm{W}_{Dec} \in \mathbb{R}^{d \times 3d}$, respectively) to act as an imagined pseudo-visual feature $\bar{h}^{v}_i$, using reparameterization strategy \cite{DBLP:journals/corr/KingmaW13} with tri-modal hybrid feature ${h}^{hyb}_i = [{h}_i^r \oplus {h}_i^a \oplus {h}_i^g]$ as the input:
\begin{align}
\,\,\,\,\,\, [\,{\mu}_i \oplus \log(\sigma_i)^2\,] & = MLP_{Enc}({h}^{hyb}_i) \,, \\
\,\,\,\,\,\, \bar{h}^{v}_i & = z  \odot {\sigma}_i + {\mu}_i \,,\,\,\, z \sim \mathcal{N}(\bm{0},\bm{I}) \,, \\
\,\,\,\,\,\, \bar{h}^{hyb}_i & = MLP_{Dec}(\bar{h}^{v}_i) \, .
\end{align}
Concretely, two reconstruction objectives $\mathcal{L}_{Re}^{vis}$ and $\mathcal{L}_{Re}^{hyb}$ are utilized to minimize $|{h}^{hyb}_i - \bar{h}^{hyb}_i|$ and $|h^{v}_i - \bar{h}^{v}_i|$, where $h^{v}_i$ represents the real image feature.
Besides, we adhere to the standard VAE algorithm \cite{DBLP:journals/corr/KingmaW13} to regularize the latent space by encouraging it to be similar to a Gaussian distribution through minimizing the Kullbackâ€“Leibler (KL) divergence:
\begin{equation}
\mathcal{L}_{KL} = \mathbb{E}_{i \in \mathcal{\bar{B}}}\, ((\mu_i)^2 + (\sigma_i)^2 - \log(\sigma_i)^2 - 1)/2 \,,
\end{equation}
where $\mathcal{\bar{B}}$ refers to those entities with complete images within a mini-batch.

Furthermore, we exploit the internal embedding similarity matrix obtained from the  hybrid embeddings ${h}^{hyb}$, and distill this information into the virtual image feature similarity matrix based on $\bar{h}^{v}$:
\begin{equation}
\mathcal{L}_{Sim} = \mathbb{E}_{i \in \mathcal{\bar{B}}}\, D_{KL}(p_{hyb}(e^1_i, e^2_i)||\bar{p}_{v}(e^1_i, e^2_i))\,,
\end{equation}
where $p_{hyb}$ and $\bar{p}_{v}$ all follow Equation \eqref{eq:macl} with $\gamma_{hyb}(e_i, e_j)$ $=$ $\exp({h^{hyb\top}_{i}}{h^{hyb}_{j}}/\tau)$ and $\bar{\gamma}_v(e_i, e_j)$ $=$ $\exp({\bar{h}^{v\top}_{i}}{\bar{h}^v_{j}}/\tau)$. 
This strategy not only curbs the overfitting of visible visual modalities in the autoencoding process, but also emphasizes the differences between distinct characteristics. Crucially, the knowledge mapping of original tri-modal hybrid features to the visual space  is maximally preserved, thereby mitigating modal collapse when most of the visual content is missing and the noise is involved. The final loss in stage two is formulated as:
\begin{equation}
\mathcal{L}_{2} = \mathcal{L}_{KL} + \mathcal{L}_{Re}^{vis} + \mathcal{L}_{Re}^{hyb} + \mathcal{L}_{Sim} \,.
\end{equation}

\subsection{Training Details}
\subsubsection{Pipeline.}
As previously mentioned, the training process consists of two stages. In the first stage, the primary model components are trained independently, while in the second stage, the CMMI module is additionally incorporated. The training objective $\mathcal{L}$ is defined as follows: 
\begin{align}
Stage~1: \mathcal{L} & \gets \mathcal{L}_1 \,, \\
Stage~2\text{-}1/2\text{-}2: \mathcal{L} & \gets \mathcal{L}_1 + \mathcal{L}_2 \,,
\end{align}
where the second stage is further divided into two sub-stages.
Concretely, in order to stabilize model training and avoid knowledge forgetting  caused by the cold-start of module insertion  \cite{DBLP:journals/corr/abs-2304-14178}, as shown in Figure \ref{fig:model}, the models from stage 1 (i.e., main model) are frozen to facilitate CMMI training when entering stage 2-1. While in stage 2-2, the CMMI is frozen and the main model undergoes further refinement to establish the entire pipeline.
This process is easy to implement, just by switching the range of learnable parameters during model training.


\subsubsection{Entity Representation.}
During evaluation, we replace the original random vectors with the generated $\mu_i$ for those entities without images. While in the second training stage, we employ the pseudo-visual embedding $\bar{h}^v_i$ (rather than $\mu_i$) as a substitute as 
 we observe that actively introducing noise during training could introduce randomness and uncertainty into the reconstruction process, which has been demonstrated to be beneficial in learning sophisticated distributions and enhances the model's robustness \cite{DBLP:conf/iclr/LeeNYH20}.
 % we observe that representing noise as multivariate random variables  
Furthermore,  we select ${h}^{GMI}_i$, as formulated in Equation \eqref{eq:cat}, for the final multi-modal entity representation. 

%We speculate that the modality specificity in late fusion embedding is always smoothed by the Transformer Layer which diminishes the distinction between entities. Our experiments have demoscoate this hypothesis.

