\subsection{Overall Results} \label{sec:overall}
\subsubsection{Uncertainly Missing Modality.}
%Typical Setting and Iterative Setting.
Our primary experiment focuses on the model performances with varying missing modality proportions $R_{img}$.
In Table \ref{tab:overall}, we select four representative proportions: $R_{img} \in \{0.05, 0.2, 0.4, 0.6\} \times 100\%$ to simulate the degree of uncertainly missing modality that may exist in real-world scenarios, and evaluate the robustness of different models.
Our UMAEA demonstrates stable improvement on the DBP15K datasets across different $R_{img}$ values in comparison to the top-performing benchmark model:  $10.3\%$ ($R_{img}=0.05$), $11.6\%$ ($R_{img}=0.2$), $11.9\%$ ($R_{img}=0.4$), and $10.3\%$ ($R_{img}=0.6$). We note that it exhibits the most significant improvement when the $R_{img}$ lies between $20\%$ and $40\%$.
For the Multi-OpenEA datasets, our average improvement is: $5.5\%$ ($R_{img}=0.05$), $5.9\%$ ($R_{img}=0.2$), $7.3\%$ ($R_{img}=0.4$), and $6.4\%$ ($R_{img}=0.6$). Although the improvement is slightly lower than in DBP15K, the overall advantage range remains consistent, aligning with our motivation.
Besides, Figure \ref{fig:line} visualizes performance variation curves for three models. 
% Among them, we specify the interval range of $R_{img}$, which is from 0 to the maximum available $R_{img}$ on DBP15K data, and from 0 to 0.95 on OpenEA. 
The overall performance trend fits the conclusions drawn in Table \ref{tab:overall}, showing that our method outperforms the baseline in terms of significant performance gap, regardless of whether iterative or non-iterative learning is employed.

Additionally, we notice a phenomenon that existing models exhibit performance oscillations (EVA) or even declines (MCLEA) at higher modality missing rates. This kind of adverse effect peaks within a particular $R_{img}^1$ range and gradually recovers and gains benefits as $R_{img}$ rises to a certain level $R_{img}^2$. In other words, when $0 \leq R_{img} \leq R_{img}^2$, the additional multi-modal data negatively impacts EA. 
This observation seems counterintuitive since providing more information leads to side effects, but it is also logical. Introducing images for half of the entities means that the remaining half may become noise, which calls for a necessary trade-off.
Under the standard (non-iterative) setting, MCLEA's $R_{img}^2$ averages $63.6\%$, which is $57.14\%$ for MSNEA and $46.43\%$ for EVA across seven datasets. Our method, augmented with the CMMI module, reaches $20.71\%$ for $R_{img}^2$. Even without CMMI, the $R_{img}^2$ of UMAEA remains at $34.29\%$. 
This implies that our method can gain benefits with fewer visual modality data in entity.
Meanwhile, UMAEA exhibits less oscillation and greater robustness than other methods, as further evidenced by the entity distribution analysis in Section \ref{sec:dist}.

% Figure environment removed

We observe that our performance improvement on Multi-OpenEA is less pronounced compared to the DBP15K dataset.  This may be due to the higher image feature quality of CLIP compared to ResNet-152, which in turn diminishes the relative benefit of our model in addressing feature ambiguity. Additionally, as the appendix shows, these datasets have fewer relation and attribute types, allowing for better feature training with comparable data sizes (with a fixed 1000-word bag size, long tail effects are minimized) which partially compensates for missing image modalities.  This finding can also explain why, as seen in Figure \ref{fig:line}, our model's performance improvement decreases as $R_{img}$ increases, and our enhancement in the dense graph (D-W-V2) is slightly less pronounced than in the sparse graph (D-W-V1) which has richer graph structure information.

% 





