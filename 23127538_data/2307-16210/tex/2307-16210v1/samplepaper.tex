% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
%\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
%\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\urlstyle{same}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{bm}
\usepackage{underscore}
\usepackage{amsfonts}
\usepackage{xcolor}
\usepackage{color, colortbl}
\usepackage{makecell}
\usepackage{enumitem}
\let\Bbbk\relax
\usepackage{amssymb}
\usepackage{colortbl}

% \usepackage[dvipsnames,svgnames]{xcolor}
%\usepackage[table, svgnames, dvipsnames]{xcolor}
\usepackage[switch]{lineno}
% Comment out this line in the camera-ready submission
%\linenumbers

\definecolor{mygray}{gray}{.9}
\definecolor{gray2}{gray}{.78}
\definecolor{gray3}{gray}{.7}
\definecolor{gray4}{gray}{.6}
\definecolor{gray5}{gray}{.5}
% \newcommand{\CC}[1]{\cellcolor{blue!#1}}
% \newcommand{\CC}[1]{\cellcolor{blue!#1}}
\newcommand{\CC}{\cellcolor{mygray}}
\newcommand{\CK}{\cellcolor{gray2}}
\newcommand{\rmnum}[1]{\romannumeral #1}
\newcommand{\Rmnum}[1]{\expandafter\@slowromancap\romannumeral #1@}
\newcommand{\todo}[1]{{\color{red}[#1]}}
\newcommand{\wen}[1]{{\color{black}#1}}
%
\begin{document}
\title{Rethinking Uncertainly Missing and Ambiguous Visual Modality in Multi-Modal \\ Entity Alignment}

\input{0-author}
\maketitle              % typeset the header of the contribution
%
% \def\thefootnote{$\dagger$}\footnotetext{Equal Contribution.}

\begin{abstract}
As a crucial extension of entity alignment (EA), multi-modal entity alignment (MMEA) aims to identify identical entities across disparate knowledge graphs (KGs) by exploiting associated visual information. However, existing MMEA approaches primarily concentrate on the fusion paradigm of multi-modal entity features, while neglecting the challenges presented by the pervasive phenomenon of missing and intrinsic ambiguity of visual images.
In this paper, we present a further analysis of visual modality incompleteness, benchmarking latest MMEA models on our proposed dataset {MMEA-UMVM}, where
the types of alignment KGs covering bilingual and monolingual, with standard (non-iterative) and iterative training paradigms to evaluate the model performance. 
Our research indicates that, in the face of modality incompleteness, models succumb to overfitting the modality noise, and exhibit performance oscillations or declines at high rates of missing modality. This proves that the inclusion of additional multi-modal data can sometimes adversely affect EA. 
To address these challenges,  we introduce {UMAEA}, a robust multi-modal \textbf{e}ntity \textbf{a}lignment approach designed to tackle \textbf{u}ncertainly \textbf{m}issing and \textbf{a}mbiguous visual modalities.
It consistently achieves SOTA performance across all 97 benchmark splits, significantly surpassing existing baselines with limited parameters and time consumption, while effectively alleviating the identified limitations of other models. 
Our code and benchmark data are available at {\color{blue} \url{https://github.com/zjukg/UMAEA}}.

\keywords{Entity Alignment  \and Knowledge Graph \and Multi-modal Learning \and Uncertainly Missing Modality.}

\end{abstract}


\input{1-intro}

\input{2-related}

\input{3-method}

\input{4-experiment}

\input{5-conclusion}
%
%
%


%
% ---- Bibliography ----

 \bibliographystyle{splncs04}
 \bibliography{mybibliography}
%
\appendix
\input{append}

\end{document}
