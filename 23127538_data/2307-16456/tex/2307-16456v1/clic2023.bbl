\begin{thebibliography}{}

\bibitem[\protect\citename{Bacciu \bgroup et al.\egroup }2023]{bacciu2023fauno}
Andrea Bacciu, Giovanni Trappolini, Andrea Santilli, Emanuele Rodolà, and
  Fabrizio Silvestri.
\newblock 2023.
\newblock Fauno: The italian large language model that will leave you senza
  parole!

\bibitem[\protect\citename{Baroni \bgroup et al.\egroup }2009]{baroni2009wacky}
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta.
\newblock 2009.
\newblock The wacky wide web: a collection of very large linguistically
  processed web-crawled corpora.
\newblock {\em Language resources and evaluation}, 43:209--226.

\bibitem[\protect\citename{Bender \bgroup et al.\egroup
  }2021]{10.1145/3442188.3445922}
Emily~M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret
  Shmitchell.
\newblock 2021.
\newblock On the dangers of stochastic parrots: Can language models be too big?
\newblock In {\em Proceedings of the 2021 ACM Conference on Fairness,
  Accountability, and Transparency}, FAccT '21, page 610–623, New York, NY,
  USA. Association for Computing Machinery.

\bibitem[\protect\citename{Bommasani \bgroup et al.\egroup
  }2021]{bommasani2021opportunities}
Rishi Bommasani, Drew~A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney
  von Arx, Michael~S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma
  Brunskill, et~al.
\newblock 2021.
\newblock On the opportunities and risks of foundation models.
\newblock {\em arXiv preprint arXiv:2108.07258}.

\bibitem[\protect\citename{Briakou \bgroup et al.\egroup
  }2021]{briakou-etal-2021-ola}
Eleftheria Briakou, Di~Lu, Ke~Zhang, and Joel Tetreault.
\newblock 2021.
\newblock Ol{\'a}, bonjour, salve! xformal: A benchmark for multilingual
  formality style transfer.
\newblock In {\em Proceedings of the 2021 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pages 3199--3216.

\bibitem[\protect\citename{Brown \bgroup et al.\egroup
  }2020]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock 2020.
\newblock Language models are few-shot learners.
\newblock {\em Advances in neural information processing systems},
  33:1877--1901.

\bibitem[\protect\citename{Casola and Lavelli}2021]{casola-lavelli-2021-wits}
Silvia Casola and Alberto Lavelli.
\newblock 2021.
\newblock Wits: Wikipedia for italian text summarization.
\newblock In {\em CLiC-it}.

\bibitem[\protect\citename{Chowdhery \bgroup et al.\egroup
  }2022]{chowdhery2022palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
  Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian
  Gehrmann, et~al.
\newblock 2022.
\newblock Palm: Scaling language modeling with pathways.
\newblock {\em arXiv preprint arXiv:2204.02311}.

\bibitem[\protect\citename{Chung \bgroup et al.\egroup }2022]{chung2022scaling}
Hyung~Won Chung, Le~Hou, Shayne Longpre, Barret Zoph, Yi~Tay, William Fedus,
  Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et~al.
\newblock 2022.
\newblock Scaling instruction-finetuned language models.
\newblock {\em arXiv preprint arXiv:2210.11416}.

\bibitem[\protect\citename{Croce \bgroup et al.\egroup
  }2018]{croce-etal-2018-neural}
Danilo Croce, Alexandra Zelenanska, and Roberto Basili.
\newblock 2018.
\newblock Neural learning for question answering in italian.
\newblock In {\em AI* IA 2018--Advances in Artificial Intelligence: XVIIth
  International Conference of the Italian Association for Artificial
  Intelligence, Trento, Italy, November 20--23, 2018, Proceedings 17}, pages
  389--402. Springer.

\bibitem[\protect\citename{Croce \bgroup et al.\egroup
  }2019]{croce-etal-2019-deep}
Danilo Croce, Giorgio Brandi, Roberto Basili, et~al.
\newblock 2019.
\newblock Deep bidirectional transformers for italian question answering.
\newblock In {\em CLiC-it}.

\bibitem[\protect\citename{De~Mattei \bgroup et al.\egroup }2020]{de2020change}
Lorenzo De~Mattei, Michele Cafagna, Aptus AI, Felice Dell’Orletta, Malvina
  Nissim, and Albert Gatt.
\newblock 2020.
\newblock Change-it@ evalita 2020: Change headlines, adapt news, generate.
\newblock {\em Proceedings of the Seventh Evaluation Campaign of Natural
  Language Processing and Speech Tools for Italian (EVALITA 2020)}, 2765.

\bibitem[\protect\citename{Dettmers \bgroup et al.\egroup
  }2022]{dettmers2022llm}
Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
\newblock 2022.
\newblock Llm. int8 (): 8-bit matrix multiplication for transformers at scale.
\newblock {\em arXiv preprint arXiv:2208.07339}.

\bibitem[\protect\citename{Devlin \bgroup et al.\egroup
  }2019]{devlin-etal-2019-bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock 2019.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In {\em Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pages 4171--4186,
  Minneapolis, Minnesota, June. Association for Computational Linguistics.

\bibitem[\protect\citename{Dosovitskiy \bgroup et al.\egroup
  }2021]{dosovitskiy2021an}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
\newblock 2021.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[\protect\citename{Holtzman \bgroup et al.\egroup }2020]{topp}
Ari Holtzman, Jan Buys, Li~Du, Maxwell Forbes, and Yejin Choi.
\newblock 2020.
\newblock The curious case of neural text degeneration.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[\protect\citename{Hu \bgroup et al.\egroup }2022]{hu2022lora}
Edward~J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean
  Wang, Lu~Wang, and Weizhu Chen.
\newblock 2022.
\newblock Lo{RA}: Low-rank adaptation of large language models.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[\protect\citename{Ji \bgroup et al.\egroup }2023]{10.1145/3571730}
Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii,
  Ye~Jin Bang, Andrea Madotto, and Pascale Fung.
\newblock 2023.
\newblock Survey of hallucination in natural language generation.
\newblock {\em ACM Comput. Surv.}, 55(12), mar.

\bibitem[\protect\citename{La~Quatra and Cagliero}2023]{fi15010015}
Moreno La~Quatra and Luca Cagliero.
\newblock 2023.
\newblock Bart-it: An efficient sequence-to-sequence model for italian text
  summarization.
\newblock {\em Future Internet}, 15(1).

\bibitem[\protect\citename{Lewis \bgroup et al.\egroup
  }2020]{lewis-etal-2020-bart}
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,
  Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer.
\newblock 2020.
\newblock {BART}: Denoising sequence-to-sequence pre-training for natural
  language generation, translation, and comprehension.
\newblock In {\em Proceedings of the 58th Annual Meeting of the Association for
  Computational Linguistics}, pages 7871--7880, Online, July. Association for
  Computational Linguistics.

\bibitem[\protect\citename{Lhoest \bgroup et al.\egroup
  }2021]{lhoest-etal-2021-datasets}
Quentin Lhoest, Albert Villanova~del Moral, Yacine Jernite, Abhishek Thakur,
  Patrick von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu,
  Lewis Tunstall, Joe Davison, Mario {\v{S}}a{\v{s}}ko, Gunjan Chhablani,
  Bhavitvya Malik, Simon Brandeis, Teven Le~Scao, Victor Sanh, Canwen Xu,
  Nicolas Patry, Angelina McMillan-Major, Philipp Schmid, Sylvain Gugger,
  Cl{\'e}ment Delangue, Th{\'e}o Matussi{\`e}re, Lysandre Debut, Stas Bekman,
  Pierric Cistac, Thibault Goehringer, Victor Mustar, Fran{\c{c}}ois Lagunas,
  Alexander Rush, and Thomas Wolf.
\newblock 2021.
\newblock Datasets: A community library for natural language processing.
\newblock In {\em Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing: System Demonstrations}, pages 175--184, Online
  and Punta Cana, Dominican Republic, November. Association for Computational
  Linguistics.

\bibitem[\protect\citename{Lin}2004]{lin-2004-rouge}
Chin-Yew Lin.
\newblock 2004.
\newblock {ROUGE}: A package for automatic evaluation of summaries.
\newblock In {\em Text Summarization Branches Out}, pages 74--81, Barcelona,
  Spain, July. Association for Computational Linguistics.

\bibitem[\protect\citename{Loshchilov and Hutter}2018]{loshchilov2018decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock 2018.
\newblock Decoupled weight decay regularization.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[\protect\citename{Mangrulkar \bgroup et al.\egroup }2022]{peft}
Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, and Sayak
  Paul.
\newblock 2022.
\newblock Peft: State-of-the-art parameter-efficient fine-tuning methods.
\newblock \url{https://github.com/huggingface/peft}.

\bibitem[\protect\citename{Mattei \bgroup et al.\egroup }2020]{geppetto}
Lorenzo~De Mattei, Michele Cafagna, Felice Dell'Orletta, Malvina Nissim, and
  Marco Guerini.
\newblock 2020.
\newblock Geppetto carves italian into a language model.
\newblock In Johanna Monti, Felice Dell'Orletta, and Fabio Tamburini, editors,
  {\em Proceedings of the Seventh Italian Conference on Computational
  Linguistics, CLiC-it 2020, Bologna, Italy, March 1-3, 2021}, volume 2769 of
  {\em {CEUR} Workshop Proceedings}. CEUR-WS.org.

\bibitem[\protect\citename{Michael}2023]{stambecco}
Michael.
\newblock 2023.
\newblock Stambecco: Italian instruction-following llama model.
\newblock \url{https://github.com/mchl-labs/stambecco}.

\bibitem[\protect\citename{OpenAI}2023]{openai2023gpt4}
OpenAI.
\newblock 2023.
\newblock Gpt-4 technical report.

\bibitem[\protect\citename{Peng \bgroup et al.\egroup
  }2023]{peng2023instruction}
Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao.
\newblock 2023.
\newblock Instruction tuning with gpt-4.
\newblock {\em arXiv preprint arXiv:2304.03277}.

\bibitem[\protect\citename{Postolache \bgroup et al.\egroup }2023]{postolache}
Emilian Postolache, Giorgio Mariani, Michele Mancusi, Andrea Santilli, Luca
  Cosmo, and Emanuele Rodolà.
\newblock 2023.
\newblock Latent autoregressive source separation.
\newblock {\em Proceedings of the AAAI Conference on Artificial Intelligence},
  37(8):9444--9452, Jun.

\bibitem[\protect\citename{Rajpurkar \bgroup et al.\egroup
  }2016]{rajpurkar-etal-2016-squad}
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.
\newblock 2016.
\newblock Squad: 100,000+ questions for machine comprehension of text.
\newblock In {\em Proceedings of the 2016 Conference on Empirical Methods in
  Natural Language Processing}, pages 2383--2392.

\bibitem[\protect\citename{Rao and Tetreault}2018]{rao-tetreault-2018-dear}
Sudha Rao and Joel Tetreault.
\newblock 2018.
\newblock Dear sir or madam, may i introduce the gyafc dataset: Corpus,
  benchmarks and metrics for formality style transfer.
\newblock In {\em Proceedings of the 2018 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long Papers)}, pages 129--140.

\bibitem[\protect\citename{Riabi \bgroup et al.\egroup
  }2021]{riabi-etal-2021-synthetic}
Arij Riabi, Thomas Scialom, Rachel Keraron, Beno{\^\i}t Sagot, Djam{\'e}
  Seddah, and Jacopo Staiano.
\newblock 2021.
\newblock Synthetic data augmentation for zero-shot cross-lingual question
  answering.
\newblock In {\em Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pages 7016--7030.

\bibitem[\protect\citename{Sanh \bgroup et al.\egroup }2022]{sanh2022multitask}
Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid
  Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M~Saiful
  Bari, Canwen Xu, Urmish Thakker, Shanya~Sharma Sharma, Eliza Szczechla,
  Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang,
  Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng~Xin Yong,
  Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen,
  Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason~Alan Fries, Ryan
  Teehan, Teven~Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander~M
  Rush.
\newblock 2022.
\newblock Multitask prompted training enables zero-shot task generalization.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[\protect\citename{Santilli \bgroup et al.\egroup
  }2023]{santilli-etal-2023-accelerating}
Andrea Santilli, Silvio Severino, Emilian Postolache, Valentino Maiorca,
  Michele Mancusi, Riccardo Marin, and Emanuele Rodola.
\newblock 2023.
\newblock Accelerating transformer inference for translation via parallel
  decoding.
\newblock In {\em Proceedings of the 61st Annual Meeting of the Association for
  Computational Linguistics (Volume 1: Long Papers)}, pages 12336--12355,
  Toronto, Canada, July. Association for Computational Linguistics.

\bibitem[\protect\citename{Sarti and Nissim}2022]{sarti2022it5}
Gabriele Sarti and Malvina Nissim.
\newblock 2022.
\newblock It5: Large-scale text-to-text pretraining for italian language
  understanding and generation.
\newblock {\em arXiv preprint arXiv:2203.03759}.

\bibitem[\protect\citename{Scaiella \bgroup et al.\egroup
  }2019]{scaiella_et_al:2019}
Antonio Scaiella, Danilo Croce, and Roberto Basili.
\newblock 2019.
\newblock Large scale datasets for image and video captioning in italian.
\newblock {\em Italian Journal of Computational Linguistics}, 2(5):49--60.

\bibitem[\protect\citename{Scao \bgroup et al.\egroup }2022]{scao2022bloom}
Teven~Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili{\'c},
  Daniel Hesslow, Roman Castagn{\'e}, Alexandra~Sasha Luccioni, Fran{\c{c}}ois
  Yvon, Matthias Gall{\'e}, et~al.
\newblock 2022.
\newblock Bloom: A 176b-parameter open-access multilingual language model.
\newblock {\em arXiv preprint arXiv:2211.05100}.

\bibitem[\protect\citename{Sheng \bgroup et al.\egroup
  }2019]{sheng-etal-2019-woman}
Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng.
\newblock 2019.
\newblock The woman worked as a babysitter: On biases in language generation.
\newblock In {\em Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing and the 9th International Joint Conference on
  Natural Language Processing (EMNLP-IJCNLP)}, pages 3407--3412, Hong Kong,
  China, November. Association for Computational Linguistics.

\bibitem[\protect\citename{Taori \bgroup et al.\egroup }2023]{alpaca}
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos
  Guestrin, Percy Liang, and Tatsunori~B. Hashimoto.
\newblock 2023.
\newblock Stanford alpaca: An instruction-following llama model.
\newblock \url{https://github.com/tatsu-lab/stanford_alpaca}.

\bibitem[\protect\citename{Touvron \bgroup et al.\egroup
  }2023]{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
  Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric
  Hambro, Faisal Azhar, et~al.
\newblock 2023.
\newblock Llama: Open and efficient foundation language models.
\newblock {\em arXiv preprint arXiv:2302.13971}.

\bibitem[\protect\citename{Trappolini \bgroup et al.\egroup
  }2023]{10.1145/3539618.3591930}
Giovanni Trappolini, Andrea Santilli, Emanuele Rodol\`{a}, Alon Halevy, and
  Fabrizio Silvestri.
\newblock 2023.
\newblock Multimodal neural databases.
\newblock In {\em Proceedings of the 46th International ACM SIGIR Conference on
  Research and Development in Information Retrieval}, SIGIR '23, page
  2619–2628, New York, NY, USA. Association for Computing Machinery.

\bibitem[\protect\citename{Wang \bgroup et al.\egroup
  }2023]{wang-etal-2023-self-instruct}
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah~A. Smith, Daniel
  Khashabi, and Hannaneh Hajishirzi.
\newblock 2023.
\newblock Self-instruct: Aligning language models with self-generated
  instructions.
\newblock In {\em Proceedings of the 61st Annual Meeting of the Association for
  Computational Linguistics (Volume 1: Long Papers)}, pages 13484--13508,
  Toronto, Canada, July. Association for Computational Linguistics.

\bibitem[\protect\citename{Wang}2023]{alpaca-lora}
Eric~J. Wang.
\newblock 2023.
\newblock Alpaca-lora.
\newblock \url{https://github.com/tloen/alpaca-lora}.

\bibitem[\protect\citename{Wei \bgroup et al.\egroup }2021]{wei2021finetuned}
Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams~Wei Yu, Brian Lester,
  Nan Du, Andrew~M Dai, and Quoc~V Le.
\newblock 2021.
\newblock Finetuned language models are zero-shot learners.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[\protect\citename{Wolf \bgroup et al.\egroup }2020]{transformersLib}
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
  Anthony Moi, Perric Cistac, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,
  Teven Le~Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and
  Alexander~M. Rush.
\newblock 2020.
\newblock {Transformers: State-of-the-Art Natural Language Processing}.
\newblock pages 38--45. Association for Computational Linguistics, October.

\bibitem[\protect\citename{Xu \bgroup et al.\egroup }2023]{xu2023baize}
Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley.
\newblock 2023.
\newblock Baize: An open-source chat model with parameter-efficient tuning on
  self-chat data.

\bibitem[\protect\citename{Xue \bgroup et al.\egroup }2021]{xue-etal-2021-mt5}
Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya
  Siddhant, Aditya Barua, and Colin Raffel.
\newblock 2021.
\newblock m{T}5: A massively multilingual pre-trained text-to-text transformer.
\newblock In {\em Proceedings of the 2021 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pages 483--498, Online, June. Association for Computational
  Linguistics.

\bibitem[\protect\citename{Zhang \bgroup et al.\egroup
  }2019]{zhang2019bertscore}
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian~Q Weinberger, and Yoav Artzi.
\newblock 2019.
\newblock Bertscore: Evaluating text generation with bert.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[\protect\citename{Zhang \bgroup et al.\egroup }2022]{zhang2022opt}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
  Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al.
\newblock 2022.
\newblock Opt: Open pre-trained transformer language models.
\newblock {\em arXiv preprint arXiv:2205.01068}.

\bibitem[\protect\citename{Zheng \bgroup et al.\egroup }2023]{zheng2023judging}
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao
  Zhuang, Zi~Lin, Zhuohan Li, Dacheng Li, Eric.~P Xing, Hao Zhang, Joseph~E.
  Gonzalez, and Ion Stoica.
\newblock 2023.
\newblock Judging llm-as-a-judge with mt-bench and chatbot arena.

\end{thebibliography}
