\appendix
\label{section-appendix}

\section{Configurations of Various Methods}\label{section-appendix-configurations}
\tabref{tab:searched_ranges_related_methods} shows the searched ranges and related methods in the grid search.
 
\begin{table}[!htbp]
\centering
\caption{Searched ranges of hyperparameters and the related methods.}
\label{tab:searched_ranges_related_methods}
\setlength{\tabcolsep}{2.0mm}
{
\begin{tabular}{c|cc}
\hline
Hyperparameters                                                             & Searched Ranges                                                                                                                        & Related Methods                                                                                      \\ \hline
Dropout Rate                                                              & \begin{tabular}[c]{@{}c@{}}[0.0, 0.1, 0.2, 0.3, \\ 0.4, 0.5, 0.6]\end{tabular}                                                         & \begin{tabular}[c]{@{}c@{}}JODIE, DyRep, TGAT, TGN, CAWN, \\ TCL, GraphMixer, DyGFormer\end{tabular} \\
\begin{tabular}[c]{@{}c@{}}Number of \\ Sampled Neighbors\end{tabular}      & [10, 20, 30]                                                                                                                           & \begin{tabular}[c]{@{}c@{}}DyRep, TGAT, TGN, \\ TCL, GraphMixer\end{tabular}                         \\
\begin{tabular}[c]{@{}c@{}}Neighbor Sampling \\ Strategies\end{tabular}     & [uniform,recent]                                                                                                                       & \begin{tabular}[c]{@{}c@{}}DyRep, TGAT, TGN, \\ TCL, GraphMixer\end{tabular}                         \\
\begin{tabular}[c]{@{}c@{}}Number of Causal \\ Anonymous Walks\end{tabular} & [32, 64]                                                                                                                      & CAWN                                                                                                 \\
\begin{tabular}[c]{@{}c@{}}Length of Input \\ Sequences \& \\ Patch Size\end{tabular} & \begin{tabular}[c]{@{}c@{}}[32 \& 1, 64 \& 2, 128 \& 4, \\ 256 \& 8, 512 \& 16]\end{tabular}   & DyGFormer                                                                                            \\ \hline
\end{tabular}
}
\end{table}

We show the configurations of different methods as follows, where DLPP and DNPP are abbreviations for Dynamic Link Property Prediction and Dynamic Node Property Prediction. Note that when the settings of DLPP and DNPP are different, we list them separately, otherwise, we only list one item.

\begin{itemize}
    \item  \textbf{JODIE}:
    \begin{itemize}
    \item Dimension of time encoding: 100
    \item Dimension of node memory: 172
    \item Dimension of output representation: 172
    \item Memory updater: vanilla recurrent neural network
    \item Dropout rate: 0.1
    \end{itemize}

    \item  \textbf{DyRep}:
    \begin{itemize}
    \item Dimension of time encoding: 100
    \item Dimension of node memory: 172
    \item Dimension of output representation: 172
    \item Number of graph attention heads: 2
    \item Number of graph convolution layers: 1
    \item Memory updater: vanilla recurrent neural network
    \item Dropout rate: 0.1 on DLPP, 0.0 on DNPP
    \item Number of sampled neighbors: 10
    \item Neighbor sampling strategy: recent
    \end{itemize}
    
    \item  \textbf{TGAT}:
    \begin{itemize}
    \item Dimension of time encoding: 100
    \item Dimension of output representation: 172
    \item Number of graph attention heads: 2
    \item Number of graph convolution layers: 2
    \item Dropout rate: 0.1 on DLPP, 0.2 on DNPP
    \item Number of sampled neighbors: 20
    \item Neighbor sampling strategy: recent
    \end{itemize}
    
    \item  \textbf{TGN}:
    \begin{itemize}
    \item Dimension of time encoding: 100
    \item Dimension of node memory: 172
    \item Dimension of output representation: 172
    \item Number of graph attention heads: 2
    \item Number of graph convolution layers: 1
    \item Memory updater: gated recurrent unit
    \item Dropout rate: 0.1 on DLPP, 0.0 on DNPP
    \item Number of sampled neighbors: 10
    \item Neighbor sampling strategy: recent
    \end{itemize}
    
    \item  \textbf{CAWN}:
    \begin{itemize}
    \item Dimension of time encoding: 100
    \item Dimension of position encoding: 172
    \item Dimension of output representation: 172
    \item Number of attention heads for encoding walks: 8
    \item Length of each walk (including the target node): 2
    \item Time scaling factor $\alpha$: 1e-6
    \item Dropout rate: 0.1
    \item Number of causal anonymous walks: 32
    \end{itemize}
    
    \item  \textbf{TCL}:
    \begin{itemize}
    \item Dimension of time encoding: 100
    \item Dimension of depth encoding: 172
    \item Dimension of output representation: 172
    \item Number of attention heads: 2
    \item Number of Transformer layers: 2
    \item Dropout rate: 0.1 on DLPP, 0.0 on DNPP
    \item Number of sampled neighbors: 20
    \item Neighbor sampling strategy: recent
    \end{itemize}

    \item  \textbf{GraphMixer}:
    \begin{itemize}
    \item Dimension of time encoding: 100
    \item Dimension of output representation: 172
    \item Number of MLP-Mixer layers: 2
    \item Time gap $T$: 2000
    \item Dropout rate: 0.5 on DLPP, 0.1 on DNPP
    \item Number of sampled neighbors: 30
    \item Neighbor sampling strategy: recent
    \end{itemize}

    \item  \textbf{DyGFormer}:
    \begin{itemize}
    \item Dimension of time encoding: 100
    \item Dimension of neighbor co-occurrence encoding: 50
    \item Dimension of aligned encoding: 50
    \item Dimension of output representation: 172
    \item Number of attention heads: 2
    \item Number of Transformer layers: 2
    \item Dropout rate: 0.1
    \item Length of input sequences: 32 on DLPP, 256 on DNPP
    \item Patch size: 1 on DLPP, 8 on DNPP
    \end{itemize}

    \item  \textbf{Moving Average}:
    \begin{itemize}
    \item Window size: 7
    \end{itemize}  
\end{itemize}
