\appendix
\label{section-appendix}

\section{Configurations of Various Methods}\label{section-appendix-configurations}
We show the configurations of different methods as follows.

\begin{itemize}
    \item  \textbf{JODIE}:
    \begin{itemize}
    \item Dimension of time encoding: 100
    \item Dimension of node memory: 172
    \item Dimension of output representation: 172
    \item Memory updater: vanilla recurrent neural network
    \item Dropout rate: 0.1
    \end{itemize}

    \item  \textbf{DyRep}:
    \begin{itemize}
    \item Dimension of time encoding: 100
    \item Dimension of node memory: 172
    \item Dimension of output representation: 172
    \item Number of graph attention heads: 2
    \item Number of graph convolution layers: 1
    \item Memory updater: vanilla recurrent neural network
    \item Dropout rate: 0.1
    \item Number of sampled neighbors: 10
    \item Neighbor sampling strategy: recent
    \end{itemize}
    
    \item  \textbf{TGAT}:
    \begin{itemize}
    \item Dimension of time encoding: 100
    \item Dimension of output representation: 172
    \item Number of graph attention heads: 2
    \item Number of graph convolution layers: 2
    \item Dropout rate: 0.1
    \item Number of sampled neighbors: 20
    \item Neighbor sampling strategy: recent
    \end{itemize}
    
    \item  \textbf{TGN}:
    \begin{itemize}
    \item Dimension of time encoding: 100
    \item Dimension of node memory: 172
    \item Dimension of output representation: 172
    \item Number of graph attention heads: 2
    \item Number of graph convolution layers: 1
    \item Memory updater: gated recurrent unit
    \item Dropout rate: 0.1
    \item Number of sampled neighbors: 10
    \item Neighbor sampling strategy: recent
    \end{itemize}
    
    \item  \textbf{CAWN}:
    \begin{itemize}
    \item Dimension of time encoding: 100
    \item Dimension of position encoding: 172
    \item Dimension of output representation: 172
    \item Number of attention heads for encoding walks: 8
    \item Length of each walk (including the target node): 2
    \item Time scaling factor $\alpha$: 1e-6
    \item Dropout rate: 0.1
    \item Number of causal anonymous walks: 32
    \end{itemize}
    
    \item  \textbf{TCL}:
    \begin{itemize}
    \item Dimension of time encoding: 100
    \item Dimension of depth encoding: 172
    \item Dimension of output representation: 172
    \item Number of attention heads: 2
    \item Number of Transformer layers: 2
    \item Dropout rate: 0.1
    \item Number of sampled neighbors: 20
    \item Neighbor sampling strategy: recent
    \end{itemize}

    \item  \textbf{GraphMixer}:
    \begin{itemize}
    \item Dimension of time encoding: 100
    \item Dimension of output representation: 172
    \item Number of MLP-Mixer layers: 2
    \item Time gap $T$: 2000
    \item Dropout rate: 0.5
    \item Number of sampled neighbors: 30
    \item Neighbor sampling strategy: recent
    \end{itemize}

    \item  \textbf{DyGFormer}:
    \begin{itemize}
    \item Dimension of time encoding: 100
    \item Dimension of neighbor co-occurrence encoding: 50
    \item Dimension of aligned encoding: 50
    \item Dimension of output representation: 172
    \item Number of attention heads: 2
    \item Number of Transformer layers: 2
    \item Dropout rate: 0.1
    \item Length of input sequences: 32
    \item Patch size: 1
    \end{itemize}

    \item  \textbf{Moving Average}:
    \begin{itemize}
    \item Window size: 7
    \end{itemize}  
\end{itemize}
