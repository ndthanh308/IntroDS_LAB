\begin{thebibliography}{10}
\providecommand{\url}[1]{\texttt{#1}}
\providecommand{\urlprefix}{URL }
\providecommand{\doi}[1]{https://doi.org/#1}

\bibitem{adams_2023_proxy}
Adams-Prassl, J., Binns, R., Kelly-Lyth, A.: Directly discriminatory
  algorithms. The Modern Law Review  \textbf{86}(1),  144--175 (2023)

\bibitem{angwin_2016_compas}
Angwin, J., Kirchner, L., Surya, M., Larson, J.: {Machine Bias. There's
  software used across the country to predict future criminals. And it's biased
  against blacks}. In: ProPublica (2016)

\bibitem{arjovsky_towards_2017}
Arjovsky, M., Bottou, L.: {Towards Principled Methods for Training Generative
  Adversarial Networks}. In: International Conference on Learning
  Representations (2017)

\bibitem{arjovsky_wasserstein_2017}
Arjovsky, M., Chintala, S., Bottou, L.: {W}asserstein generative adversarial
  networks. In: Proceedings of the 34th International Conference on Machine
  Learning (2017)

\bibitem{balagopalan_fairness_2022}
Balagopalan, A., Zhang, H., Hamidieh, K., Hartvigsen, T., Rudzicz, F.,
  Ghassemi, M.: {The Road to Explainability is Paved with Bias: Measuring the
  Fairness of Explanations}. In: {Proceedings of the Conference on Fairness,
  Accountability, and Transparency} (2022)

\bibitem{barocas_fairml_2019}
Barocas, S., Hardt, M., Narayanan, A.: Fairness and Machine Learning.
  fairmlbook.org (2019)

\bibitem{barocas_law_2016}
Barocas, S., Selbst, A.D.: {Big Data's Disparate Impact}. California Law Review
   \textbf{104}(3),  671--732 (2016)

\bibitem{binns_fairness_2018}
Binns, R.: Fairness in machine learning: {L}essons from political philosophy.
  In: {Proceedings of the Conference on Fairness, Accountability, and
  Transparency} (2018)

\bibitem{binns_apparent_2020}
Binns, R.: On the apparent conflict between individual and group fairness. In:
  {Proceedings of the Conference on Fairness, Accountability, and Transparency}
  (2020)

\bibitem{borisov_deep_2021}
Borisov, V., Leemann, T., Se{\ss}ler, K., Haug, J., Pawelczyk, M., Kasneci, G.:
  {Deep Neural Networks and Tabular Data: A Survey}. arXiv preprint: 2110.01889
   (2021)

\bibitem{buolamwini_gender_2018}
Buolamwini, J., Gebru, T.: Gender shades: {I}ntersectional accuracy disparities
  in commercial gender classification. In: {Proceedings of the Conference on
  Fairness, Accountability, and Transparency} (2018)

\bibitem{chen_gender_2018}
Chen, L., Ma, R., Hann\'{a}k, A., Wilson, C.: Investigating the impact of
  gender on rank in resume search engines. In: Proceedings of the CHI
  Conference on Human Factors in Computing Systems (2018)

\bibitem{corbett_measure_2018}
Corbett-Davies, S., Goel, S.: The measure and mismeasure of fairness: A
  critical review of fair machine learning. arXiv preprint: 1808.00023  (2018)

\bibitem{crenshaw_1991_intersection}
Crenshaw, K.: {Mapping the Margins: Intersectionality, Identity Politics, and
  Violence against Women of Color}. Stanford Law Review  \textbf{43}(6),
  1241--1299 (1991)

\bibitem{dua_adult_2017}
Dua, D., Graff, C.: {UCI} machine learning repository (2017)

\bibitem{dwork_2012_fairness}
Dwork, C., Hardt, M., Pitassi, T., Reingold, O., Zemel, R.: Fairness through
  awareness. In: Proceedings of the 3rd innovations in theoretical computer
  science conference (2012)

\bibitem{engelmann_conditional_2021}
Engelmann, J., Lessmann, S.: Conditional {Wasserstein} {GAN}-based oversampling
  of tabular data for imbalanced learning. Expert Systems with Applications
  \textbf{174},  114582 (2021)

\bibitem{fazelpour2021algorithmic}
Fazelpour, S., Danks, D.: Algorithmic bias: Senses, sources, solutions.
  Philosophy Compass  \textbf{16}(8),  e12760 (2021)

\bibitem{friedler_possibility_2016}
Friedler, S.A., Scheidegger, C., Venkatasubramanian, S.: On the (im)
  possibility of fairness. arXiv preprint: 1609.07236  (2016)

\bibitem{goethals_2022_precof}
Goethals, S., Martens, D., Calders, T.: {PreCoF: Counterfactual Explanations
  for Fairness}. Working paper  (2022)

\bibitem{goodfellow_gan_2014}
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
  S., Courville, A., Bengio, Y.: Generative adversarial nets. In: Advances in
  Neural Information Processing Systems (2014)

\bibitem{gulrajani_improvedwgan_2017}
Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., Courville, A.C.:
  {Improved Training of Wasserstein GANs}. In: Advances in Neural Information
  Processing Systems (2017)

\bibitem{jang_gumbel_2017}
Jang, E., Gu, S., Poole, B.: {Categorical Reparameterization with
  Gumbel-Softmax}. In: International Conference on Learning Representations
  (2017)

\bibitem{kearns_2018_gerrymandering}
Kearns, M., Neel, S., Roth, A., Wu, Z.S.: {Preventing Fairness Gerrymandering:
  Auditing and Learning for Subgroup Fairness}. In: Proceedings of the 35th
  International Conference on Machine Learning (2018)

\bibitem{kleinberg_simple_2019}
Kleinberg, J., Mullainathan, S.: {Simplicity Creates Inequity: Implications for
  Fairness, Stereotypes, and Interpretability}. In: {Proceedings of the ACM
  Conference on Economics and Computation} (2019)

\bibitem{kleinberg_inherent_2016}
Kleinberg, J., Mullainathan, S., Raghavan, M.: Inherent trade-offs in the fair
  determination of risk scores. arXiv preprint: 1609.05807  (2016)

\bibitem{kong_intersectionally_2022}
Kong, Y.: {Are “Intersectionally Fair” AI Algorithms Really Fair to Women
  of Color? A Philosophical Analysis}. In: {Proceedings of the Conference on
  Fairness, Accountability, and Transparency} (2022)

\bibitem{kusner_2017_counterfactual}
Kusner, M.J., Loftus, J., Russell, C., Silva, R.: Counterfactual fairness. In:
  Advances in Neural Information Processing Systems (2017)

\bibitem{lang_explaining_2021}
Lang, O., Gandelsman, Y., Yarom, M., Wald, Y., Elidan, G., Hassidim, A.,
  Freeman, W.T., Isola, P., Globerson, A., Irani, M., et~al.: {Explaining in
  Style: Training a GAN to explain a classifier in StyleSpace}. In:
  {Proceedings of the IEEE/CVF International Conference on Computer Vision}
  (2021)

\bibitem{ledford_millions_2019}
Ledford, H.: Millions of black people affected by racial bias in health-care
  algorithms. Nature  \textbf{574}(7780),  608--610 (2019)

\bibitem{lin_pacgan_2018}
Lin, Z., Khetan, A., Fanti, G., Oh, S.: Pac{GAN}: The power of two samples in
  generative adversarial networks. In: Advances in Neural Information
  Processing Systems (2018)

\bibitem{lindholm_discrimination_2022}
Lindholm, M., Richman, R., Tsanakas, A., Wüthrich, M.: Discrimination--free
  insurance pricing. {ASTIN Bulletin}  \textbf{52}(1),  55--89 (2022)

\bibitem{lum_2022_debias}
Lum, K., Zhang, Y., Bower, A.: De-biasing “bias” measurement. In:
  {Proceedings of the Conference on Fairness, Accountability, and Transparency}
  (2022)

\bibitem{makhlouf_fairness_2021}
Makhlouf, K., Zhioua, S., Palamidessi, C.: On the applicability of machine
  learning fairness notions. ACM SIGKDD Explorations Newsletter
  \textbf{23}(1),  14--23 (2021)

\bibitem{mazijn_score_2021}
Mazijn, C., Danckaert, J., Ginis, V.: {How Do the Score Distributions of
  Subpopulations Influence Fairness Notions?} In: Proceedings of the 2021
  AAAI/ACM Conference on AI, Ethics, and Society (2021)

\bibitem{mazijn_sets_2022}
Mazijn, C., Prunkl, C., Algaba, A., Danckaert, J., Ginis, V.: {LUCID: Exposing
  Algorithmic Bias through Inverse Design}. In: Proceedings of the 37th AAAI
  Conference on Artificial Intelligence (2023)

\bibitem{meng_mimic_2022}
Meng, C., Trinh, L., Xu, N., Enouen, J., Liu, Y.: Interpretability and fairness
  evaluation of deep learning models on {MIMIC-IV} dataset. Scientific Reports
  \textbf{12}(7166) (2022)

\bibitem{mirza_conditional_2014}
Mirza, M., Osindero, S.: Conditional {Generative} {Adversarial} {Nets}. arXiv
  preprint: 1411.1784  (2014)

\bibitem{nguyen_plug_2017}
Nguyen, A., Clune, J., Bengio, Y., Dosovitskiy, A., Yosinski, J.: Plug \&
  {Play} {Generative} {Networks}: {Conditional} {Iterative} {Generation} of
  {Images} in {Latent} {Space}. In: {Proceedings of the IEEE Conference on
  Computer Vision and Pattern Recognition} (2017)

\bibitem{nguyen_synthesizing_2016}
Nguyen, A., Dosovitskiy, A., Yosinski, J., Brox, T., Clune, J.: Synthesizing
  the preferred inputs for neurons in neural networks via deep generator
  networks. In: Advances in {Neural} {Information} {Processing} {Systems}
  (2016)

\bibitem{nguyen_deep_2015}
Nguyen, A., Yosinski, J., Clune, J.: Deep neural networks are easily fooled:
  {High} confidence predictions for unrecognizable images. In: {Proceedings of
  the IEEE Conference on Computer Vision and Pattern Recognition} (2015)

\bibitem{northcutt_pervasive_2021}
Northcutt, C.G., Athalye, A., Mueller, J.: Pervasive label errors in test sets
  destabilize machine learning benchmarks. In: Advances in {Neural}
  {Information} {Processing} {Systems} {Track on Datasets and Benchmark} (2021)

\bibitem{obermeyer_racialbias_2019}
Obermeyer, Z., Powers, B., Vogeli, C., Mullainathan, S.: Dissecting racial bias
  in an algorithm used to manage the health of populations. Science
  \textbf{366}(6464),  447--453 (2019)

\bibitem{odena_conditional_2017}
Odena, A., Olah, C., Shlens, J.: {Conditional Image Synthesis with Auxiliary
  Classifier GANs}. In: Proceedings of the 34th International Conference on
  Machine Learning (2017)

\bibitem{prince_proxy_2019}
Prince, A.E., Schwarcz, D.: {Proxy Discrimination in the Age of Artificial
  Intelligence and Big Data}. Iowa Law Review  \textbf{105},  1257--1318 (2019)

\bibitem{raghavan_hiring_2020}
Raghavan, M., Barocas, S., Kleinberg, J., Levy, K.: Mitigating bias in
  algorithmic hiring: Evaluating claims and practices. In: {Proceedings of the
  Conference on Fairness, Accountability, and Transparency} (2020)

\bibitem{shwartz_deep_2022}
Shwartz-Ziv, R., Armon, A.: {Tabular data: Deep learning is not all you need}.
  Information Fusion  \textbf{81},  84--90 (2022)

\bibitem{simonyan_deep_2014}
Simonyan, K., Vedaldi, A., Zisserman, A.: Deep inside convolutional networks:
  Visualising image classification models and saliency maps. In: Workshop at
  International Conference on Learning Representations (2014)

\bibitem{slack_fooling_2020}
Slack, D., Hilgard, S., Jia, E., Singh, S., Lakkaraju, H.: Fooling {LIME} and
  {SHAP}: {Adversarial} {Attacks} on {Post} hoc {Explanation} {Methods}. In:
  Proceedings of the {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}
  (2020)

\bibitem{wachter_2021_automate}
Wachter, S., Mittelstadt, B., Russell, C.: {Why fairness cannot be automated:
  Bridging the gap between EU non-discrimination law and AI}. Computer Law \&
  Security Review  \textbf{41},  105567 (2021)

\bibitem{xu_modeling_2019}
Xu, L., Skoularidou, M., Cuesta-Infante, A., Veeramachaneni, K.: Modeling
  {Tabular} data using {Conditional} {GAN}. In: Advances in {Neural}
  {Information} {Processing} {Systems} (2019)

\bibitem{zhao_ctab-gan_2021}
Zhao, Z., Kunar, A., Birke, R., Chen, L.Y.: {CTAB}-{GAN}: {Effective} {Table}
  {Data} {Synthesizing}. In: Proceedings of {The} 13th {Asian} {Conference} on
  {Machine} {Learning} (2021)

\bibitem{zhou_activation_2018}
Zhou, Z., Cai, H., Rong, S., Song, Y., Ren, K., Zhang, W., Wang, J., Yu, Y.:
  Activation maximization generative adversarial nets. In: Proceedings of the
  35th International Conference on Learning Representations (2018)

\end{thebibliography}
