{
  "title": "The Optimal Approximation Factors in Misspecified Off-Policy Value Function Estimation",
  "authors": [
    "Philip Amortila",
    "Nan Jiang",
    "Csaba Szepesvári"
  ],
  "submission_date": "2023-07-25T08:44:58+00:00",
  "revised_dates": [
    "2023-12-18T01:10:06+00:00"
  ],
  "abstract": "Theoretical guarantees in reinforcement learning (RL) are known to suffer multiplicative blow-up factors with respect to the misspecification error of function approximation. Yet, the nature of such \\emph{approximation factors} -- especially their optimal form in a given learning problem -- is poorly understood. In this paper we study this question in linear off-policy value function estimation, where many open questions remain. We study the approximation factor in a broad spectrum of settings, such as with the weighted $L_2$-norm (where the weighting is the offline state distribution), the $L_\\infty$ norm, the presence vs. absence of state aliasing, and full vs. partial coverage of the state space. We establish the optimal asymptotic approximation factors (up to constants) for all of these settings. In particular, our bounds identify two instance-dependent factors for the $L_2(μ)$ norm and only one for the $L_\\infty$ norm, which are shown to dictate the hardness of off-policy evaluation under misspecification.",
  "categories": [
    "cs.LG",
    "cs.AI",
    "stat.ML"
  ],
  "primary_category": "cs.LG",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.13332",
  "pdf_url": null,
  "comment": "Accepted to ICML 2023. The arXiv version contains improved results",
  "num_versions": null,
  "size_before_bytes": 1171706,
  "size_after_bytes": 552232
}