\begin{thebibliography}{36}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Amortila et~al.(2020)Amortila, Jiang, and Xie]{amortila2020variant}
Amortila, P., Jiang, N., and Xie, T.
\newblock A variant of the wang-foster-kakade lower bound for the discounted
  setting.
\newblock \emph{arXiv preprint arXiv:2011.01075}, 2020.

\bibitem[Antos et~al.(2008)Antos, Szepesv{\'a}ri, and Munos]{antos2008learning}
Antos, A., Szepesv{\'a}ri, C., and Munos, R.
\newblock Learning near-optimal policies with bellman-residual minimization
  based fitted policy iteration and a single sample path.
\newblock \emph{Machine Learning}, 71:\penalty0 89--129, 2008.

\bibitem[Bertsekas \& Tsitsiklis(1996)Bertsekas and
  Tsitsiklis]{bertsekas1996neuro}
Bertsekas, D. and Tsitsiklis, J.~N.
\newblock \emph{Neuro-dynamic programming}.
\newblock Athena Scientific, 1996.

\bibitem[Bradtke \& Barto(1996)Bradtke and Barto]{bradtke1996linear}
Bradtke, S.~J. and Barto, A.~G.
\newblock Linear least-squares algorithms for temporal difference learning.
\newblock \emph{Machine learning}, 22\penalty0 (1):\penalty0 33--57, 1996.

\bibitem[Chen \& Jiang(2019)Chen and Jiang]{chen2019information}
Chen, J. and Jiang, N.
\newblock Information-theoretic considerations in batch reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1042--1051. PMLR, 2019.

\bibitem[Du et~al.(2019)Du, Kakade, Wang, and Yang]{du2019good}
Du, S.~S., Kakade, S.~M., Wang, R., and Yang, L.~F.
\newblock Is a good representation sufficient for sample efficient
  reinforcement learning?
\newblock \emph{arXiv preprint arXiv:1910.03016}, 2019.

\bibitem[Duan \& Wang(2020)Duan and Wang]{duan_minimax-optimal_2020}
Duan, Y. and Wang, M.
\newblock Minimax-{Optimal} {Off}-{Policy} {Evaluation} with {Linear}
  {Function} {Approximation}.
\newblock \emph{arXiv:2002.09516 [cs, math, stat]}, February 2020.
\newblock URL \url{http://arxiv.org/abs/2002.09516}.
\newblock arXiv: 2002.09516.

\bibitem[Duan et~al.(2021)Duan, Wang, and Wainwright]{duan_optimal_2021}
Duan, Y., Wang, M., and Wainwright, M.~J.
\newblock Optimal policy evaluation using kernel-based temporal difference
  methods.
\newblock \emph{arXiv:2109.12002 [cs, math, stat]}, September 2021.
\newblock URL \url{http://arxiv.org/abs/2109.12002}.
\newblock arXiv: 2109.12002.

\bibitem[Foster et~al.(2021)Foster, Krishnamurthy, Simchi-Levi, and
  Xu]{foster2021offline}
Foster, D.~J., Krishnamurthy, A., Simchi-Levi, D., and Xu, Y.
\newblock Offline reinforcement learning: Fundamental barriers for value
  function approximation.
\newblock \emph{arXiv preprint arXiv:2111.10919}, 2021.

\bibitem[Huang \& Jiang(2022)Huang and Jiang]{huangbeyond}
Huang, A. and Jiang, N.
\newblock Beyond the return: Off-policy function estimation under
  user-specified error-measuring distributions.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Jiang(2018)]{jiang2018notes}
Jiang, N.
\newblock Notes on state abstractions, 2018.

\bibitem[Kolter(2011)]{kolter2011fixed}
Kolter, J.
\newblock The fixed points of off-policy td.
\newblock \emph{Advances in Neural Information Processing Systems}, 24, 2011.

\bibitem[Lagoudakis \& Parr(2003)Lagoudakis and Parr]{lagoudakis2003least}
Lagoudakis, M.~G. and Parr, R.
\newblock Least-squares policy iteration.
\newblock \emph{The Journal of Machine Learning Research}, 4:\penalty0
  1107--1149, 2003.

\bibitem[Lattimore et~al.(2020)Lattimore, Szepesvari, and
  Weisz]{lattimore2020learning}
Lattimore, T., Szepesvari, C., and Weisz, G.
\newblock Learning with good feature representations in bandits and in rl with
  a generative model.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5662--5670. PMLR, 2020.

\bibitem[Li et~al.(2006)Li, Walsh, and Littman]{li2006towards}
Li, L., Walsh, T.~J., and Littman, M.~L.
\newblock Towards a unified theory of state abstraction for mdps.
\newblock In \emph{AI\&M}, 2006.

\bibitem[Liu et~al.(2018)Liu, Li, Tang, and Zhou]{liu2018breaking}
Liu, Q., Li, L., Tang, Z., and Zhou, D.
\newblock Breaking the curse of horizon: Infinite-horizon off-policy
  estimation.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Miyaguchi(2021)]{miyaguchi2021asymptotically}
Miyaguchi, K.
\newblock Asymptotically exact error characterization of offline policy
  evaluation with misspecified linear models.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 28573--28584, 2021.

\bibitem[Mou et~al.(2020)Mou, Pananjady, and Wainwright]{mou_optimal_2020}
Mou, W., Pananjady, A., and Wainwright, M.~J.
\newblock Optimal oracle inequalities for solving projected fixed-point
  equations.
\newblock \emph{arXiv:2012.05299 [cs, math, stat]}, December 2020.
\newblock URL \url{http://arxiv.org/abs/2012.05299}.
\newblock arXiv: 2012.05299.

\bibitem[Munos(2007)]{munos2007performance}
Munos, R.
\newblock Performance bounds in l\_p-norm for approximate value iteration.
\newblock \emph{SIAM journal on control and optimization}, 46\penalty0
  (2):\penalty0 541--561, 2007.

\bibitem[Ortega(1990)]{ortega1990numerical}
Ortega, J.~M.
\newblock \emph{Numerical analysis: a second course}.
\newblock SIAM, 1990.

\bibitem[Perdomo et~al.(2022)Perdomo, Krishnamurthy, Bartlett, and
  Kakade]{perdomo_sharp_2022}
Perdomo, J.~C., Krishnamurthy, A., Bartlett, P., and Kakade, S.
\newblock A {Sharp} {Characterization} of {Linear} {Estimators} for {Offline}
  {Policy} {Evaluation}.
\newblock \emph{arXiv:2203.04236 [cs, stat]}, March 2022.
\newblock URL \url{http://arxiv.org/abs/2203.04236}.
\newblock arXiv: 2203.04236.

\bibitem[Pires \& Szepesv{\'a}ri(2012)Pires and
  Szepesv{\'a}ri]{pires2012statistical}
Pires, B.~{\'A}. and Szepesv{\'a}ri, C.
\newblock Statistical linear estimation with penalized estimators: an
  application to reinforcement learning.
\newblock In \emph{Proceedings of the 29th International Coference on
  International Conference on Machine Learning}, pp.\  1755--1762, 2012.

\bibitem[Puterman(2014)]{puterman2014markov}
Puterman, M.~L.
\newblock \emph{Markov decision processes: discrete stochastic dynamic
  programming}.
\newblock John Wiley \& Sons, 2014.

\bibitem[Scherrer(2010)]{scherrer2010should}
Scherrer, B.
\newblock Should one compute the temporal difference fix point or minimize the
  bellman residual? the unified oblique projection view.
\newblock \emph{arXiv preprint arXiv:1011.4362}, 2010.

\bibitem[Shalev-Shwartz \& Ben-David(2014)Shalev-Shwartz and
  Ben-David]{shalev2014understanding}
Shalev-Shwartz, S. and Ben-David, S.
\newblock \emph{Understanding machine learning: From theory to algorithms}.
\newblock Cambridge university press, 2014.

\bibitem[Sutton \& Barto(2018)Sutton and Barto]{sutton2018reinforcement}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Szepesv{\'a}ri(2010)]{szepesvari2010algorithms}
Szepesv{\'a}ri, C.
\newblock Algorithms for reinforcement learning.
\newblock \emph{Synthesis lectures on artificial intelligence and machine
  learning}, 4\penalty0 (1):\penalty0 1--103, 2010.

\bibitem[Tsitsiklis \& Van~Roy(1997)Tsitsiklis and
  Van~Roy]{tsitsiklis_analysis_1997}
Tsitsiklis, J. and Van~Roy, B.
\newblock An analysis of temporal-difference learning with function
  approximation.
\newblock \emph{IEEE Transactions on Automatic Control}, 42\penalty0
  (5):\penalty0 674--690, May 1997.
\newblock ISSN 00189286.
\newblock \doi{10.1109/9.580874}.
\newblock URL \url{http://ieeexplore.ieee.org/document/580874/}.

\bibitem[Tu \& Recht(2018)Tu and Recht]{tu2018least}
Tu, S. and Recht, B.
\newblock Least-squares temporal difference learning for the linear quadratic
  regulator.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5005--5014. PMLR, 2018.

\bibitem[Uehara et~al.(2020)Uehara, Huang, and Jiang]{uehara_minimax_2020}
Uehara, M., Huang, J., and Jiang, N.
\newblock Minimax {Weight} and {Q}-{Function} {Learning} for {Off}-{Policy}
  {Evaluation}.
\newblock \emph{arXiv:1910.12809 [cs, stat]}, October 2020.
\newblock URL \url{http://arxiv.org/abs/1910.12809}.
\newblock arXiv: 1910.12809.

\bibitem[Uehara et~al.(2021)Uehara, Zhang, and Sun]{uehara2021representation}
Uehara, M., Zhang, X., and Sun, W.
\newblock Representation learning for online and offline rl in low-rank mdps.
\newblock \emph{arXiv preprint arXiv:2110.04652}, 2021.

\bibitem[Wainwright(2019)]{wainwright2019high}
Wainwright, M.~J.
\newblock \emph{High-dimensional statistics: A non-asymptotic viewpoint},
  volume~48.
\newblock Cambridge university press, 2019.

\bibitem[Wang et~al.(2020)Wang, Foster, and Kakade]{wang2020statistical}
Wang, R., Foster, D.~P., and Kakade, S.~M.
\newblock What are the statistical limits of offline rl with linear function
  approximation?
\newblock \emph{arXiv preprint arXiv:2010.11895}, 2020.

\bibitem[Weisz et~al.(2021)Weisz, Amortila, and
  Szepesv{\'a}ri]{weisz2021exponential}
Weisz, G., Amortila, P., and Szepesv{\'a}ri, C.
\newblock Exponential lower bounds for planning in mdps with
  linearly-realizable optimal action-value functions.
\newblock In \emph{Algorithmic Learning Theory}, pp.\  1237--1264. PMLR, 2021.

\bibitem[Xie \& Jiang(2021)Xie and Jiang]{xie2021batch}
Xie, T. and Jiang, N.
\newblock Batch value-function approximation with only realizability.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  11404--11413. PMLR, 2021.

\bibitem[Yu \& Bertsekas(2010)Yu and Bertsekas]{yu_error_2010}
Yu, H. and Bertsekas, D.~P.
\newblock Error {Bounds} for {Approximations} from {Projected} {Linear}
  {Equations}.
\newblock \emph{Mathematics of Operations Research}, 35\penalty0 (2):\penalty0
  306--329, May 2010.
\newblock ISSN 0364-765X, 1526-5471.
\newblock \doi{10.1287/moor.1100.0441}.
\newblock URL
  \url{http://pubsonline.informs.org/doi/abs/10.1287/moor.1100.0441}.

\end{thebibliography}
