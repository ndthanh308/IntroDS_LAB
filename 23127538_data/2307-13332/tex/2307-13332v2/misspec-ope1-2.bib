@article{amortila2020variant,
  title={A variant of the wang-foster-kakade lower bound for the discounted setting},
  author={Amortila, Philip and Jiang, Nan and Xie, Tengyang},
  journal={arXiv preprint arXiv:2011.01075},
  year={2020}
}

@article{antos2008learning,
  title={Learning near-optimal policies with Bellman-residual minimization based fitted policy iteration and a single sample path},
  author={Antos, Andr{\'a}s and Szepesv{\'a}ri, Csaba and Munos, R{\'e}mi},
  journal={Machine Learning},
  volume={71},
  pages={89--129},
  year={2008},
  publisher={Springer}
}

@inproceedings{uehara2020minimax,
  title={Minimax weight and q-function learning for off-policy evaluation},
  author={Uehara, Masatoshi and Huang, Jiawei and Jiang, Nan},
  booktitle={International Conference on Machine Learning},
  pages={9659--9668},
  year={2020},
  organization={PMLR}
}

@article{liu2018breaking,
  title={Breaking the curse of horizon: Infinite-horizon off-policy estimation},
  author={Liu, Qiang and Li, Lihong and Tang, Ziyang and Zhou, Dengyong},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}



@article{munos2007performance,
  title={Performance bounds in l\_p-norm for approximate value iteration},
  author={Munos, R{\'e}mi},
  journal={SIAM journal on control and optimization},
  volume={46},
  number={2},
  pages={541--561},
  year={2007},
  publisher={SIAM}
}

@book{puterman2014markov,
  title={Markov decision processes: discrete stochastic dynamic programming},
  author={Puterman, Martin L},
  year={2014},
  publisher={John Wiley \& Sons}
}
@article{szepesvari2010algorithms,
  title={Algorithms for reinforcement learning},
  author={Szepesv{\'a}ri, Csaba},
  journal={Synthesis lectures on artificial intelligence and machine learning},
  volume={4},
  number={1},
  pages={1--103},
  year={2010},
  publisher={Morgan \& Claypool Publishers}
}
@inproceedings{weisz2021exponential,
  title={Exponential lower bounds for planning in mdps with linearly-realizable optimal action-value functions},
  author={Weisz, Gell{\'e}rt and Amortila, Philip and Szepesv{\'a}ri, Csaba},
  booktitle={Algorithmic Learning Theory},
  pages={1237--1264},
  year={2021},
  organization={PMLR}
}
@article{foster2021offline,
  title={Offline reinforcement learning: Fundamental barriers for value function approximation},
  author={Foster, Dylan J and Krishnamurthy, Akshay and Simchi-Levi, David and Xu, Yunzong},
  journal={arXiv preprint arXiv:2111.10919},
  year={2021}
}
@article{lagoudakis2003least,
  title={Least-squares policy iteration},
  author={Lagoudakis, Michail G and Parr, Ronald},
  journal={The Journal of Machine Learning Research},
  volume={4},
  pages={1107--1149},
  year={2003},
  publisher={JMLR. org}
}
@book{wainwright2019high,
  title={High-dimensional statistics: A non-asymptotic viewpoint},
  author={Wainwright, Martin J},
  volume={48},
  year={2019},
  publisher={Cambridge university press}
}
@book{shalev2014understanding,
  title={Understanding machine learning: From theory to algorithms},
  author={Shalev-Shwartz, Shai and Ben-David, Shai},
  year={2014},
  publisher={Cambridge university press}
}
@book{bertsekas1996neuro,
  title={Neuro-dynamic programming},
  author={Bertsekas, Dimitri and Tsitsiklis, John N},
  year={1996},
  publisher={Athena Scientific}
}
@inproceedings{pires2012statistical,
  title={Statistical linear estimation with penalized estimators: an application to reinforcement learning},
  author={Pires, Bernardo {\'A}vila and Szepesv{\'a}ri, Csaba},
  booktitle={Proceedings of the 29th International Coference on International Conference on Machine Learning},
  pages={1755--1762},
  year={2012}
}
@inproceedings{tu2018least,
  title={Least-squares temporal difference learning for the linear quadratic regulator},
  author={Tu, Stephen and Recht, Benjamin},
  booktitle={International Conference on Machine Learning},
  pages={5005--5014},
  year={2018},
  organization={PMLR}
}
@inproceedings{chen2019information,
  title={Information-theoretic considerations in batch reinforcement learning},
  author={Chen, Jinglin and Jiang, Nan},
  booktitle={International Conference on Machine Learning},
  pages={1042--1051},
  year={2019},
  organization={PMLR}
}
@article{duan_minimax-optimal_2020,
	title = {Minimax-{Optimal} {Off}-{Policy} {Evaluation} with {Linear} {Function} {Approximation}},
	url = {http://arxiv.org/abs/2002.09516},
	abstract = {This paper studies the statistical theory of batch data reinforcement learning with function approximation. Consider the off-policy evaluation problem, which is to estimate the cumulative value of a new target policy from logged history generated by unknown behavioral policies. We study a regression-based ï¬tted Q iteration method, and show that it is equivalent to a model-based method that estimates a conditional mean embedding of the transition operator. We prove that this method is information-theoretically optimal and has nearly minimal estimation error. In particular, by leveraging contraction property of Markov processes and martingale concentration, we establish a ï¬nite-sample instance-dependent error upper bound and a nearly-matching minimax lower bound. The policy evaluation error depends sharply on a restricted Ï2-divergence over the function class between the long-term distribution of the target policy and the distribution of past data. This restricted Ï2-divergence is both instance-dependent and function-class-dependent. It characterizes the statistical limit of off-policy evaluation. Further, we provide an easily computable conï¬dence bound for the policy evaluator, which may be useful for optimistic planning and safe policy improvement.},
	language = {en},
	urldate = {2022-02-23},
	journal = {arXiv:2002.09516 [cs, math, stat]},
	author = {Duan, Yaqi and Wang, Mengdi},
	month = feb,
	year = {2020},
	note = {arXiv: 2002.09516},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, Mathematics - Statistics Theory},
	annote = {Needs closure
Â 
Â },
	file = {Duan and Wang - 2020 - Minimax-Optimal Off-Policy Evaluation with Linear .pdf:/Users/philipamortila/Zotero/storage/S5E6DEY4/Duan and Wang - 2020 - Minimax-Optimal Off-Policy Evaluation with Linear .pdf:application/pdf},
}

@article{mou_optimal_2020,
	title = {Optimal oracle inequalities for solving projected fixed-point equations},
	url = {http://arxiv.org/abs/2012.05299},
	abstract = {Linear ï¬xed point equations in Hilbert spaces arise in a variety of settings, including reinforcement learning, and computational methods for solving diï¬erential and integral equations. We study methods that use a collection of random observations to compute approximate solutions by searching over a known low-dimensional subspace of the Hilbert space. First, we prove an instance-dependent upper bound on the mean-squared error for a linear stochastic approximation scheme that exploits PolyakâRuppert averaging. This bound consists of two terms: an approximation error term with an instance-dependent approximation factor, and a statistical error term that captures the instance-speciï¬c complexity of the noise when projected onto the low-dimensional subspace. Using informationtheoretic methods, we also establish lower bounds showing that both of these terms cannot be improved, again in an instance-dependent sense. A concrete consequence of our characterization is that the optimal approximation factor in this problem can be much larger than a universal constant. We show how our results precisely characterize the error of a class of temporal diï¬erence learning methods for the policy evaluation problem with linear function approximation, establishing their optimality.},
	language = {en},
	urldate = {2022-02-23},
	journal = {arXiv:2012.05299 [cs, math, stat]},
	author = {Mou, Wenlong and Pananjady, Ashwin and Wainwright, Martin J.},
	month = dec,
	year = {2020},
	note = {arXiv: 2012.05299},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, Mathematics - Optimization and Control, Mathematics - Statistics Theory},
	annote = {Pretty much what we want but on-policy case},
	file = {Mou et al. - 2020 - Optimal oracle inequalities for solving projected .pdf:/Users/philipamortila/Zotero/storage/ZMUQCW5U/Mou et al. - 2020 - Optimal oracle inequalities for solving projected .pdf:application/pdf},
}

@article{wu_chebyshev_2016,
	title = {Chebyshev polynomials, moment matching, and optimal estimation of the unseen},
	url = {http://arxiv.org/abs/1504.01227},
	abstract = {We consider the problem of estimating the support size of a discrete distribution whose minimum non-zero mass is at least \$ {\textbackslash}frac\{1\}\{k\}\$. Under the independent sampling model, we show that the sample complexity, i.e., the minimal sample size to achieve an additive error of \${\textbackslash}epsilon k\$ with probability at least 0.1 is within universal constant factors of \$ {\textbackslash}frac\{k\}\{{\textbackslash}log k\}{\textbackslash}log{\textasciicircum}2{\textbackslash}frac\{1\}\{{\textbackslash}epsilon\} \$, which improves the state-of-the-art result of \$ {\textbackslash}frac\{k\}\{{\textbackslash}epsilon{\textasciicircum}2 {\textbackslash}log k\} \$ in {\textbackslash}cite\{VV13\}. Similar characterization of the minimax risk is also obtained. Our procedure is a linear estimator based on the Chebyshev polynomial and its approximation-theoretic properties, which can be evaluated in \$O(n+{\textbackslash}log{\textasciicircum}2 k)\$ time and attains the sample complexity within a factor of six asymptotically. The superiority of the proposed estimator in terms of accuracy, computational efficiency and scalability is demonstrated in a variety of synthetic and real datasets.},
	language = {en},
	urldate = {2022-02-23},
	journal = {arXiv:1504.01227 [math, stat]},
	author = {Wu, Yihong and Yang, Pengkun},
	month = dec,
	year = {2016},
	note = {arXiv: 1504.01227},
	keywords = {Mathematics - Statistics Theory},
	annote = {Chebyshev polynomials for estimating support size},
	file = {Wu and Yang - 2016 - Chebyshev polynomials, moment matching, and optima.pdf:/Users/philipamortila/Zotero/storage/RSP2GR3H/Wu and Yang - 2016 - Chebyshev polynomials, moment matching, and optima.pdf:application/pdf},
}

@article{mahmood_emphatic_nodate,
	title = {Emphatic {Temporal}-{Diï¬erence} {Learning}},
	abstract = {Emphatic algorithms are temporal-diï¬erence learning algorithms that change their effective state distribution by selectively emphasizing and de-emphasizing their updates on diï¬erent time steps. Recent works by Sutton, Mahmood and White (2015), and Yu (2015) show that by varying the emphasis in a particular way, these algorithms become stable and convergent under oï¬-policy training with linear function approximation. This paper serves as a uniï¬ed summary of the available results from both works. In addition, we demonstrate the empirical beneï¬ts from the ï¬exibility of emphatic algorithms, including state-dependent discounting, state-dependent bootstrapping, and the user-speciï¬ed allocation of function approximation resources.},
	language = {en},
	author = {Mahmood, A Rupam and Yu, Huizhen and White, Martha and Sutton, Richard S},
	pages = {9},
	annote = {Emphatic TD converges off-policy, asymptotically },
	file = {Mahmood et al. - Emphatic Temporal-Diï¬erence Learning.pdf:/Users/philipamortila/Zotero/storage/FY5HSW7W/Mahmood et al. - Emphatic Temporal-Diï¬erence Learning.pdf:application/pdf},
}

@article{kolter2011fixed,
  title={The fixed points of off-policy TD},
  author={Kolter, J},
  journal={Advances in Neural Information Processing Systems},
  volume={24},
  year={2011}
}

@misc{jiang2018notes,
  title={Notes on state abstractions},
  author={Jiang, Nan},
  year={2018}
}

@inproceedings{li2006towards,
  title={Towards a Unified Theory of State Abstraction for MDPs.},
  author={Li, Lihong and Walsh, Thomas J and Littman, Michael L},
  booktitle={AI\&M},
  year={2006}
}
@inproceedings{xie2021batch,
  title={Batch value-function approximation with only realizability},
  author={Xie, Tengyang and Jiang, Nan},
  booktitle={International Conference on Machine Learning},
  pages={11404--11413},
  year={2021},
  organization={PMLR}
}
@article{li_accelerated_2021,
	title = {Accelerated and instance-optimal policy evaluation with linear function approximation},
	url = {http://arxiv.org/abs/2112.13109},
	abstract = {We study the problem of policy evaluation with linear function approximation and present eï¬cient and practical algorithms that come with strong optimality guarantees. We begin by proving lower bounds that establish baselines on both the deterministic error and stochastic error in this problem. In particular, we prove an oracle complexity lower bound on the deterministic error in an instance-dependent norm associated with the stationary distribution of the transition kernel, and use the local asymptotic minimax machinery to prove an instance-dependent lower bound on the stochastic error in the i.i.d. observation model. Existing algorithms fail to match at least one of these lower bounds: To illustrate, we analyze a variance-reduced variant of temporal diï¬erence learning, showing in particular that it fails to achieve the oracle complexity lower bound. To remedy this issue, we develop an accelerated, variance-reduced fast temporal diï¬erence algorithm (VRFTD) that simultaneously matches both lower bounds and attains a strong notion of instance-optimality. Finally, we extend the VRFTD algorithm to the setting with Markovian observations, and provide instance-dependent convergence results that match those in the i.i.d. setting up to a multiplicative factor that is proportional to the mixing time of the chain. Our theoretical guarantees of optimality are corroborated by numerical experiments.},
	language = {en},
	urldate = {2022-02-23},
	journal = {arXiv:2112.13109 [cs, math, stat]},
	author = {Li, Tianjiao and Lan, Guanghui and Pananjady, Ashwin},
	month = dec,
	year = {2021},
	note = {arXiv: 2112.13109},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, Mathematics - Optimization and Control},
	annote = {Followup to Mou et al., still on-policy though},
	file = {Li et al. - 2021 - Accelerated and instance-optimal policy evaluation.pdf:/Users/philipamortila/Zotero/storage/IJ6JPAFM/Li et al. - 2021 - Accelerated and instance-optimal policy evaluation.pdf:application/pdf},
}

@article{miyaguchi2021asymptotically,
  title={Asymptotically exact error characterization of offline policy evaluation with misspecified linear models},
  author={Miyaguchi, Kohei},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={28573--28584},
  year={2021}
}

@article{duan_optimal_2021,
	title = {Optimal policy evaluation using kernel-based temporal difference methods},
	url = {http://arxiv.org/abs/2109.12002},
	abstract = {We study methods based on reproducing kernel Hilbert spaces for estimating the value function of an inï¬nite-horizon discounted Markov reward process (MRP). We study a regularized form of the kernel least-squares temporal diï¬erence (LSTD) estimate; in the population limit of inï¬nite data, it corresponds to the ï¬xed point of a projected Bellman operator deï¬ned by the associated reproducing kernel Hilbert space. The estimator itself is obtained by computing the projected ï¬xed point induced by a regularized version of the empirical operator; due to the underlying kernel structure, this reduces to solving a linear system involving kernel matrices. We analyze the error of this estimate in the L2(Âµ)-norm, where Âµ denotes the stationary distribution of the underlying Markov chain. Our analysis imposes no assumptions on the transition operator of the Markov chain, but rather only conditions on the reward function and population-level kernel LSTD solutions. We use empirical process theory techniques to derive a non-asymptotic upper bound on the error with explicit dependence on the eigenvalues of the associated kernel operator, as well as the instance-dependent variance of the Bellman residual error. In addition, we prove minimax lower bounds over sub-classes of MRPs, which shows that our rate is optimal in terms of the sample size n and the eï¬ective horizon H = (1 â Î³)â1. Whereas existing worstcase theory predicts cubic scaling (H3) in the eï¬ective horizon, our theory reveals that there is in fact a much wider range of scalings, depending on the kernel, the stationary distribution, and the variance of the Bellman residual error. Notably, it is only parametric and near-parametric problems that can ever achieve the worst-case cubic scaling.},
	language = {en},
	urldate = {2022-02-23},
	journal = {arXiv:2109.12002 [cs, math, stat]},
	author = {Duan, Yaqi and Wang, Mengdi and Wainwright, Martin J.},
	month = sep,
	year = {2021},
	note = {arXiv: 2109.12002},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, Mathematics - Statistics Theory},
	annote = {On-policy, just estimation error},
	file = {Duan et al. - 2021 - Optimal policy evaluation using kernel-based tempo.pdf:/Users/philipamortila/Zotero/storage/NWA4G94J/Duan et al. - 2021 - Optimal policy evaluation using kernel-based tempo.pdf:application/pdf},
}

@article{bradtke1996linear,
  title={Linear least-squares algorithms for temporal difference learning},
  author={Bradtke, Steven J and Barto, Andrew G},
  journal={Machine learning},
  volume={22},
  number={1},
  pages={33--57},
  year={1996},
  publisher={Springer}
}
@article{scherrer2010should,
  title={Should one compute the temporal difference fix point or minimize the bellman residual? the unified oblique projection view},
  author={Scherrer, Bruno},
  journal={arXiv preprint arXiv:1011.4362},
  year={2010}
}

@article{chen_well-posedness_2022,
	title = {On {Well}-posedness and {Minimax} {Optimal} {Rates} of {Nonparametric} {Q}-function {Estimation} in {Off}-policy {Evaluation}},
	url = {http://arxiv.org/abs/2201.06169},
	abstract = {We study the oï¬-policy evaluation (OPE) problem in an inï¬nite-horizon Markov decision process with continuous states and actions. We recast the Q-function estimation into a special form of the nonparametric instrumental variables (NPIV) estimation problem. We ï¬rst show that under one mild condition the NPIV formulation of Q-function estimation is well-posed in the sense of L2-measure of ill-posedness with respect to the data generating distribution, bypassing a strong assumption on the discount factor Î³ imposed in the recent literature for obtaining the L2 convergence rates of various Q-function estimators. Thanks to this new well-posed property, we derive the ï¬rst minimax lower bounds for the convergence rates of nonparametric estimation of Q-function and its derivatives in both sup-norm and L2-norm, which are shown to be the same as those for the classical nonparametric regression (Stone, 1982). We then propose a sieve two-stage least squares estimator and establish its rate-optimality in both norms under some mild conditions. Our general results on the well-posedness and the minimax lower bounds are of independent interest to study not only other nonparametric estimators for Q-function but also eï¬cient estimation on the value of any target policy in oï¬-policy settings.},
	language = {en},
	urldate = {2022-03-14},
	journal = {arXiv:2201.06169 [cs, econ, math, stat]},
	author = {Chen, Xiaohong and Qi, Zhengling},
	month = jan,
	year = {2022},
	note = {arXiv: 2201.06169},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, Mathematics - Statistics Theory, Economics - Econometrics},
	file = {Chen and Qi - 2022 - On Well-posedness and Minimax Optimal Rates of Non.pdf:/Users/philipamortila/Zotero/storage/2Q37ZAIW/Chen and Qi - 2022 - On Well-posedness and Minimax Optimal Rates of Non.pdf:application/pdf},
}

@article{lattimore_learning_nodate,
	title = {Learning with {Good} {Feature} {Representations} in {Bandits} and in {RL} with a {Generative} {Model}},
	language = {en},
	author = {Lattimore, Tor and SzepesvÃ¡ri, Csaba and Weisz, GellÃ©rt},
	pages = {9},
	file = {Lattimore et al. - Learning with Good Feature Representations in Band.pdf:/Users/philipamortila/Zotero/storage/EL9S49NA/Lattimore et al. - Learning with Good Feature Representations in Band.pdf:application/pdf},
}

@article{tsitsiklis_analysis_1997,
	title = {An analysis of temporal-difference learning with function approximation},
	volume = {42},
	issn = {00189286},
	url = {http://ieeexplore.ieee.org/document/580874/},
	doi = {10.1109/9.580874},
	abstract = {We discuss the temporal-difference learning algorithm, as applied to approximating the cost-to-go function of an inï¬nite-horizon discounted Markov chain. The algorithm we analyze updates parameters of a linear function approximator online during a single endless trajectory of an irreducible aperiodic Markov chain with a ï¬nite or inï¬nite state space. We present a proof of convergence (with probability one), a characterization of the limit of convergence, and a bound on the resulting approximation error. Furthermore, our analysis is based on a new line of reasoning that provides new intuition about the dynamics of temporal-difference learning.},
	language = {en},
	number = {5},
	urldate = {2022-03-21},
	journal = {IEEE Transactions on Automatic Control},
	author = {Tsitsiklis, J.N. and Van Roy, B.},
	month = may,
	year = {1997},
	pages = {674--690},
	file = {Tsitsiklis and Van Roy - 1997 - An analysis of temporal-difference learning with f.pdf:/Users/philipamortila/Zotero/storage/SBFJ2LH2/Tsitsiklis and Van Roy - 1997 - An analysis of temporal-difference learning with f.pdf:application/pdf},
}

@article{uehara_minimax_2020,
	title = {Minimax {Weight} and {Q}-{Function} {Learning} for {Off}-{Policy} {Evaluation}},
	url = {http://arxiv.org/abs/1910.12809},
	abstract = {We provide theoretical investigations into offpolicy evaluation in reinforcement learning using function approximators for (marginalized) importance weights and value functions. Our contributions include: (1) A new estimator, MWL, that directly estimates importance ratios over the state-action distributions, removing the reliance on knowledge of the behavior policy as in prior work (Liu et al., 2018). (2) Another new estimator, MQL, obtained by swapping the roles of importance weights and value-functions in MWL. MQL has an intuitive interpretation of minimizing average Bellman errors and can be combined with MWL in a doubly robust manner. (3) Several additional results that offer further insights, including the sample complexities of MWL and MQL, their asymptotic optimality in the tabular setting, how the learned importance weights depend the choice of the discriminator class, and how our methods provide a uniï¬ed view of some old and new algorithms in RL.},
	language = {en},
	urldate = {2022-04-04},
	journal = {arXiv:1910.12809 [cs, stat]},
	author = {Uehara, Masatoshi and Huang, Jiawei and Jiang, Nan},
	month = oct,
	year = {2020},
	note = {arXiv: 1910.12809},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
	file = {Uehara et al. - 2020 - Minimax Weight and Q-Function Learning for Off-Pol.pdf:/Users/philipamortila/Zotero/storage/FEMCVMVT/Uehara et al. - 2020 - Minimax Weight and Q-Function Learning for Off-Pol.pdf:application/pdf},
}

@article{patterson_generalized_2022,
	title = {A {Generalized} {Projected} {Bellman} {Error} for {Off}-policy {Value} {Estimation} in {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2104.13844},
	abstract = {Many reinforcement learning algorithms rely on value estimation, however, the most widely used algorithmsânamely temporal diï¬erence algorithmsâcan diverge under both oï¬-policy sampling and nonlinear function approximation. Many algorithms have been developed for oï¬-policy value estimation based on the linear mean squared projected Bellman error (PBE) and are sound under linear function approximation. Extending these methods to the nonlinear case has been largely unsuccessful. Recently, several methods have been introduced that approximate a diï¬erent objectiveâthe mean-squared Bellman error (BE)âwhich naturally facilitate nonlinear approximation. In this work, we build on these insights and introduce a new generalized PBE that extends the linear PBE to the nonlinear setting. We show how this generalized objective uniï¬es previous work and obtain new bounds for the value error of the solutions of the generalized objective. We derive an easy-to-use, but sound, algorithm to minimize the generalized objective, and show that it is more stable across runs, is less sensitive to hyperparameters, and performs favorably across four control domains with neural network function approximation.},
	language = {en},
	urldate = {2022-04-12},
	journal = {arXiv:2104.13844 [cs]},
	author = {Patterson, Andrew and White, Adam and White, Martha},
	month = mar,
	year = {2022},
	note = {arXiv: 2104.13844},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	annote = {Comment: Accepted for publication in JMLR 2022},
	file = {Patterson et al. - 2022 - A Generalized Projected Bellman Error for Off-poli.pdf:/Users/philipamortila/Zotero/storage/LKF2BYGF/Patterson et al. - 2022 - A Generalized Projected Bellman Error for Off-poli.pdf:application/pdf},
}

@article{pires_statistical_nodate,
	title = {Statistical linear estimation with penalized estimators: an application to reinforcement learning},
	abstract = {Motivated by value function estimation in reinforcement learning, we study statistical linear inverse problems, i.e., problems where the coeï¬cients of a linear system to be solved are observed in noise. We consider penalized estimators, where performance is evaluated using a matrix-weighted two-norm of the defect of the estimator measured with respect to the true, unknown coeï¬cients. Two objective functions are considered depending whether the error of the defect measured with respect to the noisy coeï¬cients is squared or unsquared. We propose simple, yet novel and theoretically well-founded data-dependent choices for the regularization parameters for both cases that avoid datasplitting. A distinguishing feature of our analysis is that we derive deterministic error bounds in terms of the error of the coeï¬cients, thus allowing the complete separation of the analysis of the stochastic properties of these errors. We show that our results lead to new insights and bounds for linear value function estimation in reinforcement learning.},
	language = {en},
	author = {Pires, Bernardo Avila and Szepesvari, Csaba},
	pages = {14},
	file = {Pires and Szepesvari - Statistical linear estimation with penalized estim.pdf:/Users/philipamortila/Zotero/storage/REMRNGM4/Pires and Szepesvari - Statistical linear estimation with penalized estim.pdf:application/pdf},
}

@article{scherrer_should_2010,
	title = {Should one compute the {Temporal} {Difference} fix point or minimize the {Bellman} {Residual}? {The} unified oblique projection view},
	shorttitle = {Should one compute the {Temporal} {Difference} fix point or minimize the {Bellman} {Residual}?},
	url = {http://arxiv.org/abs/1011.4362},
	abstract = {We investigate projection methods, for evaluating a linear approximation of the value function of a policy in a Markov Decision Process context. We consider two popular approaches, the one-step Temporal Diï¬erence ï¬x-point computation (TD(0)) and the Bellman Residual (BR) minimization. We describe examples, where each method outperforms the other. We highlight a simple relation between the objective function they minimize, and show that while BR enjoys a performance guarantee, TD(0) does not in general. We then propose a uniï¬ed view in terms of oblique projections of the Bellman equation, which substantially simpliï¬es and extends the characterization of Schoknecht (2002) and the recent analysis of Yu \& Bertsekas (2008). Eventually, we describe some simulations that suggest that if the TD(0) solution is usually slightly better than the BR solution, its inherent numerical instability makes it very bad in some cases, and thus worse on average.},
	language = {en},
	urldate = {2022-04-12},
	journal = {arXiv:1011.4362 [cs]},
	author = {Scherrer, Bruno},
	month = nov,
	year = {2010},
	note = {arXiv: 1011.4362},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Scherrer - 2010 - Should one compute the Temporal Difference fix poi.pdf:/Users/philipamortila/Zotero/storage/R9LZ247W/Scherrer - 2010 - Should one compute the Temporal Difference fix poi.pdf:application/pdf},
}

@article{yu_error_2010,
	title = {Error {Bounds} for {Approximations} from {Projected} {Linear} {Equations}},
	volume = {35},
	issn = {0364-765X, 1526-5471},
	url = {http://pubsonline.informs.org/doi/abs/10.1287/moor.1100.0441},
	doi = {10.1287/moor.1100.0441},
	language = {en},
	number = {2},
	urldate = {2022-04-15},
	journal = {Mathematics of Operations Research},
	author = {Yu, Huizhen and Bertsekas, Dimitri P.},
	month = may,
	year = {2010},
	pages = {306--329},
	file = {Yu and Bertsekas - 2010 - Error Bounds for Approximations from Projected Lin.pdf:/Users/philipamortila/Zotero/storage/Q2SI7LSG/Yu and Bertsekas - 2010 - Error Bounds for Approximations from Projected Lin.pdf:application/pdf},
}

@article{perdomo_sharp_2022,
	title = {A {Sharp} {Characterization} of {Linear} {Estimators} for {Offline} {Policy} {Evaluation}},
	url = {http://arxiv.org/abs/2203.04236},
	abstract = {Oï¬ine policy evaluation is a fundamental statistical problem in reinforcement learning that involves estimating the value function of some decision-making policy given data collected by a potentially different policy. In order to tackle problems with complex, high-dimensional observations, there has been signiï¬cant interest from theoreticians and practitioners alike in understanding the possibility of function approximation in reinforcement learning. Despite signiï¬cant study, a sharp characterization of when we might expect oï¬ine policy evaluation to be tractable, even in the simplest setting of linear function approximation, has so far remained elusive, with a surprising number of strong negative results recently appearing in the literature.},
	language = {en},
	urldate = {2022-04-18},
	journal = {arXiv:2203.04236 [cs, stat]},
	author = {Perdomo, Juan C. and Krishnamurthy, Akshay and Bartlett, Peter and Kakade, Sham},
	month = mar,
	year = {2022},
	note = {arXiv: 2203.04236},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
	file = {Perdomo et al. - 2022 - A Sharp Characterization of Linear Estimators for .pdf:/Users/philipamortila/Zotero/storage/BBIIA929/Perdomo et al. - 2022 - A Sharp Characterization of Linear Estimators for .pdf:application/pdf},
}

@article{chen_finite-sample_nodate,
	title = {Finite-{Sample} {Analysis} of {Off}-{Policy} {TD}-{Learning} via {Generalized} {Bellman} {Operators}},
	abstract = {In TD-learning, off-policy sampling is known to be more practical than on-policy sampling, and by decoupling learning from data collection, it enables data reuse. It is known that policy evaluation has the interpretation of solving a generalized Bellman equation. In this paper, we derive ï¬nite-sample bounds for any general off-policy TD-like stochastic approximation algorithm that solves for the ï¬xedpoint of this generalized Bellman operator. Our key step is to show that the generalized Bellman operator is simultaneously a contraction mapping with respect to a weighted p-norm for each p in [1, â), with a common contraction factor. Off-policy TD-learning is known to suffer from high variance due to the product of importance sampling ratios. A number of algorithms (e.g. QÏ(Î»), Tree-Backup(Î»), Retrace(Î»), and Q-trace) have been proposed in the literature to address this issue. Our results immediately imply ï¬nite-sample bounds of these algorithms. In particular, we provide ï¬rst-known ï¬nite-sample guarantees for QÏ(Î»), TreeBackup(Î»), and Retrace(Î»), and improve the best known bounds of Q-trace in [19]. Moreover, we show the bias-variance trade-offs in each of these algorithms.},
	language = {en},
	author = {Chen, Zaiwei and Shakkottai, Sanjay and Maguluri, Siva Theja and Shanmugam, Karthikeyan},
	pages = {13},
	file = {Chen et al. - Finite-Sample Analysis of Off-Policy TD-Learning v.pdf:/Users/philipamortila/Zotero/storage/8P5XG6FY/Chen et al. - Finite-Sample Analysis of Off-Policy TD-Learning v.pdf:application/pdf},
}

@techreport{bousquet_statistically_2021,
	title = {Statistically {Near}-{Optimal} {Hypothesis} {Selection}},
	url = {http://arxiv.org/abs/2108.07880},
	abstract = {Hypothesis Selection is a fundamental distribution learning problem where given a comparator-class \$Q={\textbackslash}\{q\_1,{\textbackslash}ldots, q\_n{\textbackslash}\}\$ of distributions, and a sampling access to an unknown target distribution \$p\$, the goal is to output a distribution \$q\$ such that \${\textbackslash}mathsf\{TV\}(p,q)\$ is close to \$opt\$, where \$opt = {\textbackslash}min\_i{\textbackslash}\{{\textbackslash}mathsf\{TV\}(p,q\_i){\textbackslash}\}\$ and \${\textbackslash}mathsf\{TV\}({\textbackslash}cdot, {\textbackslash}cdot)\$ denotes the total-variation distance. Despite the fact that this problem has been studied since the 19th century, its complexity in terms of basic resources, such as number of samples and approximation guarantees, remains unsettled (this is discussed, e.g., in the charming book by Devroye and Lugosi `00). This is in stark contrast with other (younger) learning settings, such as PAC learning, for which these complexities are well understood. We derive an optimal \$2\$-approximation learning strategy for the Hypothesis Selection problem, outputting \$q\$ such that \${\textbackslash}mathsf\{TV\}(p,q) {\textbackslash}leq2 {\textbackslash}cdot opt + {\textbackslash}eps\$, with a (nearly) optimal sample complexity of{\textasciitilde}\${\textbackslash}tilde O({\textbackslash}log n/{\textbackslash}epsilon{\textasciicircum}2)\$. This is the first algorithm that simultaneously achieves the best approximation factor and sample complexity: previously, Bousquet, Kane, and Moran (COLT `19) gave a learner achieving the optimal \$2\$-approximation, but with an exponentially worse sample complexity of \${\textbackslash}tilde O({\textbackslash}sqrt\{n\}/{\textbackslash}epsilon{\textasciicircum}\{2.5\})\$, and Yatracos{\textasciitilde}(Annals of Statistics `85) gave a learner with optimal sample complexity of \$O({\textbackslash}log n /{\textbackslash}epsilon{\textasciicircum}2)\$ but with a sub-optimal approximation factor of \$3\$.},
	number = {arXiv:2108.07880},
	urldate = {2022-09-09},
	institution = {arXiv},
	author = {Bousquet, Olivier and Braverman, Mark and Efremenko, Klim and Kol, Gillat and Moran, Shay},
	month = aug,
	year = {2021},
	note = {arXiv:2108.07880 [cs, math]
type: article},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Mathematics - Optimization and Control, Computer Science - Computational Complexity, Computer Science - Information Theory},
	annote = {Comment: Accepted to FOCS 2021},
	file = {arXiv Fulltext PDF:/Users/philipamortila/Zotero/storage/3NDKIIBA/Bousquet et al. - 2021 - Statistically Near-Optimal Hypothesis Selection.pdf:application/pdf;arXiv.org Snapshot:/Users/philipamortila/Zotero/storage/TN6G4PXH/2108.html:text/html},
}

@misc{mou_off-policy_2022,
	title = {Off-policy estimation of linear functionals: {Non}-asymptotic theory for semi-parametric efficiency},
	shorttitle = {Off-policy estimation of linear functionals},
	url = {http://arxiv.org/abs/2209.13075},
	abstract = {The problem of estimating a linear functional based on observational data is canonical in both the causal inference and bandit literatures. We analyze a broad class of two-stage procedures that ï¬rst estimate the treatment eï¬ect function, and then use this quantity to estimate the linear functional. We prove non-asymptotic upper bounds on the mean-squared error of such procedures: these bounds reveal that in order to obtain non-asymptotically optimal procedures, the error in estimating the treatment eï¬ect should be minimized in a certain weighted L2-norm. We analyze a two-stage procedure based on constrained regression in this weighted norm, and establish its instance-dependent optimality in ï¬nite samples via matching non-asymptotic local minimax lower bounds. These results show that the optimal non-asymptotic risk, in addition to depending on the asymptotically eï¬cient variance, depends on the weighted norm distance between the true outcome function and its approximation by the richest function class supported by the sample size.},
	language = {en},
	urldate = {2022-10-21},
	publisher = {arXiv},
	author = {Mou, Wenlong and Wainwright, Martin J. and Bartlett, Peter L.},
	month = sep,
	year = {2022},
	note = {arXiv:2209.13075 [cs, math, stat]},
	keywords = {Statistics - Machine Learning, Mathematics - Statistics Theory, Computer Science - Information Theory},
	annote = {Comment: 56 pages, 6 figures},
	file = {Mou et al. - 2022 - Off-policy estimation of linear functionals Non-a.pdf:/Users/philipamortila/Zotero/storage/C5U35BJZ/Mou et al. - 2022 - Off-policy estimation of linear functionals Non-a.pdf:application/pdf},
}
@inproceedings{huangbeyond,
  title={Beyond the Return: Off-policy Function Estimation under User-specified Error-measuring Distributions},
  author={Huang, Audrey and Jiang, Nan},
  year={2022},
  booktitle={Advances in Neural Information Processing Systems}
}
@inproceedings{bousquet2019optimal,
  title={The optimal approximation factor in density estimation},
  author={Bousquet, Olivier and Kane, Daniel and Moran, Shay},
  booktitle={Conference on Learning Theory},
  pages={318--341},
  year={2019},
  organization={PMLR}
}
@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@article{wang2020statistical,
  title={What are the statistical limits of offline RL with linear function approximation?},
  author={Wang, Ruosong and Foster, Dean P and Kakade, Sham M},
  journal={arXiv preprint arXiv:2010.11895},
  year={2020}
}

@article{du2019good,
  title={Is a good representation sufficient for sample efficient reinforcement learning?},
  author={Du, Simon S and Kakade, Sham M and Wang, Ruosong and Yang, Lin F},
  journal={arXiv preprint arXiv:1910.03016},
  year={2019}
}
@inproceedings{lattimore2020learning,
  title={Learning with good feature representations in bandits and in rl with a generative model},
  author={Lattimore, Tor and Szepesvari, Csaba and Weisz, Gellert},
  booktitle={International Conference on Machine Learning},
  pages={5662--5670},
  year={2020},
  organization={PMLR}
}
@article{zanette2022bellman,
  title={Bellman Residual Orthogonalization for Offline Reinforcement Learning},
  author={Zanette, Andrea and Wainwright, Martin J},
  journal={arXiv preprint arXiv:2203.12786},
  year={2022}
}
@article{zanette2022realizability,
  title={When is Realizability Sufficient for Off-Policy Reinforcement Learning?},
  author={Zanette, Andrea},
  journal={arXiv preprint arXiv:2211.05311},
  year={2022}
}
@inproceedings{parr2008analysis,
  title={An analysis of linear models, linear value-function approximation, and feature selection for reinforcement learning},
  author={Parr, Ronald and Li, Lihong and Taylor, Gavin and Painter-Wakefield, Christopher and Littman, Michael L},
  booktitle={Proceedings of the 25th international conference on Machine learning},
  pages={752--759},
  year={2008}
}
@book{ortega1990numerical,
  title={Numerical analysis: a second course},
  author={Ortega, James M},
  year={1990},
  publisher={SIAM}
}

@article{uehara2021representation,
  title={Representation learning for online and offline rl in low-rank mdps},
  author={Uehara, Masatoshi and Zhang, Xuezhou and Sun, Wen},
  journal={arXiv preprint arXiv:2110.04652},
  year={2021}
}