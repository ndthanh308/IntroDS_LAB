{
  "2210-15543": {
    "title": "Beyond the Return: Off-policy Function Estimation under User-specified Error-measuring Distributions",
    "authors": [
      "Audrey Huang",
      "Nan Jiang"
    ],
    "submission_date": "2022-10-27",
    "semantic_scholar_id": "52896c9891fe0276e3e4147a178e8047e148764c"
  },
  "2111-10919": {
    "title": "Offline Reinforcement Learning: Fundamental Barriers for Value Function Approximation",
    "authors": [
      "Dylan J. Foster",
      "A. Krishnamurthy",
      "D. Simchi-Levi",
      "Yunzong Xu"
    ],
    "submission_date": "2021-11-21",
    "semantic_scholar_id": "552a953929a18d586fc2b1ed63aac306ec2d6147"
  },
  "2110-04652": {
    "title": "Representation Learning for Online and Offline RL in Low-rank MDPs",
    "authors": [
      "Masatoshi Uehara",
      "Xuezhou Zhang",
      "Wen Sun"
    ],
    "submission_date": "2021-10-09",
    "semantic_scholar_id": "8d16ffb11c7b62181146db43296852424426a3cd"
  },
  "2109-12002": {
    "title": "Optimal policy evaluation using kernel-based temporal difference methods",
    "authors": [
      "Yaqi Duan",
      "Mengdi Wang",
      "M. Wainwright"
    ],
    "submission_date": "2021-09-24",
    "semantic_scholar_id": "1a3611f1c50f3607dc2afb71aca364b2284f0428"
  },
  "2012-05299": {
    "title": "Optimal oracle inequalities for solving projected fixed-point equations",
    "authors": [
      "Wenlong Mou",
      "A. Pananjady",
      "M. Wainwright"
    ],
    "submission_date": "2020-12-09",
    "semantic_scholar_id": "6826b3fac47b9d1d5ee44199663d6805233fb46f"
  },
  "2011-01075": {
    "title": "A Variant of the Wang-Foster-Kakade Lower Bound for the Discounted Setting",
    "authors": [
      "P. Amortila",
      "Nan Jiang",
      "Tengyang Xie"
    ],
    "submission_date": "2020-11-02",
    "semantic_scholar_id": "a472a3d4785793d1ec80edf4d6bdf01e5b7465cc"
  },
  "2010-11895": {
    "title": "What are the Statistical Limits of Offline RL with Linear Function Approximation?",
    "authors": [
      "Ruosong Wang",
      "Dean Phillips Foster",
      "S. Kakade"
    ],
    "submission_date": "2020-10-22",
    "semantic_scholar_id": "bf53ddef241c58303b509faad4ec7514d3a6fc14"
  },
  "2010-01374": {
    "title": "Exponential Lower Bounds for Planning in MDPs With Linearly-Realizable Optimal Action-Value Functions",
    "authors": [
      "G. Weisz",
      "P. Amortila",
      "Csaba Szepesvari"
    ],
    "submission_date": "2020-10-03",
    "semantic_scholar_id": "3c89bcef40277c5428e71b9978ddb44f90b2814c"
  },
  "2008-04990": {
    "title": "Batch Value-function Approximation with Only Realizability",
    "authors": [
      "Tengyang Xie",
      "Nan Jiang"
    ],
    "submission_date": "2020-08-11",
    "semantic_scholar_id": "af252daf6ab49b3df9901f99af5a67ff3478edcd"
  },
  "2002-09516": {
    "title": "Minimax-Optimal Off-Policy Evaluation with Linear Function Approximation",
    "authors": [
      "Yaqi Duan",
      "Mengdi Wang"
    ],
    "submission_date": "2020-02-21",
    "semantic_scholar_id": "8664713793dbb1f6f935c617dd34be93d5b096ed"
  },
  "1911-07676": {
    "title": "Learning with Good Feature Representations in Bandits and in RL with a Generative Model",
    "authors": [
      "Tor Lattimore",
      "Csaba Szepesvari"
    ],
    "submission_date": "2019-11-18",
    "semantic_scholar_id": "57e72da5765157f72e216054f64280dbf3f8d865"
  },
  "1910-12809": {
    "title": "Minimax Weight and Q-Function Learning for Off-Policy Evaluation",
    "authors": [
      "Masatoshi Uehara",
      "Jiawei Huang",
      "Nan Jiang"
    ],
    "submission_date": "2019-10-28",
    "semantic_scholar_id": "7b5773650b4e52568d4e9e4c6384e9034d40fd66"
  },
  "1910-03016": {
    "title": "Is a Good Representation Sufficient for Sample Efficient Reinforcement Learning?",
    "authors": [
      "S. Du",
      "S. Kakade",
      "Ruosong Wang",
      "Lin F. Yang"
    ],
    "submission_date": "2019-10-07",
    "semantic_scholar_id": "8e98719529f8e029210b6c31d54e7486ee00d0af"
  },
  "1905-00360": {
    "title": "Information-Theoretic Considerations in Batch Reinforcement Learning",
    "authors": [
      "Jinglin Chen",
      "Nan Jiang"
    ],
    "submission_date": "2019-05-01",
    "semantic_scholar_id": "b32ffe8f86bf0c36d85f8274ff0b6e2c9690a133"
  },
  "1810-12429": {
    "title": "Breaking the Curse of Horizon: Infinite-Horizon Off-Policy Estimation",
    "authors": [
      "Qiang Liu",
      "Lihong Li",
      "Ziyang Tang",
      "Dengyong Zhou"
    ],
    "submission_date": "2018-10-29",
    "semantic_scholar_id": "e81ea45d8bec329fdb11fd84990852f620895d6f"
  },
  "1712-08642": {
    "title": "Least-Squares Temporal Difference Learning for the Linear Quadratic Regulator",
    "authors": [
      "Stephen Tu",
      "B. Recht"
    ],
    "submission_date": "2017-12-22",
    "semantic_scholar_id": "dcbf134deb9cc31781726ffbabd5685d6f6dfe24"
  },
  "1206-6444": {
    "title": "Statistical linear estimation with penalized estimators: an application to reinforcement learning",
    "authors": [
      "B. '. Pires",
      "Csaba Szepesvari"
    ],
    "submission_date": "2012-06-26",
    "semantic_scholar_id": "eb6b8cf1960fc3aa23658492caf61034f273b395"
  },
  "1011-4362": {
    "title": "Should one compute the Temporal Difference fix point or minimize the Bellman Residual? The unified oblique projection view",
    "authors": [
      "B. Scherrer"
    ],
    "submission_date": "2010-06-21",
    "semantic_scholar_id": "1a9bb0a637cbaca8489afd69d6840975a9834a05"
  }
}