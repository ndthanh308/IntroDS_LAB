@inproceedings{vaswani_attention_2017,
	location = {Red Hook, {NY}, {USA}},
	title = {Attention is all you need},
	isbn = {978-1-5108-6096-4},
	series = {{NIPS}'17},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 {BLEU} on the {WMT} 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 {BLEU}. On the {WMT} 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art {BLEU} score of 41.0 after training for 3.5 days on eight {GPUs}, a small fraction of the training costs of the best models from the literature.},
	pages = {6000--6010},
	booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
	publisher = {Curran Associates Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Łukasz and Polosukhin, Illia},
	urldate = {2023-07-13},
	date = {2017-12-04}
}

@online{noauthor_introducing_nodate,
	title = {Introducing {ChatGPT}},
	url = {https://openai.com/blog/chatgpt},
	abstract = {We’ve trained a model called {ChatGPT} which interacts in a conversational way. The dialogue format makes it possible for {ChatGPT} to answer followup questions, admit its mistakes, challenge incorrect premises, and reject inappropriate requests.},
	urldate = {2023-07-13},
	langid = {american},
}
@misc{Cornucopia-LLaMA-Fin-Chinese,
  title={Cornucopia-LLaMA-Fin-Chinese},
  author={YangMu Yu},
  year={2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/jerry1993-tech/Cornucopia-LLaMA-Fin-Chinese}},
}
@misc{zhang2023PICA,
      title={PICA: Unleashing The Emotional Power of Large Language Model},
      author={Zhang, Yiqun and Zhang, Jingqing and Liu, Yongkang and Gao, Chongyun and Wang, Daling and Feng, Shi and Zhang, Yifei},
      year={2023},
      month={7},
      version={1.0},
      url={https://github.com/NEU-DataMining/PICA}
}
@misc{cui_chatlaw_2023,
	title = {{ChatLaw}: Open-Source Legal Large Language Model with Integrated External Knowledge Bases},
	url = {http://arxiv.org/abs/2306.16092},
	doi = {10.48550/arXiv.2306.16092},
	shorttitle = {{ChatLaw}},
	abstract = {Large Language Models ({LLMs}) have shown the potential to revolutionize natural language processing tasks in various domains, sparking great interest in vertical-specific large models. However, unlike proprietary models such as {BloombergGPT} and {FinGPT}, which have leveraged their unique data accumulations to make strides in the finance domain, there hasn't not many similar large language models in the Chinese legal domain to facilitate its digital transformation. In this paper, we propose an open-source legal large language model named {ChatLaw}. Due to the importance of data quality, we carefully designed a legal domain fine-tuning dataset. Additionally, to overcome the problem of model hallucinations in legal data screening during reference data retrieval, we introduce a method that combines vector database retrieval with keyword retrieval to effectively reduce the inaccuracy of relying solely on vector database retrieval. Furthermore, we propose a self-attention method to enhance the ability of large models to overcome errors present in reference data, further optimizing the issue of model hallucinations at the model level and improving the problem-solving capabilities of large models. We also open-sourced our model and part of the data at https://github.com/{PKU}-{YuanGroup}/{ChatLaw}.},
	number = {{arXiv}:2306.16092},
	publisher = {{arXiv}},
	author = {Cui, Jiaxi and Li, Zongjian and Yan, Yang and Chen, Bohua and Yuan, Li},
	urldate = {2023-07-20},
	date = {2023-06-28},
	eprinttype = {arxiv},
	eprint = {2306.16092 [cs]},
	keywords = {Computer Science - Computation and Language},
}
@article{xiong2023doctorglm,
  title={Doctorglm: Fine-tuning your chinese doctor is not a herculean task},
  author={Xiong, Honglin and Wang, Sheng and Zhu, Yitao and Zhao, Zihao and Liu, Yuxiao and Wang, Qian and Shen, Dinggang},
  journal={arXiv preprint arXiv:2304.01097},
  year={2023}
}
@misc{Taoli-LLama,
  author={Jingsi Yu and Junhui Zhu and Yujie Wang and Yang Liu and Hongxiang Chang and Jinran Nie and Cunliang Kong and Ruining Cong and XinLiu and Jiyuan An and Luming Lu and Mingwei Fang and Lin Zhu},
  title={Taoli Llama},
  year={2023},
  publisher={GitHub},
  journal={GitHub repository},
  howpublished={\url{https://github.com/blcuicall/taoli}},
}
@misc{wang2023huatuo,
      title={HuaTuo: Tuning LLaMA Model with Chinese Medical Knowledge},
      author={Haochun Wang and Chi Liu and Nuwa Xi and Zewen Qiang and Sendong Zhao and Bing Qin and Ting Liu},
      year={2023},
      eprint={2304.06975},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{chen2023bianque1,
      title={BianQue-1.0: Improving the "Question" Ability of Medical Chat Model through finetuning with Hybrid Instructions and Multi-turn Doctor QA Datasets}, 
      author={Yirong Chen and Zhenyu Wang and Xiaofen Xing and Zhipei Xu and Kai Fang and Sihang Li and Junhong Wang and Xiangmin Xu},
      year={2023},
      url={https://github.com/scutcyr/BianQue}
}
@misc{chen2023soulchat,
      title={SoulChat: Fine-tuning the "empathy" capability of a large model by mixing long textual counseling instructions with multi-round empathic dialog datasets},
      author={Chen, Yirong and Xing, Xiaofen and Wang, Zhenyu and Xu, Xiangmin},
      year={2023},
      month = {6},
      version = {1.0},
      url = {https://github.com/scutcyr/SoulChat}
}
@article{touvron2023llama,
  title={LLaMA: Open and Efficient Foundation Language Models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{touvron_llama_nodate,
	title = {Llama 2: Open Foundation and Fine-Tuned Chat Models},
	abstract = {In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models ({LLMs}) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned {LLMs}, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closedsource models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of {LLMs}.},
	author = {Touvron, Hugo and Martin, Louis and Stone, Kevin},
	langid = {english},
}
@article{zeng2022glm,
  title={Glm-130b: An open bilingual pre-trained model},
  author={Zeng, Aohan and Liu, Xiao and Du, Zhengxiao and Wang, Zihan and Lai, Hanyu and Ding, Ming and Yang, Zhuoyi and Xu, Yifan and Zheng, Wendi and Xia, Xiao and others},
  journal={arXiv preprint arXiv:2210.02414},
  year={2022}
}
@inproceedings{du2022glm,
  title={GLM: General Language Model Pretraining with Autoregressive Blank Infilling},
  author={Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={320--335},
  year={2022}
}
@article{sun2023moss,
  title={MOSS: Training Conversational Language Models from Synthetic Data}, 
  author={Tianxiang Sun and Xiaotian Zhang and Zhengfu He and Peng Li and Qinyuan Cheng and Hang Yan and Xiangyang Liu and Yunfan Shao and Qiong Tang and Xingjian Zhao and Ke Chen and Yining Zheng and Zhejian Zhou and Ruixiao Li and Jun Zhan and Yunhua Zhou and Linyang Li and Xiaogui Yang and Lingling Wu and Zhangyue Yin and Xuanjing Huang and Xipeng Qiu},
  year={2023}
}
@misc{2023internlm,
    title={InternLM: A Multilingual Language Model with Progressively Enhanced Capabilities},
    author={InternLM Team},
    howpublished = {\url{https://github.com/InternLM/InternLM-techreport}},
    year={2023}
}
@misc{chang_survey_2023,
	title = {A Survey on Evaluation of Large Language Models},
	url = {http://arxiv.org/abs/2307.03109},
	abstract = {Large language models ({LLMs}) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As {LLMs} continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine {LLMs} from various perspectives. This paper presents a comprehensive review of these evaluation methods for {LLMs}, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, educations, natural and social sciences, agent applications, and other areas. Secondly, we answer the ‘where’ and ‘how’ questions by diving into the evaluation methods and benchmarks, which serve as crucial components in assessing performance of {LLMs}. Then, we summarize the success and failure cases of {LLMs} in different tasks. Finally, we shed light on several future challenges that lie ahead in {LLMs} evaluation. Our aim is to offer invaluable insights to researchers in the realm of {LLMs} evaluation, thereby aiding the development of more proficient {LLMs}. Our key point is that evaluation should be treated as an essential discipline to better assist the development of {LLMs}. We consistently maintain the related open-source materials at: https://github.com/{MLGroupJLU}/{LLM}-eval-survey.},
	number = {{arXiv}:2307.03109},
	publisher = {{arXiv}},
	author = {Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Zhu, Kaijie and Chen, Hao and Yang, Linyi and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and Ye, Wei and Zhang, Yue and Chang, Yi and Yu, Philip S. and Yang, Qiang and Xie, Xing},
	urldate = {2023-07-13},
	date = {2023-07-12},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2307.03109 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}
@article{huang2023ceval,
title={C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models}, 
author={Huang, Yuzhen and Bai, Yuzhuo and Zhu, Zhihao and Zhang, Junlei and Zhang, Jinghan and Su, Tangjun and Liu, Junteng and Lv, Chuancheng and Zhang, Yikai and Lei, Jiayi and Fu, Yao and Sun, Maosong and He, Junxian},
journal={arXiv preprint arXiv:2305.08322},
year={2023}
}
@inproceedings{Zhang2023EvaluatingTP,
  title={Evaluating the Performance of Large Language Models on GAOKAO Benchmark},
  author={Xiaotian Zhang and Chunyang Li and Yi Zong and Zhengyu Ying and Liang He and Xipeng Qiu},
  year={2023}
}
@misc{zhong2023agieval,
      title={AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models}, 
      author={Wanjun Zhong and Ruixiang Cui and Yiduo Guo and Yaobo Liang and Shuai Lu and Yanlin Wang and Amin Saied and Weizhu Chen and Nan Duan},
      year={2023},
      eprint={2304.06364},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{sun2023safety,
      title={Safety Assessment of Chinese Large Language Models}, 
      author={Hao Sun and Zhexin Zhang and Jiawen Deng and Jiale Cheng and Minlie Huang},
      journal={arXiv preprint arXiv:2304.10436},
      year={2023}
}
@misc{zhuwei2023promptcblue,
      title={PromptCBLUE: Benchmarking of Chinese Medical Large Language Models},
      author={Zhu, Wei and Chen, Mosha and Wang, Xiaoling and Chen, Liang and Huang, Xuanjing and He, Liang and Yang, Xiaochun and Tang, Buzhou and Wang, haofen},
      year={2023},
      month={5},
      url={https://github.com/michael-wzhu/PromptCBLUE}
}
@misc{liu2018chinesepersonrelationgraph,
      title={Chinese Person Relation Graph},
      author={Liu, Huanyong},
      year={2018},
      month={12},
      url={https://github.com/liuhuanyong/PersonRelationKnowledgeGraph}
}
@article{zeng_glm-130b_2023,
	title = {{GLM}-130B: {AN} {OPEN} {BILINGUAL} {PRE}-{TRAINED}},
	abstract = {We introduce {GLM}-130B, a bilingual (English and Chinese) pre-trained language model with 130 billion parameters. It is an attempt to open-source a 100B-scale model at least as good as {GPT}-3 (davinci) and unveil how models of such a scale can be successfully pre-trained. Over the course of this effort, we face numerous unexpected technical and engineering challenges, particularly on loss spikes and divergence. In this paper, we introduce the training process of {GLM}-130B including its design choices, training strategies for both efficiency and stability, and engineering efforts. The resultant {GLM}-130B model offers significant outperformance over {GPT}-3 175B (davinci) on a wide range of popular English benchmarks while the performance advantage is not observed in {OPT}-175B and {BLOOM}-176B. It also consistently and significantly outperforms {ERNIE} {TITAN} 3.0 260B—the largest Chinese language model—across related benchmarks. Finally, we leverage a unique scaling property of {GLM}-130B to reach {INT}4 quantization without post training, with almost no performance loss, making it the first among 100B-scale models and more importantly, allowing its effective inference on 4×{RTX} 3090 (24G) or 8×{RTX} 2080 Ti (11G) {GPUs}, the most affordable {GPUs} required for using 100B-scale models. The {GLM}-130B model weights are publicly accessible and its code, training logs, related toolkit, and lessons learned are open-sourced at https://github.com/{THUDM}/{GLM}-130B/.},
	author = {Zeng, Aohan and Liu, Xiao and Du, Zhengxiao and Wang, Zihan and Lai, Hanyu and Ding, Ming and Yang, Zhuoyi and Xu, Yifan and Zheng, Wendi and Xia, Xiao and Tam, Weng Lam and Ma, Zixuan and Xue, Yufei and Zhai, Jidong and Chen, Wenguang and Liu, Zhiyuan and Zhang, Peng and Dong, Yuxiao and Tang, Jie},
	date = {2023},
	langid = {english}
}

