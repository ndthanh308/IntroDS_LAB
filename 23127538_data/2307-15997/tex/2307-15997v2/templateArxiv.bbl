\begin{thebibliography}{10}

\bibitem{vaswani_attention_2017}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N. Gomez, ≈Åukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In {\em Proceedings of the 31st International Conference on Neural
  Information Processing Systems}, {NIPS}'17, pages 6000--6010. Curran
  Associates Inc.

\bibitem{noauthor_introducing_nodate}
Introducing {ChatGPT}.

\bibitem{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
  Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric
  Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and
  Guillaume Lample.
\newblock Llama: Open and efficient foundation language models.
\newblock {\em arXiv preprint arXiv:2302.13971}, 2023.

\bibitem{touvron_llama_nodate}
Hugo Touvron, Louis Martin, and Kevin Stone.
\newblock Llama 2: Open foundation and fine-tuned chat models.

\bibitem{du2022glm}
Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and
  Jie Tang.
\newblock Glm: General language model pretraining with autoregressive blank
  infilling.
\newblock In {\em Proceedings of the 60th Annual Meeting of the Association for
  Computational Linguistics (Volume 1: Long Papers)}, pages 320--335, 2022.

\bibitem{zeng2022glm}
Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi
  Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et~al.
\newblock Glm-130b: An open bilingual pre-trained model.
\newblock {\em arXiv preprint arXiv:2210.02414}, 2022.

\bibitem{sun2023moss}
Tianxiang Sun, Xiaotian Zhang, Zhengfu He, Peng Li, Qinyuan Cheng, Hang Yan,
  Xiangyang Liu, Yunfan Shao, Qiong Tang, Xingjian Zhao, Ke~Chen, Yining Zheng,
  Zhejian Zhou, Ruixiao Li, Jun Zhan, Yunhua Zhou, Linyang Li, Xiaogui Yang,
  Lingling Wu, Zhangyue Yin, Xuanjing Huang, and Xipeng Qiu.
\newblock Moss: Training conversational language models from synthetic data.
\newblock 2023.

\bibitem{2023internlm}
InternLM Team.
\newblock Internlm: A multilingual language model with progressively enhanced
  capabilities.
\newblock \url{https://github.com/InternLM/InternLM-techreport}, 2023.

\bibitem{Cornucopia-LLaMA-Fin-Chinese}
YangMu Yu.
\newblock Cornucopia-llama-fin-chinese.
\newblock \url{https://github.com/jerry1993-tech/Cornucopia-LLaMA-Fin-Chinese},
  2023.

\bibitem{zhang2023PICA}
Yiqun Zhang, Jingqing Zhang, Yongkang Liu, Chongyun Gao, Daling Wang, Shi Feng,
  and Yifei Zhang.
\newblock Pica: Unleashing the emotional power of large language model, 7 2023.

\bibitem{chen2023soulchat}
Yirong Chen, Xiaofen Xing, Zhenyu Wang, and Xiangmin Xu.
\newblock Soulchat: Fine-tuning the "empathy" capability of a large model by
  mixing long textual counseling instructions with multi-round empathic dialog
  datasets, 6 2023.

\bibitem{cui_chatlaw_2023}
Jiaxi Cui, Zongjian Li, Yang Yan, Bohua Chen, and Li~Yuan.
\newblock {ChatLaw}: Open-source legal large language model with integrated
  external knowledge bases.

\bibitem{xiong2023doctorglm}
Honglin Xiong, Sheng Wang, Yitao Zhu, Zihao Zhao, Yuxiao Liu, Qian Wang, and
  Dinggang Shen.
\newblock Doctorglm: Fine-tuning your chinese doctor is not a herculean task.
\newblock {\em arXiv preprint arXiv:2304.01097}, 2023.

\bibitem{wang2023huatuo}
Haochun Wang, Chi Liu, Nuwa Xi, Zewen Qiang, Sendong Zhao, Bing Qin, and Ting
  Liu.
\newblock Huatuo: Tuning llama model with chinese medical knowledge, 2023.

\bibitem{chen2023bianque1}
Yirong Chen, Zhenyu Wang, Xiaofen Xing, Zhipei Xu, Kai Fang, Sihang Li, Junhong
  Wang, and Xiangmin Xu.
\newblock Bianque-1.0: Improving the "question" ability of medical chat model
  through finetuning with hybrid instructions and multi-turn doctor qa
  datasets.
\newblock 2023.

\bibitem{Taoli-LLama}
Jingsi Yu, Junhui Zhu, Yujie Wang, Yang Liu, Hongxiang Chang, Jinran Nie,
  Cunliang Kong, Ruining Cong, XinLiu, Jiyuan An, Luming Lu, Mingwei Fang, and
  Lin Zhu.
\newblock Taoli llama.
\newblock \url{https://github.com/blcuicall/taoli}, 2023.

\bibitem{chang_survey_2023}
Yupeng Chang, Xu~Wang, Jindong Wang, Yuan Wu, Kaijie Zhu, Hao Chen, Linyi Yang,
  Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi~Chang,
  Philip~S. Yu, Qiang Yang, and Xing Xie.
\newblock A survey on evaluation of large language models.

\bibitem{huang2023ceval}
Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su,
  Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and
  Junxian He.
\newblock C-eval: A multi-level multi-discipline chinese evaluation suite for
  foundation models.
\newblock {\em arXiv preprint arXiv:2305.08322}, 2023.

\bibitem{Zhang2023EvaluatingTP}
Xiaotian Zhang, Chunyang Li, Yi~Zong, Zhengyu Ying, Liang He, and Xipeng Qiu.
\newblock Evaluating the performance of large language models on gaokao
  benchmark.
\newblock 2023.

\bibitem{zhong2023agieval}
Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin
  Saied, Weizhu Chen, and Nan Duan.
\newblock Agieval: A human-centric benchmark for evaluating foundation models,
  2023.

\bibitem{sun2023safety}
Hao Sun, Zhexin Zhang, Jiawen Deng, Jiale Cheng, and Minlie Huang.
\newblock Safety assessment of chinese large language models.
\newblock {\em arXiv preprint arXiv:2304.10436}, 2023.

\bibitem{zhuwei2023promptcblue}
Wei Zhu, Mosha Chen, Xiaoling Wang, Liang Chen, Xuanjing Huang, Liang He,
  Xiaochun Yang, Buzhou Tang, and haofen Wang.
\newblock Promptcblue: Benchmarking of chinese medical large language models, 5
  2023.

\bibitem{liu2018chinesepersonrelationgraph}
Huanyong Liu.
\newblock Chinese person relation graph, 12 2018.

\end{thebibliography}
