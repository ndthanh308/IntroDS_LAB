\section{Evaluation} \label{sec:eval}
We conducted empirical studies in various scenarios to evaluate the performance of the proposed quantum multi-classifier MORE. To implement MORE and related baselines, we used Python 3.8 and the IBM Qiskit package \cite{qiskit} to simulate quantum systems both with and without noise. For simulating noisy systems, we employed several noisy backends, including \texttt{FakeAuckland}, \texttt{FakeAthensV2}, and \texttt{FakeBelemV2}. The source code used to generate the experiment results is available in \href{https://github.com/Jindi0/MORE.git}{github.com/Jindi0/MORE}. 

% Figure environment removed

% Figure environment removed

\textbf{Dataset:} (1)We utilize the \textbf{MNIST} dataset \cite{deng2012mnist} to conduct experiments in the noiseless quantum system. The MNIST is a widely used benchmark for image classification tasks, consisting of ten classes of hand-written digits ranging from 0 to 9. We randomly select 1,000 training and 200 test images from each class and reduce their dimensions to 8 using PCA. (2) Additionally, we evaluate MORE using the \textbf{Iris} dataset \cite{Iris} in simulated noisy quantum systems. The Iris dataset contains 150 instances classified into three distinct classes, each consisting of 50 instances with 4 pixels. We use 70\% of the instances (105 instances) for training, while the remaining 30\% (45 instances) are reserved for testing purposes.

\textbf{Model implementation:}
We develop the QNN classifiers based on Qiskit \texttt{NeuralNetworkClassifier} class, and use the optimizer \texttt{COBYLA} to update trainable parameters. MORE employs an 8-qubit QNN for the MNIST dataset and a 4-qubit QNN for the Iris dataset, with 91 and 39 trainable parameters, respectively. We build QNNs based on the design principles for quantum convolutional neural networks proposed in \cite{cong2019quantum}. For the readout, we choose the last active qubits at the end of the circuit and measure it with $\sigma_x$, $\sigma_y$, and $\sigma_z$ observables. To efficiently convert classical labels to quantum labels during the clustering step, we randomly select five instances from each class and form pairs from the resulting dataset. The clustering dataset consists of $\binom{5K}{2}$ pairs in total, where $K$ is the number of classes. We use MSE to calculate the interclass correlations. And to make the quantum labels spread out as much as possible, the MSE between different classes is normalized to the interval [0.5, 1]. During supervised learning, then, the entire training dataset is used.










% Furthermore, we compare MulQ's performance to that of related baselines, which are quantum classifiers with generic designs, as mentioned in Section 1, and the classical counterparts with the same number of parameters.





% Figure environment removed

\subsection{Accuracy}
We evaluate the accuracy of the proposed approach \textbf{MORE} using the MNIST dataset on noise-free quantum systems. We conduct nine classification problems ranging from binary to 10-class. 

Our evaluation starts by investigating the effect of using multiple observables of a readout qubit on binary classification tasks. To do so, we compare our results to \textbf{BaseBin}, which serves as the baseline in this experiment. The comparison is illustrated in Fig.~\ref{fig:eval-bin}. Both MORE and BaseBin use the same ansatz (QCNN with 91 trainable parameters) for binary classifications, but the readout qubit of BaseBin is measured only with observable $\sigma_z$. The expected value of its measurement results is associated with two distinct classes: a positive expected value represents one class, and a negative expected value represents the other. We conduct 45 binary classifications on MNIST using MORE and BaseBin, respectively. The class pairs for classification are listed on the x-axis of Fig.~\ref{fig:eval-bin} and are sorted in descending order of interclass correlations. E.g., the training data of classes `4' and `9' have the highest similarity, while those of classes `0' and `1' have the lowest. The results indicate that MORE outperforms BaseBin in 37 out of 45 tasks and achieves comparable accuracy in the remaining tasks. MORE improves accuracy by up to 22.28\% and by an average of 4.9\%. Furthermore, the performance of MORE demonstrates greater stability across tasks of varying difficulty than BaseBin. As observed, a roughly inverse relationship exists between accuracy and interclass correlation for both MORE and BaseBin, i.e., as class correlation increases, classification becomes more challenging. Consequently, both MORE and BaseBin exhibit the highest accuracy on (0, 1)-classification and the lowest accuracy on (4, 9)-classification. Nonetheless, the variance of MORE's accuracy over 45 tasks is only 34.01, while that of BaseBin is 59.41, indicating that MORE is more stable than BaseBin. It shows the advantage of MORE, which has three observables, in terms of accuracy and stability.


Next, we evaluate MORE on multi-classification tasks using the MNIST dataset and summarize the results in Fig.~\ref{fig:eval-mul}. We conduct eight multi-classification tasks, categorizing handwritten digits into three to ten classes. Note that the ansatzes used in multi-classifications are \textit{identical} to the binary classification ansatzes. The accuracy of MOREs is compared to that of \textbf{BaseAnc} and \textbf{BaseMea}, which serve as the baselines in this experiment. BaseAnc and BaseMea are both variational quantum multi-classifiers. BaseAnc utilizes eight qubits for data processing and $n$ ancilla qubits as the readout for $n$-class classifications. And BaseMea employs a total of eight qubits and measures a subset of qubits at the end of the circuit to produce a result. In a multi-classification task with $n$ classes, $n$ qubits are measured. They encode class labels as one-hot vectors and measure the readout qubits on the z-basis. So BaseAn and BaseMea can only support the classifications involving up to eight classes, due to the limitation in dimensions and the number of qubits, respectively. MORE, however, is capable of conducting 10-class classification regardless of the number of qubits, and we believe it still qualifies if the dataset contains more classes. Fig.~\ref{fig:eval-mul} indicates that MORE outperforms BaseAnc and BaseMea across the board. The reason is that both BaseAnc and BaseMea require simultaneous control of several qubits to represent the class label, which can be challenging, particularly for an unstable quantum system. In contrast, MORE only utilizes a single qubit to record the outcome, thus reducing the number of factors contributing to instability. 

Overall, our proposed MORE approach, which uses a simple circuit with only one readout qubit, can beat other general approaches and achieve the desired performance. In the following subsections, we will analyze the impact of the MORE's components and access MORE in noisy quantum systems.

% We consider two implementations of MORE: with and without the loss regulator R during quantum label-based training procedures. , with MORE w/ R achieving the highest accuracy.





\subsection{Quantum label}

We now examine the impact of selecting quantum labels for classes. To do so, we compare the following approaches:
\begin{itemize}
    \item \textbf{BaseRand}: A baseline method that randomly assigns quantum states to classical labels.
    \item \textbf{MORE$\backslash$cor}: A variant of the MORE approach that does not consider interclass correlation during the variational quantum clustering step, i.e., all diagonal entries in the scaler array $S$ are -1s and all other entries are 1s.
    \item \textbf{MORE}: The vanilla MORE method employs interclass correlation for determining quantum labels.
\end{itemize}

Then, based on the selected quantum labels, each of the three approaches is followed by quantum label-based supervised learning using objective function Eq.~\ref{eq:sup_loss1} without the loss regulator $\mathcal{R}$. The test accuracy of the above approaches is summarized in Fig.~\ref{fig:eval-labels}. Across eight multi-classification tasks, MORE outperforms other methods in all cases. BaseRand and MORE$\backslash$cor have comparable performance, all of which are inferior to MORE. Therefore, we conclude that it is beneficial to take interclass correlations into account when deciding the quantum labels for classical labels. A possible reason is that the data distribution in the Hilbert space is strongly related to that in the classical feature space. As an example, Fig~\ref{fig:overview} (d) shows the distribution of quantum labels for class `0', `1', and `2' of MNIST, according to their relationship as listed in Fig~\ref{fig:overview} (b). Classes `1' and `2' have the smallest MSE value (strongest correlation), so their quantum labels are closer to each other compared to other class pairs. Similarly, the readout states of the instance from classes `1' and `2' are supposed to be closer than those of other classes. As a result, assigning quantum labels based on class correlation can capture the training data pattern to some extent, which is advantageous for enhancing model quality, as shown in Fig.~\ref{fig:eval-labels}. Nonetheless, the figure reveals that the accuracy of quantum classifiers decreases as the number of classes increases. This trend is also observed in quantum classifiers that employ alternative implementation strategies. Thus, we introduce a loss adjuster to alleviate this issue, as shown in Eq.~\ref{eq:reg}, and analyze its impact in the next subsection. 






\subsection{Loss adjuster} \label{sec:reg}

\begin{table}[]
\centering
    \caption{Parameters and results for loss adjuster}
\begin{tabular}{cccccc} 
\toprule
Task & \begin{tabular}[c]{@{}c@{}}Min. label \\ distance\end{tabular} & $r$    & $w$   & More\textbackslash{}R acc. & MORE acc.\\ \midrule
0-2  & 1.302                                                          & 1.5  & 0.1 & 84.13\%                & 88.7\%  \\ \midrule
0-3  & 0.713                                                          & 1.0  & 0.2 & 63.45\%                  & 70.1\%  \\ \midrule
0-4  & 0.57                                                           & 0.8  & 0.1 & 53.5\%                   & 63.3\%  \\ \midrule
0-5  & 0.077                                                          & 0.2  & 0.4 & 40.48\%                  & 50.2\%  \\ \midrule
0-6  & 0.212                                                          & 0.4  & 0.5 & 29.06\%                  & 37.2\%  \\ \midrule
0-7  & 0.067                                                          & 0.2  & 0.5 & 44.8\%                   & 48.6\%  \\ \midrule
0-8  & 0.099                                                          & 0.15 & 0.2 & 29.4\%                   & 33\%    \\ \midrule
0-9  & 0.086                                                          & 0.15 & 0.2 & 22.6\%                   & 27.8\% \\
\bottomrule
\end{tabular}
\label{tab:lossreg}
\end{table}

% Furthermore, implementing MORE with the loss regulator R improves the accuracy by an average of 10\% than the implementation without R, indicating that fine-grained tuning is necessary when mapping multiple labels in a two-dimensional Hilbert space based on interclass correlations.

% We now analyze the impact of the loss regulator $\mathcal{R}$ in this subsection. 
Table~\ref{tab:lossreg} summarizes the parameters and results for the loss adjuster over the multi-classification tasks. The accuracy of \textbf{MORE} (with $\mathcal{R}$) and \textbf{MORE\textbackslash{}R} (without $\mathcal{R}$) are listed in the last two columns. The comparison indicates that MORE, which uses the loss function (Eq.~\ref{eq:loss_r}) with $\mathcal{R}$ during supervised learning, improves the accuracy across all tasks. This improvement is caused by the corrected misclassification between the classes whose quantum labels in the Hilbert space are too near together. In Eq.~\ref{eq:loss_r}, two hyper-parameters need to be determined by users: the threshold $r$ and the weight $w$. The threshold $r$ is empirically determined based on distances between quantum labels. In our experiments, the Cosine distance between quantum labels is calculated. Usually, $r$ is specified to be slightly larger than the shortest distance, as shown in the 2nd and 3rd columns of table~\ref{tab:lossreg}. E.g., the smallest Cosine distance between quantum labels in the 0-9 task, a 10-class classification, is 0.086 (about 24 degrees), so we set the $r$ to be 0.15 (about 32 degrees around the target quantum label). Moreover, we test MORE with a $w$ range from 0.1 to 1.0 and identify the optimal value of $w$ for each task. We report these values in the 4th column of the table, although we found accuracy improvements across all values of $w$ in each task. The $w$ values demonstrate that $\mathcal{R}$ has varying importance across different tasks, but its contribution is not greater than half in any task. Hence, we recommend that users search for an appropriate value of $w$ within the range of 0.1 to 0.5.


 % It is evident that the minimum distance between quantum labels decreases as the number of classes increases. 
 
 % Additionally, the parameter R becomes increasingly important in improving the quality of the model. 





\subsection{Noisy quantum system}

\begin{table}[]
    \centering
    \caption{Test accuracy of MORE on Iris dataset with noisy backends}
    \begin{tabular}{cccc}
\toprule
\# Shots & FakeAuckland & FakeAthensV2 & FakeBelemV2 \\ \midrule
1k    & 95.56        & 95.56        & 93.33       \\ \midrule
2k    & 97.78        & 93.33        & 95.56       \\ \midrule
3k    & 97.78        & 93.33        & 95.56       \\ \midrule
4k    & 97.78        & 93.33        & 95.56       \\ \midrule
5k    & 93.33        & 93.33        & 95.56       \\ \midrule
6k    & 95.56        & 95.56        & 95.56       \\ \bottomrule
\end{tabular} 
    
    \label{tab:iris}
\end{table}

Table \ref{tab:iris} summarizes the test accuracy of MORE using the noisy quantum backends, including FakeAuckland, FakeAthensV2, and FakeBelemV2 on the Iris dataset. The hybrid quantum-classical training method of QNN typically requires a long time to execute on a quantum machine, but the IBM cloud quantum computing platform imposes time limits for tasks. In this experiment, we therefore utilize simulators of noisy quantum computing systems. The noisy backends used in this experiment have the noise model collected from real quantum machines, which includes T1 and T2 time, 1-qubit and 2-qubit gate errors, and measurement errors. So these simulated backends provide us with a practical and reasonable way to evaluate the performance of MORE in noisy quantum systems. To process the Iris instances of size 4, we construct 4-qubit QNNs with 39 trainable parameters. We update the QNNs for 300 steps using the COBYLA optimizer. We varied the number of shots for the quantum program from 1,000 to 6,000 on the backends to examine the impact of quantum computation costs. 




% The Iris dataset comprises 150 instances divided into three classes, with each class containing 50 instances. We used 70\% of the instances (105 instances) for training, while the remaining 30\% (45 instances) were reserved for testing purposes. 

The MOREs reach their highest accuracies of 97.78 (44/45), 95.56 (43/45), and 95.56 (43/45) on the FakeAuckland, FakeAthensV2, and FakeBelemV2 backends, respectively. The noisy MOREs achieve comparable performance to the noise-free version, with an accuracy of 97.78\%. Surprisingly, we found that the accuracy was not directly proportional to the number of shots, suggesting that the quantum computation cost required for this task is not significant. However, we acknowledge that the number of shots required may increase with an increase in the number of classes. Nonetheless, the cost is still acceptable only if the distribution of learned quantum labels is scattered in the Hilbert space. Furthermore, while it is true that accurate measurement results are necessary for the MORE approach, we want to highlight that using a simple quantum circuit (with fewer qubits, gates, and shallow depth) can lead to less error compared to other methods. This makes the MORE approach feasible and promising to implement on NISQ machines.

















































