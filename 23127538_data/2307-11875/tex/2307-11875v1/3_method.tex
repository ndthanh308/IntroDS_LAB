\section{Measurement and correlation-based QNN multi-classifier} \label{sec:method}

% Methodology
In this section, we present MORE, an efficient quantum multi-classifier that employs a simple QNN with a single readout qubit. We use QST to reconstruct the state of the readout qubit to retrieve the entire information from it. This allows us to fully leverage information of a single qubit to address complex multi-classification tasks via two steps. First, we employ the variational quantum clustering method to convert the classical labels of the dataset into 1-qubit states, taking into account interclass correlations. Next, we proceed to train the QNN using a quantum label-based supervised learning approach, where the QNN learns to map an input to its corresponding quantum label. After completing the training process, the model can accurately categorize new data by comparing the readout state of the data with quantum labels and assigning it to the appropriate class based on the shortest distance.







% By optimizing the QNN's parameters, we can achieve accurate predictions for multi-classification problems using quantum information.



% Then, followed by quantum label-based supervised learning, the QNN is trained to map a given input to its corresponding quantum label.



% Previous quantum applications use only partial information of readout qubits by performing regular z-measurement, wasting the remaining valuable information. To obtain sufficient information from the quantum system, they have to use ancilla readout qubits. 

 



% The majority of quantum applications output their results by measuring the readouts on z axis, due to the quantum hardware implementation only supporting z-measurement. In this case, the readout only conveys partial information for understanding the quantum application outcomes, leading to a waste of quantum information. 


% Most quantum applications only use partial information from the qubits they read, wasting the remaining valuable information. The wasted information has to be compensated with extra operations, e.g., more sophisticated quantum circuits or substantial classical post-processing. In order to overcome this shortcoming, we reconstruct the full quantum state of a readout qubit and utilize the state information exhaustively to perform complicated data processing. In this section, we present MORE, a quantum multi-classification approach based on the reconstructed quantum state of a readout qubit and the correlation across classes to infer the correlation between quantum states in a three-dimensional space.



\subsection{Overview}
The overview of MORE is presented in Fig.~\ref{fig:overview}. The goal is to solve a multi-classification task that assigns appropriate labels to classical input data. A training dataset consisting of $K$ subsets is provided, $D = \{D_1, D_2, \dots , D_K\}$. The subset $D_k$ is a collection of training data in class $k$, with each training data instance being a pair of data point $x$ and its correct classical label $y$. MORE addresses this problem by utilizing a QNN to learn the pattern that characterizes the mapping between the input data point and its correct label from the training dataset. The learning of MORE is a two-step procedure. First, the classical labels $y$ are converted into quantum labels $\overrightarrow{y}$ in the two-dimensional Hilbert space. This step employs the quantum clustering method while considering the inherent correlation between classes. (Fig.~\ref{fig:overview}(a-d) and Section~\ref{sec:clt}). Then, quantum label-based supervised learning is undertaken to fine-tune the parameters (Fig.~\ref{fig:overview}(e) and Section~\ref{sec:super}).


% Note that for the sake of unifying the notation in this section, the superscripts denote the classes rather than the exponentiation. It sequentially takes two training instances as input in each training step.

The ansatz of MORE is shown in Fig~\ref{fig:overview}(a). Its circuit includes three layers: (1) Encoding layer: the encoding layer quantizes the classical input data by preparing the quantum states based on classical data values. MORE is compatible with any general encoding scheme, such as angle and amplitude encoding. (2) Variational data processing layer $U(\theta)$: this layer transforms the prepared state using its parameterized gates. The parameter set $\theta$ of the layer is iteratively adjusted based on MORE's performance using a hybrid classical-quantum method, as introduced in section~\ref{sec:pre}. The optimal parameters should enable MORE to assign a label $y$ to unseen input $x$ with high accuracy. (3) Measurement layer $M$: One qubit is chosen as the readout in this layer, and its observable $\sigma_x$, $\sigma_y$, and $\sigma_z$ are measured to produce a three-dimensional vector (reconstructed state vector $\overrightarrow{v_i}$ and $\overrightarrow{v_j}$).


As the quantum measurement 
operator in superconducting hardware implementation can only measure the state on the z-axis, MORE requires two additional measurement units to measure the observables $\sigma_x$ and $\sigma_y$, as shown in Fig.~\ref{fig:munit}. The unit for observable $\sigma_x$ inserts an $H$ gate before the measurement operator, which can be conceptualized as rotating the x-axis of the Bloch sphere to the location of the z-axis first and then projecting the state vector to the new z-axis. Similarly, for observable $\sigma_y$ the readout qubit passes an $S^\dagger$ gate and an $H$ gate, followed by the measurement operator in the measurement unit. In the measurement layer of the QNN, these three measurement units are applied in succession to construct the state vector $\overrightarrow{v} = (\langle \sigma_x \rangle, \langle \sigma_y \rangle, \langle \sigma_z \rangle)$.

% Figure environment removed






\subsection{Class correlation-based variational quantum clustering} \label{sec:clt}

To perform multi-classification on the 1-qubit state vectors, it is necessary to determine the distribution of classes in a two-dimensional Hilbert space first. This involves converting classical labels to quantum labels. To accomplish this, we use the \textit{variational quantum clustering} method while investigating the correlation between classes. Deep clustering is a technique in classical machine learning that concurrently learns the parameters of neural networks and the cluster assignments for processed inputs \cite{caron2018deep}. Here, we borrow this idea and apply it to variational quantum clustering. Specifically, we train the parameters of the QNN to map readout state vectors from the same class to a particular quantum state.

\textbf{Class correlation.}
First of all, we calculate the correlation between classes by measuring the difference between the data instances belonging to different class collections. To be specific, we first pre-process the training data using Principal Component Analysis (PCA) to extract critical features and downscale the data to a size that is suitable for our quantum circuit. Then, the average pattern set $D_{pattern}$ is created by averaging the data instances of the class dataset $D_k$,
\begin{equation}
    D_{pattern} = \{\overline{x}_1, \overline{x}_2, \dots, \overline{x}_K\}
\end{equation}
where $\overline{x}_k = \frac{1}{|D_k|} \sum_{i=1}^{|D_k|} x^i_k$, $k \in [1, K]$. The correlation between the average pattern of classes is then recorded in a $K \times K$ array, known as scaler array $S$. For example, we use Mean square error (MSE) to quantify the difference between two average patterns. As a result, the entry $(i, j)$ of the scaler array $S$ can be calculated by 
\begin{equation}
    S_{i, j} = MSE(\overline{x}_i, \overline{x}_j)
\end{equation}
The larger the MSE value, the lower the correlation. The original diagonal entries of $S$ are 0s, indicating that the correlation between two identical patterns is maximal. We modify these 0s to negative values, which will be explained and utilized later in this section. Fig.~\ref{fig:overview}(b) shows an example of the scaler array $S$ that stores the correlations between classes `0', `1', and `2' from the MNIST handwritten digit dataset.




\textbf{Clustering dataset.}
We then prepare the clustering dataset by pairing the instances in the given dataset $D$. The clustering dataset consisting of data pairs: $D_{cluster} = \{ [(x_i, y_i), (x_j, y_j)]_t\}_{t=1}^{N}$ where $i, j \in [1, K ]$ and $N$ is a small number to ensure the model can fast converge. In our evaluation, we set it to $\binom{5K}{2}$. Each data pair is independently sampled from $D$ across all classes, i.e., $(x_i, y_i)$ and $(x_j, y_j)$ are two sampled instances from class $i$ and $j$.

% For each pair, $(x_i, y_i)$ and $(x_j, y_j)$ are independently sampled from $D$ across all classes. 

\textbf{Clustering loss.}
In a training step, the QNN $U(\theta)$ sequentially takes as input of a data pair $x_i$ and $x_j$ and generates two three-dimensional state vectors $\overrightarrow{v_i} = (\langle \sigma_x \rangle_i, \langle \sigma_y \rangle_i, \langle \sigma_z \rangle_i)$ and $\overrightarrow{v_j} = (\langle \sigma_x \rangle_j, \langle \sigma_y \rangle_j, \langle \sigma_z \rangle_j)$. We express the loss function as
\begin{equation}
    \mathcal{L}_{clt} = - S_{i,j} \times \mathcal{D}ist (\overrightarrow{v_i}, \overrightarrow{v_j})
\end{equation}
where
\begin{equation}
    \mathcal{D}ist(\overrightarrow{v_i}, \overrightarrow{v_j}) = 1 - \frac{\overrightarrow{v_i} \cdot \overrightarrow{v_j}}{|| \overrightarrow{v_i}|| \ ||\overrightarrow{v_j} ||}
\end{equation}
% \begin{equation}
%     \mathcal{L}_{clt} = - S_{i,j} \Big(1 - \frac{\overrightarrow{y_i} \cdot \overrightarrow{y_j}}{|| \overrightarrow{y_i}|| \cdot ||\overrightarrow{y_j} ||} \Big).
% \end{equation}
The function  $\mathcal{D}ist(\overrightarrow{v_i}, \overrightarrow{v_j})$ computes the Cosine distance between two vectors $\overrightarrow{v_i}$ and $\overrightarrow{v_j}$, and the coefficient $S_{i,j}$ serves as the scaler of cosine distance. We assume that the distribution of state vectors in a two-dimensional Hilbert space is relevant to the distribution of data in feature space. Therefore, the clustering step aims to bring the generated state vectors $\overrightarrow{v_i}$ and $\overrightarrow{v_j}$ closer together if $x_i$ and $x_j$ belong to the same class, and farther apart otherwise. In addition, the distance between $\overrightarrow{v_i}$ and $\overrightarrow{v_j}$ will be larger if their class correlation is significantly smaller. Specifically, as illustrated in Fig.~\ref{fig:overview}(c), if $x_i$ and $x_j$ from the same class, the negative scaler value $S_{i, j}$ where $i=j$, will minimize the cosine distance between $\overrightarrow{v_i}$ and $\overrightarrow{v_j}$. Otherwise, the distance between $\overrightarrow{v_i}$ and $\overrightarrow{v_j}$ will be maximized, and the final distance between them depends on the value of $S_{i, j}$. The higher the correlation, the closer they will be.

\textbf{Quantum labels.}
After the training of quantum clustering, the state vectors of data instances belonging to the same class are oriented in similar directions (a cluster). The centroid of these directions is recorded as the quantum label of the class. So far, we have mapped the classcial labels $y$ into quantum labels $\overrightarrow{y}$, which are distributed in the Hilbert space based on the correlation between classes, as shown in Fig.~\ref{fig:overview}(d). Then we substitute the classical labels $y$ in the training dataset $D$ with quantum labels $\overrightarrow{y}$, and now we are ready to proceed with quantum label-based quantum supervised learning.




\subsection{Quantum label-based supervised learning} \label{sec:super}

After the clustering step, we initially train the parameters $\theta$ of QNN $U(\theta)$ to determine the quantum labels for classes. However, as the $U(\theta)$ is trained on a small subset of the training data, it may not be optimal for classification. Thus, to enhance the performance of $U(\theta)$, we fine-tune the parameters by conducting quantum label-based supervised learning. We use the dataset containing the training data points $x$ and their quantum labels $\overrightarrow{y}$ to train $U(\theta)$. The goal is to map the readout state vector of inputs to be as close as possible to their corresponding quantum labels, as shown in Fig.~\ref{fig:overview} (e). In this step, the $U(\theta)$ takes one training instance as input and generates the 1-qubit state vector $\overrightarrow{v}$. Then, we compare the generated vector with its corresponding quantum label $\overrightarrow{y}$, and express the loss function of quantum label-based supervised learning as
\begin{equation}
\label{eq:sup_loss1}
    \mathcal{L}_{sup} = \mathcal{D}ist(\overrightarrow{v}, \overrightarrow{y})
\end{equation}
The training procedure aims to minimize the value of $ \mathcal{L}_{sup}$. For inference, the class of the unseen data is determined by
\begin{equation}
    y = argmin_k \{\mathcal{D}ist(\overrightarrow{v}, \overrightarrow{y_k}) \}
\end{equation}
where $\overrightarrow{y_k}$ is the quantum label for class $k \in [1, K]$.

% Figure environment removed

However, misclassifications are more likely to occur when there are a large number of classes and their quantum labels are closely distributed in the Hilbert space, which is known as the curse of density (see Fig.~\ref{fig:lossreg} left). The circle is a side view of the Bloch sphere, and the colored dots are quantum labels. The triangle represents the readout state of an input. It may be classified into a nearby but incorrect class (represented by the pink dot) due to the small distance between the readout state and the pink quantum label. To improve the accuracy of the classifier in scenarios with crowded quantum labels, we introduce a loss adjuster $\mathcal{R}$.

In each training step, if the distance between the readout state and its correct quantum label is less than a threshold $r$, we also consider the other quantum labels whose distance from the readout state is less than the threshold, as shown in Fig.~\ref{fig:lossreg} right. Specifically, when $\mathcal{D}ist(\overrightarrow{y}, \overrightarrow{v}) \leq r$, we define the loss regulator as
\begin{equation}
\label{eq:reg}
    \mathcal{R} = \sum_{k \in K'} \mathcal{D}ist(\overrightarrow{y_k}, \overrightarrow{v})
\end{equation}
where $K'$ is a subset of classes whose quantum label $\overrightarrow{y_k}$ satisfies $\mathcal{D}ist(\overrightarrow{y_k}, \overrightarrow{v}) \leq r$. 


We then assign a weight factor $w \in [0, 1]$ to the supervised loss term. Hence, the overall objective function of the quantum labels-based training process is 
\begin{equation}
\label{eq:loss_r}
    \mathcal{L}_{sup+\mathcal{R}} = w \mathcal{L}_{sup} - (1-w)\mathcal{R}.
\end{equation}
By minimizing the value of $\mathcal{L}_{sup+\mathcal{R}}$, this objective function tends to bring the readout state closer to its correct quantum label while moving farther away from the incorrect quantum labels surrounding it, resulting in the readout state being closest to its correct label among crowded quantum labels. The selection strategy for the values of $r$ and $w$ will be discussed in Sec.~\ref{sec:reg}.







% We notice, however, that misclassification occurs between classes whose spatial labels are near together, as demonstrated in figure x (mis-class between 4 and 9). To address this issue, we introduce a loss regulator for $\mathcal{L}_{sup}$.



% [figure for spatial label correlation, heatmap]

% The loss regulator $\mathcal{R}$ is defined as the distance between the state vector and the incorrect labels near it within the pre-determined threshold $\lambda$.

% For input $x_i$, we select the classes j such that $S_{i, j} \leq \lambda$.
% maximize R
% \begin{equation}
%     \mathcal{R} = \sum S_{i, j}\mathcal{D}_{cos}(\overrightarrow{y_i}, \overrightarrow{y})
% \end{equation}



% [figure: summary of the misclassification between 4 and 9 w/ and w/o regulator.]




