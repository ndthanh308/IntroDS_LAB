\section{introduction} \label{sec:intro}


% - quantum nn advantage: shallow, less parameters, fast to train

Quantum computing derived from quantum mechanics can exponentially accelerate the solution of specific problems compared to classical computing, because of the unique properties of superposition and entanglement \cite{arute2019quantum}. Nevertheless, we are now in the Noisy Intermediate-Scale Quantum (NISQ) era, in which quantum machines contain just 50-100 qubits and are error-prone. Several months ago, IBM unveiled Osprey, the world's largest quantum computer with 433 qubits \cite{Collins2022ibm}. This effort will provide the groundwork for the next era of quantum-centric supercomputing. However, its computing procedure is still plagued with qubit decoherence errors, gate errors, and measurement errors \cite{clerk2010introduction}. Quantum Machine learning (QML) is one of the most attractive applications of quantum computing because of its demand for computing power and its resilience to noise \cite{biamonte2017quantum, schuld2015introduction, aimeur2006machine}. In recent years, many efforts have been made to develop QML techniques as the counterparts of classical ones, such as quantum k-nearest neighbor \cite{aimeur2006machine, dang2018image}, quantum support vector machines \cite{rebentrost2014quantum, li2015experimental, chatterjee2016generalized}, Quantum clustering \cite{maier2005quantum, aimeur2007quantum}, and quantum neural networks (QNNs) \cite{farhi2018classification, beer2020training, jeswal2019recent}. Since classical neural networks have achieved remarkable success in various fields \cite{shahin2001artificial, fu2022application, lakhan2022deep, elahi2022application, oza2022deep}, QNNs have also been attempted for data processing in many domains, including finance \cite{paquet2022quantumleap, el2018forecasting}, chemistry \cite{kandala2017hardware, sagingalieva2022hybrid} and healthcare \cite{hassan2020discrimination, esposito2022quantum, narain2016cardiovascular, mathur2021medical, umer2022integrated}.


% Figure environment removed

Classification is a fundamental data processing technique in many domains, making QNN-based classifiers an area of significant interest. 
In 2018, \textit{Farhi} and \textit{Neven} proposed the first QNN classifiers for near-term processors \cite{farhi2018classification}. Following this, in 2019, \textit{Cong et al.} proposed quantum convolutional neural networks (QCNNs) for image processing, which adopt the architecture design of its classical analog \cite{cong2019quantum}. Based on these two quantum classifier designs, numerous variants are shown \cite{wu2022wpscalable, hur2022quantum, chalumuri2021hybrid, abohashima2020classification}. Moreover, some other QNN designs are suggested in \cite{henderson2020quanvolutional, blank2020quantum, schuld2017implementing, grant2018hierarchical, stein2022quclassi, abohashima2020classification}.
% The introduced $n$+1-qubit QNN classifier can categorize the classical dataset with binary labels $y \in \{0, 1\}$. The first $n$ qubits of the ansatz are used to process data utilizing a set of parameterized quantum gates. And the last qubit serves as a readout that produces the output by performing measurements. The approach uses a classical-quantum hybrid procedure to iteratively update the trainable parameters, with the goal of achieving the highest possible accuracy for the quantum classifier to classify previously unseen data.
% , a classical machine computes parameter updates to minimize the difference between the predicted label $y'$ and the correct label $y$. Lastly, the parameters of the QNN classifier are continuously updated until convergence, allowing the trained classifier to classify unseen data with the highest possible accuracy. 
% Following the same training procedure, \textit{Cong et al.} propose quantum convolutional neural networks (QCNNs) for image processing, which adopt the architecture design of its classical analog \cite{cong2019quantum}. 
% The QCNN's convolutional layer comprises two-qubit unitary operators for data transformation; its pooling layer is conducted by measuring a subset of qubits to reduce the problem size and introduce nonlinearities. 
% This n-qubit QCNN using only $O(log(n))$ parameters is effectively trainable and implementable on NISQs. On the basis of these two quantum classifier architectures, numerous variations are shown \cite{wu2022wpscalable, hur2022quantum, chalumuri2021hybrid, abohashima2020classification}. Moreover, many other QNN ansatz designs are suggested \cite{henderson2020quanvolutional, blank2020quantum, schuld2017implementing, grant2018hierarchical, stein2022quclassi, abohashima2020classification}.



Prior works on QNN-based classification have predominantly focused on binary classification tasks, with relatively few addressing multi-classification problems. This is partly due to the fact that QNNs typically produce binary measurement results, meaning that a single readout qubit can only represent two classes. The popular solutions for QNN-based multi-classification are: (1) use extra ancillary qubits to represent labels for multiple classes. However, qubits are a limited resource on NISQ machines, and using more ancillary qubits reduces the number of qubits available for data processing, thus limiting the capacity of the QNN. (2) Select a subset of qubits at the end of the QNN circuit to serve as the readout for multi-class labels. However, if the one-hot encoding is used to represent labels, the number of classes a QNN-based classifier can handle is limited by the number of qubits available in the circuit. On the other hand, if binary encoding is used to represent labels, there may be cases where measurement results are not interpretable, which is a limitation that also applies to the previously mentioned solution. Specifically, the binary property of qubit measurement results enables an $n$-qubit state to include $2^n$ possible labels. Thus, given a $p$-classification task, where $p \in (2^{n-1}, 2^n)$, some output states will not map to any valid class. Especially when $p = 2^{n-1} + 1$, almost half of the label space is wasted. (3) Attaching classical fully-connected layers after QNNs is a common approach to mapping the QNN outcomes to multiple classes. However, this may incur high computational costs because the fully-connected layers contain a large number of parameters to train. Moreover, adding classical layers after QNNs may offset the potential quantum advantage of using QNNs for classification. And (4) decomposing the multi-classification task into several binary classification ones. The downsides include a complicated training process and lengthy training and inference times due to the need to train multiple binary classifiers. Therefore, there is a need for a novel QNN-based multi-classifier that features a concise variational quantum circuit, a streamlined classical post-processing procedure, and a straightforward training process.


In this article, we propose \textit{MORE} approach, a novel QNN multi-classifier that utilizes measurements and interclass correlations. MORE is designed to be resource-efficient, featuring a simple ansatz similar to that used in binary classification, with only one readout qubit at the end of the circuit. MORE is a two-step approach, as shown in fig.~\ref{fig:twostep}. First, it converts the classical labels of training data into quantum labels, which are quantum states reconstructed from the measurement results of the readout qubit. The quantum label of each class is determined using the variational quantum clustering method that considers the correlation between classes. Then, quantum label-based supervised learning is conducted to converge the QNN and achieve the desired performance quickly. Compared to prior QNN-based multi-classifiers, MORE requires fewer qubits and quantum gates, resulting in fewer gate and decoherence errors during the quantum state evolution. Moreover, MORE is computationally efficient, with a fixed amount of quantum computation and linearly increasing classical computation, allowing scalability to more complex tasks.


The contribution of this work is fourfold:
\begin{itemize}
    \item We present the attempt to design efficient quantum multi-classifiers by leveraging the full quantum information contained in a single qubit.
    \item We offer a technique for converting the classical labels of training data into quantum states by investigating interclass correlations.
    \item We introduce a quantum label-based quantum supervised learning approach that includes a loss adjuster to improve the quality of the QNN model.
    \item We implement the proposed MORE approach and evaluate it from many perspectives using comprehensive experiments. And our experiment results demonstrate the advantages of our proposed approach.
\end{itemize}

In the remaining content of this article, we provide some basic knowledge needed for understanding this article in section~\ref{sec:pre}. We then introduce our motivation for the proposed approach by discussing quantum state tomography, which is a method for reconstructing quantum states, in Section~\ref{sec:tomo}. The proposed two-step approach MORE is detailed in section~\ref{sec:method}. The first step, variational quantum clustering based on interclass correlation, is described in subsection~\ref{sec:clt}, then the second step, quantum label-based supervised learning, is discussed in subsection~\ref{sec:super}. Then, we present the evaluation setup and results in Section~\ref{sec:eval}, followed by a review of related works in Section~\ref{sec:related}. Finally, we conclude our proposed MORE approach in Section~\ref{sec:conclusion}.










% MulQ performs multi-classification by making full use of the quantum information of the readout. To achieve this, first of all, we measure three observables of readout to map its quantum state from Helber space to Bloch shpere. 


% but the different is that the readout of MulQ is measured 






% relationship between classes

% 2-d to 3-d by measuring

% relative distance

% three observables, eigen basis of Hilbert space(state space of a single qubit)












 





















%every application is for classification, but for binary classification,. some works for multi-class. but not efficient








