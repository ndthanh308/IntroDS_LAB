\begin{thebibliography}{43}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Andreas(2022)]{andreas2022language}
Andreas, J.
\newblock Language models as agent models.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  EMNLP 2022}, pp.\  5769--5779, Abu Dhabi, United Arab Emirates, December
  2022. Association for Computational Linguistics.
\newblock URL \url{https://aclanthology.org/2022.findings-emnlp.423}.

\bibitem[Bai et~al.(2022)Bai, Jones, Ndousse, Askell, Chen, DasSarma, Drain,
  Fort, Ganguli, Henighan, Joseph, Kadavath, Kernion, Conerly, El-Showk,
  Elhage, Hatfield-Dodds, Hernandez, Hume, Johnston, Kravec, Lovitt, Nanda,
  Olsson, Amodei, Brown, Clark, McCandlish, Olah, Mann, and
  Kaplan]{bai2022training}
Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D.,
  Fort, S., Ganguli, D., Henighan, T., Joseph, N., Kadavath, S., Kernion, J.,
  Conerly, T., El-Showk, S., Elhage, N., Hatfield-Dodds, Z., Hernandez, D.,
  Hume, T., Johnston, S., Kravec, S., Lovitt, L., Nanda, N., Olsson, C.,
  Amodei, D., Brown, T., Clark, J., McCandlish, S., Olah, C., Mann, B., and
  Kaplan, J.
\newblock Training a helpful and harmless assistant with reinforcement learning
  from human feedback.
\newblock arXiv preprint 2204.05862, 2022.

\bibitem[Bird et~al.(2009)Bird, Loper, and Klein]{bird2009}
Bird, S., Loper, E., and Klein, E.
\newblock \emph{Natural {L}anguage {P}rocessing with {P}ython}.
\newblock O'Reilly Media, Inc., USA, 2009.

\bibitem[Bowman et~al.(2022)Bowman, Hyun, Perez, Chen, Pettit, Heiner,
  Lukošiūtė, Askell, Jones, Chen, Goldie, Mirhoseini, McKinnon, Olah,
  Amodei, Amodei, Drain, Li, Tran-Johnson, Kernion, Kerr, Mueller, Ladish,
  Landau, Ndousse, Lovitt, Elhage, Schiefer, Joseph, Mercado, DasSarma, Larson,
  McCandlish, Kundu, Johnston, Kravec, El~Showk, Fort, Telleen-Lawton, Brown,
  Henighan, Hume, Bai, Hatfield-Dodds, Mann, and Kaplan]{bowman2022measuring}
Bowman, S.~R., Hyun, J., Perez, E., Chen, E., Pettit, C., Heiner, S.,
  Lukošiūtė, K., Askell, A., Jones, A., Chen, A., Goldie, A., Mirhoseini,
  A., McKinnon, C., Olah, C., Amodei, D., Amodei, D., Drain, D., Li, D.,
  Tran-Johnson, E., Kernion, J., Kerr, J., Mueller, J., Ladish, J., Landau, J.,
  Ndousse, K., Lovitt, L., Elhage, N., Schiefer, N., Joseph, N., Mercado, N.,
  DasSarma, N., Larson, R., McCandlish, S., Kundu, S., Johnston, S., Kravec,
  S., El~Showk, S., Fort, S., Telleen-Lawton, T., Brown, T., Henighan, T.,
  Hume, T., Bai, Y., Hatfield-Dodds, Z., Mann, B., and Kaplan, J.
\newblock Measuring progress on scalable oversight for large language models.
\newblock arXiv preprint 2211.03540, 2022.

\bibitem[Branwen(2023)]{gwern2023stegonography}
Branwen, G., 01 2023.
\newblock URL
  \url{https://www.lesswrong.com/posts/bwyKCQD7PFWKhELMr/by-default-gpts-think-in-plain-sight?commentId=zfzHshctWZYo8JkLe}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan,
  Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess,
  Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{brown2020language}
Brown, T.~B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S.,
  Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler,
  D.~M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray,
  S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever,
  I., and Amodei, D.
\newblock Language models are few-shot learners, 2020.
\newblock URL \url{https://arxiv.org/abs/2005.14165}.

\bibitem[Christiano et~al.(2017)Christiano, Leike, Brown, Martic, Legg, and
  Amodei]{christiano2017deep}
Christiano, P.~F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D.
\newblock Deep reinforcement learning from human preferences.
\newblock In Guyon, I., Luxburg, U.~V., Bengio, S., Wallach, H., Fergus, R.,
  Vishwanathan, S., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems}, volume~30. Curran Associates, Inc., 2017.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2017/file/d5e2c0adad503c91f91df240d0cd4e49-Paper.pdf}.

\bibitem[Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick,
  and Tafjord]{clark2018think}
Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and
  Tafjord, O.
\newblock Think you have solved question answering? try arc, the ai2 reasoning
  challenge.
\newblock arXiv preprint 1803.05457, 2018.

\bibitem[Creswell \& Shanahan(2022)Creswell and Shanahan]{creswell2022faithful}
Creswell, A. and Shanahan, M.
\newblock Faithful reasoning using large language models.
\newblock arXiv preprint 2208.14271, 2022.

\bibitem[Creswell et~al.(2023)Creswell, Shanahan, and
  Higgins]{creswell2023selectioninference}
Creswell, A., Shanahan, M., and Higgins, I.
\newblock Selection-inference: Exploiting large language models for
  interpretable logical reasoning.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=3Pf3Wg6o-A4}.

\bibitem[Du et~al.(2023)Du, Li, Torralba, Tenenbaum, and
  Mordatch]{du2023improving}
Du, Y., Li, S., Torralba, A., Tenenbaum, J.~B., and Mordatch, I.
\newblock Improving factuality and reasoning in language models through
  multiagent debate.
\newblock arXiv preprint 2305.14325, 2023.

\bibitem[Dua et~al.(2022)Dua, Gupta, Singh, and
  Gardner]{dua-etal-2022-successive}
Dua, D., Gupta, S., Singh, S., and Gardner, M.
\newblock Successive prompting for decomposing complex questions.
\newblock In \emph{Proceedings of the 2022 Conference on Empirical Methods in
  Natural Language Processing}, pp.\  1251--1265, Abu Dhabi, United Arab
  Emirates, December 2022. Association for Computational Linguistics.
\newblock URL \url{https://aclanthology.org/2022.emnlp-main.81}.

\bibitem[Ganguli et~al.(2023)Ganguli, Askell, Schiefer, Liao, Lukošiūtė,
  Chen, Goldie, Mirhoseini, Olsson, Hernandez, Drain, Li, Tran-Johnson, Perez,
  Kernion, Kerr, Mueller, Landau, Ndousse, Nguyen, Lovitt, Sellitto, Elhage,
  Mercado, DasSarma, Rausch, Lasenby, Larson, Ringer, Kundu, Kadavath,
  Johnston, Kravec, Showk, Lanham, Telleen-Lawton, Henighan, Hume, Bai,
  Hatfield-Dodds, Mann, Amodei, Joseph, McCandlish, Brown, Olah, Clark, Bowman,
  and Kaplan]{ganguli2023capacity}
Ganguli, D., Askell, A., Schiefer, N., Liao, T.~I., Lukošiūtė, K., Chen, A.,
  Goldie, A., Mirhoseini, A., Olsson, C., Hernandez, D., Drain, D., Li, D.,
  Tran-Johnson, E., Perez, E., Kernion, J., Kerr, J., Mueller, J., Landau, J.,
  Ndousse, K., Nguyen, K., Lovitt, L., Sellitto, M., Elhage, N., Mercado, N.,
  DasSarma, N., Rausch, O., Lasenby, R., Larson, R., Ringer, S., Kundu, S.,
  Kadavath, S., Johnston, S., Kravec, S., Showk, S.~E., Lanham, T.,
  Telleen-Lawton, T., Henighan, T., Hume, T., Bai, Y., Hatfield-Dodds, Z.,
  Mann, B., Amodei, D., Joseph, N., McCandlish, S., Brown, T., Olah, C., Clark,
  J., Bowman, S.~R., and Kaplan, J.
\newblock The capacity for moral self-correction in large language models,
  2023.

\bibitem[Gao(2023)]{gao2023shapley}
Gao, L.
\newblock Shapley value attribution in chain of thought.
\newblock
  \url{https://www.lesswrong.com/posts/FX5JmftqL2j6K8dn4/shapley-value-attribution-in-chain-of-thought},
  04 2023.

\bibitem[Gunning et~al.(2019)Gunning, Stefik, Choi, Miller, Stumpf, and
  Yang]{gunning2019explainable}
Gunning, D., Stefik, M., Choi, J., Miller, T., Stumpf, S., and Yang, G.-Z.
\newblock Xai\&\#x2014;explainable artificial intelligence.
\newblock \emph{Science Robotics}, 4\penalty0 (37):\penalty0 eaay7120, 2019.
\newblock \doi{10.1126/scirobotics.aay7120}.
\newblock URL
  \url{https://www.science.org/doi/abs/10.1126/scirobotics.aay7120}.

\bibitem[Hendrycks et~al.(2021)Hendrycks, Burns, Basart, Zou, Mazeika, Song,
  and Steinhardt]{hendrycks2021measuring}
Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and
  Steinhardt, J.
\newblock Measuring massive multitask language understanding.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=d7KBjmI3GmQ}.

\bibitem[Holtzman et~al.(2020)Holtzman, Buys, Du, Forbes, and
  Choi]{holtzman2020curious}
Holtzman, A., Buys, J., Du, L., Forbes, M., and Choi, Y.
\newblock The curious case of neural text degeneration.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=rygGQyrFvH}.

\bibitem[Holzinger et~al.(2017)Holzinger, Biemann, Pattichis, and
  Kell]{holzinger2017need}
Holzinger, A., Biemann, C., Pattichis, C.~S., and Kell, D.~B.
\newblock What do we need to build explainable ai systems for the medical
  domain?
\newblock arXiv preprint 1712.09923, 2017.

\bibitem[Jacovi \& Goldberg(2020)Jacovi and Goldberg]{jacovi2020towards}
Jacovi, A. and Goldberg, Y.
\newblock Towards faithfully interpretable {NLP} systems: How should we define
  and evaluate faithfulness?
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics}, pp.\  4198--4205, Online, July 2020.
  Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.acl-main.386}.
\newblock URL \url{https://aclanthology.org/2020.acl-main.386}.

\bibitem[Lanham(2022)]{lanham2022externalized}
Lanham, T.
\newblock Externalized reasoning oversight: a research direction for language
  model alignment, 08 2022.
\newblock URL
  \url{https://www.lesswrong.com/posts/FRRb6Gqem8k69ocbi/externalized-reasoning-oversight-a-research-direction-for}.

\bibitem[Li et~al.(2022)Li, Chen, Shen, Chen, Zhang, Li, Wang, Qian, Peng, Mao,
  Chen, and Yan]{li2022explanations}
Li, S., Chen, J., Shen, Y., Chen, Z., Zhang, X., Li, Z., Wang, H., Qian, J.,
  Peng, B., Mao, Y., Chen, W., and Yan, X.
\newblock Explanations from large language models make small reasoners better.
\newblock arXiv preprint 2210.06726, 2022.

\bibitem[Lin et~al.(2022)Lin, Hilton, and Evans]{lin2022truthfulqa}
Lin, S., Hilton, J., and Evans, O.
\newblock {T}ruthful{QA}: Measuring how models mimic human falsehoods.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pp.\  3214--3252,
  Dublin, Ireland, May 2022. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.acl-long.229}.
\newblock URL \url{https://aclanthology.org/2022.acl-long.229}.

\bibitem[Ling et~al.(2017)Ling, Yogatama, Dyer, and Blunsom]{ling2017program}
Ling, W., Yogatama, D., Dyer, C., and Blunsom, P.
\newblock Program induction by rationale generation: Learning to solve and
  explain algebraic word problems.
\newblock In \emph{Proceedings of the 55th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pp.\  158--167,
  Vancouver, Canada, July 2017. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/P17-1015}.
\newblock URL \url{https://aclanthology.org/P17-1015}.

\bibitem[Liu et~al.(2020)Liu, Cui, Liu, Huang, Wang, and Zhang]{liu2020logiqa}
Liu, J., Cui, L., Liu, H., Huang, D., Wang, Y., and Zhang, Y.
\newblock Logiqa: A challenge dataset for machine reading comprehension with
  logical reasoning.
\newblock In Bessiere, C. (ed.), \emph{Proceedings of the Twenty-Ninth
  International Joint Conference on Artificial Intelligence, {IJCAI-20}}, pp.\
  3622--3628. International Joint Conferences on Artificial Intelligence
  Organization, 7 2020.
\newblock \doi{10.24963/ijcai.2020/501}.
\newblock URL \url{https://doi.org/10.24963/ijcai.2020/501}.
\newblock Main track.

\bibitem[Lyu et~al.(2023)Lyu, Havaldar, Stein, Zhang, Rao, Wong, Apidianaki,
  and Callison-Burch]{lyu2023faithful}
Lyu, Q., Havaldar, S., Stein, A., Zhang, L., Rao, D., Wong, E., Apidianaki, M.,
  and Callison-Burch, C.
\newblock Faithful chain-of-thought reasoning.
\newblock arXiv preprint 2301.13379, 2023.

\bibitem[Madaan \& Yazdanbakhsh(2022)Madaan and Yazdanbakhsh]{madaan2022text}
Madaan, A. and Yazdanbakhsh, A.
\newblock Text and patterns: For effective chain of thought, it takes two to
  tango.
\newblock arXiv preprint 2209.07686, 2022.

\bibitem[McKenzie et~al.(2023)McKenzie, Lyzhov, Pieler, Parrish, Mueller,
  Prabhu, McLean, Kirtland, Ross, Liu, Gritsevskiy, Wurgaft, Kauffman, Recchia,
  Liu, Cavanagh, Weiss, Huang, Droid, Tseng, Korbak, Shen, Zhang, Zhou, Kim,
  Bowman, and Perez]{mckenzie2023inverse}
McKenzie, I.~R., Lyzhov, A., Pieler, M., Parrish, A., Mueller, A., Prabhu, A.,
  McLean, E., Kirtland, A., Ross, A., Liu, A., Gritsevskiy, A., Wurgaft, D.,
  Kauffman, D., Recchia, G., Liu, J., Cavanagh, J., Weiss, M., Huang, S.,
  Droid, T.~F., Tseng, T., Korbak, T., Shen, X., Zhang, Y., Zhou, Z., Kim, N.,
  Bowman, S.~R., and Perez, E.
\newblock Inverse scaling: When bigger isn't better, 2023.

\bibitem[Mihaylov et~al.(2018)Mihaylov, Clark, Khot, and
  Sabharwal]{mihaylov-etal-2018-suit}
Mihaylov, T., Clark, P., Khot, T., and Sabharwal, A.
\newblock Can a suit of armor conduct electricity? a new dataset for open book
  question answering.
\newblock In \emph{Proceedings of the 2018 Conference on Empirical Methods in
  Natural Language Processing}, pp.\  2381--2391, Brussels, Belgium,
  October-November 2018. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/D18-1260}.
\newblock URL \url{https://aclanthology.org/D18-1260}.

\bibitem[Radford et~al.(2018)Radford, Narasimhan, Salimans, and
  Sutskever]{radford2018improving}
Radford, A., Narasimhan, K., Salimans, T., and Sutskever, I.
\newblock Improving language understanding by generative pre-training, 2018.
\newblock URL
  \url{https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf}.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and
  Sutskever]{radford2019language}
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I.
\newblock Language models are unsupervised multitask learners, 2019.

\bibitem[Radhakrishnan et~al.(2023)Radhakrishnan, Nguyen, Kaplan, Brauner,
  Bowman, and Perez]{radhakrishnan2023transparency}
Radhakrishnan, A., Nguyen, K., Kaplan, J., Brauner, J., Bowman, S.~R., and
  Perez, E.
\newblock Question decomposition improves the faithfulness of model-generated
  reasoning.
\newblock arXiv preprint (released concurrently), 2023.

\bibitem[Rudin(2019)]{rudin2019stop}
Rudin, C.
\newblock Stop explaining black box machine learning models for high stakes
  decisions and use interpretable models instead.
\newblock \emph{Nature Machine Intelligence}, 1:\penalty0 206--215, 05 2019.
\newblock \doi{10.1038/s42256-019-0048-x}.

\bibitem[Stiennon et~al.(2020)Stiennon, Ouyang, Wu, Ziegler, Lowe, Voss,
  Radford, Amodei, and Christiano]{stiennon2020learning}
Stiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R., Voss, C., Radford, A.,
  Amodei, D., and Christiano, P.~F.
\newblock Learning to summarize with human feedback.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H.
  (eds.), \emph{Advances in Neural Information Processing Systems}, volume~33,
  pp.\  3008--3021. Curran Associates, Inc., 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2020/file/1f89885d556929e98d3ef9b86448f951-Paper.pdf}.

\bibitem[Turpin et~al.(2023)Turpin, Michael, Perez, and
  Bowman]{turpin2023language}
Turpin, M., Michael, J., Perez, E., and Bowman, S.~R.
\newblock Language models don't always say what they think: Unfaithful
  explanations in chain-of-thought prompting.
\newblock arXiv preprint 2305.04388, 2023.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, L.~u., and Polosukhin, I.
\newblock Attention is all you need.
\newblock In Guyon, I., Luxburg, U.~V., Bengio, S., Wallach, H., Fergus, R.,
  Vishwanathan, S., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems}, volume~30. Curran Associates, Inc., 2017.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf}.

\bibitem[Wang et~al.(2023)Wang, Xu, Lan, Hu, Lan, Lee, and
  Lim]{wang2023planandsolve}
Wang, L., Xu, W., Lan, Y., Hu, Z., Lan, Y., Lee, R. K.-W., and Lim, E.-P.
\newblock Plan-and-solve prompting: Improving zero-shot chain-of-thought
  reasoning by large language models.
\newblock arXiv preprint 2305.04091, 2023.

\bibitem[Wang et~al.(2022)Wang, Wei, Schuurmans, Le, Chi, and
  Zhou]{wang2022rationaleaugmented}
Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., and Zhou, D.
\newblock Rationale-augmented ensembles in language models.
\newblock arXiv preprint 2207.00747, 2022.

\bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, ichter, Xia, Chi, Le,
  and Zhou]{wei2022cot}
Wei, J., Wang, X., Schuurmans, D., Bosma, M., ichter, b., Xia, F., Chi, E., Le,
  Q.~V., and Zhou, D.
\newblock Chain-of-thought prompting elicits reasoning in large language
  models.
\newblock In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and
  Oh, A. (eds.), \emph{Advances in Neural Information Processing Systems},
  volume~35, pp.\  24824--24837. Curran Associates, Inc., 2022.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf}.

\bibitem[Yao et~al.(2023{\natexlab{a}})Yao, Yu, Zhao, Shafran, Griffiths, Cao,
  and Narasimhan]{yao2023tree}
Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T.~L., Cao, Y., and
  Narasimhan, K.
\newblock Tree of thoughts: Deliberate problem solving with large language
  models.
\newblock arXiv preprint 2305.10601, 2023{\natexlab{a}}.

\bibitem[Yao et~al.(2023{\natexlab{b}})Yao, Zhao, Yu, Du, Shafran, Narasimhan,
  and Cao]{yao2023react}
Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K.~R., and Cao, Y.
\newblock React: Synergizing reasoning and acting in language models.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=WE_vluYUL-X}.

\bibitem[Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and
  Choi]{zellers2019hellaswag}
Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y.
\newblock {H}ella{S}wag: Can a machine really finish your sentence?
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association
  for Computational Linguistics}, pp.\  4791--4800, Florence, Italy, July 2019.
  Association for Computational Linguistics.
\newblock \doi{10.18653/v1/P19-1472}.
\newblock URL \url{https://aclanthology.org/P19-1472}.

\bibitem[Zhou et~al.(2023)Zhou, Sch{\"a}rli, Hou, Wei, Scales, Wang,
  Schuurmans, Cui, Bousquet, Le, and Chi]{zhou2023leasttomost}
Zhou, D., Sch{\"a}rli, N., Hou, L., Wei, J., Scales, N., Wang, X., Schuurmans,
  D., Cui, C., Bousquet, O., Le, Q.~V., and Chi, E.~H.
\newblock Least-to-most prompting enables complex reasoning in large language
  models.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=WZH7099tgfM}.

\bibitem[Ziegler et~al.(2019)Ziegler, Stiennon, Wu, Brown, Radford, Amodei,
  Christiano, and Irving]{ziegler2019finetuning}
Ziegler, D.~M., Stiennon, N., Wu, J., Brown, T.~B., Radford, A., Amodei, D.,
  Christiano, P.~F., and Irving, G.
\newblock Fine-tuning language models from human preferences.
\newblock \emph{CoRR}, abs/1909.08593, 2019.
\newblock URL \url{http://arxiv.org/abs/1909.08593}.

\end{thebibliography}
