
% Figure environment removed


\section{Limitation Analysis of DDNNs' Calibration}

We focus on the difficulties of calibrating over-confident DDNNs. As shown in the first column of Figure~\ref{fig:performance_comparison} and the raw ECE reported in Table~\ref{tab:mts_ece_table}, DDNNs show the common over-confidence problem of neural networks, giving higher probabilities than actual accuracy; however, when applied with existing calibration methods, DDNNs are often over-calibrated and become under-confident. In this section, we analyze the reasons that may account for the DDNNs that are not calibratable from 2 aspects: (i) the after-training prediction behaviors and (ii) the during-training network capacity in terms of feature encoding ability. We also discuss the decomposed significance of full data and distilled data on the training accuracy of the network.

\subsection{DDNNs are Less Calibratable}
\label{sec:analysis_network}

We find that the logit distribution of the DDNNs' output is more concentrated, making it difficult to calibrate. In general, a neural network can be considered as a mapping function from the source data domain to the target label distribution, and in the classification task, we use the softmax function to convert logits into label probabilities. The higher the maximum logit value compared to other values, the higher the argmax probability will be and thus the more likely such prediction is over-confident. Therefore we study the distribution of maximum logit values for fully trained DDNNs and FDNNs. As shown in Figure~\ref{fig:logit_max}, the more calibratable FDNNs (blue) output a more dispersed logit distribution, while the less calibratable DDNNs (red) output a concentrated logit distribution with a larger mean.

This mismatched behavior causes problems for after-training calibration methods such as TS and Mixup that operate on scaling output logits, because DDNNs with tight distributions of max logits struggle to distinguish between hard (e.g., out-of-distribution, OOD) and easy (e.g., in-distribution, ID) samples (top of Figure~\ref{fig:rebuttal_ipc_ood}) using the corresponding max logits ~\cite{wei2022mitigating}. Similar theoretical assumptions appear in recent work~\cite{wang2021rethinking}, where they show that the small range of logits due to regularization during training, and a large mean of logits due to the network trying to fit on hard examples may lose information about the different hardness of data points, causing the networks of after-training calibration methods to fail to calibrate.

Therefore, we infer that DDNNs are less calibratable using distilled data due to their more concentrated output distribution and larger mean values. Thus, in order to make DDNNs more calibratable in after-training calibration without modifying network weights, we aim to utilize data that force DDNNs to produce more diverse and smaller outputs.

\subsection{DD Contains Limited Semantic Information}
\label{sec:analysis_svd}

By reconstructing distilled and full data with SVD, we find that distilled data contains only condensed information about the classification task, resulting in the limited ability of DDNNs in latent representation learning.
Intuitively, distilled images should be more informative, or more representative than source full images, in order to keep the number of images small. But do distilled images discard too much source information that is not so much useful for the classification tasks they are optimized for? We hypothesize that distilled data is "simpler" than source full data, such that dropping the same amount of information from distilled datasets should hurt the training performance worse than it does on full data. We start by breaking down full datasets into smaller components of different significance. Singular value decomposition (SVD)~\cite{klema1980singular} is a powerful algorithm in linear algebra for matrix approximation:
\begin{equation}
    U, \Sigma, V = \operatorname{SVD}(X),
    \label{eq:svd_forward}
\end{equation}
where higher singular values in $\Sigma$ correspond to more significant components of $X$. Source data can then be approximately reconstructed by
\begin{equation}
    X^{\prime} \approx U \cdot \Sigma^{\prime} \cdot V^{T}
    \label{eq:svd_backward}
\end{equation}
SVD has been widely used in DNN research for model reconstruction~\cite{xue2013restructuring, xue2014singular}, knowledge distillation~\cite{lee2018self}, and analyzing data~\cite{henry19928}. For our purposes of analyzing data information diversity and significance of data components, we gradually throw away the highest singular values during SVD reconstruction and check for accuracy drop when trained on the approximately reconstructed data. 

Our assumption is that distilled data contains dense information that can be easily grouped, such that SVD decomposes distilled data into several important components and other very small components, compared to full data components whose importance can be more evenly distributed, such that dropping the same number of important components from distilled data would lose more accuracy than in full data.
We drop from 0\% to 20\% of the highest singular values from full CIFAR10 and CIFAR10 distilled by~\cite{cazenavette2022dataset} with IPC = 10, 30 and 50, then train a ConvNet for 300 epochs on the resulting data. As shown in Figure~\ref{fig:analysis_svd}, DDNNs suffer much more severely from the loss of principle components than FDNNs. 

We thus conclude that the distilled data discards too much other semantically meaningful information from the original full data due to over-optimization of the classification task, resulting in condensed information that can be easily decomposed by SVD.

% Figure environment removed


\begin{table*}[!t]
    \vspace{-0.2cm}
    \addtolength{\tabcolsep}{0pt}
    \renewcommand{\arraystretch}{0.95}
    \centering
    \caption{ECE (\%) of different calibration methods on DDNNs with different DD backbones. Our proposed method yields the best or comparable ECE results in all distillation settings, reducing ECE of DSA by 91.05\%. More importantly, our method does not over-calibrate DDNNs as other calibration methods do (italic), as counted in the last row. Although MX and LS outperform our method in distillation backbones of inferior accuracy (RTP, DC), we surpass them by fine-tuning a more aggressive $r$, as described in Section~\ref{sec:ablations}.}
    \label{tab:mts_ece_table}
    
    \setlength{\aboverulesep}{0pt}
    \setlength{\belowrulesep}{0pt}
    \setlength{\extrarowheight}{.75ex}
    
    \begin{tabular}{cl|ccccc>{\columncolor[gray]{0.9}}cc}
    \toprule
    \multicolumn{2}{c|}{DD Backbone}                            & Raw                  & TS                            & MX                      & LS                        & FL                         & Ours                          \\ \hline 
    \multicolumn{1}{c}{\multirow{4}{*}{MTT}} & CIFAR10         & 4.93 $\pm$ 0.2      & \textit{7.45 $\pm$ 2.1}      & \hspace{-0.08in}\textit{21.06 $\pm$ 0.8}  & \hspace{-0.08in}\textit{25.01 $\pm$ 0.2} & \textit{6.62 $\pm$ 0.2}   & \textbf{1.20 $\pm$ 0.3}      \\  
    \multicolumn{1}{c}{}                     & CIFAR100        & 5.95 $\pm$ 0.4      & \textit{7.76 $\pm$ 0.4}      & \hspace{-0.08in}\textit{14.19 $\pm$ 0.4}  & \hspace{-0.08in}\textit{26.36 $\pm$ 0.4} & \textit{8.30 $\pm$ 0.5}   & \textbf{2.18 $\pm$ 0.2}      \\
    \multicolumn{1}{c}{}                     & Tiny ImageNet           & \hspace{-0.08in}15.78 $\pm$ 0.3     & 2.44 $\pm$ 0.3              & 2.42 $\pm$ 0.3            & \hspace{-0.08in}12.14 $\pm$ 0.3          & 3.61 $\pm$ 0.3            & \textbf{2.26 $\pm$ 0.3}      \\
    \multicolumn{1}{c}{}                     & ImageNette           & 8.68 $\pm$ 1.9      & 4.85 $\pm$ 0.6               & 5.19 $\pm$ 0.6            & \hspace{-0.08in}\textit{23.45 $\pm$ 1.4} & 6.87 $\pm$ 1.3            & \textbf{4.78 $\pm$ 0.5}      \\ \hline 
    \multicolumn{1}{c}{\multirow{2}{*}{RTP}} & CIFAR10         & 2.96 $\pm$ 0.5     & \textit{3.28 $\pm$ 0.7}     & \hspace{-0.08in}\textit{13.35 $\pm$ 1.5}  & \textit{9.58 $\pm$ 0.5} & \textit{8.35 $\pm$ 1.4}   & \textbf{2.22 $\pm$ 0.5}               \\ 
    \multicolumn{1}{c}{}                     & CIFAR100        & \hspace{-0.08in}29.71 $\pm$ 0.6     & \hspace{-0.08in}23.72 $\pm$ 0.6              & \textbf{3.55 $\pm$ 0.6}   & 7.94 $\pm$ 0.2           & \hspace{-0.08in}18.51 $\pm$ 0.5           & \hspace{-0.08in}10.14 $\pm$ 0.4              \\ \hline 
    \multicolumn{1}{c}{DC}                   & CIFAR10         & \hspace{-0.08in}23.60 $\pm$ 0.7     & 5.00 $\pm$ 0.7              & 1.83 $\pm$ 0.3            & \textbf{1.28 $\pm$ 0.1}  & \hspace{-0.08in}13.31 $\pm$ 0.9           & \hspace{-0.08in}10.39 $\pm$ 0.8              \\ \hline 
    \multicolumn{1}{c}{DSA}                  & CIFAR10         & \hspace{-0.08in}19.91 $\pm$ 0.3     & 1.95 $\pm$ 0.4              & 6.44 $\pm$ 0.8            & 2.32 $\pm$ 0.5  & 7.95 $\pm$ 0.7            & \textbf{1.70 $\pm$ 0.4}               \\ \hline   
    \rowcolor[gray]{0.9}\multicolumn{2}{c|}{\textbf{\textit{\# over-calibration}}}          & -                    & 3                            & 3                          & 4                         & 3                          & \textbf{0}                    \\                        
    \bottomrule
    \end{tabular}
    \vspace{-0.2cm}
\end{table*}


\subsection{Limited Semantic Information Weakens Encoding Capacity}
\label{sec:analysis_feature}

We further infer that DDNNs may be less capable of tasks other than classification due to the likely loss of non-classification information. Usually, outputs of intermediate layers of DNNs could be used as feature vectors for other interesting non-classification tasks such as style transfer~\cite{gatys2016image,johnson2016perceptual} due to their unique encodings of source information. To see this, we visualize outputs of layers at different depths in the ConvNet using t-SNE that projects feature vectors down to 2 dimensions. We can see that in Figure~\ref{fig:analysis_proj}, features from FDNNs cluster slowly, and become visually separable only in the last layer, thus retaining most of the original information in its latent vectors; features from DDNNS, however, form visible cluster already in layer conv2, making more compact final clusters that are more valuable for classifications than other tasks such as feature extraction. Moreover, clusters of DDNNs from each class are closer to each other than those from FDNNs. Clearly, outputs of middle layers from DDNNs are already alike label distributions, discarding too much non-classification information. Similar observations on other distillation backbones are reported in~\cite{Wang_2022_CVPR}, in which they also account for long-tailed gradients as possible reasons. 

Therefore, DDNNs do not exhibit good encoding capability due to being trained on distilled data optimized specifically for the classification task and may be susceptible to being over-calibrated by calibration methods. We provide more details in the supplementary material.

