
\section{Introduction}

Dataset distillation (DD) has recently gained growing attention because of its ability to reduce the need for large amounts of data during deep neural network (DNN) training, thereby reducing training time and storage burden~\cite{wang2018dataset}. Despite the efficiency of training, studies have pointed out that DD still has multiple limitations. On the one hand, the distillation process is found to be time-consuming, computationally expensive, and storage intensive~\cite{wang2018dataset,zhao2020dataset,zhao2021dataset,deng2022remember,nguyen2020dataset,nguyen2021dataset,kim2022dataset,zhang2023addressing}. On the other hand, DNNs trained on DD data are said to be poorly generalizable to different models or downstream tasks~\cite{wang2018dataset, zhao2020dataset, zhao2021dataset}. Efforts have been conducted to address these issues~\cite{cazenavette2022dataset,zhang2022accelerating,loo2022efficient}.
However, the calibration of DD has been overlooked, which is important for deploying DD safely in real-world applications.

An increasing number of studies are investigating calibration as an important property of DNNs, which means that a DNN should know when it is likely to be wrong~\cite{guo2017calibration, muller2019does, abdar2021review}. In other words, the confidence~(probability related to the predicted category label) of a model should reflect its ground truth correctness likelihood~(accuracy). Previous work has found that DNNs are often too confident to realize when they are making mistakes~\cite{guo2017calibration, ovadia2019can}, which leads to safety issues, especially in safety-critical tasks, e.g., automated healthcare and self-driving cars~\cite{de2022toward, rasheed2022explainable}.

We for the \emph{first} time identify and study the calibration problem of DNNs trained on distilled data (DDNNs). 


% Figure environment removed


\textbf{\textit{Problem 1.}} \textit{We find that DDNNs still suffer from over-confidence problem.} 

We evaluate the calibration quality of DDNNs by Expected Calibration Error (ECE)~\cite{guo2017calibration}, which is a common metric to quantitatively measure the difference between confidence and accuracy. Specifically, to calculate the ECE, we categorize the output probability and accuracy into different levels and calculate the average absolute difference. The lower the ECE, the better the calibration.
As shown in Figure~\ref{fig:performance_comparison}, the ECE (red area) of DDNNs is quite visible in the figures of the first column, which means that the probability of DDNNs' output is usually higher than the actual accuracy of its prediction.
Thus, it is desirable to calibrate DDNNs for reliable prediction and decision-making.

\textbf{\textit{Problem 2.}} \textit{We find that DDNNs are not calibratable when using existing calibration methods.} 

There are calibration methods designed to align the confidence and accuracy of DNNs trained on full datasets (FDNNs). They either modify loss term during network training~\cite{lin2017focal}, use soft labels~\cite{zhang2017mixup, thulasidasan2019mixup}, or scale down the logits after training~\cite{guo2017calibration}.
However, when training on distilled data, we find that most of the existing methods tend to over-calibrate DDNNs.
As shown in Figure~\ref{fig:performance_comparison}, a DDNN trained on distilled CIFAR10 (the first column) has an initial ECE of 6.17\% (red area). 
After calibrating with focal loss (the second column), mixup (the third column), or label smoothing (the fourth column), the DDNN becomes under-confident with increased ECE of 7.79\%, 14.09\%, and 26.18\% respectively, as shown by the inverted and enlarged red bars. This over-calibration problem also occurs for various distillation methods on common datasets (Table~\ref{tab:mts_ece_table}).

In order to address the issues mentioned above, we raise the following questions:

\textbf{\textit{Question 1.}} \textit{Why are DDNNs not calibratable when using existing calibration methods?}

We first dive deep into the differences between the source full data and the distilled data. We find that the distilled data tend to retain information relevant to the classification task while discarding other distributional information in the full data, which may result in limiting DDNNs to pursuing higher accuracy in the classification task while losing more abilities in latent representation learning of FDNNs~\cite{van2017neural,oord2018representation}. By decomposing distilled and full data into smaller components and studying their corresponding significance to model training accuracy, we show that distilled data contains very condensed information, implying a loss of information and leading to harder during-training calibration.
Then, we also investigate the differences between DDNNs and FDNNs. We observe that DDNNs have a more concentrated distribution of logit values, leading to less room for after-training calibration methods such as temperature scaling.

\textbf{\textit{Question 2.}} \textit{How to calibrate DDNNs efficiently?}

To enable DDNNs to be calibratable, we propose (i) Mask Temperature Scaling and (ii) Masked Distillation Training that can be applied both during and after the training of DDNNs. We design a binary masking method for synthetic input when training for distillation objection, which effectively forces the distillation model to extract richer information from the source dataset into distilled datasets, leading to better encoding abilities and thus better calibration of DDNNs. We also show that our proposed masked temperature scaling better improves after-training calibration results on DDNNs by introducing more dynamics to network outputs. Our proposed techniques thus allow for more powerful and more calibratable DDNNs. We summarize contributions as follows:

\begin{itemize}
    \item We for the \emph{first} time study the calibration of DDNNs and find that DDNNs are not calibratable.
    \item We find that DD discards semantically meaningful information and that DDNNs produce a concentrated logit distribution, which explains the difficulty of calibrating DDNNs.
    \item We propose two masking techniques that can improve the calibration of DDNNs better than existing calibration methods, i.e., masked distillation training and masked temperature scaling. In addition, our proposed techniques can be readily deployed in existing dataset distillation methods with minimal extra cost.
    \item We perform extensive experiments on multiple benchmark datasets, model architectures, and data distillation methods. Our techniques reduce ECE values by up to \textbf{91.05\%} with comparable accuracy.
\end{itemize}
