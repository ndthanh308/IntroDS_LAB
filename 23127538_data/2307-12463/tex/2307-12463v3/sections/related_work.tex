\section{Related Work}

\noindent \textbf{Dataset Distillation.} \label{sec: DD}
First introduced by~\cite{wang2018dataset}, dataset distillation is the task of synthesizing a smaller dataset from a large-scale dataset such as CIFAR100~\cite{krizhevsky2009learning}, so that the network trained on the distilled data has a performance comparable to that of the network trained on the source large-scale data. Recent work has significantly improved the performance of networks trained on distilled data and reduced the computational and time overhead of the distillation process while compressing the dataset size to one image per class~\cite{cazenavette2022dataset,deng2022remember,loo2022efficient,nguyen2020dataset,nguyen2021dataset,zhao2020dataset,zhao2021dataset,Wang_2022_CVPR,zhang2022ideal}.
Dataset distillation problem is treated as a gradient-based hyperparameter optimization~\cite{wang2018dataset}. DC performs distillation by matching the gradients generated from distilled data and full data~\cite{zhao2020dataset}. DSA further improves the results by differentiable Siamese augmentations~\cite{zhao2021dataset}. Other SOTA methods include matching trajectories of each parameter between the training on distilled data and full data~\cite{cazenavette2022dataset}, optimizing soft labels~\cite{sucholutsky2021soft}, minimizing reconstruction errors~\cite{yu2023dataset}, and using neural networks to regress features from synthetic samples to real ones~\cite{zhou2022dataset}. The current focus of DD is on computational expense and training performance, and to the best of our knowledge, the difficulties in calibrating over-confident DDNNs remain untouched.
\\


% Figure environment removed

\noindent \textbf{Neural Network Calibration.}
\label{sec:calibration}
 The importance of neural network calibration has been emphasized and received increasing attention~\cite{guo2017calibration}, with the aim of matching the output probability of a neural network (also known as the network output confidence) with the actual accuracy. \cite{guo2017calibration} also introduces the concept of Expected Calibration Error (ECE), which has now become a standard metric for quantitatively measuring calibration quality. A higher ECE implies a poorer calibration of the neural network, while a 0 implies a perfect calibration. Recent calibration methods that have been proposed for networks trained on large-scale datasets include Label Smoothing (LS)~\cite{yuan2020revisiting}, which smooths a one-hot class label with uniform noise during training, forcing the model to learn loose predictions.
Mixup is similar to label smoothing, where different data-label pairs are mixed to form new data points~\cite{thulasidasan2019mixup, zhang2017mixup}. Focal loss (FL), originally designed to address the class imbalance, modifies the traditional cross-entropy loss in classification problems by adding a moderation term, thus allowing the model to focus more on difficult examples that are easily misclassified but difficult to learn~\cite{lin2017focal, mukhoti2020calibrating}. Temperature scaling (TS) is an after-training calibration method applied to fully trained and fixed-weight networks~\cite{guo2017calibration}.
As an extension of Platt scaling~\cite{platt1999probabilistic}, the temperature scaling method scales the output, denoted by $z$, of the last layer of the network with a scaler T before converting it into a probability:
\begin{equation}
    \hat{q_{i}} = \max_{k} \sigma_{softmax}\left (z_{i} / T \right ) ^{(k)}
    z_{i} \in \mathbb{R}^{D}.
    \label{eq:temp_scaling}
\end{equation}

Other work has discussed the necessity~\cite{wu2023towards} and hardness of network calibration~\cite{cheng2022calibrating, ghoshal2022calibrated, zhang2023accelerating}, as well as the degradation of calibration with distribution shift or model size ~\cite{krishnan2020improving, lei2023calibrating}.
