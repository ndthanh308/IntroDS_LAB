
% Figure environment removed

\section{Experiments}

\subsection{Experiment Setup}

We thoroughly evaluate our proposed MTS and MDT on different dataset distillation setups and compare them with existing calibration methods.
\noindent \textbf{Dataset Distillation Backbones:} We follow the exact settings in MTT~\cite{cazenavette2022dataset}, RTP~\cite{deng2022remember}, DC~\cite{zhao2020dataset} and DSA~\cite{zhao2021dataset}. Our experiments are based on 4 benchmark datasets: CIFAR10 \& CIFAR100~\cite{krizhevsky2009learning}, Tiny ImageNet~\cite{le2015tiny}, and ImageNette (a subset of ImageNet)~\cite{Howard_Imagenette_2019}. We mainly set image-per-class to larger values, e.g. 50 in MTT, and results on different IPCs are provided in supplement materials.
\noindent \textbf{Calibration Methods:} We compare our method with existing calibration methods including Temperature Scaling~(TS)~\cite{guo2017calibration}, mixup~(MX)~\cite{zhang2017mixup}, Label Smoothing~(LS)~\cite{yuan2020revisiting}, and Focal Loss~(FL)~\cite{lin2017focal}. 
\noindent \textbf{Implementations:} For TS, we use an initial temperature of 1.5 and LBFGS \cite{liu1989limited} optimizer with a learning rate of 0.02. For MX, we use a $\beta$ distribution with $\alpha = 1.0$ for the mixup ratio. For LS, we set $\epsilon = 0.1$. For FL, we set $\gamma = 1.$, which calibrates better on DDNNs than the best value 2 reported in the paper. For our proposed MTS, on distillation backbone MTT, we use a fixed masking ratio of 0.3, 0.3, 0.5, and 0.1 for each of the 4 datasets respectively. On backbones RTP, DSA, and DC, due to their inferior performance in accuracy, we use a more aggressive masking ratio of 0.8. 

Since the number of examples of distillation data is usually limited, we draw 10\% of all distillation data as the validation set for the after-training method, as in other existing work. The experiments are repeated five times, and the mean and standard deviation are reported. More experimental setups are available in supplementary materials.

\subsection{Empirical Analysis of MTS}

We show in Figure~\ref{fig:performance_comparison} that our proposed method is able to reduce the ECE (red bars) to almost zero for each confidence bin when using MTT as the distillation backbone on CIFAR10 and CIFAR100.
Although traditional calibration methods such as mixup~\textit{can} perform well, they could also over-calibrate and result in under-confident networks. We visualize the under-confidence in Figure~\ref{fig:performance_comparison}, in which the red bars are enlarged and switched from left to right in each bin. Additional calibration results are reported in Table~\ref{tab:mts_ece_table}, and our proposed MTS gives the best numerical ECE results in almost all settings. 
In real-world settings where no mistakes are allowed, traditional methods are regarded as unsafe due to their potential over-calibration. 

As a contrast, we propose masked temperature scaling, which not only has better performance but does not show any lack of confidence in the results at all and is therefore considered a safer choice.

\begin{table}[t]
    \addtolength{\tabcolsep}{-2pt}
    \renewcommand{\arraystretch}{1}
    \centering
    \caption{ECE (\%) of different calibration methods on distilled datasets trained with our methods. Tiny: Tiny ImageNet. Nette: Nette subset of ImageNet. Our results are in \colorbox[gray]{0.9}{shadow}.}
    \label{tab:mdt_ece_table}
    \scalebox{0.8}{
    \setlength{\aboverulesep}{0pt}
    \setlength{\belowrulesep}{0pt}
    \setlength{\extrarowheight}{.75ex}
    
    \begin{tabular}{l|c>{\columncolor[gray]{0.9}}c>{\columncolor[gray]{0.9}}c>{\columncolor[gray]{0.9}}cc}
    \toprule
    \multicolumn{1}{c|}{Dataset}         & Best of Others               & MDT              & MTS                       & MDT + MTS                \\ \hline 
    \multicolumn{1}{l|}{CIFAR10}         & 3.64 $\pm$ 0.2 (TS)         & 3.66 $\pm$ 0.3  & \textbf{1.20 $\pm$ 0.3}  & 2.50 $\pm$ 0.5          \\      
    \multicolumn{1}{l|}{CIFAR100}        & 5.95 $\pm$ 0.4 (Raw)        & 4.65 $\pm$ 0.3  & 2.18 $\pm$ 0.2           & \textbf{2.00 $\pm$ 0.5} \\   
    \multicolumn{1}{l|}{Tiny}            & 2.42 $\pm$ 0.3 (MX)      & 7.44 $\pm$ 1.4  & \textbf{2.26 $\pm$ 0.3}  & 5.91 $\pm$ 1.4          \\  
    \multicolumn{1}{l|}{Nette}           & 4.85 $\pm$ 0.6 (TS)         & 7.32 $\pm$ 1.7  & \textbf{4.78 $\pm$ 0.5}  & 5.14 $\pm$ 1.2          \\ \bottomrule
    \end{tabular}}
    \vspace{-0.2cm}
\end{table}

\begin{table}
\setlength{\abovecaptionskip}{-0.0cm}
\setlength\tabcolsep{1pt} 
\begin{small}
\centering

    \setlength{\tabcolsep}{3pt}
    \caption{ECE (\%) of MDT with dynamically sampled $r$ and MTS on CIFAR10, MTT with different IPCs.}
    \vspace{0.1cm}
    \centering
    \begin{tabular}{l|cc>{\columncolor[gray]{0.9}}c}
    \toprule
    \multicolumn{1}{c|}{IPC} & MDT\textsuperscript{ds} & MTS & MDT\textsuperscript{ds} + MTS \\ \hline
    \multicolumn{1}{c|}{10} & 1.79 $\pm$ 0.9 & 1.36 $\pm$ 0.4 & \textbf{1.13 $\pm$ 0.2}    \\
    \multicolumn{1}{c|}{50} & 5.10 $\pm$ 0.4 & 1.20 $\pm$ 0.3 & \textbf{1.26 $\pm$ 0.2}    \\
    \bottomrule
    \end{tabular}

    \label{tab:rebuttal_mdt}
\end{small}
\vspace{-0.3cm}
\end{table}

\subsection{Empirical Analysis of MDT}
\label{sec:acc_mdt}

We show that MDT improves the calibration results. As reported in Tables~\ref{tab:mdt_ece_table}-\ref{tab:rebuttal_mdt}, applying MDT alone or combining it with MTS yields comparable or better calibration performance. WE note that naively combining MDT + MTS may increase ECE due to DDNNs
overfitting to the fixed masking ratio in MDT, then being
over-calibrated by MTS. Thus we further improve (Table \ref{tab:rebuttal_mdt}) MDT + MTS by dynamically sampling the $r$ in MDT from 0 to 0.1 (denoted MDT\textsuperscript{ds}) so the resulting DDNNs are more calibratable. We fix $r$ in MTS due to the limited amount of validation data in DD. This indicates that our proposed MDT produces more robust and calibratable DDNNs than the original backbone when sufficient computational resources are available to train the distillation process from the beginning. We use MTT as the distillation backbone.

We find that MDT gives comparable model accuracy albeit altering the distillation process. With a 10\% zero masking during the distillation process, MDT only leads to a loss of as large as 1.26\% in DDNNs' accuracy on CIFAR100 and as low as 0.14\% on Tiny ImageNet. As reported in Table~\ref{tab:mdt_acc_table}, this is even better than traditional during-training calibration methods such as mixup, label smoothing, and focal loss that lead to different model results. 

This suggests that MDT yields better calibration potential at a negligible performance cost, which is desirable in an environment where security is a major concern~\cite{de2022toward,rasheed2022explainable}.

\begin{table*}[!t]
\vspace{-0.1cm}
    \addtolength{\tabcolsep}{0pt}
    \renewcommand{\arraystretch}{0.9}
    \centering
    \caption{Accuracy (\%) of different during-training calibration methods on MTT distilled datasets. While all during-training calibration methods lead to a loss in accuracy, ours loses only as small as 0.14\% at a masking ratio of 10\%. Our results are in \colorbox[gray]{0.9}{shadow}.}
    \label{tab:mdt_acc_table}

    \setlength{\aboverulesep}{0pt}
    \setlength{\belowrulesep}{0pt}
    \setlength{\extrarowheight}{.75ex}
    
    \begin{tabular}{l|c|ccc>{\columncolor[gray]{0.9}}c}
    \toprule
    \multicolumn{1}{c|}{Dataset}         & Raw                 & MX              & LS                       & FL                       & Ours                 \\ \hline
    \multicolumn{1}{l|}{CIFAR10}         & 70.48 $\pm$ 0.2    & 65.50 $\pm$ 0.5   & 67.42 $\pm$ 0.5         & 68.79 $\pm$ 0.5         & \textbf{69.98} $\pm$ 0.4    \\
    \multicolumn{1}{l|}{CIFAR100}        & 47.47 $\pm$ 0.2    & 39.65 $\pm$ 0.3   & 47.02 $\pm$ 0.2         & 46.79 $\pm$ 0.4         & 46.21 $\pm$ 0.4    \\
    \multicolumn{1}{l|}{Tiny ImageNet}            & 27.76 $\pm$ 0.2    & 21.48 $\pm$ 0.4   & 25.76 $\pm$ 0.3        & 27.42 $\pm$ 0.3          & \textbf{27.62} $\pm$ 0.4  \\
    \multicolumn{1}{l|}{ImageNette}           & 63.04 $\pm$ 1.3    & 55.60 $\pm$ 1.0   & 63.40 $\pm$ 0.9        & 61.32 $\pm$ 0.9          & 62.80 $\pm$ 1.2    \\ \bottomrule
    \end{tabular}
    \vspace{-0.1cm}
\end{table*}

% Figure environment removed

\subsection{Enabling Calibratable DDNNs}
In response to the discussion of the after-training behavior of DDNNs in Section~\ref{sec:analysis_network}, we examined improvements in DDNN calibrability.
On the validation data for Temperature Scaling, we apply zero-masking with ratio $r$ = 10\%, 20\%, and 30\% to see its effects on resulting logit distributions of DDNNs. We show in the bottom-right of Figure \ref{fig:rebuttal_ipc_ood} that our MDT produces lower probabilities on OOD samples, leading to more distinguishable logits and more calibratable DDNNs than before. We also show in Figure \ref{fig:result_max} that DDNNs given these mask-perturbed data will produce similarly diverse logits as if they are processing normal full data, allowing masked temperature scaling to better calibrate DDNNs with similar good performance on FDNNs.

\subsection{Enhancing Semantic Information of DDNNs}
\label{sec:other_mdt}
We investigate whether the semantic information of DD is enhanced according to the discussion in Section~\ref{sec:analysis_svd}. 
As shown in Figure~\ref{fig:result_svd}, when trained with MDT, our DDNNs start with a little lower accuracy than the normal MTT model. However, as we gradually drop more singular values following Eqs~(\ref{eq:svd_forward})-(\ref{eq:svd_backward}), the accuracy of the MDT model drops slower and even stays higher than the accuracy of the MTT models. 
This indicates that MDT distillation effectively retains more semantically meaningful information than normal distillation does, making MDT distilled data more difficult to be decomposed by SVD.

\subsection{Improving Encoding Capacity of DDNNs}

We study the improvement in the feature encoding capability of the DDNNs, responding to the discussion of DDNN behavior during training in Section~\ref{sec:analysis_feature}.
We experiment on the distillation backbone MTT, in which they collect network parameter trajectories from training on synthetic data in each iteration. We apply masked distillation training with masking ratio = 10\% on the synthetic data before the forward pass in each iteration. 

We show in Figure~\ref{fig:result_proj} that the hidden layers in the MDT model form larger clusters than the original MTT model, and that the clusters in each category are more intertwined with each other, retaining more information from the complete dataset and forming better feature vectors as desired.
