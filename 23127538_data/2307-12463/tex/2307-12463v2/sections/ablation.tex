\section{Ablation Studies}
\label{sec:ablations}

\noindent \textbf{Analysis of Mask Ratio $r$ in MTS.}
We analyze the effects of mask ratio $r$ on the calibration results of MTS. We set $r$ from 0.1 to 0.9, increasing by 0.1. 
As shown in Figure \ref{fig:ablation_r}, on CIFAR10 and CIFAR100, MTS works well for most of the possible $r$ ranging from 0.1 to 0.5, indicating that MTS can be tuned with minimal effort. On variants of the ImageNet dataset, however, we find that 0.3 works best for Tiny ImageNet, and 0.5 for ImageNet Subset. 
This is probably due to the large number of classes in these more complex datasets, as well as the relatively low accuracy of their corresponding distilled datasets.

\noindent \textbf{Analysis of Validation Set Size for MTS.}
We study the impact of how much data is drawn from all the distilled data as validation data for MTS.
We denote $N$ as the proportion we sample, and we set $N$ from 10\% to 50\%, increased by 10\%. We can see in Figure \ref{fig:ablation_len} that the number of samples has little effect on calibration results. We hypothesize that this is due to we only update the temperature parameter $T$ for only one step, thus not being affected by the number of examples in this step. 
This also indicates that MTS can be applied when we have only a small amount of data available, such as distillation with IPC=1 or medical image analysis scenarios~\cite{liu2022deep, chen2022recent, salahuddin2022transparency}.

% Figure environment removed

\noindent \textbf{Calibration in Lower Accuracy Settings.}
Our method outperforms other calibration methods at DD settings with extreme compression ratio, i.e. only 1 synthetic image for each label. This means traditional temperature scaling no longer applies because it requires additional validation data. As reported in Table~\ref{tab:ipc1_table}, while other calibration methods over-calibrate or don't work at all, ours still produces better results, indicating its generality to various DD settings.

\begin{table}[!t]
    \setlength{\tabcolsep}{2.7pt}
    \centering
    \caption{ECE (\%) of different calibration methods with IPC=1. Under this extreme compression rate, our method still outperforms other calibration methods. Our results are in \colorbox[gray]{0.9}{shadow}.}
    \label{tab:ipc1_table}

    \setlength{\aboverulesep}{0pt}
    \setlength{\belowrulesep}{0pt}
    \setlength{\extrarowheight}{.75ex}
    
    \scalebox{0.8}{
    \begin{tabular}{l|cccc>{\columncolor[gray]{0.9}}c}
    \toprule
    \multicolumn{1}{c|}{Dataset}         & Raw                 & MX              & LS                       & FL                       & Ours                 \\ \hline
    \multicolumn{1}{l|}{CIFAR10}     & \hspace{-0.04in}10.15 $\pm$ 1.2    & 8.40 $\pm$ 1.1   & \hspace{-0.04in}12.79 $\pm$ 0.6         & 2.05 $\pm$ 0.9         & \textbf{1.81 $\pm$ 0.7}    \\
    \multicolumn{1}{l|}{CIFAR100}     & 2.46 $\pm$ 0.6    & 4.45 $\pm$ 0.5   & 8.89 $\pm$ 0.6         & 3.24 $\pm$ 0.9         & \textbf{2.19 $\pm$ 0.5}    \\
    \bottomrule
    \end{tabular}
    }
    \vspace{-0.3cm}
\end{table}


\noindent \textbf{Comparison of Effects of Different During-Training Calibration Methods on DDNNs' Encoding Capacity.}
We provide more visualizations of projections of intermediate feature vectors obtained from DDNNs trained with different during-training calibration methods. The methods we use are mixup, focal loss, and label smoothing, in addition to the original training with cross-entropy loss. We can see in Figure~\ref{fig:supp_proj} that our proposed during-training calibration MDT alleviates the issue of concentrate features for all the traditional methods used, giving better encoding potentials of DDNNs for transfer learning tasks, which leads to more calibratable DDNNs.

\noindent \textbf{More Results on CIFAR100: ECE on different IPCs, max logits.}
We show in Figure~\ref{fig:supp_ipc_ece} that our MTS outperforms others in ECE on different IPCs. 
In the main paper, we mainly present IPC = 10 on Tiny-ImageNet \& Subsets with MTT, 10 on CIFAR100 with DC/DSA (released), and 50 on others. These DD settings have higher accuracy and would better represent real-world settings.
% Figure environment removed

We also provide visualization of maximum logits of DDNN on original MTT in Figure~\ref{fig:supp_max_logit_100}, in addition to the results on CIFAR10 in our main paper.
% Figure environment removed

% Figure environment removed
