\clearpage
\appendix

\section{Distillation Backbones}

\subsection{Datasets and Networks}
Following ~\cite{zhao2020dataset, zhao2021dataset, cazenavette2022dataset, deng2022remember}, we use a ConvNet with 3 blocks for CIFAR10 and CIFAR100~\cite{krizhevsky2009learning}, a ConvNet with 4 blocks for Tiny-ImageNet~\cite{le2015tiny}, and a ConvNet with 5 blocks for Nette (a subset of ImageNet)~\cite{Howard_Imagenette_2019}. Each block in the ConvNets contains a 3 $\times$ 3 convolutional layer with 128 channels, followed by instance normalization~\cite{ulyanov2016instance}, ReLU~\cite{nair2010rectified} and a 2 $\times$ 2 average pooling layer with stride 2. We apply Kornia ZCA~\cite{riba2020kornia} on CIFAR10 and CIFAR100 for distillation backbones~\cite{zhao2020dataset, zhao2021dataset, cazenavette2022dataset}. We pick the ConvNet in each distillation backbone because it gives the best distillation performance while keeping the distillation process under an acceptable time and computational budget.

\section{Additional Experiments}

\subsection{Details in Masked Temperature Scaling}
We sample from all the distilled data we have as the validation set to update the temperature parameter $T$ in our proposed Masked Temperature Scaling. Instead of sampling from all the shuffled data at once, we perform a per-class sampling such that there is no missing class or over-sampled class, which is especially important for distillation settings that aim for aggressive compression rates such as image-per-class $\leq$ 10. The traditional temperature scaling~\cite{guo2017calibration} separates all the data available into a training set and a validation set and uses the validation set only for updating $T$. This separated use of the distilled data is not applicable when image-per-class = 1. Moreover, a data split of 10\% can hurt training accuracy by as much as 1.68\% on the Nette subset of ImageNet, while our proposed during-training calibration method (MDT) only hurts accuracy by 0.24\%, as reported in Table~\ref{tab:ts_acc_table}. In addition, our proposed after-training method Masked Temperature Scaling keeps original training accuracy and achieves better calibration results than temperature scaling as reported in our main text.

\begin{table}[!t]
    \addtolength{\tabcolsep}{0pt}
    \renewcommand{\arraystretch}{0.9}
    \centering
    \caption{Accuracy (\%) drops by as much as 1.68\% when training with 90\% of distilled Nette (a subset of ImageNet). The rest 10\% is used in temperature scaling (TS). Our proposed after-training MTS (\colorbox[gray]{0.8}{shadow}) keeps the original accuracy. Our proposed during-training MDT (\colorbox[gray]{0.9}{shadow}) keeps a higher accuracy than that of dropping 10\% of training data for TS. We use MTT~\cite{cazenavette2022dataset} as the distillation backbone.}
    \label{tab:ts_acc_table}

    \scalebox{0.83}{

    \setlength{\aboverulesep}{0pt}
    \setlength{\belowrulesep}{0pt}
    \setlength{\extrarowheight}{0.8ex}

    \begin{tabular}{l|>{\columncolor[gray]{0.8}}c|c>{\columncolor[gray]{0.9}}c}
    \toprule
    \multicolumn{1}{c|}{Dataset}         & Full, MTS (Ours)        & TS (10\%)                 & MDT (Ours)                      \\ \hline
    \multicolumn{1}{l|}{CIFAR10}         & 70.48 $\pm$ 0.2          & 69.78 $\pm$ 0.5         & 69.98 $\pm$ 0.4        \\
    \multicolumn{1}{l|}{CIFAR100}        & 47.47 $\pm$ 0.2          & 47.10 $\pm$ 0.2      & 46.21 $\pm$ 0.4                 \\
    \multicolumn{1}{l|}{Tiny ImageNet}   & 27.76 $\pm$ 0.2          & 27.35 $\pm$ 0.2         & 27.62 $\pm$ 0.4        \\
    \multicolumn{1}{l|}{ImageNette}      & 63.04 $\pm$ 1.3          & 61.36 $\pm$ 1.6         & 62.80 $\pm$ 1.2                 \\ \bottomrule
    \end{tabular}
    }
    \vspace{-0.5cm}
\end{table}

% Figure environment removed

\subsection{More Results on SVD of Distilled Data and Full Data}

As we discussed in our main text, distilled data contain more concentrated information that easily gets grouped by algorithms such as SVD. We here illustrate the cumulative explained ratio of top singular values of data distilled by different backbones. We expect that concentrated information leads to a curve skewed to the top left and evenly distributed information leads to a smooth curve close to the diagonal. This will show how much each component corresponding to the singular values in $\Sigma$ contributes to the data reconstruction. As shown in Figure \ref{fig:supp_svd}, the cumulative explained ratio given by ours grows at the most steady rate, showing that our method produces more evenly distributed information in distilled data compared to the overly condensed information in other distillation backbones. As we concluded in our main text, this serves as a regularization to the distillation process such that it cannot discard too much information that is unrelated to the classification task but semantically meaningful for other tasks, leading to more calibratable networks trained on the resulting distilled data.

\subsection{More Results on DDNNs' Limited Encoding Capacity}

We provide more visualizations of projections of intermediate feature vectors obtained from DDNNs trained with different during-training calibration methods. The methods we use are mixup, focal loss, and label smoothing, in addition to the original training with cross-entropy loss. We can see in Figure~\ref{fig:supp_proj} that our proposed during-training calibration MDT alleviates the issue of concentrate features for all the traditional methods used, giving better encoding potentials of DDNNs for transfer learning tasks, which leads to more calibratable DDNNs.

\subsection{More Results on CIFAR100: ECE on different IPCs, max logits}
We show in Figure\ref{fig:supp_ipc_ece} that our MTS outperforms others in ECE on different IPCs. 
In the main paper, we mainly present IPC = 10 on Tiny-ImageNet \& Subsets with MTT, 10 on CIFAR100 with DC/DSA (released), and 50 on others. These DD settings have higher accuracy and would better represent real-world settings.
% Figure environment removed

We also provide visualization of maximum logits of DDNN on original MTT in Figure~\ref{fig:supp_max_logit_100}, in addition to the results on CIFAR10 in our main paper.
% Figure environment removed


\subsection{Performance Analysis of FDNNs}
\begin{table}[!t]
    \addtolength{\tabcolsep}{-2pt}
    \renewcommand{\arraystretch}{1}
    \centering
    \caption{ECE (\%) of different calibration methods on FDNNs. With a low masking ratio $r$, our results (\colorbox[gray]{0.9}{shadow}) are comparable to temperature scaling and most of the time beats other methods. As our method is specifically designed for DDNNs, in the case of FDNNs where traditional methods are suitable, we can simply convert our method to temperature scaling by setting $r$ to 0.}
    \label{tab:fd_ece_table}
    \scalebox{0.9}{

    \setlength{\aboverulesep}{0pt}
    \setlength{\belowrulesep}{0pt}
    \setlength{\extrarowheight}{.75ex}
    
    \begin{tabular}{l|c|cccc>{\columncolor[gray]{0.9}}cc}
    \toprule
    \multicolumn{1}{c|}{Dataset} & Raw & TS   & MX           & LS & FL      & MTS               \\ 
    \hline
    CIFAR10                           & 4.50       & 0.99 & 14.80 & 11.85 & 1.78 & 2.67                                \\
    CIFAR100                          & 13.05       & 1.41 & 10.69          &   7.17     &  3.49        & 1.84                                  \\
    Tiny ImageNet                      & 22.26       & 4.95          & 6.34          &   3.29      & 12.55         & 4.93                         \\
    ImageNette                    & 10.90       & 2.81 & 11.22          & 22.24 &   5.21       & 2.87                             \\ \bottomrule
    \end{tabular}}
    \vspace{-0.4cm}
\end{table}

We further test MTS on the more calibratable FDNNs. We calibrate networks trained on the full CIFAR10, CIFAR100, TinyImageNet, and Nette subset of ImageNet. We report the mean of 2 runs due to limited computational resources. As reported in Table~\ref{tab:fd_ece_table}, our method performs comparably with existing well-developed methods. In realistic settings with a large amount of training data, we can set the masking ratio $r$ to 0, which converts the MTS back to normal temperature scaling.

% Figure environment removed
