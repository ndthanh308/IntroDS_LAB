
\begin{table*}[!t]
    \vspace{-0.2cm}
    \addtolength{\tabcolsep}{0pt}
    \renewcommand{\arraystretch}{0.95}
    \centering
    \caption{ECE (\%) of different calibration methods on DDNNs with different DD backbones. Our proposed method yields the best or comparable ECE results in all distillation settings, reducing ECE of DSA by 91.05\%. More importantly, our method does not over-calibrate DDNNs as other calibration methods do, as shown in the last row. Although MX and LS outperform our method in distillation backbones of inferior accuracy (RTP, DC), we surpass them by fine-tuning a more aggressive masking ratio, as described in Section~\ref{sec:ablations}.}
    \label{tab:mts_ece_table}
    
    \setlength{\aboverulesep}{0pt}
    \setlength{\belowrulesep}{0pt}
    \setlength{\extrarowheight}{.75ex}
    
    \begin{tabular}{cl|ccccc>{\columncolor[gray]{0.9}}cc}
    \toprule
    \multicolumn{2}{c|}{DD Backbone}                            & Raw                  & TS                            & MX                      & LS                        & FL                         & Ours                          \\ \hline 
    \multicolumn{1}{c}{\multirow{4}{*}{MTT}} & CIFAR10         & 4.93 $\pm$ 0.2      & \textit{7.45 $\pm$ 2.1}      & \hspace{-0.08in}\textit{21.06 $\pm$ 0.8}  & \hspace{-0.08in}\textit{25.01 $\pm$ 0.2} & \textit{6.62 $\pm$ 0.2}   & \textbf{1.20 $\pm$ 0.3}      \\  
    \multicolumn{1}{c}{}                     & CIFAR100        & 5.95 $\pm$ 0.4      & \textit{7.76 $\pm$ 0.4}      & \hspace{-0.08in}\textit{14.19 $\pm$ 0.4}  & \hspace{-0.08in}\textit{26.36 $\pm$ 0.4} & \textit{8.30 $\pm$ 0.5}   & \textbf{2.18 $\pm$ 0.2}      \\
    \multicolumn{1}{c}{}                     & Tiny ImageNet           & \hspace{-0.08in}15.78 $\pm$ 0.3     & 2.44 $\pm$ 0.3              & 2.42 $\pm$ 0.3            & \hspace{-0.08in}12.14 $\pm$ 0.3          & 3.61 $\pm$ 0.3            & \textbf{2.26 $\pm$ 0.3}      \\
    \multicolumn{1}{c}{}                     & ImageNette           & 8.68 $\pm$ 1.9      & 4.85 $\pm$ 0.6               & 5.19 $\pm$ 0.6            & \hspace{-0.08in}\textit{23.45 $\pm$ 1.4} & 6.87 $\pm$ 1.3            & \textbf{4.78 $\pm$ 0.5}      \\ \hline 
    \multicolumn{1}{c}{\multirow{2}{*}{RTP}} & CIFAR10         & 2.96 $\pm$ 0.5     & \textit{3.28 $\pm$ 0.7}     & \hspace{-0.08in}\textit{13.35 $\pm$ 1.5}  & \textit{9.58 $\pm$ 0.5} & \textit{8.35 $\pm$ 1.4}   & \textbf{2.22 $\pm$ 0.5}               \\ 
    \multicolumn{1}{c}{}                     & CIFAR100        & \hspace{-0.08in}29.71 $\pm$ 0.6     & \hspace{-0.08in}23.72 $\pm$ 0.6              & \textbf{3.55 $\pm$ 0.6}   & 7.94 $\pm$ 0.2           & \hspace{-0.08in}18.51 $\pm$ 0.5           & \hspace{-0.08in}10.14 $\pm$ 0.4              \\ \hline 
    \multicolumn{1}{c}{DC}                   & CIFAR10         & \hspace{-0.08in}23.60 $\pm$ 0.7     & 5.00 $\pm$ 0.7              & 1.83 $\pm$ 0.3            & \textbf{1.28 $\pm$ 0.1}  & \hspace{-0.08in}13.31 $\pm$ 0.9           & \hspace{-0.08in}10.39 $\pm$ 0.8              \\ \hline 
    \multicolumn{1}{c}{DSA}                  & CIFAR10         & \hspace{-0.08in}19.91 $\pm$ 0.3     & 1.95 $\pm$ 0.4              & 6.44 $\pm$ 0.8            & 2.32 $\pm$ 0.5  & 7.95 $\pm$ 0.7            & \textbf{1.70 $\pm$ 0.4}               \\ \hline   
    \rowcolor[gray]{0.9}\multicolumn{2}{c|}{\textbf{\textit{\# over-calibration}}}          & -                    & 3                            & 3                          & 4                         & 3                          & \textbf{0}                    \\                        
    \bottomrule
    \end{tabular}
    \vspace{-0.2cm}
\end{table*}

\section{Our Proposed Techniques}

We respond to both analyses in Sections 3.1 and 3.2 so that our method can be applied during and after training, providing calibration options at different times and computational budget levels.

\subsection{Masked Temperature Scaling}

As discussed in Section~\ref{sec:analysis_network}, compared to FDNNs, DDNNs produce a more concentrated distribution of logit values with larger values, and these large and condensed logit values lead to networks that are not calibratable.
Since after-training calibration methods such as temperature scaling \cite{guo2017calibration} make use of these large and concentrated logit values from a forward pass of validation data, we seek to overcome this source of difficulty in calibration by perturbing the validation data such that the model could output more various and smaller logit values. Inspired by dropout \cite{srivastava2014dropout}, we apply a simple zero-masking on the validation data of temperature scaling. Our proposed method, which we refer to as Masked Temperature Scaling (MTS), thus modifies Eq~\eqref{eq:temp_scaling} as follows: 
\begin{equation}
    \hat{q_{i}} = \max_{k} \sigma_{softmax}\left (z_{i} * mask / T \right ) ^{(k)},
\end{equation}
\noindent where q, z, mask $\in R^{D}$ and the number of zeros in the mask is controlled by a hyperparameter masking ratio $r$. Note that masking is only applied when updating $T$, such that MTS does not change model accuracy. We use a sampled portion of the training data we have to update the temperature parameter $T$, instead of using separate validation data as in traditional temperature scaling. 

This is particularly necessary in the dataset refinement setting, as we may simply not have any extra data, for example, when each class of images is set to 1 (see Section~\ref{sec:ablations} for more details).

\subsection{Masked Distillation Training}

In response to the analysis in Sections~\ref{sec:analysis_feature}-\ref{sec:analysis_network}, we avoid over-concentration of distillation data on easily identifiable information in the source complete data by perturbing the binary mask during distillation, so that the distillation data also contain more semantically complete information.

\begin{algorithm}[t]
\caption{Masked Distillation Training}\label{alg:paradigm}
\SetNoFillComment
\SetKwInOut{Output}{Notation}
\SetKwInput{KwResult}{Definition}
\SetKwData{Data}{Definition}
\KwIn{Source training data $\mathcal{T}$, number of \\ classes $N_{c}$, deep neural network $\psi_{\theta}$ parameterized with $\theta$, criterion $C$, loss function $l$, total number of training steps $T$, masking ratio $r$}
\KwOut{Distilled dataset $\mathcal{S}$}
\For{$t\gets0$ \KwTo $T$}{
    custom pre-processing

    \For{$c\gets0$ \KwTo $N_{c}$} {
        Sample $T_{c} \sim \mathcal{T}$, $S_{c} \sim \mathcal{S}$\\
        Update synthetic data $\mathcal{S}_{c}$:\\
        $S_c \leftarrow S_c-\lambda \nabla_{S_c} C\left(S_c, Mask\left( T_c, r \right), l, \theta\right)$
    }
    custom post-processing

    Update $\theta$ of network $\psi_{\theta}$ using $T \sim \mathcal{T}$
}
\end{algorithm}

A typical DD training paradigm tries to minimize the differences of certain characteristics, as measured by some criterion $C$, between data batch $B^{\prime}$ from synthetic data and data batch $B$ from source full data. The loss function $l\left( \theta; X\right)$ used in $C$ is usually the cross entropy loss for training $\theta$ on $x$ in classification tasks. We thus put the binary mask on synthetic data before feeding it into $C$. We give the details of our method, Masked Distillation Training (MDT), in Algorithm~\ref{alg:paradigm}. MDT is applicable to various distillation backbones. For instance, in Efficient Dataset Distillation~\cite{zhang2022accelerating}, they set the criterion $C$ as the differences between gradients back-propagated from $l$ given source data $B$ and distilled data $B^{\prime}$:
\begin{equation}
    C\left(B, B^{\prime} ; l, \theta\right) = \left.\| \nabla_\theta \ell(\theta ; B)-\nabla_\theta \ell\left(\theta ; B^{\prime}\right) \|\right. 
    \\
\end{equation}
 When applied with MDT, this now becomes:
 \begin{equation}
    C\left(B, B^{\prime} ; l, \theta\right) = \left.\| \nabla_\theta \ell(\theta ; B)-\nabla_\theta \ell\left(\theta ; Mask\left( B^{\prime}, r \right)\right) \|\right. 
    \\
\end{equation}
 
 In another distillation backbone MTT~\cite{cazenavette2022dataset}, the criterion $C$ measures parameter trajectory differences between DDNNs and FDNNs:
\begin{align}
\begin{split}
    \hat{\theta}_{t+N} \leftarrow \hat{\theta}_{t+N-1}-\lambda \nabla l\left(B^{\prime}, \theta_{t+N-1}\right) \\
    C\left(B^{\prime}, \hat{\theta}; l, \theta\right) = \left \| \hat{\theta}_{t+N} - \theta^{*}_{t+M} \right \| ^{2}_{2} / \left \| \theta^{*}_{t} - \theta^{*}_{t+M} \right \| ^{2}_{2}
\end{split}
\end{align}
and when applied with MDT, this becomes:
\begin{equation}
    \hat{\theta}_{t+N} \leftarrow \hat{\theta}_{t+N-1}-\lambda \nabla l\left(Mask\left( B^{\prime}, r \right), \theta_{t+N-1}\right).
\end{equation}
We find that a masking ratio of 10\% works well for our purposes while losing minimal test accuracy, and we provide more details in Sections \ref{sec:acc_mdt} and \ref{sec:other_mdt}.

% Figure environment removed

% Figure environment removed

\subsection{Connection to Dropouts}

Dropout~\cite{srivastava2014dropout} is a common practice to prevent overfitting of neural networks. Two popular types are unit dropout (U-DP) and weight dropout (W-DP), which randomly discard units (neurons) and individual weights at each training step, respectively. The formulas are shown in Eq~(\ref{eq:dp}).
\begin{align}
    \text{U-DP:} \ Y &= (X \odot M ) W; \ \ 
    \text{W-DP:} \ Y = X (W \odot M) \label{eq:dp},
\end{align}
where $M$ denotes dropout mask and $W$ refers to weights.

Our proposed Masked Distillation Training can be viewed as a new version of dropout on the input, i.e., $X = S_c \odot M$. 
There are practices using dropout on inputs as data augmentation~\cite{bouthillier2015dropout}. In contrast to existing efforts, we apply masking in distillation backbones on synthetic data during their forward passes. Masking some of the synthetic data makes it harder to collect easily reachable information from the source dataset, and thus forces the distillation to focus on other structurally and semantically meaningful information that has not received sufficient attention in previous data distillation.
