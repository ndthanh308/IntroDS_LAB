\begin{thebibliography}{10}\itemsep=-1pt

\bibitem{abdar2021review}
Moloud Abdar, Farhad Pourpanah, Sadiq Hussain, Dana Rezazadegan, Li Liu,
  Mohammad Ghavamzadeh, Paul Fieguth, Xiaochun Cao, Abbas Khosravi, U~Rajendra
  Acharya, et~al.
\newblock A review of uncertainty quantification in deep learning: Techniques,
  applications and challenges.
\newblock {\em Information Fusion}, 76:243--297, 2021.

\bibitem{bouthillier2015dropout}
Xavier Bouthillier, Kishore Konda, Pascal Vincent, and Roland Memisevic.
\newblock Dropout as data augmentation.
\newblock {\em arXiv preprint arXiv:1506.08700}, 2015.

\bibitem{cazenavette2022dataset}
George Cazenavette, Tongzhou Wang, Antonio Torralba, Alexei~A Efros, and
  Jun-Yan Zhu.
\newblock Dataset distillation by matching training trajectories.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 4750--4759, 2022.

\bibitem{chen2022recent}
Xuxin Chen, Ximin Wang, Ke Zhang, Kar-Ming Fung, Theresa~C Thai, Kathleen
  Moore, Robert~S Mannel, Hong Liu, Bin Zheng, and Yuchen Qiu.
\newblock Recent advances and clinical applications of deep learning in medical
  image analysis.
\newblock {\em Medical Image Analysis}, page 102444, 2022.

\bibitem{cheng2022calibrating}
Jiacheng Cheng and Nuno Vasconcelos.
\newblock Calibrating deep neural networks by pairwise constraints.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 13709--13718, 2022.

\bibitem{de2022toward}
Lavindra De~Silva and Alan Mycroft.
\newblock Toward trustworthy programming for autonomous concurrent systems.
\newblock {\em AI \& SOCIETY}, pages 1--3, 2022.

\bibitem{deng2022remember}
Zhiwei Deng and Olga Russakovsky.
\newblock Remember the past: Distilling datasets into addressable memories for
  neural networks.
\newblock In {\em Neural Information Processing Systems (NeurIPS)}, 2022.

\bibitem{gatys2016image}
Leon~A Gatys, Alexander~S Ecker, and Matthias Bethge.
\newblock Image style transfer using convolutional neural networks.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 2414--2423, 2016.

\bibitem{ghoshal2022calibrated}
Biraja Ghoshal and Allan Tucker.
\newblock On calibrated model uncertainty in deep learning.
\newblock {\em arXiv preprint arXiv:2206.07795}, 2022.

\bibitem{guo2017calibration}
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian~Q Weinberger.
\newblock On calibration of modern neural networks.
\newblock In {\em International conference on machine learning}, pages
  1321--1330. PMLR, 2017.

\bibitem{henry19928}
ER Henry and J Hofrichter.
\newblock [8] singular value decomposition: Application to analysis of
  experimental data.
\newblock In {\em Methods in enzymology}, volume 210, pages 129--192. Elsevier,
  1992.

\bibitem{Howard_Imagenette_2019}
Jeremy Howard.
\newblock Imagenette: A smaller subset of 10 easily classified classes from
  imagenet, March 2019.

\bibitem{johnson2016perceptual}
Justin Johnson, Alexandre Alahi, and Li Fei-Fei.
\newblock Perceptual losses for real-time style transfer and super-resolution.
\newblock In {\em Computer Vision--ECCV 2016: 14th European Conference,
  Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14},
  pages 694--711. Springer, 2016.

\bibitem{kim2022dataset}
Jang-Hyun Kim, Jinuk Kim, Seong~Joon Oh, Sangdoo Yun, Hwanjun Song, Joonhyun
  Jeong, Jung-Woo Ha, and Hyun~Oh Song.
\newblock Dataset condensation via efficient synthetic-data parameterization.
\newblock In {\em International Conference on Machine Learning}, pages
  11102--11118. PMLR, 2022.

\bibitem{klema1980singular}
Virginia Klema and Alan Laub.
\newblock The singular value decomposition: Its computation and some
  applications.
\newblock {\em IEEE Transactions on automatic control}, 25(2):164--176, 1980.

\bibitem{krishnan2020improving}
Ranganath Krishnan and Omesh Tickoo.
\newblock Improving model calibration with accuracy versus uncertainty
  optimization.
\newblock {\em Advances in Neural Information Processing Systems},
  33:18237--18248, 2020.

\bibitem{krizhevsky2009learning}
Alex Krizhevsky and Geoffrey Hinton.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, Citeseer, 2009.

\bibitem{le2015tiny}
Ya Le and Xuan Yang.
\newblock Tiny imagenet visual recognition challenge.
\newblock {\em CS 231N}, 7(7):3, 2015.

\bibitem{lee2018self}
Seung~Hyun Lee, Dae~Ha Kim, and Byung~Cheol Song.
\newblock Self-supervised knowledge distillation using singular value
  decomposition.
\newblock In {\em Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 335--350, 2018.

\bibitem{lei2023calibrating}
Bowen Lei, Ruqi Zhang, Dongkuan Xu, and Bani Mallick.
\newblock Calibrating the rigged lottery: Making all tickets reliable.
\newblock {\em arXiv preprint arXiv:2302.09369}, 2023.

\bibitem{lin2017focal}
Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll{\'a}r.
\newblock Focal loss for dense object detection.
\newblock In {\em Proceedings of the IEEE international conference on computer
  vision}, pages 2980--2988, 2017.

\bibitem{liu1989limited}
Dong~C Liu and Jorge Nocedal.
\newblock On the limited memory bfgs method for large scale optimization.
\newblock {\em Mathematical programming}, 45(1-3):503--528, 1989.

\bibitem{liu2022deep}
Tianming Liu, Eliot Siegel, and Dinggang Shen.
\newblock Deep learning and medical image analysis for covid-19 diagnosis and
  prediction.
\newblock {\em Annual Review of Biomedical Engineering}, 24:179--201, 2022.

\bibitem{loo2022efficient}
Noel Loo, Ramin Hasani, Alexander Amini, and Daniela Rus.
\newblock Efficient dataset distillation using random feature approximation.
\newblock {\em arXiv preprint arXiv:2210.12067}, 2022.

\bibitem{mukhoti2020calibrating}
Jishnu Mukhoti, Viveka Kulharia, Amartya Sanyal, Stuart Golodetz, Philip Torr,
  and Puneet Dokania.
\newblock Calibrating deep neural networks using focal loss.
\newblock {\em Advances in Neural Information Processing Systems},
  33:15288--15299, 2020.

\bibitem{muller2019does}
Rafael M{\"u}ller, Simon Kornblith, and Geoffrey~E Hinton.
\newblock When does label smoothing help?
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{nguyen2020dataset}
Timothy Nguyen, Zhourong Chen, and Jaehoon Lee.
\newblock Dataset meta-learning from kernel ridge-regression.
\newblock {\em arXiv preprint arXiv:2011.00050}, 2020.

\bibitem{nguyen2021dataset}
Timothy Nguyen, Roman Novak, Lechao Xiao, and Jaehoon Lee.
\newblock Dataset distillation with infinitely wide convolutional networks.
\newblock {\em Advances in Neural Information Processing Systems},
  34:5186--5198, 2021.

\bibitem{oord2018representation}
Aaron van~den Oord, Yazhe Li, and Oriol Vinyals.
\newblock Representation learning with contrastive predictive coding.
\newblock {\em arXiv preprint arXiv:1807.03748}, 2018.

\bibitem{ovadia2019can}
Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian
  Nowozin, Joshua Dillon, Balaji Lakshminarayanan, and Jasper Snoek.
\newblock Can you trust your model's uncertainty? evaluating predictive
  uncertainty under dataset shift.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{platt1999probabilistic}
John Platt et~al.
\newblock Probabilistic outputs for support vector machines and comparisons to
  regularized likelihood methods.
\newblock {\em Advances in large margin classifiers}, 10(3):61--74, 1999.

\bibitem{rasheed2022explainable}
Khansa Rasheed, Adnan Qayyum, Mohammed Ghaly, Ala Al-Fuqaha, Adeel Razi, and
  Junaid Qadir.
\newblock Explainable, trustworthy, and ethical machine learning for
  healthcare: A survey.
\newblock {\em Computers in Biology and Medicine}, page 106043, 2022.

\bibitem{salahuddin2022transparency}
Zohaib Salahuddin, Henry~C Woodruff, Avishek Chatterjee, and Philippe Lambin.
\newblock Transparency of deep neural networks for medical image analysis: A
  review of interpretability methods.
\newblock {\em Computers in biology and medicine}, 140:105111, 2022.

\bibitem{srivastava2014dropout}
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
  Salakhutdinov.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock {\em The journal of machine learning research}, 15(1):1929--1958,
  2014.

\bibitem{sucholutsky2021soft}
Ilia Sucholutsky and Matthias Schonlau.
\newblock Soft-label dataset distillation and text dataset distillation.
\newblock In {\em 2021 International Joint Conference on Neural Networks
  (IJCNN)}, pages 1--8. IEEE, 2021.

\bibitem{thulasidasan2019mixup}
Sunil Thulasidasan, Gopinath Chennupati, Jeff~A Bilmes, Tanmoy Bhattacharya,
  and Sarah Michalak.
\newblock On mixup training: Improved calibration and predictive uncertainty
  for deep neural networks.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{van2017neural}
Aaron Van Den~Oord, Oriol Vinyals, et~al.
\newblock Neural discrete representation learning.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{wang2021rethinking}
Deng-Bao Wang, Lei Feng, and Min-Ling Zhang.
\newblock Rethinking calibration of deep neural networks: Do not be afraid of
  overconfidence.
\newblock {\em Advances in Neural Information Processing Systems},
  34:11809--11820, 2021.

\bibitem{Wang_2022_CVPR}
Kai Wang, Bo Zhao, Xiangyu Peng, Zheng Zhu, Shuo Yang, Shuo Wang, Guan Huang,
  Hakan Bilen, Xinchao Wang, and Yang You.
\newblock Cafe: Learning to condense dataset by aligning features.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pages 12196--12205, June 2022.

\bibitem{wang2018dataset}
Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba, and Alexei~A Efros.
\newblock Dataset distillation.
\newblock {\em arXiv preprint arXiv:1811.10959}, 2018.

\bibitem{wei2022mitigating}
Hongxin Wei, Renchunzi Xie, Hao Cheng, Lei Feng, Bo An, and Yixuan Li.
\newblock Mitigating neural network overconfidence with logit normalization.
\newblock In {\em International Conference on Machine Learning}, pages
  23631--23644. PMLR, 2022.

\bibitem{wu2023towards}
Longfeng Wu, Bowen Lei, Dongkuan Xu, and Dawei Zhou.
\newblock Towards reliable rare category analysis on graphs via individual
  calibration.
\newblock {\em arXiv preprint arXiv:2307.09858}, 2023.

\bibitem{xue2013restructuring}
Jian Xue, Jinyu Li, and Yifan Gong.
\newblock Restructuring of deep neural network acoustic models with singular
  value decomposition.
\newblock In {\em Interspeech}, pages 2365--2369, 2013.

\bibitem{xue2014singular}
Jian Xue, Jinyu Li, Dong Yu, Mike Seltzer, and Yifan Gong.
\newblock Singular value decomposition based low-footprint speaker adaptation
  and personalization for deep neural network.
\newblock In {\em 2014 IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP)}, pages 6359--6363. IEEE, 2014.

\bibitem{yu2023dataset}
Ruonan Yu, Songhua Liu, and Xinchao Wang.
\newblock Dataset distillation: A comprehensive review.
\newblock {\em arXiv preprint arXiv:2301.07014}, 2023.

\bibitem{yuan2020revisiting}
Li Yuan, Francis~EH Tay, Guilin Li, Tao Wang, and Jiashi Feng.
\newblock Revisiting knowledge distillation via label smoothing regularization.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 3903--3911, 2020.

\bibitem{zhang2017mixup}
Hongyi Zhang, Moustapha Cisse, Yann~N Dauphin, and David Lopez-Paz.
\newblock mixup: Beyond empirical risk minimization.
\newblock {\em arXiv preprint arXiv:1710.09412}, 2017.

\bibitem{zhang2022accelerating}
Lei Zhang, Jie Zhang, Bowen Lei, Subhabrata Mukherjee, Xiang Pan, Bo Zhao,
  Caiwen Ding, Yao Li, and Dongkuan Xu.
\newblock Accelerating dataset distillation via model augmentation.
\newblock {\em arXiv preprint arXiv:2212.06152}, 2022.

\bibitem{zhang2023accelerating}
Lei Zhang, Jie Zhang, Bowen Lei, Subhabrata Mukherjee, Xiang Pan, Bo Zhao,
  Caiwen Ding, Yao Li, and Dongkuan Xu.
\newblock Accelerating dataset distillation via model augmentation.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 11950--11959, 2023.

\bibitem{zhao2021dataset}
Bo Zhao and Hakan Bilen.
\newblock Dataset condensation with differentiable siamese augmentation.
\newblock In {\em International Conference on Machine Learning}, pages
  12674--12685. PMLR, 2021.

\bibitem{zhao2020dataset}
Bo Zhao, Konda~Reddy Mopuri, and Hakan Bilen.
\newblock Dataset condensation with gradient matching.
\newblock {\em arXiv preprint arXiv:2006.05929}, 2020.

\bibitem{zhou2022dataset}
Yongchao Zhou, Ehsan Nezhadarya, and Jimmy Ba.
\newblock Dataset distillation using neural feature regression.
\newblock {\em arXiv preprint arXiv:2206.00719}, 2022.

\end{thebibliography}
