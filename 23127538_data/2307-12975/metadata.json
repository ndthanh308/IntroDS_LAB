{
  "title": "Provable Benefits of Policy Learning from Human Preferences in Contextual Bandit Problems",
  "authors": [
    "Xiang Ji",
    "Huazheng Wang",
    "Minshuo Chen",
    "Tuo Zhao",
    "Mengdi Wang"
  ],
  "submission_date": "2023-07-24T17:50:24+00:00",
  "revised_dates": [
    "2023-10-31T00:43:40+00:00"
  ],
  "abstract": "For a real-world decision-making problem, the reward function often needs to be engineered or learned. A popular approach is to utilize human feedback to learn a reward function for training. The most straightforward way to do so is to ask humans to provide ratings for state-action pairs on an absolute scale and take these ratings as reward samples directly. Another popular way is to ask humans to rank a small set of state-action pairs by preference and learn a reward function from these preference data. Recently, preference-based methods have demonstrated substantial success in empirical applications such as InstructGPT. In this work, we develop a theoretical comparison between these human feedback approaches in offline contextual bandits and show how human bias and uncertainty in feedback modelings can affect the theoretical guarantees of these approaches. Through this, our results seek to provide a theoretical explanation for the empirical successes of preference-based methods from a modeling perspective.",
  "categories": [
    "cs.LG",
    "math.ST",
    "stat.ML"
  ],
  "primary_category": "cs.LG",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.12975",
  "pdf_url": null,
  "comment": null,
  "num_versions": null,
  "size_before_bytes": 320711,
  "size_after_bytes": 322360
}