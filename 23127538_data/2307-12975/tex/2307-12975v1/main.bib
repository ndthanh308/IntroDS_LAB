@inproceedings{Gonzalez10voice,
author = {Tenorio-Gonzalez, Ana C. and Morales, Eduardo F. and Villase\~{n}or-Pineda, Luis},
title = {Dynamic Reward Shaping: Training a Robot by Voice},
year = {2010},
isbn = {3642169511},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Reinforcement Learning is commonly used for learning tasks in robotics, however, traditional algorithms can take very long training times. Reward shaping has been recently used to provide domain knowledge with extra rewards to converge faster. The reward shaping functions are normally defined in advance by the user and are static. This paper introduces a dynamic reward shaping approach, in which these extra rewards are not consistently given, can vary with time and may sometimes be contrary to what is needed for achieving a goal. In the experiments, a user provides verbal feedback while a robot is performing a task which is translated into additional rewards. It is shown that we can still guarantee convergence as long as most of the shaping rewards given per state are consistent with the goals and that even with fairly noisy interaction the system can still produce faster convergence times than traditional reinforcement learning techniques.},
booktitle = {Proceedings of the 12th Ibero-American Conference on Advances in Artificial Intelligence},
pages = {483-492},
numpages = {10},
location = {Bah\'{\i}a Blanca, Argentina},
series = {IBERAMIA'10}
}

@ARTICLE{Janowski15accuracy,
  author={Janowski, Lucjan and Pinson, Margaret},
  journal={IEEE Transactions on Multimedia}, 
  title={The Accuracy of Subjects in a Quality Experiment: A Theoretical Subject Model}, 
  year={2015},
  volume={17},
  number={12},
  pages={2210-2224},
  doi={10.1109/TMM.2015.2484963}}

@article{li2020simple,
  title={A simple model for subject behavior in subjective experiments},
  author={Li, Zhi and Bampis, Christos G and Krasula, Luk{\'a}{\v{s}} and Janowski, Lucjan and Katsavounidis, Ioannis},
  journal={arXiv preprint arXiv:2004.02067},
  year={2020}
}

@article{Tarlow21Reliable,
author = {Kevin R. Tarlow and Daniel F. Brossart and Alexandra M. McCammon and Alexander J. Giovanetti and M. Camille Belle and Joshua Philip},
editor = {Marco Tommasi},
title = {Reliable visual analysis of single-case data: A comparison of rating, ranking, and pairwise methods},
journal = {Cogent Psychology},
volume = {8},
number = {1},
pages = {1911076},
year  = {2021},
publisher = {Cogent OA},
doi = {10.1080/23311908.2021.1911076},
URL = {https://doi.org/10.1080/23311908.2021.1911076},
eprint = {https://doi.org/10.1080/23311908.2021.1911076}}

@ARTICLE{Ortiz20unified,
  author={P\'{e}rez-Ortiz, María and Mikhailiuk, Aliaksei and Zerman, Emin and Hulusic, Vedad and Valenzise, Giuseppe and Mantiuk, Rafał K.},
  journal={IEEE Transactions on Image Processing}, 
  title={From Pairwise Comparisons and Rating to a Unified Quality Scale}, 
  year={2020},
  volume={29},
  number={},
  pages={1139-1151},
  doi={10.1109/TIP.2019.2936103}}

@inproceedings{li2017recover,
  title={Recover subjective quality scores from noisy measurements},
  author={Li, Zhi and Bampis, Christos G},
  booktitle={2017 Data compression conference (DCC)},
  pages={52--61},
  year={2017},
  organization={IEEE}
}

@inproceedings{li20crowdsourcing, author = {Li, Jing and Ling, Suiyi and Wang, Junle and Le Callet, Patrick}, title = {A Probabilistic Graphical Model for Analyzing the Subjective Visual Quality Assessment Data from Crowdsourcing}, year = {2020}, isbn = {9781450379885}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3394171.3413619}, doi = {10.1145/3394171.3413619}, abstract = {The swift development of the multimedia technology has raised dramatically the users' expectation on the quality of experience. To obtain the ground-truth perceptual quality for model training, subjective assessment is necessary. Crowdsourcing platform provides us a convenient and feasible way to run large-scale experiments. However, the obtained perceptual quality labels are generally noisy. In this paper, we propose a probabilistic graphical annotation model to infer the underlying ground truth and discovering the annotator's behavior. In the proposed model, the ground truth quality label is considered following a categorical distribution rather than a unique number, i.e., different reliable opinions on the perceptual quality are allowed. In addition, different annotator's behaviors in crowdsourcing are modeled, which allows us to identify the possibility that the annotator makes noisy labels during the test. The proposed model has been tested on both simulated data and real-world data, where it always shows superior performance than the other state-of-the-art models in terms of accuracy and robustness.}, booktitle = {Proceedings of the 28th ACM International Conference on Multimedia}, pages = {3339-3347}, numpages = {9}, keywords = {probabilistic graphic model, crowdsourcing, ground truth, quality assessment, annotator behavior}, location = {Seattle, WA, USA}, series = {MM '20} }

@article{zhu2023principled,
  title={Principled Reinforcement Learning with Human Feedback from Pairwise or $ K $-wise Comparisons},
  author={Zhu, Banghua and Jiao, Jiantao and Jordan, Michael I},
  journal={arXiv preprint arXiv:2301.11270},
  year={2023}
}

@inproceedings{chen2019information,
  title={Information-theoretic considerations in batch reinforcement learning},
  author={Chen, Jinglin and Jiang, Nan},
  booktitle={International Conference on Machine Learning},
  pages={1042--1051},
  year={2019},
  organization={PMLR}
}

@inproceedings{xie2020q,
  title={Q* approximation schemes for batch reinforcement learning: A theoretical comparison},
  author={Xie, Tengyang and Jiang, Nan},
  booktitle={Conference on Uncertainty in Artificial Intelligence},
  pages={550--559},
  year={2020},
  organization={PMLR}
}

@inproceedings{yin2021near,
  title={Near-optimal provable uniform convergence in offline policy evaluation for reinforcement learning},
  author={Yin, Ming and Bai, Yu and Wang, Yu-Xiang},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1567--1575},
  year={2021},
  organization={PMLR}
}

@article{uehara2021finite,
  title={Finite sample analysis of minimax offline reinforcement learning: Completeness, fast rates and first-order efficiency},
  author={Uehara, Masatoshi and Imaizumi, Masaaki and Jiang, Nan and Kallus, Nathan and Sun, Wen and Xie, Tengyang},
  journal={arXiv preprint arXiv:2102.02981},
  year={2021}
}

@inproceedings{hao2021bootstrapping,
  title={Bootstrapping fitted q-evaluation for off-policy inference},
  author={Hao, Botao and Ji, Xiang and Duan, Yaqi and Lu, Hao and Szepesv{\'a}ri, Csaba and Wang, Mengdi},
  booktitle={International Conference on Machine Learning},
  pages={4074--4084},
  year={2021},
  organization={PMLR}
}


% Preference based RL

@article{sui19,
  title={Dueling posterior sampling for preference-based reinforcement learning},
  author={Novoseller, Ellen R and Sui, Yanan and Yue, Yisong and Burdick, Joel W},
  journal={arXiv preprint arXiv:1908.01289},
  year={2019}
}

@inproceedings{xu20,
 author = {Xu, Yichong and Wang, Ruosong and Yang, Lin and Singh, Aarti and Dubrawski, Artur},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {18784--18794},
 publisher = {Curran Associates, Inc.},
 title = {Preference-based Reinforcement Learning with Finite-Time Guarantees},
 url = {https://proceedings.neurips.cc/paper/2020/file/d9d3837ee7981e8c064774da6cdd98bf-Paper.pdf},
 volume = {33},
 year = {2020}
}


@article{busa14,
	title={Preference-based reinforcement learning: evolutionary direct policy search using a preference-based racing algorithm},
	author={Busa-Fekete, R{\'o}bert and Sz{\"o}r{\'e}nyi, Bal{\'a}zs and Weng, Paul and Cheng, Weiwei and H{\"u}llermeier, Eyke},
	journal={Machine Learning},
	volume={97},
	number={3},
	pages={327--351},
	year={2014},
	publisher={Springer}
}

@inproceedings{wirth16,
	title={Model-free preference-based reinforcement learning},
	author={Wirth, Christian and Furnkranz, Johannes and Neumann, Gerhard and others},
	booktitle={30th AAAI Conference on Artificial Intelligence, AAAI 2016},
	pages={2222--2228},
	year={2016}
}

@article{wirth17,
	title={A survey of preference-based reinforcement learning methods},
	author={Wirth, Christian and Akrour, Riad and Neumann, Gerhard and F{\"u}rnkranz, Johannes},
	journal={The Journal of Machine Learning Research},
	volume={18},
	number={1},
	pages={4945--4990},
	year={2017},
	publisher={JMLR. org}
}

@article{christiano2017deep,
  title={Deep reinforcement learning from human preferences},
  author={Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}
@article{shin2023benchmarks,
  title={Benchmarks and Algorithms for Offline Preference-Based Reward Learning},
  author={Shin, Daniel and Dragan, Anca D and Brown, Daniel S},
  journal={arXiv preprint arXiv:2301.01392},
  year={2023}
}
@inproceedings{Busa14survey,
  title={A survey of preference-based online learning with bandit algorithms},
  author={Busa-Fekete, R{\'o}bert and H{\"u}llermeier, Eyke},
  booktitle={International Conference on Algorithmic Learning Theory},
  pages={18--39},
  year={2014},
  organization={Springer}
}

@inproceedings{nipsPRL17,
	title={Deep reinforcement learning from human preferences},
	author={Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
	booktitle={Advances in Neural Information Processing Systems},
	pages={4299--4307},
	year={2017}
}

@article{choifast,
	title={Fast Adaptation of Deep Reinforcement Learning-Based Navigation Skills to Human Preference},
	author={Choi, Jinyoung and Dance, Christopher and Kim, Jung-eun and Park, Kyung-sik and Han, Jaehun and Seo, Joonho and Kim, Minsu}
}

@inproceedings{sadigh17,
	title={Active Preference-Based Learning of Reward Functions.},
	author={Sadigh, Dorsa and Dragan, Anca D and Sastry, Shankar and Seshia, Sanjit A},
	booktitle={Robotics: Science and Systems},
	year={2017}
}

@incollection{kupcsik18,
	title={Learning dynamic robot-to-human object handover from human feedback},
	author={Kupcsik, Andras and Hsu, David and Lee, Wee Sun},
	booktitle={Robotics research},
	pages={161--176},
	year={2018},
	publisher={Springer}
}

@inproceedings{jain13,
	title={Learning trajectory preferences for manipulators via iterative improvement},
	author={Jain, Ashesh and Wojcik, Brian and Joachims, Thorsten and Saxena, Ashutosh},
	booktitle={Advances in neural information processing systems},
	pages={575--583},
	year={2013}
}

@article{zhao11,
	title={Reinforcement learning strategies for clinical trials in nonsmall cell lung cancer},
	author={Zhao, Yufan and Zeng, Donglin and Socinski, Mark A and Kosorok, Michael R},
	journal={Biometrics},
	volume={67},
	number={4},
	pages={1422--1433},
	year={2011},
	publisher={Wiley Online Library}
}


@article{MNP,
  title={Multinomial probit and multinomial logit: a comparison of choice models for voting research},
  author={Dow, Jay K and Endersby, James W},
  journal={Electoral studies},
  volume={23},
  number={1},
  pages={107--122},
  year={2004},
  publisher={Elsevier}
}

@article{NestedLogit,
  title={The generalized nested logit model},
  author={Wen, Chieh-Hua and Koppelman, Frank S},
  journal={Transportation Research Part B: Methodological},
  volume={35},
  number={7},
  pages={627--641},
  year={2001},
  publisher={Elsevier}
}

@inproceedings{DCM,
  title={DCM bandits: Learning to rank with multiple clicks},
  author={Katariya, Sumeet and Kveton, Branislav and Szepesvari, Csaba and Wen, Zheng},
  booktitle={International Conference on Machine Learning},
  pages={1215--1224},
  year={2016}
}

@inproceedings{SG18,
  title={Battle of Bandits},
  author={Saha, Aadirupa and Gopalan, Aditya},
  booktitle={Uncertainty in Artificial Intelligence},
  year={2018}
}

@inproceedings{SGwin18, 
title={{PAC Battling Bandits in the Plackett-Luce Model}}, author={Saha, Aadirupa and Gopalan, Aditya}, booktitle={Algorithmic Learning Theory}, pages={700--737}, year={2019} }


@article{SGrank18,
  title={Active Ranking with Subset-wise Preferences},
  author={Saha, Aadirupa and Gopalan, Aditya},
  journal={International Conference on Artificial Intelligence and Statistics (AISTATS)},
  year={2018}
}

@article{Ren+18,
  title={PAC Ranking from Pairwise and Listwise Queries: Lower Bounds and Upper Bounds},
  author={Ren, Wenbo and Liu, Jia and Shroff, Ness B},
  journal={arXiv preprint arXiv:1806.02970},
  year={2018}
}

@inproceedings{RDB,
  title={Efficient and Optimal Algorithms for Contextual Dueling Bandits under Realizability},
  author={Saha, Aadirupa and Krishnamurthy, Akshay},
  booktitle={International Conference on Algorithmic Learning Theory},
  pages={968--994},
  year={2022},
  organization={PMLR}
}

@inproceedings{gupta2021optimal,
  title={Optimal and efficient dynamic regret algorithms for non-stationary dueling bandits},
  author={Gupta, Shubham and Saha, Aadirupa},
  booktitle={International Conference on Machine Learning},
  pages={19027--19049},
  year={2022},
  organization={PMLR}
}

@inproceedings{GS21,
  title={Exploiting Correlation to Achieve Faster Learning Rates in Low-Rank Preference Bandits},
  author={Ghoshal, Suprovat and Saha, Aadirupa},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={456--482},
  year={2022},
  organization={PMLR}
}
@inproceedings{Yue+09,
  title={Interactively optimizing information retrieval systems as a dueling bandits problem},
  author={Yue, Yisong and Joachims, Thorsten},
  booktitle={Proceedings of the 26th Annual International Conference on Machine Learning},
  pages={1201--1208},
  year={2009},
  organization={ACM}
}

@inproceedings{Ailon+14,
  title={Reducing Dueling Bandits to Cardinal Bandits.},
  author={Ailon, Nir and Karnin, Zohar Shay and Joachims, Thorsten},
  booktitle={ICML},
  volume={32},
  pages={856--864},
  year={2014}
}
@inproceedings{Adv_DB, 
  title={A Relative Exponential Weighing Algorithm for Adversarial Utility-based Dueling Bandits},
  author={Gajane, Pratik and Urvoy, Tanguy and Cl{\'e}rot, Fabrice},
  booktitle={Proceedings of the 32nd International Conference on Machine Learning},
  pages={218--227},
  year={2015}
}
@article{xu2022provably,
  title={Provably efficient offline reinforcement learning with trajectory-wise reward},
  author={Xu, Tengyu and Liang, Yingbin},
  journal={arXiv preprint arXiv:2206.06426},
  year={2022}
}
@inproceedings{abdelkareem2022advances,
  title={Advances in Preference-based Reinforcement Learning: A Review},
  author={Abdelkareem, Youssef and Shehata, Shady and Karray, Fakhri},
  booktitle={2022 IEEE International Conference on Systems, Man, and Cybernetics (SMC)},
  pages={2527--2532},
  year={2022},
  organization={IEEE}
}
@inproceedings{chen2022human,
  title={Human-in-the-loop: Provably Efficient Preference-based Reinforcement Learning with General Function Approximation},
  author={Chen, Xiaoyu and Zhong, Han and Yang, Zhuoran and Wang, Zhaoran and Wang, Liwei},
  booktitle={International Conference on Machine Learning},
  pages={3773--3793},
  year={2022},
  organization={PMLR}
}
@article{pacchiano2021dueling,
  title={Dueling rl: reinforcement learning with trajectory preferences},
  author={Pacchiano, Aldo and Saha, Aadirupa and Lee, Jonathan},
  journal={arXiv preprint arXiv:2111.04850},
  year={2021}
}
@inproceedings{BTM,
  title={Beat the mean bandit},
  author={Yue, Yisong and Joachims, Thorsten},
  booktitle={Proceedings of the 28th International Conference on Machine Learning (ICML-11)},
  pages={241--248},
  year={2011} 
}
@inproceedings{Komiyama+15,
  title={Regret Lower Bound and Optimal Algorithm in Dueling Bandit Problem.},
  author={Komiyama, Junpei and Honda, Junya and Kashima, Hisashi and Nakagawa, Hiroshi},
  booktitle={COLT},
  pages={1141--1154},
  year={2015}
}

@inproceedings{Zoghi+14RCS,
  title={Relative confidence sampling for efficient on-line ranker evaluation},
  author={Zoghi, Masrour and Whiteson, Shimon A and De Rijke, Maarten and Munos, Remi},
  booktitle={Proceedings of the 7th ACM international conference on Web search and data mining},
  pages={73--82},
  year={2014},
  organization={ACM}
}
@inproceedings{Zoghi+14RUCB,
  title={Relative upper confidence bound for the $k$-armed dueling bandit problem},
  author={Zoghi, Masrour and Whiteson, Shimon and Munos, Remi and Rijke, Maarten de and others},
  booktitle={JMLR Workshop and Conference Proceedings},
  number={32},
  pages={10--18},
  year={2014},
  organization={JMLR}
}

@article{Yue+12,
  title={The $k$-armed dueling bandits problem},
  author={Yue, Yisong and Broder, Josef and Kleinberg, Robert and Joachims, Thorsten},
  journal={Journal of Computer and System Sciences},
  volume={78},
  number={5},
  pages={1538--1556},
  year={2012},
  publisher={Elsevier}
}

@article{saha2021optimal,
  title={Optimal Algorithms for Stochastic Contextual Preference Bandits},
  author={Saha, Aadirupa},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={30050--30062},
  year={2021}
}


%pessimistic offline RL
@inproceedings{jin2021pessimism,
  title={Is pessimism provably efficient for offline {RL}?},
  author={Jin, Ying and Yang, Zhuoran and Wang, Zhaoran},
  booktitle={International Conference on Machine Learning},
  pages={5084--5096},
  year={2021},
  organization={PMLR}
}
@article{xie2021bellman,
  title={Bellman-consistent pessimism for offline reinforcement learning},
  author={Xie, Tengyang and Cheng, Ching-An and Jiang, Nan and Mineiro, Paul and Agarwal, Alekh},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={6683--6694},
  year={2021}
}
@inproceedings{yin2022near,
  title={Near-optimal Offline Reinforcement Learning with Linear Representation: Leveraging Variance Information with Pessimism},
  author={Yin, Ming and Duan, Yaqi and Wang, Mengdi and Wang, Yu-Xiang},
  booktitle={International Conference on Learning Representation},
  year={2022}
}
%
cle{kostrikov2021offline,
  title={Offline reinforcement learning with implicit {Q}-learning},
  author={Kostrikov, Ilya and Nair, Ashvin and Levine, Sergey},
  journal={arXiv preprint arXiv:2110.06169},
  year={2021}
}
@article{kumar2020conservative,
  title={Conservative {Q}-learning for offline reinforcement learning},
  author={Kumar, Aviral and Zhou, Aurick and Tucker, George and Levine, Sergey},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1179--1191},
  year={2020}
}
@article{cheng2022adversarially,
  title={Adversarially trained actor critic for offline reinforcement learning},
  author={Cheng, Ching-An and Xie, Tengyang and Jiang, Nan and Agarwal, Alekh},
  journal={arXiv preprint arXiv:2202.02446},
  year={2022}
}

@article{rashidinejad2021bridging,
  title={Bridging offline reinforcement learning and imitation learning: A tale of pessimism},
  author={Rashidinejad, Paria and Zhu, Banghua and Ma, Cong and Jiao, Jiantao and Russell, Stuart},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={11702--11716},
  year={2021}
}
@article{xie2021policy,
  title={Policy finetuning: Bridging sample-efficient offline and online reinforcement learning},
  author={Xie, Tengyang and Jiang, Nan and Wang, Huan and Xiong, Caiming and Bai, Yu},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={27395--27407},
  year={2021}
}
@article{zanette2021provable,
  title={Provable benefits of actor-critic methods for offline reinforcement learning},
  author={Zanette, Andrea and Wainwright, Martin J and Brunskill, Emma},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={13626--13640},
  year={2021}
}
@article{zanette2022realizability,
  title={When is Realizability Sufficient for Off-Policy Reinforcement Learning?},
  author={Zanette, Andrea},
  journal={arXiv preprint arXiv:2211.05311},
  year={2022}
}
@article{li2022pessimism,
  title={Pessimism for offline linear contextual bandits using $\ell_p$ confidence sets},
  author={Li, Gene and Ma, Cong and Srebro, Nathan},
  journal={arXiv preprint arXiv:2205.10671},
  year={2022}
}
@article{yu2020mopo,
  title={Mopo: Model-based offline policy optimization},
  author={Yu, Tianhe and Thomas, Garrett and Yu, Lantao and Ermon, Stefano and Zou, James Y and Levine, Sergey and Finn, Chelsea and Ma, Tengyu},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={14129--14142},
  year={2020}
}
@inproceedings{shi2022pessimistic,
  title={Pessimistic q-learning for offline reinforcement learning: Towards optimal sample complexity},
  author={Shi, Laixi and Li, Gen and Wei, Yuting and Chen, Yuxin and Chi, Yuejie},
  booktitle={International Conference on Machine Learning},
  pages={19967--20025},
  year={2022},
  organization={PMLR}
}

@article{yin2021towards,
	title={Towards instance-optimal offline reinforcement learning with pessimism},
	author={Yin, Ming and Wang, Yu-Xiang},
	journal={Proc. Advances in Neural Information Processing Systems (NeurIPS)},
	volume={34},
	year={2021}
}

@article{ren2021nearly,
	title={Nearly horizon-free offline reinforcement learning},
	author={Ren, Tongzheng and Li, Jialian and Dai, Bo and Du, Simon S and Sanghavi, Sujay},
	journal={Proc. Advances in Neural Information Processing Systems (NeurIPS)},
	volume={34},
	year={2021}
}

@article{yin2021optimal,
	title={Optimal uniform ope and model-based offline reinforcement learning in time-homogeneous, reward-free and task-agnostic settings},
	author={Yin, Ming and Wang, Yu-Xiang},
	journal={Proc. Advances in Neural Information Processing Systems (NeurIPS)},
	volume={34},
	year={2021}
}

@article{li2022settling,
	title={Settling the sample complexity of model-based offline reinforcement learning},
	author={Li, Gen and Shi, Laixi and Chen, Yuxin and Chi, Yuejie and Wei, Yuting},
	journal={arXiv preprint arXiv:2204.05275},
	year={2022}
}

@article{yan2022efficacy,
	title={The efficacy of pessimism in asynchronous Q-learning},
	author={Yan, Yuling and Li, Gen and Chen, Yuxin and Fan, Jianqing},
	journal={arXiv preprint arXiv:2203.07368},
	year={2022}
}

@article{buckman2020importance,
	title={The importance of pessimism in fixed-dataset policy optimization},
	author={Buckman, Jacob and Gelada, Carles and Bellemare, Marc G},
	journal={arXiv preprint arXiv:2009.06799},
	year={2020}
}
@article{foster2021offline,
	title={Offline reinforcement learning: fundamental barriers for value function approximation},
	author={Foster, Dylan J and Krishnamurthy, Akshay and Simchi-Levi, David and Xu, Yunzong},
	journal={arXiv preprint arXiv:2111.10919},
	year={2021}
}

@article{wang2020statistical,
	title={What are the Statistical Limits of Offline RL with Linear Function Approximation?},
	author={Wang, Ruosong and Foster, Dean P and Kakade, Sham M},
	journal={arXiv preprint arXiv:2010.11895},
	year={2020}
}

@article{Bradley1952RankAO,
  title={Rank Analysis of Incomplete Block Designs: I. The Method of Paired Comparisons},
  author={Ralph Allan Bradley and Milton E. Terry},
  journal={Biometrika},
  year={1952},
  volume={39},
  pages={324}
}

@inproceedings{shah2015estimation,
  title={Estimation from pairwise comparisons: Sharp minimax bounds with topology dependence},
  author={Shah, Nihar and Balakrishnan, Sivaraman and Bradley, Joseph and Parekh, Abhay and Ramchandran, Kannan and Wainwright, Martin},
  booktitle={Artificial intelligence and statistics},
  pages={856--865},
  year={2015},
  organization={PMLR}
}

@article{silver2016,
  added-at = {2016-03-11T14:36:05.000+0100},
  author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  doi = {10.1038/nature16961},
  interhash = {48430c7891aaf9fe2582faa8f5d076c1},
  intrahash = {9e987f58d895c490144693139cbc90c7},
  journal = {Nature},
  keywords = {baduk go google},
  month = jan,
  number = 7587,
  pages = {484--489},
  publisher = {Nature Publishing Group},
  timestamp = {2016-03-11T14:37:40.000+0100},
  title = {Mastering the Game of {Go} with Deep Neural Networks and Tree Search},
  volume = 529,
  year = 2016
}

@inproceedings{mnih2016asynchronous,
  title={Asynchronous methods for deep reinforcement learning},
  author={Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  booktitle={International conference on machine learning},
  pages={1928--1937},
  year={2016},
  organization={PMLR}
}

@article{mnih2015humanlevel,
  added-at = {2015-08-26T14:46:40.000+0200},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  description = {Human-level control through deep reinforcement learning - nature14236.pdf},
  interhash = {eac59980357d99db87b341b61ef6645f},
  intrahash = {fb15f4471c81dc2b9edf2304cb2f7083},
  issn = {00280836},
  journal = {Nature},
  keywords = {deep learning toread},
  month = feb,
  number = 7540,
  pages = {529--533},
  publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  timestamp = {2015-08-26T14:46:40.000+0200},
  title = {Human-level control through deep reinforcement learning},
  url = {http://dx.doi.org/10.1038/nature14236},
  volume = 518,
  year = 2015
}

@article{li2020gpm,
  title={GPM: A Generic Probabilistic Model to Recover Annotator's Behavior and Ground Truth Labeling},
  author={Li, Jing and Ling, Suiyi and Wang, Junle and Li, Zhi and Callet, Patrick Le},
  journal={arXiv preprint arXiv:2003.00475},
  year={2020}
}

@article{Plackett1975TheAO,
  title={The Analysis of Permutations},
  author={Robin L. Plackett},
  journal={Journal of The Royal Statistical Society Series C-applied Statistics},
  year={1975},
  volume={24},
  pages={193-202}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@inproceedings{duan2020minimax,
  title={Minimax-optimal off-policy evaluation with linear function approximation},
  author={Duan, Yaqi and Jia, Zeyu and Wang, Mengdi},
  booktitle={International Conference on Machine Learning},
  pages={2701--2709},
  year={2020},
  organization={PMLR}
}

@inproceedings{antos06, author = {Antos, Andr\'{a}s and Szepesv\'{a}ri, Csaba and Munos, R\'{e}mi}, title = {Learning Near-Optimal Policies with Bellman-Residual Minimization Based Fitted Policy Iteration and a Single Sample Path}, year = {2006}, isbn = {3540352945}, publisher = {Springer-Verlag}, address = {Berlin, Heidelberg}, url = {https://doi.org/10.1007/11776420_42}, doi = {10.1007/11776420_42}, abstract = {We consider batch reinforcement learning problems in continuous space, expected total discounted-reward Markovian Decision Problems. As opposed to previous theoretical work, we consider the case when the training data consists of a single sample path (trajectory) of some behaviour policy. In particular, we do not assume access to a generative model of the environment. The algorithm studied is policy iteration where in successive iterations the Q-functions of the intermediate policies are obtained by means of minimizing a novel Bellman-residual type error. PAC-style polynomial bounds are derived on the number of samples needed to guarantee near-optimal performance where the bound depends on the mixing rate of the trajectory, the smoothness properties of the underlying Markovian Decision Problem, the approximation power and capacity of the function set used.}, booktitle = {Proceedings of the 19th Annual Conference on Learning Theory}, pages = {574-588}, numpages = {15}, location = {Pittsburgh, PA}, series = {COLT'06} }

@inproceedings{csaba05, author = {Szepesv\'{a}ri, Csaba and Munos, R\'{e}mi}, title = {Finite Time Bounds for Sampling Based Fitted Value Iteration}, year = {2005}, isbn = {1595931805}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/1102351.1102462}, doi = {10.1145/1102351.1102462}, abstract = {In this paper we consider sampling based fitted value iteration for discounted, large (possibly infinite) state space, finite action Markovian Decision Problems where only a generative model of the transition probabilities and rewards is available. At each step the image of the current estimate of the optimal value function under a Monte-Carlo approximation to the Bellman-operator is projected onto some function space. PAC-style bounds on the weighted Lp-norm approximation error are obtained as a function of the covering number and the approximation power of the function space, the iteration number and the sample size.}, booktitle = {Proceedings of the 22nd International Conference on Machine Learning}, pages = {880-887}, numpages = {8}, location = {Bonn, Germany}, series = {ICML '05} }

@INPROCEEDINGS{dong17crowdsource,
  author={Dong, Jialin and Yang, Kai and Shi, Yuanming},
  booktitle={2017 IEEE International Conference on Data Mining Workshops (ICDMW)}, 
  title={Ranking from Crowdsourced Pairwise Comparisons via Smoothed Matrix Manifold Optimization}, 
  year={2017},
  volume={},
  number={},
  pages={949-956},
  doi={10.1109/ICDMW.2017.130}}

@inproceedings{chen13crowdsourcing, author = {Chen, Xi and Bennett, Paul N. and Collins-Thompson, Kevyn and Horvitz, Eric}, title = {Pairwise Ranking Aggregation in a Crowdsourced Setting}, year = {2013}, isbn = {9781450318693}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2433396.2433420}, doi = {10.1145/2433396.2433420}, abstract = {Inferring rankings over elements of a set of objects, such as documents or images, is a key learning problem for such important applications as Web search and recommender systems. Crowdsourcing services provide an inexpensive and efficient means to acquire preferences over objects via labeling by sets of annotators. We propose a new model to predict a gold-standard ranking that hinges on combining pairwise comparisons via crowdsourcing. In contrast to traditional ranking aggregation methods, the approach learns about and folds into consideration the quality of contributions of each annotator. In addition, we minimize the cost of assessment by introducing a generalization of the traditional active learning scenario to jointly select the annotator and pair to assess while taking into account the annotator quality, the uncertainty over ordering of the pair, and the current model uncertainty. We formalize this as an active learning strategy that incorporates an exploration-exploitation tradeoff and implement it using an efficient online Bayesian updating scheme. Using simulated and real-world data, we demonstrate that the active learning strategy achieves significant reductions in labeling cost while maintaining accuracy.}, booktitle = {Proceedings of the Sixth ACM International Conference on Web Search and Data Mining}, pages = {193-202}, numpages = {10}, keywords = {crowdsourcing, ranking, pairwise preference}, location = {Rome, Italy}, series = {WSDM '13} }

@article{Chernoff1952AMO,
  title={A Measure of Asymptotic Efficiency for Tests of a Hypothesis Based on the sum of Observations},
  author={Herman Chernoff},
  journal={Annals of Mathematical Statistics},
  year={1952},
  volume={23},
  pages={493-507}
}

@article{latala2006estimates,
  title={Estimates of moments and tails of Gaussian chaoses},
  author={Lata{\l}a, Rafa{\l}},
  year={2006}
}

@book{wainwright_2019, place={Cambridge}, series={Cambridge Series in Statistical and Probabilistic Mathematics}, title={High-Dimensional Statistics: A Non-Asymptotic Viewpoint}, DOI={10.1017/9781108627771}, publisher={Cambridge University Press}, author={Wainwright, Martin J.}, year={2019}, collection={Cambridge Series in Statistical and Probabilistic Mathematics}}

@inproceedings{
ji2023sample,
title={Sample Complexity of Nonparametric Off-Policy Evaluation on Low-Dimensional Manifolds using Deep Networks},
author={Xiang Ji and Minshuo Chen and Mengdi Wang and Tuo Zhao},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=9x3CO0ZU9LR}
}