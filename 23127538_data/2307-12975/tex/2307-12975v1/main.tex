\documentclass[english]{article}
\usepackage{geometry}
\geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{bm}
\usepackage{amsmath,mathtools}
\usepackage{amssymb}
\usepackage{natbib}
\usepackage[unicode=true,bookmarks=false,breaklinks=false,pdfborder={0 0 1},colorlinks=false]{hyperref}
\hypersetup{colorlinks,citecolor=blue,filecolor=blue,linkcolor=blue,urlcolor=blue}

\setlength{\parindent}{0em}
\setlength{\parskip}{1em}
\usepackage{smile}

\title{\textbf{Provable Benefits of Policy Learning from Human
Preferences in Contextual Bandit Problems}}
\author{Xiang Ji\footnote{Department of Electrical and Computer Engineering, School of Engineering and Applied Science, Princeton University, Princeton, NJ 08544, USA.} \hspace{0.37in} Huazheng Wang\footnote{School of Electrical Engineering and Computer Science, College of Engineering, Oregon State University, Corvallis, Oregon, OR 97331, USA.} \hspace{0.37in} Minshuo Chen$^*$ \hspace{0.37in} Tuo Zhao\footnote{H. Milton Stewart School of Industrial and Systems Engineering, College of Engineering, Georgia Institute of Technology, Atlanta, GA 30332, USA.} \hspace{0.37in} Mengdi Wang$^*$
}

\date{}

\begin{document}


\maketitle


\begin{abstract}
  A crucial task in decision-making problems is reward engineering. It is common in practice that no obvious choice of reward function exists. Thus, a popular approach is to introduce human feedback during training and leverage such feedback to learn a reward function. Among all policy learning methods that use human feedback, preference-based methods have demonstrated substantial success in recent empirical applications such as InstructGPT. In this work, we develop a theory that provably shows the benefits of preference-based methods in offline contextual bandits. In particular, we improve the modeling and suboptimality analysis for running policy learning methods on human-scored samples directly. Then, we compare it with the suboptimality guarantees of preference-based methods and show that preference-based methods enjoy lower suboptimality.
\end{abstract}

\allowdisplaybreaks
\input{chapters/intro}

\input{chapters/prelim}

\input{chapters/formulation}

\input{chapters/main_results}

\input{chapters/conclusion}

\bibliography{main}
\bibliographystyle{apalike}

\newpage

\appendix

\input{chapters/appendix}

\end{document}