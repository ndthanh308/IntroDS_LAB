\section{Human Rating Models}\label{sec:formulation}

As evidenced in behavioral studies, human ratings are subject to both bias and uncertainty \citep{Janowski15accuracy,li2020simple}. Human annotators may often exhibit personal opinion bias when rating particular subjects, leading to deviations from the true score. Furthermore, due to the tedious and challenging nature of the rating process, the evaluation of the same subject by a human annotator can fluctuate, giving rise to what is known as intra-observer uncertainty. In light of these phenomena, \citet{Janowski15accuracy} propose a model that aims to characterize the behavior of a human annotator during rating, which has been widely adopted in the existing literature \citep{Ortiz20unified,li2017recover,li20crowdsourcing}. In the following, we present this model for the single annotator case in the language of contextual bandits. For any fixed state-action pair $(s,a) \in \cS\times\cA$ with true reward $r(s,a)$, the rating from human annotator $\widetilde{r}(s,a)$ can be written as
\begin{align}\label{eq:rating-existing-models}
    \widetilde{r}(s,a) = r(s,a) + \Delta_{(s,a)} + \epsilon.
\end{align}
Here, $\Delta_{(s,a)}$ represents the bias of human annotator for action $a$ at state $s$. The downstream policy learning algorithm is assumed to have no prior knowledge of such bias when using human ratings as reward. In most works \citep{Janowski15accuracy,li2020simple}, $\Delta_{(s,a)}$ is modeled as an unknown constant; in \citet{Ortiz20unified}, $\Delta_{(s,a)}$ is a Gaussian random variable with an unknown, non-zero mean. $\epsilon$ is a random noise representing the uncertainty of human annotator, which is modeled with a zero-mean Gaussian distribution and i.i.d. on every observation.

Apparently such a human rating model bears several limitations. According to \eqref{eq:rating-existing-models}, the bias $\Delta_{(s,a)}$ has a non-zero expectation, which prevents from exactly recovery of the true reward $r(s,a)$. Consequently, identifying the optimal action for a given state $s$ becomes infeasible when the bias causes a flip in expected ratings, i.e, $\EE[\widetilde{r}(s,a)] > \EE[\widetilde{r}(s,\pi^\star(s))]$ for any $a \notin \pi^\star(s)$, and the policy learning becomes inconsistent regardless based on human ratings or preferences. Therefore, the bias term considered in such a human rating model is too crude for policy learning to be meaningful and actually does not reflect the reality. On the other hand, the uncertainty is only modeled with an additive Gaussian random variable in the existing literature. In practice, such uncertainty noise can be higher-order and more complex. The uncertainty might also take influence from the bias and the true reward $r(s,a)$ and have an intricate role in the final score $\widetilde{r}(s,a)$. The existing results based on \eqref{eq:rating-existing-models} can be too restrictive to reflect the human uncertainty in practice.

To improve on the existing literature, we propose a new human rating model, under which the human rating $\widetilde{r}(s,a)$ can be written as
\begin{equation}\label{eq:rating-h-model}
    \widetilde{r}(s,a) = h(r(s,a),\epsilon),
\end{equation}
where $h$ is some transformation function and $\epsilon$ is a random noise sampled from $\cN(0,\sigma^2)$ and independent from $(s,a)$. For notational simplicity, we denote $\bar{h}(r)=\EE_{\epsilon}[h(r,\epsilon)]$, which can be interpreted as the expected bias of human annotator towards a subject with true reward $r$ without the effect of uncertainty.

We assume that $\widetilde{r}(s,a)$ satisfies the following conditions:

\textbf{Condition 1.} For any $r_1,r_2\in[0,R]$ such that $r_1 > r_2$, the function $\bar{h}(\cdot)$ satisfies
\begin{align*}
\bar{h}(r_1), \bar{h}(r_2) \in [0,R]\quad\textrm{and}\quad\bar{h}(r_1) > \bar{h}(r_2).
\end{align*}

This condition assumes that $\bar{h}(\cdot)$ is strictly monotone, implying that the human ratings should preserve the ranking of the true reward $r(s,a)$ in expectation. This condition is particularly important, as it ensures that the consistency of the policy learning problem. Therefore, we can always identify the optimal policy based on human rating data in expectation.

\begin{remark}
The monotonicity in Condition 1 also guarantees that any group of samples can be correctly compared and ranked in expectation, which is a sufficient condition for the use of preference-based methods. This unifies the admissibility assumption in the rating models and preference models, which is crucial for the validity of our subsequent theoretical comparison between the two types of methods.
\end{remark}
%The next condition imposes assumptions on both $h$ and $\epsilon$.

\textbf{Condition 2.} For any $r\in[0,R]$, $h(r,\epsilon)$ is a degree-$q$ polynomial in $\epsilon$ and symmetric in $\epsilon$ about its expectation, i.e., $$-(h(r,\epsilon) - \EE_{\epsilon}[h(r,\epsilon)]) \overset{d}{=} h(r,\epsilon') - \EE_{\epsilon'}[h(r,\epsilon')],$$ where $\epsilon'$ is a random variable identically distributed as $\epsilon$.

Since $h(r,\cdot)$ can be a very general function, the human uncertainty in the final rating $\widetilde{r}(s,a)$ is allowed to have a complicated dependency on the bias and the true reward, even if $\epsilon$ is only an independent Gaussian random noise. For instance, the uncertainty can end up amplified or reshaped by the bias and true reward from the original white noise $\epsilon$ and produce a complex concentration around $\bar{h}(r)$. This not only provides more flexibility and but also presents a greater theoretically challenge compared to the uncertainty considered in \eqref{eq:rating-existing-models}, which is modeled with a simple Gaussian noise added to the true reward and bias.

\begin{remark}
Condition 2 is supposed to regulate the effect of uncertainty only---the uncertainty does not favor any particular direction (though the bias still can). This reflects phenomena in the real world, where the random noise concentrates evenly around the expectation, and the effect of uncertainty is diminished in expectation.
\end{remark}

\textbf{Condition 3.} For any $r_1,r_2\in[0,R]$ such that $r_1 \ge r_2$, there are positive constants $C_{h,1},C_{h,2} > 0$ such that
\begin{align*}
\bar{h}^{-1}(r_1) - \bar{h}^{-1}(r_2) \le C_{h,1}\cdot \bar{h}^{-1}(r_1-r_2)\quad\textrm{and} \quad\bar{h}(r_1) -\bar{h}(r_2) \le C_{h,2}\cdot \bar{h}(r_1 - r_2).
\end{align*}

This is a technical condition on the regularity of the expected bias function. It ensures that the bias does not transform the true reward too drastically, which eases our theoretical analysis.

Lastly, we denote the class of all $h(\cdot,\cdot)$ functions satisfying the three aforementioned conditions with $\cH$. We remark that our proposed human rating model encompasses any prior model in the form of \eqref{eq:rating-existing-models} that admits a consistent policy learning problem.

We next provide a concrete example for the $h$-function satisfying our model, which will be studied in our theoretical analysis later.

\textbf{Example of Human Rating.} Consider a setting in which the true reward function satisfies $r(s,a)\in[0,1]$ for all $s\in\cS$ and $a\in\cA$. For any $r$, 
\begin{equation}\label{eq:h-example}
    h(r,\epsilon) = r^2 + r^2\epsilon|\epsilon|\quad\textrm{and}\quad\bar{h}(r) = r^2.
\end{equation}

This specific rating model has an interpretation that aligns with how human annotators score in practice. Many human annotators with strong opinion bias can exhibit an extreme taste, which makes them tend to rate subjects with low true reward even lower and rate those with high true reward even higher in expectation. This is captured by the quadratic form of $\bar{h}(\cdot)$. On the other hand, when the white noise $\epsilon$ has a large magnitude, the impact of uncertainty on the final rating should be even larger and more drastic. Since annotators tend to be more deliberate yet more uncertain about subjects with large true reward, the fluctuation of the final rating should also increase with the magnitude of the true reward of a subject. Note that the uncertainty $r^2\epsilon|\epsilon|$ is a subexponential random variable and actually not a polynomial of $\epsilon$; however, our theory still applies, since the tail of $r^2\epsilon|\epsilon|$ is dominated by the quadratic $r^2\epsilon^2$. More generally, our theory is applicable to any uncertainty noise whose tail is dominated by a degree-$q$ polynomial of $\epsilon$. Furthermore, one can easily check that this $h$-function satisfies all three conditions for $\cH$. 
