\section{Results for Human Rating}

In this section, we establish the theoretical lower bounds and upper bound on the suboptimality of running the standard LCB algorithm under our rating model. Later, these results will be compared against the suboptimality results of preference-based methods. 

In the case of human rating, we are given an offline dataset $\cD = \{(s_i,a_i, \widetilde{r}_i)\}_{i=1}^{n}$. The state-action pairs in $\cD$ are generated in an i.i.d. fashion according to a sampling distribution over the state-action space. The sampling probability of the state-action pair $(s,a)$ is denoted with $d(s,a)$. For each $(s_i,a_i)$, the human annotator provides a rating $\widetilde{r}_i$ based on the rating model \eqref{eq:rating-h-model} and the true reward $r(s_i,a_i)$.

Let us also make a brief review of the standard LCB approach for offline policy learning \citep{jin2021pessimism,rashidinejad2021bridging,yin2021towards}. In the existing works, it is common to assume the knowledge of a reasonable upper bound on the variance of reward observations. Similarly, we assume there exists an upper bound on the variance $\Var_\epsilon(h(r,\epsilon))$ for all $r\in[0,R]$, which we denote with $V_{R, \sigma}^2$ and can depend on $R$ and $\sigma$. Let us assume the learner can make a reasonable estimate $\widetilde{V}_{R,\sigma}^2$ for the true variance $V_{R, \sigma}^2$, such that $\widetilde{V}_{R, \sigma}^2 = c_VV_{R, \sigma}^2$, where $c_V > 0$ is a universal constant independent from $R$ and $\sigma$. Recall that the learner is oblivious to the bias transformation $h$. Thus, in order to learn the optimal policy with at least $1-\delta$ success probability, the standard LCB algorithm (Algorithm \ref{alg:LCB}) uses a penalty in the form of
\begin{equation}\label{eq:standard-LCB}
    b_m = c_b \sqrt{\frac{\widetilde{V}_{R,\sigma}^2\log\frac{SA}{\delta}}{m}}
\end{equation}
with an appropriately chosen $c_b$. 

To understand how conventional methods perform under our more realistic rating model, let us first establish lower bounds in two scenarios with different assumptions on the coverage of the offline dataset $\cD$.

\begin{algorithm}[t]
    \caption{LCB for contextual bandits} \label{alg:LCB}
    \begin{algorithmic}[1]
        \STATE \textbf{Input:} Offline dataset $\cD$, confidence level $\delta\in(0,1)$.

        \FOR{all $(s,a)\in\cS\times\cA$}
            \STATE Set $n_{(s,a)} = \sum_{i=1}^{n}\ind\{(s_i,a_i) = (s,a)\}$;

            \STATE Set $\widetilde{r}(s,a) = \frac{1}{n}\sum_{i=1}^{n} \widetilde{r}_{i}\ind\{(s_i,a_i) = (s,a)\}$;

            \STATE Set $\widehat{r}(s,a) = \max\{\widetilde{r}(s,a) - b_{n_{(s,a)}}, 0\}$;
        \ENDFOR

        \RETURN $\widehat{\pi}_{\rm LCB}(\cdot) = \arg\max_{a\in\cA}\widehat{r}(\cdot,a)$.
    \end{algorithmic}
\end{algorithm}

\subsection{Lower Bound with Bounded $C^\star$}

As \citet{rashidinejad2021bridging,yin2021towards} have shown, to learn the optimal policy in the offline setting, it is sufficient for the sampling distribution of the offline dataset to cover the state-action pairs that the optimal policy can reach. Concretely, this assumption can be written as follows:
\begin{assumption}\label{assumption:Cstar}
    There exists an optimal policy $\pi^\star$ such that $d(s,a) > 0$ whenever $d^{\pi^\star}_\rho(s,a) > 0$ for any $(s,a)\in\cS\times\cA$. 
\end{assumption}
Under this assumption, it makes sense to define a concentrability coefficient $C^\star$ as follows:
\begin{equation}
    C^\star := \max_{(s,a)\in\cX}\frac{d^{\pi^\star}_\rho(s,a)}{d(s,a)},
\end{equation}
where the set $\cX$ is the set of all state-action pairs that the sampling distribution of $\cD$ can cover, i.e., $\cX := \{(s,a)\in\cS\times\cA ~:~ d(s,a)> 0\}$. Under Assumption \ref{assumption:Cstar}, if the reward can be observed exactly or with only additive subgaussian noise, the standard LCB algorithm (Algorithm \ref{alg:LCB}) with penalty \eqref{eq:standard-LCB} is guaranteed to converge to the optimal policy \citep{rashidinejad2021bridging,yin2021towards}. However, this is not the case when the reward function is engineered from human rating in general. In particular, let us consider the setting beyond the one with simple additive subgaussian noise, which has been well-studied in the existing literature. That is, let us consider a more realistic model in the form of \eqref{eq:rating-h-model} with $q \ge 2$. We can prove that even when the rating model preserves reward ordering like in \eqref{eq:rating-h-model}, it is possible that the standard LCB algorithm does not converge to the optimal policy and must suffer constant suboptimality.

\begin{theorem}\label{thm:Cstar}
    For any fixed constant $0 < \delta < 1$, if one runs Algorithm \ref{alg:LCB}, then there exists a contextual bandit instance with initial state distribution $\rho$ such that when we sample a dataset $\cD$ of size $n$ such that $n \ge c(\delta,c_b,c_V,q,\sigma,R)$, where $c(\delta,c_b,c_V,q,\sigma,R)$ is a constant depending on $\delta,c_b,c_V,q,\sigma,R$, using a sampling distribution $d$ satisfying Assumption \ref{assumption:Cstar} with $C^\star = \frac{1}{2}$, the output policy $\widehat{\pi}_{\rm LCB}$ suffers constant suboptimality, i.e., 
    \begin{equation}
        \EE_\cD[\SubOpt(\widehat{\pi}_{\rm LCB})] = c_0 R,
    \end{equation}
    where $c_0$ is a universal constant.
\end{theorem}

This result is reminiscent of Proposition 1 in \citet{rashidinejad2021bridging}, which constructs a bandit and shows the empirically best policy chooses a suboptimal action with constant probability under Assumption \ref{assumption:Cstar}. The same work also shows that by adding a pessimism penalty to the empirical reward estimation, the LCB algorithm (Algorithm \ref{alg:LCB}) can converge to the optimal policy under the same data coverage assumption. In contrast, our theorem shows that even when we make pessimistic estimates and penalize less-observed state-action pairs in a general human rating dataset, a constant suboptimality can still ensue. This can demonstrate a disadvantage of using human rating directly: although the estimation problem with human rating is still consistent, using LCB penalty with only the knowledge of variance is not sufficient for convergence. Instead, the learner needs to have more knowledge about the noise distribution, but it is unrealistic to model the human rating distribution accurately in practice.

\textbf{Proof sketch} In a bandit instance with special reward design, we first find the lower bound for the probability that suboptimal actions are only observed a very small number of times in the offline dataset. Such state-action pairs can have huge fluctuation in their empirical reward average and mislead the algorithm. Then, we find the lower bound on the probability that there exists a state-action pair $(s,a)$ such that $\widehat{r}(s,a) > \widehat{r}(s,a^\star)$, which can cause the algorithm to always select the suboptimal action $a$ and suffer suboptimality. Different from Proposition 1 in \citet{rashidinejad2021bridging}, in which the reward noise for suboptimal actions is defined with two Dirac delta functions, the noise under our rating model can be unbounded and can be viewed as a Gaussian chaos, so we compute this probability using a method from the corresponding literature. Moreover, in the prior paper, a bandit instance is sufficient to induce constant suboptimality as long as its action space is designed large. In our case, since the pessimism penalty in Algorithm \ref{alg:LCB} accounts for the bandit size and larger bandit instances are penalized more, it requires a careful balance in the design of our bandit instance.

For concreteness, let us also provide a corollary under the example rating model in \eqref{eq:h-example} as follows.

\begin{corollary}\label{cor:Cstar}
    For any fixed constant $0 < \delta < 1$, if one runs Algorithm \ref{alg:LCB}, then there exists a contextual bandit instance with initial state distribution $\rho$ such that when we sample a dataset $\cD$ of size $n$ such that $n \ge c(\delta,c_b,c_V,\sigma,R)$, where $c(\delta,c_b,c_V,\sigma,R)$ is a constant depending on $\delta,c_b,c_V,\sigma,R$, using a sampling distribution $d$ satisfying Assumption \ref{assumption:Cstar} with $C^\star = \frac{1}{2}$, the output policy $\widehat{\pi}_{\rm LCB}$ suffers constant suboptimality, i.e., 
    \begin{equation}
        \EE_\cD[\SubOpt(\widehat{\pi}_{\rm LCB})] = c_0,
    \end{equation}
    where $c_0$ is a universal constant.
\end{corollary}

\subsection{Lower Bound under Uniform Coverage}

Uniform coverage is another popular assumption on the sampling distribution of offline dataset $\cD$ \citep{yin2021near,uehara2021finite,hao2021bootstrapping}. It can be written as follows:
\begin{assumption}\label{assumption:uniform}
    The sampling distribution satisfies $d(s,a) > 0$ for any $(s,a)\in\cS\times\cA$. 
\end{assumption}
It is a much stronger one than Assumption \ref{assumption:Cstar} and can make the offline policy learning problem much easier. Under Assumption \ref{assumption:uniform}, many types of algorithms without the pessimism principle can be shown to provably converge to the optimal policy \citep{chen2019information,xie2020q}. Moreover, \citet{jin2021pessimism} have shown that the suboptimality of pessimistic algorithms can be proved to decay faster when the data sampling distribution is well-explored as such. In this setting, we establish a lower bound on the suboptimality of Algorithm \ref{alg:LCB} under Assumption \ref{assumption:uniform}. 

\begin{theorem}\label{thm:uniform}
    For any fixed constant $0 < \delta < 1$, if one runs Algorithm \ref{alg:LCB}, then there exists a contextual bandit instance with initial state distribution $\rho$ such that when we sample a dataset $\cD$ of size $n \ge \max\{48\sigma^4, 60\}$ using a sampling distribution $d$ satisfying Assumption \ref{assumption:uniform}, with $d(s,a) = \frac{1}{SA}$ for every $s\in\cS$ and $a\in\cA$, the output policy $\widehat{\pi}_{\rm LCB}$ suffers suboptimality at least
    \begin{equation*}
        \EE_\cD[\SubOpt(\widehat{\pi}_{\rm LCB})] = c_0\cdot \bar{h}^{-1}\left(\sqrt{\frac{V_{R,\sigma}^2}{n}}\right),
    \end{equation*}
    where $c_0$ is a constant that depends on $q$.
\end{theorem}

In fact, under a data coverage assumption as strong as Assumption \ref{assumption:uniform}, any knowledge of the data noise distribution becomes unimportant, and this result holds regardless of what penalty $b_n$ is used in the algorithm. This theorem demonstrates another disadvantage of using human rating directly: even when the data covers the entire state-action space and the impact of the noise distribution can be eliminated, its suboptimality is still bottlenecked by the expected bias. We also provide a corollary corresponding to the example model in \eqref{eq:h-example}, which shows the suboptimality can decay more slowly under the influence of annotator bias.

\begin{corollary}\label{cor:uniform}
    For any fixed constant $0 < \delta < 1$, if one runs Algorithm \ref{alg:LCB}, then there exists a contextual bandit instance with initial state distribution $\rho$ such that when we sample a dataset $\cD$ of size $n \ge c(R,\sigma,q)$ where $c(R,\sigma,q)$ is a constant depending on $R,\sigma,q$, using a sampling distribution $d$ satisfying Assumption \ref{assumption:uniform}, with $d(s,a) = \frac{1}{SA}$ for every $s\in\cS$ and $a\in\cA$, the output policy $\widehat{\pi}_{\rm LCB}$ suffers suboptimality at least
    \begin{equation*}
        \EE_\cD[\SubOpt(\widehat{\pi}_{\rm LCB})] = \frac{c_0\sigma}{n^{1/4}},
    \end{equation*}
    where $c_0$ is a universal constant.
\end{corollary}

\subsection{Upper Bound}

To compare with the lower bounds, we prove an upper bound on the suboptimality of Algorithm \ref{alg:LCB} in the most benign case that the learner has full knowledge of the rating model as well as its noise distribution and design the LCB penalty $b_n$ accordingly.

\begin{theorem}\label{thm:upper-bound}
    Suppose Assumption \ref{assumption:Cstar} holds. For any fixed constant $0 < \delta < 1$, if one runs Algorithm \ref{alg:LCB} with 
    \begin{equation*}
        b_m = c_b\sqrt{\frac{V^2_{R,\sigma}\log^q\frac{SA}{\delta}}{m}}
    \end{equation*}
    and an appropriately chosen universal constant $c_b$, as soon as $n > 8\log\frac{2SA}{\delta}/\bar{d}$, where $\bar{d} := \min_{(s,a)\in\cX}d(s,a)$ and $\cX := \{(s,a)\in\cS\times\cA ~:~ d(s,a)> 0\}$, with probability $1-\delta$, the suboptimality of the output policy $\widehat{\pi}_{\rm LCB}$ satisfies
    \begin{equation*}
        \SubOpt(\widehat{\pi}_{\rm LCB}) \le c_0\sum_{(s,a)\in\cX}d_\rho^{\pi^\star}(s,a)\cdot \bar{h}^{-1}\left(\sqrt{\frac{V_{R,\sigma}^2\log^q\frac{SA}{\delta}}{n\cdot d(s,a)}}\right),
    \end{equation*}
    where $c_0$ is a constant that depends on $q$.
\end{theorem}

This theorem shows that even when the algorithm has full knowledge of the rating model---the annotator uncertainty in particular, the annotator bias can still influence the suboptimality of $\widehat{\pi}_{\rm LCB}$ negatively. It demonstrates the effect of bias on the suboptimality is truly unavoidable when using human rating directly. This can be further illustrated with our example model \eqref{eq:h-example} as follows, which shows the suboptimality still decays more slowly because of the quadratic annotator bias.

\begin{corollary}\label{cor:upper-bound}
    Suppose Assumption \ref{assumption:Cstar} holds. For any fixed constant $0 < \delta < 1$, if one runs Algorithm \ref{alg:LCB} with 
    \begin{equation*}
        b_m = \sqrt{\frac{64\sigma^4\log\frac{SA}{\delta}}{m}} + \frac{8\sigma^2\log\frac{SA}{\delta}}{m},
    \end{equation*}
    as soon as $n > 8\log\frac{2SA}{\delta}/\bar{d}$, where $\bar{d} := \min_{(s,a)\in\cX}d(s,a)$ and $\cX := \{(s,a)\in\cS\times\cA ~:~ d(s,a)> 0\}$, with probability $1-\delta$, the suboptimality of the output policy $\widehat{\pi}_{\rm LCB}$ satisfies
    \begin{equation*}
        \SubOpt(\widehat{\pi}_{\rm LCB}) \le 2\sum_{(s,a)\in\cX}d_\rho^{\pi^\star}(s,a)\left(\frac{256\sigma^4\log\frac{SA}{\delta}}{n\cdot d(s,a)}\right)^{1/4} + d_\rho^{\pi^\star}(s,a)\sqrt{\frac{32\sigma^2\log\frac{SA}{\delta}}{n\cdot d(s,a)}}.
    \end{equation*}
\end{corollary}

\section{Comparison with Preference-based Methods}

In this section, we compare the results for human rating with the suboptimality results for pessimistic MLE, a preference-based policy learning method introduced in \citet{zhu2023principled}. This allows us to provably demonstrate the advantage of preference-based methods over human rating. 

In contrast to rating methods, preference-based methods rely on models that characterize how an annotator would rank a group of subjects by reward. Let us consider the most basic case of pairwise comparison, which models the ranking between a pair of subjects based on their rewards. This is predominantly modeled with the Bradley-Terry-Luce (BTL) model \citep{Bradley1952RankAO}, under which human annotator gives a binary response $y = \{0,1\}$ when asked to compare two state-action pairs $(s,a^0)$ and $(s,a^1)$ with $a^0 \neq a^1$. The preference response $y$ is i.i.d. for every comparison and follows a Bernoulli distribution as follows:
\begin{equation}\label{eq:BTL}
    P(y|s,a,a') = \frac{\exp(r(s,a^y))}{\exp(r(s,a^0))+\exp(r(s,a^1))}.
\end{equation} 

Just like our rating model in \eqref{eq:rating-h-model}, the BTL model admits a consistent statistical problem. However, different from the rating case, the learner is given a dataset $\cD' = \{(s_i,a_i^0,a_i^1)\}_{i=1}^{n}$, in which the samples are i.i.d. state-action-action triples from some sampling distribution. We denote the sampling probability of the triple $(s,a^0,a^1)$ with $d(s,a^0,a^1)$.

\begin{algorithm}[t]
    \caption{Pessimistic MLE for contextual bandits} \label{alg:LCB-comp}
    \begin{algorithmic}[1]
        \STATE \textbf{Input:} Offline dataset $\cD'$, confidence level $\delta\in(0,1)$.

        \STATE Construct the reward function set
        \begin{equation*}
            \cF := \{v\in\RR^{SA} :\mathbf{1}^\top v = 0, \norm{v}_\infty \le R\};
        \end{equation*}
    
        \STATE Set 
        \begin{equation*}
            \widetilde{r} = \arg\max_{f\in\cF}\sum_{i=1}^{n}\log\left(\frac{\ind\{y_i=1\}\exp(f(s_i,a_{i}^{1}))}{\exp(f(s_i,a_{i}^{0})) + \exp(f(s_i,a_{i}^{1}))} + \frac{\ind\{y_i=0\}\exp(f(s_i,a_{i}^{0}))}{\exp(f(s_i,a_{i}^{0})) + \exp(f(s_i,a_{i}^{1}))}\right);
        \end{equation*}

        \STATE Construct empirical covariance matrix
        \begin{equation*}
            \widehat{\Sigma} = \frac{1}{n}\sum_{i=1}^n \left(\mathbf{1}_{(s_i,a_{i}^{0})} - \mathbf{1}_{(s_i,a_{i}^{1})}\right)\left(\mathbf{1}_{(s_i,a_{i}^{0})} - \mathbf{1}_{(s_i,a_{i}^{1})}\right)^\top;
        \end{equation*}

        \STATE Construct the pessimistic reward function set
        \begin{equation*}
            \cF_{\mathrm{CR}}(\widetilde{r}) = \left\{f\in \cF ~:~ \sqrt{(f - \widetilde{r})^\top \widehat{\Sigma}(f - \widetilde{r})} \le b_n'\right\};
        \end{equation*}
        
        \RETURN $\widehat{\pi}_{\rm PMLE} = \arg\max_{\pi}\min_{\widehat{r}\in\cF_{\mathrm{CR}}(\widetilde{r})}\EE_{s\sim\rho}[\widehat{r}(s,\pi(s))]$. \label{alg-line:comp-r-hat}
    \end{algorithmic}
\end{algorithm}

To find the optimal policy with human preference data, \citet{zhu2023principled} propose pessimistic MLE (Algorithm \ref{alg:LCB-comp}). The pessimistic MLE algorithm shares a similar philosophy of the LCB algorithm, which involves estimating the reward function in a pessimistic fashion and finding the greedy policy with respect to it. The assumption on the sampling distribution of the offline dataset is similar to Assumption \ref{assumption:Cstar}, which requires the sampling distribution to covers the state-actions pairs that optimal policy can reach. In the tabular case, this assumption can be written as follows:

\begin{assumption}\label{assumption:Cstar-comp}
    There exists an optimal policy $\pi^\star$ such that the pairwise concentrability coefficient 
    \begin{equation}\label{eq:C-dagger}
        C^{\dagger} := \sqrt{\sup_{v\in[-1,1]^{SA}:\mathbf{1}^\top v = 0}\frac{\Big(\sum_{(s,a)} d_\rho^{\pi^\star}(s,a) v(s,a)\Big)^2}{\sum_{(s,a^0,a^1)} d(s,a^0, a^1) \big(v(s,a^0) - v(s,a^1)\big)^2}}
    \end{equation}
    is bounded.  
\end{assumption}

\citet{zhu2023principled} prove the convergence of pessimistic MLE in the linear bandit setting. In the following theorem, which is a special case of Theorem 3.2 from \citet{zhu2023principled}, we show that pessimistic MLE can provably converge to the optimal policy under the mild data coverage assumption of Assumption \ref{assumption:Cstar-comp} and its suboptimality decays at a fast rate of $O(1/\sqrt{n})$. In particular, we characterize the concentrability coefficient specific to the sampling distribution and the tabular setting. This result marks a clear distinction from the negative results for human rating. 

\begin{theorem}\label{thm:comp}
    Denote $\gamma = \frac{1}{2+\exp(R\sqrt{SA})+\exp(-R\sqrt{SA})}$. Suppose Assumption \ref{assumption:Cstar-comp} holds. For any fixed constant $0 < \delta < 1$, if one runs Algorithm \ref{alg:LCB-comp} with 
    \begin{equation*}
        b_m' = c_b'\sqrt{\frac{SA + \log\frac{1}{\delta}}{\gamma^2 m}},
    \end{equation*}
    where $$\gamma = \frac{1}{2+\exp(R\sqrt{SA})+\exp(-R\sqrt{SA})}$$ and $c_b'$ is an appropriately chosen universal constant, with probability $1-\delta$, the suboptimality of the output policy $\widehat{\pi}_{\rm PMLE}$ satisfies
    \begin{equation*}
        \SubOpt(\widehat{\pi}_{\rm PMLE}) \le c_0C^\dagger R\left(\sqrt{\frac{SA + \log\frac{1}{\delta}}{\gamma^2 n}} + \sqrt{\frac{S^2A^2\log\frac{n}{\delta}}{n}}\right),
    \end{equation*}
    where $c_0$ is a universal constant.
\end{theorem}
