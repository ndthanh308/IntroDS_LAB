\section{Additional Notation}

Let $f:\RR\to\RR$ and $g:\RR\to\RR$ be two functions. We denote the their composition $f(g(\cdot))$ with $(f\circ g)(\cdot)$. We use $\cN(\mu,\sigma^2)$ to denote a Gaussian distribution with mean $\mu$ and variance $\sigma^2$. For a probability event $\cE$, we denote its complement event with $\cE^C$. For a vector $v$, $\norm{v}_2$ denotes the $\ell_{2}$-norm of the vector $v$. For a positive semidefinite matrix $A$, $\norm{v}_{A}$ denotes a semi-norm of the vector $v$ with respect to the matrix $A$ with $\norm{v}_{A} = \sqrt{v^\top A v}$. We denote the set of all positive integers with $\ZZ_{>0}$. 

\section{Supporting Lemmas}
\iffalse
\begin{proposition}\label{prop:e}
    For any integer $n \ge 1$ and constants $0 < c_1 < 1$, 
    \begin{equation*}
        \left(1 - \frac{c_1}{n}\right)^n \le e^{-c_1}.
    \end{equation*}
\end{proposition}

\begin{proof}[Proof of Proposition \ref{prop:e}]
    For any real $x \ge 1$, $(1 - \frac{c_1}{x})^x$ is increasing in $x$. \textcolor{red}{prove}

    Since we know 
    \begin{equation*}
        \lim_{x\to\infty}\left(1 - \frac{1}{x}\right)^x = \frac{1}{e},
    \end{equation*}
    we can apply a change of variable $y = \frac{x}{c_1}$ and conclude
    \begin{align*}
        &\quad\ \lim_{x\to\infty}\left(1 - \frac{c_1}{x}\right)^x\\
        &= \lim_{y\to\infty}\left(1 - \frac{1}{y}\right)^{c_1y}\\
        &= \frac{1}{e^{c_1}}.
    \end{align*}
\end{proof}
\fi

\begin{lemma}\label{lem:bernstein}
    Given $X_1, \cdots, X_n$ independent sub-exponential random variables, each $X_i$ with parameters $(\tau_i^2,\alpha_i)$. Define
    \begin{equation*}
        \tau_*^2 := \sum_{i=1}^n \tau_i^2 \quad \text{and} \quad \alpha_* := \max_i \alpha_i.
    \end{equation*}
    It holds that
    \begin{align*}
        \PP\left[\sum_{i=1}^n (X_i - \EE[X_i]) \ge t\right] \le \begin{cases}
        \exp\left(-\frac{t^2}{2\tau_*^2}\right), & \text{if } 0 < t < \frac{\tau_*^2}{\alpha_*};\\
        \exp\left(-\frac{t}{2\alpha_*}\right), & \text{if } t \ge \frac{\tau_*^2}{\alpha_*}.
        \end{cases}
    \end{align*}
\end{lemma}

The following lemma is modified from Lemma B.1 in \citet{yin2021towards}. Recall $n_{(s,a)}$ denotes the number of samples in the offline dataset that visits $(s,a)$. This lemma is about the concentration of $n_{(s,a)}$, the number of samples in the offline dataset that visits $(s,a)$.

\begin{lemma}\label{lem:empirical-n}
    Given a dataset $\cD$ with $n$ i.i.d. samples $\cD= \{(s_i,a_i)\}_{i=1}^n$ from a sampling distribution $d$, let $d(s,a)$ be the probability $(s,a)$ is sampled from the outcome space $\cS\times\cA$ and $n_{(s,a)}$ be the number of samples in $\cD$ for $(s,a)$. For any $\delta \in (0,1)$, the event 
    \begin{equation}
        d(s,a)\frac{n}{2} \le n_{(s,a)} \le d(s,a)\frac{3n}{2}
    \end{equation}
    holds simultaneously for all $(s,a)\in\cS\times\cA$ with probability $1-\delta$, as soon as $n > 8SA\log\frac{2SA}{\delta}/\bar{d}$, where $\bar{d} := \min_{(s,a)\in\cX}d(s,a)$ and $\cX := \{(s,a)\in\cS\times\cA ~:~ d(s,a)> 0\}$
\end{lemma}

\begin{proof}
    First, consider the event $d(s,a)\frac{n}{2} > n_{(s,a)}$ for some fixed $(s,a)\in\cS\times\cA$.

    Note that we can view $n_{(s,a)}$ as a sum of independent Bernoulli variables, i.e., $n_{(s,a)} = \sum_{i=1}^{n}\ind\{(s_i,a_i) = (s,a)\}$, so $n_{(s,a)}$ follows a binomial distribution with parameters $(d(s,a),n)$. Recall the multiplicative Chernoff bound on a binomial random variable:
    \begin{lemma}[Multiplicative Chernoff bound \citep{Chernoff1952AMO}]\label{lem:chernoff}
        Let $X$ be a Binomial random variable with parameters $(p,n)$. Denote its mean with $\mu$. For any $\epsilon\in(0,1]$, we have 
        \begin{equation}\label{eq:chernoff1}
            \PP\left[X < (1-\epsilon)\mu\right] < e^{-\frac{\epsilon^2\mu}{2}}
        \end{equation}
        and 
        \begin{equation}\label{eq:chernoff2}
            \PP\left[X \ge (1+\epsilon)\mu\right] < e^{-\frac{\epsilon^2\mu}{3}}.
        \end{equation}
    \end{lemma}
    Take $\epsilon = \frac{1}{2}$. \eqref{eq:chernoff1} suggests $d(s,a)\frac{n}{2} > n_{(s,a)}$ holds with probability at most $e^{-nd(s,a)/8}$. Similarly, taking $\epsilon = \frac{1}{2}$ in \eqref{eq:chernoff2} suggests $n_{(s,a)} > d(s,a)\frac{3n}{2}$ holds with probability at most $e^{-nd(s,a)/12}$. Taking the union bound on these two events, we have $d(s,a)\frac{n}{2} > n_{(s,a)}$ or $n_{(s,a)} > d(s,a)\frac{3n}{2}$ with probability at most $2e^{-n\bar{d}/8}$ for this fixed $(s,a)$.

    Then, we take a union bound over all $(s,a)\in \cS\times\cA$, which can give us the advertised result.
\end{proof}

Moreover, since the uncertainty in our rating model in \ref{eq:rating-h-model} can end up with a complex concentration, we need a lemma that can give the upper and lower tail bounds of Gaussian chaos.
\begin{lemma}[Corollary 1 in \citep{latala2006estimates}]\label{lem:gaussian-chaos}
    Let $X$ be a zero-mean Gaussian random variable, and let $f:\RR\to\RR$ be a polynomial for degree $q \in \ZZ_{>0}$. Then
    \begin{equation}\label{eq:gaussian-chaos-lower}
        \PP\left[|f(X) - \EE[f(X)]| \ge t\right] \ge c_{q}\exp\left(-\left(\frac{t^2}{c_{1}\Var(f(X))}\right)^{1/q}\right)
    \end{equation}
    and
    \begin{equation}\label{eq:gaussian-chaos-upper}
        \PP\left[|f(X) - \EE[f(X)]| \ge t\right] \le C_{q}\exp\left(-\left(\frac{t^2}{C_{1}\Var(f(X))}\right)^{1/q}\right),
    \end{equation}
    where $c_{1}, C_{1} > 0$ are absolute constants and $c_{q}, C_{q} > 0$ are absolute constants depending on $q$.
\end{lemma}

\begin{proposition}\label{prop:eps-sub}
    Let $\epsilon\sim\cN(0,\sigma^2)$. $\epsilon|\epsilon|$ is a sub-exponential random variable with parameters $(4\sigma^4,4\sigma^2)$.
\end{proposition}

\begin{proof}
    Let $X = Z^2$, where $Z\sim\cN(0,1)$. For $0 < \lambda < \frac{1}{2}$, 
    \begin{align*}
        \EE[e^{\lambda (X-1)/2}] = (1-2\lambda)^{-1/2}e^{-\lambda} = e^{-\frac{1}{2}\log(1-2\lambda)-\lambda} \ge e^{\lambda^2}
    \end{align*}
    and by \citet{wainwright_2019}, for $0 < \lambda \le \frac{1}{4}$, 
    \begin{align*}
        \EE[e^{\lambda (X-1)/2}] \le e^{2\lambda^2}.
    \end{align*}

    Thus, let $Y = (\sigma Z)^2$. For $0 < \lambda < \frac{1}{2\sigma^2}$, 
    \begin{align*}
        \EE[e^{\lambda (Y-\sigma^2)/2}] = e^{-\frac{1}{2}\log(1-2\lambda\sigma^2)-\lambda\sigma^2} \ge e^{\lambda^2\sigma^4}
    \end{align*}
    and by \citet{wainwright_2019}, for $0 < \lambda \le \frac{1}{4\sigma^2}$, 
    \begin{align*}
        \EE[e^{\lambda (Y-\sigma^2)/2}] \le e^{2\lambda^2\sigma^4}.
    \end{align*}
\end{proof}

\section{Proofs for human rating under Assumption \ref{assumption:Cstar}}

\subsection{Proof of Theorem \ref{thm:Cstar}}

\begin{proof}
To prove this theorem, it suffices to construct a bandit instance with $\cS = \{s\}$ and $\cA = \{a_1, \cdots, a_{A}\}$, where $A = n^2$. Let the true reward function $r$ be defined such that two conditions are satisfied: (i) let $r(s,a_1) = r(s,a_i) + c(R,\sigma,q)\bar{h}^{-1}\left(V_{R,\sigma}\right)$ for any $a_i$ such that $i=2,\cdots,A$, where $c(R,\sigma,q)$ is a constant that might depend on $R,q,\sigma$ and makes sure that $c(R,\sigma,q)\bar{h}^{-1}\left(V_{R,\sigma}\right) = cR$ for some absolute constant $c > 0$ and $r \in [0,R]^{SA}$; (ii) let $\Var\Big(h(r(s,a_i),\epsilon) - \bar{h}(r(s,a_i))\Big) = c(q)V_{R,\sigma}^2$ for any $a_i$ such that $i=2,\cdots,A$, where $c(q)$ is a constant depending on $q$. We can select $c(R,\sigma,q)$ and $c(q)$ so that such a reward function $r$ exists. It can be observed that $a_1$ is the optimal action at $s$. %Note that such a reward function $r$ exists because $\bar{h}(\cdot)$ is monotone and both of its domain and codomain are $[0,R]$, by Condition 1 in Section \ref{sec:formulation}. 

For the sampling distribution $d$, let $d(s,a_1) = \frac{1}{2}$ and $d(s,a) = \frac{1}{2(A-1)}$ for any $a_i$ such that $i=2,\cdots,A$.

Consider the event that each of the suboptimal actions is observed less than once in the offline dataset, i.e., $n_{(s,a)} = 0$ or $n_{(s,a)} = 1$ for all $a_i$ such that $i=2,\cdots,A$. For convenience, let us denote this event with $\cE_1$. Conditioned on the total number of observations of suboptimal actions in the dataset, we can obtain the following via a simple counting argument 
\begin{align}\label{eq:proof:Cstar-E1-bound}
    \PP\left[\cE_1 ~\bigg|~ \sum_{i=2}^{A}n_{(s,a_i)}\right] = \frac{(A-1)(A-2)\cdot(A-\sum_{i=2}^{A}n_{(s,a_i)})}{(A-1)^{\sum_{i=2}^{A}n_{(s,a_i)}}} \ge \left(\frac{A-n}{A-1}\right)^{n}.
\end{align}
where the last inequality is because $(\frac{A-x}{A-1})^x$ is monotonically decreasing for $x\ge 1$ and the fact $n \ge \sum_{i=2}^{A}n_{(s,a_i)}$.

In addition, let us plug in $A=n^2$, and we can observe 
\begin{align*}
    \left(\frac{A-n}{A-1}\right)^{n} \ge \lim_{n\to\infty}\left(\frac{n^2-n}{n^2-1}\right)^{n} = \frac{1}{e}.
\end{align*}
Combining the two inequalities above, we can arrive at $\PP[\cE_1 ~|~ \sum_{i=2}^{A}n_{(s,a_i)}] \ge \frac{1}{e}$. Since this bound holds for any value of $\sum_{i=2}^{A}n_{(s,a_i)}$, we have $\PP[\cE_1] \ge \frac{1}{e}$. 

On the other hand, let us consider the event $n_{(s,a_1)} < 0.9n$ and denote it with $\cE_2$. Note that this event is equivalent to the event $\sum_{i=2}^{A} n_{(s,a_i)} \ge 0.1n$. Since $n_{(s,a_1)}$ follows the binomial distribution with parameters $(n,\frac{1}{2})$, by \eqref{eq:chernoff1} in Lemma \ref{lem:chernoff}, we have
%
\begin{equation}\label{eq:proof:Cstar-E2-bound}
    \PP\left[n_{(s,a_1)} < 0.9n\right] = 1 - \PP\left[n_{(s,a_1)} > 0.9n\right] \ge 1 - e^{-8n/75} \ge 0.99517
\end{equation}
as long as $n \ge 50$.

Now, we are ready to give a lower bound for the probability of the event that a suboptimal arm is finally selected by Algorithm \ref{alg:LCB}, i.e., there exists $a\neq a^\star$, $\widehat{r}(s,a) > \widehat{r}(s,a^\star)$. 

To this end, let us define a new event $\cE := \cE_1 \cap \cE_2$. The aforementioned probability can be decomposed as follows:
\begin{align*}
    &\quad \ \PP\Big[\exists a\neq a^\star, \widehat{r}(s,a) > \widehat{r}(s,a^\star)\Big]\\
    &= 1 - \PP\Big[\forall a\neq a^\star, \widehat{r}(s,a) \le \widehat{r}(s,a^\star)\Big]\\
    &= 1 - \left(\PP\Big[\cE \wedge \forall a\neq a^\star, \widehat{r}(s,a) \le \widehat{r}(s,a^\star)\Big] + \PP\Big[\cE^C \wedge \forall a\neq a^\star, \widehat{r}(s,a) \le \widehat{r}(s,a^\star)\Big]\right)\\
    &\ge 1 - \left(\PP\Big[\cE \wedge \forall a\neq a^\star, \widehat{r}(s,a) \le \widehat{r}(s,a^\star)\Big] + \PP\Big[\cE^C\Big]\right).
\end{align*}
Let us first focus on finding an upper bound for the probability of the intersection of $\cE$ and the event that the optimal arm is selected by the algorithm correctly, i.e., for all $a\neq a^\star$, $\widehat{r}(s,a) \le \widehat{r}(s,a^\star)$. 

Consider a fixed suboptimal action $a \neq a^\star$ with only one observation in the dataset, i.e., $n_{(s,a)} = 1$. We have 
\begin{align*}
    &\quad\ \PP\left[\widehat{r}(s,a) \le \widehat{r}(s,a^\star)\right]\\
    &= \PP\bigg[\Big(h(r(s,a),\epsilon) - \bar{h}(r(s,a))\Big) - \Big(\widetilde{r}(s,a^\star) - \bar{h}(r(s,a^\star))\Big)\\ & \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \le \bar{h}(r(s,a^\star)) - \bar{h}(r(s,a)) + b_1 - b_{n_{(s,a^\star)}}\bigg]\\
    &\le \PP\bigg[\Big(h(r(s,a),\epsilon) - \bar{h}(r(s,a))\Big) - \Big(\widetilde{r}(s,a^\star) - \bar{h}(r(s,a^\star))\Big) \le \bar{h}(r(s,a^\star)) - \bar{h}(r(s,a)) + b_1\bigg]\\
    &\overset{(\mathrm{i})}{=} \PP\bigg[\Big(h(r(s,a),\epsilon) - \bar{h}(r(s,a))\Big) + \Big(\widetilde{r}(s,a^\star) - \bar{h}(r(s,a^\star))\Big) \le \bar{h}(r(s,a^\star)) - \bar{h}(r(s,a)) + b_1\bigg]\\
    &= 1 - \PP\bigg[\Big(h(r(s,a),\epsilon) - \bar{h}(r(s,a))\Big) + \Big(\widetilde{r}(s,a^\star) - \bar{h}(r(s,a^\star))\Big) \\ & \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \ge \bar{h}(r(s,a^\star)) - \bar{h}(r(s,a)) + b_1\bigg]\\
    &\overset{(\mathrm{ii})}{\le} 1-c_{q}\exp\left(-\left(\frac{\left(\bar{h}(r(s,a^\star)) -\bar{h}(r(s,a)) + b_1\right)^2}{c_1 \bigg(\Var\Big(h(r(s,a),\epsilon) - \bar{h}(r(s,a))\Big) + \Var\Big(\widetilde{r}(s,a^\star) - \bar{h}(r(s,a^\star))\Big)\bigg)}\right)^{1/q}\right)\\
    &\le 1-c_{q}\exp\left(-\left(\frac{\left(\bar{h}(r(s,a^\star)) -\bar{h}(r(s,a)) + b_1\right)^2}{c_1 \Var\Big(h(r(s,a),\epsilon) - \bar{h}(r(s,a))\Big)}\right)^{1/q}\right).
\end{align*}
In the series of equalities and inequalities above, (i) is Condition 2 of $\cH$ described in Section \ref{sec:formulation}, which the uncertainty has symmetric concentration.

(ii) can be obtained by applying \eqref{eq:gaussian-chaos-lower} in Lemma \ref{lem:gaussian-chaos} to $\widetilde{r}(s,a^\star) - \bar{h}(r(s,a^\star))$, which is a sum of $n_{(s,a^\star)}$ human rating samples at $(s,a^\star)$ and one sample at $(s,a)$.

Recall that the pessimism penalty in Algorithm \ref{alg:LCB} is
\begin{equation*}
    b_n(s,a) = c_b \sqrt{\frac{\widetilde{V}_{R,\sigma}^2\log\frac{SA}{\delta}}{n}}.
\end{equation*}
%
Bringing this and everything else into the inequality above, we have
\begin{align}\label{eq:proof:Cstar-exp-bound}
    &\quad\ \PP\left[\widehat{r}(s,a) \le \widehat{r}(s,a^\star)\right] \notag\\
    &\le 1-c_{q}\exp\left(-\left(\frac{\left(\bar{h}(r(s,a^\star)) -\bar{h}(r(s,a)) + b_1\right)^2}{c_1 \Var\Big(h(r(s,a),\epsilon) - \bar{h}(r(s,a))\Big)}\right)^{1/q}\right) \notag\\
    &\overset{(\mathrm{iii})}{\le} 1-c_{q}\exp\left(-\frac{\Big(\bar{h}(r(s,a^\star)) -\bar{h}(r(s,a))\Big)^{2/q} + b_1^{2/q}}{c_1^{2/q} \Var\Big(h(r(s,a),\epsilon) - \bar{h}(r(s,a))\Big)^{2/q}}\right) \notag\\
    &\overset{(\mathrm{iv})}{\le} 1-c_{q}\exp\left(-\frac{\Big(C_{h,2}\bar{h}\big(r(s,a^\star) - r(s,a)\big)\Big)^{2/q} + b_1^{2/q}}{c_1^{2/q} \Var\Big(h(r(s,a),\epsilon) - \bar{h}(r(s,a))\Big)^{2/q}}\right) \notag\\
    &= 1-c_{q}\exp\left(-\frac{\Big(C_{h,2}c(R,\sigma,q) V_{R,\sigma}\Big)^{2/q} + b_1^{2/q}}{c_1^{2/q} \Var\Big(h(r(s,a),\epsilon) - \bar{h}(r(s,a))\Big)^{2/q}}\right) \notag\\
    &= 1-c_{q,1}\exp\left(-\frac{C_{h,2}^{2/q}(c(R,\sigma,q))^{2/q}V_{R,\sigma}^{2/q} + \left(c_b^2 \widetilde{V}_{R,\sigma}^2\log \frac{SA}{\delta}\right)^{1/q}}{c_1^{2/q}(c(q))^{2/q} V_{R,\sigma}^{2/q}}\right) \notag\\
    &= 1-c_{q,1}\exp\left(-\frac{C_{h,2}^{2/q}(c(R,\sigma,q))^{2/q}V_{R,\sigma}^{2/q} + \left(c_b^2 c_V V_{R,\sigma}^2\log \frac{SA}{\delta}\right)^{1/q}}{c_1^{2/q}(c(q))^{2/q} V_{R,\sigma}^{2/q}}\right) \notag\\
    &= 1-c_{q,1}e^{-\left(\frac{C_{h,2}c(R,\sigma,q)}{c_{1}c(q)}\right)^{2/q}}\exp\left(-\left(\frac{c_b\sqrt{c_V}}{c_{1}c(q)}\right)^{2/q}\log^{1/q} \frac{n^2}{\delta}\right) .
\end{align}

In the inequalities above, (iii) can be obtained by Jensen's inequality, because the function $x^{2/q}$ is concave for $x \ge 0$ when $q \ge 2$.

(iv) is obtained by Condition 3 of $\cH$ described in Section \ref{sec:formulation}.

We can further obtain the following once $n \ge c(q, c_b, c_V, \delta,\sigma, R)$:
\begin{align}
    &\quad\ \PP\left[\cE \wedge \forall a\neq a^\star, \widehat{r}(s,a) \le \widehat{r}(s,a^\star)\right] \notag\\
    &= \left(\PP\left[\widehat{r}(s,a_2) \le \widehat{r}(s,a^\star)\right]\right)^{0.1n} \notag\\
    &\le \Bigg(1-c_{q,1}e^{-\left(\frac{C_{h,2}c(R,\sigma,q)}{c_{1}c(q)}\right)^{2/q}}\exp\bigg(-\Big(\frac{c_b\sqrt{c_V}}{c_{1}c(q)}\Big)^{2/q}\log^{1/q} \frac{n^2}{\delta}\bigg) \Bigg)^{0.1n} \notag\\
    &\overset{(\mathrm{v})}{\le} \Bigg(1-c_{q,1}e^{-\left(\frac{C_{h,2}c(R,\sigma,q)}{c_{1}c(q)}\right)^{2/q}}\left(\frac{\delta^{1/q}}{n^{2/q}}\right)^{\left(\frac{c_b\sqrt{c_V}}{c_{1}c(q)}\right)^{2/q}} \Bigg)^{0.1n} \label{eq:proof-eq:lb1}\\
    &\overset{(\mathrm{vi})}{<} 0.17 \label{eq:proof-eq:lb2}.
\end{align}
Above, (v) becomes true once $n$ surpasses some threshold that depends on $q, c_b, c_V, \delta, c_1, C_{h,2}, \sigma, R$. For the case $q \ge 3$, the quantity in \eqref{eq:proof-eq:lb1} decreases monotonically after a certain point that depends on $q, c_b, c_V, \delta, c_1, C_{h,2}, \sigma, R$ and its limit goes to $0$ as $n$ approaches infinity, so (vi) is true once $n\ge c(q, c_b, c_V, \delta,\sigma, R)$, where $c(q, c_b, c_V, \delta,\sigma, R)$ is a constant depending on $q, c_b, c_V, \delta,\sigma, R$. For the case $q=2$, the quantity in \eqref{eq:proof-eq:lb1} monotonically increases towards its limit, which is strictly less than 1, and we can choose $c(R,\sigma,q)$ and $c(q)$ appropriately so that (vi) holds.

On the other hand, through a union bound argument, we have
\begin{align}\label{eq:proof-eq:lb3}
    \PP\left[\cE^C\right] \le \PP\Big[\cE_1^C\Big] + \PP\Big[\cE_2^C\Big] = \left(1 - \frac{1}{e}\right) - (1 - 0.99517) < 0.63.
\end{align}

Overall, combining \eqref{eq:proof-eq:lb2} and \eqref{eq:proof-eq:lb3}, we can obtain a lower bound on the probability that Algorithm \ref{alg:LCB} fails to identify the optimal policy:
\begin{align*}
    &\quad \ \PP\Big[\exists a\neq a^\star, \widehat{r}(s,a) > \widehat{r}(s,a^\star)\Big]\\
    &\ge 1 - \left(\PP\Big[\cE \wedge \forall a\neq a^\star, \widehat{r}(s,a) \le \widehat{r}(s,a^\star)\Big] + \PP\Big[\cE^C\Big]\right)\\
    &> 1 - 0.17 - 0.63\\
    &= 0.2.
\end{align*}

Finally, using this probability lower bound, we arrive at the desired lower bound on the excepted suboptimality: 
\begin{align*}
    \EE_{\cD}[\SubOpt(\widehat{\pi}_{\rm LCB})] &= \PP\Big[\exists a\neq a^\star, \widehat{r}(s,a) > \widehat{r}(s,a^\star)\Big] \cdot (r(s,a^\star) - r(s,a))\\
    &= 0.2 c(R,\sigma,q) \bar{h}^{-1}\left(V_{R,\sigma}\right)\\
    &= c_0R.
\end{align*}
\end{proof}

\subsection{Proof of Corollary \ref{cor:Cstar}}

\begin{proof}
    To prove this corollary, we can construct a bandit instance with the same state space $\cS$ and action space $\cA$ as in the proof for Theorem \ref{thm:Cstar}. Recall the specific human rating function for this corollary is $h(r,\epsilon) = r^2 + r^2\epsilon|\epsilon|$ with $\bar{h}(r) = r^2$. The variance of $h(r,\epsilon)$ is $3r^4\sigma^4$. Let the true reward function $r$ be defined such that $r(s,a_1) = \frac{1}{2} + \frac{1}{4\cdot 3^{1/4}\sigma}(3r^4\sigma^4)^{1/4} = \frac{3}{4}$ and $r(s,a_2) = \frac{1}{2}$. It can be checked that this reward function satisfies the two conditions in the proof for Theorem \ref{thm:Cstar}. The remaining of the proof is similar to the proof for Theorem \ref{thm:Cstar}. We can conclude that Algorithm \ref{alg:LCB} suffers a suboptimality of $\frac{1}{4}$ with constant probability depending on $q,c_b,c_V, \delta,\sigma$.
\end{proof}

\section{Proofs for human rating under Assumption \ref{assumption:uniform}}

\subsection{Proof of Theorem \ref{thm:uniform}}
\begin{proof}
To prove this theorem, it suffices to construct a bandit instance with $\cS = \{s\}$ and $\cA = \{a_1, a_2\}$. Let the true reward function $r$ be defined such that two conditions are satisfied: (i) let $r(s,a_1) = r(s,a_2) + \bar{h}^{-1}\left(\sqrt{\frac{V_{R,\sigma}^2}{n}}\right)$. Note that when $n$ is sufficiently large, i.e., when $n \ge c(R,\sigma,q)$, we can make sure $r \in [0,R]^{SA}$. For (ii), we let $\Var\Big(h(r(s,a_1),\epsilon) - \bar{h}(r(s,a_i))\Big) = c_1(q)V_{R,\sigma}^2$ and $\Var\Big(h(r(s,a_2),\epsilon) - \bar{h}(r(s,a_i))\Big) = c_2(q)V_{R,\sigma}^2$, where $c_1(q)$ and $c_2(q)$ are constants depending on $q$. We can select $c(R,\sigma,q)$, $c_1(q)$ and $c_2(q)$ so that such a reward function $r$ exists. It can be observed that $a_1$ is the optimal action at $s$. Also recall the sampling distribution $d$ is uniform, i.e., $d(s,a) = \frac{1}{SA}$ for any $(s,a)\in\cS\times\cA$.

Let us consider the regime when $n > 8SA\log(\frac{SA}{0.1})/\bar{d}$, where $\bar{d} := \min_{s,a} \{d(s,a) ~:~ d(s,a) > 0\}$. In our setting, this is equivalent to $n \ge 120 > 32\log(40)$. By Lemma \ref{lem:empirical-n}, for all $(s,a) \in \cS\times \cA$ simultaneously, 
\begin{equation}\label{eq:proof-eq:bounded-n}
    \frac{1}{2}n\cdot d(s,a) \le n_{(s,a)} \le \frac{3}{2}n\cdot d(s,a)
\end{equation}
with probability at least $0.9$.

    In the event of \eqref{eq:proof-eq:bounded-n}, we have $cn_{(s,a_1)} = n_{(s,a_2)}$ with $\frac{1}{3} \le c \le 3$. Since we can design the reward function adversarially so that $a_1$ is the arm with more samples in the offline dataset, we can assume $1 \le c_s \le 3$ without loss of generality.

Similar to the proof of Theorem \ref{thm:Cstar}, we want to give a lower bound for the probability of the event that a suboptimal arm is finally selected by Algorithm \ref{alg:LCB}, i.e., $\widehat{r}(s,a_2) > \widehat{r}(s,a_1)$. We can rewrite this probability as follows:
%
\begin{align*}
        &\quad\ \PP\left[\widehat{r}(s,a_2) > \widehat{r}(s,a_1)\right]\\
        &= \PP\left[\widetilde{r}(s,a_2) - b_{n_{(s,a_2)}} > \widetilde{r}(s,a_1) - b_{n_{(s,a_1)}}\right]\\
        &= \PP\left[\widetilde{r}(s,a_2) - \widetilde{r}(s,a_1) > b_{n_{(s,a_2)}} - b_{n_{(s,a_1)}}\right]\\
        &= \PP\bigg[\Big(\widetilde{r}(s,a_2) - \bar{h}(r(s,a_2))\Big) - \Big(\widetilde{r}(s,a_1) - \bar{h}(r(s,a_1))\Big)\\ & \qquad\qquad\qquad\qquad\qquad\qquad\qquad > \bar{h}(r(s,a_1)) - \bar{h}(r(s,a_2)) + b_{n_{(s,a_2)}} - b_{n_{(s,a_1)}}\bigg]\\
        &= \PP\bigg[\Big(\widetilde{r}(s,a_2) - \bar{h}(r(s,a_2))\Big) + \Big(\widetilde{r}(s,a_1) - \bar{h}(r(s,a_1))\Big)\\ & \qquad\qquad\qquad\qquad\qquad\qquad\qquad > \bar{h}(r(s,a_1)) - \bar{h}(r(s,a_2)) + b_{n_{(s,a_2)}} - b_{n_{(s,a_1)}}\bigg],
\end{align*}
where the last step is due to Condition 2 of $\cH$ described in Section \ref{sec:formulation}, which the uncertainty has symmetric concentration.

We can invoke \eqref{eq:gaussian-chaos-lower} in Lemma \ref{lem:gaussian-chaos} on the quantity $\Big(\widetilde{r}(s,a_2) - \bar{h}(r(s,a_2))\Big) - \Big(\widetilde{r}(s,a_1) - \bar{h}(r(s,a_1))\Big)$ above, which is a sum of $n_{(s,a_1)}$ human rating samples at $(s,a_1)$ and $n_{(s,a_2)}$ human rating samples at $(s,a_2)$. This can give us:
    \begin{align*}
        &\quad\ \PP\bigg[\Big(\widetilde{r}(s,a_2) - \bar{h}(r(s,a_2))\Big) + \Big(\widetilde{r}(s,a_1) - \bar{h}(r(s,a_1))\Big)\\ & \qquad\qquad\qquad\qquad\qquad\qquad\qquad > \bar{h}(r(s,a_1)) - \bar{h}(r(s,a_2)) + b_{n_{(s,a_2)}} - b_{n_{(s,a_1)}}\bigg]\\
        &\ge c_{q}\exp\left(-\left(\frac{\left(\bar{h}(r(s,a_1)) -\bar{h}(r(s,a_2))\right)^2}{c_1(\frac{c_1(q)}{n_{(s,a_1)}}+\frac{c_2(q)}{n_{(s,a_2)}})V^2_{R,\sigma}}\right)^{1/q}\right)\\
        &\ge c_{q}\exp\left(-\left(\frac{\Big(C_{h,2}\bar{h}\big(r(s,a_1) - r(s,a_2)\big)\Big)^2}{c_1(\frac{c_1(q)}{n_{(s,a_1)}}+\frac{c_2(q)}{n_{(s,a_2)}})V^2_{R,\sigma}}\right)^{1/q}\right)\\
        &= c_{q}\exp\left(-\left(\frac{C_{h,2}^2 V^2_{R,\sigma}}{nc_1(\frac{c_1(q)}{n_{(s,a_1)}}+\frac{c_2(q)}{n_{(s,a_2)}})V^2_{R,\sigma}}\right)^{1/q}\right)\\
        &\ge c_{q}\exp\left(-\left(\frac{C_{h,2}^2 V^2_{R,\sigma}}{nc_1(\frac{c_1(q)}{\frac{3}{2}n\cdot d(s,a_1)}+\frac{c_2(q)}{\frac{3}{2}n\cdot d(s,a_2)})V^2_{R,\sigma}}\right)^{1/q}\right)\\
        &\ge c_{q}\exp\left(-\left(\frac{3C_{h,2}^2}{4c_1(c_1(q)+c_2(q))}\right)^{1/q}\right)\\
        &=: c_0.
    \end{align*}

Overall, Algorithm \ref{alg:LCB} is guaranteed to incur an expected suboptimality at least $c_0\bar{h}^{-1}\left(\sqrt{\frac{V_{R,\sigma}^2}{n}}\right)$ as soon as $n\ge \max\{c(R,\sigma,q),120\}$.
\end{proof}

\subsection{Proof of Corollary \ref{cor:uniform}}

\begin{proof}
    To prove this corollary, we can construct a bandit instance with the same state space $\cS$ and action space $\cA$ as in the proof for Theorem \ref{thm:uniform}. Recall the specific human rating function for this corollary is $h(r,\epsilon) = r^2 + r^2\epsilon|\epsilon|$ with $\bar{h}(r) = r^2$. The variance of $h(r,\epsilon)$ is $3r^4\sigma^4$. Let the true reward function $r$ be defined such that $r(s,a_1) = \frac{1}{2} + \sigma\left(\frac{1}{n}\right)^{1/4}$ and $r(s,a_2) = \frac{1}{2}$. It can be checked that this reward function satisfies the two conditions in the proof for Theorem \ref{thm:Cstar}. The remaining of the proof is similar to the proof for Theorem \ref{thm:Cstar}. We can conclude that Algorithm \ref{alg:LCB} suffers a suboptimality of $\sigma\left(\frac{1}{n}\right)^{1/4}$ with constant probability depending on $q$ as soon as $n\ge \max\{48\sigma^4, 60\}$.
\end{proof}

\section{Proofs for human rating upper bounds}

\subsection{Proof of Theorem \ref{thm:upper-bound}}

\begin{proof}
In this proof, let us denote the output of Algorithm \ref{alg:LCB} $\widehat{\pi}_{\rm LCB}$ with $\widehat{\pi}$ for short. By the definition in \eqref{eq:suboptimality-def}, we can decompose the suboptimality of the output $\widehat{\pi}$ of Algorithm \ref{alg:LCB} as follows:
    \begin{align}\label{eq:proof:upper-bound}
        &\quad\ \SubOpt(\widehat{\pi}) \notag\\
        &= \EE_{s\sim\rho}[r(s,\pi^\star(s)) - r(s,\widehat{\pi}(s))]\notag\\
        &= \EE_{s\sim\rho}[(\bar{h}^{-1}\circ\bar{h})(r(s,\pi^\star(s))) - (\bar{h}^{-1}\circ\bar{h})(r(s,\widehat{\pi}(s)))]\notag\\
        &\overset{(\mathrm{i})}{\le} C_{h,1}\EE_{s\sim\rho}\Big[\bar{h}^{-1}\Big(\bar{h}(r(s,\pi^\star(s))) - \bar{h}(r(s,\widehat{\pi}(s)))\Big)\Big]\notag\\
        &= C_{h,1}\EE_{s\sim\rho}\Big[\bar{h}^{-1}\Big(\bar{h}(r(s,\pi^\star(s))) - \widehat{r}(s,\pi^\star(s)) + \widehat{r}(s,\pi^\star(s)) - \widehat{r}(s,\widehat{\pi}(s)) + \widehat{r}(s,\widehat{\pi}(s)) - \bar{h}(r(s,\widehat{\pi}(s)))\Big)\Big]\notag\\
        &\overset{(\mathrm{ii})}{\le}  C_{h,1}\EE_{s\sim\rho}\Big[\bar{h}^{-1}\Big(\bar{h}(r(s,\pi^\star(s))) - \widehat{r}(s,\pi^\star(s)) + \widehat{r}(s,\widehat{\pi}(s)) - \bar{h}(r(s,\widehat{\pi}(s)))\Big)\Big]\notag\\
        &\overset{(\mathrm{iii})}{\le}  C_{h,1}\EE_{s\sim\rho}\Big[\bar{h}^{-1}\Big(\bar{h}(r(s,\pi^\star(s))) - \widehat{r}(s,\pi^\star(s))\Big)\Big]\notag\\
        &\overset{(\mathrm{iv})}{\le} C_{h,1}\EE_{s\sim\rho}\Big[\bar{h}^{-1}\Big(\Big|\bar{h}(r(s,\pi^\star(s))) - \widetilde{r}(s,\pi^\star(s))\Big| + b_{n_{(s,\pi^\star(s))}}\Big)\Big]\notag\\
        &\overset{(\mathrm{v})}{\le} C_{h,1}\EE_{(s,a)\sim d_\rho^{\pi^\star}}\left[\bar{h}^{-1}\left(C'\sqrt{\frac{V^2_{R,\sigma}\log^q\frac{SA}{\delta}}{n_{(s,a)}}}\right)\right]\notag\\
        &\overset{(\mathrm{vi})}{\le} C_{h,1}\EE_{(s,a)\sim d_\rho^{\pi^\star}}\left[\bar{h}^{-1}\left(C\sqrt{\frac{V^2_{R,\sigma}\log^q\frac{SA}{\delta}}{n\cdot d(s,a)}}\right)\right]\notag\\
        &\overset{(\mathrm{vii})}{\le} c_0\EE_{(s,a)\sim d_\rho^{\pi^\star}}\left[\bar{h}^{-1}\left(\sqrt{\frac{V^2_{R,\sigma}\log^q\frac{SA}{\delta}}{n\cdot d(s,a)}}\right)\right]\notag\\
        &\le c_0\sum_{(s,a)\in\cX}d_\rho^{\pi^\star}(s,a)\cdot \bar{h}^{-1}\left(\sqrt{\frac{V_{R,\sigma}^2\log^q\frac{SA}{\delta}}{n\cdot d(s,a)}}\right).
    \end{align}
    In the series of equalities and inequalities above, (i) is due to Condition 3 of $\cH$ described in Section \ref{sec:formulation}.

    (ii) can be obtained because of the fact that in Algorithm \ref{alg:LCB}, $\widehat{\pi}(s) = \arg\max_a \widehat{r}(s,a)$, so for every $s\in\cS$, $\widehat{r}(s,\widehat{\pi}(s)) \ge \widehat{r}(s,\pi^\star(s))$.

    (iii) can be obtained because the pessimistic penalty $b_n$ in Theorem \ref{thm:upper-bound} guarantees $\widehat{r}(s,\widehat{\pi}(s)) \le \bar{h}(r(s,\widehat{\pi}(s)))$ with probability $1-\delta$. 
    
    To show this, we can focus on bounding the probability of the event
    \begin{equation*}
        \widetilde{r}(s,a) - \EE[\widetilde{r}(s,a)] \le b_{n_{(s,a)}}
    \end{equation*}
    for all $(s,a)\in\cS\times\cA$ simultaneously. For simplicity in the remaining part of this proof, let us denote this event with $\cE$. This involves an understanding of the concentration of the rating observation $h(r(s,a),\epsilon)$ under \eqref{eq:rating-h-model}.

    By Lemma \ref{lem:gaussian-chaos} and a union bound argument, for all $(s,a)\in\cS\times\cA$ simultaneously, we have
    \begin{align*}
        \PP\left[|\widetilde{r}(s,a) - \EE[\widetilde{r}(s,a)]| \ge t\right] \le SA\cdot C_{q}\exp\left(-\left(\frac{t^2}{C_{1}V^2_{R,\sigma}}\right)^{1/q}\right).
    \end{align*}

    We can solve for $t$ in the inequality above. Given $b_n$ in Theorem \ref{thm:upper-bound} with a sufficiently large $c_b$, this gives 
    \begin{align}\label{eq:h-tilde-CI}
        |\widetilde{r}(s,a) - \EE[\widetilde{r}(s,a)]| \le c(q)\sqrt{\frac{V^2_{R,\sigma}\log^q\frac{SA}{\delta}}{n_{(s,a)}}} \le b_{n_{(s,a)}}
    \end{align}
    for all $(s,a)\in\cS\times\cA$ simultaneously with probability at least $1-\delta$. Here, $c(q)$ is an absolute constant depending on $q$.

    Note that $\bar{h}(r(s,a)) = \EE[\widetilde{r}(s,a)]$. Thus, we can establish (iii) on the event $\cE$, which has probability at least $1-\delta$. 

    (iv) is obtained by the monotoncity of $\bar{h}^{-1}(\cdot)$. Since $\bar{h}(r(s,a)) - \widehat{r}(s,a) \le |\bar{h}(r(s,a)) - \widetilde{r}(s,a)| + b_{n_{(s,a)}}$, $\bar{h}^{-1}(\bar{h}(r(s,a)) - \widehat{r}(s,a)) \le \bar{h}^{-1}(|\bar{h}(r(s,a)) - \widetilde{r}(s,a)| + b_{n_{(s,a)}})$, for any $(s,a)\in\cS\times\cA$.
    
    (v) can be obtained by finding an upper bound for $|\bar{h}(r(s,\pi^\star(s))) - \widetilde{r}(s,\pi^\star(s))|$, which follows from \eqref{eq:h-tilde-CI}. That is, on the event $\cE$, we have
    \begin{align*}
        |\bar{h}(r(s,\pi^\star(s))) - \widetilde{r}(s,\pi^\star(s))| \le c(q)\sqrt{\frac{V^2_{R,\sigma}\log^q\frac{SA}{\delta}}{n_{(s,\pi^\star(s))}}}
    \end{align*}
    for some absolute constant $c(q)$ depending on $q$. Then, the addition of the bound above and $b_{n_{(s,\pi^\star(s)}}$ gives (v).

    (vi) can be obtained from an invocation of Lemma \ref{lem:empirical-n}, which guarantees $n_{(s,a)} = cd(s,a)n$ for some constant $\frac{1}{2} \le c\le \frac{3}{2}$ for all $(s,a)\in\cS\times\cA$ simultaneously, with probability at least $1-\delta$. We can denote this event with $\cE'$.

    Finally, (vii) is obtained because we can pull the constant factor inside $\bar{h}^{-1}$ out due to Condition 3 of $\cH$ described in Section \ref{sec:formulation}.

    The advertised bound can be obtained after taking a union bound on the probability of the complement of $\cE$ and the complement of $\cE'$, which only changes \eqref{eq:proof:upper-bound} by a constant factor.
\end{proof}

\subsection{Proof of Corollary \ref{cor:upper-bound}}

\begin{proof}
    In this proof, let us denote the output of Algorithm \ref{alg:LCB} $\widehat{\pi}_{\rm LCB}$ with $\widehat{\pi}$ for short. By the definition in \eqref{eq:suboptimality-def}, we can decompose the suboptimality of the output $\widehat{\pi}$ of Algorithm \ref{alg:LCB} as follows:
    \begin{align}\label{eq:proof:upper-bound-h}
        &\quad\ \SubOpt(\widehat{\pi}) \notag\\
        &= \EE_{s\sim\rho}[r(s,\pi^\star(s)) - r(s,\widehat{\pi}(s))]\notag\\
        &= \EE_{s\sim\rho}\left[\sqrt{r^2(s,\pi^\star(s))} - \sqrt{r^2(s,\widehat{\pi}(s))}\right] \notag\\
        &\overset{(\mathrm{i})}{\le} \EE_{s\sim\rho}\left[\sqrt{r^2(s,\pi^\star(s)) - r^2(s,\widehat{\pi}(s))}\right] \notag\\
        &= \EE_{s\sim\rho}\Big[\sqrt{r^2(s,\pi^\star(s)) - \widehat{r}(s,\pi^\star(s)) + \widehat{r}(s,\pi^\star(s)) - \widehat{r}(s,\widehat{\pi}(s)) + \widehat{r}(s,\widehat{\pi}(s)) - r^2(s,\widehat{\pi}(s))}\Big]\notag\\
        &\overset{(\mathrm{ii})}{\le}  \EE_{s\sim\rho}\Big[\sqrt{r^2(s,\pi^\star(s)) - \widehat{r}(s,\pi^\star(s)) + \widehat{r}(s,\widehat{\pi}(s)) - r^2(s,\widehat{\pi}(s))}\Big]\notag\\
        &\overset{(\mathrm{iii})}{\le}  \EE_{s\sim\rho}\Big[\sqrt{r^2(s,\pi^\star(s)) - \widehat{r}(s,\pi^\star(s))}\Big]\notag\\
        &\overset{(\mathrm{iv})}{\le} \EE_{s\sim\rho}\Bigg[\sqrt{\Big|\bar{h}(r(s,\pi^\star(s))) - \widetilde{r}(s,\pi^\star(s))\Big| + b_{n_{(s,\pi^\star(s))}}}\Bigg]\notag\\
        &\overset{(\mathrm{v})}{\le} \EE_{(s,a)\sim d_\rho^{\pi^\star}}\left[\Bigg(2\sqrt{\frac{64\sigma^4\log\frac{SA}{\delta}}{n_{(s,a)}}} + \frac{16\sigma^2\log\frac{SA}{\delta}}{n_{(s,a)}}\Bigg)^{1/2}\right]\notag\\
        &\le 2\sum_{(s,a)\in\cX}d_\rho^{\pi^\star}(s,a)\left(\frac{256\sigma^4\log\frac{SA}{\delta}}{n\cdot d(s,a)}\right)^{1/4} + d_\rho^{\pi^\star}(s,a)\sqrt{\frac{32\sigma^2\log\frac{SA}{\delta}}{n\cdot d(s,a)}}.
    \end{align}
    In the series of equalities and inequalities above, (i) is due to Condition 3 of $\cH$ described in Section \ref{sec:formulation}, which can be proved for $\bar{h}(\cdot) = (\cdot)^2$ with Jensen's inequality.

    (ii) can be obtained because of the fact that in Algorithm \ref{alg:LCB}, $\widehat{\pi}(s) = \arg\max_a \widehat{r}(s,a)$, so for every $s\in\cS$, $\widehat{r}(s,\widehat{\pi}(s)) \ge \widehat{r}(s,\pi^\star(s))$.

    (iii) can be obtained because the pessimistic penalty $b_n$ in Corollary \ref{cor:upper-bound} guarantees $\widehat{r}(s,\widehat{\pi}(s)) \le r^2(s,\widehat{\pi}(s))$ with probability $1-\delta/2$. 
    
    To show this, we can focus on bounding the probability of the event
    \begin{equation*}
        \widetilde{r}(s,a) - \EE[\widetilde{r}(s,a)] \le b_{n_{(s,a)}}
    \end{equation*}
    for all $(s,a)\in\cS\times\cA$ simultaneously. For simplicity in the remaining part of this proof, let us denote this event with $\cE$. This involves an understanding of the concentration of the rating observation $h(r(s,a),\epsilon)$ under \eqref{eq:rating-h-model}.

    Note that $\epsilon|\epsilon|$ is a sub-exponential random variable with parameters $(4\sigma^4,4\sigma^2)$ by Proposition \ref{prop:eps-sub}. 
    
    Now, by Lemma \ref{lem:bernstein} and a union bound argument, for all $(s,a)\in\cS\times\cA$ simultaneously, we have
    \begin{align*}
        |\widetilde{r}(s,a) - \EE[\widetilde{r}(s,a)]| \le \sqrt{\frac{64\sigma^4\log\frac{SA}{\delta}}{n_{(s,a)}}} + \frac{8\sigma^2\log\frac{SA}{\delta}}{n_{(s,a)}} \le b_{n_{(s,a)}}
    \end{align*}
    with probability at least $1-\delta/2$. 

    Note that $\bar{h}(r(s,a)) = \EE[\widetilde{r}(s,a)]$. Thus, we can establish (iii) on the event $\cE$, which has probability at least $1-\delta/2$. 

    (iv) is obtained by the monotoncity of $\bar{h}^{-1}(\cdot) = \sqrt{\cdot}$. Since $\bar{h}(r(s,a)) - \widehat{r}(s,a) \le |\bar{h}(r(s,a)) - \widetilde{r}(s,a)| + b_{n_{(s,a)}}$, $\bar{h}^{-1}(\bar{h}(r(s,a)) - \widehat{r}(s,a)) \le \bar{h}^{-1}(|\bar{h}(r(s,a)) - \widetilde{r}(s,a)| + b_{n_{(s,a)}})$, for any $(s,a)\in\cS\times\cA$.
    
    (v) can be obtained by finding an upper bound for $|\bar{h}(r(s,\pi^\star(s))) - \widetilde{r}(s,\pi^\star(s))|$, which follows from \eqref{eq:h-tilde-CI}. That is, on the event $\cE$, we have
    \begin{align*}
        |\bar{h}(r(s,\pi^\star(s))) - \widetilde{r}(s,\pi^\star(s))| \le \sqrt{\frac{64\sigma^4\log\frac{SA}{\delta}}{n_{(s,a)}}} + \frac{8\sigma^2\log\frac{SA}{\delta}}{n_{(s,\pi^\star(s))}}.
    \end{align*}
    Then, the addition of the bound above and $b_{n_{(s,\pi^\star(s)}}$ gives (v).

    (vi) can be obtained from an invocation of Lemma \ref{lem:empirical-n}, which guarantees $n_{(s,a)} = cd(s,a)n$ for some constant $\frac{1}{2} \le c\le \frac{3}{2}$ for all $(s,a)\in\cS\times\cA$ simultaneously, with probability at least $1-\delta/2$. We can denote this event with $\cE'$.

    The advertised bound can be obtained after taking a union bound on the probability of the complement of $\cE$ and the complement of $\cE'$, which changes \eqref{eq:proof:upper-bound-h} by a factor of $2$.
\end{proof}

\section{Proof of Theorem \ref{thm:comp}}

\citet{zhu2023principled} considers the setting in which the reward function $r_{\theta^\star}(s,a)$ is parameterized by $\theta^\star\in \RR^d$ in a linear fashion, i.e., $r_{\theta^\star}(s,a) = (\theta^\star)^\top \phi(s,a)$. Here, $\phi(s,a)$ is a known feature in $\RR^d$. The tabular setting we consider is a special case of the linear setting. To write the tabular setting in this notation, we can let $\phi(s,a) = e_{(s,a)}$, where $e_{(s,a)} \in \RR^{SA}$ is the canonical basis vector for the $(s,a)$-th dimension, and let $\theta^\star  \in \RR^{SA}$ be a shifted version of the tabular reward function/vector $r \in [0,R]^{SA}$, i.e., $\theta^\star(s,a) = r(s,a) - \frac{1}{SA}\sum_{(s,a)}r(s,a) \in [-R,R]^{SA}$. This ensures $\mathbf{1}^\top\theta^\star = 0$, which is required by \citet{zhu2023principled} for the identifiability of $\theta^\star$. As \citet{zhu2023principled} shows, Algorithm \ref{alg:LCB-comp} converges to $\theta^\star$. 

In the analysis below, we omit the subscript of $\widehat{\pi}_{\rm PMLE}$ and write the output policy of Algorithm \ref{alg:LCB-comp} as $\widehat{\pi}$ for simplicity. We also denote the shifted true reward with $\bar{r}(s,a) := r(s,a) - \frac{1}{SA}\sum_{(s,a)}r(s,a)$ for any $s\in\cS$ and $a\in\cA$ and let $\widehat{r}$ be the one from Line \ref{alg-line:comp-r-hat} of Algorithm \ref{alg:LCB-comp}. The following analysis largely overlaps with Theorem 3.2 in \citet{zhu2023principled}, but we handle the distributional mismatch term with more care and provide a characterization specific to the tabular setting.

The suboptimality of Algorithm \ref{alg:LCB-comp} can be decomposed as follows:
\begin{align}
    &\quad\ \SubOpt(\widehat{\pi}_{\rm PMLE})\notag\\
        &= \EE_{s\sim\rho}[r(s,\pi^\star(s)) - r(s,\widehat{\pi}(s))]\notag\\
        &= \EE_{s\sim\rho}[\bar{r}(s,\pi^\star(s)) - \bar{r}(s,\widehat{\pi}(s))]\notag\\
        &= \EE_{s\sim\rho}\Big[\bar{r}(s,\pi^\star(s)) - \widehat{r}(s,\pi^\star(s)) + \widehat{r}(s,\pi^\star(s)) - \widehat{r}(s,\widehat{\pi}(s)) + \widehat{r}(s,\widehat{\pi}(s)) - \bar{r}(s,\widehat{\pi}(s))\Big]\notag\\
        &\overset{(\mathrm{i})}{\le} \EE_{s\sim\rho}\Big[\bar{r}(s,\pi^\star(s)) - \widehat{r}(s,\pi^\star(s)) + \widehat{r}(s,\widehat{\pi}(s)) - \bar{r}(s,\widehat{\pi}(s))\Big]\notag\\
        &\overset{(\mathrm{ii})}{\le} \EE_{s\sim\rho}\Big[\bar{r}(s,\pi^\star(s)) - \widehat{r}(s,\pi^\star(s))\Big]\notag\\
        &\overset{(\mathrm{iii})}{\le} \sqrt{\sup_{v:\norm{v}_\infty \le 2R,\mathbf{1}^\top v = 0}\frac{\Big(\sum_{(s,a)} d_\rho^{\pi^\star}(s,a) v(s,a)\Big)^2}{\sum_{(s,a^0,a^1)} d(s,a^0, a^1) \big(v(s,a^0) - v(s,a^1)\big)^2}}\norm{\bar{r} - \widehat{r}}_{\Sigma}\notag\\
        &\overset{(\mathrm{iv})}{=} \sqrt{\sup_{v:\norm{v}_\infty \le 1,\mathbf{1}^\top v = 0}\frac{\Big(\sum_{(s,a)} d_\rho^{\pi^\star}(s,a) v(s,a)\Big)^2}{\sum_{(s,a^0,a^1)} d(s,a^0, a^1) \big(v(s,a^0) - v(s,a^1)\big)^2}}\norm{\bar{r} - \widehat{r}}_{\Sigma}\notag\\
        &\le C^\dagger\left(\norm{\bar{r} - \widetilde{r}}_{\Sigma} + \norm{\widetilde{r} - \widehat{r}}_{\Sigma}\right)\notag\\
        &\overset{(\mathrm{v})}{\le} 2C^\dagger\left(CR\sqrt{\frac{SA + \log\frac{1}{\delta}}{\gamma^2 n}} + cR\sqrt{\frac{S^2A^2\log\frac{n}{\delta}}{n}}\right) \notag\\
        &\le c_0C^\dagger R\left(\sqrt{\frac{SA + \log\frac{1}{\delta}}{\gamma^2 n}} + \sqrt{\frac{S^2A^2\log\frac{n}{\delta}}{n}}\right). \label{eq:comp-pf1}
\end{align}

In the analysis above, (i) can be obtained because of Line \ref{alg-line:comp-r-hat} in Algorithm \ref{alg:LCB-comp}, $\widehat{\pi}(s) = \arg\max_\pi \EE_{s\sim\rho}[\widehat{r}(s,\pi(s))]$, so $\EE_{s\sim\rho}[\widehat{r}(s,\widehat{\pi}(s))] \ge \EE_{s\sim\rho}[\widehat{r}(s,\pi^\star(s))]$.

(ii) is because $\widehat{r}$ is selected pessimistically from $\cF_{\mathrm{CR}}(\widetilde{r})$ in Line \ref{alg-line:comp-r-hat} of Algorithm \ref{alg:LCB-comp} and $\bar{r} \in \cF_{\mathrm{CR}}(\widetilde{r})$ with probability at least $1-\delta/2$. This is guaranteed by Lemma 3.1 of \citet{zhu2023principled}, which proves with probability at least $1-\delta$,
    \begin{equation}
        \norm{\widetilde{r} - \bar{r}}_{\widehat{\Sigma}} \le CR\sqrt{\frac{SA + \log\frac{1}{\delta}}{\gamma^2 n}},
    \end{equation}
    where $C > 0$ is an absolute constant. Hence, we have $\EE_{s\sim\rho}[\widehat{r}(s,\widehat{\pi}(s))] \le \EE_{s\sim\rho}[\bar{r}(s,\widehat{\pi}(s))]$. 

In (iii), we change the measure by invoking the following lemma:

\begin{lemma}[modified from Lemma 1 in \citep{ji2023sample}]\label{lem:dis-mismatch}
    Given a function class $\cF$ that contains functions mapping from $\cX$ to $\RR$ and a probability distribution $p_1$ supported on $\cX$ and a joint quasiprobability probability distribution $p_2$ supported on $\cX\times\cX$, for any $g\in\cF$,
		\begin{equation*}
			\EE_{x\sim p_1}\left[g(x)\right] \le \sqrt{\EE_{(x,x')\sim p_2}[g(x)g(x')]\sup_{g\in\cF}\frac{\EE_{p_1}[g(x)]^2}{\EE_{p_2}[g(x)g(x')]}}.
		\end{equation*}
\end{lemma}

Moreover, note that $\Sigma$ is a scaled Laplacian matrix. Since $\bar{r},\widehat{r} \in \cF$ ($\cF$ is defined in Algorithm \ref{alg:LCB-comp}) and $v^\top \Sigma v = \sum_{i,j}\Sigma_{i,j}v_iv_j$ for any vector $v$, this allows us to write (the denominator of) the distributional mismatch term in the form of $C^\dagger$ defined in \eqref{eq:C-dagger}.

(iv) can be obtained because the distributional mismatch term $C^\dagger$ is constant with respect to any nonzero scaling of $v$.

(v) can be obtained by invoking the following lemma, which provides an upper bound on the difference between the ground truth $\bar{r}$ and the empirical estimation $\widetilde{r}$ with respect to $\Sigma$, the covariance matrix for the population sampling distribution. 

\begin{lemma}\label{lem:MLE-CR}
    With probability at least $1-\delta$, Algorithm \ref{alg:LCB-comp} satisfies
    \begin{equation}
        \norm{\widetilde{r} - \bar{r}}_{\Sigma} \le CR\sqrt{\frac{SA + \log\frac{1}{\delta}}{\gamma^2 n}} + cR\sqrt{\frac{S^2A^2\log\frac{n}{\delta}}{n}},
    \end{equation}
    where $\gamma = \frac{1}{2+\exp(R\sqrt{SA})+\exp(-R\sqrt{SA})}$ and $C,c > 0$ are absolute constants.
\end{lemma}

The proof of Lemma \ref{lem:MLE-CR} is deferred to Section \ref{sec:proof-lem:MLE-CR}.

Finally, we conclude with \eqref{eq:comp-pf1}, which leads to the advertised result.

\subsection{Proof of Lemma \ref{lem:MLE-CR}}\label{sec:proof-lem:MLE-CR}
    We can start with
    \begin{align}
        \norm{\widetilde{r} - \bar{r}}_{\Sigma} &= \sqrt{\norm{\widetilde{r} - \bar{r}}_{\Sigma}^2} \notag\\
        &\le 2\norm{\widetilde{r} - \bar{r}}_{\widehat{\Sigma}}^2 + c\frac{SA\log\frac{n}{\delta}}{n}\norm{\widetilde{r} - \bar{r}}_2^2, \label{eq:MLE-CR-pf1}
    \end{align}
    in which the inequality is due to the following lemma paraphrased from \citep{pacchiano2021dueling}:
    \begin{lemma}[Lemma 7 in \citep{pacchiano2021dueling}]\label{lem:cov-lemma}
        For any $\delta\in(0,1)$, Algorithm \ref{alg:LCB-comp} satisfies
        \begin{equation}
            \norm{\widetilde{r} - \bar{r}}_{\Sigma}^2 \le 2\norm{\widetilde{r} - \bar{r}}_{\widehat{\Sigma}}^2 + c^2\frac{SA\log\frac{n}{\delta}}{n}\norm{\widetilde{r} - \bar{r}}_2^2
        \end{equation}
        with probability at least $1-\delta$. $\widehat{\Sigma}$ is the empirical covariance matrix defined in Algorithm \ref{alg:LCB-comp}. $\Sigma$ is the population covariance matrix, i.e., $\Sigma = \EE[\widehat{\Sigma}]$. $c > 0$ is an absolute constant.
    \end{lemma}

    To proceed with \eqref{eq:MLE-CR-pf1}, we invoke Lemma 3.1 in \citep{zhu2023principled} for an upper bound on $\norm{\widetilde{r} - \bar{r}}_{\widehat{\Sigma}}$, which gives 
    \begin{align*}
        \norm{\widetilde{r} - \bar{r}}_{\Sigma} &\le \sqrt{C^2R^2\frac{SA + \log\frac{1}{\delta}}{\gamma^2 n} + c^2\frac{SA\log\frac{n}{\delta}}{n}\norm{\widetilde{r} - \bar{r}}_2^2}\\
        &\le CR\sqrt{\frac{SA + \log\frac{1}{\delta}}{\gamma^2 n}} + c\sqrt{\frac{SA\log\frac{n}{\delta}}{n}}\norm{\widetilde{r} - \bar{r}}_2\\
        &\le CR\sqrt{\frac{SA + \log\frac{1}{\delta}}{\gamma^2 n}} + cR\sqrt{\frac{S^2A^2\log\frac{n}{\delta}}{n}},
    \end{align*}
    where the last step is because $\norm{\widetilde{r} - \bar{r}}_2 \le 2R\sqrt{SA}$. $C > 0$ is an absolute constant.