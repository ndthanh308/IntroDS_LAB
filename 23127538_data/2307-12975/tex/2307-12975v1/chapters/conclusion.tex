\section{Conclusion}

In this work, we have studied the problem of using human feedback for reward engineering in bandit. Specifically, we have provided a theoretical comparison between human rating methods and preference-based methods to demonstrate the provable advantage of using human preference and confirm its recent empirical success. En route to this, we have proposed a new, improved model for human rating and provided suboptimality analysis for running policy learning methods under this model. While our theory does not have any new limitation compared to existing related results, our results are only focused on the case that all human feedback samples are generated following the same model. It is still open for future work to investigate the case when the human feedback is generated from a diverse set of models and provide a comparison between rating methods and preference-based methods in this setting. 
