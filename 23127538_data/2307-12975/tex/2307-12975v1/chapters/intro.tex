\section{Introduction}

Reward engineering is one of the most crucial topics in decision-making problems, particularly in bandits and reinforcement learning (RL). Since it can be prohibitively expensive to learn the entire environment through random exploration, most existing algorithms explore in a purposeful manner guided by the reward. Therefore, having a good reward function is crucial to finding the desired optimal policy.

In some cases, it can be straightforward to select reward functions, when we have enough prior knowledge about the problems such as games and simulated physical systems \citep{mnih2015humanlevel,mnih2016asynchronous,silver2016}. However, this is often not the case in practice. Real-world problems can be highly complex, and there may not be a clear choice of reward to use. Therefore, practitioners often have to handcraft a reward function from scratch for their systems. Unfortunately, such artificial rewards often end up misaligned with the overall objective of the system and fail to lead to the desired optimal policy. For instance, in the task of teaching a chatbot to converse like a human, assigning a scalar reward to a chatbot's reply is challenging since there is no scale that can objectively evaluate its quality. Therefore, reward engineering poses a significant challenge to policy learning, particularly when it is difficult to quantify policy performance or the system is multi-objective.

To address these challenges, a popular approach is to learn the reward function from human feedback instead of designing it from scratch. This approach assumes a true reward function exists but is not directly accessible, and human annotators, who are presumed capable of recognizing the desired actions of the system, are assigned the task of providing feedback and evaluating the collected data samples. In early techniques, human annotators were asked to rate the data on a scale \citep{li20crowdsourcing,li2020gpm,Janowski15accuracy}. These ratings were either used directly as the reward or incorporated into a pre-designed reward as part of the final reward \citep{Gonzalez10voice}. We refer to this use of human feedback as "rating." However, the reward derived from rating is susceptible to uncertainty and bias introduced by the human annotators, which can significantly deviate from the true reward. To characterize the behavior of human annotators during rating, several models have been proposed and widely adopted in the literature. These existing models, however, fall short on two aspects: (i) they model the relationship between the true reward and uncertainty noise in a restrictive and oversimplified manner, and (ii) the bias formulation in these models is too rigid and strong, which renders the ground truth completely unrecoverable and the problem of policy learning statistically inconsistent.

As an alternative, there has been a growing trend in recent years to employ human feedback for comparing samples rather than rating them individually. These methods are commonly referred to as preference-based methods. Instead of assigning ratings on an absolute scale, annotators are tasked with comparing and ranking a few given samples. This approach is believed to yield more accurate results as relative comparisons between a small number of choices are considered \citep{Tarlow21Reliable}. The preferences expressed by annotators are assumed to follow either the Bradley-Terry-Luce model \citep{Bradley1952RankAO} or the Plackett-Luce model \citep{Plackett1975TheAO}, both of which have found extensive application in recommender systems and crowdsourcing research \citep{chen13crowdsourcing,shah2015estimation,dong17crowdsource}. Preference-based methods have demonstrated effective in learning reward functions for bandit problems and have played a crucial role in the remarkable success of training large language models such as InstructGPT and ChatGPT.

While preference-based methods have demonstrated notable practical effectiveness in reward engineering, their theoretical properties remains largely unexplored. Existing results have primarily focused on algorithms for online bandit and RL, whose goal is to maximize a preference metric rather than to learn a reward function \citep{pacchiano2021dueling,xu20}. Recently, \citet{zhu2023principled} proved the convergence of pessimistic maximum likelihood estimation (MLE) using human preferences and analyzed its suboptimality. However, a theoretical comparison between preference-based methods and rating is still lacking to confirm the advantage of preference-based methods in reward engineering that we have witnessed empirically. In this work, we propose a theory that aims to establish the provable advantage of preference-based methods over rating in policy learning. To align with recent applications \citep{christiano2017deep,ouyang2022training}, we concentrate on tabular contextual bandits in the offline setting.

Specifically, we contribute to the existing literature by considering a new human rating model and analyzing the suboptimality guarantees associated with it. Our rating model is based on a general class of monotone functions that can account for both human bias and uncertainty during the rating process. By incorporating the concept of monotonicity, our model captures the bias observed in real-world human rating and maintains the correct reward ordering. This allows policy learning to be a consistent yet challenging statistical problem. In contrast, previous rating models fail to preserve the reward ordering, rendering the learning of optimal policy impossible. In addition, our model introduces random noise to simulate annotators' uncertainty during rating. Unlike existing models that consider only simple additive subgaussian noise, independent from other quantities in the model, our model accommodates a more general form of noise that can depend on the state-action pair. Through our models, we provide the first known suboptimality analysis for reward engineering with rating in bandit problems. Using our model, we establish suboptimality lower bounds for using standard LCB algorithms on human rating and shed light on how annotator bias and uncertainty can impact such suboptimality. Furthermore, we compare these results with a suboptimality result for preference-based methods, adapted from \citet{zhu2023principled}. The comparison reveals that preference-based methods can achieve lower suboptimality than rating, demonstrating their theoretical advantage in support of their empirical popularity.

\subsection{Related Works}

\textbf{Preference-based reinforcement learning.} 
Preference-based Reinforcement Learning (PbRL) \citep{christiano2017deep, shin2023benchmarks, busa14,wirth16,wirth17,nipsPRL17, abdelkareem2022advances} has been studied under different types of human feedback including action comparison, state comparison and trajectory comparison---see \citet{wirth17,abdelkareem2022advances} for reviews of the literature. Preference-based feedback has been well-studied in bandit settings known as dueling bandits \citep{Yue+12,Yue+09,BTM,SG18, Ailon+14,Zoghi+14RUCB,Komiyama+15,Adv_DB}. Recently, there is a growing interest in PbRL methods with theoretical guarantees, including tabular case \citep{sui19,xu20} and linear and general function approximations \citep{pacchiano2021dueling, chen2022human}. However, these works are focused on the online setting and their methods are not applicable to the offline setting. 

\textbf{Offline policy learning.} The vast literature on offline policy learning can be divided by the different assumptions on the data sampling distribution. The strongest assumption is one that requires all state-action pairs can be sampled \citep{yin2021near,duan2020minimax,chen2019information,xie2020q}. A similar assumption requires that the occupancy measure of every possible policy be dominated by the data sampling distribution, which is common in the function approximation setting \citep{antos06,csaba05}. One of the weakest assumptions is one that requires the occupancy measure of an optimal policy be dominated by the data sampling distribution. To learn the optimal policy under this setting, the principle of pessimism \citep{kumar2020conservative,buckman2020importance,jin2021pessimism} was introduced and has inspired many algorithms \citep{yu2020mopo,rashidinejad2021bridging, li2022pessimism, xie2021policy, zanette2022realizability, zanette2021provable, xie2021bellman, xu2022provably}. In particular, the sample complexity of pessimistic algorithms has been studied in the tabular case \citep{shi2022pessimistic,yan2022efficacy,li2022settling,yin2021optimal,ren2021nearly,xie2021policy,yin2021towards,rashidinejad2021bridging} and linear MDPs \citep{jin2021pessimism,xie2021bellman,zanette2021provable,wang2020statistical,foster2021offline,yin2022near}. In this spirit, the algorithms we study in this work also use human feedback with pessimism.

The rest of this paper is organized as follows: Section 2 introduces some preliminaries; Section 3 presents our proposed new human rating model; Section 4 presents the main theoretical results for rating; Section 5 compares the results for human preference-based methods with the human rating results; Section 6 draws a brief conclusion.

\paragraph{Notation} Given any vector $x\in\RR^{SA}$ that represents a function $x:\cS\times\cA\to\RR$, we use $x(s,a)$ to denote the entry corresponding to the state-action pair $(s,a)$. For any random sample $X$ with distribution $P$ and a function $f(\cdot)$ of $X$, we denote the expectation of $f(X)$ over $P$ with $\EE_{X\sim P}[f(X)]$ or $\EE_{X}[f(X)]$. Similarly, we denote the variance of $f(X)$ over $P$ with $\Var_X(f(X))$. Lastly, we denote equality in distribution with $\overset{d}{=}$.
