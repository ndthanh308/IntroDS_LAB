\begin{thebibliography}{}

\bibitem[Abdelkareem et~al., 2022]{abdelkareem2022advances}
Abdelkareem, Y., Shehata, S., and Karray, F. (2022).
\newblock Advances in preference-based reinforcement learning: A review.
\newblock In {\em 2022 IEEE International Conference on Systems, Man, and
  Cybernetics (SMC)}, pages 2527--2532. IEEE.

\bibitem[Ailon et~al., 2014]{Ailon+14}
Ailon, N., Karnin, Z.~S., and Joachims, T. (2014).
\newblock Reducing dueling bandits to cardinal bandits.
\newblock In {\em ICML}, volume~32, pages 856--864.

\bibitem[Antos et~al., 2006]{antos06}
Antos, A., Szepesv\'{a}ri, C., and Munos, R. (2006).
\newblock Learning near-optimal policies with bellman-residual minimization
  based fitted policy iteration and a single sample path.
\newblock In {\em Proceedings of the 19th Annual Conference on Learning
  Theory}, COLT'06, pages 574--588, Berlin, Heidelberg. Springer-Verlag.

\bibitem[Bradley and Terry, 1952]{Bradley1952RankAO}
Bradley, R.~A. and Terry, M.~E. (1952).
\newblock Rank analysis of incomplete block designs: I. the method of paired
  comparisons.
\newblock {\em Biometrika}, 39:324.

\bibitem[Buckman et~al., 2020]{buckman2020importance}
Buckman, J., Gelada, C., and Bellemare, M.~G. (2020).
\newblock The importance of pessimism in fixed-dataset policy optimization.
\newblock {\em arXiv preprint arXiv:2009.06799}.

\bibitem[Busa-Fekete et~al., 2014]{busa14}
Busa-Fekete, R., Sz{\"o}r{\'e}nyi, B., Weng, P., Cheng, W., and
  H{\"u}llermeier, E. (2014).
\newblock Preference-based reinforcement learning: evolutionary direct policy
  search using a preference-based racing algorithm.
\newblock {\em Machine Learning}, 97(3):327--351.

\bibitem[Chen and Jiang, 2019]{chen2019information}
Chen, J. and Jiang, N. (2019).
\newblock Information-theoretic considerations in batch reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages
  1042--1051. PMLR.

\bibitem[Chen et~al., 2013]{chen13crowdsourcing}
Chen, X., Bennett, P.~N., Collins-Thompson, K., and Horvitz, E. (2013).
\newblock Pairwise ranking aggregation in a crowdsourced setting.
\newblock In {\em Proceedings of the Sixth ACM International Conference on Web
  Search and Data Mining}, WSDM '13, pages 193--202, New York, NY, USA.
  Association for Computing Machinery.

\bibitem[Chen et~al., 2022]{chen2022human}
Chen, X., Zhong, H., Yang, Z., Wang, Z., and Wang, L. (2022).
\newblock Human-in-the-loop: Provably efficient preference-based reinforcement
  learning with general function approximation.
\newblock In {\em International Conference on Machine Learning}, pages
  3773--3793. PMLR.

\bibitem[Chernoff, 1952]{Chernoff1952AMO}
Chernoff, H. (1952).
\newblock A measure of asymptotic efficiency for tests of a hypothesis based on
  the sum of observations.
\newblock {\em Annals of Mathematical Statistics}, 23:493--507.

\bibitem[Christiano et~al., 2017a]{christiano2017deep}
Christiano, P.~F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D.
  (2017a).
\newblock Deep reinforcement learning from human preferences.
\newblock {\em Advances in Neural Information Processing Systems}, 30.

\bibitem[Christiano et~al., 2017b]{nipsPRL17}
Christiano, P.~F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D.
  (2017b).
\newblock Deep reinforcement learning from human preferences.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4299--4307.

\bibitem[Dong et~al., 2017]{dong17crowdsource}
Dong, J., Yang, K., and Shi, Y. (2017).
\newblock Ranking from crowdsourced pairwise comparisons via smoothed matrix
  manifold optimization.
\newblock In {\em 2017 IEEE International Conference on Data Mining Workshops
  (ICDMW)}, pages 949--956.

\bibitem[Duan et~al., 2020]{duan2020minimax}
Duan, Y., Jia, Z., and Wang, M. (2020).
\newblock Minimax-optimal off-policy evaluation with linear function
  approximation.
\newblock In {\em International Conference on Machine Learning}, pages
  2701--2709. PMLR.

\bibitem[Foster et~al., 2021]{foster2021offline}
Foster, D.~J., Krishnamurthy, A., Simchi-Levi, D., and Xu, Y. (2021).
\newblock Offline reinforcement learning: fundamental barriers for value
  function approximation.
\newblock {\em arXiv preprint arXiv:2111.10919}.

\bibitem[Gajane et~al., 2015]{Adv_DB}
Gajane, P., Urvoy, T., and Cl{\'e}rot, F. (2015).
\newblock A relative exponential weighing algorithm for adversarial
  utility-based dueling bandits.
\newblock In {\em Proceedings of the 32nd International Conference on Machine
  Learning}, pages 218--227.

\bibitem[Hao et~al., 2021]{hao2021bootstrapping}
Hao, B., Ji, X., Duan, Y., Lu, H., Szepesv{\'a}ri, C., and Wang, M. (2021).
\newblock Bootstrapping fitted q-evaluation for off-policy inference.
\newblock In {\em International Conference on Machine Learning}, pages
  4074--4084. PMLR.

\bibitem[Janowski and Pinson, 2015]{Janowski15accuracy}
Janowski, L. and Pinson, M. (2015).
\newblock The accuracy of subjects in a quality experiment: A theoretical
  subject model.
\newblock {\em IEEE Transactions on Multimedia}, 17(12):2210--2224.

\bibitem[Ji et~al., 2023]{ji2023sample}
Ji, X., Chen, M., Wang, M., and Zhao, T. (2023).
\newblock Sample complexity of nonparametric off-policy evaluation on
  low-dimensional manifolds using deep networks.
\newblock In {\em The Eleventh International Conference on Learning
  Representations}.

\bibitem[Jin et~al., 2018]{jin2018predicting}
Jin, X., Wu, L., Li, X., Chen, S., Peng, S., Chi, J., Ge, S., Song, C., and
  Zhao, G. (2018).
\newblock Predicting aesthetic score distribution through cumulative
  jensen-shannon divergence.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~32.

\bibitem[Jin et~al., 2021]{jin2021pessimism}
Jin, Y., Yang, Z., and Wang, Z. (2021).
\newblock Is pessimism provably efficient for offline {RL}?
\newblock In {\em International Conference on Machine Learning}, pages
  5084--5096. PMLR.

\bibitem[John~Bernardin et~al., 2016]{john2016rater}
John~Bernardin, H., Thomason, S., Ronald~Buckley, M., and Kane, J.~S. (2016).
\newblock Rater rating-level bias and accuracy in performance appraisals: The
  impact of rater personality, performance management competence, and rater
  accountability.
\newblock {\em Human Resource Management}, 55(2):321--340.

\bibitem[Komiyama et~al., 2015]{Komiyama+15}
Komiyama, J., Honda, J., Kashima, H., and Nakagawa, H. (2015).
\newblock Regret lower bound and optimal algorithm in dueling bandit problem.
\newblock In {\em COLT}, pages 1141--1154.

\bibitem[Kumar et~al., 2020]{kumar2020conservative}
Kumar, A., Zhou, A., Tucker, G., and Levine, S. (2020).
\newblock Conservative {Q}-learning for offline reinforcement learning.
\newblock {\em Advances in Neural Information Processing Systems},
  33:1179--1191.

\bibitem[Lata{\l}a, 2006]{latala2006estimates}
Lata{\l}a, R. (2006).
\newblock Estimates of moments and tails of gaussian chaoses.

\bibitem[Li et~al., 2022a]{li2022pessimism}
Li, G., Ma, C., and Srebro, N. (2022a).
\newblock Pessimism for offline linear contextual bandits using $\ell_p$
  confidence sets.
\newblock {\em arXiv preprint arXiv:2205.10671}.

\bibitem[Li et~al., 2022b]{li2022settling}
Li, G., Shi, L., Chen, Y., Chi, Y., and Wei, Y. (2022b).
\newblock Settling the sample complexity of model-based offline reinforcement
  learning.
\newblock {\em arXiv preprint arXiv:2204.05275}.

\bibitem[Li et~al., 2020a]{li20crowdsourcing}
Li, J., Ling, S., Wang, J., and Le~Callet, P. (2020a).
\newblock A probabilistic graphical model for analyzing the subjective visual
  quality assessment data from crowdsourcing.
\newblock In {\em Proceedings of the 28th ACM International Conference on
  Multimedia}, MM '20, pages 3339--3347, New York, NY, USA. Association for
  Computing Machinery.

\bibitem[Li et~al., 2020b]{li2020gpm}
Li, J., Ling, S., Wang, J., Li, Z., and Callet, P.~L. (2020b).
\newblock Gpm: A generic probabilistic model to recover annotator's behavior
  and ground truth labeling.
\newblock {\em arXiv preprint arXiv:2003.00475}.

\bibitem[Li and Bampis, 2017]{li2017recover}
Li, Z. and Bampis, C.~G. (2017).
\newblock Recover subjective quality scores from noisy measurements.
\newblock In {\em 2017 Data compression conference (DCC)}, pages 52--61. IEEE.

\bibitem[Li et~al., 2020c]{li2020simple}
Li, Z., Bampis, C.~G., Krasula, L., Janowski, L., and Katsavounidis, I.
  (2020c).
\newblock A simple model for subject behavior in subjective experiments.
\newblock {\em arXiv preprint arXiv:2004.02067}.

\bibitem[Mnih et~al., 2016]{mnih2016asynchronous}
Mnih, V., Badia, A.~P., Mirza, M., Graves, A., Lillicrap, T., Harley, T.,
  Silver, D., and Kavukcuoglu, K. (2016).
\newblock Asynchronous methods for deep reinforcement learning.
\newblock In {\em International conference on machine learning}, pages
  1928--1937. PMLR.

\bibitem[Mnih et~al., 2015]{mnih2015humanlevel}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,
  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., Petersen,
  S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra,
  D., Legg, S., and Hassabis, D. (2015).
\newblock Human-level control through deep reinforcement learning.
\newblock {\em Nature}, 518(7540):529--533.

\bibitem[Novikova et~al., 2018]{novikova2018rankme}
Novikova, J., Du{\v{s}}ek, O., and Rieser, V. (2018).
\newblock Rankme: Reliable human ratings for natural language generation.
\newblock {\em arXiv preprint arXiv:1803.05928}.

\bibitem[Novoseller et~al., 2019]{sui19}
Novoseller, E.~R., Sui, Y., Yue, Y., and Burdick, J.~W. (2019).
\newblock Dueling posterior sampling for preference-based reinforcement
  learning.
\newblock {\em arXiv preprint arXiv:1908.01289}.

\bibitem[Ouyang et~al., 2022]{ouyang2022training}
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang,
  C., Agarwal, S., Slama, K., Ray, A., et~al. (2022).
\newblock Training language models to follow instructions with human feedback.
\newblock {\em Advances in Neural Information Processing Systems},
  35:27730--27744.

\bibitem[Pacchiano et~al., 2021]{pacchiano2021dueling}
Pacchiano, A., Saha, A., and Lee, J. (2021).
\newblock Dueling rl: reinforcement learning with trajectory preferences.
\newblock {\em arXiv preprint arXiv:2111.04850}.

\bibitem[P\'{e}rez-Ortiz et~al., 2020]{Ortiz20unified}
P\'{e}rez-Ortiz, M., Mikhailiuk, A., Zerman, E., Hulusic, V., Valenzise, G.,
  and Mantiuk, R.~K. (2020).
\newblock From pairwise comparisons and rating to a unified quality scale.
\newblock {\em IEEE Transactions on Image Processing}, 29:1139--1151.

\bibitem[Plackett, 1975]{Plackett1975TheAO}
Plackett, R.~L. (1975).
\newblock The analysis of permutations.
\newblock {\em Journal of The Royal Statistical Society Series C-applied
  Statistics}, 24:193--202.

\bibitem[Rashidinejad et~al., 2021]{rashidinejad2021bridging}
Rashidinejad, P., Zhu, B., Ma, C., Jiao, J., and Russell, S. (2021).
\newblock Bridging offline reinforcement learning and imitation learning: A
  tale of pessimism.
\newblock {\em Advances in Neural Information Processing Systems},
  34:11702--11716.

\bibitem[Ren et~al., 2021]{ren2021nearly}
Ren, T., Li, J., Dai, B., Du, S.~S., and Sanghavi, S. (2021).
\newblock Nearly horizon-free offline reinforcement learning.
\newblock {\em Proc. Advances in Neural Information Processing Systems
  (NeurIPS)}, 34.

\bibitem[Saha and Gopalan, 2018]{SG18}
Saha, A. and Gopalan, A. (2018).
\newblock Battle of bandits.
\newblock In {\em Uncertainty in Artificial Intelligence}.

\bibitem[Sekhari et~al., 2023]{sekhari2023contextual}
Sekhari, A., Sridharan, K., Sun, W., and Wu, R. (2023).
\newblock Contextual bandits and imitation learning via preference-based active
  queries.
\newblock {\em arXiv preprint arXiv:2307.12926}.

\bibitem[Shah et~al., 2015]{shah2015estimation}
Shah, N., Balakrishnan, S., Bradley, J., Parekh, A., Ramchandran, K., and
  Wainwright, M. (2015).
\newblock Estimation from pairwise comparisons: Sharp minimax bounds with
  topology dependence.
\newblock In {\em Artificial intelligence and statistics}, pages 856--865.
  PMLR.

\bibitem[Shi et~al., 2022]{shi2022pessimistic}
Shi, L., Li, G., Wei, Y., Chen, Y., and Chi, Y. (2022).
\newblock Pessimistic q-learning for offline reinforcement learning: Towards
  optimal sample complexity.
\newblock In {\em International Conference on Machine Learning}, pages
  19967--20025. PMLR.

\bibitem[Shin et~al., 2023]{shin2023benchmarks}
Shin, D., Dragan, A.~D., and Brown, D.~S. (2023).
\newblock Benchmarks and algorithms for offline preference-based reward
  learning.
\newblock {\em arXiv preprint arXiv:2301.01392}.

\bibitem[Silver et~al., 2016]{silver2016}
Silver, D., Huang, A., Maddison, C.~J., Guez, A., Sifre, L., van~den Driessche,
  G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M.,
  Dieleman, S., Grewe, D., Nham, J., Kalchbrenner, N., Sutskever, I.,
  Lillicrap, T., Leach, M., Kavukcuoglu, K., Graepel, T., and Hassabis, D.
  (2016).
\newblock Mastering the game of {Go} with deep neural networks and tree search.
\newblock {\em Nature}, 529(7587):484--489.

\bibitem[Szepesv\'{a}ri and Munos, 2005]{csaba05}
Szepesv\'{a}ri, C. and Munos, R. (2005).
\newblock Finite time bounds for sampling based fitted value iteration.
\newblock In {\em Proceedings of the 22nd International Conference on Machine
  Learning}, ICML '05, pages 880--887, New York, NY, USA. Association for
  Computing Machinery.

\bibitem[Tarlow et~al., 2021]{Tarlow21Reliable}
Tarlow, K.~R., Brossart, D.~F., McCammon, A.~M., Giovanetti, A.~J., Belle,
  M.~C., and Philip, J. (2021).
\newblock Reliable visual analysis of single-case data: A comparison of rating,
  ranking, and pairwise methods.
\newblock {\em Cogent Psychology}, 8(1):1911076.

\bibitem[Tenorio-Gonzalez et~al., 2010]{Gonzalez10voice}
Tenorio-Gonzalez, A.~C., Morales, E.~F., and Villase\~{n}or Pineda, L. (2010).
\newblock Dynamic reward shaping: Training a robot by voice.
\newblock In {\em Proceedings of the 12th Ibero-American Conference on Advances
  in Artificial Intelligence}, IBERAMIA'10, pages 483--492, Berlin, Heidelberg.
  Springer-Verlag.

\bibitem[Uehara et~al., 2021]{uehara2021finite}
Uehara, M., Imaizumi, M., Jiang, N., Kallus, N., Sun, W., and Xie, T. (2021).
\newblock Finite sample analysis of minimax offline reinforcement learning:
  Completeness, fast rates and first-order efficiency.
\newblock {\em arXiv preprint arXiv:2102.02981}.

\bibitem[Wainwright, 2019]{wainwright_2019}
Wainwright, M.~J. (2019).
\newblock {\em High-Dimensional Statistics: A Non-Asymptotic Viewpoint}.
\newblock Cambridge Series in Statistical and Probabilistic Mathematics.
  Cambridge University Press.

\bibitem[Wang et~al., 2020]{wang2020statistical}
Wang, R., Foster, D.~P., and Kakade, S.~M. (2020).
\newblock What are the statistical limits of offline rl with linear function
  approximation?
\newblock {\em arXiv preprint arXiv:2010.11895}.

\bibitem[Wang et~al., 2023]{wang2023rlhf}
Wang, Y., Liu, Q., and Jin, C. (2023).
\newblock Is rlhf more difficult than standard rl?
\newblock {\em arXiv preprint arXiv:2306.14111}.

\bibitem[Warfield et~al., 2008]{warfield2008validation}
Warfield, S.~K., Zou, K.~H., and Wells, W.~M. (2008).
\newblock Validation of image segmentation by estimating rater bias and
  variance.
\newblock {\em Philosophical Transactions of the Royal Society A: Mathematical,
  Physical and Engineering Sciences}, 366(1874):2361--2375.

\bibitem[Wirth et~al., 2017]{wirth17}
Wirth, C., Akrour, R., Neumann, G., and F{\"u}rnkranz, J. (2017).
\newblock A survey of preference-based reinforcement learning methods.
\newblock {\em The Journal of Machine Learning Research}, 18(1):4945--4990.

\bibitem[Wirth et~al., 2016]{wirth16}
Wirth, C., Furnkranz, J., Neumann, G., et~al. (2016).
\newblock Model-free preference-based reinforcement learning.
\newblock In {\em 30th AAAI Conference on Artificial Intelligence, AAAI 2016},
  pages 2222--2228.

\bibitem[Xie et~al., 2021a]{xie2021bellman}
Xie, T., Cheng, C.-A., Jiang, N., Mineiro, P., and Agarwal, A. (2021a).
\newblock Bellman-consistent pessimism for offline reinforcement learning.
\newblock {\em Advances in Neural Information Processing Systems},
  34:6683--6694.

\bibitem[Xie and Jiang, 2020]{xie2020q}
Xie, T. and Jiang, N. (2020).
\newblock Q* approximation schemes for batch reinforcement learning: A
  theoretical comparison.
\newblock In {\em Conference on Uncertainty in Artificial Intelligence}, pages
  550--559. PMLR.

\bibitem[Xie et~al., 2021b]{xie2021policy}
Xie, T., Jiang, N., Wang, H., Xiong, C., and Bai, Y. (2021b).
\newblock Policy finetuning: Bridging sample-efficient offline and online
  reinforcement learning.
\newblock {\em Advances in Neural Information Processing Systems},
  34:27395--27407.

\bibitem[Xu and Liang, 2022]{xu2022provably}
Xu, T. and Liang, Y. (2022).
\newblock Provably efficient offline reinforcement learning with
  trajectory-wise reward.
\newblock {\em arXiv preprint arXiv:2206.06426}.

\bibitem[Xu et~al., 2020]{xu20}
Xu, Y., Wang, R., Yang, L., Singh, A., and Dubrawski, A. (2020).
\newblock Preference-based reinforcement learning with finite-time guarantees.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.~F., and Lin,
  H., editors, {\em Advances in Neural Information Processing Systems},
  volume~33, pages 18784--18794. Curran Associates, Inc.

\bibitem[Yan et~al., 2022]{yan2022efficacy}
Yan, Y., Li, G., Chen, Y., and Fan, J. (2022).
\newblock The efficacy of pessimism in asynchronous q-learning.
\newblock {\em arXiv preprint arXiv:2203.07368}.

\bibitem[Yannakakis and Mart{\'\i}nez, 2015]{yannakakis2015ratings}
Yannakakis, G.~N. and Mart{\'\i}nez, H.~P. (2015).
\newblock Ratings are overrated!
\newblock {\em Frontiers in ICT}, 2:13.

\bibitem[Yin et~al., 2021]{yin2021near}
Yin, M., Bai, Y., and Wang, Y.-X. (2021).
\newblock Near-optimal provable uniform convergence in offline policy
  evaluation for reinforcement learning.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 1567--1575. PMLR.

\bibitem[Yin et~al., 2022]{yin2022near}
Yin, M., Duan, Y., Wang, M., and Wang, Y.-X. (2022).
\newblock Near-optimal offline reinforcement learning with linear
  representation: Leveraging variance information with pessimism.
\newblock In {\em International Conference on Learning Representation}.

\bibitem[Yin and Wang, 2021a]{yin2021optimal}
Yin, M. and Wang, Y.-X. (2021a).
\newblock Optimal uniform ope and model-based offline reinforcement learning in
  time-homogeneous, reward-free and task-agnostic settings.
\newblock {\em Proc. Advances in Neural Information Processing Systems
  (NeurIPS)}, 34.

\bibitem[Yin and Wang, 2021b]{yin2021towards}
Yin, M. and Wang, Y.-X. (2021b).
\newblock Towards instance-optimal offline reinforcement learning with
  pessimism.
\newblock {\em Proc. Advances in Neural Information Processing Systems
  (NeurIPS)}, 34.

\bibitem[Yu et~al., 2020]{yu2020mopo}
Yu, T., Thomas, G., Yu, L., Ermon, S., Zou, J.~Y., Levine, S., Finn, C., and
  Ma, T. (2020).
\newblock Mopo: Model-based offline policy optimization.
\newblock {\em Advances in Neural Information Processing Systems},
  33:14129--14142.

\bibitem[Yue et~al., 2012]{Yue+12}
Yue, Y., Broder, J., Kleinberg, R., and Joachims, T. (2012).
\newblock The $k$-armed dueling bandits problem.
\newblock {\em Journal of Computer and System Sciences}, 78(5):1538--1556.

\bibitem[Yue and Joachims, 2009]{Yue+09}
Yue, Y. and Joachims, T. (2009).
\newblock Interactively optimizing information retrieval systems as a dueling
  bandits problem.
\newblock In {\em Proceedings of the 26th Annual International Conference on
  Machine Learning}, pages 1201--1208. ACM.

\bibitem[Yue and Joachims, 2011]{BTM}
Yue, Y. and Joachims, T. (2011).
\newblock Beat the mean bandit.
\newblock In {\em Proceedings of the 28th International Conference on Machine
  Learning (ICML-11)}, pages 241--248.

\bibitem[Zanette, 2022]{zanette2022realizability}
Zanette, A. (2022).
\newblock When is realizability sufficient for off-policy reinforcement
  learning?
\newblock {\em arXiv preprint arXiv:2211.05311}.

\bibitem[Zanette et~al., 2021]{zanette2021provable}
Zanette, A., Wainwright, M.~J., and Brunskill, E. (2021).
\newblock Provable benefits of actor-critic methods for offline reinforcement
  learning.
\newblock {\em Advances in Neural Information Processing Systems},
  34:13626--13640.

\bibitem[Zhan et~al., 2023a]{zhan2023provable}
Zhan, W., Uehara, M., Kallus, N., Lee, J.~D., and Sun, W. (2023a).
\newblock Provable offline reinforcement learning with human feedback.
\newblock {\em arXiv preprint arXiv:2305.14816}.

\bibitem[Zhan et~al., 2023b]{zhan2023query}
Zhan, W., Uehara, M., Sun, W., and Lee, J.~D. (2023b).
\newblock How to query human feedback efficiently in rl?
\newblock {\em arXiv preprint arXiv:2305.18505}.

\bibitem[Zhu et~al., 2023]{zhu2023principled}
Zhu, B., Jiao, J., and Jordan, M.~I. (2023).
\newblock Principled reinforcement learning with human feedback from pairwise
  or $ k $-wise comparisons.
\newblock {\em arXiv preprint arXiv:2301.11270}.

\bibitem[Zoghi et~al., 2014]{Zoghi+14RUCB}
Zoghi, M., Whiteson, S., Munos, R., Rijke, M.~d., et~al. (2014).
\newblock Relative upper confidence bound for the $k$-armed dueling bandit
  problem.
\newblock In {\em JMLR Workshop and Conference Proceedings}, number~32, pages
  10--18. JMLR.

\end{thebibliography}
