\documentclass[english]{article}
\usepackage{geometry}
\geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{bm}
\usepackage{amsmath,mathtools}
\usepackage{amssymb}
\usepackage{natbib}
\usepackage[unicode=true,bookmarks=false,breaklinks=false,pdfborder={0 0 1},colorlinks=false]{hyperref}
\hypersetup{colorlinks,citecolor=blue,filecolor=blue,linkcolor=blue,urlcolor=blue}

\setlength{\parindent}{0em}
\setlength{\parskip}{1em}
\usepackage{smile}

\title{\textbf{Provable Benefits of Policy Learning from Human
Preferences in Contextual Bandit Problems}}
\author{Xiang Ji\footnote{Department of Electrical and Computer Engineering, School of Engineering and Applied Science, Princeton University, Princeton, NJ 08544, USA.} \hspace{0.37in} Huazheng Wang\footnote{School of Electrical Engineering and Computer Science, College of Engineering, Oregon State University, Corvallis, Oregon, OR 97331, USA.} \hspace{0.37in} Minshuo Chen$^*$ \hspace{0.37in} Tuo Zhao\footnote{H. Milton Stewart School of Industrial and Systems Engineering, College of Engineering, Georgia Institute of Technology, Atlanta, GA 30332, USA.} \hspace{0.37in} Mengdi Wang$^*$
}

\date{}

\begin{document}


\maketitle


\begin{abstract}
  For a real-world decision-making problem, the reward function often needs to be engineered or learned. A popular approach is to utilize human feedback to learn a reward function for training. The most straightforward way to do so is to ask humans to provide ratings for state-action pairs on an absolute scale and take these ratings as reward samples directly. Another popular way is to ask humans to rank a small set of state-action pairs by preference and learn a reward function from these preference data. Recently, preference-based methods have demonstrated substantial success in empirical applications such as InstructGPT. In this work, we develop a theoretical comparison between these human feedback approaches in offline contextual bandits and show how human bias and uncertainty in feedback modelings can affect the theoretical guarantees of these approaches. Through this, our results seek to provide a theoretical explanation for the empirical successes of preference-based methods from a modeling perspective. 
\end{abstract}

\allowdisplaybreaks
\input{chapters/intro}

\input{chapters/prelim}

\input{chapters/formulation}

\input{chapters/main_results}

\input{chapters/conclusion}

\bibliography{main}
\bibliographystyle{apalike}

\newpage

\appendix

\input{chapters/appendix}

\end{document}