\section{Conclusion}

In this work, we have studied policy learning using human feedback for reward engineering in bandit. Specifically, we have provided a theoretical comparison between human rating methods and preference-based methods, which shows human bias and uncertainty can have considerable adverse effect on policy learning. Our theory also suggests the preference-based approach has no provable advantage over the traditional rating-based approach when the two types of human feedback are modeled with equally strong human bias and uncertainty. This implies the reason for the empirical success of preference-based methods might be that human preference data are subject to milder human bias and uncertainty. Beyond this work, it is still open for future work to investigate the case when the human feedback is generated from a mixture model representing a group of annotators and provide a comparison between rating methods and preference-based methods in this setting. 
