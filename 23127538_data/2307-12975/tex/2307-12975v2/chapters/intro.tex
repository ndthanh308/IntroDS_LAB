\section{Introduction}

Reward engineering is one of the most crucial aspects in real-world decision-making problems. It is particularly important to bandits and reinforcement learning (RL), since it can be prohibitively expensive to learn the entire environment through random exploration and most existing algorithms rely on a reward function to guide their exploration in a deliberate manner so that they can solve the desired tasks efficiently. 

In some cases, it can be straightforward to select reward functions when we have sufficient prior knowledge about the dynamics or rules that govern the problems of interest, such as games and simulated physical systems \citep{mnih2015humanlevel,mnih2016asynchronous,silver2016}. However, this is often not the case in practice. Real-world problems can be highly complex, and there may not be a clear choice of reward to use. Therefore, practitioners often have to construct a reward function from scratch for their algorithms to use. Unfortunately, such artificial rewards often end up misaligned with the overall objective of the system and fail to lead to the desired policy. For instance, in the task of teaching a chatbot to converse like a human, assigning a scalar reward to a chatbot's reply is challenging since there is no scale that can objectively evaluate its quality. Therefore, reward engineering poses a significant challenge to policy learning, particularly when it is difficult to quantify policy performance or the system is multi-objective.

To address these challenges, a popular methodology is to learn the reward function from human feedback instead of handcrafting it from scratch. These methods assume a true reward function exists and its corresponding optimal policy is aligned with our goal, but this true reward is not accessible or directly observable. Instead, it needs to be learned with the feedback data from human annotators, who are believed to be able to evaluate an algorithm or agent in a way aligned with the true reward. The most straightforward approach is to ask human annotators to rate subjects on an absolute scale \citep{li20crowdsourcing,li2020gpm,Janowski15accuracy}. These ratings can be either used directly as reward samples or incorporated into a pre-designed reward function as a component \citep{Gonzalez10voice}. We refer to this use of human feedback as ``rating''. The rating approach has been popular because of its easy implementation and the compatibility of such rating data with most existing algorithms. However, as some empirical studies have shown \citep{warfield2008validation,john2016rater}, the reward derived from human ratings is susceptible to bias and uncertainty of the human annotators and can deviate from the true reward. To characterize the ratings given by human annotators, several models have been proposed and widely adopted in the literature \citep{Ortiz20unified,li2017recover}. These existing models, however, fall short on two aspects: (i) they model the uncertainty noise in a simple, isolated form, and (ii) their modeling of the bias is also restrictive, which does not fully capture the bias in practice and can render the problem of policy learning statistically inconsistent.

As an alternative, there has been a growing trend in recent years to use human feedback by comparing subjects rather than rating them individually. These methods are commonly referred to as preference-based methods. In lieu of assigning ratings on an absolute scale, human annotators are given small sets of subjects and tasked with comparing and ranking the subjects within each set. Since some empirical studies have shown that humans are fairly accurate when making choices among a small number of subjects \citep{novikova2018rankme,Tarlow21Reliable,yannakakis2015ratings}, the preference-based approach is believed to be more robust to human bias and uncertainty and learn reward more accurately. The preference feedback of human annotators is commonly modeled with either the Bradley-Terry-Luce model \citep{Bradley1952RankAO} or the Plackett-Luce model \citep{Plackett1975TheAO}, both of which have found extensive applications in recommender systems and crowdsourcing research \citep{chen13crowdsourcing,shah2015estimation,dong17crowdsource}. Recently, preference-based methods have become popular for reward learning in bandit problems and have played a crucial role in the remarkable success of training large language models such as InstructGPT and ChatGPT.

While the preference-based approach has demonstrated notable empirical effectiveness in reward engineering, its theoretical properties remain largely unexplored. Existing results have primarily focused on algorithms for online bandit and RL, whose goal is to maximize a preference metric rather than to learn a reward function \citep{pacchiano2021dueling,xu20}. Recently, \cite{zhu2023principled,zhan2023provable} proved the optimal policy can be learned from preference data in the offline setting with pessimism and maximum likelihood estimation (MLE) and analyzed the suboptimality. \cite{wang2023rlhf} showed any robust RL algorithm can be adapted to find the optimal policy with preference data, suggesting preference-based policy learning is no harder than standard robust RL. However, the reason why the preference-based approach outperforms the traditional rating-based approach in practice still remains a question. In this work, we provide a theoretical comparison between these human feedback approaches and propose a theory that aims to explain the advantage of the preference-based approach over rating in policy learning from a modeling perspective. To align with recent applications \citep{christiano2017deep,ouyang2022training}, we focus on tabular contextual bandits in the offline setting.

Specifically, we first consider a new model for human rating data and analyze the suboptimality guarantees of the standard LCB algorithm under it. Our rating model is based on a general class of monotone functions that can account for both human bias and uncertainty with general forms. By incorporating the concept of monotonicity, our model captures the bias observed in real-world human rating and maintains the correct reward ordering. This allows policy learning to be a consistent yet nontrivial statistical problem, which differs from existing rating models that do not preserve the reward ordering or guarantee the consistency of the induced policy learning problem. In addition, our model is able to express a more general form of noise to represent human uncertainty during rating. Through our models, we provide the first known suboptimality analysis for reward engineering with human rating in bandit problems and shed light on how human bias and uncertainty can adversely impact policy learning. Furthermore, we compare our results with the suboptimality result from \cite{zhu2023principled} for the preference-based method pessimistic MLE. The comparison reveals that the preference-based approach enjoys lower suboptimality than the rating-based approach when human bias is extreme in human rating. Lastly, we also consider a new model for human preference with human bias and compare the sample complexity of pessimistic MLE under this new model with the results for human rating. This comparison shows when human bias and uncertainty are equally strong in both types of human feedback, the preference-based approach has no provable advantage over the rating-based one. Altogether, our theory shows the advantage of the preference-based approach can be largely attributed to its modeling with mild human bias and uncertainty, which makes it reasonable to believe the great empirical success of preference-based methods is because human preference data is subject to less bias and uncertainty in practice.

\subsection{Related Works}

\textbf{Preference-based reinforcement learning.} 
Preference-based Reinforcement Learning (PbRL) \citep{christiano2017deep, shin2023benchmarks, busa14,wirth16,wirth17,nipsPRL17, abdelkareem2022advances} has been studied under different types of human feedback including action comparison, state comparison and trajectory comparison---see \cite{wirth17,abdelkareem2022advances} for reviews of the literature. Preference-based feedback has been well-studied in a bandit setting known as dueling bandits \citep{Yue+12,Yue+09,BTM,SG18, Ailon+14,Zoghi+14RUCB,Komiyama+15,Adv_DB,sekhari2023contextual}. Recently, there is a growing interest in the theoretical guarantees of PbRL methods, including tabular case \citep{sui19,xu20} and linear and general function approximations \citep{pacchiano2021dueling, chen2022human,zhan2023query}. However, these works are focused on the online setting and their methods are not applicable to the offline setting. 

\textbf{Offline policy learning.} The vast literature on offline policy learning can be divided by the different assumptions on the data sampling distribution. The strongest assumption is one that requires all state-action pairs can be sampled \citep{yin2021near,duan2020minimax,chen2019information,xie2020q}. A similar assumption requires that the occupancy measure of every possible policy be dominated by the data sampling distribution, which is common in the function approximation setting \citep{antos06,csaba05}. One of the weakest assumptions is one that requires the occupancy measure of an optimal policy be dominated by the data sampling distribution. To learn the optimal policy under this setting, the principle of pessimism \citep{kumar2020conservative,buckman2020importance,jin2021pessimism} was introduced and has inspired many algorithms \citep{yu2020mopo,rashidinejad2021bridging, li2022pessimism, xie2021policy, zanette2022realizability, zanette2021provable, xie2021bellman, xu2022provably}. In particular, the sample complexity of pessimistic algorithms has been extensively studied in the tabular case \citep{shi2022pessimistic,yan2022efficacy,li2022settling,yin2021optimal,ren2021nearly,xie2021policy,yin2021towards,rashidinejad2021bridging} and linear MDPs \citep{jin2021pessimism,xie2021bellman,zanette2021provable,wang2020statistical,foster2021offline,yin2022near}. In this spirit, the algorithms we study in this work also use human feedback with pessimism.

\paragraph{Notation} Given any vector $x\in\RR^{SA}$ that represents a function $x:\cS\times\cA\to\RR$, we use $x(s,a)$ to denote the entry corresponding to the state-action pair $(s,a)$. For any random sample $X$ with distribution $P$ and a function $f(\cdot)$ of $X$, we denote the expectation of $f(X)$ over $P$ with $\EE_{X\sim P}[f(X)]$ or $\EE_{X}[f(X)]$. Similarly, we denote the variance of $f(X)$ over $P$ with $\Var_X(f(X))$. Lastly, we denote equality in distribution with $\overset{d}{=}$.