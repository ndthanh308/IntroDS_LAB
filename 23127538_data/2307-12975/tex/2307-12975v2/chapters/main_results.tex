\section{Results for Human Rating}

Before the theoretical comparison with the preference-based approach, let us first establish some theoretical results for our more general rating model. In particular, we analyze the suboptimality of the LCB algorithm under our more practical rating model. These results can provide some theoretical explanation for how human bias and uncertainty could adversely affect policy learning.

In the case of human rating, we are given an offline dataset $\cD = \{(s_i,a_i, \widetilde{r}_i)\}_{i=1}^{n}$. The state-action pairs in $\cD$ are generated in an i.i.d. fashion according to a sampling distribution over the state-action space. The sampling probability of the state-action pair $(s,a)$ is denoted with $d(s,a)$. For each $(s_i,a_i)$, the human annotator provides a \textit{rating sample} $\widetilde{r}_i$ following the rating model \eqref{eq:rating-h-model} based on the true reward $r(s_i,a_i)$.

Let us also make a brief review of the standard LCB approach for offline policy learning \citep{jin2021pessimism,rashidinejad2021bridging,yin2021towards}. In the existing literature, it is common to assume the knowledge of a reasonable upper bound on the variance of reward observations. Similarly, we assume there exists an upper bound on the variance $\Var_\epsilon(h(r,\epsilon))$ for all $r\in[0,R]$, which we denote with $V_{R, \sigma}^2$ and can depend on $R$ and $\sigma$. Recall that the learner has no knowledge of the transformation $h$, but let us assume the learner can make a reasonable estimate $\widetilde{V}_{R,\sigma}^2$ for the true variance $V_{R, \sigma}^2$ such that $\widetilde{V}_{R, \sigma}^2 = c_VV_{R, \sigma}^2$, where $c_V > 0$ is an absolute constant. To learn the optimal policy with at least $1-\delta$ success probability, the standard LCB algorithm (Algorithm \ref{alg:LCB}) uses a penalty in the form of
\begin{equation}\label{eq:standard-LCB}
    b_m = c_b \sqrt{\frac{\widetilde{V}_{R,\sigma}^2\log\frac{SA}{\delta}}{m}}
\end{equation}
with an appropriately chosen constant $c_b$. 

To understand the effects of human bias and uncertainty on policy learning under our more realistic rating model, let us establish the lower bound on the suboptimality of the LCB algorithm. We will consider two scenarios with different coverage assumptions for the offline dataset $\cD$.

\begin{algorithm}[t]
    \caption{LCB for contextual bandits} \label{alg:LCB}
    \begin{algorithmic}[1]
        \STATE \textbf{Input:} Offline dataset $\cD$, confidence level $\delta\in(0,1)$.

        \FOR{all $(s,a)\in\cS\times\cA$}
            \STATE Set $n_{(s,a)} = \sum_{i=1}^{n}\ind\{(s_i,a_i) = (s,a)\}$;

            \STATE Set $\widetilde{r}(s,a) = \frac{1}{n}\sum_{i=1}^{n} \widetilde{r}_{i}\ind\{(s_i,a_i) = (s,a)\}$;

            \STATE Set $\widehat{r}(s,a) = \max\{\widetilde{r}(s,a) - b_{n_{(s,a)}}, 0\}$;
        \ENDFOR

        \RETURN $\widehat{\pi}_{\rm LCB}(\cdot) = \arg\max_{a\in\cA}\widehat{r}(\cdot,a)$.
    \end{algorithmic}
\end{algorithm}

\subsection{Lower Bound under Partial Coverage}

As \cite{rashidinejad2021bridging,yin2021towards} have shown, to learn the optimal policy in the offline setting, it is sufficient for the sampling distribution of the offline dataset to cover the state-action pairs that the optimal policy can reach. Concretely, this assumption can be written as follows:
\begin{assumption}\label{assumption:Cstar}
    \textit{There exists an optimal policy $\pi^\star$ such that $d(s,a) > 0$ whenever $d^{\pi^\star}_\rho(s,a) > 0$ for any $(s,a)\in\cS\times\cA$.}
\end{assumption}
Under this assumption, it makes sense to define a concentrability coefficient $C^\star$ as follows:
\begin{equation}
    C^\star := \max_{(s,a)\in\cX}\frac{d^{\pi^\star}_\rho(s,a)}{d(s,a)},
\end{equation}
where the set $\cX$ is the set of all state-action pairs that the sampling distribution of $\cD$ can cover, i.e., $\cX := \{(s,a)\in\cS\times\cA ~:~ d(s,a)> 0\}$. Under Assumption \ref{assumption:Cstar}, if the reward can be observed exactly or with only additive sub-gaussian noise, the LCB algorithm (Algorithm \ref{alg:LCB}) with penalty \eqref{eq:standard-LCB} is guaranteed to converge to the optimal policy \citep{rashidinejad2021bridging,yin2021towards}. However, theory suggests it does not converge in the worst case when the reward function is engineered from human rating. In particular, let us consider the setting beyond the standard additive sub-gaussian noise, which has been well-studied in the existing literature. That is, let us consider a more practical model in the form of \eqref{eq:rating-h-model} with $q \ge 2$. We can prove that even when the rating model preserves the correct reward ordering in expectation and keeps the policy learning problem consistent, it is possible that the LCB algorithm does not converge to the optimal policy and must suffer constant suboptimality.

\begin{theorem}\label{thm:Cstar}
    \textit{For any fixed constant $0 < \delta < 1$, there exists a contextual bandit instance with initial state distribution $\rho$ such that if one samples a dataset $\cD$ of size $n \ge c(\delta,c_b,c_V,q,\sigma,R)$ using a sampling distribution $d$ satisfying Assumption \ref{assumption:Cstar} with $C^\star = 2$ and runs Algorithm \ref{alg:LCB} on $\cD$, the output policy $\widehat{\pi}_{\rm LCB}$ must suffer constant suboptimality, i.e., 
    \begin{equation}
        \EE_\cD[\SubOpt(\widehat{\pi}_{\rm LCB})] = c_0 R,
    \end{equation}
    where $c_0$ is a universal constant and $c(\delta,c_b,c_V,q,\sigma,R)$ is a constant depending on $\delta,c_b,c_V,q,\sigma,R$.}
\end{theorem}

This result is reminiscent of Proposition 1 in \cite{rashidinejad2021bridging}, which constructs a bandit and shows the empirically best policy chooses a suboptimal action with constant probability under Assumption \ref{assumption:Cstar}. The very same work also shows that by adding a pessimism penalty, the LCB algorithm (Algorithm \ref{alg:LCB}) can converge to the optimal policy under the same data coverage assumption. In contrast, our theorem shows that even when we make pessimistic estimates and penalize less-observed state-action pairs in human rating data, a constant suboptimality can still ensue. This shows a disadvantage of using human rating as reward samples directly: although the estimation problem induced by human rating is still consistent, using LCB with only the knowledge of variance is not sufficient for convergence. Instead, the learner needs to know the shape of the noise distribution, but it is unrealistic to model the human uncertainty accurately in practice. 

\textbf{Proof sketch} In a bandit instance with special reward design, we first find the lower bound for the probability that suboptimal actions are only observed for a very small number of times in the offline dataset. Such state-action pairs can have huge fluctuation in their empirical reward average and mislead the algorithm. Then, we find the lower bound on the probability that a state-action pair $(s,a)$ such that $\widehat{r}(s,a) > \widehat{r}(s,a^\star)$ exists, which can cause the algorithm to always select the suboptimal action $a$ and suffer suboptimality. Different from Proposition 1 in \cite{rashidinejad2021bridging}, in which the reward noise for suboptimal actions is defined with two Dirac delta functions, the noise under our rating model is unbounded and can be viewed as a Gaussian chaos, so we compute this probability using a method from the corresponding literature. Moreover, in the same paper, a bandit instance is sufficient to induce constant suboptimality as long as its action space is designed large. In our case, since the pessimism penalty in Algorithm \ref{alg:LCB} accounts for the bandit size and larger bandit instances are penalized more, it requires a careful balance in the design of our bandit instance.

For concreteness, let us also provide a corollary under the example rating model in \eqref{eq:h-example} as follows.

\begin{corollary}\label{cor:Cstar}
    \textit{For any fixed constant $0 < \delta < 1$, there exists a contextual bandit instance with initial state distribution $\rho$ such that if one samples a dataset $\cD$ of size $n \ge c(\delta,c_b,c_V,\sigma,R)$ using a sampling distribution $d$ satisfying Assumption \ref{assumption:Cstar} with $C^\star = 2$ and runs Algorithm \ref{alg:LCB} on $\cD$, the output policy $\widehat{\pi}_{\rm LCB}$ must suffer constant suboptimality, i.e., 
    \begin{equation}
        \EE_\cD[\SubOpt(\widehat{\pi}_{\rm LCB})] = c_0,
    \end{equation}
    where $c_0$ is a universal constant and $c(\delta,c_b,c_V,\sigma,R)$ is a constant depending on $\delta,c_b,c_V,\sigma,R$.}
\end{corollary}

\subsection{Lower Bound under Full Coverage}

Uniform coverage is another popular coverage assumption for offline policy learning \citep{yin2021near,uehara2021finite,hao2021bootstrapping}. It can be written as follows:
\begin{assumption}\label{assumption:uniform}
    \textit{The sampling distribution satisfies $d(s,a) > 0$ for any $(s,a)\in\cS\times\cA$.}
\end{assumption}
This coverage assumption is much stronger than Assumption \ref{assumption:Cstar} and makes the offline policy learning problem much easier. Under Assumption \ref{assumption:uniform}, many algorithms without the pessimism principle can also be shown to provably converge to the optimal policy \citep{chen2019information,xie2020q}. Moreover, \cite{jin2021pessimism} showed that the suboptimality of algorithms with pessimism can decay faster when the data are well-explored. In this setting, we establish a lower bound on the suboptimality of Algorithm \ref{alg:LCB} under Assumption \ref{assumption:uniform}. 

\begin{theorem}\label{thm:uniform}
    \textit{For any fixed constant $0 < \delta < 1$, there exists a contextual bandit instance with initial state distribution $\rho$ such that if one samples a dataset $\cD$ of size $n \ge \max\{48\sigma^4, 60\}$ using a sampling distribution $d$ satisfying Assumption \ref{assumption:uniform} with $d(s,a) = \frac{1}{SA}$ for every $s\in\cS$ and $a\in\cA$ and runs Algorithm \ref{alg:LCB} on $\cD$, the output policy $\widehat{\pi}_{\rm LCB}$ must suffer suboptimality at least
    \begin{equation*}
        \EE_\cD[\SubOpt(\widehat{\pi}_{\rm LCB})] = c_0\cdot \bar{h}^{-1}\left(\sqrt{\frac{V_{R,\sigma}^2}{n}}\right),
    \end{equation*}
    where $c_0$ is a constant that depends on $q$.}
\end{theorem}

In fact, under uniform data coverage as in Theorem \ref{thm:uniform}, pessimism becomes unnecessary and this result holds no matter what penalty $b_n$ is used in the algorithm. This theorem demonstrates another disadvantage of human rating: even when the data covers the entire state-action space and learning is no longer impeded by the lack of knowledge of human uncertainty, the suboptimality is still bottlenecked by human bias.

We also provide a corollary corresponding to the example model in \eqref{eq:h-example}, which shows the suboptimality can decay more slowly under the influence of annotator bias.

\begin{corollary}\label{cor:uniform}
    \textit{For any fixed constant $0 < \delta < 1$, there exists a contextual bandit instance with initial state distribution $\rho$ such that if one samples a dataset $\cD$ of size $n \ge \max\{48\sigma^4, 60\}$ using a sampling distribution $d$ satisfying Assumption \ref{assumption:uniform} with $d(s,a) = \frac{1}{SA}$ for every $s\in\cS$ and $a\in\cA$ and runs Algorithm \ref{alg:LCB} on $\cD$, the output policy $\widehat{\pi}_{\rm LCB}$ must suffer suboptimality at least
    \begin{equation*}
        \EE_\cD[\SubOpt(\widehat{\pi}_{\rm LCB})] = \frac{c_0\sigma}{n^{1/4}},
    \end{equation*}
    where $c_0$ is a universal constant.}
\end{corollary}

\subsection{Upper Bound with Knowledge of Noise Distribution}

To compare with the negative results for human rating under partial coverage (Assumption \ref{assumption:Cstar}), we prove an upper bound on the suboptimality of the LCB algorithm (Algorithm \ref{alg:LCB}) in the most benign case that the learner has full knowledge of the uncertainty noise distribution of the rating model and design the LCB penalty $b_n$ accordingly. This assumes the learner is able to find the confidence interval with any $\delta$, which is equivalent to knowing the cumulative density function of the distribution and can be unrealistic for real human feedback data in practice. This upper bound result provides a more direct comparison with the preference-based approach and demonstrates how human bias can affect the suboptimality when the uncertainty noise can be coped with.

\begin{theorem}\label{thm:upper-bound}
    Suppose Assumption \ref{assumption:Cstar} holds. For any fixed constant $0 < \delta < 1$, if one runs Algorithm \ref{alg:LCB} with 
    \begin{equation*}
        b_m = c_b\sqrt{\frac{V^2_{R,\sigma}\log^q\frac{SA}{\delta}}{m}}
    \end{equation*}
    and an appropriately chosen universal constant $c_b$, as soon as $n > 8\log\frac{2SA}{\delta}/\bar{d}$, where $\bar{d} := \min_{(s,a)\in\cX}d(s,a)$ and $\cX := \{(s,a)\in\cS\times\cA ~:~ d(s,a)> 0\}$, with probability $1-\delta$, the suboptimality of the output policy $\widehat{\pi}_{\rm LCB}$ satisfies
    \begin{equation*}
        \SubOpt(\widehat{\pi}_{\rm LCB}) \le c_0\sum_{(s,a)\in\cX}d_\rho^{\pi^\star}(s,a)\cdot \bar{h}^{-1}\left(\sqrt{\frac{V_{R,\sigma}^2\log^q\frac{SA}{\delta}}{n\cdot d(s,a)}}\right),
    \end{equation*}
    where $c_0$ is a constant that depends on $q$.
\end{theorem}

This theorem shows that even when the algorithm has full knowledge of the human uncertainty in the rating model, human bias can still influence the suboptimality of $\widehat{\pi}_{\rm LCB}$ negatively. It demonstrates the effect of bias on the suboptimality is truly unavoidable when using human rating directly. This can be further illustrated with our example model \eqref{eq:h-example} as follows, which shows the suboptimality still decays more slowly because of the quadratic human bias.

\begin{corollary}\label{cor:upper-bound}
    Suppose Assumption \ref{assumption:Cstar} holds. For any fixed constant $0 < \delta < 1$, if one runs Algorithm \ref{alg:LCB} with 
    \begin{equation*}
        b_m = \sqrt{\frac{64\sigma^4\log\frac{SA}{\delta}}{m}} + \frac{8\sigma^2\log\frac{SA}{\delta}}{m},
    \end{equation*}
    as soon as $n > 8\log\frac{2SA}{\delta}/\bar{d}$, where $\bar{d} := \min_{(s,a)\in\cX}d(s,a)$ and $\cX := \{(s,a)\in\cS\times\cA ~:~ d(s,a)> 0\}$, with probability $1-\delta$, the suboptimality of the output policy $\widehat{\pi}_{\rm LCB}$ satisfies
    \begin{equation*}
        \SubOpt(\widehat{\pi}_{\rm LCB}) \le 2\sum_{(s,a)\in\cX}d_\rho^{\pi^\star}(s,a)\left(\frac{256\sigma^4\log\frac{SA}{\delta}}{n\cdot d(s,a)}\right)^{1/4} + d_\rho^{\pi^\star}(s,a)\sqrt{\frac{32\sigma^2\log\frac{SA}{\delta}}{n\cdot d(s,a)}}.
    \end{equation*}
\end{corollary}

\section{Comparison with Preference-based Methods}

In contrast to rating, the preference-based approach relies on models that characterize how a human annotator would rank a group of subjects by reward. In this case, the feedback is simply the most preferred subject to the human annotator within the group. Such feedback actually contains less information than rating. Preference data are also incompatible with standard bandit algorithms and require special adaptation to use \citep{wang2023rlhf}. However, the preference-based approach has received much attention recently because some have found it easier and more accurate for human to make preferences than rating \citep{novikova2018rankme,Tarlow21Reliable,yannakakis2015ratings}. In this section, we compare the human rating approach with the preference-based approach. 

\subsection{Human Preference under BTL}

Let us consider the most basic case of human preference called pairwise comparison, which involves the ranking between a pair of state-action pairs based on their rewards. This is predominantly modeled with the Bradley-Terry-Luce (BTL) model \citep{Bradley1952RankAO}, under which a human annotator gives a binary response $y = \{0,1\}$ following a Bernoulli distribution when asked to compare two state-action pairs $(s,a^0)$ and $(s,a^1)$ with $a^0 \neq a^1$:
\begin{equation}\label{eq:BTL}
    P(y|s,a,a') = \frac{\exp(r(s,a^y))}{\exp(r(s,a^0))+\exp(r(s,a^1))}.
\end{equation} 

Like our rating model in \eqref{eq:rating-h-model}, the BTL model admits a consistent statistical problem. The learner is given a dataset $\cD' = \{(s_i,a_i^0,a_i^1,y_i)\}_{i=1}^{n}$, which contains i.i.d. human preference samples from some sampling distribution. $y_i$ is the binary human preference feedback for the comparison between $(s_i,a_i^0)$ and $(s_i,a_i^1)$. We denote the sampling probability of the state-action-action triplet $(s,a^0,a^1)$ with $d(s,a^0,a^1)$.

\begin{algorithm}[t]
    \caption{Pessimistic MLE for contextual bandits} \label{alg:LCB-comp}
    \begin{algorithmic}[1]
        \STATE \textbf{Input:} Offline dataset $\cD'$, confidence level $\delta\in(0,1)$.

        \STATE Construct the reward function set
        \begin{equation*}
            \cF := \{v\in\RR^{SA} :\mathbf{1}^\top v = 0, \norm{v}_\infty \le R\};
        \end{equation*}
    
        \STATE Set 
        \begin{equation*}
            \widetilde{r} = \arg\max_{f\in\cF}\sum_{i=1}^{n}\log\left(\frac{\ind\{y_i=1\}\exp(f(s_i,a_{i}^{1}))}{\exp(f(s_i,a_{i}^{0})) + \exp(f(s_i,a_{i}^{1}))} + \frac{\ind\{y_i=0\}\exp(f(s_i,a_{i}^{0}))}{\exp(f(s_i,a_{i}^{0})) + \exp(f(s_i,a_{i}^{1}))}\right);
        \end{equation*}

        \STATE Construct empirical covariance matrix
        \begin{equation*}
            \widehat{\Sigma} = \frac{1}{n}\sum_{i=1}^n \left(\mathbf{1}_{(s_i,a_{i}^{0})} - \mathbf{1}_{(s_i,a_{i}^{1})}\right)\left(\mathbf{1}_{(s_i,a_{i}^{0})} - \mathbf{1}_{(s_i,a_{i}^{1})}\right)^\top;
        \end{equation*}

        \STATE Construct the pessimistic reward function set
        \begin{equation*}
            \cF_{\mathrm{CR}}(\widetilde{r}) = \left\{f\in \cF ~:~ \sqrt{(f - \widetilde{r})^\top \widehat{\Sigma}(f - \widetilde{r})} \le b_n'\right\};
        \end{equation*}
        
        \RETURN $\widehat{\pi}_{\rm PMLE} = \arg\max_{\pi}\min_{\widehat{r}\in\cF_{\mathrm{CR}}(\widetilde{r})}\EE_{s\sim\rho}[\widehat{r}(s,\pi(s))]$. \label{alg-line:comp-r-hat}
    \end{algorithmic}
\end{algorithm}

To find the optimal policy with human preference data, we can use pessimistic MLE \citep{zhu2023principled}, which first computes a reward function by MLE and then outputs the optimal policy corresponding to a pessimistic version of this MLE reward (Algorithm \ref{alg:LCB-comp}). The data coverage assumption is similar to Assumption \ref{assumption:Cstar}, which essentially requires the sampling distribution to covers the state-actions pairs that optimal policy can reach. In the tabular case, this assumption can be written as follows:

\begin{assumption}\label{assumption:Cstar-comp}
    \textit{There exists an optimal policy $\pi^\star$ such that the pairwise concentrability coefficient 
    \begin{equation}\label{eq:C-dagger}
        C^{\dagger} := \sqrt{\sup_{v\in[-1,1]^{SA}:\mathbf{1}^\top v = 0}\frac{\Big(\sum_{(s,a)} d_\rho^{\pi^\star}(s,a) v(s,a)\Big)^2}{\sum_{(s,a^0,a^1)} d(s,a^0, a^1) \big(v(s,a^0) - v(s,a^1)\big)^2}}
    \end{equation}
    is bounded.}  
\end{assumption}

\cite{zhu2023principled} proved the convergence of pessimistic MLE in the linear bandit setting. The following theorem is a special case of Theorem 3.2 from \cite{zhu2023principled} with some modification, which expresses everything in the tabular setting. This shows when we assume human preference follows the BTL model, pessimistic MLE can provably converge to the optimal policy under the mild data coverage assumption of Assumption \ref{assumption:Cstar-comp} and its suboptimality decays at a fast rate of $O(1/\sqrt{n})$. This result marks a clear distinction from the negative results for human rating.

\begin{theorem}\label{thm:comp}
    \textit{Denote $\gamma = \frac{1}{2+\exp(R\sqrt{SA})+\exp(-R\sqrt{SA})}$. Suppose Assumption \ref{assumption:Cstar-comp} holds. For any fixed constant $0 < \delta < 1$, if one runs Algorithm \ref{alg:LCB-comp} with 
    \begin{equation*}
        b_m' = c_b'\sqrt{\frac{SA + \log\frac{1}{\delta}}{\gamma^2 m}},
    \end{equation*}
    where $c_b'$ is an appropriately chosen universal constant, with probability $1-\delta$, the suboptimality of the output policy $\widehat{\pi}_{\rm PMLE}$ satisfies
    \begin{equation*}
        \SubOpt(\widehat{\pi}_{\rm PMLE}) \le c_0C^\dagger R\left(\sqrt{\frac{SA + \log\frac{1}{\delta}}{\gamma^2 n}} + \sqrt{\frac{S^2A^2\log\frac{n}{\delta}}{n}}\right),
    \end{equation*}
    where $c_0$ is a universal constant.}
\end{theorem}

We can compare the suboptimality in this theorem with the results for the rating-based approach. A comparison with Theorem \ref{thm:Cstar} shows the uncertainty in human ratings may require the data to have stronger coverage in order to converge to the optimal policy. A comparison with Theorem \ref{thm:uniform} shows when the bias in human ratings distorts the reward function and makes it more extreme and drastic (less smooth in the Lipschitz sense), the $\bar{h}^{-1}(\cdot)$ can slow down the suboptimality's decay with respect to the sample size. In fact, we can observe that the preference-based approach enjoys faster suboptimality decay because preference feedback contains no bias and mild uncertainty noise according to the BTL model. While such modeling is justified by empirical evidences, it makes one wonder whether the advantage of preference-based methods mostly comes from the modeling aspect. To delve into this further, let us make another theoretical analysis for the case when preference data are affected by human bias.

\subsection{Human Preference under Biased BTL}

Let us introduce a new model for human preference called the biased BTL model. This model considers the case when human preferences are also subject to bias just like the rating model \eqref{eq:rating-h-model} and the feedback is generated with respect to the biased reward. In particular, the binary feedback $\widetilde{y} = \{0,1\}$ for $(s,a^0)$ and $(s,a^1)$ follows: 
\begin{equation}\label{eq:BTL-biased}
    P(\widetilde{y}|s,a,a') = \frac{\exp(\bar{h}(r(s,a^{\widetilde{y}})))}{\exp(\bar{h}(r(s,a^0)))+\exp(\bar{h}(r(s,a^1)))},
\end{equation} 
where $\bar{h}$ is the expected bias function from \eqref{eq:rating-h-model}.

We consider the performance of pessimistic MLE (Algorithm \ref{alg:LCB-comp}) again with human preference data generated under this model. While the data are generated under human bias this time, we still run pessimistic MLE on the new data as before. Different from the suboptimality results in the previous section, we focus on the sample complexity for learning the optimal policy. We take a gap-dependent approach in our analysis to consider the case when human bias closes the biased optimality gap $r(s,\pi^\star(s)) - r(s,a)$ and the actual optimality gap $\bar{h}(r(s,\pi^\star(s))) - \bar{h}(r(s,a))$ remains big, where $a$ is the second best action at $s$. This echoes with the type of undesirable bias we considered in the last comparison, which is true when human annotators have more extreme standards at heart. In a simple bandit instance, we can obtain the following result and notice the samples needed to find the optimal policy with the preference-based approach is no less than the samples needed for the rating-based approach.

\begin{theorem}\label{thm:both-bias}
    \textit{Consider any single-state bandit instance with $\cA=\{a_1,a_2\}$ and $0 \le \bar{h}(r(a_1)) < \bar{h}(r(a_2)) \le 1$. For any fixed constant $0 < \delta < 1$, let $n_{\text{rate}}$ be the total number of samples needed to learn the optimal action with probability at least $1-\delta$ in the human rating setting under observation model \eqref{eq:rating-h-model} with additive sub-gaussian uncertainty noise and uniform data coverage $n_{a_1} = n_{a_2}$, and let $n_{\text{pref}}$ be the number of samples needed to learn the optimal action with probability at least $1-\delta$ in the human preference setting with observation model \eqref{eq:BTL-biased}. It always holds that
    \begin{equation}
        \frac{n_{\text{rate}}}{n_{\text{pref}}} < 0.25\sigma^2. 
    \end{equation}}
\end{theorem}

We can see that when the variance proxy of the uncertainty noise $\sigma^2$ is no larger than $4$ in human rating (the expected reward is bounded in $[0,1]$), the samples needed in the rating-based approach is always fewer than the preference-based approach. This shows if one assumes a similar amount of human bias and uncertainty in both types of human feedback, the preference-based approach is no more sample-efficient. This actually contradicts with the empirical observations in the existing literature, which suggests preference-based methods have superior performance. Hence, our theory shows the bias-free modeling plays a great role in the lower sample complexity of preference-based methods, and our theoretical results can conversely confirm the standard BTL modeling of human preference feedback---it is reasonable to believe human preference data is indeed subject to less bias and uncertainty in practice. 
