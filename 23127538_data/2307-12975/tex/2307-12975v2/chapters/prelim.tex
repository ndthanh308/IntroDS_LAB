\section{Preliminaries}

In this section, we make a brief review of the contextual bandit problem and policy learning in the offline setting.

\subsection{Contextual bandit}

We consider a contextual bandit represented by $(\mathcal{S}, \mathcal{A}, r, \rho)$. Specifically, we focus on a tabular setting, where $\mathcal{S} := {1, 2, \dots, S}$ denotes the state space of size $S$, and $\mathcal{A} := {1, 2, \dots, A}$ denotes the action space of size $A$. The function $r: \mathcal{S} \times \mathcal{A} \to [0,R]$ represents the true reward function, which is assumed to be deterministic and unknown in this paper. Here, $r(s,a)$ is the immediate reward obtained when taking action $a\in \mathcal{A}$ in state $s \in \mathcal{S}$. $\rho$ denotes the initial state distribution of the bandit.

A policy $\pi: \mathcal{S}\to\Delta(\mathcal{A})$ specifies how actions are selected given a state, where $\pi(\cdot | s) \in \Delta(\mathcal{A})$ represents the action selection probability vector at state $s \in \mathcal{S}$. We also use $\pi(s)$ to denote the action selected by policy $\pi$ at state $s$. We denote the state-action visitation distribution of $\pi$ starting from the initial distribution $\rho$ with $d_\rho^\pi$. The value function of policy $\pi$ is defined as follows:
\begin{align*}
V^\pi(s) := \mathbb{E}_{s\sim\rho, a\sim\pi(\cdot | s)}\left[r(s,a)\right].
\end{align*}

Lastly, an optimal policy $\pi^\star$ maximizes the value function for all states simultaneously.

\subsection{Offline policy learning}

We consider the offline setting of policy learning, in which the learner is given a dataset of i.i.d. samples generated under a sampling distribution. While the sampling distribution can take different forms under different feedback models, the task is always to learn a policy $\pi$ from the offline dataset that performs as well as the optimal policy as possible. In particular, we evaluate the performance of $\pi$ by the suboptimality metric defined as follow:
\begin{align}\label{eq:suboptimality-def}
    \SubOpt(\pi) := \EE_{s\sim\rho}\left[V^{\pi^\star}(s) - V^{\pi}(s)\right].
\end{align}
Here the suboptimality measures the performance difference between the optimal policy $\pi^\star$ and $\pi$ in the problem bandit. Naturally, one aims to minimize the suboptimality and find an algorithm whose suboptimality converges to zero as the sample size $n$ approaches infinity. 