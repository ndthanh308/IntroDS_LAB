\section{Human Rating Models}\label{sec:formulation}

One of the most straightforward ways to use human feedback is to let human annotators evaluate on an absolute scale. Since such data can be readily used in most algorithms in the existing literature, the human rating approach has become very popular and one of the most important to study. As evidenced in behavioral studies, human ratings are subject to both bias and uncertainty \citep{Janowski15accuracy,li2020simple}. Specifically, under the influence of their own personalities and past experiences, human annotators can exhibit personal opinion bias during rating, leading to deviations from the true score. Furthermore, due to the tedious and challenging nature of the rating process, the evaluation of the same subject by the same human annotator can fluctuate over time, giving rise to what is known as intra-observer uncertainty. In light of these phenomena, \cite{Janowski15accuracy} propose a model that aims to characterize the ratings from a human annotator in the real world, which has been widely adopted in the existing literature \citep{Ortiz20unified,li2017recover,li20crowdsourcing}. In the following, we present this model in the single annotator case under the contextual bandit setting. For any fixed state-action pair $(s,a) \in \cS\times\cA$ with true reward $r(s,a)$, a rating from human annotator $\widetilde{r}(s,a)$ can be written as
\begin{align}\label{eq:rating-existing-models}
    \widetilde{r}(s,a) = r(s,a) + \Delta_{(s,a)} + \epsilon.
\end{align}
Here, $\Delta_{(s,a)}$ represents the bias of the human annotator for action $a$ at state $s$. Learners and algorithms have no knowledge of such bias and would take these observed ratings as reward samples directly. In most works \citep{Janowski15accuracy,li2020simple}, $\Delta_{(s,a)}$ is modeled as an unknown constant; in \cite{Ortiz20unified}, $\Delta_{(s,a)}$ is a Gaussian random variable with an unknown, non-zero mean. $\epsilon$ is a random noise representing the intra-observer uncertainty, which is modeled with a zero-mean Gaussian random variable in these works.

Apparently, such a human rating model bears several limitations. In \eqref{eq:rating-existing-models}, the bias $\Delta_{(s,a)}$ has an unknown non-zero expectation, which makes it impossible to recover the true reward $r(s,a)$ exactly. Furthermore, identifying the optimal action for a given state $s$ becomes infeasible when the bias causes a flip in the expected reward, i.e, $\EE[\widetilde{r}(s,a)] > \EE[\widetilde{r}(s,\pi^\star(s))]$ for any $a \notin \pi^\star(s)$, in which case the policy learning problem becomes inconsistent under this model. However, in the real world, the bias of a human annotator should not keep him or her from identifying the best action in expectation, but it is likely to take a much more general form. Neither of these is reflected in the current modeling with additive constant bias. On the other hand, the uncertainty is only modeled with an additive sub-gaussian random variable in these works. While this simple noise model is considered in many theoretical works, it cannot capture the setting when reward samples are generated from human rating. In practice, such uncertainty noise can be higher-order and more complex \citep{jin2018predicting}. In addition, the uncertainty might also be affected by the bias and reward at different state-action pairs and have an intricate role in the final observation $\widetilde{r}(s,a)$. 

To model human rating more realistically while keeping the policy learning problem consistent, we propose a new model under which the rating $\widetilde{r}(s,a)$ can be expressed with
\begin{equation}\label{eq:rating-h-model}
    \widetilde{r}(s,a) = h(r(s,a),\epsilon),
\end{equation}
where $h(\cdot, \cdot)$ is a general, deterministic transformation and $\epsilon$ is a random variable sampled from $\cN(0,\sigma^2)$ and independent from $(s,a)$. For notational simplicity, we define $\bar{h}(r):=\EE_{\epsilon}[h(r,\epsilon)]$. This can be interpreted as the reward function for this bandit instance in the human annotator's mind, which he or she uses to produce ratings. We can refer to $\bar{h}(r)$ as the biased reward and the function $\bar{h}(\cdot)$ as the expected bias function.

While the $h$ transformation can be general, for the policy learning problem to make sense, we assume it satisfies the following three condition. In this work, we only consider models that satisfy these conditions:

\textbf{Condition 1.} The function $\bar{h}(\cdot)$ satisfies
\begin{align*}
\bar{h}(r_1), \bar{h}(r_2) \in [0,R]\quad\textrm{and}\quad\bar{h}(r_1) > \bar{h}(r_2)
\end{align*}
for any $r_1,r_2\in[0,R]$ such that $r_1 > r_2$. In addition, $\bar{h}(0) = 0$.

This condition assumes that $\bar{h}(\cdot)$ is strictly monotone, implying that the biased reward function should preserve the ranking under the true reward $r$ in expectation. This condition is particularly important, as it ensures that the consistency of the policy learning problem. Therefore, we can identify the optimal policy based on human rating data in expectation.

\begin{remark}
    The monotonicity in Condition 1 also guarantees that any group of samples can be correctly compared and ranked in expectation, which is a necessary condition for the use of preference-based methods. This unifies the admissibility assumption in the rating models and preference models, which is crucial for the validity of our subsequent theoretical comparison between the two approaches.
\end{remark}

\begin{remark}
    We require $\bar{h}(0) = 0$ only to rule out arbitrary constant shift in $\bar{h}$ because shifting the reward by a constant is trivial and does not change the policy learning problem or any theoretical guarantee. 
\end{remark}

\textbf{Condition 2.} For any $r\in[0,R]$, $h(r,\epsilon)$ is a degree-$q$ polynomial in $\epsilon$ and symmetric in $\epsilon$ about its expectation, i.e., $$-(h(r,\epsilon) - \EE_{\epsilon}[h(r,\epsilon)]) \overset{d}{=} h(r,\epsilon') - \EE_{\epsilon'}[h(r,\epsilon')],$$ where $\epsilon'$ is a random variable identically distributed as $\epsilon$.

Since $h(r,\cdot)$ can be a very general function, the human uncertainty noise in the final observation $\widetilde{r}(s,a)$ is allowed to have a complicated dependency on the bias and the true reward, even though the randomness only comes from an independent Gaussian random variable $\epsilon$. For instance, the original white noise $\epsilon$ may end up amplified or reshaped by the true reward and human's internal bias and cause the final ratings to exhibit a complex concentration around $\bar{h}(r)$. This not only provides more realism and flexibility in modeling but also presents a greater theoretically challenge compared to the uncertainty considered in \eqref{eq:rating-existing-models}, which is modeled with a simple additive Gaussian noise with no interactions with the true reward and human bias.

\begin{remark}
Condition 2 is only a regulation on the effect of uncertainty---the uncertainty noise should not favor any particular direction (though the bias still can). This is in line with the real world, where the random noise concentrates evenly around the expectation and the effect of uncertainty diminishes in expectation.
\end{remark}

\textbf{Condition 3.} For any $r_1,r_2\in[0,R]$ such that $r_1 \ge r_2$, there are positive constants $C_{h,1},C_{h,2} > 0$ such that
\begin{align*}
\bar{h}^{-1}(r_1) - \bar{h}^{-1}(r_2) \le C_{h,1}\cdot \bar{h}^{-1}(r_1-r_2)\quad\textrm{and} \quad\bar{h}(r_1) -\bar{h}(r_2) \le C_{h,2}\cdot \bar{h}(r_1 - r_2).
\end{align*}

This is a technical condition on the regularity of the expected bias function. It ensures that the bias does not transform the reward too drastically, which eases our theoretical analysis.

We next provide a concrete example for the $h$-function satisfying our model, which will be studied in our theoretical analysis later.

\textbf{Example of Human Rating.} Consider a setting in which the true reward function satisfies $r(s,a)\in[0,1]$ for all $s\in\cS$ and $a\in\cA$. For any $r$, 
\begin{equation}\label{eq:h-example}
    h(r,\epsilon) = r^2 + r^2\epsilon|\epsilon|\quad\textrm{and}\quad\bar{h}(r) = r^2.
\end{equation}

This specific rating model has an interpretation that aligns with how human annotators give ratings in practice. Many human annotators with strong personal opinions and taste tend to exhibit an extreme bias in their evaluations, making them rate subjects with low true reward even lower and rate those with high true reward even higher on average. This is captured by the quadratic form of $\bar{h}(\cdot)$. On the other hand, when the white noise $\epsilon$ has a large magnitude, the impact of uncertainty on the final rating should be even larger and more drastic. Since annotators tend to be more deliberate yet more uncertain about subjects with large true reward, the fluctuation of the final rating should also increase with the magnitude of the true reward of a subject. Note that the uncertainty $r^2\epsilon|\epsilon|$ is a subexponential random variable and actually not a polynomial of $\epsilon$; however, our theory still applies, since the tail of $r^2\epsilon|\epsilon|$ is dominated by the quadratic $r^2\epsilon^2$. More generally, our theory is applicable to any uncertainty noise whose tail is dominated by a degree-$q$ polynomial of $\epsilon$. Furthermore, one can easily check that this $h$-function satisfies all three conditions in Section \ref{sec:formulation}. 
