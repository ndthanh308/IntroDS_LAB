\subsection{Rendering} \label{sec:tech_renderin}
TCP rendering techniques are adopted in the process of image and animation generation. According to the focus of used techniques in rendering, we classify the TCP rendering methods into three classes: stroke-based, image-based, and geometry-based. 

\noindent\paragraph{Stroke-Based Rendering.} Users can create TCP through simulated paint brushes to be rendered on a digital canvas. Strassmann\etal~\cite{10.1145/15886.15911} propose a realistic model of painting including Brush (a series of bristles with ink supply and positions), Stroke (a set of parameters like position and pressure), Dip (a procedure to assign states to each bristle of the brush), and Paper (the carrier of ink as it comes off the brush). With the four elements, one can build an interactive or automatic painting software on the computer. 

Typically, there are two types of methods to generate brush strokes. The first method models the stroke boundaries with B'{e}zier or B-spline curves and then fills the closed curves with designed textures~\cite{chua1990bezier,nishita1993display}. The other method directly models the two-dimensional brush, such as the work of Strassmann\etal~\cite{10.1145/15886.15911}. However, the brush bristles are visually fixed in shape, users cannot apply such an e-brush with their realistic painting skills. Some works~\cite{lee1999simulating,yeh2002effects} develop ``soft'' brushes in which the shape of bristle bundle varies in response to the forces given by users. Furthermore, Xu\etal~\cite{xu2005virtual} model brushes with writing primitives (a bundle of hair bristles), instead of each single brush bristle, to improve the simulating realism. To further simplify the model complexity, Bai\etal~\cite{bai2009chinese} propose a geometry model to simulate the entire brushes, instead of large amount of bristles. A dynamic model is also introduced to simulate the brush deformation under the internal and external forces. 
Previous methods focus on modeling general brush strokes, some methods propose tailored algorithms to model specific object shapes and textures such as rocks~\cite{way2001synthesis}, trees~\cite{way2002synthesis}, bamboos~\cite{yao2005painting}, and water~\cite{zhang2009video}. Instead of small brush strokes, Fu\etal~\cite{8419282} decompose the painting into image layers with each layer representing a class of specific strokes. With these stroke layers, they can create a new high relief, an art form between 3D sculpture and 2D painting. 

For animation generation, Xu\etal~\cite{xu2006animating} build a brush stroke library obtained from painting experts, and animate the paintings by decomposing them into brush strokes and changing these strokes. Zhang\etal~\cite{zhang2009video} create running water animations with a novel proposed painting structure generation method, which is used to estimate water flow line positions. Previous methods create brush trajectory relying on manual inputs, Yang\etal~\cite{yang2013animating} automatically estimate such trajectory from the paintings by modeling the brush footprint. Considering the automation process of extracting brush trajectory, some methods~\cite{8113507} try to reconstruct the drawing process by estimating and animating the drawing order of brush strokes.

Apart from the brush strokes, ink diffusion in paper fibers structure is also a critical characteristic, which has been studies in literature~\cite{kunii1995diffusion,zhang1999simple,lee2001diffusion,yu2002model,guo2003nijimi,10.1145/1073204.1073221,wang2007image,xu2007generic,liang2013image}. Specifically, Kunii\etal~\cite{kunii1995diffusion} propose a multidimensional diffusion model to simulate the ink density distribution as in real paper. Some works~\cite{lee2001diffusion,way2003physical,way2006computer,wang2007image} further simulate the ink of brush strokes on various types of paper based on physical-based models. Considering the potential blending of multiple strokes, Yeh\etal~\cite{yeh2002effects} and Yu\etal~\cite{yu2002model} build the ink diffusion simulation model with multi-layered structures of brush and paper. Chu\etal~\cite{chu2004real} develop a system for creating painting with more complicated ink diffusion effects based on lattice Boltzmann equation, and accelerate the algorithm for real-time process by utilizing both CPU and GPU. 

\noindent\paragraph{Image-Based Rendering.} Previous methods mainly focus on modeling the brush strokes and ink diffusion for interactive painting creation. From another technical route, one can directly synthesize Chinese painting from existing images. For instance, Yu\etal~\cite{yu2003image} propose a framework for image-based painting synthesis. Specifically, the authors build a brush stroke texture primitive collection, and map those texture primitives to a constructed mask image (named as control picture in \cite{yu2003image}). Apart from blending strokes, some works propose style transfer methods to transform natural pictures to paintings, involving handcrafted feature-based flow~\cite{liang2013image,dong2014real,guo2015novel} or deep neural networks. Typically, deep neural network based style transfer methods~\cite{wu2018research,he2018chipgan,zhang2020detail,9413063,li2021immersive,xue2021end} are data-driven, characterised by training the model with tailored training program and large scale data sets. For instance, ChipGAN~\cite{he2018chipgan} consists of a generator and a discriminator, where the generator is trained to transfer photos into paintings while the discriminator is trained to discriminate the generated paintings and real paintings. Meanwhile, ChipGAN requires thousands of images for training the model due to the large scale trainable parameters. 

For animating Chinese paintings, Liu\etal~\cite{liu2020animating} propose a sample point processing method to preserve the style of brush strokes and determine control bones, and a skeleton-based deformation method for animation generation. Liang\etal~\cite{lianginstance} leverage deep neural networks for transferring natural videos into ink wash painting-style videos. In order to enhance temporal consistency between video frames, the authors introduce multi-frame fusion and implement instance-aware style transfer, which help generate paintings with proper white space. 

\noindent\paragraph{Geometry-Based Rendering.} Stroke-based and image-based rendering are both from the view of computer vision. Instead, geometry-based methods adopt the view of computer graphics. Chan\etal~\cite{chan2002two} decompose the brush stroke-like features into layers of procedural shaders, and then mix different layers to construct desired effects in 3D models. Some works~\cite{way2002synthesis,yeh2002non,xu2012stroke} synthesis objects with ink painting styles by building polygonal models or extracting silhouette, and then mapping specific textures on the models. Amati\etal~\cite{amati2010modeling} develop a webcam-based system to capture the process of user drawing plants and build corresponding 2.5-dimensional digital models. Different from previous methods, Shi~\cite{shi2017generative} build landscape paintings from a 3-dimensional city model by generating mountains from buildings and assigning ink painting styles. 





