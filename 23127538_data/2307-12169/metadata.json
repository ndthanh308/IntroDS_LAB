{
  "title": "Rail-only: A Low-Cost High-Performance Network for Training LLMs with Trillion Parameters",
  "authors": [
    "Weiyang Wang",
    "Manya Ghobadi",
    "Kayvon Shakeri",
    "Ying Zhang",
    "Naader Hasani"
  ],
  "submission_date": "2023-07-22T21:18:41+00:00",
  "revised_dates": [
    "2023-10-24T00:11:59+00:00",
    "2023-11-02T00:29:57+00:00",
    "2024-07-30T00:27:59+00:00",
    "2024-09-17T01:05:42+00:00"
  ],
  "abstract": "This paper presents a low-cost network architecture for training large language models (LLMs) at hyperscale. We study the optimal parallelization strategy of LLMs and propose a novel datacenter network design tailored to LLM's unique communication pattern. We show that LLM training generates sparse communication patterns in the network and, therefore, does not require any-to-any full-bisection network to complete efficiently. As a result, our design eliminates the spine layer in traditional GPU clusters. We name this design a Rail-only network and demonstrate that it achieves the same training performance while reducing the network cost by 38% to 77% and network power consumption by 37% to 75% compared to a conventional GPU datacenter. Our architecture also supports Mixture-of-Expert (MoE) models with all-to-all communication through forwarding, with only 8.2% to 11.2% completion time overhead for all-to-all traffic. We study the failure robustness of Rail-only networks and provide insights into the performance impact of different network and training parameters.",
  "categories": [
    "cs.NI",
    "cs.AI",
    "cs.LG"
  ],
  "primary_category": "cs.NI",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.12169",
  "pdf_url": null,
  "comment": null,
  "num_versions": null,
  "size_before_bytes": 8368270,
  "size_after_bytes": 1400208
}