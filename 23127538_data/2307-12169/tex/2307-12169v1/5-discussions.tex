\section{Discussion} \label{sec:discussions}

\para{LLM trend.} The current growth rate of LLM computational requirement outpaces the advancements in AI accelerators and network speed as Moore's law slows down, necessitating hyper-scale clusters 
and more efficient interconnects~\cite{sirius, dnnmodelgrowth}. Our position to remove any-to-any network connectivity is the first step towards accommodating the network requirement for LLM training and sustaining the LLM growth trend. We also acknowledge ongoing efforts to reduce language models' size and resource requirements without compromising performance~\cite{dolly}. These works complement ours as our design reduces network resources and maintains performance even for smaller language models and clusters. 

\para{LLM Inference.} This paper explores the training workload of LLMs, yet inference represents another significant part of the LLM product cycle. Inference demands fewer computational resources than training as it involves moving fewer data through the LLM and only computes the forward pass~\cite{alpaserve}. As such, each HB domain naturally becomes an inference-serving domain, and the rail-only connections help load-balance multiple inference domains. We leave a detailed investigation of LLM inference to future work.

\para{Direct-connect network topology.} As mentioned in Section~\ref{sec:cost}, datacenter operators may leverage direct-connect network topologies for interconnection across the rails~\cite{jupiterevolving,topoopt,zhao2022optimal}. To maximize the effectiveness of such designs, we propose increasing the number of network interfaces connecting to each GPU through NIC interface splitting~\cite{topoopt}. Additionally, we suggest using reconfigurable optical switches to provide greater flexibility for interconnections across HB domains. Such a design also allows reconfiguring some connections across rails for forthcoming workloads that behave differently from LLMs. We believe that combining our proposed design and optical reconfigurable network switches opens a new line of research in AI-ML clusters.  

\para{Other ML workloads and limitations.} Although our proposed rail-only architecture focuses on network design specifically for LLMs, our design is efficient for many other DNN workloads when combined with other efforts. Recent works attempt to make the parallelization strategy and collective communication algorithms bandwidth-aware for any DNN model~\cite{alpa, zhao2023bandwidth}, which already produce traffic patterns resembling that of LLMs. For parallelization strategies requiring a small amount of traffic for GPUs across the rails, the cluster can use the forwarding described in Section~\ref{sec:sys-req}. Our design's primary challenge is the \textit{all-to-all} communication across all GPUs, which commonly arises in recommendation models with large embedding tables~\cite{dlrm, swhwcodesign}. The forwarding scheme induces congestion and degrades the performance of all-to-all traffic.
We acknowledge that all-to-all traffic is one of the most challenging traffic patterns that arise in ML workloads. Some potential solutions include reintroducing small any-to-any capacity through an over-subscribed network, utilizing a fast-reconfigurable network fabric, and decreasing the amount of all-to-all traffic initially generated by tweaking the ML model itself. 

\para{Fault tolerance.} At first glance, the rail-only design might appear less fault tolerant than a standard \fattree network. However, suppose a rail switch fails in either network. All the GPUs connected to the failed switch will become unavailable, rendering the two topologies identical regarding fault tolerance on rail switches. Conversely, our design requires fewer switches, which naturally reduces the points of failure. Datacenter operators can add redundant capacity by including extra rail switches, and our design remains more cost-effective compared to the state-of-the-art any-to-any network design. Fault tolerance can also be increased with a direct-connect network, as even if the control plane fails, an optical switch is likely to remain functional.

