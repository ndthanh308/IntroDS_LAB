\section{Introduction} \label{sec:intro}

The evolving field of Large Language Models (LLMs) holds great promise in revolutionizing our understanding of human language processing, driving technological advancement of artificial intelligence (AI). OpenAI's ChatGPT served over 100 million active users within three months of its release, making it the fastest-growing application ever~\cite{aibusinessgptuser}. Beyond chatbots, LLMs are progressively infiltrating our digital lives. Key service providers integrate these powerful models into co-pilot programs and search engines~\cite{bard, githubcop, bing}, transforming how humans interact with the digital sphere. 

LLMs are among the largest and most computationally-intensive Deep Neural Networks (DNNs). The latest GPT4 model is estimated to have trillions of parameters and take months to train~\cite{openai2023gpt4, gpt4params}. 
Historically, researchers seek to enhance the performance of distributed DNN training and inference through optimizing parallelization strategies~\cite{flex_flow, alpa, unity, tofu}, sophisticated scheduling~\cite{muri, gandiva, tiresias}, advanced compression~\cite{grad_comp}, and even the reconfiguration of the network topology itself~\cite{sipml, topoopt, zhao2022optimal}. Despite these efforts, LLMs still require significant raw computing power. The GPT3 model from 2020 already requires 355 GPU-years on Nvidia's V100 GPUs~\cite{gpt3, gpt3time}. As Moore's law slows down, the growth rate of LLM size and computation requirement exceeds the advancement of accelerators, making hyper-scale GPU clusters inevitable. 
Our conversations with lead machine learning architects in the industry indicate that the next-generation LLMs likely require over 30,000 GPUs of computing power to finish training within a reasonable time.

A GPU-centric cluster typically employs two types of connection~\cite{dgxh100archdoc}. For the first one, a few GPUs (e.g., eight for a DGX H100 server) reside within a high-bandwidth domain, called HB domain,  through a short-range communication protocol like NVLink~\cite{nvlnvs}.
The second connection forms a network capable of any-to-any GPU communication using RDMA-capable NICs, connected in a variant of the \fattree network. The cluster uses the RDMA protocol on this network to benefit from bypassing CPU and OS entirely through GPU-Direct~\cite{mlnxrdmagpu, gpudirect}. However, scaling up RDMA networks to tens of thousands of GPUs is challenging. Previous work demonstrated that large-scale RDMA network are prone to deadlocking and PFC storms~\cite{pfc_storm, rdma_azure, ib_deadlock, bfc,ddlindc}, degrading the performance. Furthermore, as the scale goes up, \fattree architectures become prohibitively costly~\cite{topoopt}. Datacenter providers often resort to over-subscription to tame the cost of the cluster, worsening the deadlocking problems. 

Prior work proposed several techniques to enable large-scale RDMA networks and reduce their cost~\cite{rdma_cc,rdma_azure,srnic,revisitrdmanet}. These approaches fundamentally depend on the assumption that the network is capable of any-to-any communication.
This assumption forms the bedrock upon which datacenters have been conceptualized and developed for several decades. 

In this paper, we challenge this assumption and show that LLM training traffic \textit{does not} require any-to-any connectivity across all GPUs in the network. We argue that with the optimal parallelization strategy, an LLM training workload requires high-bandwidth any-to-any connectivity only within small subsets of GPUs, and each subset fits within an HB domain. Across the HB domains, communication only happens for a few GPU pairs, and the traffic volume is insignificant. As a result, the conventional any-to-any approach for building datacenter interconnect adds unnecessary complexity and cost for distributed LLM training.

We propose a network architecture that accurately reflects LLM communication requirements.
In this architecture, a cluster is first partitioned into multiple HB domains, and each interconnected with a full-bisection bandwidth any-to-any interconnect. Across the HB domains, instead of forming a \fattree to support any-to-any communication, the network only connects sets of GPUs with network traffic.
We demonstrate that our LLM-centric network architecture archives \textit{the same performance} as a full-bisection bandwidth any-to-any \fattree cluster while reducing the cost by 37\% to 75\%.
