\section{Conclusion} \label{sec:conclusion}
This paper challenges the conventional any-to-any network architecture for GPU clusters dedicated to training large language models. We propose a new architecture, called rail-only, that aligns with the distinct characteristics and demands of LLMs, leading to up to 75\% cost reductions while maintaining identical performance to the current state-of-the-art \fattree networks. 