\section{Motivation} \label{sec:motivation}
In this section, we first introduce the architecture of a conventional GPU-centric cluster. Then we perform a thorough analysis of LLM traffic patterns to motivate a network architecture contrasting the conventional design. 

% Figure environment removed 

\subsection{State-of-the-Art GPU cluster Design} \label{sec:gpu-centric-sota}
Conventional networked clusters are designed to serve CPU-heavy workloads using a multi-layer \fattree network, illustrated in Figure~\ref{fig:dc-clos}~\cite{fattree, fbdcarch}. This architecture, known as a Fat-Tree network, is deeply studied in the system and networking communities. In a typical Fat-Tree-based cluster, each server is equipped with one NIC (40~Gbps to 400~Gbps), and $K$ servers are arranged into racks connecting to Top-of-the-Rack (ToR) switches. The ToR switches are then connected to the aggregation switches to provide connectivity across racks, forming a pod. Finally, the pods are interconnected with spine switches, allowing any-to-any communication across servers in a CPU cluster. 

In contrast, the rise of network-heavy ML workloads led to the dominance of GPU-centric clusters, where \textit{individual} GPUs 
have dedicated NICs~\cite{swhwcodesign}.
Figure~\ref{fig:rail-optimized} illustrates the network architecture of a typical GPU cluster. Each GPU has two different communication interfaces: ($i$) An NVLink interface to support high-bandwidth but short-range interconnection 
and ($ii$) a conventional RDMA-enabled NIC. The NVLinks connect $K$ GPUs to 
provide terabits of non-blocking any-to-any bandwidth in/out per GPU (7.68~Tbps for fourth-gen NVLink, for instance). This group of GPUs with fast interconnect forms a \textit{high-bandwidth domain (HB domain)}. Traditionally, HB domains were restricted to a single server (e.g., DGX servers with $K=8$ or 16 GPUs). However, recently, Nvidia announced the GH200 supercomputer interconnecting $K=256$ Grace Hopper Superchips to form one HB domain across multiple racks~\cite{gh200}.  

However, some LLMs take too long for a single HB domain to train, even with 256 GPUs. For instance, the PaLM-540B model would take 117 days to finish on a GH200 supercomputer, assuming perfect GPU utilization. These models require parallelization across multiple HB domains. 

% Figure environment removed 
To enable training an LLM across multiple HB domains, GPU cluster operators use RDMA-capable NICs to interconnect multiple HB domains together. The conventional network architecture to interconnect  HB domains is called a \textit{rail-optimized} network~\cite{dgxh100archdoc}. 
In a rail-optimized architecture, GPUs within an HB domain are labeled from 1 to $K$. A \textit{rail} is the set of GPUs with the same index (or rank) on different HB domains, interconnected with a rail switch. For instance, Figure~\ref{fig:rail-optimized} illustrates Rail 1 and Rail $K$ in red and yellow color, respectively. These rail switches are subsequently connected to spine switches to form a full-bisection any-to-any \fattree network topology. This network ensures any pair of GPUs in different HB domains can communicate at the network line rate (400~Gbps Infiniband network for GH200). For instance, traffic between \texttt{GPU 1, Domain 1} and \texttt{GPU 1, Domain 2} traverses through Rail Switch 1 only, while traffic between \texttt{GPU 1, Domain 1} and \texttt{GPU 2, Domain 2} goes through the respective rails and the spine switches.

\subsection{Analyzing Network Traffic of LLMs} % on a GPU Datacenter}

We now analyze the traffic pattern and iteration time of OpenAI's GPT3~\cite{gpt3}, Meta's OPT3-175B~\cite{zhang2022opt}, and Google's PaLM-540B~\cite{chowdhery2022palm} distributed in a cluster composed of hundreds of Nvidia GH200 supercomputers~\cite{gh200}. These LLMs represent cutting-edge language models with publicly available parameters. Each GH200 supercomputer comprises a two-tier NVSwitch architecture, facilitating 2~Pbps of full-bisection bandwidth (7.68~Tbps per GPU) across 256 H100 GPUs. Additionally, each GPU has a Connect-X7 HCA Infiniband network interface~\cite{gh200}, which provides 400~Gbps network bandwidth in/out of each GPU. In this setup, each GH200 supercomputer forms an HB domain. %A cluster of size 32768-accelerator requires 128 of these HB domains.

Our analysis uses the benchmarking parallelization strategy from Nvidia ~\cite{shoeybi2020megatronlm, korthikanti2022reducing} to ensure optimal GPU utilization. We use hybrid data parallelism (DP) and intra-operator model parallelism (MP), but our conclusions remain similar for pipeline parallelism. The DP synchronization utilizes a hierarchical \allreduce algorithm to limit traffic across HB domains. We employ the best parallelization strategy for each LLM model and compute the resulting traffic pattern. Note that for the PaLM model, the batch size varies throughout the training. %Therefore we compute a unique strategy for each model and compute the traffic generated for a training iteration.

% Figure environment removed

Figure~\ref{fig:traffic_dist}a illustrates the traffic type distribution of one training iteration across server pairs for a cluster of 128 GH200 supercomputers, and Figure~\ref{fig:traffic_dist}b shows the percentage of volume for each type of traffic. There are two primary types of communication: intra-operator model parallelism (MP) traffic and \allreduce traffic generated by data parallelism.
The MP traffic happens within GPUs that participate in a \textit{model parallel group}, which always fits in an HB domain. For data parallelism, the hierarchical \allreduce algorithm further partitions the communication into two stages, where the first stage synchronizes parameters across HB domains while the second stage is within them. The algorithm ensures the second stage carries more traffic to utilize the available bandwidth better. While these types of traffic do not overlap between different pairs of GPUs, Figure~\ref{fig:traffic_dist}a indicates that over 99\% of GPU pairs carry \textit{no traffic} and less than 0.25\% of GPU pairs carry MP and second stage DP traffic between them. Simultaneously, Figure~\ref{fig:traffic_dist}b suggests these traffic types account for over 90\% of the total transmitted data. Recall these two types of traffic stay within HB domains, suggesting efficient usage of HB domain bandwidth and low demand on the network fabric interconnecting HB domains. This pattern is consistent across all models, indicating that building a cluster with any-to-any connectivity on top of HB domains for LLM models is excessive.%, leading to unnecessary network complexity without any performance gains.

% Figure environment removed

Within HB domains, the interconnect needs to support heavy any-to-any communication for training a diverse set of LLMs. To illustrate this, Figure~\ref{fig:heatmap} shows heatmaps of the traffic matrices during a training iteration for GPT3 and OPT3-175B. In this plot, every consecutive set of 256 GPUs resides within the same HB domain. Figures~\ref{fig:heatmap}a and \ref{fig:heatmap}b demonstrate the communication pattern within an HB domain. Note that the traffic volume is significant (up to 1~GB across server pairs), and the pattern varies due to different parallelization strategies across these models. The squares on the diagonal represent the MP traffic, while the rest of the off-diagonal lines represent the second stage DP traffic. An any-to-any interconnect within an HB domain to provides the maximum flexibility to accommodate different LLMs. 

However, the high-bandwidth any-to-any connectivity required within HB domains is not needed across them. Figures~\ref{fig:heatmap}c and \ref{fig:heatmap}d zoom out to the first four HB domains, where the first stage DP traffic across HB domains occurs. Compared to the traffic within an HB domain, the cross-HB traffic is much smaller ($\approx$1~MB per entry) and more sparse. This is because cross-HB domain communication only occurs between GPUs with the same rank (i.e., GPUs on the same rail). Furthermore, the pattern is homogeneous, as the cross-HB domain communication pattern remains the same across LLMs (the off-diagonal lines in Figures~\ref{fig:heatmap}c and d).

These observations suggest that it is possible to remove links that do not carry any network traffic without hurting the training performance of LLMs. Our analysis shows that 33\% of the links in an any-to-any 400~Gbps \fattree network are removable. Figure~\ref{fig:itertime_motiv} illustrates the training iteration times for this alternative network, labeled as \textit{Any-to-Any Trimmed 400~Gbps}, compared to the state-of-the-art.
Given that the disconnected links do not support any traffic, these two topologies deliver identical performances. 

To put this into perspective, we also consider an ideal performance by assuming all GPUs in the cluster are interconnected with NVSwitch, forming a monolithic 7.68~Tbps full-bisection bandwidth any-to-any network, which represents a $19.2\times$ increase in the full-bisection bandwidth compared to the state-of-the-art. Such a network is impossible to build in practice. The performance improvement, however, is not proportional to the added  bandwidth, ranging from $1.06\times$ to $1.88\times$ for all models, as shown in Figure~\ref{fig:itertime_motiv}. Such a result provides compelling arguments for LLM cluster operators to reassess the conventional any-to-any network design. 

% Figure environment removed 
