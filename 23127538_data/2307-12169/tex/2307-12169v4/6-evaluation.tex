\section{Evaluation} \label{sec:evaluation}

\subsection{Iteration Time Modeling} \label{sec:accuracy_model}
We evaluate our Rail-only network design's performance through an analytical model of the training iteration time. Our analytical model considers the critical path for LLM training with TP, DP, and PP, similar to the approach in Calculon~\cite{calculon}. 

We demonstrate the accuracy of our analytical model by comparing its estimation of hardware FLOPs utilization (HFU) to that of published results in the literature. The HFU refers to the hardware's floating point operation performed in an iteration over the peak floating point operations. Prior work provided the full set of hyperparameters in their evaluation setup, enabling us to compare the estimated HFUs from our analytical model to the ground truth~\cite{korthikanti2022reducing}. In our evaluations, we assume a cluster of 1 to 280 DGX A100 servers. 
To compute the total required FLOPs for training per iteration of a DNN model, we use the same formula proposed by Korthikanti et al.~\cite{korthikanti2022reducing}.

% Figure environment removed

Figure~\ref{fig:hfu} compares the HFUs approximated by our analytical model with the ground truth for different GPT models and cluster scales. For GPT-1T, our computed HFU only differs from the ground truth by $0.15\%$. The discrepancy between our analytical model and the ground truth comes from our idealistic modeling of GPU computation and communication, assumptions on how computation and communication overlap, and underestimation of memory overhead. Such discrepancy goes up as the model size decreases. For the GPT-22B model, our estimation error is $15.7\%$ compared to ground truth MFU. 
The rest of this section utilizes our analytical model to estimate the training iteration time of Rail-only interconnects.

\subsection{What is the Optimal Size of an HB domain?} \label{subsec:hbd-size}

Intuitively, increasing HB domain size reduces the inter-platform network overhead during training. This section answers the following question: \textit{what is the optimal size of an HB domain in a Rail-only interconnect?} In Figure~\ref{fig:iter_time_dom_sz}, we vary the HB domain size ($K$) and plot the training iteration time for GPT-1T and GPT-146B MegatronLMs for clusters with 16384, 32768, and 65536 H100 GPUs. The global batch size in this evaluation is 4096 and 1024 for GPT-1T and GPT-146B, respectively. We enumerate all possible parallelization strategies for each cluster size and use the optimal parallelization strategy found in our analytical model, using the bandwidth and computational ability parameters of DGX GH200. In addition, to capture the ideal iteration time, we also compute training iteration time where a full-bisection monolithic NVLink fabric connects every GPU (i.e., the case where $K=N$, where $N$ is the total GPU count). \looseness=-1

As depicted in Figure~\ref{fig:iter_time_dom_sz},  
iteration times decrease as the HB domain size increases, indicating that larger HB domains reduce the network overhead during training. In both GPT models, the iteration time achieved with an HB domain size of 256 is nearly optimal. Compared to the ideal case (all GPUs are under a monolithic HB domain), GPT-146B with an HB domain of 256 is $8.9\%$ slower, while GPT-1T is $1.3\%$ slower. However, the \textit{marginal gain} decreases as the HB domain size increases.
For the larger GPT-1T model, the train iteration time plateaus above 32 GPUs due to Amdhahl's law, suggesting diminishing returns from the increased cost of bigger HB domains. 

% Figure environment removed

For a smaller GPT-146B model, shown in Figure~\ref{fig:iter_time_bw}b, the marginal gain of increasing HB domain size is higher than that of GPT-1T. Providing an HB domain of size eight reduces the iteration time by $50.6\%$ compared to the HB domain of size one, while increasing the HB domain size from 8 to 256 further achieves a $39.1\%$ reduction in iteration time. The more significant marginal gain for smaller LLMs incurs more communication overhead when distributed to the same cluster than larger models. This effect arises from how computation and communication costs scale as LLM grows. The communication requirement increases linearly with the model's hidden size and sequence length. On the other hand, the model FLOPs increase quadratically with these two parameters, as indicated by previous work~\cite{narayanan2021efficient}. Therefore, we conclude that the optimal HB domain size depends on the size of the GPT model.

\subsection{Impact of HB domain and Network Bandwidth}
% Figure environment removed 

The available bandwidth of HB and NIC domains determines the communication time during training. We analyze the impact of these bandwidths on the iteration time of a GPT model with one trillion parameters. Figure~\ref{fig:iter_time_bw}a and~\ref{fig:iter_time_bw}b show the iteration time variation for different HB domain bandwidths (different lines) and network bandwidths in the rails (on the $x$-axis) for HB domain size $K=8$ and 256, respectively. As expected, the iteration time decreases when either bandwidth increases. However, the $K=8$ case is less sensitive to the HB domain bandwidth. Increasing per-GPU bandwidth by a factor of four (from 2.4~Tbps to 9.6~Tbps) only improves the iteration time by $8.0\%$ on average for $K=8$, compared to the improvement of $13.3\%$ for $K=256$. On the other hand, larger HB domain sizes are more pronounced on network bandwidth improvement. Increasing the bandwidth from 100~Gbps to 400~Gbps, results in $35.9\%$ improvement for $K=8$ but only $8.0\%$ for $K=256$. Hence, improving the HB domain bandwidth is more beneficial than the network bandwidth for LLMs as future HB domain size increases. \looseness=-1

% Figure environment removed


\subsection{Impact of Batch Size on Network Design} \label{subsec:batch-size}

While the batch size is typically an ML-centric metric for optimization, our analysis indicates that the impact of batch size on the total training time goes beyond the total number of iterations required for convergence. To further understand such impact, we analyze the iteration time of a GPT-1T model on a 32768 GPU cluster while changing the HB domain size from $K=8$ to $32768$. In this study, we vary the global batch size from 256 to 4096. Figure~\ref{fig:iter_time_palm}a plots the change in iteration time as the batch size varies. The iteration time exhibits a similar trajectory for all HB domain sizes; however, the \textit{relative performance} (the ratio of the iteration time for an HB domain size to that of the ideal case) improves as the batch size increases. Figure~\ref{fig:iter_time_palm}b represents this trend. When $K=256$, the relative performance increases from $93\%$ to $99\%$ as the batch size increases from 256 to 4096 sequences. The iteration time advantage is prominent when the HB domain size is small. For $K=8$, increasing the batch size from 256 to 4096 improves the relative performance from $58\%$ to $82\%$, suggesting a larger batch size is preferable for a cluster with a smaller HB domain. Prior studies have shown that LLM training benefits from a larger batch size~\cite{kaplan2020scaling, gpt3}, especially for bigger models, making it a perfect fit for our Rail-only design. \looseness=-1

\subsection{Network Cost and Power Analysis} \label{sec:cost}

Our Rail-only network architecture judiciously reduces the network resources for LLM training by eliminating unused network hardware. This section compares our proposed approach's network cost and power with the state-of-the-art Rail-optimized GPU clusters. We calculate the number of switches ($N_{SW}$) and transceivers ($N_{TR}$) required for each network design and derive the network equipment cost and peak power consumption based on numbers reported in prior work and by vendors~\cite{topoopt, nvidia400gpower, qm7900spec}.\footnote{Cost: $Price_{TR}=\$199$ per transceiver, $Price_{SW}=\$694$ per switch port for 400~Gbps; Power: $Power_{TR}=9$W per transceiver, $Power_{SW}=18$W per switch port}
We enumerate the number of switches and transceivers required to build the state-of-the-art network architecture and our proposed architecture in Table~\ref{tab:nw_swich}, accounting for variable cluster sizes and network switch radix. 
For each architecture, we build a minimum layer \fattree network with the provided switch radix and calculate the minimum number of required switches and transceivers to achieve the desired connectivity. The total cost of each architecture is 
\begin{equation}
    \text{Total\ cost} = Price_{SW}\times N_{SW} + Price_{TR}\times N_{TR}
\end{equation}
while the power is
\begin{equation}
    \text{Total\ power} = Pwr_{SW}\times N_{SW} + Pwr_{TR}\times N_{TR}
\end{equation}

The last two columns of Table~\ref{tab:nw_swich} provide the cost and power savings of a Rail-only interconnect over the state-of-the-art. Our Rail-only design notably reduces the network cost by 38\% to 77\% (117 to 234 million dollars) and power consumption by 37\% to 75\% (1.7 to 6.9 megawatts) compared to the state-of-the-art design while achieving equivalent performance.
This reduction stems from eliminating spine layer switches and decreasing the number of switch tiers within each rail. Surprisingly, switches with a radix of 64 provide the worst-case cost and power reduction in both cluster sizes. In this case, the state-of-the-art design requires a three-tier \fattree network, while the Rail-only design requires two tiers for each rail. Still, our design only requires three-quarters of the total number of switches while achieving the same performance as the state-of-the-art design. 

\begin{table}[t]
\scriptsize
\centering
\caption{Number of switches and transceivers for different clusters.}

\renewcommand{\arraystretch}{1} 
\linespread{1.05}\selectfont\centering
    \begin{tabular}{|P{0.55cm}|P{0.55cm}|P{0.55cm}|P{0.65cm}|P{0.65cm}|P{0.65cm}|P{0.55cm}|P{0.55cm}|}
    \hline
    \multirow{2}{*}{\#GPUs} & \multirow{2}{*}{{\shortstack{Switch\\ Radix}}} & \multicolumn{2}{c|}{\#Switches ($N_{SW}$)} & \multicolumn{2}{c|}{\#Transceivers ($N_{TR}$)} & \multicolumn{2}{c|}{Savings} \\\cline{3-8}
       & & \multicolumn{1}{c|}{SOTA} & \multicolumn{1}{c|}{Rail-only} & \multicolumn{1}{c|}{SOTA} & \multicolumn{1}{c|}{Rail-only} & \multicolumn{1}{c|}{Cost} & \multicolumn{1}{c|}{Pwr} \\
    \hline
        \multirow{3}{*}{32768}  
                                 & \multicolumn{1}{c|}{64} & \multicolumn{1}{c|}{2560} & \multicolumn{1}{c|}{1536} & \multicolumn{1}{c|}{196608} & \multicolumn{1}{c|}{131072} & \multicolumn{1}{c|}{38\%}  & \multicolumn{1} {c|}{37\%} \\\cline{2-8}
                                 & \multicolumn{1}{c|}{128} & \multicolumn{1}{c|}{1280} & \multicolumn{1}{c|}{256} & \multicolumn{1}{c|}{196608} & \multicolumn{1}{c|}{65536} & \multicolumn{1}{c|}{77\%} & \multicolumn{1}{c|}{75\%} \\\cline{2-8}
                                 & \multicolumn{1}{c|}{256} & \multicolumn{1}{c|}{384} & \multicolumn{1}{c|}{128} & \multicolumn{1}{c|}{131072} & \multicolumn{1}{c|}{65536} & \multicolumn{1}{c|}{62\%} & \multicolumn{1}{c|}{60\%} \\\hline
        \multirow{3}{*}{65536}   
                                 & \multicolumn{1}{c|}{64} & \multicolumn{1}{c|}{5120} & \multicolumn{1}{c|}{3072} & \multicolumn{1}{c|}{393216} & \multicolumn{1}{c|}{262144} & \multicolumn{1}{c|}{38\%} & \multicolumn{1}{c|}{37\%} \\\cline{2-8}
                                 & \multicolumn{1}{c|}{128} & \multicolumn{1}{c|}{2560} & \multicolumn{1}{c|}{1536} & \multicolumn{1}{c|}{393216} & \multicolumn{1}{c|}{262144} & \multicolumn{1}{c|}{38\%}  & \multicolumn{1}{c|}{37\%} \\\cline{2-8}
                                 & \multicolumn{1}{c|}{256} & \multicolumn{1}{c|}{1280} & \multicolumn{1}{c|}{256} & \multicolumn{1}{c|}{393216} & \multicolumn{1}{c|}{131072} & \multicolumn{1}{c|}{77\%} & \multicolumn{1}{c|}{75\%} \\\hline
    \end{tabular}
    \label{tab:nw_swich}
\end{table}