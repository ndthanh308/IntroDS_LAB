\section{LLM Traffic Pattern Analysis}
\label{sec:traffic_ana}

\subsection{Traffic Pattern of MegatronLM}
We now analyze the traffic pattern generated by LLMs with hybrid data, tensor, and pipeline parallelism by computing the network transfer sizes from the model hyperparameters and the parallelization strategy. We first look at a series of GPT models with 145.6 billion, 310.1 billion, 539.6 billion, and 1 trillion parameters described in Table~1 of MegatronLM~\cite{narayanan2021efficient} paper, distributed across up to 3072 GPUs. The models are distributed in a cluster of up to 384 DGX A100 servers with an HB domain of size eight. Our analysis uses the same parallelization strategy from MegatronLM to ensure optimal GPU utilization. We use the ring-based collective communication since it is bandwidth-optimal and the default algorithm in NCCL. 

There are three primary types of communication: AllGather and ReduceScatter traffic from tensor parallelism (TP), \allreduce traffic from data parallelism (DP), and point-to-point traffic from pipeline parallelism (PP). Figure~\ref{fig:traffic_dist}a illustrates the volume percentage for each type of communication for one training iteration. Figure~\ref{fig:traffic_dist}b shows the communication type distribution across all GPU pairs. 

The TP traffic happens within GPUs participating in a TP rank, which occupies an HB domain. The DP and PP traffic happen in the NIC domain, and their volume is significantly smaller than TP traffic, as illustrated by Figure~\ref{fig:traffic_dist}a. While traffic from different parallelism does not overlap between different pairs of GPUs, Figure~\ref{fig:traffic_dist}b indicates that over 99\% of GPU pairs carry \textit{no traffic} and less than 0.04\% of GPU pairs carry TP traffic. Simultaneously, Figure~\ref{fig:traffic_dist}a suggests TP traffic accounts for over 75\% of the total transmitted data. Recall that TP traffic stays within HB domains, suggesting efficient usage of HB domain bandwidth and low demand in the NIC domain. This pattern is consistent across all GPT models we studied, indicating that building a GPU datacenter with any-to-any connectivity on top of HB domains for LLM models is excessive. 
% Figure environment removed 


\subsection{Traffic in the NIC Domain for LLMs}

% Figure environment removed

The parallelization strategy employed in MegatronLM induces an insignificant amount of traffic in the NIC domain compared to the HB domains. Figure~\ref{fig:heatmap} shows the traffic heatmap for training the GPT-1T model. In this plot, every consecutive set of eight GPUs resides within the same HB domain (highlighted in orange), and GPUs with a distance of 8 between them belong to the same rail (highlighted in pink). Figure~\ref{fig:heatmap}a demonstrates the traffic pattern within one pipeline stage, while Figure~\ref{fig:heatmap}b shows the traffic across the first four pipeline stages. The traffic volume is significant ($\sim$300~GB across GPU pairs) in an HB domain, while the communication drops to only about 6~GB in the NIC domain. Furthermore, the transfers in the NIC domain never traverse through the spine switches -- these network transfers only happen within a rail. \looseness=-1

We argue that \textit{all} LLMs without sparse MoE layers distributed with an optimal parallelization strategy always induce sparse, low-volume traffic in \textit{within the rails}. By design, the only traffic exiting the HB domains is point-to-point traffic from pipeline parallelism or collective communication traffic (AllGather, ReduceScatter, and AllReduce) from TP and DP. 

% Figure environment removed

% Figure environment removed

For PP, due to the symmetry of LLM parallelization, each pipeline stage contains the same number of GPUs. As a result, the pipeline stages can always be placed such that traffic across stages traverses the GPUs of the same rank in the NIC domain only, hence staying within the same rail. 

TP and DP can induce collective communication traffic in both the HB and the NIC domains. The examples from MegatronLM always have TP and DP contained within HB and NIC domains only, respectively. While such a partition is common for LLMs, it is not ubiquitous. For example, training a smaller model using only DP causes all GPUs to participate in the same DP rank and, thus, the same AllReduce operation across both HB and NIC domains. In these cases, the training cluster could use \textit{hierarchical collective communication} algorithms that achieve near-optimal performance. Below, we introduce these algorithms, analyze their performance, and illustrate that their traffic in the NIC domain stays within rails.

Hierarchical collective communication algorithms are designed for a multi-tiered network topology. We introduce the hierarchical AllGather algorithm here and note that for the other collectives happening in LLM training, ReduceScatter achieves the same performance by inverting the schedule of AllGather, and AllReduce is equivalent to a ReduceScatter followed by an AllGather. We focus on the bandwidth performance and ignore the latency, as the data transmission is significant during LLM training; thus, the communication runtime is bandwidth-dominated. 
Table~\ref{tab:var_anal} lists the variables used in this section. We assume the GPUs conducting an AllGather operation are arranged into an $x\times y$ grid, where each $x$ GPU belongs to the same HB domain and across $y$ total HB domains. The basic hierarchical AllGather executes in two phases: first, the algorithm collects partial data for each rail of GPUs without transferring data in the HB domain. If the total data to run AllGather is of size $D$, then the amount of data exchanged in the network by all GPUs is ${D(y-1)}/{x}$. This operation effectively creates larger data shards for the GPUs to rerun AllGather within each HB domain. Therefore, each HB domain conducts an AllGather in the second phase, inducing a total transfer of $D(x-1)$. Assume the $x$ GPUs within an HB domain have bandwidth capacity $C_F$ and $y$ GPUs in the NIC domain have bandwidth $C_S$, then the total AllGather runtime is \looseness=-1
\begin{equation}\label{eq:agtime}
    \mathtt{AGtime}(D, x, y, C_F, C_S)=\frac{(y-1)D}{xyC_S}+\frac{(x-1)D}{xC_F}
\end{equation} 
Like PP communication, by appropriately mapping the logical $x\times y$ GPUs to the GPU cluster, this algorithm only induces traffic for GPUs within the same rail. 

Figure~\ref{fig:traffic_dist}a shows the traffic pattern of training GPT-1T with hierarchical collective communication spanning both NIC and HB domains, differing from the MegatronLM cases. We compute and analyze the traffic pattern of training the GPT-1T model, with a batch size of 4096, distributed in a cluster composed of 16 Nvidia DGX GH200 supercomputers~\cite{gh200} (4096 GPUs). Each DGX GH200 supercomputer comprises a two-tier NVSwitch architecture, facilitating 7.2~Tbps GPU-to-GPU bandwidth across 256 H100 GPUs. Additionally, each GPU has a Connect-X7 HCA Infiniband network interface~\cite{gh200}, which provides 400~Gbps network bandwidth in/out of each GPU. In this setup, each DGX GH200 supercomputer forms an HB domain. Figure~\ref{fig:traffic_dist_gh200} illustrates the traffic volume percentage and heatmap in this setting. The parallelization strategy has a total data parallel degree of 64, which spans 32 GPUs in each HB domain and two HB domains across the network. Figures~\ref{fig:traffic_dist}b and~\ref{fig:traffic_dist}c illustrate the traffic heatmap of the hierarchical AllReduce algorithm, which splits the AllReduce traffic among each DP group. Note that the network traffic stays within a rail (GPUs with a distance of 256 apart). The hierarchical algorithm efficiently utilized the bandwidth in the HB domain to carry $98\%$ of the AllReduce traffic, while the network domain carries the other $2\%$.

\subsection{All-to-All Traffic for Mixture-of-Expert Models}
\label{sec:moe_traffic}
LLMs with sparsely gated Mixture-of-Expert (MoE) layers exhibit a different traffic pattern than the models described above. MoE layers provide an alternative way to increase the size of LLMs without introducing significant additional computational requirements. With MoEs, part of the model is replaced by a set of ``expert" neural networks, where a \textit{gating network} routes each token to different experts, thereby only activating part of the model. The typical parallelization strategy for MoEs is expert parallelism, where each expert is distributed to a different GPU in the cluster.

Unlike traditional LLMs, MoEs with expert parallelism require each expert to communicate with the rest of the model, creating a dense communication pattern. The exact traffic heatmap depends on the gating network and the token distribution. In this section, we assume a uniform token distribution for simplicity. Figure~\ref{fig:moe_traffic} shows the traffic matrix of training the MoE-1.3B model from DeepSpeedMoE~\cite{rajbhandari2022deepspeedmoe}, with 16 DGX A100 servers. The model contains 128 experts. The static part of the model uses DP, while the MoE part uses expert parallelism. Since each GPU contains a different expert, a uniform token distribution generates a uniform all-to-all traffic pattern across the network. At first glance, such traffic patterns make the spine switch in the rail-optimized network crucial, as the traffic across GPUs on different rails will traverse through spine switches. However, as we show in the next section, we do not have to rely on the spine switches: using the HB domain as a forwarding step achieves near-optimal performance.

\begin{table}[t]
\scriptsize
\centering
\caption{Variables used in Section~\ref{sec:traffic_ana} and ~\ref{sec:routing}.}
\renewcommand{\arraystretch}{1} 
\linespread{1.1}\selectfont\centering
    \begin{tabular}{|P{15mm}|P{65mm}|}
    \hline
    $x$ & GPU grid dimension in HB domains. \\ \hline
    $y$ & GPU grid dimension in NIC domains. \\ \hline
    $C_F$ & Bandwidth of HB domains. \\ \hline
    $C_F$ & Bandwidth of NIC domains. \\ \hline
    $D$ & Data transfer size between a pair of GPUs. \\ \hline 
    $T_{a2a}^{Rail-opt}$ & All-to-all traffic completion time for Rail-optimized networks. \\ [0.3ex] \hline 
    $T_{a2a}^{Rail-only}$ & All-to-all traffic completion time for Rail-only networks. \\ [0.3ex] \hline 
    \end{tabular}
    \label{tab:var_anal}
\end{table}

% Figure environment removed

