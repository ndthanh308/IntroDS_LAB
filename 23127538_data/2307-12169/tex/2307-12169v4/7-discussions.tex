\section{Related Work} \label{sec:discussions}

\para{LLM trend.}  
The current growth rate of LLM computational and speed requirement outpaces the advancements in AI accelerators and network speed as Moore's law slows down, necessitating hyper-scale clusters 
and more efficient interconnects~\cite{sirius, dnnmodelgrowth}. The MegatornLM line of work pioneers LLM parallelization~\cite{shoeybi2020megatronlm,narayanan2021efficient,korthikanti2022reducing}. Our position to remove any-to-any network connectivity complements MegatronLM.
We also acknowledge ongoing efforts to reduce language models' size and resource requirements without compromising performance~\cite{dolly}. These works complement our design as our design reduces network resources and maintains performance even for smaller language models and clusters. Similarly, research directions that aim to directly reduce the amount of communication through quantization and compression, like DeepSpeed Zero++, also complement our approach~\cite{deepspeedzeropp}.


\para{LLM Inference.} This paper explores the training workload of LLMs, yet inference represents another significant part of the LLM product cycle. Inference demands fewer resources as it involves moving less data through the LLM and only computes the forward pass and multiple passes to generate response tokens~\cite{alpaserve}. Pope et al. developed specific parallelism for inference on TPU-v4 architecture~\cite{pope2022efficiently}. For our design, each HB domain becomes an inference-serving domain with low latency, and the Rail-only connections help load-balance multiple inference domains. We leave a detailed investigation of LLM inference to future work. \looseness=-1 

\para{Multi-job training.} It is common for a GPU cluster to train multiple smaller jobs simultaneously. Existing works focus on \fattree-based GPU clusters and provide techniques for better fairness and shorter job-completion time~\cite{gandiva,tiresias,muri,rajasekaran2023cassini}. While this paper focuses on training a single LLM on a large cluster, the Rail-only network design is also suitable for a multi-job setting. The entire cluster can be arbitrarily partitioned by tiling into smaller rectangular partitions, similar to the case of TPU-v4~\cite{jouppi2023tpu}. Each partition then independently executes a smaller training job.  \looseness=-1 

\para{ML infrastructures and other ML workloads.} Prior works illustrated the benefits of co-designing hardware and software for ML models. For instance, Google's TPU cluster is optimized for training large models with 3D parallelism on TPUs~\cite{jouppi2023tpu}, while Meta's Neo focuses on training recommendation models with large embedding tables~\cite{mudigere2023softwarehardware}. Our work focuses on designing a cost-efficient network to train LLMs efficiently. Although our proposed Rail-only architecture focuses on network design specifically for LLMs, our design is efficient for many other DNN workloads when combined with other efforts, as the forwarding overhead is low~(\S\ref{sec:routing}). Recent works attempt to make the parallelization strategy and collective communication algorithms bandwidth-aware for any DNN model~\cite{alpa, unity, zhao2023bandwidth}, producing traffic patterns ideal for the Rail-only network. \looseness=-1  



