
\section{Background} \label{sec:background}


\subsection{Intra-platform Connectivity: High-bandwidth Domain}
The rise of resource-intensive ML workloads led to the dominance of GPU-centric platforms optimized for multi-GPU workloads. To accommodate the communication demand, these platforms use high-bandwidth local interconnects within a local domain of GPUs. Depending on the manufacturer, these GPU-centric platforms differ in computing FLOPs, GPU and CPU architectures, or even physical interconnect technology. However, these platforms all share a unifying property: they provision  \textit{several Tbps of internal bandwidth} across GPUs. 

For instance, Nvidia's DGX H100 server~\cite{dgxh100} consists of eight H100 GPUs interconnected with NVSwitches, providing 3.6~Tbps of non-blocking bandwidth internally. The GB200 NVL72 computer announced recently connects 36 GB200 Superchips with fifth-generation NVLink within a rack at 7.2~Tbps per GPU~\cite{gb200nvl72}. The AMD MI300X platform, on the other hand, employs AMD's Infinity Fabric to connect eight MI300X accelerators in a full-mesh topology with 3.6~Tbps of bandwidth per GPU~\cite{mi300x}. Similar platforms such as Nvidia's DGX GH200 Super Computer have utilized multi-tiered NVSwich topologies to scale the platform's size up to 256 GPUs while maintaining 3.6~Tbps full-bisection intra-GPU bandwidth~\cite{gh200}.
This paper refers to a platform with Tbps internal bandwidth connectivity as a ``high-bandwidth (HB) domain", and the corresponding interconnect as HB interconnects (HBI).

\subsection{Inter-platform Connectivity: NIC Domain}

While GPU-centric platforms provide high internal bandwidth using NVLink or Infinity Fabric technologies, they can only scale to a limited number of GPUs. To expand beyond a single platform, operators rely on traditional network technologies, such as Ethernet or Infiniband, to connect the NICs of different platforms. This paper refers to the inter-platform network as the ``NIC domain." 

The state-of-the-art interconnection in the NIC domain is based on a well-known network architecture called a \textit{Rail-optimized network}~\cite{nccl212}. This architecture is ubiquitously used for high-performance computing (HPC) workloads. As we discuss next, Rail-optimized networks are better suited for DNNs than conventional CPU-centric datacenter networks. However, given that Rail-optimized networks are primarily designed for HPC workloads, they miss a significant opportunity to further leverage the unique traffic patterns of LLM training workloads (\S\ref{sec:traffic_ana}).\looseness=-1

First, let us consider a conventional datacenter design specialized to serve unpredictable and bursty CPU-heavy workloads. This architecture, known as a \fattree network~\cite{fattree, fbdcarch}, provides any-to-any connectivity between server pairs. \fattree networks are well-studied in the system and networking community and are the de facto infrastructure for storage, cloud, and map-reduce workloads.  \looseness=-1
 
The Rail-optimized network for GPU training clusters evolves from the datacenter \fattree network~\cite{nccl212, dgxh100archdoc}, illustrated in Figure~\ref{fig:Rail-optimized}. For a GPU platform with an HB domain of size $K$, there are $K$ total rails, where a \textit{rail} comprises GPUs with the same local rank that belong to different HB domains~\cite{multirail}. A Rail-optimized network places these GPUs under the same set of switches, which we denote as rail switches. Figure~\ref{fig:Rail-optimized} highlights rail one and rail $K$ in red and yellow colors, respectively. Connecting same-rank GPUs to the same rail switches ensures the lowest possible latency across them. Such connectivity is desirable because an optimal DNN parallelization strategy concentrates its NIC domain traffic between GPUs with the same local rank~\cite{nccl212}.

Rail-optimized architectures enjoy low latency between GPUs in the same rail. The rest of the network employs layers of spine switches to connect the rail switches to form a full-bisection any-to-any \fattree network topology. This network ensures that any pair of GPUs in different HB domains can still communicate at the network line rate of hundreds of Gbps. For instance, traffic between \texttt{GPU 1, Domain 1} and \texttt{GPU 1, Domain 2} traverses through Rail Switch 1 only, while traffic between \texttt{GPU 1, Domain 1} and \texttt{GPU 2, Domain 2} goes through the respective rails and the spine switches. \looseness=-1


While the Rail-optimized network architecture takes advantage of the strong locality of DNN training traffic by connecting the same-rank GPUs with the same ToR switch, it overlooks a fundamental question: Are the spine switches necessary? In the next section, we analyze LLM training traffic in greater detail to explore the potential for a spineless network architecture design.


