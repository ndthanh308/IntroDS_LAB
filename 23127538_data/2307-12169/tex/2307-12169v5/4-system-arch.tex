\section{Rail-only Network Design}\label{sec:system-arch}
Based on the observations above, this section proposes \textit{Rail-only}, a novel network architecture that diverts from the any-to-any GPU connectivity paradigm. We first introduce the architecture design of a Rail-only network. We then discuss routing policy and fault-tolerance properties of Rail-only interconnects.


\subsection{Architecture Design}
Figure~\ref{fig:Rail-only} illustrates our \textit{Rail-only} network architecture. Compared to a conventional Rail-optimized GPU cluster, shown in Figure~\ref{fig:Rail-optimized}, our Rail-only network keeps the HB domains but omits the full-bisection connectivity \textit{for all GPUs} in the NIC domain. Instead, we only ensure that \textit{GPUs within each rail} are connected with a full-bisection network.\looseness=-1 

% Figure environment removed 
A straightforward illustration of our proposed architecture is to remove the spine switches (Figure~\ref{fig:Rail-optimized}) and re-purpose the uplinks connecting rail switches to the spine as downlinks to GPUs. Hence, a dedicated \fattree network connects each rail. Compared to the Rail-optimized architecture, the Rail-only design saves the number of switches and transceivers by building multiple smaller \fattree networks for individual rails, requiring fewer layers of switches in the network.

\subsection{Routing in Rail-only Networks} \label{sec:routing}
Our Rail-only network architecture removes network connectivity across GPUs with different ranks in different rails. Such communication is still possible by forwarding the data through HB domains. For instance, a message from \texttt{GPU 1, Domain 1} to \texttt{GPU 2, Domain 2} in Figure~\ref{fig:Rail-only} can first route through the first HB domain to \texttt{GPU 2, Domain 1} and then be transmitted to the final destination through the network. Previous work has shown that such a routing scheme induces a \textit{bandwdith-tax}~\cite{rotornet, opera, topoopt}, where the physical traffic increases in the network due to forwarding. However, in this section, we show that due to the bandwidth asymmetry between the HB and the NIC domain, the performance degradation caused by bandwidth tax is negligible.  \looseness=-1

We use LLMs with MoE layers described in Section~\ref{sec:moe_traffic} as an example, as it generates a challenging all-to-all communication pattern. At first glance, this traffic pattern is challenging for the Rail-only network. Most of the all-to-all traffic requires forwarding. However, since the HB domain is much faster than the NIC domain, forwarding network traffic on the HB domain incurs a slight overhead. Below, we derive the slow-down factor for uniform all-to-all traffic using a two-step forwarding routing algorithm for the Rail-only network. Note that this strategy has already been implemented in NCCL as ``PCIe $\times$ NVLink" (PXN) to avoid congestion in cases where the spine layer of the Rail-optimized network is oversubscribed~\cite{nccl212}.\looseness=-1

We use the same variables defined in Table~\ref{tab:var_anal} in the following derivation. Consider a grid of $x\times y$ GPUs where $x$ GPUs are placed in an HB domain, and a Rail-only or Rail-optimized network connects $y$ HB domains. Recall that the HB domain has bandwidth $C_F$ while the NIC domain has bandwidth $C_S$ per GPU pair. For a Rail-optimized network, every GPU sends traffic directly to its destinations through the HB and full-bisection NIC domains. Assuming each pair of GPU communicates traffic of size $D$, the total all-to-all completion time is:
\begin{equation}
T_{a2a}^{Rail-opt}=\max(\frac{(x-1)D}{C_F}, \frac{x(y-1)D}{C_S})= \frac{x(y-1)D}{C_S}
\end{equation}
For the Rail-only network, the two-step algorithm first runs an all-to-all \textit{within each rail}, preparing each GPU to have all data to send on its rail. Then, within each rail, each GPU runs a second all-to-all \textit{in the HB domain} to finish the transfers with an effective shard size of $xD$. Note that the second step here contains the \textit{bandwidth-tax}. The total transmission time is
\begin{equation}
T_{a2a}^{Rail-only}=\frac{y(x-1)D}{C_F} + \frac{x(y-1)D}{C_S}
\end{equation}
The two terms differ by ${y(x-1)D}/{C_F}$, which is the cost of forwarding within HB domains. When $ y(x-1)\approx x(y-1)$, the slow-down factor is approximately $C_S/C_F$, which equals to $8.2\%$ and $11.2\%$ for the DGX A100 and DGX H100 generations of GPU platforms, respectively. This factor is low because of the bandwidth asymmetry between the two domains. Furthermore, this slow-down only applies to the all-to-all communication, which accounts for $27\%$ of the total traffic as shown in Figure~\ref{fig:moe_traffic}. Therefore, this small overhead is negligible by Amdah's law. We note that such properties also make our network design suitable for other classes of DNN models.   \looseness=-1


\subsection{Fault Tolerance Properties of Rail-only Networks} \label{sec:fail_tol}
Fault tolerance is crucial for large GPU clusters with long-lasting LLM training jobs. This section investigates the fault tolerance properties of  Rail-only networks compared to traditional Rail-optimized networks.

\para{Link and switch failures.} Suppose a rail switch or a link fails. GPUs connected to the failed switch or link become unavailable for both Rail-optimized and Rail-only network architectures, rendering the two designs identical regarding fault tolerance on switches and links. However, our design requires fewer switches, which naturally reduces the points of failure. Datacenter operators can add redundant capacity by including extra rail switches, and our design remains more cost-effective than the any-to-any network design. \looseness=-1 

\para{GPU platform failure.} For a GPU cluster composed of DGX-like servers, each server is an HB domain. When a server fails, the network operator migrates the task to another healthy server. The Rail-only connectivity remains the same for the new server. For a GB200-like cluster, a super-chip module is analogous to a server; thus, the failure mode is the same as a single GPU failure, which we will discuss next.  \looseness=-1

\para{Single GPU failures with idle GPU in the HB domain.} We discuss two distinct scenarios separately for single GPU failures. The first case is when another idle GPU presents the same HB domain as the failed one. In this case, a Rail-optimized network directly replaces the failed GPU with the idle one without breaking the HB domain integrity. One possible solution is to leverage optical reconfigurable switches for the Rail-only network. To improve robustness, we add a small number of optical switches between the GPU and rail switches to allow the dynamic reconfiguration of rails. When a GPU fails, the optical switch reconfigures to bring a healthy, idle GPU to replace the failed one. This approach is conceptually similar to the failure recovery mechanism of network designs that primarily uses optical-reconfigurable switches~\cite{jouppi2023tpu, topoopt, jupiterevolving}. We leave an in-depth analysis of rail-only with optical switch to future work.

\para{Single GPU failure in fully occupied HB domains.} Another failure mode occurs when a GPU fails in a fully occupied HB domain and requires a substitute GPU from different HB domains. In this case, the Rail-only design prevents migrating the task on the failed GPU to another idle one in the cluster, which is possible in a Rail-optimized network. However, such a solution is undesirable even with the Rail-optimized network. The new GPU no longer belongs to the same HB domain as the failed one, creating a bottleneck that slows the HB domain into a NIC domain. Instead, we propose two solutions. For platforms with small HB domains, we migrate the tasks on the entire HB domain with the failed GPU to a new one. For larger HB domains (e.g., DGX GH200 supercomputers with $K=256$), these HB domains comprise a multi-tiered topology with an optical core-layer~\cite{gh200}. One potential approach is to add optical switches, like in the previous failure case. When a GPU failure occurs, the optical switch reconfigures, replacing a small set of GPUs (including the failed one) with healthy ones, thus maintaining the integrity of an HB domain.  \looseness=-1

