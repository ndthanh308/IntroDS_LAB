\section{Introduction} \label{sec:introduction}

Large Language Models (LLMs) are among the most complex and computationally intensive Deep Neural Networks (DNNs). The GPT3 model from 2020 already requires 355 GPU-years on Nvidia's V100 GPUs~\cite{gpt3, gpt3time}, while the recent GPT4 model is estimated to have trillions of parameters and takes months to train~\cite{openai2023gpt4, gpt4params}. 
As Moore's law slows down, the growth rate of LLM size and computation requirement exceeds the advancement of accelerators, making hyper-scale GPU datacenters inevitable. Our conversations with lead machine learning architects in the industry indicate that the next-generation LLMs likely require over 30,000 GPUs of computing power to finish training within a reasonable time.

GPU manufacturers have invested in high-bandwidth platforms, such as NVLink~\cite{nvlnvs} and Infinity Fabric~\cite{mi300x}, to enable efficient multi-GPU training. These platforms provide several Tbps of bandwidth within a few GPUs but are not scalable. To connect multiple GPU platforms, state-of-the-art approaches rely on traditional lossless network solutions, such as RDMA over converged ethernet (RoCE) or Infiniband. In particular, today's GPU clusters employ an architecture called a ``Rail-optimized" network. This architecture is derived from the classical \fattree network~\cite{fattree} to provide any-to-any connectivity to \textit{all GPUs} in a training cluster.\looseness=-1  

However, scaling a \fattree network to tens of thousands of GPUs is challenging. Previous work demonstrated that large-scale lossless networks are prone to deadlocking and PFC storms~\cite{pfc_storm, rdma_azure, ib_deadlock, bfc,ddlindc}, degrading the performance. Furthermore, as the scale increases, \fattree architectures become prohibitively costly~\cite{topoopt}. For instance, today, a full-bisection Clos fabric interconnecting 30,000 GPUs with 400~Gbps network capacity costs \$200 million. At the same time, deploying such a network requires provisioning for $\sim$4.6 megawatts of peak power consumption. Consequently, datacenter providers resort to over-subscription to tame costs and energy consumption, worsening deadlocking and degraded performance problems. 

In this paper, we show that efficiently training LLMs \textit{does not} require any-to-any connectivity across all GPUs in the network, even for DNNs with sparsely gated Mixture-of-Expert (MoE) layers, which generate all-to-all communication (\S\ref{sec:moe_traffic}). As a result, we propose an immediately deployable solution to lower the cost and energy consumption of LLM datacenters with commodity electrical switches. To do so, we make two primary contributions. First, we analyze the traffic pattern of training LLMs (\S\ref{sec:traffic_ana}). We demonstrate that with an optimal parallelization strategy, an LLM training workload requires high-bandwidth any-to-any connectivity \textit{only within a small subset of GPUs}, and each subset fits within a single GPU platform, such as an Nvidia DGX server. Across the platforms, most communication occurs between \textit{a few GPU pairs with the same rank} throughout the cluster. As a result, the conventional any-to-any approach for building Clos networks adds unnecessary complexity, cost, and power consumption for distributed LLM training.

Motivated by the above observations, we then propose a low-cost network architecture that accurately reflects LLM communication requirements, called \textit{Rail-only} (\S\ref{sec:system-arch}). Instead of forming a \fattree to support any-to-any communication, as advocated by major GPU manufacturers~\cite{dgxh100}, our architecture removes the spine layer of switches and only connects sets of GPUs with significant network traffic. Hence, compared to the state-of-the-art Rail-optimized design, our network architecture removes the network equipment that does not carry significant traffic and achieves the same performance as a Rail-optimized network. We provide routing strategies that impose minimal performance overhead for all-to-all communication. We also analyze our design's fault-tolerance properties and provide recovery methods from failure cases (\S\ref{sec:fail_tol}). \looseness=-1

 
We evaluate the performance of our Rail-only network architecture using an analytical formulation and provide insights into the performance impact of different network and training parameters. We compare the cost and power consumption of a Rail-only network to a full-bisection bandwidth any-to-any \fattree network and show that our LLM-centric network architecture reduces the network cost and power by 38\%--76\% and 37\%--75\%, respectively (\S\ref{sec:cost}). Moreover, we show that a Rail-only network achieves the same performance as a Rail-optimized cluster for LLMs without MoE layers. Finally, we demonstrate that a Rail-only interconnect only incurs 8.2\%--11.2\% throughput overhead for LLMs with MoEs that require all-to-all traffic.  \looseness=-1
