\begin{thebibliography}{53}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Artetxe et~al.(2022)Artetxe, Bhosale, Goyal, Mihaylov, Ott, Shleifer, Lin, Du, Iyer, Pasunuru, Anantharaman, Li, Chen, Akin, Baines, Martin, Zhou, Koura, O'Horo, Wang, Zettlemoyer, Diab, Kozareva, and Stoyanov]{artetxe2022efficient}
Artetxe, M., Bhosale, S., Goyal, N., Mihaylov, T., Ott, M., Shleifer, S., Lin, X.~V., Du, J., Iyer, S., Pasunuru, R., Anantharaman, G., Li, X., Chen, S., Akin, H., Baines, M., Martin, L., Zhou, X., Koura, P.~S., O'Horo, B., Wang, J., Zettlemoyer, L., Diab, M., Kozareva, Z., and Stoyanov, V.
\newblock Efficient large scale language modeling with mixtures of experts, 2022.

\bibitem[Bai et~al.(2023)Bai, Abdeen, Agrawal, Attre, Bahl, Bhagat, Bhaskara, Brokhman, Cao, Cheema, Chow, Cohen, Elhaddad, Ette, Figlin, Firestone, George, German, Ghai, Green, Greenberg, Gupta, Haagens, Hendel, Howlader, John, Johnstone, Jolly, Kramer, Kruse, Kumar, Lan, Lee, Levy, Lipshteyn, Liu, Liu, Lu, Lu, Lu, Makhervaks, Malashanka, Maltz, Marinos, Mehta, Murthi, Namdhari, Ogus, Padhye, Pandya, Phillips, Power, Puri, Raindel, Rhee, Russo, Sah, Sheriff, Sparacino, Srivastava, Sun, Swanson, Tian, Tomczyk, Vadlamuri, Wolman, Xie, Yom, Yuan, Zhang, and Zill]{rdma_azure}
Bai, W., Abdeen, S.~S., Agrawal, A., Attre, K.~K., Bahl, P., Bhagat, A., Bhaskara, G., Brokhman, T., Cao, L., Cheema, A., Chow, R., Cohen, J., Elhaddad, M., Ette, V., Figlin, I., Firestone, D., George, M., German, I., Ghai, L., Green, E., Greenberg, A., Gupta, M., Haagens, R., Hendel, M., Howlader, R., John, N., Johnstone, J., Jolly, T., Kramer, G., Kruse, D., Kumar, A., Lan, E., Lee, I., Levy, A., Lipshteyn, M., Liu, X., Liu, C., Lu, G., Lu, Y., Lu, X., Makhervaks, V., Malashanka, U., Maltz, D.~A., Marinos, I., Mehta, R., Murthi, S., Namdhari, A., Ogus, A., Padhye, J., Pandya, M., Phillips, D., Power, A., Puri, S., Raindel, S., Rhee, J., Russo, A., Sah, M., Sheriff, A., Sparacino, C., Srivastava, A., Sun, W., Swanson, N., Tian, F., Tomczyk, L., Vadlamuri, V., Wolman, A., Xie, Y., Yom, J., Yuan, L., Zhang, Y., and Zill, B.
\newblock Empowering azure storage with {RDMA}.
\newblock In \emph{20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23)}, pp.\  49--67, Boston, MA, April 2023. USENIX Association.
\newblock ISBN 978-1-939133-33-5.
\newblock URL \url{https://www.usenix.org/conference/nsdi23/presentation/bai}.

\bibitem[Bai et~al.(2021)Bai, Li, Zhou, Yi, Gong, Yan, Chen, and Xu]{grad_comp}
Bai, Y., Li, C., Zhou, Q., Yi, J., Gong, P., Yan, F., Chen, R., and Xu, Y.
\newblock Gradient compression supercharged high-performance data parallel dnn training.
\newblock In \emph{Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles}, SOSP '21, pp.\  359–375, New York, NY, USA, 2021. Association for Computing Machinery.
\newblock ISBN 9781450387095.
\newblock \doi{10.1145/3477132.3483553}.
\newblock URL \url{https://doi.org/10.1145/3477132.3483553}.

\bibitem[Ballani et~al.(2020)Ballani, Costa, Behrendt, Cletheroe, Haller, Jozwik, Karinou, Lange, Shi, Thomsen, and Williams]{sirius}
Ballani, H., Costa, P., Behrendt, R., Cletheroe, D., Haller, I., Jozwik, K., Karinou, F., Lange, S., Shi, K., Thomsen, B., and Williams, H.
\newblock Sirius: A flat datacenter network with nanosecond optical switching.
\newblock In \emph{Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols for Computer Communication}, SIGCOMM '20, pp.\  782–797, New York, NY, USA, 2020. Association for Computing Machinery.
\newblock ISBN 9781450379557.
\newblock \doi{10.1145/3387514.3406221}.
\newblock URL \url{https://doi.org/10.1145/3387514.3406221}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{gpt3}
Brown, T.~B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D.~M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D.
\newblock Language models are few-shot learners, 2020.

\bibitem[Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra, Roberts, Barham, Chung, Sutton, Gehrmann, Schuh, Shi, Tsvyashchenko, Maynez, Rao, Barnes, Tay, Shazeer, Prabhakaran, Reif, Du, Hutchinson, Pope, Bradbury, Austin, Isard, Gur-Ari, Yin, Duke, Levskaya, Ghemawat, Dev, Michalewski, Garcia, Misra, Robinson, Fedus, Zhou, Ippolito, Luan, Lim, Zoph, Spiridonov, Sepassi, Dohan, Agrawal, Omernick, Dai, Pillai, Pellat, Lewkowycz, Moreira, Child, Polozov, Lee, Zhou, Wang, Saeta, Diaz, Firat, Catasta, Wei, Meier-Hellstern, Eck, Dean, Petrov, and Fiedel]{chowdhery2022palm}
Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H.~W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B., Pope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari, G., Yin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev, S., Michalewski, H., Garcia, X., Misra, V., Robinson, K., Fedus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph, B., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal, S., Omernick, M., Dai, A.~M., Pillai, T.~S., Pellat, M., Lewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee, K., Zhou, Z., Wang, X., Saeta, B., Diaz, M., Firat, O., Catasta, M., Wei, J., Meier-Hellstern, K., Eck, D., Dean, J., Petrov, S., and Fiedel, N.
\newblock Palm: Scaling language modeling with pathways, 2022.

\bibitem[Dao(2023)]{dao2023flashattention2}
Dao, T.
\newblock Flashattention-2: Faster attention with better parallelism and work partitioning, 2023.

\bibitem[Databricks(2023)]{dolly}
Databricks.
\newblock Hello dolly: Democratizing the magic of chatgpt with open models, 2023.
\newblock URL \url{https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html}.

\bibitem[Fedus et~al.(2022)Fedus, Zoph, and Shazeer]{fedus2022switch}
Fedus, W., Zoph, B., and Shazeer, N.
\newblock Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity, 2022.

\bibitem[Goyal et~al.(2022)Goyal, Shah, Zhao, Nikolaidis, Alizadeh, and Anderson]{bfc}
Goyal, P., Shah, P., Zhao, K., Nikolaidis, G., Alizadeh, M., and Anderson, T.~E.
\newblock Backpressure flow control.
\newblock In \emph{19th USENIX Symposium on Networked Systems Design and Implementation (NSDI 22)}, pp.\  779--805, Renton, WA, April 2022. USENIX Association.
\newblock ISBN 978-1-939133-27-4.
\newblock URL \url{https://www.usenix.org/conference/nsdi22/presentation/goyal}.

\bibitem[Gu et~al.(2019)Gu, Chowdhury, Shin, Zhu, Jeon, Qian, Liu, and Guo]{tiresias}
Gu, J., Chowdhury, M., Shin, K.~G., Zhu, Y., Jeon, M., Qian, J., Liu, H., and Guo, C.
\newblock Tiresias: A {GPU} cluster manager for distributed deep learning.
\newblock In \emph{16th USENIX Symposium on Networked Systems Design and Implementation (NSDI 19)}, pp.\  485--500, Boston, MA, February 2019. USENIX Association.
\newblock ISBN 978-1-931971-49-2.
\newblock URL \url{https://www.usenix.org/conference/nsdi19/presentation/gu}.

\bibitem[Guo et~al.(2016)Guo, Wu, Deng, Soni, Ye, Padhye, and Lipshteyn]{pfc_storm}
Guo, C., Wu, H., Deng, Z., Soni, G., Ye, J., Padhye, J., and Lipshteyn, M.
\newblock Rdma over commodity ethernet at scale.
\newblock In \emph{Proceedings of the 2016 ACM SIGCOMM Conference}, SIGCOMM '16, pp.\  202–215, New York, NY, USA, 2016. Association for Computing Machinery.
\newblock ISBN 9781450341936.
\newblock \doi{10.1145/2934872.2934908}.
\newblock URL \url{https://doi.org/10.1145/2934872.2934908}.

\bibitem[Harlap et~al.(2018)Harlap, Narayanan, Phanishayee, Seshadri, Devanur, Ganger, and Gibbons]{harlap2018pipedream}
Harlap, A., Narayanan, D., Phanishayee, A., Seshadri, V., Devanur, N., Ganger, G., and Gibbons, P.
\newblock Pipedream: Fast and efficient pipeline parallel dnn training, 2018.

\bibitem[Hu et~al.(2016)Hu, Zhu, Cheng, Guo, Tan, Padhye, and Chen]{ddlindc}
Hu, S., Zhu, Y., Cheng, P., Guo, C., Tan, K., Padhye, J., and Chen, K.
\newblock Deadlocks in datacenter networks: Why do they form, and how to avoid them.
\newblock In \emph{Proceedings of the 15th ACM Workshop on Hot Topics in Networks}, HotNets '16, pp.\  92–98, New York, NY, USA, 2016. Association for Computing Machinery.
\newblock ISBN 9781450346610.
\newblock \doi{10.1145/3005745.3005760}.
\newblock URL \url{https://doi.org/10.1145/3005745.3005760}.

\bibitem[Jia et~al.(2019)Jia, Zaharia, and Aiken]{flex_flow}
Jia, Z., Zaharia, M., and Aiken, A.
\newblock Beyond data and model parallelism for deep neural networks.
\newblock \emph{SysML}, 2019.
\newblock URL \url{https://mlsys.org/Conferences/2019/doc/2019/16.pdf}.

\bibitem[Jouppi et~al.(2023)Jouppi, Kurian, Li, Ma, Nagarajan, Nai, Patil, Subramanian, Swing, Towles, Young, Zhou, Zhou, and Patterson]{jouppi2023tpu}
Jouppi, N.~P., Kurian, G., Li, S., Ma, P., Nagarajan, R., Nai, L., Patil, N., Subramanian, S., Swing, A., Towles, B., Young, C., Zhou, X., Zhou, Z., and Patterson, D.
\newblock Tpu v4: An optically reconfigurable supercomputer for machine learning with hardware support for embeddings, 2023.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei]{kaplan2020scaling}
Kaplan, J., McCandlish, S., Henighan, T., Brown, T.~B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D.
\newblock Scaling laws for neural language models, 2020.

\bibitem[Khani et~al.(2021)Khani, Ghobadi, Alizadeh, Zhu, Glick, Bergman, Vahdat, Klenk, and Ebrahimi]{sipml}
Khani, M., Ghobadi, M., Alizadeh, M., Zhu, Z., Glick, M., Bergman, K., Vahdat, A., Klenk, B., and Ebrahimi, E.
\newblock Sip-ml: High-bandwidth optical network interconnects for machine learning training.
\newblock In \emph{Proceedings of the 2021 ACM SIGCOMM 2021 Conference}, SIGCOMM '21, pp.\  657–675, New York, NY, USA, 2021. Association for Computing Machinery.
\newblock ISBN 9781450383837.
\newblock \doi{10.1145/3452296.3472900}.
\newblock URL \url{https://doi.org/10.1145/3452296.3472900}.

\bibitem[Korthikanti et~al.(2022)Korthikanti, Casper, Lym, McAfee, Andersch, Shoeybi, and Catanzaro]{korthikanti2022reducing}
Korthikanti, V., Casper, J., Lym, S., McAfee, L., Andersch, M., Shoeybi, M., and Catanzaro, B.
\newblock Reducing activation recomputation in large transformer models, 2022.

\bibitem[Labs(2020)]{gpt3time}
Labs, L.
\newblock Openai's gpt-3 language model: A technical overview, 2020.
\newblock URL \url{https://lambdalabs.com/blog/demystifying-gpt-3}.

\bibitem[Li et~al.(2022)Li, Xue, Baranwal, Li, and You]{li2022sequence}
Li, S., Xue, F., Baranwal, C., Li, Y., and You, Y.
\newblock Sequence parallelism: Long sequence training from system perspective, 2022.

\bibitem[Li et~al.(2023)Li, Zheng, Zhong, Liu, Sheng, Jin, Huang, Chen, Zhang, Gonzalez, and Stoica]{alpaserve}
Li, Z., Zheng, L., Zhong, Y., Liu, V., Sheng, Y., Jin, X., Huang, Y., Chen, Z., Zhang, H., Gonzalez, J.~E., and Stoica, I.
\newblock {AlpaServe}: Statistical multiplexing with model parallelism for deep learning serving.
\newblock In \emph{17th USENIX Symposium on Operating Systems Design and Implementation (OSDI 23)}, Boston, MA, July 2023. USENIX Association.
\newblock URL \url{https://www.usenix.org/conference/osdi23/presentation/li-zhouhan}.

\bibitem[Meta(2023)]{meta-ethernet}
Meta.
\newblock Meta platforms is determined to make ethernet work for ai, 2023.
\newblock URL \url{https://www.nextplatform.com/2023/09/26/meta-platforms-is-determined-to-make-ethernet-work-for-ai/}.

\bibitem[Microsoft(2023)]{deepspeedzeropp}
Microsoft.
\newblock Deepspeed zero++: A leap in speed for llm and chat model training with 4x less communication, 2023.
\newblock URL \url{https://www.microsoft.com/en-us/research/blog/deepspeed-zero-a-leap-in-speed-for-llm-and-chat-model-training-with-4x-less-communication/}.

\bibitem[Mittal et~al.(2018)Mittal, Shpiner, Panda, Zahavi, Krishnamurthy, Ratnasamy, and Shenker]{revisitrdmanet}
Mittal, R., Shpiner, A., Panda, A., Zahavi, E., Krishnamurthy, A., Ratnasamy, S., and Shenker, S.
\newblock Revisiting network support for rdma.
\newblock In \emph{Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication}, SIGCOMM '18, pp.\  313–326, New York, NY, USA, 2018. Association for Computing Machinery.
\newblock ISBN 9781450355674.
\newblock \doi{10.1145/3230543.3230557}.
\newblock URL \url{https://doi.org/10.1145/3230543.3230557}.

\bibitem[Mudigere et~al.(2022)Mudigere, Hao, Huang, Jia, Tulloch, Sridharan, Liu, Ozdal, Nie, Park, Luo, Yang, Gao, Ivchenko, Basant, Hu, Yang, Ardestani, Wang, Komuravelli, Chu, Yilmaz, Li, Qian, Feng, Ma, Yang, Wen, Li, Yang, Sun, Zhao, Melts, Dhulipala, Kishore, Graf, Eisenman, Matam, Gangidi, Chen, Krishnan, Nayak, Nair, Muthiah, khorashadi, Bhattacharya, Lapukhov, Naumov, Mathews, Qiao, Smelyanskiy, Jia, and Rao]{swhwcodesign}
Mudigere, D., Hao, Y., Huang, J., Jia, Z., Tulloch, A., Sridharan, S., Liu, X., Ozdal, M., Nie, J., Park, J., Luo, L., Yang, J.~A., Gao, L., Ivchenko, D., Basant, A., Hu, Y., Yang, J., Ardestani, E.~K., Wang, X., Komuravelli, R., Chu, C.-H., Yilmaz, S., Li, H., Qian, J., Feng, Z., Ma, Y., Yang, J., Wen, E., Li, H., Yang, L., Sun, C., Zhao, W., Melts, D., Dhulipala, K., Kishore, K., Graf, T., Eisenman, A., Matam, K.~K., Gangidi, A., Chen, G.~J., Krishnan, M., Nayak, A., Nair, K., Muthiah, B., khorashadi, M., Bhattacharya, P., Lapukhov, P., Naumov, M., Mathews, A., Qiao, L., Smelyanskiy, M., Jia, B., and Rao, V.
\newblock Software-hardware co-design for fast and scalable training of deep learning recommendation models.
\newblock In \emph{Proceedings of the 49th Annual International Symposium on Computer Architecture}, ISCA '22, pp.\  993–1011, New York, NY, USA, 2022. Association for Computing Machinery.
\newblock ISBN 9781450386104.
\newblock \doi{10.1145/3470496.3533727}.
\newblock URL \url{https://doi.org/10.1145/3470496.3533727}.

\bibitem[Mudigere et~al.(2023)Mudigere, Hao, Huang, Jia, Tulloch, Sridharan, Liu, Ozdal, Nie, Park, Luo, Yang, Gao, Ivchenko, Basant, Hu, Yang, Ardestani, Wang, Komuravelli, Chu, Yilmaz, Li, Qian, Feng, Ma, Yang, Wen, Li, Yang, Sun, Zhao, Melts, Dhulipala, Kishore, Graf, Eisenman, Matam, Gangidi, Chen, Krishnan, Nayak, Nair, Muthiah, khorashadi, Bhattacharya, Lapukhov, Naumov, Mathews, Qiao, Smelyanskiy, Jia, and Rao]{mudigere2023softwarehardware}
Mudigere, D., Hao, Y., Huang, J., Jia, Z., Tulloch, A., Sridharan, S., Liu, X., Ozdal, M., Nie, J., Park, J., Luo, L., Yang, J.~A., Gao, L., Ivchenko, D., Basant, A., Hu, Y., Yang, J., Ardestani, E.~K., Wang, X., Komuravelli, R., Chu, C.-H., Yilmaz, S., Li, H., Qian, J., Feng, Z., Ma, Y., Yang, J., Wen, E., Li, H., Yang, L., Sun, C., Zhao, W., Melts, D., Dhulipala, K., Kishore, K., Graf, T., Eisenman, A., Matam, K.~K., Gangidi, A., Chen, G.~J., Krishnan, M., Nayak, A., Nair, K., Muthiah, B., khorashadi, M., Bhattacharya, P., Lapukhov, P., Naumov, M., Mathews, A., Qiao, L., Smelyanskiy, M., Jia, B., and Rao, V.
\newblock Software-hardware co-design for fast and scalable training of deep learning recommendation models, 2023.

\bibitem[Narayanan et~al.(2021)Narayanan, Shoeybi, Casper, LeGresley, Patwary, Korthikanti, Vainbrand, Kashinkunti, Bernauer, Catanzaro, Phanishayee, and Zaharia]{narayanan2021efficient}
Narayanan, D., Shoeybi, M., Casper, J., LeGresley, P., Patwary, M., Korthikanti, V.~A., Vainbrand, D., Kashinkunti, P., Bernauer, J., Catanzaro, B., Phanishayee, A., and Zaharia, M.
\newblock Efficient large-scale language model training on gpu clusters using megatron-lm, 2021.

\bibitem[Nvidia(2023{\natexlab{a}})]{dgxh100archdoc}
Nvidia.
\newblock Nvidia dgx superpod: Next generation scalable infrastructure for ai leadership, reference architecture, 2023{\natexlab{a}}.
\newblock URL \url{https://docs.nvidia.com/dgx-superpod-reference-architecture-with-dgx-h100-systems.pdf}.

\bibitem[Nvidia(2023{\natexlab{b}})]{gh200}
Nvidia.
\newblock Nvidia dgx gh200, 2023{\natexlab{b}}.
\newblock URL \url{https://www.nvidia.com/en-us/data-center/dgx-gh200/}.

\bibitem[Nvidia(2023{\natexlab{c}})]{gpudirect}
Nvidia.
\newblock Nvidia gpudirect: Enhancing data movement and access for gpus, 2023{\natexlab{c}}.
\newblock URL \url{https://developer.nvidia.com/gpudirect}.

\bibitem[Nvidia(2023{\natexlab{d}})]{nvlnvs}
Nvidia.
\newblock Nvlink and nvswitch: The building blocks of advanced multi-gpu communication—within and between servers., 2023{\natexlab{d}}.
\newblock URL \url{https://www.nvidia.com/en-us/data-center/nvlink/}.

\bibitem[OpenAI(2023{\natexlab{a}})]{dnnmodelgrowth}
OpenAI.
\newblock Openai: Ai and compute, 2023{\natexlab{a}}.
\newblock URL \url{https://openai.com/research/ai-and-compute}.

\bibitem[OpenAI(2023{\natexlab{b}})]{openai2023gpt4}
OpenAI.
\newblock Gpt-4 technical report, 2023{\natexlab{b}}.

\bibitem[Pope et~al.(2022)Pope, Douglas, Chowdhery, Devlin, Bradbury, Levskaya, Heek, Xiao, Agrawal, and Dean]{pope2022efficiently}
Pope, R., Douglas, S., Chowdhery, A., Devlin, J., Bradbury, J., Levskaya, A., Heek, J., Xiao, K., Agrawal, S., and Dean, J.
\newblock Efficiently scaling transformer inference, 2022.

\bibitem[Poutievski et~al.(2022)Poutievski, Mashayekhi, Ong, Singh, Tariq, Wang, Zhang, Beauregard, Conner, Gribble, Kapoor, Kratzer, Li, Liu, Nagaraj, Ornstein, Sawhney, Urata, Vicisano, Yasumura, Zhang, Zhou, and Vahdat]{jupiterevolving}
Poutievski, L., Mashayekhi, O., Ong, J., Singh, A., Tariq, M., Wang, R., Zhang, J., Beauregard, V., Conner, P., Gribble, S., Kapoor, R., Kratzer, S., Li, N., Liu, H., Nagaraj, K., Ornstein, J., Sawhney, S., Urata, R., Vicisano, L., Yasumura, K., Zhang, S., Zhou, J., and Vahdat, A.
\newblock Jupiter evolving: Transforming google's datacenter network via optical circuit switches and software-defined networking.
\newblock In \emph{Proceedings of the ACM SIGCOMM 2022 Conference}, SIGCOMM '22, pp.\  66–85, New York, NY, USA, 2022. Association for Computing Machinery.
\newblock ISBN 9781450394208.
\newblock \doi{10.1145/3544216.3544265}.
\newblock URL \url{https://doi.org/10.1145/3544216.3544265}.

\bibitem[Rajasekaran et~al.(2023)Rajasekaran, Ghobadi, and Akella]{rajasekaran2023cassini}
Rajasekaran, S., Ghobadi, M., and Akella, A.
\newblock Cassini: Network-aware job scheduling in machine learning clusters, 2023.

\bibitem[Rajbhandari et~al.(2022)Rajbhandari, Li, Yao, Zhang, Aminabadi, Awan, Rasley, and He]{rajbhandari2022deepspeedmoe}
Rajbhandari, S., Li, C., Yao, Z., Zhang, M., Aminabadi, R.~Y., Awan, A.~A., Rasley, J., and He, Y.
\newblock Deepspeed-moe: Advancing mixture-of-experts inference and training to power next-generation ai scale, 2022.

\bibitem[Schneider et~al.(2016)Schneider, Bibartiu, and Hoefler]{ib_deadlock}
Schneider, T., Bibartiu, O., and Hoefler, T.
\newblock Ensuring deadlock-freedom in low-diameter infiniband networks.
\newblock In \emph{2016 IEEE 24th Annual Symposium on High-Performance Interconnects (HOTI)}, pp.\  1--8, 2016.
\newblock \doi{10.1109/HOTI.2016.015}.

\bibitem[Shainer et~al.(2011)Shainer, Lui, and Liu]{mlnxrdmagpu}
Shainer, G., Lui, P., and Liu, T.
\newblock The development of mellanox/nvidia gpudirect over infiniband: A new model for gpu to gpu communications.
\newblock In \emph{Proceedings of the 2011 TeraGrid Conference: Extreme Digital Discovery}, TG '11, New York, NY, USA, 2011. Association for Computing Machinery.
\newblock ISBN 9781450308885.
\newblock \doi{10.1145/2016741.2016769}.
\newblock URL \url{https://doi.org/10.1145/2016741.2016769}.

\bibitem[Shoeybi et~al.(2020)Shoeybi, Patwary, Puri, LeGresley, Casper, and Catanzaro]{shoeybi2020megatronlm}
Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., and Catanzaro, B.
\newblock Megatron-lm: Training multi-billion parameter language models using model parallelism, 2020.

\bibitem[The-Decoder(2023)]{gpt4params}
The-Decoder.
\newblock Gpt-4 has a trillion parameters - report, 2023.
\newblock URL \url{https://the-decoder.com/gpt-4-has-a-trillion-parameters/}.

\bibitem[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozière, Goyal, Hambro, Azhar, Rodriguez, Joulin, Grave, and Lample]{touvron2023llama}
Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lample, G.
\newblock Llama: Open and efficient foundation language models, 2023.

\bibitem[Unger et~al.(2022)Unger, Jia, Wu, Lin, Baines, Narvaez, Ramakrishnaiah, Prajapati, McCormick, Mohd-Yusof, Luo, Mudigere, Park, Smelyanskiy, and Aiken]{unity}
Unger, C., Jia, Z., Wu, W., Lin, S., Baines, M., Narvaez, C. E.~Q., Ramakrishnaiah, V., Prajapati, N., McCormick, P., Mohd-Yusof, J., Luo, X., Mudigere, D., Park, J., Smelyanskiy, M., and Aiken, A.
\newblock Unity: Accelerating {DNN} training through joint optimization of algebraic transformations and parallelization.
\newblock In \emph{16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)}, pp.\  267--284, Carlsbad, CA, July 2022. USENIX Association.
\newblock ISBN 978-1-939133-28-1.
\newblock URL \url{https://www.usenix.org/conference/osdi22/presentation/unger}.

\bibitem[Wang et~al.(2019)Wang, Huang, and Li]{tofu}
Wang, M., Huang, C.-c., and Li, J.
\newblock Supporting very large models using automatic dataflow graph partitioning.
\newblock In \emph{Proceedings of the Fourteenth EuroSys Conference 2019}, EuroSys '19, New York, NY, USA, 2019. Association for Computing Machinery.
\newblock ISBN 9781450362818.
\newblock \doi{10.1145/3302424.3303953}.
\newblock URL \url{https://doi.org/10.1145/3302424.3303953}.

\bibitem[Wang et~al.(2023{\natexlab{a}})Wang, Khazraee, Zhong, Ghobadi, Jia, Mudigere, Zhang, and Kewitsch]{topoopt}
Wang, W., Khazraee, M., Zhong, Z., Ghobadi, M., Jia, Z., Mudigere, D., Zhang, Y., and Kewitsch, A.
\newblock {TopoOpt}: Co-optimizing network topology and parallelization strategy for distributed training jobs.
\newblock In \emph{20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23)}, pp.\  739--767, Boston, MA, April 2023{\natexlab{a}}. USENIX Association.
\newblock ISBN 978-1-939133-33-5.
\newblock URL \url{https://www.usenix.org/conference/nsdi23/presentation/wang-weiyang}.

\bibitem[Wang et~al.(2023{\natexlab{b}})Wang, Luo, Ning, Zeng, Li, Wan, Xie, Feng, Cheng, Geng, Wang, Ling, Huo, An, Ji, Zhang, Xu, Feng, Ding, Chen, and Guo]{srnic}
Wang, Z., Luo, L., Ning, Q., Zeng, C., Li, W., Wan, X., Xie, P., Feng, T., Cheng, K., Geng, X., Wang, T., Ling, W., Huo, K., An, P., Ji, K., Zhang, S., Xu, B., Feng, R., Ding, T., Chen, K., and Guo, C.
\newblock {SRNIC}: A scalable architecture for {RDMA} {NICs}.
\newblock In \emph{20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23)}, pp.\  1--14, Boston, MA, April 2023{\natexlab{b}}. USENIX Association.
\newblock ISBN 978-1-939133-33-5.
\newblock URL \url{https://www.usenix.org/conference/nsdi23/presentation/wang-zilong}.

\bibitem[Xiao et~al.(2018)Xiao, Bhardwaj, Ramjee, Sivathanu, Kwatra, Han, Patel, Peng, Zhao, Zhang, Yang, and Zhou]{gandiva}
Xiao, W., Bhardwaj, R., Ramjee, R., Sivathanu, M., Kwatra, N., Han, Z., Patel, P., Peng, X., Zhao, H., Zhang, Q., Yang, F., and Zhou, L.
\newblock Gandiva: Introspective cluster scheduling for deep learning.
\newblock In \emph{13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)}, pp.\  595--610, Carlsbad, CA, October 2018. USENIX Association.
\newblock ISBN 978-1-939133-08-3.
\newblock URL \url{https://www.usenix.org/conference/osdi18/presentation/xiao}.

\bibitem[Zhao \& Krishnamurthy(2023)Zhao and Krishnamurthy]{zhao2023bandwidth}
Zhao, L. and Krishnamurthy, A.
\newblock Bandwidth optimal pipeline schedule for collective communication, 2023.

\bibitem[Zhao et~al.(2022{\natexlab{a}})Zhao, Pal, Chugh, Wang, Basu, Khoury, and Krishnamurthy]{zhao2022optimal}
Zhao, L., Pal, S., Chugh, T., Wang, W., Basu, P., Khoury, J., and Krishnamurthy, A.
\newblock Optimal direct-connect topologies for collective communications, 2022{\natexlab{a}}.

\bibitem[Zhao et~al.(2022{\natexlab{b}})Zhao, Liu, Peng, Zhu, Liu, and Jin]{muri}
Zhao, Y., Liu, Y., Peng, Y., Zhu, Y., Liu, X., and Jin, X.
\newblock Multi-resource interleaving for deep learning training.
\newblock In \emph{Proceedings of the ACM SIGCOMM 2022 Conference}, SIGCOMM '22, pp.\  428–440, New York, NY, USA, 2022{\natexlab{b}}. Association for Computing Machinery.
\newblock ISBN 9781450394208.
\newblock \doi{10.1145/3544216.3544224}.
\newblock URL \url{https://doi.org/10.1145/3544216.3544224}.

\bibitem[Zheng et~al.(2022)Zheng, Li, Zhang, Zhuang, Chen, Huang, Wang, Xu, Zhuo, Xing, Gonzalez, and Stoica]{alpa}
Zheng, L., Li, Z., Zhang, H., Zhuang, Y., Chen, Z., Huang, Y., Wang, Y., Xu, Y., Zhuo, D., Xing, E.~P., Gonzalez, J.~E., and Stoica, I.
\newblock Alpa: Automating inter- and {Intra-Operator} parallelism for distributed deep learning.
\newblock In \emph{16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)}, pp.\  559--578, Carlsbad, CA, July 2022. USENIX Association.
\newblock ISBN 978-1-939133-28-1.
\newblock URL \url{https://www.usenix.org/conference/osdi22/presentation/zheng-lianmin}.

\bibitem[Zhu et~al.(2015)Zhu, Eran, Firestone, Guo, Lipshteyn, Liron, Padhye, Raindel, Yahia, and Zhang]{rdma_cc}
Zhu, Y., Eran, H., Firestone, D., Guo, C., Lipshteyn, M., Liron, Y., Padhye, J., Raindel, S., Yahia, M.~H., and Zhang, M.
\newblock Congestion control for large-scale rdma deployments.
\newblock In \emph{Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication}, SIGCOMM '15, pp.\  523–536, New York, NY, USA, 2015. Association for Computing Machinery.
\newblock ISBN 9781450335423.
\newblock \doi{10.1145/2785956.2787484}.
\newblock URL \url{https://doi.org/10.1145/2785956.2787484}.

\end{thebibliography}
