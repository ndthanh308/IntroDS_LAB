\vspace*{-1mm}
\section{Conclusion} \label{sec:conclusion}
This paper challenges the conventional any-to-any network architecture for GPU clusters for training large language models. We propose a new \textit{rail-only architecture} that aligns with LLMs' distinct characteristics and demands, leading to 37\% to 75\% cost reductions while maintaining identical performance to the current state-of-the-art \fattree networks.\looseness=-1  