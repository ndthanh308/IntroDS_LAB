\section{Introduction} \label{sec:introduction}

LLMs are among the largest and most computationally intensive Deep Neural Networks (DNNs). The latest GPT4 model is estimated to have trillions of parameters and take months to train~\cite{openai2023gpt4, gpt4params}. 
Conventionally, researchers seek to enhance the performance of distributed DNN training and inference through optimizing parallelization strategies~\cite{flex_flow, alpa, unity, tofu}, sophisticated scheduling~\cite{muri, gandiva, tiresias}, advanced compression~\cite{grad_comp}, and even the reconfiguration of the network topology itself~\cite{sipml, topoopt, zhao2022optimal}. Despite these efforts, LLMs still require significant raw computing power. The GPT3 model from 2020 already requires 355 GPU-years on Nvidia's V100 GPUs~\cite{gpt3, gpt3time}. As Moore's law slows down, the growth rate of LLM size and computation requirement exceeds the advancement of accelerators, making hyper-scale GPU clusters inevitable. 
Our conversations with lead machine learning architects in the industry indicate that the next-generation LLMs likely require over 30,000 GPUs of computing power to finish training within a reasonable time. At the same time, scaling the cluster to 32,000 GPUs also allows LLM designers to train smaller models like LLaMa-65B~\cite{touvron2023llama} within a day~\cite{meta-ethernet}, expediating future development.

A GPU-centric cluster typically employs two types of interconnection domains~\cite{dgxh100archdoc}. First, a high-bandwidth domain where a few GPUs (e.g., eight for a DGX H100 server) are interconnected with high bandwidth, but short-range communication links like NVLinks~\cite{nvlnvs}. We refer to this type of interconnection as the HB domain. 
The second interconnection domain forms a network capable of any-to-any GPU communication using RDMA-capable NICs, connected in a \fattree network architecture. The cluster uses the RDMA protocol on this network to benefit from bypassing CPU and OS entirely through GPU-Direct~\cite{mlnxrdmagpu, gpudirect}. 

However, scaling up RDMA networks to tens of thousands of GPUs is challenging. Previous work demonstrated that large-scale RDMA networks are prone to deadlocking and PFC storms~\cite{pfc_storm, rdma_azure, ib_deadlock, bfc,ddlindc}, degrading the performance. Furthermore, as the scale increases, \fattree architectures become prohibitively costly~\cite{topoopt}. Datacenter providers resort to over-subscription to tame costs, worsening the deadlocking problems. Prior work proposed several techniques to enable large-scale RDMA networks and reduce their cost~\cite{rdma_cc,rdma_azure,srnic,revisitrdmanet}. These approaches fundamentally depend on the assumption that the network is capable of any-to-any communication.
This assumption forms the bedrock upon which datacenters have been conceptualized and developed for several decades. 

In this paper, we challenge this assumption and show that LLM training traffic \textit{does not} require any-to-any connectivity across all GPUs in the network. This paper makes three primary contributions. First, we analyze the traffic pattern of training dense LLMs (\S\ref{sec:traffic_ana}). We demonstrate that with an optimal parallelization strategy, an LLM training workload requires high-bandwidth any-to-any connectivity \textit{only within discrete subsets of GPUs}, and each subset fits within an HB domain. Across the HB domains, communication only occurs between a few GPU pairs with the same rank in their respective HB domains, and the traffic volume is insignificant. As a result, the conventional any-to-any approach for building datacenter networks adds unnecessary complexity and cost for distributed LLM training.

Motivated by the above observations, we propose a low-cost, high-efficiency network architecture that accurately reflects LLM communication requirements that we name ``rail-only" (\S\ref{sec:system-arch}). In our architecture, a cluster is partitioned into multiple HB domains, similar to conventional \fattree architectures. Across the HB domains, however, instead of forming a \fattree to support any-to-any communication, the network only connects sets of GPUs with non-zero network traffic. Compared to the state-of-the-art \fattree design, our network architecture removes the network equipment that does not carry traffic and achieves the same performance as a \fattree network. We also examine our design's fault-tolerance properties and provide recovery methods from failure cases. \looseness=-1

Finally, we derive an analytical formula for accurately estimating the training iteration time of LLMs (\S\ref{sec:sys-model}). This formulation provides insights into the training performance of our network design with different LLM parallelization strategies. Unlike previous approaches~\cite{narayanan2021efficient}, our formulation explicitly considers both the computation and communication time, given the LLM hyperparameters, the parallelization strategy, and the training infrastructure. We compare our formulation to published results to validate its accuracy and show that for LLMs with over one trillion parameters, our formulation estimates the training iteration time within 0.15\% of the ground truth in hardware FLOP utilization (\S\ref{sec:accuracy_model}). 
\looseness=-1

We evaluate the performance of a rail-only network architecture using our analytical formulation and provide insights into the performance impact of different network and training parameters. Our evaluation indicates that an HB domain of size 256 provides near-optimal performance, within 8.9\% of the optimal training iteration time compared to the ideal case where all GPUs reside in a monolithic HB domain. We also show that surprisingly small LLMs exhibit more network overhead than larger ones and demand more network optimization. We discuss the reasons behind this counter-intuitive result (\S\ref{subsec:hbd-size}). 
In addition, we compare the cost of our proposal (\S\ref{sec:cost}) to a full-bisection bandwidth any-to-any \fattree cluster that achieves the same performance and show that our LLM-centric network architecture reduces the network cost by 37\% to 75\%. \looseness=-1
