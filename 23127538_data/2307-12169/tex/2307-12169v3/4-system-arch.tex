\vspace*{-1mm}
\section{Rail-only Network Design}\label{sec:system-arch}
Based on the observations above, we propose a network architecture that diverts from the any-to-any paradigm across all GPUs. We introduce the architecture and discuss its fault-tolerant property. 
Figure~\ref{fig:rail-only} illustrates our network architecture, which we name \textit{rail-only}. Compared to a conventional rail-optimized GPU cluster, shown in Figure~\ref{fig:rail-optimized}, our network keeps the HB domains. It provides network connectivity \textit{only within the same rail}, without changing the bandwidth. \looseness=-1 % (HB domain)} and uses a \todo{point-to-point} connectivity across rails.
% Figure environment removed 
A straightforward way to realize our proposed architecture is to remove the spine switches from Figure~\ref{fig:rail-optimized} and re-purpose all the uplinks connecting rail switches to the spine as down-links to GPUs. Hence, a dedicated but separate \fattree network connects each rail. In the rest of this paper, we base our analysis on this realization of the rail-only network. %, though other technologies are also suitable for other rail interconnections (\S\ref{sec:discussions}).

Our rail-only network architecture removes network connectivity across GPUs with different ranks in different rails. However, such communication is still possible by forwarding the data through HB domains. For instance, a message from \texttt{GPU 1, Domain 1} to \texttt{GPU 2, Domain 2} can first route through the first HB domain to \texttt{GPU 2, Domain 1} and then be transmitted to the final destination through the network. Although our analysis shows that LLM traffic does not require such forwarding, this connectivity might be needed for control messages, measurement, or training other DNN models in this cluster. We provide more discussions on handling other DNN models in Section~\ref{sec:discussions}. \looseness=-1

\subsection{Fault Tolerant Properties of Rail-only Network}
Fault tolerances are crucial for large GPU clusters with long-lasting LLM training jobs. This section investigates the fault tolerance properties of the rail-only network design compared to the traditional rail-optimized network.

\para{Link and switch failures.} Suppose a rail switch or a link fails. GPUs connected to the failed switch or link will become unavailable for either network architecture, rendering the two topologies identical regarding fault tolerance on rail switches and links. However, our design requires fewer switches, which naturally reduces the points of failure. Datacenter operators can add redundant capacity by including extra rail switches, and our design remains more cost-effective than the any-to-any network design. %This principle remains for the switches within an HB domain (e.g., NVSwitches). 

\para{Server failure.} For a GPU cluster composed of DGX-like servers, each server forms its own HB domain. When a server fails, the network operator migrates the task to another healthy server. The rail-only connectivity remains the same for the new server. For a GH200-like cluster, a server only contains a single GPU; thus, the failure mode is the same as a single GPU failure, which we will discuss next.  \looseness=-1

\para{Single GPU failures with idle GPU in the HB domain.} We discuss two distinct scenarios separately for single GPU failures. The first case is when another idle GPU presents the same HB domain as the failed one. In this case, a rail-optimized \fattree network directly replaces the failed GPU with the idle one without breaking the HB domain integrity. We propose leveraging optical reconfigurable switches for the rail-only network to improve robustness. We add a small number of optical switches between the GPU and rail switches to allow reconfiguring rails dynamically. When a GPU fails, the optical switch reconfigures to bring a healthy, idle GPU to replace the failed one. This approach is conceptually similar to the failure recovery mechanism of network designs that uses optical-reconfigurable switches~\cite{jouppi2023tpu, topoopt, jupiterevolving}.

\para{Single GPU failure in fully occupied HB domains.} Another failure mode occurs when a GPU fails in a fully occupied HB domain and requires a substitute GPU from different HB domains. This failure mode is challenging for the rail-optimized \fattree network and rail-only networks. In this case, the rail-only design prevents the straightforward solution of migrating the task to another idle GPU in the cluster, which is possible in a \fattree network. However, this solution is undesirable since the new GPU no longer belongs to the same HB domain as the failed one, creating a bottleneck that slows the HB domain into a network domain. Instead, we propose two solutions. For smaller HB domain designs (e.g., $K=8$ with DGX servers), it is possible to directly migrate the entire HB domain with the failed GPU to a new one without incurring too much resource inefficiency. For larger HB domains (e.g., GH200 supercomputers with $K=256$), these HB domains are composed of a multi-tiered topology with an optical core-layer~\cite{gh200}. Therefore, we use the same solution as the previous case: adding optical reconfigurable switches within HB domains. When a GPU failure occurs, the optical switch reconfigures, replacing a small set of GPUs (including the failed one) with healthy ones, thus maintaining the integrity of an HB domain.  \looseness=-1

