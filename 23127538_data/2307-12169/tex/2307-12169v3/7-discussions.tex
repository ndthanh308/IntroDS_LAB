\vspace*{-1mm}
\section{Related Work} \label{sec:discussions}

\para{LLM trend.}  
The current growth rate of LLM computational and speed requirement outpaces the advancements in AI accelerators and network speed as Moore's law slows down, necessitating hyper-scale clusters 
and more efficient interconnects~\cite{sirius, dnnmodelgrowth}. The MegatornLM line of work pioneers LLM parallelization~\cite{shoeybi2020megatronlm,narayanan2021efficient,korthikanti2022reducing}. Our position to remove any-to-any network connectivity complements MegatronLM.
We also acknowledge ongoing efforts to reduce language models' size and resource requirements without compromising performance~\cite{dolly}. These works complement ours as our design reduces network resources and maintains performance even for smaller language models and clusters. Similarly, research directions that aim to directly reduce the amount of communication through quantization and compression, like DeepSpeed Zero++, also complement our approach~\cite{deepspeedzeropp}.
Another trend deviates from the dense LLM model through the Mixture-of-Expert (MoE) method~\cite{rajbhandari2022deepspeedmoe, artetxe2022efficient, fedus2022switch}, which potentially induces All-to-All traffic in the network domain. In this case, LLM designers can use host-based forwarding or an algorithm like the hierarchical All-to-All presented in DeepSpeed MoE~\cite{rajbhandari2022deepspeedmoe}, achieving a worst-case communication performance penalty of factor ${C_F}/{C_S}$ (Appendix~\ref{appendix:a2aslowdown}) compared to a full-bisection rail-optimized network. \looseness=-1 

\para{LLM Inference.} This paper explores the training workload of LLMs, yet inference represents another significant part of the LLM product cycle. Inference demands fewer resources as it involves moving less data through the LLM and only computes the forward pass and multiple passes to generate response tokens~\cite{alpaserve}. Pope et al. developed specific parallelism for inference on TPU-v4 architecture~\cite{pope2022efficiently}. For our design, each HB domain becomes an inference-serving domain with low latency, and the rail-only connections help load-balance multiple inference domains. We leave a detailed investigation of LLM inference to future work. \looseness=-1 

\para{Multi-job training.} It is common for a GPU cluster to train multiple smaller jobs simultaneously. Existing works focus on \fattree-based GPU clusters and provide techniques for better fairness and shorter job-completion time~\cite{gandiva,tiresias,muri,rajasekaran2023cassini}. While this paper focuses on training a single LLM on a large cluster, the rail-only network design is also suitable for a multi-job setting. The entire cluster can be arbitrarily partitioned by tiling into smaller rectangular partitions, similar to the case of TPU-v4~\cite{jouppi2023tpu}. Each partition then independently executes a smaller training job.  \looseness=-1 

\para{ML infrastructures and other ML workloads.} Prior works illustrated the benefits of co-designing hardware and software for ML models. For instance, Google's TPU cluster is optimized for training large models with 3D parallelism on TPUs~\cite{jouppi2023tpu}, while Meta's Neo focuses on training recommendation models with large embedding tables~\cite{mudigere2023softwarehardware}. To the best of our knowledge, our work is the first to focus on designing a cost-efficient network to train LLMs efficiently. Although our proposed rail-only architecture focuses on network design specifically for LLMs, our design is efficient for many other DNN workloads when combined with other efforts. Recent works attempt to make the parallelization strategy and collective communication algorithms bandwidth-aware for any DNN model~\cite{alpa, unity, zhao2023bandwidth}, which already produce traffic patterns resembling that of LLMs. The cluster uses forwarding described in Section~\ref{sec:system-arch} for parallelization strategies requiring communication across the rails. \looseness=-1  


