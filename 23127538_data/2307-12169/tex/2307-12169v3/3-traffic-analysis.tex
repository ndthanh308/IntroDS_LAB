\vspace*{-3mm}
\section{LLM Traffic Pattern Analysis}
\label{sec:traffic_ana}

\subsection{Traffic Pattern of MegatronLM}
We now analyze the traffic pattern generated by LLMs with PTD-P parallelism by computing the network transfers from the model hyperparameters and the parallelization strategy. We first look at the 145.6 billion, the 310.1 billion, the 539.6 billion, and the 1 trillion parameter model described in Table~1 of MegatronLM~\cite{narayanan2021efficient}, distributed in a cluster composed of DGX A100 servers with an HB domain of size eight. Our analysis uses the same parallelization strategy from MegatronLM to ensure optimal GPU utilization. We use the ring-based collective communication since it is bandwidth-optimal and the default algorithm in NCCL, the communication library backend of MegatronLM. \looseness=-1

Figure~\ref{fig:traffic_dist}a illustrates the volume percentage for each type of traffic for one training iteration, and Figure~\ref{fig:traffic_dist}b shows the traffic type distribution across GPU pairs. There are three primary types of communication: AllGather and ReduceScatter traffic from TP, \allreduce traffic from DP, and point-to-point traffic from PP. The TP traffic happens within GPUs participating in a TP rank, which occupies an HB domain. The DP and PP traffic happen in the network domain, and their volume is significantly lesser than TP traffic, as illustrated by Figure~\ref{fig:traffic_dist}a. While these types of traffic do not overlap between different pairs of GPUs, Figure~\ref{fig:traffic_dist}b indicates that over 99\% of GPU pairs carry \textit{no traffic} and less than 0.04\% of GPU pairs carry TP traffic. Simultaneously, Figure~\ref{fig:traffic_dist}a suggests these traffic types account for over 75\% of the total transmitted data. Recall that TP traffic stays within HB domains, suggesting efficient usage of HB domain bandwidth and low demand in the network domain. This pattern is consistent across all LLM models, indicating that building a cluster with any-to-any connectivity on top of HB domains for LLM models is excessive. 

% Figure environment removed

% Figure environment removed


\subsection{Traffic in the Network Domain}

The parallelization strategy employed in MegatronLM induced an insignificant amount of network traffic across the HB domains compared to within them. Figure~\ref{fig:heatmap} shows the traffic heatmap for training the GPT-1T model. In this plot, every consecutive set of eight GPUs resides within the same HB domain (highlighted in orange), and GPUs with a distance of 8 between them belong to the same rail (highlighted in red). Figures~\ref{fig:heatmap}a demonstrates the traffic pattern within one pipeline stage, while Figures~\ref{fig:heatmap}b shows the traffic across the first four pipeline stages. The traffic volume is significant ($O$(100~GB) across GPU pairs) in an HB domain, while the communication drops to only about 6~GB across them. Furthermore, the communications across HB domains never traverse through the spine switches -- these network transfers only happen within a rail. 

We argue that \textit{all} LLM distributed with an optimal PTD-P parallelization strategy always induces sparse, low-volume traffic across HB domains \textit{within the rails}. By design, the only traffic exiting the HB domains is point-to-point traffic from pipeline parallelism or collective communication traffic (AllGather, ReduceScatter, and AllReduce) from TP and DP when DP and TP dimensions traverse beyond one HB domain. Due to the symmetry of LLM parallelization, each pipeline stage contains the same number of GPUs. As a result, the pipeline stages can always be placed such that traffic across stages always traverses on GPUs of the same rank across HB domains, hence staying within the same rail. 

On the other hand, for some parallelization strategies, TP and DP can induce collective communication traffic across HB domains. For example, training a model in pure DP causes all GPUs to participate in the same DP rank and, thus, the same collective communication operation. The cluster uses hierarchical collective communication algorithms that achieve near-optimal performance in these cases.

Hierarchical collective communication algorithms are designed for a multi-tiered network topology. We introduce the method for the AllGather collective and note that ReduceScatter achieves the same performance by inverting the schedule of AllGather, and AllReduce is equivalent to a ReduceScatter followed by an AllGather. We focus on the bandwidth analysis and ignore the latency in this analysis, as the data transmission is significant during LLM training; thus, the communication runtime is bandwidth-dominated. Logically, we arrange the GPUs conducting an AllGather operation into an $x\times y$ grid, where each $x$ GPU belongs to the same HB domain and across $y$ total HB domains. The basic hierarchical AllGather finishes the operation in two phases: first, the algorithm collects partial data for each rail of GPUs without transferring data in the HB domain. If the total data to run AllGather is of size $D$, then the amount of data exchanged in the network by all GPUs is ${D(y-1)}/{x}$. This operation effectively creates larger data shards for the HB domains to rerun AllGather within each HB domain. Therefore, each HB domain conducts an AllGather in the second phase, inducing a total transfer of $D(x-1)$. Assume the $x$ GPUs within an HB domain have bandwidth capacity $C_F$ and $y$ GPUs in the network domain have bandwidth $C_S$, then the total runtime is 
\begin{equation}\label{eq:agtime}
    \mathtt{AGtime}(D, x, y, C_F, C_S)=\frac{(y-1)D}{xyC_S}+\frac{(x-1)D}{xC_F}
\end{equation} 
Like PP communication, by appropriately mapping the logical $x\times y$ GPUs to the cluster, this algorithm only induces traffic for GPUs within the same rail. Furthermore, based on a recent result on bandwidth-optimal AllGather algorithms, we argue that as HB domain size increases, having full-bisection bandwidth in the network domain does not improve performance compared to only using connections within a rail. We defer the details of this argument to the Appendix~\ref{appendix:allgather}. \looseness=-1

As an example with hierarchical collective communication, we now compute and analyze the traffic pattern of training the GPT-1T model, with a batch size 4096, distributed in a cluster composed of 16 Nvidia GH200 supercomputers~\cite{gh200} (4096 GPUs). Each GH200 supercomputer comprises a two-tier NVSwitch architecture, facilitating  1.8~Pbps of full-bisection bandwidth (7.2~Tbps per GPU) across 256 H100 GPUs. Additionally, each GPU has a Connect-X7 HCA Infiniband network interface~\cite{gh200}, which provides 400~Gbps network bandwidth in/out of each GPU. In this setup, each GH200 supercomputer forms an HB domain. Figure~\ref{fig:traffic_dist_gh200} illustrates the traffic volume percentage and heatmap in this setting. The parallelization strategy has a total data parallel degree of 64, which spans 32 GPUs in each HB domain and two HB domains across the network. Figure~\ref{fig:traffic_dist}b and~\ref{fig:traffic_dist}c illustrate the traffic heatmap of the hierarchical AllReduce algorithm, which splits the AllReduce traffic among each DP group. Note that the network traffic stays within a rail (GPUs with a distance of 256 apart). The hierarchical algorithm efficiently utilized the bandwidth in the HB domain to carry $98\%$ of the AllReduce traffic, as suggested by Figure~\ref{fig:traffic_dist}a.
