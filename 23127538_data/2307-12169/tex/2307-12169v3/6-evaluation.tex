\vspace*{-1mm}
\section{Evaluation} \label{sec:evaluation}

\subsection{Accuracy of the Iteration Time Modeling} \label{sec:accuracy_model}
We evaluate the accuracy of our iteration time model from Section~\ref{sec:sys-model} by comparing the computed hardware FLOPs utilization (HFU) against testbed results from previous work. The HFU refers to the hardware's floating point operation performed in an iteration over the peak floating point operations. The paper from Korthikanti et al.~\cite{korthikanti2022reducing} provides the full set of hyperparameters in their evaluation setup (replicated in Appendix~\ref{appendix:modeldetails}) and allows us to compare the estimated HFU to the ground truth directly. The infrastructure consists of DGX A100 servers. We follow the formula presented by Korthikanti et al.~\cite{korthikanti2022reducing} to compute the total FLOP of training with selective activation recomputation. \looseness=-1

% Figure environment removed

The discrepancy between the mathematical model and the ground truth comes from the idealistic modeling of GPU computation and communication, the assumptions on how computation and communication overlap, and ignoring the overlead of memory operations. Figure~\ref{fig:hfu} illustrates the comparison result for different GPT models and cluster scales. As the LLM size increases, our formulation's accuracy increases, and for GPT-1T, our computed HFU only differs from the ground truth by $0.15\%$. This reduction is because a larger LLM utilizes the computation resource more efficiently, resulting in a more accurate estimation of the computation time for each micro-batch. The worst-case estimation comes from the smallest GPT-22B model, with a difference of $15.7 \%$ in computed and ground truth MFU. We note that discrete event simulations are not scalable at the cluster size of our evaluation. On the other hand, a course-grain flow simulation based on the mathematical modeling of the system will produce a similar result as Eq.~\ref{eq:itertime} does. Therefore, the rest of this section utilizes the mathematical model to evaluate the training iteration time.

\vspace*{-1mm}
\subsection{What is the Ideal Size of an HB Domain?} \label{subsec:hbd-size}

Increasing HB domain size reduces the network overhead of a training iteration. The question that naturally follows the rail-only network design process is, \textit{what should be the ideal size of the HB domain?} In Figure~\ref{fig:iter_time_dom_sz}, we vary the HB domain size ($K$) and plot the training iteration time for GPT-1T and GPT-146B from MegatronLM for clusters of 16384, 32768, and 65536 H100 GPUs. The global batch size in this evaluation is 4096 and 1024, respectively. We use the optimal parallelization strategy found with our formulation for each cluster size, using the bandwidth and computational ability parameters of GH200. We also compute the ideal-case performance of the "Any-to-Any NVLink" training iteration time. This design point represents the idealized scenario where a full-bisection NVLink fabric connects every GPU or the case where $K=N$, where $N$ is the total GPU count. \looseness=-1

As depicted in Figure~\ref{fig:iter_time_dom_sz}, the iteration time goes down as the HB domain size increases, indicating that larger HB domains reduce the network overhead of training. However, the \textit{performance gain} decreases as the HB domain size increases. For the larger GPT-1T model, increasing HB domain size from one (i.e., all GPUs connected with only the network domain) to eight sees an average gain of $36.5\%$. In contrast, an increment from 8 to 256 realizes a gain of $16.3\%$. This reduction in communication time gain can be attributed to Amdahl's law, as the computation time of the DNN remains constant across all instances. \looseness=-1

% Figure environment removed

For the smaller GPT-146B model, the performance gain of increasing HB domain size is higher than that of GPT-1T. Providing an HB domain of size eight realizes a $50.6\%$ performance gain compared to the HB domain of size one, while increasing the HB domain size from 8 to 256 further achieves a $39.1\%$ gain. We reveal that smaller LLMs incur more communication overhead when distributed to the same cluster than larger models. This effect arises from how computation and communication costs scale as LLM grows. As the analysis in Section~\ref{sec:sys-model} suggests, the communication requirement increases linearly with the model's hidden size and sequence length. On the other hand, the model FLOPs increase quadratically with these two parameters, as indicated by previous work~\cite{narayanan2021efficient}. 

However, in both GPT models, the performance achieved with an HB domain size of 256 is nearly optimal. Compared to the ideal case, GPT-146B with an HB domain of 256 is $8.9\%$ slower, while GPT-1T is $1.3\%$ slower in this case. We argue that the current GH200 supercomputer, with an HB domain of size 256, is well-suited to the demands of LLM training today, especially as LLMs get larger. At the same time, prospective technological advancements augmenting this size will further benefit the training performance, especially for smaller models, reducing the training iteration time closer to the ideal case without requiring any-to-any connectivity in the network across HB domains. \looseness=-1

\subsection{Impact of HB Domain and Network Bandwidth}
% Figure environment removed 

The bandwidth of HB and network domains fundamentally determines the communication time. We analyze the impact of these bandwidths on LLM's iteration time. Figure~\ref{fig:iter_time_bw}a and~\ref{fig:iter_time_bw}b show the iteration time variation for different HB domain bandwidths (different lines) and network bandwidths in the rails (on the $x$-axis), for $K=8$ and 256, respectively. As expected, the iteration time drops when either bandwidth increases. However, the $K=8$ case is less sensitive to the HB domain bandwidth. Increasing the per-GPU bandwidth by a factor of four (from 2.4~Tbps to 9.6~Tbps) only improves the iteration time by $8.0\%$ on average for $K=8$, compared to the improvement of $13.3\%$ for $K=256$. On the other hand, larger HB domain sizes are less sensitive to network bandwidth improvement. Increasing the bandwidth from 100~Gbps to 400~Gbps, also a factor of four, results in $35.9\%$ improvement for $K=8$ but only $8.0\%$ for $K=256$. While increasing either bandwidth consists of separate efforts on the hardware, as HB domain size goes up, improving the HB domain bandwidth is more beneficial than the network bandwidth for LLMs as future HB domain size increases. \looseness=-1


\subsection{Impact of Batch Size on Network Design} \label{subsec:batch-size}
% Figure environment removed

While the batch size is typically an ML-centric metric for optimization, our analysis indicates that the impact of batch size on the comprehensive training performance goes beyond the total number of iterations required for convergence. To further understand the impact of batch size on training time, we analyze the performance of a GPT-1T model on a 32768 GPU cluster while changing the HB domain size from $K=8$ to $32768$. We vary the global batch size from 256 to 4096 in this study. Figure~\ref{fig:iter_time_palm}a plots the change in iteration time as the batch size varies. The iteration time exhibits a similar trajectory for all HB domain sizes; however, the \textit{relative performance} (the ratio to the iteration time for an HB domain size to that of the ideal case) improves as the batch size increases. Figure~\ref{fig:iter_time_palm}b represents this trend. When $K=256$, the relative performance increases from $93\%$ to $99\%$ as the batch size increases from 256 to 4096 sequences. This result also supports that $K=256$ provides a near-optimal performance. This effect is prominent when the HB domain size is small. For $K=8$, increasing the batch size from 256 to 4096 improves the relative performance from $58\%$ to $82\%$, suggesting a larger batch size is preferable for a cluster with a smaller HB domain. Prior studies have shown that LLM training benefits from a larger batch size~\cite{kaplan2020scaling, gpt3}, especially for bigger models, making it a perfect fit for our rail-only design. \looseness=-1



\subsection{Network Cost Analysis} \label{sec:cost}


Our rail-only network architecture judiciously reduces the network resources for LLM training by eliminating unused network connections. This section compares the network cost of our proposed approach with the state-of-the-art rail-optimized GPU clusters. We calculate the number of switches (\#SW) and transceivers (\#TR) required for each network design and derive the network equipment cost based on numbers reported in prior work~\cite{topoopt}\footnote{\$374 per transceiver, \$748 per switch port for 400~Gbps.}.
We enumerate the number of switches and transceivers required to build the state-of-the-art network architecture and our proposed architecture in Table~\ref{tab:nw_swich}, accounting for variable cluster sizes and network switch radix. Note that for the state-of-the-art architecture, each rail of GPUs is not physically separated in some cases to use the least amount of network resources. Thus, the datacenter operator must manually configure the switch to achieve the desired isolation across rails to achieve the rail-optimized design. 

The last column of Table~\ref{tab:nw_swich} illustrates our design's cost savings over the state-of-the-art for the total cost of switches and transceivers. Our rail-only design notably reduces the network cost by 37\% to 75\% compared to the state-of-the-art design while achieving equivalent performance. This reduction stems from eliminating core layer switches and decreasing the number of switch tiers within each rail. Surprisingly, switches with a radix of 64 provide the worst-case cost reduction in both cluster sizes. In this case, the state-of-the-art design requires a three-tier \fattree network, while the rail-only design requires two tiers for each rail. Still, our design only requires three-quarters of the total number of switches while achieving the same performance as the state-of-the-art design. 

\begin{table}[t]
\scriptsize
\centering
\vspace*{-1.5mm}
\caption{Number of switches for different clusters.}
\renewcommand{\arraystretch}{1} 
\linespread{1.05}\selectfont\centering
    \begin{tabular}{|P{0.7cm}|P{0.7cm}|P{0.7cm}|P{0.8cm}|P{0.8cm}|P{0.8cm}|P{0.8cm}|}
    \hline
    \textbf{\#GPUs (N)} & \textbf{Switch Radix} & \textbf{SOTA \#SW} & \textbf{Rail-only \#SW} & \textbf{SOTA \#TR} & \textbf{Rail-only \#TR} & \textbf{Cost Reduction} \\ 
    \hline
        \multirow{3}{*}{32768}   %& \multicolumn{1}{c|}{32} & \multicolumn{1}{c|}{7168} & \multicolumn{1}{c|}{3072} & \multicolumn{1}{c|}{262144} & \multicolumn{1}{c|}{131072} & \multicolumn{1}{c|}{54\%} \\\cline{2-7}
                                 & \multicolumn{1}{c|}{64} & \multicolumn{1}{c|}{2560} & \multicolumn{1}{c|}{1536} & \multicolumn{1}{c|}{196608} & \multicolumn{1}{c|}{131072} & \multicolumn{1}{c|}{37\%} \\\cline{2-7}
                                 & \multicolumn{1}{c|}{128} & \multicolumn{1}{c|}{1280} & \multicolumn{1}{c|}{256} & \multicolumn{1}{c|}{196608} & \multicolumn{1}{c|}{65536} & \multicolumn{1}{c|}{75\%} \\\cline{2-7}
                                 & \multicolumn{1}{c|}{256} & \multicolumn{1}{c|}{384} & \multicolumn{1}{c|}{128} & \multicolumn{1}{c|}{131072} & \multicolumn{1}{c|}{65536} & \multicolumn{1}{c|}{60\%} \\\hline
        \multirow{3}{*}{65536}   & \multicolumn{1}{c|}{64} & \multicolumn{1}{c|}{5120} & \multicolumn{1}{c|}{3072} & \multicolumn{1}{c|}{393216} & \multicolumn{1}{c|}{262144} & \multicolumn{1}{c|}{37\%} \\\cline{2-7}
                                 & \multicolumn{1}{c|}{128} & \multicolumn{1}{c|}{2560} & \multicolumn{1}{c|}{1536} & \multicolumn{1}{c|}{393216} & \multicolumn{1}{c|}{262144} & \multicolumn{1}{c|}{37\%} \\\cline{2-7}
                                 & \multicolumn{1}{c|}{256} & \multicolumn{1}{c|}{1280} & \multicolumn{1}{c|}{256} & \multicolumn{1}{c|}{393216} & \multicolumn{1}{c|}{131072} & \multicolumn{1}{c|}{75\%} \\\hline
    \end{tabular}
    \vspace*{-5mm}
    \label{tab:nw_swich}
\end{table}