\newpage 
\setlength{\abovedisplayskip}{4pt}
\setlength{\belowdisplayskip}{4pt}
\section{Bandwidth Time of AllGather Algorithm for Rail-Optimized and Rail-Only Network} \label{appendix:allgather}
Zhao et al.~\cite{zhao2023bandwidth} proved that a bandwidth-optimal AllGather schedule exists for arbitrary network topology. This section examines the best-case AllGather time for a grid of $x\times y$ GPUs in the rail-optimized and rail-only network. Following Eq.~1 in Zhao et al.~\cite{zhao2023bandwidth}, we derive an analytical expression of the bandwidth AllGather time for these two types of networks. \looseness=-1

Let $\mathbb{M}_{x y}$ be the space of all boolean matrices of size $x\times y$ except all ones and all zeros. A matrix $A\in\mathbb{M}_{x y}$ represents a specific partition of the GPUs in an $x\times y$ configuration. Then, the optimal AllGather time for one unit of data in a rail-optimized network is

\begin{align*}
\scriptsize
\max\limits_{A\in\mathbb{M}_{x y}}\frac{\max\limits_{A'\in\{A, \Bar{A}\}}\sum\limits_{i,j} A'_{ij}}{\min\limits_{A''\in\{A, \Bar{A}\}}((C_F + C_S)\sum\limits_{i,j} A''_{ij}-xR(A'')C_F)} \numberthis\label{eq:ag_ropt}
\end{align*}

Here, $\bar{A}$ denotes the negation of the boolean matrix $A$. The numerator finds the partition that sends the largest data, which equals the sum of the binary entries of $A$ or $\bar{A}$. The denominator finds the partition with the lowest ingress and egress bandwidth. For each GPU included in the partition, the total bandwidth of the partition increases by $C_F+C_S$, hence the first term in the minimization. However, whenever the partition includes an entire row of GPUs (i.e., an HB domain), the bandwidth internal to this HB domain no longer contributes to the ingress or egress bandwidth. The function $R(A)$ counts the number of rows with all ones as $A$'s entries, implying one HB domain entirely inside the partition. The second term in the minimization removes this part of the bandwidth from the egress bandwidth. 

For the rail-only network, going through the same analysis, we get the AllGather time of

{\scriptsize
\begin{align*}
\max\limits_{A\in\mathbb{M}_{x y}}\frac{\max\limits_{A'\in\{A, \Bar{A}\}}\sum\limits_{i,j} A'_{ij}}{\min\limits_{A''\in\{A, \Bar{A}\}}((C_F + C_S)\sum\limits_{i,j} A''_{ij}-xR(A'')C_F-yC(A'')C_S)} \numberthis\label{eq:ag_ronly}
\end{align*}
}

The formula remains the same as Eq.~\ref{eq:ag_ropt}, except for the last term in the denominator. This term accounts for the fact that whenever an entire rail is included in the partition, this rail no longer contributes its bandwidth as the ingress or egress bandwidth of the partition. Here, the $C(A)$ function counts the number of columns with all ones as their entries, hence the number of entire rails inside the partition. 

Intuitively, to maximize either expression, the choice of $A$ should be skewed (i.e., having a large portion of GPU in one partition) so that $\sum\limits_{i,j} A'_{ij}$ on the numerator is large but $\sum\limits_{i,j} A''_{ij}$ on the denominator is small. In addition, the GPU choice should be arranged such that the denominator can exclude as much bandwidth from the partition as possible. For Eq.~\ref{eq:ag_ropt} and Eq.~\ref{eq:ag_ronly}, one such configuration is obtained when the partition has $y-1$ HB domains. In this case. $R(A'')=1$ and $C(A'')=0$ which yield an AllGather time of ${(y-1)}/{C_S}$ per unit of data for both networks. We postulate that for $C_F >> C_S$ and $x \geq y$, this choice of partition yields the lower bound (thus bandwidth optimal) AllGather time for both of this network, as perturbing the partition by adding or removing single GPUs or entire HB domains only relaxes the bound. We leave concrete proof of the optimality in future work. \looseness=-1

\section{Estimation of Microbatch Compute Time}\label{appendix:mb_est}

While LLM designers can use profiling to obtain micro-batch computation times accurately, it is also possible to analytically estimate the execution time, especially when the hardware is unavailable. Table~\ref{tab:extra_para} shows the extra parameters used in this calculation and other sections in the appendix, in addition to Table~\ref{tab:parameters}.

\begin{table}[t]
\scriptsize
\centering
\caption{Extra parameters utilized in the appendix.}
\newcommand{\centered}[1]{\begin{tabular}{l} #1 \end{tabular}}

\renewcommand{\arraystretch}{0.92} 
\linespread{1.05}\selectfont\centering
    \begin{tabular}{|p{1cm}|p{6.5cm}|}
    \hline
    \textbf{Name} & \textbf{Description} \\ \hline
    $N$              & Total number of GPUs in the cluster\\ \hline
    $M_{FF}$         & Amount of feed-forward FLOPs required for an iteration  \\ \hline
    $M_{Attn}$       & Amount of attention block FLOPs required for an iteration \\ \hline
    $F$              & GPU Compute Speed (FLOPs)\\ \hline
    $B$              & Global batch size \\ \hline 
    $R$              & GPU memory size   \\ \hline
    % $\alpha$         & Network latency  \\ \hline
    % \hline
    % $T_c$            & Compute time of a transformer block \\ \hline
    % $T_{d}$          & Exposed communication time for data parallelism \\ \hline
    % $T_{t}$          & Communication time for tensor parallelism \\ \hline    
    % $T_{p}$          & Communication time for tensor parallelism \\ \hline 
    \end{tabular}
    \label{tab:extra_para}
\end{table}

Most FLOP for LLMs comes from the attention mechanism and general matrix multiplication (GEMM) in the feed-forward layers. Prior work reports that while GPUs operate at peak speed for GEMM in feed-forward layers, they are at most $40\%$ efficient for attention blocks without architecture-specific optimization~\cite{dao2023flashattention2}. Therefore, we model the computation time of a micro-batch accordingly, as
\begin{equation}\label{eq:computetimemb}
T_{\mu b}^{comp} = \frac{(M_{FF} + \gamma M_{Attn})b}{FB p t} \approx t(b)
\end{equation}
where $\gamma$ is a slowdown factor for attention. We use $\gamma=1/0.4$ for Section~\ref{sec:evaluation}. $M_{FF}$ and $M_{Attn}$ depend on the model's hyperparameter and whether activation recomputation is utilized. Our evaluation in Section~\ref{sec:evaluation} assumes the training uses selective activation recomputation presented by Korthikanti et al.~\cite{korthikanti2022reducing}, and the FLOPs estimation follows from Eq.~8 in the same paper. 

\section{Constraints of Parallelization Configuration} \label{appendix:constraints}
\begin{table*}[t]
\scriptsize
\centering
\caption{Model hyperparameters and iteration time comparison.}
\renewcommand{\arraystretch}{1} 
\linespread{1.05}\selectfont\centering
    \begin{tabular}{|P{0.6cm}|P{0.8cm}|P{0.6cm}|P{0.6cm}|P{0.6cm}|P{0.6cm}|P{0.6cm}|P{0.6cm}|P{0.6cm}|P{1.2cm}|P{2cm}|P{1.5cm}|}
    \hline
    {Model Size} & {Attention Heads} & {Hidden Size} & {Layers} & {TP Size} & {PP Size} & {Number of GPUs} & {Global Batch Size} & {Micro-batch Size} & {Number of Interleaved Stages} & {Measured Iteration Time in~\cite{korthikanti2022reducing}} & {Computed Iteration Time from Section~\ref{sec:sys-model}}  \\ \hline
    22B & 64 & 6144 & 48 & 8 & 1 & 8 & 4 & 4 & 1 & 1.10 & 0.78  \\ \hline
    175B & 96 & 12288 & 96 & 8 & 8 & 64 & 64 & 1 & 3 & 13.75 & 11.89  \\ \hline
    530B & 128 & 20480 & 105 & 8 & 35 & 280 & 280 & 1 & 3 & 37.83 & 35.29  \\ \hline
    530B & 128 & 20480 & 105 & 8 & 35 & 2240 & 2240 & 1 & 3 & 39.15 & 35.56  \\ \hline
    1T & 160 & 25600 & 128 & 8 & 64 & 512 & 512 & 1 & 1 & 71.49 & 70.69  \\
    \hline
    \end{tabular}
    \label{tab:model_acc_eval}
\end{table*}
Given a cluster with $N$ GPUs and an HB domain size of $K$, the following set of constraints hold for a valid parallelization strategy: \looseness=-1
\begin{align}
    pdt &= N      \label{eq:totalgpu} \\
    d_ht_hp_h &= K   \label{eq:hbd}  \\
    mb &= \frac{B}{d}  \label{eq:mbs} \\
    d_ld_h &= d     \label{eq:dpgpu} \\
    t_lt_h &= t    \label{eq:tpgpu} \\
    p_lp_h &= p    \label{eq:ppgpu} \\
    d_l, d_h, t_l, t_h, p_l, p_h, v, m, b  &\in \mathbb{Z_{++}} \label{eq:param_int} \\
     \frac{l}{pv}, \frac{s}{t}, \frac{h}{t}, \frac{B}{d} &\in \mathbb{Z_{++}} \label{eq:ratio_int}  \\
     \mathtt{MemoryConsumption}(p, t, d) &\leq R \label{eq:gpuram}
\end{align}


In the constraints above, $\mathbb{Z_{++}}$ refers to all positive integers. The first two constraints are guarantees that the parallelization strategy uses all physical GPUs in the cluster. Constraint~\ref{eq:totalgpu} ensures that all GPUs in the cluster participate in the training. Constraint~\ref{eq:hbd} ensures that the total parallelization dimensions map to the HB domain covers the entire domain. 

Then, Constraint~\ref{eq:mbs} establishes the relation of micro-batch size, number of micro-batches, and local batch size.  

Constraint~\ref{eq:dpgpu} to~\ref{eq:ratio_int} ensures that the mapping from the parallelization strategy to the physical cluster is valid. Constraint~\ref{eq:dpgpu} to~\ref{eq:ppgpu} divides each of the parallelization dimension into grids of GPUs that spans HB and network domains. Constraints~\ref{eq:param_int} guarantees that each parallelization dimension, interleaving size, micro-batch size, and micro-batch counts are integers in the final solution. In the end, Constraints~\ref{eq:ratio_int} requires the ratio of a few parameters also to be integers. $\frac{l}{pv}$ is the number of transformer blocks per interleaved scheduling stage. $\frac{s}{t}$ and $\frac{h}{t}$ refers to the number of sequences and hidden dimensions each GPU gets when $t$-way tensor (and sequence) parallelism is applied. Finally, the local batch size is $\frac{B}{d}$.

Finally, Constraint~\ref{eq:gpuram} ensures that LLMs parallelized with the generated parallelization strategy fit in the GPU RAM. We follow Korthikanti et al.~\cite{korthikanti2022reducing} for calculating the memory consumption of the LLM with selective activation recomputation. 

We exhaustively generate all valid configurations for a given GPT model and the network cluster in the evaluation. While the nature of minimizing Eq.~\ref{eq:itertime} subject to all constraints in this section is non-convex, there are a limited number of valid configurations. For instance, the GPT-1T with 128 GH200 computers yielded $O(10000)$ possible configurations, making an exhaustive search possible. 

\section{Rail-only Network Slowdown for All-to-All Traffic}\label{appendix:a2aslowdown}

This section computes the slowdown of All-to-All traffic in a rail-only network compared to a rail-optimized network. We utilize the hierarchical All-to-All algorithm described in DeepSpeed Zero++~\cite{deepspeedzeropp} for the rail-only network, but the result remains the same for a straightforward two-hop forwarding scheme.

Consider a grid of $x\times y$ GPUs where each $x$ lives in an HB domain, and the $y$ HB domains are connected with a slower network domain. For All-to-All traffic in a rail-optimized network, every GPU sends to all other GPUs within an HB domain through the local fast interconnect and sends the rest of the traffic directly to their destinations through the full-bisection network. Assuming each shard to send is of size $D$, the total completion time is:
\begin{align*}
T_{a2a}^{Full\ Bisec}&=\max(\frac{(x-1)D}{C_F}, \frac{x(y-1)D}{C_S}) \\
 &= \frac{x(y-1)D}{C_S} \numberthis
\end{align*}
The hierarchical algorithm runs an All-to-All locally for the rail-only network that prepares each GPU to have all data to send on its rail. In this step, the effective data shard size is $xD$. Then, within each rail, the GPU runs a second All-to-All to finalize the algorithm with an effective shard size of $xD$. The total transmission time is
\begin{equation}
T_{a2a}^{Rail\ Only}=\frac{y(x-1)D}{C_F} + \frac{x(y-1)D}{C_S}
\end{equation}
Note the two terms differ by ${y(x-1)D}/{C_F}$, which is the cost of forwarding All-to-All traffic within HB domains. When $ y(x-1)\approx x(y-1)$, the slow down factor is approximately 
\begin{equation}
    \frac{T_{a2a}^{Rail\ Only}-T_{a2a}^{Full\ Bisec}}{T_{a2a}^{Full\ Bisec}}\approx\frac{C_F}{C_S}
\end{equation}

\newpage 

\section{Details of Model Hyperparameters in Section~\ref{sec:evaluation}}\label{appendix:modeldetails}
Table~\ref{tab:model_acc_eval} presents the detailed parameters for the LLM models evaluated in Section~\ref{sec:evaluation}. We utilize the same batch size, micro-batch size, parallelization strategy, schedule interleaving, and enabled selective activation computation, as in Korthikant et al.~\cite{korthikanti2022reducing}, to obtain the result in Figure~\ref{fig:hfu}. Although Narayanan~\cite{narayanan2021efficient} et al. also present a similar evaluation in MegatronLM, the paper did not report the exact micro-batch size and interleaving count. Therefore, we cannot get an accurate estimation to compare for our formulation. The analysis in Section~\ref{sec:system-arch} is unaffected by the micro-batch size and assumes no interleaving, and adding interleaving does not change the analysis result. 

