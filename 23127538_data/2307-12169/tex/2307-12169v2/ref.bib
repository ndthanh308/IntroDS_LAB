@inproceedings{vanjacobson,
  author = "Van Jacobson and Michael J. Karels",
  title = "Congestion Avoidance and Control",
  booktitle = "SIGCOMM 1988",
  address = "Stanford, CA",
  month = aug,
  year = 1988
}

@inproceedings{sipac,
author = {Zhenguo Wu and Liang Yuan Dai and Ziyi Zhu and Asher Novick and Madeleine Glick and Keren Bergman},
booktitle = {Optical Fiber Communication Conference (OFC) 2023},
journal = {Optical Fiber Communication Conference (OFC) 2023},
keywords = {Erbium doped fiber amplifiers; Frequency combs; Neural networks; Packet switching; Tunable lasers; Variable optical attenuators},
pages = {W1G.1},
publisher = {Optica Publishing Group},
title = {SiP Architecture For Accelerating Collective Communication in Distributed Deep Learning},
year = {2023},
url = {https://opg.optica.org/abstract.cfm?URI=OFC-2023-W1G.1},
doi = {10.1364/OFC.2023.W1G.1},
abstract = {We present a silicon photonic architecture for accelerating collective commu nications in distributed deep learning. We demonstrate a 22\% job completion time improvement in a small-scale testbed and 1.4 to 5.9x improvement in large-scale simulations.},
}

@misc{shoeybi2020megatronlm,
      title={Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism}, 
      author={Mohammad Shoeybi and Mostofa Patwary and Raul Puri and Patrick LeGresley and Jared Casper and Bryan Catanzaro},
      year={2020},
      eprint={1909.08053},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{zhao2022optimal,
      title={Optimal Direct-Connect Topologies for Collective Communications}, 
      author={Liangyu Zhao and Siddharth Pal and Tapan Chugh and Weiyang Wang and Prithwish Basu and Joud Khoury and Arvind Krishnamurthy},
      year={2022},
      eprint={2202.03356},
      archivePrefix={arXiv},
      primaryClass={cs.NI}
}

@misc{zhao2023bandwidth,
      title={Bandwidth Optimal Pipeline Schedule for Collective Communication}, 
      author={Liangyu Zhao and Arvind Krishnamurthy},
      year={2023},
      eprint={2305.18461},
      archivePrefix={arXiv},
      primaryClass={cs.NI}
}

@inproceedings{swhwcodesign,
author = {Mudigere, Dheevatsa and Hao, Yuchen and Huang, Jianyu and Jia, Zhihao and Tulloch, Andrew and Sridharan, Srinivas and Liu, Xing and Ozdal, Mustafa and Nie, Jade and Park, Jongsoo and Luo, Liang and Yang, Jie (Amy) and Gao, Leon and Ivchenko, Dmytro and Basant, Aarti and Hu, Yuxi and Yang, Jiyan and Ardestani, Ehsan K. and Wang, Xiaodong and Komuravelli, Rakesh and Chu, Ching-Hsiang and Yilmaz, Serhat and Li, Huayu and Qian, Jiyuan and Feng, Zhuobo and Ma, Yinbin and Yang, Junjie and Wen, Ellie and Li, Hong and Yang, Lin and Sun, Chonglin and Zhao, Whitney and Melts, Dimitry and Dhulipala, Krishna and Kishore, KR and Graf, Tyler and Eisenman, Assaf and Matam, Kiran Kumar and Gangidi, Adi and Chen, Guoqiang Jerry and Krishnan, Manoj and Nayak, Avinash and Nair, Krishnakumar and Muthiah, Bharath and khorashadi, Mahmoud and Bhattacharya, Pallab and Lapukhov, Petr and Naumov, Maxim and Mathews, Ajit and Qiao, Lin and Smelyanskiy, Mikhail and Jia, Bill and Rao, Vijay},
title = {Software-Hardware Co-Design for Fast and Scalable Training of Deep Learning Recommendation Models},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3533727},
doi = {10.1145/3470496.3533727},
abstract = {Deep learning recommendation models (DLRMs) have been used across many business-critical services at Meta and are the single largest AI application in terms of infrastructure demand in its data-centers. In this paper, we present Neo, a software-hardware co-designed system for high-performance distributed training of large-scale DLRMs. Neo employs a novel 4D parallelism strategy that combines table-wise, row-wise, column-wise, and data parallelism for training massive embedding operators in DLRMs. In addition, Neo enables extremely high-performance and memory-efficient embedding computations using a variety of critical systems optimizations, including hybrid kernel fusion, software-managed caching, and quality-preserving compression. Finally, Neo is paired with ZionEX, a new hardware platform co-designed with Neo's 4D parallelism for optimizing communications for large-scale DLRM training. Our evaluation on 128 GPUs using 16 ZionEX nodes shows that Neo outperforms existing systems by up to 40\texttimes{} for training 12-trillion-parameter DLRM models deployed in production.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {993–1011},
numpages = {19},
location = {New York, New York},
series = {ISCA '22}
}

@misc{dlrm,
      title={Deep Learning Recommendation Model for Personalization and Recommendation Systems}, 
      author={Maxim Naumov and Dheevatsa Mudigere and Hao-Jun Michael Shi and Jianyu Huang and Narayanan Sundaraman and Jongsoo Park and Xiaodong Wang and Udit Gupta and Carole-Jean Wu and Alisson G. Azzolini and Dmytro Dzhulgakov and Andrey Mallevich and Ilia Cherniavskii and Yinghai Lu and Raghuraman Krishnamoorthi and Ansha Yu and Volodymyr Kondratenko and Stephanie Pereira and Xianjie Chen and Wenlin Chen and Vijay Rao and Bill Jia and Liang Xiong and Misha Smelyanskiy},
      year={2019},
      eprint={1906.00091},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}

@inproceedings {alpaserve,
author = {Zhuohan Li and Lianmin Zheng and Yinmin Zhong and Vincent Liu and Ying Sheng and Xin Jin and Yanping Huang and Zhifeng Chen and Hao Zhang and Joseph E. Gonzalez and Ion Stoica},
title = {{AlpaServe}: Statistical Multiplexing with Model Parallelism for Deep Learning Serving},
booktitle = {17th USENIX Symposium on Operating Systems Design and Implementation (OSDI 23)},
year = {2023},
address = {Boston, MA},
url = {https://www.usenix.org/conference/osdi23/presentation/li-zhouhan},
publisher = {USENIX Association},
month = jul,
}

@inproceedings {topoopt,
author = {Weiyang Wang and Moein Khazraee and Zhizhen Zhong and Manya Ghobadi and Zhihao Jia and Dheevatsa Mudigere and Ying Zhang and Anthony Kewitsch},
title = {{TopoOpt}: Co-optimizing Network Topology and Parallelization Strategy for Distributed Training Jobs},
booktitle = {20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23)},
year = {2023},
isbn = {978-1-939133-33-5},
address = {Boston, MA},
pages = {739--767},
url = {https://www.usenix.org/conference/nsdi23/presentation/wang-weiyang},
publisher = {USENIX Association},
month = apr,
}

@misc{korthikanti2022reducing,
      title={Reducing Activation Recomputation in Large Transformer Models}, 
      author={Vijay Korthikanti and Jared Casper and Sangkug Lym and Lawrence McAfee and Michael Andersch and Mohammad Shoeybi and Bryan Catanzaro},
      year={2022},
      eprint={2205.05198},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{grad_comp,
author = {Bai, Youhui and Li, Cheng and Zhou, Quan and Yi, Jun and Gong, Ping and Yan, Feng and Chen, Ruichuan and Xu, Yinlong},
title = {Gradient Compression Supercharged High-Performance Data Parallel DNN Training},
year = {2021},
isbn = {9781450387095},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477132.3483553},
doi = {10.1145/3477132.3483553},
abstract = {Gradient compression is a promising approach to alleviating the communication bottleneck in data parallel deep neural network (DNN) training by significantly reducing the data volume of gradients for synchronization. While gradient compression is being actively adopted by the industry (e.g., Facebook and AWS), our study reveals that there are two critical but often overlooked challenges: 1) inefficient coordination between compression and communication during gradient synchronization incurs substantial overheads, and 2) developing, optimizing, and integrating gradient compression algorithms into DNN systems imposes heavy burdens on DNN practitioners, and ad-hoc compression implementations often yield surprisingly poor system performance.In this paper, we first propose a compression-aware gradient synchronization architecture, CaSync, which relies on a flexible composition of basic computing and communication primitives. It is general and compatible with any gradient compression algorithms and gradient synchronization strategies, and enables high-performance computation-communication pipelining. We further introduce a gradient compression toolkit, CompLL, to enable efficient development and automated integration of on-GPU compression algorithms into DNN systems with little programming burden. Lastly, we build a compression-aware DNN training framework HiPress with CaSync and CompLL. HiPress is open-sourced and runs on mainstream DNN systems such as MXNet, TensorFlow, and PyTorch. Evaluation via a 16-node cluster with 128 NVIDIA V100 GPUs and 100Gbps network shows that HiPress improves the training speed over current compression-enabled systems (e.g., BytePS-onebit and Ring-DGC) by 17.2%-69.5% across six popular DNN models.},
booktitle = {Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles},
pages = {359–375},
numpages = {17},
keywords = {gradient compression, DNN training},
location = {Virtual Event, Germany},
series = {SOSP '21}
}

@article{flex_flow,
  author    = {Zhihao Jia and
               Matei Zaharia and
               Alex Aiken},
  title     = {Beyond Data and Model Parallelism for Deep Neural Networks},
  journal   = {SysML},
  year      = {2019},
  url       = {https://mlsys.org/Conferences/2019/doc/2019/16.pdf},
}

@inproceedings {alpa,
author = {Lianmin Zheng and Zhuohan Li and Hao Zhang and Yonghao Zhuang and Zhifeng Chen and Yanping Huang and Yida Wang and Yuanzhong Xu and Danyang Zhuo and Eric P. Xing and Joseph E. Gonzalez and Ion Stoica},
title = {Alpa: Automating Inter- and {Intra-Operator} Parallelism for Distributed Deep Learning},
booktitle = {16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)},
year = {2022},
isbn = {978-1-939133-28-1},
address = {Carlsbad, CA},
pages = {559--578},
url = {https://www.usenix.org/conference/osdi22/presentation/zheng-lianmin},
publisher = {USENIX Association},
month = jul,
}

@misc{gpt3,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings {tiresias,
author = {Juncheng Gu and Mosharaf Chowdhury and Kang G. Shin and Yibo Zhu and Myeongjae Jeon and Junjie Qian and Hongqiang Liu and Chuanxiong Guo},
title = {Tiresias: A {GPU} Cluster Manager for Distributed Deep Learning},
booktitle = {16th USENIX Symposium on Networked Systems Design and Implementation (NSDI 19)},
year = {2019},
isbn = {978-1-931971-49-2},
address = {Boston, MA},
pages = {485--500},
url = {https://www.usenix.org/conference/nsdi19/presentation/gu},
publisher = {USENIX Association},
month = feb,
}

@inproceedings{muri,
author = {Zhao, Yihao and Liu, Yuanqiang and Peng, Yanghua and Zhu, Yibo and Liu, Xuanzhe and Jin, Xin},
title = {Multi-Resource Interleaving for Deep Learning Training},
year = {2022},
isbn = {9781450394208},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544216.3544224},
doi = {10.1145/3544216.3544224},
abstract = {Training Deep Learning (DL) model requires multiple resource types, including CPUs, GPUs, storage IO, and network IO. Advancements in DL have produced a wide spectrum of models that have diverse usage patterns on different resource types. Existing DL schedulers focus on only GPU allocation, while missing the opportunity of packing jobs along multiple resource types.We present Muri, a multi-resource cluster scheduler for DL workloads. Muri exploits multi-resource interleaving of DL training jobs to achieve high resource utilization and reduce job completion time (JCT). DL jobs have a unique staged, iterative computation pattern. In contrast to multi-resource schedulers for big data workloads that pack jobs in the space dimension, Muri leverages this unique pattern to interleave jobs on the same set of resources in the time dimension. Muri adapts Blossom algorithm to find the perfect grouping plan for single-GPU jobs with two resource types, and generalizes the algorithm to handle multi-GPU jobs with more than two types. We build a prototype of Muri and integrate it with PyTorch. Experiments on a cluster with 64 GPUs demonstrate that Muri improves the average JCT by up to 3.6\texttimes{} and the makespan by up to 1.6\texttimes{} over existing DL schedulers.},
booktitle = {Proceedings of the ACM SIGCOMM 2022 Conference},
pages = {428–440},
numpages = {13},
keywords = {resource sharing, deep learning},
location = {Amsterdam, Netherlands},
series = {SIGCOMM '22}
}

@inproceedings {gandiva,
author = {Wencong Xiao and Romil Bhardwaj and Ramachandran Ramjee and Muthian Sivathanu and Nipun Kwatra and Zhenhua Han and Pratyush Patel and Xuan Peng and Hanyu Zhao and Quanlu Zhang and Fan Yang and Lidong Zhou},
title = {Gandiva: Introspective Cluster Scheduling for Deep Learning},
booktitle = {13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)},
year = {2018},
isbn = {978-1-939133-08-3},
address = {Carlsbad, CA},
pages = {595--610},
url = {https://www.usenix.org/conference/osdi18/presentation/xiao},
publisher = {USENIX Association},
month = oct,
}

@inproceedings {unity,
author = {Colin Unger and Zhihao Jia and Wei Wu and Sina Lin and Mandeep Baines and Carlos Efrain Quintero Narvaez and Vinay Ramakrishnaiah and Nirmal Prajapati and Pat McCormick and Jamaludin Mohd-Yusof and Xi Luo and Dheevatsa Mudigere and Jongsoo Park and Misha Smelyanskiy and Alex Aiken},
title = {Unity: Accelerating {DNN} Training Through Joint Optimization of Algebraic Transformations and Parallelization},
booktitle = {16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)},
year = {2022},
isbn = {978-1-939133-28-1},
address = {Carlsbad, CA},
pages = {267--284},
url = {https://www.usenix.org/conference/osdi22/presentation/unger},
publisher = {USENIX Association},
month = jul,
}

@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{aibusinessgptuser,
title={UBS: ChatGPT May Be the Fastest Growing App of All Time},
url={https://aibusiness.com/nlp/ubs-chatgpt-is-the-fastest-growing-app-of-all-time},
year={2023}}

@misc{msopenaicoll,
title={Microsoft and OpenAI extend partnership},
url={https://blogs.microsoft.com/blog/2023/01/23/microsoftandopenaiextendpartnership/},
year={2023}}

@misc{bard,
title={What’s ahead for Bard: More global, more visual, more integrated},
url={https://blog.google/technology/ai/google-bard-updates-io-2023},
year={2023}}

@misc{gpt3time,
title={OpenAI's GPT-3 Language Model: A Technical Overview},
url={https://lambdalabs.com/blog/demystifying-gpt-3},
year={2020}}

@misc{dgxh100archdoc,
title={NVIDIA DGX SuperPOD: Next Generation Scalable Infrastructure for AI Leadership, Reference Architecture},
url={https://docs.nvidia.com/dgx-superpod-reference-architecture-with-dgx-h100-systems.pdf},
year={2023}}

@misc{nvlnvs,
title={NVLink and NVSwitch: The building blocks of advanced multi-GPU communication—within and between servers.},
url={https://www.nvidia.com/en-us/data-center/nvlink/},
year={2023}}

@misc{gpudirect,
title={NVIDIA GPUDirect:
Enhancing Data Movement and Access for GPUs},
url={https://developer.nvidia.com/gpudirect},
year={2023}}

@inproceedings {rdma_azure,
author = {Wei Bai and Shanim Sainul Abdeen and Ankit Agrawal and Krishan Kumar Attre and Paramvir Bahl and Ameya Bhagat and Gowri Bhaskara and Tanya Brokhman and Lei Cao and Ahmad Cheema and Rebecca Chow and Jeff Cohen and Mahmoud Elhaddad and Vivek Ette and Igal Figlin and Daniel Firestone and Mathew George and Ilya German and Lakhmeet Ghai and Eric Green and Albert Greenberg and Manish Gupta and Randy Haagens and Matthew Hendel and Ridwan Howlader and Neetha John and Julia Johnstone and Tom Jolly and Greg Kramer and David Kruse and Ankit Kumar and Erica Lan and Ivan Lee and Avi Levy and Marina Lipshteyn and Xin Liu and Chen Liu and Guohan Lu and Yuemin Lu and Xiakun Lu and Vadim Makhervaks and Ulad Malashanka and David A. Maltz and Ilias Marinos and Rohan Mehta and Sharda Murthi and Anup Namdhari and Aaron Ogus and Jitendra Padhye and Madhav Pandya and Douglas Phillips and Adrian Power and Suraj Puri and Shachar Raindel and Jordan Rhee and Anthony Russo and Maneesh Sah and Ali Sheriff and Chris Sparacino and Ashutosh Srivastava and Weixiang Sun and Nick Swanson and Fuhou Tian and Lukasz Tomczyk and Vamsi Vadlamuri and Alec Wolman and Ying Xie and Joyce Yom and Lihua Yuan and Yanzhao Zhang and Brian Zill},
title = {Empowering Azure Storage with {RDMA}},
booktitle = {20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23)},
year = {2023},
isbn = {978-1-939133-33-5},
address = {Boston, MA},
pages = {49--67},
url = {https://www.usenix.org/conference/nsdi23/presentation/bai},
publisher = {USENIX Association},
month = apr,
}
@article{10.1145/2829988.2787484,
author = {Zhu, Yibo and Eran, Haggai and Firestone, Daniel and Guo, Chuanxiong and Lipshteyn, Marina and Liron, Yehonatan and Padhye, Jitendra and Raindel, Shachar and Yahia, Mohamad Haj and Zhang, Ming},
title = {Congestion Control for Large-Scale RDMA Deployments},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {4},
issn = {0146-4833},
url = {https://doi.org/10.1145/2829988.2787484},
doi = {10.1145/2829988.2787484},
abstract = {Modern datacenter applications demand high throughput (40Gbps) and ultra-low latency (< 10 μs per hop) from the network, with low CPU overhead. Standard TCP/IP stacks cannot meet these requirements, but Remote Direct Memory Access (RDMA) can. On IP-routed datacenter networks, RDMA is deployed using RoCEv2 protocol, which relies on Priority-based Flow Control (PFC) to enable a drop-free network. However, PFC can lead to poor application performance due to problems like head-of-line blocking and unfairness. To alleviates these problems, we introduce DCQCN, an end-to-end congestion control scheme for RoCEv2. To optimize DCQCN performance, we build a fluid model, and provide guidelines for tuning switch buffer thresholds, and other protocol parameters. Using a 3-tier Clos network testbed, we show that DCQCN dramatically improves throughput and fairness of RoCEv2 RDMA traffic. DCQCN is implemented in Mellanox NICs, and is being deployed in Microsoft's datacenters.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = {aug},
pages = {523–536},
numpages = {14},
keywords = {RDMA, PFC, datacenter transport, ECN, congestion control}
}

@inproceedings{ddlindc,
author = {Hu, Shuihai and Zhu, Yibo and Cheng, Peng and Guo, Chuanxiong and Tan, Kun and Padhye, Jitendra and Chen, Kai},
title = {Deadlocks in Datacenter Networks: Why Do They Form, and How to Avoid Them},
year = {2016},
isbn = {9781450346610},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3005745.3005760},
doi = {10.1145/3005745.3005760},
abstract = {Driven by the need for ultra-low latency, high throughput and low CPU overhead, Remote Direct Memory Access (RDMA) is being deployed by many cloud providers. To deploy RDMA in Ethernet networks, Priority-based Flow Control (PFC) must be used. PFC, however, makes Ethernet networks prone to deadlocks. Prior work on deadlock avoidance has focused on {em necessary} condition for deadlock formation, which leads to rather onerous and expensive solutions for deadlock avoidance. In this paper, we investigate {em sufficient} conditions for deadlock formation, conjecturing that avoiding {em sufficient} conditions might be less onerous.},
booktitle = {Proceedings of the 15th ACM Workshop on Hot Topics in Networks},
pages = {92–98},
numpages = {7},
location = {Atlanta, GA, USA},
series = {HotNets '16}
}



@inproceedings{revisitrdmanet,
author = {Mittal, Radhika and Shpiner, Alexander and Panda, Aurojit and Zahavi, Eitan and Krishnamurthy, Arvind and Ratnasamy, Sylvia and Shenker, Scott},
title = {Revisiting Network Support for RDMA},
year = {2018},
isbn = {9781450355674},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230543.3230557},
doi = {10.1145/3230543.3230557},
abstract = {The advent of RoCE (RDMA over Converged Ethernet) has led to a significant increase in the use of RDMA in datacenter networks. To achieve good performance, RoCE requires a lossless network which is in turn achieved by enabling Priority Flow Control (PFC) within the network. However, PFC brings with it a host of problems such as head-of-the-line blocking, congestion spreading, and occasional deadlocks. Rather than seek to fix these issues, we instead ask: is PFC fundamentally required to support RDMA over Ethernet?We show that the need for PFC is an artifact of current RoCE NIC designs rather than a fundamental requirement. We propose an improved RoCE NIC (IRN) design that makes a few simple changes to the RoCE NIC for better handling of packet losses. We show that IRN (without PFC) outperforms RoCE (with PFC) by 6-83% for typical network scenarios. Thus not only does IRN eliminate the need for PFC, it improves performance in the process! We further show that the changes that IRN introduces can be implemented with modest overheads of about 3-10% to NIC resources. Based on our results, we argue that research and industry should rethink the current trajectory of network support for RDMA.},
booktitle = {Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication},
pages = {313–326},
numpages = {14},
keywords = {RoCE, RDMA, iWARP, datacenter transport, PFC},
location = {Budapest, Hungary},
series = {SIGCOMM '18}
}


@inproceedings {srnic,
author = {Zilong Wang and Layong Luo and Qingsong Ning and Chaoliang Zeng and Wenxue Li and Xinchen Wan and Peng Xie and Tao Feng and Ke Cheng and Xiongfei Geng and Tianhao Wang and Weicheng Ling and Kejia Huo and Pingbo An and Kui Ji and Shideng Zhang and Bin Xu and Ruiqing Feng and Tao Ding and Kai Chen and Chuanxiong Guo},
title = {{SRNIC}: A Scalable Architecture for {RDMA} {NICs}},
booktitle = {20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23)},
year = {2023},
isbn = {978-1-939133-33-5},
address = {Boston, MA},
pages = {1--14},
url = {https://www.usenix.org/conference/nsdi23/presentation/wang-zilong},
publisher = {USENIX Association},
month = apr,
}

@inproceedings{rdma_cc,
author = {Zhu, Yibo and Eran, Haggai and Firestone, Daniel and Guo, Chuanxiong and Lipshteyn, Marina and Liron, Yehonatan and Padhye, Jitendra and Raindel, Shachar and Yahia, Mohamad Haj and Zhang, Ming},
title = {Congestion Control for Large-Scale RDMA Deployments},
year = {2015},
isbn = {9781450335423},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2785956.2787484},
doi = {10.1145/2785956.2787484},
abstract = {Modern datacenter applications demand high throughput (40Gbps) and ultra-low latency (< 10 μs per hop) from the network, with low CPU overhead. Standard TCP/IP stacks cannot meet these requirements, but Remote Direct Memory Access (RDMA) can. On IP-routed datacenter networks, RDMA is deployed using RoCEv2 protocol, which relies on Priority-based Flow Control (PFC) to enable a drop-free network. However, PFC can lead to poor application performance due to problems like head-of-line blocking and unfairness. To alleviates these problems, we introduce DCQCN, an end-to-end congestion control scheme for RoCEv2. To optimize DCQCN performance, we build a fluid model, and provide guidelines for tuning switch buffer thresholds, and other protocol parameters. Using a 3-tier Clos network testbed, we show that DCQCN dramatically improves throughput and fairness of RoCEv2 RDMA traffic. DCQCN is implemented in Mellanox NICs, and is being deployed in Microsoft's datacenters.},
booktitle = {Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication},
pages = {523–536},
numpages = {14},
keywords = {ECN, PFC, datacenter transport, RDMA, congestion control},
location = {London, United Kingdom},
series = {SIGCOMM '15}
}


@inproceedings {bfc,
author = {Prateesh Goyal and Preey Shah and Kevin Zhao and Georgios Nikolaidis and Mohammad Alizadeh and Thomas E. Anderson},
title = {Backpressure Flow Control},
booktitle = {19th USENIX Symposium on Networked Systems Design and Implementation (NSDI 22)},
year = {2022},
isbn = {978-1-939133-27-4},
address = {Renton, WA},
pages = {779--805},
url = {https://www.usenix.org/conference/nsdi22/presentation/goyal},
publisher = {USENIX Association},
month = apr,
}

@INPROCEEDINGS{ib_deadlock,
  author={Schneider, Timo and Bibartiu, Otto and Hoefler, Torsten},
  booktitle={2016 IEEE 24th Annual Symposium on High-Performance Interconnects (HOTI)}, 
  title={Ensuring Deadlock-Freedom in Low-Diameter InfiniBand Networks}, 
  year={2016},
  volume={},
  number={},
  pages={1-8},
  doi={10.1109/HOTI.2016.015}}

@inproceedings{pfc_storm,
author = {Guo, Chuanxiong and Wu, Haitao and Deng, Zhong and Soni, Gaurav and Ye, Jianxi and Padhye, Jitu and Lipshteyn, Marina},
title = {RDMA over Commodity Ethernet at Scale},
year = {2016},
isbn = {9781450341936},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934872.2934908},
doi = {10.1145/2934872.2934908},
abstract = {Over the past one and half years, we have been using RDMA over commodity Ethernet (RoCEv2) to support some of Microsoft's highly-reliable, latency-sensitive services. This paper describes the challenges we encountered during the process and the solutions we devised to address them. In order to scale RoCEv2 beyond VLAN, we have designed a DSCP-based priority flow-control (PFC) mechanism to ensure large-scale deployment. We have addressed the safety challenges brought by PFC-induced deadlock (yes, it happened!), RDMA transport livelock, and the NIC PFC pause frame storm problem. We have also built the monitoring and management systems to make sure RDMA works as expected. Our experiences show that the safety and scalability issues of running RoCEv2 at scale can all be addressed, and RDMA can replace TCP for intra data center communications and achieve low latency, low CPU overhead, and high throughput.},
booktitle = {Proceedings of the 2016 ACM SIGCOMM Conference},
pages = {202–215},
numpages = {14},
keywords = {Deadlock, PFC propagation, PFC, RDMA, RoCEv2},
location = {Florianopolis, Brazil},
series = {SIGCOMM '16}
}

@inproceedings{mlnxrdmagpu,
author = {Shainer, Gilad and Lui, Pak and Liu, Tong},
title = {The Development of Mellanox/NVIDIA GPUDirect over InfiniBand: A New Model for GPU to GPU Communications},
year = {2011},
isbn = {9781450308885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2016741.2016769},
doi = {10.1145/2016741.2016769},
abstract = {The usage and adoption of General Purpose GPUs (GPGPU) in HPC systems is increasing due to the unparalleled performance advantage of the GPUs and the ability to fulfill the ever-increasing demands for floating points operations. While the GPU can offload many of the application parallel computations, the system architecture of a GPU-CPU-InfiniBand server does require the CPU to initiate and manage memory transfers between remote GPUs via the high speed InfiniBand network. In this paper we introduce for the first time a new innovative technology - GPUDirect that enables Tesla GPUs to transfer data via InfiniBand without the involvement of the CPU or buffer copies, hence dramatically reducing the GPU communication time and increasing overall system performance and efficiency. We also explore for the first time the performance benefits of GPUDirect using Amber and LAMMPS applications.},
booktitle = {Proceedings of the 2011 TeraGrid Conference: Extreme Digital Discovery},
articleno = {26},
numpages = {1},
keywords = {RDMA, GPGPU, GPUDirect, InfiniBand},
location = {Salt Lake City, Utah},
series = {TG '11}
}
@misc{rfc8797,
    series =    {Request for Comments},
    number =    8797,
    howpublished =  {RFC 8797},
    publisher = {RFC Editor},
    doi =       {10.17487/RFC8797},
    url =       {https://www.rfc-editor.org/info/rfc8797},
        author =    {Chuck Lever},
    title =     {{Remote Direct Memory Access - Connection Manager (RDMA-CM) Private Data for RPC-over-RDMA Version 1}},
    pagetotal = 12,
    year =      2020,
    month =     jun,
    abstract =  {This document specifies the format of Remote Direct Memory Access - Connection Manager (RDMA-CM) Private Data exchanged between RPC-over-RDMA version 1 peers as part of establishing a connection. The addition of the Private Data payload specified in this document is an optional extension that does not alter the RPC-over-RDMA version 1 protocol. This document updates RFC 8166.},
}

@misc{gpt4params,
title={GPT-4 has a trillion parameters - Report},
url={https://the-decoder.com/gpt-4-has-a-trillion-parameters/},
year={2023}}


@misc{bing,
title={Confirmed: the new Bing runs on OpenAI’s GPT-4},
url={https://blogs.bing.com/search/march_2023/Confirmed-the-new-Bing-runs-on-OpenAI%E2%80%99s-GPT-4},
year={2023}
}

@misc{githubcop,
title={Your AI pair programmer: GitHub Copilot uses the OpenAI Codex to suggest code and entire functions in real-time, right from your editor.},
url={https://github.com/features/copilot},
year={2023}
}


@misc{gh200,
title={NVIDIA DGX GH200},
url={https://www.nvidia.com/en-us/data-center/dgx-gh200/},
year={2023}
}



@inproceedings{fattree,
author = {Al-Fares, Mohammad and Loukissas, Alexander and Vahdat, Amin},
title = {A Scalable, Commodity Data Center Network Architecture},
year = {2008},
isbn = {9781605581750},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1402958.1402967},
doi = {10.1145/1402958.1402967},
abstract = {Today's data centers may contain tens of thousands of computers with significant aggregate bandwidth requirements. The network architecture typically consists of a tree of routing and switching elements with progressively more specialized and expensive equipment moving up the network hierarchy. Unfortunately, even when deploying the highest-end IP switches/routers, resulting topologies may only support 50% of the aggregate bandwidth available at the edge of the network, while still incurring tremendous cost. Non-uniform bandwidth among data center nodes complicates application design and limits overall system performance.In this paper, we show how to leverage largely commodity Ethernet switches to support the full aggregate bandwidth of clusters consisting of tens of thousands of elements. Similar to how clusters of commodity computers have largely replaced more specialized SMPs and MPPs, we argue that appropriately architected and interconnected commodity switches may deliver more performance at less cost than available from today's higher-end solutions. Our approach requires no modifications to the end host network interface, operating system, or applications; critically, it is fully backward compatible with Ethernet, IP, and TCP.},
booktitle = {Proceedings of the ACM SIGCOMM 2008 Conference on Data Communication},
pages = {63–74},
numpages = {12},
keywords = {equal-cost routing, data center topology},
location = {Seattle, WA, USA},
series = {SIGCOMM '08}
}

@inproceedings{sirius,
author = {Ballani, Hitesh and Costa, Paolo and Behrendt, Raphael and Cletheroe, Daniel and Haller, Istvan and Jozwik, Krzysztof and Karinou, Fotini and Lange, Sophie and Shi, Kai and Thomsen, Benn and Williams, Hugh},
title = {Sirius: A Flat Datacenter Network with Nanosecond Optical Switching},
year = {2020},
isbn = {9781450379557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387514.3406221},
doi = {10.1145/3387514.3406221},
abstract = {The increasing gap between the growth of datacenter traffic and electrical switch capacity is expected to worsen due to the slowdown of Moore's law, motivating the need for a new switching technology for the post-Moore's law era that can meet the increasingly stringent requirements of hardware-driven cloud workloads. We propose Sirius, an optically-switched network for datacenters providing the abstraction of a single, high-radix switch that can connect thousands of nodes---racks or servers---in a datacenter while achieving nanosecond-granularity reconfiguration. At its core, Sirius uses a combination of tunable lasers and simple, passive gratings that route light based on its wavelength. Sirius' switching technology and topology is tightly codesigned with its routing and scheduling and with novel congestion-control and time-synchronization mechanisms to achieve a scalable yet flat network that can offer high bandwidth and very low end-to-end latency. Through a small-scale prototype using a custom tunable laser chip that can tune in less than 912 ps, we demonstrate 3.84 ns end-to-end reconfiguration atop 50 Gbps channels. Through large-scale simulations, we show that Sirius can approximate the performance of an ideal, electrically-switched non-blocking network with up to 74-77\% lower power.},
booktitle = {Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols for Computer Communication},
pages = {782–797},
numpages = {16},
keywords = {Vertical Integration, Datacenter Networks, Fast Tunable Lasers, Optical Switches, Nanosecond Switching, Scheduler-less design},
location = {Virtual Event, USA},
series = {SIGCOMM '20}
}

@misc{vaswani2017attention,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2017},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{jupiterevolving,
author = {Poutievski, Leon and Mashayekhi, Omid and Ong, Joon and Singh, Arjun and Tariq, Mukarram and Wang, Rui and Zhang, Jianan and Beauregard, Virginia and Conner, Patrick and Gribble, Steve and Kapoor, Rishi and Kratzer, Stephen and Li, Nanfang and Liu, Hong and Nagaraj, Karthik and Ornstein, Jason and Sawhney, Samir and Urata, Ryohei and Vicisano, Lorenzo and Yasumura, Kevin and Zhang, Shidong and Zhou, Junlan and Vahdat, Amin},
title = {Jupiter Evolving: Transforming Google's Datacenter Network via Optical Circuit Switches and Software-Defined Networking},
year = {2022},
isbn = {9781450394208},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544216.3544265},
doi = {10.1145/3544216.3544265},
abstract = {We present a decade of evolution and production experience with Jupiter datacenter network fabrics. In this period Jupiter has delivered 5x higher speed and capacity, 30% reduction in capex, 41% reduction in power, incremental deployment and technology refresh all while serving live production traffic. A key enabler for these improvements is evolving Jupiter from a Clos to a direct-connect topology among the machine aggregation blocks. Critical architectural changes for this include: A datacenter interconnection layer employing Micro-Electro-Mechanical Systems (MEMS) based Optical Circuit Switches (OCSes) to enable dynamic topology reconfiguration, centralized Software-Defined Networking (SDN) control for traffic engineering, and automated network operations for incremental capacity delivery and topology engineering. We show that the combination of traffic and topology engineering on direct-connect fabrics achieves similar throughput as Clos fabrics for our production traffic patterns. We also optimize for path lengths: 60% of the traffic takes direct path from source to destination aggregation blocks, while the remaining transits one additional block, achieving an average block-level path length of 1.4 in our fleet today. OCS also achieves 3x faster fabric reconfiguration compared to pre-evolution Clos fabrics that used a patch panel based interconnect.},
booktitle = {Proceedings of the ACM SIGCOMM 2022 Conference},
pages = {66–85},
numpages = {20},
keywords = {traffic engineering, topology engineering, software-defined networking, optical circuit switches, datacenter network},
location = {Amsterdam, Netherlands},
series = {SIGCOMM '22}
}


@misc{kaplan2020scaling,
      title={Scaling Laws for Neural Language Models}, 
      author={Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
      year={2020},
      eprint={2001.08361},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{tofu,
author = {Wang, Minjie and Huang, Chien-chin and Li, Jinyang},
title = {Supporting Very Large Models Using Automatic Dataflow Graph Partitioning},
year = {2019},
isbn = {9781450362818},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302424.3303953},
doi = {10.1145/3302424.3303953},
abstract = {This paper presents Tofu, a system that partitions very large DNN models across multiple GPU devices to reduce per-GPU memory footprint. Tofu is designed to partition a dataflow graph of fine-grained tensor operators used by platforms like MXNet and TensorFlow. In order to automatically partition each operator, we propose to describe the semantics of an operator in a simple language inspired by Halide. To optimally partition different operators in a dataflow graph, Tofu uses a recursive search algorithm that minimizes the total communication cost. Our experiments on an 8-GPU machine show that Tofu enables the training of very large CNN and RNN models. It also achieves 25% - 400% speedup over alternative approaches to train very large models.},
booktitle = {Proceedings of the Fourteenth EuroSys Conference 2019},
articleno = {26},
numpages = {17},
location = {Dresden, Germany},
series = {EuroSys '19}
}



@inproceedings{sipml,
author = {Khani, Mehrdad and Ghobadi, Manya and Alizadeh, Mohammad and Zhu, Ziyi and Glick, Madeleine and Bergman, Keren and Vahdat, Amin and Klenk, Benjamin and Ebrahimi, Eiman},
title = {SiP-ML: High-Bandwidth Optical Network Interconnects for Machine Learning Training},
year = {2021},
isbn = {9781450383837},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3452296.3472900},
doi = {10.1145/3452296.3472900},
abstract = {This paper proposes optical network interconnects as a key enabler for building high-bandwidth ML training clusters with strong scaling properties. Our design, called SiP-ML, accelerates the training time of popular DNN models using silicon photonics links capable of providing multiple terabits-per-second of bandwidth per GPU. SiP-ML partitions the training job across GPUs with hybrid data and model parallelism while ensuring the communication pattern can be supported efficiently on the network interconnect. We develop task partitioning and device placement methods that take the degree and reconfiguration latency of optical interconnects into account. Simulations using real DNN models show that, compared to the state-of-the-art electrical networks, our approach improves training time by 1.3--9.1x.},
booktitle = {Proceedings of the 2021 ACM SIGCOMM 2021 Conference},
pages = {657–675},
numpages = {19},
keywords = {optical networks, reconfigurable networks, distributed machine learning, silicon photonics},
location = {Virtual Event, USA},
series = {SIGCOMM '21}
}

@misc{chowdhery2022palm,
      title={PaLM: Scaling Language Modeling with Pathways}, 
      author={Aakanksha Chowdhery and Sharan Narang and Jacob Devlin and Maarten Bosma and Gaurav Mishra and Adam Roberts and Paul Barham and Hyung Won Chung and Charles Sutton and Sebastian Gehrmann and Parker Schuh and Kensen Shi and Sasha Tsvyashchenko and Joshua Maynez and Abhishek Rao and Parker Barnes and Yi Tay and Noam Shazeer and Vinodkumar Prabhakaran and Emily Reif and Nan Du and Ben Hutchinson and Reiner Pope and James Bradbury and Jacob Austin and Michael Isard and Guy Gur-Ari and Pengcheng Yin and Toju Duke and Anselm Levskaya and Sanjay Ghemawat and Sunipa Dev and Henryk Michalewski and Xavier Garcia and Vedant Misra and Kevin Robinson and Liam Fedus and Denny Zhou and Daphne Ippolito and David Luan and Hyeontaek Lim and Barret Zoph and Alexander Spiridonov and Ryan Sepassi and David Dohan and Shivani Agrawal and Mark Omernick and Andrew M. Dai and Thanumalayan Sankaranarayana Pillai and Marie Pellat and Aitor Lewkowycz and Erica Moreira and Rewon Child and Oleksandr Polozov and Katherine Lee and Zongwei Zhou and Xuezhi Wang and Brennan Saeta and Mark Diaz and Orhan Firat and Michele Catasta and Jason Wei and Kathy Meier-Hellstern and Douglas Eck and Jeff Dean and Slav Petrov and Noah Fiedel},
      year={2022},
      eprint={2204.02311},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{zhang2022opt,
      title={OPT: Open Pre-trained Transformer Language Models}, 
      author={Susan Zhang and Stephen Roller and Naman Goyal and Mikel Artetxe and Moya Chen and Shuohui Chen and Christopher Dewan and Mona Diab and Xian Li and Xi Victoria Lin and Todor Mihaylov and Myle Ott and Sam Shleifer and Kurt Shuster and Daniel Simig and Punit Singh Koura and Anjali Sridhar and Tianlu Wang and Luke Zettlemoyer},
      year={2022},
      eprint={2205.01068},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{qm9700switch,
title={QM9700/QM9790 1U NDR 400Gb/s InfiniBand Switch Systems User Manual},
url={https://docs.nvidia.com/networking/display/QM97X0PUB},
year={2023}
}

@misc{fbdcarch,
title={Introducing data center fabric, the next-generation Facebook data center network},
url={https://engineering.fb.com/2014/11/14/production-engineering/introducing-data-center-fabric-the-next-generation-facebook-data-center-network/},
year={2014}
}

@misc{DNNmodelgrowth,
title={OpenAI: AI and compute},
url={https://openai.com/research/ai-and-compute},
year={2023}
}

@misc{dolly,
title={Hello Dolly: Democratizing the magic of ChatGPT with open models},
url={https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html},
year={2023}
}

