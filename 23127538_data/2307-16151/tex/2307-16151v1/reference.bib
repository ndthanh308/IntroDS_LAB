% StyleGAN
@inproceedings{karras2019style,
  title={A style-based generator architecture for generative adversarial networks},
  author={Karras, Tero and Laine, Samuli and Aila, Timo},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={4401--4410},
  year={2019}
}
@inproceedings{karras2020analyzing,
  title={Analyzing and improving the image quality of stylegan},
  author={Karras, Tero and Laine, Samuli and Aittala, Miika and Hellsten, Janne and Lehtinen, Jaakko and Aila, Timo},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={8110--8119},
  year={2020}
}
@article{karras2020training,
  title={Training generative adversarial networks with limited data},
  author={Karras, Tero and Aittala, Miika and Hellsten, Janne and Laine, Samuli and Lehtinen, Jaakko and Aila, Timo},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={12104--12114},
  year={2020}
}
@article{karras2021alias,
  title={Alias-free generative adversarial networks},
  author={Karras, Tero and Aittala, Miika and Laine, Samuli and H{\"a}rk{\"o}nen, Erik and Hellsten, Janne and Lehtinen, Jaakko and Aila, Timo},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={852--863},
  year={2021}
}


% Attention
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
% ViT
@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}
@inproceedings{liu2021swin,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={10012--10022},
  year={2021}
}
@inproceedings{liu2022swin,
  title={Swin transformer v2: Scaling up capacity and resolution},
  author={Liu, Ze and Hu, Han and Lin, Yutong and Yao, Zhuliang and Xie, Zhenda and Wei, Yixuan and Ning, Jia and Cao, Yue and Zhang, Zheng and Dong, Li and others},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={12009--12019},
  year={2022}
}
@article{fang2021you,
  title={You only look at one sequence: Rethinking transformer in vision through object detection},
  author={Fang, Yuxin and Liao, Bencheng and Wang, Xinggang and Fang, Jiemin and Qi, Jiyang and Wu, Rui and Niu, Jianwei and Liu, Wenyu},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={26183--26197},
  year={2021}
}
@article{zhu2020deformable,
  title={Deformable detr: Deformable transformers for end-to-end object detection},
  author={Zhu, Xizhou and Su, Weijie and Lu, Lewei and Li, Bin and Wang, Xiaogang and Dai, Jifeng},
  journal={arXiv preprint arXiv:2010.04159},
  year={2020}
}
@inproceedings{yuan2021tokens,
  title={Tokens-to-token vit: Training vision transformers from scratch on imagenet},
  author={Yuan, Li and Chen, Yunpeng and Wang, Tao and Yu, Weihao and Shi, Yujun and Jiang, Zi-Hang and Tay, Francis EH and Feng, Jiashi and Yan, Shuicheng},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={558--567},
  year={2021}
}

% Latent Space Study
% InterFaceGAN
@inproceedings{shen2020interpreting,
  title={Interpreting the latent space of gans for semantic face editing},
  author={Shen, Yujun and Gu, Jinjin and Tang, Xiaoou and Zhou, Bolei},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={9243--9252},
  year={2020}
}
@inproceedings{goetschalckx2019ganalyze,
  title={Ganalyze: Toward visual definitions of cognitive image properties},
  author={Goetschalckx, Lore and Andonian, Alex and Oliva, Aude and Isola, Phillip},
  booktitle={Proceedings of the ieee/cvf international conference on computer vision},
  pages={5744--5753},
  year={2019}
}
@article{abdal2021styleflow,
  title={Styleflow: Attribute-conditioned exploration of stylegan-generated images using conditional continuous normalizing flows},
  author={Abdal, Rameen and Zhu, Peihao and Mitra, Niloy J and Wonka, Peter},
  journal={ACM Transactions on Graphics (ToG)},
  volume={40},
  number={3},
  pages={1--21},
  year={2021},
  publisher={ACM New York, NY}
}
@article{harkonen2020ganspace,
  title={Ganspace: Discovering interpretable gan controls},
  author={H{\"a}rk{\"o}nen, Erik and Hertzmann, Aaron and Lehtinen, Jaakko and Paris, Sylvain},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9841--9850},
  year={2020}
}
@article{plumerault2020controlling,
  title={Controlling generative models with continuous factors of variations},
  author={Plumerault, Antoine and Borgne, Herv{\'e} Le and Hudelot, C{\'e}line},
  journal={arXiv preprint arXiv:2001.10238},
  year={2020}
}
@inproceedings{shen2021closed,
  title={Closed-form factorization of latent semantics in gans},
  author={Shen, Yujun and Zhou, Bolei},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={1532--1540},
  year={2021}
}
@inproceedings{voynov2020unsupervised,
  title={Unsupervised discovery of interpretable directions in the gan latent space},
  author={Voynov, Andrey and Babenko, Artem},
  booktitle={International conference on machine learning},
  pages={9786--9796},
  year={2020},
  organization={PMLR}
}
@article{jahanian2019steerability,
  title={On the" steerability" of generative adversarial networks},
  author={Jahanian, Ali and Chai, Lucy and Isola, Phillip},
  journal={arXiv preprint arXiv:1907.07171},
  year={2019}
}
@inproceedings{doner2022paintinstyle,
  title={PaintInStyle: One-Shot Discovery of Interpretable Directions by Painting},
  author={Doner, Berkay and Balcioglu, Elif Sema and Barin, Merve Rabia and Kocasari, Umut and Tiftikci, Mert and Yanardag, Pinar},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={2288--2293},
  year={2022}
}
@inproceedings{abdal2022clip2stylegan,
  title={Clip2stylegan: Unsupervised extraction of stylegan edit directions},
  author={Abdal, Rameen and Zhu, Peihao and Femiani, John and Mitra, Niloy and Wonka, Peter},
  booktitle={ACM SIGGRAPH 2022 conference proceedings},
  pages={1--9},
  year={2022}
}
@inproceedings{patashnik2021styleclip,
  title={Styleclip: Text-driven manipulation of stylegan imagery},
  author={Patashnik, Or and Wu, Zongze and Shechtman, Eli and Cohen-Or, Daniel and Lischinski, Dani},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={2085--2094},
  year={2021}
}

% GAN Inversion

% Learning-based
% psp
@inproceedings{richardson2021encoding,
  title={Encoding in style: a stylegan encoder for image-to-image translation},
  author={Richardson, Elad and Alaluf, Yuval and Patashnik, Or and Nitzan, Yotam and Azar, Yaniv and Shapiro, Stav and Cohen-Or, Daniel},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={2287--2296},
  year={2021}
}
% e4e
@article{tov2021designing,
  title={Designing an encoder for stylegan image manipulation},
  author={Tov, Omer and Alaluf, Yuval and Nitzan, Yotam and Patashnik, Or and Cohen-Or, Daniel},
  journal={ACM Transactions on Graphics (TOG)},
  volume={40},
  number={4},
  pages={1--14},
  year={2021},
  publisher={ACM New York, NY, USA}
}
% BDInverter
@inproceedings{kang2021gan,
  title={Gan inversion for out-of-range images with geometric transformations},
  author={Kang, Kyoungkook and Kim, Seongtae and Cho, Sunghyun},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={13941--13949},
  year={2021}
}
@inproceedings{alaluf2021restyle,
  title={Restyle: A residual-based stylegan encoder via iterative refinement},
  author={Alaluf, Yuval and Patashnik, Or and Cohen-Or, Daniel},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={6711--6720},
  year={2021}
}
@inproceedings{alaluf2022hyperstyle,
  title={Hyperstyle: Stylegan inversion with hypernetworks for real image editing},
  author={Alaluf, Yuval and Tov, Omer and Mokady, Ron and Gal, Rinon and Bermano, Amit},
  booktitle={Proceedings of the IEEE/CVF conference on computer Vision and pattern recognition},
  pages={18511--18521},
  year={2022}
}
@inproceedings{dinh2022hyperinverter,
  title={Hyperinverter: Improving stylegan inversion via hypernetwork},
  author={Dinh, Tan M and Tran, Anh Tuan and Nguyen, Rang and Hua, Binh-Son},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11389--11398},
  year={2022}
}
@article{wei2022e2style,
  title={E2Style: Improve the efficiency and effectiveness of StyleGAN inversion},
  author={Wei, Tianyi and Chen, Dongdong and Zhou, Wenbo and Liao, Jing and Zhang, Weiming and Yuan, Lu and Hua, Gang and Yu, Nenghai},
  journal={IEEE Transactions on Image Processing},
  volume={31},
  pages={3267--3280},
  year={2022},
  publisher={IEEE}
}
% PadInv
@inproceedings{bai2022high,
  title={High-fidelity GAN inversion with padding space},
  author={Bai, Qingyan and Xu, Yinghao and Zhu, Jiapeng and Xia, Weihao and Yang, Yujiu and Shen, Yujun},
  booktitle={Computer Vision--ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23--27, 2022, Proceedings, Part XV},
  pages={36--53},
  year={2022},
  organization={Springer}
}
% HFGI
@inproceedings{wang2022high,
  title={High-fidelity gan inversion for image attribute editing},
  author={Wang, Tengfei and Zhang, Yong and Fan, Yanbo and Wang, Jue and Chen, Qifeng},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11379--11388},
  year={2022}
}
@inproceedings{hu2022style,
  title={Style transformer for image inversion and editing},
  author={Hu, Xueqi and Huang, Qiusheng and Shi, Zhengyi and Li, Siyuan and Gao, Changxin and Sun, Li and Li, Qingli},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11337--11346},
  year={2022}
}
@article{yao2022feature,
  title={Feature-style encoder for style-based GAN inversion},
  author={Yao, Xu and Newson, Alasdair and Gousseau, Yann and Hellier, Pierre},
  journal={arXiv e-prints},
  pages={arXiv--2202},
  year={2022}
}
@inproceedings{wu2021stylespace,
  title={Stylespace analysis: Disentangled controls for stylegan image generation},
  author={Wu, Zongze and Lischinski, Dani and Shechtman, Eli},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={12863--12872},
  year={2021}
}
% CLCAE
@article{liu2022delving,
  title={Delving StyleGAN Inversion for Image Editing: A Foundation Latent Space Viewpoint},
  author={Liu, Hongyu and Song, Yibing and Chen, Qifeng},
  journal={arXiv preprint arXiv:2211.11448},
  year={2022}
}

% Optimization-based
@inproceedings{abdal2019image2stylegan,
  title={Image2stylegan: How to embed images into the stylegan latent space?},
  author={Abdal, Rameen and Qin, Yipeng and Wonka, Peter},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={4432--4441},
  year={2019}
}
@inproceedings{abdal2020image2stylegan++,
  title={Image2stylegan++: How to edit the embedded images?},
  author={Abdal, Rameen and Qin, Yipeng and Wonka, Peter},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={8296--8305},
  year={2020}
}
% ii2s
@article{zhu2020improved,
  title={Improved stylegan embedding: Where are the good latents?},
  author={Zhu, Peihao and Abdal, Rameen and Qin, Yipeng and Femiani, John and Wonka, Peter},
  journal={arXiv preprint arXiv:2012.09036},
  year={2020}
}
% PTI
@article{roich2022pivotal,
  title={Pivotal tuning for latent-based editing of real images},
  author={Roich, Daniel and Mokady, Ron and Bermano, Amit H and Cohen-Or, Daniel},
  journal={ACM Transactions on Graphics (TOG)},
  volume={42},
  number={1},
  pages={1--13},
  year={2022},
  publisher={ACM New York, NY}
}
@inproceedings{mao2022cycle,
  title={Cycle Encoding of a StyleGAN Encoder for Improved Reconstruction and Editability},
  author={Mao, Xudong and Cao, Liujuan and Gnanha, Aurele Tohokantche and Yang, Zhenguo and Li, Qing and Ji, Rongrong},
  booktitle={Proceedings of the 30th ACM International Conference on Multimedia},
  pages={2032--2041},
  year={2022}
}


% Additional
@inproceedings{deng2019arcface,
  title={Arcface: Additive angular margin loss for deep face recognition},
  author={Deng, Jiankang and Guo, Jia and Xue, Niannan and Zafeiriou, Stefanos},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={4690--4699},
  year={2019}
}
@inproceedings{huang2020curricularface,
  title={Curricularface: adaptive curriculum learning loss for deep face recognition},
  author={Huang, Yuge and Wang, Yuhan and Tai, Ying and Liu, Xiaoming and Shen, Pengcheng and Li, Shaoxin and Li, Jilin and Huang, Feiyue},
  booktitle={proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={5901--5910},
  year={2020}
}
% coral
@article{cao2020rank,
  title={Rank consistent ordinal regression for neural networks with application to age estimation},
  author={Cao, Wenzhi and Mirjalili, Vahid and Raschka, Sebastian},
  journal={Pattern Recognition Letters},
  volume={140},
  pages={325--331},
  year={2020},
  publisher={Elsevier}
}
% lpips
@inproceedings{zhang2018unreasonable,
  title={The unreasonable effectiveness of deep features as a perceptual metric},
  author={Zhang, Richard and Isola, Phillip and Efros, Alexei A and Shechtman, Eli and Wang, Oliver},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={586--595},
  year={2018}
}
% PGGAN, CelebA
@article{karras2017progressive,
  title={Progressive growing of gans for improved quality, stability, and variation},
  author={Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
  journal={arXiv preprint arXiv:1710.10196},
  year={2017}
}
% CLIP
@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}
% NIQE
@article{mittal2012making,
  title={Making a “completely blind” image quality analyzer},
  author={Mittal, Anish and Soundararajan, Rajiv and Bovik, Alan C},
  journal={IEEE Signal processing letters},
  volume={20},
  number={3},
  pages={209--212},
  year={2012},
  publisher={IEEE}
}
% FID
@article{heusel2017gans,
  title={Gans trained by a two time-scale update rule converge to a local nash equilibrium},
  author={Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@article{zhang2019lookahead,
  title={Lookahead optimizer: k steps forward, 1 step back},
  author={Zhang, Michael and Lucas, James and Ba, Jimmy and Hinton, Geoffrey E},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@article{liu2019variance,
  title={On the variance of the adaptive learning rate and beyond},
  author={Liu, Liyuan and Jiang, Haoming and He, Pengcheng and Chen, Weizhu and Liu, Xiaodong and Gao, Jianfeng and Han, Jiawei},
  journal={arXiv preprint arXiv:1908.03265},
  year={2019}
}
% AFHQ
@inproceedings{choi2020stargan,
  title={Stargan v2: Diverse image synthesis for multiple domains},
  author={Choi, Yunjey and Uh, Youngjung and Yoo, Jaejun and Ha, Jung-Woo},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={8188--8197},
  year={2020}
}
% MANIQA
@inproceedings{yang2022maniqa,
  title={MANIQA: Multi-dimension Attention Network for No-Reference Image Quality Assessment},
  author={Yang, Sidi and Wu, Tianhe and Shi, Shuwei and Lao, Shanshan and Gong, Yuan and Cao, Mingdeng and Wang, Jiahao and Yang, Yujiu},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={1191--1200},
  year={2022}
}
% RAFT
@inproceedings{teed2020raft,
  title={Raft: Recurrent all-pairs field transforms for optical flow},
  author={Teed, Zachary and Deng, Jia},
  booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part II 16},
  pages={402--419},
  year={2020},
  organization={Springer}
}
% LSUN
@article{yu2015lsun,
  title={Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop},
  author={Yu, Fisher and Seff, Ari and Zhang, Yinda and Song, Shuran and Funkhouser, Thomas and Xiao, Jianxiong},
  journal={arXiv preprint arXiv:1506.03365},
  year={2015}
}
