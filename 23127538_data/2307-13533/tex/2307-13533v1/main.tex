% ****** Start of file aipsamp.tex ******
%
%   This file is part of the AIP files in the AIP distribution for REVTeX 4.
%   Version 4.1 of REVTeX, October 2009
%
%   Copyright (c) 2009 American Institute of Physics.
%
%   See the AIP README file for restrictions and more information.
%
% TeX'ing this file requires that you have AMS-LaTeX 2.0 installed
% as well as the rest of the prerequisites for REVTeX 4.1
% 
% It also requires running BibTeX. The commands are as follows:
%
%  1)  latex  aipsamp
%  2)  bibtex aipsamp
%  3)  latex  aipsamp
%  4)  latex  aipsamp
%
% Use this file as a source of example code for your aip document.
% Use the file aiptemplate.tex as a template for your document.
\documentclass[%
 aip,
% jmp,
% bmf,
% sd,
% rsi,
 amsmath,amssymb,
%preprint,%
 reprint,%
%author-year,%
%author-numerical,%
% Conference Proceedings
]{revtex4-1}

\usepackage{graphicx}% Include figure files
\usepackage{dcolumn}% Align table columns on decimal point
\usepackage{bm}% bold math
%\usepackage[mathlines]{lineno}% Enable numbering of text and display math
%\linenumbers\relax % Commence numbering lines

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
% \usepackage{mathptmx}
\usepackage{float}
\makeatletter
\let\newfloat\newfloat@ltx
\makeatother
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{etoolbox}
\usepackage{color}
\usepackage{hyperref}
\usepackage{tabularx}
\usepackage{makecell}
\usepackage[figuresleft]{rotating}
\usepackage{xcolor}
\usepackage{soul}
\usepackage{pythonhighlight}
\newcommand{\edit}[1]{\hl{#1}}
\newcommand{\mathcolorbox}[1]{\colorbox{yellow}{$\displaystyle #1$}}

\include{vpack}

%% Apr 2021: AIP requests that the corresponding 
%% email to be moved after the affiliations
\makeatletter
\def\@email#1#2{%
 \endgroup
 \patchcmd{\titleblock@produce}
  {\frontmatter@RRAPformat}
  {\frontmatter@RRAPformat{\produce@RRAP{*#1\href{mailto:#2}{#2}}}\frontmatter@RRAPformat}
  {}{}
}%
\makeatother
\begin{document}

\preprint{AIP/123-QED}

\title{Differentiable Turbulence II}



\author{Varun Shankar}
\affiliation{Carnegie Mellon University}
%\email{varunshankar@cmu.edu} 

\author{Romit Maulik}
\affiliation{Pennsylvania State University}
\email{rmaulik@psu.edu} 

\author{Venkatasubramanian Viswanathan}
\affiliation{Carnegie Mellon University}
\email{venkvis@cmu.edu}


\date{\today}

\begin{abstract}
Differentiable fluid simulators are increasingly demonstrating value as useful tools for developing data-driven models in computational fluid dynamics (CFD). Differentiable turbulence, or the end-to-end training of machine learning (ML) models embedded in CFD solution algorithms, captures both the generalization power and limited upfront cost of physics-based simulations, and the flexibility and automated training of deep learning methods. We develop a framework for integrating deep learning models into a generic finite element numerical scheme for solving the Navier-Stokes equations, applying the technique to learn a sub-grid scale closure using a multi-scale graph neural network. We demonstrate the method on several realizations of flow over a backwards-facing step, testing on both unseen Reynolds numbers and new geometry. We show that the learned closure can achieve accuracy comparable to traditional large eddy simulation on a finer grid that amounts to an equivalent speedup of 10x. As the desire and need for cheaper CFD simulations grows, we see hybrid physics-ML methods as a path forward to be exploited in the near future.
\end{abstract}

\keywords{}

\maketitle

\section{Introduction}

Computational modeling of turbulent fluid flow represents a critical tool for the analysis of physical systems in a variety of engineering and scientific disciplines \cite{anderson1995computational}. Although the equations of fluid motion are well-understood, accurate numerical simulation of the governing Navier-Stokes equations with computational fluid dynamics (CFD) methods remains a formidable task \cite{Tennekes2018-mf}. Solutions to the Navier-Stokes equations are characterized by chaotic and multi-scale behavior, which makes resolving the entirety of the flow field through direct numerical simulation (DNS) often impractical \cite{Moin1998}. The number of grid points required for DNS can be shown to scale with $Re^{9/4}$, where the Reynolds number $Re$ represents the ratio of inertial to viscous forces in the flow \cite{Pope2000}. High Reynolds number flows are often of practical interest, but simultaneously pose a challenging modeling endeavor. 

Developments in CFD have largely centered on reducing the computational complexity of attaining numerical solutions to fluid dynamics problems. Historically, major efforts have been directed towards progressing reduced-order models (ROMs) \cite{Raveh2004,Mannarino2015} and turbulence models \cite{Spalart2000,Hoffman2006,Defraeye2010}. The system complexity is decreased by modeling only a subset of dominant modes of the system, as in the case of ROMs, or the mean flow or large scales of the flow, in the case of Reynolds-Averaged Navier-Stokes (RANS) \cite{Gatski2001} or large eddy simulation (LES) \cite{Sagaut2006-ln} turbulence modeling respectively. Recently, more attention has been turned towards machine learning (ML) as a potential pathway for improving the cost of CFD methods \cite{Kutz2017}. Deep learning, for example, affords an opportunity to learn universal approximators that are highly efficient with hardware-acceleration. This flexibility opens up numerous avenues for novel model design and the integration of ML in CFD, with a variety of approaches explored in the past few years \cite{Vinuesa2022}.

The choice of modeling approach is inherently multi-objective, optimizing for accuracy, computational cost, and generalization. Purely data-driven approaches, such as some ROMs or ML surrogates, can achieve very low computational costs, but accuracy is entirely predicated on data the model has been tuned to, offering limited generalizability. Physics-driven modeling on the other hand, such as RANS or LES, is suitable for a wide range of flows at a cost that tends to scale with accuracy. There exists a middle ground, then, that has the potential to achieve the benefits of both ends of the spectrum. In CFD, this may take the form of a highly parameterized, data-driven, e.g. deep neural network (DNN), turbulence model, which acts as a correction to the underlying physics of the system \cite{Duraisamy2019}. The learning power of DNNs can enable much higher accuracy than conventional approaches at a similar cost. Since the governing equations are still solved with a traditional numerical scheme, some guarantees on the accuracy or physicality of the solution can be maintained, such as incompressibility or momentum conservation. However, pairing DNNs and partial differential equation (PDE) or CFD solvers is often nontrivial. CFD solvers have been in development far before the AI revolution, and thus the frameworks may be incompatible. DNN training typically relies on the use of automatic differentiation (AD), which is usually not available with existing CFD solvers. And while efforts have been made to develop robust CFD solvers in an AD-compatible language \cite{BEZGIN2022108527}, ultimately there exists a gap in literature for end-to-end training of turbulence models, especially with the use of unstructured meshes.

Thus far, differentiable turbulence, defined as the application of ML with differentiable CFD solvers through exploitation of the discrete adjoint method, has been confined to idealized flow configurations, often using uniform grids and simple numerical schemes \cite{Kochkov2021,List2022}. In this work, we seek to advance the prospect of differentiable turbulence by exploring its efficacy for a challenging flow, the backwards-facing step (BFS). We use a robust finite element PDE solver that enables use of the adjoint method for arbitrary grids and boundary conditions, developing a highly generalizable framework for integrating deep learning models into PDE solution schemes. The correction to the fluid motion physics is implemented as a multi-scale graph neural network (GNN), which estimates an instantaneous sub-grid scale (SGS) stress using a mixed LES model functional form. The learned SGS closure can generalize to higher Reynolds numbers and new geometry, achieving accuracy commensurate with a 10x more expensive conventional simulation. Our contributions include (1) developing a differentiable LES solution method compatible with unstructured meshes, and (2) demonstrating the technique to learn an adaptable closure for BFS flow using a GNN, significantly outperforming traditional closure models.

\section{Background and related work}

\subsection{Machine learning for fluids}

Given the nature and difficulties associated with fluid modeling, it is unsurprising that CFD has become a prime target for improvement via scientific machine learning methods \cite{Brunton2020}. The success of deep neural networks in particular has provided new strategies for interacting with flow data, for tasks such as regression, optimization, or classification. DNNs have demonstrated enormous potential to recognize and extract complex relationships in existing data, patterns which may be used for the prediction or control of new flows. Data-driven paradigms in fluid modeling have routinely been used for dimensionality reduction and reduced-order modeling, such as proper orthogonal decomposition (POD) \cite{Aubry1991} or dynamic mode decomposition (DMD) \cite{Schmid2010}; as a result, it has been a natural extension to consider enhancing or replacing these strategies with ML methods. The development of ROMs using ML was among the first efforts to tackle dynamical modeling of fluid systems with this new wave of data-driven learning \cite{Vinuesa2022}. Previous works have used deep learning architectures like convolutional networks (CNNs) \cite{Guo2016,Shankar2022} and recurrent networks (RNNs) \cite{Rajendran2018} to predict the spatiotemporal evolution of flow fields, or to learn nonlinear encodings of low dimensional flow features \cite{Milano2002}. More recent advances in ML architectures have also been applied to fluid modeling, such as using generative adversarial networks (GANs) for superresolution \cite{Xie2018}, long short term memory networks (LSTMs) for improved dynamic prediction \cite{mohan2018deep}, and GNNs for modeling with more general data discretizations \cite{Bleeg2020,pfaff2021learning}. Operator learning is another paradigm that has taken a foothold in scientific machine learning, where a discretization-independent PDE solution operator is learned from data \cite{kovachki2023neural}. Many of these DNN-based modeling strategies suffer from a fundamental drawback, which is that ML is ultimately interpolative. Without an interpretable framework, it can be difficult to determine if or how a model will fail when confronted with novel data. In contrast, the benefit of first-principles modeling is that there are stronger guarantees on generalizability, which is useful in a limited data regime. These modeling strategies are not incompatible, however, and ML can and has be used in conjunction with first-principles ideas. For example, DNNs have been used to improve near-wall modeling \cite{Bin2022}, genetic programming \cite{Schmidt2009} and sparse regression \cite{Loiseau2018} have discovered novel physics from data, and physical symmetries and constraints can be embedded into ML models using invariant and equivariant networks \cite{Berrone2022,suk2021equivariant}. Ultimately, we believe this coupling offers a balanced approach to modeling, embracing the AI revolution without eschewing the knowledge and success of more traditional methods.

\subsection{Data-driven closure modeling}

Turbulence closure modeling represents an active area of CFD research and presents an intriguing opportunity to engage ML methods. Closure models are typically partially data-driven by nature, offering a pathway to integrating ML with more fundamental first-principles knowledge. There has been significant interest in using DNNs for turbulence modeling in the past few years \cite{Beck2021,Duraisamy2019,Pandey2020}. Given their extensive use in engineering disciplines, RANS models have received considerable attention. ML has been used to identify uncertain model predictions \cite{Ling2015}, to augment existing turbulence models \cite{Singh2017,Wu2018}, and to correct discrepancies in the Reynolds stress with respect to high-fidelity simulations \cite{Ling2016,Jiang2021}. DNNs have also been used in LES, for predicting the sub-grid scale stress that encompasses the effect of unresolved scales on the resolved scales \cite{Zhou2019,Maulik2018,Stoffer2021,Wang2018}. 
Common in DNN-based modeling approaches is the use of multi-layer perceptron architectures, which predict the closure term locally using resolved quantities. However, non-local models such as CNNs have also been explored, showing improvement over these simpler architectures \cite{Guan2022,Subel2023}. Turbulence modeling with ML has often taken an \textit{a-priori} approach to learning, which means the closure models are trained prior to integration within a PDE solution scheme. This requires supervised data for training that must come from, typically, DNS of a ground truth flow configuration. Furthermore, performance in an \textit{a-posteriori} setting is not guaranteed by good \textit{a-priori} performance, as the solver introduces new artifacts, such as numerical errors and temporal effects, that cannot be captured during training \cite{Maulik2018}. These disadvantages have thus generally curbed the applicability of these approaches to modeling more canonical flows, where DNS is feasible.

\subsection{Differentiable turbulence}
Differentiable turbulence describes the use of ML to enhance CFD solution methods using a differentiable PDE solver. Differentiability allows for end-to-end training of ML models with backpropagation, which can lead to substantially improved \textit{a-posteriori} accuracy. Differentiable CFD solvers are not widespread, however, which has hindered investigation of this paradigm. Generally, differentiability can be realized via source-to-source transformations of existing codebases \cite{Bischof2008}, implementation of a solver in an AD framework, or exploitation of existing adjoint-capable PDE solvers \cite{Mitusch2019}. Several previous efforts have leveraged these methods for improving CFD schemes. Kochkov et al. accelerated DNS computations through learned interpolations of the numerical scheme \cite{Kochkov2021}. Ho et al. use the adjoint-enabled field inversion technique for improving turbulence modeling of separated flows \cite{Ho2021}. Several works have explored SGS modeling in LES using differentiable simulations \cite{Sirignano2020,MacArt2021,List2022}. Bezgin et al. have developed a differentiable CFD solver for two-phase flows in JAX \cite{BEZGIN2022108527}. These efforts have demonstrated the advantages of differentiable hybrid ML-physics approaches over both pure data-driven methods and \textit{a-priori} learning approaches. We aim to apply these techniques in a more complex setting, and to the best of the authors' knowledge, our work is the first to explore end-to-end learning of SGS closures using GNNs on an unstructured computational mesh.

\section{Methods}

\subsection{Governing equations}

We seek to approximate solutions to the incompressible Navier-Stokes equations, given in their non-dimensional form as:
\begin{align}
\frac{\partial \mathbf{u}}{\partial t}+\nabla\cdot(\mathbf{u}\otimes\mathbf{u})&=\frac{1}{Re}\nabla^2\mathbf{u}-\frac{1}{\rho}\nabla p+\mathbf{f} \\
\nabla\cdot\mathbf{u}&=0,
\label{eq:ns}
\end{align}
where $\mathbf{u}$ is the velocity vector, defined $\{ u,v\}$, $p$ is the pressure, $\rho$ is the density, $Re$ is the Reynolds number, and $\mathbf{f}$ represents any external forces.
The nonlinear nature of the convective term $\nabla\cdot(\mathbf{u}\otimes\mathbf{u})$ is a particularly problematic aspect of the system, resulting in chaotic solutions that are vastly multi-scale -- spanning several orders of magnitude in space and time. Accurate direct numerical simulation of the governing equations must resolve the smallest scales of the flow, which is infeasible with our current computational capabilities for all but simplified, idealized, canonical flows. The number of grid points required scales with $Re^{9/4}$ \cite{Pope2000}, which makes DNS for many high Reynolds' number flows intractable. Fortunately, it is often sufficient in scientific and engineering applications to obtain only coarse-grained approximations of the true solution fields. The vast majority of energy in the flow is contained within the large scales, and consequently often relevant quantities, e.g. lift or drag, are influenced mainly by the scales encompassed in coarse-grained approximations.

Coarse-grained solution fields, however, cannot be recovered simply by simulating the governing equations on a coarser computational mesh. This can be shown mathematically by modeling the coarse-graining operation as a low-pass filtering operation, denoted with an overbar $\overline{\boldsymbol{\;\cdot\;}}$. Filtering the governing equations in this manner leads to the introduction of additional terms in the equation, again due to the nonlinear convective term:
\begin{equation}
    \frac{\partial \overline{\mathbf{u}}}{\partial t}+\nabla\cdot(\overline{\mathbf{u}}\otimes\overline{\mathbf{u}})=\frac{1}{Re}\nabla^2\overline{\mathbf{u}}-\frac{1}{\rho}\nabla \overline{p}+\overline{\mathbf{f}} + \nabla\cdot \boldsymbol{\tau},
    \label{eqn:les}
\end{equation}
where $\boldsymbol{\tau}$ represents the effects of the unresolved velocity components on the resolved field and is defined as
\begin{equation}
    \tau_{ij} = \overline{u_iu_j}-\overline{u}_i\overline{u}_j.
    \label{eqn:t_dns}
\end{equation}
The additional terms are representative of the ``closure'' problem that has plagued numerical analysis of the Navier-Stokes equations for many years, and must be modeled to effectively solve these filtered equations. The equations above represent the filtered Navier-Stokes equations used in large eddy simulation, where the unknown term is treated as an additional stress, known as the sub-grid scale stress.

Numerous methods to model the SGS stress have been proposed over the last few decades, using various theoretical assumptions and empirical observations. The most well-known and perhaps widely used SGS model \cite{SMAGORINSKY1963} employs a linear eddy viscosity, which hypothesizes that the SGS stress is proportional to the filtered rate-of-strain in the fluid. The Smagorinsky model computes an effective eddy viscosity,
\begin{align}
    \nu_t &= (C_s \Delta)^2|\overline{\mathbf{S}}| \\
    \boldsymbol{\tau}^{EV} &= -2\nu_t\overline{\mathbf{S}},
    \label{eqn:smag}
\end{align}
where $C_s$ is a dimensionless empirical coefficient, $\Delta$ is a characteristic length scale, and $|\overline{\mathbf{S}}| = (2\overline{S}_{ij}\overline{S}_{ji})^{1/2}$ is the magnitude of the rate-of-strain tensor. The eddy viscosity assumption allows for a simple approximation of the SGS stress $\boldsymbol{\tau}^{EV}$, which is sufficient for certain flows. Further developments in SGS modeling over the years have incorporated additional nonlinear terms into the SGS stress to account for more complex flow phenomena \cite{Argyropoulos2015,Adams2002,Zang1993}. Our model uses this mixed model paradigm to compute a nonlinear SGS stress as
\begin{equation}
    \hat{\boldsymbol{\tau}} = \boldsymbol{\tau}^{EV} + \boldsymbol{\tau}^{NL},
    \label{eqn:sgs}
\end{equation}
where $\boldsymbol{\tau}^{EV}$ is computed from the Smagorinsky model and $\boldsymbol{\tau}^{NL}$ is a data-driven nonlinear contribution evaluated from a neural network.

\subsection{Graph neural network}

From the ansatz presented in the prior section, both the eddy viscosity and nonlinear contributions of the SGS stress can be estimated using a deep learning model. $\boldsymbol{\tau}^{EV}$ is a function of a single scalar coefficient $C_s$, while $\boldsymbol{\tau}^{NL}$ represents a rank-2 tensor whose components can be  arbitrarily learned. We set a few requirements to determine a choice of DNN architecture. We wish for $C_s$ and $\boldsymbol{\tau}^{NL}$ to be both spatiotemporally varying and functions of the instantaneous resolved velocity and pressure fields $\overline{\mathbf{u}}, \overline{p}$. In addition, it is understood that the SGS stress is non-local \cite{ClarkDiLeoni2021}, in that the stress at a point in space is influenced not only by the local flow, but also the surrounding flow as well. Lastly, the network architecture must be amenable to the choice of discretization of the coarse-grained resolved solution field, which, to be most generalizable, can be assumed to be an unstructured computational mesh. 

Based on these requirements, we propose to model the desired turbulence quantities $C_s$ and $\tau^{NL}_{ij}$ using a multi-scale graph neural network. GNNs have demonstrated a remarkable ability to model spatiotemporal data from many domains, including molecular dynamics \cite{Batzner2022}, fluid prediction \cite{Chen2021,Yang2022}, traffic forecasting \cite{Bui2021}, and structural mechanics \cite{pmlr-v119-sanchez-gonzalez20a}. Here, we use the Encode-Process-Decode paradigm that is commonly found in deep GNN models. We provide an overview of the GNN architecture in this section.

\subsubsection{Message-passing}
The GNN model is composed of sequential nonlinear message-passing layers that distribute information around the graph. We assume a graph with both edge features $\mathbf{h}_{ij}$ and node features $\mathbf{h}_{i}$. As with typical GNN schemes, we perform a residual update of the edge features $\mathbf{h}_{ij} \leftarrow \mathbf{h}_{ij} + \mathbf{h}_{ij}'$, aggregating the new edge features along edges before performing a residual node feature update $\mathbf{h}_{i} \leftarrow \mathbf{h}_{i} + \mathbf{h}_{i}'$.

The edge update is defined as follows:
\begin{align}
    \mathbf{v}_{ij} &= f_v(\mathbf{h}_{ij},\mathbf{h}_{i},\mathbf{h}_{j}) \\
    \mathbf{W}_{ij} &= f_w(||\mathbf{x}_{i}-\mathbf{x}_{j}||) \\
    \mathbf{h}_{ij}' &= \mathbf{v}_{ij} \mathbf{W}_{ij}^T , 
    \label{eq:edge}
\end{align}
where $\mathbf{v}_{ij}$ is a feature vector linearly transformed by the weight kernel $\mathbf{W}_{ij}$, which itself is a function of the magnitude of the relative edge position vector $r_{ij} = ||\mathbf{x}_{i}-\mathbf{x}_{j}||$. Both $f_v$ and $f_w$ are multi-layer perceptrons (MLPs) with one hidden layer and swish nonlinearities, which contain the trainable weights of the message-passing layer.

The node update is performed similarly:
\begin{align}
    \mathbf{h}_{i}' &= f_n\left(\frac{1}{|\mathcal{N}|}\sum_\mathcal{N}\mathbf{h}_{ij},\mathbf{h}_{i}\right),
\end{align}
where $\mathcal{N}$ represents a neighborhood of connectivity and $f_n$ is an additional trainable MLP. 

\subsubsection{Encode-Process-Decode}
The graph itself is generated in the encoder from three sets of information. First, the position vectors of each node in the computational mesh $\mathbf{x}_i$ define the point cloud on which to compute the required SGS model parameters. We specify two fixed node features: (1) a binary indicator of whether a node lies on a no-slip wall and (2) the characteristic length scale associated with the node $\Delta$, the same quantity used to compute the eddy viscosity contribution of the SGS stress. The wall-indicator is intended to assist with near wall modeling, as the SGS stress is strongly influenced by this boundary condition. Lastly, two spatiotemporally varying node features are computed and provided to the GNN as inputs: the local resolved velocity gradient tensor $\nabla\overline{\mathbf{u}}_i$ and the resolved pressure field $\overline{p}_i$. A basis transformation is first applied to the tensor-valued velocity gradient, to obtain the isotropic, symmetric-traceless, and skew-symmetric components of the tensor.

Edges are generated prior to each network evaluation from the node positions $\mathbf{x}_i$ using a radial cutoff. Next, the fixed and varying node features $\mathbf{f}_i$ are lifted to a higher dimensional latent space $\mathbf{h}_i$ using a linear projection. Edge features $\mathbf{h}_{ij}$ are generated similarly using linear projection of the relative positions $\mathbf{r}_{ij}$. One additional transformation via a message-passing layer completes the encoding procedure.

Once the graph is generated and all inputs are encoded into latent node and edge features, the processor performs successive message-passing of the latent features. Here, we choose a total of 3 message-passing layers within the processor. Each message-passing layer is followed by a batch normalization layer. Lastly, the processed node features are decoded to arrive at the target quantities for the SGS model. The decoder contains one message-passing layer and a linear projection to the outputs. We learn a total of 4 outputs in two-dimensions (2D) -- $C_s$ and 3 independent values of $\tau^{NL}_{ij}$, where $ij$ here indicates the Cartesian components of a symmetric 2D tensor. Specifically, the isotropic and symmetric-traceless components are predicted and summed to form $\tau^{NL}_{ij}$.

\subsubsection{Multi-scale message-passing} 
While the SGS closure term is most strongly influenced by small scale dynamics near the filter cutoff size, larger scale information can be useful to boost the accuracy of the SGS model. As with CNNs, the effective receptive field of a GNN can be expanded by chaining together more message-passing layers. Alternatively, the kernel size of the CNN or analogously radial cutoff of the GNN can also be increased. Both of these methods introduce a potentially substantial increase in compute or memory requirements, which is undesirable. Instead, a U-net approach \cite{ronneberger2015u} can be used to develop multi-scale features, which is similar to the multigrid methods \cite{Ruge1987} used in PDE solving.

The U-net approach requires pooling and subsequently unpooling layers that coarsen the data in space, allowing for long-range message-passing. Unlike uniform grids, however, generally graphs do not have a well-defined natural hierarchical structure that allows for straightforward coarsening. Several research efforts on graph representation learning have explored various graph coarsening or pooling strategies \citep{rethinkpooling2020,pmlr-v196-chen22a}. Proposed pooling methods have included voxel-based clustering \cite{lino2022}, which takes strong cues from multigrid methods in numerical analysis, or learned strategies such as top-k pooling \cite{graphUnets}, which allows the network to select which nodes should be included in the coarse graph.

Our coarsening module is uncomplicated and low-cost. We generate a random subgraph or coarse graph of the primary mesh using uniform sampling of the top-level graph nodes and features and a larger radial cutoff to produce edges. Latent edge features are computed from encoding the new relative position vectors using an MLP. We perform message-passing on the coarse graph only in the first message-passing layer. The coarse graph features are unpooled by distributing them to an empty feature matrix on the top-level graph. A node-wise MLP combines the top-level features and unpooled coarse features to compute new top-level features for the next layer. The next two message-passing layers use the top-level graph only.

\subsection{Implementation of the adjoint method}

The object of the differentiable turbulence paradigm is to pair the deep learning model with a differentiable solution algorithm for the governing equations, such that the parameters of the network can be trained in an end-to-end fashion. Automatic differentiation frameworks have become a necessary tool for training deep neural networks using gradient-based optimization. The AD principle relies on the fact that gradients of a composition of differentiable functions can be exactly computed using the chain rule. Most standard DNNs are composed of straightforward feed-forward operations, such as linear operations and pointwise nonlinearities, which can easily be written in high-level scientific programming languages. However, the integration of PDE solution algorithms, particularly using unstructured computational meshes, can add enormous complexity that makes writing these algorithms in an AD-enabled framework impractical. While the development of PDE solvers in AD-enabled languages is underway, there currently exists a plethora of efficient and robust non-differentiable PDE solvers that can interface with high-level languages and be called as external routines. If these can be integrated within an AD framework such as PyTorch, we can leverage the advantages of a differentiable solution scheme without rewriting complex PDE solvers from scratch.

Fortunately, AD toolchains provide a pathway to integrating this functionality through the use of custom gradients. In reverse-mode AD, we leverage the chain rule as follows:
\begin{align}
    x_1 &= f_0(x_0) \label{eqn:ch0}\\
    x_2 &= f_1(x_1) \label{eqn:ch1}\\
    &\vdots \notag \\
    y &= f_n(x_n) \label{eqn:chn}\\ 
    \frac{dy}{dx_i} &= \frac{dy}{dx_{i+1}}\frac{dx_{i+1}}{dx_i} = \frac{dy}{dx_{i+1}}\frac{df_i(x_i)}{dx_i} .\label{eqn:chain}
\end{align}
The iterative nature of eqn. \ref{eqn:chain} allows gradients of $y$ to be propagated backwards starting from $\frac{dy}{dx_n}$. For each step in the forward pass, eqn. \ref{eqn:ch0}-\ref{eqn:chn}, a corresponding reverse function should be implemented, also known as a vector-jacobian product (VJP):
\begin{equation}
    \frac{dy}{dx_i} = f_i'\left(\frac{dy}{dx_{i+1}},x_i\right).
\end{equation}
The backpropagation of gradients then constitutes a composition of these VJP functions. In an AD framework, VJPs are supplied for the majority of $f_i$'s that are encountered, e.g. addition, multiplication, and other simple operations. However, if $f_i$ represents a call to an external routine not known by the framework, a custom VJP must be supplied.

To examine the necessary custom gradients that must be implemented, we first look at an outline of the implicit pressure correction scheme (IPCS) for solving the unsteady Navier-Stokes equations, shown in Algorithm \ref{alg:ipcs}.

\algnewcommand{\Initialize}[1]{%
  \State \textbf{Initialize:}
  \Statex \hspace*{\algorithmicindent}\parbox[t]{.8\linewidth}{\raggedright #1}
}
\begin{algorithm}
\caption{IPCS method for N-S}
\label{alg:ipcs}
\begin{algorithmic}
\Initialize{\texttt{GNN}$_\theta$\\
    \texttt{tentative\_vel}\\
    \texttt{pres\_correct}\\
    \texttt{vel\_correct}\\
    $\mathbf{u},p$\\
    $t \gets 0$}
\While{$t<T$}
\State $\alpha_{\boldsymbol{\tau}} \gets \texttt{GNN}_\theta(\mathbf{u},p)$
\State $\mathbf{u}^* \gets \texttt{tentative\_vel}(\mathbf{u},p,\alpha_{\boldsymbol{\tau}})$
\State $p^* \gets \texttt{pres\_correct}(\mathbf{u}^*,p)$
\State $\mathbf{u} \gets \texttt{vel\_correct}(\mathbf{u}^*,p^*,p)$
\State $p \gets p^*$
\State $t \gets t + \Delta t$
\EndWhile
\end{algorithmic}
\end{algorithm}

The algorithm is initialized from a choice of discretization and flow configuration. At each timestep, first the GNN is used to predict the turbulence model parameters, denoted $\alpha_{\boldsymbol{\tau}}$. Since the GNN is written in the AD framework, no further work is required to enable gradient backpropagation. The next three functions, \texttt{tentative\_vel}, \texttt{pres\_correct}, and \texttt{vel\_correct}, represent solutions to three PDEs that are obtained from external routines outside of the AD framework. For this reason, we must implement a custom VJP for each of the PDE solution steps.

The PDE solves can be written generally as:
\begin{equation}
    \mathbf{x} = \mathrm{\texttt{PDE\_solve}}(\mathbf{m}),
\end{equation}
where $\mathbf{m}$ represents any time-varying inputs to the solve. A custom backwards gradient would implement:
\begin{equation}
    y_{\mathbf{m}} = \mathrm{\texttt{PDE\_solve\_vjp}}\left(y_{\mathbf{x}}, \mathbf{m}\right),
\end{equation}
where we indicate partial derivatives with a subscript.
The forward \texttt{PDE\_solve} can be further expanded as the solution to the linear system:
\begin{equation}
    \mathbf{A}(\mathbf{m})\mathbf{x} = \mathbf{b}(\mathbf{m}),
\end{equation}
where $\mathbf{A}$ and $\mathbf{b}$ represent the discretized left hand side (LHS) and right hand side (RHS) of the PDE respectively, which in general may be functions of the input $\mathbf{m}$. Given this description, we can implement \texttt{PDE\_solve\_vjp} using the discrete adjoint method. The discrete adjoint method defines
\begin{equation}
    y_{\mathbf{m}} = -\boldsymbol{\lambda}^T\left(\mathbf{A}_\mathbf{m} - \mathbf{b}_\mathbf{m}\right),
\end{equation}
where $\boldsymbol{\lambda}$ is the solution to the adjoint equation given by
\begin{equation}
    \mathbf{A}^T\boldsymbol{\lambda} = y_{\mathbf{x}}^T.
\end{equation}
A PDE solver that exposes $\mathbf{A}$, as well as enables, importantly, the computation of $\mathbf{A}_\mathbf{m}$ and $\mathbf{b}_\mathbf{m}$, offers sufficient functionality to implement \texttt{PDE\_solve\_vjp}.

One option for a solver that matches this criteria is the FEniCS ecosystem \cite{fenics}. FEniCS offers a general-purpose PDE solving framework using the finite element method. FEniCS allows the ability to define PDEs in near mathematical notation and additionally includes some automatic differentiation functionality that is useful for computing $\mathbf{A}_\mathbf{m}$ and $\mathbf{b}_\mathbf{m}$. PDEs are defined using their variational form. For example, a Poisson system can be defined as:
\begin{python}
    F = inner(nu*grad(u), grad(v))*dx
\end{python}
where \texttt{u} is a \texttt{TrialFunction}, \texttt{v} is a \texttt{TestFunction}, and \texttt{nu} is a given \texttt{Function}. The LHS $\mathbf{A}$ and RHS $\mathbf{b}$ of the discretized linear system can be produced by calling the \texttt{assemble} routine on \texttt{F}. To compute $(\mathbf{A}-\mathbf{b})_{\mathrm{\texttt{nu}}}$, one need only \texttt{assemble} a new form: 
\begin{python}
    F_nu = derivative(F, nu)
\end{python}
With these procedures in hand, a custom VJP for \texttt{PDE\_solve} can be supplied, described in Algorithm \ref{alg:adj}.

\begin{algorithm}
\caption{Custom gradient for PDE solve}
\label{alg:adj}
\begin{algorithmic}
\Function{\texttt{PDE\_solve}}{$\mathbf{m};$ F} 
\State $\mathbf{A},\mathbf{b} =$ assemble(F(\textbf{m})) \Comment{LHS/RHS of form F}
\State $\mathbf{x} =$ solve($\mathbf{A},\mathbf{b}$) \Comment{Linear solve}
\State \textbf{return x}
\EndFunction
\Function{\texttt{PDE\_solve\_vjp}}{$y_\mathbf{x},\mathbf{m};$ F} 
\State $\boldsymbol{\lambda} =$ solve($\mathbf{A}^T,y_\mathbf{x}^T$) \Comment{Adjoint solution}
\State $F_\mathbf{m} =$ derivative(F(\textbf{m}; \textbf{x}), \textbf{m}) \Comment{Residual gradient}
\State $y_{\mathbf{m}} =$ assemble($-\boldsymbol{\lambda}^TF_\mathbf{m}$) \Comment{Assembled gradient}
\State \textbf{return} $y_{\mathbf{m}}$
\EndFunction
\end{algorithmic}
\end{algorithm}

Note that for brevity, we have included only the major steps of the procedure and removed many of the specific details of the implementation, such as conversion of PyTorch tensors to FEniCS variables, the application of boundary conditions, and certain FEniCS syntax. With \texttt{PDE\_solve\_vjp} implemented, \texttt{PDE\_solve} can be used as any other differentiable PyTorch layer, and the entire IPCS algorithm can be written using PyTorch variables, which natively record the gradient tape required for automatic backwards differentiation.


\section{Datasets and training}

Flow over a backwards-facing step (BFS) is a common benchmark problem in fluid dynamics that has been widely studied in both experimental \cite{armaly1983experimental} and computational \cite{wee2004self} settings. BFS flow is characterized by flow separation at the step and a subsequent reattachment region further downstream, which introduces a recirculation region near the step and transient vortex shedding. These phenomena are found frequently in more complex non-idealized flows and can be challenging to model accurately, making the BFS a valuable case study for evaluating modeling approaches. The BFS configuration can be described as flow in a channel with a sudden expansion. An inlet is prescribed on the left with a uniform velocity in the x-direction, entering a channel of fixed width. Once the flow has fully developed, the channel width is increased, creating a step at the lower wall. This expansion produces a shear layer at the step height, which leads to turbulent mixing and vortex development. The flow reattaches downstream and eventually exits the domain through a pressure outlet.

\def\arraystretch{1.2}
\begin{table}[h]
\caption{Train and test cases}
\begin{tabular}{c|c|c|c}
\textbf{Case}    & $h (m)$    & $U_\infty (m/s)$ & $Re$   \\ \hline
Train 0 & 0.0127 & 32         & 26,051 \\
Train 1 & 0.0147 & 40         & 37,692 \\
Test 0  & 0.0127 & 56         & 45,590 \\
Test 1  & 0.0137 & 48         & 42,154
\end{tabular}
\label{tab:data}
\end{table}

% Figure environment removed

The step height $h$ represents an important characteristic length scale in the flow, controlling flow features such as vortex diameter and the size of the recirculation region. Along with the inlet velocity $U_\infty$ and kinematic viscosity of the fluid, the Reynolds number can be specified. We parameterize the BFS configuration using the inlet velocity and step height. A total of four configurations are used for training and testing the model, tabulated in Tab. \ref{tab:data}. Two cases are used to train the model while the other two are used as generalization tests. A schematic of the domain is presented in Fig. \ref{fig:dom}(a).

Ground truth data are taken from a high-resolution LES simulation of each case. We use an unstructured triangular mesh of on average 80,000 Taylor-Hood elements, which has been refined near all walls and particularly around and downstream from the step. The highest density of elements resides in the shear layer at the step, with generally decreasing density further away. The LES equations using the standard Smagorinsky turbulence model with $C_s=0.16$ are solved using an implicit pressure correction scheme and Crank-Nicolson timestepping. We additionally use streamline upwind Petrovâ€“Galerkin (SUPG) and least square incompressibility constraint (LSIC) stabilization due to the finite element implementation. For consistency, we fix the ground truth timestep $\Delta t$ to $5\times10^{-6}$ s for all cases, which was sufficient to keep the Courant-Friedrichs-Lewy (CFL) number approximately equal to or less than 1, where $\mathrm{CFL}<1$ is not strictly required due to the implicit timestepping. Snapshots of the velocity and pressure fields were saved every $10^{-4}$ s, and a total of 2048 snapshots were generated for each case. We denote results from this ground truth simulation as ``LES-80k''.

We generate a significantly coarser mesh for each geometry, averaging 5,000 elements, on which we apply our ML-based closure and evaluate baseline comparisons. Fig. \ref{fig:dom}(b) shows a portion of this mesh in the area around the step. The scaling factor for element sizes was set to be $\approx 4.5\times$ the size of the high-resolution mesh. Simulations on this coarse grid use nearly the same solution scheme as the ground truth, except the SUPG stabilization is removed and the timestep size is increased to $10^{-4}$ s. We test two baseline comparisons on this grid, no model (``NM-5k''), where no closure model is used, and the standard Smagorinsky closure (``LES-5k''), where we set $C_s=0.06$ based on optimizing \textit{a-posteriori} results with respect to the ground truth. The ML-closure is denoted ``ML-5k''.

A third mesh is also created for each geometry, with size in between the coarse ML mesh and fine ground truth mesh, averaging 20,000 elements. The third mesh represents an additional baseline comparison (``LES-20k'') for the model with respect to a more well-resolved LES simulation. The Smagorinsky model is used with $C_s=0.07$, and the timestep is reduced to $5\times10^{-5}$ s.

We optimize the ML model by minimizing the $L^2$ norm of the resulting \textit{a-posteriori} velocity field with respect to the ground truth velocity field. During training, a predicted solution trajectory $\hat{\mathbf{u}}$ is obtained by simulating the LES equations using the GNN closure. Practically, we achieve temporal stability of the model by evolving a fixed number of timesteps given an initial velocity and pressure field $\overline{\mathbf{u}}_0, \overline{p}_0$. Based on our own empirical analysis and suggestions from previous work, we set the training simulation to be 32 timesteps. 

The target field $\overline{\mathbf{u}}_0..\overline{\mathbf{u}}_n$ is generated by interpolating a ground truth velocity trajectory $\mathbf{u}_0..\mathbf{u}_n$ onto the coarse mesh used in the ML-LES simulation. The pressure field is also interpolated for the initial condition. The initial condition is integrated over 32 steps using the GNN-based turbulence model. The overall loss is computed as
\begin{equation}
    L = \frac{1}{T}\sum_{t=0}^T \int_\Omega ||\overline{\mathbf{u}}_t - \hat{\mathbf{u}}_t||_2^2 \; d\Omega,
\end{equation}
where the integral is computed using the FEM discretization. Since the integral is computed in FEniCS, an adjoint can be obtained using the same method described previously. The loss is backpropagated in an end-to-end fashion, through all PDE evaluations and timesteps, to update the parameters of the GNN.

The training dataset was generated using only the first 256 snapshots of each of the two training cases. These set of snapshots were split into data samples by extracting multiple trajectories of 32 consecutive snapshots, with initial conditions separated by 16 timesteps. Ultimately, this led to a total of 30 trajectories for training, a surprisingly low number that is enabled by the deep integration of ML and physics. Likely due to this small dataset size, models required very little training to converge. The model used for testing was trained for 50 epochs using the Adagrad optimizer \cite{adagrad} with a learning rate of $10^{-3}$. Stochastic weight averaging was used for the last 10 epochs for improved generalization. Training took about 6 hours on 2 NVIDIA A100 GPUs and 2 CPU-cores. Much of the inefficiency in the model can be attributed to data transfer between the CPU and GPU, where the CPU is used to solve the PDE and the GPU is used to evaluate the GNN. 


\section{Results and discussion}
We evaluate the performance of the hybrid ML-LES solution algorithm by examining several \textit{a-posteriori} metrics from the flow. For each generalization case, the trained ML closure is used in a simulation of the flow beginning with zero initial conditions. The solution is integrated over a total of 1024 timesteps (we remind the reader that error computation and backpropagation during training are performed over the first 256 timesteps in batches of 32). We analyze the efficacy of the model with respect to several baselines: ``NM-5k'', no closure on the coarsest mesh, ``LES-5k'', Smagorinsky model on the coarsest mesh, and ``LES-20k'', Smagorinsky model on a finer mesh. 

For all predicted velocity fields, we compute the normalized time-varying $L^2$ error with respect to the ground truth velocity, given by:
\begin{equation}
    \frac{\int_\Omega ||\overline{\mathbf{u}}_t - \hat{\mathbf{u}}_t||_2^2 \;d\Omega}{U_\infty^2},
\end{equation}
where $U_\infty^2$ is the inlet velocity. We compute ensemble average and statistics by averaging over multiple initial conditions taken from the ground truth trajectory, within the first 1024 timesteps. While this provides a good summary metric to check for qualitative fit of the model, the complexity of the problem and nuances in the flow necessitate the use of more in depth measures of accuracy. To examine the transient behavior of the flow, we show the evolution of the coefficients of the first 3 POD modes of the flow. This allows visualization of the periodic vortex shedding on a reduced set of coordinates. Additionally, we provide the velocity sampled from a probe in front of the step, at the location $(h,h)$. For these transient plots, we report values from the first 512 timesteps as all models generally diverge from the ground truth and thus pointwise accuracy at later times is not expected. We also look at several time-averaged statistics of the the flow, which are often the most relevant metrics of interest. We show the time-averaged x-velocity field $U_x$ along with the error field, to illustrate where in the flow model discrepancies lie. We plot a velocity profile downstream of the step to ensure the model can accurate capture the recirculation region, as well as the pressure coefficient on the lower wall, computed as:
\begin{equation}
    C_p = \frac{p-p_\infty}{\frac{1}{2}\rho U_\infty^2},
\end{equation}
where $p_\infty$ is the pressure far from the wall, which we take as the midpoint of the channel. These series of \textit{a-posteriori} metrics provide a holistic measure of model performance that ensures that the ground truth flow is being accurately predicted.

\subsection{Generalization to higher Reynolds number}

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

We first examine the results from Test case 0. Since the step height $h$, and therefore the computational mesh are the same as is used in Train 0, this test specifically probes the ability of the model to extrapolate to higher Reynolds number flows. Fig. \ref{fig:err56} shows the normalized $L^2$ error over time for the four models tested. We observe that all models display an initial growth region in the error that then stabilizes or fluctuates around a generally constant value. The stronger periodicity observed in LES-5k and LES-20k can be attributed to slight discrepancies in the predicted vortex shedding frequency, where lower error is attained when phases are aligned and vice versa. NM-5k shows the steepest growth and highest overall error, indicating the need for a closure model. LES-5k displays a smaller initial slope and fluctuates around a significantly lower error than NM-5k, while LES-20k results in the smallest initial slope and similar stationary error as LES-5k. We see that the learned model ML-5k generally results in the lowest overall $L^2$ error, and given that this metric is reflective of the training cost function, we determine that the training process is functioning as intended to minimize the $L^2$ error.

The transient $L^2$ error, however, is just one metric we should use to evaluate model performance. To further examine the transient behavior of the model, we can look at the evolution of the POD mode coefficients of the flow, illustrated in Fig. \ref{fig:pod56}. Here, we can clearly see the periodic vortex shedding that is caused by the shear layer dynamics. The chaotic nature of the system means that small perturbations, i.e. the choice of grid or closure model, can influence the predicted shedding frequency. Fig. \ref{fig:pod56} shows that both NM-5k and LES-5k significantly deviate from the ground truth trajectory. On the other hand, LES-20k and ML-5k are much more closely aligned with the truth. LES-20k appears to match the true field best, with only slight deviations growing over time. ML-5k does a very reasonable job staying correlated with the LES-80k, but ultimately the predicted shedding frequency is greater than the true frequency. Fig. \ref{fig:probe56} illustrates pointwise accuracy of the models from a probe at $x=h,y=h$. Similar trends are observed, in that LES-20k and ML-5k demonstrate good fit with the ground truth initially, while NM-5k and LES-5k show more deviation.

The mean flow is a particularly useful measure that decouples the transient effects of the model from flow features of interest. Fig. \ref{fig:mean56} depicts the normalized x-velocity of the flow and the resulting error field with respect to LES-80k. We also report the integrated total error for each model. We can observe the recirculation region characterized by the negative (blue) x-velocity that is present in each model. NM-5k contains discrepancies throughout the portion of the domain shown, including prior to the step, in front of the step, and at the boundary layer. LES-5k reduces the error by a factor of over 2, but still contains deviations, notably at the boundary later. Both LES-20k and ML-5k achieve similar error, reducing the error relative to LES-5k again by a factor of 2, with deviations mainly contained within the region in front of the step.

Lastly we look at two profiles from the mean flow. Fig. \ref{fig:prof56}(a) depicts the normalized x-velocity profile of the flow as a function of normalized channel height $y/h$, at a location in front of the step $x/h=1$. We see that the near-wall region is well captured by ML-5k, and that both LES-20k and ML-5k improve overall relative to the other models. Fig. \ref{fig:prof56}(b) shows the pressure coefficient on the lower wall, where we see a negative pressure coefficient in the recirculation region and positive coefficient as the flow reattaches. Again, both LES-20k and ML-5k achieve similar profiles and show good agreement with LES-80k, albeit slightly overpredicting the pressure coefficient at reattachment. 

\subsection{Generalization to new geometry}

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

Test case 1 contains BFS flow with both an unseen inlet velocity and unseen step height, making it a more challenging generalization test as the grid has not been used during training. Fig. \ref{fig:err48} shows the time-varying $L^2$ error for Test 1, and we observe similar trends between models as Test 0. ML-5k results in a larger error than Test 0, but ultimately achieves generally the lowest loss among models tested.

The POD mode coefficients are shown in Fig. \ref{fig:pod48}, where again similar behavior appears as in Test 0. LES-5k results in larger values for mode 0 than the truth, along with a lower shedding frequency. LES-20k and ML-5k show much better agreement with LES-80k, though naturally the shedding frequency for both are not perfectly predicted. Fig. \ref{fig:probe48} further supports these results, showing the velocity from a probe at $x=h,y=h$. We observe good fit with the ground truth for around 3 shedding cycles before discrepancies in the predicted frequency become more apparent.

Fig. \ref{fig:mean48} shows the mean flow, which qualitatively appears similar to Test 0, which is understandable as the Reynolds numbers are similar. NM-5k again reports the highest error with LES-5k at around half the value. For this case, we find more of a discrepancy in the total error between LES-20k and ML-5k, with LES-20k exhibiting the lowest error. We note that this is in part due to the improved performance of LES-20k on this case, as the ML-5k error is similar to that from Test 0.

Finally, the velocity profile and pressure coefficient of the mean flow are presented in Fig. \ref{fig:prof48}(a) and Fig. \ref{fig:prof48}(b) respectively. LES-20k and ML-5k show the best agreement with the ground truth, and in fact ML-5k appears to improve over LES-20k in capturing the pressure coefficient.

\def\arraystretch{1.3}
\begin{table}[]
\caption{Core/GPU-s per $10^{-4}$ s of physical time in simulation}
\begin{tabular}{l|l}
\textbf{Model}    & \textbf{Cost} \\ \hline
LES-80k & 177 s \\
LES-20k & 7.63 s \\
LES-5k & 0.557 s \\
ML-5k & 0.711 s \\
\end{tabular}
\label{tab:time}
\end{table}

A comprehensive examination of the metrics presented for both test cases allow us to conclude that the ML-5k model, using our learned GNN-based closure, is capable of matching or in some cases exceeding the accuracy of the more expensive LES-20k in a variety of measures. We compute an effective speedup by measuring the evaluation time of models, which we tabulate in Tab. \ref{tab:time}. We are able to achieve a reduction in cost of approximately a factor of 10, which we believe demonstrates the impressive potential of differentiable turbulence modeling approaches.

\section{Conclusions}

This study investigates the paradigm of differentiable turbulence, in which a data-driven ML model is embedded within a differentiable solution algorithm for the Navier-Stokes equations to augment the numerical scheme. Differentiability of the solver enables a loss function to be computed from the \textit{a-posteriori} solution field, which opens new pathways for training models in ML for CFD, such as optimizing a cost function to match a velocity profile obtained from experimental data. In this work, we embed a GNN within the solver to learn a turbulence closure model for the flow, solving the governing LES equations on a significantly coarser grid than would otherwise be required for baseline closure models. This approach aims to shift the resolution requirements for scale resolving simulations, such that the balance of scales that are resolved vs. modeled are tipped in favor of increased computational efficiency. This hybrid approach to data-driven learning strikes a balance between the benefits of pure deep learning approaches to flow modeling, such as low computational cost, and classical numerical solution schemes, such as satisfaction of physical constraints and generalizability to a wide range of flows. While there are compromises with respect to each, in that our approach is more costly than a pure DNN and not as accurate as an expensive highly resolved simulation, in many cases this middle ground is sufficient for modeling needs.

Our closure modeling strategy uses a functional form for the SGS stress that consists of an eddy viscosity contribution and a nonlinear contribution, both of which are learned using a GNN. We use a multi-scale Encode-Process-Decode GNN to approximate the discretized field of closure model parameters at each timestep. The GNN is a function of the pressure field and the velocity gradient tensor. This information is propagated through the network to output a linear eddy viscosity coefficient, and a rank-2 tensor that represents any nonlinear contributions to the SGS stress. We embed the closure model into a finite element PDE solver using the FEniCS ecosystem. The governing equations are evolved in time with an IPCS algorithm written in PyTorch to enable automatic differentiation. We provide a custom gradient that solves the discrete adjoint problem for each PDE solve, leveraging the functionality of the FEniCS environment. During training, the $L^2$ error of the \textit{a-posteriori} velocity field with respect to a ground truth velocity field is minimized end-to-end over multiple timesteps of integration.

We test our modeling approach on 2D flow over a backwards-facing step, investigating generalization to both higher Reynolds number and different step height. We present a variety of statistical and other metrics, such as the $L^2$ error, POD mode coefficients, and velocity and pressure profiles of the mean flow, to compare the performance of the ML-based turbulence model with other baseline simulation methods, including a higher resolution LES simulation. We find that the ML-based closure model is consistently able to achieve agreement on par with the higher resolution simulation, equaling an effective order of magnitude reduction in computational cost. Although our model has limitations, in that we would not expect it to generalize to entirely different flows, we believe our work represents an important milestone in the development of data-driven closure models using differentiable simulations. Given the flexibility of the framework to solve flows in arbitrary geometries, we anticipate that future development will tackle more complex three-dimensional flow problems.

\section{Acknowledgements}
This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No. DGE 1745016 awarded to VS.  The authors from CMU acknowledge the support from the Technologies for Safe and Efficient Transportation University Transportation Center, and Mobility21, A United States Department of Transportation National University Transportation Center. 
This work was supported by the U.S. Department of Energy (DOE), Office of Science, Office of Advanced Scientific Computing Research (ASCR), under Contract No. DEAC02â€“06CH11357, at Argonne National Laboratory. We also acknowledge funding support from ASCR for DOE-FOA-2493 ``Data-intensive scientific machine learning''



% \clearpage
% \nocite{*}
\bibliography{bib}

\end{document}
%
% ****** End of file aipsamp.tex ******