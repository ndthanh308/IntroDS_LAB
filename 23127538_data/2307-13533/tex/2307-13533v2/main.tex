\documentclass[preprint]{elsarticle}

\usepackage[utf8]{inputenc}
\bibliographystyle{elsarticle-num}
\usepackage{xcolor}
% \usepackage{lineno}
\usepackage[title]{appendix}
% \linenumbers


% Set page size and margins
% Replace `letterpaper' with`a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=2cm,right=2cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{palatino,graphicx,wrapfig}
\usepackage[small,it]{caption}
\usepackage{amsmath}
\usepackage{amssymb,bm}
\usepackage{parskip}

\usepackage{multirow}
\usepackage{algorithm,algpseudocode}
%\usepackage{algorithmic}
%\usepackage{program}
\usepackage{pythonhighlight}

\journal{}

\title{Generalizable data-driven turbulence closure modeling on unstructured grids with differentiable physics}

\author[label1]{Hojin Kim}
\author[label2]{Varun Shankar}
\author[label3]{Venkatasubramanian Viswanathan}
\author[label1,label4]{Romit Maulik}

\affiliation[label1]{organization={College of Information Sciences and Technology, Pennsylvania State University},
            % addressline={},
            city={University Park},
            postcode={16802},
            state={PA},
            country={USA}}
% \email{hojin.kim@psu.edu}


\affiliation[label2]{organization={Mechanical Engineering Department, Carnegie Mellon University},
            % addressline={},
            city={Pittsburgh},
            postcode={15213},
            state={PA},
            country={USA}}

\affiliation[label3]{organization={Department of Aerospace Engineering, University of Michigan},
            % addressline={},
            city={Ann Arbor},
            postcode={48109},
            state={MI},
            country={USA}}

\affiliation[label4]{organization={Mathematics and Computer Science Division, Argonne National Laboratory},
            % addressline={},
            city={Lemont},
            postcode={60439},
            state={IL},
            country={USA}}



\date{\today}

\begin{document}

\begin{abstract}
Differentiable physical simulators are proving to be valuable tools for developing data-driven models in computational fluid dynamics (CFD). These simulators enable end-to-end training of machine learning (ML) models embedded within CFD solvers. This paradigm enables novel algorithms which combine the generalization power and low cost of physics-based simulations with the flexibility and automation of deep learning methods.  In this study, we introduce a framework for embedding deep learning models within a generic finite element solver to solve the Navier-Stokes equations, specifically applying this approach to learn a subgrid scale closure with a graph neural network (GNN). We validate our method for flow over a backwards-facing step and test its performance on novel geometries, demonstrating the ability to generalize to novel geometries without sacrificing stability. Additionally, we show that our GNN-based closure model may be learned in a data-limited scenario by interpreting closure modeling as a solver-constrained optimization. Our end-to-end learning paradigm demonstrates a viable pathway for physically consistent and generalizable data-driven closure modeling across complex geometries.
\end{abstract}

\begin{keyword}
Graph neural networks \sep Differentiable turbulence \sep a-posteriori learning \sep Fluid dynamics \sep Generalization
\end{keyword}



\maketitle

\section{Introduction}
\label{sec:Introduction}


Accurate computational modeling of turbulent flows represents a critical requirement for the analysis of physical systems in a variety of engineering and scientific disciplines \cite{anderson1995computational}. Although the equations of fluid motion are well-understood, accurate numerical simulation of the governing Navier-Stokes equations with computational fluid dynamics (CFD) methods remains a formidable task \cite{Tennekes2018-mf} when significant turbulent behavior is observed. In such cases, solutions to the Navier-Stokes equations are characterized by chaotic and multi-scale behavior, which makes resolving the entirety of the flow field through direct numerical simulation (DNS) often impractical \cite{Moin1998}. The number of grid points required for DNS can be shown to scale with $Re^{9/4}$, where the Reynolds number $Re$ represents the ratio of inertial to viscous forces in the flow \cite{Pope2000}. High Reynolds number flows are often of practical interest, but simultaneously pose a challenging modeling endeavor. 

Developments in CFD have largely centered on reducing the computational complexity of attaining numerical solutions to computationally expensive flow configurations. Historically, major efforts have been directed towards progressing reduced-order models (ROMs) \cite{Raveh2004,Mannarino2015} and turbulence closure models \cite{Spalart2000,Hoffman2006,Defraeye2010}. In such cases, the system complexity is decreased by modeling only a subset of the dominant modes of the system, as in the case of ROMs, or the mean flow or large scales of the flow, in the case of Reynolds-Averaged Navier-Stokes (RANS) \cite{Gatski2001} or large eddy simulation (LES) \cite{Sagaut2006-ln} respectively. Recently, more attention has been turned towards machine learning (ML) as a potential pathway for reducing the cost of CFD methods \cite{Kutz2017}. The success of deep neural networks (DNNs), in particular, has provided new strategies for interacting with flow data, for tasks such as regression, optimization, or classification. DNNs have demonstrated enormous potential to recognize and extract complex relationships in existing data, patterns which may be used for the prediction or control of new flows. This flexibility opens up numerous avenues for novel model design and the integration of ML in CFD, with a variety of approaches explored in the past few years \cite{Vinuesa2022}. The development of ROMs using ML was among the first efforts to tackle dynamical modeling of fluid systems with this new wave of data-driven learning \cite{Vinuesa2022}. Previous works have used deep learning architectures like convolutional neural networks (CNNs) \cite{Guo2016,Shankar2022} and recurrent neural networks (RNNs) \cite{Rajendran2018} to predict the spatiotemporal evolution of flow fields, or to learn nonlinear encodings of low dimensional flow features \cite{Milano2002}. More recent advances in ML architectures have also been applied to fluid modeling, such as using generative adversarial networks (GANs) for super-resolution \cite{Xie2018}, long short-term memory networks (LSTMs) for improved dynamic prediction \cite{mohan2018deep}.
Notably, graph neural networks (GNNs) have demonstrated a remarkable ability to model spatiotemporal data for fluid-flow prediction tasks \cite{Chen2021,pfaff2021learning, Yang2022, barwey2023multiscale, barwey2025Interpretable}, in addition to applications from different domains including molecular dynamics \cite{Batzner2022}, traffic forecasting \cite{Bui2021}, and structural mechanics \cite{pmlr-v119-sanchez-gonzalez20a}. Specifically, GNNs have exhibited their capability to generalize to out-of-sample datasets (for example different flow geometries) due to their ability to perfectly represent nodes and edges in unstructured meshes, and their formulation which identifies local and non-local correlations across an unstructured dataset.


Many of these DNN-based modeling strategies, however, suffer from a fundamental drawback, which is that ML is ultimately interpolative and can fail catastrophically on out-of-distribution tasks. Without an interpretable framework, it can be difficult to determine if or how a model will fail when confronted with novel data. In contrast, hybrid learning approaches combine first-principles modeling built on fundamental physical assumptions with ML. By training ML models under more guaranteed physics constraints, hybrid methods can secure stronger guarantees on generalizability and stability. For example, physical symmetries and constraints can be embedded into ML models using invariant and equivariant networks \cite{Berrone2022,suk2021equivariant}. Turbulence closure modeling with ML represents one example of hybrid learning for CFD research. Closure models are typically partially data-driven by nature, offering a pathway to integrating ML with more fundamental first-principles knowledge. More importantly, closure models merely augment known governing laws (here the Navier Stokes equations) which must continue to be solved to machine precision albeit on coarser spatial and temporal length scales. 

There has been significant interest in using DNNs for turbulence modeling in the past few years \cite{Beck2021,Duraisamy2019,Pandey2020}. Given their extensive use in engineering disciplines, RANS models have received considerable attention. ML has been used to identify uncertain model predictions \cite{Ling2015}, to augment existing turbulence models \cite{Singh2017,Wu2018, heo2024simulation}, and to correct discrepancies in the Reynolds stress with respect to high-fidelity simulations \cite{Ling2016,Jiang2021}. DNNs have also been used in LES, for predicting the sub-grid scale stress that encompasses the effect of unresolved scales on the resolved scales \cite{Zhou2019,Maulik2018,Stoffer2021,Wang2018}. Common in DNN-based modeling approaches for sub-grid scale closure is the use of multi-layer perceptron architectures, which predict the closure term locally using resolved quantities. However, non-local models such as CNNs have also been explored, showing improvement over these simpler architectures \cite{Guan2022,Subel2023}. Turbulence modeling with ML has often taken an \textit{a-priori} approach to learning, which means the closure models are trained prior to integration within a PDE solution scheme. This requires supervised data for training that must come from, typically, DNS of a ground truth flow configuration. Furthermore, performance in an \textit{a-posteriori} setting is not guaranteed by good \textit{a-priori} performance, as the solver introduces new artifacts, such as numerical errors and temporal effects, that cannot be captured during training \cite{Maulik2018}. Even worse, solutions from turbulence modeling can become unstable, and these issues may arise despite achieving low \textit{a-priori} errors, which is due to a problem called model-data inconsistency \cite{sanderse2024review}. These disadvantages have thus generally curbed the applicability of these approaches to modeling more canonical flows, where DNS is feasible. The reader is directed to \cite{sanderse2024review} for a recent review on closure modeling using ML.

Fortunately, the accuracy and stability in \textit{a-posteriori} settings can be guaranteed by employing an \textit{a-posteriori} learning approach \cite{shankar2024differentiableturbulenceclosurepartial}, where the training procedure includes solving PDE and the solution from the PDE solver is used to train a closure model. The utilization of a differentiable PDE solver enables an end-to-end training of ML models with backpropagation, which can lead to substantially improved \textit{a-posteriori} accuracy and use insight via numerical analysis \cite{chakraborty2024note}. However, differentiable CFD solvers are not widely available due to the nontrivial nature of changing existing CFD solvers to differentiable forms, which has hindered the investigation of this paradigm. Generally, differentiability can be realized via source-to-source transformations of existing code bases \cite{Bischof2008}, implementation of a solver in an automatic differentiation (AD) framework, or exploitation of existing adjoint-capable PDE solvers \cite{Mitusch2019}. Several previous efforts have leveraged these methods for improving CFD schemes. Kochkov et al. accelerated DNS computations through learned interpolations of the numerical scheme \cite{Kochkov2021}. Ho et al. use the adjoint-enabled field inversion technique for improving turbulence modeling of separated flows \cite{Ho2021}. Bezgin et al. have developed a differentiable CFD solver for two-phase flows in JAX \cite{BEZGIN2022108527}. Several other works have explored ML modeling for sub-grid closure in LES using differentiable simulations \cite{Sirignano2020,MacArt2021,List2022, sirignano2023deep}. List et al. utilize a differentiable simulator with a loss function based on turbulence physics to model closure for various evolving turbulent flow scenarios \cite{List2022}. Sirignano et al. deploy a deep learning-based closure model in differentiable simulation for flows around bluff bodies at a moderate Reynolds number range and evaluates their model's \textit{a-posteriori} accuracy and stability \cite{sirignano2023deep}. \textcolor{black}{A more recent study by Quattromini et al. utilizes GNNs to model the Reynolds stress tensor in RANS simulations for the reconstruction of the mean flow around a cylinder at low Reynolds numbers \cite{quattromini2024graph}.}

Building on these earlier studies, the main goal of this study is to introduce a GNN-based sub-grid turbulence closure for unsteady flows, which is embedded in a differentiable finite element method (FEM)-based LES solver. Especially, to explore the efficacy of a GNN-based sub-grid turbulence and differentiable solver for challenging flows, we apply our framework to a complex flow around the backwards-facing step (BFS), where near-wall turbulent flows, flow separation, and the advection of shed vortices occur simultaneously. The overall flowchart of our work is illustrated in Fig. \ref{fig:flowchart}. The main contributions of our study are as follows:

\begin{itemize}
    \item \textbf{Differentiable FEM-based LES solver:} A differentiable FEM-based LES solver compatible with unstructured meshes is developed. This differentiable solver is used to generate datasets including the ground truth data and a solution from a trained model. For the baseline FEM solver, FEniCS, an open source FEM-based LES solver\cite{fenics} is used. By implementing the discrete adjoint method in FEniCS, differentiability is instantiated.
 
    \item \textbf{Generalizable sub-grid closure modeling using GNN:} A sub-grid closure model using GNN is trained within the differentiable algorithm. Nodes and edges in unstructured mesh elements are represented by the nodes and edges in graphs, and non-locality of the sub-grid closure is effectively learned. A single trained model trained on the BFS case is tested on extrapolation tasks with unseen geometries including a ramp and wall-mounted cube.
 
    \item \textbf{Training under incomplete data availability:} Additional GNN-based sub-grid closure models are trained under sparse data conditions to evaluate their ability to learn closures in an inverse problem formulation. Only flowfield data downstream of the step is provided as training data, and their prediction capabilities for the entire flowfield around the step and other geometries are tested.
 
\end{itemize}

 % Figure environment removed

The remainder of this paper is organized as follows. The LES governing equations, the definitions of graph and its structures, and implementation of the adjoint method are explained in Sec. \ref{sec:Methods}. Sec. \ref{sec:Datasets and traning} elaborates the data generation and training procedure. Training results and test results on unseen geometries are provided in Sec. \ref{sec:Results and discussion}, and lastly, Sec. \ref{sec:Conclusions} concludes this study.





\section{Methods}
\label{sec:Methods} 

\subsection{Governing equations}

We seek to numerically approximate solutions to the incompressible Navier-Stokes equations, given in their non-dimensional form as:
\begin{align}
\frac{\partial \mathbf{u}}{\partial t}+\nabla\cdot(\mathbf{u}\otimes\mathbf{u})&=\frac{1}{Re}\nabla^2\mathbf{u}-\frac{1}{\rho}\nabla p+\mathbf{f} \\
\nabla\cdot\mathbf{u}&=0,
\label{eq:ns}
\end{align}
where $\mathbf{u}$ is the velocity vector, defined $\{ u,v\}$, $p$ is the pressure, $\rho$ is the density, $Re$ is the Reynolds number, and $\mathbf{f}$ represents any external forces.
The nonlinear nature of the convective term $\nabla\cdot(\mathbf{u}\otimes\mathbf{u})$ is a particularly problematic aspect of the system, resulting in chaotic solutions that are multi-scale -- spanning several orders of magnitude in space and time. Accurate direct numerical simulation of the governing equations must resolve the smallest scales of the flow, which is infeasible with our current computational capabilities for all but simplified, idealized, canonical flows. The number of grid points required scales with $Re^{9/4}$ \cite{Pope2000}, which makes DNS for many high Reynolds' number flows intractable. Fortunately, it is often sufficient in scientific and engineering applications to obtain only coarse-grained approximations of the true solution fields. The vast majority of energy in the flow is contained within the large scales, and consequently often relevant quantities, e.g. lift or drag, are influenced mainly by the scales encompassed in coarse-grained approximations.

Coarse-grained solution fields, however, cannot be recovered simply by simulating the governing equations on a coarser computational mesh. This can be shown mathematically by modeling the coarse-graining operation as a low-pass filtering operation, denoted with an overbar $\overline{\boldsymbol{\;\cdot\;}}$. Filtering, as a result of coarse-graining, leads to the introduction of additional terms in the equation, again due to the nonlinear convective term:
\begin{equation}
    \frac{\partial \overline{\mathbf{u}}}{\partial t}+\nabla\cdot(\overline{\mathbf{u}}\otimes\overline{\mathbf{u}})=\frac{1}{Re}\nabla^2\overline{\mathbf{u}}-\frac{1}{\rho}\nabla \overline{p}+\overline{\mathbf{f}} + \nabla\cdot \boldsymbol{\tau},
    \label{eqn:les}
\end{equation}
where $\boldsymbol{\tau}$ represents the effects of the unresolved velocity components on the resolved field and is defined as
\begin{equation}
    \tau_{ij} = \overline{u_iu_j}-\overline{u}_i\overline{u}_j.
    \label{eqn:t_dns}
\end{equation}
The additional terms are representative of the ``closure'' problem that has plagued numerical analysis of the Navier-Stokes equations for many years, and must be modeled to effectively solve these filtered equations. The equations above represent the filtered Navier-Stokes equations used in large eddy simulation, where the unknown term is treated as an additional stress, known as the sub-grid scale (SGS) stress. We remark here, that the nature of the filtering induced by the coarse-graining is typically unknown and the rest of this manuscript will utilize filtering and coarse-graining interchangeably. Furthermore, since we are limited by no assumptions of an effective filter, we do not consider comparisons with benchmark closure modeling strategies which require approximations of low-pass filters such as dynamic Smagorinsky \cite{germano1991dynamic}, which requires a test filter, and approximate deconvolution-based methods \cite{layton2012approximate}, which iteratively invert a Gaussian filter.

Numerous methods to model the SGS stress have been proposed over the last few decades, using various theoretical assumptions and empirical observations. The most well-known and perhaps widely used SGS model \cite{SMAGORINSKY1963} employs a linear eddy viscosity, which hypothesizes that the SGS stress is proportional to the filtered rate-of-strain in the fluid. The Smagorinsky model computes an effective eddy viscosity,
\begin{align}
    \nu_t &= (C_s \Delta)^2|\overline{\mathbf{S}}| \\
    \boldsymbol{\tau}^{EV} &= -2\nu_t\overline{\mathbf{S}},
    \label{eqn:smag}
\end{align}
where $C_s$ is a dimensionless empirical coefficient, $\Delta$ is a characteristic length scale, and $|\overline{\mathbf{S}}| = (2\overline{S}_{ij}\overline{S}_{ji})^{1/2}$ is the magnitude of the rate-of-strain tensor. The eddy viscosity assumption allows for a simple approximation of the SGS stress $\boldsymbol{\tau}^{EV}$, which is sufficient for certain flows. Further developments in SGS modeling over the years have incorporated additional nonlinear terms into the SGS stress to account for more complex flow phenomena \cite{Argyropoulos2015,Adams2002,Zang1993}. Our model uses this mixed model paradigm to compute a nonlinear SGS stress as
\begin{equation}
    \hat{\boldsymbol{\tau}} = \boldsymbol{\tau}^{EV} + \boldsymbol{\tau}^{NL},
    \label{eqn:sgs}
\end{equation}
where $\boldsymbol{\tau}^{EV}$ is computed from the Smagorinsky model and $\boldsymbol{\tau}^{NL}$ is a data-driven nonlinear contribution evaluated from a neural network.

\subsection{Graph neural networks}

From the ansatz presented in the prior section, both the eddy viscosity and nonlinear contributions of the SGS stress can be estimated using a deep learning model. $\boldsymbol{\tau}^{EV}$ is a function of a single scalar coefficient $C_s$, while $\boldsymbol{\tau}^{NL}$ represents a rank-2 tensor whose components can be  arbitrarily learned. We set a few requirements to determine a choice of DNN architecture. We wish for $C_s$ and $\boldsymbol{\tau}^{NL}$ to be both spatiotemporally varying and functions of the instantaneous resolved velocity and pressure fields $\overline{\mathbf{u}}, \overline{p}$. In addition, it is understood that the SGS stress is non-local \cite{ClarkDiLeoni2021}, in that the stress at a point in space is influenced not only by the local flow, but also the surrounding flow as well. Lastly, the network architecture must be amenable to the choice of discretization of the coarse-grained resolved solution field, which, to be most generalizable, can be assumed to be an unstructured computational mesh. Based on these requirements, we propose to model the desired turbulence quantities $C_s$ and $\tau^{NL}_{ij}$ using graph neural networks. Here, we use the Encode-Process-Decode paradigm that is commonly found in deep GNN models. We provide an overview of the GNN architecture in this section.


\subsubsection{Graph representation of flowfield}
The input graph is defined by $G_0=(\mathbf{V_0}, \mathbf{E_0}, \mathbf{A})$, where $\mathbf{V_0}, \mathbf{E_0}$ and $\mathbf{A}$ represent the node features, edge features and adjacent matrix between nodes, respectively. Specifically, $\mathbf{V_0}$ contains the feature vectors for each node, while $\mathbf{E_0}$ contains the feature vectors for each edge. The adjacency matrix $\mathbf{A}$ is a binary (one-hot) matrix that defines the connections between nodes, where $\mathbf{A}_{ij}=1$ if there is an edge between node $i$ and node $j$, and $\mathbf{A}_{ij}=0$ otherwise.

To represent flowfield data from an FEM discretization as a graph, all nodes in the finite element mesh are mapped to graph nodes, $\mathbf{V_0}$, while the edges of finite elements between corresponding nodes are represented by graph edges, $\mathbf{E_0}$. Additionally, extra edges are generated based on a radial cutoff, where connections between nodes are defined by a cutoff radius, the Euclidean distance, and the maximum number of neighbors. This procedure is briefly illustrated in Fig. \ref{fig:Cutoff}. Nodes within the cutoff radius are sequentially added, starting from the one with the smallest Euclidean distance, until the total number of connected nodes reaches the specified maximum number of neighbors. 

% Figure environment removed

The node features, $\mathbf{V_0}$, are composed of both fixed and time-varying components. The fixed node features include the position vector, the binary wall indicator, and the characteristic length scale associated with each node. The position vector of each node, $\mathbf{x}_i$, defines the spatial arrangement of the computational mesh, providing a point cloud on which the required SGS stress model parameters are computed. This feature is time-invariant because the mesh coordinates remain fixed. The binary wall indicator, $\delta_{\text{wall}}$, identifies whether a node is located on a no-slip wall ($\delta_{\text{wall}} = 1$) or not ($\delta_{\text{wall}} = 0$). This feature is crucial for near-wall modeling, as SGS stress is significantly influenced by boundary conditions at walls. The characteristic length scale, $\Delta_{\text{node}}$, is the same quantity used to compute the eddy viscosity contribution in the SGS stress model. It is calculated as the minimum edge length of each element at the cell center, and then interpolated to the nodes to define $\Delta_{\text{node}}$. The time-varying node features consist of the local resolved velocity gradient tensor, $\nabla \overline{\mathbf{u}}_i$, and the resolved pressure field, $\overline{p}_i$. A basis transformation is applied to the velocity gradient tensor to decompose it into its isotropic, symmetric-traceless, and skew-symmetric components. The edge features, $\mathbf{E_0}$, include the relative position vector between nodes, $\mathbf{r}_{ij} = \mathbf{x}_i - \mathbf{x}_j$, which is time-invariant since the node positions are fixed. The node and edge features are briefly described in Fig. \ref{fig:Cutoff}. We remark that time-varying FEM discretizations are also compatible with graph representations but are not implemented in this study.



\subsubsection{Encode-Process-Decode}
We adopt a simple Encode-Process-Decode architecture for our GNN implementation as follows. After the graph is generated from the raw dataset, the node features in the input graph, $\mathbf{V_0}$, are mapped to a higher-dimensional latent node space, $\mathbf{h}_i$, through a linear projection. Similarly, the latent edge features, $\mathbf{h}_{ij}$, are obtained by applying a linear projection to the edge features from the input graph. The encoding process is formally defined as follows:
\begin{align}
    \mathbf{h}_i &= \mathcal{E}_v(\mathbf{V_{0}}) \\
    \mathbf{h}_{ij} &= \mathcal{E}_e(\mathbf{E_{0}})  
    \label{eq:encoder}
\end{align}
where $\mathcal{E}_v$ and $\mathcal{E}_e$ are linear projection-based encoders for node and edge features with the structure of $[9, 128]$ and $[2,128]$, respectively, and are applied node/edge-wise. Once the input graph's node and edge features are encoded into latent node and edge representations, the processor performs multiple rounds of message passing on these latent features. In our setup, we use three message-passing layers within the processor. Each layer consists of arithmetic operations and multi-layer perceptrons (MLPs), with batch normalization applied after each message-passing step to stabilize learning. 

After processing, the node features are decoded using a linear projection to predict the target quantities for the SGS stress model. Our model predicts a total of four outputs in two dimensions (2D): the SGS coefficient $C_s$ and four independent components of the non-linear stress tensor $\tau^{NL}_{ij}$, where $i$ and $j$ refers to the Cartesian components of a symmetric 2D tensor. Specifically, the isotropic and symmetric-traceless components of $\tau^{NL}_{ij}$ are predicted separately and then summed to form the final stress tensor, which is defined as follows: 
\begin{align}
    \boldsymbol{\tau}^{NL} &= k\begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} + \alpha\begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix} + \beta\begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix} + \gamma\begin{bmatrix} 0 & 1 \\ -1 & 0 \end{bmatrix}
    \label{eq:tau}
\end{align}
Then, the decoding process is defined as follows:
\begin{align}
    \alpha_{\boldsymbol{\tau}} &= \mathcal{D}_v(\mathbf{h}_{i}), 
    \label{eq:decoder}
\end{align}
where $\alpha_{\boldsymbol{\tau}}$ is predicted turbulence model parameters including $(C_s, k, \alpha, \beta, \gamma)$, and $\mathcal{D}_v$ is a linear projection-based decoder for the processed node features $\mathbf{h}_{i}$, with structure of $[128, 5]$.


\color{black}


\subsubsection{Message-passing}
The processor in the GNN model consists of sequential nonlinear message-passing layers that propagate information throughout the graph. The latent node and edge features, $\mathbf{h}_{i}$ and $\mathbf{h}_{ij}$,  are used as input to the successive message-passing layers. Similar to standard GNN frameworks, we apply a residual update mechanism to edge features. Specifically, the edge features are updated as $\mathbf{h}_{ij} \leftarrow \mathbf{h}_{ij} + \mathbf{h}_{ij}'$, where the new edge features are aggregated along edges. Following this, a residual update is applied to node features as $\mathbf{h}_{i} \leftarrow \mathbf{h}_{i} + \mathbf{h}_{i}'$. The edge update is defined as follows:
\begin{align}
    \mathbf{h}_{ij}' &= f_e(\mathbf{h}_{ij},\mathbf{h}_{i},\mathbf{h}_{j}) \\
    \mathbf{h}_{ij} &= \mathbf{h}_{ij} + \mathbf{h}_{ij}',
    \label{eq:edge}
\end{align}
where $f_e$ is an edge update function containing MLPs with the structure of $[384, 128, 128]$. The sigmoid linear unit (SiLU) serves as the activation function at the hidden layer.

The node update is performed similarly:
\begin{align}
    \mathbf{h}_{i}' &= f_v\left(\frac{1}{|\mathcal{N}|}\sum_\mathcal{N}\mathbf{h}_{ij},\mathbf{h}_{i}\right) \\
    \mathbf{h}_{i} &= \mathbf{h}_{i} + \mathbf{h}_{i}'
\end{align}
where $\mathcal{N}$ represents a neighborhood of connectivity and $f_v$ is a node update function containing trainable MLPs with the structure of $[256, 128, 128]$ and SiLU serving as the activation function at the hidden layer. 



\subsection{Implementation of the adjoint method}

The object of the differentiable turbulence is to pair the deep learning model with a differentiable solution algorithm for the governing equations, such that the parameters of the network can be trained in an end-to-end fashion. Automatic differentiation (AD) frameworks have become a necessary tool for training deep neural networks using gradient-based optimization. The AD principle relies on the fact that gradients of a composition of differentiable functions can be exactly computed using the chain rule. Most standard DNNs are composed of straightforward feed-forward operations, such as linear operations and pointwise nonlinearities, which can easily be written in high-level scientific programming languages. However, the integration of PDE solution algorithms, particularly using unstructured computational meshes, can add enormous complexity that makes writing these algorithms in an AD-enabled framework impractical. While the development of PDE solvers in AD-enabled languages is underway, there currently exists a plethora of efficient and robust non-differentiable PDE solvers that can interface with high-level languages and be called as external routines. If these can be integrated within an AD framework such as PyTorch \cite{paszke2019pytorch}, we can leverage the advantages of a differentiable solution scheme without rewriting complex PDE solvers from scratch.

Fortunately, AD toolchains provide a pathway to integrating this functionality through the use of custom gradients. In reverse-mode AD, we leverage the chain rule as follows:
\begin{align}
    x_1 &= f_0(x_0) \label{eqn:ch0}\\
    x_2 &= f_1(x_1) \label{eqn:ch1}\\
    &\vdots \notag \\
    y &= f_n(x_n) \label{eqn:chn}\\ 
    \frac{dy}{dx_i} &= \frac{dy}{dx_{i+1}}\frac{dx_{i+1}}{dx_i} = \frac{dy}{dx_{i+1}}\frac{df_i(x_i)}{dx_i} .\label{eqn:chain}
\end{align}
The iterative nature of Eq.~\ref{eqn:chain} allows gradients of $y$ to be propagated backwards starting from $\frac{dy}{dx_n}$. For each step in the forward pass, Eqs.~\ref{eqn:ch0}-\ref{eqn:chn}, a corresponding reverse function should be implemented, also known as a vector-jacobian product (VJP):
\begin{equation}
    \frac{dy}{dx_i} = f_i'\left(\frac{dy}{dx_{i+1}},x_i\right).
\end{equation}
The backpropagation of gradients then constitutes a composition of these VJP functions. In an AD framework, VJPs are supplied for the majority of $f_i$'s that are encountered, e.g. addition, multiplication, and other simple operations. However, if $f_i$ represents a call to an external routine not known by the framework, a custom VJP must be supplied.


We first look at an outline of the implicit pressure correction scheme (IPCS) \cite{goda1979multistep} for solving the unsteady Navier-Stokes equations, shown in Algorithm \ref{alg:ipcs}. The algorithm is initialized from a choice of discretization and flow configuration. At each timestep, the GNN is used to predict the turbulence model parameters, denoted $\alpha_{\boldsymbol{\tau}}$. Since the GNN is written in the AD framework, no further work is required to enable gradient backpropagation. The next three functions, \texttt{tentative\_vel}, \texttt{pres\_correct}, and \texttt{vel\_correct}, represent solutions to three PDEs that are obtained from external routines outside of the AD framework. For this reason, we must implement a custom VJP for each of the PDE solution steps.
\algnewcommand{\Initialize}[1]{%
  \State \textbf{Initialize:}
  \Statex \hspace*{\algorithmicindent}\parbox[t]{.8\linewidth}{\raggedright #1}
}
\begin{algorithm}
\caption{IPCS method for Navier-Stokes equations}
\label{alg:ipcs}
\begin{algorithmic}
\Initialize{\texttt{GNN}$_\theta$\\
    \texttt{tentative\_vel}\\
    \texttt{pres\_correct}\\
    \texttt{vel\_correct}\\
    $\mathbf{u},p$\\
    $t \gets 0$}
\While{$t<T$}
\State $\alpha_{\boldsymbol{\tau}} \gets \texttt{GNN}_\theta(\mathbf{u},p)$
\State $\mathbf{u}^* \gets \texttt{tentative\_vel}(\mathbf{u},p,\alpha_{\boldsymbol{\tau}})$
\State $p^* \gets \texttt{pres\_correct}(\mathbf{u}^*,p)$
\State $\mathbf{u} \gets \texttt{vel\_correct}(\mathbf{u}^*,p^*,p)$
\State $p \gets p^*$
\State $t \gets t + \Delta t$
\EndWhile
\end{algorithmic}
\end{algorithm}

The three PDE-solving functions can be represented in the FeniCS formalism as:
\begin{equation}
    \mathbf{x} = \mathrm{\texttt{PDE\_solve}}(\mathbf{m}),
\end{equation}
where $\mathbf{m}$ represents any time-varying inputs to each function, and $\mathbf{x}$ denotes the solutions to the corresponding PDEs. For example, for the function \texttt{tentative\_vel}, $\alpha_{\boldsymbol{\tau}}$ serves as time-varying input, and $\mathbf{u}^*$ is the solution to the PDE, while $\mathbf{u}^*$ represents time-varying input, and  $p^*$ is the solution to the PDE for the function \texttt{pres\_correct}. A custom reverse-mode gradient computation would implement:
\begin{equation}
    y_{\mathbf{m}} = \mathrm{\texttt{PDE\_solve\_vjp}}\left(y_{\mathbf{x}}, \mathbf{m}\right),
\end{equation}
where $y$ represents the objective function, and we indicate partial derivatives with a subscript.
The forward \texttt{PDE\_solve} can be further expanded as the solution to the linear system:
\begin{equation}
    \mathbf{A}(\mathbf{m})\mathbf{x} = \mathbf{b}(\mathbf{m}),
\end{equation}
where $\mathbf{A}$ and $\mathbf{b}$ represent the discretized left hand side (LHS) and right hand side (RHS) of the PDE respectively, which in general, are functions of the input $\mathbf{m}$. Given this description, we can implement \texttt{PDE\_solve\_vjp} using the discrete adjoint method. The discrete adjoint method defines
\begin{equation}
    y_{\mathbf{m}} = -\boldsymbol{\lambda}^T\left(\mathbf{A}_\mathbf{m} - \mathbf{b}_\mathbf{m}\right),
\end{equation}
where $\boldsymbol{\lambda}$ is the solution to the adjoint equation given by
\begin{equation}
    \mathbf{-A}^T\boldsymbol{\lambda} = y_{\mathbf{x}}^T.
\end{equation}

A PDE solver that exposes $\mathbf{A}$, as well as enables, importantly, the computation of $\mathbf{A}_\mathbf{m}$ and $\mathbf{b}_\mathbf{m}$, offers sufficient functionality to implement \texttt{PDE\_solve\_vjp}.

Our choice of FEniCS \cite{fenics} for this study is motivated by this criteria. FEniCS offers a general-purpose PDE solving framework using the finite element method. FEniCS provides the ability to define PDEs symbolically using unified form language (UFL) \cite{alnaes2014unified} and automatic differentiation functionality that is useful for computing $\mathbf{A}_\mathbf{m}$ and $\mathbf{b}_\mathbf{m}$. In addition, FEniCS incorporates an integrated PETSc library \cite{petsc-user-ref}, which facilitates the efficient computation of linear systems during the solution of PDEs and the discrete adjoint method. PDEs are defined using their variational form. For example, a Poisson system can be defined as:
\begin{python}
    F = inner(nu*grad(u), grad(v))*dx
\end{python}
where \texttt{u} is a \texttt{TrialFunction}, \texttt{v} is a \texttt{TestFunction}, and \texttt{nu} is a given \texttt{Function}. The matrix $\mathbf{A}$ at the left-hand side and vector $\mathbf{b}$ at the right-hand side of the discretized linear system can be produced by calling the \texttt{assemble} routine on \texttt{F}. To compute $(\mathbf{A}-\mathbf{b})_{\mathrm{\texttt{nu}}}$, one need only \texttt{assemble} a new form: 
\begin{python}
    F_nu = derivative(F, nu)
\end{python}
With these procedures in hand, a custom VJP for \texttt{PDE\_solve} can be supplied, described in Algorithm \ref{alg:adj}.

\begin{algorithm}
\caption{Custom gradient for PDE solve}
\label{alg:adj}
\begin{algorithmic}
\Function{\texttt{PDE\_solve}}{$\mathbf{m};$ F} 
\State $\mathbf{A},\mathbf{b} =$ assemble(F(\textbf{m})) \Comment{LHS/RHS of form F}
\State $\mathbf{x} =$ solve($\mathbf{A},\mathbf{b}$) \Comment{Linear solve}
\State \textbf{return x}
\EndFunction
\Function{\texttt{PDE\_solve\_vjp}}{$y_\mathbf{x},\mathbf{m};$ F} 
\State $\boldsymbol{\lambda} =$ solve($-\mathbf{A}^T,y_\mathbf{x}^T$) \Comment{Adjoint solution}
\State $F_\mathbf{m} =$ derivative(F(\textbf{m}; \textbf{x}), \textbf{m}) \Comment{Residual gradient}
\State $y_{\mathbf{m}} =$ assemble($-\boldsymbol{\lambda}^TF_\mathbf{m}$) \Comment{Assembled gradient}
\State \textbf{return} $y_{\mathbf{m}}$
\EndFunction
\end{algorithmic}
\end{algorithm}

Note that for brevity, we have included only the major steps of the procedure and removed many of the specific details of the implementation, such as conversion of PyTorch tensors to FEniCS variables, the application of boundary conditions, and certain FEniCS syntax. With \texttt{PDE\_solve\_vjp} implemented, \texttt{PDE\_solve} can be used as any other differentiable PyTorch layer, and the entire IPCS algorithm can be written using PyTorch variables, which naturally record the gradient tape required for automatic backwards differentiation.


\section{Datasets and training}
\label{sec:Datasets and traning}

Channel flows with obstacles positioned along one or either side of the channel wall are prevalent across various scientific and engineering applications. Examples include applications in environmental science, biomedical engineering, and industry-scale fluid dynamics \textcolor{black}{\cite{pandey2023flow, sarwar2024modeling, ma2021thermal}}. Depending on the shape and configuration of these obstacles, the flow can exhibit a complex mix of phenomena such as stagnation, separation, recirculation, reattachment, and vortex shedding. These behaviors are highly sensitive to obstacle geometry and flow conditions. Consequently, accurately predicting the intricate flow behavior around these obstacles often demands significant computational resources, particularly when dealing with turbulent or unsteady flows. Advanced modeling techniques or high-resolution simulations are typically required to capture the full dynamics, making efficient computation an ongoing challenge in the field.

We first describe our training configuration given by flow over a backwards-facing step (BFS). This is a common benchmark problem in fluid dynamics that has been widely studied in both experimental \cite{armaly1983experimental} and computational \cite{wee2004self} settings. BFS flow is characterized by flow separation at the step and a subsequent reattachment region further downstream, which introduces a recirculation region near the step and transient vortex shedding. These phenomena are found frequently in more complex non-idealized flows and can be challenging to model accurately, making the BFS a valuable case study for evaluating modeling approaches. The BFS configuration can be described as flow in a channel with a sudden expansion. An inlet is prescribed on the left with a uniform velocity in the x-direction, entering a channel of fixed width. Once the flow has fully developed, the channel width is increased, creating a step at the lower wall. This expansion produces a shear layer at the step height, which leads to turbulent mixing and vortex development. The flow reattaches downstream and eventually exits the domain through a pressure outlet. A schematic of the computational domain is presented in Fig. \ref{fig:dom}(a).


% Figure environment removed


Ground truth data is taken from a high-resolution LES simulation of the BFS flow under the Reynolds number, $Re=U_{\infty}h_0/\nu=26,051$, where $U_{\infty}=32m/s$ and $\nu=1.56\times10^{-5}$. We use an unstructured triangular mesh of on average 80,000 Taylor-Hood elements, which is refined near all walls and particularly around and downstream from the step. The highest density of elements resides in the shear layer at the step, with generally decreasing density further away. The LES equations using the standard Smagorinsky turbulence model with $C_s=0.16$ are solved using an implicit pressure correction scheme and Crank-Nicolson timestepping. We additionally use streamline upwind Petrov-Galerkin (SUPG) and least square incompressibility constraint (LSIC) stabilization due to the finite element implementation. For consistency, we set the ground truth timestep $\Delta t$ to $5\times10^{-6}s$, which is sufficient to keep the Courant-Friedrichs-Lewy (CFL) number approximately equal to or less than 1, where $\mathrm{CFL}<1$ is not strictly required due to the implicit timestepping. Snapshots of the velocity and pressure fields are saved every $10^{-4}$ seconds, and a total of 1024 snapshots are generated. We denote results from this ground truth simulation as ``LES-80k''. 
More comprehensive efforts to align the modeling with experimental or direct numerical simulation (DNS) studies under specific operating conditions are considered beyond the scope of this work, because the primary objective of this work is to demonstrate the capabilities of the graph neural network and the differentiable framework.

We generate a significantly coarser mesh for the BFS flow, consisting of 5,253 elements, on which we apply our ML-based closure and evaluate it against baseline comparisons. Fig. \ref{fig:dom}(b) shows a portion of this mesh in the area around the step. The scaling factor for element sizes was set to be $\approx 4.5\times$ the size of the high-resolution mesh. Simulations on this coarse grid use nearly the same solution scheme as the ground truth, except the SUPG stabilization is removed and the timestep size is increased to $\Delta t=10^{-4}s$. We test two baseline comparisons on this grid, no model (``NM-5k''), where no closure model is used, and the standard Smagorinsky closure (``LES-5k''), where we set $C_s=0.06$ based on optimizing \textit{a-posteriori} results with respect to the ground truth. The ML-closure is denoted ``ML-5k''.

A third mesh, consisting of 23,502 elements, is also created for each geometry, positioned between the coarse ML mesh and the fine ground truth mesh in terms of size. This mesh serves as an additional baseline comparison (referred to as 'LES-20k') for the model in relation to a more well-resolved LES simulation. The Smagorinsky model is used with $C_s=0.07$, and the timestep is reduced to $\Delta t=5\times10^{-5}s$.

We optimize the ML model by minimizing the $L^2$ norm error of the resulting \textit{a-posteriori} velocity field with respect to the training dataset from the ground truth velocity field. During training, a predicted solution trajectory $\hat{\mathbf{u}}$ is obtained by simulating the LES equations using the GNN-based SGS closure model. The training dataset is generated using only the first 256 snapshots of the LES-80k simulation. We set multiple trajectories for training dataset, where each of trajectory consists of 32 consecutive snapshots with different initial conditions separated by 16 timesteps. This is based on our own empirical analysis and suggestions from previous work to avoid gradient explosion. Ultimately, this leads to a total of 15 trajectories for training. The velocity field in each trajectory {$\{ \mathbf{u}_k \}_{k=0}^{n}$} is projected onto the coarse mesh used in the ML-LES simulation to generate the target velocity field {$\{ \overline{\mathbf{u}}_k \}_{k=0}^{n}$}. The projection is performed using the Galerkin projection method as follows. In a FEM-based CFD solver, the flow field data is represented as a function of the $x$, $y$ and $z$ coordinates within a specified function space. In this study, the two-dimensional ground truth velocity field, $\mathbf{u}$, defined on the dense grid, is represented within each cell as a function of $x$ and $y$ and belongs to a second-order continuous Galerkin function space. Likewise, the ground truth velocity data projected onto the coarse grid, $\overline{\mathbf{u}}$, is defined on each cell of the coarse grid and belongs to a second-order continuous Galerkin function space based on the coarse mesh. The weak form equation of the projection from $\mathbf{u}$ to $\overline{\mathbf{u}}$ is expressed as follows:

{
\begin{equation}
    \int_{\Omega_e} \overline{\mathbf{u}} \cdot v \; dx = \int_{\Omega_e} \mathbf{u} \cdot v \; dx, \quad \forall v \in{V_c},
\end{equation}
}
where $\Omega_e$ represents each element in the computational domain, $V_c$ is a second-order continuous Galerkin function space defined on the coarse grid, and $v$ is a test function within the function space. For simplicity, this weak form equation can be written as follows:
{
\begin{equation}
    a_e(\overline{\mathbf{u}},v) = L_e(v), \quad \forall v \in{V_c},
\end{equation}
}
where $a_e$ and $L_e$ represent bilinear and linear forms, respectively. Since this simplified weak form equation is satisfied for all test functions within $V_c$, by choosing basis functions, $\{\phi_i\}_{i=1}^6$, as a test function, a linear system can be configured as follows: 
\begin{equation}
\begin{split}
    \mathbf{A_e}\mathbf{c_e} &= \mathbf{L_e}, \\
    \text{where} \quad (\mathbf{A_e})_{ji} &= a_e(\phi_i, \phi_j), \; (i,j=1,2,...,6)\\
    \mathbf{c_e} &= [c_1\, c_2\, ...\, c_6]^T, \\
    \mathbf{L_e} &= [L_e(\phi_1)\, L_e(\phi_2)\, ...\, L_e(\phi_6)]^T
\end{split}
\label{eqn:weakform}
\end{equation}
In FEniCS, a second-order Lagrange function is chosen as the basis function within the second-order continuous Galerkin function space, where each component of $\mathbf{c_e}$ represents the coefficient of a basis function for $\overline{\mathbf{u}}$. Since the linear system in Eq.~\ref{eqn:weakform} is defined locally on each element, the global linear system is assembled to ensure continuity between elements as follows. 
{
\begin{equation}
    \mathbf{A}\mathbf{c}=\mathbf{L}
\end{equation}
}
This assembly process combines the contributions of each element, resulting in a system that enforces the continuity of the solution across shared boundaries between adjacent elements. The pressure field is also projected for the initial condition. The initial condition is integrated over 32 steps using the GNN-based turbulence model. The overall loss is computed as
\begin{equation}
    L = 0.9\times\frac{1}{T}\sum_{t=0}^T \int_\Omega ||\overline{\mathbf{u}}_t - \hat{\mathbf{u}}_t||_2^2 \; d\Omega+0.1\times\int_\Omega ||\overline{\mathbf{u}}_{mean} - \hat{\mathbf{u}}_{mean}||_2^2 \; d\Omega,
\end{equation}
where both the instantaneous and the time-averaged accuracy are considered, and the integral is computed using the FEM discretization. This formulation of the loss function enables the model to learn sub-grid closures that not only match the instantaneous flow field values but also maintain consistency with the overall trend of actual values. Since the integral is computed in FEniCS, an adjoint can be obtained using the same method described previously. The loss is backpropagated in an end-to-end fashion, through all PDE evaluations and timesteps, to update the parameters of the GNN. The model is trained for 50 epochs using the Adam optimizer \cite{kingma2014adam} with a learning rate of $10^{-4}$. Training took about 6 hours on 2 NVIDIA A100 GPUs and 8 AMD EPYC 7543P CPU-cores. Much of the inefficiency in the model can be attributed to data transfer between the CPU and GPU, where the CPU is used to solve the PDE and the GPU is used to evaluate the GNN. Training history is shown in the Fig.~\ref{fig:Training_history} in Appendix. \ref{app:Training history}.




\section{Results and discussion}
\label{sec:Results and discussion}
We evaluate the performance of the hybrid ML-LES solution algorithm by examining several \textit{a-posteriori} metrics from the flow. For each generalization case, the trained ML closure is used in a simulation of the flow beginning with zero initial conditions. The solution is integrated over a total of 1024 timesteps (we remind the reader that error computation and backpropagation during training are performed over the first 256 timesteps in batches of 32). We analyze the efficacy of the model with respect to several baselines: ``NM-5k'', no closure on the coarsest mesh, ``LES-5k'', Smagorinsky model on the coarsest mesh, and ``LES-20k'', Smagorinsky model on a finer mesh. 

For all predicted velocity fields, we compute the normalized time-varying $L^2$ error at each time step with respect to the ground truth velocity, given by:
\begin{equation}
    \frac{\int_\Omega ||\overline{\mathbf{u}}_t - \hat{\mathbf{u}}_t||_2^2 \;d\Omega}{U_\infty^2},
\end{equation}

where $U_\infty^2$ is the inlet velocity, and $\Omega$ is the computational domain. Additionally, we provide the velocity sampled from a probe in front of the step, ramp, and cube at the location $(h,h)$, $(2h,h)$, and $(6h,3,5h)$, respectively. These locations are based on the region where the error between the LES-5k and the LES-80k models is the largest. For these transient plots, we report values from the first 1024 timesteps, extending well beyond the training dataset, which consists of the first 256 timesteps. This allows us to evaluate the accuracy and stability of pointwise predictions over a longer prediction horizon. We also examine the time-averaged statistics of the flow, as these are often the most relevant metrics of interest. We present the time-averaged x-velocity field $U_x$ along with the corresponding error field to assess each model's ability to accurately capture the recirculation zone and identify areas of discrepancy within the flow. This series of \textit{a posteriori} metrics provides a comprehensive evaluation of model performance, ensuring that the predicted flow aligns closely with the ground truth.



\subsection{Training result}


We first examine the results from the BFS case. Fig. \ref{fig:BFS_error} shows the normalized $L^2$ error over time for the four models tested (with the gray dashed line representing the extent of training data observed). We observe that NM-5k shows the steepest growth and highest overall error, indicating the need for a closure model. LES-5k displays a smaller initial slope and fluctuates around a significantly lower error than the NM-5k model, while the LES-20k model results in the smallest initial slope and similar stationary error as LES-5k. We see that the learned model ML-5k generally results in the lowest overall $L^2$ error, and given that this metric is reflective of the training cost function, we determine that the training process is functioning as intended to minimize the $L^2$ error. Furthermore, the ML-5k model is stable for longer time integration than time steps used during training, $t=0.0256s$. However, the transient $L^2$ error is just one metric we should use to evaluate model performance. To further examine the transient behavior of the model, pointwise accuracy of each model near the wall is compared. Fig. \ref{fig:BFS_probe} illustrates pointwise accuracy of the models from a probe at $(x,y)=(h,h)$ using the normalized x-direction velocity. It is observed that the ML-5k model is more accurate in capturing periodic fluctuations of velocity than other models and remains stable for longer time horizon.

% Figure environment removed


% Figure environment removed


The mean flow is a particularly useful measure that decouples the transient effects of the model from flow features of interest. Fig. \ref{fig:Flowfield_BFS} depicts the normalized x-velocity of the flow and the resulting error field with respect to the LES-80k model. We also report the integrated total error for each model. We can observe the recirculation region characterized by the negative (blue) x-velocity that is present in each model. the NM-5k model contains discrepancies throughout the portion of the domain shown, including prior to the step, in front of the step, and at the boundary layer. The LES-5k model reduces the error by a factor of over 5, but still contains deviations, notably at the boundary layer, while the LES-20k model achieves smaller error than the LES-5k model by reducing the error in this region. The ML-5k model achieves the smallest errors among models by reducing the error within the region in front of the step.

\textcolor{black}{
The outputs of our sub-grid closure model include the Smagorinsky coefficient, $C_s$, and the coefficients of four independent basis matrices of the non-linear sub-grid stress tensor, $k$, $\alpha$, $\beta$, and $\gamma$. The final non-linear stress tensor can be constructed by summing these basis matrices in the following manner:
\begin{align}
    \boldsymbol{\tau}^{NL} &= \begin{bmatrix} \tau^{NL}_{xx} & \tau^{NL}_{xy} \\ \tau^{NL}_{yx} & \tau^{NL}_{yy} \end{bmatrix}=k\begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} + \alpha\begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix} + \beta\begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix} + \gamma\begin{bmatrix} 0 & 1 \\ -1 & 0 \end{bmatrix}=\begin{bmatrix} k+\alpha & \beta+\gamma \\ \beta-\gamma & k-\alpha \end{bmatrix}
    \label{eq:NL_stress}
\end{align}
Consequently, each element of the non-linear sub-grid stress tensor, $\tau^{NL}_{ij}$, can be visualized as the sum or difference of the coefficients of the basis matrices.
Fig. \ref{fig:nonlinear_stress_BFS} illustrates the visualization of the normalized x-direction velocity, the Smagorinsky coefficient, and each component of the non-linear sub-grid stress tensor predicted by the sub-grid closure model at $t= 0.02s$.} In LES simulation with the conventional Smagorinsky model, $C_s$, is predetermined before the simulation to model the viscous effect of sub-grid scale eddies, and every mesh point in the computational domain has the same $C_s$ value, which remains fixed throughout the simulation. This often leads to excessive dissipation in the viscous sub-layer region near walls where wall viscosity is already strong enough to damp the eddies. Furthermore, flow state predictions near vortices with high local shear can be inaccurate due to this over-dissipation. In contrast, the ML-5k adaptively adjusts $C_s$ during the simulation to tune the sub-grid dissipation. Specifically, the ML-5k model predicts smaller $C_s$ values in the near-wall regions compared to the areas farther from the walls. In addition, the model predicts lower $C_s$ value near the vortices where local shear is high due to rapid velocity changes around the vortex cores. This adaptive adjustment of $C_s$ reduces the dissipation caused by the sub-grid eddies, thereby decreasing errors within the near-wall region and region upstream of the step as shown in Fig. \ref{fig:Flowfield_BFS}. \textcolor{black}{We also observe that the non-linear stress components exhibit large absolute values near the separation region and vortices, where the Smagorinsky coefficient is small and, consequently, the stress predicted by the linear eddy viscosity model is also small. This indicates that our model can effectively identify regions with high values or sudden changes in strain rate, where the Boussinesq approximation \cite{boussinesq1877essai} of linear eddy viscosity is challenged. In addition, the shear stress terms, $\tau^{NL}_{xy}$ and $\tau^{NL}_{yx}$, are predicted to exhibit much larger magnitude in the negative direction compared to the normal stress terms, $\tau^{NL}_{xx}$ and $\tau^{NL}_{yy}$, near vortices. This demonstrates that our model effectively responds to the non-linear and complex turbulent interactions around vortices, including the non-linear momentum exchange between the $x$- and $y$-direction velocity components and backscatter.
It is notable that such complex sub-grid learning is extracted despite our model being exposed to a training dataset solely comprised of sampled velocity time-series across the domain.}


% Figure environment removed



% Figure environment removed



\subsection{Generalization to new geometries}
\label{sec:Generalization to new geometry}

We apply our GNN-based subgrid closure model, trained on the BFS case, to different geometries to evaluate its generalization ability. These new geometries include ramp and wall-mounted cube (WMC) cases with the same height of $h=0.0127m$ as the BFS case and are tested under the same Reynolds number, $Re=26,501$. They present a more challenging extrapolation task for the trained model than simply changing the step size in the BFS case, as these configurations have distinct geometric differences near the separation regions and exhibit different separation physics. Visualizations of the discretizations of these geometries are provided in Fig.\ref{fig:Grid_BFR_WMC} in Appendix. \ref{app:Grids used for data generation}


Fig. \ref{fig:BFR_error} shows the normalized $L^2$ error over time for the ramp case, where we compare the performance of the trained model with three other baseline predictions. We observe that NM-5k exhibits the highest overall error (due to the lack of any closure), while LES-5k has lower error than NM-5k, highlighting the need for a closure model. Furthermore, ML-5k shows a lower error than LES-5k, whereas LES-20k achieves the lowest error. This suggests that even when tested on the ramp case, which has a smoother separation due to a smaller flow turning angle, our model is improved compared to LES-5k model. Furthermore, the proposed model exhibits stability beyond the time steps used during training, suggesting its robustness in extended time-step integration. Fig. \ref{fig:BFR_probe} presents the pointwise accuracy of the models based on normalized x-direction velocity data obtained from a probe located at $(x,y)=(2h,h)$. The trends observed in the pointwise analysis are consistent with the overall error profiles, demonstrating that the ML model not only improves upon the performance of LES-5k but also remains stable for longer prediction horizons than those encountered during training.

% Figure environment removed

% Figure environment removed

Fig. \ref{fig:Flowfield_BFR} presents a contour plot of the normalized mean x-direction velocity around the ramp, along with the error distribution for each model when compared to the LES-80k reference solution. Additionally, the integrated error values for each model are reported. Across all models, the recirculation zones immediately downstream of the ramp are well captured. However, the NM-5K and LES-5k models exhibit the largest errors near these recirculation zones and along the wall boundary. In contrast, the LES-20k model shows relatively smaller errors in these areas, achieving the lowest error among the models. The ML-5k model effectively reduces errors in the high shear flow and near-wall turbulence regions by adaptively modulating the Smagorinsky coefficient. Fig. \ref{fig:Smagorinsky_BFR} shows a visualization of the Smagorinsky coefficient and the normalized x-direction velocity across the domain at $t=0.02s$. It is observed that our model changes $C_s$ dynamically depending on localized flow quantities, thereby reducing errors near the wall boundary and vortices. 

% Figure environment removed


% Figure environment removed

Fig. \ref{fig:WMC_error} compares the predictability of the trained model for the WMC case with three other models using the $L^2$ error over time. In the WMC case, simulations without a turbulence model become unstable and diverge after several time steps, indicating the necessity of incorporating a closure model. While the LES-20k achieves the lowest error among the models, using the ML-5k model does not result in significant performance improvement over the LES-5k model unlike the ramp case. This can be attributed to the difference in separation mechanisms between two cases. In contrast to the ramp case where flow originates from inflow and simply separate near the slope, the flow upstream of the cube encounters a stagnation point with high pressure and recirculation before separation---a fundamentally different separation mechanism from what the trained model is exposed to during training. Nevertheless, the trained model recovers the performance of the LES-5k model and remains stable in extended time-step integration above time span used during training. Fig. \ref{fig:WMC_probe} compares the pointwise accuracy of models from a probe at $(x,y)=(6h,3.5h)$ using the x-direction velocity normalized by the freestream velocity. At this location, the LES-20k model can capture peaks of the periodic fluctuation of the true data, while the LES-5k model predicts lower peaks than the actual values. Although using the trained model does not provide a marked improvement in peak prediction, ML-5k remains stable for longer predictions horizons than those used during training.


% Figure environment removed

% Figure environment removed


Fig. \ref{fig:Flowfield_WMC} illustrates the normalized mean x-direction velocity field and the corresponding error distribution around the cube, along with the integrated error values in comparison with the LES-80k model. Across all models, the recirculation zones upstream and downstream of the cube are effectively captured. However, the LES-5k model exhibits large errors near the vortex trajectories and wall boundaries. In contrast, the LES-20k model shows significantly smaller errors in these regions. The ML-5k model achieves a lower integrated error compared to the LES-5k model by reducing errors near these areas. Fig. \ref{fig:Smagorinsky_WMC} illustrates adaptive adjustment of the Smagorinsky coefficient and the normalized x-direction velocity across domain. This figures shows that the ML-5k model decreases $C_s$ value near the vortex-formulating region and the wall boundary layer.

% Figure environment removed

% Figure environment removed


These results suggest that a GNN-based SGS closure model trained in an \textit{a posteriori} manner can generalize to unseen geometries. Even on a geometry with significantly different separation physics, the model remains stable, recovers the baseline low-fidelity model, and improves predictability in the mean sense. This generalization capability can be attributed to the fact that the spatial mesh is perfectly represented by the graph connectivity in GNN, which inherently learns the relationships between nodes and edges through their structure. The \textit{a posteriori} learning process enhances the stability and accuracy of the trained model by ensuring model-data consistency.



\color{black}




\subsection{Model training with constrained data availability}
\label{sec:Model training with constrained data availability}


In this section, we perform experiments for training a GNN-based SGS model under the condition where only a portion of flowfield data is available. Sparse or incomplete experimental data is common in many scientific and engineering domains. This is often due to the limited number of sensors, as it is not feasible to install sensors throughout the entire field. For instance, in wind tunnel experiments, only a small number of sensors are located strategically, and weather prediction models are initialized or validated from sounding or satellite observations at specific locations. Furthermore, placing too many sensors in fluid dynamics experiments can affect the accuracy of experiments due to interference with the flow. Even in CFD simulations, resolving the whole flowfield with high accuracy is impractical, so researchers often focus on studying specific regions of interest in greater detail.


We utilize the BFS case to train new models under several scenarios where only the flowfield data downstream of the step is available. Each scenario is defined by a different threshold that specifies the accessible portion of the domain for training. Fig. \ref{fig:Sparse_domain} illustrates two example scenarios where the models are trained using only the flowfield data at $x>10h=0.127m$ and $x>20h=0.254m$, respectively. Each finite element cell is included in the training dataset when x-coordinates of all nodes in the element are included in the threshold range. In total, our scenarios include four different thresholds: $x>0, 5h, 10h, 20h$. We train four models under each of these scenarios and test them on different geometries used in the previous sections. For the training dataset, the initial 256 snapshots (i.e., until $t=0.0256s$) are utilized. Each trajectory in the training dataset is composed of 32 rollout snapshots, with initial conditions separated by 16 time steps. This results in a total of 15 trajectories. Models are trained under the same training setting as in Sec. \ref{sec:Datasets and traning}. Training history is shown in the Fig.~\ref{fig:Training_history} in Appendix. \ref{app:Training history}.

% Figure environment removed


Fig. \ref{fig:BFS_sparse_error} shows the normalized $L^2$ error over time for the BFS case, comparing the performance of models trained under different data availability conditions. We observe that all models trained with incomplete domain data remain stable and achieve lower or comparable level of errors than the LES-20k model. Notably, even the model trained using flowfield data at $x>20h$, which constitutes a small portion of the entire domain, can improve upon the baseline LES-5k model. Furthermore, the ML-5k models trained with data from $x>5$ and $x>10h$ attain lower errors than the ML-5k model trained on the full domain, achieving lower errors than the LES-20k model. Fig. \ref{fig:BFS_sparse_probe} compares the predictive accuracy using the normalized x-direction velocity data from the probe located at $(x,y)=(h,h)$, and similar trends with the $L^2$ error plot are observed. All models trained with the sparse training data are more accurate in capturing peaks of periodic velocity fluctuations than the LES-5K and LES-20K models. This indicates that far-field data can help calibrate models to perform well near the step.

% Figure environment removed

% Figure environment removed

We test our models, trained with incomplete data from the BFS case, on the ramp and WMC cases to assess generalization capability across different geometries, as discussed in Sec. \ref{sec:Generalization to new geometry}. Figure \ref{fig:BFR_sparse_error} presents the normalized $L^2$ error over time for the ramp case, comparing the performance of models trained under varying data availability with other models. All models trained with incomplete flowfield data show lower errors than the LES-5k model and remain stable for longer time integrations than during training. The ML-5k models trained with data from $x>5h$ and $x>10h$ achieve lower errors than the ML-5K model trained on the full domain data, exhibiting a similar level or errors with the LES-20k model. Figure \ref{fig:BFR_sparse_probe} presents the pointwise accuracy of the models based on normalized x-direction velocity data obtained from a probe located at $(x,y)=(2h,h)$. The ML-5k models trained with partial flowfield data show improvement in capturing velocity fluctuations just downstream of the ramp until approximately the midpoint of the simulation, while the ML-5k model trained with full domain data shows better performance in capturing the decaying magnitude of the peaks.

% Figure environment removed

% Figure environment removed

Fig. \ref{fig:WMC_sparse_error} compares the accuracy of models trained under data availability constraints with other models using the normalized $L^2$ errors over time for the WMC case. All models trained with data constraints remain stable during extended time integration. Furthermore, while the ML-5k model trained with full-domain data does not show much improvement over the LES-5k model, all ML models trained with incomplete data achieve significantly lower $L^2$ errors. Specifically, the ML-5k models trained with data from $x>5h$ and $x>10h$ maintain errors at approximately half their level throughout the simulation. Fig. \ref{fig:WMC_sparse_probe} compares the pointwise prediction accuracy of each model using the normalized x-direction velocity data from a probe located at $(x,y)=(6.5h,3h)$. We observe that the ML-5k models trained with data from $x>5h$ and $x>10h$ demonstrate enhanced accuracy in capturing the peaks of velocity fluctuations caused by separation at the cube and the resulting vortices.


% Figure environment removed

% Figure environment removed


These results suggest that a subgrid closure model using GNN can learn general flow physics and remain stable even when trained only with flowfield data near the main vortex trajectory and wall boundaries. This is in accordance with the flow physics affected by closure design. The model's generalization capability across different geometries is also enhanced when trained using more relevant sources of information --- in this case, velocity trajectories downstream of the step. This is a benefit of the GNN closure embedded in a PDE-constrained optimization formulation.

A comprehensive examination of the metrics presented for the cases mentioned in the previous sections allows us to conclude that the ML-5k model, using our learned GNN-based closure, is capable of outperforming the more expensive LES-20k model in terms of accuracy, or improving the accuracy of simulations on unseen geometries with a coarse mesh. We compute an effective speedup by measuring the evaluation time of models, which is tabulated in Tab. \ref{tab:time}. Time costs of the ML-5k model for each case are calculated by averaging simulation times of the ML-5k models trained under different constrained data. For all geometries, the ML-5k model is able to achieve a reduction in cost of approximately a factor of 7 compared to the LES-20k model. Especially, for the ramp case, the ML-5k models trained with data downstream of the step can achieve a similar level of accuracy with the LES-20k model while significantly reducing the time cost. This suggests that in the engineering design process where detailed design parameter optimization follows broader design exploration tasks, a GNN-based closure model can accelerate forward model evaluations while preserving high accuracy. Furthermore, the generalizable results indicate greater reliability during extrapolation.

\def\arraystretch{1.3}
\begin{table}[ht!]
\centering
\caption{Core/GPU-s per $10^{-4}$ s of physical time in simulation}
\begin{tabular}{l|l|l|l}
\textbf{Model}    & \textbf{BFS} & \textbf{Ramp} & \textbf{WMC} \\ \hline
LES-80k & 48.57s & 50.74s & 65.16s \\
LES-20k & 3.16s & 2.69s & 4.17s \\
LES-5k & 0.32s & 0.34s & 0.52s \\
ML-5k & 0.4s & 0.43s & 0.63s\\
\end{tabular}
\label{tab:time}
\end{table}







\section{Conclusions}
\label{sec:Conclusions}

% \color{red}

This study investigates closure model development within the paradigm of differentiable physics, where a data-driven machine learning closure model is embedded within a differentiable solver for the Navier-Stokes equations. In this work, we embed a graph neural network within a large eddy simulation solver to learn turbulence closure models on a low resolution grids. This allows for closure on unstructured meshes and generalization to new geometries through the mesh-invariant property of the graph neural network. We use an FEM-based CFD solver, FeniCS, as it is equipped with a discrete adjoint. This discrete adjoint can be coupled, via the chain-rule of calculus with the gradient obtained from the graph neural network obtained through automatic differentiation. Specifically, we implement custom vector-jacobian products for each sub-step of the solver such that the back propagation of the loss function is well defined throughout the entire training procedure.

For demonstrations, we apply our framework to two-dimensional turbulent flow over a backward-facing step to train a sub-grid scale closure model and evaluate generalization capabilities by testing it on other geometries, such as a ramp and a wall-mounted cube. When tested on the backward-facing step, the trained model demonstrates superior performance in predicting both transient and mean flow behaviors compared to a conventional (i.e., fixed Smagorinsky) LES simulation on a finer grid. Specifically, the trained model improves accuracy by adaptively adjusting the Smagorinsky coefficient near the wall boundary layer and the shear layer. When tested on extrapolation tasks with unseen geometries that involve different separation physics, the trained model continues to enhance the accuracy of LES simulations on coarse grids and remains stable over longer prediction times, beyond the time horizon used during training. These results indicate that a GNN-based closure model trained within a differentiable framework can be employed to construct generalizable turbulence closure models.

Finally, to test the applicability of our proposed methodology for data-limited scenarios, we train models with sparse wake data from the backward-facing step case and evaluate prediction accuracy. Models trained only with the data downstream of the step can show better performance than the baseline ML model which is trained with the full domain data. This improvement is expected since closure exerts a significant effect downstream of the separation point. Even when tested on unseen geometries, models trained with sparse data are stable and can enhance the accuracy of LES simulation. These results suggest that data-driven closure modeling, interpreted as a PDE-constrained inverse problem, may be used to learn effectively from sparse observational data.

While this study shows the potential of closure modeling using GNN and differentiable physics solver for generalizability, further research is required to fully achieve robust generalization capability. Our trained models are only generalizable to geometries where computational meshes have same order of density as the meshes used for training as shown in Figs.\ref{fig:BFS_error_gridconvergence} and \ref{fig:BFS_probe_gridconvergence} in Appendix. \ref{app:Grid convergence test}. Further research can improve generalization capability by making trained models applicable to other meshes with different order of density. In addition, incorporating interpretability of graph structures into the training procedure can make model's behavior more understandable especially when tested on unseen geometries. Lastly, extension to three-dimensional turbulent flows is ongoing to construct more realistic turbulence closure modeling. 

\section{Acknowledgements}
This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No. DGE 1745016 awarded to VS. VS and VV acknowledge the support from the Technologies for Safe and Efficient Transportation University Transportation Center, and Mobility21, A United States Department of Transportation National University Transportation Center. 
RM and HK acknowledge funding support from ASCR for DOE-FOA-2493 ``Data-intensive scientific machine learning'' and for SCIDAC RAPIDS2. RM and HK also acknowledge support from Penn State Institute for Computational and Data Sciences. 





\clearpage
% \nocite{*}
\bibliography{bib}


\newpage

\begin{appendices}

\section{Training history}
\label{app:Training history}

% Figure environment removed

\section{Grids used for data generation}
\label{app:Grids used for data generation}

% \clearpage
% \appendix
% \section{Title of Appendix A}

% Figure environment removed

\section{Grid convergence test}
\label{app:Grid convergence test}
The ML-20k model is a model where a subgrid GNN-based closure, trained on a coarse mesh of the backward-facing step case, is applied to a denser grid. According to Fig. \ref{fig:BFS_error_gridconvergence}, the ML-20k model significantly increases the prediction error compared to the LES-20k model. Specifically, as shown in Fig. \ref{fig:BFS_probe_gridconvergence}, the ML-20k model overpredicts peaks of velocity fluctuations. This is because the smallest edge length in the dense grid is half the size of the smallest edge length in the coarse grid, meaning that the model was never exposed to edge lengths as small as those in the dense grid during training. Fig. \ref{fig:edge_distributions} illustrates the distributions of edge lengths on dense and coarse grids. 


% Figure environment removed

% Figure environment removed

% Figure environment removed




\end{appendices}

\end{document}

%
% ****** End of file aipsamp.tex ******