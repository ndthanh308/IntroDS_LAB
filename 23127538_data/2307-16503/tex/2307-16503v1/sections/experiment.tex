% Figure environment removed


\input{tables/main_results.tex}
\section{Experiments}
% In this section, we first conducted experiments on three long-horizon surgical robot tasks from the surgical simulation platform SurRoL~\cite{surrol}. Through extensive evaluations and model analysis, we aim to demonstrate the effectiveness of our proposed value-informed skill chaining to smoothly connect multiple skills for solving the entire task. Moreover, we also demonstrated the transferability of our learned policies to a dVRK platform in real-world tasks.  


\subsection{Environment Setup}
\subsubsection{Long-horizon surgical robot tasks} To demonstrate that our method is capable of solving long-horizon surgical robot tasks, we conducted experiments on the simulated platform SurRoL~\cite{surrol} with three tasks selected to be diverse and comprehensive, which cover a different number of surgical tools, various manipulation skills, and target objects, as illustrated in Fig.~\ref{fig:task}. Specifically, 1) the \textit{BiPegTransfer} task consists of three subtasks, including picking the block up from an initial peg with a patient-sided manipulator (PSM), handing the block over to another PSM, and finally placing the block at a target peg; the 2) the \textit{BiPegBoard} task shares a similar task structure with BiPegTransfer but additionally requires one PSM to orient the object in the last subtask; 3) the \textit{MatchBoardPanel} task is a single-manual surgical training task that requires one PSM to pull the panel door out, pick up the object, place it into a random cell on the board, and finally pull the panel door back.\footnote{In order to make this task more complex and cover more surgical skills, we designed a modified version with an additional sliding panel door.} All tasks are goal-conditioned whose state space is composed of object pose and robot proprioceptive state, and the action space is delta-position
control space, and the subgoal space is composed of the positions of the object and the end-effector. 
Following~\cite{lee2022adversarial}, 200 episodes of demonstrations collected by scripted controllers are provided to each subtask policy. 


\subsubsection{Comparison methods} We compared our method against the following state-of-the-art methods: 1) \textbf{GCBC}~\cite{ding2019goal}, a representative imitation learning method that fits a parametric policy from demonstrations through a supervised objective and experience relabeling; 
2) \textbf{DEX}~\cite{huang2023guided}, an exploration-efficient demonstration-guided RL algorithm for surgical automation, which learns a single flat policy without task decomposition; 3) \textbf{T-STAR}~\cite{lee2022adversarial}, a representative skill chaining methods in robot manipulation tasks, which connects skills by regularizing the termination set to be covered by the next initiation set. While running its original implementation in the above tasks fails to make progress, which may be attributed to the sparse-reward setting, we customized this method in our framework by regularizing the terminal state to close to the initiation set of the next subtask policy.


\subsubsection{Implementation details} We adopted DEX for subtask policy learning. 
The learning rate of the actor and critic was set as 1e-4 and the coefficient of the exponential moving average was set as 5e-3 for stable training.
The hindsight experience replay was also adopted with a future sampling strategy. All subtask policies were trained until convergence, which takes around 2M environment steps. The same training steps are used to train baselines. In the skill chaining stage, we used SAC~\cite{sac} to learn both the value function and chaining policy. Specifically, we parameterized these two models as four-layer MLPs with ReLU activations, where each layer was of 256 hidden dimensions. The output of the actor was scaled to the task-specific range by a Tanh activation. All networks were trained with ADAM~\cite{adam} optimizer with a learning rate of 1e-4. 


\subsection{Main Results}
We evaluated the manipulation performance with task success rate, the number of completed subtasks, and the total steps taken to accomplish the task. The evaluation is the average over five random seeds. We present the performance of our method and baselines on all three tasks in Table~\ref{table:main_results}. The results show that the imitation approach, GCBC, learns to mimic the skills in the early subtasks, such as picking up the object and pulling the panel door, but starts to deviate from the expected trajectory which often leads to the failed execution of skills in the later subtasks. This may be attributed to the poor generalization capabilities of such an imitation learning approach that requires a large number of demonstrations to overcome the distribution shift problem. 
Compared with imitation learning, the demonstration-guided RL approach, DEX, achieves a relatively higher success rate and execution efficiency in \textit{BiPegTransfer} task by using online experiences to address the distribution shift and demonstrations to overcome the exploration issue. However, it performs worse in the \textit{BiPegBoard} task, as the latter requires additional tool rotation before placing the ring to the target peg. Moreover, as the number of subtasks increases in the \textit{MatchBoardPanel}, the DEX agent is more struggling with collecting successful trials and fails to make progress.


% Figure environment removed


% Figure environment removed

On the other hand, the prior skill chaining approach, T-STAR, explicitly harnesses the compositional structure of the long-horizon tasks by learning multiple skills to handle each subtask, achieving higher success rates in three tasks than in previous baselines. However, it often fails to chain the skills in the later stage of the entire task as the connection between current skills does not ensure the future connections will be successfully performed, which is represented by subtask completion in Table~\ref{table:main_results}. Instead, our method ensures more smooth connections of all subsequent skills when doing the current chaining, which achieves significant performance improvement on all tasks. Meanwhile, the results also show that, compared with T-STAR, our method takes slightly more steps to accomplish the task. We observe that our methods will make some tiny actions to mildly adjust the terminal state even when the subtask has been successfully executed. This verifies the effectiveness of our value-informed chaining approach that terminating each skill at states with higher values makes the overall subtask connections more smooth. 


\subsection{Ablation Study on Skill Chaining Strategy}
Essentially, our value-informed skill chaining approach takes a long-term view when connecting two skills by evaluating the expected success probability of the entire task, while prior methods take a relatively short-term strategy by matching the current initiation set and termination set. To this end, we aim to verify the effectiveness of our choice of value functions. We introduce three variants of our methods that take different value functions: 1) {\algoName-DM} that evaluates a terminal state of one subtask policy with a discriminator discerning whether the terminal states fall inside the termination set; 2) {\algoName-LDM} that extends the value function in \algoName-DM to measure whether all subsequent terminal states will fall insides their target terminal sets; 3) {\algoName-SR} that evaluates the terminal with subtask reward instead of the reward of the entire task. The results in Table~\ref{table:abl} show that, compared with \algoName-DM, \algoName-LDM achieves $9.97\%$ and $13.37\%$ performance improvement on the \textit{BiPegTransfer} and \textit{BiPegBoard}, respectively. This not only demonstrates that a value function with a long-term view makes the policy connections more smooth, but also our method is of good extensibility to existing skill chaining methods. Such a conclusion can be further verified by the observation that, compared with \algoName, \algoName-SR also exhibits a performance drop on both tasks due to its short-term evaluation. Surprisingly, we observe that \algoName-SR also underperforms \algoName-DM, indicating that evaluating state with subtask success probability is more likely to incur a terminal state that falls outside the next initiation set, while the distribution matching can ensure a successful connection between the current two subtask policies.

\input{tables/ablation}

\subsection{Analysis of Learned State Value Function}
We validate that our value function can accurately estimate the expected success probability of the whole task and accordingly instruct subtask policies to connect with each other smoothly. Specifically, three sets of different levels of state value are collected. The results in Fig.~\ref{fig:visualization} show that the empirical task success rate of each trajectory set is close to the estimated value. For instance, the trajectory with a low value of terminal state (left bottom) completes the first subtask, but fails to complete the following subtask tasks due to the insufficient contact between the tip of a surgical tool and the object at the end of the first subtask, which makes the handing over difficult. While the trajectory with intermediate value (left middle) smoothly connects the first two subtasks, the orientation of the object after the handing over leads to the failure of the final subtask that requires proper orientations to place the object to the target peg. In contrast, the trajectory generated by our learned policy is of a high value (left upper) and successfully connects all subtask policies, indicating the effectiveness of our accurate value estimates for skill chaining.


\subsection{Deployment on the dVRK Platform}
We conducted experiments on the real dVRK platform to validate the transferability of our trained policies. Specifically, the representative bi-manual task \textit{BiPegTransfer} is selected to be automated, which covers a wide range of complex skills. We run the best-performing policies of our method on the simulated environments and collect the generated trajectories for deployment, without extra real-robot learning. Each PSM is associated with 4 degrees of freedom (DoF), which consists of 3-DoF of translation and 1-DoF of rotation, and one action for controlling the jaw. The snapshots of the automation are shown in Fig.~\ref{fig:dvrk}. During the deployment, we find that the robot is able to sequentially perform the acquired skills of both single-hand manipulation and bimanual coordination as we observed in the simulator. We totally test 20 trials of task execution and observe 16 successful trials. This demonstrates the potential of our method in solving long-horizon surgical robot tasks on the real robot.

