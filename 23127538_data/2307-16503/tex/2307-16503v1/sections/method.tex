% Figure environment removed

\section{Method}
We introduce ViSkill, a novel reinforcement learning framework for precisely chaining manipulation skills for solving long-horizon surgical robot tasks, as illustrated in Fig.~\ref{fig:method}. Our method first decomposes long-horizon tasks into a sequence of subtasks and learns a subtask policy for each subtask in Section~\ref{method_skill_learning}. To ensure the smooth connections between skills, we subsequently introduce a chaining policy informed with a value function in Section~\ref{method_skill_chaining}. 


\subsection{Problem Formulation of Goal-Conditioned RL}\label{method_notations}
We formulate the problem of solving a long-horizon task as a goal-conditioned Markov decision process (MDP) $\mc{M}$~\cite{nasiriany2019planning}. Every episode starts with sampling a goal $g$ from goal space $\mc{G}$ and an initial environment state. The goal stays fixed during the whole episode. At each time step $t$, the agent receives the state $s_t$ and subsequently executes an action $a_t$ to do interactions. The environment transitions to a new state $s_{t+1}$ and yields a $0/1$ sparse reward $r(s_t,a_t,g)$, which is $1$ if the goal is reached at the successor state $s_{t+1}$. An episode terminates after taking $T$ environment steps. The agent aims to learn a control policy that maximizes the expected return $\mathbb{E}[\sum_{t=0}^{T-1}r(s_t,a_t,g)]$. 


\subsection{Skill Learning in Decomposed Long-Horizon Tasks}\label{method_skill_learning}
Solving such tasks featuring long-horizon structures and sparse-reward with a single, monolithic control policy is challenging due to its limited capacity to encode and coordinate all required skills. We decompose a long-horizon task into a sequence of subtasks and learn a distinct control policy for each subtask. Specifically, we factorize a single policy into a set of $K$ subtask policies $\pi_L:=\{\pi_L^{1}, \pi_L^{2},...,\pi_L^{K}\}$, where the order of subtasks is indicated by the superscript $i$ and is assumed to be fixed~\cite{andreas2017modular}. Each skill will be instructed with a subgoal $g^i\in\mathcal{G}^i$ at an initial state sampled from its initiation set~$\rho^i$, which is set up by the environment and fixed during the learning process following~\cite{lee2022adversarial}. Each skill agent then interacts with the environment to collect experiences and stores them in its own replay buffer. The objective of each skill is to successfully accomplish the subtask by reaching the instructed subgoal within the subtask episode $T^i$ indicated by a binary reward function $r^i$, \textit{e.g.,} placing the object to a target position.  

 However, learning each skill requires the agent to extensively explore the diverse behaviors in a prohibitively large state space, especially in surgical tasks which may include multiple surgical tools and randomly located small objects. To this regard, we aim to facilitate the exploration by providing each subtask policy with a set of demonstration data. The demonstration-guided RL algorithm DEX~\cite{huang2023guided} is then adopted for skill learning, which encourages the exploration on expert-like trajectories to reduce unproductive behaviors and use subgoal relabelling to enrich the experiences. Specifically, we train each skill by jointly maximizing the expected sum of subtask rewards and behavioral similarity between agent action and expert action $a_t^e$ estimated from subtask demonstration:
\begin{align}\label{eq:subtask_objective}
    \mathbb{E}_\pi\left[\sum_{t=0}^{T^i-1}(r^i(s_t^i,a_t^i,g^i)-\alpha\cdot\mathrm{dist}(a_t^i, a^e_t))\Big|s_0^i\sim \rho^i \right].
\end{align}

While the prior methods depend on an additionally learned reward function~\cite{lee2022adversarial} or a transitional policy~\cite{lee2019composing,byun2022training} to ensure high generalization capabilities to reach different subgoals, the introduction of subgoal-conditioned subtask policies naturally empowers the skill with such ability~\cite{schaul2015universal}. To this end, we train all skills with an initiation set in a limited size and fix them in the later chaining stage, which circumvents the necessity of policy finetuning required by the prior methods given proper subgoal instructions.



% each of which is allowed to interact with a local MDP associated with an initial state distribution $\rho^{i}$.  The goal of each skill $\pi^{i}$ is to maximize the expected return w.r.t. the local reward function $r^{i}$ within the subtask episode $T^{i}$. During the training course, each skill is associated with a termination set $\beta^{i}\subset\mc{S}$ that contains all terminal states of successful episodes. Meanwhile, we assume the agent maintains pre-collected expert demonstrations for each subtask, $\{\mathcal{D}_{e}^{1},\mathcal{D}_{e}^{2},...,\mathcal{D}_{e}^{K}\}$, where each demonstration set consists sequences of state-action pairs. 

% To execute two adjacent subtask policies successfully, the support of the initial states of the successor policy should cover the termination set of the preceding policy, \textit{i.e.}, $\beta^{i-1}\subset\mathrm{supp(\rho^i)}$. Most existing methods achieve this goal by expanding the initial state distribution of each subtask policy to cover the termination set of its preceding policy. However, the termination set can be potentially large in surgical tasks. For instance, surgical tools can be freely located to a certain degree after accomplishing the handover of an object. This issue can be exacerbated along the skill chain that leads to boundlessly large initiation sets, making learning the subtask policies difficult. Thus, to circumvent widening initial state distributions, we propose to model each skill as a goal-conditioned policy. By assigning appropriate goal parameters, each subtask policy will terminate at the state included in the support of subsequent initial state distribution. Besides, such a goal-conditioned form avoids the hand-crafted reward design by using 0/1 subgoal-reaching reward. Moreover, it also allows experience relabelling to further improve learning efficiency. 

% Specifically, each subtask policy $\pi_L^i:\mc{S}\times\mathcal{G}\rightarrow\mathcal{A}$ takes state and subgoal as input and output actions with the local objective $\mathbb{E}[\sum_{t=0}^{T^i-1}\gamma^tr^i(s_t,a_t,g)]$. Here, the local reward function is binary, which indicates whether the subgoal is reached by the subtask policy at the terminal state of each episode. However, learning each subtask policy solely from such sparse reward function is easy in surgical tasks due to the requirement of extensive data collection in large state space of surgical tools and objects. 

% To this end, we improve the exploration efficiency by using a state-of-the-art demonstration-guided method, DEX~[xx], which encourages the agent to follow expert-like trajectories to facilitate productive explorations, even at states that are far from demonstrated ones, within the actor-critic framework. Specifically, we train each subtask policy $\pi_L^i$ by jointly maximizing the return and behavioral similarity between agent action and estimated expert action $a^e_t$:
% \begin{align}\label{eq:subtask_objective}
%     \mathbb{E}_\pi\left[\sum_{t=0}^{T^i} \gamma^t(r^i(s_t,a_t,g^i)-\alpha\cdot\mathrm{dist}(a_t, a^e_t)) \Big|s_0\sim \rho^i \right].
% \end{align}

% We note that there is a trade-off between computation cost and generalization capability on different initial states of each subtask policy when tuning their initial state distributions. Prior works train each subtask policy on a modest set of initial states and widen such set during chaining each policy. We follow their pre-training approach by adding a small noise on predefined initial states, while we avoid widening such distributions during skill chaining by introducing a chaining policy that terminates the preceding policy at the initial states of the subsequent policies. 



% Needle to consider:
% 1. policy form
% 2. present loss or definition
% 3. reward function in different MDPs
% Foresight justify

\subsection{Skill Chaining via Value-Informed Subtask Termination}\label{method_skill_chaining}
Once acquired skills for each subtask, the naive sequential execution of skills is likely to fail when one skill terminates at the state that the next skill is unable to handle, \textit{e.g.,} the states lie outside the initiation set of the next skill. To overcome this issue, existing methods constrain the termination set of the previous skill to be covered by the initiation set of the next skill. Although such matching between two sets ensures that the next skill will start from its initiation set, the terminal states gradually deviate from the initial set of the following skill due to its sensitivity to the state variation~\cite{ghoshdivide}, leading to the failures of future subtasks. In other words, the connection between current skills does not ensure the successful chaining of all future skills. 

To this regard, we aim to terminate the previous skill in the states, starting from which the agent is not only likely to execute the next subtask successfully, but also is likely to accomplish all remained subtasks. Our key insight is that the chaining of the current two skills should smooth the chaining of all future skills, thus accomplishing the entire task with high probability. Specifically, we first introduce a chaining policy $\pi_C$ to instruct the subtask policies with subgoals. The termination set of one skill, which consists of terminal states of successful subtask executions, is then determined by the chaining policy. To terminate the subtask policy at desired states, we estimate the state value in terms of the whole task reward given an initial state $s^i_0\in\rho^i$ of the subtask, which is defined as follows:
\begin{align}
    V_{\pi_C}(s^i):=\mathbb{E}_{\pi_C,\pi_L}\left[\sum_{j=i}^K R_j|s_0^i=s^i,g^j\sim\pi_C(\cdot|s_0^j)\right],
\end{align}
where $R_i:=\sum_{t=0}^{T^i-1}r(s_t^i,\pi_L^i(s_t^i,g^i),g)$ denotes the subtask return. 
In our sparse-reward settings, the agent only receives a positive reward $r_T=1$ if the final state reaches the goal at the end of the episode (\textit{e.g.}, place the object to the target position successfully). Accordingly, the value function equivalently measures the expected success probability of the whole task given a state:
\begin{align}
    V_{\pi_C}(s^i):=\mathbb{E}_{\pi_C,\pi_L}\left[r_T\right]=\mathrm{Prob}(r_T=1).
\end{align}

At a high level, higher state values of initial states indicate the following subtask policies are more likely to be connected. Terminating the previous subtask policy at such states is thus desired for accomplishing the whole task. To learn such a value function, we minimize the residual Bellman error as follows:
\begin{align}
    \mathbb{E}_{\pi_C,\pi_L}\left[\sum_{i=1}^{K-1} ((R_i+V_{\pi_C}(s_{0}^{i+1})-Q_{\pi_C}(s_0^i,g^i))^2\right],
\end{align}
where $Q_{\pi_C}$ denotes the Q-value function, and the raw environment reward is used as the state value in the last subtask in practice, as the subgoal of the last subtask policy is equal to the task goal. This value function bears resemblances to the one proposed in~\cite{bagaria2021robustly}, while ours measures the success probability of the entire task instead of each subtask. 

Based on the value function, the chaining policy then instructs the subtask policy with a subgoal to terminate at states with high state values for ensuring the smooth chaining of all future skills. It can be learned through the policy gradient method with the algorithm-specific actor loss $\mathcal{L}_{actor}$. While the sparse reward function may incur sample inefficiency given the large manipulation space, we adopt the self-imitation method~\cite{oh2018self} to augment the actor loss for encouraging exploration. In our context, it essentially learns to imitate the successful trajectories $\mathcal{D}_{sil}$ that ends at the state in each termination set, which gives the final objective of the policy optimization:
\begin{align}
\mathcal{L}_{actor} + \lambda\cdot\mathbb{E}_{\mathcal{D}_{sil}}\left[ \sum_{i=1}^{K-1}\log\pi_C(g^i|s^i) w(s^i_0,g^i) \right],
\end{align}
where $\lambda$ is a temperature coefficient, and the weight function $w(s^i,g^i)$ is set as the advantage function following~\cite{awac}. By iteratively updating the value function and chaining policy under off-the-shelf RL algorithms, the subtask policies can be smoothly connected for accomplishing the entire task. 


    % Q(s,g^i)=\mathbb{E}_{\pi_C}\left[\sum_{j=i}^{K}\sum_{t=0}^{T^j} \gamma^tr(s_t,a_t,g^j) \Big|s_0\sim \rho^i \right].
% the initial states with a value function that estimates the expected success
% of the whole task given the a terminal state. Our key insight is that 

% Although such matching between two sets ensures a smooth connection between the current two skills, it considers a little about the chaining between all subsequent skills. In other words, 


% However, such approaches are difficult to scale to long-horizon surgical since the propagation of the 
% To this end, we aim to terminate the previous skill in the states, starting from which the agent is not only likely to execute the next subtask successfully, but also is likely to accomplish all remained subtasks. 



% Once acquired skills for each subtask, a naive approach to chaining these skills is to execute them subsequently. However, naively chaining these skills are likely to fail since the policies are trained to connect with subsequent policies smoothly. To chain skills successfully, existing methods typically match the terminal state distribution of the prior skill with starting states of the following skill. Nevertheless, such a chaining approach is not well-suitable in automating surgical tasks, because a tiny variation of the initial state may lead to the failure of the corresponding subtask due to the high-precision requirement of surgical manipulations. In other words, existing methods distinguish a little between states in the initiation set. Such an issue is exacerbated along the skill chain, making accomplishing the entire long-horizon task difficult. Moreover, even the previous skill terminates in the state that is likely to lead to the successful execution of the next skill, the next skill may still terminate at an undesired state that leads to failed execution of its following skill. 

% {\blue Once acquired skills for each subtask, the navie sequential execution of skills is likely to fail due to XXX. Existing methods address this issue by constraining the terminal state distribution of the prior skill to the initial state distribution of the following subtask. However, such approaches are difficult to scale to long-horizon surgical since the propagation of the }
% To this end, we aim to terminate the previous skill in the states, starting from which the agent is not only likely to execute the next subtask successfully, but also is likely to accomplish all remained subtasks. 

% subtask success indicator
% Specifically, we introduce a chaining (high-level) policy $\pi_C:\mc{S}\rightarrow\mathcal{G}$ that instructs the current subtask policy with a subgoal. Here we assume that the agent are provided with a success indicator or each subtask following~\cite{lee2022adversarial}. The chaining policy is learned to maximize the global return that measures the success rate of the entire task, which is estimated by a global Q-value function defined as follows:
% \begin{align}
%     Q(s,g^i)=\mathbb{E}_{\pi_C}\left[\sum_{j=i}^{K}\sum_{t=0}^{T^j} \gamma^tr(s_t,a_t,g^j) \Big|s_0\sim \rho^i \right].
% \end{align}
% In our sparse-reward settings, the Q-value function measures the feasibility of executing the following skills given the subgoal instruction to the current skill. We learn it through temporal difference learning by minimizing the residual Bellman error $\mathcal{L}_critic$ based on any off-the-shelf RL algorithm. 

% Meanwhile, the chaining policy can be learned through the policy gradient method with the algorithm-specific actor loss $\mathcal{L}_{actor}^{rl}$. While the sparse reward function may incur sample inefficiency, we adopt the self-imitation~[xx] method to augment the actor loss. Specifically, we utilize a weighted self-imitation learning objective as follows:
% \begin{align}
% \mathcal{L}_{actor}^{sil}=\mathbb{E}_{(s^i,g^i)\sim\beta^i}\left[\sum_{i=1}^K \log\pi(g^i|s^i)w(s^i,g^i) \right],
% \end{align}
% where we set the weight $w(s^i,g^i)$ based on the advantage function $A(\cdot,\cdot)$ following~\cite{awac}, i.e., $\frac{1}{\beta}(A(s^i,g^i))$ with temperature coefficienct $\beta$. The self-imitation objective performance BC-like fitting on successful chaining instruction to improve sample efficiency. It could be well incorporated in our framework since we maintain termination sets for each subtask. Finally, we obtain our overall training objective:
% \begin{align}
%     \mathcal{L}_{chaining}=\mathcal{L}_{critic} + \mathcal{L}_{actor}^{rl} + \mathcal{L}_{actor}^{sil}.
% \end{align}
% still need to modify formulation


% whether to mention goal-conditioned RL?

