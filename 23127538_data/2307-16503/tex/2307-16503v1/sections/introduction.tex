\section{Introduction}


Learning-based surgical robot automation has been increasingly investigated in recent years, with its potential to improve the precision and efficiency of surgical tasks~\cite{ras}.
As is known, robotic surgical tasks are typically long-horizon and composed of several sub-steps with a series of actions over an extended period of time~\cite{meli2021autonomous}.
Such tasks usually have complex specifications, involving sequential operations on small objects. For instance, even in a simplified scenario of basic skill training, automating the bimanual peg transfer task is still challenging, which requires the robot to pick the block up, hand it over to another manipulator, and finally place it to the target peg without collision (see Fig.~\ref{fig:overview}a). 
To date, how to effectively learn the control policy for long-horizon tasks via reinforcement learning (RL) is still an open challenge in the field of surgical robot learning.


% Figure environment removed


Existing RL methods still have limitations when addressing long-horizon tasks, because the agent requires extensive data collection to explore useful behaviors in a large state space, and needs many optimization steps to learn multiple skills with sparse reward signals.
Given these issues, existing methods often resort to considerable reward designing~\cite{ibarz2021train} to facilitate both exploration and policy learning. Unfortunately, these approaches usually require subjective manual engineering, which can result in undesired behaviors~\cite{riedmiller2018learning} and make the policy stuck in local optima~\cite{ddpgher}.
In addition, the cost of manual engineering would become unaffordable when the complexity of the task increases.


A more practical solution is to decompose the long-horizon task into a sequence of easier-to-learn subtasks.
In this way, each subtask is associated with a single action so that the burdens of both exploration and policy learning will be reduced. 
Then, separate control policies are learned to master the distinct skill corresponding to each subtask and all skills are sequentially executed to complete the whole long-horizon task.
Nevertheless, naively executing one policy after another would fail, because the terminal state of one subtask policy is not necessarily able to be handled by the next subtask policy~\cite{lee2020learning}.
Taking the bi-manual peg transfer task in Fig.~\ref{fig:overview} as an example, it can be split into three subtasks \textit{`block picking'}, \textit{`block handing over'}, and \textit{'block placing'}. 
If the hand-over position executed by the left robot arm is not reachable for the right robot arm, it is impossible for the right arm to complete the subsequent subtask of \textit{`block placing'}.
Connecting the subtask policies for accomplishing a long-horizon task is non-trivial, which gives rise to new skill chaining algorithms.


To ensure smooth connections, some prior methods have been proposed to learn transition policies~\cite{lee2019composing,byun2022training}, which aim to transit the agent from terminal states (\textit{i.e.,} termination set) of the subtask to the initial states (\textit{i.e.,} initiation set) of the next subtask, so that the next policy is able to accomplish the subsequent subtask. 
While straightforward, the transition between two subtasks would fail when the terminal state is far from the initiation set of the next subtask policy~\cite{lee2022adversarial}. 
Alternatively, another group of approaches attempts to directly force the termination set of one policy to be covered by the initiation set of the next policy through distribution matching~\cite{clegg2018learning,lee2022adversarial}. 
However, constraining subtask terminal states via distribution matching is too coarse.
As a result, the terminal states will gradually deviate from the initial set of the following policy and lead to the failures of future subtasks.
As illustrated in Fig.~\ref{fig:overview}b, a tiny variation of the block pose after picking up makes the block difficult to be handed over, which is aggregated along the subtask sequence and hampers the completion of placing the object to the peg. In other words, not all states are suitable for connecting two adjacent subtasks, and these methods lack an effective mechanism to evaluate the terminal states of each subtask.



In this work, we present \algoName (Value-informed Skill Chaining), a novel RL-based framework for long-horizon surgical robot tasks. 
Instead of connecting subtask policies via coarse distribution matching, our methods evaluate the terminal states of each subtask with a learned state value function, which estimates the expected success probability of the entire task given a state. 
To terminate subtask policies at states with high values, a chaining policy is introduced to instruct each policy with a subgoal at the initial state. 
Consequently, our method is of high accuracy in chaining all subtask policies and accordingly accomplishing the whole task. 
We demonstrate the effectiveness of our method on three long-horizon surgical robot tasks from the open-source surgical simulation platform SurRoL~\cite{surrol}. 
The experiment results empirically show that our method achieves high task success rates and execution efficiency. 
We also deploy the learned policy to the da Vinci Research Kit~(dVRK) hardware platform, which validates the effectiveness of \algoName on the real robot.
Our contributions are summarized as follows:
\begin{itemize}
    \item We propose a novel value-informed skill chaining algorithm, which considers the expected success probability of the entire task when learning a value function to evaluate the terminal state. The state with the highest value is selected for connecting subtask policies smoothly.
    \item Based on our skill chaining idea, we develop a novel RL framework for long-horizon tasks of surgical robots. We empirically validate the effectiveness of our methods in three representative tasks from SurRoL~\cite{surrol}.
    \item We deploy our method to the dVRK platform and demonstrate the feasibility of executing the policy's predicted motion trajectory on the real robot.
\end{itemize}