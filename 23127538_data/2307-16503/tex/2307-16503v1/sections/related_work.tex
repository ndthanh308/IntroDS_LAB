\section{Related Work}

\subsubsection{Policy learning for surgical automation} Automating surgical robot tasks with learning-based methods, representatively RL, gets increasing attention in the last decade, owing to its advantages in task generalization. However, most of the research has focused on automating elementary subtasks, such as pattern cutting~\cite{cutting1,nguyen2019new,nguyen2019manipulating} and tissue retraction~\cite{flexml,srl_reward,tissue_lfd}. Their task-specific methods are not easily applicable to the task that are long-horizon
and composed of several elementary actions over an extended period of time. Recently, visible attempts have been made to automate such long-horizon surgical tasks, including bimanual peg transfer~\cite{zhang2022human,huang2023guided}, peg-and-ring~\cite{ginesi2019knowledge,ginesi2020autonomous}, suturing~\cite{schwaner2021autonomous,schwaner2021autonomous-suture,srl_multistage,wilcox2022learning}, and tissue manipulation~\cite{meli2021autonomous}. These works mainly leverage learning from demonstrations approach, typically dynamic movement primitives, to learn the motions of each elementary action and compose them to accomplish the entire task. Nevertheless, they require substantial task-specific expertise in specifying the goal position of each subtask and designing the rule-based connections between subtasks, which makes these methods difficult to be developed at scale. In contrast, our method exhibits higher scalability by flexibly learning both subtask policies and achieving smooth connections between them with a value-informed skill chaining approach.


\subsubsection{Skill chaining for long-horizon tasks} Recently, deep reinforcement learning has presented a scalable framework for learning control policies in robotic tasks. However, solving long-horizon tasks with a single, flat RL policy is still challenging due to the exploration burden on large behavior space and typical sparse-reward setting. 
To this end, some skill chaining methods are proposed to explicitly decompose the task into multiple subtasks, learn an individual policy for each subtask, and sequentially execute subtasks policies to perform the entire task. The connections between skills are achieved by constructing skill trees~\cite{konidaris2009skill, konidaris2012robot, bagaria2020option, bagaria2021robustly}, learning transitional policies~\cite{lee2019composing,byun2022training}, and distribution matching~\cite{clegg2018learning,lee2022adversarial}. However, these methods lack an effective mechanism to evaluate the terminal states of each subtask, while the connections between future subtask policies are sensitive to the variation of the terminal state~\cite{ghoshdivide}. In contrast, we propose a value-informed skill chaining method that learns a value function to estimate the states with higher values if starting from them makes the following subtask policies more likely to be connected, thus achieving a higher success rate for the whole task.


