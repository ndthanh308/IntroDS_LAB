\section{Introduction}

Generative Adversarial Networks (GANs) achieve impressive results on unconditional ~\cite{ karras2019style, zhang2019self, karras2020analyzing,yu2021dual} and conditional image generations \cite{wang2018high, dundar2020panoptic, liu2022partial}, inpainting \cite{yu2019free, li2020recurrent,zhao2021large}, and image editing tasks \cite{abdal2021styleflow, chen2022exploring,dalva2022vecgan}. 
Traditionally, each of these tasks has been explored with a dedicated network and training pipeline.
However, recently it has been shown that high-quality image editing can be achieved with well-trained GAN models and especially by StyleGAN networks \cite{karras2019style, karras2020analyzing} that are trained to generate images without a condition \cite{tov2021designing, wang2022high}.
This approach achieves numerous edits via semantically rich feature representations of well-trained GANs \cite{shen2020interpreting, harkonen2020ganspace,patashnik2021styleclip,chen2022exploring}.
In this work, we are interested in taking this direction one step further and learning an encoder for a pretrained GAN that can achieve high-quality image inversion, diverse inpainting, and editing under one framework.

\newcommand{\interpfigt}[1]{% Figure removed}

% Figure environment removed

% \newcommand{\interpfigt}[1]{% Figure removed}
% % occlusion images
% % Figure environment removed

To benefit from GAN's image editing capabilities, extensive research is conducted on image inversion algorithms to find the corresponding latent codes that will generate a particular real image \cite{creswell2018inverting, roich2022pivotal, zhu2020domain, tov2021designing}. It is observed that there exists a trade-off between image reconstruction fidelity and image editing quality \cite{tov2021designing}.
Studies show that low-rate latent spaces ($z$, $W$, or $W ^+$ for StyleGANs) are limited in their expressiveness power, and not every image can be inverted with high-fidelity reconstruction to GAN's natural low-rate latent space \cite{tov2021designing}.
When images are not projected to GAN's natural latent space, even when an image is reconstructed with high fidelity, it will not be editable.
Methods are proposed to skip spatially higher resolution features (higher-rate latent codes) from an encoder to a GAN generator for better reconstruction and editing properties \cite{wang2022high,alaluf2022hyperstyle}. 
In this work, we are interested in a more challenging scenario; learning an image encoder that can project an erased image to a well-trained GAN's natural latent space.
This framework provides new capabilities for image editing, as shown in Fig. \ref{fig:teaser}.


Recently, GAN inversion-based image inpainting methods have been proposed \cite{yu2022high, wang2022dual}. 
These methods optimize an image encoder and learn skip connections to pretrained StyleGAN model. 
Even though promising results are achieved, these methods model image inpainting in a deterministic way.
They are trained to reconstruct the original image from the erased ones without any stochasticity. 
They are not regularized to project the image into GAN's natural latent space and do not enjoy the editing capabilities of them.

In this work, we propose a framework that achieves high-quality image inversion, diverse inpainting, and editing simultaneously. 
We design an encoder architecture that takes an erased input image and encodes latent codes for the visible parts. We also design a mixing network that combines randomly sampled latent codes with encoded ones.
This setup allows the model to output diverse results.
However, we find that the framework learns to ignore the randomly sampled latent codes and has the tendency to output deterministic results.
To achieve diversity, we propose to train the framework with a novel design.
We use generated data and make use of $W^+$ and generated image pairs to regularize the network to use both inputs; erased image and sampled $W^+$.
Secondly, to achieve high fidelity inversion of unerased pixels and to prevent the color discrepancy between the unerased and erased parts, we learn higher dimension latent codes.
In summary, our main contributions are as follows:
\begin{itemize}[leftmargin=*]
    \item We propose a novel framework for image inpainting with GAN inversion.
    Our framework includes an encoder to embed images and a mixing network to combine them with randomly sampled latent codes to achieve diversity.
    The mixing network has a gating mechanism that improves the results.
    \item To achieve diversity, we propose a novel set-up to train the networks.
    We use GAN-generated images and train the network with full image reconstruction and valid pixel image reconstruction depending on if we feed the same latent code to the mixing network that is used to generate the input image or not. 
    \item We conduct extensive experiments to show the effectiveness of our framework and achieve significant improvements over state-of-the-art models for image inpainting. 
    Additionally, we show our framework can achieve diverse inpainting and editing under one framework  as shown in Fig. \ref{fig:teaser}.

\end{itemize}



