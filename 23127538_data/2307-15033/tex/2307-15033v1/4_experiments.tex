





% \begin{table*}[h]
% \centering
% \caption{Quantitative results of our and competing methods on FFHQ validation dataset for different mask ranges. Lower FID score and higher LPIPS scores are better.}
% \resizebox{0.7\linewidth}{!}{
% \begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|}
% \hline
% Mask range            & \multicolumn{2}{c|}{0.0-0.2}          & \multicolumn{2}{c|}{0.2-0.4}          & \multicolumn{2}{c|}{0.4-0.6}          & \multicolumn{2}{c|}{0.6-0.8}          &  \multicolumn{2}{c|}{0.0-1.0}          \\ \hline
% Models                          & {FID}     & LPIPS  & {FID}     & LPIPS  & {FID}     & LPIPS  & {FID}     & LPIPS  & {FID}     & LPIPS   \\ \hline
% psp \cite{richardson2021encoding} & 3.06& - & 4.62 & - & 7.26 & - & 11.19 & -  & 8.23 & - \\
% % & 18.81 &
% HFGI \cite{wang2022high} & 2.31 & - & 3.45 & - & 6.24 & - & 10.42 & - & 7.66 & - \\
% HyperStyle \cite{alaluf2022hyperstyle}& 3.02 & 0.0004 & 4.50 & 0.0012 & 6.81 & 0.0025 & 9.38 & 0.0043 &  7.46 & 0.0037 \\
% % 18.94 & 0.0073 &
%  \hline
%  CoModGAN \cite{zhao2021large} & 4.75 & 0.0074 & 5.61 & 0.0228 & 6.99 & 0.0520 & 8.32 & 0.0938  & 7.35 & 0.0790 \\ 
%  % & 9.77 & 0.1609
%  InvertFill \cite{yu2022high} & 1.13 & - & 2.21 & - & 4.91 & - &10.22&   & 7.45 & - \\
%  DualPath \cite{wang2022dual} & 3.36 & - &  8.43 & - & 14.75 & - & - & -  - & - & - \\
%  \hline
%  Ours & 2.39 & 0.01881 & 3.57 & 0.0559 & 6.08 & 0.1248 & 8.38 & 0.2452 &  5.92 & 0.2026 \\ 
%  % 15.35 & 0.4205 &

 
%  % Ours & 2.61 & &  3.72 & &6.17& & 8.63 & & 15.54 & & 6.22 \\ 
% % HFGI\_pSp1 Concat FC            & {2.6188}  & 0.0188 & {4.4599}  & 0.0734 & {7.7673}  & 0.1475 & {11.5311} & 0.2400 & {13.7040} & 0.3533 & {8.3772}  & 0.1986 \\
% % HFGI\_pSp3 Concat FC Shared     & {2.5344}  & 0.0185 & {4.0162}  & 0.0539 & {7.0776}  & 0.1167 & {12.1906} & 0.2262 & {46.6506} & 0.3964 & {9.2259}  & 0.1893 \\ 
% % hfgi\_sigmoidL1 FC Shared & {2.2837} & 0.0159 & {3.4172} & 0.0473 & {6.3918} & 0.1076 & {11.7902} & 0.2072 & {30.1938} & 0.3569 & {8.5938} & 0.1728 \\ 
% % hfgi\_sigmoidL1 FC Shared correct & 2.61 & &  3.72 & &6.17& & 8.63 & & 15.54 & & 6.22 \\ 
% % hfgi\_1024 FC & {2.4594}  & 0.0209 & {4.0796} & 0.0569 & {6.9702} & 0.1143 & {10.2059} & 0.1899 & {13.6433} & 0.2987 & {7.6533} & 0.1595 \\ 
% % pSp\_1024 FC                    & {7.6026}  & 0.0311 & {11.1766} & 0.0654 & {16.0024} & 0.1183 & {19.8758} & 0.1842 & {20.7587} & 0.2717 & {15.3587} & 0.1556 \\ 
% % pSp\_sigmoidL1 FC Shared        & {7.6567}  & 0.0168 & {11.9555} & 0.0438 & {16.9854} & 0.0896 & {21.2994} & 0.1653 & {25.4521} & 0.3139 & {16.6519} & 0.1440 \\ 
% % pSp FC                          & {10.5612} & 0.0458 & {16.1317} & 0.0947 & {22.3353} & 0.1682 & {26.8586} & 0.2499 & {24.3212} & 0.3402 & {20.9069} & 0.2095 \\ 
% \hline
% \end{tabular}}
% \label{table:results_ffhq_masks}
% \end{table*}

% % Figure environment removed









% \begin{table*}[h]
% \caption{Quantitative results of our ablation study on CelebAHQ dataset.}
% % \tiny
% \resizebox{\linewidth}{!}{
% \centering
% \begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|c|c|}
% \hline
%  Mask range        & \multicolumn{2}{c|}{0.0-0.2}               & \multicolumn{2}{c|}{0.2-0.4}               & \multicolumn{2}{c|}{0.4-0.6}               & \multicolumn{2}{c|}{0.6-0.8} & \multicolumn{2}{c|}{0.8-1.00} & \multicolumn{2}{c|}{0.0-1.0} \\  \hline
%  Models                          & {FID}       & LPIPS     & {FID}       & LPIPS     & {FID}       & LPIPS     & {FID}       & LPIPS     & {FID}       & LPIPS     & {FID}       & LPIPS     \\ \hline
% psp \cite{richardson2021encoding} & 2.3631 & - & 5.1144 & - & 11.4680 & - & 23.5734 & - & 51.6378 & - & 17.4746 & - \\
% HFGI\cite{wang2022high} & & - & & - && - && - && - && - \\
% HyperStyle \cite{alaluf2022hyperstyle}& & - & & - && - && - && - && - \\
%  \hline
%  CoModGAN \cite{zhao2021large} & {\textbf{}} & \textbf{} & {\textbf{}} & \textbf{} & {\textbf{}} & \textbf{} & {\textbf{}} & \textbf{} & {\textbf{}} & \textbf{} & {\textbf{}} & \textbf{} \\ 
% InvertFill \cite{yu2022high} \\
%  \hline
% hfgi\_128\_64\_32\_psp\_diverse & {\textbf{}} & \textbf{} & {\textbf{}} & \textbf{} & {\textbf{}} & \textbf{} & {\textbf{}} & \textbf{} & {\textbf{}} & \textbf{} & {\textbf{}} & \textbf{} \\ 
% HFGI\_pSp1 Concat FC            & {1.8927}    & 0.0225    & {5.9183}    & 0.0640    & {15.1796}   & 0.1302    & {28.6657}   & 0.2149    & {46.6709}   & 0.3285    & {20.5887}   & 0.1786    \\ 
% HFGI\_pSp3 Concat FC Shared     & {1.8270}    & 0.0178    & {5.0138}    & 0.0523    & {13.3614}   & 0.1122    & {30.5123}   & 0.2183    & {87.9903}   & 0.3951    & {23.1089}   & 0.1838    \\ 
% hfgi\_sigmoidL1 FC Shared & {1.5085} & 0.0136 & {6.6390} & 0.0421 & {12.7444} & 0.0976 & {30.7677} & 0.1949 & {79.8896} & 0.3503 & {22.6762} & 0.1621 \\ 
% hfgi\_concat\_fc\_1024          & {}          &           & {}          &           & {}          &           & {}          &           & {}          &           & {}          &           \\ 
% pSp\_1024 FC                    & {7.5514}    & 0.0279    & {13.8860}   & 0.0573    & {24.1021}   & 0.1048    & {35.1367}   & 0.1648    & {45.3160}   & 0.2517    & {26.7832}   & 0.1398    \\ 
% pSp\_sigmoidL1 FC Shared        & {8.5231}    & 0.0170    & {17.4345}   & 0.0406    & {28.9038}   & 0.0821    & {42.8734}   & 0.1498    & {62.5134}   & 0.2917    & {33.1508}   & 0.1311    \\ 
% pSp FC                          & 11.6228   & 0.0432    & {21.1350}   & 0.0851    & {35.1843}   & 0.1507    & {49.5547}   & 0.2263    & {62.4438}   & 0.3181    & {38.5564}          & 0.1900           \\ 
% \hline
% \end{tabular}}
% \end{table*}




% \newcommand{\interpfigt}[1]{% Figure removed}



% \begin{table*}[h]
% \begin{tabular}{|l|c|c|c|c|c|c|c|}
%  \hline    
% \textbf{Model}                       & \textbf{FID}    &\textbf{ MSE Mean} & \textbf{MSE Std} & \textbf{LPIPS Mean} & \textbf{LPIPS Std} & \textbf{SSIM Mean} & \textbf{SSIM Std} \\
% \hline
% \textbf{psp\_mean001}                                           & 8.1452 & 0.1059   & 0.0832  & 0.2843     & 0.0890    & 0.6002    & 0.1438   \\
% \textbf{idinvert34\_mean001}                                    & 8.0182 & 0.1036   & 0.0801  & 0.2783     & 0.0881    & 0.6012    & 0.1462   \\
% \textbf{e4e\_mean001 \cite{tov2021designing}}                                           & 8.6711 & 0.1083   & 0.0826  & 0.2870     & 0.0880    & 0.5954    & 0.1448   \\
% \textbf{hfgi\_psp\_mean001}                                     & 8.1244 & 0.0994   & 0.0835  & 0.2553     & 0.0947    & 0.6457    & 0.1634   \\
% \textbf{hyperstyle \cite{alaluf2022hyperstyle}} & \textbf{7.4469} & 0.1073   & 0.0845  & 0.2947     & 0.0929    & 0.5867    & 0.1518 \\
% \textbf{hfgi\_128\_64\_32} & 7.9113 & \textbf{0.0964}   & 0.0789  & \textbf{0.2422}     & 0.0959    & \textbf{0.6528}    & 0.1624 \\
% \hline
% \end{tabular}
% \end{table*}



% \begin{table*}[h]
% \begin{tabular}{|l|c|c|c|c|c|c|c|}
%  \hline    
% \textbf{Model}                       & \textbf{FID}    &\textbf{ MSE Mean} & \textbf{MSE Std} & \textbf{LPIPS Mean} & \textbf{LPIPS Std} & \textbf{SSIM Mean} & \textbf{SSIM Std} \\
% \hline
% \textbf{hyperstyle} & 7.4018 & 0.1070   & 0.0845  & 0.2944   & 0.0929    & 0.5872    & 0.1521 \\
% \textbf{hfgi\_128\_64\_32} & \textbf{6.9986} & \textbf{0.0954}   & 0.0794  & \textbf{0.2389}     & 0.0958    & \textbf{0.6592}    & 0.1619 \\
% \hline
% \end{tabular}
% \end{table*}



% \textbf{Set-up.} For datasets and attribute editing, we follow the previous work  \cite{wang2022high}. For the human face domain, we train the model on the FFHQ \cite{karras2019style} dataset and evaluate it on the CelebA-HQ \cite{karras2017progressive} dataset.
% For the car domain, we use Stanford Cars \cite{krause20133d} for training and evaluation.
% We run extensive experiments with directions explored with InterfaceGAN \cite{shen2020interpreting}, GANSpace \cite{harkonen2020ganspace}, StyleClip \cite{patashnik2021styleclip}, and GradCtrl \cite{chen2022exploring} methods.


\section{Experiments}

\textbf{Baselines.} We first compare our method with state-of-the-art image inversion methods pSp \cite{richardson2021encoding}, HFGI \cite{wang2022high}, and HyperStyle \cite{alaluf2022hyperstyle}. 
We use the author's released code and train those models for inpainting tasks with an additional channel in the input for masks.
Note that this is a different comparison than the display in Fig. \ref{fig:teaser} since we train these models for inpainting.
pSp model outputs $W^+$ predictions for the generation. HFGI and HyperStyle methods use two-stage training. 
First, they rely on an encoder to output $W^+$ predictions, then a second encoder takes the input image and StyleGAN generated image with $W^+$ predictions. 
The goal is to encode the missed information to higher-rate latent codes.
We feed erased images and masks to these inversion models and train them to reconstruct the visible pixels faithfully and output realistic images overall.
Next, we experiment with state-of-the-art image inpainting models for our comparisons. 
We run inferences with CoModGAN's pretrained models \cite{zhao2021large}.
CoModGAN proposes to train a StyleGAN-like model from scratch but with co-modulation and skip connections for the inpainting task.
InvertFill \cite{yu2022high} and DualPath \cite{wang2022dual} models are built on pretrained StyleGAN models.
InvertFill's authors provide us with their inference results with the masks we provide. 
We implement DualPath following the set-up provided in their paper.
DualPath trains a generator that also takes StyleGAN features.

\textbf{Evaluation.}
We report Frechet Inception Distance (FID) metric \cite{heusel2017gans}, which looks at realism by comparing the target image distribution and reconstructed images.
We also look at the diversity scores with Learned Perceptual Image Patch Similarity (LPIPS) \cite{zhang2018unreasonable}.
That is for the models that provide diverse images, we generate two images per input image-mask pair and find the distance between these two images.
We additionally use U-IDS and P-IDS metrics that are proposed by CoModGAN \cite{zhao2021large}.
They measure unpaired and paired inception discriminative scores, respectively by measuring the linear separability in a pre-trained feature space.
Our goal is to output images that are not separable from real images. SVM is fitted to the data and we expect a higher error rate. 

\begin{table}[]
\centering
\caption{Quantitative results of our and competing methods on FFHQ validation dataset for mask ranges between 0-1. }
\resizebox{1.0\linewidth}{!}{
\begin{tabular}{|l|c|c|c|c|}
\hline
Models                & {FID}  \ \ $\Downarrow$   & LPIPS  \ \ $\Uparrow$  &  U-IDS $\Uparrow$ & P-IDS $\Uparrow$ \\ \hline
pSp \cite{richardson2021encoding}  & 8.23 & - & 14.05 & 6.60 \\
% & 18.81 &
HFGI \cite{wang2022high} & 7.66 & - & 15.63 & 7.76 \\
HyperStyle \cite{alaluf2022hyperstyle} & 7.46 & - & 16.66 & 8.55\\
% 18.94 & 0.0073 &
 \hline
 CoModGAN \cite{zhao2021large}  & 7.35 & 0.0790 &  9.78 & 3.79 \\ 
 % & 9.77 & 0.1609
 InvertFill \cite{yu2022high} & 7.45 & - & 15.83 & 7.69  \\
 DualPath \cite{wang2022dual} & 17.60 & - & 4.43 & 0.77 \\
 \hline
 Ours & \textbf{5.92} & \textbf{0.2026} & \textbf{18.43} & \textbf{10.75}  \\ 
\hline
\end{tabular}}
\label{table:results_ffhq}
\end{table}

\textbf{Datasets.} We use FFHQ human face \cite{karras2019style}, AFHQ cat, and AFHQ dog image datasets \cite{choi2020stargan}. 
We use the StyleGAN2 pre-trained models \cite{karras2020analyzing} and use their train and validation splits.
We build our extensive comparison with other works on FFHQ human face dataset following previous works.





\begin{table}[]
\centering
\caption{Quantitative results of our and competing methods on AFHQ validation datasets for mask ranges between 0-1. }
\resizebox{1.0\linewidth}{!}{
\begin{tabular}{|l|c|c|c|c|}
\hline
& \multicolumn{2}{c|}{Dog} & \multicolumn{2}{c|}{Cat} \\
\hline
Models & {FID}  \ \ $\Downarrow$   & LPIPS  \ \ $\Uparrow$  & {FID}  \ \ $\Downarrow$   & LPIPS  \ \ $\Uparrow$  \\ \hline
pSp \cite{richardson2021encoding} & 26.458 & - & 14.262 & - \\
HFGI \cite{wang2022high} & 20.576 & - & 17.007 & - \\
HyperStyle \cite{alaluf2022hyperstyle} & 23.919 & - & 13.425 & - \\
% DualPath \cite{wang2022dual}  &  & - &  &  \\
\hline
Ours & \textbf{18.890} & \textbf{0.062} & \textbf{12.275} & \textbf{0.087} \\ 
\hline
\end{tabular}}
\label{table:results_afhq}
\end{table}

% \begin{table}[]
% \centering
% \caption{Quantitative results of our and competing methods on Cat validation dataset for mask ranges between 0-1. }
% \resizebox{1.0\linewidth}{!}{
% \begin{tabular}{|l|c|c|c|c|}
% \hline
% Models & {FID}  \ \ $\Downarrow$   & LPIPS  \ \ $\Uparrow$  &  U-IDS $\Uparrow$ & P-IDS $\Uparrow$ \\ \hline
% pSp \cite{richardson2021encoding} & 14.262 & - &  &  \\
% HFGI \cite{wang2022high}  & 17.007 & - & 0.0 & 0.0 \\
% HyperStyle \cite{alaluf2022hyperstyle} & 13.425 & - &  & \\
% % DualPath \cite{wang2022dual} & 27.141  & - &  &  \\
% \hline
% Ours & 12.507 & 0.116 &  &  \\  
% Ours New* & 12.796 & 0.104 &  &   \\ 
% Ours Dilated* & 12.685 & 0.104 &  &   \\ 
% Ours Dilated Last * & 12.275 & 0.087 &  &   \\ 
% \hline
% \end{tabular}}
% \label{table:results_cat}
% \end{table}




% Figure environment removed

% comparison images
% smile removal
% Figure environment removed


% Figure environment removed


\begin{table}[t]
\caption{Quantitative results of our ablation study on FFHQ validation dataset for the mask of range 0-1.
We assign model IDs to easily refer to the model in the text and Fig. \ref{fig:ablation_div}.}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{|c|c|c|c|c|c|}
\hline
% \multicolumn{3}{c|}{Mask range}   & \multicolumn{2}{c|}{0.0-1.0}          \\ \hline
ID &   Full Recons & Gated Mi & Second Stage  &  FID   & LPIPS   \\ \hline
1 &  &  \checkmark & & 10.92 & 0.0073 \\
2&  & \checkmark &  \checkmark & 7.87 & 0.0855 \\
3&   \checkmark & & &  {20.91} & 0.2095 \\
4&    \checkmark &  \checkmark & & {16.65} & 0.1440 \\
 5&  \checkmark &  \checkmark &  \checkmark & 5.92 & 0.2026 \\
\hline
\end{tabular}}
\label{table:results_ablation}
\end{table}


% % comparison images
% % smile removal
% % Figure environment removed


\textbf{Quantitative Results.} 
We provide the quantitative results in Table \ref{table:results_ffhq}, \ref{table:results_afhq}.
We provide the LPIPS score for the models that provide diversity. 
The inversion methods specifically pSp, HFGI, and HyperStyle, that are trained for inpainting task achieves reasonable scores.
Next, we compare with image inpainting methods. 
CoModGAN trains a network from scratch for inpainting whereas InvertFill uses a pretrained StyleGAN similar to our work.
CoModGAN has a StyleGAN-like training set-up, and layers are co-modulated with sampling and encoded image features.
CoModGAN achieves diversity but is lower than our model when measured with the LPIPS score.
InvertFill, on the other hand, does not have a mechanism to provide any stochastic at all.
They achieve similar FID scores but InvertFill achieves significantly better U-IDS and P-IDS scores.
We also compare with DualPath which is also built on pretrained StyleGAN. 
DualPath trains a new generator that takes input from StyleGAN feature maps. 
We follow the setup provided in their paper and try our best on parameter tuning. 
However, we do not achieve a good FID score with DualPath. Specifically, the generated images look smooth and semantically consistent, but they do not look sharp. 
DualPath also does not provide diverse results.
We achieve significantly better results than previous methods and also achieve diversity. 



We analyze how different methods score with different difficulties of masks in Fig. \ref{fig:mask_plot}.
We set two difficulty levels, an easy one where the erased mask ratio is between (0, 0.4) and a difficult scenario where the erased mask ratio is between (0.4-1.0).
When the erased area only contains a small portion, deterministic models also achieve good results since most of the information can be encoded from the unerased parts.
InvertFill achieves even a better score than ours and they all achieve better than CoModGAN.
However, when the difficulty level increases, deterministic models struggle. 
CoModGAN, the other model that can output diverse results achieves the second-best results. 
InvertFill's FID score significantly increase. 
Our model achieves good scores in both scenarios.


% smile removal
% Figure environment removed

\textbf{Qualitative Results.} Fig. \ref{fig:results_all} shows the results of our and competing methods on the FFHQ dataset. 
As we mention in the Quantitative Results section,  DualPath outputs blurry parts on the inpainted areas in our trainings.
The pSp model achieves good semantic consistency.
However, the boundaries between the inpainted and original pixels are quite apparent.
HFGI and HyperStyle output artifacts on many of the examples.
CoModGAN and InvertFill achieve plausible predictions.
An interesting observation we make is that all other methods output similar identities for given inputs. For example, the last two row predictions of CoModGAN and InvertFill are similar to each other. 
On the other hand, since our model samples diverse predictions (more diverse than CoModGAN), our results look quite different. Our results better capture the data distribution as is also measured by FID scores.
We also present the inpainting results of our method on AFHQ cat and dog images in Fig. \ref{fig:cat}.
Our method achieves successful inpaints on these image domains as well. 




\textbf{Ablation Study.} We present the quantitative and qualitative results of our ablation study in Table \ref{table:results_ablation} and Fig. \ref{fig:ablation_div}, respectively.
During the development of our model, we track the diversity (LPIPS) and quality (FID) of our results.
LPIPS score alone does not reflect the quality of our results because random images can be generated by StyleGAN that are semantically inconsistent with valid pixels of the input image. High LPIPS scores can be obtained from those images.
In fact, during training, the LPIPS score starts from a high point and decreases as the framework learns to output consistent pixels with the unerased parts.
Therefore, it is important to achieve a reasonable FID and LPIPS together.



% comparison images


We assign IDs to each model in  Table \ref{table:results_ablation} to easily refer to them in the text and figures.
Model 1 is trained with real images to reconstruct the unerased parts together with adversarial loss.
The objective does not have the full-reconstruction image loss based on the sampled $W+$.
The model does not achieve any diversity even though the network architecture takes sampled $W+$ via the mixing network. 
The mixing network ignores the stochastic input as can be seen from Fig. \ref{fig:ablation_div}. 
In Model 2, we also add the second stage training which improves the FID results because it removes the color inconsistencies.
The diversity is negligible as can be seen from both Fig. \ref{fig:ablation_div} and Table \ref{table:results_ablation}.
Next, we compare methods that are trained with our proposed losses and training pipeline. 
Model 3 does not have the gated mixing network, instead, it employs a fully connected layer to combine encoded and sampled latent codes.
Model 4 has the gated mixing network as given in Eq. \ref{eq:gating}. 
% comparison images
% smile removal



In our experiments, we find that the proposed mixing network does a better job of combining features. 
Model 3 sometimes generates inconsistent results as can be seen in Fig. \ref{fig:ablation_div} and achieves worse FID scores.
For example, the input image may have half-erased eyeglasses and this network may not output eyeglasses for the visible parts. The gated mixing network achieves better in those scenarios.
Finally, we add the second stage of training and train skip connections to obtain our final model. 
The color discrepancies disappear after this stage of training.
Interestingly, our not-diverse model - Model 1 achieves better scores than our diverse model - Model 4. 
However, when second-stage training is added, the diverse model achieves better results.
That is because the diverse model in the first stage has more difficulties outputting perfectly semantically consistent pixels but it outputs meaningful and diverse parts.
Especially when the erased parts are large, the diverse model does a better job as was discussed for Fig. \ref{fig:mask_plot} because it augments the encoded features with sampled ones. 
The color discrepancies are resolved with the second stage of training bringing the diverse model to a better score than the non-diverse model. 











\textbf{Editing Results.}
For our editing results, we first encode the erased image and randomly sample a $W+$ to our mixing network. We edit the output of the mixing network with InterfaceGAN directions \cite{shen2020interpreting}.
Editing results are shown in Fig. \ref{fig:editing_results}.
We show smile, bangs, and beard addition results as well as image edits with blush eyebrows, blonde hair, and lipstick addition.
Our model can successfully both inpaint and edit erased images thanks to the semantically rich feature representations of the StyleGAN model bringing more capabilities under one framework.



