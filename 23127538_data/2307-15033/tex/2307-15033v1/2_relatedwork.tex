\section{Related Work}


\textbf{GAN Inversion and Editing.} Recently, extensive research is conducted on image editing tasks both via end-to-end trained image translation networks \cite{starganv2,xiao2018elegant,zhang2018generative,li2021image,gao2021high,dalva2022vecgan} and latent space manipulation of pretrained GANs \cite{creswell2018inverting, roich2022pivotal, zhu2020domain, tov2021designing, wang2022high}.
Pretrained GANs, especially StyleGAN-based methods \cite{karras2019style, karras2020analyzing} are shown to organize their latent space in a semantically meaningful way with disentangled representations.
This feature enables them to achieve various edits that are beyond what annotated datasets offer \cite{voynov2020unsupervised, wu2021stylespace, patashnik2021styleclip, harkonen2020ganspace, chen2022exploring}. 
However, the challenge in using these pretrained GANs for real image editing is that one needs to invert images to the natural latent space of GANs and there is known to be a trade-off between reconstruction fidelity of input images and their editability \cite{zhu2020domain, richardson2021encoding,  tov2021designing}. 
Different encoders and training schemes are proposed to tackle the inversion problem with great improvements and the inversion problem is still an active research area \cite{wang2022high, alaluf2021restyle, alaluf2022hyperstyle, pehlivan2022styleres}. 
This work aims to solve a more challenging inversion problem with input images that have missing pixels. This task is referred to as image inpainting or image completion and to the best of our knowledge, our work is the first to explore it with pretrained StyleGAN with the high reconstruction and editability goal.



\textbf{Inpainting.} Inpainting missing pixels especially large-scale holes receives a significant amount of attention from both the research community and industry.
This task requires filling in missing pixels in a semantically meaningful and multi-modular way.
However, most of the proposed methods are deterministic and struggle with large-scale holes \cite{pathak2016context, liu2018image, yu2019free, li2020recurrent, liu2022partial}.
Recently, CoModGAN \cite{zhao2021large} is proposed with an architecture and training pipeline similar to StyleGAN but with skip connections from the encoder to the generator to pass the valid pixels to the generator without a bottleneck. 
CoModGAN achieves diverse results.
Diffusion models have also shown great success in diverse image inpainting \cite{lugmayr2022repaint, rombach2022high, yildirim2023inst}.
Nevertheless, none of these methods can achieve diverse inpainting and enable user-controlled editing of images with one network simultaneously which is the goal of our work.

