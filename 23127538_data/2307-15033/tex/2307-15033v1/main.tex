\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{enumitem}


% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{4957} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Diverse Inpainting and Editing with GAN Inversion}


\author{Ahmet Burak Yildirim$^{*}$ \qquad Hamza Pehlivan\thanks{Joint first authors, contributed equally.}  \qquad
Bahri Batuhan Bilecen  \qquad
Aysegul Dundar \\
Bilkent University\\
\tt\small \{a.yildirim, hamza.pehlivan, batuhan.bilecen\}@bilkent.edu.tr\\
\tt\small adundar@cs.bilkent.edu.tr\\}

\maketitle

\begin{abstract}

Recent inversion methods have shown that real images can be inverted into StyleGAN's latent space and numerous edits can be achieved on those images thanks to the semantically rich feature representations of well-trained GAN models. 
However, extensive research has also shown that image inversion is challenging due to the trade-off between high-fidelity reconstruction and editability.
In this paper, we tackle an even more difficult task, inverting erased images into GAN's latent space for realistic inpaintings and editings. 
Furthermore, by augmenting inverted latent codes with different latent samples, we achieve diverse inpaintings.
Specifically, we propose to learn an encoder and mixing network to combine encoded features from erased images with StyleGAN's mapped features from random samples. 
To encourage the mixing network to utilize both inputs, we train the networks with generated data via a novel set-up.
We also utilize higher-rate features to prevent color inconsistencies between the inpainted and unerased parts.
 We run extensive experiments and compare our method with state-of-the-art inversion and inpainting methods. Qualitative metrics and visual comparisons show significant improvements.

\end{abstract}

\input{1_introduction}
\input{2_relatedwork}
\input{3_method}
\input{4_experiments}
\input{5_conclusion}

\section*{Acknowledgement}

This work has been funded by The Scientific and Technological Research Council of Turkey (TUBITAK), 3501 Research Project under Grant
No 121E097. 



\input{6_supp_release}


{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

%\newpage
%\input{6_supp}

\end{document}