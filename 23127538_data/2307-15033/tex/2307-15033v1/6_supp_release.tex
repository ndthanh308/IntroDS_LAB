
% %%%%%%%%% BODY TEXT
% In this supplementary document, we provide:

% \begin{enumerate}
%   \item Architecture and training details of the two-stage framework we propose.
%   \item Visual comparison with pSp \cite{richardson2021encoding}, HFGI \cite{wang2022high}, HyperStyle \cite{alaluf2022hyperstyle}, CoModGAN \cite{zhao2021large}, InvertFill \cite{yu2022high} and DualPath \cite{wang2022dual}. 
%   \item Visual inpainting results on AFHQ Cat and Dog datasets.
%   \item Semantic editing results on FFHQ dataset.
% \end{enumerate}

\appendix






\section{Architecture Details}
\label{sec:training}


The final architecture is given in Fig. \ref{fig:sup_overall}. We follow a two-stage training pipeline. In the first stage, we train the Encoder and Mixing network. The architectures of them are as follows:

\textbf{Encoder ($E$).}
We adopt the encoder architecture from pSp \cite{richardson2021encoding} with minor modifications. First, we increase the first layer input channel number from $3$ to $4$ for taking the mask as an additional input. Then, we disable the normalization layers since we observe they decrease the performance of the model given that many input pixels may be $0$ due to removal of them.

\textbf{Mixing Network ($Mi$).}
We equip the mixing network with a neural network and gating mechanism.

% \begin{equation}
% \label{eq:gating}
% \begin{split}
%     \text{W}^{comb}, g = \text{NN}( \text{W}^{enc} ,  \text{W}^{rand} )\\ 
%     \text{W}^{out} = \sigma(g) \cdot \text{W}^{comb} + (1-\sigma(g)) \cdot \text{W}^{rand} 
% \end{split}
% \end{equation}

The dimensions of $\text{W}^{enc}$ and $\text{W}^{rand}$ are both $14\times512$.
For the neural network $\text{NN}$, we use $14$ fully connected layers.
Each of them takes a style vector from $\text{W}^{enc}$ and $\text{W}^{rand}$ that are in $1\times512$ dimension. Fully connected layers have input and output dimensions of $512$.


% Figure environment removed

In the second stage training, we set a new encoder which we refer to as Skip Encoder (S) in Fig. \ref{fig:sup_overall}. In the second stage training, we set the first Encoder frozen. We are interested in learning high-rate features and feed them to StyleGAN generator to achieve better fidelity to input image. 
The architecture is as follows: 

\textbf{Skip Encoder ($S$).}
The Skip Encoder takes input from the final output of the first stage model. We additionally feed the mask and erased input image to the Skip Encoder ($S$). 
They are concatenated and are fed to the $S$.
% There are skip connections between S and G at $32$, $64$ and $128$ resolutions. 
$S$ starts with a convolution layer to increase the channel size from $7$ to $32$ with a filter size of $3\times3$ and padding of $1$. The $32\times 256 \times 256$ feature maps are fed into residual blocks.
The residual blocks consist of three residual layers. Each residual layer consists of two convolution layers with batch normalization and parametric ReLu activation.
Each residual block downsamples the input resolution to half in its first residual layer using max pooling layer.
At each block, the channel size increases.
The Skip Encoder decreases the resolution to $32\times32$ at the end via 3 residual blocks.
The channel size at each block are as follows $48$, $64$, $96$ in the downsampling residual layers, respectively.
After we extract the $32$, $64$, and $128$ resolution feature maps, we pass them on $2$ more convolution blocks to retrieve skip connection addition ($G_{add}$) and multiplication ($G_{mult}$) maps, whose channels are compatible with the StyleGAN at respecting resolution. We do not have an activation function for $G_{add}$, but we have a sigmoid function for extracting $G_{mult}$. Lastly, StyleGAN generator features ($G_f$) are changed as follows:
\begin{equation}
\label{eq:skip}
    G_f = G_f + G_f * G_{mult} + G_{add}
\end{equation}




\section{Training Details.} We train the first stage for $500$k iterations with batch size of $8$ on two GPUs. We use learning rate of $1 \times 10^{-4}$ for all networks. We halve the learning rates at each $50$k iterations. We use the overall objectives given below to optimize the parameters of the Encoder ($E$) and the mixing network ($Mi$) as was also given in the main paper. 
We also use the same objective to optimize the parameters of $S$ in the second stage. 


For both of the training stages, we use the same objective given in Eq. \ref{eqn:full_loss} and following hyperparameters, $\lambda_{a}=8\times10^{-2}$, $\lambda_{r1}=1$, and $\lambda_{r2}=1$.  
We set the pixel-wise and VGG reconstruction loss coefficients as $1$ and $5\times10^{-5}$, respectively for both ${L}_{rg}$ and ${L}_{gg}$.



% \section{Additional Results}
% \label{sec:results}

% We provide visual results for:
% \begin{enumerate}
%     \item FFHQ inpainting results in Fig.  \ref{fig:face_comparison_supp}.
%     \item FFHQ inpainting and editing results in  Fig. \ref{fig:editing_results_supp}.
%     \item AFHQ Cat and Dog inpainting results in Fig. \ref{fig:cat_sup}.
% \end{enumerate}

% \newcommand{\interpfigt}[1]{% Figure removed}

% % Figure environment removed

% \newcommand{\interpfigtS}[1]{% Figure removed}

% % Figure environment removed




% comparison images
% smile removal
% % Figure environment removed

























