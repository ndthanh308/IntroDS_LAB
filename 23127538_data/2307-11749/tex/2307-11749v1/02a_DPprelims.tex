\longversion{\section{Differential Privacy}\label{sec:privacy}

In this work, we will consider an algorithm that satisfies \devicenospace-level differential privacy (DP) in the aggregate model \emph{and} the local model of differential privacy. 
We focus on \devicenospace-level DP, which protects against a user changing all of the data points associated to them. Our primary privacy guarantee is the aggregate privacy guarantee, which will be specified ahead of time. \longversion{Local differentially private guarantees are achieved locally on a user's device through the use of a local randomizer.}
The local privacy guarantee will be set to be the largest epsilon such that the final algorithm satisfies the required aggregate privacy guarantee (i.e. we will not put constraints on the local privacy guarantee).

Local differentially private guarantees are achieved locally on a user's device through the use of a local randomizer.

\begin{definition}[Local Randomizer \cite{DR14,kasiviswanathan2011can}]\label{LDP}
Let $\A: \worddom \to \cY$ be a randomized algorithm mapping a data entry in $\worddom$ to an output space $\cY$.  
The algorithm $\A$ is an $\epsilon$-DP local randomizer if for all pairs of data entries $d,d'\in\worddom$, and all events $E\subset\cY$, we have $$
- \eps \leq \ln\left(\frac{\Pr[\A(d) \in E ]}{\Pr[\A(d') \in E ]}  \right)\leq \eps.
$$
\end{definition}
The privacy parameter $\eps$ captures the \emph{privacy loss} consumed by the output of the algorithm. Differential privacy for an appropriate $\eps$ ensures that it is impossible to confidently determine what the individual contribution was, given the output of the mechanism. 

In general, differential privacy is defined for algorithms with input databases with more than one record. In the local model of differential privacy, algorithms may only access the data through a local randomizer so that no raw data leaves the device. For a single round protocol, local differential privacy is defined as follows:

\begin{definition}[Local Differential Privacy \cite{kasiviswanathan2011can}]\label{localDP}
Let $\A: \worddom^n \to \cZ$ be a randomized algorithm mapping a dataset with $n$ records to some arbitrary range $\cZ$.  The algorithm $\A$ is $\epsilon$-local differentially private if it can be written as $\A(d^{(1)}, \cdots, d^{(n)}) = \phi\left(\A_1(d^{(1)}), \cdots, \A_n(d^{(n)}) \right)$ where the $\A_i: \worddom \to \cY$ are $\epsilon$-local randomizers for each $i \in [n]$ and $\phi: \cY^{n} \to \cZ$ is some post-processing function of the privatized records $\A_1(d^{(1)}),\cdots, \A_n(d^{(n)})$.  Note that the post-processing function does not have access to the raw data records.
\end{definition}

We say a multi-round algorithm $\A$ is $\eps$-DP in the local model if it is the composition of single round algorithms which are DP in the local model, and the total privacy loss of $\A$ is $\eps$-DP. More generally, we can say that an interactive algorithm is locally differentially private if the transcript of all communication between the data subjects and the curator is differentially private~\cite{joseph2019role}. Since aggregate differential privacy is our primary privacy guarantee, when we refer to local privacy guarantees, they will be for a single round of communication.


\longversion{In aggregate DP, we assume the existence of an aggregation protocol that sums the local reports before they are released to the analyst. The aggregation protocol guarantees that the analyst does not receive anything about the locally DP reports \emph{except their sum}\footnote{The aggregate model of DP is a derivative of the more general and common shuffle model of differential privacy introduced in \cite{ErlingssonFMRTT19, CheuSUZZ19}.}.} 
%A multi-round algorithm $\A$ is $(\eps,\delta)$-DP in the aggregate model if it is the composition of single round algorithms which are DP in the aggregate model, and the total privacy loss of $\A$ is $(\eps,\delta)$-DP\footnote{One can formulate more complex notations of the aggregate and local models of DP for multi-round algorithms (e.g. definitions that allow for each for users not to participate in every round), for a more in-depth discussion see \cite{Jain2021ThePO}.\am{say something about these algorithms not really being feasible?}}.

\begin{definition}\label{defnaggregateDP}
A single round algorithm $\A$ is $(\eps, \delta)$-DP in the aggregate model if the output of the aggregation protocol on two datasets that differ on the data of a single individual are close. Formally, an algorithm $\A:\cD^n\to\cZ$ is $(\eps, \delta)$-DP in the aggregate model if the following conditions both hold:
\begin{itemize}
\item it can be written as $\A(d^{(1)}, \cdots, d^{(n)})=\phi(\aggregator(f(d^{(1)}), \cdots, f(d^{(n)}))$ where $f:\cD\to\cZ$ is a randomized function that transforms that data, $\aggregator$ is an aggregation protocol, and $\phi:\cY^n\to\cZ$ is some post-processing of the aggregated report
\item for any pair of datasets $D$ and $D'$ that differ on the data of a single individual, and any event $E$ in the output space,
\[
\Pr(\A(D)\in E)\le e^{\eps} \Pr(\A(D')\in E)+\delta.
\]
\end{itemize}
Note that the post-processing function takes the aggregation as its input and does not have access to the individual reports.
\end{definition}

\longversion{When each user uses a local randomizer with a local DP guarantee to send their data to the aggregation protocol, the privacy guarantee in the aggregation model can be bounded by a quantity that is a function of both $\epslocal$, the privacy guarantee in the local model, and $n$, the number of users that participate in the aggregation protocol~\cite{ErlingssonFMRTT19, CheuSUZZ19}.} In our experimental results, we will bound the aggregate DP epsilon by the numerical bound on privacy amplification by shuffling due to \cite{feldman2023stronger}. 
Given an expected number of users, the number of iterations, and the desired aggregate privacy guarantee, we solve for the largest local epsilon that will achieve the given aggregate privacy guarantee. Since the aggregate privacy guarantee is our primary privacy guarantee, we do not put an upper bound on our local epsilon.  

As with the local model, we will say a multi-round algorithm $\A$ is $\eps$-DP in the aggregate model if it is the composition of single round algorithms which are DP in the aggregate model, and the total privacy loss of $\A$ is $\eps$-DP.
In order to analyse the privacy loss over multiple iterations, we will use a combination of the advanced composition theorem~\cite{DRV, Kairouz:2017} and composition bound in terms of R\'enyi differential privacy~\cite{DLDP, mironov2017renyi, canonne2020discrete}.
\longversion{Table~\ref{tab:eps} in Appendix~\ref{appendix:privacy} demonstrates how the local epsilon increases with the number of iterations, and the desired aggregate privacy guarantee.}}





