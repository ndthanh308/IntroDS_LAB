\section{Introduction}
\label{sec:intro}
\looseness -1
Gaining insight into population trends allows data analysts to make data-driven decisions to improve user experience.
Heavy hitter detection, or learning popular data points generated by users, plays an important role in learning about user behavior.
A well-known example of this is learning ``out-of-vocabulary" words typed on keyboard, which can then be used to improve next word prediction models. 
%The problem has been studied extensively in the setting where each user has a single data point, and one has no privacy concerns (see e.g. \cite{CHARIKAR20043, CORMODE200558}). However, t
This data is often sensitive and the privacy of users' data is paramount. When the data universe is small, one can obtain private solutions to this problem by directly using private histogram algorithms such as RAPPOR~\cite{erlingsson2014rappor}, and PI-RAPPOR~\cite{feldman2021lossless}, and reading off the heavy-hitters. However, when the data universe is large, as is the case with ``out-of-vocabulary" words, these solutions result in algorithms with either very high communication, or very high server side computation, or both.
Prefix-tree based iterative algorithms can lower communication and computation costs, while maintaining high utility by efficiently exploring the data universe for heavy hitters.
They also offer an additional advantage in the setting where users have multiple data points by refining the query in each iteration\longversion{ using the information learned thus far}, allowing each user to select amongst those data points which are more likely to be heavy hitters.

\looseness -1
In this work, we consider an iterative federated algorithm for heavy hitter detection in the aggregate model of differential privacy (DP) in the presence of computation or communication constraints. In this setting, each user has a private dataset on their device. In each round of the algorithm, the data analyst sends a query to the set of participating devices, and each participating device responds with a  \textit{response}, which is a random function of the private dataset of that user. These \textit{responses} are then summed using a secure aggregation protocol, and reported to the data analyst. The analyst can then choose a query for the next round adaptively, based on the aggregate results they have seen so far. The main DP guarantee is a user-level privacy guarantee on the outputs of the secure aggregator, accounting for the privacy cost of \emph{all} rounds of iteration. 
Our algorithm will additionally be DP in the local model of DP (with a larger privacy parameter)\footnote{A potential architecture for running iterative algorithms in this model of privacy is outlined in \cite{mcmillan2022private}.}.
\longversion{We do not assume that the set of participating devices is consistent between rounds. }

\looseness -1
In the central model of DP, there is a long line of work on adaptive algorithms for heavy hitter detection in data with a hierarchical structure such as learning popular $n$-grams~\cite{cormode2012differentially, Qardaji:2012, Song:2013, bagdasaryan2021towards, Kim2021DifferentiallyPN, mcmillan2022private}. These interactive algorithms all follow the same general structure. Each data point is represented as a sequence of data segments $d=a_1a_2\cdots a_r$ and the algorithm iteratively finds the popular values of the first segment $a_1$, then finds popular values of $a_1a_2$ where $a_1$ is restricted to only heavy hitters found in the previous iteration, and so on. 
This limits the domain of interest at each round, lowering communication and computation costs.
The method of finding the heavy hitters in each round of the algorithm varies in prior work, although is generally based on a DP frequency estimation subroutine. One should consider system constraints (communication, computation, number of participating devices, etc.) and the privacy model when choosing a frequency estimation subroutine. In this work, we will focus on using one-hot encoding with binary randomized response (inspired by RAPPOR~\cite{erlingsson2014rappor}) as our DP frequency estimation subroutine. Since we are primarily interested in algorithmic choices that affect the iterative algorithm, we believe our findings should be agnostic to the choice of frequency estimation subroutine used. 

We explore the effect on utility of different data selection schemes and algorithmic optimizations. 
We refer to our algorithm as \textit{Optimized Prefix Tree} (\textit{$\ouralgorithm$}). Our contributions are summarised below:

    \textbf{Adaptive Segmentation.} We propose an algorithm for adaptively choosing the segment length and the threshold for keeping popular prefixes. In contrast to prior works that treat the segment length as a hyperparameter, our algorithm chooses these parameters in response to user data from the previous iteration and attempts to maximize utility (measured as the fraction of the empirical probability distribution across all users captured by the returned heavy hitters), while satisfying any system constraints. We find that our method often results in the segment length varying across iterations, and outperforms the algorithm that uses a constant segment length. We also design a threshold selection algorithm that adaptively chooses the prefix list for the subsequent round. This allows us to control the false positive rate\longversion{ (the likelihood that a data point is falsely reported as a heavy hitter)}.
    
    \textbf{Analysis of the effect of on-device data selection mechanisms.} We explore the impact of interactivity in the setting where users have multiple data points. We observe empirically that when users have multiple data points, interactivity can improve utility, even in the absence of system constraints. In each round, users choose a single data point from their private data set to (privately) report to the server.
    The list of heavy hitters in the previous iteration provides a \emph{prefix list}, so users will only choose a data point with one of the allowed prefixes. If a user has several data points with allowed prefixes, then there are several selection rules they may use to choose which data point to report. Each user's private dataset defines an empirical distribution for that user. 
    We find that when users sample uniformly randomly from the support of their distribution (conditioned on the prefix list) then the algorithm is able to find more heavy hitters than when they sample from their empirical distribution (again conditioned on the prefix list). 

    \textbf{Analysis of the impact of inclusion of deny list.} Under the constraint of user-level differential privacy, each user is only able to communicate their most frequent data points, and less frequent data points are down weighted. We explore the use of a \emph{deny list} that asks users not to report data points that we already know are heavy hitters. In practice, a deny list may arise from an auxiliary data source, or from a prior run of the algorithm. Our analysis indicates even when the privacy budget is shared between multiple rounds of the algorithm, performing a second round equipped with a deny list improves performance.

The rest of the paper is organized as follows. In Section~\ref{sec:related} we discuss some of the prior works in privacy-preserving heavy hitters detection. Section~\ref{sec:privacy} explains the privacy primitives we used in this work. In Section~\ref{sec:alg} we elaborate the details of our prefix tree algorithm. Section~\ref{sec:post} explains the post-processing methods and the theoretical analysis behind it. Section~\ref{sec:expts} demonstrates the experimental results and in Section~\ref{sec:conclusion} we discuss the findings of our experiments. 
