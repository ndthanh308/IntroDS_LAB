\section{Algorithm}\label{sec:alg}
In this section, we describe our proposed algorithm $\privhh$. \longversion{This algorithm privately identifies the heavy hitters using $\numiters$ iterations where each \device sends a locally differentially private report in each round and the local reports are aggregated before reaching the server. We will first discuss an outline of the high-level algorithm.}
In Section~\ref{sec:post}, we will discuss our proposals for adaptively setting the various parameters and subroutines present in the high-level algorithm. Our focus in the experimental section to follow will be to explore these choices, and provide some guidelines on how they should be chosen. Note that while our algorithm is run over multiple iterations, it is well-suited to the federated setting since it does not require every user to be present at every iteration.


We represent the system constraints as a constraint of the size of the data domain for any single iteration, denoted by $\complimit$. This bound may be a result of communication constraints, as is the case for the local randomizer we will use (one-hot encoding with binary randomised response), or computational constraints for the server-side algorithm, as in PI-Rappor~\cite{feldman2021lossless} or Proj-Rappor~\cite{feldman2022private}.

\textbf{Notation.}
\longversion{Let $\numusers$ be the total number of users. }\longversion{Let each user $i\in[N]$ have $\ndp_i$ data points 
%sampled from distribution $\userdist_i$ \am{do we really need this? Do we ever talk about the underlying distribution?} 
denoted by $\datap_{i,1},\dots,\datap_{i,n_i}$ from domain $\worddom \subset  \worduniv = \alphadom^r$, where $\worduniv$ denotes the universe of allowable data points, and $\alphadom$ denotes an alphabet from which all data points are built.} For each user $i\in[N]$, let $\empuserdist_i$ denote the empirical distribution of user $i$'s data and $\empuserdist~\coloneqq~\frac{1}{\numusers}\sum_{i \in [\numusers]}\empuserdist_i$ denote the global empirical distribution
\longversion{\footnote{In the multiple data points per \device setting, there are several other natural ways of defining the global empirical distribution. For example, one may define the frequency of a data point $d$ to be the number of users who have the word $d$ in their support. We briefly explore this metric in Appendix~\ref{appen:expts-details}.}}. Let each data point $\datap\in \alphadom^r$ be of a fixed length $\totlen$.


\subsection{Private Heavy Hitters Algorithm} 


\longversion{$\privhh$ proceeds in iterations with the goal of efficiently exploring the large data domain to detect heavy hitters, by exploiting the hierarchical structure by sequentially learning the most popular prefixes, and only expanding on these popular prefixes in the next iteration.} \longversion{We give pseudo-code for our proposed algorithm $\privhh$ in \Cref{alg:privhh}.}
At every iteration $t$, the server sends the devices a list of live prefixes $\prefixlist_t$ of length $\preflent$, a deny list $\denylist$, and a segment length $\seglent$. The devices then use a data selection mechanism to choose a data point that is not in the deny list $\denylist$ and whose $\preflent$ length prefix belongs in $\prefixlist_t$, they then (privately) report back the length $\preflent+\seglen_t$ $(\leq r)$ prefix of the chosen data point. The server uses these local reports to define  $\prefixlist_{t+1}$ (consisting of prefixes of length $\preflent+\seglen_t$) and the segment length $\seglen_{t+1}$ for the next round.  
We will use $\numiters$ to denote the number of iterations and $\epslocal$ to be the local DP parameter for a single iteration. At the end of $\numiters$ rounds, our algorithm outputs a set of heavy hitters that includes the prefixes found in the last iteration ($\prefixlist_{T+1}$) and the contents of $\denylist$. We use $(\eps_{\rm agg}, \delta)$-DP to refer to the privacy parameters of our algorithm in the aggregate model. 

\longversion{We may also remove some prefixes during earlier iterations which we add into the final set of heavy hitters. These data points are stored in the ``discovered list", $\discoveredlist$. We'll discuss this further in a subsequent section, but this turns out to be a useful improvement when the natural encodings of data points can vary in length. For example, when using Huffman encoding.} 


\begin{algorithm}
\caption{\label{alg:privhh} Prefix tree based heavy hitter algorithm $(\privhh)$}
\begin{algorithmic}[1]
\STATE \textbf{Input}: $\numiters$: number of iterations, $\epslocal$: Local privacy parameter, $\datasamp$: Data selection mechanism, $\complimit$: Bound on the dimension, $\falseratio$: false positive ratio, $\eta$: extra parameters to pass to $\serverper$, $\denylist$: deny list.
\STATE \textbf{Output}: $\prefixlist_{T+1}$: Set of Heavy Hitters
\STATE $\seglen_1 \gets \lfloor \log(\complimit) \rfloor$, $\prefixlist_1=\emptyset, \discoveredlist=\emptyset$ \textit{{\color{darkgreen} //  Initialize segment length and prefix list }}
% \textit{{\color{darkgreen} //  $\seglen_1 = \seglen$ (fixed) for the basic version}}
\FOR{$t \in [T]$}
\STATE $\usercompdataset_t \gets \emptyset$ \textit{{\color{darkgreen} //  $V_t$ will be the set of all the device responses that is sent to the aggregation protocol }} 
\FOR{$i \in [\numusers]$}
\STATE $\usercompdata_i \gets \devicehh_i(\epsilon_l, \seglen_t, \prefixlist_{t}, \denylist, \datasamp)$
\STATE $\usercompdataset_t \gets \usercompdataset_t \cup \usercompdata_i$
\ENDFOR 
\STATE $V_t \gets \texttt{AggregationProtocol}(V_t)$ \textit{{\color{darkgreen} //  Device responses are aggregated }}
\STATE $\queryset_t \gets \prefixlist_{t} \times \alphadom^{\seglen_t}$ \textit{{\color{darkgreen} // Data domain for iteration $t$ }}
\STATE $\prefixlist_{t+1}, \seglen_{t+1}, \discoveredlist \gets \serverper(\usercompdataset_t, \queryset_t, \discoveredlist, \epsilon_l, \falseratio, \eta)$ \textit{{\color{darkgreen} //  Server returns prefix list and segment length for next round }}
%\STATE $\preflenvar{t + 1} \gets \preflent + \seglent$
\STATE Send $\mathcal \prefixlist_{t+1}$, and $\seglen_{t+1}$ to all the devices
\ENDFOR
\STATE \textbf{return} $\prefixlist_{T+1} \cup \denylist \cup \discoveredlist$
\end{algorithmic}
\end{algorithm}


\subsection{Device Algorithm} 

The device first uses the data selection mechanism \datasamp~to choose a data point from their on-device dataset that is not in the deny list, and whose $\preflent$-length prefix is in $\prefixlist$. Then we pass the $\preflent + \seglen_t$-length prefix of the chosen data point to a $\epslocal$-local DP algorithm\longversion{ $\compressor$} and send the privatized output to the aggregation protocol. \Cref{alg:client} gives more details for the device-side algorithm. 

\paragraph{The Local Randomizer} In our experiments we use one-hot encoding with asymmetric binary randomized response (denoted by \OHEBRRnospace) as the local randomizer. For details of this randomizer, see Appendix A of \cite{mcmillan2022private}.\longversion{ In practice one could use PI-RAPPOR~\cite{feldman2021lossless} or Proj-RAPPOR ~\cite{feldman2022private} for better communication-computation trade-offs (See \Cref{appen:pi-rappor}). Since the utility guarantees of these mechanisms are very similar to \OHEBRRnospace, we expect our findings on \OHEBRR to be directly applicable when using PI-Rappor or Proj-Rappor.}
\longversion{

Under the constraint of local differential privacy, a participating \device must still send a local report to the aggregation protocol, even if the \device data contains no data points with a prefix in the prefix list. In our experiments if a \device  has no data point to communicate, then it encodes this as the all-zeros vector and uses asymmetric randomized response on the coordinates of the all-zeros vector. This has the same effect as adding a special element $\perp$ to the data domain and having devices report the $\perp$ element if they have no data points to report.}


\paragraph{Data Selection} We consider two data selection mechanisms. 
In \emph{weighted selection}, each device $i$ selects a data point by sampling from its empirical distribution $\empuserdist_i$ conditioned on the datapoint having a prefix in $\prefixlist_{t}$ and not being in $\denylist$.
\longversion{Formally, we define the pdf of $\empuserdist_i$ as follows: let $f_i(\datap_j)$ represent the frequency of $d_j$ in the private data set on $device_i$ so $\sum_{j=0}^{u_i} f_i(\datap_j) = 1$ where $u_i$ is the number of unique data points on $device_i$.
In \emph{unweighted/uniform selection}, each device $i$ selects a data point by sampling uniformly from those points in the support of $\empuserdist_i$ which have a prefix in $\prefixlist_{t}$ and are not in $\denylist$. Note that the data selection mechanism does not impact the privacy guarantees.
$\datap_{i,1}$}


\begin{algorithm}
\caption{\label{alg:client} Device side algorithm $(\devicehh)$}
\begin{algorithmic}[1]
\STATE \textbf{Input:} $\epslocal$: Local privacy parameter, $\preflen$: prefix length, $\seglen_t$: segment length, $\prefixlist$: allowed prefix list, $\denylist$: deny list, $\datasamp$: Function to choose a datapoint from the data 
\STATE \textbf{Param:} $\dataset$: \device dataset
\STATE \textbf{Output:} $v$: Privatized output
\STATE $\dataset = \{\datap \in \dataset \mid \datap[0:\preflen] \in \prefixlist \wedge  \datap \notin \denylist\}$
\IF{$\dataset == \phi$} 
    \STATE $\datap \gets \perp$ \textit{{\color{darkgreen} // We reserve a special data element for users that have no eligible data points to report. }}
\ELSE
\STATE $\datap \gets \datasamp(\dataset)$
\ENDIF

\STATE $\usercompdata \gets \compressor(\datap[0:{\preflen + \seglen_t}]; \epsilon_l)$ 
\STATE \textbf{return} $\usercompdata$
\end{algorithmic}
\end{algorithm}

\subsection{Server Algorithm}
The server receives the aggregated privatized responses\longversion{ from the previous iteration, the prefix list and segment lengths from the previous iteration, and the local DP parameter $\epslocal$}. The general outline of the server-side algorithm is given in \Cref{alg:server-per}.

The first step of this process is to compute an estimated frequency for the data domain of the last iteration $\prefixlist_{t}\times A^{\seglent}$.
Given the aggregated privatized results \longversion{and the local epsilon, for every element, $d\in \prefixlist_{t} \times A^{\seglent}$, of the data domain,} the server can compute an estimate $\freqest(d)$ of the number of devices who sent the data point $d$ in the last iteration. The prefix list selection algorithm aims to keep as many of the elements\longversion{ $d\in \prefixlist_{t} \times A^{\seglent}$} such that $\freqest(d)>0$ as possible, while minimizing the number of ``false positives" in the prefix list (data elements which do not match the selected data point for \emph{any} device).
When using \OHEBRRnospace, each estimate $\freqest(d)$ is unbiased and the noise induced by the privatization scheme is approximately Gaussian with standard deviation $\sigma$, where $\sigma$ is a function of $\epslocal$ and the number of participating devices. Due to the noise, if we were to define the $\prefixlist_{t+1}$ to be all elements such that $\freqest(d)>0$, the false positive rate would be too high. 
Instead, we use a threshold multiplier $\tau$ such that the prefix list $\prefixlist_{t+1}$ contains all the elements such that $\freqest(d)\ge \tau\sigma$. This threshold should be chosen to be as small as possible while ensuring that the fraction of reported elements that are false positives does exceed a specified threshold denoted $\falseratio$.

\longversion{In the setting where there are natural encodings of the data for which different data points have different lengths (e.g. \longversion{when an encoding scheme such as }Huffman encoding\longversion{ is used}) we note a improvement that can be made when choosing the prefix list $\prefixlist_{t+1}$. In these cases, some data points may be complete before the end of the algorithm.}
\longversion{To avoid unnecessary communication and utilize the system capacity, an end character symbol can be used at the end of each encoding so that the server can detect when a data point is ``complete". After aggregating the data on the server if there are prefixes that reach the end character, they are \longversion{added to the list of already discovered prefixes and }removed from the prefix list sent to the devices for the next iteration. They are then added to the set of heavy hitters in the final output.}

In most of the prior works, the segment length and threshold $\tau$ are treated as hyperparameters that need to be tuned. Tuning hyperparameters is notoriously hard in the federated setting. In Section~\ref{sec:post} we will discuss our adaptive algorithms for choosing these parameters.
Our proposed algorithms choose these parameters in response to user data, without using additional privacy budget.


\begin{algorithm}
\caption{\label{alg:server-per} Server side algorithm per round $(\serverper)$}
\begin{algorithmic}[1]
\STATE \textbf{Input}: $\usercompdataset_t$: Aggregated sum of devices responses, $\queryset_t$: Data domain of iteration $t$, $\epslocal$: Local privacy parameter, $\falseratio$: False positive ratio, $\dpmult_0$: Initialization of threshold and $\eta$: Extra parameters for $\prunehh$
\STATE \textbf{Output}: $\prefixlist$: Heavy hitters list 
\STATE $\freqest(\cdot) \gets \decompressor(\usercompdataset_t,\queryset_t; \epslocal)$ \textit{{\color{darkgreen} // Takes the aggregated privatized responses and computes an estimate of the frequency of every data element. }}
\STATE $\sigma \gets \sqrt{\var (\decompressor)}$ \textit{{\color{darkgreen} // Computes an upper bound on the standard deviation of the frequency estimate for $d$ }}
\STATE $\prefixlist_{t+1} \gets \prunehh(\queryset,\freqest(\queryset), \dpmult_0, \falseratio, \sigma, \eta)$ 
\STATE $\prefixlist_{t+1}, \discoveredlist \gets \texttt{RemoveFinished}(\prefixlist_{t+1},\discoveredlist)$ \textit{{\color{darkgreen} // Removes any of the discovered prefixes which are "complete" data points and adds them to the discovered list. }}
\STATE $\seglen_{t+1}=\max\{\ell\;|\; |\prefixlist_{t+1}|\times 2^{\ell}\le \complimit\}$ \textit{{\color{darkgreen} // Adaptively chooses the maximum segment length }}
% \textit{{\color{darkgreen} // $\prefixlist = \{q \in \queryset \mid \freqest[q] \geq \dpmult \sigma \}$ for fixed $\dpmult$ in the basic version}}
\STATE \textbf{return} $\prefixlist_{t+1}, \seglen_{t+1}, \discoveredlist$
\end{algorithmic}
\end{algorithm}

