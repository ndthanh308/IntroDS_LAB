\section{Adaptive Segmentation Exploration}\label{appendix:tuning}

Here, in addition to showing discovered frequencies and discovered counts, we show another metric which we refer to as utility loss.

Let $H = (x_1,x_2,\dots,x_{|H|})$ denote the set of heavy hitters output by an algorithm ordered by the empirical global frequency distribution $\empuserdist$, and let $x_i^*$ denote the $i^{\rm th}$ most frequent element according to the global empirical distribution $\empuserdist$, i.e. the true $i^{\rm th}$ heavy hitter. Then, we evaluate an algorithm with output $H$ by how close the total mass of $H$ is to the total mass of the true top $|H|$ heavy hitters. %This is a standard evaluation metric for differentially private heavy hitter algorithms~\cite{durfee2019practical}.

\textit{Utility Loss:} Define the weight ratio as $\weightratio{(H)} = \frac{\sum_{x \in H} \empuserdist(x)}{\sum_{i \in [|H|]} \empuserdist(x_i^*)}$, i.e. the ratio of total probability mass of private heavy hitters $H$ over the probability mass of the actual top $|H|$ heavy hitters. The goal is to maximize the $\weightratio$ to minimize the loss (1-\weightratio).

One other potential metric for evaluating these models is to use precision and recall or different versions of their combination for instance F1-Score. However, these metrics do not take into the account the frequency of discovered items. Meaning that if any iterative algorithm discovers the most frequent heavy hitters, but some of them are not in the actual most frequent heavy hitters because of a small frequency difference, precision/recall metrics are not able to capture that. In other words, they capture those as a miss which is not fair to these algorithms.

For marginal figures, to aid with visualisation in our plots, for a window size $W=50$ and a given heavy hitter set $H$, we plot for each $i$, the sum of the probabilities (according to $\empuserdist$) of the heavy hitters in the sliding window $(x_{i - W},x_{i-W + 1},\dots, x_i)$. We also plot a true histogram line representing $(x^*_{i - W},x^*_{i-W + 1},\dots, x^*_i)$ (TruHist line) as a reference of what true histogram looks like. 

\subsection{Adaptive Segmentation for Single Data Point}
In this section, we explore the effect of different parameters in the utility of $\ouralgorithm$. For simplicity in this section, we assume each \device has a single datapoint.
 

\paragraph{False Positives Ratio}

In these experiments we investigate the effect of different $\falseratio$ parameters on the utility of the final model. False positives ratio can be defined for each application. Depending on how sensitive the application is, $\falseratio$ determines ratio of number of expected false positives to the total discovered that we keep in each iteration. Please note that this parameter is application dependent. For the applications that are more tolerant to false positives this ratio can be higher. In this part we set the $\complimit = 10^7$ and $\ouralgorithm$ finishes in 4 iterations.

% Figure environment removed

As observed increasing the $\falseratio$ from 0.5 to 1 increases the number of heavy hitters detected slightly [660, 656, 678.0, 695] but increasing $\falseratio$ also increases the number of discovered false positives significantly [357, 636, 828, 1400]. Based on Figure~\ref{fig:trueratiototal}, although lower $\falseratio$ detects fewer true bins to ensure it includes fewer false positives, it detects the top most-frequent bins correctly as the loss value shown on $y$-axis is negligible. We also remark that $\falseratio=1$ does not imply that all bins are included since the threshold is based on the expected expected value of false positives for every confidence whereas the true number typically fluctuates around the expectation.


 \paragraph{Number of Devices Limitation}

 
 In this part we analyze the effect of having constraints on the number of \devices on the utility of the model. In a production-scale model with billions of \devices sending data, dimension can grow extremely large. One way to get around this issue is to use sampling. By sampling in each iteration only a sub-set of \devices receive the query and contribute to the algorithm. In this section we discuss the effect of having different sampling rates on the utility of the model. Each \device can participate in an iteration with a $\bernoulli\gamma$ where $\gamma$ is the sub-sampling rate. We use theorem 5 described in~\cite{zhu2019poission} for estimating the upper-bound of $\epscentral$. The effect of different sub-sampling rates on the value of $\epslocal$ when $\epscentral=1$ and $\numusers = 1.6*(10)^6$ is shown in Table~\ref{tab:samp}. As shown small sampling rate, increase the $\epslocal$ by adding to the randomness. However, as shown in the table, no sampling leads to higher $\epslocal$ in comparison to the moderate sampling rates $\epslocal$. The reason is when the number of \devices increases, the privacy guarantee on the output of the aggregation protocol gets stronger (``lost in the crowd''). Hence, for moderate sampling rates having less number of users cancels the benefit of using sampling. Figure~\ref{fig:totalsamp}, ~\ref{fig:sampcount}, ~\ref{fig:sampfreq} demonstrates the effect of different sampling rates on the utility of $\ouralgorithm$. In addition to $\epslocal$ difference of using different methods, having smaller number of \devices can affect the utility by eliminating some part of the distribution. Consequently, we avoid using sampling if the dimension constraint allows.


 \begin{table}[]
    \centering
    \resizebox{\columnwidth}{!}{%
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
    \hline
        $\numiters$ & \multicolumn{10}{|c|}{$3$} & \multicolumn{10}{|c|}{$4$} & \multicolumn{10}{|c|}{$5$} \\ \hline
        sampling rate&$0.1$ & $0.2$ & $0.3$ & $0.4$ & $0.5$ & $0.6$ & $0.7$ & $0.8$ & $0.9$ & $1$&$0.1$ & $0.2$ & $0.3$ & $0.4$ & $0.5$ & $0.6$ & $0.7$ & $0.8$ & $0.9$ & $1$& $0.1$ & $0.2$ & $0.3$ & $0.4$ & $0.5$ & $0.6$ & $0.7$ & $0.8$ & $0.9$ & $1$ \\ \hline
        $\epslocal$& $8.42$ & $8.26$ & $8.19$ & $8.12$ & $8.1$ & $7.92$ & $7.81$ & $7.73$ & $7.62$ & $7.73$ & $8.32$ & $8.19$ & $8.12$ & $7.97$ & $7.87$ & $7.72$ & $7.59$ & $7.52$ & $7.4$ & $7.58$ & $8.27$ & $8.12$ & $8.1$ & $7.86$ & $7.6$ & $7.35$ & $7.19$ & $7.2$ & $7.02$ & $7.39$  \\ \hline
    \end{tabular}
    }
    \caption{Sampling rate effect on the $\epslocal$ for $\epsagg = 1$ and $\delta = 10^{-6}$}
    \label{tab:samp}
\end{table}


% Figure environment removed

 \subsection{Multiple Data point Adaptive Segmentation}\label{appendix:multioptprefixtree}
 \textbf{Segmentation Size}


 In this analysis, we used binary encoding of the data. We set the dimension limitation to $\complimit=10^7$. We ran the experiment for the utilized setting in which we have unweighted sampling and prefix list. We used both weighted and unweighted metric. As demonstrated increasing the number of iterations from 3 to 12 improves the utility of the algorithm.


% Figure environment removed

 % Figure environment removed


