\section{Experiments}\label{sec:expts}

% Figure environment removed
\textbf{Evaluation Dataset:} For our evaluations, we use the Reddit public dataset which contains the data of $1.6$ million users' comments posted on social media in December 2017~\cite{caldas2018leaf}. On the Reddit dataset each device has an average of $1092$ words and an average of $379$ unique words.
For investigations on the single data point per \device setting, each \device $i$ samples a single data point from $\empuserdist_i$. This data point remains fixed during the multiple iterations of the algorithm. 
Unless stated otherwise, we use a Huffman encoding to translate the words into binary strings. One token is reserved for unknown characters and we have an end character encoding at the end of each bit stream. We set the system constraint $\complimit=10^7$ and $\totlen = 60$ in all experiments.

\paragraph{\textbf{Evaluation metric:}}Let $H = (x_1,x_2,\dots,x_{|H|})$ denote the set of heavy hitters output by an algorithm ordered by the empirical global frequency distribution $\empuserdist$. For our evaluations in this section, we report the frequencies (according to $\empuserdist$) of heavy hitters output by the algorithm. To aid with visualization in our plots, for a window size $W=50$ and a given heavy hitter set $H$, we plot for each $i$, the sum of the probabilities (according to $\empuserdist$) of the heavy hitters in the sliding window %$(x_{i - W},x_{i-W + 1},\dots, x_i)$. We also plot a true histogram line representing $(x^*_{i - W},x^*_{i-W + 1},\dots, x^*_i)$ for reference. 
(more details on plots are in Appendix~\ref{appendix:tuning}). 



\subsection{Adaptive Segmentation in Single Data Point Setting}
\label{sec:hyper}

In this section, we focus on the simpler single data point per \device setting. \longversion{For these evaluations we used $\epsagg=1$ and $\delta = 10^{-6}$.}  
 
 \paragraph{Adaptive Segmentation}
 
We demonstrate the benefit of adaptively choosing the segment length, as opposed to using fixed-length uniform segment lengths.  \longversion{For our experimental evaluations in this part we set our data domain limit to $\complimit = 10^7$ which means that the dimension of each payload of size $2^{\seglen_{t}} \times |\prefixlist_t| $ should not exceed this limit. For both of the configurations in this part, we limit $\numiters=4$.} \longversion{In one configuration we used the fixed segment length of 15 for all the iterations. On the other hand, with $\ouralgorithm$ in each iteration we select the largest possible segmentation length for the next iteration based on the dimension limitation and the size of the {prefix list} for the next iteration.} 
Fig.~\ref{fig:uniformvsnon} compares the discovered normalized frequencies for the algorithm which uses uniform segment lengths ($\seglen_t=15$ for all $t$) to the algorithm which selects the segments adaptively, which leads to segment lengths [23, 14, 11, 12]. Our adaptive scheme is able to discover $40\%$ more heavy hitters (more plots in Fig.~\ref{fig:segmentation}). We use adaptive thresholding in both algorithms, with $\falseratio = 0.5$. However, the empirical $\falseratio$ is even lower with the values of $0.35$ and $0.41$ for adaptive and uniform segment algorithms, respectively.


 \paragraph{Dimension Limitation}
 
 In production-scale systems the computation cost of decompressing and aggregating the responses can be a significant bottleneck when the data comes from a very large domain. Increasing system limits can be expensive and resource intensive. Thus, we explore the effect of different dimension constraints on the performance of $\ouralgorithm$. In order to do so, we used different constraints of $P=$20, 10, and 1 million. This limitation in iteration $t$ specifies an upper bound on $2^{\seglen_{t+1}} \times |\prefixlist_t|$. First for all the configurations we set the number of iterations to 4. As illustrated in Figure~\ref{fig:PayloadLimitFreq} changing the limit to 1 million degrades the utility of the algorithm significantly. This is because the small number of iterations and small system limit means that in the final round, the prefix list has to be smaller than desired in order to set the segment length large enough to finish the algorithm in the desired number of iterations. One way to compensate this degradation is to allow the algorithm to run in 5 iterations instead of 4. To account for the total privacy budget in all the iterations, by having one more iteration, the per iteration local epsilon $\epslocal$ decreases (check Table~\ref{tab:eps}). However, with 5 iterations, the algorithm is able to detect $15\%$ more heavy hitters in comparison to the same dimension limit of 1 million when using 4 iterations. For these experiments we set the $\falseratio = 0.5$. However the empirical $\falseratio$ is $0.42$, $0.46$, $0.33$, and $0.42$ for $1M$ + 1 extra iteration, $1M$, $10M$, and $20M$ dimension limit, respectively.



\longversion{We explored other constraints effect on the utility of the algorithm in Appendix~\ref{appendix:tuning}. One other important constraint is the number of payloads a system can receive in a single iteration. In order to limit the number of payloads, we use Poisson sampling on the devices at each iteration, so each device tosses a coin and decides to participate with probability $p$. This reduces the number of devices participating, and allows us to take advantage of privacy amplification by sampling. For the evaluations, we use the privacy amplification by sampling for R\'enyi DP bounds due to~\cite{{zhu2019poission}}. }
% Figure environment removed
\subsection{Data Selection for Multiple Data Points per \Device}

Now we will explore the setting where each \device has multiple data points. \longversion{In these experiments we will set $\epsagg=1$ and $\delta = 10^{-6}$.} 

 
\paragraph{Effect of Data Selection}
First, we evaluate the effect of different data selection schemes, specifically focusing on weighted vs unweighted data selection. In order to highlight the benefit of conditioning on the prefix list during data selection, we also compare against the version of these schemes that do not take the prefix list into account when selecting a data point. We will refer to the version where the selected data point is conditioned to belong in the prefix list as weighted selection + prefix list and unweighted selection + prefix list, and the versions where the selected data point is not forced to be in the prefix list as weighted selection and unweighted selection.
\longversion{In this experiment we used $\numiters = 4$.} Fig.~\ref{fig:weightfreqsamp} shows the experimental results using the different data selection schemes. Perhaps surprisingly, unweighted sampling outperforms weighted sampling in both the with and without prefix list experiments. One explanation for this observation is that in unweighted selection, moderately frequent words are given more opportunity to participate to the final output, while in weighted selection, the most frequent words are selected by all the users. That is, perhaps unweighted selection allows us to explore further into the tails of the distributions. As expected, conditioning on the prefix list has a significant impact. For the rest of this paper, we use unweighted sampling conditioned on the prefix list for on-device data selection. For these experiments we set the $\falseratio = 0.5$. However the empirical $\falseratio$ is $0.30$, $0.35$, $0.35$, and $0.38$ for weighted selection + prefix list, unweighted selection + prefix list, weighted selection, and unweighted selection respectively. For more evaluations please refer to Fig.~\ref{fig:selection}.


\paragraph{Effect of adding a deny list
} In practice, it is possible that an analyst has prior knowledge about some of the heavy hitters before the start of the algorithm. In such cases, we can take advantage of a deny list which includes this prior knowledge. When sending query to the \devicesnospace, we can also send the deny list and ask the \devices to exclude these already discovered data points during data selection. We explore two ways of obtaining a deny list, $\denylist$. In the first case, the deny list comes from an auxiliary public data source (which is not protected with DP), we will refer this as a \texttt{warm start}. The second is when we run the algorithm twice, with the heavy hitters from the first round forming the deny list of the second round (referred to as \texttt{2 rounds}). In \texttt{2 rounds} case, we need to account for the privacy budget of \emph{both} rounds,\longversion{ which leads to more noise per iteration}.
In Fig.~\ref{fig:weightfreqdeny} we compare the different configurations. The warm start is initiated with the top $2000$ popular words from Twitter Sentiment140 dataset~\cite{go2009twitter}. We also present the utility of the warm start $\denylist$ on its own (denoted by \texttt{deny list} line), showing the difference between the distribution of the two datasets.\longversion{ For comparison, we also include the standard algorithm run for a single round, without a deny list (denoted by \texttt{1 round} line).}  We further show the benefit of using deny list in one round of algorithm execution. Finally, we first execute one round of algorithm without a warm start and form a deny list out of discovered prefixes, which is then used in a second round. Fig.~\ref{fig:weightfreqdeny} shows that adding a deny list significantly improves performance. Adding warm start leads to discovering $2.1\times$ more heavy hitters. The 2 round algorithm increases the number of discovered heavy hitters by $1.22\times$. For twitter sentiment140 dataset only $0.006$ of the data points in this dataset do not belong to Reddit dataset. For the rest of configurations, we set the $\falseratio = 0.5$.  For \texttt{1 round} the empirical $\falseratio$ is $0.45$ while for \texttt{1 round + warm start} it's $0.37$. For \texttt{2 rounds} of algorithm $0.39$ of discovered bins are false positives. For more evaluations regarding deny list, please refer to Fig.~\ref{fig:deny}. 

\paragraph{Comparison to prior works} 

We compare with $\triehhg$ ($\triehh$) and $\triehhp$($\opttriehhp$), Central Laplace ($\laplacetree$), and Central Gaussian ($\gaussiantree$) with the optimisation that we use our adaptive segmentation to determine the segment length at each round, in the setting where each \device has multiple data points. For all of the algorithms we use the unweighted data selection, which performed the best in our previous experiments. To have a fair comparison, the same binary encoding is used for all of the models (5 bits per character, not Huffman encoding) and we used 12 iterations (1 character per iteration) that shows the best performance for this encoding for all the methods.

In $\triehh$ and $\opttriehhp$, for $\epsagg=1$ and $\delta = 10^{-6}$, we set the threshold for the number of reports that needs to be received for a word to be a part of the prefix list (denoted by $\theta$ in their paper) to $10$. Accordingly we set the sampling rate based for $\triehhg$ based on Corollary 1 in \cite{zhu2020federated} and for $\triehhp$ based on Lemma 3 in~\cite{cormode2022sample}. Our analysis in Fig.~\ref{fig:compfreqmulti} indicates $\ouralgorithm$ is able to discover $3.2$ times more heavy hitters for the same number of iterations and same dimension constraint. One explanation for this performance difference is that $\triehh$ and $\opttriehhp$ use sampling and thresholding to achieve the aggregated privacy guarantee, without adding any local differential privacy noise. When we set the threshold ($\theta$) to 10, the sampling rate is $0.0079$ and $0.0071$ for $\triehhg$ and $\triehhp$ respectively. Hence, the low sampling rates required to achieve the privacy guarantee results in sampling error in the distribution that is larger than the noise injected by our mechanism. In Appendix~\ref{appendix:TrieHH} and Appendix~\ref{appendix:TrieHH++} we explore the effect of different numbers of iterations in the utility of $\triehhg$ and $\triehhp$ for both single and multiple data points setting.

We further compare our method with a state-of-art central differential privacy methods. The assumption here is that a trusted curator is able to collect the \devicenospace's data and add central noise. We used two types of central noise for our analysis. For providing the central DP privacy guarantee we add Gaussian or Laplace noise to each dimension of the histogram~\cite{dwork2014algorithmic, dwork2006differential}. As with our previous experiments we compute the privacy guarantee of the composition over multiple rounds using the advanced composition theorem~\cite{DRV, Kairouz:2017} and the composition bound in terms of R\'enyi differential privacy~\cite{DLDP, mironov2017renyi}. We compute the R\'enyi privacy guarantees of both noise addition methods using the bounds provided in Table 2 of ~\cite{mironov2017renyi}. We finally select the noise power using the tighter bound between these two. As shown in Fig.~\ref{fig:compfreqmulti} both of the central DP scenarios outperform $\ouralgorithm$. While $\gaussiantree$ can be implemented in our model (by distributing the Gaussian noise among the participating devices), it suffers from a significantly worse local privacy guarantee. 


