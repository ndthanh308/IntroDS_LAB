\section{Adaptive Thresholding and Segmentation}\label{sec:post}

In this section, we will describe our adaptive segmentation and thresholding algorithms. The algorithms aim to keep as many heavy hitters as possible, while maintaining a specific false positive rate, and satisfying the data domain size constraint for the next iteration.

\subsection{Adaptive Thresholding}

Let us first discuss our proposal for how to choose the threshold $\tau$.
Given a threshold $\tau$ and standard deviation $\sigma$, let $E$ denotes the probability that a data point with true count zero (i.e. no device contributes this data point) has an estimated count above $\tau\sigma$. That is, the probability of a false positive. As discussed earlier, for any $d$, the estimate $\tilde{f}(d)$ is approximately Gaussian with standard deviation $\sigma$ so we can compute $E$ based on the Gaussian approximation, i.e. $E = 1-\Phi(\tau\sigma)$, where $\Phi()$ denotes the gaussian CDF (with mean 0 and standard deviation $\sigma$)\footnote{if $d$ has true count 0, then the distribution of $\tilde{f}(d)$ is actually a shifted, scaled Binomial distribution. If $n$ is small enough that the Gaussian approximation is not accurate, then one can use high probability bounds on the Binomial.}. 

In the while loop (line 4 to 7), we first compute the expected number of false positive bins by $|\mathcal{D}|\times E$ where $|\mathcal{D}|$ represents the aggregated data domain size. We then make sure that the ratio of the expected number of false positives to the number of data points such that $\tilde{f}(d)\ge\tau\sigma$ (represented by $|\prefixlist'|$) does not go above a specified threshold. If the ratio of the expected number of false positives that threshold $\tau$ to the number of data elements with estimated frequency above $\tau\sigma$ exceeds the specified false positive ratio, we change the confidence level to the point that we make sure the algorithm satisfies this parameter.

\begin{algorithm}[H]
\caption{\label{alg:prunehh}Pruning algorithm for confident heavy hitter detection ($\prunehh$)}
\begin{algorithmic}[1]
\STATE \textbf{Input}: $\queryset,\freqest[\queryset]$: query set and estimated frequencies after aggregation, $\dpmult_0$: Initialization of threshold, $\falseratio$: ratio of expected false positives to the total number of bins, $\sigma$: aggregated noise standard deviation, $\eta$: step size

\STATE $E \gets 1-\Phi(\dpmult_0\sigma)$ 
\STATE  {$\prefixlist'= \{q \in \queryset ~\mid~ \freqest[q] >\dpmult_0 \times \sigma\} $}
{\WHILE {${\falseratio} < \frac{E \times |\mathcal{D}|}{|\prefixlist'|} $} 
\STATE $E \gets \eta*E$
\STATE $\dpmult \gets z_{(1-E)}$ 
\STATE  {$\prefixlist'= \{q \in \queryset ~\mid~ \freqest[q] >\dpmult \times \sigma\} $}
\ENDWHILE}


\STATE {\textbf{return} $\prefixlist$}

\end{algorithmic}
\end{algorithm}



\subsection{Adaptive Segmentation}\label{sec:analysis}\label{sec:adaptive}


Given the prefix list, the segment length is adaptively chosen to be as large as possible while maintaining the dimension constraint, $\seglen_{t+1}~=~\arg\max_{\ell}\{\ell\; |\;|\prefixlist_{t+1}| \cdot 2^{\ell}\le \complimit \}$. 
In the single data point per device setting, intuitively, there are two opposing factors in the performance of the private heavy hitters algorithm --- the privacy budget per iteration (which decreases with increase in $\numiters$) and the size of the total search space (which is smaller for algorithms with smaller segmentation lengths and hence more iterations). In this section, we outline an argument illustrating that the effect of decreasing in privacy budget per iteration dominates and it is better to minimize the number of iterations. We also illustrate this via experiments. Thus, we choose each segment length to be as large as possible while retaining as many popular prefixes (with suitable confidence) as possible and maintaining the dimension constraint. 

\paragraph{Intuitive theoretical analysis of $\privhh$ for single datapoint}
In this part, we try to obtain guidelines for how to set the segment length in the a single data point per device setting. We provide analysis for running $\privhh$ for one round ($T = 1$) searching over the whole high dimensional universe $ \alphadom^\totlen$. While this may be impractical to implement, it provides us with setting of hyperparameters in the case of no computation and communication constraints. We analyze the algorithm assuming good performance of the local randomizer $\compressor$, frequency estimator $\decompressor$ pair. We formalize this assumption as follows: 

\begin{assumption}\label{ass:local-rand-perf} For any $\beta\in [0,1]$, for any element $x$ in the domain,
with probability at least $1 - \beta$, we have, \[|F(x) - \freqest(x)| \leq C_1\sqrt{\frac{ne^\epslocal}{(e^\epslocal - 1)^2}\log\left(\frac{1}{\beta}\right)}=C_2\frac{\sqrt{\log(1/\delta)\log(1/\beta)}}{\epsilon_{agg}},\] where $C_1$ and $C_2$ are absolute constants, $F$ is the global empirical distribution and $\tilde{f}$ is the estimated empirical distribution. 
\end{assumption}

We note that this assumption is satisfied by \OHEBRRnospace, as well as other common frequency estimation algorithms such as PI-Rappor. For the purpose of this analysis, we will use a different metric to the metric that will be the main focus in our experimental results. We will measure the performance of $\privhh$ using a metric we define in \Cref{def:lam-acc}, which is standard in the differentially private heavy hitters literature \cite{bassily2017practical}. 

\begin{definition}[$\lambda$-accurate]\label{def:lam-acc}
A set of heavy hitters $\prefixlist_\numiters$ is said to be $(\lambda, A)$-accurate if it satisfies the following:

\begin{itemize}%[leftmargin=*]
\setlength\itemsep{1mm}
    \item For all $d \in \worduniv$, if $F(d) \geq A + \lambda$, then $d\in \prefixlist_\numiters$.
    \item For all $d \in \worduniv$, if $F(d) < A - \lambda$, then $d \notin \prefixlist_\numiters$.
\end{itemize}
\end{definition}

We now prove the utility of $\privhh$ when we have one iteration with $\seglen = \totlen$ in \Cref{prop:one-round}.
We first state the result and discuss its implications, deferring the proof to Appendix~\ref{appendix:proof}. 

\begin{proposition}\label{prop:one-round} 
    Let the local randomizer ($\compressor$) and frequency estimator ($\decompressor$) pair satisfy \Cref{ass:local-rand-perf} and let $\prefixlist_1$ be the output of $\privhh$ when run for $T = 1$ round with $\seglen = \totlen$ and the query set $\queryset = \alphadom^\totlen$ and aggregate DP parameters $({\epsilon_{agg}}, \delta)$. Then, with probability at least $(1-\beta)$, $\prefixlist_1$ is $(\lambda, \tau\sigma)$-accurate with $\lambda = O\prn{\frac{1}{\epsilon_{agg}}\sqrt{\log(1/\delta)\log\prn{\frac{|\alphadom|^\totlen}{\beta}}}}$, where $\tau\sigma$ is the final threshold.
\end{proposition}


While this only give us upper bound on the performance of the single iteration algorithm, it does help us gain some intuition into how to set the segmentation length. Suppose we were to run the algorithm for $T$ iterations with even segmentation (i.e. $\seglen_t=r/T$ for all $t$). Then, at each iteration, we would need the aggregate privacy guarantee for that round to be $\approx\epsilon_{agg}/\sqrt{T}$. If the data domain size per iteration was exactly $|A|^{r/T}$ then the impact of $T$ on the error would approximately cancel (ignoring logarithmic terms that arise from ensuring true heavy hitters survive at each iteration). However, the data domain at each round is \emph{greater} than $|A|^{r/T}$ pushing us towards using a single round.

This intuition does not generalize to the multiple data points per device setting as it is possible for iterations allows us to more intelligently select the data points that users contribute. This is aligned with our empirical results in Appendix~\ref{appendix:multioptprefixtree} which indicates sending one character at a time improves the utility. However, we conjecture that reducing the number of iterations per round, and including multiple rounds with deny-lists will improve utility. 
 

