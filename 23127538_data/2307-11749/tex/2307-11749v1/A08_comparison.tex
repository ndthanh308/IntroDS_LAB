\section{Comparison of $\triehh$, $\opttriehhp$, and $\ouralgorithm$}\label{appendix:comparison}
% Figure environment removed

Zhu et al.'s primary experiments are on learning $n$-grams (words or length $n$ sentences) where they propose setting the segmentation length to be a single character. However, our analysis indicates that, in the single data point setting, larger segments provide the higher utility. Earlier in section~\ref{appendix:TrieHH} we showed the effect of different number of iterations on the sampling rate and utility of $\triehhg$. In this set of experiments we used 4 iterations that shows the highest utility for $\triehhg$ based on the dimension limitations ($\complimit = 10^7$). We refer to this optimized version of $\triehhg$ as $\triehh$. The same observation holds for $\triehhp$ and hence we use the same configuration for this algorithm. We refer to the optimized version of $\triehhp$ as $\opttriehhp$.

To have a fair comparison, we use the same binary encoding for all the three models. This binary encoding uses $5$ bits to convert English letters to a binary representation. In these experiments, $\totlen = 60$. We set $\falseratio = 2$ for $\ouralgorithm$. Using this method, $\ouralgorithm$ needs 4 iteration of unknown dictionary and uses the segmentation of $[23, 13, 12, 12]$. Figure~\ref{fig:compfreq} shows the marginal frequencies of discovered bins on y-axis and number of heavy hitters in x-axis. This figure shows the marginal value with sliding window of $50$ on y-axis. In conclusion, with the same dimension limit and binary encoding, our method is able to outperform $\triehh$ and $\opttriehhp$ by finding $1.85X$ and $2.1X$ more heavy hitters. Figure~\ref{fig:compcounts} shows the same plot but in the y-axis we have the marginal counts of discovered bins. Also, Figure~\ref{fig:comptotal} shows the total utility loss.