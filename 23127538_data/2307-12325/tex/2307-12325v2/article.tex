\documentclass{amsart}
\pdfoutput=1
\usepackage{amsaddr}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage[shortlabels]{enumitem}
\usepackage{xr}
\usepackage{url}

\theoremstyle{plain}%
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}


\theoremstyle{remark}%
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%

\theoremstyle{definition}%
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}
\begin{document}



\title[A Robust Framework for Graph-based Two-Sample Tests]{A Robust Framework for Graph-based Two-Sample Tests Using Weights}

\author{Yichuan Bai and Lynna Chu}

\address{Department of Statistics, Iowa State University, Ames, Iowa, USA}


\begin{abstract}
Graph-based tests are a class of non-parametric two-sample tests useful for analyzing high-dimensional data. The framework offers both flexibility and power in a wide-range of testing scenarios. The test statistics are constructed from similarity graphs (such as $K$-nearest neighbor graphs) and consequently, their performance is sensitive to the structure of the graph. When the graph has problematic structures, as is common for high-dimensional data, this can result in poor or unstable performance among existing graph-based tests. We address this challenge and develop graph-based test statistics that are robust to problematic structures of the graph. The limiting null distribution of the robust test statistics is derived. We illustrate the new tests via simulation studies and a real-world application on Chicago taxi trip-data.
\end{abstract}

\keywords{Non-parametric tests, high-dimensional data, curse of dimensionality, hubs, similarity graphs}


\maketitle


\section{Introduction}

High-dimensional data are ubiquitous across a broad spectrum of fields, such as social sciences, genetics, and engineering. In this paper, we focus on testing the equality of distributions for observations in high dimensions. Suppose we have two samples $\{\mathbf{X_1, \hdots, X_{n_1}}\}$ and $\{\mathbf{Y_1, \hdots, Y_{n_2}}\}$ of $d$-dimensional observations that are independently and identically distributed from unknown distribution $F_X$ and $F_Y$, respectively. The two-sample problem aims to test  $H_0: F_X = F_Y$ against an omnibus alternative $H_1: F_X \neq F_Y$. This is a classic statistical problem, but made more challenging given the increasingly complexity of modern data where observations can be high-dimensional ($d >>n$) and/or non-Euclidean data objects. Substantial developments have been made by the contemporary statistics community to address these challenges. For example, non-parametric two-sample tests for multivariate and high-dimensional data have been proposed using distances (\cite{baringhaus2004new, biswas2014nonparametric, li2018asymptotic}), generalized ranking (\cite{liu1993quality, hall2002permutation}), and kernels (\cite{gretton2012kernel, song2020generalized, Zhu2021}).

While all of the mentioned methods can be applied to the high-dimensional setting, many of them do not explicitly address how to resolve various aspects of the curse of dimensionality. For example, distance-based test statistics are common in the high-dimensional setting, but it has been observed that distances may not be meaningful in high-dimensional space since they have a tendency to concentrate when $d$ is large. As such, distance-based test statistics may have trouble to effectively distinguish similarity between observations. Moreover, the distribution of distances also becomes considerably skewed as dimensionality increases, resulting in a phenomenon known as \textit{hubness}. To be precise, suppose we have a $k$-nearest neighbor graph, where each observation in our data set points to its $k$ nearest neighbors, according to some distance measure. Let $N_k(x)$ be the number of times an observation $x$ is among the $k$ nearest neighbors of all other points in the data set. When the dimensionality is high, the distribution of $N_k$ becomes right skewed, resulting in the emergence of hubs. This hubness phenomenon is relevant to  procedures that directly or indirectly make use of distances between observations; this includes general construction of graphs (not just $k$-NN graphs) based on interpoint distances. Existing test statistics are often vulnerable to these aspects of the dimensionality curse, which can result in poor or unstable performance under various scenarios. 

In this paper, we explore the hubness phenomenon and its effect on a class of tests based on geometric graphs constructed by using interpoint distances. We refer to these as graph-based two-sample tests;  the first test was proposed by \cite{ori1979} and later numerous extensions and theoretical developments have been made (a brief overview is provided in Section \ref{sec:overview}).  Graph-based tests are applicable to any data type, provided a reasonable similarity measure can be defined, and therefore can be applied to wide range of modern applications. We study the hubness phenomenon in the setting of graph-based tests and show why hubs can be damaging to the existing tests. We propose a robust framework for graph-based tests that utilize weights from the graph and demonstrate through simulation studies and real data application the improved performance of this robust framework. Our approach can be implemented using the R package `rgTest'. 

The paper is organized as follows. In Section \ref{sec:framework}, we review the graph-based testing framework. We identify why the hubness phenomenon can be problematic for graph-based tests. We then propose a robust solution in Section \ref{sec:method}, which involves choosing weights to dampen the effect of hubs. In Section \ref{sec:asymptotic}, the asymptotic null distributions of the proposed test statistics are derived. Section \ref{sec:performance} examines the power of the robust test statistics under different simulation settings. In Section \ref{sec:application}, the robust test statistics are illustrated in the analysis of Chicago taxi data and we conclude in Section \ref{sec:conclusion}.

\section{Graph-based Testing Framework}\label{sec:framework}

\subsection{Overview}\label{sec:overview}
Graph-based tests provide a general framework to conduct two-sample tests for multivariate and non-Euclidean data. A similarity graph is constructed from the pooled observations of both samples according to a similarity measure (such as Euclidean distance). The similarity graph can be constructed based on a certain criterion. For example a minimum spanning tree (MST) is a similarity graph that connects all observations in such a way that the total distance across edges is minimized. A $k$-MST graph is an extension of the MST: it is the union of the 1st, $\hdots$, $k$th MST, where the 1st MST is the MST and the $i$th MST $(i > 1)$ is the spanning tree with the total distances across edges minimized subject to the constraint that it does not contain any edges in the 1st, $\hdots, (i-1)$th MST(s). Other examples include the $k$-nearest neighbor graph ($k$-NNG), where each observation is connected to its $k$ nearest neighbors. The similarity graph could also be constructed according to domain knowledge. 

Three quantities of the graph are computed: the between-sample edge-count ($R_0$), the within-sample edge-count of Sample $\textbf{X}$ ($R_1$), and the within-sample edge-count of Sample $\textbf{Y}$ ($R_2$). Figure \ref{fig:example_low_d} illustrates $R_0$, $R_1$ and $R_2$ on an MST constructed from Euclidean data.  For illustration, the dimension of each observation is $2$, but these quantities can be computed for data in arbitrary dimension. Sample $\textbf{X}$ is represented by dots and Sample $\textbf{Y}$ is represented by triangles. The number of edges connecting between the dots and triangles is $R_0$ (between-sample edge-count), the number of edges connecting within the dots only is $R_1$ (within-sample edge-count of Sample $\textbf{X})$, and the number of edges connecting within the triangles only is $R_2$ (within-sample edge-count of Sample $\textbf{Y})$. A combination of these edge-counts ($R_0$, $R_1$, and $R_2$) are used to construct different graph-based test statistics. 

The first graph-based tests was proposed by \cite{ori1979}; the authors proposed using (a standardized) $R_0$ as the test statistic and a small $R_0$ was evidence against the null hypothesis that the two distributions are equal. Their rationale was that if the two samples really do come from different distributions, then then number of edges connecting between the samples should be relatively small. \cite{schilling1986multivariate} and \cite{henze1988multivariate} proposed similar test statistics for $k$-NN graphs, specifically. The same rationale was used construct two-sample graph-based tests in \cite{rosenbaum2005exact} and \cite{biswas2014distribution}, for minimum distance pairing and Hamiltonian graphs, respectively. A small $R_0$ as evidence against the null holds well when the two distributions differ in means. An illustration of this can be found in  scenario (a) of Figure \ref{fig:example_low_d}. We observe that the number of between-sample edges, $(R_0)$, is relatively small ($R_0 = 4$). 

The rationale of a small $R_0$ can breakdown for more general alternatives; for example when the change in distribution is in mean and variance. We can see this clearly in in scenario (b) of Figure \ref{fig:example_low_d}, when the distributions differ in variance.  Observe that $R_0$ is relatively large ($R_0 = 19$), even though a change in distribution is present. To resolve this, graph-based test statistics were proposed use in \cite{gen2017}, \cite{wei2018}, and \cite{max2019} that use a combination of $R_1$ and $R_2$ to create more powerful tests for general alternatives. 

% Figure environment removed


\subsection{Hubness phenomenon in high-dimensional data}
\label{sec:motivation}

Hubs, defined to be nodes in the graph with a large degree, are a product of the the curse of dimensionality. The hubness phenomenon was carefully studied in \cite{radovanovic2010hubs}, who showed that hubs are an inherent property of data distributions in high-dimensional and not an artifact of finite samples or specific data distributions. Their careful theoretical analysis showed that the probability that a hub emerges increases as the data dimension increases. The high-dimensional setting amplifies the tendency of central observations (observations close to the mean) to become hubs, effectively making it easier for an observation to become a `popular' or `central' node. As a result, $k$-MSTs and $k$-NNGs constructed on high dimensional data tend to have large hubs under standard distance measures, such as $L_p$. Hubs can be highly influential nodes and can distort final results depending on whether these nodes are included or excluded. 

To see that the presence of hubs is a common phenomenon for high-dimensional data, we construct $5$-MST graphs using Euclidean distance and report their maximum node degrees. We run 100 simulations with $n_1= n_2 = 500$, where observations are drawn from multivariate standard normal and multivariate $t$ with $3$ degrees of freedom. For each simulation setting, we report the maximum node degree and 95th percentile of node degree; boxplots of these quantities are shown in Figure \ref{fig:max_nodedegree}. We see that the 95th percentiles of node degrees are smaller than 40, but the maximum node degrees are more than three times as much as the 95th percentiles. Clearly it is not uncommon to have a node with a degree much larger than the majority of other nodes' in the similarity graph. Observe that as the dimension increases, the max node degree also increases. This phenomenon is more pronounced in heavy-tailed distributions, such as the multivariate $t$-distribution. 

% Figure environment removed

\subsection{Limitations of current graph-based tests}\label{sec:limitation}
In order to understand why large hubs may cause problems in the existing graph-based tests, let us consider the following example. We simulate observations from two distributions: Sample 1 $ \sim \mathcal{F}_1:\mathcal{N}(\textbf{0}, \textbf{I}_{500})$ and Sample 2 $\sim \mathcal{F}_2: \mathcal{N}(\boldsymbol{\mu}, \big(\begin{smallmatrix}
  1.2^2\textbf{I}_{100} & \textbf{0}\\
  \textbf{0} & \textbf{I}_{400}
\end{smallmatrix}\big))$, where $||\boldsymbol{\mu}|| = 1$.
In Scenario 1, 100 observations are generated from \(F_1\) and \(F_2\) respectively and a 5-MST is constructed based on Euclidean distance using the pooled observations. The maximum node degree in the graph is 71, and the graph is relatively flat. In Scenario 2, the observations are generated in the same manner, but one observation in Sample 2 is shifted toward the mean so that the 5-MST graph contains a hub with a node degree of 97. Moving towards the mean is a high-probability event and not an extreme scenario.

Since the type of alternatives in Scenario 1 and Scenario 2 involve both mean and scale change, 
the generalized edge-count test $S$ and the max-type edge-count test $M$ are recommended. These graph-based tests are powerful for detecting general distributional differences  and were proposed in \cite{gen2017} and \cite{max2019}, respectively. Both tests involve using combinations of $R_1$ and $R_2$ and a large value of $S$ or $M$ is evidence against $H_0$. The performance of these two tests under Scenarios 1 and 2 are shown in Table~\ref{tab:table 1}. We can see both tests work perfectly well under Scenario 1, but are unable to reject the null hypothesis at 10\% significance level in the presence of a hub (Scenario 2).


\begin{table*}[ht] 
\centering
	\caption{Comparison of tests under Scenario 1 and Scenario 2}
	\centering
	\begin{tabular*}{\columnwidth}{@{\extracolsep\fill}lcccc@{\extracolsep\fill}}
		\toprule
		\multirow{2}[3]{*}{Test Type}&\multicolumn{2}{c}{Scenario 1} &  \multicolumn{2}{c}{Scenario 2}                  \\
		\cmidrule(r){2-3}\cmidrule(r){4-5}
		&Test Statistic     & p-value     & Test Statistic     & p-value \\
		\midrule
	$S$&	17.6591 & 0.0001  & 4.3331 &0.1146   \\
	$M$&	3.3433 & 0.0012 & 1.7645 & 0.1135   \\
		
		\bottomrule
	\end{tabular*}
	\label{tab:table 1}
\end{table*}

\begin{table*}[ht]
	\caption{Graph-based quantities for 5-MST under Scenario 1 and Scenario 2}
	\centering
	\begin{tabular*}{\columnwidth}{@{\extracolsep{\fill}}lcccccc@{\extracolsep{\fill}}}
		\toprule
		\multirow{2}[3]{*}{Edge-count}&\multicolumn{3}{c}{Scenario 1} &  \multicolumn{3}{c}{Scenario 2}                  \\
		\cmidrule(r){2-4}\cmidrule(r){5-7}
		&Value & Mean     & Variance     & Value &Mean     & Variance  \\
		\midrule
    	$R_1$ &	356 & 247.5  & 1150.937 & 322&247.5 &1472.912   \\
    	$R_2$ &	161&247.5 & 1150.937 &189 &247.5 &1472.912 \\
		\bottomrule
	\end{tabular*}
	\offinterlineskip
	\label{tab:table 2}
\end{table*}

Table~\ref{tab:table 2} sheds insight on why this happens. Two graph-based quantities $R_1$ and $R_2$ are reported. Recall that $R_1$ is the number of edges that connects observations within Sample 1 and $R_2$ is the number of edges that connects observations within Sample 2. Under the alternative, we would expect the absolute value of differences $R_1 - E(R_1)$ and $R_2-E(R_2)$ to be relatively large.  Here $E(R_1)$ and $E(R_2)$ denote the expectations of the respective edge-counts under the permutation null distribution. 


Under Scenario 1, $R_1$ and $R_2$ behave as we expect: $R_1$ (356) is large relative to its expectation under the null (247.5) and $R_2$ (161) is small relative to its expectation under the null (247.5), therefore the differences between within sample edge counts and their expectations are relatively large. Under Scenario 2, two problems arise in the presence of a hub. First, observations from both samples tend to construct an edge with the hub. So, compared to Scenario 1, $R_1$ decreases (322) and $R_2$ increases (189). The relative differences between $R_1$ and $R_2$ with their respective null expectations then become smaller, causing both $S$ and $M$ to lose power. 

Second, the variances of both $R_1$ and $R_2$ also increase, further inhibiting the power of the test statistics. Let $G$ denote the similarity graph and its set of edges,  $|G|$ denote the number of edges in $G$, $G_i$ be the subgraph including all edge(s) that connect to node $i$, and $|G_i|$ be the degree of node $i$ in $G$. The symbol $(i, j)$ denotes an edge with endpoints $i$ and $j$ in graph $G$. This second problem can be clearly seen by studying the analytical expression of the variance of the edge-count, $R_i$, under the permutation null distribution:
\begin{equation*}
\begin{split}
 Var(R_j) = &\mu_j(1- \mu_j) + 2C\frac{n_j(n_j-1)(n_j-2)(N-n_j)}{N(N-1)(N-2)(N-3)}+\\
 		    &|G|(|G|-1)\frac{n_j(n_j-1)(n_j-2)(n_j-3)}{N(N-1)(N-2)(N-3)},
\end{split}
\end{equation*}
where $j = 1, 2$, $\mu_j = E(R_j) = |G|\frac{n_j(n_j-1)}{N(N-1)}$, $C = \frac{1}{2}\sum_{i = 1}^N|G_i|^2 -|G|$, $n_j$ is the number of observations in sample $j$, and $N = n_1+ n_2$. 

In the presence of a hub, $\sum_{i = 1}^N |G_i|^2$ and $C$ increase, where $C$ represents the number of edge pairs sharing a common node. Since all the other terms remain the same under both scenarios, this results in an inflated variance for $R_1$ and $R_2$ in the presence of hubs. 

\subsection{Our Contribution} 
When the size or density of hubs is large, existing graph-based tests can suffer from limited power and unstable performance. That said, it can be tricky to identify every single hub that may cause problems. Instead of identifying `influential' hubs, which can be data or application-specific, we propose new test statistics that are useful even in the presence of hubs. Specifically, we propose to apply appropriate weights to the test statistics that will dampen the effect of hubs while still retaining crucial similarity information.  We show that these weights can improve power and resolve the variance boosting problem in the presence of hubs. We provide recommendations for weights as functions of node degrees and demonstrate that these work well in a range of scenarios. The limiting null distribution of these new robust test statistics are derived under mild conditions on the weights.

\section{Robust edge-count test statistics}\label{sec:method} 

Let $d_i$ denote the node degree of node $i$ in a graph $G$. The graph $G$ is a similarity graph (such as a $k$-MST or $k$-NN, where $k$ is pre-specified). Our approach is to flatten the similarity graph, and limit the influence of hubs, without incurring too much of a loss of similarity information so that the testing procedure can still retain power.  To do so, we propose to apply weights to the test statistic. The weights should be designed such that edges connected to a hub (a node with a large $d_i$) are down-weighted, while other edges are left mostly undisturbed. Formally, let $w_{ij}$ denote the weight on edge $(i, j)$ where $w_{ij}$ is the value of the weight function $W(d_i,d_j)$, with $W(d_i,d_j)$ defined to be a function of $d_i$ and $d_j$.  Discussions about the choice of weight functions are deferred to Section \ref{sssec:weights}. 

We propose test statistics that apply weights $w_{ij}$ to the edge-counts $R_0(i,j)$, $R_1(i,j)$, and $R_2(i,j)$, such that each edge  $(i,j) \in G$ is weighted by a combination of $d_i$ and $d_j$. Let $g_i = 0$ if the observation $i$ is from Sample $\boldsymbol{X}$, and 1 otherwise. Let $n_1$ be the sample size of Sample $\boldsymbol{X}$, $n_2$ be the sample size of Sample $\boldsymbol{Y}$, and $N = n_1 + n_2$. We define
\begin{align*}
J_{(i, j)} &= \begin{cases}
1 &\text{if $g_i = g_j = 0$},\\
2 &\text{if $g_i = g_j = 1$},
\end{cases}\\
 R_1(i,j) &= w_{ij} I(J_{(i, j)}=1),\\
 R_2(i,j) &=  w_{ij} I(J_{(i, j)}=2),\\
 R_1^w &= \sum_{(i, j) \in G}R_1(i,j), \\ 
 R_2^w &= \sum_{(i, j) \in G}R_2(i,j).
\end{align*}

The robust generalized edge-count test statistic is defined to be:
\begin{equation*}
 S_R = (R_1^w - \mu_1^w,  R_2^w - \mu_2^w)(\Sigma^w)^{-1}\begin{pmatrix}
  R_1^w - \mu_1^w\\ 
  R_2^w - \mu_2^w
\end{pmatrix},
 \end{equation*}
where $ \mu_1^w= E(R_1^w), \mu_2^w= E(R_2^w)$, and
\begin{equation*}
\Sigma^w = \begin{pmatrix}
  \textbf{Var} (R_1^w) & \textbf{Cov} (R_1^w, R_2^w)\\ 
  \textbf{Cov} (R_1^w, R_2^w) & \textbf{Var} (R_2^w)
\end{pmatrix} = \begin{pmatrix}
  \Sigma_{11} & \Sigma_{12}\\ 
  \Sigma_{21} & \Sigma_{22}
\end{pmatrix}.
\end{equation*}

\begin{theorem} \label{th:decompose}
$S_R$ can be expressed as 
\begin{equation*}
S_R = (Z^R_\text{diff})^2 + (Z^R_{w})^2,
\end{equation*}
with 
\begin{equation*}
\textbf{Cov}(Z^R_\text{diff},  Z^R_{w}) = 0,
\end{equation*}
where
$
Z_\text{diff}^R = \frac{(R_1^w - R_2^w) - \textbf{E}(R_1^w - R_2^w)}{\sqrt{\textbf{Var}(R_1^w - R_2^w)}}
$, $
Z_w^R= \frac{(qR_1^w + pR_2^w) - \textbf{E}(qR_1^w + pR_2^w)}{\sqrt{\textbf{Var}(qR_1^w + pR_2^w)}}$,
$p = \frac{n_1-1}{N-2}$, and $q = 1- p$.

The proof can be found in the Appendix \ref{Appendix:decompose}. 
\end{theorem}

Theorem \ref{th:decompose} leads us to propose the robust max-type edge-count test statistic:
$$ M_R = \text{max}(Z_w^R , |Z_\text{diff}^R|).
$$

If the graph is relatively flat and no hub is present, then $d_i$ is similar for all $i \in G$, and the weights have little effect. However, in the presence of a problematic hub(s), the weights control the influence of edges connected to the hub, resulting in improved performance. This makes the test statistic more robust to the underlying similarity graph and also resolves the variance boosting problem. 

The analytic expressions of expectations and variances involved above can be obtained by combinatorial analysis under the permutation null distribution. The corresponding analytic expressions are given in Lemma \ref{th:meanvarori}.

\begin{lemma} \label{th:meanvarori}
Under the permutation distribution, we have:
\begin{equation*}
\begin{split}
\mu_1^w =& \sum_{(i, j) \in G}w_{ij} \frac{n_1(n_1-1)}{N(N-1)},\ 
\mu_2^w = \sum_{(i, j) \in G}w_{ij} \frac{n_2(n_2-1)}{N(N-1)},\\
\Sigma_{11} =&\left\{-S_2 +\frac{2(2N-3)}{N(N-1)}S_3+\frac{N-3}{n_2-1}\left(S_1+S_2\right)-\frac{4(N-3)}{N(n_2-1)}S_3 \right\} \times \\
&\frac{n_1n_2(n_1-1)(n_2-1)}{N(N-1)(N-2)(N-3)},\\
 \Sigma_{22} =&\left\{-S_2+ \frac{2(2N-3)}{N(N-1)}S_3 +\frac{N-3}{n_1-1}\left(S_1+S_2\right)-\frac{4(N-3)}{N(n_1-1)}S_3\right\} \times \\
 &\frac{n_1n_2(n_1-1)(n_2-1)}{N(N-1)(N-2)(N-3)},\\
 \Sigma_{12} = &\left\{-S_2 +\frac{2(2N-3)}{N(N-1)}S_3\right\}\frac{n_1n_2(n_1-1)(n_2-1)}{N(N-1)(N-2)(N-3)},
\end{split}
\end{equation*}
where $S_1 = \sum_{(i, j) \in G}w_{ij}^2$, $S_2 = \sum_{(i, j), (i, k) \in G} w_{ij}w_{ik}$ and $S_3 = \sum_{(i, j), (k, l) \in G}w_{ij}w_{kl}$.

The proof to this lemma can be found in the Appendix \ref{Appendix:mean_var}. 
\end{lemma} 

Using the results from Lemma \ref{th:meanvarori}, the expectations and variances involved in $Z_\text{diff}^R$ and $Z_w^R$ can be obtained as follows:
\begin{eqnarray*}
 \textbf{E}(R_1^w - R_2^w) &=& \sum_{(i, j) \in G}w_{ij} \frac{n_1 - n_2}{N},\\
 \textbf{E}(q_wR_1^w+p_wR_2^w) &=&  \sum_{(i, j) \in G}w_{ij} \frac{(n_1-1)(n_2-1)}{(N-1)(N-2)},\\
  \textbf{Var}(R_1^w - R_2^w) & = &
   \left\{ (S_1+S_2)-\frac{4}{N}S_3\right\} \frac{n_1n_2}{N(N-1)},\\
    \textbf{Var}(q_wR_1^w+p_wR_2^w)  &= &\left\{\frac{N-3}{N-2}S_1 -\frac{1}{N-2} S_2 + \frac{2}{(N-1)(N-2)}S_3\right\} \times\\& &
  \frac{n_1n_2}{N(N-1)}\frac{(n_1-1)(n_2-1)}{(N-2)(N-3)}.
\end{eqnarray*}


Large values of $S_R$ and $M_R$ are evidence against the null hypothesis of no distributional difference. The construction of both $S_R$ and $M_R$ allow them to be powerful for general alternatives. When there is a change in mean, both $R_1^w$ and $R_2^w$ tend to be large, and then $Z^R_w$ will be large, which leads to a large $S_R$ and $M_R$. When a change in variance is present, without loss of generality, suppose the sample with the smaller variance is sample $\boldsymbol{X}$, then $R_1^w$ is relatively large compared to its null expectation while $R_2^w$ is relatively small. In this case, $|Z^R_\text{diff}|$ tends to be large which also leads to a large $S_R$ and $M_R$.  

\begin{remark}\label{rm:welldefine}
The test statistics are well-defined under the following conditions:
\begin{enumerate}[(a)]
\item $\sum_{\{j, \text{s.t.} (i, j)\in G\}}w_{ij}$ are not all equal for all $i \in [1, N]$, 
\item $(N-3)S_1-S_2+\frac{2}{N-1}S_3 >0$,
\end{enumerate} 
where $S_1 = \sum_{(i, j) \in G}w_{ij}^2$, $S_2 = \sum_{(i, j), (i, k) \in G} w_{ij}w_{ik}$ and $S_3 = \sum_{(i, j), (k, l) \in G}w_{ij}w_{kl}$. 

The proof to this remark can be found in the Appendix \ref{Appendix:welldefine}. 
\end{remark}

For example, for a completely flat graph (all nodes have the same degree), then $w_{ij} = w_{i', j'},\ \forall i,j$ and $Z_\text{diff}$ is not well-defined. For a star-shaped graph (all observations connect to the same node), $Z_w$ is not well-defined.


The robust test statistics $S_R$ and $M_R$ default to the tests proposed in \cite{gen2017} and \cite{max2019} when $w_{ij} = 1$ for all $(i,j)$. In \cite{gen2017} and \cite{max2019}, each edge has an equal contribution to the test statistic so that even those edges connected to problematic hubs are treated with the same weight as those that are not. By placing weights on the edges, we dampen the influence of hubs and effectively flatten the graph.  The weights $w_{ij}$ must be positive and bounded (asymptotically) below by $1/|G|$. Theorem \ref{th:bounds} ensure  $R_k^w, k = 1,2$ does not vanish to zero when the sample size goes to infinity. 

\begin{theorem} \label{th:bounds}
Let $W$ be a weight function such that $W(i,j) = w_{ij}$ for all edges $(i,j)$. If the weight function $1/|G|\precsim W$ then $ \lim_{N\to\infty}R_k^w  > 0$.

The proof to this theorem can be found in the Appendix \ref{Appendix:bounds}. 
\end{theorem}





\subsection{Choice of Weights} \label{sssec:weights}

The test statistics are defined for general weights that are functions of the node degrees and monotonically decreasing, as defined below.

\begin{definition}
A bivariate function is called monotonically decreasing (or non-increasing), if for all $x_1$,  $x_2$ and $y_0$ such that $x_1<x_2$ one has $f(x_1, y_0) >f(x_2, y_0)$; and for all $y_1$,  $y_2$ and $x_0$ such that $y_1<y_2$ one has $f(x_0, y_1) >f(x_0, y_2)$.
\end{definition}

In practice, users have the flexibility to choose their weights, provided that the test statistics are well-defined following conditions in Remark \ref{rm:welldefine}. Since we allow the graph to be general (it could be MST, NNG, or based on domain-knowledge), and the weights are properties of the graph, this makes obtaining optimal weights for general graph construction challenging. We reserve this line of theoretical analysis for future work. In this paper, we provide recommendations for weights based on empirical studies. We recommend weights that (1) demonstrate reasonable power and (2) meet the conditions for our asymptotic theory.  In particular, we study the performance of three weight functions and show that they work well in a variety of settings.

For edge $(i, j)$, we consider the following weight functions:
\begin{align*}
	W_1(d_i, d_j) &= \frac{1}{\text{max}(d_i, d_j)},\\
	W_2(d_i, d_j) &= \frac{1}{\sqrt{d_id_j}},\\
	W_3(d_i, d_j) &= \frac{2}{(d_i+d_j)}.
\end{align*}

The weight function $W_1$ returns the inverse of the max node degree of an edge $(i,j)$. The weight functions $W_2$ and $W_3$ take the inverse of the arithmetic average and the geometric average of the node degrees of an edge, respectively. All three weight functions are bounded below by $1/|G|$ asymptotically and monotonically decreasing. 

To have a better understanding of the weight functions, we plot the three weights for increasing node degrees in Figure \ref{fig:com_wei}. For a given edge ($i,j$), we fix $d_i$ to be $5$ and increase $d_j$ from 5 to 200. We can see weight function $W_1$ gives the heaviest penalty. When the max node degree is larger than 100, the weight function $W_1$ and $W_3$ provide similar weights. The weight function $W_2$ gives a milder penalty on node degrees among these three weight functions.

% Figure environment removed


To more comprehensively evaluate the performance of these three weight functions, we simulate data with $n_1 = n_2 = 100$ and $d = 200$ from a $d$-dimensional Gaussian distribution under both mean and scale change. The similarity graph is a $5$-MST constructed from Euclidean distance. We consider ten scenarios with increasing hub sizes. Each scenario consists of 100 trials. For each trial, the max node degree is recorded and boxplots under each scenario are shown in Figure \ref{fig:max_de}. From scenario 1 to 10, the maximum node degree increases, so the hub connects to more observations, resulting in a bigger impact on the similarity graph. The number of trials with significance less than 5\% under 10,000 permutations in each scenario is shown in Figure \ref{fig:wei_fun_S} and \ref{fig:wei_fun_M}.
% Figure environment removed

% Figure environment removed

% Figure environment removed
It is clear all three weighted test statistics outperform their non-weighted counterparts. Weight functions $W_1$ and $W_3$ have similar power and tend to dominate across all scenarios. On the other hand, the weight function $W_2$ does not have enough weight to resist the impact of the hub and has slightly lower power across all scenarios compared to $W_1$ and $W_3$. This is because  $W_2$ is unable to counteract the variance boosting problem as effectively as $W_1$ and $W_3$. As shown in Figure \ref{fig:exp}, the variances of $R_{1}^w$ and $R_{2}^w$ are well controlled for $W_1$ and $W_3$, but for $W_2$ the variance grows as the max node degree grows. 

% Figure environment removed


In what follows, we recommend using the weight function $W_1$ or $W_3$. We discuss how these weights satisfy our conditions for obtaining the limiting null distribution in Section \ref{sec:asymptotic}.

\section{Asymptotic Results}  \label{sec:asymptotic}
\subsection{Asymptotic null distribution of the robust test statistics}
When the sample size is small, we can obtain $p$-values for the test statistics by resampling from the permutation distribution. However, as the sample size increases, this becomes computationally prohibitive. To make the tests practical for modern data sets, we study the limiting distributions of the robust edge-count test statistics. 

We define
\begin{itemize}
    \item $A_{(i, j)} = \{(i, j)\}\cup \{(i', j')\in G, (i, j)$ and $(i', j')$ share a node\},
    \item $B_{(i, j)} = A_{(i, j)} \cup \{(i'', j'')\in G, \exists (i', j') \in A_{(i, j)}$, such that $(i', j')$ and $(i'', j'')$ share a node\},
    \item $W(A_{(i, j)}) = \sum_{(i', j')\in A_{(i, j)}}w_{i'j'}$,
    \item $W(B_{(i, j)}) = \sum_{(i'', j'')\in B_{(i, j)}}w_{i''j''}$.
\end{itemize}

\begin{theorem}\label{th:conver}
Under conditions
\begin{enumerate}[(i)]
		\item $G = \mathcal{O}(N^{\alpha}), 1\leq \alpha < 1.25$,
		\item 
		$S_1 + S_2 - \frac{4}{N}S_3 = \mathcal{O}(S_1 + S_2)$,
        \item $\sum_{(i, j)\in G}(w_{ij}|A_{(i, j)}|)^2 = o(S_1\sqrt{N})$,
        \item $\sum_{(i, j)\in G}w_{ij}W(A_{(i, j)})W(B_{(i, j)})= o(S_1)^{1.5}$,
\end{enumerate}
 as $n_1$, $n_2$, $N\rightarrow\infty$ and $n_1/N \rightarrow \lambda \in (0, 1)$, $Z_w^R\xrightarrow{\mathcal{D}}N(0,1)$ and $Z_\text{diff}^R\xrightarrow{\mathcal{D}}N(0,1)$ under the permutation null distribution.
 
The proof utilizes Stein's theorem from \cite{chen2005stein}, and details are provided in the Supplementary Material Section \ref{Appendix:limit_distribution}. 
\end{theorem}

\begin{corollary}
Under conditions in Theorem \ref{th:conver},  as $n_1, n_2, N\rightarrow\infty$ and $n_1/N \rightarrow \lambda \in (0, 1)$, $S_R\xrightarrow{\mathcal{D}}\chi_2$ under the permutation null distribution.
\end{corollary}


Condition (ii) ensures $Z^R_\text{diff}$ is asymptotically well-defined. 
Conditions (iii) and (iv) prevent the sum of weights in the hub from growing too large.

To see that the conditions hold easily in the presence of hubs, we generated data from Gaussian distribution as well as the heavy-tailed $t_5$ distribution and construct $5$-MST from Euclidean distance. Ratios of the key quantities involved in the conditions using 5-MST are shown in Figure \ref{fig:limit}. Once we assign weights, the ratios $\sum(w_{ij}|A_{(i, j)}|)^2/ (S_1\sqrt{N})$ and $(\sum w_{ij}W(A_{(i, j)})W(B_{(i, j)})/(S_1)^{1.5}$ are bounded by $o(1)$ under all scenarios for both $W_1$ and $W_3$. 

% Figure environment removed

To assess the performance for finite sample sizes, we compare the critical values generated from the 10,000 permutations with the critical values obtained from asymptotic results under the null hypothesis. The data are generated from log-normal and $t_5$ distributions with $n_1 = n_2 = 50$ and $d=100$. The graphs are generated using 5-MST with Euclidean distance. The boxplots of the differences between asymptotic critical values and permutation critical values using different weight functions are shown in Figure \ref{fig:cri_value}. The p-value approximations are reasonable based on small differences shown in the boxplots.

% Figure environment removed

\section{Performance Analysis}\label{sec:performance}
\subsection{Hubs in high-dimensional data}

We examine the performance of the robust test statistics on high-dimensional data, which tend to contain hubs. We set $n_1=n_2=100$. The similarity graph is the 5-MST constructed from $L_2$ distances of the pooled observations. We present the power of the tests, which is estimated to be the number of trials (out of 100) with significance less than 5\%. We also report median of the max node degrees in each trial (over 100 trials), denoted as $\tilde{d}_{\text{max}}$.

Table \ref{tab:ori_both_nor} shows results for log-normal data differing in location parameter $\mu$, which results in both mean and variance differences between the two samples. The change is denoted by $\Delta_{\mu}$ and is selected so that all the tests have moderate power. As the dimension increases, and the maximum node degree subsequently increases, the robust edge-count tests out-perform their non-weighted counterparts. The improvement of the robust edge-count tests are more noticeable in the heavy-tail $t$ distribution. As shown in Table \ref{tab:ori_both_t}, $\Delta_{v}$ denotes the change in degrees of freedom compared to degrees of freedom $v = 5$ and $\Delta_{\mu}$ denotes the change in non-centrality parameter in a $d$-dimensional non-central $t$ distribution.

\begin{table*}[ht]
    \caption{Number of trials (out of 100) with significance less than 5\%. Data generated from multivariate log-normal distribution.}
    \begin{tabular*}{\columnwidth}{@{\extracolsep{\fill}}ccccccccc@{\extracolsep{\fill}}}
        \toprule
        \multicolumn{3}{c}{} & \multicolumn{6}{c}{Test-statistic} \\
        \cmidrule{4-9}
        \multirow{2}{*}{d}&\multirow{2}{*}{$\Delta_{\mu}$}&\multirow{2}{*}{$\tilde{d}_{\text{max}}$}&$S$  & \multicolumn{2}{c}{$S_R$} & $M$ & \multicolumn{2}{c}{$M_R$}  \\
        \cmidrule{5-6}\cmidrule{8-9}
       \multicolumn{4}{c}{} & $W_1$  & $W_3$ & & $W_1$  & $W_3$ \\
        \midrule
         200 & 1.7 & 120 &78 & 78 & 78& 76 & 83 & \textbf{84}\\
         500 &  1.9 & 156 & 59 & 73 & 74& 66 & 78 & \textbf{82}\\
         1000 & 2.5 &175.5 &75 & 90 & 91& 79 & \textbf{94} & 93\\
         1500 & 2.7 & 182.5 &74 & 91 & 92& 72 & \textbf{93} & 92\\
        \bottomrule
    \end{tabular*}
    \label{tab:ori_both_nor}
\end{table*}

\begin{table*}[ht]
    \caption{Number of trials (out of 100) with significance less than 5\%. Data generated from multivariate non-central $t$ distribution.}
    \begin{tabular*}{\columnwidth}{@{\extracolsep{\fill}}cccccccccc@{\extracolsep{\fill}}}
        \toprule
        \multicolumn{4}{c}{} & \multicolumn{6}{c}{Test-statistic} \\
        \cmidrule{5-10}
        \multirow{2}{*}{d}&\multirow{2}{*}{$\Delta_{\mu}$}&  \multirow{2}{*}{$\Delta_{v}$}&\multirow{2}{*}{$\tilde{d}_{\text{max}}$}&$S$  & \multicolumn{2}{c}{$S_R$} & $M$ & \multicolumn{2}{c}{$M_R$}  \\
        \cmidrule{6-7}\cmidrule{9-10}
       \multicolumn{5}{c}{} & $W_1$  & $W_3$ & & $W_1$  & $W_3$ \\
        \midrule
         200 & 0.8& 1  & 85 &71 & 86 & 86& 72 & \textbf{90} & 89\\
         500 &  1 & 0.5 & 96 & 48 & 74 & 74& 56 & \textbf{79} & 76\\
         1000 & 1.5& 0.4  &108.5 &61 & 82 & 80& 60 & \textbf{87} & \textbf{87}\\
         1500 & 2 & 0.3& 106 &63 & 84 & 81 &65 & \textbf{85} & 84\\
        \bottomrule
    \end{tabular*}
    \label{tab:ori_both_t}
\end{table*}

\subsection{Hub generated from influential observations}
In this section, we will study how the tests behave when a single observation generates a hub; consider this observation an influential point. The setup is as follows: for each trial, the two samples are generated from $d$-dimensional Gaussian distributions that differ in both mean and variance with $n_1=n_2=100$.  The mean difference (in $L_2$ distance) of the two samples is denoted by $\Delta_{\mu}$ and the ratio between the standard deviations is denoted by $\rho_{\sigma}$. 

Then one observation from the sample with a larger variance (sample 2) is replaced by an influential point that induces a hub, and the influential point is generated from the same distribution as other observations in sample 2. The results are reported in Table \ref{tab:mean_var_inf}. 

For Table \ref{tab:mean_var_inf} , the robust tests using $W_1$ and $W_3$ tend to have similar power. 
When there is an influential observation, the edge-count tests $S$ and $M$ suffer from lower power while the robust edge-count test statistics, $S_R$ and $M_R$, still provide reasonable performance.

\begin{table*}[ht]
    \caption{Number of trials (out of 100) with significance less than 5\%. Multivariate Gaussian data with influential observation.}
    \begin{tabular*}{\columnwidth}{@{\extracolsep{\fill}}cccccccccc@{\extracolsep{\fill}}}
        \toprule
        \multicolumn{4}{c}{} & \multicolumn{6}{c}{Test-statistic} \\
        \cmidrule{5-10}
        \multirow{2}{*}{d}&\multirow{2}{*}{$\Delta_{\mu}$}&  \multirow{2}{*}{$\rho_{\sigma}$}&\multirow{2}{*}{$\tilde{d}_{\text{max}}$}&$S$  & \multicolumn{2}{c}{$S_R$} & $M$ & \multicolumn{2}{c}{$M_R$}  \\
        \cmidrule{6-7}\cmidrule{9-10}
       \multicolumn{5}{c}{} & $W_1$  & $W_3$ & & $W_1$  & $W_3$ \\
        \midrule
         50 & 0.1& 1.08 & 84& 55 & 93  & 91 & 58 & \textbf{94}  & 92\\
         100 & 0.2& 1.06  & 99.5 &47 & 98  & 98&54 & 99 & \textbf{100}\\
         150 &  0.3 & 1.04&92.5 & 31 & 89  & 89  & 35 & \textbf{93}  & 92\\
         200 &0.6& 1.038  &92 &60 & 96  & 95& 60 & \textbf{97} & \textbf{97} \\
         250 & 0.8 & 1.035&  99.5 &57 & 95 & 94  &59 & \textbf{99}  & 98 \\
        \bottomrule
     \end{tabular*}
    \label{tab:mean_var_inf}
\end{table*}


\section{Real Data Application}\label{sec:application}
We illustrate the robust graph-based tests on Chicago taxi trip dataset. This data is publicly available on the Chicago Data Portal website  (\url{https://data.cityofchicago.org/Transportation/Taxi-Trips/wrvz-psew}) and includes pick-up and drop-off dates, times, and locations for each taxi trip. The pickup and drop-off locations are recorded as the center of the census tract or the community area if the census tract has been hidden for privacy. In this study, we focus on the taxi trips in 2020. There are 563 unique pick-up locations and 635 unique drop-off locations. Via an initial data inspection, we find that taxis appear to be less active between midnight and 5 am. The frequency of taxi trips also appears to vary from month to month; the most notable difference can be observed when comparing January - March to the rest of the year. This is due to the fact that the statewide stay-at-home order signed by the Illinois Governor took effect on March 21 in response to the spread of COVID-19. Taxi rides were scattered all over Chicago and the most popular spots are around Downtown Chicago and O'Hare International Airport. For most of the census tracts away from Downtown Chicago, the number of taxi rides is relatively small. 

We count the frequency of taxi pickups and drop-offs in each location for a specified time interval. We analyze taxi pickups and drop-offs separately. Each pick-up observation is a $563\times 1$ vector of taxi trip counts for a time unit, where each element represents a unique pick-up location. For example, if the time unit is days, then the pick-up observation is the frequency of taxi pick-ups across various locations for a given day. The drop-off observations are defined similarly. 

When constructing the matrices, many entries would be 0 or 1 since the taxi rides may not happen or only happen rarely in some locations for a given period. With such sparse count matrices, a hub with a large node degree is likely to arise. These hubs become problematic in the two-sample tests, and since the type of change is not specified, it is difficult to identify problematic observations just by examining the raw data. We will demonstrate that the robust edge-count test can circumvent this problem and lead to reasonable results even in the presence of sparse, high-dimensional taxi cab data. 

To illustrate the new tests, we consider three different scenarios and compare the performance of the new tests with existing methods. In Scenario I and II, we consider taxi trips in a time interval  (i.e. 5 - 9 am for each day), whereas in Scenario III, we consider taxi trips in a specific hour. The similarity graphs, $5$-MST, are constructed from the frequency of taxi trips across different locations based on $L_1$ distances, with each node representing the counts for a time unit. For this application, the robust edge-count tests are constructed using weight function $W_1$. For all tests, p-values are obtained via 10,000 permutations. 

\subsection{Scenario I} 
We compare taxi rides during morning rush hour, defined as 5 - 9 am, for November versus December. Results are shown in Table \ref{tab:6am_NovDec}, where $d_{max}$ denotes the maximum node degree, and $\sum_{i = 1}^N G_i^2$ and $C$ (described in Section \ref{sec:limitation}) reflect the structural features of the similarity graph. Pick-ups and drop-offs are analyzed separately. 

\begin{table*}[ht]
    \caption{P-values of the edge-count tests based on 10,000 permutations. Tests compare taxi trip records from 5 am to 9 am in November to trip records from 5 am to 9 am in December. The p-values smaller than 0.1 are bolded.}
    \begin{tabular*}{\columnwidth}{@{\extracolsep{\fill}}lccccccc@{\extracolsep{\fill}}}
        \toprule
        
       & $d_{max}$& $\sum_{i = 1}^N G_i^2$&$C$ &$S$ & $M$ & $S_R$ & $M_R$\\
       \midrule
      Pick-up&34&7306&3353&0.1227	& 0.1400&	\textbf{0.0316}&	\textbf{0.0221} \\
      Drop-off&22&6940&3170&0.1533 & 0.1500&	\textbf{0.0217}	&\textbf{0.0159}\\
        \bottomrule
    \end{tabular*}

    \label{tab:6am_NovDec}
\end{table*}

From Table \ref{tab:6am_NovDec}, we see that the existing tests ($S$ and $M$) are unable to reject the null, while the robust tests have power to do so. The observation which generates the node with maximum node degree is November 23, 2020, the day before Thanksgiving, for both pick-ups and drop-offs.  This hub is highly influential to our results and as a sanity check, we evaluate our results without the November 23 observation. After removing this observation, the p-values of edge-count tests are shown in Table \ref{tab:6am_NovDec_after}; we see that without the presence of this hubs all the tests reject the null hypothesis and conclude that taxi rides in November and December are significantly different. Observe that the maximum node degree of 5-MST after removing the Nov. 23 observation decreases for the taxi pick-ups. For the drop-off data, the maximum node degree does not change much after removing the observation from November 23, but the structure of the graph changes such that most nodes with large node degrees are from observations in December, instead of November. The quantities $\sum_{i = 1}^N G_i^2$ and $C$ decrease for both pick-ups and drop-offs, which means the hubness subsides when removing the influential observation.


\begin{table*}[ht]
    \caption{P-values of the edge-count tests based on 10,000 permutations. Tests compare taxi trip records from 5 am to 9 am in November and December after removing taxi rides on November 23. }
    \begin{tabular*}{\columnwidth}{@{\extracolsep{\fill}}lccccccc@{\extracolsep{\fill}}}
        \toprule
        
       & $d_{max}$& $\sum_{i = 1}^N G_i^2$&$C$ &$S$ & $M$ & $S_R$ & $M_R$\\
       \midrule
      Pick-up&21&6760 & 3085 & 0.0021	&0.0046	&0.0038&	0.0041\\
      Drop-off&20&6724& 3067&0.0390	&0.0496	&0.0121	&0.0123\\
        \bottomrule
    \end{tabular*}

    \label{tab:6am_NovDec_after}
\end{table*}

In this scenario, there appears to be potentially only one highly influential observation that can lead to unstable results in existing tests, while the new edge-count tests demonstrate robustness against this observation. 

\subsection{Scenario II} 
In the early morning hours (12 am - 6 am), the number of taxi rides is quite small compared to the daytime; this sparsity makes it easier to induce hubs with large node degrees when constructing the 5-MST. We compare taxi drop-offs during the early morning hours (12 am - 6 am) for April versus May, immediately after the lockdown was announced. The robust edge-count tests ($S_R, M_R$) and the non-weighted versions ($S$, $M$) result in conflicting conclusions at $10\%$ significance level (Table \ref{tab:5am_AprilMay}). We see that for taxi drop-offs, $S$ and $M$ cannot reject the null, while the robust tests, $S_R$ and $M_R$, provide evidence of a difference between travel patterns in the two samples. 


\begin{table*}[ht]
    \caption{P-values of the edge-count tests based on 10,000 permutations. Tests compare taxi trip records from 12 am to 6 am in April versus May. The p-values smaller than 0.1 are bolded.}
    \begin{tabular*}{\columnwidth}{@{\extracolsep{\fill}}lccccc@{\extracolsep{\fill}}}
        \toprule
        
       & $d_{max}$&$S$ & $M$ & $S_R$ & $M_R$\\
       \midrule
      Drop-off&39&0.3423&0.1738 & \textbf{0.0306} & \textbf{0.0277} \\
        \bottomrule
    \end{tabular*}

    \label{tab:5am_AprilMay}
\end{table*}

The existing tests ($S$ and $M$) are highly sensitive to the inclusion and exclusion of specific observations and can suffer from low power in the presence of a hub. The distribution of node degrees for the 5-MST is shown in Table \ref{tab:count_table}. The observation generating the node with max node degree is from May 11. As shown in Table \ref{tab:details_AprilMay}, after removing taxi trips on May 11, the generalized test ($S$) still cannot reject the null while $M$ can reject the null, leading to conflicting results among even the existing tests.
On the other hand, removing the observation on May 18 (with a moderately large node degree of 17) leads to significant test results for both $S$ and $M$. 
Similarly, removing the observation representing taxi drop-offs on April 29 (with node degree 28), leads to significant test results for both $S$ and $M$. 

\begin{table*}[ht]
    \caption{Distribution of node degrees in 5-MST generated from taxi drop-offs in April and May.}
    \begin{tabular*}{\columnwidth}{@{\extracolsep{\fill}}lccccccccccccccc@{\extracolsep{\fill}}}
        \toprule
        node degree & $<10$ & 10 & 11 & 12 & 13 & 14 & 15 & 16 & 17 & 19 & 20 & 26 & 27 & 28 & 39 \\
        \midrule
      frequency & 42 & 2 & 1 & 4 & 2 & 1 & 1 & 1 & 1 & 1  & 1 & 1 & 1 & 1 & 1\\
        \bottomrule
    \end{tabular*}
    \label{tab:count_table}
\end{table*}

\begin{table*}[ht]
 \caption{Results for the two-sample test comparing taxi drop-offs in April versus May after removing observations. We report the observation's date, their node degree $d$, the number of edges connecting to observations from the same sample and the different sample, and the p-values after removing the observation on the specified date. The p-values smaller than 0.1 are bolded.}
 \begin{tabular*}{\columnwidth}{@{\extracolsep{\fill}}lcccccccc@{\extracolsep{\fill}}}
        \toprule
        \multirow{2}{*}{date}&\multirow{2}{*}{$d$}&\multicolumn{2}{c}{number of edges}  &\multicolumn{4}{c}{p-value}                   \\
        	\cmidrule(r){3-4}\cmidrule(r){5-8}
		 & &same &different &$S$ &$M$  &$S_R$ &$M_R$ \\
        \midrule
       May 11&39&17&22& 0.2020 &\textbf{0.0857} &\textbf{0.0645} &\textbf{0.0428}  \\
          May 18 &17&6&11& \textbf{0.0787} &\textbf{0.0252} &\textbf{0.0084} & \textbf{0.0064}  \\
          April 29 & 28& 19& 9&\textbf{0.0153}&\textbf{0.0084}&\textbf{0.0071}& \textbf{0.0078} \\
        \bottomrule
    \end{tabular*}
       \label{tab:details_AprilMay}
\end{table*}

In this scenario, the inclusion and exclusion of specific observations may lead to unstable or conflicting results. On the other hand, the robust edge-count statistics provide more stable and consistent results.
 
\subsection{Scenario III}

We hereby present a scenario wherein the discernment of the problematic or influential observation becomes challenging. Table \ref{tab:1_6_MayJune} shows edge-count test results for testing taxi rides from 12 am to 6 am in May versus June. Under scenario III, we count the taxi rides every hour, so the pooled pick-up and drop-off data are a $366 \times 563$ matrix and a $366 \times 635$ matrix, respectively. As shown in the table, $S$ and $M$ cannot reject the null at $10\%$ significance level for both taxi pick-ups and drop-offs, while the robust tests draw the opposite conclusion. 

\begin{table*}[ht]
    \caption{P-values of the edge-count tests based on 10,000 permutations. Tests compare 12 am - 6 am taxi trip records between May and June. The p-values smaller than 0.1 are bolded.} 
    \begin{tabular*}{\columnwidth}{@{\extracolsep{\fill}}lccccc@{\extracolsep{\fill}}}
        \toprule
        
       & $d_{max}$&$S$ & $M$ & $S_R$ & $M_R$\\
       \midrule
      Pick-up&74&0.8793&0.6818& \textbf{0.0116} & \textbf{0.0135 }\\
      Drop-off&81&0.4251&0.2313 & \textbf{0.0075} & \textbf{0.0279}  \\
        \bottomrule
    \end{tabular*}

    \label{tab:1_6_MayJune}
\end{table*}

As a sanity check, we checked individual tests comparing taxi rides between May and June for each hour from 12 am to 6 am. As shown in Table \ref{tab:MayJune_segpickup} and Table \ref{tab:MayJune_segdropoff}, the null hypothesis that taxi rides in May and June are from the same distribution is rejected for the first two hours, and the smallest p-values, which appear in the test of taxi rides between 12 am to 1 am, are less than 1\%. This can be considered as evidence that taxi rides from 12 am and 6 am are different in May and June, which suggests that the results of edge-count tests $S$ and $M$ in Table \ref{tab:1_6_MayJune} suffer from low power.
\begin{table*}[ht]
    \caption{P-values of the edge-count tests based on 10,000 permutations. Tests compare one-hour taxi pick-up records from 12 am to 6 am between May and June. The p-values smaller than 0.1 are bolded.}
    \begin{tabular*}{\columnwidth}{@{\extracolsep{\fill}}lcccc@{\extracolsep{\fill}}}
        \toprule
         \multicolumn{1}{c}{} & \multicolumn{4}{c}{Pick-up}  \\
        \cmidrule{2-5}
       hour ( am ) & $S$ & $M$ & $S_R$ & $M_R$\\
       \midrule
      12 - 1 &$\mathbf{<0.0001}$&$\mathbf{<0.0001}$ &$\mathbf{<0.0001}$&$\mathbf{<0.0001}$\\
      1 - 2  &\textbf{0.0016}&\textbf{0.0075}&\textbf{0.0089}&\textbf{0.0408}\\
      2 - 3&0.2043&0.6474&0.1058&0.4711\\
      3 - 4&0.5574&0.4254&0.1043&\textbf{0.0528}\\
      4 - 5 &0.2370&0.3271&0.4484&0.3478 \\
      5 - 6  &0.1290&\textbf{0.0545}&\textbf{0.0358}&\textbf{0.0225}\\
        \bottomrule
    \end{tabular*}

    \label{tab:MayJune_segpickup}
\end{table*}

\begin{table*}[ht]
    \caption{P-values of the edge-count tests based on 10,000 permutations. Tests compare one-hour taxi drop-off records from 12 am to 6 am between May and June. The p-values smaller than 0.1 are bolded.}
    \begin{tabular*}{\columnwidth}{@{\extracolsep{\fill}}ccccc@{\extracolsep{\fill}}}
        \toprule
         \multicolumn{1}{c}{} & \multicolumn{4}{c}{Drop-off} \\
        \cmidrule{2-5}
       hour ( am ) & $S$ & $M$ & $S_R$ & $M_R$\\
       \midrule
     12 - 1 &$\mathbf{<0.0001}$&$\mathbf{<0.0001}$ &$\mathbf{<0.0001}$&$\mathbf{<0.0001}$\\
      1 - 2  &\textbf{0.0080}&\textbf{0.0113} & \textbf{0.0200} & \textbf{0.0488}\\
      2 - 3 &0.2177&0.2350 & 0.2874 & 0.3586 \\
      3 - 4 &0.2614&0.1145 & 0.2172 & 0.1318\\
      4 - 5  &0.2163&0.2619 & 0.6679& 0.5779\\
      5 - 6  &0.4305&0.2988 &0.7926 & 0.6540\\
        \bottomrule
    \end{tabular*}

    \label{tab:MayJune_segdropoff}
\end{table*}


%To better understand the data, we examine observations with large node degrees while constructing similarity graphs. The taxi pick-ups that generate two hubs with node degrees larger than 70 in 5-MST are rides between 5 am and 6 am on June 23, and rides between 2 am and 3 am on May 11. Removing these hubs results in similar p-values as shown in Table \ref{tab:1_6_MayJune} and does not change the conclusion. For taxi drop-offs, nodes with large degrees are generated from observations between 2 am and 3 am on May 4, between 4 am and 5 am on June 20, and between 4 am and 5 am on June 19. After removing these observations, the null can be rejected at 10\% significance level. Therefore, it is possible that we may identify potentially influential observations by examining the structure of the graph for taxi drop-offs, but for taxi pick-ups, it is difficult to discern which observations are influential.


\section{Conclusion}\label{sec:conclusion}
We propose robust edge-count two-sample tests that address an aspect of the curse of dimensionality - the tendency of high-dimensional datasets to contain hubs. This tendency manifests itself in graph-based tests when constructing similarity grabs. We can consider observations that induce hubs can be treated as influential or outliers and their inclusion/exclusion in the analysis can lead to conflicting or unstable results. Our proposed robust edge-count tests mitigates this problem and can outperform the existing edge-count tests in the presence of the hubs while providing comparable power even when the graph is relatively flat. The robust edge-count tests are constructed by applying weights that are functions of node degrees to the edge-counts. Specific weight functions with desirable properties are recommended. We reserve designing an optimal graph-based weights for generic graphs as a topic worthy for future study. 

The asymptotic permutation null distributions of the robust test statistics are given under some mild conditions on the weights. These conditions are shown through numeric studies to be easily satisfied even for heavy-tailed distributions in the presence of hubs. The $p$-value approximations based on asymptotic results are reasonably close to the permutation $p$-value for finite sample size, making the approach easy to apply to large data sets. 

Simulation studies show that the robust edge-count tests have power gains over existing edge-count tests in high dimensions, since hubs are more easily generated as the dimension increases. The improvement is significant when the hub is generated from an influential point. An application of the tests on taxi data in Chicago demonstrate the robust test statistics utility in sparse, high-dimensional settings. 

%%%%%%%%%%%%%%

\bibliographystyle{amsplain}
\bibliography{bibliography}


\appendix

\section{Proof of Lemma \ref{th:meanvarori}} \label{Appendix:mean_var}
The mean and variance of $R_1^w$ under the permutation null distribution can be derived as follows:
\begin{equation*}
\begin{split}
\mu_1^w =& \sum_{(i, j) \in G} w_{ij}P(J_{(i, j)} = 1) 
	= \sum_{(i, j) \in G}w_{ij} \frac{n_1(n_1-1)}{N(N-1)},\\
    E((R_1^w)^2) =&  \sum_{(i, j), (k, l) \in G} w_{ij}w_{kl}P(J_{(i, j)} = 1, J_{(k, l)} = 1) \\ =& S_1 \frac{n_1(n_1-1)}{N(N-1)}+S'_2 \frac{n_1(n_1-1)(n_1-2)}{N(N-1)(N-2)}+S'_3\frac{n_1(n_1-1)(n_1-2)(n_1-3)}{N(N-1)(N-2)(N-3)},\\
    \Sigma_{11} =& E((R_1^w)^2) - E^2(R_1^w),
\end{split}
\end{equation*}
where 
\begin{align*}
S_1 &= \sum_{(i, j) \in G}w_{ij}^2, \\
S'_2 &= \sum_{\substack{(i, j), (i, k) \in G\\ k, l \text{ are different}}}w_{ij}w_{ik}, \\
S'_3 &= \sum_{\substack{(i, j), (k, l) \in G\\ i, j, k, l \text{ all different}}} w_{ij}w_{kl}.
\end{align*}


Similarly, we can get the mean and variance of $R_2^w$ under the permutation null distribution:

\begin{equation*}
\begin{split}
\mu_2^w =&  \sum_{(i, j) \in G} w_{ij}P(J_{(i, j)} = 2)  = \sum_{(i, j) \in G}w_{ij} \frac{n_2(n_2-1)}{N(N-1)},\\
 E((R_2^w)^2) =& S_1 \frac{n_2(n_2-1)}{N(N-1)}+S'_2 \frac{n_2(n_2-1)(n_2-2)}{N(N-1)(N-2)}+S'_3\frac{n_2(n_2-1)(n_2-2)(n_2-3)}{N(N-1)(N-2)(N-3)}\\
    \Sigma_{22} =& E((R_2^w)^2) - E^2(R_2^w).
\end{split}
\end{equation*}

The covariance of $R_1^w$ and $R_2^w$ under the permutation null distribution can be derived as follows:
\begin{equation*}
\begin{split}
    E(R_1^wR_2^w) &=  \sum\limits_{(i, j), (k, l) \in G} w_{ij}w_{kl}P(J_{(i, j)} = 1, J_{(k, l)} = 2) \\& = S'_3\frac{n_1(n_1-1)n_2(n_2-1)}{N(N-1)(N-2)(N-3)},\\
    \Sigma_{12} &= E(R_1^wR_2^w) - E(R_1^w)E(R_2^w).
\end{split}
\end{equation*}

Note :
\begin{equation*}
\begin{split}
\sum_{\substack{{(i, j), (k, l) \in G}\\ {i, j, k, l \text{ all different}}}} w_{ij}w_{kl} = &\sum_{(i, j), (k, l) \in G} w_{ij}w_{kl} - \sum_{\substack{{(i, j), (i, k) \in G}\\ {k, l \text{ are different}}}} w_{ij}w_{ik} - \sum_{(i, j) \in G}w_{ij}^2,\\
\sum\limits_{\substack{{(i, j), (i, k) \in G}\\ {k, l \text{ are different}}}} w_{ij}w_{ik}=&\sum\limits_{(i, j), (i, k) \in G} w_{ij}w_{ik} -  \sum\limits_{(i, j) \in G}w_{ij}^2.\\
\end{split}
\end{equation*}

The variance and covariance can be simplified as
\begin{equation*}
\begin{split}
 \Sigma_{11} &= \frac{n_1n_2(n_1-1)(n_2-1)}{N(N-1)(N-2)(N-3)}\left\{\frac{N-3}{n_2-1}S_1+\frac{n_1-2}{n_2-1}S_2 +\frac{6(n_2-1)-4n_1(N-3)}{N(N-1)(n_2-1)}S_3\right\}\\
  &=\frac{n_1n_2(n_1-1)(n_2-1)}{N(N-1)(N-2)(N-3)}\left\{-S_2+\frac{2(2N-3)}{N(N-1)}S_3+\right.\\
  &\quad\left. \frac{N-3}{n_2-1}   \left(S_1+S_2\right)-\frac{4(N-3)}{N(n_2-1)}S_3\right\},\\
  \Sigma_{12} &= \frac{n_1n_2(n_1-1)(n_2-1)}{N(N-1)(N-2)(N-3)}\left\{-S_2+\frac{2(2N-3)}{N(N-1)}S_3\right\},\\
\Sigma_{22}& =\frac{n_1n_2(n_1-1)(n_2-1)}{N(N-1)(N-2)(N-3)}\left\{\frac{N-3}{n_1-1}S_1+\frac{n_2-2}{n_1-1}S_2 +\frac{6(n_1-1)-4n_2(N-3)}{N(N-1)(n_1-1)}S_3 \right\}\\
  &=\frac{n_1n_2(n_1-1)(n_2-1)}{N(N-1)(N-2)(N-3)}\left\{-S_2+\frac{2(2N-3)}{N(N-1)}S_3+\right.\\
  &\quad\left. \frac{N-3}{n_1-1}\left(S_1+S_2\right)-\frac{4(N-3)}{N(n_1-1)}S_3\right\},
\end{split}
\end{equation*}
where $S_1 = \sum_{(i, j) \in G}w_{ij}^2$, $S_2 = \sum_{(i, j), (i, k) \in G} w_{ij}w_{ik}$ and $S_3 = \sum_{(i, j), (k, l) \in G} w_{ij}w_{kl}$.
\section{Proof of Remark \ref{rm:welldefine}} \label{Appendix:welldefine}
\begin{equation*}
 \begin{split}
 &\sum\limits_{(i, j) \in G}w_{ij}^2+\sum\limits_{(i, j), (i, k) \in G} w_{ij}w_{ik} \\=&\sum\limits_{i = 1}^N(\sum\limits_{\{j, \text{s.t.} (i, j)\in G\}}w_{ij})^2\\
 \geq& \frac{1}{N}(\sum\limits_{i = 1}^N\sum\limits_{\{j, \text{s.t.} (i, j)\in G\}}w_{ij})^2 \\
 = &\frac{4 }{N}(\sum\limits_{(i, j), (k, l) \in G} w_{ij}w_{kl} ).
 \end{split}
 \end{equation*}

\begin{equation*}
\begin{split}
 \textbf{Var}(R_1^w-R_2^w) >0 \Leftrightarrow &\sum\limits_{\{j \in G_i\}}w_{ij} \text{ are not all equal} \\&\text{for all } i \in [1, N],\\
 \textbf{Var}(q_wR_1^w+p_wR_2^w) >0 \Leftrightarrow & (N-3)S_1-S_2+\frac{2}{N-1}S_3 >0.
 \end{split}
 \end{equation*}

\section{Proof of Theorem \ref{th:decompose}} \label{Appendix:decompose}
Let $\textbf{R} = \begin{pmatrix}
  R_1^w\\ 
  R_2^w
\end{pmatrix}$, $\textbf{C} = \begin{pmatrix}
  1 & -1\\ 
  q & p
\end{pmatrix}$, $R_{\text{diff}}^w = R_1^w-R_2^w$ and $R_w^w = q_wR_1^w+p_wR_2^w$.
\begin{equation*}
\begin{split}
    S &= (\textbf{R} - E(\textbf{R} ))^T\mathbf{\Sigma}^{-1}(\textbf{R} - E(\textbf{R}))\\
    &=(\textbf{R} - E(\textbf{R} ))^T\textbf{C}^T(\textbf{C}^T)^{-1}\mathbf{\Sigma}^{-1}\textbf{C}^{-1}\textbf{C}(\textbf{R} - E(\textbf{R}))\\
    &=(\textbf{C}(\textbf{R} - E(\textbf{R} )))^T(\textbf{C}\mathbf{\Sigma}\textbf{C}^T)^{-1}(\textbf{C}(\textbf{R} - E(\textbf{R}))),\\
     \textbf{C}\mathbf{\Sigma}\textbf{C}^T & = \textbf{C}\begin{pmatrix}
  \text{Var}(R_1^w) & \text{Cov}(R_1^w, R_2^w)\\ 
  \text{Cov}(R_1^w, R_2^w) & \text{Var}(R_2^w)
\end{pmatrix}\textbf{C}^T,\\
\textbf{C}\mathbf{\Sigma}\textbf{C}^T &= \begin{pmatrix}
  \text{Var}(R_{\text{diff}}^w)& C_1\\ 
  C_1 & \text{Var}(R_w^w)
\end{pmatrix},
\end{split}
\end{equation*}
where 
\begin{equation*}
\begin{split}
\text{Var}(R_{\text{diff}}^w) &= \text{Var}(R_1^w) -2\text{Cov}(R_1^w, R_2^w) + \text{Var}(R_2^w),\\
\text{Var}(R_w^w) &= q^2\text{Var}(R_1^w) +2pq\text{Cov}(R_1^w, R_2^w) + p^2\text{Var}(R_2^w),\\
C_1 &= q\text{Var}(R_1^w) +(p-q)\text{Cov}(R_1^w, R_2^w) - p\text{Var}(R_2^w)\\
    &=\frac{n_1n_2(n_1-1)(n_2-1)}{N(N-1)(N-2)N(N-3)}\left\{\frac{(N-3)(n_2-1)}{(N-2)(n_2-1)}\left(S_1+S_2 - \frac{4}{N}S_3\right)-\right.\\
    &\quad\left.\frac{(N-3)(n_1-1)}{(N-2)(n_1-1)}\left(S_1+S_2-\frac{4}{N}S_3\right)\right\} \\
    &= 0.
\end{split}
\end{equation*}

So \begin{equation*}
    S_R = \frac{(R_{\text{diff}}^w - E(R_{\text{diff}}^w))^2}{\text{Var}(R_{\text{diff}}^w)} + \frac{(R_w^w - E(R_w^w))^2}{\text{Var}(R_w^w)},
\end{equation*}
and the robust test statistic $S_R$ can be decomposed as 
$$S_R = (Z^R_\text{diff})^2 + (Z^R_{w})^2$$
and 
$$  \textbf{Cov}(Z^R_\text{diff},  Z^R_{w}) = 0.$$

\section{Proof of Theorem \ref{th:bounds}} \label{Appendix:bounds}

For $k = 1, 2$, 
$R_k^w = \sum_{(i, j)\in G} w_{ij}I_{J_{(i, j)} = k} > \min(w_{ij})\sum_{(i, j)\in G} I_{J_{(i, j)} = k}$. 
Then $1/|G|\precsim \min(w_{ij}) $ and $\sum_{(i, j)\in G} I_{J_{(i, j)} = k} = O(|G|)$.

So $\lim_{N\to\infty}\min(w_{ij})\sum_{(i, j)\in G} I_{J_{(i, j)} = k}>0$.

\section{Proof of Theorem \ref{th:conver}} \label{Appendix:limit_distribution}

We will use the bootstrap null distribution to prove Theorem \ref{th:conver}. Under the bootstrap null, the probability of a sample assigned to sample $\boldsymbol{X}$ is $\frac{n_X}{N}$, and the probability of a sample assigned to sample $\boldsymbol{Y}$ is $1-\frac{n_X}{N}$. When $n_x = n_1$, the bootstrap null distribution is equivalent to the permutation null. We use subscript to denote statistics under the bootstrap null.

First we introduce Theorem \ref{th:stein} to help prove Theorem \ref{th:conver}.

\begin{assumption}
\label{assum:a1}
[Chen and Shao, 2005, p. 17] For each $i\in J$, there exists $K_i \subset L_i \subset J$ such that $\xi_i$ is independent of $\xi_{K_i^C}$ and $\xi_{K_i}$ is independent of $\xi_{L_i^C}$.
\end{assumption}

\begin{theorem}
\label{th:stein}
[Chen and Shao, 2005, Theorem 3.4] Under Assumption \ref{assum:a1}, we have 
$$\sup\limits_{h\in Lip(1)}|\text{E}h(W) - \text{E}h(Z)| \leq \delta,$$
where $Lip(1) = \{h : R \rightarrow R\}$, $Z$ has $\mathcal{N}(0, 1)$ distribution and 
$$\delta = 2\sum\limits_{i \in J}(E|\xi_i\eta_i\theta_i| + |E(\xi_i\eta_i)|E|\theta_i|)+\sum\limits_{i \in J}|E|\xi_i\eta_i^2|, $$
with $\eta_i = \sum\limits_{j \in K_i}\xi_j$ and $\theta_i = \sum\limits_{j \in L_i} \xi_j$, where $K_i$ and $L_i$ are defined in Assumption \ref{assum:a1}.
\end{theorem}
Let $p_n = \frac{n_1}{N}$, $q_n = 1-\frac{n_1}{N} = \frac{n_2}{N},$

\begin{equation*}
\begin{split}
	\text{E}_B(R_1^w) &= \sum\limits_{(i, j) \in G}w_{ij}P(J_{(i, 1) = 1}) = \sum\limits_{(i, j) \in G}w_{ij}p_n^2:=\mu_1^B,\\
	\text{E}_B(R_2^w) &= \sum\limits_{(i, j) \in G}w_{ij}P(J_{(i, 1) = 2}) = \sum\limits_{(i, j) \in G}w_{ij}q_n^2:=\mu_2^B, \\
    \text{Var}_B(R_1^w) &= \sum\limits_{(i, j) \in G}w_{ij}^2p_n^2 + \sum\limits_{\substack{(i, j), (i, k) \in G\\ j\neq k}} w_{ij}w_{ik}p_n^3 + \\
    &\quad\sum\limits_{\substack{(i, j), (k, l) \in G\\ i, j, k, l \text{ all different}}} w_{ij}w_{kl}p_n^4 - (\sum\limits_{(i, j) \in G}w_{ij})^2p_n^4\\
    &=\sum\limits_{(i, j) \in G}w_{ij}^2(p_n^2 - p_n^4)+ \sum\limits_{\substack{(i, j), (i, k) \in G\\ j\neq k}} w_{ij}w_{ik}(p_n^3 - p_n^4)\\
    &=\sum\limits_{(i, j) \in G}w_{ij}^2(p_n^2 - p_n^4)+ \sum\limits_{(i, j), (i, k) \in G} w_{ij}w_{ik}(p_n^3 - p_n^4)-\\
    &\quad\sum\limits_{(i, j) \in G}w_{ij}^2(p_n^3 - p_n^4)\\
    &=\sum\limits_{(i, j) \in G}w_{ij}^2p_n^2q_n+\sum\limits_{(i, j), (i, k) \in G} w_{ij}w_{ik}p_n^3q_n\\&:=(\sigma_1^B)^2.
    \end{split}
\end{equation*}
Similarly, 
\begin{equation*}
\begin{split}
    \text{Var}_B(R_2^w) &=
    \sum\limits_{(i, j) \in G}w_{ij}^2q_n^2p_n+\sum\limits_{(i, j), (i, k) \in G} w_{ij}w_{ik}q_n^3p_n\\
    &:=(\sigma_2^B)^2, \\
     \text{Cov}_B(R_1^w, R_2^w) &= \text{E}_B(R_1^wR_2^w) - \text{E}_B(R_1^w)\text{E}_B(R_2^w)\\&=
    \sum\limits_{(i, j) \in G}w_{ij}\sum\limits_{\substack{ (k, l) \in G\\ i, j, k, l \text{ all different}}} w_{kl}p_n^2q_n^2-\sum\limits_{(i, j) \in G}w_{ij}p_n^2\sum\limits_{(i, j) \in G}w_{ij}q_n^2\\&=
    -\sum\limits_{(i, j), (i, k) \in G} w_{ij}w_{ik}p_n^2q_n^2\\&:=(\sigma_{12}^B)^2.\\
\end{split}
\end{equation*}

Let $R_\text{diff}^w = R_1^w - R_2^w$, we have
\begin{equation*}
\begin{split}
\text{E}_B(R_\text{diff}^w) & = \sum\limits_{(i, j) \in G}w_{ij}(p_n-q_n):=\mu_{diff}^B,\\
    \text{Var}_B(R_\text{diff}^w)  &=\text{Var}_B(R_1^w) +\text{Var}_B(R_2^w) -2\text{Cov}_B(R_1^w, R_2^w)\\&=p_nq_n \sum\limits_{(i, j) \in G}w_{ij}^2+\sum\limits_{(i, j), (i, k) \in G} w_{ij}w_{ik}(p_n^3q_n+q_n^3p_n+2p_n^2q_n^2)
    \\&= p_nq_n (\sum\limits_{(i, j) \in G}w_{ij}^2+\sum\limits_{(i, j), (i, k) \in G} w_{ij}w_{ik})\\
    &:=(\sigma_{diff}^B)^2.
\end{split}
\end{equation*}
Let $R_{w}^w = qR_1^w + pR_2^w$, we have

\begin{equation*}
	\begin{split}
	\text{E}_B(R_{w}^w)  &= \sum\limits_{(i, j) \in G}w_{ij}\frac{n_2^2(n_1-1)+n_1^2(n_2-1)}{N^2(N-2)}:=\mu_{w}^B, \\
    \text{Var}_B(R_{w}^w)  &=q^2\text{Var}_B(R_1^w) +p^2\text{Var}_B(R_2^w) +2pq\text{Cov}_B(R_1^w, R_2^w)\\
    \\&= \frac{n_1n_2(n_1-n_2)^2}{N^4(N-2)^2}\sum\limits_{(i, j), (i, k) \in G} w_{ij}w_{ik}+\\
    &\quad\frac{n_1n_2\{n_1n_2(N-4)+N\}}{N^3(N-2)^2}\sum\limits_{(i, j) \in G}w_{ij}^2\\&:=(\sigma_w^B)^2.
    \end{split}
\end{equation*}

Let, 
\begin{eqnarray*}
W_1^B &=& \frac{R_{w}^w - E_B(R_{w}^w)}{\sqrt{ \text{Var}_B(R_{w}^w)}},\\
W_2^B &=& \frac{R_\text{diff}^w - E_B(R_\text{diff}^w)}{\sqrt{ \text{Var}_B(R_\text{diff}^w)}},\\
W_3^B &=& \frac{n_X - n}{\sqrt{Np_n(1-p_n)}}.
\end{eqnarray*}

\begin{lemma}
\label{le:1}
Under conditions
\begin{enumerate}[(i)]
	\item $|G| = \mathcal{O}(N^{\alpha}), 1\leq\alpha<1.25$,
	\item $\begin{aligned}[t]
			&\sum_{(i, j) \in G}w_{ij}^2 + \sum_{(i, j), (i, k) \in G} w_{ij}w_{ik} - \frac{4}{N}\sum_{(i, j), (k, l) \in G} w_{ij}w_{kl} = \mathcal{O}(\sum_{(i, j) \in G}w_{ij}^2 +\\& \sum_{(i, j), (i, k) \in G} w_{ij}w_{ik}),
	\end{aligned}$
	
    \item $\sum\limits_{(i, j)\in G}(w_{ij}|A_{(i, j)}|)^2 = o(\sum\limits_{(i, j)\in G}w_{ij}^2N^{0.5})$,

    \item $\begin{aligned}[t]
			&\sum_{(i, j)\in G}w_{ij}\sum_{(i', j')\in A_{(i, j)}}w_{i'j'}\sum_{(i'', j'')\in B_{(i, j)}}w_{i''j''}=o(\sum_{(i, j)\in G}w_{ij}^2)^{1.5},
    	\end{aligned}$
\end{enumerate}
and under the bootstrap null,
$$(W_1^B, W_2^B, W_3^B)$$
is multivariate normal.
\end{lemma}

\begin{lemma} 
\label{le:2}
We have
\begin{itemize}
    \item $\frac{\text{Var}_B(R_{w}^w)}{\text{Var}(R_{w}^w)} \rightarrow c_1$,
    \item $\frac{\text{Var}_B(R_\text{diff}^w)}{\text{Var}(R_\text{diff}^w)} \rightarrow c_2$,
    \item $\frac{E_B(R_{w}^w) - E(R_{w}^w)}{\sqrt{\text{Var}(R_{w}^w)}}\rightarrow 0$,
    \item $\frac{E_B(R_\text{diff}^w) - E(R_\text{diff}^w)}{\sqrt{\text{Var}(R_{\text{diff}}^w)}}\rightarrow 0$,
    \item $\lim\limits_{N \to \infty}\text{Cov}(Z_w, Z_\text{diff}) = 0$,
\end{itemize}
where $c_1$ and $c_2$ are constant.
\end{lemma}


From Lemma \ref{le:1}, $(W_1^B,W_2^B|W_3^B)$ is multivariate normal under the bootstrap null. Since conditioning on $W_3^B = 0$, $(W_1^B, W_2^B|W_3^B = 0)$ and $(W_1^B, W_2^B)$ under the permutation distribution have the same distribution, and
\begin{eqnarray*}
Z^R_{w} &=& \frac{\sqrt{\text{Var}_B(R_{w}^w)}}{\sqrt{\text{Var}(R_{w}^w)}}(W_1^B + \frac{\text{E}_B(R_{w}^w) - \text{E}(R_{w}^w)}{\text{Var}_B(R_{w}^w)}),\\
Z^R_{\text{diff}} &=& \frac{\sqrt{\text{Var}_B(R_{\text{diff}}^w)}}{\sqrt{\text{Var}(R_{\text{diff}}^w)}}(W_2^B + \frac{\text{E}_B(R_{\text{diff}}^w) - \text{E}(R_{\text{diff}}^w)}{\text{Var}_B(R_{\text{diff}}^w)}),
\end{eqnarray*}
with Lemma \ref{le:2}, we conclude that $Z^R_{w}$ and $Z^R_{\text{diff}}$ are Gaussian under the permutation distribution.

\subsection{Proof of Lemma \ref{le:1}}


We first show $(W_1^B, W_2^B, W_3^B)$ is multivariate Gaussian under the bootstrap null distribution, which is equivalent to showing that $W=a_1W_1^B+a_2 W_2^B+a_3 W_3^B$ is asymptotically Gaussian distributed for each $(a_1, a_2, a_3) \in \mathbb{R}^3$ such that $\text{Var}_B(W)>0$ by Cramer-Wold theorem.

Let the index set $J = \{(i, j)\in G\} \bigcup \{1, 2, ..., N\}$,
\begin{equation*}
	\begin{split}
    \xi_{(i, j)} =& a_1\left(\frac{w_{ij}\frac{m-1}{N-2}I(J_{(i, j) = 1})+w_{ij}\frac{n-1}{N-2}I(J_{(i, j) = 2})}{\sigma_w^B} - \frac{ w_{ij}\frac{n^2(m-1)+m^2(n-1)}{N^2(N-2)}}{\sigma_w^B}\right)+\\
    &a_2\frac{w_{ij}I(J_{(i, j) = 1})-w_{ij}I(J_{(i, j) = 2}) -(w_{ij}(p_n-q_n))}{\sigma_{diff}^B}, \\
    \xi_i =& a_3\frac{I(g_i = 0) - p_n}{\sqrt{Np_n(1-p_n)}}.
    \end{split}
\end{equation*}

Let, $a = max(|a_1|, |a_2|, |a_3|)$, $\sigma = \text{min}(\sigma_w^B, \sigma_{diff}^B)$, $\sigma_0 = \sqrt{Np_n(1-p_n)}$. $\sigma^2$ is at least of order $\sum\limits_{(i, j)\in G}w_{ij}^2$, $\sigma_0 = O(N^{0.5})$.

Then $|\xi_{(i, j)}| \leq \frac{2a}{w_{ij}\sigma}$, $|\xi_i| \leq \frac{a}{\sigma_0}$ and $W = \sum_{j \in J}\xi_j$.



For $(i, j) \in J$, let
\begin{align*}
A_{(i, j)} =& \{(i, j)\} \cup \begin{aligned}[t]\{&(i', j')\in G: (i', j') \text{ and } (i, j) \text{ share a node}\},\end{aligned}\\
B_{(i, j)} =& A_{(i, j)} \cup\begin{aligned}[t]
\{&(i'', j'')\in G: \exists(i', j')\in A_{(i, j)}, \text{ s.t. } (i', j')\text{ and } (i'', j'') \text{ share a node}\},
\end{aligned}\\
K_{(i, j)} =& A_{(i, j)} \cup \{i, j\},\\
L_{(i, j)} =& B_{(i, j)} \cup \{\text{nodes in }A_{(i, j)}\}.
\end{align*}

For $i \in \{1, 2, ..., N\},$ let
\begin{align*}
G_i =& \{(i, j) \in G\},\\
G_{i, 2} =& \{(i, j) \in G\}\cup \begin{aligned}[t]\{&(i'', j'')\in G: \exists(i', j')\in G_i,  \text{ s.t. }(i', j')\text{ and } (i'', j'') \text{ share a node}\},\end{aligned} \\
K_i =& G_i \cup \{i\},\\
L_j =& G_{i, 2} \cup \{\text{nodes in }G_i\}.
\end{align*}

For $j \in J$, let $\eta_j = \sum_{k \in K_j} \xi_k$ and $\theta_j = \sum_{k \in L_j} \xi_k$ .

$$sup_{h \in Lip(1)}|E_Bh(W) - Eh(Z)|\leq \delta \text{ for } Z\sim N(0, 1),$$
where 
\begin{equation*}
	\begin{split}
	\delta \frac{1}{\sqrt{\text{Var}_B(W)}}\big(2\sum_{j\in J}(E_B|\xi_j\eta_j\theta_j|+E_B(\xi_j\eta_j)E_B|\theta_j|) + \sum_{j\in J}E_B|\xi_j\eta_j^2|\big),
\end{split}
\end{equation*}
according to Theorem \ref{th:stein}.

For $j \in \{1, 2, ..., N\}$,

\begin{equation*}
	\begin{split}
	\eta_j&=\sum_{k \in K_j} \xi_k = \xi_i + \sum_{(i', j')\in G_i}\xi_{(i', j')}\leq \frac{a}{\sigma_0}+\frac{2a}{\sigma}\sum_{(i', j')\in G_i}w_{i'j'},\\
	\theta_j&=\sum_{k \in L_j} \xi_k =\sum_{\text{nodes in }G_i} \xi_i + \sum_{(i', j')\in G_{i, 2}}\xi_{(i', j')}\leq 2\frac{a|G_i|}{\sigma_0}+\frac{2a}{\sigma}\sum_{(i', j')\in G_{i, 2}}w_{i'j'}.
	\end{split}
\end{equation*}

So,
\begin{equation*}
	\begin{split}
	 &\quad2\sum_{j \in \{1, 2, ..., N\}}(E_B|\xi_j\eta_j\theta_j|+E_B(\xi_j\eta_j)E_B|\theta_j|) + \sum_{j \in \{1, 2, ..., N\}}E_B|\xi_j\eta_j^2|\\&\leq5\frac{a^3}{\sigma_0}(\frac{1}{\sigma_0}+\frac{2}{\sigma}\sum_{(i', j')\in G_i}w_{i'j'})(2\frac{|G_i|}{\sigma_0}+\frac{2}{\sigma}\sum_{(i', j')\in G_{i, 2}}w_{i'j'}).
\end{split}
\end{equation*}

For $(i, j) \in G$,
\begin{equation*}
	\begin{split}
	\eta_{(i, j)}&=\sum_{k \in K_{(i, j)}} \xi_k \\
	&= \xi_i+\xi_j + \sum_{(i', j')\in A_{(i, j)}}\xi_{(i', j')}\\
	&\leq \frac{2a}{\sigma_0}+\frac{2a}{\sigma}\sum_{(i', j')\in A_{(i, j)}}w_{i'j'}, \\
	\theta_{(i, j)}&=\sum_{k \in L_{(i, j)}} \xi_k \\&=\sum_{\text{nodes in }A_{(i, j)}} \xi_i + \sum_{(i', j')\in B_{(i, j)}}\xi_{(i', j')}\\&\leq 2\frac{a|A_{(i, j)}|}{\sigma_0}+\frac{2a}{\sigma}\sum_{(i', j')\in B_{(i, j)}}w_{i'j'}.
	\end{split}
\end{equation*}

So,
\begin{equation*}
	\begin{split}
    &\quad 2\sum_{(i, j) \in G}\left(E_B|\xi_{(i, j)}\eta_{(i, j)}\theta_{(i, j)}|+E_B(\xi_{(i, j)}\eta_{(i, j)})E_B|\theta_{(i, j)}|\right) + \\
    &\quad\sum_{(i, j) \in G}E_B|\xi_{(i, j)}\eta_{(i, j)}^2|\\&
    \leq
    5\frac{2aw_{ij}}{\sigma }(\frac{2a}{\sigma_0}+\frac{2a}{\sigma}\sum_{(i', j')\in A_{(i, j)}}w_{i'j'})(2\frac{a|A_{(i, j)}|}{\sigma_0}+\frac{2a}{\sigma}\sum_{(i', j')\in B_{(i, j)}}w_{i'j'})\\&=
   40\frac{a^3w_{ij}}{\sigma }(\frac{1}{\sigma_0}+\frac{1}{\sigma}\sum_{(i', j')\in A_{(i, j)}}w_{i'j'})(\frac{|A_{(i, j)}|}{\sigma_0}+\frac{1}{\sigma}\sum_{(i', j')\in B_{(i, j)}}w_{i'j'})
\end{split}.
\end{equation*}

Then we have
\begin{equation*}
	\begin{split}
    \delta 
    \leq &\frac{1}{\sqrt{\text{Var}_B(W)}}[\sum\limits_{(i, j)\in G} 40\frac{a^3w_{ij}}{\sigma }(\frac{1}{\sigma_0}+\frac{1}{\sigma}\sum_{(i', j')\in A_{(i, j)}}w_{i'j'})\\
    &(\frac{|A_{(i, j)}|}{\sigma_0}+\frac{1}{\sigma}\sum_{(i', j')\in B_{(i, j)}}w_{i'j'})+\\
    &\sum\limits_{i=1}^N 5\frac{a^3}{\sigma_0}(\frac{1}{\sigma_0}+\frac{2}{\sigma}\sum_{(i', j')\in G_i}w_{i'j'})(2\frac{|G_i|}{\sigma_0}+\frac{2}{\sigma}\sum_{(i', j')\in G_{i, 2}}w_{i'j'})].
\end{split}
\end{equation*}

If we want $\delta \rightarrow 0$ as $N\rightarrow \infty$, we need the following conditions to hold:
\begin{enumerate}
    \item[(1)] $\begin{aligned}[t]
    &\sum\limits_{i=1}^N\sum\limits_{(i', j')\in G_i}w_{i'j'}\sum\limits_{(i'', j'')\in G_{i, 2}}w_{i''j''} = o(\sum\limits_{(i, j)\in G}w_{ij}^2N^{0.5}),
    \end{aligned}$
    \item[(2)] $\sum\limits_{i=1}^N\sum\limits_{(i', j')\in G_i}w_{i'j'}|G_i| = o((\sum\limits_{(i, j)\in G}w_{ij}^2)^{0.5}N)$,
    \item[(3)] $\sum\limits_{i=1}^N\sum\limits_{(i', j')\in G_{i,2}}w_{i'j'}= o((\sum\limits_{(i, j)\in G}w_{ij}^2)^{0.5}N)$,
    \item[(4)] $\sum\limits_{i=1}^N|G_i| = o(N^{1.5})$,
    \item[(5)] $\sum\limits_{(i, j)\in G}w_{ij}|A_{(i, j)}| = o((\sum\limits_{(i, j)\in G}w_{ij}^2)^{0.5}N)$,
    \item[(6)] $\sum\limits_{(i, j)\in G}w_{ij}\sum\limits_{(i', j')\in B_{(i, j)}}w_{i'j'}= o(\sum\limits_{(i, j)\in G}w_{ij}^2N^{0.5})$,
    \item[(7)] $\begin{aligned}[t]
    &\sum\limits_{(i, j)\in G}w_{ij}|A_{(i, j)}|\sum\limits_{(i', j')\in A_{(i, j)}}w_{i'j'}= o(\sum\limits_{(i, j)\in G}w_{ij}^2N^{0.5}),
    \end{aligned}$
    \item[(8)]  $\begin{aligned}[t]
			&\sum\limits_{(i, j)\in G}w_{ij}\sum\limits_{(i', j')\in A_{(i, j)}}w_{i'j'}\sum\limits_{(i'', j'')\in B_{(i, j)}}w_{i''j''}=o(\sum\limits_{(i, j)\in G}w_{ij}^2)^{1.5}.
    	\end{aligned}$
\end{enumerate}


We need conditions:
\begin{enumerate}[(i)]
    \item $|G| = \mathcal{O}(N^{\alpha}), 1\leq\alpha<1.25$,
    \item $\sum\limits_{(i, j)\in G}(w_{ij}|A_{(i, j)}|)^2 = o(\sum\limits_{(i, j)\in G}w_{ij}^2N^{0.5})$,
    \item $\sum\limits_{(i, j)\in G}w_{ij} = o((\sum\limits_{(i, j)\in G}w_{ij}^2)^{0.5}N)$,
    \item $\begin{aligned}[t]
			&\sum\limits_{(i, j)\in G}w_{ij}\sum\limits_{(i', j')\in A_{(i, j)}}w_{i'j'}\sum\limits_{(i'', j'')\in B_{(i, j)}}w_{i''j''}=o(\sum\limits_{(i, j)\in G}w_{ij}^2)^{1.5},
    	\end{aligned}$
    \item $\begin{aligned}[t]
    &\sum\limits_{(i, j) \in G}w_{ij}^2 + \sum\limits_{(i, j), (i, k) \in G} w_{ij}w_{ik} - \frac{4}{N}\sum\limits_{(i, j), (k, l) \in G} w_{ij}w_{kl} \\
    &= \mathcal{O}(\sum\limits_{(i, j) \in G}w_{ij}^2 + \sum\limits_{(i, j), (i, k) \in G} w_{ij}w_{ik}).
    \end{aligned}$
\end{enumerate} 

$\sum_{i=1}^N|G_i| = 2|G|$, so condition (4) holds according to (i).

Since 
\begin{equation*}
	\begin{split}
    \sum\limits_{(i', j')\in A_{(i, j)}}w_{i'j'}&\leq |A_{(i, j)}|\max\limits_{(i', j')\in A_{(i, j)}}w_{i'j'}\\
    &=:|A_{(i, j)}|w_{\text{max}} = |A_{(i, j)}|w_{ij}\frac{w_{\text{max}}}{w_{ij}},
\end{split}
\end{equation*}
$\sum_{(i', j')\in A_{(i, j)}}w_{i'j'} = \mathcal{O}(|A_{(i, j)}|w_{ij})$, and condition (7) holds according to (ii).

Besides, 
\begin{equation*}
	\begin{split}
&\quad\sum\limits_{(i, j)\in G}w_{ij}|A_{(i, j)}|\sum\limits_{(i', j')\in A_{(i, j)}}w_{i'j'}\\
&= \mathcal{O}(\sum\limits_{(i, j)\in G}w_{ij}^2|A_{(i, j)}|^2) \\&= \mathcal{O}(\sum\limits_{i=1}^N\sum\limits_{(i', j')\in G_i}w_{i'j'}^2|A_{(i', j')}|^2) \\&= \mathcal{O}(\sum\limits_{i=1}^N\sum\limits_{(i', j')\in G_i}w_{i'j'}^2|G_i|^2) \\&\leq \mathcal{O}(\sum\limits_{(i, j)\in G}w_{ij}^2)N^{2\alpha - 2}\\&=   o(\sum\limits_{(i, j)\in G}w_{ij}^2N^{0.5}).
\end{split}
\end{equation*}

So $2\alpha - 2 \leq 0.5$, $\alpha \leq 1.25$.


Let $\gamma_{G_i}$ denotes the vertex set of $G_i/\{i\}$,
\begin{equation*}
	\begin{split}
    &\quad\sum\limits_{i=1}^N\sum\limits_{(i', j')\in G_i}w_{i'j'}\sum\limits_{(i'', j'')\in G_{i, 2}}w_{i''j''}\\
     &\leq  \sum\limits_{i=1}^N\sum\limits_{(i', j')\in G_i}w_{i'j'}\sum\limits_{j \in \gamma_{G_i}}\sum\limits_{(i'', j'')\in G_j}w_{i''j''}\\&=
    \sum\limits_{i=1}^N\sum\limits_{j \in \gamma_{G_i}}\sum\limits_{(i', j')\in G_i}w_{i'j'}\sum\limits_{(i'', j'')\in G_j}w_{i''j''}\\&=2 \sum\limits_{(i, j)\in G}\sum\limits_{(i', j')\in G_i}w_{i'j'}\sum\limits_{(i'', j'')\in G_j}w_{i''j''}\\&\leq 2 \sum\limits_{(i, j)\in G}(\sum\limits_{(i', j')\in A_{(i, j)}}w_{i'j'})^2 \\&= \mathcal{O}(\sum\limits_{(i, j)\in G}w_{ij}|A_{(i, j)}|\sum\limits_{(i', j')\in A_{(i, j)}}w_{i'j'}).
\end{split}
\end{equation*}
So condition (7) implies condition (1).

By CauchySchwarz inequality and (ii)
\begin{equation*}
	\begin{split}
    \sum\limits_{(i, j)\in G}w_{ij}|A_{(i, j)}| &\leq \sqrt{\sum\limits_{(i, j)\in G}w_{ij}^2|A_{(i, j)}|^2|G| }\\&=o((\sum\limits_{(i, j)\in G}w_{ij}^2)^{0.5}N^{0.25})|G|^{0.5}.
\end{split}
\end{equation*}
So (i) ensures that condition (5) holds. 

$$ \sum\limits_{(i, j)\in G}\sum\limits_{(i', j')\in A_{(i, j)}}w_{i'j'}= \mathcal{O}(\sum\limits_{(i, j)\in G}w_{ij}|A_{(i, j)}| )$$
\begin{equation*}
	\begin{split}
    &\quad\sum\limits_{(i, j)\in G}\sum\limits_{(i', j')\in A_{(i, j)}}w_{i'j'}\\
    &=\sum\limits_{(i, j)\in G}(\sum\limits_{(i', j')\in G_i}w_{i'j'}+\sum\limits_{(i'', j'')\in G_j}w_{i''j''} - w_{ij})\\&=\sum\limits_{i = 1}^N\sum\limits_{j \in \gamma_{G_i}}\sum\limits_{(i', j')\in G_j}w_{i'j'}-\sum\limits_{(i, j)\in G}w_{ij}\\&=\sum\limits_{i=1}^N\sum\limits_{(i', j')\in G_i}w_{i'j'}|G_i|-\sum\limits_{(i, j)\in G}w_{ij}.
\end{split}
\end{equation*}
According to condition (5) and (iii), condition (2) holds.

$$G_{i, 2} \subset \bigcup\limits_{j\in\gamma_{G_i}}G_j,$$

\begin{equation*}
	\begin{split}
    &\quad\sum\limits_{i=1}^N\sum\limits_{(i', j')\in G_{i,2}}w_{i'j'}\\
    &\leq \sum\limits_{i=1}^N\sum\limits_{j\in\gamma_{G_i}}\sum\limits_{(i', j')\in G_j}w_{i'j'}\\& = \sum\limits_{(i, j)\in G}(\sum\limits_{(i', j')\in G_i}w_{i'j'}+\sum\limits_{(i'', j'')\in G_j}w_{i''j''})\\&\leq2\sum\limits_{(i, j)\in G}\sum\limits_{(i', j')\in A_{(i, j)}}w_{i'j'} = \mathcal{O}(\sum\limits_{(i, j)\in G}w_{ij}|A_{(i, j)}|).
\end{split}
\end{equation*}
So condition (5) implies condition (3).

\begin{equation*}
	\begin{split}
    &\quad\sum\limits_{(i, j)\in G}w_{ij}\sum\limits_{(i', j')\in B_{(i, j)}}w_{i'j'}\\
    &\leq \sum\limits_{(i, j)\in G}w_{ij}\sum\limits_{(i', j')\in A_{(i, j)}}\sum\limits_{(i'', j'')\in A_{(i', j')}}w_{i''j''}\\&=\sum\limits_{(i, j)\in G}w_{ij}(\sum\limits_{(i', j')\in A_{(i, j)}}w_{i'j'})^2 \\&= \mathcal{O}(\sum\limits_{(i, j)\in G}w_{ij}|A_{(i, j)}|\sum\limits_{(i', j')\in A_{(i, j)}}w_{i'j'}).
\end{split}
\end{equation*}
So condition (7) implies condition (6).

Finally, since $(\sum\limits_{(i, j)\in G}w_{ij})^2 \leq |G|\sum\limits_{(i, j)\in G}w_{ij}^2$ , 
\begin{equation*}
	\begin{split}
	\sum\limits_{(i, j)\in G}w_{ij} &=o( |G|^{0.5}(\sum\limits_{(i, j)\in G}w_{ij}^2)^{0.5})=  o((\sum\limits_{(i, j)\in G}w_{ij}^2)^{0.5}N),
	\end{split}
\end{equation*}
if condition (i) is satisfied. 

So we need conditions (i), (iii), (iv), (v).


\subsection{Proof of Lemma \ref{le:2}}


\begin{equation*}
	\begin{split}
	\text{Var}_B(R_{w}^w) &= \frac{n_1n_2(n_1-n_2)^2}{N^4(N-2)^2}\sum\limits_{(i, j), (i, k) \in G} w_{ij}w_{ik}+\\
	&\quad\frac{n_1n_2\{n_1n_2(N-4)+N\}}{N^3(N-2)^2}\sum\limits_{(i, j) \in G}w_{ij}^2\\
	& = \mathcal{O}(\sum\limits_{(i, j) \in G}w_{ij}^2 ),
	\end{split}
\end{equation*} 
since \begin{equation*}
	\begin{split}
	\sum\limits_{(i, j), (i, k) \in G} w_{ij}w_{ik}&\leq(\sum\limits_{(i, j) \in G}w_{ij})^2 = o(\sum\limits_{(i, j) \in G}w_{ij}^2N^2).\end{split}
\end{equation*}.


\begin{equation*}
	\begin{split}
	\text{Var}(R_{w}^w) &=  \frac{n_1n_2(n_1-1)(n_2-1)}{N(N-1)(N-2)(N-3)}\big\{\sum\limits_{(i, j) \in G}w_{ij}^2 - \\
	&\quad\frac{1}{N-2}(\sum\limits_{(i, j) \in G}w_{ij}^2 +\sum\limits_{(i, j), (i, k) \in G} w_{ij}w_{ik} - \frac{4}{N}\sum\limits_{(i, j), (k, l) \in G} w_{ij}w_{kl}) -\\
	 &\quad\frac{2}{N(N-1)}\sum\limits_{(i, j), (k, l) \in G} w_{ij}w_{kl}\big\} \\
	&=  \mathcal{O}(\sum\limits_{(i, j) \in G}w_{ij}^2 ).
	\end{split}
\end{equation*}

So, $\lim_{N \to \infty}\frac{\text{Var}_B(R_{w}^w)}{\text{Var}(R_{w}^w)}= c_1$, where $c_1$ is a constant.

\begin{equation*}
	\begin{split}
	\lim\limits_{N \to \infty}\frac{\text{Var}_B(R_\text{diff}^w)}{\text{Var}(R_\text{diff}^w)} = &\lim\limits_{N \to \infty}\left(\sum\limits_{(i, j) \in G}w_{ij}^2 + \sum\limits_{(i, j), (i, k) \in G} w_{ij}w_{ik} \right)/\\
	&\quad\left(\sum\limits_{(i, j) \in G}w_{ij}^2 + \sum\limits_{(i, j), (i, k) \in G} w_{ij}w_{ik} - \frac{4}{N}\sum\limits_{(i, j), (k, l) \in G} w_{ij}w_{kl}\right)  \\
	&= c_2,
	\end{split}
\end{equation*}
where $c_2$ is a constant, according to condition (v).

Since $E_B(R_{w}^w) - E(R_{w}^w) = \frac{n_1n_2}{N^2(N-1)}\sum_{(i, j)\in G}w_{ij}$,
$$\lim\limits_{N \to \infty}\frac{E_B(R_{w}^w) - E(R_{w}^w)}{\sqrt{\text{Var}(R_{w}^w)}} =\lim\limits_{N \to \infty}\frac{1}{N}\frac{\sum\limits_{(i, j)\in G}w_{ij}}{c_3\sqrt{\sum\limits_{(i, j) \in G}w_{ij}^2}},$$
where $c_3$ is a constant. From condition (iii) $\sum_{(i, j)\in G}w_{ij} = o((\sum_{(i, j)\in G}w_{ij}^2)^{0.5}N)$, 
$$\lim_{N \to \infty}\frac{E_B(R_{w}^w) - E(R_{w}^w)}{\sqrt{\text{Var}(R_{w}^w)}} =0.$$

Since $E_B(R_\text{diff}^w) - E(R_\text{diff}^w) = 0$, 
$$\lim_{N \to \infty}\frac{E_B(R_\text{diff}^w) - E(R_\text{diff}^w)}{\sqrt{\text{Var}(R_\text{diff}^w)}} =0.$$

We still need to show $\lim_{N \to \infty}\text{Cov}(Z_w, Z_\text{diff}) = 0$.

\begin{equation*}
	\begin{split}
	\text{Cov}(Z_w, Z_\text{diff}) &= \frac{E(R_{w}^wR_\text{diff}^w) -E(R_{w}^w)E(R_\text{diff}^w) }{\sqrt{\text{Var}(R_{w}^w)\text{Var}(R_\text{diff}^w)}}, \\
    E(R_{w}^wR_\text{diff}^w) &= S_3[q\frac{n_1^2(n_1-1)^2}{N^2(N-1)^2}- p\frac{n_2^2(n_2-1)^2}{N^2(N-1)^2}+(p-q)\frac{n_1n_2(n_1-1)(n_2-1)}{N^2(N-1)^2}]\\&=\frac{(n_1-1)(n_2-1)(n_1-n_2)}{N(N-1)(N-2)}S_3,\\
    E(R_{w}^w)E(R_\text{diff}^w) &= S_3[(\frac{n_1-n_2}{N})(\frac{n_1n_2-N+1}{(N-1)(N-2)})],
\end{split}
\end{equation*}
where $S_3 = \sum_{(i, j), (k, l) \in G} w_{ij}w_{kl}$.

\begin{equation*}
	\begin{split}
	\lim_{N \to \infty}E(R_{w}^wR_\text{diff}^w) &= \sum_{(i, j), (k, l) \in G} w_{ij}w_{kl}p_nq_n(p_n-q_n),\\
	\lim_{N \to \infty}E(R_{w}^w)E(R_\text{diff}^w) &= \sum_{(i, j), (k, l) \in G} w_{ij}w_{kl}p_nq_n(p_n-q_n).
	\end{split}
\end{equation*}



So $\lim_{N \to \infty}(E(R_{w}^wR_\text{diff}^w) -E(R_{w}^w)E(R_\text{diff}^w)) = 0$.

\end{document}
