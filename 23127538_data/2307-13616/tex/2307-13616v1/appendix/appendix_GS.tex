
\section{The Gram-Schmidt process}
\label{appendix:gram schmidt}
\paragraph{} The Gram-Schmidt process \cite{horn13} is a method for orthonormalizing a set of vectors in an inner product space. The process takes a finite and linearly independent set of vectors $(u_1,\dots,u_n)$ and produces a set of orthogonal vectors $(v_1,\dots,v_n)$0. The process works as follows:
\begin{equation*}
    \begin{split}
        v_1 & =u_1 \\
        v_2 & =u_2-\frac{\langle u_2,v_1 \rangle}{\langle v_1,v_1 \rangle}v_1  \\
        \dots & \\
        v_n & =u_n-\sum_{k=1}^{n-1} \frac{\langle u_n,v_k \rangle}{\langle v_k,v_k \rangle}v_k
    \end{split}
\end{equation*} \par
A first idea was therefore to use this process. The issue with this method is that we obtain a transformation of all variables, both sensitive and non-sensitive, except for the first one as $v_1=u_1$, and they are all orthogonal to each other, which is not what we are looking for.
% with
% \begin{equation*}
%     \begin{split}
%         z_1 & =\frac{u_1}{||u_1||} \\
%         z_2 & =\frac{u_2}{||u_2||} \\
%         \dots & \\
%         z_n & =\frac{u_n}{||u_n||}
%     \end{split}
% \end{equation*}

% \begin{definition} A set of vectors $x_1,\dots,x_n$ is linearly dependent if there exists $a_1,\dots,a_n\in\mathbb{R}\backslash\{0\}$ such that $\sum_{i=1}^n a_ix_i=\bm{0}$0. \end{definition}

% As the inner product is defined on the space of random variables with zero mean and finite variance, we first need to center our variables. A priori, the set $(X^{(1)}-\overline{X^{(1)}},\dots,X^{(n-2)}-\overline{X^{(n-2)}},A-\overline{A})$ is linearly independent, so if we take 
% \begin{equation*}
%     \begin{split}
%         v_1 & =A-\overline{A} \\
%         v_2 & =X^{(1)}-\overline{X^{(1)}} \\
%         \dots & \\
%         v_{n-1} & =X^{(n-2)}-\overline{X^{(n-2)}}
%     \end{split}
% \end{equation*}
% then we can produce an orthogonal base of the space $(u_1,\dots,u_{n-1})$0. As $u_1=v_1=A-\overline{A}$, if we project each $X^{(i)}-\overline{X^{(i)}}$ on the hyperplane defined by the base $(u_2,\dots,u_{n-1})$, we obtain the projected vectors
% \begin{equation*}
%     \tilde{X}^{(i)}=\sum_{k=2}^{n-1} \langle X^{(i)}-\overline{X^{(i)}},u_k \rangle u_k
% \end{equation*}
% then every $\tilde{X}^{(i)}$ is orthogonal to $u_1$:
% \begin{equation*}
%     \begin{split}
%         \langle\tilde{X}^{(i)},u_1\rangle= & \langle \sum_{k=2}^{n-1} \langle X^{(i)}-\overline{X^{(i)}},u_k \rangle u_k,u_1\rangle\\
%          & =\sum_{k=2}^{n-1} \langle X^{(i)}-\overline{X^{(i)}},u_k \rangle\langle  u_k,u_1\rangle \\
%          & =0
%     \end{split}
% \end{equation*}
% On the other hand,
% \begin{equation*}
%     \begin{split}
%         \langle\tilde{X}^{(i)},u_1\rangle & =\langle\tilde{X}^{(i)},A-\overline{A}\rangle \\
%          & =\langle\tilde{X}^{(i)},A\rangle-\langle\tilde{X}^{(i)},\overline{A}\rangle \\
%          & =\langle\tilde{X}^{(i)},A\rangle
%     \end{split}
% \end{equation*}
% because
% \begin{equation*}
%     \langle\tilde{X}^{(i)},\overline{A}\rangle=cov(\tilde{X}^{(i)},\overline{A})=\mathbb{E}[\tilde{X}^{(i)}\overline{A}]-\mathbb{E}[\tilde{X}^{(i)}]\mathbb{E}[\overline{A}]=\mathbb{E}[\tilde{X}^{(i)}]\overline{A}-\mathbb{E}[\tilde{X}^{(i)}]\overline{A}=0
% \end{equation*}
% To conclude, $\langle\tilde{X}^{(i)},A\rangle=0$ so $\tilde{X}^{(i)}$ and $A$ are uncorrelated. We lose information with this projection, but we obtain explanatory variables uncorrelated with $A$0.

% \paragraph{Gram-Schmidt in Python} is implemented in the numpy library through the QR factorization. It is the decomposition of a matrix $A$ of full rank (ie with linearly independent columns) into a product $A=QR$ of an orthogonal matrix Q and an upper triangular matrix R. To link with what we wrote above about Gram-Schmidt, the $(v_1,\dots,v_n)$ from before are the columns of $A=\begin{bmatrix}
%     a_1 & \dots & a_n
% \end{bmatrix}$, the inner product is $\langle x,y\rangle=x^Ty$ and we compute
% \begin{equation*}
%     u_1=a_1 \mbox{ and } u_i=a_i-\sum_{k=1}^{i-1} \frac{\langle a_i,u_k \rangle}{\langle u_k,u_k \rangle}u_k, i=2,\dots,n
% \end{equation*}
% Then $A=QR$ where
% \begin{equation*}
%     \begin{split}
%         a_i & =\sum_{k=1}^{i}\langle \frac{u_k}{||u_k||},a_k\rangle \frac{u_k}{||u_k||} \\
%         Q & =\begin{bmatrix}
%             \frac{u_1}{||u_1||} & \dots & \frac{u_n}{||u_n||}
%         \end{bmatrix} \\
%         R & =\begin{bmatrix}
%             \langle \frac{u_1}{||u_1||},a_1\rangle & \langle \frac{u_1}{||u_1||},a_2\rangle & \dots & \langle \frac{u_1}{||u_1||},a_n\rangle \\
%             0 & \langle \frac{u_2}{||u_2||},a_2\rangle & \dots & \langle \frac{u_2}{||u_2||},a_n\rangle \\
%             0 & 0 & \ddots & \\
%             \vdots & & & \\
%             0 & 0 & \dots & \langle \frac{u_n}{||u_n||},a_n\rangle
%         \end{bmatrix}
%     \end{split}
% \end{equation*}
% The $u_i$ are normalized to produce an orthonormal set instead of an orthogonal one. The inner product used in this algorithm $\langle x,y\rangle=x^Ty$ corresponds to the computation of the sample covariance for random vectors with zero mean and finite variance:
% \begin{equation*}
%     \langle X,Y \rangle = cov(X,Y)=\frac{1}{N}\sum_{i=1}^N X_iY_i=\frac{1}{N}X^TY
% \end{equation*}
% It does not matter that the $\frac{1}{N}$ is not in the inner product computed by the algorithm, because as the base is orthonormal, it cancels out in the quotient: $\frac{\langle a_i,u_k \rangle}{\langle u_k,u_k \rangle}$ and the projection of the vectors on the new base will be proportional (with a factor $N$).

% \paragraph{Final process} \label{process orthog} \begin{enumerate}
%     \item Center the vectors $A,X^{(1)},\dots,X^{(n-2)}$
%     \item Compute the QR decomposition of the matrix $\begin{bmatrix} A-\overline{A} & X^{(1)}-\overline{X^{(1)}} & \dots & X^{(n-2)}-\overline{X^{(n-2)}} \end{bmatrix}$
%     The columns of the matrix $Q=\begin{bmatrix} \frac{u_1}{||u_1||} & \dots & \frac{u_n}{||u_n||}\end{bmatrix}$ are the orthonormal base of the space. As $u_1=A-\overline{A}$, then the first column of $Q$ is colinear to $A$0. So if we want the base of a hyperplane orthogonal to $A$, we take all columns of $Q$ except for the first one.
%     \item Project the centered vectors $X^{(1)}-\overline{X^{(1)}},\dots,X^{(n-2)}-\overline{X^{(n-2)}}$ on the base $\begin{bmatrix} \frac{u_2}{||u_2||} & \dots & \frac{u_n}{||u_n||}\end{bmatrix}$
%     The projected vectors are orthogonal to $A$0.
% \end{enumerate}

\newpage
