\section{Discrimination mitigation applied to the simulated data}
\paragraph{} The goal of the section is to compare different pre-processing steps and see how they influence our fairness metrics. After this, we will apply a logistic regression model to the simulated explanatory variables predict the variable of interest. Appendix~\ref{appendix:logistic regression} gives a reminder on the logistic regression. \par
The reason why we chose the logistic regression is that it is interpretable, which is a major issue with Machine Learning, and an important characteristic to simplify our study on fairness. It also presents other advantages, such as its simplicity with a low number of parameters. The main drawback is that a lot of preprocessing must be done: we need to select variables that are not strongly correlated with each other, and to transform most continuous variables into categorical variables, or only their general effect will be captured by the coefficients. For this simulated dataset, we will keep all variables as there are only 5 of then, and they are very simple. \par

% Figure environment removed

\subsection{Regression model with no pre-processing step}
\label{baseline}
\paragraph{} In this section, we will simply predict the outcome with all the variables. We standardized the variables to have mean zero and standard deviation 1, then randomly separated the dataset into a train (80\%) and a test dataset (20\%), and applied a logistic regression model. \par
Table~\ref{tab:weights_reg all variables} gives confidence intervals of the weights of this logistic regression, their standard errors and the associated p-values. All variables have p-values below 0.05, so they are all significant to the model. \par

\begin{table}[H]
    \centering
    \begin{tabular}{c|c|c|c}
        Variable & Coefficient & Standard Error & P-value \\ \hline
        Intercept & $-1.82\pm0.01$ & $0.05\pm0.00$ & $0.00\pm0.00$ \\
        $X_1$ & $0.55\pm0.00$ & $0.02\pm0.00$ & $0.00\pm0.00$ \\
        $X_2$ & $2.81\pm0.01$ & $0.05\pm0.00$ & $0.00\pm0.00$ \\
        $X_3$ & $0.25\pm0.00$ & $0.01\pm0.00$ & $0.00\pm0.00$ \\
        $X_4$ & $-2.53\pm0.01$ & $0.03\pm0.00$ & $0.00\pm0.00$ \\
        A & $-0.17\pm0.01$ & $0.03\pm0.00$ & $0.00\pm0.00$ \\
        B & $0.41\pm0.01$ & $0.04\pm0.00$ & $0.00\pm0.00$ \\
    \end{tabular}
    \caption{Coefficients of the model using all variables}
    \label{tab:weights_reg all variables}
\end{table}

% \begin{table}[H]
%     \begin{center}
%         \begin{tabular}{ccccc}
%             \cline{2-3}
%             \multicolumn{1}{c|}{}  & \multicolumn{1}{c|}{$\hat{Y}$=0} & \multicolumn{1}{c|}{$\hat{Y}$=1} &  &  \\ \cline{1-3}
%             \multicolumn{1}{|c|}{$Y=0$} & \multicolumn{1}{c|}{$15,519.14\pm11.81$}  & \multicolumn{1}{c|}{$483.02\pm5.65$}  &  &  \\ \cline{1-3}
%             \multicolumn{1}{|c|}{$Y=1$} & \multicolumn{1}{c|}{$3,305.98\pm10.39$}  & \multicolumn{1}{c|}{$691.86\pm6.07$}  &  &  \\ \cline{1-3}
%         \end{tabular}
%     \end{center}
%     \caption{Confusion matrix (all variables)}
%     \label{tab:confusion matrix all variables}
% \end{table}

% Figure~\ref{fig:permut all variables} gives the permutation feature importance score by variable. The permutation importance is the decrease in model accuracy when a single feature value is randomly permuted with another one. This breaks the relationship between the feature and the target, so we can see how much the model depends on the feature. We must be careful when looking at permutation feature importance, because if two features that are strongly correlated are permuted, the model still has access to the same information, and this results in lower importance for both features even if they might be important. Here, we can see that $X^{(2)}$ and $X^{(4)}$ appear to have a very low importance, as we thought in the previous paragraph. $X^{(1)}$, $X^{(3)}$ and $A$ seem to be very important, more or less at the same degree. If we remember the correlation matrix from figure~\ref{fig:heatmap corr},
% \begin{itemize}
%     \item $corr(X^{(2)},X^{(4)})=-0.2$ which is noticeable, so the importance of both features might be higher than the figure shows
%     \item $corr(X^{(1)},X^{(3)})=-0.2$ which is noticeable, so the importance of both features might be even higher than the figure shows
%     \item $corr(X^{(1)},A)=-0.076$ which is negligible
%     \item $corr(X^{(3)},A)=0.076$ which is negligible
% \end{itemize}

% % Figure environment removed

\paragraph{Performance evaluation}
\begin{itemize}
    \item We have 81.05\% correct classifications on average. The accuracy acceptability depends on the business context. As we saw previously, if mistakes have a high cost then it might not be sufficient. \\
    If there is a large class imbalance, accuracy is not the best metric as it can be very high while the model only fits the majority population. As a reminder, the output imbalance ratio is $IR_Y=4.00$0. So we cannot rely solely on accuracy to evaluate our model.
    \item As a reminder, the ROC (Receiver Operating Characteristic) curve plots the true positive rate against the false positive rate for varying classification thresholds. A random classifier will exhibit a linear ROC curve as the one plotted in a dashed orange line (figure~\ref{fig:ROC_sim_allvar}). Above that line, the model performs better than the random classifier, and below, worse. The perfect classifier has a ROC curve that is confined to the (0,1) point. Here, our model performs a lot better than the random classifier: the AUC (Area Under the ROC Curve) is of 0.7568.
\end{itemize}
% The differences in group-wise metrics is explained by the importance of variable $A$ in the prediction. Individuals with $A=1$ are more likely to be predicted the outcome $Y=1$ than the others, correctly or incorrectly. This also explains why we have higher true and false positive rates for group $Y=1$, and why the acceptance rate is higher.

\begin{table}[H]
    \centering
    \begin{tabular}{c|c}
         (\%) & Global \\ \hline
        Accuracy & $81.05\pm0.05$
    \end{tabular}
    \caption{Global metrics (all variables)}
    \label{tab:metrics all variables}
\end{table}

% Figure environment removed

\paragraph{Fairness evaluation} We will compare fairness under all three definitions, first comparing the metrics between values of protected groups A and B (table~\ref{tab:fairness metrics all variables}), then between subgroups of combinations of A and B (table~\ref{tab:fairness metrics subgroups all variables}).
\begin{itemize}
    \item Statistical parity requires the same acceptance rates for all protected groups.
    \begin{itemize}
        \item It is a lot higher for group $A=0$ than for group $A=1$, by 8.26 points. So group $A=1$ is disadvantaged by the model under this definition.
        \item It is higher for group $B=0$ than for group $B=1$, by 3.47 points. So group $B=1$ is disadvantaged by the model under this definition.
        \item Looking at subgroups, the most advantaged subgroup is for $(A=0,B=0)$ and the most disadvantaged is for $(A=1,B=1)$0. This shows that looking at combinations of protected variables reveals that groups with a certain combination of characteristics are even more disadvantaged.
    \end{itemize} 
    \item Equal opportunity requires the same true positive rates for all protected groups.
    \begin{itemize}
        \item It is higher for group $A=0$, so group $A=1$ is disadvantaged by the model under this definition.
        \item It is higher for group $B=0$, although not by much, so group $B=1$ is disadvantaged by the model under this definition.
        \item It is highest for subgroup $(A=0,B=0)$ and lowest for group $(A=1,B=1)$0.
    \end{itemize} 
    \item Equalized odds requires the same true and false positive rates for both protected groups.
    \begin{itemize}
        \item The false positive rate is higher for group $A=0$ than group $A=1$0. Group $A=1$ has lower true and false positive rates, so it is disadvantaged by the model under this definition.
        \item The false positive rate is higher for group $B=0$ than $B=1$0. Group $B=1$ has lower true and false positive rates, so it is disadvantaged by the model under this definition.
        \item The false positive rate is highest for subgroup $(A=0,B=0)$ and lowest for $(A=1,B=1)$0.
    \end{itemize}
\end{itemize}
 To conclude, the model is unfair and disadvantages groups $A=1$ and $B=1$ under all three fairness definitions, and the most disadvantaged subgroup is $(A=1,B=1)$0.

\begin{table}[H]
    \centering
    \makebox[\linewidth]{
    \begin{tabularx}{19.5cm}{c|c|ccc|ccc}
    % \begin{tabular}{c|c|ccc|ccc}
        (\%) & Global & A=0 & A=1 & Difference & B=0 & B=1 & Difference \\ \hline
        AR & $94.13\pm0.05$ & $96.60\pm0.04$ & $88.34\pm0.10$ & $8.26\pm0.10$ & $97.25\pm0.09$ & $93.78\pm0.05$ & $3.47\pm0.10$ \\
        TPR & $96.98\pm0.04$ & $98.26\pm0.03$ & $93.60\pm0.09$ & $4.67\pm0.09$ & $98.62\pm0.06$ & $96.79\pm0.04$ & $1.83\pm0.07$ \\
        FPR & $82.69\pm0.15$ & $88.55\pm0.15$ & $73.95\pm0.24$ & $14.60\pm0.26$ & $89.72\pm0.39$ & $82.11\pm0.16$ & $7.62\pm0.42$
    % \end{tabular}
    \end{tabularx}
    }
    \caption{Fairness metrics globally and by protected group (all variables)}
    \label{tab:fairness metrics all variables}
\end{table}

\begin{table}[H]
    \centering
    \begin{tabular}{c|cc|cc}
        \multirow{2}{*}{(\%)} & \multicolumn{2}{c|}{A=0} & \multicolumn{2}{c}{A=1} \\ \cline{2-5}
         & \multicolumn{1}{c|}{B=0} & B=1 & \multicolumn{1}{c|}{B=0} & B=1 \\ \hline
        AR & \multicolumn{1}{c|}{$98.47\pm0.07$} & $96.39\pm0.04$ & \multicolumn{1}{c|}{$93.72\pm0.22$} & $87.92\pm0.12$ \\
        TPR & \multicolumn{1}{c|}{$99.21\pm0.05$} & $98.15\pm0.03$ & \multicolumn{1}{c|}{$96.58\pm0.19$} & $93.41\pm0.09$ \\
        FPR & \multicolumn{1}{c|}{$93.65\pm0.36$} & $88.08\pm0.16$ & \multicolumn{1}{c|}{$83.15\pm0.66$} & $73.34\pm0.26$
    \end{tabular}
    \caption{Fairness metrics by protected subgroups (all variables)}
    \label{tab:fairness metrics subgroups all variables}
\end{table}

% Figure environment removed

% \subsection{Influence of model calibration}

% \subsubsection{Train test split}
% \paragraph{} As our datasets have a limited number of observations, the characteristics of the train and test sets might vary from one execution to the next one. In order to have stable and trustworthy results, we need to have the same proportions of observed classes in the original, train and test sets. \par
% Splitting the dataset with a random selection of observations after shuffling it is a common way of proceeding and it is the default setting of the split function in the \texttt{scikit-learn} library, which we used for our baseline. As we saw previously, there are a lot more observations for $Y=0$ than for $Y=1$, 4 times more to be precise, 2.33 times more observations for $A=0$ than $A=1$, and almost 9 times more observations for $B=0$ than $B=1$0.  Within each protected group, there is an imbalance in the observed values for $Y$0. \par
% By randomly separating the dataset into train and test sets, we risk having few observations of a particular combination of $(A,B,Y)$ in the train set, for example of $(A=1,B=1,Y=1)$ and in that case, outcomes of observations with these characteristics might not be predicted accurately. \par
% To illustrate, table~\ref{tab:prop A B Y split} gives the proportions of each combination (A,B,Y) in the original, train and test sets of the baseline. As the proportions are very close in the three sets, randomly splitting the dataset is enough in our case. \par

% \begin{longtable}[H]{cccc}
%     \multicolumn{4}{c}{Original set} \\ \cline{3-4} 
%     \endfirsthead
%     %
%     \endhead
%     %
%     \multicolumn{2}{c|}{(\%)} & \multicolumn{1}{c|}{Y=0} & \multicolumn{1}{c|}{Y=1} \\ \hline
%     \multicolumn{1}{|c|}{\multirow{2}{*}{A=0}} & \multicolumn{1}{c|}{B=0} & \multicolumn{1}{c|}{$6.59\pm0.02$} & \multicolumn{1}{c|}{$1.01\pm0.01$} \\ \cline{2-4} 
%     \multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{B=1} & \multicolumn{1}{c|}{$51.46\pm0.03$} & \multicolumn{1}{c|}{$10.94\pm0.01$} \\ \hline
%     \multicolumn{1}{|c|}{\multirow{2}{*}{A=1}} & \multicolumn{1}{c|}{B=0} & \multicolumn{1}{c|}{$1.89\pm0.00$} & \multicolumn{1}{c|}{$0.52\pm0.00$} \\ \cline{2-4} 
%     \multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{B=1} & \multicolumn{1}{c|}{$20.07\pm0.02$} & \multicolumn{1}{c|}{$7.51\pm0.01$} \\ \hline
%      &  &  &  \\
%     \multicolumn{4}{c}{Train set} \\ \cline{3-4} 
%     \multicolumn{2}{c|}{(\%)} & \multicolumn{1}{c|}{Y=0} & \multicolumn{1}{c|}{Y=1} \\ \hline
%     \multicolumn{1}{|c|}{\multirow{2}{*}{A=0}} & \multicolumn{1}{c|}{B=0} & \multicolumn{1}{c|}{$6.60\pm0.01$} & \multicolumn{1}{c|}{$1.01\pm0.01$} \\ \cline{2-4} 
%     \multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{B=1} & \multicolumn{1}{c|}{$51.45\pm0.04$} & \multicolumn{1}{c|}{$10.94\pm0.021$} \\ \hline
%     \multicolumn{1}{|c|}{\multirow{2}{*}{A=1}} & \multicolumn{1}{c|}{B=0} & \multicolumn{1}{c|}{$1.89\pm0.01$} & \multicolumn{1}{c|}{$0.52\pm0.00$} \\ \cline{2-4} 
%     \multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{B=1} & \multicolumn{1}{c|}{$20.06\pm0.02$} & \multicolumn{1}{c|}{$7.51\pm0.02$} \\ \hline
%      &  &  &  \\
%     \multicolumn{4}{c}{Test set} \\ \cline{3-4} 
%     \multicolumn{2}{c|}{(\%)} & \multicolumn{1}{c|}{Y=0} & \multicolumn{1}{c|}{Y=1} \\ \hline
%     \multicolumn{1}{|c|}{\multirow{2}{*}{A=0}} & \multicolumn{1}{c|}{B=0} & \multicolumn{1}{c|}{$6.56\pm0.03$} & \multicolumn{1}{c|}{$1.00\pm0.01$} \\ \cline{2-4} 
%     \multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{B=1} & \multicolumn{1}{c|}{$51.49\pm0.07$} & \multicolumn{1}{c|}{$10.93\pm0.04$} \\ \hline
%     \multicolumn{1}{|c|}{\multirow{2}{*}{A=1}} & \multicolumn{1}{c|}{B=0} & \multicolumn{1}{c|}{$1.90\pm0.02$} & \multicolumn{1}{c|}{$0.52\pm0.01$} \\ \cline{2-4} 
%     \multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{B=1} & \multicolumn{1}{c|}{$20.08\pm0.01$} & \multicolumn{1}{c|}{$7.51\pm/04$} \\ \hline
%     \caption{Proportions of observations of (A,B,Y), splitting with random shuffle}
%     \label{tab:prop A B Y split}\\
% \end{longtable}

% \noindent \begin{center}
%     \fbox{
% \begin{minipage}{0.9\textwidth}
%   The way the dataset is split into train and test sets may impact model performances if the proportions of observations in different classes are not the same.
% \end{minipage}
% }
% \end{center}

% As the proportions are kept in the train and test splits, we do not need to have a more elaborate way of splitting our dataset. If this had not been the case, we could have looked at solutions such as a stratified split, which allows the conservation of proportions of observations in each specified class.

% \subsubsection{Re-sampling}
% \paragraph{} As we saw previously, our data is imbalanced. We talked about taking it into account in the split, to have the same proportions as in the initial dataset, but there is definitely more we can do. Solutions consist in either under-sampling the majority class or over-sampling the minority class, or even a combination of both. Sampling can be done randomly or with specific strategies that tackle different problems. We will look at a few of them and apply them to the train test. We have written down the number of observations by combination of (A,B,Y) in the train set in table~\ref{tab:A B Y counts sampling}.

% \begin{table}[H]
%     \centering
%     \begin{tabular}{cc|c|c|c|}
%         \cline{3-5}
%         \multicolumn{2}{c|}{} & Y=0 & Y=1 & Total \\ \hline
%         \multicolumn{1}{|c|}{\multirow{2}{*}{A=0}} & B=0 & $5,277.33\pm14.92$ & $811.09\pm5.48$ & $6,088.42\pm11.02$ \\ \cline{2-5} 
%         \multicolumn{1}{|c|}{} & B=1 & $41,160.61\pm29.47$ & $8,755.97\pm17.85$ & $49,916.58\pm25.20$ \\ \hline
%         \multicolumn{1}{|c|}{\multirow{2}{*}{A=1}} & B=0 & $1,515.77\pm7.51$ & $418.79\pm3.75$ & $1,934.56\pm5.92$ \\ \cline{2-5} 
%         \multicolumn{1}{|c|}{} & B=1 & $16,050.90\pm21.22$ & $6,009.54\pm13.03$ & $22,060.44\pm18.09$ \\ \hline
%         \multicolumn{2}{|c|}{Total} & $63,954.61\pm13.38$ & $15,995.39\pm5.53$ & $80,000.00\pm0.00$ \\ \hline
%     \end{tabular}
%     \caption{Number of observations by values of $A$, $B$ and $Y$ in the train set}
%     \label{tab:A B Y counts sampling}
% \end{table}

% \paragraph{Random under-sampling} of the majority class could allow better performances, but we risk modifying the data structure. In our case, the combination of values of $A$, $B$ and $Y$ gives eight samples of sizes ranging from around 400 to around 41,000. In order to have the same number of observations for each combination, we would need to strongly reduce the number of observations of certain classes, which will almost certainly modify the data structure. There are less drastic measures that can be taken. For example, we can under-sample the majority class up until the performance metrics do not decrease too much. \par

% % under-sample Y=0 with different sizes

% The majority class regarding the output $Y$ is for $Y=0$, with around 64,000 observations in the train set. We will under-sample this class to have a number of observations ranging between 63,000 and 25,000, so that the number of observations for $Y=0$ gets closer to the one for $Y=1$0. Figure~\ref{fig:RUS_Y0 metrics} in appendix~\ref{appendix:fig:RUS_Y0 metrics} gives the results. We observe that:
% \begin{itemize}
%     \item The accuracy decreases exponentially with under-sampling. Until 40,000 observations, it does not lose more than 0.01 point compared to without re-sampling.
%     \item Looking at the ROC curve, the AUC does not seem to vary substantially for different numbers of observations of Y=0.
%     \item The acceptance rate decreases, globally and for all protected groups. The differences in acceptance rates between groups increases, for both variables A and B. The same goes for the true positive rate. For false positive rates, however, they decrease, but the gap between groups does not seem to increase.
% \end{itemize}
% To conclude, with the under-sampling of class (Y=0), we have a worse accuracy and a better precision, and do not perform better in terms of fairness. \par
% Under-sampling the class (Y=0) will sometimes delete observations from the class (A=0,B=0, Y=0), other times from the class (A=0,B=1,Y=0), and so on. Some of thee classes are very populated, other are not. So we can try only under-sampling observations with (A=0,B=1,Y=0), the majority class by far, to get better results.

% % under-sample A=0,B=1,Y=0 with different sizes

% We can study the effects of under-sampling the class (A=0,B=1,Y=0) only. We will under-sample the observations with (A=0,B=1,Y=0) in our train set to have sizes ranging between 40,000 and 1,000. Figure~\ref{fig:RUS A0Y0 metrics} in appendix~\ref{appendix:fig:RUS A0Y0 metrics} gives the performance and fairness metrics of the model after under-sampling. We can observe that:

% \begin{itemize}
%     \item The accuracy exponentially decreases with under-sampling. Compared to the previous paragraph, the accuracy decrease seems to be similar.
%     \item Looking at the ROC curve, it does not seem like the under-sampling impacts the AUC too much, although we can see a slight shift of the curve towards the bottom-left, meaning the performances decreases.
%     \item The acceptance rate decreases, but less than in the previous paragraph. \\
%     For groups by value of A, the gap between acceptance rates slightly increases with under-sampling. By value of B, the gap strongly increases. \\
%     We have the same conclusions for the true positive and false positive rates.
% \end{itemize}
% This under-sampling is not conclusive: we have a worse accuracy, AUC, and all fairness metrics are worse than without under-sampling.
% We can find a threshold at which the performance metrics do not deteriorate too much with an amelioration of fairness metrics. As we saw, this threshold corresponds to having around 30,000 observations of $(A=0,B=1,Y=0)$0. \par
% % Table~\ref{tab:weights_reg A0Y0 RUS} gives the weights of the logistic regression, before and after under-sampling class (A=0,B=1,Y=0).

% % \begin{table}[H]
% %     \centering
% %     \begin{tabular}{c|c|c|c}
% %         Variable & Coefficient & Standard Error & P-value \\ \hline
% %         Intercept &  \\
% %         $X_1$ &  \\
% %         $X_2$ &  \\
% %         $X_3$ &  \\
% %         $X_4$ &  \\
% %         A &  \\
% %         B &  \\
% %     \end{tabular}
% %     \caption{Weights of the logistic regression (RUS)}
% %     \label{tab:weights_reg A0Y0 RUS}
% % \end{table}

% % Table~\ref{tab:metrics A0Y0 RUS} summarizes the performance and fairness metrics and compares them with the non-re-sampled case. The deteriorations have been colored in red. The following conclusions can be drawn:
% % \paragraph{Performance evaluation}
% % \begin{itemize}
% %     \item The global accuracy decrease is 1 point in average, which was the limit we imposed when choosing how much to under-sample the class (A=0,Y=0). The accuracy for group $A=0$ decreased by more than 1 point, but for group $A=1$, taking into account the width of the confidence interval, it remained the same.
% %     \item The global precision decrease is around 8 points. The precision for group $A=0$ decreased by around 13 points, but for group $A=1$ it remained the same.
% % \end{itemize}
% % With under-sampling class (A=0,Y=0), we have a worse accuracy, precision and false positive rate globally and for group $A=0$0. We have a better recall. None of the metrics have subsequently changed for group $A=1$0. All global metrics have changed, but only because they have changed for group $A=0$0. So the re-sampling had almost no effect on the way the model treats individuals of group $A=1$, but only on the way it treats group $A=0$0.
% % \paragraph{Fairness evaluation}
% % \begin{itemize}
% %     \item The acceptance rates are closer too, so we are closer to statistical parity, with group $A=0$ still disadvantaged. It has only changed for group $A=0$0. This means that fairness was improved by giving more positive outcomes to individuals of group $A=0$ and keeping the same proportion of positive outcomes for group $A=1$0.
% %     \item The true positive rates are closer after re-sampling, so we are closer to the equal opportunity definition. The true positive rate has only increased for group $A=0$ and is still higher for group $A=1$, so group $A=0$ is still disadvantaged. With under-sampling, we have reduced the gap between group-wise true positive rates. Fairness has improved thanks to the increase of the true positive rate for group $A=0$ only.
% %     \item The true negative rates are also closer, so we are closer to the equalized odds definition. Group $A=0$ is still disadvantaged. Once again, only the false positive rate of group $A=0$ has changed. It increased to get closer to the false positive rate of group $A=1$ and achieved this way a smaller gap between the group-wise false positive rates.
% % \end{itemize}
% % To conclude, under-sampling the majority class (A=0,Y=0) has the effect of changing performance and fairness metrics for group $A=0$ only. At not too high a cost in accuracy, we have improved fairness under all three definitions. \par

% % \begin{table}[H]
% %     \centering
% %     \begin{tabular}{c|c|ccc|ccc}
% %         \multicolumn{8}{c}{Baseline} \\
% %         (\%) & Global & A=0 & A=1 & Difference & B=0 & B=1 & Difference \\ \hline
% %         AR & $94.13\pm0.05$ & $96.60\pm0.04$ & $88.34\pm0.10$ & $8.26\pm0.10$ & $97.25\pm0.09$ & $93.78\pm0.05$ & $3.47\pm0.10$ \\
% %         TPR & $96.98\pm0.04$ & $98.26\pm0.03$ & $93.60\pm0.09$ & $4.67\pm0.09$ & $98.62\pm0.06$ & $96.79\pm0.04$ & $1.83\pm0.07$ \\
% %         FPR & $82.69\pm0.15$ & $88.55\pm0.15$ & $73.95\pm0.24$ & $14.60\pm0.26$ & $89.72\pm0.39$ & $82.11\pm0.16$ & $7.62\pm0.42$ \\
% %         \multicolumn{8}{c}{RUS} \\
% %         (\%) & Global & A=0 & A=1 & Difference & B=0 & B=1 & Difference \\ \hline
% %         AR &  \\
% %         TPR &  \\
% %         FPR & 
% %     \end{tabular}
% %     \caption{Fairness metrics globally and by protected group (RUS)}
% %     \label{tab:metrics A0Y0 RUS}
% % \end{table}

% % % \noindent \begin{center}
% % %     \fbox{
% % % \begin{minipage}{0.9\textwidth}
% % % \begin{itemize}
% % %     \item Random under-sampling can be used for imbalanced data, but can modify the data structure and greatly deteriorate performance and fairness if the sample size is greatly reduced.
% % %     \item When looking for the majority class to under-sample, we must pay attention to the fact that there might be multiple variables that define a class, and when combined, they can define more granular classes. This will be even more important when there are multiple protected variables, for example with the combination of gender and ethnicity.
% % %     \item When done right, under-sampling can improve both performance and fairness of a model.
% % % \end{itemize}
% % % \end{minipage}
% % % }
% % % \end{center}

% % \paragraph{Random over-sampling} over-samples the minority class by picking random samples with replacement. That way, the majority class does not take over the minority class during the training process. The main danger is overfitting, because randomly over-sampling simply duplicates observations. As the train set will be larger, the execution will also take longer. \par
% % As we saw with table~\ref{tab:A Y counts sampling}, the number of observations for (A=0,Y=1) and (A=1,Y=1) are approximately the same, the former representing about 11\% of the train set and the latter about 9\%. We can therefore try randomly over-sampling the observations (Y=1). We will over-sample this class to have a number of observations ranging between 17,000 and 60,000. Figure~\ref{fig:ROS Y1 metrics} in appendix~\ref{appendix:fig:ROS Y1 metrics} gives the results. We observe that:
% % \begin{itemize}
% %     \item The global accuracy linearly decreases with over-sampling.
% %     \item The global precision decreases.
% %     \item The global acceptance rate linearly increases. The difference between groups also increases.
% %     \item The global true positive rate increases. The difference between groups increases.
% %     \item The global false positive rate linearly increases. The difference between groups linearly increases.
% % \end{itemize}
% % All in all, these results are not satisfying: we have worse performance and fairness metrics, except for the higher global true positive rate. We can question the relevance of over-sampling the class (Y=1). \par
% % As we saw, group $A=0$ is disadvantaged by the model under all three definitions of fairness. We can wonder about the effect of over-sampling the class (A=0,Y=1) as it is the minority class. Figure~\ref{fig:ROS_A0Y1 metrics} in appendix~\ref{appendix:fig:ROS_A0Y1 metrics} gives the results. We observe that:
% % \begin{itemize}
% %     \item The global accuracy linearly decreases.
% %     \item The global precision decreases.
% %     \item The global acceptance rate increases. The acceptance rate difference between groups decreases.
% %     \item The global true positive rate increases. The true positive rate difference between groups decreases.
% %     \item The global false positive rate linearly increases. The false positive rate difference between groups linearly decreases.
% % \end{itemize}
% % With this over-sampling, the accuracy and precision deteriorate, but the recall and the fairness metrics improve. We can find a threshold at which the performance metrics do not deteriorate too much with an amelioration of fairness metrics. As for the case of under-sampling class (A=0,Y=0), limiting the accuracy and precision decrease at 2 points seems reasonable. This corresponds to over-sampling until we have 15,000 observations of (A=0,Y=1). \par
% % Table~\ref{tab:weights_reg A0Y1 ROS} and figure~\ref{fig:ROS_A0Y1 weights} give the weights of the logistic regression, before and after over-sampling class (A=0,Y=1). Figure~\ref{fig:ROS_A0Y1 permutfeature} gives the permutation feature importance scores. The main difference between with and without over-sampling is the importance of variable $A$: before re-sampling, it was the most important in terms of permutation scores, but now it is not the case anymore. Variable $A$ has become less important than $X^{(1)}$ and $X^{(3)}$0. Looking at the regression weights, there is little change, except for the decreased weights for the intercept and $A$0.

% % \begin{table}[H]
% %     \centering
% %     \begin{tabular}{|c|c|c|}
% %         \hline
% %         i & Without re-sampling & Random over-sampling \\ \hline
% %         0 & $-2.490\pm0.004$ & $-1.956\pm0.004$ \\ \hline
% %         1 & $1.284\pm0.003$ & $1.303\pm0.003$ \\ \hline
% %         2 & $0.308\pm0.003$ & $0.331\pm0.004$ \\ \hline
% %         3 & $-1.236\pm0.003$ & $-1.249\pm0.004$ \\ \hline
% %         4 & $0.238\pm0.002$ & $0.253\pm0.003$ \\ \hline
% %         a & $1.125\pm0.003$ & $0.760\pm0.003$ \\ \hline
% %     \end{tabular}
% %     \caption{Weights of the logistic regression}
% %     \label{tab:weights_reg A0Y1 ROS}
% % \end{table}

% % % Figure environment removed

% % % Figure environment removed

% % Table~\ref{tab:metrics A0Y1 ROS} summarizes the performance and fairness metrics and compares to the non-re-sampled case. The following conclusions can be drawn:
% % \paragraph{Performance evaluation}
% % \begin{itemize}
% %     \item The global accuracy has decreased by almost 1 point, which was the limit we imposed when choosing how much to over-sample the class (A=0,Y=1). The accuracy of group $A=0$ has also decreased, but for group $A=1$ the decrease is negligible considering the widths of the confidence intervals.
% %     \item The global precision has decreased a lot, by more than 7 points. It has decreased a lot for group $A=0$, but once again the decrease is negligible for group $A=1$0.
% %     % \item The acceptance rate has increases globally and for group $A=0$, but the change for group $A=1$ is negligible.
% %     % \item The global recall has increased a lot, by 8.8 points. It has increased for group $A=0$ and almost not changed for group $A=1$0.
% %     % \item The global false positive rate has increased. It has increased for group $A=0$, but the increase in negligible for group $A=1$0.
% % \end{itemize}
% % Just like with under-sampling class (A=0,Y=0), by over-sampling class (A=0,Y=1), we have a worse accuracy and precision globally and for group $A = 0$0. None of the metrics have subsequently changed for group $A = 1$0. All global metrics have changed, but only because they have changed for group $A = 0$0. So the re-sampling had almost no effect on the way the model treats individuals of group $A = 1$, but only on the way it treats group $A = 0$0.
% % \paragraph{Fairness evaluation} The conclusions are the same as for the random under-sampling of class (A=0,Y=0). \par
% % \paragraph{} To conclude, over-sampling the minority class (A=0,Y=1) has the effect of changing performance and fairness metrics for group $A=0$ only. At not too high a cost in accuracy, we have improved fairness under all three definitions. \par

% % \begin{table}[H]
% %     \centering
% %     \captionsetup{justification=centering}
% %     \begin{tabular}{ccccc}
% %         \multicolumn{5}{c}{Without re-sampling} \\
% %          & \multicolumn{1}{|c|}{Global} & A=0 & A=1 & \multicolumn{1}{|c}{Difference} \\ \hline
% %         Accuracy & \multicolumn{1}{|c|}{$86.69\%\pm0.05\%$} & $89.91\%\pm0.05\%$ & $79.17\%\pm0.10\%$ & \multicolumn{1}{|c}{} \\
% %         Precision & \multicolumn{1}{|c|}{$72.23\%\pm0.17\%$} & $66.06\%\pm0.33\%$ & $75.02\%\pm0.20\%$ & \multicolumn{1}{|c}{} \\
% %         AR & \multicolumn{1}{|c|}{$15.01\%\pm0.05\%$} & $6.43\%\pm0.04\%$ & $35.31\%\pm0.12\%$ & \multicolumn{1}{|c}{$-28.78\%\pm0.13\%$} \\
% %         TPR & \multicolumn{1}{|c|}{$54.38\%\pm0.17\%$} & $34.95\%\pm0.26\%$ & $68.71\%\pm0.19\%$ & \multicolumn{1}{|c}{$-33.76\%\pm0.33\%$} \\
% %         FPR & \multicolumn{1}{|c|}{$5.21\%\pm0.04\%$} & $2.49\%\pm0.03\%$ & $14.29\%\pm0.12\%$ & \multicolumn{1}{|c}{$-11.80\%\pm0.13\%$} \\
% %         \multicolumn{5}{c}{} \\
% %         \multicolumn{5}{c}{Random over-sampling (A=0,Y=1)} \\
% %          & \multicolumn{1}{|c|}{Global} & A=0 & A=1 & \multicolumn{1}{|c}{Difference} \\ \hline
% %         Accuracy & \multicolumn{1}{|c|}{{\color[HTML]{FE0000} $85.79\%\pm0.05\%$}} & {\color[HTML]{FE0000} $88.66\%\pm0.05\%$} & $79.11\%\pm0.11\%$ & \multicolumn{1}{|c}{} \\
% %         Precision & \multicolumn{1}{|c|}{{\color[HTML]{FE0000} $64.97\%\pm0.17\%$}} & {\color[HTML]{FE0000} $53.16\%\pm0.24\%$} & $74.92\%\pm0.20\%$ & \multicolumn{1}{|c}{} \\
% %         AR & \multicolumn{1}{|c|}{$19.50\%\pm0.05\%$} & $12.75\%\pm0.06\%$ & $35.22\%\pm0.12\%$ & \multicolumn{1}{|c}{$-22.48\%\pm0.13\%$} \\
% %         TPR & \multicolumn{1}{|c|}{$63.20\%\pm0.15\%$} & $55.80\%\pm0.22\%$ & $68.64\%\pm0.22$ & \multicolumn{1}{|c}{$-12.84\%\pm0.28\%$} \\
% %         FPR & \multicolumn{1}{|c|}{$8.54\%\pm0.05\%$} & $6.80\%\pm0.05\%$ & $14.35\%\pm0.13\%$ & \multicolumn{1}{|c}{$-7.56\%\pm0.13\%$}
% %     \end{tabular}
% %     \caption{95\% confidence intervals of global and group-specific metrics (all variables), decreases in accuracy and precision in red}
% %     \label{tab:metrics A0Y1 ROS}
% % \end{table}

% % \paragraph{Conclusion on re-sampling} Both re-samplings have improved fairness with a cost in performance. For the same accuracy, randomly over-sampling class (A=0,Y=1) yields a smaller true positive rate absolute difference. Considering the confidence intervals widths, both re-samplings give the same results for the absolute difference of false positive and acceptance rates. To conclude, we have a better accuracy-fairness trade-off for over-sampling class (A=0,Y=1), using the definitions of equal opportunity and equalized odds. For statistical parity, both methods yield the same results.

% % % \paragraph{SMOTE} (Synthetic Minority Over-sampling TEchnique) \cite{chawla02} over-samples the minority class by creating synthetic examples along the line segments joining the minority class nearest neighbors. To generate the synthetic samples, the algorithm goes through the following steps, illustrated in figure~\ref{fig:SMOTE}, until the number of minority observations required is satisfied:
% % % \begin{enumerate}
% % %     \item Randomly select a minority observation and one of its k nearest (minority) neighbors
% % %     \item Take the feature vector difference $D$ between the observation and the selected neighbor
% % %     \item Multiply $D$ by a random number $\alpha \in (0,1)$ and add it to the observation
% % % \end{enumerate}

% % % % Figure environment removed

% % % \noindent The SMOTE algorithm is adapted to handle categorical features: the given value for a categorical feature of a synthetic observation is the one occurring in the majority of the neighbors. \par
% % % Like in the previous paragraph, we will over-sample the class (A=0). Figure~\ref{fig:SM A0 metrics} in appendix~\ref{appendix:fig:SM A0 metrics} gives the results. We observe the accuracy and precision decrease compared to the baseline. The true positive and negative rates and acceptance rate differences decrease compared to the baseline, meaning that the model is closer to fairness under the three definitions than the baseline.

% % % \noindent \begin{center}
% % %     \fbox{
% % % \begin{minipage}{0.9\textwidth}
% % % \begin{itemize}
% % %     \item The SMOTE algorithm is a lot more time-consuming than the random over-sampler.
% % %     \item Over-sampling the disadvantaged group even if it is not a minority can help improve model fairness at the cost of model performance.
% % % \end{itemize}
% % % \end{minipage}
% % % }
% % % \end{center}

% % % % Le SMOTE repose sur l’algorithme des k-NN (k-Nearest Neighbors ou k-plus proches voisins). Cet algorithme utilise la distance euclidienne, qui peut être biaisée par l’échelle des variables. Ainsi, une variable qui prend des valeurs entre 0 et 100 aura plus d’importance dans les calculs de distance qu’une variable qui prend des valeurs entre 0 et 1.
% % % % Il est donc important – comme pour tous les algorithmes métriques – de normaliser (scaling) les données pour utiliser les différentes variables de façon non biaisée.

% % % % SMOTE approach can improve accuracy classifiers for minority class, better than random over-sampling, combi smote and under-sampling better than just under-sampling, forces focused learning and introduces new bias towards minority class, builds larger decision regions that contain nearby minority class points ie more coverage minority class

% % % \paragraph{Conclusion} The random under-sampling of (A=0,Y=0) was the most effective way of improving fairness without too great a cost in performance. Random over-sampling and SMOTE over-sampling of (Y=0) do not improve performances or fairness in a satisfying way. When over-sampling the disadvantaged group (A=0), SMOTE was better than random over-sampler regarding performance and fairness. All in all, there is a trade-off between model performance and model fairness that cannot be avoided.

% % % \subsubsection{Penalty and solver}


% % % %[48] doing nothing: Minimizing avg error fits majority populations



\subsection{Removing protected variables to avoid direct discrimination}
\paragraph{} When sensitive variables are omitted, models can still learn stereotypes, because sensitive information is embedded in datasets even if it is not intentional. Leaving out sensitive variables forces the correlated variables to take on a greater importance. This is the omitted variable bias \cite{williams18}. \par
Removing sensitive attributes is problematic, because it becomes impossible to check for bias and discrimination. We cannot see if the most important variables for prediction are strongly correlated with a protected attribute or compute metrics. This is a problem related to data regulations: as we saw in section~\ref{gdpr}, the GDPR requires minimal data collection, but more data is needed to prove discrimination. \par
We will preprocess our data by removing the sensitive attributes, $A$ and $B$, then predict the outcome with a logistic regression model. Table~\ref{tab:weights_reg without A} gives the predicted weights of this logistic regression. As $A$ and $B$ are no longer used as explanatory variables, the weights of regression have changed. For example, the biggest change is for $X^{(3)}$, which had a coefficient of 0.25 and now has a coefficient of 2.46. Remembering the correlation matrix, $X^{(3)}$ is correlated with A and B with relatively high Pearson correlation coefficients: -0.23 and 0.35 respectively. This can indicate that A and B will still indirectly play a part in the model predictions.

\begin{table}[H]
    \centering
    \begin{tabular}{c|c|c|c}
    \multicolumn{4}{c}{With all variables} \\
        Variable & Coefficient & Standard Error & P-value \\ \hline
        Intercept & $-1.82\pm0.01$ & $0.05\pm0.00$ & $0.00\pm0.00$ \\
        $X_1$ & $0.55\pm0.00$ & $0.02\pm0.00$ & $0.00\pm0.00$ \\
        $X_2$ & $2.81\pm0.01$ & $0.05\pm0.00$ & $0.00\pm0.00$ \\
        $X_3$ & $0.25\pm0.00$ & $0.01\pm0.00$ & $0.00\pm0.00$ \\
        $X_4$ & $-2.53\pm0.01$ & $0.03\pm0.00$ & $0.00\pm0.00$ \\
        A & $-0.17\pm0.01$ & $0.03\pm0.00$ & $0.00\pm0.00$ \\
        B & $0.41\pm0.01$ & $0.04\pm0.00$ & $0.00\pm0.00$ \\
        \multicolumn{4}{c}{} \\
        \multicolumn{4}{c}{Without protected variables} \\
        Variable & Coefficient & Standard Error & P-value \\ \hline
        Intercept & $-1.76\pm0.01$ & $0.05\pm0.00$ & $0.00\pm0.00$ \\
        $X_1$ & $0.65\pm0.00$ & $0.02\pm0.00$ & $0.00\pm0.00$ \\
        $X_2$ & $2.68\pm0.01$ & $0.05\pm0.00$ & $0.00\pm0.00$ \\
        $X_3$ & $2.46\pm0.00$ & $0.01\pm0.00$ & $0.00\pm0.00$ \\
        $X_4$ & $-2.41\pm0.01$ & $0.03\pm0.00$ & $0.00\pm0.00$ \\
    \end{tabular}
    \caption{Weights of the logistic regression}
    \label{tab:weights_reg without A}
\end{table}

\paragraph{Performance evaluation}
\begin{itemize}
    \item As we can see in table~\ref{tab:metrics without A}, the accuracy has only decreased by 0.01 on average, which is negligible, especially considering the width of the confidence interval.
    \item Looking at the ROC curve in figure~\ref{fig:ROC_sim_withoutprotected}, the model still performs quite well, and the AUC has decreased from 0.7568 to 0.7466 compared to the model using all variables.
\end{itemize}

\begin{table}[H]
    \centering
    \begin{tabular}{c|c|c}
         (\%) & With all variables & Without protected variables \\ \hline
        Accuracy & $81.05\pm0.05$ & $81.04\pm0.05$
    \end{tabular}
    \caption{Global metrics}
    \label{tab:metrics without protected}
\end{table}

% Figure environment removed

\paragraph{Fairness evaluation}
\begin{itemize}
    \item Acceptance rate \begin{itemize}
        \item The global acceptance rate has slightly increased.
        \item  For groups A, the gap between acceptance rates has increased by more than one point. This was expected, as $X^{(3)}$, strongly correlated with A, has taken on a great importance in the model prediction.
        \item For groups B, the difference in acceptance rates has decreased.
        \item Looking at protected subgroups, the most advantaged subgroups are the same as in the model with all variables: the most advantaged subgroup is $(A=0,B=0)$ and the most disadvantaged is $(A=1,B=1)$0. We now have a lower gap between acceptance rates for subgroups $(A=0,B=0)$ and $(A=0,B=1)$ and for $(A=1,B=0)$ and $(A=1,B=1)$, which is coherent as acceptance rates between groups B are closer than previously.
    \end{itemize}
    \item For true and false positive rates, we have the same conclusions as for the acceptance rates.
\end{itemize}
For groups A, the gaps between fairness metrics are wider when the protected variables are not used in the model, but for group B, they are smaller, although this phenomenon comes from the structure of the correlation matrix. The same groups remain disadvantaged.
\paragraph{} To conclude, the performance metrics have not deteriorated too much compared to when using all variables. The fairness metrics are worse for groups A but better when looking at groups B. To conclude, simply ignoring the protected variables is not a solution. \par

\begin{table}[H]
    \centering
    \captionsetup{justification=centering}
    \makebox[\linewidth]{
    \begin{tabularx}{19.5cm}{c|c|ccc|ccc}
    % \begin{tabular}{c|c|ccc|ccc}
        \multicolumn{8}{c}{With all variables} \\
        (\%) & Global & A=0 & A=1 & Difference & B=0 & B=1 & Difference \\ \hline
        AR & $94.13\pm0.05$ & $96.60\pm0.04$ & $88.34\pm0.10$ & $8.26\pm0.10$ & $97.25\pm0.09$ & $93.78\pm0.05$ & $3.47\pm0.10$ \\
        TPR & $96.98\pm0.04$ & $98.26\pm0.03$ & $93.60\pm0.09$ & $4.67\pm0.09$ & $98.62\pm0.06$ & $96.79\pm0.04$ & $1.83\pm0.07$ \\
        FPR & $82.69\pm0.15$ & $88.55\pm0.15$ & $73.95\pm0.24$ & $14.60\pm0.26$ & $89.72\pm0.39$ & $82.11\pm0.16$ & $7.62\pm0.42$ \\
        \multicolumn{8}{c}{} \\
        \multicolumn{8}{c}{Without protected variables} \\
        (\%) & Global & A=0 & A=1 & Difference & B=0 & B=1 & Difference \\ \hline
        AR & $94.29\pm0.05$ & $97.05\pm0.04$ & $87.55\pm0.10$ & $9.50\pm0.09$ & $95.38\pm0.10$ & $94.07\pm0.055$ & $1.31\pm0.09$ \\
        TPR & $97.02\pm0.03$ & $98.51\pm0.03$ & $93.07\pm0.09$ & $5.44\pm0.08$ & $97.41\pm0.08$ & $96.97\pm0.04$ & $0.44\pm0.08$ \\
        FPR & $82.92\pm0.12$ & $89.94\pm0.13$ & $72.53\pm0.23$ & $17.41\pm0.26$ & $84.15\pm0.46$ & $82.82\pm0.13$ & $1.33\pm0.48$ \\
    % \end{tabular}
    \end{tabularx}
    }
    \caption{Fairness metrics}
    \label{tab:metrics without A}
\end{table}

\begin{table}[H]
    \centering
    \begin{tabular}{c|cc|cc}
        \multicolumn{5}{c}{With all variables} \\
        \multirow{2}{*}{(\%)} & \multicolumn{2}{c|}{A=0} & \multicolumn{2}{c}{A=1} \\ \cline{2-5}
         & \multicolumn{1}{c|}{B=0} & B=1 & \multicolumn{1}{c|}{B=0} & B=1 \\ \hline
        AR & \multicolumn{1}{c|}{$98.47\pm0.07$} & $96.39\pm0.04$ & \multicolumn{1}{c|}{$93.72\pm0.22$} & $87.92\pm0.12$ \\
        TPR & \multicolumn{1}{c|}{$99.21\pm0.05$} & $98.15\pm0.03$ & \multicolumn{1}{c|}{$96.58\pm0.19$} & $93.41\pm0.09$ \\
        FPR & \multicolumn{1}{c|}{$93.65\pm0.36$} & $88.08\pm0.16$ & \multicolumn{1}{c|}{$83.15\pm0.66$} & $73.34\pm0.26$ \\
        \multicolumn{5}{c}{} \\
        \multicolumn{5}{c}{Without protected variables} \\
        \multirow{2}{*}{(\%)} & \multicolumn{2}{c|}{A=0} & \multicolumn{2}{c}{A=1} \\ \cline{2-5}
         & \multicolumn{1}{c|}{B=0} & B=1 & \multicolumn{1}{c|}{B=0} & B=1 \\ \hline
        AR & \multicolumn{1}{c|}{$97.43\pm0.08$} & $97.00\pm0.04$ & \multicolumn{1}{c|}{$88.94\pm0.27$} & $87.43\pm0.10$ \\
        TPR & \multicolumn{1}{c|}{$98.56\pm0.06$} & $98.50\pm0.03$ & \multicolumn{1}{c|}{$93.41\pm0.25$} & $93.04\pm0.09$ \\
        FPR & \multicolumn{1}{c|}{$90.01\pm0.44$} & $89.92\pm0.13$ & \multicolumn{1}{c|}{$72.61\pm0.87$} & $72.53\pm0.24$
    \end{tabular}
    \caption{Fairness metrics by protected subgroups}
    \label{tab:fairness metrics subgroups without protected}
\end{table}

% Figure environment removed

\subsection{Transforming the non-protected variables to mitigate indirect discrimination}
\subsubsection{Theory}
\paragraph{The idea} Focusing on the definition of fairness as statistical parity, we can view it as an independence condition. Statistical parity requires $\hat{Y} \indep A,B$0. Since we do not want $A$ or $B$ to impact the predicted output, the goal is firstly not to use them as explanatory variables and secondly to have explanatory variables that are independent of them. Dealing with independence is a complex problem, which is why we will tackle it on a linear level only - with correlation. We will try to obtain transformations of the variables $X^{(i)}$ uncorrelated with $A$ and $B$, and we will then use them as inputs of a logistic regression model to predict $Y$0. \par
Drawing inspiration from the Gram-Schmidt process (a reminder is given in appendix~\ref{appendix:gram schmidt}), the idea is to view the variables as vectors in a n-dimensional space (n being the number of variables in our dataset) and the covariance between them as a scalar product. Let us set the theoretical framework. \par

% Figure environment removed

\begin{definition} An inner product is a map
\begin{equation*}
    \langle \cdot,\cdot \rangle : V\times V\rightarrow \mathbb{R}
\end{equation*}
with $V$ a real vector space, that is symmetric, bilinear and positive-definite.
\end{definition}

\begin{proposition} Set $X$ and $Y$ real random variables with zero mean and finite variance. Their covariance
\begin{equation*}
    \langle X,Y \rangle =cov(X,Y)
\end{equation*}
is an inner product (on the space of random variables with zero mean and finite variance).
\end{proposition}
\begin{proof}
Set $X,Y,Z$ real random variables with zero mean and finite variance and $(a,b)\in\mathbb{R}^2$0. Then $cov(X,Y)=\mathbb{E}[XY]-\mathbb{E}[X]\mathbb{E}[Y]=\mathbb{E}[XY]$0. We will check the three properties of the inner product:
\begin{itemize}
    \item symmetry: $\mathbb{E}[XY]=\mathbb{E}[YX]$
    \item bilinearity: $\mathbb{E}[(aX+bY)Z]=a\mathbb{E}[XZ]+b\mathbb{E}[YZ]$
    \item positive-definiteness: $\mathbb{E}[XX] =\mathbb{E}[X^2]\geq0$ \\
    and $\mathbb{E}[X^2]=0\iff Var(X)+(\mathbb{E}[X])^2=0\iff \left\{ \begin{array}{l}
        Var(X)=0 \\
        \mathbb{E}[X]=0
    \end{array} \right. \iff X=0 \mbox{ a.s.}$
\end{itemize}
\end{proof}

\begin{definition} Two vectors are orthogonal if their inner product is zero. \end{definition}

As covariance is an inner product on the space of random variables with zero mean and finite variance, two random variables of this space are orthogonal if their covariance is zero ie if their correlation is zero, because as we saw in definition~\ref{def:pearson corr},
\begin{equation*}
    corr(X,Y)=\frac{cov(X,Y)}{\sqrt{Var(X)Var(Y)}}
\end{equation*}
and their variance is finite. So we have an interpretation of random variables of this space as vectors with an inner product. \par
Going back to the initial goal, we wanted a transformation of our variables $X^{(i)}$ that is uncorrelated to both $A$ and $B$0. With the framework we have set, it means that the transformed variables will be orthogonal to $A$ and to $B$0. Suppose the (non-orthogonal) basis of our vector space is the linearly independent set $B=\{u_1,\dots,u_n\}$ such that any vector Z of the space - which corresponds to the characteristic of one individual ie row of the dataset - can be uniquely written as a linear combination of the vectors of the basis:
\begin{equation*}
    Z=\sum_{i=1}^n x_iu_i \mbox{ with } X=\begin{bmatrix}
    x_1 \\
    \dots \\
    x_n
    \end{bmatrix}
\end{equation*}

Our goal is to find a change of basis that gives us the new basis $B'=\{v_1,\dots,v_n\}$0.
We will then be able to write 
\begin{equation*}
    Z=\sum_{j=1}^n x_j'v_j \mbox{ with } X'=\begin{bmatrix}
    x_1' \\
    \dots \\
    x_n'
    \end{bmatrix}
\end{equation*}
% The change-of-basis formula expresses the coordinates over the new basis in terms of the ones over the old basis:
% \begin{equation*}
%     \forall j \in \llbracket1,n\rrbracket, v_j=\sum_{i=1}^n a_{i,j}u_i
% \end{equation*}
$A=(a_{i,j})_{i,j}$ is called the transition matrix and its $\mbox{j}^{th}$ row is formed by the coordinates of $v_j$:
\begin{equation*}
    A=\begin{bmatrix}
    a_{1,1} & a_{1,2} & \dots & a_{1,n} \\
    a_{2,1} & a_{2,2} & \dots & \\
    \dots & & & \\
    a_{n,1} & \dots & & a_{n,n}
    \end{bmatrix}
\end{equation*}
Then, the change-of-basis formula gives in matrix form
\begin{equation*}
    \begin{split}
        X' & =AX \\
         & =\begin{bmatrix}
    a_{1,1} & a_{1,2} & \dots & a_{1,n} \\
    a_{2,1} & a_{2,2} & \dots & \\
    \dots & & & \\
    a_{n,1} & \dots & & a_{n,n}
    \end{bmatrix}\begin{bmatrix}
    x_1 \\
    x_2 \\
    \dots \\
    x_n
    \end{bmatrix} \\
    & = \begin{bmatrix}
    a_{1,1}x_1+a_{1,2}x_2+\dots+a_{1,n}x_n \\
    a_{2,1}x_1+a_{2,2}x_2+\dots+a_{2,n}x_n \\
    \dots \\
    a_{n,1}x_1+a_{n,2}x_2+\dots+a_{n,n}x_n
    \end{bmatrix}
    \end{split}
\end{equation*}
So
\begin{equation*}
    x_i'=\sum_{j=1}^n a_{i,j}x_j
\end{equation*}

% \begin{proof}
%     \begin{equation*}
%             Z=\sum_{j=1}^n x_j'v_j=\sum_{j=1}^n x_j'(\sum_{i=1}^n a_{i,j}u_i)=\sum_{i=1}^n (\sum_{j=1}^n x_j' a_{i,j})u_i
%     \end{equation*}
    % So $x_i=\sum_{j=1}^n x_j' a_{i,j}$ ie $X=AX'$0.
% \end{proof}
\par
Our goal is to have a new basis in which the non-sensitive vectors are orthogonal to the sensitive ones. For this reason, we impose the following constraints for the construction of the new basis:
\begin{enumerate}
    \item we do not want to transform the sensitive variables, so the first s vectors of the new basis will remain the same as in the old basis: $x_1=x_1',\dots,x_s=x_s'$
    \begin{equation*}
        \begin{split}
            x_i=x_i' & \Leftrightarrow x_i=\sum_{j=1}^n a_{i,j}x_j \\
             & \Leftrightarrow \left\{
            \begin{array}{l}
                a_{i,i}=1 \\
                a_{i,j}=0 \mbox{ for } j\neq i
            \end{array}
            \right.
        \end{split}
    \end{equation*}
    As a result,
    \begin{equation*}
        A=\left[ \begin{array}{cccccc}
            \multicolumn{3}{c}{\multirow{3}{*}{$I_s$}} & 0 & \dots & 0 \\
            \multicolumn{3}{c}{} & \vdots &  & \vdots \\
            \multicolumn{3}{c}{} & 0 & \dots & 0 \\
            a_{s+1,1} & \multicolumn{4}{c}{\dots} & a_{s+1,n} \\
            \vdots &  &  &  &  & \vdots \\
            a_{n,1} & \multicolumn{4}{c}{\dots} & a_{n,n} \\
      \end{array} \right]
    \end{equation*}
    with $I_s$ the identity matrix of size $s$0.
    \item the new non-sensitive vectors will be orthogonal to the sensitive vectors:
    $\forall j\in \llbracket1,s\rrbracket, \forall k\in \llbracket s+1,n\rrbracket, \langle v_j,v_k \rangle=0$ \\
    This is equivalent to: for all $k\in \llbracket s+1,n\rrbracket$ a system of $s$ equations with $n$ unknown variables (the $a_{.,k}$):
\begin{equation} \label{eq:syst}
    \left\{
    \begin{array}{ll}
        \langle v_1,v_k \rangle & =0 \\
        \multicolumn{2}{c}{\dots} \\
        \langle v_s,v_k \rangle & =0
    \end{array}
\right.
\end{equation}
The rows of $A$ are the $v_j$, so $v_j=\left\{
    \begin{array}{ll}
        u_j & \mbox{ if } j\in \llbracket1,s\rrbracket \\
        \sum_{i=1}^n a_{j,i}u_i & \mbox{ if } j\in \llbracket s+1,n\rrbracket
    \end{array}
\right.$
\begin{equation*}
    \begin{split}
        (\ref{eq:syst}) & \Leftrightarrow \left\{
        \begin{array}{ll}
            \langle u_1,\sum_{j=1}^n a_{k,j}u_j \rangle & =0 \\
            \multicolumn{2}{c}{\dots} \\
            \langle u_s,\sum_{j=1}^n a_{k,j}u_j \rangle & =0
        \end{array}
        \right. \\
         & \Leftrightarrow \left\{
        \begin{array}{ll}
            \sum_{j=1}^n a_{k,j} \langle u_1,u_j \rangle & =0 \\
            \multicolumn{2}{c}{\dots} \\
            \sum_{j=1}^n a_{k,j} \langle u_s,u_j \rangle & =0
        \end{array}
        \right.
    \end{split}
\end{equation*}
\end{enumerate}

For each row k of A, we are looking for the values of the $a_{k,1},\dots,a_{k,n}$0. The previous constraints give for each row a system of $s$ equations, but we have $n$ unknown variables, so the system is underdetermined, with an infinite number of solutions. To simplify, we can set
\begin{equation*}
    \forall j \in \{s+1,\dots,k-1,k+1,\dots,n\}, a_{k,j}=0 \mbox{ ie } v_k=\sum_{j=1}^{s} a_{k,j}u_j+a_{k,k}u_k
\end{equation*}
Meaning that each non-sensitive vector of the new basis is written as a linear combination of itself and of the sensitive vectors of the old basis. This gives a transition matrix of the shape
\begin{equation*}
    A=\left[ \begin{array}{cccccccc}
        \multicolumn{3}{c}{\multirow{3}{*}{$I_s$}} & 0 & \multicolumn{3}{c}{\dots} & 0 \\
        \multicolumn{3}{c}{} & \vdots &  &  &  & \vdots \\
        \multicolumn{3}{c}{} & 0 & \multicolumn{3}{c}{\dots} & 0 \\
        a_{s+1,1} & \dots & a_{s+1,s} & a_{s+1,s+1} & 0 & 0 & \dots & 0 \\
        a_{s+2,1} & \dots & a_{s+2,s} & 0 & a_{s+2,s+2} & 0 & \dots & 0 \\
        \vdots \\
        a_{n,1} & \dots & a_{n,s} & 0 & 0 & \dots &  0 & a_{n,s+1}
    \end{array} \right]
\end{equation*}

We now have for each row, a system of $s$ linear equations and $s+1$ unknown variables. We need one more constraints in order to have a unique solution. An idea is to minimize the distance between the old and the new basis (non-sensitive) vectors:
\begin{equation*}
    \min_{a_{k,1},\dots,a_{k,n}} d(u_k,v_k)
\end{equation*}
As the distance is positive, it has a lower bound, so this problem has a solution. We have defined the inner product as the covariance between random variables with zero mean and finite variance, so the distance between two such random variables $X$ and $Y$ is
\begin{equation*}
    d(X,Y)=\langle X-Y,X-Y\rangle=cov(X-Y,X-Y)=Var(X-Y)
\end{equation*}
So
\begin{equation*}
    \begin{split}
        d(u_k,v_k) & =d(u_k,\sum_{j=1}^{s} a_{k,j}u_j+a_{k,k}u_k) \\
         & =\langle (1-a_{k,k})u_k-\sum_{j=1}^{s} a_{k,j}u_j-a_{k,k}u_k,(1-a_{k,k})u_k-\sum_{j=1}^{s} a_{j,k}u_j \rangle \\
         & =((1-a_{k,k})u_k-\sum_{j=1}^{s} a_{k,j}u_j)^T((1-a_{k,k})u_k-\sum_{j=1}^{s} a_{k,j}u_j) \\
         & =\sum_{i=1}^n ((1-a_{k,k})u_{i,k}-\sum_{j=1}^{s} a_{k,j}u_{i,j})^2
    \end{split}
\end{equation*}
As we had a system of $s$ linear equations and $s+1$ unknown variables, we can express $a_{k,k}$ as a combination of the other $a_{.,k}$0. So the minimization of this distance gives a unique solution with the previous constraints. To summarize, for each $k=s+1,\dots,n$, we have the following minimization problem under constraints:
\begin{equation*}
        \min_{a_{k,1},\dots,a_{k,s},a_{k,k}} d(u_k,v_k) \mbox{ such that }\left\{
        \begin{array}{ll}
            \langle v_1,v_k \rangle & =0 \\
            \multicolumn{2}{c}{\dots} \\
            \langle v_s,v_k \rangle & =0
        \end{array}
        \right. \mbox{ with }  v_k=\sum_{j=1}^{s} a_{k,j}u_j+a_{k,k}u_k
\end{equation*}
Solving the problem for every $k=s+1,\dots,n$ gives us the transition matrix $A$0. Then, with $X$ the coordinates of a vector in the base $B$ and $X'$ in the base $B'$, we can write 
\begin{equation*}
    X'=AX 
\end{equation*}
Meaning that we will compute, for every observation, the transformation of each vector - corresponding to each individual - in the new basis. \par

\paragraph{Extreme-case scenarii}
We can wonder what would happen in the extreme-case scenario in which the non-sensitive variables are already uncorrelated with the sensitive ones. Then, we do not need to transform the non-sensitive variables: $A=I_n$ We have $\langle v_j,v_k\rangle=0$ for $j=1,\dots,s$ and $k=s+1,\dots,n$ and we have a minimal distance as $d(u_k,v_k)=d(u_k,u_k)=\langle u_k-u_k,u_k-u_k\rangle=0$0. \par
The other extreme-case scenario is the one in which all the variables are the non-sensitive variables are perfectly (positively or negatively) correlated with the sensitive ones. Then it means that the non-sensitive variables are a linear function of the sensitive ones, and consequently, of each other. It is therefore impossible to have non-sensitive vectors uncorrelated with the sensitive ones. Fortunately, in reality, when we have our datasets, we only have samples of `true' distributions, meaning that variables are never perfectly correlated with each other (except if a variables appears twice, but we can delete the duplicate). In the worst case, if the non-sensitive variables are very correlated to the non-sensitive ones, we will end up with transformed non-sensitive variables that have a very low variance, meaning that they will not explain the output very well.


\subsubsection{Results}
\paragraph{Transition matrix} The average transition matrix obtained on the 100 datasets is
\begin{equation*}
    A=\begin{bmatrix}
    1 & 0 & 0 & 0 & 0 & 0 \\
    0 & 1 & 0 & 0 & 0 & 0 \\
    -0.32 & 0.72 & 1 & 0 & 0 & 0 \\
    0.15 & 0.11 & 0 & 1 & 0 & 0 \\
    0.28 & 1.93 & 0 & 0 & 1 & 0 \\
    -0.21 & 0.48 & 0 & 0 & 0 & 1
    \end{bmatrix}\pm \begin{bmatrix}
    0 & 0 & 0 & 0 & 0 & 0 \\
    0 & 0 & 0 & 0 & 0 & 0 \\
    3\mathrm{e}{-3} & 6\mathrm{e}{-3} & 0 & 0 & 0 & 0 \\
    2\mathrm{e}{-3} & 3\mathrm{e}{-3} & 0 & 0 & 0 & 0 \\
    1\mathrm{e}{-2} & 2\mathrm{e}{-2} & 0 & 0 & 0 & 0 \\
    2\mathrm{e}{-3} & 3\mathrm{e}{-3} & 0 & 0 & 0 & 0
    \end{bmatrix}
\end{equation*}
This means that, on average,
\begin{equation*}
    A^{-1}=\begin{bmatrix}
    1 & 0 & 0 & 0 & 0 & 0 \\
    0 & 1 & 0 & 0 & 0 & 0 \\
    0.32 & -0.72 & 1 & 0 & 0 & 0 \\
    -0.15 & -0.11 & 0 & 1 & 0 & 0 \\
    -0.28 & -1.93 & 0 & 0 & 1 & 0 \\
    0.21 & -0.48 & 0 & 0 & 0 & 1
    \end{bmatrix}
\end{equation*}
\begin{equation*}
    \begin{split}
        X_1' & =A^{-1}X_1=.32A-0.72B+X_1 \\
        X_2' & =A^{-1}X_2=-0.15A-0.11B+X_2 \\
        X_3' & =A^{-1}X_3=-0.28A-1.93B+X_3 \\
        X_4' & =A^{-1}X_4=.21A-0.48B+X_4
    \end{split}
\end{equation*}
\paragraph{Transformed variables} Figure~\ref{fig:orthog proj heatmap} shows the correlations between variables before and after transforming the $X^{(i)}$0. We can see that the correlations between A (respectively B) and the $X{(i)}'$ have been reduced to zero, which was the goal of the procedure. Most correlations between other variables are close to before and after the change of basis, keeping the same signs and orders of magnitude. The most noticeable difference is that $corr(X^{(1)},X^{(3)})$ went from -0.019 to -0.12.

% Figure environment removed

% Figure environment removed

\paragraph{Prediction} We then apply the baseline model to predict the output, using only the transformed non-sensitive variables as explanatory variables.

\begin{table}[H]
    \centering
    \begin{tabular}{c|c|c|c}
        \multicolumn{4}{c}{Without protected variables} \\
        Variable & Coefficient & Standard Error & P-value \\ \hline
        Intercept & $-1.76\pm0.01$ & $0.05\pm0.00$ & $0.00\pm0.00$ \\
        $X_1$ & $0.65\pm0.00$ & $0.02\pm0.00$ & $0.00\pm0.00$ \\
        $X_2$ & $2.68\pm0.01$ & $0.05\pm0.00$ & $0.00\pm0.00$ \\
        $X_3$ & $2.46\pm0.00$ & $0.01\pm0.00$ & $0.00\pm0.00$ \\
        $X_4$ & $-2.41\pm0.01$ & $0.03\pm0.00$ & $0.00\pm0.00$ \\
        \multicolumn{4}{c}{} \\
        \multicolumn{4}{c}{Transformed variables} \\
        Variable & Coefficient & Standard Error & P-value \\ \hline
        Intercept & $-1.65\pm0.00$ & $0.01\pm0.00$ & $0.00\pm0.00$ \\
        $X_1$ & $0.55\pm0.00$ & $0.02\pm0.00$ & $0.00\pm0.00$ \\
        $X_2$ & $2.72\pm0.01$ & $0.05\pm0.00$ & $0.00\pm0.00$ \\
        $X_3$ & $0.24\pm0.00$ & $0.01\pm0.00$ & $0.00\pm0.00$ \\
        $X_4$ & $-2.46\pm0.01$ & $0.03\pm0.00$ & $0.00\pm0.00$
    \end{tabular}
    \caption{Weights of the logistic regression}
    \label{tab:weights_reg proj}
\end{table}

\paragraph{Performance evaluation}
\begin{itemize}
    \item Table~\ref{tab:metrics basis change} gives the accuracy of the model. Compared to when simply deleting the protected variables, the accuracy decreases by only 0.23 points.
    \item Figure~\ref{fig:ROC_sim_changebasis} gives the ROC curve of the model. As a reminder, the AUC of the model without protected variables was of 0.7466. The AUC for the model with transformed variables has only decreased by 0.0028.
\end{itemize}

\begin{table}[H]
    \centering
    \begin{tabular}{c|c|c}
         (\%) & Without protected variables & With transformed variables \\ \hline
        Accuracy & $81.04\pm0.05$ & $80.81\pm0.05$
    \end{tabular}
    \caption{Global metrics}
    \label{tab:metrics basis change}
\end{table}

% Figure environment removed

\paragraph{Fairness evaluation}
\begin{itemize}
    \item Acceptance rate \begin{itemize}
        \item The global acceptance rate has increased by 0.73 points on average, meaning that globally, more individuals get predicted the outcome $Y=0$ with the model using the transformed variables compared to the model without protected variables.
        \item For groups A: the difference in acceptance rates is now close to zero, which was the goal of the change of basis method. The reason why it is not exactly zero might come from the fact that we have approximated independence to correlation, so there might be some non linear dependence left between the transformed non-sensitive variables and A. It is interesting to note that the acceptance rate for group $A=0$ has decreased and for $A=1$ it has increased, and the advantageous position has shifted: the acceptance rate for group $A=0$ is now slightly lower than the one for group $A=1$0.
        \item For groups B: the difference in acceptance rates is now null, meaning that there we have $X^{(i)}\perp A$ ie independence.
        \item Looking at protected subgroups in figure~\ref{tab:fairness metrics subgroups change basis}, not all subgroups are treated fairly by the model as we have slight gaps between subgroups with $A=0$ and $A=1$0. The order of unfairness has also changed: now, the most disadvantaged subgroup is $(A=0,B=0)$ when it used to be the most advantaged one, and the most advantaged subgroup is $(A=1,B=1)$ when it used to be the most disadvantaged one.
        \end{itemize}
    \item True positive rate \begin{itemize}
        \item The global true positive rate has decreased compared to the model without protected variables.
        \item Groups A: as for the acceptance rate, the difference in true positive rates is now closer to zero, and the sign has changed, meaning that group $A=0$ now has a lower true positive rate than group $A=1$0.
        \item Groups B: the difference in true positive rates is now very close to zero and has also changed signs.
        \item As for the acceptance rate, the most advantaged subgroup now used to be the most disadvantaged and vice versa.
        \end{itemize}
    \item False positive rate \begin{itemize}
        \item Globally, the false positive rate has also decreased.
        \item Groups A: the difference in false positive rates has also decreased and changed signs.
        \item Groups B: surprisingly, the difference in false positive rates has increased and changed signs.
        \item Looking at subgroups, there has also been a shift in which subgroup is the most and least advantaged.
        \end{itemize}
\end{itemize}
To conclude, looking at the variable A, we have almost reached statistical parity, and for B, we have. We are closer to equal opportunity, although there was a shift in the advantage. For equalized odds, we are closer to fairness when looking at variable A but not B, and we have for both a shift in the advantage. All in all, the change of basis method has achieved the removal of linear dependence between the protected and non protected variables. We can also draw the conclusion that all three fairness definitions are not compatible.

\begin{table}[H]
    \centering
    \captionsetup{justification=centering}
    \makebox[\linewidth]{
    \begin{tabularx}{20cm}{c|c|ccc|ccc}
    % \begin{tabular}{c|c|ccc|ccc}
        \multicolumn{8}{c}{Without protected variables} \\
        (\%) & Global & A=0 & A=1 & Difference & B=0 & B=1 & Difference \\ \hline
        AR & $94.29\pm0.05$ & $97.05\pm0.04$ & $87.55\pm0.10$ & $9.50\pm0.09$ & $95.38\pm0.10$ & $94.07\pm0.055$ & $1.31\pm0.09$ \\
        TPR & $97.02\pm0.03$ & $98.51\pm0.03$ & $93.07\pm0.09$ & $5.44\pm0.08$ & $97.41\pm0.08$ & $96.97\pm0.04$ & $0.44\pm0.08$ \\
        FPR & $82.92\pm0.12$ & $89.94\pm0.13$ & $72.53\pm0.23$ & $17.41\pm0.26$ & $84.15\pm0.46$ & $82.82\pm0.13$ & $1.33\pm0.48$ \\
        \multicolumn{8}{c}{} \\
        \multicolumn{8}{c}{Transformed variables} \\
        (\%) & Global & A=0 & A=1 & Difference & B=0 & B=1 & Difference \\ \hline
        AR & $95.02\pm0.04$ & $94.91\pm0.05$ & $95.28\pm0.06$ & $-0.37\pm0.06$ & $95.02\pm0.11$ & $95.02\pm0.04$ & $0.00\pm0.01$ \\
        TPR & $97.38\pm0.03$ & $97.15\pm0.04$ & $97.97\pm0.05$ & $-0.82\pm0.05$ & $97.07\pm0.08$ & $97.41\pm0.03$ & $-0.34\pm0.08$ \\
        FPR & $85.58\pm0.13$ & $84.02\pm0.17$ & $87.92\pm0.17$ & $-3.90\pm0.21$ & $83.62\pm0.51$ & $85.74\pm0.13$ & $-2.13\pm0.50 $
    \end{tabularx}
    }
    % \end{tabular}
    \caption{Fairness metrics}
    \label{tab:metrics proj}
\end{table}

\begin{table}[H]
    \centering
    \begin{tabular}{c|cc|cc}
        \multicolumn{5}{c}{Without protected variables} \\
        \multirow{2}{*}{(\%)} & \multicolumn{2}{c|}{A=0} & \multicolumn{2}{c}{A=1} \\ \cline{2-5}
         & \multicolumn{1}{c|}{B=0} & B=1 & \multicolumn{1}{c|}{B=0} & B=1 \\ \hline
        AR & \multicolumn{1}{c|}{$97.43\pm0.08$} & $97.00\pm0.04$ & \multicolumn{1}{c|}{$88.94\pm0.27$} & $87.43\pm0.10$ \\
        TPR & \multicolumn{1}{c|}{$98.56\pm0.06$} & $98.50\pm0.03$ & \multicolumn{1}{c|}{$93.41\pm0.25$} & $93.04\pm0.09$ \\
        FPR & \multicolumn{1}{c|}{$90.01\pm0.44$} & $89.92\pm0.13$ & \multicolumn{1}{c|}{$72.61\pm0.87$} & $72.53\pm0.24$ \\
        \multicolumn{5}{c}{} \\
        \multicolumn{5}{c}{With transformed variables} \\
        \multirow{2}{*}{(\%)} & \multicolumn{2}{c|}{A=0} & \multicolumn{2}{c}{A=1} \\ \cline{2-5}
         & \multicolumn{1}{c|}{B=0} & B=1 & \multicolumn{1}{c|}{B=0} & B=1 \\ \hline
        AR & \multicolumn{1}{c|}{$94.87\pm0.11$} & $94.93\pm0.05$ & \multicolumn{1}{c|}{$95.18\pm0.23$} & $95.24\pm0.06$ \\
        TPR & \multicolumn{1}{c|}{$96.75\pm0.10$} & $97.20\pm0.04$ & \multicolumn{1}{c|}{$97.60\pm0.19$} & $97.99\pm0.04$ \\
        FPR & \multicolumn{1}{c|}{$82.62\pm0.52$} & $82.28\pm0.18$ & \multicolumn{1}{c|}{$86.34\pm0.69$} & $87.92\pm0.17$
    \end{tabular}
    \caption{Fairness metrics by protected subgroups}
    \label{tab:fairness metrics subgroups change basis}
\end{table}

% Figure environment removed

\subsection{Conclusion on the methods}

\paragraph{} We tested multiple preprocessing methods to mitigate unfairness. We compared the following: not doing anything, removing the protected variables, and transforming the non-sensitive variables in a way that they become uncorrelated to the sensitive ones. The goal of this last method was to approach fairness under the statistical parity, with equal acceptance rates for each protected group. \par
Comparing the model using all variables and the one using only non-protected variables, we can conclude that simply removing protected variables is not enough to reach fairness, under any definition. Depending on the correlation (and dependence) structure of the data, fairness can improve, with closer values of metrics for the different protected groups, as we saw is the case when looking at the B variable. But it can also magnify unfairness as we saw is the case when looking at the A variable. All in all, it is not a solution. \par
With our change of basis method, we ensure that the variables that will be used by the model are uncorrelated with the protected variables. Of course, uncorrelatedness is only an approximation of independence and there could still be non-linear relationships between the protected and transformed variables, which is why acceptance rates are only approximately equal for protected groups. \par
With this method, we have a slight decrease in accuracy, because we have no access to the information about the protected attributes, which were correlated with the outcome. \par
Now that we have studied a simulated dataset in which the correlation structure was known, we will take on a real dataset.



\newpage
