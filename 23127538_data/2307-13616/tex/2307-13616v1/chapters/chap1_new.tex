

\section{Motivation and scope of this paper}

\paragraph{} In many sectors, Machine Learning models have been exposed as unintentionally discriminatory, leading to unfair decisions that can have drastic consequences. The insurance industry has always been under close scrutiny when it comes to the use of personal data and discrimination issues, but in the light of recent denunciations in all industries, the attention on fairness issues has grown. \par
Fairness is a complex philosophical matter, and there is no single definition for it. Researchers have managed to define it statistically, but the definition depends on underlying assumptions and ideologies. Furthermore, as of today, many methods have been proposed to come closer to some definitions of fairness, but none can ensure perfect fairness. \par
Regarding previous points, we wonder to what extent fairness can be approximated when applying a Machine Learning model to insurance mortality data, and more specifically to what extent proxy discrimination can be avoided. This thesis aims at illustrating the complexity of quantifying discrimination and finding a way to mitigate it. For this purpose, we propose a simple and promising method that relies on linear algebra to mitigate indirect discrimination. \par
All along the thesis, we have provided the reader with summaries and key points in the form of light blue frames. \par

% - Un préambule qui amène le sujet et le replace dans son contexte général, par exemple, en l’illustrant sur quelques documents de référence, en indiquant les principaux résultats et modèles explicatifs déjà énoncés dans ce domaine ou bien encore, en faisant le point sur l’avancement des recherches dans le domaine ;
% - L’énoncé de la problématique actuarielle pour aboutir à la question centrale des travaux réalisés ;
% - Les éléments différenciant du travail réalisé en insistant, par exemple, sur l’originalité, l’intérêt de la recherche, les objectifs poursuivis.
% L’introduction doit poser clairement le sujet et la méthodologie qui a été suivie durant le travail, elle doit exposer la problématique et amener une présentation des références bibliographiques clé et en quoi elles ont contribué à la réflexion dans le travail.
% L’introduction devrait représenter 2 à 3 pages.

\newpage

\section{Understanding key concepts and regulations around discrimination}
\subsection{The life insurance industry}

\paragraph{} Historically, life insurers have used classic statistical models to assess risks for their products, covering Mortality, Critical Illness\footnote{Critical illness insurance compensates insureds with a lump sum payment upon diagnosis in order to cover treatment costs \cite{scor22}.}, Disability, Longevity and Medical Expenses. But with the development of Machine Learning and the now richer than ever data sources, new techniques are becoming more and more popular. These methods imply new challenges for actuaries, mostly due to the richness of information and the complexity of algorithms. \par

\subsubsection{Actuarial fairness}

\paragraph{} One of the challenges concerns the notion of equity, which has always been a key issue for insurers. `Actuarial fairness' means that risky insureds should contribute more and pay a higher premium. Actuaries have to determine how to classify policyholders between risky and non-risky, and more specifically what attributes are good indicators of risk. They then rely on historical data to estimate losses \cite{grari22}. It is sometimes complicated to know if an attribute is directly related to risk or not. As we will see later on, there are country-specific regulations concerning the use of certain attributes for risk assessment. \par

\subsubsection{Segmentation, pooling and adverse selection}
\label{segmentation}
\paragraph{} Before insurance, the only way of hedging against risk was individual prudence. Pooling offered a new way to deal with uncertainty: losses were the collective responsibility of the pool. This created insurance solidarity with an understanding of fairness \cite{frees21}. Today, insurers offer contracts at large levels, counting on the compensation between policyholders who file claims and those who do not. In order to ask for premiums accordingly to a policyholder's risk profile, insurers do a segmentation of the insureds \cite{charpentier22}. Segmentation consists in creating homogeneous classes and estimating the risk on average \cite{barry22}, so that premiums are adapted to the risk profile. \par
The pure premium is the expected loss of the insured over the coverage period. Since pooling is based on the law of large numbers, risks have to be homogeneous, which is why insurers need to classify the risks properly. The classification is based on observable factors, which should indicate what the risk is \cite{grari22}. \par
If groups are heterogeneous, policyholders could cross-subsidy. This leads to adverse selection: lower risks are asked to pay more than their expected loss because they are not classified in the right pool, so they are attracted by a competitor who will offer lower prices. \par

% Figure environment removed


% \subsubsection{Risk modeling}


% \subsubsection{Insurance coverage}
% \paragraph{} Individuals with cancer history are considered `aggravated risks' and are often offered coverage with deterrent premiums, based on limited and imprecise criteria. But each cancer is specific and the evolution of treatment possibilities has tremendously increased survival odds in the past few years. At SCOR, the medical underwriting team is in charge of providing inclusive solutions for cancer patients. By using all available information about individuals, we can precisely estimate mortality rates thanks to Machine Learning models. These precise rates can help insurers offer fair premiums to individuals with a history of cancer. \par
% The main concerned insurance products are critical illness\footnote{Critical illness insurance compensates insureds with a lump sum payment upon diagnosis in order to cover treatment costs \cite{scor22}.} and term life products. The R\&D department studies the rates linked to the covered condition so that an insurance product can be put on the market, then the rates are used by the pricing team to price the product correctly. It can be an add-on cover or a product in itself. \par

\subsubsection{Risk modeling}

\paragraph{} Life insurers model biometric risks, which relate to human life conditions, and more specifically the duration until the occurrence of an event. In our case, we will be studying the mortality of cancer patients. \par
Individuals with cancer history are considered `aggravated risks' and are often offered coverage with deterrent premiums, based on limited and imprecise criteria. But each cancer is specific and the evolution of treatment possibilities has tremendously increased survival odds in the past few years. At SCOR, the medical underwriting team is in charge of providing inclusive solutions for cancer patients. By using all available information about individuals, we can precisely estimate mortality rates thanks to Machine Learning models. These precise rates help insurers offer fair premiums to individuals with a history of cancer. \par
For cancer patients, insurers model the duration before death in the context of mortality or longevity products. Information about these individuals can also be used for critical illness products, to model the duration before the occurrence of a cancer. Once the R\&D department has studied the rates linked to the covered condition, the pricing team takes over to put the add-on cover or product on the market. The underwriters can also benefit from the study to better assess risk profiles. \par

\paragraph{}  To do this modeling, we need to take into account a few constraints: \label{constraints}
\begin{itemize}
    \item Underwriting constraints: the variables used by the model need to be available to the underwriter, i.e. be included in the medical file of the individual. Depending on the local legislation, models must not be discriminatory against certain population, so as a `solution', some variables are simply deleted. The insurer also has business constraints as he is in a competitive market.
    \item Medical constraints: the variables and the coefficients they are attributed need to be coherent with medical literature. For example, variables that are not medically relevant, such as the address, cannot be used by the model. Another example of variable coherence is if a larger tumor size implies a shorter life expectancy, it must be reflected by the model.
    \item Modeling constraints: the variables must be statistically relevant, not too numerous nor strongly correlated with each other.
\end{itemize}

% Figure environment removed

\newpage

\subsection{Fairness and bias}
\paragraph{} As we saw with the constraints of variable selection, depending on the local regulation, selected variables must not lead to discriminatory results. Fairness and bias is not a new subject in Machine Learning, but it has received increasing attention these past years, as numerous examples of unfair and biased outcomes were revealed in various fields. The most famous one is the COMPAS Recidivism Algorithm, which was proved biased against black defendants \cite{propublica}. As actuaries use high dimensional data and complex models, to price contracts for example, they need to check that the outcomes are not biased.  We need to define fairness and bias, see how it impacts insurance and find a method to detect it.

\subsubsection{The need for fairness in insurance}
\paragraph*{Reputation}
Fairness is a key issue in insurance, because actuaries need to explain to underwriters how their models work, so that in turn policy applicants can understand and trust the process to be fair. Insurance is not a well-seen sector in the public opinion and by the authorities in general, which is why fairness and transparency are crucial subjects.

\paragraph*{Regulation}
In the EU, the GDPR (General Data Protection Regulation) \label{gdpr} gives individuals the right to control their personal data, and specifically `the right not to be subject to a decision based solely on automated processing, including profiling, which produces legal effects concerning him or her or similarly significantly affects him or her' \cite{gdpr_article_22}. This means that a human intervention is required in any automated decision process. Individuals also have a right to erasure, or `right to be forgotten' \cite{gdpr_article_17}, which means that datasets might not be complete, independently of the collection process. \par
But there are specific regulations for the insurance sector. The French Supervisory Authority, the ACPR, requires appropriate data management, with ethical considerations such as fairness of processing and absence of discriminatory bias \cite{acpr_govAI}. \par
In April 2021, the EU proposed the first-ever legal framework on Artificial Intelligence (AI), the AI Act. The proposal is to define four levels of risk, from unacceptable to none, concerning AI systems. Each category will be subject to requirements and specific obligations such as conformity assessments and registration in a database. It could become applicable as soon as the second half of 2024 \cite{eu_ai_framework}. Insurers do not know how they will be impacted by this new regulation, but if their AI systems are classified as risky, they will be under strict regulation. \par
All in all, there is a regulation stacking that needs to be understood. The real question is: which regulation will be predominant?

% Figure environment removed

\subsubsection{A multitude of points of view}
\paragraph{}
Fairness is a relative notion that has many different definitions. A. Narayanan gave 21 definitions for fairness in classification tasks \cite{narayanan_yt}. It is a legal requirement, but also an ethical concept \cite{charpentier22}. Definitions can be categorized into individual or group fairness. Individual fairness aims to treat similar individuals similarly and group fairness treats different groups equally \cite{mehrabi19}. But group fairness might appear unfair at the individual level and a generalization based on group membership may be wrong \cite{binns19}. There are therefore two opposite worldviews regarding fairness with respect to a specific task: We're All Equal (WAE), meaning all groups have similar abilities, and What You See Is What You Get (WYSIWYG), meaning observations reflect similar abilities \cite{AIF360}. From this, many statistical definitions have been created to measure fairness for model outcomes, mostly for classification. We will study some of them in detail later on. \par
Machine Learning is a statistical discrimination by nature, but it becomes objectionable when there is a systematic advantage for some privileged group. All definitions of fairness cannot be reconciled and cannot satisfy all aspects. There needs to be a legal study to provide an official definition of fairness and an associated metric. \par

% Figure environment removed

\subsubsection{The different types of bias}
\paragraph{} Statistical fairness is strongly related to bias, which can be defined differently depending on the sector. In statistics, it is a systematic error in prediction outcomes \cite{AIF360}. For Machine Learning models, it can be introduced by users, come from the data or be amplified by algorithms. And it is a vicious circle, as algorithms learning from biased data give biased outcomes which will be fed into and amplified by future algorithms \cite{mehrabi19}. \par
There are three types of biases that appear in classification problems \cite{barry22}:
\begin{itemize}
    \item Type 1: classes do not reflect the reality of the risk,
    \item Type 2: classes reflect a correlation with risk that is non-causal,
    \item Type 3: classes reflect a causal statistical reality, but are unacceptable because of ethical reasons.
\end{itemize}

% Figure environment removed

\subsubsection{Unfair and fair discrimination}
\paragraph{} To better understand and scope the concept from a scientific point of view, we discussed with different lawyers specialized in data law. Those exchanges helped us focus on the different key concepts exposed below.

\paragraph{Legal definition} Discrimination is the difference in treatment between individuals in similar situations due to prohibited criteria. In France, the Penal Code \cite{penal_code} defines these criteria in the Article 225-1: \label{anti-discrim_law} \begin{quote} `their origin, their sex, their family situation, their pregnancy, their physical appearance, the particular vulnerability resulting from their economic situation, apparent or known to its author, their surname, their place of residence, their state of health, their loss of autonomy, their disability, their genetic characteristics, their morals, their sexual orientation, their gender identity, their age, their political opinions, their trade union activities, their ability to express themselves in a language other than French, their membership or non-membership, real or supposed, of an ethnic group, a nation, a so-called race or a determined religion' \end{quote} These general prohibitions to fight against discrimination are supplemented by the French Insurance Code, and the Article L117-2 states that there can be absolutely no distinction between individuals based on:
\begin{itemize}
    \item age for access to insurance guarantees or termination of insurance benefits, with the exception of pricing for life insurance contracts with mortality tables;
    \item pregnancy and motherhood for premium and benefit computation;
    \item sex for premium and benefit computation, except for mandatory supplementary pension schemes.
\end{itemize}
The Article 16-13 of the Civil Code \cite{civil_code} defines an absolutely prohibited criteria that is applicable to insurance: \begin{quote} `No one may be discriminated against because of their genetic characteristics.' \end{quote} \par
Other criteria can be used if they are a justified business necessity, or to modulate premiums and guarantees.\par

\paragraph{The nature of insurance} As we saw in section~\ref{segmentation}, insurance is based on segmentation and pooling. It is about treating different risks differently. Classification and regression tasks are by definition a form of discrimination: the aim is to distinguish individuals based on a statistical similarity \cite{barocas16}. This is a form of discrimination that is justified and deemed acceptable if it does not systematically put a protected group at a disadvantage. \par

\paragraph{Unfair discrimination} Discrimination is unfair when a certain group is treated unequally based solely on their affiliation to it \cite{kamiran11}. Direct discrimination happens when protected attributes are explicitly used to make the decision. Indirect discrimination happens when the treatment appears neutral and depends on non-protected attributes, but protected groups get treated unjustly \cite{mehrabi19}. Harmful discrimination in insurance can happen at several stages: for the decision to insure, during the underwriting or marketing phases, to renew or cancel policies, for coverage offer or for pricing \cite{frees21}.

% Figure environment removed

\subsubsection{Interpretability: a first step to tackling bias}
\paragraph{} Interpretability is the degree to which a human can understand the cause of a decision. Some models are directly interpretable and others, qualified as `black box' models, need interpretability methods. As the need to explain Machine Learning models become more and more important, a new research field XAI, for eXplainable Artificial Intelligence, was created. Understanding why a model predicted specific outcomes is a first step in bias detection \cite{molnar}.

\paragraph{Interpretable models} Some models are directly interpretable because of their structure. Linear Regressions predict the target as a weighted sum of the feature inputs, but assume linear relationships. Generalized Additive Models take into account non linear effects. Decision trees and decision rules are easy to interpret and capture feature interaction. \par

\paragraph{Model-agnostic explanation methods} Global methods explain how features affect the prediction on average. Techniques include feature interaction detection, prediction function decomposition, feature importance measure and representative data points choice. \par
Local methods explain the individual predictions. Techniques include the description of how changing a feature changes the prediction, of which features anchor a prediction, of which features would need to be changed to change a prediction and of which  individual features are attributed to a prediction.

\paragraph{Detecting bias} Having more information on which variables play an important role in predictions, which variables interact with each other and more generally what causes a prediction can help detect bias. The outcome might be biased if a protected attribute has a great importance in the prediction or if an important variable for the prediction has a strong correlation with a protected attribute. This can help find out where the bias actually comes from.

% Figure environment removed

\newpage

\subsection{The expression of fairness and bias in data}
%données pf vs monde -> filtre à la souscription
\paragraph{} Feature selection is the process of choosing what attributes are observed and taken into account for analysis. They are necessarily a reduction of reality and fail to capture real-world phenomena \cite{barocas16}. But obtaining sufficiently rich information is not always possible, and insurers have to rely on reductive data.

% Figure environment removed

\subsubsection{Sensitive variables}
\paragraph{} Insurers are not allowed to use all variables that are available to them, because they are viewed as protected or sensitive, and using them might lead to biased outcomes. The choice of which variables are allowed depends on regulators, but also on society as a whole, because it is both a legal and ethical concern. Often, attributes that are not under the control of individuals are not accepted. Attributes that change over time are accepted, because individuals are on both sides at different stages in their life. Acceptable variables should be good predictors of risk.  If attributes are known to cause a risky event they are accepted \cite{frees21}.

\paragraph{\underline{Regulation}}

%Anti-discrimination legislation: Australian Sex Discrimination Act 1984, US Equal Pay Act 1963, US Equal Credit Opportunity Act 1974, European Council Directive 2004 (auto insurance companies can't discriminate based on gender) [24]
%US 80\% rule, disparate impact \\ [53]
%[54] disparate impact for ins rates [94]

\paragraph{EU} The European Union has anti-discrimination legislation, in the Charter of Fundamental Rights, that applies to the insurance sector, but it can have the effect of restricting flexibility of risk management and raising costs and legal insecurity, which means that insurers offer less effective coverage. Lawmakers decide which factors are determining for risk assessment but they are not specialists. With these directives, insurers cannot perfectly prevent adverse selection and moral hazard, and as a consequence, consumers can end up penalized, paying higher premiums and deductibles \cite{petkantchin10}.

\paragraph{France} There is an evolving conflict between insurance and anti-discrimination standards: on the one hand, insurers classify risks and on the other, the law prohibits the differentiation of individuals based on criteria that deny equal dignity. Insurers can select risks as long as they demonstrate the objectivity and statistical foundations of the data they rely on to do so. The Insurance Code completely forbids discrimination based on pregnancy and motherhood, risk selection for supplementary pension and the use of genetic information \cite{robineau10}. In 2011, the EU court ruled that insurers offering different prices to men and women violated gender equality laws, and this affected car, term life, health insurance and annuities \cite{test-achats}. This means that there is no possible discrimination on these criteria, even though there are some technical and pragmatic arguments for using them.

\paragraph{US} Recently, unfair discrimination has become closely connected to disparate impact (see section~\ref{DI}). It is a measure of how a practice affects a group more than another, even when it appears neutral. But as of 2009, no court had applied the disparate impact standard to evaluate insurance rates \cite{miller09}. Unfair discrimination in insurance is indeed not exactly equivalent to disparate impact. In State law, unfair discrimination happens when similar risks are treated differently for determining rates, coverage, benefits and terms and conditions of policies. Depending on States, certain factors are prohibited for risk classifications and in underwriting decisions. But laws are not as restrictive as one could believe: in fifteen States, it is only prohibited to use race as the only factor regarding a decision to issue or continue a policy and in four States there are no restrictions on the use of race for underwriting personal automobile insurance \cite{stead20}. \par
In July 2021, the governor of Colorado signed a Senate Bill on insurers' use of external consumer data \cite{senatebill}. Insurers in the State are prohibited from unfairly discriminating on several variables. Unfair discrimination is defined as including \begin{quote} `the use of one or more external consumer data and information sources, as well as algorithms or predictive models using external consumer data and information sources, that have a correlation to race, color, national or ethnic origin, religion, sex, sexual orientation, disability, gender identity, or gender expression, and that use results in a \textbf{disproportionately negative outcome} for such classification or classifications, which negative outcome exceeds the reasonable correlation to the underlying insurance practice, including losses and costs for underwriting.' \end{quote} This means that insurers using external consumer data in Colorado will need to provide a disparate impact analysis \cite{krafcheck21}, even though it is not a synonym for unfair discrimination. The following question is: will other States follow in the same direction as Colorado? \par

\paragraph{\underline{What are they?}}
\label{protected_variables}
\paragraph{Forbidden variables} in France are pregnancy, motherhood and genetic information in all processes. For car, term life, health insurance and annuities, gender is a forbidden variable. In the US, there are some rules that come from federal law: insurers cannot consider pre-existing health conditions or gender in the underwriting process, genetic information in coverage availability or premium charging, or housing practices that have a disparate impact on protected classes. There are no other federal laws regulating what criteria can be taken into account. Historically, States are responsible for regulating insurance discrimination. These regulations strongly depend on which State and insurance line are in question: nine States completely prohibit the use of race and national origin in all lines, 7 States religion, one State gender and five States sexual orientation. Louisiana explicitly allows the use of race for life insurance. No State completely bans the use of age, credit score, genetic testing or ZIP Code \cite{avraham13}.

\paragraph{Sensitive variables} in France are those defined in the anti-discrimination law that are not forbidden, as we saw in section~\ref{anti-discrim_law}. They can only be used if statistical data proves their relevance and objectivity for risk analysis. Information on ethnicity, religion, sex, gender, sexual orientation, disability, and age can be viewed as sensitive. Less obvious sensitive variables include parenthood, military service, political party, socioeconomic status, or involvement in the criminal justice system. \\

% Figure environment removed

\subsubsection{Proxy variables}
\paragraph{} Proxies are unprotected variables that are strongly correlated to protected variables but also contain strong predictive information. Using proxy variables may result in indirect discrimination \cite{barocas16}. Insurers seek good segmentation of risk and profit maximization, but this might lead to perpetuating inequalities in society if outcomes are biased because of the variables that are used for prediction.

\paragraph{Name and surname} Accuracy of guessing the national origin of names varies significantly by the individual perception of national origin. Numerous characteristics matter, such as gender, popularity and the average level of educations of mothers who gave that name \cite{gaddis17}. A name depends on the culture of group, trends, the social level and time. With a name, you could identify the sex and the national origin of a person. You could also estimate the average age of people who have that name based on trends and popularity, but it would be difficult to infer the exact age \cite{charpentier22}.

\paragraph{Address} In the US, Black and Hispanic segregation and spacial isolation is still very active in some metropolitan areas \cite{rugh14}. This means that addresses are good proxies for ethnicity, and classifiers using this variable will exhibit discriminatory behavior \cite{kamiran19a}. The term redlining comes from the 1930s when residential security maps were created to indicate which parts f a city were safe to invest in: neighborhoods outlined in red were the riskiest \cite{martin17}. From the 1960s, in the context of the struggle for Black Civil Rights, the use of redlining for risk classification was strongly criticized. The address is a non-causal variable, but it is strongly correlated with non-observable risk factors and with ethnicity \cite{barry22}. \par
There is a strong correlation between income and ethnicity and between income inequality and income segregation in the US. This is partly due to housing discrimination after World War II, forcing Black families with lower incomes to live in proximity in urban areas \cite{reardon11}. Addresses are therefore strongly correlated with income, which is strongly correlated with ethnicity. \par
Night lighting and wealth are correlated, and it is possible to approximately estimate how wealthy a neighborhood is from satellite imagery with a strong predictive power \cite{jean16}. Address (and its associated satellite images) is a proxy for wealth. \par
With Google Street View, it is also possible to use the number and type of cars to infer wealth, ethnicity, education level and political preferences. It is also easy to detect the presence of handicap access ramps \cite{hara14} or a flag indicating national origins, political preferences or sexual orientations \cite{charpentier22}. \par
 
\paragraph{Occupation} Despite efforts for parity in the workplace, numerous occupations are still dominated by one of the two genders. In 2016 in a representative French region, sectors such as social working, healthcare and teaching are mostly feminine and industry, construction and transportation are mostly masculine. Another visible trend is that very feminine sectors tend to become even more so \cite{faure20}.\par
In 2018 in France, 18\% of employees worked part-time, 78\% of which were women \cite{dares20}. Knowing an individual works part-time means it is three times more likely that it is a woman.\par

\paragraph{Credit score} This variable cannot be measured directly, as it comes from the problem definition of creditworthiness. It is a non-arbitrary definition, not a given. The definition process can already itself be biased \cite{barocas16}, as it relies on measurable attributes that are available. The choice of which variables to use can introduce discrimination. Credit scores also create a vicious circle in terms of poverty, and using this variable introduces a disparate impact on racial minorities and low-income households, who then have to pay a higher premium \cite{charpentier22}.

\paragraph{Face} With the boom of facial recognition softwares, there are numerous opportunities of application. It is now possible to use facial recognition tools for health assessment by using measurements and proportions of facial attributes. They may be considered biometric data, so there are ethical issues surrounding their use \cite{boczar21}. Facial recognition can accurately predict gender and ethnicity, which raises moral questions. \par

\paragraph{Speech} The way an individual talks can indicate origin if he or she has difficulties with pronunciation, has a strong accent or speaks in a dialect. Linguistic profiling is the identification of an individual's ethnicity based on how their voice sounds and using that information for discrimination \cite{squires06}.\par
Current Natural Language Processing tools are trained on traditional written sources, which are different from spoken language, and even more from dialectal spoken language. The latter are more likely to be incorrectly classified, so bias can arise, with an incorrect representation of ideas and opinions from minority groups \cite{blodgett17}.\par
Chatbots are rule-based, information retrieval or learning-based systems that are widely used today. In March 2016, a Microsoft chatbot was supposed to improve its small-talk capabilities by learning from conversations with human users. In less than a day, it was displaying racist and sexist abusive content \cite{schlesinger18}. This shows that technical difficulties must be tackled in order to avoid such outcomes, especially when black box algorithms are used for treating voices.\par
In insurance, writing or speaking chatbots are used to report insurance claims. For example, Izzy Constat is a tool for amicable reports after a car accident. The chatbot asks for drivers' identities and context and generates a sketch representing the incident \cite{calvo22}. What if this chatbot were biased against a group that had a specific dialect? This could result in understating the severity of impact and lower benefits.\par

\paragraph{Network} Who you know either gets you access to resources or makes you guilty by association. Recommendation systems are based on similarity between individuals, and if insurers had access to this kind of information, they could find customers and limit their financial risks. But this kind of practice may not be ethical, as people who are already marginalized can be even more affected \cite{boyd14}. 

%[78] ins accused of discrim (minorities higher rates -> solution regulation or competition)

\paragraph{} There are many other variables with more or less predictive power that could be used as proxies for protected attributes. We will not be able to list them all, but the conclusion is that a study of their meaning and how they are linked to protected attributes is essential before using them as inputs in a prediction model.

% Figure environment removed

%\subsection{Privacy}

% \newpage
% \section{Debiasing methods}
% %fusionner avec section suivante?

% %[48] doing nothing: Minimizing avg error fits majority populations
% % creating a model for each class but if multiple levels protected groups
% \subsection{Pre-processing}

% \subsubsection{Removing sensitive attributes: not such a good idea} % = fairness through unawarenss [79]
% When sensitive variables are omitted, models can still learn stereotypes, because sensitive information is embedded in datasets even if it is not intentional. Leaving out sensitive variables forces the correlated variables to take on a greater importance. This is the omitted variable bias \cite{williams18}. \par
% Removing sensitive attributes is problematic, because it becomes impossible to check for bias and discrimination. We cannot see if the most important variables for prediction are strongly correlated with a protected attribute. This is a problem related to data regulations: as we saw in section~\ref{gdpr}, the GDPR requires minimal data collection, but more data is needed to prove discrimination. \par

% %[24] massaging, reweighting, sampling

% %[79] fairness under unawareness: find protected variable with proxy for discrim checks

\newpage

\subsection{How to measure fairness}
\label{fairnessmetrics}
\paragraph{} Numerous metrics have been created to measure bias and fairness, the two notions not being distinct in most works.
The general setting is the following:
\begin{itemize}
    \item $X \in \mathcal{X}\subset\mathbb{R}^{n}$ is the set of n non-protected variables,
    \item $A \in \mathcal{A}$ is the protected or sensitive variable,
    \item $Y \in \mathcal{Y}$ is the true outcome,
    \item $f$ is the predictor, classifier or regressor,
    \item $\hat{Y}=f(X,A) \in \mathcal{Y}$ is the predicted outcome.
\end{itemize}

\subsubsection{Binary classification}
\label{binary_classif}
\paragraph{} In the binary classification setting, $\mathcal{Y} = \{y^-,y^+\}$0. $Y=y^-$ means the outcome is negative and $Y=y^+$ means the outcome is positive. In this problem, it is possible to compute the confusion matrix between the true and predicted outcomes for each protected group, see table \ref{table:confusion_matrix}.

\begin{table}[H]
    \centering
    \begin{tabular}{l|l|c|c|}
        \multicolumn{2}{c}{} & \multicolumn{2}{c}{Predicted outcome} \\
        \cline{3-4}
        \multicolumn{2}{c|}{} & Positive & Negative \\
        \cline{2-4}
        \multirow{2}{*}{True outcome} & Positive & $TP$ & $FN$ \\
        \cline{2-4}
        & Negative & $FP$ & $TN$ \\
        \cline{2-4}
    \end{tabular}
    \captionsetup{justification=centering}
    \caption{Confusion matrix \\ TP: True Positive, FN: False Negative, FP: False Positive, TN: True Negative}
    \label{table:confusion_matrix}
\end{table}

The most common metrics in the literature are the following \cite{alves22}. \par

\paragraph{Statistical parity}\label{stat_parity} (or demographic parity) requires the likelihood of a positive outcome to be the same for all protected groups i.e. $\hat{Y} \indep A$:
\begin{equation*}
    \forall a \in \mathcal{A}, \mathbb{P}(\hat{Y}=y^+|A=a)=p
\end{equation*}
This is equivalent to having the same predicted acceptance rates AR for all protected groups:
\begin{equation*}
    \mbox{AR}=\frac{\mbox{TP}+\mbox{FP}}{\mbox{TP}+\mbox{TN}+\mbox{FP}+\mbox{FN}}
\end{equation*}

\paragraph{Equalized odds} requires all protected groups to have the same probabilities of being correctly assigned a positive outcome and of being incorrectly assigned a positive outcome i.e. to have the same true and false positive rates i.e. $\hat{Y} \indep A | Y$:
\begin{equation*}
    \forall (y,a) \in \mathcal{Y}\times\mathcal{A}, \mathbb{P}(\hat{Y}=y^+|Y=y,A=a)=p
\end{equation*}
This is equivalent to having the same true positive rates TPR and false positive rates FPR for all protected groups:
\begin{equation*}
    \begin{split}
        \mbox{TPR} & =\frac{\mbox{TP}}{\mbox{TP}+\mbox{FN}} \\
        \mbox{FPR} & =\frac{\mbox{FP}}{\mbox{FP}+\mbox{TN}}
    \end{split}
\end{equation*}
Remark: the true positive rate is also called recall or sensitivity.

\paragraph{Equal opportunity} is the same as equalized odds but only requires all protected groups to have the same probability of being correctly assigned a positive outcome i.e. requires the same true positive rates TPR for all protected groups:
\begin{equation*}
    \forall a \in \mathcal{A}, \mathbb{P}(\hat{Y}=y^+|Y=y^+,A=a)=p
\end{equation*}


\paragraph{Disparate Impact} \label{DI} is a popular metric in the US to measure bias. It is defined as the ratio in probability of favorable outcomes between groups $A=0$ and $A=1$ \cite{AIF360}:
\begin{equation*}
    \mbox{DI} = \frac{\mathbb{P}(\hat{Y}=1|A=0)}{\mathbb{P}(\hat{Y}=1|A=1)}
\end{equation*}
It is a consequence of the statistical parity definition, in the case where the probabilities are non-null. A Disparate Impact of 1 would mean that the model is fair, lower than 1 that the model is unfair to group $A=0$ and above 1 that the model is unfair to group $A=1$0. This Disparate Impact is only defined in the case of a binary classification with a binary protected variable. Its estimation is not as easy as we could think, because of its definition as a ratio: we can have robust estimators for both probabilities, but the estimator of a ratio is not the ratio of estimators. This is why we decided not to use this fairness metric and to keep the original definition of statistical parity which is not as restrictive. \par

% Figure environment removed

% Figure environment removed

\subsubsection{Extension of the metrics to other settings}
\paragraph{} Most of these definitions can be extended to a multi-class classification ($\mathcal{Y}\subset\mathbb{N}$) or a regression ($\mathcal{Y}\subset\mathbb{R}$) setting: by defining a subset $\mathcal{Y}^+\subset\mathcal{Y}$ (respectively $\mathcal{Y}^-\subset\mathcal{Y}$) of values reflecting a positive (respectively negative) outcome, we adapt the definitions from the previous section:
\begin{equation*}
    \begin{split}
         & \mbox{Statistical parity: } \forall a \in \mathcal{A}, \mathbb{P}(\hat{Y}\in\mathcal{Y}^+|A=a)=p \\
         & \mbox{Equalized odds: } \forall a \in \mathcal{A}, \mathbb{P}(\hat{Y}\in\mathcal{Y}^+|Y\in\mathcal{Y}^+,A=a)=\mathbb{P}(\hat{Y}\in\mathcal{Y}^+|Y\in\mathcal{Y}^-,A=a)=p \\
         & \mbox{Equal opportunity: } \forall a \in \mathcal{A}, \mathbb{P}(\hat{Y}\in\mathcal{Y}^+|Y\in\mathcal{Y}^+,A=a)=p
    \end{split}
\end{equation*}
This supposes that we can categorize every value of output as either positive or negative.

%intervalles de confiance 80% rule cf charpentier


\subsubsection{Model evaluation}
\paragraph{} We will compare models using the confusion matrix and metrics deriving from it:
\begin{itemize}
    \item The accuracy is the proportion of correct classifications among all classifications.
    \begin{equation*}
        \mbox{Accuracy}=\frac{\mbox{TP}+\mbox{TN}}{\mbox{TP}+\mbox{TN}+\mbox{FP}+\mbox{FN}}
    \end{equation*}
    % \item The precision is the proportion of correct positive classifications among positive classifications.
    % \begin{equation*}
    %     \mbox{Precision}=\frac{\mbox{TP}}{\mbox{TP}+\mbox{FP}}
    % \end{equation*}
    \item The true positive rate is the proportion of correct positive classifications among actual positive values.
    \begin{equation*}
        \mbox{TPR}=\frac{\mbox{TP}}{\mbox{TP}+\mbox{FN}}
    \end{equation*}
    \item The false positive rate is the proportion of wrong positive classifications among actual negative values. It is sometimes called the probability of false alarm.
    \begin{equation*}
        \mbox{FPR}=\frac{\mbox{FP}}{\mbox{FP}+\mbox{TN}}
    \end{equation*}
    \item The acceptance rate is the proportion of positive classifications among all classifications.
    \begin{equation*}
        \mbox{Acceptance rate}=\frac{\mbox{TP}+\mbox{FP}}{\mbox{TP}+\mbox{TN}+\mbox{FP}+\mbox{FN}}
    \end{equation*}
\end{itemize}
In order to evaluate a model, we need to take into account the facts that:
\begin{itemize}
    \item An acceptable accuracy threshold depends on the context of the prediction: do we need to be perfectly accurate in order for the decision to be accepted by society? Do mistakes cost a lot to the company? Furthermore, if there is a large class imbalance, accuracy is not the best metric as it can be very high while the model only fits the majority population.
    
% Figure environment removed

    \item Missclassification errors are more or less acceptable depending on the context. For example, in a risk selection decision, underwriters want to select `good' risks. It is generally more acceptable and less costly to falsely reject good risks than falsely accept bad risks.
    
% Figure environment removed
\end{itemize}
