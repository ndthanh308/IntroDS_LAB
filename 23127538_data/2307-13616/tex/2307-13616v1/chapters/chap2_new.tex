\section{Simulated data}
\paragraph{} We will apply discrimination detection and correction methods to simulated data before tackling a real use case. The reason for this is that we wish to know the answer to the question `Is my prediction discriminatory towards a group?' while knowing how the outcome was computed, so that we can look for a solution to a well-defined problem, instead of having to make assumptions. \par
We will begin by generating our data, which consists of two sensitive variables $A$ and $B$, a set of non-protected variables $X = \{X_i\}_{i=3,...n}$, $n \in \mathbb{N}$ and a variable of interest $Y$0. All variables can be correlated with each other, but depending on the country and its regulation, insurers are not always allowed to use the sensitive variables as explanatory variables, and sometimes, with the GDPR for example, cannot even collect the information. In this section, however, we will suppose that the variable is available, because it is the only way to measure discrimination. \par
To link with a real-life example, if we were in a pricing context for automobile insurance, $A$ could represent gender, $B$ marital status, the $X_i$ other variables such as age or car value and $Y$ the claim occurrence. Gender is not the cause of an accident, but statistically we observe that gender and claim occurrence are correlated. Intuitively, we should not discriminate based on gender, because it would be unfair as it is a stereotype, but we do not have access to a `fairer' variable, which could be driving behavior. \par

% Figure environment removed

For simplification reasons, we will suppose that the protected variables and the outcome are binary. We will begin by giving the theoretical framework, then illustrate the simulation process with two variables, and finally create the dataset. \par

\subsection{A reminder on statistical tools used to set the framework}
\paragraph{} In this section, we will pose important mathematical concepts that will help understand the observed effects on the fairness metric. \par
We want to generate the variables while controlling the relationship they have with each other. To do so, we will use the theory of copulas. \par

% Figure environment removed

\vspace{5mm}
\noindent \underline{\textbf{Notations}} \par
\vspace{5mm}
\noindent First, we need to introduce some notations:
\begin{itemize}
    \item[-] $f$ is a probability density function and $F$ the associated cumulative distribution function: $F'=f$
    \item[-] $\mathbb{E}$ is the expected value and $Var$ the variance
    \item[-] $\Sigma$ is the covariance matrix: $\Sigma_{i,j}=cov(X_i,X_j)$
    \item[-] $R$ is the correlation matrix: $R_{i,j}=corr(X_i,X_j)$
    \item[-] $\phi$ is the probability density function and $\Phi$ the cumulative distribution function of a standard normal distribution $\mathcal{N}(0,1)$
    \item[-] $\phi_R$ is the probability density function and $\Phi_R$ the cumulative distribution function of a standard multivariate normal distribution $\mathcal{N}_n(0,R)$
\end{itemize}

\vspace{5mm}
\noindent \underline{\textbf{Probabilities}} \cite{charpentier10}
\begin{definition} A $X=(X_1,\dots,X_d)^T$ d-dimensional real random vector is a standard normal random vector if all of its components $X_i, i=1,\dots,d$ are independent and follow a standard normal distribution. \end{definition}

\begin{definition} A $X=(X_1,\dots,X_d)^T$ d-dimensional real random vector follows a multivariate normal distribution if there exists a random k-vector $Z$, which is a standard normal random vector, a d-vector $\mu$ and a $k\times d$ matrix $A$ of full rank such that $X=AZ+\mu$0. \\
We denote $X \sim \mathcal{N}_d(\mu,\Sigma)$ where $\mu=\mathbb{E}[X]=(\mathbb{E}[X_1],\dots,\mathbb{E}[X_d])^T$ is the mean vector and $\Sigma=AA^T$ is the covariance matrix. \end{definition}

\begin{definition} \label{def:multiv normal} The multivariate normal distribution is non-degenerate when the covariance matrix $\Sigma$ is positive definite, in which case it has the following density
\begin{equation*}
    f_{X}(x_1,\dots,x_d)=\frac{exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))}{\sqrt{(2\pi)^k|\Sigma|}}
\end{equation*}
where $x=(x_1,\dots,x_d)^T \in \mathbb{R}^d$ and $|\Sigma|=det(\Sigma)$ is the determinant of $\Sigma$0.
\end{definition}

\begin{proposition} \label{prop:sim proc} For any univariate cumulative distribution function $F$, 
\begin{equation*}
    U \sim \mathcal{U}([0,1]) \Rightarrow F^{-1}(U):=X \sim F
\end{equation*}
\end{proposition}

\begin{definition} Set $X$ and $Y$ two continuous random variables. $X$ and $Y$ are independent if and only if $F_{X,Y}(x,y)=F_X(x)F_Y(y)$0. \end{definition}

\begin{definition} \label{def:pearson corr} Set $X$ and $Y$ two continuous random variables with finite variance. Pearson's linear correlation coefficient is defined as 
\begin{equation*}
    corr(X,Y)=\frac{cov(X,Y)}{\sqrt{Var(X)Var(Y)}}
\end{equation*}
\end{definition}

\noindent \underline{Remark:} Pearson's correlation coefficient is linear:
\begin{equation*}
    corr(X,Y)=\pm1  \Longleftrightarrow \exists \mbox{ } a, b,  Y=aX+b \mbox{ a.s.}
\end{equation*}

\begin{proposition}
    \label{prop:corr indep}
    \begin{align*}
        X \indep Y & \Rightarrow corr(X,Y)=0 \\
        corr(X,Y)=0 & \nRightarrow X \indep Y
    \end{align*}
\end{proposition}

\noindent \underline{Remark:} This proposition means that uncorrelated data is not independent in general. Furthermore, correlation does not imply causation, meaning that we cannot deduce a cause-and-effect relationship between variables solely based on their correlation. However, in the case of a multivariate normally distributed random vector, variables that are uncorrelated are independent.

\begin{proposition} \label{prop:square-integrable} Set $X$ and $Y$ two random variables. For any measurable function $\Psi$ such that $\Psi(Y)$ is square-integrable,
\begin{equation*}
    \mathbb{E}[\Psi(Y)X]=\mathbb{E}[\Psi(Y)\mathbb{E}[X|Y]]
\end{equation*}
\end{proposition}

\vspace{5mm}
\noindent \underline{\textbf{Matrices}} \cite{horn13}
\begin{definition} A real symmetrical matrix $A$ is positive definite (respectively semi-definite)  if and only if for any non zero real column vector $z$, $z^TAz$ is positive (respectively non negative). \end{definition}

\begin{proposition} \label{prop:eigen pos def} A matrix is positive definite (resp. semi-definite) if and only if all of its eigenvalues are positive (resp. non-negative). \end{proposition}

\begin{definition} \label{def:cholesky} The Cholesky decomposition of a symmetrical real positive-definite matrix is a unique decomposition of the form $A=LL^T$ where $L$ is a lower triangular matrix with real and positive diagonal entries. \end{definition}

\begin{proposition} \label{prop:positive semi-definite cholesky} If $A$ is positive definite (resp. semi-definite), it can be written as $A=LL^T$ with $L$ a lower triangular matrix with a positive (resp. non negative) diagonal. \end{proposition}

\noindent \underline{Remark:} This is the unique (resp. non unique) Cholesky decomposition.

\begin{proposition} \label{prop:eigen invert} If a matrix can be eigendecomposed and all its eigenvalues are non null then it is invertible. \end{proposition}

\vspace{5mm}
\noindent \underline{\textbf{Copulas}} \cite{charpentier10}
\paragraph{} Copulas are used to study multi-hazard risks, ie random vectors $X=(X_1,\dots,X_n)$0. Most of the time, the marginal laws are known. Copulas are then used to get the joint law and model the dependence between variables \cite{fermanian}.

\begin{definition} A d-dimensional copula is a cumulative distribution function $C:[0,1]^d \rightarrow [0,1]$ whose margins are uniform on $[0,1]$0. \end{definition}

\begin{theorem}[Sklar's theorem] \label{th:sklar} For any multivariate cumulative distribution function $H$ with marginals $F_1,\dots,F_d$, there exists a d-dimensional copula $C$ such that
\begin{center} $H(x_1\dots,x_d)=C(F_1(x_1),\dots,F_d(x_d))$ \end{center}
If $F_1,\dots,F_d$ are all continuous, then $C$ is unique. \end{theorem}

\begin{definition} The independence copula is defined as \begin{equation*} C^\perp(u_1,\dots,u_n)=u_1 \times \dots \times u_n \end{equation*} \end{definition}

\noindent \underline{Remark:} We will note $X^\perp$ a random vector which has copula $C^\perp$0.

\begin{definition} For some correlation matrix $R$, the n-dimensional Gaussian copula with parameter $R$ is defined as \begin{equation*} C_R(u) = \Phi_R(\Phi^{-1}(u_1),\dots,\Phi^{-1}(u_n)), u\in[0,1]^n \end{equation*} \end{definition}

% Figure environment removed

\subsection{Simulation process}

\paragraph{} We will now see how to simulate copula models. It all relies on proposition~\ref{prop:sim proc}. Our goal is to simulate $X=(X_1,\dots,X_n)$, which is characterized by its marginal distributions $F_1,\dots,F_n$ and copula $C_{R_Z}$, chosen to be the Gaussian copula. The procedure is the following:
\begin{enumerate}
    \item Draw $Z\sim \mathcal{N}_n(0,R_Z)$
    \item Compute $U=(\Phi(Z_1),\dots,\Phi(Z_n))$
    \item Compute $X=(F_1^{-1}(U_1),\dots,F_n^{-1}(U_n)=(F_1^{-1}(\Phi(Z_1)),\dots,F_n^{-1}(\Phi(Z_n)))$
\end{enumerate}

% \noindent \underline{Remark:} When it is not possible to directly generate a multivariate random vector, we need to independently draw n standard normal variables and transform them into a a multivariate random vector by using the Cholesky decomposition (definition~\ref{def:cholesky}). \par
% \vspace{5mm}
% \noindent \underline{Remark:} As we are generating a multivariate normal vector, as seen in proposition ??, uncorrelated variables are independent. Consequently, the correlation matrix is how we control the dependence between our variables. \par
% \noindent \underline{Remark:} There are other measures of (non-linear) dependence between variables, such as Kendall's or Spearman's rank correlation coefficients, but we will use Pearson's linear correlation coefficient in this work. \par

% Figure environment removed

\subsubsection{Illustration in dimension 2}

\paragraph{} We will look at the results in dimension n=2 before creating our final dataset. This will allow us to give a visual illustration. First, we generate $Z=(Z_1,Z_2)$ $\sim \mathcal{N}_2(0,R)$ with $R=\begin{bmatrix}
        1 & \rho \\
        \rho & 1
    \end{bmatrix}$
and $\rho=corr(Z_1,Z_2)$0. Then, we compute $X=(X_1,X_2)=\bigl(F_1^{-1}(\Phi(Z_1)),F_2^{-1}(\Phi(Z_2))\bigr)$0. \par
The correlation between the $Z_i$ is
\begin{equation*}
    corr(Z_1,Z_2)=\rho=\frac{\mathbb{E}(Z_1Z_2)-\mathbb{E}(Z_1)\mathbb{E}(Z_2)}{\sqrt{Var(Z_1)Var(Z_2)}}=\mathbb{E}(Z_1Z_2)
\end{equation*} because $Z_i\sim \mathcal{N}(0,1)$, $i=1,2$0. And the correlation between the $X_i$ is
\begin{equation*}
    corr(X_1,X_2)=\frac{\mathbb{E}(X_1X_2)-\mathbb{E}(X_1)\mathbb{E}(X_2)}{\sqrt{Var(X_1)Var(X_2)}}
\end{equation*}

\paragraph{\underline{First case:}} $X_1 \sim \mathcal{N}(\mu_1,\sigma_1^2)$ and $X_2 \sim \mathcal{N}(\mu_2,\sigma_2^2)$ with $\mu_i \in \mathbb{R}, \sigma_i^2 \in \mathbb{R}^+_{\ast}, i=1,2$ \par
\noindent Then
\begin{equation*}
    F_i^{-1}(p)=\mu_i+\sigma_i\Phi^{-1}(p), p \in [0,1], i=1,2
\end{equation*}
In order to find the correlation between the $X_i$, we need to compute:
\begin{equation*}
    \begin{split}
    \mathbb{E}(X_1X_2) & =\mathbb{E}[F_1^{-1}(\Phi(Z_1))F_2^{-1}(\Phi(Z_2))] \\
     & =\mathbb{E}[(\mu_1+\sigma_1\Phi^{-1}(\Phi(Z_1)))(\mu_2+\sigma_2\Phi^{-1}(\Phi(Z_2)))] \\
     & =\mathbb{E}[\mu_1 \mu_2+\mu_1 \sigma_2 Z_2+\mu_2 \sigma_1 Z_1+\sigma_1 \sigma_2 Z_1 Z_2] \\
     & =\mu_1 \mu_2 +\mu_1 \sigma_2 \mathbb{E}(Z_2)+\mu_2 \sigma_1 \mathbb{E}(Z_1)+\sigma_1 \sigma_2 \mathbb{E}(Z_1Z_2) \\
     & =\mu_1 \mu_2 + \sigma_1 \sigma_2 \rho
    \end{split}
\end{equation*}
And so,
\begin{equation}
    corr(X_1,X_2)=\frac{\mu_1 \mu_2 + \sigma_1 \sigma_2 \rho -\mu_1 \mu_2}{\sigma_1 \sigma_2}=\rho
    \label{eq:corr normal normal}
\end{equation}
\vspace{5mm}
\noindent \underline{Illustration:} We will take $X_1 \sim \mathcal{N}(0,2)$ and $X_2 \sim \mathcal{N}(3,1)$0. \par
As specified by step 1, we first draw
\begin{equation*}
    Z \sim \mathcal{N}_2 \Bigl(
    \begin{bmatrix}
        0\\
        0
    \end{bmatrix},
    \begin{bmatrix}
            1 & \rho \\
            \rho & 1
    \end{bmatrix} \Bigr)
\end{equation*}
of size $2\times 100,000$0. We will follow the rest of the simulation process for three values of $\rho$: $-0.6$, $0$ and $0.6$, so as to compare the results for different correlations.
Figure~\ref{fig:Jointplot_1_Z} represents the joint distribution of $(Z_1,Z_2)$0. We see that increasing the degree of correlation positively (resp. negatively) between the marginal distributions concentrates the joint distribution around the line $y=x$ (resp. $y=-x$).

% Figure environment removed

We then apply $\Phi$ to $Z_1$ and $Z_2$ (step 2) and get their joint cumulative distribution, as plotted in figure~\ref{fig:Jointplot_2_phi(Z)}. As $Z_i\sim\mathcal{N}(0,1)$, $\Phi(Z_i)\sim\mathcal{U}([0,1])$, $i=1,2$, which is another formulation of proposition~\ref{prop:sim proc}. So we get two uniform random variables that are correlated. Once again, the joint cumulative distribution concentrate around the line $y=\pm x$ when the correlation varies.

% Figure environment removed

We finally apply $F_i^{-1}$ to $\Phi(Z_i)$ to obtain $X_i$, $i=1,2$ (step 3), as plotted in figure~\ref{fig:Jointplot_3_X}. We have the same conclusion as before on the correlation. We are well-aware that we could have directly generated a multivariate normal vector with the desired expected values and standard deviations, but this way allowed us to give an easy example of how to follow the simulation process.

% Figure environment removed

We wrote down the values of the estimated Pearson correlation coefficients in table~\ref{tab:corr_norm_norm}. As computed in equation~\ref{eq:corr normal normal}, we have $corr(X_1,X_2)=corr(Z_1,Z_2)$ for the three values of $\rho$0.
\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline
        $\rho$ & -0.6 & 0 & 0.6 \\ \hline
        $corr(Z_1,Z_2)$ & -0.6018 & -0.0009 & 0.6005 \\ \hline
        $corr(X_1,X_2)$ & -0.6018 & -0.0009 & 0.6005 \\ \hline
    \end{tabular}
    \caption{$corr(Z_1,Z_2)$ and $corr(X_1,X_2)$}
    \label{tab:corr_norm_norm}
\end{table}

% Figure environment removed

\paragraph{\underline{Second case:}} $X_1 \sim \mathcal{U}([a,b])$ and $X_2 \sim \mathcal{N}(\mu,\sigma^2)$ with $(a,b) \in \mathbb{R}^2, a<b, \mu \in \mathbb{R}, \sigma^2 \in \mathbb{R}^+_{\ast}$ \par
\noindent The computation of $corr(X_1,X_2)$ is not as direct as in the first case. As $X_1$ has a uniform distribution, we have
\begin{equation*}
    \begin{split}
         & \mathbb{E}(X_1)=\frac{a+b}{2} \\
         & Var(X_1)=\frac{(b-a)^2}{12} \\
         & F_1^{-1}(p)=a+p(b-a), p \in (0,1)
    \end{split}
\end{equation*}
We can compute
\begin{equation*}
    \begin{split}
        \mathbb{E}(X_1 X_2) & =\mathbb{E}[F_1^{-1}(\Phi(Z_1))F_2^{-1}(\Phi(Z_2))] \\
         & =\mathbb{E}[(a+(b-a)\Phi(Z_1))(\mu+\sigma Z_2)] \\
         & =a\mu+a\sigma\mathbb{E}(Z_2)+(b-a)\mu\mathbb{E}[\Phi(Z_1)]+(b-a)\sigma \mathbb{E}[\Phi(Z_1)Z_2] \\
         & =a\mu+(b-a)\frac{\mu}{2}+(b-a)\sigma \mathbb{E}[\Phi(Z_1)Z_2] \\
         & =\frac{a+b}{2}\mu+(b-a)\sigma \mathbb{E}[\Phi(Z_1)Z_2]
    \end{split}
\end{equation*}
$\mathbb{E}[\Phi(Z_1)]=\frac{1}{2}$ because $Z_1\sim\mathcal{N}(0,1)$ and so $\Phi(Z_1)\sim\mathcal{U}([0,1])$ (consequence of proposition~\ref{prop:sim proc}). \\
So we have
\begin{equation*}
    \begin{split}
        corr(X_1,X_2) & =\frac{\mathbb{E}(X_1X_2)-\mathbb{E}(X_1)\mathbb{E}(X_2)}{\sqrt{Var(X_1)Var(X_2)}} \\
         & =\frac{\frac{a+b}{2}\mu+(b-a)\sigma \mathbb{E}[\Phi(Z_1)Z_2] - \frac{a+b}{2}\mu}{\frac{b-a}{\sqrt{12}}\sigma} \\
         & =2\sqrt{3}\mathbb{E}[\Phi(Z_1)Z_2] \\
         & =2\sqrt{3}\mathbb{E}[\Phi(Z_1)\mathbb{E}[Z_2|Z_1]]
    \end{split}
\end{equation*}
because of proposition~\ref{prop:square-integrable}, with $\Phi(Z_1)$ square-integrable. \\
And $\mathbb{E}[Z_2|Z_1]=\rho Z_1$, because
\begin{equation}
    Z_2|Z_1=z_1\sim\mathcal{N}(\rho z_1, 1-\rho^2)
    \label{eq:Z2|Z1}
\end{equation}
\begin{proof}
    \begin{equation*}
        \begin{split}
            f_{Z_2|Z_1=z_1}(z_1,z2) & =\frac{f_{Z_1,Z_2}(z_1,z_2)}{f_{Z_1}(z_1)} \\
             & =\frac{\frac{1}{2\pi \sqrt{1-\rho^2}}exp(-\frac{1}{2(1-\rho)^2}(z_1^2-2\rho z_1z_2+z_2^2))}{\frac{1}{\sqrt{2\pi}}exp(-\frac{z_1^2}{2})} \\
             & =\frac{1}{\sqrt{2\pi (1-\rho^2)}}exp(-\frac{1}{2(1-\rho^2)}(z_1^2-2\rho z_1z_2+z_2^2-(1-\rho^2)z_1^2)) \\
             & =\frac{1}{\sqrt{2\pi (1-\rho^2)}}exp(-\frac{1}{2(1-\rho^2)}(z_2^2-2\rho z_1z_2+(\rho z_1)^2)) \\
             & =\frac{1}{\sqrt{2\pi (1-\rho^2)}}exp(-\frac{(z_2-\rho z_1)^2}{2(1-\rho^2)}) \\
        \end{split}
    \end{equation*}
    By identification,
    \begin{equation*}
        \begin{split}
            \mathbb{E}[Z_2|Z_1=z_1]=\rho z_1 \\
            Var(Z_2|Z_1=z_1)=1-\rho^2
        \end{split}
    \end{equation*}
\end{proof}
\noindent So we have
\begin{equation*}
    \begin{split}
        corr(X_1,X_2) & =2\sqrt{3}\mathbb{E}[\Phi(Z_1)\rho Z_1] \\
        & =2\sqrt{3}\rho \mathbb{E}[\Phi(Z_1)Z_1] \\
        & =2\sqrt{3}\rho \int_\mathbb{R} z\Phi(z) \phi(z) \,dz
    \end{split}
\end{equation*}
We will do an integration by parts:
\begin{equation*}
    \int_a^b u'(x)v(x) \,dx = [u(x)v(x)]_a^b-\int_a^b u(x)v'(x) \,dx
\end{equation*}
We can set 
\begin{equation*}
    \begin{split}
        u'(x) & =x\phi(x)=x\frac{e^{-x^2/2}}{\sqrt{2\pi}} \\
        v(x) & =\Phi(x)
    \end{split}
\end{equation*}
So
\begin{equation*}
    \begin{split}
        u(x) & =-\phi(x)=-\frac{e^{-x^2/2}}{\sqrt{2\pi}} \\
        v'(x) & =\Phi'(x)=\phi(x)
    \end{split}
\end{equation*}
So we have
\begin{equation*}
    \begin{split}
        corr(X_1,X_2) & =2\sqrt{3}\rho \bigl[[-\phi(z)\Phi(z)]_\mathbb{R} - \int_\mathbb{R} -\phi^2(z) \,dz\bigr]
    \end{split}
\end{equation*}
$\lim_{z\to-\infty} \Phi(z)=0$ and $\lim_{z\to+\infty} \Phi(z)=1$ because $\Phi$ is a cumulative distribution function \\
$\phi(z)=\frac{e^{-z^2/2}}{\sqrt{2\pi}}$ so $\lim_{z\to-\infty} \phi(z)=\lim_{z\to+\infty} \phi(z)=0$ \\
So we have $[-\phi(z)\Phi(z)]_\mathbb{R}=0$
and
\begin{equation*}
    \begin{split}
        \int_\mathbb{R} \phi^2(z) \,dz & =\int_\mathbb{R} \frac{e^{-z^2}}{2\pi} \,dz \\
        & =\int_\mathbb{R} \frac{e^{-y^2/2}}{2\pi \sqrt{2}} \,dy \\
        & =\frac{1}{2\sqrt{\pi}}
    \end{split}
\end{equation*}
because $\int_\mathbb{R} \frac{e^{-y^2/2}}{\sqrt{2\pi}} \,dy=1$, as it is the integral on the entire support ($\mathbb{R}$) of a probability density function (of the standard normal distribution). In the end,
\begin{equation*}
    corr(X_1,X_2)=2\sqrt{3}\rho \frac{1}{2\sqrt{\pi}}
\end{equation*}
\begin{equation}
    corr(X_1,X_2)=\sqrt{\frac{3}{\pi}} \rho
    \label{eq: corr normal uniform}
\end{equation}
To conclude, as $\sqrt{\frac{3}{\pi}}\simeq0.997$, we almost keep the same correlation between $X_1$ and $X_2$ as between $Z_1$ and $Z_2$, in the case where $X_1\sim \mathcal{U}([a,b])$ and $X_2 \sim \mathcal{N}(\mu,\sigma^2)$0. \par
\vspace{5mm}
\noindent \underline{Illustration:} We will take $X_1\sim\mathcal{U}([0,1])$ and $X_2\sim\mathcal{N}(0,1)$0. The joint distributions are plotted in figure~\ref{fig:Jointplot_3_X_norm_unif}. Like in the previous case, increasing the degree of correlation positively (respectively negatively) between the marginal distributions shifts the joint distribution on the line $y=x$ (respectively $y=-x$).

% Figure environment removed

We computed the estimated Pearson correlation coefficients between the simulated $Z_1$ and $Z_2$, and $X_1$ and $X_2$ in table~\ref{tab:corr_norm_unif}. As found with equation~\ref{eq: corr normal uniform}, $corr(X_1,X_2)=\sqrt{\frac{3}{\pi}}\rho$ for the three values of $\rho$0. \par
\vspace{5mm}
\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline
        $\rho$ & -0.6 & 0 & 0.6 \\ \hline
        $corr(Z_1,Z_2)$ & -0.5983 & -0.0031 & 0.6006 \\ \hline
        $\sqrt{3/\pi}\rho$ & -0.586 & 0 & 0.586 \\ \hline
        $corr(X_1,X_2)$ & -0.5838 & -0.0029 & 0.5874 \\ \hline
    \end{tabular}
    \caption{$corr(X_1,X_2)$}
    \label{tab:corr_norm_unif}
\end{table}

% Figure environment removed

\paragraph{\underline{Third case:}} $X_1 \sim \mathcal{B}(p)$ and $X_2 \sim \mathcal{N}(\mu,\sigma^2)$, $p \in (0,1),\mu \in \mathbb{R}, \sigma^2 \in \mathbb{R}^+_{\ast}$ \par
\noindent As $F_1$ is not continuous, we are not under the assumptions of Sklar's theorem (theorem~\ref{th:sklar}), so the copula $C$ might not be unique. \par
We will place ourselves in the second case and simulate $X_1 \sim \mathcal{U}([0,1])$ and $X_2 \sim \mathcal{N}(\mu,\sigma^2)$ with $\mu \in \mathbb{R}, \sigma^2 \in \mathbb{R}^+_{\ast}$, then transform $X_1$ into a Bernoulli random variable using the function
\begin{equation*}
    \begin{split}
        h: & [0,1] \rightarrow \{0,1\} \\
         & u \mapsto h(u)=\left\{
    \begin{array}{ll}
        0 & \mbox{if } u<\tau \\
        1 & \mbox{if } u\geq \tau
    \end{array}
\right. =\mathbb{1}_{u \geq \tau}
    \end{split}
\end{equation*}
We then have $h(X_1) \sim \mathcal{B}(p)$0.
\begin{proof}
    \begin{equation*}
        \mathbb{P}(h(X_1)=1)=\mathbb{P}(\mathbb{1}_{X_1 \geq \tau}=1)=\mathbb{P}(X_1 \geq \tau)=1-F_1(\tau)=1-\tau
    \end{equation*}
    \begin{equation*}
        \mathbb{P}(h(X_1)=0)=\mathbb{P}(\mathbb{1}_{X_1 \geq \tau}=0)=1-\mathbb{P}(\mathbb{1}_{X_1 \geq \tau}=1)=\tau
    \end{equation*}
    By identification, $h(X_1) \sim \mathcal{B}(p)$ with $p=1-\tau$
\end{proof}
As the cumulative distribution function of a Bernoulli random variable is not invertible, we cannot go back to the generative copula from the set $(h(X_1),X_2)$0. In a sense, we have broken the correlation structure. But our transformation $h$ allows the computation of the correlation as a function of $\rho$:
\begin{equation*}
    corr(h(X_1),X_2)=\frac{\mathbb{E}[h(X_1)X_2]-\mathbb{E}[h(X_1)]\mathbb{E}[X_2]}{\sqrt{Var(h(X_1))Var(X_2)}}
\end{equation*}
with
\begin{equation*}
    \begin{split}
        \mathbb{E}[h(X_1)X_2] & =\mathbb{E}[h(X_1)F_2^{-1}(\Phi(Z_2))] \\
         & =\mathbb{E}[h(X_1)(\mu+\sigma Z_2)] \\
         & =\mu \mathbb{E}[h(X_1)]+\sigma \mathbb{E}[h(X_1)Z_2]
    \end{split}
\end{equation*}
and
\begin{equation*}
    \begin{split}
        \mathbb{E}[h(X_1)Z_2] & =\mathbb{E}[\mathbb{1}_{X_1 \geq \tau}Z_2] \\
         & =\mathbb{E}[\mathbb{1}_{F_1^{-1}(\Phi(Z_1)) \geq \tau}Z_2] \\
         & =\mathbb{E}[\mathbb{1}_{Z_1 \geq \Phi^{-1}(F_1(\tau))}Z_2] \\
         & =\mathbb{E}[\mathbb{1}_{Z_1 \geq \Phi^{-1}(\tau)}Z_2] \\
         & =\mathbb{E}[\mathbb{1}_{Z_1 \geq \Phi^{-1}(\tau)}\mathbb{E}[Z_2|Z_1]] \mbox{ (proposition~\ref{prop:square-integrable})} \\
         & =\mathbb{E}[\mathbb{1}_{Z_1 \geq \Phi^{-1}(\tau)}\rho Z_1] \mbox{ (equation~\ref{eq:Z2|Z1})} \\
         & =\rho \int_\mathbb{R} \mathbb{1}_{z \geq \Phi^{-1}(\tau)}z \phi(z) \,dz \\
         & =\rho \int_{\Phi^{-1}(\tau)}^{+\infty} z \phi(z) \,dz \\
         & =\rho[-\phi(z)]_{\Phi^{-1}(\tau)}^{+\infty} \\
         & =\rho \phi(\Phi^{-1}(\tau))
    \end{split}
\end{equation*}
So
\begin{equation*}
    \begin{split}
        corr(h(X_1),X_2) & =\frac{\mu \mathbb{E}[h(X_1)]+\sigma \rho \phi(\Phi^{-1}(\tau))-\mathbb{E}[h(X_1)]\mu}{\sqrt{\tau(1-\tau)\sigma^2}} \\
         & =\frac{\rho \phi(\Phi^{-1}(\tau))}{\sqrt{\tau(1-\tau)}} \\
         & =\frac{\rho \frac{1}{\sqrt{2\pi}}e^{-\frac{(\Phi^{-1}(\tau))^2}{2}}}{\sqrt{\tau(1-\tau)}}
    \end{split}
\end{equation*}
So we have
\begin{equation}
    corr(h(X_1),X_2)=\frac{\rho e^{-\frac{(\Phi^{-1}(\tau))^2}{2}}}{\sqrt{\tau(1-\tau)2\pi}}
    \label{eq:corr bern norm}
\end{equation}
As the correlation coefficient is in $[-,1]$,
\begin{equation*}
    \begin{split}
        \min_{\rho \in (-1,1)} corr(h(X_1),X_2) & =\min_{\rho \in (-1,1)} \frac{\rho e^{-\frac{(\Phi^{-1}(\tau))^2}{2}}}{\sqrt{\tau(1-\tau)2\pi}}=-\frac{e^{-\frac{(\Phi^{-1}(\tau))^2}{2}}}{\sqrt{\tau(1-\tau)2\pi}} \\
        \max_{\rho \in (-1,1)} corr(h(X_1),X_2) & =\max_{\rho \in (-1,1)} \frac{\rho e^{-\frac{(\Phi^{-1}(\tau))^2}{2}}}{\sqrt{\tau(1-\tau)2\pi}}=\frac{e^{-\frac{(\Phi^{-1}(\tau))^2}{2}}}{\sqrt{\tau(1-\tau)2\pi}}
    \end{split}
\end{equation*}
So $corr(h(X_1),X_2)$ has a minimal and maximal value:
\begin{equation*}
    corr(h(X_1),X_2) \in \Bigl[-\frac{e^{-\frac{(\Phi^{-1}(\tau))^2}{2}}}{\sqrt{\tau(1-\tau)2\pi}},\frac{e^{-\frac{(\Phi^{-1}(\tau))^2}{2}}}{\sqrt{\tau(1-\tau)2\pi}}\Bigr]
\end{equation*}
Figure~\ref{fig:factor_p} represents $\frac{e^{-\frac{(\Phi^{-1}(\tau))^2}{2}}}{\sqrt{\tau(1-\tau)2\pi}}$ for $\tau\in(0,1)$0.
\begin{equation*}
    \max_{\tau\in(0,1)}(\frac{e^{-\frac{(\Phi^{-1}(\tau))^2}{2}}}{\sqrt{\tau(1-\tau)2\pi}})=0.798 \mbox{ for } \tau=\frac{1}{2}
\end{equation*}
So for all $\tau\in(0,1),|corr(h(X_1),X_2)|\leq0.798$0.

% Figure environment removed

\noindent \underline{Illustration:} We will take $X_1\sim \mathcal{B}(0.8)$ and $X_2 \sim \mathcal{N}(2,0.8)$0. \par
\noindent Then $\Phi^{-1}(\tau)=\Phi^{-1}(1-0.8)=\Phi^{-1}(0.2)\simeq-0.842$ (numeric computation) and
\begin{equation*}
    \frac{e^{-\frac{(\Phi^{-1}(\tau))^2}{2}}}{\sqrt{\tau(1-\tau)2\pi}}=\frac{e^{-\frac{(-0.842)^2}{2}}}{\sqrt{(1-0.8)\times 0.8\times 2\pi}}\simeq 0.759
\end{equation*}
So
\begin{equation*}
    corr(h(X_1),X_2)=0.759\rho \in(-0.759,0.759)
\end{equation*}
The joint distributions are plotted in figure~\ref{fig:Jointplot_bern_norm}. Like in the previous cases, increasing the degree of correlation positively (respectively negatively) between the marginal distributions shifts the joint distribution on the line $y=x$ (respectively $y=-x$).

% Figure environment removed

We computed the estimated Pearson correlation coefficients between the simulated $h(X_1)$ and $X_2$ in table~\ref{tab:h(x1)x2}. We can see that $corr(h(X_1),X_2)=\frac{\rho e^{-\frac{(\Phi^{-1}(\tau))^2}{2}}}{\sqrt{\tau(1-\tau)2\pi}}$ for the three values of $\rho$, as we computed in equation~\ref{eq:corr bern norm}.

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline
        $\rho$ & -0.6 & 0 & 0.6 \\ \hline
        $\frac{\rho e^{-\frac{(\Phi^{-1}(\tau))^2}{2}}}{\sqrt{\tau(1-\tau)2\pi}}$ & -0.455 & -0.001 & 0.455 \\ \hline
        $corr(h(X_1),X_2)$ & -0.454 & 0.000 & 0.455 \\ \hline
    \end{tabular}
    \caption{$corr(h(X_1),X_2)$}
    \label{tab:h(x1)x2}
\end{table}

% Figure environment removed

\subsubsection{Illustration in dimension $n$: creating the dataset}
\paragraph{} We can now create the simulated dataset. For simplicity reasons, we will suppose that we are in the setting of section~\ref{binary_classif}, with two binary protected variables and a binary outcome. We have
\begin{itemize}
    \item a set of non-protected variables $\bm{X} = \{X^{(i)}\}_{i=1,...n-3}$, $n \in \mathbb{N}$ 
    with $X^{(i)}\sim\mathcal{N}(\mu_i,\sigma_i^2)$, $\mu_i \in \mathbb{R}$, $\sigma_i \in \mathbb{R}_\ast^+$
    \item two protected variables $A\sim\mathcal{B}(p_a),p_a\in(0,1)$ and $B\sim\mathcal{B}(p_b),p_b\in(0,1)$
    \item the output variable $Y\sim\mathcal{B}(p_y),p_y\in(0,1)$
    \item $\bm{Z}\sim \mathcal{N}_n(0,R_Z)$0. For $(i,j)\in\llbracket 1,n\rrbracket^2$,
    \begin{equation*}
        R_{Z,ij}=\left\{
        \begin{array}{ll}
            1 & \mbox{if } i=j \\
            corr(Z_i,Z_j) & \mbox{if } i\neq j
        \end{array}
        \right.
    \end{equation*}
    % \begin{equation*}
    %     R_{X,A,B,Y}=
    %     \begin{bmatrix}
    %         1 & corr(X^{(1)},X^{(2)}) & \multicolumn{2}{c}{\dots} & corr(X^{(1)},A) & corr(X^{(1)},Y) \\
    %         corr(X^{(2)},X^{(1)}) & 1 & \dots & corr(X^{(2)},A) & corr(X^{(2)},B) & corr(X^{(2)},Y) \\
    %         \vdots & & & & & \vdots \\
    %         corr(A,X^{(1)}) & corr(A,X^{(2)}) & \cdots & 1 & corr(A,B) & corr(A,Y) \\
    %         corr(B,X^{(1)}) & corr(B,X^{(2)}) & \cdots & corr(B,A) & 1 & corr(B,Y) \\
    %         corr(Y,X^{(1)}) & corr(Y,X^{(2)}) & \cdots & corr(Y,A) & corr(B,Y) & 1
    %     \end{bmatrix}
    % \end{equation*}
    \item[] with $corr(X^{(i)},X^{(j)})=corr(Z_i,Z_j)$, $(i,j)\in\llbracket 1,n-3\rrbracket$ (equation~\ref{eq:corr normal normal})
    \item[] $corr(X^{(i)},A)=\frac{e^{-\frac{(\Phi^{-1}(1-p_a))^2}{2}}}{\sqrt{p_a(1-p_a)2\pi}}corr(Z_i,Z_{n-2}),i\in\llbracket 1,n-3\rrbracket$ (equation~\ref{eq: corr normal uniform})
    \item[] $corr(X^{(i)},B)=\frac{e^{-\frac{(\Phi^{-1}(1-p_b))^2}{2}}}{\sqrt{p_b(1-p_b)2\pi}}corr(Z_i,Z_{n-1}),i\in\llbracket 1,n-3\rrbracket$ (equation~\ref{eq: corr normal uniform})
    \item[] $corr(X^{(i)},Y)=\frac{e^{-\frac{(\Phi^{-1}(1-p_y))^2}{2}}}{\sqrt{p_y(1-p_y)2\pi}}corr(Z_i,Z_n),i\in\llbracket 1,n-3\rrbracket$ (equation~\ref{eq: corr normal uniform})
    % \item[] and $corr(A,Y)=$
\end{itemize}

\noindent For the choice of $R_Z$, there are constraints. It must be a square matrix and:
\begin{itemize}
    \item symmetrical, because $corr(Z_i,Z_j)=corr(Z_j,Z_i)$
    \item with a unity diagonal, because $corr(Z_i,Z_i)=1$
    \item all values must be in $[-1,1]$ by definition of the correlation
    \item positive semi-definite
    \item invertible, otherwise the distribution is degenerate and does not have a density, as we saw in definition~\ref{def:multiv normal}
\end{itemize}
\noindent In order to have an invertible and positive semi-definite matrix, it needs to be positive definite. Indeed, proposition~\ref{prop:eigen invert} gives the equivalence between an invertible matrix and its eigenvalues being non null, and proposition ~\ref{prop:eigen pos def} gives the equivalence between a positive definite (respectively semi-definite) matrix and its eigenvalues being positive (respectively positive or null). \par
Using proposition~\ref{prop:positive semi-definite cholesky} (Cholesky decomposition), we will compute $R_Z=LL^T$, choosing $L$ as a lower triangular matrix with a positive diagonal. That way we will have a square symmetrical, positive definite (so positive semi-definite and invertible) matrix.
\begin{equation*}
    \begin{split}
        R_Z & =
        \begin{bmatrix}
            L_{11} & 0 & 0 & \dots \\
            L_{21} & L_{22} & 0 & \dots \\
            \vdots & & \ddots &
        \end{bmatrix}
        \times
        \begin{bmatrix}
            L_{11} & L_{21} & \dots \\
            0 & L_{22} & \dots \\
            \vdots & 0 & \ddots
        \end{bmatrix} \\
         & =\begin{bmatrix}
            L_{11}^2 & L_{11}L_{21} & \dots \\
            L_{11}L_{21} & L_{21}^2+L_{22}^2 & \\
            \vdots & & \ddots
        \end{bmatrix}
    \end{split}
\end{equation*}
We have
\begin{equation*}
    \begin{split}
        R_{Z,ii} & =\sum_{k=1}^i L_{ik}^2 \\
        R_{Z,ij} & =R_{Z,ji}=\sum_{k=1}^{i} L_{ik}L_{jk}, i>j
    \end{split}
\end{equation*}
As we want the diagonal of $R_Z$ to be unity, we need
\begin{equation*}
    \begin{split}
        R_{Z,ii}=1 & \text{ ie } \sum_{k=1}^i L_{ik}^2=1 \\
         & \text{ ie } L_{ii}=\sqrt{1-\sum_{k=1}^{i-1}L_{ik}^2} \mbox{ (the diagonal has to be positive)}
    \end{split}
\end{equation*}
The computation of the $L_{ii}$ implies constraints on the values of the $L_{ij}$:
\begin{equation*}
    \forall i=1,\dots,n, 1-\sum_{k=1}^{i-1}L_{ik}^2>0 \mbox{ ie } \sum_{k=1}^{i-1}L_{ik}^2<1
\end{equation*}
Finally, to set $R_Z$, we set values for the $L_{ij},i>j$, check that the constraint above is verified, compute the $L_{ii}$ accordingly and then $R_Z$0. We also need to check that we have $R_{Z,ij}\in[-1,1]$0. \par
Finally, the procedure is:
\begin{itemize}
    \item set the matrix $L$ with the constraints mentioned above
    \item compute $R_Z=LL^T$
    \item generate $Z\sim \mathcal{N}_n(0,R_Z)$
    \item set the parameters of the laws of \begin{itemize}
        \item[] $X^{(i)}\sim\mathcal{N}(\mu_i,\sigma_i^2)$
        \item[] $A\sim\mathcal{B}(p_a)$
        \item[] $B\sim\mathcal{B}(p_b)$
        \item[] $Y\sim\mathcal{B}(p_y)$
    \end{itemize}
    \item compute \begin{itemize}
        \item[] $\bm{X}=(F_1^{-1}(\Phi(Z_1)),\dots,F_{n-3}^{-1}(\Phi(Z_{n-3})))$
        \item[] $A=h_a(F_{n-3}^{-1}(\Phi(Z_{n-3})))$
        \item[] $B=h_b(F_{n-2}^{-1}(\Phi(Z_{n-2})))$
        \item[] $Y=h_y(F_{n}^{-1}(\Phi(Z_{n})))$
    \end{itemize}
    Reminder: $h_a(u) =\mathbb{1}_{u \geq 1-p_a}$
\end{itemize}

\paragraph{Final dataset} We set $n=7$ and generated 100 datasets with the same parameters:
\begin{equation*}
    \begin{split}
        \label{def x_i}
        X^{(1)} & \sim\mathcal{N}(2,0.6) \\
        X^{(2)} & \sim\mathcal{N}(0.2,0.3) \\
        X^{(3)} & \sim\mathcal{N}(-0.3,2) \\
        X^{(4)} & \sim\mathcal{N}(0.7,0.4) \\
        A & \sim\mathcal{B}(0.3) \\
        B & \sim\mathcal{B}(0.9) \\
        Y & \sim\mathcal{B}(0.2)
    \end{split}
\end{equation*}
\begin{equation*}
    \label{theoretical corr R_xa}
    R_{X,A,B,Y}=\begin{bmatrix}
        1 & 0.395 & -0.018 & 0.297 & -0.230 & 0.350 & 0.139 \\
        0.395 & 1 & -0.501 & 0.103 & 0.226 & 0.111 & 0.209 \\
        -0.018 & -0.501 & 1 & 0.294 & 0.076 & 0.294 & -0.066 \\
        0.297 & 0.103 & 0.294 & 1 & -0.227 & 0.348 & -0.208 \\
        -0.230 & 0.226 & 0.076 & -0.227 & 1 & 0.043 & 0.105 \\
        0.350 & 0.111 & 0.294 & 0.348 & 0.043 & 1 & 0.039 \\
        0.139 & 0.209 & -0.066 & -0.208 & 0.105 & 0.039 & 1
    \end{bmatrix}
\end{equation*}

The reason for that is that we want to take into account the instability of results. Instability can come from the generative process: not all datasets will have the exact same variable distribution because of the limited sample size (100,00 lines here). It can also be caused later on by the train test split or sampling which depend strongly on the execution. Generating 100 datasets allows us to average and get confidence intervals on metrics and results. In reality, we often do not have access to the data generator so, to construct confidence intervals, we use bootstraping, which estimates the sampling distribution of statistics such as sample mean thanks to random sampling with replacement. Figure~\ref{fig:table_sim_data} shows the head of one of the datasets. \par

% Figure environment removed

As mentioned previously, we will take advantage of having access to the data generation process. It means we can produce confidence intervals for every value: the mean of a variable, the number of observations of a certain class, the weights of the regression etc. This is the reason why we generated 100 datasets: we will look at the average and standard deviation over these datasets to produce the confidence interval of the value we are looking at. We gave a reminder on confidence intervals in appendix~\ref{appendix:confidence intervals}. 

% Figure environment removed

\subsection{Descriptive statistics}
\paragraph{} Before heading into applying methods, we need to prepare and explore our data. As we have built our datasets ourselves, we already know a lot about them.  \par

\subsubsection{Variable identification and univariate analysis}
\paragraph{} The explanatory variables are the $X^{(i)},i=1,\dots,4$, $A$ and $B$, and the target variable is $Y$0. As expected, we have 100,000 non-null values for each variable. The $X^{(i)}$ are continuous, and $A$, $B$ and $Y$ are categorical, taking values in $\{0,1\}$0. \par

\paragraph{X} By construction, the $X^{(i)}$ have means and standard derivations as defined in section~\ref{def x_i}, and it is verified by the computation of the sample means and standard derivations in table~\ref{tab:X_i describe}. We can notice that the confidence intervals are of size 0. This means that over our 100 datasets, we have a probability of 95\% that they all have the means and standard deviations as defined in data generation process.

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        i & 1 & 2 & 3 & 4 \\ \hline
        mean & $2.00\pm0.00$ & $0.20\pm0.00$ & $-0.30\pm0.00$ & $0.70\pm0.00$ \\ \hline
        std & $0.60\pm0.00$ & $0.30\pm0.00$ & $2.00\pm0.00$ & $0.40\pm0.00$  \\ \hline
    \end{tabular}
    \caption{Means and standard derivations of the $X^{(i)}$}
    \label{tab:X_i describe}
\end{table}

\paragraph{A} We computed it to follow a Bernoulli distribution of parameter $p_a=0.3$, so we find as planned that
\begin{equation*}
    \begin{split}
        \mbox{mean(A)} & =0.30\pm0.00=p_a \\
        \mbox{std(A)} & =0.46\pm0.00=\sqrt{0.3(1-0.3)}=\sqrt{p_a(1-p_a)}
    \end{split}
\end{equation*}
Table~\ref{tab:A counts} gives the number of observations for each value of $A$0. By construction, as $A \sim \mathcal{B}(0.3)$, there is an imbalance: about $30\%$ individuals have $A=1$ and $70\%$ individuals have $A=0$0. So the group imbalance ratio is
\begin{equation*}
    IR_A = \frac{\mbox{Number of majority observations}}{\mbox{Number of minority observations}} = 2.33\pm0.00
\end{equation*}
meaning there are 2.33 times more observations of $A=0$ than $A=1$0.

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|}
        \hline
        A & Observations \\ \hline
        0 & $70,002.22\pm290.73$ \\ \hline
        1 & $29,997.78\pm290.73$ \\ \hline
    \end{tabular}
    \caption{Number of observations by value of $A$}
    \label{tab:A counts}
\end{table}

\paragraph{B} We computed it to follow a Bernoulli distribution of parameter $p_b=0.9$, so we find as planned that
\begin{equation*}
    \begin{split}
        \mbox{mean(B)} & =0.90\pm0.00=p_a \\
        \mbox{std(B)} & =0.30\pm0.00=\sqrt{0.9(1-0.9)}=\sqrt{p_b(1-p_b)}
    \end{split}
\end{equation*}
Table~\ref{tab:B counts} gives the number of observations for each value of $B$0. By construction, as $B \sim \mathcal{B}(0.9)$, there is an imbalance: about $90\%$ individuals have $B=1$ and $10\%$ individuals have $B=0$0. So the group imbalance ratio is
\begin{equation*}
    IR_A = 8.99\pm0.02
\end{equation*}
meaning there are almost 9 times more observations of $B=0$ than $B=1$0.

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|}
        \hline
        B & Observations \\ \hline
        0 & $89,980.70\pm207.11$ \\ \hline
        1 & $10,019.30\pm207.11$ \\ \hline
    \end{tabular}
    \caption{Number of observations by value of $B$}
    \label{tab:B counts}
\end{table}

\paragraph{Y} We computed it to follow a Bernoulli distribution of parameter $p_y=0.2$, so as expected
\begin{equation*}
    \begin{split}
        \mbox{mean(Y)} & =0.20\pm0.00=p_y \\
        \mbox{std(A)} & =0.40\pm0.00=\sqrt{0.2(1-0.2)}=\sqrt{p_y(1-p_y)}
    \end{split}
\end{equation*}
This results in an imbalance too: table~\ref{tab:Y counts} gives the number of observations for each value of $Y$, and the imbalance ratio is
\begin{equation*}
    IR_Y = 4.00\pm0.01
\end{equation*}
meaning there are 4 times more observations of $Y=0$ than $Y=1$0.

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|}
        \hline
        Y & Observations \\ \hline
        0 & $80,010.49\pm262.28$ \\ \hline
        1 & $19,989.51\pm262.28$ \\ \hline
    \end{tabular}
    \caption{Number of observations by value of $Y$}
    \label{tab:Y counts}
\end{table}

\paragraph{Going back on imbalance} In real life, imbalance can be explained either by the way the data was collected or by the natural domination of one class. The collection of data can lead to an imbalance if the sampling is biased or if mistakes are made, for examples writing down the wrong labels on observations. In insurance, there are sampling biases: conclusions about risks only concern individuals who have been accepted at the underwriting stage. As underwriters aim at selecting `good' risks, most of the time, the insurer's portfolio will be very specific and predictions on claims frequency, for example, cannot be generalized to a different population.

% Figure environment removed

\subsubsection{Multivariate analysis}
\paragraph{} An important part of data exploration consists in studying the relationships between variables. To do so, we will analyze how they are correlated to each other. As a reminder, correlation is how linearly related two variables are, and is only the first order approximation of dependence, as seen previously.

\paragraph{Correlations with Y} A heatmap of the correlations can help understand which variables are correlated with each other. Figure~\ref{fig:heatmap corr} shows the heatmap of correlations. The strongest positive correlations are colored in bright red and the strongest negative correlations are colored in bright blue. The strongest correlations to $Y$ in absolute value are with $X^{(4)}$, $X^{(2)}$, $X^{(1)}$, $A$, $X^{(2)}$, $X^{(3)}$ and then $B$0. This is coherent with the theoretical values of the correlation matrix $R_{X,A,B,Y}$ in section~\ref{theoretical corr R_xa}.
% The protected variable is not the most correlated variable with the output, so one could naively believe that $Y$ does not depend on $A$ and that the statistical parity is respected. This is a good time to remember that uncorrelatedness is not equivalent to independence, as we saw in proposition~\ref{prop:corr indep}. \par

% Figure environment removed

\paragraph{Correlations with A} We saw in the previous paragraph that $A$ has the third strongest absolute value correlation to $Y$0. It is also strongly correlated to the $X^{(i)}$, as we see in figure~\ref{fig:heatmap corr}. We noted in the previous section that there are imbalances in the number of observations for both $A$ and $Y$, and we see in table~\ref{tab:A Y counts} that among group $A$ there is an imbalance in output values too. If we compute the imbalance ratio for the output, we get
\begin{equation*}
    \begin{split}
        \mbox{A=0: } & IR_Y=4.86\pm0.01 \\
        \mbox{A=1: } & IR_Y=2.73\pm0.01
    \end{split}
\end{equation*}
meaning that within group $A=0$, there are 4.86 times more observations of $Y=0$ than $Y=1$ and within group $A=1$, there are 2.73 times more observations of $Y=0$ than $Y=1$0. In general there are a lot more $Y=0$ outputs than $Y=1$ ones, and when we zoom in on protected groups, the imbalance ratio is larger for group $A=0$ than for $A=1$, meaning that the former has a larger proportion of $Y=0$ outputs than the latter.
% This gives direct hints that the demographic parity principle is not respected.

\begin{table}[H]
    \centering
    \begin{tabular}{c|c|c|}
        \cline{2-3}
         & Y=0 & Y=1 \\ \hline
        \multicolumn{1}{|c|}{A=0} & $58,048.36\pm33.08$ & $11,953.86\pm21.76$ \\ \hline
        \multicolumn{1}{|c|}{A=1} & $21,962.13\pm26.37$ & $8,035.65\pm15.49$ \\ \hline
    \end{tabular}
    \caption{Number of observations by values of $A$ and $Y$}
    \label{tab:A Y counts}
\end{table}

\paragraph{Correlations with B} We saw that $B$ has the weakest absolute value correlation to $Y$, but it is still strongly correlated to the $X^{(i)}$0. Within groups, as for $A$, there can be imbalances, as we see in table~\ref{tab:B Y counts}. The imbalance ratios by group are
\begin{equation*}
    \begin{split}
        \mbox{B=0: } & IR_Y=5.53\pm0.02 \\
        \mbox{B=1: } & IR_Y=3.88\pm0.01
    \end{split}
\end{equation*}
meaning that within group $B=0$, there are 5.53 times more observations of $Y=0$ than $Y=1$ and within group $B=1$, there are 3.88 times more observations of $Y=0$ than $Y=1$0. The imbalance ratio is larger for group $B=0$ than for $B=1$0.
% This gives direct hints that the demographic parity principle is not respected.

\begin{table}[H]
    \centering
    \begin{tabular}{c|c|c|}
        \cline{2-3}
         & Y=0 & Y=1 \\ \hline
        \multicolumn{1}{|c|}{B=0} & $8,484.30\pm19.82$ & $1,535.00\pm9.06$ \\ \hline
        \multicolumn{1}{|c|}{B=1} & $71,526.19\pm29.28$ & $18,454.51\pm24.25$ \\ \hline
    \end{tabular}
    \caption{Number of observations by values of $B$ and $Y$}
    \label{tab:B Y counts}
\end{table}

\paragraph{Correlations between the $\mathbf{X^{(i)}}$} By construction the $X^{(i)}$ are related to each other, through the correlation matrix. Figure~\ref{fig:pairplots xi} gives the pairwise relationships between the $X^{(i)}$, and the diagonal is their marginal distribution. We observe that $X^{(1)}$ is positively correlated with $X^{(2)}$ and $X^{(3)}$, $X^{(2)}$ is negatively correlated with $X^{(3)}$, and $X^{(3)}$ is negatively correlated with $X^{(4)}$0. Correlations between other variables are less obvious. \par

% Figure environment removed

\subsubsection{Outliers}
\paragraph{} As the $X^{(i)}$ were computed to follow normal distributions, and $A$, $B$ and $Y$ to take values in $\{0,1\}$, we do not expect to have any outliers. This is verified in our datasets: there are no unexpected values for the $X^{(i)}$ and we have values of $A$, $B$ and $Y$ in $\{0,1\}$0.

\newpage