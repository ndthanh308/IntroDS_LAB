\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{subcaption}
\usepackage{parskip}
% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{7060} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

\title{IML-ViT: Image Manipulation Localization by Vision Transformer}

\author{Xiaochen Ma\textsuperscript{1}, Bo Du\textsuperscript{1},  Zhuohang Jiang\textsuperscript{1}, Ahmed Y. Al Hammadi\textsuperscript{2},  and Jizhe Zhou\textsuperscript{1}  \\
\textsuperscript{1} Sichuan University \\
\textsuperscript{2} Mohamed Bin Zayed University for Humanities
}







\maketitle
%%%%%%%%% ABSTRACT
\begin{abstract}
  % 背景概述
Advanced image tampering techniques are increasingly challenging the trustworthiness of multimedia, leading to the development of Image Manipulation Localization (IML). 
But what makes a good IML model? The answer lies in the way to capture artifacts. Exploiting artifacts requires the model to extract non-semantic discrepancies between manipulated and authentic regions, necessitating explicit comparisons between the two areas. With the self-attention mechanism, naturally, the Transformer should be a better candidate to capture artifacts. 
However, due to limited datasets, there is currently no pure ViT-based approach for IML to serve as a benchmark, and CNNs dominate the entire task. Nevertheless, CNNs suffer from weak long-range and non-semantic modeling.
To bridge this gap, based on the fact that artifacts are sensitive to image resolution, amplified under multi-scale features, and massive at the manipulation border, we formulate the answer to the former question as building a ViT with high-resolution capacity, multi-scale feature extraction capability, and manipulation edge supervision that could converge with a small amount of data. We term this simple but effective ViT paradigm IML-ViT, which has significant potential to become a new benchmark for IML. Extensive experiments on five benchmark datasets verified our model outperforms the state-of-the-art manipulation localization methods. Code and models are available at \url{https://github.com/SunnyHaze/IML-ViT}
  
  % In this paper, we propose an end-to-end training paradigm to solve this benchmarking problem by pioneering a completely new track - Image Manipulation Localization based on Vision Transformer (ViT), while state-of-the-art models all use CNNs as the backbone. We call this simple but effective paradigm IML-ViT.

  % Extensive experiments on five public benchmark datasets verify that our method outperforms state-of-the-art Image Manipulation Localization methods.
  % (e.g., large private dataset during training, augumentation aiming at testing set but not generalized, over-emphasis on unimportant metrics, etc.)
  % 解决方案
  % IMD-ViT, the first vision transformer-based model to detect and localize the manipulated regions. 
  % % 现有研究的缺点
  % Existing research focuses on constructing complex feature extractors that may weaken the generalization of the model.
  % % 引导学习
  % However, we guide the model to learn the boundary artifacts of tampering recognition by itself in the following way:(i) Construct as large an input resolution as possible to ensure that details are not corrupted.
  % (ii) Enhanced boundary supervision is used to guide the model to learn the features of the boundary distribution by itself.
  % % 表现：
  % Extensive experiments on five benchmark datasets verified our model outperforms the state-of-the-art tempering detection methods.

\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

As multimedia editing technology advances, we increasingly need advanced Image Manipulation Localization(IML) methods to cope with existing tampered images and avoid security threats~\cite{Verdoliva_2020}. As shown in Figure \ref{fig:artifact}, this task aims to detect whether images have been modified and to localize the modified regions at the pixel level in a segmentation manner.
Image manipulation can be generally classified into three types~\cite{Mantra_2019, Verdoliva_2020}: (1)\textit{splicing}: copying a region from an image and pasting it to another image. (2)\textit{copy-move}: cloning a region within an image. (3)\textit{inpainting}: erasing regions from images and inpaint missing regions with visually plausible contents.

% As shown in Table \ref{tab:1}, to track manipulation, most existing methods greatly benefit from tracing \textit{artifacts} and low-level visual features to support the decision by utilizing various CNN-based feature extractors. Artifact refers to a distinctive, noticeable visual element in an image that resulted from manipulation. Since tampering aims to construct semantically meaningful and perceptually convincing images to cheat the audience, in most cases, artifact only distributes at the detailed texture level that is \textbf{non-semantic} and \textbf{non-objective}. Low-level features such as noise and high-frequency inconsistencies from different cameras can be crucial evidence to uncover manipulated regions within the authentic area. Thus, \textit{the key to IML lies in locating the artifacts and capturing the discrepancies between the manipulated and authentic regions.} 

% Figure environment removed

% % Figure environment removed

\begin{table*}[t]
\centering
\resizebox{2.0\columnwidth}{!}{
\begin{tabular}{l|l|l|l|l}
\toprule[2pt]
  \textbf{Methods} &
  \textbf{View} &
  \textbf{Training Dataset For Evaluation} &
  \textbf{Backbone} &
  \textbf{Resolution} \\ \hline
  ManTra-Net~\cite{Mantra_2019} , 2019 &
    \begin{tabular}[c]{@{}l@{}}BayarConv(Noise)\\      SRM filter(Noise)\end{tabular} &
    Private,   102,028 images &
    wider VGG &
    512×512 \\ \hline
  SPAN~\cite{SPAN_2020}, 2020 &
    \begin{tabular}[c]{@{}l@{}}BayarConv(Noise)\\      SRM filter(Noise)\end{tabular} &
    \begin{tabular}[c]{@{}l@{}}Private, 102,028 images \\      (Copied  parameters from ManTra-Net)\end{tabular} &
    \begin{tabular}[c]{@{}l@{}}wider VGG \\ self-attention\end{tabular}   &
    \begin{tabular}[c]{@{}l@{}}raw \&   224×224 \\      (resized feature)\end{tabular} \\ \hline
  CR-CNN~\cite{CR_CNN_2020}, 2020 &
    \begin{tabular}[c]{@{}l@{}}BayarConv(Noise)\\      SRM filter(Noise)\end{tabular} &
    Public: CASIA   v2 &
    mask R-CNN &
    short side to   600 \\ \hline
  GSR-Net~\cite{GSR_Net_2020}, 2020 &
    Edge Prediction &
    Public: CASIA   v2 &
    DeepLab &
    300×300 \\ \hline
  MVSS-Net~\cite{MVSS_2021}, 2021 &
    \begin{tabular}[c]{@{}l@{}}BayarConv(Noise)\\      Sobel(Edge)\end{tabular} &
    Public: CASIA v2 or Defacto &
    FCN &
    512×512 \\ \hline
  MM-Net~\cite{mmnet_2021}, 2021 &
    BayarConv(Noise) &
    Private: synthesized &
    mask R-CNN &
    short side to 800 \\ \hline
  
  TransForensics~\cite{transforensic_2021}, 2021 &
    - &
    Public: CASIA v2, COVERAGE and IMD2020 &
    FCN + Transformer blocks &
    512×512 \\ \hline
  ObjectFormer~\cite{objectformer_2022}, 2022 &
    High-Frequency &
    Private: large synthesized &
    CNN + Transformer &
    256x256 \\ \hline
  \textit{IML-ViT(This paper)} &
    Edge supervision &
    Public: CASIA v2&
    ViT &
    \begin{tabular}[c]{@{}l@{}}1024×1024\\      (zero-padding)\end{tabular} \\ 
  \bottomrule[2pt]
\end{tabular}
}
\caption{\textbf{Overview of the state-of-the-art for image manipulation localization end-to-end models} \textit{View} can be regarded as prior knowledge widely accepted in the field of manipulation detection. Edge information can better trace visible artifacts, while noise and high-frequency features mainly compare the low-level differences between tampered and authentic regions.}
\label{tab:1}
\end{table*}



As shown in Table \ref{tab:1}, most existing methods for IML tasks greatly benefit from tracing artifacts with various CNN-based feature extractors. ``Artifacts" refer to unique visible traces (as shown in Figure \ref{fig:artifact}) and invisible low-level feature inconsistencies (e.g., noise or high-frequency) resulting from manipulation.
% An artifact refers to a distinctive, noticeable visual (e.g. Figure \ref{fig:artifact}) and invisible low-level (e.g. noise or high-frequency inconsistent) features in an image that results from manipulation. 
As tampering aims to deceive the audience by creating semantically meaningful and perceptually convincing images, visual traces typically manifest at a non-semantic level, distributed in textures around the manipulated area.
Additionally, low-level features, like noise inconsistencies introduced by different cameras, can also serve as crucial evidence to reveal manipulated regions within the authentic area.
Thus, based on previous works, \textit{the key to IML lies in capturing the  artifacts by identifying non-semantic visible traces and low-level inconsistencies.}



% In addition, low-level features such as noise and high-frequency inconsistencies from different cameras can also be crucial evidence to uncover manipulated regions within the authentic area. 

% and capturing the discrepancies between the manipulated and authentic regions.
% } 

However, convolution propagates information in a \textit{collective} manner, making CNNs more suitable for semantic-related tasks, such as object detection, rather than tracing non-semantic artifacts that often surround an object. Further, to identify low-level inconsistencies, we need to explicitly compare the relationships between different regions. But in deeper networks, CNNs may overlook global dependencies~\cite{ERF_2016}, rendering them less effective in capturing differences between regions.
Given the weaknesses of CNN in non-semantic and long-distance modeling, we ask: \textit{Is there any other optimal backbone for solving IML tasks?}

Considering the goal of capturing the feature discrepancies between the manipulated and authentic regions, we argue that self-attention should be a better solution regarding IML. \textit{As self-attention can explicitly model relationships between any areas regardless of their visual semantic relevance, especially for non-adjacent regions.} The performance boost achieved by SPAN~\cite{SPAN_2020} highlights the effectiveness of integrating self-attention structures into convolutional layers.
Furthermore, as artifacts are often distributed at the patch level rather than at the pixel or image level, Vision Transformer (ViT)~\cite{ViT_2021} naturally becomes the ideal choice to trace artifacts and make comparisons. 

While ViT may be suitable for IML tasks, directly applying the original ViT architecture is insufficient. We suggest that IML involves three key discrepancies from traditional segmentation tasks, which also have not yet received sufficient attention in previous IML methods, as supported by Table 1. These discrepancies are:

  % 接下来一大段都是上面的替代品或者展开
  % The significance of artifacts in IML has led us to question whether the CNN architecture is optimal for tracking artifacts. Given that tampered content often comes from a real image, it can be challenging to detect suspicious traces if we only focus on the tampered or authentic region. Therefore, \textit{the key to locating artifacts lies in comparing the differences between the manipulated and authentic regions.}

  % In most cases, artifacts are more likely to be discovered at the junction between the authentic region and the tampered region. To address this challenge, we propose using self-attention as a better choice for artifact tracking. The self-attention mechanism can explicitly exploit relationships and interactions between different regions, making it more suitable for comparison. In contrast, CNNs may ignore some global dependencies between different regions~\cite{ERF_2016}, thereby making comparison less effective.

  % This approach has been shown to be effective in recent research. For example, the SPAN model~\cite{SPAN_2020} has made progress in artifact tracking by adding self-attention structures to some of the convolutional layers in the ManTra-Net model~\cite{Mantra_2019}. This highlights the potential benefits of using self-attention for artifact tracking and supports our hypothesis that the CNN architecture may not be optimal for this task.

  % Driven by the significance of artifacts, we raise a question: \textit{Is the CNN architecture optimal for tracking artifacts?} As tampered content often comes from a real image, it is difficult to capture suspicious traces if we only focus on this region, same with the authentic area. Besides, the key to locating artifacts lies in \textit{comparing} the differences between the manipulated and authentic regions.

  % In most cases, artifacts are more likely to be discovered at the junction between the authentic region and the tampered region.
  %  With this goal in mind, self-attention may be a better choice. Since the self-attention mechanism can explicitly exploit relationships and interactions between different regions, and is therefore more suitable for comparison. In contrast, CNNs typically use convolutional kernels to capture information from different regions through local receptive fields, this method may ignore global dependencies between different regions, making comparison less effective. Recnet example where SPAN~\cite{SPAN_2020} made progress by adding self-attention structure to some of convolution layers in ManTra-Net\cite{Mantra_2019} proved this suppose.

  % Although CNNs can also capture information from different regions through local receptive fields, Therefore, we believe that when performing comparison tasks, explicitly using the self-attention mechanism is superior to using CNNs.”

% 从尺度说明为什么是patch level 的ViT
% To further discuss the scale of comparison, it's worth noting that pixel-to-pixel comparison cannot capture artifacts appearing in a certain area, while comparing the whole image with every other image is too computationally expensive. At this point, patch-level comparison becomes the optimal choice to balance the two. Therefore, we believe that ViT is a natural choice for solving this problem, as it enables patch-level comparison using self-attention.

% Overall, our approach aims to improve the accuracy of artifact tracking by explicitly comparing different regions of the image, using self-attention to highlight the key differences. We believe that this approach has the potential to outperform existing methods, and we look forward to testing it in future research.

% 下面一整段都是从 benchmark的角度切入讨论，但是现在看有点多余和不必要。
  % As shown in table \ref{tab:1}, many state-of-the-arts were proposed to solve this problem. However, we observed that although these models have proven their performance by \textit{testing} on some publicly available datasets, it is still difficult to compare them fairly. Some works \textit{trained} their model with a huge, private, synthesized dataset, which brings difficulties for replication and comparison. 
  %  下文这个example可以删
  % For example, Mantra-Net~\cite{Mantra_2019} uses a private dataset of 102,208 images with 385 kinds of manipulation type, while RGB-N~\cite{RGBN_2018} is trained using a randomly generated dataset with more than 42k images, and ObjectFormer~\cite{objectformer_2022} utilizes a synthetic dataset based on MSCOCO~\cite{mscoco_2014}. In contrast, a commonly used IML public dataset, CASIAv2, has only a few thousand images.
  % In addition, we noticed that some "test-set oriented" data augmentation approaches may be key to raising points, but some authors obscured this information.
  % Such gaps make us wonder whether the excellence of the performance comes from the model itself or from the other factors. It is understandable that there is some chaos in the field when exploratory stage, however, IML is in need of a common benchmark to fairly evaluate the performance of each Image Manipulation Localization methods.

  % These various inconsistencies make it difficult to measure the performance of different models.
  % The major reason is the discrepancy of training datasets, 
  %  in other words, there is an absence of a common benchmark. 
  % Unsurprisingly, the state-of-the-arts are widely sharing backbone networks with semantic segmentation, especially CNNs~\cite{CNN_1989} like VGG~\cite{VGG_2015} or Resnet~\cite{Resnet_2016}. A slight exception~\cite{objectformer_2022} has the structure of convolution at first to create tokens and then feed them to the Transformer~\cite{transformer} for final prediction. 
  % Although these models have proven their performance on some publicly available datasets, it is still difficult to compare them fairly, in other words, there is an absence for a common benchmark. The major reason is the discrepancy of training datasets, for example, Mantra-Net~\cite{Mantra_2019} uses a private dataset of 102,208 images with 385 kinds of manipulation type, while RGB-N~\cite{RGBN_2018} is trained using a randomly generated dataset with more than 42k images, and ObjectFormer~\cite{objectformer_2022} utilizes a synthetic dataset based on MSCOCO~\cite{mscoco_2014}. Further, there are similar inconsistencies in data augumentation and metrics. Such a gap makes us wonder whether the excellence of the performance comes from the model itself or from the other factors. It is understandable that there is some chaos in the field when exploratory stage, however, IML is in need of a common benchmark to fairly evaluate the performance of each Image Manipulation Localization methods.

% 不能直接说出用了iml vit 要先说明我们决定用Vit，但是是不是直接把ViT套上来就完了呢？引出三点对于IML任务的理解，这三个点是IML与其他视觉任务完全不同的点，再说我们基于此建立了IML vit

\textbf{High resolution}
While semantic segmentation and IML share similar inputs and outputs, IML tasks are more information-intensive, focusing on detailed artifacts rather than macro-semantics at the object level. Existing methods use various extractors to trace artifacts, but their resizing methods already harm these first-hand artifacts.
Therefore, preserving the \textit{original resolution} of the images is crucial to retain essential artifacts for the model to learn.


% While it is true that semantic segmentation and IML have similar inputs and outputs, IML tasks are more information-intensive and need to take care of detailed artifacts rather than macro-semantics at the object level. 
% In contrast, segmentation or object detection often only acquires macro-semantics at the object level and has a higher redundancy for detailed differences, making it more information-sparse.
% Therefore, retaining the \textit{original resolution} of the images is crucial to preserve details and avoid harming the artifacts and low-level visual features. By doing so, the model can better discern the features of tampered regions.

% 这段有点啰嗦了，该删就删减
% Artifacts can be interpreted as a combination of in-camera clues(like different noise distributions coming from different cameras) and out-camera clues(various traces on the edge of the tampered region).  Only by interpreting clues from these small regions, especially the dissimilarity between the tampered and authentic regions, can effective recognition be made; in other words, IML tasks are information-dense tasks. In contrast, segmentation or object detection often only acquires macro-semantics at the object level and has a higher redundancy for detailed differences, making it more information-sparse.

\textbf{Edge supervision} 
As mentioned earlier, IML's primary focus lies in detecting the distinction between the tampered and authentic regions. This distinction is most pronounced at the boundary of the tampered region, whereas typical semantic segmentation tasks only require identifying information within the target region. From another perspective, it becomes evident that visible artifacts are more concentrated along the periphery of the tampered region rather than within it (as shown in Figure \ref{fig:artifact}). Consequently, the IML task must guide the model to concentrate on the manipulated region's edges and learn its distribution for better performance.

\textbf{Multi-scale supervision} The percentage of tampered area to the total area varies significantly across different IML datasets. CASIAv2~\cite{CASIA_2013} contains a considerable amount of sky replacement tampering, whereas Defacto~\cite{defacto_2019} mostly consists of small object manipulations. On average, CASIAv2 has 7.6\% of pixels as tampered areas, while Defacto has only 1.7\%. Additionally, IML datasets are labor-intensive and often limited in size, which poses challenges in bridging the gap between datasets.
Therefore, incorporating multi-scale supervision from the pre-processing and model design stages is essential to enhance generalization across different datasets.

%(ii) Multi-scale must take into consideration for IML.  
%This mainly comes from two aspects, one is the specificity of the way of tampering, for example, changing the sky will tamper with nearly half of an image, while a common face replacement may only affect a very small region of an image. The second is that the distribution of tampered areas in different datasets is extremely dissimilar; in CASIAv2~\cite{CASIA_2013}, for example, 7.65\% of the region is tampered with on average, while Defacto~\cite{defacto_2019} has almost only 1.7\% of the area tampered with(calculated from the pixel level). In order to improve the generalization performance, the necessary pre-processing and FPN~\cite{FPN_2017} structures need to be applied.

% When considering the specific structure suitable as a benchmark, we find that CNN-based methods dominates the IML problem. Most of the work has focused on constructing various nifty feature extractors to mine the priori knowledge such as noise, high frequency information, etc. Meanwhile, the role of backbone is not so obvious here. There is experimental evidence that vanilla FCN~\cite{FCN_2015} and DeepLabv2~\cite{Deeplab_2018} can perform well in the training set, but are difficult to generalize on non-homologous datasets~\cite{GSR_Net_2020}.  We question this: \textit{are CNNs really the optimal choice for parsing such non-semantic information comes from artifacts? } Because for a convolution layer, feature vectors (vectors at the channel direction in the feature map) at different points have only linear operations on each other. Information is passing through layers \textit{collectively} but less consideration about the similarity between feature vectors at the same layer. This bottom-up manner can be effective for missions like object detection because it cares more about the macroscopic semantic, but there is less capture and fewer secondary understanding of detailed textures when the layers are deepened~\cite{ERF_2016}, especially the relationship between pixels and patches.

% Instead, SPAN~\cite{SPAN_2020} introduces the self-attention block based on ManTra-Net~\cite{Mantra_2019} architecture, and ObjectFormer~\cite{objectformer_2022} adopts the convolution-first-then-Transformer approach.
% These successes led us to consider that self-attention may have some advantages for solving IML problems, inspiring the attempt to tackle the problem using Vision Transformer (ViT)~\cite{ViT_2021} as the backbone. As feature vectors in self-attention have inner product operations on each other, the similarity between vectors is taken into account, i.e., the relevance between different pixels is compared one by one in each block. Further, Transformer was born from NLP, which is more suitable for dense semantics, and since ViT has been successful, IML tasks with more dense information than traditional vision tasks should be more likely to succeed.

In this paper, we present IML-ViT, an end-to-end ViT-based model that solves IML tasks. Regarding the 3 key discrepancies, we devise IML-ViT with the following components:
1) a ViT which accepts high-resolution input.  Most of the global attention block is replaced with windowed attention as the trade-off for time complexity. We initialized its parameters with ImageNet-1k MAE~\cite{MAE_2022} pre-training; 2) a \textit{simple feature pyramid}~\cite{benchmark_ViT_2021} network to introduce multi-scale supervision; 3) a morphology-based edge loss strategy is proposed to ensure edge supervision. The overview of IML-ViT is shown in Figure \ref{fig:overall}.

% Aiming at propose a simple but superior performance benchmark. In this paper, guided by the above points, we propose IML-ViT, a effective end-to-end training paradigm for Image manipulation localization based on ViT~\cite{ViT_2021}. Specifically, IML-ViT contains: 1) a ViT which accepts high-resolution input, as the backbone.  Part of the global attention block is replaced with windowed attention as the trade-off for time complexity. We initialized its parameters with ImageNet-1k MAE pre-training~\cite{MAE_2022}; 2) a \textit{simple feature pyramid}~\cite{benchmark_ViT_2021} architecture to introduce multi-scale supervision;3) a morphology-based edge loss strategy is applied during training to ensure edge supervision. Overviews are shown in Figure \ref{fig:overall}. 

To the best of our knowledge, ObjectFormer~\cite{objectformer_2022} and TransForensics~\cite{transforensic_2021} are the only Transformer-related models solving the IML tasks. However, their backbone distinguishes significantly from ViT, as will explain in the Related Works section. Thus, IML-ViT can be regarded as the pioneering model that utilizes ViT as the backbone for tackling IML tasks.


The experiments were conducted based on a common evaluation protocol~\cite{objectformer_2022, RGBN_2018,MVSS_2021,Mantra_2019,GSR_Net_2020} to measure the generalizability and performance of our IML-ViT.  
% Model is trained on CASIAv2~\cite{CASIA_2013} dataset first and then test it on other public datasets including NIST16~\cite{NIST16_2019}, Columbia~\cite{Columbia_2006}, COVERAGE~\cite{Coverage_2016}, and CASIAv1\cite{CASIA_2013}.
Model is trained on CASIAv2~\cite{CASIA_2013} dataset and then test it on other 5 public datasets.
The results demonstrate that IML-ViT has surpassed all SoTA models, indirectly validating the reliability of the three key aspects of IML proposed by us. Thus, we believe that IML-ViT is a powerful candidate to become a new SoTA model for IML.

With no specialized modules,  IML-ViT has the potential to serve as a simple yet superior performance benchmark for IML and demonstrate that IML tasks can be solved without manually designed feature extractors or complicated feature fusion, promoting existing IML research into new research paradigms. 
In summary, our contributions are as follows:

\begin{itemize}
  \item We reveal the essential discrepancies between IML and traditional segmentation tasks by raising the three uniqueness, which were overlooked by previous studies: high resolution, multi-scale, and edge supervision.
  
  \item  Aiming at three uniqueness, we modify the components of ViT and establish the IML-ViT, the first ViT-based model for image manipulation localization.
  
  \item Extensive experiments show that IML-ViT outperforms state-of-the-art models in both $F_1$ and AUC scores on five public benchmark datasets. This verifies the solidity of the three uniqueness we proposed.
  

  % \item IML-ViT can be taken as a standard benchmark to measure the performance of various algorithms for Image Manipulation Localization and end the chaos of the lack of standards in previous studies.
\end{itemize}

\section{Related works}
\subsection{Transformer-based IML Method}
Currently, there are two Transformer-based models in the field of IML, namely ObjectFormer~\cite{objectformer_2022} and TransForensics~\cite{transforensic_2021}. 
% Despite incorporating ViT techniques like patching and position embeddings,
Though named ``Trans" and ``Former", these two models are hardly in line with ViT in overall structures and design philosophies.

In particular, different from ViT directly embedding the patched images before encoding, both methods utilize several CNN layers to initially extract feature maps and subsequently employ Transformers for further encoding, leading to the neglect of crucial first-hand low-level information. 
% However, IML-ViT takes a different approach by directly embedding the patched images as the vanilla ViT. 

Moreover, in ObjectFormer(OF)'s encoder, the ``query" inputs are learnable vectors representing object prototypes $o_i$, not image embeddings. As a result, it focuses on capturing dependencies between object prototypes and image tokens, whereas a standard ViT encoder solely models the dependencies between image embeddings. Besides, OF is pre-trained with a large tampering-oriented synthesized private dataset, while IML-ViT achieves superior performance  with pre-training on the more accessible ImageNet-1k dataset and outperforms OF. 

On the other hand,
the primary distinction between TransForensics and ViT lies in how to apply Transformer blocks. While ViT uses these blocks sequentially, TransForensics employs them in parallel, wherein each feature map of an FCN output is decoded with a Transformer block, then fused together for the final output.

In short, IML-ViT can be considered the first IML method with a vanilla ViT as its backbone and could easily benefit from recently advanced algorithms related to ViT, proving that IML tasks do not require complex designs.

\par
\textbf{Paradigm of IML} 
Research in the early years focused on single kind of manipulation detection, with studies on copy-move~\cite{Cozzolino_Poggi_Verdoliva_2015b,Rao_Ni_2016}, splicing~\cite{Cozzolino_Poggi_Verdoliva_2015a,Huh_Liu_Owens_Efros_2018,Kniaz_Knyaz_Remondino}, and Removal (Inpainting)~\cite{Zhu_Qian_Zhao_Sun_Sun_2018}, respectively. However, since the specific type of tampering is unknown in practice, after 2018, general manipulation detection has become the focus of research. 
Many existing works follow the paradigm of ``feature extraction + backbone inference", especially extractors to exploit tamper-related information from artifacts. Yang \textit{et al.} design CR-CNN~\cite{CR_CNN_2020} with a noise-sensitive BayarConv~\cite{Bayar_2018} as the first convolution layer. Zhou  \textit{et al.}~ develop an SRM filter to mine the difference in noise distribution to support decision-making in their RGB-N networks\cite{SRM_2018}. And Wu \textit{et al.}~\cite{Mantra_2019} and Hu \textit{et al.} ~\cite{SPAN_2020} combined SRM, BayarConv, and Conv2D as the first layer of their model. Besides noise-related extractors, Wang \textit{et al.}~\cite{objectformer_2022} employ a DCT transform to extract high-frequency features, which are then combined with RGB features and fed into a transformer decoder. And Chen \textit{et al.} suggest MVSS-Net~\cite{MVSS_2021} with a Sobel-supervised edge branch and a BayarConv noise branch. Dual attention is then utilized to fuse them. 

Nevertheless, a feature may only be effective for a single type of tampering, e.g., noise is more sensitive to splicing from different images but less effective for copy-move from the same image. To avoid this issue, our IML-ViT is aiming to step out of the paradigm of ``extraction + fusion" and let the model itself learn as much knowledge as possible from the datasets rather than simply rely on \textit{priori knowledge}. 

% \par
\textbf{Resolution of IML} 
Resolution is an important aspect but has always been neglected. Wu \textit{et al.}~\cite{Mantra_2019} reported resize(re-scale) to images can do harm to the performance of their model. However, we observed that most existing methods re-scale images to a uniform size during pre-processing for parallel training purposes (shown in \textit{Resolution} column Table \ref{tab:1}). In most cases, the images were down-sampled, destroying the aspect ratio and compressing the original information in the dataset. This manner can greatly affect the distribution of noise, edge, and high-frequency features in the raw image, then weaken the performance of the model. In this paper, we make a novel attempt of preserving the original resolution of the images as much as possible, by zero-padding them to 1024x1024 for parallel computation. This process can leave the freshest information for the model to mine the difference between the manipulated region and the authentic region, and make the best possible use of the dataset.


% Figure environment removed

% Our goal is to construct a Vision Transformer-based end-to-end image manipulation localization model, aiming at being a new benchmark for this task. Based on this premise, rather than building \textit{overly complex} models, even though this could further improve preformance, we chose to adopt as general a structure as possible. However, even with such concessions, IML-ViT succeeds in outperforming the current state of the art.

% we chose not to design an \textit{overly complex} structure, although it is likely to further improve performance. Even so, the current \textit{simple} structure can already achieve state-of-the-art results. We devote more effort to analyzing what components are compulsory when solving IML tasks using ViT.


\section{Proposed Method}
In this section, we introduce our powerful IML-ViT paradigm, as shown in Figure \ref{fig:overall}, it consists of three main components: (1) a windowed ViT to balance the high-resolution inputs and the space complexity; (2) a \textit{simple feature pyramid} network (SFPN) to introduce multi-scale features; and (3) a lightweight MLP decoder head with additional edge supervision, which aids in focusing on artifact-related features and ensures stable convergence.

\subsection{ViT Backbone}

\textbf{High resolution}
The ViT Encoder aims to mine the detailed artifacts and explore the differences between the suspicious area. Thus, it is essential to \textbf{preserve the original resolution of each image} to avoid downsampling that could potentially distort the artifacts. However, when training in parallel, all images within a batch must have the same resolution. To reconcile these demands, we adopt a novel approach that has not been applied to any IML method before. Rather than simply rescaling images to the same size, we pad images and ground truth masks with zeros and place the image on the top-left side to match a larger constant resolution. This strategy maintains crucial low-level visual information of each image, allowing the model to explore better features instead of depending on handcrafted prior knowledge. To implement this approach, we first adjust the embedding dimensions of the ViT encoder to a larger scale. 

\textbf{Windowed attention} To balance the computation cost from high resolution, we adopt a technique from previous works~\cite{MViTv2_2022,benchmark_ViT_2021}, which periodically replaces part of the global attention blocks in ViT with windowed attention blocks. This method ensures global information propagation while reducing complexity.

\textbf{MAE pre-train} We initialize the ViT with parameters pre-trained on ImageNet-1k~\cite{imagenet_2009} with Masked Auto Encoder (MAE)~\cite{MAE_2022}. This self-supervised method can greatly alleviate the over-fitting problem and helps the model generalize, supported by Table \ref{tab:ablation}.

More specifically, we represent input images as $X\in \mathbb{R}^{3\times h \times w }$, and ground truth masks as $M\in \mathbb{R}^{1\times h \times w }$, where $h$ and $w$ correspond to the height and width of the image, respectively. We then pad them to $X_p \in \mathbb{R}^{ 3 \times H \times W }$ and $M_p \in \mathbb{R}^{ 1 \times H \times W }$. Balance with computational cost and the resolution of datasets we employ in Table~\ref{tab:datasets}, we take $H=W=1024$ as constants in our implementation. Then $X_p$ is passed into the windowed ViT-Base encoder with 12 layers, with a complete global attention block retained every 3 layers. The above process can be formulated as follows:
\begin{equation}
G_e=\mathcal{V}(X_p) \in \mathbb{R}^{768 \times \frac{H}{16}\times \frac{W}{16} }
\end{equation}
where $\mathcal{V}$ denotes the ViT, and $G_e$ stands for encoded feature map. The number of channels, 768, is to keep the information density the same as the RGB image at the input, as $ 768 \times \frac{H}{16} \times  \frac{W}{16}  =  3 \times H \times W $.

\subsection{Simple Feature Pyramid}
To introduce multi-scale supervision, we adopt the \textit{simple feature pyramid} network (SFPN) after the ViT encoder, which was suggested in ViTDet~\cite{ViTDet_2022}. 
This method takes the single output feature map $G_e$ from ViT, and then uses a series of convolutional and deconvolutional layers to perform up-sampling and down-sampling to obtain multi-scale feature maps:
\begin{equation}
F_i = \mathcal{C}_i(G_e)  \in  \mathbb{R}^{ C_{S}\times \frac{H}{2^{i+2}}\times \frac{W}{2^{i+2}} } , i \in \{1,2,3,4\}
\end{equation}

where $\mathcal{C}_i$ denotes the convolution series, and $C_S$ is the output channel dimension for each layer in SFPN. 
% In our paradigm, as the default ViT output is a scale of $\frac{1}{16}$ compared to the original input size, then the resulting hierarchical scale of features from the Simple Feature Pyramid is $\{ \frac{1}{4}, \frac{1}{8}, \frac{1}{16}, \frac{1}{32}\}$.
This multi-scale method does not change the base structure of ViT, which allowed us easily introduce recently advanced algorithms to the backbone.

% Figure environment removed

\subsection{Light-weight Predict Head}
For the final prediction, we aimed to design a model that is simple enough to reduce memory consumption while also demonstrating that the improvements come from the advanced design in the ViT Encoder and the multi-scale supervision. Based on these ideas, we adopted the decoder design from SegFormer~\cite{SegFormer_2021}, which outputs a smaller predicted mask $M_e$ with a resolution of $1 \times \frac{H}{4}\times \frac{W}{4}$. The lightweight All-MLP decoder first applies a linear layer to unify the channel dimension. It then up-samples all the features to the same resolution of $  C_D \times \frac{H}{4}\times \frac{W}{4}$ with bilinear interpolation, and concatenates all the features together, as shown in Figure \ref{fig:decoder}. Finally, a series of linear layers is applied to fuse all the layers and make the final prediction. We can formulate the prediction head as follows:
\begin{equation}
% P = \mathcal{D} (Concate(Linear(F_i))) \in \mathbb{R} ^{\frac{H}{4}\times \frac{W}{4} \times 1} ,  \forall i
P = \, MLP\{ \odot_i (W_iF_i+b_i) \}  \in \mathbb{R} ^{\frac{H}{4}\times \frac{W}{4} \times 1} 
\end{equation}
Here, $P$ represents the predicted probability map for the manipulated area; $\odot$ denotes concatenation operation, and $MLP$ refers to an MLP module.

% where $P$ denotes the predict probability map for the manipulated area, and $\mathcal{D}$ is the MLP decoder. 

\subsection{Edge Supervision Loss}
% CHAT version
To account for the fact that artifacts are typically more prevalent at the edges of tampered regions, where the differences between manipulated and authentic areas are most noticeable, we developed a strategy that places greater emphasis on the boundary region of the manipulated area. Specifically, we generate a binary edge mask $M^\star$ from the original mask image $M$ using mathematical morphology operations including dilation ($\oplus$) and erosion ($\ominus$) \cite{morphology_1983image}, followed by taking the absolute values of the result. The formula we use to generate the edge mask is:
\begin{equation}
M^\star = |(M \ominus B(k)) - (M \oplus B(k))|
\end{equation}
where, $B(x)$ generates a $(2x+1) \times (2x+1)$ \textit{cross} matrix, where only the $x^{th}$ column and $x^{th}$ row have a value of 1, while the rest of the matrix contains 0s. The integer value $x$ is selected to be approximately equal to the width of the white area in the boundary mask. Examples of the edge mask generated using this approach are shown in Figure \ref{fig:edge}.

\textbf{Combined Loss}
To compute the loss function, we first pad the ground-truth mask $M$ and the edge mask $M^\star$ to the size of $H \times W$, and refer to them as $M_p$ and $M^\star_p$, respectively. We then calculate the final loss using the following formula:
\begin{equation}
\mathcal{L} = \mathcal{L}{seg}(P, M_p) + \lambda \cdot \mathcal{L}{edge}(P * M^\star_p, M_p * M^\star_p)
\end{equation}
where $*$ denotes the point-wise product, which masks the original image. Both $\mathcal{L}{seg}$ and $\mathcal{L}{edge}$ are binary cross-entropy loss functions, and $\lambda$ is a hyper-parameter that controls the balance between the segmentation and edge detection losses. By default, we set $\lambda = 20$ to guide the model to focus on the edge regions.

While this strategy is straightforward, as will discuss in the Experiments section, it remarkably accelerates model convergence, stabilizes the training process, and mitigates potential NaN (Not-a-Number) issues. Therefore, we consider this strategy to be a powerful prior knowledge for IML problems, deserving attention in future research.

% where the most significant differences between the manipulated and authentic regions are likely to occur. 

% This strategy also mitigates the issue of highly imbalanced data in the IML problem, as the model would tend to output a mask of all zeros if most of the regions are zero-padded.


% RAW version
% To capture dissimilarities between tampered and authentic regions, many hand-crafted feature extractors, such as the BayarConv2D noise extractor and DCT High-frequency extractor~\cite{objectformer_2022}, have been proposed and widely used to exploit additional information from artifacts. However, while these artificial extractors may work, they are not optimal. Features may only be effective for one type of tampering; for example, noise is more sensitive to splicing from different images but less effective for copy-move from the same image. To address this, we have adopted a more generalized solution.
% % edge loss 的直觉设计
% Since artifacts are more commonly distributed at the edges of tampered regions and there is a more noticeable difference between the manipulated and authentic regions at the edges, we have developed a strategy to give more weight to the boundary region. This guides the model to focus on the edge regions. Our approach involves generating a binary edge mask $M^\star $ from the mask image $M$ using dilation $\oplus$ and erosion $\ominus$ operations in mathematical morphology~\cite{morphology_1983image}, and then taking the absolute values of it. More precisely, the way we generate the edge mask is based on this formula:
% \begin{equation}
%   M^\star  = |(M \ominus B(k)) - (M \oplus B(k))|
% \end{equation}
% where $B(x)$ takes an integer $x$ then generates a $(2x+1) \times (2x+1)$ "cross" matrix which has only the $x^{th}$ column and $x^{th}$ row as 1 and the rest as 0. $2x$ is approximately equal to the width of the white area of the boundary mask. Examples shown in Figure \ref{fig:edge}. 

% \subsection{Loss functions}
% We use bilinear interpolation to upsample the prediction $P$ to full size with $H \times W$ resolution.
% For normal segmentation loss $\mathcal{L}_{seg}$ is compute between the padded mask $M$ and 


% Then we use $M_e$ to mask the $P$ and $M$ separately with pointwise product and compute the edge loss:
% \begin{align}
%   l'_i &= -M_{ei} \times l_i \\
%   \mathcal{L}_{edge} &=  \frac{\sum_{i}^{H\times W} l'_{i}}{H\times W}
% \end{align}
% where $M_{ei}\in\{0,1\}$.

% \textbf{Predict Loss}

% The meet the full size of input image, we simply bilinear up-sampled the prediction $M_e$  to $H \times W \times 1$. Then compute the pixel level BCE loss $\mathcal{L}_{seg}$ after remove the padding region.
% % \begin{align}
% %   l_{i}=-[M_{i} \cdot \log P_{i}& + (1-M_{i})\cdot \log(1-P_{i})] \\
% %   \mathcal{L}_{seg} & = \frac{\sum_{i}^{H\times W} l_{i}}{H\times W}
% % \end{align}
% % \begin{equation}
% %   \mathcal{L}_{seg}=-\sum_{i,j}[M_{i,j} \cdot \log P_{i,j} + (1-M_{i,j})\cdot \log(1-P_{i,j})]
% % \end{equation}
% \textbf{Edge loss}
% % 可以丢，主要是阐释为什么特征提取器不好
% To capture dissimilarities between tampered and authentic regions, many hand-crafted feature extractors, such as the BayarConv2D noise extractor and DCT High-frequency extractor~\cite{objectformer_2022}, have been proposed and widely used to exploit additional information from artifacts. However, while these artificial extractors may work, they are not optimal. Features may only be effective for one type of tampering; for example, noise is more sensitive to splicing from different images but less effective for copy-move from the same image. To address this, we have adopted a more generalized solution.
% % edge loss 的直觉设计
% Since artifacts are more commonly distributed at the edges of tampered regions and there is a more noticeable difference between the manipulated and authentic regions at the edges, we have developed a strategy to give more weight to the boundary region. This guides the model to focus on the edge regions. Our approach involves generating a binary edge mask $M_e$ from the mask image $M$ using dilation $\oplus$ and erosion $\ominus$ operations in mathematical morphology~\cite{morphology_1983image}, and then taking the absolute values of it. This can be summarized in the following equation:

% \begin{equation}
%   M_e = |(M \ominus B(k)) - (M \oplus B(k))|
% \end{equation}
% where $B(x)$ takes an integer $x$ then generates a $(2x+1) \times (2x+1)$ "cross" matrix which has only the $x^{th}$ column and $x^{th}$ row as 1 and the rest as 0. $2x$ is approximately equal to the width of the white area of the boundary mask. Examples shown in Figure \ref{fig:edge}. Then we use $M_e$ to mask the $P$ and $M$ separately with pointwise product and compute the edge loss:
% \begin{align}
%   l'_i &= -M_{ei} \times l_i \\
%   \mathcal{L}_{edge} &=  \frac{\sum_{i}^{H\times W} l'_{i}}{H\times W}
% \end{align}
% where $M_{ei}\in\{0,1\}$.

% \textbf{Combined loss}
% Finally the combined loss can be expressed using the following formula:
% \begin{equation}
%   \mathcal{L} = \mathcal{L}_{seg} + \lambda \cdot \mathcal{L}_{edge}
% \end{equation}
% where $\lambda$ is weight for balance. In order to give more weight to the edge regions, we set $\lambda=20$ during training, which also alleviated the issue of highly imbalanced data in the IML problem. Otherwise, the model would tend to output a mask that is all zeros because most of the regions are zero-padded.
% Figure environment removed


\section{Experiments}
% The experimental design in the field of image manipulation localization is currently very chaotic. We hope to clarify some of the characteristics in the experimental section and create a general benchmark.
\subsection{Experimental setup}
\textbf{Datasets} 
To make a fair comparison with state-of-the-art, we mainly evaluate the performance of our model based on a commonly used protocol~\cite{MVSS_2021,GSR_Net_2020,SPAN_2020} for image tampering localization. We train the model on the CASIAv2~\cite{CASIA_2013} dataset\footnote{We noticed some resolution errors in the public CASIAv2 dataset. Therefore, we have released an adjusted version of CASIAv2 with corrected ground truth. Details can be found at \url{https://github.com/SunnyHaze/CASIA2.0-Corrected-Groundtruth}}
and then test it on other smaller public datasets including CASIAv1~\cite{CASIA_2013}, NIST16~\cite{NIST16_2019}, COVERAGE~\cite{Coverage_2016}, Columbia~\cite{Columbia_2006} and Defacto~\cite{defacto_2019}, see Table \ref{tab:datasets}. However, there are no authentic images as negative examples in the Defacto dataset. Following the approach of MVSS-Net, we randomly selected 6000 untouched images from MS-COCO dataset~\cite{mscoco_2014} and combined them with 6000 images from Defacto to create the Defacto-12k dataset for validation. 

\begin{table}[h]
\centering
\resizebox{.95\columnwidth}{!}{
\begin{tabular}{@{}lllllllll@{}}
\toprule[2pt]
\multicolumn{1}{c}{\multirow{2}{*}{\textbf{Usage}}} &
  \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Dataset}}} &
  \multicolumn{2}{c}{\textbf{Type}} &
  \multicolumn{3}{c}{\textbf{Manipulation type}} &
  \multicolumn{2}{c}{\textbf{Resolution}} \\ \cmidrule(l){3-9} 
\multicolumn{1}{c}{} &
  \multicolumn{1}{c}{} &
  \multicolumn{1}{c}{\textbf{Authentic}} &
  \multicolumn{1}{c}{\textbf{Manipulated}} &
  \multicolumn{1}{c}{\textbf{copymv}} &
  \multicolumn{1}{c}{\textbf{spli}} &
  \multicolumn{1}{c}{\textbf{inpa}} &
  \multicolumn{1}{c}{\textbf{min}} &
  \multicolumn{1}{c}{\textbf{max}} \\ \midrule
Train                 & CASIAv%2~\cite{CASIA_2013}
& 7491 & 5063 & 3235 & 1828 & 0    & 240 & 800  \\\midrule[0.5pt]
\multirow{5}{*}{Test} & CASIAv1%~\cite{CASIA_2013} 
& 800  & 920  & 459  & 461  & 0    & 256 & 384  \\
                      & NIST16%~\cite{NIST16_2019}
                      & 0    & 564  & 68   & 288  & 208  & 480 & \textbf{5616} \\
                      & COVERAGE%~\cite{Coverage_2016}
                      & 100  & 100  & 100  & 0    & 0    & 158 & 572  \\
                      & Defacto-12k%~\cite{defacto_2019}
                      & 6000 & 6000 & 2000 & 2000 & 2000 & 120 & 640  \\
                      & Columbia%~\cite{Columbia_2006}
                      & 183  & 180  & 0    & 180  & 0    & 568 & \textbf{1152} \\ \bottomrule[2pt]
\end{tabular}
}
\caption{\textbf{Details of six datasets in our experiments}}
% In the "resolution" column, the "min" value represents the minimum edge length among all images in the corresponding dataset, while the "max" value represents the maximum edge length. The NIST and Columbia datasets exceeded the edge length limit we set, so we needed to resize them before inputting them into the model.
\label{tab:datasets}
\end{table}

\textbf{Evaluation Criteria} 
We evaluate our model using pixel-level $F_1$ score with a fixed threshold $0.5$ and Area Under the Curve (AUC), which are commonly used evaluation metrics in previous works. However, it's worth noting that AUC can be influenced by an excessive number of true negative pixels in common IML datasets, leading to an overestimation of model performance. Nevertheless, our model achieves state-of-the-art performance in both $F_1$ score and AUC.

In some previous methods, \textit{optimal F1 score} is used for evaluation, where the best $F_1$ score is selected by testing the prediction probability map at different thresholds. However, this approach becomes impractical in real-world scenarios since the optimal threshold for the real-world distribution is typically unknown. 
Instead, we report the pixel-level $F_1$ score using a uniform threshold of $0.5$, providing a more practical metric for evaluating the model's performance.


\begin{table*}[]
\centering
\resizebox*{1.95\columnwidth}{!}{ 
\begin{tabular}{@{}l|c|ccc|cccccccccc@{}}
\toprule[2pt]
\multicolumn{1}{c|}{\multirow{2}{*}{\textbf{Test   Goal}}} &
  \multirow{2}{*}{\textbf{init method}} &
  \multicolumn{3}{c|}{\textbf{Components}} &
  \multicolumn{2}{c}{\textbf{CASIAv1}} &
  \multicolumn{2}{c}{\textbf{Coverage}} &
  \multicolumn{2}{c}{\textbf{Columbia}} &
  \multicolumn{2}{c}{\textbf{NIST16}} &
  \multicolumn{2}{c}{\textbf{MAEN}} \\ \cmidrule(l){3-15} 
\multicolumn{1}{c|}{} &
   &
  \textbf{H-Reso} &
  \textbf{S-FPN} &
  \textbf{Edge} &
  \textbf{F1} &
  \textbf{AUC} &
  \textbf{F1} &
  \textbf{AUC} &
  \textbf{F1} &
  \textbf{AUC} &
  \textbf{F1} &
  \textbf{AUC} &
  \textbf{F1} &
  \textbf{AUC} \\ \midrule
\multirow{2}{*}{w/o MAE} &
  Xavier &
  + &
  + &
  + &
  0.1035 &
  - &
  0.0439 &
  - &
  0.0744 &
  - &
  0.0632 &
  - &
  0.0713 &
  - \\
 &
  ViT-B ImNet-21k &
  + &
  + &
  + &
  0.5114 &
  - &
  0.1854 &
  - &
  0.2287 &
  - &
  0.1811 &
  - &
  0.2767 &
  - \\
w/o high resolution &
  MAE ImNet-1k &
  - &
  + &
  + &
  0.5061 &
  0.8166 &
  0.2324 &
  0.825 &
  0.5409 &
  0.842 &
  0.2987 &
  0.8212 &
  0.3945 &
  0.8262 \\
w/o multi-scale &
  MAE ImNet-1k &
  + &
  - &
  + &
  0.5996 &
  0.8627 &
  \textbf{0.4457} &
  \textbf{0.8352} &
  0.6125 &
  0.8350 &
  0.1841 &
  0.6767 &
  0.4605 &
  0.8024 \\
w/o edge supervision &
  MAE ImNet-1k &
  + &
  + &
  - &
  0.5432 &
  0.8573 &
  0.2688 &
  0.8272 &
  0.3008 &
  0.7617 &
  0.2347 &
  0.7078 &
  0.3369 &
  0.7885 \\
\textit{\textbf{Full setup}} &
  MAE ImNet-1k &
  + &
  + &
  + &
  \textbf{0.5886} &
  \textbf{0.8668} &
  0.3277 &
  0.8264 &
  \textbf{0.7445} &
  \textbf{0.9076} &
  \textbf{0.2993} &
  \textbf{0.7706} &
  \textbf{0.4900} &
  \textbf{0.8429} \\ \bottomrule[2pt]
\end{tabular}
}
\caption{\textbf{Ablation study of IML-ViT.} Each model is trained for 200 epochs on CASIAv2 dataset \textbf{without authentic images}. Best scores are marked in bold. 
% %\textit{H-Reso} refers to high resolution; \textit{SFPN} refers to simple feature pyramid; and \textit{Edge} refers to edge supervision
}
\label{tab:ablation}
\end{table*}

% \begin{table*}[t]
% \centering
% \resizebox*{1.7\columnwidth}{!}{ 
% \begin{tabular}{@{}c|c|ccc|ccccc@{}}
% \toprule[2pt]
% \multirow{2}{*}{\textbf{Control variables}} &
%   \multirow{2}{*}{\textbf{init method}} &
%   \multicolumn{3}{c|}{\textbf{Components}} &
%   \multicolumn{4}{c}{\textbf{Pixel-level $F_1$}} &
%   \textbf{} \\ \cmidrule(l){3-10} 
%  &
%    &
%   \textbf{H-Reso} &
%   \textbf{SFPN} &
%   \textbf{Edge} &
%   \textbf{CASIAv1} &
%   \textbf{Coverage} &
%   \textbf{Columbia} &
%   \textbf{NIST16} &
%   \textbf{MEAN} \\ \midrule
% \multirow{2}{*}{w/o MAE} & Xavier          & + & + & + & 0.1035 & 0.0439 & 0.0744 & 0.0632     & 0.0713 \\
%                       & ViT-B ImNet-21k & + & + & + &  0.5114    & 0.1854     & 0.2287 & 0.1811      & 0.2767 \\
% w/o high resolution            & MAE ImNet-1k    & - & + & + & 0.5061      & 0.2324 & 0.5409  & 0.2987      & 0.3945 \\
% w/o multi-scale           & MAE ImNet-1k    & + & - & + & \textbf{0.5996} & \textbf{0.4457} & 0.6125 & 0.1841 & 0.4605 \\
% w/o edge supervision            & MAE ImNet-1k    & + & + & - & 0.5432 & 0.2688 & 0.3008  & 0.2347 & 0.3369 \\
% \textit{\textbf{Full setup}}            & MAE ImNet-1k    & + & + & + & 0.5886 & 0.3277 & \textbf{0.7445} & \textbf{0.2993} & \textbf{0.4900}\\ \bottomrule[2pt]
% \end{tabular}
% }
% \caption{\textbf{Ablation study of IML-ViT paradigm.} Each model is trained for 200 epochs on CASIAv2 dataset \textbf{without authentic images}. Best scores are marked in bold. 
% %\textit{H-Reso} refers to high resolution; \textit{SFPN} refers to simple feature pyramid; and \textit{Edge} refers to edge supervision
% }
% \label{tab:ablation}
% \end{table*}

\textbf{Implementation} Our IML-ViT model is implemented with PyTorch and trained on NVIDIA RTX 3090 GPUs for 200 epochs with a batch size of 1 in each GPU (16GB of graphics memory). Accumulate batch strategy is applied with a size of 32. We pad all images to a resolution of 1024x1024, except for those that exceed this limit. For the larger images, we resized them to the longer side to 1024 and maintain their aspect ratio. Before training, following MVSS-Net, common data augmentation techniques were applied, including re-scaling, flipping, blurring, rotation, and various naive manipulations (e.g., randomly copy-moving or inpainting rectangular areas within a single image).
We initialize ViT-B with MAE pre-trained weights on ImageNet-1k and used the AdamW optimizer~\cite{AdamW_2019} with a base learning rate of 1e-4. We scheduled the learning rate using a cosine decay strategy~\cite{cosine_decay_2017}. The early stop technique was employed during training.



\subsection{Ablation Studies}
\label{lab:ablation}
To better evaluate the contributions of each component to the model performance, we conducted experiments with multiple settings and compare them with a \textit{full setup} to test the four aspects we are most concerned about.
For \textit{initialization}, besides \textit{full setup} with MAE pre-training on ImageNet-1k, we test Xavier initialization and ordinary ViT pre-training on ImageNet-21k by classification.
To explore the impact of \textit{high resolution}, we simply resized all images to 512×512 during training instead of our padding strategy. For \textit{edge supervision}, and \textit{multi-scale supervision}, we evaluate them by removing the respective structures from the model.

During ablation studies, We trained model \textit{only} with manipulated images in CASIAv2 and evaluate its pixel-level $F1$ score on CASIAv1, COVERAGE, Columbia, and NIST16 datasets. The experiment setups and the results are shown in Table \ref{tab:ablation}. In conclusion, our findings are:

\textbf{MAE pretrain is mandatory.} 
Indeed, dataset insufficiency is a significant challenge in building ViT-based IML methods. 
% As IML tasks require dense annotation and expert tamper generation, which are very labor-intensive, public data sets for image manipulation localization are small in size, 
As shown in Table \ref{tab:datasets}, public data sets for IML are small in size, which cannot satisfy the appetite of vanilla ViT. As shown in \textit{w/o MAE} % TODO
aspects in Table \ref{tab:ablation}, the use of Xavier initialization to train the model resulted in complete non-convergence. However, while regular ViT pre-training initialization with Imagenet-21k achieves acceptable performance on CASIAv1 which is homologous to CASIAv2, exhibits poor generalization ability on other non-homology datasets.
The results indicate that MAE greatly alleviated the problem of non-convergence and over-fitting of ViT on small datasets. This suggests that MAE pre-training is indispensable for ViT-based image manipulation localization, and demonstrates that MAE is a powerful and effective method for downstream tasks with limited datasets.

\textbf{Edge supervision is crucial.}
The performance of IML-ViT without edge loss shows significant variability with different random seeds, all leading to gradient collapse eventually, where the F1 score reaches 0, and the loss becomes NaN, as shown in Figure \ref{fig:edge_loss}. On the other hand, when employing edge loss, all performance plots for IML-ViT exhibit consistent behavior similar to the blue line in Figure \ref{fig:edge_loss}, enabling fast convergence and smooth training up to 200 epochs. Furthermore, Table \ref{tab:ablation} confirms the effectiveness of edge loss in contributing to the final performance. In summary, these results demonstrate that edge supervision effectively stabilizes IML-ViT convergence and can serve as highly efficient prior knowledge for IML problems.
% Figure environment removed


\begin{table*}[h]
\centering
\resizebox*{1.6\columnwidth}{!}{ 
\begin{tabular}{@{}lcccccc@{}}
\toprule[2pt]
\multirow{2}{*}{\textbf{Method}} & \multicolumn{6}{c}{\textbf{Pixel-level $F_1$ score}}                                                               \\ \cmidrule(l){2-7} 
                               & \textbf{CASIAv1 
                               % \cite{CASIA_2013}
                               } & \textbf{Columbia
                               % \cite{Columbia_2006}
                               } & \textbf{NIST16
                               % \cite{NIST16_2019}
                               } & \textbf{Coverage
                               % \cite{Coverage_2016}
                               } & \textbf{Defacto-12k
                               % \cite{defacto_2019}
                               } & \textbf{MEAN} \\ \midrule
HP-FCN*, ICCV19~\cite{HPFCN_2019}                 & 0.154   & 0.067    & 0.121 & 0.003    & 0.055       & 0.080 \\
ManTra-Net*, CVPR19~\cite{Mantra_2019}           & 0.155   & 0.364    & 0     & 0.286    & 0.155       & 0.192 \\
% SPAN, ECCV20~\cite{SPAN_2020}        
CR-CNN*, ICME20~\cite{CR_CNN_2020}       & 0.405   & 0.436    & 0.238 & 0.291    & 0.132       & 0.300 \\
GSR-Net*, AAAI20~\cite{GSR_Net_2020}      & 0.387   & 0.613    & 0.283 & 0.285    & 0.051       & 0.324 \\
MVSS-Net*, ICCV21~\cite{MVSS_2021}      & 0.452   & 0.638    & 0.292 & 0.453  & 0.137       & 0.394 \\
MVSS-Net (re-trained)& 0.435   & 0.303    & 0.203 & 0.329    & 0.097       & 0.270 \\
MVSS-Net++*, PAMI22~\cite{mvsspp_2022}  & 0.513 & 0.660  & 0.304 & \textbf{0.482} & 0.095 & 0.411 \\
\textit{IML-ViT (ours)}                 & \textbf{0.658}   & \textbf{0.836}    & \textbf{0.339} & 0.425  & \textbf{0.156}   & \textbf{0.482} \\ \bottomrule[2pt]
\end{tabular}
}
\caption{\textbf{Cross-datasets evaluation of SoTA models.} Except for ManTra-Net and HP-FCN, which trained on a privately synthesized dataset, all the methods were trained on CASIAv2 datasets. The best scores are highlighted in bold. Symbol '*' marks the results are quoted from MVSS-Net paper~\cite{MVSS_2021}. We re-train MVSS-Net with their official-released code.}
\label{tab:sota}
\end{table*}

\textbf{High resolution is effective for artifacts.} 
The improved performance shown in Table \ref{tab:ablation} for the \textit{full setup} model across four datasets validates the effectiveness of the high-resolution strategy. However, it is essential to note that the NIST16 dataset shows limited improvement when using higher resolutions. This observation can be attributed to the fact that the NIST16 dataset contains numerous images with resolutions exceeding 2000, and down-sampling these images to 1024 for testing may lead to considerable distortion of the original artifacts, consequently reducing the effectiveness of learned features. Nevertheless, when considering the SoTA score achieved, it becomes evident that IML-ViT can flexibly infer the manipulated area based on the richness of different information types.

\textbf{Multi-scale supervision helps generalize.} However, after applying the SFPN, there is not much improvement in average $F_1$, even the performance slightly decreases on CASIAv1 and Coverage. But in exchange, the results on NIST16 gain a larger boost. Since CASIAv2 and CASIAv1 are homologous datasets, a good performance on CASIAv1 cannot reflect the generalization ability well. Coverage, as a dataset with only 100 splicing images, is somewhat limited as well. Therefore, it is a worthwhile trade-off to achieve more improvement on NIST16, which has more images and diverse manipulation types. This validates that SFPN does bring certain generalization performances to the model.

% Figure environment removed

\subsection{Comparison with SoTA}
\textbf{Evaluation barrier} While recent studies have introduced numerous state-of-the-art models, it remains challenging to compare them on an equal footing. This is partly due to the lack of publicly available code for the models and training processes~\cite{SPAN_2020,objectformer_2022}, as well as the utilization of massive synthesized datasets that are inaccessible to the wider research community~\cite{Mantra_2019,mmnet_2021}. Therefore, we urge for the adoption of open-source to the community and call for the generation strategies for large-scale datasets can be assessed separately from the model performance itself. Such measures are vital for ensuring fairness and promoting continued advancements in this field. To demonstrate the potential of IML-ViT as an effective benchmark, we  conduct a comprehensive comparison with existing models in a fair manner.

% \footnotetext{Since the training code for MVSS-Net~\cite{MVSS_2021} was unavailable, we reproduced it from scratch with their official model code follow their implementations in their paper. However we observes a gap between scores in this model and what they reported.} 



\begin{table}[h]
\centering

\resizebox*{0.95\columnwidth}{!}{ 
\begin{tabular}{@{}lll@{}}
\toprule[2pt]
\textbf{Method}      & \textbf{Pre-train}           & \textbf{$F_1$(\%)} \\ \midrule
RGB-N, CVPR18~\cite{RGBN_2018}        & ImageNet                             & 40.8                   \\
SPAN, ECCV20~\cite{SPAN_2020}        & Private synthesized dataset          & 38.2                   \\
Objectformer, CVPR22~\cite{objectformer_2022} & Private synthesized dataset          & 57.9                   \\
\textit{IML-ViT(Ours)}        & MAE on ImageNet-1k          & \textbf{73.4}                   \\ \bottomrule[2pt]
\end{tabular}
}
\caption{\textbf{Performance comparison with Closed-source methods} All the methods above are fine-tuning with CASIAv2 and are tested with the CASIAv1 dataset.  }
\label{tab:close}
\end{table}



\begin{table}[h]
\centering
\resizebox{0.95\columnwidth}{!}{
\begin{tabular}{@{}llllll@{}}
\toprule[2pt]
\textbf{Method}                & \textbf{CASIAv1}  & \textbf{Coverage} & \textbf{Columbia} & \textbf{NIST16} & \textbf{MEAN}     \\ \midrule

ObjectFormer & 0.882 &- &- &-&- \\
CFL-Net & 0.863 & - & - & 0.799 & - \\
\textit{IML-ViT(Ours)} & \textbf{0.931} & \textbf{0.918} & \textbf{0.962} & \textbf{0.818} & \textbf{0.917} \\
\bottomrule[2pt]
\end{tabular}
}
\caption{\textbf{Comparison of AUC trained on CASIAv2.}}
\label{tab:auc_casia}
\end{table}


\begin{table*}[t]
\centering
\resizebox{1.9\columnwidth}{!}{
\begin{tabular}{@{}l|l|cccc|cc@{}}
\toprule[2pt]
\multicolumn{1}{c|}{\multirow{2}{*}{\textbf{Method}}} &
  \multicolumn{1}{c|}{\multirow{2}{*}{\textbf{Datasets (Train/validate/test split)
  }}} &
  \multicolumn{4}{c|}{\textbf{Pixel-level AUC}} &
  \multicolumn{2}{c}{\textbf{Pixel-level $F1$}} \\ \cmidrule(l){3-8} 
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{}               & \textbf{COVER} & \textbf{NIST16}
% \footnotemark[1] 
& \textbf{CASIA} & \textbf{IMD-20} & \textbf{CASIA} & \textbf{COVER} \\ \midrule
TransForesinc,
% \cite{transforensic_2021}
 ICCV21         & COVER + CASIA + IMD20 (8:1:1)           & 0.884          & -              & 0.850          & 0.848           & 0.627          & 0.674          \\
\textit{IML-ViT(Ours) }        & COVER + CASIA + IMD20 (8:1:1)          & 0.912          & 0.821* & \textbf{0.961} & \textbf{0.943}  & \textbf{0.825} & \textbf{0.815} \\ \midrule
ObjectFormer,
% \cite{objectformer_2022}
 CVPR22          & COVER(4:1); NIST(4:1); CASIA(v2:v1) & \textbf{0.957} & 0.996          & 0.882          & -               & 0.579          & 0.758          \\ \midrule

CF-Net
% \cite{CFL_Net_2023}
, WACV23              & NIST16 + CASIA + IMD20 (8:1:1)          & -              & \textbf{0.997} & 0.863          & 0.899           & -              & -              \\
\textit{IML-ViT(Ours)  }       & NIST16 + CASIA + IMD20 (8:1:1)          & 0.801*        & \textbf{0.997} & \textbf{0.959} & \textbf{0.941}  & 0.820 & 0.505* \\ 
\bottomrule[2pt]
\end{tabular}
}
\caption{\textbf{Mix-dataset results.} $*$ marks cross-dataset results.}
\label{tab:AUC}
\end{table*}
\textbf{Cross-dataset comparison} 
% For a comprehensive evaluation on five datasets, we select six reproducible methods with publicly available code or trained models as baselines. An overview of these methods can be found in Table \ref{tab:1}.
Since MVSS-Net~\cite{MVSS_2021} has already conducted a detailed evaluation on a fair cross-dataset protocol, we directly quote their results here and train our models with the same protocol, i.e. training on both authentic and manipulated images of CASIAv2 dataset and testing on public datasets. The results measured by $F_1$ score are listed respectively in Table \ref{tab:sota}. We also list comparison with some closed-source method that only reports their F1 score tested on CASIAv1 in Table \ref{tab:close}. 



Moreover, ObjectFormer~\cite{objectformer_2022} and CFL-Net~\cite{CFL_Net_2023} evaluate their models fine-tuning with CASIAv2 on AUC. Although this metric may overestimate the models, IML-ViT has still surpassed them, as shown in Table \ref{tab:auc_casia}.



Overall, our model has achieved state-of-the-art performance compared to existing models evaluated under this fair cross-dataset protocol. Figure \ref{fig:visualization}
qualitatively illustrates the high-quality and clear boundary of our model trained on CASIAv2 and tested on various datasets under different preferences of manipulation types.




\textbf{Mix-dataset comparison} 
Some methods reported their performance based on mixed datasets, which were randomly split into train-validate-test sets, introducing sampling bias. Particularly, in the case of NIST16, there exists a considerable number of duplicated samples, and random splitting might cause the same image to appear repeatedly in both the train and test sets, resulting in artificially inflated metrics. This makes them unsuitable as reliable benchmarks. Anyway, results in Table \ref{tab:AUC} show that, under the same evaluation criteria, IML-ViT also outperforms them.

% \begin{table*}[t]
% \centering
% \resizebox{1.9\columnwidth}{!}{
% \begin{tabular}{@{}l|l|cccc|cc@{}}
% \toprule[2pt]
% \multicolumn{1}{c|}{\multirow{2}{*}{\textbf{Method}}} &
%   \multicolumn{1}{c|}{\multirow{2}{*}{\textbf{Datasets (Train/validate/test split)
%   }}} &
%   \multicolumn{4}{c|}{\textbf{Pixel-level AUC}} &
%   \multicolumn{2}{c}{\textbf{Pixel-level $F1$}} \\ \cmidrule(l){3-8} 
% \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{}               & \textbf{COVER} & \textbf{NIST16}
% % \footnotemark[1] 
% & \textbf{CASIA} & \textbf{IMD-20} & \textbf{CASIA} & \textbf{COVER} \\ \midrule
% TransForesinc,
% % \cite{transforensic_2021}
%  ICCV21         & COVER + CASIA + IMD20 (8:1:1)           & 0.884          & -              & 0.850          & 0.848           & 0.627          & 0.674          \\
% \textit{IML-ViT(Ours) }        & COVER + CASIA + IMD20 (8:1:1)          & 0.912          & 0.821* & \textbf{0.961} & \textbf{0.943}  & \textbf{0.825} & \textbf{0.815} \\ \midrule
% ObjectFormer,
% % \cite{objectformer_2022}
%  CVPR22          & COVER(4:1); NIST(4:1); CASIA(v2:v1) & \textbf{0.957} & 0.996          & 0.882          & -               & 0.579          & 0.758          \\ \midrule

% CF-Net
% % \cite{CFL_Net_2023}
% , WACV23              & NIST16 + CASIA + IMD20 (8:1:1)          & -              & \textbf{0.997} & 0.863          & 0.899           & -              & -              \\
% \textit{IML-ViT(Ours)  }       & NIST16 + CASIA + IMD20 (8:1:1)          & 0.801*        & \textbf{0.997} & \textbf{0.959} & \textbf{0.941}  & 0.820 & 0.505* \\ 
% \bottomrule[2pt]
% \end{tabular}
% }
% \caption{\textbf{Mix-dataset results.} $*$ marks cross-dataset results.}
% \label{tab:AUC}
% \end{table*}


% \textbf{Closed-source Comparison} Besides, for closed-source models, we consider a brief comparison on CASIA datasets and verify performance using the same training protocol. The specific numerical values are cited from their corresponding paper. Specifically, we compared performance with RGB-N,  SPAN and ObjectFormer. Details are shown in Table \ref{tab:close}

\subsection{Robustness Evaluation}
JPEG compression and Gaussian Blur are the common attack method for Image manipulation localization. Hence we further carried out experiments on the resistance of these two operations on CASIAv1. The evaluation results\footnote{Performance against JPEG compression is quoted from MVSS-Net, while performance against Gaussian Blur is retested by us using the publicly available model because we found a significant discrepancy between our tests and the performance reported in their paper.} are shown in Figure \ref{fig:robustness}. The  IML-ViT  exhibited excellent resistance to JPEG compression and Gaussian blur and consistently maintained the best performance of the five models.  

% Figure environment removed

% \footnotetext{}



% \subsection{Limitation}
% We observe a rapid decline for IML-ViT's performance on Gaussian blur attack
% TODO attack 需要更换
% when the filter kernel size exceeded 11, We argue that this is mainly because our motivation is to make the model focus on detailed artifacts, but excessive Gaussian blurring can significantly remove these details, leading to a sudden drop in performance. However, from another perspective, this can actually prove that our model is able to effectively capture artifacts in tampering. Currently, the training does not specifically enhance blur, so we believe that adding enough blur data augmentation can compensate for this issue.
% % Figure environment removed

\section{Conclusions}
This paper introduces IML-ViT, the first image manipulation localization model based on ViT. Extensive experiments on five public datasets demonstrate that IML-ViT achieves SoTA performance and generalization ability, validating the reliability of the three core elements of the IML task proposed in this study: high resolution, multi-scale, and edge supervision. Further, IML-ViT proves the effectiveness of self-attention in capturing non-semantic artifacts. Its simple structure also makes it a promising benchmark in this field.

% -------------------后续素材------------------
% \section{paragraph wait for use}

% Unlike the ViTDet paper applys a mask-RCNN network after the Simple Feature pyramid, we only transform all the feature maps back to the same resolution with the linear layers separately. Then use a 1×1 convolution layer to fuse the features to a fixed scale of $1/4$. Finally do a final$\frac{H}{4} \times \frac{W}{4} \times 1$ prediction with another 1x1 Conv2D layer.

% we then feed the tokens come from ViT to. This method generate 
% Followed ViTDet~\cite{ViT_2021}, we then apply a simple feature pyramid after the ViT.
% Image manipulation datasets are 


% However, dataset insufficiency is the most crucial issue in building ViT-based IML methods. As dense annotation and expert tamper generation are very labor-intensive, the public datasets~\cite{CASIA_2013, Columbia_2006, NIST16_2019, defacto_2019, Coverage_2016} for image manipulation localization are small in size (several hundred to several thousand images). This cannot in any way satisfy vanilla ViT's appetite, as it often requires training on ImageNet~\cite{imagenet_2009} or COCO~\cite{mscoco_2014} for optimal performance~\cite{ViT_2021,Carion_Massa_Synnaeve_Usunier_Kirillov_Zagoruyko_2020, Heo_Yun_Han_Chun_Choe_Oh, Meng_Li_Chen_Lan_Wu_Jiang_Lim_2022}. Yet, the rapid development of self-supervised pre-training for ViT has provided a solution to the problem, especially MAE~\cite{MAE_2022}, which we further experimentally demonstrate can greatly alleviate the overfitting problem and allow the model to converge quickly on small datasets. In short, MAE pre-training is a prerequisite for us to carry out further model design for IML problem.


% %基于MAE解决datainsufficiency的问题，这个打算放到method部分了
% \par
% Dataset insufficiency is the most crucial issue in building ViT-based IML methods. As dense annotation and expert tamper generation are very labor-intensive, the public datasets~\cite{CASIA_2013, Columbia_2006, NIST16_2019, defacto_2019, Coverage_2016} for image manipulation localization are small in size (several hundred to several thousand images). This cannot in any way satisfy vanilla ViT's appetite, as it often requires training on ImageNet~\cite{imagenet_2009} or COCO~\cite{mscoco_2014} for optimal performance~\cite{ViT_2021,Carion_Massa_Synnaeve_Usunier_Kirillov_Zagoruyko_2020, Heo_Yun_Han_Chun_Choe_Oh, Meng_Li_Chen_Lan_Wu_Jiang_Lim_2022}. Yet, the rapid development of self-supervised pre-training for ViT has provided a solution to the problem, especially MAE~\cite{MAE_2022}, which we further experimentally demonstrate can greatly alleviate the overfitting problem and allow the model to converge quickly on small datasets. In short, MAE pre-training is a prerequisite for us to carry out further model design for IML problem.

% % 手工特征简介 这段或许也可以放到3.method里面
% Specifically for IML tasks, to capture the dissimilarity, many hand-crafted feature extractors such as BayarConv~\cite{Bayar_2018} and High-frequency extractor~\cite{objectformer_2022}, etc. are proposed and widely used to exploit additional information from artifacts. We use AdamW as optimization with $lr=1e-4$ 
% % 手工特征的问题
% %设计的特征对于某个任务很好，但是另一个任务不通用（noise-> splicing or copy-move）
% %目前的所有模型会对图像进行大幅度的压缩，改变长宽比和分辨率，这会对artifact产生影响
% %精心设计的不一定是最实用的。
% However, there are two problems with this generic way of operation:(1)A feature may only be effective for one type of tampering, e.g., noise is more sensitive to splicing from different images but less effective for copy-move from the same image. (2)Most implementations scale the image to 512x512 resolution or smaller, yet in many datasets, images are not square and have a larger resolution. Such re-scale compressing has been proved that will harm the latent artifacts~\cite{Mantra_2019}. (3) These artificial extractors may work but are not optimal. This can be analogized to the replacement of symbolism by connectionism in machine learning practice. In contrast, instead of complex feature extraction, we want to find generic solutions to automatically mine as much information from artifacts as possible.

% % 没啥用的两个问句
% However, when we revisit past research, the question arises: We attempt to discuss this issue from the following perspectives:

% we have a question over this CNN prevalence: 

% % 简介图像篡改类型
% \par
% Image manipulation can be generally classified into three types:(1)splicing, copying a region from an image, and pasting it to another image. (2)copy-move, cloning a region within an image. (3)inpainting, erasing regions from images and inpaint missing regions with visually plausible contents. To track manipulation, recent detection methods major focus on in-camera clues(like different noise distributions coming from different cameras) and out-camera clues(various traces on the edge of the tampered region), clues above can be collectively termed as \textit{artifacts}. Since tampering aims to construct semantically meaningful and perceptually convincing images, in most cases, artifacts can only be found at the texture level that is non-semantic. This differs the IML tasks from traditional semantic segmentation tasks, and simply taking the segmentation model into use can have shortcomings in generalization performance~\cite{GSR_Net_2020}.
% Both manipulated and authentic regions can be semantically meaningful, and it is the generalized dissimilarity between them can support the decision. The regions with the most dissimilarities are the artifacts that need the most attention.

% % FCN在同源数据集CASIAv2训练的好可以在CASIA1上运行，但是在其他的数据集表现差
% % Previous research~\cite{GSR_Net_2020} reports that DeepLabv2~\cite{Deeplab_2018} trained on CASIAv2 datasets~\cite{CASIA_2013} performs well on homologous CASIAv1 datasets~\cite{CASIA_2013}, but has poor generalization performance on non-homologous COVERAGE~\cite{Coverage_2016} dataset. 



% % 手工特征简介
% To capture the dissimilarity, many hand-crafted feature extractors such as BayarConv~\cite{Bayar_2018} and High-frequency extractor~\cite{objectformer_2022}, etc. are proposed and widely used to exploit additional information from artifacts.
% % 手工特征的问题
% However, there are two problems with this generic way of operation:(1)A feature may only be effective for one type of tampering, e.g., noise is more sensitive to splicing from different images but less effective for copy-move from the same image. (2)Most implementations scale the image to 512x512 resolution or smaller, yet in many datasets, images are not square and have a larger resolution. Such re-scale compressing has been proved that will harm the latent artifacts~\cite{Mantra_2019}. (3) These artificial extractors may work but are not optimal. This can be analogized to the replacement of symbolism by connectionism in machine learning practice. In contrast, instead of complex feature extraction, we want to find generic solutions to automatically mine as much information from artifacts as possible.

% % 重点，辨析ViT和CNN谁更适合
% Keeping this in mind, we revisited the state-of-the-arts~\cite{Mantra_2019,GSR_Net_2020, SPAN_2020, MVSS_2021} and find that most of them adopted  CNNs~\cite{CNN_1989} like VGG~\cite{VGG_2015} or Resnet~\cite{Resnet_2016} as backbone.  A slight exception~\cite{objectformer_2022} has the structure of convolution at first to create tokens and then feed them to the Transformer~\cite{transformer} for final prediction. We question this: \textit{are CNNs really the optimal choice for parsing such non-semantic information comes from artifacts? }

% Because for a convolution layer, feature vectors (vectors at the channel direction in the feature map) at different points have only linear operations on each other. Information is passing through layers \textit{collectively} but less consideration about the similarity between feature vectors at the same layer. This bottom-up manner can be effective for missions like object detection because it cares more about the macroscopic semantic, but there is less capture and fewer secondary understanding of detailed textures when the layers are deepened~\cite{ERF_2016}, especially the relationship between pixels and patches. Based on this analysis, Vision Transformer~\cite{ViT_2021} came to our view. As feature vectors in self-attention have inner product operations on each other, the similarity between vectors is taken into account, i.e., the relevance between different pixels is compared one by one in each block. Further, in terms of information density, IML problems will require more detailed, texture-level information than traditional vision problems, i.e., they are more information-dense. This also makes Transformer-based methods from semantically dense NLP more suitable for parsing artifacts than CNNs.


% % SOTA多数使用CNN
% the state-of-the-arts are widely adopting CNNs~\cite{CNN_1989} like VGG~\cite{VGG_2015} or Resnet~\cite{Resnet_2016} as backbone.However, when we revisit past research, the question arises: We attempt to discuss about this issue from following perspectives:
% we have a question against this CNN prevalence: 

% However, there is no ViT-based~\cite{ViT_2021} model that has been introduced for solving the IML problem. 

% 使用MAE解决这个问题




% \bigskip
% \noindent Thank you for reading these instructions carefully. We look forward to receiving your electronic files!

\newpage

{\small
\bibliographystyle{ieee_fullname.bst}
\bibliography{main}
}

\newpage

\newpage

\appendix
\section{Implementation Details}

\textbf{High-resolution ViT-Base} Mostly following the original Vision Transformer, we implemented our model with a stack of Transformer blocks stacking together. Each block has a self-attention head and an MLP block in it, both having LayerNorm(LN). Every two windowed attention blocks are followed by a global attention block. The windowed attention block only computes self-attention in a small window, while the global attention block ensures global information propagation. Although we introduce the windowed attention, it only affects the self-attention manner but doesn't change the linear projection for Q, K, and V. Therefore, we can directly apply the MAE pre-trained parameters from a vanilla ViT with all global attention to this Windowed attention model without any extra process. Detailed structures are shown in Table \ref{tab:vitb}.

\begin{table}[h]
   \centering
   \resizebox{0.4\columnwidth}{!}{
   \begin{tabular}{@{}l|l@{}}
   \toprule[2pt]
   Configs              & Value     \\ \midrule
   patch size           & 16        \\
   embedding dim        & 768       \\
   depth                & 12        \\
   number of heads      & 12        \\
   input size           & 3×1024×1024 \\
   window size          & 14        \\
   Norm layer           & LN        \\
   Global block indexes & 2,5,8,11  \\ 
   Output shape & 768×64×64         \\ \bottomrule[2pt]
   \end{tabular}
   }
   % \setlength{\abovecaptionskip}{50pt}
   % \setlength{\belowcaptionskip}{50pt}
   \caption{\textbf{Detailed structure of windowed ViT-Base}}
   \label{tab:vitb}
\end{table}


\textbf{Simple Feature Pyramid} After getting the output from ViT-B, SFPN using a series of convolution, pooling, or deconvolution(ConvTranspose2D) layers to sample it to feature maps with 256 channels and in a scale of$\{4.0, 2.0, 1.0, 0.5, 0.25\}$ compared to the resolution of input feature maps(768×64×64). Each layer is followed by LayerNorm. Detailed structures can see in Table \ref{tab:sfpn}.

\begin{table}[h]
   \centering
   \resizebox{0.9\columnwidth}{!}{
   \begin{tabular}{l|l}
   \toprule[2pt]
   Scales & Layers \& channels of feature maps     \\ \midrule
   4.0  & 768 ConvT 384 ConvT 192 Conv(1,1) 256 Conv(3,3) 256 \\
   2.0  &  768 ConvT 384 Conv(1,1) 256 Conv(3,3) 256 \\
   1.0  & 768 Conv(1,1) 256 Conv(3,3) 256 \\
   0.5  & 768 maxpool2D 384 Conv(1,1) 256 Conv(3,3) 256 \\
   0.25  & 768 maxpool2D 384 Conv(1,1) 256 Conv(3,3) 256 maxpool2D 256 \\ \bottomrule[2pt]
   \end{tabular}
   }
   \caption{\textbf{Detailed structure of Simple feature pyramid} \textit{ConvT} denotes for ConvTranspose2D with kernel size of 2 and stride of 2; Conv(x,x) indicate that a Conv2D layer with kernel size of x; and maxpool2D has also a kernel size of 2. The number shown between layers indicates the number of channels for its respective feature map between layers }
   \label{tab:sfpn}
\end{table}

\textbf{Prediction head} 
We test for the performance of different norm layer used in predcition head, result are shown in Table \ref{tab:norms}.

% Please add the following required packages to your document preamble:

\begin{table}[]
   \centering
   \resizebox*{0.7\columnwidth}{!}{
   \begin{tabular}{@{}llllll@{}}
   \toprule
   \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Norm   type}}} & \multicolumn{5}{c}{\textbf{F1 score}}        \\ \cmidrule(l){2-6} 
   \multicolumn{1}{c}{} &
     \multicolumn{1}{c}{\textbf{CASIAv1}} &
     \multicolumn{1}{c}{\textbf{Columbia}} &
     \multicolumn{1}{c}{\textbf{Coverage}} &
     \multicolumn{1}{c}{\textbf{NIST16}} &
     \multicolumn{1}{c}{\textbf{Average}} \\ \midrule
   No Norm                                                   & 0.6081 & 0.6026 & 0.4361 & 0.1936 & 0.4601   \\
   Layer Norm                                                & 0.6013 & 0.5487 & 0.4066 & 0.254  & 0.45265  \\
   Batch Norm                                                & 0.5939 & 0.6607 & 0.4128 & 0.2649 & 0.483075 \\ \bottomrule
   \end{tabular}
   }
   \caption{\textbf{Testing for Norm in Predict Head} Implementation is followed to ablation study in the main paper.}
   \label{tab:norms}
\end{table}




\textbf{Training settings}
Since our model could only train with a single image for a batch, we apply to accumulate gradient during training, i.e. update the parameters every 32 images during training. 
Besides, we adopt the early stop method during training. Evaluate the performance on the F1 score for CASIAv1, and stop training when there is no improvement for 15 epochs. Other configs are described in Table \ref{tab:vitb_train}.


\begin{table}[h]
   \centering
   \resizebox{0.5\columnwidth}{!}{
   \begin{tabular}{@{}l|l@{}}
   \toprule[2pt]
   Configs              & Value     \\ \midrule
   batch size & 1 \\
   accumulate batch size & 32 \\
   epochs        & 200      \\
   warm up epochs & 4        \\
   optimizer & AdamW \\
   optimizer momentum & $\beta_1,\beta_2=0.9,0.95$ \\
   base  learning rate & 1e-4 \\
   minimum laerning rate & 5e-7 \\
   learning rate schedule & cosine decay \\
   weight decay  & 0.05 \\ \bottomrule[2pt]
   \end{tabular}
   }

   \caption{\textbf{Training settings for IML-ViT}}
   \label{tab:vitb_train}
\end{table}


\section{Visualization of Feature maps}

To gain a deeper understanding of our IML-ViT model's inner workings, we present visualizations of feature maps between layers by calculating the average channel dimensions of the feature map. The outcomes are displayed in Figure \ref{fig:output}. This visualization process allows us to shed light on the model's functioning and provides valuable insights into its mechanisms.

% Figure environment removed

\end{document}
