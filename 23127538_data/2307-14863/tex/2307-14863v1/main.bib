@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})


 @article{Bayar_2018,
  title        = {Constrained Convolutional Neural Networks: A New Approach Towards General Purpose Image Manipulation Detection},
  volume       = {13},
  issn         = {1556-6013, 1556-6021},
  doi          = {10.1109/TIFS.2018.2825953},
  abstractnote = {Identifying the authenticity and processing history of an image is an important task in multimedia forensics. By analyzing traces left by different image manipulations, researchers have been able to develop several algorithms capable of detecting targeted editing operations. While this approach has led to the development of several successful forensic algorithms, an important problem remains: creating forensic detectors for different image manipulations is a difﬁcult and time consuming process. Furthermore, forensic analysts need general purpose forensic algorithms capable of detecting multiple different image manipulations. In this paper, we address both of these problems by proposing a new general purpose forensic approach using convolutional neural networks (CNNs). While CNNs are capable of learning classiﬁcation features directly from data, in their existing form they tend to learn features representative of an image’s content. To overcome this issue, we have developed a new type of CNN layer, called a constrained convolutional layer, that is able to jointly suppress an image’s content and adaptively learn manipulation detection features. Through a series of experiments, we show that our proposed constrained CNN is able to learn manipulation detection features directly from data. Our experimental results demonstrate that our CNN can detect multiple different editing operations with up to 99.97\% accuracy and outperform the existing state-of-the-art general purpose manipulation detector. Furthermore, our constrained CNN can still accurately detect image manipulations in realistic scenarios where there is a source camera model mismatch between the training and testing data.},
  number       = {11},
  journal      = {IEEE Transactions on Information Forensics and Security},
  author       = {Bayar, Belhassen and Stamm, Matthew C.},
  year         = {2018},
  month        = {Nov},
  pages        = {2691–2706},
  language     = {en}
}


 @inbook{Carion_Massa_Synnaeve_Usunier_Kirillov_Zagoruyko_2020,
  address      = {Cham},
  series       = {Lecture Notes in Computer Science},
  title        = {End-to-End Object Detection with Transformers},
  volume       = {12346},
  isbn         = {978-3-030-58451-1},
  url          = {https://link.springer.com/10.1007/978-3-030-58452-8_13},
  doi          = {10.1007/978-3-030-58452-8_13},
  abstractnote = {We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, eﬀectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a ﬁxed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the ﬁnal set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster R-CNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a uniﬁed manner. We show that it signiﬁcantly outperforms competitive baselines. Training code and pretrained models are available at https:// github.com/facebookresearch/detr.},
  booktitle    = {Computer Vision – ECCV 2020},
  publisher    = {Springer International Publishing},
  author       = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  editor       = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  year         = {2020},
  pages        = {213–229},
  collection   = {Lecture Notes in Computer Science},
  language     = {en}
}
 
 @inproceedings{CASIA_2013,
  address      = {Beijing, China},
  title        = {CASIA Image Tampering Detection Evaluation Database},
  isbn         = {978-1-4799-1043-4},
  url          = {http://ieeexplore.ieee.org/document/6625374/},
  doi          = {10.1109/ChinaSIP.2013.6625374},
  abstractnote = {Image forensics has now raised the anxiety of justice as increasing cases of abusing tampered images in newspapers and court for evidence are reported recently. With the goal of verifying image content authenticity, passive-blind image tampering detection is called for. More realistic open benchmark databases are also needed to assist the techniques. Recently, we collect a natural color image database with realistic tampering operations. The database is made publicly available for researchers to compare and evaluate their proposed tampering detection techniques. We call this database CASIA Image Tampering Detection Evaluation Database. We describe the purpose, the design criterion, the organization and self-evaluation of this database in this paper.},
  booktitle    = {2013 IEEE China Summit and International Conference on Signal and Information Processing},
  publisher    = {IEEE},
  author       = {Dong, Jing and Wang, Wei and Tan, Tieniu},
  year         = {2013},
  month        = {Jul},
  pages        = {422–426},
  language     = {en}
}


 @article{CNN_1989,
  title     = {Backpropagation applied to handwritten zip code recognition},
  volume    = {1},
  number    = {4},
  journal   = {Neural computation},
  publisher = {MIT Press},
  author    = {LeCun, Yann and Boser, Bernhard and Denker, John S and Henderson, Donnie and Howard, Richard E and Hubbard, Wayne and Jackel, Lawrence D},
  year      = {1989},
  pages     = {541–551}
}

 
 @inproceedings{Columbia_2006,
  address      = {Toronto, ON, Canada},
  title        = {Detecting Image Splicing using Geometry Invariants and Camera Characteristics Consistency},
  isbn         = {978-1-4244-0367-7},
  url          = {http://ieeexplore.ieee.org/document/4036658/},
  doi          = {10.1109/ICME.2006.262447},
  abstractnote = {Recent advances in computer technology have made digital image tampering more and more common. In this paper, we propose an authentic vs. spliced image classiﬁcation method making use of geometry invariants in a semi-automatic manner. For a given image, we identify suspicious splicing areas, compute the geometry invariants from the pixels within each region, and then estimate the camera response function (CRF) from these geometry invariants. The cross-ﬁtting errors are fed into a statistical classiﬁer. Experiments show a very promising accuracy, 87\%, over a large data set of 363 natural and spliced images. To the best of our knowledge, this is the ﬁrst work detecting image splicing by verifying camera characteristic consistency from a single-channel image.},
  booktitle    = {2006 IEEE International Conference on Multimedia and Expo},
  publisher    = {IEEE},
  author       = {Hsu, Yu-feng and Chang, Shih-fu},
  year         = {2006},
  month        = {Jul},
  pages        = {549–552},
  language     = {en}
}

 
 @inproceedings{Coverage_2016,
  address   = {Phoenix, AZ, USA},
  title     = {COVERAGE — A novel database for copy-move forgery detection},
  isbn      = {978-1-4673-9961-6},
  url       = {http://ieeexplore.ieee.org/document/7532339/},
  doi       = {10.1109/ICIP.2016.7532339},
  booktitle = {2016 IEEE International Conference on Image Processing (ICIP)},
  publisher = {IEEE},
  author    = {Wen, Bihan and Zhu, Ye and Subramanian, Ramanathan and Ng, Tian-Tsong and Shen, Xuanjing and Winkler, Stefan},
  year      = {2016},
  month     = {Sep},
  pages     = {161–165},
  language  = {en}
}
 
 @inproceedings{Cozzolino_Poggi_Verdoliva_2015a,
  address      = {Roma, Italy},
  title        = {Splicebuster: A new blind image splicing detector},
  isbn         = {978-1-4673-6802-5},
  url          = {http://ieeexplore.ieee.org/document/7368565/},
  doi          = {10.1109/WIFS.2015.7368565},
  abstractnote = {We propose a new feature-based algorithm to detect image splicings without any prior information. Local features are computed from the co-occurrence of image residuals and used to extract synthetic feature parameters. Splicing and host images are assumed to be characterized by different parameters. These are learned by the image itself through the expectation-maximization algorithm together with the segmentation in genuine and spliced parts. A supervised version of the algorithm is also proposed. Preliminary results on a wide range of test images are very encouraging, showing that a limited-size, but meaningful, learning set may be sufﬁcient for reliable splicing localization.},
  booktitle    = {2015 IEEE International Workshop on Information Forensics and Security (WIFS)},
  publisher    = {IEEE},
  author       = {Cozzolino, Davide and Poggi, Giovanni and Verdoliva, Luisa},
  year         = {2015},
  month        = {Nov},
  pages        = {1–6},
  language     = {en}
}

 @article{Cozzolino_Poggi_Verdoliva_2015b,
  title        = {Efficient Dense-Field Copy–Move Forgery Detection},
  volume       = {10},
  issn         = {1556-6013, 1556-6021},
  doi          = {10.1109/TIFS.2015.2455334},
  abstractnote = {We propose a new algorithm for the accurate detection and localization of copy-move forgeries, based on rotationinvariant features computed densely on the image. Dense-ﬁeld techniques proposed in the literature guarantee a superior performance w.r.t. their keypoint-based counterparts, at the price of a much higher processing time, mostly due to the feature matching phase. To overcome this limitation, we resort here to a fast approximate nearest-neighbor search algorithm, PatchMatch, especially suited for the computation of dense ﬁelds over images. We adapt the matching algorithm to deal efﬁciently with invariant features, so as to achieve higher robustness w.r.t. rotations and scale changes. Moreover, leveraging on the smoothness of the output ﬁeld, we implement a simpliﬁed and reliable post-processing procedure. The experimental analysis, conducted on databases available online, proves the proposed technique to be at least as accurate, generally more robust, and typically much faster, than state-of-the-art dense-ﬁeld references.},
  number       = {11},
  journal      = {IEEE Transactions on Information Forensics and Security},
  author       = {Cozzolino, Davide and Poggi, Giovanni and Verdoliva, Luisa},
  year         = {2015},
  month        = {Nov},
  pages        = {2284–2297},
  language     = {en}
}

 @article{Deeplab_2018,
  title   = {DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs},
  volume  = {40},
  issn    = {0162-8828, 2160-9292},
  doi     = {10.1109/TPAMI.2017.2699184},
  number  = {4},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  author  = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.},
  year    = {2018},
  month   = {Apr},
  pages   = {834–848}
}

 @inproceedings{defacto_2019,
  address      = {A Coruna, Spain},
  title        = {DEFACTO: Image and Face Manipulation Dataset},
  isbn         = {978-90-827970-3-9},
  url          = {https://ieeexplore.ieee.org/document/8903181/},
  doi          = {10.23919/EUSIPCO.2019.8903181},
  abstractnote = {This paper presents a novel dataset for image and face manipulation detection and localization called DEFACTO. The dataset was automatically generated using Microsoft common object in context database (MSCOCO) to produce semantically meaningful forgeries. Four categories of forgeries have been generated. Splicing forgeries which consist of inserting an external element into an image, copy-move forgeries where an element within an image is duplicated, object removal forgeries where objects are removed from images and lastly morphing where two images are warped and blended together. Over 200000 images have been generated and each image is accompanied by several annotations allowing precise localization of the forgery and information about the tampering process.},
  booktitle    = {2019 27th European Signal Processing Conference (EUSIPCO)},
  publisher    = {IEEE},
  author       = {Mahfoudi, Gael and Tajini, Badr and Retraint, Florent and Morain-Nicolier, Frederic and Dugelay, Jean Luc and Pic, Marc},
  year         = {2019},
  month        = {Sep},
  pages        = {1–5},
  language     = {en}
}

 @inproceedings{ERF_2016,
  title     = {Understanding the Effective Receptive Field in Deep Convolutional Neural Networks},
  volume    = {29},
  url       = {https://proceedings.neurips.cc/paper/2016/file/c8067ad1937f728f51288b3eb986afaa-Paper.pdf},
  booktitle = {Advances in Neural Information Processing Systems},
  publisher = {Curran Associates, Inc.},
  author    = {Luo, Wenjie and Li, Yujia and Urtasun, Raquel and Zemel, Richard},
  editor    = {Lee, D. and Sugiyama, M. and Luxburg, U. and Guyon, I. and Garnett, R.},
  year      = {2016}
}

 @article{FPN_2017,
  title        = {Feature Pyramid Networks for Object Detection},
  url          = {http://arxiv.org/abs/1612.03144},
  abstractnote = {Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But recent deep learning object detectors have avoided pyramid representations, in part because they are compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using FPN in a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.},
  note         = {arXiv:1612.03144 [cs]},
  number       = {arXiv:1612.03144},
  publisher    = {arXiv},
  author       = {Lin, Tsung-Yi and Dollár, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
  year         = {2017},
  month        = {Apr}
}


 @article{GSR_Net_2020,
  title        = {Generate, Segment, and Refine: Towards Generic Manipulation Segmentation},
  volume       = {34},
  issn         = {2374-3468, 2159-5399},
  doi          = {10.1609/aaai.v34i07.7007},
  abstractnote = {Detecting manipulated images has become a signiﬁcant emerging challenge. The advent of image sharing platforms and the easy availability of advanced photo editing software have resulted in a large quantities of manipulated images being shared on the internet. While the intent behind such manipulations varies widely, concerns on the spread of false news and misinformation is growing. Current state of the art methods for detecting these manipulated images suffers from the lack of training data due to the laborious labeling process. We address this problem in this paper, for which we introduce a manipulated image generation process that creates true positives using currently available datasets. Drawing from traditional work on image blending, we propose a novel generator for creating such examples. In addition, we also propose to further create examples that force the algorithm to focus on boundary artifacts during training. Strong experimental results validate our proposal.},
  number       = {07},
  journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
  author       = {Zhou, Peng and Chen, Bor-Chun and Han, Xintong and Najibi, Mahyar and Shrivastava, Abhinav and Lim, Ser-Nam and Davis, Larry},
  year         = {2020},
  month        = {Apr},
  pages        = {13058–13065},
  language     = {en}
}
 
 @article{Heo_Yun_Han_Chun_Choe_Oh,
  title        = {Rethinking Spatial Dimensions of Vision Transformers},
  abstractnote = {Vision Transformer (ViT) extends the application range of transformers from language processing to computer vision tasks as being an alternative architecture against the existing convolutional neural networks (CNN). Since the transformer-based architecture has been innovative for computer vision modeling, the design convention towards an effective architecture has been less studied yet. From the successful design principles of CNN, we investigate the role of spatial dimension conversion and its effectiveness on transformer-based architecture. We particularly attend to the dimension reduction principle of CNNs; as the depth increases, a conventional CNN increases channel dimension and decreases spatial dimensions. We empirically show that such a spatial dimension reduction is beneficial to a transformer architecture as well, and propose a novel Pooling-based Vision Transformer (PiT) upon the original ViT model. We show that PiT achieves the improved model capability and generalization performance against ViT. Throughout the extensive experiments, we further show PiT outperforms the baseline on several tasks such as image classification, object detection, and robustness evaluation. Source codes and ImageNet models are available at https://github.com/naver-ai/pit.},
  author       = {Heo, Byeongho and Yun, Sangdoo and Han, Dongyoon and Chun, Sanghyuk and Choe, Junsuk and Oh, Seong Joon},
  language     = {en}
}

 
 @inbook{Huh_Liu_Owens_Efros_2018,
  address      = {Cham},
  series       = {Lecture Notes in Computer Science},
  title        = {Fighting Fake News: Image Splice Detection via Learned Self-Consistency},
  volume       = {11215},
  isbn         = {978-3-030-01251-9},
  url          = {https://link.springer.com/10.1007/978-3-030-01252-6_7},
  doi          = {10.1007/978-3-030-01252-6_7},
  abstractnote = {Advances in photo editing and manipulation tools have made it signiﬁcantly easier to create fake imagery. Learning to detect such manipulations, however, remains a challenging problem due to the lack of sufﬁcient amounts of manipulated training data. In this paper, we propose a learning algorithm for detecting visual image manipulations that is trained only using a large dataset of real photographs. The algorithm uses the automatically recorded photo EXIF metadata as supervisory signal for training a model to determine whether an image is self-consistent — that is, whether its content could have been produced by a single imaging pipeline. We apply this self-consistency model to the task of detecting and localizing image splices. The proposed method obtains state-ofthe-art performance on several image forensics benchmarks, despite never seeing any manipulated images at training. That said, it is merely a step in the long quest for a truly general purpose visual forensics tool.},
  booktitle    = {Computer Vision – ECCV 2018},
  publisher    = {Springer International Publishing},
  author       = {Huh, Minyoung and Liu, Andrew and Owens, Andrew and Efros, Alexei A.},
  editor       = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
  year         = {2018},
  pages        = {106–124},
  collection   = {Lecture Notes in Computer Science},
  language     = {en}
}

 @inproceedings{imagenet_2009,
  address      = {Miami, FL},
  title        = {ImageNet: A large-scale hierarchical image database},
  isbn         = {978-1-4244-3992-8},
  url          = {https://ieeexplore.ieee.org/document/5206848/},
  doi          = {10.1109/CVPR.2009.5206848},
  abstractnote = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a largescale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 5001000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classiﬁcation and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
  booktitle    = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
  publisher    = {IEEE},
  author       = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},
  year         = {2009},
  month        = {Jun},
  pages        = {248–255},
  language     = {en}
}

 
 @article{Kniaz_Knyaz_Remondino,
  title        = {The Point Where Reality Meets Fantasy: Mixed Adversarial Generators for Image Splice Detection},
  abstractnote = {Modern photo editing tools allow creating realistic manipulated images easily. While fake images can be quickly generated, learning models for their detection is challenging due to the high variety of tampering artifacts and the lack of large labeled datasets of manipulated images. In this paper, we propose a new framework for training of discriminative segmentation model via an adversarial process. We simultaneously train four models: a generative retouching model GR that translates manipulated image to the real image domain, a generative annotation model GA that estimates the pixel-wise probability of image patch being either real or fake, and two discriminators DR and DA that qualify the output of GR and GA. The aim of model GR is to maximize the probability of model GA making a mistake. Our method extends the generative adversarial networks framework with two main contributions: (1) training of a generative model GR against a deep semantic segmentation network GA that learns rich scene semantics for manipulated region detection, (2) proposing per class semantic loss that facilitates semantically consistent image retouching by the GR. We collected large-scale manipulated image dataset to train our model. The dataset includes 16k real and fake images with pixel-level annotations of manipulated areas. The dataset also provides ground truth pixellevel object annotations. We validate our approach on several modern manipulated image datasets, where quantitative results and ablations demonstrate that our method achieves and surpasses the state-of-the-art in manipulated image detection. We made our code and dataset publicly available 1.},
  author       = {Kniaz, Vladimir V and Knyaz, Vladimir and Remondino, Fabio},
  language     = {en}
}

 @article{Krizhevsky_Sutskever_Hinton_2017,
  title        = {ImageNet classification with deep convolutional neural networks},
  volume       = {60},
  issn         = {0001-0782, 1557-7317},
  doi          = {10.1145/3065386},
  abstractnote = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used nonsaturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
  number       = {6},
  journal      = {Communications of the ACM},
  author       = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  year         = {2017},
  month        = {May},
  pages        = {84–90},
  language     = {en}
}


@inproceedings{MAE_2022,
  address      = {New Orleans, LA, USA},
  title        = {Masked Autoencoders Are Scalable Vision Learners},
  isbn         = {978-1-66546-946-3},
  url          = {https://ieeexplore.ieee.org/document/9879206/},
  doi          = {10.1109/CVPR52688.2022.01553},
  abstractnote = {This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we ﬁnd that masking a high proportion of the input image, e.g., 75]\%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs enables us to train large models efﬁciently and effectively: we accelerate training (by 3× or more) and improve accuracy. Our scalable approach allows for learning high-capacity models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8\%) among methods that use only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pretraining and shows promising scaling behavior.},
  booktitle    = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  publisher    = {IEEE},
  author       = {He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Dollar, Piotr and Girshick, Ross},
  year         = {2022},
  month        = {Jun},
  pages        = {15979–15988},
  language     = {en}
}
 @inproceedings{Mantra_2019,
  address   = {Long Beach, CA, USA},
  title     = {ManTra-Net: Manipulation Tracing Network for Detection and Localization of Image Forgeries With Anomalous Features},
  isbn      = {978-1-72813-293-8},
  url       = {https://ieeexplore.ieee.org/document/8953774/},
  doi       = {10.1109/CVPR.2019.00977},
  booktitle = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  publisher = {IEEE},
  author    = {Wu, Yue and AbdAlmageed, Wael and Natarajan, Premkumar},
  year      = {2019},
  month     = {Jun},
  pages     = {9535–9544},
  language  = {en}
}
 
 @inproceedings{Meng_Li_Chen_Lan_Wu_Jiang_Lim_2022,
  address      = {New Orleans, LA, USA},
  title        = {AdaViT: Adaptive Vision Transformers for Efficient Image Recognition},
  isbn         = {978-1-66546-946-3},
  url          = {https://ieeexplore.ieee.org/document/9879366/},
  doi          = {10.1109/CVPR52688.2022.01199},
  abstractnote = {Built on top of self-attention mechanisms, vision transformers have demonstrated remarkable performance on a variety of tasks recently. While achieving excellent performance, they still require relatively intensive computational cost that scales up drastically as the numbers of patches, self-attention heads and transformer blocks increase. In this paper, we argue that due to the large variations among images, their need for modeling long-range dependencies between patches differ. To this end, we introduce AdaViT, an adaptive computation framework that learns to derive usage policies on which patches, self-attention heads and transformer blocks to use throughout the backbone on a perinput basis, aiming to improve inference efficiency of vision transformers with a minimal drop of accuracy for image recognition. Optimized jointly with a transformer backbone in an end-to-end manner, a light-weight decision network is attached to the backbone to produce decisions on-the-fly. Extensive experiments on ImageNet demonstrate that our method obtains more than 2× improvement on efficiency compared to state-of-the-art vision transformers with only 0.8\% drop of accuracy, achieving good efficiency/accuracy trade-offs conditioned on different computational budgets. We further conduct quantitative and qualitative analysis on learned usage polices and provide more insights on the redundancy in vision transformers. Code is available at https://github.com/MengLcool/AdaViT.},
  booktitle    = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  publisher    = {IEEE},
  author       = {Meng, Lingchen and Li, Hengduo and Chen, Bor-Chun and Lan, Shiyi and Wu, Zuxuan and Jiang, Yu-Gang and Lim, Ser-Nam},
  year         = {2022},
  month        = {Jun},
  pages        = {12299–12308},
  language     = {en}
}
 
 @inbook{mscoco_2014,
  address      = {Cham},
  series       = {Lecture Notes in Computer Science},
  title        = {Microsoft COCO: Common Objects in Context},
  volume       = {8693},
  isbn         = {978-3-319-10601-4},
  url          = {http://link.springer.com/10.1007/978-3-319-10602-1_48},
  doi          = {10.1007/978-3-319-10602-1_48},
  abstractnote = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
  booktitle    = {Computer Vision – ECCV 2014},
  publisher    = {Springer International Publishing},
  author       = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Dollár, Piotr and Zitnick, C. Lawrence},
  editor       = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
  year         = {2014},
  pages        = {740–755},
  collection   = {Lecture Notes in Computer Science},
  language     = {en}
}

 @inproceedings{MVSS_2021,
  address      = {Montreal, QC, Canada},
  title        = {Image Manipulation Detection by Multi-View Multi-Scale Supervision},
  isbn         = {978-1-66542-812-5},
  url          = {https://ieeexplore.ieee.org/document/9710015/},
  doi          = {10.1109/ICCV48922.2021.01392},
  abstractnote = {The key challenge of image manipulation detection is how to learn generalizable features that are sensitive to manipulations in novel data, whilst speciﬁc to prevent false alarms on authentic images. Current research emphasizes the sensitivity, with the speciﬁcity overlooked. In this paper we address both aspects by multi-view feature learning and multi-scale supervision. By exploiting noise distribution and boundary artifact surrounding tampered regions, the former aims to learn semantic-agnostic and thus more generalizable features. The latter allows us to learn from authentic images which are nontrivial to be taken into account by current semantic segmentation network based methods. Our thoughts are realized by a new network which we term MVSS-Net. Extensive experiments on ﬁve benchmark sets justify the viability of MVSS-Net for both pixellevel and image-level manipulation detection.},
  booktitle    = {2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
  publisher    = {IEEE},
  author       = {Chen, Xinru and Dong, Chengbo and Ji, Jiaqi and Cao, Juan and Li, Xirong},
  year         = {2021},
  month        = {Oct},
  pages        = {14165–14173},
  language     = {en}
}

 @inproceedings{NIST16_2019,
  address   = {Waikoloa Village, HI, USA},
  title     = {MFC Datasets: Large-Scale Benchmark Datasets for Media Forensic Challenge Evaluation},
  isbn      = {978-1-72811-392-0},
  url       = {https://ieeexplore.ieee.org/document/8638296/},
  doi       = {10.1109/WACVW.2019.00018},
  booktitle = {2019 IEEE Winter Applications of Computer Vision Workshops (WACVW)},
  publisher = {IEEE},
  author    = {Guan, Haiying and Kozak, Mark and Robertson, Eric and Lee, Yooyoung and Yates, Amy N. and Delgado, Andrew and Zhou, Daniel and Kheyrkhah, Timothee and Smith, Jeff and Fiscus, Jonathan},
  year      = {2019},
  month     = {Jan},
  pages     = {63–72}
}


 @inproceedings{objectformer_2022,
  address      = {New Orleans, LA, USA},
  title        = {ObjectFormer for Image Manipulation Detection and Localization},
  isbn         = {978-1-66546-946-3},
  url          = {https://ieeexplore.ieee.org/document/9880322/},
  doi          = {10.1109/CVPR52688.2022.00240},
  abstractnote = {Recent advances in image editing techniques have posed serious challenges to the trustworthiness of multimedia data, which drives the research of image tampering detection. In this paper, we propose ObjectFormer to detect and localize image manipulations. To capture subtle manipulation traces that are no longer visible in the RGB domain, we extract high-frequency features of the images and combine them with RGB features as multimodal patch embeddings. Additionally, we use a set of learnable object prototypes as mid-level representations to model the object-level consistencies among different regions, which are further used to refine patch embeddings to capture the patch-level consistencies. We conduct extensive experiments on various datasets and the results verify the effectiveness of the proposed method, outperforming state-of-the-art tampering detection and localization methods.},
  booktitle    = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  publisher    = {IEEE},
  author       = {Wang, Junke and Wu, Zuxuan and Chen, Jingjing and Han, Xintong and Shrivastava, Abhinav and Lim, Ser-Nam and Jiang, Yu-Gang},
  year         = {2022},
  month        = {Jun},
  pages        = {2354–2363},
  language     = {en}
}

 @inproceedings{Rao_Ni_2016,
  address      = {Abu Dhabi, United Arab Emirates},
  title        = {A deep learning approach to detection of splicing and copy-move forgeries in images},
  isbn         = {978-1-5090-1138-4},
  url          = {http://ieeexplore.ieee.org/document/7823911/},
  doi          = {10.1109/WIFS.2016.7823911},
  abstractnote = {In this paper, we present a new image forgery detection method based on deep learning technique, which utilizes a convolutional neural network (CNN) to automatically learn hierarchical representations from the input RGB color images. The proposed CNN is speciﬁcally designed for image splicing and copy-move detection applications. Rather than a random strategy, the weights at the ﬁrst layer of our network are initialized with the basic high-pass ﬁlter set used in calculation of residual maps in spatial rich model (SRM), which serves as a regularizer to efﬁciently suppress the effect of image contents and capture the subtle artifacts introduced by the tampering operations. The pre-trained CNN is used as patch descriptor to extract dense features from the test images, and a feature fusion technique is then explored to obtain the ﬁnal discriminative features for SVM classiﬁcation. The experimental results on several public datasets show that the proposed CNN based model outperforms some state-of-the-art methods.},
  booktitle    = {2016 IEEE International Workshop on Information Forensics and Security (WIFS)},
  publisher    = {IEEE},
  author       = {Rao, Yuan and Ni, Jiangqun},
  year         = {2016},
  month        = {Dec},
  pages        = {1–6},
  language     = {en}
}

 @inproceedings{Resnet_2016,
  address      = {Las Vegas, NV, USA},
  title        = {Deep Residual Learning for Image Recognition},
  isbn         = {978-1-4673-8851-1},
  url          = {http://ieeexplore.ieee.org/document/7780459/},
  doi          = {10.1109/CVPR.2016.90},
  abstractnote = {Deeper neural networks are more difﬁcult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classiﬁcation task. We also present analysis on CIFAR-10 with 100 and 1000 layers.},
  booktitle    = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  publisher    = {IEEE},
  author       = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year         = {2016},
  month        = {Jun},
  pages        = {770–778},
  language     = {en}
}

 @inproceedings{RGBN_2018,
  address      = {Salt Lake City, UT, USA},
  title        = {Learning Rich Features for Image Manipulation Detection},
  isbn         = {978-1-5386-6420-9},
  url          = {https://ieeexplore.ieee.org/document/8578214/},
  doi          = {10.1109/CVPR.2018.00116},
  abstractnote = {Image manipulation detection is different from traditional semantic object detection because it pays more attention to tampering artifacts than to image content, which suggests that richer features need to be learned. We propose a two-stream Faster R-CNN network and train it endto-end to detect the tampered regions given a manipulated image. One of the two streams is an RGB stream whose purpose is to extract features from the RGB image input to ﬁnd tampering artifacts like strong contrast difference, unnatural tampered boundaries, and so on. The other is a noise stream that leverages the noise features extracted from a steganalysis rich model ﬁlter layer to discover the noise inconsistency between authentic and tampered regions. We then fuse features from the two streams through a bilinear pooling layer to further incorporate spatial co-occurrence of these two modalities. Experiments on four standard image manipulation datasets demonstrate that our two-stream framework outperforms each individual stream, and also achieves state-of-the-art performance compared to alternative methods with robustness to resizing and compression.},
  booktitle    = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  publisher    = {IEEE},
  author       = {Zhou, Peng and Han, Xintong and Morariu, Vlad I. and Davis, Larry S.},
  year         = {2018},
  month        = {Jun},
  pages        = {1053–1061},
  language     = {en}
}



@inproceedings{SPAN_2020,
  title={SPAN: Spatial pyramid attention network for image manipulation localization},
  author={Hu, Xuefeng and Zhang, Zhihan and Jiang, Zhenye and Chaudhuri, Syomantak and Yang, Zhenheng and Nevatia, Ram},
  booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part XXI 16},
  pages={312--328},
  year={2020},
  organization={Springer}
}

 @article{transformer,
  title        = {Attention is All you Need},
  abstractnote = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
  author       = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
  language     = {en}
}
 @article{Verdoliva_2020,
  title   = {Media Forensics and DeepFakes: An Overview},
  volume  = {14},
  issn    = {1932-4553, 1941-0484},
  doi     = {10.1109/JSTSP.2020.3002101},
  number  = {5},
  journal = {IEEE Journal of Selected Topics in Signal Processing},
  author  = {Verdoliva, Luisa},
  year    = {2020},
  month   = {Aug},
  pages   = {910–932}
}
 @article{VGG_2015,
  title        = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  url          = {http://arxiv.org/abs/1409.1556},
  abstractnote = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  note         = {arXiv:1409.1556 [cs]},
  number       = {arXiv:1409.1556},
  publisher    = {arXiv},
  author       = {Simonyan, Karen and Zisserman, Andrew},
  year         = {2015},
  month        = {Apr}
}
 @article{ViT_2021,
  title        = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  url          = {http://arxiv.org/abs/2010.11929},
  abstractnote = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  note         = {arXiv:2010.11929 [cs]},
  number       = {arXiv:2010.11929},
  publisher    = {arXiv},
  author       = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  year         = {2021},
  month        = {Jun},
  language     = {en}
}
 @inproceedings{Wang_Wu_Chen_Han_Shrivastava_Lim_Jiang_2022,
  address      = {New Orleans, LA, USA},
  title        = {ObjectFormer for Image Manipulation Detection and Localization},
  isbn         = {978-1-66546-946-3},
  url          = {https://ieeexplore.ieee.org/document/9880322/},
  doi          = {10.1109/CVPR52688.2022.00240},
  abstractnote = {Recent advances in image editing techniques have posed serious challenges to the trustworthiness of multimedia data, which drives the research of image tampering detection. In this paper, we propose ObjectFormer to detect and localize image manipulations. To capture subtle manipulation traces that are no longer visible in the RGB domain, we extract high-frequency features of the images and combine them with RGB features as multimodal patch embeddings. Additionally, we use a set of learnable object prototypes as mid-level representations to model the object-level consistencies among different regions, which are further used to refine patch embeddings to capture the patch-level consistencies. We conduct extensive experiments on various datasets and the results verify the effectiveness of the proposed method, outperforming state-of-the-art tampering detection and localization methods.},
  booktitle    = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  publisher    = {IEEE},
  author       = {Wang, Junke and Wu, Zuxuan and Chen, Jingjing and Han, Xintong and Shrivastava, Abhinav and Lim, Ser-Nam and Jiang, Yu-Gang},
  year         = {2022},
  month        = {Jun},
  pages        = {2354–2363},
  language     = {en}
}
 @article{Zhu_Qian_Zhao_Sun_Sun_2018,
  title        = {A deep learning approach to patch-based image inpainting forensics},
  volume       = {67},
  issn         = {09235965},
  doi          = {10.1016/j.image.2018.05.015},
  abstractnote = {Although image inpainting is now an effective image editing technique, limited work has been done for inpainting forensics. The main drawbacks of the conventional inpainting forensics methods lie in the difficulties on inpainting feature extraction and the very high computational cost. In this paper, we propose a novel approach based on a convolutional neural network (CNN) to detect patch-based inpainting operation. Specifically, the CNN is built following the encoder–decoder network structure, which allows us to predict the inpainting probability for each pixel in an image. To guide the CNN to automatically learn the inpainting features, a label matrix is generated for the CNN training by assigning a class label for each pixel of an image, and the designed weighted cross-entropy serves as the loss function. They further help to strongly supervise the CNN to capture the manipulation information rather than the image content features. By the established CNN, inpainting forensics does not need to consider feature extraction and classifier design, and use any postprocessing as in conventional forensics methods. They are combined into the unique framework and optimized simultaneously. Experimental results show that the proposed method achieves superior performance in terms of true positive rate, false positive rate and the running time, as compared with state-of-the-art methods for inpainting forensics, and is very robust against JPEG compression and scaling manipulations.},
  journal      = {Signal Processing: Image Communication},
  author       = {Zhu, Xinshan and Qian, Yongjun and Zhao, Xianfeng and Sun, Biao and Sun, Ya},
  year         = {2018},
  month        = {Sep},
  pages        = {90–99},
  language     = {en}
}

 @inproceedings{SRM_2018,
  address      = {Salt Lake City, UT, USA},
  title        = {Learning Rich Features for Image Manipulation Detection},
  isbn         = {978-1-5386-6420-9},
  url          = {https://ieeexplore.ieee.org/document/8578214/},
  doi          = {10.1109/CVPR.2018.00116},
  abstractnote = {Image manipulation detection is different from traditional semantic object detection because it pays more attention to tampering artifacts than to image content, which suggests that richer features need to be learned. We propose a two-stream Faster R-CNN network and train it endto-end to detect the tampered regions given a manipulated image. One of the two streams is an RGB stream whose purpose is to extract features from the RGB image input to ﬁnd tampering artifacts like strong contrast difference, unnatural tampered boundaries, and so on. The other is a noise stream that leverages the noise features extracted from a steganalysis rich model ﬁlter layer to discover the noise inconsistency between authentic and tampered regions. We then fuse features from the two streams through a bilinear pooling layer to further incorporate spatial co-occurrence of these two modalities. Experiments on four standard image manipulation datasets demonstrate that our two-stream framework outperforms each individual stream, and also achieves state-of-the-art performance compared to alternative methods with robustness to resizing and compression.},
  booktitle    = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  publisher    = {IEEE},
  author       = {Zhou, Peng and Han, Xintong and Morariu, Vlad I. and Davis, Larry S.},
  year         = {2018},
  month        = {Jun},
  pages        = {1053–1061},
  language     = {en}
}


 @inproceedings{CR_CNN_2020,
  title     = {Constrained R-CNN: A general image manipulation detection model},
  booktitle = {2020 IEEE International conference on multimedia and expo (ICME)},
  publisher = {IEEE},
  author    = {Yang, Chao and Li, Huizhou and Lin, Fangting and Jiang, Bin and Zhao, Hao},
  year      = {2020},
  pages     = {1–6}
}

 @inproceedings{MViT_2021,
  address      = {Montreal, QC, Canada},
  title        = {Multiscale Vision Transformers},
  isbn         = {978-1-66542-812-5},
  url          = {https://ieeexplore.ieee.org/document/9710800/},
  doi          = {10.1109/ICCV48922.2021.00675},
  abstractnote = {We present Multiscale Vision Transformers (MViT) for video and image recognition, by connecting the seminal idea of multiscale feature hierarchies with transformer models. Multiscale Transformers have several channel-resolution scale stages. Starting from the input resolution and a small channel dimension, the stages hierarchically expand the channel capacity while reducing the spatial resolution. This creates a multiscale pyramid of features with early layers operating at high spatial resolution to model simple low-level visual information, and deeper layers at spatially coarse, but complex, high-dimensional features. We evaluate this fundamental architectural prior for modeling the dense nature of visual signals for a variety of video recognition tasks where it outperforms concurrent vision transformers that rely on large scale external pre-training and are 5-10× more costly in computation and parameters. We further remove the temporal dimension and apply our model for image classiﬁcation where it outperforms prior work on vision transformers. Code is available at: https: //github.com/facebookresearch/SlowFast.},
  booktitle    = {2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
  publisher    = {IEEE},
  author       = {Fan, Haoqi and Xiong, Bo and Mangalam, Karttikeya and Li, Yanghao and Yan, Zhicheng and Malik, Jitendra and Feichtenhofer, Christoph},
  year         = {2021},
  month        = {Oct},
  pages        = {6804–6815},
  language     = {en}
}
 @inproceedings{MViTv2_2022,
  address      = {New Orleans, LA, USA},
  title        = {MViTv2: Improved Multiscale Vision Transformers for Classification and Detection},
  isbn         = {978-1-66546-946-3},
  url          = {https://ieeexplore.ieee.org/document/9879809/},
  doi          = {10.1109/CVPR52688.2022.00476},
  abstractnote = {In this paper, we study Multiscale Vision Transformers (MViTv2) as a unified architecture for image and video classification, as well as object detection. We present an improved version of MViT that incorporates decomposed relative positional embeddings and residual pooling connections. We instantiate this architecture in five sizes and evaluate it for ImageNet classification, COCO detection and Kinetics video recognition where it outperforms prior work. We further compare MViTv2s’ pooling attention to window attention mechanisms where it outperforms the latter in accuracy/compute. Without bells-and-whistles, MViTv2 has state-of-the-art performance in 3 domains: 88.8% accuracy on ImageNet classification, 58.7 APbox on COCO object detection as well as 86.1% on Kinetics-400 video classification. Code and models are available at https: //github.com/facebookresearch/mvit.},
  booktitle    = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  publisher    = {IEEE},
  author       = {Li, Yanghao and Wu, Chao-Yuan and Fan, Haoqi and Mangalam, Karttikeya and Xiong, Bo and Malik, Jitendra and Feichtenhofer, Christoph},
  year         = {2022},
  month        = {Jun},
  pages        = {4794–4804},
  language     = {en}
}

 @inproceedings{Swin_2021,
  address      = {Montreal, QC, Canada},
  title        = {Swin Transformer: Hierarchical Vision Transformer using Shifted Windows},
  isbn         = {978-1-66542-812-5},
  url          = {https://ieeexplore.ieee.org/document/9710580/},
  doi          = {10.1109/ICCV48922.2021.00986},
  abstractnote = {This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with Shifted windows. The shifted windowing scheme brings greater efﬁciency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the ﬂexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classiﬁcation (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO testdev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-theart by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneﬁcial for all-MLP architectures. The code and models are publicly available at https://github. com/microsoft/Swin-Transformer.},
  booktitle    = {2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
  publisher    = {IEEE},
  author       = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  year         = {2021},
  month        = {Oct},
  pages        = {9992–10002},
  language     = {en}
}
 @inproceedings{PVT_2021,
  address   = {Montreal, QC, Canada},
  title     = {Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions},
  isbn      = {978-1-66542-812-5},
  url       = {https://ieeexplore.ieee.org/document/9711179/},
  doi       = {10.1109/ICCV48922.2021.00061},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
  publisher = {IEEE},
  author    = {Wang, Wenhai and Xie, Enze and Li, Xiang and Fan, Deng-Ping and Song, Kaitao and Liang, Ding and Lu, Tong and Luo, Ping and Shao, Ling},
  year      = {2021},
  month     = {Oct},
  pages     = {548–558},
  language  = {en}
}

 @inproceedings{mmnet_2021,
  address      = {Shenzhen, China},
  title        = {Multi-Modality Image Manipulation Detection},
  isbn         = {978-1-66543-864-3},
  url          = {https://ieeexplore.ieee.org/document/9428232/},
  doi          = {10.1109/ICME51207.2021.9428232},
  abstractnote = {State-of-the-art multi-stream methods for image manipulation detection suffer from gaps between features. Moreover, the rapid development of GANs makes it an emerging method of image tampering. However, existing natural scene image tampered datasets are limited to manual tampering. In this paper, we address these two issues. Firstly, we propose a novel two-stream multi-modality image manipulation detection model (MM-net) that abandons the way of fusion. The main idea of the proposed model is to exploit one stream to guide the learning of the other stream through attention mechanism. Therefore, our approach enjoys the beneﬁts of the multi-stream methods while avoiding the semantic gaps caused by bridging gaps between different streams. Secondly, we build the ﬁrst tampered dataset of natural images based on GANs, pushing manipulation detection toward more realistic and challenging scenarios. Extensive experimental results demonstrate that our model outperforms state-of-the-art approaches on both manually and GANs tampered images.},
  booktitle    = {2021 IEEE International Conference on Multimedia and Expo (ICME)},
  publisher    = {IEEE},
  author       = {Yang, Chao and Wang, Zhiyu and Shen, Huawei and Li, Huizhou and Jiang, Bin},
  year         = {2021},
  month        = {Jul},
  pages        = {1–6},
  language     = {en}
}


 @article{ViTDet_2022,
  title        = {Exploring Plain Vision Transformer Backbones for Object Detection},
  url          = {http://arxiv.org/abs/2203.16527},
  abstractnote = {We explore the plain, non-hierarchical Vision Transformer (ViT) as a backbone network for object detection. This design enables the original ViT architecture to be fine-tuned for object detection without needing to redesign a hierarchical backbone for pre-training. With minimal adaptations for fine-tuning, our plain-backbone detector can achieve competitive results. Surprisingly, we observe: (i) it is sufficient to build a simple feature pyramid from a single-scale feature map (without the common FPN design) and (ii) it is sufficient to use window attention (without shifting) aided with very few cross-window propagation blocks. With plain ViT backbones pre-trained as Masked Autoencoders (MAE), our detector, named ViTDet, can compete with the previous leading methods that were all based on hierarchical backbones, reaching up to 61.3 AP_box on the COCO dataset using only ImageNet-1K pre-training. We hope our study will draw attention to research on plain-backbone detectors. Code for ViTDet is available in Detectron2.},
  note         = {arXiv:2203.16527 [cs]},
  number       = {arXiv:2203.16527},
  publisher    = {arXiv},
  author       = {Yanghao, Li and Mao, Hanzi and Girshick, Ross and He, Kaiming},
  year         = {2022},
  month        = {Jun}
}

 @article{benchmark_ViT_2021, title={Benchmarking Detection Transfer Learning with Vision Transformers}, url={http://arxiv.org/abs/2111.11429}, abstractNote={Object detection is a central downstream task used to test if pre-trained network parameters confer beneﬁts, such as improved accuracy or training speed. The complexity of object detection methods can make this benchmarking non-trivial when new architectures, such as Vision Transformer (ViT) models, arrive. These difﬁculties (e.g., architectural incompatibility, slow training, high memory consumption, unknown training formulae, etc.) have prevented recent studies from benchmarking detection transfer learning with standard ViT models. In this paper, we present training techniques that overcome these challenges, enabling the use of standard ViT models as the backbone of Mask R-CNN. These tools facilitate the primary goal of our study: we compare ﬁve ViT initializations, including recent state-of-the-art self-supervised learning methods, supervised initialization, and a strong random initialization baseline. Our results show that recent masking-based unsupervised learning methods may, for the ﬁrst time, provide convincing transfer learning improvements on COCO, increasing APbox up to 4% (absolute) over supervised and prior self-supervised pre-training methods. Moreover, these masking-based initializations scale better, with the improvement growing as model size increases.}, note={arXiv:2111.11429 [cs]}, number={arXiv:2111.11429}, publisher={arXiv}, author={Li, Yanghao and Xie, Saining and Chen, Xinlei and Dollar, Piotr and He, Kaiming and Girshick, Ross}, year={2021}, month={Nov}, language={en} }


@inproceedings{FCN_2015,
  title={Fully convolutional networks for semantic segmentation},
  author={Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={3431--3440},
  year={2015}
}

 @article{SegFormer_2021, title={SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers}, url={http://arxiv.org/abs/2105.15203}, abstractNote={We present SegFormer, a simple, efﬁcient yet powerful semantic segmentation framework which uniﬁes Transformers with lightweight multilayer perceptron (MLP) decoders. SegFormer has two appealing features: 1) SegFormer comprises a novel hierarchically structured Transformer encoder which outputs multiscale features. It does not need positional encoding, thereby avoiding the interpolation of positional codes which leads to decreased performance when the testing resolution differs from training. 2) SegFormer avoids complex decoders. The proposed MLP decoder aggregates information from different layers, and thus combining both local attention and global attention to render powerful representations. We show that this simple and lightweight design is the key to efﬁcient segmentation on Transformers. We scale our approach up to obtain a series of models from SegFormer-B0 to SegFormer-B5, reaching signiﬁcantly better performance and efﬁciency than previous counterparts. For example, SegFormer-B4 achieves 50.3% mIoU on ADE20K with 64M parameters, being 5× smaller and 2.2% better than the previous best method. Our best model, SegFormer-B5, achieves 84.0% mIoU on Cityscapes validation set and shows excellent zero-shot robustness on Cityscapes-C. Code will be released at: github.com/NVlabs/SegFormer.}, note={arXiv:2105.15203 [cs]}, number={arXiv:2105.15203}, publisher={arXiv}, author={Xie, Enze and Wang, Wenhai and Yu, Zhiding and Anandkumar, Anima and Alvarez, Jose M. and Luo, Ping}, year={2021}, month={Oct}, language={en} }


@misc{morphology_1983image,
  title={Image Analysis and Mathematical Morphology},
  author={Serra, Jean},
  year={1983},
  publisher={Academic Press, Inc.}
}

 @article{cosine_decay_2017, title={SGDR: Stochastic Gradient Descent with Warm Restarts}, url={http://arxiv.org/abs/1608.03983}, abstractNote={Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets, where we demonstrate new state-of-the-art results at 3.14% and 16.21%, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at https://github.com/loshchil/SGDR}, note={arXiv:1608.03983 [cs, math]}, number={arXiv:1608.03983}, publisher={arXiv}, author={Loshchilov, Ilya and Hutter, Frank}, year={2017}, month={May} }


 @article{AdamW_2019, title={Decoupled Weight Decay Regularization}, url={http://arxiv.org/abs/1711.05101}, abstractNote={L$_2$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is emph{not} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L$_2$ regularization (often calling it “weight decay” in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by emph{decoupling} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam’s generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW}, note={arXiv:1711.05101 [cs, math]}, number={arXiv:1711.05101}, publisher={arXiv}, author={Loshchilov, Ilya and Hutter, Frank}, year={2019}, month={Jan} }


 @article{mvsspp_2022, title={MVSS-Net: Multi-View Multi-Scale Supervised Networks for Image Manipulation Detection}, ISSN={0162-8828, 2160-9292, 1939-3539}, DOI={10.1109/TPAMI.2022.3180556}, abstractNote={As manipulating images by copy-move, splicing and/or inpainting may lead to misinterpretation of the visual content, detecting these sorts of manipulations is crucial for media forensics. Given the variety of possible attacks on the content, devising a generic method is nontrivial. Current deep learning based methods are promising when training and test data are well aligned, but perform poorly on independent tests. Moreover, due to the absence of authentic test images, their image-level detection speciﬁcity is in doubt. The key question is how to design and train a deep neural network capable of learning generalizable features sensitive to manipulations in novel data, whilst speciﬁc to prevent false alarms on the authentic. We propose multi-view feature learning to jointly exploit tampering boundary artifacts and the noise view of the input image. As both clues are meant to be semantic-agnostic, the learned features are thus generalizable. For effectively learning from authentic images, we train with multi-scale (pixel / edge / image) supervision. We term the new network MVSS-Net and its enhanced version MVSS-Net++. Experiments are conducted in both within-dataset and cross-dataset scenarios, showing that MVSS-Net++ performs the best, and exhibits better robustness against JPEG compression, Gaussian blur and screenshot based image re-capturing.}, journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, author={Dong, Chengbo and Chen, Xinru and Hu, Ruohan and Cao, Juan and Li, Xirong}, year={2022}, pages={1–14}, language={en} }

 @inproceedings{HPFCN_2019, address={Seoul, Korea (South)}, title={Localization of Deep Inpainting Using High-Pass Fully Convolutional Network}, ISBN={978-1-72814-803-8}, url={https://ieeexplore.ieee.org/document/9009804/}, DOI={10.1109/ICCV.2019.00839}, abstractNote={Image inpainting has been substantially improved with deep learning in the past years. Deep inpainting can ﬁll image regions with plausible contents, which are not visually apparent. Although inpainting is originally designed to repair images, it can even be used for malicious manipulations, e.g., removal of speciﬁc objects. Therefore, it is necessary to identify the presence of inpainting in an image. This paper presents a method to locate the regions manipulated by deep inpainting. The proposed method employs a fully convolutional network that is based on high-pass ﬁltered image residuals. Firstly, we analyze and observe that the inpainted regions are more distinguishable from the untouched ones in the residual domain. Hence, a high-pass pre-ﬁltering module is designed to get image residuals for enhancing inpainting traces. Then, a feature extraction module, which learns discriminative features from image residuals, is built with four concatenated ResNet blocks. The learned feature maps are ﬁnally enlarged by an upsampling module, so that a pixel-wise inpainting localization map is obtained. The whole network is trained end-toend with a loss addressing the class imbalance. Extensive experimental results evaluated on both synthetic and realistic images subjected to deep inpainting have shown the effectiveness of the proposed method.}, booktitle={2019 IEEE/CVF International Conference on Computer Vision (ICCV)}, publisher={IEEE}, author={Li, Haodong and Huang, Jiwu}, year={2019}, month={Oct}, pages={8300–8309}, language={en} }

@inproceedings{transforensic_2021, address={Montreal, QC, Canada}, title={TransForensics: Image Forgery Localization with Dense Self-Attention}, ISBN={978-1-66542-812-5}, url={https://ieeexplore.ieee.org/document/9711339/}, DOI={10.1109/ICCV48922.2021.01478}, abstractNote={Nowadays advanced image editing tools and technical skills produce tampered images more realistically, which can easily evade image forensic systems and make authenticity verification of images more difficult. To tackle this challenging problem, we introduce TransForensics, a novel image forgery localization method inspired by Transformers. The two major components in our framework are dense self-attention encoders and dense correction modules. The former is to model global context and all pairwise interactions between local patches at different scales, while the latter is used for improving the transparency of the hidden layers and correcting the outputs from different branches. Compared to previous traditional and deep learning methods, TransForensics not only can capture discriminative representations and obtain high-quality mask predictions but is also not limited by tampering types and patch sequence orders. By conducting experiments on main benchmarks, we show that TransForensics outperforms the stateof-the-art methods by a large margin.}, booktitle={2021 IEEE/CVF International Conference on Computer Vision (ICCV)}, publisher={IEEE}, author={Hao, Jing and Zhang, Zhixin and Yang, Shicai and Xie, Di and Pu, Shiliang}, year={2021}, month={Oct}, pages={15035–15044}, language={en} }


 @inproceedings{CFL_Net_2023, address={Waikoloa, HI, USA}, title={CFL-Net: Image Forgery Localization Using Contrastive Learning}, ISBN={978-1-66549-346-8}, url={https://ieeexplore.ieee.org/document/10030939/}, DOI={10.1109/WACV56688.2023.00462}, booktitle={2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)}, publisher={IEEE}, author={Niloy, Fahim Faisal and Kumar Bhaumik, Kishor and Woo, Simon S.}, year={2023}, month={Jan}, pages={4631–4640}, language={en} 
}
main