\section{Experiments}
% The experimental design in the field of image manipulation localization is currently very chaotic. We hope to clarify some of the characteristics in the experimental section and create a general benchmark.



% we urge for the adoption of open-source to the community and call for the generation strategies for large-scale datasets can be assessed separately from the model performance itself to ensure fairness and promote continued advancements in this field.
% to demonstrate the potential of IML-ViT as an effective benchmark, we strive to conduct a comprehensive comparison with existing models in a fair manner by evaluation with three mainstream evaluation protocols.

\subsection{Experimental Setup}

\textbf{Evaluation barrier for IML} 
While recent studies have introduced numerous SoTA models, comparing them on an equal footing remains challenging. This is due to the following reasons: 1) lack of publicly available code for the models and training processes~\cite{SPAN_2020, MVSS_2021}; 2) utilization of massive synthesized datasets that are inaccessible to the wider research community~\cite{Mantra_2019,mmnet_2021, objectformer_2022};
3) training and testing datasets often vary across different papers, also bringing difficulty for comparison.

\textbf{Datasets and Evaluation Protocol}
% Considering the significance and practice of various comparison methods, we demarcate them into three mainstream evaluation protocols by how they apply to datasets. These protocols endow future works with a consistent and reproducible means of assessment.
To facilitate reproducibility and overcome the existing evaluation barrier, we demarcate existing mainstream IML methods into three distinct protocols based on different partitions of datasets. Subsequently, we compare IML-ViT against SoTA methods with these three protocols, as shown in Table \ref{tab:datasets} and Table \ref{tab:protocols}. We followed MVSS-Net~\cite{MVSS_2021} to create Defacto-12k dataset. More details will be discussed in Section \ref{sec:compare_sota}.

% evaluate IML-ViT with three different mainstream protocols that are utilized by recent SoTA methods.

% Note that some papers may pre-train their model with large private datasets in advance.
% In detail, the protocols are: 1) train the model on the CASIAv2~\cite{CASIA_2013} dataset
% \footnote{We noticed some resolution errors in public CASIAv2 dataset. Therefore, we have released an adjusted version of CASIAv2 with corrected ground truth. Details can be found at \url{https://github.com/SunnyHaze/CASIA2.0-Corrected-Groundtruth}}
% and then test it on other smaller public datasets including CASIAv1~\cite{CASIA_2013}, NIST16~\cite{NIST16_2019}, COVERAGE~\cite{Coverage_2016}, Columbia~\cite{Columbia_2006} and Defacto~\cite{defacto_2019}, see Table \ref{tab:datasets}. Note that we followed the MVSS-Net~\cite{MVSS_2021} to create the Defacto-12k dataset for validation. 2) T




% To make a fair comparison with SoTA we mainly evaluate the performance of our model based on a commonly used protocol~\cite{MVSS_2021,GSR_Net_2020,SPAN_2020} for IML task. We train the model on the CASIAv2~\cite{CASIA_2013} dataset
% \footnote{We noticed some resolution errors in public CASIAv2 dataset. Therefore, we have released an adjusted version of CASIAv2 with corrected ground truth. Details can be found at \url{https://github.com/SunnyHaze/CASIA2.0-Corrected-Groundtruth}}
% and then test it on other smaller public datasets including CASIAv1~\cite{CASIA_2013}, NIST16~\cite{NIST16_2019}, COVERAGE~\cite{Coverage_2016}, Columbia~\cite{Columbia_2006} and Defacto~\cite{defacto_2019}, see Table \ref{tab:datasets}. Note that we followed the MVSS-Net~\cite{MVSS_2021} to create the Defacto-12k dataset for validation. 


\begin{table}[t]
\caption{\textbf{All datasets in our experiments.}}
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{@{}cccccccc@{}}
\toprule[2pt]
\multirow{2}{*}{\textbf{Dataset}} & \multicolumn{2}{c}{\textbf{Type}}         & \multicolumn{3}{c}{\textbf{Manipulation type}}  & \multicolumn{2}{c}{\textbf{Resolution}} \\ \cmidrule(l){2-8} 
                                  & \textbf{Authentic} & \textbf{Manipulated} & \textbf{copymv} & \textbf{spli} & \textbf{inpa} & \textbf{min}       & \textbf{max}       \\ \midrule
CASIAv2~\cite{CASIA_2013}         & 7491               & 5123                 & 3274            & 1849          & 0             & 240                & 800                \\
CASIAv1~\cite{CASIA_2013}         & 800                & 920                  & 459             & 461           & 0             & 256                & 384                \\
NIST16~\cite{NIST16_2019}         & 0                  & 564                  & 68              & 288           & 208           & 480                & 5616               \\
COVERAGE~\cite{Coverage_2016}     & 100                & 100                  & 100             & 0             & 0             & 158                & 572                \\
Defacto-12k~\cite{defacto_2019}   & 6000               & 6000                 & 2000            & 2000          & 2000          & 120                & 640                \\
Columbia~\cite{Coverage_2016}     & 183                & 180                  & 0               & 180           & 0             & 568                & 1152               \\
IMD-20~\cite{IMD20_2020}          & 415                & 2010                 & -               & -             & -             & 176                & 4437               \\
tampCOCO~\cite{CAT-Net2022}       & 0                  & 800000               & 600000          & 200000        & 0             & 51                 & 640                \\
JPEG RAISE~\cite{CAT-Net2022}     & 24462              & 0                    & -               & -             & -             & 1515                  & 6159                  \\ \bottomrule[2pt]
\end{tabular}
}
% In the "resolution" column, the "min" value represents the minimum edge length among all images in the corresponding dataset, while the "max" value represents the maximum edge length. The NIST and Columbia datasets exceeded the edge length limit we set, so we needed to resize them before inputting them into the model.
\label{tab:datasets}
\end{table}

\input{tables/protocol_define}





% Please add the following required packages to your document preamble:
% \usepackage{booktabs}

\textbf{Evaluation Criteria} 
We evaluate our model using pixel-level $F_1$ score with a fixed threshold $0.5$ and Area Under the Curve (AUC), which are commonly used evaluation metrics in previous works. Both of them are metrics where higher values indicate better performance. However, it's worth noting that AUC can be influenced by excessive true-negative pixels in IML datasets, leading to an overestimation of model performance. Nevertheless, our model achieves SoTA performance in both $F_1$ score and AUC.

% In some previous methods, \textit{optimal F1 score} is used for evaluation, where the best $F_1$ score is selected by testing the prediction probability map at different thresholds. However, this approach becomes impractical in real-world scenarios since the optimal threshold for the real-world distribution is typically unknown. 
% Instead, we report the pixel-level $F_1$ score using a uniform threshold of $0.5$, providing a more practical metric for evaluating the model's performance.



% \begin{table*}[t]
% \centering
% \resizebox*{1.7\columnwidth}{!}{ 
% \begin{tabular}{@{}c|c|ccc|ccccc@{}}
% \toprule[2pt]
% \multirow{2}{*}{\textbf{Control variables}} &
%   \multirow{2}{*}{\textbf{init method}} &
%   \multicolumn{3}{c|}{\textbf{Components}} &
%   \multicolumn{4}{c}{\textbf{Pixel-level $F_1$}} &
%   \textbf{} \\ \cmidrule(l){3-10} 
%  &
%    &
%   \textbf{H-Reso} &
%   \textbf{SFPN} &
%   \textbf{Edge} &
%   \textbf{CASIAv1} &
%   \textbf{Coverage} &
%   \textbf{Columbia} &
%   \textbf{NIST16} &
%   \textbf{MEAN} \\ \midrule
% \multirow{2}{*}{w/o MAE} & Xavier          & + & + & + & 0.1035 & 0.0439 & 0.0744 & 0.0632     & 0.0713 \\
%                       & ViT-B ImNet-21k & + & + & + &  0.5114    & 0.1854     & 0.2287 & 0.1811      & 0.2767 \\
% w/o high resolution            & MAE ImNet-1k    & - & + & + & 0.5061      & 0.2324 & 0.5409  & 0.2987      & 0.3945 \\
% w/o multi-scale           & MAE ImNet-1k    & + & - & + & \textbf{0.5996} & \textbf{0.4457} & 0.6125 & 0.1841 & 0.4605 \\
% w/o edge supervision            & MAE ImNet-1k    & + & + & - & 0.5432 & 0.2688 & 0.3008  & 0.2347 & 0.3369 \\
% \textit{\textbf{Full setup}}            & MAE ImNet-1k    & + & + & + & 0.5886 & 0.3277 & \textbf{0.7445} & \textbf{0.2993} & \textbf{0.4900}\\ \bottomrule[2pt]
% \end{tabular}
% }
% \caption{\textbf{Ablation study of IML-ViT paradigm.} Each model is trained for 200 epochs on CASIAv2 dataset \textbf{without authentic images}. Best scores are marked in bold. 
% %\textit{H-Reso} refers to high resolution; \textit{SFPN} refers to simple feature pyramid; and \textit{Edge} refers to edge supervision
% }
% \label{tab:ablation}
% \end{table*}

\textbf{Implementation}  We pad all images to a resolution of 1024x1024, except for those that exceed this limit. For the larger images, we resized them to the longer side to 1024 and maintained their aspect ratio. During training, following MVSS-Net~\cite{MVSS_2021}, common data augmentation techniques were applied, including re-scaling, flipping, blurring, rotation, and various naive manipulations (e.g., randomly copy-moving or inpainting rectangular areas within a single image).
We used the AdamW optimizer~\cite{AdamW_2019} with a base learning rate of 1e-4, scheduled with a cosine decay strategy~\cite{cosine_decay_2017}. The early stop technique was employed during training.

\textbf{Complexity}
Training IML-ViT with a batch size of 2 per GPU consumed 22GB of GPU memory per card. Using four NVIDIA 3090 GPUs, the model was trained on a dataset of 12,000 images over 200 epochs, taking approximately 12 hours. For inference, a batch size of 6 per GPU required 20GB of GPU memory, with an average prediction time of 0.094 seconds per image. Reducing the batch size to 1 decreased the GPU memory requirement to 5.4GB. We also compare the number of parameters and FLOPs with SoTA models in Table \ref{tab:compexity} and achieve highly competitive results.

\input{tables/flops}



% \footnotetext{Since the training code for MVSS-Net~\cite{MVSS_2021} was unavailable, we reproduced it from scratch with their official model code follow their implementations in their paper. However we observes a gap between scores in this model and what they reported.} 


% \begin{table}[h]
% \centering

% \resizebox*{0.95\columnwidth}{!}{ 
% \begin{tabular}{@{}lll@{}}
% \toprule[2pt]
% \textbf{Method}      & \textbf{Pre-train}           & \textbf{$F_1$(\%)} \\ \midrule
% RGB-N, CVPR18~\cite{RGBN_2018}        & ImageNet                             & 40.8                   \\
% SPAN, ECCV20~\cite{SPAN_2020}        & Private synthesized dataset          & 38.2                   \\
% Objectformer, CVPR22~\cite{objectformer_2022} & Private synthesized dataset          & 57.9                   \\
% \textit{IML-ViT(Ours)}        & MAE on ImageNet-1k          & \textbf{72.1}                   \\ \bottomrule[2pt]
% \end{tabular}
% }
% \caption{\textbf{Performance comparison with Closed-source methods} All the methods above are fine-tuning with CASIAv2 and are tested with the CASIAv1 dataset.  }
% \label{tab:close}
% \end{table}


% \begin{table}[h]
% \centering
% \resizebox{0.95\columnwidth}{!}{
% \begin{tabular}{@{}llllll@{}}
% \toprule[2pt]
% \textbf{Method}                & \textbf{CASIAv1}  & \textbf{Coverage} & \textbf{Columbia} & \textbf{NIST16} & \textbf{MEAN}     \\ \midrule

% ObjectFormer~\cite{objectformer_2022} & 0.882 &- &- &-&- \\
% CFL-Net~\cite{CFL_Net_2023} & 0.863 & - & - & 0.799 & - \\
% \textit{IML-ViT(Ours)} & \textbf{0.931} & \textbf{0.918} & \textbf{0.962} & \textbf{0.818} & \textbf{0.917} \\
% \bottomrule[2pt]
% \end{tabular}
% }
% \caption{\textbf{Comparison of AUC scores trained on CASIAv2.}}
% \label{tab:auc_casia}
% \end{table}

\input{tables/protocol1}
\input{tables/protocol3}
\input{tables/protocol1_closed_source}
\input{tables/protocol2}



% Figure environment removed


\subsection{Compare with SoTA (See Table \ref{tab:protocols} for protocols)}
\label{sec:compare_sota}



\textbf{Protocol No.1} 
% For a comprehensive evaluation on five datasets, we select six reproducible methods with publicly available code or trained models as baselines. An overview of these methods can be found in Table \ref{tab:1}.
Since MVSS-Net~\cite{MVSS_2021} has already conducted a detailed evaluation on a fair cross-dataset protocol and later works~\cite{NCL_IML_2023} followed their setting, we directly quote their results here and train our models with the same protocol.
% , i.e. training on both authentic and manipulated images of CASIAv2 dataset and testing on public datasets. 
The results measured by $F_1$ score are listed respectively in Table \ref{tab:sota}. 
We also compare this with some closed-source methods that only report their AUC tested on CASIAv1 in Table \ref{tab:close}.

% in Table \ref{tab:close}. 




% Moreover, ObjectFormer~\cite{objectformer_2022} and CFL-Net~\cite{CFL_Net_2023} evaluate their models fine-tuning with CASIAv2 on AUC. Although this metric may overestimate the models, IML-ViT has still surpassed them, as shown in Table \ref{tab:auc_casia}.




Overall, our model achieves SoTA performance on this cross-dataset evaluation protocol. Figure \ref{fig:visualization}
illustrates that our model portrays high-quality and clear edges under different preferences of manipulation types.




\textbf{Protocol No.2} TruFor~\cite{trufor2023} is a recent strong method with extensive experimental results, training on six relatively large IML datasets proposed by CAT-Netv2~\cite{CAT-Net2022}. In our aim to establish IML-ViT as the benchmark model, we adopt their protocol to compare our model. We outperform them on four benchmark datasets. Details are shown in Table \ref{tab:Trufor}. 

% Figure environment removed

% Figure environment removed

\input{tables/ablation_study}

% Figure environment removed 





\textbf{Protocol No.3} 
TransForensic~\cite{transforensic_2021}, ObjectFormer~\cite{objectformer_2022}, HiFi-Net~\cite{HiFi-Net2023} and CFL-Net~\cite{CFL_Net_2023} reported their performance based on mixed datasets. 
They randomly split these datasets into training/validation/testing splits, causing the random splits performed by others to potentially differ, leading to a certain degree of unfairness.
Therefore, we do not recommend using this protocol in future work. However, for the sake of comparison with these state-of-the-art models, we also test IML-ViT following this protocol. 
Besides, note that HiFi-Net (1,710K images) and Objectformer (62k images) involve large IML datasets for pre-training, then tune on the specific small dataset, while we only pre-train with ImageNet-1k. Thus, we directly use results from mixed public IML datasets(14k images) to compare with them. Otherwise, it's easy to overfit on small datasets. In summary, experiment results in Table \ref{tab:AUC} show that, under this reasonable evaluation criteria, IML-ViT also outperforms these SoTA methods.

% Particularly, in the case of NIST16, there exists a considerable number of duplicated samples, and random splitting might cause the same image to appear repeatedly in both the train and test sets, resulting in artificially inflated results. This makes NIST16 unsuitable as a reliable benchmark. 





% \footnotetext{There exists a considerable number of extremely similar images in the NIST16 dataset. Thus, random splitting might cause similar images to appear repeatedly in the train and test sets, resulting in artificially inflated results.}





\subsection{Ablation Studies}
\label{lab:ablation}

To evaluate the contributions of each component to the model performance, we conducted experiments with multiple settings and compared them with a \textit{full setup} to test the four aspects we are most concerned about.
For \textit{initialization}, besides \textit{full setup} with MAE pre-training on ImageNet-1k, we test Xavier~\cite{Xavier_2010} initialization and ordinary ViT pre-training on ImageNet-21k.
To explore the impact of \textit{high resolution}, we resized all images to 512Ã—512 during training before applying our padding strategy. For \textit{edge supervision}, we remove the edge loss for evaluation, while for \textit{multi-scale supervision}, we replace the module with the same number of plain convolution layers.


To reduce expenses, we trained model \textit{only} with \textbf{Protocol \textit{No.1}} in the ablation study. Qualitative results are illustrated in Figure \ref{fig:ablation}, which vividly demonstrates the efficacy of each component in our method. For quantitative results in Table \ref{tab:ablation}, our findings are:


\textbf{MAE pretrain is mandatory.} 
Indeed, dataset insufficiency is a significant challenge in building ViT-based IML methods. 
% As IML tasks require dense annotation and expert tamper generation, which are very labor-intensive, public data sets for image manipulation localization are small in size, 
As shown in Table \ref{tab:datasets}, public datasets for IML are small in size, which cannot satisfy the appetite of vanilla ViT. As shown in \textit{w/o MAE}
aspects in Table \ref{tab:ablation}, the use of Xavier initialization to train the model resulted in complete non-convergence. However, while regular ViT pre-training initialization with Imagenet-21k achieves acceptable performance on CASIAv1, which is homologous to CASIAv2, it exhibits poor generalization ability on other non-homology datasets.
This indicates that MAE greatly alleviates the problem of non-convergence and over-fitting of ViT on limited IML datasets.


\textbf{Edge supervision is crucial.}
The performance of IML-ViT without edge loss shows significant variability with different random seeds, all leading to gradient collapse eventually, where the F1 score reaches 0, and the loss becomes \textit{NaN}, as shown in Figure \ref{fig:edge_loss}. In contrast, when employing edge loss, all performance plots exhibit consistent behavior similar to the blue line in Figure \ref{fig:edge_loss}, enabling fast convergence and smooth training up to 200 epochs. Furthermore, Table \ref{tab:ablation} confirms the effectiveness of edge loss in contributing to the final performance. In summary, these results demonstrate that edge supervision effectively stabilizes IML-ViT convergence and can serve as highly efficient prior knowledge for IML problems.




\textbf{High resolution is effective for artifacts.} 
The improved performance shown in Table \ref{tab:ablation} for the \textit{full setup} model across four datasets validates the effectiveness of the high-resolution strategy. However, it is essential to note that the NIST16 dataset shows limited improvement when using higher resolutions. This observation can be attributed to the fact that the NIST16 dataset contains numerous images with resolutions exceeding 2000, and down-sampling these images to 1024 for testing may lead to considerable distortion of the original artifacts, consequently reducing the effectiveness of learned features. Nevertheless, when considering the SoTA score achieved, it becomes evident that IML-ViT can flexibly infer the manipulated area based on the richness of different information types.

\textbf{Multi-scale supervision helps generalize.} All these datasets exhibit significant variations in the proportion of manipulated area, particularly where CASIAv2 has 8.96\% of the pixels manipulated, COVERAGE dataset has 11.26\%, Columbia dataset has 26.32\%, and NIST16 has 7.54\%. 
% Additionally, the number of parameters in \textit{w/o multiscale} is identical to the \textit{full setup}. 
Nevertheless, the comprehensive improvements in Table \ref{tab:ablation} with the aid of multi-scale supervision indicate that this technique can effectively bridge the gap in dataset distribution, enhancing generalization performance.

\subsection{Robustness Evaluation}
We conducted a robustness evaluation on our IML-ViT model following MVSS-Net~\cite{MVSS_2021}. We utilized their protocol with three common types of attacks, including JPEG compression, Gaussian Noise, and Gaussian Blur. As shown in Figure \ref{fig:robustness}, IML-ViT achieved very competitive results among SoTA models, which proved to possess excellent robustness.
