% \maketitle
%%%%%%%%% ABSTRACT
\newpage

\appendix


% \section{Complexity and training costs.} 
% Training IML-ViT with a batch size of 2 per GPU consumed 22GB of GPU memory per card. Using four NVIDIA 3090 GPUs, the model was trained on a dataset of 12,000 images over 200 epochs, taking approximately 12 hours. For inference, a batch size of 6 per GPU required 20GB of GPU memory, with an average prediction time of 0.094 seconds per image. Reducing the batch size to 1 decreased the GPU memory requirement to 5.4GB.
% Table \ref{tab:compexity} compares the complexity, measured in FLOPs and parameters, of IML-ViT with SoTA models. Overall, IML-ViT demonstrates competitive training and testing costs compared to other SoTA models.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \begin{table}[h]
% \caption{\textbf{Complexity of IML-ViT compared to open-sourced SoTA models.}}
% \label{tab:compexity}
% \centering
% \resizebox*{0.8\columnwidth}{!}{ 
% \begin{tabular}{@{}llcccc@{}}
% \toprule[2pt]
% \textbf{Method} & \textbf{Paper} & \textbf{Infer. Time(sec)/ Image} & \textbf{Params.(M)} & \textbf{512×512 FLOPS(G)} & \textbf{1024×1024 FLOPS(G)} \\ \midrule
% MVSS-Net\cite{MVSS_2021, mvsspp_2022}        & ICCV21\&TPAMI22         & 2.929                            & 147                 & 167                       & 683                         \\
% PSCC-Net~\cite{pscc_net_2022}        & TCSVT22        & 0.072                            & 3                   & 120                       & 416                         \\
% HiFi-Net~\cite{HiFi-Net2023}        & CVPR23         & 1.512                            & 7                   & 404                       & 3470                        \\
% TruFor~\cite{trufor2023}          & CVPR23         & 1.231                            & 68                  & 231                       & 1016                        \\
% IML-ViT         & Ours           & 0.094                            & 91                  & 136                       & 445                         \\ \bottomrule[2pt]
% \end{tabular}
% }
% \end{table}

\section{Futher Robustness Evaluation}
\label{app:robustness}
JPEG compression, Gaussian Noise, and Gaussian Blur are the common attack methods for Image manipulation localization. Following the convention from TruFor~\cite{trufor2023} and MVSS-Net~\cite{MVSS_2021}, we further carried out experiments on the resistance of these operations on Protocol No.1 and No.2. The evaluation results are shown in Figure \ref{fig:robustness_trufor} and  Figure \ref{fig:robustness2}. The IML-ViT exhibited excellent resistance to these attack methods and consistently maintained the best performance of the models.  


% Figure environment removed


% Figure environment removed




% \footnotetext{Performance against JPEG compression is quoted from MVSS-Net, while performance against Gaussian Blur is retested by us using the publicly available model because we found a significant discrepancy between our tests and the performance reported in their paper.}


\section{More Implementation Details}
\label{app:implementation details}

\subsection{High-resolution ViT-Base} Mostly following the original Vision Transformer, we implemented our model with a stack of Transformer blocks stacking together. LN are employed in the self-attention head and MLP blocks. Every two windowed attention blocks are followed by a global attention block. The windowed attention block only computes self-attention in a small, non-overlapped window, while the global attention block ensures global information propagation. Although we introduce the windowed attention, it only affects the self-attention manner but doesn't change the linear projection for Q, K, and V. Therefore, we can directly apply the MAE pre-trained parameters from a vanilla ViT-B with all global attention to this windowed ViT-B without any extra process. Detailed configuration are shown in Table \ref{tab:vitb}.

\begin{table}[h]
   \centering
   \resizebox{0.6\columnwidth}{!}{
   \begin{tabular}{@{}l|l@{}}
   \toprule[2pt]
   Configs              & Value     \\ \midrule
   patch size           & 16        \\
   embedding dim        & 768       \\
   depth                & 12        \\
   number of heads      & 12        \\
   input size           & 3×1024×1024 \\
   window size          & 14        \\
   Norm layer           & LN        \\
   Global block indexes & 2,5,8,11  \\ 
   Output shape & 768×64×64         \\ \bottomrule[2pt]
   \end{tabular}
   }
   % \setlength{\abovecaptionskip}{50pt}
   % \setlength{\belowcaptionskip}{50pt}
   \vspace{2pt}
   \caption{\textbf{Detailed structure of windowed ViT-Base}}
   \label{tab:vitb}
\end{table}


\subsection{Simple Feature Pyramid} After obtaining the output from ViT-B, SFPN utilizes a sequence of convolutional, pooling, or deconvolutional (ConvTranspose2D) layers to downsample it into feature maps with 256 channels, scaling them to resolutions of $\{4.0, 2.0, 1.0, 0.5, 0.25\}$ relative to the resolution of the input feature maps (768×64×64). For example, the largest output feature map with a scale of 4.0 is shaped like 256×256×256, while the smallest one with a scale of 0.25 is shaped like 256×8×8. Each layer is followed by LayerNorm. Detailed structures of each scale can be seen in Table \ref{tab:sfpn}.

\begin{table}[h]
   \centering
   \resizebox{0.9\columnwidth}{!}{
   \begin{tabular}{l|l}
   \toprule[2pt]
   Scales & Layers \& channels of feature maps     \\ \midrule
   4.0  & 768 ConvT 384 ConvT 192 Conv(1,1) 256 Conv(3,3) 256 \\
   2.0  &  768 ConvT 384 Conv(1,1) 256 Conv(3,3) 256 \\
   1.0  & 768 Conv(1,1) 256 Conv(3,3) 256 \\
   0.5  & 768 maxpool2D 384 Conv(1,1) 256 Conv(3,3) 256 \\
   0.25  & 768 maxpool2D 384 Conv(1,1) 256 Conv(3,3) 256 maxpool2D 256 \\ \bottomrule[2pt]
   \end{tabular}
   }
   \vspace{2pt}
   \caption{\textbf{Detailed structure of Simple feature pyramid} \textit{ConvT} denotes for ConvTranspose2D with kernel size of 2 and stride of 2; Conv(x,x) indicate that a Conv2D layer with kernel size of x; and maxpool2D has also a kernel size of 2. The number shown between layers indicates the number of channels for its respective feature map between layers. }
   \label{tab:sfpn}
\end{table}

% \subsection{Lambda($\lambda$) Selection for Proposed Edge Loss}
% \label{app:lambda}
% We conducted a series of experiments trained on CASIAv2 and tested on CASIAv1 to find the optimal $\lambda$. Our experiment results are shown in Figure \ref{fig:lambda}. However, it's important to note that when applying the IML-ViT model to other datasets, the optimal value of $\lambda$ may vary slightly.

% % Figure environment removed

\subsection{Predict-head's norm \& training including authentic images}
\label{sec:predict_heads}

% % Figure environment removed


The exact structure we applied in the predict-head is shown in Figure \ref{fig:decoder}. There is a norm layer before the last 1 × 1 convolution layer in the predict-head. We observed that when changing this layer may influence the following aspects: 1) convergence speed, 2) performance, and 3) generalizability. 

In particular, Layer Norm can converge rapidly but is less efficient at generalization. Meanwhile, the Batch Norm can be generalized better on other datasets. However, when including authentic images during training, the Batch Norm may sometimes fail to converge. At present, a straightforward solution is to use Instance Normalization instead, which ensures certain convergence.  Our experimental results are shown in Table \ref{tab:norms}.

Delving into the reasons, MVSS-Net~\cite{MVSS_2021} is the pioneering paper proposing the incorporation of authentic images with fully black masks during training to reduce \textit{false positives}.  We highly endorse this conclusion as it aligns more closely with the practical scenario of filtering manipulated images from a massive dataset of real-world images. However, in terms of metrics in convention, because the F1 score is meaningful only for manipulated images (as there are no positive pixels for fully black authentic images,
$F1 = \frac{2 \cdot \text{TP}}{2 \cdot \text{TP} + \text{FP} + \text{FN}}$, yielding $F1=0$), we only computed data for manipulated images. This approach may result in an ``unwarranted" metric decrease when real images are included.

\begin{table}[h]
   \centering
   \resizebox*{1.0\columnwidth}{!}{
\begin{tabular}{@{}c|c|cc|cc|cc|cc|c@{}}
\toprule[2pt]
\multirow{2}{*}{\textbf{Norm}} & \multirow{2}{*}{\textbf{Dataset}} & \multicolumn{2}{c|}{\textbf{CASIAv1}} & \multicolumn{2}{c|}{\textbf{Coverage}} & \multicolumn{2}{c|}{\textbf{Columbia}} & \multicolumn{2}{c|}{\textbf{NIST16}} & \multirow{2}{*}{\textbf{MEAN}} \\ \cmidrule(lr){3-10}
                               &                                   & \textbf{F1}      & \textbf{Epoch}     & \textbf{F1}      & \textbf{Epoch}      & \textbf{F1}      & \textbf{Epoch}      & \textbf{F1}     & \textbf{Epoch}     &                                \\ \midrule
Layer                          & CASIAv2-5k                        & 0.686           & 168                & 0.347           & 168                 & 0.760           & 128                 & 0.231          & 104                & 0.506                        \\
Batch                          & CASIAv2-5k                        & 0.702           & 184                & 0.421           & 184                 & 0.730           & 184                 & 0.317          & 184                & 0.543                        \\
Instance                       & CASIAv2-5k                        & 0.719           & 184                & 0.419           & 176                 & 0.792           & 140                 & 0.263          & 150                & 0.547                       \\
Batch                          & CASIAv2-12k                       & 0.715           & 176                & 0.352           & 128                 & 0.767           & 150                 & 0.263          & 124                & 0.524                        \\
Instance                       & CASIAv2-12k                       & 0.721           & 140                & 0.362           & 100                 & 0.784           & 136                 & 0.258          & 68                 & 0.531                         \\ \bottomrule[2pt]
\end{tabular}
   }
   \vspace{2pt}
   \caption{\textbf{Testing for norm layer in predict-head} Implementation is followed to ablation study in the main paper. CASIAv2-5k refers to manipulated images only, while 12k includes authentic images as well.}
   \label{tab:norms}
\end{table}



\textbf{Training settings}
Since our model could only train with small batch size, we applied the \textit{gradient accumulate} method during training, i.e. updating the parameters every 8 images during training. We select this parameter by experiments, details see Table \ref{tab:accumulate_grad}.


\begin{table}[h]
\centering
\resizebox*{1.0\columnwidth}{!}{
\begin{tabular}{@{}c|c|c|cc|cc|cc|cc|c@{}}
\toprule[2pt]
\multirow{2}{*}{\textbf{Batchsize}} & \multirow{2}{*}{\textbf{GPUs}} & \multirow{2}{*}{\textbf{accum iter}} & \multicolumn{2}{c|}{\textbf{CASIAv1}} & \multicolumn{2}{c|}{\textbf{Coverage}} & \multicolumn{2}{c|}{\textbf{Columbia}} & \multicolumn{2}{c|}{\textbf{NIST16}} & \multirow{2}{*}{\textbf{MEAN}} \\ \cmidrule(lr){4-11}
                                    &                                &                                      & \textbf{F1}      & \textbf{Epoch}     & \textbf{F1}      & \textbf{Epoch}      & \textbf{F1}      & \textbf{Epoch}      & \textbf{F1}     & \textbf{Epoch}     &                                \\ \midrule
2                                   & 4                              & 2                                    & 0.686            & 184                & 0.302            & 144                 & 0.685            & 92                  & 0.304           & 144                & 0.494                          \\
2                                   & 4                              & 4                                    & 0.704            & 192                & 0.386            & 140                 & 0.772            & 60                  & 0.331           & 140                & 0.548                          \\
2                                   & 4                              & 8                                    & 0.722            & 152                & 0.410            & 140                 & 0.780            & 84                  & 0.332           & 140                & 0.561                          \\
2                                   & 4                              & 16                                   & 0.706            & 184                & 0.419            & 184                 & 0.782            & 92                  & 0.314           & 184                & 0.555                          \\
2                                   & 4                              & 32                                   & 0.602            & 184                & 0.249            & 184                 & 0.740            & 184                 & 0.254           & 184                & 0.461                          \\ \bottomrule[2pt]
\end{tabular}
}
\vspace{2pt}
\caption{\textbf{Test for best accumulate gradient parameter for IML-ViT.} Train/tested on CASIAv2/v1 with four NVIDIA 3090 GPUs.}
\label{tab:accumulate_grad}
\end{table}


Besides, we adopt the early stop method during training. Evaluate the performance on the F1 score for CASIAv1, and stop training when there is no improvement for 15 epochs. Other configs are described in Table \ref{tab:vitb_train}.

\begin{table}[h]
   \centering
  \caption{\textbf{Training settings for IML-ViT}}
   \resizebox{0.8\columnwidth}{!}{
   \begin{tabular}{@{}l|l@{}}
   \toprule[2pt]
   Configs              & Value     \\ \midrule
   batch size & 2 (RTX 3090) or 4 (A40) \\
   GPU numbers & 4 (RTX 3090) or 2 (A40) \\
   accumulate gradient batch size & 8 \\
   epochs        & 200      \\
   warm up epochs & 4        \\
   optimizer & AdamW \\
   optimizer momentum & $\beta_1,\beta_2=0.9,0.95$ \\
   base  learning rate & 1e-4 \\
   minimum laerning rate & 5e-7 \\
   learning rate schedule & cosine decay \\
   weight decay  & 0.05 \\ 
   \bottomrule[2pt]
   \end{tabular}
   }
   \label{tab:vitb_train}
\end{table}



\section{What artifacts does IML-ViT capture?}
\label{sec:gradcam}
To investigate whether IML-ViT focuses on subtle artifacts as expected, we employ GradCAM~\cite{grad_cam_2017} to visualize the regions and content the model focuses on, as shown in Figure \ref{fig:grad_cam}. Additional results are in the Appendix \ref{sec:extra_gradcam}. We can observe that IML-ViT captures the traces around the manipulated region with the help of edge loss. Further, we can observe some extra subtle attention out of the manipulated region in the fourth image, proving the global dependent ability of ViT can help the model trace the tampered region.

% Figure environment removed


\section{Additional Experiments Results}
\label{app:additional_results}
Since the space limit, we place a part of our results in \textbf{Protocol No.1} here.

% \textbf{Closed-source Comparison} Besides, for closed-source models, we consider a brief comparison on CASIA datasets and verify performance using the same training protocol. The specific numerical values are cited from their corresponding paper. Specifically, we compared performance with RGB-N,  SPAN and ObjectFormer. Details are shown in Table \ref{tab:close}.



% \begin{table}[h]
% \centering

% \resizebox*{0.9\columnwidth}{!}{ 
% \begin{tabular}{@{}lll@{}}
% \toprule[2pt]
% \textbf{Method}      & \textbf{Pre-train}           & \textbf{$F_1$(\%)} \\ \midrule
% RGB-N, CVPR18~\cite{RGBN_2018}        & ImageNet                             & 40.8                   \\
% SPAN, ECCV20~\cite{SPAN_2020}        & Private synthesized dataset          & 38.2                   \\
% Objectformer, CVPR22~\cite{objectformer_2022} & Private synthesized dataset          & 57.9                   \\
% \textit{IML-ViT(Ours)}        & MAE on ImageNet-1k          & \textbf{72.0}                   \\ \bottomrule[2pt]
% \end{tabular}
% }
% \caption{\textbf{Performance comparison with Closed-source methods} All the methods above are fine-tuning with CASIAv2 and are tested with the CASIAv1 dataset.  }
% \label{tab:close}
% \end{table}


ObjectFormer~\cite{objectformer_2022} and CFL-Net~\cite{CFL_Net_2023} evaluate their models fine-tuning with CASIAv2 on AUC. Although this metric may overestimate the models, IML-ViT has still surpassed them, as shown in Table \ref{tab:auc_casia}.


\begin{table}[h]
\centering
\resizebox{0.9\columnwidth}{!}{
\begin{tabular}{@{}llllll@{}}
\toprule[2pt]
\textbf{Method}                & \textbf{CASIAv1}  & \textbf{Coverage} & \textbf{Columbia} & \textbf{NIST16} & \textbf{MEAN}     \\ \midrule

ObjectFormer~\cite{objectformer_2022} & 0.882 &- &- &-&- \\
CFL-Net~\cite{CFL_Net_2023} & 0.863 & - & - & 0.799 & - \\
\textit{IML-ViT(Ours)} & \textbf{0.931} & \textbf{0.918} & \textbf{0.962} & \textbf{0.818} & \textbf{0.917} \\
\bottomrule[2pt]
\end{tabular}
}
\caption{\textbf{Comparison of AUC scores trained on CASIAv2.}}
\label{tab:auc_casia}
\end{table}



\section{Extra Visualization} 

\subsection{Visualization of Protocol No.1 on other datasets}
\label{app:visualization_protocol_1}
Here we also present some of the predict masks under Protocol No.1, which was from dataset with other preference on manipulation types. Extended from CASIAv1 and COVERAGE datasets in the main paper, we present results in NIST16 and Columbia datasets here in Figure \ref{fig:visualization_app}.


% Figure environment removed

\subsection{Qualitative results for ablation study}
\label{sec:Qualit_ablation}
The ablation study from Figure \ref{fig:ablation} evaluates the impact of various components on IML-ViT's performance: 1) w/o multi-scale: Significant degradation with poor feature detection and blurred outputs. 2) w/o MAE: Improved over the absence of multi-scale, but still blurry with weak edge definition. 3) w/o high-resolution: Noticeable drop in detail and precision, with coarse boundaries. 4) w/o Edge Loss: Less defined edges, preserving overall shape but losing structural details. 5) Full Setup: Produces the most accurate and detailed segmentation maps, capturing fine details and clear boundaries. In summary, the ablation study highlights the critical contributions of each component to the overall performance of IML-ViT. The multi-scale processing, MAE pre-training, high-resolution input, and edge loss each play a vital role in enhancing the model's ability to produce a high-quality segmentation map.


\subsection{Extra results for CASIA datasets.}
To provide a detailed showcase of IML-ViT's performance on image manipulation localization tasks, we present additional image results on the CASIA dataset in Figure \ref{app:casia}.

% Figure environment removed

\subsection{Extra GradCAM results}
\label{sec:extra_gradcam}
Here we provide extra GradCAM results to verify if IML-ViT focuses on the artifacts we want it to trace. Artifacts are mainly distributed around the manipulated region with rapid changes. Figure \ref{app:gradcam}
vividly shows that the IML-ViT can effectively discover the artifacts from the image and support its decision.

% Figure environment removed

\subsection{Feature maps between each module}
To gain a deeper understanding of IML-ViT, we present visualizations of feature maps between layers by calculating the average channel dimensions of the feature map. The outcomes are displayed in Figure \ref{fig:output}. This visualization process allows us to shed light on the model's functioning and provides valuable insights into its mechanisms.




% Figure environment removed


\section{Limitation}
We observe a rapid decline in IML-ViT's performance on the Gaussian blur attack
% TODO attack 需要更换
when the filter kernel size exceeded 11, We argue that this is mainly because our motivation is to make the model focus on detailed artifacts, but excessive Gaussian blurring can significantly remove these details, leading to a sudden drop in performance. However, from another perspective, this can actually prove that our model is able to effectively capture artifacts in tampering. Currently, the training does not specifically enhance blur, so we believe that adding enough blur data augmentation can compensate for this issue.
% % Figure environment removed
