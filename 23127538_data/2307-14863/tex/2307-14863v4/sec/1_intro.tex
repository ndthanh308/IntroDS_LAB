\section{Introduction}

With the advances in image editing technology like Photoshop, Image Manipulation Localization (IML) methods have become urgent countermeasures to cope with existing tampered images and avoid security threats~\cite{Verdoliva_2020}. Effective IML methods play a crucial role in discerning misinformation and have the potential to contribute to the safety of multimedia world. As shown in Figure \ref{fig:artifact}, the IML task aims to detect whether images have been modified and to localize the modified regions at the pixel level.
Image manipulation can be generally classified into three types~\cite{Mantra_2019, Verdoliva_2020}: (1) \textit{splicing}: copying a region from an image and pasting it to another image. (2) \textit{copy-move}: cloning a region within an image. (3) \textit{inpainting}: erasing regions from images and inpaint missing regions with visually plausible contents.

% As shown in Table \ref{tab:1}, to track manipulation, most existing methods greatly benefit from tracing \textit{artifacts} and low-level visual features to support the decision by utilizing various CNN-based feature extractors. Artifact refers to a distinctive, noticeable visual element in an image that resulted from manipulation. Since tampering aims to construct semantically meaningful and perceptually convincing images to cheat the audience, in most cases, artifact only distributes at the detailed texture level that is \textbf{non-semantic} and \textbf{non-objective}. Low-level features such as noise and high-frequency inconsistencies from different cameras can be crucial evidence to uncover manipulated regions within the authentic area. Thus, \textit{the key to IML lies in locating the artifacts and capturing the discrepancies between the manipulated and authentic regions.} 


% Figure environment removed


% % Figure environment removed


As shown in Table \ref{tab:1}, most existing methods for IML tasks greatly benefit from tracing artifacts with various CNN-based feature extractors. ``Artifacts" refer to unique visible traces (see Figure \ref{fig:artifact}) and invisible low-level feature inconsistencies (e.g., noise or high-frequency) resulting from manipulation.
% An artifact refers to distinctive, noticeable visual (e.g. Figure \ref{fig:artifact}) and invisible low-level (e.g. noise or high-frequency inconsistent) features in an image that results from manipulation. 
As tampering aims to deceive the audience by creating semantically meaningful and perceptually convincing images, visual traces typically manifest at a non-semantic level, distributed in textures around the manipulated area.
Additionally, low-level features, like noise inconsistencies introduced by different cameras, can also serve as crucial evidence to reveal manipulated regions within the authentic area.
Thus, based on previous experiences, \textit{the key to IML lies in capturing the  artifacts by identifying non-semantic visible traces and low-level inconsistencies.}



% In addition, low-level features such as noise and high-frequency inconsistencies from different cameras can also be crucial evidence to uncover manipulated regions within the authentic area. 

% and capturing the discrepancies between the manipulated and authentic regions.
% } 



\begin{table}[t]
\centering

\resizebox{\columnwidth}{!}{
\begin{tabular}{|c|cc|c|c|cc|}
\hline
\multirow{2}{*}{\textbf{Method}} &
  \multicolumn{2}{c|}{\textbf{Backbone }} &
  \multirow{2}{*}{\textbf{Resolution}} &
  
  \multirow{2}{*}{\textbf{Manipulation   supervision}} &
  \multicolumn{2}{c|}{\textbf{IML dataset thirsty}} \\ \cline{2-3} \cline{6-7} 
 &
  \multicolumn{1}{c|}{\textbf{CNN}} &
  \textbf{Tran.} &
   &
   &
  \multicolumn{1}{c|}{\textbf{Type}} &
  \textbf{Amount} \\ \hline
\begin{tabular}[c]{@{}c@{}}ManTra-Net\\      ~\cite{Mantra_2019}\end{tabular} &    

  \multicolumn{1}{c|}{\checkmark} &
  - &
  \begin{tabular}[c]{@{}c@{}}Resize\\  512×512\end{tabular}  &
  \begin{tabular}[c]{@{}c@{}}Noise\\      (BayarConv+ SRM filter)\end{tabular} &
  \multicolumn{1}{c|}{Private} &
  102k \\ \hline
\begin{tabular}[c]{@{}c@{}}SPAN\\      ~\cite{SPAN_2020}\end{tabular} &  
  \multicolumn{1}{c|}{\checkmark} &
  - &
   \begin{tabular}[c]{@{}c@{}}Resize\\  224×224\end{tabular}
 &
  \begin{tabular}[c]{@{}c@{}}Noise\\      (BayarConv+ SRM filter)\end{tabular} &
  \multicolumn{1}{c|}{Private} &
  102k \\ \hline
\begin{tabular}[c]{@{}c@{}}CR-CNN\\      ~\cite{CR_CNN_2020}\end{tabular} &  
  \multicolumn{1}{c|}{\checkmark} &
  - &
 \begin{tabular}[c]{@{}c@{}}Resize\\   short side to 600 \end{tabular}
  % resize,&
  &
  \begin{tabular}[c]{@{}c@{}}Noise\\      (BayarConv+ SRM filter)\end{tabular} &
  \multicolumn{1}{c|}{Public} &
  5k \\ \hline
  \begin{tabular}[c]{@{}c@{}}GSR-Net\\      ~\cite{GSR_Net_2020}\end{tabular} &  
  \multicolumn{1}{c|}{\checkmark} &
  - &
   \begin{tabular}[c]{@{}c@{}}Resize\\   300×300 \end{tabular}
  % resize, 
  &
  Edge Prediction &
  \multicolumn{1}{c|}{Public} &
  5k \\ \hline
   \begin{tabular}[c]{@{}c@{}}MVSS-Net\\      ~\cite{MVSS_2021}\end{tabular} & 
  \multicolumn{1}{c|}{\checkmark} &
  - &
     \begin{tabular}[c]{@{}c@{}}Resize\\   512×512 \end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Noise (BayarConv)\\      Edge (sobel)\end{tabular} &
  \multicolumn{1}{c|}{Public} &
  5k \\ \hline
   \begin{tabular}[c]{@{}c@{}}MM-Net\\      ~\cite{mmnet_2021}\end{tabular} & 
  \multicolumn{1}{c|}{\checkmark} &
  - &
 \begin{tabular}[c]{@{}c@{}}Resize\\  short side to 800  \end{tabular}
  &
  Noise (BayarConv) &
  \multicolumn{1}{c|}{Private} &
  50k \\ \hline
    \begin{tabular}[c]{@{}c@{}}TransForensic\\    ~\cite{transforensic_2021}\end{tabular} & 

  \multicolumn{1}{c|}{\checkmark} &
  \checkmark &
\begin{tabular}[c]{@{}c@{}}Resize\\  512×512  \end{tabular}
  % resize, 
  &
  - &
  \multicolumn{1}{c|}{Public} &
  10k \\ \hline
    \begin{tabular}[c]{@{}c@{}} ObjectFormer\\    ~\cite{objectformer_2022}\end{tabular} & 
  \multicolumn{1}{c|}{\checkmark} &
  \checkmark &
  \begin{tabular}[c]{@{}c@{}}Resize\\  256×256  \end{tabular}
  &
  High-frequency  &
  \multicolumn{1}{c|}{Private} &
  62K \\ \hline

    \begin{tabular}[c]{@{}c@{}}HiFi-Net \\    ~\cite{HiFi-Net2023}\end{tabular} & 
  \multicolumn{1}{c|}{\checkmark} &
  - &
\begin{tabular}[c]{@{}c@{}}Resize\\  256×256  \end{tabular}
&
  Frequency  &
  \multicolumn{1}{c|}{Public} &
  1,710K \\ \hline

    \begin{tabular}[c]{@{}c@{}}TruFor \\    ~\cite{trufor2023}\end{tabular} & 
  \multicolumn{1}{c|}{\checkmark} &
  \checkmark &
\begin{tabular}[c]{@{}c@{}}Crop\\ 512×512 \end{tabular}
  % crop, 512×512
  &
  \begin{tabular}[c]{@{}c@{}}Noise\\  (Contrastive learning)\end{tabular} &
  \multicolumn{1}{c|}{Public} &
  35K \\ \hline
  
\textit{
\begin{tabular}[c]{@{}c@{}}IML-ViT\\ (Ours) \end{tabular}
} &
  \multicolumn{1}{c|}{-} &
  \checkmark &
  \begin{tabular}[c]{@{}c@{}}  Zero-pad\\ 1024×1024 \end{tabular}
&
  Edge loss &
  \multicolumn{1}{c|}{Public} &
  5k \\ \hline
\end{tabular}
}
\vspace{2pt}
\caption{\textbf{Overview of State-of-the-Art End-to-End Models for Image Manipulation Localization.} \textit{Tran.} stands for \textit{Transformer}. \textit{Manipulation supervision} serves as prior knowledge broadly acknowledged in the image manipulation detection field. Edge information effectively traces visible artifacts, while noise and high-frequency features primarily highlight low-level differences between tampered and authentic regions.
% \xiaodong{More previous works also utilize edge as supervision, the font of this table is too small}
}
\label{tab:1}
\end{table}


However, convolution propagates information in a \textit{collective} manner, making CNNs more suitable for semantic-related tasks, such as object detection, rather than tracing non-semantic artifacts that often surround an object. Further, to identify low-level inconsistencies, we need to explicitly compare the relationships between different regions. But in deeper networks, CNNs may overlook global dependencies~\cite{ERF_2016}, rendering them less effective in capturing differences between regions.
Given the weaknesses of CNN in non-semantic and long-distance modeling, we ask: \textit{Is there any other optimal backbone for solving IML tasks?} 
% \xiaodong{backbone difference is not new in the age of 2024}

Considering the goal of capturing the feature discrepancies between the manipulated and authentic regions, we argue that self-attention should be a better solution regarding IML. \textit{As self-attention can explicitly model relationships between any areas regardless of their visual semantic relevance, especially for non-adjacent regions.} The performance boost achieved by SPAN~\cite{SPAN_2020} highlights the effectiveness of integrating self-attention structures into convolutional layers.
Furthermore, as artifacts are often distributed at the patch level rather than at the pixel or image level, Vision Transformer (ViT)~\cite{ViT_2021} naturally becomes the ideal choice to trace artifacts and make comparisons. 

While ViT may be suitable for IML tasks, directly applying the original ViT architecture is insufficient. We suggest that IML involves three key discrepancies from traditional segmentation tasks, which also have not yet received sufficient attention in previous IML methods, as supported by Table 1. These discrepancies are:

  % 接下来一大段都是上面的替代品或者展开
  % The significance of artifacts in IML has led us to question whether the CNN architecture is optimal for tracking artifacts. Given that tampered content often comes from a real image, it can be challenging to detect suspicious traces if we only focus on the tampered or authentic region. Therefore, \textit{the key to locating artifacts lies in comparing the differences between the manipulated and authentic regions.}

  % In most cases, artifacts are more likely to be discovered at the junction between the authentic region and the tampered region. To address this challenge, we propose using self-attention as a better choice for artifact tracking. The self-attention mechanism can explicitly exploit relationships and interactions between different regions, making it more suitable for comparison. In contrast, CNNs may ignore some global dependencies between different regions~\cite{ERF_2016}, thereby making comparison less effective.

  % This approach has been shown to be effective in recent research. For example, the SPAN model~\cite{SPAN_2020} has made progress in artifact tracking by adding self-attention structures to some of the convolutional layers in the ManTra-Net model~\cite{Mantra_2019}. This highlights the potential benefits of using self-attention for artifact tracking and supports our hypothesis that the CNN architecture may not be optimal for this task.

  % Driven by the significance of artifacts, we raise a question: \textit{Is the CNN architecture optimal for tracking artifacts?} As tampered content often comes from a real image, it is difficult to capture suspicious traces if we only focus on this region, same with the authentic area. Besides, the key to locating artifacts lies in \textit{comparing} the differences between the manipulated and authentic regions.

  % In most cases, artifacts are more likely to be discovered at the junction between the authentic region and the tampered region.
  %  With this goal in mind, self-attention may be a better choice. Since the self-attention mechanism can explicitly exploit relationships and interactions between different regions, and is therefore more suitable for comparison. In contrast, CNNs typically use convolutional kernels to capture information from different regions through local receptive fields, this method may ignore global dependencies between different regions, making comparison less effective. Recnet example where SPAN~\cite{SPAN_2020} made progress by adding self-attention structure to some of convolution layers in ManTra-Net\cite{Mantra_2019} proved this suppose.

  % Although CNNs can also capture information from different regions through local receptive fields, Therefore, we believe that when performing comparison tasks, explicitly using the self-attention mechanism is superior to using CNNs.”

% 从尺度说明为什么是patch level 的ViT
% To further discuss the scale of comparison, it's worth noting that pixel-to-pixel comparison cannot capture artifacts appearing in a certain area, while comparing the whole image with every other image is too computationally expensive. At this point, patch-level comparison becomes the optimal choice to balance the two. Therefore, we believe that ViT is a natural choice for solving this problem, as it enables patch-level comparison using self-attention.

% Overall, our approach aims to improve the accuracy of artifact tracking by explicitly comparing different regions of the image, using self-attention to highlight the key differences. We believe that this approach has the potential to outperform existing methods, and we look forward to testing it in future research.

% 下面一整段都是从 benchmark的角度切入讨论，但是现在看有点多余和不必要。
  % As shown in table \ref{tab:1}, many state-of-the-arts were proposed to solve this problem. However, we observed that although these models have proven their performance by \textit{testing} on some publicly available datasets, it is still difficult to compare them fairly. Some works \textit{trained} their model with a huge, private, synthesized dataset, which brings difficulties for replication and comparison. 
  %  下文这个example可以删
  % For example, Mantra-Net~\cite{Mantra_2019} uses a private dataset of 102,208 images with 385 kinds of manipulation type, while RGB-N~\cite{RGBN_2018} is trained using a randomly generated dataset with more than 42k images, and ObjectFormer~\cite{objectformer_2022} utilizes a synthetic dataset based on MSCOCO~\cite{mscoco_2014}. In contrast, a commonly used IML public dataset, CASIAv2, has only a few thousand images.
  % In addition, we noticed that some "test-set oriented" data augmentation approaches may be key to raising points, but some authors obscured this information.
  % Such gaps make us wonder whether the excellence of the performance comes from the model itself or from the other factors. It is understandable that there is some chaos in the field when exploratory stage, however, IML is in need of a common benchmark to fairly evaluate the performance of each Image Manipulation Localization methods.

  % These various inconsistencies make it difficult to measure the performance of different models.
  % The major reason is the discrepancy of training datasets, 
  %  in other words, there is an absence of a common benchmark. 
  % Unsurprisingly, the state-of-the-arts are widely sharing backbone networks with semantic segmentation, especially CNNs~\cite{CNN_1989} like VGG~\cite{VGG_2015} or Resnet~\cite{Resnet_2016}. A slight exception~\cite{objectformer_2022} has the structure of convolution at first to create tokens and then feed them to the Transformer~\cite{transformer} for final prediction. 
  % Although these models have proven their performance on some publicly available datasets, it is still difficult to compare them fairly, in other words, there is an absence for a common benchmark. The major reason is the discrepancy of training datasets, for example, Mantra-Net~\cite{Mantra_2019} uses a private dataset of 102,208 images with 385 kinds of manipulation type, while RGB-N~\cite{RGBN_2018} is trained using a randomly generated dataset with more than 42k images, and ObjectFormer~\cite{objectformer_2022} utilizes a synthetic dataset based on MSCOCO~\cite{mscoco_2014}. Further, there are similar inconsistencies in data augumentation and metrics. Such a gap makes us wonder whether the excellence of the performance comes from the model itself or from the other factors. It is understandable that there is some chaos in the field when exploratory stage, however, IML is in need of a common benchmark to fairly evaluate the performance of each Image Manipulation Localization methods.

% 不能直接说出用了iml vit 要先说明我们决定用Vit，但是是不是直接把ViT套上来就完了呢？引出三点对于IML任务的理解，这三个点是IML与其他视觉任务完全不同的点，再说我们基于此建立了IML vit

\textbf{High Resolution}
While semantic segmentation and IML share similar inputs and outputs, IML tasks are more information-intensive, focusing on detailed artifacts rather than macro-semantics at the object level. Existing methods use various extractors to trace artifacts, but their resizing methods already harm these first-hand artifacts.
Therefore, preserving the \textit{original resolution} of the images is crucial to retain essential artifacts for the model to learn.


% While it is true that semantic segmentation and IML have similar inputs and outputs, IML tasks are more information-intensive and need to take care of detailed artifacts rather than macro-semantics at the object level. 
% In contrast, segmentation or object detection often only acquires macro-semantics at the object level and has a higher redundancy for detailed differences, making it more information-sparse.
% Therefore, retaining the \textit{original resolution} of the images is crucial to preserve details and avoid harming the artifacts and low-level visual features. By doing so, the model can better discern the features of tampered regions.

% 这段有点啰嗦了，该删就删减
% Artifacts can be interpreted as a combination of in-camera clues(like different noise distributions coming from different cameras) and out-camera clues(various traces on the edge of the tampered region).  Only by interpreting clues from these small regions, especially the dissimilarity between the tampered and authentic regions, can effective recognition be made; in other words, IML tasks are information-dense tasks. In contrast, segmentation or object detection often only acquires macro-semantics at the object level and has a higher redundancy for detailed differences, making it more information-sparse.

\textbf{Edge Supervision} 
As mentioned earlier, IML's primary focus lies in detecting the distinction between the tampered and authentic regions. This distinction is most pronounced at the boundary of the tampered region, whereas typical semantic segmentation tasks only require identifying information within the target region. From another perspective, it becomes evident that visible artifacts are more concentrated along the periphery of the tampered region rather than within it (as shown in Figure \ref{fig:artifact}). Consequently, the IML task must guide the model to concentrate on the manipulated region's edges and learn its distribution for better performance.

\textbf{Multi-scale Supervision} The percentage of tampered area to the total area varies significantly across different IML datasets. CASIAv2~\cite{CASIA_2013} contains a considerable amount of sky replacement tampering, whereas Defacto~\cite{defacto_2019} mostly consists of small object manipulations. On average, CASIAv2 has 7.6\% of pixels as tampered areas, while Defacto has only 1.7\%. Additionally, IML datasets are labor-intensive and often limited in size, which poses challenges in bridging the gap between datasets.
Therefore, incorporating multi-scale supervision from the pre-processing and model design stages is essential to enhance generalization across different datasets.

%(ii) Multi-scale must take into consideration for IML.  
%This mainly comes from two aspects, one is the specificity of the way of tampering, for example, changing the sky will tamper with nearly half of an image, while a common face replacement may only affect a very small region of an image. The second is that the distribution of tampered areas in different datasets is extremely dissimilar; in CASIAv2~\cite{CASIA_2013}, for example, 7.65\% of the region is tampered with on average, while Defacto~\cite{defacto_2019} has almost only 1.7\% of the area tampered with(calculated from the pixel level). In order to improve the generalization performance, the necessary pre-processing and FPN~\cite{FPN_2017} structures need to be applied.

% When considering the specific structure suitable as a benchmark, we find that CNN-based methods dominates the IML problem. Most of the work has focused on constructing various nifty feature extractors to mine the priori knowledge such as noise, high frequency information, etc. Meanwhile, the role of backbone is not so obvious here. There is experimental evidence that vanilla FCN~\cite{FCN_2015} and DeepLabv2~\cite{Deeplab_2018} can perform well in the training set, but are difficult to generalize on non-homologous datasets~\cite{GSR_Net_2020}.  We question this: \textit{are CNNs really the optimal choice for parsing such non-semantic information comes from artifacts? } Because for a convolution layer, feature vectors (vectors at the channel direction in the feature map) at different points have only linear operations on each other. Information is passing through layers \textit{collectively} but less consideration about the similarity between feature vectors at the same layer. This bottom-up manner can be effective for missions like object detection because it cares more about the macroscopic semantic, but there is less capture and fewer secondary understanding of detailed textures when the layers are deepened~\cite{ERF_2016}, especially the relationship between pixels and patches.

% Instead, SPAN~\cite{SPAN_2020} introduces the self-attention block based on ManTra-Net~\cite{Mantra_2019} architecture, and ObjectFormer~\cite{objectformer_2022} adopts the convolution-first-then-Transformer approach.
% These successes led us to consider that self-attention may have some advantages for solving IML problems, inspiring the attempt to tackle the problem using Vision Transformer (ViT)~\cite{ViT_2021} as the backbone. As feature vectors in self-attention have inner product operations on each other, the similarity between vectors is taken into account, i.e., the relevance between different pixels is compared one by one in each block. Further, Transformer was born from NLP, which is more suitable for dense semantics, and since ViT has been successful, IML tasks with more dense information than traditional vision tasks should be more likely to succeed.

In this paper, we present IML-ViT, an end-to-end ViT-based model that solves IML tasks. Regarding the proposed three key discrepancies, we devise IML-ViT with the following components:
1) a windowed ViT which accepts \textbf{high-resolution} input. Most of the global attention block is replaced with windowed attention as the trade-off for time complexity. We initialize it with Masked Autoencoder (MAE)~\cite{MAE_2022} pre-trained parameters on ImageNet-1k~\cite{imagenet_2009}; 2) a simple feature pyramid networt (SFPN) ~\cite{benchmark_ViT_2021} to introduce \textbf{multi-scale supervision}; 3) a morphology-based edge loss strategy is proposed to ensure \textbf{edge supervision}. The overview of IML-ViT is shown in Figure \ref{fig:overall}.

% Aiming at propose a simple but superior performance benchmark. In this paper, guided by the above points, we propose IML-ViT, a effective end-to-end training paradigm for Image manipulation localization based on ViT~\cite{ViT_2021}. Specifically, IML-ViT contains: 1) a ViT which accepts high-resolution input, as the backbone.  Part of the global attention block is replaced with windowed attention as the trade-off for time complexity. We initialized its parameters with ImageNet-1k MAE pre-training~\cite{MAE_2022}; 2) a \textit{simple feature pyramid}~\cite{benchmark_ViT_2021} architecture to introduce multi-scale supervision;3) a morphology-based edge loss strategy is applied during training to ensure edge supervision. Overviews are shown in Figure \ref{fig:overall}. 
In this manner, without any specialized modules, IML-ViT offers a general ViT structure for IML tasks. In other words, \textit{IML-ViT proves that IML tasks can be solved without hand-crafted features or deliberate feature fusion process}, promoting future IML methods into a more generalizable design paradigm. 

% Figure environment removed

To the best of our knowledge, ObjectFormer~\cite{objectformer_2022}, TransForensics~\cite{transforensic_2021}, and TruFor~\cite{trufor2023} are the only Transformer-related models solving the IML tasks. However, their backbone distinguishes significantly from vanilla ViT, as will be explained in Section \ref{sec:related_works}. Thus, IML-ViT can be regarded as the pioneering model utilizing a vanilla ViT as the backbone for IML tasks.

% With no specialized modules,  IML-ViT has the potential to serve as a simple yet superior performance benchmark for IML tasks and demonstrate that IML tasks can be solved without manually designed feature extractors or complicated feature fusion, promoting existing IML methods into new research paradigms. 

Currently, the evaluation protocol for IML tasks is rather chaotic. To bring faithful evaluations and establish IML-ViT as the benchmark model, we demarcate existing messy evaluation settings into three mainstream protocols and conduct comprehensive experiments across these protocols. The extensive experiment results demonstrate that IML-ViT has surpassed all SoTA (state-of-the-art) models, thereby validating the reliability of the three proposed key essences of IML. Thus, we believe that IML-ViT is a powerful candidate to become a new SoTA model for IML.

% The experiments are conducted on different scales to measure the performance and generalizability of our IML-ViT. 1) 5k, We first follow a common protocol~\cite{objectformer_2022, RGBN_2018, MVSS_2021, Mantra_2019, GSR_Net_2020}that trained on CASIAv2~\cite{CASIA_2013} dataset and then tested on other 5 public datasets. 



% -The experiments are mainly conducted with a common protocol~\cite{objectformer_2022, RGBN_2018, MVSS_2021, Mantra_2019, GSR_Net_2020} to measure the performance and generalizability of our IML-ViT.  
% Model is trained on CASIAv2~\cite{CASIA_2013} dataset first and then test it on other public datasets including NIST16~\cite{NIST16_2019}, Columbia~\cite{Columbia_2006}, COVERAGE~\cite{Coverage_2016}, and CASIAv1\cite{CASIA_2013}.
% Each model is trained on CASIAv2~\cite{CASIA_2013} dataset and then tested on other 5 public datasets. Additionally, to provide more comprehensive results as benchmarks, we compared IML-ViT with several latest models trained on mixed datasets in their unique protocols.


In summary, our contributions are as follows:

\begin{itemize}
  \item We reveal the significant discrepancies between IML and traditional segmentation tasks by raising the three essences, which were overlooked by previous studies: high resolution, multi-scale, and edge supervision.
  
  \item  Aiming at three essences, we modify the components of ViT and establish the IML-ViT, the first ViT-based model for image manipulation localization.
  
  \item Extensive experiments show that IML-ViT outperforms state-of-the-art models in both $F_1$ and AUC scores on various protocols. This verifies the solidity of the three essences we proposed.
  
  \item We vanish the evaluation barrier for future studies by demarcating existing evaluation settings into three mainstream protocols and implementing cross-protocols-comparisons.
\end{itemize}

