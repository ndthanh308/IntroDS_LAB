\section{Related Works}
\label{sec:related_works}
\par
\textbf{Paradigm of IML} 
Research in the early years focused on a single kind of manipulation detection, with studies on copy-move~\cite{Cozzolino_Poggi_Verdoliva_2015b, Rao_Ni_2016}, splicing~\cite{Cozzolino_Poggi_Verdoliva_2015a, Huh_Liu_Owens_Efros_2018, Kniaz_Knyaz_Remondino}, and removal (Inpainting)~\cite{Zhu_Qian_Zhao_Sun_Sun_2018}, respectively. However, since the specific type of tampering is unknown in practice, after 2018, general manipulation detection has become the focus. 
Many existing works follow the paradigm of ``feature extraction + backbone inference", especially extractors to exploit tamper-related information from artifacts. CR-CNN~\cite{CR_CNN_2020} has a noise-sensitive BayarConv~\cite{Bayar_2018} as the first convolution layer.  RGB-N networks~\cite{SRM_2018} develop an SRM filter to mine the difference in noise distribution to support decision-making.  ManTra-Net~\cite{Mantra_2019} and SPAN~\cite{SPAN_2020} combined SRM, BayarConv, and as the first layer of their model. Besides noise-related extractors, ObjectFormer \cite{objectformer_2022} employs a DCT module to extract high-frequency features, which are then combined with RGB features and fed into a transformer decoder. And MVSS-Net~\cite{MVSS_2021} combines a Sobel-supervised edge branch and a BayarConv noise branch with dual attention to fuse them. Nevertheless, a feature may only be effective for a single type of tampering, e.g., noise is more sensitive to splicing from different images but less effective for copy-move from the same image.  Recently, TruFor~\cite{trufor2023} and NCL~\cite{NCL_IML_2023} are the first to explore utilizing contrastive learning to extract features instead of manually designed filters. Proposed IML-ViT also aims to step out of the paradigm of ``extraction + fusion" and let the model itself learn as much knowledge as possible from the datasets rather than rely on \textit{priori knowledge}. 


\par
\textbf{Transformer-based IML method}
At present, there are three Transformer-based models in the field of IML, namely ObjectFormer~\cite{objectformer_2022} TransForensics~\cite{transforensic_2021}, and TruFor~\cite{trufor2023}. 
Though named ``Trans" or ``Former", these models are hardly in line with vanilla ViT in overall structures and design philosophies. 
In particular, different from ViT directly embedding the patched images before encoding, the first two methods utilize several CNN layers to extract feature maps initially and subsequently employ Transformers for further encoding, leading to neglecting crucial first-hand low-level information. On the other hand, TruFor follows SegFormer~\cite{SegFormer_2021}'s encoder, using convolution layers instead of position embedding to integrate the position information for Transformer blocks, which overlooked key global dependencies to capture differences between regions. 

Moreover, in ObjectFormer's encoder, the ``query" inputs are learnable vectors representing object prototypes $o_i$, not image embeddings. As a result, it focuses on capturing dependencies between object prototypes and image tokens, whereas a standard ViT encoder solely models the relationship between image embeddings. Besides, ObjectFormer is pre-trained with a large tampering-oriented synthesized private dataset, while IML-ViT achieves better performance with pre-training on the more accessible ImageNet-1k dataset.

Further, TransForensics has a different way to apply Transformer blocks. While ViT uses these blocks sequentially, TransForensics employs them in parallel, wherein each feature map of an FCN output is decoded with a Transformer block, and then fused for the final output.

In short, IML-ViT can be considered the first IML method with a vanilla ViT as its backbone and could easily benefit from recently advanced algorithms related to ViT, proving that IML tasks do not require complex designs.




% Our goal is to construct a Vision Transformer-based end-to-end image manipulation localization model, aiming at being a new benchmark for this task. Based on this premise, rather than building \textit{overly complex} models, even though this could further improve preformance, we chose to adopt as general a structure as possible. However, even with such concessions, IML-ViT succeeds in outperforming the current state of the art.

% we chose not to design an \textit{overly complex} structure, although it is likely to further improve performance. Even so, the current \textit{simple} structure can already achieve state-of-the-art results. We devote more effort to analyzing what components are compulsory when solving IML tasks using ViT.

