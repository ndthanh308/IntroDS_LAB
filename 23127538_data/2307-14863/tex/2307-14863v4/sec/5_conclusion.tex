\section{Conclusions}
This paper introduces IML-ViT, the first image manipulation localization model based on ViT. Extensive experiments on three mainstream protocols demonstrate that IML-ViT achieves SoTA performance and generalization ability, validating the reliability of the three core elements of the IML task proposed in this study: high resolution, multi-scale, and edge supervision. Further, IML-ViT proves the effectiveness of self-attention in capturing non-semantic artifacts. Its simple structure makes it a promising benchmark for IML.

% -------------------后续素材------------------
% \section{paragraph wait for use}

% Unlike the ViTDet paper applys a mask-RCNN network after the Simple Feature pyramid, we only transform all the feature maps back to the same resolution with the linear layers separately. Then use a 1×1 convolution layer to fuse the features to a fixed scale of $1/4$. Finally do a final$\frac{H}{4} \times \frac{W}{4} \times 1$ prediction with another 1x1 Conv2D layer.

% we then feed the tokens come from ViT to. This method generate 
% Followed ViTDet~\cite{ViT_2021}, we then apply a simple feature pyramid after the ViT.
% Image manipulation datasets are 


% However, dataset insufficiency is the most crucial issue in building ViT-based IML methods. As dense annotation and expert tamper generation are very labor-intensive, the public datasets~\cite{CASIA_2013, Columbia_2006, NIST16_2019, defacto_2019, Coverage_2016} for image manipulation localization are small in size (several hundred to several thousand images). This cannot in any way satisfy vanilla ViT's appetite, as it often requires training on ImageNet~\cite{imagenet_2009} or COCO~\cite{mscoco_2014} for optimal performance~\cite{ViT_2021,Carion_Massa_Synnaeve_Usunier_Kirillov_Zagoruyko_2020, Heo_Yun_Han_Chun_Choe_Oh, Meng_Li_Chen_Lan_Wu_Jiang_Lim_2022}. Yet, the rapid development of self-supervised pre-training for ViT has provided a solution to the problem, especially MAE~\cite{MAE_2022}, which we further experimentally demonstrate can greatly alleviate the overfitting problem and allow the model to converge quickly on small datasets. In short, MAE pre-training is a prerequisite for us to carry out further model design for IML problem.


% %基于MAE解决datainsufficiency的问题，这个打算放到method部分了
% \par
% Dataset insufficiency is the most crucial issue in building ViT-based IML methods. As dense annotation and expert tamper generation are very labor-intensive, the public datasets~\cite{CASIA_2013, Columbia_2006, NIST16_2019, defacto_2019, Coverage_2016} for image manipulation localization are small in size (several hundred to several thousand images). This cannot in any way satisfy vanilla ViT's appetite, as it often requires training on ImageNet~\cite{imagenet_2009} or COCO~\cite{mscoco_2014} for optimal performance~\cite{ViT_2021,Carion_Massa_Synnaeve_Usunier_Kirillov_Zagoruyko_2020, Heo_Yun_Han_Chun_Choe_Oh, Meng_Li_Chen_Lan_Wu_Jiang_Lim_2022}. Yet, the rapid development of self-supervised pre-training for ViT has provided a solution to the problem, especially MAE~\cite{MAE_2022}, which we further experimentally demonstrate can greatly alleviate the overfitting problem and allow the model to converge quickly on small datasets. In short, MAE pre-training is a prerequisite for us to carry out further model design for IML problem.

% % 手工特征简介 这段或许也可以放到3.method里面
% Specifically for IML tasks, to capture the dissimilarity, many hand-crafted feature extractors such as BayarConv~\cite{Bayar_2018} and High-frequency extractor~\cite{objectformer_2022}, etc. are proposed and widely used to exploit additional information from artifacts. We use AdamW as optimization with $lr=1e-4$ 
% % 手工特征的问题
% %设计的特征对于某个任务很好，但是另一个任务不通用（noise-> splicing or copy-move）
% %目前的所有模型会对图像进行大幅度的压缩，改变长宽比和分辨率，这会对artifact产生影响
% %精心设计的不一定是最实用的。
% However, there are two problems with this generic way of operation:(1)A feature may only be effective for one type of tampering, e.g., noise is more sensitive to splicing from different images but less effective for copy-move from the same image. (2)Most implementations scale the image to 512x512 resolution or smaller, yet in many datasets, images are not square and have a larger resolution. Such re-scale compressing has been proved that will harm the latent artifacts~\cite{Mantra_2019}. (3) These artificial extractors may work but are not optimal. This can be analogized to the replacement of symbolism by connectionism in machine learning practice. In contrast, instead of complex feature extraction, we want to find generic solutions to automatically mine as much information from artifacts as possible.

% % 没啥用的两个问句
% However, when we revisit past research, the question arises: We attempt to discuss this issue from the following perspectives:

% we have a question over this CNN prevalence: 

% % 简介图像篡改类型
% \par
% Image manipulation can be generally classified into three types:(1)splicing, copying a region from an image, and pasting it to another image. (2)copy-move, cloning a region within an image. (3)inpainting, erasing regions from images and inpaint missing regions with visually plausible contents. To track manipulation, recent detection methods major focus on in-camera clues(like different noise distributions coming from different cameras) and out-camera clues(various traces on the edge of the tampered region), clues above can be collectively termed as \textit{artifacts}. Since tampering aims to construct semantically meaningful and perceptually convincing images, in most cases, artifacts can only be found at the texture level that is non-semantic. This differs the IML tasks from traditional semantic segmentation tasks, and simply taking the segmentation model into use can have shortcomings in generalization performance~\cite{GSR_Net_2020}.
% Both manipulated and authentic regions can be semantically meaningful, and it is the generalized dissimilarity between them can support the decision. The regions with the most dissimilarities are the artifacts that need the most attention.

% % FCN在同源数据集CASIAv2训练的好可以在CASIA1上运行，但是在其他的数据集表现差
% % Previous research~\cite{GSR_Net_2020} reports that DeepLabv2~\cite{Deeplab_2018} trained on CASIAv2 datasets~\cite{CASIA_2013} performs well on homologous CASIAv1 datasets~\cite{CASIA_2013}, but has poor generalization performance on non-homologous COVERAGE~\cite{Coverage_2016} dataset. 



% % 手工特征简介
% To capture the dissimilarity, many hand-crafted feature extractors such as BayarConv~\cite{Bayar_2018} and High-frequency extractor~\cite{objectformer_2022}, etc. are proposed and widely used to exploit additional information from artifacts.
% % 手工特征的问题
% However, there are two problems with this generic way of operation:(1)A feature may only be effective for one type of tampering, e.g., noise is more sensitive to splicing from different images but less effective for copy-move from the same image. (2)Most implementations scale the image to 512x512 resolution or smaller, yet in many datasets, images are not square and have a larger resolution. Such re-scale compressing has been proved that will harm the latent artifacts~\cite{Mantra_2019}. (3) These artificial extractors may work but are not optimal. This can be analogized to the replacement of symbolism by connectionism in machine learning practice. In contrast, instead of complex feature extraction, we want to find generic solutions to automatically mine as much information from artifacts as possible.

% % 重点，辨析ViT和CNN谁更适合
% Keeping this in mind, we revisited the state-of-the-arts~\cite{Mantra_2019,GSR_Net_2020, SPAN_2020, MVSS_2021} and find that most of them adopted  CNNs~\cite{CNN_1989} like VGG~\cite{VGG_2015} or Resnet~\cite{Resnet_2016} as backbone.  A slight exception~\cite{objectformer_2022} has the structure of convolution at first to create tokens and then feed them to the Transformer~\cite{transformer} for final prediction. We question this: \textit{are CNNs really the optimal choice for parsing such non-semantic information comes from artifacts? }

% Because for a convolution layer, feature vectors (vectors at the channel direction in the feature map) at different points have only linear operations on each other. Information is passing through layers \textit{collectively} but less consideration about the similarity between feature vectors at the same layer. This bottom-up manner can be effective for missions like object detection because it cares more about the macroscopic semantic, but there is less capture and fewer secondary understanding of detailed textures when the layers are deepened~\cite{ERF_2016}, especially the relationship between pixels and patches. Based on this analysis, Vision Transformer~\cite{ViT_2021} came to our view. As feature vectors in self-attention have inner product operations on each other, the similarity between vectors is taken into account, i.e., the relevance between different pixels is compared one by one in each block. Further, in terms of information density, IML problems will require more detailed, texture-level information than traditional vision problems, i.e., they are more information-dense. This also makes Transformer-based methods from semantically dense NLP more suitable for parsing artifacts than CNNs.


% % SOTA多数使用CNN
% the state-of-the-arts are widely adopting CNNs~\cite{CNN_1989} like VGG~\cite{VGG_2015} or Resnet~\cite{Resnet_2016} as backbone.However, when we revisit past research, the question arises: We attempt to discuss about this issue from following perspectives:
% we have a question against this CNN prevalence: 

% However, there is no ViT-based~\cite{ViT_2021} model that has been introduced for solving the IML problem. 

% 使用MAE解决这个问题




% \bigskip
% \noindent Thank you for reading these instructions carefully. We look forward to receiving your electronic files!


