{
  "title": "IML-ViT: Benchmarking Image Manipulation Localization by Vision Transformer",
  "authors": [
    "Xiaochen Ma",
    "Bo Du",
    "Zhuohang Jiang",
    "Xia Du",
    "Ahmed Y. Al Hammadi",
    "Jizhe Zhou"
  ],
  "submission_date": "2023-07-27T13:49:27+00:00",
  "revised_dates": [
    "2023-08-07T12:13:05+00:00",
    "2023-08-31T13:25:59+00:00",
    "2024-11-24T11:40:23+00:00"
  ],
  "abstract": "Advanced image tampering techniques are increasingly challenging the trustworthiness of multimedia, leading to the development of Image Manipulation Localization (IML). But what makes a good IML model? The answer lies in the way to capture artifacts. Exploiting artifacts requires the model to extract non-semantic discrepancies between manipulated and authentic regions, necessitating explicit comparisons between the two areas. With the self-attention mechanism, naturally, the Transformer should be a better candidate to capture artifacts. However, due to limited datasets, there is currently no pure ViT-based approach for IML to serve as a benchmark, and CNNs dominate the entire task. Nevertheless, CNNs suffer from weak long-range and non-semantic modeling. To bridge this gap, based on the fact that artifacts are sensitive to image resolution, amplified under multi-scale features, and massive at the manipulation border, we formulate the answer to the former question as building a ViT with high-resolution capacity, multi-scale feature extraction capability, and manipulation edge supervision that could converge with a small amount of data. We term this simple but effective ViT paradigm IML-ViT, which has significant potential to become a new benchmark for IML. Extensive experiments on three different mainstream protocols verified our model outperforms the state-of-the-art manipulation localization methods. Code and models are available at https://github.com/SunnyHaze/IML-ViT.",
  "categories": [
    "cs.CV"
  ],
  "primary_category": "cs.CV",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.14863",
  "pdf_url": "https://arxiv.org/pdf/2307.14863v4",
  "comment": null,
  "num_versions": null,
  "size_before_bytes": 87102568,
  "size_after_bytes": 916382
}