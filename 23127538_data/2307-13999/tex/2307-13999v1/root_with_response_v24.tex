% autosam.tex
% Annotated sample file for the preparation of LaTeX files
% for the final versions of papers submitted to or accepted for
% publication in AUTOMATICA.
% See also the Information for Authors.

% Make sure that the zip file that you send contains all the
% files, including the files for the figures and the bib file.
% Output produced with the elsart style file does not imitate the
% AUTOMATICA style. The style file is generic for all Elsevier
% journals and the output is laid out for easy copy editing. The
% final document is produced from the source file in the
% AUTOMATICA style at Elsevier.

% You may use the style file autart.cls to obtain a two-column
% document (see below) that more or less imitates the printed
% Automatica style. This may helpful to improve the formatting
% of the equations, tables and figures, and also serves to check
% whether the paper satisfies the length requirements.

% Please note: Authors must not create their own macros.
% For further information regarding the preparation of LaTeX files
% for Elsevier, please refer to the "Full Instructions to Authors"
% from Elsevier's anonymous ftp server on ftp.elsevier.nl in the 
% directory pub/styles, or from the internet (CTAN sites) on
% ftp.shsu.edu, ftp.dante.de and ftp.tex.ac.uk in the directory
% tex-archive/macros/latex/contrib/supported/elsevier.

%\documentclass{elsart}               % The use of LaTeX2e is preferred.
\documentclass[twocolumn]{autart}    % Enable this line and disable the
                                     % preceding line to obtain a two-column
                                     % document whose style resembles the
                                     % printed Automatica style.
\usepackage{enumitem}                           
\usepackage {xcolor}
\usepackage{float}
\usepackage{cuted}
\usepackage{graphicx}          % Include this line if your
\usepackage{booktabs}
\usepackage{changes}
\usepackage{cancel}
\usepackage{hhline}           % document contains figures,
%\usepackage[dvips]{epsfig}    % or this line, depending on which
      \usepackage{cuted}

\usepackage[export]{adjustbox}
                         % you prefer.
\usepackage{tikz}
\usetikzlibrary{shapes,arrows}
\usepackage{amssymb,amsmath,graphicx,multicol,epstopdf}
%\usepackage{natbib}
\usepackage{multirow}
\newtheorem{Example}{Example}[section]
\newtheorem{Remark}{Remark}[section]
\newtheorem{Corollary}{Corollary}[section]
\newtheorem{Definition}{Definition}[section]
\newtheorem{Theorem}{Theorem}[section]
\newtheorem{Proposition}{Proposition}[section]
\newtheorem{proof}{Proof}
\newtheorem{Lemma}{Lemma}[section]
\newtheorem{Assumption}{Assumption}[section]
%\sectionformat{\section}{\arabic{chapter}.\arabic{section}.\arabic{subsubsection}}
%\makeatletter
%\renewcommand\thesection{\@arabic\c@section}
%\renewcommand\thesubsubsection{\@arabic\c@subsubsection}
%\makeatother

\newcommand{\alert}[1]{{\color{red} #1}}
\newcommand{\N}{{\mathcal N}}


%\def\baselinestretch{1.5}
\newcommand{\cd}[2]{#1^{(#2)}}
\newcommand{\kcd}[1]{#1^{(j+1)}}
\newcommand{\jcd}[1]{#1^{(j)}}
\newcommand{\icd}[1]{#1^{(j-1)}}
\newcommand{\fcd}[1]{#1^{(1)}}
\newcommand{\tcd}[1]{#1^{(3)}}
\newcommand{\scd}[1]{#1^{(2)}}
\newcommand{\norm}[1]{\|{#1}\|}
\newcommand{\abnorm}[1]{|{#1}|}
\newcommand{\an}[1]{\|#1\|_a}
\newcommand{\lnd}[1]{\|{#1}\|_\infty}
\newcommand{\lng}[3]{\|#1_{[#2,#3]}\|}
\newcommand{\ld}{L_}
\newcommand{\PV}{\frac{\partial V}{\partial x}}
\newcommand{\PH}{\frac{\partial H}{\partial x}}
\newcommand{\BM}{\begin{matrix}}
\newcommand{\EM}{\end{matrix}}
\newcommand{\ul}{\underline }
\newcommand{\ulx}{\underline x}
\newcommand{\ep}{\epsilon}
\newcommand{\sgm}{\sigma}
\newcommand{\Dt}{\Delta}
\newcommand{\dt}{\delta}
%\newcommand{\bt}{\beta}
\newcommand{\al}{\alpha}
\newcommand{\lbd}{{\lambda}}
\newcommand{\kth}{$k^{th}$-order }
\newcommand{\KRORP}{$k^{th}$-order robust output regulation problem }
\newcommand{\RORP}{robust output regulation problem }
\newcommand{\LORP}{linear output regulation problem }
\newcommand{\GRORP}{Global robust output regulation problem }
\newcommand{\om}{\omega}
\newcommand{\homega}{\hat{\omega}}
\newcommand{\bx}{{\bf x}}
\newcommand{\bXi}{{\bf \Xi}}
\newcommand{\Be}{{\bf e}}
\newcommand{\bz}{{\bf z}}
\newcommand{\br}{{\bf r}}
\newcommand{\bw}{{\bf w}}
\newcommand{\bxc}{{{\bf x}_c}}
\newcommand{\bu}{{\bf u}}
\newcommand{\by}{{\bf y}}
\newcommand{\bbu}{\bar{\bf u} }
\newcommand{\bbx}{\bar{\bf x} }
\newcommand{\bbz}{\bar{\bf z} }
\newcommand{\bby}{{\bf y}^{(k)} }
\newcommand{\p}{{\partial}}
\newcommand{\px}{{\partial x}}
\newcommand{\pe}{{\partial e}}
\newcommand{\pxc}{{\partial x_c}}

\newcommand{\pa}{{\partial a}}
\newcommand{\pv}{{\partial v}}
\newcommand{\pu}{{\partial u}}
\newcommand{\pr}{{\partial r}}
\newcommand{\pw}{{\partial w}}
\newcommand{\pfc}{{\partial f_c}}
\newcommand{\phc}{{\partial h_c}}
\newcommand{\ph}{{\partial h}}
\newcommand{\pk}{{\partial k}}
\newcommand{\pg}{{\partial g}}
\newcommand{\pz}{{\partial z}}
\newcommand{\nnum}{\nonumber}
\newcommand{\col}{}
\newcommand{\ba}{\begin{array}}
\newcommand{\ea}{\end{array}}
\newcommand{\be}{\begin{eqnarray}}
\newcommand{\ee}{\end{eqnarray}}
\newcommand{\EQQ}{\begin{eqnarray*}}
\newcommand{\ENN}{\end{eqnarray*}}
\newcommand{\defi}{\stackrel{\mbox{\small def}}{=}}
\newcommand{\IEEE}{{\it IEEE Transactions on Automatic Control}}
\newcommand{\letter}{{\it Systems and Control Letters}}
\newcommand{\R}{{\mathbb R}}
\newcommand{\D}{{\mathbb D}}
\newcommand{\W}{{\mathbb W}}
\newcommand{\V}{{\mathbb V}}
\newcommand{\M}{{\mathbb M}}
\newcommand{\X}{{\mathbb X}}
\newcommand{\A}{{\mathbb A}}
\newcommand{\B}{{\mathbb B}}
\newcommand{\Sig}{{\mathbb S}}

\newcommand{\calp}{{\cal P}}
\newcommand{\calq}{{\cal Q}}
\newcommand{\myast}{ }

\newcommand{\falpha}{{\mathfrak a}}
\newcommand{\fbeta}{{\mathfrak b}}
\newcommand{\fgamma}{{\mathfrak c}}
\newcommand{\ftheta}{{\mathfrak h}}
\DeclareMathOperator*\argmin{arg\,min}
\newcommand{\fg}{{\mathfrak g}}

\newcommand{\rank}[1]{rank [{#1}]}
%\def\p{{\partial}}
\def\oA{{\overline A}}
\def\of{{\overline f}}
\def\oP{{\overline P}}
\def\ophi{{\overline \Psi}}
\def\ox{{\overline x}}
\def\oX{{\overline X}}
\def\oy{{\overline y}}
\def\oz{{\overline z}}
\def\p#1#2{\dfrac{\partial #1}{\partial #2}}
\def\S{{\cal S}}
\def\tf#1#2#3#4{\left[\begin{array}{cc}#1&#2\\
                                       #3&#4 \end{array}\right]}
\def\tt#1#2{\left[\begin{array}{c}#1\\ #2\end{array}\right]}

\def\rank{\mbox{rank}}
\def\Re{\mbox{Re}}

\newcommand{\tr}{\text{tr}}
\newcommand{\mean}[1]{\mathbb{E}(#1)}
\newcommand{\var}[1]{\mathbb{V}(#1)}
\newcommand{\cov}[1]{\mathbb{C}ov(#1)}

\newcommand{\bd}{\begin{Definition}
\begin{rm} }
\newcommand{\ed}{ \end{rm}
\end{Definition} }

\newcommand{\bexercise}{\begin{exercise}\vspace{-3mm}
\begin{rm} }
\newcommand{\eexercise}{ \end{rm}
\end{exercise} }

\newcommand{\bappend}{\begin{append}\vspace{-3mm}
\begin{rm} }
\newcommand{\eappend}{ \end{rm}
\end{append} }

\newcommand{\basm}{\begin{Assumption} \begin{rm}}
\newcommand{\easm}{\end{rm} \end{Assumption}}

\newcommand{\bpropty}{\begin{property} \vspace{-3mm}\begin{rm}}
\newcommand{\epropty}{\end{rm} \end{property}}



\newcommand{\bremark}{\begin{Remark}
\begin{rm} }
\newcommand{\eremark}{\hfill$\lozenge$ \end{rm}\end{Remark} }
\newcommand{\bt}{\begin{Theorem} \begin{rm} }
\newcommand{\et}{ \end{rm}
\end{Theorem} }
\newcommand{\bl}{\begin{Lemma} \begin{rm} }
\newcommand{\el}{ \end{rm}
\end{Lemma} }
\newcommand{\bcorollary}{\begin{Corollary} \begin{rm} }
\newcommand{\ecorollary}{ \end{rm}
\end{Corollary} }
\newcommand{\bdefinition}{\begin{Definition}\begin{rm} }
\newcommand{\edefinition}{ \end{rm}
\end{Definition} }
\newcommand{\bproposition}{\begin{proposition} \begin{rm} }
\newcommand{\eproposition}{ \end{rm}
\end{proposition} }
\newcommand{\bexample}{\begin{example} \begin{rm} }
\newcommand{\eexample}{ \end{rm}
\end{example} }

\newcommand{\bproof}{\begin{proof} \begin{rm} }
\newcommand{\eproof}{ \end{rm} \end{proof} }

%\addtolength{\oddsidemargin}{-.2in}
%\addtolength{\evensidemargin}{-.5in}

\newcommand{\inn}[1]{\langle #1 \rangle}

\DeclareMathOperator*{\minimize}{minimize}
\DeclareMathOperator*{\maximize}{maximize}

%===============================================================================
\begin{document}
\begin{frontmatter}

\title{On Kernel Design for Regularized Non-Causal System Identification\thanksref{footnoteinfo}
 }
% Title, preferably not more than 10 words.

\vspace*{-8mm}

    \thanks[footnoteinfo]{A prelimnary version of this paper was presented at the 41st Chinese Control Conference, Heifei, China, July 25-27, 2022. Corresponding author Tianshi Chen.  \\ This work was funded by NSFC under contract No. 62273287, the Shenzhen Science and Technology Innovation Council under contract No. JCYJ20220530143418040, and the Thousand Youth Talents Plan funded by the central government of China.} 
    
    \author[CUHKSZ]{Xiaozhu Fang}\ead{xiaozhufang@link.cuhk.edu.cn},    % Add the 
    \author[CUHKSZ]{Tianshi Chen}\ead{tschen@cuhk.edu.cn}               % e-mail address 
    
    \address[CUHKSZ]{School of Data Science and Shenzhen Research Institute of Big Data, The Chinese University of Hong Kong, Shenzhen 518172, China} 
    
    

%\address[2]{Department of Electrical Engineering, Link\"{o}ping University, Link\"{o}ping, 58183, Sweden, (e-mail: ljung@isy.liu.se).}
\vspace{-4mm}
\begin{abstract} 
Through one decade's development, the kernel-based regularization method (KRM) has become a complement to the classical maximum likelihood/prediction error method and an emerging new system identification paradigm. 
% Its success has inspired people to construct mathematical models by using KRM  in various model-based control methods and control applications. 
One recent example is its application in the non-causal system identification, and the key issue lies in the design and  analysis of kernels for non-causal systems. In this paper, we develop systematic ways to deal with this issue. In particular, we first introduce the guidelines for kernel design and then extend the system theoretic framework to design the so-called non-causal simulation-induced (NCSI) kernel, and we also study its structural properties, including stability and semiseparability. Finally, we consider some special cases of the NCSI kernel and show their advantage over the existing kernels through numerical simulations. 
\end{abstract}

\begin{keyword}
Non-causal system identification, Kernel-based regularization method, Kernel design. 
\end{keyword}

\end{frontmatter}

\section{Introduction}\label{sec:intro}

In the control community, system identification is the term for the area of building up mathematical models of dynamical systems based on the measured input and output data, and has a history of nearly 60 years \cite{Zadeh:56}. In the first 50 years, the major advance of system identification is on the maximum likelihood/prediction error method (ML/PEM) (and its asymptotic analysis), which is often called the classical system identification, e.g., \cite{Ljung:99,SoderstromS:89,PintelonS:01,Ljung:86}. In the last decade, the major advance is on the kernel-based regularization method (KRM), e.g., the survey papers \cite{PDCDL14,Chiuso16,LCM20} and the book \cite{PCCDL22}. In contrast with ML/PEM, KRM has the feature that it finds a systematic way to engage the prior knowledge of the underlying system to be identified in the system identification loop, in particular in the selection of both the model structure and the model complexity. The carrier of the prior knowledge is the so-called kernel, which determines the model structure, and its parameter (called hyper-parameters) determines the model complexity, which can often be tuned in a continuous way, e.g., \cite{PDCDL14,Chiuso16,LCM20,CHEN18}. Hence, the kernel plays a fundamental role and its design is a key issue. A couple of kernel design methods and many kernels have been proposed, e.g., \cite{PD10,CHEN18,ZC18,MSS16}. Now KRM has become not only a complement to the classical ML/PEM \cite{Ljung:99,COL12}, but also an emerging new system identification paradigm, which is often called the regularized system identification \cite{LCM20,PCCDL22}. 



 % is the kernel design, which is regarding how to parameterize the kernel (through the hyper-parameter) to embed the prior knowledge of the underlying system to be identified. 
% { \color{purple}(Review 13 suggest to shorten Introduction)}

% System identification is instrumental for model-based control system design, which is the dominant paradigm in control engineering.} The success of KRM has inspired people  to construct mathematical models by using KRM for various model-based control methods and control applications. 


One recent example is the application of KRM in the non-causal system identification \cite{BO20}, where 
% The idea of inverse model control is to use the inverse of the system as the feedforward controller to achieve exact output tracking. %, i.e., the system's output is identical to the reference input. 
% When the system is unknown, system identification is often used to identify the inverse of the system, e.g., \cite{HE18}. 
% there are two different routes: 
% \begin{itemize}
% \item the first route is to identify the system first based on the measured input and output data, and then invert the model, e.g., \cite{BPA12,ZO18};
% \item the second route is to identify directly the inverse of the system based on the measured output and input data, and often known as inverse model identification, e.g., \cite{BOS15, HE18,JE13}. 
% \end{itemize} 
% Moreover, when the system is non-minimum phase, the identification of the inverse of the system becomes a challenging non-causal system identification problem.
the causal tuned-correlated (TC) kernel proposed in \cite{COL12} was first extended to the non-causal case, and then KRM was used to identify the non-causal impulse response and achieved satisfying results. 
% \item the model complexity for ML/PEM is governed by the model order and tuned by various statistical tests, e.g. AIC, BIC, and cross validation, but they are not as reliable as expected, especially when the data is short or has {\color{red}low signal to noise ratio } (SNR), and in contrast, the model complexity for KRM is governed by the hyper-parameter and tuned in a continuous way by e.g., the empirical Bayes (EB) method, which is quite reliable as verified by a great deal of simulations and experiments, e.g., \cite{PN10a,COL12,PDCDL14}.
% \end{itemize} 
The success of KRM in \cite{BO20} is mainly because  KRM can engage, through the designed kernel, the prior knowledge on the non-causal system to be identified, e.g., the stability and smoothness of the non-causal impulse response. This success confirms the efficacy  and also motivates us to further explore the potential of KRM in the non-causal system identification. Then, it is worth noting that all kernels, except the non-causal TC kernel proposed in \cite{BO20}, are causal kernels designed for causal systems, and the design and analysis of non-causal kernels for non-causal systems has not been studied in a systematic way before. 











% % The kernel-based regularization method (KRM) has attracted much attention in the system identification community in the last decade, see e.g.,  the survey papers \cite{PDCDL14, Chiuso16, LCM20}. It becomes not only a complement to the traditional system identification \cite{Ljung:99,COL12}, but also an emerging new system identification paradigm \cite{LCM20}. The success of the KRM is due to at least the following two reasons: first, the prior knowledge of the underlying model can be embedded into the regularization term by carefully designing kernels; second, the model complexity governed by the hyper-parameters can be tuned in a continuous manner. To fully explore the KRM for system identification, many promising results have been reported in different directions, such as the kernel design, e.g., \cite{MSS16, BP2020mathematical, CHEN18}, the efficient implementation, e.g., \cite{CL13,CA21}, etc. 

% The KRM can be extended from the LTI system identification to other problems, e.g., inverse model control \cite{BO20}. 



% Another critical issue of inverse model identification is that the standard inverse model is unstable for non-minimum phase systems, i.e., systems having unstable zeros. The control signals of the standard inversion-based feedforward controller are unbounded and thus unrealisable in practice. To tackle that problem, many approaches have been developed and one of them is the stable inversion technique, which yields bounded control signals for the exact output tracking, \cite{DCP96, ZD99, ZO18}. A critical difficulty with the stable inversion technique is non-causality, implying that the stable inversion-based feedforward controller should access not only the past but also the future inputs. As a result, inverse model identification can be reformulated as the estimation problem of a non-causal (finite) impulse response model, i.e., a non-causal generalization of the widely used (finite) impulse response models in system identification, see e.g., \cite{Ljung:99}. 
 
 
% Concerning the non-causal impulse response, the underlying kernel should be defined over $\mathbb{Z}\times \mathbb{Z}$ rather than a causal kernel defined over $\mathbb{N}\times \mathbb{N}$ with $\mathbb{N}$ and $\mathbb{Z}$ being the set of natural number and integers, respectively. However, most of the existing kernels are causal kernels, e.g., the stable spline (SS) kernel \cite{PD10}, the diagonal-correlated (DC) kernel, and the tuned$\slash$correlated (TC) kernel \cite{COL12}. By contrast, the non-causal kernels have not been studied until very recently in \cite{BO20}, and moreover, the non-causal kernel design has not been discussed throughout. 


In this paper, we focus on this problem. First, we introduce the  guidelines for the non-causal kernel design and then extend the system theoretic framework proposed in \cite{CHEN18} to design the so-called non-causal simulation-induced (NCSI) kernel. In particular, the NCSI kernel employs the multiplicative uncertainty configuration in robust control, e.g., \cite{ZDG96}, where the nominal model is used to embed the prior knowledge, the uncertainty is modelled by a Gaussian process, and the overall model is simulated with an impulsive input to get the NCSI kernel. Then, we study the structural properties of the NCSI kernel, including stability and semiseparability. Finally, we consider some special parameterizations of the NCSI kernel and show that they give better model estimates/tracking performance than the ones proposed in \cite{BO20} through numerical simulations. 
In contrast with \cite{BO20}, this paper has the following contributions: 
\begin{enumerate}
\item[(i)] we develop a systematic framework to design NCSI kernels for non-causal systems, and moreover, the prior knowledge embedded in NCSI kernels has clear physical interpretation;
% ; in contrast, \cite{BO20} used heuristics to design two individual kernels and besides the stability, the other prior knowledge embedded in the designed kernels is not clear.


% More specifically, \cite{BO20} used a heuristic way to design two kernels (NC-TC and NCBD-TC kernels), and besides the stability, the other embedded prior knowledge for the designed kernels is not clear; in contrast, in this paper, we proposed systematic ways to design NCSI kernels for non-causal systems and the embedded prior knowledge has a clear physical interpretation. Moreover, it is worth to stress that  NC-TC and NCBD-TC kernels are two individual kernels, but the NC-SI kernel is actually a class of kernels and the users can encode the prior knowledge by parameterizing the matrices of the state-space model and $b(t)$ in \eqref{eq:si_ss}.




\item[(ii)] we study the structural properties of NCSI kernels, including stability and semiseparability;
% ; in contrast, \cite{BO20} only had rough analysis over the stability of the designed two individual kernels.


% This analysis is innovative because the off-diagonal blocks does not exist  or equal 0 in the existing kernels, e.g., \cite[Sec. 5.2]{PDCDL14} and \cite[Sec. 4.3]{BO20}. 
\item[(iii)] we test the designed NCSI kernels by  Monte Carlo simulations with randomly generated systems and moreover, the simulation results show that the designed NCSI kernels perform (in terms of average accuracy and robustness) significantly better.
% than the state-of-art kernels proposed in \cite{BO20} over four test data banks including the one in  \cite{BO20}.


\end{enumerate}
% Finally, in contrast with its preliminary version \cite{FC22}, this paper has been completely rewritten with more details and analysis. In particular, the guidelines for non-causal kernel design, the analysis of the structural properties of the designed kernels, and the proof of all theoretical results were not presented in \cite{FC22} and a more detailed numerical simulation is also included in this paper. 




The remaining part of this paper is organized as follows. In Section \ref{se:background}, we first introduce some background materials and then the problem statement. In Section \ref{se:guideline}, we introduce the guidelines for non-causal kernel design. In Section \ref{se:ncsi_kernel}, we design the NCSI kernel followed by some analysis and examples. In Section \ref{se:simulation}, we run numerical simulations to illustrate the efficacy of the designed kernels. Finally,  we conclude in Section \ref{se:conclusion}. The proofs of theoretical results are deferred to the appendix.

\section{Background and Problem Statement}\label{se:background}
In this section, we first introduce some background materials and then the problem statement.


% The considered problem is formulated in this section. First, the feedforward control problem and stable inversion are defined. Second, the regularized identification for inverse models is introduced. Third, the kernel design is reviewed. Lastly, the problem is issued.   



\subsection{Non-Causal System Identification}\label{se:non-causal_sysid}
We consider linear time-invariant (LTI), discrete-time, non-causal and bounded-input-bounded-output (BIBO) stable systems described by 
% \subsubsection{Inverse Model Based Feedforward Control}
\begin{align}\label{eq:pro_sys1}
y(t)= G(q)u(t)+v(t), t=1, 2, \cdots, N,
\end{align}
where $t$ is the time instant, $u(t),y(t)\in \mathbb{R}$ are the input and output of the system, respectively, $v(t)$ is the measurement noise and assumed to be a white noise with mean 0 and variance $\sigma^2$, $q$ is the forward-shift operator, i.e., $qu(t)=u(t+1)$, and $G(q)$ the transfer function of the system. 
Moreover, $G(q)$ can be represented as
\begin{align}\label{eq:G_0(q) inverse}
G(q)= \sum\limits_{k=-\infty}^{\infty}g^0(k)q^{-k}, 
\end{align} where $\{g^0(k)\}_{k\in\mathbb Z}$ is called the non-causal impulse response of $G(q)$, and clearly, $G(q)$ is BIBO stable if its non-causal impulse response is absolutely summable. 
% , i.e., $\{g^0(k)\}_{k\in \mathbb{Z}}\in \ell_1(\mathbb{Z})$.
% % \footnote{Let $\ell_p(\mathbb{Z})$ denote a set of sequences by 
% \begin{align}
% &\ell_p(\mathbb{Z})= \{ \{x(k)\in\mathbb R, k\in\mathbb Z\}| \left(\sum\limits_{k=-\infty}^\infty |x(k)|^p\right)^{1/p}<\infty \},
% \end{align} 
% where $\mathbb{Z}$ is the set of integers, and $1\leq p\leq \infty, \nonumber $} .
The non-causal system identification problem is to estimate  $\{ g^0(k)\}_{k\in\mathbb Z}$ or simply $g^0$ below as well as possible based on the input-output data $\{u(t), y(t)\}_{t=1}^N$.



It is worth to stress that the non-causal system \eqref{eq:pro_sys1} naturally arises from the  inverse model control, where the true system is unknown, stable, non-minimum phase and has no zeros on the unit circle, e.g., \cite{BO20} for more details. Besides, the non-causal systems also arise from other contexts, e.g., the continuous-time system identification \cite{GRH21}, and unstable system identification \cite{Fujimoto22}.


% {\color{blue} Reviewer \#12: ``which are the benefits of considering non-causal systems (besides naturally emerging from the inversion)'', we may need to add: Apart from the inverse model, the non-causal systems are also used in other problems, e.g., \cite{GRH21,Fujimoto22} }




% The block diagram of system \eqref{eq:pro_sys1} with feedforward controller \eqref{eq:pro_sys2} is shown in Fig.\ref{fig:blk_sys}. 
%  % Figure environment removed
% \begin{Remark}\label{rmk:G(q) and G(z)} It is worth to recall from \cite{Ljung:99} that the transfer function $G_0(q)$ of the system in the time domain can be written in the form of 
% \begin{align}
%   G_0(q) = \sum\limits_{k=0}^\infty  g_0(k)q^{-k},
%   \end{align}   where $\{g_0(k)\}_{k\in\mathbb N}$ is the impulse response of $G_0(q)$, and is the same rational function as the pulse transfer function $G_0(z)$ of the system in the complex frequency domain, i.e., the $z$-transform of $\{g_0(k)\}_{k\in\mathbb N}$, that is, 
% \begin{align}\label{eq:g0_tf}
%   G_0(z) = \sum\limits_{k=0}^\infty  g_0(k)z^{-k}.
%   \end{align} 
% The properties of $G_0(q)$ (stability, causality, poles and zeros, and inverse) are defined in the same way as $G_0(z)$. In particular, when $G_0(z)$ is stable, causal and inversely stable (i.e., $1/G_0(z)$ is analytic for $|z|\geq 1$), there exists $\{\tilde g_0(k)\}_{k\in\mathbb N}$ such that
% $1/G_0(z)$ can be written as 
%  \begin{align}\label{eq:1overPq}
% \frac{1}{G_0(z)}= \sum\limits_{k=0}^{\infty} \tilde{g}_0(k)z^{-k},\quad \sum\limits_{k=0}^\infty |\tilde{g}_0(k)|<\infty.
%  \end{align}
% Then it follows from \cite[Lemma 3.1]{Ljung:99} that $G_0^{-1}(q)$, the inverse of $G_0(q)$, is defined in a similar way as 
% \begin{align}
% G_0^{-1}(q)= \sum\limits_{k=0}^{\infty} \tilde{g}_0(k)q^{-k},\quad \sum\limits_{k=0}^\infty |\tilde{g}_0(k)|<\infty. 
% \end{align}
% \end{Remark}
% The goal of feedforward control is to design $F(q)$ such that the tracking error 
% \begin{align}\label{eq:tracking_err}
% e(t)= r(t)-y(t)= r(t)-G_0(q)u(t),
% \end{align}
% is as small as possible. If $G_0(q)$ is minimum phase, then the inverse model control, i.e.,  
% \begin{align}\label{eq:tf_pinv}
% u(t)=F(q)r(t)=G_0^{-1}(q)r(t),
% \end{align}
% enables the exact tracking, i.e., $e(t)=0$, i.e., $y(t)=r(t)$. 


% If $G_0(q)$ is non-minimum phase, in order to apply the inverse model control \eqref{eq:tf_pinv}, there are a couple of issues to be addressed. Firstly, as can be seem from \cite[Lemma 3.1]{Ljung:99}, $G_0^{-1}(q)$, the inverse of $G_0(q)$, is only defined for the case where $G_0(q)$ is minimum phase. Secondly, if one adopts the idea in \cite[Lemma 3.1]{Ljung:99} to define $G_0^{-1}(q)$ from $1/G_0(z)$, then since $1/G_0(z)$ is unstable and thus has divergent impulse response,  a straightforward implementation of \eqref{eq:tf_pinv} would lead to unbounded $u(t)$, which cannot be realized in practice. Therefore, special care is required, and summarized in the following Lemma. 


% % it follows from \cite[Lemma 3.1]{LJUNG99} that $G_0^{-1}(q)$, the inverse of $G_0(q)$,




% % However, since $G(q)$ is NMP, $G_0^{-1}(q)$ is unstable and thus a straightforward implementation of the inverse model control \eqref{eq:tf_pinv} would lead to unbounded $u(t)$, which cannot be realized in practice. Therefore, special care is required to overcome this problem.



% % \subsubsection{Stable Inversion and Definition of $G_0^{-1}(q)$}
% % When $G_0(q)$ is non-minimum phase, the above issues can be solved by the stable inversion technique, which is summarized in Lemma \ref{lem:st_st}. 

% % \begin{Definition}\label{as:gz_no_pole}
% % Let $\mathcal{L}_2(\mathbb{T})$ denote a set of functions by
% % \begin{align}
% % \mathcal{L}_2(\mathbb{T})= \{ f(x): \mathbb{T}\rightarrow \mathbb{C}|  \int_0^{2\pi} |f(e^{j\theta})|^2 d\theta <\infty \}. 
% % \end{align}
% % where $\mathbb{T}$ is the unit circle, and $\mathcal{RL}_2(\mathbb{T})$ is  the subspace of $\mathcal{L}_2(\mathbb{T})$ with $f(x)$ is real-valued and rational. Let $\ell_p(\mathbb{Z})$ denote a set of sequences by 
% % \begin{align}
% % &\ell_p(\mathbb{Z})= \{ \{a_n\}:\mathbb{Z}\rightarrow \mathbb{C}| \left(\sum\limits_{n=-\infty}^\infty |a_n|^p\right)^{1/p}<\infty \},
% % \end{align} 
% % where $\mathbb{Z}$ the set of integers, and $1\leq p< \infty, \nonumber $
% % \end{Definition} 
% \begin{Lemma}\textit{\textbf{(\cite[Theorem 1]{BO20}, \cite[Lemma 2]{RO22},\cite[Lemma 3.1]{Ljung:99})}}\label{lem:st_st} Assume that $G_0(z)$ is stable, non-minimum phase and  has no zeros on the unit circle $\mathbb{T}$, then the following results hold:
% \begin{itemize}
% \item $1/G_0(z)\in \mathcal{RL}_2(\mathbb{T})$, and there exists $\{\tilde g_0(k)\}_{k\in\mathbb Z}\in \ell_1(\mathbb{Z})$
% such that the Laurent series of $1/G_0(z)$ at 0 can be written as\footnote{Let %$\mathcal{L}_2(\mathbb{T})$ denote a set of functions by
% $\mathcal{L}_2(\mathbb{T})= \{ f(x): \mathbb{T}\rightarrow \mathbb{C}|  \int_0^{2\pi} |f(e^{j\theta})|^2 d\theta <\infty \}$, and $\mathcal{RL}_2(\mathbb{T})$ a subspace of $\mathcal{L}_2(\mathbb{T})$ that consists of real-valued and rational functions in $\mathcal{L}_2(\mathbb{T})$. Let %$\ell_p(\mathbb{Z})$ denote a set of sequences by 
% $\ell_p(\mathbb{Z})= \{ \{x(k)\in\mathbb R, k\in\mathbb Z\}| (\sum\limits_{k=-\infty}^\infty |x(k)|^p)^{1/p}<\infty \}$,
% where $\mathbb{Z}$ is the set of integers, and $1\leq p\leq \infty$.} 
% \begin{align}
% &1/G_0(z)= \sum\limits_{k=-\infty}^\infty \tilde g_0(k)z^{-k},\label{eq:ztransform}\\
% &\tilde g_0(k)= \frac{1}{2\pi i}\oint_{|z|=1}1/G_0(z)z^{k-1}dz,\   k\in\mathbb Z  \label{eq:laurent}
% \end{align} 

% \item 
% define $G_0^{-1}(q)$, the inverse of $G_0(q)$, by 
% \begin{align}\label{eq:G_0(q) inverse}
% G_0^{-1}(q)= \sum\limits_{k=-\infty}^{\infty}\tilde g_0(k)q^{-k}. 
% \end{align} and then for any $\{r(k)\}_{k\in\mathbb Z}\in \ell_\infty(\mathbb{Z})$, there exists a unique control $u(t)\in \ell_\infty(\mathbb{Z})$, defined by
% \begin{align}
% &u(t) = G_0^{-1}(q) r(t) =\sum\limits_{k=-\infty}^\infty \tilde g_0(k)r(t-k)
% \label{eq:stable_inv_series}
% \end{align}
% leading to the exact tracking, i.e., $e(t)=0$.
% \end{itemize}
% \end{Lemma}
% % \begin{Remark}\label{re:stable_inv}
% % Note that $1/G_0(z)\in \mathcal{L}_2(\mathbb{T})$ can only guarantee $\{\tilde g_0(k)\}_{k\in\mathbb Z}\in \ell_2(\mathbb{Z})$ by the Fourier transform's isomorphism, but $1/G_0(z)\in \mathcal{RL}_2(\mathbb{T})$ can guarantee $\{\tilde g_0(k)\}_{k\in\mathbb Z}\in \ell_1(\mathbb{Z})$ \cite[Theorem 2.1.10]{CG00}. In fact, rationality of $1/G_0(z)$ is a sufficient condition, and there are weaker sufficient conditions for $\{\tilde g_0(k)\}_{k\in\mathbb Z}\in \ell_1(\mathbb{Z})$, e.g., $1/G_0(z)$ is continuously differentiable \cite[Exercise 6.6-15]{GAMELIN03}. In addition, \eqref{eq:ztransform} and \eqref{eq:laurent} are also called the (two-sided) Z transform and inverse Z transform, denoted by $1/G_0(z)= \mathcal{Z}[\{\tilde g_0(k)\}_{k\in\mathbb Z}](z)$ and $\tilde g_0(k)= \mathcal{Z}^{-1}[1/G_0(z)](k)$, respectively. 
% % \end{Remark}

% %\begin{proof}
% %Since $r(t)\in \ell_2(\mathbb{Z})$, it admits a unique Z transform as $R(z)=\mathcal{Z}[r(t)](z)$. Hence, the exact tacking exists when
% %\begin{align*}
% %y(t)= \mathcal{Z}^{-1}[G(z)\cdot 1/G(z)\cdot R(z)]= \mathcal{Z}^{-1}[R(z)]=r(t)
% %\end{align*}
% %By convolution property of the Z transform, see e.g., \cite{PROAKIS01}[Chapter 3], we obtain \eqref{eq:stable_inv_series} and \eqref{eq:laurent}. 
% %
% %Next, we show the existence and uniqueness of \eqref{eq:laurent}. Let $\mathbb{T}$ denote the unit circle and $L_2(\mathbb{T})$ denote the function square defined by
% %\begin{align}
% %L_2(\mathbb{T})= \{f(x): \mathbb{T}\rightarrow \mathbb{C}| \int_0^{2\pi }|f(e^{i\theta})|^2 d\theta <\infty\}
% %\end{align}
% %Then we have $1/G(z)\in L^2(\mathbb{T})$. Since $L^2(\mathbb{T})$ is isomorphic to $\ell^2(\mathbb{Z})$, see e.g., \cite[p153]{HB01}, there exists a unique $g(t)\in \ell_2(\mathbb{Z})$ and $u(t)\in \ell_2(\mathbb{Z})$.  
% %
% %Lastly, it is shown that $g(t)$ is real-valued because 
% %\begin{align*}
% %g(t)= \mathcal{Z}^{-1}[1/G(z)]=\mathcal{Z}^{-1}[1/\overline{G(z)}]=\overline{g(t)}
% %\end{align*}
% %\end{proof}
% %\begin{proof}
% %Consider the Laurent series expansion at 0 for $(1/G)'(z)= \partial(1/G(z))/\partial z$ such that 
% %\begin{align*}
% %g'(t)= \mathcal{Z}^{-1}[(1/G)'(z)](t).
% %\end{align*}
% %By the property of the Z transform, see e.g., \cite{PROAKIS01}[Chapter 3],  we have $g'(t)= tg(t),\ \forall t\in \mathbb{Z}$. By the Cauchy-Schwarz inequality,
% %\begin{align*}
% %\sum\limits_{t=-\infty}^\infty |g(t)|&=|g(0)|+ \sum\limits_{t\neq 0} |\frac{1}{t} g'(t)|\\
% % &\leq |g(0)|+ \sqrt{\sum\limits_{t\neq 0} \frac{1}{t^2}} \sqrt{\sum\limits_{t\neq 0}|g'(t)|^2} 
% %\end{align*}
% %where $\sum_{t=-\infty}^\infty 1/t^2$ is bounded obviously, and $\sum_{t=-\infty}^\infty|g'(t)|^2$ is proven to be bounded as follows.
% %
% %Recall that $1/G(z)$ is continuously differentiable implies that $(1/G)'(z)$ is continuous, and thus we have 
% %\begin{align*}
% %\sum\limits_{t=-\infty}^\infty |g'(t)|^2 \leq \frac{1}{2\pi}\int_{0}^{2\pi} |(1/G)'(e^{i\theta})|^2 d\theta\leq ||(1/G)'||^2_{\infty} 
% %\end{align*}
% %where the left inequality is due to the Bessel's inequality, and the right inequality is due to the $ML$-estimate, see e.g., \cite{GAMELIN03}. Lastly, $||(1/G)'||^2_{\infty} $ is bounded due to the continuity on the unit circle. 
% %\end{proof}



% % For the case where $G_0(q)$ is non-minimum phase, 
% In the literature, e.g., \cite{FL00}, the Laurent series coefficients  $\{\tilde g_0(k)\}_{k\in\mathbb Z}$ in \eqref{eq:laurent} is called the non-causal impulse response of $G_0^{-1}(q)$. Lemma \ref{lem:st_st} states that if $G_0(q)$ is non-minimum phase and has no zeros on the unit circle, then $G_0^{-1}(q)$ has absolutely summable non-causal impulse response, and in this case, for convenience, $G_0^{-1}(q)$ is hereafter said to be stable. Moreover, it also becomes clear that the aforementioned issues are solved by first calculating the non-causal impulse response of $G_0^{-1}(q)$
% and then the inverse model control \eqref{eq:stable_inv_series}.  
% % $\{\tilde g_0(k)\}_{k\in\mathbb Z}$ is called the non-causal impulse response of $1/G_0(z)$.
% % and then accordingly, we can define $G_0^{-1}(q)$, the inverse of $G_0(q)$, as follows
% % \begin{align}\label{eq:G_0(q) inverse}
% % G_0^{-1}(q)= \sum\limits_{k=-\infty}^{\infty}\tilde g_0(k)q^{-k}.
% % \end{align} 
% % As a result, when $G_0(q)$ is non-minimum phase, the inverse model control \eqref{eq:tf_pinv} should be understood based on \eqref{eq:G_0(q) inverse} and \eqref{eq:stable_inv_series}.
% % {\color{red}


% % \subsubsection{Inverse Model Identification}

% The problem becomes more complicated when $G_0(q)$ is unknown. In this case, one way is to first treat $y(t)$ and $u(t)$ as the input and output, respectively, then  estimate $G_0^{-1}(q)$, or equivalently, its non-causal impulse response $\{\tilde g_0(k)\}_{k\in\mathbb Z}$,  and finally calculate \eqref{eq:stable_inv_series}, e.g., \cite{BOS15, HE18}. This way is known as the inverse model identification, and the so-called inverse model is  
% % often used to identify $G_0^{-1}(q)$, e.g., \cite{BOS15,HE18,JE13}. \
% %  $y(t)$ and $u(t)$ are chosen as the input and output, respectively, then one can estimate $G_0^{-1}(q)$, or equivalently, its non-causal impulse response $\{\tilde g_0(k)\}_{k\in\mathbb Z}$,  and then calculate \eqref{eq:tf_pinv}, e.g., \cite{BOS15, HE18,JE13}
% % there are two different routes depending on the choice of the input and output: 
% %  If $y(t)$ and $u(t)$ are chosen as the input and output, respectively, then one can estimate $G_0^{-1}(q)$, or equivalently, its non-causal impulse response $\{\tilde g_0(k)\}_{k\in\mathbb Z}$,  and then calculate \eqref{eq:tf_pinv}, e.g., \cite{BOS15, HE18,JE13}
% % The second route is known as the inverse model identification, which is in our interest. For this case,  
% described by 
% \begin{align}%\label{eq:inverse_model}
% u(t) %&= G_0^{-1}(q)y(t) + v_u(t), \\
%  &= \sum\limits_{k=-\infty}^{\infty}\tilde g_0(k) y(t-k) + v_u(t), t=1,\cdots,N, \label{eq:inverse_model_iir}
% \end{align}
% and the goal is to estimate $\{\tilde g_0(k)\}_{k\in\mathbb Z}$ as well as possible based on the input-output data $\{u(t), y(t)\}_{t=1}^N$.


% \begin{Remark}
% For the case when $G_0(q)$ is unknown, another way is to treat $u(t)$ and $y(t)$ as the input and output, respectively, then one can first estimate $G_0(q)$ and finally apply the stable inversion technique \eqref{eq:laurent} and \eqref{eq:stable_inv_series} to calculate \eqref{eq:tf_pinv}. The discussions about these two ways can be found in  \cite[Section 2.2]{BO20}.
% \end{Remark}

\subsection{Kernel-Based Regularization Method}
% Then the simplest method to estimate $\tilde g_0$ is the method of Least Squares (LS): 
% \begin{align}\label{eq:ls_est}
% \hat{g}^{\text{LS}}= \underset{  g}{\text{argmin}}\sum\limits_{t=1}^N(u(t)-\sum\limits_{k=-\infty}^{\infty} g(k) y(t-k))^2.
% \end{align}
% However, the LS estimator \eqref{eq:ls_est} may suffer from large variance when the sample size $N$ is small or the signal-to-noise ratio (SNR) of the data is low, e.g., \cite{PDCDL14}. 
The non-causal system identification problem of \eqref{eq:pro_sys1} can be handled by the kernel-based regularization method (KRM), which relies on a positive semidefinite kernel $k(t,s; \eta): \mathbb{Z}\times \mathbb{Z}\rightarrow \mathbb{R}$ with $\eta\in\Omega\subset\R^p$ being the hyper-parameter and $p\in\mathbb N$ being its dimension. In particular, KRM searches for a regularized least squares (RLS) estimate $\hat{g}^{\text{R}}$ of $g^0$ 
in the reproducing kernel Hilbert space (RKHS) $\mathcal{H}_k$ associated with $k(t,s; \eta)$: 
\begin{equation}\label{eq:rels_est}
\hat{g}^{\text{R}}= \underset{ g\in \mathcal{H}_k}{\text{argmin}}\sum\limits_{t=1}^N(y(t)-\sum\limits_{k=-\infty}^{\infty}g(k) u(t-k))^2+ \gamma || g||^2_{\mathcal{H}_k},
\end{equation}
where $||\cdot||_{\mathcal{H}_k}$ is the norm of $\mathcal{H}_k$, and $\gamma>0$ is the regularization parameter. 


% {\color{blue}
% The simplest method to estimate $\theta$ in \eqref{eq:matrix_form} is the method of Least Squares (LS): 
% \begin{align}
% \hat{\theta}^{\text{LS}}= \underset{ \theta\in \mathbb{R}^{n}}{\text{argmin}}||U-\Psi\theta||_2^2.
% \end{align}
% where $n=n_c+n_a+1$. However, the LS estimator \eqref{eq:ls_est} may suffer from large variance when the sample size $N$ is small or the signal-to-noise ratio (SNR) of the data is low, see e.g., \cite{PDCDL14}. To handle this problem, one way is to use the kernel-based regularization method (KRM), which adds a quadratic regularization term into the LS criterion in \eqref{eq:ls_est}, leading to the following regularized least square (RLS) estimator: 
% \begin{equation}
% \hat{ \theta}^{\text{RLS}}= \underset{ \theta\in \mathbb{R}^{n}}{\text{argmin}}||U-\Psi\theta||_2^2+ \sigma^2\theta^T  K^{-1}\theta,
% \end{equation}
% where $K\in \mathbb{R}^{n\times n}$ is a positive semidefinite matrix, and its $(i,j)$th element $K_{i,j}$ is designed through a positive semidefinite kernel $k(t,s; \eta): \mathbb{Z}\times \mathbb{Z}\rightarrow \mathbb{R}$ with $\eta\in\Omega\subset\R^p$ being the hyper-parameter and $p\in\mathbb N$ the dimension of $\eta$. 
% It should be stressed that the kernel $k(t,s; \eta): \mathbb{Z}\times \mathbb{Z}\rightarrow \mathbb{R}$ for the non-causal FIR model estimation is defined over  the set of integers $\mathbb{Z}$, and thus different from the one for the FIR model estimation, which is defined over the set of natural numbers $\mathbb{N}$, i.e., $k(t,s; \eta):\mathbb{N}\times \mathbb{N}\rightarrow \mathbb{R}$. To differentiate them, they are called the non-causal kernel and causal kernel, respectively.                                                                                                         }

The design of kernels is a core issue for KRM and is referred to as the parameterization of the kernel $k(t,s;\eta)$ through the hyper-parameter $\eta$ by encoding the prior knowledge of the underlying system to be identified.  
% It should be stressed that the kernel $k(t,s;\eta):\mathbb{Z}\times \mathbb{Z}\rightarrow \mathbb{R}$ is for the non-causal impulse response estimation and defined over the set of integers $\mathbb{Z}$, different from the one for causal impulse response estimation that is defined over the set of natural numbers $\mathbb{N}$, i.e., $k(t,s; \eta):\mathbb{N}\times \mathbb{N}\rightarrow \mathbb{R}$. To differentiate them, we call them the non-causal kernel and causal kernel, respectively.                                                                                                         
The design of causal kernels, i.e., $k(t,s; \eta):\mathbb{N}\times \mathbb{N}\rightarrow \mathbb{R}$, for causal impulse responses has been well studied, e.g., \cite{CHEN18}, and many causal kernels have been introduced, e.g.,  
the diagonal-correlated (DC) kernel and the tuned/correlated (TC) kernel \cite{COL12}:
 \begin{subequations} \label{eq:causal_kernel}
\begin{align}
% &k^{\text{SS}}(t,s;\eta)=c(\frac{\lambda^{t+s+\max\{t,s\}}}{2}-\frac{\lambda^{3\max\{t,s\}}}{6}),\label{eq:causal_ss_kernel}\\
% &\eta= [c, \lambda], \quad c\geq 0,\quad  0\leq\lambda<1,\nonumber \\
 &k^{\text{DC}}(t,s;\eta)=c\lambda^{(t+s)/2}\rho^{|t-s|},\label{eq:causal_dc_kernel}\\
&\eta= [c, \lambda, \rho], \quad c\geq 0,\quad  0\leq\lambda<1, \quad |\rho|\leq 1,\nonumber \\
 &k^{\text{TC}}(t,s;\eta)=c\min\{\lambda^{t},\lambda^{s}\}, \label{eq:causal_tc_kernel} \\
 & \eta= [c, \lambda], \quad c\geq 0,\quad  0\leq\lambda<1.\nonumber     
  \end{align}
  \end{subequations} 
In contrast, the design of non-causal kernels, i.e., $k(t,s; \eta):\mathbb{Z}\times \mathbb{Z}\rightarrow \mathbb{R}$, for non-causal impulse responses has not been studied before until recently in \cite{BO20}. Therein, a so-called non-causal TC (NCTC) kernel was introduced and takes the following form
 \begin{align}\label{eq:nctc}
&k^{\text{NC-TC}}(t,s;\eta)= c\min\{b(t), b(s))\}, \\
&\nonumber \qquad b(t) = \left\{ \begin{array}{ll}
\lambda_c^t, &   t\geq 0,\\
\lambda_a^{-t},  & t<0,
\end{array}\right.\\
& \eta= [c, \lambda_c,\lambda_a],\ c\geq 0, \quad 0\leq \lambda_c< 1,0\leq \lambda_a< 1,\nonumber
\end{align} where $c$ is a scale factor, $\lambda_c, \lambda_a$ describe the decay rates for the causal part, i.e., $t\geq 0$ and the anti-causal part, i.e., $t<0$, respectively.

% {\color{blue}
%  and then a variant of the kernel \eqref{eq:nctc} was derived by removing the correlation between the causal part and the anti-causal part of the non-causal impulse response and takes the following form 
    %   \begin{align}
    %  &k^{\text{NCBD-TC}}(t,s;\eta)= \left\{ \begin{array}{ll}
    %  c\min\{\lambda_c^t, \lambda_c^s\}, &  \text{if } t\geq 0,\ s\geq 0\\
    %  c\min\{\lambda_a^{-t}, \lambda_a^{-s}\}, &  \text{if } t<0,\ s<0\\
    %  0, &  \text{otherwise}.
    %  \end{array}\right.\label{eq:bdtc}\end{align}
    % }
% Since the kernel \eqref{eq:bdtc} has a block-diagonal (BD) structure, it is referred to as the NCBD-TC kernel below. 


% only assigned values in the causal and anti-causal blocks so as to be named the diagonal-block (DB) kernel. A particular case, the BD-TC kernel, is shown as follows, where TC indicates that the causal and anti-causal blocks mimic the TC kernel \eqref{eq:causal_tc_kernel}.




% Other 


% DB kernels, e.g., the DB-DC kernel, can be formulated in the same manner. 



% : the kernel $k(t,s; \eta):\mathbb{Z}\times \mathbb{Z}\rightarrow \mathbb{R} $ for the NCFIR model \eqref{eq:inverse_model_fir} is called the non-causal kernel, and by contrast, the kernel $k(t,s; \eta):\mathbb{N}\times \mathbb{N}\rightarrow \mathbb{R}$ for the FIR model is called the causal kernel. Furthermore, we divide non-causal kernels into different sub-blocks as follows. 
%  \begin{Definition}
% The sub-blocks of the non-causal kernel are denoted by:
% \begin{itemize}
% \item the causal block, i.e., $t>0,\ s>0$, \\
% \item the anti-causal block, i.e., $t<0,\ s< 0$, \\
% \item the off-diagonal blocks, i.e., $ t\cdot s<0$. 
% \end{itemize} There are two off-diagonal blocks which are transposed to each other. The singleton $t=0$ is excluded in any sub-block above. 
% \end{Definition}
% There are two major issues for the RLS estimator \eqref{eq:rels_est}, namely kernel design and hyper-parameter estimation. In this paper, we focus on the kernel design, i.e., the design of kernel function $k(t, s; \eta)$ by parameterizing the kernel with the hyper-parameter $\eta$.  



\subsection{Problem Statement}\label{se:pb_for}
As shown in \cite{BO20},  KRM with the proposed kernels, e.g., \eqref{eq:nctc}, can achieve satisfying results for the non-causal system identification, which motivates us to further explore the potential of KRM. In particular, we aim to address the following questions in relation to the kernel design:



\begin{itemize}
\item[1)] what guidelines should be followed when designing non-causal kernels for non-causal impulse responses;
\item[2)] how to design non-causal kernels  in a systematic way  following those guidelines;
\item[3)] is it possible to design non-causal kernels that give even better performance than the ones proposed in \cite{BO20}, e.g., \eqref{eq:nctc}.
\end{itemize}



% It was addressed in \cite{BO20} that $\{g(t)\}_{t=1}^{n_c}$ and  $\{g(t)\}_{t=-n_a}^{0}$ had zero correlation such that the off-diagonal blocks in the DB kernel were set zero. However, the zero off-diagonal blocks contradict the behavior of the optimal kernel, whose off-diagonal blocks are non-zero. This contradiction motivates us to ask the following three questions:
% \begin{enumerate}[label=(\alph*)]
% \item Must off-diagonal blocks be zero? 
% \item Are non-zero off-diagonal blocks be beneficial? 
% \item How to design the off-diagonal blocks? 
% \end{enumerate}

% The first question is related to the correlation between $\{g(t)\}_{t=1}^{n_c}$ and  $\{g(t)\}_{t=-n_a}^{0}$. It is worth noting that both correlation and independence come from statistics and is defined by randomness. In system and control, this randomness is often ascribed to the uncertainty regarding the parsimonious model. Then the first question turns to be: are causal and anti-causal uncertainty independent? Not exactly, because the uncertainty is unknown and depends on our assumption. Particularly, such independence fails to hold when the uncertainty is assumed multiplicative, see Section \ref{se:non-si}.  

% For the second question, making use of off-diagonal blocks is very likely to be beneficial due to the gap between the DB kernel and the optimal kernel. Because of zero off-diagonal blocks, the DB kernel can neither reduce to the R1 kernel nor recover the optimal kernel. It suggests that the DB kernel is not flexible enough to achieve the optimal performance, and by contrast, we expect the new kernel to be able to tackle this problem. 


% In what follows, we propose a new non-causal kernel, namely the non-causal simulation-induced (SI) kernel, based on the system theory method in \cite{CHEN18}.

% \deleted{The design of off-diagonal blocks is not as simple as that of the causal and anti-causal blocks. If we use the machine learning method, i.e., treating $g$ as a function, whose amplitude varies with a certain rate and decays exponentially to zero in both ends. }









 
% In particular, as shown in Lemma \ref{lem:st_st}, the non-causal impulse response $\tilde g_0$ is known to be stable, i.e., $\{\tilde g_0(k)\}_{k\in\mathbb Z}\in \ell_1(\mathbb{Z})$. Therefore, following the first guideline of kernel design, the designed non-causal kernel should reflect this prior knowledge, e.g., $k(t,s)$ should go to 0 as $t\rightarrow \pm \infty$. More generally, the designed non-causal kernel should guarantee that its associated RHKS $\mathcal{H}_k$ satisfies $\mathcal{H}_k\subset\ell_1(\mathbb{Z})$. 
% such that the regularized estimator $\hat{g}$ satisfies $\hat{g}(t)\in \mathcal{H}_k\subset\ell_1(\mathbb{Z})$ by the representer theorem \cite{PDCDL14}[Theorem 3]. It is worth to note that $\ell_1(\mathbb{Z})$ is not a Hilbert space such that $\mathcal{H}_k\subsetneq \ell_1(\mathbb{Z})$. 
% To this purpose, we introduce some definitions, which are extended from the causal case \cite{CVT06,PDCDL14}.  
% \begin{Definition}[\cite{CVT06, PDCDL14}]
% \end{Definition}


% \begin{Definition}[\cite{CVT06, PDCDL14}]
% Let $1 \leq p \leq \infty$  and $q =\frac{p}{p-1}$ with the convection $\frac{p}{p-1}=\infty$ if $p = 1$ and $\frac{p}{p-1}=1$ if $p=\infty$. Then, the kernel $k(t,s):\mathbb{Z}\times \mathbb{Z}\rightarrow\mathbb{R}$ is said to be q-bounded if 
% \begin{enumerate}
% \item[1)] the kernel section\footnote{Given a kernel $k(t,s)$, the kernel section $k_t\in \mathcal{H}_k$ centered at $t$ is defined as $k_t(s)= k(t,s), \forall s \in\mathbb{Z}$}  $k_t\in \ell_{p}(\mathbb{Z})$ for almost all $t\in \mathbb{Z}$ except on a set of null Lebesgue measure. 
% \item[2)] the function $\sum_{a\in \mathbb{Z}} k(x,a)f(a) \in \ell_p(\mathbb{Z})$ for all $f(a)\in \ell_p(\mathbb{Z})$. 
% \end{enumerate}
% \end{Definition}
% The following result associates the $q$-boundedness  of a kernel with the stability of a kernel. 
% \begin{Proposition}[\cite{CVT06, PDCDL14}]\label{th:qbonded_stable}
% Let $\mathcal{H}_k$ be the RHKS associated with the kernel $k(t,s):\mathbb{Z}\times \mathbb{Z}\rightarrow\mathbb{R}$. Given $1\leq p\leq \infty$, the following conditions are equivalent: 
% \begin{enumerate}
% \item[1)] $\mathcal{H}_k\subset \ell_p(\mathbb{Z})$;
% \item[2)] kernel $k(t,s)$ is q-bounded with $q=\frac{p}{p-1}$.
% \end{enumerate}
% \end{Proposition}
% Similar to the statements in \cite{CVT06, PDCDL14},  the sufficient and necessary conditions for a non-causal kernel to be stable are given as follows.
% \begin{Corollary}\label{co:stability_necessary and sufficient}
% A positive semidefinite kernel $k(t,s):\mathbb{Z}\times \mathbb{Z}\rightarrow \mathbb{R}$ is stable if and only if 
% \begin{align}\label{eq:gu:stable_suff_necess}
% \sum\limits_{s=-\infty}^\infty\left|\sum\limits_{t=-\infty}^\infty k(t,s)l(t)\right|< \infty, \quad \forall\ l(t)\in \ell_{\infty}(\mathbb{Z}).
% \end{align}
% \end{Corollary}



 
% { \color{purple}(Review 13 asks to remove \textbf{Section: Guidelines} into the beginning of Section \ref{se:ncsi_kernel}. This is a large modification about structure.)}


\section{Non-Causal Kernel Design}\label{se:ncsi_kernel}

The idea is to extend the results for causal kernels to non-causal kernels. 

\subsection{Guidelines for Non-Causal Kernel Design}\label{se:guideline}
% We first introduce the guidelines and then the prior knowledge for non-causal kernel design by extending the ones for causal kernels \cite{CHEN18}.
% \subsection{Guidelines for Non-Causal Kernel Design}
Following this idea, we first derive the optimal kernel in the non-causal context and then introduce the guidelines for non-causal kernel design.
\begin{Proposition}[Optimal Kernel, \cite{COL12,PDCDL14}]
Let
$\bar {g}^0$ and $\bar{\hat{g}}$ denote any finite dimensional vector obtained by sampling $g^0(t)$ and its estimate $\hat g(t)$ at the same but arbitrary sampling time instants in  $\mathbb Z$. 
The optimal kernel is defined as 
\begin{align}\label{eq:kernel_opt}
    k^{\text{opt}}(t,s)= g^0(t)g^0(s),\quad t,s\in \mathbb{Z},
\end{align}
which minimizes the MSE matrix 
\begin{align}
MSE(k(t,s))= \mathbb{E}[(\bar{\hat{g}}-\bar {g}^0)(\bar{\hat{g}}-\bar {g}^0)^T],
\end{align} 
in the sense that $MSE(k(t,s))-MSE(k^{\text{opt}}(t,s))$ is positive semidefinite for any positive semidefinite kernel $k(t,s)$, where $\mathbb{E}$ denotes the mathematical expectation.
\end{Proposition}

Similar to the causal case, we have the following guidelines for non-causal kernel design: first, let the kernel mimic the behavior of \eqref{eq:kernel_opt}, and moreover,  the prior knowledge of ${g}^0$ should be used in the kernel design \cite{COL12,CHEN18}; second,
let the kernel have some special structure that can ease the computation of KRM \cite{CALCP14,CA21}. 
% Since $\tilde g_0$ is unknown, the optimal kernel cannot be applied in practice, but if there is some prior knowledge about $\tilde g_0$, or equivalently, $G^{-1}_0(q)$, then that would be reflected in the structure of the optimal kernel, e.g., the main diagonals, which motivates the \emph{first} guideline to design a kernel: let the kernel mimic the behavior of the optimal one, and moreover,  the prior knowledge of $\tilde{g}_0$ should be used in the kernel design \cite{COL12,CHEN18}. 
% As shown in \cite{CHEN18}, even for the same identification problem, users may obtain different kinds of prior knowledge from different perspectives, and develop different design methods accordingly. No matter what perspective the prior knowledge is obtained, or what kernel design method is chosen, one common concern for KRM in practice is whether or not the designed kernel can ease the computation of KRM, which motivates the \emph{second} guideline to design a kernel: let the kernel have some special structure such that this structure can ease the computation of KRM \cite{CALCP14,CA21}. 
% Following these guidelines, some design methods have been proposed and many kernels have been designed for causal impulse response estimation, e.g., \cite{CALCP14,CHEN18,ZC18,CA21}.
% \subsection{Stable Non-causal Kernels}
For non-causal systems, the most common prior knowledge is the BIBO stability, and besides,
% and correspondingly, the definition of stable kernels is extended to the non-causal case: let $\mathcal{H}_k$ be the RHKS associated with the kernel $k(t,s):\mathbb{Z}\times \mathbb{Z}\rightarrow\mathbb{R}$, and $\ell_1(\mathbb{Z})$ be the set of absolutely summable sequences of real numbers, indexed by $\mathbb{Z}$, and then $k(t,s)$ is said to be stable if $\mathcal{H}_k\subset \ell_1(\mathbb{Z})$, see e.g., \cite{CVT06, PDCDL14}. 
some other common prior knowledge will be introduced later in Section \ref{se:embedmoreprior}. 




\subsection{Non-Causal Simulation-Induced Kernel}\label{se:ncsi_kerenl}
Following the guidelines mentioned above, we propose to design kernels for non-causal impulse responses from a system theory perspective. As will be seen shortly, although the same idea has been used in \cite{CHEN18} to design kernels for causal impulse responses, some challenges arise due to the non-causal nature of the problem.


 % differences due to that $G_0(q)$ is non-minimum phase, which brings new challenges to the existing techniques. 




Before proceeding to the details, it is worth sketching the idea of our kernel design method first. Following the first guideline for kernel design, i.e., to mimic the optimal kernel \eqref{eq:kernel_opt}, it is natural to embed the prior knowledge of $G(q)$ from a system theoretic perspective, e.g., properness, stability, dominant dynamics, into a stochastic process $ g(t), t\in \mathbb{Z}$, and then design the kernel as follows 
\begin{align}\label{eq:cov_gtgs}
k(t,s)= \cov{ g(t),g(s)},\ t,s\in \mathbb{Z},
\end{align}
where $\cov{\cdot,\cdot}$ denotes the covariance between two random variables. 
Following this idea, the question now becomes ``how to embed the prior knowledge of $G(q)$ into a stochastic process $ g(t)$"?  
% Now that $ g(t)$ is probabilistic, it is natural to involve the uncertainty for randomness. 


One way is to  employ the multiplicative uncertainty framework in robust control, e.g., \cite{ZDG96}, as follows:  
\begin{align}\label{eq:decomp_ft}
G(q)= G_0(q)(1+G_\Delta(q))
\end{align}
where $G_0(q)$ and $G_\Delta(q)$ are called the nominal model and uncertainty, respectively.  For the nominal model $G_0(q)$, we often postulate a classical parameterization $G_0(q,\theta)$ in terms of $\theta$ to embed the prior knowledge of $G(q)$. For example, if the prior knowledge is that $G(q)$ is proper, BIBO stable and its dominant dynamics is over-damped, then we can simply choose $G_0(q,\theta)$ to be a first order system and constrain the values of $\theta$ such that the pole of $G_0(q,\theta)$ is within the unit circle. For the uncertainty $G_\Delta(q)$, we have to suppose as little prior knowledge as possible, and for stable system identification, the only prior knowledge imposed on $G_\Delta(q)$ should be the stability, and in this case, we should design a zero mean Gaussian process to embed such prior knowledge and model the impulse response of $G_\Delta(q)$, then the task of kernel design would be done.  To be more specific, we draw the block diagram of \eqref{eq:decomp_ft} in Fig. \ref{fig:blk_multi_uncertain},  where $\delta(t)$ is a unit impulsive input signal, $\bar u(t),  g(t)$ are the input and output of $G_0(q)$, respectively. Then clearly, if it is possible to construct a zero mean Gaussian process to model the impulse response $h(t)$ of $G_\Delta(q)$ in Fig. \ref{fig:blk_multi_uncertain}, then $\bar u(t)$ and in turn $ g(t)$ would be both Gaussian processes. In this way, we finish the design of the  kernel \eqref{eq:cov_gtgs}, and call \eqref{eq:cov_gtgs} the non-causal simulation-induced (NCSI) kernel.
% Figure environment removed



\subsubsection{The Nominal Model}\label{se:nominal_model}

We will show how to postulate a classical parameterization $G_0(q,\theta)$ of $G_0(q)$ in terms of $\theta$ to embed the prior knowledge of $G(q)$, e.g., properness, stability, dominant dynamics.   % The prior knowledge, e.g., properness, stability, dominant dynamics, should be embedded in $G_0(q)$, or equivalently, in its state-space model. 
In this section, we will only consider the most fundamental prior knowledge that $G(q)$ is proper and BIBO stable, and later in Section \ref{se:embedmoreprior}, we will consider some more specific prior knowledge.  

As well known from e.g., \cite{Verhaegen96,ZO18}, 
proper and BIBO stable $G_0(q)$ has the following state-space model realization:
% As shown in e.g., \cite{ZO18}, under the assumption that $M_0(q)^{-1}$ is bi-proper\footnote{Let $G(q)$ be a rational transfer function described by  $G(q)=B(q)/A(q)$ with $A(q)= a_{n_a}q^{n_a}+a_{n_a-1}q^{n_a-1}+\cdots +a_1q+1$ and $B(q)= b_{n_b}q^{n_b}+b_{n_b-1}q^{n_b-1}+\cdots +b_0 $, where $a_{n_a}, b_{n_b}\neq 0$.  Then $G(q)$ is said to be improper, bi-proper, and strictly proper if the relative degree, $r=n_a-n_b$, is negative, zero, and positive, respectively.}, non-minimum phase and has no zeros on the unit circle, the state-space model realization of $M_0(q)$ exists and takes the following form
% {\color{blue}(Remark: Not all non-causal models  can be represented by the state-space model, and it needs to be proper and rational .) }
\begin{subequations}\label{eq:si_nominal_model}
\begin{align}
&x_c(t+1)= A_cx_c(t)+ B_c\bar u(t),\label{eq:si_nominal_model1}\\
G_0(q, \theta):\quad&x_a(t+1)=  A_a x_a(t)+ B_a\bar u(t),\label{eq:si_nominal_model2}\\
 & g(t)= C_cx_c(t)+ C_ax_a(t)+D\bar u(t),\label{eq:si_nominal_model3}
% &x_c(-\infty)=0, \quad  x_a(\infty)=0, \label{eq:bound_inf}
\end{align}
\end{subequations}
where $A_c, B_c, C_c,  A_a, B_a, C_a, D$ have compatible dimensions, $t\in\mathbb Z$ is the time index, $x_c(t)$ and $x_a(t)$ are the states for the causal part and anti-causal part, respectively, such that all stable poles are contained in $A_c$ and all unstable poles are contained in $A_a$. Let  $\theta=\{A_c, B_c, C_c,  A_a, B_a, C_a, D\}$. Then the state-space model \eqref{eq:si_nominal_model} is a classical parameterization $G_0(q,\theta)$ of $G_0(q)$.




\subsubsection{The Uncertainty}\label{se:uncertainty}

% The prior knowledge is useful but rigid. Since no one can guarantee the exactness and correctness of the prior knowledge, the uncertainty $M_\Delta(q)$ is introduced. 

Noting that for stable system identification, the only prior knowledge imposed on $G_\Delta(q)$ should be its stability, we will show how to design a zero mean Gaussian process (or equivalently a kernel) to embed such prior knowledge and model the impulse response $h(t)$ of $G_\Delta(q)$. 
% For the uncertainty $G_\Delta(q)$, we have to suppose as little prior knowledge as possible, and for stable system identification, the only prior knowledge imposed on $G_\Delta(q)$ should be the stability, and in this case, we should design a zero mean Gaussian process (or equivalently a kernel) to embed such prior knowledge and model the impulse response $h(t)$ of $G_\Delta(q)$. 
To this goal, since there is no prior knowledge about the smoothness of $h(t)$, it is natural to model $h(t)$ by the following Gaussian process:
\begin{align}\label{eq:un_sibt}
G_\Delta(q): \quad h(t)= b(t)w(t), \quad  t\in \mathbb{Z}, 
\end{align}
where $w(t)$ is a white Gaussian noise with zero mean and unit variance, and $\{b(t)\}_{t\in\mathbb Z}\in\ell_1(\mathbb Z)$. Clearly, $h(t)$ is a zero mean Gaussian process with a non-causal kernel (covariance function) $b(t)b(s)\delta_{t,s}$ with $t,s\in\mathbb Z$, where $\delta_{t,s}$ is the Kronecker delta function, and which is an extension of the causal kernel \cite[eq. (21)]{CHEN18}. 




% {\color{purple}
% \begin{Remark} (This may be optional: to make uncertainty different from the causal case)
% To ensure that the NCSI kernel \eqref{eq:si_expre} have the closed form, the specific parametrization of $b(t)$ is very limited with respect to $G_0(q,\theta)$. As the counter-example, if we consider the NCSI-FO kernel \eqref{eq:kernel_si_9} but with $b(t)=\sigma \lambda^{-t^2/2}$, then $K_1(t,s)$ in Appendix \ref{ap:ncsifo} becomes 
% \begin{align*}
% K_1(t,s)=\sigma_c^2 c_a^2a_a^{-t-s}\sum_{k=\max(1,t+1)}^{\infty} \lambda^{(k^2)} a_a^{2k}    
% \end{align*}
% which does not have the closed form even if the infinite summation above is convergent.  
% \end{Remark}
% }
\subsubsection{The NCSI Kernel}\label{se:combined_si_kernel}


Now we put \eqref{eq:si_nominal_model} and \eqref{eq:un_sibt} together and embed them in \eqref{eq:decomp_ft} to get the NCSI kernel \eqref{eq:cov_gtgs}. 




% go back to the nominal model \eqref{eq:si_nominal_model} and 

% It is worth to mention that skipping $\delta(t)$ in \eqref{eq:si_expre} does not influence the derivation of the NCSI kernel \eqref{eq:cov_gtgs}, because $\delta(t)$ is deterministic but not random.


\begin{Lemma}\label{le:ss_model} Consider the following state-space model
% let $\bar u(t) = l(t)+ \delta(t)$, 
\begin{subequations}\label{eq:si_ss}
\begin{align}
&x_c(t+1)= A_cx_c(t)+ B_cb(t)w(t),\label{eq:si_expre_causal}\\
&x_a(t+1)=  A_a x_a(t)+ B_a b(t) w(t),\label{eq:si_expre_acausal}\\
& g(t)= C_cx_c(t)+ C_ax_a(t)+  Db(t)w(t),\label{eq:si_expre_causal+acausal}\\
& t= -M_c,-M_c+1,\cdots,\ M_a, \text{for    } M_c,M_a\in\mathbb N,
% &\quad x_c(-\infty)=0, \quad  x_a(\infty)=0,\label{eq:si_expre_acausal_bd}\\
\end{align}
\end{subequations}
% We first let $M_c,M_a\in\mathbb N$ be finite and consider the solution of the state-space model \eqref{eq:si_nominal_model} over the interval $[-M_c,\ M_a]$. In this case, 
% $t=-M_c$ and $t=M_a$ can be regarded as the starting time for the causal part and anti-causal part, respectively, and moreover, we let $x(-M_c)$ and $x(M_a)$ denote the initial condition for the causal part and anti-causal part, respectively.
where $-M_c$ and $M_a$ are the starting time for the causal part and anti-causal part, respectively. Assume that the initial states $x_c(-M_c)$ for the causal part and $x_a(M_a)$ for the anti-causal part  are such that $[x_c(-M_c)^T, x_a(M_a)^T]^T$ has zero mean and bounded covariance matrix.  Then for any finite number of sampling time instants $t_1,t_2,\cdots,t_{m}\in \mathbb{Z}$, 
% let $Z=[g(t_1),\cdots, g(t_m)]^T$ and then 
$\lim_{M_c\to\infty}\lim_{M_a\to\infty} [g(t_1),\cdots, g(t_m)]^T$ converges in distribution to a Gaussian random vector with mean zero and covariance matrix, whose $(t,s)$th element is defined through
\begin{subequations}\label{eq:si_expre}
\begin{align}
k^{\text{NCSI}}(t,s)&=\sum\limits_{k=-\infty}^\infty b^2(k) g_0(t-k)  g_0(s-k),\\
% \cov{ \lim_{M_c\to\infty}\lim_{M_a\to\infty}g(t), \lim_{M_c\to\infty}\lim_{M_a\to\infty}g(s)}\label{eq:si_expre_acausal5}\\
& t,s\in \{t_1, t_2, \cdots, t_{m}\},\nonumber
\end{align} 
where $\{g_0(t)\}_{t\in\mathbb Z}$ denotes the non-causal impulse response of the nominal model \eqref{eq:si_nominal_model},i.e.,   
\begin{align}
&g_0(t) =\left\{\begin{array}{ll}
     C_cA_c^{t-1}B_c,& \quad t\geq 1 \\
    D-C_a  A^{-1}_aB_a,    &\quad t=0 \\
       -C_a A_a^{t-1} B_a,    &\quad t\leq -1.
     \end{array}
     \right.,\label{eq:noncausal_g0}
     \end{align}
     \end{subequations}
      % then the NSCI kernel \eqref{eq:si_expre_acausal5} can be rewritten as follows 
% \begin{align}
% &k^{\text{NCSI}}(t,s) = \lim_{M_c\to\infty}\lim_{M_a\to\infty} \cov{ g(t), g(s)},\\
% &\qquad\qquad\  =\sum\limits_{k=-\infty}^\infty b^2(k) g_0(t-k)  g_0(s-k),\label{eq:si_expre}\\
% % &=\sum\limits_{k=-\infty}^\infty b^2(k) k_0^{\text{NCSI}}(t-k,s-k),\\
% & t,s\in \{t_1, t_2, \cdots, t_{m}\}.\nonumber
% \end{align}

% \begin{align}\label{eq:g0_ssmodel}
%     g_0(t) =\left\{\begin{array}{ll}
%     C_cA_c^{t-1}B_c,& \quad t\geq 1 \\
%    D-C_a  A^{-1}_aB_a,    &\quad t=0 \\
%       -C_a A_a^{t-1} B_a,    &\quad t\leq -1
%     \end{array},
%     \right.
% \end{align}



% as $M_c$ and $M_a$ go to infinity, for any $t\in\mathbb N$, $g(t)$ converges in distribution to $N(0,k^{\text{NCSI}}(t,t))$.

%  the NCSI kernel \eqref{eq:cov_gtgs} is given by
% \begin{align}
% % &\quad x_c(-\infty)=0, \quad  x_a(\infty)=0,\label{eq:si_expre_acausal_bd}\\
% &k^{\text{NCSI}}(t,s)= \cov{ g(t), g(s)},\label{eq:si_expre_acausal5}
% \end{align}






%  If $\{x_c(t)\}_{t\in\mathbb Z}$, $\{x_a(t)\}_{t\in\mathbb Z}$ and $\{\bar u(t)\}_{t\in\mathbb Z}$ are modelled as Gaussian processes with bounded variance over $t$, then the output $g(t)$ converges in distribution to 
% \begin{align}\label{eq:ss_model_uy} 
% &g(t)=  C_c\sum\limits_{k=-\infty}^{t-1} A_c^{t-1-k}B_c\bar u(k)\nonumber\\
% & +  C_a\sum\limits_{k=t}^{\infty} -A_a^{t-1-k}B_a\bar u(k)+D\bar u(t),\ t\in \mathbb{Z} .
% \end{align}
\end{Lemma}


 \begin{Remark}
 It is worth stressing that although the system theoretic perspective for the design of causal kernels proposed in \cite{CHEN18} is extended for that of non-causal ones
 here, due to the difference in the nature of causal and non-causal systems, the design of non-causal kernel has the combination of the following issues need to be addressed:

 \begin{itemize}
 \item the assumption on $M_c$ and $M_a$: shall they be treated
 as hyper-parameters or chosen to be such that $M_c =
 + \infty$ and $M_a = +\infty$?
 \item the assumption on $x_c(-M_c)$ and $x_a(M_a)$: shall they
 be assumed to be a Gaussian random variable with
 zero mean and a parameterized covariance matrix?
 and shall the mutual or partial independence be assumed on $x_c(-M_c)$, $x_a(M_a)$ and $w(t)$?
 \item the assumption on the uncertainty: the general
 guideline is that the assumption on the uncertainty
 should be made as little as possible (because it is
 uncertainty), but shall the uncertainty be assumed
 to be causal and stable, or anti-causal and stable,
 or non-causal and stable?
 \end{itemize}

 Based on some analysis and moreover, Monte Carlo simulations, we have solutions to the above issues, which are summarized in Lemma 4.1.
 \end{Remark}


\begin{Remark} 

Lemma \ref{le:ss_model} shows that as the starting time $M_c$ (actually $-M_c$) for the causal part and $M_a$ for the anti-causal part go to infinity,  then the initial states $x_c(-M_c)$ for the causal part and $x_a(M_a)$ for the anti-causal part will asymptotically have no influence on the NCSI kernel \eqref{eq:si_expre}. This is different from the causal case, where the initial state at starting time $t=0$ has influence on the SI kernel, e.g., \cite[Eq. (22)-(23)]{CHEN18}. 
On the other hand, \eqref{eq:si_expre} shows that the NCSI kernel can be interpreted as an infinite sum of rank-1 kernels, $g_0(t-k)  g_0(s-k)$, weighted by $b^2(k)$, see Fig. \ref{fig:stability}.

\end{Remark}




%  and then the NSCI kernel \eqref{eq:si_expre} can be rewritten as follows 
% \begin{subequations}\label{eq:si_expre}
% \begin{align}
% k^{\text{NCSI}}(t,s)&= \sum\limits_{k=-\infty}^\infty b^2(k) g_0(t-k)  g_0(s-k)\\
% &=\sum\limits_{k=-\infty}^\infty b^2(k) k_0^{\text{NCSI}}(t-k,s-k),
% \end{align}
% \end{subequations} where $k_0^{\text{NCSI}}(t,s)= g_0(t) g_0(s)$ is a rank-1 kernel. Therefore, 






% \begin{Remark}
% In \eqref{eq:si_expre}, the NCSI kernel \eqref{eq:si_expre} is equivalently represented by using the impulse response $g_0$ of the nominal model $G_0(q)$. Here, $g_0$ is obtained using  the state-space model realization of the nominal model $G_0(q)$, but this is not necessary and actually, any parameterized form of $\{g_0(k)\}_{k\in \mathbb{Z}}\in \ell_1(\mathbb{Z})$ works here. In addition, it is worth to mention that the impulse response \eqref{eq:g0_ssmodel} of $G_0(q)$ has been given in \cite{AB17}.
% % , e.g., \eqref{eq:g0_ssmodel_new}.
% % , and then the NCSI kernel \eqref{eq:si_expre} is rewritten as
% % \begin{align}
% % k^{\text{NCSI}}(t,s)&= \sum\limits_{k=-\infty}^\infty b^2(k) g_\delta(t-k)  g_\delta(s-k)\\
% % &=\sum\limits_{k=-\infty}^\infty b^2(k) k_\delta^{\text{NCSI}}(t-k,s-k)\label{eq:si_expre},
% % \end{align}

% \end{Remark}\label{re:general_si}
% Figure environment removed


% \begin{Remark} 
% The bi-proper assumption on $M_0^{-1}(q)$ in Lemma \ref{le:ss_model} can be relaxed. If $M_0^{-1}(q)$ is strictly proper, then a state-space model realization of $M_0(q)$ still exists.  Assume that the relative degree of $M_0(q)$, denoted by $r$, is negative, and then $M_0(q)$ can be divided into two unique parts by the polynomial's division: 
% \begin{align*}
% M_0(q)=M_\infty(q) + M_{r=0}(q) ,
% \end{align*}
% where $M_\infty(q)$ is a finite polynomial without $0$th order, and $M_{r=0}(q)$ is a bi-proper transfer function. Note that $M_\infty(q)$ is additional in the state-space realization \eqref{eq:si_nominal_model}, then \eqref{eq:si_nominal_model3} is modified as follows
% \begin{align*}
% g(t)= C_cx_c(t)+ C_ax_a(t)+D\bar u(t)+ \sum\limits_{k=1}^{-r} D(k)\bar u(t+k),
% \end{align*} 
% where $D(k), k=1,\cdots, -r$ are the $k$th order's coefficient of the polynomial $M_\infty(q)$, and  \eqref {eq:g0_ssmodel} is modified as 
% \begin{align}\label{eq:g0_ssmodel_new}
% g_\delta(t) =\left\{\begin{array}{lll}
%     C_cA_c^{t-1}B_c,& \quad t\geq 1 \\
%        D-C_a  A^{-1}_aB_a,    &\quad t=0 \\
%        -C_a A_a^{t-1} B_a+ D(-t),    &\quad r \leq t\leq -1 \\
%        -C_a A_a^{t-1} B_a,    &\quad t<r
%     \end{array}
%     \right. .
% \end{align}
% Then \eqref{eq:g0_ssmodel_new} can be used to form the NCSI kernel \eqref{eq:si_expre}. However, the strictly proper assumption on $M_0^{-1}(q)$ would introduce  many additional hyper-parameters, $D(k), k=1, \cdots, -r$, resulting in more difficulties in hyper-parameter estimation problem. Therefore, strictly proper $M_0^{-1}(q)$ or equivalently improper $M_0(q)$, was suggested to be avoided by pre-processing the data as discussed in \cite[Remark 1]{ZO18} such that it is okay to assume bi-proper $M_0^{-1}(q)$ (also $M_0(q)$) in Lemma \ref{le:ss_model}. 
% \end{Remark}

\subsection{Stability of NCSI Kernel}\label{se:stability}
As can be seen in the following theorem,  the NCSI kernel \eqref{eq:si_expre} is stable in the sense that its associated RKHS $\mathcal{H}_k\subset \ell_1(\mathbb{Z})$, see \cite{CVT06,PDCDL14}, where $\ell_1(\mathbb{Z})$ is  set of absolutely summable sequences of real numbers, indexed by $\mathbb{Z}$.  

\begin{Theorem}\label{th:st}
If  $ \{g_0(t)\}_{t\in\mathbb{Z}}\in \ell_1(\mathbb{Z})$ and $\{b(t)\}_{t\in\mathbb{Z}} \in \ell_1(\mathbb{Z})$, then the NCSI kernel \eqref{eq:si_expre} is stable. 
\end{Theorem}

 \begin{Remark}\label{remark:improvement_stability_si_kernel}
  The technique used in the proof of Theorem \ref{th:st} can be used to prove the stability of the causal SI kernel proposed in \cite{CHEN18}, resulting that the sufficient condition \cite[eq. (29a)]{CHEN18} can be replaced by a weaker one $\{b(t)\}_{t\in\mathbb{N}} \in \ell_1(\mathbb{N})$.
 \end{Remark}

\subsection{Semiseparability of NCSI Kernel}\label{se:semiseparability}
It was shown in \cite{CA21} that, if the kernel for regularized impulse response estimation has some special rank structures, e.g., semiseparability, \cite{VVM08a}, the computational complexity of KRM can be reduced. For example, the widely used DC and TC kernels \eqref{eq:causal_kernel} are both semiseparable, and the simulation-induced kernel for causal impulse response estimation is also semiseparable \cite{CA21}. Interestingly, we will show below that, the NCSI kernel \eqref{eq:si_expre} also has some special rank structure and in fact, it is semiseparable plus diagonal.

\begin{Definition}\label{de:semise}
A positive semidefinite kernel $k(t,s):\mathcal{X}\times\mathcal{X}\rightarrow \mathbb{R}$ is said to be extended $p$-semiseparable plus diagonal, if there exist $\mu_i, \nu_i: \mathbb{Z}\rightarrow \mathbb{R}$, $i=1, \cdots, p$, such that 
\begin{align}\label{eq:de_semise}
k(t,s)=\left\{
\begin{array}{lll}
\sum_{i=1}^p \mu_i(t)\nu_i(s),& \quad t> s\\
d(t),& \quad t= s\\
\sum_{i=1}^p \nu_i(t)\mu_i(s),& \quad t< s
\end{array}
\right.
\end{align} 
where $\mathcal{X}$ represents either $\mathbb{N}$ or $\mathbb{Z}$, $p\in \mathbb{N}$, and $d(t)\in \mathbb{R}$ with $t\in \mathcal{X}$. Moreover, if $d(t)$ further satisfies
\begin{align}
d(t)= \sum_{i=1}^p \mu_i(t)\nu_i(t),
\end{align}
then $k(t,s)$ is said to be extended $p$-semiseparable. 
\end{Definition}
Then we show that the NCSI kernel \eqref{eq:si_expre} is extended $p$-semiseparable plus diagonal. 
\begin{Theorem}\label{th:se} Consider the NCSI kernel \eqref{eq:si_expre} and assume that $A_c$ is non-singular. Then 
the NCSI kernel \eqref{eq:si_expre} is extended $\bar p$-semiseparable  plus diagonal with $\bar p \in \mathbb{N}$ and $\bar p \leq p$, where $p$ is the dimension of $A_c$ in  \eqref{eq:si_expre_causal} plus the dimension of $A_a$ in  \eqref{eq:si_expre_acausal}.
 \end{Theorem}
\begin{Corollary}\label{co:se}  Consider the NCSI kernel \eqref{eq:si_expre} and assume that $A_c$ is non-singular. Then
the NCSI kernel \eqref{eq:si_expre} is extended $\bar p$-semiseparable, if 
 \begin{align*}
 D=0,\quad \text{ or }\quad D-C_aA_a^{-1}B_a =C_cA_c^{-1}B_c
\end{align*}
 \end{Corollary}
 
  \begin{Remark}
One may wonder what would happen for the case when $A_c$ is singular. In this case, the NCSI kernel \eqref{eq:si_expre} is not semiseparable any more, but its corresponding kernel matrix may still have some rank-structure properties. Since this case is less interesting, the corresponding details will not be given here. 
% If $A_c$ is singular, there are no generators \eqref{eq:de_semise} for the NCSI kernel \eqref{eq:si_expre}, e.g., when $g_0(t)=(t)+ \delta(t-3)$ and $b(t)= exp(-|t|)$. In fact, a better description for NCSI kernel \eqref{eq:si_expre} is an extended $p$-generator representable semiseparable plus banded kernel, and Corollary \ref{co:se2} still holds. 
\end{Remark}  

% {\color{blue} 
% After the discussion on the semiseparability of $k(t,s)$, we consider the output kernel, e.g. \cite{PDCDL14}, defined by, 
% \begin{align}\label{eq:def_outputkernel}
% O(t, s)= \sum\limits_{x=-\infty}^\infty u(t-x)\sum\limits_{a=-\infty}^\infty u(s-a)k(x,a),
% \end{align}
% whose semiseparability finally decides the computational complexity of KRM. Then we consider the output kernel of the NCSI kernel \eqref{eq:si_expre}. 
% \begin{Lemma}\label{le:outputkernel}
% The output kernel of the NCSI kernel \eqref{eq:si_expre} also takes the form of the NCSI kernel, i.e., 
% \begin{align}
% O^{\text{NCSI}}(t, s)&=\sum\limits_{k=-\infty}^\infty b^2(k) \tilde g_0(t-k)  \tilde g_0(s-k),\label{eq:si_output_kernel}\\
% \quad \tilde{g}(t)&= \sum\limits_{k=-\infty}^\infty u(t-k)g_0(k),\nonumber
% \end{align}
% where $u(t)$ is the input, $b(t)$ and $g_0(t)$ are taken from the corresponding NCSI kernel \eqref{eq:si_expre}. 
% \end{Lemma}
% \begin{Theorem}\label{th:se2} Consider the output kernel  \eqref{eq:si_output_kernel} and assume that $\tilde g(t)$ is finite for all $t$. Let $\tilde G(q)$ denote the transfer function of $\tilde g(t)$ by the two-sided Z transform. Then we have that
% \begin{itemize}
% \item the output kernel \eqref{eq:si_output_kernel} is extended $\bar p$-semiseparable  plus diagonal if $\tilde G(q)$ is a proper transfer function with $\bar p$ poles non-singular and no poles on the unit circle;
% \item the output kernel \eqref{eq:si_output_kernel} is extended $\bar p$-semiseparable  if $\tilde G(q)$ is a strictly proper transfer function with $\bar p$ poles non-singular and no poles on the unit circle;
% \end{itemize}
% \end{Theorem}
% }


% {\color{blue}To be specific, we first show some definitions of higher order semiseparable structures for kernels. 
% \begin{Definition}[\cite{VVM08a, CA21}]\label{de:semise}
% A positive semi-definite kernel $k(t,s):\mathcal{X}\times\mathcal{X}\rightarrow \mathbb{R}$, with $\mathcal{X}$ being either $\mathbb{Z}$ or $\mathbb{N}$,  is said to be 
% \begin{itemize}
% \item an extended $p$-semiseparable kernel, with $p\geq 0$, if it satisfies 
% \begin{align*}
% &rank(K_l)\leq p\quad  \text{for any}\quad K_l \in M_k, \\
% &M_k=\{ k(t_1:t_2,s_1:s_2)|t_1\geq s_2, t_1,t_2,s_1,s_2\in\mathcal{X}\}
% \end{align*} 
% \item an extended $p$-generator representable semiseparable kernel, with $p\geq 0$, if there exist $\mu_i, \nu_i: \mathcal{X}\rightarrow \mathbb{R}$, $i=1, \cdots, p$, such that 
% \begin{align*}
% k(t,s)=\left\{
% \begin{array}{lll}
% \sum_{i=1}^p \mu_i(t)\nu_i(s),& \quad t\geq s\\
% \sum_{i=1}^p \nu_i(t)\mu_i(s),& \quad t< s.
% \end{array}
% \right.
% \end{align*} 
% \end{itemize}
% \end{Definition}
% \begin{Remark}
% Note that semiseparability and generator representable semiseparability are different \cite[Sec 1.2.2]{VVM08a}, and the class of extended $p$-generator representable semiseparable kernels is a subset of the class of extended $p$-semiseparable kernels.
% \end{Remark}
% Then we discuss such higher-order semiseparable structure for the NCSI kernel \eqref{eq:si_expre}.  
%  \begin{Theorem}\label{th:se}
% The NCSI kernel \eqref{eq:si_expre} with non-singular $A_c$  is an extended $p$-generator representable semiseparable plus diagonal kernel, i.e., there exist generators $\mu_i, \nu_i: \mathbb{Z}\rightarrow \mathbb{R}$, $i=1, \cdots, p$, such that 
% \begin{align}\label{eq:de_semise}
% k^{\text{NCSI}}(t,s)=\left\{
% \begin{array}{lll}
% \sum_{i=1}^p \mu_i(t)\nu_i(s),& \quad t> s\\
% d(t),& \quad t= s\\
% \sum_{i=1}^p \nu_i(t)\mu_i(s),& \quad t< s
% \end{array}
% \right.,
% \end{align} 
% where $d(t)\in \mathbb{R}$ for any $t\in \mathbb{Z}$, and $p$ is the dimension of $A_c$ plus the dimension of $A_a$.
%  \end{Theorem}
% \begin{Corollary}\label{co:se2}
% The NCSI kernel \eqref{eq:si_expre} with non-singular $A_c$ is an extended $(p+1)$-semiseparable kernel, where $p$ is the dimension of $A_c$ plus the dimension of $A_a$.
%  \end{Corollary}}
 
As shown in \cite{MC19}, there are many efficient algorithms on matrix computations for extended $p$-semiseparable plus diagonal kernels. We will illustrate this with a concrete example in the next section. 


\subsection{Embedding More Prior Knowledge}\label{se:embedmoreprior}
In this section, we try to embed some more specific prior knowledge of $G(q)$ into the NCSI kernel \eqref{eq:si_expre}. 



\subsubsection{Over-Damped Dominant Dynamics}
First, we assume that the  prior knowledge is that the dominant dynamics of $G(q)$ is over-damped  and the function $b(t)$ in the model of the uncertainty $G_\Delta(q)$ in \eqref{eq:un_sibt} is an exponential decay one. Therefore, the state-space model \eqref{eq:si_nominal_model} of $G_0(q)$ is parameterized as follows:
\begin{subequations}\label{eq:kernel_si_9}
\begin{align*}
&A_c= B_c=a_c,\quad A_a^{-1}=  B_a=a_a,\nonumber\\
&C_c=c_c,\quad  -C_aA_a^{-2}=c_a,\quad  D-C_a  A^{-1}_aB_a=c_0,\nonumber\\
&  -1< a_c, a_a< 1,\ -\infty <c_c,c_0, c_a<\infty,
\end{align*}
such that 
\begin{align}\label{eq:ncsi_fo1}
&  g_0(t) =\left\{\begin{array}{ll}
    c_ca_c^{t},& \quad t>0 \\
   c_0,    &\quad t=0 \\
      c_aa_a^{-t},    &\quad t<0
    \end{array}
    \right. ,
\end{align}
% Without further prior knowledge, it is natural to assume that the causal and anti-causal parts of $g_\delta(t)$ are dominated by a real stable pole $a_c\in \mathbb{R}$ and a real unstable pole $a_a\in \mathbb{R}$, respectively. Then $M_0(q)$ should be a second-order state-space model. 
% Secondly, we consider the parameterization of the Gaussian process model \eqref{eq:un_sibt} of the uncertainty $G_\Delta(q)$. Since the only prior knowledge is that $\{b(t)\}_{t\in \mathbb{Z}}\in\ell_1(\mathbb Z)$, the simplest choice is to let 
and moreover, $b(t)$  is parameterized as follows:
\begin{align}\label{eq:ncsi_fo2}
&b(t)=\left\{
\begin{array}{ll}
\sigma_c\lambda_c^{t/2},&\quad  t>0\\
\sigma_0,&\quad  t=0\\
\sigma_a\lambda_a^{-t/2},&\quad  t<0
\end{array}
\right.,\\
&0< \lambda_c,\lambda_a< 1 ,\  \sigma_c, \sigma_a\geq 0.\nonumber 
\end{align}
As a result, we obtain the following non-causal kernel
% where $\lambda_c^{1/2}, \lambda_a^{1/2}$ are preferred to than $\lambda_c, \lambda_a$ for the latter usage in \eqref{eq:kernel_si_dc}.
\begin{align}
&k^{\text{NCSI-FO}}(t,s;\eta)= \text{\eqref{eq:si_expre}} \text{ with } \eqref{eq:ncsi_fo1} \text{ and } \eqref{eq:ncsi_fo2},\\
&\eta= [a_c, a_a,c_c, c_0,c_a, \lambda_c,\lambda_a,\sigma_c, \sigma_a, \sigma_0].\nonumber 
\end{align}
\end{subequations}
Since the nominal model is a first order system for both the causal and anti-causal part, the kernel \eqref{eq:kernel_si_9} is called the first order NCSI (NCSI-FO) kernel.   The NCSI-FO kernel \eqref{eq:kernel_si_9} has a closed-form expression that can be found in Appendix \ref{ap:ncsifo}. As shown in Remark \ref{re:sigma_0isone}, one of the scaling hyper-parameters $c_c, c_0,c_a, \sigma_c,\sigma_0, \sigma_a$ is redundant and thus, $\sigma_0$ is fixed to be 1 and not treated as a hyper-parameter 
hereafter. 



\begin{Example}
It follows from Theorem \ref{th:se} that, the NCSI-FO kernel \eqref{eq:kernel_si_9} is an extended $2$-semiseparable plus diagonal kernel. Define the kernel matrix $K^{\text{NCSI-FO}}_{ij}= k^{\text{NCSI-FO}}(i,j;\eta_0)$, $ i,j=-n/2, \cdots, n/2$, with $n=1000,\cdots, 8000$. Then Fig. \ref{fig:semisep} shows the time for computing the Cholesky factor of $K^{\text{NCSI-FO}}$ by using chol in MATLAB (in blue) and by exploiting its high-order semiseparable structure with Algorithm 3 in \cite{MC19} (in red). 
 \end{Example}
% Figure environment removed
% \begin{Remark}\label{re:negative_off}
% % If the causal and anti-causal parts of $M_0(q)$ has the different signs, then 
% The NCSI-FO kernel \eqref{eq:kernel_si_9} can have negative correlation, which is different from the existing non-causal kernels \eqref{eq:nctc} and \eqref{eq:bdtc}, and can be seen from Fig. \ref{fig:surf_kernel}. 
% \end{Remark}
% Furthermore, we expect to trade off $M_0(q)$ against $M_\Delta(q)$: if the dominant dynamics is captured by the nominal model, then the uncertainty's dynamics would be negligible. If otherwise, the uncertainty's dynamics would be significant. To this goal, one way is to reduce the NCSI-FO kernel to the NCSI-DC kernel and NCSI-TC kernel.  



\subsubsection{Mirrored Poles}
   
Second, we further assume that there is an extra prior knowledge that the dominant dynamics of $G(q)$ has a pair of poles mirrored with respect to the unit circle. As mentioned in \cite[Sec. 4.3]{BO20}, mechanical structures may have such prior knowledge. In this case, it is natural to further let $a_c=a_a$ in the NCSI-FO kernel \eqref{eq:kernel_si_9}, leading to the following kernel
\begin{align}
&k^{\text{NCSI-FO}^\text{mp}}(t,s;\eta)= \eqref{eq:kernel_si_9} \text{ with } a_c=a_a ,\label{eq:kernel_si_9star}\\
&\eta= [a_c, c_c, c_0,c_a, \lambda_c,\lambda_a,\sigma_c, \sigma_a, \sigma_0].\nonumber 
\end{align}

% In Section \ref{se:simulation}, we will show that embedding such extra prior knowledge can help to get better model estimates.  }


% \subsection{Simplification of the Designed Kernel}


% Unfortunately, the NCSI-FO kernel \eqref{eq:kernel_si_9} still has 9 hyper-parameters, and thus the optimization in the hyper-parameter estimation might have local minima issues.
% To ease the hyper-parameter estimation, we can further simplify the NCSI-FO kernel \eqref{eq:kernel_si_9} by linking some hyper-parameters together. There are many ways to link the hyper-parameters. For example, we can link the decay rate of the nominal model with that of the uncertainty as follows:
%  % $\sigma_c$ to the ratio $a_c^2/\lambda_c$ and $\sigma_a$ to the ratio $a_a^2/\lambda_a$, respectively, in the following way:
% \begin{align}
% &k^{\text{NCSI-DC}}(t,s;\eta)=k^{\text{NCSI-FO}}(t,s;\eta) \text{ with: }\nonumber\\
% &\qquad a_c= \rho_c\lambda_c^{1/2},\ a_a= \rho_a\lambda_a^{1/2},\ -1\leq \rho_c, \rho_a\leq  1,\nonumber \\
% &\qquad \sigma_c= \sqrt{1-\rho_c^2},\ \sigma_a= \sqrt{1-\rho_a^2},\label{eq:kernel_si_dc}\\
% &\eta= [\lambda_c,\lambda_a,c_s, c_0,c_u,\rho_s,\rho_u].\nonumber 
% \end{align}
% This kernel \eqref{eq:kernel_si_dc} is called the NCSI-DC kernel, because it has a nice property similar to the DC kernel \eqref{eq:causal_dc_kernel}, i.e., 
% the kernel \eqref{eq:kernel_si_dc} reduces to
% \begin{itemize}
% \item[1)] a rank-1 kernel, $k(t,s)=g_0(t)g_0(s)$, if $\rho_c= \rho_a = 1$;
% \item[2)] a diagonal kernel, $k(t,s)=b(t)b(s)\delta(t-s)$, if $\rho_c= \rho_a = 0$;
% \end{itemize}
% These special cases show different degrees of balance between the nominal model $g_0(t)$ and the uncertainty $b(t)$ by varying $\rho_c$ and $\rho_a$. Moreover, similar to the relation betwen the TC kernel \eqref{eq:causal_tc_kernel} and DC kernel \eqref{eq:causal_dc_kernel}, we can obtain (by linking the hyper-parameters further) another special case of the NSCI-DC kernel \eqref{eq:kernel_si_dc} as follows 
% \begin{align}
% &k^{\text{NCSI-TC}}(t,s;\eta)=k^{\text{NCSI-DC}}(t,s;\eta) \text{ with: }\nonumber\\
% &\qquad \rho_c= \lambda_c^{1/2},\ \rho_a= \lambda_a^{1/2},\label{eq:kernel_si_tc}\\
% &\eta= [\lambda_c,\lambda_a,c_s, c_0,c_u].\nonumber
% \end{align}
% which is called the NCSI-TC kernel. It is worth noting that the same parametrizations like \eqref{eq:kernel_si_dc} and \eqref{eq:kernel_si_tc} have been used in \cite{CHEN18} to reduce the causal SI kernel to the DC kernel \eqref{eq:causal_dc_kernel} and TC kernel \eqref{eq:causal_tc_kernel}, respectively.



% \begin{Example}\label{re:ncsitc_not_tc}
% Despite having the same suffix 'TC', the NCSI-TC kernel \eqref{eq:kernel_si_tc} has the different structure from the NC-TC  kernel \eqref{eq:nctc} and NCBD-TC kernel \eqref{eq:bdtc}, which is illustrated in Fig. \ref{fig:surf_kernel}.
% \end{Example}
%   % Figure environment removed

% {\color{blue}
% \subsection{Embedding More Prior Knowledge(I divide the previous section into 4.5 and 4.6 )}\label{se:systematic design}

% The NCSI kernel \eqref{eq:si_expre} is a class of kernels that have embedded the prior knowledge of stability.  Furthermore, more prior knowledge can be embedded through the nominal model $g_0(t)$, and thereby we provide two examples in this section. 


% The first prior knowledge} is that the dominant dynamics of $G_0(q)$ has over-damped causal and anti-causal impulse response and thus the state-space model \eqref{eq:si_nominal_model} of $G_0(q)$ can be parameterized as follows:
% \begin{align*}
% &A_c= B_c=a_c,\quad A_a^{-1}=  B_a=a_a,\nonumber\\
% &C_c=c_c,\quad  -C_aA_a^{-2}=c_a,\quad  D-C_a  A^{-1}_aB_a=c_0,\nonumber\\
% &  -1< a_c, a_a< 1,\ -\infty <c_c,c_0, c_a<\infty,
% \end{align*}
% such that 
% \begin{align}\label{eq:ncsi_fo1}
% &  g_0(t) =\left\{\begin{array}{ll}
%     c_ca_c^{t},& \quad t>0 \\
%    c_0,    &\quad t=0 \\
%       c_aa_a^{-t},    &\quad t<0
%     \end{array}
%     \right. .
% \end{align}
% % Without further prior knowledge, it is natural to assume that the causal and anti-causal parts of $g_\delta(t)$ are dominated by a real stable pole $a_c\in \mathbb{R}$ and a real unstable pole $a_a\in \mathbb{R}$, respectively. Then $M_0(q)$ should be a second-order state-space model. 
% Then we consider the parameterization of the Gaussian process model \eqref{eq:un_sibt} of the uncertainty $G_\Delta(q)$. Since the only prior knowledge is that $\{b(t)\}_{t\in \mathbb{Z}}\in\ell_1(\mathbb Z)$, the simplest choice is to let $b(t)$ take the form of 
% \begin{align}\label{eq:ncsi_fo2}
% &b(t)=\left\{
% \begin{array}{ll}
% \sigma_c\lambda_c^{t/2},&\quad  t>0\\
% \sigma_0,&\quad  t=0\\
% \sigma_a\lambda_a^{-t/2},&\quad  t<0
% \end{array}
% \right.,\\
% &0< \lambda_c,\lambda_a< 1 ,\  \sigma_c, \sigma_a\geq 0.\nonumber 
% \end{align}
% As a result, we obtain a new non-causal kernel
% % where $\lambda_c^{1/2}, \lambda_a^{1/2}$ are preferred to than $\lambda_c, \lambda_a$ for the latter usage in \eqref{eq:kernel_si_dc}.
% \begin{align}
% &k^{\text{NCSI-FO}}(t,s;\eta)= \text{\eqref{eq:si_expre}} \text{ with } \eqref{eq:ncsi_fo1} \text{ and } \eqref{eq:ncsi_fo2},\label{eq:kernel_si_9}\\
% &\eta= [a_c, a_a,c_c, c_0,c_a, \lambda_c,\lambda_a,\sigma_c, \sigma_a, \sigma_0].\nonumber 
% \end{align}
% Since the nominal model is a first order system for both the causal and anti-causal part, the kernel \eqref{eq:kernel_si_9} is called the first order NCSI (NCSI-FO) kernel.   The NCSI-FO kernel \eqref{eq:kernel_si_9} has a closed-form expression that can be found in Appendix \ref{ap:ncsifo}. As shown in Remark \ref{re:sigma_0isone}, one of the scaling hyper-parameters $c_c, c_0,c_a, \sigma_c,\sigma_0, \sigma_a$ is redundant and thus, $\sigma_0$ is fixed to be 1 and not treated as a hyper-parameter 
% hereafter. 



% \begin{Example}
% It follows from Theorem \ref{th:se} that, the NCSI-FO kernel \eqref{eq:kernel_si_9} is an extended $2$-semiseparable plus diagonal kernel. Define the kernel matrix $K^{\text{NCSI-FO}}_{ij}= k^{\text{NCSI-FO}}(i,j;\eta_0)$, $ i,j=-n/2, \cdots, n/2$, with $n=1000,\cdots, 8000$. Then Fig. \ref{fig:semisep} shows the time for computing the Cholesky factor of $K^{\text{NCSI-FO}}$ by using chol in MATLAB (in blue) and by exploiting its high-order semiseparable structure with Algorithm 3 in \cite{MC19} (in red). 
%  \end{Example}
% % Figure environment removed

% {\color{blue}  
   
% The second prior knowledge comes from the remaining question in \cite[Sec. 4.3]{BO20}: mechanical structures may have pairs of poles are mirrored on the unit circle, and we call such systems the mirrored system. 

% Given the prior knowledge that $G_0(q)$ is a mirrored system, it is natural to let $A_c$ equal $A_a$ for the NCSI kernel \eqref{eq:si_expre}, and this leads to the following kernel. 
% \begin{align}
% &k^{\text{NCSI-FO*}}(t,s;\eta)= \eqref{eq:kernel_si_9} \text{ with } a_c=a_a ,\label{eq:kernel_si_9star}\\
% &\eta= [a_c, c_c, c_0,c_a, \lambda_c,\lambda_a,\sigma_c, \sigma_a, \sigma_0].\nonumber 
% \end{align}
% We will show in the simulation that the additional prior knowledge improve the estimate.  
% }


\subsection{Kernel Simplification }\label{sec:simplification}
Since the NCSI-FO kernel \eqref{eq:kernel_si_9} has 9 hyper-parameters, its hyper-parameter estimation may have some difficulties, e.g., its initialization is harder, its local minima issues may be more severe, and it may take more time to find the optimal solution. Hence, following the second guideline for kernel design, i.e., to ease the computation of KRM, we simplify the NCSI-FO kernel \eqref{eq:kernel_si_9} below  by linking some hyper-parameters together. 


There are many ways to link the hyper-parameters. 
First, we can link the decay rate of the nominal model with that of the uncertainty as follows:
 % $\sigma_c$ to the ratio $a_c^2/\lambda_c$ and $\sigma_a$ to the ratio $a_a^2/\lambda_a$, respectively, in the following way:
\begin{align}
&k^{\text{NCSI-DC}}(t,s;\eta)=k^{\text{NCSI-FO}}(t,s;\eta) \text{ with: }\nonumber\\
&\qquad a_c= \rho_c\lambda_c^{1/2},\ a_a= \rho_a\lambda_a^{1/2},\ -1\leq \rho_c, \rho_a\leq  1,\nonumber \\
&\qquad \sigma_c= \sqrt{1-\rho_c^2},\ \sigma_a= \sqrt{1-\rho_a^2},\label{eq:kernel_si_dc}\\
&\eta= [\lambda_c,\lambda_a,c_c, c_0,c_a,\rho_c,\rho_a].\nonumber 
\end{align}
The kernel \eqref{eq:kernel_si_dc} is called the NCSI-DC kernel, because it has a nice property similar to the DC kernel \eqref{eq:causal_dc_kernel}, that is, 
the kernel \eqref{eq:kernel_si_dc} reduces to
 a rank-1 kernel, i.e., $k(t,s)=g_0(t)g_0(s)$, for $\rho_c= \rho_a = 1$, and a diagonal kernel, i.e., $k(t,s)=b(t)b(s)\delta_{t,s}$, for $\rho_c= \rho_a = 0$, which shows different degrees of balance between the nominal model $G_0(q)$ (described by $g_0(t)$) and the uncertainty $G_\Delta(q)$ (described by $b(t)$) by varying $\rho_c$ and $\rho_a$. Moreover, similar to the relation between the TC kernel \eqref{eq:causal_tc_kernel} and DC kernel \eqref{eq:causal_dc_kernel}, we can obtain another special case of the NSCI-DC kernel \eqref{eq:kernel_si_dc} as follows 
\begin{align}
&k^{\text{NCSI-TC}}(t,s;\eta)=k^{\text{NCSI-DC}}(t,s;\eta) \text{ with: }\nonumber\\
&\qquad \rho_c= \lambda_c^{1/2},\ \rho_a= \lambda_a^{1/2},\label{eq:kernel_si_tc}\\
&\eta= [\lambda_c,\lambda_a,c_c, c_0,c_a].\nonumber
\end{align}
which is called the NCSI-TC kernel. 

% It is worth noting that the same parametrizations like \eqref{eq:kernel_si_dc} and \eqref{eq:kernel_si_tc} have been used in \cite{CHEN18} to reduce the causal SI kernel to the DC kernel \eqref{eq:causal_dc_kernel} and TC kernel \eqref{eq:causal_tc_kernel}, respectively.


\begin{Remark}\label{remark:mirrored_poles}
The $\text{NCSI-FO}^\text{mp}$ kernel \eqref{eq:kernel_si_9star} can also be seen as being obtained from the NCSI-FO kernel \eqref{eq:kernel_si_9} by hyper-parameter simplification, i.e., by letting $a_c=a_a$. The difference between the $\text{NCSI-FO}^\text{mp}$ kernel \eqref{eq:kernel_si_9star} and the NSCI-DC kernel \eqref{eq:kernel_si_dc} (also the NSCI-TC kernel \eqref{eq:kernel_si_tc}) is that the extra prior knowledge embedded in the former due to the hyper-parameter simplification $a_c=a_a$ has a clear physical interpretation, i.e., a pair of poles mirrored with respect to the unit circle, but not in the latter.

\end{Remark}


\begin{Remark}\label{re:ncsitc_not_tc}
Despite having the same suffix 'TC', as illustrated in Fig. \ref{fig:surf_kernel}, the NCSI-TC kernel \eqref{eq:kernel_si_tc} has very different behavior from the NC-TC kernel \eqref{eq:nctc} and its block diagonal variant  in \cite{BO20}, 
\begin{align}
     &k^{\text{NCBD-TC}}(t,s;\eta)= \left\{ \begin{array}{ll}
     c\min\{\lambda_c^t, \lambda_c^s\}, &  \text{if } t\geq 0,\ s\geq 0\\
     c\min\{\lambda_a^{-t}, \lambda_a^{-s}\}, &  \text{if } t<0,\ s<0\\
     0, &  \text{otherwise}.
     \end{array}\right.,\label{eq:bdtc}\\
     &\eta= [c, \lambda_c,\lambda_a],\nonumber
\end{align} which is called the NCBD-TC kernel below. Similar to the construction of  the NCBD-TC kernel, we can also construct the NCBD-DC kernel below
 \begin{align}\label{eq:ncbd_dc}
&k^{\text{NCBD-DC}}(t,s;\eta)= 
\left\{ \begin{array}{ll}
c_c^2\lambda_c^{(t+s)/2}\rho_c^{|t-s|}, &  \text{if } t\geq 1, s\geq 1\\
c_0^2, &  \text{if } t= 0, s=0\\
c_a^2\lambda_a^{-(t+s)/2}\rho_a^{|t-s|}, &  \text{if } t\leq -1, s\leq -1\\
0, &  \text{otherwise}
\end{array}\right.,\\
&\eta=[\lambda_c, \lambda_a, c_c, c_0, c_a, \rho_c, \rho_a].\nonumber 
\end{align}

\end{Remark}
  % Figure environment removed
 
\section{Numerical Simulation}\label{se:simulation}
In this section, we run simulations to test the kernels proposed in Sections \ref{se:embedmoreprior} and \ref{sec:simplification}, and the ones in \cite{BO20}. 




% Moreover, we generate the data in a similar way as \cite{BO20} and in particular, the data $\{u(t), y(t)\}_{t=1}^N$ are generated by $P(q)$ with $y(t)$ being the noiseless input and $u(t)$ being the noisy output.    



% such that 
% \begin{itemize}
% \item $G^0(q)$ is the inverse of the non-minimum phase system, denoted by $P(q)$; 
% \item The data sets $\{u(t), y(t)\}_{t=1}^N$ are generated by $P(q)$ with $y(t)$ being the noiseless input and $u(t)$ being the noisy output.    
% \end{itemize}


\subsection{Test Systems and Data-Bank}\label{sec:test_systems_and_data}
As mentioned in the last of Section \ref{se:non-causal_sysid}, system \eqref{eq:pro_sys1} often arises from the inverse model control, e.g., \cite{BO20}. More specifically, if the control plant in the feedforward control is denoted by $P(q)$ and assumed to be unknown, stable, non-minimum phase and has no zeros on the unit circle, then its non-causal inverse $G(q)$ can be put in the form of \eqref{eq:pro_sys1}, e.g., \cite[Theorem 1]{BO20}. Moreover, it should be noted that the input $u(t)$ and output $y(t)$ of $G(q)$ correspond to the output and input of $P(q)$, respectively.  

In what follows, we will generate four data banks D1-D4, associated with different types of test systems. We first introduce four types of test systems $P(q)$, then generate the input-output data for each test system $P(q)$, and finally the input-output data for the non-causal inverse of $P(q)$, i.e., $G(q)$ in \eqref{eq:pro_sys1}. 


First, we generate the test systems in D1-D4. 
\begin{itemize}
\item Data-bank D1 contains a single non-minimum phase system studied in  \cite{BO20}
\begin{equation}\label{eq:bosys}
\begin{array}{cc}
P(q)= \frac{1.550(q^2-2.035q+1.052)(q^2-1.844q+0.9391)}{q^2(q-0.9514)(q-0.9511)},
\end{array}
\end{equation}
with two stable zeros at $0.922\pm 0.298i$ and two unstable zeros at $1.018\pm 0.126i$. 
\item Data-bank D2 contains 1000 randomly generated systems $P(q)$. We generate each system as follows. First, a 30th order continuous-time system is generated using function \texttt{m=rss(30)} in MATLAB. Then system \texttt{m} is sampled at 100 times of its bandwidth by function \texttt{md=c2d(m,2pi/(100$\cdot$bandwidth(m)))} in MATLAB. Finally, if the system \texttt{md} satisfies the following two conditions:
\begin{enumerate}
\item[(a)] \texttt{md} is a stable, bi-proper and non-minimum phase system, 
\item[(b)] \texttt{md} has all poles and zeros within $\{z\in \mathbb{C}: |z|<0.96 \text{ or } |z|>1.04\}$,
\end{enumerate} 
then \texttt{md} is saved as one of 1000 systems in D2. 

\item Data-bank D3 contains 1000 randomly generated systems $P(q)$.  We generate each system as follows.
For each test system \texttt{md} in D2, we replace the two zeros (can be two real zeros or a pair of complex conjugate zeros) having the smallest magnitude with two new zeros sampled from the uniform distributions, $\mathcal{U}[0.9, 0.8]$ and $\mathcal{U}[1.1, 1.2]$, respectively.
% It may replace a complex zero with a real one, for which we transform its conjugated zero into a positive real one with the same magnitude. 
This new system is then saved as one of 1000 test systems in D3 and has the feature that its non-causal inverse $G(q)$ has dominant over-damped dynamics.

Data-bank D4 contains 1000 randomly generated systems. Each test system takes the form of 
\begin{align*}
P(q)= \frac{(q-z_1)(q-1/z_1)(q-z_2)(q-1/z_2)}{(q-p_1)(q-p_2)(q-p_3)(q-p_4)},
\end{align*} 
where $z_1=0.9$, and $z_2, p_1, p_2, p_3, p_4$ are all sampled from the uniform distribution $\mathcal{U}[0, 0.9]$, and thus its non-causal inverse $G(q)$  has mirrored poles.


% It is then saved as one of 1000 test systems \texttt{mt} in D4.   

\end{itemize}
For illustration, some true non-causal impulse responses $g^0(t)$, $t\in\mathbb Z$, of the non-causal inverse $G(q)$ of $P(q)$, are shown in Fig. \ref{fig:sim2_theta}.
The basic prior knowledge that the test systems contain in each data banks of D2-D4 are summarized below follows: 
\begin{itemize}
\item[D2:] BIBO stability and properness;
\item[D3:] BIBO stability, properness, and over-damped dominant dynamics;
\item[D4:] BIBO stability, properness, and over-damped dominant dynamics with mirrored poles.
 \end{itemize}

% The prior knowledge for test systems $G(q)$ in the data-banks D2-D4 are summarized below:
% \begin{itemize}
% \item[D2:] BIBO stable and proper;
% \item[D3:] BIBO stable, proper, and over-damped;
% \item[D4:] BIBO stable, proper, over-damped, and with mirrored poles.
% \end{itemize} 



Second, we generate the input-output data in D1-D4. Each of D1-D4 contains 1000 data sets $\{u(t), y(t)\}^{N_{max}}_{t=1}$, where $N_{max}$ is 2000 for D1 and 700 for D2-D4, respectively. Moreover, we generate 1000 data sets for the single test system \eqref{eq:bosys} in D1, but one data set for each test system in D2-D4. More specifically, the input signal $u(t)$ in \eqref{eq:pro_sys1} is simulated by MATLAB function:
\texttt{u(t)=\text{lsim}(md,$y_0(t)$)} from $t=-n_c$ to $t=N_{max}+n_a$, where $y_0(t)$ is a white Gaussian noise with unit variance. Then the output signal $y(t)$ in \eqref{eq:pro_sys1} is obtained by perturbing $y_0(t)$ with an additive white Gaussian noise $v(t)$ with mean zero and the variance one tenth of the variance of $y_0(t)$. Finally,  $\{y(t),u(t)\}_{t=1}^{N_{max}}$ is saved as one test input-output data for the identification of \eqref{eq:pro_sys1}.

% More data sets $\{u(t), y(t)\}_{t=1}^{N}$ can be obtained from the data sets above by varying the sample size $N$.  For D1, the new data set $\{u(t), y(t)\}_{t=1}^{N}$ can be truncated from $\{u(t), y(t)\}_{t=1}^{2000}$ with $N=400, 500, 600, 700$. For D2-D3, the new data set $\{u(t), y(t)\}_{t=1}^{N}$ can be truncated from $\{u(t), y(t)\}_{t=1}^{700}$ with $N=200$.  

% \begin{Remark}\label{re:sim_boundry}
% To tackle the boundary condition, the simulations run from $t=-n_c$ to $t=2000+n_c$ in D1, but only $\{u(t), y(t)\}^{2000}_{t=1}$ is recorded and accessible for identification. Similarly, the simulations run from $t=-n_c$ to $t=700+n_c$ in D2-D3. 
% \end{Remark}


% To summarize, D1 has a single system; D2 has the general non-minimum phase systems; and D3 has the non-minimum phase systems dominated by over-damped causal and anti-causal dynamics. To illustrate the difference of D1-D3, we show examples in Fig. \ref{fig:sim2_theta}. 

% Figure environment removed


\subsection{Simulation Setup}
We first truncate the non-causal impulse response \eqref{eq:G_0(q) inverse} at sufficiently large orders $n_c$ and $n_a$, and obtain the so-called non-causal finite impulse response (FIR) model
\begin{align}\label{eq:inverse_model_fir}
&y(t)=\sum\limits_{\tau=-n_a}^{n_c}g(\tau) u(t-\tau)+v(t),
\end{align} which can be rewritten into the matrix-vector form as 
\begin{align}\label{eq:matrix_form}
Y= \Psi \theta+ V,
\end{align}
where
\begin{align*}
&Y = [y(n_c+1)\ y(n_c+2)\ \cdots\ y(N-n_a)]^\text{T}, \\
&V = [v(n_c+1)\ v(n_c+2)\ \cdots\ v(N-n_a)]^\text{T}, \\
&\theta = [g(-n_a)\ g(-n_a+1)\ \cdots\  g(n_c)]^T, \\
&\Psi = \left[
\begin{array}{cccc}
u(1+n_c+n_a)&u(n_c+n_a)&\cdots& u(1)\\
u(2+n_c+n_a)&u(1+n_c+n_a)&\cdots& u(2)\\
\vdots & \vdots &\ddots& \vdots\\
u(N)&u(N-1)&\cdots& u(N-n_c-n_a)\\
\end{array}\right]. 
\end{align*}
Accordingly, the regularized least square (RLS) estimate \eqref{eq:rels_est} can be rewritten as  
\begin{align}\label{eq:rls_theta}
&\hat{ \theta}^{\text{R}}= \underset{ \theta\in \mathbb{R}^{n}}{\text{argmin}}||Y-\Psi\theta||_2^2+ \sigma^2\theta^T  K(\eta)^{-1}\theta,
\end{align}
where $\hat\theta^{\text{R}}= [\hat g^{\text{R}}(-n_a)\ \hat  g^{\text{R}}(-n_a+1)\ \cdots\  \hat g^{\text{R}}(n_c)]^T $, $\sigma^2$ is the noise variance of $v(t)$, and $K(\eta)\in \mathbb{R}^{n\times n}$ is a positive semidefinite matrix with its $(i,j)$th entry $K_{i,j}(\eta)$ corresponding to $k(i,j; \eta)$ in \eqref{eq:rels_est}. 



The non-causal FIR model orders, $n_c$ and $n_a$, in \eqref{eq:inverse_model_fir} are both chosen to be 150 for D1 and 50 for D2-D4, respectively. 
% The sample size $N$ is chosen flexibly by selecting the first $N$ data points of the $\{u(t), y(t)\}_{t=1}^{N_{max}}$. 
For each data bank, we will use data sets with the different sample size $N$ with $N\leq N_{max}$, and test the
following kernels:
\begin{itemize}
\item the NC-TC kernel \eqref{eq:nctc}, 
\item the NCBD-TC kernel \eqref{eq:bdtc},
\item the NCBD-DC kernel \eqref{eq:ncbd_dc},
\item the NCBD-TC$^\text{mp}$ kernel, \eqref{eq:bdtc} with $\lambda_c=\lambda_a$,
\item the NCBD-DC$^\text{mp}$ kernel, \eqref{eq:ncbd_dc} with $\lambda_c=\lambda_a$,
\item the NCSI-TC kernel \eqref{eq:kernel_si_tc},
\item the NCSI-DC kernel \eqref{eq:kernel_si_dc},
\item the NCSI-FO kernel \eqref{eq:kernel_si_9},
\item the NCSI-FO$^\text{mp}$ kernel \eqref{eq:kernel_si_9star}.
\end{itemize}
\begin{Remark}\label{re:ncbd-dc}
The NCBD-DC kernel \eqref{eq:ncbd_dc} is proposed by mimicking the structure of the NCBD-TC kernel \eqref{eq:bdtc}. It is worth noting that the NCBD-DC kernel \eqref{eq:ncbd_dc} and the NCSI-DC kernel \eqref{eq:kernel_si_dc} have  the same number of hyper-parameters. 
\end{Remark}

The hyper-parameters are estimated using the empirical Bayes (EB) method, i.e., 
% which assumes that $\theta$ and $V$ are independent and Gaussian distributed, i.e., 
% \begin{align*}
% \theta\sim \mathcal{N}(0,K(\eta)),\quad  V\sim \mathcal{N}(0, \sigma_{v_u}^2 I_{N-n})\\
% \implies U \sim \mathcal{N}(0, \Psi K(\eta)\Psi^T+ \sigma^2_{v_u}I_{N-n})
% \end{align*}
 % Then $\eta$ is  estimated by maximizing the marginal likelihood $p(U|\eta)$, i.e., 
\begin{align}
&\hat{\eta}=  \underset{ \eta\in \Omega_k}{\text{argmin}}\{ Y(\Psi K(\eta) \Psi^T+ \sigma^2 I_{N-n})^{-1}Y+\nonumber\\
&\qquad  \log\det(\Psi K(\eta) \Psi^T+ \sigma^2 I_{N-n})\},\label{eq:eb_opt}
\end{align}
where $\sigma^2$ is estimated from the sample variance of the estimated non-causal FIR model \eqref{eq:inverse_model_fir}  using the LS method. 
The optimization \eqref{eq:eb_opt} is implemented by the MATLAB function \texttt{MultiStart} with \texttt{fmincon} and the number of runs being ten times the number of hyper-parameters.  Substituting \eqref{eq:eb_opt} and the estimate of $\sigma^2$ into \eqref{eq:rls_theta}, we obtain the estimate $\hat\theta^{\text{R}}$. 

The performance of the estimate $\hat\theta^{\text{R}}$ is evaluated by the model fit \cite{Ljung:99} and the tracking error \cite{BO20}. More specifically, the model fit (FIT) is defined by 
\begin{align*}
&\text{FIT} =100\left(1 -
\left[\frac{\sum^{n_c}_{k=-n_a}| g^0(k)-\hat{g}^{\text{R}}(k)|^2
}{\sum^{n_c}_{k=-n_a}| g^0(k)-\bar{g}^0|^2}\right]^{\frac{1}{2}}\right),\nonumber\\
&\bar{g}^0=\frac{1}{n}\sum^{n_c}_{k=-n_a} g^0(k), \nonumber
\end{align*}
where $\{ g^0(k)\}_{k\in\mathbb{Z}}$ is the true non-causal impulse response of $P^{-1}(q)$, and the tracking error (ERR) is defined by
 \begin{align*}
&\text{ERR}= \sqrt{\sum_{k=1}^L e(k)^2/L}, \\
&e(t)=r(t)- P(q)\sum\limits_{k=-n_a}^{n_c} \hat g^{\text{R}}(k)q^{-k}r(t),\\
&t=1-2n_c, \cdots, L+n_a,\nonumber
\end{align*}
where the test length $L=1000$, and the test reference signal $r(t)$ is a white Gaussian noise with unit variance. 
% \deleted{Note that all estimates $\hat\theta^{\text{R}}$ for the same data set are evaluated by the same $r(t)$.  }
 % Figure environment removed
 %\vspace*{-10mm}


% Figure environment removed



% Figure environment removed

%\begin{table*}
% 
%  \begin{center}
%  \caption{Average FITs and ERRs and standard deviation of FITs and ERRs for the data sets in D4.}\label{tab:fiterr_D4}
%  \resizebox{0.9 \textwidth}{28mm}{
%\begin{tabular}{ccccccccc}
%\toprule
%\multicolumn{8}{c}{avg. FIT (standard deviation)} \\
%\midrule
%Data-bank&N  &NCBD-TC &NCBD-TC$^{\text{mp}}$&NCBD-DC& NCBD-DC$^{\text{mp}}$& NCSI-FO& NCSI-FO$^{\text{mp}}$\\ \midrule
%D4&200&   34.26(37.70)&   32.00(39.83)&   49.17(37.61)&   51.86(33.74)&  64.33(30.08)& \textbf{69.31}(\textbf{27.35}) \\
%D4&700&    61.80(27.65)&   64.99(27.23)&   78.15(20.33)&   77.50(20.65)&   83.84(\textbf{16.88})&   \textbf{85.32}(17.13)\\  
%\midrule
%\multicolumn{8}{c}{avg. ERR (standard deviation)} \\
%\midrule
%Data-bank&N  &NCBD-TC &NCBD-TC$^{\text{mp}}$&NCBD-DC& NCBD-DC$^{\text{mp}}$& NCSI-FO& NCSI-FO$^{\text{mp}}$\\ \midrule
%D4&200&   
%    0.208(0.065) &   0.236(0.067)  &  0.178(0.063) &   0.170(0.053)    &0.140(0.053)    &\textbf{0.132}(\textbf{0.049})  \\
%D4&700&    0.102(0.034) &   0.106(0.032) &    0.074(0.026)&    0.0761(0.026)&    0.060(\textbf{0.023})&    \textbf{0.058}(0.023)\\   
% \bottomrule
%  \end{tabular}}\\
%  \end{center}
%\end{table*}





%% Figure environment removed


% Particularly, the curves of $\hat \theta^R$ are shown in Fig. \ref{fig:sim1_theta} for a single data set of D1. 
% \subsection{Simulation Results and Findings (the first version)}

% The box plots of FIT and ERR are shown in Fig. \ref{fig:sim1_fit}, and the average FIT and ERR are shown in Table \ref{tab:fiterr_D1}. For illustration, the estimates $\hat \theta^R$ for one data set in D1 with $N=600$ are shown in Fig. \ref{fig:sim1_theta}. 
% Based on these results, we have the following findings:
% \begin{enumerate}
% \item[(a)] Fig. \ref{fig:sim1_fit} shows that, as the sample size $N$ increases, the model estimation (resp. tracking)  performance are improved in both average accuracy and robustness.

% \item[(b)] Fig. \ref{fig:sim1_fit} and Table \ref{tab:fiterr_D1} show a comparison between the NCSI kernels, i.e., \eqref{eq:kernel_si_9}, \eqref{eq:kernel_si_dc} and \eqref{eq:kernel_si_tc}, and the existing kernels, i.e., \eqref{eq:nctc} and \eqref{eq:bdtc}. On the one hand, the NCSI kernels have better model estimation (resp. tracking)  performance in term of the average FIT (resp. ERR). On the other hand, the NCSI kernels have more robust model estimation (resp. tracking)  performance in terms of the distribution of the FIT (resp. ERR). These results are consistent with Fig. \ref{fig:sim1_theta}, where the estimates $\hat \theta^{\text{R}}$ of the NCSI kernels \eqref{eq:kernel_si_tc} are closer to $\tilde g_0$ than the estimates $\hat \theta^{\text{R}}$ of the existing kernels. 

% \item[(c)] Table \ref{tab:fiterr_D1} shows a comparison among the NCSI kernels, i.e., \eqref{eq:kernel_si_9}, \eqref{eq:kernel_si_dc} and \eqref{eq:kernel_si_tc}. The NCSI-DC kernel \eqref{eq:kernel_si_dc} and NCSI-FO kernel \eqref{eq:kernel_si_9} have better model estimation (resp. tracking)  performance than the other kernels in D1-D3 in terms of the average FIT (resp. ERR).  Moreover, the NCSI-DC kernel \eqref{eq:kernel_si_dc} and NCSI-FO kernel \eqref{eq:kernel_si_9} have similar  model estimation (resp. tracking) performance in D1-D2, while the NCSI-FO kernel \eqref{eq:kernel_si_9} is better in D3, in term of the average FIT (resp. ERR).

% \item[(d)] Table \ref{tab:fiterr_D1} shows a comparison between the NCBD-DC kernel \eqref{eq:ncbd_dc} and the NCSI-DC kernel \eqref{eq:kernel_si_dc}. They have the same number of hyper-parameters, but the NCSI-DC kernel \eqref{eq:kernel_si_dc} has the better model estimation (resp. tracking) performance in terms of the average FIT (resp. ERR).  
% \end{enumerate}

% As we expected, the NCSI kernels, \eqref{eq:kernel_si_9}, \eqref{eq:kernel_si_dc} and \eqref{eq:kernel_si_tc}, show better performance than the existing non-causal kernels in general, and moreover, their advantages are more significant in D3, whose systems are closer to the assumption made for the NCSI-FO kernel \eqref{eq:kernel_si_9} in Section \ref{se:para}. 
% In what follows, we have another observation that deserves more discussion. 
% \begin{enumerate}
% \item[(f)] Recall that the systems in D3 are closer to the prior knowledge embedded in the NCSI-FO kernel \eqref{eq:kernel_si_9} than the systems in D2, implying that the NCSI kernels, \eqref{eq:kernel_si_9}, \eqref{eq:kernel_si_dc} and \eqref{eq:kernel_si_tc}, should have the higher FIT and lower ERR in D3 than D2. However, Table \ref{tab:fiterr_D1} shows the opposite result. To explain that, we give a potential example in D2 as follows.
% \begin{align}\label{eq:ex_non_sys}
% G_0(q)= (1+\frac{1}{q-0.9}+\frac{0.0001}{q-1.1} )^{-1}
% \end{align}
% Although \eqref{eq:ex_non_sys} is a non-minimum phase system by definition, it behaves almost like a minmum phase system, as can be seen from Fig. \ref{fig:sim2_theta}. To identify the inverse model of \eqref{eq:ex_non_sys}, the causal part of the kernel, i.e., $k(t,s)$ with $t,s>0$, plays a major role. Unfortunately, there are lots of systems like \eqref{eq:ex_non_sys} in D2 contributing to a very high FIT and very low ERR, which deteriorates the average value in Table \ref{tab:fiterr_D1}.  By contrast, system \eqref{eq:ex_non_sys} rarely exists in D3 for the replacement of the dominant zeros. 
% Therefore, for non-causal kernels, the data-bank D2 is a general benchmark but not a good one, and the data-bank D3 is a good benchmark but not a general one. 
% \end{enumerate}



\begin{table*}
  \begin{center}
  \caption{ Average FITs and ERRs, and standard deviation of FITs and ERRs for the data sets in D1-D3.}\label{tab:fiterr_D1}
  \resizebox{ \textwidth}{48mm}{
\begin{tabular}{ccccccccc}
\toprule
\multicolumn{8}{c}{avg. FIT (standard deviation)} \\
\hline
Data-bank&N  & NC-TC&NCBD-TC& NCBD-DC& NCSI-TC  & NCSI-DC& NCSI-FO \\ \hline
D1&600&    12.98(\ \textbf{7.62})        & 27.06(10.47)     &    30.51(11.98)        & 47.64(\ 8.98)    &     \textbf{52.97}(\ {7.77})        & 52.69(\ 8.03)\\
D1&2000&    49.48(10.11)      &   60.41(\ 4.06)     &    62.78(\ 3.80)        & 76.02(\ 3.48)     &    \textbf{76.96}(\ \textbf{3.25})        & 76.85(\ 3.33)\\
D2&200&     76.22(16.61)     &    77.49(16.50)     &   78.62(16.09)        & 78.78(15.15)      &   81.09(\textbf{13.07})       &  \textbf{81.18}(13.14)\\
D2&700&   88.54(10.87)      &   88.98(11.09)         &89.46(11.06)      &   89.87(10.38)       &  91.16(\ 9.65)      &   \textbf{91.28}(\ \textbf{9.61})\\
D3&200&       47.58(23.40)   &      55.39(23.65)      &   59.58(25.26)        & 63.37(\textbf{20.25})        & 65.16(21.29)  &       \textbf{67.20}(21.45)\\
D3&700&   74.76(16.61)  &       79.26(16.40)       &  81.73(16.54)    &     83.13(\textbf{14.93})       &  84.45(15.17)     &    \textbf{85.41}(15.25)\\
\hline
\multicolumn{8}{c}{avg. ERR (standard deviation)} \\
\hline
Data-bank&N  & NC-TC&NCBD-TC& NCBD-DC& NCSI-TC  & NCSI-DC& NCSI-FO \\ \hline
D1&600 & 0.329(0.030)  &  0.291(0.036)    &0.287(0.042)   & 0.205(\textbf{0.025})    &0.189(0.031)  &  \textbf{0.188}(0.029)\\
D1&2000& 0.165(0.020)  &  0.137(0.012)   & 0.134(0.011)    &0.093(0.008)   & 0.087(\textbf{0.007})    &\textbf{0.087}(0.007)\\
D2&200  &  0.194(0.069) &   0.181(0.067)    &0.171(0.067)   & 0.175(0.062)   &\textbf{0.163}(0.058) &0.165(\textbf{0.057})\\
D2&700 &     0.083(0.027)&    0.078(0.027) &   0.073(0.027)  &  0.074(0.024)   & 0.068(0.022)  &  \textbf{0.067}(\textbf{0.022})\\
D3&200 & 0.280(0.073)  &  0.240(0.062)    &0.219(0.072)   & 0.211(\textbf{0.053})    &0.198(0.060)    &\textbf{0.189}(0.063) \\
D3&700&    0.124(0.025) &   0.102(0.024) &   0.091(0.025)  &  0.089(\textbf{0.019}) &   0.081(0.020)  &  \textbf{0.077}(0.021) \\
 \bottomrule
  \end{tabular}}
% {\color{purple}   $\dag$\ this standard deviation is neglected for its too poor average. }
\end{center}
\end{table*}

\begin{table*}


  \begin{center}
    \caption{Average FITs and ERRs and standard deviation of FITs and ERRs for the data sets in D4.}\label{tab:fiterr_D4}
  \resizebox{\textwidth}{28mm}{
\begin{tabular}{ccccccccc}
\toprule
\multicolumn{8}{c}{avg. FIT (standard deviation)} \\
\midrule
Data-bank&N  &NCBD-TC &NCBD-TC$^{\text{mp}}$&NCBD-DC& NCBD-DC$^{\text{mp}}$& NCSI-FO& NCSI-FO$^{\text{mp}}$\\ \midrule
D4&200&   34.26(37.70)&   32.00(39.83)&   49.17(37.61)&   51.86(33.74)&  64.33(30.08)& \textbf{69.31}(\textbf{27.35}) \\
D4&700&    61.80(27.65)&   64.99(27.23)&   78.15(20.33)&   77.50(20.65)&   83.84(\textbf{16.88})&   \textbf{85.32}(17.13)\\  
\midrule
\multicolumn{8}{c}{avg. ERR (standard deviation)} \\
\midrule
Data-bank&N  &NCBD-TC &NCBD-TC$^{\text{mp}}$&NCBD-DC& NCBD-DC$^{\text{mp}}$& NCSI-FO& NCSI-FO$^{\text{mp}}$\\ \midrule
D4&200&   
    0.208(0.065) &   0.236(0.067)  &  0.178(0.063) &   0.170(0.053)    &0.140(0.053)    &\textbf{0.132}(\textbf{0.049})  \\
D4&700&    0.102(0.034) &   0.106(0.032) &    0.074(0.026)&    0.0761(0.026)&    0.060(\textbf{0.023})&    \textbf{0.058}(0.023)\\   
 \bottomrule
  \end{tabular}}
  \end{center}
\end{table*}



\subsection{Results, Observations and Take-home Messages}\label{se:result_and_finding}
First, we consider the data-banks D1-D3. 
The box plots of FITs and ERRs  are shown in Fig. \ref{fig:sim1_fit}, and the average FITs and ERRs and their standard deviations are shown in Table \ref{tab:fiterr_D1}. For illustration, the estimates $\hat \theta^R$ for one data set in D1 with $N=600$ are shown in Fig. \ref{fig:sim1_theta}. 
Based on these results, we have the following observations:
\begin{itemize}
\item The kernels more to the left in general give better
(and thus the NCSI-FO kernel \eqref{eq:kernel_si_9} in general gives
the best) performance in terms of the average FIT
and ERR, and robustness of the FITs and ERRs
through the distribution and also the standard deviation of the FITs and ERRs. In particular, for
data-banks D1-D2, the NCSI-DC kernel  \eqref{eq:kernel_si_dc} and
NCSI-FO kernel \eqref{eq:kernel_si_9} have very close performance
\end{itemize}
Second, we consider the data-bank D4. The box plots of FITs and ERRs  are shown in Fig. \ref{fig:sim4_fit}, and the average FITs and ERRs and their standard deviations are shown in Table \ref{tab:fiterr_D4}. Then we have the following observations:
\begin{itemize}
\item in contrast with the NCBD-TC kernel \eqref{eq:bdtc} (resp. the NCBD-DC kernel \eqref{eq:ncbd_dc}), the NCBD-TC$^\text{mp}$ kernel (resp. the NCBD-DC$^\text{mp}$ kernel) does not give consistent improvement in the performance in terms of the average FIT and ERR, and robustness of the FITs and ERRs through the distribution and also the standard deviation of the FITs and ERRs; on the other hand, in contrast with the NCSI-FO kernel \eqref{eq:kernel_si_9}, the NCSI-FO$^\text{mp}$ kernel \eqref{eq:kernel_si_9star} gives consistent improvement in the performance in terms of the average FIT and robustness of the FITs and ERRs through the distribution of the FITs and ERRs, but a bit worse standard deviation. 
\end{itemize}
\newpage

Finally, we have the following take-home messages: 
\begin{itemize}
\item in general, the NCSI-FO kernel \eqref{eq:kernel_si_9} is recommended to be used, and if computation issues are
concerned, then the NCSI-DC kernel \eqref{eq:kernel_si_dc} can be
tried as an alternative;
\item if there is extra prior knowledge (e.g., mirrored
poles), then it is suggested to further explore the
structure of the NCSI-FO kernel \eqref{eq:kernel_si_9} and design a
more specific kernel (e.g., the NCSI-FO$^\text{mp}$ kernel)
to embed such prior knowledge
\end{itemize}
% As we expected, the NCSI kernels, \eqref{eq:kernel_si_9}, \eqref{eq:kernel_si_dc} and \eqref{eq:kernel_si_tc}, show better performance than the existing non-causal kernels in general, and moreover, their advantages are more significant in D3, whose systems are closer to the assumption made for the NCSI-FO kernel \eqref{eq:kernel_si_9} in Section \ref{se:para}. 
% In what follows, we have another observation that deserves more discussion. 
% \begin{enumerate}
% \item[(f)] Recall that the systems in D3 are closer to the prior knowledge embedded in the NCSI-FO kernel \eqref{eq:kernel_si_9} than the systems in D2, implying that the NCSI kernels, \eqref{eq:kernel_si_9}, \eqref{eq:kernel_si_dc} and \eqref{eq:kernel_si_tc}, should have the higher FIT and lower ERR in D3 than D2. However, Table \ref{tab:fiterr_D1} shows the opposite result. To explain that, we give a potential example in D2 as follows.
% \begin{align}\label{eq:ex_non_sys}
% G_0(q)= (1+\frac{1}{q-0.9}+\frac{0.0001}{q-1.1} )^{-1}
% \end{align}
% Although \eqref{eq:ex_non_sys} is a non-minimum phase system by definition, it behaves almost like a minmum phase system, as can be seen from Fig. \ref{fig:sim2_theta}. To identify the inverse model of \eqref{eq:ex_non_sys}, the causal part of the kernel, i.e., $k(t,s)$ with $t,s>0$, plays a major role. Unfortunately, there are lots of systems like \eqref{eq:ex_non_sys} in D2 contributing to a very high FIT and very low ERR, which deteriorates the average value in Table \ref{tab:fiterr_D1}.  By contrast, system \eqref{eq:ex_non_sys} rarely exists in D3 for the replacement of the dominant zeros. 
% Therefore, for non-causal kernels, the data-bank D2 is a general benchmark but not a good one, and the data-bank D3 is a good benchmark but not a general one. 
% \end{enumerate}








\section{Conclusion}\label{se:conclusion}
In this paper, we studied the non-causal system identification problem  by using the kernel-based regularization method. To tackle the key difficulty of non-causal kernel design, we first introduced the guidelines, then extended the system theoretic framework to design the so-called non-causal simulation-induced kernel and studied its properties, including stability and semiseparability. The numerical simulation results showed that the proposed kernels can give better model estimates/tracking performance than the state-of-art kernels. The importance of our obtained results lies in that we develop a systematic framework to design kernels for non-causal systems, the prior knowledge embedded in the designed kernels have clear physical interpretation, and the designed kernels are semiseparable and thus can be used to develop efficient implementations.   





%===============================================================================


\def\thesectiondis{\thesection.}                   % I.
\def\thesubsectiondis{\thesection.\arabic{subsection}.}          % B.
\def\thesubsubsectiondis{\thesubsection.\arabic{subsubsection}.}

\setcounter{subsection}{0}

\renewcommand{\thesection}{A}
% \setcounter{theorem}{0}
% \renewcommand{\thelemma}{A.\arabic{lemma}}

\renewcommand{\theequation}{A.\arabic{equation}}
\setcounter{equation}{0}

\renewcommand{\thesubsection}{\thesection.\arabic{subsection}}

\renewcommand{\thesection}{A}
\renewcommand{\thesubsection}{A.\arabic{subsection}}

\section*{Appendix}

\subsection{Proof of Lemma \ref{le:ss_model}}



We will prove Lemma \ref{le:ss_model} in two steps.  First, it is straightforward to show that, given $x_c(-M_c)$, $x_a(M_a)$, $g(t)$ takes the form of
\begin{align}
&g(t)=\left\{
\begin{array}{ll}
&g_{ini}(t)+g_{imp}(t), \text{ if }t,s=-M_c,\cdots, M_a\\
&0, \quad \text{otherwise}
\end{array}\right.,\label{eq:gt_converg_new_variable}\\
&g_{ini}(t)= C_cA_c^{t+M_c} x_c(-M_c)+ C_a A_a^{t-M_a} x_a(M_a),\nonumber\\
&g_{imp}(t)= \sum\limits_{k=-M_c}^{M_a-1}g_0(t-k)b(k)w(k),\nonumber
\end{align}
where $g_0(t)$ is defined in \eqref{eq:noncausal_g0}, and moreover, $\{g_0(t)\}_{t\in \mathbb{Z}}\in\ell_1(\mathbb{Z})$. 
 
% Consider the nominal model \eqref{eq:si_nominal_model} with  $t=-M_c$,  $\cdots$, $M_a$. Then the causal part \eqref{eq:si_nominal_model1} leads to 
% \vspace*{-3mm}\begin{align*}
% &x_c(t)=A_c^{t+M_c} x_c(-M_c)+ \sum\limits_{k=-M_c}^{t-1} A_c^{t-1-k}B_c\bar u(k),
% \end{align*}
% and the anti-causal part  \eqref{eq:si_nominal_model2} can be rewritten in the backward propagation as
%  % the dynamics (\ref{eq:si_nominal_model2}) is reversed and followed by the similar successive substitution from the boundary $x_a(M_a)$ such that
% \begin{align*}
% &x_a(t-1)=A_a^{-1} x_a(t)-A^{-1}_a B_a\bar  u(t-1),
%  \end{align*}
% which leads to
% \begin{align*}
% &x_a(t)= A_a^{t-M_a} x_a(M_a)+ \sum\limits_{k=t}^{M_a-1} -A_a^{t-1-k}B_a\bar u(k).
% \end{align*}
% Substituting $x_c(t)$ and $x_a(t)$ into \eqref{eq:si_nominal_model3}, we complete the proof of \eqref{eq:gt_converg_new_variable}, and apparently, $\{g_0(t)\}_{t\in \mathbb{Z}}\in\ell_1(\mathbb{Z})$. 

Second, we let $Z=[g(t_1),\cdots, g(t_m)]^T$ and then decompose $Z$ as $Z=Z_{imp}+Z_{ini}$, where
\begin{align*}
&Z_{ini}=[g_{ini}(t_1),  \cdots, g_{ini}(t_{m})]^{\text{T}},\\
&Z_{imp}=[g_{imp}(t_1),  \cdots, g_{imp}(t_{m})]^{\text{T}},\\
& -M_c\leq t_1, t_2,\cdots, t_{m}\leq M_a.
\end{align*}
Then we show that as ${M_c\to\infty}$ and ${M_a\to\infty}$,  $Z_{ini}$ converges in probability to a zero vector and $Z_{imp}$
converges in distribution to a Gaussian random vector. 
 

% \begin{itemize}
% \item  $\bar {u}(t)$ with $t=-M_c$, $\cdots$, $M_a$, are mutually independent and Gaussian distributed with mean zero and bounded variance $b^2(t)$,
% \item $x_c(-M_c)$, $x_a(M_a)$  have zero mean and bounded covariance matrix $\Sigma_{M_c}$ and $\Sigma_{M_a}$, respectively, and independent with any $\bar {u}(t)$. 
% \end{itemize} 


On the one hand, for $Z_{ini}$ and for any $\epsilon>0$, we have 
\begin{align*}
P&\bigg(\|Z_{ini}\|_2>m\epsilon\bigg)\\&\leq P\bigg(|g_{ini}(t_1)|>\epsilon \text{ or }\cdots  \text{ or } |g_{ini}(t_{m})|>\epsilon\bigg)\\
&\leq \sum\limits_{i=1}^{m} P\bigg(|g_{ini}(t_i)|>\epsilon \bigg)\leq \sum\limits_{i=1}^{m}\frac{\cov{g_{ini}(t_i),g_{ini}(t_i)}}{\epsilon^2}
\end{align*} 
where $\|\cdot\|_2$ is the Euclidean norm and the last inequality is due to the Chebyshev's inequality. 
Under the assumption that  $x_{ini}(M_c, M_a)=[x_c(-M_c)^T, x_a(M_a)^T]^T$ has zero mean and bounded covariance matrix $\Sigma_{ini}(M_c, M_a)$,  we have 
\begin{align*}
&\cov{g_{ini}(t_i), g_{ini}(t_i)}= [C_cA_c^{t_i+M_c}\ C_a A_a^{t_i-M_a}] \cdots \\
& \quad \Sigma_{ini}(M_c, M_a)\ [C_cA_c^{t_i+M_c}\ C_a A_a^{t_i-M_a}]^T\rightarrow 0
\end{align*}  
as $M_c, M_a\rightarrow \infty$, and moreover, \begin{align} 
\lim\limits_{M_c\rightarrow \infty }\lim\limits_{M_a\rightarrow \infty }P\bigg(\|Z_{ini}\|_2>m\epsilon\bigg) \leq 0, \label{eq:conv_in_p}
\end{align}
implying that $Z_{ini}$ converges in probability to a zero vector.


On the other hand, for $Z_{imp}$, we consider its characteristic function given by
\begin{align*}
&\varphi(v)= \mathbb{E}[\exp(jv^T Z_{imp})]=\exp(-\frac{1}{2} v^T  \Sigma\ v)\text{ with }\nonumber\\
&( \Sigma)_{i,j} = \cov{g_{imp}(t_i), g_{imp}(t_j)},\ 1\leq i,j\leq m,\nonumber\\
&=\sum\limits_{k=-M_c}^{M_a-1} b^2(k)g_0(t_i-k)g_0(t_j-k).
\end{align*}
Then we have 
\begin{align}
&\lim\limits_{M_c\rightarrow \infty }\lim\limits_{M_a\rightarrow \infty }\varphi( v)=\tilde\varphi( v) =\exp(-\frac{1}{2}  v^T\ \tilde\Sigma\  v)\label{eq:convergence_sigma},\\
&\text{ with }( \tilde \Sigma)_{i,j} =\sum\limits_{k=-\infty}^{\infty} b^2(k)g_0(t_i-k)g_0(t_j-k),\nonumber
\end{align}
where $( \tilde \Sigma)_{i,j}$ exists for all $i,j=1,\cdots, m$ due to the boundedness of $b^2(t)$ and $\{g_0(t)\}_{t\in \mathbb{Z}}\in\ell_1(\mathbb{Z})$. It then follows from the Levy's continuity theorem, e.g., \cite[p. 204]{Loeve78I}, that \eqref{eq:convergence_sigma} leads to  \begin{align}
&Z_{imp}\overset{d}{\longrightarrow} \mathcal{N}(0,\tilde\Sigma),\quad \text{ as } M_c, M_a\rightarrow \infty,\label{eq:conv_in_d}
\end{align}
where $\overset{d}{\longrightarrow}$ denotes the convergence in distribution.

Finally, from the multivariate version of the Slutsky's theorem, e.g., \cite[Thm. 3.4.3]{SS94}, \eqref{eq:conv_in_p} and \eqref{eq:conv_in_d}, it holds that $\lim_{M_c\to\infty}\lim_{M_a\to\infty}Z$ converges in distribution to a Gaussian vector with mean zero and covariance matrix $\tilde\Sigma$. This completes the proof.  






\subsection{Proof of Theorem \ref{th:st}}

% {\color{blue}\begin{Lemma}[\protect{\cite[Lemma 3]{DINUZZO15}}]\label{co:stability_sufficient}
% A positive semidefinite kernel $k(t,s):\mathbb{Z}\times \mathbb{Z}\rightarrow \mathbb{R}$ is stable if 
% \begin{align}\label{eq:gu:stable_suff}
% \sum\limits_{s=-\infty}^\infty\sum\limits_{t=-\infty}^\infty |k(t,s)|< \infty.
% \end{align}
% \end{Lemma}}

Note that
\begin{subequations}
\begin{align*}
&\sum\limits_{s=-\infty}^\infty \sum\limits_{t=-\infty}^\infty |k^{\text{NCSI}}(t,s)|\\
&=\sum\limits_{s=-\infty}^\infty \sum\limits_{t=-\infty}^\infty | \sum\limits_{k=-\infty}^\infty b^2(k) g_0(t-k)g_0(s-k)|\\
&\leq \sum\limits_{k=-\infty}^\infty |b^2(k)|\sum\limits_{s=-\infty}^\infty \sum\limits_{t=-\infty}^\infty | g_0(t-k)g_0(s-k)|\\
&= \sum\limits_{k=-\infty}^\infty b^2(k)\sum\limits_{s=-\infty}^\infty \sum\limits_{t=-\infty}^\infty |g_0(t)g_0(s)|\\
&\leq \left(\sum\limits_{k=-\infty}^\infty b^2(k)\right)
 \left(\sum\limits_{k=-\infty}^\infty | g_0(k)|\right)^2<\infty, 
\end{align*}
where the last inequality is true for $ \{g_0(t)\}_{t\in\mathbb{Z}}\in \ell_1(\mathbb{Z})$ and  $\{b(t)\}_{t\in\mathbb{Z}}$ $\in \ell_1(\mathbb{Z})\subset \ell_2(\mathbb{Z})$. Then by \cite[Lemma 3]{DINUZZO15} we complete the proof.

\end{subequations}


% \subsection{Proof of Lemma \ref{le:trace}}


% The trace of the NCSI kernel \eqref{eq:si_expre} is defined as 
% \begin{align*}
% \sum\limits_{t=-\infty}^\infty|k^{\text{NCSI}}(t,t)|&=\sum\limits_{t=-\infty}^\infty|\sum\limits_{k=-\infty}^\infty b(k)^2 g_0(t-k)^2|\\
% &=\sum\limits_{k=-\infty}^\infty b(k)^2\sum\limits_{t=-\infty}^\infty g_0(t-k)^2\\
% &=\bigg(\sum\limits_{t=-\infty}^\infty b^2(k)\bigg)\bigg(  \sum\limits_{t=-\infty}^\infty g_0(t)^2 \bigg)
% \end{align*}


\subsection{Proof of Theorem \ref{th:se} and Corollary \ref{co:se}}
Before proceeding to the details, we introduce a lemma.
\begin{Lemma}\label{le:se}
Given three matrices $A\in\mathbb{R}^{n\times n}$, $B\in\mathbb{R}^{n\times 1}$ and $C\in\mathbb{R}^{1\times n}$ with $A$ being non-singular, there exist two real-valued functions $h_j(t), \hbar_j(t), t\in\mathbb{Z}$  such that
\begin{align*}
CA^{t-\tau}B=\sum\limits_{j=1}^{n}h_j(t)\hbar_j(\tau),\quad t,\tau\in\mathbb{Z},
\end{align*}
\end{Lemma}
\noindent \textit{Proof}:
The proof can be found in that of \cite[Theorem 1]{CA21}. 
% \end{proof}


First, the non-causal impulse response of the nominal model \eqref{eq:si_nominal_model} takes the form of \eqref{eq:noncausal_g0}.  
Then we have 
\begin{align*}
&g_0(t-\tau)= \left\{
\begin{array}{lll}
&C_c  A^{t-\tau-1}_cB_c,& \quad t-\tau\geq 1\\
&D-C_a  A^{-1}_aB_a, &\quad t-\tau= 0\\
&-C_a  A^{t-\tau-1}_aB_a, &\quad t-\tau\leq -1
\end{array}\right. 
\end{align*} 
where both $A_c$ and $A_a$ are non-singular. By Lemma \ref{le:se}, there exist four real-valued functions $h_{c,j}(t)$, $\hbar_{c,j}(t)$, $h_{a,j}(t)$, $\hbar_{a,j}(t)$, $t\in \mathbb{Z}$ such that 
\begin{align*}
&C_c  A^{t-\tau-1}_cB_c=\sum\limits_{j=1}^{n_{d,c}}h_{c,j}(t)\hbar_{c,j}(\tau),\quad  t, \tau\in \mathbb{Z},\\
&-C_a  A^{t-\tau-1}_aB_a=\sum\limits_{j=1}^{n_{d,a}}h_{a,j}(t)\hbar_{a,j}(\tau), \quad  t, \tau\in \mathbb{Z},
\end{align*}
where $n_{d,c}, n_{d,a} \in \mathbb{N}$  are the dimension of $A_c$ and $A_a$, respectively. 
Referring to Definition \ref{de:semise}, we first write down the generator functions of \eqref{eq:si_expre}, denoted by, $\mu_j(t), \nu_j(s)$, $t,s\in \mathbb{Z}$, $j=1,\cdots, n_{d,c}+ n_{d,a}$. For $j=1,\cdots, n_{d,c}$, we have 
\begin{align*}
&
\begin{array}{lll}
&\mu_j(t) = h_{c,j}(t),\\
&\nu_j(s) = M_{1,j}(s)+ M_{2,j}(s)+M_{3,j}(s),\end{array}
\\
&M_{1,j}(s)= \sum\limits_{j'=1}^{n_{d,c}} h_{c,j'}(s)  \sum\limits_{i=-\infty}^{s-1} b^2(i)\hbar_{c,j}(i)\hbar_{c,j'}(i),\\
&M_{2,j}(s)=\hbar_{c,j}(s)b^2(s)(D-C_aA_a^{-1}B_a),\\
&M_{3,j}(s)\!=\! \left\{
\begin{array}{lll}
& \sum\limits_{j'=1}^{n_{d,a}}h_{a,j'}(s) \sum\limits_{i=s+1}^{0}b^2(i)\hbar_{c,j}(i)\hbar_{a,j'}(i),& \text{ if }s\leq -1\\
&0,& \text{ if }s=0\\
&-\sum\limits_{j'=1}^{n_{d,a}}h_{a,j'}(s)\sum\limits_{i=1}^{s} b^2(i)\hbar_{c,j}(i)\hbar_{a,j'}(i),&\text{ if }s\geq 1
\end{array}\right.
\end{align*} and for $j=1,\cdots, n_{d,a}$, we have 
\begin{align*}
&
\begin{array}{lll}
&\mu_{n_{d,c}+j}(t) = N_{1,j}(t)+N_{2,j}(t)+ N_{3,j}(t),\\
&\nu_{n_{d,c}+j}(s) = h_{a,j}(s),\end{array}
\\
& N_{1,j}(t)=\sum\limits_{j'=1}^{n_{d,a}} h_{a,j'}(t) \sum\limits^{\infty}_{i=t+1} b^2(i)\hbar_{a,j}(i)\hbar_{a,j'}(i),\\
&N_{2,j}(t)= \hbar_{a,j}(t)b^2(t)(D-C_aA_a^{-1}B_a),\\
&N_{3,j}(t)\!=\! \left\{
\begin{array}{lll}
&\sum\limits_{j'=1}^{n_{d,c}}h_{c,j'}(t) \sum\limits_{i=1}^{t-1}b^2(i)\hbar_{a,j}(i)\hbar_{c,j'}(i), &\text{ if } t\geq 2\\
&0, &\text{ if }t=1\nonumber\\
&-\sum\limits_{j'=1}^{n_{d,c}}h_{c,j'}(t)\sum\limits_{i=t}^{0} b^2(i)\hbar_{a,j}(i)\hbar_{c,j'}(i), &\text{ if }t\leq 0
\end{array}\right.
\end{align*}

To prove Theorem \ref{th:se}, we need to show $k^{\text{NCSI}}(t,s)=\sum_{j=1}^{n_{d,c}+n_{d,a}}\mu_j(t)\nu_j(s)$, for $t>s$.
From  \eqref{eq:si_expre}, we have
\begin{align*}%\label{ap:semi_ncsi_decomp}
&k^{\text{NCSI}}(t,s)= \sum\limits_{i=-\infty}^{s-1} b^2(i)g_0(t-i)g_0(s-i)\\
&+ \sum\limits_{i=s}^{t} b^2(i)g_0(t-i)g_0(s-i)+\sum\limits_{i={t+1}}^{\infty} b^2(i)g_0(t-i)g_0(s-i).\\
\end{align*} 
where 
\begin{subequations}
\begin{align*}
&\sum\limits_{i=-\infty}^{s-1} b^2(i)g_0(t-i)g_0(s-i) \\
&=\sum\limits_{j=1}^{n_{d,c}} h_{c,j}(t)\left(\sum\limits_{j'=1}^{n_{d,c}} h_{c,j'}(s)  \sum\limits_{i=-\infty}^{s-1} b^2(i)\hbar_{c,j}(i)\hbar_{c,j'}(i)\right)
\\&=\sum\limits_{j=1}^{n_{d,c}}\mu_j(t)M_{1,j}(s),
\end{align*}
\begin{align*}
&\sum\limits_{i=t+1}^{\infty} b^2(i)g_0(t-i)g_0(s-i) \\
&=\sum\limits_{j=1}^{n_{d,a}} h_{a,j}(s)\left(\sum\limits_{j'=1}^{n_{d,a}} h_{a,j'}(t) \sum\limits^{\infty}_{i=t+1} b^2(i)\hbar_{a,j}(i)\hbar_{a,j'}(i)\right)\\
&= \sum\limits_{j=1}^{n_{d,a}}N_{1,j}(t)\nu_{n_{d,c}+j}(s),
\end{align*}
\begin{align*}
&\sum\limits_{i=s}^{t} b^2(i)g_0(t-i)g_0(s-i)=b^2(s)g_0(t-s)g_0(0)\\
&\quad +\sum\limits_{i=s+1}^{t-1} b^2(i)g_0(t-i)g_0(s-i)+ b^2(t)g_0(0)g_0(s-t)\\
&=\sum\limits_{j=1}^{n_{d,c}}\mu_j(t)M_{2,j}(s)+\sum\limits_{j=1}^{n_{d,c}}\mu_j(t)M_{3,j}(s) \\
&\quad + \sum\limits_{j=1}^{n_{d,a}}N_{3,j}(t)\nu_{n_{d,c}+j}(s)+ \sum\limits_{j=1}^{n_{d,a}}N_{2,j}(t)\nu_{n_{d,c}+j}(s).
\end{align*}
\end{subequations}
Then note that, if any generator functions $\mu_j(t), \nu_j(s)$, $j=1,\cdots, n_{d,c}+ n_{d,a}$ vanish, the semiseparable rank will be smaller than $n_{d,c}+n_{d,a}$.
This completes the proof of Theorem \ref{th:se}.



To prove Corollary \ref{co:se}, we need to show $k^{\text{NCSI}}(t,t)=\sum_{j=1}^{n_{d,c}+n_{d,a}}\mu_j(t)\nu_j(t)$.
On one hand, we have from  \eqref{eq:si_expre}
\begin{align}
&k^{\text{NCSI}}(t,t)= \sum\limits_{i=-\infty}^{t-1} b^2(i)g_0(t-i)g_0(t-i)\nonumber\\
&+  b^2(t)g_0(0)g_0(0)+\sum\limits_{i={t+1}}^{\infty} b^2(i)g_0(t-i)g_0(t-i).\nonumber\\
&= \sum\limits_{j=1}^{n_{d,c}}\mu_j(t)M_{1,j}(t)+b^2(t)(D-C_aA^{-1}_aB_a)^2\nonumber\\
&+ \sum\limits_{j=1}^{n_{d,a}}N_{1,j}(t)\nu_{j+n_{d,c}}(t)\label{ap:h2tt_1}
\end{align}
On the other hand, we have 
\begin{align}
&\sum\limits_{j=1}^{n_{d,c}+n_{d,a}}\mu_j(t)\nu_j(t) =\sum\limits_{j=1}^{n_{d,c}}\mu_j(t)M_{1,j}(t)+ \sum\limits_{j=1}^{n_{d,c}}\mu_j(t)M_{2,j}(t)\nonumber\\
&+\sum\limits_{j=1}^{n_{d,c}}\mu_j(t)M_{3,j}(t) + \sum\limits_{j=1}^{n_{d,a}}N_{3,j}(t)\nu_{j+n_{d,c}}(t)\nonumber\\
&+ \sum\limits_{j=1}^{n_{d,a}}N_{2,j}(t)\nu_{j+n_{d,c}}(t)+ \sum\limits_{j=1}^{n_{d,a}}N_{1,j}(t)\nu_{j+n_{d,c}}(t)\nonumber\\
&=\sum\limits_{j=1}^{n_{d,c}}\mu_j(t)M_{1,j}(t)+ b^2(t)(C_cA_c^{-1}B_c)(D-C_aA_a^{-1}B_a)\nonumber\\
&-b^2(t)(C_cA_c^{-1}B_c)(-C_aA_a^{-1}B_a)\nonumber\\
&+ b^2(t)(-C_aA_a^{-1}B_a)(D-C_aA_a^{-1}B_a)\nonumber\\&+\sum\limits_{j=1}^{n_{d,a}}N_{1,j}(t)\nu_{j+n_{d,c}}(t) \label{ap:h2tt_2}
\end{align}
Clearly, \eqref{ap:h2tt_1} and \eqref{ap:h2tt_2} are not equal in general unless  
\begin{align*}
D(D-C_cA_c^{-1}B_c-C_aA_a^{-1}B_a)=0.
\end{align*}
This completes the proof of Corollary \ref{co:se}. 

% \subsection{Proof of Lemma \ref{le:outputkernel}}
% It follows from the definition \eqref{eq:def_outputkernel} that, 
% \begin{align*}
% &O^{\text{NCSI}}(t, s)= \sum\limits_{x=-\infty}^\infty u(t-x)\sum\limits_{a=-\infty}^\infty u(s-a)\cdots\\
% &\quad \sum\limits_{k=-\infty}^\infty b^2(k) g_0(x-k)  g_0(a-k)\\
% &= \sum\limits_{k=-\infty}^\infty b(k)^2 \bigg(\sum\limits_{x=-\infty}^\infty u(t-x) g_0(x-k)\bigg)\cdots\\
% & \quad \bigg(\sum\limits_{a=-\infty}^\infty u(s-a) g_0(a-k)\bigg)
% \end{align*}

% \subsection{Proof of Theorem \ref{th:se2}}
% This is a immediate result of Theorem \ref{th:se} and Corollary \ref{co:se}. 

\subsection{Closed Form of the NCSI-FO Kernel \eqref{eq:kernel_si_9}}\label{ap:ncsifo}
To avoid the abuse of notation, we let $k(t,s;\eta)$ represent $k^{\text{NCSI-FO}}(t,s;\eta)$ in this section. Note that the kernel is symmetric, so we only consider $k(t,s;\eta)$ with $t\geq s$. It follows from \eqref{eq:si_expre} that 
\begin{align*}
&k(t,s;\eta)=k_{c}(t,s;\eta)+ k_{\delta}(t,s;\eta)+ k_{a}(t,s;\eta),\\
&k_{c}(t,s;\eta)=\sum\limits_{k=1}^{\infty}\sigma^2_c \lambda_c^{k}g_0(t-k;\eta)g_0(s-k;\eta),\\
&k_\delta(t,s;\eta)=\sigma^2_0 g_0(t;\eta)g_0(s;\eta),\\
&k_{a}(t,s;\eta)=\sum\limits_{k=-\infty}^{-1}\sigma^2_a \lambda_a^{-k}g_0(t-k;\eta)g_0(s-k;\eta).
\end{align*}
Substituting \eqref{eq:ncsi_fo1} into  $g_0(\cdot)$, we obtain the closed form as follows.
\begin{align*}
&k_\delta(t,s;\eta)=\left\{
\begin{array}{lll}
 \sigma_0^2c_c^2a_c^{t+s},& t\geq s>0\\
 \sigma_0^2c_cc_0a_c^{t}, &t>s=0\\
 \sigma_0^2c_cc_aa_c^{t}a_a^{-s},& t>0>s\\
 \sigma_0^2c_0c_aa_a^{-s}, &t=0>s\\
 \sigma_0^2c_a^2a_a^{-t-s},& 0>t\geq s\\ 
\sigma_0^2c_0^2, &t=s=0
\end{array}\right. ,
\end{align*}
\begin{align*}
&k_c(t,s;\eta)=\left\{
\begin{array}{lll}
&\sum\limits_{i=1}^5 K_i(t,s), & t> s>1\\
&\sum\limits_{i=1}^4 K_i(t,s), & t>s=1\\
&\sum\limits_{i=1}^3 K_i(t,s), & t>1>s\\
&\sum\limits_{i=1}^2 K_i(t,s), &t=1>s\\
&\sum\limits_{i=1}^1 K_i(t,s), & 1>t> s\\
&K_1(t,t)+K_5(t,t)+K_6(t,t), &t=s>1\\
&K_1(t,t)+K_6(t,t), &t=s=1\\
&K_1(t,t), &1>t=s\\
\end{array}\right. ,
\end{align*}
with \begin{align*}
&K_1(t,s)=\sigma_c^2c_a^2 a_a^{-t-s}  \frac{(\lambda_ca_a^2)^{\max\{1,t+1\}}}{1-\lambda_ca_a^2},\\
&K_2(t,s)=  \sigma_c^2c_0c_a a_a^{t-s} \lambda_c^{t}  ,  \\
&K_3(t,s)= \sigma_c^2c_cc_a a_c^{t}a_a^{-s} \frac{(\lambda_ca_aa_c^{-1})^{\max\{s+1,1\}}-(\lambda_ca_aa_c^{-1})^t}{1-\lambda_ca_aa_c^{-1}}  ,\\
&K_4(t,s)= \sigma_c^2c_0c_c a_c^{t-s} \lambda_c^{s} ,\\
&K_5(t,s)= \sigma_c^2c_c^2a_c^{t+s}\frac{\lambda_ca_c^{-2} - (\lambda_ca_c^{-2})^s}{1-\lambda_ca_c^{-2}} , \\
&K_6(t,t)= \sigma_c^2 c_0^2\lambda_c^t .
\end{align*}
Note that $k_{a}(t,s;\eta)$ can be obtained in the same way as $k_{c}(t,s;\eta)$ because 
\begin{align*}
&k_a(t,s;\eta)=k_{c}(-t,-s;\eta^*), \\
&\eta = [a_c, a_a,c_c, c_0,c_a, \lambda_c,\lambda_a,\sigma_c, \sigma_a, \sigma_0],\\
&\eta^*= [a_a, a_c,c_a, c_0,c_c, \lambda_a,\lambda_c,\sigma_a, \sigma_c, \sigma_0].
\end{align*}
%{\color{blue}
%Alternative formula (not used, just for illustration)
%\begin{align*}
%&k_{c}(t,s)=\sigma_c^2\bigg(c_c^2a_c^{t+s}\frac{\lambda_ca_c^{-2} - (\lambda_ca_c^{-2})^s}{1-\lambda_ca_c^{-2}} \pmb I_{s>1}+\nonumber \\
%&c_0c_c a_c^{t-s} \lambda_c^{s} \pmb I_{s>0}\nonumber+\\
% &c_cc_a a_c^{t}a_a^{-s} \frac{(\lambda_ca_aa_c^{-1})^{\max\{s+1,1\}}-(\lambda_ca_aa_c^{-1})^t}{1-\lambda_ca_aa_c^{-1}} \pmb I_{t>1}+\nonumber \\
%& c_0c_a a_a^{t-s} \lambda_c^{t} \pmb I_{t>0} \nonumber+\\
% &c_a^2 a_a^{-t-s}  \frac{(\lambda_ca_a^2)^{\max\{1,t+1\}}}{1-\lambda_ca_a^2}\bigg) ,\quad  \text{ when }t>s  \\
%&k_{c}(t,t)=\sigma_c^2c_c^2 a_c^{t+t}\frac{\lambda_ca_c^{-2} - (\lambda_c a_c^{-2})^t}{1-\lambda_ca_c^{-2}} \pmb I_{t>1}+\nonumber \\
%&\sigma_c^2c_0^2 \lambda_c^{t} \pmb I_{t>0} +\nonumber\\
% &\sigma_c^2 c_a^2 a_a^{-t-t}\sigma_c^2  \frac{(\lambda_ca_a^2)^{\max\{1,t+1\}}}{1-\lambda_ca_a^2}  ,\quad \text{ when }t=s
%\end{align*}}

\begin{Remark}\label{re:sigma_0isone}
It is shown that $k_{c}(t,s)$, $k_{\delta}(t,s)$ and $k_{a}(t,s)$ all have the product $\sigma^2_{(\cdot)}c_{(\cdot)}c'_{(\cdot)}$ with $\sigma_{(\cdot)}\in \{\sigma_c,\sigma_a, \sigma_0$\} and $c_{(\cdot)},c'_{(\cdot)}\in \{c_c,c_a, c_0$\}. It follows that
\begin{align*}
&k^{\text{NCSI-FO}}(t,s;\eta_1)= k^{\text{NCSI-FO}}(t,s;\eta_2),\\
&\eta_1= [a_c, a_a,c_c, c_0,c_a, \lambda_c,\lambda_a,\sigma_c, \sigma_a, \sigma_0],\\
&\eta_2= [a_c, a_a,c_c\!\cdot\! \sigma_0, c_0\!\cdot\!\sigma_0,c_a\!\cdot\!\sigma_0, \lambda_c,\lambda_a,\frac{\sigma_c}{\sigma_0}, \frac{\sigma_a}{\sigma_0}, 1],
\end{align*}
This leads to the identifiability issue in the hyper-parameter estimation. Thus, $\sigma_0$ is always fixed to be 1 and not treated as the hyper-parameter. 
\end{Remark}




%\bibliographystyle{unsrt}
%\bibliography{database}



\begin{thebibliography}{10}

\bibitem{Zadeh:56}
L.~A. Zadeh.
\newblock On the identification problem.
\newblock {\em IRE Transactions on Circuit Theory}, 3:277--281, 1956.

\bibitem{Ljung:99}
L.~Ljung.
\newblock {\em System Identification - Theory for the User}.
\newblock Prentice-Hall, Upper Saddle River, N.J., 2nd edition, 1999.

\bibitem{SoderstromS:89}
T.~{S\"{o}derstr\"{o}m} and P.~Stoica.
\newblock {\em System Identification}.
\newblock Prentice-Hall Int., London, 1989.

\bibitem{PintelonS:01}
R.~Pintelon and J.~Schoukens.
\newblock {\em System Identification -- A Frequency Domain Approach}.
\newblock IEEE Press, New York, 2001.

\bibitem{Ljung:86}
L.~Ljung.
\newblock {\em The System Identification Toolbox: The Manual}.
\newblock The MathWorks Inc. 1st edition 1986, 4th edition 1995, Natick, MA,
  1995.

\bibitem{PDCDL14}
G.~Pillonetto, F.~Dinuzzo, T.~Chen, G.~De Nicolao, and L.~Ljung.
\newblock Kernel methods in system identification, machine learning and
  function estimation: A survey.
\newblock {\em Automatica}, 50(3):657--682, 2014.

\bibitem{Chiuso16}
A.~Chiuso.
\newblock Regularization and {B}ayesian learning in dynamical systems: {P}ast,
  present and future.
\newblock {\em Annual Reviews in Control}, 41:24 -- 38, 2016.

\bibitem{LCM20}
L.~Ljung, T.~Chen, and B.~Mu.
\newblock A shift in paradigm for system identification.
\newblock {\em International Journal of Control}, 93(2):173--180, 2020.

\bibitem{PCCDL22}
G.~Pillonetto, T.~Chen, A.~Chiusoand, G.~De Nicolao, and L.~Ljung.
\newblock {\em Regularized System Identification: Learning Dynamic Models from
  Data}.
\newblock Springer Nature, 2022.

\bibitem{CHEN18}
T.~Chen.
\newblock On kernel design for regularized lti system identification.
\newblock {\em Automatica}, 90:109--122, 2018.

\bibitem{PD10}
G.~Pillonetto and G.~De Nicolao.
\newblock A new kernel-based approach for linear system identification.
\newblock {\em Automatica}, 46(1):81--93, 2010.

\bibitem{ZC18}
Mattia Zorzi and Alessandro Chiuso.
\newblock The harmonic analysis of kernel functions.
\newblock {\em Automatica}, 94:125--137, 2018.

\bibitem{MSS16}
A.~Marconato, M.~Schoukens, and J.~Schoukens.
\newblock Filter-based regularisation for impulse response modelling.
\newblock {\em IET Control Theory $\&$ Applications}, 11:194--204, 2016.

\bibitem{COL12}
T.~Chen, H.~Ohlsson, and L.~Ljung.
\newblock On the estimation of transfer functions, regularizations and gaussian
  processesrevisited.
\newblock {\em Automatica}, 48(8):1525--1535, 2012.

\bibitem{BO20}
L.~Blanken and T.~Oomen.
\newblock Kernel-based identification of non-causal systems with application to
  inverse model control.
\newblock {\em Automatica}, 114, 2020.

\bibitem{ZDG96}
K.~Zhou, J.~C. Doyle, and K.~Glover.
\newblock {\em Robust and optimal control}.
\newblock Prentice-Hall, Inc., 1996.

\bibitem{GRH21}
R.~A. Gonz{\'a}lez, C.~R. Rojas, and H.~Hjalmarsson.
\newblock Non-causal regularized least-squares for continuous-time system
  identification with band-limited input excitations.
\newblock In {\em 2021 60th IEEE Conference on Decision and Control}, pages
  114--119, 2021.

\bibitem{Fujimoto22}
Y.~Fujimoto.
\newblock Kernel-based regularization for unstable systems.
\newblock In {\em 2022 IEEE 61st Conference on Decision and Control (CDC)},
  pages 209--214. IEEE, 2022.

\bibitem{CALCP14}
T.~Chen, M.~S. Andersen, L.~Ljung, A.~Chiuso, and G.~Pillonetto.
\newblock System identification via sparse multiple kernel-based regularization
  using sequential convex optimization techniques.
\newblock {\em IEEE Transactions on Automatic Control}, 59(11):2933--2945,
  2014.

\bibitem{CA21}
T.~Chen and M.~S. Andersen.
\newblock On semiseparable kernels and efficient implementation for regularized
  system identification and function estimation.
\newblock {\em Automatica}, 132:109682, 2021.

\bibitem{Verhaegen96}
M.~Verhaegen.
\newblock A subspace model identification solution to the identification of
  mixed causal, anti-causal lti systems.
\newblock {\em SIAM Journal on Matrix Analysis and Applications},
  17(2):332--347, 1996.

\bibitem{ZO18}
J.~van Zundert and T.~Oomen.
\newblock On inversion-based approaches for feedforward and ilc.
\newblock {\em Mechatronics}, 50:282--291, 2018.

\bibitem{CVT06}
C.~Carmeli, E.~De Vito, and A.~Toigo.
\newblock Vector valued reproducing kernel hilbert spaces of integrable
  functions and mercer theorem.
\newblock {\em Analysis and Applications}, 4(04):377--408, 2006.

\bibitem{VVM08a}
R.~Vandebril, M.~van Van~Barel, and N.~Mastronardi.
\newblock {\em Matrix Computations and Semiseparable Matrices: Linear Systems}.
\newblock Johns Hopkins University Press, 2008.

\bibitem{MC19}
M.~S. Andersen and T.~Chen.
\newblock Smoothing splines and rank structured matrix: Revisiting the spline
  kernel.
\newblock {\em SIAM Journal on Matrix Analysis and Applications},
  41(2):389--412, 2020.

\bibitem{Loeve78I}
M.~Lo\`{e}ve.
\newblock {\em Probability theory, Vol. {I}}.
\newblock Springer, 1977.

\bibitem{SS94}
P.K. Sen and J.M. Singer.
\newblock {\em Large sample methods in statistics: an introduction with
  applications}, volume~25.
\newblock CRC press, 1994.

\bibitem{DINUZZO15}
F.~Dinuzzo.
\newblock Kernels for linear time invariant system identification.
\newblock {\em SIAM Journal on Control and Optimization}, 53(5):3299--3317,
  2015.

\end{thebibliography}












%
%
%
%
%
%
%
%
%
%	
%	\cleardoublepage
%	\onecolumn
%	\setcounter{page}{1}
%	\begin{center}
%		\centering{\title{\large{Authors' reply for 22-1055}}}
%	\end{center}
%	
%	\vspace{1ex}
%	\renewcommand{\baselinestretch}{1.2}
%	\setcounter{equation}{0}
%	%    \setcounter{appendix}{0}
%	\renewcommand{\theequation}{AR-\arabic{equation}}
%    \setcounter{table}{0}
%	\renewcommand{\thetable}{AR-\arabic{table}}
%	%	\numberwithin{equation}{section}
%	\normalsize
%	\parskip=10pt
%	\parindent=0pt
%	\baselineskip=15pt
%	\renewcommand\labelitemii{$\circ$}
%	%\setlist[enumerate]{topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=0.7ex}
%	%\setlist[enumerate]{parsep=0.7ex}
%	%\renewenvironment{enumerate}{\begin{enumerate}
%	%		\setlength{\itemsep}{0pt}
%	%		\setlength{\parskip}{0.8ex}}
%	%	{\end{enumerate}}
%
%
%
%
%	
%
%We are very grateful to the editors and the referees for evaluating our first and second manuscript and for providing us with detailed comments and suggestions.  All the issues raised in the second reviewing reports have been addressed. In what follows, we detail the changes made with respect to the referees' suggestions and concerns. For convenience, the comments and suggestions of the editor and the reviewers are printed in \textcolor{blue}{blue}, and our replies are printed in black.
%	
%
%
%
%
%	
%\vskip 0.4 in
%\noindent \underbar{Replies to Comments and Suggestions of the Associate Editor}
%
%\begin{itemize}
%	\item \textcolor{blue}{The authors carefully addressed all the reviewers' comments and concerns. The quality of the paper has significantly improved, and the
%contribution with respect to previous works is now clearly presented. I
%confirm that the contribution is interesting and relevant.  }
%
%	First of all, thank you for your time handling our paper. In this revision, we have addressed the concerns from Reviewers
%\#13 and \#16 and revised the paper accordingly. 
%    %\vspace{1ex}	
%    
%    
%   \item \textcolor{blue}{ Reviewers
%\#13 and \#16 still provided interesting and valid comments to further
%improve paper readability and better focus the attention of the
%readers. Before final acceptance, I encourage the authors to take these
%comments into account.}
%   
%   In the revised version, we have taken all the comments from Reviewers
%\#13 and \#16 and revise the paper accordingly. Following their comments, we have removed two Remarks and rewritten some paragraphs to make the paper more precise.  Please refer to our detailed responses to Reviewer \#13 and \#16 below.
%
%\end{itemize}
%
%
%
%
%
%
%
%\newpage
%
%
%
%
%
%
%
%
%\vskip 0.4 in
%\noindent \underbar{Changes w.r.t.Comments and Suggestions of Reviewer 13}
%
%\begin{itemize}
%
%
%
%	\item \textcolor{blue}{I thank the Authors for carefully addressing my concerns. The paper
%dramatically increased in quality. I still think that
%the paper contribution is good, but is presented in a too prolix way.}
%
% Thank you for your time reviewing our paper. Following the comments, we have removed two Remarks and rewritten some paragraphs to make the paper more precise.  The number of pages is reduced from 16.5 to 15.8.  Please refer to the detailed response below. 
%
%    	\item \textcolor{blue}{
%- the Introduction can be more concise}
%
% Thank you for your comment. We have shortened Introduction as suggested to make it more concise.  
% 
%	
%	
%\item \textcolor{blue}{
%-the comparison with \cite{BO20} is repeated many times throughout the paper.}
%
%  Thank you for your comment.  We have reduced the number of ``the comparison with \cite{BO20}'' in the revision and only kept the necessary ones. We have to remind that  the comparison with \cite{BO20} is  suggested by AE and other reviewers in the first round of the review to highlight the difference between this paper and \cite{BO20}.  
%
%\item\textcolor{blue}{-is \eqref{eq:causal_kernel} needed?}
%
%  Thank you for your question.  We think \eqref{eq:causal_kernel} is necessary for the following two reasons. First, we use lots of the suffix
%  ``TC'' and ``DC'' throughout this paper, and their origin had better be introduced. Second, the DC kernel \eqref{eq:causal_dc_kernel} and TC kernel \eqref{eq:causal_tc_kernel} are instructional for kernel simplification in Section \ref{sec:simplification}, see the paragraph below \eqref{eq:kernel_si_dc}.
%  
%\item\textcolor{blue}{- Section \ref{se:pb_for}  can be more focused and precise.}
% 
%   Thank you for your comment.  We have shortened Section \ref{se:pb_for}.
%   
%   
%\item\textcolor{blue}{-(Section 3, old version) can be shortened and included in (Section 4, old version) }
%
%  Thank you for your comment.  We have shortened (Section 3, old version) and merged it into Section \ref{se:ncsi_kernel}.
%  
%
%\item\textcolor{blue}{-The explanation in Section \ref{se:ncsi_kerenl} can be more concise and less didactic}
%
%  Thank you for your comment. We have removed some sentence in this section.  However, this explanation in Section \ref{se:ncsi_kerenl}  is written with details due to the comments from Review \#11 ``the consideration about stochasticity in the last paragraph before 
%Section \ref{se:nominal_model} is not very clear'' and Review \#12  ``Section \ref{se:ncsi_kerenl} is less self-contained with respect to the rest of the
%paper. ''. 
%  
%  
%\item\textcolor{blue}{- (Remark 4.1, old version) should act as a preamble for Lemma \ref{le:ss_model} -- but it could
%also be removed.}
%
% Thank you for your comment. We have removed (Remark 4.1, old version). 
% 
%\item\textcolor{blue}{-(Remark 4.4, old version) contains an interesting point that does have space to be
%devoted to: if such a discussion is not to be carried out, then the
%Remark
%should be removed}
%
%  Thank you for your question. We have removed (Remark 4.4, old version). 
%
%  
%  
%
%\item\textcolor{blue}{-Section 4.4: think of separating the text into paragraphs, and give
%each paragraph the title of the kind of information it is encoded in
%the kernel}
%
%Thank you for your comment.  We have added the subtitle for each type of information as suggested. 
%
%\item\textcolor{blue}{-Figure \ref{fig:surf_kernel} is not very informative if not commented}
%
%Thank you for your comment.  We have modified Figure \ref{fig:surf_kernel} to make sharp contrast among kernels, which makes it more informative. Besides, we also comment in the caption to stress the significant difference of the NCSI-TC kernel \eqref{eq:kernel_si_tc}.  
%
%
%\item\textcolor{blue}{-Results in Section \ref{se:simulation} are presented in a raw state. In general,
%comparing all kernels at once is difficult to read, especially when it
%comes to tables (which I think can be removed). It is interesting that
%the trend in data-sets D1,D2,D3 is consistent among the kernels -- this
%should be commented, and used to select a subset of kernels whose
%performance is to be displayed. Also, the list in Section \ref{se:result_and_finding} comments
%the plots and is related to the specific data-sets, while it is
%informative to have general take-home messages on which kernel(s) are
%to be used.}
%
%Thank you for your comment and suggestions. We have revised the results in Section \ref{se:result_and_finding} to make it more clear. More specifically, 
%\begin{itemize}
%\item The tables and boxplots of FIT are both important for system identification, see e.g. \cite{PD10, PDCDL14}, because tables provide the mean and standard deviation, while the box plots show the different statistics, i.e.  median,  quantiles, and outliers. 
%Moreover, we have to remind that Reviewer \# 4 have asked us to add standard deviation in tables to enrich the result. 
%\item we point out the consistent trend and conclude the best kernel in (a). Afterwards, we discuss two NCSI kernels in (b).
%\item we summarize the take-home messages at the end of each list, marked with ``Thus, ...''.
%\end{itemize}
%
%
%
%
%
%
%\item\textcolor{blue}{(Appendix 4.1, old version) is empty  }
%
%Thank you for your comment.  We have removed the empty (Appendix 4.1, old version)
%
%   \end{itemize}
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%\newpage
%
%
%
%
%
%\vskip 0.4 in
%\noindent \underbar{Changes w.r.t.Comments and Suggestions of Reviewer \#14}
%
%\begin{itemize}
%\item\textcolor{blue}{Section A.1 Proof of Lemma 4.1 is repeated twice. I don't have any
%further comments or suggestions for the authors.  }
%
%   Thank you for your time reviewing our paper. We have removed the repeated title in the Appendix. 
%
%
%
%
%
%
%
%\newpage
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%\end{itemize}
%
%\vskip 0.4 in
%\noindent \underbar{Changes w.r.t.Comments and Suggestions of Reviewer \#15}
%\begin{itemize}
%
%\item\textcolor{blue}{
%The authors have appropriately addressed my comments.
%I am convinced that the paper makes a nice contribution, and its
%relation to prior works is now clearly stated. 
%}
%
%   Thank you for your time reviewing our paper. 
%\end{itemize}
%
%
%
%
%
%
%\newpage
%
%
%
%
%
%
%
%
%
%
%
%\vskip 0.4 in
%\noindent \underbar{Changes w.r.t.Comments and Suggestions of Reviewer \#16}
%\begin{itemize}
%
%
%
%\item\textcolor{blue}{
%All my concerns have been adequately addressed, and the mistakes or
%inaccuracies I found in the previous version pertained to parts of the
%paper which have been removed.
%By slightly correcting the focus of the paper, the narrative has
%significantly improved and now it is much smoother to read.I agree with the motivation behind the first addition (blue part) in
%the introduction, but it could have been better written. 
%}
%
%
%Thank you for your time reviewing our paper and providing valuable comments. 
%We have revised the paper as suggested. 
%
%\item\textcolor{blue}{
%-  I would rephrase questions in (Remark 4.1, old version) as indirect questions.
%}
%
%Thank you for your suggestion. We are sorry that (Remark 4.1, old version) is suggested to removed by Reviewer \#13, and we have removed it in the revision. 
%
%\item\textcolor{blue}{
%-  From the abstract, it still looks like that the main reason behind
%this work can be found in inverse model control, which does not fit
%well the new narrative.
%}
%
%Thank you for your suggestion. We have rewritten the abstract  to delete the contents about inverse model control. 
%
%
%\item\textcolor{blue}{
%-  There are now many remarks: some of them could also be merged with
%the main body, for instance (Remark 2.1, old version) and (Remark 5.1, old version). 
%}
%
%Thank you for your suggestion. We have merged (Remark 2.1, old version) and (Remark 5.1, old version) into the main body, and deleted (Remark 4.1, old version) and (Remark 4.4, old version)  as suggested by Reviewer \#13. 
%
%
%\end{itemize}
%% \bibliographystyle{unsrt}
%% \bibliography{database}



\end{document}