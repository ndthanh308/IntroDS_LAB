

@article{mauck2025best,
  author       = {Mauck, MC and Barth, KS and Bell, KM and Brooks, AK and Chadwick, AL and Gunn, CA and Hurley, RW and Ivanova, A and Piva, SR and Schneider, MJ and Bailey, JF and Bagaason, S and Batorsky, A and Borckardt, JJ and Bowden, AE and Carey, TS and Castellanos, J and Chen, L and Chidgey, B and Dalton, S and Dufour, JS and Fields, AJ and Fritz, JM and Goolsby, RW and Greco, CM and Harris, RE and Harte, S and Hassett, AL and Hoffmeyer, A and Berkeley, SJ and Kaplan, C and Kidwell, KM and Knapik, GG and Kosorok, MR and Kurillo, G and Lobo, R and Lotz, JC and Mackey, S and Mageswaran, P and Majumdar, S and Mao, J and Marras, WS and McCumber, M and McLean, SA and Mehling, W and Mitchell, UH and Napadow, VJ and O’Neill, C and Patel, KV and Peltier, S and Psioda, M and Rowland, B and Rundell, SD and Schrepf, A and Sperger, J and Vo, N and Wallace, MS and Wasan, AD and Weaver, TE and Weber, KA and Williams, DA and Wilson, L and Zeidan, F and Zhao, B and Anstrom, KJ and Clauw, DJ and Sowa, GA},
  title        = {The Design and Rationale of the Biomarkers for Evaluating Spine Treatments (BEST) Trial},
  journal      = {Pain Medicine},
  year         = {2025},
  note         = {Accepted for publication March 12, 2025}
}



@article{wilson_preferences_2024,
	title = {Preferences for risks and benefits of treatment outcomes for chronic low back pain: Choice‐based conjoint measure development and discrete choice experiment},
	volume = {16},
	issn = {1934-1482, 1934-1563},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/pmrj.13112},
	doi = {10.1002/pmrj.13112},
	shorttitle = {Preferences for risks and benefits of treatment outcomes for chronic low back pain},
	pages = {836--847},
	number = {8},
	journaltitle = {{PM}\&R},
	shortjournal = {{PM}\&R},
	author = {Wilson, Leslie and Denham, Alina and Ionova, Yelena and O'Neill, Conor and Greco, Carol M. and Hassett, Afton L. and Hanmer, Janel and Shaikh, Sana and Wolf, Mehling and Berven, Sigurd and Williams, David and Ma, Yanlei and Lotz, Jeffrey and Zheng, Patricia},
	urldate = {2025-03-27},
	date = {2024-08},
	langid = {english},
	file = {Full Text:/Users/yatingzou/Zotero/storage/HKR2EWUH/Wilson et al. - 2024 - Preferences for risks and benefits of treatment ou.pdf:application/pdf},
}


@article{Wank2024PRPP,
  author       = {Marianthie Wank and Sarah Medley and Roy N. Tamura and Thomas M. Braun and Kelley M. Kidwell},
  title        = {A Partially Randomized Patient Preference, Sequential, Multiple‐Assignment, Randomized Trial Design Analyzed via Weighted and Replicated Frequentist and Bayesian Methods},
  journal      = {Statistics in Medicine},
  volume       = {43},
  number       = {30},
  pages        = {5777--5790},
  year         = {2024},
  doi          = {10.1002/sim.10276},
  publisher    = {John Wiley \& Sons},
  url          = {https://doi.org/10.1002/sim.10276}
}


@Article{Zhong2021,
AUTHOR = {Zhong, Yingchao and Wang, Chang and Wang, Lu},
TITLE = {Survival Augmented Patient Preference Incorporated Reinforcement Learning to Evaluate Tailoring Variables for Personalized Healthcare},
JOURNAL = {Stats},
VOLUME = {4},
YEAR = {2021},
NUMBER = {4},
PAGES = {776--792},
URL = {https://www.mdpi.com/2571-905X/4/4/46},
ISSN = {2571-905X},
ABSTRACT = {In this paper, we consider personalized treatment decision strategies in the management of chronic diseases, such as chronic kidney disease, which typically consists of sequential and adaptive treatment decision making. We investigate a two-stage treatment setting with a survival outcome that could be right censored. This can be formulated through a dynamic treatment regime (DTR) framework, where the goal is to tailor treatment to each individual based on their own medical history in order to maximize a desirable health outcome. We develop a new method, Survival Augmented Patient Preference incorporated reinforcement Q-Learning (SAPP-Q-Learning) to decide between quality of life and survival restricted at maximal follow-up. Our method incorporates the latent patient preference into a weighted utility function that balances between quality of life and survival time, in a Q-learning model framework. We further propose a corresponding m-out-of-n Bootstrap procedure to accurately make statistical inferences and construct confidence intervals on the effects of tailoring variables, whose values can guide personalized treatment strategies.},
DOI = {10.3390/stats4040046}
}


@article{sperger2025Statistical,
  title = {Statistical Design and Rationale of the Biomarkers for Evaluating Spine Treatments (BEST) Trial},
                journal = {Under Review},
                year = {2025},
                author = {John Sperger and
                Kelley M. Kidwell and
                Matthew C. Mauck and
                Beibo Zhao and
                Kevin J. Anstrom and
                Anna Batorsky and
                Timothy S. Carey and
                Daniel J. Clauw and
                Nikki L. B. Freeman and
                Carol M. Greco and
                Anastasia Ivanova and
                Sara Jones Berkeley and
                Samuel A. McLean and
                Matthew A. Psioda and
                Bryce Rowland and
                Gwendolyn A. Sowa and
                Ajay D. Wasan and
                Joshua P. Zitovsky and
                Michael R. Kosorok
  }
}

@book{gelman_bayesian_2014,
	location = {Boca Raton London New York},
	edition = {Third edition},
	title = {Bayesian data analysis},
	isbn = {978-1-4398-4095-5},
	series = {Texts in statistical science series},
	abstract = {Preface This book is intended to have three roles and to serve three associated audiences: an introductory text on Bayesian inference starting from first principles, a graduate text on effective current approaches to Bayesian modeling and computation in statistics and related fields, and a handbook of Bayesian methods in applied statistics for general users of and researchers in applied statistics. Although introductory in its early sections, the book is definitely not elementary in the sense of a first text in statistics. The mathematics used in our book is basic probability and statistics, elementary calculus, and linear algebra. A review of probability notation is given in Chapter 1 along with a more detailed list of topics assumed to have been studied. The practical orientation of the book means that the reader's previous experience in probability, statistics, and linear algebra should ideally have included strong computational components. To write an introductory text alone would leave many readers with only a taste of the conceptual elements but no guidance for venturing into genuine practical applications, beyond those where Bayesian methods agree essentially with standard non-Bayesian analyses. On the other hand, we feel it would be a mistake to present the advanced methods without first introducing the basic concepts from our data-analytic perspective. Furthermore, due to the nature of applied statistics, a text on current Bayesian methodology would be incomplete without a variety of worked examples drawn from real applications. To avoid cluttering the main narrative, there are bibliographic notes at the end of each chapter and references at the end of the book},
	pagetotal = {667},
	publisher = {{CRC} Press, Taylor and Francis Group},
	author = {Gelman, Andrew and Carlin, John B. and Stern, Hal S. and Dunson, David B. and Vehtari, Aki and Rubin, Donald B.},
	date = {2014},
}


@Article{Yingchao2021,
AUTHOR = {Zhong, Yingchao and Wang, Chang and Wang, Lu},
TITLE = {Survival Augmented Patient Preference Incorporated Reinforcement Learning to Evaluate Tailoring Variables for Personalized Healthcare},
JOURNAL = {Stats},
VOLUME = {4},
YEAR = {2021},
NUMBER = {4},
PAGES = {776--792},
URL = {https://www.mdpi.com/2571-905X/4/4/46},
ISSN = {2571-905X},
DOI = {10.3390/stats4040046}
}


@article{Scornet2015,
 ISSN = {00905364},
 URL = {http://www.jstor.org/stable/43556658},
 author = {Erwan Scornet and Gérard Biau and Jean-Philippe Vert},
 journal = {The Annals of Statistics},
 number = {4},
 pages = {1716--1741},
 publisher = {Institute of Mathematical Statistics},
 title = {CONSISTENCY OF RANDOM FORESTS},
 urldate = {2025-03-23},
 volume = {43},
 year = {2015}
}


@article{breiman2001,
  author    = {Breiman, Leo},
  title     = {Random Forests},
  journal   = {Machine Learning},
  volume    = {45},
  number    = {2},
  pages     = {5--32},
  year      = {2001},
  month = {10},
  publisher = {Kluwer Academic Publishers},
  doi       = {10.1093/schbul/13.2.261}
}

@article{Probst2018HyperparametersAT,
  title={Hyperparameters and tuning strategies for random forest},
  author={Philipp Probst and Marvin N. Wright and Anne‐Laure Boulesteix},
  journal={Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  year={2018},
  volume={9},
  url={https://api.semanticscholar.org/CorpusID:4753950}
}

@article{kay1987positive,
  author    = {Stanley R. Kay and Lewis A. Opler and Abraham Fiszbein},
  title     = {The Positive and Negative Syndrome Scale (PANSS) for schizophrenia},
  journal   = {Schizophrenia Bulletin},
  volume    = {13},
  number    = {2},
  pages     = {261--276},
  year      = {1987},
  publisher = {Oxford University Press},
  doi       = {10.1093/schbul/13.2.261}
}


@article{Leeper2019MeasuringSP,
  title={Measuring Subgroup Preferences in Conjoint Experiments},
  author={Thomas J. Leeper and Sara Binzer Hobolt and James Tilley},
  journal={Political Analysis},
  year={2019},
  volume={28},
  pages={207 - 221},
  url={https://api.semanticscholar.org/CorpusID:195507113}
}

@article{Liu2023MultipleHT,
  title={Multiple Hypothesis Testing in Conjoint Analysis},
  author={Guoer Liu and Yuki Shiraito},
  journal={Political Analysis},
  year={2023},
  volume={31},
  pages={380 - 395},
  url={https://api.semanticscholar.org/CorpusID:256318418}
}

@article{Janssen2017ImprovingTQ,
  title={Improving the quality of discrete-choice experiments in health: how can we assess validity and reliability?},
  author={Ellen M. Janssen and Deborah A. Marshall and A. Brett Hauber and John F. P. Bridges},
  journal={Expert Review of Pharmacoeconomics \& Outcomes Research},
  year={2017},
  volume={17},
  pages={531 - 542},
  url={https://api.semanticscholar.org/CorpusID:8008771}
}

@article{furnkranz_preference-based_2012,
	title = {Preference-based reinforcement learning: a formal framework and a policy iteration algorithm},
	volume = {89},
	rights = {http://www.springer.com/tdm},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/10.1007/s10994-012-5313-8},
	doi = {10.1007/s10994-012-5313-8},
	shorttitle = {Preference-based reinforcement learning},
	abstract = {This paper makes a ﬁrst step toward the integration of two subﬁelds of machine learning, namely preference learning and reinforcement learning ({RL}). An important motivation for a preference-based approach to reinforcement learning is the observation that in many real-world domains, numerical feedback signals are not readily available, or are deﬁned arbitrarily in order to satisfy the needs of conventional {RL} algorithms. Instead, we propose an alternative framework for reinforcement learning, in which qualitative reward signals can be directly used by the learner. The framework may be viewed as a generalization of the conventional {RL} framework in which only a partial order between policies is required instead of the total order induced by their respective expected long-term reward.},
	pages = {123--156},
	number = {1},
	journaltitle = {Machine Learning},
	shortjournal = {Mach Learn},
	author = {Fürnkranz, Johannes and Hüllermeier, Eyke and Cheng, Weiwei and Park, Sang-Hyeun},
	urldate = {2025-02-23},
	date = {2012-10},
	langid = {english},
	file = {Fürnkranz et al. - 2012 - Preference-based reinforcement learning a formal .pdf:/Users/yatingzou/Zotero/storage/JI99Y43L/Fürnkranz et al. - 2012 - Preference-based reinforcement learning a formal .pdf:application/pdf},
}

@misc{kupcsik_learning_2016,
	title = {Learning Dynamic Robot-to-Human Object Handover from Human Feedback},
	url = {http://arxiv.org/abs/1603.06390},
	doi = {10.48550/arXiv.1603.06390},
	abstract = {Object handover is a basic, but essential capability for robots interacting with humans in many applications, e.g., caring for the elderly and assisting workers in manufacturing workshops. It appears deceptively simple, as humans perform object handover almost ﬂawlessly. The success of humans, however, belies the complexity of object handover as collaborative physical interaction between two agents with limited communication. This paper presents a learning algorithm for dynamic object handover, for example, when a robot hands over water bottles to marathon runners passing by the water station. We formulate the problem as contextual policy search, in which the robot learns object handover by interacting with the human. A key challenge here is to learn the latent reward of the handover task under noisy human feedback. Preliminary experiments show that the robot learns to hand over a water bottle naturally and that it adapts to the dynamics of human motion. One challenge for the future is to combine the model-free learning algorithm with a model-based planning approach and enable the robot to adapt over human preferences and object characteristics, such as shape, weight, and surface texture.},
	number = {{arXiv}:1603.06390},
	publisher = {{arXiv}},
	author = {Kupcsik, Andras and Hsu, David and Lee, Wee Sun},
	urldate = {2025-02-23},
	date = {2016-03-21},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1603.06390 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Kupcsik et al. - 2016 - Learning Dynamic Robot-to-Human Object Handover fr.pdf:/Users/yatingzou/Zotero/storage/VE5DDY4F/Kupcsik et al. - 2016 - Learning Dynamic Robot-to-Human Object Handover fr.pdf:application/pdf},
}

@article{wirth_survey_2017,
	title = {A Survey of Preference-Based Reinforcement Learning Methods},
	abstract = {Reinforcement learning ({RL}) techniques optimize the accumulated long-term reward of a suitably chosen reward function. However, designing such a reward function often requires a lot of task-speciﬁc prior knowledge. The designer needs to consider diﬀerent objectives that do not only inﬂuence the learned behavior but also the learning progress. To alleviate these issues, preference-based reinforcement learning algorithms ({PbRL}) have been proposed that can directly learn from an expert’s preferences instead of a hand-designed numeric reward. {PbRL} has gained traction in recent years due to its ability to resolve the reward shaping problem, its ability to learn from non numeric rewards and the possibility to reduce the dependence on expert knowledge. We provide a uniﬁed framework for {PbRL} that describes the task formally and points out the diﬀerent design principles that aﬀect the evaluation task for the human as well as the computational complexity. The design principles include the type of feedback that is assumed, the representation that is learned to capture the preferences, the optimization problem that has to be solved as well as how the exploration/exploitation problem is tackled. Furthermore, we point out shortcomings of current algorithms, propose open research questions and brieﬂy survey practical tasks that have been solved using {PbRL}.},
	author = {Wirth, Christian and Akrour, Riad and Neumann, Gerhard and Fürnkranz, Johannes},
	date = {2017},
	langid = {english},
	file = {Wirth et al. - A Survey of Preference-Based Reinforcement Learnin.pdf:/Users/yatingzou/Zotero/storage/EB5BSM9S/Wirth et al. - A Survey of Preference-Based Reinforcement Learnin.pdf:application/pdf},
}

@misc{christiano_deep_2023,
	title = {Deep reinforcement learning from human preferences},
	url = {http://arxiv.org/abs/1706.03741},
	doi = {10.48550/arXiv.1706.03741},
	abstract = {For sophisticated reinforcement learning ({RL}) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals deﬁned in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex {RL} tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than 1\% of our agent’s interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art {RL} systems. To demonstrate the ﬂexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any which have been previously learned from human feedback.},
	number = {{arXiv}:1706.03741},
	publisher = {{arXiv}},
	author = {Christiano, Paul and Leike, Jan and Brown, Tom B. and Martic, Miljan and Legg, Shane and Amodei, Dario},
	urldate = {2025-02-23},
	date = {2023-02-17},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1706.03741 [stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Christiano et al. - 2023 - Deep reinforcement learning from human preferences.pdf:/Users/yatingzou/Zotero/storage/86LGR8CL/Christiano et al. - 2023 - Deep reinforcement learning from human preferences.pdf:application/pdf},
}

@article{ren_efficient_2022,
	title = {Efficient Meta Reinforcement Learning for Preference-based Fast Adaptation},
	abstract = {Learning new task-specific skills from a few trials is a fundamental challenge for artificial intelligence. Meta reinforcement learning (meta-{RL}) tackles this problem by learning transferable policies that support few-shot adaptation to unseen tasks. Despite recent advances in meta-{RL}, most existing methods require the access to the environmental reward function of new tasks to infer the task objective, which is not realistic in many practical applications. To bridge this gap, we study the problem of few-shot adaptation in the context of human-in-the-loop reinforcement learning. We develop a meta-{RL} algorithm that enables fast policy adaptation with preference-based feedback. The agent can adapt to new tasks by querying human’s preference between behavior trajectories instead of using per-step numeric rewards. By extending techniques from information theory, our approach can design query sequences to maximize the information gain from human interactions while tolerating the inherent error of non-expert human oracle. In experiments, we extensively evaluate our method, Adaptation with Noisy {OracLE} ({ANOLE}), on a variety of meta-{RL} benchmark tasks and demonstrate substantial improvement over baseline algorithms in terms of both feedback efficiency and error tolerance.},
	author = {Ren, Zhizhou and Liu, Anji and Liang, Yitao and Peng, Jian and Ma, Jianzhu},
	date = {2022},
	langid = {english},
	file = {Ren et al. - Efficient Meta Reinforcement Learning for Preferen.pdf:/Users/yatingzou/Zotero/storage/WWAZFJZZ/Ren et al. - Efficient Meta Reinforcement Learning for Preferen.pdf:application/pdf},
}

@misc{park_surf_2022,
	title = {{SURF}: Semi-supervised Reward Learning with Data Augmentation for Feedback-efficient Preference-based Reinforcement Learning},
	url = {http://arxiv.org/abs/2203.10050},
	doi = {10.48550/arXiv.2203.10050},
	shorttitle = {{SURF}},
	abstract = {Preference-based reinforcement learning ({RL}) has shown potential for teaching agents to perform the target tasks without a costly, pre-deﬁned reward function by learning the reward with a supervisor’s preference between the two agent behaviors. However, preference-based learning often requires a large amount of human feedback, making it difﬁcult to apply this approach to various applications. This data-efﬁciency problem, on the other hand, has been typically addressed by using unlabeled samples or data augmentation techniques in the context of supervised learning. Motivated by the recent success of these approaches, we present {SURF}, a semi-supervised reward learning framework that utilizes a large amount of unlabeled samples with data augmentation. In order to leverage unlabeled samples for reward learning, we infer pseudo-labels of the unlabeled samples based on the conﬁdence of the preference predictor. To further improve the label-efﬁciency of reward learning, we introduce a new data augmentation that temporally crops consecutive subsequences from the original behaviors. Our experiments demonstrate that our approach signiﬁcantly improves the feedback-efﬁciency of the state-ofthe-art preference-based method on a variety of locomotion and robotic manipulation tasks.},
	number = {{arXiv}:2203.10050},
	publisher = {{arXiv}},
	author = {Park, Jongjin and Seo, Younggyo and Shin, Jinwoo and Lee, Honglak and Abbeel, Pieter and Lee, Kimin},
	urldate = {2025-02-23},
	date = {2022-03-18},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2203.10050 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Park et al. - 2022 - SURF Semi-supervised Reward Learning with Data Au.pdf:/Users/yatingzou/Zotero/storage/VF3FJ7H8/Park et al. - 2022 - SURF Semi-supervised Reward Learning with Data Au.pdf:application/pdf},
}

@misc{an_direct_2023,
	title = {Direct Preference-based Policy Optimization without Reward Modeling},
	url = {http://arxiv.org/abs/2301.12842},
	doi = {10.48550/arXiv.2301.12842},
	abstract = {Preference-based reinforcement learning ({PbRL}) is an approach that enables {RL} agents to learn from preference, which is particularly useful when formulating a reward function is challenging. Existing {PbRL} methods generally involve a two-step procedure: they first learn a reward model based on given preference data and then employ off-the-shelf reinforcement learning algorithms using the learned reward model. However, obtaining an accurate reward model solely from preference information, especially when the preference is from human teachers, can be difficult. Instead, we propose a {PbRL} algorithm that directly learns from preference without requiring any reward modeling. To achieve this, we adopt a contrastive learning framework to design a novel policy scoring metric that assigns a high score to policies that align with the given preferences. We apply our algorithm to offline {RL} tasks with actual human preference labels and show that our algorithm outperforms or is on par with the existing {PbRL} methods. Notably, on high-dimensional control tasks, our algorithm surpasses offline {RL} methods that learn with ground-truth reward information. Finally, we show that our algorithm can be successfully applied to fine-tune large language models.},
	number = {{arXiv}:2301.12842},
	publisher = {{arXiv}},
	author = {An, Gaon and Lee, Junhyeok and Zuo, Xingdong and Kosaka, Norio and Kim, Kyung-Min and Song, Hyun Oh},
	urldate = {2025-02-23},
	date = {2023-10-27},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2301.12842 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {An et al. - 2023 - Direct Preference-based Policy Optimization withou.pdf:/Users/yatingzou/Zotero/storage/CXQXCF3C/An et al. - 2023 - Direct Preference-based Policy Optimization withou.pdf:application/pdf},
}

@article{hejna_inverse_2023,
	title = {Inverse Preference Learning: Preference-based {RL} without a Reward Function},
	abstract = {Reward functions are difficult to design and often hard to align with human intent. Preference-based Reinforcement Learning ({RL}) algorithms address these problems by learning reward functions from human feedback. However, the majority of preference-based {RL} methods naïvely combine supervised reward models with off-the-shelf {RL} algorithms. Contemporary approaches have sought to improve performance and query complexity by using larger and more complex reward architectures such as transformers. Instead of using highly complex architectures, we develop a new and parameter-efficient algorithm, Inverse Preference Learning ({IPL}), specifically designed for learning from offline preference data. Our key insight is that for a fixed policy, the ��-function encodes all information about the reward function, effectively making them interchangeable. Using this insight, we completely eliminate the need for a learned reward function. Our resulting algorithm is simpler and more parameter-efficient. Across a suite of continuous control and robotics benchmarks, {IPL} attains competitive performance compared to more complex approaches that leverage transformer-based and non-Markovian reward functions while having fewer algorithmic hyperparameters and learned network parameters. Our code is publicly released1.},
	author = {Hejna, Joey and Sadigh, Dorsa},
	date = {2023},
	langid = {english},
	file = {Hejna and Sadigh - Inverse Preference Learning Preference-based RL w.pdf:/Users/yatingzou/Zotero/storage/QQYU4AMX/Hejna and Sadigh - Inverse Preference Learning Preference-based RL w.pdf:application/pdf},
}

@misc{zhan_provable_2023,
	title = {Provable Offline Preference-Based Reinforcement Learning},
	url = {http://arxiv.org/abs/2305.14816},
	doi = {10.48550/arXiv.2305.14816},
	abstract = {In this paper, we investigate the problem of ofﬂine Preference-based Reinforcement Learning ({PbRL}) with human feedback where feedback is available in the form of preference between trajectory pairs rather than explicit rewards. Our proposed algorithm consists of two main steps: (1) estimate the implicit reward using Maximum Likelihood Estimation ({MLE}) with general function approximation from ofﬂine data and (2) solve a distributionally robust planning problem over a conﬁdence set around the {MLE}. We consider the general reward setting where the reward can be deﬁned over the whole trajectory and provide a novel guarantee that allows us to learn any target policy with a polynomial number of samples, as long as the target policy is covered by the ofﬂine data. This guarantee is the ﬁrst of its kind with general function approximation. To measure the coverage of the target policy, we introduce a new single-policy concentrability coefﬁcient, which can be upper bounded by the per-trajectory concentrability coefﬁcient. We also establish lower bounds that highlight the necessity of such concentrability and the difference from standard {RL}, where state-action-wise rewards are directly observed. We further extend and analyze our algorithm when the feedback is given over action pairs.},
	number = {{arXiv}:2305.14816},
	publisher = {{arXiv}},
	author = {Zhan, Wenhao and Uehara, Masatoshi and Kallus, Nathan and Lee, Jason D. and Sun, Wen},
	urldate = {2025-02-23},
	date = {2023-09-29},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2305.14816 [cs]},
	keywords = {Computer Science - Machine Learning, Mathematics - Statistics Theory, Statistics - Machine Learning, Statistics - Statistics Theory},
	file = {Zhan et al. - 2023 - Provable Offline Preference-Based Reinforcement Le.pdf:/Users/yatingzou/Zotero/storage/2DUM83CM/Zhan et al. - 2023 - Provable Offline Preference-Based Reinforcement Le.pdf:application/pdf},
}

@misc{hejna_contrastive_2024,
	title = {Contrastive Preference Learning: Learning from Human Feedback without {RL}},
	url = {http://arxiv.org/abs/2310.13639},
	doi = {10.48550/arXiv.2310.13639},
	shorttitle = {Contrastive Preference Learning},
	abstract = {Reinforcement Learning from Human Feedback ({RLHF}) has emerged as a popular paradigm for aligning models with human intent. Typically {RLHF} algorithms operate in two phases: first, use human preferences to learn a reward function and second, align the model by optimizing the learned reward via reinforcement learning ({RL}). This paradigm assumes that human preferences are distributed according to reward, but recent work suggests that they instead follow the regret under the user’s optimal policy. Thus, learning a reward function from feedback is not only based on a flawed assumption of human preference, but also leads to unwieldy optimization challenges that stem from policy gradients or bootstrapping in the {RL} phase. Because of these optimization challenges, contemporary {RLHF} methods restrict themselves to contextual bandit settings (e.g., as in large language models) or limit observation dimensionality (e.g., state-based robotics). We overcome these limitations by introducing a new family of algorithms for optimizing behavior from human feedback using the regret-based model of human preferences. Using the principle of maximum entropy, we derive Contrastive Preference Learning ({CPL}), an algorithm for learning optimal policies from preferences without learning reward functions, circumventing the need for {RL}. {CPL} is fully off-policy, uses only a simple contrastive objective, and can be applied to arbitrary {MDPs}. This enables {CPL} to elegantly scale to high-dimensional and sequential {RLHF} problems while being simpler than prior methods.},
	number = {{arXiv}:2310.13639},
	publisher = {{arXiv}},
	author = {Hejna, Joey and Rafailov, Rafael and Sikchi, Harshit and Finn, Chelsea and Niekum, Scott and Knox, W. Bradley and Sadigh, Dorsa},
	urldate = {2025-02-23},
	date = {2024-04-30},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2310.13639 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Hejna et al. - 2024 - Contrastive Preference Learning Learning from Hum.pdf:/Users/yatingzou/Zotero/storage/4Y82HH8M/Hejna et al. - 2024 - Contrastive Preference Learning Learning from Hum.pdf:application/pdf},
}

@misc{pace_preference_2024,
	title = {Preference Elicitation for Offline Reinforcement Learning},
	url = {http://arxiv.org/abs/2406.18450},
	doi = {10.48550/arXiv.2406.18450},
	abstract = {Applying reinforcement learning ({RL}) to real-world problems is often made challenging by the inability to interact with the environment and the difficulty of designing reward functions. Offline {RL} addresses the first challenge by considering access to an offline dataset of environment interactions labeled by the reward function. In contrast, Preference-based {RL} does not assume access to the reward function and learns it from preferences, but typically requires an online interaction with the environment. We bridge the gap between these frameworks by exploring efficient methods for acquiring preference feedback in a fully offline setup. We propose Sim-{OPRL}, an offline preference-based reinforcement learning algorithm, which leverages a learned environment model to elicit preference feedback on simulated rollouts. Drawing on insights from both the offline {RL} and the preference-based {RL} literature, our algorithm employs a pessimistic approach for out-of-distribution data, and an optimistic approach for acquiring informative preferences about the optimal policy. We provide theoretical guarantees regarding the sample complexity of our approach, dependent on how well the offline data covers the optimal policy. Finally, we demonstrate the empirical performance of Sim-{OPRL} in different environments.},
	number = {{arXiv}:2406.18450},
	publisher = {{arXiv}},
	author = {Pace, Alizée and Schölkopf, Bernhard and Rätsch, Gunnar and Ramponi, Giorgia},
	urldate = {2025-02-23},
	date = {2024-06-26},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2406.18450 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Pace et al. - 2024 - Preference Elicitation for Offline Reinforcement L.pdf:/Users/yatingzou/Zotero/storage/YKRVPKD2/Pace et al. - 2024 - Preference Elicitation for Offline Reinforcement L.pdf:application/pdf},
}

@misc{lee_off-policy_2024,
	title = {Off-Policy Reinforcement Learning with High Dimensional Reward},
	url = {http://arxiv.org/abs/2408.07660},
	doi = {10.48550/arXiv.2408.07660},
	abstract = {Conventional off-policy reinforcement learning ({RL}) focuses on maximizing the expected return of scalar rewards. Distributional {RL} ({DRL}), in contrast, studies the distribution of returns with the distributional Bellman operator in a Euclidean space, leading to highly flexible choices for utility. This paper establishes robust theoretical foundations for {DRL}. We prove the contraction property of the Bellman operator even when the reward space is an infinite-dimensional separable Banach space. Furthermore, we demonstrate that the behavior of high- or infinite-dimensional returns can be effectively approximated using a lower-dimensional Euclidean space. Leveraging these theoretical insights, we propose a novel {DRL} algorithm that tackles problems which have been previously intractable using conventional reinforcement learning approaches.},
	number = {{arXiv}:2408.07660},
	publisher = {{arXiv}},
	author = {Lee, Dong Neuck and Kosorok, Michael R.},
	urldate = {2025-02-24},
	date = {2024-08-14},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2408.07660 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Lee and Kosorok - 2024 - Off-Policy Reinforcement Learning with High Dimens.pdf:/Users/yatingzou/Zotero/storage/TVC3M5PN/Lee and Kosorok - 2024 - Off-Policy Reinforcement Learning with High Dimens.pdf:application/pdf},
}

@misc{mavrin_distributional_2019,
	title = {Distributional Reinforcement Learning for Efficient Exploration},
	url = {http://arxiv.org/abs/1905.06125},
	doi = {10.48550/arXiv.1905.06125},
	abstract = {In distributional reinforcement learning ({RL}), the estimated distribution of value function models both the parametric and intrinsic uncertainties. We propose a novel and efﬁcient exploration method for deep {RL} that has two components. The ﬁrst is a decaying schedule to suppress the intrinsic uncertainty. The second is an exploration bonus calculated from the upper quantiles of the learned distribution. In Atari 2600 games, our method outperforms {QR}-{DQN} in 12 out of 14 hard games (achieving 483 \% average gain across 49 games in cumulative rewards over {QR}-{DQN} with a big win in Venture). We also compared our algorithm with {QR}-{DQN} in a challenging 3D driving simulator ({CARLA}). Results show that our algorithm achieves near-optimal safety rewards twice faster than {QRDQN}.},
	number = {{arXiv}:1905.06125},
	publisher = {{arXiv}},
	author = {Mavrin, Borislav and Zhang, Shangtong and Yao, Hengshuai and Kong, Linglong and Wu, Kaiwen and Yu, Yaoliang},
	urldate = {2025-02-24},
	date = {2019-05-13},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1905.06125 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Mavrin et al. - 2019 - Distributional Reinforcement Learning for Efficien.pdf:/Users/yatingzou/Zotero/storage/SN5IUHUM/Mavrin et al. - 2019 - Distributional Reinforcement Learning for Efficien.pdf:application/pdf},
}

@misc{zhu_principled_2024,
	title = {Principled Reinforcement Learning with Human Feedback from Pairwise or \$K\$-wise Comparisons},
	url = {http://arxiv.org/abs/2301.11270},
	doi = {10.48550/arXiv.2301.11270},
	abstract = {We provide a theoretical framework for Reinforcement Learning with Human Feedback ({RLHF}). Our analysis shows that when the true reward function is linear, the widely used maximum likelihood estimator ({MLE}) converges under both the Bradley-Terry-Luce ({BTL}) model and the Plackett-Luce ({PL}) model. However, we show that when training a policy based on the learned reward model, {MLE} fails while a pessimistic {MLE} provides policies with improved performance under certain coverage assumptions. Additionally, we demonstrate that under the {PL} model, the true {MLE} and an alternative {MLE} that splits the K-wise comparison into pairwise comparisons both converge. Moreover, the true {MLE} is asymptotically more efficient. Our results validate the empirical success of existing {RLHF} algorithms in {InstructGPT} and provide new insights for algorithm design. We also unify the problem of {RLHF} and max-entropy Inverse Reinforcement Learning ({IRL}), and provide the first sample complexity bound for max-entropy {IRL}.},
	number = {{arXiv}:2301.11270},
	publisher = {{arXiv}},
	author = {Zhu, Banghua and Jiao, Jiantao and Jordan, Michael I.},
	urldate = {2025-02-24},
	date = {2024-02-08},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2301.11270 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, Mathematics - Statistics Theory, Statistics - Machine Learning, Statistics - Statistics Theory},
	file = {Zhu et al. - 2024 - Principled Reinforcement Learning with Human Feedb.pdf:/Users/yatingzou/Zotero/storage/SZ8EDXCI/Zhu et al. - 2024 - Principled Reinforcement Learning with Human Feedb.pdf:application/pdf},
}

@misc{hassani_towards_2024,
	title = {Towards Sample-Efficiency and Generalization of Transfer and Inverse Reinforcement Learning: A Comprehensive Literature Review},
	url = {http://arxiv.org/abs/2411.10268},
	doi = {10.48550/arXiv.2411.10268},
	shorttitle = {Towards Sample-Efficiency and Generalization of Transfer and Inverse Reinforcement Learning},
	abstract = {Reinforcement learning ({RL}) is a sub-domain of machine learning, mainly concerned with solving sequential decision-making problems by a learning agent that interacts with the decision environment to improve its behavior through the reward it receives from the environment. This learning paradigm is, however, well-known for being time-consuming due to the necessity of collecting a large amount of data, making {RL} suffer from sample inefficiency and difficult generalization. Furthermore, the construction of an explicit reward function that accounts for the trade-off between multiple desiderata of a decision problem is often a laborious task. These challenges have been recently addressed utilizing transfer and inverse reinforcement learning (T-{IRL}). In this regard, this paper is devoted to a comprehensive review of realizing the sample efficiency and generalization of {RL} algorithms through T-{IRL}. Following a brief introduction to {RL}, the fundamental T-{IRL} methods are presented and the most recent advancements in each research field have been extensively reviewed. Our findings denote that a majority of recent research works have dealt with the aforementioned challenges by utilizing human-in-the-loop and sim-to-real strategies for the efficient transfer of knowledge from source domains to the target domain under the transfer learning scheme. Under the {IRL} structure, training schemes that require a low number of experience transitions and extension of such frameworks to multi-agent and multi-intention problems have been the priority of researchers in recent years.},
	number = {{arXiv}:2411.10268},
	publisher = {{arXiv}},
	author = {Hassani, Hossein and Razavi-Far, Roozbeh and Saif, Mehrdad and Lin, Liang},
	urldate = {2025-02-24},
	date = {2024-11-15},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2411.10268 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Hassani et al. - 2024 - Towards Sample-Efficiency and Generalization of Tr.pdf:/Users/yatingzou/Zotero/storage/VT6LUS8R/Hassani et al. - 2024 - Towards Sample-Efficiency and Generalization of Tr.pdf:application/pdf},
}

@misc{zhang_distributional_2021,
	title = {Distributional Reinforcement Learning for Multi-Dimensional Reward Functions},
	url = {http://arxiv.org/abs/2110.13578},
	doi = {10.48550/arXiv.2110.13578},
	abstract = {A growing trend for value-based reinforcement learning ({RL}) algorithms is to capture more information than scalar value functions in the value network. One of the most well-known methods in this branch is distributional {RL}, which models return distribution instead of scalar value. In another line of work, hybrid reward architectures ({HRA}) in {RL} have studied to model source-speciﬁc value functions for each source of reward, which is also shown to be beneﬁcial in performance. To fully inherit the beneﬁts of distributional {RL} and hybrid reward architectures, we introduce Multi-Dimensional Distributional {DQN} ({MD}3QN), which extends distributional {RL} to model the joint return distribution from multiple reward sources. As a by-product of joint distribution modeling, {MD}3QN can capture not only the randomness in returns for each source of reward, but also the rich reward correlation between the randomness of different sources. We prove the convergence for the joint distributional Bellman operator and build our empirical algorithm by minimizing the Maximum Mean Discrepancy between joint return distribution and its Bellman target. In experiments, our method accurately models the joint return distribution in environments with richly correlated reward functions, and outperforms previous {RL} methods utilizing multi-dimensional reward functions in the control setting.},
	number = {{arXiv}:2110.13578},
	publisher = {{arXiv}},
	author = {Zhang, Pushi and Chen, Xiaoyu and Zhao, Li and Xiong, Wei and Qin, Tao and Liu, Tie-Yan},
	urldate = {2025-02-24},
	date = {2021-10-26},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2110.13578 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Zhang et al. - 2021 - Distributional Reinforcement Learning for Multi-Di.pdf:/Users/yatingzou/Zotero/storage/VQHTZPVT/Zhang et al. - 2021 - Distributional Reinforcement Learning for Multi-Di.pdf:application/pdf},
}


@book{Geron2019,
    author    = {G{\'e}ron, Aur{\'e}lien},
    title     = {Hands-On Machine Learning with {S}cikit-{L}earn, {K}eras and {T}ensor{F}low (2nd edition)},
    year      = {2019},
    publisher = {O’Reilly},
    address = {California}, 
    isbn = {9781492032649}
}


@article{Levine2020,
  title={Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems},
  author={Sergey Levine and Aviral Kumar and G. Tucker and Justin Fu},
  journal={arXiv:2005.01643},
  year={2020},
  doi = {10.48550/arXiv.2005.01643}
}


@book{Vapnik1998,
  title     = "Statistical Learning Theory",
  author    = "Vapnik, Vladimir N",
  publisher = "John Wiley \& Sons",
  year      =  1998,
  isbn = {9788126528929}
}

@article{Luckett2020,
  title={Estimating Dynamic Treatment Regimes in Mobile Health Using {V}-Learning},
  author={Daniel J. Luckett and Eric B. Laber and Anna R. Kahkoska and David M. Maahs and Elizabeth J. Mayer‐Davis and Michael R. Kosorok},
  journal={Journal of the American Statistical Association},
  year={2020},
  volume={115},
  pages={692-706},
  doi = {10.1080/01621459.2018.1537919}
}

@book{Hastie2009,
  doi = {10.1007/978-0-387-84858-7},
  year = {2009},
  publisher = {Springer},
  author = {Trevor Hastie and Robert Tibshirani and Jerome Friedman},
  title = {The Elements of Statistical Learning},
  doi = {10.1007/978-0-387-84858-7}
}


@misc{tf2015,
title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={https://www.tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{i}n ~Abadi and
    Ashish ~Agarwal and
    Paul ~Barham and
    Eugene ~Brevdo and
    Zhifeng ~Chen and
    Craig ~Citro and
    others},
  year={2015},
}


@InProceedings{Antos2007,
  title={Fitted {Q}-Iteration in Continuous Action-Space {MDP}s},
  author={Antos, Andr{\'a}s and Munos, R{\'e}mi and  Szepesv{\'a}ri, Csaba and},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  volume={20},
  pages={9--16},
  year={2007},
  url = {https://papers.nips.cc/paper_files/paper/2007/file/da0d1111d2dc5d489242e60ebcbaf988-Paper.pdf}
}


@article{Kosorok2019,
  title={Precision Medicine},
  author={Kosorok, Michael R and Laber, Eric B},
  journal={Annual Review of Statistics and its Application},
  volume={6},
  pages={263--286},
  year={2019},
  doi = {10.1146/annurev-statistics-030718-105251}
}

@book{Bertsekas2012,
  author={Bertsekas, Dimitri},
  publisher={Athena Scientific},
  year={2012},
  title={Dynamic Programming and Optimal Control: Volume I (4th edition)},
  isbn = {9781886529434}
}

@article{Osa2018,
  title={An Algorithmic Perspective on Imitation Learning},
  author={Osa, Takayuki and 
          Pajarinen, Joni and 
          Neumann, Gerhard and 
          Bagnell, J Andrew and 
          Abbeel, Pieter and
          Peters, Jan and 
          others},
  journal={Foundations and Trends{\textregistered} in Robotics},
  volume={7},
  number={1-2},
  pages={1--179},
  year={2018},
  doi = {10.1561/2300000053}
}

@Manual{R2022,
     title = {{R}: A Language and Environment for Statistical Computing},
     author = {{R Core Team}},
     organization = {R Foundation for Statistical Computing},
     address = {Vienna, Austria},
     year = {2022},
     url = {https://www.R-project.org/},
}


@article{Schulte2014,
  title={Q-and {A}-Learning Methods for Estimating Optimal Dynamic Treatment Regimes},
  author={Schulte, Phillip J and Tsiatis, Anastasios A and Laber, Eric B and Davidian, Marie},
  journal={Statistical Science},
  volume={29},
  number={4},
  pages={640--661},
  year={2014},
  doi = {10.1214/13-STS450}
}

@article{Zhao2012,
  title={Estimating Individualized Treatment Rules Using Outcome Weighted Learning},
  author={Zhao, Yingqi and Zeng, Donglin and Rush, A John and Kosorok, Michael R},
  journal={Journal of the American Statistical Association},
  volume={107},
  number={499},
  pages={1106--1118},
  year={2012},
  doi = {10.1080/01621459.2012.695674}
}

@article{Zhou2017,
  title={Residual Weighted Learning for Estimating Individualized Treatment Rules},
  author={Zhou, Xin and Mayer-Hamblett, Nicole and Khan, Umer and Kosorok, Michael R},
  journal={Journal of the American Statistical Association},
  volume={112},
  number={517},
  pages={169--187},
  year={2017},
  doi = {10.1080/01621459.2015.1093947}
}


@article{Zhao2015,
  title={Doubly Robust Learning for Estimating Individualized Treatment With Censored Data},
  author={Zhao, Ying-Qi and Zeng, Donglin and Laber, Eric B and Song, Rui and Yuan, Ming and Kosorok, Michael Rene},
  journal={Biometrika},
  volume={102},
  number={1},
  pages={151--168},
  year={2015},
  doi = {10.1093/biomet/asu050}
}

@article{Chen2016,
  title={Personalized Dose Finding Using Outcome Weighted Learning},
  author={Chen, Guanhua and Zeng, Donglin and Kosorok, Michael R},
  journal={Journal of the American Statistical Association},
  volume={111},
  number={516},
  pages={1509--1521},
  year={2016},
  doi = {10.1080/01621459.2016.1148611}
}

@article{Zhao2015bowl,
  title={New Statistical Learning Methods for Estimating Optimal Dynamic Treatment Regimes},
  author={Zhao, Ying-Qi and Zeng, Donglin and Laber, Eric B and Kosorok, Michael R},
  journal={Journal of the American Statistical Association},
  volume={110},
  number={510},
  pages={583--598},
  year={2015},
  publisher={Taylor \& Francis},
  doi = {10.1080/01621459.2014.937488}
}

@article{Liu2018,
  title={Augmented Outcome-Weighted Learning for Estimating Optimal Dynamic Treatment Regimens},
  author={Liu, Ying and 
          Wang, Yuanjia and 
          Kosorok, Michael R and 
          Zhao, Yingqi and 
          Zeng, Donglin},
  journal={Statistics in Medicine},
  volume={37},
  number={26},
  pages={3776--3788},
  year={2018},
  doi = {10.1002/sim.7844}
}

@article{Zhang2012,
  title={A Robust Method for Estimating Optimal Treatment Regimes},
  author={Zhang, Baqun and Tsiatis, Anastasios A and Laber, Eric B and Davidian, Marie},
  journal={Biometrics},
  volume={68},
  number={4},
  pages={1010--1018},
  year={2012},
  doi = {10.1111/j.1541-0420.2012.01763.x}
}


@article{Wu2020,
  title={Matched Learning for Optimizing Individualized Treatment Strategies Using Electronic Health Records},
  author={Wu, Peng and Zeng, Donglin and Wang, Yuanjia},
  journal={Journal of the American Statistical Association},
  volume={115},
  number={529},
  pages={380--392},
  year={2020},
  doi = {10.1080/01621459.2018.1549050}
}

@article{Shi2018,
  title={High-Dimensional {A}-learning for Optimal Dynamic Treatment Regimes},
  author={Shi, Chengchun and Fan, Alin and Song, Rui and Lu, Wenbin},
  journal={Annals of Statistics},
  volume={46},
  number={3},
  pages={925--957},
  year={2018},
  doi = {10.1214/17-AOS1570}
}

@article{Luckett2021,
  title={Estimation and Optimization of Composite Outcomes},
  author={Luckett, Daniel J and Laber, Eric B and Kim, Siyeon and Kosorok, Michael R},
  journal={The Journal of Machine Learning Research},
  volume={22},
  number={1},
  pages={7558--7597},
  year={2021},
  url = {https://jmlr.org/papers/v22/20-429.html}
}

@article{Butler2018,
  title={Incorporating Patient Preferences Into Estimation of Optimal Individualized Treatment Rules},
  author={Butler, Emily L and Laber, Eric B and Davis, Sonia M and Kosorok, Michael R},
  journal={Biometrics},
  volume={74},
  number={1},
  pages={18--26},
  year={2018},
  doi = {10.1111/biom.12743}
}

@article{Hayes2022,
  title={A Practical Guide to Multi-Objective Reinforcement Learning and Planning},
  author={Hayes, Conor F and 
      R{\u{a}}dulescu, Roxana and 
        Bargiacchi, Eugenio and 
        K{\"a}llstr{\"o}m, Johan and 
        Macfarlane, Matthew and 
        Reymond, Mathieu and 
        Verstraeten, Timothy 
        and others},
  journal={Autonomous Agents and Multi-Agent Systems},
  volume={36},
  number={1},
  pages={26},
  year={2022},
  doi = {10.1007/s10458-022-09552-y}
}

@article{Laber2014,
  title={Dynamic Treatment Regimes: Technical Challenges and Applications},
  author={Laber, Eric B and Lizotte, Daniel J and Qian, Min and Pelham, William E and Murphy, Susan A},
  journal={Electronic journal of statistics},
  volume={8},
  number={1},
  pages={1225},
  year={2014},
  doi = {10.1214/14-ejs920}
}

@article{Wahl2018,
  title={Artificial Intelligence ({AI}) and Global Health: How can {AI} Contribute to Health in Resource-Poor Settings?},
  author={Wahl, Brian and Cossy-Gantner, Aline and Germann, Stefan and Schwalbe, Nina R},
  journal={BMJ global health},
  volume={3},
  number={4},
  pages={e000798},
  year={2018},
  doi = {10.1136/bmjgh-2018-000798}
}

@article{Chan2013,
  title={Treatment Effectiveness and Side Effects: A Model of Physician Learning},
  author={Chan, Tat and Narasimhan, Chakravarthi and Xie, Ying},
  journal={Management Science},
  volume={59},
  number={6},
  pages={1309--1325},
  year={2013},
  doi = {10.1287/mnsc.1120.1640}
}

@article{Almirall2014,
  title={Introduction to {SMART} Designs for the Development of Adaptive Interventions: With Application to Weight Loss Research},
  author={Almirall, Daniel and Nahum-Shani, Inbal and Sherwood, Nancy E and Murphy, Susan A},
  journal={Translational Behavioral Medicine},
  volume={4},
  number={3},
  pages={260--274},
  year={2014},
  doi = {10.1007/s13142-014-0265-0}
}

@article{Mauck2023,
  title={The Back Pain Consortium ({BACPAC}) Research Program: Structure, Research Priorities, and Methods},
  author={Mauck, Matthew C and 
          Lotz, Jeffrey and 
          Psioda, Matthew A and 
          Carey, Timothy S and 
          Clauw, Daniel J and 
          Majumdar, Sharmila and 
          others},
  journal={Pain Medicine},
  year={2023},
  note = {pnac202},
  doi = {10.1093/pm/pnac202}
}

@article{UNC2022,
  author = {{U.S. National Library of Medicine}},
  journal = {ClinicalTrials.gov},
  title = {The {BEST} Trial: Biomarkers for Evaluating Spine Treatments ({BEST})},
  url = {clinicaltrials.gov/ct2/show/NCT05396014},
  year = {2022}
}

@article{Andersson1999,
  title={Epidemiological Features of Chronic Low-Back Pain},
  author={Andersson, Gunnar BJ},
  journal={The Lancet},
  volume={354},
  number={9178},
  pages={581--585},
  year={1999},
  doi = {10.1016/S0140-6736(99)01312-4}
}

@article{Hurst2004,
  title={Assessing the Clinical Significance of Change Scores Recorded on Subjective Outcome Measures},
  author={Hurst, Hugh and Bolton, Jennifer},
  journal={Journal of Manipulative and Physiological Therapeutics},
  volume={27},
  number={1},
  pages={26--35},
  year={2004},
  doi = {10.1016/j.jmpt.2003.11.003}
}

@article{Krebs2009,
  title={Development and Initial Validation of the {PEG}, a Three-Item Scale Assessing Pain Intensity and Interference},
  author={Krebs, Erin E and Lorenz, Karl A and Bair, Matthew J and Damush, Teresa M and Wu, Jingwei and Sutherland, Jason M and Asch, Steven M and Kroenke, Kurt},
  journal={Journal of General Internal Medicine},
  volume={24},
  pages={733--738},
  year={2009},
  doi = {10.1007/s11606-009-0981-1}
}

@book{Givens2012,
  title={Computational Statistics (2nd edition)},
  author={Givens, Geof H and Hoeting, Jennifer A},
  year={2012},
  publisher={John Wiley \& Sons},
  address = {New Jersey},
  isbn = {9780470533314}
}

@book{Vaart1998,
  title={Asymptotic Statistics},
  author={Van der Vaart, Aad W},
  year={1998},
  publisher={Cambridge University Press},
  address = {Cambridge, UK},
  doi = {10.1017/CBO9780511802256}
}


@InProceedings{Tang2019,
  title = 	 {Mallows Ranking Models: Maximum Likelihood Estimate and Regeneration},
  author =       {Tang, Wenpin},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {6125--6134},
  year = 	 {2019},
  url = {https://proceedings.mlr.press/v97/tang19a.html}
}

@article{Jiang2021,
  author={Jiang, Xiaotong and 
          Nelson, Amanda E and 
          Cleveland, Rebecca J and 
          Beavers, Daniel P and 
          Schwartz, Todd A and 
          Arbeeva, Liubov and 
          others},
  title={Precision Medicine Approach to Develop and Internally Validate Optimal Exercise and Weight-Loss Treatments for Overweight and Obese Adults with Knee Osteoarthritis: Data From a Single-Center Randomized Irial},
  journal={Arthritis Care \& Research},
  volume={73},
  number={5},
  pages={693--701},
  year={2021},
  doi = {10.1002/acr.24179}
}

@article{Liu1989,
  title={On the Limited Memory {BFGS} Method for Large Scale Optimization},
  author={Liu, Dong C and Nocedal, Jorge},
  journal={Mathematical Programming},
  volume={45},
  number={1-3},
  pages={503--528},
  year={1989},
  doi = {10.1007/BF01589116}
}

@book{Klein2003,
  title={Survival Analysis: Techniques for Censored and Truncated Data},
  author={Klein, John P and Moeschberger, Melvin L},
  year={2003},
  publisher={Springer},
  address = {New York},
  doi = {10.1007/b97377}
}

@book{Kosorok2008,
  title={Introduction to Empirical Processes and Semiparametric Inference},
  author={Kosorok, Michael R},
  year={2008},
  publisher={Springer},
  address = {New York},
  doi = {10.1007/978-0-387-74978-5}
}

@book{Gine2016,
  title={Mathematical Foundations of Infinite-Dimensional Statistical Models},
  author={Gin{\'e}, Evarist and Nickl, Richard},
  year={2016},
  publisher={Cambridge University Press},
  address = {Cambridge UK},
  doi = {10.1017/CBO9781107337862}
}

@book{Shwartz2014,
  title={Understanding Machine Learning: From Theory to Algorithms},
  author={Shalev-Shwartz, Shai and Ben-David, Shai},
  year={2014},
  publisher={Cambridge University Press},
  address = {Cambridge UK},
  doi = {10.1017/CBO9781107298019}
}

@article{Dehon2017,
  title={A Systematic Review of the Impact of Physician Implicit Racial Bias on Clinical Decision Making},
  author={Dehon, Erin and Weiss, Nicole and Jones, Jonathan and Faulconer, Whitney and Hinton, Elizabeth and Sterling, Sarah},
  journal={Academic Emergency Medicine},
  volume={24},
  number={8},
  pages={895--904},
  year={2017},
  doi = {10.1111/acem.13214}
}

@book{Kosorok2015,
  title={Adaptive Treatment Strategies in Practice: Planning Trials and Analyzing Data for Personalized Medicine},
  author={Kosorok, Michael R and Moodie, Erica EM},
  year={2015},
  publisher={SIAM},
  address = {Pennsylvania},
  doi = {10.1137/1.9781611974188}
}

@phdthesis{Butler2016,
    title    = {Using Patient Preferences to Estimate Optimal Treatment Strategies for Competing Outcomes},
    school   = {UNC Chapel Hill},
    author   = {Emily L. Butler},
    year     = {2016},
    doi = {10.17615/zvtg-he40}
}

@article{Stroup2003,
  title={The {N}ational {I}nstitute of {M}ental {H}ealth Clinical Antipsychotic Trials of Intervention Effectiveness ({CATIE}) Project: Schizophrenia Trial Design and Protocol Development},
  author={Stroup, T Scott and McEvoy, Joseph P and Swartz, Marvin S and Byerly, Matthew J and Glick, Ira D and Canive, Jose M and McGee, Mark F and Simpson, George M and Stevens, Michael C and Lieberman, Jeffrey A},
  journal={Schizophrenia Bulletin},
  volume={29},
  number={1},
  pages={15--31},
  year={2003},
  doi = {10.1093/oxfordjournals.schbul.a006986}
}

@article{Hogan1983,
  title={A Self-Report Scale Predictive of Drug Compliance in Schizophrenics: Reliability and Discriminative Validity},
  author={Hogan, Thomas P and Awad, AG and Eastwood, Robin},
  journal={Psychological Medicine},
  volume={13},
  number={1},
  pages={177--183},
  year={1983},
  doi = {10.1017/s0033291700050182}
}

@article{Muhlbacher2016,
  title={Choice Experiments to Quantify Preferences for Health and Healthcare: State of the Practice},
  author={M{\"u}hlbacher, Axel and Johnson, F Reed},
  journal={Applied Health Economics and Health Policy},
  volume={14},
  pages={253--266},
  year={2016},
  doi = {10.1007/s40258-016-0232-7}
}

@article{Hollin2020,
  title={Do Patient Preferences Align With Value Frameworks? A Discrete-Choice Experiment of Patients With Breast Cancer},
  author={Hollin, Ilene L and Gonz{\'a}lez, Juan Marcos and Buelt, Lisabeth and Ciarametaro, Michael and Dubois, Robert W},
  journal={MDM Policy \& Practice},
  volume={5},
  number={1},
  pages={2381468320928012},
  year={2020},
  doi = {10.1177/2381468320928012}
}

@article{Bianconcini2014,
  title={Asymptotic Properties of Adaptive Maximum Likelihood Estimators in Latent Variable Models},
  author={Bianconcini, Silvia},
  journal={ Bernoulli},
  volume={20},
  number={3},
  pages={1507--1531},
  year={2014},
  doi = {10.3150/13-BEJ531}
}

@article{Breslow1993,
  title={Approximate Inference in Generalized Linear Mixed Models},
  author={Breslow, Norman E and Clayton, David G},
  journal={Journal of the American Statistical Association},
  volume={88},
  number={421},
  pages={9--25},
  year={1993},
  doi = {10.2307/2290687}
}

@article{GLIMMIX2018,
  title={The {GLIMMIX} Procedure},
  journal={SAS/STAT 15.1 User's Guide},
  author={SAS Institute},
  year={2018},
  url ={support.sas.com/documentation/onlinedoc/stat/151/glimmix.pdf}
}

@article{Grogan2020,
  title={Statisticians Should Learn {T}ensor{F}low Too},
  journal={Medium},
  author={Michael Grogan},
  year={2020},
  url = {towardsdatascience.com/statisticians-should-learn-tensorflow-too-58309441d3c2}
}

@book{Nocedal2006,
  title={Numerical Optimization (2nd edition)},
  author={Nocedal, Jorge and Wright, Stephen J},
  year={2006},
  publisher={Springer},
  address = {New York},
  doi = {10.1007/978-0-387-40065-5}
}

@book{McCullagh1989,
  title={Generalized Linear Models (2nd edition)},
  author={McCullagh, P. and Nelder, J.A.},
  year={1989},
  publisher={Chapman and Hall},
  address = {Florida},
  doi = {10.1201/9780203753736}
}

@book{Dhaliwal2022,
title = {Duloxetine},
author = {Dhaliwal, Jaberpreet S. and Spurling, Benjamin C. and Molla, Mohammed},
year={2022},
publisher={StatPearls Publishing},
url = {europepmc.org/books/NBK549806}
}

@article{Yu2021,
  title={Reinforcement Learning in Healthcare: A Survey},
  author={Yu, Chao and Liu, Jiming and Nemati, Shamim and Yin, Guosheng},
  journal={ACM Computing Surveys (CSUR)},
  volume={55},
  number={1},
  pages={1--36},
  year={2021},
  doi={10.1145/3477600}
}

@article{Johnson2013,
title = {Constructing Experimental Designs for Discrete-Choice Experiments: Report of the {ISPOR} Conjoint Analysis Experimental Design Good Research Practices Task Force},
journal = {Value in Health},
volume = {16},
number = {1},
pages = {3-13},
year = {2013},
doi = {10.1016/j.jval.2012.08.2223},
author = {F. {Reed Johnson} and Emily Lancsar and Deborah Marshall and Vikram Kilambi and Axel Mühlbacher and Dean A. Regier and Brian W. Bresnahan and Barbara Kanninen and John F.P. Bridges}
}

@article{Bridges2011,
title = "Conjoint Analysis Applications in Health - A Checklist: A Report of the {ISPOR} Good Research Practices for Conjoint Analysis Task Force",
author = "Bridges, {John F.P.} and Hauber, {A. Brett} and Deborah Marshall and Andrew Lloyd and Prosser, {Lisa A.} and Regier, {Dean A.} and Johnson, {F. Reed} and Josephine Mauskopf",
year = "2011",
doi = "10.1016/j.jval.2010.11.013",
volume = "14",
pages = "403--413",
journal = "Value in Health",
number = "4",
}

@incollection{McFadden1974,
  author    = {Mcfadden, Daniel},
  title     = {Conditional Logit Analysis of Qualitative Choice Behavior},
  booktitle = {Frontiers in Econometrics},
  publisher = {Academic Press},
  year      = 1974,
  editor    = {Paul Zarembka},
  pages     = {105–-142},
  isbn = {9780127761503}
} 

@article{Hauber2016,
title = {Statistical Methods for the Analysis of Discrete Choice Experiments: A Report of the {ISPOR} Conjoint Analysis Good Research Practices Task Force},
journal = {Value in Health},
volume = {19},
number = {4},
pages = {300-315},
year = {2016},
doi = {10.1016/j.jval.2016.04.004},
author = {A. Brett Hauber and Juan Marcos González and Catharina G.M. Groothuis-Oudshoorn and Thomas Prior and Deborah A. Marshall and Charles Cunningham and Maarten J. IJzerman and John F.P. Bridges}
}

@article{Wilson2023,
    author = {Wilson, Leslie and 
              Zheng, Patricia and 
              Ionova, Yelena and 
              Denham, Alina and 
              Yoo, Connie and 
              Ma, Yanlei and 
              others},
    title = {{CAPER}: Patient Preferences to Inform Nonsurgical Treatment of Chronic Low Back Pain: a Discrete-Choice Experiment},
    journal = {Pain Medicine},
    year = {2023},
    doi = {10.1093/pm/pnad038},
    note = {pnad038}
}

@book{Tsiatis2019,
  author = {Tsiatis, Anastasios A and Davidian, Marie and Holloway, Shannon T and Laber, Eric B},
  publisher = {CRC Press},
  address = {Florida},
  title = {Dynamic Treatment Regimes: Statistical Methods for Precision Medicine},
  year = {2019},
  doi = {10.1201/9780429192692}
}

@inproceedings{Christiano2017,
 author = {Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Deep Reinforcement Learning from Human Preferences},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/d5e2c0adad503c91f91df240d0cd4e49-Paper.pdf},
 volume = {30},
 year = {2017}
}

@inproceedings{Ibarz2018,
 author = {Ibarz, Borja and Leike, Jan and Pohlen, Tobias and Irving, Geoffrey and Legg, Shane and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Reward Learning from Human Preferences and Demonstrations in {A}tari},
 url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/8cbe9ce23f42628c98f80fa0fac8b19a-Paper.pdf},
 volume = {31},
 year = {2018}
}





