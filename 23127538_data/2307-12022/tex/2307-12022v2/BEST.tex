\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{comment}
\usepackage{graphicx,psfrag,epsf,subcaption,multirow}
\usepackage{enumerate}
%\usepackage{natbib} 
\usepackage[backend=biber, natbib=true, style=authoryear-comp, %surname-year citation and reference list, with the name appearing once for citing multiple works of the same author
maxcitenames=2, %no more than two names for citations
uniquelist=false, %use a/b year appendices to keep max 2 names
giveninits=true, %use first name initials
maxbibnames=6,  %upto six authors appearing in reference
minbibnames=6, %for more than six authors, display the first six
sortcites=false, %citations sorted by year  
uniquename=false %no initials for citations
]{biblatex}
\DeclareNameAlias{author}{family-given} %all authors appear last name first
\DeclareDelimFormat[parencite]{nameyeardelim}{\addspace} %no punctutation between name and year for citations
\renewcommand*{\newunitpunct}{\addcomma\space} %commas after year, title
\uspunctuation %punctuation within quotes
\renewbibmacro{in:}{} %remove 'in:' before journal
\usepackage{url} %allow for urls in references, url breaking
\sloppy %force margin respect for references
\addbibresource{BEST.bib}

% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{0}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-1in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.7in}%
\addtolength{\topmargin}{-1in}%

% \usepackage[margin=1in]{geometry}


% extra packages and options  
\usepackage{float}
\usepackage{indentfirst}
\usepackage{caption}
\usepackage{doi} 
\usepackage{amsfonts} 
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{algorithm}
\usepackage{algorithmic}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\usepackage{xcolor}
\allowdisplaybreaks
\usepackage{titlesec}
\titlelabel{\thetitle.\quad}
\usepackage{sidecap,changepage} %for Appendix

\DeclareMathOperator{\E}{\mathbb{E}}

\begin{document}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \title{\bf A Flexible Framework for Incorporating Patient Preferences Into Q-Learning}
  \author{Joshua P. Zitovsky\\
    Department of Biostatistics \\ UNC Chapel Hill \\
    \and 
    Yating Zou\\
    Department of Biostatistics \\ UNC Chapel Hill \\
    \and
    Leslie Wilson\\
    Department of Clinical Pharmacy \\
    UCSF \\
    \and 
    Michael R. Kosorok\thanks{Correspondence to: Michael R. Kosorok $<$kosorok@bios.unc.edu$>$} \\
     Department of Biostatistics \\ UNC Chapel Hill}
     \date{}
  \maketitle

\vspace{-0.3cm}
\begin{abstract}
In real-world healthcare settings, treatment decisions often involve optimizing for multivariate outcomes such as treatment efficacy and severity of side effects based on individual preferences. However, existing statistical methods for estimating dynamic treatment regimes (DTRs) usually assume a univariate outcome, and the few methods that deal with composite outcomes suffer from limitations such as restrictions to a single time point and limited theoretical guarantees. To address these limitations, we propose \textit{Latent Utility Q-Learning (LUQ-Learning)}, a latent model approach that adapts Q-learning to tackle the aforementioned difficulties. Our framework allows for an arbitrary finite number of decision points and outcomes, incorporates personal preferences, and achieves asymptotic performance guarantees with realistic assumptions. We conduct simulation experiments based on an ongoing trial for low back pain as well as a well-known trial for schizophrenia. In both settings, LUQ-Learning achieves highly competitive performance compared to alternative baselines. 
\end{abstract}

\noindent%
{\it Keywords:}  Dynamic Treatment Regime, Precision Medicine, Latent Variable Model, Multiple Outcomes, Reinforcement Learning
\vfill


\spacingset{1.6} % DON'T change the spacing!
\section{Introduction}
\label{sec:introduction}

Precision medicine \citep{Kosorok2019} is a subfield of statistics concerned with \textit{dynamic treatment regimes (DTRs)} \citep{Tsiatis2019}---a sequence of treatment rules at different time points that depend on the evolving characteristics of a patient and optimize for some desired outcomes. Precision medicine allows researchers to leverage datasets collected from clinical trials, observational studies, electronic health records, and more to support clinicians and policymakers. It also has the potential to improve care in settings where interaction with medical professionals is limited \citep{Wahl2018}. 

This work is motivated primarily by the \textit{Biomarkers for Evaluating Spine Treatments (BEST)} study \citep{UNC2022}, an ongoing NIH-funded sequential multiple assignment randomized trial (SMART) \citep{Almirall2014} directed by researchers at UNC Chapel Hill as part of the Back Pain Consortium Research Program \citep{Mauck2023}. The purpose of the BEST study is to estimate an optimal DTR for patients suffering from chronic low back pain \citep{Andersson1999}. Although a naive analysis would focus solely on reducing pain, maximizing pain relief may come at the cost of side effects on fatigue and cognition. A truly optimal DTR should account for both the efficacy of the treatment and the severity of the side effects. Additionally, pain experience is multifaceted and personal. While standard pain measures in the medical field exists, they are usually designed for the general pain experience, not accounting for the unique aspects of a specific type of pain. 

Over the last decade, methods have been proposed to estimate DTRs under a variety of settings, including settings with a single \citep{Zhang2012, Zhou2017}, multiple \citep{Zhao2015bowl, Liu2018}, or an infinite number of decision points \citep{Luckett2020, Levine2020}. Most of these works assume a known univariate outcome to maximize. Specifying such an ideal reward function that clearly characterizes the intended objective is crucial for a Reinforcement Learning algorithm, yet can be difficult in the face of multi-objective settings and settings where there is no immediate reward. A straightforward solution is to define \citep{Hayes2022} or estimate from the data a summary function for multidimensional outcomes. \citep{Jiang2021} proposed a minimax approach in which utility was a convex combination of outcomes and convex weights were scalars tuned to maximize the minimum estimated value among multiple outcomes. This, however, does not account for individual-level variations. Distributional RL (DRL) provides solutions to scenarios where the outcome can be even infinite dimensional \citep{zhang_distributional_2021, lee_off-policy_2024}. However, one still needs to decide on a summary function eventually, and computation can be highly demanding. Inverse RL, a category of imitation RL, assumes that the realized trajectories come from an expert with an internal reward model, which the algorithm tries to learn \citep{hejna_inverse_2023, hassani_towards_2024}. \citet{Luckett2021} proposed a way to learn patient-specific utility. However, IRL is not ideal when the observed decisions are made randomly by the patients themselves \citep{Kosorok2015} or by clinicians who act suboptimally \citep{Dehon2017}. 

Preference-based RL (PbRL), on the other hand, abandons optimizing for some numerical reward and instead aims to find a policy that maximally complies with a collected set of preferences, where ``preference'' is the selection of one trajectory over another, by attaching any pair of trajectories with order relations \citep{wirth_survey_2017}. While being successfully applied in robotics and games, it requires heavy interactive feedback from humans to assign labels to predicted trajectories to either learn a preference classifier or a reward model \citep{Christiano2017, Ibarz2018, christiano_deep_2023}. Several latter works tried to mitigate the problem of low sample efficiency and insufficient coverage of collected data as trajectory accumulates \citep{an_direct_2023, park_surf_2022, hassani_towards_2024}, but not until \citep{zhu_principled_2024, zhan_provable_2023, pace_preference_2024} was learning from offline static data considered. Although \citep{zhan_provable_2023} considers a more general function class for the reward model than linear, elicitation of preference data is restricted to selection over trajectory pairs, which limits the types of preference data collected. The approach can also reduce data efficiency significantly since state characterization requires high dimensional information, yet comparisons are obtained from only one of many possible pairs. 

To overcome these limitations, we redefine ``preference" differently from its interpretation in PbRL, considering it as an \textit{indirect measure that informs patient preference across elements of the outcome vector.} This approach offers several advantages:
\begin{enumerate}
    \item It avoids direct action ranking, which could be problematic, as it may allow patients to select interventions based on personal interests or limited contextual knowledge. At the same time, it implicitly accounts for the effects of prior actions.
    \item As long as the primary outcome vector is comparable in scale, this framework imposes no restrictions on the format of preference data (e.g., ordinal, numeric, or categorical) and does not require scale alignment across preference data sources, which may originate from diverse battery tests and questionnaires. This flexibility greatly reduces the complexity of study design.
\end{enumerate}
The above preference perspective is the same as that given in \citep{Butler2018, Butler2016, Zhong2021}. In \citep{Butler2018, Butler2016}, access to expert-level data is not assumed; however, the times at which preference data can be collected are not fully identified and theoretical guarantees are lacking. \citep{Zhong2021} proposed SAPP-Q-Learning that combines Inverse Probability of Censoring Weighting (IPCW) with Q-Learning to target survival; however, it is designed for only two-stage scenarios, assumes censoring happen only at the second stage, and does not allow action set to vary based on history. \citep{Wank2024PRPP} proposed a Partially Randomized, Patient Preference (PRPP) SMART design and proposed using Weighted and Replicated Regression Models (WRRM) to estimate embedded DTRs. Our framework differs in several key ways: (1) we define preference to be preference over the outcomes rather than directly over the action sets; (2) we do not restrict the outcome to be binary; and (3) our estimation approach applies to settings beyond the two-stage PRPP-SMART design.

Other approaches to eliciting patient preference include discrete choice experiments \citep{Johnson2013, Janssen2017ImprovingTQ} and conjoint analysis \citep{Bridges2011, Leeper2019MeasuringSP, Liu2023MultipleHT} which aim to construct efficient surveys that accurately measure preferences or ``a stakeholders' underlying inclination when faced with multiple alternatives that vary in specific attributes and levels''. Many discrete choice models can be interpreted as estimating latent utilities that drive the choices made by the respondents \citep{McFadden1974, Hauber2016}. 

In response to these limitations, we propose \textit{Latent Utility Q-Learning (LUQ-Learning)} which incorporates individualized preference over multiple outcomes into the Q-learning algorithm \citep{Schulte2014} via a latent model approach. Our framework allows for a finite number of decision points, outcomes of interest, and treatment possibilities. It adapts to any type of outcome preference measures. For example, ranking and the Bradley Terry Luce model, which is commonly used in PbRL as the loss function, can be collected at all stages and used in latent model estimation. We identify the key causal assumptions for our preference framework, eliminating the need for action or state ranking as required in PbRL. We also derive theoretical properties of LUQ-Learning while only requiring modest assumptions, giving our framework strong theoretical guarantees. Finally, we apply LUQ-Learning to simulated patients from the chronic low back pain (BEST) study as well as the schizophrenia study used by \citep{Butler2018} for illustration. 


\section{Notation and Setup}

The motivation for this work comes from the BEST trial that targets lower back pain. Because pain experience can be very personal and its effect on daily lives multidimensional, in addition to the primary objective of estimating an optimal DTR that optimizes for the Pain, Enjoyment of Life and General Activity (PEG) score, a validated and widely used self-reported pain assessment tool, the study also seeks to take into account possible side effects such as opioid use, sleep disturbance, and several other outcomes. This accounting includes incorporating personal preference on how to prioritize among this set of pain-related outcomes when defining ``personal optimal". Although BEST is a clinical trial in which treatment assignment is conditionally random, we still adopt Rubin's potential outcome framework, as we want the optimal to be over all treatment sequence possibilities, not just the observed ones. In fact, in the BEST study, the observed policy is never a function of patient-reported preference, although incorporating this preference is desired for the estimated optimal policy.

We adopt the classic notation of $Q$-learning in the precision medicine setting and consider the problem of sequential decision optimization over $K < \infty$ decision points. We assume that the observed data consist of $n$ i.i.d. trajectories of the form
\[
\mathcal D=\{(\mathbf{Z}^{i}_1,\mathbf{Z}^{i}_2,\dots,\mathbf{Z}^{i}_K, \mathbf{Y}^i, \mathbf{W}_{K+1}^i)\}_{i=1}^n,
\text{ where }
\mathbf{Z}^{i}_{k} = (\mathbf{X}_k^i,\mathbf{W}_{k}^i,A_k^i),
\]
where $\mathbf{X}_k \in \mathcal{X}$ are patient covariates which can include baseline or summary statistics of patient status before action $A_k\in{\cal A}_{k}$; and $\mathbf{Y} \in \mathcal{Y} \subset \mathbb{R}^d$ a vector of outcome of interest measured at the end of the study, for example, the primary and secondary endpoints of the study. One of the main differences in our setting is the introduction of $\mathbf{W}_{k} \in \mathcal{W}$, the preference elicitation instrument regarding the final outcome of interest $\mathbf{Y}$. Each $\mathbf{W}_k$ is collected after $A_{k-1}$ but prior to $A_k$, which we expect to reflect individual patient preference. Examples of $\mathbf{W}_k$ includes questionnaires collected to assess how much they felt $\mathbf{Y}$ improved after the just-received treatment (satisfaction), or how they would prioritize elements of $\mathbf{Y}$ based on previous treatment experience which could involve side effects, cost of therapy, etc. Based on the patient trajectories, we define history $\mathbf{H}_k$ for $k = 1, \dots, (k-1)$ as all the information available before action $A_k$ is taken. That is, $\mathbf{H}_1 = (\mathbf{X}_1,\mathbf{W}_{1})$, $\mathbf{H}_2 = (\mathbf{H}_1, A_1, \mathbf{X}_2,\mathbf{W}_{2})$, $\dots$, $\mathbf{H}_{K} = (\mathbf{H}_{K-1}, A_K, \mathbf{X}_K, \mathbf{W}_{K})$. Although not followed by an action, for completeness, we also define $\mathbf{H}_0 = \emptyset$, $\mathbf{H}_{K+1} = (\mathbf{H}_{K}, \mathbf{Y}, \mathbf{W}_{K+1})$. We further assume that for each $k$, $\mathcal{A}_k=\mathcal{A}_{{\mathbf{H}}_k}$, which is the finite set of feasible actions (treatments) for a patient with observed history ${\mathbf{H}}_k$.  This allows the incorporation of restrictions on treatment based on patient medical history. Accordingly, define $\mathcal{A}_{\mathcal{H}_k} = \cup_{H_k \in \mathcal{H}_k} \mathcal{A}_{H_k}$. Figure \ref{setup_plot} illustrates this overall structure for a setting where $K = 3$. 

The goal of this paper is to find an optimal sequence of decision rules as a function of history, often called a dynamic treatment regime (DTR) ${\pi}^{opt} = (\pi^{opt}_1, \dots, \pi^{opt}_K)$, where $\pi^{opt}_k: \mathcal{H}_k \mapsto \mathcal{A}_{\mathcal{H}_k}$, such that 
\begin{equation}
    V_1^{{\pi}^{opt}}(\mathbf{H}_1) \geq V_1^{{\pi}}( \mathbf{H}_1), \quad 
    \forall {\pi} \in \Pi, \forall \mathbf{H}_1 \in \mathcal{H}_1
    \label{equation:opt_condition}
\end{equation}
and where $V_k^{{\pi}}(\mathbf{H}_k) = \mathbb{E}_{a_{k+1}, \dots, a_{K} \sim \pi}[\mathbf{E}^T \mathbf{Y}^*({\pi}) | \mathbf{H}_k]$, our preference-incorporated \textit{value function}. We let $\mathbf Y^*({\pi})$ be the $d$-dimension vector of outcomes that would be observed if the subject received the treatment sequence ${\pi} = (\pi_1, \dots, \pi_K)$ and we let $\mathbf{E}\in\mathcal E$ be the unobserved patient preference, where $\mathcal E$ a $(d-1)$-dimensional probability simplex. We assume that latent preference $\mathbf E$ affects subjective measures $\mathbf{W}_k$ at all stages, and we allow $\mathbf{W}_k$ to also affect $\mathbf{X}_{k+1}$, the patient status at the next stage. This can happen, for example, when patients may respond better if they are more satisfied with their prior treatments. Note that $\mathbf{X}_k$ and $\mathbf{W}_k$ at $k = 2, \dots, K$ are all observed values of potential outcomes $\mathbf{X}_{k}^*$ and $\mathbf{W}_{k}^*$ as depicted in Figure \ref{setup_plot}. Define also the Q function indexed by $\mathbf{E}_k$ at the k-th stage as $\tilde{Q}^{\pi^{opt}}_k({\mathbf{H}}_k, A_k, \mathbf{E}_k) = \mathbf{E}_k ^T \mathbb{E}_{a_{k+1}, \dots, a_{K} \sim {\pi^{opt}}}[\mathbf{Y}^*(\Bar{a}_K) | {\mathbf{H}}_k, A_k]$, the inner product of some given latent preference $\mathbf{E}_k \sim P(\mathbf{E}_k | \mathbf{H}_k)$ and the expected potential outcomes if all future actions follow the optimal regime. Further, define $Q^{\pi^{opt}}_k({\mathbf{H}}_k, A_k) = \mathbb{E}_{a_{k+1}, \dots, a_{K} \sim {\pi^{opt}}}[U^* | {\mathbf{H}}_k, A_k] =  \mathbb{E}_{a_{k+1}, \dots, a_{K} \sim {\pi^{opt}}}[\mathbf{E}^T \mathbf{Y}^*(\Bar{a}_K) | {\mathbf{H}}_k, A_k]$, where $\Bar{a}_k = (a_1, \dots, a_{k})$ is a give sequence of treatments up to and including decision time $k$, where $1 \leq k \leq K$.

% Figure environment removed

\section{Methodology}
\label{sec:method}

\subsection{Assumptions}
\label{sec:assumptions}

We make the following assumptions throughout: 
\begin{enumerate}
\setlength{\itemsep}{-2pt}
    \item[(A1)] Consistency: If treatment sequence $\Bar{a}_K = (a_1, \dots, a_K)$ is the actual treatment sequence received by subject $i$, then ${\mathbf{Y}^{*}}^i(\Bar{{a}}_{K}) = \mathbf{Y}^i$, and ${\mathbf{X}^*}_{k}^{i}(\Bar{a}_{k-1}) = {\mathbf{X}}_{k}^{i}$, ${\mathbf{W}^*}_{k}^{i}(\Bar{{a}}_{k-1}) = \mathbf{W}_{k}^i$ for all $1 \leq k \leq (K-1)$.
    \item[(A2)] SUTVA (Stable Unit Treatment Value Assumption): One version of treatment. Each value of $a \in \mathcal{A}_{{\mathcal{H}}_k}$ $\forall 1 \leq k \leq K$ is unambiguously defined.
    \item[(A3)] Positivity: $1 > P(A_k = a_k | {\mathbf{H}}_k) > c$, for some $c > 0$, $\forall a_k \in \mathcal{A}_{{\mathbf{H}}_k}, {\mathbf{H}}_k \in {\mathcal{H}}_k$ and $1 \leq k \leq K$.
    \item[(A4)] Sequential Weak Unconfoundedness: $\mathbf{X}_{k+1}^{*}(\Bar{a}_{k}) \perp \!\!\! \perp A_k | \mathbf{H}_k$ for all $\Bar{a}_{k}, 1 \leq k \leq (K-1)$, and $\mathbf{Y}^{*}(\Bar{a}_{K}) \perp \!\!\! \perp A_K | \mathbf{H}_K$ for all $\Bar{a}_{K}$.
    \item[(A5)] Sequential Weak Preference Independence: $(\mathbf{X}_{k+1}^{*}(\Bar{a}_{k}), A_k)\perp \!\!\! \perp \mathbf{E}|\mathbf{H}_k$ for all $\Bar{a}_{k}, 1 \leq k \leq (K-1)$, and $(\mathbf{Y}^{*}(\Bar{a}_{K}), A_K)\perp \!\!\! \perp \mathbf{E}|\mathbf{H}_K$ for all $\Bar{a}_{K}$.
\end{enumerate}
(A1)-(A4) are standard assumptions when working with sequences of potential outcomes. (A1) and (A2) relate potential outcomes to the observables and can always be satisfied by choosing a good definition for the random variables involved. (A3) ensures that given history $\mathbf{H}_k$ at any decision point, data has sufficient variability in its assigned action for the algorithm to learn the value associated with interventions that are not the observed ones. It can be checked by referring to the study design and looking at $\widehat{P}(A_k | {\mathbf{H}}_k)$ fitted using flexible models. (A4) means that all confounding variables between action $A_k$ and the next state $\mathbf{X}^{*}_{k}$ have been captured in the history $\mathbf{H}_k$. While in an observational study this is unverifiable, in a SMART study this assumption can be ensured by the conditional randomized treatment assignment structure. (A5) is specific to our Latent Utility Q-Learning algorithm. It is equivalent to $A_k \perp \!\!\! \perp \mathbf{E} | \mathbf{H}_k$ and $\mathbf{X}_{k+1}^{*}(\Bar{a}_{k}) \perp \!\!\! \perp \mathbf{E} | \mathbf{H}_k, A_k$ at all $1 \leq k \leq (K-1)$ and $\mathbf{Y}^{*}(\Bar{a}_{K}) \perp \!\!\! \perp \mathbf{E} | \mathbf{H}_K, A_K$ at the last time point. As showed in Figure \ref{setup_plot}, the key in satisfying this assumption is to ensure that the collected preference information $\mathbf{W}_k$ is rich enough so that 1) it captures all influence of preference $\mathbf{E}$ on patient status at the next stage $\mathbf{X}_{k+1}$ and 2) once we control on the history that includes collected preference, latent preference $\mathbf{E}$ does not have additional influence on the observed treatment assignment mechanism. See Section \ref{sec:best} for a concrete example of how the random variables can be defined and assumptions checked.

\subsection{Latent Utility Q-Learning}
\label{sec:LUQ-learning}
The key insight of LUQ-Learning is that the additional assumption (A5) does not invalidate the backward sequential optimization scheme to arrive at an optimal policy, yet it is sufficiently strong to disentangle the outcome model $\mathbb{E}[\mathbf{Y} | \mathbf{H}_k , A_k]$ and the preference model $P(\mathbf{E} | \mathbf{H}_k)$ sequentially. Specifically, (A1)-(A5) guarantee $\mathbb{E}[U^*|\mathbf{H}_{k}, A_k] = \mathbb{E}[\mathbf{E}|\mathbf{H}_{k}]^T \mathbb{E}[\mathbf{Y}|\mathbf{H}_{k}, A_k]$ for all k. We propose the following scheme to estimate the preference model parametrically, although we note that this can also be done non-parametrically since the algorithm remains valid as long as we can sample from the posterior $P(\mathbf{E}|\mathbf{H}_k)$ at all stages. The Bayesian perspective is directly motivated by the desirability of incorporating randomness in $\mathbf{E}$, especially given its unobserved nature. By explicitly incorporating uncertainty, Bayesian methods provide a principled framework for inference in the presence of latent variables. They also inherently introduce regularization through prior distributions, stabilizing parameter estimation particularly in small-sample settings. This is supported by the results presented in Table (S4) in the Supplementary Material.

Denote $\theta$ as the vector of parameters in the models used to define $P(\mathbf{H}_k|\mathbf{E})$. For any $1 \leq k \leq K$, (A1)--(A5) allows us to write the recursive equation for the observed likelihood 
$
P(\mathbf{H}_k | \mathbf{E}) = \sum_{i = 1}^n
P(\mathbf{W}^{i}_k| \mathbf{X}^{i}_k, {\mathbf{H}}^{i}_{k-1}, \mathbf{E}^{i})
P(\mathbf{X}^{i}_k| {\mathbf{H}}^{i}_{k-1}, A^{i}_{k-1})
P(A^i_{k-1} | {\mathbf{H}}^i_{k-1})
P(\mathbf{H}^{i}_{k-1} | \mathbf{E}^{i})
$
which is identifiable from the data. This is done similarly for $P(\mathbf{H}_{K+1} | \mathbf{E})$ but with $\mathbf{Y}$ in the place of $\mathbf{X}$. A key observation is that it is sufficient to estimate the part regarding $\mathbf{W}$ to sample from $P(\mathbf{E}|\mathbf{H}_k)$. To do so, we parametrize $P(\mathbf{W}_k|\mathbf{X}_k, \mathbf{H}_{k-1}, \mathbf{E})$ with $\theta_k$ and obtain $\widehat{\theta} = (\widehat{\theta}_1, \dots, \widehat{\theta}_k)$, $1 \leq k \leq K+1$ as the maximizer of the data log posterior, with randomness in $\mathbf{E}$ marginalized out:
\begin{align}
\begin{split}
    \log P(\theta|\mathbf{H}_{k}) 
    &\propto 
    \sum_{i=1}^n \log \Bigg\{\int_{\mathcal E}
    P(\mathbf{W}^{i}_{K+1}| \mathbf{Y}^{i}, \mathbf{H}^{i}_K, \mathbf{E}^{i},\theta_{K+1})^{I(k=(K+1))} \\
     &\times \prod_{l = 1}^{k}
    P(\mathbf{W}^{i}_l| \mathbf{X}^{i}_l, \mathbf{H}^{i}_{l-1}, \mathbf{E}^{i},\theta_l) 
     d \Lambda_{E}(\mathbf{E}^{i}) \Bigg\}\\
    &+ I(k = (K+1)) \log(\Lambda_{\theta}(\theta_{K+1})) +
    \sum_{l=1}^{k} \log(\Lambda_{\theta}(\theta_l)).
     \label{eq:posterior}
\end{split}
\end{align}
The full LUQ-Learning algorithm is summarized below.

\begin{algorithm}[H]
\footnotesize
\caption{LUQ-Learning}
\label{alg:luq_v1}
\begin{algorithmic}[1]
    \STATE Specify a parametric model $P(\mathbf{W}|\mathbf{X}, \mathbf{E}, \theta)$. Specify a prior distribution on $\theta$ and $\mathbf E$, denoted $\Lambda_\theta$ and $\Lambda_E$, respectively. Denote $\gamma$ as the parameter for the outcome model, possibly infinite dimensional.
    \STATE \textbf{Input} Observed trajectories $\mathcal D=\{(\mathbf{Z}^{i}_1,\mathbf{Z}^{i}_2,\dots,\mathbf{Z}^{i}_K, \mathbf{Y}^i, \mathbf{W}^i_{K+1})\}_{i=1}^N$,  
    $\mathbf{Z}^{i}_{k} = (\mathbf{X}_k^i,\mathbf{W}_{k}^i,A_k^i)$
    \FOR{k = K}
        \STATE Obtain $\hat{\theta}_n = argmax_\theta \log P(\theta|\mathbf{H}_{K+1})$ in (\ref{eq:posterior}).
        \STATE Fit outcome model using some regression algorithm to obtain $\hat{\mathbb{E}}[\mathbf{Y} | \mathbf{H}_k, A_k; \hat{\gamma}]$.
        \STATE Obtain 
        \[
        \hat{\pi}^{opt}_{K}(\mathbf{H}_K) = argmax_{a_K \in \mathcal{A}_{{H}_K}} \widehat{Q}_K(\mathbf{H}_K, a_K) =  argmax_{a_K \in \mathcal{A}_{{H}_K}} \widehat{\mathbb{E}} [\mathbf{E}|\mathbf{H}_K; \hat{\theta}_n]^{T} \hat{\mathbb{E}}[\mathbf{Y} | \mathbf{H}_K, a_K; \hat{\gamma}],
        \]
        where 
        $\widehat{\mathbb{E}} [\mathbf{E}|\mathbf{H}_k; \hat{\theta}_n]$ is calculated using integration methods such as Monte-Carlo integration.
        \STATE Let $\widehat{V}_K^{\hat\pi}(\mathbf{H}_K) \leftarrow \widehat{Q}_K(\mathbf{H}_K, \pi^{opt}_K)$.
    \ENDFOR
    \FOR{$k = K-1$ to $1$}
        \STATE Obtain $\hat{\pi}^{opt}_{k}(\mathbf{H}_k) = argmax_{a_k \in \mathcal{A}_{{H}_k}} \widehat{\mathbb{E}} [\widehat{V}^{\hat\pi}_{k+1}(\mathbf{H}_{K+1}) | \mathbf{H}_k, a_k]$.
        \STATE Let $\widehat{V}_k^{\hat\pi} (\mathbf{H}_{k}) \leftarrow \widehat{\mathbb{E}} [\widehat{V}^{\hat\pi}_{k+1}(\mathbf{H}_{k+1}) | \mathbf{H}_k, \hat{\pi}^{opt}_{k}]$.
    \ENDFOR
    \STATE \textbf{Output} $\{\hat{\pi}^{opt}_k(\mathbf{H}_k)\}_{k = 1}^{K}$, each $\hat{\pi}^{opt}_k(\mathbf{H}_k)$ is deterministic.
\end{algorithmic}
\end{algorithm}
\normalsize
While LUQ-Learning obtains an estimate $\hat{\pi}^{opt}$ of ${\pi}^{opt}_k$ that strictly satisfying ($\ref{equation:opt_condition}$) as shown in Lemma 1 in the Supplement (section S1), a slight modification of this algorithm provides practitioners uncertainty quantification of the latent preference which then translates to scores over the action sets. At any decision stage, healthcare practitioners can examine these scores to assess whether a suboptimal action may be preferable --- particularly if its score is close to that of the optimal action and additional patient-specific considerations, such as treatment costs or other personalized constraints, favor its selection. Another notable advantage of algorithm \ref{alg:luq_v2} is its ability to incorporate expert opinion as a safeguard against regions on the support of $P(\mathbf{E}|\mathbf{H}_k)$ that correspond to preferences deemed unethical or misaligned with the overarching goal of improving patient well-being. For example, end-of-life care preferences or an overly strong inclination toward drug abuse. See section \ref{sec:discussions} for a more detailed discussion of this. This variation remains valid under the same setup and assumptions previously introduced and is summarized below.

\begin{algorithm}[H]
\footnotesize
\caption{LUQ-Learning}
\label{alg:luq_v2}
\begin{algorithmic}[2]
    \STATE Specify a parametric model $P(\mathbf{W}|\mathbf{X}, \mathbf{E}, \theta)$. Specify a prior distribution on $\theta$ and $\mathbf E$, denoted $\Lambda_\theta$ and $\Lambda_E$, respectively. Denote $\gamma$ as the parameter for the outcome model, possibly infinite dimensional.
    \STATE \textbf{Input} Observed trajectories $\mathcal D=\{(\mathbf{Z}^{i}_1,\mathbf{Z}^{i}_2,\dots,\mathbf{Z}^{i}_K, \mathbf{Y}^i, \mathbf{W}^i_{K+1})\}_{i=1}^N$,  
    $\mathbf{Z}^{i}_{k} = (\mathbf{X}_k^i,\mathbf{W}_{k}^i,A_k^i)$
    \STATE \textbf{Estimate} Obtain $\hat{\theta}_n = argmax_\theta \log P(\theta|\mathbf{H}_{K+1})$ in (\ref{eq:posterior}).
    \STATE Let $\widehat{\tilde{V}^{\hat\pi}}_{K+1} \leftarrow \mathbf{Y}$.
    \FOR{k = K, K-1 to 1}
        \STATE Fit outcome model using some regression algorithm
        to obtain $\hat{\mathbb{E}}[\widehat{\tilde{V}^{\hat\pi}}_{k+1} | \mathbf{H}_k, A_k; \hat{\gamma}]$.
        \STATE Obtain $\{\hat{\pi}^{opt, b}_{k}\}_{b=1}^B$, where
        \[
        \hat{\pi}^{opt, b}_{k}(\mathbf{H}_{k}) = argmax_{a_k \in \mathcal{A}_{{H}_k}} \widehat{\tilde Q}_k(\mathbf{H}_k, a_k, \mathbf{E}_k^b) =  argmax_{a_k \in \mathcal{A}_{{H}_k}} {\mathbf{E}_k^b}^T  \widehat{\mathbb{E}}[\widehat{\tilde{V}^{\hat\pi}}_{k+1} | \mathbf{H}_k, a_k; \hat{\gamma}],
        \]
        and $\{\mathbf{E}_{k}^b\}_{b=1}^B$ i.i.d. draws from $\hat{P}(\mathbf{E} | \mathbf{H}_k; \hat{\theta}_n)$.
        \STATE Obtain $\hat{P}(\hat{\pi}^{opt}_k(\mathbf{H}_k) = a_k) = \sum_{b} I(\hat{\pi}^{opt, b}_{k}(\mathbf{H}_k) = a_k) / B$ for any $a_k \in \mathcal{A}_{H_k}$.
        \STATE Let $\widehat{\tilde{V}^{\hat\pi}}_k \leftarrow \widehat{Q}_k(\mathbf{H}_k, argmax_{a_k \in \mathcal{A}_{H_k}}\hat{P}(\hat{\pi}^{opt}_k(\mathbf{H}_k) = a_k))$.
    \ENDFOR
    \STATE \textbf{Output} $\{\hat{\pi}^{opt}_k(\mathbf{H}_k)\}_{k = 1}^{K}$, each $\hat{\pi}^{opt}_k(\mathbf{H}_k)$ as a distribution over $\mathcal{A}_{H_k}$.
\end{algorithmic}
\end{algorithm}
\normalsize

If one strictly follows the action with the highest chance of maximizing $\tilde{Q}_k(\mathbf{H}_k, A_k, \mathbf{E})$, the selected $\hat{\pi}^{opt}$ likely is not the same as that obtained from the first algorithm and thus is not ``optimal'' in the sense of (\ref{equation:opt_condition}), unless $B$ is sufficiently large so that the empirical distribution of $\mathbf{E}$ converges to that of the estimated optimal, and the mean and mode of $P(\mathbf{E}|\mathbf{H}_k)$ are the same. In this regard, algorithm 1 is more conservative in that given the same underlying preference posterior, it is more likely to optimize for preferences towards the center region of the probability simplex. This also justifies taking near-optimal actions for algorithm 2. Many downstream analyses are also possible. For example, one can examine the change of $P(\mathbf{E}|\mathbf{H}_k)$ over decision times and test for deviation of the estimated $P(\mathbf{E}|\mathbf{H}_k)$ from $Dirichlet(\mathbf{1}_d)$, the uniform distribution over $\mathcal{E}$. Finally, for both algorithms, the preference-through-weights framework offers a notable advantage: it allows for easy recovery of a traditional Q-learning algorithm that maximizes the outcome vector. This can be achieved by simply omitting $\widehat{\mathbb{E}}[\mathbf{E}|\mathbf{H}_k]$ in the maximization step, without the need to refit the outcome models. In other words, for any finite-horizon Q-learning algorithm, the above framework can be seamlessly integrated whenever the collected data includes ``preference'' information on the targeted outcomes.

\textbf{Adaptation to the Online Setting}.
In addition to learning an optimal DTR using static offline data, our framework also adapts to the case when data enters online thanks to the Sequential Weak Preference Independence (A5) assumption. Suppose subjects enter a study with prior treatment history, prior information can be summarized together into $\mathbf{X}_1$, and any outcome preference information can be summarized into $\mathbf{W}_1$ and used in the estimation of $P(\mathbf{E}|\mathbf{H}_k)$. The difference compared with the offline case is then the need to re-fit the preference model instead of fitting it once using all the data as described in (algorithm \ref{alg:luq_v1}). This aligns well with intuition: Think of $\mathbf{E} \sim \Lambda_E$ as the distribution of preference in the general population. Once patients accumulate history over time, we obtain information in the likelihood $P(\mathbf{H}_k | \mathbf{E})$, which we then use to obtain a personalized posterior. This posterior is updated as more information becomes available and represents the best guess based on current information. We leave regret bound characterization in this setting for the future work.

\textbf{Implementation Suggestions}.
We provide additional suggestions on the implementation of LUQ-Learning in this section. 
First of all, when defining the variables, as we are maximizing for $U^* = \mathbf{E}^T\mathbf{Y^*}$, it is important to standardize the units across coordinates of $\mathbf{Y}$ so that they are comparable and to properly assign signs to each coordinate (so that larger is better for all coordinates). Also, we recommend spreading coordinates of $\mathbf{Y}$ to measure different aspects around the objective so that the convex hull of $\mathbf{Y}$ is rich enough to contain the true utility. Second, although we denote the trajectory in the order of $\mathbf{X}_k$ or $\mathbf{Y}$ coming before $\mathbf{W}_k$, as reasoned in section \ref{sec:LUQ-learning}, the order in which these two are measured in reality can be reversed, as long as we can assume patient reported satisfaction $\mathbf{W}_k$ reflects the new state $\mathbf{X}_k$ after action $A_{k-1}$. Third, while we have identified all possible places where preference information can be collected, one can omit preference collection at certain stages at the cost of less precise estimates of the preference model, but the algorithm still applies. Finally, we point out that estimation of the preference model is crucial for the performance of LUQ-Learning. Thus, it is important to consider model selection for $P(\mathbf{W}_k|\mathbf{X}_k,\mathbf{H}_{k-1},\mathbf{E})$ and $\Lambda_{E}$. This can be done in the usual way of cross-fitting and using likelihood-based metrics on the held-out set, such as Bayesian Information Criterion (BIC). We suggest keeping $\Lambda_{E}$ simple, such as flat over the probability simplex or approximated from existing data, and focus on model selection for $P(\mathbf{W}_k|\mathbf{X}_k, \mathbf{H}_{k-1},\mathbf{E})$ over a diverse class of models to align well with the observed data. Prior $\Lambda_\theta$ can be seen as a penalty in the estimation of $\theta$. Supplementary Material (section S2) provides further discussion on this.



\section{Theoretical Results}
%\textcolor{red}{Theorem statement updated, wordings pending update.}\\
The proofs for all theorems in the following can be found in the Supplement (section S1). 
Denote $P$ the probability measure that corresponds to the true data-generating process, where within it,
denote $\theta_0$ the true parameter that identifies $P(\mathbf{W}_{K+1} | \mathbf{Y}, \mathbf{H}_K, \mathbf{E}) \prod_{k=1}^K P(\mathbf{W}_{k} | \mathbf{X}_k, \mathbf{H}_{k-1}, \mathbf{E})$. Denote $\hat\theta_n$ its estimate obtained following $(\ref{eq:posterior})$. 
We use $\|\cdot\|$ to denote a general norm, $\|\cdot\|_{P}$ to denote the $L^2(P)$ norm, and $\|\cdot\|_{L^\infty(P)}$ the $L^\infty(P)$ norm. We implicitly require $X \in L^2(P)$ whenever we write $\|X\|_{P}$ in the assumption.

The first theorem proves the validity of our proposed approach for estimating $\theta$. We assume that for our (selected) parametric model $\{M_{\theta}(\mathbf{H}_{K+1},\mathbf{E}): \theta \in \Theta\}$, there exists an interior point $\theta_0$ of $\Theta$ some compact normed space that indexes the part of the true conditional probability related to latent preference $\mathbf E$. That is, $P(\mathbf H_{K+1}|\mathbf E) = M_{\theta_0}(\mathbf H_{K+1}, \mathbf E)g(\mathbf{H}_{K+1})$. We denote $P(\mathbf H_{K+1};M_{\theta})$ the probability of $\mathbf H_{K+1}$ induced by model $M_{\theta}$.


\begin{theorem}
\label{thm1}
$\hat\theta_n\to_p \theta_0$ provided: 
(C1) $\exists \theta_0 $ an interior point of compact $\Theta$ such that $P(\mathbf H_{K+1}|\mathbf E) = M_{\theta_0}(\mathbf H_{K+1}, \mathbf E)g(\mathbf H_{K+1})$, with $M_{\theta}$ some partial likelihood model and $g$ some non-negative measurable function bounded from above; 
(C2) $M_\theta(\mathbf H_{K+1}, \mathbf E)$ is continuous in $\theta$ for a.s. $(\mathbf H_{K+1}, \mathbf E)$; 
(C3) $\forall \theta$, $|M_\theta(\mathbf H_{K+1}, \mathbf E)|\leq F_1(\mathbf H_{K+1}, \mathbf E)$ for some $F_1$ satisfying $\E_{\theta_0, \mathbf E, \mathbf H_{K+1}}[F_1(\mathbf H_{K+1}, \mathbf E)] < \infty$; 
(C4) $\exists c>0$ such that the measure induced by model $M_{\theta}$, $P(\mathbf H_{K+1}; M_{\theta}) >c$ a.s. in $\mathbf H_{K+1}$; 
(C5) $P(\mathbf H_{K+1}; M_{\theta_0})\neq P(\mathbf H_{K+1};M_\theta)$ for all $\theta\neq\theta_0$. 

Moreover, $\sqrt{n}(\hat\theta_n-\theta_0)\to_d \mathcal N (0,I(\theta_0)^{-1})$, provided that in addition to the above:
(N1) $I(\theta_0)$ is non-singular; 
(N2) $\forall \theta_1,\theta_2\in \mathcal{N}_\epsilon(\theta_0)=\{\theta \in \Theta:||\theta-\theta_0|| < \epsilon\}$, for any $\epsilon > 0$, $|M_{\theta_1}(\mathbf H_{K+1}, \mathbf E)-M_{\theta_2}(\mathbf H_{K+1}, \mathbf E)|\leq F_2(\mathbf H_{K+1},\mathbf E)\|\theta_1-\theta_2\|$ for some measurable function $F_2$ satisfying $\mathbb E_{\theta_0,\mathbf E}[F_{2}^2(\mathbf H_{K+1},\mathbf E)]<\infty$ a.s. in $\mathbf H_{K+1}$;
(N3) $M_{\theta}(\mathbf H_{K+1} , \mathbf E)$ is continuously differentiable in $\theta$ for a.s. all $\mathbf E$, with $||\nabla_\theta M_{\theta}(\mathbf H_{K+1},\mathbf E)||_{L^\infty(P_{\theta_0})} < G(\mathbf H_{K+1},\mathbf E)$ for some measurable function $G$ satisfying $\mathbb E_{\theta_0,\mathbf E}[G^2(\mathbf H_{K+1},\mathbf E)]<\infty$ a.s. in $\mathbf H_{K+1}$.
\end{theorem}

\textbf{Remark 4.1}:
Most of the above conditions can be verified directly using the proposed model $M_\theta(\mathbf{H}_{K+1},\mathbf{E})$, without worrying about the integral $P(\mathbf{H}_{K+1}; \theta) = \int M_\theta(\mathbf{H}_{K+1},\mathbf{E})dP(\mathbf{E})$ whose close form is often difficult to obtain.
Condition (C4) is related to $P(\mathbf H_{K+1};M_\theta)$, but can usually be easily verified. For example, if preference $\mathbf W$ lies in a compact space, then combined with $\Theta$ compact, one can derive a lower bound for $\min_{\mathbf E}M_\theta(\mathbf H_{K+1}, \mathbf{E})$, then show that the lower bound is strictly away from 0 on some non-trivial sets in $\mathcal{E}$.
Conditions (C5) and (N1), however, cannot be easily reduced to the corresponding conditions on $M_\theta(\mathbf{H}_{K+1},\mathbf{E})$, though it is standard to assume non-singularity and identifiability when deriving theoretical results for latent variable models (\cite{McCullagh1989} and \cite{Breslow1993, Bianconcini2014, Butler2018}, respectively). \\


The next theorem shows that $\hat\pi_n$ obtained from LUQ-Learning \ref{alg:luq_v1} achieves the optimal value $V(\pi^{opt})$ asymptotically, where in Lemma 1 in the Supplementary Material (section S1), we show that $\pi^{opt}$ satisfies our definition of optimality given in $(\ref{equation:opt_condition})$. With the identifiability assumption, $P_{\theta_0} = P$ denotes the truth. 

\begin{theorem}
\label{thm2}
$V_1(\widehat\pi_n)-V_1(\pi^{opt}) \to_p 0$ provided that in addition to (A1)-(A5): \\
(V1)  $||\widehat{\mathbb E}[\mathbf E|\mathbf H_K; \hat\theta_n ]-\mathbb E[\mathbf E|\mathbf H_K]||_{P_{\theta_0}}\to 0$; \\
(V2) $\|\widehat{\mathbb E}[\mathbf Y|\mathbf H_K,A_K]\|_{L^\infty(P_{\theta_0})} < \infty$ and $\widehat{\mathbb E}[\mathbf Y|\mathbf H_K,A_K] \to_p \mathbb E[\mathbf Y|\mathbf H_K,A_K]$; \\
(V3) $\| \widehat{\mathbb E}[\widehat V^{\hat\pi}_{n,k}(\mathbf H_k)|\mathbf H_{k-1},A_{k-1}]-{\mathbb E}[\widehat V^{\hat\pi}_{n,k}(\mathbf H_{k})|\mathbf H_{k-1},A_{k-1}]| \|_{P_{\theta_0}}\to 0$, for $k = 2, \dots, K$.
\end{theorem}

\textbf{Remark 4.2}:
All (V1)-(V3) can be relatively easily verified. 
(V1) requires $L^2(P_{\theta_0})$ convergence of the estimated preference model. If we assume the parametric model for $P(\mathbf H_{K+1}|\mathbf E)$ and the model for $P(\mathbf E)$ properly selected so that $\hat\theta_n \to_p \theta_0$ based on Theorem \ref{thm1} (V1) and that we use Monte-Carlo (MC) integration based on $\hat\theta_n$ to obtain $\widehat{\mathbb E}[\mathbf E|\mathbf H_K; \hat\theta_n ]$ and let the number of MC samples to grow to infinity, verification of (V1) can be much simplified, as demonstrated in the proof of Theorem \ref{thm3} in the Supplementary Material (section S1). 
(V2) requires that the estimated outcome regression model is bounded almost everywhere and that the regression model is consistent. If $\max_j |Y_j| < \infty$, which has to be true practically, using flexible regression algorithms with consistency guarantee and checking the predicted values over $\mathcal{H}_K \times \mathcal{A}_{\mathcal{H}_K}$ verifies (V2). Finally, as $\hat\pi_{n,k} = \arg\max_{A_k} \hat{Q}(\mathbf{H}_{k}, A_k) = argmax_{A_k} \hat{ \mathbb{E}}[ \hat{V}^{\hat{\pi}}_{k+1}|\mathbf{H}_{k}, A_k]$, based on backward induction, (V3) is satisfied if the regression algorithm used for the Q functions at each time point prior to $k = K$ is also good enough. Similar to (V2), this can be satisfied with a wide class of flexible regression algorithms including RF, Generalized Additive Models, Spline Regression, etc. under suitable conditions.




\section{Application to the BEST Study}
\subsection{The BEST Study}
\label{sec:best}
The Biomarkers for Evaluating Spine Treatments (BEST)
Trial is a two-stage SMART (sequential, multiple assignment, randomized trial) to investigate four evidence-based interventions targeting chronic low back pain (cLBP). It consists of two 12-week treatment periods and one 12-week follow-up period, with no washout period in between. We expect to collect complete data from at least 600 patients. The study is motivated by the observation that while many treatments show small-to-moderate average treatment effects (ATE), some patients appear to benefit substantially from certain specific treatment plans. Additionally, due to the chronic nature of cLBP, treatment plans often evolve over time, and the sequence of treatment could affect the effectiveness of the overall treatment plan. Our simulation is designed to capture the essence of the data structure in the BEST study.

Based on the design of the BEST trial, the full data trajectory $\mathbf{H}_3$ can be summarized as $(\mathbf{X}_1, \mathbf{W}_1, A_1, \mathbf{X}_2, \mathbf{W}_2, A_2, \mathbf{Y}, \mathbf{W}_3)$, where $\mathbf{Y} \in \mathbb{R}^3$, consists of cognition, pain intensity, and substance use. Three types of questionnaires were used to collect preference information. First, a questionnaire adapted from the CAPER Treatment framework \citep{wilson_preferences_2024, Wilson2023} was administered, with modified attributes to focus on outcome preferences. It contains 12 binary questions, each of which asks patients to choose one over another described scenario. Denote $\mathbf{W}^{B}_k$ responses to this questionnaire. Second, there is a question asking patients to rank the three outcomes. Denote $\mathbf{W}^{R}_k$ their ordinal responses to this question. Third, there is a questionnaire asking how satisfied are they with the most recent treatment received on a scale of one to seven. Denote responses to this question $\mathbf{W}^{Sat}_k$. Following the study design, we have $\mathbf{W}_1 = (\mathbf{W}^B_1, \mathbf{W}^R_1)$, $\mathbf{W}_2 = (\mathbf{W}^B_2, \mathbf{W}^R_2, \mathbf{W}^{Sat}_2)$, $\mathbf{W}_3 = (\mathbf{W}^{Sat}_3)$. In our simulation, we define $\mathbf{X}_1$, $\mathbf{X}_2$, and $\mathbf{Y}$ as 10 minus the PEG score \citep{Krebs2009} so that they remain on a scale of 0-10, but with a higher score corresponding to a better pain experience.

At the first randomization stage, $\mathcal{A}_{\mathcal{H}_1} = \{a_1, \dots, a_4\}$ and all subjects are randomized to one of the four treatments with equal probability. In the second randomization stage, $\mathcal{A}_{\mathcal{H}_2} = \{ \{a_j\}, \{a_j, a_k\}: j,k = 1, \dots, 4 \}$ where the specific subset depends on the observed value of $\mathbf{H}_2$. Denote the response groups after the first treatment $\mathcal{C} = \{c_1, \dots, c_4\}$. Specifically, if $C = c_1$, it indicates that a patient responds well to the first treatment, the patient maintains the previously assigned treatment plan; if $C = c_2$, we randomize subjects to a specific treatment augmenting plan; if $C = c_3$, we randomize subjects to receive a randomly assigned treatment augmentation or to switch to a randomly assigned new treatment; finally, if $C = c_4$, we consider the first treatment non-effective and randomize subjects to a new treatment. The exception is when $A_1= a_1, C\in\{c_3, c_4\}$, in which case a patient will always augment the current treatment instead of switching due to the nature of $a_1$. We refer readers to \citep{sperger2025Statistical, mauck2025best} for additional details on the trial design.

All assumptions are reasonably satisfied in this application: (A2) SUTVA is met, as each version of treatment is clearly defined; (A3) Positivity is satisfied by design; (A4) Sequential Weak Unconfoundedness is met, as the first-stage treatment assignment is fully randomized as of a Randomized Controlled Trial, and the second-stage assignment is conditionally randomized, with the response category not a confounder; Finally, (A5) Sequential Weak Independent Preference can be reasonably assumed to hold, given the availability of a rich set of expert-validated preference measures.

We specify the following model for stated preferences $\mathbf{W}$.
\footnotesize
\begin{align*}
& \mathbf{V} \sim \mathcal{N}_{2}(0,\mathbf{I}), \\
& \mathbf{E} = \text{SoftMax}((\mathbf{V}, 1)) = \frac{(\exp(\mathbf{V}), 1)}{ \sum_{j=1}^2 \exp(V_j) + 1}, \\
& \mathbf{W}^B_{1j} | \mathbf{V} \sim \text{Bern}\left(p = \sigma\left(\beta_{1,j,0}+\beta^T_{1,j,1}\mathbf{V}\right)\right), \quad (1 \leq j \leq 12),\\
& P(\mathbf{W}_1^R = \mathbf{w}^R | \mathbf{E}^R) = \frac{\exp\left(-\lambda_1 \tau(\mathbf{w}^R,\mathbf{E}^R)\right)}{\sum_{\mathbf{v}^R \in \mathcal{\textit{Perm}}} \exp\left(-\lambda_1 \tau(\mathbf{v}^R,\mathbf{E}^R)\right)}, \\
& P(\mathbf{W}^{Sat}_2 \leq j | \mathbf{X}_2, \mathbf{E}) = \sigma\left(\alpha_{2,j,0} - \alpha_{2,\cdot,1} \mathbf{E}^T \mathbf{X}_2\right), \quad (1 \leq j \leq 6), \\
& \mathbf{W}^{B}_{2j} | \mathbf{V} \sim \text{Bern}\left(p = \sigma\left(\beta_{2,j,0}+\beta^T_{2,j,1}\mathbf{V}\right)\right), \quad (1 \leq j \leq 12),\\
& P(\mathbf{W}_2^R = \mathbf{w}^R | \mathbf{E}^R) = \frac{\exp\left(-\lambda_2 \tau(\mathbf{w}^R,\mathbf{E}^R)\right)}{\sum_{\mathbf{v}^R \in \mathcal{\textit{Perm}}} \exp\left(-\lambda_2 \tau(\mathbf{v}^R,\mathbf{E}^R)\right)}, \\
& P(\mathbf{W}^{Sat}_3 \leq j | \mathbf{Y}, \mathbf{E}) = \sigma\left(\alpha_{3,j,0} - \alpha_{3,\cdot,1} \mathbf{E}^T \mathbf{Y}\right), \quad (1 \leq j \leq 6)
\end{align*}
\normalsize
where $\sigma(\cdot)$ the sigmoid function, $\tau(\cdot, \cdot)$ the Kendall's Tau metric with $\mathbf{E}^R$ the rank vector of coordinates of $\mathbf{E}$, and $\textit{Perm}$ the set of all permutations of $\{1,2,3\}$. For computational tractability and ease of comparison, we follow the modeling choice made by \citet{Butler2018} for $\mathbf{E}$ and $\mathbf{W}^B | \mathbf{V}$, assuming binary preference questions related to latent factors $\mathbf{V}$ through independent logistic regression models. Our assumed model for $P(\mathbf{W}_k^R|\mathbf{E}^R)$ is the Mallow's $\phi$ model \citep{Tang2019}. While the BEST study allows for tied ranks, our distribution assumes no tied ranks for simplicity, although it is not difficult to extend the distribution to allow for ties. $\mathbf{W}^{Sat}_k$ are assumed to be positively related to the preference-weighted outcomes via the proportional-odds logistic regression model. 

Outcomes $\mathbf{Y}$ and $\mathbf{X}_1, \mathbf{X}_2$, and covariates other than preferences, are generated as follows. The action set at the second decision time can thus be characterized as $\mathcal{A}_{\mathcal{H}_2} = \{ \{x\}, \{x, y\} : x, y \in \{a_1, \dots, a_4\}, x \neq y\}$, with cardinality $|\mathcal{A}_{\mathcal{H}_2}| = 10$.
\footnotesize
\begin{align*}
& \mathbf{X}_{1} \sim \text{Bin}(n = 10, p = 0.5)^3, \\
& A_1 \sim \text{Unif}(\mathcal{A}_{\mathcal{H}_1}), \quad \mathcal{A}_{\mathcal{H}_1} = \{a_1, \dots, a_4\}, \\
& X_{2j} \sim \text{Bin}\left(n = 10, p = \sigma\left[\sum_{a_l \in \mathcal{A}_{\mathcal{H}_1}} \gamma_{2,j,l,0} I(A_1 = a_l) + \frac{X_{1j} - \mathbb{E}[X_{1j}]}{\sqrt{\text{Var}[X_{1j}]}} \sum_{a_l \in \mathcal{A}_{\mathcal{H}_1}} \gamma_{2,j,k,1} I(A_1 = a_l) \right] \right), \quad (1 \leq j \leq 3), \\
& C \sim \text{Unif}(\mathcal{C}), \quad \mathcal{C} = \{c_1, \dots, c_4\}, \\
& A_2 = \begin{cases}
\begin{array}{lll}
\{A_1\}, & & C = c_1 \\
\{A_1, \tilde{A}\}, & \tilde{A} \sim \text{Unif}(\mathcal{A}_{\mathcal{H}_1} \setminus A_1) & C = c_2 \text{ or } (A_1 = a_1 \text{ and } C \in \{c_2, c_3, c_4\}) \\
B\{A_1, \tilde{A}\} + (1 - B)\{\tilde{A}\}, & B \sim \text{Bern}(0.5), \tilde{A} \sim \text{Unif}(\mathcal{A}_{\mathcal{H}_1} \setminus A_1) & C = c_3 \text{ and } A_1 \neq a_1 \\
\{\tilde{A}\}, & \tilde{A} \sim \text{Unif}(\mathcal{A}_{\mathcal{H}_1} \setminus A_1) & C = c_4 \text{ and } A_1 \neq a_1 \\
\end{array},
\end{cases} \\
& Y_j \sim \text{Bin}\left(n = 10, p = \sigma\left[\sum_{a_l \in \mathcal{A}_{\mathcal{H}_1}} \gamma_{3,j,l,0} I(a_l \in A_2) + \frac{X_{2j} - \mathbb{E}[X_{2j}]}{\sqrt{\text{Var}[X_{2j}]}} \sum_{a_l \in \mathcal{A}_{\mathcal{H}_1}} \gamma_{3,j,l,1} I(a_l \in A_2) \right] \right), \quad (1 \leq j \leq 3).
\end{align*}
\normalsize 

Denote ${\theta} = ({\alpha}, {\beta}, {\lambda})$ the unknown true parameters related to the preference model and $\gamma$ parameters for the outcome model, where ${\alpha} = (\alpha_{k,j,0}, \alpha_{k,\cdot,1})^{k=3, j = 6}_{k = 2, j = 1}$, ${\beta} = (\beta_{k,j,0}, \beta_{k,j,1})^{k=2, j = 3}_{k = 1, j = 1}$, ${\lambda} = (\lambda_k)_{k=1}^{k=2}$, ${\gamma} = (\gamma_{k,j,l,0}, \gamma_{k,j,l,1})^{k=3, j = 12, l = 4}_{k = 2, j = 1, l = 1}$. Throughout, we use $k$ to index the decision times, $j$ to index dimensionality; and $l$ to index over the action set. 

We generate the parameters as follows.
\footnotesize
\begin{align*}
&\beta_{1,j,0}=0, \quad \beta_{1,j,1} {\sim} \mathcal N_2(0,\mathbf{I}), \quad (1 \leq j \leq 12),\\
&\beta_{2,j,0}=0,\quad \beta_{2,j,1}=\sqrt{0.8}\beta_{1,j,1}+\sqrt{0.2}\epsilon_{\beta},\quad \epsilon_{\beta} {\sim} \mathcal N_2(0,\mathbf{I}), \quad (1\leq j \leq 12), \\
&\alpha_{2,\cdot, 1}=0.5,\quad \alpha_{2,j,0}=0.75 j, \quad (1\leq j \leq 6), \\
&\alpha_{3,\cdot,1}=0.6,\quad \alpha_{3,j,0}=\alpha_{2,j,0} + 0.5, \quad (1\leq j \leq 6), \\
&\lambda_1=0.5,\quad \lambda_2=2, \\
&\gamma_{2,j,l,0} \sim \mathcal{N}(0,0.5^2),\quad  \gamma_{2,j,l,1} \sim \mathcal{N}(0,1), \quad ( 1\leq j \leq 3, \quad 1 \leq l \leq 4), \;\mbox{and}\\
&\gamma_{3,j=1,l,0} = 0, \quad \gamma_{3,j=3,l,0} = -\gamma_{3,j=2,l,0}, \quad (1 \leq l \leq 4), \;\mbox{with}\\
&\gamma_{3,j=2,l,0} = \sqrt{0.8}\gamma_{2,j=2,l,0}+\sqrt{0.2}\epsilon_{\gamma}, \quad \epsilon_{\gamma} {\sim} \mathcal{N}(0,0.5^2),\\
&\gamma_{3,j,l,1}=0, \quad (1 \leq j \leq 3, \quad 1 \leq l \leq 4)
\end{align*}
\normalsize

We set up parameters $\gamma$ so that, at the second decision time, $A_2$ has opposing effects on $Y_2$ and $Y_3$, with $Y_1\sim \text{Binomial}(n=10,p=0.5)$, $Y_2\sim \text{Binomial}(n=10,p=\sigma(g(A_2)))$ and $Y_3\sim \text{Binomial}(n=10,p=\sigma(-g(A_2)))$ where $g(A_2)= \sum_{a_l \in \mathcal{A}_{\mathcal{H}_1}} \gamma_{3,2,l,0}I(a_l \in A_2)$. Now $Y_1$ can be thought of as a random intercept in our utilities $\mathbf{E}^T\mathbf{Y}$, while $Y_2$ and $Y_3$ can be thought of as conflicting outcomes. As $\mathbf{X}_2$ is now independent of $\mathbf{Y}$, it may appear that covariates are no longer relevant to the DTR. However, this is not the case: the observed data is still useful for estimating the expected value of $\mathbf{E}$, which determines the best sequence of treatments to take. 

We end this subsection with a theorem that identifies sufficient conditions for the consistency and asymptotic normality of $\hat\theta_n$, as well as for $||\widehat{\mathbb E}[\mathbf{E}|\mathbf{H}_2; \hat\theta_n]-\mathbb E[\mathbf{E}|\mathbf{H}_2;\theta_0]||_{P_{\theta_0}}\to 0$, which is one of the conditions required for the convergence of $V(\hat\pi_n)$  (Theorem \ref{thm2}). 
We note that the remaining conditions in Theorem \ref{thm2} can also be reasonably assumed to hold based on the consistency of Random Forest (RF) \citep{Scornet2015}, boundedness of outcome $\max_j |Y_j| \leq 10$, and $\mathcal{A}_{\mathcal{H}_1} \subset \mathcal{A}_{\mathcal{H}_2}$ with $\mathcal{H}_2 \times \mathcal{A}_{\mathcal{H}_2}$ discrete.

\begin{theorem}
\label{thm3}
Under the proposed model and the estimation method described above,
$\hat\theta_n\to_{p}\theta_0$ as $n \to \infty$ and 
$||\widehat{\mathbb E}[\mathbf{E}|\mathbf{H}_2; \hat\theta_n]-\mathbb E[\mathbf{E}|\mathbf{H}_2;\theta_0]||_{P_{\theta_0}}\to 0$ as $N_{sim}, n \to \infty$ 
provided that there exists some interior point $\theta_0\in\Theta$ compact such that $P(\mathbf{H}_{3}|\mathbf{V})=M_{\theta_0}(\mathbf{H}_{3}, \mathbf E)g(\mathbf{H}_3)$ almost surely; $P(\mathbf{H}_{3};M_{\theta_0}) \neq P(\mathbf{H}_{3}; M_\theta)$ for all $\theta\neq\theta_0$; and $g(\mathbf{H}_{3})> c$ for some $c>0$. Moreover, $\sqrt{n}(\hat\theta_n -\theta_0)\to_d \mathcal N (0,I(\theta_0)^{-1})$ as $n \to \infty$, provided that in addition to the above, $I(\theta_0)$ is non-singular. 
\end{theorem}

\subsection{Simulation Result}
The following result is based on LUQ-learning (algorithm \ref{alg:luq_v1}). Monte Carlo (MC) integration is used for calculating expected preference given history with $N_{sim} = 1000$ and can be expressed as
\begin{align*}
    \widehat{\mathbb{E}}[\mathbf{E}|\mathbf{H}_2; {\widehat\theta_n}]=\frac{\sum_{b=1}^{N_{\text{sim}}}\mathbf{E}_{\text{MC}}^{(b)}{{P}}(\mathbf{H}_2|\mathbf{V}_{\text{MC}}^{(b)}, \widehat{\theta}_n)}{\sum_{b=1}^{N_{\text{sim}}}{{P}}(\mathbf{H}_2|\mathbf{V}_{\text{MC}}^{(b)}, \widehat{\theta}_n)},
\end{align*}
where $\mathbf{V}_{\text{MC}}^{(b)}$ and $\mathbf{E}_{\text{MC}}^{(b)}$ the b-th MC draw from the proposed model.
We considered sample sizes between 150 and 2500, which explores around 600. As $Card(\theta) = 88$, sample sizes 150 and 300 are toward the extreme end of small-sample settings. For each N, we ran 10 replicates using different random seeds. In each replicate, parameters were sampled and used to generate training data. Testing data were generated independently using a different seed and matched in size to the training data. In all reported tables regarding $V(\hat\pi)$, $\hat\pi$ is estimated from the training data while its value $V(\hat\pi)$ computed on the testing data.

We consider two alternative Q-learning algorithms for comparisons: Q-learning \citep{Schulte2014} with the objective set as the average of $\mathbf{Y}$ and with the objective set to be $\mathbf{W}^{Sat}$, the reported preference collected at the end of the study. To investigate information loss during preference modeling, we consider the case where the true preference is known. This is done by replacing $\hat{\mathbb{E}}[\mathbf{E}|\mathbf{H}_2; \hat\theta_n]$ with the truth at $k = 2$; replacing $\mathbf{W}$ with $\mathbf{E}$ in the Q model at $k = 1$, and consequently letting $\pi_{Known}$ a function that accepts $\mathbf E$. At each $N$, all algorithms share the same training and testing data. Random Forest (RF) with 500 trees is used to fit $\mathbb{E}[\mathbf{Y}|\mathbf{H}_K, A_K]$, $\mathbb{E}[\widehat{V}^{\hat\pi^{opt}}_{k+1}|\mathbf{H}_k, A_k]$, and the estimated optimal DTR for all algorithms. RF is chosen to remain flexible and mitigate the issue of small training sample size relative to the state and action space, especially for cases where $N < 600$. Training was done using R packages \textit{caret} and \textit{ranger} with hyperparameters \textit{mtry} and \textit{minimum node size} selected using a grid search via 5-fold cross-validation. The candidate values for \textit{mtry} are set to center around $\left\lfloor \sqrt{\text{number of predictors}} \right\rfloor$ and that for minimum node size are set to be 5, 10, and 25, following common practice \citep{breiman2001, Probst2018HyperparametersAT}.

For LUQ-Learning, we run preference model estimation on a GPU using TensorFlow \citep{tf2015}. Reverse-mode automatic differentiation \citep{Geron2019} is used to compute $\nabla_\theta\log P(\theta|\mathcal D)$, and L-BFGS algorithm \citep{Liu1989} is used for optimization with 5 random starting points, and the estimate that yields the largest observed log-likelihood is selected. We also performed 500 simple gradient descent steps with a small learning rate prior to applying L-BFGS to improve stability. Finally, to constrain $\hat\theta_n$ to be within $\Theta$, a penalty of $-\sum_m(1/100)e^{-100c_m}$ is added to the objective where $c$ a vector of linear combinations of coordinates of $\theta$ that we assumed to be positive. This penalty acts as a smooth approximation of the hard constraint $-\infty I(\min(c)<0)$ or $-\infty I(\theta\notin\Theta)$. 

\begin{table}[H]
\centering
\captionsetup{margin=2cm}
\caption{Mean (SD) of $V(\hat{\pi}) - V(\pi_{obs})$ across Sample Sizes.}\label{table:best_n}
\begin{tabular}{lllllll}
\hline
DTR & N = 150 & N = 300 & N = 600 & N = 1200 & N = 2500 \\
\hline
$\hat\pi_{\text{Known}}$ & 0.60 (0.12) & 0.67 (0.13) & 0.67 (0.11) & 0.647 (0.102) & 0.678 (0.099)  \\
$\hat\pi_{\text{LUQL}}$ & 0.31 (0.18) & 0.43 (0.09) & 0.41 (0.09) & 0.410 (0.053) & 0.433 (0.048) \\
$\hat\pi_{\text{Wlast}}$ & 0.08 (0.19) & 0.21 (0.12) & 0.31 (0.14) & 0.362 (0.126) & 0.426 (0.115) \\
$\hat\pi_{\text{Naive}}$ & -0.07 (0.13) & 0.03 (0.11) & 0.03 (0.05) & -0.005 (0.059) & 0.027 (0.041) \\
\hline
\end{tabular}
\end{table}


\noindent
The value the estimated DTR-s improved compared with that observed is shown in Table (\ref{table:best_n}). In the presence of competing outcomes in $\mathbf{Y}$, for example, treatment effects and side effects, we can see that $\hat\pi_{\text{LUQL}}$ performs better than $\hat\pi_{Wlast}$, and both perform much better than $\hat\pi_{\text{Naive}}$ at all sample sizes, highlighting the benefits of LUQ-Learning and incorporating outcome preferences into the objective more generally. In the Supplementary Material (section S3.2), we considered the case of model mis-specification by letting the true latent preference to be uniform on the simplex instead. The results show that LUQ-Learning remains a better choice compared with the two baseline algorithms, although the gap between $\hat\pi_{Known}$ and $\hat\pi_{LUQL}$ widens.
\\

\textbf{Effect of trajectory length}\\
To demonstrate LUQ-Learning applies to scenarios with more than two decision stages, we consider the following data generating process, which is more general than the previous setup tailored towards BEST. We generate treatment assignment generated uniformly over $\mathcal{A} = \{a_1, \dots, a_4\}$ at all stages, considered only the binary questionnaires and reported satisfaction after treatment, and remove the opposing effect of $A$ on $Y_2$ and $Y_3$. As expected, this would reduce the difference between LUQ-Learning and the naive approach. The data trajectory is now $(\mathbf{X}_1, \mathbf{W}_1, A_1, \dots, \mathbf{Y}, \mathbf{W}_{K+1})$, where $\mathbf{W}_1 = (\mathbf{W}^B_1), \mathbf{W}_k = (\mathbf{W}^B_k, \mathbf{W}^{Sat}_k), k \in \{2, \dots, K\},$ and $\mathbf{W}_{K+1} = (\mathbf{W}^{Sat}_{K+1})$. Training and testing sample size are fixed to be 600, with optimization scheme and modeling approaches same as before.
\footnotesize
\begin{align*}
    & \mathbf{V} \sim \mathcal N_{2}(0,\mathbf I), \quad 
    \mathbf E=\text{SoftMax}(( \mathbf{V}, 1)) = \frac{( \exp(\mathbf{V}), 1)}{\sum_{j=1}^2 \exp(V_j)+1}, \\
&\text{At k = 1:}\\
    &\quad \mathbf{X}_{1} \sim \text{Bin}\left(n = 10, p = 0.5\right)^3, \\
    &\quad W^{B}_{1j} \sim \text{Bern}\left( p = \sigma(\beta_{1,j,0} + \beta_{1,j,1}^T \mathbf V) \right), \quad (1 \leq j \leq 2), \\
    &\quad A_1 \sim Unif(\mathcal{A}_{\mathcal{H}_1}), \quad \mathcal{A}_{\mathcal{H}_1} = \{a_1, \dots, a_4\}. \\
& \text{At $k = 2, \dots, K$:}\\
    &\quad X_{kj} \sim \text{Bin}\left(n = 10, p = g(A_{k-1}, X_{k-1,j}) \right), \quad (1 \leq j \leq 3), \\
    &\quad g(A_{k-1}, X_{k-1,j}) = \sigma\left[\sum_{a_l \in \mathcal{A}_{\mathcal{H}_{k-1}}} \gamma_{k,j,l,0} I(A_{k-1} = a_l) + \frac{X_{k-1,j} - \mathbb{E}[X_{k-1,j}]}{\sqrt{\text{Var}[X_{k-1,j}]}} \sum_{a_l \in \mathcal{A}_{\mathcal{H}_{k-1}}} \gamma_{k,j,l,1} I(A_{k-1} = a_l) \right],\\
    &\quad W^{B}_{kj} \sim \text{Bern}\left( p = \sigma(\beta_{k,j,0} + \beta_{k,j,1}^T \mathbf V) \right), \quad (1 \leq j \leq 2), \\
    &\quad P(\mathbf{W}^{Sat}_k \leq j | \mathbf{Y}, \mathbf{E}) = \sigma\left(\alpha_{k,j,0} - \alpha_{k,\cdot,1} \mathbf{E}^T \mathbf{X}_k\right), \quad (1 \leq j \leq 2),\\
    &\quad A_k \sim Unif(\mathcal{A}_{\mathcal{H}_k}), \quad \mathcal{A}_{\mathcal{H}_k} = \{a_1, \dots, a_4\}. \\
&\text{At k = K+1:}\\
    &\quad Y_{j} \sim \text{Bin}\left(n = 10, p = g(A_{K}, X_{K,j}) \right), \quad (1 \leq j \leq 3), \\
    &\quad g(A_{K}, X_{K,j}) = \sigma\left[\sum_{a_l \in \mathcal{A}_{\mathcal{H}_{K}}} \gamma_{k,j,l,0} I(A_{K} = a_l) + \frac{X_{K,j} - \mathbb{E}[X_{K,j}]}{\sqrt{\text{Var}[X_{K,j}]}} \sum_{a_l \in \mathcal{A}_{\mathcal{H}_{K}}} \gamma_{k,j,l,1} I(A_{K} = a_l) \right],\\
    &\quad \mathbf{W}^{Sat}_{K+1} \text{ generated by the same means as when k = 2, ..., K.}
\end{align*} \normalsize
Parameters are generated as follows. We let parameters at the next stage to be positively correlated to the previous stage with some additive random noise. In this setting, for $K \geq 2$, $\theta(K) = (\alpha(K), \beta(K))$, where $\alpha(K) = (\alpha_{k,j,0}, \alpha_{k,\cdot,1})_{k=2,j=1}^{k=K, j = 2}$, $\beta(K) = (\beta_{k,j,0}, \beta_{k,j,1})_{k=1,j=1}^{k=K, j=2}$, so $Card(\theta(K)) = 7K-3$.
\footnotesize
\begin{align*}
&\text{At k = 1:}\\
    &\quad \beta_{1,j,0} = 0, \quad \beta_{1,j,1} \sim \mathcal{N}_2(0, \mathbf{I}), \quad (1 \leq j \leq 2).\\
&\text{At k = 2}\\
    &\quad \alpha_{2,j,0} = 0.75 j, \quad \alpha_{2,\cdot,1} = 0.6 + 0.05 - 0.1 K, \quad (1 \leq j \leq 2),\\
    &\quad \beta_{2,j,0} = 0, \quad \beta_{2,j,1} \sim \mathcal{N}_2(0, \mathbf{I}), \quad (1 \leq j \leq 2),\\
    &\quad \gamma_{2,j,l,0} \sim \mathcal{N}(0, 0.5^2), \quad \gamma_{2,j,l,1} \sim \mathcal{N}(0, 1), \quad (1 \leq j \leq 3, \;\; 1 \leq l \leq 4). \\
&\text{At k = 3 to K}\\
    &\quad \alpha_{k, j, 0} = \alpha_{1, j, 0} + (k-1)/(4 (K-1)), \quad 
    \alpha_{k, \cdot, 1} = 0.6 + 0.05 (k-1) - 0.1 K, \quad (1 \leq j \leq 2),\\
    &\quad \beta_{k,j,0} = 0, \quad  \beta_{k, j, 1} = \sqrt{0.8} \beta_{k-1,j,1} + \sqrt{0.2} \epsilon_\beta, \quad \epsilon_\beta \sim \mathcal{N}(0, 1), \quad (1 \leq j \leq 2), \\
    &\quad \gamma_{k,j,l,0} = \sqrt{0.8} \gamma_{k-1,j,l,0} + \sqrt{0.2} \epsilon_{\gamma_0}, \quad \epsilon_{\gamma_0} \sim \mathcal{N}(0, 0.5^2), \\
    &\quad \gamma_{k,j,l,1} = \sqrt{0.8} \gamma_{k-1,j,l,1} + \sqrt{0.2} \epsilon_{\gamma_1}, \quad \epsilon_{\gamma_1} \sim \mathcal{N}(0, 1) \quad (1 \leq j \leq 3, \;\; 1 \leq l \leq 4).\\
&\text{At k = K + 1}\\
    &\quad \gamma_{k,j,l,1} = \sqrt{0.8} \gamma_{K,j,l,1} + \sqrt{0.2} \epsilon_{\gamma_1}, \quad \epsilon_{\gamma_1} \sim \mathcal{N}(0, 1) \quad (1 \leq j \leq 3, \;\; 1 \leq l \leq 4).
\end{align*}
\normalsize
The results are summarized in Table (\ref{table:best_K}). The relationships of improvement in value across algorithms are consistent across trajectory lengths and sample sizes: In all cases, DTR obtained from LUQ-Learning outperforms Q-learning that optimizes for the lastly reported satisfaction across all trajectory lengths, demonstrating the advantage of including additional preference data collected during the decision-making process. LUQ-Learning also outperforms Q-Learning that optimizes for the average of $\mathbf{Y}$, demonstrating the utility of latent variable modeling even in the absence of conflicting outcomes. Finally, LUQ-Learning is competitive with the setting in which the true preference is known. Although the difference between $V(\hat \pi_{LUQL})$ and $V(\hat \pi_{Known})$ stays relatively constant across trajectory lengths, we hypothesize that this is due to the linear expansion of the parameter space with respect to $K$. However, as shown in Figure \ref{fig:comptime_k}, $\|\hat\theta_n - \theta_0\|_1/dim(\hat\theta_n)$ grows with increasing trajectory length for both moderate and large sample sizes, suggesting caution when applying our framework to long decision sequences. Further investigation into alternative regularization strategies or adaptive estimation techniques could help mitigate these effects, which we consider one important area for future work.


\begin{table}[H]
\centering
\captionsetup{margin=2cm}
\caption{Mean (SD) of $V(\hat{\pi}) - V(\pi_{obs})$ across Trajectory Lengths.}\label{table:best_K}
\begin{tabular}{lllllll}
\hline
N & DTR & K = 2 & K = 4 & K = 6 & K = 8 & K = 10 \\
\hline
\multirow{4}{*}{600} 
& $\hat\pi_{\text{Known}}$ & 1.23 (0.32) & 1.42 (0.28) & 1.47 (0.21) & 1.43 (0.22) & 1.35 (0.16) \\
& $\hat \pi_{\text{LUQL}}$ & 1.42 (0.97) & 1.23 (0.27) & 1.27 (0.22) & 1.26 (0.21) & 1.13 (0.20) \\
& $\hat\pi_{\text{Wlast}}$ & 0.99 (0.93) & 0.66 (0.16) & 0.63 (0.29) & 0.22 (0.38) & 0.13 (0.29) \\
& $\hat\pi_{\text{Naive}}$ & 1.34 (1.06) & 1.21 (0.30) & 1.21 (0.19) & 1.13 (0.23) & 1.10 (0.18) \\
\hline
\multirow{4}{*}{2500} 
& $\hat\pi_{\text{Known}}$ & 1.40 (0.41) & 1.43 (0.34) & 1.58 (0.25) & 1.43 (0.18) & 1.42 (0.13) \\
& $\hat \pi_{\text{LUQL}}$ & 1.23 (0.42) & 1.26 (0.28) & 1.42 (0.29) & 1.25 (0.18) & 1.03 (0.16) \\
& $\hat\pi_{\text{Wlast}}$ & 1.01 (0.28) & 0.95 (0.36) & 0.79 (0.36) & 0.44 (0.33) & 0.20 (0.28) \\
& $\hat\pi_{\text{Naive}}$ & 1.14 (0.42) & 1.14 (0.32) & 1.26 (0.30) & 1.13 (0.15) & 1.14 (0.15) \\
\hline
\end{tabular}
\end{table}



% Figure environment removed

Finally, we note that simulation results under the setting designed for the Clinical Antipsychotic Trials of Intervention Effectiveness (CATIE) trial are included in the Supplementary Material (section S4). In this setup, there is only one decision stage, so DTR estimated from Butler's approach \citep{Butler2018} is also included as a comparator. We found LUQ-Learning consistently outperforms Butlers approach as well as naive and last-outcome-based Q-learning across sample sizes. 

All scripts used to create the simulation results can be found on GitHub at: \href{https://github.com/yatingz205/LUQ-Learning.git}{/LUQ-Learning.git}.


\section{Discussions}
\label{sec:discussions}
Despite the prevalence of healthcare decision-making problems with multiple outcomes of interest, the few applicable solutions from previous work suffer from limitations that hinder applicability to settings such as the BEST study. To address these challenges, we propose LUQ-Learning, a novel framework that integrates latent variable modeling into Q-learning by optimizing a preference-weighted latent utility. 
This approach personalizes treatment recommendations based on individual outcome preferences, optimizing a more holistic measure of quality of life rather than treatment effectiveness alone. While not the primary aim, it may also enhance adherence by improving the treatment experience.
Unlike previous approaches, LUQ-Learning accommodates an arbitrary number of time points and outcomes, avoids direct ranking of trajectories, and systematically identifies all potential decision points where preference data can be collected. Additionally, it establishes the sufficient conditions that must be met within a causal inference framework to enhance the estimation accuracy of latent utilities. 

Theoretical performance of our approach was investigated, where we demonstrated that our application to the BEST study achieves consistency and asymptotic normality under mild assumptions. Our theoretical results extend easily to other proposed latent models as well, such as that proposed for the CATIE study. Extensive simulations highlight LUQ-Learnings flexibility and robust performance across varying sample sizes and trajectory lengths. In contrast, dynamic treatment regimes (DTRs) optimized using more nave utility functions such as self-reported satisfaction at the end of the study or the mean of observed outcomes exhibited inferior performance. 

Despite our progress in multi-objective, preference-incorporated precision medicine, several promising directions remain for future work. For example, while our theoretical results make fewer assumptions than those of many previous approaches, they still assume identifiability of the latent model. Developing new theoretical results and proof techniques to establish identifiability of likelihoods with integrals in the objective function would benefit not only our method, but also for other latent variable and hierarchical Bayes models \citep{Givens2012}. Additionally, although we use a parametric modeling approach, nonparametric Bayesian methods such as Dirichlet process mixtures or Plya tree models could be used to sample from the posterior $P(\mathbf{E}|\mathbf{H}_k)$. Exploring the adaptability of LUQ-Learning to nonparametric Bayes in complex data-generating processes is a promising direction. Finally, extending our method to handle nonlinear utility functions and censored outcomes would also be valuable.

Personalized pain management for chronic conditions, as well as areas such as personalized nutrition plans, exercise recommendations, and physical rehabilitation strategies, offer relatively safer contexts where patient preferences can be incorporated to enhance treatment experiences without significant ethical concerns or unintended harm. However, careful consideration is required when defining key variables that shape the framing of the problem, particularly when utilizing a preference-incorporated objective.
Problems might arise if patients lack full awareness of the long-term consequences associated with the outcomes. If their reported preferences are formed primarily based on immediate experiences rather than long-term well-being, this could lead to myopic decision-making, resulting in dynamic treatment regimes (DTRs) that ultimately do not serve their best interests. Researchers must ensure that patient-reported preferences are well-informed and reflect a comprehensive understanding of potential trade-offs over time. Furthermore, it is essential to align the population used to develop the DTR with the population on which it will be applied. Cultural differences, regional variations, and socioeconomic factors can influence preferences for certain treatments or health outcomes. Applying a preference-based DTR derived from one group to another without appropriate adjustments could introduce bias.

Additionally, the preference-incorporated framework implicitly assumes alignment between the objectives of three key stakeholders: the algorithm generating recommendations, the healthcare providers implementing the treatments, and the patients reporting their preferences. In practice, this assumption may not always hold. Providers or patients may have incentives to manipulate the system to serve their own interests. Safeguards should be in place to prevent gaming the algorithm and ensure that preference-based DTRs remain patient-centered and ethical. 

In conclusion, while integrating patient preferences into treatment decision-making holds significant promise for improving care and enhancing patient satisfaction, it requires careful thinking in defining the study question, rigorous study design, diagnostics after model fitting, and ethical oversight along the process. When thoughtfully implemented, preference-incorporated approaches have the potential to provide highly personalized, cost-effective, and patient-centered treatments, leveraging simple yet powerful validated tools like structured questionnaires rather than costly laboratory tests or medical procedures. Overall, LUQ-Learning contributes to more adaptive, data-driven, and patient-centered decision-making in precision medicine.


  \section{Acknowledgements}
  The authors thank John Sperger for relevant references and helpful discussion. 
  
  \section{Funding}
  This research was supported by the National Institutes of Health (NIH) through the NIH HEAL Initiative under award number 1U24 AR076730-01 and is part of the Back Pain Consortium (BACPAC). The BACPAC Research Program is administered by the National Institute of Arthritis and Musculoskeletal and Skin Diseases (NIAMS). The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health or its NIH HEAL Initiative. The last author was also supported in part by the Center for Artificial Intelligence and Public Health at the University of North Carolina at Chapel Hill.

  
  \section{Disclosure Statement}
  The authors report that there are no competing interests to declare.


\printbibliography

\newpage
\appendix
\part*{Appendix}
\counterwithin{figure}{section}
\newtheorem{lemma}{Lemma}
\newcommand{\argmax}[0]{\text{argmax}}
\newcommand{\bI}[0]{\mathbf{I}}
\newcommand{\bV}[0]{\mathbf{V}}
\newcommand{\bv}[0]{\mathbf{v}}
\newcommand{\bW}[0]{\mathbf{W}}
\newcommand{\bw}[0]{\mathbf{w}}
\newcommand{\bH}[0]{\mathbf{H}}
\newcommand{\bh}[0]{\mathbf{h}}
\newcommand{\bX}[0]{\mathbf{X}}
\newcommand{\bE}[0]{\mathbf{E}}
\newcommand{\bY}[0]{\mathbf{Y}}
\newcommand{\iid}[0]{\overset{iid}\sim}
\newcommand{\Bern}[0]{\text{Bernoulli}}
\newcommand{\ind}[0]{\overset{ind}\sim}
\newcommand{\indep}{\perp \!\!\! \perp}
\newcommand{\eps}[0]{\epsilon}
\newcommand{\MC}[0]{\text{MC}}


\section{Proof of Theorems}
\label{append:proofs}
In the following, denote $\theta_0$ the parameter that identifies the true preference model, $\pi^{opt}$ the true optimal policy (in the class of deterministic policies). Denote $\hat\theta_n$ its estimate obtained by maximizing the data log posterior. 
Denote $P$ the true probability measure that corresponds to the observed data. 
Recall our definition of the Q-function: 
\[
Q^{\pi}_k(\bH_k, A_k) = \E_{A_{k+1}, \dots, A_K \sim \pi, \bX_{k+1}, \bW_{k+1}, \dots, \bY \sim P_{\theta_0}}[\bE^T\bY^*(A_1, \dots, A_k, A_{k+1}, \dots, A_K) | \bH_k, A_k].
\]
Recall $(A_1, \dots, A_{k-1}) \subset \bH_k$ so that all actions before $A_{k+1}$ are conditioned. 
Accordingly, denote 
\[V_k^{\pi^{opt}}(\bH_k) 
    = \max_{a_k \in \mathcal{A}_{\bH_k}}Q_k^{\pi^{opt}}(\bH_k, a_k) 
    = \max_{a_k \in \mathcal{A}_{\bH_k}}\mathbb{E}[\mathbf{E}^T \mathbf{Y}^*|\mathbf{H}_k, a_k].\]
For any $\pi \in \Pi$, the class of deterministic policies,
\[
\widehat{V}_k^{\pi}(\bH_k) 
    = \widehat{Q}_k^{\pi}(\bH_k, \pi_k) 
    = \widehat{\E}[\widehat{V}_{k+1}^{\pi}(\bH_{k+1}) \mid \bH_{k}, \pi_k].
\]
Denote $\|\cdot\|$ the general norm, $\|\cdot\|_{L^\infty(P_{\theta_0})}$ the $L^\infty(P_{\theta_0})$ norm, and $\|\cdot\|_{P_{\theta_0}}$ the $L^2(P_{\theta_0})$ norm, so that for example $\|Q^{\pi^{opt}}(\bH_k, A_k)\|_{P_{\theta_0}} = \mathbb{E}_{A_{k} \sim \mu_k, \bH_k \sim P_{\theta_0}}[Q^{\pi^{opt}}(\bH_k, A_k))]$. We implicitly require $X \in L^2(P_{\theta_0})$ whenever we write $\|X\|_{P_{\theta_0}}$ in the assumption. Denote also $\mu_k(A_k|\bH_k)$ the behavior policy; that is, $\mu_k(A_k|\bH_k) = P(A_k | \bH_k)$.

\vspace*{1cm}
\noindent
We begin with the proof of Lemma that justifies LUQ-Learning by showing that, without approximation error, LUQ-Learning finds $\pi^{opt}$ the true optimal. This is a direct modification of Section A.1 in the Supplement of \citep{Schulte2014}.
\begin{lemma}
\label{lemma:luq_opt}
    LUQ-Learning (Algorithm 1) based on the true Q and value function finds $\pi^{opt}$ satisfying the optimal condition: $V_1^{\pi^{\text{opt}}}(\mathbf{H}_1) \geq V_1^{\pi}(\mathbf{H}_1), \quad \forall \pi \in \Pi, \; \forall \mathbf{H}_1 \in \mathcal{H}_1$.
\end{lemma}

\begin{proof}
At $k = K$, we have that for any $\Bar{a}_K \in \otimes_{k=1}^K \mathcal{A}_{\mathcal{H}_k}$,
\begin{align*}
    &\E[\bE^T \bY^*(\Bar{a}_{K-1}, a_K) | \bX_1, \bW_1, A_1, \bX^*_2(A_1), \bW^*_2(A_1), \dots, \bW^*_K(A_1, \dots, A_{K-1})] \\
    = &\E[\bE^T \bY(\Bar{a}_{K-1}, a_K) | \bX_1, \bW_1, A_1, \bX_2, \bW_2, \dots, \bW_K] \\
    \leq &\E[\bE^T \bY(\Bar{a}_{K-1}, \pi^{opt}_K) | \bH_K] = V^{\pi^{opt}}_K(\bH_K).
\end{align*}
The first equality holds because, once $A_1$ conditioned on, $\bW^*_2(a_1) = \bW_2$ and $\bX^*_2(a_1) = \bX_2$, the observed data, and similarly for all $\{\bW^*_k, \bX^*_k\}_{k=3}^{K}$. Finally, the conditioning set includes all $(A_1, \dots, A_K)$, so $\bY^* = \bY$ by (A1) and (A2). The inequality follows from the algorithm that 
\[
\pi^{opt}_K = \argmax_{a_K} \E[\bE^T \bY(\Bar{a}_{K-1}, a_K) | \bX_1, \bW_1, A_1, \bX_2, \bW_2, \dots, \bW_K]
\]
The last equality comes from the definition of $V^{\pi^{opt}}_K$.
Taking expectation on both sides gives 
\begin{align}
    \E[\bE^T \bY(\Bar{a}_{K-1}, a_K) | \bH_1] \leq \E[\bE^T \bY(\Bar{a}_{K-1}, \pi^{opt}_K) | \bH_1] \;\; \forall \Bar{a}_K \in \otimes_{k=1}^K \mathcal{A}_{\mathcal{H}_k}.
    \label{eq:ineqK}
\end{align}
Similarly,
at $k = K - 1, \dots, 1$, for any $\Bar{a}_{k}$,
\begin{align*}
    &\E[\bE^T \bY(\Bar{a}_{k-1}, a_k, \pi^{opt}_{k+1} \dots, \pi^{opt}_K) | \bX_1, \bW_1, \dots, \bW_k] \\
    \leq &\E[\bE^T \bY(\Bar{a}_{k-1}, \pi^{opt}_k, \pi^{opt}_{k+1} \dots, \pi^{opt}_K) | \bH_k] = V^{\pi^{opt}}_k(\bH_k), \;\; \text{implying}
\end{align*}
\begin{align}
    \E_{a_{k+1}, \dots, a_K \sim \pi^{opt}}[\bE^T \bY(\Bar{a}_{k-1}, a_k) | \bH_1] \leq \E[\bE^T \bY(\Bar{a}_{k-1}, \pi^{opt}_k) | \bH_1], \;\; \forall \Bar{a}_k \in \otimes \mathcal{A}_{\mathcal{H}_k}.
    \label{eq:ineqk}
\end{align}
Consequently, chaining inequalities \ref{eq:ineqK} and \ref{eq:ineqk}, we have
\begin{align*}
    \E[\bE^T Y(\Bar{a}_K) | \bH_1] 
    \leq \cdots \leq \E[\bE^T \bY(a_1, \pi^{opt}_2, \dots, \pi^{opt}_{K-1}, \pi^{opt}_K) | \bH_1]
    \leq \E[\bE^T \bY(\pi^{opt}) | \bH_1],
\end{align*}
completing the proof.
\end{proof}

\vspace*{1cm}
\noindent
Next, we prove asymptotic results regarding $\hat\theta_n$.\\
\textbf{Proof of Theorem 4.1:}
\begin{proof}
Denote $L_n(\theta)=\sum_{i=1}^n\log P(\bH_{K+1}^i; M_\theta)$ and $L(\theta)=\mathbb E\left[\log P(\bH_{K+1}^i; M_\theta)\right]$, where $P(\bH^i_{K+1}; M_\theta) = \int_{\mathcal E} M_{\theta}(\bH^i_{K+1}, \bE)g(\bH^i_{K+1}) dP(\bE)$. Denote $P_n(\bH_{K+1}; M_\theta) = \frac{1}{n}\sum_{i=1}^n P(\bH^i_{K+1}; M_\theta)$, so $\hat\theta_n = \argmax_\theta P_n(\bH_{K+1}; M_{\theta})$. By Theorem 5.7 of \citet{Vaart1998}, $\hat\theta_n\to_p \theta_0$ provided (A1) $L_n(\hat\theta_n)\geq L_n(\theta_0)- o_p(1)$; (A2) $\sup_{\theta:d(\theta,\theta_0)\geq \eps}P(\bH_{K+1}; M_\theta)<P(\bH_{K+1}; M_{\theta_0})$ for all $\eps>0$; (A3) $\sup_{\theta\in\Theta}|P_n(\bH_{K+1}; M_\theta)-P(\bH_{K+1}; M_\theta)|\to_p 0$.  

Assumption (A1) is satisfied by $\hat\theta_n$ being an M-estimator. By (C1), $P(\bH_{K+1})=\int_{\mathcal E}P(\bH_{K+1}|\bE)dP(\bE)=\int_{\mathcal E}M_{\theta_0}(\bH_{K+1},\bE)g(\bH_{K+1})dP(\bE)=P(\bH_{K+1}; M_{\theta_0})$. Thus by (C1) and (C5), we have by Lemma \ref{lemma:modelselect} that $L(\theta)$ is uniquely maximized at $\theta_0$. By (C3), it must be that for any $\theta$, $P(\bH_{K+1}; M_\theta)=\int_{\mathcal E}M_\theta(\bH_{K+1}, \bE)dP(\bE)<\infty$ almost surely over $\bH_{K+1}$, and thus by (C2) and the dominated convergence theorem, $P(\bH_{K+1}; M_\theta)$ is continuous in $\theta$ almost surely. By (C4), $\log P(\bH_{K+1}; M_\theta)$ is well-defined and also continuous in $\theta$ almost surely. Therefore, by compactness of $\Theta$ and (C5), assumption (A2) is satisfied by Problem 5.27 of \citet{Vaart1998}. Finally, by almost-sure continuity of $\log P(\bH_{K+1}; M_\theta)$, (C4), and compactness of $\Theta$, we have by example 19.8 of \citet{Vaart1998} that $\{P(\bH_{K+1}; M_\theta):\theta\in\Theta\}$ defines a $P_{\theta_0}$-Glivenko-Cantelli class. Thus assumption (A3) is satisfied.

By Theorem 5.39 of \citet{Vaart1998}, $\sqrt{n}(\hat\theta_n-\theta_0)\to_d\mathcal \mathcal{N}(0,I(\theta_0)^{-1})$ provided that (B1) $\hat\theta_n\to_p \theta_0$; (B2) $I(\theta_0)$ is non-singular; (B3) $\log P(\bH_{K+1}; M_{\theta})$ is Lipschitz continuous in the neighborhood of $\theta_0$ with some Lipschitz constant $F_3(\bH_{K+1})$ square-integrable; and (B4) $P(\bH_{K+1}; M_{\theta})$ is Hellinger differentiable. 

(B2) is satisfied by assumption and (B1) is satisfied by conditions (C1)-(C5) by the reasoning above. By (N2) and Jensen's inequality, $|P(\bH_{K+1} ;M_{\theta_1})- P(\bH_{K+1}; M_{\theta_2})|\leq \mathbb{E}_\bE[F_2(\bH_{K+1},\bE)]\|\theta_1-\theta_2\|_2$ and by (C4), $\frac{d}{dx} \log(x)$ with $x = M_\theta(\bH_{K+1})$ is upper bounded by $1/c$. As the composition of Lipschitz continuous functions are also Lipschitz continuous with the Lipschitz constant being the product of those of the composing functions \citep{Shwartz2014}, $|\log P(\bH_{K+1}; M_{\theta_1}) - \log P(\bH_{K+1}; M_{\theta_2})|\leq \frac{1}{c}\mathbb{E}_\bE[F_2(\bH_{K+1},\bE)]||\theta_1-\theta_2|$ with $\mathbb{E}_{\theta_0}\frac{1}{c}\mathbb{E}_{E}[F_2(\bH_{K+1}|\bE)]<\infty$. Thus condition (B3) is satisfied. 

By (N3), we have $\mathbb E_{E}[G(\bH_{K+1}, \bE)]<\infty$ almost surely and thus by the Leibniz integral theorem, $\nabla_\theta P(\bH_{K+1}; M_\theta)= g(\bH_{K+1}) \mathbb{E}_\bE [\nabla_\theta M_\theta(\bH_{K+1},\bE)]$, and we have by the dominated convergence theorem that $\nabla_\theta P(\bH_{K+1}; M_\theta)$ is continuous. $\nabla_\theta \sqrt{P(\bH_{K+1}; M_\theta)}=\frac{g(\bH_{K+1})}{2\sqrt{P(\bH_{K+1}; M_\theta)}}\sqrt{ \nabla_\theta M_\theta(\bH_{K+1}, \bE)} = \frac{\sqrt{g(\bH_{K+1})}}{2\sqrt{M_\theta(\bH_{K+1})}}\sqrt{ \nabla_\theta M_\theta(\bH_{K+1}, \bE)}$. By (C4) $P(\bH_{K+1}; M_\theta) > c > 0$ so the quantity is well-defined; then by (N3), $\nabla_\theta M_\theta(\bH_{K+1}, \bE)$ and $M_\theta(\bH_{K+1}, \bE)$ are both continuous, so $\nabla_\theta \sqrt{P(\bH_{K+1}; M_\theta)}$ is also continuous. Finally, under our assumptions $I(\theta)=\mathbb E_{\theta_0}\left[\frac{g(\bH_{K+1})^2}{P(\bH_{K+1}; M_\theta)^2}\mathbb{E}_\bE [\nabla_\theta M_\theta(\bH_{K+1}, \bE)]\mathbb{E}_\bE [\nabla_\theta M_\theta(\bH_{K+1}, \bE)]^T\right]$ with each element of this matrix bounded by $(\frac{C}{c})^2\mathbb{E}_\bE^2[G(\bH_{K+1},\bE)]\leq (\frac{C}{c})^2\mathbb{E}_\bE[G^2(\bH_{K+1},\bE)]$ where $C < \infty$ the upper bound of $g$ and $c > 0$, so $(\frac{C}{c})^2\mathbb E_{\bE,\theta_0}[G^2(\bH_{K+1},\bE)]<\infty$. Thus, using the dominated convergence theorem once more, we have that $I(\theta)=\mathbb E_{\theta_0}\left[\frac{g(\bH_{K+1})^2}{P(\bH_{K+1}; M_\theta)^2}\mathbb{E}_\bE [\nabla_\theta M_\theta(\bH_{K+1},\bE)]\mathbb{E}_\bE [\nabla_\theta M_\theta(\bH_{K+1},\bE)]^T\right]$ is continuous. As $\sqrt{P(\bH_{K+1}; M_\theta)}$ is continuously differentiable and $I(\theta)$ is continuous, we have by Lemma 7.6 of \citet{Vaart1998} that $P(\bH_{K+1}; M_\theta)$ is Hellinger differentiable, satisfying condition (B4). 
\end{proof}


\vspace*{0.5cm}
\noindent
To prove Theorem 4.2 regarding $V(\hat\pi_n)$, we first prove the following lemmas.
\begin{lemma}
\label{lemma:3.1}
    Assume (A1)-(A5), following LUQ-Learning (Algorithm 1), for any $k = 2, \dots, K$,
    \begin{align}
        \left\| \widehat{Q}_{k-1}^{\pi^{opt}}(\bH_{k-1}, \pi^{opt}_{k-1}) - 
    \widehat{\E}[\widehat{V}^{\hat\pi} (\bH_{k+1}) \mid \bH_{k-1}, \hat{\pi}_{k-1}] \right\|_{P_{\theta_0}} \leq 0.
    \end{align}
\end{lemma}
\begin{proof}
    \begin{align*}
LHS = &\Big\| \widehat{\E}[\widehat{V}^{\pi^{opt}_k, \dots, \pi^{opt}_K}(\bH_k) | \bH_{k-1}, \pi^{opt}_{k-1}]
    - \widehat{\E}[\widehat{V}^{\hat\pi_{k}, \dots, \hat\pi_{K}} (\bH_{k}) \mid \bH_{k-1}, \hat{\pi}_{k-1}] \Big\|_{P_{\theta_0}}\\
    = &\Big\| \widehat{\E}[\widehat{V}^{\pi^{opt}_k, \dots, \pi^{opt}_K}(\bH_k) | \bH_{k-1}, \pi^{opt}_{k-1}]
    - \widehat{\E}[\widehat{V}^{\pi^{opt}_k, \dots, \hat\pi_K}(\bH_k) | \bH_{k-1}, \pi^{opt}_{k-1}] \\
      & + \widehat{\E}[\widehat{V}^{\pi^{opt}_k, \dots, \hat\pi_K}(\bH_k) | \bH_{k-1}, \pi^{opt}_{k-1}]
    - \widehat{\E}[\widehat{V}^{\pi^{opt}_k, \dots, \hat\pi_{K-1}, \hat\pi_K}(\bH_k) | \bH_{k-1}, \pi^{opt}_{k-1}] \\
      & + \cdots - \cdots \\
      & + \widehat{\E}[\widehat{V}^{\hat\pi_k, \dots, \hat\pi_K}(\bH_k) | \bH_{k-1}, \pi^{opt}_{k-1}]
    - \widehat{\E}[\widehat{V}^{\hat\pi_{k}, \dots, \hat\pi_K} (\bH_{k}) \mid \bH_{k-1}, \hat{\pi}_{k-1}] \Big\|_{P_{\theta_0}} \\
    = &\Big\| \widehat{\E}_{A_{k-1}, \dots, A_{K-1} \sim \pi^{opt}}\left[\widehat{\E}[\bE^T \bY | \bH_{K}, {\pi}^{opt}_K] - \widehat{\E}[\bE^T \bY | \bH_{k}, \hat{\pi}_K] \mid \bH_{k-1}, A_{k-1} \right] \\
      & + \widehat{\E}_{A_{k-1}, \dots, A_{K-2} \sim \pi^{opt}}\left[\widehat{\E}[\widehat{V}^{\hat{\pi}_K}(\bH_K) | \bH_{K-1}, \hat{\pi}_{K-1}] - \widehat{\E}[\widehat{V}^{\hat{\pi}_K}(\bH_K) | \bH_{K-1}, {\pi}^{opt}_{K-1}] \mid \bH_{k-1}, A_{k-1} \right] \\
    &+ \dots +
    \widehat{\E}[\widehat{V}^{\hat\pi_{k}, \dots, \hat\pi_{K}} (\bH_{k}) \mid \bH_{k-1}, {\pi}^{opt}_{k-1}] 
     - \widehat{\E}[\widehat{V}^{\hat\pi_{k}, \dots, \hat\pi_{K}} (\bH_{k}) \mid \bH_{k-1}, \hat{\pi}_{k-1}] \Big\|_{P_{\theta_0}}
    \leq 0,
\end{align*}
as $\hat\pi_{k-1} = \argmax_{A_{k-1}} \widehat{\E}[\widehat{V}_k^{\hat{\pi}_k, \dots, \hat{\pi}_K}(\bH_k) | \bH_{k-1}, A_{k-1}]$, allowing each pair inside $\|\cdot\|_{P(\theta_0)}$ to be non-positive for almost sure $\bH_{k-1}$ for all $k = 2, \dots, K$.
\end{proof}

\vspace*{0.5cm}
\begin{lemma}
\label{lemma:3.2}
    {For any $k = 2, \dots, K$, $\pi \in \Pi$, the class of deterministic policy}:
If $||\widehat Q^{\pi}_{n,k}(\bH_k,A_k)-Q^{\pi}_k(\bH_k,A_k)||_{P_{\theta_0}}\to 0$, then together with (A3), we have
\begin{align}
    &\left\| \max_{A_k \in \mathcal{A}_{\mathcal{H}_k}}|\widehat Q^{\pi}_{n,k}(\bH_k,A_k)-Q^{\pi}_k(\bH_k,A_k)|\right\|_{P_{\theta_0}}\to 0 \quad \text{ and }\\
    &\left\|\mathbb E\left[ \left| \max\nolimits_{A_k \in \mathcal{A}_{\mathcal{H}_k}}\widehat Q^{\pi}_{n,k}(\bH_k,A_k)-\max\nolimits_{A_k \in \mathcal{A}_{\mathcal{H}_k}}Q^{\pi}_k(\bH_k,A_k) \right| |\bH_{k-1},A_{k-1}\right]\right\|_{P_{\theta_0}}\to 0.
\end{align}
\end{lemma}
\begin{proof}
First observe that 
\begin{eqnarray*}
& &\big|\big|\max_{A_k \in \mathcal{A}_{\mathcal{H}_k}}|\widehat Q^{\pi}_{n,k}(\bH_k,A_k)-Q^{\pi}_k(\bH_k,A_k)|\big|\big|_{P_{\theta_0}}^2 \\
&\leq& |\mathcal A_{\mathcal{H}_k}|\times||\widehat Q^{\pi}_{n,k}(\bH_k,A_k)-Q^{\pi}_k(\bH_k,A_k)||_{P_{\theta_0}(\bH_k),A_k \sim e}^2 \\
&=& |\mathcal A_{\mathcal{H}_k}|\times {\mathbb E_{\bH_k,A_k\sim P_{\theta_0}}\left[\frac{\pi(\bH_k)}{\mu_k(A_k|\bH_k)}\left(\widehat Q^{\pi}_{n,k}(\bH_k, A_k)-Q^{\pi}_k(\bH_k, A_k)\right)^2\right]}\\
&\leq& 1/{c} \times {\mathbb E_{\bH_k, A_k\sim P_{\theta_0}}\left[\left(\widehat Q^{\pi}_{n,k}(\bH_k, A_k)-Q^{\pi}_k(\bH_k, A_k)\right)^2\right]}.
\end{eqnarray*} 
Additionally, 
\begin{eqnarray*}
\lefteqn{\left\|\max_{A_k \in \mathcal{A}_{\mathcal{H}_k}}|\widehat Q^{\pi}_{n,k}(\bH_k,A_k)-Q^{\pi}_k(\bH_k,A_k)| \right\|_{P_{\theta_0}}^2}&&\\
&=&\mathbb E\left[\max\nolimits_{A_k} \left| \widehat Q^{\pi}_{n,k}(\bH_k,A_k)-Q^{\pi}_k(\bH_k,A_k)\right|^2\right]\\
&=& \mathbb E\left[\mathbb E\left(\max\nolimits_{A_k} \left| \widehat Q^{\pi}_{n,k}(\bH_k,A_k)-Q^{\pi}_k(\bH_k,A_k)\right|^2\big |\bH_{k-1},A_{k-1}\right)\right] \\
&\geq& \mathbb E\left[\mathbb E^2\left(\max\nolimits_{A_k}|\widehat Q^{\pi}_{n,k}(\bH_k,A_k)-Q^{\pi}_k(\bH_k,A_k)|\big |\bH_{k-1},A_{k-1}\right)\right] \\
&=& \big|\big|\mathbb E\left(\max\nolimits_{A_k}|\widehat Q^{\pi}_{n,k}(\bH_k,A_k)-Q^{\pi}_k(\bH_k,A_k)|\big |\bH_{k-1},A_{k-1}\right)\big|\big|_{P_{\theta_0}}^2 \\
&\geq& \Big\|\mathbb E\left( \left| \max\nolimits_{A_k}\widehat Q^{\pi}_{n,k}(\bH_k,A_k)-\max\nolimits_{A_k}Q^{\pi}_k(\bH_k,A_k) \right| \big |\bH_{k-1},A_{k-1}\right)\Big\|_{P_{\theta_0}}^2,
\end{eqnarray*}
where the first inequalities by Jensen's inequality and the second by property of $max$ for sequences of real numbers. 
Combining the two inequalities completes the proof.
\end{proof}

\vspace*{0.5cm}
\begin{lemma}
\label{lemma:3.3}
Assume (A1)-(A5). We show utilizing Lemma \ref{lemma:3.2}, that following LUQ-Learning (Algorithm 1), for any $1 \leq k \leq K$ and any $\pi \in \Pi$,
\begin{align}
    \left\| \max_{A_k \in \mathcal{A}_{H_k}} \left| \widehat Q^{\pi}_{n,k}(\bH_k,A_k)-Q^{\pi}_k(\bH_k,A_k) \right| \right\|_{P_{\theta_0}} = o(1).
\end{align}
\end{lemma}
\begin{proof}
    We first show $\left\| \widehat Q^{\pi}_{n,k}(\bH_k,A_k)-Q^{\pi}_k(\bH_k,A_k)\right\|_{P_{\theta_0}} = o(1)$ for any $k$. Then following a similar argument as that used in the first set of inequalities, this Lemma can be proved together with (A3).
{At k = K}:
\begin{align*}
&\left\|\widehat Q_{n,K}(\bH_K,A_K)-Q_K(\bH_K,A_K)\right\|_{P_{\theta_0}}\\
=&\left\|\widehat{\mathbb E}_{\hat{\theta}_n}[\bE|\bH_K]^T\widehat{\mathbb E}[\bY|\bH_K,A_K]-{\mathbb E}[\bE|\bH_K]^T{\mathbb E}[\bY|\bH_K,A_K]\right\|_{P_{\theta_0}}\\
=& \left\|(\widehat{\mathbb E}_{\hat{\theta}_n}[\bE|\bH_K]-\mathbb E[\bE|\bH_K])^T\widehat{\mathbb E}[\bY|\bH_K,A_K] + \mathbb E[\bE|\bH_K]^T(\widehat{\mathbb E}(\bY|\bH_K,A_K)-\mathbb E(\bY|\bH_K,A_K))\right\|_{P_{\theta_0}} \\
\leq& \left\|\widehat{\mathbb E}_{\hat{\theta}_n}[\bE|\bH_K]-\mathbb E[\bE|\bH_K] \right\|_{P_{\theta_0}}^T\left\|\widehat{\mathbb E}[\bY|\bH_K,A_K]\right\|_{L^\infty(P_{\theta_0})} \\
 &+ \Big\|\mathbb E[\bE|\bH_K]\Big\|_{L^\infty(P_{\theta_0})}^T \left\|\widehat{\mathbb E}(\bY|\bH_K,A_K)-\mathbb E(\bY|\bH_K,A_K)\right\|_{P_{\theta_0}},
\end{align*}
where the last inequality uses Minkowski's inequality. Observe that $\mathbb E[\bE|\bH_K]||_{L^\infty(P_{\theta_0})} = \E_{P_{\theta_0}}[\bE]$ which is always upper bounded by definition of $\bE$. By (V1), (V2), and Slutsky's theorem, the upper bound above converges to zero in probability, thus 
$||\widehat Q_{n,K}(\bH_K,A_K)-Q_K(\bH_K,A_K)||_{P_{\theta_0}} \to 0$. \\

We now show by induction that if for any $\|\widehat Q^\pi_{n,k+1}(\bH_{k+1}, A_{k+1})-Q^\pi_{k+1}(\bH_{k+1}, A_{k+1})\|_{P_{\theta_0}} \to 0$, then $\|\widehat Q^\pi_{n,k}(\bH_{k}, A_{k})-Q^\pi_{k}(\bH_{k}, A_{k})\|_{P_{\theta_0}} \to 0$, completing the proof of Lemma 3:
\begin{align*}
&||\widehat Q^{\pi}_{n,k}(\bH_k, A_k) - Q^{\pi}_k (\bH_k, A_k)||_{P_{\theta_0}}\\
=&\left\|\widehat{\mathbb E}[\max_{A_{k+1}}\widehat Q^{\pi}_{n,k+1}(\bH_{k+1},A_{k+1})|\bH_k,A_k]-{\mathbb E}[\max_{A_{k+1}}Q^{\pi}_{2}(\bH_{k+1},A_{k+1})|\bH_k,A_k]\right\|_{P_{\theta_0}}\\
\leq& \left\|\widehat{\mathbb E}[\max_{A_{k+1}}\widehat Q^{\pi}_{n,k+1}(\bH_{k+1},A_{k+1})|\bH_k,A_k]-{\mathbb E}[\max_{A_{k+1}}\widehat Q^{\pi}_{n,k+1}(\bH_{k+1},A_{k+1})|\bH_k,A_k]\right\|_{P_{\theta_0}}\\
&+\left\|{\mathbb E}[\max\nolimits_{A_{k+1}}\widehat Q^{\pi}_{n,k+1}(\bH_{k+1},A_{k+1})|\bH_k,A_k]-\mathbb E[\max\nolimits_{A_{k+1}}Q^{\pi}_{k+1}(\bH_{k+1},A_{k+1})|\bH_k,A_k]\right\|_{P_{\theta_0}}\\
\leq& \left\|\widehat{\mathbb E}[\widehat{V}^{\pi}_{n,k+1}(\bH_{k+1})|\bH_k,A_k]-{\mathbb E}[\widehat V^{\pi}_{n,k+1}(\bH_{k+1})|\bH_k,A_k]\right\|_{P_{\theta_0}} \\
&+\left\|{\mathbb E}[ \left| \max\nolimits_{A_{k+1}}\widehat Q^{\pi}_{n,k+1}(\bH_{k+1},A_{k+1}) - \max\nolimits_{A_{k+1}}Q^{\pi}_{k+1}(\bH_{k+1},A_{k+1}) \right| |\bH_k,A_k]\right\|_{P_{\theta_0}}.
\end{align*}
Therefore, $\|\widehat Q^{\pi}_{n,k+1}(\bH_{k+1}, A_{k+1})-Q^{\pi}_{k+1}(\bH_{k+1}, A_{k+1})\|_{P_{\theta_0}} \to 0$ with Lemma 2 implies the second term in the upper bound is $o(1)$. Together with (V3), it follows from Slutsky's theorem that $||\widehat Q^{\pi}_{n,k}(\bH_k, A_k) -Q^{\pi}_k(\bH_k, A_k) ||_{P_{\theta_0}} = o(1)$.
\end{proof}


\vspace*{1cm}
\noindent Combining the previous three Lemmas proves Theorem 4.2.
\textbf{Proof of Theorem 4.2:}
\begin{proof}
For $k = K$,
\begin{align*}
      \|V_{K}(\pi^{opt}) - V_K(\hat{\pi}_n)\|_{P_{\theta_0}}
    &= \left\| \max_{a_K} Q_K(\bH_K, a_K) - \mathbb{E}[\bE^T \bY | \bH_K, \hat{\pi}_{n,K}(\bH_K)] \right\|_{P_{\theta_0}}\\
    &\leq \left\| Q_K(\bH_K, \pi^{opt}_K) - \widehat{\mathbb{E}}[\bE^T \bY | \bH_K, {\pi}^{opt}_{K}(\bH_K)] \right\|_{P_{\theta_0}} \\
    &\quad + \left\| \widehat{\mathbb{E}}[\bE^T \bY | \bH_K, {\pi}^{opt}_{K}(\bH_K)] - \widehat{\mathbb{E}}[\bE^T \bY | \bH_K, \hat{\pi}_{n,K}(\bH_K)] \right\|_{P_{\theta_0}}  \\
    &\quad + \left\| \widehat{\mathbb{E}}[\bE^T \bY | \bH_K, \hat{\pi}_{n,K}(\bH_K)] - \mathbb{E}[\bE^T \bY | \bH_K, \hat{\pi}_{n,K}(\bH_K)] \right\|_{P_{\theta_0}} = o(1),
\end{align*}
as the first and third term are $o(1)$ following from Lemma 3, with the second term also $o(1)$ by definition of $\hat\pi_K$.\\

\noindent
For any $1 \leq k \leq K-1$,
\begin{align*}
    &\|V_{k}(\pi^{opt}) - V_k(\hat{\pi}_n)\|_{P_{\theta_0}} \\
    = &\left\| \max_{a_k}Q^{\pi^{opt}}_k(\bH_k, a_k) - \E[V_{k+1}^{\hat{\pi}_n}(\bH_{k+1}) | \bH_k, \hat{\pi}_{n,k}(\bH_k)]\right\|_{P_{\theta_0}}\\
    = &\Bigg\| 
        \max_{a_k}Q^{\pi^{opt}}_k(\bH_k, a_k) - \widehat{Q}^{\pi^{opt}}_k(\bH_k, \pi^{opt}_k)
        + \widehat{Q}^{\pi^{opt}}_k(\bH_k, \pi^{opt}_k) - \widehat{\E}[\widehat{V}^{\hat\pi}(\bH_{k+1}) \mid  \bH_k, \hat\pi_k(\bH_k)] \\
        & + \widehat{\E}[\widehat{V}^{\hat\pi}(\bH_{k+1}) \mid  \bH_k, \hat\pi_k(\bH_k)] - {\E}[\widehat{V}^{\hat\pi}(\bH_{k+1}) \mid  \bH_k, \hat\pi_k(\bH_k)] \\
        & + {\E}[\widehat{V}^{\hat\pi}(\bH_{k+1}) \mid  \bH_k, \hat\pi_k(\bH_k)] - {\E}[{V}^{\hat\pi}(\bH_{k+1}) \mid  \bH_k, \hat\pi_k(\bH_k)]
        \Bigg\|_{P_{\theta_0}} \\
    \leq &\Big\| 
        Q^{\pi^{opt}}_k(\bH_k, \pi^{opt}_k) - \widehat{Q}^{\pi^{opt}}_k(\bH_k, \pi^{opt}_k)\Big\|_{P_{\theta_0}} 
          + \Big\| \widehat{Q}^{\pi^{opt}}_k(\bH_k, \pi^{opt}_k) - \widehat{\E}[\widehat{V}^{\hat\pi}(\bH_{k+1}) \mid  \bH_k, \hat\pi_k(\bH_k)] \Big\|_{P_{\theta_0}} \\
        & + \Big\| \widehat{\E}[\widehat{V}^{\hat\pi}(\bH_{k+1}) \mid  \bH_k, \hat\pi_k(\bH_k)] - {\E}[\widehat{V}^{\hat\pi}(\bH_{k+1}) \mid  \bH_k, \hat\pi_k(\bH_k)]\Big\|_{P_{\theta_0}} \\
        &  + \Big\| {\E}[\widehat{V}^{\hat\pi}(\bH_{k+1}) \mid  \bH_k, \hat\pi_k(\bH_k)] - {\E}[{V}^{\hat\pi}(\bH_{k+1}) \mid  \bH_k, \hat\pi_k(\bH_k)] \Big\|_{P_{\theta_0}}\\
    \leq & \quad o(1) 
        + \Big\| \max_{A_k \in \mathcal{A}_{{H}_k}} \left| \widehat{\E}[\widehat{V}^{\hat\pi}(\bH_{k+1}) \mid  \bH_k, A_k] - {\E}[\widehat{V}^{\hat\pi}(\bH_{k+1}) \mid  \bH_k, A_k] \right| \Big\|_{P_{\theta_0}} \\
        & + \Big\| \max_{A_k \in \mathcal{A}_{{H}_k}} \left| 
                \E\left[ \max_{A_{k+1}}\left| \widehat{Q}^{\hat\pi}(\bH_{k+1}, A_{k+1}) - Q^{\hat\pi}(\bH_{k+1}, A_{k+1}) \right| \mid \bH_k, A_k \right]
            \right| \Big\|_{P_{\theta_0}} \to 0,
\end{align*}
where the first inequality follows by Minkowski's inequality, and the second follows by Lemma \ref{lemma:3.3} with $\pi = \pi^{opt}$ and Lemma \ref{lemma:3.1}, completing the proof.
\end{proof}


\vspace*{1cm}
\noindent
The next theorem shows that under the proposed model for the BEST study, $\hat\theta_n$ is consistent and asymptotically normal under regularity conditions, and that $\|\hat{\mathbb{E}}[\mathbf{E}|\mathbf{H}_2; \hat\theta_n] - {\mathbb{E}}[\mathbf{E}|\mathbf{H}_2; \theta_0]\|_{P(\theta_0)} \to 0$, providing justification for $V(\hat\pi_n) - V(\pi^{opt}) \to_p 0$ when combined with mild conditions as mentioned at the end of section 5.1. \\
\textbf{Proof of Theorem 5.1:}
\begin{proof}
To show that $\hat\theta_n \to_{p}\theta_0$, by Theorem 4.1, we require 
(C1) $P(\bH_{K+1}|\bE)=M_{\theta_0}(\bH_{K+1},\bE)g(\bH_{K+1})$ for some interior point $\theta_0\in\Theta$ compact and $g$ bounded from above; (C2) $M_\theta(\bH_{K+1},\bE)$ continuous in $\theta$; 
(C3) $\forall \theta$, $|M_\theta(\bH_{3}, \bE)|\leq F_1(\bH_{3}, \bE)$ for some $F_1$ integrable; 
(C4) $\exists c>0$ such that the measure induced by model $M_{\theta}$, $P(\bH_{3}; M_{\theta}) > c$ a.s. in $\bH_{3}$; 
(C5) $P(\bH_{3}; M_{\theta_0})\neq P(\bH_{3};M_\theta)$ for all $\theta\neq\theta_0$.

Identifiability assumptions (C1) and (C5) are assumed, and $\theta = (\alpha, \beta, \lambda) \in \Bar{\mathbb{R}}^d$ which is closed and bounded and thus we have $\Theta$ compact. 
Also, the proposed model $P(\bW^B_1|\bV,\beta)$, $P(\bW_1^{R}|\bV,\lambda)$, $P(\mathbf{W}^{Sat}_2|\bE^T\bX_2,\alpha)$, $P(\bW^B_2|\bV,\beta)$, $P(\bW_2^{R}|\bV,\lambda)$ and $P(\mathbf{W}^{Sat}_3|\bE^T\bY,\alpha)$ detailed in section 5.1 are all continuous w.r.t. the parameters. Let $M_\theta(\bH_{3},\bE)$ be the product of these terms and note that $P(\bH_{3}; M_\theta)= \mathbb{E}_\bE[M_\theta(\bH_{3},\bE)]g(\bH_{3})$ and $P_\theta(\bH_{3}|\bE)=M_\theta(\bH_{3},\bE)g(\bH_{3})$ As the product of continuous functions is continuous, (C2) is satisfied. Moreover, $(\bW^B_k,\bW^R_k,\bW^{Sat}_{t+1})_{1\leq  t \leq 2}$ all categorical implying $M_\theta(\bH_{3},\bE) \leq 1$ pointwise, so (C3) is satisfied. 

It remains to show (C4). We have assumed $\exists C < \infty$ such that $||\theta||_{L^\infty(P_{\theta_0})}\leq C$ and some small $\eps>0$ such that $\alpha_{k,\cdot,1},\lambda_k,\alpha_{k,j+1,0}-\alpha_{k,j,0}\geq\eps$, for all $\theta \in \Theta$. Then it can be seen that for all $\mathbf{V} \in\mathbb{R}^2$, $\min_{\theta,\bH_{3}\in\Theta\times\mathcal H_{3}}P(W^{B}_{k,j}|\bV,\theta) \geq \min\{\sigma(-C-C\sum_{j=1,2}V_j), 1-\sigma(C+C\sum_{j=1,2}V_j)\} = 1 - \sigma[C(\sum_{j=1,2}V_j + 1)], \forall (t,k)\in\{1,2\}\times\{1,\dots,12\}$; $\min_{\theta,\bH_{3}\in\Theta\times\mathcal H_{3}}P(\bW_t^R|\bE^R,\theta)\geq \exp(-3C)/6 \stackrel{\text{denote}}{=} C_1 >0$; and $\min_{\theta,\bH_{3}\in\Theta\times\mathcal H_{3}}P(\bW^{Sat}_{k}|\bE^T\bX_{k})\geq \min\{1-\sigma(C),\sigma(\eps-10 C),\sigma(C)-\sigma(C-\eps)\} \stackrel{\text{denote}}{=} C_2>0$ for $k = 2,3$. Combined with the assumption that $\min_{\bH_{3}\in\mathcal H_{3}} g(\bH_{3}) > c > 0$, we have that
$P(\bH_3;M_\theta) 
\geq c K^2_1 K^2_2 \int_{\mathbb{R}^2}\left[1 - \sigma(C(\sum_{j=1,2}V_j+1))\right]^{24}dP(\bV) 
\geq c K^2_1 K^2_2 \{ 1 - \int_{\mathbb{R}} \sigma\left[C(Z + 1)\right] dP(Z)\}^{24} 
> 0$, 
where $Z \sim \mathcal{N}(0, 2)$ as $\bV \sim N_2(0, \mathbf{I})$; the second inequality follows from Jensen's inequality and the last inequality follows from $\sigma(x) < 1$ for any finite $x$, $C<\infty$, and $Z$ continuous so its value equaling infinity is of measure zero. Thus (C4) is also satisfied. 


We now show $||\widehat{\mathbb E}[\bE|\bH_2;\hat{\theta}_n]-\mathbb E[\bE|\bH_2;\theta_0]||_{P_{\theta_0}} \to 0$. 
Let $\bH_{3,D}=(\bW^B_1,\bW^B_2,\bW_1^R,\bW_2^R,\\\bW_2^{Sat},\bW_3^{Sat})$ be the components of $\bH_{K+1}$ dependent on $\bE$ and $\bH_{3,I}=(\bX_1,\bX_2,A_1,A_2,\bY)$ be the components conditionally independent of $\bE$. Note that $\bH_{3}=\bH_{3,D}\cup \bH_{3,I}$, and $M_\theta(\bH_{K+1},\bE)$ is a function of only $\bE$ and $\bH_{3,D}$. 
At any $\theta \in \Theta$, $\widehat\E[\bE|\bH_2;\theta] = \frac{1/N_{sim}\sum_{b=1} \bE^{(b)} P(\bH_2 | \bE^{(b)}; \theta)}{1/N_{sim}\sum_{b=1} P(\bH_2 | \bE^{(b)}; \theta)}$. Applying the strong law of large numbers for both the numerator and denominator and the continuous mapping theorem gives $\widehat\E[\bE|\bH_2;\theta] \to_{a.s.} \E[\bE|\bH_2;\theta]$ as $N_{sim} \to \infty$. Combined with the assumption that $\|\E[\bE | \bH_2; \theta]\|_{L^\infty(P_{\theta_0})} < \infty$, we have that 
$\| \widehat\E[\bE|\bH_2;\theta] - \E[\bE|\bH_2;\theta]\|_{P_{\theta_0}} \to 0$.
Also, $\mathbb E_{\theta}[\bE|\bH_2]=\frac{\int_{\mathcal E}\bE P_{\theta}(\bH_2|\bE)dP(\bE)}{\int_{\mathcal E}P_{\theta}(\bH_2|\bE)dP(\bE)}=\frac{\int_{\mathcal E}\bE M_{\theta}(\bH_2,\bE)dP(\bE)}{\int_{\mathcal E}M_{\theta}(\bH_2, \bE)dP(\bE)}$, $M_{\theta}(\bH_2, \bE)=\sum_{\bH_{3,D}\in\mathcal H_{3,D}(\bH_{2,D})}M_\theta(\bH_{3}, \bE)$, $\mathcal H_{3,D}(\bh_{2,D})$ is the set of $\bH_{3,D}$ where $\bH_{2,D}=\bh_{2,D}$ and $\bH_{2,D}=(\bW^B_1,\bW^B_2,\bW_1^R,\bW_2^R,\bW^{Sat}_2)$. This is a finite sum, and each element $M_\theta(\bH_{3},\bE)$ of this sum is continuous in $\theta$. Therefore, $M_{\theta}(\bH_2, \bE)$ is continuous in $\theta$. As $M_\theta(\bH_2, \bE) < 1$, we have that both the numerator and the denominator are continuous in $\theta$ by the dominated convergence theorem, and that $\E[M_\theta(\bH_2, \bE)] > 0$ for any $\theta$ a.s. in $\bH_2$. Thus $\E_\theta[\bE|\bH_2]$ continuous in $\theta$, and by the continuous mapping theorem, $\mathbb E_{\hat\theta_n}[\bE|\bH_2] \to_p \mathbb E_{\theta}[\bE|\bH_2]$ as $n \to \infty$. Note that $||\widehat{\mathbb E}[\bE|\bH_2]-\mathbb E[\bE|\bH_2]||_{P_{\theta_0}}^2=\sum_{\bH_2\in\mathcal H_{2,D}}\left[\left(\mathbb E_{\hat\theta_n}[\bE|\bH_{2,D}]-\mathbb E[\bE|\bH_{2,D}]\right)^2\right] P(\bH_{2,D})$ is a finite sum. Thus $\|\E[\bE|\bH_2;\hat\theta_n] - \E[\bE|\bH_2;\theta_0]\|_{P_{\theta_0}} = o(1)$, implying 
$\|\widehat{\mathbb E}[\bE|\bH_2;\hat{\theta}_n]-\mathbb E[\bE|\bH_2;\theta_0]\|_{P_{\theta_0}} 
\leq \|\widehat{\mathbb E}[\bE|\bH_2;\hat{\theta}_n]-\mathbb{E}[\bE|\bH_2; \hat{\theta}_n]\|_{P_{\theta_0}} 
+ \|{\mathbb E}[\bE|\bH_2;\hat{\theta}_n]-\mathbb E[\bE|\bH_2;\theta_0]\|_{P_{\theta_0}} 
\to 0$ as $N_{sim}, n \to \infty$. 

To show that $\sqrt{n}(\hat\theta_n - \theta_0)\to_d \mathcal N (0,I(\theta_0)^{-1})$, by Theorem 4.1, as we have shown $\hat\theta_n = \theta + o_p(1)$, it remains to have: 
(N1) $I(\theta_0)$ non-singular; 
(N2) $\forall \theta_1,\theta_2\in \mathcal{N}_\eps(\theta_0)=\{\theta \in \Theta:||\theta-\theta_0||_2<\eps\}$, with any $\eps > 0$, $|M_{\theta_1}(\bH_{3}, \bE)-M_{\theta_2}(\bH_{3}, \bE)|\leq F_2(\bH_{3},\bE)\|\theta_1-\theta_2\|_2$ for some measurable function $F_2$ satisfying $\mathbb E_{\theta_0,\bE}[F_{2}^2(\bH_{3},\bE)]<\infty$ a.s. in $\bH_{3}$;
(N3) $M_{\theta}(\bH_{3} , \bE)$ is continuously differentiable in $\theta$ for a.s. $\bE$ with $||\nabla_\theta M_{\theta_1}(\bH_{3},\bE)||_{L^\infty(P_{\theta_0})} < G(\bH_{3},\bE)$ for some measureable function $G$ satisfying $\mathbb E_{\theta_0,\bE}[G^2(\bH_{3},\bE)]<\infty$ a.s. in $\bH_{3}$.

(N1) is satisfied by our assumption. To prove the remaining conditions, we need to derive the gradient of the log-likelihood. We can derive the relevant quantities in closed form on the basis of our proposed data generating process. Specifically:
\small
\begin{align*}
P(W^B_{1,j}|\bV,\beta)=&\sigma(\beta_{1,j,0}+\beta_{1,j,1}^T\bV)^{W^B_{1,j}}(1-\sigma(\beta_{1,j,0}+\beta_{1,j,1}^T\bV))^{1-W^B_{1,j}},\\
\nabla_{\beta_{1,j,0}}P(W^B_{1,j}|\bV,\beta)=&P(W_{1,j}^B | \mathbf{V}, \beta) (W_{1,j}^B - \sigma(\beta_{1,j,0} + \beta_{1,j,1}^T \mathbf{V})),\\
\nabla_{\beta_{1,j,1}} P(W_{1,j}^B | \mathbf{V}, \beta) =
&P(W_{1,j}^B | \mathbf{V}, \beta) (W_{1,j}^B - \sigma(\beta_{1,j,0} + \beta_{1,j,1}^T \mathbf{V})) \mathbf{V}.\\
\nabla_{\lambda_1}P(\bW_1^R|\bV,\lambda)=& \frac{\left[\sum_{\bv\in\textit{Perm}}\exp(-\lambda_1 T(\bv,\bE^R))(T(\bv,\bE^R)-T(\bW_1^R,\bE^R)\right]}{\left(\sum_{\bv\in\textit{Perm}}\exp(-\lambda_1 T(\bv,\bE^R))\right)^2}\exp(-\lambda_1 T(\bW_1^R,\bE^R)), \\
P(\bW^{Sat}_2 | \bE^T\bX_2, \alpha)=&\sigma(\alpha_{2,\bW^{Sat}_2,0}-\alpha_{2,\cdot,1}\bE^T\bX_2)^{1-I(\bW^{Sat}_2=7)}\\
&-I(\bW^{Sat}_2\neq 1)\sigma(\alpha_{2,\bW^{Sat}_2-1,0}-\alpha_{2,\cdot,1}\bE^T\bX_2), \\
\nabla_{\alpha_{2,j,0}}P(\bW^{Sat}_2 | \bE^T\bX_2, \alpha)=& I(\bW^{Sat}_2=j)\sigma'(\alpha_{2,j,0}-\alpha_{2,\cdot,1}\bE^T\bX_2)-I(\bW^{Sat}_2= j-1 )\sigma'(\alpha_{2,j,0}-\alpha_{2,\cdot,1}\bE^T\bX_2)\\
=& \left[I(\bW^{Sat}_2 = j)- I(\bW^{Sat}_2 = j-1)\right]\sigma'(\alpha_{2,j,0}-\alpha_{2,\cdot,1}\bE^T\bX_2), \\
\nabla_{\alpha_{2,\cdot,1}}P(\bW^{Sat}_2 | \bE^T\bX_2, \alpha)
=& -\Big[I(\bW^{Sat}_2\neq 7)\sigma'(\alpha_{2,\bW^{Sat}_2,0}-\alpha_{2,\cdot,1}\bE^T\bX_2) \\
&+I(\bW^{Sat}_2\neq 1)\sigma'(\alpha_{2,\bW^{Sat}_2-1,0}-\alpha_{2,\cdot,1}\bE^T\bX_2)\Big](\bE^T\bX_2).
\end{align*}
\normalsize
Similarly,
\small
\begin{align*}
\nabla_{\beta_{2,j,0}}P(W^{B}_{2,j}|\bV,\beta)=&P(W_{2,j}^B | \mathbf{V}, \beta) (W_{2,j}^B - \sigma(\beta_{2,j,0} + \beta_{2,j,1}^T \mathbf{V})), \\
\nabla_{\beta_{2,j,1}}P(W^B_{2,j}|\bV,\beta)=&P(W_{2,j}^B | \mathbf{V}, \beta) (W_{2,j}^B - \sigma(\beta_{2,j,0} + \beta_{2,j,1}^T \mathbf{V})) \mathbf{V}, \\
\nabla_{\lambda_2}P(\bW_{2}^R|\bV,\lambda)=& \frac{\left[\sum_{\bv\in\textit{Perm}}\exp(-\lambda_2 T(\bv,\bE^R))(T(\bv,\bE^R)-T(\bW_2^R,\bE^R)\right]}{\left(\sum_{\bv\in\textit{Perm}}\exp(-\lambda_2 T(\bv,\bE^R))\right)^2}\exp(-\lambda_2 T(\bW_2^R,\bE^R)), \\
\nabla_{\alpha_{3,j,0}}P(\bW^{Sat}_3 | \bE^T\bY, \alpha)=& \left[I(\bW^{Sat}_3 = j)- I(\bW^{Sat}_3 = j-1)\right]\sigma'(\alpha_{3,j,0}-\alpha_{3,\cdot,1}\bE^T\bY),\\
\nabla_{\alpha_{3,\cdot,1}}P(\bW^{Sat}_3 | \bE^T\bY, \alpha)=&  - \Big[ I(\bW^{Sat}_3 \neq 7)\sigma'(\alpha_{3,\bW^{Sat}_3,0}-\alpha_{3,\cdot,1}\bE^T\bY)\\
&+I(\bW^{Sat}_3\neq 1)\sigma'(\alpha_{3,\bW^{Sat}_3-1,0}-\alpha_{3,\cdot,1}\bE^T\bY)\Big](\bE^T\bY). 
\end{align*}
\normalsize

\noindent
Denote $\beta_{k,j,\cdot}=(\beta_{k,j,0},\beta_{k,j,1}^T)^T$, $\bV^*=(1,\bV^T)^T$, and $\bW_k^{B(-j)}=(\bW^B_{k,1},\dots,\bW^B_{k,j-1},\bW^B_{k,j+1},\\\dots,\bW^B_{k,12})^T$. We have:
\small
\begin{align*}
M_\theta(\bH_{3}, \bV)=& P_\theta(\bW^B_1|\bV)P_\theta(\bW^B_2|\bV)P_\theta(\bW_1^R|\bV)P(\bW_2^R|\bV)P_\theta(\bW^{Sat}_2|\bE^T\bX_2)P_\theta(\bW^{Sat}_3|\bE^T\bY), \text{ so}
\end{align*}
{For any $1\leq j \leq 12, \; 1\leq k \leq 2$}, 
\begin{align*}
\nabla_{\beta_{k,j,\cdot}}M_\theta(\bH_{3}, \bV)=& P_\theta(\bW_1^{R}|\bV)P_\theta(\bW^{Sat}_2|\bE^T\bX_2)P_\theta(\bW_2^{R}|\bV)P_\theta(\bW^{Sat}_3|\bE^T\bY)\\ &\times P_\theta(\bW^B_{3-k}|\bV)P_\theta(\bW_k^{B,(-j)}|\bV)P_\theta(W_{k,j}^B | \mathbf{V}) (W_{k,j}^B - \sigma(\beta_{1,j,\cdot}^T \mathbf{V}^*)) \mathbf{V}^*, \\
\nabla_{\lambda_k}M_\theta(\bH_{3}, \bV)=& P_\theta(\bW^B_1|\bV)P_\theta(\bW^{Sat}_2|\bE^T\bX_2)P_\theta(\bW^B_2|\bV)P_\theta(\bW^{Sat}_3|\bE^T\bY)P_\theta(\bW_{3-k}^{R}|\bV)\\ &\times\exp(-\lambda_k T(\bW_k^R,\bE^R))\frac{\left[\sum_{\bv\in\textit{Perm}}\exp(-\lambda_k T(\bv,\bE^R))(T(\bv,\bE^R)-T(\bW_k^R,\bE^R)\right]}{\left(\sum_{\bv\in\textit{Perm}}\exp(-\lambda_k T(\bv,\bE^R))\right)^2}.
\end{align*}
{And for any $1 \leq j \leq 6, \;  2 \leq k \leq 3$}, 
\begin{align*}
\nabla_{\alpha_{k,j,0}}M_\theta(\bH_{3}, \bV)=& P_\theta(\bW^B_1|\bV)P_\theta(\bW_1^{R}|\bV)P_\theta(\bW^B_2|\bV)P_\theta(\bW_2^{R}|\bV)\\
&\times P_\theta(\bW^{Sat}_{k}|\bE^T\bX_2)^{I(k=2)}P_\theta(\bW^{Sat}_{k}|\bE^T\bY)^{I(k=3)}
\left(I(\bW^{Sat}_{k} = j)- I(\bW^{Sat}_{k} = j-1)\right) \\
&\times \{\sigma'(\alpha_{k,j,0}-\alpha_{k,\cdot,1}\bE^T\bX_{k})\}^{I(k=2)} \{\sigma'(\alpha_{k,j,0}-\alpha_{k,\cdot,1}\bE^T\bY)\}^{I(k=3)},  \\
\nabla_{\alpha_{k,\cdot,1}}M_\theta(\bH_{3}, \bV)=
&P_\theta(\bW^B_1|\bV)P_\theta(\bW_1^{R}|\bV)P_\theta(\bW^B_2|\bV)P_\theta(\bW_2^{R}|\bV)\\
&\times P_\theta(\bW^{Sat}_{k}|\bE^T\bX_2)^{I(k=2)}P_\theta(\bW^{Sat}_{k}|\bE^T\bY)^{I(k=3)}\\
&\times \Big[- I(\bW^{Sat}_{k} \neq 7)\sigma'(\alpha_{k,\bW^{Sat}_{k},0}-\alpha_{k,\cdot,1}(\bE^T\bX_2)^{I(k=2)}(\bE^T\bY)^{I(k=3)})\\
&+ I(\bW^{Sat}_{k}\neq 1)\sigma'(\alpha_{k,\bW^{Sat}_{k}-1,0}-\alpha_{k,\cdot,1}(\bE^T\bX_2)^{I(k=2)}(\bE^T\bY)^{I(k=3)})\Big]\\
&\times (\bE^T\bX_2)^{I(k=2)}(\bE^T\bY)^{I(k=3)}.
\end{align*}
\normalsize

We can see that $\nabla_\theta M_\theta(\bH_{3}, \bV)$ is continuous in $\theta$ and so is $\nabla_\theta P_\theta(\bH_{3}|\bV)=g(\bH_{3})\nabla_\theta M_\theta(\bH_{3}, \bV)$. We now derive an upper bound for $||\nabla_\theta M_\theta(\bH_{3}, \bV)||_{L^\infty(P_{\theta_0})}$.  As $|P(\bW_k^R | \bV ;\theta)| \leq 1$,
$P(\bW^{Sat}_{2}| \bE^T\bX_2 ;\theta)| \leq 1$, $P(\bW^{Sat}_{3}| \bE^T\bY ;\theta)| \leq 1$ and $|P(\bW^{B}_k ; \bV ;\theta)|\leq 1$, $\forall \theta\in\Theta$, $k = 1,2$, and $|\sigma'(x)|\leq 1$, $\forall x\in\mathbb{R}$, we have that $|\nabla_{\beta_{k,j,\cdot}} M_\theta(\bH_{3}, \bV)|\leq \max_j |\bV^*_j|$. Further, $|\exp(-\lambda_k T(\bW_k^R,\bE^R)|<1$ and $|T(\bv,\bE^R)|$, and $|T(\bW_k^R,\bE^R)|\leq 3$ for $\bv\in\textit{Perm}$, $\exp(-\lambda_k T(\bv,\bE^R))=1$ for some $\bv \in \textit{Perm}$ and $|\textit{Perm}|=6$,  we have that $|\nabla_{\lambda_k} M_\theta(\bH_{3}, \bV)|\leq 18$. As $|\bE^T\bX|\leq10$, $|\nabla_{\alpha_{k,j,0}} M_\theta(\bH_{3}, \bV)|\leq 1$ and $|\nabla_{\alpha_{k,\cdot,1}} M_\theta(\bH_{3}, \bV)|\leq 20$, we 
then have that $||\nabla_\theta M_\theta(\bH_{3}, \bV)||_{L^\infty(P_{\theta_0})} \leq \max\{\max_{j}|\bV^*_j|, 20\} = \max\{Z, 20\}$, with $Z \sim \mathcal{N}(0,1)$, so $\E_{Z}[\max\{Z^2, 400\}] < \infty$ and (N3) is satisfied.

It remains to show (N2). By the mean value theorem, an everywhere-differentiable function $f:\mathcal X\to\mathbb{R}$ with bounded first derivatives will be Lipschitz continuous over $\mathcal X$  with Lipschitz constant $L$ upper-bounded as $\sup_{x\in\mathcal X} |f'(x)|$ \citep{Shwartz2014}. In the following, we use superscript $(1)$ and $(2)$ to denote two parameters in the neighborhood of $\theta_0$. First, as $P(W^B_{k,j}|\bV,\theta)=W^B_{k,j}\sigma(\beta_{k,j,0}+\beta_{k,j,1}^{T}\bV)+(1-W^B_{k,j})(1-\sigma(\beta_{k,j,0}+\beta_{k,j,1}^{T}\bV))$, we have $\forall 1 \leq k \leq 2, \;\; 1 \leq j \leq 12$:
\small
\begin{align*}
&|P(W^B_{k,j}|\bV,\theta^{(1)})-P(W^B_{k,j}|\bV,\theta^{(2)})|\\
\leq& W^B_{k,j}\left|\sigma(\beta_{k,j,\cdot}^{(1)T}\bV^*)-\sigma(\beta_{k,j,\cdot}^{(2)T}\bV^*)\right| +(1-W^B_{k,j})\left|\sigma(\beta_{k,j,\cdot}^{(2)T}\bV^*)-\sigma(\beta_{k,j,\cdot}^{(1)T}\bV^*)\right|\\
\leq& W^B_{k,j}|\beta_{k,j,\cdot}^{(1)T}\bV^*-\beta_{k,j,\cdot}^{(2)T}\bV^*|+(1-W^B_{k,j})|\beta_{k,j,\cdot}^{(2)T}\bV^*-\beta_{k,j,\cdot}^{(1)T}\bV^*| \\
=& |(\beta_{k,j,\cdot}^{(2)}-\beta_{k,j,\cdot}^{(1)})^T\bV^*|\\
\leq& ||\bV^*||_2||\beta_{k,j,\cdot}^{(2)}-\beta_{k,j,\cdot}^{(1)}||_2 \leq ||\bV^*||_2 ||\theta^{(2)}-\theta^{(1)}||_2.
\end{align*}
\normalsize
The first inequality follows from the triangle inequality; The second follows from the fact that the sigmoid function is everywhere-differentiable and $|\sigma'(x)|\leq 1,\forall x\in\mathbb{R}$, making it Lipschitz with constant $L=1$; And the third inequality follows from the Cauchy-Schwartz inequality. Moreover, $\forall 2 \leq k \leq 3, \;\; 1 \leq j \leq 6$:
\small
\begin{align*}
P(\bW^{Sat}_{k}|\bE^T\bX_{k},\theta)
=&I(\bW^{Sat}_{k}=7)+I(\bW^{Sat}_{k}\neq 7)\sigma(\alpha_{k,\bW^{Sat}_{k},0}-\alpha_{k,\cdot,1}\bE^T\bX_{k})\\
&-I(\bW^{Sat}_{k}\neq 1)\sigma(\alpha_{k,\bW^{Sat}_{k}-1,0}-\alpha_{k,\cdot,1}\bE^T\bX_{k}), \;\text{so} \\
|P(\bW^{Sat}_{k}|\bE^T\bX_{k},\theta^{(1)})&-P(\bW^{Sat}_{k}|\bE^T\bX_{k},\theta^{(2)})|\\
\leq& I(\bW^{Sat}_{k}\neq 7)\left|\sigma(\alpha_{k,\bW^{Sat}_{k},0}^{(1)}-\alpha_{k,\cdot,1}^{(1)}\bE^T\bX_{k})-\sigma(\alpha_{k,\bW^{Sat}_{k},0}^{(2)}-\alpha_{k,\cdot,1}^{(2)}\bE^T\bX_{k})\right|\\&+I(\bW^{Sat}_{k}\neq 1)\left|\sigma(\alpha_{k,\bW^{Sat}_{k}-1,0}^{(1)}-\alpha_{k,\cdot,1}^{(1)}\bE^T\bX_{k})-\sigma(\alpha_{k,\bW^{Sat}_{k}-1,0}^{(2)}-\alpha_{k,\cdot,1}^{(2)}\bE^T\bX_{k})\right| \\
\leq&|(\alpha_{k,\bW^{Sat}_{k},0}^{(1)}-\alpha_{k,\bW^{Sat}_{k},0}^{(2)})+(\alpha_{k,\cdot,1}^{(2)}-\alpha_{k,\cdot,1}^{(1)})\bE^T\bX_{k}|\\&+|(\alpha_{k,\bW^{Sat}_{k}-1,0}^{(1)}-\alpha_{k,\bW^{Sat}_{k}-1,0}^{(2)})+(\alpha_{k,\cdot,1}^{(2)}-\alpha_{k,\cdot,1}^{(1)})\bE^T\bX_{k}| \\
\leq& |\alpha_{k,\bW^{Sat}_{k},0}^{(1)}-\alpha_{k,\bW^{Sat}_{k},0}^{(2)}|+|\alpha_{k,\cdot,1}^{(1)}-\alpha_{k,\cdot,1}^{(2)}|\bE^T\bX_{k}\\
&+|\alpha_{k,\bW^{Sat}_{k}-1,0}^{(1)}-\alpha_{k,\bW^{Sat}_{k}-1,0}^{(2)}|+|\alpha_{k,\cdot,1}^{(1)}-\alpha_{k,\cdot,1}^{(2)}|\bE^T\bX_{k} \\
\leq& 2||\alpha_{k,\cdot,0}^{(1)}-\alpha_{k,\cdot,0}^{(2)}||_2+20|\alpha_{k,\cdot,1}^{(1)}-\alpha_{k,\cdot,1}^{(2)}|\\
\leq& 22\left\|\theta^{(2)}-\theta^{(1)}\right\|_2.
\end{align*}
\normalsize
The first inequality uses the triangle inequality for absolute values. The second inequality uses the fact that the sigmoid function has Lipschitz constant $L=1$. The third inequality uses the triangle inequality again. The fourth inequality uses $|\bE^T\bX_{k}|\leq 10, |\bE^T\bY|\leq 10$. 

Note that $f:[0,\infty)\to[0,1]$ defined by $f(x)=\exp(-x)$ is Lipschitz with constant $L=\sup_{x\in[0,\infty)}\{f'(x)\}\leq 1$. Moreover, note that the set $\mathcal T= \{T(\bv,\bE^R): \bv\in\textit{Perm}\}$ is equivalent for all $\bE^R\in\textit{Perm}$. Finally, observe that $|\exp(-\lambda_k T(\bW_1^R,\bE^R))| \leq 1, | \{\sum_{\bv\in \textit{Perm}}\exp(-\lambda_k T(\bv,\bE^R))\}^{-1} |\leq 1$,  $T(x,y)\in\{0,1,2,3\}$, and $|\mathcal T|=6$. Putting these all together, we have that $\forall 1 \leq k \leq 2$,
\small
\begin{align*}
\nabla_{\lambda_k}\left({\sum_{\bv\in\textit{Perm}}\exp(-\lambda_kT(\bv,\bE^R))}\right)^{-1}= \frac{{\sum_{T\in \mathcal T}\exp(-\lambda_kT)T}}{\left(\sum_{T\in\mathcal T}\exp(-\lambda_k T)\right)^2}\leq 18
\end{align*}
\normalsize

Thus $f:[0,\infty)\to\mathbb{R}$ defined as $f(x)=1/\sum_{T\in\mathcal T}\exp(-xT)$ is Lipschitz with constant $L\leq 18$. Then:
\small
\begin{align*}
P(\bW_k^R|\bV,\lambda_k)=&\frac{\exp(-\lambda_k T(\bW_k^R,\bE^R))}{\sum_{\bv\in\textit{Perm}}\exp(-\lambda_k T(\bv,\bE^R))}\;\;\mbox{and}
\end{align*}
\begin{align*}
&|P(\bW_k^R|\bV,\lambda_k^{(2)})-P(\bW_k^R|\bV,\lambda_k^{(1)})|\\
=& \left|\frac{\exp(-\lambda_k^{(2)} T(\bW_k^R,\bE^R))}{\sum_{\bv\in\textit{Perm}}\exp(-\lambda_k^{(2)} T(\bv,\bE^R))}-\frac{\exp(-\lambda_k^{(1)} T(\bW_k^R,\bE^R))}{\sum_{\bv\in\textit{Perm}}\exp(-\lambda_k^{(1)} T(\bv,\bE^R))}\right|\\
\leq& \left|\frac{\exp(-\lambda_k^{(2)} T(\bW_k^R,\bE^R))}{\sum_{\bv\in\textit{Perm}}\exp(-\lambda_k^{(2)} T(\bv,\bE^R))}-\frac{\exp(-\lambda_k^{(1)} T(\bW_k^R,\bE^R))}{\sum_{\bv\in\textit{Perm}}\exp(-\lambda_k^{(2)} T(\bv,\bE^R))}\right|\\&+
\left|\frac{\exp(-\lambda_k^{(2)} T(\bW_k^R,\bE^R))}{\sum_{\bv\in\textit{Perm}}\exp(-\lambda_k^{(2)} T(\bv,\bE^R))}-\frac{\exp(-\lambda_k^{(1)} T(\bW_k^R,\bE^R))}{\sum_{\bv\in\textit{Perm}}\exp(-\lambda_k^{(1)} T(\bv,\bE^R))}\right| \\
\leq& \frac{}{}\left|\exp(-\lambda_k^{(2)} T(\bW_k^R,\bE^R))-\exp(-\lambda_k^{(1)} T(\bW_k^R,\bE^R))\right|\\&+
\left|\frac{1}{\sum_{\bv\in\textit{Perm}}\exp(-\lambda_k^{(2)} T(\bv,\bE^R))}-\frac{1}{\sum_{\bv\in\textit{Perm}}\exp(-\lambda_k^{(1)} T(\bv,\bE^R))}\right| \\
\leq& |\lambda_k^{(2)}T(\bW_k^R,\bE^R)-\lambda_k^{(1)}T(\bW_k^R,\bE^R)|+18|\lambda_k^{(2)}-\lambda_k^{(1)}|\\
\leq& 21 |\lambda_k^{(2)}-\lambda_k^{(1)}| \\
\leq& 21 \|\theta^{(2)}-\theta^{(1)}\|_2.
\end{align*}
\normalsize
As the product of Lipschitz continuous functions is also Lipschitz continuous with the Lipschitz constant being the sum of those of the functions being multiplied \citep{Shwartz2014}, $M_\theta(\bH_{3}, \bV)$ is Lipschitz continuous in $\theta\in\Theta$ with Lipschitz constant $L(\bV^*) \leq 24||\bV^*||_2+43$, satisfying $\E_{\theta_0, \bV}[ (24||\bV^*||_2+43)^2] < \infty$, so condition (N2) is satisfied, concluding the proof. 
\end{proof}


\vspace*{1cm}
\section{Latent Modeling}
\label{append:latentmodeling}

\subsection{Model Selection}
\label{append:modelselection}

Note that we require specifying a parametric model for $P(\bH_{K+1}|\bE)$ with parameter vector $\theta$ and a proposed distribution for $\bE$. 
In practice, we do not know in advance what they are, yet obtaining a good estimate of $\theta$ is important in our framework. 

One way to improve parameter estimation is to consider a finite set of choices for $P(\bE)$ and propose a finite set of diverse parametric models $M_1(\bH_{K+1}|\bE, \theta_1),\dots,M_P(\bH_{K+1},|\bE,\theta_P)$ for $P(\bH_{K+1}|\bE)$. We can then make the weaker assumption that one pair of our models is correct. Here, $\theta_p$ denotes the parameter vector associated with the $p$-th proposed parametric model $M_p(\bH_{K+1}|\bE,\theta_p)$. 
For any $P(\bE)$, one can then partition the data as $\mathcal D=\mathcal D_T\cup \mathcal D_V$, train each parametric model $M_p(\bH_{K+1}|\bE,\theta_p), 1\leq p \leq P$ as $M_p(\bH_{K+1}|\bE,\hat\theta_p)$ on the training set $\mathcal D_T$, and evaluate the estimated models using metrics on the held-out validation set $\mathcal D_V$ such as the observed log-likelihood $\sum_{\bH_{K+1}\in\mathcal D_V}\log\int_{\mathcal E}M_p(\bH_{K+1}|\bE,\hat\theta_p)dP(\bE)$ or BIC which includes an additional penalty term $Card(\theta_p)log(n)$. 
As long as the proposed finite models for $P(\bH_{K+1}|\bE)$ encompass a large function class, one can usually obtain a good estimate of $\theta$ without the need to enumerate over a large set of $P(\bE)$.
Additionally, as $P(\bH_{K+1}|\bE) = f(\bH_{K+1}|\bE)g(\bH_{K+1})$ where $g(\bH_{K+1})$ is unknown, proposing parametric models for the partial likelihood $f(\bH_{K+1}|\bE)$ and selecting from them is also valid. The Lemma \ref{lemma:modelselect} provides theoretical justification for such a procedure, the proof of which is a direct application of Lemma 5.35 of \citet{Vaart1998}. Finally, if the selected models for $P(\bH | \bE)$ and $P(\bE)$ happened to be conjugate pairs, close form solution for $\hat\pi_n$ 

\vspace*{0.5cm}
\begin{lemma}
\label{lemma:modelselect}
Suppose $\mathcal M =\{f_{\theta}: \theta \in \Theta = \{\theta_1, \dots, \theta_P\}\}$ models for $f(\bH_{K+1}|\bE)$ where $F_{\theta_p}(\bH_{K+1}) = f_p(\bH_{K+1}|\bE, \theta_p)g(\bH_{K+1}), 1\leq p \leq P$ define valid probability measures. Suppose $\exists \theta_0 \in \Theta$ such that $F_{\theta_0}(\bH_{K+1}|\bE)=f(\bH_{K+1}|\bE)$ and $F_{\theta_p}(\bH_{K+1}) \neq F_{\theta_0}(\bH_{K+1})$ for every other $\theta \in \Theta$. Then $\mathbb{E}_{\theta_0}[log(dF_{\theta_0}/ dF_{\theta})]$ attains the unique maximum at $\theta = \theta_0$.
\end{lemma}

\subsection{Specification of Priors}
Maximum A Posteriori (MAP) estimation requires selecting priors for parameters $\theta$. As shown in Theorem 4.1, the consistency and asymptotic normality of $\hat\pi_n$ relies on the compactness of $\Theta$. Combined with the parametric modeling approach, we recommend proper priors supported on bounded $\Theta$ to ensure well-posed inference and avoid unrealistic values of random variables. Non-informative or normal priors are common choices; more robust heavy-tail priors are also good choices to consider. 

In our simulation setup for BEST, normal priors are used, which corresponds to adding $L_2$ regularization. Laplace prior would correspond to an $L_1$ regularization. In this respect, one could select priors that regulate $\theta$ towards desired properties, such as smoothness and sparsity.

One could conduct sensitivity analysis to check how $\hat\pi_n$ varies under different priors to gauge robustness, especially under small or moderate sample sizes. In cases of high variation, one may consider collecting more data, simplifying the model for $P(\bH_{K+1}|\bE)$, or adopt full Bayesian inference to better capture posterior uncertainty. We refer readers to \citep{gelman_bayesian_2014} for further details on Bayesian modeling.


\section{Application to the BEST Trial, further results}
\subsection{Latent Model Estimation for the BEST Study}
\label{append:modelestimation}
Here we provide further details regarding parameter estimation for the preference model $P(\mathbf{H}_{K+1}|\mathbf{V})$ under the setting tailored towards the BEST study introduced in section 5. Denote our parametric model $P(\mathbf{H}_{K+1}|\mathbf{V},\theta)$. We calculate $\hat\theta_n $ and plot the mean absolute error $\dim(\theta)^{-1}||\hat\theta_n-\theta_0||_1$ for varying sample sizes in Figure \ref{fig:best_mae}. 10 random seeds are run for each sample size to account for parameter and sample variability. We can see that error declines with sample size at an approximately linear rate, verifying the results given in Theorem 4.1. We also note that the identifiability assumption made in Theorem 4.1 is a necessary condition for consistency. Our algorithm converges to values close to the true parameter vector for large sample sizes across multiple seeds, supporting the identifiability of our proposed model. 

% Figure environment removed

Our optimization algorithm also performed well. Across sample sizes and seeds, we consistently observed $\log P(\widehat\theta|\mathcal D)\geq \log P(\theta_0|\mathcal D)$ and $||\nabla_\theta\log P(\widehat\theta|\mathcal D)||_{L^\infty(P_{\theta_0})}<10^{-7}$, indicating high convergence quality. Computation times for model fitting with varying sample sizes are reported in Figure \ref{figure:best_comp}. With $N=600$ simulated patients, which is a conservative estimate of the actual sample size for the BEST study, model-fitting took around $100$ seconds on average. Even with $10,000$ simulated patients, model fitting took under $900$ seconds on average. Computational performance can further be improved if needed by reducing the number of starting points and gradient descent iterations used as a warm-up for L-BFGS. These results demonstrate the efficiency and scalability of our optimization algorithm. 

GPU computing and TensorFlow are usually used for optimizing deep learning models and are more common in computer science. It is less commonly used in the statistical literature to implement MC integration and quasi-Newton algorithms. Instead, other integration and optimization algorithms such as (adaptive) Gauss-Hermite quadrature, Markov Chain Monte Carlo (MCMC) or expectation-maximization (EM) combined with CPU computing are more popular \citep{Givens2012, GLIMMIX2018, Butler2018}, all of these methods would have taken significantly more time for our setting. We hope that our results will motivate better computational approaches in the statistical literature in the future. 

\sidecaptionvpos{figure}{c}
\begin{SCfigure}[][ht]
\centering
% Figure removed
\caption{Mean computation time (seconds) across 10 seeds is shown with standard error bars for various sample sizes. Optimization was performed with a single Tesla V100-SXM2 GPU, five 2.40 GHz Intel CPU cores, and 10GB of RAM.}
\label{figure:best_comp}
\vspace{-0.4cm}
\end{SCfigure}


\subsection{Model-Misspecification}
\label{append:modelmisspec}
This subsection provides additional simulation result when $P(\bE)$ is mis-specified under the data-generating process designed for BEST (section 5).

We generate the true latent preference as $\bE \sim Dirichlet(\alpha = c(1,1,1))$, with $\tilde{\bV} = \bE$; but we estimate $\theta$ assuming $\bV \sim \mathcal{N}_2(0, I)$, $\tilde{\bV} = (0, \bV)$, $\bE = SoftMax(\tilde{\bV})$. The evaluation data is again an independent data generation based on the truth. The estimation method, modeling choice for the Q functions, and baseline comparators remain the same as those described in section 5.2. Table (\ref{apptable:best_Eerr}) summarizes the estimated conditional preference when $P(\bE)$ is specified correctly and when it is not in various sample sizes. The sample sizes displayed are both the training and evaluation sample sizes. Mean and standard deviation are taken across results over 10 different seeds.


\begin{table}[H]
\centering
\captionsetup{margin=1.5cm}
\caption{Mean (SD) of $10 \times MAE(\mathbb{E}[\mathbf{E}|\mathbf{H}_2; \theta_0]- {\mathbb{E}}[\mathbf{E}|\mathbf{H}_2;\hat\theta_n])$ across Sample Sizes.}\label{apptable:best_Eerr}
\begin{tabular}{llllll}
\hline
$\text{}$ & N = 150 & N = 300 & N = 600 & N = 1200 & N = 2500 \\
\hline
Correct $P(\bE)$ & 0.07 (0.03) & 0.06 (0.03) & 0.04 (0.01) & 0.03 (0.01) & 0.01 (0.01) \\
Mis-specified $P(\bE)$ \quad & 0.32 (0.20) & 0.37 (0.19) & 0.16 (0.05) & 0.14 (0.03) & 0.12 (0.02) \\
\hline
\end{tabular}
\end{table}

We can see that, when the models for $P(\bW|\bX, \bE)$ are held to be the same, the mean absolute error (MAE) of $\mathbb{E}[\mathbf{E}|\mathbf{H}_2; \theta_0]- {\mathbb{E}}[\mathbf{E}|\mathbf{H}_2;\hat\theta_n]$ is consistently larger if $P(\bE)$ mis-specified, highlighting the importance of model selection to better align the observed likelihood.
The effect of model mis-specification on the resulting value of the estimated DTRs can be seen by comparing Table (\ref{apptable:best_n_correct}) and Table (\ref{apptable:best_n_mis}).
Note that Table (\ref{apptable:best_n_correct}) is the same as Table 1 in the main text, but is included here for ease of comparison.
When mis-specified, there is a greater gap between $V(\hat\pi_{Known})$ and $V(\hat\pi_{LUQL})$ especially when sample sizes are small. However, $\hat\pi_n$ improves more quickly with sample sizes compared with when $P(\bE)$ is correctly specified. Additionally, LUQ-Learning's DTR still outperforms both baselines, with an even larger gap between $\hat\pi_{LUQL}$ and $\hat\pi_{Wlast}$ compared to the correctly specified case. 

\begin{table}[H]
\centering
\captionsetup{margin=2cm}
\caption{Mean (SD) of $V(\hat{\pi}) - V(\pi_{obs})$ across Sample Sizes, $P(\bE)$ Correctly specified.}\label{apptable:best_n_correct}
\begin{tabular}{lllllll}
\hline
DTR & N = 150 & N = 300 & N = 600 & N = 1200 & N = 2500 \\
\hline
$\hat\pi_{\text{Known}}$ & 0.60 (0.12) & 0.67 (0.13) & 0.67 (0.11) & 0.647 (0.102) & 0.678 (0.099)  \\
$\hat\pi_{\text{LUQL}}$ & 0.31 (0.18) & 0.43 (0.09) & 0.41 (0.09) & 0.410 (0.053) & 0.433 (0.048) \\
$\hat\pi_{\text{Wlast}}$ & 0.08 (0.19) & 0.21 (0.12) & 0.31 (0.14) & 0.362 (0.126) & 0.426 (0.115) \\
$\hat\pi_{\text{Naive}}$ & -0.07 (0.13) & 0.03 (0.11) & 0.03 (0.05) & -0.005 (0.059) & 0.027 (0.041) \\
\hline
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\captionsetup{margin=2cm}
\caption{Mean (SD) of $V(\hat{\pi}) - V(\pi_{obs})$ across Sample Sizes, $P(\bE)$ Mis-specified.}\label{apptable:best_n_mis}
\begin{tabular}{lllllll}
\hline
DTR & N = 150 & N = 300 & N = 600 & N = 1200 & N = 2500 \\
\hline
$\hat\pi_{\text{Known}}$ & 0.88 (0.16) & 0.94 (0.19) & 0.90 (0.16) & 0.95 (0.14) & 0.96 (0.16)  \\
$\hat\pi_{\text{LUQL}}$ & 0.28 (0.23) & 0.40 (0.19) & 0.57 (0.15) & 0.59 (0.09) & 0.64 (0.11) \\
$\hat\pi_{\text{Wlast}}$ & 0.01 (0.11) & 0.07 (0.16) & 0.16 (0.15) & 0.23 (0.18) & 0.35 (0.12) \\
$\hat\pi_{\text{Naive}}$ & -0.01 (0.17) & 0.02 (0.11) & 0.01 (0.10) & -0.02 (0.04) & 0.004 (0.035) \\
\hline
\end{tabular}
\end{table}

In summary, we stress the importance of model selection to maximize the likelihood; but even when part of the model is mis-specified, LUQ-Learning remains a good option compared with alternatives that avoids using a preference model.



\section{Application to the CATIE Trial}
\label{sec:catie}

To demonstrate the broad applicability of LUQ-Learning, we apply our method to the setting considered by \citet{Butler2018}. Their simulation setting was inspired by the first phase of the Clinical Antipsychotic Trials of Intervention Effectiveness (CATIE) trial \citep{Stroup2003}. Focusing on the first phase, this becomes a single decision point problem, with the data trajectory summarized as $(\textbf{X}_1, \textbf{W}_1, A_1, \textbf{Y}, \textbf{W}_2)$. The authors dichotomize the five treatment options into traditional and atypical antipsychotics, resulting in $\mathcal{A}_{\mathcal{H}_1} = \{0, 1\}$. $\textbf{Y} \in \mathbb{R}^2$ comprises two continuous outcomes: efficacy measured using the Positive and Negative Syndromes Scale (PANSS) \citep{kay1987positive} and side effect burden measured as the sum of side effects and adverse events. $\mathbf{W}_1 \in \{0, 1\}^{10}$ are 10 Yes/No questions from the Drug Attitude Inventory \citep{Hogan1983}.  We adopt the same generative model as in \cite{Butler2018} with the addition of a log-linear model for $\mathbf{W}_2$, the reported treatment satisfaction collected at the end of study.
\footnotesize
\begin{align*}
& V\sim \mathcal{N}(0,1), \\
& \mathbf{E}=(\Phi(V), 1-\Phi(V)), \\
& \textbf{X}_1 \sim \mathcal{N}_{5}(0,\mathbf{I}), \\
& {W}_{1,j} \sim \text{Bernoulli}(p = \sigma(\beta_{j,0}+\beta_{j,1}V)), \quad (1 \leq j \leq 10), \\
& A_1 \sim \text{Bernoulli}(p = 0.5), \\
& Y_j = \mathbf{X}_{1*}^T\gamma_{j,0} + A\mathbf{X}_{1*}^T\gamma_{j,1}+\epsilon_j, \text{ where } \epsilon_j \sim \mathcal{N}(0,1), \quad (1 \leq j \leq 2), \\
& \mathbf{W}_2 \sim \text{Pois}\left(\lambda=\exp(\alpha_0+\alpha_1 \mathbf{E}^T \mathbf{Y})\right).
\end{align*}
\normalsize
Here $\mathbf{X}_{1*}=(\mathbf{1},\mathbf{X}_1)$ and $\Phi(\cdot)$ is the standard normal cumulative distribution function. The parameters are generated as follows. $\gamma=(\gamma_{ij})_{i,j=1}^{2}$ was fixed as in \citet{Butler2018} to make outcomes $Y_1$ and $Y_2$ in a competing relationship. Here, $\theta = (\alpha, \beta)$, where $\alpha = (\alpha_0, \alpha_1), \beta = (\beta_{j,0}, \beta_{j,1})_{j=1}^{10}$, so $Card(\theta) = 22$.
\footnotesize
\begin{align*}
    &\beta_{j,0}=0, \quad \beta_{j,1}\sim \mathcal{N}(0,1) \quad (1 \leq j \leq 10) \\
    &\alpha_0= - \alpha_1\min_{i}(\mathbf{E}_{i}^T \mathbf{Y}_{i}) - 3, \quad \alpha_1= 6 /(\max_{i}(\mathbf{E}_{i}^T \mathbf{Y}_{i})-\min_i(\mathbf{E}_{i}^T \mathbf{Y}_{i}))\\
    &\gamma_{1,0} = (2.5, 0.2, 0.25, -0.7, -2.5, 2.4), \quad
    \gamma_{1,1} = (1.7, -2.3, 4.5, 6, -7.3, -1.6)\\
    &\gamma_{2,0} = 3 - 2\gamma_{1,0}, \quad \gamma_{2,1} = 3 - 2\gamma_{1,1}
\end{align*}
\normalsize

\begin{table}[H]
\centering
\captionsetup{margin=1.5cm}
\caption{Mean (SD) of $MAE(\hat{\mathbb{E}}[\mathbf{E}|\mathbf{H}_1;\hat\theta_n]- \hat{\mathbb{E}}[\mathbf{E}|\mathbf{H}_1;\theta_0])$ by Sample Sizes.}\label{table:catie_Eerr}
\begin{tabular}{lllll}
\hline
$\text{}$ & N = 100 & N = 200 & N = 500 & N = 1000 \\
\hline
LUQ-Learning & 0.04 (0.02) & 0.03 (0.01) & 0.02 (0.006) & 0.01 (0.003)  \\
Butler's Method \quad\quad & 0.18 (0.19) & 0.25 (0.19) & 0.21 (0.20) & 0.24 (0.20) \\
\hline
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\captionsetup{margin=2cm}
\caption{Mean (SD) of $V(\hat{\pi}) - V(\pi_{obs})$ by Sample Sizes.}\label{table:catie_n}
\begin{tabular}{lllll}
\hline
DTR & N = 100 & N = 200 & N = 500 & N = 1000 \\
\hline
$\hat\pi_{\text{Known}}$ & 2.78 (1.57) & 3.18 (1.15) & 3.54 (0.53) & 3.73 (0.62) \\
$\hat\pi_{\text{LUQL}}$ & 2.60 (1.86) & 2.70 (1.20) & 3.00 (0.65) & 3.11 (0.60) \\
$\hat\pi_{\text{Butler}}$ & 1.92 (2.07) & 1.31 (1.07) & 1.85 (1.41) & 1.50 (1.62) \\
$\hat\pi_{\text{Wlast}}$ & 1.57 (1.94) & 1.59 (1.16) & 1.71 (0.72) & 2.02 (0.59) \\
$\hat\pi_{\text{Naive}}$ & 2.58 (1.74) & 2.30 (1.05) & 2.56 (0.63) & 2.50 (0.62) \\
\hline
\end{tabular}
\end{table}


We assume a correctly specified model for $P(\mathbf{W}_k|\mathbf{X}_k, \mathbf{E})$, $k = 1, 2$, and $P(\mathbf{W}_{K+1}|\mathbf{Y}, \mathbf{E})$. We estimated the preference model parameters $\theta = (\alpha, \beta)$ via partial maximum likelihood and fitted the Q-functions using RF with hyperparameters selected using the same strategy as described in section 5.1. Since the simulated datasets contain a single decision point and two outcomes, the methodology of \citet{Butler2018} is also applicable. In this setting, Butler's method reduces to LUQ-Learning with $\mathbf{W}_2$ excluded from the partial likelihood and an EM algorithm for parameter estimation. We consider sample sizes around $N = 200$, approximating the actual CATIE trial size. The policies $\pi_{Wlast}$, $\pi_{Naive}$, and $\pi_{Known}$ are defined as before.

Table \ref{table:catie_n} summarizes the results. LUQ-Learning yields more accurate estimates of expected preference weights (Table \ref{table:catie_Eerr}), leading to superior estimated DTR performance. In contrast, Butler's method exhibits high estimation error, making $\hat{\pi}_{Butler}$ less robust to small sample sizes.

A narrower posterior for a latent variable often results in a more precise posterior for model parameters. \citet{Butler2018} observed that increasing $\dim(\mathbf{W})$ reduced estimation variance despite an increase in $\dim(\theta)$a contrast to complete data log-likelihoods, where more parameters typically increase variance. While $\mathbb{E}[\mathbf{E}|\mathbf{H}_1]$ does not depend on $\mathbf{W}_2$, omitting $\mathbf{W}_2$ results in a broader posterior for $P(\mathbf{E}|\mathbf{H}_1)$ compared to $P(\mathbf{E}|\mathbf{H}_2)$, increasing estimation variance, even for parameters solely related to $P(\mathbf{E}|\mathbf{H}_1)$.


We point out that Theorem 5.1 extends naturally to the latent variable model proposed here, following the same proof structure. In fact, Table (\ref{table:catie_n}) provides empirical evidence supporting the assumption that (V1): $\|\widehat{\mathbb{E}}[\mathbf{E}|\mathbf{H}_2; \hat\theta_n] - \mathbb{E}[\mathbf{E}|\mathbf{H}_2; \theta_0]\|_{P_{\theta_0}} \to 0$ holds for LUQ-Learning. This is because the table shows the decrease of the average mean absolute error to a fairly small value as sample size grows. Given that $P(\mathbf{H}_K)$ is uniformly bounded and that MC integration provides consistent estimate as $N_{sim} \to \infty$, (V1) has to follow. 

\end{document}

