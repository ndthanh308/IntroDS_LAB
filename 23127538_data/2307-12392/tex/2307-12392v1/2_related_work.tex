\section{Related Work}

\noindent
\textbf{Visual Grounding.} The Visual Grounding task is an important problem in computer vision that aims to localize an object within an image based on a given language reference expression. The existing approaches typically extend the object detection framework, such as YOLOV3~\cite{YOLOV3}, Faster-RCNN~\cite{Faster-RCNN}, RetinaNet~\cite{RetinaNet}, CenterNet~\cite{CenterNet}, and DETR~\cite{DETR}, by incorporating a visual-linguistic fusion module. These approaches can be categorized into two main categories: \textit{two-stage methods}~\cite{CMN, NMTree, Ref-NMS,Two-Branch, MAttNet} and \textit{one-stage methods}~\cite{FAOA,RealTime,ReSC-Large,image_captioning,Zero-shot,onestage1}. Two-stage approaches, including CMN~\cite{CMN}, NMTree~\cite{NMTree} and RefNMS~\cite{Ref-NMS}, Two-branch Network~\cite{Two-Branch} and MAttNet~\cite{MAttNet}, utilize an object detector to generate region proposals and then use textual descriptions to select the highest scoring proposal in the second stage. However, this approach can be computationally expensive due to the large number of proposals, and the matching process for each proposal may slow down the inference speed. On the other hand, one-stage approaches~\cite{FAOA,RealTime,ReSC-Large,image_captioning,Zero-shot,onestage1,hu2016segmentation} directly incorporate the linguistic context into visual features to predict the object's location, without generating region proposals. Although one-stage approaches are simple and efficient, they typically rely on pointwise feature representations, which may not be flexible enough to achieve a global context understanding from the vision-language information. Recently, \textit{transformer-based} Visual Grounding approaches have gained popularity due to their attention capacity and efficiency. For instance, TransVG~\cite{TransVG} captures intra- and inter-modal contexts using transformers in a uniform manner, while VLTVG~\cite{vltvg} builds discriminative feature maps and detects the target object through a multi-stage decoder.

\noindent
\textbf{Robustness in Visual Grounding.} Recent studies have explored CNN robustness in various benchmarks~\cite{Robust1}~\cite{Robust2}, and some works have evaluated and improved CNN robustness for practical applications~\cite{Robust3}~\cite{Robust4}~\cite{Robust5}~\cite{RRIS}. RefSegformer~\cite{RRIS} incorporates negative sentence inputs to handle false-alarm issues in referring segmentation tasks. However, to the best of our knowledge, no existing benchmarks or approaches have explored the robustness of the Visual Grounding task. In practice, existing approaches often fail to generate accurate targets when an irrelevant or inaccurate language expression is given. Therefore, this paper takes a further step by proposing a new iterative \textbf{{\it robust}} VG framework and building two robust VG datasets to address this research problem. It is important to note that, within the context of this paper, the term ``\textbf{robust}'' refers to the ability of the proposed method to produce accurate results and avoid false-alarm predictions even when provided with irrelevant and incorrect expressions. 

\noindent
\textbf{Multi-modal Transformer.} Vision transfomer~\cite{DETR,dosovitskiy2020image,li2023transformer,zhang2022eatformer,zhang2023rethinking,li2023tube,zhou2022transvod,li2023panopticpartformer++,li2022videoknet,cheng2021maskformer,xu2022fashionformer,swin} has a a wide range of application, including detecction, representation learning, and segmentation. Recent works~\cite{mdetr,zhu2021uni,zhu2022uni,wu2023open,wu2023betrayed} unify different modal inputs and outputs, mainly representation learning, open vocalbulary, and large language models. For visual grounding, recent works~\cite{RRIS,yang2021lavt,kim2022restr,TransVG} also adopt multu-modal transformer framework, our method belong to this scope. In partilcaur, we pay more attention on the robustness and fine-grained supervision design.

