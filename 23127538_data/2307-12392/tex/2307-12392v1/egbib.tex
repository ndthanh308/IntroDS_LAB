\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multicol}
\usepackage[switch]{lineno}
\usepackage{color}
\usepackage{multirow}
\usepackage[marginal]{footmisc}
\usepackage{bbding}
\renewcommand{\thefootnote}{}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{11} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{Iterative Robust Visual Grounding with Masked Reference based Centerpoint Supervision}

\author{Menghao Li\textsuperscript{1} \textsuperscript{*} \and Chunlei Wang\textsuperscript{1} \thanks{Contribute Equally.} \and Wenquan Feng\textsuperscript{1}\and Shuchang Lyu\textsuperscript{1}\and Guangliang Cheng\textsuperscript{2~\Envelope}\and Xiangtai Li\textsuperscript{3}\and Binghao Liu\textsuperscript{1}\and Qi Zhao\textsuperscript{1~\Envelope}
\and \\
\textsuperscript{1} Beihang University \textsuperscript{2} University of Liverpool \textsuperscript{3} S-Lab, Nanyang Technological University \\
{\tt\small \{sy2102227, wcl\_buaa, buaafwq, lyushuchang, liubinghao, zhaoqi\}@buaa.edu.cn}\and {\tt\small \{Guangliang.Cheng\}@liverpool.ac.uk}\and {\tt\small \{xiangtai.li\}@ntu.edu.sg}
}

\maketitle
%%%%%%%%% ABSTRACT
\begin{abstract}
Visual Grounding (VG) aims at localizing target objects from an image based on given expressions and has made significant progress with the development of detection and vision transformer.
%
However, existing VG methods tend to generate \textbf{false-alarm} objects when presented with inaccurate or irrelevant descriptions, which commonly occur in practical applications.
%
Moreover, existing methods fail to capture fine-grained features, accurate localization, and sufficient context comprehension from the whole image and textual descriptions. 
%
To address both issues, we propose an Iterative Robust Visual Grounding (\textbf{IR-VG}) framework with Masked Reference based Centerpoint Supervision (MRCS).
%
The framework introduces iterative multi-level vision-language fusion (IMVF) for better alignment.
%
We use MRCS to ahieve more accurate localization with point-wised feature supervision.
% to comprehensively amplify the vision-language understanding and alignment.
%
Then, to improve the robustness of VG, we also present a multi-stage false-alarm sensitive decoder (MFSD) to prevent the generation of false-alarm objects when presented with inaccurate expressions. 
%
%The proposed framework is evaluated on five {\it \textbf{regular}} VG datasets and two newly constructed {\it \textbf{robust}} VG datasets. 
Extensive experiments demonstrate that IR-VG achieves new state-of-the-art (SOTA) results, with improvements of 25\% and 10\% compared to existing SOTA approaches on the two newly proposed robust VG datasets. Moreover, the proposed framework is also verified effective on five {\it \textbf{regular}} VG datasets. Codes and models will be publicly at \url{https://github.com/cv516Buaa/IR-VG}.
\end{abstract}
\vspace{-0.7cm}

\input{1_intro}
\input{2_related_work}
\input{3_method}
\input{4_exp}
\input{5_conclusion}


{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}