\section{Experiment}

\subsection{Experimental Settings}

\par \noindent \textbf{Datasets.} To comprehensively verify the effectiveness of the proposed robust VG approach, we evaluate it on two types of datasets: the regular VG datasets and the robust VG datasets.
\par \noindent \textbf{Regular VG datasets.} We evaluate our proposed approach on five regular VG datasets, including the RefCOCO~\cite{RefCOCO}, RefCOCO+~\cite{RefCOCO}, RefCOCOg~\cite{RefCOCOg}, ReferItGame~\cite{ReferItGame}, and Flickr30k~\cite{Flickr30k}. The RefCOCO datasets series, including RefCOCO, RefCOCO+, and RefCOCOg, are three commonly used benchmarks for visual grounding, the images used in these datasets are collected from the train2014 set of MSCOCO dataset. Specifically, the RefCOCO dataset contains 19,994 images, 50,000 reference objects, and a total of 142,210 reference expressions. Among them, 120,624 reference expressions are used as the training set, 10,834 as the validation set, 5657 and 5095 expressions for test A and test B, respectively. The RefCOCO+ dataset provides 19,992 images with 49,856 reference objects and 141,564 reference expressions. Similar to RefCOCO, RefCOCO+ is also divided into training, validation, test A, and test B sets, with 120,191, 10,758, 5,726, and 4,889 reference expressions in these datasets. RefCOCOg contains a total of 25,799 images, 49,822 objects, and 95,010 reference expressions. Compared to the first two datasets, most of the expressions in RefCOCOg have longer sentences and more complex statement structures. RefCOCOg contains two sub-datasets, RefCOCOg-google and RefCOCOg-umd. Since the former dataset does not provide a test set, we mainly use the RefCOCOg-umd dataset. ReferItGame contains 20,000 images, which are collected from the SAIAPR-12 dataset. This dataset has a total of 120,072 reference expressions and is divided into a training set with 54,127 reference expressions, a validation set with 5,842 reference expressions, and a test set with 60,103 reference expressions. Flickr30k contains 31,783 images and 427,000 reference expressions. We divide the training, validation, and test sets using the same ratio as the previous work.
% RefCOCO~\cite{RefCOCO}, RefCOCO+~\cite{RefCOCO}, RefCOCOg~\cite{RefCOCOg}, ReferItGame~\cite{ReferItGame}, Flickr30k~\cite{Flickr30k}, under the experimental settings identical to those of \cite{vltvg}
\par \noindent \textbf{Robust VG Datasets} We construct two robust VG datasets based on the existing benchmarks RefCOCOg and ReferItGame, termed RefCOCOg\_F and ReferItGame\_F. The train set of our robust VG datasets contains two parts of data, the first part is the train set of the original dataset, while the second part is a random matching dataset, which destroys the correspondence between the image information and the language descriptions. 
Specifically, for each target on the image, we select one description that is different from its original one among all the text descriptions in the dataset, thus building a dataset where the image is with irrelevant or inaccurate descriptions. During training, the ratio of these two parts of data is 1:1. The test set of our robust VG datasets also consists of two parts of data, the first part is the test set of the original dataset while the second part is the manually modified robust VG dataset, which requires manual intervention to modify some keywords in the descriptions, thus modifying the semantics of the descriptions and building a more difficult dataset. For instance, we manually modify the expression “The man in white T-shirt is riding a bike” to “The man in blue T-shirt is riding a bike”. Specifically, the test set of the RefCOCOg\_F dataset contains 2000 pairs of false-alarm data and 9602 pairs of regular data that are from the original RefCOCOg test set. The test set of the ReferItGame\_F dataset contains 1000 pairs of false-alarm data and 9000 pairs of regular data that are randomly sampled from the test set of the original ReferItGame dataset. 
\par Specifically, the data combination method of the random matching dataset is to randomly replace the description in each group of data in the training set with a random other description in the dataset to construct false-alarm data. Of course, the description of the same image will not be selected to avoid the existence of the target corresponding to the ran71 dom description on the image. It can be observed that the probability of the existence of the target corresponding to the description on the image is very low for the false alarm data formed by this random selection description method. 
\par We build the manually modified robust VG dataset by manually modifying some keywords in the description. In general, we mainly modify words from the following perspectives. First, modifying key nouns can greatly change the semantics of words, thus generating false alarm data. For example, modify ”Two men on a horse” to ”Two men on a car” (as shown in the first row of Fig.~\ref{fig:FA_Modified}). Second, modifying key adjectives can also change the description semantics. For example, modify ”A man with a bat wearing a red helmet” to ”A man with a bat wearing a yellow helmet” (as shown in the second row of Fig.~\ref{fig:FA_Modified}). Third, modify words in the text that relate to spatial location can mismatch the original target with the newly generated text. For example, modify ”An elephant trainer standing beside an elephant walking down the street” to ”An elephant trainer standing far away from an elephant walking down the street” (as shown in the third row of Fig.~\ref{fig:FA_Modified}). Fourth, changing the words corresponding to some fine-grained features can generate false-alarm data. For example, modify ”A man wearing glasses” to ”A man without glasses” (as shown in the fourth row of Fig.~\ref{fig:FA_Modified}). Experiments show that our pro95 posed IR-VG is effective for all four types of false alarm data.
\par \noindent \textbf{Implementation Details.} Consistent with SOTA approaches such as TransVG~\cite{TransVG} and vltvg~\cite{vltvg}, our proposed method employs ResNet101~\cite{ResNet} as the backbone, augmented with 6 transformer layers in the image feature extraction branch, initialized using weights from DETR~\cite{DETR}. The textual embedding extraction branch is initialized with BERT~\cite{Bert}, while the parameters of other components use Xavier scheme~\cite{Xavier} initialization. We resize all images to $640\times640$ and fill them with black to form a square. We perform experiments using PyTorch, a 3090ti GPU, a batch size of 16, and run training for 90 epochs using the AdamW optimizer with a learning rate of $3\times10^{-4}$ and a weight decay of $1\times10^{-4}$.
% Figure environment removed
% Figure environment removed 
\begin{table*}[tbh]
\begin{center}
\setlength{\tabcolsep}{2.0mm}{
\begin{tabular}{|c|c c c|c c c|c c|c|c|}
\hline
 \multirow{2}{*}{Method} &
      \multicolumn{3}{c|}{RefCOCO}  & \multicolumn{3}{c|}{RefCOCO+} & \multicolumn{2}{c|}{RefCOCOg} &\multicolumn{1}{c|}{ReferItGame}  &\multicolumn{1}{c|}{Flickr30k} \\
                                   &$\text{val}$   &$\text{testA}$  &$\text{testB}$  
    &$\text{val}$   &$\text{testA}$  &$\text{testB}$ 
    &$\text{val-u}$   &$\text{test-u}$
    &$\text{test}$
    &$\text{test}$  \\
\hline
  CMN~\cite{CMN} &-  &71.03 &65.77 &-  &54.32 &47.76 &-  &- &28.33  &-\\
  VC~\cite{VC} &-  &73.33 &67.44 &-  &58.40 &53.18 &-  &- &31.13  &-\\
  NMTree~\cite{NMTree} &76.41  &81.21 &70.09 &66.46  &72.02 &57.52 &65.87  &66.44 &-  &-\\
  Ref-NMS~\cite{Ref-NMS} &80.70  &84.00 &76.04 &68.25  &73.68 &59.42 &70.55  &70.62 &-  &-\\
  FAOA~\cite{FAOA} &72.54 &74.35 &68.50  &56.81 &60.23 &49.60  &61.33 &60.36  &60.67 &68.71\\
  LBYLNet~\cite{LBYL-Net} &79.67  &82.91 &74.15 &68.64  &73.38 &59.49 &-  &- &67.47  &-\\
  TransVG~\cite{TransVG} &81.02  &82.72 &78.35 &64.82  &70.70 &56.94 &68.67  &67.73 &70.73  &79.10\\
  VLTVG~\cite{vltvg} &84.77  &87.24 &80.49 &74.19  &78.93 &65.17 &76.04  &74.98 &71.98  &79.84\\
  IR-VG (Ours) &\textbf{86.82}  &\textbf{88.75} &\textbf{82.60} &\textbf{76.22}  &\textbf{80.75} &\textbf{67.33} &\textbf{77.86}  &\textbf{76.24} &\textbf{74.03}  &\textbf{81.45}\\
\hline
\end{tabular}}
\end{center}
\caption{Comparisons with SOTA visual grounding methods.}
\label{result}
\end{table*}
% \vspace{-0.1cm}
\begin{table}
\begin{tabular}{|c|c c|c c|}
\hline
  \multirow{2}{*}{Methods} & \multicolumn{2}{c|}{RefCOCOg\_F~~}  & \multicolumn{2}{c|}{ ReferItGame\_F}
  \\
  &$R_\text{fad}$ &$R_\text{mix}$
  &$R_\text{fad}$ &$R_\text{mix}$
  \\
  \hline
  CMN~\cite{CMN} &~27.10&~65.10 &~24.75 &~21.41~  \\
  VC~\cite{VC} &~42.45 &~68.85 &~31.03 &~25.69~  \\
  SSG~\cite{SSG} &~34.15  &~61.25 &~32.44 &~46.43~ \\
  Ref-NMS~\cite{Ref-NMS} &~43.90  &~62.40 &~41.39 &~48.15~\\
  ReSC-Large~\cite{ReSC-Large} &~37.35  &~60.55 &~32.54 &~59.89~ \\
  LBYLNet~\cite{LBYL-Net} &~45.40  &~63.32 &~45.40 &~60.57~ \\
  IR-VG (Ours) &\textbf{~67.32}  &\textbf{~73.61} &\textbf{~69.44} &\textbf{~72.03~}\\
\hline
\end{tabular}
\caption{Comparisons with SOTA approaches on {\it \textbf{robust}} VG datasets.}
\label{false_alarm}
\end{table}
% \vspace{-0.2cm}
\begin{table}
\setlength{\tabcolsep}{0.5mm}{
\begin{tabular}{|c c|c c c|c c c|c c|}
\hline
  \multicolumn{2}{|c|}{Methods}
  &\multicolumn{3}{c|}{RefCOCO}
  &\multicolumn{3}{c|}{RefCOCO+}
  &\multicolumn{2}{c|}{RefCOCOg} 
  \\
  $\text{I}$     &$\text{M}$
  &$\text{val}$   &$\text{testA}$  &$\text{testB}$  
  &$\text{val}$   &$\text{testA}$  &$\text{testB}$ 
  &$\text{val-u}$   &$\text{test-u}$
  \\
  \hline
  - &- &84.77  &87.24 &80.49 &74.19  &78.93 &65.17 &76.04  &74.98\\
  \checkmark &- &85.92  &88.41 &81.77 &75.27  &80.06 &66.33 &77.10  &76.06 \\
  - &\checkmark &85.53  &88.09 &81.23 &75.34  &79.97 &66.18 &77.21  &75.75 \\
  \checkmark &\checkmark &\textbf{86.82}  &\textbf{88.75} &\textbf{82.60} &\textbf{76.22}  &\textbf{80.75} &\textbf{67.33} &\textbf{77.86}  &\textbf{76.24}\\
\hline
\end{tabular}}
\caption{Ablation studies on three benchmarks, "I" and "M" denote IMVF and the MRCS.}
\label{result2}
\end{table}
% \vspace{-0.2cm}
\par \noindent \textbf{Evaluation Metrics.}
For the {\it \textbf{regular}} VG datasdet, following previous works~\cite{TransVG}~\cite{Metric1}, we adopt the commonly used top1 accuracy (acc-1) as the evaluation metric. For the {\it \textbf{robust}} VG dataset, we propose two novel evaluation metrics, i.e., false alarm discovery rate $R_\text{fad}$ with only false-alarm data, and correct rate among the mixed data $R_\text{mix}$ with both false-alarm and regular data, which are defined as, 
%\begin{multicols}{2}
\begin{equation}
    R_\text{fad} = \frac{\text{FA}^\text{acc}}{\text{FA}^\text{all}},~~~
    R_\text{mix} = \frac{\text{FA}^{\text{acc}} + \text{Regular}^\text{acc}}{\text{FA}^\text{all} + \text{Regular}^\text{all}},
\end{equation}
where FA denotes the false-alarm data with irrelevant or inaccurate descriptions, and Regular means the regular data with accurate descriptions. The superscript \textbf{acc} and \textbf{all} represent the number of accurate predictions and the total number of the data. The detailed dataset descriptions, training loss and other experiment implementation details will be shown in the supplementary materials.

\subsection{Comparisons with Existing SOTA Methods}

\par As presented in Tab.~\ref{result}, we evaluate the proposed approach against other SOTA VG methods. Numerically, we improve over the best SOTA approaches by about 2\% in all five benchmarks, indicating the effectiveness of our proposed method.
\par Tab.~\ref{false_alarm} demonstrates the numerical comparisons on the {\it \textbf{robust}} VG datasets. Obviously, we improve over the SOTA approaches by a nontrivial margin in competitive benchmarks of RefCOCOg\_F and ReferItGame\_F. Specifically, on ReferItGame\_F dataset, we achieve about 25\% and 10\% improvement in $R_\text{fad}$ and $R_\text{mix}$ metrics, respectively. It is worth noting that TransVG~\cite{TransVG} and VLTVG~\cite{vltvg} are not included in the comparison because they only provide one predicted bounding box without any extra information to determine whether the target object is a false alarm. As a result, they will definitely generate false-alarm objects when given inaccurate or irrelevant language expressions, which is not a fair comparison. 
% Figure environment removed
% Figure environment removed
% \vspace{-0.2cm}
\subsection{Ablation Study}

\par \textbf{Numerical Component Analysis.} Tab.~\ref{result2} shows the effectiveness of each component on the {\it regular} VG datasets. The proposed approach outperforms the baseline by 2.1\% top1 accuracy in RefCOCO testB dataset. Specifically, IMVF improves by 1.3\% and MRCS improves by 0.7\%. Similar conclusions can be drawn from other {\it regular} VG datasets. Tab.~\ref{false_alarm} illustrates the effectiveness and robustness of the proposed MFSD module, which achieves a significant improvement on two competitive robust benchmarks. For instance, the MFSD module improves by 25\% and 10\% compared with existing SOTA approaches in $R_\text{fad}$ and $R_\text{mix}$ metrics, respectively.
\begin{table}\scriptsize
  \small
  \centering
  \setlength{\tabcolsep}{0.3mm}{
  \begin{tabular}{|c|c c c|c c c|c c|}
  \hline
  \multicolumn{1}{|c|}{Methods}
  &\multicolumn{3}{c|}{RefCOCO}
  &\multicolumn{3}{c|}{RefCOCO+}
  &\multicolumn{2}{c|}{RefCOCOg} 
  \\
                &$\text{val}$   &$\text{testA}$  &$\text{testB}$  
                &$\text{val}$   &$\text{testA}$  &$\text{testB}$ 
                &$\text{val-u}$   &$\text{test-u}$\\
                \hline
   Baseline~ &~85.92  &~88.41 &~81.77~ &~75.27  &~80.06 &~66.33~ &~77.10  &76.06 \\
   Wo masked~ &~86.57  &~88.52 &~82.26~ &~75.84  &~80.41 &~67.02~ &~77.57  &75.93 \\
  Ours~ &~\textbf{86.82}  &~\textbf{88.75} &~\textbf{82.60}~ &~\textbf{76.22}  &~\textbf{80.75} &~\textbf{67.33}~ &~\textbf{77.86}  &~\textbf{76.24}\\
  \hline
  \end{tabular}}
 \caption{Ablation study on multiple masked strategies. ``Baseline'' denotes the experiment with one full text without centerpoint supervision, ``Wo masked'' denotes the result with one full text and centerpoint supervision, and ``Ours" represents the experiment with MRCS.}
 \label{result_s}
\end{table}
% \vspace{-0.2cm}
\subsection{Rules for masking words in MRCS module.}

When we mask lexical words, we prioritize them differently. We first mask prepositions, conjunctions, and qualifiers because they usually do not significantly impact the sentence’s meaning. If these types of words are not present, the module then masks auxiliaries, pronouns, and numbers, which can partly affect the sentence’s semantics. Finally, the module masks adjectives and verbs, which are critical for the sentence’s meaning. If there is only one non-noun word remaining or only nouns remain in the sentence, no further masking is performed. However, even with this priority order, some important words may still get masked, introducing noise into the training. Nevertheless, we empirically demonstrate that the language comprehension improvement from masking operations outweighs the negative effects of introducing noise (shown in Tab.~\ref{result_s}). In all datasets, the number of words exceeds 3, and through three masking operations, we find that the majority of the masked words are prepositions, conjunctions, and qualifiers. Therefore, in most cases, this operation will not affect the meaning of the sentence.
\par \noindent \textbf{Qualitative Component Analysis.} 
\textit{Qualitative analysis of MRCS.} 
Fig.~\ref{fig:FA_vis} illustrates the visualization of prediction results with or without the MFSD module on the robust VG datasets. It shows that the MFSD module enables the model to efficiently identify the presence or absence of targets described in the text on the image. The first row of the figure shows the false alarm data generated by the key nouns in the description being changed, the second row shows the false alarm data generated by the modification of key adjectives (e.g., color). The third line of the figure shows the spatial location relations in the description being modified and the fourth row of the figure shows the fine-grained features in the description being modified. Our MFSD module can effectively identify the false alarm data generated by all the above modification methods.
\textit{Qualitative analysis of MRCS.} Fig.~\ref{fig:kp_vis} presents the visual-linguistic feature map with or without MRCS module. We intuitively observe that the MRCS enables the feature map to attend more accurately to the target object's location, and generates a more precise foreground map. To avoid interactions from IMVF module, we conduct this experiment only with MRCS module and MFSD module. \textit{Qualitative analysis of IMVF.} Fig.~\ref{fig:TA} illustrates the visual-linguistic feature map with or without IMVF module. The figure indicates that the IMVF module reduces interference and allows the model to concentrate more on target by better understanding visual and textual information. To ensure fairness, we performed this experiment only with IMVF module and MFSD module.
