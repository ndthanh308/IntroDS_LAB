\section{Introduction}
\par Visual Grounding (VG) is a crucial computer vision task gaining significant attention due to its potential for enabling practical applications such as robot navigation~\cite{robot_navigation} and visual dialog~\cite{visual_dialog1,visual_dialog2}. VG aims to locate a target object within an image based on the given language reference expressions by incorporating information from both textual and visual modalities. However, existing VG methods suffer from false-alarm issues, where they assume that the referred object always exists in the image, leading to inaccurate or wrong targets being detected when irrelevant or inaccurate textual expressions are provided, shown in Fig.~\ref{fig:failurecases} (a).
% Figure environment removed
\par Previous works~\cite{previous1,previous2,previous3} have made significant progress in VG through various techniques. However, the task of cross-modal learning involved in the VG task remains challenging, and current approaches can be broadly divided into two main categories: two-stage methods~\cite{CMN, NMTree, Ref-NMS,Two-Branch} and one-stage methods~\cite{FAOA,RealTime,ReSC-Large,image_captioning,Zero-shot,onestage1}. Despite the significant achievements, the VG approaches suffer from some limitations, such as failing to capture the detailed feature representation accurately, resulting in a lack of discrimination between fine-grained objects with reference expressions shown in Fig.~\ref{fig:failurecases} (b), and detecting irrelevant or incorrect targets without understanding the whole context shown in Fig.~\ref{fig:failurecases} (c).
\par To address the above issues, this paper proposes a novel iterative robust visual grounding (IR-VG) approach with masked reference based centerpoint supervision. The approach first constructs two new robust VG datasets and proposes a multi-stage false-alarm sensitive decoder (MFSD) module to handle the case when there is no target object from the textual expression, avoiding generating false alarms. Secondly, a new masked reference based centerpoint supervision (MRCS) module is proposed to capture the fine-grained feature and enhance the localization capacity from the given reference expressions. Finally, an iterative multi-level vision-language fusion (IMVF) module is leveraged to fuse multi-level visual and textual information that are crucial for vision-language understanding.
\par The contributions of this paper are summarized as follows: firstly, the proposed approach handles the false-alarm issue in VG task for the \textbf{first time} by constructing two new robust VG benchmarks and introducing a multi-stage false-alarm sensitive decoder (MFSD) module. Secondly, a new masked reference based centerpoint supervision (MRCS) module is proposed to achieve much more accurate fine-grained feature and better localization capacity from fully visual-textual comprehension. Lastly, the iterative multi-level vision-language fusion (IMVF) module is introduced to comprehensively fuse multi-level visual and textual information for better vision-language understanding and alignment. Extensive experiments on five {\it regular} VG benchmarks and two newly constructed {\it robust} VG benchmarks demonstrate the effectiveness of the proposed approach, achieving above \textbf{10\%} improvement on robust datasets.