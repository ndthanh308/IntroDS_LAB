\section{Method}

In this section, we present the architecture of the proposed robust VG pipeline and its components. Fig.~\ref{fig:maingraph} illustrates the pipeline.
In this section, we present the architecture of the proposed robust VG pipeline and its components. Fig.~\ref{fig:maingraph} illustrates the pipeline, where the image and corresponding language description are processed separately to obtain different feature embeddings in two distinct branches.
% Figure environment removed
% \vspace{-0.2cm}
\subsection{Masked Reference based Centerpoint Supervision}

\par \textbf{Motivation.} Existing VG approaches suffer from inadequate visual-linguistic feature representation, insufficient fine-grained feature representation, and poor localization capacity, leading to the detection of irrelevant or inaccurate objects. To address these issues, we propose the masked reference based centerpoint supervision (MRCS) approach, as illustrated in Fig.~\ref{fig:maingraph}. MRCS comprises three parts: masked reference augmentation, visual-linguistic alignment, and centerpoint supervision. This approach aims to enhance context understanding from the whole image and improve the accuracy of object detection in VG tasks.
\par \noindent \textbf{Masked reference augmentation.} As illustrated in the down-left part of Fig.~\ref{fig:maingraph}, we propose a text augmentation approach to generate diversified textual information given an input language expression. We employ the NLTK~\cite{nltk} tokenization strategy to extract lexical properties for each word, followed by masking one word in the text according to the well-designed rules (shown in the supplementary materials). This masking process is repeated at most three times, achieving one full text and three masked texts in total. BERT~\cite{Bert} is then utilized to generate different textual embeddings for these sentences.
\par It is important to note that we prioritize the masking of lexical words differently based on their semantic significance. Prepositions, conjunctions, and qualifiers are masked first, as they generally have minimal impact on the sentence's meaning. If these types of words are absent, the module masks auxiliaries, pronouns, and numbers, which can partially affect the sentence's semantics. Finally, the module masks adjectives and verbs, which are critical for the sentence's meaning. If there is only one non-noun word or only nouns remaining in the sentence, no further masking is performed. More specific rules will be shown in the supplementary materials.
\par \noindent \textbf{Visual-linguistic alignment.} The proposed model, illustrated in Fig.~\ref{fig:KP_Reg}, incorporates a visual-linguistic alignment module with two consecutive MHA layers. The visual feature map $F_{v}$ is input as the {\it Query}, and the textual embeddings are input as {\it Key} and {\it Value} to the first MHA. This process produces an enhanced feature map that gathers relevant semantic information from the corresponding linguistic representation. Subsequently, the enhanced feature map undergoes another MHA operation that performs self-attention on the visual features to encode the involved visual contexts. The features from the two MHAs are element-wisely summed in a residual manner for the centerpoint supervision component.
The goal of these two MHA operations is to encode the related descriptions into the visual feature and enhance the visual context information from the whole image. The features from the two MHAs are element-wisely summed in a residual manner for the centerpoint supervision component.
As shown in Fig.~\ref{fig:KP_Reg}, the textual embeddings from the language branch and the visual feature map from the image branch will be input to the visual-linguistic alignment module based on two consecutive multi-head attention (MHA) layers. Specifically, we input the visual feature map $F_{v}$ as the {\it Query}, and textual embeddings as {\it Key} and {\it Value} into the first MHA layer, where enhanced feature map will be achieved by collecting the relevant semantic information from the corresponding linguistic representation. The enhanced feature will then again be processed through another MHA operator that performs self-attention for the visual features to encode the involved visual contexts. The two consecutive MHA operations try to encode the related descriptions into the visual feature and enhance the visual context information from the whole image. The features from the two MHAs will be element-wisely summed together in a residual manner, which will be employed in the keypoint supervision part.
% Figure environment removed
% Figure environment removed
% Figure environment removed


\noindent \textbf{Centerpoint supervision.} To obtain the final centerpoint heatmaps, the summed feature map obtained from each language expression is processed through two consecutive convolutional layers. Multiple centerpoint heatmaps (one from the full text and three from the masked text) are then fused by performing a {\it maxpooling} process, with the centerpoint coordinates determined by performing a {\it argmax} operation on the resulting heatmap. The cross entropy loss is then utilized as the supervision loss between the centerpoint heatmap and the corresponding ground truth, given by $\mathcal{L}_\text{key} = \text{CELoss}(y, \hat{y}_{i})$, where $\text{CELoss}(\cdot, \cdot)$ is the cross entropy loss, $\hat{y}_{i}$ is the predicted centerpoints, and $y$ is the centerpoint ground truth that is obtained from the center point of each ground truth box. 

\subsection{Iterative Multi-level Vision-language Fusion} 

\noindent
\textbf{Motivation.} Through empirical analysis, we have observed that the visual-textual misunderstanding issue arises due to inadequate and poor textual embeddings in the multi-head vision-language fusion module, which incorporates different visual features from various stages. To address this challenge, we propose a multi-level textual feature enhancement (MTFE) module that enhances textual embeddings from low-level to high-level, analogous to the image feature extraction branch. The module extracts multi-level textual information from the entire sentence, resulting in more comprehensive and robust textual embeddings. 
\par \noindent \textbf{Multi-level textual feature enhancement.} The MTFE module improves textual embedding representation by performing two consecutive fully-connected layers with 768 nodes in each stage. Specifically, as highlighted with yellow color in Fig.~\ref{fig:maingraph}, the IMVF comprises four stages, and each stage contains an MTFE module. The MTFE module consists of two fully connected layers and a corresponding dropout layer with a 0.1 ratio, aimed at obtaining multi-level textual features that match the multi-level visual features. This enables the model to focus on different key descriptions in the referring expressions and obtain more complete and reliable features for the referred object.
\par \noindent \textbf{Iterative multi-level vision-language fusion.} Fig.~\ref{fig:V-L_Fusion} illustrates the IMVF module, which is based on MHA and consists of four iterative stages. Each stage includes two MHA layers. The first layer uses the visual feature map $F_{v} \in \mathcal{R}^{C\times H \times W}$ as the {\it Query} and the textual embeddings $F_{l} \in \mathcal{R}^{C \times L}$ from the multi-level textual feature enhancement module as the {\it Key} and {\it Value}. Multi-head cross-attention enables the comprehensive incorporation of textual information into the visual feature map $F_{g} \in \mathcal{R}^{C \times H \times W}$. In the second layer, $F_{g}$ serves as both the {\it Query} and {\it Key}, while $F_{v}$ serves as the {\it Value}. This self-attention operator allows the model to gather crucial context features for the referred object based on the textual descriptions provided, and the final feature is $F_{c} \in \mathcal{R}^{C \times H \times W}$. We sum the $F_{v}$, $F_{g}$, and $F_{c}$ element-wisely to obtain the final visual feature map $F_{m}$. In each iteration, the $i$-th visual feature map $F^{i}_{m}$ becomes the initial feature map (i.e., $F^{i+1}_{v}$). Our experiments include four iterations, and we use element-wise {\it max} strategy to obtain the final fusion feature $F=\text{max}(F^{1}_{m}, F^{2}_{m}, F^{3}_{m}, F^{4}_{m})$. Actually, other fusion strategies can also be considered. We experimentally find that element-wise summation or product achieves inferior performance than the proposed strategy.

\subsection{Multi-stage False-alarm Sensitive Decoder}


\noindent
\textbf{Motivation.} The current SOTA approaches in VG task assume that the language expressions are precisely matched with the visual image. However, this assumption may not hold in practical applications. Specifically, when an inaccurate or irrelevant text expression is provided, the existing SOTA VG approaches~\cite{TransVG,vltvg} often generate false-alarm results. To address this issue, we introduce several \textbf{robust} VG datasets (described in Sec. 4) and propose a new multi-stage false-alarm sensitive decoder (MFSD) module.
\par \noindent \textbf{Multi-stage false-alarm sensitive decoder.} As shown in Fig.~\ref{fig:falsealarm}, the MFSD module consists of several iterative stages, each contains two consecutive multi-head attention (MHA)~\cite{MHA} layers. In the first stage, we randomly initialize a series of learnable queries. To handle the false-alarm case, we introduce a random embedding with the same size as textual embedding from the IMVF module. We concatenate the textual embedding and the random embedding in the batch dimension, termed as {\it mixture embedding}. For the first MHA layer, the learnable queries serves as {\it Query}, and the mixture embedding acts as {\it Key} and {\it Value}. With this layer, the textual embedding can be more easily attended to the target tokens, thus achieving enhanced textual embedding. For the second MHA layer, the enhanced textual embedding is treated as {\it Query}, and the visual-linguistic feature map from the IMVF module as well as the visual feature map $F_v$ are employed as {\it Key} and {\it Value}. Through the second MHA layer, the textual information can be comprehensively fused with the visual feature map to achieve an enhanced vision-language feature, which is then taken into a feed-forward network (FFN). We fuse the enhanced vision-language feature and the feature from the second MHA in a residual manner, termed as R\_feature, which serves as the {\it Query} in the next iteration. Then, R\_feature is taken into two decoupled heads: one for classification to indicate whether there exists false-alarm result, and another for regression to generate the predicted bounding boxes (bbox). 
Specifically the classification loss $\mathcal{L}_{\text{cls}}$ and the regression loss $\mathcal{L}_{\text{reg}}$ are defined as,
\begin{equation}
    \mathcal{L}_{\text{cls}} = \sum_{t=1}^{N}\sum_{i=1}^{K}\text{CELoss}(y^{t}, \hat{y}^{t}_{i}),
\end{equation}
\begin{equation}
     \mathcal{L}_{\text{reg}} = \sum_{t=1}^{N}\sum_{i=1}^{K}\lambda_\text{GIOU}\mathcal{L}_{\text{GIOU}}(b^{t},\hat{b}^{t}_{i}) + \lambda_{\text{L1}}\mathcal{L}_{\text{L1}}(b^{t},\hat{b}^{t}_{i}),
\end{equation}
where $\text{CELoss}(\cdot, \cdot)$, $\mathcal{L}_{\text{GIOU}}(\cdot, \cdot)$ and $\mathcal{L}_{\text{L1}}(\cdot, \cdot)$ are the cross entropy loss, GIOU loss~\cite{GIOU} and L1 loss, respectively. $y^{t}$ and $\hat{y}^{t}_{i}$ denote the ground truth label and predicted result in $t$-th iteration. Similarly, $b^{t}$ and $\hat{b}^{t}_{i}$ denote the ground truth bbox and predicted bbox. $t$ denotes the $t$-th iteration, and $i$ represents the $i$-th bbox. $\lambda_{\text{GIOU}}$ and $\lambda_{\text{L1}}$ are empirically adjusted, here we set them as 3 and 7 by default for all the following experiments.

\subsection{Details of Determining False-alarm Detection of the Previous Methods}

In the previous methods, we follow the same rules as ours to obtain the false alarm. Firstly, we achieve the top1 scoring box as the final prediction box. Then, we calculate the IOU value with the ground truth box. If the IOU value is greater than 0.5, we consider it a true positive, otherwise, we treat it as a false positive. However, the proposed method differs in that it combines the top1 scoring box and its existing result (exist or non-exist) to achieve the final prediction box. During our experiments, we attempted to add an irrelevant text reference head to some previous networks, such as VLTVG~\cite{vltvg} but the results were inferior to their baselines. It may not be fair to compare these results in the paper, thus we do not show these results.

\subsection{Training Loss}

\par In the training stage, the proposed VG framework is trained end-to-end using the aforementioned losses. The overall loss function for the proposed framework is $\mathcal{L} = \mathcal{L}_{\text{cls}} + \lambda_{\text{reg}}\mathcal{L}_{\text{reg}} + \lambda_{\text{key}}\mathcal{L}_{\text{key}}$ as follows, where $\mathcal{L}_{\text{cls}}$, $\mathcal{L}_{\text{reg}}$, and $\mathcal{L}_{\text{key}}$ denote the classification loss, regression loss and centerpoint loss, respectively. $\lambda_{\text{reg}}$ and $\lambda_{\text{key}}$ are introduced to balance the above losses.We empirically set $\lambda_{\text{reg}}$ and $\lambda_{\text{key}}$ as 2 and 5 by default. 
\begin{equation}
    \mathcal{L} = \mathcal{L}_{\text{cls}} + \lambda_{\text{reg}}\mathcal{L}_{\text{reg}} + \lambda_{\text{key}}\mathcal{L}_{\text{key}},
\end{equation}
