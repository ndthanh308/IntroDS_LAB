{
  "title": "Open-Set Domain Adaptation with Visual-Language Foundation Models",
  "authors": [
    "Qing Yu",
    "Go Irie",
    "Kiyoharu Aizawa"
  ],
  "submission_date": "2023-07-30T11:38:46+00:00",
  "revised_dates": [],
  "abstract": "Unsupervised domain adaptation (UDA) has proven to be very effective in transferring knowledge obtained from a source domain with labeled data to a target domain with unlabeled data. Owing to the lack of labeled data in the target domain and the possible presence of unknown classes, open-set domain adaptation (ODA) has emerged as a potential solution to identify these classes during the training phase. Although existing ODA approaches aim to solve the distribution shifts between the source and target domains, most methods fine-tuned ImageNet pre-trained models on the source domain with the adaptation on the target domain. Recent visual-language foundation models (VLFM), such as Contrastive Language-Image Pre-Training (CLIP), are robust to many distribution shifts and, therefore, should substantially improve the performance of ODA. In this work, we explore generic ways to adopt CLIP, a popular VLFM, for ODA. We investigate the performance of zero-shot prediction using CLIP, and then propose an entropy optimization strategy to assist the ODA models with the outputs of CLIP. The proposed approach achieves state-of-the-art results on various benchmarks, demonstrating its effectiveness in addressing the ODA problem.",
  "categories": [
    "cs.CV"
  ],
  "primary_category": "cs.CV",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.16204",
  "pdf_url": "https://arxiv.org/pdf/2307.16204v1",
  "comment": null,
  "num_versions": null,
  "size_before_bytes": 2236580,
  "size_after_bytes": 121983
}