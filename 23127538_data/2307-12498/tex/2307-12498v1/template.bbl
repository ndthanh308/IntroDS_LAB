% Generated by IEEEtran.bst, version: 1.13 (2008/09/30)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{baevski2020wav2vec}
A.~Baevski, Y.~Zhou, A.~Mohamed, and M.~Auli, ``wav2vec 2.0: A framework for
  self-supervised learning of speech representations,'' \emph{Advances in
  neural information processing systems}, vol.~33, pp. 12\,449--12\,460, 2020.

\bibitem{parada2022pmct}
P.~P. Parada, A.~Dobrowolska, K.~Saravanan, and M.~Ozay, ``pmct: Patched
  multi-condition training for robust speech recognition,'' \emph{arXiv
  preprint arXiv:2207.04949}, 2022.

\bibitem{hu2022dual}
Y.~Hu, N.~Hou, C.~Chen, and E.~S. Chng, ``Dual-path style learning for
  end-to-end noise-robust speech recognition,'' \emph{arXiv preprint
  arXiv:2203.14838}, 2022.

\bibitem{fan2022draft}
R.~Fan and A.~Alwan, ``Draft: A novel framework to reduce domain shifting in
  self-supervised learning and its application to children's asr,'' \emph{arXiv
  preprint arXiv:2206.07931}, 2022.

\bibitem{hu2023gradient}
Y.~Hu, C.~Chen, R.~Li, Q.~Zhu, and E.~S. Chng, ``Gradient remedy for multi-task
  learning in end-to-end noise-robust speech recognition,'' \emph{arXiv
  preprint arXiv:2302.11362}, 2023.

\bibitem{gajic2006robust}
B.~Gajic and K.~K. Paliwal, ``Robust speech recognition in noisy environments
  based on subband spectral centroid histograms,'' \emph{IEEE Transactions on
  Audio, Speech, and Language Processing}, vol.~14, no.~2, pp. 600--608, 2006.

\bibitem{ko2017study}
T.~Ko, V.~Peddinti, D.~Povey, M.~L. Seltzer, and S.~Khudanpur, ``A study on
  data augmentation of reverberant speech for robust speech recognition,'' in
  \emph{2017 IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2017, pp.
  5220--5224.

\bibitem{jaitly2013vocal}
N.~Jaitly and G.~E. Hinton, ``Vocal tract length perturbation (vtlp) improves
  speech recognition,'' in \emph{Proc. ICML Workshop on Deep Learning for
  Audio, Speech and Language}, vol. 117, 2013, p.~21.

\bibitem{peddinti2015time}
V.~Peddinti, D.~Povey, and S.~Khudanpur, ``A time delay neural network
  architecture for efficient modeling of long temporal contexts,'' in
  \emph{Sixteenth annual conference of the international speech communication
  association}, 2015.

\bibitem{park2019specaugment}
D.~S. Park, W.~Chan, Y.~Zhang, C.-C. Chiu, B.~Zoph, E.~D. Cubuk, and Q.~V. Le,
  ``Specaugment: A simple data augmentation method for automatic speech
  recognition,'' \emph{arXiv preprint arXiv:1904.08779}, 2019.

\bibitem{ko2015audio}
T.~Ko, V.~Peddinti, D.~Povey, and S.~Khudanpur, ``Audio augmentation for speech
  recognition,'' in \emph{Sixteenth annual conference of the international
  speech communication association}, 2015.

\bibitem{tan2020improving}
K.~Tan and D.~Wang, ``Improving robustness of deep learning based monaural
  speech enhancement against processing artifacts,'' in \emph{ICASSP 2020-2020
  IEEE International Conference on Acoustics, Speech and Signal Processing
  (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2020, pp. 6914--6918.

\bibitem{zhang2019theoretically}
H.~Zhang, Y.~Yu, J.~Jiao, E.~Xing, L.~El~Ghaoui, and M.~Jordan, ``Theoretically
  principled trade-off between robustness and accuracy,'' in
  \emph{International conference on machine learning}.\hskip 1em plus 0.5em
  minus 0.4em\relax PMLR, 2019, pp. 7472--7482.

\bibitem{ivgi2021achieving}
M.~Ivgi and J.~Berant, ``Achieving model robustness through discrete
  adversarial training,'' \emph{arXiv preprint arXiv:2104.05062}, 2021.

\bibitem{mao2022enhance}
X.~Mao, Y.~Chen, R.~Duan, Y.~Zhu, G.~Qi, S.~Ye, X.~Li, R.~Zhang, and H.~Xue,
  ``Enhance the visual representation via discrete adversarial training,''
  \emph{arXiv preprint arXiv:2209.07735}, 2022.

\bibitem{kharitonov2021data}
E.~Kharitonov, M.~Rivi{\`e}re, G.~Synnaeve, L.~Wolf, P.-E. Mazar{\'e},
  M.~Douze, and E.~Dupoux, ``Data augmenting contrastive learning of speech
  representations in the time domain,'' in \emph{2021 IEEE Spoken Language
  Technology Workshop (SLT)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE,
  2021, pp. 215--222.

\bibitem{zhang2022speechlm}
Z.~Zhang, S.~Chen, L.~Zhou, Y.~Wu, S.~Ren, S.~Liu, Z.~Yao, X.~Gong, L.~Dai,
  J.~Li \emph{et~al.}, ``Speechlm: Enhanced speech pre-training with unpaired
  textual data,'' \emph{arXiv preprint arXiv:2209.15329}, 2022.

\bibitem{gandhi2022esb}
S.~Gandhi, P.~Von~Platen, and A.~M. Rush, ``Esb: A benchmark for multi-domain
  end-to-end speech recognition,'' \emph{arXiv preprint arXiv:2210.13352},
  2022.

\bibitem{damania2022combining}
R.~Damania, C.~Homan, and E.~Prudâ€™hommeaux, ``Combining simple but novel data
  augmentation methods for improving low-resource asr,'' 2022.

\bibitem{sun2018training}
S.~Sun, C.-F. Yeh, M.~Ostendorf, M.-Y. Hwang, and L.~Xie, ``Training
  augmentation with adversarial examples for robust speech recognition,''
  \emph{arXiv preprint arXiv:1806.02782}, 2018.

\bibitem{yang2020characterizing}
C.-H. Yang, J.~Qi, P.-Y. Chen, X.~Ma, and C.-H. Lee, ``Characterizing speech
  adversarial examples using self-attention u-net enhancement,'' in
  \emph{ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE,
  2020, pp. 3107--3111.

\bibitem{defossez2020real}
A.~Defossez, G.~Synnaeve, and Y.~Adi, ``Real time speech enhancement in the
  waveform domain,'' \emph{arXiv preprint arXiv:2006.12847}, 2020.

\bibitem{madry2017towards}
A.~Madry, A.~Makelov, L.~Schmidt, D.~Tsipras, and A.~Vladu, ``Towards deep
  learning models resistant to adversarial attacks,'' \emph{arXiv preprint
  arXiv:1706.06083}, 2017.

\bibitem{goodfellow2014explaining}
I.~J. Goodfellow, J.~Shlens, and C.~Szegedy, ``Explaining and harnessing
  adversarial examples,'' \emph{arXiv preprint arXiv:1412.6572}, 2014.

\bibitem{diaz2021gpurir}
D.~Diaz-Guerra, A.~Miguel, and J.~R. Beltran, ``gpurir: A python library for
  room impulse response simulation with gpu acceleration,'' \emph{Multimedia
  Tools and Applications}, vol.~80, pp. 5653--5671, 2021.

\bibitem{panayotov2015librispeech}
V.~Panayotov, G.~Chen, D.~Povey, and S.~Khudanpur, ``Librispeech: an asr corpus
  based on public domain audio books,'' in \emph{2015 IEEE international
  conference on acoustics, speech and signal processing (ICASSP)}.\hskip 1em
  plus 0.5em minus 0.4em\relax IEEE, 2015, pp. 5206--5210.

\bibitem{ardila2019common}
R.~Ardila, M.~Branson, K.~Davis, M.~Henretty, M.~Kohler, J.~Meyer, R.~Morais,
  L.~Saunders, F.~M. Tyers, and G.~Weber, ``Common voice: A
  massively-multilingual speech corpus,'' \emph{arXiv preprint
  arXiv:1912.06670}, 2019.

\bibitem{wang2021voxpopuli}
C.~Wang, M.~Riviere, A.~Lee, A.~Wu, C.~Talnikar, D.~Haziza, M.~Williamson,
  J.~Pino, and E.~Dupoux, ``Voxpopuli: A large-scale multilingual speech corpus
  for representation learning, semi-supervised learning and interpretation,''
  \emph{arXiv preprint arXiv:2101.00390}, 2021.

\bibitem{rousseau2012ted}
A.~Rousseau, P.~Del{\'e}glise, and Y.~Esteve, ``Ted-lium: an automatic speech
  recognition dedicated corpus.'' in \emph{LREC}, 2012, pp. 125--129.

\bibitem{carletta2007unleashing}
J.~Carletta, ``Unleashing the killer corpus: experiences in creating the
  multi-everything ami meeting corpus,'' \emph{Language Resources and
  Evaluation}, vol.~41, pp. 181--190, 2007.

\bibitem{chen2021gigaspeech}
G.~Chen, S.~Chai, G.~Wang, J.~Du, W.-Q. Zhang, C.~Weng, D.~Su, D.~Povey,
  J.~Trmal, J.~Zhang \emph{et~al.}, ``Gigaspeech: An evolving, multi-domain asr
  corpus with 10,000 hours of transcribed audio,'' \emph{arXiv preprint
  arXiv:2106.06909}, 2021.

\bibitem{o2021spgispeech}
P.~K. O'Neill, V.~Lavrukhin, S.~Majumdar, V.~Noroozi, Y.~Zhang, O.~Kuchaiev,
  J.~Balam, Y.~Dovzhenko, K.~Freyberg, M.~D. Shulman \emph{et~al.},
  ``Spgispeech: 5,000 hours of transcribed financial audio for fully formatted
  end-to-end speech recognition,'' \emph{arXiv preprint arXiv:2104.02014},
  2021.

\bibitem{del2022earnings}
M.~Del~Rio, P.~Ha, Q.~McNamara, C.~Miller, and S.~Chandra, ``Earnings-22: A
  practical benchmark for accents in the wild,'' \emph{arXiv preprint
  arXiv:2203.15591}, 2022.

\bibitem{vincent2017analysis}
E.~Vincent, S.~Watanabe, A.~A. Nugraha, J.~Barker, and R.~Marxer, ``An analysis
  of environment, microphone and data simulation mismatches in robust speech
  recognition,'' \emph{Computer Speech \& Language}, vol.~46, pp. 535--557,
  2017.

\bibitem{snyder2015musan}
D.~Snyder, G.~Chen, and D.~Povey, ``Musan: A music, speech, and noise corpus,''
  \emph{arXiv preprint arXiv:1510.08484}, 2015.

\bibitem{kireev2022effectiveness}
K.~Kireev, M.~Andriushchenko, and N.~Flammarion, ``On the effectiveness of
  adversarial training against common corruptions,'' in \emph{Uncertainty in
  Artificial Intelligence}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2022,
  pp. 1012--1021.

\end{thebibliography}
