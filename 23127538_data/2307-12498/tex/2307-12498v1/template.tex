\documentclass{INTERSPEECH2023}
\usepackage{amsmath, amssymb}
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage{booktabs}
\usepackage{bm}
\usepackage{bigstrut}
\urlstyle{same}

\usepackage{amsmath}
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{hyperref}
\usepackage{wrapfig}

\usepackage{algorithmic}
\usepackage[ruled]{algorithm2e}
\renewcommand{\algorithmiccomment}[1]{//#1}
\SetKwInput{KwInput}{Input}
\SetKwInput{KwOutput}{Output}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\usepackage{indentfirst}

\usepackage{booktabs}  
\usepackage{threeparttable}  
\usepackage{multicol}  
\usepackage{multirow}  
\usepackage{xspace}
\usepackage{colortbl}
% \usepackage{ulem}

\def\etal{\emph{et al.}}
\def\eg{\emph{e.g.}}
\def\ie{\emph{i.e.}}

\newcommand{\wapat}{\textsc{Wapat}\xspace}
\newcommand{\yf}[1]{{\color{blue}{[YF: #1]}}}
\newcommand{\mxf}[1]{{\color{red}{[Xiaofeng: #1]}}}
\newcommand{\delete}[1]{{\color{green}{[DELETE: #1]}}}


\setlength{\parskip}{1ex plus 0.5ex minus 0.2ex}

% 2023-01-06 modified by Simon King (Simon.King@ed.ac.uk)  

% **************************************
% *    DOUBLE-BLIND REVIEW SETTINGS    *
% **************************************
% Comment out \interspeechcameraready when submitting the 
% paper for review.
% If your paper is accepted, uncomment this to produce the
%  'camera ready' version to submit for publication.
\interspeechcameraready 


% **************************************
% *                                    *
% *      STOP !   DO NOT DELETE !      *
% *          READ THIS FIRST           *
% *                                    *
% * This template also includes        *
% * important INSTRUCTIONS that you    *
% * must follow when preparing your    *
% * paper. Read it BEFORE replacing    *
% * the content with your own work.    *
% **************************************

\title{Robust Automatic Speech Recognition via WavAugment Guided Phoneme Adversarial Training}
\name{Gege Qi$^1$, Yuefeng Chen$^1$, Xiaofeng Mao$^1$, Xiaojun Jia$^2$, Ranjie Duan$^1$, Rong Zhang$^1$, Hui Xue$^1$}
%The maximum number of authors in the author list is 20. If the number of contributing authors is more than this, they should be listed in a footnote or the acknowledgement section.
\address{
  $^1$Alibaba Group, China\\
  $^2$Chinese Academy of Sciences, China}
\email{\{qigege.qgg,yuefeng.chenyf,mxf164419\}@alibaba-inc.com}
\begin{document}

% \linenumbers
\maketitle

\begin{abstract}
% 1000 characters. ASCII characters only. No citations.
Developing a practically-robust automatic speech recognition (ASR) is challenging since the model should not only maintain the original performance on clean samples, but also achieve consistent efficacy under small volume perturbations and large domain shifts. To address this problem, we propose a novel WavAugment Guided Phoneme Adversarial Training (\wapat).
\wapat use adversarial examples in phoneme space as augmentation to make the model invariant to minor fluctuations in phoneme representation and preserve the performance on clean samples.
% In order to make the model invariant to small fluctuations in phoneme representation for better generalization, we augments adversarial examples in phoneme space during ASR training.
In addition, \wapat utilizes the phoneme representation of augmented samples to guide the generation of adversaries, which helps to find more stable and diverse gradient-directions, resulting in improved generalization.
Extensive experiments demonstrate the effectiveness of \wapat on End-to-end Speech Challenge Benchmark (ESB).
Notably, SpeechLM-\wapat outperforms the original model by 6.28\% WER reduction on ESB, achieving the new state-of-the-art.

%that is able to improve generalization under cross-domain scenarios.
%By conducting the AT on symbolic phoneme space, \wapat can improve the robustness without sacrificing standard performance.
% AWA can improve performance across multiple domain datasets simultaneously.
% Leveraging five time-domain augmentations as guidance, AWA can achieve state-of-the-art results on in-domain and out-of-domain datasets simultaneously.
% However, the DA technique needs lots of knowledge of human expert, and sometimes shows the weak transferability across different datasets. 

% of augmented inputs to guide the generation of adversaries, thus without sacrificing
% By finding more stable and diverse gradient-directions for generating adversaries, \wapat can learn more robust features, thereby resulting in improving the generalization of ASRs.
% gets a 6.28\% relative WER reduction on a challenging End-to-end Speech Challenge (ESB) benchmark.
\end{abstract}
\noindent\textbf{Index Terms}: robust automatic speech recognition, data augmentation, adversarial training 
% For enhancing the robustness with no sacrificing of performance on in-domain data, the clean and adversaries are both introduced in training.

\section{Introduction}

Nowadays, there have been remarkable advancements in Deep Neural Network (DNN) based Automatic Speech Recognition (ASR)~\cite{baevski2020wav2vec}, resulting in the emergence of numerous speech-related applications that assist humans in their daily activities. 
However, despite the impressive performance of ASR systems, they are limited to specific tasks since they assume that the training and testing data are drawn from the same distribution~\cite{parada2022pmct}.
Thus, applying ASR in real-world applications under diverse environment is still a huge challenge ~\cite{hu2022dual,fan2022draft,hu2023gradient}.

In this work, we aim to address such a challenging cross-domain scenario where an ASR system needs to be robust against various potential distortion. However, there are two major challenges:
  1) \textbf{Robustness against perturbation:} Real-world volume perturbation (\eg, environmental noise, reverberation, and background speakers) significantly impacts the performance of an ASR system~\cite{gajic2006robust,ko2017study}.
  2) \textbf{Robustness Generalization:} There exist various type of volume perturbation in practical scenario. However, a ASR is robust against one type of perturbation not promised being robust under unknown domain (\eg, change of speaking style). 
Existing work either adopt data augmentation to improve ASR's robustness against specific perturbation but limited under unseen domain \cite{jaitly2013vocal, peddinti2015time, park2019specaugment, ko2015audio}, or use speech enhancement as a pre-processing to deal with various potential noise \cite{tan2020improving}. Both of them fail to achieve a real-robust ASR system which can be applied in the real world. Therefore, enhancing the robustness of ASRs while improving their robustness generalization across different perturbation remains a significant challenge.

To address the challenge, we propose a novel method called \textbf{W}av\textbf{A}ugment Guided \textbf{P}honeme \textbf{A}dversarial \textbf{T}raining (\wapat) by leveraging adversarial training (AT) technique. Though previous work \cite{zhang2019theoretically} claimed AT results in a trade-off between robustness and clean accuracy. However, research in the field of natural language processing~\cite{ivgi2021achieving} and recent in computer vision~\cite{mao2022enhance} demonstrated that aligning the distributions of adversarial and original samples during AT can benefit robustness and clean accuracy simultaneously. Borrowing the idea, we propose applying AT on phoneme space to create adversarial speech with realistic semantic. To be specific, \wapat employs a single-step attack to generate adversarial perturbations on phoneme representations. A data augmentation is further applied to guide the attack for generating more stable and diverse phoneme adversarial examples.
In detail, with the time-domain WavAugment~\cite{kharitonov2021data} technique, we use Kullback-Leibler Divergence (KLD) to align the adversaries on original samples and those on augmented samples. Furthermore, multiple augmentations in WavAugment are used to guide the adversarial training for searching for diverse gradient directions and leading to better generalization.
Figure~\ref{fig:3_1} shows the overall pipeline of WavAugment guided phoneme adversarial training.

% Figure environment removed

We further explore the impact of individual techniques in WavAugment and their combinations with \wapat on the performance of the model.
Our findings indicate that while hard augmentations can improve robustness on some datasets, they fails on others.
It is reasonable to expect challenges in generalizing audio augmentations across different domains, given the inherent complexity of audio signals.
Instead, our \wapat consistently improves performance in terms of both cross-domain datasets and different types of transformations.
The stability of generalization indicates that the WavAugment-guided adversary is effective in inducing robust features into target ASR.

In summary, we make the following contributions:
\begin{itemize}
    \item To our knowledge, this is the first work that sheds light on adversarial training on phoneme-unit space for improving standard performance and generalization simultaneously.
    \item We propose WavAugment Guided Phoneme Adversarial Training (\wapat), which employs phoneme representation of the augmented audios to guide the generation of adversaries, resulting in more diverse robust features.
    \item By combining SpeechLM~\cite{zhang2022speechlm} pre-training and \wapat fine-tuning, our method achieves new state-of-the-art performance on the challenging benchmark ESB~\cite{gandhi2022esb}, which contains multiple speech datasets from a broad range of domains.
\end{itemize}


% Recent research in the fields of Natural Language Processing~\cite{ivgi2021achieving} and Computer Vision~\cite{mao2022enhance} has demonstrated that aligning the distributions of adversarial and original samples in AT can benefit standard performance and robustness simultaneously. Building upon this idea, we propose to apply a phoneme unit AT for ASR tasks, where adversarial speech is commonly encountered.
% For example, speakers may alter the pronunciation of a phoneme for easier production.
% Regarding the second problem, we believe that improving the stability and diversity of adversarial samples in AT can further enhance the generalization of ASRs.

% As for the second problem, there are two critical properties, \ie, stability and diversity, along which we can improve the transferability of adversarial examples.

% Such transferable adversarial examples are expected to introduce more robust features into target ASRs, thus improving the generalization capability.

%To this end, we propose a new type of adversarial training called \textbf{W}av\textbf{A}ugment Guided \textbf{P}honeme \textbf{A}dversarial \textbf{T}raining (\wapat).
%\wapat employs a single-step attack to generate adversarial perturbations on phoneme representations. To enhance the stability and diversity of phoneme adversarial examples, we further design a data augmentation guided adversarial attack.
% Specifically, with the time-domain WavAugment~\cite{kharitonov2021data} technique, we use Kullback-Leibler Divergence (KLD) to measure the similarity between the adversaries of the original samples and those of the augmented samples. Then, we constrain them to be as close as possible, thereby improving the stability of adversarial examples.
% Furthermore, multiple augmentations in WavAugment are used to guide phoneme adversarial training, encouraging the search for diverse gradient directions and leading to better generalization.
% Figure~\ref{fig:3_1} shows the overall pipeline of WavAugment guided phoneme adversarial training.

% Surprisingly, some recent works in Natural Language Processing (NLP)~\cite{ivgi2021achieving} and Computer Vision (CV)~\cite{mao2022enhance} found that the AT can benefit for standard performance and robustness simultaneously. By automatically finding adversarial textual inputs, AT will not hurt the accuracy and even benefit for both generalization and robustness of language models~\cite{ivgi2021achieving}. DAT~\cite{mao2022enhance} trains the model on example which has the adversarially altered discrete visual representation. 

% Data augmentation is an effective technique to increase overall model robustness in standard training~\cite{rebai2017improving,park2019specaugment}. However, there has been no uncontroversial evidence showing that generalization of AT can be further improved by it.
%For reducing the robust generalization gap between the training data and the testing data, we consider to increase the diversity of generated adversaries by symbolic data augmentation. 
% On top of that, we propose a constraint called distribution alignment which could stabilize the training of \wapat. 
% Finally, the phoneme adversarial representations and clean inputs are fed into models for training. 
% Such ability of generating adversarial examples in phoneme representation space makes \wapat can improve the generalization without sacrificing the performance on in-domain datasets. 

% Data augmentation (DA) has been observed by many previous works to be effective in increasing overall model robustness~\cite{rebai2017improving,park2019specaugment}.
% Conventionally, augmented samples retain their original label, so that models effectively see a larger set of data-label training pairs.

%However, it is hard to carefully select the appropriate DA techniques based on the characteristics of the dataset and the task at hand to achieve optimal results. 
% The speech data augmentation techniques are often too narrow in scope, making it difficult to generalize them to a wide variety of cross-domain datasets. For example, adding noise or reverberation may failed to improve the robustness of ASRs trained on datasets with varying speaking rates.
% On the other hand, combining multiple data augmentation techniques makes the speech too different from the original, resulting in the loss of inherent features.

% We noticed that, as another family of augmentation schemes, Adversarial Training (AT)~\cite{sun2018training,na2021accented} automatically finds failure input cases of DNNs and augment online with these cases for enhancing the adversarial robustness.
% To some extend, the generated adversarial examples in AT have the characteristics that we expected. However, there is no undisputed evidence that adversarial robustness is helpful for generalization in the speech recognition area.

% In-so-doing, we first investigate the effects of DA and AT on the accuracy and generalization of ASRs.
% It is found that, in general, hard augmentation can alleviate overfitting and improve robustness, but in some cases it fails.
% We also observe that combining multiple DA techniques can often lead to even better results.
% On the other hand, traditional AT generally can alleviate overfitting, but at the expense of clean accuracy.
% Motivated by above observations, we propose Adversarial WavAugment (AWA) method based on the time-domain augmentations\cite{kharitonov2021data}, which utilizes phoneme representation of the augmented audios to guide the generation of adversaries, thereby increasing the hardness and the diversity of DAs. By combining both clean and AWA examples during training, the target ASRs can improve the robustness without sacrificing the performance on in-domain datasets.

%Another family of augmentation schemes called adversarial training has attracted active research interest~\cite{sun2018training,na2021accented} This training regime attempts to solve the minimax optimization problem of firstly generating strong adversarial samples by maximizing a loss, and subsequently training the model to minimize loss on these adversarial samples. Data augmentation is an effective technique to alleviate overfitting in standard training~\cite{ramirez2019survey}, but there has been no uncontroversial evidence showing that robust generalization of AT can be further improved by it. 


\section{Related Work}
Existing approaches for improving ASR generally from two aspects: \textbf{1) Improving ASR's robustness against specific perturbation:} Early works have shown that several data augmentation methods, such as vocal tract length perturbation~\cite{jaitly2013vocal}, volume perturbation~\cite{peddinti2015time} and speed perturbation~\cite{ko2015audio}, can improve the robustness of ASR models.
SpecAugment~\cite{park2019specaugment} is widely used to train ASR models due to its efficiency.
Specifically, SpecAugment randomly masks chunks of time or frequency channels on spectrograms.
However, these DA techniques are typically designed manually for specific domains based on domain-specific knowledge and experience.
When dealing with an unknown target domain or multiple domains, it can be challenging for experts to apply specific transformations, or to construct and fine-tune more sophisticated augmentation compositions~\cite{damania2022combining}.
Besides data augmentation, several work utilize adversarial training \cite{sun2018training, yang2020characterizing} aiming to improve ASR's adversarial robustness under adversarial examples. However, all of these work target ASR's robustness under specific perturbation, and the improved ASR is still limited on unseen domain.
\textbf{2) Improving ASR's performance via pre-processing:} To achive better performance, there are also several works propose using speech enhancement methods to remove noise from speech signals before passing them through a standard ASR~\cite{tan2020improving}. A more recent approach that operates on raw waveform for real-time speech enhancement is~\cite{defossez2020real}. However, these methods often rely on front-end processing modules, which decrease efficiency and add computational overhead. Also, the speech enhancement method do not really improve the robustness of ASR itself.
In this paper, we aim to build a truly robust ASR which is robust under multiple or even unseen perturbations. It has the potential to be applied in various applications in real-world setting.

\section{Method}
\subsection{Adversarial Training on ASR}
Consider the training utterance and text label set $\mathcal{D}=\{(x_i, y_i)_{i=1}^n\}$, an ASR model with learnable parameters $\theta$, and a recognition objective given by Connectionist Temporal Classification (CTC) loss $\mathcal{L}_{ctc}$. Adversarial Training (AT) aims to optimize $\theta$ by solving a minimax optimization problem:
\begin{align}
\min_{\theta} \mathbb{E}_{(x, y) \sim \mathcal{D}} \left[ \max_{\delta} \mathcal{L}_{ctc}(x+\delta, y, \theta) \right] \ s.t. ||\delta||_p \leq \epsilon,
\label{equation:eq1}
\end{align}
where the inner optimization seeks perturbations $\delta$ on speech values that maximize the loss, and the outer minimization update $\theta$ to improve the worst-case performance of the network. The boundary $||\delta||_p \leq \epsilon$ restricts the magnitude of the perturbation.
We use projected gradient descent (PGD)~\cite{madry2017towards} for the inner optimization.
In the following section, we will tackle the challenges of mitigating performance degradation and enhancing the generalization ability of ASR models through AT techniques. 
% , which iteratively solves the $\max$ problem with a step size $\alpha$:
% %
% \begin{equation}
% \begin{aligned}
% x^0 \sim &\mathcal{U}(\mathcal{B}_{\epsilon}^{\infty
% }(x)), \\
% x^{t+1}=\prod\limits_{\mathcal{B}_{\epsilon}^{\infty}(x)}(x^t+ &\alpha \rm sign(\nabla_x \mathcal{L}_{ctc}(x^t,y,\theta)))
% \label{equation:eq2}
% \end{aligned}
% \end{equation}
% %
% where $\mathcal{B}_{\epsilon}^{\infty}(x):=\{x^{\prime}:||x^{\prime}-x||_{\infty} \leq \epsilon\}$ defines a ball of radius $\epsilon$ around $x$ in the $l_\infty$ norm. The symbol $\mathcal{U}$ denotes the uniform distribution, and $\prod$ denotes a projection function. 

% Although AT has shown promise in improving ASR model robustness against adversarial examples, it remains unclear if it can mitigate standard performance degradation, and a significant gap still exists between training and cross-domain testing data.

% Unlike adversarially trained models, data augmentation is a simple method to improve ASR’ robustness. To date, it is still unclear if data augmentation benefits generalization in adversarial training.

% \section{Preliminaries}

% Although such human-designed data augmentations have been used in the training of ARSs, the limited randomness will make it very difﬁcult to generate effective samples. To enforce the network learn more robust features, the adversarial policies is taken to explore harder examples for training. 

% \subsection{WavAugment Guided Phoneme Adversarial Training}
\subsection{Phoneme Adversarial Training}
% Although adversarial examples can successfully fool the models, they are still different from the real "hard" examples appeared in practice, \ie, human does not recognize utterances from speech values, but from phoneme of the input speech. 
% In the text domain, the AT on the discrete and symbolic text space will not hurt the accuracy and even benefit for both generalization and robustness of language models.
% Therefore, we borrow the symbolic nature of languages, and apply it on ASR tasks by conducting AT on the phoneme space of speech instead of inputs.

We borrow the perspective of the AT on the contextualized language representation, and propose a new Phoneme Adversarial Training (PAT) for ASRs, \ie, conducting AT on the phoneme representation space instead of raw input space.
To accomplish this, we leverage the SpeechLM framework proposed by~\cite{zhang2022speechlm} to recognize speeches. The phoneme unit sequence of input $x$ can be obtained by applying a transformer based phoneme-unit tokenizer $\mathcal{T}$.
%\ie, $z = \mathcal{T}(x)$.
In the inner maximization step of AT, we generate phoneme adversarial examples by slightly modifying Equation~\ref{equation:eq1}. The objective of PAT can be formulated as follows:
%
\begin{align}
\min_{\theta} \mathbb{E}_{(x, y) \sim \mathcal{D}} \left[ \max_{\delta} \mathcal{L}_{ctc}(\mathcal{T}(x)+\delta, y, \theta) \right] \ s.t. ||\delta||_{\infty} \leq \epsilon,
\label{equation:eq3}
\end{align}
%
where $\epsilon$ is the magnitude of the perturbation, PAT finds the failure case, \ie, $\mathcal{T}(x)+\delta$ in phoneme space.


%The loss function of our method is as follows:
% %
% \begin{align}
% \mathcal{L} = \mathcal{L}_{ctc}(x, y, \theta) + \mathcal{L}_{ctc}(\mathcal{T}_s(x)+\delta,y,\theta)  
% \label{equation:eq4}
% \end{align}
% %

% \IncMargin{-1.5em} % 行号不向外突出 
% \begin{algorithm}[t]
% \caption{\indent Pseudo code of \wapat }
% \KwInput{Speech tokenizer $\mathcal{T}$; A sampled mini-batch of clean audios $x$ with labels $y$; attack step size $\alpha$.}
% \KwOutput{Learned network parameter $\theta$} 
% \begin{algorithmic}[1]
% \STATE Fix the network parameters of $\mathcal{T}$
% \FOR{each training steps}
% \STATE $\delta_{0} \leftarrow \mathcal{U}(-\epsilon,\epsilon)$\hfill\COMMENT{Initialize perturbation}
% \STATE $z \leftarrow \mathcal{T}(x)$\hfill\COMMENT{Get the phoneme unit $z$}
% \STATE $\delta \leftarrow \alpha \rm (\nabla_z \mathcal{L}_{ctc}(z,y,\theta) +\nabla_z \mathcal{L}_{WAG}(z, z^{aug},\theta)$ \hfill\COMMENT{Estimate the WavAugment guided perturbations}
% \STATE $\hat{\delta} \leftarrow Clip(\delta,-\epsilon,\epsilon)$
% \STATE $z_{adv} \leftarrow z+\hat{\delta}$ \hfill\COMMENT{Generate discrete adversarial examples}
% \STATE Update model parameter on $\mathcal{L}_{ctc}(z_{adv},y,\theta)$
% \\
% \ENDFOR
% \end{algorithmic}
% \label{alg:awat}
% \end{algorithm}
% \DecMargin{-1.5em}
% % Figure environment removed

\IncMargin{-1.5em} % 行号不向外突出 
\begin{algorithm}[ht]
\caption{Pseudo code of \wapat}
\label{alg:awat}
\KwInput{Speech tokenizer $\mathcal{T}$; A sampled mini-batch of clean audios $x$ with labels $y$; Perturbation size $\epsilon$.}
\KwOutput{Learned network parameter $\theta$} 
% \algsetup{linenosize=\tiny}
\begin{flushleft}
\begin{algorithmic}[1]
\raggedright
\STATE Fix the network parameters of $\mathcal{T}$
\FOR{each training step}
\STATE $z \leftarrow \mathcal{T}(x)$
\STATE $z \leftarrow \mathcal{U}(\mathcal{B}_{\epsilon}^{\infty}(z))$\hfill\COMMENT{Initialize adversarial example}
\STATE $z^{a} \leftarrow \mathcal{T}(\mathcal{DA}(x))$
\STATE $\eta, \eta^a \leftarrow \nabla_{z} \mathcal{L}_{ctc}(z,y,\theta), \nabla_{z} \mathcal{L}_{ctc}(\mathcal{T}(z^a,y,\theta)$
\STATE $\delta \leftarrow \nabla_{z} [\mathcal{L}_{ctc}(z,y,\theta) + \mathcal{L}_{wag}(z+\eta, z^{a}+\eta^a,\theta)]$
\STATE $\hat{z} \leftarrow \prod\limits_{\mathcal{B}_{\epsilon}^{\infty}(z)}(z+ \delta)$\hfill\COMMENT{Generate adversarial examples}
\STATE Update model parameter on $\mathcal{L}_{ctc}(\hat{z},y,\theta)$
\ENDFOR
\end{algorithmic}
\end{flushleft}
\end{algorithm}

\subsection{WavAugment Guided Phoneme Adversarial Training}
For improving the generalization of ASR by AT, we aim to generate adversarial examples that exhibit both stability and diversity.
Towards this end, we propose a novel WavAugment Guided Phoneme Adversarial Training (\wapat) method.

Adversarial examples are typically distributed near the decision boundary, and slight variations can cause them to lose their adversarial nature~\cite{goodfellow2014explaining}.
Enhancing the stability of adversarial examples is beneficial for obtaining more robust features.
To tackle the instability problem, we introduce the WavAugment guided term along with the CTC loss to form a new objective function during the generation of adversarial examples.

The WavAugment operation, denoted as $\mathcal{DA(\cdot)}$, applies time-domain data augmentation to an audio sample. 
We represent the phoneme representation of the original sample and the augmented sample as $z$ and $z^a$, respectively. 
Then, the perturbations generated for the two samples are denoted as $\eta$ and $\eta^a$.
The loss WavAugment guided term encourages the predictions of the adversarial examples of the original and augmented samples to be similar.
Formally, the objective function can be written as:
\vspace{-0.5em}
\setlength{\belowdisplayskip}{-0.000000000005em}
\begin{align}
\mathcal{L}_{wag}(z+\eta,z^a+\eta^a,\theta)&\!=\! -\mathcal{D}_{KL}\left[p(z+\eta,\theta)||
p(z^a+\eta^a,\theta)\right] \nonumber\\
z=&\mathcal{T}(x), z^a=\mathcal{T}(\mathcal{DA}(x))
\end{align}
where $p(x|\theta)$ is the joint probability and $\mathcal{D}_{KL}$ is the KL-divergence.
From an optimization perspective, the WavAugment guided term helps in avoiding local optima during the perturbation generation process, leading to the creation of more stable and robust features for ASR models.
% %
% \begin{align}
% %\mathcal{L}_{wag}=-\mathcal{D}_{KL}(\mathcal{T}(x)+\delta,\mathcal{T}(DA(x)), \theta)
% \mathcal{L}_{wag}(x,\delta^o,\delta^a,\theta)=-\mathcal{D}_{KL}\left[p_{ctc}(\mathcal{T}(x)+\delta^o,\theta)|| \\
% p_{ctc}(\mathcal{DA}(\mathcal{T}(x))+\delta^a,\theta)\right]
% \label{equation:eq5}
% \end{align}
% %

%&=-\mathcal{D}_{KL}\left[p_{ctc}(\mathcal{T}(x)+\delta, \theta)||p_{ctc}(\mathcal{T}(\mathcal{DA}(x) + \eta, \theta\right)]\nonumber

%This term encourages the predictions of adversarial examples of original samples and those of augmented samples to be similar.
% To improve the stability of adversarial examples, we can constrain the generation of adversarial examples from a regularization perspective. 
% The perturbation is iteratively learned to enlarge the difference between predictions and text label.
% However, we observe that the objective function with the loss deviation term alone has a major unstable issue,

In this paper, the basic data augmentations of WavAugment~\cite{kharitonov2021data} is reserved, including pitch modification (\texttt{pitch}), additive noise (\texttt{add}), band reject filtering (\texttt{band\_rej}), time masking (\texttt{time\_mask}) and reverberation (\texttt{reverb}).
% We denote the WavAugment set as $\mathcal{M}$ containing above augmentations.
\texttt{pitch} and \texttt{add} are intended to simulate variations in the speaker's voice and environmental noise.
\texttt{band\_rej} and \texttt{time\_mask} augmentations can introduce noise into the neural representation of speech, which can help the model learn to better handle noisy speech.
The \texttt{reverb} simulates the effect of sound reflections in a room, which can help the model learn to better handle the effects of reverberation in real-world environments.
Here, we use gpuRIR~\cite{diaz2021gpurir} to obtain acoustic room impulse responses.

To enhance the diversity of adversarial examples, we utilize all of the augmentations available in WavAugment to guide the generation process. 
During training, one of the transformations from WavAugment is applied to each batch of samples.
In Figure~\ref{fig:3_1}, we show an example of log mel spectrograms augmented with different transformations.
%For clarity, let us restate the pipeline of our \wapat. 
Further details regarding the \wapat can be found in Algorithm~\ref{alg:awat}.
Given a SpeechLM based speech recognition model, the speech transformer $\mathcal{T}$ first yields a higher level phoneme representation $z$ from speech input. The WavAugment guided perturbation $\delta$ can be obtained by computing the gradient of $z$ towards maximizing the $\mathcal{L}_{ctc}$ and $\mathcal{L}_{wag}$. For clarity, $\mathcal{B}_{\epsilon}^{\infty}(z):=\{z^{\prime}:||z^{\prime}-z||_{\infty} \leq \epsilon\}$ defines a ball of radius $\epsilon$ around $z$ in the $l_\infty$ norm. The symbol $\mathcal{U}$ denotes the uniform distribution, and $\prod$ denotes a projection function. 
Finally, the adversarial example $\hat{z}$ is fed into models for training.

% \texttt{pitch} serves as a transformation on the source, i.e., how the speaker talks. 
% Following WavAugment, \texttt{pitch} randomly modifies the pitch of the raw waveform by $n\in[-300,300]$ semitones.
% The \texttt{add} transformation adds random noise sampled from the MUSAN database to the input, with the scaled noise controlled by randomly sampling a signal-to-noise ratio value between 0 and 40.
% \texttt{band\_rej} and \texttt{time\_mask} can noise the neural representation of speech. The maximal width of the rejected spectrum is 150 HZ. The time mask zeroes out ten random subsequence of the input with a maximal length of 2000ms. 
% For \texttt{reverb}, we use gpuRIR~\cite{diaz2021gpurir} to obtain acoustic room impulse responses.
% The room dimensions, wall reflection coefficients, and source and receiver positions are all randomly sampled within default specified ranges~\footnote{https://github.com/DavidDiazGuerra/gpuRIR}.

% The room dimensions are randomly sampled among $[3,3,2.5]\sim[4,5,3]$, and the reflections coefficients of the walls are also selected in range [0.5, 0.6]. Moreover, the source positions is [0.1,0.1,0.1] and the receiver positions are sampled from the range (0, 1). 


% With above time-domain augmentations, 
% One of the most effective methods to improve the robustness of ASRs is data augmentation. 
% However, as shown in, given the results that even the compound time-domain data augmentations are less effective on improving the generalization. 

% More than the embedding feature space, the gradient direction of adversarial example is also guided by the corresponding augmentation. Specifically, the adversarial gradient direction in each iteration is averaged by that of augmented samples, we dESBribe this method as ‘directional’.

%The perturbed unit representation thus can be created by adding $\delta$ on $z^0$. Noted that, the target ASRs are trained by minimizing the CTC loss on clean inputs and adversarial phoneme representations simultaneously.

% \begin{table}[t]  
%     \centering
%     \caption{The effect of the transformation diversity on test sets and ESB benchmark scores (WER).}
%     \resizebox{0.8\hsize}{!}{
%     \begin{tabular}{lccc}  
%     \toprule
%     {\multirow{2}*{\textbf{\textsc{Diversity}}}}& \multicolumn{2}{c}{\textbf{\textsc{Librispeech}}} & {\multirow{2}*{\textbf{\textsc{ESB Score}}}}\\ 
%     ~ & test-clean & test-other & ~ \\
%     \midrule
%     Baseline & 3.34 & 7.38 & 36.47 \\
%     \midrule
%     DA-1N    & 3.35 & 7.37 & 34.18 \\
%     AWA-1N   & 3.32 & 7.28 & 32.58 \\
%     DA-2N    &  \\
%     AWA-2N    &  \\
%     \midrule
%     \rowcolor[RGB]{237,237,237} -3B & $\textbf{70.1}_\textbf{ ($\uparrow$ 5.4)}$  & $\textbf{60.6}_\textbf{ ($\uparrow$ 3.9)}$ & $\textbf{73.8}_\textbf{ ($\uparrow$ 4.3)}$\\
%     \bottomrule
%     \end{tabular}}
%     \label{tab:diversity}
%     % \vspace{-0.4cm}
% \end{table}


% This work focuses on improving the generalization ability of representation adversarial training guided
% by WavAUgment. We investigated the effects of individual and composed augmentations for adversarial training.
% We present our results on End-to end Speech Challenge benchmark (ESB)~\cite{gandhi2022esb}, which contains multiple speech datasets spanning different domains and speech recognition conditions. The details of ESB refers to Section~\ref{sec:exp}. During training, the model’s robustness was tracked at each epoch using PGD-1 applied to the inputs.
% \noindent \textbf{Type of transformations.}~For the individual augmentations the following five wavform transformations are choosen: \texttt{pitch}, \texttt{add}, \texttt{band\_rej}, \texttt{time\_mask} and \texttt{reverb} described in Section~\ref{sec:2.2}. 

% On analyzing single augmentations in Figure~\ref{fig:3_1}, we first observed that the standard DAs perform only slightly better than the baseline on ESB score. In all cases, AWA's ESB scores are further lower than DA. This shows that training with adversarial augmented instances of each mini-batch can indeed improve the generalization of the model. However, this gain is not consistent on all test datasets. 
% Four of the five AWAs have one lower WER. Besides, the \texttt{reverb} of AWA has lower WERs on Earnings-2 and SPGISpeech, which are similar in speaking style (Spontaneous) and collected from earnings calls.
% From the result, we can conclude that AWA makes the network have to learn more robust features.

% For \texttt{add} and \texttt{pitch}, the bad cases occur on the VoxPopuli. 
%VoxPopuli is largely sourced from non-native speakers occupied the unique domain of oratory and political speech, the \texttt{add} and \texttt{pitch} in time-domain is enough for generalization. 
% The \texttt{time\_mask} of AWA with the lower WER contains SPGISpeech only. 
%SPGISpeech is an English speech recognition corpus composed of company earnings calls. The adversarial \texttt{time\_mask} is tough for the transcription of audios with multiple silent fragments. 

% \noindent \textbf{Diversity of transformations.}
% Next, we study the performance of combinations of augmentations. Due to complex combinations, the diversity is defined as the number of transformations superimposed $N \in \{1,5\}$. With the same pool of transformations as the individual experiment, audios in a batch are augmented using $N$ transformations sampled at random.
% The results are reported in Table~\ref{tab:diversity} and show that, the standard DA-1N yields relative improvements of 6.28\% over no augmentation on ESB score. This indicates that the diversity of training data is crucial for the generation of ASR. However, the standard DA results in a performance degradation on Librispeech's clean test sets.
% AWA helps the ASR gain more robust features by generating adversarial examples in representation space.
% Compared with DA, training on these examples will reduce the shift of clean distribution, yielding both the robustness and generalization improvement.

\begin{table*}[t]
\centering
\caption{WER comparison on the ESB benchmark over various methods for enhancing the robustness of ASR. Best performances are highlighted in bold.}
\vspace{-0.1cm}
\resizebox{1\hsize}{!}{
\begin{tabular}{l|cc|c|c|c|c|c|c|c|c|cc}
\toprule
{\multirow{2}*{\textbf{Method}}} & \multicolumn{2}{c|}{\textbf{Librispeech}} & {\multirow{2}*{\textbf{Chime-4}}}  & {\multirow{2}*{\textbf{Common Voice}}}  & {\multirow{2}*{\textbf{VoxPopuli}}}&  {\multirow{2}*{\textbf{TED-LIUM}}} &  {\multirow{2}*{\textbf{GigaSpeech}}} & {\multirow{2}*{\textbf{SPGISpeech}}}  &  {\multirow{2}*{\textbf{Earnings-22}}} &  {\multirow{2}*{\textbf{AMI}}} & {\multirow{2}*{\textbf{ESB score}}} \\
     ~ & \textbf{test-clean} & \textbf{test-other} & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~\\
\midrule
SpecAugment~\cite{park2019specaugment}  & 3.32 & 7.34 &	45.49 &	38.46 &	36.47 &	19.03 &	24.57 &	20.10 &	51.10 &	45.02 &	36.19 \\
WavAugment~\cite{kharitonov2021data}  & 3.34 & 7.35 & 35.65 & 38.16 & 36.64 & 18.12 & 25.53 & 18.99 & 52.79 & 46.10 & 34.18 \\
AdvEx~\cite{sun2018training}     & 3.36 & 7.36 & 46.10 &	38.35 & 36.74 &	18.18 &	24.49 &	19.36 &	52.01 &	44.79 &	36.24  \\
DEMUCS~\cite{defossez2020real}      & 3.33 & 7.29 &	33.57 &	43.63 &	36.71 & 18.31 & 24.39 &	26.24 &	56.76 &	44.63 &	35.32\\
\wapat        & \textbf{3.32} & \textbf{7.28} &	\textbf{32.68} &	\textbf{36.43} &	\textbf{36.38} &	\textbf{18.12} & \textbf{24.25} & \textbf{18.40} &	\textbf{49.78} & \textbf{44.53} &	\textbf{32.58} \\
\bottomrule
\end{tabular}}
\label{tab:overall}
\vspace{-0.2cm}
\end{table*}

% Figure environment removed

\section{Experiment}
\label{sec:exp}
%\subsection{Set up}
\noindent \textbf{Datasets and Settings}
We conducted experiments on the ESB~\cite{gandhi2022esb} benchmark to evaluate cross-domain ASR robustness. 
ESB comprises eight datasets with a broad range of domains, acoustic conditions, speaker styles, and transcription requirements. Notably, Librispeech~\cite{panayotov2015librispeech} and Common Voice~\cite{ardila2019common} only contain narrated style speeches, while VoxPopuli~\cite{wang2021voxpopuli} and TED-LIUM~\cite{rousseau2012ted} have oratory style speeches, and AMI ~\cite{carletta2007unleashing} contains spontaneous style speeches. GigaSpeech~\cite{chen2021gigaspeech}, SPGISpeech~\cite{o2021spgispeech}, and Earnings-22~\cite{del2022earnings} cover two different styles of speeches. Additionally, We included the optional CHiME-4~\cite{vincent2017analysis} dataset with narrated style to test generalization. We use the standard split of the above datasets and unify the transcription format as normalised.
We finetune the SpeechLM-P model\footnote{\url{https://github.com/microsoft/SpeechT5/tree/main/SpeechLM}} on the Librispeech-100h dataset, which is pre-trained on both the LibriSpeech-960h audio and the LibriSpeechLM corpus2. And we evaluate the robustness on datasets in ESB. 
Audio format is 16-bit WAV with 16 kHz, and transcription format is unified into the normalized form.

% Among them, Librispeech~\cite{panayotov2015librispeech} is a standard large-scale dataset derived from English audio books and contains 1000 hours of speech. 
% Common Voice~\cite{ardila2019common} is collected from Wikipedia with narrated speaking style. VoxPopuli~\cite{wang2021voxpopuli} is sourced from 2009-2020 European Parliament event recordings with oratory speaking style. TED-LIUM~\cite{rousseau2012ted} is based on English-language TED Talk conference videos with oratory speaking style. GigaSpeech~\cite{chen2021gigaspeech} is a multi-domain English speech recognition corpus curated from audiobooks, podcasts and YouTube, which covers both narrated and spontaneous speech.
% The last SPGISpeech~\cite{o2021spgispeech}, Earnings-22~\cite{del2022earnings} and AMI~\cite{carletta2007unleashing} datasets are all from meetings and narrated or oratory styles. 
\noindent \textbf{Implementation Details}
The hyper-parameters used in WavAugment are as follows: 
\texttt{pitch} randomly modifies the pitch of the waveform by $n\in[-300,300]$ semitones.
\texttt{add} randomly adds noise from MUSAN~\cite{snyder2015musan} dataset with a scaled signal-to-noise ratio between $[0,40]$.
The maximal width of the rejected spectrum in \texttt{band\_rej} is 150 Hz. 
The \texttt{time\_mask} operation zeros out ten random subsequences of the inputs with a maximum length of 2000 ms.
The room dimensions and other parameters in \texttt{reverb} are randomly sampled within default ranges~\footnote{https://github.com/DavidDiazGuerra/gpuRIR}.

% We use word error rate (WER) to evaluate predictions against target transcriptions. 
We evaluate the accuracy of our predictions against target transcriptions using the word error rate (WER). 
The ESB score is the macro-averaged value of datasets in the ESB benchmark, excluding Librispeech.
We implement \wapat on the pre-trained SpeechLM-P~\cite{zhang2022speechlm}, which consists of a Speech Transformer, a Shared Transformer and a CTC head. 
By default, we refer SpeechLM-P-Base to SpeechLM in all tables and figures.
Models are optimized by Adam with a maximum learning rate of 1e-5 and a tri-stage learning rate schedule with the warming-up, holding, and decay periods of [0.1, 0.4, 0.5]. 
We train the models for a total of 30K steps with a batch size of 800 seconds.
Perturbations are bounded with an $l_{\infty}$-norm of $0.01$. 
All experiments are conducted on four NVIDIA Tesla A100.
% We will release our code upon the acceptance of this paper.
% The code will be available \footnote{https://github.com/WAPATASR}.
% \texttt{pitch} serves as a transformation on the source, i.e., how the speaker talks. 
% Following WavAugment, \texttt{pitch} randomly modifies the pitch of the raw waveform by $n\in[-300,300]$ semitones.
% The \texttt{add} transformation adds random noise sampled from the MUSAN database to the input, with the scaled noise controlled by randomly sampling a signal-to-noise ratio value between 0 and 40.
% \texttt{band\_rej} and \texttt{time\_mask} can noise the neural representation of speech. The maximal width of the rejected spectrum is 150 HZ. The time mask zeroes out ten random subsequence of the input with a maximal length of 2000ms. 
% For \texttt{reverb}, we use gpuRIR~\cite{diaz2021gpurir} to obtain acoustic room impulse responses.
% The room dimensions, wall reflection coefficients, and source and receiver positions are all randomly sampled within default specified ranges~\footnote{https://github.com/DavidDiazGuerra/gpuRIR}.

% The room dimensions are randomly sampled among $[3,3,2.5]\sim[4,5,3]$, and the reflections coefficients of the walls are also selected in range [0.5, 0.6]. Moreover, the source positions is [0.1,0.1,0.1] and the receiver positions are sampled from the range (0, 1). 

% The models in domain generalization experiments are optimized by AdamW with learning rate of 5e-6 and weight decay of 0.1. For data augmentation, simple random resize crop and random horizontal filp are used. The other hyper-parameters, such as batch size, dropout rate, and training steps, we keep consistent with the default configuration in DomainBed

\subsection{Overall Performance}
To demonstrate the effectiveness of \wapat, we first compared it with data augmentation and adversarial training methods.
We make a fair comparison with standard WavAugment~\cite{park2019specaugment} and SpecAugment~\cite{park2019specaugment}. 
%SpecAugment is used to augment inputs on the log mel spectrogram. 
Although SpecAugment performs well on Librispeech test datasets, it shows poor performance in terms of robustness on ESB.
In addition, WavAugment has the suboptimal performance of robustness, with an ESB score of 34.18.
Notably, our \wapat achieves superior performance compared to the above data augmentation methods on both in-domain and out-of-domain datasets by a large margin.
Compared with the waveform space AT method AdvEx~\cite{sun2018training}, \wapat achieves 10.01\% improvement in ESB score.
This further verifies the strengths of our proposed \wapat in terms of generalization on the phoneme space.
Interestingly, \wapat shows obvious advantages on Chime-4 and Common Voice datasets, which share the same speaking style (Narrated) as the LibriSpeech set. 
To provide a more comprehensive evaluation, we test the SpeechLM with the speech enhancement-based method DEMUCS~\cite{defossez2020real}. With sacrificing of some computational efficiency, DEMUCS achieves good performance on generalization, however, still inferior to our method.
% This indicates that the proposed \wapat is helpful to enhance ASRs for compositional generation.



% We conduct a comparison of speech enhancement-based techniques, data augmentations, and adversarial training methods for robust ASR, as shown in Table~\ref{tab:overall}. First, we demonstrate that AWA achieves the state-of-the-art on this challenging cross-domain ASR benchmark. Moreover, the enhancement technique DEMUCS~\cite{defossez2020real} has the suboptimal performance.
% Notably, AWA reach performance superior to the traditional adversarial training method AdvEx~\cite{sun2018training} by 10\% on WER.
% This even outperforms the SpecAugment~\cite{park2019specaugment} by 9.97\% on WER.
% Also, it is evident that AWA is vastly superior to other methods on in-domain data and out-of-domain data simultaneously.
% Therefore, it is vital for augmentation design to allow optimization with a hardness.

\begin{table}[t]  
    \centering
    \caption{Ablation study of the proposed \wapat on cross-domain datasets, (a) is different adversarial training variant, (b) is magnitude $\epsilon$.}
    \vspace{-0.1cm}
    \resizebox{0.99\hsize}{!}{
    \begin{tabular}{lccc}  
    \toprule
    {\multirow{2}*{\textbf{Method}}}& \multicolumn{2}{c}{\textbf{Librispeech}} & {\multirow{2}*{\textbf{ESB Score}}} \\
    ~ & \textbf{test-clean} & \textbf{test-other} & ~ \\
    \midrule
    (a) \textsc{No-AT} & 3.34 & 7.38 & 36.47 \\
     \quad \quad w/ \textsc{Phoneme AT}  & 3.32 & 7.34 & 35.18 \\
     \quad \quad w/ \textsc{WavAugment PAT} & \textbf{3.32} & \textbf{7.28} & \textbf{32.58}\\
    \midrule
    \midrule
    (b) \wapat \\
    \quad \quad $\epsilon$ = \textsc{0.005} & 3.32 & 7.35 & 34.42 \\
    \quad \quad $\epsilon$ = \textsc{0.01}  & \textbf{3.32} & \textbf{7.28} & \textbf{32.58} \\
    \quad \quad $\epsilon$ = \textsc{0.015} & 3.32 & 7.31 & 33.24 \\
    \bottomrule
    \end{tabular}
    }
    \label{tab:ablation}
    \vspace{-0.4cm}
\end{table}

\subsection{Discussion}
% \subsection{Whether the robustness of ASR come from WavAugment?}
%\textbf{whether the robustness of \wapat is attributed to WavAugment or our guided phoneme adversarial training strategy?}

We further explore the impact of individual techniques in WavAugment and their combinations with \wapat on the performance of the model, as shown in Figure~\ref{fig:3_2}.
Specifically, we report the percentage of WER reduction for both standard WavAugment and our \wapat, compared to the baseline model.

It can be seen that WavAugment is a useful technique for improving the robustness of models.
However, there are cases where the individual augmentation perform worse than the baseline on certain datasets.
% some individual augmentations may perform worse than the baseline on certain datasets,
For example, \texttt{time\_mask} increases the WER on the TED-LIUM dataset. 
Furthermore, we note that with the same transformation, the WER reduction of \wapat is greater than that of WavAugment. 
Additionally, for all transformations, there are some oscillations in WavAugment while \wapat is consistently increased compared to the baseline.
The results accords with the expected that phoneme adversarial training with WavAugment guidance constrains stable optimization of adversaries, resulting in better generalization.

\subsection{Ablation Study}
As shown in Table~\ref{tab:ablation}, to better understand the function of each component of \wapat, ablation studies are performed and expected to answer the following questions.

\noindent \textbf{How effective is the PAT?} Echoing (a) in Table~\ref{tab:ablation}, SpeechLM--P with proposed phoneme adversarial training (\textsc{Phoneme AT}) can achieve the better performance on in-domain and out-of-domain datasets than baseline (\textsc{No-AT}). It indicates adversarially altered phoneme perturbations are much closer to the clean distribution, while strengthen the robustness by capturing more robust features.

\noindent \textbf{Is \wapat superior than PAT?}
With \textsc{WavAugment PAT} means that PAT is guided by the WavAugment, \ie, proposed \wapat. The ESB score of \wapat has decreased by roughly 7.4\% when compared to PAT. It is evident that WavAugment guidance AT indeed aids in finding stronger robust features.

\noindent \textbf{Does the choice of magnitude $\epsilon$ matter?}
We present the \wapat results with different magnitude $\epsilon$ in Table~\ref{tab:ablation} (b).
$\epsilon=0$ means the standard training of SpeechLM (\textsc{No-AT}), which makes the models have the worst performance on clean WER and robustness. 
With the increase of $\epsilon$ to 0.01, there is a drop of both clean WER and ESB score.
Moreover, we find the clean WER of target model has the lower sensibility on $\epsilon$.
But with the $\epsilon$ becoming larger, AT greatly damages the generalization, \eg, with $\epsilon=0.015$, ESB score increases to 33.24. This finding is also revealed by~\cite{kireev2022effectiveness}.

\section{Conclusions and Limitations}
In this paper, we propose a novel WavAugment Guided Phoneme Adversarial Training (\wapat) method, to enhance the cross-domain generalization of ASR systems.
\wapat utilizes the phoneme representation of augmented audios to guide the generation of adversarial examples, resulting in consistently stronger generalization on multiple datasets without sacrificing clean performance.
Our experiments demonstrate that \wapat achieves state-of-the-art robustness on challenging ESB benchmark. 
However, \wapat still costs increased training time, this limitation also holds for any adversarial training. This limitation is remained as the future optimization direction.
\bibliographystyle{IEEEtran}
\bibliography{mybib}

\end{document}
