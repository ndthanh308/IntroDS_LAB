\section{Preliminary Experiments}


%\han{(1) GUI conversion experiment; (2) GUI retrieve; (3) GUI recommendation}

To demonstrate the usability of our dataset, we perform preliminary experiments on the dataset.
These experiments contain two types of tasks: GUI conversion and GUI retrieval.
We select current state-of-art approaches for these two tasks and use the automatic metrics for evaluation. 
We will discuss the qualitative and quantitative performance as well as the limitations of selected approaches on our proposed dataset.


\subsection{Tasks}

\paragraph{\textbf{GUI Conversion}}
This task is to automatically convert an existing GUI to a new layout of GUI~\cite{behan2012adaptive}. 
For example, given the present phone layout, generate a tablet layout automatically.
It is also a GUI generation task.
The goal of GUI conversion task is to generate a flexible interface that is applicable to different platform.
By generating GUI layouts that fit the needs of developers, we can make developer more productive and decrease engineering effort.

\paragraph{\textbf{GUI Retrieval}} 
This task is to retrieval the most relevant GUI from the database and recommend it to the most appropriate users~\cite{yeh2009sikuli}.
GUI template search and recommendation is an essential direction for current automated GUI development to accelerate the development process. 





\subsection{Selected Approaches}

\subsubsection{GUI Conversion}
As far as we know, no researcher has proposed a method for generating a tablet GUI from a phone GUI, so we will use some existing relevant GUI generation methods to apply to this task.
Three approaches, which are all widely used in GUI generation, are selected in this task:

%\han{more details}

\noindent \textbf{LayoutTransformer}~\cite{gupta2021layouttransformer}: a simple yet powerful auto-regressive model based on Transformer framework that leverages self-attention to generate layouts by learning contextual relationships between different layout elements. It is able to generate a brand new layout either from an empty set or from an initial seed set of primitives, and can easily scale to support an arbitrary of primitives per layout.

\begin{comment}
\noindent \textbf{LayoutGAN}~\cite{li2019layoutgan}: a GAN-based network , with the generator taking randomly sampled inputs as parameters, arranging them and thus producing refined geometric and class for layout generation.
\end{comment}

\noindent \textbf{LayoutVAE}~\cite{jyothi2019layoutvae}: a stochastic model based on variational autoencoder architecture. It is composed of two modules: CountVAE which predicts the number of objects and BBoxVAE which predicts the bounding box of each object. It is capable of generating full image layouts given a label set, or per label layouts for an existing image given a new label. Besides, it is also capable of detecting unusual layouts, potentially providing a way to evaluate layout generation problem.

\noindent \textbf{VTN}~\cite{arroyo2021variational}: a VAE-based framework advanced by Transformer model, which is able to learn margins, alignments and other elements without explicit supervision. Specifically, the encoder and decoder are parameterized by attention-based neural network. During the variational process, VTN sample latent representations from the prior distributions and transform those into layouts using self-attention based decoder.

\subsubsection{GUI Retrieval}
%\han{more details about baselines}

Researchers have not yet attempted to get the comparable tablet GUI design from a phone GUI design, but numerous methods for locating relevant GUI designs have been proposed. 
These comparable GUI design retrieval approaches will be selected and adapted to fit this task.
For GUI retrieval task, we employ three semantic matching and learning-based models:

\begin{comment}
\noindent \textbf{Hist}~\cite{jain1996image}: a classical image retrieval model using image features such as visual color and shape.

\noindent \textbf{SIFT}~\cite{lowe1999object}: a scale-invariant feature transform to extract image features widely used for image retrieval, object recognition tasks.
\end{comment}

\noindent \textbf{Rico}~\cite{deka2017rico}: a neural-based training framework that utilizing content-agnositc similarity heuristic method for UI comparing and matching. To facilitate query-by-example search, Rico exposes a vector representation for each UI that encodes layout. Rico provides search engines with several visual representations that can be served up as results: UI screenshots, flows, and animations.

\noindent \textbf{GUIFetch}~\cite{behrang2018guifetch}: a method that takes an input the sketch for an app and leverages the growing number of open source apps in public repositories and then devise a component-matching model to rank the identified apps using a combination of static and dynamic analysie and computes a similarity metric between the models and the provided sketch.

\begin{comment}

\nondent \textbf{ListNet}~\cite{revaud2019listnet}: an image retrieval benchmark utilizing a Listwise loss to learn with average precision.
\end{comment}

\noindent \textbf{WAE}~\cite{chen2020wireframe}: a wireframe-based UI searching model using image autoencoder architecture. Specifically, it is a  neural based approach using convolutional neural network (CNN) in an unsupervised manner for building a UI design search engine that is flexible and robust in face of the great variations in UI designs. The enhancement of wireframe will facilitate the layout generation process.


\input{tables/GUIConversion.tex}

\subsection{Evaluation Metrics}
We consider automatic evaluation on both these two tasks.  



\subsubsection{GUI Conversion}

%\han{refer to layout transformer in section 4.2 evaluation methodology to write more details}
For GUI conversion task, it's important to evaluate layouts in terms of two perspectives: perceptual quality and diversity. We follow with the similar evaluation protocol in~\cite{arroyo2021variational, gupta2021layouttransformer, bunian2021vins} and utilize a set of metrics to evaluate the quality and diversity aspects. Specifically, we use the following metrics:

\noindent \textbf{Mean Intersection over Union (mIoU)~\cite{rezatofighi2019generalized}}: also known as the Jaccard index~\cite{hamers1989similarity}, is a method to quantify the percent overlap between the ground-truth and generated output. 
The IoU is calculated by dividing the overlap area between predicted class positions and ground truth by the area of union between predicted position and ground truth.
So, it is computed by
\begin{equation}
    mIoU = \frac{1}{k}\sum_{i=0}^k\frac{TP(i)}{TP(i) + FP(i) + FN(i)}
\end{equation}
where $k$ means $k$ classes in both images, $TP(i)$, $FP(i)$ and $FN(i)$ represent the distribution of true positive, false positive and false negative of $i_{th}$ class between two compared images. 


\noindent \textbf{Overlap~\cite{yeghiazaryan2018family, gupta2021layouttransformer}}: measures the overlap ratio between the ground-truth and our generated output. The overlap metric use the total overlapping area among any two bounding boxes inside the whole page and the average IoU between elements.

\begin{comment}
\noindent \textbf{Alignment}: is an index 
\end{comment}

\noindent \textbf{Wasserstein (W) distance~\cite{panaretos2019statistical}}: is the distance of the classes and bounding boxes to the real data. It contains W class and W bbox metrics. Wasserstein distance is to evaluate the diversity between the real and generated data distributions.

\noindent \textbf{Unique matches~\cite{patil2020read}}: is the number of unique matchies according to the DocSim~\cite{patil2020read}. It measures the matching overlap between real sets of layouts and generated layouts. It is designed for diversity evaluation. 

\noindent \textbf{Matched rate}: To more directly show how many UI components in the ground truth's tablet GUI are successfully and automatically converted by the model, we select the metric: the matched rate. 
Suppose there are a total of $m$ UI components in the ground truth, and there are $n$ components in the generated tablet GUI that match the components in the ground truth, then the matched rate is calculated as $n/m$.



\subsubsection{GUI Search}
For GUI retrieval task, we evaluate the performance of a UI-design search method by two metrics: Precision and Mean Reciprocal Rank (MRR), which have been widely-used in GUI search~\cite{chen2020wireframe, deka2017rico, yeh2009sikuli, chen2019gallery, huang2019swire}. 

\noindent \textbf{Precision@k (Pre@k)}: Precision@k is the proportion of the top-k results for a query UI that are relevant UI designs.  Specifically, the calculation of ranking Precision@k is defined as follows:
\begin{equation}
\mathrm{Precision@k} = \frac{\# relevant UI design}{k},
\end{equation}
As we consider the original UI as the only relevant UI for a treated UI in this study, we use the strictest metric Pre@1: Pre@1=1 if the first returned UI is the original UI, otherwise Pre@1=0.

\noindent \textbf{Mean Reciprocal Rank (MRR)}: MRR is another method to evaluate systems that return a ranked list. It computes the mean of the reciprocal rank (i.e., 1/rank) of the first relevant UI design in the search results over all query UIs. Specifically, the calculation of MRR is defined as follows:
\begin{equation}
    MRR = \frac{1}{Q} \sum_{i=1}^{Q} \frac{1}{rank_{i}},
\end{equation}
where $Q$ refers to the number of all the query UIs. The higher value a metric is, the better a search method performs.  




\subsection{GUI Conversion Task}



\subsubsection{Experimental Setup}
\label{sec:conversionSetup}
Since we are comparing the structure of the rendered GUI as opposed to its pixels, we do not consider the contents of images and texts inside the GUI.
Therefore, we convert GUI pages to wireframes by converting each category of UI component to a box of the specified color, which has been widely adopted in recently related works~\cite{gupta2021layouttransformer, li2019layoutgan, jyothi2019layoutvae}.
In our GUI conversion work, unlike typical GUI generation tasks, we ask the model to learn to generate a tablet GUI comparable to the input phone GUI, rather than another phone GUI.
As input to the training model, we encode all UI components of a phone GUI as a component sequence from top to bottom and left to right. 
Each UI component in the GUI is encoded as a quaternion $(x,y,w,h)$. Where $x$ and $y$ denote the x and y coordinates of the upper left corner of this UI component, and $w$ and $h$ denote the length and width of the component.
Similarly, the tablet GUIs in the pair are encoded into a sequence that serves as the ground truth for training and validation.

Following the setup of experiments in LayoutTransoformer and LayoutVAE, we randomly select 1000 pairs (~10\%) in the total dataset as the test set and the rest of the data as the training set. 
All the results of the metrics are calculated on the test set.
All of the results are based on 5-fold cross validation on the test set. 



\subsubsection{Quantitative Evaluation} 



 We present the quantitative evaluation results on the Table~\ref{tab:conversion}. 
As expected, we observe that VTN model achieves the best performances in terms of all metrics. 
 It yields large improvement regarding perceptual quality, such as \emph{W class}, and \emph{Unique matches}. 
 Besides, it obvious outperforms the LayoutVAE model across all metrics and slightly better than LayoutTransformer model. 
 The reason heavily rely on the mutual enhancement between self-attention and VAE mechanisms. 
However, according to the final metric \emph{Matched rate}, only less than 20\% of the UI components can be matched between the generated tablet GUI layouts and ground truth. 
Figure~\ref{fig:simiDis} shows that the most phone-tablet GUI pairs in the dataset have the similarity between 50\% and 70\%, so current model's capacity to learn the relationship between phone-tablet GUI pairs still requires improvement.
The results suggest that further work is needed to design and train a more effective model in the GUI conversion task.
We hope that our open source dataset will enable more researchers to participate in automated UI development and contribute more effective methods.
 





\subsubsection{Qualitative Evaluation}

% Figure environment removed


To better understand the performance of different models on our task, we present qualitative results of selected models and their generated outputs in Figure~\ref{fig:generationExp}. 
In alignment with the quantitative results in Table~\ref{tab:conversion}, we observe that the VTN model outperform other selected models. 
% Based on our observation that the Transformer-based models LayoutTransformer and VTN outperforms the previous RNN-based models such as LayoutVAE. 
It demonstrates the efficiency of self-attention mechanism on the layout generation task. 
However, we can observe that the best selected model VTN still fall short of generating precise margins and positions towards the ground-truth. 
There is still much room for improvement in learning the relationship between phone and tablet GUIs, and the current results are far from helpful to GUI developers.
The GUI conversion in the crossing platform poses huge challenges for the existing state-of-the-art model. 
Therefore, simply utilizing previous layout generation model cannot tackle the challenges of GUI conversion in both Android phones and tablets.


 
\subsection{GUI Retrieval}
\subsubsection{Experimental Setup}
Following related GUI search works~\cite{chen2020wireframe, revaud2019listnet}, we convert the GUI pages into wireframes in the same way as the GUI conversion task in Section~\ref{sec:conversionSetup}.
The input in this experiment is the phone GUI wireframe and the ground truth is the corresponding tablet GUI wireframe.
We train a GUI encoder to encode GUI into numerous vectors and compare the similarity of these numerous vectors.
We randomly select 1000 pairs (~10\%) in the total dataset as the test set and the rest of the data as the training set. 
The results of this test set is automatically checked.

Due to the existence of comparable GUI designs on the tablet, sometimes the search result is not the ground truth, but the GUI design is reasonable and very close to the ground truth.
We would also consider this result to be an appropriate design.
Therefore, we randomly selected 100 cases from the test set and manually verify the results.
We invited three industry developers with at least one year of experience in Android development to manual evaluate search results.
Each participant evaluate the top 10 search results to determine if they are a reasonable tablet design.
After the initial evaluating, three volunteers have a discussion and merge conflicts.
They clarify their findings, scope boundaries among categories and misunderstanding in this step.
Finally, they iterate to revise evaluation results and discuss with each other until consensus is reached.
To differentiate, we call these results manual checked results.


\input{tables/GUISearch.tex}



\subsubsection{Results}
Table~\ref{tab:retrieval} shows the performance metrics of the three selected methods in our dataset. 
We report four metrics in terms of precision following with previous UI search work: \emph{Pre\@1}, \emph{Pre\@5}, \emph{Pre\@10} and \emph{MRR}. 
The results in column \emph{Pre@1} and \emph{MRR} are automatic checked results.
The results in column \emph{Pre@5} and \emph{Pre@10} are manual checked results.
We can observe that the \emph{WAE} model outperforms other two approaches in terms of automated and manual results. 
Compared with GUI conversion task, GUI search task is more developed and applicable.
Learning-based approaches \emph{Rico} and \emph{WAE} both achieved high search accuracy.
It demonstrates the advantages of neural network models with huge training parameters in extracting features and patterns of GUI. 
We hope that more researchers in the future will design more search-related tasks based on our dataset, such as semantic-based search, GUI template recommendation, etc.


