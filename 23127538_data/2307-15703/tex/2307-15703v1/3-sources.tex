\section{Sources of Uncertainty in NLG}
\label{sec:sources}
From the perspective of an NLG agent, there are many phenomena or \textit{sources} that lead to uncertainty about a response given a context. A \textit{disentangled} representation of these sources is crucial. For example, to ask a clarification question when the context is under-specified or ambiguous or to abstain from answering when the context is not familiar to the agent (more applications in \Cref{sec:informed_decisions}). In this section, we identify and characterise the main sources of uncertainty when generating natural language. 
We start by characterising sources of uncertainty in human language production; the data.\footnote{Data might nowadays include generated text, see \citet{taori2022data, veselovsky2023artificial}.} 
We then move to sources of uncertainty in modelling decisions, and finally propose a fluid, two-dimensional taxonomy that we argue to be more informative and faithful than the popular aleatoric/epistemic dichotomy.


\subsection{Sources of Uncertainty in Data}
\label{sec:uncertainty-data}
As discussed in \Cref{sec:background}, multiple possible worlds give rise to uncertainty. Variation in human language production, therefore, gives rise to uncertainty about a response given a context. While sources of disagreement in classification tasks have been studied extensively \cite{poesio-etal-2019-crowdsourced,basile-etal-2021-need,plank-2022-problem,jiang-tacl_a_00523,sandri-etal-2023-dont}, there is less understanding of where variation in NLG data stems from. 

To address this issue, we build on the Triangle of Reference \cite{ogden-richards-1923meaning}, used to identify types of disagreement in classification \cite{Aroyo_Welty_2015,jiang-tacl_a_00523}, and extend it to the more complex case of language production. 
\cref{fig:production-diagram} shows a schematic diagram of the key aspects involved in the production of a linguistic signal (output) by a speaker given a prompt (input; be it a sentence in a source language, an image, a text, or a sequence of dialogue turns, \ia) in the context of a communicative task, such as translation, image description, summarisation, or dialogue response generation. 
% Figure environment removed

The left-hand-side triangle in this diagram is akin to the ``Triangle of Reference'', which here corresponds to the \textbf{input interpretation process}: the speaker infers a meaning for the input. Such meaning is not explicitly constrained and remains unobserved (unlike in classification tasks, where the space of possible meanings is restricted by the possible labels an annotator can assign). The right-hand-side triangle depicts the \textbf{output production process}: the interpretation of the input in the context of a given task evokes a communicative goal or intended meaning (\textit{what} the speaker wishes to convey), which is then overtly expressed by the output (\textit{how} the intent is conveyed via language).\footnote{These two latter steps loosely correspond to content determination and surface realisation in traditional data-to-text NLG \citep{reiter_dale_2000,gatt-survey-2018}.}

We are interested in characterising the sources of variation in the output for a fixed input.\footnote{Typically this will correspond to variability across speakers (\eg, different summaries of a given text produced by different crowdworkers), but nothing hinges on this: different outputs may be produced by the same individual.}  
We introduce these sources in the context of different production sub-processes where a one-to-many mapping can occur. The blue arrows in \cref{fig:production-diagram} refer to such sub-processes.\footnote{While in this section we focus on sources inherent to the language production process, data collection design can also have an impact on output variation \cite{geva-etal-2019-modeling, parmar-etal-2023-dont} (\eg, 
insufficient guidelines regarding the target length of a summary, the use of slang, or the level of detail with which an image should be described). Yet, many current NLG systems are not trained on curated datasets with annotator guidelines but on fortuitous data such as movie subtitles or online user-generated content.}




% Figure environment removed

\paragraph{Input {\color[HTML]{4d99df}$\rightarrow$} inferred meaning.} 
Variation in the output can stem from multiple ways to infer a meaning for the input. %
Such variation has three main sources: 
\textit{input ambiguity}, 
\textit{input complexity}, and 
\textit{input error}. 
Input ambiguity can be due to polysemous words (\eg,``I have a date'', where ``date'' may be a fruit, an appointment or a certain day), syntactic ambiguity (\eg, ``the fish is ready to eat'', where the fish may be ready to be fed or to be eaten) or underspecification (\eg, in pro-drop languages where the subject, and hence its grammatical gender, is not explicit:  in Greek, `Tρώει.' may be translated as ``she/he/it eats.'').   
The degree of complexity in the input---such as the   presence of many entities in an image or of infrequent words in textual prompts---may result in different degrees of processing difficulty and thus also lead to variation in the interpretation process. 
Finally, input error (\eg, spelling errors) may also lead to noise in how speakers interpret the input and consequently to output variation. 

\paragraph{Inferred meaning {\color[HTML]{4d99df}$\rightarrow$} intended meaning.} 
Given a fixed input interpretation, outputs with different intended meanings may be plausible, thus leading to semantic variation in the output. 
This is modulated by two main factors: the \textit{open-endedness} of the task and the \textit{personal perspective} of the speakers. 
For example, in more open-ended tasks, such as dialogue response generation or storytelling, a given prompt may be followed up by semantically diverse dialogue acts or story continuations. This contrasts with tasks such as machine translation, where the intended meaning behind the output is determined by the input's meaning. In addition, speakers may have different perspectives due to different social backgrounds or points of view. Thus, if a task allows for semantic variation, such diversity of perspectives is likely to result in diverse outputs. For example, asking different individuals to answer the question `Should we have guns at home?' might trigger a range of different intents based on geographical and societal backgrounds.


\paragraph{Intended meaning {\color[HTML]{4d99df}$\rightarrow$} output.}
The sources of variation discussed above lead to semantic variation (\ie, different surface-form outputs with different meanings). 
However, given a fixed inferred meaning for the input and a fixed intended meaning, we may still observe variation in the output (\ie, different surface-form outputs that are semantic paraphrases). There are indeed many ways to express the same idea. Such meaning-preserving  variation stems from the \textit{non-determinism} of the cognitive processes involved in linguistic realisation \cite{levelt1993speaking}. For example, speakers' cognitive and social differences may lead to variability related to lexical and construction accessibility: \eg, choosing among different synonyms or between an active vs.\ a passive syntactic structure. Similarly, the cognitive effort of linguistic production itself may result in less than optimal outcomes including \textit{output errors} \cite{goldberg2022good}, which again are likely to lead to variation. 



\subsection{Sources of Uncertainty in Modelling Decisions}
\label{sec:uncertainty-modelling-decision}
There are many ways to model language production. These decisions lead to additional uncertainty about the plausibility of a generation. As discussed in \Cref{sec:background}, to represent, learn and reason about uncertainty, these modelling decisions must be included in the possible worlds. 
We identify three main sources of model uncertainty.


\paragraph{Model Specification.}
Uncertainty about decisions that influence the hypothesis space, including neural architectures such as attention, enforced sparsity, parameter tying, number of parameters, decoding algorithm, width, depth, factorising sequence-level probability, etc. 


\paragraph{Parameter Estimation.}
Uncertainty about parameters given a specified model. This includes decisions about optimisation objective, learning algorithm, training time (epochs), batch size, initialisation, etc.

\paragraph{Distribution Shift.}
Uncertainty due to a shift in distribution from observed data points to unseen data points. Shifts can be co-variate, where the input-output mapping remains constant but the input distribution shifts, or conditional, where the input distribution shifts. More details in \citep[\eg,][]{moreno2012unifying, hupkes2022state}.

\subsection{Aleatoric vs.\ Epistemic Uncertainty}
\label{sec:aleatoric-epistemic}
\textit{Aleatoric} and \textit{epistemic} uncertainty are popular terms in the deep learning community, and often used to categorise sources of uncertainty. Their precise definition, however, is not obvious \cite{der2009aleatory, hullermeier2021aleatoric}. Aleatoric uncertainty is often described as irreducible and related to data; epistemic as reducible and related to a model. We take the stance that this dichotomy \textbf{conflates two aspects} that are arguably orthogonal for most (if not all) sources of uncertainty. %
Namely, the agent's uncertainty being qualitatively different as a function of the \textbf{source} (\ie, the data being modelled or the model being used) and its \textbf{reducibility}. %
Reducibility depends on the ability and willingness of the practitioner to represent, learn and reduce a source of uncertainty, and, while the pathways to do so depend on the precise source, whether the source is related to data or model seems of little to no importance. 
For example, if uncertainty about parameters---clearly related to model  and often considered reducible---is not represented (\ie, not part of possible worlds), it cannot be reduced (unlike the common notion of epistemic uncertainty suggests); if annotator guidelines influence human response variability---clearly related to data and often considered irreducible---it \textit{can} be reduced (unlike the common notion of aleatoric uncertainty suggests).\footnote{Our view resonates with recent views in statistics: in concurrent work, \citet{gruber2023sources} echo that reducibility can only be discussed in the context of a choice of data and model, advocating examining different \emph{sources} of uncertainty.}

We propose a taxonomy beyond aleatoric/epistemic in \Cref{fig:spectrum} that depicts two dimensions. To the left we have sources that are linked to what the agent contemplates or interacts with (under the label `data'). To the right we have sources that are linked to the tools and assumptions the agent makes (under the label `model'). The boundary between the two is not always clear cut, as some sources can be regarded one way or the other (\eg, input error relates to data, but a model that encodes words at character level will be affected very differently than one that uses word2vec). The vertical axis captures reducibility and we illustrate with labelled arrows  examples of changes the agent can make in order to move a source along the reducibility dimension.