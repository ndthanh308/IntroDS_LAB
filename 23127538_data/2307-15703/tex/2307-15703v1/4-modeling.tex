\section{Applications in NLG}
\label{sec:applications}
We have laid down the theory and tools required to \textit{represent}, \textit{learn}, and \textit{reason about} uncertainty in \Cref{sec:background}, and characterised important \textit{sources} of uncertainty in NLG in \Cref{sec:sources}. 
We now highlight how to leverage this to create decision-making systems and evaluation frameworks that are flexible and representative of the diversity and complexity of language and its speakers, and can self-assess their own plausibility. We discuss applications exploiting disentangled representations of uncertainty related to \emph{data} in \Cref{sec:informed_decisions} and 
related to related to a \emph{model} in \Cref{sec:self_assessment}.


\subsection{Exploiting Data Uncertainty}
\label{sec:informed_decisions}
At its core, a good representation of uncertainty allows for informed decision making, weighing potential outcomes (\ie, possible worlds) against the risks or rewards associated with them. In NLG, the predictive distribution over tokens reflects a model's `beliefs' given modelling assumptions, data and learning algorithm. A decision is generating a string given a prompt by composing a sequence of tokens (\ie, \textit{decoding} token-level distributions). %

\subsubsection{Decoding}
A common decoding strategy is to search for the mode; the most-likely response under the statistical model (\eg, using beam search). However, this largely neglects the fact that models define a complex and rich representation of uncertainty. Consider two very different distributions with the same mode; one is mostly concentrated, the other is not. This information is ignored, which has been shown sub-optimal \cite[\eg][]{stahlberg-byrne-2019-nmt,eikema-aziz-2020-map,meister-etal-2020-beam}.
Decoding algorithms can increase diversity, quality, and similarity to the data by better utilising the full representation of uncertainty \cite[\eg][]{fan-etal-2018-hierarchical,Holtzman2020The,hewitt-etal-2022-truncation,li-etal-2016-diversity, zhang2018generating}; power utility-aware objectives \cite[\eg][]{eikema-aziz-2020-map, fernandes-etal-2022-quality, freitag2023epsilon,johnson2023ru};
or account for surface-form variation---for example in generative QA \cite{holtzman-etal-2021-surface}.


\subsubsection{Controllable Generation}
Disentangled representations of uncertainty provide valuable information about and control over types of variability in the data as captured by the model. This can further drive decision making. For example, a user might 1) not care about uncertainty about the syntactic structure of a response, but rather about uncertainty about how to interpret the input, or 2) adapt generations to adhere to a specific style.

\paragraph{Exploiting Latent Context.}
One approach for increased control and interpretability is to introduce a \textit{latent} variable in the possible worlds that represents a specific source of uncertainty. For example, a syntactically ambiguous input causes uncertainty about the response, but this could be decomposed into the uncertainty over structures and the uncertainty of the generation given a specific structure. Past work has attempted to model latent grammatical structures of sequences to guide generations \citep{kim-etal-2019-compound,kim-etal-2019-unsupervised}, for example by matching attention patterns to tree constituents \citep{bradbury2017towards, wang2019tree, ahmed2019improving, nguyen2020tree}. Others model input ambiguity, for example in MT \cite{stahlberg2022scones} or semantic parsing \citep{stengel2023zero}, or employ variational inference \citep{bowman-etal-2016-generating, zhang-etal-2016-variational-neural, calixto-etal-2019-latent, eikema2019auto, brazinskas-etal-2020-unsupervised} for attention mechanisms \citep{DengEtAl2018} and the morphology of the input \citep{Ataman2020A}.


\paragraph{Exploiting Observable Context.}
Sometimes we can exploit an \textit{observable} auxiliary ``context'' $C=c$ that helps reduce uncertainty about the response $Y$ (\eg, a speaker's age, the newspaper an article is published in, \etc). Unlike before we \emph{know} (or predict) $C=c$ and can therefore exploit it.\footnote{This is subtly different from the previous paragraph, where the total uncertainty (marginally across assignments of the auxiliary variable) remains the same---we only \textit{decompose} it and lack the knowledge to actually \textit{reduce} it.}
The realisation $c$ can take many forms, including but not limited to information about the speaker or social context \citep{zhang-etal-2018-personalizing, sennrich-etal-2016-controlling}, knowledge graphs \citep{moon-etal-2019-opendialkg} or unstructured documents (\eg Wikipedia in the case of \citealp{dinan2018wizard}) in dialogue; document-level context in translation \cite{wang-etal-2017-exploiting-cross,zhang-etal-2018-improving,sun-etal-2022-rethinking}. If it is not possible to provide $c$ explicitly, it can also be predicted or retrieved dynamically. 
For example, \citet{lewis2020rag} propose retrieval-augmented generation (RAG) models that are trained to retrieve relevant documents; \citet{yao2022react} propose to train models that execute Wikipedia search calls and condition on them. Later, \citet{schick2023toolformer} train models to execute API calls on other models, calculators, and Wikipedia search, and \citet{gao2022pal} train language models to translate mathematical reasoning tasks into executable programs that generate expected answers. 

Recently, with the rise of LLMs, $c$ often takes the form of \textit{instructions} expressed through natural language \citep{raffel2020exploring}. This powerful mechanism crucially allows us to condition on additional variables (thus revising uncertainty) simply by reformulating the input, without any changes to model design, and has been interpreted as models reasoning over the latent concepts in the context $c$ \citep{xie2022explanation} or being biased by its structure \citep{hahn2023theory}. For instance, steering generations  towards a specific style, length, or content by prompting with initial paragraphs \citep{radford2019language} or providing a detailed description \citep{bubeck2023sparks, zhou2023controlled}.


\subsubsection{Evaluating Decisions under Uncertainty}
\label{subsec:eval-variability}
Plausible response variability is not only challenging for generating high quality text, it also complicates automatic evaluation---especially for metrics that compare individual  generations to individual human responses.\footnote{A single outcome probable under the model is compared to a single outcome recorded in a dataset. Ideally, one compares events: sets of semantically equivalent outcomes, but defining these is non-trivial.} 

BLEU \cite{papineni-etal-2002-bleu}, ROUGE \cite{lin-2004-rouge}, METEOR \cite{banerjee-lavie-2005-meteor} and other word-overlap-based metrics remain de-facto standard for evaluating language generation but fail at capturing the diverse output space, especially for open-ended tasks \cite{gehrmann2022repairing}. Most datasets do not contain multiple references, and generations are rewarded for surface-level similarities to just one reference. Moreover, even datasets that do contain multiple references (observed outcomes) often show poor diversity, even though using multiple high-quality references can greatly improve the reliability of automatic metrics \cite{freitag-etal-2020-bleu}. %
We emphasise their importance when designing evaluation protocols and advocate for collecting multiple references per linguistic context to obtain a sense of plausible variability, as well as establishing datasets that represent different sources of uncertainty. Not observing variability in a dataset does not mean that it does not exist: it is simply a form of data sparsity.

\paragraph{Learned Metrics.}
Learned metrics such as BERTScore \cite{Zhang2020BERTScore}, BLEURT \cite{sellam-etal-2020-bleurt}, or COMET \cite{rei-etal-2020-comet}, have been proposed to overcome the shortcomings of overlap-based metrics but are themselves trained on data that might not reflect sufficient diversity \cite{gehrmann2022repairing}. Therefore, their reliability is heavily dependent upon their ability to generalise, which has been questioned in recent works \citep[inter alia]{amrhein-sennrich-2022-identifying, he2023blind}. Therefore \citet{glushkova-etal-2021-uncertainty-aware} advocate for modelling uncertainty in such metrics too, with \citet{zerva-etal-2022-disentangling} extending it to disentangling different types of uncertainty. %

\paragraph{Statistical Evaluation.}
An interesting evaluation framework complementary to standard NLG evaluation, especially for open-ended tasks, compares \textit{distributions of statistics} in machine generated text to those in human produced text. Usually, this comparison is done globally across an entire test set. For example, by comparing surface-form text statistics like type-token and rank-frequency ratio \cite{meister-cotterell-2021-language}, or by mapping generations and references to an embedding space and comparing the distribution over embedding clusters rather than over strings \cite{pillutla2021mauve,xiang-etal-2021-assessing, pimentel2023on}. 
Some even evaluate generations against human responses \textit{for individual inputs}. For example, by quantifying lexical, syntactic and semantic variability between multiple samples of generation and reference \cite{giulianelli2023comes}; comparing section structure, topics and coreference chains in long-form generation \cite{deng-etal-2022-model}; or comparing sequence length and topic structure \cite{barkhof2022statistical}.




\subsection{Exploiting Model Uncertainty}
\label{sec:self_assessment}
The previous section highlighted applications of disentangled uncertainty about \textit{data} in NLG. Now, we turn towards applications of uncertainty related to \textit{models}. These are often described as mechanisms to ``know what is known'', and revolve around \textit{trustworthiness} of model predictions. Applications of self-assessments include communicating model uncertainty to users to increase trust and transparency \cite{bhatt-uncertainty-2021}, allowing models to selectively answer to reduce errors \cite{el2010foundations}, or improve learning in low-resource settings by selecting data points with high model uncertainty \cite{lewis1995sequential}. Self-assessment in NLG is particularly hard due to the unbounded sample space (all strings); token-level factorised probability; huge models; and high data uncertainty. We discuss several approaches to representing and learning these sources of uncertainty. 


\subsubsection{Calibration}
\label{sec:predictive-distribution-and-calibration}
The predictive distribution itself is often taken as a measure of uncertainty indicative of model error, although it is not obvious why this should be the case, as it accounts neither for model specification, nor for parameter estimation as part of the possible worlds. \textit{Calibration} is a frequentist framework that attempts to push probabilities towards the relative frequencies with which predictions are judged to be correct \cite[\textit{inter alia};][]{eisape2020cloze, dhuliawala-etal-2022-calibration, lee-etal-2022-adaptive, zhao2022calibrating}, with various proposed methods such as temperature scaling, label smoothing and knowledge distillation. Recently, \citet{kuhn2023semantic, lin2023generating} show that entropy over clusters of semantically equivalent generations---such a cluster is a good example of an event in NLG---rather than their surface forms is more predictive of model error for QA. These approaches do not disentangle data from model uncertainty, and potentially interfere with good representations of data uncertainty (\Cref{sec:informed_decisions}).


\subsubsection{Conformal Prediction}\label{sec:conformal-prediction}
Another frequentist framework that exploits the predictive distribution 
sorts outcomes by their predicted probability and adds them to a set until some threshold of mass---\eg, 90 \%---is reached.
Though attractive due to its simplicity \citep{kompa2021empirical, ulmer2022exploring}, 
the sets or intervals might be miscalibrated or not contain plausible outcomes. \emph{Conformal prediction} \citep{vovk2005algorithmic, papadopoulos2002inductive, angelopoulos2021gentle} 
uses a held-out calibration set on which the ideal threshold or intervals are computed and guarantees that a plausible prediction will be included with a predefined probability, in expectation. Recently, some works have applied conformal prediction to quality estimation for MT  \citep{giovannotti2023evaluating, zerva2023conformalizing} or even language generation directly \citep{ravfogel2023conformal, quach2023conformal, ren2023robots}.


\subsubsection{Bayesian Inference}

Bayesian methods (\Cref{sec:background} \textbf{Statistics})
offer principled and disentangled representations of uncertainty about the model parameters and the data by entertaining the possibility of multiple parameter settings %
and specifying a probability distribution over them.
Roughly speaking, the spread of the distribution over parameters (or the variance of the generations given a different \textit{sample} of parameters) provides information about parameter uncertainty and can be used to improve active learning \cite{ambati2012active,lyu2020you,gidiotis-tsoumakas-2022-trust}, alleviate hallucinations \cite{xiao2021hallucination}, or provide uncertainty about evaluation metrics \cite{fomicheva2020unsupervised,glushkova-etal-2021-uncertainty-aware, zerva-etal-2022-disentangling}. 




In practice, however, the posterior is intractable, motivating approximate methods. For example, based on variational inference \cite{jordan1999introduction}:
\citet{gal2016theoretically} use Monte-Carlo Dropout, \citet{fortunato2017bayesian} extend Bayes-by-Backprop \cite{blundell2015weight} to recurrent models, and \citet{gan2017scalable} use MCMC \cite{robert1999monte}.
\citet{sankararaman2022bayesformer, xiao2020wat} apply MC Dropout to Transformers. 
\citet{zablotskaia2023uncertainty} benchmark methods like Gaussian Process \cite{rasmussen2006gaussian} output layers \cite{liu2020sngp} and how they improve the quality of model uncertainty in summarisation models.
\citet{malinin2020uncertainty} formalise different information-theoretic measures of parameter uncertainty for structured prediction, and in particular their estimators.%


\subsubsection{Verbalised Uncertainty}
Because obtaining probabilities that comply with specific interpretations is difficult (\Cref{sec:background}), an exciting research direction is to re-express the representation with non numerical qualifiers, for example, linguistic expressions such as ``I think'', ``undoubtedly'', or ``high confidence''. \citet{mielke2022reducing} collect human judgements to quantify verbalised uncertainty and quality; measure their correlation; and employ controlled generation to re-generate responses with more aligned verbal expressions of uncertainty. \citet{zhou2023navigating} investigate the effect of adding linguistic (un)certainty markers to prompts, and \citet{lin2022teaching} prompt NLG models to provide a verbal expression of confidence in their response. \citet{kadavath2022language} sample multiple generations and design a prompt asking if one of the proposed answers is true. Then, they assess whether that probability correlates with quality. Although potentially particularly friendly to non-expert users, such re-expressions are not yet well understood, \eg, may not be faithful to the underlying representation of uncertainty they are sampled from.



\subsubsection{Evaluating Self-assessment}
\label{sec:eval_self_unc}
Evaluating the quality of model uncertainty summaries is difficult, because ground truth usually does not exist. Therefore, proxy tasks like error detection \cite{malinin2020uncertainty}, out of distribution detection \citep{malinin2020uncertainty, lahlou2021deup, ulmer2022exploring, van2022benchmarking}, and active learning \cite{osband2023finetuning} are often used.

\paragraph{Error Detection.}
Various NLG studies evaluate how indicative predictive probabilities are of errors or generation quality \cite{jiang2021can, si2023prompting, chen2022close, kumar2019calibration}. This is often operationalised with the Expected Calibration Error \citep[ECE;][]{naeini2015obtaining,guo2017calibration}, which measures whether average predicted probability aligns with model accuracy for groups of predictions. However, ECE's desideratum was recently shown to be unreliable in settings with annotator disagreement as it disregards data uncertainty \citep{baan2022stop}, which might make it ill-suited for NLG. Furthermore %
since evaluating the quality of generations is difficult (\Cref{subsec:eval-variability}), %
notions of correlation between probability and quality suffer from the same issue \cite{jiang2021can, bulian2022tomayto}. Others investigate if predictive probabilities indicate critical errors (such as hallucinations or significant deviations in meaning) in MT \citep{guerreiro2022looking}, abstractive summarisation \citep{van2022mutual}, image captioning \citep{xiao2021hallucination}, and common-sense knowledge \cite{yoshikawa2023selective}. An evaluation framework that is gaining traction is selective answering, \ie, detecting when a model should avoid responding \cite{kamath2020selective, cole2023selectively, kuhn2023semantic, ren2023outofdistribution}, often operationalised with area under the curve of the accuracy/rejection trade-off. 

\paragraph{Out of Distribution Detection.}
The second proxy task is assessing how model uncertainty differs for samples in in-distribution and out-of-distribution settings; essentially performing anomaly detection. The expectation is that models should be uncertain on OOD instances \citep{malinin2020uncertainty, lahlou2021deup, ulmer2022exploring, van2022benchmarking, ren2023outofdistribution}. This task evaluates mostly distribution shift (\Cref{sec:uncertainty-modelling-decision}).

\paragraph{Active Learning.}
Active learning attempts to select the most ``informative'' data instances from which a model will learn the most. The goal is to achieve comparable (or better) performance with less training instances \cite{lewis1995sequential, houlsby2011bayesian, siddhant-lipton-2018-deep,osband2023finetuning}. One way to define informativeness is with model uncertainty. The quality of the quantities that summarise model uncertainty (\eg, parameter uncertainty) can be used as selection criteria, where better active learning performance implies higher quality. However, its efficacy has been questioned as it conflates inference and model problems \cite{yao2019quality} and can under perform uniform sampling both from an empirical and theoretic perspective \cite{tifrea2022uniform}.
