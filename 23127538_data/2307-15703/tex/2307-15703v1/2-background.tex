\section{Uncertainty}
\label{sec:background}
Uncertainty is a rich concept that has received various reasonable treatments before today's understanding of it.\footnote{From uncertainty's connection to (mostly abandoned) views on what is `knowable' \citep{knight1921risk}, to its central role in decision theories  \citep{ramsey1931foundations,von1947theory,Wald1951StatisticalDF,bernardo1994bayesian} and mathematical statistics \citep{savage1972foundations} to its modern understanding in terms of state of knowledge \citep{morgan_henrion_1990,lindley2013understanding}, to its  mathematical representation detached from philosophical interpretation \citep{halpern2017reasoning}.} We begin discussing it through common language. The online edition of the Oxford English dictionary listed five senses of uncertainty (retrieved in May 2023), two of which we partly quote here (those general enough to include the others as special case): \emph{(i) the state of not being definitely known or perfectly clear}; and \emph{(ii) the state or character of being uncertain in mind}. Both definitions regard uncertainty as \emph{a state of affairs}: in \emph{(i)}, the state of the world; in \emph{(ii)}, the state of an agent contemplating the world. They are subtly different: \emph{(i)} encompasses situations of inherent randomness (\eg, the result of a coin flip), \emph{(ii)} concerns one's inability to predict the state of the world regardless of any inherent randomness (\eg, a reader wondering about the content of the next paragraph). As we shall see, this difference leads to rather different interpretations %
of uncertainty as an aspect of reality. Yet, at the level of mathematical treatment, they share the same formal devices. Hence, with no loss of generality, we choose to talk about uncertainty from the point of view of an agent contemplating or interacting with the world, while possessing limited knowledge about it. Our presentation is inspired by various reference texts,  in particular, \citet{dubois2009formal} and \citet{halpern2017reasoning}. 

\paragraph{Agents.} We posit that any one agent shall represent the state of their knowledge in a way sufficient for reasoning about the truth value of claims (or propositions) that they make about aspects of the world. In particular, the agent is able to state their preference for claims they find themselves less uncertain about (\ie, possessing better information about those).\footnote{Agents and worlds are abstractions to be adapted and tailored to each application, commonly in NLG an agent is a model and a world is a response to a given prompt.}  An agent then uses this \emph{uncertainty representation} to interact with the environment (\eg, inform their actions) and, when they acquire new knowledge, they update the representation in a coherent manner. 
To illustrate formal concepts, we use three example agents. \textbf{A1}\hspace{0.5mm}\twemoji{game die} rolls a six-sided die; we seek to represent their state of knowledge about the outcome. \textbf{A2}\hspace{0.5mm}\twemoji{busts in silhouette} resolves mentions of entities to unique names in a knowledge base (KB); we seek to represent their state of knowledge about entity names given any one mention. Last, \textbf{A3}\hspace{0.5mm}\twemoji{speech balloon} provides written answers to questions; we seek to represent their state of knowledge about answers given any one question. 
For simplicity, we assume that our agents already acquired their knowledge, by means which are not relevant for now, and their state of knowledge is frozen.  
We begin by outlining the formal tools common to all frameworks for uncertainty representation we are aware of, we then zoom into the most commonly used framework (probability) and discuss the role of statistics in acquisition and revision of knowledge.  

\paragraph{Possible worlds.} Our agent does not know the state of the actual world, but they assume that it must be one of a collection of possible worlds (the universe). They represent a world as a unique symbol (or string, or collection of attributes; the level of detail being dictated by the agent's needs), and the universe of what is possible as a set $\Omega$ of mutually exclusive worlds.\footnote{This framework, \textit{possible worlds}, is familiar to linguists and philosophers alike \citep{hintikka1957modality,hintikka1961modality,sep-possible-worlds}.
} 
\textbf{A1}\hspace{0.5mm}\twemoji{game die}  might represent a world as a symbol $f_k$, with $k$ denoting the number of pips the die shows as a result of the roll; they might assume the die always lands showing one of six numbered faces and thus take $\{f_1, \ldots, f_6\}$ to represent all possible worlds. 
For \textbf{A2}\hspace{0.5mm}\twemoji{busts in silhouette}, a world is a symbol like $e_i$, with $i$ denoting an entity's identifier (\eg, a standardised unique name), and the universe is the finite set of entities in the English Wikipedia. 
For \textbf{A3}\hspace{0.5mm}\twemoji{speech balloon}, a world is a symbol like $u_s$, with $s$ an English sentence produced in response to a question. This agent happens to be unable to describe the set of all valid English sentences (they cannot enumerate its elements nor state a finite set of properties that all valid sentences must satisfy). Motivated by convenience, \textbf{A3}\hspace{0.5mm}\twemoji{speech balloon} uses a set large enough to encompass most of it while being specifiable in a compact manner: the set of all finite-length strings made by concatenation of known symbols (\eg, words, punctuation, \etc). These examples show that the agent's choice of universe can be a difficult one, often requiring simplifying assumptions: on soft or irregular terrain, a die could land on an edge; a KB may be incomplete (sometimes in known ways, \eg, under-representing the contributions of Black women to science); a regular language is a too loosely constrained representation of the English language (\eg, it includes infinitely many strings that will never correspond to any actual world). 

\paragraph{Propositions.} The possible worlds framework gives agents a mechanism to represent claims about specific aspects of the world. A \emph{proposition}  $E_t$ is the claim that the actual world $\omega$ is one where some property $t$ holds (which we denote $t(\omega)$). A property is something that can be assessed for any one world (\eg, $f_2$ is even and prime, $e_{\operatorname{Katherine\_Johnson}}$ is African-American, female and mathematician, $u_\text{`Biden is the 46th US president'}$ expresses the relation $\operatorname{presidentof}(\operatorname{Joe\_Biden}, \operatorname{USA})$). Not knowing the state of the actual world,  our agent represents $E_t$ by the set  $E_t = \{w \in \Omega: t(w)\} \subseteq \Omega$ of all possible worlds where the property holds. If the agent knew the state of the actual world $\omega$, then the truth value of the proposition would be determined simply by set membership (\ie, $\omega \in E_t$ or $\omega \not\in E_t$). 
For example, \textbf{A1}\hspace{0.5mm}\twemoji{game die}  represents the claim ``the roll is odd'' as $E_{\text{odd}} = \{f_1, f_3, f_5\}$.
\textbf{A2}\hspace{0.5mm}\twemoji{busts in silhouette} represents the claim ``mention to a female mathematician'' by the set $\{e_i \in \Omega: \operatorname{female}(e_i) \wedge \operatorname{mathematician}(e_i)\}$. %
\textbf{A3}\hspace{0.5mm}\twemoji{speech balloon} might use equivalence classes, for example, they use the set $E_a = \{u_s \in \Omega : \operatorname{equivalent}_{a}(u_s)\}$ to claim that the answer is a sentence semantically equivalent to some other sentence $u_a \in \Omega$ (\eg, $u_\text{`The 46th US president is Joe Biden'}$). Because propositions are semantic in nature, they can be difficult to represent explicitly. For example, \textbf{A3}\hspace{0.5mm}\twemoji{speech balloon}'s equivalence classes require sophisticated natural language understanding. 
A representation of all propositions an agent deems possible is a set $\mathcal E$ of subsets of $\Omega$.\footnote{If an agent has no knowledge of the impossibility of any proposition, or does not care to exclude those from the representation, the powerset of (countable) $\Omega$ is a reasonable choice for $\mathcal E$. In NLG, we often implicitly make this choice.}

\paragraph{Preferences.} The agent's imperfect knowledge of the actual world $\omega$ translates to limited knowledge about propositions. However, the agent's ignorance is qualitatively different depending on the claims they make. Intuitively, some claims are compatible with many of the possible worlds, while others hold in but a few (\eg, \textbf{A1}\hspace{0.5mm}\twemoji{game die}  knows that only one prime number is even), and though the various worlds are all possible, they may not be equally plausible (\eg, \textbf{A2}\hspace{0.5mm}\twemoji{busts in silhouette} knows that most mentions resolve to politicians, \textbf{A3}\hspace{0.5mm}\twemoji{speech balloon} knows that most answers are only a few words long), \etc.
Considerations of those kinds motivate an agent to express a \emph{preference} for claims they find themselves less uncertain about (\ie, possessing better information about those). The agent does so by prescribing a \emph{plausibility measure} \citep{friedman96}, a function that attaches a token of uncertainty---a qualifier that the agent knows how to sort---to each proposition in $\mathcal E$. Plausibility measures are very diverse, the most well known instance of it being axiomatic probability \citep{kolmogorov1960foundations}.\footnote{Other plausibility measures include belief functions \citep{shafer76}, possibility measures \citep{duboisprade90}, ordinal ranking functions \citep{GoldszmidtPearl92} and (non-numerical) preference orders \citep{friedman96}. 
Concrete instances of plausibility measures vary in descriptive power. Under certain documented assumptions \citep{friedman96}, they enable something like a `calculus of uncertainty' which formalises the procedures the agent must follow to incorporate additional information about the world and revise their uncertainty representation coherently (in axiomatic probability, this is known as \emph{conditioning}).} 

\paragraph{Probability.} Probability is a numerical qualifier that we can attach to events in $\mathcal E$.\footnote{In probability, propositions are  \emph{events}, worlds are \emph{outcomes} and universes are \emph{sample spaces}.} 
For any event, this qualifier is a positive real number bounded to be at most $1$. Probability values inherit various properties of real numbers: we can add, multiply and sort them. A function $\Pr$ over $\mathcal E$ is a \emph{probability measure} if a) it maps $\Omega$ to $1$, and b) it maps any two disjoint sets $U$ and $V$ in $\mathcal E$ to $\Pr(U)+\Pr(V)$, which is known as additivity. 
 Additivity, in particular, implies that we can identify a probability measure over all of $\mathcal E$ by assigning probability to elementary outcomes in $\Omega$, for example, using a probability mass function (pmf) or probability density function (pdf; for uncountable $\Omega$). This has massive consequences for uncertainty representation, since working with elementary outcomes is much simpler than working with sets of outcomes (for example, difficulty in prescribing equivalence classes such as `all sentences that talk about Joe Biden' need not stop \textbf{A3}\hspace{0.5mm}\twemoji{speech balloon} from identifying a probability measure for their reasoning needs).
 
\paragraph{Interpretations.} 
Probability has been motivated and justified from different angles, each building on a specific interpretation of what probability as a number must signify \citep{hacking1975emergence}. However different they are, they all lead to the same formal device. Under certain idealisations, \textit{objectivists} regard events as \emph{repeatable} (\eg, we may prompt a human speaker multiple times). Repetitions allow an agent to perceive what may be thought of as an inherent \emph{property} of an event: its \emph{frequency} in a large enough number of repetitions. The \emph{subjectivist} interpretation \citep{ramsey1931foundations,definetti2017theory} views probability as a degree of belief, personal to an agent, and deprived of any interpretation beyond its formal role as an expression of the agent's preferences.\footnote{Dictionary definition \textit{(i)} is objectivist; \textit{(ii)} subjectivist.} 
Different interpretations have an impact on the procedures that an agent acknowledges as logical or rational for knowledge acquisition and revision, as we discuss next.

\paragraph{Statistics.} We have described the general tools that agents can use to represent and convey their uncertainty. But where do their preferences (probabilities, in particular) come from? The \emph{Frequentist} agent is essentially an objectivist who assumes the existence of a precise statistical law that describes the phenomena in consideration. They assume to have access to this law up to an unknown parameter $\theta^\star \in  \mathbb R^D$. %
Given a parameter $\theta$, their preferences are specified via a pmf (or pdf) $p(x|\theta)$. Given data $\mathbf x = \langle x_1, \ldots, x_N \rangle$, this law identifies the so called likelihood function $\ell_{\mathbf x}(\theta) = \prod_n p(x_n|\theta)$, a measure of the compatibility between observed data $\mathbf x$ and the statistical model identified by $\theta$. The agent uses $\mathbf x$ to estimate the parameter $\theta^\star$: they pick the parameter $\hat\theta$ that maximises the likelihood function, %
this is known as maximum likelihood estimation (MLE). They do not entertain parameters as part of the possible worlds, hence have no uncertainty representation about them. Given the  parameter  estimate $\hat\theta$, the agent uses $p(x_{n+1}|\hat\theta)$ to make predictive inferences about future data $x_{n+1}$. When necessary (\eg, the agent suspects to have found a better statistical law), the agent studies properties of their parameter estimator(s) by repeated experimentation, for example to establish confidence intervals %
 and other tools for model selection (see for example \citealp{lehmann1993fisher}).
The \emph{Bayesian} agent, a subjectivist, %
also picks a statistical law, but makes no assumption about its correctness. Given some data $\mathbf x$, they too construct a likelihood function $\ell_{\mathbf x}(\theta)$, but use it differently. As a formal tool, probability comes with a mechanism for belief revision: conditioning. %
To make use of it, the agent augments their possible worlds to include possible values of $\theta$ and its interaction with possible values of the observable variable, they then state their preferences over parameters in the form of a pdf $p(\theta)$. %
This is called a \emph{prior} (conveys one's knowledge and experience before observing $\mathbf x$).
The agent then revises their preferences using Bayes rule %
to obtain a posterior pdf $p(\theta|\mathbf x) \propto p(\theta)\ell_{\mathbf x}(\theta)$. This object supports all inferences the agent will ever make (\eg, 
about parameters, or about future data $x_{n+1}$---for which the agent builds a posterior predictive function  $p(x_{n+1}|\mathbf x) = \int p(x_{n+1}|\theta)p(\theta|\mathbf x) \dd{\theta}$).
In essence, Frequentist procedures revolve around point estimation (\eg, MLE) and null hypothesis significance testing \citep{LehmCase98,lehmann2005testing}, %
Bayesian theory \citep{bernardo1994bayesian} and practice \citep{gelmanbda04}, instead, frame statistical inference as an iterative process of belief revision  (\eg, conditioning, marginalisation, expectation). 


\paragraph{Natural Language Generation.} 
Most NLG models (like \textbf{A3}\hspace{0.5mm}\twemoji{speech balloon}) acquire knowledge through MLE. Alternatives include Bayesian inference \cite[\eg,][]{malinin2020uncertainty,sankararaman2022bayesformer} and utility- and reward-based training (\eg, minimum risk \citep{shen-etal-2016-minimum},  reinforcement learning \citep{paulus2018a}). Recently, pre-training on enormous unlabelled corpora, and reinforcement learning from human feedback \cite[RLHF, \eg,][]{christiano2017deep,stiennon2020learning,ouyang2022training} or \textit{instruction tuning} \cite[\eg,][]{mishra-etal-2022-cross,wei2022finetuned} have become popular to refine the representation of uncertainty towards something that decodes more easily into strings preferred by human users. %

Generating a response is simulating an outcome. %
The event space is the powerset of all token sequences from a fixed vocabulary \cite[BPE tokens, \eg,][]{sennrich-etal-2016-neural}. Rather than prescribing a probability measure (mapping each event to a probability value) directly, we parameterise a pmf (typically parameterised via an autoregressive factorisation of the probability of any one sequence) with a neural network and exploit countable additivity to assign probability to any event (\eg, all token sequences that map to the same sentence \citep{cao-rimell-2021-evaluate,chirkova-etal-2023-marginalize} or all sentences that map to the same meaning representation \cite{kuhn2023semantic}). %


\paragraph{Key Takeaways.}
(1) Uncertainty is a state to be represented. %
(2) To represent uncertainty about something observable or not (\eg, responses, parameters, modelling assumptions) we need to acknowledge and order a whole space of alternatives: our choice of possible worlds must capture interaction amongst possible values of the variables we aim to express our uncertainty about. 
(3) Probability is not constrained to abide by any one interpretation. To regard probabilities in a specific human-interpretable way (\eg, relative frequencies), we need learning techniques yielding that result, and we need to verify that our setting actually meets all necessary conditions (\eg, the Frequentist interpretation of probability is sensitive to modelling choices, local optimality, and  data sparsity: most practical NLG agents are unable to meet the necessary formal requirements).