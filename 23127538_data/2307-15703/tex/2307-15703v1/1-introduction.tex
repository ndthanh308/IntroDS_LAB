\section{Introduction}

Natural Language Generation (NLG) has long been one of the ultimate goals of artificial intelligence, exemplified by the Turing test \citep{turing1950computing} and systems such as ELIZA \citep{weizenbaum1966eliza} and Watson \citep{high2012era}. It includes a vast number of applications like translation, 
summarisation, question answering, story telling and image captioning. Recently, NLG systems are gaining traction as general purpose interfaces through which users can interact with any application using natural language (\eg, Google's Bard, OpenAI's ChatGPT and Meta's LLaMA; \citealp{google_bard, openai_chatgpt, touvron2023llama, touvron2023llama2}). Their widespread use makes it increasingly important to build NLG systems that are trustworthy and representative of the diversity of its users \citep{bhatt-uncertainty-2021,jacovi2021formalizing, liao2023ai}. 

In this paper, we argue that an explicit and principled treatment of \textit{uncertainty} can assist in creating systems better aligned with these goals. For example by enriching predictions with a degree of confidence that is predictive of error, and ensuring that multiple views, backgrounds and writing styles from diverse populations are well represented. Practical applications of high quality, and, importantly, \textit{disentangled} representations of uncertainty include the ability to abstain from answering, ask clarification questions, or defer a decision; to predict a distribution over a range of human interpretations and perspectives, or to cater to specific users through controllable generation.

One key aspect of human language that makes a principled treatment of uncertainty in NLG models particularly important and challenging is that given a context---for example, a dialogue history---there are many different ways in which someone might respond  (\eg, \citealp{scovel1998psycholinguistics}, p. 37). From the perspective of an NLG system, this causes \textit{uncertainty} about the desired response \citep{ferreira2000effect}. Similar observations have been made in classification, where annotators might pick a different but equally plausible class label given a task and an input text---a line of work recently united under the term \textit{human label variation} \cite{plank-2022-problem}. In NLG, however, the unbounded output space (all possible strings) and enormously open-ended tasks make plausible response variability particularly high \cite{giulianelli2023comes}. On top of uncertainty due to rich variability in language use, other sources of uncertainty stem from lack of knowledge about the plausibility of the NLG system itself, for example, due to neural architecture or parameter setting. Most NLG models, however, have no disentangled representation---or no representation at all---for these sources of uncertainty. 

Despite its importance, (survey) papers on uncertainty in machine learning do not discuss NLG \cite{abdar-review-2020, gawlikowski-survey-2021, hullermeier2021aleatoric, mena-survey-2022, gruber2023sources}. Likewise, those about NLG do not discuss uncertainty \cite{gatt-survey-2018, celikyilmaz-evaluation-2020, hi-survey-2022, erdem-neural-2022, dong-survey-2022}. One exception is concurrent work by \citet{hu2023uncertainty} from which we differ by presenting a comprehensive theoretical background and identifying the main sources of uncertainty in NLG from a linguistic perspective, with complementary applications.

We summarise our contributions as follows. To encourage a principled and shared understanding of uncertainty, we provide a detailed account of uncertainty, and how to represent, learn and reason about it in \Cref{sec:background}. We characterise the main sources of uncertainty in NLG from a linguistic perspective by proposing the \textit{double triangle of language production}, and a fluid, two-dimensional taxonomy that is more informative and faithful than the popular aleatoric/epistemic dichotomy in \Cref{sec:sources}. Finally, we move from theory to exciting applications of (disentangled) representations of uncertainty, such as decoding, controllable generation, self-assessment, selective answering, and active learning in \Cref{sec:applications}.