% ********************************************************************
% *                  Format for IMVIP 2021  papers,                  *
% *                  based on the IMVIP 2014, 2015, 2020 templates   *
% ********************************************************************
\documentclass[a4paper,11pt]{article}



\setlength{\topmargin}{-0.5cm}
\setlength{\headsep}{.5cm}
%\setlength{\footskip}{1.0cm}
\setlength{\textheight}{24cm}
\setlength{\textwidth}{17cm}
\setlength{\evensidemargin}{-.5cm}
\setlength{\oddsidemargin}{-.5cm}


% \usepackage{natbib}
\usepackage{fourier}
\usepackage{color}
 \usepackage{graphicx}
\usepackage{url}
\usepackage[affil-it]{authblk}
\usepackage{amsmath}
\usepackage{wrapfig}
\usepackage{hyperref}
\usepackage[T1]{fontenc}
\usepackage{times}
\usepackage{fancyhdr}
% \usepackage{minted}
\pagestyle{fancy}
\usepackage{titlesec}
\usepackage{multirow}
\fancyhf{}
\setlength{\headheight}{13.59999pt}
\addtolength{\topmargin}{-1.59999pt}

% \rfoot{\thepage} % Change this line to change the position of the page number
\chead{\thepage}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}
% \renewcommand{\thesubsubsection}{\thesubsection.\arabic{subsubsection}}

% \pagestyle{empty}


%%%%
\begin{document}
%Lean & Potent: Combining Graph Neural Networks and Edge Convolution for Optimized Medical Image Interpretation
%Compact & Capable: Unleashing the Synergy of Graph Neural Networks and Edge Convolution for Advanced Medical Image Insights
\title{Compact \& Capable: Harnessing Graph Neural Networks and Edge Convolution for Medical Image Classification}

\author{Aryan Singh}
\author{Pepijn Van de Ven}
\author{Ciar√°n Eising}
\author{Patrick Denny}
\affil{University of Limerick}
\date{}
\maketitle
\thispagestyle{empty}



\begin{abstract}
Graph-based neural network models are gaining traction in the field of representation learning due to their ability to uncover latent topological relationships between entities that are otherwise challenging to identify. These models have been employed across a diverse range of domains, encompassing drug discovery, protein interactions, semantic segmentation, and fluid dynamics research. In this study, we investigate the potential of Graph Neural Networks (GNNs) for medical image classification. We introduce a novel model that combines GNNs and edge convolution, leveraging the interconnectedness of RGB channel feature values to strongly represent connections between crucial graph nodes. Our proposed model not only performs on par with state-of-the-art Deep Neural Networks (DNNs) but does so with 1000 times fewer parameters, resulting in reduced training time and data requirements. We compare our Graph Convolutional Neural Network (GCNN) to pre-trained DNNs for classifying MedMNIST dataset classes, revealing promising prospects for GNNs in medical image analysis. Our results also encourage further exploration of advanced graph-based models such as Graph Attention Networks (GAT) and Graph Auto-Encoders in the medical imaging domain. The proposed model yields more reliable, interpretable, and accurate outcomes for tasks like semantic segmentation and image classification compared to simpler GCNNs. Code available at \href{https://github.com/anonrepo-keeper/GCNN-EC}{AnonRepo}.
% This is the Latex  template to be used for submitting a paper to the
% Irish Machine Vision and Image Processing (IMVIP). The conference website is at
% \url{https://imvipconference.github.io/}. This  template is mainly filled in with \textit{lorem
% ipsum} to illustrate the expected layout of the publication to submit.
% Some examples for inserting and referring to equations, bibliographic references, tables and
% figures are given.  
% Should an author  prefer to use a text editor other than Latex (not
% advised), please keep the same look and dimensions. 
% This template can be compiled with pdflatex \& bibtex to create the final paper
% in pdf format. Full papers must be a 
% \textbf{maximum of 8 pages (assuming that the last two pages are only references and figures)}.  
\end{abstract}
\textbf{Keywords:} Medical Imaging, Machine Vision, GNN, GCNN, Image classification. 



%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
% \begin{wrapfigure}{r}{0.5\textwidth}
%   \vspace{-20pt}
%   \begin{center}
%     % Figure removed
% \end{center}
% \vspace{-20pt}
%   \caption{Introduction Banner.}
%   \vspace{-0pt}
% \end{wrapfigure}
%%%%%%%%%%%%%%%%%%%%%%
Medical image classification and segmentation play critical roles in the field of medical imaging. Although there have been considerable advancements in image classification, medical image classification faces unique challenges due to the diverse dataset modalities, such as X-ray, Positron Emission Tomography (PET), Magnetic Resonance Imaging (MRI), Ultrasound (US), and Computed Tomography (CT). Variations within and between modalities, mainly stemming from the inherent differences in imaging technologies, complicate the classification process. Additionally, obtaining labeled training data is costly in the medical domain. Pre-trained DNNs address these issues through transfer learning techniques, yielding impressive results. However, DNNs exhibit limitations, including inductive bias, inefficient capture of spatial and local-level associations, and inconsistent performance across modalities \cite{cnnlim,Zhou_2021}.

GNNs offer a solution to these complexities, handling variations in data with embedded relationships effectively and accommodating heterogeneous graph nodes\cite{hetro1}. The successful application of Knowledge-Based Graph Methods in medical diagnosis supports this notion. We have compared GNN architecture with Convolutional Neural Networks (CNNs) and discussed various types of Graph Convolutional Neural Networks (GCNNs). We propose a GCNN model integrated with Edge Convolution \cite{edcnn}(GCNN-EC) for medical image classification. By performing graph convolution and edge convolution for edge prediction. Edge convolution overcomes the limitations of vanilla GCNN thus improving classification. Our method enhances model performance with reduced training time and data requirements. This research validates graph-based learning's efficiency for medical image data.  In this study, we focus on classifying the MedMNIST dataset \cite{medmnist}, featuring 10 pre-processed datasets from various sources and modalities, with 708,069 images in 12 2D datasets. We narrow our research scope to six categories/classes\ref{fig:medm} containing 58,954 images with dimensions 28x28 as these classes represent diverse modalities, reflecting the compilation of images from various imaging techniques. These classes are AbdomenCT, BreastMRI, CXR, ChestCT, Hand, and HeadCT. The subsets are balanced.

We observe that our simple GCNN-EC outperforms leading state-of-the-art DNNs for specific MedMNIST dataset classes. Proposed model required less training than compared DNNs while using 100 times fewer parameters.  

% We compare GNN architecture with Convolutional Neural Networks (CNNs) and discuss various types of Graph Convolutional Neural Networks (GCNNs). We propose a GCNN model integrated with Edge Convolution \cite{edcnn}(GCNN-EC) for medical image classification. By performing graph convolution and edge convolution for edge prediction, our method enhances model performance. 
% With reduced training time and data requirements, our method achieves comparable results to DNNs, and edge convolution improves classification. 

% Figure environment removed

\vspace{-10pt}

\section{Prior Art}
In this section, we will delve into the technicalities of CNNs, and compare their mechanisms with GCNNs. We will also shed light on three contemporary, state-of-the-art CNN models that have been utilized for the task of medical image classification. Furthermore, this section will introduce the diverse variants of GNNs, providing a comprehensive comparison from a technical standpoint. It will also cover the various applications of these models in medical domains.
% \bigbreak
\subsection{CNNs}
CNNs\cite{cnn} owe their name to the convolution operation, which involves overlaying a kernel onto the image grid and sliding it across the grid to extract local information, such as details from neighboring pixels. Technically, the convolution operation involves performing a dot product between the filter's elements and the corresponding elements of the image grid, then storing the result in an output matrix (often termed a feature map or convolved feature). As illustrated in Figure \ref{fig:cnn}, the dot product employed in the convolution process is an aggregation operation. The main objective of this operation is to consolidate image data into a compressed form, making it feasible to extract global-level features from an image.

\begin{wrapfigure}{r}{0.5\linewidth}
 \vspace{-3mm}
 \centering
 % Figure removed
 \vspace{-3mm}
 \caption{
 CNN architecture.
}
 \label{fig:cnn}
 \vspace{-3mm}
\end{wrapfigure}


Thus, convolution as a process systematically extracts spatial hierarchies or patterns, starting from local pixel interactions (low-level features) to more abstract concepts (high-level features) as we progress deeper into the network. Finally, the hierarchical feature extracted from the preceding convolution and pooling operations is compressed into a compact and linear representation. The flattened feature vector derived is used for various tasks such as classification, segmentation, or feature localization.
%%%% to add more content about cnn, maybe types etc  TODO


We have chosen three state-of-the-art DNNs that have demonstrated robust performance in image classification in the medical imaging domain for further discussion in this paper. The effectiveness of our proposed model is assessed in relation to these distinguished DNN models in the later section, thereby offering a comparative study.
% \bigbreak

\textbf{ResNet} \cite{resnet} is a deep neural network architecture with a varying number of hidden layers, including a large number of convolutional layers to work efficiently by using residual blocks \cite{resnet} that allow the network to effectively learn the residual or the difference between the input and output features. ResNets has been one of the best-performing models on the ImageNet dataset \cite{imagenet} for classification tasks. It has served as a skeleton for several DNNs that continue to use similar skip connection methods for achieving state-of-the-art performance. It has been applied for the classification of medical image data and has proved to produce state-of-the-art results \cite{breast}, the ResNets-based model showed 99.05\% and 98.59\% testing accuracy for binary and multi-class breast cancer recognition.

% \bigbreak

\par \textbf{DenseNet} \cite{densenet} is one of the densely connected deep-layered neural network architectures that also use residual blocks. They exploit the potential of the deep network by feature reuse, producing more condensed models that are easy to train and highly parameter efficient. Concatenating feature maps learned by different layers increases variation in the input of subsequent layers and improves efficiency. DenseNets uses the Network in Network architecture \cite{nin} which uses multi-layer perceptrons in the filters of convolutional layers to extract more complicated features. DenseNet has increasingly been applied as the backbone model for various medical imaging tasks\cite {Zhou_2021} from image registry to image embedding generation which has further been used for tasks like segmentation and classification.

% \bigbreak

\par \textbf{EfficientNet} \cite{effnet} makes use of techniques like compound scaling, that enable the efficient scaling of deep neural network architectures to meet specific requirements regarding data or resource limitations. Unlike other deep neural networks, EfficientNet achieves improved model performance without increasing the number of floating-point operations per second (FLOPS), resulting in enhanced efficiency. The method introduces the concept of efficient compound coefficients to uniformly scale the depth, width, and resolution of the network. When scaling a model by a factor of $2^{N}$ in terms of computational resources, EfficientNet scales the network depth by $\alpha^{N}$, network width by $\beta^{N}$, and image size by $\gamma^{N}$. These coefficients, namely $\alpha$, $\beta$, and $\gamma$, are determined through a grid search on the base model. By employing this compound scaling approach, EfficientNet strikes a balance between network accuracy and efficiency. EfficientNet has been proven to produce excellent results for medical image classification \cite{Zhou_2021} even in resource-constrained environments.

% \begin{center}
% % Figure removed
% \caption{Figure 3}{ Model Scaling in EfficientNet \cite{modelscale}}
% \end{center}


\subsection{GNNs}
A GNN is a specialized kind of neural network tailored for handling graph-structured data. It exploits the attributes of nodes and edges to learn representations for nodes, edges, and the overall graph. Its working principle is iterative message passing, where features from neighboring nodes are gathered by each node to update its own feature set. CNNs demonstrate limitations in capturing the associations between features within an image \cite{cnnong}. However, these intricate interconnections can be effectively captured by representing images as graphs and then utilizing GNNs to comprehend these intricate interdependencies. Also, GNNs better capture topological data features compared to CNNs\cite{rep}

The intricate complexities of graph data, ranging from structures as varied as protein sequences and chemical molecules to pixels in medical images serving as nodes, necessitate the transformation of these structures into suitable vector spaces. This transformation is crucial for performing computations and analyses. However, this comes with the daunting task of handling graph isomorphism issues, which involve identifying topological similarities between different graphs. In solving these intricate problems, the significance of GNNs becomes especially apparent, specifically in conjunction with the Weisfeiler-Lehman (WL) Isomorphism algorithm\cite{wlm}.

WL algorithm, a prevalent technique in this context, generates graph embeddings by aggregating colors, or more general features (image features, etc), of proximate nodes, culminating in a histogram-like representation for each graph. The conventional GNN architecture mirrors a neural rendition of the 1-WL algorithm, where '1' represents a single neighborhood hop. In this transformation, discrete colors evolve into continuous feature vectors, and neural network mechanisms are harnessed to aggregate information from the neighborhood of each node.

% % Figure environment removed

By virtue of this design, GNNs inherently embody a continuous variant of graph-based message passing similar to the WL algorithm. In this paradigm, details from a node's immediate surroundings are accumulated and relayed to the node, thereby facilitating learning from local graph structures\cite{gnnplus}. This characteristic is at the core of the utility of GNNs in various domains requiring graph-based data analysis. Furthermore, we elaborate in detail on the specific type of GNN employed in this study, namely the GCNN.
%  \begin{center}
% % Figure removed
% \begin{center}
% \caption{Figure 3}{ WL algorithm graphically \cite{wlg}}
% \end{center}
% \end{center}
% \begin{wrapfigure}{r}{0.5\linewidth}
%  \vspace{-3mm}
%  \centering
% % % Figure removed
% %  % Figure removed
%  % Figure removed
%  % \vspace{-3mm}
%  \caption{
%   WL algorithm graphically.
% }
%  \label{fig:samples}
%  % \vspace{-3mm}
% \end{wrapfigure}

\bigbreak
There are two types of GCNNs:

\begin{enumerate}
\item GCNNs based on \textbf{spectral methods} (using convolutions via the convolution theorem \cite{spcnn}). Spectral methods fall into the category of transductive learning, where learning and inference take place on the entire dataset. Spectral CNNs (SCNN) \cite{spcnn} was the first implementation of CNNs on graphs, leveraging the graph Fourier transform \cite{reviewgnn} and defining the convolution kernel in the spectral domain. Examples include the Dynamic Graph Convolutional Network (DGCN), which has been effectively applied to detect relation heat maps in images for pose and gesture detection. HACT-Net \cite{hgcnn} is a further example, which has been applied for the classification of Histopathological images.


% \bigbreak

\item GCNNs based on \textbf{spatial methods}. These GCNNs fall into the category of inductive learning, where learning and inference can be performed on a test and train dataset. They define convolution as a weighted average function over the neighborhood of the target vertex. For example, GraphSAGE \cite{sage} takes one-hop neighbors as neighborhoods and defines the weighting function as various aggregators over the neighborhood. The spatial GCNN is extremely robust due to its inductive learning which makes spatial GCNNs highly scalable.
\end{enumerate}

%  \begin{center}
% % Figure removed
% \begin{center}
% \caption{Figure 3:}{ WL algorithm graphically \cite{wlg}}
% \end{center}
% \end{center}
% % Figure environment removed

% \vspace{-10pt}

We use a simple spectral GCNN \(f(X, A)\) that takes input X which is a vector of node features and an adjacency matrix A, along with a layer-wise propagation rule. The matrix ${A}$ is normalized using methods mentioned in \cite{gcnn} as multiplication of ${X}$ and ${A}$ will change the scale of feature vectors, which leads to disproportional learning from neighbors. The equation \ref{eq:eq1} defines the aggregation operation from one layer to another.
% \begin{equation}
% f(x)=x^2+2 \label{eq:mylabel}
% \end{equation}
\begin{equation}
% \[
H^{(l+1)}=\sigma\left(\tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}} H^{(l)} W^{(l)}\right) . \label{eq:eq1}
% \] 
\end{equation}
\(\tilde{A}\) is the adjacency matrix of the graph \(\mathcal{G}\) , \(\tilde{D}\) is the degree matrix and \(W^{(l)}\) is a trainable weight matrix. The result is passed to an activation function \(\sigma(\cdot)\). The output is then concatenated to get a new hidden state ${H^{(l+1)}}$ for hidden layer ${l+1}$ as shown in Figure \ref{fig:proposed_gnn}.

% \begin{center}
% % Figure removed
% \begin{center}
% \caption{Figure 4:}{ A simple GCN with 3 hidden layers}    
% \end{center}
% \end{center}
% Figure environment removed

% \vspace{-10pt}
GCNN has its inherent limitations. They exhibit poor performance when confronted with dynamic graph structures, causing over-fitting to the training set. Furthermore, GCNNs are susceptible to the issue of over-smoothing\cite{gcnlim}, whereby the addition of more convolutional layers results in an indistinguishable final embedding.

% Unlike GCNNs, the aggregate operation performed in CNNs is not permutation invariant as ${X_{1}W_{1}+X_{2}W_{2} \not= X_{2}W_{1}+X_{1}W_{2}}$. Nodes in a graph are elements of a set with no order, thus the aggregation operation must be permutation invariant. For example for a node with feature vector ${X_{1}}$, ${\tilde{X}_{1}=(X_{1}+X_{2}+X_{3}+X_{4})W_{1}}$ is same as ${\tilde{X}_{1}=(X_{3}+X_{2}+X_{4}+X_{1})W_{1}}$, where ${\{X_{2},X_{3},X_{4}\}} \in N$, N is the set of neighbouring nodes of ${X_{1}}$. 
% The aggregate operation based on message passing \cite{mpass} from neighbors in GCNNs can colloquially be called a special type of convolution.


% \bigbreak
% When compared with DNNs, GCNNs can significantly reduce the number of parameters in a neural network without compromising accuracy, while providing a faster forward propagation. These methods can be scaled to data with a large number of coordinates that have a notion of locality \cite{spcnn}. 
% \bigbreak
In this section, we defined and elaborated on the different types of CNN used in this study along with the types of GNNs and an explanation of spectral GCNN which has been used in the proposed method. In the next section, we explain the proposed method that leverages the power of GNNs, while also overcoming its limitations. 

% In this section, we saw a key difference between DNNs and GNNs. In our work, we have compared the performance of the above-mentioned state-of-the-art models with our GCNN-EC on the MedMNIST dataset.

%%
\section{Our work}
In this work, we present GCNN-EC which resolves identified issues around the limitation of CNNs in capturing the inherent connections between features within an image. The proposed method aims to leverage both local and global inter-pixel relationships by incorporating edge convolution along with graph convolution. 
Our procedure involves three stages: edge convolution, graph convolution, and classification. We merge RGB values into a node feature and compute edge features using a dynamic filter, the system processes the graph representation through multiple convolution layers before flattening the embedding for final classification. We explain these steps in detail in this section.   

\subsection{Edge convolution}
We begin by creating a node feature vector by combining RGB channel values. This is a vital step in transforming the RGB image data into a grid format, where each pixel is a node connected to adjacent pixels. Next, we use a dynamic filter\cite{dync} to learn edge features.  This filter incorporates the node features from the immediate neighbors and also those at a two-hop distance, meaning it considers not just the nearest node but also the nodes that are connected to these nearest nodes. This filter is unique for each input and is learned by the network based on node features and the Euclidean distance between the node feature vectors. This learned filter is stored as a registered buffer in the network and not used during back-propagation. Rather, it is used as an edge feature while being passed through convolution layers. It is through this process that we generate an abstract understanding of the relationships between various components of the image. The graph augmented with node and edge features is further improved by an edge convolution layer \cite{edcn}, which performs convolution operations on the graph using the edge features. The edge convolution layer detects and enhances the edges or boundaries in an image (grid of pixels). It focuses on identifying sharp transitions in intensity/color, which are indicative of object edges. Finally giving a more distinguishable graph representation.


\subsection{Graph convolution}
The graph representation enriched by edge convolution is passed through graph convolution layers, capturing features of the graph by incorporating features of nodes and their connections to finally create a more accurate graph-level embedding. One of the strengths of the graph convolution layer is its ability to incorporate both local and global information. By considering the neighborhood relationships between nodes, it captures local patterns and structures within the graph. Additionally, by aggregating information from neighboring nodes iteratively, it gradually incorporates global information, allowing for a comprehensive understanding of the overall graph. Graph convolution alone suffers from the problem of over-smoothing, This limitation can be overcome by enhancing the graph representation quality by edge convolution to capture meaningful edge information.

\subsection{Classification}
The graph embedding, obtained by flattening the output of the graph convolution layers, is classified using a dense layer. This layer transforms the embedding by matrix multiplication and bias addition thus learning  weighted connections and finally applying non-linear activation. Enabling accurate predictions based on the learned representation of the graph. 

We have employed PyTorch to implement our pipeline, while the Monai framework has been utilized for medical image processing. We generate a graph using MedMNIST images as the input and incorporate the Dynamic Edge Convolution layer\cite{edcn} to perform edge convolution. The resulting learned representation then undergoes 3 graph convolution layers. We fine-tuned the model using Optuna \cite{optuna}, obtaining a learning rate of 0.001 and a weight decay of 0.01. The parameter count in our models ranged from 24,967 to 67,938. We have used the Cross-Entropy loss function with Adam Optimizer and have trained our models for 4 epochs. The batch size used was 64, and the training, testing, and validation splits were 80\%,10\%, and 10\% respectively. GCNN-EC model architecture is shown in Figure\ref{fig:proposed}.

% \bigbreak




% \begin{minted}[mathescape, linenos]{python}
% # differentiable function layer for edge feature creation
% nn.Sequential(nn.Linear(pixel_coords, number_of_row),  
%               nn.ReLU(),         
%               nn.Linear(number_of_row, 1),
%               nn.Tanh())
% \end{minted}

\section{Result}
In this section, we present the results achieved by our model on the 6 classes of the MedMNIST dataset to demonstrate the efficacy of simple GNN when compared with sophisticated DNNs. GCNN-EC model converges to stable loss value within 4 epochs. Our results are presented in Table \ref{tab:tb1} comparing the Area under the Curve (AUC) and Accuracy (ACC) of our model with the DNN models. From the plot in Figure \ref{fig:auc}, it is evident that our method is comparable to DenseNet and outperforms ResNet and EfficientNet, showing it as an effective classifier. The proposed method demonstrates superior performance compared to ResNet18 and EfficientNet-B0 while performing on par with DenseNet121. Notably, the GCNN-EC model utilizes 100 times fewer parameters than the three DNN models considered in this study. Furthermore, our model achieves a remarkable accuracy of 99.13\% on the MNIST dataset (as indicated in the "gcnn-ec-mnist.py" file in the code).

% We have used Receiver Operating Characteristic Curve (ROC) for our performance metric. We have compared the Area under the curve (AUC) and Accuracy (ACC) of our model with DNN models in Table 1. From the plot in Figure 8, we can see that our method is comparable with DenseNet and a better classifier when compared with ResNet and EfficientNet.
% \begin{minted}[mathescape, linenos]{python}
% # differentiable function layer for edge feature creation
% nn.Sequential(nn.Linear(pixel_coords, number_of_row),  
%               nn.ReLU(),         
%               nn.Linear(number_of_row, 1),
%               nn.Tanh())
% \end{minted}
% \begin{center}
% % Figure removed
% \caption{Figure 5:}{ Our GCNN with edge prediction classifier}
% \end{center}
% Figure environment removed

\vspace{-10pt}

% \begin{center}
% % Figure removed
% \caption{Figure 6:}{ Loss and Area under the curve (AUC) over epochs for test dataset}
% \end{center}
% Figure environment removed

\vspace{-10pt}
          
% \begin{table}
%                 \centering
%                 % \captionsetup{type=table}
%                 \label{tab:tb1}
%                 \renewcommand{\arraystretch}{1.4}
%                 \resizebox{\textwidth}{!}{
%              \begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|l|l|l|}
%     \hline
%     % \\[-1em]
%     \multirow{}{}{ Methods } & \multicolumn{2}{l|}{ \,\,AbdomenCT } & \multicolumn{2}{l|}{  \,\,BreastMRI  } & \multicolumn{2}{l|}{ \,\,CXR } & \multicolumn{2}{l|}{ \,\,ChestCT } & \multicolumn{2}{l|}{ \,\,Hand } & \multicolumn{2}{l|}{ \,\,HeadCT } & \multirow{}{}{ Parameters\,\, }\\ \cline{2-13}
    
%                       &    \,\,AUC\,\,   &   \,\,ACC\,\,   &    \,\,AUC\,\,    &    \,\,ACC\,\,    &    \,\,AUC\,\,    &    \,\,ACC\,\,    &    \,\,AUC\,\,    &    \,\,ACC\,\,    &    \,\,AUC\,\,    &    \,\,ACC\,\,    &    \,\,AUC\,\,    &    \,\,ACC\,\,   & 
%                       \\ \hline
%                     %   \\[-0.75em]
%              \,\,ResNet18  & \,\,0.800\,\,      &  \,\,0.839\,\,     &  \,\,0.897\,\,         &  \,\,0.899\,\,         &   \,\,0.832\,\,        &   \,\,0.842\,\,        &      \,\,0.901\,\,     &      \,\,0.940\,\,     &    \,\,0.915\,\,       &  \,\,0.921\,\,         &  \,\,0.733\,\,  &    \,\,0.762\,\,  &    \,\,11,689,512\,\,  
%                     %   \\[-0.75em]      
%              \\ \hline
%             %  \\[-0.75em]
%              \,\,EfficientNet-B0   &   \,\,0.901\,\,        &  \,\,0.907\,\,         &     \,\,0.905\,\,      &    \,\,0.918\,\,       &  \,\,0.958\,\,         & \,\,0.960\,\,          &     \,\,\textbf{0.913}\,\,      &    \,\,\textbf{0.948}\,\,       &    \,\,0.907\,\,        &     \,\,0.911\,\,      &    \,\,0.874\,\,       &  \,\,0.894\,\,   &    \,\,4,014,658\,\,  
%                     %   \\[-0.75em]       
%              \\ \hline  
%          \,\,DenseNet121          &  \,\,\textbf{0.936}\,\,         &     \,\,\textbf{0.942}\,\,      &    \,\,0.961\,\,       &    \,\,0.971\,\,       &      \,\,\textbf{0.972}\,\,     &    \,\,\textbf{0.985}\,\,       &   \,\,0.887\,\,        &   \,\,0.901\,\,        &  \,\,\  \textbf{0.916} \,\,         &  \,\,\textbf{0.925}\,\,         &  \,\,\textbf{0.899}\,\,          &  \,\,\textbf{0.914}\,\,   &   \,\,7,978,856\,\,
%          \\ \hline
%             %  \\[-0.75em]
%             \,\,GCN-EC(ours)          &  \,\,0.876\,\,         &     \,\,0.882\,\,      &    \,\,\textbf{0.983}\,\,       &    \,\,\textbf{0.985}\,\,       &      \,\,0.957\,\,     &    \,\,0.965\,\,       &   \,\,0.748\,\,        &   \,\,0.813\,\,        &  \,\,\  0.886 \,\,         &  \,\,0.905\,\,         &  \,\,0.869\,\,          &  \,\,0.874\,\,   &   \,\,\textbf{24,967}\,\,
%                     %   \\[-0.75em]       
%              \\ \hline      
           
%     \end{tabular}
% }
% \end{table}
\begin{table}
    \centering
    \caption{Comparison of DNNs with proposed method for MedMNIST}
    \label{tab:tb1}
    \renewcommand{\arraystretch}{1.4}
    \resizebox{\textwidth}{!}{%
        \begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|l|l|l|}
        \hline
        \multirow{2}{*}{Methods} & \multicolumn{2}{l|}{AbdomenCT} & \multicolumn{2}{l|}{BreastMRI} & \multicolumn{2}{l|}{CXR} & \multicolumn{2}{l|}{ChestCT} & \multicolumn{2}{l|}{Hand} & \multicolumn{2}{l|}{HeadCT} & \multirow{2}{*}{Parameters} \\ \cline{2-13}
        & AUC & ACC & AUC & ACC & AUC & ACC & AUC & ACC & AUC & ACC & AUC & ACC & \\ \hline
        ResNet18 & 0.800 & 0.839 & 0.897 & 0.899 & 0.832 & 0.842 & 0.901 & 0.940 & 0.915 & 0.921 & 0.733 & 0.762 & 11,689,512 \\ \hline
        EfficientNet-B0 & 0.901 & 0.907 & 0.905 & 0.918 & 0.958 & 0.960 & \textbf{0.913} & \textbf{0.948} & 0.907 & 0.911 & 0.874 & 0.894 & 4,014,658 \\ \hline
        DenseNet121 & \textbf{0.936} & \textbf{0.942} & 0.961 & 0.971 & \textbf{0.972} & \textbf{0.985} & 0.887 & 0.901 & \textbf{0.916} & \textbf{0.925} & \textbf{0.899} & \textbf{0.914} & 7,978,856 \\ \hline
        GCN-EC (ours) & 0.876 & 0.882 & \textbf{0.983} & \textbf{0.985} & 0.957 & 0.965 & 0.748 & 0.813 & 0.886 & 0.905 & 0.869 & 0.874 & \textbf{24,967} \\ \hline
        \end{tabular}%
    }
\end{table}

\vspace{1cm}

\section{Conclusion}
Our model exhibited superior performance when compared to renowned CNNs like ResNet18 and EfficientNet-B0 while achieving comparable results to DenseNet121 on MedMNIST dataset. Notably, our model achieved this with significantly fewer parameters (GCNN-EC: 24,967 vs. ResNet: 11.68M, EfficientNet: 4.01M, DenseNet: 6.95M), highlighting its efficiency and effectiveness in capturing meaningful features. This efficiency suggests the possibility of training our GCNN with significantly fewer data, an important factor in the medical field where properly labeled data is scarce and expensive.

% Moreover, the ability of GNNs to handle heterogeneous graphs opens doors for training a single model on a multimodal dataset, including diverse clinical data \cite{hetro1}

\section*{Acknowledgments}

This publication has emanated from research supported in part by a grant from Science Foundation Ireland under Grant number 18/CRT/6049. For the purpose of Open Access, the author has applied a CC BY public copyright licence to any Author Accepted Manuscript version arising from this submission.

%%%%%%%%%%%%%%%%%%%%%%%%
\appendix

% \section{My first appendix }

% Sed ut perspiciatis, unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam eaque ipsa, quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt, explicabo. Nemo enim ipsam voluptatem, quia voluptas sit, aspernatur aut odit aut fugit, sed quia consequuntur magni dolores eos, qui ratione voluptatem sequi nesciunt, neque porro quisquam est, qui dolorem ipsum, quia dolor sit amet consectetur adipisci[ng] velit, sed quia non numquam [do] eius modi tempora inci[di]dunt, ut labore et dolore magnam aliquam quaerat voluptatem. Ut enim ad minima veniam, quis nostrum exercitationem ullam corporis suscipit laboriosam, nisi ut aliquid ex ea commodi consequatur? Quis autem vel eum iure reprehenderit, qui in ea voluptate velit esse, quam nihil molestiae consequatur, vel illum, qui dolorem eum fugiat, quo voluptas nulla pariatur?


% \section{My second appendix }


% Sed ut perspiciatis, unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam eaque ipsa, quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt, explicabo. Nemo enim ipsam voluptatem, quia voluptas sit, aspernatur aut odit aut fugit, sed quia consequuntur magni dolores eos, qui ratione voluptatem sequi nesciunt, neque porro quisquam est, qui dolorem ipsum, quia dolor sit amet consectetur adipisci[ng] velit, sed quia non numquam [do] eius modi tempora inci[di]dunt, ut labore et dolore magnam aliquam quaerat voluptatem. Ut enim ad minima veniam, quis nostrum exercitationem ullam corporis suscipit laboriosam, nisi ut aliquid ex ea commodi consequatur? Quis autem vel eum iure reprehenderit, qui in ea voluptate velit esse, quam nihil molestiae consequatur, vel illum, qui dolorem eum fugiat, quo voluptas nulla pariatur?


\bibliographystyle{apalike}

\bibliography{imvip}


\end{document}

