% Figure environment removed


\section{IMPLEMENTATION DETAILS}
\subsection{Network Architecture for Image Feature Extraction}
We employed a single stacked hourglass~\cite{hourglass16} network to compute the feature maps $\{ \mathbf{F}^I_i \}$ and $\{ \mathbf{F}^N_i \}$ from input images and normal maps. To preserve more details, we change the header of the original hourglass from $7 \times 7$ filter with stride 2 to $3 \times 3$ filter with stride 1. The hourglass networks take an input image of $512 \times 512$ resolution and output a 128-channel feature map at a  $256 \times 256$ resolution.

Similarly, we utilize the same hourglass network \cite{hourglass16}  with reduced feature channels to compute the feature map $\{ \mathbf{F}^C_i \}$ for texture prediction. It takes an input image of $512 \times 512$ resolution and outputs a 32-channel feature map at a $256\times256$ resolution.

\subsection{Network Architecture for 3D Convolutions}
We build two 3D sparse CNNs in the coarse and fine stages to compute the TSDF volume. 
Figure~\ref{fig:3DCNN} shows the network structure of both networks, with color-coded blocks representing different components.
The 3D CNN at the coarse stage is a U-Net with skip layers. 
%As shown in Figure~\ref{fig:3DCNN}, the 3D CNN at the coarse stage is a U-Net with skip layers. 
%Its encoder includes three modules, 
Its encoder consists of convolution blocks, either `Conv-S1' or `Conv-S2', followed by a residual block, denoted as `ResBlock'. The decoder consists of similar convolution blocks and a transposed convolution block, referred to as `InvserConv'. The details of these blocks are shown in Figure~\ref{fig:3DCNN}.
%each with Conv blocks followed by Resblocks.
%Its decoder employs transposed convolution to upsample features. The decoder is also composed with three modules.


The 3D CNN at the fine stage consists of several convolution blocks followed by an FC layer. It takes a $160\times512^3$ feature volume as input and outputs the TSDF volume. The input feature volume is formed by concatenating the $128\times512^3$ normal feature $\{ \mathbf{F}^N_i \}$ with the $32\times512^3$ coarse feature from the coarse stage.

For blending weight regression in texture prediction, we employed the same network architecture as the coarse stage.

\subsection{Details of the Training Data}
Our method is trained using the Twindom~\cite{twindom} dataset which includes non-human objects. During the training phase, the ground-truth TSDF of each model is precomputed by the mesh-to-SDF library ~\cite{mesh2sdf} and cached for training purpose.
All examples in our training data include only a single individual. However, our method demonstrates strong generalization capabilities in challenging cases with multiple persons. 

%\section{EXPERIMENT DETAILS}
%\subsection{Comparison Setting}
%All methods are evaluated under $256^3$ resolution. The results of DoubleField are missing at Fig 9 (e) and (f), because its authors told us that DoubleField failed on 2-view input and they could not provide any valid result. 
\section{Discrete vs Continuous Formulation}
Our formulation computes the TSDF values only at pre-sampled volume grids. To further verify the impact of this discretization, we experiment with the formulation using 2D convolutions, i.e.
\begin{equation}
	\mathbb{D} (\mathbf{X}) = f (\mathbf{X}, \mathbf{F}^{2D}(\Pi( \mathbf{X}))) = s, \qquad s\in [-1,1].
	\label{equation:continuous}
\end{equation}
Specifically, we discretize it with a pre-sampled volume grid $\{ \mathbb{X}_{whd} \}$ at different resolutions, ranging from $32\times32\times32$ to $512\times512\times512$, as follows,
\begin{equation}
	\mathbb{D} (\mathbb{X}_{whd}) = f (\mathbb{X}_{whd}, \mathbf{F}^{2D}(\Pi( \mathbb{X}_{whd}))) = s, \qquad s\in [-1,1].
	\label{equation:discrete}
\end{equation}
We train these discretized networks on the Twindom\cite{twindom} dataset and test them on the 160 testing human models. The original continuous version can be trained using randomly sampled points nearby the surface boundary as in PIFu\cite{iccv2020PIFu}.

As we can see from Figure~\ref{fig:reso_p2s}, the Point-to-Surface (P2S) error of the reconstructed surface decreases  quickly as the volume resolution increases. The error at 512 resolution, i.e. $0.49$ cm, is very close to the error of the original continuous formulation in Equation~\ref{equation:continuous}, which is $0.48$ cm. Note that, as shown in Table 1 in the main submission, introducing 3D convolution can significantly reduce  the P2S error,
lowering it down to $0.385$ cm.

% Figure environment removed



\section{Results with Different Volume Resolutions}
In Table 3 and Table 4 of the main submission, we observe that a feature volume of 512 resolution produces a more accurate shape reconstruction than that with a 256 resolution.
We provide qualitative comparisons of these two settings in Figure~\ref{fig:compare_pvsc_9} and Figure~\ref{fig:compare_pvsc_10}. All the included examples are drawn from Figure 9 and Figure 11 of the main submission.

\section{Qualitatively Results for the Toy Network}
Figure~\ref{fig:ablation_rebutal} visualizes some results obtained with different network settings, as discussed in Section 3.2 of the main submission. We can see that higher resolution feature volumes reveal more shape details. Meanwhile, the discrete MLP and even a 1-layer discrete MLP can achieve  results comparable to the original continuous MLP. The last column shows that, without 3D convolutions, the network  struggles to handle challenging poses and occlusions present in multi-human scenarios.

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

\section{Results with single view input}
In Table 3 and Table 4 of the main submission, we report the accuracy of different methods using a single input image.
Figure~\ref{fig:compare_singleview} provides qualitative comparisons of these results. Our method achieves shape details similar to those of PIFuHD using a single image.