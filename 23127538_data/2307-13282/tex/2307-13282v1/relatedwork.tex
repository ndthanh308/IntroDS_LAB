\section{Related work}
%How to represent 3d body shape is quite essential in human body reconstruction. Based on different shape representations, human body modeling can be categorized as follows:
\noindent\textbf{Parametric Model Based Methods.}
Parametric human models such as SCAPE~\cite{anguelov2005scape}, SMPL~\cite{Loper2015}, and SMPL-X~\cite{pavlakos2019expressive} have been widely adopted to recover human pose and shapes. SMPLify~\cite{bogo2016keep}  estimates the SMPL model from a single image using 2D keypoint detection. 
Recently, deep neural networks~\cite{hmrKanazawa17,omran2018neural,pavlakos2018learning, xu2019denserac} have been trained to directly regress SMPL model parameters from a single image. The accuracy is further improved by combining bottom-up optimization~\cite{guler2019holopose, kolotouros2019learning} and using temporal generation networks~\cite{kocabas2020vibe}.
However, the SMPL estimation from a single image suffers from shape ambiguity. Huang et al.~\shortcite{3dv2017Towards} and Liang et.al~\shortcite{liang2019shapeaware} generalize SMPL model fitting to multiple input images. 
To capture cloth shape details, methods like ~\cite{alldieck2019tex2shape,bhatnagar2019mgn} use SMPL+D representation to explain high-frequency details. However, this representation struggles to handle loose clothes and long hair. In contrast to  these methods, we aim to reconstruct a clothed human without relying on any parametric model to achieve better generalization to different poses and garments.


\noindent\textbf{Implicit Function Based Methods.} 
Implicit functions~\cite{Mescheder2019,Park2019,Chen2019} provide a powerful shape representation for 3D reconstruction, enabling surface reconstruction at arbitrary resolutions and topologies.
Many recent works utilize implicit functions to reconstruct clothed humans. Huang et al.~\shortcite{Huang18ECCV} and PIFu~\cite{iccv2020PIFu} introduce implicit functions for clothed human reconstruction, where a 3D point is projected onto the input images to gather features for occupancy regression. PIFuHD~\cite{saito2020pifuhd} improves PIFu by extracting high-resolution features from estimated normal maps. PHORHUM~\cite{alldieck2022phorhum} also predicts shading parameters to generate  more realistic rendering results. MonoPort~\cite{li2020monoport} accelerates the occupancy evaluation in a coarse-to-fine manner using Octree structures.
%but fails to recover surface details. 


However, the aforementioned methods compute the occupancy or TSDF of a 3D point using point-wise inference and planar image features, which are limited in exploring 3D context information, especially for self-occlusions and challenging poses.
As a result, Arch~\cite{huang2020arch}, Arch++~\cite{He2021ARCHAC}, PaMIR \cite{zheng2021pamir}, and ICON~\cite{xiu2022icon} employ the SMPL model as a shape prior to improve robustness to different body poses. DeepMultiCap~\cite{zheng2021deepmulticap} further employs a spatial attention network and a temporal fusion method for multi-view input videos. 
However, the SMPL model estimation itself is fragile, StereoPIFu \cite{hong2021stereopifu} takes stereo images as input to exploit the geometric constraints of stereo vision. 
The recent work, DoubleField~\cite{shao2022doublefield}, combines the neural radiance field with an implicit surface field to  generate high-quality results. DiffuStereo~\cite{shao2022diffustereo} further introduces a diffusion-based stereo algorithm to enhance shape accuracy by enforcing multi-view correspondences. 

As observed in \cite{chibane20ifnet,Peng2020ECCV},  implicit function based 3D reconstruction can benefit from a 3D convolutional feature encoder. While these two methods are designed for 3D reconstruction from point clouds or sparse voxel inputs, we extend a similar idea for image-based reconstruction of clothed humans. 3D convolutions can easily encode geometric contexts and compute the TSDF values at nearby points jointly. However, it requires a high-resolution feature volume to capture shape details.

%, which makes 3D convolution expensive in memory storage and run-time computation. We introduce the submanifold sparse convolution \cite{3DSemanticSegmentationWithSubmanifoldSparseConvNet} with sophisticated voxel culling and a coarse-to-fine approach to make the computation feasible. 

%However, all these methods have difficulties to deal with rare poses and loose garments. %\sicong{Not all of these methods have problems to deal with rare poese and loose garments, for example: Doublfield, but Doublefield does not work well with 2 input views, and its inference time is very lone}
%Furthermore, these implicit function based methods are computationally inefficient, since they need to evaluate the occupancy of many 3D points at testing time.

%PIFu~\cite{iccv2020PIFu} and PIFuHD~\cite{saito2020pifuhd} also be extended to supports 3D human reconstruction from multi-view images by projecting the query 3D point to multiple images and extracting their pixel-aligned image features. 




\noindent\textbf{Volumetric Methods.} 
There are relatively fewer works adopting volumetric representation for clothed human reconstruction, as it is known to have expensive memory and running time costs. 
To reduce memory and computation costs, earlier works~\cite{varol18_bodynet,Jackson20183DHB} employ 2D convolutions to regress the occupancy volume from a single RGB image, but only recover limited shape details and suffer from challenging poses. DeepHuman~\cite{Zheng2019DeepHuman} employs 3D convolution and uses SMPL models as shape priors to guide the volume regression. However, like parametric model-based methods, its SMPL estimation tends to fail at challenging poses and loose garments. Gilbert et al.~\shortcite{gilbert2018volumetric} use multi-view images to recover a visual hull, and then apply 3D CNN to compute the occupancy values at the discretized visual hull. However, they do not involve any image features in the 3D CNN, which is crucial to recover shape details, and only generate over-smoothed results. Similar 3D convolution is also applied to the discretized SMPL model in \cite{zheng2021pamir, zheng2021deepmulticap} to facilitate learning implicit functions, but image features are not involved in the 3D convolution. 
Furthermore, most previous methods~\cite{varol18_bodynet,Jackson20183DHB,Zheng2019DeepHuman,zheng2021pamir,zheng2021deepmulticap} use low volume resolution of 128, except the method in \cite{gilbert2018volumetric} uses 256 volume resolution without including image features. 

We find it is important to use high-resolution volumes, e.g. 512 or higher, for accurate 3D reconstruction of clothed humans. Furthermore, it is important to include image features in the 3D convolution to reconstruct shape details. To make these ideas feasible, we design a sophisticated volumetric method by combining efficient sparse 3D convolution, voxel culling, and coarse-to-fine computation. 

%their results lack of shape details, since the visual hull does not contain any image features, which is crucial to recover  shape details. 


%Unlike these methods, we adopt a coarse to fine approach combined with sparse 3D CNNs to regress a TSDF and recover much more shape details.


\noindent\textbf{Surface Color Prediction.} 
Traditional methods like \cite{Waechter2014Texturing} color surface points according to images with front parallel viewing directions. For human modeling tasks, PIFu~\cite{iccv2020PIFu} and its follow-up works \cite{zheng2021deepmulticap,tao2021function4d} often use an additional implicit function to compute a continuous color field, where each 3D point is associated with a color. Implicit functions can hallucinate colors in unobserved regions. However, it is also difficult to capture appearance details like high-frequency textures due to the compact representation.
%fit a continues color field, during the inference phase, these works query RGB value for each mesh vertices, these works can even hallucinates the color output for those invisible region observed from the input views. However, this per-vertices query methods suffer from blurry texture since the MLP used in implicit function is highly compact representation. 
Recently, neural radiance field (NeRF) ~\cite{mildenhall2020nerf} has shown its great potential of generating high quality view synthesis.
%However, the original version of NeRF requires dense input views to generate plausible view interpolation, which will generate ghost artifacts when the input views are sparse. 
NeuralBody ~\cite{peng2021neural} further introduces a SMPL model to aggregate color constraints in the canonical frame, extending the NeRF-based human reconstruction to sparse multi-view inputs. 
To capture 3D shape details while generating realistic rendering, Doublefield ~\cite{shao2022doublefield} combines the advantages of implicit surface field~\cite{iccv2020PIFu} and neural radiance field ~\cite{mildenhall2020nerf}, which further speedup the convergence of NeRF models.
While these methods generate high-quality results, NeRF-based methods still require expensive per-instance optimization, which is undesirable in many real applications.
To address these problems, we follow the spirit of traditional methods to compute a texture map on the mesh surface. Instead of computing the texture color directly, we design a network to estimate a blending weight to evaluate the color according to the input images. In this way, our texture map can easily inherit high-frequency details from high-resolution input images.
%make radiance field converged well on sparse multi-view input images by utilizing the shape priors learned from large scale human scan dataset. 
%However, in order to get visual pleasing rendering results, most of these NeRF based methods requires time costing fine-tuning for each human identity, which limit the application in VR&AR. 
%In contrast, the color prediction module in our framework is generalized on different poses, clothes and illuminations, and it does not rely on any other handcraft prior information like SMPL fitting and fine-tuning.


