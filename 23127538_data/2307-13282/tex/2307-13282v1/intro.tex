% Figure environment removed

\section{Introduction}
Automatic 3D reconstruction of clothed humans using image inputs has gained increasing significance due to its potential applications in a wide array of AR/VR scenarios. High-fidelity reconstructions typically depend on sophisticated capture systems, which are developed with dense camera arrays~\cite{collet2015high,joo2015panoptic,joo2018total}, programmable light-stages~\cite{Vlasic2009, guo2019relightables}, and depth sensors~\cite{newcombe2011kinectfusion,DoubleFusion,BodyFusion,dou2016fusion4d,newcombe2015dynamicfusion}. However, stringent capture environments equipped with complex hardware pose significant challenges for consumer-level applications.


In this context, considerable research effort has been dedicated to developing methods that allow for more flexible capture configurations, such as utilizing a few RGB inputs. Among these works, learning implicit functions \cite{iccv2020PIFu, saito2020pifuhd, hong2021stereopifu} has proven effective in achieving highly detailed reconstructions by integrating the advancements of deep neural networks. These methods employ large multi-layer perceptrons (MLPs) to predict the occupancy probability or truncated signed distance function (TSDF) value of every queried 3D point based on its associated local feature, which is extracted from images. They can recover a continuous surface at arbitrary resolutions without topology restrictions.


However, in typical MLP-based implicit networks, the occupancy or TSDF value at each location is solved independently with planar image features, rendering them less capable of addressing challenging cases such as occlusions. Consequently, these methods suffer from generalization and robustness issues, particularly when tackling strong occlusions caused by large motion or multiple interacting humans. 
Some follow-up studies  \cite{zheng2021deepmulticap,zheng2021pamir,huang2020arch} utilize an extra geometric model, SMPL~\cite{Loper2015}, to improve robustness by introducing strong shape priors. 
Their success typically relies on the assumption of geometrical similarity \cite{huang2020arch} between the shape prior and target reconstruction, making them intractable for handling complex cases with loose clothes and sensitive to errors in SMPL model fitting.



%\ping{this paragraph sounds like `TSDF is better than MLP/SMPL, and we use TSDF to solve the problem'. But in Sec 3, we are telling a different story, saying `MLP needs a 3D convolutional encoder'. We need to make these two sections consistent.}\sicong{I think in this paragraph we claim that the TSDF}


%We opt for Trucated Signed Distance Funtion (TSDF) volumetric representations as they are naturally suitable for convolution operations, which have shown remarkable performance for learning hierarchical features on 2D visual perception tasks \cite{SunXLW19}. 
%Meanwhile, TSDF also describes the gradual geometry change around shape surface, which is not reflected by occupancy volume. 

We instead revisit the 3D volumetric representation and resort to 3D convolutional neural networks (CNNs) for feature learning, due to their impressive performance in feature learning and the ability to incorporate spatial context. However, volumetric methods and 3D convolution involve discretization, which might raise concerns regarding whether a discretized volume can preserve subtle geometric details as continuous representations learned in implicit functions. We investigate the relationship between volume resolution and quantization error on synthetic data by converting target mesh objects to TSDF volumes, as shown in Figure~\ref{fig:quantization_error}. We observe that the quantization errors are significantly reduced by increasing volume resolution and become nearly negligible when reaching a relatively high resolution (e.g., 512 or higher). In other words, achieving fine-detailed reconstruction is not supposed to be restricted by the use of volume representations as long as a proper volume resolution is utilized. Therefore, we present a method with high-resolution feature volumes, e.g., 256 and 512, while traditional volumetric methods \cite{varol18_bodynet,gilbert2018volumetric} are often limited to much lower resolutions, such as 32 or 128.



On the other hand, an increase in volume resolution may lead to a cubic growth of memory overhead \cite{8100085}. Reducing memory costs while guaranteeing the granularity of volumetric representations is necessary for pursuing high-quality reconstruction. Thus, we adopt a coarse-to-fine approach and cull away irrelevant voxels to build a sparse high-resolution feature volume. At the coarse level, the network computes an initial TSDF by applying a U-Net with sparse 3D CNN \cite{3DSemanticSegmentationWithSubmanifoldSparseConvNet} on the sparse feature volume, which is carved by a visual hull. Through our experiments, it turns out that more than 95\% of the volume grids are discarded by the visual hull culling, making the sparse 3D CNN efficient. At the fine level, the network focuses on a narrow band near the zero-level set of the initial TSDF and discretizes the narrow band with smaller voxels. By employing this narrow-band culling, we further shrink the sampling space, resulting in a relatively small range of grid numbers (usually 300K--500K in our experiments) even with a high volume resolution of 512. The remaining voxels in the narrow band are associated with features that fuse high-frequency information from the computed normal maps upon the low-frequency shape from the coarse level to compute the TSDF at high resolution. The final mesh is then extracted from the TSDF using the Marching-Cube algorithm ~\cite{Lorensen87marchingcubes}.
% Different from the u-net sturcture to preserve global topology context, we then apply a shallow 3dcnn to compute the final TSDF $D_{final}$ which contain more local geometry detail.




% \ping{this paragraph can be expanded. It is an important contribution and often ignored by other works. stress on the novel idea of regressing blending weights instead of colors}

In addition to geometry, high-quality mesh texture is also a crucial factor contributing to visual appearance. Directly computing a color field in 3D space, as in \cite{iccv2020PIFu}, struggles to capture high-frequency texture details, while the neural radiance field (NeRF) \cite{yu2020pixelnerf} or the DoubleField~\cite{shao2022doublefield} require expensive per-instance optimization and are often unstable for sparse input images. In contrast, we adopt an image-based rendering approach to compute a texture atlas map, which is efficient and widely supported in existing computer graphics tools. 
Specifically, we compute a blending weight at each 3D point on the mesh surface to determine its color as a weighted average of the colors at its image projections. The blending weights can be computed at a relatively coarse resolution, e.g., 512 volume resolution in our case, and leave texture details to the high-resolution images, such as 1K or 2K. Unlike previous methods that generate blurry texturing results under sparse input, our method generalizes well on both synthetic and real data with just a few input views. 
Figure~\ref{fig:teaser} shows two examples reconstructed by our method. Despite the challenging garment, pose, and occlusion, our method recovers faithful shape, normal, and texture on the right.

%with a wide variety of poses and clothing styles, and it is also adaptive to handle input image with arbitrary resolutions.
%\sicong{For this concern we claim that when the resolution of dicretized volume meets certain threshold (which is 256 in our experiment), the quantization error can be neglected.} 



In summary, the main contributions of this paper are as follows:
\begin{itemize}
\vspace{-0.1in}
  \item 
  We revisit the 3D volumetric representation and demonstrate that it can support clothed human reconstruction with equal or even better performance compared to implicit representation. 
  \item 
  We develop a memory and computation-efficient method for high-resolution volumetric reconstruction using sophisticated sparse 3D CNN, coarse-to-fine estimation, and voxel culling by visual hull and narrow bands. 
  \item 
  We introduce a novel method to compute a texture atlas map, which captures rich appearance details from high-resolution input images.
  \item 
  We achieve impressive results on standard benchmark datasets Twindom and MultiHuman, significantly reducing the point-2-surface (P2S) precision to approximately 0.2cm from just six input views, with more than $50\%$ error reduction compared to the state-of-the-art methods, including DoubleField~\cite{shao2022doublefield} and PIFuHD~\cite{saito2020pifuhd}.
\end{itemize}