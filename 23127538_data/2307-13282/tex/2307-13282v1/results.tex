%The error of DeepMultiCap is mainly caused by challenging garments and SMPL fitting error. \ping{more discussions are needed.}
% \begin{table*}
% \begin{tabular}{ccccccccccccc}
% \hline
% \multicolumn{1}{l}{} & \multicolumn{6}{c|}{Twindom}                                                & \multicolumn{6}{c}{MultiHuman}                                                          \\ \hline
%                      & \multicolumn{2}{c}{2 views} & \multicolumn{2}{c}{4 views} & \multicolumn{2}{c|}{6 views} & \multicolumn{2}{c}{2 views} & \multicolumn{2}{c}{4 views} & \multicolumn{2}{c}{6 views} \\
%                      & Chamfer        & P2S        & Chamfer        & P2S        & Chamfer        & \multicolumn{1}{c|}{P2S}        & Chamfer        & P2S        & Chamfer        & P2S        & Chamfer        & P2S        \\ \hline
% MultiView PIFu           &           1.626     &    1.507        &           0.895     &      0.773      &      0.776           &    \multicolumn{1}{c|}{0.725}        &        1.216        &       1.169     &     0.653           &       0.604    &       0.523        &      0.462      \\ \hline
% MultiView PIFuHD         &         1.369       &       1.223     &           0.821    &      0.720      &         0.720      &    \multicolumn{1}{c|}{0.705}        &      0.915          &      0.873      &        0.589        &    0.523        &        0.492        &      0.433      \\ \hline
% DeepMultiCap         &         1.159       &    1.117        &      0.969          &      1.001     &      0.890          &      \multicolumn{1}{c|}{0.944}      &        0.928        &    0.914        &        0.781       &      0.777      &        0.678        &      0.676      \\ \hline
% DoubleField         &         -       &      -      &         0.836       &     0.822    &        0.711        &     \multicolumn{1}{c|}{0.690}       &        -        &    -        &       0.743        &      0.621      &        0.652        &    0.579        \\ \hline
% %Ours                 &         \textbf{0.899}      &    \textbf{0.860}       &        \textbf{0.550}        &     \textbf{0.500}       &          \textbf{0.459}      &      \multicolumn{1}{c|}{\textbf{0.402}}      &         \textbf{0.735}       &     \textbf{0.691}       &     \textbf{0.412}           &       \textbf{0.345}     &        \textbf{0.341}        &      \textbf{0.275}      \\ \hline
% Ours(256)                 &         {0.899}      &    {0.860}       &        {0.550}        &     {0.500}       &          {0.459}      &      \multicolumn{1}{c|}{{0.402}}      &         {0.735}       &     {0.691}       &     {0.412}           &       {0.345}     &        {0.341}        &      {0.275}      \\ \hline
% Ours(512)                 &         \textbf{0.889}      &    \textbf{0.837}       &        \textbf{0.447}        &     \textbf{0.389}       &          \textbf{0.314}      &      \multicolumn{1}{c|}{\textbf{0.242}}      &         \textbf{0.689}       &     \textbf{0.602}       &     \textbf{0.360}           &       \textbf{0.296}     &        \textbf{0.271}        &      \textbf{0.195}      \\ \hline
% \end{tabular}
% \caption{Mean Chamfer and point-2-surface (P2S) errors of the reconstructed mesh on the synthetic dataset. Our method reduces these errors significantly by about $60$\% over the SOTA methods.}
% \label{tab:comparison}
% \vspace{-0.15in}
% \end{table*}

\begin{table*}[]
\begin{tabular}{ccccccccc}
\hline
\multicolumn{1}{l}{} & \multicolumn{2}{c}{1 view}             & \multicolumn{2}{c}{2 views}                 & \multicolumn{2}{c}{4 views}                 & \multicolumn{2}{c}{6 views}                 \\
\multicolumn{1}{l}{} & Chamfer              & P2S                  & Chamfer              & P2S                  & Chamfer              & P2S                  & Chamfer              & P2S                  \\ \hline
PIFu                 & 2.528/1.612          & 2.421/1.587          & 1.626/1.200          & 1.507/1.170          & 0.929/0.823          & 0.783/0.773          & 0.776/0.678          & 0.725/0.625          \\ \hline
PIFuHD               & 2.814/1.725          & 2.793/1.704          & 1.369/1.162          & 1.223/1.126          & 0.821/0.765          & 0.719/0.645          & 0.720/0.698          & 0.705/0.501          \\ \hline
DeepMultiCap         &     --               &                --               & 1.529/1.159          & 1.496/1.117          & 1.150/0.969          & 1.115/1.001          & 1.062/0.890          & 1.024/0.944          \\ \hline
Doublefield          &     --               &              --                 &     --               &              --                 & 0.836/0.905          & 0.822/0.869          & 0.711/0.779          & 0.690/0.740          \\ \hline
Ours(256)            & 2.457/1.563          & 2.374/1.537          & 1.221/0.899          & 1.080/0.860          & 0.810/0.550          & 0.629/0.500          & 0.668/0.459          & 0.470/0.402          \\ \hline
Ours(512)            & \textbf{2.398/1.565} & \textbf{2.363/1.539} & \textbf{1.110/0.889} & \textbf{1.052/0.837} & \textbf{0.514/0.447} & \textbf{0.429/0.389} & \textbf{0.390/0.314} & \textbf{0.287/0.242} \\ \hline
\end{tabular}
\caption{Mean Chamfer and point-2-surface (P2S) errors of the reconstructed mesh on the Twindom dataset. In each entry, we report two error metrics as $x/y$, where $x$ represents recall and $y$ stands for precision.}
\label{tab:comparison}
\vspace{-0.1in}
\end{table*}

\begin{table*}[]
\begin{tabular}{ccccccccc}
\hline
\multicolumn{1}{l}{} & \multicolumn{2}{c}{1 view}             & \multicolumn{2}{c}{2 views}                 & \multicolumn{2}{c}{4 views}                 & \multicolumn{2}{c}{6 views}                 \\
\multicolumn{1}{l}{} & Chamfer              & P2S                  & Chamfer              & P2S                  & Chamfer              & P2S                  & Chamfer              & P2S                  \\ \hline
PIFu                 & 1.975/1.540          & 1.872/1.511          & 1.370/1.216          & 1.249/1.169          & 0.893/0.653          & 0.742/0.604          & 0.660/0.523          & 0.472/0.462          \\ \hline
PIFuHD               & 2.142/1.936          & 2.121/1.916          & 1.140/0.915          & 0.998/0.873          & 0.739/0.589          & 0.564/0.523          & 0.630/0.492          & 0.438/0.433          \\ \hline
DeepMultiCap         &          --          &              --                 & 1.114/0.928          & 1.077/0.914          & 0.932/0.781          & 0.891/0.777          & 0.681/0.678          & 0.678/0.676          \\ \hline
Doublefield          &          --          &              --                 &          --          &              --                 & 0.743/0.788          & 0.621/0.748          & 0.652/0.664          & 0.579/0.621          \\ \hline
Ours(256)            & 1.922/1.516          & 1.824/1.485          & 0.995/0.735          & 0.847/0.691          & 0.644/0.412          & 0.445/0.345          & 0.570/0.341          & 0.356/0.275          \\ \hline
Ours(512)            & \textbf{1.852/1.515} & \textbf{1.819/1.480} & \textbf{0.822/0.689} & \textbf{0.763/0.602} & \textbf{0.453/0.360} & \textbf{0.372/0.296} & \textbf{0.348/0.271} & \textbf{0.252/0.195} \\ \hline
\end{tabular}
\caption{Mean Chamfer and point-2-surface (P2S) errors of the reconstructed mesh on the Multihuman dataset. In each entry, we report two error metrics as $x/y$, where $x$ represents  recall and $y$ stands for precision.}
\label{tab:comparison_multihuman}
\end{table*}

\section{Experiments}

%%\subsection{Data Generation}
\subsection{Implementation Details}
We experimented with three commonly used datasets, Twindom ~\cite{twindom}, THuman2.0~\cite{zheng2021deepmulticap}, and MultiHuman\cite{zheng2021deepmulticap}. These datasets consist of high-quality scanned 3D models of clothed humans with varying poses and body shapes. We followed \cite{iccv2020PIFu} to generate multi-view images under spherical harmonic lighting to train our network. Before training our shape networks, we pre-computed the ground truth TSDF $\mathbb{D}^{\text{gt}}$ for each human model in our training set. For  training the texture network, we computed the ground truth color volume $\mathbb{C}^{\text{gt}}$ by finding the nearest mesh vertex for each volume grid. In the experiment, we used 1,000 models from Twindom~\cite{twindom} and THuman2.0~\cite{zheng2021deepmulticap} for training. Another 200 models from Twindom and 30 models from MultiHuman were used for testing. We employed the Adam optimizer with a learning rate of 1e-4. The network was trained in an end-to-end fashion for 30 epochs, and the training of our pipeline took approximately 8 hours using 8 NVIDIA A100 GPUs. 

At testing time, for each model, we used 1/2/4/6 input images from different viewpoints to reconstruct the clothed human model. To estimate normal maps, we employed the pre-trained model from \cite{zheng2021deepmulticap}. Our testing experiments were performed with an NVIDIA 3090 GPU. The breakdown of  the running time for our method with 6 input images and 512 volume resolution is provided in Table~\ref{tab:shape_time_cost} and Table~\ref{tab:texture_time_cost} for shape and texture estimation respectively. The most time-consuming step of our shape reconstruction is to extract feature maps $\{ \mathbf{F}^I_i \}$ and $\{ \mathbf{F}^N_i \}$.  We can use buffer swapping techniques with two GPUs to achieve $\times$2 speedup. %\sicong{Note that UV atlas map is optional for mesh coloring, since our network is also able to predict the color for each mesh vertex which saves the time cost for computing the Atlas map.} \ping{this argument is very weak. Our Figure 14 proves color regression is poor. It is much better to have a faster method for UV Atlas map computing.}

In terms of memory consumption, our method takes 18G and 45G of GPU memory during training for the 256 and 512 volume resolutions respectively. The testing time memory consumption is 12G and 18G.

\begin{table}[]
\small
\begin{tabular}{c|ccc|ccc|c}
\hline
     & \multicolumn{3}{c|}{Coarse Stage}                                       & \multicolumn{3}{c|}{Fine Stage}                                        & Total \\ \hline
     & \multicolumn{1}{c|}{$\{ \mathbf{F}^I_i$ \}} & \multicolumn{1}{c|}{VH}   & 3D CNN & \multicolumn{1}{c|}{$\{ \mathbf{F}^N_i$ \}} & \multicolumn{1}{c|}{NB}   & 3D CNN &       \\ \hline
Time (ms) & \multicolumn{1}{c|}{112}   & \multicolumn{1}{c|}{85} & 52   & \multicolumn{1}{c|}{112}   & \multicolumn{1}{c|}{11} & 93  & 465 \\ \hline
\end{tabular}
\caption{Shape reconstruction time cost. Here, columns $\{ \mathbf{F}^I_i \}$ and $\{ \mathbf{F}^N_i \}$ are the time on computing these feature maps. `VH' and `NB' are the time on visual hull culling and narrow band culling. }
\label{tab:shape_time_cost}
\vspace{-0.15in}
\end{table}

% \begin{table}[]
% \begin{tabular}{c|ccc|cc|c}
% \hline
%      & \multicolumn{3}{c|}{Coarse}                                              & \multicolumn{2}{c|}{Fine}            & total                \\ \hline
%      & \multicolumn{1}{c|}{encoder} & \multicolumn{1}{c|}{visual hull} & 3d cnn & \multicolumn{1}{c|}{encoder} & 3dcnn & \multicolumn{1}{l}{} \\ \hline
% Time & \multicolumn{1}{c|}{112ms}     & \multicolumn{1}{c|}{85ms}          & 52ms     & \multicolumn{1}{c|}{112ms}     & 93ms    & 454ms                  \\ \hline
% \end{tabular}
% \caption{Shape reconstruction time cost.}
% \label{tab:shape_time_cost}
% \end{table}

\begin{table}[]
\begin{tabular}{c|c|c|c|c|c}
\hline
     & $\{ \mathbf{F}^C_i \}$  & Attention & 3D CNN & UV Atlas & Total \\ \hline
Time (ms) & 28    & 18      & 150 & 57  & 253 \\ \hline
\end{tabular}
\caption{Texture reconstruction time cost. Here, the column $\{ \mathbf{F}^C_i \}$ indicates the time for computing the feature maps.} 
\label{tab:texture_time_cost}
\vspace{-0.15in}
\end{table}

\subsection{Quantitative Results on Synthetic Data}
In this subsection, we compare our methods against other reconstruction methods, including Multi-view PIFu~\cite{iccv2020PIFu}, Multi-view PIFuHD~\cite{saito2020pifuhd}, DeepMultiCap \cite{zheng2021deepmulticap}, DoubleField ~\cite{shao2022doublefield}, and DiffuStereo~\cite{shao2022diffustereo}.
%. Note that DiffuStereo ~\cite{shao2022diffustereo} also take multi-view input images for humanbody modeling, however, their input must be narrow based image pairs due to the requirement of stereo matching, which does not work on our sparse view setting. 
We test the robustness  of these methods with different numbers of input images. To ensure a fair comparison, we implemented MultiView PIFuHD~\cite{saito2020pifuhd} and MultiView PIFu~\cite{iccv2020PIFu} based on the public code of their single-view versions. We used the same training and testing data for MultiView PIFu, MultiView PIFuHD, and our method. The authors of DeepMultiCap~\cite{zheng2021deepmulticap} and DoubleField~\cite{shao2022doublefield} kindly provided us with their results. According to their paper, these two methods were trained on a larger set of data than our method. To make the comparison fair, we sample a 512$\times$512$\times$512 volume to compute the final shape using the marching cube algorithm in all the compared methods.

In the case of a single input image, we determine the visual hull by truncating a cone defined by the camera center and the image silhouette with depth thresholds of $-0.5$m and $0.5$m.
%\sicong{Meanwhile, like PIFu~\cite{iccv2020PIFu}, we only compute the loss for the points close to the ground truth surface.} \ping{what does this sentence mean?}
We retrain the public code of PIFu~\cite{iccv2020PIFu} on our dataset. PIFuHD~\cite{saito2020pifuhd} only releases testing code, so we test its pre-trained model on our dataset.
 
%A robust reconstruction method should be able to generate reasonable good results even with very few input views. Hence we evaluate the performance of our and other methods with different view numbers.
\begin{table}[]
\begin{tabular}{l|l|l}
\hline
            & DiffStereo  & Ours        \\ \hline
Chamfer/P2S & 0.120/0.126 & 0.158/0.103 \\ \hline
\end{tabular}
\caption{The shape precision of our method and the DiffuStereo\cite{shao2022diffustereo} on the 8-view setting.}
\label{tab:compare_diffstereo}
\vspace{-0.15in}
\end{table}


Table~\ref{tab:comparison} and Table~\ref{tab:comparison_multihuman} provide a quantitative comparison of different methods with 1--6 input images. Note that Deepmulticap~\cite{zheng2021deepmulticap} cannot work with a single input image, while DoubleField~\cite{shao2022doublefield} cannot work with a single or two input images. Thus, their results are absent for those settings. 
We report our results with two different volume resolutions, 256 and 512 for the fine stage (with corresponding coarse stage resolutions of 128 and 256, respectively). To better evaluate these methods, we report both recall and precision errors for all  methods. Precision is computed as the average distance between each vertex on the predicted mesh and its nearest correspondence on the ground truth mesh. Conversely, recall is the average distance between each vertex on the ground truth mesh and its nearest correspondence on the predicted mesh. 
Our method, with a 512 volume resolution, has the lowest errors among all methods for different numbers of input images. On the Twindown dataset (shown in Table~\ref{tab:comparison}), our method reduces the mean point-2-surface (P2S) precision to $0.24$cm  with $6$ input images, a remarkable error reduction of $51\%$ over DoubleField~\cite{shao2022doublefield} and Multi-view PIFu~\cite{iccv2020PIFu,saito2020pifuhd}. Furthermore, it achieves a mean P2S precision of $0.39$cm using just $4$ input images, with over a $39\%$ error reduction. Even with only $2$ input images, our method can achieve a $0.84$cm mean P2S precision, surpassing  DeepMultiCap~\cite{zheng2021deepmulticap} with 6 input views. This significant  improvement over SOTA methods is also apparent on the MultiHuman dataset (shown in Table~\ref{tab:comparison_multihuman}) and with the Chamffer error metric. %Note DoubleField does not work under our setting of 2 input images. 
When using a single input image, the performance improvement by our method is smaller. This is reasonable since the main source of error here is the single view depth ambiguity, which cannot be solved by our 3D convolution without additional input images.



% Figure environment removed


DiffuStereo~\cite{shao2022diffustereo} requires image pairs with a smaller baseline for their diffusion-based stereo matching, which does not work on our sparse view setting in Table~\ref{tab:comparison} and Table~\ref{tab:comparison_multihuman}. Table~\ref{tab:compare_diffstereo} compares our method with DiffuStereo using the same 8-view setting as \cite{shao2022diffustereo}, where each view has an adjacent view facilitating stereo reconstruction. We employ our model trained for the 6-view input, which has not been trained or finetuned on the 8-view setting. Our P2S precision is 18\% smaller than that of DiffuStereo, which demonstrates the generalization capability of our method.  In this case, we follow \cite{shao2022diffustereo} to normalize the height of all human subjects to 1 meter when evaluating the error metrics, resulting in smaller error metrics than those in Table~\ref{tab:comparison} and Table~\ref{tab:comparison_multihuman}.

\begin{table}[]
\begin{tabular}{l|l|l|l|l|l}
\hline
           & PIFu   & PIFuHD & DMC & DoubleField & Ours  \\ \hline
Twindom    & 9.76  & 9.69  & 13.02       & 11.18      & \textbf{6.94} \\ \hline
Multihuman & 10.61 & 10.46 & 11.92       & 11.66      & \textbf{7.25} \\ \hline
\end{tabular}
\caption{The normal errors of different methods measured by the mean angular error (in degrees). Here DMC stands for the `DeepMultiCap' method. }
\label{tab:normal_error}
\vspace{-0.15in}
\end{table}

Table~\ref{tab:normal_error} and Figure~\ref{fig:normal_compare} show the surface normal error of different methods with 6 input views to evaluate their capability in capturing fine-scale shape details. To compute the surface normal error, we re-render the reconstructed surface into normal maps and compare them with the ground truth results. Our method achieves the smallest mean angular error on both datasets, demonstrating our capability of reconstructing shape details. As shown in Figure~\ref{fig:normal_compare}, other methods often produce larger errors at concave regions, such as the pants in the second row.


Table~\ref{tab:color_comparison} assesses the rendering quality of our textured models using  PSNR and SSIM metrics. All results are obtained under the 6-view setting. We cite the results of PixelNerf~\cite{yu2020pixelnerf} and DoubleField~\cite{shao2022doublefield} from DoubleField~\cite{shao2022doublefield}. 
Our method achieves the highest score on both metrics. 

% Figure environment removed

% \begin{table}[]
% \begin{tabular}{c|cc|cc}
% \hline
%                   & \multicolumn{2}{c|}{\begin{tabular}[c]{@{}c@{}}Twindom \end{tabular}} & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}MultiHuman\end{tabular}} \\
%                   & PSNR                                       & SSIM                         %               & PSNR                                  & SSIM                                   
%                   \\ \hline
% MultiView PIFU        &    20.66                                      &    0.837                                   %&                                          &                                      \\ \hline
% PixelNerf &                                          &                                       &                                          &                                     \\ \hline
% DoubleField              &      23.56                                        &             0.857                             &                                          &                                       \\ \hline
% Proposed method                &        24.51                                      &     0.863
% &                                           &                                      \\ \hline
% \end{tabular}
% \caption{Results of color comparison.
% }
% \label{tab:color_comparison}
% \end{table}

\begin{table}[]
\begin{tabular}{l|l|l|l|l}
\hline
     & PIFu  & PixelNerf & DoubleField & Ours  \\ \hline
PSNR & 20.66 & 21.85     & 23.56   & \textbf{26.31} \\ \hline
SSIM & 0.807 & 0.813     & 0.857   & \textbf{0.863} \\ \hline
\end{tabular}
\caption{PSNR and SSIM of the re-rendered mesh on the synthetic dataset. Our method produces results most consistent with the ground truth.}
\label{tab:color_comparison}
\vspace{-0.15in}
\end{table}

%The improvements on Chamfer error is also significant.

%comment for speed
% % Figure environment removed


\begin{table}[]
\begin{tabular}{c|cc|cc}
\hline
                   & \multicolumn{2}{c|}{\begin{tabular}[c]{@{}c@{}}Twindom \end{tabular}} & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}MultiHuman\end{tabular}} \\
                   & Chamfer                                       & P2S                                        & Chamfer                                   & P2S                                   \\ \hline
AB1 (G)              &     0.351                                          &                 0.286                           &    0.303                                      &              0.233                         \\ \hline
AB2 (N)                  &     0.339                                          &                 0.275                           &                        0.280                   &          0.207                             \\ \hline

Proposed method                &         0.314                                      &             0.242                               &     0.271                                      &       0.195                                \\ \hline
\end{tabular}
\caption{Results of different ablation settings in shape reconstruction. AB1 and AB2 use different input features at the fine stage. }
\label{tab:Ablation_shape}
\vspace{-0.15in}
\end{table}


% \ping{(a) -- (f) should be added for rows, put the ground truth to the right most, next to our results}

% \ping{replace figures, highlighting examples that exemplify problems of previous methods.}




% Figure environment removed




% Figure environment removed

To better understand the quantitative comparison, we visualize some of the results in Figure~\ref{fig:compare}, where (a)--(b), (c)--(d), and (e)--(f) are results reconstructed with 6, 4, and 2 input images, respectively. From left to right, the shown figures are input images, results from Multi-view PIFu, Multi-view PIFuHD, DeepMultiCap, DoubleField, our method, and ground truth, respectively. It is evident that our method generates more shape details and is more robust to loose garments and rare poses. From examples (e, f), we can observe that Multi-View PIFu and Multi-View PIFuHD often generate broken arms when the number of input images is small, while our method does not suffer from this problem. Examples (a, b) highlight our strength in handling  loose garments, where other methods often generate noisy reconstructions. Examples (c, d) showcase our capability in addressing rare poses and unusual  objects. 


%Multi-view PIFuHD~\cite{saito2020pifuhd} cannot work well with challenging poses, as shown in the red rectangle in xxx example where the arms are broken. 
%For DeepMultiCap~\cite{zheng2021deepmulticap}, although it can recover high quality details by leveraging estimated normal maps, the inaccurate SMPL estimation causes corrupted results, \ping{refer to specific examples} also note that since DeepMultiCap~\cite{zheng2021deepmulticap} fails to recover backpack and losses clothes which can not be explained by SMPL model(example c). 
%For partial occluded cases, for the methods ~\cite{iccv2020PIFu,saito2020pifuhd} without shape prior, and the surface details closed to the obstacles is very noisy. And DeepMultiCap~\cite{zheng2021deepmulticap} requires extra instance mask for inference. Different from DeepMultiCap~\cite{zheng2021deepmulticap}, our method does not need extra instance segmentation, and the surface quality of reconstructed human body meshes are not affected by the obstacles.







Figure~\ref{fig:texture} visualizes the recovered normal maps and blending weight maps for some examples. We visualize the blending weights of three input images in the respective RGB channels. The smooth transition of these weights generates seamless textured models with vivid texture details, as shown in the zoomed-in regions.


Figure~\ref{fig:challenging} shows some challenging examples, such as occlusion and multiple persons. Our method can still recover faithful shape details and poses in these situations. Note that we use the ground truth foreground segmentation, which includes the luggage and all persons together. Instance segmentation, as employed in DeepMultiCap ~\cite{zheng2021deepmulticap}, is not used here. Surprisingly, our method can even reconstruct backpacks and luggage quite  well, even though it has never been trained on these types of objects. We believe our 3D feature volume helps to learn local implicit functions, like those in \cite{Jiang2020}, which generalize well across object categories. This is because, locally, backpacks and luggage have similar shapes as clothed humans. More animated examples are provided in the supplementary video.
%Also, our method works well even when observed model is occluded by other objects, Table~\ref{table:compare_with_other_occlu} shows the reconstruction accuracy under occlusion situations, our method can still obtain accurate shape when human body is partially invisible. 


% Figure environment removed

\subsection{Qualitative Results on Real Data}
We also experiment with our own real data, which is captured using 6 calibrated and synchronized Kinect cameras (only RGB images are used) surrounding the subject. We employ \cite{matting} to generate the image masks. Figure~\ref{fig:real} shows some of the results. As demonstrated  in these examples, our method does not rely on SMPL model estimation, enabling it to handle challenging cases with loose garments and unusual poses. All these examples are reconstructed using $6$ input images. Video results and additional  examples can be found in the supplementary files. 

\subsection{Ablation Study}
We conduct ablation studies to examine the effectiveness of our various design choices. To justify our system design, we also test a na\"{\i}ve implementation using a 3D UNet, which has the same architecture as the coarse stage with conventional 3D convolutions. We report the system performance and GPU memory consumption for different volume resolutions in Figure~\ref{fig:dense-3dcnn}. It is evident that higher volume resolution can significantly reduce shape errors, especially recall errors. However, GPU memory consumption also increases substantially, from 6G to 69G for volume resolution of 32 and 256, respectively. It is not feasible to scale this  na\"{\i}ve  implementation to a volume resolution of 512 on an A100.

% \noindent \paragraph{\textbf{Ablation I: w/wo coarse-to-fine architecture}}
% In order to validate our coarse-to-fine architecture, we test with just one stage at $256\times256\times256$ resolution \ping{why not $512\times512\times512$??}. We build the input feature volume from: AB1. RGB images only; AB2. RGB images and estimated normal maps. 

\noindent \paragraph{\textbf{Ablation I: Shape ablation}}


To test the effectiveness of the normal feature and coarse level feature at the fine stage shape reconstruction, we conduct experiments using: AB1. only the coarse level feature; AB2. only the normal feature; and the proposed method, which uses both normal and coarse level features together.

We summarize the mean Chamfer and P2S errors of these different settings in Table~\ref{tab:Ablation_shape}. From AB1 and AB2, we can see that the normal features and the coarse level feature complement each other and should  both be included when computing the final TSDF.


% \begin{table}[]
% \begin{tabular}{|c|c|c|c|c|c|c|c|}
% \hline
%           & 64featvol noconv & 64featvol conv & 128featvol noconv & 128 featvol conv & 256 featvol noconv & 256 featvol conv & 256 featvol conv + shallowmlp \\ \hline
% Precision & 0.657            & 0.574          & 0.599             & 0.459            & 0.592              & 0.420            & 0.480                         \\ \hline
% Recall    & 0.820            & 0.838          & 0.776             & 0.654            & 0.762              & 0.556            & 0.593                         \\ \hline
% \end{tabular}
% \caption{The Chamfer error with different feature volume architectures and MLP, the results show that chamfer error decreses with high resolution feature volume and 3D sparse CNN.}
% \label{tab:resolution_ablation}
% \end{table}

% Figure environment removed

%For texture prediction, we also evaluate the color regression framework with different alternatives to figure out the factors contributing the high-quality mesh texture. 

\noindent \paragraph{\textbf{Ablation II: Texture ablation}}
To test the effectiveness of our texture estimation, we conduct experiments using: AB3. the network to directly compute a color field as in previous methods\cite{iccv2020PIFu,zheng2021deepmulticap}; AB4 \& AB5. after solving our blending weight field (with input images of 512 resolution by optimizing Equation~\ref{equation:color_loss}), we use 1K \& 2K images to compute the texture atlas map, respectively.

% \noindent \paragraph{\textbf{Ablation IV: color regression with different input}}
% We evaluate the truncated PSDF feature and color feature by: AB6. predicting the mesh texture with PSDF feature only; AB7. predicting the mesh texture with color feature only. \ping{this one is unnecessary.}

%To demonstrate that our blending weight mechanise is scaleble to any resolution input. We tested the performance with different image resolutions(AB4 for 1k image, AB5 for 2k image). 
%\ping{I think this is unnecessary, we should by default use high resolution images, 2K or 4K. High res images does not cause additional cost to our method (only requiring a larger texture atlas map or denser triangle mesh). We should in fact avoid using the dense triangle mesh. }




Table~\ref{tab:ablation_color} summarizes the PSNR and SSIM of the different settings. Firstly, AB3 has much poorer results than the other two settings  that involve blending weight estimation. It is evident that our blending weight strategy is crucial for generating sharp texture. Visualization in Figure~\ref{fig:ablation_color} reveals  that our strategy can produce sharp high-frequency texture details, while color field regression causes blurriness. 
AB4 and AB5 further demonstrate the scalability of our method. Both settings share the same blending weight volume $\mathbb{W}$, computed from input images of 512 resolution. We apply $\mathbb{W}$ to 1K and 2K images to compute the texture atlas map. Both settings generate high-quality results, while AB5 is slightly better. These two experiments demonstrate that our method can capture more texture details by re-evaluating the texture atlas map without the need for retraining.
%This ablation study proves that we can easily recover more texture details by simply using high resolution images to evaluate the texture atlas map.


%Regarding to the scalability of the color prediction network, from AB4 and AB5 we can the that blending weight volume with different resolutions can generate comparable mesh texture, which means our color prediction method can handle ultra high resolution texture by interpolating a blending weight volume with relatively low resolution.

% Figure environment removed

\begin{table}
\begin{tabular}{c|cc|cc}
\hline
                  & \multicolumn{2}{c|}{\begin{tabular}[c]{@{}c@{}}Twindom \end{tabular}} & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}MultiHuman\end{tabular}} \\
                  & PSNR                                       & SSIM                                        & PSNR                                  & SSIM                                   \\ \hline
AB3              &      23.776                                       &             0.854                             &             24.027                            &            0.860                          \\ \hline
AB4                &       26.309                                      &      0.862
&    26.033                                       &   0.866                                  \\ \hline
AB5                &        26.656                                      &     0.864
&     26.544                                       &   0.867                                   \\ \hline
\end{tabular}
\caption{Results of different ablation settings in texture prediction. Please refer to text for more details. 
}
\label{tab:ablation_color}
\vspace{-0.15in}
\end{table}

%\ping{the following sentence needs pictures to illustrate}
%by observing the results we see that with the normal features the surface details are more obvious, especially for facial expressions. However, the numerical error is not sensitive to the surface details.
%after re-sampling global feature in narrow band, the accuracy gets lower after fine stage significantly. Meanwhile, 
%, the system can get slightly low error, 


%% Figure environment removed

%% Figure environment removed
%comment for speed
% Figure environment removed

\subsection{Limitations and Future Work}
Our method has difficulties when addressing cases where the segmentation module fails. Figure~\ref{fig:failure_case} (a) shows such an example, where inaccurate segmentation due to motion blur results in an incomplete feature volume, consequently leading to poor final results. Inaccurate camera calibration may also contribute to poor feature volume construction and, subsequently, inferior shape results. As for texture prediction, our method computes texture maps by blending input images, which makes it difficult to handle unobserved regions, as shown in Figure~\ref{fig:failure_case} (b). In the future, we might consider employing an implicit function to address this problem.


% % Figure environment removed

%we assume different input views have consistent camera setting
%different illumination will cause inconsistent texture on the surface, a potential solution would be disentangling the diffuse color with albedo.