\section{Conclusion}
%We present a novel method to reconstruct a high resolution TSDF volume representing a clothed human from sparse multi-view images. 
%We present a novel method to reconstruct clothed humans from sparse multi-view images. 
%Our method represents detailed 3D shape by a high resolution TSDF volume, and surface texture by a volume of blending weights. 
We re-examine volumetric reconstruction for clothed humans and demonstrate that,  with proper system design, it can generate superior results than recent deep implicit methods. 
We find that a high volume resolution, such as 512 or above, effectively reduces the notorious  quantization error and capitalizes on the advantages of 3D CNNs for enhanced  exploration of local context information. 
%Our method employs high resolution volumes, e.g. 256 and 512, and applies 3D CNNs to better exploit local context information for shape reconstruction.
%Unlike many recent methods employing an MLP with planar image features, we use 3D CNNs with 3D features to better integrate local information and compute a discrete TSDF volume. 
To address the memory and computational challenges associated with high-resolution volumes, our method takes a coarse-to-fine approach, integrating sparse 3D CNN and voxel culling through  visual hulls and narrow bands.
Finally, it employs  an image-based rendering approach to compute the texture atlas map by blending input images with learned weights.
Extensive experiments demonstrate that our method significantly improves shape accuracy over SOTA techniques and captures vivid appearance details.
