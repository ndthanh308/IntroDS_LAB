
% Figure environment removed

% Figure environment removed


\section{Motivation}
\label{sec:motivation}
Given sparse view RGB images $\{ \mathbf{I}_i \}$ capturing a clothed human and their calibrations, our goal is to estimate the truncated signed distance function (TSDF) $\mathbb{D}$ which describes the clothed human shape. This TSDF $\mathbb{D}$ might be directly discretized as a 3D volume, where each voxel grid stores the TSDF value. In contrast, recent learning based reconstruction methods~\citep{Mescheder2019,Park2019,Chen2019} employ a  multi-layer perceptron (MLP) as an implicit representation of the TSDF or occupancy function, which is continuous and free from resolution and topology limitations. Following this idea, PIFu\cite{iccv2020PIFu} computes the occupancy function with pixel-aligned features as,
\begin{equation}
	\mathbb{D} (\mathbf{X}) = f (\mathbf{X}, \mathbf{F}^{2D}(\Pi( \mathbf{X}))) = s, \qquad s\in [-1,1],
	\label{equation:2Dfeature}
\end{equation}
where $f$ is an MLP, $\mathbf{X}$ is a 3D point, and $\Pi(\cdot)$ projects 3D points to the input image. The feature map $\mathbf{F}^{2D}$ is computed from the input image with 2D convolutions. 


On the other hand, the simple fully-connected network architecture of MLPs is inefficient in integrating context information as studied in \cite{chibane20ifnet,Peng2020ECCV}. These studies suggest combining a 3D convolutional encoder with an MLP decoder for 3D reconstruction from point clouds. In the same spirit, we might solve the TSDF $\mathbb{D}$ as
\begin{equation}
	\mathbb{D} (\mathbf{X}) = f (\mathbf{X}, \mathbf{F}^{3D}( \mathbf{X})) = s, \qquad s \in [-1,1],
	\label{equation:3Dfeature}
\end{equation}
where the feature volume $\mathbf{F}^{3D}$ is computed in 3D by a 3D convolutional encoder. As reported in ~\cite{Peng2020ECCV}, learning this 3D feature volume is superior to its counterpart 2D feature maps (i.e. the single-plane or multi-plane feature encoder), which are commonly employed in earlier clothed human reconstruction methods including PIFu\cite{iccv2020PIFu}, PaMIR\cite{zheng2021pamir}, and DeepMultiCap\cite{zheng2021deepmulticap}.

%\ping{maybe we can draw an illustrative figure to explain that 3D feature is superior than the 2D features, which is simply the multi-plane convolution case in the Conv Occu-Net paper}
%\ping{Talking about different presentations of $\mathbb{D}$, i.e. implicit function vs volume. Write down the two formulations. Motivate the volume representation with Conv Occupancy Net. Use multi-plane CNN to motivate 3D CNN.}

% \begin{table}
% \begin{tabular}{|l|l|l|l|l|l|}
% \hline
%  & Chamfer  & P2S   \\ \hline
% % Setting1&  0.689 & 0.481   \\ \hline
%  2D Features&  0.712 & 0.509   \\ \hline
%  3D Features&  0.622 & 0.385  \\ \hline
% \end{tabular}
% \caption{Features of different 3d human body scan datasets. \ping{somehow make it one row, save some page space}}
% \label{tab:ablation_palvo}
% \end{table}

\begin{table}
\begin{tabular}{|l|l|l|l|l|l|}
\hline
 & 2D Features  & 3D Features   \\ \hline
 Chamfer/P2S&  0.601/0.549 &  0.404/0.358    \\ \hline
 %%Chamfer/P2S&  0.712/0.509 & 0.622/0.385   \\ \hline
%  3D Features&  0.622 & 0.385  \\ \hline
\end{tabular}
\caption{Chamfer and P2S precision errors of the two toy networks with 2D or 3D features tested on the Twindom\cite{twindom} dataset. }
\label{tab:ablation_palvo}
\vspace{-0.1in}
\end{table}

\subsection{3D Feature Volume}\label{sec:feature}
To demonstrate the strength of 3D convolution in Equation~\ref{equation:3Dfeature}, we experiment with a toy network by directly appending a 3D convolutional feature encoder with an MLP decoder for clothed human reconstruction. 
Specifically, as in Figure~\ref{fig:toy_ablation} (b), we sample a set of regular volume grid $\{ \mathbb{X}_{whd} \}$ in 3D space, where $\{w, h, d\}$ are the grid indices, and associate each grid vertex with the image features at its projected image positions. We then apply 5 layers of 3D convolutions with filter size $3\times3\times3$ to the feature volume, and use an MLP to decode the TSDF value at each sampled grid vertex from its feature. In this way, we can compute the TSDF values at all the grid vertices $\{ \mathbb{X}_{whd} \}$ as,
\begin{equation}
	\mathbb{D} (\mathbb{X}_{whd}) = f (\mathbb{X}_{whd}, \mathbf{F}^{3D}( \mathbb{X}_{whd})) = s, \qquad s \in [-1,1].
	\label{equation:3D_discrete}
\end{equation}
Note that, Equation~\ref{equation:3D_discrete} is a discretized version of Equation~\ref{equation:3Dfeature} and only computes TSDF values for the pre-sampled volume grids. 
%In principle, we can also make Equation~\ref{equation:3D_discrete} continuous by interpolating the 3D feature volume and evaluating the value of $f(\cdot)$ at any 3D point $\mathbf{X}$ as in \cite{chibane20ifnet,Peng2020ECCV}. But 
As we discuss in the supplementary file, we empirically find this discrete approach is close to the original continuous version with high-resolution feature volumes. The MLP $f(\cdot)$ can also be heavily simplified to just one layer in our experiments.
%often sufficient and

Alternatively, as shown in Figure~\ref{fig:toy_ablation} (a), we might use the 2D image features directly as input to the MLP to decode the TSDF values at grid vertices $\{ \mathbb{X}_{whd} \}$. To ensure a fair comparison, we employ additional $3\times3$ 2D convolutions in the image space to make the number of learnable parameters similar. 



%\ping{maybe we should draw the network structure of these two toy networks.}

%Regularly sample 3d points on a fixed volume grid, projecting each sampled 3d point onto feature images to get the averaged feature volume, then apply sparse 3d convolution on the feature volume, then using a mlp to predict the tsdf value of each volume grid. 

%Note that, this toy network will be PIFu\cite{iccv2020PIFu} if without the 3D feature encoder.

%Setting 1: Randomly sample 3d points, projecting each sampled 3d point onto feature images to get the averaged feature, then using a mlp to predict its tsdf value. 

%Setting 2: Regularly sample 3d points on a fixed volume grid, projecting each sampled 3d point onto feature images to get the averaged feature volume, then using a mlp to predict the tsdf value of each volume grid. 

%Setting 3: 

We train these two toy networks using pre-sampled volume grids at 256 resolution on the Twindom ~\cite{twindom} dataset, and evaluate them on the 160 testing human models. Table~\ref{tab:ablation_palvo} shows the Chamfer distance and point-2-surface errors of both methods. It becomes evident that learning a 3D feature volume with 3D convolutions leads to more accurate reconstructions\footnote{Note that PIFu~\cite{iccv2020PIFu} reconstructs a continuous surface which is not limited to the pre-sampled grid vertices $\{ \mathbb{X}_{whd} \}$. In the same experiment, its Chamfer distance error is 0.592 and P2S error is 0.538, which are slightly better than our discretized version with 2D features, but inferior to the version employing 3D features.} since the 3D CNNs can better leverage context information.

%We can see that instead of use random sampling strategy to train a MLP, the regular sampling on fixed volume grid still can achieve a comparable performance, notice that with feature volume generated by grid sampling, we can apply 3d-cnn on sampled feature volume to extract shape context information, and the result of setting3 shows the significant improvement.

%\ping{We need to be careful here, it seems we are heavily comparing with PIFu, but it is a quite old method now}


% Figure environment removed



%% Figure environment removed


%\begin{table*}[]
%\begin{tabular}{|c|c|c|c|c|c|c|c|}
%\hline
%            & 64F-5MLP & 64F*-5MLP & 128F-5MLP & 128F*-5MLP & 256F-5MLP & 256F*-5MLP & 256F*-1MLP \\ \hline
%Chamfer/P2S   & 0.657/0.608     & 0.574/0.524      & 0.599/0.549      & 0.459/0.395       & 0.592/0.538      & 0.420/0.348       & 0.480/0.401                    \\ \hline
%Memory cost & 6G        & 6G         & 9G         & 12G         & 33G        & 49G         & 16G                      \\ \hline
%\end{tabular}
%\caption{
%Results of the toy network with different voxel grid resolutions and depths of the MLP decoder. At each column, the F-number indicates the grid resolution and the MLP-number is the depth of the decoder. The symbol $*$  indicates 3D convolution is enabled. }
%\label{tab:resolution_ablation}
%\end{table*}


\begin{table}[]
\small
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
            & 64F5M  &   128F5M
            & 256F5M &   256F5M*  &
            256F1M* &
            256F'5M \\ \hline
Chamfer   & 0.574      &  0.459   & 0.406        & 
0.404   &   0.432 &   0.592             \\ \hline
P2S       & 0.524      &
0.395   & 0.332        &
0.358   & 0.365 & 0.538\\
\hline
%\revision{Memory}   & \revision{6G}               & 
%\revision{12G}           & \revision{49G}              & 
%\revision{50G}           &           \\ \hline
\end{tabular}
\caption{
Results of the toy network using various network settings. In each column, the F-number represents  the volume resolution, while the M-number denotes  the depth of the MLP decoder. M* signifies  a discrete MLP which  evaluates TSDF values only on the grid vertices, and F' indicates  convolution is not applied to the 3D features.}
\label{tab:resolution_ablation}
\vspace{-0.2in}
\end{table}


%%Recall      & 0.820     & 0.838      & 0.776      & 0.654       & 0.762      & 0.556       & 0.593                    \\ \hline

\subsection{Network Settings}
Our formulation in Equation~\ref{equation:3D_discrete} includes a convolutional encoder and an MLP decoder, similar to the hybrid representation in \cite{chibane20ifnet,Peng2020ECCV}. In this subsection, we explore variations in the network settings, including feature volume resolution, discrete versus continuous MLP, and the depth of MLP, to understand their impact on shape reconstruction results.
We first test our toy network with 3D features using different volume resolutions. In these experiments, we choose to learn a continuous surface represented by the MLP $f(\cdot)$, as in PIFu~\cite{iccv2020PIFu}.
Specifically, we randomly sample 3D points and trilinearly interpolate features at these sampled points from the discrete grid vertices $\{ \mathbb{X}_{whd} \}$, and then use the MLP $f(\cdot)$ to compute the TSDF value. 
Table~\ref{tab:resolution_ablation} summarizes the results of various settings, where the F-number and M-number in each column represent  the volume resolution and the MLP depth, respectively. From the left three columns, it is evident  that increasing the volume resolution from $64$ to $256$ can significantly reduce reconstruction errors by about $30\%$, indicating that a high-resolution feature volume is crucial for precise results.


We further test other network settings. 
The two columns with an M* indicate results with a discrete MLP, which computes TSDF results only on the grid vertices $\{ \mathbb{X}_{whd} \}$. From these two columns, it is apparent that the discrete MLP only slightly compromises result quality, and even a 1-layer discrete MLP can achieve satisfactory  results. 
The rightmost column with an F' represents  results where convolution is not applied to the 3D feature volume. In this scenario, even a high-resolution 3D feature volume produces a substantial  error, which clearly highlights the effectiveness of 3D convolution.

%Meanwhile, it is also crucial to learn the 3D context by 3D convolution. After removing the 3D convolution, the shape error is significantly increased, e.g. from $0.42$ to $0.59$ for 256 grid resolution and a 5-layer MLP. 


%In order to further illustrate the motivation of our network designs, we extend the pipeline described in \ref{sec:feature}: 


%Building the feature volume with different resolution (from 64 to 256) by the method described in \ref{sec:feature}, 

% and instead of compute the TSDF value for all the grids in visual hull, we randomly and continuously sample 3d points, then tri-linearly interpolate features for these sampled 3d points from the feature volume and use a deep MLP to compute the TSDF value, 

%this experiment is to figure out the relation between feature volume resolution and shape accuracy without being affected by the quantization (considering the quantization error with low resolution volume, we continuously sample the 3d points for training like ~\cite{iccv2020PIFu}). 

%In order to see the performance gain from 3d context, based on the previous setting, we add sparse 3D convolution after feature volume. 

%Based on setting 2 with 256 resolution feature volume, we remove the continuously sampling and deep MLP, and predict TSDF for all the grids in the visual hull(in our experiment, it is implemented by simply applying 1$\times$1$\times$1 convolution at the end of sparse convolution module).


%The summarized Chamfer and P2S errors shows in Table~\ref{tab:resolution_ablation} shows that higher resolution feature volume and the 3d context feature extracted from sparse 3d CNN is the key to improve the geometry accuracy (setting 1 and setting 2), 

%However, the memory cost dramatically increases with larger feature volume, hence in the last setting we can see that removing deep MLP only causes only small performance loss, while shallow MLP allows us to predict the dense TSDF volume with reasonable memory cost, which enables us to perform fine-grid (e.g. 512 resolution volume) shape estimation upon it. 


%\ping{I suggest moving this figure to the supplementary file.}
%Figure~\ref{fig:resolution_ablation} also shows that the convolution module can help the network predicting the shape with challenging poses and multi-person. At the first row all the settings without sparse convolution only generate broken legs due to the lack of 3d context feature, and the case (a) shows that the continues sampling strategy can not ensure high quality details with low resolution feature volume, more surface details appears only when we use higher resolution feature volumes. Meanwhile, from the second row we can see all the settings without sparse convolution can not recover correct shape due to the severer occlusion caused by multi-human case.

%we motivate high-res volume regression. To this end, we propose a coarse to fine framework to overcome the memory/computation limits. 

%\ping{some more thoughts are needed. In Equation~\ref{equation:3Dfeature}, we still have the MLP, i.e. $f$, which is continuous and should not have the discretization problem.}


%Previous arts using implicit functions encode each 3D point $X$ into a latent code $F$ then regress a continuous occupancy or TSDF representation $s$ of the shape:

%SMPL model used as a shape prior in~\cite{zheng2021deepmulticap,zheng2021pamir} provides point-to-point correlation, however, it is fragile to inaccurate SMPL estimation and with limited shape representation ability. 
%To tackle limitations above, we proposed a novel learn-able 3D feature volume as the shape context agent, here we call it PALVO, and then 
%We estimate a TSDF $\mathbb{D}$ of the observed clothed human surface from sparse multi-view images $\{\ve{I}_i\}$. 
%With PALVO, the computation of $F$ in Eq.~\ref{equation:PIFu_formulation} would be spatial structure awareness in our framework. 
%Furthermore, in order to guarantee the granularity of volumetric shape representation, we designed a coarse to fine architecture to achieve a proper volume resolution.

% The shape formation becomes: 
% \begin{equation}
% 	f(V, F_{\{I_i\}}(V)) = \mathbb{D}
% 	\label{equation:PEVO_formulation}
% \end{equation}

%Based on PALVO, our goal is to estimate a TSDF $\mathbb{D}$ capturing the observed clothed human from sparse multi-view images $\{\ve{I}_i\}$.

\subsection{Quantization Error}\label{sec:quantization}
The formulation in Equation~\ref{equation:3D_discrete} involves discretization, which is often undesirable and motivates implicit function representation~\citep{Mescheder2019,Park2019,Chen2019}. In the following, we analyze the quantization error of the TSDF $\mathbb{D}$ with different volume resolutions. Surprisingly, we find that with relatively high volume resolution, e.g. 512 or higher, the quantization error is no longer a limiting factor for the reconstruction accuracy of clothed humans. 
Specifically, we compute ground truth TSDFs according to the ground truth mesh models in the THuman2.0~\cite{zheng2021deepmulticap} dataset. We then discretize those TSDFs into volumes of different resolutions, ranging from $32\times32\times 32$ to $1024\times1024\times1024$. The quantization error is measured by the average error in TSDF values on the ground truth mesh surface. As shown in Figure~\ref{fig:quantization_error}, the quantization error drops quickly with higher volume resolution. 
When the volume resolution is 512 or higher, the quantization error is less than 0.05 cm, much smaller than the reconstruction error of SOTA methods~\cite{iccv2020PIFu,zheng2021pamir,zheng2021deepmulticap}, which typically exceeds 0.5 cm. This indicates that  discrete TSDF representation is not a limiting factor for clothed human reconstruction with a volume resolution of 512 or higher. 



\section{Method}
% \ping{stress the culling of voxels to make it efficient.}
As discussed in Section~\ref{sec:motivation}, a convolutional encoder with 3D feature volume can boost shape reconstruction accuracy. While discretization causes additional quantization errors, a high-resolution volume can effectively mitigate this issue. Therefore, it is important to design an efficient method to overcome the memory and computation costs associated with high-resolution volumes for improved results. For this purpose, we design a sophisticated voxel culling process, implement a coarse-to-fine strategy, and employ the efficient submanifold sparse convolutional networks~\cite{3DSemanticSegmentationWithSubmanifoldSparseConvNet}. 


Figure~\ref{fig:pipeline} shows our system pipeline for shape reconstruction. Our method works in two stages from coarse to fine. In the coarse stage, we adopt a $256\times 256\times 256$ resolution volume and employ the visual hull of the foreground object to eliminate irrelevant voxels. 
The remaining voxel grids are associated with image features at their projected positions. We then apply the efficient subspace sparse 3D CNN~\cite{3DSemanticSegmentationWithSubmanifoldSparseConvNet} to compute an initial TSDF, $\mathbb{D}_0$. Unlike conventional 3D CNN, sparse 3D CNN builds a hash-table for indexing non-zero elements and the convolution operator only applies to those non-zeros elements, which makes the convolution computational and memory efficient when the input tensor is sparse. In the fine stage, we focus on the narrow band nearby the zero-level set of $\mathbb{D}_0$ and discretize that narrow band into smaller voxels of $512\times 512\times 512$ resolution.
Each voxel grid is then associated with features from normal maps computed from the input images by the method~\cite{hourglass16}. We further fuse a coarse geometry feature from the coarse level, and apply the sparse 3D CNN again on the fine volume to compute the final TSDF, $\mathbb{D}_{\text{final}}$. The visual hull culling and narrow-band culling substantially  reduce the sampling grids, making the feature volume sparse enough for efficient sparse convolution.

After reconstructing the 3D shape, we proceed to estimate the surface texture. Texture maps need even higher resolution to capture appearance details. To address this problem, instead of na\"{\i}vely applying our TSDF regression network to compute a color volume that evaluates color as a function of coordinates, we choose to solve a field of blending weights. At each 3D point, the surface color is the weighted average of the colors at its image projections. We only solve the blending weights for a narrow band nearby the final surface $\mathbb{D}_{\text{final}}$ for better efficiency. Our pipeline to solve this blending weight volume is shown in Figure~\ref{fig:pipe_color}.
%volume with the same narrow-band used in fine-stage, and the vertices color of predicted mesh are computed by aggregating the RGB color from input images by the blending weights interpolated from the blending weight volume. 
%\ping{1-2 sentences to describe the texture estimation. }


\subsection{Feature Volume Construction}\label{sssec:featurevolume}
To construct the feature volume, we initial a cubic volume grid $\mathbb{V}$ with an edge length of $256$cm and a resolution  of ${256}\times{256}\times{256}$. We then project each voxel grid point $\mathbb{V}_{whd}$ on the input mask images $\{{M}_i\}$ to discard points outside of the visual hull. Pruning by the visual hull significantly reduce the number of `active' voxel vertices in the volume. Typically, over $95\%$ of the voxel grids are culled away by the visual hull, leaving around 200--300K voxels remaining. Given the set of remaining voxel grids $\{ \mathbb{V}_{whd} \}$, we project them onto feature maps to compute a feature on each grid vertex as follows,
\begin{equation}
	\mathbb{F}^I_{whd} = Mean({F}^{I}_i(\Pi_i (\mathbb{V}_{whd}))).
	\label{equation:projection}
\end{equation}
Here, $\Pi_i$ is the perspective projection of the input image ${I}_i$. We sample the 2D feature maps ${F}^I_i$ using bi-linear interpolation at the projected positions, and average the sampled features from all views to compute the feature volume $\mathbb{F}^I$. The image feature ${F}^I_i$ is computed from the input image ${I}_i$ by a single stacked hourglass network~\cite{hourglass16}, which has 128 feature channels and at resolution of $256\times256$. 
%, by doing this we obtain the 2D feature maps $\ve{F}_i$ with size 128x512x512, which are used to infer the geometry of observed human body.

% \ping{we need to explain the details of the image feature map $\ve{F}_i$. This questions was asked by the reviewers last time.}

%\\


%2. By the analysis in the introduction section, we can see that the quantization artifacts caused by dicritization can be neglected when using proper TSDF volume resolution. Hence, for the purpose of reducing cubical growth of memory overhead to make the computation of shape regression feasible, we designed our coarse to fine architecture which is described in sec3.2.

% The explicit representation may cause quantization artifacts. For this concern, we did another experiment to measure the quantazation error caused by discritizing the 3d space. Specifically, we compute a groundtruth discrtized TSDF volume with different volume resolution (32,64,128,256,512,1024), and for each resolution, we evenly sample 3d points on groundtruth mesh surface, and use the 3d location of these sampled points to get its tsdf value by interpolating in discrtized TSDF volume. Because of the discritization, the sampled TSDF value will not be zero, the table ~\ref{fig:sdf_error} shows the quatization error with different volume resolution.


%% Figure environment removed

\subsection{Coarse to Fine Reconstruction}
%\sicong{In human dataset (e.g. thuman2.0). Stress proper volume resolution(512) will not casued obvious quantization error issue, and to make the 512 volume regression fesible. we introduce coare to fine archtecture. (narrowband, fine stage, small kernal size, aspp)}

To ensure the memory consumption and inference speed, we take a coarse-to-fine architecture to compute the TSDF $\mathbb{D}$. %from the feature volume $\mathbb{F}^c$.
%Briefly speaking, we firstly using sparse convolution neural net to regress a coarse shape, then we re-sample features grids around the coarse surface for further refinement. 

\paragraph{Coarse stage}
We use a 3D U-Net with skip layers to encode the topology context. As shown in Figure~\ref{fig:pipeline} the 3D U-Net consists of three conv and deconv blocks with skip connections. The initial TSDF $\mathbb{D}_0$ at each volume grid is computed by an FC layer, i.e. the MLP $f(\cdot)$ in Equation~\ref{equation:3D_discrete}. More network details are provided in the supplementary file. We have experimented with more FC layers and empirically found that adding more layers does not help, thanks to the 3D convolutional feature encoder with proper local information encoding.
%since we find with topology context encoding, adding more layers to MLP does not contribute to more accurate shape.
%The network structure is implemented based on sparse convolution, the complexity will not be affected by the resolution of the volume grid. 
This network outputs a coarse TSDF $\mathbb{D}_0$ with the same resolution as the feature volume $\mathbb{F}^I$. 
During training, we calculate the ground truth TSDF for each clothed human model from the ground truth mesh model. We further truncate the TSDF value within [-5cm,5cm]. The training loss function for the coarse stage is then defined as:
\begin{equation}
	L_c = \sum_{(w,h,d) \in \mathcal{V}_{\text{hull}} }
	 \left \| (\mathbb{D}^{\text{pred}}_{whd} + bias_c) - \mathbb{D}^{\text{gt}}_{whd} \right \|_{L1} 
	\label{equation:coarse_loss}
\end{equation}
Here, $\mathbb{D}^{\text{pred}}$ and $\mathbb{D}^{\text{gt}}$ are the predicted and ground truth TSDF volume respectively, and $\mathcal{V}_{\text{hull}}$ is the set of remaining voxel grids after visual hull culling. 
The training loss is the L1 distance between the predicted and ground truth TSDF values. 
Note that the predicted TSDF values of voxel grids outside the visual hull will be zero according to the submanifold sparse convolution (SSC). Hence, we add a constant bias $bias_c = 0.05$m which is the truncation distance to the TSDF.

% Figure environment removed

\paragraph{Fine stage}
After getting the coarse TSDF $\mathbb{D}_0$, we use another branch to further refine geometry details. At this stage, we down-sample the volume to denser voxels and associate each voxel vertex with high frequency shape information encoded in normal maps like~\cite{zheng2021deepmulticap,saito2020pifuhd}. To facilitate computation, we focus on a narrow band nearby the zero-level set of $\mathbb{D}_0$.
%Different from coarse stage, we shift the focus of the network from visual hull space to the surface details. 
Specifically, we  tri-linearly interpolate the coarse TSDF volume $\mathbb{D}_0$ by $2$ times to a $512\times512\times512$ voxel grid. We only preserve all the voxel vertices satisfying: $ \left | \mathbb{D}_0 \right | < 0.03m$, which are within a narrow band of 6cm width around the zero-level set surface of $\mathbb{D}_0$. Figure~\ref{fig:c2f} shows the narrow band for re-sampling.
This narrow band removes irrelevant voxel vertices based on the initial result, helping to further improve the storage and computation efficiency of our method.
%the narrow band computed from coarse TSDF volume allows us to locate the potential surface region efficiently, which means we can apply 3D sparse convolution even at very high resolution grid. 

We compute a feature at each voxel vertex in the fine stage  from the normal feature maps as follows,
\begin{equation}
	\mathbb{F}^N_{whd} = Mean({F}^N_i(\Pi_i (\mathbb{V}_{whd}))).
	\label{equation:fine_projection}
\end{equation}
We use the same normal estimator proposed in ~\cite{zheng2021deepmulticap} to estimate the normal image $N_i$ for each input view $I_i$, and the normal feature map ${F}^N_i$ is computed from the input normal image ${N}_i$ by the same hourglass network as ${F}^I_i$. Furthermore, this feature is concatenated with the down-sampled coarse level features $\mathbb{F}^I_{whd}$ at the last two convolution layers, which have large receptive fields and encode strong shape information.
Since the fine stage mainly focuses on local shape details, we use a shallow sparse 3D CNN which has 3 Conv blocks followed by an FC layer to regress the final TSDF volume. More details of network architecture are in the supplementary file.

We define the training loss for the fine stage as follows,
\begin{equation}
	L_f = \sum_{(w,h,d) \in \mathcal{V}_{\text{band}}}
	\left \| ( \mathbb{D}^{\text{pred}}_{whd} + bias_f) - \mathbb{D}^{\text{gt}}_{whd} \right \|_{L1}.
	\label{equation:fine_loss}
\end{equation}
Here, $\mathcal{V}_{\text{band}}$ is the set of voxel grids within the narrow band. We also add a constant bias $bias_f = 0.03m$ to deal with vertices outside of the narrow band. 

\subsection{Texture Prediction}
With the shape reconstructed, we then estimate the color at each surface point. Instead of solving a color field encoded by an implicit function like the earlier works\cite{iccv2020PIFu,shao2022doublefield}, we exploit high-resolution input images for rich appearance details. 
%The earlier works PIFu\cite{iccv2020PIFu} estimates an MLP to compute the color as an implicit function, which tends to produce blurry textures. 
%The purpose of this part is to predict the texture for any surface vertex of the predicted mesh generated by shape prediction module. A straightforward way is to extend our shape prediction network to regress a color volume \ping{PIFu computes a color volume, which should be blurry. What about DoubleField?}.
%However, due to the limited representative ability of color volume, directly regressing the color per-voxel will generate blurry results. Inspired by SparseNeus\cite{sparsenues}, 
Specifically, we estimate a blending weight vector $\mathbf{W}$ at each surface point $\mathbf{X}$. The color at $\mathbf{X}$ is then computed as a weighted average of the colors at its image projections,
\begin{equation}
    c(\mathbf{X}) = \sum_i \mathbf{W}_i \mathbf{I}_i (\mathbf{x_i}),
    \label{equ:color}
    \vspace{-0.1in}
\end{equation}
where $\mathbf{x}_i = \Pi_i(\mathbf{X})$ is the projection of $\mathbf{X}$ in image $\mathbf{I}_i$. Similar to shape reconstruction, we also sample a volume grid $\mathbb{W}$ and estimate the blending weights at the grid vertices $\mathbb{W}_{whd}$. In this way, our method essentially estimates a blended texture map over the surface, instead of computing a color field which tends to be limited by the 3D sampling rate. With our method, the predicted texture map carries sharp appearance details inherited from the input images.

%Furthermore, we can choose a coarse resolution for the weight volume $\mathbb{W}$, since most of the high frequency details are encoded in the input images. In practice, we use $256\times256\times256$ resolution for $\mathbb{W}$.

%Meanwhile, the resolution of blending weight can be light weight compared to the TSDF volume because the texture details can be recovered by interpolating the smooth blending weight volume due to blending weight changes smoothly among adjacent grids.


%volume, for any given 3D point $\mathbf{X}$, we project it onto each input view to get the corresponding pixels $\mathbf{I}_{i}(\Pi_i(\mathbf{X}))$, then the predicted color of the sampled 3d point should be the linear combination of the color of projections: $C_v = \sum_{i}{\mathbb{W}_{iv} * I_{iv}}$, here the weights of projections from input views are blending weights $\mathbb{W}_{iv}$. 


%By using blending weight volume, the predicted texutre will not suffer from blurry artifacts since the predicted mesh color is directly derived from the input image. 




% Figure environment removed

%% Figure environment removed

% \ping{use consistent symbols as the first part of the paper, use mathbb, mathbf, etc}

The network architecture of our texture weight estimation is shown in Figure~\ref{fig:pipe_color}. Thanks to the precisely reconstructed TSDF $\mathbb{D}_{\text{final}}$ from the shape branch, we only consider a 2cm width narrow-band nearby its zero-level set, which is defined as $ \left | \mathbb{D}_{\text{final}} \right | < 0.01m$. For each input image, we compute its texture feature ${\mathbf{F}^C_i}$ and construct a volume ${\mathbb{F}^C_i}$ by projecting them back to the discretized narrow-band. 
The texture feature maps $\mathbf{F}^C_i$ are also computed by an hourglass ~\cite{hourglass16} network from the input image $\mathbf{I}_i$ with a smaller network to extract a 32-channel feature of size $256\times256$. We further compute the truncated PSDF~\cite{tao2021function4d}, which is a view-dependent function indicating if the surface is viewed from a slanted direction, and concatenate it to the texture feature. 

% We then use an attention model~\cite{attention} to re-weight these features across different views. The PSDF value helps the attention network to reweight the texture features. In principle, if a 3D point is occluded from a view, the texture feature collect from that view should be less weighted. Finally, we apply a sparse 3D CNN on these reweighted features and normalize the results across views by a soft-max to obtain the normalized blending weights $\mathbb{W}$.

The attention model~\cite{attention} is used here to handle visibility by re-weighting features across different views. Ideally, if a 3D point is not visible from a particular view, the projected color from that view should have less influence on the final blended color. Therefore, the attention module is used to adjust the contribution of the projected texture features by re-weighting them. Following this, we apply a sparse 3D CNN on these re-weighted feature volumes individually to regress the blending weight volume of each view. We then normalize these blending weight volumes across views by a soft-max to obtain the normalized blending weights $\mathbb{W}$.

%the distance between the sample point and the nearest shape surface, to be concatenated with the texture feature, here PSDF is view-dependent, it describe the occlusion information for the given camera view, which can be used to determine the blending weight regression.

The training loss for the texture blending weight estimation is defined as:
\begin{equation}
	L_c = \sum_{(w,h,d) \in \mathcal{W}_{\text{band}}}
	\left \|  \mathbb{C}^{\text{pred}}_{whd}  - \mathbb{C}^{\text{gt}}_{whd} \right \|_{L1}.
	\label{equation:color_loss}
     \vspace{-0.1in}
\end{equation}
Here, $\mathbb{C}^{\text{pred}}_{whd}$ is the surface color volume computed by applying the blending weights  $\mathbb{W}_{whd}$. The ground truth color volume $\mathbb{C}^{\text{gt}}_{whd}$ is generated from the ground truth textured mesh with nearest search. The set $\mathcal{W}_{\text{band}}$ includes all voxel grids within the narrow band for color estimation.

% After obtaining the blending volume, in order to get the mesh texture, a naive way is to use each vertex of the predicted mesh to do color aggregation. Specifically, we projecting the sampled mesh vertex $v$ \ping{again, use consistent symbols as the earlier part of the paper.} onto each input view and get the interpolated RGB value $C_{iv}$, then the final color is given by the following equation: $C_v = \sum_{i}{W_{iv} * C_{iv}}$\ping{you need to move the explanation of color computing to the beginning of this subsection}. However, the blended mesh texture still blurry since the high frequency detailed texture within each mesh face is missing, this is because only mesh vertices has texture color. A simple but effective way to solve this problem is to subdivide the predicted mesh to get more mesh vertices \ping{this is very brute force, you introduced too many unnecessary vertices}, by doing this the blending mesh texture can achieve the same quality as input images. From the comparison on fig ~\ref{fig:blending_vs_regression} we can see that the result shows the sparse blending volume can be used to generate very high quality mesh texture.

%\ping{you haven't explain how to define loss, how to train the network, etc.}

% % Figure environment removed

% Figure environment removed

After solving the blending volume $\mathbb{W}$, we use the Blender ~\cite{Blender} to generate a texture atlas map of the reconstructed mesh model. For each pixel in the atlas map, we use barycentric interpolation to compute its 3D location from the mesh vertices, and then determine its color according to Equation~\ref{equ:color}. Figure ~\ref{fig:atlas} shows the atlas map computed by our method. To capture rich details in the input images, a high-resolution texture atlas map, such as 2K resolution, can be chosen. In this way, our method generates high-quality textured results.

%the final texture of each for blending. 
%By using blending volume the predicted mesh texture can achieve the same quality as input images. 
%in order to get the mesh texture, a naive way is to use each vertex of the predicted mesh to do color aggregation. However, the blended mesh texture still blurry since the high frequency detailed texture within each mesh face is missing, this is because only mesh vertices has texture color. A simple but effective way to solve this problem is to subdivide the predicted mesh to get more textured mesh vertices. Or alternatively 

