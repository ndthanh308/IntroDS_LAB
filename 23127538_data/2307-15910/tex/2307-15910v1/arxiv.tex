
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out
                                                          % if you need a4paper
%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4
                                                          % paper

\IEEEoverridecommandlockouts                              % This command is only
                                                          % needed if you want to
                                                          % use the \thanks command
\overrideIEEEmargins
% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document


\usepackage{subfigure}

\usepackage{setspace} 
\setstretch{0.978}
\usepackage{colortbl}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{epsfig} % for postscript graphics files
\usepackage{mathptmx} % assumes new font selection scheme installed
\DeclareMathAlphabet{\mathcal}{OMS}{cmsy}{m}{n}
\usepackage{times} % assumes new font selection scheme installed
\usepackage{amsmath} % assumes amsmath package installed
\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{tabularx}
\usepackage{dcolumn}
\usepackage{color} 
\usepackage{algorithm} 
\usepackage{algorithmic}  
\usepackage[algo2e,linesnumbered,vlined,ruled]{algorithm2e} 
\usepackage{balance}
\usepackage{accents}
\usepackage{resizegather}
\usepackage{theorem} 
\usepackage{float}
\usepackage{mathtools}
\usepackage{multirow}
%\mathtoolsset{showonlyrefs} 
\usepackage{caption}

\usepackage[normalem]{ulem}
\usepackage[short]{optidef}
\usepackage{xcolor}
\usepackage{adjustbox}

\usepackage{setspace} 
\setstretch{0.935} 

\DeclareMathOperator{\ap}{\emph{A}\MRkern \emph{P}}
\newcommand{\MRkern}{%
  \mkern-3mu
  \mathchoice{}{}{\mkern0.2mu}{\mkern0.5mu}%
}

\newtheorem{definition}{\bf{Definition}}
\newtheorem{assumption}{\bf{Assumption}}
\newtheorem{example}{\bf{Example}}
\newtheorem{problem}{\bf{Problem}}
\newtheorem{remark}{\bf{Remark}}
\newtheorem{theorem}{\bf{Theorem}}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}[theorem]{\bf{Proposition}}
% \newtheorem{corollary}[theorem]{\bf{Corollary}}
\newtheorem{corollary}{Corollary}[theorem]

\renewcommand\thesubfigure{(\alph{subfigure})}

%\newcommand{\arg}{arg}
\newcommand{\LTLUNTIL}{\ensuremath{\mathcal{U}}}
\newcommand{\LTLEVENTUALLY}{\ensuremath{ F}}
\newcommand{\LTLALWAYS}{\ensuremath{ G }}
\newcommand{\LANG}{\ensuremath{\mathcal{L}}}
\newcommand{\FUNCTION}[2]{\STATE \textbf{function} #1(#2)}
\newcommand{\st}{\sigma_{\tau}}
\newcommand{\pt}{\psi}
\newcommand{\St}{\mathcal{S}_{\tau}}
\newcommand{\Mt}{\mathcal{M}_{\tau}}
\newcommand{\sigt}{s^{t:t+\tau}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\newcommand{\BF}{\mathbf}
\newcommand{\notltl}{\neg}
\newcommand{\andltl}{\wedge}
\newcommand{\orltl}{\vee}
\newcommand{\Next}{\BF{X}}
\newcommand{\Always}{\BF{G}}
\newcommand{\Event}{\BF{F}}
\newcommand{\Until}{\CA{U}}
\newcommand{\Implies}{\Rightarrow}
\newcommand{\Not}{\lnot}
\newcommand{\True}{\top}
\def\prop{\TT{data}}
\def\popt{\pi}

\newcommand{\buchi}{B\"uchi\ }

%% Symbols of automata
%\newcommand{\PA}{\mathcal{P}}
\newcommand{\BA}{\mathcal{B}}
\newcommand{\FP}{\mathcal{P}}
\newcommand{\FA}{\mathcal{A}}
\newcommand{\TS}{\mathcal{T}}
\newcommand{\LA}{\mathcal{L}}
\newcommand{\KA}{\mathcal{K}}
\newcommand{\tuple}[1]{\ensuremath{\left \langle #1 \right \rangle }}
\DeclareMathOperator*{\argmax}{arg\,max}
\title{\LARGE \bf
%  Probabilistically Guaranteed Satisfaction of Temporal Logic Constraints During Reinforcement Learning}

%An Automaton-Based Approach to Reinforcement Learning Under Probabilistic Constraints

Reinforcement Learning Under Probabilistic Spatio-Temporal Constraints with Time Windows}

\author{Xiaoshan Lin$^{1*}$, Abbasali Koochakzadeh$^{2*}$, Yasin Yaz{\i}c{\i}o\u{g}lu$^3$, and Derya Aksaray$^4$ %<-this % stops a space
%\thanks{*This work was partially supported by the Office of Naval Research under grants ONR N00014-14-1-0554 and MURI N00014-09-1051 and by the National Science Foundation under grants NRI-1426907 and CMMI-1400167.}% <-this % stops a space
\thanks{$^*$These authors contributed equally to the paper.}
\thanks{This paper was supported by DARPA contract HR0011-21-2-0015.}
\thanks{$^1$X. Lin is with the Department of Aerospace Engineering and Mechanics, University of Minnesota, Minneapolis, MN, 55455, {\tt\small lin00668@umn.edu}.
}
\thanks{$^2$A. Koochakzadeh is with the Department of Electrical and Computer Engineering, Purdue University, West Lafayette, IN, 47907, {\tt\small akoochak@purdue.edu}}
\thanks{$^3$Y. Yaz{\i}c{\i}o\u{g}lu is with the Department of Mechanical and Industrial Engineering, Northeastern University, Boston, MA, 02115, {\tt\small y.yazicioglu@northeastern.edu}.}
\thanks{$^4$D. Aksaray is with the Department of Electrical and Computer Engineering, Northeastern University, Boston, MA, 02115, {\tt\small d.aksaray@northeastern.edu}}
}



\begin{document}
%\bibliographystyle{abbrv}
\maketitle
% \def\thefootnote{*}\footnotetext{These authors contributed equally to this work}\def\thefootnote{\arabic{footnote}}


\begin{abstract}
We propose an automata-theoretic approach for reinforcement learning (RL) under complex spatio-temporal constraints with time windows. The problem is formulated using a Markov decision process under a bounded temporal logic constraint. Different from existing RL methods that can eventually learn optimal policies satisfying such constraints, our proposed approach enforces a desired probability of constraint satisfaction throughout learning. This is achieved by translating the bounded temporal logic constraint into a total automaton and avoiding ``unsafe" actions based on the available prior information regarding the transition probabilities, i.e., a pair of upper and lower bounds for each transition probability.  We provide theoretical guarantees on the resulting probability of constraint satisfaction. We also provide numerical results in a scenario where a robot explores the environment to discover high-reward regions while fulfilling some periodic pick-up and delivery tasks that are encoded as temporal logic constraints. 

% We propose an automata-theoretic approach for constrained reinforcement learning (RL) on a Markov Decision Process with unknown transition probabilities. We use bounded temporal logic to specify complex time-bounded constraints. By translating the bounded temporal logic into a total automaton, the proposed approach allows for evaluating the risk of constraint violation under some partial information about the transition model. By avoiding ``unsafe" actions that can violate constraint satisfaction, the proposed algorithm learns a policy with a theoretical guarantee that the probability of constraint satisfaction is above the desired probability in every learning episode. With the proposed approach, the trade-off between constraint satisfaction and reward collection can be achieved by adjusting the desired probability and the partial information about the transition model. We numerically test the proposed constrained RL algorithm in a scenario where a robot explores the environment to discover high-reward regions while it fulfills the periodic pick-up and delivery tasks with a probability greater than a user specified threshold. 

% We propose a novel constrained reinforcement learning method \color{black} for finding optimal policies in  Markov Decision Processes while satisfying temporal logic constraints with a desired probability throughout the learning process. 
% An automata-theoretic approach is proposed to ensure the probabilistic satisfaction of the constraint in each episode, which is different from penalizing violations to achieve constraint satisfaction after a sufficiently large number of episodes. The proposed approach is based on computing a lower bound on the probability of constraint satisfaction and adjusting the exploration behavior as needed. We present theoretical results on the probabilistic constraint satisfaction achieved by the proposed approach. We also numerically demonstrate the proposed idea in a drone scenario, where  the constraint is to perform periodically arriving pick-up and delivery tasks and the objective is to fly over high-reward zones to simultaneously perform aerial monitoring.   


%We present a novel approach to reinforcement learning, where a temporal logic specification expressing a persistent mission is aimed to be satisfied throughout the learning process. In real-life applications of autonomous systems and robots, specifications implying a persistent mission may encode safety (e.g., never run out of fuel) or a critical mission requirement (e.g., periodically visit a specific location). These specifications are required to be repetitively satisfied during the operation. In this paper, we benefit from rich expressiveness of the temporal logics to express a given task to be persistently accomplished. We propose a reinforcement learning algorithm that maximizes the expected sum of rewards while ensuring probabilistic guarantees on the satisfaction of the desired task in every episode during learning. We prove the correctness of the proposed algorithm and show simulation results.
\end{abstract}

\section{Introduction}

Reinforcement learning (RL) has been widely used to learn optimal policies for Markov decision processes (MDPs) through trial and error. Due to its capability to learn from interactions, adapt to changes in the environment, and handle different types of tasks in a general setting, RL has been widely adopted in the field of robotics. When the system is subject to constraints, traditional RL algorithms can learn a feasible policy by penalizing actions that lead to violating the constraints (e.g., \cite{49999}, \cite{sutton2018}). However, the lack of formal guarantees on constraint satisfaction during
the training phase can potentially cause undesirable outcomes, which limits the application of RL in many real-world scenarios. %\cite{amodei2016concrete}. 

In recent years, safety in RL has been intensively studied. For example, control barrier function can be adopted to ensure safety during reinforcement learning by constraining the exploration set \cite{cheng2019} or augmenting the cost function \cite{marvi2021safe}. Safety in RL can be achieved by solving the constrained Markov decision process, where a cost function is used in a way that trajectories with expected costs smaller than a safety threshold will be considered safe (e.g., \cite{hasanzadezonuzy2021learning,Simo2021AlwaysSafeRL}, \cite{Altman1999ConstrainedMD, moldovan2012safe}). %Furthermore, safe RL obtains a policy that maximizes the expected reward among the feasible trajectories (e.g., \cite{efronijjj}).
In conjunction with policy-gradient RL algorithms, the establishment of safety frameworks using the Hamilton-Jacobi reachability method is proposed in \cite{general-safety} to guarantee safety during learning for safety-critical systems. While many studies in safe RL ensure safety by avoiding undesirable states or actions, constraints beyond safety are not explored in the aforementioned works.

Temporal logic (TL) is a specification language that can describe rich behaviors, making it one of the most useful tools for specifying complex tasks for robots. There exist some works in the literature that explore constrained RL under TL constraints. For example, the works in \cite{hasan2020,cai2023safe} encode the desired constraints as Linear Temporal Logic (LTL) in a model-free learning framework and maximize the probability of satisfying LTL constraints. In \cite{alshiekh2018}, the risk of violating LTL constraints can trigger a reactive shield system that modifies the chosen action to ensure constraint satisfaction with maximum probability. Alternatively, Gaussian Process is adopted to estimate the uncertain system dynamics to ensure safety with high probability in \cite{cai2021safe}. In \cite{aksaray2021probabilistically}, the authors propose a constrained RL algorithm with probabilistic guarantees on the satisfaction of relaxed bounded TL constraints throughout learning. 

%considers bounded TL constraints, which can describe richer behaviors with explicit time windows. type of TL to represent explicit time constraints. While most of the literature in safe RL with TL constraints aim at maximizing the probability of satisfaction, these approaches do not provide a formal guarantee of the satisfaction rate, especially in the early stages of the learning process. Since RL is usually applied on MDPs with unknown transition probabilities, some RL algorithms can formally guarantee the satisfaction of constraints, given some information on the system's transition probability. For example, by overestimating the transition uncertainty, \cite{aksaray2021probabilistically} guarantees the probability of satisfying a relaxed bounded TL constraint is always greater than a desired threshold. In \cite{cai2021safe}, Gaussian Process is adopted to estimate the uncertain system dynamics to ensure safety with high probability.

This paper addresses a  constrained RL problem as in \cite{aksaray2021probabilistically}. The objective is to learn a policy that ensures the satisfaction of a bounded TL constraint with a probability greater than a desired threshold in every episode. The main differences of this paper from \cite{aksaray2021probabilistically} are as follows: 1) The method in \cite{aksaray2021probabilistically} has guarantees on satisfying a temporal relaxation of the TL constraint. On the other hand, we provide guarantees on the satisfaction of TL constraint without any relaxation. 2) The method in \cite{aksaray2021probabilistically} is obtained by first deriving a closed-form lower bound on the probability of constraint satisfaction, which is applicable under a more restrictive setting in terms of the MDP, TL constraint, and available prior information. For instance, that method is not applicable under TL constraints such as ``globally stay in a specified region". In this paper, we significantly relax these limitations and, instead of a closed-form expression, we provide a recursive algorithm for computing a lower bound on the probability of constraint satisfaction in this generic setting.  

%on temporally relaxed  relaxed bounded TL, the proposed method uses a total FSA to represent a non-relaxed bounded TL constraint which enables tracking violation trajectories as well as the satisfying ones. We proposed a generic algorithm that can guarantee probabilistic satisfaction of a bounded TL with prior knowledge of lower and upper bound of the transitions' uncertainty.

% Moreover, in \cite{aksaray2021probabilistically} they assume a priori knowledge of the system's transition probabilities which reveals the most likely transition for each state in the system. Using this a priori knowledge they provide a probabilistic guarantee for TL constraint satisfaction. In contrast, with a generic setup, this paper guarantees TL satisfaction where there is no need for knowing likely transitions in the system. 

% Reinforcement learning (RL) has been widely used to learn optimal control policies for Markov Decision Processes (MDPs) through trial and error [23]. When the system is also subject to constraints, traditional RL algorithms can be used to learn optimal feasible solutions after a sufficiently long amount of training by severely penalizing infeasible trajectories. However, this approach does not provide any formal guarantees on constraint satisfaction during the early stages of the learning process. Hence, this is not a viable approach for many real-life applications where the constraint violations during training may have severe consequences. 
% The use of reinforcement learning (RL) in distributed control algorithms has demonstrated impressive results in the past decade. Therefore, RL is emerging as a viable alternative for traditional decision-making and control tasks in engineering systems, e.g. multi-agent systems, robotics, and energy systems. However, the issue of safe exploration still prevents RL to be widespread use in the real world \cite{amodei2016concrete}. To overcome this challenge, traditional algorithms facilitate learning optimal feasible solutions for Markov Decision Processes (MDPs), by sufficiently penalizing infeasible actions (e.g., \cite{altman1999}, \cite{49999}, [cite book]). Lacking formal constraint satisfaction guarantees for early stages in learning process where constraint violations may cause catastrophic and unacceptable outcomes. Engineering a reward signal that allows the agent to maximize its performance while remaining safe is not trivial (e.g., [cite book], \cite{10.1145/2461328.2461372,Simo2021AlwaysSafeRL}).
% % physical systems are not able to be trained by applying this approach 


% Safe reinforcement learning ensures reasonable performance or safety while learning optimal policies (e.g., \cite{garcia2015comprehensive}), over a constraint Markov Decision Process (MDP) where agents-environment interactions are modeled (e.g., \cite{efroni2020exploration,Simo2021AlwaysSafeRL}, \cite{altman1999}). In order to distinguish between safe and unsafe behavior, cost function is used in a way that trajectories with expected cost smaller than a safety threshold will be considered as safe. Furthermore, safe RL policy obtains a trajectory which maximizes the expected reward among the safe (feasible) trajectories.


% inserting a-priory knowledge or transferring learning ideas, help to reduce the time spent with random actions during exploration, (e.g., [12], [1]).



%Markov Decision Process (MDP) framework has been frequently used in the literature when solving stochastic decision-making and planning problems (e.g., \cite{papadimitriou1987,feinberg2012}). In cases where there is an unknown element in the MDP such as unknown reward or unknown transition probabilities, 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% down this
% Reinforcement learning (RL) has been widely used to learn optimal control policies for Markov Decision Processes (MDPs) through trial and error \cite{sutton2018}. When the system is also subject to constraints, traditional RL algorithms can be used to learn optimal feasible solutions after a sufficiently long amount of training by severely penalizing infeasible trajectories. However, this approach does not provide any formal guarantees on constraint satisfaction during the early stages of the learning process. Hence, this is not a viable approach for many real-life applications where the constraint violations during training may have severe consequences. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%For example, in an aerial monitoring mission, a drone may need to learn the most informative paths to take while also ensuring that it never runs out of fuel or collides with other objects. In such a scenario, any violation of these constraints would have severe consequences.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% down this
% Safe reinforcement learning is the process of learning optimal policies while ensuring a reasonable performance or safety during the learning process \cite{garcia2015}. One prominent way to achieve safe RL is modifying the exploration process so that the system selects actions while avoiding unsafe configurations. For example, prior information or transfer learning ideas can be used to reduce the time spent with random actions during exploration (e.g., \cite{driessens2004,abbeel2005}), which typically do no have theoretical guarantees. Learning can also be achieved over a constrained MDP (e.g., \cite{efroni2020}), where the goal is to maximize the expected sum of reward subject to the expected sum of cost being smaller than a threshold. Such works typically have theoretical guarantees such as bounded regret on the performance and constraint violation. Control-barrier functions or Hamilton-Jacobi methods can also be adopted to stay inside a safe region during learning (e.g., \cite{cheng2019,fisac2018}). In these approaches, safety is mainly understood as visiting ``good states" and avoiding ``bad states". However, complex missions typically involve constraints on not only the current state but also the system's trajectory. For example, suppose that a robot must visit first region A and then region B. Regions A and B may not be categorized as safe or unsafe, but visiting B before visiting A may imply a mission failure. 
% Temporal logics (TL) \cite{baier2008} provide a powerful way of describing such complex spatial and temporal specifications. Some studies in the literature address the RL problem under TL constraints. For example, Linear Temporal Logic (LTL) constraints are considered in a model-free learning framework and maximum possible LTL constraint satisfaction is achieved in \cite{hasan2020}. A reactive system called shield is proposed in \cite{alshiekh2018} which corrects the chosen action if it causes the violation of an LTL specification.  

% In this paper, we introduce a reinforcement learning algorithm for maximizing the expected sum of rewards subject to the satisfaction of a TL constraint during the learning process with some desired probability. The decision-making of an agent is modeled as an MDP, and the constraint is expressed using a bounded TL which is encoded as a finite state automaton. We construct a time-product MDP and formulate the learning problem over the time-product MDP. We prove that the proposed learning algorithm enables the agent to satisfy the TL constraint in each episode with a probability greater than a desired threshold.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%while learning to maximize the cumulative expected reward. 

%This paper is different than the existing works addressing RL under TL constraints as follows: 1) our algorithm gets a desired probability of constraint satisfaction , i.e., $Pr_{des}$, as an input, and 2) we ensure that the probability of TL satisfaction during each episode of learning is at least $Pr_{des}$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% down this
% This paper differs from the existing works on RL under TL
% constraints (e.g., \cite{hasan2020,alshiekh2018}) as follows: 1) we consider bounded TL constraints with explicit time parameters, which are richer than LTL
% constraints, 2) we ensure that the probability of TL
% satisfaction in each episode of learning is not below a desired
% threshold,  $Pr_{des}$.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%This paper is different than the existing works addressing RL under TL constraints, which typically focus on maximizing the probability of constraint satisfaction (E>G> REFS). The proposed algorithm ensures that the probability of TL satisfaction during each episode of learning is at least $Pr_{des}$, which is an input to the algorithm.\color{black}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% down this
% Note that this formulation is more flexible than enforcing the maximum probability of satisfaction (e.g., \cite{hasan2020,alshiekh2018}) since it allows the user to tune the performance by selecting $Pr_{des}$ based on the trade-off between risk (constraint violation) and efficiency (reward collection). To the best of our knowledge, this is the first study that addresses a constrained reinforcement learning problem, where the goal is to maximize expected reward while satisfying a bounded temporal logic constraint with a probability greater than a desired threshold throughout the learning process (even in the first episode).
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%\textcolor{blue}{Moreover, this work is also related to constrained MDP \cite{altman1999} by having separate constraint and objective functions. However, our work differs from learning over constrained MDP works (e.g., ) by accommodating more complex constraints than the expected sum of cost form and ensuring the satisfaction of the constraint all the time.} 
%does not require to know the exact knowledge of transition probabilities, can accommodate more complex constraints by using TLs, and ensure constraint satisfaction even in the first episode of learning.}    
%\color{black}\color{red} CONSTRAINED MDP den fark prob bilmek mi??\color{black} 

%  In many real-life applications, such learning process should be realized under various constraints on the system trajectory. Traditional RL algorithms facilitate learning optimal feasible solutions by severely penalizing infeasible trajectories. However, this is not a viable approach for most physical systems since violations during training may correspond to catastrophic or unacceptable events (e.g., \cite{garcia2012,turchetta2016}). For example, in an aerial monitoring mission, a drone may need to learn the most informative paths to take while also ensuring that it never runs out of fuel or collides with other objects. In such a scenario, any violation of these constraints would have severe consequences. 


%Learning can be achieved by exploration/exploitation methods. However, these methods typically focus only on the reward and are oblivious to the satisfaction  of complex constraints by the resulting trajectories. Significant modifications can be done to the exploration phase by incorporating prior knowledge of the mission to avoid risky actions \cite{koppejan2009}, \cite{abbeel2010}. However, such a biased exploration may lead to a sub-optimal policy and it may even be an infeasible option for satisfying complex constraints. Furthermore, while these approaches may ensure that the probability of violating the constraints goes down to zero over time, they do not guarantee the constraint satisfaction (such as safety or liveness) at all times. Some authors utilize invariance-based approaches or model predictive control for model-based methods to ensure safety constraints, which approximately corresponds to a limited family of temporal logic specifications that only consist of the globally operator, \cite{chow2018}, \cite{wang2018}, \cite{berkenkamp2017}, \cite{ohnishi2019}, \cite{koller2018}. There are also some other works providing the guaranteed safety by switching between the backup controllers although this approach largely restricts the exploration \cite{fisac2018}, \cite{perkins2003}, \cite{gillula2012}, \cite{mannucci2017}, \cite{li2018}.



\section{Preliminaries: Bounded Temporal Logic}
Temporal logic (TL) is a formal language to specify the temporal property of a system. Bounded TL such as Bounded Linear Temporal Logic (\cite{ishii2015}), Interval Temporal Logic \cite{10.1007/3-540-63010-4_6}, and Time Window Temporal Logic \cite{twtl} are expressive languages, as they can specify explicit time constraints (e.g., visit region $B$ in time-steps 0 to 2). Such bounded TL specifications can be translated to a deterministic finite state automaton (FSA).
\vspace{-2mm}
\begin{definition} [FSA]
A deterministic finite state automaton $\FA$ is a tuple $(Q, Q_{init}, \Sigma, \delta, F_{\FA})$ where
\begin{itemize}
\item $Q$ is a finite set of states;
\item $Q_{init} \subseteq Q$ is a set of initial states;
\item $\Sigma$ is a finite set of inputs;
\item $\delta : Q \times \Sigma \rightarrow Q$ represents the state transition relations;
\item $F_{\FA} \subseteq Q$ is a set of accepting states;
\end{itemize}
\end{definition}

%$\FA = (Q, q_{init}, 2^{\ap}, \delta, F_{\FA})$ where $Q$ is a finite set of states, $q_{init} \in Q$ is the initial state, $2^{\ap}$ is the input alphabet, $\delta: Q \times 2^{\ap} \rightarrow Q$ is the transition function, $F_{\FA} \subseteq Q$ is the set of accepting states. 
%An example FSA for a bounded TL specification ``Stay at region B for one time step within a time interval [0,2]" is illustrated in Fig. \ref{fig:FSA}(a), where the self-loop in the accepting state allows for the continuation of the mission after the satisfaction. 

Moreover, temporal relaxation of bounded TL can also be defined \cite{twtl}. A temporally relaxed formula is mainly the same formula structure with some extended or shrunk time windows. In that case, an FSA that encodes all temporal relaxations of a formula includes some backward edges and self-loops in non-accepting states. For example, consider a specification as ``Stay at region B for one time step within a time interval [0,2]" and its temporally relaxed version is ``Stay at region B for one time step within a time interval [0,2$+\tau$]". The illustrations of the original FSA and an FSA  encoding all temporal relaxations are shown in \ref{fig:FSA}. Note that the self-loops in the accepting states allow for the continuation of the mission after the satisfaction.


%Bounded TL specifications can either be represented as an FSA that encodes both violative and acceptable trajectories or a relaxed FSA by applying temporal relaxation (\cite{VASILE201727,7487481}). Fig. \ref{fig:FSA} illustrates the FSA of a bounded TL formula and its temporally relaxed version. %In opposition to relaxation, any TL language with an FSA representation can be represented by a total  Fig. \ref{fig:totalfsa} illustrates a total FSA for the described specification $\phi$.

% Figure environment removed
 \vspace{-8mm}
\begin{definition}
[Word, Accepting Word, Language] A word $\mathbf{o}=o(1) o(2) \dots$ is a sequence where $o(i)\in \Sigma$ for all $i \geq 1$. Any path over an FSA constitutes a word based on the respective edge labels. A path that starts from the initial state of the FSA and ends at an accepting state results in a satisfactory sequence, which is called an accepting word. The set of all accepting words is called the language of the corresponding FSA.
\end{definition}

Note that the original FSA only encodes the accepting words. In order to track the violation cases, one can also construct a total FSA.
\begin{definition}
[Total FSA] An FSA is called total if for all $q \in Q$ and any event $\sigma \in \Sigma$, the transition $\delta(q, \sigma) \neq \emptyset$ \cite{lin2015hybrid}.
\end{definition}
For any given FSA $\FA$, it is always possible to obtain a language-equivalent total FSA by introducing a trash state and adding a transition $\delta(q, \sigma)=\left\{trash\right\}$ if and only if $\delta(q, \sigma)=\emptyset$ in $\FA$. Fig.~\ref{fig:totalfsa} illustrates the total FSA for the specification ``Stay at region B for one time step within a time interval [0,2]". In Fig.~\ref{fig:totalfsa}, any path that starts from $q_{init}$ state and ends at state $4$ results in an accepting word; whereas any path that starts from $q_{init}$ state and ends at the trash state results in a rejecting word encoding a failure case.

%\vspace{-2mm}
%\begin{definition} {(Total FSA)
%A total finite state automaton $\FA$ is a tuple ($Q$, $Q_0$, $\Sigma$, $\delta$, $F$, $\Psi$) where
%\begin{itemize}
%\item $Q$ is a finite set of states;
%\item $Q_0 \subseteq Q$ is a set of initial states;
%\item $\Sigma$ is a finite set of inputs;
%\item $\delta : Q \times \Sigma \rightarrow 2^Q$ represents the state transition relations;
%\item $F \subseteq Q$ is a set of accepting states;
%\item $ \Psi \subseteq Q \setminus F$ is a set of trash states, from which no accepting state in $F$ is reachable;
%\end{itemize}}
%\vspace{-2mm}
%\end{definition}
% Figure environment removed


%\subsection{Overview}
 
% % Figure environment removed

 %\vspace{-4mm}
% Temporal relaxation can be defined for any bounded TL specification with explicit time parameters. For instance, consider a specification as ``eventually from time 1 to 3, visit region A for 2 consecutive time steps". Its temporally relaxed version will be ``eventually from time 1 to $3+\tau$, visit region A for 2 consecutive time steps" where $\tau$ is a slack variable that can expand or shrink the time window. Such a temporal relaxation idea has been introduced for TWTL in \cite{twtl,aksaray2016dynamic}, and the authors also propose an algorithm that constructs an FSA encoding all the possible temporal relaxations of a formula. For example, Fig.~\ref{fig:fsa} illustrates the FSAs of a bounded TL formula and its temporally relaxed version. %Accordingly, an FSA for all possible temporal relaxations can be obtained by adding backward edges, which also makes the size of the modified FSA more compact as in Fig.~\ref{fig:fsa}(b). 

% \begin{itemize}
% \item $Q$ \textit{is a finite set of states;}
% \item $q_{init} \in Q$ \textit{is the initial state;}
% \item $\ap$ \textit{is the input alphabet;}
% \item $\delta : Q \times \ap \rightarrow Q$ \textit{is the transition function;}
% \item $F_\FA \subseteq Q$ \textit{is the set of accepting states.}
% \end{itemize}

% \subsection{Time Window Temporal Logic (TWTL)}
% We investigate RL under bounded TL constraints. While our proposed method can be used with other bounded TLs, we will use TWTL for demonstration. This section presents some background on TWTL. Interested readers are referred to \cite{twtl} for further details. The syntax of TWTL is defined by the grammar:
% \begin{equation*}
% \label{eq:logic-def}
% \phi :: = \True \, | \ \mu \, | \, \phi_1 \wedge \phi_2 \, | \, \phi_1 \vee \phi_2 \, | \, \notltl \phi_1
% %\phi : = s \, | \True \, | \, \phi_i \wedge \phi_j \, | \, \notltl \phi_i
% \, | \, \phi_i \cdot \phi_j \, | \, H^d s \, | \, [\phi_i]^{[a, b]},
% \end{equation*}
% where $\mu \in \ap$ is an atomic proposition; $\True$ is the ``true'' constant;
% $\andltl$, $\vee$, and $\notltl$ are the Boolean operators for conjunction, disjunction, and negation;
% $\cdot$ is the concatenation operator;
% $H^d$ with $d \in \mathbb{Z}_{\geq 0}$ is the {\em hold} operator;
% and $[\ ]^{[a,b]}$ with $0 \leq a \leq b$ is the {\em within} operator. 

% The semantics are defined with respect to finite output words $\mathbf{o}$ over $\ap$ where $\mathbf{o}(k)$ denotes the $k^{th}$ element on $\mathbf{o}$. For any $\mu \in \ap$,  
% the {\em hold} operator $H^d \mu$ specifies that $\mu$ should be satisfied for $d$ time units (i.e., $\mathbf{o} \models H^d \mu$ if $\mathbf{o}(t)=\mu \;\; \forall t\in [0,d]$).
% The {\em within} operator $[\phi]^{[a, b]}$ bounds the satisfaction of $\phi$ to the time window $[a, b]$ (i.e., $\BF{o} \models [\phi]^{[a, b]}$ if $\exists k \in (0,b-a) \text{ s.t. } \mathbf{o}^{\prime} \models \phi$ where $\mathbf{o}^{\prime} = \BF{o}(a+k)\dots\BF{o}(b)$).
% Lastly, the concatenation of $\phi_i$ and $\phi_j$ (i.e., $\phi_i \cdot \phi_j$) means that first $\phi_i$ and immediately after that $\phi_j$ must be satisfied.

% The satisfaction of a TWTL formula can be decided within bounded time. The {\em time bound} of $\phi$ ($\norm{\phi}$) is the maximum time needed to satisfy $\phi$ and computed as follows:
% \begin{equation}
% \label{eq:bound-def}
% \norm{\phi} = \begin{cases}
% 0 & \mbox{if } \phi \in \{\True, \mu \} \\
% %\max(\norm{\phi_1}, \norm{\phi_2}) & \mbox{if } \phi \in \{ \phi_1 \andltl \phi_2, \phi_1 \orltl \phi_2 \} \\
% \max(\norm{\phi_1}, \norm{\phi_2}) & \mbox{if } \phi = \phi_1 \andltl \phi_2 \\
% \norm{\phi_1} & \mbox{if } \phi = \notltl \phi_1 \\
% \norm{\phi_1} + \norm{\phi_2} + 1 & \mbox{if } \phi = \phi_1 \cdot \phi_2\\
% d & \mbox{if } \phi = H^d \mu \\
% b%-a
% & \mbox{if } \phi = [\phi_1]^{[a, b]}
% \end{cases}
% \end{equation}
% \noindent An example TWTL formula and its time bound is: ``monitor region $A$ for 3 time units within ${[0, 5]}$, and immediately after this, monitor region $B$ for 2 time units within ${[4, 10]}$'', expressed as ${\phi = [H^3 A]^{[0,5]} \cdot [H^2 B]^{[4,10]}}$ where $\norm{\phi} = 16$. 


% A TWTL specification can be temporally relaxed and represented in a parametric way by adding slack variables to the deadlines of the time windows. In this regard, the temporally relaxed version of ${\phi = [H^3 A]^{[0,5]} \cdot [H^2 B]^{[4,10]}}$ can be written as:
% %\begin{equation}
% %\label{eq:tr-example-twtl-relaxed}
% $\phi(\boldsymbol{\tau}) = [H^{3} A]^{[0,5+\tau_1]} \cdot \big[H^{2} B \big ] ^{[4,10+\tau_2]}$
% %\end{equation}%
% where $\mbox{\boldmath$\tau$}=(\tau_1, \tau_2) \in \mathbb{Z}^2$ is the temporal relaxation vector. Overall, the temporal relaxation of $\phi(\mbox{\boldmath$\tau$})$ is $|\mbox{\boldmath$\tau$}|_{TR} =\max_j (\tau_j)$ that is the amount of maximum extension of a deadline \cite{twtl}. If $\tau_i>0$, then the corresponding time window is extended (implying a relaxer formula allowing a task to be completed after its deadline). On the other hand, if $\tau_i<0$, then the time window is shrank (implying a stronger formula requiring to complete a task earlier then the original deadline). While temporal relaxation is formally defined for TWTL in \cite{twtl}, such a metric can also be defined for other bounded TLs.
\color{black}
\vspace{-3mm}
\section{Problem Statement}
We consider a Markov Decision Process (MDP) as $M=(S,A,\Delta_M,R)$, where $S$ is the set of states, $A$ is the set of actions, $\Delta_M:S \times A \times S \rightarrow [0,1]$ is the transition probability, and $R: S \times A \rightarrow \mathbb{R}$ is a reward function. We assume that every state $s \in S$ has a set of labels, and the labeling function $l:S \rightarrow 2^{\ap}$ maps every ${s \in S}$ to the power set of atomic propositions $\ap$. Fig.~\ref{fig:mdp} shows an example MDP with a set of atomic propositions and a labeling function. 
% Figure environment removed

Given an MDP, $\pi:S\rightarrow A$  is called a policy. In RL, the transition probability $\Delta_M$ is unknown, and the agent is required to find the optimal control policy $\pi^*$ that maximizes the expected sum of rewards, i.e., $E^{\pi}\Big[\sum_{t=0}^T \gamma^t r(t)\Big]\text{ or } E^{\pi}\Big[\sum_{t=0}^\infty \gamma^t r(t)\Big]$ where $r(t)$ is the reward collected at time $t$ and $\gamma \in (0,1]$ is a discount factor. In the literature, various learning algorithms, e.g., Q-learning \cite{watkins1992}, were shown to find the optimal policy. 

%For example, a bounded TL specification such as "visit region A within 3 time steps and then immediately visit region B".  Although the relaxed automaton can identify mission failure such as "BBAAA", it identifies "BBBAB" as a successful mission, since it satisfies the relaxed bounded specification "visit region A within 3+$\tau$ time steps and then immediately visit region B".  To avoid the limitation of relaxed FSA and ensure the exact satisfaction of the constraint, we propose to construct the decision-making model by using the total FSA.

% Therefore, learning a policy over time-product MDP constructed by such automaton can involve undetectable failed trajectories.

% Total automaton for FSA producible temporal languages benefits representing non-relaxed (i.e. exact) constraints along with the capability of tracking violation trajectories (e.g., \cite{hailinbook,straubing2012introduction}).
% Total automaton represents non-relaxed (i.e. exact) constraints along with the capability of tracking violation trajectories, when using TLs with FSA production (e.g., \cite{hailinbook,straubing2012introduction}). 
%A total FSA encodes every possible violative transition in the sense that taking such transition from a system's state results in the so-called trash state, i.e. a blocking state that represents a violation of the constraint. Therefore, learning over a decision-making model built on total FSA implies 


% building a decision-making model based on total FSA involves all possible trajectories in the system. Thus, undetectable failures cannot happen and the policy learned over such time-total product MDPs imply satisfaction of the exact constraint. Moreover, the proposed RL algorithm guarantees that constraint satisfaction is larger than a desired probability in every single episode of learning procedure, which in turn enables tuning the risk (constraint violation) and efficiency (reward collection) performances by changing the desired threshold. 



% In more detail, in FSA graph, every possible violative transition has been considered in the sense that taking such transition from a system's state results into the, so-called, trash state, i.e. one blocking state that stands for violation of the constraint. Therefore, the time-total MDP constructed based on total FSA involves all possible trajectories in the system. Thus, undetectable failures cannot happen and the policy learned over such time-total product MDPs imply satisfaction of the exact constraint. Moreover, the proposed RL algorithm guarantees that constraint satisfaction is larger than a desired probability in every single episode of the learning procedure, which in turn enables tuning the risk (constraint violation) and efficiency (reward collection) performances by changing the desired threshold.

% \begin{equation}
% \label{eq:Qobj}
% E\Big[\sum_{k=0}^T r(s_{k+t+1})\Big] \quad \text{ or } \quad E\Big[\sum_{k=0}^\infty \gamma^k r(s_{k+t+1})\Big], 
% \end{equation}

% In this paper, we address a similar problem as \cite{aksaray2021probabilistically} which presents a learning algorithm to find the optimal policy for a TL constraint with a probabilistic guarantee, while maximizing the reward. For the TL constraint in \cite{aksaray2021probabilistically} they leverage a relaxed FSA which is more compact but it might not exactly satisfy the specification. 

% Safety in practice may not be guaranteed by just pruning the hazardous states from the state-space available to the agent. In many scenarios, learning a suitable trajectory may require completing some auxiliary mission (e.g., regional monitoring missions such as traffic, infrastructure, or environmental monitoring) while satisfying a main task (e.g., periodic pick-up and delivery task). In such scenarios, one solution is to consider the main task as the constraint, and the auxiliary mission as an RL problem where the reward function $R$ for each state encodes achievements in the auxiliary mission from the agent's location. For instance, consider a drone that should monitor traffic over an area (auxiliary mission) while doing a periodic pick-up and delivery task (main task). Here, some important locations such as roads or intersections in the area can be assigned high rewards in the corresponding RL problem where the drone objective is to learn a policy that maximizes the expected cumulative reward subject to the pick-up and delivery constraint (main task), which can be encoded as
% In many real life problems, the agent may also have constraints that should be satisfied during learning. For example, some safety constraints can be enforced by redefining the state-space (e.g., $S^{\prime} = S \setminus S_c$ where $S_c \subset S$ is the set of unsafe states and learning over $S^{\prime}$). However, not every constraint can be easily satisfied by removing the violating states from the state-space. In some cases, the agent may be required to follow a trajectory that satisfies a complex specification throughout the learning process. For instance, consider a drone whose primary task is achieving a pick up and delivery task arriving periodically. Moreover, the drone can have a secondary task of maximizing situational awareness via aerial monitoring (e.g., traffic, infrastructure, or environmental monitoring). In this scenario, the primary task can be considered as the constraint of the drone, and the secondary task can be formulated as an RL problem where the reward at each state represents the value of information that can be collected from the corresponding location. Overall, the objective is to learn a policy that maximizes the expected cumulative reward subject to the pick-up and delivery constraint, which can be encoded as a TL specification. 

In this paper, we address the problem of learning a policy such that a bounded TL specification is satisfied with a probability greater than a desired threshold throughout learning.  
 To provide a probabilistic satisfaction guarantee on the bounded TL constraint throughout the learning process, we assume some prior information about the transition probabilities of the MDP, even though the exact transition probabilities are unknown. We consider a generic setting where the prior information is encoded as lower and upper bounds on the transition probabilities. More specifically, for any two states $s,s' \in S$ and any action $a\in A$, let the probability of transitioning from $s$ to $s'$ under the action $a$ be bounded as:
\begin{equation}
\label{not_really_an_assumption}
\Delta_{min}(s,a,s') \leq \Delta_M(s,a,s') \leq \Delta_{max}(s,a,s'),
\end{equation}
where $\Delta_{min}:S \times A \times S \rightarrow [0,1]$ and $\Delta_{max}:S \times A \times S \rightarrow [0,1]$ are the given lower and upper bounds, respectively. %\textcolor{red}{Such lower and upper bounds can come from prior information and domain knowledge, or be obtained by estimating the confidence interval of the transition probabilities \cite{calinescu2015formal, zhao2019probabilistic}. } 
The case where no prior information is available can be encoded in this setup as $\Delta_{min}(s,a,s')=0$ and $\Delta_{max}(s,a,s')=1$ for every tuple $(s,a,s')$. These intervals become narrower if more accurate information about the transition probability is provided\footnote{Such lower and upper bounds can come from prior information and domain knowledge, or be obtained by estimating the confidence interval of the transition probabilities \cite{calinescu2015formal, zhao2019probabilistic}. }, and ultimately $\Delta_{min}(s,a,s')=\Delta_{max}(s,a,s')$ if $\Delta_M(s,a,s')$ is known exactly. %The problem addressed in this paper is formally presented below.



%In a traditional reinforcement (RL) algorithm , the goal of the agent is to learn the best policy in an environment by trial and error. %It simply learns from the experience, i.e., the agent needs to reach a state causing a violation so that it could learn that state is not satisfying the specification. 
%However, this approach might not be feasible in real-world missions. For instance, some missions require safety constraints (e.g.,  never run out of fuel). Also, there might be some other cases where the agent has to complete a persistent task (e.g.,  periodically  visit some specific regions) during the learning. Furthermore, certain tasks may have the highest priority among other interests. In that case, the agent would continually need to complete the given task while learning the environment in each episode.  The conventional RL algorithms does not offer any solutions for such systems.
%of the constraint satisfaction concerns as guaranteeing not to violate the constraints in real-life applications is the highest priority among the other interests. 
%In this paper,  we propose a novel learning algorithm that enables an agent to learn optimal policy to maximize a cumulative reward function while it persistently guarantees to complete 
%complex tasks during all training episodes. %This problem can be formulated as follows:



%\subsection{Problem Statement}
 
\vspace{-2mm}
\begin{problem} 
Suppose that the following are given:
\begin{itemize}
\item an MDP $M=(S,A,\Delta_M,R)$, where the transition probability $\Delta_M$ and reward function $R$ are unknown,
\item $\Delta_{min}:S \times A \times S \rightarrow [0,1]$ and $\Delta_{max}:S \times A \times S \rightarrow [0,1]$ such that \eqref{not_really_an_assumption} holds for every tuple $(s,a,s')$,
\item a set of atomic propositions $\ap$, 
\item a labeling function $l:S \rightarrow 2^\emph{$\ap$}$,
\item a bounded TL specification (constraint) $\phi$ 
\item a desired probability of satisfaction ${Pr_{des} \in (0,1]}$.
\end{itemize}
Learn the optimal control policy
\begin{equation}
\label{pistareq}
\pi^* = \arg\max\limits_{\pi } E^{\pi}\Big[\sum_{t=0}^\infty \gamma^t r(t) \Big] 
\end{equation}
such that, in each episode $m\geq1$ of the learning process,
\begin{equation}
\label{eq:Qobj}
\begin{aligned}
%\Pr\big(\mathbf{o}(m(T+1)),\mathbf{o}(m(T+1)+1),\dots,\mathbf{o}(m(T+1)+T) \models \phi\big) \color{black} &\geq Pr_{des},
\Pr(\mathbf{o}_m,\phi)\geq Pr_{des},
\end{aligned}
\end{equation}
where $\mathbf{o}_m=o_m(1) o_m(2) \dots o_m(T)$ is a word based on the sequence of MDP states $s_m(t)$ observed in the $m^{th}$ episode, i.e., $o_m(t) = l(s_m(t))$, $T$ is the episode length determined by the time bound\footnote{The time bound of $\phi$ is the maximum time needed to satisfy it \cite{twtl}.} of $\phi$, $\Pr(\mathbf{o}_m,\phi)$ is the probability of satisfying $\phi$ by the word $\mathbf{o}_m$, and $\gamma \in (0,1)$ is the discount factor.  

%where $T$ is the time bound\footnote{The time bound of $\phi$ is the maximum time needed to satisfy it \cite{VASILE201727}.} of $\phi$, $\mathbf{o}(m(T+1)),\mathbf{o}(m(T+1)+1),\dots,\mathbf{o}(m(T+1)+T)$ is an output word based on the sequence of MDP states $s(t)$ observed in the $m^{th}$ episode, i.e., $\mathbf{o}(t) = l(s(t))$, and $\gamma \in (0,1)$ is the discount factor.
\label{problem}
\end{problem}

% {\color{blue}
% \begin{assumption}
% \label{assm}
%  Given some $p_1$ and $p_2$ such that $[p_1,p_2] \in [0,1)$, for each state $s$ and action $a$ of the MDP, the states $s^'$ such that $Pr(\Delta_M(s,a,s^\prime)) \in [p_1,p_2]$ and the states $s^{''}$ such that $Pr(\Delta_M(s,a,s^{\prime\prime})) \notin [p_1,p_2]$ are known.
% \end{assumption}
% Note that Assumption \ref{assm} does not require knowing the actual transition probabilities. Instead, it only requires knowing which transitions are feasible (i.e. transitions to the set of $s^{'} \cup s^{''}$ states) and which of those feasible transitions are sufficiently likely to occur for each state-action pair within an interval $[p_1,p_2]$.
% }

%if the reward is designed in a manner that captures both the information in the environment and the pick up and delivery task, then a standard reinforcement learning algorithm can be utilized to learn a policy that maximizes the sum of reward as well as satisfying the constraint. However, while such a policy can be learned, there is no guarantee on the constraint satisfaction during learning. This paper is different than the state-of-the art techniques by providing constraint satisfaction guarantee during every episode of the learning process. Moreover, the proposed idea can accommodate rich and complex specifications (including spatial, temporal, and logical relations) that can be expressed by bounded temporal logics with automaton representation. %\footnote{Signal Temporal Logic is also a bounded temporal logic but it does not have an automaton representation}.   




\section{Proposed Approach}


% Learning with TL objectives or constraints cannot be
% achieved over a standard MDP which contains only the
% agents current state $s(t)$. This is due to the fact that the decision at any time $t$ should also depend on the how close the agent is to the satisfaction/violation of the TL specification. In this regard, a common approach is to construct a new MDP that keeps track of not only $s(t)$ but also the progress with respect to the TL specification (e.g., by using a suitable automaton representation) and the time (for bounded TL specifications). For bounded TL specifications, the progress towards satisfaction can be encoded via its FSA representation, which encodes all the accepting words. However, in a stochastic setting, the agent is not always guaranteed to make progress toward satisfaction. Hence, for such 

Learning with TL constraints typically involves an MDP that encodes both the state information and progress towards the constraint satisfaction. 
A common approach is to construct a product MDP from the FSA encoding the TL constraint and the MDP encoding the state information. A similar approach was considered in \cite{aksaray2021probabilistically}, where a product MDP was constructed by using the relaxed FSA of the desired TL specification. While the use of relaxed FSA provides the benefit of a more compact representation (improving the scalability of learning), it also allows for learning control policies that result in trajectories satisfying not only the original specification but also its temporally relaxed version. In this paper, we focus on the problem of learning control policies that only generate trajectories ensuring the satisfaction of the original specification. To this end, we utilize total FSA instead of relaxed FSA. 

%While the previous study \cite{aksaray2021probabilistically} uses a relaxed FSA, as shown in Fig.~\ref{fig:FSA} (b), we construct an MDP using a total FSA, as shown in Fig.~\ref{fig:totalfsa}, to allow for exact satisfaction of the bounded TL specification.

% \subsection{Total Automaton}
% \begin{definition} {(Total FSA)
% A total finite state automaton $\FA$ is a tuple ($Q$, $Q_0$, $\Sigma$, $\delta$, $F$, $\Psi$) where
% \begin{itemize}
% \item $Q$ is a finite set of states;
% \item $Q_0 \subseteq Q$ is a set of initial states;
% \item $\Sigma$ is a finite set of symbols representing inputs;
% \item $\delta : Q \times \Sigma \rightarrow 2^Q$ represents the state transition relations;
% \item $F \subseteq Q$ is a set of accepting states;
% \item $ \Psi \subseteq Q \setminus F$ is a set of trash states, from which no accepting state in $F$ is reachable;
% \end{itemize}}
% An automaton is called total if for all states $q \in Q$ and any event $\sigma \in \Sigma$, the transition $\delta(q, \sigma) \neq \emptyset$ \cite{hailinbook,straubing2012introduction}.
% Note that for any given automaton $\FA$, it is always possible to obtain a language equivalent total automaton by introducing a trash state and adding a transition $\delta(q, \sigma)=\left\{trash\right\}$ if and only if $\delta(q, \sigma)=\emptyset$ in $\FA$. Fig \ref{fig:FSA} illustrates the total FSA for $\phi = H^{1}_{[0,2]}B$.
% \end{definition}
% % Figure environment removed



% % Figure environment removed

% % Figure environment removed

% Given an MDP $M$, a set of atomic propositions \emph{$\ap$}, a labeling function $l:S \rightarrow 2^\emph{$\ap$}$, and an FSA $\FA$, a product MDP is a tuple $\FP =M \times \FA = (S_{\FP},P_{init},A,\Delta_\FP, R_{\FP}, F_{\FP})$, where

% \begin{itemize}
% \item $S_\FP = S \times Q$ \textit{is a finite set of states;}
% \item $P_{init}=S \times \{q_{init}\} \subseteq S_\FP$ \textit{is the set of initial states;}
% \item $A$ \textit{is the set of actions;}
% \item $\Delta_\FP : S_\FP \times A \times S_\FP \rightarrow [0,1]$ \textit{is the probabilistic transition relation such that for any two states, $p=(s,q) \in S_\FP$ and $p^{\prime}=(s^{\prime},q^{\prime}) \in S_\FP$, and any action $a \in A$, $\Delta_\FP(p,a,p^\prime)=\Delta_M(s,a,s^\prime)$ and $\delta(q,l(s))=q^\prime$}; 
% \item $R_{\FP}:S_\FP \times A \rightarrow \mathbb{R}$ \textit{is the reward function such that $R_{\mathcal{P}}(p,a) = R(s,a)$ for $p=(s,q) \in S_\FP$;}
% \item $F_\FP = (S \times F_\FA) \subseteq S_\FP$ \textit{is the set of accepting states.}
% \end{itemize}
% \end{definition}

% \begin{definition} (Product MDP)
% Given an MDP $M$, a set of atomic propositions \emph{$\ap$}, a labeling function $l:S \rightarrow 2^\emph{$\ap$}$, and an FSA $\FA$, a product MDP is a tuple $\FP =M \times \FA = (S_{\FP},P_{init},A,\Delta_\FP, R_{\FP}, F_{\FP})$, where

% \begin{itemize}
% \item $S_\FP = S \times Q$ \textit{is a finite set of states;}
% \item $P_{init}=S \times \{q_{init}\} \subseteq S_\FP$ \textit{is the set of initial states;}
% \item $A$ \textit{is the set of actions;}
% \item $\Delta_\FP : S_\FP \times A \times S_\FP \rightarrow [0,1]$ \textit{is the probabilistic transition relation such that for any two states, $p=(s,q) \in S_\FP$ and $p^{\prime}=(s^{\prime},q^{\prime}) \in S_\FP$, and any action $a \in A$, $\Delta_\FP(p,a,p^\prime)=\Delta_M(s,a,s^\prime)$ and $\delta(q,l(s))=q^\prime$}; 
% \item $R_{\FP}:S_\FP \times A \rightarrow \mathbb{R}$ \textit{is the reward function such that $R_{\mathcal{P}}(p,a) = R(s,a)$ for $p=(s,q) \in S_\FP$;}
% \item $F_\FP = (S \times F_\FA) \subseteq S_\FP$ \textit{is the set of accepting states.}
% \end{itemize}
% \end{definition}
\vspace{-2mm}
\begin{definition} (Total Product MDP)
Given an MDP $M=(S,A,\Delta_M,R)$, a set of atomic propositions \emph{$\ap$}, a labeling function $l:S \rightarrow 2^\emph{$\ap$}$, and a total FSA $\FA=(Q, Q_{init}, 2^{\ap}, \delta, F_{\FA}, q_{trash})$, a product MDP is a tuple $\FP =M \times \FA = (S_{\FP},P_{init},A,\Delta_\FP, R_{\FP}, F_{\FP}, \Psi_{\FP})$, where

\begin{itemize}
\item $S_\FP = S \times Q$ \textit{is a finite set of states;}
\item $P_{init}=\{(s,\delta(q_{init},l(s)) \mid \forall s \in S \}$ is the set of initial states;
\item $A$ \textit{is the set of actions;}
\item $\Delta_\FP : S_\FP \times A \times S_\FP \rightarrow [0,1]$ \textit{is the probabilistic transition relation such that for any two states, $p=(s,q) \in S_\FP$ and $p^{\prime}=(s^{\prime},q^{\prime}) \in S_\FP$, and any action $a \in A$, $\Delta_\FP(p,a,p^\prime)=\Delta_M(s,a,s^\prime)$ and $\delta(q,l(s'))=q^\prime$}; 
\item $R_{\FP}:S_\FP \times A \rightarrow \mathbb{R}$ \textit{is the reward function such that $R_{\mathcal{P}}(p,a) = R(s,a)$ for $p=(s,q) \in S_\FP$;}
\item $F_\FP = (S \times F_\FA) \subseteq S_\FP$ \textit{is the set of accepting states.}
% \item \color{red}{trash states?}
\item {$\Psi_{\FP} = (S \times q_{trash}) \subseteq S_\FP$ \textit{is the set of trash states.}}
\end{itemize}
\end{definition}

% In Problem~\ref{problem}, the time bound of the TL constraint $\phi$ determines the episode length, and the remaining episode time is also critical to the optimal action selection. Again, consider the example in Fig.~\ref{fig:productmdp-mot}. This time, let ``eventually visit A and then visit B" be a constraint during learning. If the agent shown in triangle has not satisfied this constraint yet and the episode has just started, it can either explore the environment by selecting an admissible action or make progress towards the satisfaction of the constraint by selecting the green arrow. Now, suppose that the agent is at the same state, the constraint has not been satisfied yet, but the episode is about to finish. In that case, the agent must pick the green arrow for constraint satisfaction. Overall, learning under a bounded TL constraint needs to be achieved over a space that encodes the physical state, the automaton state, and the remaining episode time. %Hence, we define a time-product MDP.   

The knowledge of remaining time is also crucial in deciding actions under bounded time constraints. For example, suppose that a robot must visit region A before a specified deadline, and it has not visited region A yet. If there is a small amount of time left, the optimal action would be moving towards A. However, if there is more remaining time, the optimal action might be moving towards other regions that yield higher rewards. For this reason, we construct a time-total product MDP.    
%also requires the knowledge of remaining time.  satisfaction According to Problem~\ref{problem}, each learning episode's length is extracted from the time bound of the TL constraint $\phi$. Also, in order to maximize the reward, the remaining episode time matters. Furthermore, a space that encodes the physical state, the automaton state, and the remaining episode time are needed in order to accomplish the learning procedure under a TL constraint. %Thus, a time-total product MDP is defined as follows.
\vspace{-2mm}
\begin{definition} (Time-Total Product MDP)
Given a product MPD $\mathcal{P}=(S_{\FP},P_{init},A,\Delta_\FP, R_{\FP}, F_{\FP}, \Psi_{\FP})$ and a time set $\mathcal{T}=\{0,\dots,T\}$, a time-product MPD is a tuple $\mathcal{P}^{\mathcal{T}} = \mathcal{P} \times \mathcal{T} =(S^{\mathcal{T}}_{\mathcal{P}}, P^{\mathcal{T}}_{init}, A, \Delta^{\mathcal{T}}_\FP, R^{\mathcal{T}}_{\mathcal{P}}, F^{\mathcal{T}}_{\mathcal{P}}, {\Psi}^{\mathcal{T}}_{\mathcal{P}})$ where
\begin{itemize}
\item $S^{\mathcal{T}}_{\mathcal{P}} = S_{\mathcal{P}} \times \mathcal{T}$ is a finite set of states;
%\item $p^t=(p,t) \in S^{\mathcal{T}}_{\mathcal{P}}$ is a time product MDP state at the time instance $t \in \mathcal{T} $;
\item $P^{\mathcal{T}}_{init} = P_{init} \times \{0\} \subseteq S^{\mathcal{T}}_{\mathcal{P}}$ is the set of initial states;% where $ p^{\mathcal{T}}_{init} \subseteq S^{\mathcal{T}}_{\mathcal{P}}$;
\item $A$ is the set of actions;
\item $\Delta_\FP^{\mathcal{T}} : S^{\mathcal{T}}_{\mathcal{P}} \times A \times S^{\mathcal{T}}_{\mathcal{P}} \mapsto [0,1]$ is the probabilistic transition relation such that $\Delta_\FP^{\mathcal{T}}(p_i^t,a,p_j^{t+1})=\Delta_\FP(p_i,a,p_j)$ for an action $a \in A$ and two time-total product MDP states $p_i^t=(p_i,t) \in S^{\mathcal{T}}_{\mathcal{P}}$ and $p_j^{t+1}=(p_j,t+1) \in S^{\mathcal{T}}_{\mathcal{P}}$;
% {\color{blue} \item $D_\FP^{\mathcal{T}}(p_i^t,a) : S^{\mathcal{T}}_{\mathcal{P}} \times A$ is the set of all time-total product MDP states $p_j^{t+1}=(p_j,t+1) \in S^{\mathcal{T}}_{\mathcal{P}}$ reachable from $p_i^t=(p_i,t) \in S^{\mathcal{T}}_{\mathcal{P}}$ by taking action $a\in A$;}
\item $R^{\mathcal{T}}_{\mathcal{P}}  : S^{\mathcal{T}}_{\mathcal{P}} \times A \mapsto \mathbb{R}$ is the reward function such that $R^{\mathcal{T}}_{\mathcal{P}}(p^t ,a)=R_{\mathcal{P}}(p,a)$ and $p^t=(p,t) \in S^{\mathcal{T}}_{\mathcal{P}}$;
\item $F^{\mathcal{T}}_{\mathcal{P}} = (F_{\mathcal{P}} \times \mathcal{T}) \subseteq S^{\mathcal{T}}_{\mathcal{P}}$ is the set of accepting states.
\item ${\Psi}^{\mathcal{T}}_{\mathcal{P}} = (\Psi_{\mathcal{P}} \times \mathcal{T}) \subseteq S^{\mathcal{T}}_{\mathcal{P}}$ is the set of trash states.
\end{itemize}
\label{def:tmdp}
\end{definition}

%Let $p=(s,q)$ and $p^{\prime}=(s^{\prime},q^{\prime})$ be two product MDP states. Suppose that $a\in A$ is admissible action at state $s$. Then, $\Delta(p,a,p^\prime)=P(s,a,s^\prime)$ and $\delta(q,l(s^\prime))=q^\prime$. Moreover, $R_{\mathcal{P}}(p) = R(s)$. BUNLARIN BIR KISMI DEFINITIONA GIRMELI.



% \begin{problem}
% Given an MDP $M=(S,s_{init},A,P,R)$ with unknown transition probability function $P$ and initially unknown reward function $R$, a set of atomic propositions $\ap$, a labeling function $l:S \rightarrow 2^\emph{$\ap$}$, and an FSA $\FA$ that encodes all temporal relaxations of a bounded TL task $\phi(\tau)$, let $\FP =M \times \FA = (S_{\FP},p_{init},A,\Delta, R_{\FP}, F_{\FP})$ be the corresponding product MDP. Let $T=\norm{\phi(0)}$ be the time bound of the TL constraint with no relaxation. Given a desired probability of satisfaction $Pr_{des} \in [0,1)$, learn the optimal control policy
% \begin{equation}
% \pi^* = \arg\max\limits_{\pi } E^{\pi}\Big[\sum_{t=0}^T R_{\FP}(p_{t})\Big] 
% \end{equation}
% such that, in each episode $n$ of the learning process,
% \begin{equation}
% \label{eq:Qobj}
% \begin{aligned}
% \Pr\big(o(iT),o(iT+1),\dots,o(iT+T-1) \models \phi{(\tau_n)}\big) \geq Pr_{des}, \quad \forall i\geq0\\
% \end{aligned}
% \end{equation}
% where $\BF{o}_n$ is a $T$-length output word defined over $\ap$ based on state sequence $p_0,p_1,p_2,\dots$ over a product MDP, and $\norm{\phi(\tau_n)}$ is the time bound of the relaxed formula $\phi(\tau_n)$. 
% \label{problem}
% \end{problem}

%In Problem 1, $\norm{\phi(\tau_n)} \leq T$ enforces any satisfied relaxed formula in each episode $n$ to have a time bound of at least $T$. Consider a constraint such as ``visit region A once in 10 time steps and immediately after that visit region B once in 5 time steps" which has a time bound of 16 time steps. Satisfying a relaxed formula, which should also has a time bound of $16$, means that satisfying ``visit region A once in 8 time steps and immediately after that visit region B once in 7 time steps", ``visit region A once in 12 time steps and immediately after that visit region B once in 3 time steps", ``visit region A once in 3 time steps and immediately after that visit region B once in 4 time steps" are all acceptable. 

%\subsection{Energy Function}
%In this section, we define a scalar non-negative function $V(p_i)$, defined over the product automaton states, and it is called the \emph{energy function}. This function will be utilized in our safe learning algorithm to ensure the accomplishment of the task $\varphi_{task}$ in each episode, which will lead to satisfaction of the persistent mission $\varphi_{PM}$.  Such an energy-like function was also used in \cite{ding2014}. Prior to the definition of the energy function, we first define the distance between states on the product automaton.
% For any pair of states $p_i,p_j \in Q_{\mathcal{P}}$, the distance $d(p_i,p_j)$ is defined as the length of the shortest path from $p_i$ to $p_j$ on the product automaton, which is also equal to the minimum number of time steps it takes to reach $p_j$ from $p_i$. By definition, $d(p_i,p_j)=\infty$ if there is no feasible path from $p_i$ to $p_j$ ($p_j$ is not reachable from $p_i$).
 


 
%\begin{definition} (Energy function) For any state of the product automaton, $p_i \in Q_{\mathcal{P}}$, the energy is 

%\begin{equation}
%V(p_i) = \left
%\lbrace
%\begin{array}{cl}
%\min\limits_{ p_j \in F_{\mathcal{P}}^{*}}d(p_i,p_j), & \text{if } p_i\notin F_{\mathcal{P}}^{*},\\
%0, & \text{otherwise,}
%\end{array}
%\right.
%\label{eq:EnergyFunction}
%\end{equation}
%where $F_{\mathcal{P}}^{*} \subseteq F_{\mathcal{P}}$ is the largest self-reachable set of accepting states, i.e.,  $p_j \in F_{\mathcal{P}}$ such that there exists a path from $p_j$ to some $p_k \in F_{\mathcal{P}}^{*}$.
%\label{energy}
%\end{definition}
\vspace{-2mm}
Note that the time set in constructing the time-total product MDP is selected according to the time bound of the constraint $\phi$ as defined in Problem~\ref{problem}. A time-total product MDP state with a time element $T$ should be either an \emph{accepting} or a \emph{trash} state. Moreover, the lower and upper bounds of the transition probabilities as shown in \eqref{not_really_an_assumption} can be projected on the time-total product MDP as follows:


\vspace{-3mm}
%with a time-set of $\mathcal{T}=\{0,\dots,T\}$, states with the time notion of $T$ have to be either $accepting$ or $trash$ states. Also considering the definition above, the set of initial states can be progressed based on the label observation methodology. 

%Moreover, we can update the FSA state based on either the current MDP state label or the next MDP state label. The next observation method (our choice) requires having initial total-product MDP states whose notion of FSA states are representing the progress after observing the MDP state label instead of the initial states of FSA. Otherwise, we might have some states which are neither $accepting$ nor $trash$ at $t=T$. In general at each time $t$ and for some time-total product MDP states $p_i^{t}=(s_i,q_i,t)$ and $p_j^{t+1}=(s_j,q_j,t+1)$, where $s_i$ and $s_j$ denote the MDP states, and $q_i$ and $q_j$ denote the FSA states. Furthermore, if in the FSA $\delta(q_i,l(s_j))=q_j$, then since in time-total product MDP we have transitions probability same as in the MDP, from \eqref{not_really_an_assumption} we can derive
\begin{equation}
\label{not_really_an_assumption_tpmdp}
\Delta_{min}(p^{t}_i,a,p_j^{t+1}) \leq 
\Delta_\FP^{\mathcal{T}}(p^{t}_i,a,p_j^{t+1})
\leq \Delta_{max}(p_i^{t},a,p_j^{t+1}),
\end{equation}

where 
\begin{equation}
\small
\begin{split}
  & \Delta_{min}(p^{t}_i,a,p_j^{t+1}) =
    \begin{cases}
      \Delta_{min}(s_i,a,s_j) & \text{if $\delta(q_i,l(s_j))=q_j$ }\\
      0 & \text{else}
    \end{cases}  \\
  & \Delta_{max}(p^{t}_i,a,p_j^{t+1}) =
    \begin{cases}
      \Delta_{max}(s_i,a,s_j) & \text{if  $\delta(q_i,l(s_j))=q_j$ }\\
      0 & \text{else}
    \end{cases}
\end{split}
\end{equation}

\subsection{Evaluation of the Minimum Probability of Satisfaction}  
Given a time-total product MDP and lower/upper bounds of transition probabilities as in \eqref{not_really_an_assumption_tpmdp}, we propose an algorithm based on backward recursion to compute a lower bound on the probability of constraint satisfaction (i.e., the probability of reaching the set of $accepting$ states in the remaining time) for each state $p_i^t \in S^{\mathcal{T}}_{\mathcal{P}}$, denoted as $f(p_i^t) \in [0,1].$ Since any time-total product MDP states at $t=T$ is either a $trash$ or $accepting$ state, we can set the lower bound on constraint satisfaction for all those states as:
\begin{equation}\label{v(s)_T}
f(p_i^{T}) = \left\{\begin{array}{ll} \mbox{1, if $p_i^{T} \in F^{\mathcal{T}}_{\mathcal{P}}$,} \\ \mbox{0, if $p_i^{T} \in {\Psi}^{\mathcal{T}}_{\mathcal{P}}$.}\end{array}\right.
\end{equation}
Staring with these values, the proposed algorithm computes $f(p_i^{t})$ recursively for all $0\leq t \leq T-1$ as follows. For any action $a \in A$, let 
\begin{equation}
\label{N_definition}    
N(p_i^{t},a) = \{p_j^{t+1} \in S^{\mathcal{T}}_{\mathcal{P}} \mid \Delta_{max}(p_i^{t},a,p_j^{t+1})>0\}
\end{equation}
denote the set of states that may be reached from $p_i^{t}$ by taking action $a$ and let $n=|N(p_i^{t},a)|$. Then, for each state-action pair $(p_i^{t},a)$, we can obtain a lower bound on the probability of reaching an accepting state from $p_i^{t}$, conditioned on first taking action $a$. We obtain this lower bound by considering the ``worst-case", i.e., transitioning to states in $N(p_i^{t},a)$ with the smallest value of $f(p_i^{t+1})$ with the maximum possible probability.  More specifically, we formulate the following linear optimization problem to solve for a lower bound on the probability of reaching the accepting states from state $p_i^{t}$ by first taking action $a$:
%\sout{Note that having more information about the system (i.e. knowing a narrower bound than $[0,1]$ for each transition's interval  $[\Delta_{min}(\cdot),\Delta_{max}(\cdot)]$) may force some transition probability to zero, thus cause having fewer states in the neighbor set $N(p_i^{t},a)$.}
% Let $P^{t-1}=\{p^t_1,p^t_2,...,p^t_{|S|}\}$ denote the set of states $p_i^{t} \in S^{\mathcal{T}}_{\mathcal{P}}$ to which there are transitions from state $p^{t-1}$,
% In the most generic case, we have no information to use for separating feasible and infeasible MDP transitions, $\Delta_M$, from each other. Hence in such case, $N(p_i^t,a)$ will include all states $P^{t}$ whose
% where $S$ is the set of MDP states and $1\leq i\leq|S|$. Note that definition of $P^{t-1}$ implies that transition between every two MDP states is considered, hence we have a constant $|P^{t-1}|=|S|$, and independent of pair $(p^{t-1},a)$ selection. Furthermore, having more information about the system (i.e. knowing a narrower bound than $[0,1]$ for each transition's $[\Delta_{min}(\cdot),\Delta_{max}(\cdot)]$) may force some transition probability to zero.
% Now, for $p^{t-1}$, let  $F(p^{t-1})=[f(p^t_0),f(p^t_1),...,f(p^t_{|S-1|})]$ be a vector containing the the function $f(\cdot)$ results for all next states $p^t_j$ for $1\leq j\leq |S-1|$.
% $D(p^{t-1}_i,a)=[\hat{\Delta}_{i,0},\hat{\Delta}_{i,1},\hdots,\hat{\Delta}_{i,|S-1|}]^T$ minimizes $F(p^{t-1}_i).D(p^{t-1}_i,a)$. i.e. for each $p^{t-1}_i\in p^{t-1}$ we have
\footnotesize
 \begin{mini!}|s|[2]  
% mini! = minimize 
    {\hat{\Delta}_{1},\hat{\Delta}_{2},\hdots,\hat{\Delta}_{n}}                               % optimization variable
    {\sum_{j=1}^{n} f(p^{t+1}_{j}) \hat{\Delta}_{j} \label{eq:eq1}}   % objective function and label
    {\label{opt_problem}}  % label for optimizatio problem
    {\kappa(p_i^{t},a) =}                               % optimization result
    \addConstraint{\sum_{j=1}^{n} \hat{\Delta}_{j}}{=1 \label{eq:con1}}    % constraint 1
    \addConstraint{ \Delta_{min}(p_i^{t},a,p^{t+1}_j) \leq \hat{\Delta}_{j} }{\leq \Delta_{max}(p_i^{t},a,p^{t+1}_j), \label{eq:con2}}  % constraint 2
\end{mini!}
\normalsize

\noindent where $\Delta_{min}(p_i^{t},a,p^{t+1}_j)$ and $\Delta_{max}(p_i^{t},a,p^{t+1}_j)$ are lower and upper bound transition probabilities as in \eqref{not_really_an_assumption_tpmdp}.  Finally, for each state $p_i^{t}$ we define $f(p_i^{t})$ as
\begin{equation}
\label{opt_max_kappa}
f(p_i^{t}) = \max_{a\in A} \kappa(p_i^{t},a), 
\end{equation}
where $f(p_i^{t})$ denotes the largest lower bound on the probability of constraint satisfaction that can be achieved by properly choosing the action to take at $p_i^{t}$. 
 Note that once the values $f(\cdot)$ for states at time step $t$ are given, values $f(\cdot)$ for states at time step $t-1$ can be obtained by solving the optimization problem in \eqref{opt_problem} and using \eqref{opt_max_kappa}.
 
 % For example, Fig.~\ref{fig:example1} illustrates a time-total product MDP, where each state is composed of the MDP state (i.e., ${s_0,s_1,s_2,s_3}$), the total FSA state (i.e., ${1,2,3,4,trash}$), and the time stamp (i.e., ${0,1,2}$). In the last layer ($t=2$), each time-total product MDP state is either an accepting or a trash state. Hence, we can assign $f(s_0,trash,2)=f(s_1,trash,2)=f(s_2,trash,2)=f(s_3,trash,2)=0$ and $f(s_0,4,2)=f(s1,4,2)=f(s_2,4,2)=f(s_3,4,2)=1$. Then, for each state at $t=1$, we know the information in \eqref{N_definition} so we can calculate the corresponding probability of reaching an accepting state by using \eqref{opt_problem} and \eqref{opt_max_kappa}. Once the probability values of all states are computed in the layer $t=1$, the same procedure is followed for the states at $t=0$.  
 
% \st{Note that starting from time $T$ and following this procedure, we can compute $\kappa(p^{T-1},a)$ for states at $T-1$ based on $v(p^{T})$. Moreover, for computing recursive values of states at time $t=T-k$ we only need recursive values of next states at $t=T-k+1$, for $1\leq k\leq T$. Therefore, doing the same procedure on the time-total product MDP would give us all recursive values for all the states.}

%\noindent\textbf{Example 1}
%\label{ex1}
%Consider \eqref{not_really_an_assumption} holds for Given an MDP where $S=\{s_0,s_1,s_2,s_3\}$, $A=\{a_1,a_2,a_3,a_4\}$, $\ap=\{B\}$, $l(s_{3})=\{B\}$, $l(s_0)=l(s_1)=l(s_2)=\emptyset$, upper and lower bounds of the transition probabilities as in \eqref{not_really_an_assumption}, and a TWTL\footnote{The syntax and semantics of TWTL can be found in the Appendix.} constraint as $[H^{1}B]^{[0,2]}$, which means ``stay at B for one time step within [0,2]". The time-total product MDP is constructed as in Fig.~\ref{fig:example1}, where each state is composed of the MDP state, the total FSA state, and the time stamp. Similar to computing Bellman equation in dynamic programming, we compute the minimum probability of satisfaction for each state in the time-total product MDP in a recursive manner. 

%At the last time step, the FSA state is either trash state or accepting state, and therefore the probability of satisfaction can be directly obtained. 

%In Fig.~\ref{fig:example1}, we obtain the $max\_prob$ function for the states at the last time step $t=2$ by $f(s_0,trash,2)=f(s_1,trash,2)=f(s_2,trash,2)=f(s_3,trash,2)=0$ and $f(s_0,4,2)=f(s1,4,2)=f(s_2,4,2)=f(s_3,4,2)=1$. For states at $t=1$, the value can be computed by solving the following minimization problem. We take state $(s_3,3,1)$ as an example and assume that the available actions are $a_1, a_2, a_3, a_4$. 
% Note that since we do not know the exact transitions' probabilities in Fig.~\ref{fig:mdp}, we solve the minimization problem using some estimation variables $\hat{\Delta}_j$ where for transitions happened under taking an action from set of available actions, say $a_1$.
% $\Delta^{1}_{3,0}, \Delta^{1}_{3,1}, \Delta^{1}_{3,2}, \Delta^{1}_{3,3}$
% represent the transition probability by taking action $a_1$. 
%Let $p_4^1$ denote state $(s_3,3,1)$ and let
% $$P^{1}=\{(s_0,trash,2),(s_1,trash,2),(s_2,trash,2),(s_3,4,2)\}=\{p^{2}_{1},p^{2}_{2},p^{2}_{3},p^{2}_{4}\}$$ 
%\begin{align*}
%N(p_4^1,a_1) & =\{(s_0,trash,2),(s_1,trash,2),(s_2,trash,2),(s_3,4,2)\}\\
%& = \{p^{2}_{1},p^{2}_{2},p^{2}_{3},p^{2}_{4}\}
%\end{align*}
%denote its neighbors set where $n=4$. The minimum probability of satisfaction from state $p_4^1$ by taking action $a_1$ can be computed by solving the following problem
%\begin{mini*}|s|[2]   
%{\hat{\Delta}_{1},\hat{\Delta}_{2},\hat{\Delta}_{3},\hat{\Delta}_{4}} {\sum_{j=1}^{4} f(p^{2}_{j}) \hat{\Delta}_{j}}{}{}
%\addConstraint{\sum_{j=1}^{4} \hat{\Delta}_{j}}{=1 }
%\addConstraint{ \Delta_{min}(p_4^1,a_1,p^2_j) \leq \hat{\Delta}_{j} }{\leq \Delta_{max}(p_4^1,a_1,p^2_j).}{}
%\end{mini*}
%where $\Delta_{min}(\cdot)$ and $\Delta_{max}(\cdot)$ are the given bounds of the transition probabilities. Let $\Delta^{*}_{1},\Delta^{*}_{2},\Delta^{*}_{3},\Delta^{*}_{4}$ denote the solution of the problem described. Then we can compute $$\kappa(p_4^1,a_1)=\sum_{j=1}^{4} f(p^{2}_{j})\cdot \Delta^{*}_{j}.$$
%Similarly, $\kappa(p_4^1,a_2)$, $\kappa(p_4^1,a_3)$, and $\kappa(p_4^1,a_4)$ can be computed, and the minimum probability of satisfaction from state $p_4^1$ is set as $f(p_4^1) = max\{\kappa(p_4^1,a_1),\kappa(p_4^1,a_2),\kappa(p_4^1,a_3),\kappa(p_4^1,a_4)\}.$ The same procedure can be repeated to any states at $t=1$, then to any states at $t=0$.

% % Figure environment removed
\subsection{Reinforcement Learning with Constraint Satisfaction}
%This section will present the proposed one-shot pruning algorithm and modified Q-learning that solves Problem 1. We prove  the proposed algorithms guarantee that the probability of satisfying the constraint is at least $Pr_{des}$ in each episode during learning.
We first define a policy $\pi_C$ that drives the agent towards the set of accepting states $F_{\FP}^{\mathcal{T}}$ by choosing the actions that yield the maximum lower bound $\kappa(\cdot)$ for each state. 
\vspace{-2mm}
\begin{definition}
[Go-to-$F_{\FP}^{\mathcal{T}}$ Policy] Given a time-total product MDP with the lower/upper bounds on transition probabilities as in \eqref{not_really_an_assumption_tpmdp}, Go-to-$F_{\FP}^{\mathcal{T}}$ policy $\pi_{C}: S_{\FP}^{\mathcal{T}} \rightarrow A $ is defined as
\begin{equation}
\label{policy_equation}  
\pi_{C}(p_i^{t})= \argmax \limits_{a \in A}\kappa(p_i^{t},a), \; \forall p_i^t\in S_{\FP}^{\mathcal{T}}.
\end{equation}

% to achieve $v(p^{t})$ as noted in \eqref{opt_max_kappa}.
% returns an action $a^* \in A_{p^t}$ in which $\kappa(p^t,a^*) = v(p^t)$.
\end{definition}

\begin{lemma} Given a time-total product MDP $\FP^{\mathcal{T}}$ with known lower and upper bounds defined in \eqref{not_really_an_assumption_tpmdp} and time bound $T$, for any $p_i^t \in  S_{\FP}^{\mathcal{T}}$, let ${\Pr\big(p_i^t \rightarrow F_\FP^{\mathcal{T}};\pi_{C} \big)}$ be the probability of reaching the set of accepting states $F_{\FP}^\mathcal{T} \subseteq S_{\FP}^\mathcal{T}$ from $p_i^t$ in the next $T-t$ time steps under the policy $\pi_{C}$. Then
% If $d^\epsilon(p^t) < \infty$ for every $p^t \in  S_{\FP}^{\mathcal{T}}$, then
\begin{equation}
\label{lower-bound}
\Pr\big(p_i^t \rightarrow F_\FP^{\mathcal{T}};\pi_{C} \big) \geq f(p_i^t),
% \sum\limits_{i=0}^{\lfloor \frac{k-d^\epsilon(p^t)}{2}\rfloor}  \frac{k!}{(k-i)!i!} \epsilon^i (1-\epsilon)^{k-i},
\end{equation}
where $f(p_i^t)$ is derived from \eqref{opt_max_kappa}.
% the greatest solution of the optimization problem for each state action pair $(p^t,a) \mid a \in A$.
% for every state $p^t \in S_{\FP}^\mathcal{T}$ such that $k \geq d^\epsilon(p^t)$.
\label{lemma}
\end{lemma}

\begin{proof}
We will prove the lemma by induction. We first show that \eqref{lower-bound} holds for step $t=T$. Since the states in $\FP^{\mathcal{T}}$ at $t=T$ are either $trash$ or $accepting$, under any policy the probability of reaching $F_{\FP}^\mathcal{T}$ from any of those states is  1 ($accepting$) or 0 ($trash$) by definition. Hence,
\begin{equation}
\label{induction_first_step}
{\Pr\big(p_i^T \xrightarrow{} F_\FP^{\mathcal{T}};\pi_{C} \big)} = \left\{\begin{array}{ll} \mbox{1, if $p_i^T \in F^{\mathcal{T}}_{\mathcal{P}}$,} \\ \mbox{0, if $p_i^T \in \Psi^{\mathcal{T}}_{\mathcal{P}}$.}\end{array}\right.
\end{equation}
By \eqref{v(s)_T} and \eqref{induction_first_step}, it can be concluded that \eqref{lower-bound} holds for $t=T$. 

Now suppose that \eqref{lower-bound} holds for all states at $t=k+1$ for some $0\leq k\leq T-1$. Then, we will show that \eqref{lower-bound} also holds for all states at $t=k$. Let $p_i^{k}$ denote a state in $\FP^{\mathcal{T}}$ at $t=k$. Let $a^*= \pi_{C}(p_i^{k})$ be the action at $p_i^{k}$ given by the policy $\pi_{C}$. For any state $p^{k+1}_{j} \in N(p_i^{k},a^*)$, recall that $\Delta_\FP^{\mathcal{T}}(p_i^{k},a^*,p^{k+1}_j)$ denotes the probability of reaching $p^{k+1}_{j}$ from $p_i^{k}$ by taking action $a^*$. Accordingly,
\begin{equation}
\footnotesize
\label{lemma_eq2}
\begin{split}
\Pr\big(p_i^{k} \rightarrow F_\FP^{\mathcal{T}};\pi_{C} \big) 
    & = \sum_{j=1}^{n} {\Pr\big(p_j^{k+1} \rightarrow F_\FP^{\mathcal{T}};\pi_{C} \big)} \Delta_\FP^{\mathcal{T}}(p_i^{k},a^*,p_j^{k+1}) \\
    & \geq \sum_{j=1}^{n} {f(p_j^{k+1})} \Delta_\FP^{\mathcal{T}}(p_i^{k},a^*,p_j^{k+1}),  
\end{split}
\end{equation}
where $n=|N(p_i^{k},a^*)|$ and the inequality is obtained from the premise that  \eqref{lower-bound} holds for every state at $t=k+1$. Furthermore, due to \eqref{opt_problem}, \eqref{opt_max_kappa}, and \eqref{policy_equation},

\begin{equation}\label{lemma_eq3}
       \sum_{j=1}^{n} f(p_j^{k+1}) \Delta_\FP^{\mathcal{T}}(p_i^{k},a^*,p_j^{k+1})  
        \geq  \kappa(p_i^{k},a^*) = f(p_i^{k}) .
\end{equation}

Due to \eqref{lemma_eq2} and \eqref{lemma_eq3}, \eqref{lower-bound} holds for $t = k$. Hence, we conclude by induction that \eqref{lower-bound} holds for any $t$ in $\{0,1,\dots,T\}$.
\end{proof}

%\section{Satisfying Temporal Logic Constraint During Q-learning}

Next, we present a modified Q-learning algorithm that learns a policy while the desired TL constraint is satisfied with a probability greater than $Pr_{des}$ in every episode. 
In a nutshell, the algorithm first prunes actions that may lead to states with a lower bound $f(\cdot)$ smaller than $Pr_{des}$ and only allows the remaining actions during learning. In cases where all the actions are pruned this way, an action is selected to maximize $\kappa(\cdot)$. In light of Lemma~\ref{lemma}, we propose the so-called one-shot pruning algorithm (Alg.~1a) to construct a pruned time-total product MDP. In Alg.~1b, we present a modified Q-learning algorithm to learn a policy over the pruned time-total product MDP.


%\begin{assumption}
%\label{assm}
%Given an MDP and some $\epsilon \in [0,1)$, we assume that any feasible transition can be reverted in one time step with probability at least $1-\epsilon$, i.e., for any $s,s' \in S$
%\begin{equation}
%\label{asseq}
%\resizebox{0.91\columnwidth}{!}{
%\exists a \in A: \Delta_M(s,a,s^\prime) > 0 \Rightarrow \exists a^\prime \in A: \Delta_M(s^\prime,a^\prime,s) \geq 1-\epsilon.
%}
%\end{equation}
%Furthermore, for each state $s$ and $a$, the set of states $s^'$ such that $\Delta_M(s,a,s^\prime)> 0$ and the set of states $s^{''}$ such that $\Delta_M(s,a,s^{\prime\prime})\geq1-\epsilon$ are known.
%\label{reversible}
%\end{assumption}

 
% For example, for a mobile agent as in Fig. \ref{fig:productmdp-mot}, suppose that each action (e.g., ``move up") results in moving to the desired cell with probability $0.9$ or moving to one of the other adjacent cells with probability $0.1$. While the actual values of these probabilities are unknown, some prior information (empirical data) may indicate that for each action the only transition that occurs with probability at least $0.7$ ($\epsilon=0.3$) is moving to the desired cell. 

%In the same example, the assumption also implies that any feasible transition can also be reverted with probability at least $0.7$ by choosing to move back to the previous location in the next time step. \textcolor{blue}{For the simplicity of the expressions derived in this paper, we will assume such one-step reversibility, which is satisfied in many real high-level planning problems (e.g., a mobile robot can usually go back to its previous location in the next time step). However, the results of this paper can also be extended by considering $\alpha$-step reversibility (each feasible transition can be reverted via a sequence of at most $\alpha$ likely transitions for some positive integer $\alpha$) to accommodate more generalized cases.} 

%but we assume that a conservative lower bound is known (e.g., the transition probability is at least ).  
%JUSTIFY THE ASSUMPTION

% \begin{assumption}
% Given an MDP and some $\epsilon \in [0,1)$, we assume that, for each $s \in S$, the set of states reachable from $s$ in one time step with probability at least $1-\epsilon$, i.e.,
% \begin{equation}\small
% N(s) = \{s^\prime \mid \exists a \in A: P(s,a,s^\prime) \geq 1-\epsilon\},    
% \end{equation}
% is non-empty. We call the set $N(s)$ as the neighborhood of $s$ under \textbf{$\epsilon$-filtered transitions}. 
% \label{1-eps_known}
% \end{assumption}

% \begin{assumption}
% Given an MDP and some $\epsilon \in [0,1)$, we assume that any feasible transition can be reverted in one time step with probability at least $1-\epsilon$, i.e., for any $s,s' \in S$
% \begin{equation}
% \exists a \in A: P(s,a,s^\prime) > 0 \Rightarrow \exists a^\prime \in A: P(s^\prime,a^\prime,s) \geq 1-\epsilon. 
% \end{equation}
% \label{reversible}
% \end{assumption}

% %Similar to $N(s)$, in a product MDP, let 

% Now, we will define the \emph{distance under $\epsilon-$filtered transitions} between two product MDP states.


%Similar to $N(s)$, in a product MDP, let 


% % %\vspace{-3mm}



% \begin{definition}[Distance under $\epsilon-$Filtered Transitions] 
% The distance under $\epsilon-$filtered transitions between any two product MDP states, $p,p' \in S_\FP$, $dist^{\epsilon}(p,p')$, is equal to the minimum number of  $\epsilon-$filtered transitions that takes the product MDP from $p$ to $p'$. For any $p=(s,q)$, $N(p)$ denotes the neighborhood of $p$ under $\epsilon-$filtered transitions,
% \begin{equation}
% \small
% N(p) = \{p'=(s',q') \mid s' \in N(s),  \delta(q,l(s^\prime)) = q' \},    
% \end{equation}
% which is the set of states $p'$ such that $dist^{\epsilon}(p,p')=1$.
% \label{ML-distance}
% \end{definition}

%The distance between two product automaton states, i.e., $dist(p_i,p_j)$ where $p_i,p_j \in S_{\FP}$, is the number of edges in a shortest path. 

% \begin{definition}
% [Go-to-$F_{\FP}^{\mathcal{T}}$ Policy] Given any time-product MDP and $\epsilon \in [0,1)$, 
% Go-to-$F_{\FP}^{\mathcal{T}}, $  $\pi^{\epsilon}_{GO}: S_{\FP}^{\mathcal{T}} \rightarrow A $, is a stationary policy over the time-product MDP $\FP^{\mathcal{T}}$ such that
% \begin{equation}
% \label{goeq}
% \pi^{\epsilon}_{GO}(p^t) = \arg\min_{a \in A} d^{\epsilon}_{min}(p^t,a),
% \end{equation}
% where $d^{\epsilon}_{min}(p^t,a)$ is the smallest distance-to-$F_{\FP}^{\mathcal{T}}$ among the states that can be reached from $p^t$ via $a$ with probability at least $1-\epsilon$, i.e., $d^{\epsilon}_{min}(p^t,a)= \min\limits_{p_j^{t+1}:\Delta_\FP^{\mathcal{T}}(p^t,a,p_j^{t+1})\geq1-\epsilon} d^\epsilon(p_j^{t+1}).$
% % \begin{equation}
% % d^{\epsilon}_{min}(p^t,a)= \min_{p_j^{t+1}:\Delta_\FP^{\mathcal{T}}(p^t,a,p_j^{t+1})\geq1-\epsilon} d^\epsilon(p_j^{t+1}).
% % \end{equation}
% \end{definition}



%\begin{assumption}
%Given a product MDP, any state $p_i \in S_{\FP}$ satisfies $d^{ML}(p_i) < \infty$.
%\label{strongly-connected}
%\end{assumption}

% \begin{lemma}
% Let $Pr^{\pi^{ML}}\big(d^{ML}(p_j)=0,k\big)$ be the probability of reaching the set of accepting states from $p_j$ in the next $k$ time steps under the stationary policy $\pi^{ML}$. Let $d_{max} = \max_{p_m \in N(p_j)} d^{ML}(p_m)$ be the maximum distance under most-likely transitions after taking a transition from $p_j$ where $N(p_j)$ is the set of states reachable from $p_j$ under each action $a$ with probability $1-\epsilon$. Given $\epsilon \in (0,1)$, if any state $p_i \in S_{\FP}$ satisfies $d^{ML}(p_i) < \infty$ and $k-1 > d_{max}$, then
% \begin{equation}
% Pr^{\pi^{ML}}\big(d^{ML}(p_j)=0,k\big) \geq \sum\limits_{i=0}^{\lfloor \frac{k-1-d_{max}}{2}\rfloor}  \frac{k!}{(k-i)!i!} \epsilon^i (1-\epsilon)^{k-i}0.    
% \label{lower-bound}
% \end{equation}
% \label{lemma}
% \end{lemma}

% \begin{proof}
% In the premise of Lemma~\ref{lemma}, any state $p_i \in S_{\FP}$ satisfying $d^{ML}(p_i) < \infty$ implies that it is possible to reach the set of accepting states from any state of the product MDP. Moreover, $k-1 \geq d_{max}$ means that after taking a transition from $p_j$, the remaining time $k-1$ needs to be at least $d_{max}$ which can also be referred as the minimum total number of transitions to reach the set of accepting states. Note that if $k-1 < d_{max}$, $Pr^{\pi^{ML}}\big(d^{ML}(p_j)=0,k\big) = 0$ because, even in the deterministic case, the set of accepting states is not reachable from $p_j$ in $k$ time steps. Now, we will first show what the expression inside the summation in \eqref{lower-bound} implies, and then show the summation term.

% \emph{Part 1:} Starting from state $p_j$, the policy $\pi^{ML}$ results in to select actions that drive the system to the set of accepting states over a shortest path under the most likely transitions. Note that, if the system is under policy $\pi^{ML}$, each action reduces its distance to the set of accepting states by $-1$ with probability $1-\epsilon$, and in the worst case, that action might increase its distance to the set of accepting states by $+1$ with probability $\epsilon$\footnote{An increase in the distance by $+1$ is ensured by Assumption~\ref{reversible}.}. Considering the worst case, given the remaining time $k>0$, let $i<k$ be the total number of transitions, each of which increases the distance to the set of accepting states by $+1$. Note that $\frac{k!}{(k-i)!i!}$ represents the number of all possible sequences where there exist $i$ transitions that increase the distance and $k-i$ transitions that reduces the distance. Hence, the probability of realizing all these sequences will become $\frac{k!}{(k-i)!i!} \epsilon^i (1-\epsilon)^{k-i}$.

% \emph{Part 2:} Given the remaining time $k$ and assuming that $k > d^{ML}(p_j)$, reaching the set of accepting states under the policy $\pi^{ML}$ can be achieved by a sequence where at most $\lfloor \frac{k-1-d_{max}}{2}\rfloor$ number of transitions cause an increase in the distance. In light of Part 1, the expression $\sum\limits_{i=0}^{\lfloor \frac{k-1-d_{max}}{2}\rfloor}  \frac{k!}{(k-i)!i!} \epsilon^i (1-\epsilon)^{k-i}$ quantifies a lower bound to the probability of reaching the accepting state from $p_j$ under policy $\pi^{ML}$ in $k$ time steps. 
% \end{proof}
 \begin{algorithm}[htb!]
 \label{alg:alg1}
 \begin{center}
\resizebox{\columnwidth}{!}{
\begin{tabular}{ll}
%\hline 
\bf{Alg. 1a:} \textbf{One-shot Pruning Algorithm for Time-total Product MDP} \\
\hline
 \emph{Input:} $ \mathcal{P}^{\mathcal{T}} = (S^{\mathcal{T}}_{\mathcal{P}}, P^{\mathcal{T}}_{init}, A, \Delta_\FP^{\mathcal{T}}, R^{\mathcal{T}}_{\mathcal{P}}, F^{\mathcal{T}}_{\mathcal{P}}, \Psi^{\mathcal{T}}_{\mathcal{P}})$ (time-total product MDP)\\
 \emph{Input:} $Pr_{des}$ (desired satisfaction probability)\\
\emph{Output:} $\mathcal{P}^{\mathcal{T}}$ (pruned time-product MDP), $\pi_C$\\
\hline 
\mbox{\small $\;1:\;$}\textbf{Initialization:}  $T_s= T,\,\,T_e=0,\,\,$ $Act(p^t)=A$ for all $p^t \in S^\mathcal{T}_\mathcal{P}$ \\
\hspace{2.5cm}  $f(p^{T_s}) \leftarrow \eqref{v(s)_T}$ for all $p^T \in S^\mathcal{T}_\mathcal{P}$ \\
\mbox{\small $\;2:\;$}\hspace{0.1cm}\textbf{for} each $t \in\{T_s-1,T_s-2,\dots,T_e\}$ \\
\mbox{\small $\;3:\;$}\hspace{0.5cm}\textbf{for} each $p^t$ s.t. $p^t \notin F^{\mathcal{T}}_{\mathcal{P}}$ and $p^t \notin \Psi^{\mathcal{T}}_{\mathcal{P}}$\\
\mbox{\small $\;4:\;$}\hspace{0.9cm}\textbf{for} each action $a \in Act(p^t)$\\
\mbox{\small $\;5:\;$}\hspace{1.3cm}$ N(p^{t},a) \leftarrow \eqref{N_definition}$; \\
\mbox{\small $\;6:\;$}\hspace{1.3cm}\textbf{if } $\exists p^{t+1}\in N(p^{t},a)$ s.t. $f(p^{t+1}) < Pr_{des}$ \\
\mbox{\small $\;7:\;$}\hspace{1.55cm}$Act(p^t)=Act(p^t)\setminus \{a\}$ ;\\
\mbox{\small $\;8:\;$}\hspace{1.3cm}\textbf{end if}\\
\mbox{\small $\;9:\;$}\hspace{1.3cm}$ \kappa(p^{t},a) \leftarrow \eqref{opt_problem}$; \\
\mbox{\small $\;10:\;$}\hspace{0.75cm}\textbf{end for}\\
\mbox{\small $\;11:\;$}\hspace{0.75cm}$f(p^t) \leftarrow $\eqref{opt_max_kappa};\\
\mbox{\small $\;12:\;$}\hspace{0.75cm}$ \pi_{C}(p^t)= \argmax \limits_{a}\kappa(p^{t},a) $;\\
\mbox{\small $\;13:\;$}\hspace{0.4cm}\textbf{end for}\\
\mbox{\small $\;14:\;$}\hspace{0.1cm}\textbf{end for}\\
\mbox{\small $\;15:\;$} $\mathcal{P}^{\mathcal{T}} =(S^{\mathcal{T}}_{\mathcal{P}}, P^{\mathcal{T}}_{init}, Act:S_\FP^\mathcal{T} \rightarrow 2^A, \Delta_\FP^{\mathcal{T}}, R^{\mathcal{T}}_{\mathcal{P}}, F^{\mathcal{T}}_{\mathcal{P}}, \Psi^{\mathcal{T}}_{\mathcal{P}})$
\end{tabular}}
\end{center}
\end{algorithm}

 \begin{algorithm}[htb!]
 \label{alg:alg1}
 \begin{center}
\resizebox{\columnwidth}{!}{
\begin{tabular}{ll}
%\hline 
\bf{Alg. 1b:} \textbf{Q-Learning with Constraint Satisfaction Guarantee} \\
\hline
 \emph{Input:} $\mathcal{P}^{\mathcal{T}} =(S^{\mathcal{T}}_{\mathcal{P}}, P^{\mathcal{T}}_{init}, Act:S_\FP^\mathcal{T} \rightarrow 2^A, \Delta_\FP^{\mathcal{T}}, R^{\mathcal{T}}_{\mathcal{P}}, F^{\mathcal{T}}_{\mathcal{P}}, \Psi^{\mathcal{T}}_{\mathcal{P}})$ \\
\emph{Input:} $p_{init}=(s_{init},q_{init},0) \in P_{init}^{\mathcal{T}}$, $\pi_C$ \\
\emph{Output:}  $\pi:S^\mathcal{T}_\mathcal{P} \rightarrow A$ \\
\hline 
\mbox{\small $\;1:\;$}\textbf{Initialization:} Initial $Q-$table, $p \leftarrow p_{init}$;  \\
\mbox{\small $\;2:\;$}$\text{flag}^{\pi_C} \leftarrow $ False; \\
\mbox{\small $\;3:\;$}\hspace{0.05cm}\textbf{for}\hspace{0.1cm}  $j=0:N_{episode}$ \\
\mbox{\small $\;4:\;$}\hspace{0.5cm}\textbf{for} $t=0:T$ \\ 
\mbox{\small $\;5:\;$}\hspace{0.9cm}\textbf{if} $\text{flag}^{\pi_C}$ or $Act(p)=\emptyset$\\
\mbox{\small $\;6:\;$}\hspace{1.25cm} Select an action \emph{a} from $\pi_C(p)$;\\
\mbox{\small $\;7:\;$}\hspace{1.25cm} $\text{flag}^{\pi_C} \leftarrow $ True;\\
\mbox{\small $\;8:\;$}\hspace{0.9cm}\textbf{else}\\
\mbox{\small $\;9:\;$}\hspace{1.4cm}Select an action \emph{a} from $Act(p)$ via $\epsilon-$greedy ($\pi$ policy);\\
\mbox{\small $\;11:\;$}\hspace{0.75cm}Take action \emph{a}, observe the next state $p^\prime=(s^\prime,q^\prime, t+1)$ and reward \emph{r};\\
\mbox{\small $\;12:\;$}\hspace{0.75cm}$Q(p,a) = (1-\alpha_{ep}) Q(p,a) + \alpha_{ep} \big[ r + \gamma \max\limits_{a^\prime}  Q(p^\prime,a^{\prime}) \big]$; \\
\mbox{\small $\;13:\;$}\hspace{0.75cm}$\pi(p) = \arg\max\limits_a Q(p,a))$; \\
\mbox{\small $\;14:\;$}\hspace{0.75cm}$p = p^\prime$; \\
\mbox{\small $\;15:\;$}\hspace{0.75cm}\textbf{if} $p$ is $Accepting$ or $Trash$\\
\mbox{\small $\;16:\;$}\hspace{1.1cm} $\text{flag}^{\pi_C} \leftarrow $ False;\\
\mbox{\small $\;17:\;$}\hspace{0.35cm}\textbf{end for}\\
\mbox{\small $\;18:\;$}\hspace{0.35cm}$ p = (s^\prime,q_{init},0)$; \\
\mbox{\small $\;19:\;$}\textbf{end for} 
\end{tabular}}
\end{center}
\end{algorithm}

Algorithm~1a is executed offline and its inputs are the desired probability of satisfaction $Pr_{des}$ and the time-total product MDP constructed from the MDP and the TL constraint $\phi$. The algorithm starts with initializing the time horizon $T$ as the time-bound and feasible action set $Act(\cdot)$ for each time-total product MDP state as $A$, the action set of the MDP. 
For each time-total product MDP state $p^t$ that is neither a trash state nor an accepting state, the pruning is done by inspecting every action. If an action $a$ can cause the system to move to a state $p^{t+1}$ where $f(p^{t+1})$ is less than $Pr_{des}$, then the action is pruned from $Act(p^t)$ (lines 6-8). Then, $f(p^t)$ is computed based on the values of $f(p^{t+1})$, and the action $\pi_{C}(p^t)$ is obtained. The output of the algorithm is a time-total product MDP with the pruned action sets $Act(\cdot)$ and the policy $\pi_C$.

A modified $Q$-learning algorithm for learning a policy on the pruned time-total product MDP is presented in Alg.~1b. For each time-total product MDP state $p$, if the feasible action set $Act(p)$ is not empty, then an action is selected from $Act(p)$ (line 9). If $Act(p)$ is empty, the RL agent will follow the policy $\pi_{C}$ until it reaches accepting states or trash states (lines 15-16). The general steps for the $Q$-updates are achieved in lines 11-13. 

The following theorem shows that the probability of reaching an accepting state (i.e., satisfying the desired TL constraint)  while executing Alg.~1 is at least $Pr_{des}$ in every episode of learning. 
%Since the probability of satisfying $\phi$ is equivalent to the probability of reaching the accepting states in the time-total product MDP, the following discussion will focus on providing a theoretical guarantee of the probability of reaching the accepting states.

\vspace{-2mm}
\begin{theorem}
\label{theorem1}
Given a time-total product MDP and lower/upper bounds of transition probabilities $\Delta_{min}$ and $\Delta_{max}$ as in \eqref{not_really_an_assumption}, %Let $T$ be the time bound of the time-bounded temporal logic constraint $\phi$ that should be satisfied with a probability of at least $Pr_{des}$ in each episode. 
let $f(\cdot)$ denote the lower bound function computed in Alg. 1a. If the set of initial states of the time-total product MDP, i.e., $P_{init}^\mathcal{T}$, satisfies
\begin{equation}
    \label{eq:th1}
    f(p_i^0) \geq Pr_{des}, \quad \forall p_i^0 \in P_{init}^\mathcal{T},
\end{equation}
then the probability of reaching the accepting states is at least $Pr_{des}$ in every episode while Alg. 1b is executed.
\end{theorem}
% %\vspace{-2mm}

\begin{proof}
In each episode of learning, there are $T$ actions to be taken. Each action is either from the set $Act$ (line 9 Alg. 1b) or from policy $\pi_C$ (line 6 Alg. 1b). Line 5 in Alg. 1b implies that the agent might switch from $Act$ to $\pi_C$ at some point before reaching accepting states or trash states. Once the agent adopts $\pi_C$, it will follow it until reaching either the accepting states or trash states (Lines 7, 15-16). Therefore, there are three different cases of action sequences that could be observed in each episode in Alg. 1b:
\begin{itemize}
    \item \textbf{Case(a)}: Sequences such that all actions from time 0 to $T-1$ are taken from set $Act$.
    \item \textbf{Case(b)}: Sequences that start with $Act$ actions and switch to $\pi_C$ at some point until reaching a $trash$ or $accepting$ states. 
    % such that switching from $Act$ to $\pi_C$ actions happened at some time $0<t'< t$, and all the following actions from $t'$ to $t$ are taken according to the policy $\pi_C$.
    \item \textbf{Case(c)}: Sequences that start with $\pi_C$ until reaching a $trash$ or $accepting$ state. 
\end{itemize}
Since these cases are disjoint,
%and for each the probability of constraint satisfaction is at least $Pr_{des}$, 
we prove the theorem by showing that the probability of reaching an accepting state at time $t$ is at least $Pr_{des}$ in each of these cases.

% These action sequences can be categorized into three different sequence types: (a) sequences such that the last action taken at time $T - 1$ is selected from the set $Act$; (b) sequences such that the action at some $t \in \{0,1,...,{T-2}\}$ is selected from the set $Act$, and all the following actions from $t+1$ to the end of episode are taken according to the policy $\pi_C$; and (c), sequences such that all actions are taken according to the policy $\pi_C$. These cases are shown in Fig. \ref{fig:cases}.
\noindent\textbf{Case(a)} Remember that every state at $t=T$ is either $trash$ ($f(p_i^T)=0$) or $accepting$ ($f(p_i^T)=1$). Hence, for any $Pr_{des} \in (0,1]$, the only actions left in $Act(p_i^{T-1})$ after the pruning in lines 6-8 of Alg. 1a are those that surely lead to an accepting state, i.e., $N(p_i^{T-1},a)\subseteq F^{\mathcal{T}}_{\mathcal{P}}$ for every $a\in Act(p_i^{T-1})$. Thus,  the system surely reaches an accepting state in this case.



% For any state $p_i^{T-1}$, let  $Act(p_i^{T-1})$ denote the set of available actions from which the last action at $T - 1$ is chosen. For all $a \in Act(p_i^{T-1})$, the probability of reaching $accepting$ states from $p_i^{T-1}$ by taking action $a$, i.e. $\Pr\big(p_i^{T-1} \rightarrow F_\FP^{\mathcal{T}}; a \big)$ is
% \begin{equation*}
% \begin{split}
% \Pr\big(p_i^{T-1} \rightarrow F_\FP^{\mathcal{T}}; a \big)  
%    & =  \sum_{j=1}^{n} f(p_j^{T}) \Delta_\FP^{\mathcal{T}}(p_i^{T-1},a,p_j^{T})\\
%    & \geq \sum_{j=1}^{n} f(p_j^{T}) \Delta^{*}_{j} = \kappa(p_i^{T-1},a).
% \end{split}
% \end{equation*}
% Note that $n=|N(p_i^{T-1},a)|$ and values $\Delta^{*}_{1},\hdots,\Delta^{*}_{n}$ are solution to minimization problem \eqref{opt_problem} for pair $(p_i^{T-1},a)$. From lines 9-10 in Alg. 1 we have
% \begin{equation*}
%  \kappa(p_i^{T-1},a) \geq Pr_{des}, \quad \forall a \in Act(p_i^{T-1}). 
% \end{equation*}
% Hence $\Pr\big(p_i^{T-1} \rightarrow F_\FP^{\mathcal{T}}; a \big) \geq Pr_{des}.$

\noindent \textbf{Case(b)} For any such sequence of actions, let $t^{\prime} \in\{0,1,2, \ldots, T-2\}$ be the instant such that $a \in Act(p_i^{t'})$ and all the following actions are taken according to the policy $\pi_{C}$ until reaching a $trash$ or $accepting$ state. Due to the pruning in lines 6-8 of Alg. 1a, taking $a \in Act(p_i^{t'})$ surely leads to a state $p_i^{t'+1}$ such that ${\Pr\big(p_i^{t'+1} \xrightarrow{} F_\FP^{\mathcal{T}}; \pi_{C}\big)} \geq f(p_i^{t'+1}) \geq Pr_{des}$. Given that the system switches to $\pi_C$ starting at $p_i^{t'+1}$, we conclude that the probability of reaching an accepting state in this case is also at least $Pr_{des}$.

\noindent \textbf{Case(c)} In this case, each action is taken according to the policy $\pi_{C}$ for all time steps from 0 to $T-1$. By Lemma~\ref{lemma} and \eqref{eq:th1},
\begin{equation*}
    \Pr\big(p_i^{0} \rightarrow F_\FP^{\mathcal{T}};\pi_{C} \big) \geq f(p_i^0) \geq Pr_{des}, \quad \forall p_i^0 \in P_{init}^\mathcal{T},
\end{equation*} 
implying that the probability of reaching $accepting$ state (i.e. satisfying the constraint) is at least $Pr_{des}$. 
\end{proof}

\subsection{Multi-shot Pruning Algorithm}
While the proposed algorithm can guarantee probabilistic constraint satisfaction in every episode during RL, the learned policy might be overly-conservative since the agent will follow $\pi_C$ until it achieves the constraint satisfaction once it switches to $\pi_C$ at any point. This behavior eliminates the possibility of switching between making progress towards constraint satisfaction and collecting rewards as needed. To address this issue, we also propose a multi-shot pruning algorithm that allows the agent to switch between $\pi_C$ and exploration more effectively. The multi-shot pruning algorithm essentially decomposes the time-total product MDP into several subgraphs and applies the one-shot algorithm in each subgraph to ensure constraint satisfaction.

%o reduce conservatism and guarantee constraint satisfaction with a probability greater than the desired threshold. Essentially, 

In the example shown in Fig.~\ref{fig:multi-layer}, a time-total product MDP $\mathcal{P}^{\mathcal{T}}$ is decomposed into three subgraphs, $G_1$(green), $G_2$(yellow) and $G_3$(red), given two time stamps $t_1$ and $t_2$. Let $Pr_1$, $Pr_2$, and $Pr_3$ be the desired probability thresholds for these subgraphs such that $\Pi_{i=1}^3 Pr_i = Pr_{des}$. Starting from the last sub-graph, the backward propagation is executed until the first layer of that sub-graph (instead of the initial layer of the time-total product MDP as depicted in the previous section). After calculating the worst-case maximum probability values, the states at the last layer of each sub-graph can be classified as accepting and trash. Overall, the backward propagation procedure that is described in the previous section is adopted to the sub-graphs of the time-total product MDP. The decomposition of the time-total product MDP into subgraphs is not unique. As we will show later in Theorem \ref{theorem2}, the probability of satisfaction can be ensured as long as the provided set of time and desired probabilities comply with the required conditions. 

%\textbf{(a)} Denote $F^{\mathcal{T}}_{\mathcal{P}}$ the set of accepting states of $\mathcal{P}^{\mathcal{T}}$ as $A_3$. We execute the one-shot pruning algorithm on $G_3$ with a desired threshold $Pr_3$ and obtain the $\pi_C^3$ policy, the $max\_prob$ function $f^3(\cdot)$ and a pruned time-total product MDP $G'_3$. By Theorem 1, starting from any state $s^{t_2}$ at $t=t_2$ such that $f^3(s^{t_2})\geq Pr_3$, the RL agent is guaranteed to reach $A_3$ with a probability of at least $Pr_3$. \\
%\textbf{(b)} In $G_2$, any state $s^{t_2}$ at $t=t_2$ such that $f^3(s^{t_2})\geq Pr_3$ is considered as accepting states, while any other states at $t=t_2$ are considered as trash states. Denote the set of accepting states as $A_2 = \{s^{t_2}|f^3(s^{t_2})\geq Pr_3\}$. Repeat step (a) in $G_2$ with probability threshold $Pr_2$ to get the $\pi_C^2$ policy, the $max\_prob$ function $f^2(\cdot)$ and a pruned time-total product MDP $G'_2$. By Theorem 1, starting from any state $s^{t_1}$ at $t=t_1$ such that $f^2(s^{t_1})\geq Pr_2$, the RL agent is guaranteed to reach $A_2$ with a probability of at least $Pr_2$.\\
%\textbf{(c)} In $G_1$, any state $s^{t_1}$ at $t=t_1$ such that $f^2(s^{t_1})\geq Pr_2$ is considered as accepting states, while any other states at $t=t_1$ are considered as trash states. Denote the set of accepting states as $A_1 = \{s^{t_1}|f^2(s^{t_1})\geq Pr_2\}$ and the set of trash states. Repeat step (a) in $G_1$ with probability threshold $Pr_1$ to get the $\pi_C^1$ policy, the $max\_prob$ function $f^1(\cdot)$ and a pruned time-total product MDP $G'_1$. By Theorem 1, starting from any state $s^{t_0}$ at $t=t_0$ such that $f^1(s^{t_0})\geq Pr_1$, the RL agent is guaranteed to reach $A_1$ with a probability of at least $Pr_1$.

%Assume that any initial state at $t=t_0$ satisfies $f^1(s^{t_0})\geq Pr_1$. By (c), any initial states can reach $A_1$ with a probability of at least $Pr_1$. By (b), any states in $A_2$ can reach $A_2$ with a probability of at least $Pr_2$. By (a), any states in $A_2$ can reach $A_3$ with a probability of at least $Pr_3$. Therefore, any initial state can reach the set of accepting states with a probability of at least $Pr_1*Pr_2*Pr_3 = Pr_{des}$.
% Figure environment removed
\vspace{-2mm}
We generalize Algs.~1a and 1b as a multi-shot pruning algorithm in Alg.~2a and the corresponding modified Q-learning algorithm in Alg.~2b. The inputs of the multi-shot pruning algorithm are a time-total product MDP, a set of time stamps $\{t_0=0, t_1, t_2, ..., t_{N-1}, t_N = T\}$ for decomposing the time-total product MDP, and a set of desired probabilities $\{Pr_1, Pr_2, ..., Pr_N\}$ such that $\prod\limits_{i=1}^{N}Pr_i = Pr_{des}$. In line 1, the time-total product MDP is divided into N subgraphs $\{G_i\}_{i=1,2,...N}$ (see Fig.~\ref{fig:multi-layer}), where each subgraph $G_i$ spans from $t_{i-1}$ to $t_i$. The one-shot pruning algorithm with a probability threshold $Pr_N$ is applied on the last subgraph $G_N$ to obtain the lower bound function $f^N$, pruned action set $Act^N$, and $\pi_C^N$ policy for $G_N$ (lines 2-3). We denote the sets of accepting states and trash states of time-total product MDP as $A_N$ and $T_N$ respectively (line 4). For each subgraph $G_i (i=N-1, N-2, ..., 1)$, we define the set of accepting states $A_i$ and set of trash states $T_i$ based on the function $f^{i+1}$ from the next subgraph $G_{i+1}$ (lines 6-7).  We then repeatedly apply the one-shot pruning algorithm with a probability threshold $Pr_i$ to obtain the function $f^i$, pruned action set $Act^i$, and $\pi_C^i$ policy for $G_i$ (lines 8-10). For each subgraph $G_i$, the pruning is carried out from $t_{i-1}$ to $t_{i}-1$, and thus the pruned action set $Act^i$ and policy $\pi_C^i$ cover from $t_{i-1}$ to $t_{i}-1$. Therefore, $Act^i$ and $\pi_C^i$ for the subgraphs are disjoint and can combine into a single $Act(\cdot)$ and $\pi_C$ (lines 11-12). Note that the one-shot algorithm is essentially a special case of the multi-shot algorithm with the set of time $\{t_0=0, t_1 = T\}$ and set of probabilities $\{Pr_1 = Pr_{des}\}$.

In Alg.~2b, once the agent adopts the $\pi_C$ policy (line 7), it can switch to $Act$ if it reaches the accepting states or trash states of any subgraph (lines 15-16). Essentially, Alg.~2b applies Alg.~1b on each subgraph to learn a policy with a probabilistic constraint satisfaction guarantee. 

\begin{algorithm}[htb!]
 \label{alg:alg1}
 \begin{center}
\resizebox{\columnwidth}{!}{
\begin{tabular}{ll}
%\hline 
\bf{Alg. 2a:} \textbf{Multi-shot Pruning Algorithm for Time-total Product MDP} \\
\hline
 \emph{Input:} $ \mathcal{P}^{\mathcal{T}} = (S^{\mathcal{T}}_{\mathcal{P}}, P^{\mathcal{T}}_{init}, A, \Delta_\FP^{\mathcal{T}}, R^{\mathcal{T}}_{\mathcal{P}}, F^{\mathcal{T}}_{\mathcal{P}}, \Psi^{\mathcal{T}}_{\mathcal{P}})$ (time-total product MDP)\\
 \emph{Input:} $\{t_0=0, t_1, t_2, ..., t_{N-1}, t_N = T\}$, $\{Pr_1, Pr_2, ..., Pr_N\}$\\
\emph{Output:} $\mathcal{P}^{\mathcal{T}}$ (pruned time-total product MDP),\,$\pi_C$\\
\hline 

\mbox{\small $\;1:\;$}\hspace{0.1cm}$G_{i} \leftarrow \mathcal{P}^{\mathcal{T}}_{t_{i-1}:\,t_{i}}$, $i = N, N-1, ..., 1 $\\
\mbox{\small $\;2:\;$}\hspace{0.1cm}\textbf{Initialization:}  $T_s \leftarrow t_N,\,\, T_e \leftarrow t_{N-1}$, \,\,$Act(p^t)=A$ for all $p^t \in S^\mathcal{T}_\mathcal{P}$ \\
\hspace{2.5cm}  $f^N(p^{T_s}) \leftarrow \eqref{v(s)_T}$ for all $p^T \in S^\mathcal{T}_\mathcal{P}$ \\
\mbox{\small $\;3:\;$}\hspace{0.1cm}$f^N(\cdot), \,\pi_C^N, \,Act^N(\cdot) \leftarrow $ run lines 2-18 in Alg.~1a on $G_N$ with $Pr_N$\\
\mbox{\small $\;4:\;$}\hspace{0.1cm}$A_N \leftarrow F^{\mathcal{T}}_{\mathcal{P}}; T_N \leftarrow \Psi^{\mathcal{T}}_{\mathcal{P}} $\\
\mbox{\small $\;5:\;$}\hspace{0.1cm}\textbf{for} $i = N-1, N-2, ..., 1 $\\
\mbox{\small $\;6:\;$}\hspace{0.5cm} $A_i \leftarrow \{p^{t_i}|f^{i+1}(p^{t_i})\geq Pr_{i+1}\}$(set of accepting states for $G_i$)\\
\mbox{\small $\;7:\;$}\hspace{0.5cm} $T_i \leftarrow \{p^{t_i}|f^{i+1}(p^{t_i})< Pr_{i+1}\}$(set of trash states for $G_i$)\\
\mbox{\small $\;8:\;$}\hspace{0.5cm} $T_s \leftarrow t_i, T_e \leftarrow t_{i-1}$\\
\mbox{\small $\;9:\;$}\hspace{0.5cm} for $\forall p^{T_s}$ \textbf{if} $p^{t_i} \in A_i$ \textbf{then} $f^i(p^{t_i}) \leftarrow 1$; \textbf{else} $f^i(p^{t_i}) \leftarrow 0$  \\
\mbox{\small $\;10:\;$}\hspace{0.36cm} $G'_{i}, \,f^i(\cdot), \,\pi_C^i, \,Act^i(\cdot) \leftarrow $ run lines 2-18 in Alg.~1a on $G_i$ with $Pr_i$\\
\mbox{\small $\;11:\;$}$\pi_C \leftarrow$ combine $\pi_C^1, \pi_C^2, ..., \pi_C^N$\\
\mbox{\small $\;12:\;$}$Act(\cdot) \leftarrow$ combine $ Act^1, Act^2, ..., Act^N$\\
\mbox{\small $\;13:\;$}$\mathcal{P}^{\mathcal{T}} =(S^{\mathcal{T}}_{\mathcal{P}}, P^{\mathcal{T}}_{init}, Act:S_\FP^\mathcal{T} \rightarrow 2^A, \Delta_\FP^{\mathcal{T}}, R^{\mathcal{T}}_{\mathcal{P}}, F^{\mathcal{T}}_{\mathcal{P}}, \Psi^{\mathcal{T}}_{\mathcal{P}})$
\end{tabular}}
\end{center}
\end{algorithm}

\begin{algorithm}[htb!]
 \label{alg:alg1}
 \begin{center}
\resizebox{\columnwidth}{!}{
\begin{tabular}{ll}
%\hline 
\bf{Alg. 2b:} \textbf{Q-Learning with Constraint Satisfaction Guarantee} \\
\hline
 \emph{Input:} $\mathcal{P}^{\mathcal{T}} =(S^{\mathcal{T}}_{\mathcal{P}}, P^{\mathcal{T}}_{init}, Act:S_\FP^\mathcal{T} \rightarrow 2^A, \Delta_\FP^{\mathcal{T}}, R^{\mathcal{T}}_{\mathcal{P}}, F^{\mathcal{T}}_{\mathcal{P}}, \Psi^{\mathcal{T}}_{\mathcal{P}})$ \\
  \emph{Input:} $p_{init}=(s_{init},q_{init},0) \in P_{init}^{\mathcal{T}}$, $\pi_C$, $A_{i(i=1,2,...N)}$, $T_{i(i=1,2,...N)}$ \\
\emph{Output:}  $\pi:S^\mathcal{T}_\mathcal{P} \rightarrow A$ \\
\hline 
\mbox{\small $\;1:\;$}\textbf{Initialization:} Initial $Q-$table, $p \leftarrow p_{init}$;  \\
\mbox{\small $\;2:\;$}$\text{flag}^{\pi_C} \leftarrow $ False; \\
\mbox{\small $\;3:\;$}\hspace{0.05cm}\textbf{for}\hspace{0.1cm}  $j=0:N_{episode}$ \\
\mbox{\small $\;4:\;$}\hspace{0.5cm}\textbf{for} $t=0:T$ \\ 
\mbox{\small $\;5:\;$}\hspace{0.9cm}\textbf{if} $\text{flag}^{\pi_C}$ or $Act(p)=\emptyset$\\
\mbox{\small $\;6:\;$}\hspace{1.25cm} $a= \pi_C(p)$;\\
\mbox{\small $\;7:\;$}\hspace{1.25cm} $\text{flag}^{\pi_C} \leftarrow $ True;\\
\mbox{\small $\;8:\;$}\hspace{0.9cm}\textbf{else}\\
\mbox{\small $\;9:\;$}\hspace{1.4cm}Select an action \emph{a} from $Act(p)$ via $\epsilon-$greedy ($\pi$ policy);\\
\mbox{\small $\;11:\;$}\hspace{0.75cm}Take action \emph{a}, observe the next state $p^\prime=(s^\prime,q^\prime, t+1)$ and reward \emph{r};\\
\mbox{\small $\;12:\;$}\hspace{0.75cm}$Q(p,a) = (1-\alpha_{ep}) Q(p,a) + \alpha_{ep} \big[ r + \gamma \max\limits_{a^\prime}  Q(p^\prime,a^{\prime}) \big]$; \\
\mbox{\small $\;13:\;$}\hspace{0.75cm}$\pi(p) = \arg\max\limits_a Q(p,a))$; \\
\mbox{\small $\;14:\;$}\hspace{0.75cm}$p = p^\prime$; \\
\mbox{\small $\;15:\;$}\hspace{0.75cm}\textbf{if} $p$ in $A_i$ or $T_i$\\
\mbox{\small $\;16:\;$}\hspace{1.1cm} $\text{flag}^{\pi_C} \leftarrow $ False;\\
\mbox{\small $\;17:\;$}\hspace{0.35cm}\textbf{end for}\\
\mbox{\small $\;18:\;$}\hspace{0.35cm}$ p = (s^\prime,q_{init},0);$ \\
\mbox{\small $\;19:\;$}\textbf{end for} 
\end{tabular} }
\end{center}
\end{algorithm}
\vspace{-2mm}

\begin{theorem}
\label{theorem2}
Given a time-total product MDP $\mathcal{P}^{\mathcal{T}}$, a set of time $\{t_0=0, t_1, t_2, ..., t_{N-1}, t_N = T\}$, and a set of desired probabilities $\{Pr_1, Pr_2, ..., Pr_N\}$ such that $\prod\limits_{i=1}^{N}Pr_i = Pr_{des}$, if the following conditions hold in Alg.~2a: 
\begin{itemize}
   \item any initial state of the time-total product MDP $p^0_i \in P^{\mathcal{T}}_{init}$ satisfies $f^1(p^0_i) \geq Pr_{1}$,
\item  the set of accepting states $A_i$ defined in Alg.~2a (line 5) is nonempty for every $i\in \{1, \hdots, N-1\} $,
\end{itemize}
then in Alg.~2b, the probability of reaching the accepting states of $\mathcal{P}^{\mathcal{T}}$ in every episode is at least $Pr_{des}$.
\end{theorem}
\begin{proof}
Alg.~2b divides the time-total product MDP $\mathcal{P}^{\mathcal{T}}$ into $N$ subgraphs and applies Alg.~1b on each subgraph $G_i$ in a concatenated manner. By condition (a) and Theorem \ref{theorem1}, the probability of reaching the set of accepting states $A_1$ on subgraph $G_1$ from any initial state is at least $Pr_1$. For any $i=1,2,...,N-1$, since $A_i$ is nonempty (condition (b)), each state $p^{t_i} \in A_i$ satisfies $f^{i+1}(p^{t_i})\geq Pr_{i+1}$ (line 5 in Alg.~2a). By Theorem \ref{theorem1}, starting from any state in $A_i$, the probability of reaching the set of accepting states $A_{i+1}$ on subgraph $G_{i+1}$ is at least $Pr_{i+1}$. Then, starting from any initial state of $\mathcal{P}^{\mathcal{T}}$, the probability of reaching $A_N$, i.e., the set of accepting states of $\mathcal{P}^{\mathcal{T}}$ is at least $\prod\limits_{i=1}^{N}Pr_i = Pr_{des}$.
\end{proof}

\begin{table*}[ht!]
\centering
\begin{adjustbox}{width=0.93\textwidth}
\begin{tabular}{||c c c c c c c c c c c||}
 \hline
 Algorithm & ($\epsilon, Pr_{des}$) & (0.03, 0.5) & (0.03, 0.7) & (0.03, 0.9)  & (0.08, 0.5) & (0.08, 0.7) & (0.08, 0.9) & (0.13, 0.5) & (0.13, 0.7) & (0.13, 0.9) \\ [0.5ex] 
 \hline\hline
 \multirow{3}{*}{One-shot} &
 \multicolumn{1}{l}{Learning} & 
 \cellcolor{gray!20}\textcolor{blue}{91.66\%} & 
 \cellcolor{gray!45}\textcolor{blue}{91.63\%} & \cellcolor{gray!60}\textcolor{blue}{91.64\%} & \cellcolor{gray!20}\textcolor{blue}{91.60\%} & \cellcolor{gray!45}\textcolor{blue}{91.61\%} & \cellcolor{gray!60}\textcolor{blue}{99.48\%} & \cellcolor{gray!20}\textcolor{blue}{91.68\%} & \cellcolor{gray!45}\textcolor{blue}{99.32\%} & \cellcolor{gray!60}\textcolor{blue}{99.53\%} \\ &
 \multicolumn{1}{l}{Testing} & \cellcolor{gray!20}\textcolor{blue}{90.92\%} & \cellcolor{gray!45}\textcolor{blue}{91.08\%} & \cellcolor{gray!60}\textcolor{blue}{91.77\%} & \cellcolor{gray!20}\textcolor{blue}{91.43\%} & \cellcolor{gray!45}\textcolor{blue}{91.5\%} & \cellcolor{gray!60}\textcolor{blue}{99.39\%} & \cellcolor{gray!20}\textcolor{blue}{91.55\%} & \cellcolor{gray!45}\textcolor{blue}{99.53\%} & \cellcolor{gray!60}\textcolor{blue}{99.67\%} \\ &
 \multicolumn{1}{l}{Avg. Rewards} & \cellcolor{gray!20}\textcolor{blue}{20.99} & \cellcolor{gray!45}\textcolor{blue}{20.97} & \cellcolor{gray!60}\textcolor{blue}{20.88} & \cellcolor{gray!20}\textcolor{blue}{20.93} & \cellcolor{gray!45}\textcolor{blue}{20.90} & \cellcolor{gray!60}\textcolor{blue}{19.72} & \cellcolor{gray!20}\textcolor{blue}{20.91} & \cellcolor{gray!45}\textcolor{blue}{19.73} & \cellcolor{gray!60}\textcolor{blue}{19.70}
 \\\hline\hline
 \multirow{3}{*}{Multi-shot} &
 \multicolumn{1}{l}{Learning} & 
 \cellcolor{gray!20}\textcolor{red}{78.93\%} & 
 \cellcolor{gray!45}\textcolor{red}{93.52\%} & \cellcolor{gray!60}\textcolor{red}{97.96\%} & \cellcolor{gray!20}\textcolor{red}{93.18\%} & \cellcolor{gray!45}\textcolor{red}{97.61\%} & \cellcolor{gray!60}\textcolor{red}{99.65\%} & \cellcolor{gray!20}\textcolor{red}{97.54\%} & \cellcolor{gray!45}\textcolor{red}{99.22\%} & \cellcolor{gray!60}\textcolor{red}{99.93\%} \\ &
 \multicolumn{1}{l}{Testing} & \cellcolor{gray!20}\textcolor{red}{77.4\%} & \cellcolor{gray!45}\textcolor{red}{92.34\%} & \cellcolor{gray!60}\textcolor{red}{97.8\%}& \cellcolor{gray!20}\textcolor{red}{97.04\%} & \cellcolor{gray!45}\textcolor{red}{99.11\%} & \cellcolor{gray!60}\textcolor{red}{99.61\%} & \cellcolor{gray!20}\textcolor{red}{99.15\%} & \cellcolor{gray!45}\textcolor{red}{99.33\%}& \cellcolor{gray!60}\textcolor{red}{99.89\%} \\ 
 &
 \multicolumn{1}{l}{Avg. Rewards} & \cellcolor{gray!20}\textcolor{red}{79.03} & \cellcolor{gray!45}\textcolor{red}{55.98} & \cellcolor{gray!60}\textcolor{red}{49.84} & \cellcolor{gray!20}\textcolor{red}{49.29} & \cellcolor{gray!45}\textcolor{red}{39.65} & \cellcolor{gray!60}\textcolor{red}{38.84} & \cellcolor{gray!20}\cellcolor{gray!20}\textcolor{red}{40.55} &\cellcolor{gray!45}\textcolor{red}{39.62} & \cellcolor{gray!60}\textcolor{red}{20.74}
 \\
 \hline
\end{tabular}
\end{adjustbox}
\caption{Simulation results for
the task
$[H^1 P]^{[0,8]} \cdot ([H^1 D_1]^{[0,6]})\cdot ([H^1 D_2]^{[0,6]}) \; \vee \; [H^1 D_3]^{[0,6]}) \cdot [H^1 Base]^{[0,12]}$ and the real action uncertainty of $\epsilon_{real}=0.03$ 
for the one-shot and the multi-shot learning algorithms. The first two rows for each algorithm present the satisfaction ratio of the constraint over 1000000 learning episodes and 10000 testing episodes respectively. The third row for each algorithm shows the average episodic rewards for 10000 episodes during testing.}
\label{table:result}
\end{table*}

\section{Simulation Results}
%In this section, we  implemented on Python 3.10 on a PC with an Intel i5-8265U CPU at 1.6 GHz processor and 16.0 GB RAM. 
We present a case study where a robot operates on a 6$\times$6 grid environment with an action set $A = \{N,NE,E,SE,S,SW,W,NW,Stay\}$ as shown in Fig.~\ref{fig:case}. Under these actions, the agent can either move to any of the feasible adjacent cells in the 8 directions or stay at its current location. The transition probability (unknown to the agent) is as follows: Each of the first 8 actions leads to the intended transition with a probability of 0.97 or to one of the other feasible transitions (selected uniformly at random) with a probability of 0.03. For example, if the agent takes the action $E$, it will move to the adjacent cell in the direction $E$ with a probability of 0.97 (intended transition) or, with a probability of 0.03, it will stay at its location or move to a feasible adjacent cell in any of the other 7 directions (unintended transitions). If the agent takes the action $Stay$, it will stay at the original position with probability 1. In this case study, the lower bound and upper bound in \eqref{not_really_an_assumption} is given by introducing an overestimated transition uncertainty $\epsilon \geq 0.03$. More specifically, the available prior information is that each action yields the corresponding intended transition with some probability in $[1-\epsilon, 1]$. Furthermore, each of the unintended transitions has a probability in $[0,\epsilon]$. 

We consider a scenario where the agent is required to periodically perform a pickup and delivery task while maximizing situational awareness by collecting measurements from the environment. The pickup and delivery task is encoded as a TWTL\footnote{The syntax and semantics of TWTL can be found in the Appendix.} constraint: $[H^1 P]^{[0,8]} \cdot ([H^1 D_1]^{[0,6]})\cdot ([H^1 D_2]^{[0,6]}) \; \vee \; [H^1 D_3]^{[0,6]}) \cdot [H^1 Base]^{[0,12]}$, which means that \textit{``go to the pickup location $P$ and stay there for $1$ time step in the first $8$ time steps and immediately after that go to $D_1$ and stay there for $1$ time step within $6$ time steps, and immediately after that go to either $D_2$ or $D_3$ within $6$ time steps and stay there for $1$ time step, and immediately after that go to $Base$ and stay there for $1$ time step within $12$ time steps."}. Based on the time bound of this TWTL specification, the length of each episode is selected as $35$ time steps. 
%  Accordingly, the reward $r_t$  represents the value of monitoring the agent's current position on the grid and the discount factor in \eqref{pistareq} is selected as  $\gamma = 0.95$.
% In Fig.~\ref{fig:sim_benchmarks}, the light gray cells, the dark gray cell, and all the other cells yield a reward of $1$, $10$, and $0$, respectively. 

In Fig.~\ref{fig:case}, we present two sample trajectories by applying the learned policies obtained by the one-shot and multi-shot algorithms, with $\epsilon = 0.08$ and $Pr_{des}=0.9$. Following the one-shot policy, the robot adopts the $\pi_C$ policy at $t=4$ until it satisfies the TWTL constraint. Though the one-shot algorithm provides a constraint satisfaction guarantee, following the $\pi_C$ policy until satisfying the constraints prevents the agent from learning a policy to visit the high-reward location. The multi-shot algorithm relaxes such a requirement by allowing the agent to explore before satisfying the constraints (lines 15-16 in Alg.~2b) and can potentially learn a policy yielding higher reward than the one-shot algorithm.

% Figure environment removed

We investigate how the parameters $\epsilon$ and $Pr_{des}$ influence the performance of the one-shot and multi-shot algorithms. The proposed algorithms are executed with varying values of $Pr_{des} =0.5,0.7, 0.9 $ and $\epsilon =0.03,0.08,0.13$. Since the constraint involves 4 sub-tasks, for the multi-shot algorithm, the set of time for dividing the time-total product MDP is chosen as $\{t_0=0, t_1 = 8, t_2 = 15,t_3 = 22, t_4 = 35\}$ where $t_1$, $t_2$, $t_3$, and $t_4$ are the deadline by which the sub-tasks must be completed. The set of desired probabilities for the multi-shot algorithm is selected as $\{\sqrt[\uproot{3}4]{Pr_{des}}, \sqrt[\uproot{3}4]{Pr_{des}}, \sqrt[\uproot{3}4]{Pr_{des}}, \sqrt[\uproot{3}4]{Pr_{des}} \}$.
Note that the selection of the set of time steps and the set of desired probabilities is not unique, as long as the conditions in Theorem 2 hold. The results are shown in Table \ref{table:result}.

 The performance of the multi-shot algorithm is significantly influenced by the parameters $\epsilon$ and $Pr_{des}$. For a fixed $Pr_{des}$, we observe that the satisfaction rate increases with $\epsilon$, while the average rewards decrease with $\epsilon$. Since we use the transition uncertainty $\epsilon$ to determine the lower and upper bounds of the transition probability of the system and solve for the worst-case maximum satisfaction probability, we will obtain lower probability values with a higher $\epsilon$. Therefore, more actions will be pruned (see Alg.~1a line 6) and the RL agent is more likely to adopt the $\pi_C$ policy which maximizes the satisfaction probability. The more confined action set resulting from higher $\epsilon$ also explains the decrease in average rewards. Similarly, for a fixed $\epsilon$, we notice that a higher $Pr_{des}$ results in a higher satisfaction rate and lower rewards. A higher $Pr_{des}$ implies pruning more actions (see Alg.~1a line 6), which also explains the increase in satisfaction rate and decrease in average rewards. 
 
 The $\epsilon$ and $Pr_{des}$ parameters have no significant influence on the average rewards and satisfaction rate in the one-shot algorithm. This is due to the fact that the agent has to adopt the $\pi_C$ policy in the earlier stage of the episodes regardless of the $Pr_{des}$ and $\epsilon$ parameters. Otherwise, it will not be able to reach the P location before $t=8$ and will fail the constraints. As a result, the agent adopts the $\pi_C$ policy in the early stage until it satisfies or fails the TWTL constraint. Therefore, the one-shot algorithm learns a similar policy under different parameters (as in Fig.~\ref{fig:case}a).





%In this case, $\epsilon \geq 0.03$ indicates a conservative bounds for the transition probabilities. For example, assume that agent take action $a$ at state $s$. For $\epsilon = 0.13$, the algorithm assumes that the probability of reaching the intended state is bounded by $[0.87, 1]$, and the probability of reaching each unintended transitions is bounded by $[0, 0.13]$. While in reality, the intended transition actually occurs with a probability of $0.97$, and the probability of intended transitions sums up to $0.03$. The results for Case 4 are depicted in Table \ref{table:case4}. For a fixed $Pr_{des}$, we observe that increasing $\epsilon$ reduces collected reward and increases the probability of constraint satisfaction. This is due to the fact that overestimating the uncertainty makes the algorithm overly cautious in exploration and more inclined to first satisfy the TWTL constraint. Moreover, for a fixed $\epsilon$, increasing $Pr_{des}$ reduces the collected reward since the constraint becomes more restrictive.

%We consider a same scenario as in \cite{aksaray2021probabilistically}, where the robot periodically performs a pickup and delivery task while trying to maximize situational awareness by collecting rewards from the environment. In Fig.~\ref{fig:case1&2}, the light gray cells, the dark gray cell, and all the other cells yield a reward of 1, 10, and 0, respectively. The pickup and delivery task is expressed as a TWTL formula: $[H^1P]^{[0,20]}\cdot([H^1D_1]^{[0,20]} |[H_1D_2]^{[0,20]})\cdot[H^1Base]^{[0,20]}$, which means that ``reach the pickup location $P$ and stay there for 1 time step in the first 20 time steps and immediately after that reach one of the delivery locations, $D_1$ or $D_2$, and stay there for 1 time step within 20 time steps, and immediately after that go to Base and stay there for 1 time step within 20 time steps.". Based on the time bound of the TWTL formula, we select the length of each episode as 62 time steps.

%The simulation results are presented under the following four cases. The first two cases are performed to compare the performance of the proposed algorithm with the standard Q-learning: 1) learning to satisfy the TWTL formula, and 2) maximizing the expected reward while satisfying the TWTL constraint (proposed approach). The third case is performed to demonstrate how the performance under the proposed approach changes when the desired probability of constraint satisfaction, $Pr_{des}$, increases or the given bounds of probabilities of transitions become conservative, i.e., Alg. 1 is executed with some $\epsilon > 0.1$. The fourth case is performed to compare the performance of the proposed algorithm with that in \cite{aksaray2021probabilistically}.

%In Case 1,  we use Q-learning to learn a policy that satisfies the desired TWTL task, without any consideration of the monitoring performance. To accomplish this, positive rewards are assigned to the accepting states of the time-total product MDP and the rewards at any other state are zero. The Q-learning algorithm is run for 400000 episodes, and the agent eventually learns to satisfy the TWTL constraint by following a shortest path. However, there is no guarantee on the constraint satisfaction in the early stages of learning. In the simulation, the agent almost never satisfies the constraints before 120000 episodes and it learns a policy with constraint satisfaction after 180000 episodes. The total run time for this case is 659.3 seconds and a sample trajectory by following the learned policy is shown in Fig.~\ref{fig:case1&2} (a).

%In Case 2,  $Pr_{des}$ is chosen as 0.7 and Alg. 1 is executed by selecting $\epsilon =0.1$. We use the proposed algorithm to train a policy for 400000 episodes. The TWTL constraint is satisfied in 351841 episodes, implying a success ratio $0.879 > Pr_{des} = 0.7$. Note that the proposed algorithm can guarantee the constraint satisfaction even for the first episode, with the TWTL constraint being satisfied in 85 times in the first 100 episodes. The run time for this case is 799.2 seconds and a sample trajectory by following the learned policy is shown in Fig.~\ref{fig:case1&2} (b).

%% Figure environment removed

%In Case 3, we investigate the performance of the  algorithm presented in \cite{aksaray2021probabilistically} under the following TWTL constraints $[H^1P]^{[0,20]}\cdot([H^1D_1]^{[0,20]} |[H_1D_2]^{[0,20]})\cdot[H^1Base]^{[0,20]}$, and compare the results to Case 2. Let $\Delta_1$ denote the time it takes to reach the pickup location, $\Delta_2$ denote the time from the pickup location to the delivery location, and $\Delta_3$ denote the time from the delivery location to the base. We simulate the system with the policies obtained from both algorithms and obtain 1000 satisfaction trajectories for each policy, and the distribution of $\Delta_1, \Delta_2,$ and $\Delta_3$ is shown in Fig.~\ref{fig:case4}. The results indicates that the satisfaction trajectory obtained from the proposed algorithm strictly satisfies the TWTL formula, while the one obtained from algorithm in \cite{aksaray2021probabilistically} satisfies the relaxed version of the TWTL formula. 
%% Figure environment removed


%In Case 4, we investigate how the performance of the proposed algorithm is influenced by parameters $\epsilon$ and $Pr_{des}$. The proposed algorithm is executed with varying values of $Pr_{des} =0.5,0.7, 0.9 $ and $\epsilon =0.03,0.08,0.13$. In this case, $\epsilon \geq 0.03$ indicates a conservative bounds for the transition probabilities. For example, assume that agent take action $a$ at state $s$. For $\epsilon = 0.13$, the algorithm assumes that the probability of reaching the intended state is bounded by $[0.87, 1]$, and the probability of reaching each unintended transitions is bounded by $[0, 0.13]$. While in reality, the intended transition actually occurs with probability $0.97$, and the probability of intended transitions sums up to $0.03$. The results for Case 4 are depicted in Table \ref{table:case4}. For a fixed $Pr_{des}$, we observe that increasing $\epsilon$ reduces collected reward and increases the probability of constraint satisfaction. This is due to the fact that overestimating the uncertainty makes the algorithm overly cautious in exploration and more inclined to first satisfy the TWTL constraint. Moreover, for a fixed $\epsilon$, increasing $Pr_{des}$ reduces the collected reward since the constraint becomes more restrictive.







%% Figure environment removed

%% Figure environment removed

%% Figure environment removed


% \section{Simulation Results}
% In this section, we present some case studies implemented on Python 2.7, and performed on a PC with an Intel i7-7700HQ CPU at 2.8 GHz processor and 16.0 GB RAM. In these case studies, we consider an agent moving over an $8 \times 8$ grid as shown in Fig.~\ref{fig:sim_benchmarks} and selecting actions from the set $A = \{N, NE, E, SE, S, SW, W, NW, Stay\}$. Under these actions, the agent can maintain its current position or move to any of the feasible neighboring cells (including those in the ordinal directions). 
%  Each action leads to the intended transition with probability $0.9$ or to one of the other feasible transitions (selected uniformly at random) with probability $0.1$. For example, if the agent takes the action $NE$, it moves to the neighboring cell in the direction $NE$ with probability 
% $0.9$ or, with probability  $0.1$, it stays at its location or moves to a feasible neighboring cell in any of the other 7 directions. 

% %  In this TWTL specification, returning to the base is included to address the energy limitation of the agent (e.g., for refueling or recharging).

% We consider a scenario where the agent is required to periodically perform a pickup and delivery task while maximizing situational awareness by collecting measurements from the environment.
%  Accordingly, the reward $r_t$  represents the value of monitoring the agent's current position on the grid and the discount factor in \eqref{pistareq} is selected as  $\gamma = 0.95$.
% In Fig.~\ref{fig:sim_benchmarks}, the light gray cells, the dark gray cell, and all the other cells yield a reward of $1$, $10$, and $0$, respectively. The pickup and delivery task is encoded as a TWTL constraint: $[H^1 P]^{[0,20]} . ([H^1 D_1]^{[0,20]} \; \vee \; [H^1 D_2]^{[0,20]}) . [H^1 Base]^{[0,20]}$, which means that \textit{"go to the pickup location $P$ and stay there for $1$ time step in the first $20$ time steps and immediately after that go to one of the delivery locations, $D_1$ or  $D_2$, and stay there for $1$ time step within $20$ time steps, and immediately after that go to $Base$ and stay there for $1$ time step within $20$ time steps."}. Based on the time bound of this TWTL specification, the length of each episode is selected as $62$ time steps. 


% %Overall, the agent's goal is to learn a control policy that maximizes the expected sum of rewards while satisfying the TWTL constraint in each episode with a probability at least $Pr_{des}$. 


% We present our simulation results under five cases. The first three cases are performed to compare the performance under three different behaviors: 1) maximizing the expected reward under the TWTL constraint (proposed approach), 2) maximizing the expected reward without any constraint, and 3) learning to satisfy the TWTL formula. %We show the corresponding learning curves in Fig.~\ref{fig:sim2}. 
% The fourth case is performed to demonstrate how the performance under the proposed approach changes when the desired probability of constraint satisfaction, $Pr_{des}$, increases or the probabilities of likely transitions are underestimated, i.e., Alg.~1b is executed in this scenario with some $\epsilon>0.1$. Finally, the fifth case is performed to demonstrate the scalability of the proposed algorithm under varying sizes of the time-product MDP.

%  {In Case 1, $Pr_{des}$ is chosen as $0.7$ and Alg.~1b is executed by using $\epsilon=0.1$ for $400000$ episodes. The TWTL constraint is satisfied in $341060$ episodes implying a success ratio $0.853 > Pr_{des} = 0.7$. The run time of the algorithm is $537.3 $ seconds. We show a sample trajectory after the first episode and $400000$ episodes in Fig.~\ref{fig:sim_benchmarks} (a) and (b), respectively. Note that the agent satisfies the constraint by delivering to $D_1$ in the first episode. However, the agent eventually learns that it can collect more rewards by delivering to $D_2$ instead of $D_1$.}

% \begin{table*}[t!]
%   \centering
%   \resizebox{\textwidth}{!}{
%   \begin{tabular}{|c |c |c |c |c |c |c |c |c |c |c|}%{|P{0.7cm}|P{0.9cm}|P{0.9cm}|P{0.9cm}|P{0.9cm}|P{0.9cm}|P{0.9cm}|P{0.9cm}|P{0.9cm}|P{0.9cm}|P{0.95cm}|}
%     \hline
%   $(\epsilon_{est}, Pr_{des})$& (0.1, 0.5) & (0.1, 0.6) & (0.1, 0.7) &  (0.15, 0.5) & (0.15, 0.6) & (0.15, 0.7) & (0.2, 0.5) & (0.2, 0.6) & (0.2, 0.7)  \\ \hline
%     Success Ratio [\%]& 54.1& 69.0& 83.9& 75.8& 90.2 &95.5 &93.6 &98.1 &99.0
%  \\ \hline
%     Avg. Rewards at the Last 5000 Episodes & 205.8
%  & 195.9 & 193.9& 175.4 &176.7 &155.5 &141.6 &144.1 &130.6
%  \\ \hline
%   \end{tabular}}
%   \caption{Simulation results for the task $[H^1 P]^{[0,20]} . ([H^1 D_1]^{[0,20]} \; \vee \; [H^1 D_2]^{[0,20]}) . [H^1 Base]^{[0,20]}$ and the real action uncertainty of $\epsilon_{real}=0.1$}\label{tab_all}
%   %\vspace{-8mm}
% \end{table*}

% In Case 2, we remove the TWTL constraint and demonstrate the performance when the agent follows the standard Q-learning. This case is simulated for $15 000$ episodes with a run time of $5$ seconds. As shown in Fig.~\ref{fig:sim_benchmarks}(c), the agent simply learns to quickly go to the highest reward zone.% as quickly as possible.

% In Case 3, we use the standard Q-learning to learn satisfying the desired TWTL task used in Case 1, without any consideration of the monitoring performance. To accomplish this, we assign rewards to the accepting states of the time-product MDP (the reward at any other state is zero). While the agent eventually learns to satisfy the TWTL constraint by following a shortest path as shown in Fig.~\ref{fig:sim_benchmarks}(d), there is no guarantee on the constraint satisfaction in the early stages of learning. In this case, the constraint is satisfied in only $143 312$ episodes, which is $35.8\% $ of the total number of episodes. The total run time for this case is $199.8$ seconds.

% In Case 4, we investigate how the parameters $\epsilon$ and $Pr_{des}$ influence the performance. To this end, the proposed algorithm is executed under varying values of $Pr_{des}=0.5, 0.6, 0.7$ and $\epsilon = 0.1, 0.15, 0.2$. In this case, $\epsilon>0.1$ indicates a conservative estimation of uncertainty. For example, for $\epsilon = 0.2$, the proposed algorithm assumes that the agent can move in its intended direction with a probability at least $0.8$ whereas such transitions actually occur with probability 0.9. The results for Case 4 are depicted in Table~\ref{tab_all}. For a fixed $Pr_{des}$, we observe that increasing $\epsilon$ reduces collected reward and increases the probability of constraint satisfaction. This is due to the fact that overestimating the uncertainty makes the algorithm overly cautious in exploration and more inclined to first satisfy the TWTL constraint. Moreover, for a fixed $\epsilon$, increasing $Pr_{des}$ reduces the collected reward since the constraint becomes more restrictive.    

% %\color{red}
% In Case 5, we consider the same structure of the TWTL task but with different time windows, i.e., we create three scenarios considering the TWTL specification $[H^1 P]^{[0,k]} . ([H^1 D_1]^{[0,k]} \; \vee \; [H^1 D_2]^{[0,k]}) . [H^1 Base]^{[0,k]}$ with $k=20, 30, 40$). In all scenarios, the product-MDPs have $154$ states since the structure of the relaxed automaton is independent from the time windows. However, the sizes of the time-product MDPs vary since the lengths of episodes are determined by the time windows ($62, 92, 122$ time steps). The time-product MDPs have $9548, 14168, 18788$ states for $k= 20, 30, 40$, respectively, and their offline construction took approximately $3.44, 8.65, 22.66$ seconds.  The algorithm parameters are selected as $\epsilon=0.15$ and $Pr_{des}=0.7$. The success ratios in these three scenarios are observed as $0.95, 0.97, 0.97$ which are all greater than $Pr_{des} = 0.85$. This difference between the success ratios and $Pr_{des}$ is mainly due to the overestimation of uncertainty, i.e., $\epsilon=0.15>0.1$. The average rewards in the last 5000 episodes are observed as $155.04, 361.99, 582.39$, respectively. As the time window of the TWTL task (hence the episode length) increases, the agent has more time in each episode to explore the environment so the collected reward increases.


\section{Conclusion}

We presented a constrained RL algorithm for keeping the probability of satisfying a bounded TL constraint above a desired threshold throughout learning. The proposed method is based on integrating the total automaton representation of the constraint into the underlying MDP and avoiding ``unsafe" actions based on some available prior information given as upper and lower bounds for each transition probability.  We theoretically showed that, under some conditions on the desired probability threshold and the MDP, the proposed approach ensures the desired probabilistic constraint satisfaction throughout learning. We also provided numerical results to demonstrate the performance of our proposed approach. As a future research, we plan to investigate the optimal implementation of the multi-shot algorithm; i.e., the optimal decomposition of the time and probability sets.  

%\textcolor{red}{It's worth noting that the proposed multi-shot algorithm does not directly find the optimal decomposition of the time-total product MDP. Instead, it primarily serves to establish a feasible decomposition criterion that ensures constraint satisfaction. As a future research direction, we plan to investigate the optimal decomposition, i.e., the optimal sets of time and desired probabilities of the subgraphs. We also recognize that the use of time-total product MDP may suffer from scalability issues as the problem size increases. Future research will also focus on improving the scalability of the algorithm by exploring alternatives to the time-total product MDP or utilizing more efficient state-of-the-art RL algorithms. }

%As a future direction, we plan to explore how similar guarantees on the probability of satisfaction of temporal logic constraints during learning can be achieved by multi-agent systems in a distributed manner (e.g., \cite{Yazicioglu17TCNS,Bhat19,Peterson20}), and 2) extend our methods to dynamic environments, where the constraint satisfaction requires reaching the accepting states on a time-varying graph (time-product MDP) (e.g., \cite{yaziciouglu2020}).


%To fulfill the safety constraints we  This can be achieved for any constraints set represented by a temporal logic language where the finite state automaton is feasible. Furthermore, by creating such a CMDP we were able to theoretically prove that the proposed algorithm satisfies the exact constraints specification. Moreover, it also learns to satisfy the constraints with a probability more than a desired threshold in each episode of learning, when no a-priori knowledge about sufficiently likely transitions in the system is given.


% This paper proposes a constrained reinforcement learning algorithm for maximizing the expected sum of rewards in a Markov Decision Process (MDP) while satisfying a bounded temporal logic constraint in each episode with a desired probability. We represent the bounded temporal logic constraint as a finite state automaton. We then construct a time-product MDP and formulate a constrained reinforcement learning problem. We derive a lower bound on the probability of satisfying the constraint from each state of the time-product MDP in the remaining episode time. This lower bound is computed by using some limited knowledge on which transitions are sufficiently likely in the system. The proposed approach uses this lower bound to keep the probability of constraint satisfaction above the desired threshold by restricting the actions that can be taken during learning. %The proposed approach is also demonstrated via simulations. 

% As a future direction, we plan to 1) explore how similar guarantees on the probability of satisfaction of temporal logic constraints during learning can be achieved by multi-agent systems in a distributed manner (e.g., \cite{Yazicioglu17TCNS,Bhat19,Peterson20}), and 2) extend our methods to dynamic environments, where the constraint satisfaction requires reaching the accepting states on a time-varying graph (time-product MDP) (e.g., \cite{yaziciouglu2020}).



\bibliographystyle{IEEEtran}
\bibliography{refer}

\appendix
\noindent \textit{Time-Window Temporal Logic \cite{twtl}:}
Let $AP$ be a set of atomic propositions, each of which has a truth value over the state-space. A TWTL formula is defined over the set $AP$ with the following syntax:
\vspace{2mm}

\centerline{$\phi ::= H^d x | H^d \notltl x | \phi_1 \andltl \phi_2 | \phi_1 \orltl \phi_2  | \notltl \phi_1 |  \phi_1 . \phi_2 | [\phi_1]^{[a,b]}$, where}
\begin{itemize}
\item $x$ is either the true constant $\top$ or an atomic proposition from $AP$;
\item $\andltl$, $\orltl$, and $\notltl$ are the conjunction, disjunction, and negation Boolean operators, respectively;
\item $\cdot$ is the concatenation operator;
\item $H^d$ with d $\in$ $\mathbb{Z}_{\geq 0}$ is the hold operator;
\item $[]^{[a,b]}$ with $0 \leq a \leq b$ is the within operator. 
\end{itemize}

\vspace{1mm}
The semantics of TWTL is defined according to the finite words $\mathbf{o}$ over $AP$, and $o(k)$ refers to the $k^{th}$ element on $\mathbf{o}$. For any $x \in AP$,  
the {\em hold} operator $H^d x$ indicates that $x$ should be true (serviced) for $d$ time units (i.e., $o \models H^d x$ if $o(t)=x \;\; \forall t\in [0,d]$).
The {\em within} operator $[\phi]^{[a, b]}$ means that the satisfaction of $\phi$ is bounded to the time window $[a, b]$ (i.e., $o \models [\phi]^{[a, b]}$ if $\exists k \in (0,b-a) \text{ s.t. } o^{\prime} \models \phi$ where $o^{\prime} = o(a+k)\dots o(b)$).
Finally, the concatenation of $\phi_i$ and $\phi_j$ (i.e., $\phi_i \cdot \phi_j$) designates that the first $\phi_i$ must be satisfied and then immediately after that
$\phi_j$ must be satisfied.
\end{document}


