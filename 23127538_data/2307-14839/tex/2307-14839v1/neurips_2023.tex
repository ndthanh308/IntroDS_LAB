\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023

\usepackage[square, numbers]{natbib}
\bibliographystyle{abbrvnat}

% ready for submission
% \usepackage{neurips_2023}



% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
\usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%\usepackage[nonatbib]{neurips_2023}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

\usepackage{graphicx}
\usepackage{svg}

\usepackage{mathrsfs}


\usepackage{hyperref}
\usepackage{url}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

\usepackage{amsthm}
\usepackage{bbold}
\usepackage{mathtools}
\usepackage{enumerate}
\usepackage{pgfplots}

\usepackage{nicefrac}
\usepackage{multirow}

\usepackage{mathrsfs}
\usepackage{wrapfig,booktabs}



\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}

\newcommand{\BB}{\mathcal{B}}
\newcommand{\EE}{\mathcal{E}}
\newcommand{\FF}{\mathcal{F}}
\newcommand{\GG}{\mathcal{G}}
\newcommand{\HH}{\mathcal{H}}
\newcommand{\JJ}{\mathcal{J}}
\newcommand{\LL}{\mathcal{L}}
\newcommand{\MM}{\mathcal{M}}
\newcommand{\NN}{\mathcal{N}}
\newcommand{\UU}{\mathcal{U}}
\newcommand{\OO}{\mathcal{O}}
\newcommand{\RR}{\mathcal{R}}
\newcommand{\VV}{\mathcal{V}}
\newcommand{\WW}{\mathcal{W}}
\newcommand{\XX}{\mathcal{X}}
\newcommand{\YY}{\mathcal{Y}}
\newcommand{\ZZ}{\mathcal{Z}}

\newcommand{\Rf}{\mathfrak{R}}

\newcommand{\Var}{\mbox{Var}}
\newcommand{\TF}{\mathcal{TF}}
\newcommand{\Cf}{\frac{1}{C}}
\newcommand{\Cff}{[-\Cf, \Cf]}
\newcommand{\hue}{h_{u_{\epsilon}}}
\newcommand{\MMD}{\mbox{MMD}}
\newcommand{\Cov}{\mbox{Cov}}
\newcommand{\avg}{\mbox{avg}}
\newcommand{\iid}{\stackrel{iid}{\sim}}
\newcommand{\1}{\mathbb{1}}
\newcommand{\pto}{\overset{p}{\to}}
\newcommand{\dto}{\overset{d}{\to}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}




\title{Kernelised Normalising Flows}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Eshant English%\thanks{Use footnote for providing further information
    %about author (webpage, alternative address)---\emph{not} for acknowledging
    %funding agencies.} \\
  \\Digital Health Machine Learning\\
  Hasso-Plattner-Institute, Germany
  % examples of more authors
  \And
  Matthias Kirchler
  \\Digital Health Machine Learning\\
  Hasso-Plattner-Institute, Germany
  \And
  Christoph Lippert \\
  Digital Health Machine Learning\\
  Hasso-Plattner-Institute, Germany
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}


\begin{document}


\maketitle


\begin{abstract}
  Normalising Flows are generative models characterised by their invertible architecture. However, the requirement of invertibility imposes constraints on their expressiveness, necessitating a large number of parameters and innovative architectural designs to achieve satisfactory outcomes. Whilst flow-based models predominantly rely on neural-network-based transformations for expressive designs, alternative transformation methods have received limited attention. In this work, we present Ferumal flow, a novel kernelised normalising flow paradigm that integrates kernels into the framework. Our results demonstrate that a kernelised flow can yield competitive or superior results compared to neural network-based flows whilst maintaining parameter efficiency. 
  Kernelised flows excel especially in the low-data regime, enabling flexible non-parametric density estimation in applications with sparse data availability. %Furthermore, we highlight the advantages of our method in low data regimes, where neural network-based architectures struggle to produce desirable outcomes.

\end{abstract}
\iffalse
Kind of extra observations
1) The kernel hyperparameters can have a huge impact, for example, with similar settings, I can get a training error as low as 3.8 but bad generalisation or 9.8 with good generalisation.
2) I contemplated the idea of combining multiple kernels(maybe as linear combinations), it gives more flexibility. Many works show it's better to use combinations of kernels,
3) Interestingly, if I am not learning any auxiliary points then it's better to fix them with some sample of training data. 
4) I wanted to relate the idea to efficient deep learning.
5) If you remember about the inductive biases thingy, since we are transforming to a gaussian distribution, the latter layers' auxiliary variables can be initialised with gaussian variables to provide more inductive biases?
6) Alex had questioned what inductive biases are provided.
7) maybe adding a couple of lines about RKHS in the background section
8) there was a failed experiment, where I shared weights in all the layers. It was naive and didn't learn well.
9) maybe a quick mathematical formula of rbf kernel?
10) The initialisation of auxiliary points is just like learning actnorm. I will try to elaborate on it more.
11) I guess I never mentioned that I am using ActNorm?
12) I wanted to comment on the training times, but the evaluation was not possible cos our model is run on Julia and all the others were in python.
13) btw I don't know if it would make sense but most models didn't converge. I had too many experiments to run so I didn't train for too long(no using GPUs is not always good).
14) you had mentioned adding a diagram, that's something I can not even attempt; don't have the required creativity for that.
15) Previously, I had mentioned that learning the auxiliary points using predictive outcome can help generalise it to flows for predictive setting/conditioning, or maybe applying the same kernel based transformation on conditioned input to get the auxiliary variables. I think it can be cool to write down in notational form.
16) I just realised that there is a cool related work using Kernel Transport operator and it's a generative model, I think it uses VAE to get some hiiden representation. Anyway, is it someting I should mention in the realted work section?
17) Prof Naumann had suggested that I also talk more about energy efficiency of the model especially when the model needs constant training in time-series/ autonomous driving. 

Well, that's all roughly what I can think of. If any of it makes sense to adapt, it would be good. Otherwise, it is what it is, sadly.

P.S. I asked Christoph to provide a review. He will most likely read it on the train. I would email him whatever version I have before sleeping.

edit: sent to Christoph, will ask his opinion soon and let you know.
\fi


\section{Introduction}

Maximum likelihood is a fundamental approach to parameter estimation in the field of machine learning and statistics. However, its direct application to deep generative modelling is rare due to the intractability of the likelihood function. Popular probabilistic generative models such as Diffusion Models \citep{Dai2020SlicedIN} and Variational Autoencoders \citep{kingma2022autoencoding} instead resort to optimising the Evidence Lower Bound (ELBO), a lower bound on the log-likelihood, due to the challenges in evaluating likelihoods.

The change of variables theorem offers a straightforward and elegant solution to compute the exact likelihood for deep generative models. These models, known as normalising flows, employ invertible architectures to transform complex probability distributions into simpler ones. Normalising flows excel in efficient density estimation and exact sampling, making them suitable for various applications.

Whilst flow-based models possess appealing properties rooted in invertibility, they also impose limitations on modelling choices, which can restrict their expressiveness. This limitation can be mitigated by employing deeper models with a higher number of parameters. For instance, the Glow model \cite{kingma2018glow} utilised approximately 45 million parameters for image generation on CIFAR-10 \cite{Krizhevsky09learningmultiple}, whereas StyleGAN3 \cite{karras2019stylebased}, a method that doesn't use likelihood optimisation, achieved a superior FID score with only about a million parameters. 

The issue of over-parameterisation in flow-based models hinders their effectiveness in domains with limited data, such as medical applications.
For example, normalising flows can be used to model complex phenotypic or genotypic data in genetic association studies~\citep{hansen2022normalizing,kirchler2022training}; collection of high-quality data in these settings is expensive, with many studies only including data on a few hundred to a few thousand instances.
In scenarios with low data availability, a flow-based network can easily memorise the entire dataset, leading to an unsatisfactory performance on the test set. While existing research has focused on enhancing the expressiveness of flows through clever architectural techniques, the challenge of achieving parameter efficiency has mostly been overlooked with few exceptions \cite{lee2020nanoflow}.
Most normalising flows developed to date rely on neural networks to transform complex distributions into simpler distributions.
However, there is no inherent requirement for flows to use neural networks.
Due to their over-parameterisation and low inductive bias, neural networks tend to struggle with generalisation in the low-data regime, making them inapplicable to many real-world applications.
%It should be noted that neural network-based generative models, including flows, are generally ineffective in low data regimes, especially when invertibility is a crucial constraint.

In this work, we propose a novel approach to flow-based distribution modelling by replacing neural networks with kernels.
Kernel machines work well in low-data regimes and retain expressiveness even at scale.
We introduce Ferumal flows, a kernelised normalising flow paradigm that outperforms or achieves competitive performance in density estimation for tabular data compared to other efficiently invertible neural network baselines like RealNVP and Glow.
Ferumal flows require up to 93\% fewer parameters than their neural network-based counterparts whilst still matching or outperforming them in terms of likelihood estimation.   
We also investigate efficient training strategies for larger-scale datasets and show that kernelising the flows works especially well on small datasets.


\section{Background}
\label{sec:background}

\subsection{Maximum likelihood optimisation with normalising flows}
%\paragraph{Maximum likelihood optimisation with normalising flows}
A normalising flow is an invertible neural network, $f: \R^D \to \R^D$ that maps real data $x$ onto noise variables $z$.
The noise variable $z$ is commonly modelled by a simple distribution with explicitly known density, such as a normal or uniform distribution, while the distribution of $x$ is unknown and needs to be estimated.
As normalising flows are maximum likelihood estimators, for a given data set of instances $x_1, \ldots, x_n$, we want to maximise the log-likelihood over model parameters,
\begin{align*}
    \max_f \sum_{i=1}^n \log\left(p_X(x_i)\right).
\end{align*}
With the change of variables formula,
\begin{align*}
    p_X(x) = p_Z\left(f(x)\right) \left\vert  \det J_f(x) \right\vert,
\end{align*}
where $J_f(x)$ is the Jacobian of $f$ in $x$, we can perform this optimisation directly:
\begin{align*}
        \max_f \sum_{i=1}^n \log\left(p_Z\left(f\left(x_i\right)\right)\right) + \log\left(\left|\det J_f\left(x_i\right)\right|\right).
\end{align*}
 The first part, $\log(p_Z(f(x_i)))$, can be computed in closed form due to the choice of $p_Z$.
 The second part,  $\log(|\det J_f(x_i)|)$, can only be computed efficiently if $f$ is designed appropriately.







%\paragraph{Coupling layers:}
\subsection{Coupling layers:}
One standard way to design such an invertible neural network $f$ with tractable Jacobian is affine coupling layers \citep{dinh2017density}.
A coupling layer $C_\ell: \R^D \to \R^D$ is an invertible layer that maps an input $y_{\ell-1}$ to an output $y_\ell$ ($\ell$ is the layer index):
first, permute data dimensions with a fixed permutation $\pi_\ell$, and split the output into the first $d$ and second $D-d$ dimensions: 
\begin{align*}
    \left[u_\ell^1,\,\, u_\ell^2\right] = \left[\pi_\ell(y_{\ell-1})_{1:d}, \,\, \pi_\ell(y_{\ell-1})_{d+1:D}\right].
\end{align*}
Commonly, the permutation is either a reversal of dimensions or picked uniformly at random.
Next, we apply two neural networks, $s_\ell, t_\ell: \R^d \to \R^{D-d}$, to the first output, $u_\ell^1$, and use it to scale and translate the second part, $u_\ell^2$:
\begin{align*}
    y_{\ell}^2 = \exp\left(s_\ell\left(u_\ell^1\right)\right) \odot u_\ell^2 + t_\ell\left(u_\ell^1\right).
\end{align*}
The first part of $y_{\ell}$ remains unchanged, i.e., $y_{\ell}^1 = u_\ell^1$, and we get the output of the coupling layer as:
\begin{align*}
    y_\ell = C_\ell\left(y_{\ell-1}\right) = \left[y_{\ell}^1, \,\, y_{\ell}^2\right] = \left[u_\ell^1, \, \, \exp\left(s_\ell\left(u_\ell^1\right)\right) \odot u_\ell^2 + t_\ell\left(u_\ell^1\right)\right].
\end{align*}


The Jacobian matrix of this transformation is a permutation of a lower triangular matrix resulting from $u_\ell^1$ undergoing an identity transformation and $u_\ell^2$ getting transformed elementwise by a function of $u_\ell^1$.
The Jacobian of the permutation has a determinant with an absolute value of 1 by default.
The diagonal of the remaining Jacobian consists of $d$ elements equal to unity and the other $D-d$ elements equal the scaling vector $s_\ell\left(u_\ell^1\right)$.
Thus, the determinant can be efficiently computed as the product of the elements of the scaling vector $s_\ell\left(u_\ell^1\right)$. 

The coupling layers are also efficiently invertible as only some of the dimensions are transformed and the unchanged dimensions can be used to obtain the scaling and translation factors used for the forward transformation to reverse the operation.

Multiple coupling layers can be linked in a chain of $L$ layers such that all dimensions can be transformed:
\begin{align*}
    f(x) = C_L \circ \dots \circ C_1(x), 
\end{align*}
i.e., $y_0 = x$ and $y_L = f(x)$.


\subsection{Kernel machines}
%\paragraph{Kernel machines}
Kernel machines \citep{scholkopf2002learning} implicitly map a data vector $u \in \R^p$ into a high-dimensional (potentially infinite-dimensional) reproducing kernel Hilbert space (RKHS), $\HH$, by means of a fixed feature map $\phi: \R^p \to \HH$.
The RKHS $\HH$ has an associated positive definite kernel $k(u, v) = \left\langle \phi(u), \phi(v) \right\rangle_\HH$, where $\langle \cdot, \cdot \rangle_\HH : \HH \times \HH \to \R$ is the inner product of $\HH$.
The kernel $k$ can oftentimes be computed in closed form without requiring the explicit mapping of $u, v$ into $\HH$, making computation of many otherwise intractable problems feasible.
In particular, as many linear learning  algorithms, such as Ridge Regression or Support Vector Machines, only require explicit computations of norms and inner products, these algorithms can be efficiently kernelised.
Instead of solving the original learning problem in $\R^p$, kernel machines map data into the RKHS and solve the problem with a linear algorithm in the RKHS, offering both computational efficiency (due to linearity and kernelisation) and expressivity (due to the nonlinearity of the feature map and the high dimensionality of the RKHS).


\section{Ferumal flows: kernelising flow-based architectures}

In this section, we extend standard coupling layers to use kernel-based scaling and translation functions instead.
Whilst neural networks are known to perform well in large-data regimes or when transfer-learning from larger datasets can be applied, kernel machines perform well even at small sample sizes and naturally trade-off model complexity against dataset size without losing expressivity.

%\paragraph{Kernelised coupling layers}
\subsection{Kernelised coupling layers}

We keep the definition of coupling layers in Section~\ref{sec:background}, and only replace the functions $s_\ell$ and $t_\ell$ by functions mapping to and from an RKHS $\HH$.
We have to deal with two main differences to the kernelisation of many other learning algorithms:
firstly, the explicit likelihood optimisation does not include a regularisation term that penalises the norm of the prediction function.
And secondly, instead of a single mapping from origin space to RKHS to single-dimensional output, we aim to combine multiple layers, in each of which the scaling and translation map from origin space to RKHS to multi-dimensional output.
As a result, the optimisation problem will not be convex in contrast to standard kernel learning, and we have to derive a kernelised and tractable representation of the learning objective.

In particular, in layer $\ell$, we introduce RKHS elements $V_{\ell}^s, V_{\ell}^t \in \HH^{D-d}$ and define scaling and translation as
\begin{align*}
    s_\ell\left(u_\ell^1\right) = \left[\left\langle V_{\ell,j}^s, \phi\left(u_\ell^1\right) \right\rangle_\HH\right]_{j=1}^{D-d} \in \R^{D-d} \quad\quad \mbox{ and } \quad\quad t_\ell\left(u_\ell^1\right) = \left[\left\langle V_{\ell,j}^t, \phi\left(u_\ell^1\right) \right\rangle_\HH\right]_{j=1}^{D-d} \in \R^{D-d}.
\end{align*}
We summarise $V_\ell = \left[V_{\ell}^s, V_{\ell}^t\right]$ and $V = \left[V_1, \ldots, V_L\right]$ for the full flow $f(x) = C_L \circ \dots \circ C_1 (x)$.
Since elements $V \in \HH^{2L(D-d)}$ and $\HH$'s dimensionality is potentially infinite, we cannot directly optimise the objective:
\begin{align}
    \label{kernel-obj}
    \max_{V} & \sum_{i=1}^n p_Z\left(C_L \circ \ldots \circ C_1 (x_i)\right) + \log\left(\left|\det J_{C_L \circ \ldots \circ C_1}(x_i)\right|\right) = L(V).
\end{align}
However, we can state a version of the representer theorem \citep{scholkopf2001generalized} that allows us to kernelise the objective:
\begin{proposition}
    Given the objective $L$ in equation~\eqref{kernel-obj}, for any $V'=\left[V_{1}', \ldots, V_L'\right]  \in \HH^{L(D-d)}$ there also exists a $V$ with $L(V) =L\left(V'\right)$ such that
    \begin{align*}
        V_{\ell} = \sum_{i=1}^n k\left(\cdot, u_{\ell,i}^1\right)  A_{\ell, i}
    \end{align*}
    for some $A_{\ell,i} = \left[A_{\ell, i}^s, A_{\ell, i}^t\right] \in \R^{2(D-d)}$.
    Here, $u_{\ell,i}^1 = \pi_\ell(C_{\ell-1} \circ \dots \circ C_{1}(x_i))_{1:d}$, i.e., the first part of the permutated input to layer $\ell$ for data point $i$.
    In particular, if there exists a solution $V'\in \argmax_V L(V)$, then there's also a solution $V^*$ of the form 
    \begin{align*}
        V_{\ell}^* = \sum_{i=1}^n k\left(\cdot, u_{\ell,i}^1\right)  A_{\ell, i}.
    \end{align*}
\end{proposition}
\begin{proof}
    Let $V' \in \HH^{2L(D-d)}$ and $\Phi_\ell = \text{span}\{\phi(u_{\ell,1}^1), \ldots, \phi(u_{\ell, n}^1) \}$ be the space spanned by the feature maps of layer $\ell$ inputs, and let $\Phi_\ell^\bot$ denote its orthogonal complement in $\HH$.
    We can then represent each element ${V_{\ell,j}^s}'\in \HH$ ($j \in \{1, \ldots, D-d\}$) as an orthogonal sum of an element of $\Phi_\ell$ and $\Phi_\ell^\bot$,
    \begin{align*}
        {V_{\ell,j}^s}' = \phi_{\ell,j} + \phi_{\ell,j}^\bot, \mbox{ with } \phi_{\ell, j} = \sum_{i=1}^n A_{\ell,i,j}^s \phi(u_{\ell,i}^1) \mbox{ and } \langle \phi_{\ell,j}^\bot, \phi(u_{\ell,i}^1) \rangle_\HH = 0 \,\, \forall i=1, \ldots, n
    \end{align*}
    for some values $A_{\ell, i, j}^s \in \R$.
    In the objective~\eqref{kernel-obj}, we only use ${V_{\ell,j}^s}'$ to compute $\langle {V_{\ell,j}^s}', \phi(u_{\ell,i}^1) \rangle_\HH$ as part of the computation of $s_\ell(u_{\ell,i}^1)$.
    But due to orthogonality, it holds that
    \begin{align*}
        \langle {V_{\ell,j}^s}', \phi(u_{\ell,i}^1) \rangle_\HH = \langle \phi_{\ell,j} + \phi_{\ell,j}^\bot, \phi(u_{\ell,i}^1) \rangle_\HH =\langle \phi_{\ell,j}, \phi(u_{\ell,i}^1) \rangle_\HH + \langle  \phi_{\ell,j}^\bot, \phi(u_{\ell,i}^1) \rangle_\HH = \langle \phi_{\ell,j}, \phi(u_{\ell,i}^1) \rangle_\HH.
    \end{align*}
    Hence, replacing ${V_{\ell,j}^s}'$ by $\phi_{\ell,j} = \sum_{i=1}^n A_{l,i,j}^s \phi(u_{\ell, i}^1)$ keeps the objective unchanged.
    The reproducing property of the RKHS $\HH$ now states that $\langle \phi(u_{\ell,i}^1), \phi(\cdot) \rangle_\HH = k(u_{\ell, i}^1, \cdot)$.

    Repeating this for all $\ell = 1, \ldots, L$, all $j = 1, \ldots, D-d$ and also for translations $t_\ell$ yields a version of $V'$ that can be represented as a linear combination of the stated form.
\end{proof}


Note that, in contrast to the classical representer theorem, since the objective doesn't contain a strictly convex regularisation term that penalises the model complexity, the solution is not necessarily unique.
While there may be other solutions, we know that there exists a solution that can be expressed as a linear combination of kernel evaluations, so we can re-insert this solution $V^*$ into the objective~\ref{kernel-obj}.
For layer $\ell$ and arbitrary $a \in \R^d$,
\begin{align*}
s_\ell(a) = \left[\langle V_{\ell, j}^{s*}, \phi(a)\right\rangle]_{j=1}^{D-d} = \left[\sum_{i=1}^n A_{\ell,i,j}^s k\left(u_{\ell,i}^1, a\right) \right]_{j=1}^{D-d} = \sum_{i=1}^n  k\left(u_{\ell,i}^1, a\right) A_{\ell, i}^s.
\end{align*}
As in the objective~\eqref{kernel-obj}, $s_\ell$ gets only evaluated in points $a \in \left\{ u_{\ell,i}^1 | i = 1, \ldots, n\right\}$, this simplifies to
\begin{align*}
    s_\ell\left(u_{\ell,m}^1\right) = \sum_{i=1}^n k\left(u_{\ell,i}^1, u_{\ell, m}^1\right) A_{\ell,i}^s =  A_{\ell}^s K\left(U_\ell^1, U_\ell^1\right)_m,
\end{align*}
where $K\left(U_\ell^1, U_\ell^1\right) = \left[k\left(u_{\ell, i}^1, u_{\ell, m}\right)^1\right]_{i,m=1}^n$ is the kernel matrix at layer $\ell$ and $A_\ell^s = \left[A_{\ell, 1}^s, \ldots, A_{\ell, n}^s\right] \in \R^{(D-d) \times n}$ is the weight matrix.
A similar derivation holds for $t_\ell$.

In total, we can kernelise the objective~\eqref{kernel-obj} and optimise over parameters $A \in \R^{L \times n \times 2(D-d)}$ instead of over $V \in \HH^{2L(D-d)}$. 
In contrast to neural network-based learning, the number of parameters, $2Ln(D-d)$, is fixed except for the number of layers (since $d$ is usually set to $\floor{D/2}$), but increases linearly with the dataset size, $n$.
This makes kernelised flows especially promising for learning in the low-data regime, as their model complexity naturally scales with dataset size and does not over-parametrise as much as neural networks.

Since the resulting objective function is not convex, optimisers targeted to standard kernel machines such as Sequential Minimal Optimisation~\citep{platt1998sequential} are not applicable.
Instead, we optimise~\eqref{kernel-obj} with variations of stochastic gradient descent~\citep{kingma2014adam}.






\subsection{Efficient learning with auxiliary points}
%\paragraph{Efficient learning with auxiliary points}
The basic kernelised formulation can be very computationally expensive even at moderate dataset sizes and can tend towards overfitting in the lower-data regime.
In Gaussian Process (GP) regression, this problem is usually addressed via sparse GPs and the introduction of \emph{inducing variables}~\citep{JMLR:v6:quinonero-candela05a}.
In a similar spirit, we introduce \emph{auxiliary points}.
In layer $\ell$, instead of computing the kernel with respect to the full data $u_{\ell,1}^1, \ldots, u_{\ell,n}^1$, we take $N \ll n$ new variables $W_\ell^1 = [w_{\ell, 1}^1, \ldots, w_{\ell,N}^1] \in \R^{d \times N}$ and compute the scaling transform as
\begin{align*}
    \hat{s}_{\ell}(u_{\ell, m}^1) = \hat{A}_\ell^sK(U_\ell^1, W_\ell^1)_m
\end{align*}
with $\hat{A}_\ell^s \in \R^{(D-d)\times N}$ (analogously for $\hat{t}_\ell$).
We make these auxiliary points learnable and initialise them to a randomly selected subset of $u_{\ell, 1}^1, \ldots, u_{\ell, n}^1$.
This procedure reduces the learnable parameters from $2n(D-d)L$ (for both $s_\ell$ and $t_\ell$) to $2NDL$.

In another variation we make these auxiliary points shared between layers.
In particular, instead of selecting $L$ times $N$ points $w_{\ell,1}^1, \ldots, w_{\ell,N}^1$, we instead only select $W^1 = [w_1^1, \ldots, w_N^1] \in \R^{d \times N}$ once and compute at layer $\ell$
\begin{align*}
        \bar{s}_{\ell}(u_{\ell, m}^1) = \hat{A}_\ell^sK(U_\ell^1, W^1)_m.
\end{align*}
This further reduces the learnable parameters to $2N(D-d)L + 2dN$.





\section{Related works}

We are unaware of any prior work that attempts to directly replace neural networks with kernels in flow-based architectures.
However, there is a family of flow models based on Iterative Gaussianisation (IG) \cite{chen2000gaussianization} that utilise kernels. Notable works using Iterative Gaussianisation include Gaussianisation Flows \cite{meng2020gaussianization} and Rotation-Based Iterative Gaussianisation (RBIG) \cite{Laparra_2011}.
These IG-based methods differ significantly from our methodology. They rely on kernel density estimation and inversion of the cumulative distribution function for each dimension individually and incorporate the dependence between input dimensions through a rotation matrix, which aims to reduce interdependence. In contrast, our method integrates kernels into coupling layer-based architectures. Furthermore, IG-based methods typically involve a large number of layers, resulting in inefficiency during training and a comparable number of parameters to neural network-based flow architectures. In contrast, the Ferumal flow approach of incorporating kernels can act as a drop-in replacement in many standard flow-based architectures, ensuring parameter efficiency without compromising effectiveness.
Another generative model using kernels is the work on kernel transport operators \citep{huang2021forward}.
\citet{huang2021forward} demonstrated promising results in low-data scenarios and favourable empirical outcomes. However, their approach differs from ours as they employed kernel mean embeddings and transfer operators, along with a pre-trained autoencoder.

Other works focusing on kernel machines in a deep learning context are deep Gaussian processes \cite{damianou2013deep} and deep kernel learning \citep{wilson2016deep,wenliang2019learning}.
Deep GPs concatenate multiple layers of kernelised GP operations; however, they are Bayesian, non-invertible models for prediction tasks instead of density estimation and involve high computational complexity due to operations that require inverting a kernel matrix.
Deep kernel learning, on the other hand, designs new kernels that are parametrised by multilayer perceptrons.


\citet{maroÃ±as2021transforming} integrated normalising flows within Gaussian processes. Their approach differs significantly from ours as they aimed to exploit the invertibility property of flows by applying them to the prior or the likelihood. Their combined models consist of kernels in the form of GPs but also involve neural networks in the normalising flows network, resembling more of a hybrid model.

NanoFlow \cite{lee2020nanoflow} also targets parameter efficiency in normalising flows.
They rely on parameter sharing across different layers, whereas we utilise kernels. We also attempted to implement the naive parameter-sharing technique suggested by \citet{lee2020nanoflow}, but we found no improvement in performance. %it resulted in a decline in performance.



\section{Experiments}

We assess the performance of our Ferumal flow paradigm both on synthetic 2D toy datasets and on five real-world benchmark datasets sourced from \citet{Dua:2019}. The benchmark datasets include Power, Gas, Hepmass, MiniBoone, and BSDS300. To ensure consistency, we adhere to the preprocessing procedure outlined by \citet{papamakarios2018masked}.

\paragraph{Implementation details}
As previously mentioned, our architecture closely resembles RealNVP, which is a fundamental flow-based architecture. We have chosen to refrain from incorporating architectural innovations such as $1\times1$ convolutions \cite{kingma2018glow}, which could potentially enhance the effectiveness of our architecture in creating optimal splits. 
% We made a deliberate decision to not include architectural innovations like $1\times1$ convolutions \cite{kingma2018glow}, which have the potential to enhance the effectiveness of our architecture in achieving optimal splits. 
% This decision was made to showcase the effectiveness of kernelisation in its simplest form.

In our setting, we employ a specific permutation scheme to manipulate the data dimensions. In the first layer, we apply a fixed random permutation, followed by a reversal permutation of the data dimensions in the next layer. This scheme guarantees that all data dimensions undergo a transformation in the second layer, even when the number of layers is limited. This is advantageous compared to the scenario where a fixed random permutation is applied, which may leave some data dimensions unchanged. We refer to these two layers as a ``block,'' and before each block, we apply ActNorm \cite{kingma2018glow} to facilitate faster convergence.

For comparison purposes, we use RealNVP and Glow as  baseline models. We have also included RBIG and basic autoregressive methods, MADE \cite{papamakarios2018masked} and MAF \cite{papamakarios2018masked}, in our evaluations (results taken from existing literature). Most autoregressive flow models outperform non-autoregressive flow models. However, they usually come with the trade-off of either inefficient sampling or inefficient density estimation, i.e., either the forward or the inverse computation is computationally very expensive.

%Throughout our experiments, we have consistently employed the Square-Exponential kernel, opting not to introduce additional hyperparameters. Nevertheless, it is worth noting that the use of alternative kernels may enhance the expressiveness of the network. It is essential to clarify that our objective is not to assert that this simple architecture is state-of-the-art, but rather to demonstrate that kernelising flow-based architectures can yield high expressiveness while maintaining low parameter complexity. Additionally, it can help bridge the performance gap between coupling layer-based flow architectures and autoregressive flow architectures if employed with much more powerful architectures than ours.
% Figure environment removed

\iffalse
We aim to answer the following questions:
\begin{enumerate}
    \item Does Ferumal Flow have better initialisation than other flow-based architectures?
    \item Does Ferumal Flow perform better than other neural-net-based architectures in terms of density estimation?
    \item Is Ferumal Flow capable of data generation in low-data regimes?
    \item Is Ferumal Flow parameter-efficient whilst still retaining expressiveness?
\end{enumerate}
\fi


\paragraph{Training details} In our study, we utilised the Squared Exponential kernel, $k(x, y) = \exp(-\gamma ||x-y||^2)$, exclusively for all experiments, with hyperparameter $\gamma > 0$.
%This omission does not affect the overall optimisation objective and can be succinctly summarised by the length scale alone, unlike in Gaussian Processes.
To select the kernel length scale, we performed a hyperparameter search; we initially defined a broader grid and conducted a basic hyperparameter search to acquire a more refined grid. Subsequently, we performed a random hyperparameter search within this finer grid. Throughout the experiments, we exclusively employed the Adam \citep{kingma2014adam} optimiser, whilst adjusting the $\beta_1$ and $\beta_2$ parameters of the optimiser. Additionally, we decayed the learning rate either with predefined steps (StepLR) or with cosine annealing.
In all the experiments, we incorporated auxiliary points as we observed that they provided better results. 

We implemented Ferumal flows with the Flux library \citep{Flux.jl-2018} in Julia, whereas we used existing implementations in Python's PyTorch \citep{paszke2019pytorch} for the baseline algorithms.
We ran all experiments for Ferumal flows on CPUs, but all other non-kernelised experiments were trained on NVIDIA A100 GPUs.
It is worth noting that we did not observe a substantial difference in runtimes between our model on CPU (Intel Xeon 3.7 GHz) and other baselines. Nevertheless, certain methods such as RBIG exhibited much longer runtimes even on an A100 GPU.
%However, due to the disparate programming languages employed, a fair comparison cannot be made.
For more comprehensive training details, please refer to the appendix.

\subsection{2D toy datasets}

%\begin{table}
\begin{wraptable}{r}{0.59\linewidth}
  \caption{Results on toy datasets. NLL in nats, lower is better}
  \label{toy datasets}
  \centering
  \begin{tabular}{lll}
    \toprule
%    \multicolumn{3}{c}{On toy datasets}                   \\
    %\cmidrule(r){1-3}
    Dataset     & Ours (\#params) & Glow (\#params) \\ 
    \midrule
    Line   & \textbf{-3.75} (\textbf{5K}) & -3.15  (44K)\\
    Pinwheel   & \textbf{2.44 } (\textbf{4K}) & 2.48 (44K)\\
    Moons     &\textbf{2.43 } (\textbf{5K}) & 2.54 (44K)\\
    \bottomrule
  \end{tabular}
\end{wraptable}
%\end{table}
Initially, we conducted density estimation experiments on three synthetic datasets that were sampled from two-dimensional distributions exhibiting diverse shapes and numbers of modes.
%Our aim was to transform these intricate two-dimensional distributions into isotropic Gaussians.
Figure~\ref{modelling} showcases the original data distribution alongside the samples generated using the Ferumal flow paradigm and the Glow model.
The results demonstrate that Ferumal flows can outperform Glow on these toy datasets.
All the toy datasets were trained with a batch size of 200 and for 10K iterations.






\subsection{Real-world datasets}
We conducted density estimation experiments on five UCI tabular benchmark datasets, employing the preprocessing method proposed by \citet{papamakarios2018masked}. In our comparisons, we included two baseline models, RealNVP and Glow, which share a similar architecture with coupling layers for efficient sampling and inference. Additionally, we also considered comparisons with RBIG, MAF, and MADE, architectures that don't use coupling layers.
These methods are not directly comparable to the coupling layer-based methods, as they require significantly higher computational costs.
In particular, RBIG, requires much more layers (up to 100, in our settings), whilst autoregressive flows are slow to sample from, due to their autoregressive nature.

Table~\ref{tab:nats} presents the results of our experiments, revealing that Ferumal flows consistently achieve better or competitive outcomes across all five datasets.
Despite its straightforward coupling layer architecture, our approach surpasses both RBIG and MADE, and achieves similar performance than the much more expensive MAF method, underscoring the efficacy of integrating kernels. 
% The superior performance of our method reinforces the advantages of incorporating kernels within the flow-based framework.

\begin{table}
  \caption{Negative log-likelihood measured in nats. Smaller values are better. Results for autoregressive methods taken from the respective publications (denoted by *). }
  \label{tab:nats}
  \centering
  \begin{tabular}{lllllll}
    \toprule
    \multicolumn{7}{c}{Datasets}                   \\
    \cmidrule(r){3-7}
    & Method     & Power     & Gas & Hepmass & Miniboone & BSDS300 \\
    \midrule
   Coupling & RealNVP & -0.17  & -8.33 & 18.71 & 13.55 & -153.28   \\
   &  Glow     & -0.17  & -8.15 & 18.92 & 11.35 & -155.07  \\
   & Ferumal Flow & \textbf{-0.23} & \textbf{-9.51}  & \textbf{18.34} & \textbf{11.25}  &  \textbf{-157.01}  \\
    %\cmidrule(r){1-7}
    \midrule
   Non-coupling & MADE*     & 3.08  & -3.56 & 20.98 & 15.59 & -148.85    \\
  &  RBIG     & 1.02  & 0.05 & 24.59 & 25.41 & -115.96    \\
   & MAF*     & -0.24  & -10.08 & 17.70 & 11.75 & -155.69    \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Initial performance}
Figure~\ref{fig:nats} presents the learning curves of the train and test loss for Ferumal flows and the two baselines.
These findings demonstrate that the Ferumal flow-based architecture exhibits faster convergence compared to the neural network baselines.
This expedited convergence may be due to the data-dependent initialisation of the auxiliary variables.
Throughout our experiments, we maintained default settings and ensured consistent batch sizes across all models. 

%% Figure environment removed
% Figure environment removed




\subsection{Low-data regime}
In certain applications, such as medical settings, data availability is often limited. Neural network-based flows typically suffer from parameter inefficiency, leading to challenges in generalisation within low-data regimes. To assess the generalisation capability of our model under such conditions, we trained our model using only 500 examples and evaluated its performance on the same benchmark datasets.
To address the challenges of limited data, we opted to tie the learned auxiliary variables across layers in this setting. This approach helped mitigate parameter complexity whilst maintaining the benefits of utilising auxiliary points.

As highlighted by \citet{meng2020gaussianization}, Glow and RealNVP struggled to generalise in low-data regimes, evidenced by increasing validation and test losses whilst the training losses decreased.
To provide a stronger benchmark, we included the FFJORD model \citep{grathwohl2018ffjord}. FFJORD is a continuous normalising flow method with a full-form Jacobian and exhibits superior performance to Glow or RealNVP in density estimation and generation tasks.

Table~\ref{low data regimes} presents the results, demonstrating that our method achieves superior generalisation. This may be attributed to the significantly lower number of parameters required compared to the continuous FFJORD method.
%It is important to note that our approach showcases enhanced generalisation performance while employing substantially fewer parameters than FFJORD (See Appendix for details).


\begin{table}
\parbox{.54\linewidth}{

  \caption{Results on small subset of 500 examples. NLL in nats, lower the better}
  \label{low data regimes}
  \centering
  \begin{tabular}{lll}
    \toprule
%    \multicolumn{3}{c}{On a subset of 500 examples}                   \\
 %   \cmidrule(r){1-3}
    Dataset     & Ours (\#params)  & FFJORD (\#params) \\ % & FFJORD(params) \\ 
    \midrule
    Miniboone   & \textbf{27.75} (\textbf{58K}) & 39.92  (821K)\\
    Hepmass     & \textbf{27.90 }(\textbf{41K}) &28.17  (197K)\\
    Gas     &\textbf{-0.22 } (\textbf{11K}) & 7.50  (279K)\\
    Power     & \textbf{2.91} (\textbf{8K})& 11.33 (43K)\\
    BSDS300     &  \textbf{-121.22} (\textbf{85K}) &-100.32 (3,127K) \\
    \bottomrule
  \end{tabular}
%\end{table}
}
\hfill
\parbox{.49\linewidth}{

%\begin{table}
  \caption{Number of parameters}
  \label{Parameter count}
   \centering
  \begin{tabular}{llll}
    \toprule
    \multicolumn{4}{c}{Architectures}                   \\
    \cmidrule(r){2-4}
    Dataset     & RealNVP     & Glow & Ours  \\%reduction \\
    \midrule
    Miniboone & 377K  & 395K & \textbf{117K} \\%& \textbf{70\%}  \\
    Hepmass   & 288K & 293K & \textbf{76K} \\%& \textbf{75\%}\\
    Gas         & 236K & 237K & \textbf{22K} \\%& \textbf{90\% } \\
    Power    & 228K & 228K &  \textbf{16K } \\%& \textbf{93 \%} \\
    BSDS300   & 458K & 497K & \textbf{171K } \\%& \textbf{65 \%} \\
    \bottomrule
  \end{tabular}
 }
\end{table}

\subsection{Parameter efficiency}
%We conducted a comprehensive comparison of parameter counts between Ferumal Flows and the baselines, namely Glow and RealNVP. The results, as shown in 
Table~\ref{Parameter count} shows the parameter counts of Ferumal flows against the baseline methods.
Kernelising the models results in a parameter reduction of up to 93\%.
This reduction can be further improved by implementing strategies such as sharing auxiliary variables between layers or potentially with low-rank approximations, particularly in scenarios where data is limited and concerns about overfitting arise (see Appendix for additional details).

\section{Conclusion}

We have introduced Ferumal flows, a novel approach to integrate kernels into flow-based generative models.
Our study highlighted that Ferumal flows exhibit faster convergence rates, thanks to the inductive biases imparted by data-dependent initialisation. Moreover, we have demonstrated that kernels can significantly reduce the parameter count without compromising the expressive power of the density estimators.
Especially in the low-data regime, our method shows  superior generalisation capabilities, while Glow and RealNVP fail entirely, and FFJORD lags significantly in performance.


%\subsection{Future Work}


% In our pursuit of further parameter reduction, we are interested in exploring Support Vector Machines (SVMs)\cite{young1988introduction} and their potential to transform the optimisation objective into a convex one. By investigating ResFlow-like architectures, we aim to establish an analogy between them and multiple-output residual SVMs.
% The theoretical analysis of our current work and its interpretation in terms of SVMs are deferred to future investigations.

\subsection{Limitations and future work}
%\paragraph{Limitations and future work}

In contrast to neural-network-based flows, kernelised flows require a different hyperparameter selection.
In classical kernel machines, the choice of kernel usually implies a type of inductive bias (e.g., for specific data types \citep{vishwanathan2010graph}).
This is not the case for our deep kernelised flows.
Consequently, in this work, we focus on Squared Exponential kernels, but incorporating kernels with strong inductive biases may be a promising avenue for future research.
Ferumal flows also come with other additional hyperparameters, such as the length scale of the Squared Exponential kernel or the number of inducing points.
This does not increase the hyperparameter search space, though, as other hyperparameters specific to neural networks fall away (such as the number of neurons per layer). 

One drawback of vanilla Ferumal flows is that their parameter count grows linearly with the number of data points, making them all but infeasible for higher data regimes.
Similarly, the computational cost is quadratic in the number of data points.
We provided one possible solution for this, using a limited number of auxiliary points for approximate kernel computations instead.

%The only limitation of our work lies in the fact that incorporating kernels can augment the hyperparameter space, necessitating the addition of at least the length scale of a kernel as a hyperparameter for each layer (varying types of kernels may introduce additional hyperparameters). However, we would like to highlight that in our experiments, this did not result in increased optimisation complexity. In fact, we posit that the kernel hyperparameters aided in identifying superior transformations for the split. Unlike neural network-based architectures, which heavily rely on weights to empower transformations for the other split, our approach offers greater flexibility by the addition of kernel length scale.

The present work introduces kernels only in basic affine coupling layers as introduced with RealNVPs.
However, the concepts also directly apply to other coupling-layer-type networks, such as neural spline flows \citep{durkan2019neural}, ButterflyFlows \citep{pmlr-v162-meng22a}, or invertible attention \citep{sukthanker2022generative} for greater expressiveness and parameter efficiency.
Ferumal flows can also be directly enhanced with other building blocks such as $1\times 1$ convolutions instead of permutations \citep{kingma2018glow} or MixLogCDF-coupling layers \citep{ho2019flow++}.
% It is worth mentioning that incorporating advances in selecting dimensions for coupling layers, such as 1x1 convolutions or Butterfly matrices\cite{pmlr-v162-meng22a}, can further enhance the architecture.
% We would like to emphasise that Ferumal Flow is not an architecture; it represents a paradigm for seamlessly integrating kernels into flows.
% While we have showcased its capabilities in a basic coupling layer architecture, the concept can be readily extended to highly expressive flows like Generative Flows with Invertible Attentions\cite{sukthanker2022generative}, FFJORD, and Neural Splines Flow\cite{durkan2019neural}, enabling greater expressiveness and reducing parameter complexity.

Whilst our method can be applied to coupling-type flow-based architectures, it poses challenges when it comes to ResFlow-like architectures \cite{behrmann2019invertible,chen2000gaussianization}, which require explicit control of Lipschitz properties of the residual blocks. As a result, extending our approach to ResFlow-like architectures is left as a direction for future research.

\subsection{Broader impact}
%\paragraph{Broader impact}
%The primary objective of this study was to enhance the prominence of flow-based models as generative models for real-world applications that face limitations in data requirements, parameter complexity and reduced expressiveness due to invertibility.
%Certain issues, such as data requirements and parameter complexity, are inherent to neural networks and become more pronounced when applied to flows.
%Given the proven efficacy of kernel-based methods in low data regimes, we sought ways to integrate them into flow-based architectures.
%It is our hope that this integration will encourage further exploration of kernels and their adaptation for diverse applications.
One major drawback of existing normalising flow algorithms is their dependence on an abundance of training data.
The introduction of kernels into these models may allow the application of flows in low-data settings.  
Additionally, in the era of increasingly large and complex models, energy consumption has become a significant concern. The reduction in parameters can contribute to energy savings. Notably, our models, owing to their low parameter complexity, were successfully trained without the need for a GPU. We anticipate that future research will continue to explore efficient methodologies and strive for reduced energy demands.




\begin{ack}
We extend our gratitude to Noel Danz for his valuable discussions on coding in Julia. We would also like to express our appreciation Arkadiusz Kwasigroch, and Alexander Rakowski for their initial feedback on the draft. Additionally, we acknowledge the support provided by HPI DSE research school.

\iffalse
Use unnumbered first level headings for the acknowledgments. All acknowledgments
go at the end of the paper before the list of references. Moreover, you are required to declare
funding (financial activities supporting the submitted work) and competing interests (related financial activities outside the submitted work).
More information about this disclosure can be found at: \url{https://neurips.cc/Conferences/2023/PaperInformation/FundingDisclosure}.


Do {\bf not} include this section in the anonymized submission, only in the final paper. You can use the \texttt{ack} environment provided in the style file to automatically hide this section in the anonymized submission.
\fi
\end{ack}





%\section*{References}

\medskip
\bibliography{neurips_2023}






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\iffalse
\appendix

\paragraph{Low-rank approximations}
For datasets characterised by high dimensionality and complex structures, such as images, relying solely on auxiliary points for the weight matrix ($W_{p \ N}$) is inadequate. When half of the dimensions are employed (as is the case in any coupling layer), this matrix becomes excessively large, a problem that magnifies when generating high-resolution images. It is worth highlighting that flow-based models encounter inefficiencies when producing high-resolution images.

To preserve the desirable quality of parameter reduction in our models, we propose an alternative approach to obtaining the weight matrix for a kernelised layer. We suggest using the product of two smaller matrices (with fewer parameters) instead. For a weight matrix $W_{p * N}$, responsible for producing $p$ affine parameters using N auxiliary points, we can learn two smaller matrices: $W^1_{c * N}$ and $W^2_{c * N}$, where $c < p$. By employing the product of these two matrices as a proxy for a full-weight matrix, we can effectively reduce the number of parameters. In image datasets, neighbouring pixels often exhibit high correlation, making parameter-sharing techniques like convolutions more effective than fully connected neural networks. Consequently, this low-rank approximation can be regarded as a form of parameter sharing specifically tailored for image datasets.
\fi


\end{document}