
\documentclass{article} % For LaTeX2e
\usepackage{iclr2024_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
%\input{math_commands.tex}
%\usepackage[square, numbers]{natbib}
\usepackage{hyperref}
\usepackage{url}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{svg}

\usepackage{mathrsfs}


\usepackage{hyperref}
\usepackage{url}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

\usepackage{amsthm}
\usepackage{bbold}
\usepackage{mathtools}
\usepackage{enumerate}
\usepackage{pgfplots}

\usepackage{nicefrac}
\usepackage{multirow}

\usepackage{mathrsfs}
\usepackage{wrapfig,booktabs}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}

\newcommand{\BB}{\mathcal{B}}
\newcommand{\EE}{\mathcal{E}}
\newcommand{\FF}{\mathcal{F}}
\newcommand{\GG}{\mathcal{G}}
\newcommand{\HH}{\mathcal{H}}
\newcommand{\JJ}{\mathcal{J}}
\newcommand{\LL}{\mathcal{L}}
\newcommand{\MM}{\mathcal{M}}
\newcommand{\NN}{\mathcal{N}}
\newcommand{\UU}{\mathcal{U}}
\newcommand{\OO}{\mathcal{O}}
\newcommand{\RR}{\mathcal{R}}
\newcommand{\VV}{\mathcal{V}}
\newcommand{\WW}{\mathcal{W}}
\newcommand{\XX}{\mathcal{X}}
\newcommand{\YY}{\mathcal{Y}}
\newcommand{\ZZ}{\mathcal{Z}}

\newcommand{\Rf}{\mathfrak{R}}

\newcommand{\Var}{\mbox{Var}}
\newcommand{\TF}{\mathcal{TF}}
\newcommand{\Cf}{\frac{1}{C}}
\newcommand{\Cff}{[-\Cf, \Cf]}
\newcommand{\hue}{h_{u_{\epsilon}}}
\newcommand{\MMD}{\mbox{MMD}}
\newcommand{\Cov}{\mbox{Cov}}
\newcommand{\avg}{\mbox{avg}}
\newcommand{\iid}{\stackrel{iid}{\sim}}
\newcommand{\1}{\mathbb{1}}
\newcommand{\pto}{\overset{p}{\to}}
\newcommand{\dto}{\overset{d}{\to}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\title{Kernelised Normalising Flows}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{
    Eshant English\thanks{Equal contribution}$^{\ \ 1}$ \quad
    Matthias Kirchler$^*$$^{1,2}$ \quad
    Christoph Lippert$^{1,3}$
    \\
    \texttt{eshant.english@hpi.de}
    \\
    $^1$Hasso Plattner Institute for Digital Engineering, Germany
    \\
    $^2$University of Kaiserslautern-Landau, Germany    
    \\
    $^3$Hasso Plattner Institute for Digital Health at the Icahn School of Medicine at Mount Sinai, NYC, USA
}
\iffalse
\author{%
  Eshant English\thanks{Equal contribution
  \\
  \texttt{contact email: eshant.english@hpi.de}
  }%\thanks{Use footnote for providing further information
    %about author (webpage, alternative address)---\emph{not} for acknowledging
    %funding agencies.} \\
  \\Digital Health Machine Learning\\
  Hasso-Plattner-Institute, Germany
  % examples of more authors
  \And
  Matthias Kirchler$^*$
  \\Digital Health Machine Learning\\
  Hasso-Plattner-Institute, Germany\\
  University of Kaiserslautern-Landau, Germany    
  \And
  Christoph Lippert \\
  Digital Health Machine Learning\\
  Hasso-Plattner-Institute, Germany\\
  Hasso Plattner Institute for Digital Health at\\ the Icahn School of Medicine at Mount Sinai, NYC, USA
}
\fi


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Normalising Flows are non-parametric statistical models characterised by their dual capabilities of density estimation and generation. This duality requires an inherently invertible architecture. However, the requirement of invertibility imposes constraints on their expressiveness, necessitating a large number of parameters and innovative architectural designs to achieve good results. Whilst flow-based models predominantly rely on neural-network-based transformations for expressive designs, alternative transformation methods have received limited attention. In this work, we present Ferumal flow, a novel kernelised normalising flow paradigm that integrates kernels into the framework. Our results demonstrate that a kernelised flow can yield competitive or superior results compared to neural network-based flows whilst maintaining parameter efficiency.
Kernelised flows excel especially in the low-data regime, enabling flexible non-parametric density estimation in applications with sparse data availability.
\end{abstract}


\section{Introduction}

Maximum likelihood is a fundamental approach to parameter estimation in the field of machine learning and statistics. However, its direct application to deep generative modelling is rare due to the intractability of the likelihood function. Popular probabilistic generative models such as Diffusion Models \citep{Dai2020SlicedIN} and Variational Autoencoders \citep{kingma2022autoencoding} instead resort to optimising the Evidence Lower Bound (ELBO), a lower bound on the log-likelihood, due to the challenges in evaluating likelihoods.

The change of variables theorem offers a neat and elegant solution to compute the exact likelihood for deep generative modelling. These models, known as normalising flows, employ invertible architectures to transform complex probability distributions into simpler ones. Normalising flows \citep{papamakarios2021normalizing,Kobyzev_2021} excel in efficient density estimation and exact sampling, making them suitable for various applications.

Whilst flow-based models possess appealing properties rooted in invertibility, they also impose limitations on modelling choices, which can restrict their expressiveness. This limitation can be mitigated by employing deeper models with a higher number of parameters. For instance, the Glow model \cite{kingma2018glow} utilised approximately 45 million parameters for image generation on CIFAR-10 \cite{Krizhevsky09learningmultiple}, whereas StyleGAN3 \cite{karras2019stylebased}, a method that doesn't use likelihood optimisation, achieved a superior FID score with only about a million parameters. 

The issue of over-parameterisation in flow-based models hinders their effectiveness in domains with limited data, such as medical applications.
For example, normalising flows can be used to model complex phenotypic or genotypic data in genetic association studies~\citep{hansen2022normalizing,kirchler2022training}; collection of high-quality data in these settings is expensive, with many studies only including data on a few hundred to a few thousand instances.
In scenarios with low data availability, a flow-based network can easily memorise the entire dataset, leading to an unsatisfactory performance on the test set. While existing research has focused on enhancing the expressiveness of flows through clever architectural techniques, the challenge of achieving parameter efficiency has mostly been overlooked with few exceptions \cite{lee2020nanoflow}.
Most normalising flows developed to date rely on neural networks to transform complex distributions into simpler distributions.
However, there is no inherent requirement for flows to use neural networks.
Due to their over-parameterisation and low inductive bias, neural networks tend to struggle with generalisation in the low-data regime, making them inapplicable to many real-world applications.
%It should be noted that neural network-based generative models, including flows, are generally ineffective in low data regimes, especially when invertibility is a crucial constraint.

In this work, we propose a novel approach to flow-based distribution modelling by replacing neural networks with kernels.
Kernel machines work well in low-data regimes and retain expressiveness even at scale.
We introduce Ferumal flows, a kernelised normalising flow paradigm that outperforms or achieves competitive performance in density estimation for tabular data compared to other efficiently invertible neural network baselines like RealNVP and Glow.
Ferumal flows require up to 93\% fewer parameters than their neural network-based counterparts whilst still matching or outperforming them in terms of likelihood estimation.   
We also investigate efficient training strategies for larger-scale datasets and show that kernelising the flows works especially well on small datasets.


\section{Background}
\label{sec:background}

\subsection{Maximum likelihood optimisation with normalising flows}
%\paragraph{Maximum likelihood optimisation with normalising flows}
A normalising flow is an invertible neural network, $f: \R^D \to \R^D$ that maps real data $x$ onto noise variables $z$.
The noise variable $z$ is commonly modelled by a simple distribution with explicitly known density, such as a normal or uniform distribution, while the distribution of $x$ is unknown and needs to be estimated.
As normalising flows are maximum likelihood estimators, for a given data set of instances $x_1, \ldots, x_n$, we want to maximise the log-likelihood over model parameters,
\begin{align*}
    \max_f \sum_{i=1}^n \log\left(p_X(x_i)\right).
\end{align*}
With the change of variables formula,
\begin{align*}
    p_X(x) = p_Z\left(f(x)\right) \left\vert  \det J_f(x) \right\vert,
\end{align*}
where $J_f(x)$ is the Jacobian of $f$ in $x$, we can perform this optimisation directly:
\begin{align*}
        \max_f \sum_{i=1}^n \log\left(p_Z\left(f\left(x_i\right)\right)\right) + \log\left(\left|\det J_f\left(x_i\right)\right|\right).
\end{align*}
 The first part, $\log(p_Z(f(x_i)))$, can be computed in closed form due to the choice of $p_Z$.
 The second part,  $\log(|\det J_f(x_i)|)$, can only be computed efficiently if $f$ is designed appropriately.







%\paragraph{Coupling layers:}
\subsection{Coupling layers}
One standard way to design such an invertible neural network $f$ with tractable Jacobian is affine coupling layers \citep{dinh2017density}.
A coupling layer $C_\ell: \R^D \to \R^D$ is an invertible layer that maps an input $y_{\ell-1}$ to an output $y_\ell$ ($\ell$ is the layer index):
first, permute data dimensions with a fixed permutation $\pi_\ell$, and split the output into the first $d$ and second $D-d$ dimensions: 
\begin{align*}
    \left[u_\ell^1,\,\, u_\ell^2\right] = \left[\pi_\ell(y_{\ell-1})_{1:d}, \,\, \pi_\ell(y_{\ell-1})_{d+1:D}\right].
\end{align*}
Commonly, the permutation is either a reversal of dimensions or picked uniformly at random.
Next, we apply two neural networks, $s_\ell, t_\ell: \R^d \to \R^{D-d}$, to the first output, $u_\ell^1$, and use it to scale and translate the second part, $u_\ell^2$:
\begin{align*}
    y_{\ell}^2 = \exp\left(s_\ell\left(u_\ell^1\right)\right) \odot u_\ell^2 + t_\ell\left(u_\ell^1\right).
\end{align*}
The first part of $y_{\ell}$ remains unchanged, i.e., $y_{\ell}^1 = u_\ell^1$, and we get the output of the coupling layer as:
\begin{align*}
    y_\ell = C_\ell\left(y_{\ell-1}\right) = \left[y_{\ell}^1, \,\, y_{\ell}^2\right] = \left[u_\ell^1, \, \, \exp\left(s_\ell\left(u_\ell^1\right)\right) \odot u_\ell^2 + t_\ell\left(u_\ell^1\right)\right].
\end{align*}


The Jacobian matrix of this transformation is a permutation of a lower triangular matrix resulting from $u_\ell^1$ undergoing an identity transformation and $u_\ell^2$ getting transformed elementwise by a function of $u_\ell^1$.
The Jacobian of the permutation has a determinant with an absolute value of 1 by default.
The diagonal of the remaining Jacobian consists of $d$ elements equal to unity and the other $D-d$ elements equal the scaling vector $s_\ell\left(u_\ell^1\right)$.
Thus, the determinant can be efficiently computed as the product of the elements of the scaling vector $s_\ell\left(u_\ell^1\right)$. 

The coupling layers are also efficiently invertible as only some of the dimensions are transformed and the unchanged dimensions can be used to obtain the scaling and translation factors used for the forward transformation to reverse the operation.

Multiple coupling layers can be linked in a chain of $L$ layers such that all dimensions can be transformed:
\begin{align*}
    f(x) = C_L \circ \dots \circ C_1(x), 
\end{align*}
i.e., $y_0 = x$ and $y_L = f(x)$.


\subsection{Kernel machines}
%\paragraph{Kernel machines}
Kernel machines \citep{scholkopf2002learning} implicitly map a data vector $u \in \R^p$ into a high-dimensional (potentially infinite-dimensional) reproducing kernel Hilbert space (RKHS), $\HH$, by means of a fixed feature map $\phi: \R^p \to \HH$.
The RKHS $\HH$ has an associated positive definite kernel $k(u, v) = \left\langle \phi(u), \phi(v) \right\rangle_\HH$, where $\langle \cdot, \cdot \rangle_\HH : \HH \times \HH \to \R$ is the inner product of $\HH$.
The kernel $k$ can oftentimes be computed in closed form without requiring the explicit mapping of $u, v$ into $\HH$, making computation of many otherwise intractable problems feasible.
In particular, as many linear learning algorithms, such as Ridge Regression or Support Vector Machines, only require explicit computations of norms and inner products, these algorithms can be efficiently kernelised.
Instead of solving the original learning problem in $\R^p$, kernel machines map data into the RKHS and solve the problem with a linear algorithm in the RKHS, offering both computational efficiency (due to linearity and kernelisation) and expressivity (due to the nonlinearity of the feature map and the high dimensionality of the RKHS).


\section{Ferumal flows: kernelisation of flow-based architectures}

In this section, we extend standard coupling layers to use kernel-based scaling and translation functions instead.
Whilst neural networks are known to perform well in large-data regimes or when transfer-learning from larger datasets can be applied, kernel machines perform well even at small sample sizes and naturally trade-off model complexity against dataset size without losing expressivity.

%\paragraph{Kernelised coupling layers}
\subsection{Kernelised coupling layers}

We keep the definition of coupling layers in Section~\ref{sec:background}, and only replace the functions $s_\ell$ and $t_\ell$ by functions mapping to and from an RKHS $\HH$.
We have to deal with two main differences to the kernelisation of many other learning algorithms:
firstly, the explicit likelihood optimisation does not include a regularisation term that penalises the norm of the prediction function.
And secondly, instead of a single mapping from origin space to RKHS to single-dimensional output, we aim to combine multiple layers, in each of which the scaling and translation map from origin space to RKHS to multi-dimensional output.
As a result, the optimisation problem will not be convex in contrast to standard kernel learning, and we have to derive a kernelised and tractable representation of the learning objective.

In particular, in layer $\ell$, we introduce RKHS elements $V_{\ell}^s, V_{\ell}^t \in \HH^{D-d}$ and define scaling and translation as
\begin{align*}
    s_\ell\left(u_\ell^1\right) = \left[\left\langle V_{\ell,j}^s, \phi\left(u_\ell^1\right) \right\rangle_\HH\right]_{j=1}^{D-d} \in \R^{D-d} \quad\quad \mbox{ and } \quad\quad t_\ell\left(u_\ell^1\right) = \left[\left\langle V_{\ell,j}^t, \phi\left(u_\ell^1\right) \right\rangle_\HH\right]_{j=1}^{D-d} \in \R^{D-d}.
\end{align*}
We summarise $V_\ell = \left[V_{\ell}^s, V_{\ell}^t\right]$ and $V = \left[V_1, \ldots, V_L\right]$ for the full flow $f(x) = C_L \circ \dots \circ C_1 (x)$.
Since elements $V \in \HH^{2L(D-d)}$ and $\HH$'s dimensionality is potentially infinite, we cannot directly optimise the objective:
\begin{align}
    \label{kernel-obj}
    \max_{V} & \sum_{i=1}^n p_Z\left(C_L \circ \ldots \circ C_1 (x_i)\right) + \log\left(\left|\det J_{C_L \circ \ldots \circ C_1}(x_i)\right|\right) = L(V).
\end{align}
However, we can state a version of the representer theorem \citep{scholkopf2001generalized} that allows us to kernelise the objective:
\begin{proposition}
\label{prop:representer}
    Given the objective $L$ in equation~\eqref{kernel-obj}, for any $V'=\left[V_{1}', \ldots, V_L'\right]  \in \HH^{L(D-d)}$ there also exists a $V$ with $L(V) =L\left(V'\right)$ such that
    \begin{align*}
        V_{\ell} = \sum_{i=1}^n k\left(\cdot, u_{\ell,i}^1\right)  A_{\ell, i}
    \end{align*}
    for some $A_{\ell,i} = \left[A_{\ell, i}^s, A_{\ell, i}^t\right] \in \R^{2(D-d)}$.
    Here, $u_{\ell,i}^1 = \pi_\ell(C_{\ell-1} \circ \dots \circ C_{1}(x_i))_{1:d}$, i.e., the first part of the permutated input to layer $\ell$ for data point $i$.
    In particular, if there exists a solution $V'\in \argmax_V L(V)$, then there's also a solution $V^*$ of the form 
    \begin{align*}
        V_{\ell}^* = \sum_{i=1}^n k\left(\cdot, u_{\ell,i}^1\right)  A_{\ell, i}.
    \end{align*}
\end{proposition}
\begin{proof}
    Let $V' \in \HH^{2L(D-d)}$ and $\Phi_\ell = \text{span}\{\phi(u_{\ell,1}^1), \ldots, \phi(u_{\ell, n}^1) \}$ be the space spanned by the feature maps of layer $\ell$ inputs, and let $\Phi_\ell^\bot$ denote its orthogonal complement in $\HH$.
    We can then represent each element ${V_{\ell,j}^s}'\in \HH$ ($j \in \{1, \ldots, D-d\}$) as an orthogonal sum of an element of $\Phi_\ell$ and $\Phi_\ell^\bot$,
    \begin{align*}
        {V_{\ell,j}^s}' = \phi_{\ell,j} + \phi_{\ell,j}^\bot, \mbox{ with } \phi_{\ell, j} = \sum_{i=1}^n A_{\ell,i,j}^s \phi(u_{\ell,i}^1) \mbox{ and } \langle \phi_{\ell,j}^\bot, \phi(u_{\ell,i}^1) \rangle_\HH = 0 \,\, \forall i=1, \ldots, n
    \end{align*}
    for some values $A_{\ell, i, j}^s \in \R$.
    In the objective~\eqref{kernel-obj}, we only use ${V_{\ell,j}^s}'$ to compute $\langle {V_{\ell,j}^s}', \phi(u_{\ell,i}^1) \rangle_\HH$ as part of the computation of $s_\ell(u_{\ell,i}^1)$.
    But due to orthogonality, it holds that
    \begin{align*}
        \langle {V_{\ell,j}^s}', \phi(u_{\ell,i}^1) \rangle_\HH = \langle \phi_{\ell,j} + \phi_{\ell,j}^\bot, \phi(u_{\ell,i}^1) \rangle_\HH =\langle \phi_{\ell,j}, \phi(u_{\ell,i}^1) \rangle_\HH + \langle  \phi_{\ell,j}^\bot, \phi(u_{\ell,i}^1) \rangle_\HH = \langle \phi_{\ell,j}, \phi(u_{\ell,i}^1) \rangle_\HH.
    \end{align*}
    Hence, replacing ${V_{\ell,j}^s}'$ by $\phi_{\ell,j} = \sum_{i=1}^n A_{l,i,j}^s \phi(u_{\ell, i}^1)$ keeps the objective unchanged.
    The reproducing property of the RKHS $\HH$ now states that $\langle \phi(u_{\ell,i}^1), \phi(\cdot) \rangle_\HH = k(u_{\ell, i}^1, \cdot)$.

    Repeating this for all $\ell = 1, \ldots, L$, all $j = 1, \ldots, D-d$ and also for translations $t_\ell$ yields a version of $V'$ that can be represented as a linear combination of the stated form.
\end{proof}
In contrast to the classical representer theorem, the objective doesn't contain a regulariser of the model's norm, which would ensure that \emph{any} solution can necessarily be represented as a linear combination of kernel evaluations.
However, if a solution exists, Proposition~\ref{prop:representer} ensures that there also exists a solution that can be expressed as a linear combination of kernel evaluations.
Therefore, we can re-insert this solution $V^*$ into the objective~\ref{kernel-obj} to get a kernelised objective.

% Note that, in contrast to the classical representer theorem, since the objective doesn't contain a strictly convex regularisation term that penalises the model complexity, the solution is not necessarily unique.
% While there may be other solutions, we know that there exists a solution that can be expressed as a linear combination of kernel evaluations, so we can re-insert this solution $V^*$ into the objective~\ref{kernel-obj}.
For layer $\ell$ and arbitrary $a \in \R^d$,
\begin{align*}
s_\ell(a) = \left[\langle V_{\ell, j}^{s*}, \phi(a)\right\rangle]_{j=1}^{D-d} = \left[\sum_{i=1}^n A_{\ell,i,j}^s k\left(u_{\ell,i}^1, a\right) \right]_{j=1}^{D-d} = \sum_{i=1}^n  k\left(u_{\ell,i}^1, a\right) A_{\ell, i}^s.
\end{align*}
As in the objective~\eqref{kernel-obj}, $s_\ell$ gets only evaluated in points $a \in \left\{ u_{\ell,i}^1 | i = 1, \ldots, n\right\}$, this simplifies to
\begin{align*}
    s_\ell\left(u_{\ell,m}^1\right) = \sum_{i=1}^n k\left(u_{\ell,i}^1, u_{\ell, m}^1\right) A_{\ell,i}^s =  A_{\ell}^s K\left(U_\ell^1, U_\ell^1\right)_m,
\end{align*}
where $K\left(U_\ell^1, U_\ell^1\right) = \left[k\left(u_{\ell, i}^1, u_{\ell, m}\right)^1\right]_{i,m=1}^n$ is the kernel matrix at layer $\ell$ and $A_\ell^s = \left[A_{\ell, 1}^s, \ldots, A_{\ell, n}^s\right] \in \R^{(D-d) \times n}$ is the weight matrix.
A similar derivation holds for $t_\ell$.

In total, we can kernelise the objective~\eqref{kernel-obj} and optimise over parameters $A \in \R^{L \times n \times 2(D-d)}$ instead of over $V \in \HH^{2L(D-d)}$. 
In contrast to neural network-based learning, the number of parameters, $2Ln(D-d)$, is fixed except for the number of layers (since $d$ is usually set to $\floor{D/2}$), but increases linearly with the dataset size, $n$.
This makes kernelised flows especially promising for learning in the low-data regime, as their model complexity naturally scales with dataset size and does not over-parametrise as much as neural networks (as long as one does not employ an excessive number of layers).

Since the resulting objective function is not convex, optimisers targeted to standard kernel machines such as Sequential Minimal Optimisation~\citep{platt1998sequential} are not applicable.
Instead, we optimise~\eqref{kernel-obj} with variations of stochastic gradient descent~\citep{kingma2014adam}.



\subsection{Efficient learning with auxiliary points}
%\paragraph{Efficient learning with auxiliary points}
The basic kernelised formulation can be very computationally expensive even at moderate dataset sizes and can tend towards overfitting in the lower-data regime.
In Gaussian Process (GP) regression, this problem is usually addressed via sparse GPs and the introduction of \emph{inducing variables}~\citep{JMLR:v6:quinonero-candela05a}.
In a similar spirit, we introduce \emph{auxiliary points}.
In layer $\ell$, instead of computing the kernel with respect to the full data $u_{\ell,1}^1, \ldots, u_{\ell,n}^1$, we take $N \ll n$ new variables $W_\ell^1 = [w_{\ell, 1}^1, \ldots, w_{\ell,N}^1] \in \R^{d \times N}$ and compute the scaling transform as
\begin{align*}
    \hat{s}_{\ell}(u_{\ell, m}^1) = \hat{A}_\ell^sK(U_\ell^1, W_\ell^1)_m
\end{align*}
with $\hat{A}_\ell^s \in \R^{(D-d)\times N}$ (analogously for $\hat{t}_\ell$).
We make these auxiliary points learnable and initialise them to a randomly selected subset of $u_{\ell, 1}^1, \ldots, u_{\ell, n}^1$.
This procedure reduces the learnable parameters from $2n(D-d)L$ (for both $s_\ell$ and $t_\ell$) to $2NDL$.

In another variation we make these auxiliary points shared between layers.
In particular, instead of selecting $L$ times $N$ points $w_{\ell,1}^1, \ldots, w_{\ell,N}^1$, we instead only select $W^1 = [w_1^1, \ldots, w_N^1] \in \R^{d \times N}$ once and compute at layer $\ell$
\begin{align*}
        \bar{s}_{\ell}(u_{\ell, m}^1) = \hat{A}_\ell^sK(U_\ell^1, W^1)_m.
\end{align*}
This further reduces the learnable parameters to $2N(D-d)L + 2dN$.





\section{Related works}

We are unaware of any prior work that attempts to replace neural networks with kernels in flow-based architectures directly.
However, there is a family of flow models based on Iterative Gaussianisation (IG) \cite{chen2000gaussianization} that utilise kernels. Notable works using Iterative Gaussianisation include Gaussianisation Flows \cite{meng2020gaussianization}, Rotation-Based Iterative Gaussianisation (RBIG) \cite{Laparra_2011}, and Sliced Iterative Normalising Flows \cite{DBLP:journals/corr/abs-2007-00674}.
These IG-based methods differ significantly from our methodology. They rely on kernel density estimation and inversion of the cumulative distribution function for each dimension individually and incorporate the dependence between input dimensions through a rotation matrix, which aims to reduce interdependence. In contrast, our method integrates kernels into coupling layer-based architectures. Furthermore, IG-based methods typically involve a large number of layers, resulting in inefficiency during training and a comparable number of parameters to neural network-based flow architectures. In contrast, the Ferumal flow approach of incorporating kernels can act as a drop-in replacement in many standard flow-based architectures, ensuring parameter efficiency without compromising effectiveness.
Another generative model using kernels is the work on kernel transport operators \citep{huang2021forward}. demonstrated promising results in low-data scenarios and favourable empirical outcomes. However, their approach differs from ours as they employed kernel mean embeddings and transfer operators, along with a pre-trained autoencoder.

Other works focusing on kernel machines in a deep learning context are deep Gaussian processes \cite{damianou2013deep} and deep kernel learning \citep{wilson2016deep,wenliang2019learning}.
Deep GPs concatenate multiple layers of kernelised GP operations; however, they are Bayesian, non-invertible models for prediction tasks instead of density estimation and involve high computational complexity due to operations that require inverting a kernel matrix. Some works \cite{rudi2021psd,marteauferey2021sampling,tsuchida2023squared} use kernels for nonnegative functions for modelling densities but are also not invertible.
Deep kernel learning, on the other hand, designs new kernels that are parametrised by multilayer perceptrons.


\citet{maro√±as2021transforming} integrated normalising flows within Gaussian processes. Their approach differs significantly from ours as they aimed to exploit the invertibility property of flows by applying them to the prior or the likelihood. Their combined models consist of kernels in the form of GPs but also involve neural networks in the normalising flows network, resembling more of a hybrid model.

NanoFlow \cite{lee2020nanoflow} also targets parameter efficiency in normalising flows.
They rely on parameter sharing across different layers, whereas we utilise kernels. We also attempted to implement the naive parameter-sharing technique suggested by \citet{lee2020nanoflow}, but we found no improvement in performance. %it resulted in a decline in performance.



\section{Experiments}

We assess the performance of our Ferumal flow kernelisation both on synthetic 2D toy datasets and on five real-world benchmark datasets sourced from \citet{Dua:2019}. The benchmark datasets include Power, Gas, Hepmass, MiniBoone, and BSDS300. To ensure consistency, we adhere to the preprocessing procedure outlined by \citet{papamakarios2018masked}.

\paragraph{Implementation details}
We kernelised the RealNVP and Glow architectures. 
% We made a deliberate decision to not include architectural innovations like $1\times1$ convolutions \cite{kingma2018glow}, which have the potential to enhance the effectiveness of our architecture in achieving optimal splits. 
% This decision was made to showcase the effectiveness of kernelisation in its simplest form.
For comparison purposes, RealNVP and Glow act as direct comparisons being the neural-net counterparts of our models. We have also included basic autoregressive methods, Masked Autoregressive Flows \citep{papamakarios2018masked}, and Masked Autoregressive Distribution Estimation \citep{germain2015made}, FFJORD \citep{grathwohl2018ffjord} as a continuous normalising flow, and Gaussianisation Flows \citep{meng2020gaussianization}, Rotation-based Iterative Gaussianisation \citep{Laparra_2011}, Sliced Iterative Normalising Flows \citep{Dai2020SlicedIN} as iterative gaussianisation methods for our evaluations. Most autoregressive flow models outperform non-autoregressive flow models. However, they usually come with the trade-off of either inefficient sampling or inefficient density estimation, i.e., either the forward or the inverse computation is computationally very expensive.

%Throughout our experiments, we have consistently employed the Square-Exponential kernel, opting not to introduce additional hyperparameters. Nevertheless, it is worth noting that the use of alternative kernels may enhance the expressiveness of the network. It is essential to clarify that our objective is not to assert that this simple architecture is state-of-the-art, but rather to demonstrate that kernelising flow-based architectures can yield high expressiveness while maintaining low parameter complexity. Additionally, it can help bridge the performance gap between coupling layer-based flow architectures and autoregressive flow architectures if employed with much more powerful architectures than ours.
\begin{wrapfigure}{r}{0.53\linewidth}
    \centering
    % Figure removed
    \caption{Histogram of 2D toy datasets. \textbf{Left:} True distribution. \textbf{Middle:} NN-based. \textbf{Right:} FF-kernelisation}
    \label{modelling}
\end{wrapfigure}

\iffalse
We aim to answer the following questions:
\begin{enumerate}
    \item Does Ferumal Flow kernelisation have better initialisation than other flow-based architectures?
    \item Does Ferumal Flow kernelisation perform better than other neural-net-based architectures in terms of density estimation?
    \item Is Ferumal Flow kernelisation capable of data generation in low-data regimes?
    \item Is Ferumal Flow kernelisation parameter-efficient whilst still retaining expressiveness?
\end{enumerate}
\fi


\paragraph{Training details} Our Ferumal Flow kernelisation has a negligible number of hyperparameters. Apart from learning rate hyperparameters (i.e., learning rate, $\beta_1$, $\beta_2$ for Adam) and the number of layers, that are central to both kernelised and neural-net-based flows, we only need to choose a kernel with its corresponding hyperparameters (and a number of auxiliary points for large-scale experiments). This is in contrast with neural-net-based flows where choices for a flow layer include a number of hidden sub-layers, the number of nodes in each sub-layer, residual connections, type of normalisation, activation function, dropout, and many more. Coupled with longer convergence times this necessitates considerably more time and resources in hyperparameter tuning than our proposed kernel methods. In our study, we utilised either the Squared Exponential kernel or Matern Kernels exclusively for all experiments. We learnt all the kernel hyperparameters using the GPyTorch library for Python for the main experiments. Throughout the experiments, we used the Adam \citep{kingma2014adam} optimiser, whilst adjusting the $\beta_1$ and $\beta_2$ parameters of the optimiser. Additionally, we decayed the learning rate either with predefined steps (StepLR) or with cosine annealing.
In all the experiments, we incorporated auxiliary points as we observed that they provided better results. In most cases, we persisted with 150 auxiliary points.
 

We coded our method in PyTorch \citep{paszke2019pytorch} and used existing implementations for the other algorithms.
We ran all experiments for Ferumal flows and other baselines on CPUs (Intel Xeon 3.7 GHz).
%However, due to the disparate programming languages employed, a fair comparison cannot be made.
For more comprehensive training details, please refer to the Table \ref{train details}

\subsection{2D toy datasets}

%\begin{table}

\begin{wraptable}{r}{0.55\linewidth}
  \caption{Results on toy datasets. Log Likelihood in nats, higher is better}
  \label{toy datasets}
  \centering
  \begin{tabular}{lll}
    \toprule
%    \multicolumn{3}{c}{On toy datasets}                   \\
    %\cmidrule(r){1-3}
    Dataset     & Ours (\#params) & NN-based (\#params) \\ 
    \midrule
    Line   & \textbf{3.75} (\textbf{5K}) & 3.15  (44K)\\
    Pinwheel   & \textbf{-2.44 } (\textbf{4K}) & -2.48 (44K)\\
    Moons     &\textbf{-2.43 } (\textbf{5K}) & -2.54 (44K)\\
    \bottomrule
  \end{tabular}
\end{wraptable}
%\end{table}
Initially, we conducted density estimation experiments on three synthetic datasets that were sampled from two-dimensional distributions exhibiting diverse shapes and numbers of modes.
%Contrary to the tabular real-world datasets, two-dimensional densities can be easily visualised.
%Our aim was to transform these intricate two-dimensional distributions into isotropic Gaussians.
Figure~\ref{modelling} showcases the original data distribution alongside the samples generated using the Ferumal flow kernelisation and the corresponding neural network counterpart. The neural-net-based architecture clearly shows the distortion of density in many regions whereas the kernelised counterpart has much better modelling. Table~\ref{toy datasets} shows the corresponding log-likelihood in nats, quantitatively showing the enhancement from our kernelisation.
The results demonstrate that Ferumal flow kernelisation can outperform its neural net counterpart on these toy datasets.
All the toy datasets were trained with a batch size of 200 and for 10K iterations.
We also investigated the effect of our kernelisation on highly discontinuous densities strengthening our argument for kernelisation. Please refer to Appendix~\ref{sec:modelling discontinued densities}


\subsection{Real-world datasets}
We conducted density estimation experiments on five real-world tabular benchmark datasets (description can be found in Appendix~\ref{sec:dataset detail}), employing the preprocessing method proposed by \citet{papamakarios2018masked}. In our experiments, we kernelised two flow architectures, i.e., RealNVP and Glow, that utilise the coupling layer for efficient sampling and inference and making direct comparisons with them. Additionally, we also considered comparisons with GF (Gaussianisation Flows) \citep{meng2020gaussianization}, RBIG (Rotation-based Iterative Gaussianisation) \citep{Laparra_2011}, GIS (Gaussianied Iterative Slicing/Sliced Iterative Normalising Flows) \citep{Dai2020SlicedIN}, MAF (Masked Autoregressive Flows) \citep{papamakarios2018masked}, and MADE (Masked Autoregressive Distribution Estimation) \citep{germain2015made}, FFJORD \citep{grathwohl2018ffjord}, architectures that do not use coupling layers.
These methods are not directly comparable to the coupling-layer-based methods, as they have significantly higher computational costs.
In particular, Gaussianisation-based methods, require many more layers (up to 100, in our settings), whilst autoregressive flows are slow to sample from, due to their autoregressive nature. In contrast to Gaussianisation-based methods, our kernelisation of coupling layers does not increase the computational complexity and the training time under a fixed number of epochs is similar to neural-net-based coupling layers. Run-time comparisons under a fixed number of epochs are provided in Table~\ref{run time} in Appendix~\ref{sec:run time}),.

Table~\ref{tab:nats} presents the results of our experiments, revealing that Ferumal flow kernelisation consistently achieves better or competitive outcomes across all five datasets.
Despite its straightforward coupling layer architecture, our approach surpasses RBIG, GIS, and MADE on all the datasets and achieves competitive performance to the much more expensive MAF, GF, and FFJORD methods, underscoring the efficacy of integrating kernels. Please refer to Table~\ref{tab:error bars} for error bars on coupling and non-coupling experiments.
% The superior performance of our method reinforces the advantages of incorporating kernels within the flow-based framework.


\begin{table}[t]
  \caption{Log-likelihood measured in nats. Larger values are better. Methods prepended with FF are our kernelised versions and the results with * are taken from existing literature.}
  \label{tab:nats}
  \centering
  \begin{tabular}{lllllll}
    \toprule
    \multicolumn{7}{c}{Datasets}                   \\
    \cmidrule(r){3-7}
    & Method     & Power     & Gas & Hepmass & Miniboone & BSDS300 \\
    \midrule
   Coupling & RealNVP & 0.17  & 8.33 & -18.7 & -13.55 & 153.28   \\
   & FF-RealNVP {\small(ours)} & {0.24} & {9.55 } & {-18.20} & {-11.19}  &  {154.30}   \\
   &  Glow     & 0.17  & 8.15 & -18.92 & -11.35 & {155.07}  \\
   & FF-Glow {\small(ours)} & {0.35} & {10.75}  & {-17.11} & {-10.76 }& 154.71\\
    %\cmidrule(r){1-7}
    \midrule
    
   Non-coupling & MADE* & -3.08 & 3.56 & -20.98 & -15.59 & 148.85    \\
   & MAF     & 0.24  & 10.08& -17.70 & -11.75 & 155.69   \\
   & FFJORD     & 0.46 & 8.59& -14.92 & -10.43 & 157.40    \\
   \midrule
   Gaussianisation & GF     & 0.57  & 10.08 & -17.59 & -10.32 & 152.82    \\
   methods & GIS*     & 0.32  & 10.30 & -19.00 & -14.26 & 157.31    \\
   &  RBIG     & -1.02  & -0.05 & -24.59 & -25.41 & 115.96    \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Initial performance}
Figure~\ref{fig:nats} presents the learning curves of the train and test loss for our Ferumal flow kernelisation and the two neural-network counterparts.
These findings demonstrate that the Ferumal-flow-based architecture exhibits faster convergence compared to the neural network baselines.
%The better initialisation helps in faster convergence.
This may be due to the parameter efficiency provided by our kernelisation or due to the stronger inductive biases provided by kernel machines. Throughout our experiments, we maintained default settings and ensured consistent batch sizes across all models. 






\subsection{Low-data regime}
\label{sec:low data}
In certain applications, such as medical settings, data availability is often limited. Neural-network-based flows typically suffer from over-parameterisation, leading to challenges in generalisation within low-data regimes. To assess the generalisation capability of our model under such conditions, we trained our model using only 500 examples and evaluated its performance on the same benchmark datasets.
To address the challenges of limited data, we opted to tie the learned auxiliary variables across layers in this setting. This approach helped mitigate parameter complexity whilst maintaining the benefits of utilising auxiliary points.
\begin{wraptable}{r}{0.59\linewidth}
  \caption{Results on a small subset of 500 examples. LL in nats, higher the better}
  \label{low data regimes}
 \centering 
  \begin{tabular}{lll}
    \toprule
%    \multicolumn{3}{c}{On a subset of 500 examples}                   \\
 %   \cmidrule(r){1-3}
    Dataset     & Ours (\#params)  & FFJORD (\#params) \\ % & FFJORD(params) \\ 
    \midrule
    Miniboone   & \textbf{-27.75} (\textbf{58K}) & -39.92  (821K)\\
    Hepmass     & \textbf{-27.90 }(\textbf{41K}) &-28.17  (197K)\\
    Gas     &\textbf{0.22 } (\textbf{11K}) & -7.50  (279K)\\
    Power     & \textbf{-2.91} (\textbf{8K})& -11.33 (43K)\\
    BSDS300     &  \textbf{121.22} (\textbf{85K}) &100.32 (3,127K) \\
    \bottomrule
  \end{tabular}
\end{wraptable}
As highlighted by \citet{meng2020gaussianization}, Glow and RealNVP struggled to generalise in low-data regimes, evidenced by increasing validation and test losses whilst the training losses decreased.
To provide a stronger benchmark, we included the FFJORD model \citep{grathwohl2018ffjord}. FFJORD is a continuous normalising flow method with a full-form Jacobian and exhibits superior performance to Glow or RealNVP in density estimation and generation tasks. For our model, we used a kernelised version of RealNVP which is notably weaker than the Glow version. This also proves that kernelisation can make flow-based models more data-efficient.

Table~\ref{low data regimes} presents the results, demonstrating that our method achieves superior generalisation. This may be attributed to the significantly lower number of parameters required compared to the continuous FFJORD method.
%It is important to note that our approach showcases enhanced generalisation performance while employing substantially fewer parameters than FFJORD (See Appendix for details).



%\hfill



\subsection{Parameter efficiency}
%We conducted a comprehensive comparison of parameter counts between Ferumal Flows and the baselines, namely Glow and RealNVP. The results, as shown in 
Table~\ref{Parameter count} shows the parameter counts of Ferumal flows against the baseline methods.
Kernelising the models results in a parameter reduction of up to 93\%. The parameter efficiency consequently results in less data requirement, better generalisation, and faster convergence.
This reduction can be further improved by implementing strategies such as sharing auxiliary variables between layers or potentially with low-rank approximations, particularly in scenarios where data is limited and concerns about overfitting arise (see Appendix~\ref{sec:low rank}
 for additional details).

\begin{table}[t]%{r}{1.8\linewidth}
 \caption{Number of parameters. Methods prepended with FF are our kernelised versions with \% reduction in brackets}
  \label{Parameter count}
   \centering
  \begin{tabular}{lllll}
    \toprule
    \multicolumn{4}{c}{Architectures}                   \\
    \cmidrule(r){2-5}
    Dataset     & RealNVP  &FF-RealNVP {\small(ours)}   & Glow  & FF-Glow {\small(ours)} \\%reduction \\
    \midrule
Miniboone & 377K  & 117K (69\%) & 395K  &   141K (64\%)      \\%& \textbf{70\%}  \\
    Hepmass   & 288K & 76K (74\%)  & 293K &  79K  (73\%)   \\%& \textbf{75\%}\\
    Gas         & 236K & 22K (91\%)& 237K  &   23K (90\%)  \\%& \textbf{90\% } \\
    Power    & 228K &16K (93\%)  & 228K   &    20K (91\%)  \\%& \textbf{93 \%} \\
    BSDS300   & 458K & 171K (63\%) & 497K  &  279K (44\%)  \\%& \textbf{65 \%} \\
    \bottomrule
  \end{tabular}
\end{table}
\section{Discussion and Limitations}

We have introduced Ferumal flows, a novel approach to integrate kernels into flow-based generative models.
Our study highlighted that Ferumal flows exhibit faster convergence rates, thanks to the inductive biases imparted by data-dependent initialisation and parameter efficiency. Moreover, we have demonstrated that kernels can significantly reduce the parameter count without compromising the expressive power of the density estimators.
Especially in the low-data regime, our method shows superior generalisation capabilities, while Glow and RealNVP fail entirely, and FFJORD lags significantly in performance. We also demonstrate the application of our method in hybrid modelling. (Please refer to Appendix~\ref{sec:vae improvement})


%\subsection{Future Work}


% In our pursuit of further parameter reduction, we are interested in exploring Support Vector Machines (SVMs)\cite{young1988introduction} and their potential to transform the optimisation objective into a convex one. By investigating ResFlow-like architectures, we aim to establish an analogy between them and multiple-output residual SVMs.
% The theoretical analysis of our current work and its interpretation in terms of SVMs are deferred to future investigations.

%\subsection{Limitations and future work}
%\paragraph{Limitations and future work}

In contrast to neural-network-based flows, kernelised flows require a different hyperparameter selection.
In classical kernel machines, the choice of kernel usually implies a type of inductive bias (e.g., for specific data types \citep{vishwanathan2010graph}).
Consequently, in this work, we mostly focus on Squared Exponential kernels and Matern kernels, but incorporating kernels with strong inductive biases may be a promising avenue for future research.
%Ferumal flow kernelisation helps to kernelise the neural-network-based flows, however, flows utilising Convolutional neural nets are not readily replaceable.
In particular, parameter sharing for highly structured modalities such as images is another potential direction for future research.


%The only limitation of our work lies in the fact that incorporating kernels can augment the hyperparameter space, necessitating the addition of at least the length scale of a kernel as a hyperparameter for each layer (varying types of kernels may introduce additional hyperparameters). However, we would like to highlight that in our experiments, this did not result in increased optimisation complexity. In fact, we posit that the kernel hyperparameters aided in identifying superior transformations for the split. Unlike neural network-based architectures, which heavily rely on weights to empower transformations for the other split, our approach offers greater flexibility by the addition of kernel length scale.

The present work introduces kernels only for some affine coupling layer architectures such as RealNVP and Glow.
However, the concepts also directly apply to other coupling-layer-type networks, such as neural spline flows \citep{durkan2019neural}, ButterflyFlows \citep{pmlr-v162-meng22a}, or invertible attention \citep{sukthanker2022generative} for greater expressiveness and parameter efficiency.
Ferumal flow kernelisation can also be directly enhanced with other building blocks such as MixLogCDF-coupling layers \citep{ho2019flow++}.
% It is worth mentioning that incorporating advances in selecting dimensions for coupling layers, such as 1x1 convolutions or Butterfly matrices\cite{pmlr-v162-meng22a}, can further enhance the architecture.
% We would like to emphasise that Ferumal Flow is not an architecture; it represents a paradigm for seamlessly integrating kernels into flows.
% While we have showcased its capabilities in a basic coupling layer architecture, the concept can be readily extended to highly expressive flows like Generative Flows with Invertible Attentions\cite{sukthanker2022generative}, FFJORD, and Neural Splines Flow\cite{durkan2019neural}, enabling greater expressiveness and reducing parameter complexity.

Whilst our method can be applied to coupling-type flow-based architectures, it poses challenges when it comes to ResFlow-like architectures \cite{behrmann2019invertible,chen2000gaussianization}, which require explicit control of Lipschitz properties of the residual blocks. As a result, extending our approach to ResFlow-like architectures is left as a direction for future research.

%\subsection{Broader impact}
%\paragraph{Broader impact}
%The primary objective of this study was to enhance the prominence of flow-based models as generative models for real-world applications that face limitations in data requirements, parameter complexity and reduced expressiveness due to invertibility.
%Certain issues, such as data requirements and parameter complexity, are inherent to neural networks and become more pronounced when applied to flows.
%Given the proven efficacy of kernel-based methods in low data regimes, we sought ways to integrate them into flow-based architectures.
%It is our hope that this integration will encourage further exploration of kernels and their adaptation for diverse applications.
One major drawback of existing normalising flow algorithms is their dependence on an abundance of training data.
The introduction of kernels into these models may allow the application of flows in low-data settings.  
Additionally, in the era of increasingly large and complex models, energy consumption has become a significant concern. Faster convergence can contribute to energy savings. Notably, our models, owing to their faster convergence and few hyperparameters needed fewer training runs than the neural-network counterparts. We anticipate that future research will continue to explore efficient methodologies and strive for reduced energy and data demands.

\subsubsection*{Acknowledgments}
We extend our gratitude to Noel Danz for his valuable discussions on coding. We would also like to express our appreciation to Arkadiusz Kwasigroch and Alexander Rakowski for their initial feedback on the draft. This research was funded by the HPI research school on Data Science and by the European Commission in the Horizon 2020 project INTERVENE (Grant agreement ID: 101016775)



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\iffalse
\appendix

\paragraph{Low-rank approximations}
For datasets characterised by high dimensionality and complex structures, such as images, relying solely on auxiliary points for the weight matrix ($W_{p \ N}$) is inadequate. When half of the dimensions are employed (as is the case in any coupling layer), this matrix becomes excessively large, a problem that magnifies when generating high-resolution images. It is worth highlighting that flow-based models encounter inefficiencies when producing high-resolution images.

To preserve the desirable quality of parameter reduction in our models, we propose an alternative approach to obtaining the weight matrix for a kernelised layer. We suggest using the product of two smaller matrices (with fewer parameters) instead. For a weight matrix $W_{p * N}$, responsible for producing $p$ affine parameters using N auxiliary points, we can learn two smaller matrices: $W^1_{c * N}$ and $W^2_{c * N}$, where $c < p$. By employing the product of these two matrices as a proxy for a full-weight matrix, we can effectively reduce the number of parameters. In image datasets, neighbouring pixels often exhibit high correlation, making parameter-sharing techniques like convolutions more effective than fully connected neural networks. Consequently, this low-rank approximation can be regarded as a form of parameter sharing specifically tailored for image datasets.
\fi




\bibliography{iclr2024_conference}
\bibliographystyle{iclr2024_conference}


\newpage

\appendix

\section{Additional experimental details for our method}
\label{sec:training details}






We employed the Adam optimiser exclusively for all our experiments. The other hyperparameters are chosen using a random grid search, i.e. $lr \in [0.01,0.005,0.001]$, $\beta_1, \beta_2 \in [0.85,0.9,0.95,0.99]$, kernel $\in [matern32, matern52, rbf]$. We used Cosine decay for all experiments with the minimum learning rate equalling zero. In initial experiments, we found that 150 auxiliary points performed satisfactorily and we persisted with it for the majority of datasets and tried 200 for datasets with high-dimensionality. For comprehensive information, please refer to Table~\ref{train details}.

For the experiments presented in Table~\ref{toy datasets}, we employed Glow-based architectures for both approaches. We trained these datasets for 1000 training steps, with the data synthesised at every training step (as done in existing implementations).
For experiments in Table~\ref{toy datasets}, Table~\ref{low data regimes}, we exclusively use the RBF/Squared-Exponential kernel and randomly sampled the kernel length scale from a log-uniform distribution, i.e., $\gamma \sim \exp(U)$, where $U \sim \UU_{[-2, 2]}$.

\iffalse
We would like to highlight that several of our models had not yet converged, and extended training durations might have yielded improved results.
\fi
%% Figure environment removed


\begin{table}[h!]
  \caption{Model Architectures and hyperparameters for our method. }
  \label{train details}
  \centering
  \begin{tabular}{lllllll}
    \toprule
    \multicolumn{7}{c}{Datasets}                   \\
    \cmidrule(r){3-7}
    & Method     & Power     & Gas & Hepmass & Miniboone & BSDS300 \\
    \midrule
    & Dimensionality    & 6     & 8 & 21 & 43 & 63 \\
    & Training Points      & 1,615,917     & 852,174 & 315,123 & 29,556 & 1,000,000 \\
    \midrule
    & layers    & 14 & 12 & 15 & 12  &  12\\
    & kernel     & matern52 &matern32 & matern32  & matern52  &  rbf   \\
   &  auxiliary points &150  & 150 & 150 & 150 & 200  \\
   & learning rate(lr) & 0.005 & 0.005  & 0.01 & 0.005  & 0.005   \\
    %\cmidrule(r){1-7}
    &  $\beta_1$   & 0.95  & 0.9 & 0.99 & 0.99 &  0.95  \\
  &   $\beta_2$     & 0.9  & 0.99& 0.99 & 0.90 &  0.85   \\
   & lr schedular & cosine &cosine & cosine & cosine &  cosine   \\
   & min lr & 0 & 0.95& 0.95 & 0.95 &   0  \\
   & epochs     & 200  & 200& 500 & 600 &   400  \\
   & batch size     & 2000 &2000 & 1024  & 2000  &  1024   \\
    \bottomrule
  \end{tabular}
\end{table}

\section{Modelling Discontinuous Densities}
\label{sec:modelling discontinued densities}
%\begin{table}
\begin{wraptable}{r}{0.59\linewidth}
  \caption{Results on discontinuous densities. Log Likelihood in nats, higher is better}
  \label{discontinued densities}
  \centering
  \begin{tabular}{lll}
    \toprule
%    \multicolumn{3}{c}{On toy datasets}                   \\
    %\cmidrule(r){1-3}
    Dataset     & Ours (\#params) & NN-based (\#params) \\ 
    \midrule
    Checkerboard   & \textbf{-3.61} (\textbf{1.8K}) & -3.68  (18.4K)\\
    Diamond   & \textbf{-3.24 } (\textbf{1.8K}) & -3.39 (18.4K)\\
    \bottomrule
  \end{tabular}
\end{wraptable}
%\end{table}
We consider two toy datasets with discontinuous densities, CheckerBoard and Diamond. We used 4 flow steps for all the models and piecewise polynomial kernel and learnt its hyperparameters. 
%Contrary to the tabular real-world datasets, two-dimensional densities can be easily visualised.
%Our aim was to transform these intricate two-dimensional distributions into isotropic Gaussians.
Figure~\ref{discontinued densities} showcases the original data samples(Left) alongside the modelled density(Middle) and the samples generated using the flow(Right). The neural-net-based architecture for the checkerboard dataset in the first row shows blurry boundaries and ill-defined corners. The kernelised counterpart in the second row has better-defined boundaries in some cases. However, this effect is even more pronounced in the diamond dataset with the kernelised counterpart in the fourth row modelling the discontinuity way better. Table~\ref{discontinued densities} shows the corresponding log-likelihood in nats, quantitatively showing the enhancement from our kernelisation.
The results demonstrate that Ferumal flow kernelisation can outperform its neural net counterpart on these toy datasets.
All the toy datasets were trained with a batch size of 512 and for 100K iterations. We found that the piecewise polynomial kernel was better suited for discontinuous densities than Matern kernels. This provides another evidence of better performance due to inductive biases of a kernel. 
% Figure environment removed

\section{Low-rank approximations}
\label{sec:low rank}

For datasets characterised by high dimensionality and complex structures, relying solely on auxiliary points for the weight matrix ($A \in \R^{2(D-d) \times N}$) is inefficient. When half of the dimensions are transformed (as is the case in any coupling layer), this matrix becomes excessively large. 

To preserve the desirable quality of generalisation in our models, we propose an alternative approach to obtaining the weight matrix for a kernelised layer. We suggest using the product of two smaller matrices (with fewer parameters) instead. For a weight matrix $A \in \R^{p\times N}$, responsible for producing $p$ affine parameters using $N$ auxiliary points, we can learn two smaller matrices: $A^1 \in \R^{c \times N}$ and $A^2 \in \R^{c \times p}$ where $c < p$. By employing the outer product $\hat{A} = A^{2\top} A^1 \approx A$ as a proxy for a full-weight matrix, we can effectively reduce the number of parameters. 
% In image datasets, neighbouring pixels often exhibit high correlation, making parameter-sharing techniques like convolutions more effective than fully connected neural networks. Consequently, this low-rank approximation can be regarded as a form of parameter sharing specifically tailored for image datasets.

In Table~\ref{low rank}, we present the effectiveness of employing a low-rank approximation on the identical subset of 500 samples, as depicted in Table 3 in the main manuscript. This technique ensures a minimal number of parameters while achieving satisfactory generalisation. During the experimental setup, we endeavoured to utilise the lowest feasible value of $c$ (chosen via a hyperparameter grid of $\{4,8,12\}$) that would still yield reliable generalisation. Notably, our approach achieves good results while providing superior control over the parameters, in contrast to the shared auxiliary variable method.

\paragraph{Learning without auxiliary points} We also present a comparison of another variation of our method, i.e., learning without the use of auxiliary variables. Notably, the sharing of auxiliary variables yields the best results, followed by the utilisation of low-rank matrices in conjunction with auxiliary variables. Whilst the results obtained with low matrices are not significantly different from those obtained with shared auxiliary variables, they offer a further reduction in parameters. As seen in Table~\ref{low rank}, not using auxiliary variables causes a high number of parameters and the model overfits easily causing comparatively poor results (notably fewer parameters than FFJORD, depicted in Table~\ref{low data regimes} in the main manuscript, while achieving somewhat similar performance).
% We posit that these findings can be extended to image datasets, where parameter sharing has proven to be advantageous.

\begin{table}

  \caption{Results on a small subset of 500 examples. Log-Likelihood in nats, Higher the better}
  \label{low rank}
  \centering
  \begin{tabular}{llll}
    \toprule
%    \multicolumn{3}{c}{On a subset of 500 examples}                   \\
 %   \cmidrule(r){1-3}
    Dataset  &  shared auxiliary (\#params)  & low rank (\#params)  & no auxiliary (\#params)\\% & FFJORD (\#params) \\ % & FFJORD(params) \\ 
    \midrule
    Miniboone & -27.75 (58K)  & -28.53 (19K) & -41.83 (345K) \\%& 39.92  (821K)\\
    Hepmass  &  -27.90 (41K)  & -27.91 (14K) & -29.01 (126K)\\% & 28.17  (197K)\\
    Gas & +0.22 (11K)    & -1.85 (6K) & -10.19 (32K)\\ %& 7.50  (279K)\\
    Power & -2.91 (8K)    & -3.13 (6K) & -9.26 (36K)\\ %%& 11.33 (43K)\\
    BSDS300 & +121.22 (85K)    & +111.91 (17K) & +109.265 (505K) \\%&-100.32 (3,127K) \\
    \bottomrule
  \end{tabular}
\end{table}

\section{Density estimation on real world medical dataset}
Following our experiments within the low-data regime, as discussed in Section~\ref{sec:low data}, we extend our analysis to a real-world medical dataset‚Äîthe UK Biobank \citep{8f1d51a8c3974c15a75b78320bffac08, kirchler2022training}. This dataset encompasses phenotype and genotype information for a substantial cross-section of the UK population, encompassing a total of 30 biomarkers. Notably, only 3,240 individuals within the dataset possess complete information on all biomarkers.



In line with our experiments in Section~\ref{sec:low data}, we conducted a comparative analysis between our kernelised-RealNVP and FFJORD. The density estimation results presented in Table~\ref{table:medical data} exhibit better performance whilst training significantly faster, thereby reinforcing the findings in Table~\ref{low data regimes}. 
\begin{table}[!htbp]%{r}{0.40\linewidth}
  %\vspace{-1}
  \caption{Results on the UKBiobank's biomarker data. Log Likelihood in nats, higher is better}
  \label{table:medical data}
  \centering
  \begin{tabular}{llll}
    \toprule
%    \multicolumn{3}{c}{On toy datasets}                   \\
    %\cmidrule(r){1-3}
    Method     & Ours & params & train time \\ 
    \midrule
   \textbf{ Ours}   & \textbf{-29.11} & \textbf{41K} & \textbf{21 min}\\
    FFJORD  & -31.01  & 1.1M & 13.1 hrs \\
    \bottomrule
  \end{tabular}
\end{table}

\section{Discussion on our use of kernels}

\paragraph{Effect of kernel length scale} As with any kernel machine, the length scale serves as a highly sensitive hyperparameter in our method. During our investigation, we discovered that in identical settings, distinct kernel length scales produce varying outcomes. Certain scales have a tendency to overfit easily on the training set (e.g., -3.76 nats training likelihood on Miniboone), while some tend to underfit (e.g.,-40.23 nats training likelihood on Miniboone). This diverges from neural-net-based flows, where overfitting necessitates additional layers or nodes within each layer (bearing in mind that this results in an increase in parameters unlike our method). Such findings vividly demonstrate the high expressiveness of kernelisation in flow-based models.

\paragraph{Composite kernels} In our experiments, we mostly employed the Squared Exponential Kernel/RBF, Matern kernels. Nevertheless, it is feasible to employ a combination of kernels, also known as multiple kernel learning \citep{sonnenburg2006large}. We defer the comprehensive analysis of kernel composition and its application to future endeavours. 

\section{Runtime comparisons}
\label{sec:run time}


% Figure environment removed

\begin{table}[!htbp]
%\parbox{.54\linewidth}{

  \caption{Training time, hours and minutes}
  \label{run time}
  \centering
  \begin{tabular}{lllll}
    \toprule
%    \multicolumn{3}{c}{On a subset of 500 examples}                   \\
 %   \cmidrule(r){1-3}
    Dataset     & Glow  & Ours  & FFJORD & GF \\ 
    \midrule
    Miniboone   & 63m  &51m & 5h 21m & 4h 37 m\\ 
    Hepmass     & 8h 23m  &8h 49m & > 1 day & > 1 day\\
    Gas     & 8h 45m  &9h 07m & > 1 day & > 1 day\\
    Power     & 8h 13m  &8h 50m & > 1 day & > 1 day\\
    BSDS300    & 10h 57m  &10 49m & > 1 day & > 1 day \\
    \bottomrule
  \end{tabular}
\end{table}

We picked the best models in each category from Table~\ref{tab:nats} and compared them with our kernelised Glow model. For fair comparisons, we ran the models for the same number of epochs and used the same number of flow steps for the neural-net counterpart, Glow model. It is worth noting that our model has faster convergence up to 3 times than the Glow model. Table~\ref{run time} shows that despite using kernels, we have comparable run times with the neural-network counterpart. We perform significantly better than continuous time normalising flow, FFJORD, and iterative-gaussianisation-based kernel method, Gaussianisation flow, both take longer than a day on bigger datasets. 

\section{Improving MixerFlow's image generation}

% Figure environment removed

MixerFlow \citep{english2023mixerflow} is a flow architecture for image modelling. Unlike Glow-based architectures that relies on convolutional neural networks for image generation, MixerFlow offers a flexible way to integrate any flow method, making it suitable for image generation with our kernelised coupling layers.
In this section, we showcase the application of our kernelisation to the MixerFlow model on the CIFAR-10 dataset \citep{Krizhevsky2009LearningML}. We employed small models(30 layers) and trained on a single gpu. For fair comparison, we kept the same model architecture with the sole change being the replacement of the neural-network-based coupling layers with our kernelised coupling layer. Our results as shown in Table~\ref{table:mixerflow} demonstrate kernelisation can yield better result with faster convergence attributed to reduction of parameters. The generated samples can be seen in Figure~\ref{sub:cifar10nn} and ~\ref{sub:cifar10kernel}.

\begin{table}[!htbp]%{0.50\linewidth}
  \caption{Results on the CIFAR-10 dataset. Log Likelihood in nats, higher is better}
  \label{table:mixerflow}
  \centering
  \begin{tabular}{llll}
    \toprule
%    \multicolumn{3}{c}{On toy datasets}                   \\
    %\cmidrule(r){1-3}
    Method     & Ours & params & convergence step \\ 
    \midrule
   \textbf{ Ours(kernelised)}   & \textbf{-7644.39} & \textbf{4.85M} & \textbf{103K}\\
    NN-based MixerFlow  & -7665.65  & 11.43M & 195K\\
    \bottomrule
  \end{tabular}
\end{table}



\section{Improving VAE's ELBO}
\label{sec:vae improvement}
We also try our kernelised flows in hybrid settings, demonstrating that we can integrate them with neural-net-based architectures.  We apply our kernelised model, FF-GLow, to the variational autoencoder \citep{kingma2022autoencoding} in the form of flexible prior and approximate posterior distributions. We apply the methods to Kuzushiji-MNIST, which is a variant of MNIST containing Japanese script. We investigate the capacity of our kernelisation to improve over the baseline of standard-normal prior and diagonal-normal approximate posterior, and its neural network counterpart, Glow. We use 6 flow steps for each flow-based model and the latent hidden dimension equals 16. The quantitative results are shown in Table~\ref{vae improvement} and generated image samples in Figure~\ref{fig: vae samples} 

Both models (Glow and ours FF-Glow) improve significantly over the standard baseline. However, there is no considerable quantitative gain by using the kernelised version. We believe that this might be due to the Glow model being sufficient to model the latent space on the dataset and having a little margin for kernelisation to shine. However, our kernelisation still helps in making the model parameter efficient with only a small increase in parameter complexity compared to the baseline.

\begin{table}[!htbp]
%\begin{wraptable}{r}{0.59\linewidth}
  \caption{VAE test-set results (in nats) for the evidence lower bound (ELBO) on the Kuzushiji-Mnist dataset. Error bars correspond to two standard deviations.
}
  \label{vae improvement}
  \centering
  \begin{tabular}{llll}
    \toprule
%    \multicolumn{3}{c}{On toy datasets}                   \\
    %\cmidrule(r){1-3}
         & ELBO & $\log p(x)$ & Params\\ 
    \midrule
    Baseline   & -195.61$\pm$1.25 & -182.33$\pm$ 1.30& 1.18M \\
    \midrule
    Glow   & -189.99$\pm$1.35 & -178.89$\pm$1.25& 2.05M\\
    FF-Glow (ours)   & -189.48$\pm$1.36 & -178.54$\pm$1.25 & 1.23M\\
    \bottomrule
  \end{tabular}
%\end{wraptable}
\end{table}




\iffalse
In a limited series of experiments, we attempted a composition of the Squared Exponential kernel with the Matern32 kernel and observed a modest improvement in convergence. However, we lack substantial empirical evidence to substantiate this claim. Therefore, we defer the comprehensive analysis of kernel composition to future endeavours.
\fi



\section{Details of the datasets}
\label{sec:dataset detail}

In the following paragraphs, a brief description of the five datasets used in Table~\ref{tab:nats} (POWER, GAS, HEPMASS, MINIBOONE, BSDS300) and their preprocessing methods is provided.

\begin{table}[!htbp]
  \caption{Log-likelihood measured in nats. larger values are better. Methods prepended with FF are
our kernelised versions and the results with * are taken from existing literature. Error bars correspond to 2 standard deviations.}
  \label{tab:error bars}
  \centering
  \resizebox{\textwidth}{!}{
  \begin{tabular}{lllllll}
    \toprule
    \multicolumn{7}{c}{Datasets}                   \\
    \cmidrule(r){3-7}
    & Method     & Power     & Gas & Hepmass & Miniboone & BSDS300 \\
    \midrule
   Coupling & RealNVP & 0.17$\pm$ 0.01  & 8.33$\pm$ 0.14 & -18.7$\pm$ 0.02 & -13.55$\pm$ 0.49 & 153.28$\pm$ 1.78   \\
   & FF-RealNVP {\small(ours)}  & 0.24$\pm$ 0.01 & 9.55$\pm$0.03  & -18.20$\pm$0.04 & 1-1.19$\pm$ 0.35  &  154.30$\pm$ 2.11   \\
   &  Glow     & 0.17$\pm$ 0.01  & 8.15$\pm$ 0.40 & -18.92$\pm$0.08 & -11.35$\pm$0.07 & 155.07 $\pm$ 1.03  \\
   & FF-Glow {\small(ours)} & 0.35$\pm$ 0.01 & 10.75$\pm$ 0.02  & -17.11$\pm$0.02 & -10.76$\pm$0.44 & 154.71$\pm$ 0.28  \\
    %\cmidrule(r){1-7}
    \midrule
    
   Non-coupling & MADE* & -3.08 $\pm$ 0.03& 3.56$\pm$ 0.04 & -20.98$\pm$ 0.02 & -15.59$\pm$ 0.50 & 148.85$\pm$ 0.28    \\
   & MAF     & 0.24$\pm$ 0.01  & 10.08$\pm$ 0.02 & -17.70$\pm$ 0.02 & -11.75$\pm$ 0.44 & 155.69 $\pm$ 0.28   \\
   & FFJORD     & 0.46 $\pm$ 0.01 & 8.59$\pm$ 0.12 & -14.92 $\pm$ 0.08 & -10.43$\pm$ 0.04 & 157.40$\pm$ 0.19    \\
    \bottomrule
  \end{tabular}
  }
  
\end{table}



\paragraph{POWER:} The POWER dataset comprises measurements of electric power consumption in a household spanning 47 months. Although it is essentially a time series, each example is treated as an independent and identically distributed (i.i.d.) sample from the marginal distribution. The time component was converted into an integer representing the number of minutes in a day, followed by the addition of uniform random noise. The date information is omitted and the global reactive power parameter, as it had numerous zero values that could potentially introduce large spikes in the learned distribution. Uniform random noise was also added to each feature within the interval [0, $\epsilon_i$], where $\epsilon_i$ is chosen to ensure that there are likely no duplicate values for the i-th feature while maintaining the integrity of the data values.

\paragraph{GAS:} The GAS dataset records readings from an array of 16 chemical sensors exposed to gas mixtures over a 12-hour period. Like the POWER dataset, it is essentially a time series but was treated as if each example followed an i.i.d. distribution. The data selected represents a mixture of ethylene and carbon monoxide. After removing strongly correlated attributes, the dataset's dimensionality was reduced to 8.

\paragraph{HEPMASS:} The HEPMASS dataset characterizes particle collisions in high-energy physics. Half of the data correspond to particle-producing collisions (positive), while the remaining data originate from a background source (negative). In this analysis, we utilized the positive examples from the "1000" dataset, where the particle mass is set to 1000. To prevent density spikes and misleading results, five features with frequently recurring values were excluded.

\paragraph{MINIBOONE:} The MINIBOONE dataset is derived from the MiniBooNE experiment at Fermilab. Similar to HEPMASS, it comprises positive examples (electron neutrinos) and negative examples (muon neutrinos). In this case, only the positive examples were employed. Some evident outliers (11) with values consistently set to -1000 across all columns were identified and removed. Additionally, seven other features underwent preprocessing to enhance data quality.

\paragraph{BSDS300:}The dataset was created by selecting random 8x8 monochrome patches from the BSDS300 dataset, which contains natural images. Initially, uniform noise was introduced to dequantize the pixel values, after which they were rescaled to fall within the range [0, 1]. Furthermore, the average pixel value was subtracted from each patch, and the pixel located in the bottom-right corner was omitted.





\iffalse
\section{Julia code for our kernel block}


% Figure environment removed
\fi

% Figure environment removed

% Figure environment removed

% Figure environment removed




\end{document}