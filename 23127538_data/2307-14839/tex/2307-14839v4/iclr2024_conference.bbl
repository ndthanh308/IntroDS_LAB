\begin{thebibliography}{43}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Behrmann et~al.(2019)Behrmann, Grathwohl, Chen, Duvenaud, and Jacobsen]{behrmann2019invertible}
Jens Behrmann, Will Grathwohl, Ricky T.~Q. Chen, David Duvenaud, and Jörn-Henrik Jacobsen.
\newblock Invertible residual networks, 2019.

\bibitem[Bycroft et~al.(2018)Bycroft, Freeman, Petkova, Band, Elliott, Sharp, Motyer, Vukcevic, Delaneau, O{\textquoteright}Connell, Cortes, Welsh, Young, Effingham, McVean, Leslie, Allen, Donnelly, and Marchini]{8f1d51a8c3974c15a75b78320bffac08}
Clare Bycroft, Colin Freeman, Desislava Petkova, Gavin Band, {Lloyd T.} Elliott, Kevin Sharp, Allan Motyer, Damjan Vukcevic, Olivier Delaneau, Jared O{\textquoteright}Connell, Adrian Cortes, Samantha Welsh, Alan Young, Mark Effingham, Gil McVean, Stephen Leslie, Naomi Allen, Peter Donnelly, and Jonathan Marchini.
\newblock The uk biobank resource with deep phenotyping and genomic data.
\newblock \emph{Nature}, 562\penalty0 (7726):\penalty0 203--209, October 2018.
\newblock ISSN 0028-0836.
\newblock \doi{10.1038/s41586-018-0579-z}.
\newblock Funding Information: Acknowledgements We acknowledge Wellcome Trust Core Awards 090532/Z/09/Z and 203141/Z/16/Z and grants 095552/Z/11/Z (to P.D.), 100956/Z/13/Z (to G.M.) and 100308/Z/12/Z (to A.C.). J.M. is supported by European Research Council grant 617306. S.L. is supported by Australian NHMRC Career Development Fellowship 1053756. The sample processing and genotyping was supported by the National Institute for Health Research, Medical Research Council, and British Heart Foundation. We thank the Research Computing Core at the Wellcome Centre for Human Genetics for assistance with the computational workload. We thank Affymetrix for discussions concerning quality control. We thank A. Young, A. Dilthey and L. Moutsianas for their assistance with aspects of the data analysis. We acknowledge UK Biobank co-ordinating centre staff for their role in extracting the DNA for this project. We thank M. Kuzma-Kuzniarska (http://mybioscience. org/) for Fig. 1. Publisher Copyright: {\textcopyright} 2018,
  Springer Nature Limited.

\bibitem[Chen \& Gopinath(2000)Chen and Gopinath]{chen2000gaussianization}
Scott Chen and Ramesh Gopinath.
\newblock Gaussianization.
\newblock \emph{Advances in neural information processing systems}, 13, 2000.

\bibitem[Dai \& Seljak(2020{\natexlab{a}})Dai and Seljak]{DBLP:journals/corr/abs-2007-00674}
Biwei Dai and Uros Seljak.
\newblock Sliced iterative generator.
\newblock \emph{CoRR}, abs/2007.00674, 2020{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2007.00674}.

\bibitem[Dai \& Seljak(2020{\natexlab{b}})Dai and Seljak]{Dai2020SlicedIN}
Biwei Dai and Uros Seljak.
\newblock Sliced iterative normalizing flows.
\newblock In \emph{International Conference on Machine Learning}, 2020{\natexlab{b}}.

\bibitem[Damianou \& Lawrence(2013)Damianou and Lawrence]{damianou2013deep}
Andreas~C. Damianou and Neil~D. Lawrence.
\newblock Deep gaussian processes, 2013.

\bibitem[Dinh et~al.(2017)Dinh, Sohl-Dickstein, and Bengio]{dinh2017density}
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio.
\newblock Density estimation using real nvp, 2017.

\bibitem[Dua \& Graff(2017)Dua and Graff]{Dua:2019}
Dheeru Dua and Casey Graff.
\newblock {UCI} machine learning repository, 2017.
\newblock URL \url{http://archive.ics.uci.edu/ml}.

\bibitem[Durkan et~al.(2019)Durkan, Bekasov, Murray, and Papamakarios]{durkan2019neural}
Conor Durkan, Artur Bekasov, Iain Murray, and George Papamakarios.
\newblock Neural spline flows, 2019.

\bibitem[English et~al.(2023)English, Kirchler, and Lippert]{english2023mixerflow}
Eshant English, Matthias Kirchler, and Christoph Lippert.
\newblock Mixerflow for image modelling, 2023.

\bibitem[Germain et~al.(2015)Germain, Gregor, Murray, and Larochelle]{germain2015made}
Mathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle.
\newblock Made: Masked autoencoder for distribution estimation, 2015.

\bibitem[Grathwohl et~al.(2018)Grathwohl, Chen, Bettencourt, Sutskever, and Duvenaud]{grathwohl2018ffjord}
Will Grathwohl, Ricky T.~Q. Chen, Jesse Bettencourt, Ilya Sutskever, and David Duvenaud.
\newblock Ffjord: Free-form continuous dynamics for scalable reversible generative models, 2018.

\bibitem[Hansen et~al.(2022)Hansen, Manzo, and Regier]{hansen2022normalizing}
Derek Hansen, Brian Manzo, and Jeffrey Regier.
\newblock Normalizing flows for knockoff-free controlled feature selection.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 16125--16137, 2022.

\bibitem[Ho et~al.(2019)Ho, Chen, Srinivas, Duan, and Abbeel]{ho2019flow++}
Jonathan Ho, Xi~Chen, Aravind Srinivas, Yan Duan, and Pieter Abbeel.
\newblock Flow++: Improving flow-based generative models with variational dequantization and architecture design.
\newblock In \emph{International Conference on Machine Learning}, pp.\  2722--2730. PMLR, 2019.

\bibitem[Huang et~al.(2021)Huang, Chakraborty, and Singh]{huang2021forward}
Zhichun Huang, Rudrasis Chakraborty, and Vikas Singh.
\newblock Forward operator estimation in generative models with kernel transfer operators, 2021.

\bibitem[Karras et~al.(2019)Karras, Laine, and Aila]{karras2019stylebased}
Tero Karras, Samuli Laine, and Timo Aila.
\newblock A style-based generator architecture for generative adversarial networks, 2019.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kingma \& Dhariwal(2018)Kingma and Dhariwal]{kingma2018glow}
Diederik~P. Kingma and Prafulla Dhariwal.
\newblock Glow: Generative flow with invertible 1x1 convolutions, 2018.

\bibitem[Kingma \& Welling(2022)Kingma and Welling]{kingma2022autoencoding}
Diederik~P Kingma and Max Welling.
\newblock Auto-encoding variational bayes, 2022.

\bibitem[Kirchler et~al.(2022)Kirchler, Lippert, and Kloft]{kirchler2022training}
Matthias Kirchler, Christoph Lippert, and Marius Kloft.
\newblock Training normalizing flows from dependent data.
\newblock \emph{arXiv preprint arXiv:2209.14933}, 2022.

\bibitem[Kobyzev et~al.(2021)Kobyzev, Prince, and Brubaker]{Kobyzev_2021}
Ivan Kobyzev, Simon~J.D. Prince, and Marcus~A. Brubaker.
\newblock Normalizing flows: An introduction and review of current methods.
\newblock \emph{{IEEE} Transactions on Pattern Analysis and Machine Intelligence}, 43\penalty0 (11):\penalty0 3964--3979, nov 2021.
\newblock \doi{10.1109/tpami.2020.2992934}.
\newblock URL \url{https://doi.org/10.1109%2Ftpami.2020.2992934}.

\bibitem[Krizhevsky(2009{\natexlab{a}})]{Krizhevsky09learningmultiple}
Alex Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, 2009{\natexlab{a}}.

\bibitem[Krizhevsky(2009{\natexlab{b}})]{Krizhevsky2009LearningML}
Alex Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009{\natexlab{b}}.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:18268744}.

\bibitem[Laparra et~al.(2011)Laparra, Camps-Valls, and Malo]{Laparra_2011}
V~Laparra, G~Camps-Valls, and J~Malo.
\newblock Iterative gaussianization: From {ICA} to random rotations.
\newblock \emph{{IEEE} Transactions on Neural Networks}, 22\penalty0 (4):\penalty0 537--549, apr 2011.
\newblock \doi{10.1109/tnn.2011.2106511}.
\newblock URL \url{https://doi.org/10.1109%2Ftnn.2011.2106511}.

\bibitem[Lee et~al.(2020)Lee, Kim, and Yoon]{lee2020nanoflow}
Sanggil Lee, Sungwon Kim, and Sungroh Yoon.
\newblock Nanoflow: Scalable normalizing flows with sublinear parameter complexity, 2020.

\bibitem[Maroñas et~al.(2021)Maroñas, Hamelijnck, Knoblauch, and Damoulas]{maroñas2021transforming}
Juan Maroñas, Oliver Hamelijnck, Jeremias Knoblauch, and Theodoros Damoulas.
\newblock Transforming gaussian processes with normalizing flows, 2021.

\bibitem[Marteau-Ferey et~al.(2021)Marteau-Ferey, Bach, and Rudi]{marteauferey2021sampling}
Ulysse Marteau-Ferey, Francis Bach, and Alessandro Rudi.
\newblock Sampling from arbitrary functions via psd models, 2021.

\bibitem[Meng et~al.(2020)Meng, Song, Song, and Ermon]{meng2020gaussianization}
Chenlin Meng, Yang Song, Jiaming Song, and Stefano Ermon.
\newblock Gaussianization flows, 2020.

\bibitem[Meng et~al.(2022)Meng, Zhou, Choi, Dao, and Ermon]{pmlr-v162-meng22a}
Chenlin Meng, Linqi Zhou, Kristy Choi, Tri Dao, and Stefano Ermon.
\newblock {B}utterfly{F}low: Building invertible layers with butterfly matrices.
\newblock In Kamalika Chaudhuri, Stefanie Jegelka, Le~Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), \emph{Proceedings of the 39th International Conference on Machine Learning}, volume 162 of \emph{Proceedings of Machine Learning Research}, pp.\  15360--15375. PMLR, 17--23 Jul 2022.
\newblock URL \url{https://proceedings.mlr.press/v162/meng22a.html}.

\bibitem[Papamakarios et~al.(2018)Papamakarios, Pavlakou, and Murray]{papamakarios2018masked}
George Papamakarios, Theo Pavlakou, and Iain Murray.
\newblock Masked autoregressive flow for density estimation, 2018.

\bibitem[Papamakarios et~al.(2021)Papamakarios, Nalisnick, Rezende, Mohamed, and Lakshminarayanan]{papamakarios2021normalizing}
George Papamakarios, Eric Nalisnick, Danilo~Jimenez Rezende, Shakir Mohamed, and Balaji Lakshminarayanan.
\newblock Normalizing flows for probabilistic modeling and inference, 2021.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan, Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Platt(1998)]{platt1998sequential}
John Platt.
\newblock Sequential minimal optimization: A fast algorithm for training support vector machines.
\newblock 1998.

\bibitem[Qui{{\~n}}onero-Candela \& Rasmussen(2005)Qui{{\~n}}onero-Candela and Rasmussen]{JMLR:v6:quinonero-candela05a}
Joaquin Qui{{\~n}}onero-Candela and Carl~Edward Rasmussen.
\newblock A unifying view of sparse approximate gaussian process regression.
\newblock \emph{Journal of Machine Learning Research}, 6\penalty0 (65):\penalty0 1939--1959, 2005.
\newblock URL \url{http://jmlr.org/papers/v6/quinonero-candela05a.html}.

\bibitem[Rudi \& Ciliberto(2021)Rudi and Ciliberto]{rudi2021psd}
Alessandro Rudi and Carlo Ciliberto.
\newblock Psd representations for effective probability models, 2021.

\bibitem[Sch{\"o}lkopf et~al.(2001)Sch{\"o}lkopf, Herbrich, and Smola]{scholkopf2001generalized}
Bernhard Sch{\"o}lkopf, Ralf Herbrich, and Alex~J Smola.
\newblock A generalized representer theorem.
\newblock In \emph{Computational Learning Theory: 14th Annual Conference on Computational Learning Theory, COLT 2001 and 5th European Conference on Computational Learning Theory, EuroCOLT 2001 Amsterdam, The Netherlands, July 16--19, 2001 Proceedings 14}, pp.\  416--426. Springer, 2001.

\bibitem[Sch{\"o}lkopf et~al.(2002)Sch{\"o}lkopf, Smola, Bach, et~al.]{scholkopf2002learning}
Bernhard Sch{\"o}lkopf, Alexander~J Smola, Francis Bach, et~al.
\newblock \emph{Learning with kernels: support vector machines, regularization, optimization, and beyond}.
\newblock MIT press, 2002.

\bibitem[Sonnenburg et~al.(2006)Sonnenburg, R{\"a}tsch, Sch{\"a}fer, and Sch{\"o}lkopf]{sonnenburg2006large}
S{\"o}ren Sonnenburg, Gunnar R{\"a}tsch, Christin Sch{\"a}fer, and Bernhard Sch{\"o}lkopf.
\newblock Large scale multiple kernel learning.
\newblock \emph{The Journal of Machine Learning Research}, 7:\penalty0 1531--1565, 2006.

\bibitem[Sukthanker et~al.(2022)Sukthanker, Huang, Kumar, Timofte, and Gool]{sukthanker2022generative}
Rhea~Sanjay Sukthanker, Zhiwu Huang, Suryansh Kumar, Radu Timofte, and Luc~Van Gool.
\newblock Generative flows with invertible attentions, 2022.

\bibitem[Tsuchida et~al.(2023)Tsuchida, Ong, and Sejdinovic]{tsuchida2023squared}
Russell Tsuchida, Cheng~Soon Ong, and Dino Sejdinovic.
\newblock Squared neural families: A new class of tractable density models, 2023.

\bibitem[Vishwanathan et~al.(2010)Vishwanathan, Schraudolph, Kondor, and Borgwardt]{vishwanathan2010graph}
S~Vichy~N Vishwanathan, Nicol~N Schraudolph, Risi Kondor, and Karsten~M Borgwardt.
\newblock Graph kernels.
\newblock \emph{Journal of Machine Learning Research}, 11:\penalty0 1201--1242, 2010.

\bibitem[Wenliang et~al.(2019)Wenliang, Sutherland, Strathmann, and Gretton]{wenliang2019learning}
Li~Wenliang, Danica~J Sutherland, Heiko Strathmann, and Arthur Gretton.
\newblock Learning deep kernels for exponential family densities.
\newblock In \emph{International Conference on Machine Learning}, pp.\  6737--6746. PMLR, 2019.

\bibitem[Wilson et~al.(2016)Wilson, Hu, Salakhutdinov, and Xing]{wilson2016deep}
Andrew~Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric~P Xing.
\newblock Deep kernel learning.
\newblock In \emph{Artificial intelligence and statistics}, pp.\  370--378. PMLR, 2016.

\end{thebibliography}
