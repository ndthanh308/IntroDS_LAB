\begin{thebibliography}{37}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Behrmann et~al.(2019)Behrmann, Grathwohl, Chen, Duvenaud, and
  Jacobsen]{behrmann2019invertible}
J.~Behrmann, W.~Grathwohl, R.~T.~Q. Chen, D.~Duvenaud, and J.-H. Jacobsen.
\newblock Invertible residual networks, 2019.

\bibitem[Chen and Gopinath(2000)]{chen2000gaussianization}
S.~Chen and R.~Gopinath.
\newblock Gaussianization.
\newblock \emph{Advances in neural information processing systems}, 13, 2000.

\bibitem[Dai and Seljak(2020{\natexlab{a}})]{DBLP:journals/corr/abs-2007-00674}
B.~Dai and U.~Seljak.
\newblock Sliced iterative generator.
\newblock \emph{CoRR}, abs/2007.00674, 2020{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2007.00674}.

\bibitem[Dai and Seljak(2020{\natexlab{b}})]{Dai2020SlicedIN}
B.~Dai and U.~Seljak.
\newblock Sliced iterative normalizing flows.
\newblock In \emph{International Conference on Machine Learning},
  2020{\natexlab{b}}.

\bibitem[Damianou and Lawrence(2013)]{damianou2013deep}
A.~C. Damianou and N.~D. Lawrence.
\newblock Deep gaussian processes, 2013.

\bibitem[Dinh et~al.(2017)Dinh, Sohl-Dickstein, and Bengio]{dinh2017density}
L.~Dinh, J.~Sohl-Dickstein, and S.~Bengio.
\newblock Density estimation using real nvp, 2017.

\bibitem[Dua and Graff(2017)]{Dua:2019}
D.~Dua and C.~Graff.
\newblock {UCI} machine learning repository, 2017.
\newblock URL \url{http://archive.ics.uci.edu/ml}.

\bibitem[Durkan et~al.(2019)Durkan, Bekasov, Murray, and
  Papamakarios]{durkan2019neural}
C.~Durkan, A.~Bekasov, I.~Murray, and G.~Papamakarios.
\newblock Neural spline flows, 2019.

\bibitem[Germain et~al.(2015)Germain, Gregor, Murray, and
  Larochelle]{germain2015made}
M.~Germain, K.~Gregor, I.~Murray, and H.~Larochelle.
\newblock Made: Masked autoencoder for distribution estimation, 2015.

\bibitem[Grathwohl et~al.(2018)Grathwohl, Chen, Bettencourt, Sutskever, and
  Duvenaud]{grathwohl2018ffjord}
W.~Grathwohl, R.~T.~Q. Chen, J.~Bettencourt, I.~Sutskever, and D.~Duvenaud.
\newblock Ffjord: Free-form continuous dynamics for scalable reversible
  generative models, 2018.

\bibitem[Hansen et~al.(2022)Hansen, Manzo, and Regier]{hansen2022normalizing}
D.~Hansen, B.~Manzo, and J.~Regier.
\newblock Normalizing flows for knockoff-free controlled feature selection.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 16125--16137, 2022.

\bibitem[Ho et~al.(2019)Ho, Chen, Srinivas, Duan, and Abbeel]{ho2019flow++}
J.~Ho, X.~Chen, A.~Srinivas, Y.~Duan, and P.~Abbeel.
\newblock Flow++: Improving flow-based generative models with variational
  dequantization and architecture design.
\newblock In \emph{International Conference on Machine Learning}, pages
  2722--2730. PMLR, 2019.

\bibitem[Huang et~al.(2021)Huang, Chakraborty, and Singh]{huang2021forward}
Z.~Huang, R.~Chakraborty, and V.~Singh.
\newblock Forward operator estimation in generative models with kernel transfer
  operators, 2021.

\bibitem[Karras et~al.(2019)Karras, Laine, and Aila]{karras2019stylebased}
T.~Karras, S.~Laine, and T.~Aila.
\newblock A style-based generator architecture for generative adversarial
  networks, 2019.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
D.~P. Kingma and J.~Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kingma and Dhariwal(2018)]{kingma2018glow}
D.~P. Kingma and P.~Dhariwal.
\newblock Glow: Generative flow with invertible 1x1 convolutions, 2018.

\bibitem[Kingma and Welling(2022)]{kingma2022autoencoding}
D.~P. Kingma and M.~Welling.
\newblock Auto-encoding variational bayes, 2022.

\bibitem[Kirchler et~al.(2022)Kirchler, Lippert, and
  Kloft]{kirchler2022training}
M.~Kirchler, C.~Lippert, and M.~Kloft.
\newblock Training normalizing flows from dependent data.
\newblock \emph{arXiv preprint arXiv:2209.14933}, 2022.

\bibitem[Kobyzev et~al.(2021)Kobyzev, Prince, and Brubaker]{Kobyzev_2021}
I.~Kobyzev, S.~J. Prince, and M.~A. Brubaker.
\newblock Normalizing flows: An introduction and review of current methods.
\newblock \emph{{IEEE} Transactions on Pattern Analysis and Machine
  Intelligence}, 43\penalty0 (11):\penalty0 3964--3979, nov 2021.
\newblock \doi{10.1109/tpami.2020.2992934}.
\newblock URL \url{https://doi.org/10.1109%2Ftpami.2020.2992934}.

\bibitem[Krizhevsky(2009)]{Krizhevsky09learningmultiple}
A.~Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, 2009.

\bibitem[Laparra et~al.(2011)Laparra, Camps-Valls, and Malo]{Laparra_2011}
V.~Laparra, G.~Camps-Valls, and J.~Malo.
\newblock Iterative gaussianization: From {ICA} to random rotations.
\newblock \emph{{IEEE} Transactions on Neural Networks}, 22\penalty0
  (4):\penalty0 537--549, apr 2011.
\newblock \doi{10.1109/tnn.2011.2106511}.
\newblock URL \url{https://doi.org/10.1109%2Ftnn.2011.2106511}.

\bibitem[Lee et~al.(2020)Lee, Kim, and Yoon]{lee2020nanoflow}
S.~Lee, S.~Kim, and S.~Yoon.
\newblock Nanoflow: Scalable normalizing flows with sublinear parameter
  complexity, 2020.

\bibitem[Maro単as et~al.(2021)Maro単as, Hamelijnck, Knoblauch, and
  Damoulas]{maro単as2021transforming}
J.~Maro単as, O.~Hamelijnck, J.~Knoblauch, and T.~Damoulas.
\newblock Transforming gaussian processes with normalizing flows, 2021.

\bibitem[Meng et~al.(2020)Meng, Song, Song, and Ermon]{meng2020gaussianization}
C.~Meng, Y.~Song, J.~Song, and S.~Ermon.
\newblock Gaussianization flows, 2020.

\bibitem[Meng et~al.(2022)Meng, Zhou, Choi, Dao, and Ermon]{pmlr-v162-meng22a}
C.~Meng, L.~Zhou, K.~Choi, T.~Dao, and S.~Ermon.
\newblock {B}utterfly{F}low: Building invertible layers with butterfly
  matrices.
\newblock In K.~Chaudhuri, S.~Jegelka, L.~Song, C.~Szepesvari, G.~Niu, and
  S.~Sabato, editors, \emph{Proceedings of the 39th International Conference on
  Machine Learning}, volume 162 of \emph{Proceedings of Machine Learning
  Research}, pages 15360--15375. PMLR, 17--23 Jul 2022.
\newblock URL \url{https://proceedings.mlr.press/v162/meng22a.html}.

\bibitem[Papamakarios et~al.(2018)Papamakarios, Pavlakou, and
  Murray]{papamakarios2018masked}
G.~Papamakarios, T.~Pavlakou, and I.~Murray.
\newblock Masked autoregressive flow for density estimation, 2018.

\bibitem[Papamakarios et~al.(2021)Papamakarios, Nalisnick, Rezende, Mohamed,
  and Lakshminarayanan]{papamakarios2021normalizing}
G.~Papamakarios, E.~Nalisnick, D.~J. Rezende, S.~Mohamed, and
  B.~Lakshminarayanan.
\newblock Normalizing flows for probabilistic modeling and inference, 2021.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
A.~Paszke, S.~Gross, F.~Massa, A.~Lerer, J.~Bradbury, G.~Chanan, T.~Killeen,
  Z.~Lin, N.~Gimelshein, L.~Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Platt(1998)]{platt1998sequential}
J.~Platt.
\newblock Sequential minimal optimization: A fast algorithm for training
  support vector machines.
\newblock 1998.

\bibitem[Qui{{\~n}}onero-Candela and
  Rasmussen(2005)]{JMLR:v6:quinonero-candela05a}
J.~Qui{{\~n}}onero-Candela and C.~E. Rasmussen.
\newblock A unifying view of sparse approximate gaussian process regression.
\newblock \emph{Journal of Machine Learning Research}, 6\penalty0
  (65):\penalty0 1939--1959, 2005.
\newblock URL \url{http://jmlr.org/papers/v6/quinonero-candela05a.html}.

\bibitem[Sch{\"o}lkopf et~al.(2001)Sch{\"o}lkopf, Herbrich, and
  Smola]{scholkopf2001generalized}
B.~Sch{\"o}lkopf, R.~Herbrich, and A.~J. Smola.
\newblock A generalized representer theorem.
\newblock In \emph{Computational Learning Theory: 14th Annual Conference on
  Computational Learning Theory, COLT 2001 and 5th European Conference on
  Computational Learning Theory, EuroCOLT 2001 Amsterdam, The Netherlands, July
  16--19, 2001 Proceedings 14}, pages 416--426. Springer, 2001.

\bibitem[Sch{\"o}lkopf et~al.(2002)Sch{\"o}lkopf, Smola, Bach,
  et~al.]{scholkopf2002learning}
B.~Sch{\"o}lkopf, A.~J. Smola, F.~Bach, et~al.
\newblock \emph{Learning with kernels: support vector machines, regularization,
  optimization, and beyond}.
\newblock MIT press, 2002.

\bibitem[Sonnenburg et~al.(2006)Sonnenburg, R{\"a}tsch, Sch{\"a}fer, and
  Sch{\"o}lkopf]{sonnenburg2006large}
S.~Sonnenburg, G.~R{\"a}tsch, C.~Sch{\"a}fer, and B.~Sch{\"o}lkopf.
\newblock Large scale multiple kernel learning.
\newblock \emph{The Journal of Machine Learning Research}, 7:\penalty0
  1531--1565, 2006.

\bibitem[Sukthanker et~al.(2022)Sukthanker, Huang, Kumar, Timofte, and
  Gool]{sukthanker2022generative}
R.~S. Sukthanker, Z.~Huang, S.~Kumar, R.~Timofte, and L.~V. Gool.
\newblock Generative flows with invertible attentions, 2022.

\bibitem[Vishwanathan et~al.(2010)Vishwanathan, Schraudolph, Kondor, and
  Borgwardt]{vishwanathan2010graph}
S.~V.~N. Vishwanathan, N.~N. Schraudolph, R.~Kondor, and K.~M. Borgwardt.
\newblock Graph kernels.
\newblock \emph{Journal of Machine Learning Research}, 11:\penalty0 1201--1242,
  2010.

\bibitem[Wenliang et~al.(2019)Wenliang, Sutherland, Strathmann, and
  Gretton]{wenliang2019learning}
L.~Wenliang, D.~J. Sutherland, H.~Strathmann, and A.~Gretton.
\newblock Learning deep kernels for exponential family densities.
\newblock In \emph{International Conference on Machine Learning}, pages
  6737--6746. PMLR, 2019.

\bibitem[Wilson et~al.(2016)Wilson, Hu, Salakhutdinov, and
  Xing]{wilson2016deep}
A.~G. Wilson, Z.~Hu, R.~Salakhutdinov, and E.~P. Xing.
\newblock Deep kernel learning.
\newblock In \emph{Artificial intelligence and statistics}, pages 370--378.
  PMLR, 2016.

\end{thebibliography}
