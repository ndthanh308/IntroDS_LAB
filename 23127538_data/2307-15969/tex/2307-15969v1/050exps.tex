% !TeX root = main. tex
\begin{table}[t]
    \centering
    \caption{\textnormal{Statistical information of Datasets.}}
    \label{tab:datasets}
    \vspace{-0.1in}
    \begin{tabular}{l|c|r} \toprule
      \textbf{Dataset} & \textbf{$ |\nodes|$} & \textbf{$|\edges|$} \\ \midrule
        ca-HepPh                & 12006                 & 118,489       \\
        ca-AstroPh              & 18771                 & 198,050       \\
        PP-Pathways             & 21267                 & 340,905       \\
        comm-EmailEnron         & 36692                 & 183,831       \\
        soc-Twitter\_ICWSM      & 465017                & 834,797       \\
        soc-sign\_slashdot      & 77350                 & 937,108       \\
        rating-StackOverflow    & {[}545195,96678{]}    & 1,301,942     \\
        soc-sign\_epinion       & 131828                & 1,422,420     \\
        ego-twitter             & 81306                 & 2,420,744     \\
        soc-Youtube             & 1134890               & 2,987,624     \\
        comm-WikiTalk           & 2394385               & 5,021,410     \\
        nov\_user\_msg\_time    & {[}2748001,8083629{]} & 48,078,692    \\
        cit-Patents\_AMINER     & 6840994               & 54,022,588    \\
        soc-Twitter\_ASU        & 11316811              & 85,331,843    \\
        soc-Livejournal         & {[}3201203,7489073{]} & 112,307,385   \\
        soc-Orkut               & 3072441               & 117,185,083   \\
        soc-SinaWeibo           & 58655849              & 261,321,033   \\
        \hline
        wang-tripadvisor        & {[}145316,1759{]}     & 175,765       \\
        rec-YelpUserBusiness    & {[}45982,11538{]}     & 229,906       \\
        bookcrossing            & {[}77802,185955{]}    & 433,652       \\
        librec-ciaodvd-review   & {[}21019,71633{]}     & 1,625,480     \\
        movielens-10m           & {[}69878,10677{]}     & 10,000,054    \\
        epinions                & {[}120492,755760{]}   & 13,668,320    \\
        libimseti               & 220970                & 17,359,346    \\
        rec-movielens           & {[}283228,193886{]}   & 27,748,887    \\
        yahoo-song              & {[}1000990,624961{]}  & 256,804,235   \\ \bottomrule
    \end{tabular}
\end{table}

\begin{table*}[htbp]
    \centering
    \caption{\textnormal{Running time (sec.) comparison for utilizing the pruning pre-process.}}
    \label{tab:time}
    \vspace{-0.1in}
    \resizebox{0.98\textwidth}{!}{
    \begin{tabular}{l|c|c||c|c|c||c|c||c|c} \toprule
        \textbf{Dataset}& w$\_$Pruning & uw$\_$Pruning  & BBST & Priority Tree & Pruning+PT & Max-Flow & w$\_$Pruning+M & Doubly-Linked List & uw$\_$Pruning+DLL \\ \midrule
        ca-HepPh        & 0.008           & 0.008   & 0.196    & 0.033   & \textbf{0.016}   & 1.351   & \textbf{0.145}    & 0.013   & \textbf{0.009}   \\
        comm-EmailEnron          & 0.016  & 0.017   & 0.377    & 0.066   & \textbf{0.025}   & 2.834   & \textbf{0.272}    & 0.03    & \textbf{0.019}   \\
        ca-AstroPh      & 0.013          & 0.013   & 0.354    & 0.065   & \textbf{0.043}   & 2.818   & \textbf{0.812}     & 0.025   & \textbf{0.017}   \\
        PP-Pathways  & 0.037 & 0.028  & 0.73     & 0.141   & \textbf{0.045}   & 4.92    & \textbf{0.412}      & 0.044   & \textbf{0.031}   \\
        soc-Twitter\_ICWSM       & 0.168 & 0.15  & 4.807    & 0.537   & \textbf{0.202}   & 29.824  & \textbf{0.746}       & 0.288   & \textbf{0.156}   \\
        soc-sign\_slashdot       & 0.041 & 0.041  & 1.112    & 0.187   & \textbf{0.072}   & 9.894   & \textbf{0.626}      & 0.082   & \textbf{0.046}   \\
        rating-StackOverflow     & 0.327 & 0.288  & 7.087    & 0.986   & \textbf{0.35}    & 55.803  & \textbf{0.515}      & 0.572   & \textbf{0.297}   \\
        soc-sign\_epinion        & 0.063   & 0.064  & 1.877    & 0.31    & \textbf{0.111}   & 16.354  & \textbf{0.92}     & 0.15    & \textbf{0.073}   \\
        ego-twitter     & 0.084         & 0.084   & 2.724    & 0.425   & \textbf{0.149}   & 24.217  & \textbf{1.357}      & 0.162   & \textbf{0.097}   \\
        soc-Youtube    & 0.569  & 0.577  & 18.292   & 2.035   & \textbf{0.631}   & 118.627 & \textbf{1.805}     & 1.118   & \textbf{0.585}   \\
        comm-WikiTalk   & 0.94   & 0.904   & 60.556   & 3.399   & \textbf{1.053}   & 230.949 & \textbf{2.862}     & 2.08    & \textbf{0.939}   \\
        nov\_user\_msg\_time     & 8.68  & 9.987   & 1317.44  & 58.34   & \textbf{9.268}   & $\ge$6h     & \textbf{35.823}     & 26.425  & \textbf{10.08}   \\
        cit-Patents\_AMINER   & 3.796  & 3.865    & 536.038  & 30.414  & \textbf{6.748}   & $\ge$6h     & \textbf{98.012}   & 18.005  & \textbf{4.967}   \\
        soc-Twitter\_ASU  & 6.68   & 6.867     & 1169.1   & 42.905  & \textbf{8.026}   & $\ge$6h     & \textbf{48.612}  & 20.659  & \textbf{7.22}    \\
        soc-Livejournal  & 9.676  & 10.326  & 1239.405 & 67.74   & \textbf{16.951}  & $\ge$6h     & \textbf{405.168} & 32.478  & \textbf{11.861}  \\
        soc-Orkut        & 9.693  & 10.173  & 506.536  & 80.45   & \textbf{13.225}  & 5274.2  & \textbf{339.112}   & 34.934  & \textbf{10.748}  \\
        soc-SinaWeibo    & 51.417 & 46.132  & 32197.2  & 225.065 & \textbf{56.184}  & $\ge$6h     & \textbf{252.011}   & 146.471 & \textbf{48.488}  \\
        \hline
        wang-tripadvisor       & 0.052 &     --   & 0.772    & 0.122   & \textbf{0.059}   & 6.203   & \textbf{0.278}     &  --       &  --                \\
        rec-YelpUserBusiness     & 0.026 &   --    & 0.61     & 0.105   & \textbf{0.04}    & 5.305   & \textbf{0.433}        &   --      & --                 \\
        bookcrossing       & 0.107  &    --   & 1.979    & 0.341   & \textbf{0.122}   & 17.467  & \textbf{0.426}   &    --         &    --              \\
        librec-ciaodvd-review    & 0.093  &  --   & 3.245    & 0.543   & \textbf{0.152}   & 41.812  & \textbf{1.629}        &    --     &   --               \\
        movielens-10m       & 0.3  &   --     & 18.088   & 2.781   & \textbf{1.433}   & 412.519 & \textbf{65.712}       & --         &     --             \\
        epinions     & 0.943  &   --  & 36.722   & 5.685   & \textbf{1.383}   & 624.063 & \textbf{17.495}        & --        &      --            \\
        libimseti       & 0.844  &   --   & 78.901   & 7.302   & \textbf{1.767}   & 942.125 & \textbf{67.012}        &  --       &   --               \\
        rec-movielens     & 0.98  &  --    & 27.152   & 9.493   & \textbf{3.587}   & 1376.71 & \textbf{135.278}      &   --      &      --            \\
        yahoo-song     & 10.936  & --   & 691.58   & 125.762 & \textbf{27.115}  & $\ge$6h     & \textbf{2732.31}    & --        &      --         \\ 
        \hline
        \multicolumn{10}{l}{\multirow{2}{0.98\textwidth}{\textbf{note:}`w\_' and `uw\_' denote the weighted and unweighted version; `Pruing+PT', `Pruning+M', and `Pruing+DLL' denote using the pruning to process the dataset at first and following with Priority Tree, Max-Flow, and Doubly-linked List, respectively.}} \\
        \multicolumn{10}{l}{} \\
        \bottomrule
    \end{tabular}
    }
\end{table*}

We design experiments to answer the following questions:
\begin{compactitem}
    \item{\textbf{Effectiveness Of The Pruning Technique:}} How significantly can the pruning reduce the size of the graph? How about its speedup to the approximation and exact algorithms?
    \item{\textbf{Effectiveness of \emph{\method} When Solving DSP:}} Does \emph{\method} detect the densest subgraph faster than other iterative algorithms?
    \item{\textbf{Effectiveness of \emph{\method} When Solving LDD:}} Does \emph{\method} optimize the linear programming of locally-dense decomposition faster than other baselines?
\end{compactitem}


\textbf{Datasets\&Implementation.}
We collected 26 networks from popular publicly available repositories, including Stanford’s SNAP database~\cite{jure2014snapnets}, AMiner scholar datasets~\cite{wan2019aminer}, Network Repository~\cite{nr2015aaai}, ASU’s Social Computing Data Repository~\cite{ZafaraniLiu2009}, and Konect~\cite{kunegis2013konect} etc. Multiple edges, self-loops are removed, and the directionality is ignored for directed graphs. Table~\ref{tab:datasets} lists their statistical information, where the first group (17 in total) are unweighted and the remaining are weighted. All the experiments are performed on a machine with 2.4GHz Intel(R) Xeon(R) CPU(8 cores) %20MB cache size, 64G swap size 
and 500GB of RAM. All baselines are implemented in C++ 14. The sizes of the networks range from 10 thousand to 10 million.

% Figure environment removed

\subsection{ Effectiveness of The Pruning Technique}
We choose Max-Flow\cite{goldberg1984finding}, Greedy in priority tree(PT)\cite{hooi2016fraudar}, Greedy in doubly-linked list(DLL)\cite{boob2020flowless} and Greedy in Balanced Binary Search Tree(BBST)\cite{boob2020flowless} as baseline algorithms. PT, DL, BBST are different data structures used in Greedy, DLL can only be applied on unweighted graphs while BBST and PT can be applied on weighted graphs. We implement Max-Flow, Greedy in PT by ourselves, the code of Greedy in DLL and BBST is from \cite{boob2020flowless} because they optimize it enough well. 

Although Core-App\cite{fang2019efficient} can attain $k_{max}$-core efficiently which is also 1/2-approximation like Greedy, its density is no more than the density attained by Greedy because Greedy can choose the subgraph with the maximum density and $k_{max}$-core is attained at Greedy's some time according to Lemma \ref{lem:k-core}. Therefore, among approximation algorithms, we only select Greedy in different data structures because they can achieve
equivalent results.

Figure~\ref{fig:pruning} compares the size of graphs before and after the pruning pre-process. We can see that applying the pruning leads to about two orders of magnitude of reduction for the size of all networks. The effect of pruning is more obvious on larger graphs.


\begin{table*}[t]
    \centering
    \caption{Comparison between \emph{\method} and baselines.}
    \label{tab:lowd}
    \vspace{-0.1in}
    \resizebox{0.95\textwidth}{!}{
        \begin{tabular}{l|l|l|l|l|l|l|l|l|l|l|l}\toprule
        \multicolumn{2}{c}{}&\multicolumn{5}{|c|}{Running time}&\multicolumn{5}{c}{The number of iteration rounds}\\
        \midrule
        \textbf{Dataset} &
        \textbf{$|\mathcal{H}^{\prime}|$} & \textbf{LOWD} & \textbf{Greedy++} & \textbf{FW} & \textbf{FISTA} & \textbf{MWU} & \textbf{LOWD} & \textbf{Greedy++} & \textbf{FW} & \textbf{FISTA} & \textbf{MWU}\\
        \midrule
        ca-HepPh                 & 239          & 0.0019      & \first{0.0013}     & 0.0018        & 0.0041    & \second{0.0014}    & 1             & 1            & 1               & 1         & 1       \\
        comm-EmailEnron          & 592          & \first{0.0115}      & 0.0187     & 0.0645        & 0.2417    & \second{0.0163}    & 13            & 10           & 78              & 109       & 19      \\
        ca-AstroPh               & 2441         & \first{0.1202}      & \second{0.1759}     & 0.7566        & 1.2247    & 0.2125    & 36            & 28           & 345             & 176       & 83      \\
        PP-Pathways                    & 527          & \second{0.0089}      & \first{0.0029}     & 0.0354        & 0.0994    & 0.0241    & 6             & 1            & 24              & 28        & 16      \\
        soc-Twitter\_ICWSM            & 2423         & \first{0.0944}      & 0.5038     & 4.9809        & 1.4450     & 0.9862    & 39            & 120          & 2820            & 256       & 573     \\
        soc-sign\_slashdot       & 1709         & \second{0.0255}      & \first{0.0045}     & 0.0807        & 0.3096    & 0.0361    & 10            & 1            & 42              & 50        & 18      \\
        rating-StackOverflow   & 1008         & \first{0.0199}      & 0.0666     & 0.1635        & 0.4448    & \second{0.0286}    & 24            & 45           & 272             & 239       & 44      \\
        soc-sign\_epinion        & 1345         & \first{0.0178}      & 0.0830      & 0.1309        & 0.2598    & \second{0.0700}      & 4             & 12           & 43              & 27        & 22      \\
        ego-twitter                    & 2523         & \first{0.0929}      & 0.4272     & 1.9648        & 0.6219    & \second{0.0813}    & 13            & 46           & 478             & 46        & 19      \\
        soc-Youtube              & 2269         & \first{0.1077}      & 0.8683     & 1.7423        & 3.1899    & \second{0.3113}    & 29            & 127          & 608             & 342       & 106     \\
        comm-WikiTalk            & 1384         & 0.0099      & 0.0107     & \second{0.0094}        & 0.0185    & \first{0.0093}    & 1             & 1            & 1               & 1         & 1       \\
        nov\_user\_msg\_time          & 3779         & \second{3.026}       & 5.5366     & 12.2961       & 19.1805   & \first{2.1637}    & 96            & 72           & 436             & 212       & 76      \\
        cit-Patents\_AMINER            & 28546        & \first{2.3379}      & 29.3613    & 23.5359       & 28.5621   & \second{10.5380}    & 17            & 65           & 224             & 80        & 100     \\
        soc-Twitter\_ASU               & 3834         & 0.1189      & 0.2180      & \first{0.1034}        & 0.2458    & \second{0.1088}    & 1             & 1            & 1               & 1         & 1       \\
        soc-Livejournal & 105265       & \first{152.9460}     & 13795.667  & 24839.696     & 41770.72  & \second{1322.3124} & 437           & 8436         & 83549           & 42101     & 4615    \\
        soc-Orkut                & 26670        & \first{14.9269}     & 111.2410    & 1440.7987     & 1263.6286 & \second{86.1052}   & 71            & 150          & 8242            & 2262      & 495     \\
        soc-SinaWeibo          & 25556        & \first{36.0107}     & 308.7090    & 2694.3677     & 2524.9365 & \second{280.3085}  & 262           & 594          & 21266           & 6044      & 2263    \\
        \hline
        wang-tripadvisor                    & 3103         & \first{0.0708}      & 0.8382     & 1.7039        &    \makecell[c]{--}       & \second{0.6695}    & 121           & 187          & 4031            &    \makecell[c]{--}        & 1403    \\
        rec-YelpUserBusiness  & 1343         & \first{0.0732}      & 1.5679     & 0.7741       &  \makecell[c]{--}          & \second{0.0987}    & 46            & 207          & 784             &   \makecell[c]{--}        & 104     \\
        bookcrossing        & 1946         & \first{0.0703}      & 1.6941     & 1.0291        &    \makecell[c]{--}    & \second{0.1824}    & 75            & 278          & 1383            &  \makecell[c]{--}       & 250     \\
        librec-ciaodvd-review      & 2195         & \first{0.2090}       & 3.6041     & 14.9112       &    \makecell[c]{--}     & \second{0.2609}    & 49            & 139          & 4644            &   \makecell[c]{--}     & 80      \\
        movielens-10m               & 6049         & 0.1322      & 0.5096     & \first{0.1245}        &  \makecell[c]{--}        & \second{0.1249}    & 1             & 1            & 1               &   \makecell[c]{--}     & 1       \\
        epinions                     & 6616         & \first{1.2117}      & 140.5527   & 187.0548      &    \makecell[c]{--}    & \second{5.0036}    & 43            & 788          & 9081            &  \makecell[c]{--}     & 239     \\
        libimseti                           & 7179         & \first{1.1102}      & 247.5964   & 57.4351       &   \makecell[c]{--}    & \second{3.0025}    & 18            & 594          & 1142            &  \makecell[c]{--}    & 59      \\
        rec-movielens             & 9335         & \second{8.8373}      & 839.4542   & 83.8077       &    \makecell[c]{--}     & \first{1.7434}    & 52            & 738          & 563             & \makecell[c]{--}       & 11      \\
        yahoo-song                          & 34352        & \second{155.6296}    & 11453.641  & 4144.8288     &    \makecell[c]{--}      & \first{51.6024}   & 154           & 1532         & 4959            &    \makecell[c]{--}    & 61      \\
        \hline
        \multicolumn{12}{l}{\multirow{1}{0.95\textwidth}{\textbf{note:} $|\mathcal{H}^{\prime}|$ denotes the size of the subgraph after the pruning, \textbf{FW} means Frank-Wolfe in \cite{danisch2017large}.}} \\
        \bottomrule
        \end{tabular}
    }
\end{table*}

We compare the running time of the approximation and exact algorithms on weighted and unweighted graphs in Table~\ref{tab:time}, where the results are the average of 5 trails. We report the density of detected subgraphs by approximation and exact algorithms in the appendix. The density obtained by the same type of algorithm is equivalent (We use $||$ to distinguish different types). It shows that the pruning can save a lot of time for subsequent algorithms, i.e., Greedy in different data structures and Max-Flow; the pruning achieves the best results (as the bold labeled) on both weighted graphs and unweighted graphs.

Columns 2 and 3 are the pruning used on weighted and unweighted graphs respectively (Of course an unweighted graph can be seen as a weighted graph with $w_e=1$ for any edge $e$). They prune the graph and lock the densest subgraph into a much smaller range.

Columns 4-6 show the results of approximation algorithms for all networks, we can find that PT is much faster than BBST. Although BBST has the same average time complexity as PT, i.e., $O(M \log N)$, it actually runs much slower than PT, especially on large datasets, which is attributed to the higher probability of a worse situation in BBST since deleting the node with the lowest weight iteratively in Greedy is not a balanced operation in BBST. Therefore, we just combine the pruning technique with PT, and it can detect the same subgraph faster,  which achieves about $3.563 \times$ on average by utilizing the pruning technique than using PT alone.

Although the time complexity of the pruning on weighted graphs is $O(M+TN)$ and we are not sure about the iteration count $T$, in our experiments, the number of deleted nodes approximately obeys the exponential law (in the appendix we plot 6 datasets to illustrate the exponential law). Therefore, $T$ is $O(\log N)$ according to experimental performance so the pruning is also efficient on weighted graphs, which can be confirmed with the results of columns 5 and 6, i.e., w\_Pruning+PT runs faster than using PT alone.

Columns 7-8 are the results of the exact algorithms. It shows that the pruning accelerates Max-Flow by $28.215 \times$ on average (ignoring the time-out datasets, although it performs better on large datasets), which can be attributed to two reasons: First, a higher density lower bound $\delta$ for the subsequent binary search. Second, a smaller graph needs to be explored next. These factors greatly reduce the number of binary-search iterations and the execution time of the max flow algorithm.

Columns 9-10 are the results of approximation algorithms for unweighted graphs. Greedy in doubly-linked list achieves $2.312 \times$ speedup on average by utilizing the pruning. Although the speed of uw\_Pruning is not faster than that of w\_Pruning evidently, it has an accurate time complexity, i.e., $O(M+N)$. By utilizing the pruning technique, we acelerate DLL about $2.312\times$ on average.


\subsection{ Effectiveness of \emph{\method} When Solving DSP}


% Figure environment removed

% Figure environment removed

To fully demonstrate the efficiency of \emph{\method}, we compare it with the state-of-the-art iterative algorithms including Frank-Wolfe in \cite{danisch2017large}, Greedy++ in \cite{boob2020flowless} and FISTA in \cite{harb2022faster}, we also compare it with a version of MWU executed in \cite{harb2022faster}. In this subsection, we want to figure out how well these iterative algorithms perform on detecting the densest subgraph and fully demonstrate the effectiveness of \emph{\method}. We devise our experiments as below:
\begin{compactitem}
    \item{\textbf{Comparison on detecting the densest subgraph.}} We execute it to figure out whether \emph{\method} can perform better than other iterative algorithms on detecting the densest subgraph.
    \item{\textbf{The relative error on the density as iterations count increases.}} We supplement it to exhibit the convergence speed of \emph{\method} and other iterative algorithms on 8 randomly selected datasets.
\end{compactitem}

All these experiments are executed with the pruning technique to pre-process these datasets. Corresponding experiments without the pruning are also executed whose results are listed in the appendix.

We initialize \emph{\method}, Frank-Wolfe, FISTA and MWU by distributing the weight of each edge equally to its associated nodes like lines 1-4 in Algorithm \ref{alg:lowd}, which makes the node weight positively correlated with its degree, our intuitive consideration is that nodes with higher degrees should not be deleted first while it is unclear which node each edge should be distributed to. This sort method is also used in \cite{danisch2017large}.

In Table~\ref{tab:lowd}, we compared \emph{\method} and other baselines on the subgraphs after pruning in Algorithm \ref{alg:pruning} on unweighted and weighted graphs. The optimal result is bolded and underlined, while the suboptimal result is only underlined. It shows that \emph{\method} detects the densest subgraphs much faster than other iterative algorithms from the overall experimental performance, most of its results are optimal or suboptimal. Besides, Greedy++ is easier to detect the densest subgraph in only one iteration, which partly shows the effectiveness of Greedy. Although FISTA has the best theoretical lower bound for convergence rate, it doesn't show good experimental performance in detecting the densest subgraph. Besides, it meets with a numerical stability issue especially on weighted graphs which needs more study and solution. Therefore, we ignore the application of FISTA on weighted graphs.

Next, we randomly choose 8 datasets to exhibit the relative error of these algorithms as iterations count increases. The definition of relative error is:
$(\rho^*-\rho(\subnode))/\rho^*$. We use logarithmic coordinates to exhibit it more explicitly. We add a small amount \textit{eps=1e-9} to the relative error because when we detect the graph with density $\rho(\subnode)=\rho^*$, its relative error is 0 and doesn't appear on logarithmic coordinates. \emph{\method} has the optimal convergence speed on 7 datasets and the suboptimal convergence speed on 1 dataset, which is slower than MWU. \emph{\method}'s relative error seldom vibrates because it monotonically decreases the optimization of LP \ref{eq:tighter_dual} and QP \ref{eq:decomposition} based on its local optimality. Greedy doesn't vibrate because it stores the best result of all iterations while the relative errors of Frank-Wolfe, FISTA and MWU vibrate apparently because they are based on classical methods towards convex optimization like gradient descent and its variants. They don't ensure monotonic decline, which we will observe again in experiments about LDD.

\subsection{ Effectiveness of \emph{\method} When Solving LDD}

% Figure environment removed

We also conduct experiments about the convergence speed to the locally-dense decomposition in Figure \ref{fig:pruning_ldd}. The definition of QP error on the y-axis is: $\sum_{v \in \nodes}{{\ell_{v}}^{2}}-\sum_{v \in \nodes}{{\ell_{v}^{*}}^{2}}$, where $\sum_{v \in \nodes}{{\ell_{v}}^{2}}$ is attained by these iterative algorithms and $\sum_{v \in \nodes}{{\ell_{v}^{*}}^{2}}$ is the optimal value of QP \eqref{eq:decomposition}. We use the minimum $\sum_{v \in \nodes}{{\ell_{v}}^{2}}$ attained by these algorithms to represent the optimal value and add a small amount \textit{eps=1e-3} to QP error similarly.

From Figure \ref{fig:pruning_ldd}, we can see that \emph{\method} optimize $\sum_{v \in \nodes}{{\ell_{v}}^{2}}$ much faster than any other iterative algorithms because the x-axis is logarithmic coordinate. We conclude this to the local optimality of \emph{\method} in QP \eqref{eq:decomposition}. FISTA performs better on LDD than MWU, Frank-Wolfe and Greedy++, which is consistent with the conclusion in \cite{harb2022faster}. However, it will meet with a numerical stability issue, for example, on dataset \textit{ego-twitter} it vibrates seriously around the optimal value. In Figure \ref{fig:pruning_ldd} all baselines will vibrate to some degree, while our method \emph{\method} can optimize QP \eqref{eq:decomposition} monotonically.

To observe the locally-dense decomposition, we run \emph{\method} 500 iterations on \textit{ego-twitter} after pruning to achieve Figure \ref{fig:BDSS} as an example. We use colormap $gist\_rainbow$ in $matplotlib$ to dye nodes according to their loads, therefore, nodes with the same loads should have the same color. In Figure \ref{fig:BDSS} red means the minimum node load and violet means the maximum node load. And the nodes with maximum loads represent $B_1$ and the nodes with the second largest loads represent $B_{1}\setminus B_{2}$. We can really observe the hierarchical phenomenon on node loads in locally-dense decomposition. Besides, we checked some edges with $e=(u,v)$ and $u$,$v$ have different node loads, and these edge weights are all distributed in one-way to endpoints with lower loads. Another interesting discovery is that the node position is based on the Kamada-Kawai path-length in $matplotlib$, we can see nodes with similar loads are close to each other, which also implies that Kamada-Kawai path-length has connections with locally-dense decomposition.

By observing the locally-dense decomposition, it not only helps to detect different variants of the densest subgraph like DkS in Corollary \ref{coroll:dks} and locally densest subgraph in \cite{ma2022finding}, but also helps us to understand the whole graph about the theme of "edge-density". Given that we get the densest subgraph by distributing edge weights, we can understand the reason why some nodes with high degrees are not in the densest subgraph is that they have to distribute some edge weights to their neighbors with low loads. The influence is transmitted along edges, so the loads of two nodes can be influenced by each other as long as they are in the same connected branch. The previous work \cite{veldt2021generalized} has found this phenomenon, they generalized the Greedy algorithm of \cite{charikar2000greedy} with a parameter $p$ (the Greedy algorithm is equal to the generalized form with $p=1$) and set $p=1.05$ to spread the influence of node neighbors slightly, which helps to detect denser subgraphs than Greedy.
