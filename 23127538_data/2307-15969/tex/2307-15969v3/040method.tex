% !TeX root = main. tex
\begin{algorithm}[bt]
    \caption{\method: Locally optimal weight distribution for densest subgraph detection}
    \label{alg:lowd}
    \KwIn{Undirected graph $\graph$, iteration count $T$.}
    \KwOut{An approximately densest subgraph of $\graph$.}
        % $\optset \leftarrow \nodes$
        \For{$e = (u, v) \in \edges$}{
            $f_e(u) = f_e(v) = \frac{w_e}{2}$;
            \hfill \linecomment{initialize edge weight}
        }
        \For{$u \in \nodes$}{
            $\ell_u = \sum_{e\in \edges:u\in e}{f_e(u)}$
            \hfill \linecomment{initialize node load}
        }
        \For{$k : 1 \to T$}{
            \For{$e = (u, v) \in \edges$}{ 
                % \linecomment{balance weight of $u$ and $v$ with $w_e$}
                \If{$ \ell_{u} > \ell_{v}$}{
                    \hfill \linecomment{balance nodes loads as much as possible}
                    
                    $d \leftarrow \min \{(\ell_{u} - \ell_{v})/2, \, f_{e}(u) \} $;

                    $\ell_{u} \leftarrow \ell_{u} - d$;
                    
                    $ f_e(u) \leftarrow f_e(u) - d$;  
                    
                    $\ell_{v} \leftarrow \ell_{v} + d$;
                    
                    $f_e(v) \leftarrow f_e(v) + d$;
                } 
                \Else {
                    $d \leftarrow \min \{(\ell_{v} - \ell_{u})/2, \, f_{e}(v) \} $;
                    
                    $\ell_{u} \leftarrow \ell_{u} + d$;
                    
                    $f_e(u) \leftarrow f_e(u)+d$;
                     
                    $\ell_{v} \leftarrow \ell_{v} - d$;
                    
                    $f_e(v) \leftarrow f_e(v)-d$;
                }
            }
        }
        $\optset, \subnode \leftarrow \nodes, \nodes $; 
        \hfill \linecomment{sort $l_u$ in a non-decreasing order}
        
        $V_{s} \leftarrow \argsort_{u \in \nodes} l_u$;
        
        \For{$i :  1 \to N$}{
            $\subnode \leftarrow \subnode \setminus \{\nodes_{s}(i)\}$; \hfill \linecomment{$\nodes_{s}(i):$ the $i$-th element of $\nodes_{s}$}
            
            \If{$\rho(\optset) < \rho(\subnode)$}{
                $\optset \leftarrow \subnode$;
            }   
        }
    \Return{$ \graph(\optset) $.}
\end{algorithm}

In this section, through the lens of the primal-dual formulation of linear programming, 
we illustrate our idea for solving DSP
and propose the \textbf{l}ocally \textbf{o}ptimal \textbf{w}eight \textbf{d}istribution algorithm, \emph{\method}, 
which is a fast iterative approach to searching the densest subgraph according to the LP dual of DSP.
We also theoretically prove that \emph{\method} makes the graph converge into locally-dense decomposition.
To search the densest subgraph more efficiently, we propose a pruning algorithm, rendering 
the necessary condition for DSP and corresponding to a subprocess of Greedy. 

\subsection{Locally Optimal Weight Distribution Algorithm}

Firstly we present the pseudo code of \emph{\method} in algorithm \ref{alg:lowd} and explain it. In algorithm \ref{alg:lowd}, given an undirected graph $\graph$ and iteration count T, we distribute each edge $e$'s weight to its two endpoints $u$ and $v$ and use vector $\bm{f}$ to describe it. Specifically, $f_e(u)$ is the weight distributed to node $u$ from edge $e$. Besides, we use vector $\bm{\ell}$ to remark the load of each node received from all corresponding edges, specifically, $\ell_u=\sum_{e\in \edges:u\in e}{f_e(u)}$. In lines 1-4, we distribute each edge equally to two endpoints as the initial state and
accordingly calculate node loads. Its time complexity is $O(M)$. Lines 5-14 mean that it redistributes the weight of edge $e=(u,v)$ to minimize the difference of $\ell_u$ and $\ell_v$ for every $e\in \edges$ in iterations. Its time complexity is $O(MT)$. In lines 7-10 we distribute more edge weight from $u$ to $v$, and update step $d$ is $(\ell_{u}-\ell_{v})/2$ if it will not make any $f_{e}(u)<0$, otherwise $d$ is $f_{e}(u)$. In lines 11-14 the circumstance is the opposite. In line 15 we set the whole nodeset as the initial nodeset $S$. In lines 16-20 we sort S according to node loads and delete the node with the lowest weight one by one to get a subgraph with high density, its time complexity is $O(M+NlogN)$.

\begin{complexity}
    The time complexity of \emph{\method} is $O(MT+M+NlogN)$.
\end{complexity}

\textbf{Remark.} Although \emph{\method} can detect the densest subgraph with enough iterations, but we cannot determine iteration count required. Therefore, our \emph{\method} belongs to the approximation algorithm in strict terms because actually we just set the iteration count to get an approximate solution for DSP. The same circumstance happens on other approximation algorithms including Frank-Wolfe in \cite{danisch2017large}, Greedy++ in \cite{boob2020flowless} and FISTA in \cite{harb2022faster}.

% After defining the above problems, in section $IV.A$ we introduce its primal-dual form of linear programming and explain it, then we develop a fast iterative algorithm called LOWD to search the densest subgraph according to the LP dual of DSP. In section $IV.B$ we analyse the reason why it also deals with the locally-dense decomposition problem. At last, we introduce a pruning technique that can prune the graph and lock the densest subgraph into a much smaller subgraph, we use modified Counting Sort to speed it up and prove it is a subprocess of Greedy in \cite{charikar2000greedy}.

\subsection{\emph{\method} converges to DSP solution}

In order to figure out how \emph{\method} searches the densest subgraph iteratively, we first introduce the LP primal-dual of DSP from \cite{charikar2000greedy,boob2020flowless,danisch2017large} as follows. The notation of LP primal-dual is the same as \cite{boob2020flowless}.
%\cite{boob2020flowless} gives an unweighted version but LP \eqref{eq:primal} is a weighted version, the weighted version of DSP can also be attained in \cite{danisch2017large}.



\begin{equation}
    \centering
    \label{eq:primal}
    \begin{aligned}
        \textrm{maximize}   \qquad & \sum_{e\in \edges}{w_{e}y_{e}} \\
        \textrm{subject to} \qquad & y_e \le x_u, \qquad \forall e = uv \in \edges \\
                            & y_e \le x_v, \qquad \forall e = uv \in \edges \\
                            & \sum_{v \in \nodes}{x_v} \le 1 \\
                            & y_e \ge 0 \qquad \forall e \in \edges \\
                            & x_v \ge 0 \qquad \forall v \in \nodes \\
    \end{aligned}
\end{equation}

In LP \eqref{eq:primal}, the binary $x_u$ and $y_e$ indicate the contribution to density of the densest subgraph from node $u$ and edge $e$. The maximum of LP \eqref{eq:primal} is $\rho^*$, i.e., the maximum density in DSP. You can set $y_e=\frac{1}{\optset}$ if $e \in \edges(\optset)$ otherwise $y_e=0$ and set $x_u=\frac{1}{\optset}$ if $u \in \optset$ otherwise $x_u=0$, then you will get $\frac{\weights(\optset)}{\optset}$ as the optimal value of $\sum_{e\in \edges}{w_{e}y_{e}}$. Instead of using the primal problem, we resort to the LP dual for DSP to illustrate our motivation:

\begin{equation}
    \centering
    \begin{aligned}
        \textrm{minimize}  \qquad &  D \\
        \textrm{subject to} \qquad & f_e(u)+f_e(v)\ge w_{e}\qquad\forall e=uv\in \edges \\
                        & \ell_v \overset{\text{def}}{=} \sum_{e \ni v}{f_e(v)} \le D \qquad \forall v \in \nodes\\
                        & f_e(u) \ge0\qquad\forall e = uv \in E \\
                        & f_e(v) \ge0\qquad\forall e = uv \in E \\
    \end{aligned}
    \label{eq:dual}
\end{equation}

From strong duality, we know its optimal value is also $\rho^*$. The symbols in LP \eqref{eq:dual} can be interpreted in accordance with the description in \emph{\method}. $f_e(u)$ and $f_e(v)$ should be both positive and the sum is not less than $w_e$ to ensure the edge is distributed thoroughly. Actually, in the LP dual of DSP, we can keep $f_e(u)+f_e(v)=w_{e}$ instead of $f_e(u)+f_e(v)\ge w_{e}$, because the former can keep constraint condition and doesn't increase optimization objective $D$. Consequently, we can get the optimization objective $D$ with tighter constraint in LP \eqref{eq:tighter_dual} as below.

\textbf{note:} During the whole process we set $D =\max_{v\in \nodes} \ell_v$ to minimize it as much as possible.

\begin{equation}
    \centering
    \label{eq:tighter_dual}
    \begin{aligned}
        \textrm{minimize}   \qquad & D \\
        \textrm{subject to} \qquad & f_e(u) + f_e(v) = w_e \qquad \forall e = uv \in \edges \\
                        & \ell_v = \sum_{e\ni v}{f_e(v)} \le D \qquad \forall v \in \nodes\\
                        & f_e(u) \ge 0 \qquad \forall e = uv \in \edges \\
                        & f_e(v) \ge 0 \qquad \forall e = uv \in \edges \\
    \end{aligned}
\end{equation}

Intuitively speaking, what \emph{\method} does is to redistribute each edge weight to minimize the difference between loads of two endpoints for each edge in iterations, i.e., propagate edge weights from nodes with higher loads to nodes with lower loads. And the impact of \emph{\method} on LP \eqref{eq:tighter_dual} is that the node $v$ with the highest node load $l_{v}=D$ will decrease its weight because $v$ transits its edge weights to its neighbors. After experiencing some iterations, the highest node load will decrease gradually to some value, in fact, the value is the minimum of $D$, i.e., $\rho^*$. Formally we propose the following theorem:

\begin{theorem}
    The iterative operation in \emph{\method} can make the sequence $\left \{ D_t \right \}$ converge to $\rho^*$, where $D_t (t>0)$  means the optimization objective $D$ after $t$ iterations of \emph{\method} and $D_{0}$ is the value in the initial state. 
    \label{th:dsp}
\end{theorem}

%We give an intuitive explanation for LOWD and claim that it can minimize D and detect the densest subgraph. The impact of LOWD on LP \eqref{eq:tighter_dual} is that the node $v$ with the highest node load $l_{v}=D$ will decrease its weight because $v$ transits its edge weights to its neighbors. After experiencing some iterations, the highest node load will decrease gradually to the minimum of $D$, i.e., $\rho^{*}$. And then we can search the densest subgraph like Greedy, i.e., delete the node with the lowest degree from the left nodeset $\subnode$ one by one.

\begin{comment}
Next, we will give theoretical proofs for our claim, and we raise the following questions:

\begin{enumerate}[label={\arabic*.}]
    \item Why does the optimization objective $D$ converge through LOWD?
    \item Why will the optimization objective $D$ converge to the minimum, which is $\rho^*$, through LOWD?
    \item How can we get the densest subgraph through LOWD?
\end{enumerate}

To answer the first question, we have the following classical theorem.
\end{comment}

Next, we will give theoretical proofs for Theorem \ref{th:dsp}, first we need the following classical theorem for convergence.


\begin{theorem}[Monotone Convergence Theorem~\cite{bibby1974axiomatisations}]
If the sequence $\left \{ a_n \right \} $ has an upper bound and it is monotonically non-decreasing (or has a lower bound and it is monotonically non-increasing), then the sequence $\left \{ a_n \right \} $ converges, i.e., a monotonically bounded sequence must have a limit.
\label{th:tmct}
\end{theorem}

Now that $\left\{D_t \right \}$ has a lower bound $\rho^{*}$ according to strong duality, and $\left \{ D_t \right \} $ is monotonically non-increasing under \emph{\method}'s iterative operation (because in the whole process \emph{\method} just balances loads of two endpoints and it will not produce any new node with load higher than the optimization objective $D$). Then $D_t$ is non-increasing and it must converge to some value.

It is helpless for solving DSP if the optimization objective converges to some value which is not the minimum, i.e., $\rho^{*}$. However, we can use the following lemma to help to prove Theorem \ref{th:dsp}.

\begin{lemma}
    for $\forall t \in \mathbb{N}$, there must be $D_{t+N}<D_{t}$ if $D_t \ne \rho^{*}$, where N is the node number of the whole graph.
    \label{lem:decrease}
\end{lemma}

\begin{proof}
    For $\forall t \in \mathbb{N}$, if $D_t>\rho^{*}$, we make an assertion that there must be some nodes $u$ and $v$ and an edge $e=(u,v)$ with $\ell_u=D_t$, $\ell_v<D_t$ and $f_e(u)>0$. Suppose this doesn't stand up, so if we set $A=\left \{u|l_u=D_t \right \}$ and $B=\left \{u|l_u<D_t \right \}$, for $\, \forall u \in A,v \in B$ and $e=(u,v)$, then $f_e(u)=0$. Therefore we conclude that:
    \begin{equation*}
        \footnotesize
        \centering
        \begin{aligned}
            \rho \left ( A \right ) 
            &=\frac{\sum_{e \in \edges(A)} w_e}{|A|}=\frac{\sum_{e=(u,v),\,u,v\in A} f_e(u)+f_e(v)}{|A|}\\
            &=\frac{\sum_{e=(u,v),\,u,v\in A} {(f_e(u)+f_e(v))}+\sum_{e=(u,v),\,u\in A,v\in B}{f_e(u)}}{|A|}\\
            &=\frac{\sum_{u\in A}{\sum_{e\ni u}{f_e(u)}}}{|A|}\\
            &=\frac{\sum_{u\in A}{l_u}}{|A|}=\frac{\sum_{u\in A}{D_t}}{|A|}=D_t>\rho^*\\
        \end{aligned}
    \end{equation*}

    It will produce a subgraph whose density is larger than the densest subgraph. That will lead to a contradiction. 

    Therefore in each iteration, node $u$ will transit edge weight to node $v$ and then there will be $l_u<D_t$ according to our assertion. Notice that in the whole process, \emph{\method} will not produce any new node $v$ with $l_v\ge D_t$ after t iterations. The number of nodes with load $D_t$ will decrease in each iteration until 0. Given that $N$ is the node number of the whole graph, after $N$ iterations, there isn't any node $v$ with $l_v=D_t$, i.e., $D_{t+N}<D_{t}$.
\end{proof}

Combining Lemma \ref{lem:decrease} and Theorem \ref{th:tmct} is not enough to prove our claim because $\left\{D_t \right \}$ may decrease infinitesimally and converge to another value instead of the minimum. However, when dealing with this difficulty, it is useful to combine the proof of Lemma \ref{lem:decrease} in the limit sense.

\begin{proof}[Proof of Theorem \ref{th:dsp}]
    We adopt the proof by contradiction which is similar to the proof of Lemma \ref{lem:decrease}. Suppose that \emph{\method} makes $\left \{ D_t \right \}$ converge to any other value $D$ which $D > \rho^{*}$. We set $A=\left \{u|l_u\to D \right \}$ and $B=\left \{u|l_u\not\to D \right \}$. When $T \to \infty$, $|A|$ will decrease and converge to a fixed number according to Theorem \ref{th:tmct}, and all edges connecting $A$ and $B$ are distributed to $B$ in the limit sense. Then $\rho(A)$ will be tending to $D$, which makes a contradiction because $D > \rho^{*}$. Only when $D = \rho^{*}$, there will be no contradiction.
\end{proof}

The explanation for lines 19-24 in Algorithm \ref{alg:lowd} is closely related to the locally-dense decomposition, which will be proved in the next subsection. In fact, $B_1$ in LDD is the maximal densest subgraph and nodes in $B_1$ will have the max node load $\rho^*$. Therefore in LDD, as long as we delete all the nodes whose weights are not the maximum, the remaining subgraph is the maximal densest subgraph. If \emph{\method} makes the graph converge into LDD, given that the node load in the densest subgraph will be not completely the same after several iterations, it is safe to delete the node with the lowest weight one by one.

A drawback of the iterative algorithm is that we don't know whether we have found the densest subgraph so as to stop iterations. However, on unweighted graphs, if the difference between optimization objective $D$ and the maximum density found by \emph{\method} is less than $\frac{1}{n(n-1)}$, we can confirm that \emph{\method} has found the densest subgraph and it can stop iterations, which is similar with the maximum flow algorithms in \cite{goldberg1984finding}. Given that the edge weights satisfy $\weights \in \numR_{+}$ on weighted graphs, it doesn't work to use the difference $\frac{1}{n(n-1)}$ to determine whether we have found the densest subgraph if it is weighted.

%Take Figure \ref{fig:example} as an example, in the locally-dense decomposition, the nodes in the maximal densest subgraph have the same loads $1.6$, and the subgraph consisting of these nodes has the maximal density $1.6$, while the other nodes have lower loads $1.5$. In the locally-dense decomposition, they form nodesets respectively and edge loads between different nodesets are distributed from nodes in the maximal densest subgraph to nodes with loads $1.5$ in one-way. Although node loads in the densest subgraph may be not completely the same after several iterations, it is safe to delete the node with the lowest load one by one, and then get the maximal densest subgraph, i.e, the subgraph consisting of nodes $1$,$2$,$3$,$4$,$5$. 

Next, we will explan the relationship between \emph{\method} and LDD.

\subsection{\emph{\method} converges to LDD's solution}
\label{subsecion:ldd}
There are many iterative methods dealing with the LP dual of DSP including Frank-Wolfe in \cite{danisch2017large}, Greedy++ in \cite{boob2020flowless} and FISTA in \cite{harb2022faster}. Among them, \cite{harb2022faster,danisch2017large} claim that their methods can converge into the locally-dense decomposition. And \cite{harb2022faster} also claims that Greedy++ will converge into it. As an iterative method, \emph{\method} also does it. Firstly, let's introduce the quadratic program(QP) formula of locally-dense decomposition in \cite{danisch2017large,harb2022faster}.

\begin{equation}
    \label{eq:decomposition}
    \centering
    \begin{aligned}
        \textrm{minimize} \qquad  & \sum_{v \in \nodes}{\ell_v^2} \\
        \textrm{subject to} \qquad & f_e(u) + f_e(v) = 1 \qquad \forall e = uv \in \edges \\
                    & \ell_v = \sum_{e \ni v}{f_e(v)} \le D \qquad \forall v \in \nodes\\
                    & f_e(u) \ge 0 \qquad \forall e = uv \in \edges \\
                    & f_e(v) \ge 0 \qquad \forall e = uv \in \edges \\
    \end{aligned}
\end{equation}

The relationship between QP \eqref{eq:decomposition} and LDD is: When \emph{\method} makes $\sum_{v \in \nodes}{{\ell_v}^2}$ converge to the minimum, then the solution $(\bm{f},\bm{\ell})$ converges to the solution of locally-dense decomposition. 

In algorithm \ref{alg:lowd}, \emph{\method} will decide an update step d and redistribute the weight of edge e to decrease the optimization objective of QP \eqref{eq:decomposition} as much as possible. For example, for $e=(v_1,v_2)$ where $\ell_{v_1}>\ell_{v_2}$ and $f_e(v_1)>0$, we use $\bm{\ell}^{'}$ to represent the node loads after updating, i.e., ${\ell_{v_1}}^{'}\gets \ell_{v_1}-d$, ${\ell_{v_1}}^{'}\gets \ell_{v_1}+d$ and ${\ell_{v}}^{'} \gets \ell_v$ for $v\neq v_1,v_2$. Then:

\begin{center}
    \vspace{-0.15in}
    \begin{equation*}
        \begin{aligned}
            \sum_{v \in \nodes}{{{\ell_{v}}^{'}}^{2}}&=\sum_{v \in \nodes,v\ne v_1,v_2}{{{\ell_{v}}^{'}}^{2}}+{{\ell_{v_1}}^{'}}^{2}+{{\ell_{v_2}}^{'}}^{2}\\
            &=\sum_{v \in \nodes,v\ne v_1,v_2}{{\ell_{v}}^{2}}+{(\ell_{v_1}-d)}^2+{(\ell_{v_2}+d)}^2\\
            &=\sum_{v \in \nodes}{{\ell_{v}}^2}+2d\cdot(d+\ell_{v_2}-\ell_{v_1})<\sum_{v \in \nodes}{{\ell_{v}}^2}\\
        \end{aligned}
        \label{eq:decrease}
    \end{equation*}
\end{center}

In algorithm \ref{alg:lowd}, update step $d=\min \{(\ell_{v_1}-\ell_{v_2})/2, \, f_{e}(v_1) \}$ then $d+\ell_{v_2}-\ell_{v_1}<0$, and in this case $d>0$. The less-than sign in the last line holds true.

The local optimality of \emph{\method} is because it decreases the optimization objective of QP \eqref{eq:decomposition} and LP \eqref{eq:tighter_dual} as much as possible. In the above equation of QP \eqref{eq:decomposition}, $d=(\ell_{v_1}-\ell_{v_2})/2$ can minimize $2d\cdot(d+\ell_{v_2}-\ell_{v_1})$ according to the mean inequality so that $\sum_{v \in \nodes}{{{\ell_{v}}^{'}}^2}$ is the minimum, and there should be $d\ge f_e(v_1)$ to satisfy constraint $f_e(v_1)\ge 0$, then $d=\min \{(\ell_{v_1}-\ell_{v_2})/2, \, f_{e}(v_1) \}$ is a locally optimal operation which satisfies the constraint and decrease $\sum_{v \in \nodes}{{\ell_{v}}^2}$ as much as possible. As for LP \eqref{eq:tighter_dual}, \emph{\method} is also locally optimal to minimize $D$.

\begin{theorem}
    \emph{\method} will optimize QP \eqref{eq:decomposition} until $\sum_{v \in \nodes}{{l_v}^2}$ converge to the minimum.
    \label{th:decomposition}
\end{theorem}

\begin{proof}
    First, any edge redistribution in \emph{\method} will decrease the optimization objective of QP \eqref{eq:decomposition} as we claimed before. According to Theorem \ref{th:tmct}, $\sum_{v \in \nodes}{{\ell_{v}}^2}$ will converge to some value. Suppose it is not the minimum, if \emph{\method} stops changing any edge redistribution, the node loads in $\graph$ must be satisfied with property \ref{prop:one-way}, i.e., if two endpoints have different loads, the edge connecting them should be only distributed in one-way to the node with a lower load. Otherwise, \emph{\method} can continue its iterative operation to decrease $\sum_{v \in \nodes}{{{\ell_{v}}^{'}}^{2}}$. Then, the nodes with the same loads will consist of new nodesets $B_i$, from a global perspective, it will result in a new sequence $\emptyset=B'_0 \subsetneqq B' _1 \subsetneqq B' _2 \subsetneqq ... \subsetneqq B' _k=\nodes$ with $\lambda' _1>\lambda' _2>...>\lambda' _k$. According to property \ref{prop:one-way}, $B' _i=\mathop{\arg\max}\limits_{W \supsetneqq B' _i-1}{\frac{\weights(\edges(W))-\weights(\edges(B' _{i-1}))}{|W\setminus B' _{i-1}|}}$ so it is satisfied with the definition of locally-dense decomposition. Now that we suppose this decomposition doesn't converges to the minimum of QP \eqref{eq:decomposition}, it must be another LDD with a different {$\bm{\ell}^*$}, which contradicts with the property \ref{prop:uniquity1} and \ref{prop:uniquity2}.
    
    If \emph{\method} doesn't stop, it must decrease the optimization objective of QP \eqref{eq:decomposition} infinitesimally. If so, edge weights must be changed infinitesimally when $T \to \infty $, otherwise the optimization objective will not decrease infinitesimally, which makes a contradiction to convergence. Given that \emph{\method} manages to balance loads of two endpoints connected by an edge, node loads in the locally-dense decomposition must obey property \ref{prop:one-way} in the sense of limit because edge weights only can be changed infinitesimally. At last, these node loads will produce a new sequence $\emptyset=B_0 \subsetneqq B _1 \subsetneqq B_2 \subsetneqq ... \subsetneqq B_k=\nodes$ in the limit sense, which makes a contradiction similarly as above.

    Therefore, the optimization objective in QP \eqref{eq:decomposition} will converge to the minimum, and the minimum represents the graph converges into the locally-dense decomposition.
\end{proof}

Therefore, \emph{\method} can make $(\bm{f},\bm{\ell)}$ converge into locally-dense decomposition. LDD is an important basic problem for many variants of DSP. \cite{ma2022finding} use it to detect a variant of DSP called locally densest subgraph. Besides, we provide a perspective on the relationship between LDD and another variant of DSP concerning the densest subgraph with size constraint, called \textit{densest k-subgraph} (DkS) and \textit{at-least-k subgraph} (DalkS) problems. Its proof is provided in the appendix.  

\begin{corollary}
     \label{coroll:dks}
    The following results hold up on DkS and DalkS problems:
    \begin{enumerate}[label={\arabic*.}]
        \item For $k=|B_j|\,,\forall j \in \left \{ 1,2,...,k \right \} $, the DkS (or DalkS) is just the subgraph composed of nodes in $B_j$.
        \item For $|B_{j-1}|<k<|B_j|\,, \forall j \in \left \{ 1,2,...,k \right \}$, the upper bound of density in DkS (or DalkS) is $\frac{\sum_{i=0}^{j-1}{\lambda_{i}*|B_i|}+(k-|B_{j-1}|)*\lambda_{j}}{k}$.
    \end{enumerate}
\end{corollary}

\begin{comment}

The second paragraph in proof of the Theorem  \ref{th:decomposition} also shows an important conclusion, we give it as the following lemma:

\begin{lemma}
    As $\sum_{v \in \nodes}{{l_v}^2}$ converge to the minimum, it has $l_v \to {l_v}^*$ for any node $v$. 
    \label{lem:convergence}
\end{lemma}

\begin{proof}
    As $\sum_{v \in \nodes}{{l_v}^2}$ converges to the minimum, then any edge redistribution will only decrease it infinitesimally. If there are nodes $u$ and $v$, an edge e with $f_e(u)>0$ and $f_e(u)>0$, then there must be $|l_u-l_v| \to 0$ or $|l_u-l_v| = 0$, otherwise $\sum_{v \in \nodes}{{l_v}^2}$ will decrease non-infinitesimally which leads to a contradiction. Then node loads must converge to the optimal value of the locally-dense decomposition, or it will produce another decomposition in the sense of limit and make a contradiction with its uniqueness.
\end{proof}

\end{comment}

\subsection{Pruning pre-process}

In our experiments, \emph{\method} can detect the densest subgraph much faster than other baselines without pruning, but it is more efficient to use a pruning technique to locate the densest subgraph before using any iterative algorithm. And the pruning is also a subprocess of Greedy, which is also useful to speed up other approximation and exact algorithms.

First, we have the following necessity condition about the optimal solution for DSP, i.e., the optimal set $\optset$.

\begin{theorem}[Lower Bound\cite{khuller2009finding}]
    For each node $v \in \optset$ of the densest subgraph, $\setndeg{\optset}{v} \ge \rho(\optset)$.
    \label{th:lowerbound}
\end{theorem}

Therefore, we can conclude that $\setndeg{\mathcal{H}}{v} \ge \setndeg{\optset}{v} \ge \rho(\optset) \ge \rho(\mathcal{H})$ if $\optset \subset \mathcal{H}$, which means $v\not\in \optset$ if $v\in \mathcal{H}$ and $\setndeg{\mathcal{H}}{v}<\rho(\mathcal{H})$. Then we can use the density of a subgraph $\mathcal{H}$ as the lower bound to filter out the candidates of $\optset$ as long as the density of $\mathcal{H}$ can be easily obtained.  The pruning technique first estimates the lower bound based on the density of the remaining subgraph $\mathcal{H}$ at the current time and deletes nodes whose degrees are lower than $\rho(\mathcal{H})$ iteratively and their adjacent edges, then it updates the bound using updated $\rho(\mathcal{H})$. Details are in Algorithm \ref{alg:pruning}.

\begin{algorithm}[t]
    \SetKwFunction{DSPSolver}{DSPSolver}
    \caption{\textsc{Pruning}}
    \label{alg:pruning}
    \KwIn{Undirected graph $\graph$; plug-in \DSPSolver: \{ \textsc{Maxflow}, \textsc{greedy}, \textsc{greedy++}, \method, \textsc{Frank-Wolfe}, \textsc{FISTA}, etc. \}.}
    \KwOut{Solution of the densest subgraph of $\graph$.}

    $\mathcal{H} \leftarrow \graph$;

    $\delta \leftarrow \rho(\mathcal{H})$;

    % \If{$ \exists \mathcal{H^{'}} \subseteq  \mathcal{H}$ and $\setndeg{\mathcal{H}}{u} < \delta$}
    % {
    %     \For
    % }
    
    \While{$\exists \, \mathcal{H^{'}} \subseteq  \mathcal{H}$ with $\setndeg{\mathcal{H}}{u} < \delta \quad \forall u \in \mathcal{H^{'}}$}{
            
            Remove all nodes in $\mathcal{H^{'}}$ and all its associated edges from $\mathcal{H}$;
            
            $\delta \leftarrow \rho(\mathcal{H})$;
    }
    
    $\subnode \leftarrow$ \DSPSolver{$\mathcal{H}$};    
    \hfill \linecomment{plug-in DSP solver}
    
    \Return{$\graph(\subnode)$.}
\end{algorithm}

This pruning technique is similar to pruning 1 in \cite{fang2019efficient}. However, we don't consider k-core explicitly and we modify the data structure Counting Sort to speed it up on unweighted graphs, which assigns it a specific time complexity, i.e., $O(M+N)$. On an unweighted graph, first, we calculate the degree of all nodes and sort them by Counting Sort, i.e., we record nodesets of each degree 0-$d_{max}$, and update the degrees of remaining nodes and corresponding sets after each iteration. In each round of scanning, we only need to scan all the sets in the range of $\left [ \left  \lfloor LowBound_{1}  \right \rfloor, \left \lceil LowBound_{2}-1  \right \rceil  \right ] $ , which are exactly the nodes to be deleted in the next iteration. $LowBound_{1}$ is the bound before updating $\rho(\mathcal{H})$ while $LowBound_{2}$ is the bound after updating $\rho(\mathcal{H})$. Although nodes with degrees less than $LowBound_{1}$ will appear after deleting nodes in each iteration, we can just put them into the set $\left \lfloor LowBound_{1} \right \rfloor $. Throughout the process, we check all the nodesets in the range of $\left [ 0,d_{max}  \right ] $ and only repeat searching $\lfloor LowBound_{1} \rfloor$ at most $T$ times. It is obvious to know $d_{max}$ and $T$ are both lower than $N$. So the time complexity of checking the nodeset of each degree is $O(N)$ on an unweighted graph. Therefore, the time complexity of the pruning on an unweighted graph is $O(M+N)$. 

On a weighted graph, after each iteration, we have to traverse the remaining nodes to find the nodes that can be deleted in the next iteration. Then the time complexity of the pruning on weighted graphs is $O(M+TN)$, where $T$ is the number of iterations until it stops.

Their difference in time complexity is because we use modified Counting Sort to make sure each node is only checked one time on unweighted graphs. 

\begin{complexity}
    For the pruning technique, its time complexity is $O(M+N)$ on unweighted graphs and $O(M+TN)$ on positive weighted graphs.
\end{complexity}

It seems that on weighted graphs it is not efficient because we don't know iteration count $T$ in advance, in fact in our experiments the time consumption of these two versions on unweighted and weighted graphs doesn't differ a lot. It is also an efficient solution that we can set an upper bound for $T$ on weighted graphs.

Next, we claim that pruning is a specific subprocess of Greedy as below. Lemma \ref{lem:k-core} is a crucial property of Greedy and we use it to prove Theorem \ref{th:pruning}. The definition of k-core and their proofs are in the appendix.

\begin{lemma}
     For any k, k-core can be achieved by the Greedy algorithm in \cite{charikar2000greedy}.
    \label{lem:k-core}
\end{lemma}
\begin{theorem}
    In Greedy\cite{charikar2000greedy}, when the density decreases for the first time, the remaining subgraph is the subgraph $\mathcal{H}$ when the pruning in Algorithm \ref{alg:pruning} stops iterations.
    \label{th:pruning}
\end{theorem}

In the above theorem, we can see that although the deletion rule is different in the pruning and Greedy. Pruning will get the same subgraph as the end of the monotonic increase of density in Greedy. Given that the pruning doesn't need to know which node has the lowest degree, it can achieve a faster deletion speed than Greedy.

In fact, the pruning is so efficient that it can delete most nodes and ensure that the densest graph is in the remaining subgraph with a much smaller size. 