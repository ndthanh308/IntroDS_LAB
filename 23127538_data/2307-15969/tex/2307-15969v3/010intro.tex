% !TeX root = main. tex

Finding subgraphs with the highest average degrees in large networks is an important primitive problem in data mining, 
and has been applied to different areas including social networks~\cite{shen2010spectral}, biological analysis~\cite{wong2018sdregion}, traffic pattern mining~\cite{liu2019coupled}, 
% graph compression~\cite{buehrer2008scalable} 
and graph database~\cite{cohen2003reachability, jin20093}. 
% DSP has been widely studied and developed for decades. 
% To address the DSP problem, 
The maximum-flow-based algorithm~\cite{goldberg1984finding} can exactly solve DSP by utilizing the binary search in polynomial time,
which is ill-suited for large graphs due to the prohibitive cost.
% Large-scale graphs with hundreds of millions or billions of nodes require more scalable algorithms to process efficiently, therefore, 
The greedy-peeling algorithm (`Greedy' as an abbreviation) proposed by Charikar~\cite{charikar2000greedy} can provide a $1/2$-approximation guaranteed solution in linear time by greedily deleting the nodes with the minimum degree.
There are also various variants for both the exact and approximation algorithms applied to different scenarios.
% an approximation algorithm called Greedy (In \cite{fang2019efficient,luo2023survey} they call it $PeelApp$), which deletes node with the minimal degree one by one and has a 2-approximation theoretical guarantee in nearly linear time.

% Figure environment removed

\begin{comment}
Many further studies are developed based on Greedy, for example, the data structure priority tree mentioned in \cite{hooi2016fraudar} and the doubly-linked tree in \cite{boob2020flowless} are used to realize it efficiently on weighted graphs and unweighted graphs. SpecGreedy~\cite{feng2021specgreedy} performs a greedy search in a subspace instead of the whole N-dimensional space based on Singular Value Decomposition (SVD).
\cite{fang2019efficient} proved that $k_{max}$-core is also a $1/2$-approximation for DSP and proposed their algorithm called Core-App to search the $k_{max}$-core efficiently. 
\atn{However,}
we find that the nodes deleted at the beginning period of Greedy cannot be within the densest subgraph. Therefore, we needn't store the node with the lowest degree dynamically and we can prune them at a faster pace by setting a lower bound. The pruning technology is used in \cite{fang2019efficient} but we use a modified Counting Sort to speed it up and analyze the relationship between it and Greedy.
\end{comment}


In addition, to detect the densest subgraph more efficiently, in recent years, many DSP algorithms and discoveries towards its dual linear programming (LP) are developed, which was first introduced in \cite{charikar2000greedy}. 
E.g., Danisch et al. \cite{danisch2017large} redistribute edge weights based on the Frank-Wolfe algorithm, 
\cite{sawlani2020near} relaxes the constraints of LP dual and proposes an algorithm for dynamic graphs, 
Greedy++ was proposed in \cite{boob2020flowless} based on the multiplicative weights update (MWU) framework, which iteratively implements Greedy to search the densest subgraph. 
In \cite{chekuri2022densest}, it gives a theoretical guarantee to the relationship between iteration count and approximation ratio of Greedy++, and popularizes Greedy++ to super-modular function $f(S)$, i.e., 
the definition of density is $\frac{f(S)}{|S|}$. Besides, \cite{harb2022faster} proposes FISTA algorithm with a faster convergence speed than Greedy++.
Frank-Wolfe, Greedy++, and FISTA are all developed based on classical methods towards convex optimization, like gradient descent and MWU. 
However, these iterative methods will vibrate around the optimal solution, which is time-consuming. 

In this paper, We elaborately design a novel algorithm called \emph{locally optimal weight distribution} (\method) algorithm, 
based on the dual problem of LP for DSP using a locally optimal operation with monotonic convergence to the optimal value, 
it detects the densest subgraph more efficiently than previous studies.
Besides, we develop a pruning technology to remove most nodes that don't belong to the densest subgraph before using \emph{\method} to detect the densest subgraph. Some pruning technique is used in \cite{fang2019efficient} but we use a modified Counting Sort to make our pruning comply with linear time complexity on unweight graphs, we also prove that our pruning technique is a subprocess of Greedy. In addition, we theoretically prove that \method converges to the locally-dense decomposition solution, which is a well-studied problem \cite{khuller2009finding,tatti2015density,danisch2017large,harb2022faster,ma2022finding,tatti2019density}. The locally-dense decomposition consists of a set of subgraphs with nested structures and densities. Such decomposition can be derived by various iterative methods, including Frank-Wolfe, Greedy++, and FISTA.


Moreover, we comprehensively verify the performance of \method over 26 real-world networks.
The experimental results show that \method has 
a better convergence rate than other iterative algorithms, i.e., Frank-Wolfe, Greedy++, and FISTA, when detecting the densest subgraph and locally-dense decomposition.

% apply it to  real-world datasets and 
% find it has a much faster convergence rate than 
% Frank-Wolfe, Greedy++, and FISTA in our experiments. 


In Figure \ref{fig:example}, we illustrate an example of \emph{\method} algorithm, and the final locally-dense decomposition. First, we use the pruning technique to delete unnecessary nodes in $(a)$. In the initial state of \method, we distribute each edge weight equally to endpoints, accumulating it as the node loads on the endpoints. Then, \method redistributes edge weights to minimize the difference between two endpoints' loads, for example, in $(b)$ and $(c)$, node 4 with load 2 transits edge weight to node 6 with load 1, and their loads both become 1.5, a similar scenario happens on nodes 5 and 7. Therefore, in order to make node loads more balanced, a node with a higher load should transit edge weight to its neighbors with lower loads. From a global perspective, edge weight should be redistributed from nodes with higher loads to nodes with lower loads in iterations, and the densest subgraph can be easily discovered by deleting the node with the lowest load one by one. And the graph finally converges to the locally-dense decomposition like $(e)$.

In summary, our main contributions include as follows:
\begin{itemize}
    \item[$\bullet$] We propose an iterative mining method called \emph{\method} for DSP according to its LP dual and prove it will converge into the locally-dense decomposition, which is a variant of DSP. We also exhibit how \emph{\method} solves both problems in a locally optimal operation iteratively.
    \item[$\bullet$] We develop a pruning technique to lock the densest subgraph into a graph with a much smaller size using a modified Counting Sort and prove that it is a subprocess of Greedy search.
    \item[$\bullet$] We did sufficient experiments on 26 real-world datasets in various fields, with sizes up to hundreds of millions of edges. Our result shows that \emph{\method} can detect the densest subgraph much faster than other baselines and it can converge to the locally-dense decomposition more efficiently, while the pruning technique as a pre-process can effectively reduce the computation. \end{itemize}

\textbf{Organization.} We organize the rest paper as below. In section \ref{sec:pre} we present two problems formally: the densest subgraph problem and the locally-dense decomposition problem. In section \ref{sec:method} we proposes an iterative method to deal with the linear programming of DSP and quadratic programming of locally-dense decomposition, and develop a pruning technique to lock the densest subgraph into a much smaller graph. We exhibit their efficient performance based on various experiments in section \ref{sec:exp}. In section \ref{sec:related} we review the related work about DSP and its variants. We summarize our work and make a prospect for future work In section \ref{sec:con}.

Due to space limits, part of the theorems, lemmas, and experimental results are given in the appendix.


%\textbf{Reproducibility:} Our codes are publicly available \footnote{https://anonymous.4open.science/r/Boundcut-BGreedy-and-BDSS-DB7E}.