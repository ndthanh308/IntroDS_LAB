\section{Introduction}
\label{sec:intro}
% To insert a figure: \input{figs/template}
% Or table: \input{tables/template}

Object detection constitutes a pivotal task in the field of computer vision, entailing the critical process of identifying and localizing objects present within an image. Applications of object detection models include autonomous vehicles, surveillance, robotics, and augmented reality \cite{yolo_applications}. The central problem of deploying deep learning-based object detection solutions on embedded hardware platforms is the amount of computation, memory, and power required for their inference \cite{lane2017squeezing}. This necessitates the development of efficient object detection models specialized for low-footprint hardware devices to achieve an optimal trade-off of accuracy and latency.
% Figure environment removed

For years, the state-of-the-art (SOTA) deep learning approach to object detection has been the series of YOLO architectures \cite{yolo_survey}. In recent years, remarkable strides have been taken in advancing YOLO-like single-stage object detectors, prioritizing real-time operation while simultaneously striving for higher accuracy and deployability on low-power devices. These advancements have primarily focused on enhancing various components of the detection pipeline. Key areas of improvement include the design of accurate and efficient backbone and neck structures within the network \cite{wang2023yolov7}, exploration of different detection head designs (e.g. anchor-based \cite{wang2023yolov7} vs. anchor-free \cite{ge2021yolox}), utilization of diverse loss functions \cite{dfl}, and implementation of novel training procedures including innovative data augmentation techniques \cite{dataaug}. 
These collective efforts have continually refined and evolved YOLO-like architectures, enhancing object detection effectiveness and efficiency in real-time scenarios. The differences between consecutive YOLO versions, such as YOLOv5 \cite{Jocher_YOLOv5_by_Ultralytics_2020} and YOLOv6 \cite{li2023yolov6}, span various pipeline components, making it challenging to isolate their individual contributions. This paper aims to address these challenges by providing a fair comparison of recent YOLO versions under controlled conditions (e.g. same training loop for all models) to
demonstrate the impact of the backbone and neck structure of
YOLO-based models in embedded inference applications. 
We also use the collected accuracy and latency data for multiple YOLO-based detector variations to empirically evaluate training-free performance predictors commonly used in neural architecture search \cite{abdelfattah2021zero}. We summarize our contributions as follows: 

% Zero-cost-proxy \cite{abdelfattah2021zero} techniques used in neural architecture search (NAS) \cite{mellor2021neural} to minimize the time needed for search by using training free methods to estimate accuracies. There are many different zero cost proxies methods like Zico \cite{li2023zico}, fisher \cite{turner2019blockswap}, jacobCov \cite{mellor2021neural}, SNIP \cite{lee2018snip}, Synflow \cite{tanaka2020pruning} used in the literature.  \textit{YOLOBench} can also be looked as a NAS space. This reveals and interesting observation that, while most of the state-of-the-art zero-cost (training-free) proxies for model accuracy estimation are outperformed by simple baselines such as MAC count, the NWOT estimator \cite{mellor2021neural} can be effectively used to identify potential Pareto-optimal YOLO detectors in a training-free manner. 

\begin{itemize}
    \item We provide a latency-accuracy benchmark of $550$+ YOLO-based object detection models on $4$ different datasets, called \textit{YOLOBench}. All the models are validated on $4$ different embedded hardware platforms (Intel CPU, ARM CPU, Nvidia GPU, NPU),
    % \item We provide a latency-accuracy benchmark of 550+ YOLO-based object detection models on 5 different datasets and 4 different embedded hardware platforms (CPU, ARM, GPU, NPU) called \textit{YOLOBench}    

    \item We show that if modern detection heads and training techniques are implemented for the detector training pipeline, multiple backbone and neck variations, including those of older architectures (e.g. YOLOv3 and YOLOv4), can be used to achieve state-of-the-art latency-accuracy trade-off,
    
    % \item We show that although years of progress in YOLO architecture design have provided improvements, a fair benchmarking of YOLO variations in the same training setup reveals that older models (such as YOLOv3) are still optimal in certain latency regions.
    
    %\item We show that GPU-timing benchmarks do not necessarily translate to other hardware, highlighting the potential role of hardware-aware architecture design and effective model scaling techniques.% Matteo: This /item can be improved..

    \item Looking at \textit{YOLOBench} as a neural architecture search (NAS) space, we demonstrate that, while most of the state-of-the-art zero-cost (training-free) proxies for model accuracy estimation are outperformed by simple baselines such as MAC count, the NWOT estimator \cite{mellor2021neural} can be effectively used to identify potential Pareto-optimal YOLO detectors in a training-free manner,
    
    % \item Leverage YOLOBench to demonstrate to analyze the effectiveness of zero-cost-proxy methods for model accuracy estimation. 

    \item We showcase the effectiveness of the NWOT estimator for
    optimal detector prediction by using it to identify a YOLO-like model with FBNetV3 backbone that outperforms YOLOv8 on the Raspberry Pi 4 ARM CPU.
    
    % \item Looking at \textit{YOLOBench} as a neural architecture search (NAS) space, we demonstrate that state-of-the-art training-free proxies for architecture accuracy estimation are outperformed by simple baselines such as MAC count.
\end{itemize}

%The following sections summarizes the YOLOBench architecture space by summarizing different YOLO models. It follows by the explanation of the methodolgy used to collect the latency and accuracy numbers needed for this benchmark and concludes with the discussion of results using the Pareto optimal model graphs. At the end, YOLOBench is used to analyze the effectiveness of zero-cost proxy methods. 

