\section{Results}
\label{sec:results}

\subsection{Pareto-optimal YOLO models}
By computing the proxy metric for model accuracy (mAP$_{50-95}$ in VOC training from scratch) and latency values for the whole \textit{YOLOBench} architecture space on several hardware platforms, we determine the Pareto sets containing the most promising models (in terms of latency-accuracy trade-off) for each HW platform. The first and second Pareto sets for each device are merged into a unified list of best architectures, which is comprised of 52 backbone/neck combinations for COCO pre-training. Same architectures with different input resolutions are considered as the same data points in this list since COCO pre-training is regardless done on a fixed resolution of 640x640. The COCO pre-training phase is followed by fine-tuning at 11 different resolutions (from 160x160 to 480x480 with a step of 32) on all downstream datasets (except for COCO), resulting in 572 models total for each dataset.


% Merging the first and second Pareto sets for each device into a single list of best architectures, we arrived at a set of 52 backbone/neck combinations for COCO pre-training (same architectures with different input resolutions are considered as the same data points here, since COCO training is regardless done on a fixed resolution of 640x640). After performing the COCO training for these selected models, we fine-tune these models at 11 different resolutions (from 160x160 to 480x480 with a step of 32) on all downstream datasets (except for COCO), resulting in 572 models total for each dataset.
% \textcolor{blue}{we identified the Pareto Sets comprising the most promising models concerning the latency-accuracy trade-off for each hardware platform
% The first and second Pareto sets were merged for each device into a unified list of best architectures, resulting in a compilation of 52 backbone/neck combinations for COCO pre-training. Notably, architectures with distinct input resolutions were considered as the same data points here, as COCO training adheres to a fixed resolution of 640x640. Subsequently, these selected models underwent COCO training, followed by fine-tuning at 11 different resolutions (ranging from 160x160 to 480x480 with a step of 32) on all downstream datasets, resulting in a total of 572 models for each dataset.}

% Figure environment removed


Finally, with the obtained fine-tuned model accuracy on several datasets and latency measurements on several devices, we compute the actual Pareto sets for each particular dataset/HW platform combination. Figure \ref{fig:voc_pareto} shows the Pareto frontiers of \textit{YOLOBench} models fine-tuned on the VOC dataset on 4 different devices. Notably, significant differences emerge in these Pareto frontiers between different devices. In particular, the Pareto-optimal set for VIM3 NPU is mostly comprised of YOLOv6 models, with some YOLOv5, YOLOv7, and YOLOv8 models present in the higher accuracy region. This is not found to be the case for the Pareto sets of Intel and ARM CPUs. Despite containing a few YOLOv6 models in the lower latency region, these sets also encompass numerous YOLOv5 and YOLOv7 variations, with limited representation from other model families such as YOLOv3 and YOLOv4.
While the Pareto sets for Intel and ARM CPUs exhibit a certain degree of similarity, the Jetson Nano GPU stands out from the rest of the devices. It showcases a non-uniform distribution of model families, with YOLOv5, YOLOv6, YOLOv7, and YOLOv8 models all represented across the entire accuracy/latency space. Table \ref{tab:pareto_table} shows representative Pareto-optimal models for 3 different datasets (VOC, SKU-110k, WIDERFACE) and 3 hardware platforms under certain latency thresholds. Note that although there are similarities of model family distributions in Pareto sets computed for different datasets (see Appendix B), the exact optimal model for a given latency threshold depends on the specific dataset of interest.

Next, we analyze the statistics of Pareto-optimal models depending on the dataset and hardware platform. Figure \ref{fig:pareto_scaling} shows the distribution of depth factor, width factor, and input resolution values in Pareto frontier models for VOC and SKU-110k datasets on Jetson Nano GPU (data for other datasets and devices are available in Appendix B). The general trend indicates that models at lower input resolutions mostly have lower depth and width factors. This suggests that achieving an optimal latency-accuracy trade-off involves scaling down both the architecture's depth and width before reducing the input resolution. This effect is more pronounced in some datasets (SKU-110k and WIDERFACE), where almost all optimal models are either at the maximal resolution we considered (480x480) with variation in width and depth, or at lower resolutions with minimal width and depth factors. This effect is dataset-dependent, as a more relaxed trend is observed for VOC and COCO datasets, where many optimal models with a variation in width and depth factor are found at resolutions lower than 480x480.

To summarize, we demonstrate that with a state-of-the-art training pipeline and detection head structure, YOLO-based models with various backbone/neck combinations could achieve good latency-accuracy trade-offs in various deployment scenarios, including older backbone/neck structures from YOLOv4 and YOLOv3 models. Furthermore, we show that depth/width reduction precedes input resolution down-scaling in optimal YOLO-based detectors.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Ranking training-free accuracy predictors}
\label{sec:ranking_zc}

% ability to rank candidates will help efficiently finding new YOLO backbones

With an increasing number of architecture blocks and hyperparameter combinations, the size of the candidate model space in \textit{YOLOBench} can further grow exponentially. Hence, it is important to develop efficient methods of filtering out bad architecture proposals before running them through the full training pipeline, including pre-training on the COCO dataset. In the field of neural architecture search, recent works have proposed a handful of training-free, {\em zero-cost} (ZC) estimators that have been shown to perform well on various (relatively simple) benchmarks \cite{mellor2021neural, abdelfattah2021zero, li2023zico}.

Zero-cost estimators were originally proposed by Mellor et al. \cite{mellor2021neural}, and later expanded by Abdelfattah et al. \cite{abdelfattah2021zero} as a means to quickly evaluate the performance of an architecture using only a mini-batch of data. These estimators work by extracting statistics obtained from a forward (and/or backward) pass of a few mini-batches of data through the network, hence eliminating the need for full training of the model. Despite the fact that over 20 different zero-cost accuracy estimators have been introduced in recent years, simple baselines like the number of parameters and MAC count are still found to be hard to outperform \cite{li2023zico}.

The vast search space of YOLO-like architectures necessitates the development of effective training-free estimators to filter out bad candidates and reduce the search space. We examine the performance of a representative subset of zero-cost estimators on \textit{YOLOBench}, namely: 
Fisher~\cite{turner2019blockswap}, GradNorm~\cite{abdelfattah2021zero}, GraSP~\cite{wang2020picking}, JacobCov~\cite{abdelfattah2021zero}, Plain~\cite{abdelfattah2021zero}, SNIP~\cite{lee2018snip}, SynFlow~\cite{tanaka2020pruning}, ZiCo~\cite{li2023zico}, Zen-score~\cite{lin2021zen} and NWOT~\cite{mellor2021neural}. The NWOT metric is computed by measuring the Hamming distance between binary codes produced by each layer's activations \cite{mellor2021neural}. Although originally proposed for ReLU-based networks, we observe that it works well in practice for YOLO variations, most of which contain SiLU activations. The NWOT metric can also be computed by taking the signs of each layer's output features before the activation layer to form the binary code. We refer to that version of the NWOT metric as \textit{NWOT (pre-act)} ("pre-activation") and find that its performance might differ significantly from the original NWOT metric, primarily because the binary codes are computed before the normalization layers followed by the activations. We also compare the performance of the zero-cost predictors with simple baselines such as the number of trainable parameters and MAC count, as well as with a training-based proxy that we have used to pre-select models for \textit{YOLOBench} (mAP$_{50-95}$ in training from scratch on the VOC dataset). 

All zero-cost metrics are computed on randomly initialized models with the same loss function as used for training of all \textit{YOLOBench} models and using a single mini-batch of data with a corresponding image resolution (except for ZiCo, which requires two different mini-batches of data~\cite{li2023zico}). We empirically evaluate the considered set of zero-cost proxies on \textit{YOLOBench} using the following metrics: 
%(i) Kendall $\tau$ (global): Kendall rank correlation coefficient evaluated on all \textit{YOLOBench} models, (ii) Kendall $\tau$ (top-15\%): Kendall rank correlation coefficient evaluated on the top-15\% performing \textit{YOLOBench} models (in terms of mAP$_{50-95}$ value), (iii) Percentage of all actual Pareto-optimal models in the Pareto set determined with the zero-cost estimator in the zero-cost proxy-latency space (recall for Pareto-optimal model prediction using the ZC-based Pareto set).
\begin{itemize}
    \item Kendall $\tau$ (global): Kendall rank correlation coefficient evaluated on all \textit{YOLOBench} models
    \item Kendall $\tau$ (top-15\%): Kendall rank correlation coefficient evaluated on the top-15\% performing \textit{YOLOBench} models (in terms of mAP$_{50-95}$ value)
    \item Percentage of all actual Pareto-optimal models in the Pareto set determined with the zero-cost estimator in the zero-cost proxy-latency space (recall for Pareto-optimal model prediction using the ZC-based Pareto set)
\end{itemize}
The last metric effectively measures how accurate the computed Pareto set would be if the proxy values are used instead of actual mAP to rank models. It is calculated by determining Pareto fronts for model rankings based on zero-cost proxies (and real latency measurements) and then estimating how many models present in the actual Pareto set are also present in the ZC-based Pareto set. In other words, a recall value of 0.7 would mean that by taking the models from the ZC-based Pareto set as candidates, we find 70\% of all actual Pareto-optimal models in that candidate set. We report values for Pareto fronts computed with latency measurements on the Jetson Nano GPU in Table \ref{tab:zc_table}.

We generally find that all of the zero-cost predictors we consider (except for NWOT) are outperformed by the simple baseline of MAC count both in terms of Kendall-Tau scores as well as in the percentage of predicted Pareto-optimal models (see Table \ref{tab:zc_table}). Furthermore, when compared with using mAP$_{50-95}$ on VOC training from scratch as a predictor, we observe that only NWOT comes close to it in terms of ranking scores. We also find that the pre-activation version of NWOT tends to work better than standard NWOT on \textit{YOLOBench}. For the task of predicting mAP$_{50-95}$ of models fine-tuned on SKU-110k, we notably observe that pre-activation NWOT outperforms VOC training from scratch metric in terms of Kendall-Tau scores (possibly due to domain difference between VOC and SKU-110k datasets), but the VOC-based proxy metric still performs better for Pareto-optimal model prediction on SKU-110k. For the data on the sensitivity of NWOT predictions to hyperparameter values please refer to Appendix C.

\begin{table}
  \small
  %\begin{center}
    \caption{COCO test and minival mAP and inference latency on Raspberry Pi 4 CPU (TFLite, FP32) for YOLOv8s vs. a model identified from the NWOT-latency Pareto frontier. For latency, mean and standard deviation over 5 runs (each run done for 100 iterations) are shown, with 640x640 input resolution. For mAP, the mean and standard deviation over three random seeds are shown.} %All models trained for 300 epochs from random initialization, benchmarked on 640x640 input resolution.}
    \label{tab:timm_coco}
    \vspace*{2mm}
    \begin{tabularx}{\columnwidth}{X|X|X|X} % <-- Alignments: 1st column left, 2nd middle and 3rd right, with vertical lines in between
      \hline
      {Model} & {mAP$^{test}_{50-95}$} & {mAP$^{val}_{50-95}$} & {Latency, ms}\\
      % {} & {} & {ms} \\
      \hline
      {YOLOv8s} & {43.17\% (0.12\%)} & {44.43\% (0.23\%)} & {1476.09 (1.49)} \\
      \hline
      {YOLOv8s (HSwish)} & {42.90\% (0.00\%)} & {44.23\% (0.10\%)} & {1381.62 (7.34)} \\
      \hline
      {YOLO-FBNetV3-D-PAN-C3} & {\textbf{43.87\%} (0.05\%)} & {\textbf{45.30\%} (0.08\%)} & {\textbf{1355.21} (9.93)} \\
      \hline
    \end{tabularx}
  %\end{center}
\end{table}


In trying to capture all the real Pareto-optimal models using ZC scores, one could expand the ZC-based candidate pool by calculating subsequent Pareto sets (second, third, fourth, and so forth) and incorporating them into the candidate pool. By applying this strategy, it's possible to identify the complete set of actual Pareto-optimal models while examining only a subset of the entire dataset (e.g., the first $N$ ZC-based Pareto fronts). In this context, we compute candidate pools consisting of $N$ ZC Pareto fronts for each ZC metric and look at the percentage of actual Pareto-optimal models found in the pool versus the pool size (as \% of the full dataset size). Looking at the pool size is motivated by the observation that the number of models in ZC-based Pareto fronts can significantly vary depending on the specific ZC metric used.

Figure \ref{fig:zc_pareto_voc} shows the percentage of predicted real Pareto-optimal models on the VOC dataset contained in pools of $N$ first Pareto fronts for 4 different predictors (VOC training from scratch, NWOT, pre-activation NWOT, and MAC count). For ARM and Intel CPUs, we observe a general trend of VOC training from scratch being the best predictor and MAC count being the worst at all points. Interestingly, for Jetson Nano GPU NWOT performs close to VOC training from scratch for $N = 1,2$ but starts to perform worse with more models in the pool. Surprisingly, MAC count and pre-activation NWOT, which are training-free predictors, outperform VOC training from scratch in predicting Pareto-optimal models on VIM3 NPU.

\subsection{Pareto-optimal detector identification using NWOT score}

To demonstrate the potential of using ZC-based Pareto sets in identifying promising detector architectures with good accuracy-latency trade-off, we additionally generate multiple candidate architectures based on CNN backbones provided by the \texttt{timm} library \cite{rw2019timm}. The architectures are generated by using one of the 347 CNN-based backbones available in \texttt{timm} as a feature extractor followed by a modified Path Aggregation Network (PAN) (same structure with C3 blocks as in YOLOv5 is used, with the number of channels corresponding to YOLOv5s, without the SPPF block) and a YOLOv8 detection head, as in all other \textit{YOLOBench} models. 

We compute the pre-activation NWOT scores as well as measure inference latency on Raspberry Pi 4 ARM CPU with TFLite for all candidate models. We then use the NWOT score and latency values for each model to compute the Pareto frontier in the NWOT-latency space (see Appendix D). We then train one of the models identified to belong to the NWOT-based Pareto frontier (YOLO with FBNetV3-D backbone) on the COCO dataset with a similar setup used to pre-train \textit{YOLOBench} models (640x640 input resolution, 500 epochs, batch size 256, other hyperparameters set to default of Ultralytics YOLOv8 \cite{ultralytics})\footnote{Note that YOLOv8s results provided by Ultralytics \cite{ultralytics} are slightly higher than the ones we report. However, no script to reproduce those results has been released to date.}. The resulting model is found to be more accurate and faster than YOLOv8s (a model in a similar latency range) when tested on Raspberry Pi 4 CPU with TFLite (FP32, XNNPACK backend) (see Table \ref{tab:timm_coco}). Furthermore, we look at the accuracy and latency of a YOLOv8s modification with SiLU activations replaced with Hardswish activations (Table \ref{tab:timm_coco}), as we observe the choice of activation function to be a significant factor affecting TFLite inference latency. We find that the identified NWOT-Pareto model (also containing Hardswish activations in the backbone, neck, and head) still outperforms YOLOv8s-HSwish in terms of latency and accuracy.
