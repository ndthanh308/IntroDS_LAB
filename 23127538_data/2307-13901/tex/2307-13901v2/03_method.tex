\section{Methodology}
\label{sec:method}

\begin{table}
  \small
  %\begin{center}
    \caption{\textit{YOLOBench} architecture space (variation of backbone/neck, depth, width, and input resolution).}
    \label{tab:yolobench_space}
    \vspace*{1mm}
    \begin{tabularx}{\columnwidth}{c|c|c} % <-- Alignments: 1st column left, 2nd middle and 3rd right, with vertical lines in between
      \textbf{Model} & \textbf{Backbone} & \textbf{Neck}\\
      \hline
      YOLOv3 \cite{redmon2018yolov3} & DarkNet53 & FPN \\
      YOLOv4 \cite{bochkovskiy2020yolov4} & CSPDNet53 & SPP-PAN \\
      YOLOv5 \cite{Jocher_YOLOv5_by_Ultralytics_2020} & CSPDNet53-C3 & SPPF-PAN-C3 \\
      YOLOv6s-3 \cite{li2023yolov6} & EfficientRep & RepBiFPAN\\
      YOLOv6m-3 \cite{li2023yolov6} & CSPBep (e=2/3) & CSPRepBiFPAN\\
      YOLOv6l-3 \cite{li2023yolov6} & CSPBep (e=1/2) & CSPRepBiFPAN\\
      YOLOv7 \cite{wang2023yolov7} & E-ELAN & SPPF-ELAN-PAN \\
      YOLOv8 \cite{ultralytics}&  CSPDNet53-C2f & SPPF-PAN-C2f \\
      \hline
    \end{tabularx}
    \vspace*{1mm}
    \\
    \centering
    Width factor $\in$ \{0.25, 0.5, 0.75, 1.0\} \\
    Depth factor $\in$ \{0.33, 0.67, 1.0\} \\
    Input resolution $\in$ \{160:480:32\}
  %\end{center}
\end{table}

The purpose of the current study is to thoroughly study the impact of the backbone and neck and its parameters (width, depth, input resolution) on the 
performance of YOLO detectors in terms of their accuracy and latency. For the rest of the factors influencing the accuracy-latency trade-off, such as choice of the detection head, loss function, training pipeline, and hyperparameters, we aim to have a fixed, controlled setup, so that we can isolate the effect of backbone and neck design on model performance. For this reason, we use the anchor-free decoupled detection head of YOLOv8 \cite{yolo_survey}, as well as CIoU and DFL losses for bounding box prediction used in YOLOv8, as they have been shown to produce state-of-the-art results on the COCO dataset. Anchor-free detection in YOLO models has been also shown to provide latency benefits in the end-to-end detection pipelines \cite{rtdetr}. Hence, the main source of variation in \textit{YOLOBench} models is the structure and parameters of the backbone and neck. We also use the same training code and hyperparameters for all models, as set by default in the YOLOv8 training code released by Ultralytics \cite{ultralytics}, which provides a relatively simple training loop capable of producing SOTA results.

The flow of candidate model generation, pre-selection, and training is shown in Figure \ref{fig:scheme}. First, we generate the full architecture space consisting of about $1000$ models by independently varying the backbone/neck structure, depth factor, width factor, and input resolution (Table \ref{tab:yolobench_space}). For each architecture, we consider its variations trained and tested on $11$ different input resolutions (from $160$x$160$ to $480$x$480$ with a step of $32$) and $12$ variations in depth and width, aside from $4$ usually considered scaling variants ($n$, $s$, $m$ and $l$). The only exception is the YOLOv7 models, for which we only vary the width factor producing $4$ variations of the model. For YOLOv6 models, we use the v3.0 version \cite{li2023yolov6}, for which provided $s$, $m$ and $l$ variations actually represent different architectures aside from different depth and width factors (see Table \ref{tab:yolobench_space}). Hence, we consider YOLOv6s, YOLOv6m and YOLOv6l as different model families and generate the same 12 depth-width combinations for each one.

{\bf Latency measurements.} The actual inference latency for each model might vary significantly depending on the deployment environment and runtime. Therefore, we collect the latency measurements for each of the models by running inference on $4$ different hardware platforms (runtime and inference precisions specified in brackets): 
%(i) NVIDIA Jetson Nano GPU (ONNX Runtime, FP32), (ii) Khadas VIM3 NPU (AML NPU SDK, INT16), (iii) Raspberry Pi 4 Model B CPU (TFLite with XNNPACK, FP32), (iv) Intel$^{\circledR}$ Core\texttrademark i7-10875H CPU (OpenVINO, FP32).

\begin{itemize}
    \item NVIDIA Jetson Nano GPU (ONNX Runtime, FP32)
    \item Khadas VIM3 NPU (AML NPU SDK, INT16)
    \item Raspberry Pi 4 Model B CPU (TFLite with XNNPACK, FP32)
    \item Intel$^{\circledR}$ Core\texttrademark i7-10875H CPU (OpenVINO, FP32)
\end{itemize}

We did not consider latency measurements for INT8 precision, as depending on the quantization scheme (e.g. per-tensor vs. per-channel) and approach (e.g. post-training quantization vs. quantization-aware training), there can be a varied impact of INT8 quantization on accuracy. Adding INT8 results for both accuracy and latency in \textit{YOLOBench} is a matter of future work. All latency measurements were performed with a batch size of $1$ averaged over $200$ inference cycles (with $5$ warmup steps). We measured the inference time required to execute the YOLO model graph, without taking bounding box post-processing (e.g. non-maximum suppression) into account. Note that for VIM3 NPU measurements, the bounding box decoding post-processing operations (operations after the last convolutional layers of the network) were also skipped due to the limitations of VIM3 SDK.

% isn't it better to enumerate using (iii) instead of 1. ?
{\bf Training pipeline.} To obtain the accuracy metric values for the models, we consider the following $4$ datasets: (i) PASCAL VOC (20 object categories) \cite{pascal-voc-2012}, (ii) SKU-110k (1 class, retail item detection) \cite{sku-110k}, (iii) WIDER FACE (1 class, face detection) \cite{widerface}, (iv) COCO (80 object categories) \cite{lin2014microsoft}. Our motivation to include several smaller-scale (with respect to COCO) but challenging datasets stems from the fact that for many practical deployment use cases, the number of object categories to detect and the amount of available data might be limited. The metric of interest for all datasets is mAP$_{50-95}$. For all selected models, the training procedure starts with pretraining on the COCO dataset (for $300$ epochs, with a batch size of $64$ and $640$x$640$ resolution), afterward the best COCO weights are used as initialization for other datasets, on which we perform fine-tuning for $100$ epochs (batch size of $64$) on all $11$ \textit{YOLOBench} resolutions and select the best weights (in terms of mAP$_{50-95}$ value) for each one. For the COCO dataset, we do not perform fine-tuning on target resolutions, rather we evaluate the model trained on $640$x$640$ images on all target resolutions (to mimic the deployment of pre-trained COCO weights). All other training hyperparameters are set as per default values of the Ultralytics YOLOv8 codebase \cite{ultralytics}. Model weights are randomly initialized for all experiments (e.g. no transfer of ImageNet weights for the backbone is performed).

{\bf Candidate model pre-selection.} In order to reduce the number of training runs on the COCO dataset, we filter out some of the least promising model candidates from the \textit{YOLOBench} architecture space as an initial step of our benchmarking procedure. To determine the most promising models in terms of the accuracy-latency trade-off, we compute a proxy metric that is well correlated with the final mAP values of the models fine-tuned on the target datasets. A natural choice for such a metric is model performance when trained on a smaller-scale representative dataset. For this purpose, we use the VOC dataset to train all the model candidates from scratch (random initialization) for $100$ epochs and use the resulting mAP$_{50-95}$ value as a proxy metric to predict performance on all target datasets (with models fine-tuned on these datasets from COCO pre-trained weights). We observe a good correlation of such a training-based accuracy proxy with final metrics on all considered datasets (even on datasets from other domains, like SKU-110k; see Appendix C). We also examine the performance of training-free accuracy estimators for this task and compare it to mAP of VOC training from scratch (see Section \ref{sec:ranking_zc}).

Once we have the accuracy proxy values and latency measurements for all models in the dataset, we determine the models with the best accuracy-latency trade-off (the Pareto frontier models). We use the OApackage software library \cite{eendebak2019oapackage} to determine the Pareto optimal elements in the latency-accuracy space. We define the second Pareto set as the set of models that are Pareto-optimal if the initial Pareto set models are removed (so that the ``second best'' models in terms of latency-accuracy trade-off become the best). Correspondingly, we define the $N$-th Pareto set. 

For our model pre-selection procedure, we consider the models contained in the first and second Pareto fronts (in terms of mAP$_{50-95}$ in VOC training from scratch), with latency for each considered hardware platform separately. We merge all the first and second Pareto sets for each HW platform to form the list of promising architectures to be selected for COCO pre-training. After the COCO pre-training phase is finished for a model, variations of that architecture on multiple resolutions are considered in the benchmark.

% Figure environment removed

\begin{table*}
\small
\caption{Performance of training-free accuracy predictors on \textit{YOLOBench} models and two datasets (VOC and SKU-110k, from COCO-pretrained weights) compared to using metrics of models trained from scratch on the VOC dataset as a predictor. Refer to Appendix C for the data on all considered zero-cost metrics.}
\vspace{2mm}
\label{tab:zc_table}
\begin{tabularx}{\linewidth}{l|XXX|XXX}
% \begin{tabularx}{\linewidth}{lssssss}
\toprule
{} & \multicolumn{3}{c|}{\text{VOC, mAP$_{50-95}$}} & \multicolumn{3}{c}{\text{SKU-110k, mAP$_{50-95}$}}\\
\midrule
{Predictor metric} & {global $\tau$} & {top-15\% $\tau$} & {\%Pareto pred. (GPU)} & {global $\tau$} & {top-15\% $\tau$} & {\%Pareto pred. (GPU)}\\
\midrule
%{GraSP} & {-0.011} & {-0.068} & {0.062} & {0.040} & {0.032} & {0.025}\\
%{Plain} & {0.029} & {0.069} & {0.015} & {-0.388} & {-0.176} & %{0.025}\\
{JacobCov} & {0.095} & {-0.078} & {0.015} & {0.541} & {0.136} & {0.025}\\
{ZiCo} & {0.195} & {0.016} & {0.015} & {0.115} & {0.081} & {0.025}\\
{Zen} & {0.255} & {0.092} & {0.062} & {0.146} & {0.121} & {0.050}\\
%{GradNorm} & {0.262} & {0.173} & {0.015} & {-0.331} & {-0.072} & {0.025}\\
{Fisher} & {0.280} & {0.156} & {0.015} & {-0.380} & {-0.096} & {0.025}\\
% {L2 norm} & {0.326} & {0.090} & {0.015} & {0.189} & {0.118} & {0.025}\\
{SNIP} & {0.336} & {0.217} & {0.015} & {-0.290} & {-0.059} & {0.025}\\
{\#params} & {0.399} & {0.372} & {0.031} & {0.256} & {0.119} & {0.050}\\
{SynFlow} & {0.558} & {0.227} & {0.062} & {0.512} & {0.254} & {0.100}\\
{MACs} & {0.739} & {0.520} & {0.123} & {0.604} & {0.314} & {0.125}\\
{NWOT} & {0.756} & {0.622} & {0.262} & {0.703} & {0.321} & {\bf{0.200}}\\
{NWOT (pre-act)} & {{\bf 0.827}} & {{\bf 0.623}} & {{\bf 0.292}} & {{\bf 0.765}} & {{\bf 0.406}} & {{\bf 0.200}}\\
\midrule
{VOC training} & {0.847} & {0.665} & {0.369} & {0.739} & {0.374} & {0.425}\\
{from scratch (mAP$_{50-95}$)} &  {} & {} & {} & {}\\
% {mAP$_{50-95}$} & {} & {} & {} & {}\\
\bottomrule
\end{tabularx}
\end{table*}