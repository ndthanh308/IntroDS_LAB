\renewcommand{\thefigure}{S\arabic{figure}}
\renewcommand{\thetable}{S\arabic{table}}

\appendix
\label{sec:appendix}

\section{Latency measurements.}
\label{sec:appendix_A}

Details regarding hardware platforms used to collect latency measurements are outlined in Table \ref{tab:benchmarking_hardware}. Figures \ref{fig:nano_timing_corr} and \ref{fig:raspi4_timing_corr} show the difference of latency value distributions between devices computed for the full initial \textit{YOLOBench} architecture space consisting of $\sim$1000 models. While generally good correlation is observed between model inference latencies on different devices (see also Figure \ref{fig:timing_corr_matrix}), notably latency values measured on Khadas VIM3 NPU differ significantly from latency values on other devices. That is, for models with roughly the same latency on Jetson Nano GPU or Raspi4 ARM CPU, the difference in VIM3 NPU latency could be up to several times.  This difference between NPU values from other common GPU/CPU-based platforms highlights the necessity to develop hardware-aware architecture design and search methods. The difference in the NPU benchmark is also reflected in the structure of model Pareto frontiers (Figs. \ref{fig:voc_pareto}, \ref{fig:coco_pareto}, \ref{fig:sku_pareto}) and the performance of zero-cost predictors in identifying Pareto-optimal models (Figs. \ref{fig:zc_pareto_voc}, \ref{fig:zc_sku_parero_front}). 

\section{\textit{YOLOBench} Pareto frontiers for different datasets.}
\label{sec:appendix_B}

\textit{YOLOBench} Pareto frontiers for SKU-110k, WIDER FACE, and COCO datasets are shown in Figs. \ref{fig:sku_pareto}, \ref{fig:wider_pareto}, \ref{fig:coco_pareto}, correspondingly. Note that while mAP$_{50-95}$ values for VOC, SKU-110k, and WIDER FACE datasets are obtained by fine-tuning COCO pre-trained weights (all trained at 640x640 image resolution) on multiple image resolutions considered in \textit{YOLOBench} (11 values from 160 to 480 with a step of 32), the mAP$_{50-95}$ values on the COCO dataset are obtained by directly evaluating pre-trained COCO weights, without fine-tuning on the corresponding target image resolutions. This corresponds to the situation of deployment of pre-trained COCO weights without any additional training.

Table \ref{tab:pareto_table_full} shows the identified Pareto-optimal YOLO models on 3 different datasets and 4 hardware platforms under several latency thresholds. It can be noted that under the same latency threshold on a given hardware platform, the optimal YOLO model family and input image resolution are typically dataset-dependent.

Figures \ref{fig:scaling_raspi4} and \ref{fig:scaling_vim3} show the statistics of architecture scaling parameters (width factor, depth factor, image resolution) in Pareto-optimal models on Raspberry Pi4 CPU and VIM3 NPU, respectively. Although some differences are observed between devices and datasets (in particular depth factor distributions), there is a general trend in all computed Pareto fronts where a variation in depth/width factors is observed at higher resolutions, and resolution is reduced when the depth/width factors (especially the width factor) already have low values.

\begin{table*}[ht!]
\caption{Details on hardware platforms and corresponding runtimes used for benchmarking.}
\vspace*{3mm}
\label{tab:benchmarking_hardware}
\resizebox{\linewidth}{!}{
\begin{tabular}{l|c|c|c|c}
% \begin{tabularx}{\linewidth}{lsssss}
\toprule
{\bf } & {\bf Raspberry Pi 4 Model B} & {\bf Jetson Nano (NVIDIA)} & {\bf Khadas VIM3} & {\bf Lambda tensorbook}\\
\midrule
{CPU} & {\vtop{\hbox{\strut Quad Core Cortex-A72,}\hbox{\strut 64-bit SoC @1.8GHz}}} & {\vtop{\hbox{\strut Quad Core Cortex-A57 MPCore,}\hbox{\strut 64-bit SoC @1.43GHz}}} & {\vtop{\hbox{\strut Quad Core Cortex-A73 @2.2Ghz,}\hbox{\strut Dual Core Cortex-A53 @1.8Ghz}}} & {\vtop{\hbox{\strut Intel$^{\circledR}$ Core\texttrademark i7-10875H CPU}\hbox{\strut @ 2.30GHz}}} \\
\midrule
{Memory} & {4GB LPDDR4-3200 SDRAM} & {\vtop{\hbox{\strut 4 GB 64-bit LPDDR4,}\hbox{\strut 1600MHz 25.6 GB/s}}} & {4GB LPDDR4/4X} & {64GB DDR4 SDRAM}\\
\midrule
{AI-chip} & {-} & {\vtop{\hbox{\strut NVIDIA Maxwell GPU,}\hbox{\strut 128 NVIDIA CUDA$^{\circledR}$ cores}}} & {\vtop{\hbox{\strut Custom NPU}\hbox{\strut INT8 inference up to 1536 MAC}}} & {NVIDIA RTX 2080 Super Max-Q} \\
%\midrule
%{Interface} & {USB 3.0/2.0} & {1 x4(PCIe Gen2)} & {USB-3.0/PCIe Gen2}& {1 x4(PCIe Gen2)} \\
\midrule
{Ops}  & {-}& {472 GFLOPs} & {5.0 TOPS}& {-}\\
%\midrule
%{TDP} & {2.7-6.4 W}& {5W / 10W}& {24-30W}& {CPU: 45 W}\\
\midrule 
%{First Party OS} & {Raspbian 32/64 bit} & {Ubuntu 64 bit} & {Ubuntu 64 bit}& {Ubuntu 64 bit}\\
%\midrule
{Framework/runtime} & {TensorFlow Lite (FP32, XNNPACK backend)}& { ONNX Runtime (FP32, GPU)} & {AML NPU SDK (INT16)} & {OpenVINO (CPU, FP32)}\\
\bottomrule
\end{tabular}}
\end{table*}
 
\section{Performance of zero-cost accuracy predictors on \textit{YOLOBench}.}
\label{sec:appendix_C}

The performance of zero-cost accuracy predictors used in neural architecture search \cite{abdelfattah2021zero} is empirically evaluated on \textit{YOLOBench} models on VOC and SKU-110k. Table \ref{tab:zc_table_full} shows the Kendall-Tau scores and Pareto-optimal model prediction recall values obtained by a variety of zero-cost predictors. The zero-cost predictor values are computed using a randomly sampled batch of test set data with batch size $= 16$ (the used batch was the same for all ZC metrics). MAC count and the number of parameters are computed for models in evaluation mode, with normalization layers fused into preceding convolutions (if possible), and RepVGG-style blocks \cite{ding2021repvgg} also fused, if present in the model. Hence, the performance of MAC and parameter counts might slightly differ if computed for models in training mode. Most predictors perform poorly and are outperformed by the MAC count baseline, except for the NWOT score (in particular the pre-activation version of it). The good performance of NWOT can be also observed in Fig. \ref{fig:ZC_scatter}, where scatter plots of fine-tuned model mAP$_{50-95}$ vs. zero-cost predictor value are shown for a few predictors. Some predictors (notably parameter count, ZiCo, and Zen-score) can be observed to produce very close values for subsets of models with significantly different accuracy. This is an indication of the fact that these predictors perform poorly in estimating accuracy differences in models when the underlying architecture is fixed, but the input image resolution is varied. 

We also test the performance of a training-based predictor on \textit{YOLOBench} which is the mAP$_{50-95}$ values of models trained on a representative dataset (VOC) from scratch for 100 epochs. This predictor sets a strong baseline to be outperformed by training-free predictors, as it is generally found to perform well on a variety of datasets (see Fig. \ref{fig:voc_scratch_corr}), including datasets from different visual domains (e.g. SKU-110k).

We further look into the robustness of the results obtained with the pre-activation NWOT estimator. Since this zero-cost estimator does not require computing the loss function, the main parameters that could influence its performance are the exact batch of data sampled, the batch size, and the dataset split (training or test data) used to sample the batch. Figure \ref{fig:nwot_robust} shows the global Kendall-Tau scores achieved with pre-activation NWOT on VOC \textit{YOLOBench} models with different batches sampled, different batch sizes and different data splits used. There is an observed variance in performance depending on the sampled batch, which is higher when the test set data are used (with an absolute difference of up to $0.05$ in global Kendall-Tau score). Notably, scores computed on training set data (with augmentations) performed better on average compared to test set data, and performance is observed to decrease with increasing batch size. Furthermore, Table \ref{tab:nwot_robust_hohead} shows the mean and standard deviation of Kendall-Tau scores for the standard and pre-activation versions of NWOT on VOC \textit{YOLOBench} models computed on 5 different batches of size 16. We also estimate the performance of the mean predictor values averaged over the 5 sampled batches, which is expectedly found to outperform predictors computed on single batches. Moreover, we compute the pre-activation NWOT scores for all layers in YOLO models except the ones contained in detection heads. This is motivated by the fact that the larger distances between binary activation codes in NWOT are meant to correlate with better performance for the feature extraction layers (e.g. layers in the backbone and neck of YOLO), not the last layers used to compute model predictions. We find an overall performance boost in terms of Kendall-Tau scores for the case when the NWOT score is computed only for the layers in the backbone and neck (Table \ref{tab:nwot_robust_hohead}).

\section{Pareto-optimal model prediction using training-free proxies.}
\label{sec:appendix_D}

We evaluate the training-free accuracy predictors (and the training-based one, VOC training from scratch) for the task of predicting Pareto-optimal models. That is, if one computes the ZC values for each model and determines the Pareto set of models in the ZC value-real latency two-dimensional space, we want to estimate how many models in that Pareto set are going to also be present in the actual Pareto frontier (computed in the two-dimensional mAP$_{50-95}$-latency space). Two metrics are of importance here: recall (how many of actual mAP-latency Pareto-optimal models are captured by a ZC-based Pareto set) and precision (how many of ZC-based Pareto set models are actually Pareto optimal in the real mAP-latency space). Additionally, one could consider the first $N$ ($N=1, 2, 3,...$) ZC-based Pareto sets to expand the set of potential model candidates. We look at how precision and recall values change with $N$ for a few well-performing predictors (NWOT, pre-activation NWOT, MAC count, and VOC training from scratch) with latency values taken from different target devices. 

Recall values for several zero-cost predictors for Pareto models on Jetson Nano GPU and VOC dataset are shown in Fig. \ref{fig:ZC_pareto_VOC_nano}. Corresponding precision values for a few well-performing predictors on 3 different HW platforms are shown in Fig. \ref{fig:ZC_pareto_VOC_precision}. Recall values for these best-performing predictors on the SKU-110k dataset are shown in Fig. \ref{fig:zc_sku_parero_front}.

A different way to evaluate the predictors on \textit{YOLOBench} is to treat models with the same architectures but different input image resolutions as identical data points. That is, if a certain architecture is predicted by ZC-based Pareto front to be optimal on a certain resolution, we count that as a correct prediction if that same architecture on a different resolution is found to be really Pareto-optimal. Such a way to evaluate ZC performance stems from the fact that in practice one typically wishes to predict the most promising architectures, not necessarily the particular optimal image resolution (since that architecture would be pre-trained with a certain fixed resolution, e.g. 640x640 on a dataset like COCO for further fine-tuning on the target dataset). Recall and precision values for such an evaluation protocol for the VOC dataset are shown in Figs. \ref{fig:ZC_pareto_VOC_nores}, \ref{fig:ZC_pareto_VOC_nores_precision}.

We also evaluate the performance of the best training-free predictor (pre-activation NWOT) in predicting Pareto-optimal models, when the latency values used are different from actual latency measurements, but either are computed via a latency proxy like MAC count or measurement on another device. Note that in the case of MAC count as a latency predictor, the whole Pareto-frontier computation process is zero-cost: the approximation for mAP is given by the pre-activation NWOT score, the approximation for latency by MAC count. One might wonder how such a fully zero-cost approach performs in practice. Figures \ref{fig:ZC_pareto_VOC_latproxy} and \ref{fig:ZC_pareto_VOC_latproxy_prec} show the recall and precision values when accuracy predictor is taken to be pre-activation NWOT and latency predictors are varied from MAC count to latencies from other (proxy) devices. Interestingly, MAC count is found to perform relatively well in terms of recall, specifically for Raspberry Pi 4 CPU. Notably, none of the latency proxies work well to predict Pareto-optimal models on VIM3 NPU. Also, perhaps not surprisingly, using Intel CPU latency measurements works well to predict Pareto-optimal models on Raspberry Pi 4 CPU, but does not significantly outperform MAC count.

Finally, we test the pre-activation NWOT accuracy estimator to predict potentially well-performing models out of a set of YOLO models we generated with different CNN backbones from the \texttt{timm} package \cite{rw2019timm}. We have computed the NWOT-latency Pareto set for YOLO-PAN-C3 models with \texttt{timm} backbones on input images of 480x480 resolution, with latency measured on Raspberry Pi 4 ARM CPU (TFLite, FP32). The neck structure (PAN-C3) for each of the candidate models was taken to be that of YOLOv5s and the detection head to be that of YOLOv8 (same as for all \textit{YOLOBench} models), with Hardswish activations in the neck and head, and activation function(s) in the backbone kept the same as originally implemented in \texttt{timm}. Table \ref{tab:nwot_timm_pareto} shows examples of predicted Pareto-optimal models (a subset of the full NWOT-latency Pareto set). Based on these observations, we have selected FBNetV3-D as a potential backbone of a YOLO model to be trained on the COCO dataset and compared it to a reference YOLOv8 model in a similar latency range (YOLOv8s). 

Table \ref{tab:timm_coco_full} shows COCO minival mAP$_{50-95}$ and inference latency results for a YOLO-FBNetV3-D-PAN-C3 model trained on the COCO dataset for 300 epochs and profiled on 640x640 input resolution on Raspi4 CPU with TFLite. We observe that the choice of activation function significantly affects TFLite model inference latency, so for a more fair comparison we also train and profile a Hardswish-based version of YOLOv8s in addition to its default SiLU-based version. While we observe a significant reduction in inference latency with a negligible mAP drop shifting from SiLU to Hardswish, the FBNetV3-based model still outperforms YOLOv8s-HSwish. Furthermore, we train and profile a ReLU-based version of YOLO-FBNetV3-D-PAN-C3 (with activation functions in the backbone kept to be those of the original backbone, i.e. Hardswish, but neck and detection head activations replaced with ReLU) and observe further latency improvements at the cost of $\sim 0.56\%$ drop in mAP$_{50-95}$. However, this model is still found to outperform YOLOv8s in terms of both accuracy and latency (see Table \ref{tab:timm_coco_full}). Furthermore, we train the same models for 500 epochs with a batch size of $256$, which is found to achieve better results on COCO minival and test (Table \ref{tab:timm_coco}). Although we could not exactly reproduce COCO minival mAP results for YOLOv8s reported by Ultralytics \cite{ultralytics}, we find that the FBNetV3-based model outperforms both our YOLOv8s mAP results as well as those of Ultralytics, with lower latency on Raspberry Pi 4 CPU. The COCO minival mAP$_{50-95}$ values reported in Table \ref{tab:timm_coco} were obtained using \texttt{pycocotools} \cite{pycocotools} (with IoU threshold for NMS $=0.6$ and object confidence threshold for detection $=0.001$), and mAP values on test-dev were obtained using the same evaluation parameters by submitting to the competition server \cite{coco_test}. More details on the performance comparison of models on COCO test-dev are shown in Table \ref{tab:timm_coco_test}. 

%\clearpage

\begin{table*}
\small
\caption{Pareto-optimal \textit{YOLOBench} models on 3 datasets and 4 hardware platforms. Shown are the best models in terms of mAP$_{50-95}$ under a given latency threshold (max. latency). For each model, the scaling parameters are given (d33w25 means depth factor $=0.33$ and width factor $=0.25$), corresponding input resolution of the models is indicated in brackets.}
\vspace{1mm}
\label{tab:pareto_table_full} 
\begin{tabularx}{\linewidth}{lXXXXXX}
%\begin{tabularx}{\linewidth}{lssssss}
\toprule
{HW/max.} & {VOC} & {VOC} & {SKU-110k} & {SKU-110k} & {WIDERFACE} & {WIDERFACE} \\ %& {WIDER} & {WIDER} \\
{latency} & {model} & {mAP$_{50-95}$} & {model} & {mAP$_{50-95}$} & {model} & {mAP$_{50-95}$} \\ %& {model} & {mAP} \\
\midrule
{Nano/0.5 sec} & {YOLOv8} & {0.726} & {YOLOv7} & {0.593} & {YOLOv7} & {0.382}\\
{} & {d67w1 (448)} & {} & {d1w75 (480)} & {} & {d1w75 (480)} & {}\\
{Nano/0.3 sec} & {YOLOv7} & {0.701} & {YOLOv7} & {0.589} & {YOLOv7} & {0.369}\\
{} & {d1w5 (480)} & {} & {d1w5 (480)} & {} & {d1w5 (480)}\\
{Nano/0.1 sec} & {YOLOv7} & {0.657} & {YOLOv8} & {0.567} & {YOLOv7} & {0.336}\\
{} & {d1w5 (288)} & {} & {d1w25 (480)} & {} & {d1w25 (480)}\\
\midrule
{VIM3/0.3 sec} & {YOLOv8} & {0.726} & {YOLOv7} & {0.593} & {YOLOv7} & {0.382}\\
{} & {d67w1 (448)} & {} & {d1w75 (480)} & {} & {d1w75 (480)} & {}\\
{VIM3/0.1 sec} & {YOLOv6l} & {0.669} & {YOLOv8} & {0.567} & {YOLOv6m} & {0.350}\\
{} & {d67w5 (384)} & {} & {d1w25 (480)} & {} & {d33w5 (480)}\\
{VIM3/0.05 sec} & {YOLOv6l} & {0.620} & {YOLOv6s} & {0.556} & {YOLOv6m} & {0.318}\\
{} & {d67w25 (416)} & {} & {d33w25 (480)} & {} & {d67w25 (480)}\\
\midrule
{Intel/0.08 sec} & {YOLOv8} & {0.719} & {YOLOv7} & {0.593} & {YOLOv7} & {0.382}\\
{} & {d1w75 (416)} & {} & {d1w75 (480)} & {} & {d1w75 (480)} & {}\\
{Intel/0.04 sec} & {YOLOv7} & {0.701} & {YOLOv7} & {0.589} & {YOLOv7} & {0.369}\\
{} & {d1w5 (480)} & {} & {d1w5 (480)} & {} & {d1w5 (480)}\\
{Intel/0.02 sec} & {YOLOv6l} & {0.682} & {YOLOv6l} & {0.576} & {YOLOv6l} & {0.346}\\
{} & {d6w5 (448)} & {} & {d33w5 (480)} & {} & {d33w5 (480)}\\
\midrule
{Raspi4/3 sec} & {YOLOv8} & {0.719} & {YOLOv7} & {0.593} & {YOLOv7} & {0.382}\\
{} & {d1w75 (416)} & {} & {d1w75 (480)} & {} & {d1w75 (480)} & {}\\
{Raspi4/1 sec} & {YOLOv7} & {0.701} & {YOLOv7} & {0.589} & {YOLOv7} & {  0.369}\\
{} & {d1w5 (480)} & {} & {d1w5 (480)} & {} & {d1w5 (480)}\\
{Raspi4/0.5 sec} & {YOLOv6l} & {0.669} & {YOLOv4} & {0.569} & {YOLOv7} & {0.336}\\
{} & {d67w5 (384)} & {} & {d1w25 (480)} & {} & {d1w25 (480)}\\
\bottomrule
\end{tabularx}
\end{table*}

\begin{table*}
\small
\caption{Performance of training-free accuracy predictors on \textit{YOLOBench} models and two datasets (VOC and SKU-110k, from COCO-pretrained weights) compared to using mAP$_{50-95}$ of models trained from scratch on the VOC dataset as a predictor.}
\vspace{2mm}
\label{tab:zc_table_full}
\begin{tabularx}{\linewidth}{l|XXX|XXX}
% \begin{tabularx}{\linewidth}{lssssss}
\toprule
{} & \multicolumn{3}{c|}{\text{VOC, mAP$_{50-95}$}} & \multicolumn{3}{c}{\text{SKU-110k, mAP$_{50-95}$}}\\
\midrule
{Predictor metric} & {global $\tau$} & {top-15\% $\tau$} & {\%Pareto pred. (GPU)} & {global $\tau$} & {top-15\% $\tau$} & {\%Pareto pred. (GPU)}\\
\midrule    
{GraSP} & {-0.011} & {-0.068} & {0.062} & {0.040} & {0.032} & {0.025}\\
{Plain} & {0.029} & {0.069} & {0.015} & {-0.388} & {-0.176} & {0.025}\\
{JacobCov} & {0.095} & {-0.078} & {0.015} & {0.541} & {0.136} & {0.025}\\
{ZiCo} & {0.195} & {0.016} & {0.015} & {0.115} & {0.081} & {0.025}\\
{Zen} & {0.255} & {0.092} & {0.062} & {0.146} & {0.121} & {0.050}\\
{GradNorm} & {0.262} & {0.173} & {0.015} & {-0.331} & {-0.072} & {0.025}\\
{Fisher} & {0.280} & {0.156} & {0.015} & {-0.380} & {-0.096} & {0.025}\\
{L2 norm} & {0.326} & {0.090} & {0.015} & {0.189} & {0.118} & {0.025}\\
{SNIP} & {0.336} & {0.217} & {0.015} & {-0.290} & {-0.059} & {0.025}\\
{\#params} & {0.399} & {0.372} & {0.031} & {0.256} & {0.119} & {0.050}\\
{SynFlow} & {0.558} & {0.227} & {0.062} & {0.512} & {0.254} & {0.100}\\
{MACs} & {0.739} & {0.520} & {0.123} & {0.604} & {0.314} & {0.125}\\
{NWOT} & {0.756} & {0.622} & {0.262} & {0.703} & {0.321} & {\bf{0.200}}\\
{NWOT (pre-act)} & {{\bf 0.827}} & {{\bf 0.623}} & {{\bf 0.292}} & {{\bf 0.765}} & {{\bf 0.406}} & {{\bf 0.200}}\\
\midrule
{VOC training} & {0.847} & {0.665} & {0.369} & {0.739} & {0.374} & {0.425}\\
{from scratch (mAP$_{50-95}$)} &  {} & {} & {} & {}\\
% {mAP$_{50-95}$} & {} & {} & {} & {}\\
\bottomrule
\end{tabularx}
\end{table*}

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed


% Figure environment removed

% Figure environment removed

% Figure environment removed

\begin{table*}
  \small
  \begin{center}
    \caption{Mean and standard deviation of the global Kendall-Tau scores for NWOT metrics computed for 5 different randomly sampled batches of size 16 on VOC \textit{YOLOBench} models. The metric denoted as "no head" was computed only for the layers contained in the neck and backbone of YOLO models, not in the detection head. The second column shows Kendall-Tau scores for prediction with the mean ZC metric values averaged over the 5 batches.}
    \label{tab:nwot_robust_hohead}
    \vspace*{2mm}
    \begin{tabularx}{\linewidth}{X|X|X} % <-- Alignments: 1st column left, 2nd middle and 3rd right, with vertical lines in between
      \hline
      {ZC metric} & {global $\tau$} & {global $\tau$ (prediction with mean ZC value)}\\
      % {} & {} & {ms} \\
      \hline
      {NWOT} & {0.7839 (0.0159)} & {0.7895} \\
      \hline
      {NWOT (pre-act)} & {0.8402 (0.0191)} & {0.8486} \\
      \hline
      {NWOT (pre-act, no head)} & {\textbf{0.8472 (0.0194)}} & {\textbf{0.8570}} \\
      \hline
    \end{tabularx}
  \end{center}
\end{table*}

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed


% Figure environment removed

% Figure environment removed

% Figure environment removed



\begin{table*}
\centering
\caption{Example YOLO-PAN-C3 models with \texttt{timm} backbones identified in the NWOT-latency Pareto frontier, with pre-activation NWOT score computed on the VOC dataset. Latency values are measured on Raspberry Pi 4 ARM CPU with TFLite (FP32), batch size 1.}
\label{tab:nwot_timm_pareto}
%\begin{tabular}{c|c|c}
\begin{tabularx}{\textwidth}{l|X|X|X}
\toprule
% Model name & Input resolution & GMACs & NWOT (pre-act) \\
Model name & Input resolution & Raspi4 CPU latency, sec & NWOT (pre-act) \\
\midrule
\texttt{yolo\_pan\_efficientnet\_b4} & { 480} & { 1.72} & { 511.84 }  \\
\texttt{yolo\_pan\_tf\_efficientnet\_b4\_ap} & { 480} & { 1.71} & { 511.77 }  \\
\texttt{yolo\_pan\_gc\_efficientnetv2\_rw\_t} & { 480} & { 1.41} & { 508.73 }  \\
\texttt{yolo\_pan\_tf\_efficientnet\_lite4} & { 480} & { 1.08} & { 506.67 }  \\
\texttt{yolo\_pan\_fbnetv3\_d} & { 480} & { 0.71} & { 502.71 }  \\
\texttt{yolo\_pan\_tf\_efficientnet\_lite1} & { 480} & { 0.61} & { 493.48 }  \\
\texttt{yolo\_pan\_efficientnet\_lite1} & { 480} & { 0.61} & { 493.32 }  \\
\texttt{yolo\_pan\_mobilenetv2\_110d} & { 480} & { 0.54} & { 480.92 }  \\
\texttt{yolo\_pan\_mobilenetv2\_075} & { 480} & { 0.45} & { 480.14 }  \\
\texttt{yolo\_pan\_tf\_mobilenetv3\_large\_075} & { 480} & { 0.45} & { 468.85 }  \\
\texttt{yolo\_pan\_mobilenetv2\_035} & { 480} & { 0.37} & { 457.41 }  \\
\texttt{yolo\_pan\_tf\_mobilenetv3\_small\_minimal\_100} & { 480} & { 0.36} & { 451.10 }  \\
% \texttt{yolo\_pan\_efficientnet\_b6} & 416 & 265.67 & 520.07 \\
% \texttt{yolo\_pan\_mixnet\_xxl} & 480 & 234.37 & 518.63 \\
% \texttt{yolo\_pan\_regnetz\_d8\_evos} & 384 & 221.56 & 515.61 \\
% \texttt{yolo\_pan\_fbnetv3\_g} & 480 & 157.49 & 515.17 \\
% \texttt{yolo\_pan\_fbnetv3\_g} & 448 & 129.21 & 509.66 \\
% \texttt{yolo\_pan\_fbnetv3\_g} & 416 & 120.60 & 506.66 \\
% \texttt{yolo\_pan\_fbnetv3\_d} & 480 & 111.20 & 502.71 \\
% \texttt{yolo\_pan\_mobilenetv2\_120d} & 448 & 100.48 & 500.79 \\
% \texttt{yolo\_pan\_fbnetv3\_g} & 384 & 95.99 & 500.18 \\
% \texttt{yolo\_pan\_fbnetv3\_d} & 448 & 91.22 & 497.67 \\
% \texttt{yolo\_pan\_fbnetv3\_g} & 352 & 88.62 & 496.58 \\
% \texttt{yolo\_pan\_fbnetv3\_d} & 416 & 85.15 & 494.13 \\
% \texttt{yolo\_pan\_fbnetv3\_g} & 320 & 81.24 & 492.63 \\
% \texttt{yolo\_pan\_mobilenetv2\_120d} & 384 & 74.64 & 491.50 \\
% \texttt{yolo\_pan\_fbnetv3\_d} & 384 & 67.77 & 488.27 \\
% \texttt{yolo\_pan\_fbnetv3\_g} & 288 & 61.56 & 484.83 \\
% \texttt{yolo\_pan\_fbnetv3\_g} & 256 & 55.41 & 480.02 \\
% \texttt{yolo\_pan\_mobilenetv2\_120d} & 288 & 47.85 & 475.86 \\
% \texttt{yolo\_pan\_tf\_efficientnet\_b1\_ap} & 288 & 44.74 & 472.46 \\
% \texttt{yolo\_pan\_tf\_efficientnet\_b1\_ns} & 288 & 44.74 & 472.46 \\
\bottomrule
% \end{tabular}
\end{tabularx}
\end{table*}

\begin{table*}
  \small
  \begin{center}
    \caption{COCO test mAP values and inference latency on Raspberry Pi 4 CPU (TFLite with XNNPACK backend, FP32) for YOLOv8s vs. a model identified from the NWOT-latency Pareto frontier (YOLO-FBNetV3-D-PAN). For mAP values, the mean and standard deviation over three random seeds are shown. For inference time, mean and standard deviation of inference time over 5
    runs (each one 100 iterations) are shown.} 
    \label{tab:timm_coco_test}
    \vspace*{2mm}
    \begin{tabularx}{\linewidth}{X|X|X|X|X|X|X|X} % <-- Alignments: 1st column left, 2nd middle and 3rd right, with vertical lines in between
      \hline
      {Model} & {AP$^{test}_{50-95}$} & {AP$^{test}_{50}$} & {AP$^{test}_{75}$} & {AP$^{test}_{S}$} & AP$^{test}_{M}$ & AP$^{test}_{L}$ & {Latency, ms}\\
      % {} & {} & {ms} \\
      \hline
      {YOLOv8s} & {43.17\% (0.12\%)} & {60.53\% (0.09\%)} & {46.5\% (0.08\%)} & {\textbf{22.7\%} (0.14\%)} & {47.13\% (0.17\%)} & {57.0\% (0.22\%)} & {1476.09 (1.49)} \\
      \hline
      {YOLOv8s-HSwish} & {42.90\% (0.0\%)} & {60.3\% (0.0\%)} & {46.30\% (0.0\%)} & {22.46\% (0.09\%)} & {47.0\% (0.08\%)} & {56.39\% (0.08\%)} & {1381.62 (7.34)} \\
      \hline
      {YOLO-FBNetV3-D-PAN} & {\textbf{43.87\%} (0.05\%)} & {\textbf{61.53\%} (0.09\%)} & {\textbf{47.23\%} (0.05\%)} & {22.67\% (0.19\%)} & {\textbf{47.87\%} (0.05\%)} & {\textbf{58.36\%} (0.12\%)} & {\textbf{1355.21} (9.93)} \\
      \hline
    \end{tabularx}
  \end{center}
\end{table*}


\begin{table*}
  \small
  \begin{center}
    \caption{COCO minival mAP and inference latency on Raspberry Pi 4 CPU (TFLite with XNNPACK backend, FP32) for YOLOv8s vs. a model identified from the NWOT-latency Pareto frontier (YOLO-FBNetV3-D-PAN). Mean and standard deviation of inference time over 5 runs (each one 100 iterations) are shown. The input image resolution used was 640x640, batch size $=1$ for latency measurements. Models were trained for 300 epochs, with batch size $=64$.} %All models trained for 300 epochs from random initialization, benchmarked on 640x640 input resolution.}
    \label{tab:timm_coco_full}
    \vspace*{2mm}
    \begin{tabularx}{\linewidth}{X|X|X} % <-- Alignments: 1st column left, 2nd middle and 3rd right, with vertical lines in between
      \hline
      {Model} & {COCO mAP$^{val}_{50-95}$} & {Raspberry Pi 4 ARM CPU latency, ms}\\
      % {} & {} & {ms} \\
      \hline
      {YOLOv8s} & {43.64\%} & {1476.09 (1.49)} \\
      \hline
      {YOLOv8s-HSwish} & {43.55\%} & {1381.62 (7.34)} \\
      \hline
      {YOLO-FBNetV3-D-PAN} & {44.63\%} & {1355.21 (9.93)} \\
      \hline
      {YOLO-FBNetV3-D-PAN-ReLU} & {44.07\%} & {1344.50 (8.06)} \\
      \hline
    \end{tabularx}
  \end{center}
\end{table*}