\section{Conclusion}
\label{sec:conclusion}

In this work, we have presented \textit{YOLOBench}, a latency-accuracy benchmark of several hundred YOLO-based models on 4 different object detection datasets and 4 different hardware platforms. The accuracy and latency data were collected in a fixed, controlled environment with the only variation in backbone/neck structure and input image resolution of the detectors. We used these data to demonstrate that it is possible to achieve Pareto-optimal results with a range of different backbone structures, including those of the older architectures in the YOLO series, such as YOLOv3 and YOLOv4. We also observed that depth and width scaling precede input resolution scaling in optimal YOLO-based detectors. 

Finally, we have used \textit{YOLOBench} to evaluate zero-cost accuracy predictors, and have found that, while many of the existing state-of-the-art predictors perform poorly, pre-activation NWOT score can be effectively used to identify Pareto-optimal detectors for specific target devices of interest. We have demonstrated that by using NWOT to find a YOLO backbone (FBNetV3-D) that outperforms a state-of-the-art YOLOv8 model when deployed on a Raspberry Pi 4 ARM CPU.