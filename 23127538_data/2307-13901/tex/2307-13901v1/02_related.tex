\section{Related Work}
\label{sec:related}

\begin{table*}
\small
\caption{Pareto-optimal \textit{YOLOBench} models on 3 datasets and 3 hardware platforms. Shown are the best models in terms of mAP$_{50-95}$ under a given latency threshold (max. latency). For each model, the scaling parameters are given (d33w25 means depth factor $=0.33$ and width factor $=0.25$), corresponding input resolution of the models is indicated in brackets.}
\vspace{1mm}
\label{tab:pareto_table} 
\begin{tabularx}{\linewidth}{lXXXXXX}
\toprule
{HW/max.} & {VOC} & {VOC} & {SKU-110k} & {SKU-110k} & {WIDERFACE} & {WIDERFACE} \\ %
{latency} & {model} & {mAP$_{50-95}$} & {model} & {mAP$_{50-95}$} & {model} & {mAP$_{50-95}$} \\ %
\midrule
{Nano/0.1 sec} & {YOLOv7} & {0.657} & {YOLOv8} & {0.567} & {YOLOv7} & {0.336}\\
{} & {d1w5 (288)} & {} & {d1w25 (480)} & {} & {d1w25 (480)}\\
\midrule
{VIM3/0.05 sec} & {YOLOv6l} & {0.620} & {YOLOv6s} & {0.556} & {YOLOv6m} & {0.318}\\
{} & {d67w25 (416)} & {} & {d33w25 (480)} & {} & {d67w25 (480)}\\
\midrule
{Raspi4/0.5 sec} & {YOLOv6l} & {0.667} & {YOLOv4} & {0.569} & {YOLOv7} & {0.336}\\
{} & {d67w5 (384)} & {} & {d1w25 (480)} & {} & {d1w25 (480)}\\
\bottomrule
\end{tabularx}
\end{table*}

% Figure environment removed

There has been a tremendous amount of progress in efficient object detection in recent years pushing the accuracy-latency frontier, including architectures like YOLOv7 \cite{wang2023yolov7}, YOLOv6-3.0 \cite{li2023yolov6}, DAMO-YOLO \cite{xu2022damo}, RTMDet \cite{lyu2022rtmdet}, RT-DETR \cite{rtdetr} and PP-YOLOE \cite{xu2022pp}. These works oftentimes improve upon state-of-the-art latency-accuracy trade-off, providing comparisons of several generations of detectors on the COCO dataset. Benchmarks of different model families are also provided by framework developers, such as MMYOLO \cite{mmyolo2022} and Ultralytics \cite{ultralytics}. Additionally, there exist third-party benchmarks of several architectures from the YOLO series on server-grade and embedded GPUs as well as specialized accelerators \cite{stereo_labs,opencv_yolo_benchmarking,feng2022benchmark,zhu2022performance}. We have identified a few limitations of the existing efficient detector benchmarks that have served as motivation for \textit{YOLOBench}:

\begin{itemize}
    \item Comparisons of different YOLO versions are frequently done either by using a proxy metric for the actual latency like MAC count and number of parameters or by reporting latency values on server-grade GPUs, neither of which is directly indicative of latency on embedded devices,
    \item Accuracy metrics are usually reported on the COCO dataset, which could be considered too large-scale with respect to actual practical use cases,
    \item Some architecture parameters (like input resolution) are often considered to be fixed in detector benchmarking, while it is known that they serve as important factors in optimal CNN scaling \cite{tinynet},
    \item Different YOLO variations being compared to one another are typically trained with different training codebases, training techniques (loss functions, data augmentations), and hyperparameter values, making it hard to disentangle the contribution of the training pipeline improvements vs. better architecture design. 
\end{itemize}

To address these issues, we have conducted a thorough accuracy and latency benchmarking of state-of-the-art YOLO detector versions in controlled, fixed conditions to study the impact of backbone and neck design proposed by several YOLO model families.