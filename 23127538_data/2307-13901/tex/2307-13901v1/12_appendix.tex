\appendix
\label{sec:appendix}

\section{Latency measurements.}
\label{sec:appendix_A}

Details regarding hardware platforms used to collect latency measurements are outlined in Table \ref{tab:benchmarking_hardware}. Figures \ref{fig:nano_timing_corr} and \ref{fig:raspi4_timing_corr} show the difference of latency value distributions between devices computed for the full initial \textit{YOLOBench} architecture space consisting of $\sim$1000 models. While generally good correlation is observed between model inference latencies on different devices (see also Figure \ref{fig:timing_corr_matrix}), notably latency values measured on Khadas VIM3 NPU differ significantly from latency values on other devices. That is, for models with roughly the same latency on Jetson Nano GPU or Raspi4 ARM CPU, the difference in VIM3 NPU latency could be up to several times.  This difference between NPU values from other common GPU/CPU-based platforms highlights the necessity to develop hardware-aware architecture design and search methods. The difference in the NPU benchmark is also reflected in the structure of model Pareto frontiers (Figs. \ref{fig:voc_pareto}, \ref{fig:coco_pareto}, \ref{fig:sku_pareto}) and the performance of zero-cost predictors in identifying Pareto-optimal models (Figs. \ref{fig:zc_pareto_voc}, \ref{fig:zc_sku_parero_front}). 

\section{\textit{YOLOBench} Pareto frontiers for different datasets.}
\label{sec:appendix_B}

\textit{YOLOBench} Pareto frontiers for SKU-110k, WIDER FACE and COCO datasets are shown in Figs. \ref{fig:sku_pareto}, \ref{fig:wider_pareto}, \ref{fig:coco_pareto}, correspondingly. Note that while mAP$_{50-95}$ values for VOC, SKU-110k and WIDER FACE datasets were obtained by fine-tuning COCO pre-trained weights (all trained at 640x640 image resolution) on multiple image resolutions considered in \textit{YOLOBench} (11 values from 160 to 480 with a step of 32), the mAP$_{50-95}$ values on the COCO dataset were obtained by directly evaluating pre-trained COCO weights, without fine-tuning on the corresponding target image resolutions. This corresponds to the situation of deployment of pre-trained COCO weights without any additional training.

Table \ref{tab:pareto_table_full} shows the identified Pareto-optimal YOLO models on 3 different datasets and 4 hardware platforms under several latency thresholds. It can be noted that under the same latency threshold on a given hardware platform, the optimal YOLO model family and input image resolution are typically dataset-dependent.

Figures \ref{fig:scaling_raspi4} and \ref{fig:scaling_vim3} show the statistics of architecture scaling parameters (width factor, depth factor, image resolution) in Pareto-optimal models on Raspberry Pi4 CPU and VIM3 NPU, respectively. Although some differences are observed between devices and datasets (in particular depth factor distributions), there is a general trend in all computed Pareto fronts where a variation in depth/width factors is observed at higher resolutions, and resolution is reduced when the depth/width factors (especially the width factor) already have low values.

\begin{table*}[ht!]
\caption{Details on hardware platforms and corresponding runtimes used for benchmarking.}
\vspace*{3mm}
\label{tab:benchmarking_hardware}
\resizebox{\linewidth}{!}{
\begin{tabular}{l|c|c|c|c}
\toprule
{\bf } & {\bf Raspberry Pi 4 Model B} & {\bf Jetson Nano (NVIDIA)} & {\bf Khadas VIM3} & {\bf Lambda tensorbook}\\
\midrule
{CPU} & {\vtop{\hbox{\strut Quad Core Cortex-A72,}\hbox{\strut 64-bit SoC @1.8GHz}}} & {\vtop{\hbox{\strut Quad Core Cortex-A57 MPCore,}\hbox{\strut 64-bit SoC @1.43GHz}}} & {\vtop{\hbox{\strut Quad Core Cortex-A73 @2.2Ghz,}\hbox{\strut Dual Core Cortex-A53 @1.8Ghz}}} & {\vtop{\hbox{\strut Intel$^{\circledR}$ Core\texttrademark i7-10875H CPU}\hbox{\strut @ 2.30GHz}}} \\
\midrule
{Memory} & {4GB LPDDR4-3200 SDRAM} & {\vtop{\hbox{\strut 4 GB 64-bit LPDDR4,}\hbox{\strut 1600MHz 25.6 GB/s}}} & {4GB LPDDR4/4X} & {64GB DDR4 SDRAM}\\
\midrule
{AI-chip} & {-} & {\vtop{\hbox{\strut NVIDIA Maxwell GPU,}\hbox{\strut 128 NVIDIA CUDA$^{\circledR}$ cores}}} & {\vtop{\hbox{\strut Custom NPU}\hbox{\strut INT8 inference up to 1536 MAC}}} & {NVIDIA RTX 2080 Super Max-Q} \\
\midrule
{Ops}  & {-}& {472 GFLOPs} & {5.0 TOPS}& {-}\\
\midrule 
{Framework/runtime} & {TensorFlow Lite (FP32, XNNPACK backend)}& { ONNX Runtime (FP32, GPU)} & {AML NPU SDK (INT16)} & {OpenVINO (CPU, FP32)}\\
\bottomrule
\end{tabular}}
\end{table*}
 
\section{Performance of zero-cost accuracy predictors on \textit{YOLOBench}.}
\label{sec:appendix_C}

The performance of zero-cost accuracy predictors used in neural architecture search \cite{abdelfattah2021zero} was empirically evaluated on \textit{YOLOBench} models on VOC and SKU-110k. Table \ref{tab:zc_table_full} shows the Kendall-Tau scores and Pareto-optimal model prediction recall values obtained by a variety of zero-cost predictors. MAC count and the number of parameters are computed for models in evaluation mode, with normalization layers fused into preceding convolutions (if possible), and RepVGG-style blocks \cite{ding2021repvgg} also fused, if present in the model. Hence, the performance of MAC and parameter counts might slightly differ if computed for models in training mode. Most predictors perform poorly and are outperformed by MAC count baseline, except for the NWOT score (in particular the pre-activation version of it). The good performance of NWOT can be also observed in Fig. \ref{fig:ZC_scatter}, where scatter plots of fine-tuned model mAP$_{50-95}$ vs. zero-cost predictor value are shown for a few predictors. Some predictors (notably parameter count, ZiCo and Zen-score) can be observed to produce very close values for subsets of models with significantly different accuracy. This is an indication of the fact that these predictors perform poorly in estimating accuracy differences in models when the underlying architecture is fixed, but the input image resolution is varied. 

We have also tested the performance of a training-based predictor on \textit{YOLOBench} which is the mAP$_{50-95}$ values of models trained on a representative dataset (VOC) from scratch for 100 epochs. This predictor sets a strong baseline to be outperformed by training-free predictors, as it is generally found to perform well on a variety of datasets (see Fig. \ref{fig:voc_scratch_corr}), including datasets from different visual domains (e.g. SKU-110k).

\section{Pareto-optimal model prediction using training-free proxies.}
\label{sec:appendix_D}

We have evaluated the training-free accuracy predictors (and the training-based one, VOC training from scratch) for the task of predicting Pareto-optimal models. That is, if one computes the ZC values for each model and determines the Pareto set of models in the ZC value-real latency two-dimensional space, we want to estimate how many models in that Pareto set are going to also be present in the actual Pareto frontier (computed in the two-dimensional mAP$_{50-95}$-latency space). Two metrics are of importance here: recall (how many of actual mAP-latency Pareto-optimal models are captured by a ZC-based Pareto set) and precision (how many of ZC-based Pareto set models are actually Pareto optimal in the real mAP-latency space). Additionally, one could consider the first $N$ ($N=1, 2, 3,...$) ZC-based Pareto sets to expand the set of potential model candidates. We looked at how precision and recall values change with $N$ for a few well-performing predictors (NWOT, pre-activation NWOT, MAC count, and VOC training from scratch) with latency values taken from different target devices. 

Recall values for several zero-cost predictors for Pareto models on Jetson Nano GPU and VOC dataset are shown in Fig. \ref{fig:ZC_pareto_VOC_nano}. Corresponding precision values for a few well-performing predictors on 3 different HW platforms are shown in Fig. \ref{fig:ZC_pareto_VOC_precision}. Recall values for these best-performing predictors on the SKU-110k dataset are shown in Fig. \ref{fig:zc_sku_parero_front}.

A different way to evaluate the predictors on \textit{YOLOBench} is to treat models with the same architectures but different input image resolutions as identical data points. That is, if a certain architecture is predicted by ZC-based Pareto front to be optimal on a certain resolution, we count that as a correct prediction if that same architecture on a different resolution is found to be really Pareto-optimal. Such a way to evaluate ZC performance stems from the fact that in practice one typically wishes to predict the most promising architectures, not necessarily the particular optimal image resolution (since that architecture would be pre-trained with a certain fixed resolution, e.g. 640x640 on a dataset like COCO for further fine-tuning on the target dataset). Recall and precision values for such an evaluation protocol for the VOC dataset are shown in Figs. \ref{fig:ZC_pareto_VOC_nores}, \ref{fig:ZC_pareto_VOC_nores_precision}.

We also evaluated the performance of the best training-free predictor (pre-activation NWOT) in predicting Pareto-optimal models, when the latency values used are different from actual latency measurements, but either are computed via a latency proxy like MAC count or measurement on another device. Note that in case of MAC count as a latency predictor, the whole Pareto-frontier computation process is zero-cost: the approximation for mAP is given by the pre-activation NWOT score, the approximation for latency by MAC count. One might wonder how such a fully zero-cost approach performs in practice. Figures \ref{fig:ZC_pareto_VOC_latproxy} and \ref{fig:ZC_pareto_VOC_latproxy_prec} show the recall and precision values when accuracy predictor is taken to be pre-activation NWOT, and latency predictors are varied from MAC count to latencies from other (proxy) devices. Interestingly, MAC count is found to perform relatively well in terms of recall, specifically for Raspberry Pi 4 CPU. Notably, none of the latency proxies work well to predict Pareto-optimal models on VIM3 NPU. Also, perhaps not surprisingly, using Intel CPU latency measurements works well to predict Pareto-optimal models on Raspberry Pi 4 CPU, but does not significantly outperform MAC count.

Finally, we have tested the pre-activation NWOT accuracy predictor to predict potentially well-performing models out of a set of YOLO models we generated with different CNN backbones from the \texttt{timm} package \cite{rw2019timm}. We have computed the NWOT-latency Pareto set for YOLO-PAN-C3 models with \texttt{timm} backbones on input images of 480x480 resolution, with latency measured on Raspberry Pi 4 ARM CPU (TFLite, FP32). The neck structure (PAN-C3) for each of the candidate models was taken to be that of YOLOv5s and the detection head to be that of YOLOv8 (same as for all \textit{YOLOBench} models), with Hardswish activations in the neck and head, and activation function(s) in the backbone kept the same as originally implemented in \texttt{timm}. Table \ref{tab:nwot_timm_pareto} shows examples of predicted Pareto-optimal models (a subset of the full NWOT-latency Pareto set). Based on these observations, we have selected FBNetV3-D as a potential backbone of a YOLO model to be trained on the COCO dataset and compared it to a reference YOLOv8 model in a similar latency range (YOLOv8s). 

Table \ref{tab:timm_coco_full} shows mAP$_{50-95}$ and inference latency results for a YOLO-FBNetV3-D-PAN-C3 model trained on the COCO dataset and profiled on 640x640 input resolution on Raspi4 CPU with TFLite. We have observed that the choice of activation function significantly affects TFLite model inference latency, so for a more fair comparison we have also trained and profiled a Hardswish-based version of YOLOv8s in addition to its default SiLU-based version. While we have observed a significant reduction in inference latency with a negligible mAP drop shifting from SiLU to Hardswish, the FBNetV3-based model still outperformed YOLOv8s-HSwish. Furthermore, we have trained and profiled a ReLU-based version of YOLO-FBNetV3-D-PAN-C3 (with activation functions in the backbone kept to be those of the original backbone, i.e. Hardswish, but neck and detection head activations replaced with ReLU) and observed further latency improvements at the cost of $\sim 0.56$ drop in mAP$_{50-95}$. However, this model was still found to outperform YOLOv8s in terms of both accuracy and latency (see Table \ref{tab:timm_coco_full}).




\begin{table*}
\small
\caption{Pareto-optimal \textit{YOLOBench} models on 3 datasets and 4 hardware platforms. Shown are the best models in terms of mAP$_{50-95}$ under a given latency threshold (max. latency). For each model, the scaling parameters are given (d33w25 means depth factor $=0.33$ and width factor $=0.25$), corresponding input resolution of the models is indicated in brackets.}
\vspace{1mm}
\label{tab:pareto_table_full} 
\begin{tabularx}{\linewidth}{lXXXXXX}
\toprule
{HW/max.} & {VOC} & {VOC} & {SKU-110k} & {SKU-110k} & {WIDERFACE} & {WIDERFACE} \\ %
{latency} & {model} & {mAP$_{50-95}$} & {model} & {mAP$_{50-95}$} & {model} & {mAP$_{50-95}$} \\ %
\midrule
{Nano/0.5 sec} & {YOLOv8} & {0.726} & {YOLOv7} & {0.593} & {YOLOv7} & {0.382}\\
{} & {d67w1 (448)} & {} & {d1w75 (480)} & {} & {d1w75 (480)} & {}\\
{Nano/0.3 sec} & {YOLOv7} & {0.701} & {YOLOv7} & {0.589} & {YOLOv7} & {0.369}\\
{} & {d1w5 (480)} & {} & {d1w5 (480)} & {} & {d1w5 (480)}\\
{Nano/0.1 sec} & {YOLOv7} & {0.657} & {YOLOv8} & {0.567} & {YOLOv7} & {0.336}\\
{} & {d1w5 (288)} & {} & {d1w25 (480)} & {} & {d1w25 (480)}\\
\midrule
{VIM3/0.3 sec} & {YOLOv8} & {0.726} & {YOLOv7} & {0.593} & {YOLOv7} & {0.382}\\
{} & {d67w1 (448)} & {} & {d1w75 (480)} & {} & {d1w75 (480)} & {}\\
{VIM3/0.1 sec} & {YOLOv6l} & {0.669} & {YOLOv8} & {0.567} & {YOLOv6m} & {0.350}\\
{} & {d67w5 (384)} & {} & {d1w25 (480)} & {} & {d33w5 (480)}\\
{VIM3/0.05 sec} & {YOLOv6l} & {0.620} & {YOLOv6s} & {0.556} & {YOLOv6m} & {0.318}\\
{} & {d67w25 (416)} & {} & {d33w25 (480)} & {} & {d67w25 (480)}\\
\midrule
{Intel/0.08 sec} & {YOLOv8} & {0.719} & {YOLOv7} & {0.593} & {YOLOv7} & {0.382}\\
{} & {d1w75 (416)} & {} & {d1w75 (480)} & {} & {d1w75 (480)} & {}\\
{Intel/0.04 sec} & {YOLOv7} & {0.701} & {YOLOv7} & {0.589} & {YOLOv7} & {0.369}\\
{} & {d1w5 (480)} & {} & {d1w5 (480)} & {} & {d1w5 (480)}\\
{Intel/0.02 sec} & {YOLOv6l} & {0.682} & {YOLOv6l} & {0.576} & {YOLOv6l} & {0.346}\\
{} & {d6w5 (448)} & {} & {d33w5 (480)} & {} & {d33w5 (480)}\\
\midrule
{Raspi4/3 sec} & {YOLOv8} & {0.719} & {YOLOv7} & {0.593} & {YOLOv7} & {0.382}\\
{} & {d1w75 (416)} & {} & {d1w75 (480)} & {} & {d1w75 (480)} & {}\\
{Raspi4/1 sec} & {YOLOv7} & {0.701} & {YOLOv7} & {0.589} & {YOLOv7} & {  0.369}\\
{} & {d1w5 (480)} & {} & {d1w5 (480)} & {} & {d1w5 (480)}\\
{Raspi4/0.5 sec} & {YOLOv6l} & {0.669} & {YOLOv4} & {0.569} & {YOLOv7} & {0.336}\\
{} & {d67w5 (384)} & {} & {d1w25 (480)} & {} & {d1w25 (480)}\\
\bottomrule
\end{tabularx}
\end{table*}

\begin{table*}
\small
\caption{Performance of training-free accuracy predictors on \textit{YOLOBench} models and two datasets (VOC and SKU-110k, from COCO-pretrained weights) compared to using mAP$_{50-95}$ of models trained from scratch on the VOC dataset as a predictor.}
\vspace{2mm}
\label{tab:zc_table_full}
\begin{tabularx}{\linewidth}{l|XXX|XXX}
\toprule
{} & \multicolumn{3}{c|}{\text{VOC, mAP$_{50-95}$}} & \multicolumn{3}{c}{\text{SKU-110k, mAP$_{50-95}$}}\\
\midrule
{Predictor metric} & {global $\tau$} & {top-15\% $\tau$} & {\%Pareto pred. (GPU)} & {global $\tau$} & {top-15\% $\tau$} & {\%Pareto pred. (GPU)}\\
\midrule    
{GraSP} & {-0.011} & {-0.068} & {0.062} & {0.040} & {0.032} & {0.025}\\
{Plain} & {0.029} & {0.069} & {0.015} & {-0.388} & {-0.176} & {0.025}\\
{JacobCov} & {0.095} & {-0.078} & {0.015} & {0.541} & {0.136} & {0.025}\\
{ZiCo} & {0.195} & {0.016} & {0.015} & {0.115} & {0.081} & {0.025}\\
{Zen} & {0.255} & {0.092} & {0.062} & {0.146} & {0.121} & {0.050}\\
{GradNorm} & {0.262} & {0.173} & {0.015} & {-0.331} & {-0.072} & {0.025}\\
{Fisher} & {0.280} & {0.156} & {0.015} & {-0.380} & {-0.096} & {0.025}\\
{L2 norm} & {0.326} & {0.090} & {0.015} & {0.189} & {0.118} & {0.025}\\
{SNIP} & {0.336} & {0.217} & {0.015} & {-0.290} & {-0.059} & {0.025}\\
{\#params} & {0.399} & {0.372} & {0.031} & {0.256} & {0.119} & {0.050}\\
{SynFlow} & {0.558} & {0.227} & {0.062} & {0.512} & {0.254} & {0.100}\\
{MACs} & {0.739} & {0.520} & {0.123} & {0.604} & {0.314} & {0.125}\\
{NWOT} & {0.756} & {0.622} & {0.262} & {0.703} & {0.321} & {\bf{0.200}}\\
{NWOT (pre-act)} & {{\bf 0.827}} & {{\bf 0.623}} & {{\bf 0.292}} & {{\bf 0.765}} & {{\bf 0.406}} & {{\bf 0.200}}\\
\midrule
{VOC training} & {0.847} & {0.665} & {0.369} & {0.739} & {0.374} & {0.425}\\
{from scratch (mAP$_{50-95}$)} &  {} & {} & {} & {}\\
\bottomrule
\end{tabularx}
\end{table*}

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed


% Figure environment removed

% Figure environment removed


% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed


% Figure environment removed

% Figure environment removed

% Figure environment removed



\begin{table*}
\centering
\caption{Example YOLO-PAN-C3 models with \texttt{timm} backbones identified in the NWOT-latency Pareto frontier, with pre-activation NWOT score computed on the VOC dataset. Latency values are measured on Raspberry Pi 4 ARM CPU with TFLite (FP32), batch size 1.}
\label{tab:nwot_timm_pareto}
\begin{tabularx}{\textwidth}{l|X|X|X}
\toprule
Model name & Input resolution & Raspi4 CPU latency, sec & NWOT (pre-act) \\
\midrule
\texttt{yolo\_pan\_efficientnet\_b4} & { 480} & { 1.72} & { 511.84 }  \\
\texttt{yolo\_pan\_tf\_efficientnet\_b4\_ap} & { 480} & { 1.71} & { 511.77 }  \\
\texttt{yolo\_pan\_gc\_efficientnetv2\_rw\_t} & { 480} & { 1.41} & { 508.73 }  \\
\texttt{yolo\_pan\_tf\_efficientnet\_lite4} & { 480} & { 1.08} & { 506.67 }  \\
\texttt{yolo\_pan\_fbnetv3\_d} & { 480} & { 0.71} & { 502.71 }  \\
\texttt{yolo\_pan\_tf\_efficientnet\_lite1} & { 480} & { 0.61} & { 493.48 }  \\
\texttt{yolo\_pan\_efficientnet\_lite1} & { 480} & { 0.61} & { 493.32 }  \\
\texttt{yolo\_pan\_mobilenetv2\_110d} & { 480} & { 0.54} & { 480.92 }  \\
\texttt{yolo\_pan\_mobilenetv2\_075} & { 480} & { 0.45} & { 480.14 }  \\
\texttt{yolo\_pan\_tf\_mobilenetv3\_large\_075} & { 480} & { 0.45} & { 468.85 }  \\
\texttt{yolo\_pan\_mobilenetv2\_035} & { 480} & { 0.37} & { 457.41 }  \\
\texttt{yolo\_pan\_tf\_mobilenetv3\_small\_minimal\_100} & { 480} & { 0.36} & { 451.10 }  \\
\bottomrule
\end{tabularx}
\end{table*}


\begin{table*}
  \small
  \begin{center}
    \caption{COCO mAP and inference latency on Raspberry Pi 4 CPU (TFLite with XNNPACK backend, FP32) for YOLOv8s vs. a model identified from the NWOT-latency Pareto frontier (YOLO-FBNetV3-D-PAN). Mean and standard deviation of inference time over 5 runs (each one 100 iterations) are shown. Input image resolution used was 640x640, batch size $=1$ for latency measurements.} %
    \label{tab:timm_coco_full}
    \vspace*{2mm}
    \begin{tabularx}{\linewidth}{X|X|X} %
      \hline
      {Model} & {COCO mAP$^{val}_{50-95}$} & {Raspberry Pi 4 ARM CPU latency, ms}\\
      \hline
      {YOLOv8s} & {0.4364} & {1476.09 $\pm$ 1.49} \\
      \hline
      {YOLOv8s-HSwish} & {0.4355} & {1381.62 $\pm$ 7.34} \\
      \hline
      {YOLO-FBNetV3-D-PAN} & {0.4463} & {1355.21 $\pm$ 9.93} \\
      \hline
      {YOLO-FBNetV3-D-PAN-ReLU} & {0.4407} & {1344.50 $\pm$ 8.06} \\
      \hline
    \end{tabularx}
  \end{center}
\end{table*}