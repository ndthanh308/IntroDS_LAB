\section{Results}
\label{sec:results}

\subsection{Pareto-optimal YOLO models}

By computing the proxy metric for model accuracy (mAP$_{50-95}$ in VOC training from scratch) and latency values for the whole \textit{YOLOBench} architecture space on several hardware platforms, we have determined the Pareto sets containing the most promising models (in terms of latency-accuracy trade-off) for each HW platform. Merging the first and second Pareto sets for each device into a single list of best architectures, we arrived at a set of 52 backbone/neck combinations for COCO pre-training (same architectures with different input resolutions are considered as the same data points here, since COCO training is regardless done on a fixed resolution of 640x640). After performing the COCO training for these selected models, we fine-tune these models at 11 different resolutions (from 160x160 to 480x480 with a step of 32) on all downstream datasets (except for COCO), resulting in 572 models total for each dataset.

% Figure environment removed


Finally, given fine-tuned model accuracy on several datasets and latency measurements on several devices, we compute the actual Pareto sets for each particular dataset/HW platform combination. Figure \ref{fig:voc_pareto} shows the Pareto frontiers of \textit{YOLOBench} models fine-tuned on the VOC dataset on 4 different devices. One could observe significant differences in the Pareto frontiers between devices. In particular, the Pareto-optimal set for VIM3 NPU is mostly comprised of YOLOv6 models, with some YOLOv5, YOLOv7, and YOLOv8 models present in the higher accuracy region. This is not the case for Pareto sets of Intel and ARM CPUs that, despite containing a few YOLOv6 models in the lower latency region, also contain many YOLOv5 and YOLOv7 variations in the higher accuracy region (with some other models families like YOLOv3 and YOLOv4 also present in a limited capacity). While the Pareto sets for Intel and ARM CPUs are found to be similar to each other, the Pareto set on Jetson Nano GPU is different from the rest of the devices and is very non-uniform in terms of model family distribution, with YOLOv5, YOLOv6, YOLOv7 and YOLOv8 models all represented across the whole accuracy/latency range. Table \ref{tab:pareto_table} shows representative Pareto-optimal models for 3 different datasets (VOC, SKU-110k, WIDERFACE) and 3 hardware platforms under certain latency thresholds. Note that although there are similarities of model family distributions in Pareto sets computed for different datasets (see Appendix B), the exact optimal model for a given latency threshold depends on the specific dataset of interest.

Next, we looked at statistics of Pareto-optimal models depending on the dataset and hardware platform. Figure \ref{fig:pareto_scaling} shows the distribution of depth factor, width factor, and input resolution values in Pareto frontier models for VOC and SKU-110k datasets on Jetson Nano GPU (data for other datasets and devices available in Appendix B). The general trend observed is that models at lower input resolutions mostly have lower depth and width factors. This means that to achieve optimal results in terms of latency-accuracy trade-off, one has to scale down the width and depth of the architecture before lowering the input resolution. This effect is more pronounced in some datasets (SKU-110k and WIDERFACE), where almost all optimal models are either at the maximal resolution we considered (480x480) with variation in width and depth, or at lower resolutions with minimal width and depth factors. This effect is dataset-dependent, as it is observed to be more relaxed for VOC and COCO datasets, where many optimal models with a variation in width and depth factor are found at resolutions lower than 480x480.

To summarize, we have demonstrated that with a state-of-the-art training pipeline and detection head structure, YOLO-based models with various backbone/neck combinations could achieve good latency-accuracy trade-off in various deployment scenarios, including older backbone/neck structures from YOLOv4 and YOLOv3 models. Furthermore, we have shown that depth/width reduction precedes input resolution down-scaling in optimal YOLO-based detectors.

\subsection{Ranking training-free accuracy predictors}
\label{sec:ranking_zc}


With an increasing number of architecture blocks and hyperparameter combinations, the size of the candidate model space in \textit{YOLOBench} can further grow exponentially. Hence, it is important to develop efficient methods of filtering out bad architecture proposals before running them through the full training pipeline, including pre-training on the COCO dataset. In the field of neural architecture search, recent works have proposed a handful of training-free, {\em zero-cost} (ZC) estimators that have been shown to perform well on various (relatively simple) benchmarks \cite{mellor2021neural, abdelfattah2021zero, li2023zico}.

Zero-cost estimators were originally proposed by Mellor et al. \cite{mellor2021neural}, and later expanded by Abdelfattah et al. \cite{abdelfattah2021zero} as a means to quickly evaluate the performance of an architecture using only a mini-batch of data. These estimators work by extracting statistics obtained from a forward (and/or backward) pass of a few mini-batches of data through the network, hence eliminating the need for full training of the model. Despite the fact that over 20 different zero-cost accuracy estimators have been introduced in recent years, simple baselines like the number of parameters and MAC count are still found to be hard to outperform \cite{li2023zico}.

The vast search space of YOLO-like architectures necessitates the development of effective training-free estimators to filter out bad candidates and reduce the search space. We have examined the performance of a representative subset of zero-cost estimators on \textit{YOLOBench}, namely: 
Fisher~\cite{turner2019blockswap}, GradNorm~\cite{abdelfattah2021zero}, GraSP~\cite{wang2020picking}, JacobCov~\cite{abdelfattah2021zero}, Plain~\cite{abdelfattah2021zero}, SNIP~\cite{lee2018snip}, SynFlow~\cite{tanaka2020pruning}, ZiCo~\cite{li2023zico}, Zen-score~\cite{lin2021zen} and NWOT~\cite{mellor2021neural}. The NWOT metric is computed by measuring the Hamming distance between binary codes produced by each layer's activations \cite{mellor2021neural}. Although originally proposed for ReLU-based networks, we found that it works well in practice for YOLO variations, most of which contain SiLU activations. The NWOT metric can be also computed by looking at signs of each layer's output features before the activation layer to form the binary code. We refer to that version of the NWOT metric as  \textit{NWOT (pre-act)} ("pre-activation"), and find that its performance might differ significantly from the original NWOT metric, primarily because the binary codes are computed before the normalization layers followed by the activations. We also compare the performance of the above metrics with simple baselines such as the number of trainable parameters and MAC count, as well as with a training-based proxy that we have used for pre-select models for \textit{YOLOBench} (mAP$_{50-95}$ of training from scratch on the VOC dataset). 

All zero-cost metrics are computed on randomly initialized models using the same loss function as used for training of all \textit{YOLOBench} models, using a single mini-batch of data with a corresponding image resolution (except for ZiCo, which requires two different mini-batches of data). We empirically evaluate the considered set of zero-cost proxies on \textit{YOLOBench} using the following metrics:

\begin{itemize}
    \item Kendall $\tau$ (global): Kendall rank correlation coefficient evaluated on all \textit{YOLOBench} models
    \item Kendall $\tau$ (top-15\%): Kendall rank correlation coefficient evaluated on the top-15\% performing \textit{YOLOBench} models (in terms of mAP$_{50-95}$ value)
    \item Percentage of all actual Pareto-optimal models in the Pareto set determined with the zero-cost estimator in the zero-cost proxy-latency space (recall for Pareto-optimal model prediction using the ZC-based Pareto set)
\end{itemize}

The last metric effectively measures how accurate the computed Pareto set would be if the proxy values are used instead of actual mAP to rank models. It is calculated by determining Pareto fronts for model rankings based on zero-cost proxies (and real latency measurements) and looking at how many models present in the actual Pareto set are also present in the ZC-based Pareto set. In other words, a recall value of 0.7 would mean that by taking the models from the ZC-based Pareto set as candidates, we find 70\% of all actual Pareto-optimal models in that candidate set. We report values for Pareto fronts computed with latency measurements on the Jetson Nano GPU in Table \ref{tab:zc_table}.

We generally found that all of the zero-cost predictors we considered (except for NWOT) were outperformed by the simple baseline of MAC count both in terms of Kendall-Tau scores as well as in the percentage of predicted Pareto-optimal models (see Table \ref{tab:zc_table}). Furthermore, when compared with using mAP$_{50-95}$ on VOC training from scratch as a predictor, we observed that only NWOT came close to it in terms of ranking scores. We have also found that a pre-activation version of NWOT tends to work better than standard NWOT on \textit{YOLOBench}. For the task of predicting mAP$_{50-95}$ of models fine-tuned on SKU-110k, we notably found that pre-activation NWOT outperforms VOC training from scratch metric in terms of Kendall-Tau scores (possibly due to domain difference between VOC and SKU-110k datasets), but the VOC-based proxy metric still performs better for Pareto-optimal model prediction on SKU-110k.

\begin{table}
  \small
    \caption{COCO mAP and inference latency on Raspberry Pi 4 CPU (TFLite, FP32) for YOLOv8s vs. a model identified from the NWOT-latency Pareto frontier. Mean and standard deviation of inference time over 5 runs (each run done for 100 iterations) shown, with 640x640 input resolution.} %
    \label{tab:timm_coco}
    \vspace*{2mm}
    \begin{tabularx}{\columnwidth}{X|X|X} %
      \hline
      {Model} & {COCO mAP$^{val}_{50-95}$} & {Latency, ms}\\
      \hline
      {YOLOv8s} & {0.4364} & {1476.09 $\pm$ 1.49} \\
      \hline
      {YOLOv8s (HSwish)} & {0.4355} & {1381.62 $\pm$ 7.34} \\
      \hline
      {YOLO-FBNetV3-D-PAN-C3} & {0.4463} & {1355.21 $\pm$ 9.93} \\
      \hline
    \end{tabularx}
\end{table}


In trying to capture all the real Pareto-optimal models using ZC scores, one could increase the size of the ZC-based candidate pool by computing the second (third, fourth) ZC-based Pareto set and adding it to the pool of ZC-based candidates. Such a procedure naturally increases the percentage of actual Pareto-optimal models in that pool, and the full set of actual Pareto-optimal models could be found this way by looking only at a portion of the full dataset (e.g. at first $N$ ZC-based Pareto fronts). In this context, we compute candidate pools consisting of $N$ ZC Pareto fronts for each ZC metric and look at the percentage of actual Pareto-optimal models found in the pool versus the pool size (as \% of the full dataset size). Looking at the pool size is motivated by the observation that the number of models in ZC-based Pareto fronts can significantly vary depending on the ZC metric.

Figure \ref{fig:zc_pareto_voc} shows the percentage of predicted real Pareto-optimal models on the VOC dataset contained in pools of $N$ first Pareto fronts for 4 different predictors (VOC training from scratch, NWOT, pre-activation NWOT and MAC count). For ARM and Intel CPUs, we observe a general trend of VOC training from scratch being the best predictor and MAC count being the worst at all points. Interestingly, for Jetson Nano GPU NWOT performs close to VOC training from scratch for $N = 1,2$ but starts to perform worse with more models in the pool. Surprisingly, the training-free predictors MAC count and pre-activation NWOT outperform VOC training from scratch in predicting Pareto-optimal models on VIM3 NPU.

\subsection{Pareto-optimal detector identification using NWOT score}

To demonstrate the potential of using ZC-based Pareto sets in identifying promising detector architectures with good accuracy-latency trade-off, we have additionally generated multiple candidate architectures based on CNN backbones provided by the \texttt{timm} library \cite{rw2019timm}. The architectures were generated by using one of the 347 CNN-based backbones available in \texttt{timm} as a feature extractor followed by a modified Path Aggregation Network (PAN) (same structure with C3 blocks as in YOLOv5 was used, with the number of channels corresponding to YOLOv5s, without the SPPF block) and a YOLOv8 detection head, as in all other \textit{YOLOBench} models. 

We computed the pre-activation NWOT scores as well as measured inference latency on Raspberry Pi 4 ARM CPU with TFLite for all candidate models. We then used the NWOT score and latency values for each model to compute the Pareto frontier in the NWOT-latency space (see Appendix D). We then trained one of the models identified to belong to the NWOT-based Pareto frontier (YOLO with FBNetV3-D backbone) on the COCO dataset with the same setup used to pre-train \textit{YOLOBench} models (640x640 input resolution, 300 epochs, batch size 64, other hyperparameters set to default of Ultralytics YOLOv8 \cite{ultralytics})\footnote{Note that YOLOv8s results provided by Ultralytics \cite{ultralytics} are higher than the ones we have reported, as the models were trained for 500+ epochs on COCO. However, no script to reproduce those results has been released to date.}. The resulting model was found to be more accurate and faster than YOLOv8s (a model in a similar latency range) when tested on Raspberry Pi 4 CPU with TFLite (FP32, XNNPACK backend) (see Table \ref{tab:timm_coco}). Furthermore, we have looked at the accuracy and latency of a YOLOv8s modification with SiLU activations replaced with Hardswish activations (Table \ref{tab:timm_coco}), as we have observed the choice of activation function to be a significant factor affecting TFLite inference latency. We found that the identified NWOT-Pareto model (also containing Hardswish activations in the backbone, neck and head) still outperformed YOLOv8s-HSwish in terms of latency and accuracy.
