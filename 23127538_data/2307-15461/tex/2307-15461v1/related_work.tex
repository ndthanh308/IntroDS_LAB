\section{Related work} \label{sec:sec2}

\subsection{Deblurring and Super-Resolution}

Recent image deblurring solutions are based on convolutional neural networks and employ residual learning, multiscale training, GAN-based models or a combination of these. Quan et al. \cite{Quan2021} propose a non-blind deblurring network based on a scale-recurrent attention module. In \cite{Wang2022} and \cite{Basty2018}, the authors use multiscale U-net architectures with residual learning, for recovery from defocus blur and image super-resolution tasks.These papers rely on local or global residual connections, which are useful for recovering information that may be lost through down sampling, as well as for optimizing the training process for the models \cite{Mao2016}. The authors of \cite{Lee2019} propose a defocus map estimation model and use the predicted maps for enhancing blur in images and for deblurring. For blurring, the defocus map is used to compute the pixel-wise blur level and magnify it by a given factor. For deblurring, they use the defocus map to estimate a per-pixel Gaussian kernel and perform a hyper-Laplacian deconvolution. They do not quantify the performance of blurring and deblurring images using the estimated defocus maps, so it is not clear how well this method works and whether it is suitable for microscopic images as well.

Jiang et al. \cite{Jiang2020} and Zhang et al. \cite{Zhang2021} tackle multi-cause blur and propose deep-learning models that can recover sharp images from either motion or defocus blur. Both solutions have the disadvantage of being too slow for real-time usage, predicting the sharp image in 1.5 and 1.67 seconds, respectively.

A different approach is to process the image at a patch-level, to classify images based on blur quality \cite{Senaras2018, Yang2018} or to remove blur from images \cite{Liang2021, He2019}. This technique is based on the idea that the blur level is invariant throughout the image and that it is well-defined only in the foreground, therefore, a patch-level approach having a more powerful blur-level assessment and blur removal quality. In \cite{Lee2021}, a new type of filter is proposed to handle spatially-varying and large defocus blur. The pixel-wise filter approach handles spatially variant blur, while stacks of small, separable filters aid recovery from large defocus levels. Despite showing very good results, their model is sensible to bright patches which are usually present in microscopy images.

\subsection{Regularized autoencoders}

Vanilla Autoencoders are prone to model a latent space which does not reflect the real distribution of the data and cannot capture the relationship between data points \cite{Garnelo2019}. Several solutions based on adversarial training of plain autoencoders have emerged. Oring et al. \cite{Oring2021} propose a regularized version of an autoencoder, based on several loss terms used to impose smoothness and convexity on the latent space distribution. Similarly to our method, they perform interpolation between latent representations of the same object, captured at different rotation angles. Hence, they generate a smooth transition from one position to another by traversing the latent space between representations of the same object at different rotation angles. A shortcoming of their study is that they only test the proposed method on a simple dataset of rotating objects, so it is unknown whether the approach performs well on more complex data distributions. In \cite{Sainburg2018}, an adversarial netowork used autoencoders as the generator and discriminator. The network is trained for image reconstruction based on a pixel-wise similarity loss.  

