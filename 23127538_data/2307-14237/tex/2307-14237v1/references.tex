\documentclass{article}


\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}


% \usepackage{amsmath,amssymb,amsfonts}
% \usepackage{algorithmicx}
% \usepackage{algpseudocode}% http://ctan.org/pkg/algorithmicx
% \usepackage[]{algorithm2e}
% \usepackage{siunitx}
% \usepackage{gensymb}
% \usepackage{graphicx}

\usepackage{balance} % for balancing columns on the final page
\usepackage{amsmath}
% \usepackage{lipsum}  
%\usepackage{algorithm}
%\usepackage{algorithmic}
\usepackage{mathrsfs}
\usepackage{bm}
\usepackage{booktabs}
\usepackage{amsfonts,amssymb}
\usepackage{colortbl}
\usepackage{color}
\usepackage{tikz}
\usepackage{multicol}
\usepackage{threeparttable}
\usepackage{makecell}
\usepackage{multirow}


\usepackage{algorithmicx}
\usepackage{algpseudocode}% http://ctan.org/pkg/algorithmicx
\usepackage[]{algorithm2e}
\usepackage{siunitx}
\usepackage{gensymb}

\usepackage{graphicx}

\usepackage{upgreek}
\usepackage{amsmath,amssymb,amsfonts}

\usepackage{amsmath}

\title{Evolving Multi-Objective Neural Network Controllers for Robot Swarms}


\author{
  Karl Mason\\
  %\thanks{Use footnote for providing further information about author (webpage, alternative address)---\emph{not} for acknowledging funding agencies.} \\
  School of Computer Science\\ 
  University of Galway\\
  Galway, Ireland\\
  \texttt{karl.mason@universityofgalway.ie} \\
  %% examples of more authors
   \And
 Sabine Hauert \\
  Bristol Robotics Laboratory\\ 
  University of Bristol\\ 
  Bristol, UK\\
  \texttt{sabine.hauert@bristol.ac.uk} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}



\begin{document}
\maketitle 

\begin{abstract}
Many swarm robotics tasks consist of multiple conflicting objectives. This research proposes a multi-objective evolutionary neural network approach to developing controllers for swarms of robots. The swarm robot controllers are trained in a low-fidelity Python simulator and then tested in a high-fidelity simulated environment using Webots. Simulations are then conducted to test the scalability of the evolved multi-objective robot controllers to environments with a larger number of robots. The results presented demonstrate that the proposed approach can effectively control each of the robots. The robot swarm exhibits different behaviours as the weighting for each objective is adjusted. The results also confirm that multi-objective neural network controllers evolved in a low-fidelity simulator can be transferred to high-fidelity simulated environments and that the controllers can scale to environments with a larger number of robots without further retraining needed.
\end{abstract}


% keywords can be removed


\keywords{Swarm Robotics \and Evolutionary Robotics \and Neural Networks \and Natural Evolution Strategies \and Evolutionary Algorithms \and Multi-Objective}


% footnote
% Proc. of the Autonomous Robots and Multirobot Systems (ARMS) 2023, Basilico et al. (eds.), May 9-10, 2023, https:// u.cs.biu.ac.il/ ~agmon/ arms2023/ . 2023.

\section{Introduction}\label{sec:Intro}

%what is the problem?
\let\thefootnote\relax\footnotetext{\textit{Proc. of the Autonomous Robots and Multirobot Systems (ARMS) 2023, Basilico et al. (eds.), May 9-10, 2023, \url{https://u.cs.biu.ac.il/~agmon/arms2023/}. 2023.}}


Many robotics tasks consist of multiple objectives. For example, manufacturing robots must accomplish tasks quickly and accurately \cite{chen2003multi}. These are conflicting objectives. Similarly, multiple objectives are also present in swarm robotics tasks. Minimizing both movement time and energy costs is a primary example of this \cite{mai2019multi}.

Current approaches to addressing these multi-objective robotic tasks often involve applying a multi-objective optimisation algorithm to optimise the actions of the robot for different objectives, e.g. for path planning \cite{nazarahari2019multi,sathiya2019evolutionary,wang2018car}. These approaches work well for determining the optimum paths under multiple objectives for a specific environment configuration. If the environment changes, e.g. if more robots are added to the environment, these optimisation based techniques must be reapplied to determine the new set of optimal paths under the new conditions. This requires additional computational time.

An alternative solution to this multi-objective robotics problem is presented in this paper using Neuroevolution, or evolutionary neural networks \cite{stanley2019designing}. Neuroevolution involves applying evolutionary algorithms to train the parameters of neural networks to solve a machine learning task, e.g. play Atari games \cite{hausknecht2014neuroevolution}, energy forecasting \cite{mason2018forecasting}, CPU utilization prediction \cite{mason2018predicting} and economic dispatch \cite{mason2018multi}. A multi-objective evolutionary neural network approach is proposed for developing swarm robot controllers. This proposed approach to developing multi-objective swarm robot controllers has the benefit of evolving a control policy that can still effectively control the robots even if the environmental conditions change and will not require the computational cost associated with retraining the robot controller.




The research presented in this paper makes the following contributions:  


\begin{enumerate}
    \item To propose a Natural Evolution Strategies based evolutionary multi-object neural network approach for swarm robotics.
    
    \item To investigate the transferability of multi-objective neural network controllers trained in a low-fidelity simulator to a high-fidelity simulator.
    
    \item To determine if controllers trained in an environment with a small number of robots, can scale to larger robot swarms.
    
\end{enumerate}




The structure of the paper is as follows. Sections 2 will give an overview of the background literature relating to this research. Section 3 will outline the experimental methods, including simulator design, application of proposed approach and experiments conducted. Section 4 will present the results of the simulations conducted. Finally, Section 6 will present a conclusion for the research described in the paper.


\section{Background}

\subsection{Swarm Robotics}

Robot swarms typically have characteristics including: acting autonomously within their environment, homogeneity across robots, and utilizing local information only \cite{csahin2004swarm}. Robot swarms have many practical applications \cite{schranz2020swarm}, e.g. search and rescue \cite{bakhshipour2017swarm} and warehouse operations \cite{liu2017novel}.

There have been a number of publications that have explored the application of machine learning to robot swarms, e.g. in 2020 Tolstaya et al. applied a graph neural network to learn to control robots in a swarm \cite{tolstaya2020learning}. A 2021 paper by Dorigo et al. mentions the application of machine learning to robot swarms as one of the future directions of research in swarm robotics \cite{dorigo2021swarm}. 



Research published recently in the literature has recognised that the task of robot path planning often consists of multiple objectives \cite{ma2018multi}, e.g. to minimize the time taken to locate a target, to minimise chance of collisions, to maximize energy efficiency, etc. Each of these objectives are important and should be considered when determining the behaviour of the robot. This motivates the research presented in this paper to develop robot controllers that can take preferences for each objective as input to influence the behaviour of robot swarms.

Multiple studies have been published in the literature that apply evolutionary methods to swarm robotics. Birattari et al. provide a manifesto for automatic off-line design in the area of robot swarms \cite{birattari2019automatic}. This paper discusses existing applications of evolutionary neural networks to robot controllers. Floreano et al. provide a comprehensive account of applications of evolutionary computing to robotics \cite{Floreano2008}. Evolutionary methods have been successfully applied in swarm robotics tasks. Hauert et al. applied evolutionary neural networks to control individuals in a swarm of simulated Micro Air Vehicles (MAVs) \cite{hauert2009evolved}. Evolutionary methods have also been applied to swarm robotics by evolving behaviour trees \cite{jones2019onboard,jones2018evolving}. Recent studies have applied MAP-Elites for swarm robotics \cite{kaiser2022minimize}. These studies demonstrate the effectiveness of evolutionary methods for swarm robotics tasks. The research outlined in the paper builds on these previous studies by extending evolutionary swarm robot controllers to multi-objective tasks using Natural Evolution Strategies.


There have been a number of studies published in the literature that explore multi-objective control in robot swarms. Mai et al. developed a Multi-Objective collective search strategy for robot swarms based on the Particle Swarm Optimisation algorithm \cite{mai2019multi}. Miao et al. proposed a Multi-objective region reaching controller for a swarm of robots \cite{miao2019multi}. These studies demonstrate the utility of multi-objective control and path planning for robot swarms. These studies do not consider the use of multi-objective neural network controllers for robot swarm control, as this research paper presents.


% **maybe more here***

\subsection{Evolutionary Neural Networks}

Neural networks are machine learning models that take inspiration from the brain. The field of Evolutionary Neural Networks (or Neuroevolution) utilize evolutionary algorithms and principals to train the parameters of neural networks \cite{galvan2021neuroevolution}. Target network outputs are not required when evolving neural networks, only a fitness function. Neuroevolution has been shown to be a competitive approach when compared to reinforcement learning algorithms \cite{salimans2017evolution,mason2021building}. 
% Evolutionary neural networks have been applied to a wide range of problems, e.g. energy forecasting \cite{mason2018forecasting}, CPU utilization prediction \cite{mason2018predicting} and economic dispatch \cite{mason2018multi}.

\begin{algorithm}[h]
\caption{xNES Algorithm}
\begin {algorithmic}
\State \textbf{Initialize algorithm}\\
\While{curGen < maxGen}{\\
    \For{i in $\lambda$}{
            \State Sample $z_i \leftarrow \mathcal{N}(0,1)$ 
            \State $x_i \leftarrow \mu + \sigma \textbf{B} z_i$}
    \State calculate utilities $u$ by sorting $\{(z_i, x_i)\}$ w.r.t $f(x_i)$
    \State $G_\delta \leftarrow \Sigma_{i=1}^{\lambda} u_i \cdot z_i$
    \State $G_M \leftarrow \Sigma_{i=1}^{\lambda} u_i \cdot (z_i z_{i}^T - \textbf{I})$
    \State $G_\sigma \leftarrow \frac{trace(G_M)}{d}$
    \State $G_B \leftarrow G_M -G_\sigma \cdot \textbf{I}$
    \State $\mu \leftarrow \mu + \eta_\mu \cdot \sigma \textbf{B} \cdot G_\delta$
    \State $\sigma \leftarrow \sigma \cdot exp (0.5 \eta_\sigma \cdot  G_\sigma)$
    \State $\textbf{B} \leftarrow \textbf{B} \cdot exp (0.5 \eta_\textbf{B} \cdot  G_\textbf{B})$
    \State $curGen++$}
\end{algorithmic}
\label{Alg:xNES}
\end{algorithm}

There are many Neuroevolution algorithms that evolve the weights and architecture of the neural network, e.g. NeuroEvolution of Augmenting Topologies (NEAT) \cite{stanley2002evolving} and hyperNEAT \cite{d2014hyperneat}. Many studies implement methods such as evolutionary strategies to evolve only the weights of the network \cite{chen2019restart,pourchot2018importance,mason2021building}. Covariance Matrix Adaption Evolutionary Strategy (CMA-ES) \cite{hansen2003reducing} and Natural Evolution Strategies (NES) \cite{wierstra2014natural} are two well known evolutionary strategies. This research will use a variant of NES called Exponential Natural Evolution Strategies (xNES) \cite{glasmachers2010exponential} when evolving MO-NNs. It was selected as it is an effective optimisation algorithm.

%\subsection{Exponential Natural Evolution Strategies}
Algorithm \ref{Alg:xNES} describes the xNES algorithm. The algorithm samples $\lambda$ normally distributed solutions $z_i$. These are used to calculate $\lambda$ solutions $x_i = \mu + \sigma \textbf{B} z_i$, based on the center of the search distribution $\mu$, the normalized covariance factor $\textbf{B} = \textbf{A}/\sigma$, and $\sigma$ is the scalar step size. When the algorithm begins, $\textbf{A}$ is initialized as the identity matrix and $\sqrt[d]{|det(\textbf{A})|}$, where $d$ is the number of dimensions.

% \begin{algorithm}[H]
% \caption{xNES Algorithm}
% \begin {algorithmic}
% \State \textbf{Initialize algorithm}\\
% \While{curGen < maxGen}{\\
%     \For{i in $\lambda$}{
%             \State Sample $z_i \leftarrow \mathcal{N}(0,1)$ 
%             \State $x_i \leftarrow \mu + \sigma \textbf{B} z_i$}
%     \State calculate utilities $u$ by sorting $\{(z_i, x_i)\}$ w.r.t $f(x_i)$
%     \State $G_\delta \leftarrow \Sigma_{i=1}^{\lambda} u_i \cdot z_i$
%     \State $G_M \leftarrow \Sigma_{i=1}^{\lambda} u_i \cdot (z_i z_{i}^T - \textbf{I})$
%     \State $G_\sigma \leftarrow \frac{trace(G_M)}{d}$
%     \State $G_B \leftarrow G_M -G_\sigma \cdot \textbf{I}$
%     \State $\mu \leftarrow \mu + \eta_\mu \cdot \sigma \textbf{B} \cdot G_\delta$
%     \State $\sigma \leftarrow \sigma \cdot exp (0.5 \eta_\sigma \cdot  G_\sigma)$
%     \State $\textbf{B} \leftarrow \textbf{B} \cdot exp (0.5 \eta_\textbf{B} \cdot  G_\textbf{B})$
%     \State $curGen++$}
% \end{algorithmic}
% \label{Alg:xNES}
% \end{algorithm}



% Algorithm \ref{Alg:xNES} describes the xNES algorithm. 


% The algorithm samples $\lambda$ normally distribution solutions $z_i$. These are used to calculate $\lambda$ solutions $x_i = \mu + \sigma \textbf{B} z_i$, based on the center of the search distribution $\mu$, the normalized covariance factor $\textbf{B} = \textbf{A}/\sigma$, and $\sigma$ is the scalar step size. When the algorithm begins, $\textbf{A}$ is initialized as the identity matrix and $\sqrt[d]{|det(\textbf{A})|}$, where $d$ is the number of dimensions.

% \begin{algorithm}[t]
% \caption{xNES Algorithm}
% \begin {algorithmic}
% \State \textbf{Initialize algorithm}\\
% \While{curGen < maxGen}{\\
%     \For{i in $\lambda$}{
%             \State Sample $z_i \leftarrow \mathcal{N}(0,1)$ 
%             \State $x_i \leftarrow \mu + \sigma \textbf{B} z_i$}
%     \State calculate utilities $u$ by sorting $\{(z_i, x_i)\}$ w.r.t $f(x_i)$
%     \State $G_\delta \leftarrow \Sigma_{i=1}^{\lambda} u_i \cdot z_i$
%     \State $G_M \leftarrow \Sigma_{i=1}^{\lambda} u_i \cdot (z_i z_{i}^T - \textbf{I})$
%     \State $G_\sigma \leftarrow \frac{trace(G_M)}{d}$
%     \State $G_B \leftarrow G_M -G_\sigma \cdot \textbf{I}$
%     \State $\mu \leftarrow \mu + \eta_\mu \cdot \sigma \textbf{B} \cdot G_\delta$
%     \State $\sigma \leftarrow \sigma \cdot exp (0.5 \eta_\sigma \cdot  G_\sigma)$
%     \State $\textbf{B} \leftarrow \textbf{B} \cdot exp (0.5 \eta_\textbf{B} \cdot  G_\textbf{B})$
%     \State $curGen++$}
% \end{algorithmic}
% \label{Alg:xNES}
% \end{algorithm}



After solutions are sampled, the gradients of the objective function are calculated with respect to $\delta, M, \sigma$ and $\textbf{B}$. Where M is a $d \times d$ exponential map used to represent the covariance matrix $\textbf{C}$, where $\textbf{C = AA}^{T}$ and $\delta$ is the change in $\textbf{C}$. 





\section{Experimental Methods}
\label{sec:Exp}

\subsection{Evolving Multi-Objective Neural Networks}
This research consists of evolving neural networks for a multi-objective swarm robotics task. The pseudocode in Algorithm \ref{Alg:MO_NN} illustrates the MO-NN training process.

\begin{algorithm}[h]
\caption{Evolving MO-NNs using xNES}
\begin {algorithmic}
\State \textbf{Initialize problem domain}
\State \textbf{Initialize algorithm}\\
\While{curGen < maxGen}{\\
    \State Sample $\lambda$ solutions $x$ (Alg \ref{Alg:xNES})\\
    \For{i in $\lambda$}{
        \State Set parameters of NNs as $x$
        \State fitness of NN $= networkFit_i = 0$\\
        \For{$\Updelta w=0$ to $1$}{  
            \State $w_1 = 1 - \Updelta w$
            \State $w_2 = \Updelta w$
            \State Reset environment\\
            \For{timeStep $t=0$ to $maxTime$}{\\
                \For{robot $r=1$ to $numRobots$}{
                    \State Robot $r$ observes state $s_{r,t}$ of the environment, i.e. sensor readings
                    \State Pass $w_1$, $w_2$ and $s_{r,t}$ into NN and record NN output $o$
                    \State Act in environment from NN output $o$, i.e. rotate and move
                    \State $currentFit =$ fitness of action using objective function (Equation \ref{eqn:curFit})
                    \State $networkFit_i += currentFit$
                    }
            }
        }    
    }
    \State calculate utilities $u$ by sorting $\{(z_i, x_i)\}$ w.r.t $networkFit_i$
    \State update gradients $G$, $\mu$, $\sigma$, and $\textbf{B}$ (Alg \ref{Alg:xNES})
    \State $curGen++$
    }
\end{algorithmic}
\label{Alg:MO_NN}
\end{algorithm}

 In Algorithm \ref{Alg:MO_NN}, $w_i$ is the weighting of objective $i$ and $\Updelta w$ is the change in objective weight. Each network must be capable of producing different outputs for different objective weightings given as input to the network. Mason et al. proposed a Multi-Objective Neural Network training architecture in 2018, that will be utilized in this research \cite{mason2018multi}. The distinction should be noted between evolving multi-objective neural networks and applying multi-objective evolutionary algorithms to train neural networks. The former consists of evolving a \textit{single set of network parameters} which enables the network to provide different outputs as the problem objective weighting varies, while the latter consists of utilizing a MO-EA, e.g. NSGA-II \cite{deb2002fast}, to evolve \textit{multiple sets of network parameters} for different objective weightings. This research evolves a single set of MO-NN parameters that are assigned the NN used to control each robot. The implementation of the MO-NN as a robot controller will be discussed in Section \ref{sec:EvoMONN_Robot}.

% \begin{algorithm}[H]
% \caption{Evolving MO-NNs using xNES}
% \begin {algorithmic}
% \State \textbf{Initialize problem domain}
% \State \textbf{Initialize algorithm}\\
% \While{curGen < maxGen}{\\
%     \State Sample $\lambda$ solutions $x$ (Alg \ref{Alg:xNES})\\
%     \For{i in $\lambda$}{
%         \State Set parameters of NNs as $x$
%         \State fitness of NN $= networkFit_i = 0$\\
%         \For{$\Updelta w=0$ to $1$}{
%             \State $w_1 = 1 - \Updelta w$
%             \State $w_2 = \Updelta w$
%             \State Reset environment\\
%             \For{timeStep $t=0$ to $maxTime$}{\\
%                 \For{robot $r=1$ to $numRobots$}{
%                     \State Robot $r$ observes state $s_{r,t}$ of the environment, i.e. sensor readings
%                     \State Pass $w_1$, $w_2$ and $s_{r,t}$ into NN and record NN output $o$
%                     \State Act in environment from NN output $o$, i.e. rotate and move
%                     \State $currentFit =$ fitness of action using objective function (Equation \ref{eqn:curFit})
%                     \State $networkFit_i += currentFit$
%                     }
%             }
%         }    
%     }
%     \State calculate utilities $u$ by sorting $\{(z_i, x_i)\}$ w.r.t $networkFit_i$
%     \State update gradients $G$, $\mu$, $\sigma$, and $\textbf{B}$ (Alg \ref{Alg:xNES})
%     \State $curGen++$
%     }
% \end{algorithmic}
% \label{Alg:MO_NN}
% \end{algorithm}


\subsection{Simulator Design}
\label{sec:simDesign}

% % Figure environment removed

In order to train the proposed multi-objective neural network (MO-NN) controllers, a low-fidelity swarm robot simulator was developed in Python. This was necessary as evolving neural networks for control tasks requires many trial evaluations. This would be too computationally expensive for high-fidelity simulators.

% Figure environment removed


The low-fidelity simulator consisted of a $3.7m \times 3.7m$ arena with boundaries at the edge of the arena. Mobile robots were deployed within the arena. The simulated robots had a diameter of $0.25m$. Each robot had 4 rangefinder sensors, located at the front, back and on each side. Each robot can rotate $\pm 45\deg$ and can only move forward up to a maximum velocity of $2m/sec$. This simulator design is based on the DOTS swarm robot testbed \cite{jones2022dots}. 

A high-fidelity simulator was also developed using Webots \cite{Webots,Webots04}. This was used to test the evolved controllers. The parameters of the Webots simulator are the same as the Python simulator. Figure \ref{fig:simEnv} illustrates the simulated robot and arena.

Both simulators model non-holonomic robot drive. The low-fidelity simulator does not simulate any physics. Collision detection is implemented in the low-fidelity simulator to prevent a robot from colliding with another robot or the environment boundary. Physics is simulated in the high-fidelity Webots simulator. Collisions are not detected/prevented in the high-fidelity simulator.


% % Figure environment removed




% \subsection{Evolving Multi-Objective Neural Networks}
% This research consists of evolving neural networks for a multi-objective swarm robotics task. The pseudocode in Algorithm \ref{Alg:MO_NN} illustrates the MO-NN training process. 

% Each network must be capable of producing different outputs for different objective weightings given as input to the network. Mason et al. proposed a Multi-Objective Neural Network training architecture in 2018, that will be utilized in this research \cite{mason2018multi}. The distinction should be noted between evolving multi-objective neural networks and applying multi-objective evolutionary algorithms to train neural networks. The former consists of evolving a \textit{single set of network parameters} which enables the network to provide different outputs as the problem objective weighting varies, while the latter consists of utilizing a MO-EA, e.g. NSGA-II \cite{deb2002fast}, to evolve \textit{multiple sets of network parameters} for different objective weightings. This research evolves a single set of MO-NN parameters that are assigned the NN used to control each robot. The implementation of the MO-NN as a robot controller will be discussed in Section \ref{sec:EvoMONN_Robot}.

% \begin{algorithm}[H]
% \caption{Evolving MO-NNs using xNES}
% \begin {algorithmic}
% \State \textbf{Initialize problem domain}
% \State \textbf{Initialize algorithm}\\
% \While{curGen < maxGen}{\\
%     \State Sample $\lambda$ solutions $x$ (Alg \ref{Alg:xNES})\\
%     \For{i in $\lambda$}{
%         \State Set parameters of NNs as $x$
%         \State fitness of NN $= networkFit_i = 0$\\
%         \For{$\Updelta w=0$ to $1$}{
%             \State $w_1 = 1 - \Updelta w$
%             \State $w_2 = \Updelta w$
%             \State Reset environment\\
%             \For{timeStep $t=0$ to $maxTime$}{\\
%                 \For{robot $r=1$ to $numRobots$}{
%                     \State Robot $r$ observes state $s_{r,t}$ of the environment, i.e. sensor readings
%                     \State Pass $w_1$, $w_2$ and $s_{r,t}$ into NN and record NN output $o$
%                     \State Act in environment from NN output $o$, i.e. rotate and move
%                     \State $currentFit =$ fitness of action using objective function (Equation \ref{eqn:curFit})
%                     \State $networkFit_i += currentFit$
%                     }
%             }
%         }    
%     }
%     \State calculate utilities $u$ by sorting $\{(z_i, x_i)\}$ w.r.t $networkFit_i$
%     \State update gradients $G$, $\mu$, $\sigma$, and $\textbf{B}$ (Alg \ref{Alg:xNES})
%     \State $curGen++$
%     }
% \end{algorithmic}
% \label{Alg:MO_NN}
% \end{algorithm}








% Each network must be capable of producing different outputs for different objective weightings given as input to the network. Mason et al. proposed a Multi-Objective Neural Network training architecture in 2018, that will be utilized in this research \cite{mason2018multi}. The distinction should be noted between evolving multi-objective neural networks and applying multi-objective evolutionary algorithms to train neural networks. The former consists of evolving a \textit{single set of network parameters} which enables the network to provide different outputs as the problem objective weighting varies, while the latter consists of utilizing a MO-EA, e.g. NSGA-II \cite{deb2002fast}, to evolve \textit{multiple sets of network parameters} for different objective weightings. This research evolves a single set of MO-NN parameters that are assigned the NN used to control each robot. The implementation of the MO-NN as a robot controller will be discussed in Section \ref{sec:EvoMONN_Robot}.

% \begin{algorithm}[t]
% \caption{Evolving MO-NNs using xNES}
% \begin {algorithmic}
% \State \textbf{Initialize problem domain}
% \State \textbf{Initialize algorithm}\\
% \While{curGen < maxGen}{\\
%     \State Sample $\lambda$ solutions $x$ (Alg \ref{Alg:xNES})\\
%     \For{i in $\lambda$}{
%         \State Set parameters of NNs as $x$
%         \State fitness of NN $= networkFit_i = 0$\\
%         \For{$\Updelta w=0$ to $1$}{
%             \State $w_1 = 1 - \Updelta w$
%             \State $w_2 = \Updelta w$
%             \State Reset environment\\
%             \For{timeStep $t=0$ to $maxTime$}{\\
%                 \For{robot $r=1$ to $numRobots$}{
%                     \State Robot $r$ observes state $s_{r,t}$ of the environment, i.e. sensor readings
%                     \State Pass $w_1$, $w_2$ and $s_{r,t}$ into NN and record NN output $o$
%                     \State Act in environment from NN output $o$, i.e. rotate and move
%                     \State $currentFit =$ fitness of action using objective function (Equation \ref{eqn:curFit})
%                     \State $networkFit_i += currentFit$
%                     }
%             }
%         }    
%     }
%     \State calculate utilities $u$ by sorting $\{(z_i, x_i)\}$ w.r.t $networkFit_i$
%     \State update gradients $G$, $\mu$, $\sigma$, and $\textbf{B}$ (Alg \ref{Alg:xNES})
%     \State $curGen++$
%     }
% \end{algorithmic}
% \label{Alg:MO_NN}
% \end{algorithm}


%\clearpage
% \section{Experimental Methods}
% \label{sec:Exp}


% \subsection{Simulator Design}
% \label{sec:simDesign}

% In order to train the proposed multi-objective neural network (MO-NN) controllers, a low-fidelity swarm robot simulator was developed in Python. This was necessary as evolving neural networks for control tasks requires many trial evaluations. This would be too computationally expensive for high-fidelity simulators.

% The low-fidelity simulator consisted of a $3.7m \times 3.7m$ arena with boundaries at the edge of the arena. Mobile robots were deployed within the arena. The simulated robots had a diameter of $0.25m$. Each robot had 4 rangefinder sensors, located at the front, back and on each side. Each robot can rotate $\pm 45\deg$ and can only move forward up to a maximum velocity of $2m/sec$. This simulator design is based on the DOTS swarm robot testbed \cite{jones2022dots}. 

% A high-fidelity simulator was also developed using Webots \cite{Webots,Webots04}. This was used to test the evolved controllers. The parameters of the Webots simulator are the same as the Python simulator. Figure \ref{fig:simEnv} illustrates the simulated robot and arena.

% Both simulators model non-holonomic robot drive. The low-fidelity simulator does not simulate any physics. Collision detection is implemented in the low-fidelity simulator to prevent a robot from colliding with another robot or the environment boundary. Physics is simulated in the high-fidelity Webots simulator. Collisions are not detected/prevented in the high-fidelity simulator.


% % Figure environment removed






\subsection{Multi-Objective Neural Network Robot Controllers}
\label{sec:EvoMONN_Robot}


The implementation of the neural network controller as a robot controller is illustrated in Figure \ref{fig:NN_Control}. The robot senses its environment using 4 rangefinder sensors. These measure the distance between the sensor and the nearest object. These values are normalized and passed to the network as input. In addition to sensor measurements, two additional inputs are passed into the network representing the weightings assigned to each of the two objectives $[w_1, w_2]$. The network then does a forward pass and gives two outputs, i.e. robot commands. These commands are the rotation angle $[-45\deg, 45 \deg]$ and the forward velocity $[0m/sec, 2m/sec]$. The robot will move to a new position based on these commands.



% Figure environment removed


Figure \ref{fig:EA_NN_Swarm} illustrates the training process of the MO-NNs for the robot swarm. Homogeneity is a key property of robot swarms, therefore the same network parameters are assigned to each NN controller for all robots in the swarm during each simulation.

% Figure environment removed

The quality of each set of NN parameters $x_i$ is measured using the fitness function outlined in Equation \ref{eqn:totFit}.

\begin{equation}
  networkFit(NN(x_i)) = \sum\limits_{\Updelta w=0}^{1} \sum\limits_{t=1}^{t_{Max}} currentFit_{w,t}(NN(x_i))
  \label{eqn:totFit}
\end{equation}

\noindent where $\Updelta w$ is the change in objective weighting applied to $w_1$ and $w_2$. These are updated as $w_1 = 1 - \Updelta w$ and $w_2 = \Updelta w$ $\therefore$ $w_1 + w_2 = 1$  $\forall$  $\Updelta w$. The fitness at the current time-step for the current objective weights is calculated using Equation \ref{eqn:curFit}.

\begin{equation}
\begin{split}
  currentFit_{w,t}(NN(x_i)) = & w_1 \times - 1 \times Obj1_{w,t}(NN(x_i)) \\
  & + w_2 \times Obj2_{w,t}(NN(x_i)) 
\end{split}
\label{eqn:curFit}
\end{equation}

\noindent where $w_1$ is the weighting of objective 1 ($Obj1$) and $w_2$ is the weighting of objective 2 ($Obj2$). Objective 1 is to minimize the distance between robots and the origin (center of the arena), calculated using Equation \ref{eqn:obj1}. Objective 2 is to maximize the velocity of the robots, calculated using Equation \ref{eqn:obj2}. These two objectives were chosen as they are in direct conflict with one another. It should be noted here that $Obj1$ is multiplied by $-1$ as the overall optimisation problem is framed as a maximization problem. 

% swarm dist to origin obj
\begin{equation}
  Obj1_{w,t}(NN(x_i)) = \sum\limits_{r=1}^{numRobots} [|position_{x,r}|+ |position_{y,r}|]
  \label{eqn:obj1}
\end{equation}

\noindent where $position_{x,r}$ and $position_{y,r}$ represent the x and y positions of robot r, respectively. 

% swarm velocity obj
\begin{equation}
  Obj2_{w,t}(NN(x_i)) = \sum\limits_{r=1}^{numRobots} [|velocity_{x,r}|+ |velocity_{y,r}|]
  \label{eqn:obj2}
\end{equation}

\noindent where $velocity_{x,r}$ and $velocity_{y,r}$ represent the x and y velocities of robot r, respectively.

During training, each set of NN parameters $x_i$ is evaluated for 30 seconds of simulated time ($t_{Max} = 30sec$) for each increment of $\Updelta w$. The simulator time step is 1 second. The value of $\Updelta w$ is incremented between 0 and 1 in increments of 0.5. This is to ensure that each network is evaluated based on its ability to control the robot such that each objective is optimised with maximum weighting, i.e. when $w_1 = 1, w_2 = 0$ and $w_1 = 0, w_2 = 1$, and also its ability to control the robot such that each objective is weighted equally, i.e. when $w_1 = 0.5, w_2 = 0.5$. This number of $\Updelta w$ increments was selected to minimize training time. A smaller $\Updelta w$ increment can be implemented during training but would increase training time. Note, a smaller $\Updelta w$ increment can be applied when evaluating the MO-NN, irrespective of the increment size during training. 

% The results presented in Section \ref{sec:res_lowFid} utilize a $\Updelta w$ increment of $0.1$ when generating the Pareto Front, despite implementing a $\Updelta w$ increment of $0.5$ during training.

The NN architecture implemented in this research consisted of 6 input nodes (4 sensor inputs and 2 objective weights), 1 hidden layer with 5 nodes, and an outputs layer with 2 nodes (1 for rotation, 1 for forward movement). The network was evolved over 20,000 evaluations.






\subsection{Experimental Setup}

\begin{enumerate}
    \item \textbf{Evolving Neural Network Controller}.
This experiment consists of evolving the multi-objective neural network controller in the low-fidelity python robot simulator. The MO-NN is evolved in the low-fidelity python simulator with the Objective (obj) preferences: MO-NN 1: Obj 1 - Maximize velocity. Obj 2 - Minimize distance to origin (center of arena).

\item \textbf{Deployment to High-Fidelity Webots Simulator}
The next experiment is to test the performance of the best performing network evolved in the low-fidelity python simulator by deploying the network to control robots in a more realistic high-fidelity Webots simulator.

\item \textbf{Evaluating Evolved Controller on Larger Swarm Sizes}
The final experiment is to determine the scalability of the evolved MO-NN to a larger number of robots, specifically 5 and 10 robots. 

\end{enumerate}


\section{Results}

\subsection{Evolving Neural Network Controller in Low-Fidelity Python Simulator}
\label{sec:res_lowFid}

The evolved MO-NNs were capable of controlling the robot swarm in the desired manner in the low-fidelity python simulator. Figure \ref{fig:lowFidTraject1} illustrate the trajectories for the evolved MO-NN. 

% Figure environment removed

The trajectories in Figure \ref{fig:lowFidTraject1} refer to the training objectives: Objective 1 - Maximize velocity and Objective 2 - Minimize distance to origin (center of arena). It is clear from Figure \ref{fig:lowFidTraject1} that the robot trajectories are significantly different when a high weighting is given to maximizing velocity compared to when a high weighting is given to minimizing distance to the origin. When a high weighting is given to maximizing velocity, the robots move in large circles around the arena. When a high weighting is given to minimizing distance to the origin, the robots move to the center of the arena and stay there. When an equal weighting is given to both objectives, the robots traverse the arena close to the center in smaller circles.


% The trajectories in Figure \ref{fig:lowFidTraject2} refer to the training objectives: Objective 1 - Maximize distance to origin and Objective 2 - Minimize distance to origin. Similarly, different behaviour can be observed for different objective weighting passed into the NN controller. When a high weight value for the maximize distance to the origin objective is selected, the robots remain close to the boundary. When the weight is then changed to minimize distance to the origin, the robots move to the center of the arena and stay there.


% % Figure environment removed

% % Figure environment removed

% ****edit this***
% The convergence graph of the evolutionary multi-objective neural network is illustrated in Figure \ref{fig:convergence_pareto} (left). This figure illustrates that the algorithm has converged within the 20,000 evaluations of different robot neural network controller parameters.


% Figure environment removed

% % Figure environment removed


% % Figure environment removed



Figure \ref{fig:convergence_pareto} illustrates how the each of the objective function evaluations of the behaviour of the evolved neural network change as the objective weighting changes. When a maximum weighting is given to minimizing the distance to the origin objective, both the distance and velocity objective scores are lowest. Conversely when maximizing velocity, the distance to the origin also increases as the robots are moving with higher velocity around their environment. This graph illustrates how different behaviour can be observed from the swarm of robots using a single neural network by simply modifying the weighting for each objective function. No retraining is required for different objectives or different robots. This is the key advantage of the proposed MO-NN approach.

% % Figure environment removed


\subsection{Testing Evolved Controllers in High-Fidelity Webots Simulator}

% % Figure environment removed

After training in a low-fidelity Python simulator, the evolved MO-NN robot controller was then deployed to control robots implemented in the high-fidelity Webots simulator. The motivation for doing this was to test whether the behaviours of the evolved controllers have the potential to translate to simulators with more realistic physics without the need to adapt the motor commands.

Two simulations were conducted. In the first simulation, the robots' objective preference was to minimize distance to the origin, i.e., all robots were passed an objective weighting of 1 for the minimize distance to the origin objective, and 0 for the maximize velocity objective. In the second simulation, the robots' objective preference was to maximize velocity. It was observed that the trained MO-NNs exhibited similar behaviour when tested in the Webots environment. 

% Figure \ref{fig:Webots60Sec} illustrates the robots in their starting position at time $t=0$ seconds and also after approximately 60 seconds with max weighting for each objectives, i.e. minimizing distance to origin and maximizing velocity.

% % Figure environment removed

When minimizing distance to the origin, the robots moved much slower. The robots gradually made their way to the center of the arena and do not adjust their position much thereafter. Each robot continually rotates in order to sense more of its environment. 

When maximizing velocity, the robots move continuously around the arena and do not stop at the center. There is a greater risk of collisions when moving with higher velocity. In order to reduce the severity of the collisions, the maximum wheel rotational speed was reduced from 80 radians/sec to 60 radians/sec. Without this speed reduction, robots collided with one another, overturned and remained immobilized. Note, this velocity clamping was applied in both simulations reported.




\subsection{Larger Robot Swarms in High-Fidelity Webots Simulator}

The next set of simulations conducted was to establish if the evolved MO-NN can scale to larger swarm sizes without retraining. The motivation for this was to test if the controllers evolved in an environment with fewer robots are robust enough to scale to environments with more robots and therefore greater chance of collisions. In order to test this, simulations were conducted for robot swarm sizes of 5 and 10 robots, using the evolved MO-NN trained in a swarm size of 3. 

It was observed that the robot swarm gather at the center of the arena after 60 seconds in all robot swarm sizes when minimizing distance to the origin. Different behaviour was observed when maximizing velocity. After 60 seconds, the robots are dispersed around the arena as they are traversing the arena with higher velocity.

%Figure \ref{fig:largerSwarms} depicts the positions of the robots after 60 seconds in each simulation conducted.



% ***maybe move this image to appendix***
% % Figure environment removed

% As Figure \ref{fig:largerSwarms} illustrates, 

% The evolved MO-NN scales well when deployed in larger robot swarms than it was trained in. From the bottom set of results in Figure \ref{fig:largerSwarms}, the robots gather at the center of the arena after 60 seconds in all robot swarm sizes when minimizing distance to the origin. The top set of results show different behaviour when maximizing velocity. The robots here are more dispersed as they are traversing the arena with higher velocity.


% Figure environment removed

Figure \ref{fig:boxPlot} illustrates the spread in the distance to the origin and the velocity averaged over 10 robots at each second in the 10 robot swarm for 60 seconds. This graph clearly illustrates how the robots travel with significantly higher velocity when the weighting on the velocity objective is at its maximum, compared to when a maximum weighting is applied to the distance to origin objective. This difference is statistically significant when compared using the two tailed Wilcoxon signed rank test, with significance level $\alpha = 1\%$. Similarly, when comparing the distance to the origin at each second, it can be seen that the distance is significantly lower when maximizing the weighting for the distance objective compared to maximizing the velocity objective, with a significance level $\alpha = 1\%$.

% % Figure environment removed


% Figure \ref{fig:heatMap} presents a heatmap of the positions of all robots in the 10 robot swarm over 60 seconds when maximizing velocity (left) and minimizing distance to the origin (right). The figure on the left illustrates how the robots are more dispersed throughout the map when maximizing velocity. When minimizing distance to the origin, the robot positions are concentrated heavily in the center of the map. This is as expected.



% Figure environment removed


Figure \ref{fig:heatMap} presents a heatmap of the positions of all robots in the 10 robot swarm over 60 seconds when maximizing velocity (a) and minimizing distance to the origin (b). The figure on the left illustrates how the robots are more dispersed throughout the map when maximizing velocity. When minimizing distance to the origin, the robot positions are concentrated heavily in the center of the map. This is as expected.

% Figure environment removed


Figure \ref{fig:metricsPerTime} presents the robot velocity (left) and distance to origin (right) over 60 seconds of simulation time when maximizing the weighting to each objective. Under both objective preferences, the average velocity of the swarm is slow initially. When minimizing the distance to the origin, there is a large spike in velocity at time step 8. This corresponds in a significant reduction in the average distance to the origin. There are multiple smaller spikes in average velocity which further reduce the swarm distance to the origin. When maximizing velocity, there is a similar large increase in velocity early in the simulation. The average velocity of the robots then stabilize with some oscillations thereafter.









\section{Conclusion}

This research proposed an evolutionary multi-objective (MO) neural network (NN) for robot swarm control. The MO-NN was evolved using a low-fidelity Python simulator in an environment with 3 robots. The controller was then tested in a high-fidelity simulated environment developed using Webots. The MO-NN controller was then evaluated for a larger numbers of robots. 


The primary findings of this research are:

1) It was demonstrated that the evolved MO-NN is an effective approach for controlling robots in a swarm in the presence of multiple objectives. As the weightings for each objectives were varied, the robots adjusted their behaviour to account for the preference for each objective. The velocities and distances to the origin during simulation were significantly different (p value $<1\%$) when comparing a maximum weighting for each objective. 

2) MO-NN evolved in a low-fidelity simulator can be transferred to a high-fidelity simulator and exhibit similar behaviour. This is because the low-fidelity simulator can replicate the relevant features of the high-fidelity simulator, specifically sensor input, decision making and updates to robot positions. This approach is therefore recommended to reduce the computational cost associated training NN controllers. This indicates that the evolved MO-NN controllers are robust to environmental changes.

3) The MO-NN evolved in a low-fidelity simulated environment with 3 robots scales well when tested in a high-fidelity simulated environment with 10 robots. No further NN training is required. This is a key advantage of the proposed evolutionary MO-NN approach as opposed to MO path planning.


There are multiple directions for future research that have stemmed from this research. In particular, the next natural step is to test the evolved MO-NN on a physical robot swarm outside of simulation, e.g. the DOTS swarm arena \cite{jones2022dots}. Another avenue for future research would be to investigate the scalability of the MO-NN to greater than two objectives. The experimental results presented in this paper is for a relatively small swarm size. It is hypothesized that the proposed approach would scale well for larger swarms. This will be evaluated in future work.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% The acknowledgments section is defined using the "acks" environment
%%% (rather than an unnumbered section). The use of this environment 
%%% ensures the proper identification of the section in the article 
%%% metadata as well as the consistent spelling of the heading.

\section*{Acknowledgements}
%\begin{acks}
Research reported in this publication was funded by the Royal Irish Academy. The content of this publication is solely the responsibility of the authors and does not necessarily represent the official views of the Royal Irish Academy.

% Figure environment removed
%\end{acks}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% The next two lines define, first, the bibliography style to be 
%%% applied, and, second, the bibliography file to be used.

\bibliographystyle{ACM-Reference-Format} 
\bibliography{references}













%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



% \section{Introduction}\label{sec:Intro}

% %what is the problem?

% Buildings consume a significant portion of this global energy consumption everyday for their various operations \cite{nejat2015global}. HVAC is one of the highest loads and is estimated to be responsible for approximately 50\% of total building energy consumption \cite{perez2008review}. Thermal comfort is now an expectation in most work environments, resulting in a vast amount of energy being utilized from HVAC operations to maintain a comfortable work environment. 72\% and 70\% of the total office energy consumption in the UK and US respectively is for HVAC and lighting \cite{juan2010hybrid}. Of this 70\%, it is estimated that 40\% of this energy is consumed by HVAC and 30\% is consumed by lighting \cite{boyano2013energy}. Reducing this vast energy consumption while still maintaining satisfactory building operation is therefore crucial. 



% % problem 1

% In order to reduce this energy consumption, it is vital to develop effective control policies for HVAC operations \cite{mason2019RLBEM}. Machine learning algorithms have received a significant amount of attention in recent years due to their impressive performance at many tasks including control. Neural networks are one of primary sub fields within machine learning and have had many successful applications within building energy management \cite{kumar2013energy}. In order for neural networks to perform any task, the parameters of the network must be optimized to match inputs with the desired outputs. Traditionally, methods such as gradient descent are used to train the network's parameters. However gradient decent relies on the use of problem gradients to optimize the networks parameters, which rely on knowledge of what the target outputs are. When considering the task of HVAC control, target outputs are not available as the optimal control policy is not known in advance. This poses a problem as to how to optimize a neural network's parameters to perform the task of HVAC control.  


% % Solutions
% A solution that has been proposed in the literature to this problem is to use evolutionary algorithms to optimize neural network parameters \cite{stanleyNE2019}. This is commonly referred to as Neuroevolution. Evolutionary algorithms have the desirable property of not relying on target outputs. Instead, these methods only need an objective function to provide a measure of performance. Natural Evolution Strategies (NES) are a family of evolutionary algorithms that evolve a distribution used to sample solutions from the space of all possible solutions. Evolutionary strategies such as NES have proven to be effective optimizers and have provided competitive performance when compared to approaches such as reinforcement learning for tasks such as playing Atari games \cite{salimans2017evolution}. Zemella et al. demonstrated that evolutionary neural networks can be effective for energy efficient building design \cite{zemella2011optimised}. This paper builds on this prior research and aims to demonstrate that the same benefits observed in building design can carry over to the area of HVAC control.



% The research presented in this paper makes the following contributions:  

% \begin{enumerate}
%     \item To introduce the Natural Evolution Strategy algorithm to the domain of building energy management.
    
%     \item The apply evolutionary neural networks to HVAC control.
    
%     \item To compare the performance of evolutionary neural networks with reinforcement learning for the task of HVAC control.
    
% \end{enumerate}


% The outline of the paper is as follows. Sections 2 will give an overview of the area of HVAC control and the HVAC simulator design. Section 3 will provide an description of the machine learning methods implemented in this paper, in particular Genetic Algorithms, Exponential Natural Evolution Strategies and Deep Q-Networks. An outline of the experiments conducted will be outlined in Section 4. Section 5 will then present the results of the experiments conducted. Finally, Section 6 will outline what conclusions can be made as a result of the research described in the paper.

% % Section 6 will then provide a discussion of the results and their impact.

% \section{HVAC}

% \subsection{Background}


% HVAC systems operate by monitoring the ambient temperature of the room and by adjusting their operation based on the current temperature and the desired temperature. A basic threshold approach operates by checking if the temperature is within the comfort region. If the temperature is too cold, the heat is turned on and if the temperature is too warm, the air conditioner is switched on. There are other factors to consider when managing building energy consumption, including the occupancy of the building, renewable energy generation, the air velocity, humidity, etc. This research only considers temperature.


% There are many applications of evolutionary algorithms to optimize HVAC systems. Kusiak et al. implemented a multi-objective evolutionary algorithm to optimize the temperature set points of a HVAC system \cite{kusiak2011multi}. Similarly, Ghahramani et al. utilized a hill climbing meta heuristic to optimize the temperature set points of a HVAC system \cite{ghahramani2017hvac}. Lu et al. implemented a Genetic Algorithm to optimize temperature set points of a HVAC system \cite{lu2005hvac}. Each of these studies utilize evolutionary computing to optimize parameters of a HVAC system. There are no studies in the literature that utilize evolutionary methods to optimize a neural network controller in a similar manner to Reinforcement Learning (RL) algorithms to control the operation of a HVAC system. That is the key contribution of the research presented in this paper. 
% % The neural network reads as input the state of the environment and outputs a control action for the HVAC system. This will be expanded upon in detail in Section \ref{sec:Exp}. There have been multiple studies demonstrating the competitiveness of evolving neural networks when compared to reinforcement learning \cite{salimans2017evolution,such2017deep}. Each of these studies only consider their performance in video game environments. This research contrasts the performance of evolutionary neural networks and reinforcement learning for the task of HVAC control.

% % There is a wealth of applications in the literature of RL algorithms to HVAC control \cite{mason2019RLBEM}. It is common for applications of RL to provide 10\% energy savings when compared to standard threshold based approaches \cite{mason2019RLBEM}. Some of the RL algorithms that have been applied to HVAC control include: Q Learning \cite{barrett2015autonomous}, DQN \cite{wei2017deep} and A3C \cite{zhang2018deep}. Each of these algorithms demonstrated significant improvements when compared to threshold based controllers.


% \subsection{Simulator Design}
% \label{sec:simDesign}

% The simulated environment in this paper is largely based on that of Barrett and Linder \cite{barrett2015autonomous}. The environment consists of a $3m \times 3m$ room which contains a heater of power $P_{Heater} = 1kW$. At every time step, the controller must make a decision to either turn on or off the heater in the room. The time step in this paper was of length 1 minute. The temperature of the room at time t is represented as $T_{r,t}$, while the outside temperature is represented as $T_{o,t}$. The heat transfer between the room and the outside environment $Q$ is calculated using Equation \ref{eqn:heatTransfer}.

% \begin{equation}
%   Q = U_{Val} \times A \times (T_{r,t} - T_{o,t})
%   \label{eqn:heatTransfer}
% \end{equation}

% Where $A$ is the surface area and $U_{Val} (W/m^{2}K)$ is the heat transfer coefficient for a given material. In the simulated environment outlined here, the $U_{Val} = 0.4, 0.5, 0.6$ for the ceiling, floor and walls respectively. At each time step, the total heat transfer $Q_{Total}$ is calculated as the sum of the heat transferred through the ceiling, floor and walls, as outlined in Equation \ref{eqn:totHeatTransfer}.

% \begin{equation}
%   Q_{Total} = Q_{Ceiling} + Q_{Walls} + Q_{Floor}
%   \label{eqn:totHeatTransfer}
% \end{equation}

% Once the total heat transfer is calculated, the temperature change $\Delta T$ of the room can be calculated using Equation \ref{eqn:tempChange}.

% \begin{equation}
%   \Delta T = \frac{t \times (P_{Heater} - Q_{Total})}{718 \times 1.3 \times Vol_{Room}} 
%   \label{eqn:tempChange}
% \end{equation}

% Where $718 J/Kg K$ is the specific heat capacity of air, $1.3 Kg/m^{3}$ is the density of air, $t = 60 sec$ is the time between each time step and $Vol_{Room}$ is the volume of the room. It is assumed that the heater is 100\% efficient, as was the case in previous studies \cite{barrett2015autonomous}.

% The temperature data used in the simulations outlined in this paper was sourced from the National Oceanic and Atmospheric Administration (NOAA) website \cite{diamond2013us,NOAA}. The data obtained from the NOAA website is for Ithica, New York and is in increments of 5 minutes. The data was then interpolated to refine the granularity to one minute by assuming a linear change in the values between each 5 minute time step. For example, if the temperature $T = \ang{11} C$ at time $t = 10$ and $T = \ang{12} C$ at time $t = 15$, the intermediate temperatures were estimated to be $T = \ang{11.2} C, \ang{11.4} C, \ang{11.6} C, \ang{11.8} C$ at times $t = 11, 12, 13, 14$. A one minute resolution is the lowest realistic time step for HVAC control. A lower resolution, e.g. 30 seconds, would result in too many frequent changes in the HVAC operation which would be undesirable for the occupants. A higher resolution, e.g. 5 minutes, would be too coarse and result in regular over and under heating of the simulated room. The HVAC controller was initially evaluated over just one day (1st of March 2018) and then for the full month of March.





% \section{Machine Learning Methods}
% This section will outline the algorithms evaluated in this study along with their implementation.


% \subsection{Evolutionary Methods}
% \label{sec:evoAlg}


% Evolutionary algorithms operate by creating a population of $N$ random initial solutions $P$, where each solution is often referred to as genotype. At each generation $G$, the population of solutions are evaluated to identify the fittest individual, based on an objective function $F$. Fitter individuals are more likely to be selected to create the next generation of solutions. Evolutionary strategies are a subset of evolutionary algorithms that do not typically use crossover and are primarily applied to problems with real value variables. Beyer provides a comprehensive overview of evolutionary strategies \cite{beyer2002evolution}. 

% % A number of studies have demonstrated the competitiveness of evolving neural networks when compared to reinforcement learning \cite{salimans2017evolution,such2017deep}. Each of these studies only consider their performance in video game environments. This research contrasts the performance of evolutionary neural networks and reinforcement learning for the task of HVAC control.



% % Evolutionary algorithms date back to the 1970s \cite{schwefel1977numerische,holland1973genetic}. Evolutionary algorithms operate by creating a population of $N$ random initial solutions $P$, where each solution is often referred to as genotype. At each generation $G$, the population of solutions are evaluated to identify the fittest individual, based on an objective function $F$. Fitter individuals are more likely to be selected to create the next generation of solutions. Elitism is often implemented, whereby the fittest $E$ individuals are directly passed onto the next generation unchanged. Figure \ref{fig:EvoAlg} depicts a flowchart outlining the broad steps taken in a typical evolutionary algorithm. Evolutionary strategies are a subset of evolutionary algorithms that do not typically use crossover and are primarily applied to problems with real value variables. Beyer provides a comprehensive overview of evolutionary strategies \cite{beyer2002evolution}.

% % % Figure environment removed



% % % Figure environment removed




% \subsection{Evolutionary Neural Networks}
% % Evolutionary Neural Networks (often referred to as Neuroevolution) is a sub field of machine learning that consists of utilizing evolutionary algorithms to optimize the parameters of neural networks to achieve some task \cite{stanleyNE2019}. A neural network is a function approximator inspired by the biological brain. They have been applied to a wide range from wireless sensor networks \cite{wang2019network} to wind speed forecasting \cite{begam2019optimized}. Neural networks read a signal through the input layer of neurons. This signal is then propagated though the subsequent hidden layers of the network via weighted connections. At each neuron in a layer, the incoming signal is processed by an activation function which determines that neuron's output. Common activation functions include sigmoid, tanh and relu functions. Eventually the signal is outputted from the final output layer of the network which is used for some task, e.g. classifying an image. When evolving neural networks with a direct encoding scheme, the parameters of the network are the genotype. The algorithm is initialized with a population of random neural network parameters $\theta$ where each individual (set of parameters $\theta_{i}$) is evaluated on some task, in this case HVAC control (See Figure \ref{fig:EA_NN_HVAC} in Section \ref{sec:Exp}). The fitness of each individual is determined based on an objective function $F$. The next generation of network parameters are generated using the operations previously outlined in Section \ref{sec:evoAlg}. Evolutionary neural networks have proven to be effective algorithms for many problem areas, from cloud computing \cite{mason2018predicting} to energy forecasting \cite{mason2018forecasting}.

% Evolutionary Neural Networks (often referred to as Neuroevolution) is a sub field of machine learning that consists of utilizing evolutionary methods to optimize the parameters of neural networks to achieve a machine learning task \cite{stanleyNE2019}. When evolving neural networks with a direct encoding scheme, the parameters of the network are the genotype. The algorithm is initialized with a population of random neural network parameters $\theta$ where each individual (set of parameters $\theta_{i}$) is evaluated on some task, in this case HVAC control (See Figure \ref{fig:EA_NN_HVAC} in Section \ref{sec:Exp}). The fitness of each individual is determined based on an objective function $F$. The next generation of network parameters are generated using the operations previously outlined in Section \ref{sec:evoAlg}. Evolutionary neural networks have proven to be effective algorithms for many problem areas, e.g. energy forecasting \cite{mason2018forecasting}, power generation scheduling \cite{mason2018multi} and cloud computing \cite{mason2018predicting},






% \subsection{Genetic Algorithm}

% In order to establish a baseline level of performance, a standard Genetic Algorithm (GA) is implemented to compare with the performance of the Exponential Natural Evolution Strategies algorithm (xNES). The GA implemented here is the same baseline GA implemented in multiple previous studies \cite{such2017deep,lehman2018more}. This algorithm consists of a population $P$ of $N$ individuals. Each of these individuals correspond to a solution to the optimization problem, i.e. a set of neural network parameters $\theta$ initialized at random $U (-1,1)$. At each generation, all individuals are evaluated using a predefined objective function to determine their fitness. A truncated selection is applied whereby the fittest $T$ individuals are selected as parents for the next generation. To form the next generation, a parent is selected uniformly at random from $T$. This parent is then mutated by applying Gaussian noise to its parameters $\theta$. The Gaussian noise is scaled by $\sigma \in \mathbb{R}$. $\sigma$ is an experimentally determined constant used to determine the strength of the mutations, i.e. size of the Gaussian signal. This is repeated $N-1$ times. The final $N^{th}$ individual to go into the next generation is the fittest individual from the current generation. The pseudocode in Algorithm \ref{Alg:GA} outlines the steps taken in the GA.

% \begin{algorithm}[]
% \caption{GA Pseudocode}
% \begin {algorithmic}
% \State \textbf{Input variables:} $N, T, \sigma$ 
% \State Generation $G=0$\\

% \While{Termination criteria not met}{\\

%     \If{Generation G = 0}{\\
%     \For{Individual i in $N$}{\\
%                 \State Initialize Individual $\theta_{P_{i}}^{G} \sim U(-1,1)$}
%     }\\
% \Else{\\
% \For{Individual i in $N-1$}{\\
%     \State Select Parent Index $k = U(1,T)$
%     \State Individual $\theta_{P_{i}}^{G} = \theta_{P_{k}}^{G-1} + \sigma \mathcal{N}(0,1)$}
    
% \State Insert Elite Individual $P_{elite}^{G-1}$ into $P^{G}$\\
% }
% \State Evaluate Fitness $F_{i}^{G}$ using :  $F(P_{i}^{G})$ 
% \State Sort $\{(P^{G},F^{G})\}$ w.r.t. $F^G$
% \State $G ++$}
% \end{algorithmic}
% \label{Alg:GA}
% \end{algorithm}



% \subsection{Exponential Natural Evolution Strategies}

% Exponential Natural Evolution Strategies (xNES) \cite{glasmachers2010exponential} is an algorithm within the family of Natural Evolution Strategies (NES) algorithms \cite{wierstra2014natural}. The xNES algorithm is initialized by sampling $\lambda$ solutions at random. The matrix $\textbf{A}$ represents the covariance factor, initialized as the Identity matrix $\textbf{I}$. The normalized covariance factor $\textbf{B}$ is calculated as a $\textbf{B} = \textbf{A}/\sigma$, where $\sigma$ is the scalar step size initialized as $\sqrt[d]{|det(\textbf{A})|}$ and $d$ is the dimensionality of the optimization problem. At each generation, the algorithm samples $\lambda$ values $z_i \sim \mathcal{N}(0,1)$. These values are then used to calculate $\lambda$ solutions $x_i$ as $x_i = \mu + \sigma \textbf{B} z_i$, where $\mu$ is the center of the search distribution. These solutions are then evaluated and ranked based on their fitness $f(x_i)$ to calculate their utility $u$. The utility $u$ is implemented as it allows for rank based fitness shaping and is calculated using Equation \ref{eqn:UtilityFunction}.

% \begin{equation}
%   u_i = \frac{max (0, log(0.5*n + 1) - log(i))}   {\Sigma_{j=1}^{n} max (0, log (0.5*n +1) - log(j))} - \frac{1}{n}
%   \label{eqn:UtilityFunction}
% \end{equation}

% The gradients $G$ of the objective function can now be calculated w.r.t. $\delta, M, \sigma$ and $\textbf{B}$. Where M is a $d \times d$ exponential map used to represent the covariance matrix $\textbf{C}$, where $\textbf{C = AA}^{T}$ and $\delta$ is the change in $\textbf{C}$. The pseodocode in Algorithm \ref{Alg:xNES} demonstrates how the xNES algorithm operates. 

% % Unlike the GA outlined in the previous section, the xNES algorithm evolutionary operators are more abstract. The GA represents the population of solutions directly and applies the evolutionary operations directly to those solutions. The xNES algorithm maintains a normalized covariance matrix and a scalar step size rather than a population of solutions. Rather than perturbing solutions with Gaussian noise, xNES calculates natural gradients and utilizes these gradients to adjust how it samples the solutions.




% \begin{algorithm}[]
% \caption{xNES Pseudocode}
% \begin {algorithmic}
% \State \textbf{Input variables:} $d, \mu, \textbf{A}$ 
% \State $\sigma \leftarrow \sqrt[d]{|det(\textbf{A})|}$
% \State $\textbf{B} \leftarrow \textbf{A}/\sigma$\\
% \While{termination criteria not met}{\\
%     \For{individual i in $\lambda$}{
%             \State Sample $z_i \leftarrow \mathcal{N}(0,1)$ 
%             \State $x_i \leftarrow \mu + \sigma \textbf{B} z_i$}
%     \State sort $\{(z_i, x_i)\}$ w.r.t $f(x_i)$ to calculate utilities $u$
%     \State $G_\delta \leftarrow \Sigma_{i=1}^{\lambda} u_i \cdot z_i$
%     \State $G_M \leftarrow \Sigma_{i=1}^{\lambda} u_i \cdot (z_i z_{i}^T - \textbf{I})$
%     \State $G_\sigma \leftarrow \frac{trace(G_M)}{d}$
%     \State $G_B \leftarrow G_M -G_\sigma \cdot \textbf{I}$
%     \State $\mu \leftarrow \mu + \eta_\mu \cdot \sigma \textbf{B} \cdot G_\delta$
%     \State $\sigma \leftarrow \sigma \cdot exp (0.5 \eta_\sigma \cdot  G_\sigma)$
%     \State $\textbf{B} \leftarrow \textbf{B} \cdot exp (0.5 \eta_\textbf{B} \cdot  G_\textbf{B})$}
% \end{algorithmic}
% \label{Alg:xNES}
% \end{algorithm}



% \subsection{Reinforcement Learning}

% Reinforcement learning (RL) algorithms date back over 50 years, the first of which were proposed in the 1960s \cite{waltz1965heuristic} and 1970s \cite{fu1970learning}. RL algorithms operate by replicating the process of learning to enable agents to complete some task. The RL agent typically interacts with a simulated environment. The agent will observe the state of the environment $s$, then make an action $a$ based on its policy $\pi$. The environment will then transition to the next state $s'$ with a probability $P(s' \mid s, a)$. The agent will then receive a reward $r$. This discrete time framework is referred to as a Markov Decision Process (MDP) \cite{sutton2018reinforcement}. Figure \ref{fig:EA_NN_HVAC} in Section \ref{sec:Exp} illustrates how an RL agent interacts with its environment, in the context of a HVAC simulator. In this paper, an RL agent's reward function and evolutionary NN's fitness function have the same meaning.



% \subsection{Deep Q-Network Algorithm}
% % Q Learning is one of the most well studied RL algorithms \cite{Watkins89}. The Q Learning algorithm represents the agents policy as a look up table, which maps Q values to state action pairs. The agent can then use this look up table to decide which actions to take in a given state. This approach works well when considering problems with small state action spaces, but scales poorly to larger or continuous state spaces. To overcome this issue, neural networks are now used to represent the agents policy in place of look up tables when addressing larger problems. This is the approach used in the Deep Q-Network (DQN) algorithm \cite{mnih2015human}. 

% The Deep Q-Network (DQN) algorithm consists of both a policy network ($Q$) parameterized by $\theta$ and a target network ($\hat{Q}$) parameterized by $\theta^{-}$ \cite{mnih2015human}. For a given state $s$, the agent takes a random action with probability $\epsilon$, otherwise its action is determined by $Q$. The environment transition to the state $s' = s_{t+1}$ at time t+1 and the agent receives reward $r$. The DQN algorithm keeps a record of past experiences. Once the environment transitions to $s'$, the experience ($s_{t}, a_{t}, r_{t}, s_{t+1}$) is added to the agents replay memory D. The agent then samples a mini batch of transitions from D and performs gradient decent to update parameters $\theta$, as illustrated in Algorithm \ref{Alg:DQN}. Every C steps, the target network $\hat{Q} = Q$. DQN is considered a state of the art deep RL algorithm and has had many successful applications including HVAC control. As such, it is implemented as a representative RL algorithms to compare evolutionary neural networks to for the task of HVAC control. 

% \begin{algorithm}[]
% \caption{DQN Algorithm}
% \begin {algorithmic}
% \State \textbf{Initialize} replay memory D to capacity N
% \State \textbf{Initialize} policy network $Q$ with random parameters $\theta$
% \State \textbf{Initialize} target network $\hat{Q}$ with random parameters $\theta^{-}$\\
% \While{Episode e \textless Emax}{
%     \State Reset environment\\
%     \For{Time $t = 1$ \textbf{to} T}{
%         \State Observe state $s_t$ 
%         \State With prob $\epsilon$, select random action $a_t$ $\in A$ 
%         \State Otherwise select action $a_t = argmax_{a} Q(s_{t},a ; \theta)$ 
%         \State Observe reward $r_t$ and new state $s_{t+1}$ given $a_t$ 
%         \State Store transition ($s_{t}, a_{t}, r_{t}, s_{t+1}$) in D
%         \State Sample minibatch ($s_{j}, a_{j}, r_{j}, s_{j+1}$) from $D$ \\
%         \If{$j+1$ is terminal state}{
%             \State $y_j = r_{j}$
%         }\\
%         \Else{
%             \State $y_j = r_{j} + \gamma max_{a'}\hat{Q}(s_{j+1},a' ; \theta^{-})$
%         }
%         \State Perform gradient decent step on $(y_j - Q(s_{j},a_j ; \theta))^{2}$ w.r.t. $\theta$
%         \State $\hat{Q} = Q$ every C steps
        
%     }
% }
% \end{algorithmic}
% \label{Alg:DQN}
% \end{algorithm}









% %\clearpage
% \section{Experimental Procedure}
% \label{sec:Exp}
% This section will outline the experimental set up and simulation parameters implemented in this paper. Figure \ref{fig:EA_NN_HVAC} illustrates how a neural network controller is applied to the task of HVAC control. The network consists of 5 hidden units, each with a Sigmoid activation function.

% % % Figure environment removed

% % Figure environment removed





% This network input is the state of the simulated HVAC environment. The state space implemented here corresponds to: 1) The room temperature $T_{r,t}$, normalized between 0 and 1, $\in \mathbb{R} [0,1] $. 2) The outside temperature $T_{o,t}$, normalized between 0 and 1, $\in \mathbb{R} [0,1] $. 3) Whether the room is occupied or not, $\in \mathbb{N} [0,1] $. The outputs of the network are the HVAC control decisions. The actions available to the network are to: 1) Turn the heat on. 2) Turn the heat off. The fitness of the network ($f_{t}(NN)$), or reward ($R_t$) at each time step is defined by Equation \ref{eqn:fit_per_timestep}. The values of the reward at each scenario were determined experimentally and are similar to those implemented by Barrett and Linder \cite{barrett2015autonomous}. The HVAC simulator implemented here is the same as the simulator outlined in Section \ref{sec:simDesign}, also based on the simulator implemented by Barrett and Linder.




% \begin{equation}
%     f_{t}(NN) = \begin{cases}
%             -20       ,& \textbf{if } \neg Occ \And hON \And T_{r,t}>SPhigh \\
%             -10,      & \textbf{elif } \neg Occ \And hON \And T_{r,t} \leq SPhigh \\ 
%               0,      & \textbf{elif } \neg Occ \And \neg hON\\ 
%             -40       ,& \textbf{elif } Occ \And hON \And T_{r,t}>SPhigh \\
%             -5       ,& \textbf{elif} Occ \And hON \And T_{r,t}<SPlow \\
%              -5       ,& \textbf{elif} Occ \And hON \\
%             0       ,& \textbf{elif} Occ \And \neg hON \And ComfortZone \\
%             -50       ,& \textbf{elif} Occ \And \neg hON \And T_{r,t}<SPlow \\
%             -5       ,& \textbf{elif} Occ \And \neg hON \And T_{r,t}>SPhigh \\
            
%             \end{cases} \\
%     \label{eqn:fit_per_timestep}
% \end{equation}

% Where $Occ$ is a boolean variable that determines if the room is occupied or not and $hON$ is a boolean variable that determines if the heat is on in the room. $SP = \ang{23}C$ is the set point temperature for the room. In order to achieve thermal comfort in the room, this must be adjusted to be a range so that the thermal comfort can be achieved if the room temperature $T_{r,t}$ is within the range. This range is defined as $\ang{1}C$ over and under the set point temperature, i.e. $SPhigh = \ang{24}C$ and $SPlow = \ang{22}C$. $ComfortZone$ is a boolean variable that indicates the temperature is within the acceptable temperature range. Throughout this paper, thermal discomfort is defined as the number of minutes where the room temperature is outside of the desirable range while the room is occupied. The room is occupied between the hours of 7 am and 6 pm, Monday through Friday. The agents must learn to turn the heat off when the room is unoccupied.

% The total fitness $F(NN)$ after the full number of time steps $t_{Max}$ is defined as the sum of fitness values at each time step (Equation \ref{eqn:totFit}). 


% \begin{equation}
%   F(NN) = \sum\limits_{t=1}^{t_{Max}} f_{t}(NN) 
%   \label{eqn:totFit}
% \end{equation}

% The experiments conducted in this paper will consider two scenarios: 1) 1 simulated day of HVAC operation. 2) 1 simulated month of HVAC operation.



% \section{Results}

% % When evaluating the performance of each algorithm, it is important to consider the convergence rate of each. Figure \ref{fig:1DayConverge} illustrates the convergence of the GA, xNES and DQN algorithms when learning the HVAC controls for 1 simulated day. Each episode in Figure \ref{fig:1DayConverge} consists of 1 simulated day of the HVAC environment. This graph therefore illustrates the convergence of each algorithm over a total of 20,000 episodes or simulated days of the HVAC environment (similarly Figure \ref{fig:1MonthConverge} later illustrates the convergence of a total of 20,000 simulated months). 

% It is evident from Figure \ref{fig:1DayConverge} that the xNES algorithm converges to the solution with the highest fitness when compared to the other algorithms. xNES converges to an average fitness of -4632.0 (Calculated using Equation \ref{eqn:fit_per_timestep}). This is significantly higher than the best fitness (reward) achieved of DQN (-5610.5) and the GA (-6867.0). The xNES and GA algorithms have much slower initial convergence rates than DQN. The convergence of the DQN algorithm is much less gradual than xNES. DQN starts with a much higher initial fitness however ($\approx -20000$). This is because DQN is implemented with an $\epsilon$-greedy policy which makes it take random actions initially in order to explore the problem. This results in the RL agent turning the heat on and off randomly which gives a fitness of $\approx -20000$. Conversely both the xNES and GA trained neural networks do not have such a policy and begin with networks with random parameters. These untrained initial neural networks result in either the heat always on or always off which results in a much lower fitness $\approx -40000$. As the DQN algorithm learns, its $\epsilon$ value gradually decreases from $0.9$ to a lower limit of $0.1$. Between 7300 and 7400 episodes, $\epsilon$ reaches its lower limit of $\epsilon = 0.1$ and its convergence rate slows. The convergence of the GA is much less smooth than xNES and DQN. This is because a larger population was required for the GA to preform well than was required for the xNES algorithm. Each generation of the GA can be observed as a jump in reward/fitness in the convergence graph. 

% % The peak at the beginning of each generation corresponds to the best solution being passed directly onto the next generation (elitism).

% % After 5000 episodes, $\epsilon = 0.2$ which is low enough for the RL agent to begin to improve its solution and achieve a higher fitness.

% % % Figure environment removed


% % Figure environment removed


% Figure \ref{fig:1DayRTvsOT} illustrates the room temperature vs the outside temperature that the optimally trained HVAC controller produces for one day. The temperature is initialized at $15\degree C$ in the simulated room. This quickly drops to below $0\degree C$ to match the outside temperature. Once the simulator reaches 7 am (420 minutes after beginning), the room becomes occupied and the heat is turned on. The room reaches a satisfactory temperature approximately 15 - 20 mins after the room is occupied. The heat is then turn on and off accordingly for the next 11 hours until 6 pm when the room becomes unoccupied and the heat is turned off, at which point the temperature drops again. 


% % % Figure environment removed

% % Figure environment removed


% Figure \ref{fig:1DayBoxPot} illustrates the spread of the solutions reached by each algorithm in terms of both the energy consumption (Figure \ref{fig:1DayBoxPot} (L)) and the thermal discomfort (Figure \ref{fig:1DayBoxPot} (R)) achieved by each method. It is apparent that in each of these plots that xNES is the most consistent with the smallest deviation in its performance on both metrics. What is the most striking feature of these box plots is the low energy consumption produced by the GA trained NN, when considering its high number of instances of thermal discomfort and lower fitness from the convergence graphs in Figure \ref{fig:1DayConverge}. The explanation for this is that the problem of HVAC control is inherently multi-objective when considering both energy consumption and thermal comfort. For instance, if the heat were never switched on, this solution would optimize the energy consumption by reducing it to 0 but would result in a significant amount of thermal discomfort. Conversely if the temperature of the room were maintained within the temperature set point region constantly, the thermal discomfort would be minimized as there would be no period when the room is occupied but the room is still heating up. This would however result in exceedingly high energy consumption. The best solution will therefore result in a trade off between these two objectives. There are many studies in the literature that frame this problem as a multi-objective optimization problem and produce the pareto set of solutions that optimize each objective to varying degrees. This is outside of the scope of this research but would be in interesting direction for future research. Many of the solutions found by the GA trained NN minimize energy consumption to the detriment of thermal comfort. This is reflected by the worse fitness/ reward that the GA converges too. The fitness/reward in Equation \ref{eqn:fit_per_timestep} is only ever 0 when the room is unoccupied and the heat is off or if the room is occupied and the heat is off but the temperature is in the acceptable region. An optimal policy would ideally result in a fitness of 0, however all HVAC controllers fail to achieve this and must therefore either prioritize comfort or energy consumption. Since the purpose of HVAC is to maintain a comfortable temperature for its inhabitants, Equation \ref{eqn:fit_per_timestep} produces the most sever penalties when the controller is not taking actions that move the temperature towards a comfortable region for the inhabitants when occupied. Figure \ref{fig:1DayBoxPot} (R) reveals that the GA trained NN often finds solutions that do not priorities the comfort of the inhabitants and is therefore sub optimal. 

% In terms of thermal comfort, both xNES and DQN have a much smaller spread and a lower mean and median than GA. This is also reflected by the fact that xNES and DQN converge to a higher fitness / reward than GA. The xNES algorithm does however converge to a solution with higher fitness than DQN. Figure \ref{fig:1DayBoxPot} illustrates that xNES is capable of finding solutions with significantly lower energy consumption and thermal discomfort than DQN. When comparing xNES with GA, it is clear that xNES is far more consistent than the GA, with a lower spread on both energy consumption and thermal discomfort. 





% % % Figure environment removed


% % Figure environment removed








% %Month:%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% % % Figure environment removed


% Figure \ref{fig:1MonthConverge} plots the convergence of each algorithm when learning to control the HVAC system over 1 month. This convergence graph depicts similar characteristics as were observed in Figure \ref{fig:1DayConverge} for the 1 day simulation convergence graph. This graph illustrates that DQN provides more rapid initial convergence than either xNES or GA. Similar to Figure \ref{fig:1DayConverge} DQN converges to a policy with lower fitness than xNES. xNES does a significant amount of exploring at the early stage of learning but converges rapidly at approximately 10000 episodes. Conversely the population of the GA algorithm remains diverse, in terms of the fitness of each individual, even at the end of training. However, GA produces solutions with high fitness comparable to xNES. 

% % Due to the computational cost of running the simulations for 1 simulated month, Figure \ref{fig:1MonthConverge} plots the convergence curves for 1 run.

% % The computational costs of each method are outlined in Table \ref{table:CompTime} and will be discussed later.

% % % Figure environment removed


% % Figure environment removed


% Table \ref{table:BestMonthPolicy} outlines the fitness, energy consumption and discomfort over 1 month of the best policy found by each algorithm. From this table, it is clear that xNES performs the best of the three machine learning algorithms evaluated. A HVAC control policy trained with xNES consumes the same amount of energy in a month as a GA, however xNES results in fewer instances of discomfort and therefore receives a higher cumulative fitness. DQN converges to a sub optimal policy when trained on 1 month of simulated HVAC operation. The higher energy and discomfort observed when trained with DQN is due to the algorithm being unable to learn to not turn on the heat at all during the weekend as the room is unoccupied. In order to further gauge the performance of the proposed xNES trained NN for HVAC control, Table \ref{table:BestMonthPolicy} also presents the results of a threshold based HVAC control policy. It is clear from Table \ref{table:BestMonthPolicy} that the threshold based approach performs worse that all other algorithms when evaluating the threshold based policy using the fitness function and the amount of thermal discomfort experienced over the month. The threshold based approach does reduce energy consumption by 0.869\% when compared to the xNES algorithm. This is a poor trade-off when considering the number of instances of thermal discomfort is increased by 46.174\% compared to xNES. Based on these results it is fair to say that the proposed approach xNES NN approach is justified.


% \begin{table}[h]
% \centering
% \caption{Performance of Best HVAC Control Policies: HVAC 1 Month}
% %\makebox[\textwidth][c]{
% \scalebox{1.0}{
% \begin{tabular}{c | c c c}
% \noalign{\smallskip}\hline \noalign{\smallskip}
    
%         % Algorithm   	&	Fitness &   Energy   &   Discomfort 	 \\
%     	   %	&	 &   (Joules)   &    (Minutes)	 \\
    	   	
% 	   	Algorithm   	&	Fitness &   Energy   &   Discomfort 	 \\
% 	   	&	 &   (kWh)   &    (Minutes)	 \\

% 	\noalign{\smallskip}\hline\noalign{\smallskip}
% % xNES	    &	-118415	&	4.3014 $\times 10^{8}$	&	2744	\\
% % GA	        &	-119500	&	4.3014 $\times 10^{8}$  &	2757	\\
% % DQN	        &	-172260	&	5.1306 $\times 10^{8}$	&	3774	\\
% % Threshold	&	-176220	&	4.2642 $\times 10^{8}$	&	4011	\\

% xNES	    &	-118415	&	119.48	&	2744	\\
% GA	        &	-119500	&	119.48	  &	2757	\\
% DQN	        &	-172260	&	142.52	&	3774	\\
% Threshold	&	-176220	&	118.45	&	4011	\\


% \noalign{\smallskip}
% \hline
% \end{tabular}} %}
% \label{table:BestMonthPolicy}
% \end{table}


% Since the number of episodes corresponds to the number of trials that each NN controller has within the simulated environment, the number of trials (approx 10000) required to learn a near optimal policy for both 1 day and 1 month environments are similar. It appears that the ML algorithms need a similar number of trials within the environment irrespective of the length of the trial (1 day vs 1 month). This is both an interesting observation and also particularly relevant to the HVAC industry. It was stated earlier that long training times are one of the primary reasons for the reluctance of the HVAC industry to adopt learning algorithms for HVAC control. This results indicates that training a NN controller on a shorter duration of simulated HVAC operation gives an accurate indication of the number of training episodes required for longer duration HVAC simulations. This could result in computational time savings as the NN can be trained for the minimum number of episodes required to converge for the shorter duration simulation. Future research would investigate the minimum duration of simulated HVAC operation required to achieve satisfactory performance for longer a duration of simulated HVAC time. For example, how does a NN trained for a day or a week of HVAC operation perform when then tested on a month or year of HVAC operation. It is possible that a small duration of simulated HVAC operation could be sufficient to generalize to longer time frames, thus reducing the computational time cost of the training period. This is outside of the scope of this paper however and is deferred to future research. It should also be noted here however that the amount of computational time required for each trial is directly dependent on the length of the trial. For example, 1 month requires 30 times as much time to simulate. 20000 was selected as the number of training episodes to ensure that each algorithm has definitely converged. This does require extra computational time however.









% % Figure \ref{fig:1MonthRTvsOT} illustrates the room temperature (blue plot) vs outside temperature (black plot) after training the NN HVAC controller using xNES. It is apparent that the xNES algorithm is capable of evolving an suitable controller that effectively controls the heater during the 5 weekdays when the building is occupied but does not turn on the heat at the weekend when the building is unoccupied. This strategy is optimal as thermal comfort can be disregarded at the weekend when the building is not occupied. The lower graph in Figure \ref{fig:1MonthRTvsOT} focuses on the final day of the month when the building is occupied and the outside temperature increases to be within the predefined comfort range. As the lower plot illustrates, the controller can identify that the outside temperature is at a sufficiently high temperature that the heat does not need to be turned on. This further highlights the utility and effectiveness of xNES for evolving NN HVAC controllers.

% %% graph that shows zoomed in final day temp %%%

% % Figure environment removed


% Figure \ref{fig:1MonthRTvsOT} illustrates the room temperature (blue plot) vs outside temperature (black plot) after training the NN HVAC controller using xNES. It is apparent that the xNES algorithm is capable of evolving an suitable controller that effectively controls the heater during the 5 weekdays when the building is occupied but does not turn on the heat at the weekend when the building is unoccupied. This strategy is optimal as thermal comfort can be disregarded at the weekend when the building is not occupied. 



% % % Figure environment removed






% % Table \ref{table:CompTime} presents the computational time taken to train each algorithm for 20000 episodes on the HVAC simulator for both 1 day and 1 month of simulated time. It is clear from these results that xNES and GA are much faster than DQN. Both GA and xNES were 63 times faster than DQN when learning to control a HVAC for one day. When applied to 1 month of HVAC simulation, GA was the fastest algorithm to execute (5.5 hours), followed closely by xNES. Again DQN was significantly slower than GA and xNES, which required almost 10 days to execute.


% % In order to provide a fair comparison between each algorithm, these results report each algorithm running on a single CPU core with no multi-threading or GPU use. One of the benefits of evolutionary approaches is that by nature they are easy to implement to run in parallel across multiple cores. This would of course provide computational speed gains. However in order to give a fair comparison of each, the values reported in Table \ref{table:CompTime} correspond to no multi-threading.




% % \begin{table}[h]
% % \centering
% % \caption{Training Computational Cost of Each Algorithm}
% % %\makebox[\textwidth][c]{
% % \scalebox{1.0}{
% % \begin{tabular}{c | c c c}
% % \noalign{\smallskip}\hline \noalign{\smallskip}
    
% %         HVAC   	&	xNES &   GA   &   DQN 	 \\
% %     	Environment   	&	(Hrs) &   (Hrs)   &   (Hrs) 	 \\

% % 	\noalign{\smallskip}\hline\noalign{\smallskip}
% % 1 Day Simulation	    &	0.2	&	0.2	&	12.6	\\
% % 1 Month Simulation	    &  	5.9    &	5.5	&   239	\\

% % \noalign{\smallskip}
% % \hline
% % \end{tabular}} %}
% % \label{table:CompTime}
% % \end{table}


% % \section{Discussion}

% % The purpose of the experiments conducted in the previous section was to demonstrate the competitiveness of evolutionary neural networks, in particular the xNES algorithm, for training HVAC controllers when compared to other learning approaches such as reinforcement learning. Recent previous studies have demonstrated this for toy game playing problems \cite{salimans2017evolution,such2017deep}. The research presented in this paper confirms that the impressive performance observed previously in game playing environments also carries over to the area of building energy management. 


% % This paper presents the application of evolutionary neural networks for HVAC control as a proof of concept without field trials in order to ensure a fair comparison between each of the machine learning algorithms evaluated. Real world trials would consist of a significant amount of environmental perturbations that would make it impossible to evaluate each of the control policies in the same conditions. It would not be possible to determine if a decrease in energy consumption over a period of time would be due to a particular algorithms characteristics or the state of the environment over that period. In addition to this there are very few installed HVAC platforms that would allow a new control to be trialled directly on. 



% % It was stated previously that this is the first study that applies evolutionary neural networks to the problem of HVAC operation. There are many previous studies that apply more general evolutionary methods to the problem of HVAC operation \cite{kusiak2011multi,ghahramani2017hvac,lu2005hvac,kim2016simulation}. None of these previous studies implement evolutionary computing in the same machine learning framework as implemented in this study and none of these studies utilize evolutionary principles to train a neural network for HVAC control. In contrast, this paper uses evolutionary techniques to evolve a neural network controller for the HVAC system. Previous studies often use evolutionary computing to optimize the systems operation in conjunction with some forecasting techniques. The unique contribution of this research lies in the way the evolutionary algorithm is applied to the HVAC control problem. This research optimizes the parameters of a neural network controller that interacts with the HVAC system, similar to reinforcement learning, rather than optimizing the controls of a HVAC system directly, as previous studies have done. This research demonstrates that xNES is a very effective optimization algorithm. Previous studies that have explored applications of evolutionary algorithms for various optimization problems within the area of building energy management could benefit by applying xNES to these problems to achieve more optimal solutions, e.g. when determining set point temperatures for HVAC operation.



% % As was mentioned previously, the problem of HVAC control is inherently a multi-objective control task. There is an unavoidable trade off between energy consumption and thermal comfort. It would be worth investigating in future research how multi-objective learners would perform when learning policies that can account for this trade off and present multiple HVAC configurations that optimize thermal comfort and energy consumption to various degrees. The Multi-Objective Neural Network trained with Differential Evolution (MONNDE) algorithm \cite{mason2018multi} and Pareto Q-Learning algorithm\cite{van2014multi} are examples of such algorithms that would be applicable to this task.



% % As is the case with reinforcement learning, in order for evolutionary neural networks to achieve satisfactory performance when applied to real HVAC systems outside of simulation, an accurate HVAC simulator is of crucial importance. This is a fundamental step when applying machine learning control algorithms to HVAC control. The HVAC simulator implemented in this research is of course a very simple HVAC simulator that does not account for factors such as heat generated by the occupants for example. The purpose of this paper was to conceptually demonstrate the utility of evolutionary neural networks as a HVAC controller. Liu et al. demonstrated that with accurate simulation, it is possible to implement reinforcement learning from a simulated environment to a physical building \cite{liu2006experimental}. Future work would aim to evaluate an xNES trained neural network in a more realistic HVAC simulator and ultimately implement the resulting controller in a physical building.



% \section{Conclusion}



% This study introduces evolutionary neural networks to the problem of HVAC operation. This research evaluated the performance of a neural network controller trained with evolutionary algorithms: xNES and GA. The performance of these evolutionary methods were benchmarked against a state of the art deep reinforcement learning algorithm: DQN. Each of these methods were evaluated for 1 day and 1 month of simulated HVAC operation.

% The primary findings of this research are:
% \begin{enumerate}
%     \item It was demonstrated that the xNES algorithm is an effective algorithm for training a neural network controllers for building energy management. xNES achieved the highest fitness ($-118415$: 1 Month) when compared to both GA ($-119500$: 1 Month) and DQN ($-172260$: 1 Month). The xNES algorithm provides less variance in the optimum policy found than both GA and DQN, and also better convergence. xNES also provides a 31.588 \% decrease in the number of instances of thermal discomfort when compared to a simple threshold based approach, with less than 1\% difference in energy consumption. Evolutionary neural networks also significantly outperform reinforcement learning in terms of energy and thermal discomfort. xNES reduces energy consumption by 16.162\% and thermal discomfort by 27.292\% when compared to DQN.
%     \item Evolutionary algorithms were applied in a novel way for the task of building HVAC control, i.e. to train a neural network controller rather than to directly optimize the problem variables. The xNES trained NN is capable of finding a near optimum solution after approximately 10,000 episodes in the simplified building energy environment evaluated here. In order for this approach to scale to more complex HVAC simulation environments, a larger number of training episodes and a larger neural network will be needed. Previous studies have demonstrated how learning algorithms such as reinforcement learning can be effective control techniques for more complex building simulations \cite{zhang2018deep}. It is expected that with more training episodes and a larger network size, evolutionary neural networks effectively can scale to more complex environments as reinforcement learning has.
%     \item Evolutionary neural networks are a competitive alternative to reinforcement learning for HVAC control. An xNES train neural network achieves a significantly higher fitness when compared to the DQN algorithm .
%     % and executes significantly faster ($\times 63$ for one simulated day of HVAC operation).
    
% \end{enumerate}


% % \subsection{Future Work}
% % There are many directions for future research that have arisen as a result of this research. One such direction would be to apply the xNES algorithm within the multi-objective control framework \cite{mason2018multi} to produce a range of solutions rather than just a singe solution.
% % Additionally, future work would involve testing evolutionary neural networks on a more realistic building simulation, e.g. one that accounts for the heat generated by the occupants. 
% % The ultimate goal would be to implement the trained neural network to control a HVAC system outside of simulation. 



% \bibliographystyle{abbrv}
% \bibliography{references}

% % \bibliographystyle{unsrt}  
% % %\bibliography{references}






\end{document}
