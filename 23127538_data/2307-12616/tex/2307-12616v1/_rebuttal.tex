\input{_constants}
\rebuttal

\documentclass[10pt,twocolumn,letterpaper]{article}
\input{cvpr_header}
\myexternaldocument{_main}
\begin{document}
%% TITLE
\title{CTVIS: Consistent Training for Online Video Instance Segmentation\\
\big(Authors' Response to Review\big)}
\maketitle
\thispagestyle{empty}
\appendix
%%

\noindent\textbf{Initial Paper Ratings}: B/L (\R1), W/A (\R2), W/A (\R3).

% We sincerely thank all the reviewers for their valuable time and thoughtful feedback. All of them acknowledged the high performance of our proposed CTVIS model. \R1 deemed our paper to be easy and reasonable for understanding. Moreover, both \R2 and \R3 found our method to be novel as well as thoroughly motivated. 
% They also give some questions and suggestions, which are replied below.

% All of them acknowledged the novelty, clear-motivation and high performance of our proposed CTVIS model. They also give some questions and suggestions, which are replied to below.
We thank all the reviewers for their comments and suggestions. Overall, they thought our CTVIS method is novel, well-motivated (\R2 and \R3), reasonable and easy to understand (\R1). They also found that CTVIS is very competitive in performance (\R1, \R2 and \R3). 
% Below we answer the questions raised in the review.
Below are our responses to concerns and suggestions raised in the review.

\section{Response to \R1}

\noindent\textbf{Weakness \#1:} \textbf{Inconsistent datasets and backbones in ablation study.} 
Thank you for this question. The ablation study (\ie Section~4.2) used ResNet-50 as the backbone and performed experiments on both YTVIS19 and OVIS, which are actually consistent. 
However, if you meant the experiments in Sections~4.3 and 4.4, where YTVIS21 is used, instead of YTVIS19, is because the \emph{hand} class in YTVIS19 conflicts with COCO's \emph{person} class (note that YTVIS21 removed the \emph{hand} class). 
Also, in Section~{4.3}, to allow a fair comparison with MinVIS (which only reports the results 
% of the model trained on 
by training the model on
sampling frames using Swin-L), we employed Swin-L.  We will clarify these points in the new version. 

\noindent\textbf{Weakness \#2:} \textbf{Add a column about data augmentation strategy in Table~1.}
It is worth noting that \textbf{NO} extra augmentations (compared with SOTA models) were adopted to achieve our reported results. However, we think it would be helpful to add the suggested column if the space permits.

\noindent\textbf{Weakness \#3:} \textbf{Inconsistent results in Tables~1 and 5.}
As pointed out in L578, L741 and L785, the two versions are trained (YTVIS21 \vs COCO) and tested (YTVIS21 \vs YTVIS21$^*$) on different benchmarks. To be specific, $50.1$ in Table~1 is obtained by training jointly on COCO and YTVIS21, under standard supervision, enabling fair comparison with other fully-supervised methods. The 49.7 in Table 5 is obtained on YTVIS21$^*$,  
% with the model
where the model was \emph{trained on pseudo videos}. Please refer to Section~4.3 for more details. 
% (we kindly invite the reviewer to read Section~4.3 again).

\noindent\textbf{Additional comments with respect to codes and models.} 
Yes, we will definitely release codes and models upon the acceptance of this work.

\section{Response to \R2}

\noindent\textbf{Weakness \#1:} \textbf{Implementation details of extending vanilla IDOL to Multiple Reference Frames.}
We extended vanilla IDOL to multi-reference IDOL in a straightforward way. Specifically, we randomly sample $L-1$ reference frames surrounding the key frame, forming a training clip of $L$-frames (note that CTVIS uses clips of $L$-frames as well). Our new version will include the details for this extension. 


\noindent\textbf{Weakness \#2:} \textbf{Report the training GPU time and inference FPS.}
Vanilla IDOL is faster than CTVIS. For training (batch size is 1), the throughput of IDOL is 3.3 samples per second, while the throughput of CTVIS is 1.3. 
% add
% However, the training burden of CTVIS arises chiefly from the incorporation of more frames. 
% Therefore, faster training speed can be achieved by shortening the input clip length at the cost of decreasing AP (as shown in Figure 4).
In terms of inference (using ResNet-50 as the backbone, tested on 1 piece of GeForce RTX 3090, and the batch size is 1), CTVIS runs at 13.9 frames per second, which is 
% marginally worse
negligibly slower than IDOL's 14.3. However, CTVIS notably outperforms IDOL by 5\% in terms of AP. 
% We will put these results in our new version.
As suggested, the details and comparison to IDOL on GPU training time and inference FPS will be added in our next version.  

\noindent\textbf{Additional comments: Some minor typos.} 
Thank you for your meticulous review. We will definitely fix these typos.

\section{Response to \R3}

\noindent\textbf{Weakness \#1:} \textbf{Evaluate how incorporating noise helps reduce ID switch.} 
Thanks for your question and we agree that evaluating the occurrence of ID switches is necessary to understand the contribution of noise better.
%We think this question is insightful, and your idea on evaluating the occurrence of ID switches is inspiring. 
However, at this stage, we have a tight budget for time and GPU resources, and would like to try this (need to train at least two models) in the future. We also agree that the results in Table~3 cannot directly verify that adding noise can reduce the ID switch, as the performance gain may also come from some other aspects. 
% However, our Figure~5 show
Intuitively, Figure 5 shows that the IDs of a fish (the left example) and a dog (the right example) switched when using VITA and IDOL, but CTVIS does not have this trouble (or can recover the tracking ID). 

\noindent\textbf{Weakness \#2:} \textbf{Missing performance without using COCO joint training.} 
We didn't mention the results due to the fact that all (nearly) SOTA methods (\eg VITA, SeqFromer and IDOL) use COCO joint training as default (as reported in Table~1).
% This result is missing because almost all SOTA methods just use COCO joint training (included in Table~1). 
Moreover, CTVIS (ResNet-50 as the backbone) trained without COCO data achieves 49.3 on AP, close to 50.1 with COCO joint training.

\noindent\textbf{Weakness \#3:} \textbf{The comparison of FPS.} 
Please refer to our reply to \R2's Weakness \#2.

\noindent\textbf{Additional comments:} \textbf{The limitation and failure cases of the proposed method are not discussed.} 
This question is inspiring. We admit that CTVIS has its own limitations. For example, it introduces extra computation in order to maintain the memory bank during training. Moreover, as CTVIS is a training strategy, its performance 
% lower-bounded by 
heavily depends on the performance of the base frame-wise segmentor (\eg Mask2Former). 
% As a result, it can fail when the frame-wise segmentation is bad. 
Hence, CTVIS is liable to go wrong if the base segmentor cannot provide decent segmentation results. We will add a discussion on this.

\end{document}