\section{Related Work}
\label{sec:related}

\noindent\textbf{Online VIS Method} \cite{masktrackrcnn, crossvis, minvis, idol, stc} typically builts upon image-level instance segmentation models \cite{maskrcnn, mask2former, deformabledetr, detr, isda}. MaskTrack R-CNN \cite{masktrackrcnn} extends Mask R-CNN \cite{maskrcnn} by incorporating an additional tracking head, which associates instances across videos using heuristic cues. CrossVIS \cite{crossvis} proposes to guide the segmentation of the current frame by the features extracted from previous frames. With the emergence of query-based instance segmentors \cite{deformabledetr, detr, mask2former}, matching with query embeddings instead of hand-designed rules boosts the performance of online VIS \cite{idol, minvis}. Utilizing the temporal consistency of intra-frame instance queries predicted by the image-level segmentor \cite{mask2former, deformabledetr}, MinVIS \cite{minvis} tracks instances by Hungarian matching of the corresponding queries frame by frame without video-based training. IDOL \cite{idol} supervises the matching between instances that appeared within two adjacent frames during training. During inference, IDOL maintains a memory bank to store instance momentum averaged embeddings detected from previous frames, which are employed to match with newly detected foreground instance embeddings. 
Concurrent work GenVIS \cite{genvis} applies a query-propagation framework to bridge the gap between training and inference in online or semi-online manners. 
Different from previous approaches, CTVIS aims to absorb ideas from the inference stage of online methods and learn more robust and discriminative instance embeddings during training. 

% $$ YKN: %while our CTVIS aims at obtaining discriminative instance embeddings through the proposed consistent training.
% In this paper, we propose to align the training-inference pipelines to learn more robust and discriminative instance embeddings. To this end, our CTVIS shares the fundamental online VIS architectures with MinVIS and IDOL, with modifications applied to fit the proposed learning paradigm. 

\noindent\textbf{Offline VIS Method} \cite{vistr, ifc, mask2formervis, vita, seqformer} takes as input the entire video and predicts masks for all frames in a single run. 
VisTR \cite{vistr} utilises clip-level instance features as input and predicts clip-level mask sequences in an end-to-end manner. Subsequently, several follow-up works, such as Mask2Former-VIS \cite{mask2formervis}, and SeqFormer \cite{seqformer}, exploit attention \cite{transformer} to process spatio-temporal features and directly predict instance mask sequences. To mitigate the memory consumption on extremely long videos, VITA \cite{vita} proposes to decode video object queries from sparse frame-level object tokens instead of dense spatio-temporal features.

\noindent\textbf{Discriminative Instance-Level Feature Learning.} The discrimination of instance embeddings plays a vital role in instance-level association tasks. Most works absorb the ideas from contrastive learning in self-supervised representation learning. IDOL \cite{idol} and QDTrack \cite{qdtrack} supervise the learning of contrastive instance representations between two adjacent frames. SimCLR \cite{simclr} argues that contrastive learning can benefit from larger batches. Inspired by this, CTVIS introduces long video training samples instead of key-reference image pairs, which leads to more robust instance embeddings. 

\noindent\textbf{VIS Model Training with Sparse Annotations.} Annotating masks for each object instance in every frame and linking them across the video is prohibitively expensive. 
Furthermore, recent works \cite{minvis, nguyen, qdtrack} suggest that the dense video annotations for VIS are unnecessary.  
MinVIS\cite{minvis} makes a per-frame image-level segmentation and associates the generated instance queries to obtain the video-level results. Since the training of the MinVIS model is agnostic to the temporal association of masks, it can benefit from the availability of large-scale datasets for image-level instance segmentation\cite{mscoco}. 
QDTrack \cite{qdtrack} learns compelling instance similarity using pairs of transformed views of images. 
MS COCO \cite{mscoco}, which contains abundant image-level mask annotations, is typically taken to supplement the training of models for VIS \cite{seqformer, idol, vita}. 
Following this, we propose to train VIS models with pseudo-videos generated by augmenting images instead of natural videos. We show that CTVIS models trained on pseudo-videos can surpass SOTA models \cite{seqformer, idol, vita, mask2formervis, masktrackrcnn, ifc} trained with densely annotated videos by clear margins. Different from techniques taking augmentation to enrich the training set \cite{detr, copypaste, qdtrack}, we use augmentation to create the set, which contains pseudo-videos and the associated mask annotations (as well as their spatio-temporal tracks). Moreover, we carefully design the video generation routines based on classical augmentation techniques (%\, 
i.e. \emph{rotation}, \emph{crop} and \emph{copy\&paste}), such that the pseudo-videos are realistic and can cover VIS challenges (including \emph{object occlusion}, \emph{fast-motion}, \emph{re-identification} and \emph{deformation}).
