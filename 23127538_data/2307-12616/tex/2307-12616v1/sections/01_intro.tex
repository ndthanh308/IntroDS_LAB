\section{Introduction}
\label{sec:intro}

% Figure environment removed

Video instance segmentation is a joint vision task involving classifying, segmenting, and tracking interested instances across videos \cite{masktrackrcnn}. It is critical in many video-based applications, such as video surveillance, video editing, autonomous driving, augmented reality, \etc. Current mainstream VIS methods \cite{crossvis, masktrackrcnn, mask2formervis, idol, ifc, minvis, vita, vistr, seqformer} can be categorized into offline and online groups. The former \cite{ifc, mask2formervis, seqformer, vita, vistr} segments and classifies all video frames simultaneously and makes the instance association in a single step. The latter \cite{crossvis, idol, masktrackrcnn, minvis} takes as input a video in a frame-by-frame fashion, detecting and segmenting objects per frame while associating instances across time. In this paper, we focus on the online branch.

Online methods are typically built upon image-level instance segmentation models\cite{maskrcnn, mask2former, deformabledetr, fcos}. 
Several works \cite{masktrackrcnn, ovis, lsrf} utilize convolution-based instance segmentation models to segment each frame and associate instances by incorporating heuristic clues, such as mask-overlapping ratios and the similarity of appearance. 
However, these hand-designed approaches always fail to tackle complicated cases, which typically include severe target occlusion, deformation and re-identification. 
Recently, encouraged by the thriving of Transformer-based \cite{transformer} architectures in object detection and segmentation \cite{detr, deformabledetr, mask2former}, a bunch of query-based online frameworks have been proposed \cite{idol, minvis}, which take advantage of the temporal consistency of query embeddings and associate instances by linking corresponding query embeddings frame by frame. These advances boost the performance of online VIS models, which become de-facto leading VIS performance on most benchmarks (especially on challenging ones such as OVIS \cite{ovis}). 

Though the importance of the discrimination of query embeddings to associate instances has been nominated \cite{idol, minvis}, less research attention has been paid in this vein. 
% KY: MODIFY
MinVIS \cite{minvis} simply trains a single-frame segmentor, and the quality of its query embedding is hampered by the segmentor originally proposed for image-based instance segmentation. 
% MinVIS \cite{minvis} incorporates Mask2Former \cite{mask2former} to merely train a single-frame segmentor, and the quality of its query embedding is hampered by Mask2Former \cite{mask2former} proposed for image-based instance segmentation. 
As shown in Figure~\ref{fig:teaser}(a), recent methods \cite{idol, stc} merely supervise instance embedding generation between two temporally adjacent frames with thein contrastive losses computed upon contrastive items.
Specifically, for each instance at the key frame, if the same instance appears on the reference frame, the embedding of it is selected as the anchor embedding $\mathbf{v}$. Meanwhile, its embedding in the reference frame is taken as the positive embedding $\mathbf{k}^+$, and the embeddings of other instances in the reference frame are used as the negative embeddings $\mathbf{k}^-$. In convention the set $\{\mathbf{v}, \mathbf{k}^+, \mathbf{k}^-\}$ is called \textbf{contrastive item} (CI). 
This training paradigm is \textit{inconsistent} with the inference (shown in the right of Figure~\ref{fig:teaser}), as it overlooks the interaction with the long-term memory bank to construct contrastive items and lacks modelling for long videos. To bridge this gap, we propose CTVIS (as shown in Figure~\ref{fig:teaser}(b)), which intuitively brings in useful tactics from inference, including memory bank, momentum-averaged (MA) embedding and noise training. 
Specifically, CTVIS samples several frames from a long video to form one training sample. Then we process each sample frame by frame, which can produce abundant CIs. Moreover, we sample momentum-averaged (MA) embeddings from the memory bank to create positive and negative embeddings. Furthermore, we introduce noise training for VIS, incorporating a few noises into the memory bank updating procedure to simulate the tracking failure scenarios during the inference process. 
%embeddings into positive embeddings, \eg we set the embedding of the cat at the $3rd$ frame as one of the positive embeddings of the dog at the $5th$ frame. 

% As shown in Figure~\ref{fig:teaser} (a), IDOL supervises the instance embedding between two temporally adjacent frames with contrastive losses computed upon contrastive items 
% KY: Modify
% Specially, for each instance at the key frame, if the same instance appears on the reference frame, the embedding of it is selected as the anchor embedding $\mathbf{v}$. Meanwhile, the same ID embedding in the reference frame is taken as the positive embeddings $\mathbf{k}^+$, and all the rest embeddings in the reference frame are used as the negative embeddings $\mathbf{k}^-$. By convention the set $\{\mathbf{v}, \mathbf{k}^+, \mathbf{k}^-\}$ is called \textbf{contrastive item} (CI).
%Specially, for the dog at the key frame, the embedding of it is selected as the anchor embedding $\mathbf{v}$. Meanwhile, embedding the same dog in the reference frame is taken as the positive embeddings $\mathbf{k}^+$, and all the rest embeddings in the reference frame are used as the negative embeddings $\mathbf{k}^-$. By convention the set $\{\mathbf{v}, \mathbf{k}^+, \mathbf{k}^-\}$ is called \textbf{contrastive items} (CIs).
% KY Modify
% We argue the construction of contrastive items directly influences the discrimination of instance embedding. As shown in 

% We argue the construction of contrastive items directly influences the discrimination of instance embedding. 
% In this paper, we aim to obtain prime contrastive items for online VIS. To this end, our fundamental idea is to make the construction of CIs for training and inference aligned with each other. As shown in Figure~\ref{fig:teaser} (b), CTVIS samples several frames from a long video to form one training sample. Following the pipeline of inference, we process each sample frame by frame, which is able to produce abundant CIs. Moreover, we sample momentum-averaged (MA) embeddings from the memory bank to create positive and negative embeddings. To relieve the issue of ID drift during inference, we introduce noise training for VIS, which incorporates a few noise embeddings into positive embeddings, \eg we set the embedding of the cat at the $3rd$ frame as one of the positive embeddings of the dog at the $5th$ frame. 

We also consider the availability of large-scale training samples, which are especially expensive to annotate and maintain for VIS. To tackle this, we implement and test several goal-oriented augmentation methods (to align with the distribution of real data) to produce pseudo-videos. Different from the COCO joint training, we only use pseudo-videos to train VIS models. 

Without bells and whistles, CTVIS outperforms the state-of-the-art by large margins on all benchmark datasets, including YTVIS19 \cite{masktrackrcnn}, YTVIS22 \cite{masktrackrcnn}, and OVIS \cite{ovis}. Even trained with pseudo-videos only, CTVIS surpasses fully supervised VIS models \cite{idol, seqformer, vita}. Here we summarize our key contributions as

% \begin{itemize}
%     % modify
%     \item We propose a simple yet effective training framework (CTVIS) for online VIS. CTVIS promotes the discriminative ability of the instance embedding by 
%     interacting with long-term memory bank to build CIs, 
%     % extending the temporal range of CI scanning, 
%     and by introducing noise into memory bank updating procedure.

%     \item We propose to create pseudo-VIS training samples by augmenting still images and their mask annotations. CTVIS models trained with pseudo-data only surpass their fully-supervised opponents already, which suggests that it is a desirable choice especially when dense temporal mask annotations are limited.
    
%     \item Equipped with the introduced consistent training strategy, the state-of-the-art (SOTA) VIS models achieve remarkable performance gains, and outperform their original results on three widely-adopted benchmarks. we also conduct an ablation to validate the effectiveness of each building block of CTVIS.
% \end{itemize}

\begin{list}{\labelitemi}{\leftmargin=1em}
    \setlength{\topmargin}{0pt}
    \setlength{\itemsep}{0em}
    \setlength{\parskip}{0pt}
    \setlength{\parsep}{0pt}
    \item We propose a simple yet effective training framework (CTVIS) for online VIS. CTVIS promotes the discriminative ability of the instance embedding by 
    interacting with long-term memory banks to build CIs, 
    % extending the temporal range of CI scanning, 
    and by introducing noise into the memory bank updating procedure.

    \item We propose to create pseudo-VIS training samples by augmenting still images and their mask annotations. CTVIS models trained with pseudo-data only surpass their fully-supervised opponents already, which suggests that it is a desirable choice, especially when dense temporal mask annotations are limited.

    \item CTVIS achieves impressive performance on three public datasets. Meanwhile, extensive ablation validates the method's effectiveness.
    
    % \item Equipped with the introduced consistent training strategy, the SOTA VIS models achieve remarkable performance gains, and outperform their original results on three widely-adopted benchmarks. We also conduct an ablation to validate the effectiveness of each building block of CTVIS.
  \end{list}