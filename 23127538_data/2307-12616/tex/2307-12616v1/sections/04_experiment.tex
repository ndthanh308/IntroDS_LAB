\section{Experiment}
\label{sec:experiment}

\noindent\textbf{Datasets.} 
The proposed methods are evaluated on three VIS benchmarks: YTVIS19\cite{masktrackrcnn}, YTVIS21\cite{masktrackrcnn} and OVIS\cite{ovis}.

% We provide more dataset details in the supplementary material.
% YTVIS19 covers 40 object classes and contains 2,238/302 videos for training/validation. YTVIS21 retains the number of categories of YTVIS19 while expanding the datasets to 2,985/421 videos for training/validation and improving the instance annotation quality. OVIS has 25 object classes and contains 607/140 videos for training/validation. While the number of videos is less, each OVIS video includes more frames (69.4 frames on average) than YTVIS19 (27.6 frames) and YTVIS21 (30.2 frames). 
% Moreover, OVIS samples typically involve more instances and severe occlusion and thus are more challenging.

\begin{table*}[!htbp]
\vspace{-2mm}
  \centering
    \resizebox{0.95\textwidth}{!}{%
    \begin{tabular}{c|c|c|ccccc|ccccc|ccccc}
    \toprule
    \multicolumn{2}{c|}{\multirow{2}[1]{*}{Methods}} & \multirow{2}[1]{*}{\shortstack{Params.}} & \multicolumn{5}{c|}{YTVIS19\cite{masktrackrcnn}}                               & \multicolumn{5}{c|}{YTVIS21\cite{masktrackrcnn}}                               & \multicolumn{5}{c}{OVIS\cite{ovis}} \\
    \multicolumn{2}{c|}{}     &             & AP          & AP$_{\mathtt{50}}$        & AP$_{\mathtt{75}}$        & AR$_{\mathtt{1}}$         & AR$_{\mathtt{10}}$        & AP          & AP$_{\mathtt{50}}$        & AP$_{\mathtt{75}}$        & AR$_{\mathtt{1}}$         & AR$_{\mathtt{10}}$        & AP          & AP$_{\mathtt{50}}$        & AP$_{\mathtt{75}}$        & AR$_{\mathtt{1}}$         & AR$_{\mathtt{10}}$ \\
    \midrule
    \multirow{11}[2]{*}{\begin{sideways}ResNet-50\cite{resnet}\end{sideways}} & MaskTrack R-CNN\cite{masktrackrcnn} & -           & 30.3        & 51.1        & 32.6        & 31          & 35.5        & 28.6        & 48.9        & 29.6        & 26.5        & 33.8        & 10.8        & 25.3        & 8.5         & 7.9         & 14.9 \\
                & SipMask\cite{sipmask}     & -           & 33.7        & 54.1        & 35.8        & 35.4        & 40.1        & 31.7        & 52.5        & 34          & 30.8        & 37.8        & 10.2        & 24.7        & 7.8         & 7.9         & 15.8 \\
                & CrossVIS\cite{crossvis}    & -           & 36.3        & 56.38       & 38.9        & 35.6        & 40.7        & 34.2        & 54.4        & 37.9        & 30.4        & 38.2        & 14.9        & 32.7        & 12.1        & 10.3        & 19.8 \\
                & IFC\cite{ifc}         & -           & 41.2        & 65.1        & 44.6        & 42.3        & 49.6        & 35.2        & 55.9        & 37.7        & 32.6        & 42.9        & 13.1        & 27.8        & 11.6        & 9.4         & 23.9 \\
                & Mask2Former-VIS\cite{mask2formervis} & 44          & 46.4        & 68          & 50          & -           & -           & 40.6        & 60.9        & 41.8        & -           & -           & 17.3        & 37.3        & 15.1        & 10.5        & 23.5 \\
                & TeViT\cite{tevit}       & -           & 46.6        & 71.3        & 51.6        & 44.9        & 54.3        & 37.9        & 61.2        & 42.1        & 35.1        & 44.6        & 17.4        & 34.9        & 15          & 11.2        & 21.8 \\
                & SeqFormer\cite{seqformer}   & 48        & 47.4        & 69.8        & 51.8        & 45.5        & 54.8        & 40.5        & 62.4        & 43.7        & 36.1        & 48.1        & 15.1        & 31.9        & 13.8        & 10.4        & 27.1 \\
                & MinVIS\cite{minvis}      & 44          & 47.4        & 69          & 52.1        & 45.7        & 55.7        & 44.2        & 66          & 48.1        & 39.2        & 51.7        & 25          & 45.5        & 24          & 13.9        & 29.7 \\
                & IDOL\cite{idol}        & \textbf{43}        & 49.5        & \underline{74}          & 52.9        & 47.7        & 58.7        & 43.9        & \underline{68}          & \underline{49.6}        & 38          & 50.9        & \underline{30.2}        & \underline{51.3}        & \underline{30}          & \underline{15}          & \underline{37.5} \\
                & VITA\cite{vita}        & 57        & \underline{49.8}        & 72.6        & \underline{54.5}        & \underline{49.4}        & \underline{61}          & \underline{45.7}        & 67.4        & 49.5        & \underline{40.9}        & \underline{53.6}        & 19.6        & 41.2        & 17.4        & 11.7        & 26 \\
                & \textbf{CTVIS (Ours)} & \underline{44}          & \textbf{55.1} & \textbf{78.2} & \textbf{59.1} & \textbf{51.9} & \textbf{63.2} & \textbf{50.1} & \textbf{73.7} & \textbf{54.7} & \textbf{41.8} & \textbf{59.5} & \textbf{35.5} & \textbf{60.8} & \textbf{34.9} & \textbf{16.1} & \textbf{41.9} \\
    \midrule
    \multirow{6}[1]{*}{\begin{sideways}Swin-L \cite{swin}\end{sideways}} 
    & SeqFormer\cite{seqformer}   & 219       & 59.3        & 82.1        & 66.4        & 51.7        & 64.6        & 51.8        & 74.6        & 58.2        & 42.8        & 58.1        & -           & -           & -           & -           & - \\
                & Mask2Former-VIS\cite{mask2formervis} & 216       & 60.4        & 84.4        & 67          & -           & -           & 52.6        & 76.4        & 57.2        & -           & -           & 25.8        & 46.5        & 24.4        & 13.7        & 32.2 \\
                & MinVIS\cite{minvis}      & 216         & 61.6        & 83.3        & 68.6        & 54.8        & 66.6        & 55.3        & 76.6        & 62          & 45.9        & 60.8        & 39.4        & 61.5        & 41.3        & \underline{18.1}        & 43.3 \\
                & VITA\cite{vita}        & 229       & 63          & 86.9        & 67.9        & \underline{56.3}        & 68.1        & \underline{57.5}        & 80.6        & 61          & \underline{47.7}        & \underline{62.6}        & 27.7        & 51.9        & 24.9        & 14.9        & 33 \\
                & IDOL\cite{idol}        & \textbf{213}       & \underline{64.3}        & \underline{87.5}        & \underline{71}          & 55.5        & \underline{69.1}        & 56.1        & \underline{80.8}        & \underline{63.5}        & 45          & 60.1        & \underline{42.6}        & \underline{65.7}        & \underline{45.2}        & 17.9        & \underline{49.6} \\
                & \textbf{CTVIS (Ours)} & \underline{216}         & \textbf{65.6} & \textbf{87.7} & \textbf{72.2} & \textbf{56.5} & \textbf{70.4} & \textbf{61.2} & \textbf{84} & \textbf{68.8} & \textbf{48} & \textbf{65.8} & \textbf{46.9} & \textbf{71.5} & \textbf{47.5} & \textbf{19.1} & \textbf{52.1} \\
    \bottomrule
    \end{tabular}%
    }
    \vspace{-1mm}
    \caption{Compare CTVIS with SOTA methods. The best and second best are highlighted by \textbf{bold} and \underline{underlined} numbers, respectively.
    %CTVIS models outperform SOTA approaches by clear margins, and achieve leading results on YTVIS19, YTVIS21 and OVIS.
    }
    \label{tab:main}%
    \vspace{-3mm}
\end{table*}%

\noindent\textbf{Metrics.}
Following prior studies \cite{idol, minvis, mask2formervis, masktrackrcnn, crossvis, ifc, seqformer, vita}, we use Average Precision (AP) and Average Recall (AR) as the evaluation metrics. 
% The intersection-over-union (IoU) thresholds, which range from 50\% to 95\% with a 5\% resolution, are used to calculate AP. 
%To be concrete, the AP and AR are first computed for each object class. Then they are averaged across all classes to get the final results. 
% Unless otherwise specified, all metrics' results are calculated using the open source platform \cite{codalab}. 
%Furthermore, we report the median of three runs to relieve the influence of random seeds.

\noindent\textbf{Implementation Details.}
%Our codebase is built upon PyTorch \cite{pytorch} and Detectron2 \cite{detectron2}. 
For the hyper-parameters of Mask2Former\cite{mask2former}, we just use its officially released version. The number of layers of the instance embedding head is 3. All models are initialized with parameters pre-trained on COCO \cite{mscoco}, and then they are trained on 8 NVIDIA A100 GPUs. Following prior works \cite{seqformer, vita, genvis},  we use the COCO joint training (CJT) setting to train our models unless otherwise specified. We set the lengths of training videos as 8 and 10 for YTVIS19\&21 and OVIS, respectively. For data augmentation, we use clip-level random crop and flip. During the training phase, we resize the input frames so that the shortest side is at least 320 and at most 640p, while the longest side is at most 768p. During inference, the input frames are downsampled to 480p. We set $\lambda_{\text{emb}}$, $\lambda_{\text{cls}}$, $\lambda_{\text{ce}}$, $\lambda_{\text{dice}}$ as 2.0, 2.0, 5.0 and 5.0, respectively. The mini-batch size is 16 and the maximum training iterations is 16,000. The initial learning rate is 0.0001 and decays at 6,000 and 12,000 iterations, respectively. 

\subsection{Main Result}
As shown in Table~\ref{tab:main}, we compare CTVIS against SOTA methods \cite{masktrackrcnn, sipmask, crossvis, ifc, mask2formervis, tevit, seqformer,minvis,idol,vita}, respectively using ResNet-50 \cite{resnet} and Swin-L \cite{swin} as the backbone on three benchmarks.

\noindent\textbf{YTVIS19 \& YTVIS21.} consist of relatively simple videos with short durations. Thanks to the introduced consistent learning paradigm and the extracted discriminative embeddings, CTVIS outperforms recent best methods on AP by $\sim5\%$ with ResNet-50 on both benchmarks. With the stronger backbone Swin-L, CTVIS surpasses the second best by $3.7\%$ on YTVIS21. Compared with IDOL\cite{idol}, CTVIS considerably improves the performance in terms of all metrics with tolerable parameter overheads. 

\noindent\textbf{OVIS.} This dataset contains longer videos and more intricate contents, on which online methods \cite{idol, minvis} perform much better than offline models \cite{vita, mask2former, seqformer}. 
Thanks to the effective embedding learning with long video samples, CTVIS gains $5.3$ and $4.3$ points in terms of AP, taking as inputs ResNet-50 and Swin-L, respectively. To summarize, CTVIS is highly competitive on benchmarks with varying complexities.

\subsection{Ablation Study}
We conduct extensive ablation to verify the effectiveness of CTVIS. Unless specified otherwise, we take the ResNet-50 as the backbone and train models under the CJT setting. Here we report AP$^{\mathtt{YV19}}$ and AP$^{\mathtt{OVIS}}$ on YTVIS19 and OVIS.

\noindent\textbf{Do improvements mainly come from better image-level instance segmentation models?} 
The answer is no. We validate this in Table~\ref{tab:detector}: 
1) Compared with IDOL with Deformable DETR, IDOL with Mask2Former is 1.0 and 1.5 points higher, suggesting the influence of a better detector is not that significant; 
2) Since our CTVIS is not restricted to a specific network, we implement Deformable DETR with CTVIS, which brings 4.2  and 3.6 points of AP gains. Similarly, CTVIS on Mask2Former also boosts the results by 3.9 and 3.8 points, which indicates that the improvements mainly come from our proposed CTVIS. 

\begin{table}[t]
\vspace{-2mm}
  \centering
  \resizebox{0.9\columnwidth}{!}{%
    \begin{tabular}{c|ll|ll}
    \toprule
    \multirow{2}[2]{*}{Methods} & \multicolumn{2}{c|}{Deformable DETR$^*$ \cite{idol}} & \multicolumn{2}{c}{Mask2Former \cite{mask2former}} \\
                           & AP$^{\mathtt{YV19}}$     & AP$^{\mathtt{OVIS}}$        & AP$^{\mathtt{YV19}}$     & AP$^{\mathtt{OVIS}}$ \\
    \midrule
    IDOL\cite{idol}        & 49.5        & 30.2        & 51.2        & 31.7 \\
    CTVIS                  &  \textbf{53.7} \blue{(+4.2)}        &  \textbf{33.8} \blue{(+3.6)}        &  \textbf{55.1} \blue{(+3.9)}        &  \textbf{35.5} \blue{(+3.8)} \\
    \bottomrule
  \end{tabular}%
  }
  \vspace{-1mm}
  \caption{Comparison of different instance segmentation methods with IDOL and CTVIS, respectively. 
  Deformable DETR$^*$ is extended to instance segmentation as suggested in \cite{idol}. }  
  \label{tab:detector}%
  \vspace{-1mm}
\end{table}%

% Figure environment removed 

\noindent\textbf{Long-video training.}
To verify the effectiveness of long-video training, we ablate the number of frames of each video used for training. For a fair comparison, we extend IDOL\cite{idol} to a multiple references (MR) version, by replacing its segmentor with the stronger Mssk2Former and using multiple reference frames.  Figure~\ref{fig:ablate_num_frame} shows the results. Thanks to the CI construction method employed by CTVIS, the performance has seen a dynamic increase by using more frames (peaked at 8 and 10 frames). In comparison, MR cannot benefit from long-video training and even degrades on OVIS. Hence we conclude that the performance of CTVIS stems from the effective video-level embedding learning (for tracking), rather than training an enhanced instance segmentor with larger batch sizes (more images per batch).

\noindent\textbf{Components of CTVIS.} First, removing all components of CTVIS sets a baseline, which utilizes a single reference to learn embeddings in a frame-by-frame way. As shown in Table~\ref{tab:ctvis}, the baseline gets 51.6 and 32.6 on YTVIS19 and OVIS. Based on this baseline, we gradually add CTVIS components: 1) We take the latest embedding of each instance to build CIs (instead of MA embeddings), which improves AP$^{\mathtt{YV19}}$ and AP$^{\mathtt{OVIS}}$ to 52.1 and 33.3. This suggests that the sampling domain CIs do indeed influence the instance embedding learning; 2) When MA is incorporated, the results see salient increases (52.1 \vs 54.2 and 33.3 \vs 34.9), which indicates that our CI-building method renders the embedding learning more stable and consistent; 3) When incorporating noise in the memory bank, which is designed to alleviate the ID switch issue, the performance sees non-trivial increases (0.9 and 0.6 on two datasets). Put all components together, CTVIS obtains remarkable results on both datasets and outperforms the strong baseline by 3.5 and 2.9 points, which validates the significance of the temporal alignment between training and inference pipelines, at least for VIS. 

\noindent\textbf{Sampling of $\mathbf{k}^-$.} 
% Recall that CTVIS samples major embedding from the MA embedding from the memory bank and supplementary embeddings from background embeddings from previous frames.  
We test different ways of building the negative embeddings $\mathbf{k}^-$. Table~\ref{tab:negative} presents four configurations and the corresponding results. Recall that the supplementary negative embeddings represent the background, and training with such negative samples only corrupts the performance (the 1st row). On the other hand, using major negative samples only gives decent results. A conjunctive usage of both negative-sampling types improves the performance significantly. In this line, we further consider sampling supplementary negative instances from either the local (sampled from the preceding frame only) or global domain (sampled from all previous frames). We found that the local setting gives the best results. This is probably because the model only needs to check the background in the local domain during inference. Hereafter we simply use the local setting.



% \begin{table*}[t]
%     \vspace{-.2em}
%     \centering
%   % \resizebox{0.3\columnwidth}{!}{%
%     \subfloat[
%     \textbf{Effect of the Components of CTVIS.}
%     \label{tab:ctvis}
%     ]{
%     \begin{minipage}{0.29\linewidth}{\begin{center}
%     \centering
%     \begin{tabular}{ccc|cc}
%     \toprule
%     Memory Bank & Momentum & Noise Training & AP$^{\mathtt{YV19}}$ & AP$^{\mathtt{OVIS}}$     \\
%     \midrule
%                      &                  &                  & 51.6  & 32.6                     \\
%     $\checkmark$     &                  &                  & 52.1  & 33.3                     \\
%     $\checkmark$     & $\checkmark$     &                  & 54.2  & 34.9                     \\
%     $\checkmark$     & $\checkmark$     & $\checkmark$     & \textbf{55.1}  & \textbf{35.5}   \\
    
%     \bottomrule
%     \end{tabular}%
%     \end{center}}\end{minipage}
%     }
%     \hspace{2em}
%     \subfloat[
%     \textbf{Ablation study of negative embeddings sampling strategy. }
%     \label{tab:negative}%
%     ]{
%     \begin{minipage}{0.29\linewidth}{\begin{center}
%     \centering
%     \begin{tabular}{cc|cc}
%     \toprule
%     Major       & Supplementary          & AP$^{\mathtt{YV19}}$ & AP$^{\mathtt{OVIS}}$ \\
%     \midrule
%                 & $\checkmark$           &   16.5      &  0.5  \\
%     $\checkmark$           &             &   50.8	   &  31.6 \\
%     $\checkmark$           & global      &   54.6      &  33.4 \\
%     $\checkmark$           & local       &   55.1      &  35.5 \\
%     \bottomrule
%     \end{tabular}%
%     \end{center}}\end{minipage}
%   }
%   % }
%   % \caption{\textbf{Effect of the Components of CTVIS.}}
%   % \label{tab:ctvis}
% \end{table*}%

\begin{table}[t]
\vspace{-2mm}
  \centering
  \resizebox{0.85\columnwidth}{!}{%
    \begin{tabular}{ccc|cc}
    \toprule
    Memory bank & Momentum & Noise & AP$^{\mathtt{YV19}}$ & AP$^{\mathtt{OVIS}}$     \\
    \midrule
                     &                  &                  & 51.6  & 32.6                     \\
    $\checkmark$     &                  &                  & 52.1  & 33.3                     \\
    $\checkmark$     & $\checkmark$     &                  & 54.2  & 34.9                     \\
    $\checkmark$     & $\checkmark$     & $\checkmark$     & \textbf{55.1}  & \textbf{35.5}   \\
    
    \bottomrule
    \end{tabular}%
  }
  \vspace{-1mm}
  \caption{Effectiveness of different CTVIS components.}
  \label{tab:ctvis}
\end{table}%

\begin{table}[t]
\vspace{-2mm}
  \centering
  \resizebox{0.7\columnwidth}{!}{%
    \begin{tabular}{cc|cc}
    \toprule
    Major       & Supplementary          & AP$^{\mathtt{YV19}}$ & AP$^{\mathtt{OVIS}}$ \\
    \midrule
                & $\checkmark$           &   16.5      &  0.5  \\
    $\checkmark$           &             &   50.8	   &  31.6 \\
    $\checkmark$           & global      &   54.6      &  33.4 \\
    $\checkmark$           & local       &   \textbf{55.1}      &  \textbf{35.5} \\
    \bottomrule
    \end{tabular}%
  }
  \vspace{-1mm}
  \caption{Ablate the sampling strategy of negative embeddings.} 
  \vspace{-3mm}
  \label{tab:negative}%
\end{table}%

\subsection{Pseudo Video as Training Example}
\label{sec:exp_pseudo}
We train VIS models on pseudo-videos, which are created with COCO images and the method described in Section~\ref{sec:pseudo}. Since COCO classes do not match that of VIS datasets, we only adopt the overlapping categories for training. For evaluation, we sample 421 and 140 videos with overlapping categories from the train sets of YTVIS21 and OVIS train sets, respectively. For more dataset information, please refer to the supplementary material. Specially, we denote the sampled version of YTVIS21 and OVIS as YTVIS21$^*$ and  OVIS$^*$. We use Swin-L as the backbone, and investigate the impacts of augmentation techniques in terms of generating pseudo-video datasets for training. Here \emph{rotation} is taken as the baseline. As shown in Table~\ref{tab:augs}, both \emph{crop} and \emph{copy\&paste} bring gains on both datasets over the baseline. Because YTVIS21 is relatively simple, \emph{crop} and \emph{copy\&paste} only improve the results by $0.2$ and $0.5$, respectively. However, for the complicated OVIS, they offer much larger performance gains, \ie $1.3$ and $2.0$ on two datasets, which suggests that pseudo videos generated with stronger augmentations are especially suitable to tackle complicated VIS tasks. We also train VITA and IDOL models using the generated pseudo-samples. Again, CTVIS surpasses them by clear margins, as that demonstrated in Table~\ref{tab:pseudo_sota}.

\subsection{Training with Limited Supervision}
Following MinVIS\cite{minvis}, we train CTVIS and MinVIS models on only a proportion ($\%$) of VIS training set. Specifically, we sample 1\%, 5\%, 10\%, and 100\% frames respectively from the training set to create pseudo videos for training. As shown in Table~\ref{tab:st_img}, with a 5\% proportion, CTVIS outperforms MinVIS with 100\% samples on all datasets.  More importantly, CTVIS trained with pseudo videos, which are created from 100\% frame samples, even surpasses the fully supervised competitors, and achieves close performance compared with CTVIS learned from genuine videos. 

\begin{table}[t]
  \centering
  \resizebox{0.8\columnwidth}{!}{%
    \begin{tabular}{ccc|cc}
    \toprule
        Rotation  &Crop  & Copy\&Paste       & AP$^{\mathtt{YV21^*}}$ & AP$^{\mathtt{OVIS^*}}$ \\
    \midrule
    $\checkmark$ &  &                                      & 48.5             & 27.3          \\
    $\checkmark$ & $\checkmark$  &                         & 48.7             & 28.6           \\
    $\checkmark$ &  & $\checkmark$                         & 49               & 29.3    \\
    $\checkmark$ & $\checkmark$ & $\checkmark$             & \textbf{49.7}             &\textbf{30.5}    \\
    \bottomrule
  \end{tabular}%
  }
  \vspace{-1mm}
  \caption{Influence of augmentations on producing pseudo-videos.}
  \label{tab:augs}%

\end{table}%

% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
\begin{table}[t]
\vspace{-1mm}
\centering
\resizebox{0.8\columnwidth}{!}{%
\begin{tabular}{c|c|cc}
\toprule
    Methods &   Supervision &  AP$^{\mathtt{YV21^*}}$ & AP$^{\mathtt{OVIS^*}}$ \\ 
    \midrule
    MinVIS \cite{minvis}  &  Image  & 43.9             & 24.4 \\
    VITA   \cite{vita}    &  Pseudo video & 44.4             & 19.1 \\
    IDOL   \cite{idol}    &  Pseudo image pair& 47.8             & 27.8 \\
    CTVIS                 &  Pseudo video & \textbf{49.7}             & \textbf{30.5} \\
    \bottomrule
\end{tabular}%
}
\vspace{-1mm}
\caption{Compare with SOTA models trained with pseudo-samples, which are generated based on COCO images.}
\label{tab:pseudo_sota}
\vspace{-3mm}
\end{table}

% Figure environment removed 

\begin{table}[t]
    % \vspace{-1mm}
    \centering
    \footnotesize
    \begin{tabular}{c|c|ccc}
    \toprule
    Methods                  & Training                 & AP$^{\mathtt{YV19}}$ & AP$^{\mathtt{YV21}}$ & AP$^{\mathtt{OVIS}}$ \\
    \midrule
    VITA \cite{vita}        & \multirow{3}{*}{Full} & 63           & 57.5         & 27.7      \\
    IDOL \cite{idol}        &                       & 64.3         & 56.1         & 42.6      \\
    \textbf{CTVIS (Ours)}   &                       & \textbf{65.6}         & \textbf{61.2}         & \textbf{46.9}      \\
    \midrule
    \multirow{4}{*}{MinVIS \cite{minvis}} 
                            & 1\%                   & 59           & 52.9         & 31.7 \\
                            & 5\%                   & 59.3         & 54.3         & 35.7 \\
                            & 10\%                  & 61           & 54.9         & 37.2 \\
                            & 100\%                 & \textbf{61.6}         & \textbf{55.3}         & \textbf{39.4} \\
    \midrule
    \multirow{4}{*}{\textbf{CTVIS (Ours)}}  
                            & 1\%                   & 62.4         & 57.8        & 36.2 \\
                            & 5\%                   & 63.4         & 59.4         & 41.9 \\
                            & 10\%                  & 64.2         & 60.0         & 42.1 \\
                            & 100\%                 & \textbf{64.8}         & \textbf{60.7}         & \textbf{44.1} \\
    \bottomrule
    \end{tabular}%
    \vspace{-1mm}
    \caption{Compare with SOTA models trained with either the entire or a part ($x\%$) of training examples. Full means training with annotated videos.}
    \label{tab:st_img}
    \vspace{-3mm}
\end{table}


\subsection{Qualitative Results}
We visualize some VIS results obtained by SOTA offline\cite{vita} and online\cite{idol} approaches in Figure~\ref{fig:video}. The left example includes heavy occulusion caused by moving pedestrian, the swap of instance positions, and target-disappearing-reappearing. Under such case, VITA \cite{vita} fails to segment and track the pedestrian. IDOL\cite{idol} mistakenly assigns the ID of the dog in the two rightmost images, and the squatting person is recognized as a dog. In comparison, our proposed CTVIS is able to segment, classify and track all instances successfully.
%
For the right example, both VITA and IDOL fail to track the fish, and their ID switched after the video suddenly darkened. CTVIS also undergoes and ID switch (the middle image). Thanks to the noise introduced during training, CTVIS is more robust to tackle such occasional failure, and it reidentifies the fish later (the rightmost image).
