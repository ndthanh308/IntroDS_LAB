\vspace{8pt}
\input{tab/short}
\input{tab/short_varies}
\section{Experiments}

\vspace{-5pt}

We conduct quantitative and qualitative evaluations between MovieChat and previous methods. Additionally, we perform ablation studies to investigate MovieChat. Experimental settings and analyses can be found in appendix.

\subsection{Quantitative Evaluation}



\label{exp:quantitative}


\paragraph{Short video question-answering.} We use several widely used open-ended datasets: MSVD-QA~\cite{xu2017video}, MSRVTT-QA~\cite{xu2016msr-vtt}, and ActivityNet-QA~\cite{yu2019activitynet} for short video question-answering tasks. The evaluation process is under the assistance of LLM with the default hyper-parameter settings. The accuracy and relative scores on a scale of $0$ to $5$ are reported. Compared to previous methods~\cite{maaz2023video,li2023videochat,zhang2023llama, zhang2023video}, MovieChat achieves comparable performance even it is not specifically designed for short video question-answering tasks, as shown in Tab.~\ref{tab:short}.



\vspace{-12pt}

\paragraph{Short video generative performance.} Following ~\cite{maaz2023video}, we employ GPT-assisted evaluation to conduct a more comprehensive comparison of the text generation performance between MovieChat and previous methods~\cite{maaz2023video,li2023videochat,yang2022zero} on processed ActivityNet-QA~\cite{yu2019activitynet}. The evaluation pipeline covers crucial metrics (including \textit{Correctness of Information}, \textit{Detailed Orientation}, \textit{Contextual Understanding}, \textit{Temporal Understanding} and \textit{Consistency}) and assigns relative scores to the generated predictions on a scale of 1-5. We present the results of the generation performance evaluation in Tab.~\ref{tab:short_varies}. The results reveal its competitive performance across all key aspects compared to previous methods. 






\vspace{-12pt}

\paragraph{Long video question-answering.} We evaluate the long video question-answering performance of MovieChat with our proposed MovieChat-1K. We split 1,000 videos into training set~(800), test set~(100), validation set~(100) and only use test set for final performance evaluation. We select three recent LLM-based video understanding models~(\eg Video Chat~\cite{li2023videochat}, Video LLaMA~\cite{zhang2023video}, and Video-ChatGPT~\cite{maaz2023video}) as the baselines. Yet, none of those methods can support such long video~($\textgreater 10$K frames). Therefore, to accommodate their length limitations in global questions, we uniformly sample from the original video up to the maximum frame count which can be officially supported by each individual model. For breakpoint questions, we extend half of the maximum frame count before and after the breakpoint (\ie, placing the breakpoint at the center frame). 

\input{tab/long}
\input{tab/long_varies}
\input{fig/ablation}
To enhance the robustness of the results, we simultaneously employ GPT-3.5~\cite{gpt3.5} and Claude~\cite{examplewebpage} as LLM assistants, with the additional support of human blind rating. We observe a discrepancy between the accuracy and relative score generated by the previously LLM-assisted evaluation method~\cite{maaz2023video} for video question-answering tasks. However, merely adjusting the prompt for the LLM cannot effectively address this issue. Therefore, after obtaining the accuracy and score from the LLM-assisted evaluation method, we implement manual filtering to remove results with inconsistent values, thus improving the reliability of our outcomes.


As shown in Tab.~\ref{tab:long}, compared to previous methods~\cite{maaz2023video,li2023videochat,zhang2023video}, MovieChat reads more video frames. In both global mode and breakpoint mode, our method maintains a performance gain in terms of the average accuracy and score provided by LLM assistants and human blind rating. We comprehensively evaluate MovieChat's question-answering performance across different question types compared to baselines. The results indicate that our approach outperforms the baselines in both open-ended and true-false questions.



\vspace{-8pt}

\paragraph{Long video generative performance.} 

We compare the quality of answers generated by MovieChat and previous methods~\cite{maaz2023video,li2023videochat,zhang2023video} in long video question-answering on MovieChat-1K. As shown in Tab.~\ref{tab:long_varies}, with the average score provided by GPT-3.5~\cite{gpt3.5}, Claude~\cite{examplewebpage} and human bling rating, our approach continues to generate higher-quality answers even as the video contents become more extensive.


\subsection{Ablation Study}



\input{tab/videollama_score}

\vspace{-8pt}

\paragraph{Short-term and long-term memory buffers.} 
As MovieChat incorporates a memory mechanism including short-term memory and long-term memory, it is imperative to evaluate how the proposed memory mechanism influences the performance. Tab.~\ref{tab:videollama_score} and Tab.~\ref{tab:videollama_5} provide the memory-dependent performance of MovieChat for long video question-answering and generative tasks with the average results of GPT-3.5~\cite{gpt3.5}, Claude~\cite{examplewebpage}, and human blind rating. MovieChat with the memory mechanism significantly outperforms the memory-independent variant, which signifies the importance of memory mechanisms.

\vspace{-8pt}

\paragraph{Hyper-parameter ablations.} 
We perform a series of hyperparameter ablations based on the MovieChat-1K dataset to better understand MovieChat. Fig.~\ref{fig:ablation} shows the performance when ablating the length of memory buffers, consolidation length and short-term initialization with the average results of GPT-3.5~\cite{gpt3.5}, Claude~\cite{examplewebpage}, and human blind rating. The performance of MovieChat degrades when all four are significantly changed, showing the validity of our empirically chosen hyperparameyers. Fig.~\ref{fig:ablation} demonstrates that information obtained from the video expands with the growing length of memory buffers, while the loss of finer details intensifies with the fixed length of consolidation. Furthermore, using merged tokens for short-term initialization outperforms last few tokens and uniform sampling. Additionally, the length of merged tokens and the memory buffer size have a combined effect on MovieChat's performance.

\definecolor{global}{RGB}{21,96,130}
\definecolor{breakpoint}{RGB}{51,0,111}

\input{tab/videollama_5}

\vspace{-8pt}

\subsection{Case Study}

\vspace{-7pt}

We perform an extensive case study of MovieChat on a variety of open-ended long video~(such as cartoon movie and TV series) for long video question-answering, including the \parbox[c][8pt][l]{8pt}{\colorbox{breakpoint}{}}breakpoint mode (Q\#1) and the \parbox[c][8pt][l]{8pt}{\colorbox{global}{}}global mode (Q\#2). The evaluation is conducted between MovieChat and previous methods~\cite{maaz2023video,li2023videochat,zhang2023llama} as shown in Fig.~\ref{fig:case} . For Q\#1 in breakpoint mode, we mark the timestamp when the question is asked. For long videos over $10$K frames, MovieChat is still capable of providing excellent responses to questions regarding both the current moment and the entire video content with less hallucination. More examples to show long video scene understanding and temporal understanding ability of MovieChat are available in appendix. 

\input{fig/case}
