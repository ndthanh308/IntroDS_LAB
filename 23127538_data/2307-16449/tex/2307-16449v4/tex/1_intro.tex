\section{Introduction}

Recent advances in Large Language Models (LLMs)~\cite{gpt4,brown2020language,touvron2023llama,chiang2023vicuna,taori2023stanford} acheive great success in Natural Language Processing (NLP) . It is a natural progression to introduce multi-modality~\cite{chai2022deep} into LLMs and turn it into Multi-modal Large Language Models (MLLMs), which is able to conduct multimodal rationalization and understanding. MLLMs have shown incredible emergent capabilities in various multimodal tasks such as perception (\eg, existence, count, position, OCR)~\cite{wang2023visionllm, maaz2023video, alayrac2022flamingo,zhu2023minigpt,li2023otter,li2023blip}, commonsense reasoning~\cite{gao2023llama,zhu2023minigpt,li2023otter,li2022blip,li2023blip,gong2023multimodal,su2023pandagpt, maaz2023video}, and code reasoning~\cite{fu2023mme,lyu2023macaw,ye2023mplug,dai2023instructblip,liu2023visual,gao2023llama}, resulting in a potential path to Artificial General Intelligence (AGI). Compared to LLMs and other task-specific models, MLLMs  provide a more human-like interpretation of the scenarios, a user-friendly interface for interaction, and a broader range of capabilities.

\input{fig/intro}

Existing vision-centric MLLMs follow the paradigm that utilizing pre-trained LLMs and visual encoder with additional learnable modules (Q-former~\cite{dai2023instructblip,li2022blip,li2023blip,zhang2023video} or simple projection layer~\cite{driess2023palm,maaz2023video,liu2023visual,su2023pandagpt}). In video field, some previous works~\cite{zhang2023video,maaz2023video} follow this paradigm to build video MLLMs, while works in the other paradigm~\cite{wang2023chatvideo,li2023videochat} combine existing visual perception tools (\eg, tracking and classification) and LLMs through Application Programming Interface (API) to build a system without training. Yet, previously, there is no exploration of a model or system based on long videos (over one minute), and there is also a lack of a standardized benchmark to evaluate the capabilities of these systems.

In this paper, we present MovieChat, a novel framework that integrates vision models and LLMs to conduct long video understanding tasks. We claim that the computation complexity, memory cost, and long-term temporal connection are the main challenges for long video understanding. Atkinson-Shiffrin memory model~\cite{atkinson1968chapter} proposes that short-term memory functions as a buffer of long-term memory, serving as a processor for the encoding of information into long-term memory. Inspired by this, we propose a memory mechanism to deal with long video understanding tasks, which includes a rapidly updated short-term memory and a compact thus sustained long-term memory. We use a sliding window approach to extract video features and represent them in token form, which are then sequentially fed into the short-term memory frame by frame. The short-term memory has a fixed length, and when it reaches its set limit, the earliest tokens are popped and consolidated into the long-term memory. After passing through a projection layer, the video representation is inputted into a large language model for interaction with the user. As shown in Fig.~\ref{fig:intro}, our proposed MovieChat mechanism outperforms other existing methods in terms of Video Random Access Memory (VRAM) cost. We also release a new benchmark, MovieChat-1K, with 1K long videos and 13K manual question-answering pairs for validation of the effectiveness of our proposed MovieChat. 

The contributions of this work are summarized as:

\vspace{-5pt}

\begin{itemize}
    \vspace{-5pt}
    \item We present MovieChat, a novel framework that integrates vision models and LLMs, which is the first to support long video ($\textgreater 10$K frames) understanding tasks.
    \vspace{-6pt}
    \item We propose an effective memory management mechanism to reduce the computation complexity and memory cost, while enhancing the long-term connection.
    \vspace{-6pt}
    \item We release the first long video understanding benchmark, MovieChat-1K, with manual annotations and conduct extensive quantitative evaluation and case studies to evaluate the comparable performance of both understanding capability and inference cost.
\end{itemize}