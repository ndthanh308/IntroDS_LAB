
\appendix
\renewcommand\thefigure{\Alph{section}\arabic{figure}}
\renewcommand\thetable{\Alph{section}\arabic{table}}
\setcounter{figure}{0}
\setcounter{table}{0}


The supplementary material is structured as follows:

\begin{enumerate}[label=\arabic*.]

\item We first provide detailed supplementary statistical information for MovieChat-1K in Section~\ref{sec:benchmark_stat}.

\item To demonstrate the outstanding performance of MovieChat across a wide range of categories, we calculate the Pearson correlation coefficient of different score methods in Section~\ref{sec:pear}.

\item The performance of MovieChat varies across different categories of questions, and we present the results in Section~\ref{sec:type_result}.

\item We add the result of quantitative evaluation for long video generative performance in breakpoint mode in Section~\ref{sec:break_quan}.

\item We mention the specifical LLM-Assisted Evaluation method employed for the assessment of short video generative performance in Section~\ref{sec:gpt-eval-varies}.

\item The prompt tamplate we use for LLM-Assisted Evaluation is shown in Section~\ref{sec:gpt-eval}.

\item We list the evaluaotion results with GPT-3.5~\cite{gpt3.5}, Claude~\cite{examplewebpage} and human blind rating in Section~\ref{sec:eva_methods}.

\item We also list the hyperparameter settings of MovieChat in Section~\ref{sec:hyper-param}.

\item We conduct result analyses for the ablation study on hyperparameter settings in Section~\ref{sec:hyp_ana}.

\item To avoid the impact of misjudgments by LLM assistants on the results, we introduce the manual filtering strategy in Section~\ref{sec:filter}.

\item We then present schematic diagram of the memory consolidation algorithm of MovieChat in Section~\ref{sec:merge}.

\item In comparison to the LLM currently used by MovieChat, we switch to a different LLM and compare the results in Section~\ref{sec:llm}.

\item Lastly, we give more examples for scene understanding and temporal understanding of MovieChat in Section~\ref{sec:task_case}.
  
\end{enumerate}

\section{MovieChat-1K Statistics Information}
\label{sec:benchmark_stat}

\paragraph{Distribution of video categories.}
MovieChat-1K contains videos from 15 popular categories with varying distribution. As shown in Tab.~\ref{tab:cate}, every video comprises multiple alternating scenes.


\paragraph{Video information and visual question-answer data format.}

To the best of our knowledge, a long video understanding dataset has not
yet been established. Our work represents the initial step in creating and making it publicly available.We create MovieChat1K, containing 1k long videos and corresponding 1k dense captions, and 13k visual question-answer pairs.One visual example of these arrangements is provided in Figure~\ref{fig:data_format}.
\input{tab/cate}
\input{fig/data_format}
\input{fig/question}
\input{fig/answer}
\paragraph{Sentence length distribution of question-answer pairs.}
MovieChat1K exhibits diverse lengths of question-answer pairs in the segmented clip level. Fig.~\ref{fig:question} and Fig.~\ref{fig:answer} demonstrate the length distribution of question-answer pairs in different modes. Despite the distribution of question-answer pairs varies between the global mode and breakpoint mode, the majority of questions tends to concentrate between 5-15 words in length, while the length of answers generally have fewer than 10 words.


\paragraph{Stastics information of dense captions.}
\input{fig/caption_length}
\input{fig/caption_wordcloud}

To facilitate a more detailed understanding of long videos, we provide a dense caption for each video. As shown in Fig.~\ref{fig:caption_length}, MovieChat-1K exhibits diverse caption lengths in the segmented clip level. Approximately two-thirds
of the clips have captions with 100-149 words, while one-fifth of the clip captions have fewer than 100 words. About 11\% of clips have long captions with more than 150 words.

To analyze the word distribution of our generated
captions, we compute their distributions. The resulting word
distribution of the captions is presented in Fig.~\ref{fig:caption_wordcloud}, which includes common objects (man, woman, people,
girl, etc.), attributes (detective, various, small, white, etc.), locations (inside, behind, south, next, etc.),
scenes (room, house, building, office, etc.), actions/events (talk, enter, leave, take, etc.), and
more.

In terms of actionness, MovieChat-1K captions contains nearly the same number of verbs as with the WebVid10M dataset~\cite{DBLP:journals/corr/abs-2104-00650}. To evaluate this, we use the NLTK toolkit to analyze the number of verbs in captions, focusing on extracting and tagging all unique verbs. We find a total of 109,485 verbs in the WebVid10M caption dataset, while the MovieChat-1K captions contain 102,988 unique instances of verbs. While these counts may not be entirely accurate due to our simple counting method, we believe they provide a rough indication of the actionness of the two datasets.

\paragraph{Comparison between MovieChat-1K and other benchmarks.}
MovieChat-1K provides a large-scale benchmark for long video understanding, which contains
1K movies, 1K dense captions and 13k question-answer pairs. The comparison between different
datasets are shown in Tab.~\ref{tab:benchmarks}. 
It is evident that MovieChat-1K provides the longest average duration for movie clips. MovieQA~\cite{tapaswi2016movieqa} exclusively offers question-answer pairs related to movies, while MovieGraphs~\cite{vicol2018moviegraphs} supplies captions associated with movies. Unlike other datasets, MovieNet~\cite{huang2020movienet} encompasses three main types of texts: subtitle, synopsis, and script, excluding question-answer pairs. Additionally, the synopsis category is designed for the entire movie rather than video clips. Consequently, MovieChat-1K is more suitable for studying long video comprehension compared to other datasets.
\input{tab/benchmarks}



\section{Pearson correlation coefficient of different score
methods.}
\label{sec:pear}
\input{fig/pear}
\input{tab/Pearson}


The Pearson correlation coefficient is represented by the formula:

$$r_{xy}=\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sqrt{\sum_{i=1}^n(x_i-\bar{x})^2}\sqrt{\sum_{i=1}^n(y_i-\bar{y})^2}}$$

where $r_{xy}$ is the Pearson correlation coefficient between two variables $x$ and $y$, $x_{i}$ and $y_{i}$ are the individual sample points for variables $x$ and $y$, $\overline{x}$ and $\overline{y}$ are the averages of the $x$ and $y$ samples respectively, and $n$ is the number of sample points.
The formula essentially assesses the extent of linear correlation between two variables by evaluating the product of their deviations from their respective means. The numerator represents the covariance between the two variables, and the denominator normalizes this value, ensuring that the coefficient remains between -1 and +1. The Pearson correlation coefficient quantifies the extent to which two variables co-vary in comparison to their individual variations.

As shown in Tab.~\ref{tab:pearson} and Fig.~\ref{fig:pearson}, we conduct pearson correlation analysis between GPT-3.5~\cite{gpt3.5}, Claude~\cite{examplewebpage}, and human blind rating. The result indicates a substantial agreement among these evaluation methods. The alignment of scores across different score methods strengthens the reliability of our assessment. Crucially, our proposed method, MovieChat outperforms previous methods~\cite{maaz2023video,li2023videochat,zhang2023llama, zhang2023video} in long video understanding tasks. The superior performance of MovieChat is evident across a broad spectrum of categories, suggesting that our model not only has a deeper understanding of long videos and respective questions but also exhibits a more accurate and consistent ability to generate relevant responses.




\section{Quantitative evaluation for long video different types question answering.}
\label{sec:type_result}
\input{tab/type_result_global}
\input{tab/type_result_break}

To better assess the performance of MovieChat, we conduct evaluations on the long video question answering task using various types of questions. We roughly categorize the question types into multiple-choice questions and open-ended questions. With the average results of GPT-3.5~\cite{gpt3.5}, Claude~\cite{examplewebpage} and human blind rating, Tab.~\ref{tab:type_result_global} and Tab.~\ref{tab:type_result_break} respectively present the accuracy and scores of MovieChat and the baseline across different question categories in both global mode and breakpoint mode. In various research conditions, our approach consistently outperforms the baseline, thus substantiating the robustness of MovieChat.



\section{Quantitative evaluation for long video generative performance in breakpoint mode}
\label{sec:break_quan}
\input{tab/break_5}

With the average results of GPT-3.5~\cite{gpt3.5}, Claude~\cite{examplewebpage} and human blind rating, Tab.~\ref{tab:break_5} demonstrates that our method outperforms the baseline in long video generative performance in breakpoint mode.




\section{LLM-Assisted Evaluation for short video generative performance.}
\label{sec:gpt-eval-varies}
We use LLM-Assisted Evaluation proposed by~\cite{maaz2023video} for short video generative performance. The evaluation pipeline assesses various capabilities of the model and assigns a relative score ($1$ to $5$) to the generated predictions, in the following five aspects:\textit{ Correctness of Information}, \textit{Detail Orientation}, \textit{Contextual Understanding}, \textit{Temporal Understanding} and \textit{Consistency}. We follow the corresponding prompts provided in \url{https://github.com/mbzuai-oryx/Video-ChatGPT}
and report the baseline results of short video generative performance from it.





\section{Prompt template of LLM-Assisted Evaluation.}
\label{sec:gpt-eval}
Following~\cite{maaz2023video}, we use LLM-Assisted Evaluation for both the short video question-answering task and the long video question-answering task. Given the question, correct answer, and predicted answer by the model, the LLM assistants should return the \textit{True} or \textit{False} judgement and relative score ($0$ to $5$). The whole prompt is shown in Fig.~\ref{fig:prompt}. It takes about $250$ tokens per question. We report the baseline results of short video question-answering from \url{https://github.com/mbzuai-oryx/Video-ChatGPT}.
\setcounter{figure}{0}
\input{fig/prompt}


\section{Evaluation results with GPT, Claude and human blind rating.}
\label{sec:eva_methods}

As shown in~\ref{tab:long_gpt}--\ref{tab:break_5_human}, we provide detailed scoring results for GPT-3.5~\cite{gpt3.5}, Claude~\cite{examplewebpage}, and human blind rating across various experiments.

\input{tab/long_gpt}
\input{tab/long_claude}
\input{tab/long_human}

\input{tab/long_varies_gpt}
\input{tab/long_varies_claude}
\input{tab/long_varies_human}

\input{tab/break_5_gpt}
\input{tab/break_5_claude}
\input{tab/break_5_human}




\section{Hyperparameter Setting}
\label{sec:hyper-param}
\input{tab/hyper-param}


We report the detailed hyperparameter settings of MovieChat in Tab.~\ref{tab:hyper}. The sliding window size of MovieChat is set to 16, which means that every slide involves the extraction of 16 frames. We configure the short-term memory to consist of 18 frames, with each frame containing 32 tokens. When the short-term memory reaches its capacity, it is directed to the memory consolidation module to be merged into 2 representative frames. The 2 frames are simultaneously input into the long-term memory with a total length of 256 and used to reinitialize the short-term memory.



\section{Analysis on hyperparameter ablations.}
\label{sec:hyp_ana}
As the lengths of the short-term and long-term memory buffers increase, the information acquired by MovieChat from the video expands. However, more video compression leads to the loss of more detailed information, while the length of the merged tokens remains constant. Therefore, as the lengths of two memory buffers increase, the performance of MovieChat exhibits a trend of initially rising and then declining.

Since the LLM-based evaluation shows a positive correlation between accuracy and score, we use accuracy to gauge performance. When memory buffer parameters remain constant, shorter merged tokens indicate increased frame information compression, potentially resulting in information loss when excessive. Conversely, longer merged tokens, despite retaining a greater extent of short-term memory in the face of compression, correspondingly result in less overall information acquisition. Moreover, when the length of the memory buffer changes, as exemplified by long-term memory, the corresponding peak performance of MovieChat shifts in response to the altered length of merged tokens. This demonstrates the need to strike a balance between dense information extraction and information compression in long video understanding tasks.

We also conduct experiments to compare various methods for initializing the short-term memory, including selecting the last few tokens, uniform sampling, and using merged tokens. The results indicate that the use of merged tokens produces the best performance. When initializing the next short-term memory with the last few tokens from the previous short-term memory, it is unable to adequately represent the information from the previous time step. Consequently, this leads to the final merged tokens being either repetitive or lacking coherence with the previous time step. Uniform sampling faces similar issues, but it manages to capture information with representative frames from the previous time step. Consequently, its performance surpasses that of initializing with the last few tokens, yet it remains inferior to using merged tokens for initialization.





\section{Manual filtering strategy for LLM-Assisted Evaluation.}
\label{sec:filter}
For each test data, ~\cite{maaz2023video} utilized GPT-3.5~\cite{gpt3.5} to provide an evaluation result in terms of a 'yes/no' response and a corresponding score, as demonstrated in Fig.~\ref{fig:prompt}. The score is an integer value ranging from 0 to 5, where a score of 5 indicates the highest degree of meaningful correspondence. However, we observe instances where GPT-3.5~\cite{gpt3.5} offered judgments and scores that do not align, such as providing a 'yes' response with a score of 0 or a 'no' response with a score of 5. This discrepancy has the potential to impact the accuracy of results and introduce fluctuations. We adapt the prompts used for GPT-3.5~\cite{gpt3.5} with the aim of addressing this concern and did not yield the desired mitigation.
Hence, we introduce an artificial filtering strategy. For each evaluation result generated by GPT-3.5~\cite{gpt3.5}, we conduct manual screening. We retain only those outcomes that exhibited consistency between the 'yes/no' judgments and the associated scores, thus enhancing the reliability of the evaluations. 
Similarly, we applied the same filtering strategy to the evaluation results generated by Claude~\cite{examplewebpage}.




\section{Memory consolidation algorithm of MovieChat.}
\label{sec:merge}
\input{fig/merge}
As shown in Fig.~\ref{fig:merge}, for each sampled frame $x_{i}$, we calculate its similarity with adjacent frames. After that, we select the pair with the greatest similarity, merge and replace these two frames, resulting in a new sequence. We conduct the merge operation repeatedly until the count of existing frames in short-term memory reaches the predefined value.



\section{Ablation study on large language models.}
\label{sec:llm}
\input{tab/llm_score}
\input{tab/llm_5}
\input{fig/llm}

Most previous video understanding methods~\cite{maaz2023video,li2023videochat,zhang2023llama, zhang2023video} primarily employed LLama~\cite{touvron2023llama} and its variants~\cite{stablevicuna-github} as text decoders. With the average results of GPT-3.5~\cite{gpt3.5}, Claude~\cite{examplewebpage} and human blind rating, Tab.~\ref{tab:llm_score} and Tab.~\ref{tab:llm_5} illustrate how the performance of MovieChat changes when using LLama~\cite{touvron2023llama} and LLama2~\cite{touvron2023llama2} as the large language model respectively. 

Contrary to our hypothesis, under every evaluation conditions, the performance metrics of MovieChat with LLama2~\cite{touvron2023llama2} hardly surpassed those of MovieChat with LLama~\cite{touvron2023llama}. We further investigate a specific example to analyze this phenomenon. As shown in Fig.~\ref{fig:llm}, the bold segments represent direct responses to the questions from two versions of MovieChat. MovieChat with LLama~\cite{touvron2023llama} provided answers that are more aligned with the video content. Surprisingly, MovieChat with LLama2~\cite{touvron2023llama2} offer an approximation of the time required for each step (indicated by underlines Fig.~\ref{fig:llm}). While its time estimates do not precisely match the actual durations, the proportion of time provided was realistic. Even though LLama2~\cite{touvron2023llama2} cannot obtain specific time information when processing feature-rich video frames, MovieChat's memory buffer design allows for dense sampling of video frames, enabling LLama2~\cite{touvron2023llama2} to estimate the proportion of time for each scene based on adjacent similar frames. Therefore, we propose that the lower evaluation metric results of MovieChat with LLama2~\cite{touvron2023llama2} compared to MovieChat with LLama~\cite{touvron2023llama} may be attributed to the question-answer pairs provided by the dataset.


\section{Examples for scene understanding and temporal understanding of MovieChat.}
\label{sec:task_case}

We perform an extensive case study of MovieChat on a variety of open-ended long video~(such as cartoon movie in and TV series) for long video question-answering and captioning task, including the \parbox[c][8pt][l]{8pt}{\colorbox{global}{}}global mode and the \parbox[c][8pt][l]{8pt}{\colorbox{breakpoint}{}}breakpoint mode. 
The evaluation tasks include scene understanding and temporal understanding as shown in Fig.~\ref{fig:case1}, Fig.~\ref{fig:case2}, Fig.~\ref{fig:case3} and Fig.~\ref{fig:case4}. For Q\#1 and Q\#2, we remarks timestamps in frames. For long videos over $10$K frames, MovieChat is still capable of providing excellent responses to questions regarding both the current moment and the entire video content. 

\input{fig/case1}
\input{fig/case2}