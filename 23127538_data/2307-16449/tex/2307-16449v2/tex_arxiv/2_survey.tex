\section{Related Works}

\subsection{Multi-modal Large Language Models}
LLMs~\cite{gpt4,brown2020language,touvron2023llama,touvron2023llama2,chiang2023vicuna,taori2023stanford} have achieved great success in natural language processing (NLP) tasks recently. Many works try to build MLLMs~\cite{alayrac2022flamingo,zhu2023minigpt,li2023otter,li2023blip,gong2023multimodal,ye2023mplug,dai2023instructblip,wang2023visionllm,maaz2023video,gao2023llama} by combining models of other modalities.
Flamingo~\cite{alayrac2022flamingo} bridges powerful pre-trained vision-only and language-only models and achieves state-of-the-art performance with few-shot learning.
BLIP-2~\cite{li2023blip} proposes a generic and efficient pre-training strategy that bootstraps vision-language pre-training from an off-the-shelf frozen pre-trained image encoders and a frozen large language model.
MiniGPT-4~\cite{zhu2023minigpt} also aligns a frozen visual encoder with a frozen LLM, Vicuna~\cite{chiang2023vicuna}, using just one projection layer to realize the system.
Otter~\cite{li2023otter} showcases improved instruction-following ability and in-context learning.
In video field, ChatVideo~\cite{wang2023chatvideo} treats tracklets as the basic video unit and allows users' interacting with the LLMs.
VideoChat~\cite{li2023videochat} integrates video foundation models and LLMs via a learnable neural interface, excelling in spatiotemporal reasoning, event localization, and causal relationship inference. 
Video-LLaMA~\cite{zhang2023video} further leverages pre-trained models ImageBind~\cite{girdhar2023imagebind} and LLaMA~\cite{touvron2023llama}, bootstraping cross-modal training in videos following BLIP-2.
Yet, these methods fail to handle long video understanding because of high computation complexity, large memory cost, and weak long-term temporal connection. Therefore, our main effort is to introduce an effective memory mechanism to overcome these challenges.

\input{fig/overview}

\subsection{Long Video Understanding}

Understanding long videos is a challenging task in computer vision. Prior arts use 3D CNN for long-term feature bank~\cite{wu2019long}, object/human-centric motion~\cite{wu2021towards,rohrbach2017generating}, or other forms~\cite{wu2022memvit,sener2020temporal} as video representations. MIST~\cite{gao2023mist} decomposes dense self-attention into a cascade segment and
region selection module to increase the computation efficiency for understanding minutes of long videos. Building long-form video understanding datasets is challenging and rarely explored.  \cite{shou2021generic} captures large scale data from Kinetics-400~\cite{carreira2017quo}, but only for generic event boundary detection tasks. \cite{soldan2022mad} creates a language grounding benchmark from audio descriptions of movies, but it lacks long-term understanding evaluation. \cite{tapaswi2016movieqa} successfully builds a benchmark contains multiple sources of information (\eg, video clips, plots, and DVS) for question-answering tasks in the movie field. There are also several datasets of video-caption/description pairs among various domains, such as cooking (\eg, MPII Cooking~\cite{rohrbach2012database,rohrbach2012script,rohrbach2016recognizing} and TACoS~\cite{regneri2013grounding,rohrbach2014coherent}), instruction (\eg, HowTo100M~\cite{miech2019howto100m} and HiREST~\cite{zala2023hierarchical}), Ego~\cite{mangalam2023egoschema}, and movie (\eg, MovieQA~\cite{tapaswi2016movieqa} and MovieNet~\cite{huang2020movienet}) from different sources such as YouTube~\cite{chen2011collecting,zeng2016title,miech2019howto100m}, Twitter~\cite{awad2017trecvid,awad2018trecvid,awad2020trecvid,awad2021trecvid}, and Internet~\cite{bain2021frozen}. 
Yet, those datasets lack diverse and fine-grained dense captioning for long videos.

\subsection{Memory Models in Vision Tasks}

There are some prior works exploring memory models~\cite{squire2015memory} in various vision tasks in videos, such as video object segmentation~(VOS)~\cite{cheng2022xmem,hu2021learning,seong2020kernelized,seong2021hierarchical}, multi-object tracking~(MOT)~\cite{cai2022memot,hao2022umotma,xin2022multi,allen2006multiple}, visual object tracking~(VOT)~\cite{zhou2023memory,yang2018learning,liu2017mavot,ma2018adaptive}, and action understanding~\cite{wang2023memory}.
MeMOT~\cite{cai2022memot} builds a large spatiotemporal memory that stores the past observations of the
tracked objects. 
XMem~\cite{cheng2022xmem} develops an architecture that incorporates multiple independent yet deeply-connected feature memory storage to handle long videos with thousands of frames.
We learn from the experience of those prior arts and further adopt an effective memory mechanism in combination with LLMs. 

Our method focuses on reducing the redundancy of visual tokens in the video and building a memory mechanism to pass the information among a large temporal range.