% Figure environment removed

% We use a sliding window approach to extract video features and represent them in token form, which are then sequentially fed into the short-term memory frame by frame. The short-term memory has a fixed length, and when it reaches its set limit, the earliest tokens are popped and consolidated into the long-term memory. We have designed two inference modes: global mode, which exclusively utilizes the long-term memory, and breakpoint mode, which additionally incorporates the current short-term memory as part of the video representation. Breakpoint mode allows for understanding the video at a specific moment in time. Finally, after passing through a projection layer, the video representation is inputted into a large language model for interaction with the user.



