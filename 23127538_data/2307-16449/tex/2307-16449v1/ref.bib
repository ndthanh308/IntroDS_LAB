@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})


@inproceedings{lei2018tvqa,
  title={TVQA: Localized, Compositional Video Question Answering},
  author={Lei, Jie and Yu, Licheng and Bansal, Mohit and Berg, Tamara},
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  pages={1369--1379},
  year={2018}
}

@inproceedings{lei2020tvqa+,
  title={TVQA+: Spatio-Temporal Grounding for Video Question Answering},
  author={Lei, Jie and Yu, Licheng and Berg, Tamara and Bansal, Mohit},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={8211--8225},
  year={2020}
}

@article{kong2022human,
  title={Human action recognition and prediction: A survey},
  author={Kong, Yu and Fu, Yun},
  journal={International Journal of Computer Vision},
  volume={130},
  number={5},
  pages={1366--1401},
  year={2022},
  publisher={Springer}
}

@article{poppe2010survey,
  title={A survey on vision-based human action recognition},
  author={Poppe, Ronald},
  journal={Image and vision computing},
  volume={28},
  number={6},
  pages={976--990},
  year={2010},
  publisher={Elsevier}
}

@inproceedings{iashin2020multi,
  title={Multi-modal dense video captioning},
  author={Iashin, Vladimir and Rahtu, Esa},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops},
  pages={958--959},
  year={2020}
}

@inproceedings{yang2023vid2seq,
  title={Vid2seq: Large-scale pretraining of a visual language model for dense video captioning},
  author={Yang, Antoine and Nagrani, Arsha and Seo, Paul Hongsuck and Miech, Antoine and Pont-Tuset, Jordi and Laptev, Ivan and Sivic, Josef and Schmid, Cordelia},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10714--10726},
  year={2023}
}

@article{qi2022weakly,
  title={Weakly Supervised Two-Stage Training Scheme for Deep Video Fight Detection Model},
  author={Qi, Zhenting and Zhu, Ruike and Fu, Zheyu and Chai, Wenhao and Kindratenko, Volodymyr},
  journal={arXiv preprint arXiv:2209.11477},
  year={2022}
}

@inproceedings{miech2020end,
  title={End-to-end learning of visual representations from uncurated instructional videos},
  author={Miech, Antoine and Alayrac, Jean-Baptiste and Smaira, Lucas and Laptev, Ivan and Sivic, Josef and Zisserman, Andrew},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={9879--9889},
  year={2020}
}

@article{li2020learning,
  title={Learning spatiotemporal features via video and text pair discrimination},
  author={Li, Tianhao and Wang, Limin},
  journal={arXiv preprint arXiv:2001.05691},
  year={2020}
}

@inproceedings{xu2021videoclip,
  title={VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding},
  author={Xu, Hu and Ghosh, Gargi and Huang, Po-Yao and Okhonko, Dmytro and Aghajanyan, Armen and Metze, Florian and Zettlemoyer, Luke and Feichtenhofer, Christoph},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={6787--6800},
  year={2021}
}

@article{li2022uniformerv2,
  title={UniFormerV2: Spatiotemporal Learning by Arming Image ViTs with Video UniFormer},
  author={Li, Kunchang and Wang, Yali and He, Yinan and Li, Yizhuo and Wang, Yi and Wang, Limin and Qiao, Yu},
  journal={arXiv preprint arXiv:2211.09552},
  year={2022}
}

@article{li2023unmasked,
  title={Unmasked teacher: Towards training-efficient video foundation models},
  author={Li, Kunchang and Wang, Yali and Li, Yizhuo and Wang, Yi and He, Yinan and Wang, Limin and Qiao, Yu},
  journal={arXiv preprint arXiv:2303.16058},
  year={2023}
}

@inproceedings{sun2019videobert,
  title={Videobert: A joint model for video and language representation learning},
  author={Sun, Chen and Myers, Austin and Vondrick, Carl and Murphy, Kevin and Schmid, Cordelia},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={7464--7473},
  year={2019}
}

@inproceedings{zhu2020actbert,
  title={Actbert: Learning global-local video-text representations},
  author={Zhu, Linchao and Yang, Yi},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={8746--8755},
  year={2020}
}

@article{wang2022internvideo,
  title={InternVideo: General Video Foundation Models via Generative and Discriminative Learning},
  author={Wang, Yi and Li, Kunchang and Li, Yizhuo and He, Yinan and Huang, Bingkun and Zhao, Zhiyu and Zhang, Hongjie and Xu, Jilan and Liu, Yi and Wang, Zun and others},
  journal={arXiv preprint arXiv:2212.03191},
  year={2022}
}

@inproceedings{li2023lavender,
  title={Lavender: Unifying video-language understanding as masked language modeling},
  author={Li, Linjie and Gan, Zhe and Lin, Kevin and Lin, Chung-Ching and Liu, Zicheng and Liu, Ce and Wang, Lijuan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={23119--23129},
  year={2023}
}

@inproceedings{tongvideomae,
  title={VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training},
  author={Tong, Zhan and Song, Yibing and Wang, Jue and Wang, Limin},
  booktitle={Advances in Neural Information Processing Systems}
}

@inproceedings{wang2023videomae,
  title={Videomae v2: Scaling video masked autoencoders with dual masking},
  author={Wang, Limin and Huang, Bingkun and Zhao, Zhiyu and Tong, Zhan and He, Yinan and Wang, Yi and Wang, Yali and Qiao, Yu},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14549--14560},
  year={2023}
}

@article{fu2021violet,
  title={Violet: End-to-end video-language transformers with masked visual-token modeling},
  author={Fu, Tsu-Jui and Li, Linjie and Gan, Zhe and Lin, Kevin and Wang, William Yang and Wang, Lijuan and Liu, Zicheng},
  journal={arXiv preprint arXiv:2111.12681},
  year={2021}
}

@inproceedings{wang2023all,
  title={All in one: Exploring unified video-language pre-training},
  author={Wang, Jinpeng and Ge, Yixiao and Yan, Rui and Ge, Yuying and Lin, Kevin Qinghong and Tsutsui, Satoshi and Lin, Xudong and Cai, Guanyu and Wu, Jianping and Shan, Ying and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={6598--6608},
  year={2023}
}

@article{gpt4,
  title={GPT-4 Technical Report},
  author={OpenAI},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{chiang2023vicuna,
  title={Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt quality},
  author={Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E and others},
  journal={See https://vicuna. lmsys. org (accessed 14 April 2023)},
  year={2023}
}

@misc{taori2023stanford,
  title={Stanford alpaca: An instruction-following llama model},
  author={Taori, Rohan and Gulrajani, Ishaan and Zhang, Tianyi and Dubois, Yann and Li, Xuechen and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B},
  year={2023}
}

@article{chai2022deep,
  title={Deep vision multimodal learning: Methodology, benchmark, and trend},
  author={Chai, Wenhao and Wang, Gaoang},
  journal={Applied Sciences},
  volume={12},
  number={13},
  pages={6588},
  year={2022},
  publisher={MDPI}
}

@article{alayrac2022flamingo,
  title={Flamingo: a visual language model for few-shot learning},
  author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={23716--23736},
  year={2022}
}

@article{li2023blip,
  title={Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
  author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  journal={arXiv preprint arXiv:2301.12597},
  year={2023}
}

@article{zhu2023minigpt,
  title={Minigpt-4: Enhancing vision-language understanding with advanced large language models},
  author={Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed},
  journal={arXiv preprint arXiv:2304.10592},
  year={2023}
}

@article{li2023otter,
  title={Otter: A multi-modal model with in-context instruction tuning},
  author={Li, Bo and Zhang, Yuanhan and Chen, Liangyu and Wang, Jinghao and Yang, Jingkang and Liu, Ziwei},
  journal={arXiv preprint arXiv:2305.03726},
  year={2023}
}

@article{li2023mimic,
  title={MIMIC-IT: Multi-Modal In-Context Instruction Tuning},
  author={Li, Bo and Zhang, Yuanhan and Chen, Liangyu and Wang, Jinghao and Pu, Fanyi and Yang, Jingkang and Li, Chunyuan and Liu, Ziwei},
  journal={arXiv preprint arXiv:2306.05425},
  year={2023}
}

@article{wang2023chatvideo,
  title={Chatvideo: A tracklet-centric multimodal and versatile video understanding system},
  author={Wang, Junke and Chen, Dongdong and Luo, Chong and Dai, Xiyang and Yuan, Lu and Wu, Zuxuan and Jiang, Yu-Gang},
  journal={arXiv preprint arXiv:2304.14407},
  year={2023}
}

@article{li2023videochat,
  title={Videochat: Chat-centric video understanding},
  author={Li, KunChang and He, Yinan and Wang, Yi and Li, Yizhuo and Wang, Wenhai and Luo, Ping and Wang, Yali and Wang, Limin and Qiao, Yu},
  journal={arXiv preprint arXiv:2305.06355},
  year={2023}
}

@article{zhang2023video,
  title={Video-llama: An instruction-tuned audio-visual language model for video understanding},
  author={Zhang, Hang and Li, Xin and Bing, Lidong},
  journal={arXiv preprint arXiv:2306.02858},
  year={2023}
}

@article{zhang2023llama,
  title={Llama-adapter: Efficient fine-tuning of language models with zero-init attention},
  author={Zhang, Renrui and Han, Jiaming and Zhou, Aojun and Hu, Xiangfei and Yan, Shilin and Lu, Pan and Li, Hongsheng and Gao, Peng and Qiao, Yu},
  journal={arXiv preprint arXiv:2303.16199},
  year={2023}
}

@inproceedings{girdhar2023imagebind,
  title={Imagebind: One embedding space to bind them all},
  author={Girdhar, Rohit and El-Nouby, Alaaeldin and Liu, Zhuang and Singh, Mannat and Alwala, Kalyan Vasudev and Joulin, Armand and Misra, Ishan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={15180--15190},
  year={2023}
}

@inproceedings{cheng2022xmem,
  title={XMem: Long-Term Video Object Segmentation with an Atkinson-Shiffrin Memory Model},
  author={Cheng, Ho Kei and Schwing, Alexander G},
  booktitle={Computer Vision--ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23--27, 2022, Proceedings, Part XXVIII},
  pages={640--658},
  year={2022},
  organization={Springer}
}

@inproceedings{cai2022memot,
  title={MeMOT: multi-object tracking with memory},
  author={Cai, Jiarui and Xu, Mingze and Li, Wei and Xiong, Yuanjun and Xia, Wei and Tu, Zhuowen and Soatto, Stefano},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={8090--8100},
  year={2022}
}

@inproceedings{oh2019video,
  title={Video object segmentation using space-time memory networks},
  author={Oh, Seoung Wug and Lee, Joon-Young and Xu, Ning and Kim, Seon Joo},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={9226--9235},
  year={2019}
}

@article{cheng2021rethinking,
  title={Rethinking space-time networks with improved memory coverage for efficient video object segmentation},
  author={Cheng, Ho Kei and Tai, Yu-Wing and Tang, Chi-Keung},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={11781--11794},
  year={2021}
}

@article{squire2015memory,
  title={Memory consolidation},
  author={Squire, Larry R and Genzel, Lisa and Wixted, John T and Morris, Richard G},
  journal={Cold Spring Harbor perspectives in biology},
  volume={7},
  number={8},
  pages={a021766},
  year={2015},
  publisher={Cold Spring Harbor Lab}
}

@inproceedings{hu2021learning,
  title={Learning position and target consistency for memory-based video object segmentation},
  author={Hu, Li and Zhang, Peng and Zhang, Bang and Pan, Pan and Xu, Yinghui and Jin, Rong},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={4144--4154},
  year={2021}
}

@inproceedings{lu2020video,
  title={Video object segmentation with episodic graph memory networks},
  author={Lu, Xiankai and Wang, Wenguan and Danelljan, Martin and Zhou, Tianfei and Shen, Jianbing and Van Gool, Luc},
  booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part III 16},
  pages={661--679},
  year={2020},
  organization={Springer}
}

@inproceedings{mao2021joint,
  title={Joint inductive and transductive learning for video object segmentation},
  author={Mao, Yunyao and Wang, Ning and Zhou, Wengang and Li, Houqiang},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={9670--9679},
  year={2021}
}

@inproceedings{seong2020kernelized,
  title={Kernelized memory network for video object segmentation},
  author={Seong, Hongje and Hyun, Junhyuk and Kim, Euntai},
  booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part XXII 16},
  pages={629--645},
  year={2020},
  organization={Springer}
}

@inproceedings{seong2021hierarchical,
  title={Hierarchical memory matching network for video object segmentation},
  author={Seong, Hongje and Oh, Seoung Wug and Lee, Joon-Young and Lee, Seongwon and Lee, Suhyeon and Kim, Euntai},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={12889--12898},
  year={2021}
}

@inproceedings{xie2021efficient,
  title={Efficient regional memory network for video object segmentation},
  author={Xie, Haozhe and Yao, Hongxun and Zhou, Shangchen and Zhang, Shengping and Sun, Wenxiu},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={1286--1295},
  year={2021}
}

@article{hao2022umotma,
  title={UMOTMA: Underwater multiple object tracking with memory aggregation},
  author={Hao, Zhicheng and Qiu, Jun and Zhang, Haimiao and Ren, Guangbo and Liu, Chang},
  journal={Frontiers in Marine Science},
  volume={9},
  pages={1071618},
  year={2022},
  publisher={Frontiers}
}

@article{zhou2023memory,
  title={Memory Network with Pixel-level Spatio-Temporal Learning for Visual Object Tracking},
  author={Zhou, Zechu and Zhou, Xinyu and Chen, Zhaoyu and Guo, Pinxue and Liu, Qian-Yu and Zhang, Wenqiang},
  journal={IEEE Transactions on Circuits and Systems for Video Technology},
  year={2023},
  publisher={IEEE}
}

@inproceedings{xin2022multi,
  title={Multi-Object Tracking with Spatial-Temporal Correlation Memory Networks},
  author={Xin, Ming and Sun, Wenjie and Li, Kaifang and Hui, Guancheng},
  booktitle={2022 3rd International Conference on Computer Vision, Image and Deep Learning \& International Conference on Computer Engineering and Applications (CVIDL \& ICCEA)},
  pages={616--619},
  year={2022},
  organization={IEEE}
}

@inproceedings{yang2018learning,
  title={Learning dynamic memory networks for object tracking},
  author={Yang, Tianyu and Chan, Antoni B},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={152--167},
  year={2018}
}

@inproceedings{fu2021stmtrack,
  title={Stmtrack: Template-free visual tracking with space-time memory networks},
  author={Fu, Zhihong and Liu, Qingjie and Fu, Zehua and Wang, Yunhong},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={13774--13783},
  year={2021}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@article{gong2023multimodal,
  title={Multimodal-gpt: A vision and language model for dialogue with humans},
  author={Gong, Tao and Lyu, Chengqi and Zhang, Shilong and Wang, Yudong and Zheng, Miao and Zhao, Qian and Liu, Kuikun and Zhang, Wenwei and Luo, Ping and Chen, Kai},
  journal={arXiv preprint arXiv:2305.04790},
  year={2023}
}

@article{lyu2023macaw,
  title={Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration},
  author={Lyu, Chenyang and Wu, Minghao and Wang, Longyue and Huang, Xinting and Liu, Bingshuai and Du, Zefeng and Shi, Shuming and Tu, Zhaopeng},
  journal={arXiv preprint arXiv:2306.09093},
  year={2023}
}

@article{ye2023mplug,
  title={mplug-owl: Modularization empowers large language models with multimodality},
  author={Ye, Qinghao and Xu, Haiyang and Xu, Guohai and Ye, Jiabo and Yan, Ming and Zhou, Yiyang and Wang, Junyang and Hu, Anwen and Shi, Pengcheng and Shi, Yaya and others},
  journal={arXiv preprint arXiv:2304.14178},
  year={2023}
}

@article{dai2023instructblip,
  title={Instructblip: Towards general-purpose vision-language models with instruction tuning},
  author={Dai, Wenliang and Li, Junnan and Li, Dongxu and Tiong, Anthony Meng Huat and Zhao, Junqi and Wang, Weisheng and Li, Boyang and Fung, Pascale and Hoi, Steven},
  journal={arXiv preprint arXiv:2305.06500},
  year={2023}
}

@article{wang2023visionllm,
  title={Visionllm: Large language model is also an open-ended decoder for vision-centric tasks},
  author={Wang, Wenhai and Chen, Zhe and Chen, Xiaokang and Wu, Jiannan and Zhu, Xizhou and Zeng, Gang and Luo, Ping and Lu, Tong and Zhou, Jie and Qiao, Yu and others},
  journal={arXiv preprint arXiv:2305.11175},
  year={2023}
}

@article{liu2023visual,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal={arXiv preprint arXiv:2304.08485},
  year={2023}
}

@article{maaz2023video,
  title={Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models},
  author={Maaz, Muhammad and Rasheed, Hanoona and Khan, Salman and Khan, Fahad Shahbaz},
  journal={arXiv preprint arXiv:2306.05424},
  year={2023}
}

@article{su2023pandagpt,
  title={Pandagpt: One model to instruction-follow them all},
  author={Su, Yixuan and Lan, Tian and Li, Huayang and Xu, Jialu and Wang, Yan and Cai, Deng},
  journal={arXiv preprint arXiv:2305.16355},
  year={2023}
}

@article{gao2023llama,
  title={Llama-adapter v2: Parameter-efficient visual instruction model},
  author={Gao, Peng and Han, Jiaming and Zhang, Renrui and Lin, Ziyi and Geng, Shijie and Zhou, Aojun and Zhang, Wei and Lu, Pan and He, Conghui and Yue, Xiangyu and others},
  journal={arXiv preprint arXiv:2304.15010},
  year={2023}
}

@inproceedings{li2022blip,
  title={Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation},
  author={Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
  booktitle={International Conference on Machine Learning},
  pages={12888--12900},
  year={2022},
  organization={PMLR}
}

@article{liu2017mavot,
  title={Mavot: Memory-augmented video object tracking},
  author={Liu, Boyu and Wang, Yanzhao and Tai, Yu-Wing and Tang, Chi-Keung},
  journal={arXiv preprint arXiv:1711.09414},
  year={2017}
}

@article{ma2018adaptive,
  title={Adaptive correlation filters with long-term and short-term memory for object tracking},
  author={Ma, Chao and Huang, Jia-Bin and Yang, Xiaokang and Yang, Ming-Hsuan},
  journal={International Journal of Computer Vision},
  volume={126},
  pages={771--796},
  year={2018},
  publisher={Springer}
}

@inproceedings{fang2018recurrent,
  title={Recurrent autoregressive networks for online multi-object tracking},
  author={Fang, Kuan and Xiang, Yu and Li, Xiaocheng and Savarese, Silvio},
  booktitle={2018 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  pages={466--475},
  year={2018},
  organization={IEEE}
}

@article{allen2006multiple,
  title={Multiple-target tracking: A role for working memory?},
  author={Allen, Roy and Mcgeorge, Peter and Pearson, David G and Milne, Alan},
  journal={Quarterly journal of experimental psychology},
  volume={59},
  number={6},
  pages={1101--1116},
  year={2006},
  publisher={SAGE Publications Sage UK: London, England}
}

@article{yin2023survey,
      title={A Survey on Multimodal Large Language Models}, 
      author={Yin, Shukang and Fu, Chaoyou and Zhao, Sirui and Li, Ke and Sun, Xing and Xu, Tong and Chen, Enhong},
      journal={arXiv preprint arXiv:2306.13549},
      year={2023},
}

@article{fu2023mme,
      title={MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models}, 
      author={Fu, Chaoyou and Chen, Peixian and Shen, Yunhang and Qin, Yulei and Zhang, Mengdan and Lin, Xu and Qiu, Zhenyu and Lin, Wei and Qiu, Zhenyu and Lin, Wei and others},
      journal={arXiv preprint arXiv:2306.13394},
      year={2023},
}

@inproceedings{wu2021towards,
  title={Towards long-form video understanding},
  author={Wu, Chao-Yuan and Krahenbuhl, Philipp},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={1884--1894},
  year={2021}
}

@inproceedings{wu2019long,
  title={Long-term feature banks for detailed video understanding},
  author={Wu, Chao-Yuan and Feichtenhofer, Christoph and Fan, Haoqi and He, Kaiming and Krahenbuhl, Philipp and Girshick, Ross},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={284--293},
  year={2019}
}

@inproceedings{wu2022memvit,
  title={Memvit: Memory-augmented multiscale vision transformer for efficient long-term video recognition},
  author={Wu, Chao-Yuan and Li, Yanghao and Mangalam, Karttikeya and Fan, Haoqi and Xiong, Bo and Malik, Jitendra and Feichtenhofer, Christoph},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={13587--13597},
  year={2022}
}

@inproceedings{sener2020temporal,
  title={Temporal aggregate representations for long-range video understanding},
  author={Sener, Fadime and Singhania, Dipika and Yao, Angela},
  booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part XVI 16},
  pages={154--171},
  year={2020},
  organization={Springer}
}

@inproceedings{zala2023hierarchical,
  title={Hierarchical Video-Moment Retrieval and Step-Captioning},
  author={Zala, Abhay and Cho, Jaemin and Kottur, Satwik and Chen, Xilun and Oguz, Barlas and Mehdad, Yashar and Bansal, Mohit},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={23056--23065},
  year={2023}
}

@inproceedings{rohrbach2017generating,
  title={Generating descriptions with grounded and co-referenced people},
  author={Rohrbach, Anna and Rohrbach, Marcus and Tang, Siyu and Joon Oh, Seong and Schiele, Bernt},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={4979--4989},
  year={2017}
}

@inproceedings{das2013thousand,
  title={A thousand frames in just a few words: Lingual description of videos through latent topics and sparse object stitching},
  author={Das, Pradipto and Xu, Chenliang and Doell, Richard F and Corso, Jason J},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2634--2641},
  year={2013}
}

@inproceedings{zhou2018towards,
  title={Towards automatic learning of procedures from web instructional videos},
  author={Zhou, Luowei and Xu, Chenliang and Corso, Jason},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={32},
  number={1},
  year={2018}
}

@inproceedings{rohrbach2012database,
  title={A database for fine grained activity detection of cooking activities},
  author={Rohrbach, Marcus and Amin, Sikandar and Andriluka, Mykhaylo and Schiele, Bernt},
  booktitle={2012 IEEE conference on computer vision and pattern recognition},
  pages={1194--1201},
  year={2012},
  organization={IEEE}
}

@inproceedings{rohrbach2012script,
  title={Script data for attribute-based recognition of composite activities},
  author={Rohrbach, Marcus and Regneri, Michaela and Andriluka, Mykhaylo and Amin, Sikandar and Pinkal, Manfred and Schiele, Bernt},
  booktitle={Computer Vision--ECCV 2012: 12th European Conference on Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part I 12},
  pages={144--157},
  year={2012},
  organization={Springer}
}

@article{rohrbach2016recognizing,
  title={Recognizing fine-grained and composite activities using hand-centric features and script data},
  author={Rohrbach, Marcus and Rohrbach, Anna and Regneri, Michaela and Amin, Sikandar and Andriluka, Mykhaylo and Pinkal, Manfred and Schiele, Bernt},
  journal={International Journal of Computer Vision},
  volume={119},
  pages={346--373},
  year={2016},
  publisher={Springer}
}

@article{regneri2013grounding,
  title={Grounding action descriptions in videos},
  author={Regneri, Michaela and Rohrbach, Marcus and Wetzel, Dominikus and Thater, Stefan and Schiele, Bernt and Pinkal, Manfred},
  journal={Transactions of the Association for Computational Linguistics},
  volume={1},
  pages={25--36},
  year={2013},
  publisher={MIT Press}
}

@inproceedings{rohrbach2014coherent,
  title={Coherent multi-sentence video description with variable level of detail},
  author={Rohrbach, Anna and Rohrbach, Marcus and Qiu, Wei and Friedrich, Annemarie and Pinkal, Manfred and Schiele, Bernt},
  booktitle={Pattern Recognition: 36th German Conference, GCPR 2014, M{\"u}nster, Germany, September 2-5, 2014, Proceedings 36},
  pages={184--195},
  year={2014},
  organization={Springer}
}

@article{torabi2015using,
  title={Using descriptive video services to create a large data source for video annotation research},
  author={Torabi, Atousa and Pal, Christopher and Larochelle, Hugo and Courville, Aaron},
  journal={arXiv preprint arXiv:1503.01070},
  year={2015}
}

@inproceedings{rohrbach2015dataset,
  title={A dataset for movie description},
  author={Rohrbach, Anna and Rohrbach, Marcus and Tandon, Niket and Schiele, Bernt},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={3202--3212},
  year={2015}
}

@inproceedings{huang2020movienet,
  title={Movienet: A holistic dataset for movie understanding},
  author={Huang, Qingqiu and Xiong, Yu and Rao, Anyi and Wang, Jiaze and Lin, Dahua},
  booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part IV 16},
  pages={709--727},
  year={2020},
  organization={Springer}
}

@inproceedings{miech2019howto100m,
  title={Howto100m: Learning a text-video embedding by watching hundred million narrated video clips},
  author={Miech, Antoine and Zhukov, Dimitri and Alayrac, Jean-Baptiste and Tapaswi, Makarand and Laptev, Ivan and Sivic, Josef},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={2630--2640},
  year={2019}
}

@inproceedings{chen2011collecting,
  title={Collecting highly parallel data for paraphrase evaluation},
  author={Chen, David and Dolan, William B},
  booktitle={Proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies},
  pages={190--200},
  year={2011}
}

@inproceedings{zeng2016title,
  title={Title generation for user generated videos},
  author={Zeng, Kuo-Hao and Chen, Tseng-Hung and Niebles, Juan Carlos and Sun, Min},
  booktitle={Computer Vision--ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14},
  pages={609--625},
  year={2016},
  organization={Springer}
}

@inproceedings{awad2017trecvid,
  title={Trecvid 2017: evaluating ad-hoc and instance video search, events detection, video captioning, and hyperlinking},
  author={Awad, George and Butt, Asad A and Fiscus, Jonathan and Joy, David and Delgado, Andrew and Mcclinton, Willie and Michel, Martial and Smeaton, Alan F and Graham, Yvette and Kraaij, Wessel and others},
  booktitle={TREC Video Retrieval Evaluation (TRECVID)},
  year={2017}
}

@inproceedings{awad2018trecvid,
  title={Trecvid 2018: Benchmarking video activity detection, video captioning and matching, video storytelling linking and video search},
  author={Awad, George and Butt, Asad A and Curtis, Keith and Lee, Yooyoung and Fiscus, Jonathan and Godil, Afzad and Joy, David and Delgado, Andrew and Smeaton, Alan F and Graham, Yvette and others},
  booktitle={Proceedings of TRECVID 2018},
  year={2018}
}

@article{awad2020trecvid,
  title={Trecvid 2019: An evaluation campaign to benchmark video activity detection, video captioning and matching, and video search \& retrieval},
  author={Awad, George and Butt, Asad A and Curtis, Keith and Lee, Yooyoung and Fiscus, Jonathan and Godil, Afzal and Delgado, Andrew and Zhang, Jesse and Godard, Eliot and Diduch, Lukas and others},
  journal={arXiv preprint arXiv:2009.09984},
  year={2020}
}

@article{awad2021trecvid,
  title={TRECVID 2020: A comprehensive campaign for evaluating video retrieval tasks across multiple application domains},
  author={Awad, George and Butt, Asad A and Curtis, Keith and Fiscus, Jonathan and Godil, Afzal and Lee, Yooyoung and Delgado, Andrew and Zhang, Jesse and Godard, Eliot and Chocot, Baptiste and others},
  journal={arXiv preprint arXiv:2104.13473},
  year={2021}
}

@inproceedings{bain2021frozen,
  title={Frozen in time: A joint video and image encoder for end-to-end retrieval},
  author={Bain, Max and Nagrani, Arsha and Varol, G{\"u}l and Zisserman, Andrew},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={1728--1738},
  year={2021}
}

@inproceedings{tapaswi2016movieqa,
  title={Movieqa: Understanding stories in movies through question-answering},
  author={Tapaswi, Makarand and Zhu, Yukun and Stiefelhagen, Rainer and Torralba, Antonio and Urtasun, Raquel and Fidler, Sanja},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4631--4640},
  year={2016}
}

@misc{videochatlong,
  author = {OpenGVLab},
  title = {Ask-Anything},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/OpenGVLab/Ask-Anything/tree/long_video_support}},
}

@misc{langchain,
  author = {hwchase17},
  title = {langchain},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/hwchase17/langchain}},
}

@misc{pos,
  author = {Su, Jianlin},
  title = {BERT Position Encoding},
  year = {2023},
  journal = {Blog},
  howpublished = {\url{https://kexue.fm/archives/7947}},
}

@article{driess2023palm,
  title={Palm-e: An embodied multimodal language model},
  author={Driess, Danny and Xia, Fei and Sajjadi, Mehdi SM and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and others},
  journal={arXiv preprint arXiv:2303.03378},
  year={2023}
}

@article{atkinson1968chapter,
  title={Chapter: Human memory: A proposed system and its control processes},
  author={Atkinson, Richard C and Shiffrin, Richard M},
  journal={The psychology of learning and motivation},
  volume={2},
  pages={89--195},
  year={1968}
}

@article{xie2023funqa,
  title={FunQA: Towards Surprising Video Comprehension},
  author={Xie, Binzhu and Zhang, Sicheng and Zhou, Zitang and Li, Bo and Zhang, Yuanhan and Hessel, Jack and Yang, Jingkang and Liu, Ziwei},
  journal={arXiv preprint arXiv:2306.14899},
  year={2023}
}

@article{fang2022eva,
      title={EVA: Exploring the Limits of Masked Visual Representation Learning at Scale}, 
      author={Yuxin Fang and Wen Wang and Binhui Xie and Quan Sun and Ledell Wu and Xinggang Wang and Tiejun Huang and Xinlong Wang and Yue Cao},
      year={2022},
      eprint={2211.07636},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{li2023blip2,
      title={BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models}, 
      author={Junnan Li and Dongxu Li and Silvio Savarese and Steven Hoi},
      year={2023},
      eprint={2301.12597},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{arnab2021vivit,
  title={Vivit: A video vision transformer},
  author={Arnab, Anurag and Dehghani, Mostafa and Heigold, Georg and Sun, Chen and Lu{\v{c}}i{\'c}, Mario and Schmid, Cordelia},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={6836--6846},
  year={2021}
}

@inproceedings{liu2022video,
  title={Video swin transformer},
  author={Liu, Ze and Ning, Jia and Cao, Yue and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Hu, Han},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={3202--3211},
  year={2022}
}

@inproceedings{bolya2022token,
  title={Token Merging: Your ViT But Faster},
  author={Bolya, Daniel and Fu, Cheng-Yang and Dai, Xiaoliang and Zhang, Peizhao and Feichtenhofer, Christoph and Hoffman, Judy},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@inproceedings{kenton2019bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Kenton, Jacob Devlin Ming-Wei Chang and Toutanova, Lee Kristina},
  booktitle={Proceedings of NAACL-HLT},
  pages={4171--4186},
  year={2019}
}

@article{yang2022zero,
  title={Zero-shot video question answering via frozen bidirectional language models},
  author={Yang, Antoine and Miech, Antoine and Sivic, Josef and Laptev, Ivan and Schmid, Cordelia},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={124--141},
  year={2022}
}

@inproceedings{xu2017video,
  title={Video question answering via gradually refined attention over appearance and motion},
  author={Xu, Dejing and Zhao, Zhou and Xiao, Jun and Wu, Fei and Zhang, Hanwang and He, Xiangnan and Zhuang, Yueting},
  booktitle={Proceedings of the 25th ACM international conference on Multimedia},
  pages={1645--1653},
  year={2017}
}

@inproceedings{jang2017tgif,
  title={Tgif-qa: Toward spatio-temporal reasoning in visual question answering},
  author={Jang, Yunseok and Song, Yale and Yu, Youngjae and Kim, Youngjin and Kim, Gunhee},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2758--2766},
  year={2017}
}

@article{touvron2023llama2,
  title={Llama 2: Open Foundation and Fine-Tuned Chat Models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@inproceedings{yu2019activitynet,
  title={Activitynet-qa: A dataset for understanding complex web videos via question answering},
  author={Yu, Zhou and Xu, Dejing and Yu, Jun and Yu, Ting and Zhao, Zhou and Zhuang, Yueting and Tao, Dacheng},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  number={01},
  pages={9127--9134},
  year={2019}
}

@article{ying2023ctvis,
  title={CTVIS: Consistent Training for Online Video Instance Segmentation},
  author={Ying, Kaining and Zhong, Qing and Mao, Weian and Wang, Zhenhua and Chen, Hao and Wu, Lin Yuanbo and Liu, Yifan and Fan, Chengxiang and Zhuge, Yunzhi and Shen, Chunhua},
  journal={arXiv preprint arXiv:2307.12616},
  year={2023}
}