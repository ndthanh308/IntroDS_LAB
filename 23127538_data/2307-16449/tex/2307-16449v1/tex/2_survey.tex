\section{Related Works}

\subsection{Video Foundation Models}
Video foundation models have various applications on downstream tasks (\eg, video question answering~\cite{lei2018tvqa,lei2020tvqa+}, video captioning~\cite{iashin2020multi,yang2023vid2seq}, and human action recognition~\cite{poppe2010survey,kong2022human,qi2022weakly}). The common paradigm in the field of video foundation models is now characterized by the combination of extensive large-scale video-language pre-training, followed by fine-tuning on specific downstream tasks~\cite{miech2020end,li2020learning,xu2021videoclip,li2022uniformerv2,li2023unmasked,sun2019videobert,zhu2020actbert,wang2022internvideo}. Such paradigm depends on end-to-end video-language joint training with pretext pre-training tasks such as masked language modeling~\cite{li2023lavender}, masked video modeling~\cite{tongvideomae,wang2023videomae}, video-language masked modeling~\cite{fu2021violet}, video-text matching~\cite{wang2023all}, and video-text contrastive learning~\cite{xu2021videoclip}. These prior arts yield impressive performance in multimodal video tasks. Yet, they can only train with limited video-language pairs or videos without detailed annotations, which leads to difficulties in language-related tasks. With connecting to LLMs, video foundation models serve as visual encoder and achieve state-of-the-art performance in various tasks and become user-friendly.

\subsection{Multi-modal Large Language Models}
LLMs~\cite{gpt4,brown2020language,touvron2023llama,touvron2023llama2,chiang2023vicuna,taori2023stanford} achieves great success recently. Many works try to build MLLMs~\cite{yin2023survey,fu2023mme,alayrac2022flamingo,zhu2023minigpt,li2023otter,li2022blip,li2023blip,gong2023multimodal,lyu2023macaw,ye2023mplug,dai2023instructblip,wang2023visionllm,liu2023visual,maaz2023video,su2023pandagpt,gao2023llama} by combining models of other modalities.
Flamingo~\cite{alayrac2022flamingo} bridge powerful pretrained vision-only and language-only models and achieve state-of-the-art performance with few-shot learning, simply by prompting the model with task-specific examples.
BLIP-2~\cite{li2023blip} proposes a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models.
MiniGPT-4~\cite{zhu2023minigpt} aligns a frozen visual encoder with a frozen LLM, Vicuna~\cite{chiang2023vicuna}, using just one projection layer.
Otter~\cite{li2023otter} is trained on MIMIC-IT~\cite{li2023mimic} and showcasing improved instruction-following ability and in-context learning.
In video field, ChatVideo~\cite{wang2023chatvideo} treats tracklets as the basic video unit and allow user interacting with the LLMs.
VideoChat~\cite{li2023videochat} integrates video foundation models and LLMs via a learnable neural interface, excelling in spatiotemporal reasoning, event localization, and causal relationship inference. VideoChat Longvideo~\cite{videochatlong} further incorporates LangChain~\cite{langchain} into VideoChat to support video which more than one minutes.
Video-LLaMA~\cite{zhang2023video} further leverage pre-trained models ImageBind~\cite{girdhar2023imagebind} and LLaMA~\cite{touvron2023llama} bootstraping cross-modal training in video following BLIP-2.
Yet, those methods fails in handling long video understanding since high computation complexity, large memory cost, and weak long-term temporal connection. Therefore, our main efforts is to introduce memory mechanism to enhance those aspects.

\subsection{Long Video Understanding}
Understanding long videos is a challenging task in computer vision. Prior arts use 3D volumn~\cite{wu2019long}, object/human-centric~\cite{wu2021towards,rohrbach2017generating}, or other forms~\cite{wu2022memvit,sener2020temporal} as video representations. There are also several datasets of video-caption/description pairs among various domains such as cooking (\eg, YouCook~\cite{das2013thousand,zhou2018towards}, MPII Cooking~\cite{rohrbach2012database,rohrbach2012script,rohrbach2016recognizing}, and TACoS~\cite{regneri2013grounding,rohrbach2014coherent}), instruction (\eg, HowTo100M~\cite{miech2019howto100m} and HiREST~\cite{zala2023hierarchical}), and movie (\eg, MovieQA~\cite{tapaswi2016movieqa}, M-VAD~\cite{torabi2015using}, MPII-MD~\cite{rohrbach2015dataset}, and MovieNet~\cite{huang2020movienet}) from different sources such as YouTube~\cite{chen2011collecting,zeng2016title,miech2019howto100m}, Twitter~\cite{awad2017trecvid,awad2018trecvid,awad2020trecvid,awad2021trecvid}, Flirck~\cite{awad2020trecvid,awad2021trecvid}, and internet~\cite{bain2021frozen}. 
Yet, those datasets lack diverse and fine-grained dense captioning for long videos. % benchmark in the future

\input{fig/overview}

\subsection{Memory Models in Vision Tasks}
There are some prior works exploring memory models~\cite{squire2015memory} in various vision tasks in video, such as video object segmentation~(VOS)~\cite{cheng2022xmem,oh2019video,cheng2021rethinking,hu2021learning,lu2020video,mao2021joint,seong2020kernelized,seong2021hierarchical,xie2021efficient,ying2023ctvis}, multi-object tracking~(MOT)~\cite{cai2022memot,hao2022umotma,xin2022multi,fang2018recurrent,allen2006multiple} and visual object tracking~(VOT)~\cite{zhou2023memory,fu2021stmtrack,yang2018learning,liu2017mavot,ma2018adaptive}.
MeMOT~\cite{cai2022memot} build a large spatiotemporal memory that stores the past observations of the
tracked objects. 
XMem~\cite{cheng2022xmem} develop an architecture that incorporates multiple independent yet deeply-connected feature memory stores to handle long videos with thousands frames.
We drew the experience of those prior arts and further adopt memory model combining with LLMs. Unlike using embedded feature given by certain visual encoder, we found that using tokens in Transformers~\cite{vaswani2017attention} as the carriers of memory suitable for both LLMs and ViT~\cite{dosovitskiy2020image} based visual encoder. Our proposed method mainly focus on reducing the redundant of visual tokens in video and building a memory mechanism to pass the information among large temporal range.