\input{fig/case}
\section{Experiments}
\input{tab/qa}
\subsection{Quantitative Evaluation}
\label{exp:quantitative}

\paragraph{Short video question answering.} We conducted a comprehensive quantitative evaluation in this section. We use several widely used open-ended datasets: MSVD-QA~\cite{xu2017video}, MSRVTT-QA~\cite{xu2017video}, and ActivityNet-QA~\cite{yu2019activitynet} for short video question answering task. The evaluation process is under the assistant of LLM (details in Appendix~\ref{sec:gpt-eval}) under default hyper-parameter settings as shown in Appendix~\ref{sec:hyper-param}. The accuracy and relative score on a scale of $0$ to $5$ are reported. Compared to previous method~\cite{maaz2023video,li2023videochat,yang2022zero}, MovieChat achieves competitive result although there is no specific design for short video understanding.

\subsection{Case Study}
We perform an extensive case study of MovieChat on a variety of open-ended long video~(such as cartoon movie in and TV series) for long video question-answering and captioning task, including the \textcolor{global}{\textbf{global mode}} and the \textcolor{breakpoint}{\textbf{breakpoint mode}} as shown in Figure~\ref{fig:case}. For Q\#1 and Q\#2, we annotate timestamps in frames. For long videos over $10$K frames, MovieChat is still capable of providing excellent responses to questions regarding both the current moment and the entire video content.