\section{Introduction}

Recent advances in Large Language Models (LLMs)~\cite{gpt4,brown2020language,touvron2023llama,chiang2023vicuna,taori2023stanford} acheive great success in Natural Language Processing (NLP) field. It is a natural progression to introduce multi-modality~\cite{chai2022deep} into LLMs and turn it into Multi-modal Large Language Models (MLLMs)~\cite{yin2023survey,fu2023mme,alayrac2022flamingo,zhu2023minigpt,li2023otter,li2022blip,li2023blip,gong2023multimodal,lyu2023macaw,ye2023mplug,dai2023instructblip,wang2023visionllm,liu2023visual,maaz2023video,su2023pandagpt,gao2023llama}, which is able to conduct multimodal perception and understanding. MLLMs have shown incredible emergent capabilities in various multimodal tasks such as perception (\eg, existence, count, position, OCR), commonsense reasoning, and code reasoning, leading a potential path to Artificial General Intelligence (AGI). Compared to LLMs and other task-specific models, MLLMs  provide a more human-like perception of the world, a user-friendly interface for interaction, and a broader range of task-solving capabilities.

\input{fig/intro}

Existing vision-centric MLLMs follows the paradigm that utilizing pre-trained LLMs and visual encoder with additional learnable modules (Q-former~\cite{dai2023instructblip,li2022blip,li2023blip,zhang2023video} or simple projection layer~\cite{driess2023palm,maaz2023video,liu2023visual,su2023pandagpt}). In video field, some previous works~\cite{zhang2023video,maaz2023video} follow this paradigm to build video MLLMs, while another paradigm~\cite{wang2023chatvideo,li2023videochat} is that combining existing visual perception tools (\eg, tracking and classification) and LLMs through API to build a system without training. Yet, previously, there was no exploration of a model or system based on long videos (over one minute), and there was also a lack of a standardized benchmark to evaluate the capabilities of these systems in this regard.

In this paper, we present MovieChat, a novel framework that integrating vision models and LLMs to conduct long video understanding tasks. We claim that the computation complexity, memory cost, and long-term temporal connection are the remaining challenges for long video understanding. To this end, we propose a memory mechanism inspired by Atkinson-Shiffrin memory model~\cite{atkinson1968chapter}, which including a rapidly updated short-term memory and a compact thus sustained long-term memory.

% To be specific, 

% benchmark in the future

The contributions of this work are summarized as:

\begin{itemize}
    \item We present MovieChat, a novel framework that integrating vision models and LLMs, is the first to support long video understanding tasks.
    \item We propose a kind of memory mechanism to reduce the computation complexity and memory cost, while enhancing the long-term temporal connection.
    \item We conduct extensive quantitative evaluation and case studies to evaluate the performance of both understanding capability and inference cost. % need to be change to benchmark in the future
\end{itemize}