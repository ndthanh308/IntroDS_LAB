\begin{thebibliography}{100}\itemsep=-1pt

\bibitem{alayrac2022flamingo}
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr,
  Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds,
  et~al.
\newblock Flamingo: a visual language model for few-shot learning.
\newblock {\em Advances in Neural Information Processing Systems},
  35:23716--23736, 2022.

\bibitem{allen2006multiple}
Roy Allen, Peter Mcgeorge, David~G Pearson, and Alan Milne.
\newblock Multiple-target tracking: A role for working memory?
\newblock {\em Quarterly journal of experimental psychology}, 59(6):1101--1116,
  2006.

\bibitem{arnab2021vivit}
Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lu{\v{c}}i{\'c},
  and Cordelia Schmid.
\newblock Vivit: A video vision transformer.
\newblock In {\em Proceedings of the IEEE/CVF international conference on
  computer vision}, pages 6836--6846, 2021.

\bibitem{atkinson1968chapter}
Richard~C Atkinson and Richard~M Shiffrin.
\newblock Chapter: Human memory: A proposed system and its control processes.
\newblock {\em The psychology of learning and motivation}, 2:89--195, 1968.

\bibitem{awad2021trecvid}
George Awad, Asad~A Butt, Keith Curtis, Jonathan Fiscus, Afzal Godil, Yooyoung
  Lee, Andrew Delgado, Jesse Zhang, Eliot Godard, Baptiste Chocot, et~al.
\newblock Trecvid 2020: A comprehensive campaign for evaluating video retrieval
  tasks across multiple application domains.
\newblock {\em arXiv preprint arXiv:2104.13473}, 2021.

\bibitem{awad2020trecvid}
George Awad, Asad~A Butt, Keith Curtis, Yooyoung Lee, Jonathan Fiscus, Afzal
  Godil, Andrew Delgado, Jesse Zhang, Eliot Godard, Lukas Diduch, et~al.
\newblock Trecvid 2019: An evaluation campaign to benchmark video activity
  detection, video captioning and matching, and video search \& retrieval.
\newblock {\em arXiv preprint arXiv:2009.09984}, 2020.

\bibitem{awad2018trecvid}
George Awad, Asad~A Butt, Keith Curtis, Yooyoung Lee, Jonathan Fiscus, Afzad
  Godil, David Joy, Andrew Delgado, Alan~F Smeaton, Yvette Graham, et~al.
\newblock Trecvid 2018: Benchmarking video activity detection, video captioning
  and matching, video storytelling linking and video search.
\newblock In {\em Proceedings of TRECVID 2018}, 2018.

\bibitem{awad2017trecvid}
George Awad, Asad~A Butt, Jonathan Fiscus, David Joy, Andrew Delgado, Willie
  Mcclinton, Martial Michel, Alan~F Smeaton, Yvette Graham, Wessel Kraaij,
  et~al.
\newblock Trecvid 2017: evaluating ad-hoc and instance video search, events
  detection, video captioning, and hyperlinking.
\newblock In {\em TREC Video Retrieval Evaluation (TRECVID)}, 2017.

\bibitem{bain2021frozen}
Max Bain, Arsha Nagrani, G{\"u}l Varol, and Andrew Zisserman.
\newblock Frozen in time: A joint video and image encoder for end-to-end
  retrieval.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 1728--1738, 2021.

\bibitem{bolya2022token}
Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph
  Feichtenhofer, and Judy Hoffman.
\newblock Token merging: Your vit but faster.
\newblock In {\em The Eleventh International Conference on Learning
  Representations}, 2022.

\bibitem{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock {\em Advances in neural information processing systems},
  33:1877--1901, 2020.

\bibitem{cai2022memot}
Jiarui Cai, Mingze Xu, Wei Li, Yuanjun Xiong, Wei Xia, Zhuowen Tu, and Stefano
  Soatto.
\newblock Memot: multi-object tracking with memory.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 8090--8100, 2022.

\bibitem{chai2022deep}
Wenhao Chai and Gaoang Wang.
\newblock Deep vision multimodal learning: Methodology, benchmark, and trend.
\newblock {\em Applied Sciences}, 12(13):6588, 2022.

\bibitem{chen2011collecting}
David Chen and William~B Dolan.
\newblock Collecting highly parallel data for paraphrase evaluation.
\newblock In {\em Proceedings of the 49th annual meeting of the association for
  computational linguistics: human language technologies}, pages 190--200,
  2011.

\bibitem{cheng2022xmem}
Ho~Kei Cheng and Alexander~G Schwing.
\newblock Xmem: Long-term video object segmentation with an atkinson-shiffrin
  memory model.
\newblock In {\em Computer Vision--ECCV 2022: 17th European Conference, Tel
  Aviv, Israel, October 23--27, 2022, Proceedings, Part XXVIII}, pages
  640--658. Springer, 2022.

\bibitem{cheng2021rethinking}
Ho~Kei Cheng, Yu-Wing Tai, and Chi-Keung Tang.
\newblock Rethinking space-time networks with improved memory coverage for
  efficient video object segmentation.
\newblock {\em Advances in Neural Information Processing Systems},
  34:11781--11794, 2021.

\bibitem{chiang2023vicuna}
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin
  Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph~E Gonzalez, et~al.
\newblock Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt
  quality.
\newblock {\em See https://vicuna. lmsys. org (accessed 14 April 2023)}, 2023.

\bibitem{dai2023instructblip}
Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng~Huat Tiong, Junqi Zhao,
  Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi.
\newblock Instructblip: Towards general-purpose vision-language models with
  instruction tuning.
\newblock {\em arXiv preprint arXiv:2305.06500}, 2023.

\bibitem{das2013thousand}
Pradipto Das, Chenliang Xu, Richard~F Doell, and Jason~J Corso.
\newblock A thousand frames in just a few words: Lingual description of videos
  through latent topics and sparse object stitching.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 2634--2641, 2013.

\bibitem{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock {\em arXiv preprint arXiv:2010.11929}, 2020.

\bibitem{driess2023palm}
Danny Driess, Fei Xia, Mehdi~SM Sajjadi, Corey Lynch, Aakanksha Chowdhery,
  Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et~al.
\newblock Palm-e: An embodied multimodal language model.
\newblock {\em arXiv preprint arXiv:2303.03378}, 2023.

\bibitem{fang2018recurrent}
Kuan Fang, Yu Xiang, Xiaocheng Li, and Silvio Savarese.
\newblock Recurrent autoregressive networks for online multi-object tracking.
\newblock In {\em 2018 IEEE Winter Conference on Applications of Computer
  Vision (WACV)}, pages 466--475. IEEE, 2018.

\bibitem{fang2022eva}
Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun
  Huang, Xinlong Wang, and Yue Cao.
\newblock Eva: Exploring the limits of masked visual representation learning at
  scale.
\newblock 2022.

\bibitem{fu2023mme}
Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin,
  Zhenyu Qiu, Wei Lin, Zhenyu Qiu, Wei Lin, et~al.
\newblock Mme: A comprehensive evaluation benchmark for multimodal large
  language models.
\newblock {\em arXiv preprint arXiv:2306.13394}, 2023.

\bibitem{fu2021violet}
Tsu-Jui Fu, Linjie Li, Zhe Gan, Kevin Lin, William~Yang Wang, Lijuan Wang, and
  Zicheng Liu.
\newblock Violet: End-to-end video-language transformers with masked
  visual-token modeling.
\newblock {\em arXiv preprint arXiv:2111.12681}, 2021.

\bibitem{fu2021stmtrack}
Zhihong Fu, Qingjie Liu, Zehua Fu, and Yunhong Wang.
\newblock Stmtrack: Template-free visual tracking with space-time memory
  networks.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 13774--13783, 2021.

\bibitem{gao2023llama}
Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei
  Zhang, Pan Lu, Conghui He, Xiangyu Yue, et~al.
\newblock Llama-adapter v2: Parameter-efficient visual instruction model.
\newblock {\em arXiv preprint arXiv:2304.15010}, 2023.

\bibitem{girdhar2023imagebind}
Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan~Vasudev
  Alwala, Armand Joulin, and Ishan Misra.
\newblock Imagebind: One embedding space to bind them all.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 15180--15190, 2023.

\bibitem{gong2023multimodal}
Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao,
  Kuikun Liu, Wenwei Zhang, Ping Luo, and Kai Chen.
\newblock Multimodal-gpt: A vision and language model for dialogue with humans.
\newblock {\em arXiv preprint arXiv:2305.04790}, 2023.

\bibitem{hao2022umotma}
Zhicheng Hao, Jun Qiu, Haimiao Zhang, Guangbo Ren, and Chang Liu.
\newblock Umotma: Underwater multiple object tracking with memory aggregation.
\newblock {\em Frontiers in Marine Science}, 9:1071618, 2022.

\bibitem{hu2021learning}
Li Hu, Peng Zhang, Bang Zhang, Pan Pan, Yinghui Xu, and Rong Jin.
\newblock Learning position and target consistency for memory-based video
  object segmentation.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 4144--4154, 2021.

\bibitem{huang2020movienet}
Qingqiu Huang, Yu Xiong, Anyi Rao, Jiaze Wang, and Dahua Lin.
\newblock Movienet: A holistic dataset for movie understanding.
\newblock In {\em Computer Vision--ECCV 2020: 16th European Conference,
  Glasgow, UK, August 23--28, 2020, Proceedings, Part IV 16}, pages 709--727.
  Springer, 2020.

\bibitem{langchain}
hwchase17.
\newblock langchain.
\newblock \url{https://github.com/hwchase17/langchain}, 2023.

\bibitem{iashin2020multi}
Vladimir Iashin and Esa Rahtu.
\newblock Multi-modal dense video captioning.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition Workshops}, pages 958--959, 2020.

\bibitem{kenton2019bert}
Jacob Devlin Ming-Wei~Chang Kenton and Lee~Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In {\em Proceedings of NAACL-HLT}, pages 4171--4186, 2019.

\bibitem{kong2022human}
Yu Kong and Yun Fu.
\newblock Human action recognition and prediction: A survey.
\newblock {\em International Journal of Computer Vision}, 130(5):1366--1401,
  2022.

\bibitem{lei2018tvqa}
Jie Lei, Licheng Yu, Mohit Bansal, and Tamara Berg.
\newblock Tvqa: Localized, compositional video question answering.
\newblock In {\em Proceedings of the 2018 Conference on Empirical Methods in
  Natural Language Processing}, pages 1369--1379, 2018.

\bibitem{lei2020tvqa+}
Jie Lei, Licheng Yu, Tamara Berg, and Mohit Bansal.
\newblock Tvqa+: Spatio-temporal grounding for video question answering.
\newblock In {\em Proceedings of the 58th Annual Meeting of the Association for
  Computational Linguistics}, pages 8211--8225, 2020.

\bibitem{li2023mimic}
Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu, Jingkang Yang,
  Chunyuan Li, and Ziwei Liu.
\newblock Mimic-it: Multi-modal in-context instruction tuning.
\newblock {\em arXiv preprint arXiv:2306.05425}, 2023.

\bibitem{li2023otter}
Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu.
\newblock Otter: A multi-modal model with in-context instruction tuning.
\newblock {\em arXiv preprint arXiv:2305.03726}, 2023.

\bibitem{li2023blip}
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
\newblock Blip-2: Bootstrapping language-image pre-training with frozen image
  encoders and large language models.
\newblock {\em arXiv preprint arXiv:2301.12597}, 2023.

\bibitem{li2023blip2}
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
\newblock Blip-2: Bootstrapping language-image pre-training with frozen image
  encoders and large language models.
\newblock 2023.

\bibitem{li2022blip}
Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
\newblock Blip: Bootstrapping language-image pre-training for unified
  vision-language understanding and generation.
\newblock In {\em International Conference on Machine Learning}, pages
  12888--12900. PMLR, 2022.

\bibitem{li2023videochat}
KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang,
  Limin Wang, and Yu Qiao.
\newblock Videochat: Chat-centric video understanding.
\newblock {\em arXiv preprint arXiv:2305.06355}, 2023.

\bibitem{li2022uniformerv2}
Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Limin Wang, and Yu Qiao.
\newblock Uniformerv2: Spatiotemporal learning by arming image vits with video
  uniformer.
\newblock {\em arXiv preprint arXiv:2211.09552}, 2022.

\bibitem{li2023unmasked}
Kunchang Li, Yali Wang, Yizhuo Li, Yi Wang, Yinan He, Limin Wang, and Yu Qiao.
\newblock Unmasked teacher: Towards training-efficient video foundation models.
\newblock {\em arXiv preprint arXiv:2303.16058}, 2023.

\bibitem{li2023lavender}
Linjie Li, Zhe Gan, Kevin Lin, Chung-Ching Lin, Zicheng Liu, Ce Liu, and Lijuan
  Wang.
\newblock Lavender: Unifying video-language understanding as masked language
  modeling.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 23119--23129, 2023.

\bibitem{li2020learning}
Tianhao Li and Limin Wang.
\newblock Learning spatiotemporal features via video and text pair
  discrimination.
\newblock {\em arXiv preprint arXiv:2001.05691}, 2020.

\bibitem{liu2017mavot}
Boyu Liu, Yanzhao Wang, Yu-Wing Tai, and Chi-Keung Tang.
\newblock Mavot: Memory-augmented video object tracking.
\newblock {\em arXiv preprint arXiv:1711.09414}, 2017.

\bibitem{liu2023visual}
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong~Jae Lee.
\newblock Visual instruction tuning.
\newblock {\em arXiv preprint arXiv:2304.08485}, 2023.

\bibitem{liu2022video}
Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu.
\newblock Video swin transformer.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and
  pattern recognition}, pages 3202--3211, 2022.

\bibitem{lu2020video}
Xiankai Lu, Wenguan Wang, Martin Danelljan, Tianfei Zhou, Jianbing Shen, and
  Luc Van~Gool.
\newblock Video object segmentation with episodic graph memory networks.
\newblock In {\em Computer Vision--ECCV 2020: 16th European Conference,
  Glasgow, UK, August 23--28, 2020, Proceedings, Part III 16}, pages 661--679.
  Springer, 2020.

\bibitem{lyu2023macaw}
Chenyang Lyu, Minghao Wu, Longyue Wang, Xinting Huang, Bingshuai Liu, Zefeng
  Du, Shuming Shi, and Zhaopeng Tu.
\newblock Macaw-llm: Multi-modal language modeling with image, audio, video,
  and text integration.
\newblock {\em arXiv preprint arXiv:2306.09093}, 2023.

\bibitem{ma2018adaptive}
Chao Ma, Jia-Bin Huang, Xiaokang Yang, and Ming-Hsuan Yang.
\newblock Adaptive correlation filters with long-term and short-term memory for
  object tracking.
\newblock {\em International Journal of Computer Vision}, 126:771--796, 2018.

\bibitem{maaz2023video}
Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad~Shahbaz Khan.
\newblock Video-chatgpt: Towards detailed video understanding via large vision
  and language models.
\newblock {\em arXiv preprint arXiv:2306.05424}, 2023.

\bibitem{mao2021joint}
Yunyao Mao, Ning Wang, Wengang Zhou, and Houqiang Li.
\newblock Joint inductive and transductive learning for video object
  segmentation.
\newblock In {\em Proceedings of the IEEE/CVF international conference on
  computer vision}, pages 9670--9679, 2021.

\bibitem{miech2020end}
Antoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, Ivan Laptev, Josef Sivic,
  and Andrew Zisserman.
\newblock End-to-end learning of visual representations from uncurated
  instructional videos.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 9879--9889, 2020.

\bibitem{miech2019howto100m}
Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan
  Laptev, and Josef Sivic.
\newblock Howto100m: Learning a text-video embedding by watching hundred
  million narrated video clips.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 2630--2640, 2019.

\bibitem{oh2019video}
Seoung~Wug Oh, Joon-Young Lee, Ning Xu, and Seon~Joo Kim.
\newblock Video object segmentation using space-time memory networks.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 9226--9235, 2019.

\bibitem{gpt4}
OpenAI.
\newblock Gpt-4 technical report.
\newblock {\em arXiv preprint arXiv:2303.08774}, 2023.

\bibitem{videochatlong}
OpenGVLab.
\newblock Ask-anything.
\newblock
  \url{https://github.com/OpenGVLab/Ask-Anything/tree/long_video_support},
  2023.

\bibitem{poppe2010survey}
Ronald Poppe.
\newblock A survey on vision-based human action recognition.
\newblock {\em Image and vision computing}, 28(6):976--990, 2010.

\bibitem{qi2022weakly}
Zhenting Qi, Ruike Zhu, Zheyu Fu, Wenhao Chai, and Volodymyr Kindratenko.
\newblock Weakly supervised two-stage training scheme for deep video fight
  detection model.
\newblock {\em arXiv preprint arXiv:2209.11477}, 2022.

\bibitem{regneri2013grounding}
Michaela Regneri, Marcus Rohrbach, Dominikus Wetzel, Stefan Thater, Bernt
  Schiele, and Manfred Pinkal.
\newblock Grounding action descriptions in videos.
\newblock {\em Transactions of the Association for Computational Linguistics},
  1:25--36, 2013.

\bibitem{rohrbach2014coherent}
Anna Rohrbach, Marcus Rohrbach, Wei Qiu, Annemarie Friedrich, Manfred Pinkal,
  and Bernt Schiele.
\newblock Coherent multi-sentence video description with variable level of
  detail.
\newblock In {\em Pattern Recognition: 36th German Conference, GCPR 2014,
  M{\"u}nster, Germany, September 2-5, 2014, Proceedings 36}, pages 184--195.
  Springer, 2014.

\bibitem{rohrbach2015dataset}
Anna Rohrbach, Marcus Rohrbach, Niket Tandon, and Bernt Schiele.
\newblock A dataset for movie description.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 3202--3212, 2015.

\bibitem{rohrbach2017generating}
Anna Rohrbach, Marcus Rohrbach, Siyu Tang, Seong Joon~Oh, and Bernt Schiele.
\newblock Generating descriptions with grounded and co-referenced people.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 4979--4989, 2017.

\bibitem{rohrbach2012database}
Marcus Rohrbach, Sikandar Amin, Mykhaylo Andriluka, and Bernt Schiele.
\newblock A database for fine grained activity detection of cooking activities.
\newblock In {\em 2012 IEEE conference on computer vision and pattern
  recognition}, pages 1194--1201. IEEE, 2012.

\bibitem{rohrbach2012script}
Marcus Rohrbach, Michaela Regneri, Mykhaylo Andriluka, Sikandar Amin, Manfred
  Pinkal, and Bernt Schiele.
\newblock Script data for attribute-based recognition of composite activities.
\newblock In {\em Computer Vision--ECCV 2012: 12th European Conference on
  Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part I
  12}, pages 144--157. Springer, 2012.

\bibitem{rohrbach2016recognizing}
Marcus Rohrbach, Anna Rohrbach, Michaela Regneri, Sikandar Amin, Mykhaylo
  Andriluka, Manfred Pinkal, and Bernt Schiele.
\newblock Recognizing fine-grained and composite activities using hand-centric
  features and script data.
\newblock {\em International Journal of Computer Vision}, 119:346--373, 2016.

\bibitem{sener2020temporal}
Fadime Sener, Dipika Singhania, and Angela Yao.
\newblock Temporal aggregate representations for long-range video
  understanding.
\newblock In {\em Computer Vision--ECCV 2020: 16th European Conference,
  Glasgow, UK, August 23--28, 2020, Proceedings, Part XVI 16}, pages 154--171.
  Springer, 2020.

\bibitem{seong2020kernelized}
Hongje Seong, Junhyuk Hyun, and Euntai Kim.
\newblock Kernelized memory network for video object segmentation.
\newblock In {\em Computer Vision--ECCV 2020: 16th European Conference,
  Glasgow, UK, August 23--28, 2020, Proceedings, Part XXII 16}, pages 629--645.
  Springer, 2020.

\bibitem{seong2021hierarchical}
Hongje Seong, Seoung~Wug Oh, Joon-Young Lee, Seongwon Lee, Suhyeon Lee, and
  Euntai Kim.
\newblock Hierarchical memory matching network for video object segmentation.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 12889--12898, 2021.

\bibitem{squire2015memory}
Larry~R Squire, Lisa Genzel, John~T Wixted, and Richard~G Morris.
\newblock Memory consolidation.
\newblock {\em Cold Spring Harbor perspectives in biology}, 7(8):a021766, 2015.

\bibitem{pos}
Jianlin Su.
\newblock Bert position encoding.
\newblock \url{https://kexue.fm/archives/7947}, 2023.

\bibitem{su2023pandagpt}
Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai.
\newblock Pandagpt: One model to instruction-follow them all.
\newblock {\em arXiv preprint arXiv:2305.16355}, 2023.

\bibitem{sun2019videobert}
Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid.
\newblock Videobert: A joint model for video and language representation
  learning.
\newblock In {\em Proceedings of the IEEE/CVF international conference on
  computer vision}, pages 7464--7473, 2019.

\bibitem{taori2023stanford}
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos
  Guestrin, Percy Liang, and Tatsunori~B Hashimoto.
\newblock Stanford alpaca: An instruction-following llama model, 2023.

\bibitem{tapaswi2016movieqa}
Makarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen, Antonio Torralba, Raquel
  Urtasun, and Sanja Fidler.
\newblock Movieqa: Understanding stories in movies through question-answering.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 4631--4640, 2016.

\bibitem{tongvideomae}
Zhan Tong, Yibing Song, Jue Wang, and Limin Wang.
\newblock Videomae: Masked autoencoders are data-efficient learners for
  self-supervised video pre-training.
\newblock In {\em Advances in Neural Information Processing Systems}.

\bibitem{torabi2015using}
Atousa Torabi, Christopher Pal, Hugo Larochelle, and Aaron Courville.
\newblock Using descriptive video services to create a large data source for
  video annotation research.
\newblock {\em arXiv preprint arXiv:1503.01070}, 2015.

\bibitem{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
  Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric
  Hambro, Faisal Azhar, et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock {\em arXiv preprint arXiv:2302.13971}, 2023.

\bibitem{touvron2023llama2}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine
  Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
  et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock {\em arXiv preprint arXiv:2307.09288}, 2023.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{wang2023chatvideo}
Junke Wang, Dongdong Chen, Chong Luo, Xiyang Dai, Lu Yuan, Zuxuan Wu, and
  Yu-Gang Jiang.
\newblock Chatvideo: A tracklet-centric multimodal and versatile video
  understanding system.
\newblock {\em arXiv preprint arXiv:2304.14407}, 2023.

\bibitem{wang2023all}
Jinpeng Wang, Yixiao Ge, Rui Yan, Yuying Ge, Kevin~Qinghong Lin, Satoshi
  Tsutsui, Xudong Lin, Guanyu Cai, Jianping Wu, Ying Shan, et~al.
\newblock All in one: Exploring unified video-language pre-training.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 6598--6608, 2023.

\bibitem{wang2023videomae}
Limin Wang, Bingkun Huang, Zhiyu Zhao, Zhan Tong, Yinan He, Yi Wang, Yali Wang,
  and Yu Qiao.
\newblock Videomae v2: Scaling video masked autoencoders with dual masking.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 14549--14560, 2023.

\bibitem{wang2023visionllm}
Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping
  Luo, Tong Lu, Jie Zhou, Yu Qiao, et~al.
\newblock Visionllm: Large language model is also an open-ended decoder for
  vision-centric tasks.
\newblock {\em arXiv preprint arXiv:2305.11175}, 2023.

\bibitem{wang2022internvideo}
Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie
  Zhang, Jilan Xu, Yi Liu, Zun Wang, et~al.
\newblock Internvideo: General video foundation models via generative and
  discriminative learning.
\newblock {\em arXiv preprint arXiv:2212.03191}, 2022.

\bibitem{wu2019long}
Chao-Yuan Wu, Christoph Feichtenhofer, Haoqi Fan, Kaiming He, Philipp
  Krahenbuhl, and Ross Girshick.
\newblock Long-term feature banks for detailed video understanding.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 284--293, 2019.

\bibitem{wu2021towards}
Chao-Yuan Wu and Philipp Krahenbuhl.
\newblock Towards long-form video understanding.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 1884--1894, 2021.

\bibitem{wu2022memvit}
Chao-Yuan Wu, Yanghao Li, Karttikeya Mangalam, Haoqi Fan, Bo Xiong, Jitendra
  Malik, and Christoph Feichtenhofer.
\newblock Memvit: Memory-augmented multiscale vision transformer for efficient
  long-term video recognition.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 13587--13597, 2022.

\bibitem{xie2021efficient}
Haozhe Xie, Hongxun Yao, Shangchen Zhou, Shengping Zhang, and Wenxiu Sun.
\newblock Efficient regional memory network for video object segmentation.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 1286--1295, 2021.

\bibitem{xin2022multi}
Ming Xin, Wenjie Sun, Kaifang Li, and Guancheng Hui.
\newblock Multi-object tracking with spatial-temporal correlation memory
  networks.
\newblock In {\em 2022 3rd International Conference on Computer Vision, Image
  and Deep Learning \& International Conference on Computer Engineering and
  Applications (CVIDL \& ICCEA)}, pages 616--619. IEEE, 2022.

\bibitem{xu2017video}
Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting
  Zhuang.
\newblock Video question answering via gradually refined attention over
  appearance and motion.
\newblock In {\em Proceedings of the 25th ACM international conference on
  Multimedia}, pages 1645--1653, 2017.

\bibitem{xu2021videoclip}
Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko, Armen Aghajanyan, Florian
  Metze, Luke Zettlemoyer, and Christoph Feichtenhofer.
\newblock Videoclip: Contrastive pre-training for zero-shot video-text
  understanding.
\newblock In {\em Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pages 6787--6800, 2021.

\bibitem{yang2022zero}
Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid.
\newblock Zero-shot video question answering via frozen bidirectional language
  models.
\newblock {\em Advances in Neural Information Processing Systems}, 35:124--141,
  2022.

\bibitem{yang2023vid2seq}
Antoine Yang, Arsha Nagrani, Paul~Hongsuck Seo, Antoine Miech, Jordi
  Pont-Tuset, Ivan Laptev, Josef Sivic, and Cordelia Schmid.
\newblock Vid2seq: Large-scale pretraining of a visual language model for dense
  video captioning.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 10714--10726, 2023.

\bibitem{yang2018learning}
Tianyu Yang and Antoni~B Chan.
\newblock Learning dynamic memory networks for object tracking.
\newblock In {\em Proceedings of the European conference on computer vision
  (ECCV)}, pages 152--167, 2018.

\bibitem{ye2023mplug}
Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang
  Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et~al.
\newblock mplug-owl: Modularization empowers large language models with
  multimodality.
\newblock {\em arXiv preprint arXiv:2304.14178}, 2023.

\bibitem{yin2023survey}
Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen.
\newblock A survey on multimodal large language models.
\newblock {\em arXiv preprint arXiv:2306.13549}, 2023.

\bibitem{ying2023ctvis}
Kaining Ying, Qing Zhong, Weian Mao, Zhenhua Wang, Hao Chen, Lin~Yuanbo Wu,
  Yifan Liu, Chengxiang Fan, Yunzhi Zhuge, and Chunhua Shen.
\newblock Ctvis: Consistent training for online video instance segmentation.
\newblock {\em arXiv preprint arXiv:2307.12616}, 2023.

\bibitem{yu2019activitynet}
Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng
  Tao.
\newblock Activitynet-qa: A dataset for understanding complex web videos via
  question answering.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pages 9127--9134, 2019.

\bibitem{zala2023hierarchical}
Abhay Zala, Jaemin Cho, Satwik Kottur, Xilun Chen, Barlas Oguz, Yashar Mehdad,
  and Mohit Bansal.
\newblock Hierarchical video-moment retrieval and step-captioning.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 23056--23065, 2023.

\bibitem{zeng2016title}
Kuo-Hao Zeng, Tseng-Hung Chen, Juan~Carlos Niebles, and Min Sun.
\newblock Title generation for user generated videos.
\newblock In {\em Computer Vision--ECCV 2016: 14th European Conference,
  Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14},
  pages 609--625. Springer, 2016.

\bibitem{zhang2023video}
Hang Zhang, Xin Li, and Lidong Bing.
\newblock Video-llama: An instruction-tuned audio-visual language model for
  video understanding.
\newblock {\em arXiv preprint arXiv:2306.02858}, 2023.

\bibitem{zhang2023llama}
Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu,
  Hongsheng Li, Peng Gao, and Yu Qiao.
\newblock Llama-adapter: Efficient fine-tuning of language models with
  zero-init attention.
\newblock {\em arXiv preprint arXiv:2303.16199}, 2023.

\bibitem{zhou2018towards}
Luowei Zhou, Chenliang Xu, and Jason Corso.
\newblock Towards automatic learning of procedures from web instructional
  videos.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~32, 2018.

\bibitem{zhou2023memory}
Zechu Zhou, Xinyu Zhou, Zhaoyu Chen, Pinxue Guo, Qian-Yu Liu, and Wenqiang
  Zhang.
\newblock Memory network with pixel-level spatio-temporal learning for visual
  object tracking.
\newblock {\em IEEE Transactions on Circuits and Systems for Video Technology},
  2023.

\bibitem{zhu2023minigpt}
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.
\newblock Minigpt-4: Enhancing vision-language understanding with advanced
  large language models.
\newblock {\em arXiv preprint arXiv:2304.10592}, 2023.

\bibitem{zhu2020actbert}
Linchao Zhu and Yi Yang.
\newblock Actbert: Learning global-local video-text representations.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and
  pattern recognition}, pages 8746--8755, 2020.

\end{thebibliography}
