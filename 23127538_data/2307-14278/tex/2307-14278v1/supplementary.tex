\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{xcolor}
\usepackage{color}
\usepackage{multirow}

\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\definecolor{green(html/cssgreen)}{rgb}{0.0, 0.5, 0.0}

\newcommand{\rv}[1]{\textcolor{red}{#1}}
\newcommand{\ov}[1]{\textcolor{orange}{#1}}
\newcommand{\bv}[1]{\textcolor{blue}{#1}}
\newcommand{\gv}[1]{\textcolor{green(html/cssgreen)}{#1}}

\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021

\begin{document}

\title{Supplementary Material: \\ Large-scale Fully-Unsupervised Re-Identification}

\author{Gabriel~Bertocco,
        Fernanda~Andal\'{o},~\IEEEmembership{Member,~IEEE,}
        Terrance~Boult,~\IEEEmembership{Fellow~Member,~IEEE,} and~Anderson~Rocha,~\IEEEmembership{Senior~Member,~IEEE}}
        % <-this % stops a space


% The paper headers
\markboth{Paper submitted for a possible publication in an IEEE Transactions}%
{Bertocco \MakeLowercase{\textit{et al.}}: Large-scale Fully-Unsupervised Re-Identification}

%\IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

\maketitle

Here we present further analysis and details about the method proposed in the main article. More specifically, we present a brief theoretical overview of Barlow Twins and its implementation details. We also show results considering R5 and R10 for Person Re-Identification datasets, and R5 for Vehicle Re-Identification datasets. We also provide analysis for the simpler evaluation scenarios of \texttt{VehicleID} and \texttt{Veri-Wild}. We extend our comparison to methods that use side information, such as camera labels or viewpoint information. Finally, we also show further ablation studies with Barlow Twins and the parameters $\tau$ and $\lambda$ used in the final loss function. 

\section{Theory and details on Barlow Twins}
\label{sec:barlowtwins}

In this section, we provide further explanation about Barlow Twins, the method used for the self-supervised initialization of the backbones before employing our proposed pipeline.  

Given a batch of randomly selected images $B \in X$, the algorithm generates two augmentation views for each image through some common image transformations, resulting in two new augmented batches $B^{1}$ and $B^{2}$. In our case, some image transformations might degrade Re-Identification performance, so we consider just random cropping, horizontal flipping, and shifts in brightness, contrast, and saturation. After that, we feed both batches to the backbone $f$ and get the feature representations $Z^{1}, Z^{2} \in R^{N \times d}$, respectively, for each batch, where $d$ is the feature dimension. The features are normalized by mean subtraction and diving by the standard deviation per dimension. After that, we train the model to achieve maximum decorrelation between the dimensions to encourage the model to learn complementary features and, at the same time, be robust among different augmentations. To do so, the algorithm performs the multiplication of batch features, $C = (Z^{1})^{T}Z^{2} \in R^{d \times d}$, to calculate the cross-correlation among the different dimensions of the feature vectors. The next step is to maximize the agreement between features in the same dimension (represented by values in the main diagonal of $C$) and minimize it between features from different dimensions through the following loss function:
\begin{equation}
\label{eq:barlow_twins_equation}
L_{BT} = \sum_{i}(1-C_{ii})^2 + \lambda_{BT}\sum_{i}\sum_{j\neq i}C_{ij}^2,
\end{equation}
\noindent where the first term aims to increase the invariance representation between different augmentation views of the same image, and the second decouples feature representation considering the dimensions; and $\lambda_{BT}$ weights the contribution of the second term in the loss. For further details, we refer to the original article~\cite{zbontar2021barlow}.

%The labels $Y(f,D,\varepsilon) = \{y_{i}\}_{i=1}^{N}$ also depend on the backbone $f$, so we consider three different backbones ($f_{1}, f_{2}, f_{3}$) and perform self-supervised pre-training in each one is initialized independently by doing three epochs of pre-training, then we fine-tuned them with the proposed pipeline. We are the first to proposed a Barlow Twins-based initialization for Person Re-Identification considering three different backbones before employing the proposed unsupervised learning pipeline. 

\section{Implementation Details}

We perform the self-supervised initialization with Barlow Twins for three epochs. The setup for each experiment may vary due to the availability of GPUs and memory constraints, as the datasets have different sizes. For \texttt{MSMT17}, \texttt{Veri776}, and \texttt{Veri-Wild}, we used five NVIDIA RTX A6000 with 49GB of RAM, and a batch size of 1024. For \texttt{Market} and \texttt{VehicleID}, we keep the same setup, except that for \texttt{Market}, we train OSNet in three Quadro RTX 8000 with a batch size of 768, and for \texttt{VehicleID}, we set the batch size to 768 for OSNet and DenseNet121. The Adam~\cite{kingma2014adam} optimizer was employed with a learning rate set to $3.5e-5$ for the Person ReID datasets and $3.5e-6$ for Vehicle ReID datasets. The weight decay was set to $1.5e-6$, and the $\lambda_{BT}$ in Equation~\ref{eq:barlow_twins_equation} was set to $5e-3$ for all datasets following the original implementation~\cite{zbontar2021barlow}. 

\section{Impact of Pre-training with Barlow Twins}

The impact of employing the self-supervised pre-training with Barlow Twins (BT) is shown in Table~\ref{tab:ablation_study}. We compare this initialization against simple pre-training with ImageNet. Pre-training with BT has proved crucial. For \texttt{Market}, our method produces state-of-the-art results when the self-supervised initialization is used, but yields degenerated clusters in the first epochs when BT is not used. For \texttt{Veri776}, results are similar in both scenarios; however, for \texttt{MSMT17}, mAP and R1 increase by $4.7$ and $4.2$, respectively, when BT is considered. 

\begin{table}[ht]
\caption{Comparison between our model with and without pre-training with Barlow Twins (BT)}
\centering
\begin{tabular}{P{0.1cm}|P{0.25cm}|P{0.45cm}|P{0.4cm}|P{0.45cm}|P{0.4cm}|P{0.45cm}|P{0.4cm}}
\multicolumn{2}{c}{} &
\multicolumn{2}{|c|}{\textbf{Market}} &
\multicolumn{2}{|c}{\textbf{MSMT17}} &
\multicolumn{2}{|c}{\textbf{Veri776}} \\ \hline
& BT & mAP & R1 & mAP & R1 & mAP & R1 \\ \hline
%\multirow{4}{*}{LRR} & \#4 & \checkmark  &  & & \rv{\underline{86.4}} & \rv{\underline{94.1}} & \rv{\underline{42.5}} & \rv{\underline{69.3}} & \rv{\underline{41.6}} & \rv{\underline{85.1}} \\
%& \#5 &\checkmark &\checkmark & & 84.8 & 93.3 & 38.8 & 67.1 & 39.8 & 82.8\\
\#1 & & - & - & 38.5 & 66.7 & 41.3 &  86.8\\
\#2 & \checkmark & 85.8 & 94.0 & 43.2 & 70.9 & 41.3 & 86.3\\
\hline
\end{tabular}
\label{tab:ablation_study}
\end{table}

\section{Impact of loss hyper-parameters}

In this section, we verify the impact of the hyper-parameters $\tau$ and $\lambda$ in the loss function and its terms (Equations 9, 10, and 11 in the main article).
%\begin{equation}
%\label{eq:final_loss}
%   L_{final} = L_{proxy} + \lambda L_{hard}.
%\end{equation}

The $\tau$ parameter has the goal of changing the distribution of the scores, which allows smoother gradients to aid the optimization. Its impact is verified in Figure~\ref{fig:tau_sensitivity} for the \texttt{Market} and \texttt{Veri776} datasets. We see that mAP and R1 reach their maximum when $\tau = 0.04$. After that, performance starts to decrease rapidly, for both datasets. This happens because the gradients increase together with  $\tau$, causing instability during training. Therefore, we set $\tau = 0.04$ for all datasets.

% Figure environment removed

The $\lambda$ value weights the contribution of the batch-hard triplet loss ($L_{hard}$) in the final loss function. While $L_{proxy}$ is a more global loss term since it considers all class proxies for optimization, $L_{hard}$ enforces a more local view since the hard triplets are mined at the batch level. That is, $\lambda = 0.0$ means that there is no local contribution, while a too-large value might make $L_{hard}$ dominant over $L_{proxy}$ and hinder model optimization. The impact of the $\lambda$ value is shown in Figure~\ref{fig:lambda_sensitivity}.  


% Figure environment removed

When $\lambda = 0.0$ (i.e., no $L_{hard}$), the performance is among the worst for both datasets, which shows the importance of having a local view during optimization. Higher values negatively impact the performance in the \texttt{Market} dataset, but it does not affect results in \texttt{Veri776}. To achieve a trade-off we keep $\lambda = 0.5$ for all datasets. 

\section{Extended Results}

% \subsection{Extended Re-Identification Results}

We extend the comparison from the main article by including R5 and R10 for Person Re-Identification and R5 for Vehicle Re-Identification (Tables~\ref{tab:state_of_art_reid_fully_unsupervised}, \ref{tab:vehicleID_fully_unsupervised}, and~\ref{tab:veri_wild_fully_unsupervised}). For \texttt{VehicleID} and \texttt{Veri-Wild}, we also show the performance in the simpler evaluation scenarios (the medium and largest scenarios are presented in the main paper). 

For Person ReID, the results in Table~\ref{tab:state_of_art_reid_fully_unsupervised} follow the same conclusions presented in the main article. We have the best performance for all metrics in \texttt{MSMT17}, and our method, with only $75\%$ of the data, still ranks second place in this dataset. For \texttt{Market}, we show competitive results, ranking third place in R5 and being below AdaMG in R10 just by $0.4$ p.p. However, AdaMG adopts an unrealistic scenario where the clustering hyper-parameter is tuned per dataset. Moreover, AdaMG adopts a more memory- and time-complex Re-Ranking strategy. Therefore, with a less complex method and without requiring any per-dataset hyper-parameter tuning, we achieve competitive performance in \texttt{Market} and outperform prior art in all metrics in \texttt{MSMT17} with $100\%$ and $75\%$ of the data.      

\begin{table*}[ht]
%\vspace{-55pt}
\caption{Comparison with relevant fully-unsupervised Person ReID methods. The best result is highlighted in \bv{blue}, the second best in \gv{green}, and the third in \ov{orange}. RRMC means Re-Ranking Memory Complexity and CPD (Cluster Parameter per Dataset) indicates if the method relies on specific clustering parameters per dataset. (p\%) means that p\% of all data points are sampled in the Local Neighborhood Sampling and used in the current epoch.}
\label{tab:state_of_art_reid_fully_unsupervised}
\centering
\begin{tabular}{|p{2.4cm}| p{1.2cm}|P{1.3cm}|P{0.8cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|}
\hline
\multicolumn{1}{|c|}{} &
\multicolumn{1}{|c|}{} &
\multicolumn{1}{|c|}{} &
\multicolumn{1}{|c|}{} &
\multicolumn{4}{|c|}{\textbf{Market}} &
\multicolumn{4}{|c|}{\textbf{MSMT17}} \\ \hline
%\multicolumn{14}{|c|}{\textit{Fully Unsupervised}}  \\ \hline
Method & Reference & RRMC & CPD & mAP & R1 & R5 & R10
& mAP & R1 & R5 & R10 \\ \hline
%BUC~\cite{lin2019bottom} & AAAI'19 & 38.3 & 66.2 & 79.6 & 84.5 & 27.5 & 47.4 & 62.6 & 68.4 & - & - & - & - \\
%Nikhal et. al.~\cite{nikhal2021unsupervised} & WACV'21 & - & \gv{No} & 35.7 & 63.9 & 78.8 & 85.1 & - & - & - & - \\
%Zheng \textit{et. al.}~\cite{zheng2022clustering} & TOMM'21 & - & \gv{No} & 42.0 & 68.8 & - & - & - & - & - & - \\
%DBC~\cite{ding2019dispersion} & BMVC'19 & - & \gv{No} & 41.3 & 69.2 & 83.0 & 87.8 & - & - & - & -\\ 
%GPUFL~\cite{sun2021unsupervised} & ICIP'21 & - & \rv{Yes} & 42.3 & 69.6 & - & - & - & - & - & -  \\
%MV-ReID~\cite{yin2021multi} & SPL'21 & - & \gv{No} & 45.6 & 73.3 & 85.3 & 89.1 & - & - & - & - \\
%MFC-LCDM~\cite{pan2020unsupervised} & ICASSP'20 & - & \gv{No} & 53.6 & 77.5 & 87.9 & 91.7 & - & - & - & - \\
%MMCL~\cite{wang2020unsupervised} & CVPR'20 & - & \gv{No} & 45.5 & 80.3 & 89.4 & 92.3 & 11.2 & 35.4 & 44.8 & 49.8 \\ 
%MCGA~\cite{nikhal2022multi} & TBIOM'22 & - & \gv{No} & - & - & - & - & 14.0 & 36.1 & 49.5 & 56.2 \\
%GSMLP-BMLC~\cite{yu2022graph} & AI'22 & - & \gv{No} & 48.4 & 80.9 & 90.2 & 93.1 & 15.8 & 36.2 & 44.0 & 48.1 \\
%MLJT~\cite{tang2021fully} & Access'21 & - & \gv{No} &55.3 & 81.6 & 90.4 & 93.5 & - & - & - & -\\
%HCT~\cite{zeng2020hierarchical} & CVPR'20 & - & \gv{No} & 56.4 & 80.0 & 91.6 & 95.2 & - & - & - & - \\
%NNCT~\cite{tang2021unsupervised} & ICIP'21 & - & \gv{No} & 57.6 & 85.2 & 92.3 & 94.3 & - & - & - & - \\
%SNNet~\cite{tang2022unsupervised} & ICIT'22 & - & \gv{No} & 58.3 & 85.1 & 92.6 & 94.3 & - & - & - & - \\
%PNNM~\cite{zhao2022unsupervised} & DSP'22 & - & \gv{No} & 45.6 & 71.8 & 84.0 & 88.3 & 19.5 & 43.1 & 56.8 & 65.4 \\
ABMT~\cite{chen2020enhancing} & WACV'20 & $\mathcal{O}(N^{2})$ & \gv{No} & 65.1 & 82.6 & - & - & - & - & - & - \\
SpCL~\cite{ge2020self} & NeurIPS'20 & $\mathcal{O}(N^{2})$ & \gv{No} & 73.1 & 88.1 & 95.1 & 97.0 & 19.1 & 42.3 & 55.6 & 61.2 \\
GCL+~\cite{chen2022learning} & TPAMI'22 & $\mathcal{O}(N^{2})$ & \gv{No} & 69.3 & 89.0 & 94.6 & 96.0 & 22.0 & 47.9 & 61.3 & 67.1 \\ 
GSam~\cite{han2022rethinking} & TIP'22 & $\mathcal{O}(N^{2})$ & \gv{No} & 79.2 & 92.3 & 96.6 & 97.8 & 24.6 & 56.2 & 67.3 & 71.5 \\
%SpCL-IBN~\cite{ge2020self} & NeurIPS'20 & 73.8 & 88.4 & 95.3 & 97.3 & - & - & - & - & 24.0 & 48.9 & 61.8 & 67.1 \\
RLCC~\cite{zhang2021refining} & CVPR'21 & $\mathcal{O}(N^{2})$ & \gv{No}&  77.7 & 90.8 & 96.3 & 97.5 & 27.9 & 56.5 & 68.4 & 73.1 \\
%MaskPre~\cite{yin2022unsupervised} & PR'22 & - & \gv{No} & 77.7 & 90.4 & 96.4 & 97.9 & 26.7 & 58.5 & 69.3 & 73.8 \\
HCLP~\cite{zheng2021online} & ICCV'21 & - & \gv{No} & 78.1 & 91.1 & 96.4 & 97.7 & 26.9 & 53.7 & 65.3 & 70.2 \\
%DFC~\cite{si2023diversity} & IPM'23 & - & \gv{No} &  78.6 & 90.8 & 97.5 & - & - & - & - & - \\
ICE~\cite{Chen_2021_ICCV} & ICCV'21 & $\mathcal{O}(N^{2})$ & \gv{No} &79.5 & 92.0 & 97.0 & 98.1 & 29.8 & 59.0 & 71.7 & 77.0 \\
%MGCE-HCL~\cite{sun2022hybrid} & ACPR'22 & - & \gv{No} & 79.6 & 92.1 & - & - & - & - & - & - \\
%MPC~\cite{li2023multi} & CVIU'23 &$\mathcal{O}(|C_{i}|^{2})$ & \gv{No} & 77.4 & 90.9 & 96.4 & 97.6 & - & - & - & - \\
CACL~\cite{li2022cluster} & TIP'22 & - & \gv{No} & 80.9 & 92.7 & 97.4 & 98.5 & 23.0 & 48.9 & 61.2 & 66.4 \\
HDCRL~\cite{cheng2022hybrid} & TIP'22 & - & \gv{No} & 81.7 & 92.4 & 97.4 & 98.1 & 24.6 & 50.2 & 61.4 & 65.7 \\
PPLR~\cite{cho2022part} & CVPR'22 & $\mathcal{O}(N^{2})$ & \gv{No} & 81.5 & 92.8 & 97.1 & 98.1 & 31.4 & 61.1 & 73.4 & 77.8 \\
%RMCL~\cite{pang2023reliability} & KBS'23 & $\mathcal{O}(N^{2})$ & \gv{No} & 81.7 & 93.0 & 97.6 & 98.4 & 32.5 & 62.3 & 73.6 & 78.0 \\
%Zhang et. al.~\cite{zhang2022unsupervised} & AI'22 & $\mathcal{O}(N^{2})$ & \gv{No} & 83.0 & 93.2 & 97.3 & - & - & - & - & - \\
RTMem~\cite{yin2023real} & TIP'23 & - & \rv{Yes} & 83.0 & 92.8 & 97.4 & 98.3 & 32.8 & 57.1 & 70.0 & 74.9\\
CCons~\cite{dai2022cluster} & ACCV'22 & $\mathcal{O}(N^{2})$ & \gv{No} & 83.0 & 92.9 & 97.2 & 98.0 & 33.0 & 62.0 & 71.8 & 76.7 \\
%MCL~\cite{jin2022meta} & ICM'22 & $\mathcal{O}(N^{2})$ & \rv{Yes} & 83.3 & 93.0 & - & - & 33.4 & 62.9 & - & - \\
%GCM~\cite{zhang2022graph} & SIVP'22 & $\mathcal{O}(N^{2})$ & \gv{No} & 83.4 & 93.3 & - & - & - & - & - & - \\
ISE~\cite{zhang2022implicit} & CVPR'22 & - & \rv{Yes} & \gv{84.7} & \bv{94.0} & \gv{97.8} & \gv{98.8} & 35.0 & 64.7 & 75.5 & 79.4 \\
%Zheng~\cite{zheng2022multi} & ArXiv'22 & $\mathcal{O}(N^{2})$ & \gv{No} & 83.1 & 92.8 & 97.1 & 98.0 & 35.6 & 63.8 & 75.3 & 79.5 \\
HHCL~\cite{hu2021hard} & NIDC'21 & $\mathcal{O}(N^{2})$ & \gv{No} &  84.2 & \ov{93.4} & \ov{97.7} & 98.5 & - & - & - & - \\
GRACL~\cite{zhang2022global} & TCSVT'22 & $\mathcal{O}(N^{2})$ & \gv{No} &  83.7 & 93.2 & 97.6 & \ov{98.6} & 34.6 & 64.0 & 75.0 & 79.3 \\
%CGCL~\cite{miao2022confidence} & ArXiv'22 & - & \gv{No} & 85.3 & 94.2 & 97.6 & 98.5 & 34.6 & 63.4 & 74.6 & 79.3 \\
%NCPLR~\cite{cheng2022neighbour} & ArXiv'22 & - & \rv{Yes} & 86.3 & 94.3 & 98.0 & 98.7 & 35.7 & 66.3 & 76.9 & 80.6 \\
AdaMG~\cite{peng2023adaptive} & TCSVT'23 &  $\mathcal{O}(N^{2})$ & \rv{Yes} & \ov{84.6} & \gv{93.9} & \bv{97.9} & \bv{98.9} & \ov{38.0} & \ov{66.3} & \ov{76.9} & \ov{80.6} \\
%DCCT~\cite{chen2022dual} & ArXiv'22 &  $\mathcal{O}(N^{2})$ & \rv{Yes} & 86.3 & 94.4 & 97.7 & 98.5 & 41.8 & 68.7 & 79.0 & 82.6 \\ 
\hline
%\textbf{Ours} & & 84.4 & 93.2 & 96.7 & 97.6 & 45.3 & 71.9 & 81.0 & 83.9\\
%\textbf{Ours (best $k$)} & & 85.7 & 93.9 & 97.2 & 98.0 &  &  &  & \\
\textbf{Ours (25\%)} &  & $\mathcal{O}(kN)$& \gv{No} & - & - & - & - & 32.0 & 60.5 & 71.0 & 75.2\\
\textbf{Ours (50\%)} & & $\mathcal{O}(kN)$ & \gv{No} & - & - & - & - & 24.3 & 50.4 & 60.6 & 65.4\\
\textbf{Ours (75\%)} & & $\mathcal{O}(kN)$ & \gv{No} & 82.9 & 92.6 & 97.0 & 97.8 & \gv{39.3} & \gv{67.3} & \gv{77.3} & \gv{80.8}\\
\textbf{Ours (100\%)} &  & $\mathcal{O}(kN)$ & \gv{No} & \bv{85.8} & \bv{94.0} & \ov{97.7} & 98.5 & \bv{43.2} & \bv{70.9} & \bv{80.8} & \bv{84.2}\\

\hline
\end{tabular}
\end{table*}

In \texttt{VehicleID} (Table~\ref{tab:vehicleID_fully_unsupervised}), our method presents the same behavior shown in the main article for the simplest evaluation scenario (``Test size = 800''). That is, we obtain the best or second-best performance in all metrics. The same happens for R5 in ``Test size = 1600'' and ``Test size = 2400''. 

\begin{table*}[ht]
\caption{Comparison with relevant fully-unsupervised Vehicle ReID methods in \texttt{VehicleID}. The best result is highlighted in \bv{blue}, the second best in \gv{green}, and the third in \ov{orange}. RRMC means Re-Ranking Memory Complexity and CPD (Cluster Parameter per Dataset) indicates if the method relies on specific clustering parameters per dataset. Methods with * were reproduced from~\cite{he2022multi}, which seems to follow the same evaluation protocol as ours. (p\%) means that p\% of all data points are sampled in the Local Neighborhood Sampling and used in the current epoch.}
\label{tab:vehicleID_fully_unsupervised}
\centering
\begin{tabular}{p{2.7cm}| P{1.5cm}|P{1.0cm}|P{0.7cm}|p{0.7cm}|p{0.5cm}|p{0.5cm}|p{0.5cm}|p{0.5cm}|p{0.5cm}|p{0.5cm}|p{0.5cm}|p{0.5cm}}
\hline
\multicolumn{1}{c|}{} &
\multicolumn{1}{|c|}{} &
\multicolumn{1}{|c|}{} &
\multicolumn{1}{|c|}{} &
\multicolumn{3}{|c|}{\textbf{Test size  = 800}} &
\multicolumn{3}{|c|}{\textbf{Test size  = 1600}} &
\multicolumn{3}{|c}{\textbf{Test size  = 2400}} \\
\hline
%\multicolumn{14}{|c|}{\textit{Camera-based}} \\ \hline
%\multicolumn{14}{|c|}{\textit{Segmentation-based}} \\ \hline
%\multicolumn{13}{|c|}{\textit{Part-based Models}} \\ \hline
%Method & Reference & RR-MC & CPD & mAP & R1 & R5 & mAP & R1 & %R5 & mAP & R1 & R5 \\ \hline
%MLPL~\cite{he2022multi}* & TVT'22 & - & \gv{No} & 65.3 & 61.1 & %69.8 & 62.7 & 57.3 & 68.4 & 59.6 & 52.4 & 66.2 \\ \hline
%\multicolumn{13}{|c|}{\textit{Fully Unsupervised}} \\ \hline
Method & Reference & RRMC & CPD &  mAP & R1 & R5 & mAP & R1 & R5 & mAP & R1 & R5 \\ \hline
BUC~\cite{lin2019bottom}* & AAAI'19 & - & \gv{No} & 51.8 & 49.5 & 62.6 & 46.2 & 45.9 & 59.8 & 42.4 & 39.7 & 57.3 \\
%PAL~\cite{peng2020unsupervised}* & ArXiv'20 & - & \gv{No} & 53.5 & 50.3 & 64.9 & 48.1 & 44.3 & 60.9 & 45.1 & 41.1 & 59.1 \\ 
MAC~\cite{zhu2022manifold} & KBS'22 & - & \gv{No} & 56.2 & 54.3 & 71.1 & 51.9 & 47.5 & 66.8 & 47.4 & 44.4 & \bv{65.9} \\
%MSCL~\cite{wang2022unsupervised} & SVIP'22 & $\mathcal{O}(N^{2})$ & \gv{No} &  - & - & - & - & - & - & 52.9 & \bv{51.8} & - \\
SpCL~\cite{ge2020self}* & NeurIPS'20 & $\mathcal{O}(N^{2})$ & \gv{No} &  60.2 & 55.4 & 67.5 & 58.7 & 53.1 & 67.1 & 54.3 & \ov{48.9} & 64.8 \\
CCons~\cite{dai2022cluster}* & ACCV'22 & $\mathcal{O}(N^{2})$ & \gv{No} & \bv{62.6} & \bv{57.7} & \ov{68.0} & \bv{60.3} & \bv{54.0} & \bv{67.9} & \bv{57.1} & \bv{50.1} & \bv{65.9} \\
%TCL~\cite{shen2023triplet} & ArXiV'23 & & & 66.3 & 60.4 & - & 63.7 & 56.2 & - & 61.1 & 52.9 & -  \\
\hline
& Speedup & \multicolumn{11}{c}{} \\ \hline
\textbf{Ours (25\%)} & $4.97\times$ & $\mathcal{O}(kN)$ & \gv{No} & 61.0 & 55.1 & 68.0 & 59.0 & 52.3 & 67.2 & 55.6 & 48.5 & 64.3 \\
\textbf{Ours (50\%)} & $2.44\times$ &  $\mathcal{O}(kN)$ & \gv{No} & 61.0 & 55.0 & \gv{68.1} & \ov{59.2} & 52.8 & 67.1 & 56.0 & \ov{48.9} & 64.5 \\
\textbf{Ours (75\%)} & $1.31\times$ &  $\mathcal{O}(kN)$ & \gv{No} & \ov{61.6} & \ov{55.7} & \bv{68.6} & \gv{59.7} & \ov{53.3} & \ov{67.6} & \ov{56.6} & \gv{49.6} & \ov{65.0} \\
\textbf{Ours (100\%)} & $1.00\times$ &  $\mathcal{O}(kN)$ & \gv{No} & \gv{61.7} & \gv{56.0} & \bv{68.6} & \gv{59.7} & \gv{53.4} & \gv{67.7} & \gv{56.9} & \bv{50.1} & \gv{65.2}\\
\hline
\end{tabular}
\end{table*}

Similarly to the previous datasets, the conclusion for \texttt{Veri-Wild}, shown in Table~\ref{tab:veri_wild_fully_unsupervised}, follows the same ones in the main article. For \texttt{Veri-Wild (Small)}, we have the best performance with $75\%$ of the data, the second-best with $100\%$ of the data, and the third-best with $50\%$ of the data. This shows that even on simpler evaluation scenarios and higher ranking metrics (e.g., R5 and R10), our method still provides competitive or, as in most cases, state-of-the-art performance compared to the prior art.  

\begin{table*}[ht]
\caption{Comparison with relevant fully-unsupervised Vehicle ReID methods in \texttt{Veri-Wild}. The best result is highlighted in \bv{blue}, the second best in \gv{green}, and the third in \ov{orange}. RRMC means Re-Ranking Memory Complexity and CPD (Cluster Parameter per Dataset) indicates if the method relies on specific clustering parameters per dataset. Speedup values are measured in comparison to the version with $100\%$ of the data. (p\%) means that p\% of all data points are sampled in the Local Neighborhood Sampling and used in the current epoch.}
\label{tab:veri_wild_fully_unsupervised}
\centering
\begin{tabular}{p{2.7cm}| P{1.2cm}|P{1.0cm}|P{0.7cm}|p{0.5cm}|p{0.5cm}|p{0.5cm}|p{0.5cm}|p{0.5cm}|p{0.5cm}|p{0.5cm}|p{0.5cm}|p{0.5cm}}
\hline
\multicolumn{1}{c|}{} &
\multicolumn{1}{|c|}{} &
\multicolumn{1}{|c|}{} &
\multicolumn{1}{|c|}{} &
\multicolumn{3}{|c|}{\textbf{Veri-Wild (Small)}} &
\multicolumn{3}{|c|}{\textbf{Veri-Wild (Medium)}} &
\multicolumn{3}{|c}{\textbf{Veri-Wild (Large)}} \\
\hline
Method & Reference & RR-MC & CPD & mAP & R1 & R5 & mAP & R1 & R5 & mAP & R1 & R5 \\ \hline
BUC~\cite{lin2019bottom} & AAAI'19 &  - & \gv{No} & 15.2 & 37.5 & 53.0 & 14.8 & 33.8 & 51.1 & 9.2 & 25.2 & 41.6 \\ 
MMCL~\cite{wang2020unsupervised} & CVPR'20 & - & \gv{No} & 15.9 & 40.1 & 63.5 & 19.2 & 39.1 & 60.4 & 14.1 & 33.1 & 50.4 \\
SSML~\cite{yu2021unsupervised} & IROS'21 & - & \gv{No} & 23.7 & 49.6 & 71.0 & 20.4 & 43.9 & 64.9 & 15.8 & 34.7 & 55.4 \\
%MSCL~\cite{wang2022unsupervised} & SVIP'22 & - & - & - & - & - & - & \bv{30.4} & \bv{57.2} & - \\
\hline
& Speedup & \multicolumn{11}{c}{} \\ \hline
\textbf{Ours (25\%)} & $7.60\times$ & $\mathcal{O}(kN)$ & \gv{No}  & 28.0 & 50.4 & 74.4 & 23.6 & 42.2 & 66.5 & 18.0 & 32.2 & 55.1\\ 
\textbf{Ours (50\%)} & $2.62\times$ & $\mathcal{O}(kN)$ & \gv{No} & \ov{29.8} & \ov{53.5} & \ov{76.4} & \ov{25.6} & \ov{45.7} & \ov{69.2} & \ov{19.8} & \ov{35.5} & \ov{58.6}\\ 
\textbf{Ours (75\%)} & $1.48\times$ & $\mathcal{O}(kN)$ & \gv{No}  & \bv{30.2} & \bv{54.6} & \bv{77.1} & \bv{26.0} & \bv{46.8} & \bv{70.0} & \bv{20.3} & \bv{36.4} & \bv{59.2}\\ 
\textbf{Ours (100\%)} & $1.00\times$ & $\mathcal{O}(kN)$ & \gv{No}  & \gv{29.9} & \gv{54.1} & \gv{76.6} & \gv{25.8} & \gv{46.4} & \gv{69.2} & \gv{20.0} & \gv{36.2} & \gv{59.0}\\ 
%\textbf{Ours (best $k$)}  & &  &  &  &  &  &  &  &  & &  & &  \\ 
\hline
\end{tabular}
\end{table*}

% \subsection{Comparison to Not-Fully-Unsupervised Re-Identification methods}

We also compare, in Table~\ref{tab:state_of_art_reid}, our method to not-fully unsupervised re-identification methods, i.e., the ones that leverage camera labels or tracklets to help the optimization. Particularly, the camera labels provide strong regularization since they enable the model to train in the same scenario of the cross-camera evaluation. For this reason, methods that consider camera labels have usually higher performance. However, they are \textbf{not} fully unsupervised.

% In Table~\ref{tab:state_of_art_reid}, we provide further results comparing our method to our prior works that leverage our camera labeling or tracklets to help model learning. Particularly, the camera labeling brings a strong regularization performance since it enables the model to train in the same scenario of the cross-camera evaluation. However, the camera labeling might not be available, and the models are not fully unsupervised. For this reason, usually, models that consider camera labels have higher performance. 

\begin{table*}[ht]
%\vspace{-55pt}
\caption{Comparison with relevant Person ReID methods considering some meta-information or camera/viewpoint labeling. The best result is highlighted in \bv{blue}.}
\label{tab:state_of_art_reid}
\centering
\begin{tabular}{|p{2.5cm}| p{1.8cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|}
\hline
\multicolumn{1}{|c|}{} &
\multicolumn{1}{|c|}{} &
\multicolumn{4}{|c|}{\textbf{Market}} &
\multicolumn{4}{|c|}{\textbf{MSMT17}} \\
\hline
Method & Reference & mAP & R1 & R5 & R10
& mAP & R1 & R5 & R10 \\ \hline
\multicolumn{10}{|c|}{\textit{Camera-based}} \\ \hline
SSL~\cite{lin2020unsupervised} & CVPR'20 & 37.8 & 71.7 & 83.8 & 87.4 & - & - & - & - \\
CCSE~\cite{lin2020unsupervisedccse} & TIP'20 & 38.0 & 73.7 & 84.0 & 87.9 & 9.9 & 31.4 & 41.4 & 45.7\\
MPRD~\cite{ji2021meta}(*) & ICCV'21 & 51.1 & 83.0 & 91.3 & 93.6 & 14.6 & 37.7 & 51.3 & 57.1 \\ 
Xie~\textit{et. al.}~\cite{xie2021unsupervised} & IJMLC'21 & 54.1 & 82.6 & 91.3 & 94.5 & 13.4 & 37.5 & 48.5 & 52.0 \\
DSCE-MC~\cite{yang2021joint} & CVPR'21 & 61.7 & 83.9 & 92.3 & - & 15.5 & 35.2 & 48.3 & - \\
JVTC~\cite{li2020joint} & ECCV'20 & 47.5 & 79.5 & 89.2 & 91.9 & 17.3 & 43.1 & 53.8 & 59.4 \\ 
JGCL~\cite{chen2021joint} & CVPR'21 & 66.8 & 87.3 & 93.5 & 95.5 & 21.3 & 45.7 & 58.6 & 64.5 \\
IICS~\cite{xuan2021intra} & CVPR'21 & 72.9 & 89.5 & 95.2 & 97.0 & 26.9 & 56.4 & 68.8 & 73.4 \\
IIDS~\cite{xuan2022intra} & TPAMI'22 & 78.0 & 91.2 & 96.2 & 97.7 & 35.1 & 64.4 & 76.2 & 80.5\\ 
CAP~\cite{wang2021camera} & AAAI'21 & 79.2 & 91.4 & 96.3 & 97.7 & 36.9 & 67.4 & 78.0 & 81.4\\
CCTSE~\cite{9521886} & TIFS'21 & 67.7 & 89.5  & 94.8 & 96.5 & - & - & - & - \\
CAPL~\cite{liu2023camera} &  NCA'23 & 80.4 & 92.8 & 97.3 & - & 40.7 & 71.2 & 81.4 & - \\
MGH~\cite{wu2021mgh} & ICM'21 & 81.7 & 93.2 & 96.8 & 98.1 & 40.6 & 70.2 & 81.2 & 84.5 \\
ICE~\cite{Chen_2021_ICCV} & ICCV'21 & 82.3 & 93.8 & 97.6 & 98.4 & 38.9 & 70.2 & 80.5 & 84.4 \\
O2CAP~\cite{wang2022offline} & TIP'22 & 82.7 & 92.5 & 96.9 & 98.0 & 42.4 & 72.0 & 81.9 & 85.4 \\
O2CAP-IBN~\cite{wang2022offline} & TIP'22 & 83.7 & 93.1 & 97.4 & 98.1 & \bv{46.9} & \bv{75.5} & \bv{84.8} & \bv{87.7} \\
CASTOR-ICE~\cite{xu2022pseudo} & TITS'22 & 82.8 & 93.6 & 97.5 & 98.5 & 41.7 & 72.3 & 82.3 & 85.8 \\ 
CASTOR-CCL~\cite{xu2022pseudo} & TITS'22 & 84.5 & 93.0 & 97.8 & 98.6 & 33.2 & 61.9 & 74.0 & 78.2 \\ 
Liu~\textit{et. al.}~\cite{liu2022unsupervised} & TIP'22 & 82.4 & 93.0 & - & - & 38.4 & 68.6 & - & - \\
Liu~\textit{et. al.}-IBN~\cite{liu2022unsupervised} & TIP'22 & 82.0 & 92.8 & - & - & 42.4 & 71.6 & - & - \\
CIFL~\cite{pang2022camera} & TMM'22 & 82.4 & 93.9 & 97.9 & 98.1 & 38.8 & 70.1 & 80.7 & 83.9 \\
RTMem~\cite{yin2023real} & TIP'23 & 83.1 & 93.9 & 97.7 & 98.4 & 40.8 & 72.0 & 81.5 & 84.6\\
MCSL~\cite{he2023multiple} & OPTIK'23 & 83.5 & 93.7 & 97.5 & 98.4 & 38.7 & 71.1 & 80.8 & 84.3 \\
PPLR~\cite{cho2022part} & CVPR'22 & 84.4 & 94.3 & 97.8 & 98.6 & 42.2 & 73.3 & 83.5 & 86.5 \\
PPSL~\cite{wu2022pseudo} & TIP'22 & 68.7 & 88.6 & 95.2 & 96.6 & 40.9 & 71.1 & 83.3 & 87.0 \\
PPSL(Concat)~\cite{wu2022pseudo} & TIP'22 & 82.3 & 94.1 & 97.4 & \bv{98.8} & 43.1 & 73.2 & 89.4 & 90.8 \\
PEG~\cite{zhai2022population} & IJCV'22 & 84.5 & 94.3 & 98.0 & 98.5 & 44.9 & 73.9 & 83.2 & 86.3 \\ 
PPCL+CAP~\cite{zheng2022plausible} & TCSVT'22 & 82.4 & 94.0 & 98.1 & - & 37.8 & 70.8 & 80.7 & - \\
PPCL+ICE~\cite{zheng2022plausible} & TCSVT'22 & 82.8 & 93.9 & 97.6 & - & 39.8 & 70.8 & 81.2 & - \\
DiDAL~\cite{10105456} & TMM'23 & 84.8 & 94.2 & \bv{98.2} & - & 45.4 & 74.0 & 84.3 & - \\
CCL~\cite{zhang2023camera} & TCSVT'23 & 85.3 & 94.1 & 97.8 & \bv{98.8} & 41.8 & 71.4 & - & - \\ \hline
\multicolumn{10}{|c|}{\textit{Multi-part based models}}  \\ \hline
PPLR~\cite{cho2022part} & CVPR'22 & 81.5 & 92.8 & 97.1 & 98.1 & 31.4 & 61.1 & 73.4 & 77.8 \\
LPur~\cite{lan2023learning} & TIP'23 & \bv{85.8} & \bv{94.5} & 97.8 & 98.7 & 39.5 & 67.9 & 78.0 & 81.6 \\ \hline
%UPRSTS & ArXiV & \bv{82.4} & \gv{93.0} & \gv{97.5} & - & \bv{72.2} & \bv{84.9} & \bv{92.3} & - & \gv{38.4} & \gv{68.6} & \gv{79.4} & - \\ \hline
%\textbf{Ours} & \textbf{This work} & \rv{82.2} & \rv{92.3} & \rv{96.5} & \rv{97.5} & \gv{70.3} & \gv{83.6} & \rv{90.2} & \gv{92.2} & \rv{37.5} & 63.5 & 73.7 & \rv{77.4} \\ 
\multicolumn{10}{|c|}{\textit{Tracklet-based}}  \\ \hline
Star-Dac~\cite{prasad2022spatio} & PR'21 & 33.9 & 67.0 & 80.6 & 84.9 & - & - & - & - \\
%TAUDL~\cite{li2018unsupervised} & ECCV'18 & 41.2 & 63.7 & - & - & 43.5 & 61.7 & - & - & - & - & - & - \\
TSSL~\cite{wu2020tracklet} & AAAI'20 & 43.3 & 71.2 & - & - & - & - & - & - \\ 
UTAL~\cite{li2019unsupervised} & TPAMI'20 & 46.2 & 69.2 & - & - & 13.1 & 31.4 & - & - \\
CycAs~\cite{wang2020cycas} & ECCV'20 & 64.8 & 84.8 & - & - & 26.7 & 50.1 & - & - \\ 
UGA~\cite{wu2019unsupervised} & ICCV'19 & 70.3 & 87.2 & - & - & 21.7 & 49.5 & - & - \\ \hline
\multicolumn{10}{|c|}{\textit{Fully-Unsupervised}}  \\ \hline
\textbf{Ours} & & \bv{85.8} & 94.0 & 97.7 & 98.5 & 43.2 & 70.9 & 80.8 & 84.2 \\ \hline
%\textbf{Ours} & \textbf{This work} & \bv{82.2} & \bv{92.3} & \bv{96.5} & \bv{97.5} & \bv{70.3} & \bv{83.6} & \bv{90.2} & \bv{92.2} & \bv{37.5} & \bv{63.5} & \bv{73.7} & \bv{77.4} \\ 
\end{tabular}
% *MPRD originally does not use camera information in their pipeline. However, it leverages CamStyle~\cite{zhong2018camstyle} to perform augmentation, \\ which is based on camera labels.
\end{table*}

Even though our model does not use camera labels, we still have competitive performance, and the best mAP for \texttt{Market}. Considering the tracklet-based models, we outperform them in all metrics. Despite the tracklets being a meta-information about the pedestrian motion, which enables the possibility to leverage temporal information, our method better mines the discriminant information just relying upon people's still images.


\begin{table}[ht]
\caption{Comparison with relevant Vehicle ReID methods in the \texttt{Veri776} dataset considering some meta-information or camera/viewpoint labeling. The best results are highlighted in \bv{blue}.}
\label{tab:state_of_art_vehicle}
\centering
\begin{tabular}{|p{2.2cm}|p{1.5cm}|p{0.5cm}|p{0.5cm}|p{0.5cm}|}
\hline
\multicolumn{1}{|c|}{} &
\multicolumn{1}{|c|}{} &
\multicolumn{3}{|c|}{Veri} \\
\hline
Method & Reference & mAP & R1 & R5 \\ \hline
\multicolumn{5}{|c|}{\textit{Camera/Viewpoint-based}} \\ \hline
SSL~\cite{lin2020unsupervised} & CVPR'20 & 23.8 & 69.3 & 72.1 \\
VAPC~\cite{zheng2021aware} & TITS'21 & 30.4 & 76.2 & 81.2 \\
CAPL~\cite{liu2023camera} &  NCA'23 & 41.1 & 87.3 & 91.3 \\
O2CAP~\cite{wang2022offline} & TIP'22 & 41.9 & 87.5 & 92.7\\
O2CAP-IBN~\cite{wang2022offline} & TIP'22 & 42.4 & 89.6 & \bv{93.5}\\
CCL~\cite{zhang2023camera} & TCSVT'23 & 42.6 & 87.0 & -\\
Liu~\textit{et. al.}~\cite{liu2022unsupervised} & TIP'22 & 43.2 & 87.0 & -\\
Liu~\textit{et. al.}-IBN~\cite{liu2022unsupervised} & TIP'22 & 43.9 & 88.9 & -\\
PPLR~\cite{cho2022part} & CVPR'22 & 43.5 & 88.3 & 92.7 \\ 
DiDAL~\cite{10105456} & TMM'23 & 43.5 & \bv{89.0} & 93.5 \\
CTACL~\cite{yu2022camera} & ICRA'22 & 44.2 & 81.6 & 89.5 \\
%CTACL-DA~\cite{yu2022camera} & ICRA'22 & \bv{55.2} & \bv{89.3} & \bv{93.9}\\\hline 
\hline
\multicolumn{5}{|c|}{\textit{Segmentation-based}} \\ \hline
MAPLD~\cite{lu2023mask} & TITS'23 & 33.4 & 78.7 & 83.5\\ \hline
\multicolumn{5}{|c|}{\textit{Multi-part-based models}} \\ \hline
PPLR~\cite{cho2022part} & CVPR'22 & 41.6 & 85.6 & 91.1\\ 
MLPL~\cite{he2022multi} & TVT'22 & \bv{45.1} & 88.3 & 91.1\\ \hline
\multicolumn{5}{|c|}{\textit{Attribute-based Models}} \\ \hline
Method & Reference & mAP & R1 & R5\\ \hline
VRPRD~\cite{bashir2019vr} & PR'19 & 40.1 & 83.2 & 91.1 \\ \hline
\multicolumn{5}{|c|}{\textit{Fully-Unsupervised}} \\ \hline
\textbf{Ours} & & 41.3 & 86.3 & 89.9\\
\hline
\end{tabular}
\end{table}

Similar conclusions can be reached for the \texttt{Veri} dataset, as shown in Table~\ref{tab:state_of_art_vehicle}. Considering mAP, the best method is MLPL~\cite{he2022multi} which relies on multi-part analysis that improves feature description but adds complexity to the training process. Our method relies solely on the feature map extracted from the bounding boxes, without any kind of sub-part analysis. Other methods employ camera or viewpoint labeling, making the task easier. Indeed, the best R1 is achieved by DiDAL~\cite{10105456}, which employs camera labels for optimization. Some methods employ segmentation (MAPLD~\cite{lu2023mask}) or color information (VRPRD~\cite{bashir2019vr}) to help optimization, but our model performs better than both without any kind of supervision or side information. 

% Similar analysis we have for Vehicles in the Veri dataset in Table~\ref{tab:state_of_art_vehicle}. The best method in mAP is MLPL~\cite{he2022multi} which relies on multi-part analysis to improve the feature description, which more complexity to the training process, while ours and other methods rely in the whole image feature map without any kind of sub-part analysis. However, other methods employ the camera or viewpoint labeling, which allows their models to effectively improve the mAP values. Indeed, the best R1 is from DiDAL~\cite{10105456} which employs the camera labeling as part of their training. Other methods still employ segmentation (MAPLD~\cite{lu2023mask}) or color information (VRPRD~\cite{bashir2019vr}) to help model training but our model is better than both in those categories without any kind of supervision or side information. 

\begin{table}[ht]
\caption{Comparison with relevant fully-unsupervised Vehicle ReID methods in \texttt{VehicleID}. The best result is highlighted in \bv{blue}.}
\label{tab:vehicleID}
\centering
\begin{tabular}{|p{1.2cm}| p{1.0cm}|p{0.7cm}|p{0.4cm}|p{0.4cm}|p{0.4cm}|p{0.4cm}|p{0.4cm}|}
\hline
\multicolumn{2}{|c|}{} &
\multicolumn{2}{|c|}{\textbf{TS  = 800}} &
\multicolumn{2}{|c|}{\textbf{TS  = 1600}} &
\multicolumn{2}{|c|}{\textbf{TS  = 2400}} \\
\hline
%\multicolumn{14}{|c|}{\textit{Camera-based}} \\ \hline
%\multicolumn{14}{|c|}{\textit{Segmentation-based}} \\ \hline
\multicolumn{8}{|c|}{\textit{Part-based Models}} \\ \hline
Method & Reference & mAP & R1 & mAP & R1 & mAP & R1\\ \hline
MLPL~\cite{he2022multi} & TVT'22 & \bv{65.3} & \bv{61.1} & \bv{62.7} & \bv{57.3} & \bv{59.6} & \bv{52.4} \\ \hline
\multicolumn{8}{|c|}{\textit{Fully Unsupervised}} \\ \hline
Method & Reference & mAP & R1 & mAP & R1 & mAP & R1\\ \hline
\textbf{Ours} & & 61.7 & 56.0 & 59.7 & 53.4 & 56.9 & 50.1\\
\hline
\end{tabular}
\end{table}

\begin{table*}[ht]
\caption{Comparison with relevant Vehicle ReID methods in \texttt{Veri-Wild} dataset considering some meta-information or camera/viewpoint labeling. The best result is highlighted in \bv{blue}.}
\label{tab:veri_wild}
\centering
\begin{tabular}{|p{1.3cm}| p{1.0cm}|P{0.4cm}|P{0.4cm}|P{0.4cm}|P{0.4cm}|P{0.4cm}|P{0.4cm}|P{0.4cm}|P{0.4cm}|P{0.4cm}|}
\hline
\multicolumn{1}{|c|}{} &
\multicolumn{1}{|c|}{} &
\multicolumn{3}{|c|}{\textbf{Veri-Wild (Small)}} &
\multicolumn{3}{|c|}{\textbf{Veri-Wild (Medium)}} &
\multicolumn{3}{|c|}{\textbf{Veri-Wild (Large)}} \\
\hline
Method & Reference & mAP & R1 & R5 & mAP & R1 & R5 & mAP & R1 & R5 \\ \hline
\multicolumn{11}{|c|}{\textit{Camera/Viewpoint-based}} \\ \hline
SSL~\cite{lin2020unsupervised} & CVPR'20 & 16.1 & 38.5 & 58.1 & 17.9 & 36.4 & 56.0 & 13.6 & 32.7 & 48.2\\ 
VAPC~\cite{zheng2021aware} & TITS'21 & 33.0 & \bv{72.1} & \bv{87.7} & 28.1 & 64.3 & 83.0 & 22.6 & 55.9 & 75.9\\
CTACL~\cite{yu2022camera} & ICRA'22 & \bv{58.2} & 71.1 & 86.6 & \bv{49.2} & \bv{69.2} & 83.7 & \bv{41.2} & \bv{60.1} & \bv{81.5}\\ \hline
%CTACL-DA~\cite{yu2022camera} & ICRA'22 & 65.0 & 79.2 & 93.6 & 56.2 & 73.1 & 89.5 & 44.9 & 63.6 & 83.5\\\hline
\multicolumn{11}{|c|}{\textit{Segmentation-based}} \\ \hline
MAPLD~\cite{lu2023mask} & TITS'23 & 36.6 & \bv{72.1} & 87.6 & 33.4 & 66.2 & \bv{84.5} & 27.7 & 55.9 & 77.3\\ \hline
\multicolumn{11}{|c|}{\textit{Fully-Unsupervised}} \\ \hline
\textbf{Ours} & & 29.9 & 54.1 & 76.6 & 25.8 & 46.4 & 69.2 & 20.0 & 36.2 & 59.0\\
%\textbf{Ours(OSNet)} & & \gv{18.6} & 35.8 & \ov{59.3} & \bv{70.8} & \ov{15.5} & 29.1 & 50.5 & \bv{62.8} & 11.2 & 21.5 & 40.0 & \bv{50.3} \\
%\textbf{Ours(DenseNet121)} & & 14.6 & 27.5 & 50.9 & 63.4 & 12.1 & 23.4 & 42.3 & 54.3 & 8.4 & 16.7 & 31.4 & 41.7\\
%\textbf{Ours(Ensemble)} & & \ov{17.7} & 31.7 & 56.4 & \gv{69.6} & 14.5 & 25.9 & 47.4 & \gv{60.4} & 10.2 & 18.7 & 35.8 & \gv{47.1}\\
\hline
\end{tabular}
\end{table*}

In \texttt{Veri-Wild} (Table~\ref{tab:veri_wild}), following previous conclusions, camera and viewpoint labeling provides a strong capacity to improve model learning. Differently than in \texttt{Veri}, the segmentation-based model MAPLD~\cite{lu2023mask} achieves higher performance than ours. Since \texttt{Veri-Wild} is the most challenging dataset, any side information has the potential to aid model learning. As our model operates in the fully-unsupervised scenario, it sometimes faces performance drops compared to other methods that rely upon meta-information or labeling.

%{\appendices
%\section*{Proof of the First Zonklar Equation}
%Appendix one text goes here.
% You can choose not to have a title for an appendix if you want by leaving the argument blank
%\section*{Proof of the Second Zonklar Equation}
%Appendix two text goes here.}

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv, refs}


\end{document}


