{
  "title": "GridMM: Grid Memory Map for Vision-and-Language Navigation",
  "authors": [
    "Zihan Wang",
    "Xiangyang Li",
    "Jiahao Yang",
    "Yeqi Liu",
    "Shuqiang Jiang"
  ],
  "submission_date": "2023-07-24T16:02:42+00:00",
  "revised_dates": [
    "2023-07-26T00:13:03+00:00",
    "2023-08-24T00:24:33+00:00",
    "2023-08-25T00:10:12+00:00"
  ],
  "abstract": "Vision-and-language navigation (VLN) enables the agent to navigate to a remote location following the natural language instruction in 3D environments. To represent the previously visited environment, most approaches for VLN implement memory using recurrent states, topological maps, or top-down semantic maps. In contrast to these approaches, we build the top-down egocentric and dynamically growing Grid Memory Map (i.e., GridMM) to structure the visited environment. From a global perspective, historical observations are projected into a unified grid map in a top-down view, which can better represent the spatial relations of the environment. From a local perspective, we further propose an instruction relevance aggregation method to capture fine-grained visual clues in each grid region. Extensive experiments are conducted on both the REVERIE, R2R, SOON datasets in the discrete environments, and the R2R-CE dataset in the continuous environments, showing the superiority of our proposed method.",
  "categories": [
    "cs.CV",
    "cs.AI"
  ],
  "primary_category": "cs.CV",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.12907",
  "pdf_url": null,
  "comment": "Accepted by ICCV 2023. The code is available at https://github.com/MrZihan/GridMM",
  "num_versions": null,
  "size_before_bytes": 25132497,
  "size_after_bytes": 733593
}