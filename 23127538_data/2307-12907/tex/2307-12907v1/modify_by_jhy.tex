\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{comment}
\usepackage[export]{adjustbox}
\usepackage{stfloats}
\usepackage[numbers]{natbib}
\usepackage{setspace}
\usepackage{indentfirst}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

% \iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{4779} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}
%%%%%%%%% TITLE
%\title{Grid Memory Map for Vision-and-Language Navigation}

\title{GridMM: Grid Memory Map for Vision-and-Language Navigation}

\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi

%%%%%%%%% ABSTRACT

\begin{abstract}
Vision-and-language navigation (VLN) enables the agent to navigate to a remote location following the natural language instruction in 3D environments. To represent the previously visited environment, most approaches for VLN implement memory using recurrent states, topological maps, or top-down semantic maps. In contrast to these approaches, we build an egocentric and dynamically growing Grid Memory Map to structure the visited environment. From a global perspective, historical observations are projected into a unified grid map in a top-down view, which can better represent the spatial relations of the environment. From a local perspective, we further propose an instruction relevance aggregation method to capture fine-grained visual clues in each grid region. Extensive experiments are conducted on both the REVERIE, R2R, SOON datasets in the discrete environments, and the R2R-CE dataset in the continuous environments, showing the superiority of our proposed method. %demonstrating its effectiveness.%Extensive experiments are not only conducted on the REVERIE, R2R and SOON datasets in discrete environments, but also on the R2R-CE datasets in continuous environments. Experimental results demonstrate the effectiveness of the proposed method.
\end{abstract}
%%%%%%%%% BODY TEXT


\section{Introduction}\label{introduction}

Vision-and-language navigation (VLN) tasks\ \cite{VLN-2018vision,2020-RXR,2020reverie} %zhu2021soon}
 require an agent to understand
natural language instructions and act according to the instructions. Two
distinct VLN scenarios have been proposed, being navigation in discrete
environments (\textit{e.g.}, R2R\ \cite{VLN-2018vision}, REVERIE\ \cite{2020reverie}, SOON\ \cite{zhu2021soon}) and in continuous environments
(\textit{e.g.}, R2R-CE\ \cite{Krantz2020r2r-ce}, RxR-CE\ \cite{2020-RXR}). The discrete environment in VLN
is abstracted as a topology structure of interconnected navigable nodes.
With the connectivity graph, the agent can move to an adjacent node
on the graph by selecting a direction from navigable directions.
Different from the discrete environments, VLN tasks in continuous
environments require the agent to move through
low-level controls (\textit{i.e.}, turn left 15 degrees, turn right 15 degrees, or move forward 0.25
meters), which is closer to real-world robot navigation and more
challenging.

% Figure environment removed

Whether
%it is 
in discrete environments or continuous environments,
historical information during navigation plays an important role
in environment understanding and instruction grounding. In previous
works\ \cite{VLN-2018vision,2018-speaker,tan2019learning,2019reinforced,hong2021vln-bert}, recurrent states are most commonly used as historical information
for VLN, which encode historical observations and actions within a
fixed-size state vector. However, such condensed states might be insufficient for capturing essential information in trajectory history. Therefore,
E.T.~\cite{pashevich2021episodic} and HAMT~\cite{chen2021history} propose to %explicitly 
 directly encode the trajectory
history and actions as a sequence of previous observations instead of using recurrent states. Furthermore, in order to structure the
visited environment and make global planning, a few recent approaches~\cite{chen2022think-GL, VLN_LAD_2023} structure
the topological map in discrete
environments, as shown in Fig.~\ref{fig:introduction}(a). However, these methods are difficult to represent the spatial relations among objects and scenes in historical observations, %and 
thus a lot of detailed information is lost. %Although the above approaches have achieved considerable results in the VLN tasks of discrete environments, they have great limitations in a continuous environment without a prior topological map consisting of navigable nodes. In continuous environments of VLN, some 
%More recent works\ \cite{SASRA,georgakis2022cm2,chen2022weakly,huang23vlmaps} use the top-down semantic map to construct the environment, as shown in Fig.~\ref{fig:introduction}(b), which represents spatial relations more precisely, but there is an extremely limited semantic space (\textit{i.e.}, semantic labels). As shown in Fig. \ref{fig:introduction}, for example, it cannot represent objects or scenes that are not included in prior semantic labels, such as “refrigerator”. The objects with attributes such as “wood table” and “blue couch” cannot be fully expressed by the semantic map which misses object attributes.
As shown in Fig.~\ref{fig:introduction}(b), more recent works~\cite{SASRA,georgakis2022cm2,chen2022weakly,huang23vlmaps} model the navigation environment using the top-down semantic map, which represents spatial relations more precisely. But such semantic maps contain an extremely limited semantic space with a few prior semantic labels. So the objects or scenes, which are not included in prior semantic labels, cannot be represented, such as the “refrigerator” in Fig.~\ref{fig:introduction}(b). While some other objects with attributes cannot be fully represented by the semantic map without object attributes, such as the “wood table” and “blue couch” in Fig.~\ref{fig:introduction}(b).

%Expected to solve these disadvantages in previous works
In contrast to the above works~\cite{chen2021topo,chen2022think-GL,georgakis2022cm2,huang23vlmaps}, we propose the Grid Memory
Map, a new visual representation structure for modeling global historical observations during navigation. Mapping observed visual features into a global grid map, the GridMM has more considerable spatial representation and captures more fine-grained visual clues.
%Specifically, we use the pre-trained CLIP\ \cite{radford2021learning} model to extract the fine-grained visual features and save them in a memory bank during navigation. At each step of navigation, the agent uses all visual features in the memory bank to build an egocentric and dynamically growing grid map for action planning. The grid map divides the visited environment into many equally large square regions and obtains a holistic representation of each region by aggregating all features in this region. To that end, we designed an instruction relevance aggregation method to capture the visual clues most relevant to navigation instructions. A wealth of ablation experiments illustrate the effectiveness of our GridMM compared with the previous mapping methods in VLN. %Then we compare multiple construction methods based on the grid map and analyze some characteristics of the grid map. 
Specifically, the grid map divides the visited environment into many equally large grid regions, and each grid region contains many fine-grained visual features. We construct a grid memory bank to update the grid map during navigation. At each step of navigation, the visual features from the pre-trained CLIP~\cite{radford2021learning} model are saved into the memory bank, and all of them are categorized into the grid map regions based on their coordinates calculated via the depth information. To obtain a holistic representation of each region, we designed an instruction relevance aggregation method to capture the visual representations most relevant to navigation instructions. With the help of aggregated visual representations in the grid map, the agent conduct the next action planning. A wealth of experiments illustrate the effectiveness of our GridMM compared with the previous mapping methods in VLN.

In summary, we make the following contributions:
\begin{itemize}
\item We propose the Grid Memory Map for VLN to structure the spatial relations of the visited environment and adopt instruction relevance aggregation to capture fine-grained visual clues relevant to instructions.
\item We utilize detailed experiments to compare different mapping methods in VLN and analyze many characteristics of grid maps, we believe they will give some insights into future works in VLN.
\item We conduct extensive experiments to verify the effectiveness of our method in both discrete environments and continuous environments, which show that it outperforms existing methods on many benchmark datasets.
\end{itemize}


\section{Related work}

\noindent \textbf{Vision-and-Language Navigation (VLN).} VLN\ \cite{VLN-2018vision,2019reinforced,hong2020l-e-graph,2021structured-scene,qiao2022HOP,chen2022think-GL, Chen_2022_HM3D_AutoVLN} has received significant attention in recent years with the continual
improvement. %of benchmark models. 
The VLN tasks include 
 %for indoor navigation with
step-by-step instructions such as R2R\ \cite{VLN-2018vision} and RxR \ \cite{2020-RXR}, navigation with dialog
such as CVDN\ \cite{thomason2020cvdn}, and navigation for remote object grounding
such as REVERIE\ \cite{2020reverie} and SOON\ \cite{zhu2021soon}. All tasks require the agent's ability to use time-dependent
visual observations for decision-making. Restricted by the heavy computation
of exploring the large action space in continuous environments, early works mainly focused on discrete environments. Among them,
a recurrent unit is usually utilized to encode historical
observations and actions within a fixed-size state vector \ \cite{VLN-2018vision,2018-speaker,tan2019learning,2019reinforced,hong2021vln-bert}.
Instead of relying on the recurrent states, HAMT\ \cite{chen2021history} explicitly encodes
the panoramic observation history to capture long-range dependency,
and DUET\ \cite{chen2022think-GL} proposes to encode the topological map for efficient planning
of global actions. Inspired by the success of vision-and-language
pre-training\ \cite{2019-lxmert,radford2021learning}, HOP\ \cite{qiao2022HOP} utilizes well-designed proxy tasks for pre-training to enhance
the interaction between vision and language modalities. ADAPT~\cite{lin2022adapt} employs action prompts to improve the cross-modal alignment ability. Based on data augmentation methods, some
approaches enlarge training data of visual modality\ \cite{li2022envedit} and linguistic
modality\ \cite{2018-speaker,liang2022visual,dou2022foam,Scaling_rxr} depending on existing VLN datasets. Moreover, AirBERT\ \cite{2021airbert} and
HM3D-AutoVLN\ \cite{Chen_2022_HM3D_AutoVLN} improve the performance by creating large-scale
training dataset. In this work, we propose a dynamically growing grid map for structuring the visited environment and making long-term planning, which facilitates environment understanding and instruction grounding.

\noindent \textbf{VLN in Continuous Environments (VLN-CE).} VLN-CE~\cite{Krantz2020r2r-ce}
%as proposed by Krantz \textit{et al.} \cite{Krantz2020r2r-ce} 
converts the topologically-defined VLN
tasks such as R2R\ \cite{VLN-2018vision} into the continuous environment tasks, which
are closer to real-world navigation. Different from the discrete environments,
the agent in VLN-CE must navigate to the destination by selecting low-level
action. Some approaches such as\ \cite{georgakis2022cm2,chen2022weakly} apply top-down semantic maps and use language-aligned waypoints supervision~\cite{SASRA}. Recently, some methods\ \cite{krantz2022sim2sim, Hong2022bridging} for transferring pre-trained VLN
agents to continuous environments have achieved considerable results.
Compared with training agents from scratch in VLN-CE,
this strategy can reduce the computational cost of pre-training
and accelerate model convergence.
In this work, we pre-train our model in discrete environments
and then transfer the pre-trained model to continuous environments
following Hong\ \textit{et al.}\ \cite{Hong2022bridging}. Experiments in both discrete environments and continuous environments illustrate the effectiveness and versatility of our method.

% Figure environment removed


\noindent \textbf{Maps for Navigation.} The works on visual navigation
has a long tradition of constructing maps. Some works represent the
map as topological structures for back-tracking to other locations~\cite{chen2021topo}
or supporting global action planning~\cite{chen2022think-GL}. In addition, some approaches\ \cite{georgakis2022cm2,huang23vlmaps}
construct a top-down semantic map to more precisely represent spatial relations. Recently, BEVBert\ \cite{Dong2022bevbert}  introduced topo-metric
maps from robotics into VLN, which uses topological maps for long-term planning and applies hybrid metric maps for short-term reasoning. Its metric map divides the local environment around the agent into $21$$\times$$21$ cells, and each cell represents a square region with a side length of $0.5$$m$. Moreover, the short-term visual observations within two steps are mapped into these cells.
%However, the hybrid metric map in BEVBert is only a small-size local map, which cannot structure all of the visited environments and rely on a topological map for global planning. It might impose some limitations in a continuous environment without a prior topological map.
Our approach differs from BEVBert in three ways. First, the metric map in BEVBert is only a small-size map, which can only do short-term reasoning limited by local spatial perception. Our grid memory map expands with the expansion of the visited environment, which makes better long-term planning with global spatial perception. Second, the local metric map only memorizes the short-term historical environment, our approach store all historical observations to better handle time-dependent tasks. Third, BEVBert obtains a cell representation via average pooling features mapped into this cell region, which might introduce noise. Our aggregation method evaluates the relevance of each feature to instruction, filters out irrelevant features, and captures critical clues.






\section{Method}

\begin{comment}
In this section, we will expound the process of building a grid memory map in Sec.\ \ref{sec:grid_memory_mapping} and Sec.\ \ref{sec:instruction relevance_grid_memory_encoding}, we explain how to utilize the map to benefit VLN in Sec.\ \ref{sec:cross-modal_reasoning}. We pre-train our model in discrete environments and then transfer the pre-trained model to continuous environments, which is shown in Sec\ \ref{sec:navigation_setups}.
\end{comment}


\subsection{Navigation Setups}\label{sec:navigation_setups}

For VLN in discrete environments, the navigation connectivity graph\ $\mathcal{G}=\{\mathcal{V},\mathcal{E}\}$ is provided in the Matterport3D simulator\ \cite{matterport3d}, where\ $\mathcal{V}$ denotes navigable
nodes and\ $\mathcal{E}$ denotes edges. An agent is equipped with
RGB cameras, depth cameras, and a GPS sensor. Initialized at a starting
node and given natural language instructions, the agent needs to explore
the navigation connectivity graph $\mathcal{G}$ and reach the target
node. \ $\mathcal{W}=\{w_{l}\}_{l=1}^{L}$ denote the word embeddings
of the instruction with $L$ words. At each time step $t$, the agent
observes panoramic RGB images $\mathcal{R}_{t}=\{r_{t,k}\}_{k=1}^{K}$ and the depth images $\mathcal{D}_{t}=\{d_{t,k}\}_{k=1}^{K}$ of its current node $\mathcal{V}_{t}$, which contains $K$ single
view images. The agent is aware of a few navigable
views $\mathcal{N}(\mathcal{R}_{t})\in\mathcal{R}_{t}$ corresponding to its neighboring nodes and their coordinates. 

VLN in continuous environments is established over Habitat\ \cite{2019habitat}, where the agent's position $\mathcal{P}_{t}$ can be any point in the open space. In each navigation step, we use a pre-trained waypoint predictor\ \cite{Hong2022bridging} to generate navigable waypoints in continuous environments, which assimilates the task with the VLN in discrete environments.

\begin{comment}
Specifically, we use a pre-trained waypoint
predictor\ \cite{Hong2022bridging} to generate navigable waypoints in continuous environments,
at each step, the waypoint predictor infers a local sub-graph which
consists of a set of edges pointing from the agent toward accessible
positions in space. As a result, VLN in continuous spaces can be performed effectively using high-level actions, and the VLN model that is pre-trained in discrete environments can be used in continuous environments.
\end{comment}




\subsection{Grid Memory Mapping}\label{sec:grid_memory_mapping}

As illustrated in Fig.\ \ref{fig:framework}, we present our grid memory mapping pipeline. 

At each navigation step $t$, we first calculate the fine-grained visual representations and their corresponding coordinate into the grid memory. For the panoramic RGB images $\mathcal{R}_{t}=\{r_{t,k}\}_{k=1}^{K}$, we use a pre-trained ViT-B/32-CLIP\ \cite{radford2021learning} model to extract grid features $G_{t}=\{g_{t,k}\in\mathcal{\mathbb{R}}^{H\times W\times D}\}_{k=1}^{K}$, and the grid feature cell of row $h$ column $w$ is denoted as $g_{t,k,h,w}\in {\mathbb{R}}^{D}$. The corresponding depth images $\mathcal{D}_{t}$ are downsized to the same scale as $\mathcal{D}_{t}^{'}=\{d_{t,k}^{'}\in\mathcal{\mathbb{R}}^{H\times W}\}$, and the depth feature cell of row $h$ column $w$ is denoted as $d_{t,k,h,w}^{'}$. For convenience, we denote all the subscripts $(k,h,w)$ as $i\in(1, K\cdot H\cdot W)$. Similar to \cite{vln-pano2real} and \cite{huang23vlmaps}, we calculate the absolute position $P(g_{t,i})$ of the position of each grid feature $g_{t,i}$:
\begin{align}
&P(g_{t,i})=(x_{t,i}\ ,\ y_{t,i})\\&=(\mathcal{X}_{t}+d_{t,i}^{line}\cdot cos\theta_{t,i}\ ,\ \mathcal{Y}_{t}+d_{t,i}^{line}\cdot sin\theta_{t,i})\notag
\end{align}
where $\{\mathcal{X}_{t},\mathcal{Y}_{t}\}$ denotes the the agent's current coordinate, $\theta_{i}$ denotes the heading angle between the position of $g_{t,i}$ and the agent, $d_{i}^{line}$ denotes the euclidean distance between the position of $g_{t,i}$ and the agent calculated by $d_{t,i}^{'}$ and $\theta_{t,i}$. We store all these grid features and their absolute coordinates into the grid memory:
\begin{align}
\mathcal{M}_{t}= \mathcal{M}_{t-1} \cup \{[{g}_{t,i}, P({g}_{t,i})]\}_{i=1}^{I}
\end{align}


Then we proposed a dynamic coordinate transformation method for constructing the grid memory map from the grid memory. Intuitively, we can construct the grid map as shown in Fig.\ \ref{fig:map_grow} (a). The visited environment is represented by projecting all historical observations $g_{t,i}$ into a unified map based on their absolute coordinates $P(g_{t, i})$. However, such a construction manner has two drawbacks. First, it's not intuitive enough to align with the local panoramic observation of the current agent. Second, it's difficult to determine the scale and extent of the map without prior information about the environment~\cite{zhao2022target}. To address these deficiencies, we proposed a new mapping method to construct an egocentric and dynamically growing map, as illustrated in Fig.\ \ref{fig:map_grow} (b). At each step, we build a grid map in an egocentric view by projecting all features of the grid memory $\mathcal{M}_{t}$ into a new planar cartesian coordinate system with the agent's position as the coordinate origin and the agent's current direction in the positive direction of the y-axis.
In this new coordinate system of an egocentric view, for each grid feature $g_{\tau,i}$,$ \tau\in(1,t)$, we can calculate the new step $t$ coordinates $P^{t}(g_{\tau,i})$:
\begin{align}
&P^{t}({g}_{\tau,i})=(x_{\tau,i}^{t}\ ,\ y_{\tau,i}^{t})\notag\\&=(({x}_{\tau,i}-\mathcal{X}_{t})\cdot cos\Theta_{t}+({y}_{\tau,i}-\mathcal{Y}_{t})\cdot sin\Theta_{t}\ ,\ \notag\\&\ \ \ \ \ \ ({y}_{\tau,i}-\mathcal{Y}_{t})\cdot cos\Theta_{t}-({x}_{\tau,i}-\mathcal{X}_{t})\cdot sin\Theta_{t})\label{formula:relative_coordinates}
\end{align}
where $\Theta_{t}$ represents the heading angle between the new coordinate system and the old coordinate system. 

Further, we construct the grid memory map with the grid feature and their new coordinate. 
The size of the grid memory map should be confirmed first. At step $t$, the grid memory map takes $L_{t}$ as the side length: 
\begin{equation}
L_{t}=2\times max(max(\{|x_{\tau,i}^{t}|\})\ ,\ max(\{|y_{\tau,i}^{t}|\})\label{formula:side_length}
\end{equation}
So that the size of the grid memory map increases with the expansion of the visited environment. The agent is always in the center of this map and aligned with current panoramic observations in an egocentric view. Then the map is divided into $N$$\times$$N$ cells and all features of $\mathcal{M}_{t}$ are projected into these cells according to their new coordinate. Finally, we construct the grid memory map containing multiple fine-grained visual representations under each grid region in each grid cell. 

% Figure environment removed

\subsection{Navigation model}

\subsubsection{Instruction and Observation Encoding}\label{rec:observation_encoding}
\par
For the text encoding for instruction, each word embedding in $\mathcal{W}$ is added with a position embedding and a token type embedding. All word tokens are then fed into a multi-layer transformer to obtain word representations, denoted as $\mathcal{W}^{'}=\{w_{l}^{'}\}_{l=1}^{L}$.

For view images $\mathcal{R}_{t}$ of the panoramic observation, we use the ViT-B/16\ \cite{2020image-vit} pre-trained on ImageNet to extract visual features $\mathcal{R}_{t}^{'}$.
Then we represent their relative angles as $a_{t}=(sin\theta_{t}^a,cos\theta_{t}^a,sin\varphi_{t}^a,cos\varphi_{t}^a)$, where $\theta_{t}^a$ and $\varphi_{t}^a$ are the relative heading
and elevation angles to the agent's orientation. The candidate waypoints are represented as $\mathcal{N}(\mathcal{R}_{t}^{'})$, and the line distances between waypoints and the current agent are denoted as $b_{t}$. Similarly, we represent the relative angles between the agent and the start waypoint as $c_{t}=(sin\theta_{t}^c,cos\theta_{t}^c,sin\varphi_{t}^c,cos\varphi_{t}^c)$. Then we concatenate the line distance $dist_{line}(\mathcal{V}_{0},\mathcal{V}_{t})$,
navigation trajectory length $dist_{traj}(\mathcal{V}_{0},\mathcal{V}_{t})$,
and action step $dist_{step}(\mathcal{V}_{0},\mathcal{V}_{t})$
between agent and the start waypoint to obtain $e_{t}=(dist_{line}(\mathcal{V}_{0},\mathcal{V}_{t}),dist_{traj}(\mathcal{V}_{0},\mathcal{V}_{t}),dist_{step}(\mathcal{V}_{0},\mathcal{V}_{t}))$.
Finally, the observation embeddings
are as follows:
\vspace{-3pt}
\begin{equation}
\small
\mathcal{O}_{t}=LN(W_{1}^\mathcal{O}[\mathcal{R}_{t}^{'};\mathcal{N}(\mathcal{R}_{t}^{'})])+LN(W_{2}^\mathcal{O}[a_{t};b_{t};c_{t};e_{t}])
\end{equation}
where the $LN$ denotes layer normalization, $W_{1}^\mathcal{O}$ and $W_{2}^\mathcal{O}$ are learnable parameters. A special “stop” token $\mathcal{O}_{t,0}$ is added to $\mathcal{O}_{t}$ for the stop action. 

We use a two-layer transformer to model relations among observation embeddings and output $\mathcal{O}_{t}^{'}$.

\subsubsection{Grid Memory Encoding}\label{sec:instruction relevance_grid_memory_encoding}

Following Sec.~\ref{sec:grid_memory_mapping}, we aggregate the grid features in each grid memory map cell into the $N$$\times$$N$ map features $M_{t}$. Specifically, due to the complexity of the navigation environment, a large number of grid features within each grid region are not all needed by the agent to complete navigation. The agent needs more critical and highly correlated information with the instruction to understand the environment. Therefore, we propose an instruction relevance method to aggregate multiple grid features in the grid memory map. For grid features in each cell $\mathcal{M}_{t,m,n}^{rel}=\{\hat{g}_{j}\in\mathcal{\mathbb{R}}^{D}\}_{j=1}^{J}$, where
the corresponding coordinates $\{P^{rel}_t(\hat{g}_{j})\}_{j=1}^{J}$ are all within the gird cell of row $m$ column $n$, we evaluate the relevance of each grid feature to each
token of navigation instruction by computing the relevance matrix $A$ as:
\begin{equation}
A=(\mathcal{M}_{t,m,n}^{rel}W_{1}^{A})(\mathcal{W}^{'}W_{2}^{A})^{T}
\end{equation}
where $W_{1}^{A}$ and $W_{2}^{A}$ are learned parameters. After that, we compute the row-wise max-pooling on $A$ to evaluate the relevance of each grid feature to the instruction as:
\begin{equation}
\alpha_{j}=max(\{A_{j,l}\}_{l=1}^{L})
\end{equation}
At last, we aggregate the grid features within each cell into an embedding vector $E_{t,m,n}$:
\begin{equation}
\eta=softmax(\{\alpha_{j}\}_{j=1}^{J})
\end{equation}
\begin{equation}
E_{t,m,n}=\sum_{j=1}^{J}\eta_{j}(W^{E}\hat{g}_{j})
\end{equation}
where $W^{E}$ are learned parameters.
To represent the spatial relations, we introduce positional information into our grid memory map. Specifically, between each cell center and agent,
we denote the line distance as $q_{t}^{M}$ and represent relative
heading angles as $h_{t}^{M}=(sin\Phi_{t}^{M},cos\Phi_{t}^{M})$.
Then the map features can be obtained:

{\small
\begin{equation}
M_{t}=LN(E_{t})+LN(W^{M}[q_{t}^{M};h_{t}^{M}])
\end{equation}
}
where $W^{M}$ are learned parameters.

\subsubsection{Navigation Trajectory Encoding}\label{sec:navigation_trajectory_encoding}
In order to implement global action planning, we further introduce the navigation trajectory representation into our GridMM. As shown in Sec. \ref{rec:observation_encoding}, at time step $t$, the agent receives panoramic features $\mathcal{O}_{t}^{'}$ of waypoint $\mathcal{V}_{t}$.
Then we can obtain visual representation $Avg(\mathcal{O}_{t}^{'})$
of the current waypoint by average pooling of $\mathcal{O}_{t}^{'}$.
As the agent also partially observes candidate waypoints, we use the
view image features $\mathcal{N}(\mathcal{O}_{t}^{'})$ that contains
these navigable waypoints as their visual representation. Between waypoints
and current agent, we denote the line distances as $q^{\mathcal{T}}$,
the relative heading angles as $h_{t}^{\mathcal{T}}=(sin\Phi_{t}^{\mathcal{T}},cos\Phi_{t}^{\mathcal{T}})$,
and the action step embeddings as $u^{\mathcal{T}}$. All historical
waypoint features $\{Avg(\mathcal{O}_{i}^{'})\}_{i=1}^{t-1}$, current
waypoint feature $Avg(\mathcal{O}_{t}^{'})$ and the candidate waypoint
features $\mathcal{N}(\mathcal{O}_{t}^{'})$ form the navigation trajectory:
\begin{align}
\small
\mathcal{T}_{t}&=[\{LN(Avg(\mathcal{O}_{i}^{'})+LN(W_{1}^{\mathcal{T}}[q_{i}^{\mathcal{T}};h_{i}^{\mathcal{T}}])+u_{i}^{\mathcal{T}}\}_{i=1}^{t};\notag\\&\ \ \ \ \ \ \ LN(\mathcal{N}(\mathcal{O}_{t}^{'}))+LN(W_{2}^{\mathcal{T}}[q_{\mathcal{N}}^{\mathcal{T}};h_{\mathcal{N}}^{\mathcal{T}}])+u_{\mathcal{N}}^{\mathcal{T}}]
\end{align}
where $W_{1}^{\mathcal{T}}$ and $W_{2}^{\mathcal{T}}$ are learned parameters, a special “stop” token $\mathcal{T}_{t,0}$ is added to $\mathcal{T}_{t}$ for the stop action.

\subsubsection{Cross-modal Reasoning}\label{sec:cross-modal_reasoning}

As illustrated in Fig. \ref{fig:framework}, we concatenate map features and navigation trajectory as $[M_{t};\mathcal{T}_{t}]$, and then use a cross-modal transformer to fuse features from instruction $\mathcal{W}^{'}$ and model space-time
relations, forming the features $[M_{t}^{'};\mathcal{T}_{t}^{'}]$. We specifically designed a training loss $\mathcal{L}_{HER}$ (illustrated
in Sec. \ref{sec:training_and_inference}) to supervise this module.

Subsequently, we use another cross-modal transformer with 4 layers to
model vision-language relations and space-time relations. Specifically,
each transformer layer consists of a cross-attention layer and a self-attention
layer. For the cross-attention layer, we input panoramic observation and navigation
trajectory $[\mathcal{O}_{t}^{'};\mathcal{T}_{t}^{'}]$ as queries which
attend over encoded instruction tokens, navigation trajectory and
map features $[\mathcal{W}^{'};\mathcal{T}_{t}^{'}; M_{t}^{'}]$, and
then the self-attention layer takes encoded panoramic observation and navigation
trajectory as input for action reasoning. We denote the output as $[\hat{\mathcal{O}_{t}};\mathcal{\hat{T}}_{t}]$.

\subsubsection{Action Prediction}

We predict a local navigation score for each navigable candidate view $\mathcal{N}(\mathcal{\hat{O}}_{t})$ as below:
\begin{equation}
S_t^{\mathcal{O}}=FFN(\mathcal{N}(\mathcal{\hat{O}}_{t}))
\end{equation}
and predict a global navigation score for each candidate navigable
waypoints $\mathcal{N}(\mathcal{\hat{T}}_{t})$ as below:
\begin{equation}
S_t^{\mathcal{T}}=FFN(\mathcal{N}(\mathcal{\hat{T}}_{t}))
\end{equation}
 where $FFN$ denotes a two-layer feed-forward network.
 To be noted, $S_{t,0}^{\mathcal{O}}$ and $S_{t,0}^{\mathcal{T}}$ are the stop scores. Two separate FFNs are
used to predict local action scores and global action scores, we gated fuse the scores following \cite{chen2022think-GL}:
\begin{equation}
S_t^{fusion}=\lambda_{t}S_t^{\mathcal{O}}+(1-\lambda_{t})S_t^{\mathcal{T}}
\end{equation}
where $\lambda_{t}=sigmoid(FFN([\mathcal{\hat{O}}_{t,0};\mathcal{\hat{T}}_{t,0}]))$

\subsection{Training and Inference}\label{sec:training_and_inference}

\paragraph*{Pre-training.}We utilize four tasks to pre-train our model. 

1) Masked language modeling (MLM). We randomly mask out the words
of the instruction with a probability of 15\% and then predict the masked
words $\mathcal{W}_{masked}$.

2) Masked view modeling (MVM). We randomly mask out view
images with a probability of 15\% and predict the semantic labels of masked view images. Similar to \cite{chen2022think-GL}, the target
labels for view images are obtained by an image classification model \cite{2020image-vit} pre-trained on ImageNet.

3) Single-step action prediction (SAP). Given the ground truth action $\mathcal{A}_{t}$, the SAP loss is defined as follows:
\begin{equation}
\mathcal{L}_{SAP}=\sum_{t=1}^{T}CrossEntropy(S_t^{fusion},\mathcal{A}_{t})
\end{equation}

4) Historical environment reasoning (HER). The HER requires the agent to predict the next action only based on the map features and navigation trajectory, without panoramic observations:
\begin{equation}
S_t^{HER}=FFN(\mathcal{N}(\mathcal{T}_{t}^{'}))
\end{equation}
\begin{equation}
\mathcal{L}_{HER}=\sum_{t=1}^{T}CrossEntropy(S_t^{HER},\mathcal{A}_{t})
\end{equation}
\paragraph*{Fine-tuning and Inference.}

For fine-tuning, we follow existing works~\cite{chen2022think-GL,Hong2022bridging} to use Dagger~\cite{ross2011reduction} training techniques. Different from the pre-training process which uses the demonstration path, the supervision of fine-tuning
is from a pseudo-interactive demonstrator which selects a navigable
waypoint as the next target with the overall shortest distance from
the current waypoint to the destination. 

For inference, the model predicts an action at each step. If it's not the stop action, the agent performs this action and moves to the predicted waypoint, otherwise, the agent stops at the current waypoint.
In VLN tasks of discrete environments, the agent will be forced to stop if it exceeds the maximum action
steps and then return to the waypoint with maximum stop probability as its final prediction. Differently, the agent stops where it is when it exceeds the maximum action steps in the continuous environments.

\section{Experiment}\label{sec:experiment}

\subsection{Datasets and Evaluation Metrics}
We evaluate our model on the REVERIE~\cite{2020reverie}, R2R~\cite{VLN-2018vision}, SOON~\cite{zhu2021soon} datasets in discrete environments and R2R-CE\ \cite{Krantz2020r2r-ce} in continuous environments. 

\textbf{REVERIE} contains high-level instructions which contain 21 words on average and the path length is between 4 and 7 steps. The predefined object bounding boxes are provided for each panorama, and the agent should select the correct object bounding box from candidates at the end of the navigation path.

\textbf{R2R} provides step-by-step instructions. The average length of instructions is 32 words and the average path length is 6 steps.  

\textbf{SOON} also provides instructions that describe the target locations and target objects. The average length of instructions is 47 words, and the path length is between 2 and 21 steps. However, the object bounding boxes are not provided, the agent needs to predict the center location of the target object. Similar to the settings in~\cite{chen2022think-GL}, we use object detectors~\cite{anderson2018bottom} to obtain candidate object boxes.

\begin{comment}
RxR\ \cite{2020-RXR} is a larger multilingual VLN dataset containing 126K instructions in English, Hindi, and Telugu, and it includes trajectories that are diverse in terms of length (average is 15m) and the landmarks that are referred to, which is more challenging.
\end{comment}
\textbf{R2R-CE} are collected based on the
discrete Matterport3D environments\ \cite{matterport3d}, but use the Habitat simulator\ \cite{ramakrishnan2021habitat} to navigate in the continuous environments.

There are several standard metrics\ \cite{VLN-2018vision,2020reverie} in
VLN for evaluating the agent’s performance, including Trajectory Length (TL), Navigation Error (NE), Success Rate
(SR), SR given the Oracle stop policy (OSR), Normalized inverse of the Path Length (SPL), Remote Grounding Success (RGS), RGS penalized by Path Length (RGSPL). %Normalized Dynamic Time Warping (nDTW), and Success weighted by normalized Dynamic Time Warping (SDTW). 

\begin{comment}
More details about datasets and evaluation metrics can be found in supplementary materials.
\end{comment}

\subsection{Implementation Details}
We use the ViT-B/16\ \cite{2020image-vit} pre-trained on ImageNet to extract panoramic view features $\mathcal{R}_{t}^{'}$ on all datasets and extract object features on the REVERIE dataset as it provides bounding boxes. The BUTD object detector~\cite{anderson2018bottom} is utilized on the SOON dataset to extract object bounding boxes. We adopt the pre-trained CLIP-ViT-B/32~\cite{radford2021learning} to extract grid features $G_{t}$ on all datasets. The number of layers for the language encoder, panorama encoder, map and trajectory encoder, and the cross-modal reasoning encoder are  respectively set as 9, 2, 1, and 4 as shown in Fig.~\ref{fig:framework}, all with a hidden size of 768. The parameters of all transformer layers are initialized with the pre-trained LXMERT~\cite{2019-lxmert}.

\subsection{Comparison to State-of-the-Art Methods}
Table \ref{REVERIE_sota},\ref{R2R_sota},\ref{SOON_sota} compare our approach with the previous VLN methods on the REVERIE, R2R and SOON benchmarks. Table~\ref{R2R-CE_sota} compares our approach with the previous VLN-CE methods on the R2R-CE benchmark. Our approach achieves state-of-the-art performance on most datasets, demonstrating the effectiveness of the proposed approach. For the val unseen split of REVERIE dataset in Table \ref{REVERIE_sota}, our model outperforms the previous DUET \cite{chen2022think-GL} by 4.39\% on SR and 2.74\% on SPL. As shown in Table \ref{R2R_sota} and \ref{SOON_sota}, it also shows performance gains on the R2R and
SOON dataset compared to DUET. In particular, our approach significantly outperforms all previous methods on the R2R-CE dataset in Table \ref{R2R-CE_sota}, which demonstrates the potential of grid-based mapping method.

\begin{table*}
\small
\tabcolsep=0.15cm
\centering
{\centering{}}%
\begin{tabular}{c|cccc|cc|cccc|cc}
\hline 
\multirow{3}{*}{Methods} & \multicolumn{6}{c|}{{Val Unseen}} & \multicolumn{6}{c}{{Test Unseen}}\tabularnewline
\cline{2-13} \cline{3-13} \cline{4-13} \cline{5-13} \cline{6-13} \cline{7-13} \cline{8-13} \cline{9-13} \cline{10-13} \cline{11-13} \cline{12-13} \cline{13-13}
 & \multicolumn{4}{c|}{{Navigation}} & \multicolumn{2}{c|}{{Grounding}} & \multicolumn{4}{c|}{{Navigation}} & \multicolumn{2}{c}{{Grounding}}\tabularnewline
\cline{2-13} \cline{3-13} \cline{4-13} \cline{5-13} \cline{6-13} \cline{7-13} \cline{8-13} \cline{9-13} \cline{10-13} \cline{11-13} \cline{12-13} \cline{13-13}
 & {TL\textdownarrow{}} & {OSR\textuparrow{}} & {SR\textuparrow{}} & {SPL\textuparrow{}} & {RGS\textuparrow{}} & {RGSPL\textuparrow{}} & {TL\textdownarrow{}} & {OSR\textuparrow{}} & {SR\textuparrow{}} & {SPL\textuparrow{}} & {RGS\textuparrow{}} & {RGSPL\textuparrow{}}\tabularnewline
\hline 
{VLNBERT\ \cite{hong2021vln-bert}} & {16.78} & {35.02} & {30.67} & {24.90} & {18.77} & {15.27} & {15.68} & {32.91} & {29.61} & {23.99} & {16.50} & {13.51}\tabularnewline
 
{AirBERT\ \cite{2021airbert}} & {18.71} & {34.51} & {27.89} & {21.88} & {18.23} & {14.18} & {17.91} & {34.20} & {30.28} & {23.61} & {16.83} & {13.28}\tabularnewline

{HOP\ \cite{qiao2022HOP}} & {16.46} & {36.24} & {31.78} & {26.11} & {18.85} & {15.73} & {16.38} & {33.06} & {30.17} & {24.34} & {17.69} & {14.34}\tabularnewline

{HAMT\ \cite{chen2021history}} & {14.08} & {36.84} & {32.95} & {30.20} & {18.92} & {17.28} & {13.62} & {33.41} & {30.40} & {26.67} & {14.88} & {13.08}\tabularnewline

{TD-STP\ \cite{zhao2022target}} & {-} & {39.48} & {34.88} & {27.32} & {21.16} & {16.56} & {-} & {40.26} & {35.89} & {27.51} & {19.88} & {15.40}\tabularnewline

{DUET\ \cite{chen2022think-GL}} & {22.11} & {51.07} & {46.98} & {33.73} & {32.15} & {23.03} & {21.30} & {56.91} & {52.51} & {36.06} & {31.88} & {22.06}\tabularnewline 


\iffalse
{LAD\ \cite{VLN_LAD_2023}} & {26.39} & {63.96} & {57.00} & {37.92} & {37.80} & {24.59} & {25.87} & {62.02} & {56.53} & {37.8} & {35.31} & {23.38}\tabularnewline
\fi

{BEVBert\ \cite{Dong2022bevbert}} & {-} & {56.40} & \textbf{51.78} & {36.37} & \textbf{34.71} & {24.44} & {-} & {57.26} & {52.81} & {36.41} & {32.06} & {22.09}\tabularnewline

\hline 
GMap (Ours) & {23.20} & \textbf{57.48} & {51.37} & \textbf{36.47} & {34.57} & \textbf{24.56} & {19.97} & \textbf{59.55} & \textbf{53.13} & \textbf{36.60} & \textbf{34.87} & \textbf{23.45}\tabularnewline
\hline 
\end{tabular}
\vspace{2pt}
\caption{Evaluation on REVERIE dataset.}\label{REVERIE_sota}
\end{table*}



\begin{table}
\small
\tabcolsep=0.05cm
\centering{}%
\begin{tabular}{c|cccc|cccc}
\hline 
\multirow{2}{*}{Methods} & \multicolumn{4}{c|}{Val Unseen} & \multicolumn{4}{c}{Test Unseen}\tabularnewline
\cline{2-9} \cline{3-9} \cline{4-9} \cline{5-9} \cline{6-9} \cline{7-9} \cline{8-9} \cline{9-9}
& TL\textdownarrow{} & NE\textdownarrow{} & SR\textuparrow{} & SPL\textuparrow{} & TL\textdownarrow{} & NE\textdownarrow{} & SR\textuparrow{} & SPL\textuparrow{}\tabularnewline
\hline 
VLNBERT\ \cite{hong2021vln-bert} & 12.01 & 3.93 & 63 & 57 & 12.35 & 4.09 & 63 & 57\tabularnewline

 
AirBERT\ \cite{2021airbert} & 11.78 & 4.01 & 62 & 56 & 12.41 & 4.13 & 62 & 57\tabularnewline

SEvol\ \cite{Chen_2022_Reinforced} & 12.26 & 3.99 & 62 & 57 & 13.40 & 4.13 & 62 & 57\tabularnewline
 
HOP\ \cite{qiao2022HOP} & 12.27 & 3.80 & 64 & 57 & 12.68 & 3.83 & 64 & 59\tabularnewline
 
HAMT\ \cite{chen2021history} & 11.46 & 2.29 & 66 & 61 & 12.27 & 3.93  & 65 & 60\tabularnewline
 
TD-STP\ \cite{zhao2022target} & - & 3.22 & 70 & 63 & - & 3.73 & 67 & 61\tabularnewline

DUET\ \cite{chen2022think-GL} & 13.94 & 3.31 & 72 & 60 & 14.73 & 3.65 & 69 & 59\tabularnewline
 
BEVBert\ \cite{Dong2022bevbert} & 14.55 & \textbf{2.81}  & \textbf{75} & \textbf{64} & 15.87 & \textbf{3.13} & \textbf{73} & \textbf{62}\tabularnewline
\hline

GMap (Ours) & 13.27 & 2.83 & \textbf{75} & \textbf{64} & 14.43 & 3.35 & \textbf{73} & \textbf{62} \tabularnewline

\hline 
\end{tabular}
\vspace{2pt}
\caption{Evaluation on R2R dataset.}\label{R2R_sota}
\end{table}


\begin{table}
\small
\tabcolsep=0.04cm
\centering{}%
\begin{tabular}{c|c|ccccc}
\hline 
Split & Method & TL\textdownarrow{} & OSR\textuparrow{} & SR\textuparrow{} & SPL\textuparrow{} & RGSPL\textuparrow{}\tabularnewline\cline{2-7} \cline{3-7} \cline{4-7} \cline{5-7} \cline{6-7} \cline{7-7}
\hline 
\multirow{3}{*}{Val Unseen} & GBE\ \cite{zhu2021soon} & 28.96 & 28.54 & 19.52 & 13.34 & 1.16\tabularnewline
& DUET\ \cite{chen2022think-GL} & 36.20 & 50.91 & 36.28 & 22.58 & 3.75\tabularnewline
& Gmap (Ours) & 38.92 & \textbf{53.39} & \textbf{37.46} & \textbf{24.81} & \textbf{3.91}\tabularnewline
\hline 
\multirow{3}{*}{Test Unseen} & GBE\ \cite{zhu2021soon} & 27.88 & 21.45 &  12.90 & 9.23 & 0.45
\tabularnewline
& DUET\ \cite{chen2022think-GL} & 41.83 & 43.00 & 33.44 &  \textbf{21.42} &  \textbf{4.17}\tabularnewline
& Gmap (Ours) & 46.20 & \textbf{48.02} & \textbf{36.27} & 21.25 & 4.15\tabularnewline
\hline 
\end{tabular}
\vspace{1pt}
\caption{Evaluation on SOON dataset.}\label{SOON_sota}
\end{table}


\begin{table*}
\small
\tabcolsep=0.1cm
\centering{}%
\begin{tabular}{c|ccccc|ccccc|ccccc}
\hline 
\multirow{2}{*}{Methods} & \multicolumn{5}{c|}{Val Seen} & \multicolumn{5}{c|}{Val Unseen} & \multicolumn{5}{c}{Test Unseen}\tabularnewline
\cline{2-16} \cline{3-16} \cline{4-16} \cline{5-16} \cline{6-16} \cline{7-16} \cline{8-16} \cline{9-16} \cline{10-16} \cline{11-16} \cline{12-16} \cline{13-16} \cline{14-16} \cline{15-16} \cline{16-16}
 & TL\textdownarrow{} & NE\textdownarrow{} & OSR\textuparrow{} & SR\textuparrow{} & SPL\textuparrow{} & TL\textdownarrow{} & NE\textdownarrow{} & OSR\textuparrow{} & SR\textuparrow{} & SPL\textuparrow{} & TL\textdownarrow{} & NE\textdownarrow{} & OSR\textuparrow{} & SR\textuparrow{} & SPL\textuparrow{}\tabularnewline
\hline 
VLN-CE\textcolor{black}{${\color{red}^{{\color{black}\ast}}}$}\ \cite{Krantz2020r2r-ce} & 9.26 & 7.12 & 46 & 37 & 35 & 8.64 & 7.37 & 40 & 32 & 30 & 8.85 & 7.91 & 36 & 28 & 25\tabularnewline
 
AG-CMTP\ \cite{chen2021topo} & - & 6.60 & 56.2 & 35.9 & 30.5 & - & 7.9 & 39.2 &23.1 & 19.1 & - & - & - & - & -\tabularnewline

R2R-CMTP\ \cite{chen2021topo} & - & 7.10 & 45.4 & 36.1 & 31.2 & - & 7.9 & 38.0 & 26.4 & 22.7 & - & - & - & - & -\tabularnewline

WPN\ \cite{Krantz2021waypointmodel} & 8.54 & 5.48 & 53 & 46 & 43 & 7.62 & 6.31 & 40 & 36 & 34 & 8.02 & 6.65 & 37 & 32 & 30\tabularnewline

LAW\textcolor{black}{${\color{red}^{{\color{black}\ast}}}$}\ \cite{Raychaudhuri2021law} & 9.34 & 6.35 & 49 & 40 & 37 & 8.89 & 6.83 & 44 & 35 & 31 & - & - & - & - & -\tabularnewline

CM$^{2}$\textcolor{black}{${\color{red}^{{\color{black}\ast}}}$}\ \cite{georgakis2022cm2} & 12.05 & 6.10 & 50.7 & 42.9 & 34.8 & 11.54 & 7.02 & 41.5 & 34.3 & 27.6  
 & 13.9 & 7.7 & 39 & 31 & 24\tabularnewline

CM$^{2}$-GT\textcolor{black}{${\color{red}^{{\color{black}\ast}}}$}\ \cite{georgakis2022cm2} & 12.60 & 4.81 & 58.3 & 52.8 & 41.8 & 10.68 & 6.23 & 41.3 & 37.0 & 30.6 & - & - & - & - & -\tabularnewline

WS-MGMap\textcolor{black}{${\color{red}^{{\color{black}\ast}}}$}\ \cite{chen2022weakly} & 10.12 & 5.65 & 51.7 & 46.9 & 43.4 & 10.00 & 6.28 & 47.6 & 38.9 & 34.3 & 12.30 & 7.11 & 45 & 35 & 28\tabularnewline

Sim-2-sim\ \cite{krantz2022sim2sim} & 11.18 & 4.67 & 61 & 52 & 44 & 10.69 & 6.07 & 52 & 43 & 36 & 11.43 & 6.17 & 52 & 44 & 37\tabularnewline

ERG\textcolor{black}{${\color{red}^{{\color{black}\dagger}}}$}\ \cite{Ting2023graph-vlnce} & 11.8 & 5.04 & 61 & 46 & 42 & 9.96 & 6.20 & 48 & 39 & 35 & - & - & - & - & -\tabularnewline

CMA\textcolor{black}{${\color{red}^{{\color{black}\dagger}}}$} \ \cite{Hong2022bridging} & 11.47 & 5.20 & 61 & 51 & 45 & 10.90 & 6.20 & 52 & 41 & 36 & 11.85 & 6.30 & 49 & 38 & 33\tabularnewline

VLNBERT\textcolor{black}{${\color{red}^{{\color{black}\dagger}}}$}\ \cite{Hong2022bridging} & 12.50 & 5.02 & 59 & 50 & 44 & 12.23 & 5.74 & 53 & 44 & 39 & 13.31 & 5.89 & 51 & 42 & 36\tabularnewline

\hline 

DUET\textcolor{black}{${\color{red}^{{\color{black}\dagger}}}$} (Ours)\ \cite{chen2022think-GL} & 12.62 & \textbf{4.13} & 67 & 57 & 49 & 13.04 & 5.26 & 58 & 47 & 39 & 13.13 & 5.82 & 50 & 42 & 36\tabularnewline

GMap\textcolor{black}{${\color{red}^{{\color{black}\dagger}}}$} (Ours) & 12.69 & 4.21 & \textbf{69} & \textbf{59} & \textbf{51} & 13.36 & \textbf{5.11} & \textbf{61} & \textbf{49} & \textbf{41} & 13.31 & \textbf{5.64} & \textbf{56} & \textbf{46} & \textbf{39}
 \tabularnewline
\hline 
\end{tabular}
\vspace{2pt}
\caption{Evaluation on R2R-CE dataset. The methods marked with \textcolor{black}{${\color{red}^{{\color{black}\ast}}}$} use a forward-facing camera with a 90-degree HFOV instead of panoramic images. The methods marked with \textcolor{black}{${\color{red}^{{\color{black}\dagger}}}$} use the same waypoint predictor\ \cite{Hong2022bridging} for a fair comparison. Especially, we transfer the pre-trained DUET model to R2R-CE.}\label{R2R-CE_sota}
\end{table*}


\begin{table}
\small
\tabcolsep=0.08cm
\centering{}%
\begin{tabular}{c|ccccc}
\hline 
Mapping methods & TL\textdownarrow{} & NE\textdownarrow{} & OSR\textuparrow{} & SR\textuparrow{} & SPL\textuparrow{}\tabularnewline
\hline 
No Map & 14.61 & 5.64 & 57.24 & 45.19 & 37.82\tabularnewline

DUET (topological map) & 13.04 & 5.26 & 57.91 & 47.02 & 38.86\tabularnewline

GMap (semantic map) & 13.78 & 5.33 & 57.46 & 46.36 & 38.41\tabularnewline
 
GMap (objects) & 13.15 & 5.39 & 59.12 & 47.61 & 40.13\tabularnewline

GMap (grid features) & 13.36 & \textbf{5.11} & \textbf{60.90} & \textbf{49.05} & \textbf{40.99}\tabularnewline
\hline 
\end{tabular}
\vspace{1pt}
\caption{Comparison among different mapping methods on val unseen split of R2R-CE dataset. “No Map” is our baseline method that uses our proposed model but without grid map features. GMap (semantic map) takes top-down semantic map as substitutes for grid features. GMap (objects) takes object features extracted from a detection model\ \cite{zhang2021vinvl} as substitutes for grid features. GMap (grid features) is our proposed approach with grid features.} \label{map_comparison}
\end{table}



\begin{center}
\begin{table}
\small
\noindent\begin{minipage}[t]{1\columnwidth}%
%\tabcolsep=0.08cm
\tabcolsep=0.08cm
\begin{center}
\begin{tabular}{cccc|ccccc}
\hline 
GMap & Ego. & Traj. & Instr. & TL\textdownarrow{} & NE\textdownarrow{} & OSR\textuparrow{} & SR\textuparrow{} & SPL\textuparrow{}\tabularnewline
\hline 
  & $\checkmark$ & $\checkmark$ & & 14.61 & 5.64 & 57.24 & 45.19 & 37.82
\tabularnewline

$\checkmark$ &  &$\checkmark$ &$\checkmark$ & 13.24 & 5.23 & 59.11 & 48.72
 & 40.14\tabularnewline
 
$\checkmark$ & $\checkmark$ &  &$\checkmark$& 13.14 & 5.24 & 58.35 & 47.42
 & 39.41\tabularnewline

$\checkmark$ & $\checkmark$ & $\checkmark$ &  & 13.22 & 5.39 & 59.75 & 48.63 & 39.83\tabularnewline
\hline 
$\checkmark$ & $\checkmark$ & $\checkmark$ &$\checkmark$& 13.36 & \textbf{5.11} & \textbf{60.90} & \textbf{49.05} & \textbf{40.99}\tabularnewline
\hline 
\end{tabular}
\par\end{center}%
\end{minipage}
\vspace{1pt}
\caption{Ablation study results on val unseen split of R2R-CE dataset. “GMap” denotes using grid map features. “Ego.” denotes using the egocentric view shown in Fig.\ \ref{fig:map_grow} (b) instead of the map in Fig.\ \ref{fig:map_grow} (a). “Traj.” denotes using trajectory memory and features. “Instr.” denotes using instruction relevance aggregation method instead of average pooling grid features in each cell.}\label{ablation_study}
\vspace{-0.3cm}
\end{table}
\par\end{center}


\begin{table}
\small
\tabcolsep=0.1cm
\centering{}%
\begin{tabular}{c|ccccc}
\hline 
Map scale & TL\textdownarrow{} & NE\textdownarrow{} & OSR\textuparrow{} & SR\textuparrow{} & SPL\textuparrow{}\tabularnewline
\hline 
8$\times$8 & 13.42 & 5.23 & 58.58 & 47.07 & 39.49\tabularnewline

14$\times$14 & 13.36 & 5.11 & \textbf{60.90} & 49.05 & 40.99\tabularnewline

20$\times$20 & 12.59 & \textbf{4.95} & 57.86 & \textbf{49.86} & \textbf{42.52}\tabularnewline

\hline
\end{tabular}
\vspace{2pt}
\caption{The effect of different map scales on val unseen split of R2R-CE dataset.}\label{map_scale}
\vspace{-0.3cm}
\end{table}

\subsection{Ablation Study}
We compare several different mapping methods on the val unseen split of the R2R-CE dataset, and some analyses of our approach are given.
\vspace{-3ex}
\paragraph*{1) Topological map vs. Semantic map vs. Grid map.} 
As shown in Table \ref{map_comparison}, we compare the effects of three different mapping methods on the R2R-CE dataset. For DUET (topological map), we followed the same model structure as\ \cite{chen2022think-GL}. For GMap (semantic map), we take the top-down semantic map as a substitute for grid features. Specifically, we followed CM$^{2}$\ \cite{georgakis2022cm2} to obtain an egocentric top-down semantic map, and use a convolution layer to extract semantic features in each cell instead of grid features. Different from GMap (grid features), the GMap  (objects) uses a pre-trained object detection model VinVL\ \cite{zhang2021vinvl} to detect multiple objects and extract their features as substitutes for grid features. More detailed experimental setups can be found in the supplementary materials.

In Table~\ref{map_comparison}, all results with environment map (rows 2-5) are better than the baseline method (“No Map” in row 1). This fully demonstrates the necessity of environmental maps for VLN. Furthermore, GMap (grid features) and GMap (objects) are both better than DUET (topological map), which might show that grid map works better than current topology-based mapping methods in VLN-CE. 

The method with a top-down semantic map (in row 3) are beneficial to navigation, but it's still inferior to row 4-5 and even row 2 with a topological map. We suspect that map cell features extracted from the semantic map have a large gap with panoramic visual features. In contrast, previous works \cite{georgakis2022cm2}~\cite{chen2022weakly} use the semantic map and visual features both from UNet~\cite{unet} for feature fusion and navigation.

Results in Table \ref{map_comparison} indicate that grid maps are superior to topological maps and semantic maps on VLN-CE under current settings.
\vspace{-3ex}
\paragraph*{2) Grid features vs. object features.}
By comparing the results of row 4 and row 5 in Table \ref{map_comparison}, we can find out that the grid map using grid features works better than using object features. There are two possible reasons, object features from object detection model \cite{zhang2021vinvl} are not enough to represent all visual information, such as house structure and background, furthermore, grid features from CLIP \cite{radford2021learning} have larger semantic space and greater generalization ability. Different from previous methods \cite{Chen_2022_Reinforced}~\cite{Ting2023graph-vlnce} of obtaining environment representation based on objects, grid features might be another viable option to structure environment maps.
\vspace{-3ex}
\paragraph*{3) Is it necessary that map in an egocentric view?}
As shown in Sec. \ref{sec:grid_memory_mapping} and Fig. \ref{fig:map_grow}, we discussed two coordinate systems for our grid map, \textit{i.e.}, absolute coordinates and dynamically relative coordinates. Row 2 in Table \ref{ablation_study} shows the result of the absolute coordinate system. Specifically, we remove the process of coordinate transformation in Formula (\ref{formula:relative_coordinates}) but keep that side length $L_t$ increases with the expansion of the visited environment in Formula (\ref{formula:side_length}). Then we replaced $q_{t}^{M}$ and $h_{t}^{M}$ with the line distance and heading angle between each cell center and the start waypoint, in Sec. \ref{sec:instruction relevance_grid_memory_encoding}. The experimental results show that the egocentric relative coordinate system works better than the absolute coordinate system under our settings. It might be that a grid map in an egocentric view can be better aligned with local observations in terms of spatial representation.
\vspace{-3ex}
\paragraph*{4) The effect of navigation trajectory information.}
Row 3 is inferior to row 5 in Table \ref{ablation_study}, which verifies the necessity of navigation trajectory. The trajectory help with instruction grounding, for example, grounding the next step is to \textit{“cross in front of the refrigerator”} or to \textit{“walk past the wood table and chairs on your right”} in Fig. \ref{fig:introduction} (c).
\vspace{-3ex}
\paragraph*{5) The effect of instruction relevance aggregation method.} 
As shown in Table \ref{ablation_study}, row 5 with instruction relevance aggregation has better performance than row 4. Row 4 simply aggregates features in each map cell via average pooling, which makes it difficult to dig out critical visual cues. Our aggregation method evaluates the relevance of each grid feature to navigation instruction and uses the attention mechanism to filter out irrelevant features and capture critical clues.
\vspace{-3ex}
\paragraph*{6) The effect of map scale.}
As shown in Table \ref{map_scale}, we evaluate
the scale of grid maps. We observe an upward trend in navigation performance as the map scale increase (row 1 vs. row 2 vs. row 3). This is intuitive because a map with a larger scale can accommodate more environmental details and represent spatial relations more precisely. However, due to heavy computation as the map scale increases, we choose a relatively balanced scale, which is 14$\times$14.


\section{Conclusion}

In this paper, we propose an egocentric and dynamically growing Grid Memory Map to structure the visited environment for VLN. We further propose an instruction relevance aggregation method to capture fine-grained visual clues most relevant to instructions. We compared with various mapping methods used in previous works, and analyze some characteristics of grid map (\textit{e.g.}, features, egocentric view, navigation trajectory, aggregation methods, and map scale). We expect this work will benefit and give some insights into future works in VLN and Embodied AI. 
%In the future, we expect to achieve more efficient methods of environment mapping, instead of a topology or grid structure.

\clearpage
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}
\end{document}
