{
  "title": "Revisiting Event-based Video Frame Interpolation",
  "authors": [
    "Jiaben Chen",
    "Yichen Zhu",
    "Dongze Lian",
    "Jiaqi Yang",
    "Yifu Wang",
    "Renrui Zhang",
    "Xinhang Liu",
    "Shenhan Qian",
    "Laurent Kneip",
    "Shenghua Gao"
  ],
  "submission_date": "2023-07-24T06:51:07+00:00",
  "revised_dates": [],
  "abstract": "Dynamic vision sensors or event cameras provide rich complementary information for video frame interpolation. Existing state-of-the-art methods follow the paradigm of combining both synthesis-based and warping networks. However, few of those methods fully respect the intrinsic characteristics of events streams. Given that event cameras only encode intensity changes and polarity rather than color intensities, estimating optical flow from events is arguably more difficult than from RGB information. We therefore propose to incorporate RGB information in an event-guided optical flow refinement strategy. Moreover, in light of the quasi-continuous nature of the time signals provided by event cameras, we propose a divide-and-conquer strategy in which event-based intermediate frame synthesis happens incrementally in multiple simplified stages rather than in a single, long stage. Extensive experiments on both synthetic and real-world datasets show that these modifications lead to more reliable and realistic intermediate frame results than previous video frame interpolation methods. Our findings underline that a careful consideration of event characteristics such as high temporal density and elevated noise benefits interpolation accuracy.",
  "categories": [
    "cs.CV",
    "cs.RO"
  ],
  "primary_category": "cs.CV",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.12558",
  "pdf_url": null,
  "comment": "Accepted by IROS2023 Project Site: https://jiabenchen.github.io/revisit_event",
  "num_versions": null,
  "size_before_bytes": 2237153,
  "size_after_bytes": 511305
}