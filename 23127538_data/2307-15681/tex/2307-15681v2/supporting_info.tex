\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{setspace}
\usepackage{etoolbox}
\usepackage{natbib}
\usepackage{setspace}


\singlespacing


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\title{\large{Supporting Information for:} \\ \Large{\textbf{A Continuous-Time Dynamic Factor Model for Intensive Longitudinal Data Arising from Mobile Health Studies}} \\ \large{by \\ Madeline R. Abbott, Walter H. Dempsey, Inbal Nahum-Shani, Cho Y. Lam, David W. Wetter, and Jeremy M. G. Taylor}}

\date{  }


\begin{document}


\maketitle

\renewcommand{\figurename}{Supplementary Figure}
\renewcommand{\tablename}{Supplementary Table}
\renewcommand{\thesection}{A.\arabic{section}}


\setcounter{section}{0}
\section*{Section A}

\section{Derivation of the analytic form of the conditional covariance function of the OU process}\label{a:integral_free}

Assume $\eta(t)$ is a $p$-dimensional Ornstein-Uhlenbeck (OU) stochastic process with a marginal mean of $0$.  From \cite{vatiwutipong_2019}, if we assume that the initial state $\eta(t_0 = 0)$ is known, then the cross-covariance function of the OU process at times $s$ and $t$ is 

\begin{equation*}
\begin{aligned}
    Cov\{\eta(s), \eta(t) | \eta(t_0 = 0) \} = \int_{0}^{\min(s,t)} e^{-\theta(s-u)} \sigma \sigma^{\top} e^{-\theta^{\top}(t-u)}du
\end{aligned}
\end{equation*}

where $e^A$ is the matrix exponential. Note that we can assume that $t_0 = 0$ without loss of generality because this stochastic process is stationary.  Using the identity for matrices $A$, $B$, and $C$ that $vec(ABC) = (C^{\top} \otimes A) vec(B)$, we can re-write the vectorized version of the cross-covariance function as
\begin{equation*}
\begin{aligned}
    vec\{Cov\{\eta(s), \eta(t) | \eta(t_0) \}\} = \int_{0}^{\min(s,t)} e^{-\theta(t-u)} \otimes e^{-\theta(s-u)} \ du \  vec\{\sigma \sigma^{\top}\}
\end{aligned}
\end{equation*}

We can also use the identity that $e^A \otimes e^B = e^{A \oplus B}$, so
\begin{equation}
\begin{aligned}
\label{eq:xcov_int_cond}
    vec\{Cov\{\eta(s), \eta(t) | \eta(t_0) \}\} = \int_{0}^{\min(s,t)} e^{[-\theta(t-u)] \oplus [-\theta(s-u)]} du \ vec\{\sigma \sigma^{\top}\}
\end{aligned}
\end{equation}

Next, we simplify Equation \ref{eq:xcov_int_cond} by pulling all the $u$'s into a single term. For now, focus on the term in the exponential:
\begin{equation*}
\begin{aligned}
    \left[-\theta(t-u)\right] \oplus \left[-\theta(s-u)\right] &\overset{(a)}{=} -\theta(t-u) \otimes I + I \otimes (-\theta(s-u)) \\
    &= -t(\theta \otimes I) + u(\theta \otimes I + I \otimes \theta) - s(I \otimes \theta) \\
    &= -(t\theta \oplus s\theta) + u (\theta \oplus \theta)
\end{aligned}
\end{equation*}

where equality $(a)$ is by the definition of the Kronecker sum; $A \oplus B = A \otimes I_B + I_A \otimes A$, where $I_A$ and $I_B$ are identity matrices with dimensions of $A$ and $B$, respectively. Now, substituting this new term back into the exponential term in Equation \ref{eq:xcov_int_cond}, we get
\begin{equation}
\begin{aligned}
\label{eq:exp_term}
    e^{[-\theta(t-u)] \oplus [-\theta(s-u)]} = e^{-(t\theta \oplus s\theta) + u (\theta \oplus \theta)}
\end{aligned}
\end{equation}

We can simplify this further using the identity $e^{A + B} = e^A e^B$ if $A$ and $B$ commute. Letting $A = (t\theta) \oplus (s\theta)$ and $B = (\theta \oplus \theta)$, we first show that these terms commute:
\begin{equation*}
\begin{aligned}
    A \cdot B = &\left[ (t \theta) \oplus (s \theta) \right] \cdot \left[ \theta \oplus \theta \right] \\
    =&\left[ t \theta \otimes I + I \otimes s \theta \right] \cdot \left[ \theta \otimes I + I \otimes \theta \right] \\
    =& (t\theta \otimes I) (\theta \otimes I) + (t \theta \otimes I) (I \otimes \theta)  + (I \otimes s \theta) (\theta \otimes I) + (I \otimes s \theta) (I \otimes \theta) \\
    =& (t \theta \otimes I) (\theta \otimes I) + (I \otimes \theta) (t \theta \otimes I)  + (\theta \otimes I) (I \otimes s \theta) + (I \otimes s \theta) (I \otimes \theta) \\
    =&(\theta \otimes I) \left[ (t \theta \otimes I) + (I \otimes s \theta) \right] + (I \otimes \theta) \left[ (t \theta \otimes I)  + (I \otimes s \theta) \right] \\
    =&\left[ (\theta \otimes I) + (I \otimes \theta) \right] \cdot \left[ (t \theta \otimes I)  + (I \otimes s \theta) \right] \\
    =&\left[ (\theta \oplus \theta) \right] \cdot \left[ (t \theta \oplus s \theta) \right] \\
\end{aligned}
\end{equation*}

where line 4 uses the mixed-product property of the Kronecker product.

Referring back to Equation \ref{eq:exp_term}, we now have
\begin{equation*}
\begin{aligned}
    e^{-(t\theta \oplus s\theta) + u (\theta \oplus \theta)} = e^{-(t\theta \oplus s\theta)} e^{u(\theta \oplus \theta)}
\end{aligned}
\end{equation*}

We can substitute this term into Equation \ref{eq:xcov_int_cond} to get
\begin{equation*}
\begin{aligned}
    vec\{Cov\{\eta(s), \eta(t) | \eta(t_0 = 0) \}\} &= \int_{0}^{\min(s,t)} e^{-(t\theta \oplus s\theta)} e^{u(\theta \oplus \theta)} du \ vec\{\sigma \sigma^{\top}\} \\
    &= \int_{0}^{\min(s,t)} e^{u(\theta \oplus \theta)} du \ e^{-(t\theta \oplus s\theta)} vec\{\sigma \sigma^{\top}\}
\end{aligned}
\end{equation*}

Now that we have rewritten the conditional cross-covariance function in this form, the only term that we need to integrate is $e^{u(\theta \oplus \theta)}$. We find
\begin{equation*}
\begin{aligned}
    \int_0^{min(s,t)} e^{u(\theta \oplus \theta)} du = (\theta \oplus \theta)^{-1} \big[ e^{min(s,t)(\theta \oplus \theta)} - I \big]
\end{aligned}
\end{equation*}

We now have an integral-free analytic form of the conditional cross-covariance function:
\begin{equation*} 
\begin{aligned}
    vec\{Cov\{\eta(s), \eta(t) | \eta(t_0 = 0) \}\} &= (\theta \oplus \theta)^{-1} \big[ e^{min(s,t) (\theta \oplus \theta)} - I \big] e^{-(t \theta \oplus s \theta)} vec\{ \sigma \sigma^{\top} \}
\end{aligned}
\end{equation*}

Note that if $s = t$, then the conditional cross-covariance function simplifies to the conditional covariance function given in \citet{vatiwutipong_2019}.

\section{Derivation of the analytic form of the marginal covariance function of the OU process}\label{a:marginal_xcov}

The analytic form of the conditional covariance function given in Lemma 1 (in the main text) is based on the assumption that the initial state $\eta(t_0)$, with $t_0 = 0$ is \textit{known}.  We now derive the analytic form of the unconditional cross-covariance function that accounts for the additional uncertainty of an unknown initial state.  From \citet{vatiwutipong_2019}, if $\eta(t_0)$, with $t_0 = 0$, is known, then
\begin{equation*}
\begin{aligned}
    \mathbb{E}\{ \eta(t) | \eta(t_0) \} = e^{-\theta t} \eta(t_0)
\end{aligned}
\end{equation*}

Assuming that $s \le t$, from Lemma 1, we have
\begin{equation*}
\begin{aligned}
    Cov\{ \eta(s), \eta(t) | \eta(t_0) \} = vec^{-1}\Big\{(\theta \oplus \theta)^{-1}\Big[ e^{(\theta \oplus \theta) s} - I \Big] e^{-(\theta t \oplus \theta s)} vec\{\sigma \sigma^{\top} \} \Big\}
\end{aligned}
\end{equation*}

% HEREEEE

If $\eta(t_0)$ is \textit{unknown} and $t_0 = 0$, then using the Law of Total Covariance we can calculate
\begin{equation*}
\begin{aligned}
    Cov\{ \eta(s), \eta(t)\} &= \mathbb{E}\big\{ Cov\big( \eta(s), \eta(t) | \eta(t_0) \big) \big\} + Cov\big\{ \mathbb{E}\big( \eta(s) | \eta(t_0) \big), \mathbb{E}\big( \eta(t) | \eta(t_0) \big) \big\} \\
    &=  vec^{-1}\Big\{(\theta \oplus \theta)^{-1}\Big[ e^{(\theta \oplus \theta) s} - I \Big] e^{-(\theta t \oplus \theta s)} vec\{\sigma \sigma^{\top} \} \Big\} \\ & \ \ + Cov\big\{ e^{-\theta s} \eta(t_0), e^{-\theta t} \eta(t_0) \big\} \\
    &= vec^{-1}\Big\{(\theta \oplus \theta)^{-1}\Big[ e^{(\theta \oplus \theta) s} - I \Big] e^{-(\theta t \oplus \theta s)} vec\{\sigma \sigma^{\top} \} \Big\} \\ & \ \ + e^{-\theta s} Var\big\{  \eta(t_0)\big\} [e^{-\theta t}]^{\top}
\end{aligned}
\end{equation*}

If we assume that $\eta(t_0)$ is drawn from the stationary distribution, then $Var(\eta(t_0)) = vec^{-1}\big\{ (\theta \oplus \theta)^{-1} vec\{ \sigma \sigma^{\top} \} \big\}$. Then, we have 
\begin{equation*}
\begin{aligned}
    Cov\{ \eta(s), \eta(t)\} =& vec^{-1}\Big\{(\theta \oplus \theta)^{-1}\Big[ e^{(\theta \oplus \theta) s} - I \Big] e^{-(\theta t \oplus \theta s)} vec\{\sigma \sigma^{\top} \} \Big\} \\ &+ e^{-\theta s} vec^{-1}\big\{ (\theta \oplus \theta)^{-1} vec\{\sigma \sigma^{\top} \} \big\}  [e^{-\theta t}]^{\top}
\end{aligned}
\end{equation*}

Now we simplify this function. Consider the terms involving $\theta$ in the first term of the sum,
\begin{equation*}
\begin{aligned}
    (\theta \oplus \theta)^{-1}\Big[ e^{(\theta \oplus \theta) s} - I \Big] e^{-(\theta t \oplus \theta s)}
\end{aligned}
\end{equation*}

We can simplify this expression using the fact that $e^A e^B = e^B e^A$ in our setting. This property means that both
\begin{equation}
\begin{aligned}
\label{eq:commute1}
    (\theta \oplus \theta)^{-1}\big[ e^{s (\theta \oplus \theta)} - I \big] e^{-(t \theta \oplus s \theta)} = e^{-(t \theta \oplus s \theta)} (\theta \oplus \theta)^{-1}\big[ e^{s (\theta \oplus \theta)} - I \big] 
\end{aligned}
\end{equation}

and

\begin{equation}
\begin{aligned}
\label{eq:commute2}
    (\theta \oplus \theta)^{-1}\big[ e^{s (\theta \oplus \theta)} - I \big] e^{- (t \theta \oplus s \theta)} = (\theta \oplus \theta)^{-1} e^{-(t \theta \oplus s \theta)} \big[ e^{s (\theta \oplus \theta)} - I \big]
\end{aligned}
\end{equation}

Setting Equations \ref{eq:commute1} and \ref{eq:commute2} equal and cancelling the final term implies that 
\begin{equation*}
\begin{aligned}
    e^{-(t \theta \oplus s \theta)} (\theta \oplus \theta)^{-1} = (\theta \oplus \theta)^{-1} e^{-(t \theta \oplus s \theta)}
\end{aligned}
\end{equation*}

We will use this proof of the commutative property later and now return to our expression for the unconditional cross-covariance function, $Cov\{ \eta(s), \eta(t) \}$,
\begin{equation}
\begin{aligned}
\label{eq:xcov_intermed}
    Cov\{ \eta(s), \eta(t)\} =& vec^{-1}\Big\{(\theta \oplus \theta)^{-1}\Big[ e^{(\theta \oplus \theta) s} - I \Big] e^{-(\theta t \oplus \theta s)} vec\{\sigma \sigma^{\top} \} \Big\} \\&+ e^{-\theta s} vec^{-1}\big\{ (\theta \oplus \theta)^{-1} vec\{\sigma \sigma^{\top} \} \big\}  [e^{-\theta t}]^{\top}
\end{aligned}
\end{equation}

Consider the second term in the sum,
\begin{equation*}
\begin{aligned}
    e^{-\theta s} vec^{-1}\big\{ (\theta \oplus \theta)^{-1} vec\{\sigma \sigma^{\top} \} \big\}  [e^{-\theta t}]^{\top}
\end{aligned}
\end{equation*}

By applying the identity $vec(ABC) = (C^{\top} \otimes A) vec(B)$, we can rewrite the vectorized form of the expression as
\begin{equation*}
\begin{aligned}
    vec\big\{e^{-\theta s} vec^{-1}\big\{ (\theta \oplus \theta)^{-1} vec\{\sigma \sigma^{\top} \} \big\}  [e^{-\theta t}]^{\top} \big\} &= e^{-\theta t} \otimes e^{-\theta s} vec\big\{ vec^{-1}\{ (\theta \oplus \theta)^{-1} vec\{ \sigma \sigma^{\top} \} \big\} \\
    &= e^{-\theta t} \otimes e^{-\theta s} (\theta \oplus \theta)^{-1} vec\{ \sigma \sigma^{\top} \} \\
    &= e^{-(\theta t \oplus \theta s)} (\theta \oplus \theta)^{-1} vec\{ \sigma \sigma^{\top} \}
\end{aligned}
\end{equation*}

Reversing the vectorization operation and applying the commutative property, we then get
\begin{equation*}
\begin{aligned}
    e^{-\theta s} vec^{-1}\big\{ (\theta \oplus \theta)^{-1} vec\{\sigma \sigma^{\top} \} \big\}  [e^{-\theta t}]^{\top} &= vec^{-1} \big\{ e^{-(\theta t \oplus \theta s)} (\theta \oplus \theta)^{-1} vec\{ \sigma \sigma^{\top} \} \big\} \\
    &= vec^{-1} \big\{ (\theta \oplus \theta)^{-1} e^{-(\theta t \oplus \theta s)} vec\{ \sigma \sigma^{\top} \} \big\}
\end{aligned}
\end{equation*}

Plugging the term above into the second term of Equation \ref{eq:xcov_intermed}, the cross-covariance function becomes
\begin{equation}
\begin{aligned}
\label{eq:xcov}
    Cov\{ \eta(s), \eta(t) \} &= vec^{-1} \Big\{ (\theta \oplus \theta)^{-1} \Big[ e^{(\theta \oplus \theta) s} e^{-(\theta t \oplus \theta s)} - e^{-(\theta t \oplus \theta s)} \Big] vec\{ \sigma \sigma^{\top} \} \Big\} \\ & \ \ \ + vec^{-1} \Big\{ (\theta \oplus \theta)^{-1} e^{-(\theta t \oplus \theta s)} vec\{ \sigma \sigma^{\top} \} \Big\} \\
    &= vec^{-1} \Big\{ (\theta \oplus \theta)^{-1} e^{(\theta \oplus \theta)s} e^{-(\theta t \oplus \theta s)} vec\{ \sigma \sigma^{\top}\} - (\theta \oplus \theta)^{-1} e^{-(\theta t \oplus \theta s)} vec\{ \sigma \sigma^{\top} \} \Big\} \\ & \ \ \ + vec^{-1} \Big\{ (\theta \oplus \theta)^{-1} e^{-(\theta t \oplus \theta s)} vec\{ \sigma \sigma^{\top} \} \Big\}\\
    &= vec^{-1} \Big\{ (\theta \oplus \theta)^{-1} \big[ e^{(\theta \oplus \theta) s} e^{-(\theta t \oplus \theta s)} \big] vec\{ \sigma \sigma^{\top} \} \Big\} \\
    &= vec^{-1} \Big\{ (\theta \oplus \theta)^{-1} \big[ e^{(\theta \oplus \theta)s - (\theta t \oplus \theta s)} \big] vec\{\sigma \sigma^{\top}\} \Big\}
\end{aligned}
\end{equation}

Equation \ref{eq:xcov} is the marginal cross-covariance function of the OU process when the initial state at time $t_0 = 0$ is \textit{unknown}.

\section{Derivation of the precision matrix for the OU process}
\label{a:precision}

We derive the sparse precision matrix for the multivariate OU process assuming an unknown initial state. This sparsity results from the Markov property.  We use $\Omega$ to represent the precision matrix and $\Psi$ for the covariance matrix.

\hspace{2em}First, we start in the simplest setting in which we assume a stationary univariate OU process with evenly spaced measurement occasions. The spacing of the measurement times is given by $|t_j - t_{j-1}| =: d > 0$. The covariance matrix takes the form,
\begin{equation*}
\begin{aligned}
    \Psi = \frac{\sigma^2}{2 \theta} \begin{bmatrix} 1 & e^{-\theta d} & \dots & e^{-\theta(n-2)\cdot d} & e^{-\theta(n-1)\cdot d} \\
    e^{-\theta d} & 1 & \dots & e^{-\theta(n-3)\cdot d} & e^{-\theta(n-2)\cdot d} \\ \vdots & \vdots & \ddots & \vdots & \vdots \\
    e^{-\theta(n-2)\cdot d} & e^{-\theta(n-3)\cdot d} & \dots & 1 & e^{-\theta d} \\ e^{-\theta(n-1)\cdot d} & e^{-\theta(n-2)\cdot d} & \dots & e^{-\theta d} & 1 \end{bmatrix}
\end{aligned}
\end{equation*}

We know that the univariate OU process is equal to the AR(1) process when measurements are evenly spaced, so the OU process precision matrix (assuming evenly spaced measurements) can be expressed as
\begin{equation*}
    \Omega = \frac{2\theta}{\sigma^2}\frac{1}{1 - e^{-2\theta d}} \begin{bmatrix} 1 & -e^{-\theta d} & \dots & 0 & 0 \\
    -e^{-\theta d} & 1 + e^{-2\theta d} & \dots & 0 & 0 \\
    \vdots & \vdots & \ddots & \vdots & \vdots \\
    0 & 0 & \dots & 1 + e^{2 \theta d} & -e^{-\theta d} \\
    0 & 0 & \dots & -e^{-\theta d} & 1\end{bmatrix}
\end{equation*}

Now, consider a more general setting in which measurements do not necessarily occur at evenly spaced intervals. Assume that $t_1 < t_2 < \dots < t_{n-1} < t_n$. Then, the covariance matrix takes the form
\begin{equation*}
    \Psi = \frac{\sigma^2}{2 \theta} \begin{bmatrix}
    1 & e^{-\theta |t_2 - t_1|} & \dots & e^{-\theta|t_{n-1} - t_1|} & e^{-\theta|t_n - t_1|} \\
    e^{-\theta |t_2 - t_1|} & 1 & \dots & e^{-\theta|t_{n-1} - t_2|} & e^{-\theta|t_n - t_2|} \\
    \vdots & \vdots & \ddots & \vdots & \vdots \\
    e^{-\theta|t_{n-1} - t_1|} & e^{-\theta|t_{n-1} - t_2|} & \dots & 1 & e^{-\theta|t_{n-1} - t_n|} \\ e^{-\theta|t_n - t_1|} & e^{-\theta|t_n - t_2|} & \dots & e^{-\theta|t_n - t_{n-1}|} & 1 \end{bmatrix}
\end{equation*}

and the precision matrix can be expressed as
\begin{equation*}
\footnotesize
    \Omega = \frac{2\theta}{\sigma^2} \begin{bmatrix} \frac{1}{1 - e^{-2\theta |t_2 - t_1|}} & -\frac{e^{-\theta |t_2 - t_1|}}{1 - e^{-2\theta |t_2 - t_1|}} & \dots & 0 & 0 \\
    -\frac{e^{-\theta |t_2 - t_1|}}{1 - e^{-2\theta |t_2 - t_1|}} & \frac{1 - e^{-2\theta |t_3 - t_1|}}{(1 - e^{-2\theta |t_2 - t_1|})(1 - e^{-2\theta | t_3 - t_2|})} & \dots & 0 & 0 \\ \vdots & \vdots & \ddots & \vdots & \vdots \\
    0 & 0 & \dots & \frac{1 - e^{-2\theta |t_n - t_{n-2}|}}{(1 - e^{-2\theta |t_2 - t_1|})(1 - e^{-2\theta|t_3 - t_2|})} & -\frac{e^{-2\theta|t_n - t_{n-1}|}}{1 - e^{-2\theta|t_n - t_{n-1}|}} \\
    0 & 0 & \dots & -\frac{e^{-2\theta|t_n - t_{n-1}}}{1 - e^{-2\theta|t_n - t_{n-1}|}} & \frac{1}{1 - e^{-2\theta |t_n - t_{n-1}|}} \end{bmatrix}
\end{equation*}

Next, we move from the one-dimensional case to the two-dimensional case. We start by re-arranging the terms in the definition of the cross-covariance function for the bivariate OU process.
\begin{equation*}
    \begin{aligned}
        Cov\{ \eta(s), \eta(t) \} &= vec^{-1}\big\{ (\theta \oplus \theta)^{-1} e^{s \wedge t (\theta \oplus \theta) - (\theta t) \oplus (\theta s)} vec(\sigma \sigma^{\top})\big\} \\
        & \overset{(a)}{=} vec^{-1}\big\{ e^{s \wedge t (\theta \oplus \theta)} e^{-(\theta t) \oplus (\theta s)} (\theta \oplus \theta)^{-1} vec(\sigma \sigma^{\top}) \big\} \\
        &= vec^{-1}\big\{ \big[ e^{s \wedge t \theta} \otimes e^{s \wedge t \theta} \big] \big[ e^{-\theta t} \otimes e^{-\theta s} \big] (\theta \oplus \theta)^{-1} vec(\sigma \sigma^{\top}) \big\} \\
        &= vec^{-1}\big\{ \big[ e^{s \wedge t \theta} e^{-\theta t}\big] \otimes\big[ e^{s \wedge t \theta} e^{-\theta s} \big] (\theta \oplus \theta)^{-1} vec(\sigma \sigma^{\top}) \big\} \\
        &= vec^{-1}\big\{ \big[ e^{-\theta(t - s \wedge t)}\big] \otimes \big[ e^{-\theta (s - s \wedge t)} \big] (\theta \oplus \theta)^{-1} vec(\sigma \sigma^{\top}) \big\} \\
        &\overset{(b)}{=} vec^{-1}\big\{ \big[ e^{-\theta(t - s \wedge t)}\big] \otimes I (\theta \oplus \theta)^{-1} vec(\sigma \sigma^{\top}) \big\} \\
        &= vec^{-1}\big\{(\theta \oplus \theta)^{-1} vec(\sigma \sigma^{\top}) \big\} e^{-\theta^{\top}|t-s|} \\
        &:= V \cdot e^{-\theta^{\top}|t-s|}
    \end{aligned}
\end{equation*}

where equality (a) is because these terms commute and equality (b) holds when we assume that $min(s,t) = s$. We can make this assumption without loss of generality because the matrices are symmetric. When $min(s,t) = t$, $Cov\{ \eta(s), \eta(t) \} = e^{-\theta |t - s|}V^{\top} = e^{-\theta|t-s|}V$. %\textbf{check}.
Then, the covariance matrix is given by

\begin{equation*}
    \Psi =\begin{bmatrix}
    V & V e^{-\theta^{\top} |t_2 - t_1|} & \dots & V e^{-\theta^{\top}|t_{n-1} - t_1|} & V e^{-\theta^{\top} |t_n - t_1|} \\
    e^{-\theta |t_2 - t_1|} V & V & \dots & V e^{-\theta^{\top}|t_{n-1} - t_2|} & V e^{-\theta^{\top} |t_n - t_2|} \\
    \vdots & \vdots & \ddots & \vdots & \vdots \\
    e^{-\theta|t_{n-1} - t_1|} V & e^{-\theta|t_{n-1} - t_2|} V & \dots & V & V e^{-\theta^{\top}|t_{n-1} - t_n|} \\ e^{-\theta|t_n - t_1|} V & e^{-\theta|t_n - t_2|}  V & \dots & e^{-\theta|t_n - t_{n-1}|} V & V \end{bmatrix}
\end{equation*}

By the definition of the OU process, we know that the precision matrix, $\Omega = \Psi^{-1}$, is block tri-diagonal. We start by solving for two blocks, $\Omega_{11}$ and $\Omega_{12}$. We assume that $\Omega_{11} = A^{-1}$ and $\Omega_{12} = A^{-1}B$, based on the form of the precision matrix in the case of the univariate OU process.  Based on patterns seen when multiplying the AR(1) precision and covariance matrices, we assume that, for the OU process, the first row of blocks in the precision matrix, $[ \Omega_{11}, \Omega_{12}, 0, \dots, 0 ]$ times the second column of blocks in the covariance matrix, $[ V e^{-\theta^{\top}(t_2 - t_1)}, V, \dots ]^{\top}$, is equal to 0.  So,
\begin{equation*}
\begin{aligned}
    0 &= \Omega_{11} V e^{-\theta^{\top}(t_2 - t_1)} + \Omega_{12} V \\
    \Longrightarrow 0 &= A^{-1} V e^{-\theta^{\top}(t_2 - t_1)} + A^{-1} B V \\
    \Longrightarrow 0 &= V e^{-\theta^{\top}(t_2 - t_1)} + BV \\
    \Longrightarrow BV &= - V e^{-\theta^{\top}(t_2 - t_1)} \\
    \Longrightarrow B &= -V e^{-\theta^{\top}(t_2 - t_1)} V^{-1}
\end{aligned}
\end{equation*}

By similar logic, the first row of blocks in the precision matrix times the first column of blocks in the covariance matrix is equal to the identity matrix.  So,
\begin{equation*}
\begin{aligned}
    I &= \Omega_{11} V + \Omega_{12} e^{-\theta(t_2 - t_1)} V \\
    \Longrightarrow I &= A^{-1} V + A^{-1} B e^{-\theta(t_2 - t_1)} V \\
    \Longrightarrow A &= V + B e^{-\theta(t_2 - t_1)} V
\end{aligned}
\end{equation*}

We know that $B = -V e^{-\theta^{\top}(t_2 - t_1)} V^{-1}$ so
\begin{equation*}
\begin{aligned}
    A &= V - V e^{-\theta^{\top} (t_2 - t_1)} V^{-1} e^{-\theta (t_2 - t_1)} V \\
    \Longrightarrow A^{-1} &= [V - V e^{-\theta^{\top} (t_2 - t_1)} V^{-1} e^{-\theta (t_2 - t_1)} V]^{-1}
\end{aligned}
\end{equation*}

Now we have
\begin{equation*}
\begin{aligned}
    \Omega_{11} &= \big[ V - V e^{-\theta^{\top} (t_2 - t_1)} V^{-1} e^{-\theta (t_2 - t_1)} V \big]^{-1} \\
    \Omega_{12} &= -\big[ V - V e^{-\theta^{\top} (t_2 - t_1)} V^{-1} e^{-\theta (t_2 - t_1)} V \big]^{-1} V e^{-\theta^{\top} (t_2 - t_1)} V^{-1}
\end{aligned}
\end{equation*}


\vspace{0.5cm}

Continuing with this logic, we can check the first row of blocks in $\Omega$ against all other columns of $\Psi$ and see that
\begin{equation*}
\begin{aligned}
    0 &= \Omega_{11} V e^{-\theta^{\top}(t_k - t_1)} + \Omega_{12} V e^{-\theta^{\top}(t_k - t_2)} \\
    &= A^{-1} V e^{-\theta^{\top}(t_k - t_1)} + A^{-1} B V e^{-\theta^{\top}(t_k - t_2)} \\
    &= V e^{-\theta^{\top}(t_k - t_1)} + B V e^{-\theta^{\top}(t_k - t_2)} \\
    &= V e^{-\theta^{\top}(t_k - t_1)} - V e^{-\theta^{\top} (t_2 - t_1)} V^{-1} V e^{-\theta^{\top}(t_k - t_2)} \\
    &= V e^{-\theta^{\top}(t_k - t_1)} - V e^{-\theta^{\top} (t_2 - t_1)} e^{-\theta^{\top}(t_k - t_2)} \\
    &= V e^{-\theta^{\top}(t_k - t_1)} - V e^{-\theta^{\top} (t_k - t_1)} \\
    &= 0
\end{aligned}
\end{equation*}

Now we move to the second row of blocks in $\Omega$.  Because $\Omega = \Omega^{\top}$, we also know that $\Omega_{21} = \Omega^{\top}_{12}$.  This symmetry means that we only need to derive $\Omega_{22}$ and $\Omega_{23}$.  Based on previous results, we have
\begin{equation*}
\begin{aligned}
    \Omega_{23} = - \Big[ V - V e^{-\theta^{\top} (t_3 - t_2)} V^{-1} e^{-\theta (t_3 - t_2)} V \Big]^{-1} V e^{-\theta^{\top} (t_3 - t_2)} V^{-1}
\end{aligned}
\end{equation*}

Then we find the form of $\Omega_{22}$ by once again using the same logic to say that the second row of blocks in $\Omega$ times the second column of blocks in $\Psi$ will be equal to an identity matrix:
\begin{equation*}
\begin{aligned}
    I &= \Omega_{21} V e^{-\theta^{\top}(t_2 - t_1)} + \Omega_{22} V + \Omega_{23} e^{-\theta (t_3 - t_2)} V \\
    \Rightarrow V^{-1} &= \Omega_{21} V e^{-\theta^{\top} (t_2 - t_1)} V^{-1} + \Omega_{22} + \Omega_{23} e^{-\theta (t_3 - t_2)} \\
    \Rightarrow \Omega_{22} &= V^{-1} + V^{-1} e^{-\theta (t_2 - t_1)} V \left[ V - V e^{-\theta^{\top}(t_2 - t_1)} V^{-1} e^{-\theta(t_2 - t_1)} V \right]^{-1^{\top}} V e^{-\theta^{\top} (t_2 - t_1)} V^{-1} \\ \ \ \ \ &+ \left[ V - V e^{-\theta^{\top}(t_3 - t_2)} V^{-1} e^{-\theta(t_3 - t_2)} V \right]^{-1} V e^{-\theta^{\top}(t_3 - t_2)} V^{-1} e^{-\theta (t_3 - t_2)}
\end{aligned}
\end{equation*}

%We next simplify the terms in $\Omega_{22}$. Combining the first and last terms of $\Omega_{22}$, we get
%\begin{equation*} % OLD!
%\begin{aligned}
%    & V^{-1} + \Big[ V - e^{-\theta(t_3 - t_2)} V e^{-\theta^{\top}(t_3 - t_2)} \Big]^{-1} e^{-\theta(t_3 - t_2)} V e^{-\theta^{\top}(t_3 - t_2)} V^{-1} \\
%    & \ \ \ \ = \Big[ I + \big[ V - e^{-\theta(t_3 - t_2)} V e^{-\theta^{\top}(t_3 - t_2)} \big]^{-1} e^{-\theta(t_3 - t_2)} V e^{-\theta^{\top}(t_3 - t_2)} \Big] V^{-1} \\
%    & \ \ \ \ = \Big[ V - e^{-\theta(t_3 - t_2)} V e^{-\theta^{\top}(t_3 - t_2)} \Big]^{-1} \Big[ V - e^{-\theta(t_3 - t_2)} V e^{-\theta^{\top}(t_3 - t_2)} + e^{-\theta(t_3 - t_2)} V e^{-\theta^{\top}(t_3 - t_2)} \Big] V^{-1} \\
%    & \ \ \ \ = \Big[ V - e^{-\theta(t_3 - t_2)} V e^{-\theta^{\top}(t_3 - t_2)} \Big]^{-1}
%\end{aligned}
%\end{equation*}

% ALSO OLD
%Combining the expression above with the middle term in $\Omega_{22}$, we get
%\begin{equation*}
%    \Big[ V - e^{-\theta(t_3 - t_2)} V e^{-\theta^{\top}(t_3 - t_2)} \Big]^{-1} + e^{-\theta^{\top}(t_2 - t_1)}\Big[ V - e^{-\theta(t_2 - t_1)} V e^{-\theta^{\top}(t_2 - t_1)} \Big]^{-1} e^{-\theta(t_2 - t_1)}
%\end{equation*}

% ALSO OLD
%To confirm our work so far, we can check to see that this definition of $\Omega_{22}$ agrees with the simpler univariate version in which $V = \frac{\sigma^2}{2\theta}$:
%\begin{equation*}
%\begin{aligned}
%    & \frac{2\theta}{\sigma^2} \Big[ (1 - e^{-2\theta(t_3 - t_2)})^{-1} + e^{-2\theta(t_2 - t_1)} (1 - e^{-2\theta(t_2 - t_1)})^{-1} \Big] \\
%    & \ \ \ \ = \frac{2\theta}{\sigma^2} \frac{1}{ (1 - e^{-2\theta(t_3 - t_2)}) (1 - e^{-2\theta(t_2 - t_1)}) } \Big[ (1 - e^{-2\theta(t_2 - t_1)}) + e^{-2\theta(t_2 - t_1)} (1 - e^{-2\theta(t_3 - t_2)}) \Big] \\
%    & \ \ \ \ = \frac{2 \theta}{\sigma^2} \frac{1}{ (1 - e^{-2\theta(t_3 - t_2)}) (1 - e^{-2\theta(t_2 - t_1)}) } \Big[ 1 - e^{-2\theta(t_2 - t_1)} + e^{-2\theta(t_2 - t_1)} - e^{-2\theta(t_3 - t_1)} \Big] \\
%    & \ \ \ \ = \frac{2\theta}{\sigma^2} \frac{1 - e^{-2\theta (t_3 - t_1)}}{ (1 - e^{-2\theta(t_3 - t_2)}) (1 - e^{-2\theta(t_2 - t_1)}) }
%\end{aligned}
%\end{equation*}

The final terms are then given by:
\begin{equation*}
\begin{aligned}
    I &= \Omega_{n,n-1} V e^{-\theta^{\top} (t_n - t_{n-1})} + \Omega_{nn} V \\
    \Rightarrow I &= -V^{-1} e^{-\theta(t_n - t_{n-1})} V \left[  V - V e^{\theta^{\top}(t_n - t_{n-1})} V^{-1} e^{-\theta (t_n - t_{n-1})} V \right]^{-1} V e^{-\theta^{\top} (t_n - t_{n-1})} + \Omega_{nn} V \\
    \Rightarrow \Omega_{nn} &= V^{-1} + V^{-1} e^{-\theta(t_n - t_{n-1})} V \left[ V - V e^{-\theta^{\top} (t_n - t_{n-1})} V^{-1} e^{-\theta (t_n - t_{n-1})} V \right]^{-1} V e^{-\theta^{\top} (t_n - t_{n-1})} V^{-1}
\end{aligned}
\end{equation*}

Thus, the precision matrix $\Omega$ is block tri-diagonal with the following entries (indexed by j) for $1 < j < n$:
\begin{equation*}
\begin{aligned}
    & V := vec^{-1} \big\{ (\theta \oplus \theta)^{-1} vec\{\sigma \sigma^{\top}\} \big\} \\
    & \Omega_{11} = \big[ V - V  e^{-\theta^{\top} (t_2 - t_1)} V^{-1} e^{-\theta (t_2 - t_1)} V \big]^{-1} \\
    & \Omega_{j,j+1} = \Omega_{j+1,j}^{\top} = -\big[V - V e^{-\theta^{\top} (t_{j+1} - t_j)} V^{-1} e^{-\theta (t_{j+1} - t_j)}  V \big]^{-1}  V e^{-\theta^{\top} (t_{j+1} - t_j)} V^{-1} \\
    & \Omega_{jj} = V^{-1} + V^{-1} e^{-\theta (t_j - t_{j-1})} V \big[V - V e^{-\theta^{\top} (t_j - t_{j-1})} V^{-1} e^{-\theta (t_j - t_{j-1})} V \big]^{-1} V e^{-\theta^{\top} (t_j - t_{j-1})} V^{-1} \\ & \hspace{0.9cm} + \big[V - V e^{-\theta^{\top} (t_{j+1} - t_j)} V^{-1} e^{-\theta (t_{j+1} - t_j)} V \big]^{-1} V e^{-\theta^{\top} (t_{j+1} - t_j)} V^{-1} e^{-\theta (t_{j+1} - t_j)} \\
    & \Omega_{nn} = V^{-1} + V^{-1} e^{-\theta (t_n - t_{n-1})} V \big[V - V e^{-\theta^{\top} (t_n - t_{n-1})} V^{-1} e^{-\theta (t_n - t_{n-1})} V \big]^{-1} V e^{-\theta^{\top} (t_n - t_{n-1})} V^{-1}
\end{aligned}
\end{equation*}

\section{Identifiability constraint: re-scaling the OU process}\label{ss:rescaling_OU}

Let $(\theta^*, \sigma^*)$ be a pair of OU process parameters satisfying the identifiability constraint that the stationary variance of the OU process is equal to 1; that is, $diag\{\Psi(\theta^*, \sigma^*)\} = 1$, where $\Psi$ is the covariance matrix of the OU process. We show that we can always find a pair of $(\theta^*, \sigma^*)$ that defines a valid mean-reverting OU process with stationary variance of 1 that has the same correlation structure as the original unconstrained OU process defined by ($\theta$, $\sigma$). As an example, consider the stochastic differential equation definition of the bivariate OU process. For an arbitrary mean-reverting OU process, $\eta(t)$, 
\begin{equation*}
    d\begin{bmatrix}
     \eta_1(t) \\ \eta_2(t)
    \end{bmatrix} =
    - \begin{bmatrix}
    \theta_{11} & \theta_{12} \\ \theta_{21} & \theta_{22}
    \end{bmatrix}
    \begin{bmatrix}
    \eta_1(t) \\ \eta_2(t)
    \end{bmatrix} dt + 
    \begin{bmatrix}
    \sigma_{11} & 0 \\ 0 & \sigma_{22}
    \end{bmatrix}
    d \begin{bmatrix}
    W_1(t) \\ W_2(t)
    \end{bmatrix}
\end{equation*}

We could equivalently define this OU process $\eta(t)$ as
\begin{equation*}
\begin{aligned}
    d\begin{bmatrix}
     \eta_1(t) \\ \eta_2(t)
    \end{bmatrix} &=
    - \begin{bmatrix}
    \theta_{11} & \theta_{12} \\ \theta_{21} & \theta_{22}
    \end{bmatrix}
    \begin{bmatrix}
        c_1 & 0 \\ 0 & c_2
    \end{bmatrix}
    \begin{bmatrix}
        1/c_1 & 0 \\ 0 & 1/c_2
    \end{bmatrix}
    \begin{bmatrix}
    \eta_1(t) \\ \eta_2(t)
    \end{bmatrix} dt + 
    \begin{bmatrix}
    \sigma_{11} & 0 \\ 0 & \sigma_{22}
    \end{bmatrix}
    d \begin{bmatrix}
    W_1(t) \\ W_2(t)
    \end{bmatrix} \\
    &= - \begin{bmatrix}
    c_1\theta_{11} & c_2\theta_{12} \\ c_1\theta_{21} & c_2\theta_{22}
    \end{bmatrix}
    \begin{bmatrix}
    \frac{1}{c_1}\eta_1(t) \\ \frac{1}{c_2}\eta_2(t)
    \end{bmatrix} dt + 
    \begin{bmatrix}
    \sigma_{11} & 0 \\ 0 & \sigma_{22}
    \end{bmatrix}
    d \begin{bmatrix}
    W_1(t) \\ W_2(t)
    \end{bmatrix}
\end{aligned}
\end{equation*}

Let $\eta^*(t)$ be a scaled version of $\eta$ where
$$\begin{bmatrix}
\eta^*_1(t) \\ \eta^*_2(t) \end{bmatrix} = \begin{bmatrix}
\frac{1}{c_1}\eta_1(t) \\ \frac{1}{c_2}\eta_2(t)
\end{bmatrix}$$  and $$\begin{bmatrix} \theta^*_{11} & \theta^*_{12} \\ \theta^*_{21} & \theta^*_{22} \end{bmatrix} = \begin{bmatrix} c_1\theta_{11} & c_2\theta_{12} \\ c_1\theta_{21} & c_2\theta_{22} \end{bmatrix}$$

and assume that $\eta^*(t)$ has a stationary variance equal to 1. Then,
\begin{equation*}
\begin{aligned}
    d \eta^*(t) &= - \begin{bmatrix} \theta^*_{11} & \theta^*_{21} \\ \theta^*_{12} & \theta^*_{22} \end{bmatrix} \begin{bmatrix} \eta^*_1(t) \\ \eta^*_2(t) \end{bmatrix} dt + \begin{bmatrix} \sigma^*_{11} & 0 \\ 0 & \sigma^*_{22} \end{bmatrix} d \begin{bmatrix} W_1(t) \\ W_2(t) \end{bmatrix} \\
    &= - \begin{bmatrix} \theta^*_{11} & \theta^*_{21} \\ \theta^*_{12} & \theta^*_{22} \end{bmatrix} \begin{bmatrix} c_1 & 0 \\ 0 & c_2 \end{bmatrix} \begin{bmatrix} \eta_1(t) \\ \eta_2(t) \end{bmatrix} dt + \begin{bmatrix} \sigma^*_{11} & 0 \\ 0 & \sigma^*_{22} \end{bmatrix} d \begin{bmatrix} W_1(t) \\ W_2(t) \end{bmatrix} \\
    &= - \begin{bmatrix} c_1\theta^*_{11} & c_2\theta^*_{21} \\ c_1\theta^*_{12} & c_2\theta^*_{22} \end{bmatrix} \begin{bmatrix} \eta_1(t) \\ \eta_2(t) \end{bmatrix} dt + \begin{bmatrix} \sigma^*_{11} & 0 \\ 0 & \sigma^*_{22} \end{bmatrix} d \begin{bmatrix} W_1(t) \\ W_2(t) \end{bmatrix}
\end{aligned}
\end{equation*}

Looking back at the original OU process $\eta(t)$,
\begin{equation*}
\begin{aligned}
    d\eta(t) &= d\begin{bmatrix} \frac{1}{c_1} & 0 \\ 0 & \frac{1}{c_2} \end{bmatrix} \eta^*(t) \\
    &= -\begin{bmatrix} \frac{1}{c_1} & 0 \\ 0 & \frac{1}{c_2} \end{bmatrix} \begin{bmatrix}  c_1 \theta^*_{11} & c_2\theta^*_{12} \\ c_1 \theta^*_{21} & c_2 \theta^*_{22} \end{bmatrix} \eta(t) dt + \begin{bmatrix} \frac{1}{c_1} \sigma^*_{11} & 0 \\ 0 & \frac{1}{c_2}\sigma^*_{22} \end{bmatrix} dW(t) \\
    &= - \begin{bmatrix} \frac{c_1}{c_1} \theta^*_{11} & \frac{c_2}{c_1} \theta^*_{12} \\ \frac{c_1}{c_2} \theta^*_{21} & \frac{c_2}{c_2} \theta^*_{22} \end{bmatrix} \eta(t) dt + \begin{bmatrix} \frac{1}{c_1} \sigma^*_{11} & 0 \\ 0 & \frac{1}{c_2} \sigma^*_{22} \end{bmatrix} dW(t)
\end{aligned}
\end{equation*}

Finally, we see that the parameters for $\eta(t)$ can easily be re-scaled to satisfy our identifiability assumption:
\begin{equation*}
    \begin{bmatrix} \theta_{11} & \frac{c_1}{c_2} \theta_{12} \\ \frac{c_2}{c_1} \theta_{21} & \theta_{22} \end{bmatrix} = \begin{bmatrix} \theta^*_{11} & \theta^*_{12} \\ \theta^*_{21} & \theta^*_{22} \end{bmatrix}
\end{equation*}

and
\begin{equation*}
    \begin{bmatrix} c_1 \sigma_{11} & 0 \\ 0 & c_2 \sigma_{22} \end{bmatrix} = \begin{bmatrix} \sigma^*_{11} & 0 \\ 0 & \sigma^*_{22} \end{bmatrix}
\end{equation*}

Thus, we have shown that for a mean-reverting bivariate OU process defined by $\theta$ and $\sigma$ with covariance matrix $\Psi(\theta, \sigma)$ and correlation matrix $\Psi^*(\theta, \sigma)$, we can re-scale this OU process to have stationary variance equal to 1 by scaling $\theta_{12}, \theta_{21}$ and $\sigma_{11}, \sigma_{22}$ by a pair of positive scalar constants, $(c_1, c_2)$. This proof can easily be extended to higher dimensional OU processes.

\section{Derivation of the analytic gradients for the measurement submodel}\label{fa_grads}

We have previously defined the log-likelihood for a single subject $i$ as
\begin{equation}
\begin{aligned}
    \ell_i = -\frac{1}{2}log|\Sigma_i^{*}| + Y_i^{\top} \Sigma_i^{*-1} Y_i
\end{aligned}
\end{equation}

where we ignore the constant terms and
\begin{equation}
\begin{aligned}
    \Sigma_i^* = (I_{n_i} \otimes \Lambda) \Psi_i (I_{n_i} \otimes \Lambda)^{\top} +  J_{n_i} \otimes \Sigma_{u} +  I_{n_i} \otimes \Sigma_{\epsilon}
\end{aligned}
\end{equation}

\paragraph{Gradient w.r.t. the loadings:}

We first take the derivative of $\ell_i$ with respect to the elements of the loadings matrix $\Lambda$, $\lambda_k$, $k = 1, ...., p \times K$. The first element of the loadings matrix is parameterized on the log scale in order to restrict this element to positive values for identifiability purposes and so the gradient of this element looks slightly different.  For $k > 1$, we have
\begin{equation}
\begin{aligned}
\label{eq:grad_lambda}
    \frac{\partial \ell_i}{\partial \lambda_k} = -\frac{1}{2} \Bigg[ tr\Big\{ \Sigma_i^{*-1} \frac{\partial \Sigma_i^*}{\partial \lambda_k} \Big\} - Y_i^{\top} \Sigma_i^{*-1} \frac{\partial \Sigma_i^*}{\partial \lambda_k} \Sigma_i^{*-1} Y_i \Bigg]
\end{aligned}
\end{equation}

where
\begin{equation}
\begin{aligned}
    \frac{\partial \Sigma_i^*}{\partial \lambda_k} = (I_{n_i} \otimes \Lambda) \Psi_i (I_{n_i} \otimes J^{k^{\top}}) + (I_{n_i} \otimes J^k) \Psi_i (I_{n_i} \otimes \Lambda^{\top})
\end{aligned}
\end{equation}

We use $J^{k}$ as an indicator matrix that has the same dimension as $\Lambda$ but contains all zeros except for a single 1 indicating the location of element $\lambda_k$.  For $k = 1$, we apply the chain rule and have
\begin{equation}
\begin{aligned}
    \frac{\partial \ell_i}{\partial log(\lambda_k)} = \frac{\partial \ell_i}{\partial \lambda_k} \Bigg[ \frac{\partial log(\lambda_k)}{\partial \lambda_k} \Bigg]^{-1} = \frac{\partial \ell_i}{\partial \lambda_k} \lambda_k
\end{aligned}
\end{equation}

\paragraph{Gradient w.r.t. the random effects:}

Next, we take the gradient of $\ell_i$ with respect to the elements of $R_u$ where $R_u$ comes from the Cholesky decomposition of the random effects covariance matrix, $\Sigma_u = R_u^{\top}R_u$.  For $p,q = 1, ..., K$ and $p \neq q$,
\begin{equation}
\begin{aligned}
    \frac{\partial \Sigma_i^*}{\partial r_{pq}} = J_{n_i} \otimes (J^{k^{\top}}R_u + R_u^{\top} J^k)
\end{aligned}
\end{equation}

\begin{equation}
\begin{aligned}
\label{eq:grad_sigma_u}
    \frac{\partial \ell_i}{\partial r_{pq}} = -\frac{1}{2} \Bigg[ tr\Bigg\{ \Sigma_i^{*-1} \frac{\partial \Sigma_i^*}{\partial r_{pq}} \Bigg\} + Y_i^{\top} \Sigma_i^{*-1} \frac{\partial \Sigma_i^*}{\partial r_{pq}} \Sigma_i^{*-1} Y_i \Bigg]
\end{aligned}
\end{equation}

where again $J^k$ is an indicator matrix of the same dimensions as $\Sigma_u$.

For $p,q = 1, ..., K$ and $p = q$,
\begin{equation}
\begin{aligned}
    \frac{\partial \ell_i}{\partial log(r_{u_{pp}})} = \frac{\partial \ell_i}{\partial r_{u_{pp}}} \Bigg[ \frac{\partial log(r_{u_{pp}})}{\partial r_{u_{pp}}} \Bigg]^{-1} = \frac{\partial \ell_i}{\partial r_{u_{pp}}} r_{u_{pp}}
\end{aligned}
\end{equation}

Note that if we assume only random intercepts (i.e., a diagonal covariance matrix) then we can avoid the Cholesky decomposition by estimating $\sigma_u$ on the log scale. In this case, the gradient simplifies to the form given below for the measurement error.


\paragraph{Gradient w.r.t. the measurement error:}
\label{sss:grad_sigma_e}

Finally, we take the gradient of $\ell_i$ with respect to the elements of the measurement error covariance matrix, $\Sigma_{\epsilon}$. For $k = 1, ..., K$, we have
\begin{equation}
\begin{aligned}
    \frac{\partial \Sigma_i^*}{\partial \sigma_{\epsilon_{k}}} = I_{n_i} \otimes 2 \sigma_{\epsilon_k} J^k
\end{aligned}
\end{equation}

\begin{equation}
\begin{aligned}
\label{eq:grad_sigma_e}
    \frac{\partial \ell_i}{\partial \sigma_{\epsilon_{k}}} &= -\frac{1}{2} \Bigg[ tr \Bigg\{ \Sigma_i^{*-1} \frac{\partial \Sigma_i^*}{\partial \sigma_{\epsilon_{k}}} \Bigg\} - Y_i^{\top} \Sigma_i^{*-1} \frac{\partial \Sigma_i^*}{\partial \sigma_{\epsilon_{k}}} \Sigma_i^{*-1} Y_i \Bigg]
\end{aligned}
\end{equation}

\begin{equation}
\begin{aligned}
    \frac{\partial \ell_i}{\partial log(\sigma_{\epsilon_k})} = \frac{\partial \ell_i}{\partial \sigma_{\epsilon_k}} \Bigg[ \frac{log(\sigma_{\epsilon_k})}{\partial \sigma_{\epsilon_k}} \Bigg]^{-1} = \frac{\partial \ell_i}{\partial \sigma_{\epsilon_k}} \sigma_{\epsilon_k}
\end{aligned}
\end{equation}

where $J^k$ is an indicator matrix of the same dimensions as $\Sigma_{\epsilon}$.

%\subsubsection{Notes on computation of gradients}

%To increase the efficiency of calculating these gradients, we can use properties of the trace operator, specifically that $tr(ABC) = tr(BCA) = tr(CAB)$ and that if $c$ is a scalar, then $tr(c) = c$. These properties allow us to write Equations \ref{eq:grad_lambda}, \ref{eq:grad_sigma_u}, and \ref{eq:grad_sigma_e} in the simpler form of

%\begin{equation}
%\begin{aligned}
%    -\frac{1}{2} \Bigg[ tr \Bigg\{ \Big( I - \Sigma_i^{*-1} Y_i Y_i^{\top} \Big) \Sigma_i^{*-1} \frac{d\Sigma_i^{*}}{d \Theta_j}  \Bigg\} \Bigg]
%\end{aligned}
%\end{equation}

%where $\Theta_j$ could be either $\lambda_k$, $\sigma_{r_{pq}}$, or $\sigma_{\epsilon_k}$. By pre-computing $(I - \Sigma_i^{*-1} Y_i Y_i^{\top})\Sigma_i^{*-1}$ prior to calculating each gradient, we can reduce the computational expense of our estimation algorithm.


%\section{Supplement}

\section{Parameterization of the log-likelihood for standard error estimation}\label{sup:loglik_se_param}

To make our OUF model identifiable, we impose a constraint on the scale of the OU process by forcing the stationary variance equal to 1 via a set of $p$ positive scalar constants.  These constants are functions of OU parameters $\theta$ and $\sigma$.

When the log-likelihood is allowed to vary as a function all parameters, rather than just a single block of parameters as in our block coordinate descent algorithm, our model is no longer identifiable.  To estimate standard errors, we take advantage of the fact that under the identifiability constraint, $\sigma$ can be written as a function of $\theta$, as shown here:

Recall that the stationary variance of the OU process is $V := vec^{-1}\Big\{ (\theta \oplus \theta)^{-1} vec\{\sigma \sigma^{\top} \} \Big\}$.  Assuming a bivariate OU process, under the identifiability constraint, $V$ takes the form $\begin{bmatrix} 1 & \rho \\ \rho & 1 \end{bmatrix}$ where the off-diagonal element $\rho$ is the correlation.  Then,
\begin{equation*}
\begin{aligned}
    \begin{bmatrix}
    1 & \rho \\ \rho & 1
    \end{bmatrix} = vec^{-1} \Big\{ (\theta \oplus \theta)^{-1} vec\{ \sigma \sigma^{-1} \} \Big\} \Longrightarrow \begin{bmatrix}
    1 \\ \rho \\ \rho \\ 1
    \end{bmatrix} = (\theta \oplus \theta)^{-1} \begin{bmatrix}
    \sigma_{11}^2 \\ 0 \\ 0 \\ \sigma_{22}^2
    \end{bmatrix}.
\end{aligned}
\end{equation*}

Letting $$(\theta \oplus \theta)^{-1} = \begin{bmatrix} x_{11} & x_{12} & x_{13} & x_{14} \\ x_{21} & x_{22} & x_{23} & x_{24} \\ x_{31} & x_{32} & x_{33} & x_{34} \\ x_{41} & x_{42} & x_{43} & x_{44} \end{bmatrix},$$ where each element $x_{ij}$ is some function of the elements of $\theta$, we can solve for $\sigma$ in the ($\theta$, $\sigma$) pair that satisfies the identifiability constraint via
\begin{equation*}
\begin{aligned}
    1 &= x_{11} \sigma^2_{11} + x_{14} \sigma^2_{22} \\
    1 &= x_{41} \sigma^2_{11} + x_{44} \sigma^2_{22}
\end{aligned}
\end{equation*}

By constraining $\sigma$ to be a function of $\theta$, we take an alternative approach to identification and no longer require use of the scaling constants here.

\section{Calculating AIC and BIC}\label{ss:aic_formula}

To compare models fit with different numbers of latent factors, we use AIC and BIC.  The formulas for AIC and BIC are given below.  Note that these formulas account for the identifiability constraints described above in Section \ref{ss:rescaling_OU} and in Lemma 4 of the main text.  Letting $\hat{L}$ denote the maximized value of the likelihood of the OUF model used to calculate point estimates; $q$ be the total number of non-zero parameters in $\Lambda, \Sigma_u, \Sigma_{\epsilon}, \theta_{OU}$ and $\sigma_{OU}$; and $p$ be the number of latent factors (which corresponds to the number of scaling constants needed to impose the identifiability constraint), AIC is calculated as:

\begin{equation*}
\begin{aligned}
    2 \times (q-p) - 2log\hat{L}
\end{aligned}
\end{equation*}

\noindent BIC is calculated similarly as


\begin{equation*}
\begin{aligned}
    2 \times log(N) \times (q-p) - 2log\hat{L}
\end{aligned}
\end{equation*}
where $N$ is the total number of independent subjects in the data.


\section{Choice of true OU process in simulation study}\label{ss:true_ou}

In the simulation study described in the main text (Section 4.1-4.2), we generate data in three different settings in which the true OU process has varying degrees of auto-correlation.  We present the true OU process parameters here:

\paragraph{Setting 1:}

\begin{align*}
    \theta = \begin{bmatrix}
        1 & 0.6 \\ 4 & 5
    \end{bmatrix} \text{  and  }
    \sigma = \begin{bmatrix}
        1 & 0 \\ 0 & 2
    \end{bmatrix}
\end{align*}

\paragraph{Setting 2:}

\begin{align*}
    \theta = \begin{bmatrix}
        1.0 & 0.4 \\ 1.8 & 3.0
    \end{bmatrix} \text{  and  }
    \sigma = \begin{bmatrix}
        1.25 & 0 \\ 0 & 2.00
    \end{bmatrix}
\end{align*}

\paragraph{Setting 3:}

\begin{align*}
    \theta = \begin{bmatrix}
         1 & 0.5 \\ 2 & 5
    \end{bmatrix} \text{  and  }
    \sigma = \begin{bmatrix}
        2 & 0 \\ 0 & 3
    \end{bmatrix}
\end{align*}

\vspace{1cm}

In the simulation study assessing use of AIC and BIC to select the correct number of latent factors in a model (described in the main text in Section 4.3), the true parameters were set to the values listed below.  The true values used for $\Sigma_u$ and $\Sigma_{\epsilon}$ were the same as in the original simulation study (see Section 4.1 in the main text).

\paragraph{One factor model:}

\begin{align*}
    \Lambda = \begin{bmatrix}
    1.2 \\ 1.8\\ -0.4\\ 2
        \end{bmatrix}, \ \ 
    \theta = 0.8, \ \  \sigma = 1
\end{align*}

\paragraph{Two factor model with low signal:}

\begin{align*}
    \Lambda = \begin{bmatrix}
    1.2 & 0 \\ 1.8 & 0 \\ 0 & -0.4 \\ 0 & 2
        \end{bmatrix}, \ \ 
    \theta = \begin{bmatrix}
        2 & 0.5 \\ 0.4 & 4
    \end{bmatrix}, \ \ 
    \sigma = \begin{bmatrix}
        2 & 0 \\ 0 & 1
    \end{bmatrix}
\end{align*}


\paragraph{Two factor model with high signal:}

\begin{align*}
    \Lambda = \begin{bmatrix}
    1.2 & 0 \\ 1.8 & 0 \\ 0 & -0.4 \\ 0 & 2
        \end{bmatrix}, \ \ 
    \theta = \begin{bmatrix}
        1 & 1.5 \\ 2 & 5
    \end{bmatrix}, \ \ 
    \sigma = \begin{bmatrix}
        2 & 0 \\ 0 & 3
    \end{bmatrix}
\end{align*}


\paragraph{Three factor model with low signal:}

\begin{align*}
    \Lambda = \begin{bmatrix}
    1.2 & 0 & 0 \\ 1.8 & 0 & 0 \\ 0 & -0.4 & 0 \\ 0 & 0 & 2
        \end{bmatrix}, \ \ 
    \theta = \begin{bmatrix}
        2 & 0.2 & 0.4 \\ 0.8 & 1.1 & 0.5 \\ 0.7 & 0.5 & 1.2
    \end{bmatrix}, \ \ 
    \sigma = \begin{bmatrix}
        1.2 & 0 & 0 \\ 0 & 0.8 & 0 \\ 0 & 0 & 0.4 
    \end{bmatrix}
\end{align*}


\paragraph{Three factor model with high signal:}

\begin{align*}
    \Lambda = \begin{bmatrix}
    1.2 & 0 & 0 \\ 1.8 & 0 & 0 \\ 0 & -0.4 & 0 \\ 0 & 0 & 2
        \end{bmatrix}, \ \ 
    \theta = \begin{bmatrix}
        1 & 0.4 & 0.6 \\ 1.8 & 3 & 0.9 \\ 0.9 & 1 & 1.2
    \end{bmatrix}, \ \ 
    \sigma = \begin{bmatrix}
        1.2 & 0 & 0 \\ 0 & 0.8 & 0 \\ 0 & 0 & 0.4 
    \end{bmatrix}
\end{align*}


\section{Discussion of numerical issues in simulation results}\label{supp:sim_issues}

\paragraph{Simulation study: settings 1-3}

The estimation algorithm failed to converge due to numerical issues when applied to a few of the simulated datasets generated in the simulation study described in Section 4.1-4.2. The failures were caused by a singular V matrix at the start of the first block update of the structural submodel parameters.  Slightly altering the values at which the OU process parameters were initialized resolved this issue.  Point estimates were ultimately calculate for all 1000 simulated datasets in each setting.  In Setting 3, an invalid variance for the measurement submodel parameter $\sigma_{\epsilon_4}$ was estimated from one dataset. In this instance, the variance estimated for this parameter was negative.  We attribute this issue to the numerical approximation used to calculate the Hessian when applied to these this dataset of size $N = 200$.  We anticipate that a larger dataset would improve the approximation of the numerical Hessian but chose to simulate a dataset of this size in order to assess model performance in a realistic setting similar to that encountered in the motivating data application.  In practical application, if a negative variance were to be estimated, it could be rounded to 0. In the results presented in the main text, we ignore the variance estimate for this one $\sigma_{\epsilon_4}$.


\paragraph{Simulation study: model selection}

In our simulation studies, we aimed to assess simulated datasets with sample sizes similar to that of our motivating dataset.  For datasets of fixed size ($N = 200$ subjects), we found that convergence speeds decrease and estimation becomes more difficult as the number of factors in the model increases.  We found that point estimates of the diagonal elements of $\theta_{OU}$ hit the lower bound of $1\times10^{-4}$ less than 1\% of the time.  To improve convergence, we slightly altered the set of default parameter values considered during the initialization steps of the block-wise estimation algorithm for a subset of datasets.  However, when assessing AIC and BIC as model selection criteria (see Section 4.3), we very occasionally encountered numerical issues and so failed to calculate parameter estimates for a subset of models applied to the simulated datasets.  The results reported in the main paper correspond to a comparison of AIC and BIC across datasets for which the algorithm used to fit all three models (the one-factor, two-factor, and three-factor models) either converged or reached the maximum number of iterations prior to convergence.  We assessed whether or not including results in which the maximum number of iterations was reached prior to convergence impacted our model selection results and found no substantial changes.  Supplementary Table \ref{tab:GoF_results_convg_only} shows the equivalent version of Table 1 presented in the main text if only results from datasets that had converged were shown.

\begin{table}
    \centering
    % Figure removed
    \caption{For datasets generated under each true model, we summarize the percent of times that the model-selection metric chose the fitted model with the indicated number of factors.  The settings in which the fitted model has the same number of factors as the true data-generating model are emphasized with bold orange text. These results are presented for datasets on which the algorithm converged prior to reaching the maximum number of iterations (200) for all three models.}\label{tab:GoF_results_convg_only} %\ref{supp:sim_issues}
\end{table}

In Supplementary Table \ref{tab:GoF_results_convg_summary}, we summarize the number (out of 100) of datasets (in each setting) for which the algorithm converged (using $\delta = 1\times10^{-6})$ or reached the maximum number of iterations prior to convergence.  When this total number does not add up to 100, the remaining datasets correspond to situations in which the algorithm failed due to numerical issues (e.g., current OU parameter estimates corresponded to a singular stationary covariance matrix).

\begin{table}
    \centering
    % Figure removed
    \caption{For datasets generated under each true model, we summarize the number of datasets (out of 100) on which the algorithm converged or reached the maximum number of block-wise iterations prior to convergence (when $\delta = 1\times10^{-6}$).  For totals that do not sum to 100, the remaining cases correspond to instances in which the algorithm failed due to numerical issues prior to converging or reaching the maximum number of block-wise iterations (200).}\label{tab:GoF_results_convg_summary} %\ref{supp:sim_issues}
\end{table}

After loosening the convergence criteria across the block-wise iterations, we did not find substantially different results when evaluating AIC and BIC as model selection criteria when compared to results under the original convergence criteria.  For example, if we categorized convergence using $\delta \le 1\times10^{-3}$, rather than only the original $\delta = 1\times10^{-6}$, the algorithm would have converged when fitting almost every model to almost every dataset (see Supplementary Table \ref{tab:GoF_results_convg_summary_1e-3}) but the model selection results would not have changed (see Supplementary Table \ref{tab:GoF_results_convg_only_1e-3}).

\begin{table}
    \centering
    % Figure removed
    \caption{For datasets generated under each true model, we summarize the number of datasets (out of 100) on which the algorithm converged or reached the maximum number of block-wise iterations prior to convergence (when $\delta \le 1\times10^{-3}$).  For totals that do not sum to 100, the remaining cases correspond to instances in which the algorithm failed due to numerical issues prior to converging or reaching the maximum number of block-wise iterations (200).}
    \label{tab:GoF_results_convg_summary_1e-3} %\ref{supp:sim_issues}
\end{table}

\begin{table}
    \centering
    % Figure removed
    \caption{For datasets generated under each true model, we summarize the percent of times that the model-selection metric chose the fitted model with the indicated number of factors.  The settings in which the fitted model has the same number of factors as the true data-generating model are emphasized with bold orange text. These results are presented for datasets on which the algorithm converged (using $\delta \le 1\times10^{-3}$)  prior to reaching the maximum number of iterations (200) for all three models.}\label{tab:GoF_results_convg_only_1e-3} %\ref{supp:sim_issues}
\end{table}


We expect that increasing the size of the simulated dataset would increase the rate at which we successfully fit models with more factors.

\renewcommand{\thesection}{B.\arabic{section}}
\setcounter{section}{0}
\section*{Section B}



\section{Application to mHealth emotion data}

\subsection{OUF model with one factor}

In this model, we assume that a single latent factor generates all observed emotions of happy, joyful, enthusiastic, active, calm, determined, grateful, proud, attentive, sad, scared, disgusted, angry, ashamed, guilty, irritable, lonely, and nervous.  We plot the point estimates from this model in Supplementary Figure \ref{fig:point_ests_1factor}.  Using these estimated parameters, we calculate the auto-correlation half-life of this latent factor as approximately 27 days.  This model has a total of 56 free parameters, along with one constraint, which we use when calculating AIC and BIC.

% point estimates
% Figure environment removed

\subsection{OUF model with two factors}

In this model, we assume that two latent factors generate the observed emotions.  The latent factors represent positive affect (which underlies happy, joyful, enthusiastic, active, calm, determined, grateful, proud, and attentive) and negative affect (which underlies sad, scared, disgusted, angry, ashamed, guilty, irritable, lonely, and nervous).  Results from this fitted model are available in Section 5 of the main text.  This model has a total of 60 free parameters, along with two constraints, which we use when calculating AIC and BIC.


\subsection{OUF model with three factors}

We assume that three latent emotional states underlie the emotions observed during this study.  The emotions load on to the latent factors as follows:

\begin{enumerate}
    \item enthusiastic, proud, active, calm, determined, attentive, grateful [$\eta_1 = $ high arousal positive affect]
    \item calm, happy, joyful [$\eta_2 = $ no-to-low arousal positive affect]
    \item sad, scared, disgusted, angry, ashamed, guilty, irritable, lonely, nervous [$\eta_3 = $ negative affect]
\end{enumerate}

We use behavioral science literature and theory---namely the circumplex model of emotion---to inform the division of the positive affect emotions into groups representing high arousal positive affect and no-to-low arousal positive affect (see \cite{reisenzein_1994, remington_2000, gilbert_2008, mcmanus_2019}).  Literature supporting the placement of each positive affect emotion is summarized in Table \ref{tab:refs_for_3factor_mod}.  Happy and joyful are also commonly placed midway between high and low arousal in the circumplex model of emotion (see \cite{remington_2000}) and so we chose to assess the fit of the OUF model when these emotion items load onto the latent factor representing no-to-low arousal positive affect.  This model converged after 211 block iterations and we present point estimates in Supplementary Figure \ref{fig:point_ests_3factor}.  This model has a total of 66 free parameters, along with three constraints, which we use when calculating AIC and BIC.

\begin{table}
    \centering
    % Figure removed
    \caption{Behavioral science literature supporting the division of the positive emotions into two groups representing no-to-low arousal positive affect and high arousal positive affect.}
    \label{tab:refs_for_3factor_mod} %\ref{supp:sim_issues}
\end{table}


% point estimates
% Figure environment removed

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\renewcommand{\thesection}{C.\arabic{section}}
\setcounter{section}{0}
\section*{Section C}


\section{Estimation algorithm}\label{sup:est_alg}

\subsection{Parameter initialization}

Due to the complexity of our model, our estimation algorithm is sensitive to the choice of initial estimates. Here we present an approach to estimating reasonable starting values based on simple existing models prior to maximizing the entire likelihood.

\begin{enumerate}
    \item To initialize the \textbf{measurement submodel parameters}, fit a standard cross-sectional factor model to the data collapsed across time (do not include a random intercept but do assume that the positions of the non-zero loadings are known).
    \item Using this fitted factor model, estimate the factor scores (predicted values for $\eta_1$ and $\eta_2$). 
    \item Fit four separate linear mixed effects models---one for each of the observed outcomes, $Y_1, ..., Y_K$---including the factor scores as fixed effects and a random intercept for subject. We do not include a fixed effect intercept in these models. For outcome $k = 1, ..., K$, subject $i = 1, ..., N$, and measurement occasion $j = 1, ..., n_i$, the mixed model is given by $$Y_{kij} = \lambda_{k} \eta_i(t_j) + u_{k0i} + \epsilon_{kij}$$ where $u_{k0i} \sim N(0, \sigma^2_{u_k})$ and $\epsilon_{kij} \sim N(0, \sigma^2_{\epsilon_k}$).
    \item From each of these $K$ mixed models, extract estimates of the coefficient for the fixed effect, the variance for the random intercept, and the residual variance.  Use the coefficients of the fixed effects to initialize the non-zero elements of $\Lambda$ and the variance estimates to initialize the diagonal components of $\Sigma_u$ and $\Sigma_{\epsilon}$.  In some cases, the estimated variances were very small, so a lower limit of 0.1 was set for the initial parameter values to avoid extremely negative estimates after logging. We also set the same lower bound for initial values of the elements in the loadings matrix.
    \item To initialize the \textbf{structural submodel parameters}, we add a term for white noise to the OU process likelihood. This noise term will absorb some of the extra variability in the predicted factor scores and allow for more stable estimation. Let $\Gamma_i$ be white noise, then $\eta_i \sim N(0, \Psi_i + \Gamma_i)$ where $\Gamma_i$ is a diagonal matrix (of the same dimension as OU covariance matrix $\Psi_i$) with constant but unknown diagonal $\gamma$. We then maximize this likelihood and use the estimated OU process parameter values as initial values, restricting the maximum initial values of the diagonals of $\theta_{OU}$ to be less than 7.  This maximum helps deal with instability in the initial estimate of $\theta$.
\end{enumerate}

\subsection{Maximization of the marginal log-likelihood}

To maximize the log-likelihood, we use quasi-Newton optimizers as implemented in the \texttt{stats} package in \texttt{R} \citep{R_stats}. To prevent the parameter estimates from diverging to infinite values, we set the maximum allowed step size to 10.

Using the initial parameter values estimated via the approach described in the previous section, we iteratively update measurement and structural submodel parameter estimates in blocks:


\begin{enumerate}
    \item Initialize estimates: $\Lambda^{(0)}, \Sigma_u^{(0)}, \Sigma_{\epsilon}^{(0)}, \theta^{(0)}, \sigma^{(0)}$.  Measurement submodel parameters are always initialized empirically; for structural submodel parameters, two sets of initial estimates are considered---an empirical set of values estimated as described above and a default set of values that are based on a reasonable guess.  The set of values that corresponds to the higher log-likelihood is used.
    \item Set $r = 1$ and $\delta = 0$. While $r \le 200$ and $\delta = 0$,
    \begin{enumerate}
        \item Update block of \textbf{measurement submodel parameters}: $$\Lambda^{(r)}, \Sigma_u^{(r)}, \Sigma_{\epsilon}^{(r)} = \underset{\Lambda, \Sigma_u, \Sigma_{\epsilon}}{argmax}\big\{ logL(Y | \theta^{(r-1)}, \sigma^{(r-1)}) \big\}.$$ We solve this iteratively using \texttt{nlm} \citep{R_stats} and analytic gradients with convergence criteria set to \texttt{gradtol} $= max(1\times10^{-4}/10^r, 1\times10^{-8})$ and \texttt{steptol} $= max(1\times10^{-4}/10^r, 1\times10^{-8})$.  \texttt{gradtol} is the tolerance for the scaled gradient and \texttt{steptol} is the tolerance for parameter estimates across iterations.  We model the first element of the loadings matrix and the variance parameters on the log scale, since all of these estimates are required to be positive.
        \item Update block of \textbf{structural submodel parameters}: $$\theta^{(r)}, \sigma^{(r)} = \underset{\theta, \sigma}{argmax}\big\{ logL(Y | \Lambda^{(r)}, \Sigma_u^{(r)}, \Sigma_{\epsilon}^{(r)}) \big\}.$$ We solve this iteratively using \texttt{nlminb} and numeric approximations to the gradients.  For estimates of $\theta$, the diagonal elements must be positive and the matrix must have eigenvalues with positive real parts.  The diagonal element of $\sigma$ are estimated on the log scale, since they are required to be positive.
        \item Check for block-wise convergence: Let $\Theta$ be a vector containing all elements of $\Lambda$, $\Sigma_u$, $\Sigma_{\epsilon}$, $\theta$, and $\sigma$. Then, calculate $$\delta = \max \Big\{I\big\{|\Theta^{(r)} - \Theta^{(r-1)}|/\Theta^{(r)} < 10^{-6}\big\},  I\big\{logL(\Theta^{(r)} | Y) - logL(\Theta^{(r-1)} | Y) < 10^{-6}\big\}\Big\}$$ where all operations on $\Theta$ are element-wise.
        \item Rescale OU process parameters so stationary variance is equal to 1 using Lemma 4.
        \item Update $r$: $r = r+1$
    \end{enumerate}
    \item Estimate standard errors using a numerical approximation to the Hessian of the joint negative log-likelihood for $\Lambda^{(r)}, \Sigma_u^{(r)}, \Sigma_{\epsilon}^{(r)}, \theta^{(r)}$ at the current parameter values.  Rather than rescaling the OU parameters so stationary variance is equal to 1 using Lemma 4, we assume that $\sigma$ is a function of $\theta$. See Section A.6 of the supplementary material for further description of this function. The numeric approximation to the Hessian is carried out using the \texttt{optimHess} function in the \texttt{stats} package.
    \item Estimate confidence interval for OU process parameter $\sigma$ based on a parametric bootstrap of $\theta$.
\end{enumerate}


\section{Comparison with Tran et al. (2021b)} % updated

To illustrate the computational benefits of our proposed block coordinate descent algorithm for estimation relative to the Bayesian approach taken in \cite{tran_2021b}, we apply both methods to simulated datasets.  Because we only consider continuous outcomes in this work, we slightly modify the original model proposed in \cite{tran_2021b} and do not estimate the additional parameters used to account for non-continuous outcomes.  \cite{tran_2021b} also consider two different sets of constraints on the OU process drift matrix (denoted here as $\theta_{OU}$); we use the set of constraints that specify the eigenvalues of $\theta_{OU}$ to have positive real parts.

We use the same simulation set-up as described in the main text (Section 4.1) with the true OU process parameters corresponding to Setting 1 (Section \ref{ss:true_ou}).  We make one modification to the true values of the loadings parameters: we restrict \textit{all} elements of the loadings matrix to be positive.  This restriction means that $\lambda_3 = 0.4$, rather than the original $\lambda_3 = -0.4$.  We make this assumption in order to make identification of parameters more straightforward in this comparison of methods.

We generate 100 replicates of the simulated dataset and fit the OUF model using our proposed estimation algorithm and the algorithm proposed in \cite{tran_2021b}.  \cite{tran_2021b} use a slightly different parameterization of the OU process than we use in this work. 
 In our implementation of the OU process, we restrict the volatility parameter matrix, $\sigma_{OU}$, to be a diagonal matrix.  Although \cite{tran_2021b} do not make this assumption, there is still a one-to-one correspondence between the set of parameters estimated in our work and the set of posterior estimates resulting from their Bayesian method.  As a result of these differences in parameters, we do not report estimates of $\sigma_{OU}$ in the plot below and instead present parameter estimates for $\rho$, which is the stationary correlation between $\eta_1$ and $\eta_2$.  Tran et al. estimate this parameter directly and we can calculate an estimate for it using $\hat{\theta}_{OU}$ and $\hat{\sigma}_{OU}$.

When applying the Bayesian approach, we use our proposed empirical approach to initializing parameter values, assume 4 chains, and allow the sampler to run for 2,000 iterations.  We discard the first half of these samples as burn-in.  The computation time of both approaches---excluding time required to compute initial parameter estimates---is shown in Figure \ref{fig:comp_time_comparison}.  Computing resources are the same across all replicates; we use 4 cores with a total of 4GB of memory for each replicate (this allows gradients to be evaluated or chains to be sampled in parallel, depending on the method).  We find that our approach, on average, takes approximately 5\% of the time required by the method in Tran et al (2021b).

% point estimates
% Figure environment removed

Point estimates for both estimation approaches are shown in Supplementary Figure \ref{fig:pt_est_comparison}.  We present the posterior means for each parameter across the 100 simulated datasets as estimated using the method from \cite{tran_2021b}; maximum likelihood estimates resulting from our block coordinate descent algorithm are also summarized across the 100 simulated datasets.  Note that the posterior estimates from the Bayesian estimate may be slightly improved by running the sampling algorithm for additional iterations; we limit the MCMC algorithm to 2,000 iterations since our focus is on comparing computation time.

% point estimates
% Figure environment removed

%\processdelayedfloats

\newpage 
\bibliographystyle{biom} 
\bibliography{references.bib}

\end{document}