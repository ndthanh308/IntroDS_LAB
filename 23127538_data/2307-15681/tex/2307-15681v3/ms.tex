%  template.tex for Biometrics papers
%
%  This file provides a template for Biometrics authors.  Use this
%  template as the starting point for creating your manuscript document.
%  See the file biomsample.tex for an example of a full-blown manuscript. 
%  
%\documentclass[useAMS,referee,usenatbib]{biom}
\documentclass[useAMS,referee,usenatbib]{biom}

\usepackage{setspace}
\singlespacing

\def\bSig\mathbf{\Sigma}
\newcommand{\VS}{V\&S}
\newcommand{\tr}{\mbox{tr}}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{mathtools}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title[Dynamic factor model for ILD]{A Continuous-Time Dynamic Factor Model for Intensive Longitudinal Data Arising from Mobile Health Studies}

\author
{Madeline R. Abbott\emailx{mrabbott@umich.edu} \\
Department of Biostatistics, University of Michigan--Ann Arbor, Ann Arbor, Michigan, U.S.A.
\and
Walter H. Dempsey\emailx{wdem@umich.edu} \\
Department of Biostatistics, University of Michigan--Ann Arbor, Ann Arbor, Michigan, U.S.A.
\and
Inbal Nahum-Shani\emailx{inbal@umich.edu} \\
Institute for Social Research, University of Michigan--Ann Arbor, Ann Arbor, Michigan, U.S.A.
\and
Cho Y. Lam\emailx{cho.lam@hci.utah.edu} \\
Department of Population Health Sciences and Huntsman Cancer Institute, University of Utah, \\ Salt Lake City, UT, U.S.A.
\and
David W. Wetter\emailx{david.wetter@hci.utah.edu} \\
Department of Population Health Sciences and Huntsman Cancer Institute, University of Utah, \\ Salt Lake City, UT, U.S.A.
\and
Jeremy M. G. Taylor\emailx{jmgt@umich.edu} \\
Department of Biostatistics, University of Michigan--Ann Arbor, Ann Arbor, Michigan, U.S.A.}


\begin{document}

\date{   }

\pagerange{\pageref{firstpage}--\pageref{lastpage}} 
  
\label{firstpage}

\begin{abstract}
Intensive longitudinal data (ILD) collected in mobile health (mHealth) studies contain rich information on multiple outcomes measured frequently over time that have the potential to capture short-term and long-term dynamics. Motivated by an mHealth study of smoking cessation in which participants self-report the intensity of many emotions multiple times per day, we describe a dynamic factor model that summarizes the ILD as a low-dimensional, interpretable latent process. This model consists of two submodels: (i) a measurement submodel---a factor model---that summarizes the multivariate longitudinal outcome as lower-dimensional latent variables and (ii) a structural submodel---an Ornstein-Uhlenbeck (OU) stochastic process---that captures the temporal dynamics of the multivariate latent process in continuous time. We derive a closed-form likelihood for the marginal distribution of the outcome and the computationally-simpler sparse precision matrix for the OU process. We propose a block coordinate descent algorithm for estimation. Finally, we apply our method to the mHealth data to summarize the dynamics of 18 different emotions as two latent processes. These latent processes are interpreted by behavioral scientists as the psychological constructs of positive and negative affect and are key in understanding vulnerability to lapsing back to tobacco use among smokers attempting to quit.
\end{abstract}

\begin{keywords}
dynamic factor model, intensive longitudinal data, mobile health, Ornstein-Uhlenbeck stochastic process
\end{keywords}

\maketitle

\section{Introduction}
\label{s:intro}

Intensive longitudinal data (ILD) can capture rapid changes in outcomes over time. In mobile health (mHealth) studies, information about multiple longitudinal outcomes is often collected with the aim of understanding the temporal dynamics of unobservable constructs related to mental or physical health. Our work is motivated by an mHealth study of smoking cessation in which the intensity of emotions over time was collected from current smokers attempting to quit. Participants self-reported the intensity of 18 different emotions up to four times per day over 10 days, resulting in a substantial quantity of rich data. For smoking cessation researchers, understanding the temporal dynamics of the latent psychological states that underlie these emotions is of scientific interest.

The volume and complexity of ILD, however, make them challenging to analyze since longitudinal outcomes are often measured irregularly across many individuals; thus statistical methods must be able to handle the high volume of irregularly spaced data. At the same time, the frequent measurements in ILD create many opportunities to discover new information, particularly if the latent constructs of interest vary rapidly. We present a dynamic factor model that is motivated by the need to model multiple longitudinal outcomes measured frequently over time in a flexible yet interpretable manner. Our proposed model consists of two submodels: (i) a measurement submodel---a factor model---that summarizes the multiple observed longitudinal outcomes as lower-dimensional latent factors and (ii) a structural submodel---an Ornstein-Uhlenbeck (OU) stochastic process---that captures the evolution of the multiple correlated latent factors over time. Together, these components of our dynamic factor model are flexible enough to capture the variability in the longitudinal outcomes while avoiding use of a non-parametric or other many-parameter model that inhibits interpretability. In addition to improving interpretability, the low-dimensional nature of the structural submodel also greatly reduces computational complexity, as opposed to fitting a high-dimensional stochastic process directly to the observed outcomes.

One standard approach to modeling changes in multiple correlated longitudinal variables over time is to use an autoregressive (AR) model. These models, which are called vector autoregressive (VAR) models when data are multivariate, have been widely used to model observed outcomes as well as latent variables. For example, \citet*{dunson_2003}, \citet*{cui_2014}, and \citet{tran_2019} have proposed related methods in which observed longitudinal outcomes are summarized as time-varying lower-dimensional latent variables. The correlation of these latent variables is then modeled with AR or VAR processes. VAR models, however, are specified for balanced data. This situation is often not realistic in the case of ILD, which generally consists of irregularly-measured outcomes, and can lead to biased estimates in cases where the assumption is made but does not hold.

Mixed models have been proposed as alternatives to discrete-time processes for modeling the evolution of latent variables over time and have been previously used in combination with factor models. Unlike the AR and VAR processes, mixed models do not require balanced data. Existing work has focused both on the development of mixed models for modeling the evolution of a single latent factor over time (e.g., \citealp{roy_2000}; \citealp{proust_2006}; \citealp*{proust_lima_2013}) or multiple latent factors (e.g., \citealp{liu_2019}; \citealp*{wang_2013}). Overall, these mixed model-based approaches are useful tools for capturing smooth trends in latent factors. In our application, however, we aim to develop a method that can capture the correlation between and rapid variation in multiple latent emotional constructs over time.

The OU process, which can be thought of as a continuous-time analog of the AR or VAR process, is a stochastic process well-suited for capturing rapid variations over time. Existing work has frequently focused on using the OU process or integrated OU process to model longitudinal outcomes that have been directly observed (or observed with measurement error); e.g., \citet*{taylor_1994}; \citet*{sy_1997}; \citet*{oravecz_2009}; \citet*{oravecz_2016}.

In more recent work, the OU process has also been used in the context of latent variable models. \citet{tran_2021a} propose a latent linear mixed model that summarizes multiple observed longitudinal outcomes as a smaller number of latent factors while accounting for the serial correlation between repeated measurements over time via an OU process. This work differs from ours, however, in that the fixed effects that capture the association between the observed covariates and the latent factors are of primary interest; the OU process is incorporated into the structural mixed model as a tool for accounting for serial correlation between repeated observations.

Most closely related to our proposed approach is work by \citet{tran_2021b}. Like us, they propose a longitudinal latent variable model that consists of two parts: a measurement submodel to summarize observed outcomes as lower dimensional latent factors and an OU process as the structural submodel for the latent factors. While we differ in the exact specification of the measurement submodel, our chosen models are related. Key distinctions between this existing work and the approach presented in this manuscript are in the model parameterization and computational approach. Tran et al. (2021b) take a Bayesian approach, which requires sampling values of the latent process at each measurement occasion. In the ILD setting, we need approaches that can scale to large numbers of repeated measurements. Here, we choose to work in the frequentist framework.  As a result of taking a maximum likelihood-based approach, we can directly maximize the marginal log-likelihood of the observed longitudinal outcome.  Furthermore, this framework enables us to employ various algebraic and computational strategies to make estimation faster, resulting in a method more suitable for ILD.

In this work, we fill a gap in the existing literature by proposing an Ornstein-Uhlenbeck factor (OUF) model that captures the temporal dynamics between rapidly-varying correlated latent factors observed via multiple longitudinal outcomes and an estimation algorithm with the computational efficiency to handle ILD. Our novel contributions include (i) a closed-form likelihood for the marginal distribution of the observed outcome, (ii) the derivation of the computationally-simpler sparse precision matrix for the multivariate OU process, (iii) identifiability constraints imposed via scaling constants, and (iv) a block coordinate descent algorithm for estimation and inference in a maximum likelihood framework.

The remainder of this paper is organized as such:  In Section \ref{s:motivating_data}, we describe the motivating mHealth data; in Section \ref{s:method}, we present our novel method and contributions; in Section \ref{s:sim_study}, we demonstrate the performance of our method via simulation; in Section \ref{s:data_app}, we illustrate our model via a scientifically--meaningful application to mHealth data; and in Section \ref{s:discussion}, we provide a discussion.

\section{Motivating data}\label{s:motivating_data}

Data motivating this work come from an mHealth study of smoking cessation \citep{potter_2023}. In this observational study, current smokers (N = 218) who were attempting to quit were followed for 10 days. During these 10 days, ecological momentary assessments (EMAs), which enable repeated sampling of individuals' current states and contexts in real time, were used to track participants' emotions as they were experienced in a high-frequency manner. Specifically, participants were prompted to respond to a series of questions sent to their smartphones multiple per day at random occasions; the original study design intended for individuals to receive up to four EMAs per day. The EMAs contained a set of questions that assessed the current intensity of multiple emotions measured on a 5-point Likert scale. We focus on a set of 18 emotions consisting of both positive and negative emotions that attempts to capture the distinct-but-correlated underlying emotional states of positive and negative affect (i.e., summary measures of overall positive and negative feeling). The resulting data contain frequent measurements of a substantial number of longitudinal outcomes, where the number of measurement occasions per person ranges from 2 to 47 (mean = 17). Note that these data are only the subset of the full study data that were available at the time of drafting this manuscript.  Additional details on the study procedures can be found in \cite{potter_2023}. The high rate of measurement enables us to capture rapid changes in emotions over time. To illustrate the dynamics of these responses, Figure \ref{fig:real_dat} shows the responses to emotion-related EMA questions over time for one participant in the study. Understanding the dynamics of smokers' latent emotional states that underlie the measured responses is of scientific interest among smoking cessation researchers and behavioral scientists more broadly.


% Figure environment removed
        

\section{Methods}\label{s:method}

In this section, we present the OUF model that jointly models multiple observed longitudinal outcomes and the lower dimensional latent factors assumed to generate the observed longitudinal outcomes. Our proposed model consists of two submodels: a measurement submodel and a structural submodel.

\subsection{Measurement submodel}

Let $\bm{Y}_i(t) = [Y_{i1}(t), Y_{i2}(t), ..., Y_{iK}(t)]^{\top}$ be a $K \times 1$ vector of measured longitudinal outcomes for individual $i, i = 1, ..., N$, at time $t$. Assume that individual $i$ has longitudinal outcomes measured at $n_i$ occasions. Using the measurement submodel, we model the observed longitudinal outcome $\bm{Y}_i(t)$ as
\begin{equation}
\begin{aligned}
\label{eq:meas_submodel}
    \bm{Y}_{i}(t) = \bm{\Lambda} \bm{\eta}_i(t) + \bm{u}_i + \bm{\epsilon}_i(t)
\end{aligned}
\end{equation}

\noindent where $\bm{\eta}_i(t)$ is a vector of $p$ time-varying latent factors (where $p < K$); $\bm{\Lambda}$ is a $K \times p$-dimensional time-invariant loadings matrix with elements $\lambda_{k,j}$ that captures the degree of association between the latent factors and observed longitudinal outcomes; $\bm{u}_i \sim N(0, \bm{\Sigma}_u)$ is a vector of length $K$ of random intercepts; and $\bm{\epsilon}_i(t) \sim N(0, \bm{\Sigma}_{\epsilon})$ is a vector representing measurement error, where $\bm{\Sigma}_{\epsilon}$ is assumed to be a diagonal matrix.

This model builds upon a standard factor model but also includes (i) a random intercept and (ii) a multivariate model for the evolution of the correlated latent processes $\bm{\eta}_i(t)$ (described in Section \ref{ss:structural_submod}).  We assume that $\bm{\Sigma}_u$ is diagonal, as we include this term to account for the longitudinal correlation in the repeated measurements but then model the correlated change in outcomes through the structural submodel.  Allowing a non-diagonal $\bm{\Sigma}_u$ is possible, but we opt not to do so to avoid the substantial increase computational cost associated with estimation of these extra parameters.  We also assume that $\bm{\Lambda}$ contains many structural zeros such that each row of the loadings matrix contains only one non-zero element; this structure means that each observed outcome is a measurement of only a single underlying latent factor. The decision to incorporate structural zeros in the loadings matrix is supported by behavioral science concepts (i.e., Positive and Negative Affect Schedule; PANAS \citep*{watson_1988}), which classify a given emotion as a measurement of a specific category of emotional state.  Learning the location of the structural zeros, rather than pre-specifying them, is a possible direction for future work.

\subsection{Structural submodel}\label{ss:structural_submod}

The structural submodel captures the evolution of the latent factors, $\bm{\eta}_i(t)$, over time. We use a multivariate OU process, which can be understood as a continuous-time analog of a VAR process and has the ability to capture rapid temporal variation. Here, we assume a bivariate OU process ($p = 2$) for illustrative purposes. The stochastic differential equation definition of the bivariate OU process is
\begin{equation}
\begin{aligned}
    d \begin{bmatrix} \eta_{i1}(t) \\ \eta_{i2}(t) \end{bmatrix} =  - \underbrace{\begin{bmatrix} \theta_{11} & \theta_{12} \\ \theta_{21} & \theta_{22} \end{bmatrix}}_{\coloneqq\bm{\theta}} \begin{bmatrix} \eta_{i1}(t) \\ \eta_{i2}(t) \end{bmatrix} dt + \underbrace{\begin{bmatrix} \sigma_{11} & 0 \\ 0 & \sigma_{22} \end{bmatrix}}_{\coloneqq\bm{\sigma}} d\begin{bmatrix} W_{i1}(t) \\ W_{i2}(t) \end{bmatrix}
\end{aligned}
\end{equation}

\noindent where the diagonal elements of matrix $\bm{\theta}$ capture the mean-reverting tendency of the latent factors (where the mean is assumed to be $0$) and the off-diagonal elements of $\bm{\theta}$ capture correlation between the latent factors.  The diagonal elements of $\bm{\theta}$ are required to be positive.  The matrix $\bm{\sigma}$, with elements $\sigma_{11}$ and $\sigma_{22} > 0$, describes the volatility of the process, where $W_{i1}(t)$ and $W_{i2}(t)$ are both standard Brownian motion. In general, the standard definition of the OU process allows $\bm{\sigma}$ to take non-zero values in the off-diagonal. By restricting $\bm{\sigma}$ to be a simpler diagonal matrix here, we consider the Brownian motion terms as separate noise processes for each latent variable and thus capture all correlation between the latent processes through the $\bm{\theta}$ matrix. We also require that all eigenvalues of the $\bm{\theta}$ matrix have a positive real part; this constraint ensures a mean-reverting process (see \cite{vatiwutipong_2019}).  

The multivariate stochastic process provides advantages over a simpler univariate stochastic process as it captures correlation between multiple variables over time; however, the multivariate nature does increase the complexity of the model and thus the computational burden. We address this complexity in the next section.


\subsubsection{Marginal covariance and precision matrices for the OU process}\label{ss:marginal_ou}

Rather than taking a Bayesian strategy or relying on the complete-data likelihood and taking an expectation-maximization (EM) approach to estimation, we directly maximize the likelihood of the observed data.  Direct maximization of the marginal likelihood allows us to avoid repeatedly calculating values of the latent factors at each measurement occasion (via posterior sampling in a Bayesian framework or via complex integrals in the E-step of the EM algorithm). Thus, our approach is more scalable to the ILD setting.

%complex integration that would be required in the E-step of the EM algorithm and does not require computing the expectation of the latent factors at each measurement occasion, making our approach more scalable to the ILD setting.

In order to carry out our estimation algorithm (described in Section \ref{ss:est_approach}), we require the marginal covariance matrix of the OU process. \citet{vatiwutipong_2019} present a form of the conditional variance and cross-covariance function for the OU process but provide these functions in integral forms that are not amenable to likelihood-based inference. To avoid approximations resulting from numerical integration, (i) we derive an analytic form of the conditional covariance function and (ii) we account for the additional uncertainty of an unknown initial state by deriving the analytic form of the marginal covariance function. For a stationary OU process with known initial state at time $t_0 = 0$, $\bm{\eta}(t_0)$, the conditional mean at time $t$ is $\mathbb{E}\{\bm{\eta}(t) | \bm{\eta}(t_0)\} = e^{-\bm{\theta} t} \bm{\eta}(t_0)$.  Assuming a marginal mean of 0, the conditional and marginal covariance functions follow:

\begin{lemma}\label{def:conditional_cross_cov}
The analytic form of the OU conditional covariance at times $s$ and $t$, where $s \leq t$, is
\begin{equation*} 
\begin{aligned}
\label{eq:conditional_cross_cov}
    Cov\{\eta(s), \eta(t)| \eta(t_0 = 0)\} &= vec^{-1}\Big\{(\theta \oplus \theta)^{-1} \big[ e^{s (\theta \oplus \theta)} - I \big] e^{-[t\theta \oplus s\theta]} vec\{ \sigma \sigma^{\top} \} \Big\}
\end{aligned}
\end{equation*}
\end{lemma}

\begin{lemma}\label{def:marginal_cross_cov}
The analytic form of the OU marginal covariance at times $s$ and $t$, $s \leq t$, is
\begin{equation*}
\begin{aligned}
\label{eq:marginal_cross_cov}
    Cov\{ \eta(s), \eta(t) \} = vec^{-1} \Big\{ (\theta \oplus \theta)^{-1} \big[ e^{(\theta \oplus \theta)s - (\theta t \oplus \theta s)} \big] vec\{\sigma \sigma^{\top}\} \Big\}
\end{aligned}
\end{equation*}
\end{lemma}

\noindent Here, $\oplus$ denotes the Kronecker sum, defined for square matrices $\bm{A}$ and $\bm{B}$ of sizes $a$ and $b$, respectively, as $\bm{A} \oplus \bm{B} = \bm{A} \otimes \bm{I}_b + \bm{I}_a \otimes \bm{B}$; and the $vec\{ \bm{A} \}$ operation consists of stacking the columns of matrix $\bm{A}$ into a column vector. For details on the derivations of these results, see Section A.1 and A.2 of the supplementary material.

In addition to the marginal covariance function of the OU process, we derive the precision matrix. Due to the Markov property of the OU process, the precision matrix is block tri-diagonal and thus much simpler to calculate than the dense covariance matrix.

\begin{lemma}\label{def:precision_matrix}

Let $\bm{\Omega}$ be the precision matrix of the OU process observed at $n$ occasions and define the stationary variance as $\bm{V} := vec^{-1} \big\{ (\bm{\theta} \oplus \bm{\theta})^{-1} vec\{\bm{\sigma} \bm{\sigma}^{\top}\} \big\}$. Then $\bm{\Omega}$ has the structure
\begin{equation}
\begin{aligned}
\bm{\Omega} = \begin{bmatrix}
            \bm{\Omega}_{11} & \bm{\Omega}_{12} & 0 & \cdots & 0 \\
            \bm{\Omega}_{12}^{\top} & \bm{\Omega}_{22} & \bm{\Omega}_{23} & \cdots & 0 \\
            0 & \bm{\Omega}_{23}^{\top} & \bm{\Omega}_{33} & \ddots & \vdots \\
            \vdots & \vdots & \ddots & \ddots & \bm{\Omega}_{n-1,n} \\
            0 & 0 & \cdots & \bm{\Omega}_{n-1,n}^{\top} & \bm{\Omega}_{nn} \\
         \end{bmatrix}
\end{aligned}
\end{equation}

\noindent and each block indexed by $j$ for $1 < j < n$ in the tri-diagonal matrix is
\begin{equation}
\begin{aligned}
    & \bm{\Omega}_{11} = \big[ \bm{V} - \bm{V}  e^{-\bm{\theta}^{\top} (t_2 - t_1)} \bm{V}^{-1} e^{-\bm{\theta} (t_2 - t_1)} \bm{V} \big]^{-1} \\
    & \bm{\Omega}_{j,j+1} = -\big[\bm{V} - \bm{V} e^{-\bm{\theta}^{\top} (t_{j+1} - t_j)} \bm{V}^{-1} e^{-\bm{\theta} (t_{j+1} - t_j)}  \bm{V} \big]^{-1}  \bm{V} e^{-\bm{\theta}^{\top} (t_{j+1} - t_j)} \bm{V}^{-1} \\
    & \bm{\Omega}_{jj} = \bm{V}^{-1} + \bm{V}^{-1} e^{-\bm{\theta} (t_j - t_{j-1})} \bm{V} \big[\bm{V} - \bm{V} e^{-\bm{\theta}^{\top} (t_j - t_{j-1})} \bm{V}^{-1} e^{-\bm{\theta} (t_j - t_{j-1})} \bm{V} \big]^{-1} \bm{V} e^{-\bm{\theta}^{\top} (t_j - t_{j-1})} \bm{V}^{-1} \\ & \hspace{0.9cm} + \big[\bm{V} - \bm{V} e^{-\bm{\theta}^{\top} (t_{j+1} - t_j)} \bm{V}^{-1} e^{-\bm{\theta} (t_{j+1} - t_j)} \bm{V} \big]^{-1} \bm{V} e^{-\bm{\theta}^{\top} (t_{j+1} - t_j)} \bm{V}^{-1} e^{-\bm{\theta} (t_{j+1} - t_j)} \\
    & \bm{\Omega}_{nn} = \bm{V}^{-1} + \bm{V}^{-1} e^{-\bm{\theta} (t_n - t_{n-1})} \bm{V} \big[\bm{V} - \bm{V} e^{-\bm{\theta}^{\top} (t_n - t_{n-1})} \bm{V}^{-1} e^{-\bm{\theta} (t_n - t_{n-1})} \bm{V} \big]^{-1} \bm{V} e^{-\bm{\theta}^{\top} (t_n - t_{n-1})} \bm{V}^{-1}
\end{aligned}
\end{equation}
\end{lemma}

\noindent The derivation for each block is given in Section A.3 of the supplementary material. Later, during estimation, we take advantage of the sparse precision matrix to simplify computation. This sparsity becomes particularly advantageous as the number of individuals and observations per individual in a dataset increase. 

\subsection{Joint distribution and likelihood}

Together, the measurement and structural submodels imply that the observed longitudinal outcomes are normally distributed with mean 0 and covariance $\bm{\Sigma}_i^* := Var(\bm{Y}_i) = (\bm{I}_{n_i} \otimes \bm{\Lambda}) Var(\bm{\eta}_i) (\bm{I}_{n_i} \otimes \bm{\Lambda})^{\top} + \bm{J}_{n_i} \otimes \bm{\Sigma}_u + \bm{I}_{n_i} \otimes \bm{\Sigma}_{\epsilon}$, where $\bm{I}_{n_i}$ is an $n_i \times n_i$ identity matrix and $\bm{J}_{n_i}$ is an $n_i \times n_i$ matrix of ones.  We estimate the OUF model by minimizing the following function, which equal to twice the negative log-likelihood up to a constant: $-2logL(\bm{Y}) = \sum_{i = 1}^{N} log|\bm{\Sigma}_i^{*}| + \sum_{i = 1}^{N} \bm{Y}_i^{\top} \bm{\Sigma}_i^{*-1} \bm{Y}_i$.

\subsection{Identification issues}\label{ss:identification}

Before fitting our model, we must make additional assumptions to address identifiability issues common to factor models. Because both $\bm{\Lambda}$ and $\bm{\eta}_i(t)$ are unknown, multiplying $\bm{\Lambda}$ by some matrix, say $\bm{A}$, and multiplying $\bm{\eta}_i(t)$ by $\bm{A}^{-1}$ will result in the same model. To make a factor model identifiable, constraints must be placed on either the loadings matrix or the latent factors. \citet{aguilar_2000} and \citet{carvalho_2008}, for example, make the standard assumption of requiring the loadings matrix to be triangular while \citet{tran_2019}, for example, fix the variance of the latent factors at 1. We also fix the scale of the latent factors but propose a novel approach for doing so. Letting $\bm{\eta}_i$ be the $(p \times n_i)$-length vector of latent variables values stacked over measurement occasions, we constrain $Var(\bm{\eta}_i)$ to have diagonal elements equal to 1. This constraint means that the OU process must have a stationary variance equal to 1. By fixing the scale of the latent factors, we can allow the elements of the loadings matrix $\bm{\Lambda}$ to vary almost freely during estimation. For a generic $\bm{\Lambda}$ (without structural zeros), the only constraint on the loadings matrix is that the sign of the first element must be positive.  Together these constraints make our model identifiable; the constraint on the OU process identifies the scale and the constraint on the first element of the loadings matrix identifies the direction.  Because we later make the simplifying assumption that $\bm{\Lambda}$ contains structural zeros with a single non-zero loading per row, flipping the signs on both the loadings and the latent factors results in the same model; we choose to keep the signs that correspond to the most relevant interpretation of the model given the application.  Another constraint could be added to require that one loading per column of $\bm{\Lambda}$ is positive; this would avoid sign flipping.  We revisit our approach to selecting the correct sign later in the context of rescaling the OU process.

To impose this identifiability constraint, we use a set of $p$ constants to re-scale the OU process parameters. We summarize this identifiability constraint for the bivariate OU process as:
\begin{lemma}\label{def:rescale_theta_sigma}
Using a pair of positive scalar constants $c_1$ and $c_2$, we can re-scale an arbitrary OU process parameterized by $\bm{\theta}$ and $\bm{\sigma}$ to have stationary variance of 1, where this re-scaled OU process is parameterized by $\bm{\theta}^*$ and $\bm{\sigma}^*$ according to
\begin{equation}\label{eq:rescale_theta_sigma}
    \begin{bmatrix} \theta^*_{11} & \theta^*_{12} \\ \theta^*_{21} & \theta^*_{22} \end{bmatrix} = \begin{bmatrix} \theta_{11} & \frac{c_1}{c_2} \theta_{12} \\ \frac{c_2}{c_1} \theta_{21} & \theta_{22} \end{bmatrix} \text{  and  }
    \begin{bmatrix} \sigma^*_{11} & 0 \\ 0 & \sigma^*_{22} \end{bmatrix} = \begin{bmatrix} c_1 \sigma_{11} & 0 \\ 0 & c_2 \sigma_{22} \end{bmatrix}
\end{equation}
\end{lemma}


\noindent In Section A.4 of the supplementary material, we show why this re-scaling approach works for any mean-reverting OU process. This constraint can also be extended to OU processes of higher dimensions.

Although this identifiability assumption allows us to identify the magnitude of the loadings in the factor model, it does so only up to a sign change. Consider again the case of a bivariate OU process.  The likelihood for our model is equivalent for pairs of scaling constants $(c_1 = 1, c_2 = 1)$ and $(c_1 = 1, c_2 = -1)$. In practice, the model would be the same under both pairs of scaling constants (and so we restrict $c_1$ and $c_2$ to be positive during estimation) but interpretation of model parameters would differ. After estimation, the signs on estimated model parameters can easily be flipped to match the most relevant interpretation of the data by multiplying estimates of $\bm{\Lambda}$ and $\bm{\theta}$ by a $p \times p$ matrix with the constants along the diagonal.

\subsection{Estimation algorithm}\label{ss:est_approach}

To fit this model, we take an iterative approach to estimation in which we directly maximize the marginal likelihood of our observed longitudinal outcome using a block coordinate descent algorithm and rely on simpler existing models to inform the initial parameter estimates. In the block coordinate descent algorithm, we split parameters into two different blocks: one block for parameters in the measurement submodel ($\bm{\Lambda}$, $\bm{\Sigma}_{u}$, $\bm{\Sigma}_{\epsilon}$) and the other for parameters in the structural submodel ($\bm{\theta}$, $\bm{\sigma}$). Note that each element of these blocks is actually a matrix of parameters.  Within each block-wise iteration, we minimize the log-likelihood with respect to one block of parameters, given the current estimates of the other block of parameters, using Newton algorithms as implemented in \texttt{R}'s \texttt{stats} package \citep{R_stats}. By updating parameters in blocks, we can leverage the availability of analytic gradients for parameters in the measurement submodel. The Kronecker structure of the covariance matrix for each individualâ€™s longitudinal outcomes $\bm{Y}_i$ allows us to derive these analytic gradients. We present the general structure of the gradient here:
\begin{lemma}\label{def:fa_gradients}
The gradient of the log-likelihood for a single individual with respect to one of the measurement submodel parameters, $\Theta_j$, has the general form 
\begin{equation*}
\begin{aligned}
    \frac{\partial logL(\bm{Y}_i)}{\partial\Theta_j} = -\frac{1}{2} \Bigg[ tr \Bigg\{ \Big( I - \bm{\Sigma}_i^{*-1} \bm{Y}_i \bm{Y}_i^{\top} \Big) \Sigma_i^{*-1} \frac{\partial \bm{\Sigma}_i^{*}}{\partial \Theta_j}  \Bigg\} \Bigg]
\end{aligned}
\end{equation*}
where the exact form of $\frac{\partial \bm{\Sigma}_i^{*}}{\partial \Theta_j}$ depends on the specific parameter; either $\lambda_k$, $\sigma_{u_k}$, or $\sigma_{\epsilon_k}$.
\end{lemma}

\noindent The complete set of analytic gradients is given in Section A.5 of the supplementary material.  The computational advantage of using the analytic gradient, as opposed to a numerical approach to differentiation, is particularly notable as the number of longitudinal outcomes---and thus parameters in the measurement submodel---increases.

Prior to maximizing the marginal likelihood, we use a cross-sectional factor model to initialize $\bm{\Lambda}$, $\bm{\theta}$, and $\bm{\sigma}$, and use linear mixed models to initialize $\bm{\Sigma}_u$ and $\bm{\Sigma}_{\epsilon}$. Then, we iteratively update parameter estimates using the following block coordinate descent algorithm:
\begin{enumerate}
    \item \textit{Initialize estimates of $\bm{\Lambda}^{(0)}, \bm{\Sigma}_u^{(0)}, \bm{\Sigma}_{\epsilon}^{(0)}, \bm{\theta}^{(0)}, \bm{\sigma}^{(0)}$. Measurement submodel parameters are always initialized empirically; for structural submodel parameters, two sets of initial estimates are considered---an empirical set of values estimated from cross-sectional factor scores and a default set of values.  The set of values that corresponds to the higher log-likelihood given the current data is used.}
    \item \textit{Set iteration index $r = 1$ and convergence indicator $\delta = 0$. While $\delta = 0$,}
    \begin{enumerate}
        \item \textit{Update block 1 (measurement submodel): $$\bm{\Lambda}^{(r)}, \bm{\Sigma}_u^{(r)}, \bm{\Sigma}_{\epsilon}^{(r)} = \underset{\bm{\Lambda}, \bm{\Sigma}_u, \bm{\Sigma}_{\epsilon}}{argmax}\big\{ logL(\bm{\Lambda}, \bm{\Sigma}_u, \bm{\Sigma}_{\epsilon} | Y; \bm{\theta}^{(r-1)}, \bm{\sigma}^{(r-1))}) \big\}.$$  Maximization is done via a Newton-type algorithm using analytic gradients (Lemma \ref{def:fa_gradients}).}
        \item \textit{Update block 2 (structural submodel): $$\bm{\theta}^{(r)}, \bm{\sigma}^{(r)} = \underset{\bm{\theta}, \bm{\sigma}}{argmax}\big\{ logL(\bm{\theta}, \bm{\sigma} | Y; \bm{\Lambda}^{(r)}, \bm{\Sigma}_u^{(r)}, \bm{\Sigma}_{\epsilon}^{(r)}) \big\}.$$ Maximization is done via a quasi-Newton algorithm using numerical gradients.}
        \item \textit{Using Lemma \ref{def:rescale_theta_sigma}, re-scale OU parameters to satisfy the identifiability constraint.}
        \item \textit{Check for block-wise convergence: Let $\bm{\Theta}$ be a vector containing all elements of $\bm{\Lambda}$, $\bm{\Sigma}_u$, $\bm{\Sigma}_{\epsilon}$, $\bm{\theta}$, and $\bm{\sigma}$. Then, calculate $$\delta = \max \Big\{I\big\{|\bm{\Theta}^{(r)} - \bm{\Theta}^{(r-1)}|/\bm{\Theta}^{(r)} < 10^{-6}\big\}, \ I\big\{logL(\bm{\Theta}^{(r)} | \bm{Y}) - logL(\bm{\Theta}^{(r-1)} | \bm{Y}) < 10^{-6}\big\}\Big\}$$ where all operations on $\bm{\Theta}$ are element-wise.}
        \item \textit{Update} $r$: $r = r+1$
    \end{enumerate}
    \item \textit{Estimate Fisher Information-based standard errors from a numerical approximation to the Hessian of the log-likelihood, $\frac{\partial^2}{\partial\Theta \partial\Theta^{\top}}logL(\bm{\Lambda}^{(r)}, \bm{\Sigma}_u^{(r)}, \bm{\Sigma}_{\epsilon}^{(r)}, \bm{\theta}^{(r)} | Y).$}
\end{enumerate}

Note that when estimating standard errors, the parameterization of the likelihood differs slightly: the likelihood now depends on only one of the parameter matrices in the structural submodel, $\bm{\theta}$, and not the other, $\bm{\sigma}$. This change in parameterization is a result of the identifiability constraint that is placed on the stationary variance of the OU process. Since we are no longer conditioning on fixed measurement submodel parameters in this step, we restrict $\bm{\sigma}$ to be a function of $\bm{\theta}$, where this function is derived from the identifiability constraint; thus, the likelihood is not over-parameterized. Standard error estimates for $\bm{\sigma}$ can be calculated via parametric bootstrap. By sampling values of $\bm{\theta}$ from a Normal distribution defined by its point estimate and estimated covariance matrix, bootstrapped values of $\bm{\sigma}$ are calculated as a function of $\bm{\theta}$ and a confidence interval can be estimated based on the empirical distribution. More details on the parameterization of the log-likelihood for standard error estimation are in Section A.6 of the supplementary material. 

To increase the computational efficiency of this estimation algorithm, we (i) leverage the Markov property of the OU process and use the computationally-simpler sparse precision matrix derived in Lemma \ref{def:precision_matrix} rather than the dense covariance matrix, (ii) take advantage of tractable analytic gradients for the measurement submodel given in Lemma \ref{def:fa_gradients}, avoiding the need to calculate computationally expensive numerical gradients, and (iii) implement portions of our algorithm in C++.

\section{Simulation study}\label{s:sim_study}

\subsection{Data generation}

We conduct a simulation study to assess the bias and variance of estimates produced by our model. We assume that there are $K = 4$ longitudinal outcomes recorded over time for $N = 200$ individuals. For individual $i$, these longitudinal outcomes are measured at $n_i$ different occasions where $n_i \sim Uniform(10, 20)$. The gap times between measurement occasions are drawn from a $Uniform(0.1, 2)$ distribution. We consider simulated data in three different settings in which the true bivariate OU process has varying degrees of autocorrelation (see Section A.8 of the supplementary material for details). Using each true OU process, we generate the observed longitudinal outcomes by drawing from $\bm{Y}_i \sim N(0, \bm{\Sigma}_i^*)$ where $\bm{\Sigma}_i^*$ is defined using
\begin{equation}
\begin{aligned}
    \bm{\Lambda} = \begin{bmatrix}
    1.2 & 0 \\
    1.8 & 0 \\
    0 & -0.4 \\
    0 & 2
    \end{bmatrix} \text{,  }
    \bm{\Sigma}_u = \begin{bmatrix}
    1.1 & 0 & 0 & 0 \\
    0 & 1.3 & 0 & 0 \\
    0 & 0 & 1.4 & 0 \\
    0 & 0 & 0 & 0.9
    \end{bmatrix} \text{, and  }
    \bm{\Sigma}_{\epsilon} = \begin{bmatrix}
    0.6 & 0 & 0 & 0 \\
    0 & 0.5 & 0 & 0 \\
    0 & 0 & 0.4 & 0 \\
    0 & 0 & 0 & 0.7
    \end{bmatrix}.
\end{aligned}
\end{equation}

\noindent When fitting this model, we assume that the structural zeros within the loadings matrix and random intercept covariance matrix are known.

Importantly, some of the parameter values used to generate the data are different from the parameters that will be estimated by the model; this difference is a side-effect of the identifiability assumption. While unbiased estimates of $\bm{\Sigma}_u$ and $\bm{\Sigma}_{\epsilon}$ will match the values used in data generation, the values of $\bm{\Lambda}$ and the OU process parameters $\bm{\theta}$ and $\bm{\sigma}$ will differ. As a result of the re-scaling approach for identification described in Section \ref{ss:identification}, the estimated OU process has a stationary variance of 1. The additional variation present in the OU process during data generation must be absorbed by the loadings matrix $\bm{\Lambda}$. Specifically, the data-generating loadings matrix will be re-scaled according to $\bm{\Lambda} \bm{D}$ where $\bm{D}:= \sqrt{diag\{ V(\bm{\theta}, \bm{\sigma})\}}$ and $\bm{V}$ is the stationary variance of the OU process as defined in Lemma \ref{def:precision_matrix}.  $\bm{\Lambda} \bm{D}$ will be estimated by our algorithm.  The data-generating OU parameters $\bm{\theta}$ and $\bm{\sigma}$ will be re-scaled according to scalar constants chosen such that the stationary variance of the re-scaled OU process is equal to 1 via Lemma \ref{def:rescale_theta_sigma}.  True parameter values indicated in the simulation results have all been re-scaled to match the values targeted by our estimation algorithm. In setting 2, the true OU process used to generate data does have a stationary variance equal to 1 and thus the target parameter values do match the data-generating parameter values.

\subsection{Simulation results}\label{ss:sim_results}

In each of the three settings, we generate 1,000 datasets and carry out the estimation algorithm described in Section \ref{ss:est_approach}. Final point estimates are shown in Figure \ref{fig:point_ests} and information-based standard errors are summarized in Figure \ref{fig:se}. In all settings, we consistently recover unbiased estimates of the true values and find that the average of the standard errors is similar to the empirical standard deviation of the point estimates, indicating that confidence intervals will have close to nominal coverage. In a rare case, numerical issues result in a negative variance estimate; this specific case is discussed in Section A.9 of the supplementary material. 

% Figure environment removed

% Figure environment removed

\subsection{Model selection}\label{ss:model_selection}

We next carry out a simulation study in which we evaluate the ability of Akaike information criterion (AIC) and Bayesian information criterion (BIC) to correctly select the true number of latent factors among the fitted models. Formulas for AIC and BIC are given in Section A.7 of the supplementary material. Assuming the same true measurement submodel parameters as before, we now generate data from five different factor models: a one-factor model, a two-factor model with low signal (i.e., high correlation between latent factors), a two-factor model with high signal (i.e., low correlation between latent factors), a three-factor model with low signal, and a three-factor model with high signal.  Data-generating parameter values are given in the Section A.8 of the supplementary material.  For 100 datasets generated from each of these true models, we fit a one-, two-, and three-factor model and compare fit indices.  We do not consider a four-factor model in this simulation study because our data only contain four longitudinal outcomes and so fitting a four-factor model would no longer fall into the dimension-reduction setting in which factor models are generally used.

We present model selection results in Table \ref{tab:mod_selection}.  In both the high and low signal settings, the model with the lowest AIC and BIC most often has the same number of factors as the true model used to generate the data. For models fit to data generated from a true model with three factors, BIC incorrectly selects a model with two factors more often than AIC. This difference make sense given the increased penalty that BIC places on model complexity. For datasets of this size ($N = 200$), estimation becomes more difficult as the number of factors increases and so our algorithm did not converge for a few simulated datasets (see Section A.9 of the supplementary material for details). 

\begin{table}
    \centering
    % Figure removed
    \caption{For datasets generated under each true model, we summarize the percent of times that the model-selection metric chose the fitted model with the indicated number of factors. When generating data from models with 2 and 3 factors, we considered two different settings: a high signal setting in which latent factors have lower correlation and a low signal setting in which latent factors have high correlation. The settings in which the fitted model has the same number of factors as the true data-generating model are emphasized with bold orange text. These results are presented for datasets on which the algorithm either converged or reached the maximum number of iterations (200) for all three models. See Section A.9 of the supplementary material for more details.}\label{tab:mod_selection} 
\end{table}


\section{Application to mHealth emotion data}\label{s:data_app}

For illustrative purposes, we apply our method to the data on momentary emotions collected in the mHealth study. Using the OUF model, we summarize the longitudinal responses to 18 emotion-related questions as two latent factors interpreted as positive and negative affect. Positive and negative affect are two distinct-but-correlated emotional states known to be key in understanding smoking habits (e.g., \citet{minami_2014}, \citet{langdon_2016}, \citet{leventhal_2013}, \cite{baker_2004}). The proposed model accounts for both the rapid temporal variation in these states and their correlation over time.  In these data, positive affect was measured by how strongly individuals agreed with feeling happy, joyful, enthusiastic, active, calm, determined, grateful, proud, and attentive; negative affect was measured by how strongly individuals agreed with feeling sad, scared, disgusted, angry, shameful, guilty, irritable, lonely, and nervous.

When applying the OUF model, the assumed structural zeros within the loadings matrix result in positive emotions loading onto one of the latent variables, $\bm{\eta}_1(t)$, and negative emotions loading onto the other, $\bm{\eta}_2(t)$. Point estimates and 95\% confidence intervals are in Figure \ref{fig:real_dat_results}. Measures of happiness, joy, and enthusiasm are most strongly correlated with positive affect and measures of sadness and irritability are most strongly correlated with negative affect. We use the estimated parameters of the OU process to understand the latent dynamics of positive and negative affect by plotting the degree of correlation for these two latent variables across varying time intervals between consecutive observations (see Figure \ref{fig:real_dat_autocor}). We see that positive and negative affect are negatively correlated as expected, and that the correlation between the latent states decays slowly.


We also fit a univariate OUF model and a trivariate OUF model and compare these models to the bivariate OUF model.  In the univariate OUF model, all emotions are assumed to be generated from a single common underlying factor; in the trivariate OUF model, we further divide the positive emotions into two latent factors that we call high arousal positive affect (measured by feeling grateful, proud, enthusiastic, active, determined, attentive) and no-to-low arousal positive affect (measured by feeling calm, happy, and joyful), while the negative emotions are still assumed to be generated from one latent factor.  Coefficient estimates from these fitted models are given in Section B.1 of the supplementary material.


Both AIC and BIC indicate that, of the three models considered, the two factor model fits best: $AIC_{\text{1 factor}} = 123,309$ vs. $AIC_{\text{2 factors}} = 121,069$ vs. $AIC_{\text{3 factors}} = 124,957$ and $BIC_{\text{1 factor}} = 123,791$ vs. $BIC_{\text{2 factor}} = 121,577$ vs. $BIC_{\text{3 factor}} = 125,509$.  We provide more details on the calculation of AIC and BIC in Section B.1 of the supplementary material.  Psychological literature supports our conclusion that two factors represent our data better than one as it suggests that positive and negative affect are not opposites, rather they capture distinct-but-correlated components of psychological state (\citealp*{reich_2003}). The lower AIC and BIC of the two factor model compared to the three factor model suggest that the emotions corresponding to high arousal positive affect and no-to-low arousal positive affect are not different enough to justify the additional complexity of the three factor model given the current data.

% Figure environment removed

% Figure environment removed


\section{Discussion}\label{s:discussion}

We developed a dynamic OUF model that combines a factor model to summarize multivariate observed longitudinal outcomes as lower dimensional latent factors and an OU process to describe the temporal evolution of the latent factors in continuous time. By using the OU process, instead of a discrete time approach such as a VAR process, our model can be applied to irregularly-measured ILD commonly produced by mHealth studies. The OU process captures rapid variations in the correlated latent factors over time, in contrast to a multivariate mixed model that is more suitable for capturing smooth trends over time. To fit our model, we use a block coordinate descent algorithm to directly maximize the log-likelihood of the observed multivariate longitudinal outcome.  We derive both the close-form likelihood of the measured outcome and the sparse precision matrix for the multivariate OU process.  Our block coordinate descent algorithm leverages analytic gradients for a subset of parameters to improve computational efficiency. Finally, we applied our method to study the dynamics of emotions among smokers attempting to quit.

Through the marginal distribution of the multivariate OU process, we parameterize our likelihood in terms of the standard OU drift ($\bm{\theta}$) and volatility ($\bm{\sigma}$) parameters. Having estimates for these parameters enables us to understand the dynamics of the latent factors, including generating new trajectories using the estimated values and examining the decay in the trajectories' correlation over time. Through examination of decay in correlation over time, our method could help inform the design of future studies that aim to collect ILD by providing insight into how frequently the longitudinal outcomes must be measured in order to capture the correlation between them.

In our simulation study in Section \ref{s:sim_study}, we generated data under true OU processes that showed reasonably slow decay in correlation over time given the intervals between measurements. We found that estimation of the OU parameters is difficult if correlation decays quickly relative to gaps between measurements. When longitudinal outcomes are measured frequently enough that correlation between consecutive measurements is captured, our method consistently returns unbiased estimates of the OU process parameters. If this method were applied to data in which longitudinal outcomes are not measured often enough to capture the correlation, estimation would be difficult. Like all statistical methods, when enough signal exists in the data, our method works well. It does, however, require studies to be designed such that longitudinal outcomes are measured with sufficient frequency that the correlation between consecutive measurements is captured.

Although we use the sparse OU precision matrix, leverage the availability of analytic gradients for the measurement submodel parameters, and implement a portion of our algorithm in C++, the computation time of our estimation algorithm increases rapidly as the number of longitudinal outcomes increases. We successfully fit our model to a dataset containing 18 longitudinal outcomes but this does require approximately 27 hours. In order to make application of our model to datasets with larger numbers of longitudinal outcomes feasible, computational efficiency must be increased.  However, our proposed marginal likelihood-based method has substantial computational benefits when compared to alternative methods.  In comparison to the Bayesian approach proposed for fitting a similar model in \cite{tran_2021b}, our approach requires less computation time.  In a simulation study with $K = 4$ longitudinal outcomes measured at 10-20 occasions on $N = 200$ individuals, we found that estimation via our block coordinate descent algorithm required approximately 5\% of the time required by the Bayesian approach proposed in \cite{tran_2021b} given the same computing resources.  More details on this comparison are given in Section C.2 of the supplementary material.

Finally, the mHealth dataset to which we applied our method also contains information on demographic characteristics and on the timing of cigarette use. Including baseline covariates in either the measurement or structural submodel would be a useful extension. In behavioral science, specific emotional states, such as negative affect or craving, are expected to be correlated with cigarette use and so future work could involve combining our OUF model with a submodel for event-time outcomes. Our model could also be modified to account for treatment or for drift in the OU process to better capture the dynamics of the latent processes after a key event such as smoking cessation or relapse.


\section*{Acknowledgements}

This work was supported by the National Institutes of Health [grant numbers F31DA057048, P30CA042014, P50DA054039, R01DA039901, R01MD010362, T32CA083654, U01CA229437]; and the Huntsman Cancer Foundation. The National Institutes of Health had no role in the study design, collection, analysis or interpretation of the data, writing the manuscript, or the decision to submit the paper for publication. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health or the Huntsman Cancer Foundation. This work is not peer reviewed. The authors declare no conflicts of interest. 


\section*{Supporting Information}

Supplementary material is available with this paper.  Example code and simulated data are available on Github at https://github.com/madelineabbott/OUF.

\vspace*{-8pt}


\bibliographystyle{biom} 
\bibliography{references.bib}


\label{lastpage}

\end{document}
