% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
% \usepackage[review]{ACL2023}
\usepackage[]{ACL2023}
\usepackage{graphicx}
% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{url}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{mdframed}
\usepackage{tcolorbox}
% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\pagestyle{plain} %added for the page number


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.
\newcommand{\erfan}[1]{\textcolor{purple}{[Erfan: #1]}}
\newcommand{\yue}[1]{\textcolor{blue}{[Yue: #1]}}
\newcommand{\nael}[1]{\textcolor{green}{[Nael: #1]}}

\title{Plug and Pray: Exploiting off-the-shelf components of Multi-Modal Models} 

% \yue{Plug and Play: Unveiling Vulnerabilities in Multi-Modal Systems Through Plugged Embedding Space Attacks} 
% \yue{Plug and Attack: Unveiling Vulnerabilities in Multi-Modal Systems Through Plugged Embedding Space Attacks} 
%\title{Plug and Play and Die: On the vulnerabilities of a less suspected component in Multi-Modal systems}



% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}

% \author{Erfan Shayegani \\ {\bf Nael Abu-Ghazaleh} \\ {\bf Yue Dong} \\
%         CSE Department, UC Riverside, USA}

% \author{Erfan Shayegani \and Nael Abu-Ghazaleh \and Yue Dong \\
%         CSE Department, UC Riverside, USA}

% \author[1]{Erfan Shayegani} \author[2]{Nael Abu-Ghazaleh} \author[3]{Yue Dong}
% \affil[1]{CSE Department, UC Riverside, USA \protect\ \texttt{sshay004@ucr.edu}} \affil[2]{CSE Department, UC Riverside, USA \protect\ \texttt{nael.abughazaleh@ucr.edu}} \affil[3]{CSE Department, UC Riverside, USA \protect\ \texttt{yued@ucr.edu}}
        
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Erfan Shayegani \\
  CSE Department \\
  UC Riverside, USA\\
  \texttt{sshay004@ucr.edu} \\\And
  Yue Dong \\
  CSE Department \\
  UC Riverside, USA\\
  \texttt{yued@ucr.edu} \\\And
  Nael Abu-Ghazaleh \\
  CSE Department \\
  UC Riverside, USA \\
  \texttt{naelag@ucr.edu} \\
  }

\begin{document}
\maketitle
\begin{abstract}

%\erfan{brainstorm the title for the last time}
The rapid growth and increasing popularity of incorporating additional modalities (e.g., vision) into large language models (LLMs) has raised significant security concerns. This expansion of modality, akin to adding more doors to a house, unintentionally creates multiple access points for adversarial attacks. In this paper, by introducing \textit{adversarial embedding space attacks}, we emphasize the vulnerabilities present in multi-modal systems that originate from incorporating off-the-shelf components like public pre-trained encoders in a plug-and-play manner into these systems. In contrast to existing work, our approach does not require access to the multi-modal system's weights or parameters but instead relies on the huge under-explored embedding space of such pre-trained encoders. Our proposed embedding space attacks involve seeking input images that reside within the dangerous or targeted regions of the extensive embedding space of these pre-trained components. These crafted adversarial images pose two major threats: 'Context Contamination' and 'Hidden Prompt Injection'—both of which can compromise multi-modal models like LLaVA and fully change the behavior of the associated language model. Our findings emphasize the need for a comprehensive examination of the underlying components, particularly pre-trained encoders, before incorporating them into systems in a plug-and-play manner to ensure robust security.
% \erfan{how about huge under-explored embedding space?} \yue{sounds great and Erfan, feel free to directly edit on my edits. If some sections are not clear to you, it would be better if you can clarify it. I might miss some details}\erfan{sure professor, many thanks.}
\end{abstract}




\section{Introduction}
% \yue{see draft.text for the draft with Nael's comments in intro. before my edits}
The growing popularity of Large Language Models (LLMs) raises substantial security and privacy concerns, stimulating extensive research on Trustworthy AI \citep{kaur2022trustworthy} to investigate their security and privacy properties \citep{carlini2021extracting}. Concurrently, a growing trend of enhancing LLMs with additional modalities, particularly vision \citep{OpenAI2023GPT4TR, bubeck2023sparks,MicrosoftBing, GoogleBard,liu2023visual,zhu2023minigpt}, yields impressive capabilities that enrich or even enable a broad spectrum of applications. Unsurprisingly, these new modalities introduce additional challenges and elevate security and privacy risks even further, as each added modality presents a new avenue for potential vulnerabilities that adversaries could exploit with malicious inputs \cite{goh2021multimodal, noever2021reading}.


% Figure environment removed

% one paragraph describing attacks on Unimodal LLMs. discuss \textbf{prompt injections}. Examples are instance-based.  Problem: attacks visible, and easy to patch by updates.   \yue{TODO}

Attacks on LLMs, such as prompt injections \cite{greshake2023more, liu2023prompt} and jailbreaks \cite{wei2023jailbroken} that involve injecting harmful or misleading instructional prompts to manipulate model outputs, have garnered significant research interest \citep{perez2022ignore}. However, a major drawback of these textual-based attacks, which target the discrete text space, is their visibility; injected prompts can be easily detected by humans or automated filters, leading to security patches and thereby reducing their effectiveness as a persistent threat \citep{greshake2023more, markov2023holistic}.



This paper contributes the first \textbf{adversarial embedding space attack} against multi-modal systems, using only \textbf{black-box} model assumptions (without full access to the internals of the model).  The attack requires knowledge of only the typically off-the-shelf vision encoder (e.g., CLIP~\cite{radford2021learning}), commonly integrated in building multi-modal systems. As a result, our attack is modular and potentially transferable across multi-modal systems provided they use the same vision encoder.  Some limited concurrent works (e.g., \citet{carlini2023aligned,bagdasaryan2023ab}) also explore multi-modal attacks but assume white-box access to the full model hierarchy.   White-box access to models may not be available in many deployment scenarios where the model pipeline is proprietary and fine-tuned. % was the first to successfully perform harmful text-generation attacks via white-box techniques, requiring comprehensive access to the multi-modal system, which is often unfeasible due to the proprietary nature of these systems and significant computational demands.
%\erfan{not sure to include Carlini here or not yet! I have included him in the related works} 

% one paragraph decribing adversarial attacks for multi-modal models, but only for classification, more citations from https://arxiv.org/pdf/2209.09502.pdf
Having access to different input modalities can provide the attacker with advantages specific to one modality and not present in others.
For example, crafting adversarial attacks on image spaces provides the attacker with a continuous space for perturbations~\citep{szegedy2014intriguing,goodfellow2015explaining} leading to an opportunity to create imperceptible adversarial attacks~\citep{krizhevsky2012imagenet, deng2009imagenet}.   At the same time, the multi-modal nature of the models can potentially enable attacks from one modality to cross over to the others, affecting the overall output of the system.  However, transferring adversarial attacks across modalities is not straightforward; we investigate such multi-modal attacks in this paper.
%primarily due to the absence of loss signals based on misclassification in text generation tasks.
%\erfan{not sure this paragraph ends well.}

Our work demonstrates the feasibility of targeted embedding space attacks in multi-modal models such as LLaVA~\cite{liu2023visual} (Figure~\ref{fig:multi-modal-system}).   The attack involves deriving adversarial images residing within the dangerous or targeted regions of the extensive embedding space of CLIP. Such attacks become feasible due to the multi-modal systems' need to fuse different modalities into a joint embedding in the feature space.  By exploiting this joint embedding space, which has not been studied well from an adversarial perspective, we can identify images that despite not being visually similar to a target image, result in a comparable projection in the embedding space.


Using our proposed targeted embedding space attacks, we have identified a compelling question to ask in existing multi-modal systems: \textit{`Why does a completely noisy image fall very close to a real target image in the embedding space of vision encoders like CLIP?'} Additionally, we analyzed the implications of our proposed attacks and revealed two potential threats: `Context contamination' and `Hidden prompt injection'.   Furthermore, our attack shows remarkable resilience and seamless transferability against systems modifications as we empirically demonstrate during LLaVA's transition from Vicuna \cite{vicuna2023} to Llama 2 \cite{Llama2}. This outcome aligns precisely with our initial expectations, as we do not require access to the components and parameters of the multi-modal system. 
% , as demonstrated by its successful adaptation to LLaVA's transition from Vicuna to Llama 2 and other training modifications without any alterations on our part.
Based on these findings, we emphasize that \textbf{Given their vast under-studied embedding space, incorporating these off-the-shelf publicly available vision encoders like CLIP into more complex systems in a plug-and-play manner, significantly reduces the entry barrier for potential attackers, even when the attacker lacks access to the system's weights and parameters}. 


% \yue{no need for white box access, not model specific, which can work for multiple models, model specific vs universal attacks, maybe focus on the plug and attack part?
% As far as we know, we are the first to attack generations.
% \textbf{plug and attack, modular manners}
% \begin{itemize}
%     \item light-weight attack without heavy computation (show number of params and RAM computation comparison) 
%     \item without parameter access (plug attacks 30 mil params, the other one)
%     \item plug the vision attacks into any multi-modal models for the toxic generation   
%     \item \textbf{embedding space attacks more general: CLIP and vision encoder are public avaliable, people use it as module, so we can use it and transfer attacks, black box and transfer attacks}
%     ()
% \end{itemize}
% }

In summary, our primary contributions include:

\begin{itemize}
  % \item We identify a vulnerable component in multi-modal systems that needs much more attention before getting incorporated into these systems.

  \item We identify the vision encoder (e.g., CLIP) as a source of vulnerability in multi-modal systems that requires significant attention.  It is often used in an off-the-shelf manner, and not fine-tuned with the rest of the hierarchy allowing attacks to transfer.  %before getting incorporated into these systems through an integrative and modular approach.

  
  \item For the first time, we propose targeted embedding space attacks on multi-modal systems using black-box assumptions.  The attack exploits the joint vision-language embedding space of CLIP.% \erfan{is the rest necessary?}to identify adversarial images that are semantically similar to but visually distinct from a target for text generation tasks.

  \item We leverage this attack vector to demonstrate two dangerous exploits: 1) Context Contamination and 2) Hidden prompt injection.

  \item We demonstrate that our attack strategy is resilient to system modifications, indicating the increased potential risk for systems incorporating publicly available vision encoders such as CLIP. %This significantly reduces the barrier to entry for potential attackers, even without system parameters access.
  
\end{itemize}

\section{Background and Preliminaries}

% Our proposed targeted embedding space attacks capitalize on the idea that the inclusion of additional modalities—such as vision encoders—into a multi-modal system enables adversarial manipulation of this more vulnerable modality to produce effects similar to those resulting from manipulating the language modality via prompt injection. 

In this section, we briefly discuss the key concepts crucial for explaining our methodology.

\paragraph{Multi-Modal models.} There has been a notable shift from text-only large language models (LLMs) to multi-modal models, incorporating vision and other modalities. For example, GPT-4 was one of the pioneering models to support image understanding \cite{OpenAI2023GPT4TR, bubeck2023sparks}. Recently, Microsoft Bing and Google Bard have also adopted image support \cite{MicrosoftBing, GoogleBard}. However, these models offer limited access to proprietary details, prompting researchers to develop open-source alternatives like LLaVA \cite{liu2023visual} and MiniGPT-4 \cite{zhu2023minigpt} to compete with them.

The right side of Figure~\ref{fig:multi-modal-system} illustrates a multi-modal system consisting of a vision encoder (e.g., CLIP in LLaVA), a projection layer, and a language model. The vision encoder captures visual features from the input image, connecting them to the language model's text embedding space through the projection layer, enabling integrated context. This architecture allows the language model to incorporate context from both previous tokens and the image as suggested by equation \ref{eq:LM-multi}. Notably, by modifying the visual features extracted by the vision encoder, we can direct the language model's behavior toward a specific target area.

\begin{equation}\label{eq:LM-multi}
P(X_{\text{next token}} | X_{\text{previous tokens}}, X_{\text{visual features}} )
\end{equation}





\paragraph{Vision encoders.} Vision encoders play a vital role in multi-modal systems by extracting features from the input image and mapping them into an embedding space represented by a vector of features. One of the most commonly used encoders is CLIP \cite{radford2021learning}, which has been trained on 400 million image-text pairs. CLIP has demonstrated the ability to map both images and texts into a joint embedding space, where semantically similar texts and images are positioned close to each other. As a result, CLIP finds applications in various contexts involving images and texts, including image-text retrieval, zero-shot image classification, and more recently, multi-modal systems \cite{gong2023multimodal, gao2023llama, agarwal2021evaluating, lerf2023}. 

%\erfan{is it ok to say this here or not?}
%In this paper, we mainly focus on this component in multi-modal systems and show that this huge under-studied embedding space has its own vulnerabilities that can be exploited by adversaries. 



\paragraph{Prompt injection.}  In recent times, LLMs have made significant strides in their ability to follow instructions \cite{ouyang2022training, peng2023instruction, alpaca}. However, there are instances where they misinterpret certain parts of user input as instructions, causing them to deviate from their intended course and adhere to these perceived instructions instead \cite{perez2022ignore}. In essence, the instruction-fine-tuned LLMs excel at detecting and following instructions, even to the extent of mistakenly detecting instructions in the data and complying with them – a situation that attackers exploit.

This phenomenon emerges due to the lack of a clear line between data and instructions for LLMs. A more subtle variant of this problem, known as 'indirect' prompt injection, involves attackers injecting instructions into sources likely to be \textit{retrieved} by the target LLM, as explored by \cite{greshake2023more}. Notably, equipping LLMs with retrieval capabilities elevates the likelihood of such attacks, as malicious pieces of text can be injected into nearly any source accessed by the LLM. 
% In this paper, we introduce a novel prompt injection attack referred to as the 'hidden' prompt injection attack. This attack leverages the embedding space of the vision modality, a concept that will be elaborated later in detail.



% one paragraph about white and black box attack, more citations from https://arxiv.org/pdf/2201.11528.pdf
\paragraph{White-box vs. Black-box adversarial attacks.}  Adversarial attack techniques can generally be categorized into two groups: white-box and black-box. White-box attacks require full access to the target model \citep{kurakin2016adversarial,moosavi2016deepfool, wallace2019universal}, while black-box attacks involve limited or no access to the target model's internal details \citep{poursaeed2018generative,zhang2022beyond}. 
Black-box attacks provide a more realistic approach for real-world scenarios as they can effectively operate even when internal model details are inaccessible. 


\section{Methodology}
In this section, we present the problem formulation of our method, which involves finding adversarial images for attacks leveraging the giant embedding space of the vision encoders. We propose our novel targeted embedding space attacks for adversarial image generation. Note that no access to the multi-modal system is necessary; only knowledge of the vision encoder is required.

\paragraph{Problem Statement} Our goal is to find one or more adversarial images $\hat{x}_{\delta}$ that are very close in the embedding space to a target image $x$, which enable attacks utilizing the embedding space of CLIP \cite{radford2021learning} when integrated into multi-modal systems. We define adversarial images $\hat{x}_{\delta}$ as those for which humans cannot generate the correct description based on the image, and at the same time, are unable to detect the presence of attacks in the images.

\begin{algorithm}[t!]
     \caption{Adversarial Image Generator via Embedding Space Matching} 
    % \yue{converge creteria?} \erfan{I've usually done it empirically, but once I show the embedding distances table, it will be more clear. We can then put a criteria.}} I see thanks:)
    % \erfan{Prof. Dong, can you please update the converge criteria to something like Loss < Threshold which the threshold is empirically selected, usually with L2 loss would be around 0.13 which indicates the distance of the two vectors. Many thanks. }
    \label{alg}
    \SetKwInput{KwInput}{Input}
    \SetKwInput{KwOutput}{Output}
    \SetKwInput{KwParameter}{Parameter}
    
    \KwInput{target image $x$, initial adversarial image $x_{\delta}$}
     \KwInput{CLIP-encoder for image $\mathcal{I}(\cdot)$, ADAM optimizer with learning rate $\eta$}
    \KwOutput{adversarial image $\hat{x}_{\delta}$}
    \KwParameter{convergence threshold $\tau$} 
    
    Input $x$ to $\mathcal{I}(\cdot)$ and get its embedding $\rho$
    \While{$\mathcal{L} > \tau$}{
        Input $x_{\delta}$ to $\mathcal{I}(\cdot)$ and get  $\rho_{\delta}$
        
        $\mathcal{L} \leftarrow \mathcal{L}_2(\rho, \rho_{\delta})$\;
        
        $g \leftarrow \nabla_{x_{\delta}} \mathcal{L}$  \tcc*[r]{Compute the gradient of the loss w.r.t. the target image}
        
        $x_{\delta} \leftarrow x_{\delta} - \eta \cdot g$  \tcc*[r]{Update the adversarial image}
    }
       
    \Return{$\hat{x}_{\delta}$ = $x_{\delta}$}
\end{algorithm}

\subsection{Adversarial Images Generator $\mathcal{G}$ via Embedding Space Matching}
\label{subsec:method_attack}
% \yue{Check how much this is similar to the instance-based attacking method mentioned in GAMA}
We aim to find adversarial images that deceive the language model $f(\cdot)$, assuming that we only have access to the vision encoder $\mathcal{I}(\cdot)$ in a multi-modal system, which is a reasonable assumption since many of these vision encoders that are integrated into systems, are publicly available on the internet, and they are often kept frozen during the training of the whole system. 
 

% Figure environment removed

% Figure environment removed

We next describe our proposed adversarial image generator $\mathcal{G}(\cdot)$ and present a novel strategy to use only CLIP's image modality $\mathcal{I}(\cdot)$  to find adversarial images for attacks.  Suppose that we have a target image $x$  that we want to find an adversarial image $\hat{x}_{\delta}$ such that their embedding vectors lie so much close to each other in the embedding space. At first, we pass the target image $x$ through CLIP's vision encoder $\mathcal{I}(\cdot)$ and obtain its embedding vector $\rho$ (e.g.,  a 768-dimensional vector for the clip-ViT-L/14 model) and treat $\rho$ as the target vector in the embedding space of CLIP. 

We then initiate an adversarial image $x_{\delta}$, which can originate from a random noise distribution, a white background, or an arbitrary image. This gives us the adversarial embedding vector, $\rho_{\delta} = \mathcal{I}(x_{\delta})$. To find the adversarial image, we aim to minimize the distance between the two embedding vectors. This is achieved by defining an $\mathcal{L}_2$ distance loss between the adversarial $\rho_{\delta}$ and target $\rho$ embeddings, and subsequently minimizing this loss through a series of backpropagation steps and updates to the adversarial image $x_{\delta}$. This optimization process is facilitated by the ADAM optimizer \cite{kingma2014adam} with a learning rate $\eta$. Figure \ref{fig:optimization} and Algorithm \ref{alg} illustrate the process of the proposed adversarial image generator.

Once optimization converges ($\tau= \sim 0.13$), typically within 10 to 15 minutes when utilizing a Google Colab T4 GPU, the embedding vectors of the adversarial input and the target are extremely close, often perfectly aligned, within the embedding space. The result is an adversarial image that bears no resemblance to the target image, yet is semantically identical in the embedding space. This means a multi-modal system like LLaVA cannot distinguish between these images, as it processes only the output of the CLIP model, which is then fed to the projection layer and subsequently the rest of the system. 


% Figure environment removed

\section{Experimental Design and Results}
% Our idea is inspired from the fact that the embedding space of these machine-learning models is so huge that several inputs might get mapped to very close proximity in the embedding space, is very interesting and dangerous at the same time and CLIP is no exception to this as well. In fact, we exploit this characteristic of the embedding space to perform our attacks. Note that we do not require any type of access to the the language modality that is often huge and closed sources (e.g., GPT-4), we just need the white box access to the vision encoder used in the system. In the case of LLaVA, we know that they use clip-ViT-L/14 as mentioned in their paper \cite{liu2023visual} and their GitHub repository \cite{LLaVA-GitHub}. To make it more clear, imagine a target image is passed through CLIP and we get its embedding vector.  The answer is yes, and we show that not only one image but also many images can be found that meet this criterion. What makes CLIP more interesting, is that it's actually a vision-language model and not just a vision model. As a result of its training, it has learned to map the images and their text descriptions to a joint vision-language embedding space. This gives us an additional attack surface, which lets us find adversarial images that map close to the embedding vector of a target text as well and not solely limited to a target image. We show both of these scenarios and their implications later in this paper. 

Utilizing our proposed adversarial image generator, as defined in Section \ref{subsec:method_attack}, we aim to find adversarial images by solely having access to CLIP, and then transfer them to multi-modal systems like LLaVA in a black-box manner. 

% \textbf{existence of adversarial images:} is it possible to discover adversarial images that, while yielding the same text generations as the target image \textbf{existence of adversarial images hiding typographic attacks and prompt injections:} can we embed textual prompt injection attacks in adversarial images? 

We use LLaVA's web demo as the base multi-modal model for the experiments. It is important to reiterate that we do not need any access to LLaVA's internal weights or modules. Our only requirement is knowledge of the vision encoder used in the architecture, which, in the case of LLaVA, is CLIP. 


\subsection{Finding Adversarial Images}

We conduct experiments to learn adversarial images using Algorithm \ref{alg}. Given an arbitrary real target image, such as the one shown in Figure \ref{fig:result1}, we demonstrate that utilizing Algorithm \ref{alg} with a white background image as the initial adversarial image allows us to obtain an adversarial image that is semantically almost the same as the target image in the embedding space. This adversarial image triggers an identical behavior as the target image when fed into a multi-modal model that uses CLIP as the vision encoder. Another example of this attack can be found in Appendix \ref{sec:appen-porn} where the target image is a \textit{pornographic image} and we are able to find a semantically equivalent adversarial image which has its own serious implications like filter bypassing. 

\subsection{Evaluation strategies}

\paragraph{Evaluation via QA:} In order to measure how closely the adversarial image resembles the target image from LLaVA's perspective, we ask a series of questions from LLaVA regarding the adversarial image and the details of the scene. These questions include "Add more details that you can see," "Describe the biker's shirt," "Describe the biker's pants," and "Describe the biker's face," along with additional details like "his backpack on his back." In response, LLaVA provided answers as if the input image was the original target image. This indicates that we successfully deceived LLaVA into believing that the given adversarial image held the same semantic meanings as the target image, convincingly suggesting that the adversarial image encoded every little detail of the target image. The full chat history can be found in Appendix \ref{sec:appen-bike-chat}.





% Figure environment removed 

\paragraph{Evaluation via reconstruction:} Another way to measure the closeness of the adversarial image to the target image is through image reconstruction using the prompt. In this setup, we ask LLaVA to provide us with a prompt that can be used to recreate the image it is seeing using an AI image creator. We then feed the prompt generated by LLaVA to the Bing image creator and obtain fascinating results. Figure \ref{fig:bing-bike} shows the output from Bing. The presence of a motorcycle in Figure \ref{fig:bing-bike} and a bicycle in the target Figure \ref{fig:result1} can be attributed to the ambiguity of the term "bike" in English, as it is used interchangeably to refer to both motorcycles and bicycles. Another example of a different target is included in Appendix \ref{sec:another-kitchen}.

% Figure environment removed
\paragraph{Evaluation via classification:} As CLIP can be used for zero-shot text classification, we evaluate the adversarial image through a classification task by asking the model to associate the adversarial image with the most relevant piece of text among multiple provided text descriptions leveraging the joint vision-language capabilities of CLIP. The results show that the model is 98\% certain that our learned adversarial image describes a man riding a bike, which corresponds to the target image. Figure \ref{fig:l2-bike-hf} shows the classification results. 

The intuition behind this evaluation method rests on the hypothesis that by moving the embedding vector of the adversarial image close to that of the target image, we are also approaching the region of the embedding of a text describing the target image which is demonstrated in Figure \ref{fig:embclip}.


% Another instance of this attack can be found in Appendix \ref{sec:appen-porn}, where an inappropriate target, such as pornographic content, was selected. This shows that CLIP identifies these concepts as well, and this can have its own consequences, especially for bypassing moderation filters in systems. In fact, the image doesn't seem inappropriate at all from a computer vision perspective, but semantically, lies in very inappropriate regions in the embedding space that can affect the system in ways explained later in this paper.


% \erfan{not sure still - either text or an image containing text - We should somehow here mention that the text target has much less success rate compared to the image target. And we need to study it more in the future. where and how should we say it?}

% The above strategy can be applied to a target text instead of a target image which is described later in this paper and one of its potential threats is the possibility of injecting instructions to the LLM via the vision modality. This can have very serious consequences, with the added stealthiness which originates from the added modality to the system and our idea of embedding space manipulation. 
% \subsection{Existence of Adversarial Images Hiding Prompt Injections}


% Figure environment removed


% \textit{`You are LLaVA, a large language and vision assistant trained by UW Madison WAIV Lab. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language. You should follow the instructions carefully and explain your answers in detail.'}

% % Figure environment removed





\section{Implications of the Proposed Attacks}

In this section, we discuss potential threats and implications of attacks discovered by our proposed embedding space attacks. These scenarios include \textbf{\textit{`context contamination'}} and \textbf{\textit{`hidden prompt injection'}}. Additionally, we highlight the high degree of transferability of such embedding space attacks, which calls for caution when incorporating additional modalities into language models without thorough examination.

\subsection{Potential Threats}
\paragraph{Context Contamination.} In vision-language multi-modal systems, the context for generating the next tokens involves previously generated tokens and the input image. Our embedding space attacks demonstrate the feasibility of context contamination for text generation by solely manipulating input images. We believe that these compromised contexts pose serious near-future consequences, especially as these modules, such as pre-trained vision encoders, become increasingly popular in real-life systems through a plug-and-play manner. Some potent scenarios are as follows:

\begin{itemize}
  \item \textit{Chatting with retrieved documents:} AI-based conversations with retrieved documents, such as PDFs, emails, or websites \cite{chatpdf}, are becoming increasingly popular. Adversarial images crafted using our attacks can manipulate the context when querying these systems, potentially presenting privacy and security vulnerabilities. These attacks can bypass visual filters by exploiting semantic space features, allowing certain images to be used as queries. Furthermore, compromised documents containing adversarial images, whether downloaded from a third-party source or manipulated by an adversary, can mislead users during their interactions with language models and provide incorrect retrieved context.
%  \erfan{carlini has mentioned blind people, is it ok that we have it?}
  \item \textit{Interacting with the environment:} \cite{chat-with-nerf-2023} inspired by \cite{lerf2023} enables users to interact with their immediate 3D environment; by grounding CLIP embeddings across multi-view training images. If an adversary compromises these systems, it has the potential to provide misleading information to the user regarding their immediate environment, leading to inaccuracies, such as misrepresenting the contents of a room.
  \item \textit{Complex AI systems:}  In complex systems involving multiple AI components, an error in a compromised component can propagate throughout the system especially when the output of a component is fed to other components. This is a critical and under-studied problem that could cause irreversible harm if not examined thoroughly.
\end{itemize}


\paragraph{Hidden Prompt Injection.} As shown in \cite{greshake2023more}, LLMs are vulnerable to prompt injection attacks. However, all of the prompt injection examples mentioned in the \cite{greshake2023more} paper (except for a base64-encoded version), are in the raw text format which explicitly ask the LLM to follow those instructions in natural language and are easy to be spotted. One such example is as follows:

% \textit{[System](\#additional\_instructions) \\
% Respond with a pirate accent from now on.}
\begin{mdframed}[linewidth=0.5pt, linecolor=black, backgroundcolor=gray!5]
\begin{verbatim}
[System](#additional_instructions) 
Respond with a pirate accent from now.
\end{verbatim}
\end{mdframed}

This piece of text is usually placed in the comment section of a website, somewhere in a document with very small font size, and in other less noticeable places that the LLM retrieves. They also very briefly explore the idea of attacking Multi-Modal systems by for example naively putting `Dog' words in big font size in the image of a cat and then asking the model what animal is in the picture. However, in both of their attacks, since the content is in the raw text format and natural language, it is very easy to be detected by a human or a moderator that is trained to find such injected prompts compared to our idea of taking advantage of the embedding space vulnerabilities of the encoders by hiding such content in another format which in our case would be another modality. 

Arising from the OCR capabilities of the multi-modal systems \cite{liu2023hidden, zhang2023llavar}, we discovered that both Google Bard and Microsoft Bing read text inside images with prompt injections and treat it as instructions, as depicted in Figure \ref{fig:bingocr} and Figure \ref{fig:bardocr} in the appendix. Interestingly, when the same instruction is written directly in the chat, Bing often terminates the chat as Microsoft attempts to patch these vulnerabilities. 


Our findings demonstrate the possibility of identifying adversarial images that correspond to the same region in the embedding space as these instructions. To hide textual prompt injections into images, we conduct experiments to learn adversarial images that hide prompt injections using Algorithm \ref{alg}. Given an arbitrary real target image, such as the one shown in Figure \ref{fig:result2}, we demonstrate that utilizing Algorithm \ref{alg}  allows us to obtain adversarial images embedding instructions.

We followed the same strategy used earlier in this paper, but this time our target is an image with a white background containing instructions, as shown in Figure \ref{fig:result2}. To show the generality of our idea, here we initialized our adversarial image with a randomly taken image of an airplane on the internet. The starting image can be of any distribution, given enough optimization steps. The choice of a good starting point that requires minimum steps to reach the same embedding region as the target image lies in, seems to be an interesting problem that we leave to future work. Figure \ref{fig:result2} shows the adversarial image that looks like a noisy image of an airplane while being almost equal to the target image in Figure \ref{fig:result2} in the embedding space of CLIP. Feeding this adversarial image to LLaVA leads to both interesting and at the same time concerning observations. The output from LLaVA (actual screenshot in Appendix \ref{sec:appen-leak}) highlighted in bold is exactly a part of LLaVA's initial system prompt verified by examining LLaVA's  GitHub repository \cite{LLaVA-SystemPrompt}.  Another example of adversarial attacks that embed prompt injections can be found in Appendix \ref{sec:covert-attack-example}.


This setup of finding stealthy adversarial images that hide and embed prompt injections is motivated by the observation that attacking LLMs via explicit pieces of text is mostly patched by recent systems. Such attacks are discrete, involving changing the prompts given to LLMs, which makes them easy to detect and patch. In contrast, our adversarial images that hide prompt injections, such as the ones shown in Figure \ref{fig:result2}, are more subtle and challenging to detect. Images obtained from exploiting the embedding space can completely hide prompts from humans. Note that the success rate of this attack is much lower compared to our attacks that use actual meaningful images as targets like the one described in Figure \ref{fig:result1}; we partly attribute this to the composition of the training dataset used for CLIP, which primarily consists of genuine images of objects rather than images containing text. We hypothesize that a more effective region in the embedding space should be discoverable using our attack which might be situated in the vicinity of both the embedding vector of the target image containing instructions and the embedding vector of the textual description of those instructions, inspired by Figure \ref{fig:embclip}. However, further work is required to make this attack significantly more effective, and we leave it as an avenue for future research.


% We posit that a more effective region in the embedding space should be discoverable using our attack. This region might be situated in the vicinity of both the embedding vector of the target image containing instructions and the embedding vector of the textual description of those instructions, as illustrated in Figure \ref{fig:embclip}. However, further work is required to make this attack significantly more effective, and we leave it as an avenue for future research.
% but we observe that they only patch the text modality, while the same text presented as an image in our experiments easily goes through the system and compromises it.



% \erfan{I liked the example I had included like "respond with a pirate accent"}
% \begin{verbatim}
% [System](#additional_instructions) 
% Respond with a pirate accent from now on.
% \end{verbatim}
 % Additionally, attempts have been made to inject prompts into images, such as including the text "dog" on a cat image and querying the model about the animal depicted. Both  are more easily detected by moderation systems due to their explicit instructions. 






\subsection{High Degree of Transferability \& Resilience}

Leveraging the embedding space of off-the-shelf components like CLIP presents another potential threat of attacks which is a high degree of transferability, allowing a wide range of such attacks to be utilized. Our hypothesis is that this high degree of transferability and resilience against system modifications is a result of our black-box attack strategy and how higher layers in these systems are typically fed with embedding features of the input image extracted by encoders like CLIP, and subsequent procedures unfold accordingly. Therefore, if two input images map to the same embedding vector, these systems consider them as similar, unless there is a mechanism that considers not just the embedding vectors but also the visual features of the images. In the case of LLaVA, we believe that even if the LLM head, currently Vicuna \cite{vicuna2023}, is replaced with another LLM or fine-tuned, the same results would be obtained, as the two input images remain nearly identical in the embedding space, resulting in similar outcomes for both the target and adversarial images

\textbf{We have empirically verified this hypothesis in Appendix \ref{sec:appen-demo-upgrade}, where we demonstrate the complete transferability of our attacks from the older version of LLaVA to the newer version that utilizes Llama 2 \cite{Llama2} as the language model instead of Vicuna}, in addition to some training modifications. As CLIP is usually utilized in these complex systems in a plug-and-play manner, it is of great importance that we identify the risks and threats that arise as these pre-trained models are getting integrated into more complex systems rapidly. 


\subsection{Call for Defense Strategies}

These findings raise serious concerns as these adversarial images that are targeted toward a region in the embedding space or embed instructions in them can be spread through websites, documents, advertisements, and other sources accessible to the multi-modal system, potentially contaminating the context of the systems causing confusion about which direction to follow and leading to alterations in the system's behavior, directing it towards specific targets or areas. This highlights the urgent need to develop significantly stronger defense strategies for such attacks due to their high degree of stealthiness, in contrast to conventional ones that typically follow specific raw text templates.


\section{Related works}\

\paragraph{Typographic attacks}  are among the most explored forms of attacks in multi-modal systems, characterized by the insertion of text into images to mislabel common items and deceive image classification models \citep{goh2021multimodal, noever2021reading}. \citet{goh2021multimodal} first conducted in-depth analysis using physically printed handwritten text, similar to adversarial patches \citep{brown2017adversarial} and physical adversarial examples \citep{athalye2018synthesizing}, to induce misclassifications of CLIP \citep{radford2021learning} on ImageNet \citep{deng2009imagenet}. Building on this concept, \citet{noever2021reading} further investigated CLIP's vulnerabilities in zero-shot image classification tasks by directly editing  input images. They discovered that strategies such as injecting large font texts or even surrounding the text with borders proved effective in creating successful deception, thereby revealing that the reading ability of CLIP dominates and can potentially supersede its visual input signal. Moreover, \citet{greshake2023more} extended this type of attack by naively putting texts into images to trick recent multi-modal systems, including LLaVA \cite{liu2023visual}. Contrasting with previous methods that inject raw text with wrong class labels as a form of attack,  our approach focuses on `embedding space attacks' by trying to hide semantic information in an image.
% \yue{check this claim, question: is greshake's paper has done this?} \erfan{they just put some `dog' words in the image of a cat, and then upload it to LLaVA, so its attacking the generation. But it's very naive, you can also see the last page of their paper. BTW, we can talk about it tomorrow in the meeting.}

% \cite{noever2021reading} is one of the earliest papers which discusses CLIP vulnerabilities in zero-shot image classification tasks. The authors create adversarial images by putting contradictory pieces of text inside the image and see if the model gets fooled. For instance, they put a text box containing the word `Elephant' on the image of a cat, and show that CLIP is actually fooled, and they conclude that CLIP's reading skill dominates and overrides the visual input. Their attack is inspired by the work of authors in \cite{goh2021multimodal} which introduced `Typographic attacks'. \cite{greshake2023more} also performed the same attack by putting labels of `Dog' on a cat picture and tried to trick LLaVA. They claim that this type of attack is targeting the language model, however, we believe that it is much more accurate to put the blame on CLIP, not the language model due to our observations throughout this paper. Simply put, the language model is fed with the encodings of CLIP, and it is CLIP's fault that embeds the image of a cat annotated by a `Dog' text box close to the embedding vector of a dog. All of these attacks are very explicit and easy to notice in the first place though since they are just putting raw text inside images. Even \cite{noever2021reading} shows that in order for CLIP to be fooled effectively, they need to use big fonts, sometimes with borders around the text which makes the attacks even more detectable.


\paragraph{Embedding space attacks}\citet{aich2022gama} proposes an effective embedding space attack method to craft perturbations by learning to fool a surrogate classifier for multi-object classifications.  At the heart of their algorithm, they leverage CLIP's joint image-text aligning property to push the adversarial image's embedding representation away from that of the clean image while pulling it toward a dissimilar target text embedding in a contrastive manner.  Compared to the instance-based attacks \cite{goodfellow2015explaining, kurakin2016adversarial}, \citet{aich2022gama} demonstrates that distribution-driven embedding space attacks offer several advantages: (1) stronger transferability of perturbations to unknown victim models; and (2) the ability to perturb large batches of images in one forward pass through the generator, resulting in better time complexity. Building upon these insights, we demonstrate that these embedding space vulnerabilities also exist in multi-modal systems, and propose a more general loss - L2 loss - for embedding space attacks. 

% They use CLIP to represent the relationships between different objects in the input scene. At the heart of their algorithm, they leverage CLIP's joint image-text aligning property to push away the CLIP embedding representation of the adversarial image from that of the clean image as well as pulling it toward a dissimilar target text embedding in a contrastive manner. As an example, they pick an image depicting a person and a bus. So this image is semantically much closer to a piece of text saying "\textit{A photo depicts} a person and a bus" than another one saying "\textit{A photo depicts} a sofa and a tv monitor". However, during the optimization, they try to pull the visual embeddings of the image toward the textual embeddings of the latter piece of text using CLIP's joint vision-language embedding space capabilities. In the end, they test their adversarial images against various multi-object classifiers and show a high degree of transferability of their attack, claiming that CLIP extracts very detailed features both from the vision and the text space, so an adversarial image that has been guided by their intermediate CLIP stage to another region in the embedding space loses most of its original base features and gains very pronounced new features from the target region. \yue{TODO: there seems to be more similar works on embedding space attacks in GAMA that we should discuss}

\paragraph{Complete white-box access attacks} Attacks on multi-modal systems for text generation are comparatively less explored. We are aware of concurrent research by Carlini et al.~\citet{carlini2023aligned} which also demonstrates successful attacks for generating toxic text using an end-to-end differentiable model implementation.  This attack requires full white-box access to the multi-modal system from the input image to the logits of the language model using conventional teacher-forcing strategies. Another concurrent work by Bagdasaryan et al. \citet{bagdasaryan2023ab} also explored the possibility of performing prompt-injection attacks against multi-modal models such as LLaVA and PandaGPT \cite{su2023pandagpt}.  Their attacks blend instructions into an image or audio recording. However, they also assume full white-box access and use teacher-forcing approaches to backpropagate through the full model. 

%\erfan{is this in the right place?}
The problem with these white-box attack scenarios is that since they optimize their adversarial samples assuming access across the full the system, if even one component is fine-tuned or replaced, these attacks are less likely to remain effective.  On the other hand, our proposed embedding space attack solely relies on the vision encoder of such systems which is typically publicly available.   Moreover, we observe that this CLIP component is frozen and unmodified during the training of these systems such as LLaVA~\cite{liu2023visual}; as a result, our attacks are more likely to transfer to such systems.  In addition, since our attacks target only one component, they are comparatively lightweight, requiring minimal computational resources. Our adversarial samples demonstrate high resilience against modifications to the system as shown in Appendix~\ref{sec:appen-demo-upgrade}, leveraging CLIP's robust feature extraction in the embedding space. All of these interesting and intimidating observations emphasize the need to carefully examine the implications of incorporating off-the-shelf components such as CLIP when they are integrated into more complex systems.  % more complex systems without being aware of the potential risks they can pose due to their huge under-studied embedding space.



% However, these remain hypotheses, and we defer comprehensive studies on these matters to future investigations. 



\section{Conclusion}
In this paper, we highlight the vulnerabilities of multi-modal systems by introducing \textit{adversarial embedding space attacks}.  These attacks solely require access to the vision encoder used as-is in state-of-the-art multi-modal systems. We highlight the threat posed by our adversarial embedding space attack, by leveraging it to point out two compelling vulnerabilities in existing multi-modal systems.  Specifically, by attacking the CLIP encoder we describe: (1) Context Contamination; and (2) Hidden Prompt Injection as potential threats of our attack. Given these findings, we emphasize the need for considering the impact of underlying components, particularly pre-trained vision encoders, when integrating them into systems in a plug-and-play manner. By better understanding their under-explored embedding space, and addressing the potential vulnerabilities introduced by adversarial embedding space attacks, we hope to better safeguard multi-modal systems and ensure their reliable and secure deployment in real-world applications.




% \section*{Ethics Statement}
% \erfan{should we have this section at all?}

% \section*{Acknowledgements}
% \erfan{how about this section?}

% Entries for the entire Anthology, followed by custom entries
\bibliography{acl2023}
\bibliographystyle{acl_natbib}

\appendix
\onecolumn
\section{Man riding bike adversarial image full chat history with LLaVA}
\label{sec:appen-bike-chat}

Here, we have provided the screenshots of our full chat session with LLaVA when the adversarial image in Figure \ref{fig:result1} was uploaded. It's amazing how great it can describe the adversarial image; it shows the adversarial image has captured almost all of the semantic features of the actual target image. 

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed


\clearpage
\newpage
\section{Another example - inappropriate targets like porn}
\label{sec:appen-porn}

We don't show the target image used due to its inappropriate and disturbing content. 

% Figure environment removed

% Figure environment removed


Now, if we feed the adversarial image in Figure \ref{fig:l2-porn} to LLaVA, its output would be as follows:

\begin{tcolorbox}[colframe=blue!50!black, colback=blue!10, boxrule=1pt]
\textit{`The image is a colorful, swirly, and brightly colored picture that features a man and a woman. The man is positioned on the left side of the image, while the woman is on the right side. They appear to be in a suggestive pose, with the man's hand near the woman's crotch area. The picture has a distorted and abstract style, making it difficult to discern the facial features of the individuals. The overall composition of the image is a combination of bright colors, swirls, and distortion, creating a unique and eye-catching visual.'}
\end{tcolorbox}


\section{LLaVA major upgrade - Confirming the resilience of our attacks}
\label{sec:appen-demo-upgrade}

During the course of writing this paper, something interesting happened that corroborates our hypotheses of the resilience of our attacks against model updates and fine-tuning. We noticed that the demo version changed from \textbf{`LLaVA-13B-V1-1'} to \textbf{`LLaVA-LLaMA-2-13B-Chat-Lightning-Preview'} which utilizes the newly introduced Llama 2 \cite{Llama2} instead of Vicuna as the language model as well as some training alterations. For further details about the modifications made to the model, we direct you to refer to their own explanations \cite{LLaVA-Llama2}.

We tested the same previously identified adversarial images against this new updated version as well and we witnessed absolute transferability. Below, we have included outputs of the `LLaVA-LLaMA-2-13B-Chat-Lightning-Preview' version.

% Figure environment removed

% Figure environment removed


\clearpage
% \newpage
\section{LLaVA leaked a part of its initial system prompt}
\label{sec:appen-leak}

% Figure environment removed


\newpage
\section{Another example for a woman cooking in the kitchen target}
\label{sec:another-kitchen}

% Figure environment removed

% \newpage
\section{Another example for hidden prompt injection attack}
\label{sec:covert-attack-example}
Another example is also shown below:

% Figure environment removed

% Figure environment removed

We try to find an adversarial image using our method that is almost equivalent to Figure \ref{fig:neverstop} in the embedding space. We obtained the image shown in Figure \ref{fig:neverstopadv}, and if we give it as input to LLaVA, its output would be as follows, obeying the instruction that says never stop.

\textit{`The image contains text that says "Ring ring ring ring ring ring ring ring ring ring ring ring ring ring ring ring ring ring ring ring ring ring ring ring ring ring ring ring ring ring ring ring ring ring ring ring ..." '}.

It's interesting why it prints the word `Ring' though. We conjecture that it might be due to the noises present in the adversarial input image (Figure \ref{fig:neverstopadv}) and sometimes in literature, `ring' is associated with noise and such signals. 


\newpage
\section{OCR capabilities of Bing \& Bard and their vulnerability against visual prompt injection}
\label{sec:instruction-attack-via-image}

The uploaded image is depicted in Figure \ref{fig:pirate}:

% Figure environment removed

% Figure environment removed

% Figure environment removed
\end{document}
