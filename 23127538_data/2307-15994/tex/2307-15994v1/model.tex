\section{Methodology}
In this section, 
we will provide a detailed description 
of the learning setting of \setting~ towards new clients. 
We will then introduce our proposed method \algo~ and discuss an enhanced version \algopp~, 
which incorporates a regularization on the base prediction model and entropy-based early stopping. 
To address practical scenarios where devices are heterogeneous, 
we will also propose \heteroalgo, 
which employs a novel knowledge distillation loss 
to effectively distill the dark knowledge from the teacher models. 
This further expands the application scope of \algo. 

\subsection{Problem Formulation: Unsupervised Personalized Federated Learning (UPFL)}
In this work, we focus on the scenario 
where training is performed on $N$ clients, 
denoted by $\{c_{1}, \cdots, c_{N}\}$, 
each with a private dataset $\mathcal{D}_i = \{ (x_{j}^{(i)}, y_{j}^{(i)}) \}_{j=1}^{m_i}$ of size $m_i$, 
and novel clients join in at the inference stage. 
Our objective is to learn an unsupervised adaptation strategy from the training clients, 
which can provide a personalized model for a novel client 
based on its unlabelled dataset $\mathcal{D}_{new} = \{ x_{j}^{(i)} \}_{j=1}^{m_{new}}$. 
We refer to this learning setting as the \underline{u}nsupervised \underline{p}ersonalized \underline{f}ederated \underline{l}earning setting (\setting). 
We will primarily focus on a multi-class classification problem, but much of our analysis can be extended directly to regression and other problems. 
\subsection{Algorithm \algo}
The proposed framework is illustrated in Fig.~\ref{fig:framework}. 
It can be seen from the figure that 
each client's local model mainly consists of two components: 
base prediction model $f(\cdot;\psi)$ and adaptation model $g(\cdot;\phi)$. 
The base prediction model $f$ is the task network for classification, 
which takes a batch of samples $X:=\{x_1, \cdots, x_{B}\}$ 
and outputs the corresponding logits $Z:=\{z_1, \cdots, z_{B}\}$: 
\begin{equation}
Z = f(X, \psi)
\end{equation}
The adaptation model is responsible for 
adapting the base prediction model 
to a personalized prediction model, 
which takes the output of the base prediction model 
and outputs a scalar for each sample 
representing 
how \textit{poorly} current base prediction model $f$ 
performs on corresponding unlabelled data. 
Following~\cite{zhang2021adaptive}, 
we use the $\ell_{2}$-norm of these scalars 
across the batch of inputs 
as the loss 
for updating base prediction model $f$. 
Specifically, 
\begin{equation}
\ell_{per} = \Vert g(Z; \phi) \Vert_{2}
\label{eq:personalization-loss}
\end{equation}
Obviously, the output of the adaptation model 
is independent of samples order. 
Regardless of how the samples are sorted, 
the adaptation model's evaluation 
of the performance of the base prediction model 
on the dataset
is consistent. 

\subsubsection{Training Phase}
During training, each client has access to its labelled dataset 
$\mathcal{D}_i := \{ (x_{j}^{(i)}, y_{j}^{(i)}) \}_{j=1}^{m_i}$. 
And we simulate the testing environement 
to meta-train the base prediction model 
and the adaptation model. 
Specifically, we first adapt the base prediction model towards a personalized model: 
\begin{equation}
Z_{i} = f(\mathcal{D}_i; \psi^{i})
\end{equation}
\begin{equation}
\tilde{\psi}^{i} \leftarrow \psi^{i} - \eta_{inner} \nabla_{\psi}g(Z_{i}; \phi^{i})
\end{equation}
, where $\eta_{inner}$ is the learning rate of the prediction model in the inner loop. 
Then, 
we optimize the parameters $\psi^{i}$ and $\phi^{i}$ to minimize the loss of the personalized model $\tilde{\psi}^{i}$ on its local dataset: 
\begin{equation}
\tilde{Z_{t}} = f(X_t; \tilde{\psi}^{i})
\end{equation}
\begin{equation}
\mathcal{L} = \ell_{CE}(\tilde{Z_{t}}; Y_{t})
\label{eq:meta-loss}
\end{equation}
\begin{equation}
\psi^{i} \leftarrow \psi^{i} - \eta_{outer} \nabla_{\psi}\mathcal{L}
\end{equation}
\begin{equation}
\phi^{i} \leftarrow \phi^{i} - \eta_{adapt} \nabla_{\phi}\mathcal{L}
\end{equation}
, where $\eta_{outer}$ is the learning rate of the prediction model in the outer loop 
and $\eta_{adapt}$ denotes the learning rate of the adaptation model. 
\subsubsection{Testing Phase}
Once a new client $c_{new}$ joins 
after the federated model has been deployed, 
it downloads the models from the server 
and adapts the base prediction model 
under the supervision of the adaptation model, 
and then makes predictions 
based on the personalized prediction model:
\begin{equation}
Z_{new} = f(\mathcal{D}_{new}; \psi^{new})
\end{equation}

\begin{equation}
\psi^{new} \leftarrow \psi^{new} - \eta_{inner} \nabla_{\psi}g(Z_{new}; \phi^{new})
\end{equation}

\begin{equation}
\tilde{Z}_{new} = f(\mathcal{D}_{new}; \psi^{new})
\end{equation}

% Figure environment removed

\subsection{Improved \algo~(\algopp)} 
\subsubsection{Enhanced Personalization with Regularization}
In federated learning, 
the client data is often limited, 
which can easily lead to the overfitting of the base prediction model. 
A highly personalized base prediction model 
presents challenges for the adaptation model 
learning to adapts the base model towards personalization. 
To mitigate the issue, 
during local training, 
we constrain the output of the base prediction model 
to approach that of the server prediction model, 
which enables the adaptation model 
to be effectively trained to fine-tune a generalized model into a personalized one. 
We call such an extended version of \algo~as \algoprox. 
With the constraint, the loss function in Equation~\ref{eq:meta-loss} becomes: 
\begin{equation}
\mathcal{L} = \ell_{CE}(\tilde{Z_{t}}; Y_{t})+\mu \ell_{KL}(s(Z_{t})||s(\dot{Z_{t}}))
\end{equation}
, where $\mu$ is a balancing coefficient and $\ell_{KL}$ denotes the KL divergence. 

Different from \method{FedProx}~\cite{li2020federated}, 
which limits the local updates 
directly in parameter space for mitigating client variances, 
we regularize the base prediction model's outputs (logits) to be general (less personalized) 
and expect the critic can adjust them to be personalized ones. 


\subsubsection{Entropy-based Early Stopping} 
\algo~outputs a base prediction model and an adaptation model, 
aiming to personalize the base prediction model quickly for a new client 
through \textit{a one-step update} under the adaptation model's supervision. 
However, a \textit{one-step update} may not be sufficient, 
and \textit{multi-step} fine-tuning can often yield a better personalized model. 
We run \algo~ on CIFAR-10 dataset, 
and during testing, 
each client updates the base prediction model for 50 iterations. 
Interestingly, we have empirically observed a slight mismatch 
between $\ell_{per}$ (defined in Equation~\ref{eq:personalization-loss}) 
and the extent of model personalization 
as fine-tuning progresses. 
Fig~\ref{fig:mismatch} shows that additional fine-tuning 
can improve the personalized model in the first 10 steps, 
but excessive fine-tuning can lead to a suboptimal personalized model. 
Obviously the number of fine-tuning steps 
is a crucial parameter 
that could vary among clients, 
and it is unreasonable to apply a universal hyperparameter to all clients. 

% Figure environment removed

To address this, we suggest an adaptive early stopping strategy utilizing \textit{entropy}. 
In general, a better personalized model will produce a more confident prediction with low entropy. 
Specifically, we calculates the entropy of the personalized model's predictions during fine-tuning 
and terminate fine-tuning when the entropy does not decrease 
within a certain steps (as illustrated in Fig~\ref{fig:mismatch}). 


\subsection{Heterogeneous \algo~(\heteroalgo)}
In practical applications, parties may exhibit device heterogeneity, with variations in computing capability and storage, resulting in heterogeneous models in terms of model structure or size. 
Furthermore, as the model architecture is considered the intellectual property of the client, it is not desirable to expose it to the server. 
Therefore, we propose \heteroalgo, which allows clients to have diverse model structures, and the local models are kept locally throughout the training process. 

\heteroalgo~ builds upon a classic heterogeneous federated learning framework called \method{FedMD}~\cite{li2019fedmd}, 
which facilitates knowledge transfer between clients through knowledge distillation on a public dataset. 
Our contribution lies in proposing a \textit{more efficient} knowledge distillation loss function for \algo, 
which significantly enhances the transfer of knowledge in generating personalized models. 

Before presenting the details of our proposed loss function, 
we provide a brief overview of how \heteroalgo~ runs under the framework of \method{FedMD}. 
\method{FedMD} assumes that all clients have access to a public \textit{unlabeled} dataset 
$\mathcal{D}_p:=\{ x_{j}^{(p)} \}_{j=1}^{m_p}$ 
that follows a distribution similar to the overall data distribution across all clients. 
$\mathcal{D}_p$ can be collected from public sources or generated using generative methods~\cite{Goodfellow2014GenerativeAN,zhu2021data}. 
During training, the server sends each client the ensemble knowledge about $\mathcal{D}_p$, 
i.e., the average of the logits produced by clients' models on $\mathcal{D}_p$. 
Each client first digests the ensemble knowledge via knowledge distillation 
and then meta-trains local models on its labelled dataset. 
Upon completion of local training, 
each client obtains the personalized model 
by fine-tuning its base prediction model 
under the adaptation model's supervision 
and then sends the logits to the server 
made by the personalized prediction model on $\mathcal{D}_p$. 
The server aggregates received logits and updates the consensus about $\mathcal{D}_p$. 
When a new client joins for testing, 
it can customize the architecture 
of its base prediction model 
and adaptation model 
and then learns the knowledge from the training clients 
by knowledge distillation on the public dataset. 

Efficient knowledge distillation is critical for this framework. 
Each client locally trains a pair of networks, 
consisting of a base prediction model and an adaptation model. 
For ease of description, 
we denote the base prediction model, personalized prediction model and adaptation model of the teacher and student as 
$\{\psi^t, \tilde{\psi}^t, \phi^t\}$ and 
$\{\psi^s, \tilde{\psi}^s, \phi^s\}$. 
A \textit{vanilla} knowledge distillation approach is to meta-train ${\psi^s, \phi^s}$ 
such that the output of $\tilde{\psi}^s$ 
approximates that of $\tilde{\psi}^t$ as closely as possible. 
However, 
we believe that 
the teacher's knowledge comes from two sources: 
the general prediction ability of the base prediction model 
and the adaptation model's personalization capacity. 
\textit{End-to-end distillation} might result in insufficient training of $\phi^s$, 
as $\psi^{s}$ might be lazy and directly learn from $\tilde{\psi}^t$. 

To address this, 
we propose that the student learns from the teacher in a \textit{pipeline} fashion. 
Specifically, 
$\psi^{s}$ should mimic the output of $\psi^{t}$, 
while $\phi^{s}$ should learn the personalization capability of $\phi^{t}$. 
Under the constraint of 
minimizing the distance between the output of $\psi^{s}$ and $\psi^{t}$, 
$\phi^{s}$ can learn the personalization ability of $\phi^{t}$ 
by minimizing the divergence between the outputs of $\tilde{\psi}^{s}$ and $\tilde{\psi}^{t}$. 
We formulate the distillation loss function as: 
\begin{equation}
\label{eq:kd_loss}
\begin{split}
\mathcal{L}_{KD}(\psi^{s},\phi^{s}) = 
\sum_{x\in \mathcal{D}_{p}} \ell_{KL} (\sigma(f(x|\tilde{\psi}^{s})), \sigma(f(x|\tilde{\psi}^{t}))) \\ + 
\lambda \ell_{KL} (\sigma(f(x|\psi^{s})), \sigma(f(x|\psi^{t})))
\end{split}
\end{equation}
, where KL denotes the KL-divergence and $\sigma(\cdot)$ denotes the softmax function. 
% We update the parameters $\psi^{s}$ and $\phi^{s}$ in an alternating manner to improve the stability of the training process. 
\todo{We illustrate the knowledge distillation loss in Fig.~\ref{fig:kd-framework}} 
and the detailed algorithm of \heteroalgo~ is provided in Algorithm~\ref{alg:hetero_fedtta}. 

\begin{algorithm}[t]
\caption{HeteroFedTTA}
\label{alg:hetero_fedtta}
\textbf{Input}: public unlabelled dataset $\mathcal{D}_p$ \todo{xx} \\
\textbf{Output}: $\theta_{R}$
\begin{algorithmic}[1] %[1] enables line numbers
\STATE \textbf{Training: }
\FOR{\texttt{each round} $r$ : 0 \texttt{to} $R-1$}
    \STATE // Local Training 
    \STATE \textbf{Distribute:} Each client downloads the updated consensus $\mathcal{F}_{base}(\mathcal{D}_p)$ and $\mathcal{F}_{per}(\mathcal{D}_p)$ 
    \STATE \textbf{Digest:} Each client distills the knowledge of the ensemble of clients' predictions through the KD loss defined in Equation~\ref{eq:kd_loss}. 
    \STATE \textbf{Revisit:} Each client meta-trains its local models on its own labelled dataset. 
    \STATE \textbf{Communicate:} Each client computes the logits on the public dataset made by the base prediction model and personalzide prediction model, 
    i.e., $f(\mathcal{D}_p|\psi^i)$ and $f(\mathcal{D}_p|\tilde{\psi^i})$,
    and transmits the results to the central server. 
    \STATE // Server Aggregation
    \STATE \textbf{Aggregate}: The server updates the consensus $\mathcal{F}_{base}(\mathcal{D}_p) = \frac{1}{N} \sum_{i} f(\mathcal{D}_p|\psi^{i})$ and 
    $\mathcal{F}_{per}(\mathcal{D}_p) = \frac{1}{N} \sum_{i} f(\mathcal{D}_p|\tilde{\psi^i})$ .    
\ENDFOR
\STATE \textbf{Test of client $c_{i}$:}
\STATE \textbf{Digest:} Client $c_{i}$ distills the knowledge of the ensemble of training clients' predictions through the KD loss defined in Equation~\ref{eq:kd_loss}. 
\STATE \textbf{Revisit:} Client $c_{i}$ adapts its base prediction model with the adaptation model and makes predictions with the personalized prediction model. 


\end{algorithmic}
\end{algorithm}

% Figure environment removed

\subsection{Summary}
In Algorithm~\ref{alg:algorithm}, we provide a detailed description of the training and testing procedures for each method in the family of \algo. 

During training, \algo~ is trained solely on the training clients without accessing any data from new clients. 
During testing, a well-trained critic personalizes the base model for the novel client based on its unlabeled dataset. 
\algopp~ improves \algo's perfo rmance through two simple strategies: augmenting query set (line xx to xx) and early stopping adaptation (line xx to xx), 
which incur almost no additional computational and storage costs. 
\heteroalgo~ extends the application of \algo~ to device heterogeneous settings by aggregating the knowledge of various base networks and critics through knowledge distillation (line xx to xx).


\begin{algorithm}[h!]
\caption{FedTTA++}
\label{alg:algorithm}
\textbf{Input}: 
initial parameters of base prediction model $\psi^{0}$ and adaptation model $\phi^{0}$, 
total communication rounds $R$, 
local iterations $\tau$, 
local minibatch size $B$, 
fraction of active users $p$,
and maximum number of iterations for fine-tuning during testing $E$ \\
\textbf{Output}: $\theta_{R}$
\begin{algorithmic}[1] %[1] enables line numbers
\STATE \textbf{Training: }
\FOR{\texttt{each round} $r$ : 0 \texttt{to} $R-1$}
    \STATE Select a set of clients $\mathcal{S}_{r}$ of size $M$ 
    \FOR{\texttt{each client} $c_{i} \in \mathcal{S}_{r}$}
        \STATE // Local Training
        \STATE $\{\psi^{i}, \phi^{i}\} \leftarrow \{\psi_{r}, \phi_{r}\}$
        \STATE $\mathcal{B}_{i} \leftarrow$ Split local dataset $\mathcal{D}_i$ into batches of size $B$
        \FOR{\texttt{each iteration} $t$ : 1 \texttt{to} $\tau$}
            \STATE Sample a batch $(X_{t}, Y_{t}) \sim \mathcal{B}_{i}$
            \STATE $\dot{Z_{t}} = f(X_t; \psi_r)$
            \STATE $Z_{t} = f(X_t; \psi^{i})$ 
            \STATE $\tilde{\psi}^{i} \leftarrow \psi^{i} - \eta_{inner} \nabla_{\psi}g(Z_{t}; \phi^{i})$ 
            \STATE $\tilde{Z_{t}} = f(X_t; \tilde{\psi}^{i})$ 
            \STATE $\mathcal{L} = \ell_{CE}(\tilde{Z_{t}}; Y_{t})+\mu \ell_{KL}(s(Z_{t})||s(\dot{Z_{t}}))$
            \STATE $\psi^{i} \leftarrow \psi^{i} - \eta_{base} \nabla_{\psi}\mathcal{L}$
            \STATE $\phi^{i} \leftarrow \phi^{i} - \eta_{adapt} \nabla_{\phi}\mathcal{L}$
        \ENDFOR
        \STATE $\{\psi_{r+1}^{i}, \phi_{r+1}^{i}\} \leftarrow \{\psi^{i}, \phi^{i}\}$
        \STATE Client $c_{i}$ sends $\psi_{r+1}^{i}$ and $\phi_{r+1}^{i}$ back to server; \\
    \ENDFOR
    \STATE // Server Aggregation
    \STATE $\psi_{r+1} \leftarrow \frac{1}{M}\sum_{c_{i} \in \mathcal{S}_{r}} \psi_{r+1}^{i}$
    \STATE $\phi_{r+1} \leftarrow \frac{1}{M}\sum_{c_{i} \in \mathcal{S}_{r}} \phi_{r+1}^{i}$
\ENDFOR
\STATE
\STATE \textbf{Test of client $c_{i}$:}
\STATE $\{\psi^{i}, \phi^{i}\} \leftarrow \{\psi_{r}, \phi_{r}\}$
\FOR{\texttt{each epoch} $e$: 1 \texttt{to} $E$}
    \STATE $Z_i = f(\mathcal{D}_{i}; \psi^{i})$ 
    \STATE $\psi^{i} \leftarrow \psi^{i} - \eta_{inner} \nabla_{\psi}g(Z_i; \phi^{i})$
    \STATE $\tilde{Z_{i}} = f(\mathcal{D}_{i}; \psi^{i})$
\ENDFOR 
\STATE Select the model whose predictions have the least entropy, i.e., $\mathcal{H}(s(\tilde{Z_{i}}))$
\end{algorithmic}
\end{algorithm}