@article{johnson2019billion,
  title={Billion-scale similarity search with {GPUs}},
  author={Johnson, Jeff and Douze, Matthijs and J{\'e}gou, Herv{\'e}},
  journal={IEEE Transactions on Big Data},
  volume={7},
  number={3},
  pages={535--547},
  year={2019},
  publisher={IEEE}
}

@article{dosovitskiy2020vit,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and  Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  journal={ICLR},
  year={2021}
}

@inproceedings{brown2020,
  author       = {Tom B. Brown and
                  Benjamin Mann and
                  Nick Ryder and
                  Melanie Subbiah and
                  Jared Kaplan and
                  Prafulla Dhariwal and
                  Arvind Neelakantan and
                  Pranav Shyam and
                  Girish Sastry and
                  Amanda Askell and
                  Sandhini Agarwal and
                  Ariel Herbert{-}Voss and
                  Gretchen Krueger and
                  Tom Henighan and
                  Rewon Child and
                  Aditya Ramesh and
                  Daniel M. Ziegler and
                  Jeffrey Wu and
                  Clemens Winter and
                  Christopher Hesse and
                  Mark Chen and
                  Eric Sigler and
                  Mateusz Litwin and
                  Scott Gray and
                  Benjamin Chess and
                  Jack Clark and
                  Christopher Berner and
                  Sam McCandlish and
                  Alec Radford and
                  Ilya Sutskever and
                  Dario Amodei},
  editor       = {Hugo Larochelle and
                  Marc'Aurelio Ranzato and
                  Raia Hadsell and
                  Maria{-}Florina Balcan and
                  Hsuan{-}Tien Lin},
  title        = {Language Models are Few-Shot Learners},
  booktitle    = {Advances in Neural Information Processing Systems 33: Annual Conference
                  on Neural Information Processing Systems 2020, NeurIPS 2020, December
                  6-12, 2020, virtual},
  year         = {2020},
  url          = {https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html},
  timestamp    = {Tue, 19 Jan 2021 15:56:50 +0100},
  biburl       = {https://dblp.org/rec/conf/nips/BrownMRSKDNSSAA20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{pmcclip,
  title={Pmc-clip: Contrastive language-image pre-training using biomedical documents},
  author={Lin, Weixiong and Zhao, Ziheng and Zhang, Xiaoman and Wu, Chaoyi and Zhang, Ya and Wang, Yanfeng and Xie, Weidi},
  journal={arXiv preprint arXiv:2303.07240},
  year={2023}
}

@article{moor2023foundation,
  title={Foundation models for generalist medical artificial intelligence},
  author={Moor, Michael and Banerjee, Oishi and Abad, Zahra Shakeri Hossein and Krumholz, Harlan M and Leskovec, Jure and Topol, Eric J and Rajpurkar, Pranav},
  journal={Nature},
  volume={616},
  number={7956},
  pages={259--265},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@article{alayrac2022flamingo,
  title={Flamingo: a visual language model for few-shot learning},
  author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={23716--23736},
  year={2022}
}
@inproceedings{yasunaga2022retrieval,
  title={Retrieval-Augmented Multimodal Language Modeling},
  author={Yasunaga, Michihiro and Aghajanyan, Armen and Shi, Weijia and James, Rich and Leskovec, Jure and Liang, Percy and Lewis, Mike and Zettlemoyer, Luke and Yih, Wen-tau},
  booktitle=ICML,
  year={2023}
}
@article{zhang2023large,
  title={Large-Scale Domain-Specific Pretraining for Biomedical Vision-Language Processing},
  author={Zhang, Sheng and Xu, Yanbo and Usuyama, Naoto and Bagga, Jaspreet and Tinn, Robert and Preston, Sam and Rao, Rajesh and Wei, Mu and Valluri, Naveen and Wong, Cliff and others},
  journal={arXiv preprint arXiv:2303.00915},
  year={2023}
}
@article{zhang2023pmc,
  title={Pmc-vqa: Visual instruction tuning for medical visual question answering},
  author={Zhang, Xiaoman and Wu, Chaoyi and Zhao, Ziheng and Lin, Weixiong and Zhang, Ya and Wang, Yanfeng and Xie, Weidi},
  journal={arXiv preprint arXiv:2305.10415},
  year={2023}
}
@article{gu2021domain,
  title={Domain-specific language model pretraining for biomedical natural language processing},
  author={Gu, Yu and Tinn, Robert and Cheng, Hao and Lucas, Michael and Usuyama, Naoto and Liu, Xiaodong and Naumann, Tristan and Gao, Jianfeng and Poon, Hoifung},
  journal={ACM Transactions on Computing for Healthcare (HEALTH)},
  volume={3},
  number={1},
  pages={1--23},
  year={2021},
  publisher={ACM New York, NY}
}

@article{bommasani2021opportunities,
  title={On the opportunities and risks of foundation models},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}

@article{acosta_multimodal_2022,
	title = {Multimodal biomedical {AI}},
	volume = {28},
	rights = {2022 Springer Nature America, Inc.},
	issn = {1546-170X},
	url = {https://www.nature.com/articles/s41591-022-01981-2},
	doi = {10.1038/s41591-022-01981-2},
	abstract = {The increasing availability of biomedical data from large biobanks, electronic health records, medical imaging, wearable and ambient biosensors, and the lower cost of genome and microbiome sequencing have set the stage for the development of multimodal artificial intelligence solutions that capture the complexity of human health and disease. In this Review, we outline the key applications enabled, along with the technical and analytical challenges. We explore opportunities in personalized medicine, digital clinical trials, remote monitoring and care, pandemic surveillance, digital twin technology and virtual health assistants. Further, we survey the data, modeling and privacy challenges that must be overcome to realize the full potential of multimodal artificial intelligence in health.},
	pages = {1773--1784},
	number = {9},
	journaltitle = {Nature Medicine},
	shortjournal = {Nat Med},
	author = {Acosta, Julián N. and Falcone, Guido J. and Rajpurkar, Pranav and Topol, Eric J.},
	urldate = {2023-01-10},
	date = {2022-09},
	langid = {english},
	note = {Number: 9
Publisher: Nature Publishing Group},
	keywords = {Computational biology and bioinformatics, Health care},
	file = {Full Text PDF:/Users/mdmoor/Zotero/storage/MHSUDHZJ/Acosta et al. - 2022 - Multimodal biomedical AI.pdf:application/pdf},
}

@misc{bommasani_opportunities_2022,
	title = {On the Opportunities and Risks of Foundation Models},
	url = {http://arxiv.org/abs/2108.07258},
	doi = {10.48550/arXiv.2108.07258},
	abstract = {{AI} is undergoing a paradigm shift with the rise of models (e.g., {BERT}, {DALL}-E, {GPT}-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.},
	number = {{arXiv}:2108.07258},
	publisher = {{arXiv}},
	author = {Bommasani, Rishi and Hudson, Drew A. and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S. and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and Brynjolfsson, Erik and Buch, Shyamal and Card, Dallas and Castellon, Rodrigo and Chatterji, Niladri and Chen, Annie and Creel, Kathleen and Davis, Jared Quincy and Demszky, Dora and Donahue, Chris and Doumbouya, Moussa and Durmus, Esin and Ermon, Stefano and Etchemendy, John and Ethayarajh, Kawin and Fei-Fei, Li and Finn, Chelsea and Gale, Trevor and Gillespie, Lauren and Goel, Karan and Goodman, Noah and Grossman, Shelby and Guha, Neel and Hashimoto, Tatsunori and Henderson, Peter and Hewitt, John and Ho, Daniel E. and Hong, Jenny and Hsu, Kyle and Huang, Jing and Icard, Thomas and Jain, Saahil and Jurafsky, Dan and Kalluri, Pratyusha and Karamcheti, Siddharth and Keeling, Geoff and Khani, Fereshte and Khattab, Omar and Koh, Pang Wei and Krass, Mark and Krishna, Ranjay and Kuditipudi, Rohith and Kumar, Ananya and Ladhak, Faisal and Lee, Mina and Lee, Tony and Leskovec, Jure and Levent, Isabelle and Li, Xiang Lisa and Li, Xuechen and Ma, Tengyu and Malik, Ali and Manning, Christopher D. and Mirchandani, Suvir and Mitchell, Eric and Munyikwa, Zanele and Nair, Suraj and Narayan, Avanika and Narayanan, Deepak and Newman, Ben and Nie, Allen and Niebles, Juan Carlos and Nilforoshan, Hamed and Nyarko, Julian and Ogut, Giray and Orr, Laurel and Papadimitriou, Isabel and Park, Joon Sung and Piech, Chris and Portelance, Eva and Potts, Christopher and Raghunathan, Aditi and Reich, Rob and Ren, Hongyu and Rong, Frieda and Roohani, Yusuf and Ruiz, Camilo and Ryan, Jack and Ré, Christopher and Sadigh, Dorsa and Sagawa, Shiori and Santhanam, Keshav and Shih, Andy and Srinivasan, Krishnan and Tamkin, Alex and Taori, Rohan and Thomas, Armin W. and Tramèr, Florian and Wang, Rose E. and Wang, William and Wu, Bohan and Wu, Jiajun and Wu, Yuhuai and Xie, Sang Michael and Yasunaga, Michihiro and You, Jiaxuan and Zaharia, Matei and Zhang, Michael and Zhang, Tianyi and Zhang, Xikun and Zhang, Yuhui and Zheng, Lucia and Zhou, Kaitlyn and Liang, Percy},
	urldate = {2023-01-11},
	date = {2022-07-12},
	eprinttype = {arxiv},
	eprint = {2108.07258 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computers and Society},
	file = {arXiv Fulltext PDF:/Users/mdmoor/Zotero/storage/62K7RADC/Bommasani et al. - 2022 - On the Opportunities and Risks of Foundation Model.pdf:application/pdf;arXiv.org Snapshot:/Users/mdmoor/Zotero/storage/HTNV5XSU/2108.html:text/html},
}

@inproceedings{alayrac_flamingo_2022,
	title = {Flamingo: a Visual Language Model for Few-Shot Learning},
	url = {https://openreview.net/forum?id=EbMuimAbPbs},
	shorttitle = {Flamingo},
	abstract = {Building models that can be rapidly adapted to novel tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research. We introduce Flamingo, a family of Visual Language Models ({VLM}) with this ability. We propose key architectural innovations to: (i) bridge powerful pretrained vision-only and language-only models, (ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as inputs. Thanks to their flexibility, Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities. We perform a thorough evaluation of our models, exploring and measuring their ability to rapidly adapt to a variety of image and video tasks. These include open-ended tasks such as visual question-answering, where the model is prompted with a question which it has to answer, captioning tasks, which evaluate the ability to describe a scene or an event, and close-ended tasks such as multiple-choice visual question-answering. For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a new state of the art with few-shot learning, simply by prompting the model with task-specific examples. On numerous benchmarks, Flamingo outperforms models fine-tuned on thousands of times more task-specific data.},
	eventtitle = {Advances in Neural Information Processing Systems},
	booktitle = {Advances in Neural Information Processing Systems},
	author = {Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and Ring, Roman and Rutherford, Eliza and Cabi, Serkan and Han, Tengda and Gong, Zhitao and Samangooei, Sina and Monteiro, Marianne and Menick, Jacob and Borgeaud, Sebastian and Brock, Andrew and Nematzadeh, Aida and Sharifzadeh, Sahand and Binkowski, Mikolaj and Barreira, Ricardo and Vinyals, Oriol and Zisserman, Andrew and Simonyan, Karen},
	urldate = {2023-01-11},
	date = {2022-10-31},
	langid = {english},
	file = {Full Text PDF:/Users/mdmoor/Zotero/storage/RBFLWDFP/Alayrac et al. - 2022 - Flamingo a Visual Language Model for Few-Shot Lea.pdf:application/pdf},
}

@misc{lu_unified-io_2022,
	title = {Unified-{IO}: A Unified Model for Vision, Language, and Multi-Modal Tasks},
	url = {http://arxiv.org/abs/2206.08916},
	doi = {10.48550/arXiv.2206.08916},
	shorttitle = {Unified-{IO}},
	abstract = {We propose Unified-{IO}, a model that performs a large variety of {AI} tasks spanning classical computer vision tasks, including pose estimation, object detection, depth estimation and image generation, vision-and-language tasks such as region captioning and referring expression, to natural language processing tasks such as question answering and paraphrasing. Developing a single unified model for such a large variety of tasks poses unique challenges due to the heterogeneous inputs and outputs pertaining to each task, including {RGB} images, per-pixel maps, binary masks, bounding boxes, and language. We achieve this unification by homogenizing every supported input and output into a sequence of discrete vocabulary tokens. This common representation across all tasks allows us to train a single transformer-based architecture, jointly on over 90 diverse datasets in the vision and language fields. Unified-{IO} is the first model capable of performing all 7 tasks on the {GRIT} benchmark and produces strong results across 16 diverse benchmarks like {NYUv}2-Depth, {ImageNet}, {VQA}2.0, {OK}-{VQA}, Swig, {VizWizGround}, {BoolQ}, and {SciTail}, with no task-specific fine-tuning. Code and demos for Unified-{IO} are available at: https://unified-io.allenai.org.},
	number = {{arXiv}:2206.08916},
	publisher = {{arXiv}},
	author = {Lu, Jiasen and Clark, Christopher and Zellers, Rowan and Mottaghi, Roozbeh and Kembhavi, Aniruddha},
	urldate = {2023-01-11},
	date = {2022-10-04},
	eprinttype = {arxiv},
	eprint = {2206.08916 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/mdmoor/Zotero/storage/JNWCINQU/Lu et al. - 2022 - Unified-IO A Unified Model for Vision, Language, .pdf:application/pdf;arXiv.org Snapshot:/Users/mdmoor/Zotero/storage/QQC2NGZY/2206.html:text/html},
}

@inproceedings{brown_language_2020,
	title = {Language Models are Few-Shot Learners},
	volume = {33},
	url = {https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html},
	abstract = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train {GPT}-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting.  For all tasks, {GPT}-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model.  {GPT}-3 achieves strong performance on many {NLP} datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where {GPT}-3's few-shot learning still struggles, as well as some datasets where {GPT}-3 faces methodological issues related to training on large web corpora.},
	pages = {1877--1901},
	booktitle = {Advances in Neural Information Processing Systems},
	author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and {McCandlish}, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	urldate = {2023-01-11},
	date = {2020},
	file = {Full Text PDF:/Users/mdmoor/Zotero/storage/2SPSKXXK/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:application/pdf},
}

@misc{aghajanyan_cm3_2022,
	title = {{CM}3: A Causal Masked Multimodal Model of the Internet},
	url = {http://arxiv.org/abs/2201.07520},
	doi = {10.48550/arXiv.2201.07520},
	shorttitle = {{CM}3},
	abstract = {We introduce {CM}3, a family of causally masked generative models trained over a large corpus of structured multi-modal documents that can contain both text and image tokens. Our new causally masked approach generates tokens left to right while also masking out a small number of long token spans that are generated at the end of the string, instead of their original positions. The casual masking object provides a type of hybrid of the more common causal and masked language models, by enabling full generative modeling while also providing bidirectional context when generating the masked spans. We train causally masked language-image models on large-scale web and Wikipedia articles, where each document contains all of the text, hypertext markup, hyperlinks, and image tokens (from a {VQVAE}-{GAN}), provided in the order they appear in the original {HTML} source (before masking). The resulting {CM}3 models can generate rich structured, multi-modal outputs while conditioning on arbitrary masked document contexts, and thereby implicitly learn a wide range of text, image, and cross modal tasks. They can be prompted to recover, in a zero-shot fashion, the functionality of models such as {DALL}-E, {GENRE}, and {HTLM}. We set the new state-of-the-art in zero-shot summarization, entity linking, and entity disambiguation while maintaining competitive performance in the fine-tuning setting. We can generate images unconditionally, conditioned on text (like {DALL}-E) and do captioning all in a zero-shot setting with a single model.},
	number = {{arXiv}:2201.07520},
	publisher = {{arXiv}},
	author = {Aghajanyan, Armen and Huang, Bernie and Ross, Candace and Karpukhin, Vladimir and Xu, Hu and Goyal, Naman and Okhonko, Dmytro and Joshi, Mandar and Ghosh, Gargi and Lewis, Mike and Zettlemoyer, Luke},
	urldate = {2023-01-11},
	date = {2022-01-19},
	eprinttype = {arxiv},
	eprint = {2201.07520 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/mdmoor/Zotero/storage/TZ62R4CC/Aghajanyan et al. - 2022 - CM3 A Causal Masked Multimodal Model of the Inter.pdf:application/pdf;arXiv.org Snapshot:/Users/mdmoor/Zotero/storage/IPIMQIT2/2201.html:text/html},
}

@article{wei_emergent_2022,
	title = {Emergent Abilities of Large Language Models},
	url = {https://openreview.net/forum?id=yzkSU5zdwD},
	abstract = {Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence raises the question of whether additional scaling could potentially further expand the range of capabilities of language models.},
	journaltitle = {Transactions on Machine Learning Research},
	author = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and Chi, Ed H. and Hashimoto, Tatsunori and Vinyals, Oriol and Liang, Percy and Dean, Jeff and Fedus, William},
	urldate = {2023-01-11},
	date = {2022-08-31},
	langid = {english},
	file = {Full Text PDF:/Users/mdmoor/Zotero/storage/GDJBUJQL/Wei et al. - 2022 - Emergent Abilities of Large Language Models.pdf:application/pdf},
}

@article{tiu_expert-level_2022,
  title={Expert-level detection of pathologies from unannotated chest X-ray images via self-supervised learning},
  author={Tiu, Ekin and Talius, Ellie and Patel, Pujan and Langlotz, Curtis P and Ng, Andrew Y and Rajpurkar, Pranav},
  journal={Nature Biomedical Engineering},
  volume={6},
  number={12},
  pages={1399--1406},
  year={2022},
  publisher={Nature Publishing Group UK London}
}


@article{steinberg_language_2021,
	title = {Language models are an effective representation learning technique for electronic health record data},
	volume = {113},
	issn = {1532-0464},
	url = {https://www.sciencedirect.com/science/article/pii/S1532046420302653},
	doi = {10.1016/j.jbi.2020.103637},
	abstract = {Widespread adoption of electronic health records ({EHRs}) has fueled the development of using machine learning to build prediction models for various clinical outcomes. However, this process is often constrained by having a relatively small number of patient records for training the model. We demonstrate that using patient representation schemes inspired from techniques in natural language processing can increase the accuracy of clinical prediction models by transferring information learned from the entire patient population to the task of training a specific model, where only a subset of the population is relevant. Such patient representation schemes enable a 3.5\% mean improvement in {AUROC} on five prediction tasks compared to standard baselines, with the average improvement rising to 19\% when only a small number of patient records are available for training the clinical prediction model.},
	pages = {103637},
	journaltitle = {Journal of Biomedical Informatics},
	shortjournal = {Journal of Biomedical Informatics},
	author = {Steinberg, Ethan and Jung, Ken and Fries, Jason A. and Corbin, Conor K. and Pfohl, Stephen R. and Shah, Nigam H.},
	urldate = {2023-01-11},
	date = {2021-01-01},
	langid = {english},
	keywords = {Electronic health record, Machine learning, Representation learning, Risk stratification, Transfer learning},
	file = {ScienceDirect Full Text PDF:/Users/mdmoor/Zotero/storage/NU9MSPX2/Steinberg et al. - 2021 - Language models are an effective representation le.pdf:application/pdf;ScienceDirect Snapshot:/Users/mdmoor/Zotero/storage/X6G3RBNX/S1532046420302653.html:text/html},
}

@online{noauthor_us_2022,
	title = {U.S. Food \& Drug Administration},
	url = {https://www.fda.gov/medical-devices/software-medical-device-samd/artificial-intelligence-and-machine-learning-aiml-enabled-medical-devices},
	abstract = {The {FDA} has updated the list of {AI}/{ML}-enabled medical devices marketed in the United States as a resource to the public.},
	urldate = {2022-10-15},
	date = {2022-10-05},
	langid = {english},
	note = {Publisher: {FDA}},
	file = {Snapshot:/Users/mdmoor/Zotero/storage/GDGCJX7C/artificial-intelligence-and-machine-learning-aiml-enabled-medical-devices.html:text/html},
}

@article{krishnan_self-supervised_2022,
	title = {Self-supervised learning in medicine and healthcare},
	volume = {6},
	rights = {2022 Springer Nature Limited},
	issn = {2157-846X},
	url = {https://www.nature.com/articles/s41551-022-00914-1},
	doi = {10.1038/s41551-022-00914-1},
	abstract = {The development of medical applications of machine learning has required manual annotation of data, often by medical experts. Yet, the availability of large-scale unannotated data provides opportunities for the development of better machine-learning models. In this Review, we highlight self-supervised methods and models for use in medicine and healthcare, and discuss the advantages and limitations of their application to tasks involving electronic health records and datasets of medical images, bioelectrical signals, and sequences and structures of genes and proteins. We also discuss promising applications of self-supervised learning for the development of models leveraging multimodal datasets, and the challenges in collecting unbiased data for their training. Self-supervised learning may accelerate the development of medical artificial intelligence.},
	pages = {1346--1352},
	number = {12},
	journaltitle = {Nature Biomedical Engineering},
	shortjournal = {Nat. Biomed. Eng},
	author = {Krishnan, Rayan and Rajpurkar, Pranav and Topol, Eric J.},
	urldate = {2023-01-11},
	date = {2022-12},
	langid = {english},
	note = {Number: 12
Publisher: Nature Publishing Group},
	keywords = {Computational biology and bioinformatics, Health care},
	file = {Full Text PDF:/Users/mdmoor/Zotero/storage/P6MSPDZE/Krishnan et al. - 2022 - Self-supervised learning in medicine and healthcar.pdf:application/pdf},
}

@article{zhang_zoonotic_2022,
	title = {A Zoonotic Henipavirus in Febrile Patients in China},
	volume = {387},
	issn = {0028-4793},
	url = {https://doi.org/10.1056/NEJMc2202705},
	doi = {10.1056/NEJMc2202705},
	pages = {470--472},
	number = {5},
	journaltitle = {New England Journal of Medicine},
	author = {Zhang, Xiao-Ai and Li, Hao and Jiang, Fa-Chun and Zhu, Feng and Zhang, Yun-Fa and Chen, Jin-Jin and Tan, Chee-Wah and Anderson, Danielle E. and Fan, Hang and Dong, Li-Yan and Li, Chang and Zhang, Pan-He and Li, Yue and Ding, Heng and Fang, Li-Qun and Wang, Lin-Fa and Liu, Wei},
	urldate = {2023-01-11},
	date = {2022-08-04},
	pmid = {35921459},
	note = {Publisher: Massachusetts Medical Society
\_eprint: https://doi.org/10.1056/{NEJMc}2202705},
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is All you Need},
	volume = {30},
	url = {https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 {BLEU} {onEnglish}-to-German translation, improving over the existing best ensemble result by over 1 {BLEU}. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 {BLEU}, achieving a {BLEU} score of 41.1.},
	booktitle = {Advances in Neural Information Processing Systems},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	urldate = {2023-01-11},
	date = {2017},
	file = {Full Text PDF:/Users/mdmoor/Zotero/storage/JWJLD2N9/Vaswani et al. - 2017 - Attention is All you Need.pdf:application/pdf},
}

@inproceedings{guu_realm_2020,
	title = {{REALM}: retrieval-augmented language model pre-training},
	series = {{ICML}'20},
	shorttitle = {{REALM}},
	abstract = {Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for {NLP} tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring everlarger networks to cover more facts. To capture knowledge in a more modular and interpretable way, we augment language model pretraining with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. We demonstrate the effectiveness of Retrieval-Augmented Language Model pretraining ({REALM}) by fine-tuning on the challenging task of Open-domain Question Answering (Open-{QA}). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-{QA} benchmarks, and find that we outperform all previous methods by a significant margin (4-16\% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity.},
	pages = {3929--3938},
	booktitle = {Proceedings of the 37th International Conference on Machine Learning},
	author = {Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Ming-Wei},
	urldate = {2023-01-11},
	date = {2020-07-13},
	file = {Full Text PDF:/Users/mdmoor/Zotero/storage/94F7GBLZ/Guu et al. - 2020 - REALM retrieval-augmented language model pre-trai.pdf:application/pdf},
}

@inproceedings{borgeaud_improving_2022,
	title = {Improving Language Models by Retrieving from Trillions of Tokens},
	url = {https://proceedings.mlr.press/v162/borgeaud22a.html},
	abstract = {We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a 2 trillion token database, our Retrieval-Enhanced Transformer ({RETRO}) obtains comparable performance to {GPT}-3 and Jurassic-1 on the Pile, despite using 25\{{\textbackslash}texttimes\} fewer parameters. After fine-tuning, {RETRO} performance translates to downstream knowledge-intensive tasks such as question answering. {RETRO} combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train {RETRO} from scratch, yet can also rapidly {RETROfit} pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale.},
	eventtitle = {International Conference on Machine Learning},
	pages = {2206--2240},
	booktitle = {Proceedings of the 39th International Conference on Machine Learning},
	author = {Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and Driessche, George Bm Van Den and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and Casas, Diego De Las and Guy, Aurelia and Menick, Jacob and Ring, Roman and Hennigan, Tom and Huang, Saffron and Maggiore, Loren and Jones, Chris and Cassirer, Albin and Brock, Andy and Paganini, Michela and Irving, Geoffrey and Vinyals, Oriol and Osindero, Simon and Simonyan, Karen and Rae, Jack and Elsen, Erich and Sifre, Laurent},
	urldate = {2023-01-11},
	date = {2022-06-28},
	langid = {english},
	note = {{ISSN}: 2640-3498},
	file = {Full Text PDF:/Users/mdmoor/Zotero/storage/AKRFZTMG/Borgeaud et al. - 2022 - Improving Language Models by Retrieving from Trill.pdf:application/pdf},
}

@article{igelstrom_causal_2022,
	title = {Causal inference and effect estimation using observational data},
	volume = {76},
	rights = {© Author(s) (or their employer(s)) 2022. Re-use permitted under {CC} {BY}. Published by {BMJ}.. https://creativecommons.org/licenses/by/4.0/This is an open access article distributed in accordance with the Creative Commons Attribution 4.0 Unported ({CC} {BY} 4.0) license, which permits others to copy, redistribute, remix, transform and build upon this work for any purpose, provided the original work is properly cited, a link to the licence is given, and indication of whether changes were made. See: https://creativecommons.org/licenses/by/4.0/.},
	issn = {0143-005X, 1470-2738},
	url = {https://jech.bmj.com/content/76/11/960},
	doi = {10.1136/jech-2022-219267},
	abstract = {Observational studies aiming to estimate causal effects often rely on conceptual frameworks that are unfamiliar to many researchers and practitioners. We provide a clear, structured overview of key concepts and terms, intended as a starting point for readers unfamiliar with the causal inference literature. First, we introduce theoretical frameworks underlying causal effect estimation methods: the counterfactual theory of causation, the potential outcomes framework, structural equations and directed acyclic graphs. Second, we define the most common causal effect estimands, and the issues of effect measure modification, interaction and mediation (direct and indirect effects). Third, we define the assumptions required to estimate causal effects: exchangeability, positivity, consistency and non-interference. Fourth, we define and explain biases that arise when attempting to estimate causal effects, including confounding, collider bias, selection bias and measurement bias. Finally, we describe common methods and study designs for causal effect estimation, including covariate adjustment, G-methods and natural experiment methods.},
	pages = {960--966},
	number = {11},
	journaltitle = {J Epidemiol Community Health},
	shortjournal = {J Epidemiol Community Health},
	author = {Igelström, Erik and Craig, Peter and Lewsey, Jim and Lynch, John and Pearce, Anna and Katikireddi, Srinivasa Vittal},
	urldate = {2023-01-11},
	date = {2022-11-01},
	langid = {english},
	note = {Publisher: {BMJ} Publishing Group Ltd
Section: Glossary},
	keywords = {epidemiology, methods, research design, statistics, study design},
	file = {Full Text PDF:/Users/mdmoor/Zotero/storage/HRHGRF2M/Igelström et al. - 2022 - Causal inference and effect estimation using obser.pdf:application/pdf},
}

@article{wang_extending_2023,
	title = {Extending the Nested Model for User-Centric {XAI}: A Design Study on {GNN}-based Drug Repurposing},
	volume = {29},
	issn = {1941-0506},
	doi = {10.1109/TVCG.2022.3209435},
	shorttitle = {Extending the Nested Model for User-Centric {XAI}},
	abstract = {Whether {AI} explanations can help users achieve specific tasks efficiently (i.e., usable explanations) is significantly influenced by their visual presentation. While many techniques exist to generate explanations, it remains unclear how to select and visually present {AI} explanations based on the characteristics of domain users. This paper aims to understand this question through a multidisciplinary design study for a specific problem: explaining graph neural network ({GNN}) predictions to domain experts in drug repurposing, i.e., reuse of existing drugs for new diseases. Building on the nested design model of visualization, we incorporate {XAI} design considerations from a literature review and from our collaborators' feedback into the design process. Specifically, we discuss {XAI}-related design considerations for usable visual explanations at each design layer: target user, usage context, domain explanation, and {XAI} goal at the domain layer; format, granularity, and operation of explanations at the abstraction layer; encodings and interactions at the visualization layer; and {XAI} and rendering algorithm at the algorithm layer. We present how the extended nested model motivates and informs the design of {DrugExplorer}, an {XAI} tool for drug repurposing. Based on our domain characterization, {DrugExplorer} provides path-based explanations and presents them both as individual paths and meta-paths for two key {XAI} operations, why and what else. {DrugExplorer} offers a novel visualization design called {MetaMatrix} with a set of interactions to help domain users organize and compare explanation paths at different levels of granularity to generate domain-meaningful insights. We demonstrate the effectiveness of the selected visual presentation and {DrugExplorer} as a whole via a usage scenario, a user study, and expert interviews. From these evaluations, we derive insightful observations and reflections that can inform the design of {XAI} visualizations for other scientific applications.},
	pages = {1266--1276},
	number = {1},
	journaltitle = {{IEEE} Transactions on Visualization and Computer Graphics},
	author = {Wang, Qianwen and Huang, Kexin and Chandak, Payal and Zitnik, Marinka and Gehlenborg, Nils},
	date = {2023-01},
	note = {Conference Name: {IEEE} Transactions on Visualization and Computer Graphics},
	keywords = {Artificial intelligence, Data visualization, Diseases, Drug Repurposing, Drugs, Graph Neural Network, Guidelines, Task analysis, Visual Explanation, Visualization, Visualization Design Model, {XAI}},
	file = {IEEE Xplore Abstract Record:/Users/mdmoor/Zotero/storage/KMSARB2D/9916585.html:text/html;IEEE Xplore Full Text PDF:/Users/mdmoor/Zotero/storage/A7SEYSQ8/Wang et al. - 2023 - Extending the Nested Model for User-Centric XAI A.pdf:application/pdf},
}

@inproceedings{li_align_2021,
	title = {Align before Fuse: Vision and Language Representation Learning with Momentum Distillation},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/505259756244493872b7709a8a01b536-Abstract.html},
	shorttitle = {Align before Fuse},
	pages = {9694--9705},
	booktitle = {Advances in Neural Information Processing Systems},
	author = {Li, Junnan and Selvaraju, Ramprasaath and Gotmare, Akhilesh and Joty, Shafiq and Xiong, Caiming and Hoi, Steven Chu Hong},
	urldate = {2023-01-11},
	date = {2021},
	file = {Full Text PDF:/Users/mdmoor/Zotero/storage/CJ699IX6/Li et al. - 2021 - Align before Fuse Vision and Language Representat.pdf:application/pdf},
}

@inproceedings{wang_simvlm_2022,
	title = {{SimVLM}: Simple Visual Language Model Pretraining with Weak Supervision},
	url = {https://openreview.net/forum?id=GUrhfTuf_3},
	shorttitle = {{SimVLM}},
	abstract = {With recent progress in joint modeling of visual and textual representations, Vision-Language Pretraining ({VLP}) has achieved impressive performance on many multimodal downstream tasks. However, the requirement for expensive annotations including clean image captions and regional labels limits the scalability of existing approaches, and complicates the pretraining procedure with the introduction of multiple dataset-specific objectives. In this work, we relax these constraints and present a minimalist pretraining framework, named Simple Visual Language Model ({SimVLM}). Unlike prior work, {SimVLM} reduces the training complexity by exploiting large-scale weak supervision, and is trained end-to-end with a single prefix language modeling objective. Without utilizing extra data or task-specific customization, the resulting model significantly outperforms previous pretraining methods and achieves new state-of-the-art results on a wide range of discriminative and generative vision-language benchmarks, including {VQA} (+3.74\% vqa-score), {NLVR}2 (+1.17\% accuracy), {SNLI}-{VE} (+1.37\% accuracy) and image captioning tasks (+10.1\% average {CIDEr} score). Furthermore, we demonstrate that {SimVLM} acquires strong generalization and transfer ability, enabling zero-shot behavior including open-ended visual question answering and cross-modality transfer.},
	eventtitle = {International Conference on Learning Representations},
	author = {Wang, Zirui and Yu, Jiahui and Yu, Adams Wei and Dai, Zihang and Tsvetkov, Yulia and Cao, Yuan},
	urldate = {2023-01-11},
	date = {2022-05-11},
	langid = {english},
	file = {Full Text PDF:/Users/mdmoor/Zotero/storage/ILV95VRG/Wang et al. - 2022 - SimVLM Simple Visual Language Model Pretraining w.pdf:application/pdf},
}

@inproceedings{yasunaga_qa-gnn_2021,
	location = {Online},
	title = {{QA}-{GNN}: Reasoning with Language Models and Knowledge Graphs for Question Answering},
	url = {https://aclanthology.org/2021.naacl-main.45},
	doi = {10.18653/v1/2021.naacl-main.45},
	shorttitle = {{QA}-{GNN}},
	abstract = {The problem of answering questions using knowledge from pre-trained language models ({LMs}) and knowledge graphs ({KGs}) presents two challenges: given a {QA} context (question and answer choice), methods need to (i) identify relevant knowledge from large {KGs}, and (ii) perform joint reasoning over the {QA} context and {KG}. Here we propose a new model, {QA}-{GNN}, which addresses the above challenges through two key innovations: (i) relevance scoring, where we use {LMs} to estimate the importance of {KG} nodes relative to the given {QA} context, and (ii) joint reasoning, where we connect the {QA} context and {KG} to form a joint graph, and mutually update their representations through graph-based message passing. We evaluate {QA}-{GNN} on the {CommonsenseQA} and {OpenBookQA} datasets, and show its improvement over existing {LM} and {LM}+{KG} models, as well as its capability to perform interpretable and structured reasoning, e.g., correctly handling negation in questions.},
	eventtitle = {{NAACL}-{HLT} 2021},
	pages = {535--546},
	booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	author = {Yasunaga, Michihiro and Ren, Hongyu and Bosselut, Antoine and Liang, Percy and Leskovec, Jure},
	urldate = {2023-01-11},
	date = {2021-06},
	file = {Full Text PDF:/Users/mdmoor/Zotero/storage/V2AK85C8/Yasunaga et al. - 2021 - QA-GNN Reasoning with Language Models and Knowled.pdf:application/pdf},
}

@inproceedings{yasunaga_linkbert_2022,
	location = {Dublin, Ireland},
	title = {{LinkBERT}: Pretraining Language Models with Document Links},
	url = {https://aclanthology.org/2022.acl-long.551},
	doi = {10.18653/v1/2022.acl-long.551},
	shorttitle = {{LinkBERT}},
	abstract = {Language model ({LM}) pretraining captures various knowledge from text corpora, helping downstream tasks. However, existing methods such as {BERT} model a single document, and do not capture dependencies or knowledge that span across documents. In this work, we propose {LinkBERT}, an {LM} pretraining method that leverages links between documents, e.g., hyperlinks. Given a text corpus, we view it as a graph of documents and create {LM} inputs by placing linked documents in the same context. We then pretrain the {LM} with two joint self-supervised objectives: masked language modeling and our new proposal, document relation prediction. We show that {LinkBERT} outperforms {BERT} on various downstream tasks across two domains: the general domain (pretrained on Wikipedia with hyperlinks) and biomedical domain (pretrained on {PubMed} with citation links). {LinkBERT} is especially effective for multi-hop reasoning and few-shot {QA} (+5\% absolute improvement on {HotpotQA} and {TriviaQA}), and our biomedical {LinkBERT} sets new states of the art on various {BioNLP} tasks (+7\% on {BioASQ} and {USMLE}). We release our pretrained models, {LinkBERT} and {BioLinkBERT}, as well as code and data.},
	eventtitle = {{ACL} 2022},
	pages = {8003--8016},
	booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Yasunaga, Michihiro and Leskovec, Jure and Liang, Percy},
	urldate = {2023-01-11},
	date = {2022-05},
	file = {Full Text PDF:/Users/mdmoor/Zotero/storage/ITU3442Y/Yasunaga et al. - 2022 - LinkBERT Pretraining Language Models with Documen.pdf:application/pdf},
}
@article{huang2019clinicalbert,
  title={Clinicalbert: Modeling clinical notes and predicting hospital readmission},
  author={Huang, Kexin and Altosaar, Jaan and Ranganath, Rajesh},
  journal={arXiv preprint arXiv:1904.05342},
  year={2019}
}
@article{lee2020biobert,
  title={BioBERT: a pre-trained biomedical language representation model for biomedical text mining},
  author={Lee, Jinhyuk and Yoon, Wonjin and Kim, Sungdong and Kim, Donghyeon and Kim, Sunkyu and So, Chan Ho and Kang, Jaewoo},
  journal={Bioinformatics},
  volume={36},
  number={4},
  pages={1234--1240},
  year={2020},
  publisher={Oxford University Press}
}

@article{liang2022holistic,
  title={Holistic evaluation of language models},
  author={Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and others},
  journal={arXiv preprint arXiv:2211.09110},
  year={2022}
}
@article{qin2023chatgpt,
  title={Is ChatGPT a general-purpose natural language processing task solver?},
  author={Qin, Chengwei and Zhang, Aston and Zhang, Zhuosheng and Chen, Jiaao and Yasunaga, Michihiro and Yang, Diyi},
  journal={arXiv preprint arXiv:2302.06476},
  year={2023}
}
@article{su2019vl,
  title={Vl-bert: Pre-training of generic visual-linguistic representations},
  author={Su, Weijie and Zhu, Xizhou and Cao, Yue and Li, Bin and Lu, Lewei and Wei, Furu and Dai, Jifeng},
  journal={arXiv preprint arXiv:1908.08530},
  year={2019}
}

@article{luo2022biogpt,
  title={BioGPT: generative pre-trained transformer for biomedical text generation and mining},
  author={Luo, Renqian and Sun, Liai and Xia, Yingce and Qin, Tao and Zhang, Sheng and Poon, Hoifung and Liu, Tie-Yan},
  journal={Briefings in Bioinformatics},
  volume={23},
  number={6},
  pages={bbac409},
  year={2022},
  publisher={Oxford University Press}
}

@article{guha_roy_does_2022,
	title = {Does your dermatology classifier know what it doesn’t know? Detecting the long-tail of unseen conditions},
	volume = {75},
	issn = {1361-8415},
	url = {https://www.sciencedirect.com/science/article/pii/S1361841521003194},
	doi = {10.1016/j.media.2021.102274},
	shorttitle = {Does your dermatology classifier know what it doesn’t know?},
	abstract = {Supervised deep learning models have proven to be highly effective in classification of dermatological conditions. These models rely on the availability of abundant labeled training examples. However, in the real-world, many dermatological conditions are individually too infrequent for per-condition classification with supervised learning. Although individually infrequent, these conditions may collectively be common and therefore are clinically significant in aggregate. To prevent models from generating erroneous outputs on such examples, there remains a considerable unmet need for deep learning systems that can better detect such infrequent conditions. These infrequent ‘outlier’ conditions are seen very rarely (or not at all) during training. In this paper, we frame this task as an out-of-distribution ({OOD}) detection problem. We set up a benchmark ensuring that outlier conditions are disjoint between the model training, validation, and test sets. Unlike traditional {OOD} detection benchmarks where the task is to detect dataset distribution shift, we aim at the more challenging task of detecting subtle differences resulting from a different pathology or condition. We propose a novel hierarchical outlier detection ({HOD}) loss, which assigns multiple abstention classes corresponding to each training outlier class and jointly performs a coarse classification of inliers vs. outliers, along with fine-grained classification of the individual classes. We demonstrate that the proposed {HOD} loss based approach outperforms leading methods that leverage outlier data during training. Further, performance is significantly boosted by using recent representation learning methods ({BiT}, {SimCLR}, {MICLe}). Further, we explore ensembling strategies for {OOD} detection and propose a diverse ensemble selection process for the best result. We also perform a subgroup analysis over conditions of varying risk levels and different skin types to investigate how {OOD} performance changes over each subgroup and demonstrate the gains of our framework in comparison to baseline. Furthermore, we go beyond traditional performance metrics and introduce a cost matrix for model trust analysis to approximate downstream clinical impact. We use this cost matrix to compare the proposed method against the baseline, thereby making a stronger case for its effectiveness in real-world scenarios.},
	pages = {102274},
	journaltitle = {Medical Image Analysis},
	shortjournal = {Medical Image Analysis},
	author = {Guha Roy, Abhijit and Ren, Jie and Azizi, Shekoofeh and Loh, Aaron and Natarajan, Vivek and Mustafa, Basil and Pawlowski, Nick and Freyberg, Jan and Liu, Yuan and Beaver, Zach and Vo, Nam and Bui, Peggy and Winter, Samantha and {MacWilliams}, Patricia and Corrado, Greg S. and Telang, Umesh and Liu, Yun and Cemgil, Taylan and Karthikesalingam, Alan and Lakshminarayanan, Balaji and Winkens, Jim},
	urldate = {2023-01-11},
	date = {2022-01-01},
	langid = {english},
	keywords = {Representation learning, Deep learning, Dermatology, Ensembles, Long-tailed recognition, Out-of-distribution detection, Outlier exposure},
	file = {ScienceDirect Snapshot:/Users/mdmoor/Zotero/storage/ELTVJT8Q/S1361841521003194.html:text/html;Submitted Version:/Users/mdmoor/Zotero/storage/QV82GPRM/Guha Roy et al. - 2022 - Does your dermatology classifier know what it does.pdf:application/pdf},
}

@article{vegunta_secondary_2019,
	title = {Secondary Aortoduodenal Fistula Presenting as Gastrointestinal Bleeding and Fungemia},
	volume = {11},
	issn = {2168-8184},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6820657/},
	doi = {10.7759/cureus.5575},
	abstract = {A 55‐year‐old African American man with a history of abdominal aortic pseudoaneurysm repair presented to the {ED} with complaints of black-colored stools mixed with fresh blood and fever for three days duration. The exam was unremarkable except for abdominal bruits and pallor. {CT} angiogram showed perigraft fluid collection, bowel wall thickening, and loss of normal fat planes between the aorta and adjacent bowel at the level of the third portion of the duodenum. Polymicrobial infection was noted in the aortic graft and blood cultures grew Candida. The patient underwent urgent removal of the infected graft, duodenal repair along with appropriate antimicrobial therapy. He did well postoperatively and was discharged in a stable condition. Our case highlights the importance of maintaining a high index of suspicion of aortoenteric fistula ({AEF}) when a patient with a prior abdominal aortic graft develops gastrointestinal ({GI}) bleeding as this condition is universally fatal if unrecognized.},
	pages = {e5575},
	number = {9},
	journaltitle = {Cureus},
	shortjournal = {Cureus},
	author = {Vegunta, Radhakrishna and Vegunta, Rathnamitreyee and Kutti Sridharan, Gurusaravanan},
	urldate = {2023-01-11},
	date = {2019},
	pmid = {31695994},
	pmcid = {PMC6820657},
	file = {PubMed Central Full Text PDF:/Users/mdmoor/Zotero/storage/UM3BUCIW/Vegunta et al. - Secondary Aortoduodenal Fistula Presenting as Gast.pdf:application/pdf},
}

@misc{radford_robust_2022,
	title = {Robust Speech Recognition via Large-Scale Weak Supervision},
	url = {http://arxiv.org/abs/2212.04356},
	doi = {10.48550/arXiv.2212.04356},
	abstract = {We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zero-shot transfer setting without the need for any fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.},
	number = {{arXiv}:2212.04356},
	publisher = {{arXiv}},
	author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and {McLeavey}, Christine and Sutskever, Ilya},
	urldate = {2023-01-11},
	date = {2022-12-06},
	eprinttype = {arxiv},
	eprint = {2212.04356 [cs, eess]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv Fulltext PDF:/Users/mdmoor/Zotero/storage/8GEKPMP6/Radford et al. - 2022 - Robust Speech Recognition via Large-Scale Weak Sup.pdf:application/pdf;arXiv.org Snapshot:/Users/mdmoor/Zotero/storage/TTZ3K6GQ/2212.html:text/html},
}

@article{dixon_virtual_2020,
	title = {A Virtual Type 2 Diabetes Clinic Using Continuous Glucose Monitoring and Endocrinology Visits},
	volume = {14},
	issn = {1932-2968},
	url = {https://doi.org/10.1177/1932296819888662},
	doi = {10.1177/1932296819888662},
	abstract = {The Onduo Virtual Diabetes Clinic ({VDC}) telehealth technology/care model for adults with type 2 diabetes (T2D) combines connected devices, remote lifestyle coaching, and clinical support with a mobile App. Key differentiating program features are the availability of live video consultations with board-certified endocrinologists for medication management and real-time continuous glucose monitor use for higher-risk participants. Preliminary data (n = 740) suggest that participation was associated with a significant improvement in {HbA}1c with up to 6 months follow-up in those not meeting treatment targets. {HbA}1c decreased by 2.3\% ± 1.9\%, 0.7\% ± 1.0\%, and 0.2\% ± 0.8\% across baseline categories of {\textgreater}9.0\%, 8.0\%-9.0\% and 7.0\% to {\textless}8.0\%, respectively (all P {\textless} .001). These findings suggest that the {VDC} has potential to support individuals with T2D and their clinicians in diabetes management between office visits.},
	pages = {908--911},
	number = {5},
	journaltitle = {Journal of Diabetes Science and Technology},
	shortjournal = {J Diabetes Sci Technol},
	author = {Dixon, Ronald F. and Zisser, Howard and Layne, Jennifer E. and Barleen, Nathan A. and Miller, David P. and Moloney, Daniel P. and Majithia, Amit R. and Gabbay, Robert A. and Riff, Josh},
	urldate = {2023-01-11},
	date = {2020-09-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	file = {SAGE PDF Full Text:/Users/mdmoor/Zotero/storage/TCFKD7MH/Dixon et al. - 2020 - A Virtual Type 2 Diabetes Clinic Using Continuous .pdf:application/pdf},
}

@article{kucera_conditional_2022,
	title = {Conditional generative modeling for de novo protein design with hierarchical functions},
	volume = {38},
	issn = {1367-4803},
	url = {https://doi.org/10.1093/bioinformatics/btac353},
	doi = {10.1093/bioinformatics/btac353},
	abstract = {Protein design has become increasingly important for medical and biotechnological applications. Because of the complex mechanisms underlying protein formation, the creation of a novel protein requires tedious and time-consuming computational or experimental protocols. At the same time, machine learning has enabled the solving of complex problems by leveraging large amounts of available data, more recently with great improvements on the domain of generative modeling. Yet, generative models have mainly been applied to specific sub-problems of protein design.Here, we approach the problem of general-purpose protein design conditioned on functional labels of the hierarchical Gene Ontology. Since a canonical way to evaluate generative models in this domain is missing, we devise an evaluation scheme of several biologically and statistically inspired metrics. We then develop the conditional generative adversarial network {ProteoGAN} and show that it outperforms several classic and more recent deep-learning baselines for protein sequence generation. We further give insights into the model by analyzing hyperparameters and ablation baselines. Lastly, we hypothesize that a functionally conditional model could generate proteins with novel functions by combining labels and provide first steps into this direction of research.The code and data underlying this article are available on {GitHub} at https://github.com/timkucera/proteogan, and can be accessed with doi:10.5281/zenodo.6591379.Supplemental data are available at Bioinformatics online.},
	pages = {3454--3461},
	number = {13},
	journaltitle = {Bioinformatics},
	shortjournal = {Bioinformatics},
	author = {Kucera, Tim and Togninalli, Matteo and Meng-Papaxanthos, Laetitia},
	urldate = {2023-01-11},
	date = {2022-07-01},
	file = {Full Text PDF:/Users/mdmoor/Zotero/storage/IHUXGG2A/Kucera et al. - 2022 - Conditional generative modeling for de novo protei.pdf:application/pdf;Snapshot:/Users/mdmoor/Zotero/storage/VJVKS9KQ/6593486.html:text/html},
}

@inproceedings{ramesh_zero-shot_2021,
	title = {Zero-Shot Text-to-Image Generation},
	url = {https://proceedings.mlr.press/v139/ramesh21a.html},
	abstract = {Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.},
	eventtitle = {International Conference on Machine Learning},
	pages = {8821--8831},
	booktitle = {Proceedings of the 38th International Conference on Machine Learning},
	author = {Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
	urldate = {2023-01-11},
	date = {2021-07-01},
	langid = {english},
	note = {{ISSN}: 2640-3498},
	file = {Full Text PDF:/Users/mdmoor/Zotero/storage/W8DBW9Q6/Ramesh et al. - 2021 - Zero-Shot Text-to-Image Generation.pdf:application/pdf},
}

@inproceedings{rombach_high-resolution_2022,
	title = {High-Resolution Image Synthesis With Latent Diffusion Models},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.html},
	eventtitle = {Proceedings of the {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition},
	pages = {10684--10695},
	author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Björn},
	urldate = {2023-01-11},
	date = {2022},
	langid = {english},
	file = {Full Text PDF:/Users/mdmoor/Zotero/storage/BTV6PXG9/Rombach et al. - 2022 - High-Resolution Image Synthesis With Latent Diffus.pdf:application/pdf},
}

@misc{zvyagin_genslms_2022,
	title = {{GenSLMs}: Genome-scale language models reveal {SARS}-{CoV}-2 evolutionary dynamics},
	rights = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NonCommercial}-{NoDerivs} 4.0 International), {CC} {BY}-{NC}-{ND} 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2022.10.10.511571v1},
	doi = {10.1101/2022.10.10.511571},
	shorttitle = {{GenSLMs}},
	abstract = {Our work seeks to transform how new and emergent variants of pandemic causing viruses, specially {SARS}-{CoV}-2, are identified and classified. By adapting large language models ({LLMs}) for genomic data, we build genome-scale language models ({GenSLMs}) which can learn the evolutionary landscape of {SARS}-{CoV}-2 genomes. By pretraining on over 110 million prokaryotic gene sequences, and then finetuning a {SARS}-{CoV}-2 specific model on 1.5 million genomes, we show that {GenSLM} can accurately and rapidly identify variants of concern. Thus, to our knowledge, {GenSLM} represents one of the first whole genome scale foundation models which can generalize to other prediction tasks. We demonstrate the scaling of {GenSLMs} on both {GPU}-based supercomputers and {AI}-hardware accelerators, achieving over 1.54 zettaflops in training runs. We present initial scientific insights gleaned from examining {GenSLMs} in tracking the evolutionary dynamics of {SARS}-{CoV}-2, noting that its full potential on large biological data is yet to be realized.},
	publisher = {{bioRxiv}},
	author = {Zvyagin, Maxim and Brace, Alexander and Hippe, Kyle and Deng, Yuntian and Zhang, Bin and Bohorquez, Cindy Orozco and Clyde, Austin and Kale, Bharat and Perez-Rivera, Danilo and Ma, Heng and Mann, Carla M. and Irvin, Michael and Pauloski, J. Gregory and Ward, Logan and Hayot, Valerie and Emani, Murali and Foreman, Sam and Xie, Zhen and Lin, Diangen and Shukla, Maulik and Nie, Weili and Romero, Josh and Dallago, Christian and Vahdat, Arash and Xiao, Chaowei and Gibbs, Thomas and Foster, Ian and Davis, James J. and Papka, Michael E. and Brettin, Thomas and Stevens, Rick and Anandkumar, Anima and Vishwanath, Venkatram and Ramanathan, Arvind},
	urldate = {2023-01-11},
	date = {2022-10-11},
	langid = {english},
	note = {Pages: 2022.10.10.511571
Section: New Results},
	file = {Full Text PDF:/Users/mdmoor/Zotero/storage/IQSHA8FN/Zvyagin et al. - 2022 - GenSLMs Genome-scale language models reveal SARS-.pdf:application/pdf},
}

@article{jumper_highly_2021,
	title = {Highly accurate protein structure prediction with {AlphaFold}},
	volume = {596},
	rights = {2021 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-021-03819-2},
	doi = {10.1038/s41586-021-03819-2},
	abstract = {Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort1–4, the structures of around 100,000 unique proteins have been determined5, but this represents a small fraction of the billions of known protein sequences6,7. Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence—the structure prediction component of the ‘protein folding problem’8—has been an important open research problem for more than 50 years9. Despite recent progress10–14, existing methods fall far short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, {AlphaFold}, in the challenging 14th Critical Assessment of protein Structure Prediction ({CASP}14)15, demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of {AlphaFold} is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.},
	pages = {583--589},
	number = {7873},
	journaltitle = {Nature},
	author = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and Žídek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A. A. and Ballard, Andrew J. and Cowie, Andrew and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W. and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},
	urldate = {2023-01-11},
	date = {2021-08},
	langid = {english},
	note = {Number: 7873
Publisher: Nature Publishing Group},
	keywords = {Machine learning, Computational biophysics, Protein structure predictions, Structural biology},
	file = {Full Text PDF:/Users/mdmoor/Zotero/storage/LHCFZRHS/Jumper et al. - 2021 - Highly accurate protein structure prediction with .pdf:application/pdf},
}

@misc{watson_broadly_2022,
	title = {Broadly applicable and accurate protein design by integrating structure prediction networks and diffusion generative models},
	rights = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NoDerivs} 4.0 International), {CC} {BY}-{ND} 4.0, as described at http://creativecommons.org/licenses/by-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2022.12.09.519842v1},
	doi = {10.1101/2022.12.09.519842},
	abstract = {There has been considerable recent progress in designing new proteins using deep learning methods1–9. Despite this progress, a general deep learning framework for protein design that enables solution of a wide range of design challenges, including de novo binder design and design of higher order symmetric architectures, has yet to be described. Diffusion models10,11 have had considerable success in image and language generative modeling but limited success when applied to protein modeling, likely due to the complexity of protein backbone geometry and sequence-structure relationships. Here we show that by fine tuning the {RoseTTAFold} structure prediction network on protein structure denoising tasks, we obtain a generative model of protein backbones that achieves outstanding performance on unconditional and topology-constrained protein monomer design, protein binder design, symmetric oligomer design, enzyme active site scaffolding, and symmetric motif scaffolding for therapeutic and metal-binding protein design. We demonstrate the power and generality of the method, called {RoseTTAFold} Diffusion ({RFdiffusion}), by experimentally characterizing the structures and functions of hundreds of new designs. In a manner analogous to networks which produce images from user-specified inputs, {RFdiffusion} enables the design of diverse, complex, functional proteins from simple molecular specifications.},
	publisher = {{bioRxiv}},
	author = {Watson, Joseph L. and Juergens, David and Bennett, Nathaniel R. and Trippe, Brian L. and Yim, Jason and Eisenach, Helen E. and Ahern, Woody and Borst, Andrew J. and Ragotte, Robert J. and Milles, Lukas F. and Wicky, Basile I. M. and Hanikel, Nikita and Pellock, Samuel J. and Courbet, Alexis and Sheffler, William and Wang, Jue and Venkatesh, Preetham and Sappington, Isaac and Torres, Susana Vázquez and Lauko, Anna and Bortoli, Valentin De and Mathieu, Emile and Barzilay, Regina and Jaakkola, Tommi S. and {DiMaio}, Frank and Baek, Minkyung and Baker, David},
	urldate = {2023-01-11},
	date = {2022-12-10},
	langid = {english},
	note = {Pages: 2022.12.09.519842
Section: New Results},
	file = {Full Text PDF:/Users/mdmoor/Zotero/storage/8JDDA2MC/Watson et al. - 2022 - Broadly applicable and accurate protein design by .pdf:application/pdf},
}

@inproceedings{radford_learning_2021,
	title = {Learning Transferable Visual Models From Natural Language Supervision},
	url = {https://proceedings.mlr.press/v139/radford21a.html},
	abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn {SOTA} image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as {OCR}, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original {ResNet}-50 on {ImageNet} zero-shot without needing to use any of the 1.28 million training examples it was trained on.},
	eventtitle = {International Conference on Machine Learning},
	pages = {8748--8763},
	booktitle = {Proceedings of the 38th International Conference on Machine Learning},
	author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
	urldate = {2023-01-11},
	date = {2021-07-01},
	langid = {english},
	note = {{ISSN}: 2640-3498},
	file = {Full Text PDF:/Users/mdmoor/Zotero/storage/R5FP8P45/Radford et al. - 2021 - Learning Transferable Visual Models From Natural L.pdf:application/pdf;Supplementary PDF:/Users/mdmoor/Zotero/storage/HC4FGMZK/Radford et al. - 2021 - Learning Transferable Visual Models From Natural L.pdf:application/pdf},
}

@article{theuniprotconsortium_uniprot_2017,
	title = {{UniProt}: the universal protein knowledgebase},
	volume = {45},
	issn = {0305-1048},
	url = {https://doi.org/10.1093/nar/gkw1099},
	doi = {10.1093/nar/gkw1099},
	shorttitle = {{UniProt}},
	abstract = {The {UniProt} knowledgebase is a large resource of protein sequences and associated detailed annotation. The database contains over 60 million sequences, of which over half a million sequences have been curated by experts who critically review experimental and predicted data for each protein. The remainder are automatically annotated based on rule systems that rely on the expert curated knowledge. Since our last update in 2014, we have more than doubled the number of reference proteomes to 5631, giving a greater coverage of taxonomic diversity. We implemented a pipeline to remove redundant highly similar proteomes that were causing excessive redundancy in {UniProt}. The initial run of this pipeline reduced the number of sequences in {UniProt} by 47 million. For our users interested in the accessory proteomes, we have made available sets of pan proteome sequences that cover the diversity of sequences for each species that is found in its strains and sub-strains. To help interpretation of genomic variants, we provide tracks of detailed protein information for the major genome browsers. We provide a {SPARQL} endpoint that allows complex queries of the more than 22 billion triples of data in {UniProt} (http://sparql.uniprot.org/). {UniProt} resources can be accessed via the website at http://www.uniprot.org/.},
	pages = {D158--D169},
	issue = {D1},
	journaltitle = {Nucleic Acids Research},
	shortjournal = {Nucleic Acids Research},
	author = {{The UniProt Consortium}},
	urldate = {2023-01-11},
	date = {2017-01-04},
	file = {Full Text PDF:/Users/mdmoor/Zotero/storage/CEZP3ZEK/The UniProt Consortium - 2017 - UniProt the universal protein knowledgebase.pdf:application/pdf;Snapshot:/Users/mdmoor/Zotero/storage/WP2B5MXX/2605721.html:text/html},
}

@article{finlayson_clinician_2021,
	title = {The Clinician and Dataset Shift in Artificial Intelligence},
	volume = {385},
	issn = {0028-4793},
	url = {https://doi.org/10.1056/NEJMc2104626},
	doi = {10.1056/NEJMc2104626},
	pages = {283--286},
	number = {3},
	journaltitle = {New England Journal of Medicine},
	author = {Finlayson, Samuel G. and Subbaswamy, Adarsh and Singh, Karandeep and Bowers, John and Kupke, Annabel and Zittrain, Jonathan and Kohane, Isaac S. and Saria, Suchi},
	urldate = {2023-01-11},
	date = {2021-07-15},
	pmid = {34260843},
	note = {Publisher: Massachusetts Medical Society
\_eprint: https://doi.org/10.1056/{NEJMc}2104626},
	file = {Full Text PDF:/Users/mdmoor/Zotero/storage/ZNCY4YRJ/Finlayson et al. - 2021 - The Clinician and Dataset Shift in Artificial Inte.pdf:application/pdf},
}

@article{guo_systematic_2021,
	title = {Systematic Review of Approaches to Preserve Machine Learning Performance in the Presence of Temporal Dataset Shift in Clinical Medicine},
	volume = {12},
	rights = {Georg Thieme Verlag {KG} Rüdigerstraße 14, 70469 Stuttgart, Germany},
	issn = {1869-0327},
	url = {http://www.thieme-connect.de/DOI/DOI?10.1055/s-0041-1735184},
	doi = {10.1055/s-0041-1735184},
	abstract = {Objective The change in performance of machine learning models over time as a result of temporal dataset shift is a barrier to machine learning-derived models facilitating decision-making in clinical practice. Our aim was to describe technical procedures used to preserve the performance of machine learning models in the presence of temporal dataset shifts.

  Methods Studies were included if they were fully published articles that used machine learning and implemented a procedure to mitigate the effects of temporal dataset shift in a clinical setting. We described how dataset shift was measured, the procedures used to preserve model performance, and their effects.

  Results Of 4,457 potentially relevant publications identified, 15 were included. The impact of temporal dataset shift was primarily quantified using changes, usually deterioration, in calibration or discrimination. Calibration deterioration was more common (n = 11) than discrimination deterioration (n = 3). Mitigation strategies were categorized as model level or feature level. Model-level approaches (n = 15) were more common than feature-level approaches (n = 2), with the most common approaches being model refitting (n = 12), probability calibration (n = 7), model updating (n = 6), and model selection (n = 6). In general, all mitigation strategies were successful at preserving calibration but not uniformly successful in preserving discrimination.

  Conclusion There was limited research in preserving the performance of machine learning models in the presence of temporal dataset shift in clinical medicine. Future research could focus on the impact of dataset shift on clinical decision making, benchmark the mitigation strategies on a wider range of datasets and tasks, and identify optimal strategies for specific settings.},
	pages = {808--815},
	number = {4},
	journaltitle = {Applied Clinical Informatics},
	shortjournal = {Appl Clin Inform},
	author = {Guo, Lin Lawrence and Pfohl, Stephen R. and Fries, Jason and Posada, Jose and Fleming, Scott Lanyon and Aftandilian, Catherine and Shah, Nigam and Sung, Lillian},
	urldate = {2023-01-11},
	date = {2021-08},
	langid = {english},
	note = {Publisher: Georg Thieme Verlag {KG}},
	keywords = {clinical data, dataset shift, machine learning, systematic review},
	file = {Full Text PDF:/Users/mdmoor/Zotero/storage/88D52BK9/Guo et al. - 2021 - Systematic Review of Approaches to Preserve Machin.pdf:application/pdf},
}

@misc{lampinen_can_2022,
	title = {Can language models learn from explanations in context?},
	url = {http://arxiv.org/abs/2204.02329},
	doi = {10.48550/arXiv.2204.02329},
	abstract = {Language Models ({LMs}) can perform new tasks by adapting to a few in-context examples. For humans, explanations that connect examples to task principles can improve learning. We therefore investigate whether explanations of few-shot examples can help {LMs}. We annotate questions from 40 challenging tasks with answer explanations, and various matched control explanations. We evaluate how different types of explanations, instructions, and controls affect zero- and few-shot performance. We analyze these results using statistical multilevel modeling techniques that account for the nested dependencies among conditions, tasks, prompts, and models. We find that explanations can improve performance -- even without tuning. Furthermore, explanations hand-tuned for performance on a small validation set offer substantially larger benefits, and building a prompt by selecting examples and explanations together substantially improves performance over selecting examples alone. Finally, even untuned explanations outperform carefully matched controls, suggesting that the benefits are due to the link between an example and its explanation, rather than lower-level features. However, only large models benefit. In summary, explanations can support the in-context learning of large {LMs} on challenging tasks.},
	number = {{arXiv}:2204.02329},
	publisher = {{arXiv}},
	author = {Lampinen, Andrew K. and Dasgupta, Ishita and Chan, Stephanie C. Y. and Matthewson, Kory and Tessler, Michael Henry and Creswell, Antonia and {McClelland}, James L. and Wang, Jane X. and Hill, Felix},
	urldate = {2023-01-11},
	date = {2022-10-10},
	eprinttype = {arxiv},
	eprint = {2204.02329 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/mdmoor/Zotero/storage/M6L4H56X/Lampinen et al. - 2022 - Can language models learn from explanations in con.pdf:application/pdf;arXiv.org Snapshot:/Users/mdmoor/Zotero/storage/FAWP2XR4/2204.html:text/html},
}

@article{yoon_chest_2023,
	title = {Chest {CT} Findings in Hospitalized Patients with {SARS}-{CoV}-2: Delta versus Omicron Variants},
	volume = {306},
	issn = {0033-8419},
	url = {https://pubs.rsna.org/doi/10.1148/radiol.220676},
	doi = {10.1148/radiol.220676},
	shorttitle = {Chest {CT} Findings in Hospitalized Patients with {SARS}-{CoV}-2},
	abstract = {Background

{CT} manifestations of {SARS}-{CoV}-2 may differ among variants.

Purpose

To compare the chest {CT} findings of {SARS}-{CoV}-2 between the Delta and Omicron variants.

Materials and Methods

This retrospective study collected consecutive baseline chest {CT} images of hospitalized patients with {SARS}-{CoV}-2 from a secondary referral hospital when the Delta and Omicron variants were predominant. Two radiologists categorized {CT} images according to the {RSNA} classification system for {COVID}-19 and visually graded pneumonia extent. Pneumonia, pleural effusion, and intrapulmonary vessels were segmented and quantified on {CT} images using a priori–developed neural networks, followed by reader confirmation. Multivariable logistic and linear regression analyses were performed to examine the associations between the variants and {CT} category, distribution, severity, and peripheral vascularity.

Results

In total, 88 patients with the Delta variant (mean age, 67 years ± 15 [{SD}]; 46 men) and 88 patients with the Omicron variant (mean age, 62 years ± 19; 51 men) were included. Omicron was associated with less frequent, typical peripheral bilateral ground-glass opacity (32\% [28 of 88] vs 57\% [50 of 88], P = .001), more frequent peribronchovascular predilection (38\% [25 of 66] vs 7\% [five of 71], P {\textless} .001), lower visual pneumonia extent (5.4 ± 6.0 vs 7.7 ± 6.6, P = .02), similar pneumonia volume (5\% ± 1 vs 7\% ± 11, P = .14), and a higher proportion of vessels with a cross-sectional area smaller than 5 mm2 relative to the total pulmonary blood volume ({BV}5\%; 48\% ± 11 vs 44\% ± 8; P = .004). In adjusted analyses, Omicron was associated with a nontypical appearance (odds ratio, 0.34; P = .006), peribronchovascular predilection (odds ratio, 9.2; P {\textless} .001), and higher {BV}5\% (β = 3.8; P = .01) but similar visual pneumonia extent (P = .17) and pneumonia volume (P = .67) relative to the Delta variant.

Conclusion

At chest {CT}, the Omicron {SARS}-{COV}-2 variant showed nontypical peribronchovascular pneumonia and less pulmonary vascular involvement than did the Delta variant in hospitalized patients with similar disease severity.

© {RSNA}, 2022

Online supplemental material is available for this article.},
	pages = {252--260},
	number = {1},
	journaltitle = {Radiology},
	author = {Yoon, Soon Ho and Lee, Jong Hyuk and Kim, Baek-Nam},
	urldate = {2023-01-11},
	date = {2023-01},
	note = {Publisher: Radiological Society of North America},
	file = {Full Text PDF:/Users/mdmoor/Zotero/storage/DIMKEGJW/Yoon et al. - 2023 - Chest CT Findings in Hospitalized Patients with SA.pdf:application/pdf},
}

@inproceedings{ouyang_training_2022,
	title = {Training language models to follow instructions with human feedback},
	url = {https://openreview.net/forum?id=TG8KACxEON},
	abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through a language model {API}, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune {GPT}-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models {InstructGPT}. In human evaluations on our prompt distribution, outputs from the 1.3B parameter {InstructGPT} model are preferred to outputs from the 175B {GPT}-3, despite having 100x fewer parameters. Moreover, {InstructGPT} models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public {NLP} datasets. Even though {InstructGPT} still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
	eventtitle = {Advances in Neural Information Processing Systems},
	author = {Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Gray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
	urldate = {2023-01-11},
	date = {2022-10-31},
	langid = {english},
	file = {Full Text PDF:/Users/mdmoor/Zotero/storage/4ATFNKYA/Ouyang et al. - 2022 - Training language models to follow instructions wi.pdf:application/pdf},
}

@online{noauthor_openai_2021,
	title = {{OpenAI}},
	url = {https://openai.com/blog/gpt-3-apps/},
	abstract = {Over 300 applications are delivering {GPT}-3–powered search, conversation, text completion, and other advanced {AI} features through our {API}.},
	urldate = {2022-10-15},
	date = {2021-03-25},
	langid = {english},
	file = {Snapshot:/Users/mdmoor/Zotero/storage/L5YBLDUS/gpt-3-apps.html:text/html},
}

@misc{burns_discovering_2022,
	title = {Discovering Latent Knowledge in Language Models Without Supervision},
	url = {http://arxiv.org/abs/2212.03827},
	doi = {10.48550/arXiv.2212.03827},
	abstract = {Existing techniques for training language models can be misaligned with the truth: if we train models with imitation learning, they may reproduce errors that humans make; if we train them to generate text that humans rate highly, they may output errors that human evaluators can't detect. We propose circumventing this issue by directly finding latent knowledge inside the internal activations of a language model in a purely unsupervised way. Specifically, we introduce a method for accurately answering yes-no questions given only unlabeled model activations. It works by finding a direction in activation space that satisfies logical consistency properties, such as that a statement and its negation have opposite truth values. We show that despite using no supervision and no model outputs, our method can recover diverse knowledge represented in large language models: across 6 models and 10 question-answering datasets, it outperforms zero-shot accuracy by 4{\textbackslash}\% on average. We also find that it cuts prompt sensitivity in half and continues to maintain high accuracy even when models are prompted to generate incorrect answers. Our results provide an initial step toward discovering what language models know, distinct from what they say, even when we don't have access to explicit ground truth labels.},
	number = {{arXiv}:2212.03827},
	publisher = {{arXiv}},
	author = {Burns, Collin and Ye, Haotian and Klein, Dan and Steinhardt, Jacob},
	urldate = {2023-01-11},
	date = {2022-12-07},
	eprinttype = {arxiv},
	eprint = {2212.03827 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/mdmoor/Zotero/storage/P2HG4BPT/Burns et al. - 2022 - Discovering Latent Knowledge in Language Models Wi.pdf:application/pdf;arXiv.org Snapshot:/Users/mdmoor/Zotero/storage/GJINT7WA/2212.html:text/html},
}

@collection{cirillo_sex_2022,
	location = {Amsterdam},
	title = {Sex and gender bias in technology and artificial intelligence: biomedicine and healthcare applications},
	isbn = {978-0-12-821392-6},
	series = {{WBP} book series},
	shorttitle = {Sex and gender bias in technology and artificial intelligence},
	pagetotal = {252},
	publisher = {Academic Press},
	editor = {Cirillo, Davide and Catuara-Solarz, Silvina and Guney, Emre},
	date = {2022},
}

@article{obermeyer_dissecting_2019,
	title = {Dissecting racial bias in an algorithm used to manage the health of populations},
	volume = {366},
	url = {https://www.science.org/doi/10.1126/science.aax2342},
	doi = {10.1126/science.aax2342},
	abstract = {Health systems rely on commercial prediction algorithms to identify and help patients with complex health needs. We show that a widely used algorithm, typical of this industry-wide approach and affecting millions of patients, exhibits significant racial bias: At a given risk score, Black patients are considerably sicker than White patients, as evidenced by signs of uncontrolled illnesses. Remedying this disparity would increase the percentage of Black patients receiving additional help from 17.7 to 46.5\%. The bias arises because the algorithm predicts health care costs rather than illness, but unequal access to care means that we spend less money caring for Black patients than for White patients. Thus, despite health care cost appearing to be an effective proxy for health by some measures of predictive accuracy, large racial biases arise. We suggest that the choice of convenient, seemingly effective proxies for ground truth can be an important source of algorithmic bias in many contexts.},
	pages = {447--453},
	number = {6464},
	journaltitle = {Science},
	author = {Obermeyer, Ziad and Powers, Brian and Vogeli, Christine and Mullainathan, Sendhil},
	urldate = {2023-01-11},
	date = {2019-10-25},
	note = {Publisher: American Association for the Advancement of Science},
	file = {Full Text:/Users/mdmoor/Zotero/storage/ZQZTY9CG/Obermeyer et al. - 2019 - Dissecting racial bias in an algorithm used to man.pdf:application/pdf},
}

@misc{srivastava_beyond_2022,
	title = {Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models},
	url = {http://arxiv.org/abs/2206.04615},
	doi = {10.48550/arXiv.2206.04615},
	shorttitle = {Beyond the Imitation Game},
	abstract = {Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark ({BIG}-bench). {BIG}-bench currently consists of 204 tasks, contributed by 442 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. {BIG}-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of {OpenAI}'s {GPT} models, Google-internal dense transformer architectures, and Switch-style sparse transformers on {BIG}-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit "breakthrough" behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting.},
	number = {{arXiv}:2206.04615},
	publisher = {{arXiv}},
	author = {Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R. and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adrià and Kluska, Agnieszka and Lewkowycz, Aitor and Agarwal, Akshat and Power, Alethea and Ray, Alex and Warstadt, Alex and Kocurek, Alexander W. and Safaya, Ali and Tazarv, Ali and Xiang, Alice and Parrish, Alicia and Nie, Allen and Hussain, Aman and Askell, Amanda and Dsouza, Amanda and Slone, Ambrose and Rahane, Ameet and Iyer, Anantharaman S. and Andreassen, Anders and Madotto, Andrea and Santilli, Andrea and Stuhlmüller, Andreas and Dai, Andrew and La, Andrew and Lampinen, Andrew and Zou, Andy and Jiang, Angela and Chen, Angelica and Vuong, Anh and Gupta, Animesh and Gottardi, Anna and Norelli, Antonio and Venkatesh, Anu and Gholamidavoodi, Arash and Tabassum, Arfa and Menezes, Arul and Kirubarajan, Arun and Mullokandov, Asher and Sabharwal, Ashish and Herrick, Austin and Efrat, Avia and Erdem, Aykut and Karakaş, Ayla and Roberts, B. Ryan and Loe, Bao Sheng and Zoph, Barret and Bojanowski, Bartłomiej and Özyurt, Batuhan and Hedayatnia, Behnam and Neyshabur, Behnam and Inden, Benjamin and Stein, Benno and Ekmekci, Berk and Lin, Bill Yuchen and Howald, Blake and Diao, Cameron and Dour, Cameron and Stinson, Catherine and Argueta, Cedrick and Ramírez, César Ferri and Singh, Chandan and Rathkopf, Charles and Meng, Chenlin and Baral, Chitta and Wu, Chiyu and Callison-Burch, Chris and Waites, Chris and Voigt, Christian and Manning, Christopher D. and Potts, Christopher and Ramirez, Cindy and Rivera, Clara E. and Siro, Clemencia and Raffel, Colin and Ashcraft, Courtney and Garbacea, Cristina and Sileo, Damien and Garrette, Dan and Hendrycks, Dan and Kilman, Dan and Roth, Dan and Freeman, Daniel and Khashabi, Daniel and Levy, Daniel and González, Daniel Moseguí and Perszyk, Danielle and Hernandez, Danny and Chen, Danqi and Ippolito, Daphne and Gilboa, Dar and Dohan, David and Drakard, David and Jurgens, David and Datta, Debajyoti and Ganguli, Deep and Emelin, Denis and Kleyko, Denis and Yuret, Deniz and Chen, Derek and Tam, Derek and Hupkes, Dieuwke and Misra, Diganta and Buzan, Dilyar and Mollo, Dimitri Coelho and Yang, Diyi and Lee, Dong-Ho and Shutova, Ekaterina and Cubuk, Ekin Dogus and Segal, Elad and Hagerman, Eleanor and Barnes, Elizabeth and Donoway, Elizabeth and Pavlick, Ellie and Rodola, Emanuele and Lam, Emma and Chu, Eric and Tang, Eric and Erdem, Erkut and Chang, Ernie and Chi, Ethan A. and Dyer, Ethan and Jerzak, Ethan and Kim, Ethan and Manyasi, Eunice Engefu and Zheltonozhskii, Evgenii and Xia, Fanyue and Siar, Fatemeh and Martínez-Plumed, Fernando and Happé, Francesca and Chollet, Francois and Rong, Frieda and Mishra, Gaurav and Winata, Genta Indra and de Melo, Gerard and Kruszewski, Germán and Parascandolo, Giambattista and Mariani, Giorgio and Wang, Gloria and Jaimovitch-López, Gonzalo and Betz, Gregor and Gur-Ari, Guy and Galijasevic, Hana and Kim, Hannah and Rashkin, Hannah and Hajishirzi, Hannaneh and Mehta, Harsh and Bogar, Hayden and Shevlin, Henry and Schütze, Hinrich and Yakura, Hiromu and Zhang, Hongming and Wong, Hugh Mee and Ng, Ian and Noble, Isaac and Jumelet, Jaap and Geissinger, Jack and Kernion, Jackson and Hilton, Jacob and Lee, Jaehoon and Fisac, Jaime Fernández and Simon, James B. and Koppel, James and Zheng, James and Zou, James and Kocoń, Jan and Thompson, Jana and Kaplan, Jared and Radom, Jarema and Sohl-Dickstein, Jascha and Phang, Jason and Wei, Jason and Yosinski, Jason and Novikova, Jekaterina and Bosscher, Jelle and Marsh, Jennifer and Kim, Jeremy and Taal, Jeroen and Engel, Jesse and Alabi, Jesujoba and Xu, Jiacheng and Song, Jiaming and Tang, Jillian and Waweru, Joan and Burden, John and Miller, John and Balis, John U. and Berant, Jonathan and Frohberg, Jörg and Rozen, Jos and Hernandez-Orallo, Jose and Boudeman, Joseph and Jones, Joseph and Tenenbaum, Joshua B. and Rule, Joshua S. and Chua, Joyce and Kanclerz, Kamil and Livescu, Karen and Krauth, Karl and Gopalakrishnan, Karthik and Ignatyeva, Katerina and Markert, Katja and Dhole, Kaustubh D. and Gimpel, Kevin and Omondi, Kevin and Mathewson, Kory and Chiafullo, Kristen and Shkaruta, Ksenia and Shridhar, Kumar and {McDonell}, Kyle and Richardson, Kyle and Reynolds, Laria and Gao, Leo and Zhang, Li and Dugan, Liam and Qin, Lianhui and Contreras-Ochando, Lidia and Morency, Louis-Philippe and Moschella, Luca and Lam, Lucas and Noble, Lucy and Schmidt, Ludwig and He, Luheng and Colón, Luis Oliveros and Metz, Luke and Şenel, Lütfi Kerem and Bosma, Maarten and Sap, Maarten and ter Hoeve, Maartje and Farooqi, Maheen and Faruqui, Manaal and Mazeika, Mantas and Baturan, Marco and Marelli, Marco and Maru, Marco and Quintana, Maria Jose Ramírez and Tolkiehn, Marie and Giulianelli, Mario and Lewis, Martha and Potthast, Martin and Leavitt, Matthew L. and Hagen, Matthias and Schubert, Mátyás and Baitemirova, Medina Orduna and Arnaud, Melody and {McElrath}, Melvin and Yee, Michael A. and Cohen, Michael and Gu, Michael and Ivanitskiy, Michael and Starritt, Michael and Strube, Michael and Swędrowski, Michał and Bevilacqua, Michele and Yasunaga, Michihiro and Kale, Mihir and Cain, Mike and Xu, Mimee and Suzgun, Mirac and Tiwari, Mo and Bansal, Mohit and Aminnaseri, Moin and Geva, Mor and Gheini, Mozhdeh and T, Mukund Varma and Peng, Nanyun and Chi, Nathan and Lee, Nayeon and Krakover, Neta Gur-Ari and Cameron, Nicholas and Roberts, Nicholas and Doiron, Nick and Nangia, Nikita and Deckers, Niklas and Muennighoff, Niklas and Keskar, Nitish Shirish and Iyer, Niveditha S. and Constant, Noah and Fiedel, Noah and Wen, Nuan and Zhang, Oliver and Agha, Omar and Elbaghdadi, Omar and Levy, Omer and Evans, Owain and Casares, Pablo Antonio Moreno and Doshi, Parth and Fung, Pascale and Liang, Paul Pu and Vicol, Paul and Alipoormolabashi, Pegah and Liao, Peiyuan and Liang, Percy and Chang, Peter and Eckersley, Peter and Htut, Phu Mon and Hwang, Pinyu and Miłkowski, Piotr and Patil, Piyush and Pezeshkpour, Pouya and Oli, Priti and Mei, Qiaozhu and Lyu, Qing and Chen, Qinlang and Banjade, Rabin and Rudolph, Rachel Etta and Gabriel, Raefer and Habacker, Rahel and Delgado, Ramón Risco and Millière, Raphaël and Garg, Rhythm and Barnes, Richard and Saurous, Rif A. and Arakawa, Riku and Raymaekers, Robbe and Frank, Robert and Sikand, Rohan and Novak, Roman and Sitelew, Roman and {LeBras}, Ronan and Liu, Rosanne and Jacobs, Rowan and Zhang, Rui and Salakhutdinov, Ruslan and Chi, Ryan and Lee, Ryan and Stovall, Ryan and Teehan, Ryan and Yang, Rylan and Singh, Sahib and Mohammad, Saif M. and Anand, Sajant and Dillavou, Sam and Shleifer, Sam and Wiseman, Sam and Gruetter, Samuel and Bowman, Samuel R. and Schoenholz, Samuel S. and Han, Sanghyun and Kwatra, Sanjeev and Rous, Sarah A. and Ghazarian, Sarik and Ghosh, Sayan and Casey, Sean and Bischoff, Sebastian and Gehrmann, Sebastian and Schuster, Sebastian and Sadeghi, Sepideh and Hamdan, Shadi and Zhou, Sharon and Srivastava, Shashank and Shi, Sherry and Singh, Shikhar and Asaadi, Shima and Gu, Shixiang Shane and Pachchigar, Shubh and Toshniwal, Shubham and Upadhyay, Shyam and Shyamolima and Debnath and Shakeri, Siamak and Thormeyer, Simon and Melzi, Simone and Reddy, Siva and Makini, Sneha Priscilla and Lee, Soo-Hwan and Torene, Spencer and Hatwar, Sriharsha and Dehaene, Stanislas and Divic, Stefan and Ermon, Stefano and Biderman, Stella and Lin, Stephanie and Prasad, Stephen and Piantadosi, Steven T. and Shieber, Stuart M. and Misherghi, Summer and Kiritchenko, Svetlana and Mishra, Swaroop and Linzen, Tal and Schuster, Tal and Li, Tao and Yu, Tao and Ali, Tariq and Hashimoto, Tatsu and Wu, Te-Lin and Desbordes, Théo and Rothschild, Theodore and Phan, Thomas and Wang, Tianle and Nkinyili, Tiberius and Schick, Timo and Kornev, Timofei and Telleen-Lawton, Timothy and Tunduny, Titus and Gerstenberg, Tobias and Chang, Trenton and Neeraj, Trishala and Khot, Tushar and Shultz, Tyler and Shaham, Uri and Misra, Vedant and Demberg, Vera and Nyamai, Victoria and Raunak, Vikas and Ramasesh, Vinay and Prabhu, Vinay Uday and Padmakumar, Vishakh and Srikumar, Vivek and Fedus, William and Saunders, William and Zhang, William and Vossen, Wout and Ren, Xiang and Tong, Xiaoyu and Zhao, Xinran and Wu, Xinyi and Shen, Xudong and Yaghoobzadeh, Yadollah and Lakretz, Yair and Song, Yangqiu and Bahri, Yasaman and Choi, Yejin and Yang, Yichi and Hao, Yiding and Chen, Yifu and Belinkov, Yonatan and Hou, Yu and Hou, Yufang and Bai, Yuntao and Seid, Zachary and Zhao, Zhuoye and Wang, Zijian and Wang, Zijie J. and Wang, Zirui and Wu, Ziyi},
	urldate = {2023-01-11},
	date = {2022-06-10},
	eprinttype = {arxiv},
	eprint = {2206.04615 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Computers and Society, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/mdmoor/Zotero/storage/HKEHYUC4/Srivastava et al. - 2022 - Beyond the Imitation Game Quantifying and extrapo.pdf:application/pdf;arXiv.org Snapshot:/Users/mdmoor/Zotero/storage/YM6LKE2M/2206.html:text/html},
}

@inproceedings{carlini_extracting_2021,
	title = {Extracting Training Data from Large Language Models},
	url = {https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting},
	pages = {2633--2650},
	booktitle = {30th {USENIX} Security Symposium, {USENIX} Security 2021, August 11-13, 2021},
	author = {Carlini, Nicholas and Tramèr, Florian and Wallace, Eric and Jagielski, Matthew and Herbert-Voss, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom B. and Song, Dawn and Erlingsson, Úlfar and Oprea, Alina and Raffel, Colin},
	editor = {Bailey, Michael and Greenstadt, Rachel},
	date = {2021},
}

@misc{branch_evaluating_2022,
	title = {Evaluating the Susceptibility of Pre-Trained Language Models via Handcrafted Adversarial Examples},
	url = {http://arxiv.org/abs/2209.02128},
	doi = {10.48550/arXiv.2209.02128},
	abstract = {Recent advances in the development of large language models have resulted in public access to state-of-the-art pre-trained language models ({PLMs}), including Generative Pre-trained Transformer 3 ({GPT}-3) and Bidirectional Encoder Representations from Transformers ({BERT}). However, evaluations of {PLMs}, in practice, have shown their susceptibility to adversarial attacks during the training and fine-tuning stages of development. Such attacks can result in erroneous outputs, model-generated hate speech, and the exposure of users' sensitive information. While existing research has focused on adversarial attacks during either the training or the fine-tuning of {PLMs}, there is a deficit of information on attacks made between these two development phases. In this work, we highlight a major security vulnerability in the public release of {GPT}-3 and further investigate this vulnerability in other state-of-the-art {PLMs}. We restrict our work to pre-trained models that have not undergone fine-tuning. Further, we underscore token distance-minimized perturbations as an effective adversarial approach, bypassing both supervised and unsupervised quality measures. Following this approach, we observe a significant decrease in text classification quality when evaluating for semantic similarity.},
	number = {{arXiv}:2209.02128},
	publisher = {{arXiv}},
	author = {Branch, Hezekiah J. and Cefalu, Jonathan Rodriguez and {McHugh}, Jeremy and Hujer, Leyla and Bahl, Aditya and Iglesias, Daniel del Castillo and Heichman, Ron and Darwishi, Ramesh},
	urldate = {2023-01-11},
	date = {2022-09-05},
	eprinttype = {arxiv},
	eprint = {2209.02128 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/mdmoor/Zotero/storage/STI4G4Q5/Branch et al. - 2022 - Evaluating the Susceptibility of Pre-Trained Langu.pdf:application/pdf;arXiv.org Snapshot:/Users/mdmoor/Zotero/storage/RU66AE86/2209.html:text/html},
}

@misc{chowdhery_palm_2022,
	title = {{PaLM}: Scaling Language Modeling with Pathways},
	url = {http://arxiv.org/abs/2204.02311},
	doi = {10.48550/arXiv.2204.02311},
	shorttitle = {{PaLM}},
	abstract = {Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model {PaLM}. We trained {PaLM} on 6144 {TPU} v4 chips using Pathways, a new {ML} system which enables highly efficient training across multiple {TPU} Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, {PaLM} 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released {BIG}-bench benchmark. A significant number of {BIG}-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. {PaLM} also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.},
	number = {{arXiv}:2204.02311},
	publisher = {{arXiv}},
	author = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and Schuh, Parker and Shi, Kensen and Tsvyashchenko, Sasha and Maynez, Joshua and Rao, Abhishek and Barnes, Parker and Tay, Yi and Shazeer, Noam and Prabhakaran, Vinodkumar and Reif, Emily and Du, Nan and Hutchinson, Ben and Pope, Reiner and Bradbury, James and Austin, Jacob and Isard, Michael and Gur-Ari, Guy and Yin, Pengcheng and Duke, Toju and Levskaya, Anselm and Ghemawat, Sanjay and Dev, Sunipa and Michalewski, Henryk and Garcia, Xavier and Misra, Vedant and Robinson, Kevin and Fedus, Liam and Zhou, Denny and Ippolito, Daphne and Luan, David and Lim, Hyeontaek and Zoph, Barret and Spiridonov, Alexander and Sepassi, Ryan and Dohan, David and Agrawal, Shivani and Omernick, Mark and Dai, Andrew M. and Pillai, Thanumalayan Sankaranarayana and Pellat, Marie and Lewkowycz, Aitor and Moreira, Erica and Child, Rewon and Polozov, Oleksandr and Lee, Katherine and Zhou, Zongwei and Wang, Xuezhi and Saeta, Brennan and Diaz, Mark and Firat, Orhan and Catasta, Michele and Wei, Jason and Meier-Hellstern, Kathy and Eck, Douglas and Dean, Jeff and Petrov, Slav and Fiedel, Noah},
	urldate = {2023-01-11},
	date = {2022-10-05},
	eprinttype = {arxiv},
	eprint = {2204.02311 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/mdmoor/Zotero/storage/Z5M6U2WL/Chowdhery et al. - 2022 - PaLM Scaling Language Modeling with Pathways.pdf:application/pdf;arXiv.org Snapshot:/Users/mdmoor/Zotero/storage/SYYXA9K8/2204.html:text/html},
}

@misc{zhang_opt_2022,
	title = {{OPT}: Open Pre-trained Transformer Language Models},
	url = {http://arxiv.org/abs/2205.01068},
	doi = {10.48550/arXiv.2205.01068},
	shorttitle = {{OPT}},
	abstract = {Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through {APIs}, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers ({OPT}), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that {OPT}-175B is comparable to {GPT}-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.},
	number = {{arXiv}:2205.01068},
	publisher = {{arXiv}},
	author = {Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and Mihaylov, Todor and Ott, Myle and Shleifer, Sam and Shuster, Kurt and Simig, Daniel and Koura, Punit Singh and Sridhar, Anjali and Wang, Tianlu and Zettlemoyer, Luke},
	urldate = {2023-01-11},
	date = {2022-06-21},
	eprinttype = {arxiv},
	eprint = {2205.01068 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/mdmoor/Zotero/storage/KE7FA8EZ/Zhang et al. - 2022 - OPT Open Pre-trained Transformer Language Models.pdf:application/pdf;arXiv.org Snapshot:/Users/mdmoor/Zotero/storage/TNHNW5P6/2205.html:text/html},
}

@inproceedings{hoffmann_empirical_2022,
	title = {An empirical analysis of compute-optimal large language model training},
	url = {https://openreview.net/forum?id=iBBcRUlOAPR},
	abstract = {We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4\${\textbackslash}times\$ more data. Chinchilla uniformly and significantly {outperformsGopher} (280B), {GPT}-3 (175B), Jurassic-1 (178B), and Megatron-Turing {NLG} (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5\% on the {MMLU} benchmark, a 7\% improvement over Gopher.},
	eventtitle = {Advances in Neural Information Processing Systems},
	author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Tom and Noland, Eric and Millican, Katherine and Driessche, George van den and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Karen and Elsen, Erich and Vinyals, Oriol and Rae, Jack William and Sifre, Laurent},
	urldate = {2023-01-11},
	date = {2022-10-31},
	langid = {english},
	file = {Full Text PDF:/Users/mdmoor/Zotero/storage/ZWY8S7QY/Hoffmann et al. - 2022 - An empirical analysis of compute-optimal large lan.pdf:application/pdf},
}

@misc{lievin_can_2022,
	title = {Can large language models reason about medical questions?},
	url = {http://arxiv.org/abs/2207.08143},
	doi = {10.48550/arXiv.2207.08143},
	abstract = {Although large language models ({LLMs}) often produce impressive outputs, it remains unclear how they perform in real-world scenarios requiring strong reasoning skills and expert domain knowledge. We set out to investigate whether {GPT}-3.5 (Codex and {InstructGPT}) can be applied to answer and reason about difficult real-world-based questions. We utilize two multiple-choice medical exam questions ({USMLE} and {MedMCQA}) and a medical reading comprehension dataset ({PubMedQA}). We investigate multiple prompting scenarios: Chain-of-Thought ({CoT}, think step-by-step), zero- and few-shot (prepending the question with question-answer exemplars) and retrieval augmentation (injecting Wikipedia passages into the prompt). For a subset of the {USMLE} questions, a medical expert reviewed and annotated the model's {CoT}. We found that {InstructGPT} can often read, reason and recall expert knowledge. Failure are primarily due to lack of knowledge and reasoning errors and trivial guessing heuristics are observed, e.g.{\textbackslash} too often predicting labels A and D on {USMLE}. Sampling and combining many completions overcome some of these limitations. Using 100 samples, Codex 5-shot {CoT} not only gives close to well-calibrated predictive probability but also achieves human-level performances on the three datasets. {USMLE}: 60.2\%, {MedMCQA}: 57.5\% and {PubMedQA}: 78.2\%.},
	number = {{arXiv}:2207.08143},
	publisher = {{arXiv}},
	author = {Liévin, Valentin and Hother, Christoffer Egeberg and Winther, Ole},
	urldate = {2023-01-11},
	date = {2022-12-20},
	eprinttype = {arxiv},
	eprint = {2207.08143 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, I.2.1, I.2.7},
	file = {arXiv Fulltext PDF:/Users/mdmoor/Zotero/storage/F4YKNFKT/Liévin et al. - 2022 - Can large language models reason about medical que.pdf:application/pdf;arXiv.org Snapshot:/Users/mdmoor/Zotero/storage/YWXMB5H9/2207.html:text/html},
}

@misc{chung_scaling_2022,
	title = {Scaling Instruction-Finetuned Language Models},
	url = {http://arxiv.org/abs/2210.11416},
	doi = {10.48550/arXiv.2210.11416},
	abstract = {Finetuning language models on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. We find that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes ({PaLM}, T5, U-{PaLM}), prompting setups (zero-shot, few-shot, {CoT}), and evaluation benchmarks ({MMLU}, {BBH}, {TyDiQA}, {MGSM}, open-ended generation). For instance, Flan-{PaLM} 540B instruction-finetuned on 1.8K tasks outperforms {PALM} 540B by a large margin (+9.4\% on average). Flan-{PaLM} 540B achieves state-of-the-art performance on several benchmarks, such as 75.2\% on five-shot {MMLU}. We also publicly release Flan-T5 checkpoints, which achieve strong few-shot performance even compared to much larger models, such as {PaLM} 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.},
	number = {{arXiv}:2210.11416},
	publisher = {{arXiv}},
	author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Castro-Ros, Alex and Pellat, Marie and Robinson, Kevin and Valter, Dasha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason},
	urldate = {2023-01-11},
	date = {2022-12-06},
	eprinttype = {arxiv},
	eprint = {2210.11416 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/mdmoor/Zotero/storage/AC39ZHWN/Chung et al. - 2022 - Scaling Instruction-Finetuned Language Models.pdf:application/pdf;arXiv.org Snapshot:/Users/mdmoor/Zotero/storage/79998HKK/2210.html:text/html},
}

@inproceedings{huang_gloria_2021,
	title = {{GLoRIA}: A Multimodal Global-Local Representation Learning Framework for Label-Efficient Medical Image Recognition},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Huang_GLoRIA_A_Multimodal_Global-Local_Representation_Learning_Framework_for_Label-Efficient_Medical_ICCV_2021_paper.html},
	shorttitle = {{GLoRIA}},
	eventtitle = {Proceedings of the {IEEE}/{CVF} International Conference on Computer Vision},
	pages = {3942--3951},
	author = {Huang, Shih-Cheng and Shen, Liyue and Lungren, Matthew P. and Yeung, Serena},
	urldate = {2023-01-11},
	date = {2021},
	langid = {english},
	file = {Full Text PDF:/Users/mdmoor/Zotero/storage/Z4E9KWQ3/Huang et al. - 2021 - GLoRIA A Multimodal Global-Local Representation L.pdf:application/pdf},
}

@article{johnson_mimic-iv_2023,
	title = {{MIMIC}-{IV}, a freely accessible electronic health record dataset},
	volume = {10},
	rights = {2023 The Author(s)},
	issn = {2052-4463},
	url = {https://www.nature.com/articles/s41597-022-01899-x},
	doi = {10.1038/s41597-022-01899-x},
	abstract = {Digital data collection during routine clinical practice is now ubiquitous within hospitals. The data contains valuable information on the care of patients and their response to treatments, offering exciting opportunities for research. Typically, data are stored within archival systems that are not intended to support research. These systems are often inaccessible to researchers and structured for optimal storage, rather than interpretability and analysis. Here we present {MIMIC}-{IV}, a publicly available database sourced from the electronic health record of the Beth Israel Deaconess Medical Center. Information available includes patient measurements, orders, diagnoses, procedures, treatments, and deidentified free-text clinical notes. {MIMIC}-{IV} is intended to support a wide array of research studies and educational material, helping to reduce barriers to conducting clinical research.},
	pages = {1},
	number = {1},
	journaltitle = {Scientific Data},
	shortjournal = {Sci Data},
	author = {Johnson, Alistair E. W. and Bulgarelli, Lucas and Shen, Lu and Gayles, Alvin and Shammout, Ayad and Horng, Steven and Pollard, Tom J. and Moody, Benjamin and Gow, Brian and Lehman, Li-wei H. and Celi, Leo A. and Mark, Roger G.},
	urldate = {2023-01-11},
	date = {2023-01-03},
	langid = {english},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Epidemiology, Health services, Public health},
	file = {Full Text PDF:/Users/mdmoor/Zotero/storage/67DXN6VF/Johnson et al. - 2023 - MIMIC-IV, a freely accessible electronic health re.pdf:application/pdf},
}

@article{sudlow_uk_2015,
	title = {{UK} Biobank: An Open Access Resource for Identifying the Causes of a Wide Range of Complex Diseases of Middle and Old Age},
	volume = {12},
	issn = {1549-1676},
	url = {https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1001779},
	doi = {10.1371/journal.pmed.1001779},
	shorttitle = {{UK} Biobank},
	abstract = {Cathie Sudlow and colleagues describe the {UK} Biobank, a large population-based prospective study, established to allow investigation of the genetic and non-genetic determinants of the diseases of middle and old age.},
	pages = {e1001779},
	number = {3},
	journaltitle = {{PLOS} Medicine},
	shortjournal = {{PLOS} Medicine},
	author = {Sudlow, Cathie and Gallacher, John and Allen, Naomi and Beral, Valerie and Burton, Paul and Danesh, John and Downey, Paul and Elliott, Paul and Green, Jane and Landray, Martin and Liu, Bette and Matthews, Paul and Ong, Giok and Pell, Jill and Silman, Alan and Young, Alan and Sprosen, Tim and Peakman, Tim and Collins, Rory},
	urldate = {2023-01-11},
	date = {2015-03-31},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Cohort studies, Global health, Intelligence tests, Magnetic resonance imaging, Prospective studies, Questionnaires, Research ethics, Scientists},
	file = {Full Text PDF:/Users/mdmoor/Zotero/storage/WMYJ75E9/Sudlow et al. - 2015 - UK Biobank An Open Access Resource for Identifyin.pdf:application/pdf},
}

@article{gou_knowledge_2021,
	title = {Knowledge Distillation: A Survey},
	volume = {129},
	issn = {1573-1405},
	url = {https://doi.org/10.1007/s11263-021-01453-z},
	doi = {10.1007/s11263-021-01453-z},
	shorttitle = {Knowledge Distillation},
	abstract = {In recent years, deep neural networks have been successful in both industry and academia, especially for computer vision tasks. The great success of deep learning is mainly due to its scalability to encode large-scale data and to maneuver billions of model parameters. However, it is a challenge to deploy these cumbersome deep models on devices with limited resources, e.g., mobile phones and embedded devices, not only because of the high computational complexity but also the large storage requirements. To this end, a variety of model compression and acceleration techniques have been developed. As a representative type of model compression and acceleration, knowledge distillation effectively learns a small student model from a large teacher model. It has received rapid increasing attention from the community. This paper provides a comprehensive survey of knowledge distillation from the perspectives of knowledge categories, training schemes, teacher–student architecture, distillation algorithms, performance comparison and applications. Furthermore, challenges in knowledge distillation are briefly reviewed and comments on future research are discussed and forwarded.},
	pages = {1789--1819},
	number = {6},
	journaltitle = {International Journal of Computer Vision},
	shortjournal = {Int J Comput Vis},
	author = {Gou, Jianping and Yu, Baosheng and Maybank, Stephen J. and Tao, Dacheng},
	urldate = {2023-01-11},
	date = {2021-06-01},
	langid = {english},
	keywords = {Deep neural networks, Knowledge distillation, Knowledge transfer, Model compression, Teacher–student architecture},
	file = {Full Text PDF:/Users/mdmoor/Zotero/storage/7TDK7FKA/Gou et al. - 2021 - Knowledge Distillation A Survey.pdf:application/pdf},
}

@inproceedings{yasunaga_deep_2022,
	title = {Deep Bidirectional Language-Knowledge Graph Pretraining},
	url = {https://openreview.net/forum?id=4NpoSrT8uU-},
	abstract = {Pretraining a language model ({LM}) on text has been shown to help various downstream {NLP} tasks. Recent works show that a knowledge graph ({KG}) can complement text data, offering structured background knowledge that provides a useful scaffold for reasoning. However, these works are not pretrained to learn a deep fusion of the two modalities at scale, limiting the potential to acquire fully joint representations of text and {KG}. Here we propose {DRAGON} (Deep Bidirectional Language-Knowledge Graph Pretraining), a self-supervised approach to pretraining a deeply joint language-knowledge foundation model from text and {KG} at scale. Specifically, our model takes pairs of text segments and relevant {KG} subgraphs as input and bidirectionally fuses information from both modalities. We pretrain this model by unifying two self-supervised reasoning tasks, masked language modeling and {KG} link prediction. {DRAGON} outperforms existing {LM} and {LM}+{KG} models on diverse downstream tasks including question answering across general and biomedical domains, with +5\% absolute gain on average. In particular, {DRAGON} achieves notable performance on complex reasoning about language and knowledge (+10\% on questions involving long contexts or multi-step reasoning) and low-resource {QA} (+8\% on {OBQA} and {RiddleSense}), and new state-of-the-art results on various {BioNLP} tasks. Our code and trained models are available at https://github.com/michiyasunaga/dragon.},
	eventtitle = {Advances in Neural Information Processing Systems},
	booktitle = {Advances in Neural Information Processing Systems},
	author = {Yasunaga, Michihiro and Bosselut, Antoine and Ren, Hongyu and Zhang, Xikun and Manning, Christopher D. and Liang, Percy and Leskovec, Jure},
	urldate = {2023-01-11},
	date = {2022-10-31},
	langid = {english},
	file = {Full Text PDF:/Users/mdmoor/Zotero/storage/PGRTKW2F/Yasunaga et al. - 2022 - Deep Bidirectional Language-Knowledge Graph Pretra.pdf:application/pdf},
}

@article{biomedlm,
  title={{BioMedLM}: a domain-specific large language model for biomedical text},
  author={Elliot Bolton and David Hall and Michihiro Yasunaga and Tony Lee and Chris Manning and Percy Liang},
  url={https://crfm.stanford.edu/2022/12/15/biomedlm.html}
}

@misc{singhal_large_2022,
	title = {Large Language Models Encode Clinical Knowledge},
	url = {http://arxiv.org/abs/2212.13138},
	doi = {10.48550/arXiv.2212.13138},
	abstract = {Large language models ({LLMs}) have demonstrated impressive capabilities in natural language understanding and generation, but the quality bar for medical and clinical applications is high. Today, attempts to assess models' clinical knowledge typically rely on automated evaluations on limited benchmarks. There is no standard to evaluate model predictions and reasoning across a breadth of tasks. To address this, we present {MultiMedQA}, a benchmark combining six existing open question answering datasets spanning professional medical exams, research, and consumer queries; and {HealthSearchQA}, a new free-response dataset of medical questions searched online. We propose a framework for human evaluation of model answers along multiple axes including factuality, precision, possible harm, and bias. In addition, we evaluate {PaLM} (a 540-billion parameter {LLM}) and its instruction-tuned variant, Flan-{PaLM}, on {MultiMedQA}. Using a combination of prompting strategies, Flan-{PaLM} achieves state-of-the-art accuracy on every {MultiMedQA} multiple-choice dataset ({MedQA}, {MedMCQA}, {PubMedQA}, {MMLU} clinical topics), including 67.6\% accuracy on {MedQA} ({US} Medical License Exam questions), surpassing prior state-of-the-art by over 17\%. However, human evaluation reveals key gaps in Flan-{PaLM} responses. To resolve this we introduce instruction prompt tuning, a parameter-efficient approach for aligning {LLMs} to new domains using a few exemplars. The resulting model, Med-{PaLM}, performs encouragingly, but remains inferior to clinicians. We show that comprehension, recall of knowledge, and medical reasoning improve with model scale and instruction prompt tuning, suggesting the potential utility of {LLMs} in medicine. Our human evaluations reveal important limitations of today's models, reinforcing the importance of both evaluation frameworks and method development in creating safe, helpful {LLM} models for clinical applications.},
	number = {{arXiv}:2212.13138},
	publisher = {{arXiv}},
	author = {Singhal, Karan and Azizi, Shekoofeh and Tu, Tao and Mahdavi, S. Sara and Wei, Jason and Chung, Hyung Won and Scales, Nathan and Tanwani, Ajay and Cole-Lewis, Heather and Pfohl, Stephen and Payne, Perry and Seneviratne, Martin and Gamble, Paul and Kelly, Chris and Scharli, Nathaneal and Chowdhery, Aakanksha and Mansfield, Philip and Arcas, Blaise Aguera y and Webster, Dale and Corrado, Greg S. and Matias, Yossi and Chou, Katherine and Gottweis, Juraj and Tomasev, Nenad and Liu, Yun and Rajkomar, Alvin and Barral, Joelle and Semturs, Christopher and Karthikesalingam, Alan and Natarajan, Vivek},
	urldate = {2023-01-18},
	date = {2022-12-26},
	eprinttype = {arxiv},
	eprint = {2212.13138 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/mdmoor/Zotero/storage/DM7VX9FC/Singhal et al. - 2022 - Large Language Models Encode Clinical Knowledge.pdf:application/pdf;arXiv.org Snapshot:/Users/mdmoor/Zotero/storage/XQ39ERBD/2212.html:text/html},
}

@article{yang_large_2022,
	title = {A large language model for electronic health records},
	volume = {5},
	rights = {2022 The Author(s)},
	issn = {2398-6352},
	url = {https://www.nature.com/articles/s41746-022-00742-2},
	doi = {10.1038/s41746-022-00742-2},
	abstract = {There is an increasing interest in developing artificial intelligence ({AI}) systems to process and interpret electronic health records ({EHRs}). Natural language processing ({NLP}) powered by pretrained language models is the key technology for medical {AI} systems utilizing clinical narratives. However, there are few clinical language models, the largest of which trained in the clinical domain is comparatively small at 110 million parameters (compared with billions of parameters in the general domain). It is not clear how large clinical language models with billions of parameters can help medical {AI} systems utilize unstructured {EHRs}. In this study, we develop from scratch a large clinical language model—{GatorTron}—using {\textgreater}90 billion words of text (including {\textgreater}82 billion words of de-identified clinical text) and systematically evaluate it on five clinical {NLP} tasks including clinical concept extraction, medical relation extraction, semantic textual similarity, natural language inference ({NLI}), and medical question answering ({MQA}). We examine how (1) scaling up the number of parameters and (2) scaling up the size of the training data could benefit these {NLP} tasks. {GatorTron} models scale up the clinical language model from 110 million to 8.9 billion parameters and improve five clinical {NLP} tasks (e.g., 9.6\% and 9.5\% improvement in accuracy for {NLI} and {MQA}), which can be applied to medical {AI} systems to improve healthcare delivery. The {GatorTron} models are publicly available at: https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara/models/gatortron\_og.},
	pages = {1--9},
	number = {1},
	journaltitle = {npj Digital Medicine},
	shortjournal = {npj Digit. Med.},
	author = {Yang, Xi and Chen, Aokun and {PourNejatian}, Nima and Shin, Hoo Chang and Smith, Kaleb E. and Parisien, Christopher and Compas, Colin and Martin, Cheryl and Costa, Anthony B. and Flores, Mona G. and Zhang, Ying and Magoc, Tanja and Harle, Christopher A. and Lipori, Gloria and Mitchell, Duane A. and Hogan, William R. and Shenkman, Elizabeth A. and Bian, Jiang and Wu, Yonghui},
	urldate = {2023-01-19},
	date = {2022-12-26},
	langid = {english},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Health care, Medical research},
	file = {Full Text PDF:/Users/mdmoor/Zotero/storage/C3EIRX4G/Yang et al. - 2022 - A large language model for electronic health recor.pdf:application/pdf},
}

@misc{kung_performance_2022,
	title = {Performance of {ChatGPT} on {USMLE}: Potential for {AI}-Assisted Medical Education Using Large Language Models},
	rights = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NoDerivs} 4.0 International), {CC} {BY}-{ND} 4.0, as described at http://creativecommons.org/licenses/by-nd/4.0/},
	url = {https://www.medrxiv.org/content/10.1101/2022.12.19.22283643v2},
	doi = {10.1101/2022.12.19.22283643},
	shorttitle = {Performance of {ChatGPT} on {USMLE}},
	abstract = {We evaluated the performance of a large language model called {ChatGPT} on the United States Medical Licensing Exam ({USMLE}), which consists of three exams: Step 1, Step 2CK, and Step 3. {ChatGPT} performed at or near the passing threshold for all three exams without any specialized training or reinforcement. Additionally, {ChatGPT} demonstrated a high level of concordance and insight in its explanations. These results suggest that large language models may have the potential to assist with medical education, and potentially, clinical decision-making.},
	publisher = {{medRxiv}},
	author = {Kung, Tiffany H. and Cheatham, Morgan and {ChatGPT} and Medenilla, Arielle and Sillos, Czarina and Leon, Lorie De and Elepaño, Camille and Madriaga, Maria and Aggabao, Rimel and Diaz-Candido, Giezel and Maningo, James and Tseng, Victor},
	urldate = {2023-01-19},
	date = {2022-12-21},
	langid = {english},
	note = {Pages: 2022.12.19.22283643},
	file = {Full Text PDF:/Users/mdmoor/Zotero/storage/82YXS7G7/Kung et al. - 2022 - Performance of ChatGPT on USMLE Potential for AI-.pdf:application/pdf},
}

@inproceedings{devlin_bert_2019,
	location = {Minneapolis, Minnesota},
	title = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	volume = {1},
	url = {https://aclanthology.org/N19-1423},
	doi = {10.18653/v1/N19-1423},
	shorttitle = {{BERT}},
	abstract = {We introduce a new language representation model called {BERT}, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), {BERT} is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained {BERT} model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. {BERT} is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the {GLUE} score to 80.5 (7.7 point absolute improvement), {MultiNLI} accuracy to 86.7\% (4.6\% absolute improvement), {SQuAD} v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and {SQuAD} v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	eventtitle = {{NAACL}-{HLT} 2019},
	pages = {4171--4186},
	booktitle = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	urldate = {2023-01-19},
	date = {2019-06},
	file = {Full Text PDF:/Users/mdmoor/Zotero/storage/HVNBXJ5R/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf},
}

@article{reed_generalist_2022,
	title = {A Generalist Agent},
	issn = {2835-8856},
	url = {https://openreview.net/forum?id=1ikK0kHjvj},
	abstract = {Inspired by progress in large-scale language modeling, we apply a similar approach towards building a single generalist agent beyond the realm of text outputs. The agent, which we refer to as Gato, works as a multi-modal, multi-task, multi-embodiment generalist policy. The same network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens. In this report we describe the model and the data, and document the current capabilities of Gato.},
	journaltitle = {Transactions on Machine Learning Research},
	author = {Reed, Scott and Zolna, Konrad and Parisotto, Emilio and Colmenarejo, Sergio Gómez and Novikov, Alexander and Barth-maron, Gabriel and Giménez, Mai and Sulsky, Yury and Kay, Jackie and Springenberg, Jost Tobias and Eccles, Tom and Bruce, Jake and Razavi, Ali and Edwards, Ashley and Heess, Nicolas and Chen, Yutian and Hadsell, Raia and Vinyals, Oriol and Bordbar, Mahyar and Freitas, Nando de},
	urldate = {2023-03-16},
	date = {2022-11-10},
	langid = {english},
	file = {Full Text PDF:/Users/mdmoor/Zotero/storage/NVHSQ97P/Reed et al. - 2022 - A Generalist Agent.pdf:application/pdf},
}

@article{reed_generalist_2022-1,
	title = {A Generalist Agent 2},
	issn = {2835-8856},
	url = {https://openreview.net/forum?id=1ikK0kHjvj},
	journaltitle = {Transactions on Machine Learning Research},
	author = {Reed, Scott and Zolna, Konrad and Parisotto, Emilio and Colmenarejo, Sergio Gómez and Novikov, Alexander and Barth-maron, Gabriel and Giménez, Mai and Sulsky, Yury and Kay, Jackie and Springenberg, Jost Tobias and Eccles, Tom and Bruce, Jake and Razavi, Ali and Edwards, Ashley and Heess, Nicolas and Chen, Yutian and Hadsell, Raia and Vinyals, Oriol and Bordbar, Mahyar and Freitas, Nando de},
	date = {2022},
}

@inproceedings{brown_language_2020-1,
	title = {Language Models are Few-Shot Learners},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
	pages = {1877--1901},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and {McCandlish}, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
	date = {2020},
}

@software{awadalla2023,
  author = {Awadalla, Anas and Gao, Irena and Gardner, Joshua and Hessel, Jack and Hanafy, Yusuf and Zhu, Wanrong and Marathe, Kalyani and Bitton, Yonatan and Gadre, Samir and Jitsev, Jenia and Kornblith, Simon and Koh, Pang Wei and Ilharco, Gabriel and Wortsman, Mitchell and Schmidt, Ludwig},
  title = {OpenFlamingo},
  month        = mar,
  year         = 2023,
  publisher    = {Zenodo},
  version      = {v0.1.1},
  doi          = {10.5281/zenodo.7733589},
  url          = {https://doi.org/10.5281/zenodo.7733589}
}

@article{li2023llava,
  title={Llava-med: Training a large language-and-vision assistant for biomedicine in one day},
  author={Li, Chunyuan and Wong, Cliff and Zhang, Sheng and Usuyama, Naoto and Liu, Haotian and Yang, Jianwei and Naumann, Tristan and Poon, Hoifung and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2306.00890},
  year={2023}
}

@article{steinberg2021language,
  title={Language models are an effective representation learning technique for electronic health record data},
  author={Steinberg, Ethan and Jung, Ken and Fries, Jason A and Corbin, Conor K and Pfohl, Stephen R and Shah, Nigam H},
  journal={Journal of biomedical informatics},
  volume={113},
  pages={103637},
  year={2021},
  publisher={Elsevier}
}

@inproceedings{bert-score,
  title={BERTScore: Evaluating Text Generation with BERT},
  author={Tianyi Zhang and Varsha Kishore* and Felix Wu and Kilian Q. Weinberger and Yoav Artzi},
  booktitle={International Conference on Learning Representations},
  year={2020},
  url={https://openreview.net/forum?id=SkeHuCVFDr}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{kiyasseh2023vision,
  title={A vision transformer for decoding surgeon activity from surgical videos},
  author={Kiyasseh, Dani and Ma, Runzhuo and Haque, Taseen F and Miles, Brian J and Wagner, Christian and Donoho, Daniel A and Anandkumar, Animashree and Hung, Andrew J},
  journal={Nature Biomedical Engineering},
  pages={1--17},
  year={2023},
  publisher={Nature Publishing Group UK London}
}