\begin{table*}[t]
    \centering
    \resizebox{1\textwidth}{!}{%
    \begin{tabular}{l|cccccc|cc|cc|cc|cc|cccc|}
    \toprule
        % &&&&&Zero-shot&&&&&&&&Zero-shot&&&&&&Detection&&&&\\
        % Model&&&Linear Probe&Fine-tuning&ImageNet&&&&Linear Probe Transfer Leraning&&&&Text Retrieval&&Image Retrieval&&&&&&Pascal Context &&\\
        & \multicolumn{2}{c}{\underline{ImageNet}} & \multicolumn{4}{c}{\underline{Zero-shot ImageNet}} & \multicolumn{2}{|c}{\underline{TR}} & \multicolumn{2}{|c}{\underline{IR}} &  \multicolumn{2}{|c}{\underline{VQA}} &  \multicolumn{2}{|c}{\underline{Detection}} &  \multicolumn{2}{|c}{\underline{Segmentation}}\\
        &LP &FT&Std&R&A&V2&Flicker&COCO &Flicker&COCO~&VQA&VQAv2&COCO &RefCOCOg&Pascal&ADE20K&\\
        & \cite{fei2009imagenet} & \cite{fei2009imagenet} & \cite{fei2009imagenet}& \cite{hendrycks2021many} & ~\cite{djolonga2021robustness} & ~\cite{recht2019imagenet} & ~\cite{young2014image} & ~\cite{chen2015microsoft} & \cite{young2014image} & \cite{chen2015microsoft} & \cite{antol2015vqa} & ~\cite{goyal2016making} & ~\cite{lin2014microsoft} &  ~\cite{mao2016generation} & ~\cite{mottaghi2014role} & ~\cite{zhou2017scene}  \\
        \toprule
         CLIP~\cite{radford2021learning}&85.4&-&76.2&88.9&77.1&70.1&88&58.4&76.7&37.8&76.7&-&&-&-&-&\\
        ALIGN~\cite{jia2021scaling}&85.5&88.64&76.4&92.2&75.8&70.1&88.6&58.6&75.7&45.6&-&-&-&-&-&-&\\
        WenLan~\cite{huo2021wenlan}&-&-&-&-&-&-&-&-&-&-&-&-&-&-&-&-&\\
        Florence~\cite{yuan2021florence}&-&90.05&83.74&-&-&-&90.9&64.7&76.7&47.2&80.36&&62.4&-&-&-&\\
        FILIP~\cite{yao2021filip}&-&-&77.1&&&&95.4&61.3&84.7&45.9&-&-&-&-&-&-&\\
        SLIP~\cite{mu2022slip}&75.1&-&47.9&-&-&-&-&-&-&-&-&&&-&-&-&\\
        FLIP~\cite{li2023scaling}&83.6&-&74.3&86.5&51.2&66.8&89.8&61.3&75&45.9&&77.3&-&-&-&-&\\
        MaskCLIP~\cite{dong2023maskclip}&73.7&-&44.5&-&-&-&-&-&70.1&-&-&-&45.4&-&-&50.5&\\
        CLIPA~\cite{li2023inverse}&-&-&69.3&84&43.6&61.7&81.9&56.8&67.5&39.8&-&-&-&-&-&-&\\
        CLIPAv2~\cite{li2023clipav2}&-&-&81.8&94.4&82.7&75.6&-&-&-&-&-&-&-&-&-&-&\\
        EVA~\cite{fang2023eva}&86.5&-&89.6&88.3&86.2&81.6&-&-&-&-&-&-&58.9&-&-&62.3&\\
        EVA-CLIP~\cite{sun2023eva}&-&-&82&94.5&82.1&75.7&-&-&-&-&-&-&-&-&-&-&\\
        EVA-02~\cite{fang2023eva2}&&90&80.4&93.2&82.9&73.8&89.2&64.1&77.9&47.9&-&-&64.1&-&-&62&\\
        OpenCLIP~\cite{cherti2023reproducible}&-&-&77.97&89.32&59.32&70.82&-&-&-&-&-&-&-&&&&\\
        CRIS~\cite{wang2022cris}&-&-&-&-&-&-&-&-&-&-&-&-&-&60.36&&&\\
        MaskCLIP~\cite{zhou2022maskclip}&73.7&83.6&44.5&&&&70.1&&45.6&-&-&-&-&-&31.1&18&\\
        GLIP~\cite{li2022grounded}&-&-&-&-&-&-&-&-&-&-&-&-&61.5&-&17.2&-&\\
        Ground.DINO~\cite{liu2023grounding}&-&-&-&-&-&-&-&-&-&-&-&-&63&87.02&-&-&\\
        OWL-ViT~\cite{minderer2022simple}&-&-&-&-&-&-&-&-&-&-&-&-&-&-&-&-&\\
        ViLD~\cite{gu2021open}&-&-&-&-&-&-&-&-&-&-&-&-&59.5&-&-&-&\\
        GroupViT~\cite{xu2022groupvit}&-&-&-&-&-&-&-&-&-&-&-&&-&-&22.4&-&\\
        OpenSeg~\cite{ghiasi2021scaling}&-&-&-&-&-&-&-&-&-&-&-&&-&-&29.1&-&\\
        % &&&&&&&&&&&-&&&&&&\\
        \midrule
        Frozen~\cite{tsimpoukelli2021multimodal}&-&-&-&-&-&-&-&-&-&-&-&48.4*&-&-&-&-&\\
        Flamingo~\cite{alayrac2022flamingo}&-&-&-&-&-&-&89.3&65.9&79.5&48&-&82.1&-&-&-&-&\\
        OpenFlamingo~\cite{anas_awadalla_2023_7733589}&&&&&&&&&&&&&&&&&\\
        {MetaLM}~\cite{hao2022language}&-&-&-&-&-&-&-&-&-&-&-&74.5&-&-&-&-&\\
        {KOSMOS-1}~\cite{huang2023language}&-&-&-&-&-&-&-&-&-&-&-&46.7*&-&-&-&-&\\
        {KOSMOS-v2}\cite{peng2023kosmos2}&&-&-&-&-&-&-&-&-&-&-&45.6*&&61.65*&&&\\
        SimVLM~\cite{wang2021simvlm}&83.6&-&-&-&-&-&-&-&-&-&80.3&&-&-&-&-&\\
        CapPa~\cite{tschannen2023image}&-&-&77.5&-&-&-&&62.6&&45.4&&68.6&-&-&-&-&\\
        % &&&&&&&&&&&&&&&&&\\
        \midrule
        UNITER~\cite{chen2020uniter}&-&-&-&-&-&-&93.03&65.68&84.34&52.93&74&-&-&87.73&-&-&\\
        Pixel2Seqv2~\cite{chen2022unified}&-&-&-&-&-&-&-&-&-&-&-&-&46.5&-&-&-&\\
        VL-x ~\cite{cho2021unifying}&-&-&-&-&-&-&-&-&-&-&&&-&71.3&-&-&\\
        CoCa~\cite{yu2022coca}&90.6&91&86.3&96.5&90.2&80.7&92.5&66.3&80.4&51.2&82.3&-&-&-&-&-&\\
        FLAVA~\cite{singh2022flava}&75.54&-&-&-&-&-&69.3&43.48&65.22&38.46&-&73.75&-&-&-&-&\\
        PaLI~\cite{chen2022pali}&-&-&72.11&81.97&44.7&64.46&-&-&-&-&-&83.4&-&-&-&-&\\
        BLIP~\cite{li2022blip}&-&-&-&-&-&-&97.4&82.4&97.7&65.1&78.3&-&-&-&-&-&\\
        BridgeTower~\cite{xu2022bridge}&-&-&-&-&-&-&94.7&75&85.8&62.4&-&74.52&-&-&-&-&\\
        X-FM~\cite{zhang2023toward}&81&86.3&-&-&-&-&97.9&81.8&89.4&64.7&79.1&79.5&-&-&-&-&\\
        BLIP-2~\cite{li2023blip}&-&-&-&-&-&-&97.6&85.4&89.7&68.3&&84.03&-&-&-&-&\\
        % Instruct-BLIP&-&-&-&-&-&-&-&-&-&-&&&&&&&\\
        % TaCA~\cite{zhang2023taca}&-&-&-&-&-&-&-&-&-&-&&&&-&&&\\
        % VPGTrans~\cite{zhang2023transfer}&&&&&&&&&&&&&&&&&\\
        FIBER~\cite{dou2022coarse}&&&&&&&96&80.06&91.08&69.63&-&78.46&58.4&87.32&-&52&\\
        UniDetector~\cite{wang2023detecting}&-&-&-&-&-&-&-&-&-&-&-&-&49.3&-&-&-&\\
        X-Decoder~\cite{zou2022xdecoder}&-&-&-&-&-&-&94.4&76.1&84.4&58.6&-&77&46.7&64.6&-&58.1&\\
        MaskVLM~\cite{kuwon2022masked}&-&-&-&-&-&-&95.6&76.3&84.5&60.1&75.4&&-&-&-&-&\\
        GLIPv2~\cite{zhang2022glipv2}&-&-&-&-&-&-&-&-&-&-&-&-&62.4&-&-&-&\\
        
    \bottomrule
    \end{tabular} 
    }
\vspace{0.5em}
\caption{Comparison of textually prompted models for a representative set of tasks. For ImageNet, we curate results with Linear Probe (LP), Fine Tuning (FT), Standard (Std) Zero-shot, Renditions, Adversarial, and V2. Furthermore, we present @1 results for text Retrieval (RT) and Image Retrieval (IR) for Flicker and COCO datasets. We also show results for visual question answering, detection, and segmentation tasks for two datasets each. Results with * show different evaluation mechanisms.} 
\label{tab:results_textually_prompted} 
\end{table*}