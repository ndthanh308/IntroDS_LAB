\begin{table*}
% \vspace{-1.5em}
% \small
\centering
\resizebox{1\textwidth}{!}{%
\begin{tabular}{l|cx{4cm}r|lx{3cm}l|lx{4cm}|cr}
% \begin{tabulary}{\textwidth}{L|CLR|LLL|LL|CL}
\toprule
\multirow{2}[3]{*}{Method} &  \multicolumn{3}{|c}{\underline{Multimodal Pretraining data}} & \multicolumn{3}{|c}{\underline{Pretraining Objectives}} &  \multicolumn{2}{|c}{\underline{Architecture}} & \multicolumn{2}{|c}{\underline{Information}} \\
 & Public & \multicolumn{1}{c}{Dataset(s)} & Size & Contrastive & Generative & Others  & Type & \multicolumn{1}{c}{Base} &  \url{Link} & Venue  \\
 \midrule
% \multirow{22}{*}{\rotatebox{90}{Contrastive}} & \multirow{12}{*}{\rotatebox{90}{General Purpose}} & 
CLIP~\cite{radford2021learning}  & \xmark     & WebImageText~\cite{radford2021learning}   & 400M    & ITC     & -  & - & Dual-Enc & ResNet~\cite{he2016deep}, ViT~\cite{dosovitskiy2020image}, GPT2~\cite{radford2019language} & \href{https://github.com/openai/CLIP}{Link} & arXiv'21\\

ALIGN~\cite{jia2021scaling}      & \xmark     & ALIGN1.8B~\cite{jia2021scaling}      & 1800M   & ITC      & -  & - & Dual-Enc & EffNet-L2~\cite{tan2019efficientnet}, BERT-Large~\cite{devlin2018bert}&  - & ICML'21 \\

WenLan~\cite{huo2021wenlan}      & \xmark     & RUC-CAS-WenLan~\cite{huo2021wenlan} & 30M    & InfoCE   & -  & - &  Dual-Enc & RoBERTAa-Large\footnote{\url{https://github.com/brightmart/roberta_zh}}, FasterRCNN~\cite{ren2015faster}, EffNet-B7~\cite{tan2019efficientnet} &  \href{https://github.com/BAAI-WuDao/BriVl}{Link} & arXiv'23 \\ 
 Florence~\cite{yuan2021florence} & \xmark     & FLD-900M~\cite{yuan2021florence}      & 900M    & UniCL    & - & -  & Dual-Enc& CoSwnT~\cite{dong2022cswin}, GPT2~\cite{radford2019language} & - & ECCV'22\\

FILIP~\cite{yao2021filip}      & \xmark     & FILIP300M~\cite{yao2021filip}, CC3M~\cite{sharma2018conceptual}, C12M~\cite{changpinyo2021conceptual}, YFCC100M~\cite{thomee2016yfcc100m} & 340M         & FILIP &  & -      &  Dual-Enc & ViT~\cite{dosovitskiy2020image}, GPT2~\cite{radford2019language}   & - & ICLR'22  \\

SLIP~\cite{mu2022slip}           & \cmark & YFCC15M~\cite{thomee2016yfcc100m, radford2019language} & 15M         & ITC, SimCLR & - & - & Dual-Enc & ViT~\cite{dosovitskiy2020image}, ViT-S~\cite{touvron2021training}, GPT2~\cite{radford2019language}  & \href{https://github.com/facebookresearch/SLIP}{Link} & ECCV'22 \\

FLIP~\cite{li2023scaling}        & \cmark & LAION400M~\cite{schuhmann2021laion} & 400M &   ITC & - & - & Dual-Enc &  ViT~\cite{dosovitskiy2020image}, Transformer~\cite{vaswani2017attention} & \href{https://github.com/facebookresearch/flip}{Link} & arXiv'23 \\

MaskCLIP~\cite{dong2023maskclip} & \cmark &  YFCC15M~\cite{thomee2016yfcc100m, radford2019language} & 15M & ITC & MLM & Distil & Dual-Enc & ViT~\cite{vaswani2017attention}, GPT2~\cite{radford2019language}&  \href{https://github.com/LightDXY/MaskCLIP}{Link} & CVPR'23 \\

CLIPA~\cite{li2023inverse} & \cmark & LAION-400M~\cite{schuhmann2021laion} & 400M & ITC & - & - & Dual-Enc & ViT~\cite{dosovitskiy2020image}, Transformer~\cite{vaswani2017attention} & \href{https://github.com/UCSC-VLAA/CLIPA}{Link} & arXiv'23 \\

CLIPAv2~\cite{li2023clipav2} & \cmark &  LAION-2B~\cite{schuhmann2021laion}, DataComp-1B~\cite{gadre2023datacomp}  & 3000M & ITC & - & - & Dual-Enc  & ViT~\cite{dosovitskiy2020image}, Transformer~\cite{vaswani2017attention}  & \href{https://github.com/UCSC-VLAA/CLIPA}{Link} & arXiv'23 \\

EVA~\cite{fang2023eva}  & \cmark & IN21K~\cite{fei2009imagenet}, CC12M~\cite{changpinyo2021conceptual}, CC3M~\cite{sharma2018conceptual}, O365~\cite{shao2019objects365}, COCO~\cite{lin2014microsoft}, ADE~\cite{zhou2019semantic} & 29.6M& ITC & - & - & Dual-Enc & ViT-G~\cite{zhai2022scaling}, BEiT-3~\cite{wang2022image} & \href{ https://github.com/baaivision/EVA}{Link} & CVPR'23\\

EVA-CLIP~\cite{sun2023eva} & \cmark & Merged-2B~\cite{sun2023eva} & 2000M    & ITC & - & - & Dual-Enc & ViT-G~\cite{zhai2022scaling}, BEiT-3~\cite{wang2022image} & \href{https://github.com/baaivision/EVA/tree/master/EVA-CLIP}{Link} & arXiv'23\\

EVA-02~\cite{fang2023eva2} & \cmark & Merged-2B~\cite{sun2023eva} & 2000M & ITC & - & - & Enc-Dec & TrV~\cite{fang2023eva2}, - &  \href{https://github.com/baaivision/EVA/tree/master/EVA-02}{Link} & arXiv'23   \\

OpenCLIP~\cite{cherti2023reproducible} & \cmark & LAION-400M~\cite{schuhmann2021laion}LAION-5B~\cite{schuhmann2022laion} & 5400M & ITC & - & - & Dual-Enc & ViT~\cite{dosovitskiy2020image}, GPT2~\cite{radford2019language} & \href{https://github.com/mlfoundations/open_clip}{Link} & CVPR'23\\

% & \multirow{9}{*}{\rotatebox{90}{Visual Grounding}} & 
\cmidrule{2-11}
CRIS~\cite{wang2022cris}  & \cmark & RefCOCO, RefCOCO+~\cite{kazemzadeh2014referitgame}, G-Ref~\cite{nagaraja2016modeling} & 0.4M & TPC & - & - & Fusion & ResNet~\cite{he2016deep}, GPT2~\cite{radford2019language}  & \href{https://github.com/DerrickWang005/CRIS.pytorch}{Link} & CVPR'22  \\

MaskCLIP~\cite{zhou2022maskclip}  & \cmark & PASCAL VOC12~\cite{everingham2015pascal}, COCO Stuff~\cite{caesar2018coco}, PASCAL Context~\cite{mottaghi2014role} & 0.17M & - 
& - & Task & Dual-Enc & ResNet~\cite{he2016deep}, ViT~\cite{dosovitskiy2020image}, GPT2~\cite{radford2019language} & \href{https://github.com/chongzhou96/MaskCLIP}{Link} & ECCV'22 \\ 

GLIP~\cite{li2022grounded}  & \cmark & GoldG~\cite{kamath2021mdetr} (Flicker30K~\cite{young2014image}, VG~\cite{krishna2017visual}, GQA~\cite{hudson2019gqa}), OI~\cite{krasin2017openimages},  O365~\cite{shao2019objects365}, IN-Boxes~\cite{krizhevsky2012imagenet}, CC12M~\cite{changpinyo2021conceptual}, SBU~\cite{ordonez2011im2text}, Cap24M~\cite{li2022grounded}  & - & RWA & & & Fusion & ViT~\cite{dosovitskiy2020image}, GPT2~\cite{radford2019language} & \href{https://github.com/DerrickWang005/CRIS.pytorch}{Link}&  CVPR'22 \\

G-DINO~\cite{liu2023grounding} & \cmark & O365~\cite{shao2019objects365},OI~\cite{krasin2017openimages}, GoldG~\cite{kamath2021mdetr} (Flicker30K~\cite{young2014image},  VG~\cite{krishna2017visual}, GQA~\cite{hudson2019gqa}), Cap4M~\cite{li2022grounded}, COCO~\cite{lin2014microsoft}, RefCOCO~\cite{kazemzadeh2014referitgame} & - & GLIP & - & Task & Fusion & Swin-\{T, L\}~\cite{liu2021swin}, BERT-base~\cite{lu2019vilbert} &\href{https://github.com/IDEA-Research/GroundingDINO}{Link} & arXiv'23\\


OWL-ViT~\cite{minderer2022simple}  & \cmark & OI~\cite{krasin2017openimages}, O365~\cite{shao2019objects365}, VG~\cite{krishna2017visual} & 2M & - & - & DETR & Dual-Enc & ViT~\cite{dosovitskiy2020image}, Transformer~\cite{vaswani2017attention} & \href{https://github.com/google-research/scenic/tree/main/scenic/projects/owl_vit}{Link} & ECCV'22\\

ViLD~\cite{gu2021open}   & \cmark & LVIS~\cite{gupta2019lvis}, COCO~\cite{lin2014microsoft} &- & & & Task & Dual-Enc & Mask-RCNN~\cite{he2017mask}, FPN~\cite{lin2017feature}, CLIP-GPT2~\cite{radford2021learning} &\href{https://github.com/tensorflow/tpu/tree/master/models/official/detection/projects/vild}{Link} & ICLR'22 \\

GroupViT~\cite{xu2022groupvit}  & \cmark & YFCC~\cite{thomee2016yfcc100m}, CC12M~\cite{changpinyo2021conceptual} & 26M & ITC, MITC & - & - & Dual-Enc & ViT~\cite{vaswani2017attention, touvron2021training}, GPT2~\cite{radford2019language} &  \href{https://github.com/NVlabs/GroupViT}{Link} & CVPR'22 \\

OpenSeg~\cite{ghiasi2021scaling} & \cmark & COCO~\cite{lin2014microsoft}, LocalNarr~\cite{pont2020connecting} & 0.77M & & & Task & Dual-Enc& EffNet-B7~\cite{tan2019efficientnet}, ResNet~\cite{he2016deep}, BERT-Large~\cite{devlin2018bert} & \href{https://github.com/tensorflow/tpu/tree/641c1ac6e26ed788327b973582cbfa297d7d31e7/models/official/detection/projects/openseg}{Link} & ECCV'22\\

\midrule
% \multirow{10}{*}{\rotatebox{90}{Generative}}&

Frozen~\cite{tsimpoukelli2021multimodal} & \cmark & CC3M~\cite{sharma2018conceptual} & 3M & - & Cap & -  & Enc-Dec & NF-ResNet~\cite{brock2021high}, GPT2~\cite{radford2019language} &  - & NeurIPS'21\\

Flamingo~\cite{alayrac2022flamingo}  & \xmark & M3W~\cite{alayrac2022flamingo} & 43M &  - & Flamingo & -  & AdaptedLLm & NF-ResNet~\cite{brock2021high}, Chinchilla~\cite{hoffmann2022training} & - & NeurIPS'22 \\

OpenFlamingo~\cite{anas_awadalla_2023_7733589} & \cmark & LAION2B~\cite{schuhmann2022laion}, MMC4~\cite{zhu2023multimodal} & 2571M & - & Flamingo & - & AdaptedLLm & CLIPL-ViT~\cite{radford2019language}, LLMs  & \href{https://github.com/mlfoundations/open_flamingo}{Link} & github'23 \\

{MetaLM}~\cite{hao2022language}  & \cmark & CC~\cite{sharma2018conceptual}, VG~\cite{wu2023visual}, COCO Captions~\cite{chen2015microsoft}, SBU~\cite{ordonez2011im2text}& 10M & - & SemiCasualLM & - & Enc-Dec &  ViT~\cite{dosovitskiy2020image}, GPT2~\cite{radford2019language} & - &arXiv'23\\

{KOSMOS-1}~\cite{huang2023language} & \cmark & LAION-\{400M, 2B\}~\cite{schuhmann2021laion, schuhmann2022laion}, COYO-700M~\cite{kakaobrain2022coyo-700m}, CC3M~\cite{sharma2018conceptual}, CC12M~\cite{changpinyo2021conceptual}& 3115M & - & SemiCasualLM & - & AdaptedLLm &  CLIP-ViT~\cite{radford2021learning}, MAGNETO~\cite{wang2022foundation} & - & arXiv'23\\

{KOSMOS-v2}\cite{peng2023kosmos2}   & - & GRIT~\cite{peng2023kosmos2}, LLaVA-Instruct~\cite{liu2023llava} & 115M & - & SemiCasualLM & - & AdaptedLLm  & CLIP-ViT~\cite{radford2021learning}, MAGNETO~\cite{wang2022foundation} & \href{https://github.com/microsoft/unilm/tree/master/kosmos-2}{Link} & arXiv'23\\

SimVLM~\cite{wang2021simvlm} & \cmark & ALIGN1.8B~\cite{jia2021scaling} & 1800M &  & PrefixLM & -   & Enc-Dec & ViT~\cite{dosovitskiy2020image}, BERT~\cite{devlin2018bert}&  - & ICLR'22 \\
% , C4~\cite{raffel2020exploring}
MaskVLM~\cite{kuwon2022masked} & \cmark & CC, SBU~\cite{ordonez2011im2text}, VG~\cite{krishna2017visual}, COCO-C & 4M & ITC, ITM & MVLM & - & Fusion & ViT~\cite{dosovitskiy2020image}, RoBERTa~\cite{liu2019roberta} &  - & ICLR23 \\
mPLUG-OWL~\cite{ye2023mplugowl} & \cmark & LAION-400M~\cite{schuhmann2021laion}, COYO-700M~\cite{kakaobrain2022coyo-700m}, CC~\cite{sharma2018conceptual}, COCO~\cite{chen2015microsoft} & 1100+M & - & LM & - & AdaptedLLm & CLIP-ViT~\cite{radford2021learning}, LLaMA-7B~\cite{touvron2023llama} & \href{https://github.com/X-PLUG/mPLUG-Owl}{Link} & arXiv'23  \\

CapPa~\cite{tschannen2023image} & \xmark & WebLI~\cite{chen2022pali} & 12B & - & Cap, CapPa & - & Dual-Enc & &  - & arXiv'23  \\
\midrule
UNITER~\cite{chen2020uniter}  & \cmark & COCO~\cite{lin2014microsoft}, VG~\cite{krishna2017visual}, CC3M~\cite{sharma2018conceptual}, SBU~\cite{ordonez2011im2text} & 9.5M & ITM & MLM & - & Fusion  & Faster-RCNN~\cite{ren2015faster}, BERT~\cite{devlin2018bert} & \href{https://github.com/ChenRocks/UNITER}{Link} & ECCV'20 \\

Pixel2Seqv2~\cite{chen2022unified} & \cmark & COCO~\cite{lin2014microsoft} & 0.12M & - & VLM & - &   Enc-Dec & ViT-B~\cite{dosovitskiy2020image}, Transformer~\cite{vaswani2017attention} &  - & NeurIPS'22\\

VL-x ~\cite{cho2021unifying} & \cmark & COCO~\cite{lin2014microsoft, chen2015microsoft}, VG~\cite{krishna2017visual}, VQAv2~\cite{goyal2016making}, GQA~\cite{hudson2019gqa}, Visual7W~\cite{zhu2016visual7w, tan2019lxmert}& 9.18M & ITM & MLM & Task & Enc-Dec &  BEiTv2~\cite{peng2022beit}, RoBERTa~\cite{liu2019roberta} &  &ICML'21\\


CoCa~\cite{yu2022coca}        & \xmark & JFT3B~\cite{zhai2022scaling}, ALIGN~\cite{jia2021scaling} & 4800M   & NSL  & Cap & - & Fusion & ViT~\cite{dosovitskiy2020image}, Transformer~\cite{vaswani2017attention} & - & TMLR'23 \\

FLAVA~\cite{singh2022flava}   & \cmark & PMD (COCO~\cite{lin2014microsoft}, SBU~\cite{ordonez2011im2text}, LocaNarr~\cite{pont2020connecting}, VG~\cite{krishna2017visual}, WikiIT~\cite{srinivasan2021wit}, CC12M~\cite{changpinyo2021conceptual}, RedCaps~\cite{DBLP:conf/nips/DesaiKA021}, YFCC100M~\cite{thomee2016yfcc100m}) & 70M & ITC,ITM & MMM,MIM,MLM & - & Fusion & ViT~\cite{dosovitskiy2020image}, Transformer~\cite{vaswani2017attention} & \href{https://github.com/facebookresearch/multimodal/tree/main/examples/flava}{Link}&  CVPR'22  \\

PaLI~\cite{chen2022pali} & \xmark & WebLI~\cite{chen2022pali}, CC3M~\cite{sharma2018conceptual}+   & 1600M & - & - & - & Enc-Dc & ViT-e~\cite{chen2022pali}, mT5~\cite{xue2020mt5} & - & arXiv'23 \\

BLIP~\cite{li2022blip} & \cmark & COCO~\cite{lin2014microsoft}, VG~\cite{wu2023visual}, CC3M~\cite{sharma2018conceptual},C12M~\cite{changpinyo2021conceptual}, SBU~\cite{ordonez2011im2text}, LAION~\cite{schuhmann2021laion}, WebCapFit~\cite{li2022blip} & 129M & ITC,ITM &  LM & - & Fusion & ViT~\cite{dosovitskiy2020image}, BERT~\cite{devlin2018bert} & \href{https://github.com/salesforce/BLIP}{Link}& arXiv'22 \\ 


BridgeTower~\cite{xu2022bridge} & \cmark & CC3M~\cite{sharma2018conceptual}, SBU~\cite{ordonez2011im2text}, COCO~\cite{chen2015microsoft}, VG~\cite{krishna2017visual} & 4M & ITM & MLM & - & Fusion & CLIP-ViT~\cite{radford2021learning}, RoBERTa~\cite{liu2019roberta} &  
\href{https://github.com/microsoft/BridgeTower}{Link} & AAAI'23\\

X-FM~\cite{zhang2023toward} & \cmark & COCO~\cite{lin2014microsoft}, SBU~\cite{ordonez2011im2text}, RefCOCO~\cite{yu2016modeling},  VG~\cite{wu2023visual}, CC3M~\cite{sharma2018conceptual}, O365~\cite{shao2019objects365}, OI~\cite{krasin2017openimages}, IN21K~\cite{fei2009imagenet}, C4~\cite{raffel2020exploring}, RoBERTa Corpus~\cite{liu2019roberta} & 20M & ITC,ITM & MLM,MIM,IMLM & BBP & Fusion & BEiTv2~\cite{wang2022image}, RoBERTa~\cite{liu2019roberta} &  \href{https://github.com/zhangxinsong-nlp/XFM}{Link} &  arXiv'23 \\

BLIP-2~\cite{li2023blip} & \cmark & COCO~\cite{lin2014microsoft}, VG~\cite{wu2023visual}, CC3M~\cite{sharma2018conceptual}, C12M~\cite{changpinyo2021conceptual},SBU~\cite{ordonez2011im2text}, LAION~\cite{schuhmann2021laion} & 129M & ITC,ITM & ITG & - & AdaptedLLm & CLIP-ViT~\cite{radford2021learning}, EVA-CLIP~\cite{fang2023eva}, OPT~\cite{zhang2022opt}, FlanT5~\cite{chung2022scaling}  & \href{
https://github.com/salesforce/LAVIS/tree/main/projects/blip2}{Link}& arXiv'23\\

Instruct-BLIP~\cite{dai2023instructblip} & \cmark &COCO~\cite{lin2014microsoft}, WebCapFit~\cite{li2022blip}, NoCaps~\cite{agrawal2019nocaps}, Flicker30k~\cite{young2014image}, TextCaps~\cite{sidorov2020textcaps}, VQAv2~\cite{goyal2016making}, VizWiz~\cite{gurari2018vizwiz}, GQA~\cite{hudson2019gqa},  VSR, IconQA~\cite{lu2021iconqa}, OKVQA~\cite{marino2019ok}, A-OKVQA~\cite{schwenk2022okvqa}, SciQA~\cite{lu2022learn}, VD~\cite{das2017visual}, OCRVQA~\cite{mishra2019ocr}, TVQA~\cite{singh2019towards}, HM~\cite{kiela2020hateful}, LI~\cite{liu2023visual}, MQA, MSQA~\cite{xu2017video}, iVQA~\cite{yang2021just} & - & & LM & - &  AdaptedLLm & EVA-ViT~\cite{sun2023eva}, LLM~\cite{vicuna2023, chung2022scaling}\footnote{FlanT5, Vicuna} & \href{https://github.com/salesforce/LAVIS/tree/main/projects/instructblip}{Link} & arXiv'23\\

TaCA~\cite{zhang2023taca} & \cmark & LAION-400M~\cite{schuhmann2021laion} & 400M & ITC &  - & Distil & - & CLIP-ViT~\cite{radford2019language}, GPT2~\cite{radford2019language} &  \href{https://github.com/TencentARC/TaCA}{Link} & arXiv'23\\

VPGTrans~\cite{zhang2023transfer} & \cmark & COCO-C, SBU & 1.4M & - & - & Sim & - & BLIP-2 &    \href{https://github.com/VPGTrans/VPGTrans}{Link} & arXiv'23\\

FIBER~\cite{dou2022coarse} & \cmark & COCO~\cite{lin2014microsoft}, CC, SBU~\cite{ordonez2011im2text}, VG~\cite{wu2023visual}, O365~\cite{shao2019objects365} & 4.8M & ITC,ITM & MLM & Task & Fusion & Swin~\cite{liu2021swin}, RoBERTa~\cite{liu2019roberta} &   \href{https://github.com/microsoft/FIBER}{Link} & NeurIPS'22\\

UniDetector~\cite{wang2023detecting} & \cmark & COCO~\cite{lin2014microsoft}, O365~\cite{shao2019objects365}, OI~\cite{krasin2017openimages} & - & - & - & Task & Dual-Enc & RegionCLIP~\cite{zhong2022regionclip} & \href{https://github.com/zhenyuw16/UniDetector}{Link} &  CVPR'23\\

X-Decoder~\cite{zou2022xdecoder}  & \cmark & COCO17, CC, SBU~\cite{ordonez2011im2text}, VG~\cite{wu2023visual} COCO-C & 4.07M & ITC & Cap & Task & Fusion & Focal-T~\cite{yang2022focal}, DaViT~\cite{ding2022davit}, GPT2~\cite{radford2019language}  & \href{https://github.com/microsoft/X-Decoder/tree/main}{Link} & CVPR'23 \\

GLIPv2~\cite{zhang2022glipv2} &  \cmark & FiveODs, GoldG, CC15M, SBU~\cite{ordonez2011im2text}, COCO, LVIS & - & RWC & - & Task & Fusion & Swin-T~\cite{liu2021swin}, Transformer~\cite{vaswani2017attention}  & \href{https://github.com/microsoft/GLIP}{Link}& NeurIPS'22\\
% &&Stone Needle~\cite{liu2023stone}\\
% UnmaskedTeacher~\cite{li2023unmasked} & & & & VTC,VTM & MLM & UTA\\
\bottomrule
\end{tabular}}
% \vspace{-0.5em}
    \caption{An overview of Textually Prompted Models. We exhibit different facets of these models that contrast them, including pre-training datasets and their sizes, pre-training objectives, architecture, publication venues, and online information. More details are discussed in Section~\ref{sec:background}.}
    \label{tab:textually_prompted_models}
\end{table*}
