\begin{thebibliography}{366}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[cha()]{chatgpt}
Introducing chatgpt.
\newblock \url{https://openai.com/blog/chatgpt}.
\newblock Accessed: 2023-06-30.

\bibitem[Cha(2023)]{ChatgptJailbreak}
How to jailbreak chatgpt, 2023.
\newblock URL \url{https://watcher.guru/news/how-to-jailbreak-chatgpt}.

\bibitem[Red(2023)]{RedteamingLLMs}
Red-teaming large-language models, 2023.
\newblock URL \url{https://huggingface.co/blog/red-teaming}.

\bibitem[Abdelhamed et~al.(2018)Abdelhamed, Lin, and Brown]{abdelhamed2018high}
Abdelrahman Abdelhamed, Stephen Lin, and Michael~S Brown.
\newblock A high-quality denoising dataset for smartphone cameras.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 1692--1700, 2018.

\bibitem[Agrawal et~al.(2019)Agrawal, Desai, Wang, Chen, Jain, Johnson, Batra,
  Parikh, Lee, and Anderson]{agrawal2019nocaps}
Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark
  Johnson, Dhruv Batra, Devi Parikh, Stefan Lee, and Peter Anderson.
\newblock Nocaps: Novel object captioning at scale.
\newblock In \emph{Proceedings of the IEEE/CVF international conference on
  computer vision}, pages 8948--8957, 2019.

\bibitem[Ahn et~al.(2022)Ahn, Brohan, Brown, Chebotar, Cortes, David, Finn, Fu,
  Gopalakrishnan, Hausman, et~al.]{ahn2022can}
Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron
  David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman,
  et~al.
\newblock Do as i can, not as i say: Grounding language in robotic affordances.
\newblock \emph{arXiv preprint arXiv:2204.01691}, 2022.

\bibitem[Alayrac et~al.(2022)Alayrac, Donahue, Luc, Miech, Barr, Hasson, Lenc,
  Mensch, Millican, Reynolds, et~al.]{alayrac2022flamingo}
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr,
  Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds,
  et~al.
\newblock Flamingo: a visual language model for few-shot learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 23716--23736, 2022.

\bibitem[Anil et~al.(2023)Anil, Dai, Firat, Johnson, Lepikhin, Passos, Shakeri,
  Taropa, Bailey, Chen, et~al.]{anil2023palm}
Rohan Anil, Andrew~M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin,
  Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen,
  et~al.
\newblock Palm 2 technical report.
\newblock \emph{arXiv preprint arXiv:2305.10403}, 2023.

\bibitem[Antol et~al.(2015)Antol, Agrawal, Lu, Mitchell, Batra, Zitnick, and
  Parikh]{antol2015vqa}
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra,
  C~Lawrence Zitnick, and Devi Parikh.
\newblock Vqa: Visual question answering.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pages 2425--2433, 2015.

\bibitem[Antonelli et~al.(2022)Antonelli, Reinke, Bakas, Farahani,
  Kopp-Schneider, Landman, Litjens, Menze, Ronneberger, Summers,
  et~al.]{antonelli2022medical}
Michela Antonelli, Annika Reinke, Spyridon Bakas, Keyvan Farahani, Annette
  Kopp-Schneider, Bennett~A Landman, Geert Litjens, Bjoern Menze, Olaf
  Ronneberger, Ronald~M Summers, et~al.
\newblock The medical segmentation decathlon.
\newblock \emph{Nature communications}, 13\penalty0 (1):\penalty0 4128, 2022.

\bibitem[Awadalla et~al.(2023)Awadalla, Gao, Gardner, Hessel, Hanafy, Zhu,
  Marathe, Bitton, Gadre, Jitsev, Kornblith, Koh, Ilharco, Wortsman, and
  Schmidt]{anas_awadalla_2023_7733589}
Anas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel, Yusuf Hanafy, Wanrong
  Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Jenia Jitsev, Simon
  Kornblith, Pang~Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig
  Schmidt.
\newblock Openflamingo, March 2023.
\newblock URL \url{https://doi.org/10.5281/zenodo.7733589}.

\bibitem[Bain et~al.(2021)Bain, Nagrani, Varol, and Zisserman]{bain2021frozen}
Max Bain, Arsha Nagrani, G{\"u}l Varol, and Andrew Zisserman.
\newblock Frozen in time: A joint video and image encoder for end-to-end
  retrieval.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 1728--1738, 2021.

\bibitem[Baker et~al.(2022)Baker, Akkaya, Zhokhov, Huizinga, Tang, Ecoffet,
  Houghton, Sampedro, and Clune]{baker2022learning}
Bowen Baker, Ilge Akkaya, Peter Zhokhov, Joost Huizinga, Jie Tang, Adrien
  Ecoffet, Brandon Houghton, Raul Sampedro, and Jeff Clune.
\newblock Learning to play minecraft with video pretraining (vpt).
\newblock \emph{OpenAI Blog, June}, 23, 2022.

\bibitem[Bernal et~al.(2015)Bernal, S{\'a}nchez, Fern{\'a}ndez-Esparrach, Gil,
  Rodr{\'\i}guez, and Vilari{\~n}o]{bernal2015wm}
Jorge Bernal, F~Javier S{\'a}nchez, Gloria Fern{\'a}ndez-Esparrach, Debora Gil,
  Cristina Rodr{\'\i}guez, and Fernando Vilari{\~n}o.
\newblock Wm-dova maps for accurate polyp highlighting in colonoscopy:
  Validation vs. saliency maps from physicians.
\newblock \emph{Computerized medical imaging and graphics}, 43:\penalty0
  99--111, 2015.

\bibitem[Berrios et~al.(2023)Berrios, Mittal, Thrush, Kiela, and
  Singh]{berrios2023towards}
William Berrios, Gautam Mittal, Tristan Thrush, Douwe Kiela, and Amanpreet
  Singh.
\newblock Towards language models that can see: Computer vision through the
  lens of natural language.
\newblock \emph{arXiv preprint arXiv:2306.16410}, 2023.

\bibitem[Bilic et~al.(2023)Bilic, Christ, Li, Vorontsov, Ben-Cohen, Kaissis,
  Szeskin, Jacobs, Mamani, Chartrand, et~al.]{bilic2023liver}
Patrick Bilic, Patrick Christ, Hongwei~Bran Li, Eugene Vorontsov, Avi
  Ben-Cohen, Georgios Kaissis, Adi Szeskin, Colin Jacobs, Gabriel
  Efrain~Humpire Mamani, Gabriel Chartrand, et~al.
\newblock The liver tumor segmentation benchmark (lits).
\newblock \emph{Medical Image Analysis}, 84:\penalty0 102680, 2023.

\bibitem[Bolya et~al.(2019)Bolya, Zhou, Xiao, and Lee]{bolya2019yolact}
Daniel Bolya, Chong Zhou, Fanyi Xiao, and Yong~Jae Lee.
\newblock Yolact: Real-time instance segmentation.
\newblock In \emph{Proceedings of the IEEE/CVF international conference on
  computer vision}, pages 9157--9166, 2019.

\bibitem[Bommasani et~al.(2021)Bommasani, Hudson, Adeli, Altman, Arora, von
  Arx, Bernstein, Bohg, Bosselut, Brunskill,
  et~al.]{bommasani2021opportunities}
Rishi Bommasani, Drew~A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney
  von Arx, Michael~S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma
  Brunskill, et~al.
\newblock On the opportunities and risks of foundation models.
\newblock \emph{arXiv preprint arXiv:2108.07258}, 2021.

\bibitem[Brock et~al.(2021)Brock, De, Smith, and Simonyan]{brock2021high}
Andy Brock, Soham De, Samuel~L Smith, and Karen Simonyan.
\newblock High-performance large-scale image recognition without normalization.
\newblock In \emph{International Conference on Machine Learning}, pages
  1059--1071. PMLR, 2021.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 1877--1901, 2020.

\bibitem[Bubeck et~al.(2023)Bubeck, Chandrasekaran, Eldan, Gehrke, Horvitz,
  Kamar, Lee, Lee, Li, Lundberg, et~al.]{bubeck2023sparks}
S{\'e}bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric
  Horvitz, Ece Kamar, Peter Lee, Yin~Tat Lee, Yuanzhi Li, Scott Lundberg,
  et~al.
\newblock Sparks of artificial general intelligence: Early experiments with
  gpt-4.
\newblock \emph{arXiv preprint arXiv:2303.12712}, 2023.

\bibitem[Byeon et~al.(2022)Byeon, Park, Kim, Lee, Baek, and
  Kim]{kakaobrain2022coyo-700m}
Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and
  Saehoon Kim.
\newblock Coyo-700m: Image-text pair dataset.
\newblock \url{https://github.com/kakaobrain/coyo-dataset}, 2022.

\bibitem[Caesar et~al.(2018)Caesar, Uijlings, and Ferrari]{caesar2018coco}
Holger Caesar, Jasper Uijlings, and Vittorio Ferrari.
\newblock Coco-stuff: Thing and stuff classes in context.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 1209--1218, 2018.

\bibitem[Cao et~al.(2022)Cao, Tan, Gao, Chen, Heng, and Li]{cao2022survey}
Hanqun Cao, Cheng Tan, Zhangyang Gao, Guangyong Chen, Pheng-Ann Heng, and
  Stan~Z Li.
\newblock A survey on generative diffusion model.
\newblock \emph{arXiv preprint arXiv:2209.02646}, 2022.

\bibitem[Carion et~al.(2020)Carion, Massa, Synnaeve, Usunier, Kirillov, and
  Zagoruyko]{carion2020end}
Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander
  Kirillov, and Sergey Zagoruyko.
\newblock End-to-end object detection with transformers.
\newblock In \emph{Computer Vision--ECCV 2020: 16th European Conference,
  Glasgow, UK, August 23--28, 2020, Proceedings, Part I 16}, pages 213--229.
  Springer, 2020.

\bibitem[Carlini et~al.(2023)Carlini, Nasr, Choquette-Choo, Jagielski, Gao,
  Awadalla, Koh, Ippolito, Lee, Tramer, et~al.]{carlini2023aligned}
Nicholas Carlini, Milad Nasr, Christopher~A Choquette-Choo, Matthew Jagielski,
  Irena Gao, Anas Awadalla, Pang~Wei Koh, Daphne Ippolito, Katherine Lee,
  Florian Tramer, et~al.
\newblock Are aligned neural networks adversarially aligned?
\newblock \emph{arXiv preprint arXiv:2306.15447}, 2023.

\bibitem[Caron et~al.(2021)Caron, Touvron, Misra, Jégou, Mairal, Bojanowski,
  and Joulin]{caron2021emerging}
Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr
  Bojanowski, and Armand Joulin.
\newblock Emerging properties in self-supervised vision transformers.
\newblock \emph{arXiv preprint arXiv: 2104.14294}, 2021.

\bibitem[Carpenter et~al.(1990)Carpenter, Just, and Shell]{carpenter1990one}
Patricia~A Carpenter, Marcel~A Just, and Peter Shell.
\newblock What one intelligence test measures: a theoretical account of the
  processing in the raven progressive matrices test.
\newblock \emph{Psychological review}, 97\penalty0 (3):\penalty0 404, 1990.

\bibitem[Chakraborty et~al.(2023)Chakraborty, Weerakoon, Poddar, Tokekar, Bedi,
  and Manocha]{chakraborty2023re}
Souradip Chakraborty, Kasun Weerakoon, Prithvi Poddar, Pratap Tokekar,
  Amrit~Singh Bedi, and Dinesh Manocha.
\newblock Re-move: An adaptive policy design approach for dynamic environments
  via language-based feedback.
\newblock \emph{arXiv preprint arXiv:2303.07622}, 2023.

\bibitem[Changpinyo et~al.(2021)Changpinyo, Sharma, Ding, and
  Soricut]{changpinyo2021conceptual}
Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut.
\newblock Conceptual 12m: Pushing web-scale image-text pre-training to
  recognize long-tail visual concepts.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 3558--3568, 2021.

\bibitem[Chao et~al.(2019)Chao, Kao, Ruan, Huang, and Lin]{chao2019hardnet}
Ping Chao, Chao-Yang Kao, Yu-Shan Ruan, Chien-Hsiang Huang, and Youn-Long Lin.
\newblock Hardnet: A low memory traffic network.
\newblock In \emph{Proceedings of the IEEE/CVF international conference on
  computer vision}, pages 3552--3561, 2019.

\bibitem[Chavan et~al.(2023)Chavan, Liu, Gupta, Xing, and Shen]{chavan2023one}
Arnav Chavan, Zhuang Liu, Deepak Gupta, Eric Xing, and Zhiqiang Shen.
\newblock One-for-all: Generalized lora for parameter-efficient fine-tuning.
\newblock \emph{arXiv preprint arXiv:2306.07967}, 2023.

\bibitem[Chen and Dolan(2011)]{chen2011collecting}
David Chen and William~B Dolan.
\newblock Collecting highly parallel data for paraphrase evaluation.
\newblock In \emph{Proceedings of the 49th annual meeting of the association
  for computational linguistics: human language technologies}, pages 190--200,
  2011.

\bibitem[Chen et~al.(2023{\natexlab{a}})Chen, Zhu, Haydarov, Li, and
  Elhoseiny]{chen2023video}
Jun Chen, Deyao Zhu, Kilichbek Haydarov, Xiang Li, and Mohamed Elhoseiny.
\newblock Video chatcaptioner: Towards the enriched spatiotemporal
  descriptions.
\newblock \emph{arXiv preprint arXiv:2304.04227}, 2023{\natexlab{a}}.

\bibitem[Chen et~al.(2023{\natexlab{b}})Chen, Liu, Chen, Zhang, Li, Zou, and
  Shi]{chen2023rsprompter}
Keyan Chen, Chenyang Liu, Hao Chen, Haotian Zhang, Wenyuan Li, Zhengxia Zou,
  and Zhenwei Shi.
\newblock Rsprompter: Learning to prompt for remote sensing instance
  segmentation based on visual foundation model, 2023{\natexlab{b}}.

\bibitem[Chen et~al.(2021)Chen, Tworek, Jun, Yuan, Pinto, Kaplan, Edwards,
  Burda, Joseph, Brockman, et~al.]{chen2021evaluating}
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de~Oliveira
  Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
  Brockman, et~al.
\newblock Evaluating large language models trained on code.
\newblock \emph{arXiv preprint arXiv:2107.03374}, 2021.

\bibitem[Chen et~al.(2023{\natexlab{c}})Chen, He, Li, Jin, Feng, and
  Liu]{chen2023cosa}
Sihan Chen, Xingjian He, Handong Li, Xiaojie Jin, Jiashi Feng, and Jing Liu.
\newblock Cosa: Concatenated sample pretrained vision-language foundation
  model.
\newblock \emph{arXiv preprint arXiv: 2306.09085}, 2023{\natexlab{c}}.

\bibitem[Chen et~al.(2023{\natexlab{d}})Chen, Zhu, Ding, Cao, Zhang, Wang, Li,
  Sun, Mao, and Zang]{chen2023sam}
Tianrun Chen, Lanyun Zhu, Chaotao Ding, Runlong Cao, Shangzhan Zhang, Yan Wang,
  Zejian Li, Lingyun Sun, Papa Mao, and Ying Zang.
\newblock Sam fails to segment anything?--sam-adapter: Adapting sam in
  underperformed scenes: Camouflage, shadow, and more.
\newblock \emph{arXiv preprint arXiv:2304.09148}, 2023{\natexlab{d}}.

\bibitem[Chen et~al.(2020{\natexlab{a}})Chen, Kornblith, Norouzi, and
  Hinton]{chen2020simple}
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In \emph{International conference on machine learning}, pages
  1597--1607. PMLR, 2020{\natexlab{a}}.

\bibitem[Chen et~al.(2020{\natexlab{b}})Chen, Kornblith, Swersky, Norouzi, and
  Hinton]{chen2020big}
Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey~E
  Hinton.
\newblock Big self-supervised models are strong semi-supervised learners.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 22243--22255, 2020{\natexlab{b}}.

\bibitem[Chen et~al.(2022{\natexlab{a}})Chen, Saxena, Li, Lin, Fleet, and
  Hinton]{chen2022unified}
Ting Chen, Saurabh Saxena, Lala Li, Tsung-Yi Lin, David~J Fleet, and Geoffrey~E
  Hinton.
\newblock A unified sequence interface for vision tasks.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 31333--31346, 2022{\natexlab{a}}.

\bibitem[Chen et~al.(2022{\natexlab{b}})Chen, Wang, Changpinyo, Piergiovanni,
  Padlewski, Salz, Goodman, Grycner, Mustafa, Beyer, et~al.]{chen2022pali}
Xi~Chen, Xiao Wang, Soravit Changpinyo, AJ~Piergiovanni, Piotr Padlewski,
  Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer,
  et~al.
\newblock Pali: A jointly-scaled multilingual language-image model.
\newblock \emph{arXiv preprint arXiv:2209.06794}, 2022{\natexlab{b}}.

\bibitem[Chen et~al.(2015)Chen, Fang, Lin, Vedantam, Gupta, Doll{\'a}r, and
  Zitnick]{chen2015microsoft}
Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr
  Doll{\'a}r, and C~Lawrence Zitnick.
\newblock Microsoft coco captions: Data collection and evaluation server.
\newblock \emph{arXiv preprint arXiv:1504.00325}, 2015.

\bibitem[Chen et~al.(2020{\natexlab{c}})Chen, Li, Yu, El~Kholy, Ahmed, Gan,
  Cheng, and Liu]{chen2020uniter}
Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El~Kholy, Faisal Ahmed, Zhe Gan,
  Yu~Cheng, and Jingjing Liu.
\newblock Uniter: Universal image-text representation learning.
\newblock In \emph{Computer Vision--ECCV 2020: 16th European Conference,
  Glasgow, UK, August 23--28, 2020, Proceedings, Part XXX}, pages 104--120.
  Springer, 2020{\natexlab{c}}.

\bibitem[Cheng et~al.(2021)Cheng, Choudhuri, Misra, Kirillov, Girdhar, and
  Schwing]{cheng2021mask2former}
Bowen Cheng, Anwesa Choudhuri, Ishan Misra, Alexander Kirillov, Rohit Girdhar,
  and Alexander~G Schwing.
\newblock Mask2former for video instance segmentation.
\newblock \emph{arXiv preprint arXiv:2112.10764}, 2021.

\bibitem[Cheng et~al.(2014)Cheng, Han, Zhou, and Guo]{cheng2014multi}
Gong Cheng, Junwei Han, Peicheng Zhou, and Lei Guo.
\newblock Multi-class geospatial object detection and geographic image
  classification based on collection of part detectors.
\newblock \emph{ISPRS Journal of Photogrammetry and Remote Sensing},
  98:\penalty0 119--132, 2014.

\bibitem[Cheng and Schwing(2022)]{cheng2022xmem}
Ho~Kei Cheng and Alexander~G Schwing.
\newblock Xmem: Long-term video object segmentation with an atkinson-shiffrin
  memory model.
\newblock In \emph{Computer Vision--ECCV 2022: 17th European Conference, Tel
  Aviv, Israel, October 23--27, 2022, Proceedings, Part XXVIII}, pages
  640--658. Springer, 2022.

\bibitem[Cheng et~al.(2023)Cheng, Li, Xu, Li, Yang, Wang, and
  Yang]{cheng2023segment}
Yangming Cheng, Liulei Li, Yuanyou Xu, Xiaodi Li, Zongxin Yang, Wenguan Wang,
  and Yi~Yang.
\newblock Segment and track anything.
\newblock \emph{arXiv preprint arXiv:2305.06558}, 2023.

\bibitem[Cherti et~al.(2023)Cherti, Beaumont, Wightman, Wortsman, Ilharco,
  Gordon, Schuhmann, Schmidt, and Jitsev]{cherti2023reproducible}
Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel
  Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev.
\newblock Reproducible scaling laws for contrastive language-image learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 2818--2829, 2023.

\bibitem[Chiang et~al.(2023)Chiang, Li, Lin, Sheng, Wu, Zhang, Zheng, Zhuang,
  Zhuang, Gonzalez, Stoica, and Xing]{vicuna2023}
Wei-Lin Chiang, Zhuohan Li, Zi~Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin
  Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph~E. Gonzalez, Ion Stoica, and
  Eric~P. Xing.
\newblock Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt
  quality, March 2023.
\newblock URL \url{https://lmsys.org/blog/2023-03-30-vicuna/}.

\bibitem[Cho et~al.(2021)Cho, Lei, Tan, and Bansal]{cho2021unifying}
Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal.
\newblock Unifying vision-and-language tasks via text generation.
\newblock In \emph{International Conference on Machine Learning}, pages
  1931--1942. PMLR, 2021.

\bibitem[Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra,
  Roberts, Barham, Chung, Sutton, Gehrmann, et~al.]{chowdhery2022palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
  Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian
  Gehrmann, et~al.
\newblock Palm: Scaling language modeling with pathways.
\newblock \emph{arXiv preprint arXiv:2204.02311}, 2022.

\bibitem[Christiano et~al.(2017)Christiano, Leike, Brown, Martic, Legg, and
  Amodei]{christiano2017deep}
Paul~F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario
  Amodei.
\newblock Deep reinforcement learning from human preferences.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Chung et~al.(2022)Chung, Hou, Longpre, Zoph, Tay, Fedus, Li, Wang,
  Dehghani, Brahma, et~al.]{chung2022scaling}
Hyung~Won Chung, Le~Hou, Shayne Longpre, Barret Zoph, Yi~Tay, William Fedus,
  Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et~al.
\newblock Scaling instruction-finetuned language models.
\newblock \emph{arXiv preprint arXiv:2210.11416}, 2022.

\bibitem[Conti et~al.(2018)Conti, Madhavan, Petroski~Such, Lehman, Stanley, and
  Clune]{conti2018improving}
Edoardo Conti, Vashisht Madhavan, Felipe Petroski~Such, Joel Lehman, Kenneth
  Stanley, and Jeff Clune.
\newblock Improving exploration in evolution strategies for deep reinforcement
  learning via a population of novelty-seeking agents.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Croitoru et~al.(2023)Croitoru, Hondru, Ionescu, and
  Shah]{croitoru2023diffusion}
Florinel-Alin Croitoru, Vlad Hondru, Radu~Tudor Ionescu, and Mubarak Shah.
\newblock Diffusion models in vision: A survey.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 2023.

\bibitem[Dai et~al.(2023)Dai, Li, Li, Tiong, Zhao, Wang, Li, Fung, and
  Hoi]{dai2023instructblip}
Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng~Huat Tiong, Junqi Zhao,
  Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi.
\newblock Instructblip: Towards general-purpose vision-language models with
  instruction tuning.
\newblock \emph{arXiv preprint arXiv: 2305.06500}, 2023.

\bibitem[Das et~al.(2017)Das, Kottur, Gupta, Singh, Yadav, Moura, Parikh, and
  Batra]{das2017visual}
Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav,
  Jos{\'e}~MF Moura, Devi Parikh, and Dhruv Batra.
\newblock Visual dialog.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 326--335, 2017.

\bibitem[Desai et~al.(2021)Desai, Kaul, Aysola, and
  Johnson]{DBLP:conf/nips/DesaiKA021}
Karan Desai, Gaurav Kaul, Zubin Aysola, and Justin Johnson.
\newblock Redcaps: Web-curated image-text data created by the people, for the
  people.
\newblock In Joaquin Vanschoren and Sai{-}Kit Yeung, editors, \emph{Proceedings
  of the Neural Information Processing Systems Track on Datasets and Benchmarks
  1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual}, 2021.
\newblock URL
  \url{https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/e00da03b685a0dd18fb6a08af0923de0-Abstract-round1.html}.

\bibitem[Dettmers et~al.(2023)Dettmers, Pagnoni, Holtzman, and
  Zettlemoyer]{dettmers2023qlora}
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer.
\newblock Qlora: Efficient finetuning of quantized llms.
\newblock \emph{arXiv preprint arXiv:2305.14314}, 2023.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Ding et~al.(2022)Ding, Xiao, Codella, Luo, Wang, and
  Yuan]{ding2022davit}
Mingyu Ding, Bin Xiao, Noel Codella, Ping Luo, Jingdong Wang, and Lu~Yuan.
\newblock Davit: Dual attention vision transformers.
\newblock In \emph{European Conference on Computer Vision}, pages 74--92.
  Springer, 2022.

\bibitem[Ding et~al.(2023)Ding, Zhang, Paxton, and Zhang]{ding2023task}
Yan Ding, Xiaohan Zhang, Chris Paxton, and Shiqi Zhang.
\newblock Task and motion planning with large language models for object
  rearrangement.
\newblock \emph{arXiv preprint arXiv: 2303.06247}, 2023.

\bibitem[Djolonga et~al.(2021)Djolonga, Yung, Tschannen, Romijnders, Beyer,
  Kolesnikov, Puigcerver, Minderer, D'Amour, Moldovan,
  et~al.]{djolonga2021robustness}
Josip Djolonga, Jessica Yung, Michael Tschannen, Rob Romijnders, Lucas Beyer,
  Alexander Kolesnikov, Joan Puigcerver, Matthias Minderer, Alexander D'Amour,
  Dan Moldovan, et~al.
\newblock On robustness and transferability of convolutional neural networks.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 16458--16468, 2021.

\bibitem[Dong et~al.(2022{\natexlab{a}})Dong, Li, Dai, Zheng, Wu, Chang, Sun,
  Xu, and Sui]{dong2022survey}
Qingxiu Dong, Lei Li, Damai Dai, Ce~Zheng, Zhiyong Wu, Baobao Chang, Xu~Sun,
  Jingjing Xu, and Zhifang Sui.
\newblock A survey for in-context learning.
\newblock \emph{arXiv preprint arXiv:2301.00234}, 2022{\natexlab{a}}.

\bibitem[Dong et~al.(2022{\natexlab{b}})Dong, Bao, Chen, Zhang, Yu, Yuan, Chen,
  and Guo]{dong2022cswin}
Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming Zhang, Nenghai Yu, Lu~Yuan,
  Dong Chen, and Baining Guo.
\newblock Cswin transformer: A general vision transformer backbone with
  cross-shaped windows.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 12124--12134, 2022{\natexlab{b}}.

\bibitem[Dong et~al.(2023)Dong, Bao, Zheng, Zhang, Chen, Yang, Zeng, Zhang,
  Yuan, Chen, et~al.]{dong2023maskclip}
Xiaoyi Dong, Jianmin Bao, Yinglin Zheng, Ting Zhang, Dongdong Chen, Hao Yang,
  Ming Zeng, Weiming Zhang, Lu~Yuan, Dong Chen, et~al.
\newblock Maskclip: Masked self-distillation advances contrastive
  language-image pretraining.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 10995--11005, 2023.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly,
  et~al.]{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem[Dou et~al.(2022{\natexlab{a}})Dou, Kamath, Gan, Zhang, Wang, Li, Liu,
  Liu, LeCun, Peng, et~al.]{dou2022coarse}
Zi-Yi Dou, Aishwarya Kamath, Zhe Gan, Pengchuan Zhang, Jianfeng Wang, Linjie
  Li, Zicheng Liu, Ce~Liu, Yann LeCun, Nanyun Peng, et~al.
\newblock Coarse-to-fine vision-language pre-training with fusion in the
  backbone.
\newblock \emph{arXiv preprint arXiv:2206.07643}, 2022{\natexlab{a}}.

\bibitem[Dou et~al.(2022{\natexlab{b}})Dou, Xu, Gan, Wang, Wang, Wang, Zhu,
  Zhang, Yuan, Peng, et~al.]{dou2022empirical}
Zi-Yi Dou, Yichong Xu, Zhe Gan, Jianfeng Wang, Shuohang Wang, Lijuan Wang,
  Chenguang Zhu, Pengchuan Zhang, Lu~Yuan, Nanyun Peng, et~al.
\newblock An empirical study of training end-to-end vision-and-language
  transformers.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 18166--18176, 2022{\natexlab{b}}.

\bibitem[Doveh et~al.(2023)Doveh, Arbelle, Harary, Alfassy, Herzig, Kim,
  Giryes, Feris, Panda, Ullman, et~al.]{doveh2023dense}
Sivan Doveh, Assaf Arbelle, Sivan Harary, Amit Alfassy, Roei Herzig, Donghyun
  Kim, Raja Giryes, Rogerio Feris, Rameswar Panda, Shimon Ullman, et~al.
\newblock Dense and aligned captions (dac) promote compositional reasoning in
  vl models.
\newblock \emph{arXiv preprint arXiv:2305.19595}, 2023.

\bibitem[Driess et~al.(2023)Driess, Xia, Sajjadi, Lynch, Chowdhery, Ichter,
  Wahid, Tompson, Vuong, Yu, Huang, Chebotar, Sermanet, Duckworth, Levine,
  Vanhoucke, Hausman, Toussaint, Greff, Zeng, Mordatch, and
  Florence]{driess2023palme}
Danny Driess, Fei Xia, Mehdi S.~M. Sajjadi, Corey Lynch, Aakanksha Chowdhery,
  Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong
  Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine,
  Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng,
  Igor Mordatch, and Pete Florence.
\newblock Palm-e: An embodied multimodal language model.
\newblock \emph{arXiv preprint arXiv: 2303.03378}, 2023.

\bibitem[Du et~al.(2022)Du, Liu, Li, and Zhao]{Yifan2022VLM}
Yifan Du, Zikang Liu, Junyi Li, and Wayne~Xin Zhao.
\newblock A survey of vision-language pre-trained models.
\newblock In \emph{IJCAI}, 2022.

\bibitem[Everingham et~al.(2015)Everingham, Eslami, Van~Gool, Williams, Winn,
  and Zisserman]{everingham2015pascal}
Mark Everingham, SM~Ali Eslami, Luc Van~Gool, Christopher~KI Williams, John
  Winn, and Andrew Zisserman.
\newblock The pascal visual object classes challenge: A retrospective.
\newblock \emph{International journal of computer vision}, 111:\penalty0
  98--136, 2015.

\bibitem[Eysenbach et~al.(2018)Eysenbach, Gupta, Ibarz, and
  Levine]{eysenbach2018diversity}
Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine.
\newblock Diversity is all you need: Learning skills without a reward function.
\newblock \emph{arXiv preprint arXiv:1802.06070}, 2018.

\bibitem[Fan et~al.(2022)Fan, Wang, Jiang, Mandlekar, Yang, Zhu, Tang, Huang,
  Zhu, and Anandkumar]{fan2022minedojo}
Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu,
  Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar.
\newblock Minedojo: Building open-ended embodied agents with internet-scale
  knowledge.
\newblock \emph{arXiv preprint arXiv: 2206.08853}, 2022.

\bibitem[Fan et~al.(2023)Fan, Li, Ma, Lee, Yu, and
  Hemphill]{fan2023bibliometric}
Lizhou Fan, Lingyao Li, Zihui Ma, Sanggyu Lee, Huizi Yu, and Libby Hemphill.
\newblock A bibliometric review of large language models research from 2017 to
  2023.
\newblock \emph{arXiv preprint arXiv:2304.02020}, 2023.

\bibitem[Fang et~al.(2021)Fang, Xiong, Xu, and Chen]{fang2021clip2video}
Han Fang, Pengfei Xiong, Luhui Xu, and Yu~Chen.
\newblock Clip2video: Mastering video-text retrieval via image clip.
\newblock \emph{arXiv preprint arXiv: 2106.11097}, 2021.

\bibitem[Fang et~al.(2023{\natexlab{a}})Fang, Sun, Wang, Huang, Wang, and
  Cao]{fang2023eva2}
Yuxin Fang, Quan Sun, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao.
\newblock Eva-02: A visual representation for neon genesis.
\newblock \emph{arXiv preprint arXiv:2303.11331}, 2023{\natexlab{a}}.

\bibitem[Fang et~al.(2023{\natexlab{b}})Fang, Wang, Xie, Sun, Wu, Wang, Huang,
  Wang, and Cao]{fang2023eva}
Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun
  Huang, Xinlong Wang, and Yue Cao.
\newblock Eva: Exploring the limits of masked visual representation learning at
  scale.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 19358--19369, 2023{\natexlab{b}}.

\bibitem[Fedorov et~al.(2012)Fedorov, Beichel, Kalpathy-Cramer, Finet,
  Fillion-Robin, Pujol, Bauer, Jennings, Fennessy, Sonka,
  et~al.]{fedorov20123d}
Andriy Fedorov, Reinhard Beichel, Jayashree Kalpathy-Cramer, Julien Finet,
  Jean-Christophe Fillion-Robin, Sonia Pujol, Christian Bauer, Dominique
  Jennings, Fiona Fennessy, Milan Sonka, et~al.
\newblock 3d slicer as an image computing platform for the quantitative imaging
  network.
\newblock \emph{Magnetic resonance imaging}, 30\penalty0 (9):\penalty0
  1323--1341, 2012.

\bibitem[Fei-Fei et~al.(2009)Fei-Fei, Deng, and Li]{fei2009imagenet}
Li~Fei-Fei, Jia Deng, and Kai Li.
\newblock Imagenet: Constructing a large-scale image database.
\newblock \emph{Journal of vision}, 9\penalty0 (8):\penalty0 1037--1037, 2009.

\bibitem[Gadre et~al.(2023)Gadre, Ilharco, Fang, Hayase, Smyrnis, Nguyen,
  Marten, Wortsman, Ghosh, Zhang, et~al.]{gadre2023datacomp}
Samir~Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios
  Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu
  Zhang, et~al.
\newblock Datacomp: In search of the next generation of multimodal datasets.
\newblock \emph{arXiv preprint arXiv:2304.14108}, 2023.

\bibitem[Gan et~al.(2022)Gan, Li, Li, Wang, Liu, Gao, et~al.]{gan2022vision}
Zhe Gan, Linjie Li, Chunyuan Li, Lijuan Wang, Zicheng Liu, Jianfeng Gao, et~al.
\newblock Vision-language pre-training: Basics, recent advances, and future
  trends.
\newblock \emph{Foundations and Trends{\textregistered} in Computer Graphics
  and Vision}, 14\penalty0 (3--4):\penalty0 163--352, 2022.

\bibitem[Gao et~al.(2023{\natexlab{a}})Gao, Han, Zhang, Lin, Geng, Zhou, Zhang,
  Lu, He, Yue, Li, and Qiao]{gao2023llamaadapterv2}
Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei
  Zhang, Pan Lu, Conghui He, Xiangyu Yue, Hongsheng Li, and Yu~Qiao.
\newblock Llama-adapter v2: Parameter-efficient visual instruction model.
\newblock \emph{arXiv preprint arXiv:2304.15010}, 2023{\natexlab{a}}.

\bibitem[Gao et~al.(2021)Gao, Fisch, and Chen]{gao2021making}
Tianyu Gao, Adam Fisch, and Danqi Chen.
\newblock Making pre-trained language models better few-shot learners, 2021.

\bibitem[Gao et~al.(2023{\natexlab{b}})Gao, Xia, Hu, and Gao]{gao2023desam}
Yifan Gao, Wei Xia, Dingdu Hu, and Xin Gao.
\newblock Desam: Decoupling segment anything model for generalizable medical
  image segmentation.
\newblock \emph{arXiv preprint arXiv:2306.00499}, 2023{\natexlab{b}}.

\bibitem[Ge et~al.(2023)Ge, Hua, Mei, Ji, Tan, Xu, Li, and
  Zhang]{ge2023openagi}
Yingqiang Ge, Wenyue Hua, Kai Mei, Jianchao Ji, Juntao Tan, Shuyuan Xu, Zelong
  Li, and Yongfeng Zhang.
\newblock Openagi: When llm meets domain experts.
\newblock \emph{arXiv preprint arXiv: 2304.04370}, 2023.

\bibitem[Gemmeke et~al.(2017)Gemmeke, Ellis, Freedman, Jansen, Lawrence, Moore,
  Plakal, and Ritter]{gemmeke2017audio}
Jort~F Gemmeke, Daniel~PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence,
  R~Channing Moore, Manoj Plakal, and Marvin Ritter.
\newblock Audio set: An ontology and human-labeled dataset for audio events.
\newblock In \emph{2017 IEEE international conference on acoustics, speech and
  signal processing (ICASSP)}, pages 776--780. IEEE, 2017.

\bibitem[Ghiasi et~al.(2021{\natexlab{a}})Ghiasi, Gu, Cui, and
  Lin]{ghiasi2021scaling}
Golnaz Ghiasi, Xiuye Gu, Yin Cui, and Tsung-Yi Lin.
\newblock Scaling open-vocabulary image segmentation with image-level labels.
\newblock \emph{arXiv preprint arXiv: 2112.12143}, 2021{\natexlab{a}}.

\bibitem[Ghiasi et~al.(2021{\natexlab{b}})Ghiasi, Zoph, Cubuk, Le, and
  Lin]{ghiasi2021multi}
Golnaz Ghiasi, Barret Zoph, Ekin~D Cubuk, Quoc~V Le, and Tsung-Yi Lin.
\newblock Multi-task self-training for learning general representations.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 8856--8865, 2021{\natexlab{b}}.

\bibitem[Girdhar et~al.(2023)Girdhar, El-Nouby, Liu, Singh, Alwala, Joulin, and
  Misra]{girdhar2023imagebind}
Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan~Vasudev
  Alwala, Armand Joulin, and Ishan Misra.
\newblock Imagebind: One embedding space to bind them all.
\newblock \emph{CVPR}, 2023.

\bibitem[Gong et~al.(2023)Gong, Zhong, Ma, Li, Wang, Zhang, Heng, and
  Dou]{Gong20233DSAMadapterHA}
Shizhan Gong, Yuan Zhong, Wenao Ma, Jinpeng Li, Zhao Wang, Jingyang Zhang,
  Pheng-Ann Heng, and Qi~Dou.
\newblock 3dsam-adapter: Holistic adaptation of sam from 2d to 3d for
  promptable medical image segmentation.
\newblock \emph{ArXiv}, abs/2306.13465, 2023.

\bibitem[Goyal et~al.(2016)Goyal, Khot, Summers-Stay, Batra, and
  Parikh]{goyal2016making}
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.
\newblock Making the v in vqa matter: Elevating the role of image understanding
  in visual question answering.
\newblock \emph{Computer Vision And Pattern Recognition}, 2016.
\newblock \doi{10.1007/s11263-018-1116-0}.

\bibitem[Grauman et~al.(2022)Grauman, Westbury, Byrne, Chavis, Furnari,
  Girdhar, Hamburger, Jiang, Liu, Liu, et~al.]{grauman2022ego4d}
Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino
  Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu,
  et~al.
\newblock Ego4d: Around the world in 3,000 hours of egocentric video.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 18995--19012, 2022.

\bibitem[Greshake et~al.(2023)Greshake, Abdelnabi, Mishra, Endres, Holz, and
  Fritz]{greshake2023more}
Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres, Thorsten
  Holz, and Mario Fritz.
\newblock More than you've asked for: A comprehensive analysis of novel prompt
  injection threats to application-integrated large language models.
\newblock \emph{arXiv preprint arXiv:2302.12173}, 2023.

\bibitem[Gu et~al.(2021)Gu, Lin, Kuo, and Cui]{gu2021open}
Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui.
\newblock Open-vocabulary object detection via vision and language knowledge
  distillation.
\newblock \emph{arXiv preprint arXiv:2104.13921}, 2021.

\bibitem[Guo et~al.(2023)Guo, Tang, Zhang, Wang, Wang, Zhao, and
  Li]{guo2023viewrefer}
Ziyu Guo, Yiwen Tang, Renrui Zhang, Dong Wang, Zhigang Wang, Bin Zhao, and
  Xuelong Li.
\newblock Viewrefer: Grasp the multi-view knowledge for 3d visual grounding
  with gpt and prototype guidance.
\newblock \emph{arXiv preprint arXiv: 2303.16894}, 2023.

\bibitem[Gupta et~al.(2019)Gupta, Dollar, and Girshick]{gupta2019lvis}
Agrim Gupta, Piotr Dollar, and Ross Girshick.
\newblock Lvis: A dataset for large vocabulary instance segmentation.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision
  and pattern recognition}, pages 5356--5364, 2019.

\bibitem[Gupta and Kembhavi(2023)]{gupta2023visual}
Tanmay Gupta and Aniruddha Kembhavi.
\newblock Visual programming: Compositional visual reasoning without training.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 14953--14962, 2023.

\bibitem[Gurari et~al.(2018)Gurari, Li, Stangl, Guo, Lin, Grauman, Luo, and
  Bigham]{gurari2018vizwiz}
Danna Gurari, Qing Li, Abigale~J Stangl, Anhong Guo, Chi Lin, Kristen Grauman,
  Jiebo Luo, and Jeffrey~P Bigham.
\newblock Vizwiz grand challenge: Answering visual questions from blind people.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 3608--3617, 2018.

\bibitem[Guzhov et~al.(2021)Guzhov, Raue, Hees, and
  Dengel]{guzhov2021audioclip}
Andrey Guzhov, Federico Raue, Jörn Hees, and Andreas Dengel.
\newblock Audioclip: Extending clip to image, text and audio.
\newblock \emph{arXiv preprint arXiv: 2106.13043}, 2021.

\bibitem[Hall et~al.(2023)Hall, Abrantes, Zhu, Sodunke, Shtedritski, and
  Kirk]{hall2023visogender}
Siobhan~Mackenzie Hall, Fernanda~Gon{\c{c}}alves Abrantes, Hanwen Zhu, Grace
  Sodunke, Aleksandar Shtedritski, and Hannah~Rose Kirk.
\newblock Visogender: A dataset for benchmarking gender bias in image-text
  pronoun resolution.
\newblock \emph{arXiv preprint arXiv:2306.12424}, 2023.

\bibitem[Hao et~al.(2022)Hao, Song, Dong, Huang, Chi, Wang, Ma, and
  Wei]{hao2022language}
Yaru Hao, Haoyu Song, Li~Dong, Shaohan Huang, Zewen Chi, Wenhui Wang, Shuming
  Ma, and Furu Wei.
\newblock Language models are general-purpose interfaces.
\newblock \emph{arXiv preprint arXiv: 2206.06336}, 2022.

\bibitem[Harley et~al.(2022)Harley, Fang, and Fragkiadaki]{harley2022particle}
Adam~W Harley, Zhaoyuan Fang, and Katerina Fragkiadaki.
\newblock Particle video revisited: Tracking through occlusions using point
  trajectories.
\newblock In \emph{European Conference on Computer Vision}, pages 59--75.
  Springer, 2022.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem[He et~al.(2017)He, Gkioxari, Doll{\'a}r, and Girshick]{he2017mask}
Kaiming He, Georgia Gkioxari, Piotr Doll{\'a}r, and Ross Girshick.
\newblock Mask r-cnn.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pages 2961--2969, 2017.

\bibitem[He et~al.(2020)He, Fan, Wu, Xie, and Girshick]{he2020momentum}
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.
\newblock Momentum contrast for unsupervised visual representation learning.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision
  and pattern recognition}, pages 9729--9738, 2020.

\bibitem[He et~al.(2022)He, Chen, Xie, Li, Doll{\'a}r, and
  Girshick]{he2022masked}
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll{\'a}r, and Ross
  Girshick.
\newblock Masked autoencoders are scalable vision learners.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 16000--16009, 2022.

\bibitem[Heller et~al.(2021)Heller, Isensee, Maier-Hein, Hou, Xie, Li, Nan, Mu,
  Lin, Han, et~al.]{heller2021state}
Nicholas Heller, Fabian Isensee, Klaus~H Maier-Hein, Xiaoshuai Hou, Chunmei
  Xie, Fengyi Li, Yang Nan, Guangrui Mu, Zhiyong Lin, Miofei Han, et~al.
\newblock The state of the art in kidney and kidney tumor segmentation in
  contrast-enhanced ct imaging: Results of the kits19 challenge.
\newblock \emph{Medical image analysis}, 67:\penalty0 101821, 2021.

\bibitem[Hendrycks et~al.(2020)Hendrycks, Burns, Basart, Zou, Mazeika, Song,
  and Steinhardt]{hendrycks2020measuring}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn
  Song, and Jacob Steinhardt.
\newblock Measuring massive multitask language understanding.
\newblock \emph{arXiv preprint arXiv:2009.03300}, 2020.

\bibitem[Hendrycks et~al.(2021)Hendrycks, Basart, Mu, Kadavath, Wang, Dorundo,
  Desai, Zhu, Parajuli, Guo, et~al.]{hendrycks2021many}
Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan
  Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et~al.
\newblock The many faces of robustness: A critical analysis of
  out-of-distribution generalization.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 8340--8349, 2021.

\bibitem[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai,
  Rutherford, Casas, Hendricks, Welbl, Clark, et~al.]{hoffmann2022training}
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor
  Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes
  Welbl, Aidan Clark, et~al.
\newblock Training compute-optimal large language models.
\newblock \emph{arXiv preprint arXiv:2203.15556}, 2022.

\bibitem[Hossain et~al.(2019)Hossain, Sohel, Shiratuddin, and
  Laga]{hossain2019comprehensive}
MD~Zakir Hossain, Ferdous Sohel, Mohd~Fairuz Shiratuddin, and Hamid Laga.
\newblock A comprehensive survey of deep learning for image captioning.
\newblock \emph{ACM Computing Surveys (CsUR)}, 51\penalty0 (6):\penalty0 1--36,
  2019.

\bibitem[Hu(2023)]{hu2023neural}
Anthony Hu.
\newblock Neural world models for computer vision.
\newblock \emph{arXiv preprint arXiv:2306.09179}, 2023.

\bibitem[Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, and
  Chen]{hu2021lora}
Edward~J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean
  Wang, and Weizhu Chen.
\newblock Lora: Low-rank adaptation of large language models.
\newblock \emph{International Conference On Learning Representations}, 2021.

\bibitem[Hu et~al.(2023)Hu, Pan, Li, and Yang]{hu2023advancing}
Mingzhe Hu, Shaoyan Pan, Yuheng Li, and Xiaofeng Yang.
\newblock Advancing medical imaging with language models: A journey from
  n-grams to chatgpt.
\newblock \emph{arXiv preprint arXiv: 2304.04920}, 2023.

\bibitem[Hu et~al.(2016)Hu, Rohrbach, and Darrell]{hu2016segmentation}
Ronghang Hu, Marcus Rohrbach, and Trevor Darrell.
\newblock Segmentation from natural language expressions.
\newblock In \emph{Computer Vision--ECCV 2016: 14th European Conference,
  Amsterdam, The Netherlands, October 11--14, 2016, Proceedings, Part I 14},
  pages 108--124. Springer, 2016.

\bibitem[Huang and Chang(2023)]{Huang2023ACL}
Jie Huang and Kevin Chen-Chuan Chang.
\newblock Towards reasoning in large language models: A survey.
\newblock \emph{ACL Findings}, 2023.

\bibitem[Huang et~al.(2023{\natexlab{a}})Huang, Dong, Wang, Hao, Singhal, Ma,
  Lv, Cui, Mohammed, Patra, Liu, Aggarwal, Chi, Bjorck, Chaudhary, Som, Song,
  and Wei]{huang2023language}
Shaohan Huang, Li~Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma,
  Tengchao Lv, Lei Cui, Owais~Khan Mohammed, Barun Patra, Qiang Liu, Kriti
  Aggarwal, Zewen Chi, Johan Bjorck, Vishrav Chaudhary, Subhojit Som, Xia Song,
  and Furu Wei.
\newblock Language is not all you need: Aligning perception with language
  models.
\newblock \emph{arXiv preprint arXiv: 2302.14045}, 2023{\natexlab{a}}.

\bibitem[Huang et~al.(2023{\natexlab{b}})Huang, Cao, Li, Juefei-Xu, Lin, Tsang,
  Liu, and Guo]{huang2023robustness}
Yihao Huang, Yue Cao, Tianlin Li, Felix Juefei-Xu, Di~Lin, Ivor~W Tsang, Yang
  Liu, and Qing Guo.
\newblock On the robustness of segment anything.
\newblock \emph{arXiv preprint arXiv:2305.16220}, 2023{\natexlab{b}}.

\bibitem[Hudson and Manning(2019)]{hudson2019gqa}
Drew~A Hudson and Christopher~D Manning.
\newblock Gqa: A new dataset for real-world visual reasoning and compositional
  question answering.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision
  and pattern recognition}, pages 6700--6709, 2019.

\bibitem[Huo et~al.(2021)Huo, Zhang, Liu, Lu, Gao, Yang, Wen, Zhang, Xu, Zheng,
  et~al.]{huo2021wenlan}
Yuqi Huo, Manli Zhang, Guangzhen Liu, Haoyu Lu, Yizhao Gao, Guoxing Yang,
  Jingyuan Wen, Heng Zhang, Baogui Xu, Weihao Zheng, et~al.
\newblock Wenlan: Bridging vision and language by large-scale multi-modal
  pre-training.
\newblock \emph{arXiv preprint arXiv:2103.06561}, 2021.

\bibitem[Ilharco et~al.(2021)Ilharco, Wortsman, Wightman, Gordon, Carlini,
  Taori, Dave, Shankar, Namkoong, Miller, Hajishirzi, Farhadi, and
  Schmidt]{ilharco_gabriel_2021_5143773}
Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas
  Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John
  Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt.
\newblock Openclip, July 2021.
\newblock URL \url{https://doi.org/10.5281/zenodo.5143773}.
\newblock If you use this software, please cite it as below.

\bibitem[Jain et~al.(2023)Jain, Saifullah, Wen, Kirchenbauer, Shu, Saha,
  Goldblum, Geiping, and Goldstein]{jain2023bring}
Neel Jain, Khalid Saifullah, Yuxin Wen, John Kirchenbauer, Manli Shu, Aniruddha
  Saha, Micah Goldblum, Jonas Geiping, and Tom Goldstein.
\newblock Bring your own data! self-supervised evaluation for large language
  models, 2023.

\bibitem[Jha et~al.(2020)Jha, Smedsrud, Riegler, Halvorsen, de~Lange, Johansen,
  and Johansen]{jha2020kvasir}
Debesh Jha, Pia~H Smedsrud, Michael~A Riegler, P{\aa}l Halvorsen, Thomas
  de~Lange, Dag Johansen, and H{\aa}vard~D Johansen.
\newblock Kvasir-seg: A segmented polyp dataset.
\newblock In \emph{MultiMedia Modeling: 26th International Conference, MMM
  2020, Daejeon, South Korea, January 5--8, 2020, Proceedings, Part II 26},
  pages 451--462. Springer, 2020.

\bibitem[Ji et~al.(2022)Ji, Xiao, Chou, Fan, Zhao, Chen, and
  Van~Gool]{ji2022video}
Ge-Peng Ji, Guobao Xiao, Yu-Cheng Chou, Deng-Ping Fan, Kai Zhao, Geng Chen, and
  Luc Van~Gool.
\newblock Video polyp segmentation: A deep learning perspective.
\newblock \emph{Machine Intelligence Research}, 19\penalty0 (6):\penalty0
  531--549, 2022.

\bibitem[Ji et~al.(2018)Ji, Wei, and Lu]{ji2018fully}
Shunping Ji, Shiqing Wei, and Meng Lu.
\newblock Fully convolutional networks for multisource building extraction from
  an open aerial and satellite imagery data set.
\newblock \emph{IEEE Transactions on geoscience and remote sensing},
  57\penalty0 (1):\penalty0 574--586, 2018.

\bibitem[Jia et~al.(2021{\natexlab{a}})Jia, Yang, Xia, Chen, Parekh, Pham, Le,
  Sung, Li, and Duerig]{jia2021scaling}
Chao Jia, Yinfei Yang, Ye~Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le,
  Yun-Hsuan Sung, Zhen Li, and Tom Duerig.
\newblock Scaling up visual and vision-language representation learning with
  noisy text supervision.
\newblock In \emph{International Conference on Machine Learning}, pages
  4904--4916. PMLR, 2021{\natexlab{a}}.

\bibitem[Jia et~al.(2021{\natexlab{b}})Jia, Zhu, Li, Tang, and
  Zhou]{jia2021llvip}
Xinyu Jia, Chuang Zhu, Minzhen Li, Wenqi Tang, and Wenli Zhou.
\newblock Llvip: A visible-infrared paired dataset for low-light vision.
\newblock In \emph{Proceedings of the IEEE/CVF international conference on
  computer vision}, pages 3496--3504, 2021{\natexlab{b}}.

\bibitem[Jiang et~al.(2022)Jiang, Gupta, Zhang, Wang, Dou, Chen, Fei-Fei,
  Anandkumar, Zhu, and Fan]{jiang2022vima}
Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang Dou, Yanjun
  Chen, Li~Fei-Fei, Anima Anandkumar, Yuke Zhu, and Linxi Fan.
\newblock Vima: General robot manipulation with multimodal prompts.
\newblock \emph{arXiv preprint arXiv: 2210.03094}, 2022.

\bibitem[Jingyi et~al.(2023)Jingyi, Jiaxing, Sheng, and Shijian]{Zhang2023VLMs}
Zhang Jingyi, Huang Jiaxing, Jin Sheng, and Lu~Shijian.
\newblock Vision-language models for vision tasks: A survey.
\newblock \emph{arXiv preprint arXiv:2304.00685}, 2023.

\bibitem[Jocher et~al.(2023)Jocher, Chaurasia, and Qiu]{Jocher2023yolo}
Glenn Jocher, Ayush Chaurasia, and Jing Qiu.
\newblock Yolo by ultralytics.
\newblock \url{https://github.com/ultralytics/ultralytics}, 2023.

\bibitem[Kamath et~al.(2021)Kamath, Singh, LeCun, Synnaeve, Misra, and
  Carion]{kamath2021mdetr}
Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel Synnaeve, Ishan Misra, and
  Nicolas Carion.
\newblock Mdetr - modulated detection for end-to-end multi-modal understanding.
\newblock \emph{arXiv preprint arXiv: 2104.12763}, 2021.

\bibitem[Kazemzadeh et~al.(2014)Kazemzadeh, Ordonez, Matten, and
  Berg]{kazemzadeh2014referitgame}
Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg.
\newblock Referitgame: Referring to objects in photographs of natural scenes.
\newblock In \emph{Proceedings of the 2014 conference on empirical methods in
  natural language processing (EMNLP)}, pages 787--798, 2014.

\bibitem[Khan et~al.(2022)Khan, Naseer, Hayat, Zamir, Khan, and
  Shah]{khan2022transformers}
Salman Khan, Muzammal Naseer, Munawar Hayat, Syed~Waqas Zamir, Fahad~Shahbaz
  Khan, and Mubarak Shah.
\newblock Transformers in vision: A survey.
\newblock \emph{ACM computing surveys (CSUR)}, 54\penalty0 (10s):\penalty0
  1--41, 2022.

\bibitem[Khoreva et~al.(2019)Khoreva, Rohrbach, and Schiele]{khoreva2019video}
Anna Khoreva, Anna Rohrbach, and Bernt Schiele.
\newblock Video object segmentation with language referring expressions.
\newblock In \emph{Computer Vision--ACCV 2018: 14th Asian Conference on
  Computer Vision, Perth, Australia, December 2--6, 2018, Revised Selected
  Papers, Part IV 14}, pages 123--141. Springer, 2019.

\bibitem[Kiela et~al.(2020)Kiela, Firooz, Mohan, Goswami, Singh, Ringshia, and
  Testuggine]{kiela2020hateful}
Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Amanpreet Singh,
  Pratik Ringshia, and Davide Testuggine.
\newblock The hateful memes challenge: Detecting hate speech in multimodal
  memes.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 2611--2624, 2020.

\bibitem[Kim et~al.(2023)Kim, Lee, Kim, Jung, Park, Kim, Yun, Kil, Lee, and
  Park]{kim2023cream}
Geewook Kim, Hodong Lee, Daehee Kim, Haeji Jung, Sanghee Park, Yoonsik Kim,
  Sangdoo Yun, Taeho Kil, Bado Lee, and Seunghyun Park.
\newblock Cream: Visually-situated natural language understanding with
  contrastive reading model and frozen large language models.
\newblock \emph{arXiv preprint arXiv:2305.15080}, 2023.

\bibitem[Kirillov et~al.(2023)Kirillov, Mintun, Ravi, Mao, Rolland, Gustafson,
  Xiao, Whitehead, Berg, Lo, et~al.]{kirillov2023segment}
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura
  Gustafson, Tete Xiao, Spencer Whitehead, Alexander~C Berg, Wan-Yen Lo, et~al.
\newblock Segment anything.
\newblock \emph{arXiv preprint arXiv:2304.02643}, 2023.

\bibitem[Koh et~al.(2023)Koh, Salakhutdinov, and Fried]{koh2023grounding}
Jing~Yu Koh, Ruslan Salakhutdinov, and Daniel Fried.
\newblock Grounding language models to images for multimodal inputs and
  outputs.
\newblock 2023.

\bibitem[Krasin et~al.(2017)Krasin, Duerig, Alldrin, Ferrari, Abu-El-Haija,
  Kuznetsova, Rom, Uijlings, Popov, Veit, et~al.]{krasin2017openimages}
Ivan Krasin, Tom Duerig, Neil Alldrin, Vittorio Ferrari, Sami Abu-El-Haija,
  Alina Kuznetsova, Hassan Rom, Jasper Uijlings, Stefan Popov, Andreas Veit,
  et~al.
\newblock Openimages: A public dataset for large-scale multi-label and
  multi-class image classification.
\newblock \emph{Dataset available from https://github. com/openimages},
  2\penalty0 (3):\penalty0 18, 2017.

\bibitem[Krishna et~al.(2017)Krishna, Zhu, Groth, Johnson, Hata, Kravitz, Chen,
  Kalantidis, Li, Shamma, et~al.]{krishna2017visual}
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua
  Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David~A Shamma, et~al.
\newblock Visual genome: Connecting language and vision using crowdsourced
  dense image annotations.
\newblock \emph{International journal of computer vision}, 123:\penalty0
  32--73, 2017.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{krizhevsky2012imagenet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock \emph{Advances in neural information processing systems}, 25, 2012.

\bibitem[Kumar et~al.(2019)Kumar, Verma, Anand, Zhou, Onder, Tsougenis, Chen,
  Heng, Li, Hu, et~al.]{kumar2019multi}
Neeraj Kumar, Ruchika Verma, Deepak Anand, Yanning Zhou, Omer~Fahri Onder,
  Efstratios Tsougenis, Hao Chen, Pheng-Ann Heng, Jiahui Li, Zhiqiang Hu,
  et~al.
\newblock A multi-organ nucleus segmentation challenge.
\newblock \emph{IEEE transactions on medical imaging}, 39\penalty0
  (5):\penalty0 1380--1391, 2019.

\bibitem[Kwon et~al.(2022)Kwon, Cai, Ravichandran, Bas, Bhotika, and
  Soatto]{kwon2022masked}
Gukyeong Kwon, Zhaowei Cai, Avinash Ravichandran, Erhan Bas, Rahul Bhotika, and
  Stefano Soatto.
\newblock Masked vision and language modeling for multi-modal representation
  learning.
\newblock \emph{arXiv preprint arXiv:2208.02131}, 2022.

\bibitem[Kwon et~al.(2023)Kwon, Cai, Ravichandran, Bas, Bhotika, and
  Soatto]{kuwon2022masked}
Gukyeong Kwon, Zhaowei Cai, Avinash Ravichandran, Erhan Bas, Rahul Bhotika, and
  Stefano Soatto.
\newblock Masked vision and language modeling for multi-modal representation
  learning.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations, {ICLR} 2023, Kigali, Rwanda, May 1-5, 2023}. OpenReview.net,
  2023.
\newblock URL \url{https://openreview.net/pdf?id=ZhuXksSJYWn}.

\bibitem[Lei et~al.(2021)Lei, Xu, Gu, Fu, Zhang, Zhang, and
  Wang]{lei2021contrastive}
Wenhui Lei, Wei Xu, Ran Gu, Hao Fu, Shaoting Zhang, Shichuan Zhang, and Guotai
  Wang.
\newblock Contrastive learning of relative position regression for one-shot
  object localization in 3d medical images.
\newblock In \emph{Medical Image Computing and Computer Assisted
  Intervention--MICCAI 2021: 24th International Conference, Strasbourg, France,
  September 27--October 1, 2021, Proceedings, Part II 24}, pages 155--165.
  Springer, 2021.

\bibitem[Lester et~al.(2021)Lester, Al-Rfou, and Constant]{lester2021power}
Brian Lester, Rami Al-Rfou, and Noah Constant.
\newblock The power of scale for parameter-efficient prompt tuning.
\newblock \emph{arXiv preprint arXiv:2104.08691}, 2021.

\bibitem[Lewis et~al.(2019)Lewis, Liu, Goyal, Ghazvininejad, Mohamed, Levy,
  Stoyanov, and Zettlemoyer]{lewis2019bart}
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,
  Omer Levy, Ves Stoyanov, and Luke Zettlemoyer.
\newblock Bart: Denoising sequence-to-sequence pre-training for natural
  language generation, translation, and comprehension.
\newblock \emph{arXiv preprint arXiv:1910.13461}, 2019.

\bibitem[Li et~al.(2023{\natexlab{a}})Li, Fang, Yang, Wang, Ye, Zhao, and
  Zhang]{li2023evaluating}
Bo~Li, Gexiang Fang, Yang Yang, Quansen Wang, Wei Ye, Wen Zhao, and Shikun
  Zhang.
\newblock Evaluating chatgpt's information extraction capabilities: An
  assessment of performance, explainability, calibration, and faithfulness.
\newblock \emph{arXiv preprint arXiv:2304.11633}, 2023{\natexlab{a}}.

\bibitem[Li et~al.(2023{\natexlab{b}})Li, Zhang, Chen, Wang, Pu, Yang, Li, and
  Liu]{li2023mimic}
Bo~Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu, Jingkang Yang,
  Chunyuan Li, and Ziwei Liu.
\newblock Mimic-it: Multi-modal in-context instruction tuning.
\newblock \emph{arXiv preprint arXiv:2306.05425}, 2023{\natexlab{b}}.

\bibitem[Li et~al.(2021)Li, Selvaraju, Gotmare, Joty, Xiong, and
  Hoi]{li2021align}
Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong,
  and Steven Chu~Hong Hoi.
\newblock Align before fuse: Vision and language representation learning with
  momentum distillation.
\newblock \emph{Advances in neural information processing systems},
  34:\penalty0 9694--9705, 2021.

\bibitem[Li et~al.(2022{\natexlab{a}})Li, Li, Xiong, and Hoi]{li2022blip}
Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
\newblock Blip: Bootstrapping language-image pre-training for unified
  vision-language understanding and generation.
\newblock In \emph{International Conference on Machine Learning}, pages
  12888--12900. PMLR, 2022{\natexlab{a}}.

\bibitem[Li et~al.(2023{\natexlab{c}})Li, Li, Savarese, and Hoi]{li2023blip}
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
\newblock Blip-2: Bootstrapping language-image pre-training with frozen image
  encoders and large language models.
\newblock \emph{arXiv preprint arXiv:2301.12597}, 2023{\natexlab{c}}.

\bibitem[Li et~al.(2022{\natexlab{b}})Li, Zhang, Zhang, Yang, Li, Zhong, Wang,
  Yuan, Zhang, Hwang, et~al.]{li2022grounded}
Liunian~Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li,
  Yiwu Zhong, Lijuan Wang, Lu~Yuan, Lei Zhang, Jenq-Neng Hwang, et~al.
\newblock Grounded language-image pre-training.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 10965--10975, 2022{\natexlab{b}}.

\bibitem[Li and Liang(2021)]{li2021prefix}
Xiang~Lisa Li and Percy Liang.
\newblock Prefix-tuning: Optimizing continuous prompts for generation.
\newblock \emph{arXiv preprint arXiv:2101.00190}, 2021.

\bibitem[Li et~al.(2023{\natexlab{d}})Li, Wang, and Xie]{li2023clipav2}
Xianhang Li, Zeyu Wang, and Cihang Xie.
\newblock Clipa-v2: Scaling clip training with 81.1% zero-shot imagenet
  accuracy within a \$10,000 budget; an extra \$4,000 unlocks 81.8% accuracy.
\newblock \emph{arXiv preprint arXiv: 2306.15658}, 2023{\natexlab{d}}.

\bibitem[Li et~al.(2023{\natexlab{e}})Li, Wang, and Xie]{li2023inverse}
Xianhang Li, Zeyu Wang, and Cihang Xie.
\newblock An inverse scaling law for clip training.
\newblock \emph{arXiv preprint arXiv: 2305.07017}, 2023{\natexlab{e}}.

\bibitem[Li et~al.(2023{\natexlab{f}})Li, Fan, Hu, Feichtenhofer, and
  He]{li2023scaling}
Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichtenhofer, and Kaiming He.
\newblock Scaling language-image pre-training via masking.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 23390--23400, 2023{\natexlab{f}}.

\bibitem[Li et~al.(2023{\natexlab{g}})Li, Du, Zhou, Wang, Zhao, and
  Wen]{li2023evaluatingb}
Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne~Xin Zhao, and Ji-Rong Wen.
\newblock Evaluating object hallucination in large vision-language models.
\newblock \emph{arXiv preprint arXiv:2305.10355}, 2023{\natexlab{g}}.

\bibitem[Liangliang et~al.(2023)Liangliang, Haobo, Guangze, Changhong, and
  Jia]{sam_da}
Yao Liangliang, Zuo Haobo, Zheng Guangze, Fu~Changhong, and Pan Jia.
\newblock Sam-da: Uav tracks anything at night with sam-powered domain
  adaptation.
\newblock \emph{arXiv:2307.01024}, 2023.

\bibitem[Lin et~al.(2014)Lin, Maire, Belongie, Hays, Perona, Ramanan,
  Doll{\'a}r, and Zitnick]{lin2014microsoft}
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
  Ramanan, Piotr Doll{\'a}r, and C~Lawrence Zitnick.
\newblock Microsoft coco: Common objects in context.
\newblock In \emph{Computer Vision--ECCV 2014: 13th European Conference,
  Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13}, pages
  740--755. Springer, 2014.

\bibitem[Lin et~al.(2017{\natexlab{a}})Lin, Doll{\'a}r, Girshick, He,
  Hariharan, and Belongie]{lin2017feature}
Tsung-Yi Lin, Piotr Doll{\'a}r, Ross Girshick, Kaiming He, Bharath Hariharan,
  and Serge Belongie.
\newblock Feature pyramid networks for object detection.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 2117--2125, 2017{\natexlab{a}}.

\bibitem[Lin et~al.(2017{\natexlab{b}})Lin, Goyal, Girshick, He, and
  Doll{\'a}r]{lin2017focal}
Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll{\'a}r.
\newblock Focal loss for dense object detection.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pages 2980--2988, 2017{\natexlab{b}}.

\bibitem[Lin et~al.(2023)Lin, Karlinsky, Shvetsova, Possegger, Kozinski, Panda,
  Feris, Kuehne, and Bischof]{lin2023match}
Wei Lin, Leonid Karlinsky, Nina Shvetsova, Horst Possegger, Mateusz Kozinski,
  Rameswar Panda, Rogerio Feris, Hilde Kuehne, and Horst Bischof.
\newblock Match, expand and improve: Unsupervised finetuning for zero-shot
  action recognition with language knowledge.
\newblock \emph{arXiv preprint arXiv:2303.08914}, 2023.

\bibitem[Lin and Chen(2023)]{lin2023llm}
Yen-Ting Lin and Yun-Nung Chen.
\newblock Llm-eval: Unified multi-dimensional automatic evaluation for
  open-domain conversations with large language models.
\newblock \emph{arXiv preprint arXiv:2305.13711}, 2023.

\bibitem[Liu et~al.(2023{\natexlab{a}})Liu, Chen, Guan, Zhou, Zhu, and
  Zhou]{liu2023remoteclip}
Fan Liu, Delong Chen, Zhangqingyun Guan, Xiaocong Zhou, Jiale Zhu, and Jun
  Zhou.
\newblock Remoteclip: A vision language foundation model for remote sensing.
\newblock \emph{arXiv preprint arXiv: 2306.11029}, 2023{\natexlab{a}}.

\bibitem[Liu et~al.(2023{\natexlab{b}})Liu, Li, Wu, and Lee]{liu2023llava}
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong~Jae Lee.
\newblock Visual instruction tuning.
\newblock 2023{\natexlab{b}}.

\bibitem[Liu et~al.(2023{\natexlab{c}})Liu, Li, Wu, and Lee]{liu2023visual}
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong~Jae Lee.
\newblock Visual instruction tuning.
\newblock \emph{arXiv preprint arXiv:2304.08485}, 2023{\natexlab{c}}.

\bibitem[Liu et~al.(2023{\natexlab{d}})Liu, Jin, Wang, Cheng, Dou, and
  Wen]{liu2023reta}
Jiongnan Liu, Jiajie Jin, Zihan Wang, Jiehan Cheng, Zhicheng Dou, and Ji-Rong
  Wen.
\newblock Reta-llm: A retrieval-augmented large language model toolkit.
\newblock \emph{arXiv preprint arXiv:2306.05212}, 2023{\natexlab{d}}.

\bibitem[Liu et~al.(2023{\natexlab{e}})Liu, Li, Calinon, and
  Chen]{liu2023softgpt}
Junjia Liu, Zhihao Li, Sylvain Calinon, and Fei Chen.
\newblock Softgpt: Learn goal-oriented soft object manipulation skills by
  generative pre-trained heterogeneous graph transformer.
\newblock \emph{arXiv preprint arXiv:2306.12677}, 2023{\natexlab{e}}.

\bibitem[Liu et~al.(2023{\natexlab{f}})Liu, Fan, Johns, Yu, Xiao, and
  Anandkumar]{liu2023prismer}
Shikun Liu, Linxi Fan, Edward Johns, Zhiding Yu, Chaowei Xiao, and Anima
  Anandkumar.
\newblock Prismer: A vision-language model with an ensemble of experts.
\newblock \emph{arXiv preprint arXiv: 2303.02506}, 2023{\natexlab{f}}.

\bibitem[Liu et~al.()Liu, Zeng, Ren, Li, Zhang, Yang, Li, Yang, Su, Zhu,
  et~al.]{liu2023grounding}
Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan
  Li, Jianwei Yang, Hang Su, Jun Zhu, et~al.
\newblock Grounding dino: Marrying dino with grounded pre-training for open-set
  object detection.
\newblock \emph{arXiv preprint arXiv:2303.05499}.

\bibitem[Liu and Zuo(2023)]{liu2023stone}
Weihua Liu and Yong Zuo.
\newblock Stone needle: A general multimodal large-scale model framework
  towards healthcare.
\newblock \emph{arXiv preprint arXiv: 2306.16034}, 2023.

\bibitem[Liu et~al.(2021{\natexlab{a}})Liu, Ji, Fu, Tam, Du, Yang, and
  Tang]{liu2021p}
Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng~Lam Tam, Zhengxiao Du, Zhilin Yang, and
  Jie Tang.
\newblock P-tuning v2: Prompt tuning can be comparable to fine-tuning
  universally across scales and tasks.
\newblock \emph{arXiv preprint arXiv:2110.07602}, 2021{\natexlab{a}}.

\bibitem[Liu et~al.(2023{\natexlab{g}})Liu, Zhang, She, Kheradmand, and
  Armand]{liu2023samm}
Yihao Liu, Jiaming Zhang, Zhangcong She, Amir Kheradmand, and Mehran Armand.
\newblock Samm (segment any medical model): A 3d slicer integration to sam.
\newblock \emph{arXiv preprint arXiv:2304.05622}, 2023{\natexlab{g}}.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov]{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Roberta: A robustly optimized bert pretraining approach, 2019.

\bibitem[Liu et~al.(2021{\natexlab{b}})Liu, Lin, Cao, Hu, Wei, Zhang, Lin, and
  Guo]{liu2021swin}
Ze~Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and
  Baining Guo.
\newblock Swin transformer: Hierarchical vision transformer using shifted
  windows.
\newblock In \emph{Proceedings of the IEEE/CVF international conference on
  computer vision}, pages 10012--10022, 2021{\natexlab{b}}.

\bibitem[Long et~al.(2022)Long, Cao, Han, and Yang]{Long2022VLM}
Siqu Long, Feiqi Cao, Soyeon~Caren Han, and Haiqin Yang.
\newblock Vision-and-language pretrained models: A survey.
\newblock In \emph{IJCAI}, 2022.

\bibitem[Lu et~al.(2019)Lu, Batra, Parikh, and Lee]{lu2019vilbert}
Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee.
\newblock Vilbert: Pretraining task-agnostic visiolinguistic representations
  for vision-and-language tasks.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Lu et~al.(2021)Lu, Qiu, Chen, Xia, Zhao, Zhang, Yu, Liang, and
  Zhu]{lu2021iconqa}
Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu,
  Xiaodan Liang, and Song-Chun Zhu.
\newblock Iconqa: A new benchmark for abstract diagram understanding and visual
  language reasoning.
\newblock \emph{arXiv preprint arXiv:2110.13214}, 2021.

\bibitem[Lu et~al.(2022)Lu, Mishra, Xia, Qiu, Chang, Zhu, Tafjord, Clark, and
  Kalyan]{lu2022learn}
Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu,
  Oyvind Tafjord, Peter Clark, and Ashwin Kalyan.
\newblock Learn to explain: Multimodal reasoning via thought chains for science
  question answering.
\newblock In \emph{The 36th Conference on Neural Information Processing Systems
  (NeurIPS)}, 2022.

\bibitem[Lu et~al.(2023)Lu, Peng, Cheng, Galley, Chang, Wu, Zhu, and
  Gao]{lu2023chameleon}
Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying~Nian Wu,
  Song-Chun Zhu, and Jianfeng Gao.
\newblock Chameleon: Plug-and-play compositional reasoning with large language
  models.
\newblock \emph{arXiv preprint arXiv:2304.09842}, 2023.

\bibitem[L{\"u}ddecke and Ecker(2022)]{luddecke2022image}
Timo L{\"u}ddecke and Alexander Ecker.
\newblock Image segmentation using text and image prompts.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 7086--7096, 2022.

\bibitem[Luo et~al.(2023)Luo, Zhao, Yang, Dong, Qiu, Lu, Wang, and
  Wei]{luo2023valley}
Ruipu Luo, Ziwang Zhao, Min Yang, Junwei Dong, Minghui Qiu, Pengcheng Lu, Tao
  Wang, and Zhongyu Wei.
\newblock Valley: Video assistant with large language model enhanced ability.
\newblock \emph{arXiv preprint arXiv: 2306.07207}, 2023.

\bibitem[Lynch et~al.(2022)Lynch, Wahid, Tompson, Ding, Betker, Baruch,
  Armstrong, and Florence]{lynch2022interactive}
Corey Lynch, Ayzaan Wahid, Jonathan Tompson, Tianli Ding, James Betker, Robert
  Baruch, Travis Armstrong, and Pete Florence.
\newblock Interactive language: Talking to robots in real time.
\newblock \emph{arXiv preprint arXiv:2210.06407}, 2022.

\bibitem[Lyu et~al.(2023)Lyu, Liu, Wu, Du, Huang, Tu, Shi, and Wang]{Macaw-LLM}
Chenyang Lyu, Bingshuai Liu, Minghao Wu, Zefeng Du, Xinting Huang, Zhaopeng Tu,
  Shuming Shi, and Longyue Wang.
\newblock Macaw-llm: Multi-modal language modeling with image, video, audio,
  and text integration.
\newblock \url{https://github.com/lyuchenyang/Macaw-LLM}, 2023.

\bibitem[Ma and Wang(2023)]{ma2023segment}
Jun Ma and Bo~Wang.
\newblock Segment anything in medical images.
\newblock \emph{arXiv preprint arXiv: 2304.12306}, 2023.

\bibitem[Maaz et~al.(2023)Maaz, Rasheed, Khan, and Khan]{maaz2023videochatgpt}
Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad~Shahbaz Khan.
\newblock Video-chatgpt: Towards detailed video understanding via large vision
  and language models.
\newblock \emph{arXiv preprint arXiv: 2306.05424}, 2023.

\bibitem[Mahajan et~al.(2018)Mahajan, Girshick, Ramanathan, He, Paluri, Li,
  Bharambe, and Van Der~Maaten]{mahajan2018exploring}
Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri,
  Yixuan Li, Ashwin Bharambe, and Laurens Van Der~Maaten.
\newblock Exploring the limits of weakly supervised pretraining.
\newblock In \emph{Proceedings of the European conference on computer vision
  (ECCV)}, pages 181--196, 2018.

\bibitem[Mai et~al.(2023)Mai, Chen, Li, Qian, Elhoseiny, and
  Ghanem]{mai2023llm}
Jinjie Mai, Jun Chen, Bing Li, Guocheng Qian, Mohamed Elhoseiny, and Bernard
  Ghanem.
\newblock Llm as a robotic brain: Unifying egocentric memory and control.
\newblock \emph{arXiv preprint arXiv:2304.09349}, 2023.

\bibitem[Mao et~al.(2016)Mao, Huang, Toshev, Camburu, Yuille, and
  Murphy]{mao2016generation}
Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan~L Yuille, and
  Kevin Murphy.
\newblock Generation and comprehension of unambiguous object descriptions.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 11--20, 2016.

\bibitem[Marino et~al.(2019)Marino, Rastegari, Farhadi, and
  Mottaghi]{marino2019ok}
Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi.
\newblock Ok-vqa: A visual question answering benchmark requiring external
  knowledge.
\newblock In \emph{Proceedings of the IEEE/cvf conference on computer vision
  and pattern recognition}, pages 3195--3204, 2019.

\bibitem[Maus et~al.(2023)Maus, Chao, Wong, and Gardner]{maus2023adversarial}
Natalie Maus, Patrick Chao, Eric Wong, and Jacob Gardner.
\newblock Adversarial prompting for black box foundation models.
\newblock \emph{arXiv preprint arXiv:2302.04237}, 2023.

\bibitem[Minderer et~al.(2022)Minderer, Gritsenko, Stone, Neumann, Weissenborn,
  Dosovitskiy, Mahendran, Arnab, Dehghani, Shen, Wang, Zhai, Kipf, and
  Houlsby]{minderer2022simple}
Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk
  Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa
  Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, and Neil
  Houlsby.
\newblock Simple open-vocabulary object detection with vision transformers.
\newblock \emph{arXiv preprint arXiv: 2205.06230}, 2022.

\bibitem[Mishra et~al.(2019)Mishra, Shekhar, Singh, and
  Chakraborty]{mishra2019ocr}
Anand Mishra, Shashank Shekhar, Ajeet~Kumar Singh, and Anirban Chakraborty.
\newblock Ocr-vqa: Visual question answering by reading text in images.
\newblock In \emph{2019 international conference on document analysis and
  recognition (ICDAR)}, pages 947--952. IEEE, 2019.

\bibitem[Moor et~al.(2023)Moor, Banerjee, Abad, Krumholz, Leskovec, Topol, and
  Rajpurkar]{moor2023foundation}
Michael Moor, Oishi Banerjee, Zahra Shakeri~Hossein Abad, Harlan~M Krumholz,
  Jure Leskovec, Eric~J Topol, and Pranav Rajpurkar.
\newblock Foundation models for generalist medical artificial intelligence.
\newblock \emph{Nature}, 616\penalty0 (7956):\penalty0 259--265, 2023.

\bibitem[Mottaghi et~al.(2014)Mottaghi, Chen, Liu, Cho, Lee, Fidler, Urtasun,
  and Yuille]{mottaghi2014role}
Roozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu Cho, Seong-Whan Lee, Sanja
  Fidler, Raquel Urtasun, and Alan Yuille.
\newblock The role of context for object detection and semantic segmentation in
  the wild.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 891--898, 2014.

\bibitem[Mu et~al.(2022)Mu, Kirillov, Wagner, and Xie]{mu2022slip}
Norman Mu, Alexander Kirillov, David Wagner, and Saining Xie.
\newblock Slip: Self-supervision meets language-image pre-training.
\newblock In \emph{Computer Vision--ECCV 2022: 17th European Conference, Tel
  Aviv, Israel, October 23--27, 2022, Proceedings, Part XXVI}, pages 529--544.
  Springer, 2022.

\bibitem[Mu et~al.(2023)Mu, Zhang, Hu, Wang, Ding, Jin, Wang, Dai, Qiao, and
  Luo]{mu2023embodiedgpt}
Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun Jin, Bin
  Wang, Jifeng Dai, Yu~Qiao, and Ping Luo.
\newblock Embodiedgpt: Vision-language pre-training via embodied chain of
  thought.
\newblock \emph{arXiv preprint arXiv:2305.15021}, 2023.

\bibitem[M{\"u}ndler et~al.(2023)M{\"u}ndler, He, Jenko, and
  Vechev]{mundler2023self}
Niels M{\"u}ndler, Jingxuan He, Slobodan Jenko, and Martin Vechev.
\newblock Self-contradictory hallucinations of large language models:
  Evaluation, detection and mitigation.
\newblock \emph{arXiv preprint arXiv:2305.15852}, 2023.

\bibitem[Nagaraja et~al.(2016)Nagaraja, Morariu, and
  Davis]{nagaraja2016modeling}
Varun~K Nagaraja, Vlad~I Morariu, and Larry~S Davis.
\newblock Modeling context between objects for referring expression
  understanding.
\newblock In \emph{Computer Vision--ECCV 2016: 14th European Conference,
  Amsterdam, The Netherlands, October 11--14, 2016, Proceedings, Part IV 14},
  pages 792--807. Springer, 2016.

\bibitem[OpenAI(2023)]{openai2023gpt4}
OpenAI.
\newblock Gpt-4 technical report.
\newblock \emph{PREPRINT}, 2023.

\bibitem[Ordonez et~al.(2011)Ordonez, Kulkarni, and Berg]{ordonez2011im2text}
Vicente Ordonez, Girish Kulkarni, and Tamara Berg.
\newblock Im2text: Describing images using 1 million captioned photographs.
\newblock \emph{Advances in neural information processing systems}, 24, 2011.

\bibitem[Pan et~al.(2023)Pan, Lin, Ge, Zhu, Zhang, Wang, Qiao, and
  Li]{pan2023retrieving}
Junting Pan, Ziyi Lin, Yuying Ge, Xiatian Zhu, Renrui Zhang, Yi~Wang, Yu~Qiao,
  and Hongsheng Li.
\newblock Retrieving-to-answer: Zero-shot video question answering with frozen
  large language models.
\newblock \emph{arXiv preprint arXiv:2306.11732}, 2023.

\bibitem[Parisi et~al.(2019)Parisi, Kemker, Part, Kanan, and
  Wermter]{parisi2019continual}
German~I Parisi, Ronald Kemker, Jose~L Part, Christopher Kanan, and Stefan
  Wermter.
\newblock Continual lifelong learning with neural networks: A review.
\newblock \emph{Neural networks}, 113:\penalty0 54--71, 2019.

\bibitem[Park et~al.(2023)Park, Lim, Lee, Park, Yu, and Choi]{park2023clara}
Jeongeun Park, Seungwon Lim, Joonhyung Lee, Sangbeom Park, Youngjae Yu, and
  Sungjoon Choi.
\newblock Clara: Classifying and disambiguating user commands for reliable
  interactive robotic agents.
\newblock \emph{arXiv preprint arXiv:2306.10376}, 2023.

\bibitem[Peng et~al.(2022)Peng, Dong, Bao, Ye, and Wei]{peng2022beit}
Zhiliang Peng, Li~Dong, Hangbo Bao, Qixiang Ye, and Furu Wei.
\newblock Beit v2: Masked image modeling with vector-quantized visual
  tokenizers.
\newblock \emph{arXiv preprint arXiv:2208.06366}, 2022.

\bibitem[Peng et~al.(2023)Peng, Wang, Dong, Hao, Huang, Ma, and
  Wei]{peng2023kosmos2}
Zhiliang Peng, Wenhui Wang, Li~Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and
  Furu Wei.
\newblock Kosmos-2: Grounding multimodal large language models to the world.
\newblock \emph{arXiv preprint arXiv: 2306.14824}, 2023.

\bibitem[Perez and Ribeiro(2022)]{perez2022ignore}
F{\'a}bio Perez and Ian Ribeiro.
\newblock Ignore previous prompt: Attack techniques for language models.
\newblock \emph{arXiv preprint arXiv:2211.09527}, 2022.

\bibitem[Pont-Tuset et~al.(2020)Pont-Tuset, Uijlings, Changpinyo, Soricut, and
  Ferrari]{pont2020connecting}
Jordi Pont-Tuset, Jasper Uijlings, Soravit Changpinyo, Radu Soricut, and
  Vittorio Ferrari.
\newblock Connecting vision and language with localized narratives.
\newblock In \emph{Computer Vision--ECCV 2020: 16th European Conference,
  Glasgow, UK, August 23--28, 2020, Proceedings, Part V 16}, pages 647--664.
  Springer, 2020.

\bibitem[Qiu et~al.(2023)Qiu, Hu, Li, and Liu]{qiu2023learnable}
Zhongxi Qiu, Yan Hu, Heng Li, and Jiang Liu.
\newblock Learnable ophthalmology sam.
\newblock \emph{arXiv preprint arXiv:2304.13425}, 2023.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever,
  et~al.]{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
  Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In \emph{International conference on machine learning}, pages
  8748--8763. PMLR, 2021.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{The Journal of Machine Learning Research}, 21\penalty0
  (1):\penalty0 5485--5551, 2020.

\bibitem[Rajič et~al.(2023)Rajič, Ke, Tai, Tang, Danelljan, and Yu]{sam-pt}
Frano Rajič, Lei Ke, Yu-Wing Tai, Chi-Keung Tang, Martin Danelljan, and Fisher
  Yu.
\newblock Segment anything meets point tracking.
\newblock \emph{arXiv:2307.01197}, 2023.

\bibitem[Ranjit et~al.(2023)Ranjit, Ganapathy, Manuel, and
  Ganu]{ranjit2023retrieval}
Mercy Ranjit, Gopinath Ganapathy, Ranjit Manuel, and Tanuja Ganu.
\newblock Retrieval augmented chest x-ray report generation using openai gpt
  models.
\newblock \emph{arXiv preprint arXiv:2305.03660}, 2023.

\bibitem[Raven(2003)]{raven2003raven}
Jean Raven.
\newblock Raven progressive matrices.
\newblock In \emph{Handbook of nonverbal assessment}, pages 223--237. Springer,
  2003.

\bibitem[Recht et~al.(2019)Recht, Roelofs, Schmidt, and
  Shankar]{recht2019imagenet}
Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar.
\newblock Do imagenet classifiers generalize to imagenet?
\newblock In \emph{International conference on machine learning}, pages
  5389--5400. PMLR, 2019.

\bibitem[Ren et~al.(2023)Ren, Dixit, Bodrova, Singh, Tu, Brown, Xu, Takayama,
  Xia, Varley, et~al.]{ren2023robots}
Allen~Z Ren, Anushri Dixit, Alexandra Bodrova, Sumeet Singh, Stephen Tu, Noah
  Brown, Peng Xu, Leila Takayama, Fei Xia, Jake Varley, et~al.
\newblock Robots that ask for help: Uncertainty alignment for large language
  model planners.
\newblock \emph{arXiv preprint arXiv:2307.01928}, 2023.

\bibitem[Ren et~al.(2015)Ren, He, Girshick, and Sun]{ren2015faster}
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
\newblock Faster r-cnn: Towards real-time object detection with region proposal
  networks.
\newblock \emph{Advances in neural information processing systems}, 28, 2015.

\bibitem[Rezatofighi et~al.(2019)Rezatofighi, Tsoi, Gwak, Sadeghian, Reid, and
  Savarese]{rezatofighi2019generalized}
Hamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, Amir Sadeghian, Ian Reid, and
  Silvio Savarese.
\newblock Generalized intersection over union: A metric and a loss for bounding
  box regression.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision
  and pattern recognition}, pages 658--666, 2019.

\bibitem[Ronneberger et~al.(2015)Ronneberger, Fischer, and
  Brox]{ronneberger2015u}
Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
\newblock U-net: Convolutional networks for biomedical image segmentation.
\newblock In \emph{Medical Image Computing and Computer-Assisted
  Intervention--MICCAI 2015: 18th International Conference, Munich, Germany,
  October 5-9, 2015, Proceedings, Part III 18}, pages 234--241. Springer, 2015.

\bibitem[Schaeffer et~al.(2023)Schaeffer, Miranda, and
  Koyejo]{schaeffer2023emergent}
Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo.
\newblock Are emergent abilities of large language models a mirage?
\newblock \emph{arXiv preprint arXiv:2304.15004}, 2023.

\bibitem[Schuhmann et~al.(2021)Schuhmann, Vencu, Beaumont, Kaczmarczyk, Mullis,
  Katta, Coombes, Jitsev, and Komatsuzaki]{schuhmann2021laion}
Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk,
  Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran
  Komatsuzaki.
\newblock Laion-400m: Open dataset of clip-filtered 400 million image-text
  pairs.
\newblock \emph{arXiv preprint arXiv:2111.02114}, 2021.

\bibitem[Schuhmann et~al.(2022)Schuhmann, Beaumont, Vencu, Gordon, Wightman,
  Cherti, Coombes, Katta, Mullis, Wortsman, Schramowski, Kundurthy, Crowson,
  Schmidt, Kaczmarczyk, and Jitsev]{schuhmann2022laion}
Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross
  Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell
  Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig
  Schmidt, Robert Kaczmarczyk, and Jenia Jitsev.
\newblock Laion-5b: An open large-scale dataset for training next generation
  image-text models, 2022.

\bibitem[Schwenk et~al.(2022)Schwenk, Khandelwal, Clark, Marino, and
  Mottaghi]{schwenk2022okvqa}
Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and
  Roozbeh Mottaghi.
\newblock A-okvqa: A benchmark for visual question answering using world
  knowledge.
\newblock In \emph{European Conference on Computer Vision}, pages 146--162.
  Springer, 2022.

\bibitem[Seo et~al.(2020)Seo, Lee, and Han]{seo2020urvos}
Seonguk Seo, Joon-Young Lee, and Bohyung Han.
\newblock Urvos: Unified referring video object segmentation network with a
  large-scale benchmark.
\newblock In \emph{Computer Vision--ECCV 2020: 16th European Conference,
  Glasgow, UK, August 23--28, 2020, Proceedings, Part XV 16}, pages 208--223.
  Springer, 2020.

\bibitem[Shah et~al.(2021{\natexlab{a}})Shah, Eysenbach, Kahn, Rhinehart, and
  Levine]{shah2021ving}
Dhruv Shah, Benjamin Eysenbach, Gregory Kahn, Nicholas Rhinehart, and Sergey
  Levine.
\newblock Ving: Learning open-world navigation with visual goals.
\newblock In \emph{2021 IEEE International Conference on Robotics and
  Automation (ICRA)}, pages 13215--13222. IEEE, 2021{\natexlab{a}}.

\bibitem[Shah et~al.(2022)Shah, Osinski, Ichter, and Levine]{shah2022lmnav}
Dhruv Shah, Blazej Osinski, Brian Ichter, and Sergey Levine.
\newblock Lm-nav: Robotic navigation with large pre-trained models of language,
  vision, and action.
\newblock \emph{arXiv preprint arXiv: 2207.04429}, 2022.

\bibitem[Shah et~al.(2021{\natexlab{b}})Shah, Wild, Wang, Alex, Houghton, Guss,
  Mohanty, Kanervisto, Milani, Topin, et~al.]{shah2021minerl}
Rohin Shah, Cody Wild, Steven~H Wang, Neel Alex, Brandon Houghton, William
  Guss, Sharada Mohanty, Anssi Kanervisto, Stephanie Milani, Nicholay Topin,
  et~al.
\newblock The minerl basalt competition on learning from human feedback.
\newblock \emph{arXiv preprint arXiv:2107.01969}, 2021{\natexlab{b}}.

\bibitem[Shaharabany et~al.(2023)Shaharabany, Dahan, Giryes, and
  Wolf]{shaharabany2023autosam}
Tal Shaharabany, Aviad Dahan, Raja Giryes, and Lior Wolf.
\newblock Autosam: Adapting sam to medical images by overloading the prompt
  encoder.
\newblock \emph{arXiv preprint arXiv:2306.06370}, 2023.

\bibitem[Shao et~al.(2019)Shao, Li, Zhang, Peng, Yu, Zhang, Li, and
  Sun]{shao2019objects365}
Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing
  Li, and Jian Sun.
\newblock Objects365: A large-scale, high-quality dataset for object detection.
\newblock In \emph{Proceedings of the IEEE/CVF international conference on
  computer vision}, pages 8430--8439, 2019.

\bibitem[Sharma et~al.(2018)Sharma, Ding, Goodman, and
  Soricut]{sharma2018conceptual}
Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut.
\newblock Conceptual captions: A cleaned, hypernymed, image alt-text dataset
  for automatic image captioning.
\newblock In \emph{Proceedings of ACL}, 2018.

\bibitem[Shen et~al.(2023)Shen, Song, Tan, Li, Lu, and
  Zhuang]{shen2023hugginggpt}
Yongliang Shen, Kaitao Song, Xu~Tan, Dongsheng Li, Weiming Lu, and Yueting
  Zhuang.
\newblock Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging
  face.
\newblock \emph{arXiv preprint arXiv: 2303.17580}, 2023.

\bibitem[Shtedritski et~al.(2023)Shtedritski, Rupprecht, and
  Vedaldi]{shtedritski2023does}
Aleksandar Shtedritski, Christian Rupprecht, and Andrea Vedaldi.
\newblock What does clip know about a red circle? visual prompt engineering for
  vlms.
\newblock \emph{arXiv preprint arXiv:2304.06712}, 2023.

\bibitem[Shukor et~al.(2023)Shukor, Dancette, and Cord]{shukor2023ep}
Mustafa Shukor, Corentin Dancette, and Matthieu Cord.
\newblock ep-alm: Efficient perceptual augmentation of language models.
\newblock \emph{arXiv preprint arXiv:2303.11403}, 2023.

\bibitem[Sidorov et~al.(2020)Sidorov, Hu, Rohrbach, and
  Singh]{sidorov2020textcaps}
Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh.
\newblock Textcaps: a dataset for image captioning with reading comprehension.
\newblock In \emph{Computer Vision--ECCV 2020: 16th European Conference,
  Glasgow, UK, August 23--28, 2020, Proceedings, Part II 16}, pages 742--758.
  Springer, 2020.

\bibitem[Silberman et~al.(2012)Silberman, Hoiem, Kohli, and
  Fergus]{silberman2012indoor}
Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus.
\newblock Indoor segmentation and support inference from rgbd images.
\newblock In \emph{Computer Vision--ECCV 2012: 12th European Conference on
  Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part V
  12}, pages 746--760. Springer, 2012.

\bibitem[Silva et~al.(2014)Silva, Histace, Romain, Dray, and
  Granado]{silva2014toward}
Juan Silva, Aymeric Histace, Olivier Romain, Xavier Dray, and Bertrand Granado.
\newblock Toward embedded detection of polyps in wce images for early diagnosis
  of colorectal cancer.
\newblock \emph{International journal of computer assisted radiology and
  surgery}, 9:\penalty0 283--293, 2014.

\bibitem[Singh et~al.(2019)Singh, Natarajan, Shah, Jiang, Chen, Batra, Parikh,
  and Rohrbach]{singh2019towards}
Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu~Jiang, Xinlei Chen, Dhruv
  Batra, Devi Parikh, and Marcus Rohrbach.
\newblock Towards vqa models that can read.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision
  and pattern recognition}, pages 8317--8326, 2019.

\bibitem[Singh et~al.(2022)Singh, Hu, Goswami, Couairon, Galuba, Rohrbach, and
  Kiela]{singh2022flava}
Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech
  Galuba, Marcus Rohrbach, and Douwe Kiela.
\newblock Flava: A foundational language and vision alignment model.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 15638--15650, 2022.

\bibitem[Sirinukunwattana et~al.(2017)Sirinukunwattana, Pluim, Chen, Qi, Heng,
  Guo, Wang, Matuszewski, Bruni, Sanchez, et~al.]{sirinukunwattana2017gland}
Korsuk Sirinukunwattana, Josien~PW Pluim, Hao Chen, Xiaojuan Qi, Pheng-Ann
  Heng, Yun~Bo Guo, Li~Yang Wang, Bogdan~J Matuszewski, Elia Bruni, Urko
  Sanchez, et~al.
\newblock Gland segmentation in colon histology images: The glas challenge
  contest.
\newblock \emph{Medical image analysis}, 35:\penalty0 489--502, 2017.

\bibitem[Skreta et~al.(2023)Skreta, Yoshikawa, Arellano-Rubach, Ji, Kristensen,
  Darvish, Aspuru-Guzik, Shkurti, and Garg]{skreta2023errors}
Marta Skreta, Naruki Yoshikawa, Sebastian Arellano-Rubach, Zhi Ji, Lasse~Bjørn
  Kristensen, Kourosh Darvish, Alán Aspuru-Guzik, Florian Shkurti, and Animesh
  Garg.
\newblock Errors are useful prompts: Instruction guided task programming with
  verifier-assisted iterative prompting, 2023.

\bibitem[Song et~al.(2015)Song, Lichtenberg, and Xiao]{song2015sun}
Shuran Song, Samuel~P Lichtenberg, and Jianxiong Xiao.
\newblock Sun rgb-d: A rgb-d scene understanding benchmark suite.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 567--576, 2015.

\bibitem[Srinivasan et~al.(2021)Srinivasan, Raman, Chen, Bendersky, and
  Najork]{srinivasan2021wit}
Krishna Srinivasan, Karthik Raman, Jiecao Chen, Michael Bendersky, and Marc
  Najork.
\newblock Wit: Wikipedia-based image text dataset for multimodal multilingual
  machine learning.
\newblock \emph{arXiv preprint arXiv: 2103.01913}, 2021.

\bibitem[Stella et~al.(2023)Stella, Della~Santina, and Hughes]{stella2023can}
Francesco Stella, Cosimo Della~Santina, and Josie Hughes.
\newblock Can large language models design a robot?
\newblock \emph{arXiv preprint arXiv:2303.15324}, 2023.

\bibitem[Sun et~al.(2023{\natexlab{a}})Sun, Fang, Wu, Wang, and
  Cao]{sun2023eva}
Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao.
\newblock Eva-clip: Improved training techniques for clip at scale.
\newblock \emph{arXiv preprint arXiv:2303.15389}, 2023{\natexlab{a}}.

\bibitem[Sun et~al.(2022)Sun, Dong, Patra, Ma, Huang, Benhaim, Chaudhary, Song,
  and Wei]{sun2022length}
Yutao Sun, Li~Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim,
  Vishrav Chaudhary, Xia Song, and Furu Wei.
\newblock A length-extrapolatable transformer.
\newblock \emph{arXiv preprint arXiv:2212.10554}, 2022.

\bibitem[Sun et~al.(2023{\natexlab{b}})Sun, Dong, Huang, Ma, Xia, Xue, Wang,
  and Wei]{sun2023retentive}
Yutao Sun, Li~Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong
  Wang, and Furu Wei.
\newblock Retentive network: A successor to transformer for large language
  models.
\newblock \emph{arXiv preprint arXiv:2307.08621}, 2023{\natexlab{b}}.

\bibitem[Surís et~al.(2023)Surís, Menon, and Vondrick]{surís2023vipergpt}
Dídac Surís, Sachit Menon, and Carl Vondrick.
\newblock Vipergpt: Visual inference via python execution for reasoning.
\newblock \emph{arXiv preprint arXiv: 2303.08128}, 2023.

\bibitem[Tajbakhsh et~al.(2015)Tajbakhsh, Gurudu, and
  Liang]{tajbakhsh2015automated}
Nima Tajbakhsh, Suryakanth~R Gurudu, and Jianming Liang.
\newblock Automated polyp detection in colonoscopy videos using shape and
  context information.
\newblock \emph{IEEE transactions on medical imaging}, 35\penalty0
  (2):\penalty0 630--644, 2015.

\bibitem[Tan and Bansal(2019)]{tan2019lxmert}
Hao Tan and Mohit Bansal.
\newblock Lxmert: Learning cross-modality encoder representations from
  transformers.
\newblock \emph{arXiv preprint arXiv:1908.07490}, 2019.

\bibitem[Tan and Le(2019)]{tan2019efficientnet}
Mingxing Tan and Quoc Le.
\newblock Efficientnet: Rethinking model scaling for convolutional neural
  networks.
\newblock In \emph{International conference on machine learning}, pages
  6105--6114. PMLR, 2019.

\bibitem[Tarvainen and Valpola(2017)]{tarvainen2017mean}
Antti Tarvainen and Harri Valpola.
\newblock Mean teachers are better role models: Weight-averaged consistency
  targets improve semi-supervised deep learning results.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Thawkar et~al.(2023)Thawkar, Shaker, Mullappilly, Cholakkal, Anwer,
  Khan, Laaksonen, and Khan]{thawkar2023xraygpt}
Omkar Thawkar, Abdelrahman Shaker, Sahal~Shaji Mullappilly, Hisham Cholakkal,
  Rao~Muhammad Anwer, Salman Khan, Jorma Laaksonen, and Fahad~Shahbaz Khan.
\newblock Xraygpt: Chest radiographs summarization using medical
  vision-language models.
\newblock \emph{arXiv preprint arXiv:2306.07971}, 2023.

\bibitem[Thomee et~al.(2016)Thomee, Shamma, Friedland, Elizalde, Ni, Poland,
  Borth, and Li]{thomee2016yfcc100m}
Bart Thomee, David~A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni,
  Douglas Poland, Damian Borth, and Li-Jia Li.
\newblock Yfcc100m: The new data in multimedia research.
\newblock \emph{Communications of the ACM}, 59\penalty0 (2):\penalty0 64--73,
  2016.

\bibitem[Tong et~al.(2023)Tong, Jones, and Steinhardt]{tong2023mass}
Shengbang Tong, Erik Jones, and Jacob Steinhardt.
\newblock Mass-producing failures of multimodal systems with language models.
\newblock \emph{arXiv preprint arXiv:2306.12105}, 2023.

\bibitem[Touvron et~al.(2021)Touvron, Cord, Douze, Massa, Sablayrolles, and
  J{\'e}gou]{touvron2021training}
Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre
  Sablayrolles, and Herv{\'e} J{\'e}gou.
\newblock Training data-efficient image transformers \& distillation through
  attention.
\newblock In \emph{International conference on machine learning}, pages
  10347--10357. PMLR, 2021.

\bibitem[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux,
  Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
  Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric
  Hambro, Faisal Azhar, et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}, 2023.

\bibitem[Tschannen et~al.(2023)Tschannen, Kumar, Steiner, Zhai, Houlsby, and
  Beyer]{tschannen2023image}
Michael Tschannen, Manoj Kumar, Andreas Steiner, Xiaohua Zhai, Neil Houlsby,
  and Lucas Beyer.
\newblock Image captioners are scalable vision learners too.
\newblock \emph{arXiv preprint arXiv:2306.07915}, 2023.

\bibitem[Tsimpoukelli et~al.(2021)Tsimpoukelli, Menick, Cabi, Eslami, Vinyals,
  and Hill]{tsimpoukelli2021multimodal}
Maria Tsimpoukelli, Jacob~L Menick, Serkan Cabi, SM~Eslami, Oriol Vinyals, and
  Felix Hill.
\newblock Multimodal few-shot learning with frozen language models.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 200--212, 2021.

\bibitem[Tu et~al.(2005)Tu, Chen, Yuille, and Zhu]{tu2005image}
Zhuowen Tu, Xiangrong Chen, Alan~L Yuille, and Song-Chun Zhu.
\newblock Image parsing: Unifying segmentation, detection, and recognition.
\newblock \emph{International Journal of computer vision}, 63:\penalty0
  113--140, 2005.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Wake et~al.(2023)Wake, Kanehira, Sasabuchi, Takamatsu, and
  Ikeuchi]{wake2023chatgpt}
Naoki Wake, Atsushi Kanehira, Kazuhiro Sasabuchi, Jun Takamatsu, and Katsushi
  Ikeuchi.
\newblock Chatgpt empowered long-step robot control in various environments: A
  case application.
\newblock \emph{arXiv preprint arXiv: 2304.03893}, 2023.

\bibitem[Wang et~al.(2023{\natexlab{a}})Wang, Xie, Jiang, Mandlekar, Xiao, Zhu,
  Fan, and Anandkumar]{wang2023voyager}
Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu,
  Linxi Fan, and Anima Anandkumar.
\newblock Voyager: An open-ended embodied agent with large language models.
\newblock \emph{arXiv preprint arXiv:2305.16291}, 2023{\natexlab{a}}.

\bibitem[Wang et~al.(2022{\natexlab{a}})Wang, Ma, Huang, Dong, Wang, Peng, Wu,
  Bajaj, Singhal, Benhaim, Patra, Liu, Chaudhary, Song, and
  Wei]{wang2022foundation}
Hongyu Wang, Shuming Ma, Shaohan Huang, Li~Dong, Wenhui Wang, Zhiliang Peng,
  Yu~Wu, Payal Bajaj, Saksham Singhal, Alon Benhaim, Barun Patra, Zhun Liu,
  Vishrav Chaudhary, Xia Song, and Furu Wei.
\newblock Foundation transformers, 2022{\natexlab{a}}.

\bibitem[Wang et~al.(2022{\natexlab{b}})Wang, Yang, Hu, Li, Lin, Gan, Liu, Liu,
  and Wang]{wang2022git}
Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan,
  Zicheng Liu, Ce~Liu, and Lijuan Wang.
\newblock Git: A generative image-to-text transformer for vision and language.
\newblock \emph{arXiv preprint arXiv:2205.14100}, 2022{\natexlab{b}}.

\bibitem[Wang et~al.(2022{\natexlab{c}})Wang, Ge, Cai, Yan, Lin, Shan, Qie, and
  Shou]{wang2022object}
Jinpeng Wang, Yixiao Ge, Guanyu Cai, Rui Yan, Xudong Lin, Ying Shan, Xiaohu
  Qie, and Mike~Zheng Shou.
\newblock Object-aware video-language pre-training for retrieval.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision
  and pattern recognition}, pages 3313--3322, 2022{\natexlab{c}}.

\bibitem[Wang et~al.(2023{\natexlab{b}})Wang, Liu, Park, Chen, and
  Xiao]{wang2023adversarial}
Jiongxiao Wang, Zichen Liu, Keun~Hee Park, Muhao Chen, and Chaowei Xiao.
\newblock Adversarial demonstration attacks on large language models.
\newblock \emph{arXiv preprint arXiv:2305.14950}, 2023{\natexlab{b}}.

\bibitem[Wang et~al.(2023{\natexlab{c}})Wang, Chen, Luo, Dai, Yuan, Wu, and
  Jiang]{wang2023chatvideo}
Junke Wang, Dongdong Chen, Chong Luo, Xiyang Dai, Lu~Yuan, Zuxuan Wu, and
  Yu-Gang Jiang.
\newblock Chatvideo: A tracklet-centric multimodal and versatile video
  understanding system.
\newblock \emph{arXiv preprint arXiv:2304.14407}, 2023{\natexlab{c}}.

\bibitem[Wang et~al.(2023{\natexlab{d}})Wang, Zhang, Su, and
  Zhu]{wang2023comprehensive}
Liyuan Wang, Xingxing Zhang, Hang Su, and Jun Zhu.
\newblock A comprehensive survey of continual learning: Theory, method and
  application.
\newblock \emph{arXiv preprint arXiv:2302.00487}, 2023{\natexlab{d}}.

\bibitem[Wang et~al.(2023{\natexlab{e}})Wang, Zhang, Fei, Zheng, Tang, Li, Gao,
  and Zhao]{wang2023caption}
Teng Wang, Jinrui Zhang, Junjie Fei, Hao Zheng, Yunlong Tang, Zhe Li, Mingqi
  Gao, and Shanshan Zhao.
\newblock Caption anything: Interactive image description with diverse
  multimodal controls.
\newblock \emph{arXiv preprint arXiv: 2305.02677}, 2023{\natexlab{e}}.

\bibitem[Wang et~al.(2021{\natexlab{a}})Wang, Feiszli, Wang, and
  Tran]{wang2021unidentified}
Weiyao Wang, Matt Feiszli, Heng Wang, and Du~Tran.
\newblock Unidentified video objects: A benchmark for dense, open-world
  segmentation.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 10776--10785, 2021{\natexlab{a}}.

\bibitem[Wang et~al.(2023{\natexlab{f}})Wang, Chen, Chen, Wu, Zhu, Zeng, Luo,
  Lu, Zhou, Qiao, and Dai]{wang2023visionllm}
Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping
  Luo, Tong Lu, Jie Zhou, Yu~Qiao, and Jifeng Dai.
\newblock Visionllm: Large language model is also an open-ended decoder for
  vision-centric tasks.
\newblock \emph{arXiv preprint arXiv: 2305.11175}, 2023{\natexlab{f}}.

\bibitem[Wang et~al.(2022{\natexlab{d}})Wang, Bao, Dong, Bjorck, Peng, Liu,
  Aggarwal, Mohammed, Singhal, Som, et~al.]{wang2022image}
Wenhui Wang, Hangbo Bao, Li~Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti
  Aggarwal, Owais~Khan Mohammed, Saksham Singhal, Subhojit Som, et~al.
\newblock Image as a foreign language: Beit pretraining for all vision and
  vision-language tasks.
\newblock \emph{arXiv preprint arXiv:2208.10442}, 2022{\natexlab{d}}.

\bibitem[Wang et~al.(2019)Wang, Wu, Chen, Li, Wang, and Wang]{wang2019vatex}
Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang Wang, and William~Yang
  Wang.
\newblock Vatex: A large-scale, high-quality multilingual dataset for
  video-and-language research.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 4581--4591, 2019.

\bibitem[Wang et~al.(2023{\natexlab{g}})Wang, Wang, Cao, Shen, and
  Huang]{wang2023images}
Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, and Tiejun Huang.
\newblock Images speak in images: A generalist painter for in-context visual
  learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 6830--6839, 2023{\natexlab{g}}.

\bibitem[Wang et~al.(2023{\natexlab{h}})Wang, Zhang, Cao, Wang, Shen, and
  Huang]{wang2023seggpt}
Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang, Chunhua Shen, and Tiejun
  Huang.
\newblock Seggpt: Segmenting everything in context.
\newblock \emph{arXiv preprint arXiv:2304.03284}, 2023{\natexlab{h}}.

\bibitem[Wang et~al.(2022{\natexlab{e}})Wang, Lu, Li, Tao, Guo, Gong, and
  Liu]{wang2022cris}
Zhaoqing Wang, Yu~Lu, Qiang Li, Xunqiang Tao, Yandong Guo, Mingming Gong, and
  Tongliang Liu.
\newblock Cris: Clip-driven referring image segmentation.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision
  and pattern recognition}, pages 11686--11695, 2022{\natexlab{e}}.

\bibitem[Wang et~al.(2023{\natexlab{i}})Wang, Li, Chen, Lim, Torralba, Zhao,
  and Wang]{wang2023detecting}
Zhenyu Wang, Yali Li, Xi~Chen, Ser-Nam Lim, Antonio Torralba, Hengshuang Zhao,
  and Shengjin Wang.
\newblock Detecting everything in the open world: Towards universal object
  detection.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 11433--11443, 2023{\natexlab{i}}.

\bibitem[Wang et~al.(2021{\natexlab{b}})Wang, Yu, Yu, Dai, Tsvetkov, and
  Cao]{wang2021simvlm}
Zirui Wang, Jiahui Yu, Adams~Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao.
\newblock Simvlm: Simple visual language model pretraining with weak
  supervision.
\newblock \emph{arXiv preprint arXiv:2108.10904}, 2021{\natexlab{b}}.

\bibitem[Wanna et~al.(2023)Wanna, Parra, Valner, Kruusam{\"a}e, and
  Pryor]{wanna2023multimodal}
Selma Wanna, Fabian Parra, Robert Valner, Karl Kruusam{\"a}e, and Mitch Pryor.
\newblock Multimodal grounding for embodied ai via augmented reality headsets
  for natural language driven task planning.
\newblock \emph{arXiv preprint arXiv:2304.13676}, 2023.

\bibitem[Wei et~al.(2018)Wei, Wang, Yang, and Liu]{wei2018deep}
Chen Wei, Wenjing Wang, Wenhan Yang, and Jiaying Liu.
\newblock Deep retinex decomposition for low-light enhancement.
\newblock \emph{arXiv preprint arXiv:1808.04560}, 2018.

\bibitem[Wei et~al.(2022{\natexlab{a}})Wei, Tay, Bommasani, Raffel, Zoph,
  Borgeaud, Yogatama, Bosma, Zhou, Metzler, et~al.]{wei2022emergent}
Jason Wei, Yi~Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian
  Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et~al.
\newblock Emergent abilities of large language models.
\newblock \emph{arXiv preprint arXiv:2206.07682}, 2022{\natexlab{a}}.

\bibitem[Wei et~al.(2022{\natexlab{b}})Wei, Wang, Schuurmans, Bosma, Xia, Chi,
  Le, Zhou, et~al.]{wei2022chain}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed~Chi, Quoc~V
  Le, Denny Zhou, et~al.
\newblock Chain-of-thought prompting elicits reasoning in large language
  models.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 24824--24837, 2022{\natexlab{b}}.

\bibitem[Wenhui et~al.(2023)Wenhui, Xu, Xiaofan, Kang, and
  Zhang]{Lei2023medlam}
Lei Wenhui, Wei Xu, Zhang Xiaofan, Li~Kang, and Shaoting Zhang.
\newblock Medlsam: Localize and segment anything model for 3d medical images.
\newblock \emph{arXiv preprint arXiv:}, 2023.

\bibitem[Wu et~al.(2023{\natexlab{a}})Wu, Yin, Qi, Wang, Tang, and
  Duan]{wu2023visual}
Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan
  Duan.
\newblock Visual chatgpt: Talking, drawing and editing with visual foundation
  models.
\newblock \emph{arXiv preprint arXiv: 2303.04671}, 2023{\natexlab{a}}.

\bibitem[Wu et~al.(2020)Wu, Lin, Cohen, Bui, and Maji]{wu2020phrasecut}
Chenyun Wu, Zhe Lin, Scott Cohen, Trung Bui, and Subhransu Maji.
\newblock Phrasecut: Language-based image segmentation in the wild.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 10216--10225, 2020.

\bibitem[Wu et~al.(2023{\natexlab{b}})Wu, Antonova, Kan, Lepert, Zeng, Song,
  Bohg, Rusinkiewicz, and Funkhouser]{wu2023tidybot}
Jimmy Wu, Rika Antonova, Adam Kan, Marion Lepert, Andy Zeng, Shuran Song,
  Jeannette Bohg, Szymon Rusinkiewicz, and Thomas Funkhouser.
\newblock Tidybot: Personalized robot assistance with large language models.
\newblock \emph{arXiv preprint arXiv:2305.05658}, 2023{\natexlab{b}}.

\bibitem[Wu et~al.(2023{\natexlab{c}})Wu, Fu, Fang, Liu, Wang, Xu, Jin, and
  Arbel]{wu2023medical}
Junde Wu, Rao Fu, Huihui Fang, Yuanpei Liu, Zhaowei Wang, Yanwu Xu, Yueming
  Jin, and Tal Arbel.
\newblock Medical sam adapter: Adapting segment anything model for medical
  image segmentation.
\newblock \emph{arXiv preprint arXiv:2304.12620}, 2023{\natexlab{c}}.

\bibitem[Wu et~al.(2023{\natexlab{d}})Wu, Wang, Xu, Lu, and
  Yan]{wu2023embodied}
Zhenyu Wu, Ziwei Wang, Xiuwei Xu, Jiwen Lu, and Haibin Yan.
\newblock Embodied task planning with large language models.
\newblock \emph{arXiv preprint arXiv:2307.01848}, 2023{\natexlab{d}}.

\bibitem[Xu et~al.(2017)Xu, Zhao, Xiao, Wu, Zhang, He, and Zhuang]{xu2017video}
Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting
  Zhuang.
\newblock Video question answering via gradually refined attention over
  appearance and motion.
\newblock In \emph{Proceedings of the 25th ACM international conference on
  Multimedia}, pages 1645--1653, 2017.

\bibitem[Xu et~al.(2022{\natexlab{a}})Xu, Zhang, Cai, Rezatofighi, Yu, Tao, and
  Geiger]{xu2022unifying}
Haofei Xu, Jing Zhang, Jianfei Cai, Hamid Rezatofighi, Fisher Yu, Dacheng Tao,
  and Andreas Geiger.
\newblock Unifying flow, stereo and depth estimation.
\newblock \emph{arXiv preprint arXiv:2211.05783}, 2022{\natexlab{a}}.

\bibitem[Xu et~al.(2022{\natexlab{b}})Xu, Mello, Liu, Byeon, Breuel, Kautz, and
  Wang]{xu2022groupvit}
Jiarui Xu, Shalini~De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, J.~Kautz,
  and X.~Wang.
\newblock Groupvit: Semantic segmentation emerges from text supervision.
\newblock \emph{Computer Vision And Pattern Recognition}, 2022{\natexlab{b}}.
\newblock \doi{10.1109/CVPR52688.2022.01760}.

\bibitem[Xu et~al.(2023{\natexlab{a}})Xu, Liu, Vahdat, Byeon, Wang, and
  De~Mello]{xu2023open}
Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiaolong Wang, and Shalini
  De~Mello.
\newblock Open-vocabulary panoptic segmentation with text-to-image diffusion
  models.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 2955--2966, 2023{\natexlab{a}}.

\bibitem[Xu et~al.(2016)Xu, Mei, Yao, and Rui]{xu2016msr}
Jun Xu, Tao Mei, Ting Yao, and Yong Rui.
\newblock Msr-vtt: A large video description dataset for bridging video and
  language.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 5288--5296, 2016.

\bibitem[Xu et~al.(2023{\natexlab{b}})Xu, Zhu, and Clifton]{Peng2023MMT}
Peng Xu, Xiatian Zhu, and David Clifton.
\newblock Multimodal learning with transformers: A survey.
\newblock \emph{TPAMI}, pages 1--21, 2023{\natexlab{b}}.

\bibitem[Xu et~al.(2022{\natexlab{c}})Xu, Wu, Rosenman, Lal, and
  Duan]{xu2022bridge}
Xiao Xu, Chenfei Wu, Shachar Rosenman, Vasudev Lal, and Nan Duan.
\newblock Bridge-tower: Building bridges between encoders in vision-language
  representation learning.
\newblock \emph{arXiv preprint arXiv:2206.08657}, 2022{\natexlab{c}}.

\bibitem[Xue et~al.(2020)Xue, Constant, Roberts, Kale, Al-Rfou, Siddhant,
  Barua, and Raffel]{xue2020mt5}
Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya
  Siddhant, Aditya Barua, and Colin Raffel.
\newblock mt5: A massively multilingual pre-trained text-to-text transformer.
\newblock \emph{arXiv preprint arXiv:2010.11934}, 2020.

\bibitem[Yang et~al.(2021)Yang, Miech, Sivic, Laptev, and Schmid]{yang2021just}
Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid.
\newblock Just ask: Learning to answer questions from millions of narrated
  videos.
\newblock In \emph{Proceedings of the IEEE/CVF international conference on
  computer vision}, pages 1686--1697, 2021.

\bibitem[Yang et~al.(2023{\natexlab{a}})Yang, Tan, Jin, Liu, Fu, Song, and
  Wang]{yang2023pave}
Jiange Yang, Wenhui Tan, Chuhao Jin, Bei Liu, Jianlong Fu, Ruihua Song, and
  Limin Wang.
\newblock Pave the way to grasp anything: Transferring foundation models for
  universal pick-place robots.
\newblock \emph{arXiv preprint arXiv:2306.05716}, 2023{\natexlab{a}}.

\bibitem[Yang et~al.(2022{\natexlab{a}})Yang, Li, Dai, and Gao]{yang2022focal}
Jianwei Yang, Chunyuan Li, Xiyang Dai, and Jianfeng Gao.
\newblock Focal modulation networks.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 4203--4217, 2022{\natexlab{a}}.

\bibitem[Yang et~al.(2022{\natexlab{b}})Yang, Li, Zhang, Xiao, Liu, Yuan, and
  Gao]{yang2022unified}
Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Bin Xiao, Ce~Liu, Lu~Yuan, and
  Jianfeng Gao.
\newblock Unified contrastive learning in image-text-label space.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 19163--19173, 2022{\natexlab{b}}.

\bibitem[Yang et~al.(2023{\natexlab{b}})Yang, Jin, Tang, Han, Feng, Jiang, Yin,
  and Hu]{Jingfeng2023LLM}
Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming
  Jiang, Bing Yin, and Xia Hu.
\newblock Harnessing the power of llms in practice: A survey on chatgpt and
  beyond.
\newblock \emph{arXiv preprint arXiv:2304.13712}, 2023{\natexlab{b}}.

\bibitem[Yang et~al.(2023{\natexlab{c}})Yang, Gao, Li, Gao, Wang, and
  Zheng]{yang2023track}
Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing Wang, and Feng Zheng.
\newblock Track anything: Segment anything meets videos.
\newblock \emph{arXiv preprint arXiv: 2304.11968}, 2023{\natexlab{c}}.

\bibitem[Yang et~al.(2022{\natexlab{c}})Yang, Zhang, Song, Hong, Xu, Zhao,
  Shao, Zhang, Cui, and Yang]{yang2022diffusion}
Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao,
  Yingxia Shao, Wentao Zhang, Bin Cui, and Ming-Hsuan Yang.
\newblock Diffusion models: A comprehensive survey of methods and applications.
\newblock \emph{arXiv preprint arXiv:2209.00796}, 2022{\natexlab{c}}.

\bibitem[Yang et~al.(2023{\natexlab{d}})Yang, Li, Wang, Lin, Azarnasab, Ahmed,
  Liu, Liu, Zeng, and Wang]{yang2023mmreact}
Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal
  Ahmed, Zicheng Liu, Ce~Liu, Michael Zeng, and Lijuan Wang.
\newblock Mm-react: Prompting chatgpt for multimodal reasoning and action.
\newblock \emph{arXiv preprint arXiv: 2303.11381}, 2023{\natexlab{d}}.

\bibitem[Yang and Yang(2022)]{yang2022decoupling}
Zongxin Yang and Yi~Yang.
\newblock Decoupling features in hierarchical propagation for video object
  segmentation.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 36324--36336, 2022.

\bibitem[Yao et~al.(2021)Yao, Huang, Hou, Lu, Niu, Xu, Liang, Li, Jiang, and
  Xu]{yao2021filip}
Lewei Yao, Runhui Huang, Lu~Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan
  Liang, Zhenguo Li, Xin Jiang, and Chunjing Xu.
\newblock Filip: fine-grained interactive language-image pre-training.
\newblock \emph{arXiv preprint arXiv:2111.07783}, 2021.

\bibitem[Ye et~al.(2022)Ye, Fu, Zheng, Paudel, and Chen]{ye2022unsupervised}
Junjie Ye, Changhong Fu, Guangze Zheng, Danda~Pani Paudel, and Guang Chen.
\newblock Unsupervised domain adaptation for nighttime aerial tracking.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 8896--8905, 2022.

\bibitem[Ye et~al.(2019)Ye, Rochan, Liu, and Wang]{ye2019cross}
Linwei Ye, Mrigank Rochan, Zhi Liu, and Yang Wang.
\newblock Cross-modal self-attention network for referring image segmentation.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision
  and pattern recognition}, pages 10502--10511, 2019.

\bibitem[Ye et~al.(2023)Ye, Xu, Xu, Ye, Yan, Zhou, Wang, Hu, Shi, Shi, Jiang,
  Li, Xu, Chen, Tian, Qi, Zhang, and Huang]{ye2023mplugowl}
Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang
  Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chaoya Jiang, Chenliang Li, Yuanhong
  Xu, Hehong Chen, Junfeng Tian, Qian Qi, Ji~Zhang, and Fei Huang.
\newblock mplug-owl: Modularization empowers large language models with
  multimodality, 2023.

\bibitem[Yoneda et~al.(2023)Yoneda, Fang, Li, Zhang, Jiang, Lin, Picker, Yunis,
  Mei, and Walter]{yoneda2023statler}
Takuma Yoneda, Jiading Fang, Peng Li, Huanyu Zhang, Tianchong Jiang, Shengjie
  Lin, Ben Picker, David Yunis, Hongyuan Mei, and Matthew~R Walter.
\newblock Statler: State-maintaining language models for embodied reasoning.
\newblock \emph{arXiv preprint arXiv:2306.17840}, 2023.

\bibitem[Yonglin et~al.(2023)Yonglin, Jing, Xiao, and Long]{refsam}
Li~Yonglin, Zhang Jing, Teng Xiao, and Lan Long.
\newblock Refsam: Efficiently adapting segmenting anything model for referring
  video object segmentation.
\newblock \emph{arXiv:2307.00997}, 2023.

\bibitem[You et~al.(2023)You, Ye, Zhou, Zhu, and Du]{you2023robot}
Hengxu You, Yang Ye, Tianyu Zhou, Qi~Zhu, and Jing Du.
\newblock Robot-enabled construction assembly with automated sequence planning
  based on chatgpt: Robogpt.
\newblock \emph{arXiv preprint arXiv:2304.11018}, 2023.

\bibitem[Young et~al.(2014)Young, Lai, Hodosh, and Hockenmaier]{young2014image}
Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier.
\newblock From image descriptions to visual denotations: New similarity metrics
  for semantic inference over event descriptions.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  2:\penalty0 67--78, 2014.

\bibitem[Yu et~al.(2022)Yu, Wang, Vasudevan, Yeung, Seyedhosseini, and
  Wu]{yu2022coca}
Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and
  Yonghui Wu.
\newblock Coca: Contrastive captioners are image-text foundation models.
\newblock \emph{arXiv preprint arXiv:2205.01917}, 2022.

\bibitem[Yu et~al.(2016)Yu, Poirson, Yang, Berg, and Berg]{yu2016modeling}
Licheng Yu, Patrick Poirson, Shan Yang, Alexander~C Berg, and Tamara~L Berg.
\newblock Modeling context in referring expressions.
\newblock In \emph{Computer Vision--ECCV 2016: 14th European Conference,
  Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14},
  pages 69--85. Springer, 2016.

\bibitem[Yuan et~al.(2021)Yuan, Chen, Chen, Codella, Dai, Gao, Hu, Huang, Li,
  Li, et~al.]{yuan2021florence}
Lu~Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao,
  Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, et~al.
\newblock Florence: A new foundation model for computer vision.
\newblock \emph{arXiv preprint arXiv:2111.11432}, 2021.

\bibitem[Yuan et~al.(2023)Yuan, Xue, Wang, Liu, Zhao, and Wang]{yuan2023artgpt}
Zhengqing Yuan, Huiwen Xue, Xinyi Wang, Yongming Liu, Zhuanzhe Zhao, and Kun
  Wang.
\newblock Artgpt-4: Artistic vision-language understanding with
  adapter-enhanced minigpt-4.
\newblock \emph{arXiv preprint arXiv:2305.07490}, 2023.

\bibitem[Zhai and Wu(2018)]{zhai2018classification}
Andrew Zhai and Hao-Yu Wu.
\newblock Classification is a strong baseline for deep metric learning.
\newblock \emph{arXiv preprint arXiv:1811.12649}, 2018.

\bibitem[Zhai et~al.(2022)Zhai, Kolesnikov, Houlsby, and
  Beyer]{zhai2022scaling}
Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer.
\newblock Scaling vision transformers.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 12104--12113, 2022.

\bibitem[Zhang et~al.(2023{\natexlab{a}})Zhang, Fei, Yao, Ji, Li, Liu, and
  Chua]{zhang2023transfer}
Ao~Zhang, Hao Fei, Yuan Yao, Wei Ji, Li~Li, Zhiyuan Liu, and Tat-Seng Chua.
\newblock Transfer visual prompt generator across llms.
\newblock \emph{arXiv preprint arXiv: 2305.01278}, 2023{\natexlab{a}}.

\bibitem[Zhang et~al.(2023{\natexlab{b}})Zhang, Ge, Xu, Shan, and
  Shou]{zhang2023taca}
Binjie Zhang, Yixiao Ge, Xuyuan Xu, Ying Shan, and Mike~Zheng Shou.
\newblock Taca: Upgrading your visual foundation model with task-agnostic
  compatible adapter.
\newblock \emph{arXiv preprint arXiv: 2306.12642}, 2023{\natexlab{b}}.

\bibitem[Zhang et~al.(2023{\natexlab{c}})Zhang, Han, Qiao, Kim, Bae, Lee, and
  Hong]{mobile_sam}
Chaoning Zhang, Dongshen Han, Yu~Qiao, Jung~Uk Kim, Sung-Ho Bae, Seungkyu Lee,
  and Choong~Seon Hong.
\newblock Faster segment anything: Towards lightweight sam for mobile
  applications.
\newblock \emph{arXiv preprint arXiv:2306.14289}, 2023{\natexlab{c}}.

\bibitem[Zhang et~al.(2023{\natexlab{d}})Zhang, Han, Qiao, Kim, Bae, Lee, and
  Hong]{zhang2023faster}
Chaoning Zhang, Dongshen Han, Yu~Qiao, Jung~Uk Kim, Sung-Ho Bae, Seungkyu Lee,
  and Choong~Seon Hong.
\newblock Faster segment anything: Towards lightweight sam for mobile
  applications.
\newblock \emph{arXiv preprint arXiv: 2306.14289}, 2023{\natexlab{d}}.

\bibitem[Zhang et~al.(2023{\natexlab{e}})Zhang, Zhang, Kang, Kim, Bae, and
  Kweon]{zhang2023attack}
Chenshuang Zhang, Chaoning Zhang, Taegoo Kang, Donghun Kim, Sung-Ho Bae, and
  In~So Kweon.
\newblock Attack-sam: Towards evaluating adversarial robustness of segment
  anything model.
\newblock \emph{arXiv preprint arXiv:2305.00866}, 2023{\natexlab{e}}.

\bibitem[Zhang et~al.(2023{\natexlab{f}})Zhang, Zhang, Zhang, and
  Kweon]{zhang2023text}
Chenshuang Zhang, Chaoning Zhang, Mengchun Zhang, and In~So Kweon.
\newblock Text-to-image diffusion model in generative ai: A survey.
\newblock \emph{arXiv preprint arXiv:2303.07909}, 2023{\natexlab{f}}.

\bibitem[Zhang et~al.(2023{\natexlab{g}})Zhang, Liu, Cui, Huang, Lin, Yang, and
  Hu]{Chunhui2023SAM}
Chunhui Zhang, Li~Liu, Yawen Cui, Guanjie Huang, Weilin Lin, Yiqian Yang, and
  Yuehong Hu.
\newblock A comprehensive survey on segment anything model for vision and
  beyond.
\newblock \emph{arXiv preprint arXiv:2305.08196}, 2023{\natexlab{g}}.

\bibitem[Zhang et~al.(2022{\natexlab{a}})Zhang, Zhang, Hu, Chen, Li, Dai, Wang,
  Yuan, Hwang, and Gao]{zhang2022glipv2}
Haotian Zhang, Pengchuan Zhang, Xiaowei Hu, Yen-Chun Chen, Liunian Li, Xiyang
  Dai, Lijuan Wang, Lu~Yuan, Jenq-Neng Hwang, and Jianfeng Gao.
\newblock Glipv2: Unifying localization and vision-language understanding.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 36067--36080, 2022{\natexlab{a}}.

\bibitem[Zhang et~al.(2023{\natexlab{h}})Zhang, Du, Shan, Zhou, Du, Tenenbaum,
  Shu, and Gan]{zhang2023building}
Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun Du, Joshua~B
  Tenenbaum, Tianmin Shu, and Chuang Gan.
\newblock Building cooperative embodied agents modularly with large language
  models.
\newblock \emph{arXiv preprint arXiv:2307.02485}, 2023{\natexlab{h}}.

\bibitem[Zhang et~al.(2023{\natexlab{i}})Zhang, Pertsch, Zhang, and
  Lim]{zhang2023sprint}
Jesse Zhang, Karl Pertsch, Jiahui Zhang, and Joseph~J Lim.
\newblock Sprint: Scalable policy pre-training via language instruction
  relabeling.
\newblock \emph{arXiv preprint arXiv:2306.11886}, 2023{\natexlab{i}}.

\bibitem[Zhang(2023{\natexlab{a}})]{zhang2023graph}
Jiawei Zhang.
\newblock Graph-toolformer: To empower llms with graph reasoning ability via
  prompt augmented by chatgpt.
\newblock \emph{arXiv preprint arXiv:2304.11116}, 2023{\natexlab{a}}.

\bibitem[Zhang(2023{\natexlab{b}})]{zhang2023graphtoolformer}
Jiawei Zhang.
\newblock Graph-toolformer: To empower llms with graph reasoning ability via
  prompt augmented by chatgpt.
\newblock \emph{arXiv preprint arXiv: 2304.11116}, 2023{\natexlab{b}}.

\bibitem[Zhang et~al.(2023{\natexlab{j}})Zhang, Han, Zhou, Hu, Yan, Lu, Li,
  Gao, and Qiao]{zhang2023llama}
Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu,
  Hongsheng Li, Peng Gao, and Yu~Qiao.
\newblock Llama-adapter: Efficient fine-tuning of language models with
  zero-init attention.
\newblock \emph{arXiv preprint arXiv:2303.16199}, 2023{\natexlab{j}}.

\bibitem[Zhang et~al.(2023{\natexlab{k}})Zhang, Sun, Chen, Xiao, Shao, Zhang,
  Chen, and Luo]{zhang2023gpt4roi}
Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei Zhang, Kai
  Chen, and Ping Luo.
\newblock Gpt4roi: Instruction tuning large language model on
  region-of-interest, 2023{\natexlab{k}}.

\bibitem[Zhang et~al.(2022{\natexlab{b}})Zhang, Roller, Goyal, Artetxe, Chen,
  Chen, Dewan, Diab, Li, Lin, et~al.]{zhang2022opt}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
  Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al.
\newblock Opt: Open pre-trained transformer language models.
\newblock \emph{arXiv preprint arXiv:2205.01068}, 2022{\natexlab{b}}.

\bibitem[Zhang et~al.(2021)Zhang, Zhang, Li, Xu, Wang, Zhan, Xu, Ke, Zeng, Su,
  et~al.]{zhang2021sar}
Tianwen Zhang, Xiaoling Zhang, Jianwei Li, Xiaowo Xu, Baoyou Wang, Xu~Zhan,
  Yanqin Xu, Xiao Ke, Tianjiao Zeng, Hao Su, et~al.
\newblock Sar ship detection dataset (ssdd): Official release and comprehensive
  data analysis.
\newblock \emph{Remote Sensing}, 13\penalty0 (18):\penalty0 3690, 2021.

\bibitem[Zhang et~al.(2023{\natexlab{l}})Zhang, Zeng, Zhang, and
  Li]{zhang2023toward}
Xinsong Zhang, Yan Zeng, Jipeng Zhang, and Hang Li.
\newblock Toward building general foundation models for language, vision, and
  vision-language understanding tasks.
\newblock \emph{arXiv preprint arXiv:2301.05065}, 2023{\natexlab{l}}.

\bibitem[Zhang et~al.(2023{\natexlab{m}})Zhang, Zhang, Gu, Zhou, Lipka, Yang,
  and Sun]{zhang2023llavar}
Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi Yang, and
  Tong Sun.
\newblock Llavar: Enhanced visual instruction tuning for text-rich image
  understanding.
\newblock \emph{arXiv preprint arXiv: 2306.17107}, 2023{\natexlab{m}}.

\bibitem[Zhang et~al.(2023{\natexlab{n}})Zhang, Huang, Ma, Li, Luo, Xie, Qin,
  Luo, Li, Liu, et~al.]{zhang2023recognize}
Youcai Zhang, Xinyu Huang, Jinyu Ma, Zhaoyang Li, Zhaochuan Luo, Yanchun Xie,
  Yuzhuo Qin, Tong Luo, Yaqian Li, Shilong Liu, et~al.
\newblock Recognize anything: A strong image tagging model.
\newblock \emph{arXiv preprint arXiv:2306.03514}, 2023{\natexlab{n}}.

\bibitem[Zhang et~al.(2023{\natexlab{o}})Zhang, Zhang, Li, Zhao, Karypis, and
  Smola]{zhang2023multimodal}
Zhuosheng Zhang, Aston Zhang, Mu~Li, Hai Zhao, George Karypis, and Alex Smola.
\newblock Multimodal chain-of-thought reasoning in language models.
\newblock \emph{arXiv preprint arXiv:2302.00923}, 2023{\natexlab{o}}.

\bibitem[Zhao et~al.(2023{\natexlab{a}})Zhao, Yuan, Jiang, Cai, Yu, Wang, and
  Chen]{zhao2023erra}
Chao Zhao, Shuai Yuan, Chunli Jiang, Junhao Cai, Hongyu Yu, Michael~Yu Wang,
  and Qifeng Chen.
\newblock Erra: An embodied representation and reasoning architecture for
  long-horizon language-conditioned manipulation tasks.
\newblock \emph{IEEE Robotics and Automation Letters}, 2023{\natexlab{a}}.

\bibitem[Zhao et~al.(2023{\natexlab{b}})Zhao, Zhou, Li, Tang, Wang, Hou, Min,
  Zhang, Zhang, Dong, Du, Yang, Chen, Chen, Jiang, Ren, Li, Tang, Liu, Liu,
  Nie, and Wen]{Zhao2023LLM}
Wayne~Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,
  Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang,
  Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang,
  Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen.
\newblock A survey of large language models.
\newblock \emph{arXiv preprint arXiv:2303.18223}, 2023{\natexlab{b}}.

\bibitem[Zhao et~al.(2023{\natexlab{c}})Zhao, Ding, An, Du, Yu, Li, Tang, and
  Wang]{zhao2023fast}
Xu~Zhao, Wenchao Ding, Yongqi An, Yinglong Du, Tao Yu, Min Li, Ming Tang, and
  Jinqiao Wang.
\newblock Fast segment anything.
\newblock \emph{arXiv preprint arXiv: 2306.12156}, 2023{\natexlab{c}}.

\bibitem[Zhao et~al.(2023{\natexlab{d}})Zhao, Li, Weber, Hafez, and
  Wermter]{zhao2023chat}
Xufeng Zhao, Mengdi Li, Cornelius Weber, Muhammad~Burhan Hafez, and Stefan
  Wermter.
\newblock Chat with the environment: Interactive multimodal perception using
  large language models.
\newblock \emph{arXiv preprint arXiv: 2303.08268}, 2023{\natexlab{d}}.

\bibitem[Zhao et~al.(2023{\natexlab{e}})Zhao, Lin, Zhou, Huang, Feng, and
  Kang]{zhao2023bubogpt}
Yang Zhao, Zhijie Lin, Daquan Zhou, Zilong Huang, Jiashi Feng, and Bingyi Kang.
\newblock Bubogpt: Enabling visual grounding in multi-modal llms.
\newblock \emph{arXiv preprint arXiv:2307.08581}, 2023{\natexlab{e}}.

\bibitem[Zheng et~al.(2023{\natexlab{a}})Zheng, Chiang, Sheng, Zhuang, Wu,
  Zhuang, Lin, Li, Li, Xing, et~al.]{zheng2023judging}
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao
  Zhuang, Zi~Lin, Zhuohan Li, Dacheng Li, Eric Xing, et~al.
\newblock Judging llm-as-a-judge with mt-bench and chatbot arena.
\newblock \emph{arXiv preprint arXiv:2306.05685}, 2023{\natexlab{a}}.

\bibitem[Zheng et~al.(2023{\natexlab{b}})Zheng, Su, You, Wang, Qian, Xu, and
  Albanie]{zheng2023can}
Mingkai Zheng, Xiu Su, Shan You, Fei Wang, Chen Qian, Chang Xu, and Samuel
  Albanie.
\newblock Can gpt-4 perform neural architecture search?
\newblock \emph{arXiv preprint arXiv:2304.10970}, 2023{\natexlab{b}}.

\bibitem[Zheng et~al.(2023{\natexlab{c}})Zheng, Su, You, Wang, Qian, Xu, and
  Albanie]{zheng2023gpt4}
Mingkai Zheng, Xiu Su, Shan You, Fei Wang, Chen Qian, Chang Xu, and Samuel
  Albanie.
\newblock Can gpt-4 perform neural architecture search?
\newblock \emph{arXiv preprint arXiv: 2304.10970}, 2023{\natexlab{c}}.

\bibitem[Zhong et~al.(2023)Zhong, Wei, Yang, Wu, Liu, Wei, Li, Yao, Ma, Li,
  et~al.]{zhong2023chatabl}
Tianyang Zhong, Yaonai Wei, Li~Yang, Zihao Wu, Zhengliang Liu, Xiaozheng Wei,
  Wenjun Li, Junjie Yao, Chong Ma, Xiang Li, et~al.
\newblock Chatabl: Abductive learning via natural language interaction with
  chatgpt.
\newblock \emph{arXiv preprint arXiv:2304.11107}, 2023.

\bibitem[Zhong et~al.(2022)Zhong, Yang, Zhang, Li, Codella, Li, Zhou, Dai,
  Yuan, Li, et~al.]{zhong2022regionclip}
Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan Li, Noel Codella,
  Liunian~Harold Li, Luowei Zhou, Xiyang Dai, Lu~Yuan, Yin Li, et~al.
\newblock Regionclip: Region-based language-image pretraining.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 16793--16803, 2022.

\bibitem[Zhou et~al.(2017)Zhou, Zhao, Puig, Fidler, Barriuso, and
  Torralba]{zhou2017scene}
Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio
  Torralba.
\newblock Scene parsing through ade20k dataset.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 633--641, 2017.

\bibitem[Zhou et~al.(2019)Zhou, Zhao, Puig, Xiao, Fidler, Barriuso, and
  Torralba]{zhou2019semantic}
Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso,
  and Antonio Torralba.
\newblock Semantic understanding of scenes through the ade20k dataset.
\newblock \emph{International Journal of Computer Vision}, 127:\penalty0
  302--321, 2019.

\bibitem[Zhou et~al.(2023)Zhou, Li, Li, Yu, Liu, Wang, Zhang, Ji, Yan, He,
  et~al.]{zhou2023comprehensive}
Ce~Zhou, Qian Li, Chen Li, Jun Yu, Yixin Liu, Guangjing Wang, Kai Zhang, Cheng
  Ji, Qiben Yan, Lifang He, et~al.
\newblock A comprehensive survey on pretrained foundation models: A history
  from bert to chatgpt.
\newblock \emph{arXiv preprint arXiv:2302.09419}, 2023.

\bibitem[Zhou et~al.(2022)Zhou, Loy, and Dai]{zhou2022maskclip}
Chong Zhou, Chen~Change Loy, and Bo~Dai.
\newblock Extract free dense labels from clip.
\newblock In \emph{European Conference on Computer Vision (ECCV)}, 2022.

\bibitem[Zhu et~al.(2023{\natexlab{a}})Zhu, Chen, Haydarov, Shen, Zhang, and
  Elhoseiny]{zhu2023chatgpt}
Deyao Zhu, Jun Chen, Kilichbek Haydarov, Xiaoqian Shen, Wenxuan Zhang, and
  Mohamed Elhoseiny.
\newblock Chatgpt asks, blip-2 answers: Automatic questioning towards enriched
  visual descriptions.
\newblock \emph{arXiv preprint arXiv:2303.06594}, 2023{\natexlab{a}}.

\bibitem[Zhu et~al.(2023{\natexlab{b}})Zhu, Chen, Shen, Li, and
  Elhoseiny]{zhu2023minigpt}
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.
\newblock Minigpt-4: Enhancing vision-language understanding with advanced
  large language models.
\newblock \emph{arXiv preprint arXiv:2304.10592}, 2023{\natexlab{b}}.

\bibitem[Zhu et~al.(2007)Zhu, Mumford, et~al.]{zhu2007stochastic}
Song-Chun Zhu, David Mumford, et~al.
\newblock A stochastic grammar of images.
\newblock \emph{Foundations and Trends{\textregistered} in Computer Graphics
  and Vision}, 2\penalty0 (4):\penalty0 259--362, 2007.

\bibitem[Zhu et~al.(2023{\natexlab{c}})Zhu, Hessel, Awadalla, Gadre, Dodge,
  Fang, Yu, Schmidt, Wang, and Choi]{zhu2023multimodal}
Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir~Yitzhak Gadre, Jesse Dodge, Alex
  Fang, Youngjae Yu, Ludwig Schmidt, William~Yang Wang, and Yejin Choi.
\newblock Multimodal c4: An open, billion-scale corpus of images interleaved
  with text, 2023{\natexlab{c}}.

\bibitem[Zhu et~al.(2016)Zhu, Groth, Bernstein, and Fei-Fei]{zhu2016visual7w}
Yuke Zhu, Oliver Groth, Michael Bernstein, and Li~Fei-Fei.
\newblock Visual7w: Grounded question answering in images, 2016.

\bibitem[Zong et~al.(2023)Zong, Aodha, and Hospedales]{Zong2023SSML}
Yongshuo Zong, Oisin~Mac Aodha, and Timothy Hospedales.
\newblock Self-supervised multimodal learning: A survey.
\newblock \emph{arXiv preprint arXiv:2304.01008}, 2023.

\bibitem[Zou et~al.(2022)Zou, Dou, Yang, Gan, Li, Li, Dai, Wang, Yuan, Peng,
  Wang, Lee, and Gao]{zou2022xdecoder}
Xueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li, Chunyuan Li, Xiyang
  Dai, Jianfeng Wang, Lu~Yuan, Nanyun Peng, Lijuan Wang, Yong~Jae Lee, and
  Jianfeng Gao.
\newblock Generalized decoding for pixel, image and language.
\newblock 2022.

\bibitem[Zou et~al.(2023)Zou, Yang, Zhang, Li, Li, Gao, and
  Lee]{zou2023segment}
Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li, Jianfeng Gao, and
  Yong~Jae Lee.
\newblock Segment everything everywhere all at once.
\newblock \emph{arXiv preprint arXiv:2304.06718}, 2023.

\end{thebibliography}
