\documentclass[10pt,journal,compsoc]{IEEEtran}

\usepackage[numbers]{natbib}
\usepackage{graphicx}
\usepackage[pagebackref=true,breaklinks,colorlinks,citecolor=blue,linkcolor=blue,urlcolor=blue,hypertexnames=false]{hyperref}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{bbm}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{xspace}
\usepackage{multirow}
\usepackage{tablefootnote}
\usepackage{colortbl}
\usepackage{chngcntr}
\usepackage{inconsolata}
% \hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
\usepackage{tabulary}
\usepackage{pifont}\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}

\renewcommand*\ttdefault{cmvtt}
\newcommand{\txt}[1]{{\texttt{#1}}}
% math commands
\newcommand{\loss}{\mathcal{L}}
\newcommand{\T}{\top}
\newcommand{\Prob}{P}
\newcommand{\E}{\mathbb{E}}
\newcommand{\sumN}{\sum_{i=0}^N}
\newcommand{\s}{\text{sim}}
% This is for a temporary reason, we will remove it for our final round.
\newcommand{\methodname}[1]{}

\usepackage{array}
\newcolumntype{x}[1]{>{\arraybackslash\hspace{0pt}}p{#1}}

\begin{document}

\title{Foundational Models Defining a New Era in Vision: A Survey and Outlook}

\author{Muhammad Awais, Muzammal Naseer, Salman Khan, Rao Muhammad Anwer, Hisham Cholakkal, Mubarak Shah, Ming-Hsuan Yang, Fahad Shahbaz Khan%~\IEEEmembership{Staff,~IEEE,}
        % <-this % stops a space
\IEEEcompsocitemizethanks{\IEEEcompsocthanksitem  M. Awais, M. Naseer, S. Khan, R. M. Anwer, H. Cholakkal, and F. S. Khan are with the MBZ University of AI, Abu Dhabi, UAE. \protect \\
E-mail: awais.muhammad@mbzuai.ac.ae
% <-this % stops a space
\IEEEcompsocthanksitem S. Khan and M. Naseer are also with the CECS, Australian National
University, Canberra ACT 0200, Australia.
% \thanks{R. M. Anwer is also with Aalto University, Finland}
\IEEEcompsocthanksitem  F. S. Khan is also with the Computer Vision Laboratory, Linköping
University, Sweden.
\IEEEcompsocthanksitem M. Shah is with the Center for Research in Computer Vision, University of Central Florida, Orlando, FL 32816, United States.

\IEEEcompsocthanksitem M.-H. Yang is with the University of California, Merced, CA 95344, Yonsei University, and Google Research.
}
}

% The paper headers
\markboth{}%
{}

\IEEEpubid{}


\IEEEtitleabstractindextext{%
\begin{abstract}
Vision systems to see and reason about the compositional nature of visual scenes are fundamental to understanding our world. 
%
The complex relations between objects and their locations, ambiguities, and variations in the real-world environment can be better described in human language, naturally governed by grammatical rules and other modalities such as audio and depth. 
%
The models learned to bridge the gap between such modalities coupled with large-scale training data facilitate contextual reasoning, generalization, and prompt capabilities at test time. 
%
These models are referred to as \emph{foundational models}. 
%
The output of such models can be modified through human-provided prompts without retraining, e.g.,  segmenting a particular object by providing a bounding box, having interactive dialogues by asking questions about an image or video scene or manipulating the robot's behavior through language instructions. 
%
In this survey, we provide a comprehensive review of such emerging foundational models, including typical architecture designs to combine different modalities (vision, text, audio, etc), training objectives (contrastive, generative), pre-training datasets, fine-tuning mechanisms, and the common prompting patterns; textual, visual, and heterogeneous. 
%
We discuss the open challenges and research directions for foundational models in computer vision, including difficulties in their evaluations and benchmarking, gaps in their real-world understanding, limitations of their contextual understanding,  biases, vulnerability to adversarial attacks, and interpretability issues. 
%
We review recent developments in this field, covering a wide range of applications of foundation models systematically and comprehensively. 
%
A comprehensive list of foundational models studied in this work is available at \url{https://github.com/awaisrauf/Awesome-CV-Foundational-Models}.
\end{abstract}

\begin{IEEEkeywords}
Language and Vision, Large Language Models, Self-supervised Learning, Contrastive Learning, and Masked Modeling.
\end{IEEEkeywords}}

\IEEEdisplaynontitleabstractindextext
\maketitle


\section{Introduction}
%\IEEEPARstart{I}{n} recent years, 

\IEEEPARstart{R}{ecent} years have witnessed remarkable success towards developing \textit{foundation models}, that are trained on a large-scale broad data, and once trained, they operate as a basis and can be adapted (e.g., fine-tuned) to a wide range of downstream tasks related to the original trained model~\cite{bommasani2021opportunities}. 
%
While the basic ingredients of the foundation models, such as deep neural networks and self-supervised learning, have been around for many years, the recent surge, specifically through large language models (LLMs), can be mainly attributed to massively scaling up both data and model size \cite{Zhao2023LLM}. 
%
For instance, recent models with billion parameters such as GPT-3 \cite{brown2020language} have been effectively utilized for zero/few-shot learning, achieving impressive performance without requiring large-scale task-specific data or model parameter updating.
%
Similarly, the recent 540-billion parameter Pathways Language Model (PaLM) has demonstrated state-of-the-art capabilities on numerous challenging problems ranging from language understanding and generation to reasoning and code-related tasks \cite{chowdhery2022palm,anil2023palm}.


Concurrent to LLMs in natural language processing, large foundation models for different perception tasks have also been explored in the literature recently. 
%
For instance, pre-trained vision-language models (VL) such as CLIP \cite{radford2019language} have demonstrated promising zero-shot performance on different downstream vision tasks, including image classification and object detection. 
%
These VL foundation models are typically trained using millions of image-text pairs collected from the web and provide representations with generalization and transfer capabilities. 
%
These pre-trained VL foundation models can then be adapted to a downstream task by presenting it with a natural language description of the given task and prompts.
%
For instance, the seminal CLIP model utilizes carefully designed prompts to operate on different downstream tasks, including zero-shot classification, where the text encoder dynamically constructs the classifiers via class names or other free-form texts. 
%
Here, the textual prompts are handcrafted templates, e.g., “\txt{A photo of a \{label\}}", that aid in specifying the text as corresponding to the visual image content. 
%
Recently, numerous works have also explored adding conversational capabilities to the VL models by fine-tuning them on a specific instruction set \cite{liu2023llava,zhu2023minigpt,dai2023instructblip,maaz2023videochatgpt,ye2023mplugowl}.

% Figure environment removed

Besides large VL foundation models, several research efforts have been devoted to developing large foundation models that visual inputs can prompt. 
%
For instance, the recently introduced SAM \cite{kirillov2023segment} performs a class-agnostic segmentation given an image and a visual prompt such as box, point, or mask, which specifies what to segment in an image. 
%
Such a model is trained on billions of object masks following a model-in-the-loop (semi-automated) dataset annotation setting. 
%
Further, such a generic visual prompt-based segmentation model can be adapted for specific downstream tasks such as medical image segmentation \cite{ma2023segment,wu2023medical}, video object segmentation \cite{refsam}, robotics \cite{yang2023pave}, and remote sensing \cite{chen2023rsprompter}.
%
In addition to textual and visual prompt-based foundation models, research works have explored developing models that strive to align multiple paired modalities (e.g., image-text, video-audio, or image-depth) to learn meaningful representations helpful for different downstream tasks \cite{girdhar2023imagebind,guzhov2021audioclip,Macaw-LLM}. 

%HERE
In this work, we present a systematic review of foundation models in computer vision. 
%
First, we present the background and preliminaries for foundation models briefly covering common architecture types, self-supervised learning objectives, large-scale training, and prompt engineering (Sec. \ref{sec:background}). 
%
Then, we distinguish existing works into textually prompted (Sec. \ref{sec:textually-prompted}-\ref{sec:conversationLLMs}), visually prompted (Sec. \ref{sec:visually-prompted}), heterogeneous modality-based (Sec. \ref{sec:Heterogeneous Modalities based Models}) and embodied foundation models (Sec. \ref{sec:Embodied Foundational Agents}). 
%
Within the textually prompted foundation models, we further distinguish them into contrastive, generative, hybrid (contrastive and generative), and conversational VL models. 
%
Finally, we discuss open challenges and research directions based on our analysis (Sec. \ref{sec:Open Challenges and Research Directions}). 
%
Next, we review other surveys related to ours and discuss the differences and uniqueness. 

%HERE
%\subsection{Literature Review}
%\label{subsec:Literature Review}
\noindent \textbf{Related Reviews and 
Differences.}
In the literature,  few recent works have reviewed large language models (LLMs) in natural language processing \cite{Zhao2023LLM,fan2023bibliometric,Huang2023ACL,dong2022survey,zhou2023comprehensive}. 
%
The work of~\citet{Zhao2023LLM} reviews recent advances in LLMs, distinguishing different aspects of LLMs such as pre-training, adaptation tuning, LLM utilization, and evaluation. 
%
This survey also summarizes resources available to develop LLMs and discusses potential future directions. 
%
The work of~\cite{Huang2023ACL} discusses LLMs in terms of their reasoning capabilities in performing a benchmark evaluation. 
%
A practical guide for practitioners using LLMs is presented in~\cite{Jingfeng2023LLM}, where a detailed discussion and insights are provided regarding the usage of LLMs from the viewpoint of downstream tasks. 
%
This work also analyzes the impact of pre-training, training, and testing data on LLMs. Furthermore, the work also discusses different limitations of LLMs in real-world scenarios. 
%
In the context of VLMs, the work of~\cite{Long2022VLM} performs a preliminary review of vision-language pre-trained models regarding task definition and general architecture. 
%
Similarly, \cite{Yifan2022VLM} discusses different techniques to encode images and texts to embeddings before the pre-training step and reviews different pre-training architectures. 
%
The work of~\cite{Peng2023MMT} reviews transformers techniques for multimodal data with a survey of vanilla transformers, vision transformers, and multimodal transformers from a geometrically topological perspective. 
%
In the context of multimodal learning, the recent review~\cite{Zong2023SSML} focuses on self-supervised multimodal learning techniques to effectively utilize supervision from raw multimodal data. 
%
The survey distinguishes existing approaches based on objective functions, data alignment, and architectures. 
%
The work of~\cite{Zhang2023VLMs,gan2022vision}  summarizes different vision-language pre-training network architectures, objectives, and downstream tasks and categorizes vision-language pre-training frameworks. 
%
Recently, the work of~\cite{Chunhui2023SAM} reviews the visually prompted foundation segmentation model, segmenting anything, and discusses its potential downstream tasks. 

The main differences between this survey and the aforementioned works are as follows. 
%
Unlike previous surveys that primarily focus on textual prompt-based vision-language models, our work focuses on the three different classes of foundation models: textually prompted models (contrastive, generative, hybrid, and conversational), visually prompted models (e.g., SegGPT \cite{wang2023seggpt}, SAM \cite{kirillov2023segment}) and heterogeneous modalities-based models (e.g., ImageBind \cite{girdhar2023imagebind}, Valley \cite{luo2023valley}).
%
We present the background theory behind foundation models, briefly covering from architectures to prompt engineering (Sec. \ref{sec:background}). Our work provides an extensive and up-to-date overview of the recent vision foundation models (Sec. \ref{sec:textually-prompted}, \ref{sec:visually-prompted}, \ref{sec:Heterogeneous Modalities based Models}, and \ref{sec:Embodied Foundational Agents}).
%
Finally, we present a detailed discussion on open challenges and potential research directions of foundation models in computer vision (Sec. \ref{sec:Open Challenges and Research Directions}). 

% Figure environment removed

%HERE
\section{Preliminaries}
\label{sec:background}
We first define the foundational models and scope of this survey. Then, we present a concise background overview to help readers understand the rest of the material. 
%
We focus on three main contributing factors for Foundational Models in computer vision: a) Model architecture, b) Training objectives, and c) Large-scale training and prompting. 

%HERE
\subsection{Foundational Models and Scope of the Survey}

The term “foundational models” was first introduced by \citet{bommasani2021opportunities} at Stanford Institute for Human-Centered AI. Foundational models are defined as \emph{“the base models trained on large-scale data in a self-supervised or semi-supervised manner that can be adapted for several other downstream tasks”}. The paradigm shift towards foundational models is significant because it allows replacing several narrow task-specific models with broader and generic base models that can be once trained and quickly adapted for multiple applications. It not only enables rapid model development and provides better performance for both in-domain and out-domain scenarios, but also leads to the so-called “emergent properties” of intelligence from large-scale foundational models trained on massive datasets \cite{wei2022emergent,bubeck2023sparks}. 

Computer vision has recently witnessed remarkable progress fueled by foundational models \cite{yuan2021florence, kirillov2023segment} with an extensive body of literature encompassing both discriminative and generative models. In this survey, we focus on Multimodal (vision and language) Foundational Models trained on large-scale data that can be adapted for several computer vision tasks involving non-image outputs (e.g., generated text, segmentation masks). Note that we do not cover image generative models aimed to model data distribution such as GANs, VAEs, and Diffusion models owing to dedicated surveys already existing in this area \cite{cao2022survey,zhang2023text,yang2022diffusion,croitoru2023diffusion} and because the former model class can cover a broader range of downstream applications.

\subsection{Architecture Types}
\label{sec:arch}
As depicted in Fig.~\ref{fig:arch_types}, Vision-Language (VL) models primarily use four architectural designs. We start by introducing the Dual-Encoder architecture, wherein separate encoders are utilized for processing visual and textual modalities. The output of these encoders is subsequently optimized through an objective function. The second architecture type, fusion, incorporates an additional fusion encoder, which takes the representations generated by the vision and text encoders and learns fused representations. The third type, Encoder-Decoder, consists of an encoder-decoder-based language model and a visual encoder. Lastly, the fourth architecture type, Adapted LLM, leverages a Large Language Model (LLM) as its core component, with a visual encoder employed to convert images into a format compatible with the LLM. For a more comprehensive understanding of these architectures, we refer the readers to the corresponding sections of the survey where each work is discussed. Next, we discuss the loss functions used to train different architecture types.


\subsection{Training Objectives}
\label{sec:losses}

\subsubsection{Contrastive Objectives}

To learn from unlabeled image-text data, ~\cite{radford2021learning, jia2021scaling} utilized a simple Image-Text Contrastive (\textbf{ITC}) loss which aims to learn representations by learning to predict correct image-text pairs. Given a batch of $N$ examples, ITC loss aims to match correct image-text pairs among $N\times N$ possible configurations. ITC loss maximizes cosine similarity between $N$ correct pairs and minimizes it among $N^2-N$ incorrect pairs. Let $(x_i, t_i)$ be $i$-th image-text example and $(v_i, t_i)$ be its corresponding representations, then image-to-text loss is calculated as follows, 

$$
\loss_{v2t} = -  \log \bigg[ \dfrac{\exp(\s(v_i, t_i)/\tau)}{\sum_{j=1}^N \exp( \s(v_i, t_j)/\tau) } \bigg],
$$
where $\tau$ is temprature. Text-to-image loss is also calculated similarly and the total loss is the sum of these two terms, 
$
\loss_{ITC} = \dfrac{1}{N} \sum_{i=1}^N [\loss_{v2t} + \loss_{t2v}]
$. 

Image-Text Matching (\textbf{ITM}) loss~\cite{li2021align} aims to correctly predict whether a pair of images and text is positively or negatively matched. A few perceptron layers are added which predicts the probability $p^{itm}$ of a pair being matched. The loss is then calculated based on cross-entropy loss. 

Similar to ITC and ITM, several contrastive losses have also been utilized in the subsequent papers. These losses include image-based self-supervision losses (i.e. Simple Contrastive Learning of Representations (\textbf{SimCLR})~\cite{chen2020simple, chen2020big}) and variants of ITC loss (i.e. \textbf{FILIP Loss}~\cite{yao2021filip}, Text-to-Pixel Contrastive (\textbf{TPC}) Loss~\cite{wang2022cris}, Region-Word Alignment (\textbf{RWA})~\cite{li2022grounded}, Multi-label Image-Text Contrastive (\textbf{MITC})~\cite{xu2022groupvit}, Unified Contrastive Learning (\textbf{UniCL})~\cite{yang2022unified}, Region-Word Contrastive (\textbf{RWC}) Loss \cite{zhang2022glipv2}).


\subsubsection{Generative Objectives}


Masked Language Modeling (\textbf{MLM}) loss~\cite{lu2019vilbert} is a bi-directional, non-casual language modeling loss that aims to reconstruct masked-out tokens. Let's assume $\hat{x}^t$ to be masked input tokens where a certain percentage of tokens are randomly masked and replaced with some special token of choice. MLM aims to model $x^t$ given masked tokens, 
$$
\loss_{\text{MLM}} = - \E_{x^t \sim D} \big[ \log p(x^t | \hat{x}^t)  \big].
$$

Language Modeling (\textbf{LM}) loss aims to model language generation in an auto-regressive manner. It models the prediction of the current token ($l$-th) given previous tokens ($<l$),
% to maximize likelihood of text in an auto-regressive manner. 
\begin{align*}
    \loss_{\text{LM}} = - \E_{x^t \sim D} \bigg[ \sum_{l=1}^L \log p(x^t_l | x^t_{<l}) \bigg], 
\end{align*}

where $L$ is the total number of tokens. 

\noindent Standard Captioning (\textbf{Cap}) loss~\cite{hossain2019comprehensive} also aims to predict the next token given previous tokens and image ($x^v$), 
$$
\loss_{\text{Cap}} = - \E_{x \sim D} \sum_{l=0}^L \log p(x^t_l | x^t_{<l}, x^v), 
$$
where $x = [x^v, x^t]$. 
Similarly, \textbf{Flamingo} Loss by \citet{alayrac2022flamingo} is also about the prediction of $l$-th token given previous image and text tokens.
This is different from captioning loss as tokens consist of several interleaved image and text inputs. 
Prefix Language Modeling (\textbf{PrefixML})~\cite{wang2021simvlm} extends language to vision-language modeling loss. It considers images to be a prefix of textual descriptions as they appear before images on the web, and therefore appended image tokens to text tokens $x = [x^v, x^t]$. Then, a prefix sequence of randomly selected length ($L_p$) is truncated from text tokens reconstructed through, 
\begin{equation*}
\begin{aligned}
 \loss_{\text{PrefixLM}} & = - \E_{x \sim D}\big[ \log p_{\theta}(x_{\geq L_p} | x_{(x<L_p)}\big] \\
 & = - \E_{x \sim D} \bigg[ \sum_{l=L_p}^L \log p_{\theta} (x_l | x_{[L_p, l]}, x_{<L_p}) \bigg],
\end{aligned}
\end{equation*}

$x_l$ represents current token, $x_{[L_p, l]}$ is the prefix sequence, and $x_{<L_p}$ is previous sequence. 
  
Similarly, several other generative losses have also been proposed. Some examples include Masked Multimodal Modeling (\textbf{MMM}) loss~\cite{singh2022flava}, Semi Casual Language Modeling (\textbf{SemiCasualLM}) \cite{hao2022language}, Image-conditioned Masked Language Modeling (\textbf{IMLM}) loss \cite{zhang2023toward}, Image-grounded Text Generation (\textbf{ITG}) loss~\cite{li2023blip}, Masked Image Modeling (\textbf{MIM})~\cite{zhang2023toward} and Captioning with Parallel prediction (\textbf{CapPa})~\cite{tschannen2023image}. 


\subsection{Large-scale Training}
\label{sec:data}
Large-scale training followed by effective prompting at inference has been a crucial ingredient of vision and language foundational models. We discuss here the role of pre-training, finetuning, and prompting techniques (see Tab.~\ref{tab:datasets}).

\begin{table}[]
    \centering
    \setlength{\tabcolsep}{12pt}
    \resizebox{\columnwidth}{!}{%
    \begin{tabular}{lr}
    \toprule
        Data Type  & Examples \\
    \toprule
        \multicolumn{2}{c}{\textbf{Pre-training} (Sec.~\ref{sec:pre_training})} \\
        \midrule
        Image-Text  & WIT~\cite{radford2021learning}, LAION~\cite{schuhmann2021laion, schuhmann2022laion} \\
        w/ Pseudo Labels  & Cap24M~\cite{li2022grounded}, SA-1B~\cite{kirillov2023segment} \\
        Benchmark Combination & UNITER~\cite{chen2020uniter}, PMD~\cite{singh2022flava} \\
        \midrule
        \multicolumn{2}{c}{\textbf{Fine-tuning} (Sec.~\ref{sec:fine_tuning})} \\
        \midrule
        Task Specific &  ImageNet~\cite{fei2009imagenet}\\
        Capability Specific & OWL-ViT~\cite{minderer2022simple} \\
        Instruction-Following &  InstructBLIP~\cite{dai2023instructblip}\\
        \midrule
        \multicolumn{2}{c}{\textbf{Prompt Engineering} (Sec.~\ref{sec:prompt_engineering})} \\
        \midrule
        Train Time  & GLIP~\cite{li2022grounded} \\
        Evaluation Time  & CLIP~\cite{radford2021learning}\\
        \bottomrule
    \end{tabular}}
    \vspace{0.5em}
    \caption{An overview of different settings under which datasets are utilized for training, fine-tuning, and prompting in foundational models. More details are discussed in Sec.~\ref{sec:data}.}
    \label{tab:datasets}
\end{table}

\subsubsection{Pre-training Data}
\label{sec:pre_training}
Large-scale data is at the heart of modern vision-language foundational models. The datasets that have been utilized to pre-train these models can be divided into three broad categories: image-text datasets (such as WebImageText used in CLIP~\cite{radford2021learning}), partially synthetic datasets (such as SA-1B used in SAM~\cite{kirillov2023segment}), and Combination dataset (such as PMD used in FLAVA~\cite{singh2022flava}). Here, we discuss these categories briefly. 

% ImageText Data
\noindent \textbf{Image-Text Data: } CLIP~\cite{radford2021learning} showed remarkable effectiveness of web-scale image-text data to pre-train foundational models. This type of data is often combed through from web crawls (e.g., CommonCrawl~\footnote{https://commoncrawl.org}). The final dataset is the result of a filtering process that is applied to remove noisy, useless, or harmful data points. Numerous subsequent works collected similar datasets such as ALIGN1.8B~\cite{jia2021scaling}, RUC-CAS-WenLan~\cite{huo2021wenlan}, FLD900M~\cite{yuan2021florence}, FILIP300M~\cite{yao2021filip}, WebLi~\cite{chen2022pali}, etc). However, these datasets are not public. To make large-scale training more accessible, several open-source curation efforts have contributed significantly to the community such as LAION~\cite{schuhmann2021laion, schuhmann2022laion} and COYO-700M~\cite{kakaobrain2022coyo-700m}. 

\noindent \textbf{Partially Pseudo Labels-based Data: } 
Similar to image-text models, visual grounding can also benefit from the large-scale training data but such datasets are not available on the web. Collecting grounding datasets is also costly as they require significant human annotation effort. One cost-effective method is to leverage a good teacher to convert image-text datasets into mask-description datasets. This strategy was first adopted by GLIP~\cite{li2022grounded}, however, \citet{kirillov2023segment} took it to a billion scale with SA-1B. The process of curation often involves training a good teacher for the generation of masks and then utilizing it on an Image-Text dataset along with NLP parsers. These datasets include SAM, GLIP, and KOSMOS-2. GLIP~\cite{li2022grounded} trained a teacher GLIP on human-annotated LVIS~\cite{gupta2019lvis} and Visual Genome~\cite{krishna2017visual} datasets and then utilize it to predict boxes for image-text data with noun phrases detected by an NLP model. GRIT used in KOSMOS-2~\cite{peng2023kosmos2} is also prepared in a similar fashion. SAM~\cite{kirillov2023segment} introduced a data engine that consists of three stages (assisted manual, semi-automatic, and fully automatic stage). They generated one billion high-quality masks with this process. 

% Figure environment removed

\noindent \textbf{Combination of Datasets: } It is not always possible to curate and train on web-scale datasets. To circumvent this problem, several works~\cite{chen2020uniter, tsimpoukelli2021multimodal, xu2022unifying} have utilized a combination of benchmark vision datasets. These works combine datasets that have image-text pairs such as captioning and visual questioning answering, etc. Some works have also used non-image-text datasets and used template-based prompt engineering to convert labels into descriptions. Moreover, visual grounding-related works have also utilized grounding datasets such as COCO~\cite{lin2014microsoft}, OpenImages~\cite{krasin2017openimages}, Objects365~\cite{shao2019objects365}. 


\subsubsection{Fine-tuning}
\label{sec:fine_tuning}
Fine-tuning is employed under three primary settings: to improve a model's performance on a specific task (e.g., open-world object detection), to improve a model for a certain capability (e.g., visual grounding), and to instruction-tune a model to make it solve different downstream vision tasks (e.g., InstructBLIP~\cite{dai2023instructblip}). Firstly, a model's performance for a specific task can be improved even if only a linear layer is fine-tuned. Hence, task-specific datasets (e.g., ImageNet) can be used to improve pre-trained models for specific tasks. Second, some works have utilized a pre-trained vision-language model for grounding tasks by fine-tuning the model on grounding datasets. For instance, \citet{minderer2022simple} fine-tuned vision transformer on detection datasets to create an open vocabulary object detector. Finally, some works (such as InstructBLIP~\cite{dai2023instructblip}) transformed vision datasets into instruction-tuning datasets to enable VL models to work for downstream tasks. 



\subsubsection{Prompt Engineering}
\label{sec:prompt_engineering}
Prompt engineering has primarily been used with Large Language Models (LLMs) to make them do certain tasks~\cite{brown2020language, gao2021making}. In the context of vision-language models or visually-prompted models, prompt engineering is predominately used for two purposes: to convert vision datasets to image-text training data (e.g., CLIP for image classification) to provide human intractability to the foundational models, and use vision-language models for vision tasks. Most vision datasets consist of images and corresponding one-word labels. To leverage vision-language models on vision datasets, several works have utilized template-based prompt engineering. In this prompt engineering, a set template is used to generate a description from the label. For instance, \txt{`image of a \{label\}', `a type of \{type\}'}. As noted by \cite{radford2021learning,yuan2021florence}, additional context helps the model, hence, these text prompts can be utilized by vision-language models during training or evaluation. 


With this context into architecture types, objectives, and data used for training foundational models in vision, next we explain their main classes i.e., textually prompted (Sec. \ref{sec:textually-prompted} and \ref{sec:conversationLLMs}) and visually prompted (Sec. \ref{sec:visually-prompted}) models as well as heterogeneous (Sec. \ref{sec:Heterogeneous Modalities based Models}), generalist (Sec. \ref{subsec:Generalist Models}) and embodied (Sec. \ref{sec:Embodied Foundational Agents}) foundational models (see Fig. \ref{fig:categorization} for a taxonomy of vision-language foundation models). 



\section{Textually Prompted Models}
\label{sec:textually-prompted}

\input{tables/information_textually_prompted}

Traditionally, vision-language models have primarily been employed for tasks that necessitate the joint understanding of both visual and textual modalities. However, with the remarkable performance exhibited by CLIP, language supervision-based models have gained significant prominence and have become the mainstream approach. In this section, we focus on exploring methods that rely on language as their primary source of supervision. These textually prompted models are broadly categorized into three main types based on their training objectives: contrastive, generative, and hybrid approaches. We discuss contrastive-based methods in Sec.~\ref{sec:contrastive}, generative-based methods in Sec.~\ref{sec:generative}, and combination methods in Sec.~\ref{sec:hybrid}. We provide an overview of these methods in Tab.~\ref{tab:textually_prompted_models}. We also exhibit a comparison of these models for a representative set of tasks in Tab.~\ref{tab:results_textually_prompted}.



\subsection{Contrastive Learning (CL)}
\label{sec:contrastive}
SOTA computer vision models are trained to predict a set of pre-determined categories which restricts their generalization and usability. Conventionally, most deep learning methods have used supervised pre-training such as training on ImageNet~\cite{fei2009imagenet}, and weak supervision such as training on hash-tags of images~\cite{mahajan2018exploring}. \citet{radford2021learning} argued for learning perception from the visual concepts present in the natural language and proposed Contrastive Language Image Pre-training (CLIP). In this section, we discuss CLIP and subsequent contrastive-based approaches. We have divided them into two parts: contrastive approaches for general purpose models (Sec.~\ref{sec:cl_general}) and approaches for visual grounding foundational models (Sec.~\ref{sec:cl_grounding}). We illustrate CLIP architecture and its main variants in Fig. \ref{fig:clip_variants}.


\subsubsection{CL for General Purpose Foundational Models}
\label{sec:cl_general}
In this section, we explain contrastive methods that aim to train general-purpose vision-language foundational models. The mainstream traction of these approaches started with CLIP~\cite{radford2021learning}, however, many subsequent efforts have provided better ways to utilize datasets, proposed modified architectures and training methods, expanded its utility, reproduced it, and studied its properties and scaling laws. We describe these methods here.  


\noindent\textbullet \textbf{Methods Solely based on CL.}
\methodname{CLIP~\cite{radford2021learning}: }\citet{radford2021learning} proposed jointly training an image and text encoder on the contrastive pre-training task of the correct pairing of images and their captions in a batch. 
The \textbf{CLIP} model consists of an image encoder (a ViT or a scaled CNN) and a text encoder (a GPT-like transformer~\cite{brown2020language}). These encoders produce a multi-modal embedding space for $N$ image-text pairs. 
The CLIP is trained to minimize the cosine similarity of embeddings of $N$ correct image-text pairs and maximize the cosine similarity of embeddings of $N^2-N$ incorrect pairs via a symmetric cross-entropy loss.
One main motivation of CLIP-framework is the scale of natural language supervision data. To train models at scale, authors also curated a 400 million image-text pairs dataset from the internet. The CLIP framework shows excellent performance when trained on such large-scale datasets. CLIP shows good zero-shot generalization, has significantly higher robustness to natural and synthetic distribution shifts, and works well with linear probe-based fine-tuning. 

\methodname{ALIGN~\cite{li2021align}:} Visual-language dataset used by \citet{radford2021learning} require non-trivial and computationally expensive pre-processing and cleaning, thereby limiting the scale of the dataset. Instead of applying these pre-processing steps, Jia et al. in \textbf{ALIGN}~\cite{jia2021scaling} collected one billion noisy image-caption datasets curated from Conceptual Captions Dataset~\cite{sharma2018conceptual}. They trained dual encoder-based architecture with CLIP-like normalized contrastive objectives on this dataset. To align visual and language embeddings, cosine similarity of image-text embeddings are optimized via. normalized softmax loss~\cite{zhai2018classification}. The authors showed that the scale of the dataset can make up for the noisy nature of it. The resulting aligned image-text representations show excellent performance for cross-modal matching/retrieval tasks and zero-shot classification. 

% Florence
\methodname{Florence~\cite{yuan2021florence}: } \citet{yuan2021florence} argued that a truly foundational model should work for Space-Time-Modality space. Specifically, a foundational model should be able to handle representation from coarse to fine (Space), static to dynamic (Time), and from RGB to multi-modalities (Modality). To achieve this level of generalizability, they introduced \textbf{Florence} model that starts with CLIP-like pre-training on the large curated dataset and uses improved contrastive objective, and efficient training. A pre-trained model is then extended to have three different adapter heads for each space. The Dynamic DETR-based adapter learns representation for fine-grained dense tasks with large-scale object detection datasets. Similarly, a METER~\cite{dou2022empirical} head is used for vision language representation, and CSwin~\cite{dong2022cswin} is used for video-based understanding. This framework results in a foundational model that generalizes across domains.

% Figure environment removed 

Most vision-language methods are focused on language supervision and have overlooked the role of the visual part. \methodname{SLIP~\cite{mu2022slip}: } \citet{mu2022slip} investigated whether image-based self-supervised learning can help language supervision frameworks. To this end, authors proposed \textbf{SLIP} that adds an adaptation of SimCLR~\cite{chen2020simple, chen2020big} loss for self-supervision based on different views or augmentation of the input image. The authors trained the CLIP-like model on the YFCC15M dataset and showed that SLIP performs better compared with either language supervision or self-supervision alone on a battery of tasks including zero-shot and linear probe-based methods. 

% WenLan
\methodname{WenLan~\cite{huo2021wenlan}: }Most text-image-based methods assume a strong semantic correlation between the pair. However, web-scale data is littered with pairs that have weak correlations (e.g., captions that do not reflect images accurately). \citet{huo2021wenlan} propose \textbf{WenLan} to solve this by using two-tower architecture and cross-modal contrastive learning based on MoCo~\cite{he2020momentum}, which can leverage more negative samples in limited GPU resources. This strategy leverages both negative and positive examples and text-to-image and image-to-text-based contrastive losses. This results in a better model that is also efficient to train and shows improved performance on many tasks. They also curated the first large-scale Chinese image-text dataset with 500 million data points. They trained their proposed model to solve Chinese-language tasks and demonstrated its superior zero-shot ability. 

% FILIP
\methodname{FILIP~\cite{yao2021filip}:} CLIP-like methods use separate encoders for each modality which makes them inference efficient as each encoder can be decoupled and pre-computed representations can be utilized. However, such models rely solely on global features for cross-modal interaction which makes it hard for them to capture finer-level information between modalities. \citet{yao2021filip} proposed a cross-modal late interaction approach to model the token-wise cross-modal interaction which helps capture fine-grained semantic alignment. The proposed \textbf{FILIP} (Fine-grained Interactive Language Image Pre-training) loss maximizes the similarity between token-wise visual and text embeddings. Specifically, similarity for each input visual token with all text-tokens is calculated, and maximum similarity is used. Similarly, the maximum similarity of each text token is also calculated. Then, a simple average is used to calculate the overall loss. This means each image token's closest text token is used. Similarly, each text's closest image patch is also used. This helps in modeling fine-grained interaction between the two modalities without sacrificing the inference-efficient nature of CLIP. Moreover, the authors also collected 340M large image-text pairs to train their model. Their method outperforms CLIP and other methods on zero-shot classification as well as for image-text retrieval tasks.

\noindent\textbullet \textbf{Masked Contrastive Learning. }
% FLIP
\methodname{FLIP~\cite{li2023scaling}: }Inspired by the Masked Auto Encoders~\cite{he2022masked}, \citet{li2023scaling} presented an efficient alternative of CLIP called \textbf{FLIP} which masks 50-75\% of input pixels in CLIP training. This masking scheme reduces computation by 2-4$\times$, allows 2-4$\times$ larger batches, and improves accuracy. FLIP is $> 3\times$ faster to reach the same accuracy as CLIP. Compared with the CLIP baseline, their method can save ~1800 TPU days. Based on this faster method, they also studied the scaling in CLIP across models, datasets size, and training length. 

% MaskCLIP
\methodname{MaskCLIP~\cite{dong2023maskclip}: } \citet{dong2023maskclip} argued that the language description of an image can not express complete information as an image is a continuous and fine-grained signal. To fully leverage images in contrastive vision-language training, they proposed \textbf{MaskCLIP} that randomly masks the input image along with mean teacher-based self-distillation~\cite{tarvainen2017mean} to learn local semantic features. Specifically, the representation of the whole image and masked image are obtained from the mean teacher and student respectively, and cross-entropy loss between the two representations is minimized. Similarly, BERT~\cite{lu2019vilbert} pre-training is used in the language encoder. These two modifications in the contrastive learning framework help the model learn local and fine-grained semantics. MaskCLIP significantly improves CLIP in zero-shot, linear probe, and fine-tuning settings on several vision datasets.

% EVA-CLIP
\methodname{EVA-CLIP~\cite{sun2023eva}: } While the previous approaches in this section mainly focus on the efficiency aspect of CLIP via masking, \textbf{EVA-CLIP} addresses instability and optimization efficiency together with masking visual inputs. 
Specifically, \citet{sun2023eva} offered solutions to improve the stability of training and reduce computational costs, including improved initialization, better optimizer, and random masking of images~\cite{li2023scaling}. Their efficient solution-based model, EVA-CLIP, performs better and is trained on the version of datasets that are curated from open-source resources.
\methodname{EVA~\cite{fang2023eva}: }\citet{fang2023eva} supplemented this effort to scale models. They masked out image-text inputs along with CLIP loss to scale the model to 1 billion parameters. Their scaled model, named EVA, performs strongly on several downstream tasks, including COCO, LVIS, ImageNet1k, etc.


\noindent\textbullet \textbf{Scaling and Reproducing CLIP. }
OpenAI released pre-trained weights and code for CLIP. However, they did not release the training mechanism and dataset which limits the ability to study it. To increase the accessibility, several subsequent works have open-sourced large-scale image-text datasets, reproduced CLIP, and studied its properties. 
% LAION-400M 
\methodname{LAION-400M dataset~\cite{schuhmann2021laion}: } The excellent performance of CLIP hinges on the large-scale image-text dataset, which is not publicly available. To address this problem, \citet{schuhmann2021laion} released LAION-400M, an image-text dataset consisting of 400 million data points curated after filtering common crawl. 
% LAION-5B
\methodname{LAION-5B dataset~\cite{schuhmann2022laion}: } \citet{schuhmann2022laion} further scaled it up and released a multilingual, multi-modal dataset called LAION-5B which contains 5.8 billion data points curated from Common Crawl after filtering through existing CLIP model. 
% OpenCLIP
\methodname{OpenCLIP~\cite{ilharco_gabriel_2021_5143773, cherti2023reproducible}: } Utilizing large-scale LAION datasets~\cite{schuhmann2021laion, schuhmann2022laion}, \textbf{Open-CLIP} \cite{ilharco_gabriel_2021_5143773} trained and reproduced CLIP training experiments and studied its properties. 
% OpenCLIP Scaling Laws
\citet{cherti2023reproducible} supplemented this open-source effort by studying the scaling laws of CLIP. The trained OpenCLIP on LAION-5B~\cite{schuhmann2022laion} dataset demonstrated consistent improvements in performance as data, model, and compute are scaled. They also observed some divergence of scaling from the OpenAI's CLIP~\cite{radford2021learning} and postulated that the difference arises because of the difference in training distributions. 

\methodname{CLIPA~\cite{li2023inverse}: } It is well known that the performance of CLIP scales with model and dataset sizes~\cite{radford2021learning, cherti2023reproducible}. \citet{li2023inverse} revealed a surprising finding: larger image-text models allow the use of smaller token sizes during the training without significant sacrifice in accuracy. Based on this finding, called \emph{inverse scaling law}, they introduced a new efficient training recipe and  CLIP-like model trained on academic-scale resources and dubbed it \textbf{CLIPA}. CLIPA can achieve 63.2\%, 67.8\%, and 6.9.3\% zero-shot ImageNet accuracy in 2, 3, and 4 days of training on 8 A100 GPUs, respectively. 
% CLIPv2
\methodname{CLIPv2~\cite{li2023clipav2}: } Building on the \emph{inverse scaling law} observed by CLIPA~\cite{li2023inverse}, \citet{li2023clipav2} trained CLIP-like model on a large scale with significantly less computational budget and training costs. With their large-scale training, they demonstrated two interesting results. First, they showed that the inverse scaling law is also applicable to fine-tuning: models can be fine-tuned on fewer input tokens. Second, larger models exhibit a smaller performance drop compared with smaller models when fine-tuned with the same number of input tokens. Their trained CLIPA model achieves 69.3\% zero-shot ImageNet classification accuracy in 4 days of training on 8 A100 GPUs. Several works have explored CLIP and contrastive methods from different prespectives~\cite{liu2023remoteclip, liu2023stone, kim2023cream}

\subsubsection{CL for Visual Grounding Foundational Models}
\label{sec:cl_grounding}
% general paragraph
CLIP and its variants have shown impressive performance for tasks that require global information (e.g., classification and image-text retrieval). However, they perform poorly on localization tasks that need fine-grained, pixel, and region-level information. 
% \cite{Hanoona2022Bridging}.
We illustrate two failure cases obtained from \citet{zhong2022regionclip,ghiasi2021scaling} in Fig.~\ref{fig:clip_error_localization}. In this section, we discuss foundational models that are designed to leverage contrastive learning for visual grounding tasks. 

% Figure environment removed


\noindent\textbullet \textbf{CLIP-adaptation for Grounding: }
% LSeg, MaskCLIP, RegionCLIP, CRIS, OWL-ViT
\methodname{MaskCLIP~\cite{zhou2022maskclip}: } 
MaskCLIP by \citet{dong2023maskclip} model earlier investigated masked self-distillation for contrastive learning. Different f that, \textbf{MaskCLIP} by \citet{zhou2022maskclip} proposes to use the CLIP model for dense prediction with minimal changes. To this end, they proposed to extract dense features from the vision encoder and use text embeddings for classification. To further enhance dense predictions, they also proposed to train a backbone for classification. Their method shows a reasonable performance of CLIP for localization tasks. 

% RegionCLIP
\methodname{RegionCLIP \cite{zhong2022regionclip}: } 
\citet{zhong2022regionclip} proposed \textbf{RegionCLIP} that extends CLIP to explicitly align image regions and their textual descriptions for object detection. Its training consists of three phases: a CLIP-based image-text pre-training, a CLIP-like region-text contrastive training, and object-detection-specific fine-tuning. Since large-scale region-description datasets are not widely available, authors utilized region-class names, prompt templates, and a pre-trained CLIP to bootstrap a dataset. Specifically, they used a pre-trained teacher encoder to extract regions. All class labels are converted into phrases following simple prompt templates and a teacher language encoder is utilized to get corresponding embeddings. The matching score between image features and class embeddings is calculated and the highest score pair is used as a pseudo-region-text pair. The authors pre-trained their dual-encoder-based model on these pseudo-region-description pairs. Finally, they also proposed a simple fine-tuning method to mitigate the noisy nature of region-text pairs. For task-specific finetuning, the visual encoder is used as a base network initiated from a pre-trained ViT. An off-the-shelf region proposal network (RPN) localizes objects and the language encoder's embeddings are used to obtain object categories. RegionCLIP has zero-shot capabilities and when transferred, established a new SOTA for open-vocabulary object detection. 

% CRIS: but this only solves referring image segmantiaont
\methodname{CRIS \cite{wang2022cris}: } \citet{wang2022cris} extended CLIP for referring image segmentation task~\cite{hu2016segmentation, ye2019cross} and proposed CLIP-Driven Referring Image Segmentation (\textbf{CRIS}). Referring image segmentation task aims to segment a region of the image based on an input text prompt~\cite{hu2016segmentation}, and hence a natural fit for a CLIP-like framework. 
However, CLIP is not designed for learning pixel-level information as it is focused on global features. 
\citet{wang2022cris} proposed two modifications in the CLIP framework to make it learn pixel-level information. First, a visual-language decoder is introduced to capture long-range dependencies. Second, a text-to-pixel contrastive loss is introduced to align textual features with the corresponding pixel-level features. CRIS outperforms previous SOTA for three referring image segmentation tasks.  

% GLIP, Grounding DINO, Group ViT and OpenSeg
\noindent\textbullet \textbf{Direct localized Visual-Semantic Alignment: } Instead of adapting CLIP for grounding tasks, some works leveraged strong pre-trained specialized models and modified them to add language-vision modeling through contrastive learning.
% GLIP
\methodname{GLIP \cite{li2021align}: } Phrase grounding is the task of identifying phrases in the input text for corresponding regions of an image. \citet{li2022grounded} argues that phrase grounding is a scalable and effective pretraining task for object detection, and hence reformulated the object detection task to phrase grounding. This provides benefits for both tasks: phrase grounding provides better visual concepts and object detection provides more annotations for bounding boxes. They proposed Grounded Language Image Pretraining (\textbf{GLIP}) which trains a dual vision-language encoder-based architecture with fusion layers on phrase-region dataset. To train models at scale, a pre-trained grounding model is applied to the image-text datasets to get phrase-region pseudo labels. To use GLIP models for object detection datasets, all the class names are combined in one sentence, and the model is prompted to output the correct class associated with a region. This simple scaling approach brings significant improvements for 14 downstream tasks and its fine-tuned version sets a new SOTA on the COCO dataset. 


\methodname{Grunding DINO~\cite{liu2023grounding}: } Instead of extending CLIP framework, \citet{liu2023grounding} proposed to ground state-of-the-art transformer-based object detector DINO~\cite{caron2021emerging} with language pre-training for open-set generalization, hence named \textbf{Grounding-DINO}. To this end, they partitioned the closed-set object detector into three parts consisting of a backbone, a neck, and a head and fused language features at each level. A text and image backbone is utilized to extract multi-scale features that are fed to the neck. The text and image features produced by the neck are then used to create language-guided query selection. These cross-modal queries along with image and text features are fed to a cross-modality decoder which has image and text cross-attention and an FFN layer. The model is trained end-to-end with a contrastive loss between predicted objects and language tokens as well as task-specific losses such as L1 loss, Grounded Intersection over Union (GIOU) loss~\cite{rezatofighi2019generalized}, and focal loss~\citet{lin2017focal}. Grounding DINO outperforms GLIP and other competitors for closed-set, open-set, and referring object detection by significant margins. 

% OWL-ViT
\methodname{OWL-ViT \cite{minderer2022simple}: } \citet{minderer2022simple} introduced a CLIP-based training recipe for open vocabulary object detection called \textbf{OWL-ViT}. Their proposed training method consists of two phases: a CLIP-like pre-training for learning image-level features and a fine-tuning stage for object-level features for open-vocabulary object detection. Their dual encoder architecture is similar to CLIP except for task-specific modifications. Specifically, the output of a ViT-based image encoder consists of a projection layer for classification embeddings and an MLP head for box predictions and respective probabilities. To enable open vocabulary detection, the language encoder produces text embeddings (queries) based on input prompts which can be different for each image. The visual encoder's role, then, is to predict the bounding box and probabilities with which the query is applied to it. This dual-encoder architecture is first trained with CLIP-like contrastive learning and then fine-tuned on object detection datasets with bipartite matching loss~\cite{carion2020end} adapted for long-tailed/open vocabulary object detection. 


% OpenSeg 
\methodname{OpenSeg \cite{ghiasi2021scaling}: } \citet{ghiasi2021scaling} argued that CLIP-like methods perform poorly for localization tasks because they do not group first and hence lose local information. To solve this issue, they propose \textbf{OpenSeg} that performs visual-semantic alignment after the grouping. Their method involves learning segmentation masks, visual-semantic alignment of these masks, and generating pseudo masks for large-scale pre-training. Their model represents an image with segmentation masks which enables segmentation-based weakly supervised learning along with region-word grounding. This type of training requires segmentation labels and therefore difficult to scale. To solve the scaling problem, the authors followed MuST~\cite{ghiasi2021multi} and first trained the model on segmentation data with only segmentation loss. This model is used to generate pseudo labels for image-text pairs. The openSeg-based model can generalize well for new datasets and outperforms previous SOTA on several benchmarks. 

% GroupViT
\methodname{GroupViT \cite{xu2022groupvit}: } \citet{xu2022groupvit} proposed to leverage visual grouping mechanism~\cite{tu2005image, zhu2007stochastic} to get semantic segmentation with only language supervision. To this end, they proposed a hierarchical Grouping Vision Transformer (\textbf{GroupViT}) as an image encoder along with the standard CLIP-like language encoder. The proposed GroupViT has multiple grouping layers that learn to group regions of an image into progressively larger segments of similar visual concepts by learning segment tokens. Each stage also consists of transformer layers that aggregate segment tokens from smaller groups from previous stages to progressively larger segments. An overview of their architecture is shown in Fig.~\ref{fig:groupvit}. The GroupViT is trained with Image-Text contrastive (ITC) loss and multi-label contrastive loss with prompt engineering, which uses prompts to create multiple descriptions of a single image. For zero-shot segmentation, the segment token in the last layer corresponds to an arbitrarily shaped segment whose class can be determined by finding a class label that has maximum similarity with the segment token. GroupViT performs competitively compared to specialized SOTA methods without requiring any supervision. Similarly, ODISE \cite{xu2023open} is also an open vocabulary segmentation model that utilizes pre-trained diffusion features.


% Figure environment removed


\subsection{Generative Learning}
\label{sec:generative}
% A para on a general description
Large Language Models (LLMs) have shown impressive zero and few-shot performance for NLP tasks. However, these LLMs lack vision modality and only recently multimodal models have been trained with vision and language modalities. Contrastive vision-language models have also shown good generalization capabilities but they can only address limited problems since they provide a similarity score between text and image. Here, we describe works that aim to equip LLMs with eyes to see the world by training them on vision-conditioned language generation tasks. 

\noindent\textbullet \textbf{In-context Learning with Multimodal Inputs: }
Large language models are excellent few-shot learners~\cite{brown2020language} but in their conventional form, they are blind to the visual modality. Here, we explain methods that aim to endow LLMs with visual modality using interleaved image-text data. 

% Frozen 
\methodname{Frozen~\cite{tsimpoukelli2021multimodal}:}  \citet{tsimpoukelli2021multimodal} proposed \textbf{Frozen}, an efficient approach to add visual modality in the LLMs without updating their weights. Frozen consists of an image encoder that encodes input images to the word embedding space of LLMs such that these LLMs can generate image captions. To learn joint embeddings, LLM is kept frozen and the vision encoder is trained on captioning datasets with the task of the conditional generation of caption given an image.
Although Frozen is trained on single image-text pairs, it can work with an ordered set of multiple image-text pairs enabling it to do few-shot tasks. At inference, the LLM encoder and vision encoder are prompted with the ordered textual and visual prompts. The textual and visual embeddings are concatenated and fed to the decoder of the LLM, which generates a textual output autoregressively. Frozen has demonstrated few-shot visual-language capabilities across vision-language tasks.

% Flamingo
\methodname{Flamingo~\cite{alayrac2022flamingo}:} Similar to Frozen, \citet{alayrac2022flamingo} also aimed to build models that can adapt to new tasks using only a few examples. For this purpose, they proposed a new family of \textbf{Flamingo} models that leverage fixed pre-trained vision and language models along with a Perceiver Resampler-based bridge. The Perceiver Resampler connects the visual encoder to LLM by producing a fixed number of visual tokens. These visual tokens conditions LLM's output using gated cross-attention dense blocks that are interleaved between LM's layers. These new layers provide an efficient way for the LLM to incorporate visual information and are trained with next-token prediction tasks conditioned on preceding text and a set of images or videos.

Flamingo models can handle large image and video inputs since Perceiver Resample converts varying size visual inputs to a few visual tokens. During inference, interleaved support examples,  (image, text) or (video, text), are followed by a query visual input fed to the model for computation. Flamingo shows excellent few-shot performance for several vision-language tasks, even surpassing state-of-the-art for fine-tuned models despite requiring significantly fewer annotated examples. 
% OpenFlamgino
\methodname{OpenFlamingo~\cite{anas_awadalla_2023_7733589}:} \citet{anas_awadalla_2023_7733589} aimed to build an open-source version of the Flamingo model called \textbf{OpenFlamingo}. They mostly followed Flamingo's original architecture but trained it on a new Multimodal C4 dataset and 10M samples from LAION-2B and released open-source checkpoints. Their models utilized LLaMA-7B and CLIP's visual encoder and achieve 80\% performance of the respective Flamingo model.

\noindent\textbullet \textbf{LLMs as a General Interface for other Modalities: }
\citet{hao2022language} proposed to utilize language models as a universal task layer and to dock other modalities to it through the pre-trained encoders. Here, we describe these methods.
% first kosmos: interface modalities with LMs
\methodname{MetaLM~\cite{hao2022language}: }\citet{hao2022language} proposed \textbf{MetaLM}, a semi-casual model that consists of a unidirectional transformer decoder and multiple bi-directional encoders that are connected with the decoder through the connector layers. In this way, MetaLM enjoys excellent zero and few-shot capabilities of the casual language models~\cite{brown2020language} and better transferability of non-casual encoders~\cite{devlin2018bert,raffel2020exploring}. The authors propose to jointly train encoders and decoders on a new semi-casual language modeling objective which learns to generate the next word given the previous tokens and encoded representations. This joint framework inherits in-context learning, instruction following, and fine-tuning abilities. To understand the capabilities of MetaLM a multitude of experiments are performed. On NLP tasks, it outperforms GPT on a multitude of multi-task fine-tuning, single-task fine-tuning, instruction-tuned zero-shot, and in-context learning. Similarly, zero-shot generalization, in-context learning, and fine-tuning capabilities on two vision-language tasks (captioning and VQA) show MetaLM's superior performance compared with previous strong baselines. 


\methodname{KOSMOS-1~\cite{huang2023language}: }Following MetaLM~\cite{hao2022language}, \citet{huang2023language} aimed to align perception with LLMs to create models that can work with multiple modalities. The proposed model, \textbf{KOSMOS-1}, consists of a Magneto-based LLM~\cite{wang2022foundation} as a general interface and xPos~\cite{sun2022length} encoders to encode different modalities. This model is trained on web-scale data consisting of text corpus, image-caption pairs, and interleaved image-captions pairs to generate the next token given context. To further align it with the human interface, training data also consists of several language-only instruction tuning datasets that are also dealt with as language modeling tasks. To demonstrate the capabilities of KOSMOS-1, a large set of experiments are performed across NLP (e.g., language generation, OCR-free text classification), cross-modal transfer (e.g., common-sense reasoning ), non-verbal reasoning (Raven's Progressive Matirces-based IQ tast~\cite{carpenter1990one, raven2003raven}), vision-language (e.g, captioning), and vision (e.g, zero-shot classification). These experimental results show the general-purpose nature of LLMs.


\methodname{KOSMOS-2~\cite{peng2023kosmos2}:} \citet{peng2023kosmos2} extended KOSMOS-1~\cite{huang2023language} for grounding capabilities and named it \textbf{KOSMOS-2}. To this end, they kept the KOSMOS-1's architecture and training objective and proposed a pipeline to extract text spans (i.e. noun phrases and referring expressions) and link them to corresponding regions in the images. This pipeline consists of two steps. First, non-chunks are extracted from the text and linked with regions in the image based on a pre-trained detector. Second, noun chunks are expanded to referring expressions by traversing nouns' dependency tree. Based on this pipeline, they curated GRIT (GRounded Image-Text pairs) from COYO-700M~\cite{kakaobrain2022coyo-700m} and LIAON-2B~\cite{schuhmann2022laion} consisting of 91M images, 115 text spans, and 137M bounding boxes. The input text is represented as a hyperlink similar to markdown where bounding box coordinates are converted into discrete location tokens and added in the appropriate segments. The model is trained on a combination of multi-modal corps from KOSMOS-1~\cite{huang2023language} and GRIT for the next token prediction. After this training, instruction tuning is carried out on language-only and grounded-instruction data. KOSMOS-2 achieves excellent performance on language and vision tasks, grounding tasks, and referring tasks which expands it to a more diverse set of downstream tasks. 


\noindent\textbullet \textbf{Training with a General Generative Objective: }
LLMs demonstrate excellent capabilities although they are trained on simple language modeling tasks. Inspired by this success, many works aimed to transfer this to vision-language modeling. Here, we describe methods that propose or train models on simple modeling tasks for pre-training of vision-language models. 
% SimVLM
\methodname{SimVLM~\cite{wang2021simvlm}: } \citet{wang2021simvlm} proposed a minimalist pre-training framework for vision-language models. The proposed Simple Vision Language Modeling (\textbf{SimVLM}) framework trains encoder-decoder style models on Prefix Language Modeling (PrefixLM) objective. The prefixLM considers the image to be a prefix for textual description, and thereby, forces the model to complete a description ($x_{\geq T_p}$) given an image and its partial description ($x_{< T_p}$) of randomly selected length ($T_p$). A simple transformer-based encoder-decoder architecture is utilized, and textual and visual embeddings (extracted by the first three blocks of a ResNet) are fed to the encoder, and the decoder outputs a text string. This model is trained on prefixLM on noisy image-text pairs dataset~\cite{jia2021scaling}. SimVLM does not require task-specific architecture or training and beats previous pre-training and state-of-the-art methods on several vision-language tasks. 

\methodname{MaskVLM~\cite{kwon2022masked}: } Motivated by the fact that both text and image can represent the same reality in different formats, \citet{kwon2022masked} proposed joint masked reconstruction language modeling where masked input of one is reconstructed conditioned on other unmasked input. Their model called \textbf{MaskVLM}, consists of an image and language encoder to encode corresponding modalities and a cross-modal decoder with cross-attention to align both modalities. Image and text are masked randomly following \citet{devlin2018bert} and \citet{he2022masked} and the model is trained on joint conditional reconstruction task as well as Image-text Contrastive (ITC) ~\cite{radford2021learning,jia2021scaling} and Image Text Matching (TIM)~\cite{chen2020uniter} task. This results in an efficient model that outperforms similar models for vision-language tasks in a low data regime.

\methodname{mPLUG-OWL~\cite{ye2023mplugowl}: }\citet{ye2023mplugowl} proposed \textbf{mPLUG-OWL}, a modular vision-languages model trained on language modeling objective. The model consists of an image encoder, an image abstractor, and a frozen LLM. This model is trained in two phases. In the first phase, the image encoder and visual abstractor are trained on image-text pairs with language modeling tasks. In the second phase, a visual abstractor along with a Low-Rank Adaptation module is fine-tuned with language-only and multi-modal datasets. mPLUG-OWL performs well on multi-turn dialogue as well as on instruction understanding, visual understanding, and knowledge transfer. 

% CapPa
\methodname{CapPa~\cite{tschannen2023image}: }Since the CLIP's~\cite{radford2021learning} demonstration of remarkable scaling properties of contrastive learning, contrastive methods have become a norm in vision-language pre-training. \citet{tschannen2023image} revisited the effectiveness of captioning for vision-language pre-training on webscale image-text pair datasets and systematically compared them with contrastive approaches. First, they compared the performance of Captioning-based models (Cap) with CLIP-style models on similar scales and compute budgets. They trained a simple encoder-decoder architecture (ViT as a vision encoder and transformer as a decoder) on a standard next-word prediction task. Their experimental results show that captioning models: a) generally lags behind CLIP-style models in zero-shot classification but the gap reduces with scale, b) matches or outperforms them in few-shot classification, c) have competitive performance for classification task when fine-tuned with large labeled data, and d) with ViT backbone outperforms CLIP-style models for multi-modal tasks. Second, they proposed a new generative pre-training method called \textbf{CapPa}, which, as shown in Fig.~\ref{fig:CapPa}, alternates training between standard auto-regressive prediction (CaP) and parallel prediction (Pa) where the entire caption is predicted in a single pass. The CapPa pre-training improves the performance of ViT. Third, they revealed the scaling properties of captioners by studying various architectures and training procedures and showed performance improvements when training data and architecture are scaled. 

% Figure environment removed



\subsection{Hybrid Contrastive and Generative Learning}
\label{sec:hybrid}

\subsubsection{Foundational Models for Generic Vision-Language Learning}

\noindent\textbullet \textbf{Unification of tasks: }

\methodname{UNITER~\cite{chen2020uniter}: } Inspired by the generalizability of BERT~\cite{lu2019vilbert} in Natural Langauge Processing (NLP) tasks~\cite{devlin2018bert}, \citet{chen2020uniter} proposed Universal Image-TExt Representation (\textbf{UNITER}), a method to leverage conventional image-text datasets (COCO~\cite{lin2014microsoft}, Visual Genome~\cite{krishna2017visual}, Conceptual Captions~\cite{sharma2018conceptual}, SBU Captions~\cite{ordonez2011im2text}) to train foundational models that can be used for heterogeneous vision-language tasks. The authors designed four pre-training tasks spanning generative (i.e., Masked Language Modeling (MLM), Masked Region Modeling (MRM)) and contrastive (Image-Text Matching (ITM), and Word Region Alignment (WRA)) objectives. The UNITER architecture consists of an image and text embedder and a cross-modality contextualized embedding transformer. The text and images of these datasets are fed to respective embedders to extract embeddings. Individual embeddings are fed to the cross-modality transformer for a cross-modal representation. This model is trained on a combination of four different vision-language datasets to optimize the pre-training tasks mentioned earlier. UNITER exhibits excellent generalizability across nine different vision-language tasks, setting state-of-the-art for most tasks.

% Pixel2Seqv2
\methodname{Pixel2Seqv2~\cite{chen2022unified}: }

\citet{chen2022unified} proposed to reformulate and unify four core vision tasks (object detection, instance segmentation, key points prediction, and captioning) into a single pixel-to-sequence interface where both task description and outputs are converted into tokens. Their proposed method, called \textbf{Pixel2Seqv2}, utilized an encoder-decoder architecture with a vision encoder that encodes image inputs and a sequence decoder that generates a single token conditioned on previous tokens, and encoded image. This model is trained on a simple language modeling task conditioned on previous tokens and encoded images. At inference time, output tokens are sampled given a task prompt and input image, and task-specific de-tokenization is performed. This Pixel2Seq can solve four vision tasks efficiently performs without requiring any specialized architecture or losses. 


% VL-x
\methodname{VL-x~\cite{cho2021unifying}: }\citet{cho2021unifying} proposed a unified framework to learn different computer vision tasks in a single architecture. The unification is achieved by reformulating these tasks into multi-modal conditional text generation tasks. The proposed Vision-Language (\textbf{VL-x}) employs a pre-trained encoder-decoder language model, such as BART or T5~\cite{lewis2019bart, raffel2020exploring}. Text and visual embeddings are fed to the encoder of this language model. Visual embeddings are extracted from a pre-trained object detector model and consist of Region of Interest (RoI) object features, RoI bounding box coordinates, and image and region IDs. The output of the visual task is converted into a sequence of characters and a task-specific prefix is added (e.g., classification: bird). This augmented text is encoded as learned embeddings and fed to the encoder of the language model along with the visual embeddings. This model is then trained with the task of multi-modal language modeling as well as related vision-language pre-training tasks, such as visual question-answering, image-text matching, visual grounding, and grounded captioning. This framework results in a multi-tasking model that can handle a diverse set of outputs. The models achieve comparable performance to specialized models. 


\input{tables/results_textually_prompted}

\noindent\textbullet \textbf{Universal Architectures: }
% general paragraph
Purely contrastive vision-language models with separate encoders (e.g., CLIP, ALIG) have shown impressive performance but are not well suited for multi-modal problems that require dealing with both modalities at the same time. On the other hand, multi-modal models, that have fusion and shared attention across modality encoders, are not suitable for uni-modal vision or language tasks. Here, we describe methods that aim to perform well across uni, cross, and multi-modal tasks by proposing new novel architectures and training them on multiple objectives including contrastive, generative, and task-specific losses. 

% CoCa
\methodname{CoCa~\cite{yu2022coca}: }\citet{yu2022coca} proposed a single unified encoder-decoder-based model called Contrastive Captioner (\textbf{CoCa}) that has the capabilities of the single encoder, dual encoder, and encoder-decoder models. The CoCa model consists of a unimodal image and text encoder and a decoupled multi-modal decoder with a cross-attention layer. The unimodal encoders are trained on a contrastive loss like CLIP. This helps the model learn robust and aligned global representations. The decoupled decoder is trained with a generative approach to captioning loss, which helps it learn detailed granularity and region-level information. A combination of these two approaches endows the model with both contrastive and generative capabilities. This strategy results in a foundational model that performs well across a set of diverse vision datasets. A single-trained CoCa model outperforms many specialized models under zero-shot and few-shot and light fine-tuning settings. For instance, it achieves 86.3\%, 88.0\%, and 91.0\% accuracy on ImageNet under zero and few-shot settings and light fine-tuning, respectively. 

% FLAVA
\methodname{FLAVA~\cite{singh2022flava}: }\citet{singh2022flava} argued that a truly foundational model must perform well across vision, language, and vision-language tasks. To this end, they proposed an architecture named \textbf{FLAVA} that consists of image and text encoders as well as multi-modal encoders, vision-task heads, language task heads, and multi-modal task heads. This makes FLAVA suitable for both uni-modal and multi-modal tasks. An overview of the proposed architecture is shown in Fig.~\ref{fig:falava}. The image and text encoders convert the input to a representation that is fed to a multi-modal encoder. The multi-modal encoder transformers apply cross-attention and fuse both modalities. This multi-modal representation is fed to modality-specific heads (vision, language, and vision language). To get strong generalization capabilities, this model is trained on multiple uni and multi-modal losses, including a global contrastive loss similar to CLIP~\cite{radford2021learning} for cross-modal alignment, masked multi-modal masking and image-text matching, masked-image modeling, masked-language modeling, etc. The training consists of a uni-modal pre-training of image and text encoders on supervised datasets followed by joint uni-modal and multi-modal training on image-text datasets. To demonstrate the generalizability of FLAVA, it is evaluated on 35 tasks across vision, language, and vision-language tasks and shows impressive performance. 

% BridgeTower
\methodname{BridgeTower~\cite{xu2022bridge}: }\citet{xu2022bridge} explored how to combine information from different layers of uni-modal encoders. They proposed \textbf{BridgeTower}, an architecture to combine information from different layers of uni-modal decoders without affecting their ability to perform uni-modal tasks. Their architecture consists of a standard vision and language encoder and a cross-modal encoder with multiple bridge layers that connects the top layers of both encoders by co-attention~\cite{lu2019vilbert}. These multi-modal bridges enable bottom-up, cross-modal alignment and fusion of different levels of semantic visual and textual features. Their results demonstrate superior performance on downstream VL tasks despite training on a smaller dataset. 

% Figure environment removed
 

% PaLI
\methodname{PaLI~\cite{chen2022pali}: }\citet{chen2022pali} investigated the effect of scaling for large image-text models by proposing a new jointly scaled architecture and a new large multilingual image-text dataset. First, they proposed \textbf{PaLI}, a jointly scaled, multilingual, modularized language-vision model which can perform unimodal (language, vision) and multimodal tasks. PaLI architecture consists of a text encoder-decoder transformer (mT5)~\cite{xue2020mt5} and a ViT~\cite{zhai2022scaling} for visual tokens. Both components are pre-trained and only the language component is updated and trained on a myriad of vision and language tasks. The language model is also trained on pure language understanding tasks to avoid catastrophic forgetting.

Second, they introduced a WebLI, 10 billion images, and 12 billion alt-text (textual alternative for images~\footnote{\url{https://webaim.org/techniques/alttext/}}) datasets. For their training, authors used a 1 billion clean subset of this dataset. They also used a combination of vision and language task-specific datasets, such as span corruption and object detection. For task-specific vision dataset training (such as object detection), the output is reformulated with the help of a template-based prompt. Third, they studied scaling laws in vision-language models and showed the importance of scaling vision components and the benefits of a mixture of language models. The PaLI model is pre-trained on over 100 languages and achieves SOTA results on various vision, language, and vision-language tasks. 

% X-FM
\methodname{X-FM~\cite{zhang2023toward}: }Existing models can work across modalities but their performance is not comparable to individual-type foundational models. To solve this problem, ~\citet{zhang2023toward} proposed a new foundational model called \textbf{X-FM} and a new training mechanism. The X-FM architecture consists of three modular encoders, including language, vision, and fusion encoders. The language and vision encoders are a stack of BERT~\cite{devlin2018bert} and ViT~\cite{dosovitskiy2020image} like transformer layers along with post-layer and pre-norm, respectively. In the fusion encoder's self-attention sub-layers, queries are from language, and keys and values are from vision. 

The proposed learning method for X-FM trains encoder with a combination of uni and multi-modal objectives and two new techniques. The language is trained with an encoder with masked language modeling (MLM) and image-text contrastive learning (ITC), a vision encoder with masked image modeling (MIM) and ITC, and a fusion encoder is trained with ITM and image-conditioned masked language modeling (IMLM), and bounding box prediction (BBP). The first new training technique is stopping gradient from vision-language when learning the language encoder. This way, the language encoder is isolated from fusion and trained on MLM and ITC for language modeling and language-vision alignment. The second new technique is the training vision encoder with masked images where the difference between its masked and unmasked outputs is minimized by using MSE loss. This way, the vision encoder is trained on cross-modal and unimodal objectives. This MIM training is resource-efficient, and convenient, enhancing vision and fusion encoder mutually. The X-FM outperforms other general foundational models on twenty-two tasks across language, vision, and language-vision tasks. 

% BLIP
\methodname{BLIP~\cite{li2022blip}: }Previous works rely on the large-scale nature of noisy image-text datasets which, \citet{li2022blip} argues, is sub-optimal. They introduced the \textbf{BLIP} framework which has both understanding and generation capabilities as it effectively utilizes image-text datasets and uses a new architecture. First, they proposed a captioner that produces synthetic captions and a filter that filters noisy captions. This makes it possible to synthesize and filter noisy captions cost-effectively. Second, BLIP has a Multimodal mixture of Encoder-Decocer (MED) architecture which consists of unimodal encoders for image and text and image-grounded text encoder and image-grounded text decoder. This model is trained on two understanding-based (i.e. image-text contrastive, image-text matching) objectives and one generative objective (i.e. language modeling). This framework achieves significant improvements and state-of-the-art performance across a wide variety of tasks. 

\noindent\textbullet \textbf{Efficient Utilization of Pre-Trained Models: }
BLIP and other similar models are prohibitively expensive to train as they require large-scale, end-to-end image-text training, often from scratch. Here, we explain methods that aim to leverage pre-trained vision and language models efficiently for vision-language modeling. 

% BLIP-2
\methodname{BLILP-2~\cite{li2023blip}: } \citet{li2023blip} proposed \textbf{BLIP-2}, a method to align pre-trained and frozen uni-modal text and image encoders on an image-caption dataset in a computationally efficient way. BLIP-2 bridges the modality gap of frozen uni-modal encoders by using a querying transformer. The parameters of the Q-former are trained to align two modalities by using image-text contrastive learning, image-grounded text generation, and image-text matching losses. This framework is computationally efficient and can leverage large pre-trained uni-modal models. 
% InstructBLIP
\methodname{InstructBLIP\cite{dai2023instructblip}: } \citet{dai2023instructblip} argued that aligning pre-training models on just image-caption can not allow broader generalization. They proposed \textbf{InstructBLIP}, a vision-language instruction tuning framework that enables general foundational models to solve multi-modal tasks through a unified language interface. Similar to BLIP-2~\cite{li2023blip}, their proposed architecture consists of a visual encoder, a Q-Former, and an LLM. Different from BLIP-2, they propose instruction-aware visual feature extraction. Specifically, the Q-former takes encoded images as well as instruction embeddings. This enables Q-Former to extract instruction-related visual features. Like BLIP-2, their model is trained in two phases: first Q-former is trained on image-text pairs, and then instruction-tuning is performed where both LLM and visual encoder are kept frozen. To train this model to perform multi-modal tasks, authors converted a suite of 26 datasets into instruction-tuning format following set templates. InstructBLIP achieves state-of-the-art zero-shot performance across a wide range of vision-language tasks.

% VPGTrans
\methodname{VPGTrans~\cite{zhang2023transfer}: } Most multi-modal models add a visual encoder (Visual Prompt Generator or VPG) and projection layer to the input of LLMs to enable perception. However, training these visual components is computationally expensive. \citet{zhang2023transfer} proposed \textbf{VPGTrans}, an efficient method to transfer visual encoders across LLMs. To this end, they performed an extensive experimental study to understand how to transfer VPGs across different sizes and types of LLMs. Based on their exploratory analysis, they proposed a two-stage strategy for VPG transfer. In the first stage, trained VPG from source LLM is kept frozen, and the projection module is fine-tuned on target LLM. In the second phase, both VPG and projection layer are trained with target LLM. Their empirical results across different sizes and types of LLMs show performance transfer with significantly less training data and compute resources. 

% TaCA
\methodname{TaCA~\cite{zhang2023taca}: } \citet{zhang2023taca} proposed an efficient framework to upgrade an old foundational model to a new task. To this end, they proposed an adapter called \textbf{TaCA} (Task Agnostic Compatible Adapter), a small module that aligns representations of old and new encoders with distillation loss between the features of old and new encoders and cross-modal contrastive loss. This results in a framework that makes it possible to upgrade modules of these models without the need to re-train. 

% Figure environment removed

\subsubsection{Foundational Models for Visual Grounding Tasks}
In this section, we describe methods that aim to solve visual grounding tasks by utilizing contrastive, generative, and other objectives. 


% ViLD
\methodname{ViLD~\cite{gu2021open}: } Open-vocabulary object detectors are difficult to train as data requires scales with the number of categories. On the other hand, vision-language models trained on web-scale image-text pairs have shown impressive open-vocabulary performance for classification. \citet{gu2021open} proposed an efficient, two-stage open-vocabulary object detection method that distills knowledge from a pre-trained one-vocabulary classification model. The proposed ViLD (Vision Language Distillation) method consists of a Region Proposal Network (RPN) and a CLIP-like pre-trained vision-language model. First, a Mask-RCNN~\cite{he2017mask} is modified to output class-agnostic object proposals and respective embeddings. Second, the vision head of the pre-trained vision-language model is utilized to extract embeddings, these embeddings are then used to distill knowledge into an object detector. $\ell_1$-loss. Third, the classifier is replaced with a pre-trained text encoder of the vision-language model which generates text embeddings for all categories. Similar to CLIP, cross-entropy loss followed is employed for this purpose. During inference, text embeddings of novel categories are also computed and each region is classified based on the highest similarity with the corresponding text embeddings. ViLD has shown significant improvements over supervised state-of-the-art open-vocabulary methods. 

% FIBER
\methodname{FIBER~\cite{dou2022coarse}: }Most vision-language foundational models either work with image-level understanding tasks (e.g., classification) or region-level understanding tasks(e.g., object detection). \citet{dou2022coarse} proposed two new ideas in FIBER to make vision-language models work for both types of tasks. Specifically, they propose to insert cross-attention layers inside the vision and language encoder for alignment. Moreover, they propose a two-stage, coarse-to-fine training pipeline. In this pipeline, the model is first trained with coarse-grained datasets such as image-text matching, masked language modeling, and image-text contrastive loss. During fine-grained training, the coarse pre-trained model is used at initialization and trained with high-resolution images with bounding box loss, word-region alignment, etc. FIBER can handle a diverse set of image understanding and groundings tasks and consistently improves upon strong baselines for these tasks. 


% UniDetector
\methodname{UniDetector~\cite{wang2023detecting}: } \citet{wang2023detecting} proposed a method for universal object detection that aims to detect novel categories in the open world. The UniDetector method aims to solve two main problems of universal object detection: training on heterogeneous object detection datasets and novel category discrimination. To this end, they proposed a three-phase training method for universal object detection. First, RegionCLIP-like pre-training methodology is adapted to align vision and text encoders. Second, training on heterogeneous datasets is performed where a Class-agnostic Localization Network (CLN) extracts regions that are fed to the vision encoder. Similarly, template-based category text is fed to the language encoder, and vision-language training is performed. Finally, during inference, a probability calibration is applied to improve novel category detection. An overview of UniDetector is shown in Fig.~\ref{fig:UniDetector}. UniDetector beats supervised state-of-the-art models for large-vocabulary object detection without being trained on training data and set new SOTA for closed-vocabulary object detection. 


\methodname{X-Decoder~\cite{zou2022xdecoder}: }Different vision methods operate at different levels of granularity such as image level, object level, and pixel level. \citet{zou2022xdecoder} argued that a method operating at all three levels can exploit the synergy of these tasks and proposed X-Decoder which formulates all levels of a task into a general decoding procedure. X-Decoder is built on top of a vision backbone based on Mask2Former~\cite{cheng2021mask2former}. The decoder takes multi-scale image features extracted by a vision encoder and two sets of queries: textual queries encoded by a text encoder and generic non-semantic queries that aim to decode segmentation masks. The decoder has two different types of outputs: pixel-level masks and token-level semantics. Different combinations of queries and output types can facilitate different tasks. This architecture is trained end-to-end on panoptic segmentation, referring segmentation, and image-text pairs datasets with task-specific semantic losses and mask-based loss. X-decoder exhibits strong zero-shot and task-specific transferability across a wide range of segmentation tasks as well as vision-language tasks. 


\methodname{GLIPv2~\cite{zhang2022glipv2}: } \citet{zhang2022glipv2} propose to use vision-language grounding as a meta capability for both localization and understanding tasks. To this end, they proposed a new inter-image region-word contrastive loss that utilizes negative phrases from different examples as potential negatives. A generic model consisting of a vision and a text encoder and a fusion decoder is trained with their proposed loss and grounding loss and masked language modeling loss~\cite{lu2019vilbert} on both image-text and detection datasets. The model can be used in zero-shot settings as well as in fine-tuning settings. Experimental results show both task benefits from each other. 

\section{Conversational Vision-Language Models}
\label{sec:conversationLLMs}

% a general paragraph: LLMs --> GPT4 --> variants
After the impressive performance of Large Language Models (LLMs) for comprehending, reasoning, and holding human-like conversations, there have been several works to incorporate visual modality in them. Conversational VLMs are a subcategory of textually prompted models, however, are equipped to hold human-like conversations based on multi-modal inputs. In this section, we review efforts to create conversational VLMs.

% GPT4
\methodname{GPT4~\cite{openai2023gpt4}:} OpenAI developed the first vision-language \textbf{GPT4} model~\cite{openai2023gpt4} which c hold multi-modal conversations and can describe intricate images and solve complex real-world problems. Due to `competitive landscape and ethical considerations', they decided not to open-source this model but provided an API-access through a paywall\footnote{https://help.openai.com/en/articles/7127956-how-much-does-gpt-4-cost}. This model is based on transformer-based architecture~\cite{vaswani2017attention}, pre-trained to predict the next word token using public and private datasets. GPT4 is then fine-tuned with Reinforcement Learning from Human Feedback (RLHF)~\cite{christiano2017deep}. GPT4 shows excellent performance across a span of conventional and real-world NLP, vision, and vision-language tasks. GPT4 performs exceptionally well on HumanEval~\cite{chen2021evaluating}, has a human-level performance on professional and academic exams designed for humans, outperforms previous SOTA language models on conventional NLP tasks, works well on even MMLU~\cite{hendrycks2020measuring} dataset translated across languages, and substantially improves model's ability to follow human intent. GPT4 also shows remarkable performance on vision tasks and describing intricate and complex scenes, of which an example is shown in Fig.~\ref{fig:gpt4_example}. 

% Figure environment removed

% minigpt4
\methodname{MiniGPT-4\cite{zhu2023minigpt}: }GPT4~\cite{openai2023gpt4} has remarkable emerging properties but the model behind it is closed-sourced and even its architectural details are unknown. \citet{zhu2023minigpt} aimed to unravel this and hypothesized that these models utilize large language models. To this end, they presented an open-source version of GPT4 called \textbf{miniGPT4} that consists of a pre-trained large language model (LLM) called Vicuna~\cite{vicuna2023} and a visual component that consists of ViT-G~\cite{sun2023eva} and a Q-Former. MiniGPT-4 adds a single linear projection layer on top of the vision encoder and freezes all other parameters. To align visual features with the LLM, the authors proposed a two-stage training-finetuning scheme. First, MiniGPT-4 is trained on a large set of multi-modal examples consisting of Conceptual Captions~\cite{sharma2018conceptual}, SBU~\cite{ordonez2011im2text}, and LAION~\cite{schuhmann2021laion}. Second, to improve the naturalness and usability, MiniGPT-4 is fine-tuned on a high-quality curated dataset of instructions and respective image and text pairs. MiniGPT-4 exhibits several intriguing properties of GPT4 such as generating intricate image descriptions, creating a website from its sketch, and explaining visual scenarios (see Fig.~\ref{fig:minigpt4_example}).

% Figure environment removed

% VideoChatGPT
\methodname{Video-ChatGPT~\cite{maaz2023videochatgpt}: } \citet{maaz2023videochatgpt} proposed \textbf{Video-ChatGPT}, a model that aligns video representation with a Vicuna LLM~\cite{vicuna2023} to enable interaction with videos. Their model consists of a pre-trained Vicuna LLM for language encoding, and a CLIP visual encoder pre-trained with visual instruction following in LLaVA. To make it compatible with videos, frame-level embeddings are averaged pooled along the temporal and spatial dimensions, and concatenated. These concatenated features are fed to a linear layer which transforms them for LLM. This model is instruction-tuned on video-text pairs using auto-regressive training objectives. To enable large-scale instruction fine-tuning, 100,000 video-text data is also developed using human annotations and semi-automatic annotations.

\methodname{XrayGPT~\cite{thawkar2023xraygpt}: } \citet{thawkar2023xraygpt} proposed a model that can analyze and answer open-ended questions about x-ray radiographs. Their proposed model, \textbf{XrayGPT}, consists of a Vicuna LLM as a text encoder and a MedClip for an image encoder. Multi-modal alignment is performed by updating a single linear projection layer on large radiology-specific data curated by authors. The resulting open-source model shows an impressive ability to hold conversations about a radiograph. 

Instruction tuning has played an important role in the alignment of LLMs for following instructions and solving various tasks. Here, we explain methods that aim to extend this behavior to vision-language models. 

% LLaVA
\methodname{LLaVA~\cite{liu2023llava}: } Inspired by the amazing instruction-following capabilities of LLMs and closed-sourced vision-language models, \citet{liu2023llava} proposed an open-source visual instruction tuning framework and model dubbed \textbf{LLaVA}. To this end, they have two main contributions. First, they proposed a cost-effective method to curate multi-modal instruction following data. This method leverages ChatGPT~\cite{chatgpt} and GPT4~\cite{openai2023gpt4} for data curation. This data consists of conversations, detailed descriptions of visual inputs, and complex reasoning. Second, they developed a large multi-modal model which utilizes a large pre-trained language model (LLaMA~\cite{touvron2023llama}) and CLIP's vision encoder (ViT). The vision encoder converts input images into features athat are fed to a linear projection layer. This layer converts these features into a space compatible with the LLM. This model is then trained with a two-phase strategy where the model is first trained for vision-language alignment and only the projection layer's parameters are updated. In the second phase, the LLMs and projection layer parameters are fine-tuned end-to-end on the curated dataset. LLaVA is the first visual-instruction following method and demonstrates excellent performance and can even explain complex visual scenes. Similarly, \citet{zhang2023llavar} also presented a visual tuning dataset. 

% LLaMA-Adapter
\methodname{LLaMA-Adapter~\cite{zhang2023llama}: }\citet{zhang2023llama} proposed \textbf{LLaMA-Adapter}, an efficient method that makes LLaMA~\cite{touvron2023llama} model into instruction following method. LLaMA-Adapter is primarily for text-based instruction fine-tuning but it also incorporates visual knowledge. The primary idea behind LLaMA-Adapter is to append a set of learnable adaption prompts as prefixes in the input tokens of the early transformer layers. LLaMA-Adapter can also handle input images by converting them into visual tokens using CLIP~\cite{radford2021learning} based image encoders. These adaptable visual prompts are also incorporated into LLaMA for vision-language tasks. LLaMA is primarily designed for the language but authors also show its superior performance on a large-scale multi-modal dataset called ScienceQA~\cite{lu2022learn}.

% LLaMA-Adapter V2
\methodname{LLaMA-Adapter V2~\cite{gao2023llamaadapterv2}:} Due to lack of instruction following dataset, LLaMA-Adapter~\cite{zhang2023llama} can only work with traditional vision-langauge tasks. \citet{gao2023llamaadapterv2} designed a parameter-efficient visual instruction that shows better performance on language-vision tasks and can conduct multi-run dialogs. LLaMA-Adapter V2 introduces the following improvements for this purpose. First, the authors introduced an early fusion of visual knowledge and added adaptable tokens to different transformer layers which avoids fusion. Second, they introduced disjoint visual-language and language-only training on disjoint parameters with image-captioning and instruction-following data. Third, more learnable parameters are introduced in LLM which includes retraining normalization layers and introduction of a new bias and scale factor for each linear layer of transformer. Finally, to improve image understanding ability, a visual expert is introduced which adds its expert knowledge as a context as shown in Fig.~\ref{fig:llama_adapterv2}. LLaMA-Adapter V2 enhances the instruction following ability of LLaMA, performs better on conventional vision-language tasks, and it is better at visual instruction following compared with V1.


We end our discussion about large visual language models with a brief discussion of their extensions and applications. The above conversational VLMs are generally not adept at visual grounding tasks and can reason about holistic images. Similar to the visual grounding works based on contrastive learning framework, recent efforts try to have visually grounded conversations, e.g., answering questions about a particular object \cite{zhao2023bubogpt,zhang2023gpt4roi,koh2023grounding,berrios2023towards}.
\citet{wu2023visual, zhu2023chatgpt} proposed ways to incorporate visual inputs to ChatGPT~\cite{chatgpt}. Several works also introduced methods to incorporate programming~\cite{surís2023vipergpt, skreta2023errors, mai2023llm, ren2023robots, zhang2023building, wu2023embodied, yoneda2023statler, park2023clara}. Several works have also expanded ChatGPT and other LLMs for diverse applications such as robotics~\cite{stella2023can, yang2023mmreact, jiang2022vima, wu2023tidybot, lin2023match, chakraborty2023re, you2023robot, wanna2023multimodal},  and a multitude of dimensions~\cite{zhao2023chat, ding2023task, chen2023video, ge2023openagi, wake2023chatgpt, hu2023advancing, zhao2023erra, shen2023hugginggpt, guo2023viewrefer, lu2023chameleon, zheng2023gpt4, zhang2023graphtoolformer, skreta2023errors, shukor2023ep, tong2023mass, liu2023softgpt, zhang2023sprint, li2023mimic, doveh2023dense, mu2023embodiedgpt, yuan2023artgpt, li2023evaluating, chen2023video, liu2023visual, zheng2023can, zhang2023graph, zhong2023chatabl, wang2023chatvideo, zhang2023recognize}.

% Figure environment removed


\section{Visually Prompted Models}
\label{sec:visually-prompted}

In this section, we discuss foundational models that can be prompted by non-textual prompts and have been designed for various visual tasks. In Sec. \ref{subsec:Foundational Models for Segmentation}, we discuss foundational models for image segmentation; CLIPSeg \cite{luddecke2022image}, SegGPT \cite{wang2023seggpt}, SAM \cite{kirillov2023segment}, and  SEEM \cite{zou2023segment}. These models can be prompted using diverse prompt types such as text, points, bounding boxes, or even a mask of the desired region to get the target segmentation. A visual foundational model like SAM \cite{kirillov2023segment} is trained on large-scale datasets that contain more than one billion masks and 11 million images. In other domains, such as medical image understanding, large-scale datasets on this scale may not be available. Our discussion then moves on to how SAM can be effectively adapted for other domains, such as medical \cite{ma2023segment, Lei2023medlam, Gong20233DSAMadapterHA, shaharabany2023autosam, gao2023desam, qiu2023learnable, wu2023medical, fedorov20123d}, tracking \cite{yang2023track, sam-pt}, remote sensing \cite{chen2023rsprompter}, and captioning \cite{wang2023caption}. Further, the models like SAM are based on high-complexity Vision Transformer-based architecture \cite{khan2022transformers} and trained on high-resolution inputs, making them less friendly for edge devices. We then discuss how these models can be efficiently adapted \cite{zhang2023faster, zhao2023fast, refsam} to mobile devices. In Sec. \ref{subsec:Generalist Models}, we describe generalist models \cite{wang2023images, wang2023visionllm}  that can perform different tasks simultaneously and can even adapt to the new task given a prompt and very few tasks-specific examples (a.k.a in-context learning). 

% Foundational models that align multiple paired modalities e.g., image-text, video-audio, or image-depth, etc., to learn meaningful representations are presented in Sec. \ref{sec:Heterogeneous Modalities based Models}. Finally, in Sec. \ref{sec:Embodied Foundational Agents}, we discuss foundational embodied agents that learn to connect the representation learning with real-world visuals and physical sensor modalities. Further, We summarize the publicly available information about these foundational models along with their prompting design and training in Tab. \ref{tab:visually_prompted_models_insights}.


% Figure environment removed

\subsection{Foundational Models for Segmentation}
\label{subsec:Foundational Models for Segmentation}
Segmentation involves grouping pixels in meaningful concepts within an image and pixel-wise object identification. % reorganizing meaningful concepts at the pixel level.
% In other words, segmentation involves grouping pixels within an image.
There are different types of segmentation based on how pixels are grouped, including panoptic, instance, and semantic segmentation. The existing segmentation models are specialized based on the type of segmentation or dataset. The foundational segmentation models aim to develop models that can be generalized universally for segmentation tasks. 

A classic segmentation model cannot generalize to new categories or incorporate new queries without retraining on a relevant dataset.  \textbf{CLIPSeg \cite{luddecke2022image}} exploits CLIP \cite{radford2021learning} generalization capabilities for zero-shot and one-shot segmentation tasks. Their proposed model CLIPSeg achieves this feat by conditioning transformer-based decoder on joint text-visual CLIP embeddings. The CLIPSeg consists of CLIP-based image and text encoders and a transformer-based decoder with U-net \cite{ronneberger2015u} inspired skip connections. The visual and text queries are passed through relevant CLIP encoders to get embeddings which are then fed to the CLIPSeg decoder. In this manner, target segmentation can be prompted by text or an image using CLIP. Therefore, CLIPSeg can generate image segmentations based on arbitrary prompts at test time. %

\textbf{Diversifying Segmentation Tasks:} The segmentation task can be applied to a variety of datasets, including part, semantic, instance, panoptic, person, medical, and aerial images. Here, we cover a few selected methods. \textbf{SegGPT \cite{wang2023seggpt}} provides an in-context learning paradigm and aims to train a single foundational model with a generalizable training scheme for these diverse segmentation tasks. The challenge is to accommodate different segmentation tasks and datasets in a single training framework. SegGPT accomplishes this by mapping different kinds of segmentation data into the same format of images  (random color mapping for each data sample) using an in-context learning framework \cite{wang2023images}. The goal is to color the appropriate areas, such as classes, object instances, parts, etc., in accordance with the context.  After training, the SegGPT model can perform few-shot semantic segmentation, video object segmentation, semantic segmentation, and panoptic segmentation without fine-tuning for the downstream tasks. 

\textbf{SAM \cite{kirillov2023segment}} is a zero-shot segmentation model that does not depend on CLIP and is instead trained from scratch on 1.1 billion masks and only 11 million images. Given an image and visual prompt (box, points, text, or mask) that specifies what to segment in an image, SAM encodes image and prompt embeddings using an image and prompt encoder, respectively which are then combined in a lightweight mask decoder that predicts segmentation masks (Fig. \ref{fig:sam}). SAM is trained to output a valid segmentation mask even when the given prompt is ambiguous e.g., given a point prompt on a person wearing a shirt, the model has to segment the shirt or the person wearing it. SAM is trained on over 1 billion masks with privacy-respecting images and model-in-the-loop dataset annotation settings. The data annotation has three stages: assisted-manual, semi-automatic, and fully automated. In the first stage, SAM assists annotators in annotating masks. By prompting SAM with likely object locations, SAM can generate masks for a subset of objects, while annotators focus on annotating the remaining objects. The final step involves prompting SAM with a regular grid of foreground points, yielding on average 100 high-quality masks per image.

\textbf{Diversifying SAM's Prompting Mechanism:} Inspired by the success of interactive LLMs like ChatGPT, \citet{zou2023segment} argued that the AI-human interaction is important but not well-explored in vision. SAM~\cite{kirillov2023segment} has a limited number of options for this purpose but still lacks a more comprehensive interactive system based on human conversations and it does not support high-level semantic tasks. Hence, the authors aimed to purpose a `universal interface for segmentation everything, everywhere with multi-modal prompts' a.k.a. SEEM~\cite{zou2023segment}. It can take multiple types of prompts including points, masks, text, boxes, and refereed regions of another image, and thus has strong composability. SEEM consists of a text and image encoder as well as a visual sampler for such prompts. These encoded inputs are projected to a joint image-text representations space, which is then fed to a decoder that outputs classes and mask embeddings. SEEM exhibits strong generalization capabilities and is efficient to run. It is also more interactive as it can take five different types of prompts. 

Many vision tasks may not have large-scale datasets to train task-specific large-scale models. Next, we will discuss how a foundational model SAM can be adapted for a variety of vision tasks in Sec. \ref{subsubsec:SAM for Captioning} to \ref{subsubsec:SAM for Mobile Applications}. An overview of various adaptations of SAM is exhibited in Fig.~\ref{fig:sam_med}.

\subsubsection{SAM for Medical Segmentation}
\label{subsubsec:SAM for Medical Segmentation}
 Segmenting medical images is fundamental to medical image analysis, which identifies and labels regions of interest (ROI) in different medical images, including organs, lesions, and tissues \cite{moor2023foundation}. SAM \cite{kirillov2023segment}, which is trained on natural images, has difficulty generalizing to medical image segmentation. In this section, we discuss strategies to effectively adapt SAM for medical image datasets.

\noindent \textbf{ Adapting by Fine-Tuning:}  Authors in \cite{ma2023segment} developed \textbf{MedSAM} by extending the SAM approach to medical images. They created a large-scale medical segmentation dataset with 33 segmentation tasks containing over 200,000 masks across 11 different data modalities. Then, they fine-tuned  SAM on the collected dataset for generalized medical image segmentation. In their fine-tuning approach, the image and prompt encoders are frozen, while the SAM decoder is trained on only medical datasets. MedSAM outperformed SAM on 21 3D segmentation tasks and 9 2D segmentation tasks.

\noindent \textbf{Adapting through Auxuliary Prompt Encoder:}  Authors of \cite{shaharabany2023autosam} present a fully automated solution for SAM's prompting for medical datasets, AutoSAM, and propose an auxiliary prompt encoder.
A surrogate prompt for SAM is generated by the \textbf{AutoSAM} auxiliary prompt encoder network based on the input image. In contrast to the prompt encoder provided by SAM, AutoSAM's encoder takes as input the image itself, rather than bounding boxes, points, or masks. A binary cross-entropy loss and a Dice loss are used to propagate gradients from the SAM network to the prompt encoder network during training. In comparison to SAM's decoders, the AutoSAM encoder network uses the Harmonic DenseNet \cite{chao2019hardnet} as its backbone and has fewer trainable parameters. By maintaining the integrity of the main SAM network, AutoSAM can be easily implemented and does not require SAM's fine-tuning to be determined according to an appropriate training schedule.

\noindent \textbf{Adapting Through Adapters:} %In the field of ophthalmology, segmentation is crucial for both diagnosis and treatment.  %In the field of ophthalmology, segmentation is crucial for both diagnosis and treatment. 
Different multi-modal images bring different segmentation targets, so segmenting multiple targets in ophthalmology can be challenging, such as segmenting blood vessels based on color fundus imagery, or retinal layers based on optical coherence tomography imaging. While SAM can locate several blood vessels from OCTA images, it cannot segment them from color fundus images because blood vessels or lesions may not be distinctive enough to identify \cite{qiu2023learnable}. SAM is extended to segment blood vessels or lesions or retinal layers accurately after one-shot fine-tuning with a new learnable prompt layer \cite{qiu2023learnable}. In addition to learning its target automatically in different modal images, the proposed learnable prompt possesses generalization abilities between datasets. 

Since SAM was originally designed for 2D natural images, it cannot effectively extract 3D spatial information from volumetric medical data. In \textbf{3DSAM-adapter}~\citet{Gong20233DSAMadapterHA}, a modification scheme is devised for the image encoder at the input level to enable the original 2D transformer to accommodate volumetric inputs. This scheme ensures the maximum reusability of pre-trained weights while still allowing them to capture certain 3D spatial patterns through parameter-efficient fine-tuning. Secondly, at the prompt encoder level, an alternative approach is proposed to positional encoding by introducing a visual sampler derived from the image embedding as the representation of the point prompt. Additionally, a set of global queries is employed to filter out noisy prompts. As image token sizes increase with higher dimensions, this strategy mitigates over-smoothing issues caused by the substantial increase in image token size. It also enhances the model's resilience to inaccurate prompts. 
Thus, with minimal adaptations, the transformer, originally trained on natural images, can capture spatial patterns inherent in volumetric medical images despite the domain gap between natural and medical data as well as the disparity in the spatial arrangement between 2D and 3D.

In \textbf{Medical SAM Adapter \cite{wu2023medical}}, a general medical image segmentation adapter is proposed for SAM that is designed with domain-specific knowledge, such as the high dimensionality (3D) of medical data, as well as unique visual prompts, such as clicks and BBoxes. Adapter modules are inserted into the original fundamental model, and then only Adapter parameters are adjusted while the pre-trained parameters are frozen. After training, Medical SAM Adapter (MSA) has demonstrated superior performance on 19 medical image segmentation tasks.

\noindent \textbf{Adapting by Modifying SAM's Decoder:} Through SAM, automatic segmentation can be achieved in two ways. The first option is to use grid points as prompts, and the second is to use boxes that are the same size as the image as prompts. Despite complete fine-tuning, fully automated SAM tends to generate a lot of false positive masks, and its performance falls well short of what clinicians expect. In \textbf{DeSAM \cite{gao2023desam}}, authors argue that in the cross-attention transformer layer of the SAM mask decoder, image embeddings, and prompt tokens interact with each other, resulting in a highly dependent final output mask. Therefore, the model remains sensitive to incorrect prompts, even after fine-tuning. DeSAM separates the mask decoder of SAM into two subtasks: 1) prompt-relevant IoU regression, and 2) prompt-invariant mask learning. The prompt-relevant IoU module predicts IoU scores based on the given prompt and generates mask embeddings. The prompt-invariant mask module (PIMM) generates the mask by combining the embeddings of the image encoder and the mask embeddings from PRIM. The performance degradation of SAM caused by wrong prompts in segment-everything mode is minimized with DeSAM. 


\noindent \textbf{SAM as a Medical Annotator:} 
\citet{Lei2023medlam} propose an annotation process for medical datasets using SAM and introduce a few-shot localization framework which is an extension of \cite{lei2021contrastive} and capable of locating any target anatomical part. \textbf{MedLAM} leverages a comprehensive dataset of 14,012 CT scans and incorporates two self-supervised tasks: relative distance regression (RDR) and multi-scale similarity (MSS). MedLAM significantly reduces the annotation burden by requiring annotations of only six extreme points on a few template images. These annotations then enable MedLAM to autonomously identify the target anatomical region across the entire dataset scheduled for annotation. As a result, MedLAM generates a 2D bounding box for each image slice, which is effectively utilized by SAM for subsequent segmentation tasks. Two 3D datasets containing 38 organs were evaluated by MedLAM, demonstrating that MedLSAM is comparable in performance to SAM and its medical adaptations, while minimizing the need to annotate extreme points across the dataset.

Similarly,  Segment Any Medical Model (\textbf{SAMM \cite{liu2023samm}}) is a medical image segmentation tool that combines 3D Slicer and SAM to assist with the development, assessment, and application of SAM. 3D Slicer \cite{fedorov20123d} is an open-source application that is capable of reading and writing a wide variety of file formats, manipulating 2D coordinate systems, and providing consistent user interfaces and tools to facilitate medical image analysis. Segmentation can be automated through prompts that can be applied automatically to subsequent slices. SAM's integration with 3D Slicer enables researchers to segment medical images with a state-of-the-art foundation model. 


\subsubsection{SAM for Tracking}
\label{subsubsec:SAM for Tracking}
One of the most important computer vision tasks is to track arbitrary objects in generic scenes and to distinguish regions of interest in videos from the background (also known as video object segmentation or VOS). A bounding box or segmentation mask is typically used to initialize trackers and segmenters trained on large datasets with manually annotated annotations. A specific object mask ground truth is also required to initialize the model under current initialization settings, particularly the semi-supervised VOS. SAM \cite{kirillov2023segment}, a foundational model for segmentation can be used to segment across frames within a video but it leads to poor results due to a lack of temporal consistency. Track Anything (\textbf{TAM}) \cite{yang2023track} proposes to use SAM and the off-the-shelf tracker XMem \cite{cheng2022xmem} to segment and track anything within a video. A user can simply click on an object to initialize SAM and predict the mask. XMem then tracks the object using the initial mask prediction provided by SAM in video based on spatiotemporal correspondences. The user can pause the tracking process and correct any errors immediately. In a similar manner,  SAM-Track \cite{cheng2023segment} utilizes DeAOT \cite{yang2022decoupling} with SAM. 

TAM \cite{yang2023track} and SAM-Track \cite{cheng2023segment} perform well but they do not effectively preserve the original performance of SAM in zero-shot scenarios, which are more challenging. Similarly, semi-supervised methods \cite{cheng2021mask2former, cheng2022xmem} for video object segmentation (VOS) and video instance segmentation (VIS) have performance gaps when they are applied to unseen data, especially when utilizing zero-shot models, which are used to segment object categories outside of the training distribution of video domains where they have not been trained. 

To solve these issues, \textbf{SAM-PT \cite{sam-pt}} proposes to combine SAM sparse point tracking for video segmentation, thus a sparse point annotation is all that is needed for the first frame to denote the target object. The open-world UVO \cite{wang2021unidentified} benchmark shows its strength in generalization to unseen objects. Utilizing state-of-the-art point trackers, such as PIPS \cite{harley2022particle}, SAM-PT provides sparse point trajectory predictions for video segmentation. SAM-PT shows that K-Medoids cluster centers are the most compatible for initiating SAM by using mask labels as initial points to track. Further, to distinguish target objects from their backgrounds, SAM-PT tracks positive and negative points simultaneously. 

\noindent \textbf{SAM-DA \cite{sam_da}} is another approach that uses SAM auto-segment abilities for tracking. Specifically, it determines enormous high-quality target domain training samples automatically from every nighttime image for tracking nighttime UAVs using SAM auto segmentation.

\subsubsection{SAM for Remote Sensing}
\label{subsubsec:SAM for Remote Sensing}
Guided primarily by points, boxes, and coarse-grained masks, SAM \cite{kirillov2023segment} relies heavily on manual guidance due to its interactive nature. Therefore, it makes SAM ineffective when it comes to understanding remote-sensing images in a fully automatic way. The results of SAM are heavily dependent on the type, location, and quantity of prompts used to segment remote-sensing image targets. To achieve desired results, it is usually necessary to refine manual prompts. As a result, SAM presents considerable limitations when used for remote-sensing image segmentation. \textbf{RsPrompter \cite{chen2023rsprompter}} incorporates semantic categorization information with SAM for automated instance segmentation for remote sensing images. RsPrompter proposes learning to generate appropriate prompts for SAM input. It generates prompts containing information about semantic categories by analyzing the intermediate layers of the encoder,  generating prompt embeddings, which can be viewed as point or box embeddings.


% Figure environment removed

\subsubsection{SAM for Captioning}
\label{subsubsec:SAM for Captioning}
An emerging multimodal topic, controlled image captioning uses natural language to explain an image according to human goals, such as examining certain regions of the image or describing it in a particular way. However, the scalability and usability of such interactive image captioning systems are greatly limited by the lack of well-annotated multimodal data.  Caption AnyThing (\textbf{CAT \cite{wang2023caption}}) is a zero-shot image captioning model that leverages pre-trained image captioners \cite{li2022blip, li2023blip, wang2022git} with segment anything (SAM) and large language models, ChatGPT. A user can define visual controls through visual prompts which are then converted into a mask using SAM to select the region of interest. Based on the original image and the mask provided, an image captioner predicts the raw caption. A text refiner (a large language model such as ChatGPT) then modifies user-defined language controls to tailor the language style to the user's preference, thus optimizing the raw descriptions.


\subsubsection{SAM for Mobile Applications}
\label{subsubsec:SAM for Mobile Applications}
A significant amount of attention has been directed toward SAM for its impressive zero-shot transfer performance as well as the ease with which it can be integrated with other models for advanced vision applications such as image editing. It is often necessary to run such use cases on edge devices with limited resources, such as mobile apps. In this section, we discuss efforts to make the SAM model mobile friendly with minimal impact on its generalizability.

One can get MobileSAM by following \cite{kirillov2023segment} pipeline for retraining a new SAM with a smaller image encoder such as replacing the ViT-H with a smaller ViT-L or even ViT-B. However, it can take multiple days and 128 GPUs to train a new SAM using ViT-L or ViT-B as its image encoder. Such resource-intensive retraining can be a non-trivial burden to reproduce or improve their results. A major reason for this optimization difficulty is that the mask decoder and image encoder are coupled together. In \textbf{FasterSAM \cite{zhang2023faster}}, the heavyweight image encoder is replaced by a lightweight one, making SAM more mobile-friendly. The first step is to distill the knowledge from the default image encoder, ViT-H, into a tiny version, ViT. Afterward, FasterSAM aligns the original SAM's mask decoder with the distilled image encoder more closely. MobileSAM, which is more than 60 times smaller than the original SAM, can be trained on a single GPU within less than one day.
%
In a similar manner, \textbf{Fast Segment Anything \cite{zhao2023fast}} also tries to bring the SAM \cite{kirillov2023segment} like capabilities to the mobile applications. It advocates the use of CNN rather than a vision transformer for image encoding and achieves segmenting anything pipeline into two stages. The first stage consists of implementing a Convolutional Neural Network (CNN)-based detector. Specifically, this method is based on YOLOv8-seg \cite{Jocher2023yolo}, a detector that uses the YOLACT \cite{bolya2019yolact} method for instance segmentation. It then outputs the region of interest that corresponds to the prompt in the second stage. In comparison to SAM, it provides comparable performance with dramatically reduced computational and resource demands, thus enabling real-time applications using only 2\% (1/50) of the SA-1B dataset.

\noindent \textbf{RefSAM \cite{refsam}} is also an efficient and end-to-end  SAM-based framework for the task of referring video object segmentation (RVOS), which performs accurate target object segmentation in videos based on the powerful foundation model SAM. An efficient and lightweight CrossModal MLP that converts the textual features of the referring expression into dense and sparse feature representations is employed. Further, the authors propose to align vision and language features using a parameter-efficient strategy. 

\subsection{Generalist Models}
\label{subsec:Generalist Models}

Using contextual learning, the model can be quickly adapted to a variety of tasks with only a few prompts and examples. The difficulty with in-context learning in computer vision comes from the fact that output representations vary greatly from task to task (requiring different loss functions and architectures.), therefore it's unclear how to define general-purpose task prompts or instructions for a visual model that can reconfigure the model for out-of-domain tasks. 
\textbf{Painter \cite{wang2023images}} is a generalist model that can perform different tasks simultaneously and even adapt to a new task given a prompt and very few task-specific examples. Given an input and output image for a certain task, the pixels of the output image are masked. The objective of the Painter model is then to inpaint the masked output image. This simple training objective allows the unification of several vision tasks (without modifications to model architecture or loss function) including depth estimation, human keypoint detection, semantic segmentation, instance segmentation, image denoising, image deraining, and image enhancement. After training, the Painter can determine which task to perform during inference using the input/output paired images from the same task as the input condition.

\noindent \textbf{VisionLLM \cite{wang2023visionllm}} is another generalist model that aligns vision and language modalities to solve open-ended tasks. Given an image, VisionLLM learns image features using a vision model; these image features along with a language instruction e.g., "describe the image in detail" are passed through a language-guided image tokenizer. The output of the image tokenizer along with language instructions is provided to an open-ended LLM-based task decoder designed to orchestrate various tasks in accordance with language instructions. \textbf{Prismer} \cite{liu2023prismer} is also a vision-language model that leverages diverse pre-trained domain experts in semantic segmentation, object, text, and edge detection, surface normal and depth estimation to perform multiple reasoning tasks such as image captioning and visual question answering.




\section{Heterogeneous Modalities based Models}
\label{sec:Heterogeneous Modalities based Models}
In this section, we discuss foundational models that align multiple paired modalities e.g., image-text, video-audio, or image-depth etc., to learn meaningful representations.


\noindent \textbf{Aligning CLIP with Heterogeneous Modalities:}  \cite{fang2021clip2video} extends the CLIP model for videos. There are two aspects of video-and-language understanding, spatial representation of multimodal image-text training, and temporal relationships of video frames and video language.  \textbf{CLIP2Video} \cite{fang2021clip2video} transfers the spatial semantics of the image-text CLIP model to the video-text retrieval problem by introducing temporal consistency with CLIP using the proposed Temporal Difference Block (TDB) and Temporal Alignment Block (TAB). They are designed to deal with the temporal relationships between video frames and video language. Adding the difference between two frames of an image to the sequence simulates motion change through Temporal Difference Blocks. The Temporal Alignment Block enhances the correlation between video and language by aligning them in the same feature space. 

Similarly, the \textbf{AudioCLIP} \cite{guzhov2021audioclip} model extends the CLIP model to also handle audio. AudioCLIP is therefore a tri-modal hybrid architecture that incorporates the ESResNeXt audio model into the CLIP framework using the AudioSet dataset. New text-to-audio- and image-to-audio loss terms are introduced along with the existing text-to-image-similarity loss term. After training, AudioCLIP is capable of processing all three modalities and any pair of them simultaneously. AudioCLIP outperforms previous approaches in environmental sound classification tasks and extends CLIP zero-shot capabilities to audio modality. Therefore AudioCLIP is capable of cross-modal querying using text, image, and audio or any combination of these modalities.

Both CLIP2Video \cite{fang2021clip2video} and AudioCLIP \cite{guzhov2021audioclip} extend CLIP to include one additional modality, however, there can be multiple types of paired modalities available in practice. \textbf{Image Bind \cite{girdhar2023imagebind}} includes multiple modalities by learning common representations for the paired data modalities e.g., (video, audio) or (image, depth). By aligning visual features with any of the sensory experiences associated with them, images have this 'binding' property that can offer many sources of supervision to learn visual features. For better representation learning, different sensors should be aligned to a single joint embedding space to learn visual features. The problem, however, is that it is not feasible to obtain every type and combination of paired data with the same set of images. The lack of multimodal data across all modalities is one of the major obstacles to learning a joint embedding. Using multiple types of image-paired data, ImageBind learns a single shared representation space, therefore it is independent of the constraint where datasets from all modalities need to co-occur with each other for joint learning. ImageBind combines large-scale paired data (image, text) with other paired data modalities (video, audio) or (image, depth) to develop a joint feature representation, thereby aligning each other modality (audio, depth) with textual embedding. ImageBind expands zero-shot capabilities across four modalities including audio, depth, thermal, and Inertial Measurement Unit (IMU) readings.

% Figure environment removed

\input{tables/visually_prompted_models}

\noindent \textbf{Aligning LLM with Heterogeneous Modalities:}  \textbf{MACAW-LLM}  \cite{Macaw-LLM} is an instruction-tuned multi-modal LLM that integrates four different modalities including image, video, audio, and text into a single model. It aligns the feature representation of different data modalities into the embeddings of an LLM thereby bringing those features closer to the textual representation of a large language model. A large-scale multimodal instruction dataset that combines image and video modalities is used to train the MACAW-LLM, which facilitates future work on this type of model learning. MACAW-LLM consists of three modules including Modality Module integrates different modality (e.g., visual and audio data) encoders into  MACAW-LLM, Alignment Module which unifies different modality encoders which are trained independently, and Cognitive Module which is pre-trained LLM.% also used as text encoder. 

For video-language reasoning, temporal consistency and context are important. The current image-text foundation models which are trained exclusively on image-text corpora have gained a comprehensive understanding of the semantic correlations between visual concepts and language, however, they lack the temporal contexts required for videos. A solution to this problem is to train on large-scale video-text corpora which is costly to obtain. \textbf{COSA} \cite{chen2023cosa} proposes to generate a video paragraph corpus from the image-text corpus by dynamically transforming it into long-form video paragraph samples. It randomly concatenates a certain number of image-text training samples together from the same batch at each training step. The images and texts are concatenated, ensuring that events and sentences correspond explicitly. The on-the-fly concatenated corpus is superior to short-form video-text corpora \cite{bain2021frozen} due to its richer scene transformations, reduced vision redundancy, and finer-grained captions that describe each frame sequentially.  Concatenated samples are used as input during pretraining by COSA, which has a simple architecture. In addition to visual retrieval, captioning, and answering questions, this model is capable of handling both discriminative and generative tasks.

%Aligning the semantics of the vision encoder with the language model leads to a successful understanding of images and languages, however, this pipeline has not been extensively investigated when it comes to a joint understanding of video and languages. 
\textbf{Valley} (Video Assistant with a Large Language model) \cite{luo2023valley} is another multi-modal framework capable of integrating video, image, and language perceptions. A simple projection module is used in the Valley model to bridge video, image, and language modalities, and is further unified with a multi-lingual LLM through the instructions-tuned pipeline. To obtain a unified vision encoding of video and image input, Valley also employs a spatiotemporal pooling strategy. Various video tasks including video question-answering, long description, casual relationship inference, and action recognition are used to collect instructions-following data. This data is then used for instruction fine-tuning to obtain a foundational model for videos.

\section{Embodied Foundational Agents}
\label{sec:Embodied Foundational Agents}
The training of LLMs on massive textual data may result in representations that are relevant to the real world, but to solve a wider range of computer vision and robotics problems grounded in reality, connecting these representations to real-world visual and physical sensor modalities is essential. In this section, we discuss foundational embodied agents for robot manipulation \cite{driess2023palme, jiang2022vima}.

\noindent \textbf{For Robot Manipulation:} By incorporating continuous inputs from sensor modalities of the embodied agent directly into embodied language models, \textbf{Palm-E} \cite{driess2023palme} (see Fig.~\ref{fig:palm})solves this challenging task, enabling the language model itself to make more grounded inferences about sequential decision-making. An LLM based on a Transformer embeds inputs such as images and state estimates into the same latent embedding as language tokens and processes them the same way as texts. The continuous inputs are injected through an encoder into a pre-trained LLM.  These encodings are trained from end to end by enforcing output sequential decisions in terms of natural text that can be understood by the embodied agents.

There are various forms of robotic task specification, including imitating one-shot demonstrations, following linguistic instructions, and reaching visual targets. Different models are often used for these different tasks. \textbf{ViMA} \cite{jiang2022vima} showed that multimodal prompting, interleaving textual and visual tokens, is effective at expressing a wide range of robot manipulation tasks thus learning robot manipulation through multimodal prompts. ViMA follows the transformer-based encoder-decoder network proposed by \cite{raffel2020exploring}. By encoding textual and visual prompt tokens using a pre-trained language model \cite{tsimpoukelli2021multimodal} and decoding robot control actions auto-regressively, VIMA encodes and decodes robot actions for every set of environment interactions. Further, they developed a new simulation benchmark with multimodal prompts that contains more than 600K expert trajectories for imitation learning. They also developed a four-level evaluation protocol for systematic generalization.

\noindent \textbf{For Continual Learners:} For such agents,  \cite{fan2022minedojo} provides a convenient API that makes it easy for users to specify task specifications, change world settings, and observe and act on tasks in Minecraft. There are thousands of open-ended tasks prompted by natural language. As a part of MineDojo, they collected a wealth of data from Minecraft including 30K+ YouTube videos with time-aligned transcripts, 6K+ free-form Wiki pages, and 340K+ Reddit posts with multimedia content. \textbf{MineDojo} \cite{fan2022minedojo} then devised a novel learning algorithm for embodied agents using such data. Their video-text model associates natural language subtitles with time-aligned YouTube videos from MineDojo. Further, they proposed a method for evaluating agents by using such a large pre-trained video-language model that was developed based on Minecraft YouTube videos. This complements human scoring \cite{shah2021minerl}, which is expensive.

Inspired by how humans play Minecraft, \textbf{VOYAGER}  \cite{wang2023voyager} argues that lifelong learning agents should be able to perform similar tasks to humans e.g., it should solve tasks based on the current level of their skill, adapt to environmental feedback to refine skills and actively seek out new tasks and explore the world.  VOYAGER is one of the first embodied lifelong learning agents powered by LLM. It is designed to drive exploration, hone a wide range of skills, and continuously discover new things in Minecraft. Based on the automatic curriculum generated by GPT-4, VOYAGER is designed to solve increasingly difficult tasks so that it discovers as much variety as possible. This is similar to novelty search  \cite{eysenbach2018diversity, conti2018improving}. As VOYAGER stores action programs to complete tasks, a skill library is incrementally built. VOYAGER's capability is compounded over time by composing smaller programs, alleviating the catastrophic forgetting associated with other continual learning methods \cite{parisi2019continual, wang2023comprehensive}.

\noindent \textbf{For Navigation Planning:} An actionable plan can be derived without prior fine-tuning in the target environment using \textbf{LM-Nav} \cite{shah2022lmnav}, which combines pre-trained vision and language models with a goal-conditioned controller. A pre-trained navigation model is combined with two robot-agnostic pre-trained models to achieve this. Using the robot's observations, we construct a topological "mental map" of the environment using a visual navigation model,  ViNG \cite{shah2021ving}. Then to decode free-form textual instructions into textual landmarks, LM-Nav uses a large language model, GPT-3 \cite{brown2020language}. To ground these textual landmarks in the topological map, it employs a vision-language model such as CLIP \cite{radford2021learning}, which infers a joint likelihood over the landmarks and nodes. Finally, to find a plan for the robot, VNM uses a novel search algorithm, which maximizes a probabilistic objective. In a nutshell, LM-Nav provides long-horizon instruction following in complex, real-world environments by combining three large pre-trained models; a self-supervised robotic control model, a vision-language model, and a large language model. 

\section{Open Challenges \& Research Directions}
\label{sec:Open Challenges and Research Directions}
While individual models discussed in this survey may have respective shortcomings and open challenges, this section aims to provide a holistic overview of the shared challenges these approaches (or their subset) face. We also highlight research directions that can help address these challenges. 


\noindent \textbf{Multimodal Open-source Models:} 
In NLP tasks, the transition from GPT3 to ChatGPT shows the importance of instruction-following and human-feedback-based reinforcement learning.  For multimodal (text and image) inputs, a similar capability is claimed by GPT4 \cite{openai2023gpt4} to allow reasoning and understanding based on vision-language inputs. However, GPT4 is a closed-source model with restricted access to date and its training details also remain unknown. To bridge this gap, multimodal open-source foundation models such as BLIP2 \cite{li2023blip}, GIT \cite{wang2022git} and Flamingo \cite{alayrac2022flamingo} can be extended with the instruction following and human intent alignment to have ChatGPT-like capabilities in multimodal spaces. To this end, initial efforts have been reported such as Intruct-BLIP, miniGPT4, LLaVA, and Video-ChatGPT.  However, matching the capabilities of GPT4 with open-source public models is still a major challenge for multi-modal foundational models.

\noindent \textbf{Evaluation and Benchmarking:} The open-ended nature of large-scale conversational Vision-Language Models make their comprehensive evaluation challenging. This challenge is shared with the progress in LLM but more severe for visual inputs since the possible tasks and reasoning capabilities become quite diverse for a broad and extensive evaluation. One \emph{quantitative} approach taken is to define a set of instructions covering multiple reasoning aspects and forward the responses from two competing chatbot VLMs to GPT4 to rate them on a scale of 1 to 10. This `LLM-as-a-judge' approach was introduced by Vicuna-Instruction-80 \cite{vicuna2023,zheng2023judging} benchmark for LLM that comprises of 9 instruction categories: \emph{generic, knowledge, math, counterfactual, Fermi, coding, writing, roleplay, common-sense}. This approach has also been extended for VLMs e.g., \cite{maaz2023videochatgpt} uses four criteria (\emph{correctness of information, detail orientation, contextual understanding, temporal understanding, consistency}) scored by GPT4 for benchmarking a VLM tailored for videos. However, the use of an external GPT4 model as a gold standard is still debatable and new efforts in LLM for benchmarking and identifying the corner cases have been reported to address the limitations of existing evaluation measures e.g., \cite{jain2023bring, li2023evaluating, schaeffer2023emergent, bubeck2023sparks, lin2023llm}. Such efforts are likely to be extended to VLMs with even more attention to the peculiar visual aspect of VLMs.


\noindent \textbf{Hallucination:} 
Hallucination refers to the phenomenon where the output generated from a large VLM/LLM  is unreal or nonsensical, often based on a hypothetical scenario.
Foundational language and vision models, specifically those based on Generative Pretrained Models for open-ended conversations \cite{openai2023gpt4,zhu2023minigpt,maaz2023videochatgpt,liu2023llava}, can sometimes fabricate answers, even if they are technically correct in specific contexts. This is because they are trained on massive datasets of text and/or images, that are often noisy, and they may not be able to distinguish between what is real and what is not. Specifically for VLMs where visual data is provided as an input condition, e.g., image-based visual question answering, one form of hallucination ignores the visual input and can only provide an answer based on the text prompt. As an example, an image with a green-colored apple together with a corresponding question \emph{what color is the apple in this image?} may result in an answer \emph{red} due to over-reliance on training data and ignoring the prompt context. One way to control hallucinations is to provide explicit instructions (or so-called \emph{system commands}) to a conversational LLM to provide its answers based on the provided context e.g., asking the chatbot to provide the missing information in patient health records while being strictly grounded in the facts available in the patient data. Other strategies to mitigate hallucination include the chain of thought prompting \cite{lu2023chameleon,zhang2023multimodal}, self-consistency (voting) \cite{mundler2023self,li2023evaluatingb} and use of knowledge bases for retrieval augmented generation \cite{ranjit2023retrieval,liu2023reta,pan2023retrieving}. 

\noindent \textbf{Multimodal Alignment:} Existing VLMs sometimes also suffer from poor alignment between vision-language (or other modalities). For instance, Segment anything \cite{kirillov2023segment} performance with text prompts is weaker as compared to the visual prompts (points, boxes, or masks). For heterogeneous modalities, such an alignment can be even more challenging. Approaches like ImageBind \cite{girdhar2023imagebind}  demonstrate viable ways in which the alignment between several modalities can be achieved, however, there is still much room to demonstrate strong alignment capabilities for even a wider range of related inputs that have the shared semantic space. For example, when a human sees a picture of a food item, it not only recognizes tcomputing item's category but also remembers the taste, the recipe used to cook it, and the crunchy sound each bite generates while eating the food. For a unified representation space that can provide a complete understanding of the world around us, foundational models targeted at learning joint embedding spaces will be crucial for further development.


\noindent \textbf{Large Data and Compute Requirements:}
Training large-scale vision and language models are data and compute-intensive. Labeled data on a large scale can be costly and time-consuming to acquire, especially for specialized visual domains or low-resource languages. Similarly, their inference is also costly due to the many parameters involved. The computational demands of these models limit their accessibility and scalability in many real-world applications. As an example, applications requiring real-time inference capability or ones requiring deployment on the edge and mobile devices with limited on-device compute and restricted battery times. Similarly, visual prompt-based models like Segment Anything \cite{kirillov2023segment} would benefit from a real-time speed to ensure intractability \cite{mobile_sam,zhao2023fast}. However, the current versions with a high-performing image encoder do not offer real-time overall processing. Efforts, such as retentive networks \cite{sun2023retentive}, can be integrated within VLMs for high throughput processing.  

\noindent \textbf{Adaptation of FMs:} Foundational model training often consumes long training times and massive compute resources. Therefore, FMs are adapted to several downstream tasks and applications. The efficient adaptation of FMs without damaging the extensive knowledge learned by the model is an open-research question with many interesting initial efforts reported in recent years. Due to great interest in LLMs and Diffusion models, such Parameter-Efficient Fine-tuning (PEFT) approaches are primarily explored for these two model classes but are directly applicable to adapt other vision foundational models as well. Some representative approaches include Low-rank Adaptation (LoRA) \cite{hu2021lora} and its variants such as QLoRA \cite{dettmers2023qlora} and GLoRA \cite{chavan2023one}, Prefix tuning \cite{li2021prefix,liu2021p}, Adapters \cite{zhang2023llama,gao2023llamaadapterv2}, Prompt tuning \cite{lester2021power,chen2023sam}. Reducing the compute and memory footprint for quick adaptation of textually and visually prompted foundational models is still an open research direction since the existing approaches require careful selection of hyper-parameters (e.g., rank in LoRA or the placement and dimensions of bottleneck adapters) and can result in loss of generalization performance.


 
\noindent \textbf{Vulnerability to Adversarial Attacks:}
Foundation models, similar to other neural network-based models, can be fooled by adversarial attacks. These attacks involve carefully crafted inputs that can cause the model to generate incorrect or harmful output. However, there are specific ways for adversarial attacks on foundational models that make them susceptible to unwanted behavior. As an example, models based on conversational LLMs have been shown vulnerable to adversarially prompt injection which needs a direct interaction between the adversary and the LLM-based conversational agent \cite{ChatgptJailbreak,perez2022ignore,RedteamingLLMs}. Greshake et al. \cite{greshake2023more} demonstrate that even a direct interaction between the model and adversary is not needed in LLM-integrated applications, and an adversary can remotely poison the information retrieved by the conversational agent via indirect prompt injection. This leads to a spectrum of vulnerabilities for LLMs and VLMs including manipulated content, fraud, malware, intrusion, personal information leakage, and denial of services via language and visual prompt injections. Carlini et al. \cite{carlini2023aligned} recently showed that NLP-based optimization attacks are weak and their failure on foundational models should not be taken as a certification of robustness. They further demonstrate that in conversational VLMs, an attack can be easily launched by adversarially perturbing the inputs to get harmful responses from the model. Maus et al. \cite{maus2023adversarial} show how non-meaningful text can be appended with the prompts to fool generative text and image models. The exemplars (input-output pairs) provided for In-Context Learning of VLMs can also be changed to fool the model \cite{wang2023adversarial}. Visual prompt-based models e.g., SAM have also been attacked by corrupting their inputs and associated prompts \cite{zhang2023attack, huang2023robustness}. Strategies to robustify foundational VLMs against such attacks is an open research question of high significance. 


\noindent \textbf{Bias and Fairness:}
Foundational models in vision and language can inherit and amplify biases present in the data used for training them. Biases, stereotypes, and prejudices related to race, underrepresented groups, minority cultures, and gender can make the models output biased predictions or exhibit skewed behavior.  For example, recent work \cite{shtedritski2023does} shows the sensitivity of the CLIP model towards red circles, where simply drawing a red circle around a person's face increases its chances of being misclassified as a murderer, suspect, or a missing person. This behavior potentially emerges from the data that contain examples from news media outlets that typically put a red circle around criminals in their broadcasts. In concurrent efforts, new benchmarks have been developed to assess the capability of existing VLMs towards certain biases \cite{hall2023visogender}.
Addressing biases in foundational AI models is crucial to ensure fairness, inclusivity, and ethical deployment of these systems.

\noindent \textbf{Interpretablity:} 
Foundation models are often difficult to interpret, which can make it difficult to understand how they work and why they generate the output they do. In this direction, existing methods have investigated chain-of-thought reasoning to explain the outputs generated by vision and language models \cite{wei2022chain}. New benchmarks are also developed to evaluate and train explicitly focusing on providing detailed step-wise rationales for model choices e.g., ScienceQA \cite{lu2022learn}. An interesting idea, called Visual Programming, is to use interpretable neuro-symbolic representation to break down a complex task into simpler steps that explain the rationale of a particular output from GPT-3 \cite{gupta2023visual}. While these efforts are promising, they have several failure cases and can be further improved e.g., by allowing user feedback to improve the interpretation generated by the model. 

\noindent \textbf{Limited Contextual Understanding:}
While transformer-based foundational models have shown impressive language and vision understanding capabilities, they can still struggle with certain contextual nuances. Understanding sarcasm, irony, or other forms of figurative image and language inputs (e.g., memes) can be challenging for these models, leading to inaccurate interpretations or responses. While there have been initial efforts on this topic in language-only models, similar efforts for large multi-modal models are much needed and remain an open problem. 

\noindent \textbf{Lack of Real-world Understanding:}
Foundational models for language and vision lack a deep understanding of the world. They can only generate prompt-conditioned text, code, or visual outputs that are consistent with the data they have been trained on. This training is focused on outputting the next sequence element given previous sequence elements or learning multimodal alignment. However, such training is much different from human learning and reasoning which is grounded in physical reality \cite{hu2023neural}. This means that large language and vision models may not be able to understand the meaning of what they are generating or to reason about complex concepts grounded in the real world. Egocentric perception and embodied AI agents based on foundational multimodal models need to develop world models and the alignment of heterogeneous modalities for improved physically grounded reasoning.  In this direction, works on embodied foundational models such as MineDojo \cite{fan2022minedojo}, VPT \cite{baker2022learning} and Voyager \cite{wang2023voyager} use open-ended nature of Minecraft game as a testbed for embodied agents based on GPT models. However, taking these agents to real-world tasks and complex environments is a challenging problem requiring much further work. Google's Palm-E \cite{driess2023palme} is a step in this direction that combines ViTs with Palm \cite{chowdhery2022palm,anil2023palm} LLM model trained with language and sensory inputs to allow embodied agents to understand commands and take informed actions. 

\section{Conclusion}
Models with a foundational understanding of multiple modalities, including natural language and vision, are essential for developing AI systems that can effectively perceive and reason about the real world. This survey reviews vision and language foundational models focusing on their architecture types, training objectives, downstream task adaption, and their prompting designs. We provide systematic categorizations of textually-prompted, visually-prompted, and heterogeneous modality models. We provide a broad coverage of their applications in a variety of visual tasks including zero-shot recognition and localization abilities, visual dialogue about an image or a video, cross-modal, and medical data understanding. We summarize how foundational models in vision can act as generalist models solving multiple tasks simultaneously and their combination with large language models gives rise to foundational embodied agents that can continually learn and navigate in a complex environment. We hope that this effort will spur further research in leveraging the potential of foundational models along with addressing their limitations, e.g. limited contextual understanding, biases, and vulnerability to malicious uses.

{
\small
\bibliographystyle{plainnat}
\bibliography{refs}
}


\newpage
\end{document}


