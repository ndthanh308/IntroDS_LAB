\documentclass{article}


\usepackage{PRIMEarxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{fancyhdr}       % header
\usepackage{graphicx}       % graphics
\graphicspath{{media/}}     % organize your images and other figures under media/ folder


\usepackage{color}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{tabularx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{booktabs}  
\usepackage{threeparttable} \usepackage{multirow}
\usepackage{subfigure}
\usepackage{tablefootnote}
\usepackage{footnote}

\newtheorem{theorem}{\textbf{Theorem}}
\newtheorem{definition}{\textbf{Definition}}
\newtheorem{assumption}{\textbf{Assumption}}
\newtheorem{fact}{\textbf{Fact}}
\newtheorem{setting}{\textbf{Setting}}
\newtheorem{lemma}{\textbf{Lemma}}

%Header
\pagestyle{fancy}
\thispagestyle{empty}
\rhead{ \textit{ }} 

% Update your Headers here
% \fancyhead[LO]{Federated Distributionally Robust Optimization with
% Non-Convex Objectives: Algorithm and Analysis}
% \fancyhead[RE]{Firstauthor and Secondauthor} % Firstauthor et al. if more than 2 - must use \documentclass[twoside]{article}



  
%% Title
\title{Federated Distributionally Robust Optimization with
Non-Convex Objectives: Algorithm and Analysis}

\author{
  Yang Jiao \\
  Tongji University \\
  %% examples of more authors
   \And
  Kai Yang\thanks{Corresponding author.} \\
  Tongji University \\
     \And
  Dongjin Song \\
  University of Connecticut \\
}


\begin{document}
\maketitle

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
Distributionally Robust Optimization (DRO), which aims to find an optimal decision that minimizes the worst case cost over the ambiguity set of probability distribution, has been widely applied in diverse applications, \textit{e.g.}, network behavior analysis, risk management, \textit{etc.} However, existing DRO techniques face three key challenges: 1) how to deal with the asynchronous updating in a distributed environment;  2) how to leverage the prior distribution effectively; 3) how to properly adjust the degree of robustness according to different scenarios. To this end, we propose an asynchronous distributed algorithm, named \textbf{A}synchronous \textbf{S}ingle-loo\textbf{P} alternat\textbf{I}ve g\textbf{R}adient proj\textbf{E}ction (ASPIRE) algorithm with the it\textbf{E}rative \textbf{A}ctive \textbf{S}\textbf{E}t method (EASE) to tackle the federated distributionally robust optimization (FDRO) problem. Furthermore, a new uncertainty set, \textit{i.e.}, constrained $D$-norm uncertainty set, is developed to effectively leverage the prior distribution and flexibly control the degree of robustness. Finally, our theoretical analysis elucidates that the proposed algorithm is guaranteed to converge and the iteration complexity is also analyzed. Extensive empirical studies on real-world datasets demonstrate that the proposed method can not only achieve fast convergence, and remain robust against data heterogeneity as well as malicious attacks, but also tradeoff robustness with performance.
\end{abstract}


% % % keywords can be removed
% \begin{keywords}
% Distributionally Robust Optimization, Distributed Optimization, Federated Learning, Uncertainty Set, Complexity Analysis
% \end{keywords}



\section{Introduction}

The past decade has witnessed the proliferation of smartphones and Internet of Things (IoT) devices, which generate a plethora of data everyday. Centralized machine learning requires gathering the data to a particular server to train models which incurs high communication overhead \cite{sun2019communication} and suffers privacy risks \cite{sicari2015security}. As a remedy, distributed machine learning methods, e.g., federated learning have been proposed. Considering a distributed system composed of $N$ workers (devices), we denote the dataset of these workers as $\{ {D_1}, \cdots,{D_N}\} $. For the $j^{\rm{th}}$ ($1 \! \le \! j \!\le \! N$) worker, the labeled dataset is given as ${D_j} = \{ {\bf{x}}^i_j,y^i_j\} $, where ${\bf{x}}^i_j \! \in \! \mathbb{R}^{d}$ and $y^i_j \in  \{1,\cdots,c\}$ denote the $i^{\rm{th}}$ data sample and the corresponding label, respectively. The distributed learning tasks can be formulated as the following optimization problem,
\begin{align}
\label{eq:0}
\mathop {\min }\limits_{\boldsymbol{w}\in {\boldsymbol{\mathcal{W}}}} \; F(\boldsymbol{w}) \quad {\rm{with}} \quad F(\boldsymbol{w}): = \sum\nolimits_{j} {{f_j}} (\boldsymbol{w}),
\end{align}
where $\boldsymbol{w}\in {\mathbb{R}^p}$ is the model parameter to be learned and ${\boldsymbol{\mathcal{W}}}\!\subseteq\! \mathbb{R}^p$ is a nonempty closed convex set, ${f_j}( \cdot )$ is the empirical risk over the $j^{\rm{th}}$ worker involving only the local data:
\begin{equation}
\label{eq:2}
\begin{aligned}
{f_j}({\boldsymbol{w}}) = \sum\nolimits_{i:{\bf{x}}_j^i \in {D_j}}^{} {\frac{1}{{|{D_j}|}}} \mathcal{L}_j({\bf{x}}_j^i,y_j^i;{\boldsymbol{w}}),
\end{aligned}
\end{equation}
where $\mathcal{L}_j$ is the local objective function over the $j^{\rm{th}}$ worker. Problem in Eq. (\ref{eq:0}) arises in numerous areas, such as federated learning \cite{mohri2019agnostic}, distributed signal processing \cite{geraci2015energy}, multi-agent optimization \cite{nedic2009distributed}, \textit{etc}. However, such problem does not consider the data heterogeneity \cite{zawad2021curse,qian2020robustness,qian2020towards,liao2021investigation}  among different workers (\textit{i.e.}, data distribution of workers could be substantially different from each other \cite{singhal2021federated}). Indeed, it has been shown that traditional federated approaches, such as FedAvg \cite{mcmahan2017communication}, built for independent and identically distributed (IID) data may perform poorly when applied to Non-IID data \cite{karimireddy2019scaffold}. This issue can be mitigated via learning a robust model that aims to achieve uniformly good performance over all workers by solving the following distributionally robust optimization (DRO) problem in a distributed manner:
\begin{align}
\label{eq:4_new}
\mathop {{\rm{min}}}\limits_{\boldsymbol{w}\in {\boldsymbol{\mathcal{W}}}} {\rm{ }}\mathop {{\rm{max}}}\limits_{{\bf{p}} \in {\bf{\Omega} } \subseteq {\Delta _N}} F(\boldsymbol{w},{\bf{p}}): = \sum\nolimits_{j} p_j {{f_j}} (\boldsymbol{w}),
\end{align}
where ${\bf{p}}  =  [{p_1}, \cdots ,{p_N}] \! \in \! {\mathbb{R}^N}$ is the adversarial distribution in $N$ workers, the $j^{\rm{th}}$ entry in this vector, \textit{i.e.}, ${p_j}$ represents the adversarial distribution value for the $j^{\rm{th}}$ worker. ${\Delta _N}=\{ {\bf{p}} \in {\mathbb{R}^{N}_{+}}: {\bf{1}^ \top }{\bf{p}} = 1 \} $  and ${\bf{\Omega} }$ is a subset of ${\Delta _N}$.  Agnostic federated learning (AFL)~\cite{mohri2019agnostic} firstly introduces the distributionally robust (agnostic) loss in federated learning and provides the convergence rate for (strongly) convex functions. However, AFL does not discuss the setting of ${\bf{\Omega} }$. DRFA-Prox~\cite{deng2021distributionally} considers ${\bf{\Omega} }={\Delta _N}$ and imposes a regularizer on adversarial distribution to leverage the prior distribution. Nevertheless, three key challenges have not yet been addressed by prior works. First, whether it is possible to construct an uncertainty framework that can not only flexibly maintain the trade-off between the model robustness and performance but also effectively leverage the prior distribution?  Second, how to design asynchronous algorithms with guaranteed convergence?  Compared to synchronous algorithms, the master in asynchronous algorithms can update its parameters after receiving updates from only a small subset of workers~\cite{zhang2014asynchronous,chang2016asynchronous}.  Asynchronous algorithms are particularly desirable in practice since they can relax strict data dependencies and ensure convergence even in the presence of device failures \cite{zhang2014asynchronous}. Finally, whether it is possible to flexibly adjust the degree of robustness?  Moreover, it is necessary to provide convergence guarantee when the objectives (\textit{i.e.}, ${f_j}({\boldsymbol{w}_j}),\forall j$) are non-convex. 

To this end,  we propose ASPIRE-EASE to effectively address the aforementioned challenges. Firstly, different from existing works, the prior distribution is incorporated within the constraint in our formulation, which can not only leverage the prior distribution more effectively but also achieve guaranteed feasibility for any adversarial distribution within the uncertainty set. The prior distribution can be obtained from side information or uniform distribution \cite{qian2019robust}, which is necessary to construct the uncertainty (ambiguity) set and obtain a more robust model~\cite{deng2021distributionally}.  Specifically, we formulate the prior distribution informed distributionally robust optimization (PD-DRO) problem as:
\begin{align}
\label{eq:3}
\mathop {{\rm{min}}}\limits_{\boldsymbol{z}\in{{\boldsymbol{\mathcal{Z}}}},\{{\boldsymbol{w}_j}\in{{\boldsymbol{\mathcal{W}}}}\}} & \mathop {{\rm{max}}} \limits_{{\bf{p}}  \in \boldsymbol{\mathcal{P}}} \sum\nolimits_{j} {{p_j}{f_j}({\boldsymbol{w}_j})} \\ 
{\rm{s.t.}} \; \quad    \boldsymbol{z}&  =    {\boldsymbol{w}_j}, \; j\!=\!1,\!\cdots\!, N , \nonumber\\
{\rm{var.}} \; \;  \; \;  \boldsymbol{z}&,{\boldsymbol{w}_1},{\boldsymbol{w}_2}, \cdots ,{\boldsymbol{w}_N}, \nonumber
\end{align}
where $\boldsymbol{z}\! \in \! {\mathbb{R}^p}$ is the global consensus variable, $\boldsymbol{w}_j\! \in \! {\mathbb{R}^p}$ is the local variable (local model parameter) of $j^{\rm{th}}$ worker and ${\boldsymbol{\mathcal{Z}}}\!\subseteq\! \mathbb{R}^p$ is a nonempty closed convex set.  $\boldsymbol{\mathcal{P}}\!\subseteq\! \mathbb{R}_ + ^N$  is the uncertainty (ambiguity) set of adversarial distribution ${\bf{p}}$, which is set based on the prior distribution. To solve the PD-DRO problem in an asynchronous distributed manner, we first propose \textbf{A}synchronous \textbf{S}ingle-loo\textbf{P} alternat\textbf{I}ve g\textbf{R}adient proj\textbf{E}ction (ASPIRE), which employs \emph{simple} gradient projection steps for the update of primal and dual variables at every iteration, thus is computationally \emph{efficient}. Next, the it\textbf{E}rative \textbf{A}ctive \textbf{S}\textbf{E}t method (EASE) is employed to replace the traditional cutting plane method to improve the computational efficiency and speed up the convergence. We further provide the convergence guarantee for the proposed algorithm.  {\color{black} We further propose an adaptive ASPIRE that can flexibly adjust the number of active workers (\textit{i.e.}, the number of workers that communicate with master at each iteration).} Furthermore, a new uncertainty set, \textit{i.e.}, constrained $D$-norm ($CD$-norm), is proposed in this paper and its advantages include: 1) it can flexibly control the degree of robustness; 2) the resulting subproblem is computationally simple; 3) it can effectively leverage the prior distribution and flexibly set the bounds for every $p_j$. {\color{black}In addition to the proposed $CD$-norm uncertainty set, we also provide a comprehensive analysis about different uncertainty sets that can be employed in our framework.}

{\bf{Contributions.}} Our contributions can be summarized as follows:

\begin{enumerate}
    \item We formulate a PD-DRO problem with $CD$-norm uncertainty set. PD-DRO incorporates the prior distribution as constraints which can leverage prior distribution more effectively and guarantee robustness. In addition, $CD$-norm is developed to model the ambiguity set around the prior distribution and it provides a flexible way to control the trade-off between model robustness and performance.

    \item We develop a \emph{single-loop} \emph{asynchronous} algorithm, namely ASPIRE-EASE, to optimize PD-DRO in an asynchronous distributed manner. ASPIRE employs simple gradient projection steps to update the variables at every iteration, which is computationally efficient. And EASE is proposed to replace cutting plane method to enhance the computational efficiency and speed up the convergence. We demonstrate that even if the objectives ${f_j}({\boldsymbol{w}_j}),\forall j$ are non-convex, the proposed algorithm is guaranteed to converge. We also theoretically derive the iteration complexity of ASPIRE-EASE. 

    \item {\color{black}We extend the proposed framework to incorporate a variety of different uncertainty sets, e.g., ellipsoid uncertainty set and Wasserstein-1 distance uncertainty set. We theoretically analyze the computational complexity for each uncertainty set. To accelerate the convergence speed, we further propose ASPIRE-ADP, which can adaptively adjust the number of active workers.}

    \item Extensive empirical studies on four different real world datasets demonstrate the superior performance of the proposed algorithm. It is seen that ASPIRE-EASE can not only ensure the model's robustness against data heterogeneity but also mitigate malicious attacks. 

    

    
\end{enumerate}


{\color{black}
{\bf{Comparison with the conference paper.}} This work significantly extends the conference paper \cite{jiao2022distributed}. Specifically, the major difference between this paper and the conference paper can be summarized as follows. 1) Besides the proposed CD-norm uncertainty set, we also provide five more uncertainty sets that can be incorporated into our framework. Please see Section \ref{Uncertainty Sets}. 2) The unified complexity analysis regarding ASPIRE-EASE with different uncertainty sets is conducted, please refer to Section \ref{Unified Complexity Analysis}. 3) ASPIRE-ADP is proposed in this work, i.e., ASPIRE-EASE with an adaptive NAW, to effectively accelerate the converge of the proposed algorithm. Please see Section \ref{ASPIRE-ADP}. Furthermore, we theoretically analyze the iteration complexity of the proposed ASPIRE-ADP, please refer to Theorem 2 in Section \ref{ASPIRE-ADP}, and the detailed proof is given in Appendix \ref{appendix:Theorem2}.} 

\section{Preliminaries}\label{Preliminaries}

\subsection{Distributionally Robust Optimization}

Optimization problems often contain uncertain parameters. % due to limitations of data observation, measurement noise, measurement errors, and so on. 
A small perturbation of the parameters could render the optimal solution of the original optimization problem infeasible or completely meaningless \cite{bertsimas2004price}. Distributionally robust optimization (DRO) \cite{kuhn2019wasserstein,duchi2021learning,blanchet2019quantifying} assumes that the probability distributions of uncertain parameters are unknown but remain in an ambiguity (uncertainty) set and aims to find a decision that minimizes the worst case expected cost over the ambiguity set, whose general form can be expressed as,
\begin{align}
\mathop {\min }\limits_{\boldsymbol{x} \in \boldsymbol{\mathcal{X}}} \mathop {\max }\limits_{P \in {{\bf{P}}}} {\mathbb{E}_P}[r(\boldsymbol{x},\boldsymbol{\xi})],
\end{align}
where $\boldsymbol{x} \! \in \! \boldsymbol{\mathcal{X}} $ represents the decision variable,  ${\bf{P}}$ is the ambiguity set of probability distributions $P$ of uncertain parameters $\boldsymbol{\xi}$. Existing methods for solving DRO can be broadly grouped into two widely-used categories \cite{rahimian2019distributionally}: 1) Dual methods \cite{delage2010distributionally,wiesemann2013robust,gao2016distributionally} reformulate the primal DRO problems as deterministic optimization problems through duality theory. Ben-Tal et al. \cite{ben1999robust} reformulate the robust linear optimization (RLO) problem with an ellipsoidal uncertainty set as a second-order cone optimization problem (SOCP).  2) Cutting plane methods \cite{mehrotra2014cutting,bertsimas2016reformulation} (also called adversarial approaches \cite{gorissen2015practical}) continuously solve an approximate problem with a finite number of constraints of the primal DRO problem, and subsequently check whether new constraints are needed to refine the feasible set.  Recently, several new methods~\cite{qian2019robust,levy2020large,hu2021bias} have been developed to solve DRO, which need to solve the inner maximization problem at every iteration.


\subsection{Cutting Plane Method for PD-DRO}

In this section, we introduce the cutting plane method for PD-DRO in Eq. (\ref{eq:3}). We first reformulate PD-DRO by introducing an additional variable  $h \!\in\! {{\boldsymbol{\mathcal{H}}}} $ (${{\boldsymbol{\mathcal{H}}}}\! \subseteq\! {\mathbb{R}^1}$ is a nonempty closed convex set) and protection function $g(\{ {\boldsymbol{w}_j}\} )$ \cite{yang2014distributed}. Introducing additional variable $h$  is an epigraph reformulation \cite{ben2009robust,yanikouglu2019survey}. In this case, Eq. (\ref{eq:3}) can be reformulated as the form with uncertainty in the constraints:
\begin{align}
 \label{eq:4-1}
\mathop {{\rm{min}}}\limits_{\boldsymbol{z}\in{{\boldsymbol{\mathcal{Z}}}},\{{\boldsymbol{w}_j}\in{{\boldsymbol{\mathcal{W}}}}\}, h \in {{\boldsymbol{\mathcal{H}}}}} &\quad  h  \nonumber\\
{\rm{s.t.}} \; \sum\nolimits_{j}  {\overline{p} {f_j}({\boldsymbol{w}_j})} \!  + \!   g&(\{ {\boldsymbol{w}_j}\} ) - h \! \le \! 0, \\
\boldsymbol{z}  =    {\boldsymbol{w}_j} , \; j\!=&1,\!\cdots\!, N , \nonumber\\
{\rm{var.}}\quad \boldsymbol{z},{\boldsymbol{w}_1},{\boldsymbol{w}_2}, &\cdots ,{\boldsymbol{w}_N}, h. \nonumber
\end{align}
where $\overline{p} $ is the nominal value of the adversarial distribution for every worker and $g(\{ {\boldsymbol{w}_j}\} ) = \mathop {\max }\limits_{{\bf{p}} \in \boldsymbol{\mathcal{P}}} \sum\nolimits_{j} {({p_j} - \overline{p} ){f_j}({\boldsymbol{w}_j})} $ is the protection function. Eq. (\ref{eq:4-1}) is a semi-infinite program (SIP) which contains infinite constraints and cannot be solved directly \cite{rahimian2019distributionally}{}. Denoting the set of cutting plane parameters in $(t\!+\!1)^{\rm{th}}$ iteration as ${{\bf{A}}^t} \!  \subseteq \!  {\mathbb{R}^N}$, the following function is used to approximate $g(\{ {\boldsymbol{w}_j}\} ) $:
\begin{equation}
\begin{aligned}
\overline{g} (\{ {\boldsymbol{w}_j}\} ) = \mathop {\max }\limits_{{\boldsymbol{a}_l} \in {{\bf{A}}^t}} {\boldsymbol{a}^\top_l} {\bf{f}} (\boldsymbol{w}) = \mathop {\max }\limits_{{\boldsymbol{a}_l} \in {{\bf{A}}^t}} \sum\nolimits_{j}  {{a_{l,j}}{{f}_j}({\boldsymbol{w}_j})}, 
\end{aligned}
\end{equation}
where ${\boldsymbol{a}_l = [{a_{l,1}}, \cdots ,{a_{l,N}}]} \! \in \! {\mathbb{R}^N}$ denotes the parameters of $l^{\rm{th}}$ cutting plane in ${{\bf{A}}^t}$ and ${\bf{f}}(\boldsymbol{w}) \! = \! [{{f}_1}(\boldsymbol{w}_1),\cdots, {{f}_N}(\boldsymbol{w}_N)] \! \in \! {\mathbb{R}^N}$. Substituting the protection function $g(\{ {\boldsymbol{w}_j}\} ) $ with $\overline{g} (\{ {\boldsymbol{w}_j}\} )$, we can obtain the following approximate problem:
\begin{align}
\label{eq:9}
\mathop {{\rm{min}}}\limits_{\boldsymbol{z}\in{{\boldsymbol{\mathcal{Z}}}},\{{\boldsymbol{w}_j}\in{{\boldsymbol{\mathcal{W}}}}\}, h \in {{\boldsymbol{\mathcal{H}}}}} & \quad h \nonumber\\
{\rm{s.t.}} \; \sum\nolimits_{j}\! {(\overline{p}  + {a_{l,j}}){{f}_j}({{\boldsymbol{w}}_j})}&  - h \!\le\! 0,{\rm{     }}\forall {\boldsymbol{a}_l} \! \in \! {{\bf{A}}^t}, \\
\boldsymbol{z}  =    {\boldsymbol{w}_j} , \; j\!=&1,\!\cdots\!, N , \nonumber\\
{\rm{var.}}\quad \boldsymbol{z},{\boldsymbol{w}_1},{\boldsymbol{w}_2}, & \cdots ,{\boldsymbol{w}_N}, h. \nonumber
\end{align}
	

\section{ASPIRE} 
Distributed optimization is an attractive approach for large-scale learning tasks \cite{yang2008distributed,bottou2018optimization} since it does not require data aggregation, which protects data privacy while also reducing bandwidth requirements \cite{subramanya2021centralized}.  When the neural network models (\textit{i.e.}, ${f_j}({\boldsymbol{w}_j}),\forall j$ are non-convex functions) are used, solving problem in Eq. (\ref{eq:9}) in a distributed manner facing two challenges. 1) Computing the optimal solution to a non-convex subproblem requires a large number of iterations and therefore is highly computationally intensive if not impossible. Thus, the traditional Alternating Direction Method of Multipliers (ADMM) is ineffective. 2) The communication delays of workers may differ significantly \cite{chen2020asynchronous}, thus, asynchronous algorithms are strongly preferred.



To this end, we propose the \textbf{A}synchronous \textbf{S}ingle-loo\textbf{P} alternat\textbf{I}ve g\textbf{R}adient proj\textbf{E}ction (ASPIRE). The advantages of the proposed algorithm include: 1) ASPIRE uses simple gradient projection steps to update variables in each iteration and therefore it is computationally more efficient than the traditional ADMM method, which seeks to find the optimal solution in non-convex (for ${\boldsymbol{w}_j}, \forall j$) and convex (for ${\boldsymbol{z}}$ and $h$) optimization subproblems every iteration,  2) the proposed asynchronous algorithm does not need strict synchronization among different workers. Therefore, ASPIRE remains resilient against communication delays and potential hardware failures from workers. Details of the algorithm are given below. Firstly, we define the node as master which is responsible for updating the global variable $\boldsymbol{z}$, and we define the node which is responsible for updating the local variable ${\boldsymbol{w}_j}$ as worker $j$. In each iteration, the master updates its variables once it receives updates from at least $S$ workers, \textit{i.e.}, active workers, satisfying $1 \le S \le N$. For brevity, we call $S$ number of active workers (NAW) hereafter. ${{\bf{Q}}^{t + 1}}$denotes the index subset of workers from which the master  receives updates during $(t+1)^{\rm{th}}$ iteration. We also assume the master will receive updated variables from every worker at least once for each $\tau$ iterations. The augmented Lagrangian function of Eq. (\ref{eq:9}) can be written as:
\begin{align}
\label{eq:11}
\! {L_p}  =  h \! + \! \sum\nolimits_{l}\! {{\lambda _l}(\sum\nolimits_{j}\! {(\overline{p}  + {a_{l,j}}){f_j}({\boldsymbol{w}_j})}  \!-\! h)} \! + \! \sum\nolimits_{j}\! {{\boldsymbol{\phi}^\top_j}\!(\boldsymbol{z}\! -\! {\boldsymbol{w}_j})} \! + \! \sum\nolimits_{j} \! {\frac{{{\kappa _1}}}{2}||\boldsymbol{z} \! - \! {\boldsymbol{w}_j}|{|^2}}\!,  
\end{align}
where ${L_p}\!=\!{L_p}{\rm{(\{ }}{\boldsymbol{w}_j}{\rm{\} ,}}\boldsymbol{z},h,\{ {\lambda _l}\} ,\{ {\boldsymbol{\phi}_j}\})$, ${\lambda _l}\!\in\! {{\boldsymbol{\Lambda}}},\forall l$  and ${\boldsymbol{\phi}_j}\! \in \! {{\boldsymbol{\Phi}}},\forall j$ represent the dual variables of inequality and equality constraints in Eq. (\ref{eq:9}), respectively. ${{\boldsymbol{\Lambda}}}\! \subseteq \! \mathbb{R}^1$ and ${{\boldsymbol{\Phi}}}\! \subseteq \! \mathbb{R}^p$ are nonempty closed convex sets, constant ${\kappa _1} > 0$  is a penalty parameter. Note that Eq. (\ref{eq:11}) does not consider the second-order penalty term for inequality constraint since it will invalidate the distributed optimization. Following \cite{xu2020unified}, the regularized version of Eq. (\ref{eq:11}) is employed to update all variables as follows,
\begin{align}
 {\widetilde{L}_p}{\rm{(\{ }}{\boldsymbol{w}_j}{\rm{\} ,}}\boldsymbol{z},h,\{ {\lambda _l}\} ,\{ {\boldsymbol{\phi}_j}\}) 
 ={L_p}- \sum\nolimits_{l} {\frac{{c_1^{t}}}{2}||{\lambda _l}|{|^2}}  - \sum\nolimits_{j} {\frac{{c_2^{t}}}{2}||{\boldsymbol{\phi}_j}|{|^2}},
\end{align}
where $c_1^{t}$ and $c_2^{t}$ denote the regularization terms in $(t+1)^{\rm{th}}$ iteration.  In $(t+1)^{\rm{th}}$ master iteration, the proposed algorithm proceeds as follows.

\textbf{1)} \emph{Active} \emph{workers} update the local variables ${\boldsymbol{w}_j}$ as follows,
\begin{equation}
\label{eq:15}
{\boldsymbol{w}_j^{t+1}} \!=\! \left\{ \begin{array}{l}
{\mathcal{P}_{{\boldsymbol{\mathcal{W}}}}}({\boldsymbol{w}_j^{t}}  -  {\alpha _{\boldsymbol{w}}^{\widetilde{t}_j}}{\nabla _{{\boldsymbol{w}_j}}}{ \widetilde{L}_p}{\rm{(\{ }}{\boldsymbol{w}_j^{\widetilde{t}_j}}{\rm{\} ,}}\boldsymbol{z}^{\widetilde{t}_j},h^{\widetilde{t}_j},\!\{ {\lambda _l^{\widetilde{t}_j}}\} ,\!\{ {\boldsymbol{\phi}_j^{\widetilde{t}_j}}\} {\rm{)}}),\forall j \! \in \! {{\bf{Q}}^{t + 1}},\\
{\boldsymbol{w}_j^t},\forall j \notin {{\bf{Q}}^{t + 1}},
\end{array} \right.
\end{equation}
where $\widetilde{t}_j$ is the last iteration during which worker $j$ was active. It is seen that $ {\boldsymbol{w}_j^{t}} = {\boldsymbol{w}_j^{\widetilde{t}_j}} $ and ${\boldsymbol{\phi}_j^{t}} = {\boldsymbol{\phi}_j^{\widetilde{t}_j}},\forall j \! \in \! {{\bf{Q}}^{t + 1}}$. ${\alpha _{\boldsymbol{w}}^{\widetilde{t}_j}}$ represents the step-size and we set  ${\alpha _{\boldsymbol{w}}^t} = {\eta _{\boldsymbol{w}}^t}$ when $t<T_1$ and ${\alpha _{\boldsymbol{w}}^t} = {\underline{\eta _{\boldsymbol{w}}}}$ when $t \ge T_1$, where  ${\eta _{\boldsymbol{w}}^t}$ and constant ${\underline{\eta _{\boldsymbol{w}}}}$ will be introduced below. $\mathcal{P}_{{\boldsymbol{\mathcal{W}}}}$ represents the projection onto the closed convex set ${{\boldsymbol{\mathcal{W}}}}$ and we set ${{\boldsymbol{\mathcal{W}}}} = \{ {{{\boldsymbol{w}}_j}}|\;|| {{\boldsymbol{w}}_j}||_{\infty} \! \le {\alpha _1}\} $, ${\alpha _1}$ is a positive constant. And  
then, the active workers ($ j \! \in \! {{\bf{Q}}^{t + 1}}$) transmit their local model parameters ${{\boldsymbol{w}}_j^{t+1}}$ and loss ${f}_j({\boldsymbol{w}_j})$ to the master.

\textbf{2)} After receiving the updates from active workers, the \emph{master} updates the global consensus variable $\boldsymbol{z}$, additional variable $h$ and dual variables ${\lambda _l}$ as follows,
\begin{equation}
\label{eq:16}
\boldsymbol{z}^{t+1}\! =\! {\mathcal{P}_{{\boldsymbol{\mathcal{Z}}}}}({\boldsymbol{z}}^{t} - {\eta _{\boldsymbol{z}}^t}{\nabla _{\boldsymbol{z}}}{\widetilde{L}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\} ,}}{\boldsymbol{z}}^{t},h^{t},\! \{ {\lambda _l^{t}}\} ,\! \{ {{\boldsymbol{\phi}}_j^{t}}\} {\rm{)}}),
\end{equation}
\begin{equation}
\label{eq:17}
h^{t+1} \!=\! {\mathcal{P}_{{\boldsymbol{\mathcal{H}}}}}(h^{t} - {\eta _h^t}{\nabla _h}{\widetilde{L} _p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\} ,}}{\boldsymbol{z}}^{t+1},h^{t},\! \{ {\lambda _l^{t}}\} ,\!\{ {{\boldsymbol{\phi}}_j^{t}}\} {\rm{)}}),
\end{equation}
\begin{equation}
\label{eq:lambda_update}
{\lambda _l^{t+1}} \!=\! {\mathcal{P}_{{\boldsymbol{\Lambda}}} }({\lambda _l^{t}} \! + \! {\rho _1}{\nabla _{{\lambda _l}}}{\widetilde{L}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\} ,}}{\boldsymbol{z}}^{t+1},h^{t+1},\! \{ {\lambda _l^{t}}\} , \! \{ {{\boldsymbol{\phi}}_j^{t}}\} )), \; l\!=\!1,\!\cdots\!, |{{\bf{A}}^t}|,
\end{equation}
where ${\eta _{\boldsymbol{z}}^t}$, ${\eta _h^t}$ and $\rho_1$ represent the step-sizes. ${\mathcal{P}_{{\boldsymbol{\mathcal{Z}}}}}$, ${\mathcal{P}_{{\boldsymbol{\mathcal{H}}}}}$ and ${\mathcal{P}_{{\boldsymbol{\Lambda}}}}$ respectively represent the projection onto the closed convex sets ${{\boldsymbol{\mathcal{Z}}}}$, ${{\boldsymbol{\mathcal{H}}}}$ and ${{\boldsymbol{\Lambda}}}$. We set ${{\boldsymbol{\mathcal{Z}}}} = \{ {{{\boldsymbol{z}}}}|\;|| {{\boldsymbol{z}}}||_{\infty} \! \le {\alpha _1}\} $, ${{\boldsymbol{\mathcal{H}}}} = \{ h|\; 0 \le \! h \! \le {\alpha _2}\} $ and ${{\boldsymbol{\Lambda}}} = \{ {\lambda _l}|\; 0 \le \! {\lambda _l} \! \le {\alpha _3}\} $, where $\alpha _2$ and $\alpha _3$ are positive constants. $|{{\bf{A}}^t}|$ denotes the number of cutting planes. Then, master broadcasts ${\boldsymbol{z}}^{t+1}$, $h^{t+1}$, $\{{\lambda _l^{t+1}}\}$ to the active workers.



\textbf{3)} \emph{Active} \emph{workers} update the local dual variables ${\boldsymbol{\phi}_j}$ as follows,
\begin{equation}
\label{eq:y_update}
{\boldsymbol{\phi}_j^{t+1}}\! =\! \left\{\! \begin{array}{l}
{\mathcal{P}_{{\boldsymbol{\Phi}}}}({{\boldsymbol{\phi}}_j^{t}}\! +\! {\rho _2}{\nabla _{{{\boldsymbol{\phi}}_j}}}{\widetilde{L}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\} ,}}{\boldsymbol{z}}^{t+1},h^{t+1},\! \{ {\lambda _l^{t+1}}\} ,\! \{ {{\boldsymbol{\phi}}_j^{t}}\} {\rm{)}}{\rm{)}},\forall j \!\in\! {{\bf{Q}}^{t + 1}},\\
{{\boldsymbol{\phi}}_j^{t}},\forall j \notin {{\bf{Q}}^{t + 1}},
\end{array} \right.
\end{equation}
where $\rho_2$ represents the step-size and  ${\mathcal{P}_{{\boldsymbol{\Phi}}}}$ represents the projection onto the closed convex set  ${{\boldsymbol{\Phi}}}$ and we set ${{\boldsymbol{\Phi}}} = \{ {{{\boldsymbol{\phi}}_j}}|\;|| {{\boldsymbol{\phi}}_j}||_{\infty} \! \le {\alpha _4}\} $, $\alpha _4$ is a positive constant. And master can also obtain $\{{\boldsymbol{\phi}_j^{t+1}}\}$ according to Eq. (\ref{eq:y_update}). It is seen that the projection operation in each step is computationally simple since the closed convex sets have simple structures \cite{bertsekas1997nonlinear}.


\section{Iterative Active Set Method}\label{sec:ease}


Cutting plane methods may give rise to numerous linear constraints and lots of extra message passing \cite{yang2014distributed}. To improve the computational efficiency and speed up the convergence, we consider removing the inactive cutting planes. The proposed it\textbf{E}rative \textbf{A}ctive \textbf{S}\textbf{E}t method (EASE) can be divided into the two steps: during $T_1$ iterations, 1) solving the cutting plane generation subproblem to generate cutting plane, and 2) removing the inactive cutting plane every $k$ iterations, where $k\!>\!0$ is a pre-set constant and can be controlled flexibly.


The cutting planes are generated according to the uncertainty set. For example, if we employ ellipsoid uncertainty set, the cutting plane is generated via solving a SOCP. In this paper, we propose  $CD$-norm uncertainty set, which can be expressed as follows,
\begin{align}
\label{eq:D norm}
\boldsymbol{\mathcal{P}} \!= \! \{ {\bf{p}} \!:  - \widetilde{p}_j \! \le \!  p_j -q_j \! \le \! \widetilde{p}_j,   \sum\nolimits_{j} \!|{\frac{{  p_j -q_j }}{ \widetilde{p}_j  }}| \! \le \! \Gamma   ,  {\bf{1}^ \top }{\bf{p}}\! =\! 1  \},
\end{align}
where  $\Gamma \! \in \! {\mathbb{R}^1}$ can flexibly control the level of robustness, ${\bf{q}}=[q_1, \cdots, q_N] \! \in \! {\mathbb{R}^N}$ represents the prior distribution, $-\widetilde{p}_j$ and $\widetilde{p}_j$ ($\widetilde{p}_j \ge 0$) represent the lower and upper bounds for $p_j -q_j$, respectively. The setting of ${\bf{q}}$ and $\widetilde{p}_j, \forall j$ are based on the prior knowledge. $D$-norm is a classical uncertainty set (which is also called as budget uncertainty set) \cite{bertsimas2004price}. We call Eq. (\ref{eq:D norm}) $CD$-norm uncertainty set since ${\bf{p}}$ is a probability vector so all the entries of this vector are non-negative and add up to exactly one, \textit{i.e.}, ${\bf{1}^ \top }{\bf{p}} = 1$. Due to the special structure of $CD$-norm, the cutting plane generation subproblem is easy to solve and the level of robustness in terms of the outage probability, \textit{i.e.}, probabilistic bounds of the violations of constraints can be flexibly adjusted via a single parameter $\Gamma$. We claim that $l_1$-norm (or twice total variation distance) uncertainty set is closely related to $CD$-norm uncertainty set. Nevertheless, there are two differences: 1) $CD$-norm uncertainty set could be regarded as a weighted $l_1$-norm with additional constraints. 2) $CD$-norm uncertainty set can flexibly set the lower and upper bounds for every $p_j$ (\textit{i.e.}, $q_j \! - \! \widetilde{p}_j  \! \le \!  p_j \! \le \! p_j \! + \! \widetilde{p}_j$), while $0 \! \le \! p_j  \! \le \! 1, \forall j$ in $l_1$-norm uncertainty set. Based on the $CD$-norm uncertainty set, the cutting plane can be derived as follows,

\begin{algorithm}[tb]
   \caption{ASPIRE-EASE}
   \label{algorithm1}
\begin{algorithmic}
   \STATE {\bfseries Initialization:}  iteration $t = 0$, variables $\{{{\boldsymbol{w}}_j^0}\}$, ${\boldsymbol{z}}^0$, ${h}^0$, $\{{\lambda _l^0}\}$, $\{{{\boldsymbol{\phi}}_j^0}\}$  and set ${{\bf{A}}^0} $.
   \REPEAT
   \FOR{\emph{active worker}}

   \STATE updates local  ${\boldsymbol{w}_j^{t+1}}$ according to Eq. (\ref{eq:15});
   \ENDFOR
   
   \STATE \emph{active workers} transmit local model parameters and loss to \emph{master};
   
   
   \STATE \emph{master} receives updates from \emph{active workers}  \textbf{do}
    
   
   \STATE  \quad  updates ${\boldsymbol{z}}^{t+1}$, $h^{t+1}$, $\{{\lambda _l^{t + 1}}\}$, $\{{\boldsymbol{\phi}_j^{t+1}}\}$ in master according to Eq. (\ref{eq:16}), (\ref{eq:17}), (\ref{eq:lambda_update}), (\ref{eq:y_update});
    
    
    \STATE  \emph{master} broadcasts ${\boldsymbol{z}}^{t+1}$, $h^{t+1}$, $\{{\lambda _l^{t+1}}\}$ to \emph{active workers};
    
    \FOR{\emph{active worker}}
   \STATE updates local  ${\boldsymbol{\phi}_j^{t+1}}$ according to Eq. (\ref{eq:y_update});
   \ENDFOR
   
   \IF{$(t+1)$ mod $k$ $==$ 0 and $t< T_1$}
   \STATE   \emph{master} updates ${{\rm \bf{A}}^{t + 1}}$ according to Eq. (\ref{eq:cutting set}) and (\ref{eq:active}), and broadcast parameters to all workers;
   
   \ENDIF
  
   \STATE $t =t+1$;
   \UNTIL{convergence}
\end{algorithmic}
\end{algorithm}

1) Solve the following problem,
\begin{align}
\label{eq:18}
& {\bf{p}}^{t + 1}  = \mathop {\arg \max }\limits_{  {p_1},\cdots,{p_N} }  \sum\nolimits_{j} {({p_j}  - \overline{p}  ){{ f}_j}({{\boldsymbol{w}_j}})} \nonumber \vspace{-3mm}\\
{\rm{s.t.}}\;  \sum\nolimits_{j} & |{\frac{{  p_j \!- \!q_j }}{ \widetilde{p}_j  }}|  \! \le \!  \Gamma,\;  - \widetilde{p}_j \! \le \!   p_j \!- \! q_j  \! \le \!  \widetilde{p}_j, \forall{j},\; \sum\nolimits_{j}\! {p_j} \! = \! 1  \\
&{\rm{var.}}\quad \quad \quad \quad \; {p_1},\cdots,{p_N}  \nonumber
\end{align}
where ${\bf{p}}^{t + 1} \! = \! [ p_1^{t + 1},\! \cdots, p_N^{t + 1}]\! \in \! {\mathbb{R}^N} $. Let $\widetilde{{\bf{a}}}^{t + 1} \!= \! {\bf{p}} ^{t + 1} - \overline{{\bf{p}}}  $, where $\overline{{\bf{p}}}    = [\overline{p}  , \cdots ,\overline{p} ] \! \in \! {\mathbb{R}^N}$. This first step aims to obtain the distribution $\widetilde{{\bf{a}}}^{t+1}$  by solving problem in Eq. (\ref{eq:18}). This problem can be effectively solved through combining merge sort \cite{cole1988parallel} (for sorting $\widetilde{p}_j{f}_j({{\boldsymbol{w}_j}}), j\!=\!1,\cdots,N$) with few basic arithmetic operations (for obtaining $p_j^{t + 1}, j\!=\!1,\cdots,N$). Since $N$ is relatively large in distributed system, the arithmetic complexity of solving problem in Eq. (\ref{eq:18}) is dominated by merge sort, which can be regarded as $\mathcal{O}(N\log (N))$.
 
2) Let ${\bf{f}}(\boldsymbol{w}) \! =\! [{{f}_1}(\boldsymbol{w}_1),\cdots , {{f}_N}(\boldsymbol{w}_N)]\!\in\!{\mathbb{R}^N}$, check the feasibility of the following constraints:
\begin{equation}
\label{eq:19}
{\widetilde{{\bf{a}}}^{t+1}}{}^\top {\bf{f}}({\boldsymbol{w}}) \! \le \!\mathop {\max }\limits_{{\boldsymbol{a}_l} \in {{\bf{A}}^t}} {\boldsymbol{a}_l}{}^\top {\bf{f}} (\boldsymbol{w}),
\end{equation}



3) If Eq. (\ref{eq:19}) is violated,  $\widetilde{{\bf{a}}}^{t+1}$ will be added into ${{\rm \bf{A}}^t}$:
\begin{equation}
\label{eq:cutting set}
{{\rm \bf{A}}^{t + 1}} = \left\{ \begin{array}{l}
{{\rm \bf{A}}^t} \cup \{ \widetilde{{\bf{a}}}^{t+1} \},{\rm{ if \;  Eq. (\ref{eq:19}) \; is \; violated }},\\
{{\rm \bf{A}}^t},{\rm{ otherwise}},
\end{array} \right.
\end{equation}
when a new cutting plane is added, its corresponding dual variable ${\lambda _{|{{\bf{A}}^t}| + 1}^{t+1}}=0$ will be generated. After the cutting plane subproblem is solved, the inactive cutting plane will be removed, that is:
\begin{equation}
\label{eq:active}
{{\rm \bf{A}}^{t + 1}} =\left\{ \begin{array}{l}
\complement_{ {{\rm \bf{A}}^{t+1}}} \{{\boldsymbol{a}_l}\} ,{\rm{if}} \; \lambda_l^{t+1}\!=\!0 \, {\rm{and}}\, \lambda_l^{t} \!=\! 0, 1\!\le\!l \!\le\! |{{\bf{A}}^t}|, \\
{{\rm \bf{A}}^{t+1}},{\rm{ otherwise}},
\end{array} \right.
\end{equation}
where $\complement_{ {{\rm \bf{A}}^{t+1}}}\{{\boldsymbol{a}_l}\}$ is the complement of $\{{\boldsymbol{a}_l}\}$ in ${ {{\rm \bf{A}}^{t+1}}}$, and the dual variable will be removed. Then master broadcasts ${{\rm \bf{A}}^{t + 1}}$, $\{ \lambda_l^{t+1} \}$ to all workers.  Details of algorithm are summarized in Algorithm \ref{algorithm1}.




\begin{table*}[t]
{\color{black}\caption{Comparison among different uncertainty sets.}}
\renewcommand\arraystretch{2.2}
\renewcommand\tabcolsep{3pt}
\label{tab:uncertainty set comparison}

{\centering
\scalebox{0.79}{\begin{tabular}{l|c|c|c|c} %几个1表示有几列
\toprule
Uncertainty Set &  $\boldsymbol{\mathcal{P}}$  & Formulation  & Flexibility$^2$  & Complexity$^3$
\\
\hline
Box & $\{ {\bf{p}}:{p_j^{low}} \le {p_j} \le {\rm{ }}{p_j^{upp}},{\bf{1}^ \top }{\bf{p}} = 1\}$  & $\rm{\underline{LP}}^1$   &  2$N$  & $\mathcal{O}(n\log (n))$ \\
\hline
Ellipsoid &  $\boldsymbol{\mathcal{P}} = \{ {\bf{p}}:{({\bf{p}} - {\bf{q}})^T}{{\bf{Q}}^{ - 1}}({\bf{p}} - {\bf{q}}) \le {\beta},{\bf{1}^ \top }{\bf{p}} = 1\}$ &  SOCP  & 1 & $\mathcal{O}((m\!+\!1)\!^{1\!/\!2}n({n^{2}\!+\!m\!+\!\sum\limits_{i = 1}^{m} \!{k_i^2} })\! \log (\frac{1}{{\varepsilon '}}))$  \\
\hline
Polyhedron & $ \{ {\bf{p}}: {\bf{D}}{\bf{p}} \preceq {\bf{c}},{\bf{1}^ \top }{\bf{p}} = 1\}$   &  $\rm{{LP}}$  & ${L_{\rm{in}} \! \times \! ({N\!+\!1})}$ & $\mathcal{O}((m\!+\!n)^{3/2}{n^{2}}\log (\frac{1}{{\varepsilon '}}))$  \\
\hline
KL-Divergence &  $ \{ {\bf{p}}: \sum\limits_{j = 1}^N {{p_j}\log \frac{{{p_j}}}{{{q_j}}}}   \le {\beta},{\bf{1}^ \top }{\bf{p}} = 1\}$  &  REP  &  1 & $\mathcal{O}({(n)^{7/2}}|\log (\varepsilon ')|)$ \\
\hline
Wasserstein-1 Distance & $\{ {\bf{p}}\!:\! \mathop {\min }\limits_{\gamma  \in \Pi ({\bf{p}},{\bf{q}})} \sum\limits_{i = 1}^N \! {\sum\limits_{j = 1}^N \! {\gamma ({x_i},{y_j})||{x_i} - {y_j}||} } \! \le \! {\beta},{\bf{1}^ \top }{\bf{p}}\! =\! 1\}$   &  $\rm{LP}$  & 1  & $\mathcal{O}((m\!+\!n)^{3/2}{n^{2}}\log (\frac{1}{{\varepsilon '}}))$ \\
\hline
$CD$-norm   & $\{ {\bf{p}} \!: \! - \widetilde{p}_j \! \le \!  p_j -q_j \! \le \! \widetilde{p}_j, \!  \sum\limits_{j = 1}^N \!|{\frac{{  p_j -q_j }}{ \widetilde{p}_j  }}| \! \le \! \Gamma   ,  {\bf{1}^ \top }{\bf{p}} = 1  \}$   &  $\rm{\underline{LP}}$    & 1 & $\mathcal{O}(n\log (n))$ \\
\bottomrule
\end{tabular}}}
\\\footnotesize{{\color{black}$^1$ $\rm{\underline{LP}}$ represents the Linear Programming which can be solved by combing merge sort with few basic arithmetic operations.  $^2$ Flexibility denotes the number of parameters that utilized to tradeoff between robustness with performance, lower value represents better flexibility. $^3$ Complexity denotes the arithmetic complexity of solving the cutting plane generation subproblem.}}
\end{table*}






{\color{black}
\section{Uncertainty Sets}
\label{Uncertainty Sets}
In addition to the proposed $CD$-norm uncertainty set, there exist a collection of other uncertainty sets that can be utilized in our framework. In this section, different uncertainty sets that can be used in our framework are discussed. Specifically, we formulate cutting plane generation subproblems for different uncertainty sets, respectively and discuss the arithmetic complexity of solving these cutting plane generation subproblems. Moreover, we focus on whether utilizing different uncertainty sets can flexibly tradeoff between robustness with
performance. 

\subsection{Box Uncertainty Set}
The box uncertainty set was proposed in \cite{soyster1973convex}, which utilizes the box to characterize the uncertainty set and can be expressed as,
\begin{align}
\label{eq:Box}
\boldsymbol{\mathcal{P}} =  \{ {\bf{p}}:{p_j^{low}} \le {p_j} \le {\rm{ }}{p_j^{upp}},{\bf{1}^ \top }{\bf{p}} = 1\}.
\end{align}
where ${p_j^{low}}$ and ${p_j^{upp}}, \forall j$ are preset constants. The interval of every uncertain coefficient is specified by the box uncertainty set, \textit{i.e.}, ${p_j^{low}} \le {p_j} \le {\rm{ }}{p_j^{upp}}$. When utilizing the box uncertainty set, the following cutting plane generation subproblem is required to be solved in the process of updating cutting planes,
\begin{align}
\label{eq:Box_cutting}
& {\bf{p}}^{t+1}  = \mathop {\arg \max }\limits_{  {p_1},\dots,{p_N} }  \sum\limits_{j = 1}^N {({p_j}  - \mathop {{\rm{ }}p}\limits^ -  ){{f}_j}({{\boldsymbol{w}_j}})}\\
&{\rm{s.t.}}\quad \quad  {p_j^{low}} \le {p_j} \le {\rm{ }}{p_j^{upp}} ,\sum\limits_{j = 1}^N {{p_j} }  = 1,  \nonumber\\
&{\rm{var.}}\quad \quad \quad \quad \; {p_1},\dots,{p_N}.  \nonumber
\end{align}

It is seen that the problem in Eq. (\ref{eq:Box_cutting}) is an LP. Similar to the problem in Eq. (\ref{eq:18}), Eq. (\ref{eq:Box_cutting}) can be efficiently solved through combining merge sort (for sorting ${f}_j({{\boldsymbol{w}_j}}), j\!=\!1,\dots,N$) with few basic arithmetic operations (for obtaining $p_j(t\!+\!1), j\!=\!1,\dots,N$). As mentioned before, the arithmetic complexity is  $\mathcal{O}(n\log (n))$, where $n=N$ in this problem. Nevertheless, the box uncertainty set is generally considered to be too conservative, which tends to induce over-conservative decisions \cite{bertsimas2004price,li2011comparative}. And the box uncertainty set cannot flexibly tradeoff between robustness with performance since it is required to adjust 2$N$ parameters (\textit{i.e.}, lower and upper bounds for every $p_j$).

\vspace{2ex}

\subsection{Ellipsoid Uncertainty Set}
We next proceed with discussions on the  ellipsoid uncertainty set. Firstly, the ellipsoid uncertainty set is given by,
\begin{align}
\label{eq:ellipsoid}
\boldsymbol{\mathcal{P}} = \{ {\bf{p}}:{({\bf{p}} - {\bf{q}})^T}{{\bf{Q}}^{ - 1}}({\bf{p}} - {\bf{q}}) \le {\beta},{\bf{1}^ \top }{\bf{p}} = 1\},
\end{align}
where ${\bf{Q}}\!\in\!{\mathbb{R}^{N \times N}}$ is a positive definite matrix. And the ellipsoid is a ball when ${\bf{Q}}$ is an identity matrix (\textit{i.e.}, ${\bf{Q}}={\bf{I}}\in{\mathbb{R}^{N \times N}}$).
The ellipsoid is widely employed to approximate complicated uncertainty sets since it can succinctly describe a set of discrete points in Euclidean geometry \cite{yang2014distributed,boyd2004convex}. Compared with box uncertainty set, the ellipsoid uncertainty set is less conservative, but more computationally more intensive \cite{bertsimas2004price} since it leads to a nonlinear optimization subproblem, as given below.
\begin{align}
\label{eq:ellipsoid_cutting}
& {\bf{p}}^{t+1}  = \mathop {\arg \max }\limits_{{p_1},\dots,{p_N} }  \sum\limits_{j = 1}^N {({p_j}  - \mathop {{\rm{ }}p}\limits^ -  ){{f}_j}({{\boldsymbol{w}_j}})}\\
&{\rm{s.t.}}\quad  {({\bf{p}} - {\bf{q}})^T}{{\bf{Q}}^{ - 1}}({\bf{p}} - {\bf{q}}) \le {\beta},\sum\limits_{j = 1}^N {{p_j} }  = 1,  \nonumber\\
&{\rm{var.}}\quad \quad \quad \quad \; {p_1},\dots,{p_N}.  \nonumber
\end{align}

It is seen that the problem in Eq. (\ref{eq:ellipsoid_cutting}) is a SOCP, which can be solved in polynomial time through interior point method \cite{boyd2004convex}. Specifically, the arithmetic complexity of interior point method to find the $\varepsilon'$-solution for SOCP is upper bounded by $\mathcal{O}((m\!+\!1)^{1/2}n({n^{2}\!+\!m\!+\!\sum\limits_{i = 1}^{m} {k_i^2} })\log (\frac{1}{{\varepsilon '}}))$ \cite{ben2011lectures}, where $m$ and $n$ are respectively the number of inequality constraints and variables, and $k_i$ represents that the $i$-th inequality constraint is a $k_i+1$ dimension second-order cone. In this problem, $n=N$, $m=1$, $k_1=N$. Compared with box uncertainty set, the ellipsoid uncertainty set can flexibly tradeoff between robustness with performance by adjusting a single parameter $\beta$. 

% Figure environment removed


\vspace{2ex}

\subsection{Polyhedron Uncertainty Set}
The form of polyhedron uncertainty set is given by,
\begin{align}
\label{eq:polyhedron}
\boldsymbol{\mathcal{P}} =  \{ {\bf{p}}: {\bf{D}}{\bf{p}} \preceq {\bf{c}}, {\bf{1}^ \top }{\bf{p}} = 1\},
\end{align}
where ${\bf{D}} \in {\mathbb{R}^{L_{\rm{in}}\times N}}$, ${\bf{C}} \in {\mathbb{R}^{L_{\rm{in}}}}$, and $L_{\rm{in}}$ represents the number of linear inequalities. And we use $\preceq$ to denote component-wise inequality. The polyhedron is characterized by a set of linear inequalities, \textit{i.e.}, ${\bf{D}}{\bf{p}} \preceq {\bf{c}}$. Considering the cutting plane generation subproblem with polyhedron uncertainty set, which is required to solve the following problem,
\begin{align}
\label{eq:polyhedron_cutting}
& {\bf{p}}^{t+1}  = \mathop {\arg \max }\limits_{  {p_1},\dots,{p_N} }  \sum\limits_{j = 1}^N {({p_j}  - \mathop {{\rm{ }}p}\limits^ -  ){{f}_j}({{\boldsymbol{w}_j}})}\\
&{\rm{s.t.}}\quad \quad \quad  {\bf{D}}{\bf{p}} \preceq {\bf{c}} ,\sum\limits_{j = 1}^N {{p_j} }  = 1,  \nonumber\\
&{\rm{var.}}\quad \quad \quad \; {p_1},\dots,{p_N}.  \nonumber
\end{align}

The problem in Eq. (\ref{eq:polyhedron_cutting}) is an LP, which can be solved in polynomial time through interior point method \cite{boyd2004convex}. Specifically, the arithmetic complexity for interior point method to find the $\varepsilon'$-solution for LP is upper bounded by $\mathcal{O}((m\!+\!n)^{3/2}{n^{2}}\log (\frac{1}{{\varepsilon '}}))$ \cite{ben2011lectures}, where $m$ and $n$ are the number of inequality constraints and variables, respectively. In Eq. (\ref{eq:polyhedron_cutting}), $m\! =\! L_{\rm{in}}$ and $n\! = \!N$. The polyhedron uncertainty set cannot flexibly tradeoff between robustness with performance since it needs to adjust ${L_{\rm{in}} \! \times \! ({N\!+\!1})}$ parameters, \textit{i.e.}, ${\bf{D}}$ and ${\bf{c}}$.

\vspace{2ex}
\subsection{KL-Divergence Uncertainty Set}
The form of KL-divergence uncertainty set is given by,
\begin{align}
\label{eq:KL}
\boldsymbol{\mathcal{P}} =   \{ {\bf{p}}: \sum\limits_{j = 1}^N {{p_j}\log \frac{{{p_j}}}{{{q_j}}}}   \le {\beta}, {\bf{1}^ \top }{\bf{p}} = 1\} .
\end{align}

Considering the cutting plane generation subproblem with KL-divergence uncertainty set, which is required to solve the following problem,
\begin{align}
\label{eq:KL_cutting}
& {\bf{p}}^{t+1}  = \mathop {\arg \max }\limits_{{p_1},\dots,{p_N} }  \sum\limits_{j = 1}^N {({p_j}  - \mathop {{\rm{ }}p}\limits^ -  ){{f}_j}({{\boldsymbol{w}_j}})}\\
&{\rm{s.t.}}\quad \;  \sum\limits_{j = 1}^N {{p_j}\log \frac{{{p_j}}}{{{q_j}}}}   \le {\beta},\sum\limits_{j = 1}^N {{p_j} }  = 1,  \nonumber\\
&{\rm{var.}}\quad \quad \quad \quad \; {p_1},\dots,{p_N} \nonumber 
\end{align}


The above problem is a relative entropy programming (REP) \cite{chandrasekaran2017relative}. The arithmetic complexity for interior point method to find the $\varepsilon'$-solution for REP is bounded from above by $\mathcal{O}({(n)^{7/2}}|\log (\varepsilon ')|)$ \cite{potra1993quadratically} and $n=N$ in Eq. (\ref{eq:KL_cutting}). KL-divergence uncertainty set can flexibly tradeoff between robustness with performance by adjusting the parameter $\beta$. 


\vspace{2ex}
\subsection{Earth-Mover (Wasserstein-1) Distance  Uncertainty Set}

KL-Divergence cannot deal with the prior distribution with zero elements \cite{qian2019robust} and have many drawbacks, \textit{e.g.}, asymmetry \cite{arjovsky2017wasserstein}. Earth-Mover (Wasserstein-1) distance becomes popular recently which can overcome the aforementioned drawbacks. The Earth-Mover (Wasserstein-1) distance uncertainty set can be expressed as,
\begin{align}
\label{eq:Wasserstein}
\boldsymbol{\mathcal{P}} \! = \!  \{ {\bf{p}}\!:\! \mathop {\min }\limits_{\gamma  \in \Pi ({\bf{p}},{\bf{q}})} \sum\limits_{i = 1}^N {\sum\limits_{j = 1}^N \! {\gamma ({x_i},{y_j})||{x_i} - {y_j}||} } \! \le \! {\beta},{\bf{1}^ \top }{\bf{p}} \!=\! 1\}  ,
\end{align}
where $\Pi ({\bf{p}},{\bf{q}})$ denotes the set of all joint distributions $\gamma $ whose marginal distributions are ${\bf{p}}$ and ${\bf{q}}$, respectively. And $x_i \! = \! i, \; i=1,\dots,N $, $y_j \!=\! j, \; j=1,\dots,N$. Intuitively, $\gamma ({x_i},{y_j})$ denotes the amount of ``mass'' that be moved from worker $i$ to  worker $j$ to transform the distribution ${\bf{p}}$ into the distribution ${\bf{q}}$. Thus, the Earth-Mover (Wasserstein-1) distance can be regarded as the ``cost'' of the optimal transport plan \cite{arjovsky2017wasserstein}. Considering the cutting plane generation subproblem with Earth-Mover (Wasserstein-1) distance uncertainty set, which is required to solve the following problem,
\begin{align}
\label{eq:Wasserstein_cutting}
& {\bf{p}}^{t+1}  = \mathop {\arg \max }\limits_{{p_1},\dots,{p_N}}  \sum\limits_{j = 1}^N {({p_j}  - \mathop {{\rm{ }}p}\limits^ -  ){{f}_j}({{\boldsymbol{w}_j}})}\\
&{\rm{s.t.}}\;   \sum\limits_{i = 1}^N {\sum\limits_{j = 1}^N {\gamma ({x_i},{y_j})||{x_i} - {y_j}||} } \! \le \! {\beta},\sum\limits_{j = 1}^N \! {{p_j} }  = 1,  \nonumber\\
&\quad \sum\limits_{j = 1}^N \! {\gamma ({x_i},{y_j})}  = {p_i},\forall i,  \sum\limits_{i = 1}^N \! {\gamma ({x_i},{y_j})}  = {q_j},\forall j, \nonumber\\
&{\rm{var.}}\quad  {p_1},\dots,{p_N}, {\gamma ({x_1},{y_1})},\dots,{\gamma ({x_N},{y_N})} \nonumber 
\end{align}

The above problem is an LP. As mentioned above, the arithmetic complexity of finding the $\varepsilon'$-solution for problem (\ref{eq:Wasserstein_cutting}) through interior point method is bounded from above by $\mathcal{O}((m+n)^{3/2}{n^{2}}\log (\frac{1}{{\varepsilon '}}))$ \cite{ben2011lectures}, and  $m\! =\! 1$ and $n\! = \!N+N^2$ in this problem. It is seen that problem in Eq. (\ref{eq:Wasserstein_cutting}) is computationally more expensive to be solved since there is a quadratic
number of variables \cite{wong2019wasserstein}. The Earth-Mover (Wasserstein-1) distance uncertainty set can flexibly tradeoff between robustness with performance by adjusting one parameter, \textit{i.e.}, $\beta$. 


Utilizing different uncertainty set results in different arithmetic complexity when solving the corresponding cutting plane generation subproblem. And the arithmetic complexity is also related to the number of workers $N$ (since $n$ in Table \ref{tab:uncertainty set comparison} is related to $N$). It is seen from Figure \ref{fig:complexity} that, the arithmetic complexity of KL-Divergence, ellipsoid, polyhedron and Wasserstein-1 distance uncertainty sets will increase significantly with the number of workers $N$. And the complexity of solving the cutting plane generation subproblem when utilizing Wasserstein-1 distance uncertainty set will increase significantly with the number of workers $N$, since there is a quadratic number of variables (\textit{i.e.}, $n=N+N^2$) in the cutting plane generation subproblem in Eq. (\ref{eq:Wasserstein_cutting}). And it is seen from Figure \ref{fig:complexity} (b) that, the complexity of utilizing ellipsoid uncertainty set will significantly increases when the number of constraints $m$ in cutting plane generation subproblem increases. Moreover, the complexity of utilizing   KL-Divergence, ellipsoid, polyhedron and Wasserstein-1 distance uncertainty sets will also increase quickly when $\varepsilon'$ decreases, which can be seen in Figure \ref{fig:complexity} (c) and (d). As a result, from Figure \ref{fig:complexity} we can conclude that utilizing box and $CD$-norm uncertainty sets is more computationally efficient, especially when the distributed system is large (corresponding to a large $N$).

In summary, we can flexibly choose uncertainty set in different scenes. For instance, the box and $CD$-norm uncertainty sets can be utilized when the master has limited computational capability. And we can choose ellipsoid, KL-divergence, Wasserstein-1 distance and $CD$-norm uncertainty sets when we need to flexibly control the level of robustness. The results of different uncertainty sets are summarized in Table \ref{tab:uncertainty set comparison}.}









\section{Convergence Analysis}\label{convergence analysis}
    



\noindent \textbf{Definition 1} {\rm{(Stationarity gap)}} Following \cite{xu2020unified,lu2020hybrid,jiao2022asynchronous}, the \textit{stationarity} \textit{gap} of our problem at $t^{{th}}$ iteration is defined as:
\begin{equation}
\nabla   G^t \! = \! \left[ \begin{array}{l}
   \{   \frac{1}{{{\alpha _{\boldsymbol{w}}^t}}}  (   {{\boldsymbol{w}}_j^t} \! -\!  {\mathcal{P}_{{\boldsymbol{\mathcal{W}}}}} ( {{\boldsymbol{w}}_j^t}  \! -\!  {\alpha _{\boldsymbol{w}}^t} {\nabla _{{{\boldsymbol{w}}_j}}}       {L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} {\rm{   )  )     \}  } } \vspace{0.3ex}\\
\, \frac{1}{{{\eta _{\boldsymbol{z}}^t}}}   (  {\boldsymbol{z}}^t   \! - \!   {\mathcal{P}_{{\boldsymbol{\mathcal{Z}}}}}(  {\boldsymbol{z}}^t   \! -  \!  {\eta _{\boldsymbol{z}}^t} {\nabla _{\boldsymbol{z}}}       {L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}}  ) ) \vspace{0.3ex}\\
\, \frac{1}{{{\eta _h^t}}}   (   h^t  \!- \! {\mathcal{P}_{{\boldsymbol{\mathcal{H}}}}}(  h^t \! - \! {\eta _h^t} {\nabla _h} {L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}}  ) )  \vspace{0.3ex}\\
\{  \frac{1}{{{\rho _1}}}( {\lambda _l^t}  \! -\!   {\mathcal{P}_{{\boldsymbol{\Lambda}}} }( {\lambda _l^t}  \! + \!   {\rho _1}{\nabla _{{\lambda _l}}} {L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} {\rm{   )  )     \}  } } \vspace{0.3ex}\\
\{   \frac{1}{{{\rho _2}}}( {{\boldsymbol{\phi}}_j^t}  \! - \!  {\mathcal{P}_{{\boldsymbol{\Phi}}}}( {{\boldsymbol{\phi}}_j^t}  \! + \!  {\rho _2}{\nabla _{{{\boldsymbol{\phi}}_j}}} {L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} {\rm{   )  )     \}   } }  \end{array} \right],
\end{equation}
\noindent where $\nabla G^t$ is the simplified form of $\nabla G{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}}$.

 

\noindent \textbf{Definition 2}
{\rm{($\varepsilon$-stationary point)}}  ${\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}}$ is an $\varepsilon$-stationary point ($\varepsilon  \ge 0$) of a differentiable function ${L_p}$,  if $\,||\nabla G^t|| \le \varepsilon $.  $T(\varepsilon )$ is the first iteration index such that $||\nabla G^t|| \! \le \! \varepsilon$, \textit{i.e.}, $T(\varepsilon ) \! = \! \min \{ t \ |\; ||\nabla G^t|| \! \le \! \varepsilon \}  $.



\noindent \textbf{Assumption 1} {\rm{(Smoothness/Gradient Lipschitz)}} $L_p$ has Lipschitz continuous gradients. We assume that there exists $L>0$ satisfying
\begin{equation}
\begin{array}{l}
||{\nabla  _\theta }{L_p}( \{ {{\boldsymbol{w}}_j}\},{\boldsymbol{z}},h, \! \{ {\lambda _l}\}, \!\{ {{\boldsymbol{\phi}}_j}\} ) \! - \! {\nabla  _\theta }{L_p}( \{ {\hat{\boldsymbol{w}}_j}\},\hat{\boldsymbol{z}},\hat{h}, \! \{ {\hat{\lambda }_l}\}, \!\{ {\hat{\boldsymbol{\phi}}_j}\}  )|| \vspace{0.5ex}\\
 \le L||[ {{\boldsymbol{w}}_{\rm{cat}}} \! - \!  {{\hat{\boldsymbol{w}}_{\rm{cat}}}}   ;{\boldsymbol{z}} \! - \!{\hat{\boldsymbol{z}}}  ;h \! - \! \hat{h} ; {\boldsymbol{\lambda} _{\rm{cat}}} \! - \! {{\hat{\boldsymbol{\lambda}} _{\rm{cat}}}}  ; {{\boldsymbol{\phi}}_{\rm{cat}}} \! - \! {{\hat{\boldsymbol{\phi}}_{\rm{cat}}}}  ]||,
\end{array}
\end{equation}
where $\theta \!  \in \! \{ \{ {{\boldsymbol{w}}_j}\} ,{\boldsymbol{z}},h,\{ {\lambda _l}\} ,\{ {{\boldsymbol{\phi}}_j}\} \} $ and $[;]$ represents the concatenation. ${{\boldsymbol{w}}_{\rm{cat}}} \!-\! {{\hat{\boldsymbol{w}}_{\rm{cat}}}}   \!=\![{\boldsymbol{w}}_1 \! - \! {\hat{\boldsymbol{w}}_1};\cdots;{\boldsymbol{w}}_N \! - \! {\hat{\boldsymbol{w}}_N}] \! \in \! {\mathbb{R}^{pN}}$, ${\boldsymbol{\lambda} _{\rm{cat}}} \! - \! {{\hat{\boldsymbol{\lambda}} _{\rm{cat}}}} \!=\! [{\lambda_1} \! - \! {\hat{\lambda}_1};\cdots;{\lambda}_{|{{\bf{A}}^t}|} \! - \! {\hat{\lambda}_{|{{\bf{A}}^t}|}}]\! \in \! {\mathbb{R}^{|{{\bf{A}}^t}|}}$, ${{\boldsymbol{\phi}}_{\rm{cat}}}\!-\!{{\hat{\boldsymbol{\phi}}_{\rm{cat}}}} \!=\![{\boldsymbol{\phi}}_1 \! - \! {\hat{\boldsymbol{\phi}}_1};\cdots;{\boldsymbol{\phi}}_N \! - \! {\hat{\boldsymbol{\phi}}_N}]\! \in \! {\mathbb{R}^{pN}}$.



\noindent \textbf{Assumption 2} {\rm{(Boundedness)}} Before obtaining the $\varepsilon$-stationary point (\textit{i.e.}, $t\!\le\! T(\varepsilon )\!-\!1$), we assume variables in master satisfy that $||\boldsymbol{z}^{t+1} \!-\! \boldsymbol{z}^t|{|^2}\!+\!||h^{t + 1}\! -\! h^t|{|^2}\!+\!\sum\nolimits_l||{\lambda _l^{t + 1}} \!-\! {\lambda _l^t}|{|^2} \ge \vartheta $, where $\vartheta >0$ is a relative small constant. The change of the variables in master is upper bounded within $\tau$ iterations:
\begin{equation}
 \begin{array}{*{20}{l}}
{||\boldsymbol{z}^t - \boldsymbol{z}^{t - k}|{|^2} \! \le \! \tau{k_1}\vartheta}, \;\;
{||h^t - h^{t - k}|{|^2} \! \le \! \tau{k_1}\vartheta},\;\;
{\sum\nolimits_l||{\lambda _l^t} - {\lambda _l^{t - k}}|{|^2} \! \le \! \tau{k_1}\vartheta}, {\forall 1 \! \le \! k \! \le \! \tau },
\end{array}
\end{equation}
where $k_1 >0$ is a constant.

% \centerline{ $\begin{array}{*{20}{l}}
% {||\boldsymbol{z}^t - \boldsymbol{z}^{t - k}|{|^2} \le \tau{k_1}{k_z}}, {\forall 1 \le k \le \tau },\vspace{0.5ex}\\
% {||h^t - h^{t - k}|{|^2} \le \tau{k_2}{k_h}}, {\forall 1 \le k \le \tau },\vspace{0.5ex}\\
% {||{\lambda _l^t} - {\lambda _l^{t - k}}|{|^2} \le \tau{k_3}{k_\lambda }}, {\forall 1 \le k \le \tau },
% \end{array}$}
% where $k_1 >0$, $k_2 >0$ and $k_3 >0$ are constants and ${k_z} = \min \{ ||\boldsymbol{z}^{t+1} - \boldsymbol{z}^t|{|^2},||\boldsymbol{z}^{t - k + 1} - \boldsymbol{z}^{t-k}|{|^2}\} $, ${k_h} = ||h^{t + 1} - h^t|{|^2}$ and ${k_\lambda } = ||{\lambda _l^{t + 1}} - {\lambda _l^t}|{|^2}$.






\noindent \textbf{Setting 1} {\rm{(Bounded $|{{\bf{A}}^t}|$)}} $|{{\bf{A}}^t}| \le M,{\rm{    }}\forall t$, \textit{i.e.}, an upper bound is set for the number of cutting planes.




\noindent \textbf{Setting 2} {\rm{(Setting of ${c_1^t}$, ${c_2^t}$)}}
${c_1^t}\! = \!\frac{1}{{{\rho _1}{(t+1)^{\frac{1}{6}}}}} \!\ge\! \underline{c}_1$ and ${c_2^t} \!=\! \frac{1}{{{\rho _2}{(t+1)^{\frac{1}{6}}}}} \!\ge\! \underline{c}_2$ are nonnegative non-increasing sequences, where $\underline{c}_1$ and $\underline{c}_2$ are positive constants and meet $M{\underline{c}_1}^2+N{\underline{c}_2}^2 \le \frac{{{\varepsilon ^2}}}{4}$.


\vspace{2mm}

\noindent \textbf{Theorem 1} {\rm{(Iteration complexity)}} Suppose Assumption 1 and 2 hold. We set  
${\eta _{\boldsymbol{w}}^t} = {\eta _{\boldsymbol{z}}^t} = {\eta _h^t} = \frac{2}{{L + {\rho _1}|{{\bf{A}}^t}|{L^2} + {\rho _2}N{L^2} + 8(\frac{{|{{\bf{A}}^t}|\gamma {L^2}}}{{{\rho _1}({c_1^t})^2}} + \frac{{N\gamma {L^2}}}{{{\rho _2}({c_2^t})^2}})}}$ and $\underline{\eta _{\boldsymbol{w}}}=\frac{2}{{L + {\rho _1}M{L^2} + {\rho _2}N{L^2} + 8(\frac{{M\gamma {L^2}}}{{\rho _1}{\underline{c}_1}^2} + \frac{{N\gamma {L^2}}}{{\rho _2}{\underline{c}_2}^2})}}$. And we set constants  ${\rho _1} \!< \!\min \{\frac{ 2}{{L + 2c_1^0}}  ,\frac{1}{{15\tau{k_1}N{L^2}}}\} $ and $ \rho _2 \!\le\! \frac{2}{{L + 2c_2^0}} $, respectively. For a given $\varepsilon $, we have:
\begin{equation}
     T( \varepsilon ) \! \sim  \! \mathcal{O}(\max  \{ {(\frac{{4M\!{\sigma _1}^2}}{{{\rho _1}^2}}\! +\! \frac{{4N\!{\sigma _2}^2}}{{{\rho _2}^2}}\!)^3}\!\frac{1}{{{\varepsilon ^6}}}, 
{(\frac{{4{{{(d_6+ \frac{{{\rho _2}(N  -  S){{L}^2}}}{2}\!)}\!}^2}\! (\mathop d\limits^ -  +  k_d(\tau \! - \! 1))  {d_5}}}{{{\varepsilon ^2}}}\! +\! (T_1\!+\!\tau)^{\frac{1}{3}})^3}\}), 
\end{equation}
where ${\sigma _1}$, ${\sigma _2}$, $\gamma$, $k_d$, $\mathop d\limits^ -  $, ${d_5}$, ${d_6}$ and ${T_1}$ are constants. The detailed proof is given in Appendix \ref{appendix:Theorem1}.




There exists a wide array of works
regarding the convergence analysis of various algorithms for nonconvex/convex optimization problems involved in machine learning \cite{jin2020local,xu2021zeroth}.  Our analysis, however, differs from existing works in two aspects.  First, we solve the non-convex PD-DRO in an \emph{asynchronous} \emph{distributed} \emph{manner}. To our best knowledge, there are few works focusing on solving the DRO in a distributed manner. Compared to solving the non-convex PD-DRO in a centralized manner, solving it in an \emph{asynchronous} \emph{distributed} \emph{manner} poses significant challenges in algorithm design and convergence analysis. Secondly, we do not assume the inner problem can be solved nearly optimally for each outer iteration, which is numerically difficult to achieve in practice \cite{bertsekas1997nonlinear}. Instead, ASPIRE-EASE is \emph{single} \emph{loop} and involves simple gradient projection operation at each step.


\begin{table*}[t]
{\color{black}\caption{Unified complexity analysis for different uncertainty sets.}}
\renewcommand\arraystretch{2.2}
\renewcommand\tabcolsep{15pt}
\label{tab:complexity}
\centering
{\scalebox{0.95}{\begin{tabular}{l|c} %几个1表示有几列
\toprule
Uncertainty Set &   Overall Arithmetic Complexity
\\
\hline
Box  & $\mathcal{O}\left(\frac{{\mathcal{K}_1}}{\varepsilon^6}   +  \left\lfloor {\frac{T_1}{k}} \right\rfloor n\log (n)\right)$ \\
\hline
Ellipsoid  & $\mathcal{O}\left(\frac{{\mathcal{K}_1}}{\varepsilon^6}   +  \left\lfloor {\frac{T_1}{k}} \right\rfloor (m+1)^{1/2}n({n^{2}+m+\sum\limits_{i = 1}^{m} {k_i^2} }) \log (\frac{1}{{\varepsilon '}})\right)$  \\
\hline
Polyhedron  & $\mathcal{O}\left(\frac{{\mathcal{K}_1}}{\varepsilon^6}   +  \left\lfloor {\frac{T_1}{k}} \right\rfloor (m+n)^{3/2}{n^{2}}\log (\frac{1}{{\varepsilon '}})\right)$  \\
\hline
KL-Divergence  & $\mathcal{O}\left(\frac{{\mathcal{K}_1}}{\varepsilon^6}   +  \left\lfloor {\frac{T_1}{k}} \right\rfloor{(n)^{7/2}}|\log (\varepsilon ')|\right)$ \\
\hline
Wasserstein-1 Distance   & $\mathcal{O}\left(\frac{{\mathcal{K}_1}}{\varepsilon^6}   +  \left\lfloor {\frac{T_1}{k}} \right\rfloor(m+n)^{3/2}{n^{2}}\log (\frac{1}{{\varepsilon '}})\right)$ \\
\hline
CD-norm    & $\mathcal{O}\left(\frac{{\mathcal{K}_1}}{\varepsilon^6}   +  \left\lfloor {\frac{T_1}{k}} \right\rfloor n\log (n)\right)$ \\
\bottomrule
\end{tabular}}}
\\\footnotesize{{\color{black}${\mathcal{K}_1}$ denotes the arithmetic complexity of gradient projections from Eq. (\ref{eq:15}) to Eq. (\ref{eq:y_update}).}}
\end{table*}

{\color{black}
\section{Unified Complexity Analysis}
\label{Unified Complexity Analysis}
In this section, we extend the proposed ASPIRE-EASE algorithm to different uncertainty sets and make a unified analysis regarding its arithmetic complexity. Recall that according to Theorem 1, the iteration complexity of the proposed method is upper bounded by $\mathcal{O}(\frac{1}{\varepsilon^6})$. In the first $T_1$ iterations, the cutting planes will be updated by solving the cutting plane generation subproblem for every $k$ iterations. Notice that the overall arithmetic complexity of the proposed algorithm is dominated by complexity of the gradient projection operations (from Eq. (\ref{eq:15}) to Eq. (\ref{eq:y_update})) and solving the cutting plane generation subproblem. Consequently, we can obtain the arithmetic complexity of the proposed algorithm with different uncertainty sets are summarized in Table \ref{tab:complexity}.}

{\color{black} 
\section{ASPIRE-ADP}
\label{ASPIRE-ADP}
We next propose ASPIRE-ADP, i.e., ASPIRE-EASE with an adaptive NAW. It is seen that such an adaptive technique can effectively accelerate the converge of the proposed algorithm. The setting of $S$ \textit{i.e.}, the number of active workers, controls the level of asynchrony of ASPIRE-EASE and has a direct impact on the training efficiency. For example, if we set $S=1$ the proposed algorithm is fully asynchronous. Likewise, if we let $S=N$, we obtain a fully synchronous distributed algorithm. Consequently, choosing a proper $S$ is crucial for the training efficiency. For instance, if all workers have almost the same capacities of computation and communication, \textit{i.e.}, each worker has roughly the same computation and communication 
delay, a fully synchronous algorithm, i.e., $S=N$ requires less training rounds \cite{chen2021semi}) and easier to implement \cite{assran2020advances}. And in the other case, when there are stragglers in the distributed system, the algorithm will be more efficient if we set $S<N$ instead of $S=N$ \cite{zhang2014asynchronous}. 

% Figure environment removed

Thus, setting a proper $S$ in the distributed system is crucial. Nevertheless, the delay of some workers may change abrubptly in the process of training. As a result, a fixed NAW may not be the optimal choice, adaptive NAW is more preferred. For example, \cite{su2022gba} has pointed out that switching the mode between synchronous and asynchronous training will enhance the training efficiency. We therefore propose ASPIRE-ADP. Specifically, the $S$ will be updated in master based on the esimated delay information of each worker as follows,
\begin{equation}
S = \left\{ \begin{array}{l}
s,\;{\rm{ if  }}\quad \max \{ \mathcal{T}_j \}  - \min \{ \mathcal{T}_j\}  \le {\beta _1}\\
N,\;{\rm{ if  }}\quad \max \{ \mathcal{T}_j\}  - \min \{ \mathcal{T}_j\}  > {\beta _1}
\end{array} \right.,
\end{equation}
where $1\le s<N$ is an integer, $\beta _1$ is the threshold. $\mathcal{T}_j$ denotes the estimated delay of worker $j$, and master can obtain $\mathcal{T}_j$ based on the communication time interval with worker $j$. We give an example to show how ASPIRE-ADP works, which can be seen in Figure 2. In the experiment, we assume that there are three workers in a distributed system. In the first time interval (time 0$\sim$10s and 40$\sim$50s), all workers have the similar time delay. And in the second time interval (time 10$\sim$40s), there is a straggler (\textit{i.e.}, worker 3), which leads to larger delay than the other two workers. As shown in Figure 2, ASPIRE-ADP can adjust NAW according to the estimated delay information.

Then, we analyze the iteration complexity of ASPIRE-ADP in Theorem 2. With an adaptive NAW, from $T_1+\tau$ iteration to $T( \varepsilon )$ iteration, we assume that the number of iterations when $S=s$ is $\beta_2(T( \varepsilon )-T_1-\tau+1)$ and the number of iterations when $S=N$ is $(1-\beta_2)(T( \varepsilon )-T_1-\tau+1)$, where $ 0\le \beta_2 \le 1$.

\vspace{2mm}

{\color{black}\noindent \textbf{Theorem 2} {\rm{(Iteration complexity)}} Suppose Assumption 1 and 2 hold. We set  
${\eta _{\boldsymbol{w}}^t} = {\eta _{\boldsymbol{z}}^t} = {\eta _h^t} = \frac{2}{{L + {\rho _1}|{{\bf{A}}^t}|{L^2} + {\rho _2}N{L^2} + 8(\frac{{|{{\bf{A}}^t}|\gamma {L^2}}}{{{\rho _1}({c_1^t})^2}} + \frac{{N\gamma {L^2}}}{{{\rho _2}({c_2^t})^2}})}}$ and $\underline{\eta _{\boldsymbol{w}}}=\frac{2}{{L + {\rho _1}M{L^2} + {\rho _2}N{L^2} + 8(\frac{{M\gamma {L^2}}}{{\rho _1}{\underline{c}_1}^2} + \frac{{N\gamma {L^2}}}{{\rho _2}{\underline{c}_2}^2})}}$. And we set constants  ${\rho _1} \!< \!\min \{\frac{ 2}{{L + 2c_1^0}}  ,\frac{1}{{15\tau{k_1}N{L^2}}}\} $ and $ \rho _2 \!\le\! \frac{2}{{L + 2c_2^0}} $, respectively. For a given $\varepsilon $, we have:
\begin{equation}
T( \varepsilon ) \! \sim  \! \mathcal{O}(\max  \{ {(\frac{{4M\!{\sigma _1}^2}}{{{\rho _1}^2}}\! +\! \frac{{4N\!{\sigma _2}^2}}{{{\rho _2}^2}}\!)^3}\!\frac{1}{{{\varepsilon ^6}}}, 
{(\frac{{(\mathop d\limits^ -  + k_d(\tau -1))   {d_5}}}{{({\frac{\beta_2}{{{{d_6 + \frac{{{\rho _2}(N - s){L^2}}}{2}}}}}} + {\frac{1-\beta_2}{{d_6}}}  ){\varepsilon ^2}}} + (T_1+\tau)^{\frac{1}{3}})^3}\}),
\end{equation}
where ${\sigma _1}$, ${\sigma _2}$, $\gamma$, $k_d$, $\mathop d\limits^ -  $, ${d_5}$, ${d_6}$ and ${T_1}$ are constants. The detailed proof is given in Appendix \ref{appendix:Theorem2}. It is seen from Theorem 2 that ASPIRE-ADP can effectively improve the training efficiency and reduce the iteration complexity compared with ASPIRE-EASE.}

}

\section{Experiment} \label{experiment}

In this section, we conduct experiments on four real-world datasets to assess the performance of the proposed method. Specifically, we evaluate the robustness against data heterogeneity, robustness against malicious attacks and efficiency of the proposed method. Ablation study is also carried out to demonstrate the excellent performance of ASPIRE-EASE.
%We conduct our experiments on training deep neural networks over distributed communication networks with multiple workers. We firstly introduce the datasets and baselines in our experiments.

\subsection{Datasets and Baseline Methods}\label{dataset}

We compare the proposed ASPIRE-EASE with baseline methods based on SHL~\cite{gjoreski2018university}, Person Activity~\cite{kaluvza2010agent}, Single Chest-Mounted Accelerometer (SM-AC)~\cite{casale2012personalization} and Fashion MNIST~\cite{xiao2017fashion} datasets. The baseline methods include ${\rm{Ind}}_j$ (learning the model from an individual worker $j$), ${\rm{Mix}}{\rm{_{Even}}}$ (learning the model from all workers with even weights using ASPIRE),  FedAvg \cite{mcmahan2017communication}, AFL \cite{mohri2019agnostic} and DRFA-Prox \cite{deng2021distributionally}. The detailed descriptions of datasets and baselines are given in Appendix \ref{appendix:experiment}.



In our empirical studies, since the downstream tasks are multi-class classification, the cross entropy loss is used on each worker (\textit{i.e.}, ${\mathcal{L}_j}( \cdot ),\forall j$). For SHL, Person Activity, and SM-AC datasets, we adopt the deep multilayer perceptron~\cite{wang2017time} as the base model. And we use the same logistic regression model as in \cite{mohri2019agnostic,deng2021distributionally} for Fashion MNIST dataset. The base models are trained with SGD. Following related works in this direction~\cite{qian2019robust,mohri2019agnostic,deng2021distributionally}, worst case performance are reported for the comparison of robustness. Specifically, we use {${\bf{Acc}}_{w}$} and {${\bf{Loss}}_{w}$} to represent the worst case test accuracy and training loss (\textit{i.e.}, the test accuracy and training loss on the worker with worst performance), respectively. We also report the standard deviation ${\bf{Std}}$ of $[\rm{Acc}_1,\cdots , \rm{Acc}_N]$ (the test accuracy on every worker). In the experiment, $S$ is set as 1, that means the master will make an update once it receives a message.  Each experiment is repeated 10 times, both mean and standard deviations are reported.

\renewcommand\arraystretch{1.3}
\renewcommand\tabcolsep{4pt}
\begin{table}[t]
\caption{Performance comparisons based on {${\bf{Acc}}_{w}$} (\%) $\uparrow$, ${\bf{Loss}}_{w}$ $\downarrow$ and ${\bf{Std}}$ $\downarrow$  ($\uparrow$ and $\downarrow$ respectively denote higher scores represent better performance and lower scores represent better performance). The boldfaced digits represent the best results, ``$-$'' represents not available.}
\centering
\label{tab:III}
\scalebox{0.73}{
\begin{tabular}{l|cccccccccccc}
\toprule
\multirow{2}{*}{Model}&  
    \multicolumn{3}{c}{SHL}&\multicolumn{3}{c}{Person Activity}&\multicolumn{3}{c}{SC-MA}&\multicolumn{3}{c}{Fashion MNIST}\cr  
    \cmidrule(lr){2-4} \cmidrule(lr){5-7} 
    \cmidrule(lr){8-10}
    \cmidrule(lr){11-13}
    & ${\bf{Acc}}_{w}$$\uparrow$  & ${\bf{Loss}}_{w}$$\downarrow$  & {\bf{Std}}$\downarrow$  & ${\bf{Acc}}_{w}$$\uparrow$ & ${\bf{Loss}}_{w}$ $\downarrow$ & {\bf{Std}}$\downarrow$  & ${\bf{Acc}}_{w}$ $\uparrow$  & ${\bf{Loss}}_{w}$ $\downarrow$ & {\bf{Std}}$\downarrow$   & ${\bf{Acc}}_{w}$ $\uparrow$  & ${\bf{Loss}}_{w}$ $\downarrow$ & {\bf{Std}}$\downarrow$ \\ \hline
${\rm{max}}\{{\rm{Ind}}_j\}$  & 19.06±0.65 &  $-$  &  29.1   & 49.38±0.08  & $-$  &  8.32  & 22.56±0.78  & $-$  &   17.5  & $-$   &  $-$ &  $-$   \\
${\rm{Mix}}{\rm{_{Even}}}$   & 69.87±3.10  & 0.806±0.018  &  4.81    & 56.31±0.69  & 1.165±0.017  & 3.00     & 49.81±0.21  &  1.424±0.024 &   6.99   & 66.80±0.18   & 0.784±0.003  &   10.1   \\ 
FedAvg \cite{mcmahan2017communication}  & 69.96±3.07  & 0.802±0.023  &   5.21   & 56.28±0.63  & 1.154±0.019  &   3.13   & 49.53±0.96  & 1.441±0.015  &   7.17  & 66.58±0.39   & 0.781±0.002  &   10.2   \\ 
AFL  \cite{mohri2019agnostic}    & 78.11±1.99   & 0.582±0.021  &   1.87   & 58.39±0.37   &  1.081±0.014 &    0.99   & 54.56±0.79   & 1.172±0.018  &   3.50   & 77.32±0.15   & 0.703±0.001  &   1.86   \\
DRFA-Prox \cite{deng2021distributionally}    & 78.34±1.46   & 0.532±0.034  &   1.85    & 58.62±0.16  & 1.096±0.037  &   1.26   & 54.61±0.76  &  1.151±0.039 &  4.69   & 77.95±0.51 &  0.702±0.007 &   1.34   \\ \hline
ASPIRE-EASE     & \textbf{79.16±1.13}  & \textbf{0.515±0.019}  &  \textbf{1.02}    & 59.43±0.44  &   1.053±0.010 &  0.82    &  56.31±0.29 &  1.127±0.021 &  \textbf{3.16}  &     \textbf{78.82±0.07}  &  \textbf{0.696±0.004} &   \textbf{1.01}   \cr
ASPIRE-EASE$_{\rm{per}}$     &  78.94±1.27    &  0.521±0.023 &  1.36      & \textbf{59.54±0.21}  &   \textbf{1.051±0.016} &  \textbf{0.79}    &  \textbf{56.71±0.16} &  \textbf{1.119±0.028} &  3.48  &     78.73±0.06  &  0.698±0.006 &   1.09 \cr
\bottomrule  
\end{tabular}}
\end{table}



\subsection{Results}

\subsubsection{Robustness against Data Heterogeneity} 
We first assess the robustness of the proposed ASPIRE-EASE by comparing it with baseline methods when data are heterogeneously distributed across different workers. Specifically, we compare the ${\bf{Acc}}_{w}$, ${\bf{Loss}}_{w}$ and ${\bf{Std}}$ of different methods on all datasets. The performance comparison results are shown in Table~\ref{tab:III}.  In this table, we can observe that ${\rm{max}}\{{\rm{Ind}}_j\}$, which represents the best performance of individual training over all workers, exhibits the worst robustness on SHL, Person Activity, and SC-MA. This is because individual training (${\rm{max}}\{{\rm{Ind}}_j\}$) only learns from the data in its local worker and cannot generalize to other workers due to different data distributions. Note that ${\rm{max}}\{{\rm{Ind}}_j\}$ is unavailable for Fashion MNIST since each worker only contains one class of data and cross entropy loss cannot be used in this case. ${\rm{max}}\{{\rm{Ind}}_j\}$ also does not have ${\bf{Loss}}_{w}$, since ${\rm{Ind}}_j$ is trained only on individual worker $j$. The FedAvg and ${\rm{Mix}}{\rm{_{Even}}}$ exhibit better performance than ${\rm{max}}\{{\rm{Ind}}_j\}$ since they consider the data from all workers. Nevertheless, FedAvg and  ${\rm{Mix}}{\rm{_{Even}}}$ only assign the fixed weight for each worker. 
 %The weight of each worker is critical for the robustness of distributed model, the worker with higher loss deserves a higher weight. It is easy to understand the principle, much as a teacher in a class needs to devote more attention to students with low marks in order for the class to have a low fail rate on exams. 
AFL is more robust than FedAvg and  ${\rm{Mix}}{\rm{_{Even}}}$ since it not only utilizes the data from all workers but also considers optimizing the weight of each worker. DRFA-Prox outperforms AFL since it also considers the prior distribution and regards it as a regularizer in the objective function. Finally, we can observe that the proposed ASPIRE-EASE shows excellent robustness, which can be attributed to two factors: 1) ASPIRE-EASE considers data from all workers and can optimize the weight of each worker;
2) compared with DRFA-Prox which uses prior distribution as a regularizer, the prior distribution is incorporated within the constraint in our formulation (Eq. \ref{eq:3}), which can be leveraged more effectively. And it is seen that ASPIRE-EASE can perform periodic communication since ASPIRE-EASE$_{\rm{per}}$, which represents ASPIRE-EASE with periodic communication, also has excellent performance.


Within ASPIRE-EASE, the level of robustness can be controlled by adjusting $\Gamma$. Specially, when $\Gamma=0$, we obtain a nominal optimization problem in which no adversarial distribution is considered. The size of the uncertainty set will increase with $\Gamma$ (when $\Gamma \le N$), which enhances the adversarial robustness of the model. As shown in Figure \ref{fig:gamma}, the robustness of ASPIRE-EASE can be gradually enhanced when  $\Gamma$ increases.

\makeatletter\def\@captype{figure}\makeatother 
\subfigure[Person Activity] %第二张子图
{\begin{minipage}[t]{0.24\linewidth}
	\centering      %子图居中
	% Figure removed   %以pic.jpg的0.5倍大小输出
	\end{minipage}}
\subfigure[SC-MA] %第二张子图
{\begin{minipage}[t]{0.24\linewidth}
	\centering      %子图居中
	% Figure removed   %以pic.jpg的0.5倍大小输出
	\end{minipage}}
\subfigure[SHL] %第二张子图
{\begin{minipage}[t]{0.24\linewidth}
	\centering      %子图居中
	% Figure removed   %以pic.jpg的0.5倍大小输出
	\end{minipage}}
\subfigure[Fashion MNIST] %第二张子图
{\begin{minipage}[t]{0.24\linewidth}
	\centering      %子图居中
	% Figure removed   %以pic.jpg的0.5倍大小输出
	\end{minipage}}
\caption{$\Gamma$ control the degree of robustness (worst case performance in the problem) on  (a) Person Activity, (b) SC-MA, (c) SHL, (d) Fashion MNIST  datasets.} %  %大图名称
\label{fig:gamma}  %图片引用标记


\makeatletter\def\@captype{figure}\makeatother 
\subfigure[Person Activity] %第二张子图
{\begin{minipage}[t]{0.24\linewidth}
	\centering      %子图居中
	% Figure removed   %以pic.jpg的0.5倍大小输出
	\end{minipage}}
\subfigure[SC-MA] %第二张子图
{\begin{minipage}[t]{0.24\linewidth}
	\centering      %子图居中
	% Figure removed   %以pic.jpg的0.5倍大小输出
	\end{minipage}}
\subfigure[SHL] %第二张子图
{\begin{minipage}[t]{0.24\linewidth}
	\centering      %子图居中
	% Figure removed   %以pic.jpg的0.5倍大小输出
	\end{minipage}}
\subfigure[Fashion MNIST] %第二张子图
{\begin{minipage}[t]{0.24\linewidth}
	\centering      %子图居中
	% Figure removed   %以pic.jpg的0.5倍大小输出
	\end{minipage}}
\caption{Comparison of the convergence time on worst case worker on  (a) Person Activity, (b) SC-MA, (c) SHL, (d) Fashion MNIST  datasets.} %  %大图名称
\label{fig:time efficientcy}  %图片引用标记



\makeatletter\def\@captype{figure}\makeatother 
%\setlength{\abovecaptionskip}{-0.5mm} 
\subfigure[Person Activity] %第二张子图
{\begin{minipage}[t]{0.24\linewidth}
	\centering      %子图居中
	% Figure removed   %以pic.jpg的0.5倍大小输出
	\end{minipage}}
\subfigure[SC-MA] %第二张子图
{\begin{minipage}[t]{0.24\linewidth}
	\centering      %子图居中
	% Figure removed   %以pic.jpg的0.5倍大小输出
	\end{minipage}}
\subfigure[SHL] %第二张子图
{\begin{minipage}[t]{0.24\linewidth}
	\centering      %子图居中
	% Figure removed   %以pic.jpg的0.5倍大小输出
	\end{minipage}}
\subfigure[Fashion MNIST] %第二张子图
{\begin{minipage}[t]{0.24\linewidth}
	\centering      %子图居中
	% Figure removed   %以pic.jpg的0.5倍大小输出
	\end{minipage}}
\caption{Comparison of ASPIRE-CP and ASPIRE-EASE regarding the number of cutting planes on  (a) Person Activity, (b) SC-MA, (c) SHL, (d) Fashion MNIST datasets.} %  %大图名称
\label{fig:cutting plane}  %图片引用标记





\subsubsection{Robustness against Malicious Attacks}
To assess the model robustness against malicious attacks, malicious workers with backdoor attacks \cite{bagdasaryan2020backdoor,wang2019neural}, which attempt to mislead the model training process, are added to the distributed system.  Following \cite{dai2019backdoor}, we report the success attack rate of backdoor attacks for comparison. It can be calculated by checking how many instances in the backdoor dataset can be misled and categorized into the target labels. Lower success attack rates indicate more robustness against backdoor attacks.  The comparison results are summarized in Table~\ref{tab:SAR} and more detailed settings of backdoor attacks are available in Appendix \ref{appendix:experiment}.  In Table~\ref{tab:SAR}, we observe that AFL can be attacked easily since it could assign higher weights to malicious workers. Compared to AFL, FedAvg and ${\rm{Mix}}{\rm{_{Even}}}$ achieve relatively lower success attack rates since they assign equal weights to the malicious workers and other workers. DRFA-Prox can achieve even lower success attack rates since it can leverage the prior distribution to assign lower weights for malicious workers.  The proposed ASPIRE-EASE achieves the lowest success attack rates since it can leverage the prior distribution more effectively. Specifically, it will assign lower weights to malicious workers with tight theoretical guarantees.

\renewcommand\tabcolsep{10pt}
\renewcommand\arraystretch{1.4}
\begin{table}[t]
\caption{Performance comparisons about the success attack rate ($\%$) $\downarrow$. The boldfaced digits represent the best results.}
\label{tab:SAR}
\centering
\scalebox{0.85}{
\begin{tabular}{l|cccc}
\toprule
Model    & SHL     & Person Activity         & SC-MA     & Fashion MNIST        \\ \hline
${\rm{Mix}}{\rm{_{Even}}}$ & 36.21±2.23 &  34.32±2.18  & 52.14±2.89   & 83.18±2.07  \\ 
FedAvg \cite{mcmahan2017communication} & 38.15±3.02  & 33.25±2.49 & 55.39±3.13    &  82.04±1.84\\ 
AFL \cite{mohri2019agnostic} & 68.63±4.24  &  43.66±3.87 & 75.81±4.03  &  90.04±2.52 \\
DRFA-Prox \cite{deng2021distributionally} & 21.23±3.63  &  27.27±3.31 & 30.79±3.65  &  63.24±2.47  \\
\hline
ASPIRE-EASE & \textbf{9.17±1.65}  &  \textbf{22.36±2.33}  & \textbf{14.51±3.21}  & \textbf{45.10±1.64}
\cr
\bottomrule  
\end{tabular}}
\end{table}

\subsubsection{Efficiency}
In Figure \ref{fig:time efficientcy}, we compare the convergence speed of the proposed ASPIRE-EASE with AFL and DRFA-Prox by considering different communication and computation delays for each worker. The proposed ASPIRE-EASE has two variants, ASPIRE-CP (ASPIRE with cutting plane method), ASPIRE-EASE(-)(ASPIRE-EASE without asynchronous setting). Based on the comparison, we can observe that the proposed ASPIRE-EASE generally converges faster than baseline methods and its two variants. This is because 1) compared with AFL, DRFA-Prox, and ASPIRE-EASE(-), ASPIRE-EASE is an asynchronous algorithm in which the master updates its parameters only after receiving the updates from active workers instead of all workers; 2) unlike DRFA-Prox, the master in ASPIRE-EASE only needs to communicate with active workers once per iteration; 3) compared with ASPIRE-CP, ASPIRE-EASE utilizes active set method instead of cutting plane method, which is more efficient. It is seen from Figure \ref{fig:time efficientcy} that, the convergence speed of ASPIRE-EASE mainly benefits from the asynchronous setting.


\subsubsection{Ablation Study}
For ASPIRE, compared with cutting plane method, EASE is more efficient since it considers removing the inactive cutting planes. To demonstrate the efficiency of EASE, we firstly compare ASPIRE-EASE with ASPIRE-CP concerning the number of cutting planes used during the training. In Figure \ref{fig:cutting plane}, we can observe that ASPIRE-EASE uses fewer cutting planes than ASPIRE-CP, thus is more efficient. The convergence speed of ASPIRE-EASE and ASPIRE-CP in Figure \ref{fig:time efficientcy} also suggests that ASPIRE-EASE converges much faster than ASPIRE-CP. 


\section{Conclusion} \label{conclusion}

In this paper, we present ASPIRE-EASE method to effectively solve the distributed distributionally robust optimization problem with non-convex objectives. In addition, $CD$-norm uncertainty set has been proposed to effectively incorporate the prior distribution into the problem formulation, which allows for flexible adjustment of the degree of robustness of DRO. Theoretical analysis has also been conducted to analyze the convergence properties and the iteration complexity of ASPIRE-EASE. ASPIRE-EASE exhibits strong empirical performance on multiple real-world datasets and is effective in tackling DRO problems in a fully distributed and asynchronous manner. In the future work, more uncertainty sets could be designed for our framework and more update rule for variables in ASPIRE could be considered.

% Acknowledgements should go at the end, before appendices and references
% Manual newpage inserted to improve layout of sample file - not
% needed in general before appendices/bibliography.

%Bibliography
\bibliographystyle{unsrt}  
\bibliography{references}  


\newpage

\appendix
%\section*{Appendix A.}
\section{Proof of Theorem 1}
\label{appendix:Theorem1}

Before proceeding to the detailed proofs, we provide some notations for the clarity in presentation. We use notation $<  \cdot , \cdot  >$ to denote the inner product and we use $||\cdot||$ to denote the $l_2$-norm. $|{{\bf{A}}^t}|$ and $|{{\bf{Q}}^{t + 1}}|$ respectively denote the number of cutting planes and active workers in $(t+1)^{\rm{th}}$ iteration.

Then, we cover some Lemmas which are useful for the deduction of Theorem 1.


\begin{lemma} \label{lemma 1}
Suppose Assumption 1 and 2 hold, $\forall t \ge T_1+\tau$, we have,
\begin{equation}
\label{eq:A.a111}
\begin{array}{l}
{L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\} ,}}\boldsymbol{z}^t,h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} \!-\! {L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}\boldsymbol{z}^t,h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}}\vspace{0.5ex}\\
\! \le \! \sum\limits_{j  = 1}^N \! {(\frac{{L  +  1}}{2} \!-\! \frac{1}{{{\eta _{\boldsymbol{w}}^t}}})||{{\boldsymbol{w}}_j^{t+1}} \!-\! {{\boldsymbol{w}}_j^t}|{|^2}}  \! + \! \frac{{3\tau{ k_1 }N{L^2}}}{2}(||\boldsymbol{z}^{t+1} \!-\! \boldsymbol{z}^t|{|^2} \! + \! ||h^{t+1} \!-\! h^t|{|^2} \! + \!  \! \sum\limits_{l = 1}^{|{{\bf{A}}^t}|}\! {||{\lambda _l^{t+1}} \!-\! {\lambda _l^t}|{|^2}}), 
\end{array}
\end{equation}
\begin{equation}
\label{eq:A.a12}
\begin{array}{l}
{L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\} ,}}{\boldsymbol{z}}^{t+1},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} \!-\! {L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} 
 \le (\frac{L}{2} \!-\! \frac{1}{{{\eta _{\boldsymbol{z}}^t}}})||{\boldsymbol{z}}^{t+1} \!-\! {\boldsymbol{z}^t}|{|^2},
\end{array}
\end{equation}
\begin{equation}
\label{eq:A.a13}
\begin{array}{l}
{L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\} ,}}{\boldsymbol{z}}^{t+1},h^{t+1},\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} \!-\! {L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\} ,}}{\boldsymbol{z}}^{t+1},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}}
 \le (\frac{L}{2} \!-\! \frac{1}{{{\eta _h^t}}})||h^{t+1} \!-\! h^t|{|^2}.
\end{array}
\end{equation}
\end{lemma}

\noindent \emph{\textbf{Proof of Lemma \ref{lemma 1}}:} 

According to Assumption 1, we have, 
\begin{equation}
\label{eq:A4-1}
\begin{array}{l}
{L_p}{\rm{(\{ }}{{\boldsymbol{w}}_1^{t+1}},{{\boldsymbol{w}}_2^t}, \cdots ,{{\boldsymbol{w}}_N^t}{\rm{\} ,}}\boldsymbol{z}^t,h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} \!-\! {L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}\boldsymbol{z}^t,h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}}\vspace{1.5ex}\\
 \le \left\langle {{\nabla _{{{\boldsymbol{w}}_1}}}{L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}\boldsymbol{z}^t,h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{),}}{{\boldsymbol{w}}_1^{t+1}} \!-\! {{\boldsymbol{w}}_1^t}} \right\rangle  \! + \! \frac{L}{2}||{{\boldsymbol{w}}_1^{t+1}} \!-\! {{\boldsymbol{w}}_1^t}|{|^2}, \vspace{3ex}\\
{L_p}{\rm{(\{ }}{{\boldsymbol{w}}_1^{t+1}},{{\boldsymbol{w}}_2^{t+1}}, {{\boldsymbol{w}}_3^t}, \cdots ,{{\boldsymbol{w}}_N^t}{\rm{\} ,}}\boldsymbol{z}^t,h^t,\!\{ {\lambda _l^t}\} ,\!\{ {{\boldsymbol{\phi}}_j^t}\} )
\!-\! {L_p}{\rm{(\{ }}{{\boldsymbol{w}}_1^{t+1}},{{\boldsymbol{w}}_2^t}, \cdots ,{{\boldsymbol{w}}_N^t}{\rm{\} ,}}\boldsymbol{z}^t,h^t,\!\{ {\lambda _l^t}\} ,\!\{ {{\boldsymbol{\phi}}_j^t}\})  \vspace{1.5ex}\\
 \le \left\langle {{\nabla _{{{\boldsymbol{w}}_2}}}{L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}\boldsymbol{z}^t,h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{),}}{{\boldsymbol{w}}_2^{t+1}} \!-\! {{\boldsymbol{w}}_2^t}} \right\rangle  \! + \! \frac{L}{2}||{{\boldsymbol{w}}_2^{t+1}} \!-\! {{\boldsymbol{w}}_2^t}|{|^2}, \vspace{2ex}\\
\qquad \qquad \qquad  \qquad \qquad \qquad \qquad \qquad \qquad \vdots  \vspace{2ex}\\
{L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\} ,}}\boldsymbol{z}^t,h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} \!-\! {L_p}{\rm{(\{ }}{{\boldsymbol{w}}_1^{t+1}}, \cdots ,{w_{N \!-\! 1}^{t+1}},{{\boldsymbol{w}}_N^t}{\rm{\} ,}}\boldsymbol{z}^t,h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} \vspace{1.5ex}\\
 \le \left\langle {{\nabla _{{{\boldsymbol{w}}_N}}}{L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}\boldsymbol{z}^t,h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{),}}{{\boldsymbol{w}}_N^{t+1}} \!-\! {{\boldsymbol{w}}_N^t}} \right\rangle  \! + \! \frac{L}{2}||{{\boldsymbol{w}}_N^{t+1}} \!-\! {{\boldsymbol{w}}_N^t}|{|^2}.
\end{array}
\end{equation}

\vspace{0.5ex}

Summing up the above inequalities in Eq. (\ref{eq:A4-1}), we have,
\begin{equation}
\label{eq:A4}
\begin{array}{l}
{L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} \!-\! {L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}}\\
 \le \sum\limits_{j  = 1}^N {(\left\langle {{\nabla _{{{\boldsymbol{w}}_j}}}{L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{),}}{{\boldsymbol{w}}_j^{t+1}} \!-\! {{\boldsymbol{w}}_j^t}} \right\rangle  \! + \! \frac{L}{2}||{{\boldsymbol{w}}_j^{t+1}} \!-\! {{\boldsymbol{w}}_j^t}|{|^2})}.
\end{array}
\end{equation}



According to ${\nabla _{{{\boldsymbol{w}}_j}}}{L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} = {\nabla _{{{\boldsymbol{w}}_j}}}{\widetilde{L}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}}$ and the optimal condition for Eq. (11), for active nodes, \textit{i.e.}, $\forall j \in {{\bf{Q}}^{t + 1}}, \forall t\ge T_1+\tau$, we have,
\begin{equation}
\label{eq:A5}
\left\langle {{{\boldsymbol{w}}_j^t} - {{\boldsymbol{w}}_j^{t+1}},{{\boldsymbol{w}}_j^{t+1}} - {{\boldsymbol{w}}_j^t}  +  {\eta _{\boldsymbol{w}}^{\widetilde{{t}}_j+\tau}}{\nabla _{{{\boldsymbol{w}}_j}}}{L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{\widetilde{{t}}_j}}{\rm{\} ,}}{\boldsymbol{z}}^{{\widetilde{{t}}_j}},h^{{\widetilde{{t}}_j}},\{ {\lambda _l^{\widetilde{{t}}_j}}\} ,\{ {{\boldsymbol{\phi}}_j^{\widetilde{{t}}_j}}\} {\rm{)}}} \right\rangle  \ge 0.
\end{equation}

According to Eq. (\ref{eq:A5}), $\forall t\ge T_1+\tau$, we have,
\begin{equation}
\label{eq:A5-2}
\left\langle {{{\boldsymbol{w}}_j^{t+1}} \!-\! {{\boldsymbol{w}}_j^t},{\nabla _{{{\boldsymbol{w}}_j}}}{L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{\widetilde{{t}}_j}}{\rm{\} ,}}{\boldsymbol{z}}^{{\widetilde{{t}}_j}},h^{{\widetilde{{t}}_j}},\{ {\lambda _l^{\widetilde{{t}}_j}}\} ,\{ {{\boldsymbol{\phi}}_j^{\widetilde{{t}}_j}}\} {\rm{)}}} \right\rangle \! \le\!  -\frac{1}{{{\underline{\eta _{\boldsymbol{w}}}}}}||{{\boldsymbol{w}}_j^{t+1}} - {{\boldsymbol{w}}_j^t}|{|^2} \!\le\!  -\frac{1}{{{\eta _{\boldsymbol{w}}^t}}}||{{\boldsymbol{w}}_j^{t+1}} - {{\boldsymbol{w}}_j^t}|{|^2}.
\end{equation}

And according to the Cauchy-Schwarz inequality, Assumption 1 and 2, we can get,
\begin{equation}
\label{eq:A5-3}
\begin{array}{l}
\left\langle {{\boldsymbol{w}}_j^{t+1} \!-\! {\boldsymbol{w}}_j^t,{\nabla _{{{\boldsymbol{w}}_j}}}{L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\} , \{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} \!-\! {\nabla _{{{\boldsymbol{w}}_j}}}{L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{\widetilde{{t}}_j}}{\rm{\} ,}}{\boldsymbol{z}^{{\widetilde{{t}}_j}}},h^{{\widetilde{{t}}_j}}, \{ {\lambda _l^{\widetilde{{t}}_j}}\} , \!\{ {{\boldsymbol{\phi}}_j^{\widetilde{{t}}_j}}\} {\rm{)}}} \right\rangle \\
\! \le \! \frac{1}{2}||{{\boldsymbol{w}}_j^{t+1}} \!-\! {{\boldsymbol{w}}_j^t}|{|^2} \! + \! \frac{L^2}{2}(||{\boldsymbol{z}^t} \!-\! {\boldsymbol{z}^{{\widetilde{{t}}_j}}}|{|^2} \! + \! ||h^t \!-\! h^{{\widetilde{{t}}_j}}|{|^2} \! + \! \sum\limits_{l = 1}^{|{{\bf{A}}^t}|} {||{\lambda _l^t} \!-\! {\lambda _l^{\widetilde{{t}}_j}}|{|^2}} )\\
\! \le \! \frac{1}{2}||{{\boldsymbol{w}}_j^{t+1}} \!-\! {{\boldsymbol{w}}_j^t}|{|^2} \! + \! \frac{{{3\tau{ k_1 }L^2}}}{2}(||{\boldsymbol{z}}^{t+1} \!-\! {\boldsymbol{z}^t}|{|^2} \! + \! ||h^{t+1} \!-\! h^t|{|^2} \! + \! \sum\limits_{l = 1}^{|{{\bf{A}}^t}|} {||{\lambda _l^{t+1}} \!-\! {\lambda _l^t}|{|^2}} ).
\end{array}
\end{equation}

Combining the above Eq. (\ref{eq:A4}), (\ref{eq:A5-2}) with Eq.  (\ref{eq:A5-3}), we can obtain Eq. (\ref{eq:A.a111}), that is,
\begin{equation*}
\begin{array}{l}
{L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\} ,}}\boldsymbol{z}^t,h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} \!-\! {L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}\boldsymbol{z}^t,h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}}\vspace{0.5ex}\\
\! \le \! \sum\limits_{j  = 1}^N \! {(\frac{{L  +  1}}{2} \!-\! \frac{1}{{{\eta _{{\boldsymbol{w}}}^t}}})||{{\boldsymbol{w}}_j^{t+1}} \!-\! {{\boldsymbol{w}}_j^t}|{|^2}}  \! + \! \frac{{3\tau{ k_1 }N{L^2}}}{2}(||\boldsymbol{z}^{t+1} \!-\! \boldsymbol{z}^t|{|^2} \! + \! ||h^{t+1} \!-\! h^t|{|^2} \! + \! \sum\limits_{l = 1}^{|{{\bf{A}}^t}|}\! {||{\lambda _l^{t+1}} \!-\! {\lambda _l^t}|{|^2}}).
\end{array}
\end{equation*}


\vspace{0.5ex}

Following Assumption 1, we have,
\begin{equation}
\label{eq:A6}
\begin{array}{l}
{L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\} ,}}{\boldsymbol{z}}^{t+1},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} \!-\! {L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} \vspace{1ex}\\
 \le \left\langle {{\nabla _{\boldsymbol{z}}}{L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}},{\boldsymbol{z}}^{t+1} \!-\! {\boldsymbol{z}^t}} \right\rangle  \! + \! \frac{L}{2}||{\boldsymbol{z}}^{t+1} \!-\! {\boldsymbol{z}^t}|{|^2}.
\end{array}
\end{equation}

According to ${\nabla _{\boldsymbol{z}}}{L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} = {\nabla _{\boldsymbol{z}}}{\widetilde{L}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}}$ and the optimal condition for Eq. (12), we have,
\begin{equation}
\label{eq:A7}
\left\langle {{\boldsymbol{z}^t} \!-\! {\boldsymbol{z}}^{t+1},{\boldsymbol{z}}^{t+1} \!-\! {\boldsymbol{z}^t} \! + \! {\eta _{\boldsymbol{z}}^t}{\nabla _{\boldsymbol{z}}}{L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}}} \right\rangle  \ge 0.
\end{equation}

Combining Eq.  (\ref{eq:A6}) with Eq.  (\ref{eq:A7}), we can obtain the Eq. (\ref{eq:A.a12}), that is,
\begin{equation*}
\begin{array}{l}
{L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\} ,}}{\boldsymbol{z}}^{t+1},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} \!-\! {L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}}
 \le (\frac{L}{2} \!-\! \frac{1}{{{\eta _{\boldsymbol{z}}^t}}})||{\boldsymbol{z}}^{t+1} \!-\! {\boldsymbol{z}^t}|{|^2}.
\end{array}
\end{equation*}

\vspace{0.5ex}

According to Assumption 1, we have:
\begin{equation}
\label{eq:A8}
\begin{array}{l}
{L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\} ,}}{\boldsymbol{z}}^{t+1},h^{t+1},\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} \!-\! {L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\} ,}}{\boldsymbol{z}}^{t+1},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} \vspace{1ex}\\
 \le \left\langle {{\nabla _h}{L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\} ,}}{\boldsymbol{z}}^{t+1},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}},h^{t+1} \!-\! h^t} \right\rangle  \! + \! \frac{L}{2}||h^{t+1} \!-\! h^t|{|^2}.
\end{array}
\end{equation}


According to ${\nabla _h}{L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\} ,}}{\boldsymbol{z}}^{t+1},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} = {\nabla _h}{\widetilde{L}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\} ,}}{\boldsymbol{z}}^{t+1},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}}$ and the optimal condition for Eq. (13), we have:
\begin{equation}
\label{eq:A9}
\left\langle {h^t \!-\! h^{t+1},h^{t+1} \!-\! h^t \! + \! {\eta _h^t}{\nabla _h}{L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\} ,}}{\boldsymbol{z}}^{t+1},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}}} \right\rangle  \ge 0.
\end{equation}

Combining Eq.  (\ref{eq:A8}) with Eq.  (\ref{eq:A9}), we can show that,
\begin{equation*}
\begin{array}{l}
{L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\} ,}}{\boldsymbol{z}}^{t+1},h^{t+1},\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} \!-\! {L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\} ,}}{\boldsymbol{z}}^{t+1},h^t,\!\{ {\lambda _l^t}\} ,\!\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}}
 \!\le\! (\frac{L}{2} \!-\! \frac{1}{{{\eta _h^t}}})||h^{t+1} \!-\! h^t|{|^2}.
\end{array}
\end{equation*}

\vspace{2ex}

\begin{lemma} \label{lemma 2}
Suppose Assumption 1 and 2 hold, $\forall t \ge T_1 + \tau$, we have:
\begin{equation}
\label{eq:A10}
\begin{array}{l}
{L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\} ,}}\boldsymbol{z}^{t+1},h^{t+1},\{ {\lambda _l^{t+1}}\} ,\{ {{\boldsymbol{\phi}}_j^{t+1}}\} {\rm{)}} \!-\! {L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}\boldsymbol{z}^t,h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}}\vspace{0.5ex}\\
\! \le \! (\frac{{L \! + \! 1}}{2} \!-\! \frac{1}{{{\eta _{\boldsymbol{w}}^t}}} \! + \! \frac{{|{{\bf{A}}^t}|{L^2}}}{{2{a_1}}} \! + \! \frac{{|{{\bf{Q}}^{t \! + \! 1}}|{L^2}}}{{2{a_3}}})\!\sum\limits_{j = 1}^N \!{||{{\boldsymbol{w}}_j^{t+1}} \!-\! {{\boldsymbol{w}}_j^t}|{|^2}}  \! + \! (\frac{{L \! + \! 3\tau{ k_1 }N{L^2}}}{2} \!-\! \frac{1}{{{\eta _{\boldsymbol{z}}^t}}} \! + \! \frac{{|{{\bf{A}}^t}|{L^2}}}{{2{a_1}}} \! + \! \frac{{|{{\bf{Q}}^{t \! + \! 1}}|{L^2}}}{{2{a_3}}})||\boldsymbol{z}^{t+1} \!-\! \boldsymbol{z}^t|{|^2}\vspace{0.5ex}\\
 \! +  (\frac{{L \! + \! 3\tau{ k_1 }N{L^2}}}{2} \!-\! \frac{1}{{{\eta _h^t}}} \! + \! \frac{{|{{\bf{A}}^t}|{L^2}}}{{2{a_1}}} \! + \! \frac{{|{{\bf{Q}}^{t \! + \! 1}}|{L^2}}}{{2{a_3}}})||h^{t+1} \!-\! h^t|{|^2} \! + \! (\frac{{{a_1} \! + \! 3\tau{ k_1 }N{L^2}}}{2} \!-\! \frac{{{c_1^{t-1}} \!-\! {c_1^t}}}{2} \! + \! \frac{1}{{2{\rho _1}}})\!\sum\limits_{l = 1}^{|{{\bf{A}}^t}|}\! {||{\lambda _l^{t+1}} \!-\! {\lambda _l^t}|{|^2}} \vspace{0.5ex}\\
 
 \! +  \frac{{{c_1^{t-1}}}}{2} \! \sum\limits_{l = 1}^{|{{\bf{A}}^t}|}\! {(||{\lambda _l^{t+1}}|{|^2} \!-\! ||{\lambda _l^t}|{|^2})}  \! + \! \frac{1}{{2{\rho _1}}} \! \sum\limits_{l = 1}^{|{{\bf{A}}^t}|}\! {||{\lambda _l^t} \!-\! {\lambda _l^{t-1}}|{|^2}} \! + \! (\frac{{{a_3}}}{2} \!-\! \frac{{{c_2^{t-1}} \!-\! {c_2^t}}}{2} \! + \! \frac{1}{{2{\rho _2}}}) \! \sum\limits_{j = 1}^N\! {||{{\boldsymbol{\phi}}_j^{t+1}} \!-\! {{\boldsymbol{\phi}}_j^t}|{|^2}} \vspace{0.5ex}\\
 
 \! +  \frac{{{c_2^{t-1}}}}{2}\sum\limits_{j = 1}^N {(||{{\boldsymbol{\phi}}_j^{t+1}}|{|^2} \!-\! ||{{\boldsymbol{\phi}}_j^t}|{|^2})}  \! + \! \frac{1}{{2{\rho _2}}}\sum\limits_{j = 1}^N {||{{\boldsymbol{\phi}}_j^t} \!-\! {{\boldsymbol{\phi}}_j^{t-1}}|{|^2}}, 
\end{array}
\end{equation}
where $a_1 >0$ and $a_3 >0$ are constants.
\end{lemma}

\noindent \emph{\textbf{Proof of Lemma \ref{lemma 2}}:} 

First of all, at $(t+1)^{\rm{th}}$ iteration, the following equations hold and  will be used in the derivation:
\begin{equation*}
\sum\limits_{j = 1}^N \! {||{{\boldsymbol{\phi}}_j^{t+1}} \!-\! {{\boldsymbol{\phi}}_j^t}|{|^2}} \! = \! \sum\limits_{j \in {{\bf{Q}}^{t \! + \! 1}}}\! {||{{\boldsymbol{\phi}}_j^{t+1}} \!-\! {{\boldsymbol{\phi}}_j^t}|{|^2}}, \;
\sum\limits_{j = 1}^N \! {(||{{\boldsymbol{\phi}}_j^{t+1}}|{|^2} \!-\! ||{{\boldsymbol{\phi}}_j^t}|{|^2})} \! = \! \sum\limits_{j \in {{\bf{Q}}^{t + 1}}}\! {(||{{\boldsymbol{\phi}}_j^{t+1}}|{|^2} \!-\! ||{{\boldsymbol{\phi}}_j^t}|{|^2})}. 
\end{equation*}

According to Eq. (14), in $(t+1)^{\rm{th}}$ iteration, $\forall \lambda  \in {\bf{\Lambda}} $, it follows that:
\begin{equation}
\label{eq:A11}
\left\langle {{\lambda _l^{t+1}} \!-\! {\lambda _l^t} \!-\! {\rho _1}{\nabla _{{\lambda _l}}}{{\widetilde{L}}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\} ,}}{\boldsymbol{z}}^{t+1},h^{t+1},\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}},\lambda  \!-\! {\lambda _l^{t+1}}} \right\rangle  \ge 0.
\end{equation}

Let $\lambda  = {\lambda _l^t}$, we can obtain:
\begin{equation}
\label{eq:A12}
\left\langle {{\nabla _{{\lambda _l}}}{{\widetilde{L}}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\} ,}}{\boldsymbol{z}}^{t+1},h^{t+1},\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} \!-\! \frac{1}{{{\rho _1}}}({\lambda _l^{t+1}} \!-\! {\lambda _l^t}),{\lambda _l^t} \!-\! {\lambda _l^{t+1}}} \right\rangle  \le 0.
\end{equation}

Likewise, in $t^{\rm{th}}$ iteration, we can obtain:
\begin{equation}
\label{eq:A12}
\left\langle {{\nabla _{{\lambda _l}}}{{\widetilde{L}}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^{t-1}}\} ,\{ {{\boldsymbol{\phi}}_j^{t-1}}\} {\rm{)}} \!-\! \frac{1}{{{\rho _1}}}({\lambda _l^t} \!-\! {\lambda _l^{t-1}}),{\lambda _l^{t+1}} \!-\! {\lambda _l^t}} \right\rangle  \le 0.
\end{equation}

$\forall t \ge T_1$, since ${\widetilde{L}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j}{\rm{\} ,}}{\boldsymbol{z}},h,\!\{ {\lambda _l}\} ,\!\{ {{\boldsymbol{\phi}}_j}\} {\rm{)}}$ is concave with respect to ${\lambda _l}$, we have,
\begin{equation}
\label{eq:A14}
\begin{array}{l}
{\widetilde{L}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\} ,}}{\boldsymbol{z}}^{t+1},h^{t+1},\{ {\lambda _l^{t+1}}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} - {\widetilde{L}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\} ,}}{\boldsymbol{z}}^{t+1},h^{t+1},\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}}\vspace{1ex}\\
\! \le \! \sum\limits_{l = 1}^{|{{\bf{A}}^t}|} \! {\left\langle \! {{\nabla _{{\lambda _l}}}{{\widetilde{L} }_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\} ,}}{\boldsymbol{z}}^{t+1},h^{t+1},\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}},{\lambda _l^{t+1}} - {\lambda _l^t}} \! \right\rangle } \\
\! \le \! \sum\limits_{l  =  1}^{|{{\bf{A}}^t}|} \! {( \! \left\langle \! {{\nabla _{{\lambda _l}}}{{\widetilde{L} }_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\} , }}{\boldsymbol{z}}^{t+1},h^{t+1},\!\{ {\lambda _l^t}\} ,\! \{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} \! -\! {\nabla _{{\lambda _l}}}{{\widetilde{L}}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\},}}{\boldsymbol{z}^t},h^t,\! \{ {\lambda _l^{t-1}}\}, \!\{ {{\boldsymbol{\phi}}_j^{t-1}}\} {\rm{)}},{\lambda _l^{t+1}}\! -\! {\lambda _l^t}} \! \right\rangle } \vspace{0.5ex}\\
\qquad \quad  + \frac{1}{{{\rho _1}}}\left\langle {{\lambda _l^t} - {\lambda _l^{t-1}},{\lambda _l^{t+1}} - {\lambda _l^t}} \right\rangle).
\end{array}
\end{equation}


Denoting ${{\boldsymbol{v}}_{1,l}^{t + 1}} = {\lambda _l^{t+1}} - {\lambda _l^t} - ({\lambda _l^t} - {\lambda _l^{t-1}})$, we have,
\begin{equation}
\label{eq:A15}
\begin{array}{l}
\sum\limits_{l = 1}^{|{{\bf{A}}^t}|} \! {\left\langle \! {{\nabla _{{\lambda _l}}}{{\widetilde{L}}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\},}}{\boldsymbol{z}}^{t+1},h^{t+1}, \! \{ {\lambda _l^t}\} , \{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}}  \! -\! {\nabla _{{\lambda _l}}}{{\widetilde{L} }_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t, \! \{ {\lambda _l^{t-1}}\} , \! \{ {{\boldsymbol{\phi}}_j^{t-1}}\} {\rm{)}},{\lambda _l^{t+1}}  \! -\! {\lambda _l^t}} \! \right\rangle } \\
 \!= \! \sum\limits_{l = 1}^{|{{\bf{A}}^t}|} \! {\left\langle \! {{\nabla _{{\lambda _l}}}{{\widetilde{L}}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\},}}{\boldsymbol{z}}^{t+1},h^{t+1},\!\{ {\lambda _l^t}\},\!\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}}  \! -\! {\nabla _{{\lambda _l}}}{{\widetilde{L}}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t, \! \{ {\lambda _l^t}\} , \! \{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}},{\lambda _l^{t+1}}  \! -\! {\lambda _l^t}} \! \right\rangle } (1a)\\
  \! +\! \sum\limits_{l = 1}^{|{{\bf{A}}^t}|} \! {\left\langle \! {{\nabla _{{\lambda _l}}}{{\widetilde{L}}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t, \! \{ {\lambda _l^t}\}, \! \{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}}  \! -\! {\nabla _{{\lambda _l}}}{{\widetilde{L}}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t, \! \{ {\lambda _l^{t-1}}\} , \! \{ {{\boldsymbol{\phi}}_j^{t-1}}\} {\rm{)}}, {{\boldsymbol{v}}_{1,l}^{t + 1}}} \right\rangle } (1b)\\
  \! +\! \sum\limits_{l = 1}^{|{{\bf{A}}^t}|} \! {\left\langle {{\nabla _{{\lambda _l}}}{{\widetilde{L}}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\!\{ {\lambda _l^t}\} ,\!\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}}  \! -\! {\nabla _{{\lambda _l}}}{{\widetilde{L}}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\!\{ {\lambda _l^{t-1}}\} ,\!\{ {{\boldsymbol{\phi}}_j^{t-1}}\} {\rm{)}},{\lambda _l^t}  \! -\! {\lambda _l^{t-1}}} \! \right\rangle } (1c).
\end{array}
\end{equation}

\vspace{0.5ex}

Firstly, we focus on the ($1a$) in Eq.  (\ref{eq:A15}), we can write ($1a$) as:
\begin{equation}
\label{eq:A16}
\begin{array}{l}
\left\langle \!{{\nabla _{{\lambda _l}}}{{\widetilde{L}}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\} ,}}{\boldsymbol{z}}^{t+1},h^{t+1},\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} \! - \! {\nabla _{{\lambda _l}}}{{\widetilde{L}}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}},{\lambda _l^{t+1}} \! - \! {\lambda _l^t}}\! \right\rangle  \vspace{0.5ex}\\
\! =\! \left\langle {{\nabla _{{\lambda _l}}}{L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\} ,}}{\boldsymbol{z}}^{t+1},h^{t+1},\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} \! - \! {\nabla _{{\lambda _l}}}{L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}},{\lambda _l^{t+1}} \! - \! {\lambda _l^t}} \right\rangle  \vspace{0.5ex}\\
 \! +  ({c_1^{t-1}} \! - \! {c_1^t})\left\langle {{\lambda _l^t},{\lambda _l^{t+1}} \! - \! {\lambda _l^t}} \right\rangle  \vspace{0.5ex}\\
\! = \!\left\langle {{\nabla _{{\lambda _l}}}{L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\} ,}}{\boldsymbol{z}}^{t+1},h^{t+1},\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} \! - \! {\nabla _{{\lambda _l}}}{L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}},{\lambda _l^{t+1}} \! - \! {\lambda _l^t}} \right\rangle \vspace{0.5ex}\\
 \! +  \frac{{{c_1^{t-1}} - {c_1^t}}}{2}(||{\lambda _l^{t+1}}|{|^2} \! - \! ||{\lambda _l^t}|{|^2}) \! - \! \frac{{{c_1^{t-1}} -  {c_1^t}}}{2}||{\lambda _l^{t+1}} \! - \! {\lambda _l^t}|{|^2}.
\end{array}
\end{equation}


And according to Cauchy-Schwarz inequality and Assumption 1, we can obtain,
\begin{equation}
\label{eq:A17}
\begin{array}{l}
\left\langle {{\nabla _{{\lambda _l}}}{L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\} ,}}{\boldsymbol{z}}^{t+1},h^{t+1},\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}}\! -\! {\nabla _{{\lambda _l}}}{L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}},{\lambda _l^{t+1}} - {\lambda _l^t}} \right\rangle \vspace{0.5ex} \\
 \!\le \frac{{{L^2}}}{{2{a_1}}}(\sum\limits_{j = 1}^N {||{{\boldsymbol{w}}_j^{t+1}} - {{\boldsymbol{w}}_j^t}|{|^2}}  \! + \! ||{\boldsymbol{z}}^{t+1} - {\boldsymbol{z}^t}|{|^2} \! + \! ||h^{t+1} - h^t|{|^2}) \! + \! \frac{{{a_1}}}{2}||{\lambda _l^{t+1}} - {\lambda _l^t}|{|^2},
\end{array}
\end{equation}
where $a_1 >0$  is a constant. Combining Eq. (\ref{eq:A16}) with Eq. (\ref{eq:A17}), we can obtain the upper bound of ($1a$),
\begin{equation}
\label{eq:A18}
\begin{array}{l}
\sum\limits_{l = 1}^{|{{\bf{A}}^t}|}\! {\left\langle\! {{\nabla _{{\lambda _l}}}{{\widetilde{L}}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\} ,}}{\boldsymbol{z}}^{t+1},h^{t+1},\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} \! - \! {\nabla _{{\lambda _l}}}{{\widetilde{L}}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}},{\lambda _l^{t+1}} \!-\! {\lambda _l^t}} \! \right\rangle } \\
 \!\le \sum\limits_{l = 1}^{|{{\bf{A}}^t}|} ( \frac{{{L^2}}}{{2{a_1}}}(\sum\limits_{j = 1}^N {||{{\boldsymbol{w}}_j^{t+1}} \! - \! {{\boldsymbol{w}}_j^t}|{|^2}}  + ||{\boldsymbol{z}}^{t + 1} \! - \! {\boldsymbol{z}^t}|{|^2} + ||h^{t + 1} \! - \! h^t|{|^2}) + \frac{{{a_1}}}{2}||{\lambda _l^{t + 1}} \! - \! {\lambda _l^t}|{|^2}\vspace{1ex}\\
\qquad \quad + \frac{{{c_1^{t-1}} \! - \! {c_1^t}}}{2}(||{\lambda _l^{t + 1}}|{|^2} \! - \! ||{\lambda _l^t}|{|^2}) \! - \! \frac{{{c_1^{t-1}} \! - \! {c_1^t}}}{2}||{\lambda _l^{t + 1}} \! - \! {\lambda _l^t}|{|^2}).
\end{array}
\end{equation}


Secondly, we focus on the ($1b$) in Eq. (\ref{eq:A15}). According to Cauchy-Schwarz inequality we can write the ($1b$) as,
\begin{equation}
\label{eq:A19}
\begin{array}{l}
\sum\limits_{l = 1}^{|{{\bf{A}}^t}|} {\left\langle {{\nabla _{{\lambda _l}}}{{\widetilde{L}}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} \! - \! {\nabla _{{\lambda _l}}}{{\widetilde{L}}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^{t-1}}\} ,\{ {{\boldsymbol{\phi}}_j^{t-1}}\} {\rm{)}},{{\boldsymbol{v}}_{1,l}^{t + 1}}} \right\rangle } \\

\! \le \! \sum\limits_{l = 1}^{|{{\bf{A}}^t}|}\! {( \frac{{{a_2}}}{2}\!||{\nabla _{{\lambda _l}}}{{\widetilde{L}}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t, \! \{ {\lambda _l^t}\} , \! \{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} \! - \! {\nabla _{{\lambda _l}}}{{\widetilde{L}}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t, \! \{ {\lambda _l^{t-1}}\} , \! \{ {{\boldsymbol{\phi}}_j^{t-1}}\} {\rm{)}}|{|^2}\! +\! \frac{1}{{2{a_2}}}\!||{{\boldsymbol{v}}_{1,l}^{t + 1}}|{|^2})}. 
\end{array}
\end{equation}
where $a_2 >0$  is a constant. Then, we focus on the ($1c$) in Eq. (\ref{eq:A15}). Firstly, $\forall \lambda _l$, we have,
\begin{equation}
\label{eq:A20}
\begin{array}{l}
||{\nabla _{{\lambda _l}}}{\widetilde{L} _p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} \! - \! {\nabla _{{\lambda _l}}}{\widetilde{L} _p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^{t-1}}\} ,\{ {{\boldsymbol{\phi}}_j^{t-1}}\} {\rm{)||}}\vspace{1ex}\\
 \!= \! ||{\nabla _{{\lambda _l}}}{L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t, \!\{ {\lambda _l^t}\} ,\! \{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} \! - \! {\nabla _{{\lambda _l}}}{L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t, \! \{ {\lambda _l^{t-1}}\} , \{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} \! - \! {c_1^{t-1}}({\lambda _l^t} \! - \! {\lambda _l^{t-1}}){\rm{||}}\vspace{1ex}\\
 \! \le\! (L + {c_1^{t-1}})||{\lambda _l^t}  \! - \! {\lambda _l^{t-1}}{\rm{||}}.
\end{array}
\end{equation}
where the last inequality comes from the Assumption 1 and the trigonometric inequality. Denoting ${L_1}' = L + {c_1^0}$, we can obtain,
{\begin{equation}
\label{eq:A22}
\begin{array}{l}
 ||{\nabla _{{\lambda _l}}}{\widetilde{L}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}}\!  -\!  {\nabla _{{\lambda _l}}}{\widetilde{L}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^{t-1}}\} ,\{ {{\boldsymbol{\phi}}_j^{t-1}}\} {\rm{)||}} \le {L_1}'||{\lambda _l^t}  \! - \! {\lambda _l^{t-1}}{\rm{||}}.
\end{array}
\end{equation}}


Following from Eq. (\ref{eq:A22}) and the strong  concavity of ${\widetilde{L}}_p{\rm{(\{ }}{{\boldsymbol{w}}_j}{\rm{\} ,}}{\boldsymbol{z}},h,\{ {\lambda _l}\} ,\{ {{\boldsymbol{\phi}}_j}\} {\rm{)}}$ \textit{w.r.t} ${\lambda _l}$ \cite{nesterov2003introductory,xu2020unified}, we can obtain the upper bound of ($1c$):
{\begin{equation}
\label{eq:A23}
\begin{array}{l}
\sum\limits_{l = 1}^{|{{\bf{A}}^t}|} {\left\langle {{\nabla _{{\lambda _l}}}{{\widetilde{L}}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} - {\nabla _{{\lambda _l}}}{{\widetilde{L}}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^{t-1}}\} ,\{ {{\boldsymbol{\phi}}_j^{t-1}}\} {\rm{)}},{\lambda _l^t}\! - \!{\lambda _l^{t-1}}} \right\rangle } \\
\! \le\! \sum\limits_{l = 1}^{|{{\bf{A}}^t}|} (  - \frac{1}{{{L_1}' + {c_1^{t-1}}}}||{\nabla _{{\lambda _l}}}{\widetilde{L}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} - {\nabla _{{\lambda _l}}}{\widetilde{L}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^{t-1}}\} ,\{ {{\boldsymbol{\phi}}_j^{t-1}}\} {\rm{)}}|{|^2} \vspace{0.5ex}\\
\qquad \; \; - \frac{{{c_1^{t-1}}{L_1}'}}{{{L_1}' + {c_1^{t-1}}}}||{\lambda _l^t} \! -\! {\lambda _l^{t-1}}|{|^2}).
\end{array}
\end{equation}}


In addition, the following inequality can be obtained,
{\begin{equation}
\label{eq:A24}
\begin{array}{l}
\frac{1}{{{\rho _1}}}\left\langle {{\lambda _l^t} - {\lambda _l^{t-1}},{\lambda _l^{t + 1}} - {\lambda _l^t}} \right\rangle 
 \le \frac{1}{{2{\rho _1}}}||{\lambda _l^{t + 1}} - {\lambda _l^t}|{|^2} - \frac{1}{{2{\rho _1}}}||{{\boldsymbol{v}}_{1,l}^{t + 1}}|{|^2} + \frac{1}{{2{\rho _1}}}||{\lambda _l^t} - {\lambda _l^{t-1}}|{|^2}.
\end{array}
\end{equation}}


According to Eq. (\ref{eq:A14}), (\ref{eq:A15}), (\ref{eq:A18}), (\ref{eq:A19}), (\ref{eq:A23}), (\ref{eq:A24}), $ \frac{{{\rho _1}}}{2} \le \frac{1}{{{L_1}' + c_1^0}} $, and setting  ${a_2} = {\rho _1}$, we have that,
{\begin{equation}
\label{eq:A25}
\begin{array}{l}
{L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\} ,}}{\boldsymbol{z}}^{t + 1},h^{t+1},\{ {\lambda _l^{t+1}}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} \! - \! {L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\} ,}}{\boldsymbol{z}}^{t + 1},h^{t+1},\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} \vspace{1ex}\\

\! \le \! \sum\limits_{l = 1}^{|{{\bf{A}}^t}|} \! {(\! \left\langle \! {{\nabla _{{\lambda _l}}}{{\widetilde{L}}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\},}}{\boldsymbol{z}}^{t + 1},h^{t+1}, \! \{ {\lambda _l^t}\} , \! \{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} \! - \! {\nabla _{{\lambda _l}}}{{\widetilde{L}}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t, \! \{ {\lambda _l^{t-1}}\} , \! \{ {{\boldsymbol{\phi}}_j^{t-1}}\} {\rm{)}},{\lambda _l^{t+1}} \! - \! {\lambda _l^t}} \! \right\rangle } \vspace{1ex}\\

 \!  +  \frac{1}{{{\rho _1}}}\left\langle {{\lambda _l^t} \! - \! {\lambda _l^{t-1}},{\lambda _l^{t+1}} \! - \! {\lambda _l^t}} \right\rangle  \!  + \! \frac{{{c_1^t}}}{2}(||{\lambda _l^{t+1}}|{|^2} \! - \! ||{\lambda _l^t}|{|^2})) \vspace{1ex}\\
\! \le\! \sum\limits_{l = 1}^{|{{\bf{A}}^t}|} {(\frac{{{L^2}}}{{2{a_1}}}(\sum\limits_{j = 1}^N {||{{\boldsymbol{w}}_j^{t+1}} \! - \! {{\boldsymbol{w}}_j^t}|{|^2}}  \!  + \! ||{\boldsymbol{z}}^{t + 1} \! - \! {\boldsymbol{z}^t}|{|^2} \!  + \! ||h^{t+1} \! - \! h^t|{|^2})} \vspace{1ex}\\

 \!  +  (\frac{{{a_1}}}{2} \! - \! \frac{{{c_1^{t-1}} \! - \! {c_1^t}}}{2} \!  + \! \frac{1}{{2{\rho _1}}})||{\lambda _l^{t+1}} \! - \! {\lambda _l^t}|{|^2} \!  + \! \frac{{{c_1^{t-1}} }}{2}(||{\lambda _l^{t+1}}|{|^2} \! - \! ||{\lambda _l^t}|{|^2}) \!  + \! \frac{1}{{2{\rho _1}}}||{\lambda _l^t} \! - \! {\lambda _l^{t-1}}|{|^2})\vspace{1ex}\\
\! =\! \frac{{|{{\bf{A}}^t}|{L^2}}}{{2{a_1}}}(\sum\limits_{j = 1}^N {||{{\boldsymbol{w}}_j^{t+1}} \! - \! {{\boldsymbol{w}}_j^t}|{|^2}}  \!  + \! ||{\boldsymbol{z}}^{t + 1} \! - \! {\boldsymbol{z}^t}|{|^2} \!  + \! ||h^{t+1} \! - \! h^t|{|^2})\\
 \!  +  (\frac{{{a_1}}}{2} \! - \! \frac{{{c_1^{t-1}} \! - \! {c_1^t}}}{2} \!  + \! \frac{1}{{2{\rho _1}}})\sum\limits_{l = 1}^{|{{\bf{A}}^t}|} {||{\lambda _l^{t+1}} \! - \! {\lambda _l^t}|{|^2}}  \!  + \! \frac{{{c_1^{t-1}}}}{2}\sum\limits_{l = 1}^{|{{\bf{A}}^t}|} {(||{\lambda _l^{t+1}}|{|^2} \! - \! ||{\lambda _l^t}|{|^2})}  + \frac{1}{{2{\rho _1}}}\sum\limits_{l = 1}^{|{{\bf{A}}^t}|} {||{\lambda _l^t} \! - \! {\lambda _l^{t-1}}|{|^2}}.
\end{array}
\end{equation}}

According to Eq. (15), $\forall {\boldsymbol{\phi}} \in {{\boldsymbol{\Phi}}}$, it follows that,
{\begin{equation}
\label{eq:A26}
\left\langle {{{\boldsymbol{\phi}}_j^{t+1}} \! - \! {{\boldsymbol{\phi}}_j^t} - {\rho _2}{\nabla _{{{\boldsymbol{\phi}}_j}}}{{\widetilde{L}}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\} ,}}{\boldsymbol{z}}^{t+1},h^{t+1},\{ {\lambda _l^{t+1}}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}},{\boldsymbol{\phi}} \! - \! {{\boldsymbol{\phi}}_j^{t+1}}} \right\rangle  \ge 0.
\end{equation}}

Choosing ${\boldsymbol{\phi}} = {{\boldsymbol{\phi}}_j^t}$, we can obtain,
{\begin{equation}
\label{eq:A27}
\left\langle {{\nabla _{{{\boldsymbol{\phi}}_j}}}{{\widetilde{L}}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\} ,}}{\boldsymbol{z}}^{t+1},h^{t+1},\{ {\lambda _l^{t+1}}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} \!-\! \frac{1}{{{\rho _2}}}({{\boldsymbol{\phi}}_j^{t+1}}\! -\! {{\boldsymbol{\phi}}_j^t}),{{\boldsymbol{\phi}}_j^t}\! -\! {{\boldsymbol{\phi}}_j^{t+1}}} \right\rangle  \le 0.
\end{equation}}


Likewise, we have,
{\begin{equation}
\label{eq:A28}
\left\langle {{\nabla _{{{\boldsymbol{\phi}}_j}}}{{\widetilde{L}}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^{t-1}}\} {\rm{)}} \!-\! \frac{1}{{{\rho _2}}}({{\boldsymbol{\phi}}_j^t}\! -\! {{\boldsymbol{\phi}}_j^{t-1}}),{{\boldsymbol{\phi}}_j^{t+1}} \!-\! {{\boldsymbol{\phi}}_j^t}} \right\rangle  \le 0.
\end{equation}}

Since ${\widetilde{L}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j}{\rm{\} ,}}{\boldsymbol{z}},h,\{ {\lambda _l}\} ,\{ {{\boldsymbol{\phi}}_j}\} {\rm{)}}$ is concave with respect to ${{\boldsymbol{\phi}}_j}$ and follows from Eq. (\ref{eq:A28}):
{\begin{equation}
\label{eq:A29}
\begin{array}{l}
{\widetilde{L}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\} ,}}{\boldsymbol{z}}^{t+1},h^{t+1},\{ {\lambda _l^{t+1}}\} ,\{ {{\boldsymbol{\phi}}_j^{t+1}}\} {\rm{)}} \! - \! {\widetilde{L}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\} ,}}{\boldsymbol{z}}^{t+1},h^{t+1},\{ {\lambda _l^{t+1}}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} \vspace{0.5ex}\\

\! \le \! \sum\limits_{j = 1}^N \! {\left\langle \! {{\nabla  _{{{\boldsymbol{\phi}}_j}}}{{\widetilde{L}}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\} ,}}{\boldsymbol{z}}^{t+1},h^{t+1},\{ {\lambda _l^{t+1}}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}},{{\boldsymbol{\phi}}_j^{t+1}} \! - \! {{\boldsymbol{\phi}}_j^t}}\! \right\rangle } \\
 
\! \le \! \sum\limits_{j = 1}^N \! {(\! \left\langle \! {{\nabla \! _{{{\boldsymbol{\phi}}_j}}}{{\widetilde{L}}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\} ,}}{\boldsymbol{z}}^{t+1},h^{t+1}, \! \{ {\lambda _l^{t+1}}\}, \! \{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} \! - \! {\nabla \! _{{{\boldsymbol{\phi}}_j}}}{{\widetilde{L}}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\},}}{\boldsymbol{z}^t},h^t, \!\{ {\lambda _l^t}\} , \! \{ {{\boldsymbol{\phi}}_j^{t-1}}\} {\rm{)}},  {{\boldsymbol{\phi}}_j^{t+1}} \! - \! {{\boldsymbol{\phi}}_j^t}}\! \right\rangle } \vspace{0.5ex}\\
 \qquad \quad +  \frac{1}{{{\rho _2}}}\left\langle {{{\boldsymbol{\phi}}_j^t} \! - \! {{\boldsymbol{\phi}}_j^{t-1}},{{\boldsymbol{\phi}}_j^{t+1}} \! - \! {{\boldsymbol{\phi}}_j^t}}  \right\rangle ).
\end{array}
\end{equation}}


Denoting ${{\boldsymbol{v}}_{2,l}^{t + 1}} = {{\boldsymbol{\phi}}_j^{t+1}} - {{\boldsymbol{\phi}}_j^t} - ({{\boldsymbol{\phi}}_j^t} - {{\boldsymbol{\phi}}_j^{t-1}})$, we can write the first term in the last inequality of Eq.  (\ref{eq:A29}) as
{\begin{equation}
\label{eq:A30}
\begin{array}{l}
\sum\limits_{j = 1}^N \! {\left\langle \! {{\nabla _{{{\boldsymbol{\phi}}_j}}}{{\widetilde{L}}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\},}}{\boldsymbol{z}}^{t+1},h^{t+1}, \!\{ {\lambda _l^{t+1}}\},\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} \! - \! {\nabla _{{{\boldsymbol{\phi}}_j}}}{{\widetilde{L}}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\},}}{\boldsymbol{z}^t},h^t, \{ {\lambda _l^t}\},\{ {{\boldsymbol{\phi}}_j^{t-1}}\} {\rm{)}},{{\boldsymbol{\phi}}_j^{t+1}} \! - \! {{\boldsymbol{\phi}}_j^t}} \! \right\rangle } \\

 \!= \! \sum\limits_{j = 1}^N \! {\left\langle \! {{\nabla\! _{{{\boldsymbol{\phi}}_j}}}{{\widetilde{L}}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\} ,}}{\boldsymbol{z}}^{t+1},h^{t+1}, \! \{ {\lambda _l^{t+1}}\}, \! \{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} \! - \! {\nabla \! _{{{\boldsymbol{\phi}}_j}}}{{\widetilde{L}}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\},}}{\boldsymbol{z}^t},h^t, \! \{ {\lambda _l^t}\} , \! \{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}},{{\boldsymbol{\phi}}_j^{t+1}} \! - \! {{\boldsymbol{\phi}}_j^t}} \! \right\rangle } (2a) \\
 \! + \! \sum\limits_{j = 1}^N \! {\left\langle\! {{\nabla _{{{\boldsymbol{\phi}}_j}}}{{\widetilde{L}}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t, \!\{ {\lambda _l^t}\} , \!\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} \! - \! {\nabla _{{{\boldsymbol{\phi}}_j}}}{{\widetilde{L}}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\!\{ {\lambda _l^t}\}, \!\{ {{\boldsymbol{\phi}}_j^{t-1}}\} {\rm{)}},{{\boldsymbol{v}}_{2,l}^{t + 1}}} \!\right\rangle } (2b) \\
 
 \! + \! \sum\limits_{j = 1}^N {\left\langle\! {{\nabla _{{{\boldsymbol{\phi}}_j}}}{{\widetilde{L}}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\!\{ {\lambda _l^t}\},\!\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} \! - \! {\nabla _{{{\boldsymbol{\phi}}_j}}}{{\widetilde{L}}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\},}}{\boldsymbol{z}^t},h^t,\!\{ {\lambda _l^t}\} ,\!\{ {{\boldsymbol{\phi}}_j^{t-1}}\} {\rm{)}},{{\boldsymbol{\phi}}_j^t} \! - \! {{\boldsymbol{\phi}}_j^{t-1}}}\! \right\rangle } (2c).
\end{array}
\end{equation}}


We firstly focus on the ($2a$) in Eq. (\ref{eq:A30}), we can write the ($2a$) as,
{\begin{equation}
\label{eq:A31}
\begin{array}{l}
\left\langle\! {{\nabla _{{{\boldsymbol{\phi}}_j}}}{{\widetilde{L} }_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\} ,}}{\boldsymbol{z}}^{t+1},h^{t+1},\{ {\lambda _l^{t+1}}\},\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}}\! - \!{\nabla _{{{\boldsymbol{\phi}}_j}}}{{\widetilde{L} }_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\},\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}},{{\boldsymbol{\phi}}_j^{t+1}}\! -\! {{\boldsymbol{\phi}}_j^t}}\! \right\rangle \vspace{1ex} \\
\! = \!\left\langle {{\nabla _{{{\boldsymbol{\phi}}_j}}}{L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\},}}{\boldsymbol{z}}^{t+1},h^{t+1},\{ {\lambda _l^{t+1}}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} \!-\! {\nabla _{{{\boldsymbol{\phi}}_j}}}{L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}},{{\boldsymbol{\phi}}_j^{t+1}} \!-\! {{\boldsymbol{\phi}}_j^t}} \right\rangle \vspace{1ex} \\
\! + ({c_2^{t-1}} - {c_2^t})\left\langle {{{\boldsymbol{\phi}}_j^t},{{\boldsymbol{\phi}}_j^{t+1}} - {{\boldsymbol{\phi}}_j^t}} \right\rangle \vspace{1ex}\\

\! = \! \left\langle  {{\nabla _{{{\boldsymbol{\phi}}_j}}}{L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\},}}{\boldsymbol{z}}^{t+1},h^{t+1},\{ {\lambda _l^{t+1}}\},\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} \!-\! {\nabla _{{{\boldsymbol{\phi}}_j}}}{L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\},}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\} , \{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}},{{\boldsymbol{\phi}}_j^{t+1}}\! -\! {{\boldsymbol{\phi}}_j^t}} \right\rangle \vspace{1ex}
 \vspace{1ex}\\
  \! +  \frac{{{c_2^{t-1}}  -  {c_2^t}}}{2}(||{{\boldsymbol{\phi}}_j^{t+1}}|{|^2} \! - \! ||{{\boldsymbol{\phi}}_j^t}|{|^2}) \! - \! \frac{{{c_2^{t-1}}  -  {c_2^t}}}{2}||{{\boldsymbol{\phi}}_j^{t+1}} \! - \! {{\boldsymbol{\phi}}_j^t}|{|^2}).
\end{array}
\end{equation}}

\vspace{1ex}

And according to Cauchy-Schwarz inequality and Assumption 1, we can obtain,
\begin{equation}
\label{eq:A32}
\begin{array}{l}
\left\langle {{\nabla _{{{\boldsymbol{\phi}}_j}}}{L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\},}}{\boldsymbol{z}}^{t + 1},h^{t+1},\{ {\lambda _l^{t+1}}\},\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}}\! -\! {\nabla _{{{\boldsymbol{\phi}}_j}}}{L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}},{{\boldsymbol{\phi}}_j^{t+1}}\! -\! {{\boldsymbol{\phi}}_j^t}} \right\rangle \vspace{1.5ex}\\
\! =\! \left\langle {{\nabla _{{{\boldsymbol{\phi}}_j}}}{L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\} ,}}{\boldsymbol{z}}^{t+1},h^{t+1},\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}}\! -\! {\nabla _{{{\boldsymbol{\phi}}_j}}}{L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\},}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\} , \{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}},{{\boldsymbol{\phi}}_j^{t+1}}\! -\! {{\boldsymbol{\phi}}_j^t}} \right\rangle \vspace{1ex}\\
\! \le \! \frac{{{L^2}}}{{2{a_3}}}(\sum\limits_{j = 1}^N {||{{\boldsymbol{w}}_j^{t+1}}\! -\! {{\boldsymbol{w}}_j^t}|{|^2}}  + ||{\boldsymbol{z}}^{t+1} \!-\! {\boldsymbol{z}^t}|{|^2} + ||h^{t+1}\! -\! h^t|{|^2}) + \frac{{{a_3}}}{2}||{{\boldsymbol{\phi}}_j^{t+1}}\! -\! {{\boldsymbol{\phi}}_j^t}|{|^2},
\end{array}
\end{equation}
where $a_3 >0$ is a constant. Thus, we can obtain the upper bound of ($2a$) by combining the above Eq. (\ref{eq:A31}) and Eq. (\ref{eq:A32}),
{\begin{equation}
\label{eq:A33}
\begin{array}{l}
\sum\limits_{j = 1}^N \! {\left\langle\! {{\nabla _{{{\boldsymbol{\phi}}_j}}}{{\widetilde{L} }_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\},}}{\boldsymbol{z}}^{t+1},h^{t+1},\{ {\lambda _l^{t+1}}\},\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} \! - \! {\nabla _{{{\boldsymbol{\phi}}_j}}}{{\widetilde{L} }_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\},}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}},{{\boldsymbol{\phi}}_j^{t+1}} \! - \! {{\boldsymbol{\phi}}_j^t}}\! \right\rangle } \vspace{0.5ex}\\
\! = \! \sum\limits_{j \in {{\bf{Q}}^{t + 1}}}^{} \! {\left\langle\! {{\nabla _{{{\boldsymbol{\phi}}_j}}}{{\widetilde{L} }_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\},}}{\boldsymbol{z}}^{t+1},h^{t+1}, \! \{ {\lambda _l^{t+1}}\}, \! \{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} \! - \! {\nabla _{{{\boldsymbol{\phi}}_j}}}{{\widetilde{L} }_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\},}}{\boldsymbol{z}^t},h^t, \! \{ {\lambda _l^t}\} , \! \{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}},{{\boldsymbol{\phi}}_j^{t+1}} \! - \! {{\boldsymbol{\phi}}_j^t}}\! \right\rangle } \vspace{0.5ex}\\
\! \le \! \sum\limits_{j \in {{\bf{Q}}^{t + 1}}}^{} \! {(\frac{{{L^2}}}{{2{a_3}}}(\sum\limits_{j = 1}^N {||{{\boldsymbol{w}}_j^{t+1}} \! - \! {{\boldsymbol{w}}_j^t}|{|^2}}  \! + \! ||{\boldsymbol{z}}^{t+1} \! - \! {\boldsymbol{z}^t}|{|^2} \! + \! ||h^{t+1} \! - \! h^t|{|^2}) \! + \! \frac{{{a_3}}}{2}||{{\boldsymbol{\phi}}_j^{t+1}} \! - \! {{\boldsymbol{\phi}}_j^t}|{|^2}} \vspace{1ex}\\
 \qquad \qquad +  \frac{{{c_2^{t-1}}  -  {c_2^t}}}{2}(||{{\boldsymbol{\phi}}_j^{t+1}}|{|^2} \! - \! ||{{\boldsymbol{\phi}}_j^t}|{|^2}) \! - \! \frac{{{c_2^{t-1}}  -  {c_2^t}}}{2}||{{\boldsymbol{\phi}}_j^{t+1}} \! - \! {{\boldsymbol{\phi}}_j^t}|{|^2}).
\end{array}
\end{equation}}

\vspace{1ex}

Next we focus on the ($2b$) in Eq. (\ref{eq:A30}). According to Cauchy-Schwarz inequality we can write the ($2b$) as
\begin{equation}
\label{eq:A34}
\begin{array}{l}
\sum\limits_{j = 1}^N \!\left\langle \! {{\nabla _{{{\boldsymbol{\phi}}_j}}}{{\widetilde{L}}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\},}}{\boldsymbol{z}^t},h^t, \{ {\lambda _l^t}\}, \!\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} \!-\! {\nabla _{{{\boldsymbol{\phi}}_j}}}{{\widetilde{L}}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\},\{ {{\boldsymbol{\phi}}_j^{t-1}}\} {\rm{)}},{\boldsymbol{v}}_{2,l}^{t+1}} \! \right\rangle \\
\! \le \! \sum\limits_{j = 1}^N \! {(\frac{{{a_4}}}{2}||{\nabla _{{{\boldsymbol{\phi}}_j}}}{{\widetilde{L}}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\},}}{\boldsymbol{z}^t},h^t, \! \{ {\lambda _l^t}\} , \!\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} \!-\! {\nabla _{{{\boldsymbol{\phi}}_j}}}{{\widetilde{L}}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\},}}{\boldsymbol{z}^t},h^t, \! \{ {\lambda _l^t}\} , \!\{ {{\boldsymbol{\phi}}_j^{t-1}}\} {\rm{)}}|{|^2}\! +\! \frac{1}{{2{a_4}}}\!||{\boldsymbol{v}}_{2,l}^{t+1}|{|^2})},
\end{array}
\end{equation}
where $a_4>0$ is a constant. Then, we focus on the ($2c$) in Eq. (\ref{eq:A30}), we have,
\begin{equation}
\label{eq:A35}
\begin{array}{l}
||{\nabla _{{{\boldsymbol{\phi}}_j}}}{\widetilde{L}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} \!-\! {\nabla _{{{\boldsymbol{\phi}}_j}}}{\widetilde{L}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^{t-1}}\} {\rm{)||}}\vspace{1.5ex}\\

\! \le\! ||{\nabla _{{{\boldsymbol{\phi}}_j}}}{L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\},}}{\boldsymbol{z}^t},h^t, \! \{ {\lambda _l^t}\} , \!\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}}\! -\! {\nabla _{{{\boldsymbol{\phi}}_j}}}{L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t, \! \{ {\lambda _l^t}\}, \! \{ {{\boldsymbol{\phi}}_j^{t-1}}\} {\rm{)}}||\! +\! {c_2^{t-1}}||{{\boldsymbol{\phi}}_j^t}\! -\! {{\boldsymbol{\phi}}_j^{t-1}}{\rm{||}}\vspace{1.5ex}\\

\! \le\! (L + {c_2^{t-1}})||{{\boldsymbol{\phi}}_j^t} \!-\! {{\boldsymbol{\phi}}_j^{t-1}}{\rm{||}},
\end{array}
\end{equation}
where the last inequality comes from the Assumption 1 and the trigonometric inequality. Denoting ${L_2}' = L \! + \! {c_2^0}$, we can obtain,
{\begin{equation}
\label{eq:A36}
\begin{array}{l}
||{\nabla _{{{\boldsymbol{\phi}}_j}}}{\widetilde{L}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}}\! -\! {\nabla _{{{\boldsymbol{\phi}}_j}}}{\widetilde{L}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^{t-1}}\} {\rm{)||}}
 \le {L_2}'||{{\boldsymbol{\phi}}_j^t} \!-\! {{\boldsymbol{\phi}}_j^{t-1}}{\rm{||}}.
\end{array}
\end{equation}}

\vspace{1ex}

Following Eq. (\ref{eq:A36}) and the strong  concavity of ${\widetilde{L}}_p{\rm{(\{ }}{{\boldsymbol{w}}_j}{\rm{\} ,}}{\boldsymbol{z}},h,\{ {\lambda _l}\} ,\{ {{\boldsymbol{\phi}}_j}\} {\rm{)}}$ \textit{w.r.t} ${\boldsymbol{\phi}}_j$, we can obtain the upper bound of ($2c$),
{\begin{equation}
\label{eq:A37}
\begin{array}{l}
\sum\limits_{j = 1}^N \! {\left\langle\! {{\nabla _{{{\boldsymbol{\phi}}_j}}}{{\widetilde{L}}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\},}}{\boldsymbol{z}^t},h^t, \! \{ {\lambda _l^t}\}, \! \{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}}\! - \!{\nabla _{{{\boldsymbol{\phi}}_j}}}{{\widetilde{L}}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\},}}{\boldsymbol{z}^t},h^t, \!\{ {\lambda _l^t}\} , \!\{ {{\boldsymbol{\phi}}_j^{t-1}}\} {\rm{)}},{{\boldsymbol{\phi}}_j^t}\! -\! {{\boldsymbol{\phi}}_j^{t-1}}}\! \right\rangle } \vspace{0.5ex}\\
\! \le \! \sum\limits_{j = 1}^N \! {( - \frac{1}{{{L_2}' + {c_2^{t-1}}}}||{\nabla _{{{\boldsymbol{\phi}}_j}}}{{\widetilde{L}}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t, \! \{ {\lambda _l^t}\} , \! \{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}}\! -\! {\nabla _{{{\boldsymbol{\phi}}_j}}}{{\widetilde{L}}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t, \! \{ {\lambda _l^t}\} , \!\{ {{\boldsymbol{\phi}}_j^{t-1}}\} )|{|^2}} \vspace{1ex}\\
\qquad \;\; - \frac{{{c_2^{t-1}}{L_2}'}}{{{L_2}' + {c_2^{t-1}}}}||{{\boldsymbol{\phi}}_j^t} \!-\! {{\boldsymbol{\phi}}_j^{t-1}}|{|^2}).
\end{array}
\end{equation}}


In addition, the following inequality can also be obtained,
{\begin{equation}
\label{eq:A38}
\begin{array}{l}
\sum\limits_{j = 1}^N {\frac{1}{{{\rho _2}}}\left\langle {{{\boldsymbol{\phi}}_j^t} \!-\! {{\boldsymbol{\phi}}_j^{t-1}},{{\boldsymbol{\phi}}_j^{t+1}} \!-\! {{\boldsymbol{\phi}}_j^t}} \right\rangle } 
 \le \sum\limits_{j = 1}^N {(\frac{1}{{2{\rho _2}}}||{{\boldsymbol{\phi}}_j^{t+1}} \!-\! {{\boldsymbol{\phi}}_j^t}|{|^2} - \frac{1}{{2{\rho _2}}}||{{\boldsymbol{v}}_{2,l}^{t+1}}|{|^2} + \frac{1}{{2{\rho _2}}}||{{\boldsymbol{\phi}}_j^t} - {{\boldsymbol{\phi}}_j^{t-1}}|{|^2})}.
\end{array}
\end{equation}}


According to Eq. (\ref{eq:A29}), (\ref{eq:A30}), (\ref{eq:A33}), (\ref{eq:A34}), (\ref{eq:A37}), (\ref{eq:A38}),  $ \frac{{{\rho _2}}}{2} \le \frac{1}{{{L_2}' + c_2^0}}$, and setting ${a_4} = {\rho _2}$, we have,
{\begin{equation}
\label{eq:A39}
\begin{array}{l}
{L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\} ,}}{\boldsymbol{z}}^{t+1},h^{t+1},\{ {\lambda _l^{t+1}}\},\{ {{\boldsymbol{\phi}}_j^{t+1}}\} {\rm{)}} \! - \! {L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\} ,}}{\boldsymbol{z}}^{t+1},h^{t+1},\{ {\lambda _l^{t+1}}\},\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}}\vspace{1ex}\\

\! \le \! \sum\limits_{j = 1}^N \! {(\!\left\langle\! {{\nabla \! _{{{\boldsymbol{\phi}}_j}}}{{\widetilde{L}}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\} ,}}{\boldsymbol{z}}^{t+1},h^{t+1}, \! \{ {\lambda _l^{t+1}}\} , \! \{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} \! - \! {\nabla \! _{{{\boldsymbol{\phi}}_j}}}{{\widetilde{L}}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t, \! \{ {\lambda _l^t}\}, \{ {{\boldsymbol{\phi}}_j^{t-1}}\} {\rm{)}},{{\boldsymbol{\phi}}_j^{t+1}} \! - \! {{\boldsymbol{\phi}}_j^t}}\! \right\rangle } \vspace{0.5ex}\\
 \qquad \;\; +  \frac{1}{{{\rho _2}}}\left\langle {{{\boldsymbol{\phi}}_j^t} \! - \! {{\boldsymbol{\phi}}_j^{t-1}},{{\boldsymbol{\phi}}_j^{t+1}} \! - \! {{\boldsymbol{\phi}}_j^t}} \right\rangle  \! + \! \frac{{{c_2^t}}}{2}(||{{\boldsymbol{\phi}}_j^{t+1}}|{|^2} \! - \! ||{{\boldsymbol{\phi}}_j^t}|{|^2}))\vspace{1ex}\\
\! \le \! \frac{{|{{\bf{Q}}^{t + 1}}|{L^2}}}{{2{a_3}}}(\sum\limits_{j = 1}^N {||{{\boldsymbol{w}}_j^{t+1}} \! - \! {{\boldsymbol{w}}_j^t}|{|^2}}  \! + \! ||{\boldsymbol{z}}^{t+1} \! - \! {\boldsymbol{z}^t}|{|^2} \! + \! ||h^{t+1} \! - \! h^t|{|^2})\vspace{0.5ex}\\
 \! +  (\frac{{{a_3}}}{2} \! - \! \frac{{{c_2^{t-1}}  -  {c_2^t}}}{2} \! + \! \frac{1}{{2{\rho _2}}})\! \sum\limits_{j = 1}^N\! {||{{\boldsymbol{\phi}}_j^{t+1}} \! - \! {{\boldsymbol{\phi}}_j^t}|{|^2}}  \! + \! \frac{{{c_2^{t-1}}}}{2}\! \sum\limits_{j = 1}^N\! {(||{{\boldsymbol{\phi}}_j^{t+1}}|{|^2} \! - \! ||{{\boldsymbol{\phi}}_j^t}|{|^2})} 
 \! + \! \frac{1}{{2{\rho _2}}}\! \sum\limits_{j = 1}^N \! {||{{\boldsymbol{\phi}}_j^t} \! - \! {{\boldsymbol{\phi}}_j^{t-1}}|{|^2}}. 
\end{array}
\end{equation}}

By combining the Lemma \ref{lemma 1} with Eq. (\ref{eq:A25}) and Eq. (\ref{eq:A39}), we conclude the proof of Lemma \ref{lemma 2}.

\vspace{3ex}

\begin{lemma} \label{lemma3}
Firstly,  we denote ${S_1^{t+1}}$, ${S_2^{t+1}}$ and ${F^{t+1}}$  as,
\begin{equation}
\label{eq:A40}
{S_1^{t+1}} = \frac{4}{{{\rho _1}^2{c_1^{t+1}}}}\sum\limits_{l = 1}^{|{{\bf{A}}^t}|} {||{\lambda _l^{t+1}} \! - \! {\lambda _l^t}|{|^2}}  \! - \! \frac{4}{{{\rho _1}}}(\frac{{{c_1^{t-1}}}}{{{c_1^t}}} \! - \! 1)\sum\limits_{l = 1}^{|{{\bf{A}}^t}|} {||{\lambda _l^{t+1}}|{|^2}},
\end{equation}
\begin{equation}
\label{eq:A41}
{S_2^{t+1}} = \frac{4}{{{\rho _2}^2{c_2^{t+1}}}}\sum\limits_{j = 1}^N {||{{\boldsymbol{\phi}}_j^{t+1}} \! - \! {{\boldsymbol{\phi}}_j^t}|{|^2}}  \! - \! \frac{4}{{{\rho _2}}}(\frac{{{c_2^{t-1}}}}{{{c_2^t}}} \! - \! 1)\sum\limits_{j = 1}^N {||{{\boldsymbol{\phi}}_j^{t+1}}|{|^2}}, 
\end{equation}
\begin{equation}
\label{eq:A42}
\begin{array}{l}
F^{t+1} = {L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\} ,}}{\boldsymbol{z}}^{t+1},h^{t+1},\{ {\lambda _l^{t+1}}\} ,\{ {{\boldsymbol{\phi}}_j^{t+1}}\} {\rm{)}}  +  {S_1^{t+1}}  +  {S_2^{t+1}}\vspace{1ex}\\
 \quad  \quad   \quad \;    \! -  \frac{7}{{2{\rho _1}}}\!\sum\limits_{l = 1}^{|{{\bf{A}}^t}|}\! {||{\lambda _l^{t+1}} \! - \! {\lambda _l^t}|{|^2}} 
 \! - \! \frac{c_1^t}{{2}}\!\sum\limits_{l = 1}^{|{{\bf{A}}^t}|}\! {||{\lambda _l^{t+1}}|{|^2}} 
 
 \! - \! \frac{7}{{2{\rho _2}}}\! \sum\limits_{j = 1}^N\! {||{{\boldsymbol{\phi}}_j^{t+1}} \! - \! {{\boldsymbol{\phi}}_j^t}|{|^2}}
 
  \! - \! \frac{c_2^t}{{2}} \! \sum\limits_{j = 1}^N\! {||{{\boldsymbol{\phi}}_j^{t+1}}|{|^2}},
\end{array}
\end{equation}
then $\forall t \ge T_1 + \tau$, we have,
\begin{equation}
\label{eq:A43}
\begin{array}{l}
F^{t+1} \! - \! F^{t}
\! \le \! (\frac{{L  +  1}}{2} \! - \! \frac{1}{{{\eta _{\boldsymbol{w}}^t}}} \! + \! \frac{{{\rho _1}|{{\bf{A}}^t}|{L^2}}}{2} \! + \! \frac{{{\rho _2}|{{\bf{Q}}^{t + 1}}|{L^2}}}{2} \! + \! \frac{{8|{{\bf{A}}^t}|{L^2}}}{{{\rho _1}({c_1^t}){^2}}} \! + \! \frac{{8N{L^2}}}{{{\rho _2}({c_2^t}){^2}}})\sum\limits_{j = 1}^N {||{{\boldsymbol{w}}_j^{t+1}} \! - \! {{\boldsymbol{w}}_j^t}|{|^2}} \vspace{0.5ex}\\

\quad \quad \quad \quad \;\;  \! +  (\frac{{L  +  3\tau{ k_1 }N{L^2}}}{2} \! - \! \frac{1}{{{\eta _{\boldsymbol{z}}^t}}} \! + \! \frac{{{\rho _1}|{{\bf{A}}^t}|{L^2}}}{2} \! + \! \frac{{{\rho _2}|{{\bf{Q}}^{t + 1}}|{L^2}}}{2} \! + \! \frac{{8|{{\bf{A}}^t}|{L^2}}}{{{\rho _1}({c_1^t}){^2}}} \! + \! \frac{{8N{L^2}}}{{{\rho _2}({c_2^t}){^2}}})||\boldsymbol{z}^{t+1} \! - \! \boldsymbol{z}^t|{|^2}\vspace{0.5ex}\\

\quad \quad \quad \quad \;\; \! +  (\frac{{L  +  3\tau{ k_1 }N{L^2}}}{2} \! - \! \frac{1}{{{\eta _h^t}}} \! + \! \frac{{{\rho _1}|{{\bf{A}}^t}|{L^2}}}{2} \! + \! \frac{{{\rho _2}|{{\bf{Q}}^{t + 1}}|{L^2}}}{2} \! + \! \frac{{8|{{\bf{A}}^t}|{L^2}}}{{{\rho _1}({c_1^t}){^2}}} \! + \! \frac{{8N{L^2}}}{{{\rho _2}({c_2^t}){^2}}})||h^{t+1} \! - \! h^t|{|^2}\vspace{0.5ex}\\
 
\quad \quad \quad \quad \;\; \! -  (\frac{1}{{10{\rho _1}}} \! - \! \frac{{3\tau{ k_1 }N{L^2}}}{2})\!\sum\limits_{l = 1}^{|{{\bf{A}}^t}|}\! {||{\lambda _l^{t+1}} \! - \! {\lambda _l^t}|{|^2}}  \! - \! \frac{1}{{10{\rho _2}}}\!\sum\limits_{j = 1}^N\! {||{{\boldsymbol{\phi}}_j^{t+1}} \! - \! {{\boldsymbol{\phi}}_j^t}|{|^2}}  \! + \! \frac{{{c_1^{t-1}} \! - \! {c_1^t}}}{2}\!\sum\limits_{l = 1}^{|{{\bf{A}}^t}|}\! {||{\lambda _l^{t+1}}|{|^2}}  \\

\quad \quad \quad \quad \;\; \! +  \frac{{{c_2^{t-1}} \! - \! {c_2^t}}}{2}\sum\limits_{j = 1}^N {||{{\boldsymbol{\phi}}_j^{t+1}}|{|^2}}
 \! + \! \frac{4}{{{\rho _1}}}(\frac{{{c_1^{t-2}}}}{{{c_1^{t-1}}}} \! - \! \frac{{{c_1^{t-1}}}}{{{c_1^t}}})\sum\limits_{l = 1}^{|{{\bf{A}}^t}|} {||{\lambda _l^t}|{|^2}}  \! + \! \frac{4}{{{\rho _2}}}(\frac{{{c_2^{t-2}}}}{{{c_2^{t-1}}}} \! - \! \frac{{{c_2^{t-1}}}}{{{c_2^t}}})\sum\limits_{j = 1}^N {||{{\boldsymbol{\phi}}_j^t}|{|^2}}.
\end{array}
\end{equation}
\end{lemma}

\vspace{-2mm}

\noindent \emph{\textbf{Proof of Lemma \ref{lemma3}}:} 

Let ${a_1} = \frac{1}{{{\rho _1}}}$, ${a_3} = \frac{1}{{{\rho _2}}}$ and substitute them into the Lemma \ref{lemma 2}, $\forall t \ge T_1 + \tau $, we have,
{\begin{equation}
\label{eq:A44}
\begin{array}{l}
{L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\} ,}}{\boldsymbol{z}}^{t + 1},h^{t+1},\{ {\lambda _l^{t+1}}\} ,\{ {{\boldsymbol{\phi}}_j^{t+1}}\} {\rm{)}} - {L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}\boldsymbol{z}^t,h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}}\\
\! \le\! (\frac{{L + 1}}{2} \!-\! \frac{1}{{{\eta _{\boldsymbol{w}}^t}}}\! +\! \frac{{{\rho _1}|{{\bf{A}}^t}|{L^2}}}{2} \!+\! \frac{{{\rho _2}|{{\bf{Q}}^{t + 1}}|{L^2}}}{2})\! \sum\limits_{j = 1}^N\! {||{{\boldsymbol{w}}_j^{t+1}}\! -\! {{\boldsymbol{w}}_j^t}|{|^2}} \\

\! + (\frac{{L + 3\tau{ k_1 }N{L^2}}}{2} \!-\! \frac{1}{{{\eta _{\boldsymbol{z}}^t}}}\! + \!\frac{{{\rho _1}|{{\bf{A}}^t}|{L^2}}}{2}\! +\! \frac{{{\rho _2}|{{\bf{Q}}^{t + 1}}|{L^2}}}{2})||{\boldsymbol{z}}^{t + 1}\! -\! \boldsymbol{z}^t|{|^2}\\

\! + (\frac{{L + 3\tau{ k_1 }N{L^2}}}{2}\! -\! \frac{1}{{{\eta _h^t}}}\! +\! \frac{{{\rho _1}|{{\bf{A}}^t}|{L^2}}}{2}\!+\! \frac{{{\rho _2}|{{\bf{Q}}^{t + 1}}|{L^2}}}{2})||h^{t+1} \!-\! h^t|{|^2}\\

\!+ (\frac{{3\tau{ k_1 }N{L^2}}}{2} \!+\! \frac{1}{{{\rho _1}}} \!-\! \frac{{{c_1^{t-1}} - {c_1^t}}}{2})\! \sum\limits_{l = 1}^{|{{\bf{A}}^t}|} \! {||{\lambda _l^{t+1}} \!-\! {\lambda _l^t}|{|^2}} \\

\! + \frac{{{c_1^{t-1}} }}{2}\! \sum\limits_{l = 1}^{|{{\bf{A}}^t}|} \! {(||{\lambda _l^{t+1}}|{|^2}\! -\! ||{\lambda _l^t}|{|^2})} \! +\! \frac{1}{{2{\rho _1}}} \!\sum\limits_{l = 1}^{|{{\bf{A}}^t}|} \! {||{\lambda _l^t} - {\lambda _l^{t-1}}|{|^2}} \! +\! (\frac{1}{{{\rho _2}}} \!-\! \frac{{{c_2^{t-1}} - {c_2^t}}}{2}) \!\sum\limits_{j = 1}^N \! {||{{\boldsymbol{\phi}}_j^{t+1}}\! -\! {{\boldsymbol{\phi}}_j^t}|{|^2}} \\

\! + \frac{{{c_2^{t-1}}}}{2} \! \sum\limits_{j = 1}^N \! {(||{{\boldsymbol{\phi}}_j^{t+1}}|{|^2} \!- \!||{{\boldsymbol{\phi}}_j^t}|{|^2})}\!  +\! \frac{1}{{2{\rho _2}}}\!\sum\limits_{j = 1}^N\! {||{{\boldsymbol{\phi}}_j^t} - {{\boldsymbol{\phi}}_j^{t-1}}|{|^2}}.
\end{array}
\end{equation}}

According to Eq. (14), in $(t+1)^{\rm{th}}$ iteration, it follows that:
{\begin{equation}
\label{eq:A45}
\left\langle {{\lambda _l^{t+1}}\! -\! {\lambda _l^t}\! -\! {\rho _1}{\nabla _{{\lambda _l}}}{{\widetilde{L}}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\} ,}}{\boldsymbol{z}}^{t+1},h^{t+1},\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}},{\lambda _l^t}\! -\! {\lambda _l^{t+1}}} \right\rangle  \ge 0,
\end{equation}}

Similar to Eq. (\ref{eq:A45}), in $t^{\rm{th}}$ iteration, we have,
{\begin{equation}
\label{eq:A46}
\left\langle {{\lambda _l^t}\! -\! {\lambda _l^{t-1}} \!-\! {\rho _1}{\nabla _{{\lambda _l}}}{{\widetilde{L}}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^{t-1}}\} ,\{ {{\boldsymbol{\phi}}_j^{t-1}}\} {\rm{)}},{\lambda _l^{t+1}} \!-\! {\lambda _l^t}} \right\rangle  \ge 0.
\end{equation}}

$\forall t \ge T_1$, we can obtain the following inequality, 
{\begin{equation}
\label{eq:A47}
\begin{array}{l}
\! \sum\limits_{l = 1}^{|{{\bf{A}}^t}|}\! \frac{1}{{{\rho _1}}} \left\langle {{{\boldsymbol{v}}_{1,l}^{t + 1}},{\lambda _l^{t+1}} \! - \! {\lambda _l^t}} \right\rangle  \\
\! \le \! \sum\limits_{l = 1}^{|{{\bf{A}}^t}|} ( \left\langle \!  {{\nabla _{{\lambda _l}}}{{\widetilde{L}}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\},}}{\boldsymbol{z}}^{t+1},h^{t+1},\{ {\lambda _l^t}\},\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} \! - \! {\nabla _{{\lambda _l}}}{{\widetilde{L}}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\},}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}},{\lambda _l^{t+1}} \! - \! {\lambda _l^t}} \! \right\rangle  \vspace{0.5ex}\\

 \qquad \;  + \!  \left\langle \! {{\nabla _{{\lambda _l}}}{{\widetilde{L}}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\},}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} \! - \! {\nabla _{{\lambda _l}}}{{\widetilde{L}}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\},}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^{t-1}}\},\{ {{\boldsymbol{\phi}}_j^{t-1}}\} {\rm{)}},{{\boldsymbol{v}}_{1,l}^{t + 1}}} \! \right\rangle \vspace{0.5ex} \\
 
 \qquad \; + \!  \left\langle \! {{\nabla _{{\lambda _l}}}{{\widetilde{L}}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\},}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\},\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} \! - \! {\nabla _{{\lambda _l}}}{{\widetilde{L}}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\},}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^{t-1}}\},\{ {{\boldsymbol{\phi}}_j^{t-1}}\} {\rm{)}},{\lambda _l^t} \! - \! {\lambda _l^{t-1}}} \! \right\rangle ). 
\end{array}
\end{equation}}

Since we have the following equality, 
{\begin{equation}
\label{eq:A48}
\begin{array}{l}
\frac{1}{{{\rho _1}}}\left\langle {{{\boldsymbol{v}}_{1,l}^{t + 1}},{\lambda _l^{t+1}} \!-\! {\lambda _l^t}} \right\rangle 
 = \frac{1}{{2{\rho _1}}}||{\lambda _l^{t+1}} \!-\! {\lambda _l^t}|{|^2} \! + \! \frac{1}{{2{\rho _1}}}||{{\boldsymbol{v}}_{1,l}^{t + 1}}|{|^2} - \frac{1}{{2{\rho _1}}}||{\lambda _l^t} - {\lambda _l^{t-1}}|{|^2},
\end{array}
\end{equation}}

it follows that,
\begin{equation}
\label{eq:A49}
\begin{array}{l}
\sum\limits_{l = 1}^{|{{\bf{A}}^t}|}(\frac{1}{{2{\rho _1}}}||{\lambda _l^{t+1}} \! - \! {\lambda _l^t}|{|^2} \! + \! \frac{1}{{2{\rho _1}}}||{{\boldsymbol{v}}_{1,l}^{t + 1}}|{|^2} \! - \! \frac{1}{{2{\rho _1}}}||{\lambda _l^t} \! - \! {\lambda _l^{t-1}}|{|^2}) \vspace{0.5ex}\\

\! \le\!\sum\limits_{l = 1}^{|{{\bf{A}}^t}|}( \frac{{{L^2}}}{{2{b_1^t}}}(\sum\limits_{j = 1}^N {||{{\boldsymbol{w}}_j^{t+1}} \! - \! {{\boldsymbol{w}}_j^t}|{|^2}}  \! + \! ||{\boldsymbol{z}}^{t+1} \! - \! {\boldsymbol{z}^t}|{|^2} \! + \! ||h^{t+1} \! - \! h^t|{|^2}) \! + \! \frac{{{b_1^t}}}{2}||{\lambda _l^{t+1}} \! - \! {\lambda _l^t}|{|^2} \vspace{0.5ex}\\

\qquad \; +  \frac{{{c_1^{t-1}} \! - \! {c_1^t}}}{2}(||{\lambda _l^{t+1}}|{|^2} \! - \! ||{\lambda _l^t}|{|^2}) \! - \! \frac{{{c_1^{t-1}} \! - \! {c_1^t}}}{2}||{\lambda _l^{t+1}} \! - \! {\lambda _l^t}|{|^2} \vspace{1ex}\\

\qquad \; +  \frac{{{\rho _1}}}{2}||{\nabla _{{\lambda _l}}}{\widetilde{L}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\},}}{\boldsymbol{z}^t},h^t,\!\{ {\lambda _l^t}\},\!\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} \! - \! {\nabla _{{\lambda _l}}}{\widetilde{L}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\},}}{\boldsymbol{z}^t},h^t,\!\{ {\lambda _l^{t-1}}\},\!\{ {{\boldsymbol{\phi}}_j^{t-1}}\} {\rm{)}}|{|^2} \! + \! \frac{1}{{2{\rho _1}}}\!||{{\boldsymbol{v}}_{1,l}^{t + 1}}|{|^2} \vspace{1ex}\\

\qquad \; - \frac{1}{{{L_1}'  +  {c_1^{t-1}}}}||{\nabla _{{\lambda _l}}}{\widetilde{L}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\},}}{\boldsymbol{z}^t},h^t, \! \{ {\lambda _l^t}\} , \! \{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} \! - \! {\nabla _{{\lambda _l}}}{\widetilde{L}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\},}}{\boldsymbol{z}^t},h^t, \! \{ {\lambda _l^{t-1}}\}  , \!\{ {{\boldsymbol{\phi}}_j^{t-1}}\} {\rm{)}}|{|^2} \vspace{1ex}\\

\qquad \;  - \frac{{{c_1^{t-1}}{L_1}'}}{{{L_1}'  +  {c_1^{t-1}}}}||{\lambda _l^t} \! - \! {\lambda _l^{t-1}}|{|^2}),
\end{array}
\end{equation}
where ${b_1^t} > 0$. According to the setting that ${c_1^0} \le {L_1}'$, we have $- \frac{{{c_1^{t-1}}{L_1}'}}{{{L_1}' + {c_1^{t-1}}}} \le  - \frac{{{c_1^{t-1}}{L_1}'}}{{2{L_1}'}} =  - \frac{{{c_1^{t-1}}}}{2} \le  - \frac{{{c_1^t}}}{2}$. Multiplying both sides of the inequality Eq. (\ref{eq:A49}) by $\frac{8}{{{\rho _1}{c_1^t}}}$, we have,
{\begin{equation}
\label{eq:A51}
\begin{array}{l}
\sum\limits_{l = 1}^{|{{\bf{A}}^t}|}(\frac{4}{{{\rho _1}^2{c_1^t}}}||{\lambda _l^{t+1}} - {\lambda _l^t}|{|^2} - \frac{4}{{{\rho _1}}}(\frac{{{c_1^{t-1}} - {c_1^t}}}{{{c_1^t}}})||{\lambda _l^{t+1}}|{|^2}) \\

 \!\le \!\sum\limits_{l = 1}^{|{{\bf{A}}^t}|}\!( \frac{4}{{{\rho _1}^2{c_1^t}}}||{\lambda _l^t} - {\lambda _l^{t-1}}|{|^2} - \frac{4}{{{\rho _1}}}(\frac{{{c_1^{t-1}} - {c_1^t}}}{{{c_1^t}}})||{\lambda _l^t}|{|^2}  + \frac{{4{b_1^t}}}{{{\rho _1}{c_1^t}}}||{\lambda _l^{t+1}} - {\lambda _l^t}|{|^2} - \frac{4}{{{\rho _1}}}||{\lambda _l^t} - {\lambda _l^{t-1}}|{|^2} \\
 
 \qquad \; + \frac{{4{L^2}}}{{{\rho _1}{c_1^t}{b_1^t}}}(\sum\limits_{j = 1}^N {||{{\boldsymbol{w}}_j^{t+1}} - {{\boldsymbol{w}}_j^t}|{|^2}}  \! +\! ||{\boldsymbol{z}}^{t + 1} - {\boldsymbol{z}^t}|{|^2} \! +\! ||h^{t+1} - h^t|{|^2})).
\end{array}
\end{equation}}


Setting ${b_1^t} = \frac{{{c_1^t}}}{2}$ in Eq. (\ref{eq:A51}) and using the definition of ${S_1^t}$, we have,
\begin{equation}
\label{eq:A52}
\begin{array}{l}
{S_1^{t+1}} \! - \! {S_1^t} \\
\! \le\! \sum\limits_{l = 1}^{|{{\bf{A}}^t}|} {(\frac{4}{{{\rho _1}}}(\frac{{{c_1^{t-2}}}}{{{c_1^{t-1}}}} \! - \! \frac{{{c_1^{t-1}}}}{{{c_1^t}}})||{\lambda _l^t}|{|^2}}
 \! +\! \frac{{8{L^2}}}{{{\rho _1}({c_1^t}){^2}}}(\sum\limits_{j = 1}^N {||{{\boldsymbol{w}}_j^{t+1}} \! - \! {{\boldsymbol{w}}_j^t}|{|^2}}  \! +\! ||{\boldsymbol{z}}^{t + 1} \! - \! {\boldsymbol{z}^t}|{|^2} \! +\! ||h^{t+1} \! - \! h^t|{|^2}) \vspace{0.5ex}\\
 \! + (\frac{2}{{{\rho _1}}} \! +\! \frac{4}{{{\rho_1}^2}}(\frac{1}{{{c_1^{t+1}}}} \! - \! \frac{1}{{{c_1^t}}}))||{\lambda _l^{t+1}} \! - \! {\lambda _l^t}|{|^2} \! - \! \frac{4}{{{\rho _1}}}||{\lambda _l^t} \! - \! {\lambda _l^{t-1}}|{|^2}) \\
\! = \!\sum\limits_{l = 1}^{|{{\bf{A}}^t}|} {\frac{4}{{{\rho _1}}}(\frac{{{c_1^{t-2}}}}{{{c_1^{t-1}}}} \! - \! \frac{{{c_1^{t-1}}}}{{{c_1^t}}})||{\lambda _l^t}|{|^2}}  \! +\! \sum\limits_{l = 1}^{|{{\bf{A}}^t}|} {(\frac{2}{{{\rho _1}}} \! +\! \frac{4}{{{\rho_1}^2}}(\frac{1}{{{c_1^{t+1}}}} \! - \! \frac{1}{{{c_1^t}}}))||{\lambda _l^{t+1}} \! - \! {\lambda _l^t}|{|^2}} \\
\! - \sum\limits_{l = 1}^{|{{\bf{A}}^t}|} {\frac{4}{{{\rho _1}}}||{\lambda _l^t} \! - \! {\lambda _l^{t-1}}|{|^2} \vspace{1ex}}
\! +\! \frac{{8|{{\bf{A}}^t}|{L^2}}}{{{\rho _1}({c_1^t}){^2}}}(\sum\limits_{j = 1}^N {||{{\boldsymbol{w}}_j^{t+1}} \! - \! {{\boldsymbol{w}}_j^t}|{|^2}}  \! +\! ||{\boldsymbol{z}}^{t + 1} \! - \! {\boldsymbol{z}^t}|{|^2} \! +\! ||h^{t+1} \! - \! h^t|{|^2}).
\end{array}
\end{equation}
Likewise, according to Eq. (15), we have that,
\begin{equation}
\label{eq:A53}
\begin{array}{l}
\frac{1}{{{\rho _2}}}\left\langle {{{\boldsymbol{v}}_{2,l}^{t + 1}},{{\boldsymbol{\phi}}_j^{t+1}} \! - \! {{\boldsymbol{\phi}}_j^t}} \right\rangle \vspace{0.5ex}\\
\! \le \! \left\langle\! {{\nabla _{{{\boldsymbol{\phi}}_j}}}{{\widetilde{L}}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\},}}{\boldsymbol{z}}^{t+1},h^{t+1}, \! \{ {\lambda _l^{t+1}}\} , \!\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)  -  }}{\nabla _{{{\boldsymbol{\phi}}_j}}}{{\widetilde{L}}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\},}}{\boldsymbol{z}^t},h^t, \!\{ {\lambda _l^t}\},\{ {{\boldsymbol{\phi}}_j^{t-1}}\} {\rm{)}},{{\boldsymbol{\phi}}_j^{t+1}} \! - \! {{\boldsymbol{\phi}}_j^t}} \! \right\rangle \vspace{0.5ex}\\
\! = \! \left\langle\! {{\nabla _{{{\boldsymbol{\phi}}_j}}}{{\widetilde{L}}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^{t+1}}{\rm{\},}}{\boldsymbol{z}}^{t+1},h^{t+1}, \! \{ {\lambda _l^{t+1}}\} , \!\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} \! - \! {\nabla _{{{\boldsymbol{\phi}}_j}}}{{\widetilde{L}}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t, \! \{ {\lambda _l^t}\} , \!\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}},{{\boldsymbol{\phi}}_j^{t+1}} \! - \! {{\boldsymbol{\phi}}_j^t}} \!\right\rangle \vspace{0.5ex}\\
 \! + \! \left\langle\! {{\nabla _{{{\boldsymbol{\phi}}_j}}}{{\widetilde{L}}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\},}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} \! - \! {\nabla _{{{\boldsymbol{\phi}}_j}}}{{\widetilde{L}}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\},\{ {{\boldsymbol{\phi}}_j^{t-1}}\} {\rm{)}},{{\boldsymbol{v}}_{2,l}^{t + 1}}} \!\right\rangle \vspace{0.5ex}\\
 \! + \! \left\langle \! {{\nabla _{{{\boldsymbol{\phi}}_j}}}{{\widetilde{L}}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\},}}{\boldsymbol{z}^t},h^t, \!\{ {\lambda _l^t}\},\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} \! - \! {\nabla _{{{\boldsymbol{\phi}}_j}}}{{\widetilde{L}}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\},}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\},\{ {{\boldsymbol{\phi}}_j^{t-1}}\} {\rm{)}},{{\boldsymbol{\phi}}_j^t} \! - \! {{\boldsymbol{\phi}}_j^{t-1}}} \! \right\rangle .
\end{array}
\end{equation}
In addition, since
\begin{equation}
\label{eq:A54}
\begin{array}{l}
\frac{1}{{{\rho _2}}}\left\langle {{{\boldsymbol{v}}_{2,l}^{t+1}},{{\boldsymbol{\phi}}_j^{t+1}} - {{\boldsymbol{\phi}}_j^t}} \right\rangle 
 = \frac{1}{{2{\rho _2}}}||{{\boldsymbol{\phi}}_j^{t+1}} - {{\boldsymbol{\phi}}_j^t}|{|^2} \! +\! \frac{1}{{2{\rho _2}}}||{{\boldsymbol{v}}_{2,l}^{t+1}}|{|^2} - \frac{1}{{2{\rho _2}}}||{{\boldsymbol{\phi}}_j^t} - {{\boldsymbol{\phi}}_j^{t-1}}|{|^2},
\end{array}
\end{equation}
\vspace{-2mm}
it follows that,
{\begin{equation}
\label{eq:A55}
\begin{array}{l}
\frac{1}{{2{\rho _2}}}||{{\boldsymbol{\phi}}_j^{t+1}} \! - \! {{\boldsymbol{\phi}}_j^t}|{|^2} \! +\! \frac{1}{{2{\rho _2}}}||{{\boldsymbol{v}}_{2,l}^{t+1}}|{|^2} \! - \! \frac{1}{{2{\rho _2}}}||{{\boldsymbol{\phi}}_j^t} \! - \! {{\boldsymbol{\phi}}_j^{t-1}}|{|^2} \\

 \le \frac{{{L^2}}}{{2{b_2^t}}}(\sum\limits_{j = 1}^N {||{{\boldsymbol{w}}_j^{t+1}} \! - \! {{\boldsymbol{w}}_j^t}|{|^2}}  \! +\! ||{\boldsymbol{z}}^{t + 1} \! - \! {\boldsymbol{z}^t}|{|^2} \! +\! ||h^{t+1} \! - \! h^t|{|^2}) \! +\! \frac{{{b_2^t}}}{2}||{{\boldsymbol{\phi}}_j^{t+1}} \! - \! {{\boldsymbol{\phi}}_j^t}|{|^2} \\
 
 \! + \frac{{{c_2^{t-1}}  -  {c_2^t}}}{2}(||{{\boldsymbol{\phi}}_j^{t+1}}|{|^2} \! - \! ||{{\boldsymbol{\phi}}_j^t}|{|^2}) \! - \! \frac{{{c_2^{t-1}}  -  {c_2^t}}}{2}||{{\boldsymbol{\phi}}_j^{t+1}} \! - \! {{\boldsymbol{\phi}}_j^t}|{|^2}  -  \frac{{{c_2^{t-1}}L_2'}}{{L_2'  + {c_2^{t-1}}}}||{{\boldsymbol{\phi}}_j^t} \! - \! {{\boldsymbol{\phi}}_j^{t-1}}|{|^2} \vspace{0.5ex}\\
 
 \! + \frac{{{\rho _2}}}{2}||{\nabla _{{{\boldsymbol{\phi}}_j}}}{\widetilde{L}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\},}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\},\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} \! - \! {\nabla _{{{\boldsymbol{\phi}}_j}}}{\widetilde{L}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\},}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\},\{ {{\boldsymbol{\phi}}_j^{t-1}}\} {\rm{)}}|{|^2} \! +\! \frac{1}{{2{\rho _2}}}||{{\boldsymbol{v}}_{2,l}^{t+1}}|{|^2} \vspace{0.5ex}\\
 
 \! -  \frac{1}{{L_2'  + {c_2^{t-1}}}}||{\nabla _{{{\boldsymbol{\phi}}_j}}}{\widetilde{L}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\},}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\},\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}} \! - \! {\nabla _{{{\boldsymbol{\phi}}_j}}}{\widetilde{L}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\},}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\},\{ {{\boldsymbol{\phi}}_j^{t-1}}\} )|{|^2}.
\end{array}
\end{equation}}


According to the setting ${c_2^0} \le {L_2}'$, we have $- \frac{{{c_2^{t-1}}{L_2}'}}{{{L_2}' + {c_2^{t-1}}}} \le  - \frac{{{c_2^{t-1}}{L_2}'}}{{2{L_2}'}} =  - \frac{{{c_2^{t-1}}}}{2} \le  - \frac{{{c_2^t}}}{2}$.  Multiplying both sides of the inequality Eq. (\ref{eq:A55})  by $\frac{8}{{{\rho _2}{c_2^t}}}$, we have,
{\begin{equation}
\label{eq:A57}
\begin{array}{l}
\frac{4}{{{\rho _2}^2{c_2^t}}}||{{\boldsymbol{\phi}}_j^{t+1}} \! - \! {{\boldsymbol{\phi}}_j^t}|{|^2} \! - \! \frac{4}{{{\rho _2}}}(\frac{{{c_2^{t-1}}  -  {c_2^t}}}{{{c_2^t}}})||{{\boldsymbol{\phi}}_j^{t+1}}|{|^2} \\
\! \le \! \frac{4}{{{\rho _2}^2{c_2^t}}}||{{\boldsymbol{\phi}}_j^t} \! - \! {{\boldsymbol{\phi}}_j^{t-1}}|{|^2} \! - \! \frac{4}{{{\rho _2}}}(\frac{{{c_2^{t-1}}  -  {c_2^t}}}{{{c_2^t}}})||{{\boldsymbol{\phi}}_j^t}|{|^2} \! +\! \frac{{4{b_2^t}}}{{{\rho _2}{c_2^t}}}||{{\boldsymbol{\phi}}_j^{t+1}} \! - \! {{\boldsymbol{\phi}}_j^t}|{|^2} \! - \! \frac{4}{{{\rho _2}}}||{{\boldsymbol{\phi}}_j^t} \! - \! {{\boldsymbol{\phi}}_j^{t-1}}|{|^2} \\
 \! + \frac{{4{L^2}}}{{{\rho _2}{c_2^t}{b_2^t}}}(\sum\limits_{j = 1}^N {||{{\boldsymbol{w}}_j^{t+1}} \! - \! {{\boldsymbol{w}}_j^t}|{|^2}}  \! +\! ||{\boldsymbol{z}}^{t + 1} \! - \! {\boldsymbol{z}^t}|{|^2} \! +\! ||h^{t+1} \! - \! h^t|{|^2}).
\end{array}
\end{equation}}

Setting ${b_2^t} = \frac{{{c_2^t}}}{2}$ in Eq. (\ref{eq:A57}) and using the definition of ${S_2^t}$, we can obtain,
\begin{equation}
\label{eq:A58}
\begin{array}{l}
{S_2^{t+1}} \! - \! {S_2^t} \\
\! \le \! \sum\limits_{j = 1}^N \! {(\frac{4}{{{\rho _2}}}(\frac{{{c_2^{t-2}}}}{{{c_2^{t-1}}}} \! - \! \frac{{{c_2^{t-1}}}}{{{c_2^t}}})||{{\boldsymbol{\phi}}_j^t}|{|^2}}  
 \! +\! \frac{{8{L^2}}}{{{\rho _2}({c_2^t})^2}}(\!\sum\limits_{j = 1}^N\! {||{{\boldsymbol{w}}_j^{t+1}} \! - \! {{\boldsymbol{w}}_j^t}|{|^2}}  \! +\! ||{\boldsymbol{z}}^{t + 1} \! - \! {\boldsymbol{z}^t}|{|^2} \! +\! \sum\limits_{l = 1}^{|{{\bf{A}}^t}|}\! {||{\lambda _l^{t+1}} \! - \! {\lambda _l^t}|{|^2}} ) \\
 \! + (\frac{2}{{{\rho _2}}} \! +\! \frac{2}{{{\rho _2}^2}}(\frac{1}{{{c_2^{t+1}}}} \! - \! \frac{1}{{{c_2^t}}}))||{{\boldsymbol{\phi}}_j^{t+1}} \! - \! {{\boldsymbol{\phi}}_j^t}|{|^2} \! - \! \frac{4}{{{\rho _2}}}||{{\boldsymbol{\phi}}_j^t} \! - \! {{\boldsymbol{\phi}}_j^{t-1}}|{|^2}) \\
\! = \! \sum\limits_{j = 1}^N \! {\frac{4}{{{\rho _2}}}(\frac{{{c_2^{t-2}}}}{{{c_2^{t-1}}}} \! - \! \frac{{{c_2^{t-1}}}}{{{c_2^t}}})||{{\boldsymbol{\phi}}_j^t}|{|^2}}
 \! +\! \sum\limits_{j = 1}^N \!{(\frac{2}{{{\rho _2}}} \! +\! \frac{4}{{{\rho _2}^2}}(\frac{1}{{{c_2^{t+1}}}} \! - \! \frac{1}{{{c_2^t}}}))||{{\boldsymbol{\phi}}_j^{t+1}} \! - \! {{\boldsymbol{\phi}}_j^t}|{|^2}} \\
 \! - \! \sum\limits_{j = 1}^N \! {\frac{4}{{{\rho _2}}}||{{\boldsymbol{\phi}}_j^t} \! - \! {{\boldsymbol{\phi}}_j^{t-1}}|{|^2}}  \! +\! \frac{{8N{L^2}}}{{{\rho _2}({c_2^t})^2}}(\!\sum\limits_{j = 1}^N\! {||{{\boldsymbol{w}}_j^{t+1}} \! - \! {{\boldsymbol{w}}_j^t}|{|^2}}  \! +\! ||{\boldsymbol{z}}^{t + 1} \! - \! {\boldsymbol{z}^t}|{|^2} \! +\! ||h^{t+1} \! - \! h^t|{|^2}).
\end{array}
\end{equation}

According to the setting about ${c_1^t}$ and ${c_2^t}$, we have $\frac{{{\rho _1}}}{{10}} \ge \frac{1}{{{c_1^{t+1}}}} - \frac{1}{{{c_1^t}}},
\frac{{{\rho _2}}}{{10}} \ge \frac{1}{{{c_2^{t+1}}}} - \frac{1}{{{c_2^t}}},{\rm{    }}\forall t \ge T_1$.
Using the definition of $F^{t+1}$ and combining it with Eq. (\ref{eq:A52}), (\ref{eq:A58}), $\forall t \ge T_1 + \tau$, we have,
{\begin{equation}
\label{eq:A59}
\begin{array}{l}
F^{t+1} \! - \! F^{t}\\
 \!\le\! (\frac{{L  + 1}}{2} \! - \! \frac{1}{{{\eta _{\boldsymbol{w}}^t}}} \! +\! \frac{{{\rho _1}|{{\bf{A}}^t}|{L^2}}}{2} \! +\! \frac{{{\rho _2}|{{\bf{Q}}^{t + 1}}|{L^2}}}{2} \! +\! \frac{{8|{{\bf{A}}^t}|{L^2}}}{{{\rho _1}({c_1^t})^2}} \! +\! \frac{{8N{L^2}}}{{{\rho _2}({c_2^t})^2}})\sum\limits_{j = 1}^N {||{{\boldsymbol{w}}_j^{t+1}} \! - \! {{\boldsymbol{w}}_j^t}|{|^2}} \\
 
 \! + (\frac{{L  + 3\tau{ k_1 }N{L^2}}}{2} \! - \! \frac{1}{{{\eta _{\boldsymbol{z}}^t}}} \! +\! \frac{{{\rho _1}|{{\bf{A}}^t}|{L^2}}}{2} \! +\! \frac{{{\rho _2}|{{\bf{Q}}^{t + 1}}|{L^2}}}{2} \! +\! \frac{{8|{{\bf{A}}^t}|{L^2}}}{{{\rho _1}({c_1^t})^2}} \! +\! \frac{{8N{L^2}}}{{{\rho _2}({c_2^t})^2}})||\boldsymbol{z}^{t+1} \! - \! \boldsymbol{z}^t|{|^2}\\
 
 \! + (\frac{{L  + 3\tau{ k_1 }N{L^2}}}{2} \! - \! \frac{1}{{{\eta _h^t}}} \! +\! \frac{{{\rho _1}|{{\bf{A}}^t}|{L^2}}}{2} \! +\! \frac{{{\rho _2}|{{\bf{Q}}^{t + 1}}|{L^2}}}{2} \! +\! \frac{{8|{{\bf{A}}^t}|{L^2}}}{{{\rho _1}({c_1^t})^2}} \! +\! \frac{{8N{L^2}}}{{{\rho _2}({c_2^t})^2}})||h^{t+1} \! - \! h^t|{|^2}\\
 
 \! -  (\frac{1}{{10{\rho _1}}} \! - \! \frac{{3\tau{ k_1 }N{L^2}}}{2})\sum\limits_{l = 1}^{|{{\bf{A}}^t}|} {||{\lambda _l^{t+1}} \! - \! {\lambda _l^t}|{|^2}}  \! - \! \frac{1}{{10{\rho _2}}}\sum\limits_{j = 1}^N {||{{\boldsymbol{\phi}}_j^{t+1}} \! - \! {{\boldsymbol{\phi}}_j^t}|{|^2}} 
 \! +\! \frac{{{c_1^{t-1}}  -  {c_1^t}}}{2}\sum\limits_{l = 1}^{|{{\bf{A}}^t}|} {||{\lambda _l^{t+1}}|{|^2}} \\
 
 \! + \frac{{{c_2^{t-1}}  -  {c_2^t}}}{2}\sum\limits_{j = 1}^N {||{{\boldsymbol{\phi}}_j^{t+1}}|{|^2}} 
 \! +\! \frac{4}{{{\rho _1}}}(\frac{{{c_1^{t-2}}}}{{{c_1^{t-1}}}} \! - \! \frac{{{c_1^{t-1}}}}{{{c_1^t}}})\sum\limits_{l = 1}^{|{{\bf{A}}^t}|} {||{\lambda _l^t}|{|^2}}  \! +\! \frac{4}{{{\rho _2}}}(\frac{{{c_2^{t-2}}}}{{{c_2^{t-1}}}} \! - \! \frac{{{c_2^{t-1}}}}{{{c_2^t}}})\sum\limits_{j = 1}^N {||{{\boldsymbol{\phi}}_j^t}|{|^2}},
\end{array}
\end{equation}}

%\vspace{3ex}

Next, we will combine Lemma \ref{lemma 1}, Lemma \ref{lemma 2} with Lemma \ref{lemma3} to derive Theorem 1. Firstly, we make some definitions about our problem.

\renewcommand{\thedefinition}{A.\arabic{definition}}
\begin{definition}
\label{definition:A1}
The \textit{stationarity} \textit{gap} at $t^{{th}}$ iteration is defined as:
{\begin{equation}
\label{eq:A61}
\nabla G^t = \left[ \begin{array}{l}
\{ \frac{1}{{{\alpha _{\boldsymbol{w}}^t}}}({{\boldsymbol{w}}_j^t} - {\mathcal{P}_{{\boldsymbol{\mathcal{W}}}}}({{\boldsymbol{w}}_j^t} - {\alpha _{\boldsymbol{w}}^t}{\nabla _{{{\boldsymbol{w}}_j}}}{L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)))\}}} \vspace{0.2ex}\\
\frac{1}{{{\eta _{\boldsymbol{z}}^t}}}({\boldsymbol{z}^t} - {\mathcal{P}_{{\boldsymbol{\mathcal{Z}}}}}({\boldsymbol{z}^t} - {\eta _{\boldsymbol{z}}^t}{\nabla _{\boldsymbol{z}}}{L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{))}}) \vspace{0.2ex}\\
\frac{1}{{{\eta _h^t}}}(h^t - {\mathcal{P}_{{\boldsymbol{\mathcal{H}}}}}(h^t - {\eta _h^t}{\nabla _h}{L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}})) \vspace{0.2ex}\\
 \{ \frac{1}{{{\rho _1}}}({\lambda _l^t} - {\mathcal{P}_{\bf{\Lambda}} }({\lambda _l^t} \! +\! {\rho _1}{\nabla _{{\lambda _l}}}{L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)))\}}} \vspace{0.2ex}\\
\{ \frac{1}{{{\rho _2}}}({{\boldsymbol{\phi}}_j^t} - {\mathcal{P}_{{\boldsymbol{\Phi}}}}({{\boldsymbol{\phi}}_j^t} \! +\! {\rho _2}{\nabla _{{{\boldsymbol{\phi}}_j}}}{L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)))\} }}
\end{array} \right].
\end{equation}}

And we also define:
{\begin{equation}
\label{eq:A62}
\begin{array}{l}
{(\nabla G^t)_{{{\boldsymbol{w}}_j}}} = \frac{1}{{{\alpha _{\boldsymbol{w}}^t}}}({{\boldsymbol{w}}_j^t} - {\mathcal{P}_{{\boldsymbol{\mathcal{W}}}}}({{\boldsymbol{w}}_j^t} - {\alpha _{\boldsymbol{w}}^t}{\nabla _{{{\boldsymbol{w}}_j}}}{L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)))}}, \vspace{0.2ex}\\
{(\nabla G^t)_{\boldsymbol{z}}} = \frac{1}{{{\eta _{\boldsymbol{z}}^t}}}({\boldsymbol{z}^t} - {\mathcal{P}_{{\boldsymbol{\mathcal{Z}}}}}({\boldsymbol{z}^t} - {\eta _{\boldsymbol{z}}^t}{\nabla _{\boldsymbol{z}}}{L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}})), \vspace{0.2ex}\\
{(\nabla G^t)_h} = \frac{1}{{{\eta _h^t}}}(h^t - {\mathcal{P}_{{\boldsymbol{\mathcal{H}}}}}(h^t - {\eta _h^t}{\nabla _h}{L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}})), \vspace{0.2ex}\\
{(\nabla G^t)_{{\lambda _l}}} = \frac{1}{{{\rho _1}}}({\lambda _l^t} - {\mathcal{P}_{\bf{\Lambda}} }({\lambda _l^t} \! +\! {\rho _1}{\nabla _{{\lambda _l}}}{L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)))}}, \vspace{0.2ex}\\
{(\nabla G^t)_{{{\boldsymbol{\phi}}_j}}} = \frac{1}{{{\rho _2}}}({{\boldsymbol{\phi}}_j^t} - {\mathcal{P}_{{\boldsymbol{\Phi}}}}({{\boldsymbol{\phi}}_j^t} \! +\! {\rho _2}{\nabla _{{{\boldsymbol{\phi}}_j}}}{L_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)))}}.
\end{array}
\end{equation}}

It follows that,
{\begin{equation}
\label{eq:A63}
||\nabla G^t|{|^2} = \sum\limits_{j = 1}^N {||{{(\nabla G^t)}_{{{\boldsymbol{w}}_j}}}|{|^2}}  \! +\! ||{(\nabla G^t)_{\boldsymbol{z}}}|{|^2} \! +\! ||{(\nabla G^t)_h}|{|^2} \! +\! \sum\limits_{l = 1}^{|{{\bf{A}}^t}|} {||{{(\nabla G^t)}_{{\lambda _l}}}|{|^2}}  \! +\! \sum\limits_{j = 1}^N {||{{(\nabla G^t)}_{{{\boldsymbol{\phi}}_j}}}|{|^2}}. 
\end{equation}}
\end{definition}


\begin{definition}
\label{definition:A2}
At $t^{{th}}$ iteration, the \textit{stationarity} \textit{gap} \textit{w.r.t} ${\widetilde{L}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j}{\rm{\} ,}}{\boldsymbol{z}},h,\{ {\lambda _l}\} ,\{ {{\boldsymbol{\phi}}_j}\} {\rm{)}}$ is defined as:

{\begin{equation}
\label{eq:A64}
\nabla {\widetilde{G}}^t = \left[ \begin{array}{l}\{ \frac{1}{{{\alpha _{\boldsymbol{w}}^t}}}({{\boldsymbol{w}}_j^t} - {\mathcal{P}_{{\boldsymbol{\mathcal{W}}}}}({{\boldsymbol{w}}_j^t} - {\alpha _{\boldsymbol{w}}^t}{\nabla _{{{\boldsymbol{w}}_j}}}{\widetilde{L}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)))\}}} \vspace{0.2ex}\\
\frac{1}{{{\eta _{\boldsymbol{z}}^t}}}({\boldsymbol{z}^t} - {\mathcal{P}_{{\boldsymbol{\mathcal{Z}}}}}({\boldsymbol{z}^t} - {\eta _{\boldsymbol{z}}^t}{\nabla _{\boldsymbol{z}}}{\widetilde{L}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{))}}) \vspace{0.2ex}\\
\frac{1}{{{\eta _h^t}}}(h^t - {\mathcal{P}_{{\boldsymbol{\mathcal{H}}}}}(h^t - {\eta _h^t}{\nabla _h}{\widetilde{L}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}})) \vspace{0.2ex}\\
\{ \frac{1}{{{\rho _1}}}({\lambda _l^t} - {\mathcal{P}_{\bf{\Lambda}} }({\lambda _l^t} \! +\! {\rho _1}{\nabla _{{\lambda _l}}}{\widetilde{L}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)))\} }} \vspace{0.2ex}\\
\{ \frac{1}{{{\rho _2}}}({{\boldsymbol{\phi}}_j^t} - {\mathcal{P}_{{\boldsymbol{\Phi}}}}({{\boldsymbol{\phi}}_j^t} \! +\! {\rho _2}{\nabla _{{{\boldsymbol{\phi}}_j}}}{\widetilde{L}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)))\} }}
\end{array} \right].
\end{equation}}

We further define:
{\begin{equation}
\label{eq:A65}
\begin{array}{l}
{(\nabla{\widetilde{G}}^t)_{{{\boldsymbol{w}}_j}}} = \frac{1}{{{\alpha _{\boldsymbol{w}}^t}}}({{\boldsymbol{w}}_j^t} - {\mathcal{P}_{{\boldsymbol{\mathcal{W}}}}}({{\boldsymbol{w}}_j^t} - {\alpha _{\boldsymbol{w}}^t}{\nabla _{{{\boldsymbol{w}}_j}}}{\widetilde{L}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)))}}, \vspace{0.2ex}\\
{(\nabla{\widetilde{G}}^t)_{\boldsymbol{z}}} = \frac{1}{{{\eta _{\boldsymbol{z}}^t}}}({\boldsymbol{z}^t} - {\mathcal{P}_{{\boldsymbol{\mathcal{Z}}}}}({\boldsymbol{z}^t} - {\eta _{\boldsymbol{z}}^t}{\nabla _{\boldsymbol{z}}}{\widetilde{L}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}})), \vspace{0.2ex}\\
{(\nabla{\widetilde{G}}^t)_h} = \frac{1}{{{\eta _h^t}}}(h^t - {\mathcal{P}_{{\boldsymbol{\mathcal{H}}}}}(h^t - {\eta _h^t}{\nabla _h}{\widetilde{L}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)}})), \vspace{0.2ex}\\
{(\nabla{\widetilde{G}}^t)_{{\lambda _l}}} = \frac{1}{{{\rho _1}}}({\lambda _l^t} - {\mathcal{P}_{\bf{\Lambda}} }({\lambda _l^t} \! +\! {\rho _1}{\nabla _{{\lambda _l}}}{\widetilde{L}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)))}}, \vspace{0.2ex}\\
{(\nabla{\widetilde{G}}^t)_{{{\boldsymbol{\phi}}_j}}} = \frac{1}{{{\rho _2}}}({{\boldsymbol{\phi}}_j^t} - {\mathcal{P}_{{\boldsymbol{\Phi}}}}({{\boldsymbol{\phi}}_j^t} \! +\! {\rho _2}{\nabla _{{{\boldsymbol{\phi}}_j}}}{\widetilde{L}_p}{\rm{(\{ }}{{\boldsymbol{w}}_j^t}{\rm{\} ,}}{\boldsymbol{z}^t},h^t,\{ {\lambda _l^t}\} ,\{ {{\boldsymbol{\phi}}_j^t}\} {\rm{)))}}.
\end{array}
\end{equation}}

It follows that,
{\begin{equation}
\label{eq:A66}
||\nabla{\widetilde{G}}^t|{|^2} = \sum\limits_{j = 1}^N {||{{(\nabla{\widetilde{G}}^t)}_{{{\boldsymbol{w}}_j}}}|{|^2}}  \! +\! ||{(\nabla{\widetilde{G}}^t)_{\boldsymbol{z}}}|{|^2} \! +\! ||{(\nabla{\widetilde{G}}^t)_h}|{|^2} \! +\!\sum\limits_{l = 1}^{|{{\bf{A}}^t}|} {||{{(\nabla{\widetilde{G}}^t)}_{{\lambda _l}}}|{|^2}}  \! +\! \sum\limits_{j = 1}^N {||{{(\nabla{\widetilde{G}}^t)}_{{{\boldsymbol{\phi}}_j}}}|{|^2}}. 
\end{equation}}
\end{definition}

\begin{definition}
\label{definition:A3}
In asynchronous algorithm, for worker $j$ in $t^{th}$ iteration, we define the last iteration where worker $j$ was active as $\widetilde{{t}}_j $. And we define the next iteration that  worker $j$ will be active as $\overline{{t}_j}$. For the iteration index set that worker $j$ is active from $T_1^{th}$ to $(T_1 + T+\tau)^{th}$ iteration, we define it as $\mathcal{V}_j(T)$. And the $i^{{th}}$ element in $\mathcal{V}_j(T)$ is defined as $\hat{v}_j(i)$.
\end{definition}


\vspace{1ex}

\noindent \emph{\textbf{Proof of Theorem 1:}}

Firstly, setting:
\begin{equation}
\label{eq:A67}
\begin{array}{l}
{a_5^t} = \frac{{4|{{\bf{A}}^t}|(\gamma  - 2){L^2}}}{{{\rho _1}({c_1^t})^2}} + \frac{{4N(\gamma  - 2){L^2}}}{{{\rho _2}({c_2^t})^2}} + \frac{{{\rho _2}(N - |{{\bf{Q}}^{t + 1}}|){L^2}}}{2} - \frac{1}{2},
\end{array}
\end{equation}
\begin{equation}
\label{eq:A67-1}
\begin{array}{l}
{a_6^t} = \frac{{4|{{\bf{A}}^t}|(\gamma  - 2){L^2}}}{{{\rho _1}({c_1^t})^2}} + \frac{{4N(\gamma  - 2){L^2}}}{{{\rho _2}({c_2^t})^2}} + \frac{{{\rho _2}(N - |{{\bf{Q}}^{t + 1}}|){L^2}}}{2} - \frac{{3\tau{ k_1 }N{L^2}}}{2},
\end{array}
\end{equation}

\noindent where $\gamma $ is a constant which satisfies $\gamma \ge 2$ and $\frac{{4(\gamma  - 2){L^2}}}{{{\rho _1}({c_1^0}){^2}}} + \frac{{4N(\gamma  - 2){L^2}}}{{{\rho _2}({c_2^0}){^2}}} + \frac{{{\rho _2}(N - S){L^2}}}{2} \ge \max \{ \frac{1}{2},\frac{{3\tau{ k_1 }N{L^2}}}{2}\}$. It is seen that the ${a_5^t},{a_6^t}$ are nonnegative sequences. Since $\forall t \ge 0$, $|{{\bf{A}}^0}| \le |{{\bf{A}}^t}|$, $({c_1^0}){^2} \ge ({c_1^t})^2$, $({c_2^0}){^2} \ge ({c_2^t})^2$, and we assume that $|{{\bf{Q}}^{t + 1}}|=S, \forall t$, thus we have ${a_5^0} \le {a_5^t}, {a_6^0} \le {a_6^t},  \forall t \ge 0$. According to the setting of ${\eta _{\boldsymbol{w}}^t}$, ${\eta _{\boldsymbol{z}}^t}$, ${\eta _h^t}$ and $c_1^t$, $c_2^t$, we have,
\begin{equation}
\label{eq:A68}
\frac{L\! +\!1}{2} - \frac{1}{{{\eta _{\boldsymbol{w}}^t}}} \! +\! \frac{{{\rho _1}|{{\bf{A}}^t}|{L^2}}}{2} \! +\! \frac{{{\rho _2}|{{\bf{Q}}^{t + 1}}|{L^2}}}{2} \! +\! \frac{{8|{{\bf{A}}^t}|{L^2}}}{{{\rho _1}({c_1^t})^2}} \! +\! \frac{{8N{L^2}}}{{{\rho _2}({c_2^t})^2}} =  - {a_5^t},
\end{equation}
\begin{equation}
\label{eq:A69}
\frac{L \! +\! 3\tau{ k_1 }N{L^2}}{2} - \frac{1}{{{\eta _{\boldsymbol{z}}^t}}} \! +\! \frac{{{\rho _1}|{{\bf{A}}^t}|{L^2}}}{2} \! +\! \frac{{{\rho _2}|{{\bf{Q}}^{t + 1}}|{L^2}}}{2} \! +\! \frac{{8|{{\bf{A}}^t}|{L^2}}}{{{\rho _1}({c_1^t})^2}} \! +\! \frac{{8N{L^2}}}{{{\rho _2}({c_2^t})^2}} =  - {a_6^t},
\end{equation}
\begin{equation}
\label{eq:A70}
\frac{L\! +\! 3\tau{ k_1 }N{L^2}}{2} - \frac{1}{{{\eta _h^t}}} \! +\! \frac{{{\rho _1}|{{\bf{A}}^t}|{L^2}}}{2} \! +\! \frac{{{\rho _2}|{{\bf{Q}}^{t + 1}}|{L^2}}}{2} \! +\! \frac{{8|{{\bf{A}}^t}|{L^2}}}{{{\rho _1}({c_1^t})^2}} \! +\! \frac{{8N{L^2}}}{{{\rho _2}({c_2^t})^2}} =  - {a_6^t}.
\end{equation}

Combining Eq. (\ref{eq:A68}), (\ref{eq:A69}), (\ref{eq:A70})  with Lemma \ref{lemma3}, $\forall t \ge T_1 + \tau$, it follows that,
{\begin{equation}
\label{eq:A71}
\begin{array}{l}
{a_5^t}\sum\limits_{j = 1}^N {||{{\boldsymbol{w}}_j^{t+1}} - {{\boldsymbol{w}}_j^t}|{|^2}}  \! +\! {a_6^t}||\boldsymbol{z}^{t+1} - \boldsymbol{z}^t|{|^2}  + {a_6^t}||h^{t+1} - h^t|{|^2}\\
 \! + {(\frac{1}{{10{\rho _1}}}\! -\! \frac{{3\tau{ k_1 }N{L^2}}}{2})  \sum\limits_{l = 1}^{|{{\bf{A}}^t}|}||{\lambda _l^{t+1}} - {\lambda _l^t}|{|^2}}   \! +\! \frac{1}{{10{\rho _2}}}\sum\limits_{j = 1}^N {||{{\boldsymbol{\phi}}_j^{t+1}} - {{\boldsymbol{\phi}}_j^t}|{|^2}} \\
 
 \le F^{t} - F^{t+1} \! +\! \frac{{{c_1^{t-1}} - {c_1^t}}}{2}\sum\limits_{l = 1}^{|{{\bf{A}}^t}|} {||{\lambda _l^{t+1}}|{|^2}}  \! +\! \frac{{{c_2^{t-1}} - {c_2^t}}}{2}\sum\limits_{j = 1}^N {||{{\boldsymbol{\phi}}_j^{t+1}}|{|^2}} \\
  +  {\frac{4}{{{\rho _1}}}(\frac{{{c_1^{t-2}}}}{{{c_1^{t-1}}}} - \frac{{{c_1^{t-1}}}}{{{c_1^t}}})\sum\limits_{l = 1}^{|{{\bf{A}}^t}|}||{\lambda _l^t}|{|^2}}  \! +\!  {\frac{4}{{{\rho _2}}}(\frac{{{c_2^{t-2}}}}{{{c_2^{t-1}}}} - \frac{{{c_2^{t-1}}}}{{{c_2^t}}})\sum\limits_{j = 1}^N||{{\boldsymbol{\phi}}_j^t}|{|^2}}.
\end{array}
\end{equation}}

Combining the definition of ${(\nabla{\widetilde{G}}^t)_{{{\boldsymbol{w}}_j}}}$ with trigonometric inequality, Cauchy-Schwarz inequality and Assumption 1 and 2, $\forall t \ge T_1 + \tau$, we have,
{\begin{equation}
\label{eq:A72}
\begin{array}{l}
||{(\nabla {\widetilde{G}}^t)_{{{\boldsymbol{w}}_j}}}|{|^2}
 \!\le\! \frac{2}{{\underline{\eta _{\boldsymbol{w}}}}^2}||{{\boldsymbol{w}}_j^{\overline{{t}_j}}} \!-\! {{\boldsymbol{w}}_j^t}|{|^2} \! +\! 6\tau{k_1}{L^2}(||\boldsymbol{z}^{t+1} \!-\! \boldsymbol{z}^t|{|^2} \! +\! ||h^{t+1} \!-\! h^t|{|^2} \! +\! \sum\limits_{l = 1}^{|{{\bf{A}}^t}|} \! {||{\lambda _l^{t+1}} \!-\! {\lambda _l^t}|{|^2}}).
\end{array}
\end{equation}}



Combining the definition of ${(\nabla {\widetilde{G}}^t)_{\boldsymbol{z}}}$ with trigonometric inequality and Cauchy-Schwarz inequality, we can obtain the following inequality,
{\begin{equation}
\label{eq:A73}
\begin{array}{l}
||{(\nabla {\widetilde{G}}^t)_{\boldsymbol{z}}}|{|^2}

\! \le \! 2{L^2}\sum\limits_{j = 1}^N {||{{\boldsymbol{w}}_j^{t+1}} - {{\boldsymbol{w}}_j^t}|{|^2}}  \! +\! \frac{2}{({\eta _{\boldsymbol{z}}^t})^2}||\boldsymbol{z}^{t+1} - \boldsymbol{z}^t|{|^2}.
\end{array}
\end{equation}}


Likewise, combining the definition of ${(\nabla {\widetilde{G}}^t)_h}$ with trigonometric inequality and Cauchy-Schwarz inequality, we have that,
{\begin{equation}
\label{eq:A75}
\begin{array}{l}
||{(\nabla {\widetilde{G}}^t)_h}|{|^2}
 \le 2{L^2}(\sum\limits_{j = 1}^N {||{{\boldsymbol{w}}_j^{t+1}} \!-\! {{\boldsymbol{w}}_j^t}|{|^2}}  \! +\! ||{\boldsymbol{z}}^{t + 1} \!-\! {\boldsymbol{z}^t}|{|^2}) \! +\! \frac{2}{{({\eta _h^t})^2}}||h^{t+1} \!-\! h^t|{|^2}.
\end{array}
\end{equation}}

Combining the definition of ${(\nabla {\widetilde{G}}^t)_{{\lambda _l}}}$ with trigonometric inequality and Cauchy-Schwarz inequality,  we have that,
{\begin{equation}
\label{eq:A76}
\begin{array}{l}
||{(\nabla {\widetilde{G}}^t)_{{\lambda _l}}}|{|^2}\\
\! \le \! \frac{3}{{{\rho_1}^2}}||{\lambda _l^{t+1}} \! -\! {\lambda _l^t}|{|^2} \! +\! 3{L^2}(\!\sum\limits_{j = 1}^N\! {||{{\boldsymbol{w}}_j^{t+1}} \! -\! {{\boldsymbol{w}}_j^t}|{|^2}}  \! +\! ||{\boldsymbol{z}}^{t + 1} \! -\! {\boldsymbol{z}^t}|{|^2} \! +\! ||h^{t+1} \! -\! h^t|{|^2}) 
 \! +\! 3({{c_1^{t-1}} \! -\! {c_1^t})^2}||{\lambda _l^t}|{|^2}\\
\! \le \! \frac{3}{{{\rho_1}^2}}||{\lambda _l^{t+1}} \! -\! {\lambda _l^t}|{|^2} \! +\! 3{L^2}(\!\sum\limits_{j = 1}^N\! {||{{\boldsymbol{w}}_j^{t+1}} \! -\! {{\boldsymbol{w}}_j^t}|{|^2}}  \! +\! ||{\boldsymbol{z}}^{t + 1} \! -\! {\boldsymbol{z}^t}|{|^2} \! +\! ||h^{t+1} \! -\! h^t|{|^2}) 
 \! +\! 3{(({c_1^{t-1}})^2 \! -\! ({c_1^t})^2)}||{\lambda _l^t}|{|^2}.
\end{array}
\end{equation}}


Combining the definition of ${(\nabla {\widetilde{G}}^t)_{{{\boldsymbol{\phi}}_j}}}$ with Cauchy-Schwarz inequality and Assumption 2, we have,
{\begin{equation}
\label{eq:A77}
\begin{array}{l}
||{(\nabla {\widetilde{G}}^t)_{{{\boldsymbol{\phi}}_j}}}|{|^2}\\
\! \le \! \frac{3}{{{\rho_2}^2}}||{{\boldsymbol{\phi}}_j^{\overline{{t}_j}}} \! -\! {{\boldsymbol{\phi}}_j^t}|{|^2} \! +\! 3{L^2}(\!\sum\limits_{j = 1}^N\! {||{{\boldsymbol{w}}_j^{\overline{{t}_j}}} \! -\! {{\boldsymbol{w}}_j^t}|{|^2}}  \! +\! ||{\boldsymbol{z}}^{\overline{{t}_j}} \! -\! {\boldsymbol{z}^t}|{|^2})
 \! +\! 3{({c_2^{\widetilde{{t}}_j  - 1}} \! -\! {c_2^{\overline{{t}_j} - 1}})^2}||{{\boldsymbol{\phi}}_j^t}|{|^2}\\
\! \le\! \frac{3}{{{\rho_2}^2}}||{{\boldsymbol{\phi}}_j^{\overline{{t}_j}}} \! -\! {{\boldsymbol{\phi}}_j^t}|{|^2} \! +\! 3{L^2}(\!\sum\limits_{j = 1}^N\! {||{{\boldsymbol{w}}_j^{\overline{{t}_j}}} \! -\! {{\boldsymbol{w}}_j^t}|{|^2}}  \! +\!\tau{ k_1 }(||\boldsymbol{z}^{t+1} \!-\! \boldsymbol{z}^t|{|^2} \! +\! ||h^{t+1} \!-\! h^t|{|^2} \! +\! \sum\limits_{l = 1}^{|{{\bf{A}}^t}|} \! {||{\lambda _l^{t+1}} \!-\! {\lambda _l^t}|{|^2}}))\\
  + 3{(({c_2^{\widetilde{{t}}_j  - 1}})^2 \! -\! ({c_2^{\overline{{t}_j} -1}})^2)}||{{\boldsymbol{\phi}}_j^t}|{|^2}.
\end{array}
\end{equation}}



According to the Definition \ref{definition:A2} as well as Eq. (\ref{eq:A72}), (\ref{eq:A73}), (\ref{eq:A75}), (\ref{eq:A76}) and Eq. (\ref{eq:A77}), $\forall t \ge T_1 + \tau$, we have that,
{\begin{equation}
\label{eq:A78}
\begin{array}{l}
||\nabla {\widetilde{G}}^t|{|^2}
 = \sum\limits_{j = 1}^N {||{{(\nabla {\widetilde{G}}^t)}_{{{\boldsymbol{w}}_j}}}|{|^2}}  \! +\! ||{(\nabla {\widetilde{G}}^t)_{\boldsymbol{z}}}|{|^2} \! +\! ||{(\nabla {\widetilde{G}}^t)_h}|{|^2} \! +\! \sum\limits_{l = 1}^{|{{\bf{A}}^t}|} {||{{(\nabla {\widetilde{G}}^t)}_h}|{|^2}}  \! +\! \sum\limits_{j = 1}^N {||{{(\nabla {\widetilde{G}}^t)}_{{{\boldsymbol{\phi}}_j}}}|{|^2}} \\
 
\quad \quad \quad \; \; \; \le (\frac{2}{{\underline{\eta _{\boldsymbol{w}}}}^2} \! +\! 3NL^2)\sum\limits_{j = 1}^N {||{{\boldsymbol{w}}_j^{\overline{{t}_j}}} - {{\boldsymbol{w}}_j^t}|{|^2}}\! +\! (4 \! +\! 3|{{\bf{A}}^t}|){L^2}\sum\limits_{j = 1}^N {||{{\boldsymbol{w}}_j^{t+1}} - {{\boldsymbol{w}}_j^t}|{|^2}} \\

\quad \quad \quad \; \; \; + (\frac{2}{({\eta _{\boldsymbol{z}}^t})^2} \! +\! (2 \! +\! 9\tau{ k_1 }N \! +\! 3|{{\bf{A}}^t}|){L^2})||\boldsymbol{z}^{t+1} \! -\! \boldsymbol{z}^t|{|^2}
 \! +\! (\frac{2}{({\eta _h^t})^2} \! +\! (9\tau{ k_1 }N \! +\! 3|{{\bf{A}}^t}|){L^2})||h^{t+1}\! -\! h^t|{|^2} \\ 
 
\quad \quad \quad \; \; \; + \sum\limits_{l = 1}^{|{{\bf{A}}^t}|} {(\frac{3}{{{\rho_1}^2}} \! +\! 9\tau{ k_1 }N{L^2})||{\lambda _l^{t+1}} - {\lambda _l^t}|{|^2}}  \! +\! \sum\limits_{l = 1}^{|{{\bf{A}}^t}|} {3{{(({c_1^{t-1}})^2 - ({c_1^t})^2)}}||{\lambda _l^t}|{|^2}} \\

\quad \quad \quad \; \; \; + \sum\limits_{j = 1}^N {\frac{3}{{{\rho_2}^2}}||{{\boldsymbol{\phi}}_j^{\overline{{t}_j}}} - {{\boldsymbol{\phi}}_j^t}|{|^2}}  \! +\! \sum\limits_{j = 1}^N {3{(({c_2^{\widetilde{{t}}_j - 1}})^2 - ({c_2^{\overline{{t}_j}-1}})^2)}||{{\boldsymbol{\phi}}_j^t}|{|^2}} .
\end{array}
\end{equation}}



We set constant $d_1$, $d_2$, $d_3$ as,
\begin{equation}
\label{eq:A79}{d_1} = \frac{{2 k_{\tau} \tau \! +\! (4 \! +\! 3M \! +\! 3 k_{\tau}  \tau N){L^2}\underline{\eta _{\boldsymbol{w}}}^2}}{{\underline{\eta _{\boldsymbol{w}}}^2({a_5^0}){^2}}} \ge \frac{{2 k_{\tau}  \tau \! +\! (4 \! +\! 3|{{\bf{A}}^t}| \! +\! 3k_{\tau}   \tau N){L^2}{\underline{\eta _{\boldsymbol{w}}}}^2}}{{{\underline{\eta _{\boldsymbol{w}}}}^2({a_5^t}){^2}}},
\end{equation}
\begin{equation}
\label{eq:A80}
{d_2} = \frac{{2 \! +\! (2 \! +\!{9\tau{ k_1 }N} \! +\! 3M){L^2}\underline{\eta _{\boldsymbol{z}}}^2}}{{\underline{\eta _{\boldsymbol{z}}}^2({a_6^0}){^2}}} \ge \frac{{2 \! +\! (2 \! +\!{9\tau{ k_1 }N} \! +\! 3|{{\bf{A}}^t}|){L^2}({\eta _{\boldsymbol{z}}^t})^2}}{{({\eta _{\boldsymbol{z}}^t})^2({a_6^t}){^2}}},
\end{equation}
\begin{equation}
\label{eq:A81}
{d_3} = \frac{{2 \! +\! ({9\tau{ k_1 }N} \! +\! 3M){L^2}\underline{\eta _h}^2}}{{\underline{\eta _h}^2({a_6^0}){^2}}} \ge \frac{{2 \! +\! ({9\tau{ k_1 }N} +3|{{\bf{A}}^t}|){L^2}({\eta _h^t})^2}}{{({\eta _h^t})^2({a_6^t}){^2}}},
\end{equation}

\vspace{1ex}

\noindent where  $k_{\tau}$, $\underline{\eta _{\boldsymbol{w}}}$, $\underline{\eta _{\boldsymbol{z}}}$ and $\underline{\eta _{h}}$ are positive constants. $\underline{\eta _{\boldsymbol{w}}} = \frac{2}{{L + {\rho _1}M{L^2} + {\rho _2}N{L^2} + 8(\frac{{M\gamma {L^2}}}{{\rho _1}{\underline{c}_1}^2} + \frac{{N\gamma {L^2}}}{{\rho _2}{\underline{c}_2}^2})}} \le {\eta _{\boldsymbol{w}}^t}$, $ \underline{\eta _{\boldsymbol{z}}} = \frac{2}{{L + {\rho _1}M{L^2} + {\rho _2}N{L^2} + 8(\frac{{M\gamma {L^2}}}{{\rho _1}{\underline{c}_1}^2} + \frac{{N\gamma {L^2}}}{{\rho _2}{\underline{c}_2}^2})}} \le \eta _{\boldsymbol{z}}^t $ and  $\underline{\eta _h} = \frac{2}{{L + {\rho _1}M{L^2} + {\rho _2}N{L^2} + 8(\frac{{M\gamma {L^2}}}{{\rho _1}{\underline{c}_1}^2} + \frac{{N\gamma {L^2}}}{{\rho _2}{\underline{c}_2}^2})}} \le \eta _h^t, \forall t$. Thus, combining Eq. (\ref{eq:A78}) with Eq. (\ref{eq:A79}), (\ref{eq:A80}), (\ref{eq:A81}), $\forall t \ge T_1 + \tau$, we can obtain,
{\begin{equation}
\label{eq:A82}
\begin{array}{l}
||\nabla {\widetilde{G}}^t|{|^2} 
\! \le \! \sum\limits_{j = 1}^N {{d_1}({a_5^t}){^2}||{{\boldsymbol{w}}_j^{t+1}} - {{\boldsymbol{w}}_j^t}|{|^2}}  \! +\! {d_2}({a_6^t}){^2}||{\boldsymbol{z}}^{t + 1} - {\boldsymbol{z}^t}|{|^2} \! +\! {d_3}({a_6^t}){^2}||h^{t+1} - h^t|{|^2}\\

\quad \quad \quad \; \; \; \! +\! \sum\limits_{l = 1}^{|{{\bf{A}}^t}|}\! {(\frac{3}{{{\rho_1}^2}} \! +\! 9\tau{ k_1 }N{L^2})||{\lambda _l^{t+1}}\! -\! {\lambda _l^t}|{|^2}}  \! +\! \sum\limits_{l = 1}^{|{{\bf{A}}^t}|} {3(({c_1^{t - 1}}){^2} \!-\! ({c_1^t})^2)||{\lambda _l^t}|{|^2}} \! +\! \sum\limits_{j = 1}^N  \! {\frac{3}{{{\rho_2}^2}}||{{\boldsymbol{\phi}}_j^{\overline{{t}_j}}} - {{\boldsymbol{\phi}}_j^t}|{|^2}}   \\ 
 
\quad \quad \quad \; \; \;  \! +\! \sum\limits_{j = 1}^N {3{(({c_2^{\widetilde{{t}}_j - 1}})^2 - ({c_2^{\overline{{t}_j}-1}})^2)}||{{\boldsymbol{\phi}}_j^t}|{|^2}} \! +\! (\frac{2}{{\underline{\eta _{\boldsymbol{w}}}}^2} \! +\! 3NL^2)\sum\limits_{j = 1}^N {||{{\boldsymbol{w}}_j^{\overline{{t}_j}}} - {{\boldsymbol{w}}_j^t}|{|^2}} \\
 
\quad \quad \quad \; \; \; \! - (\frac{2 k_{\tau} \tau}{{\underline{\eta _{\boldsymbol{w}}}}^2} \! +\! 3 k_{\tau} \tau NL^2)\sum\limits_{j = 1}^N {||{{\boldsymbol{w}}_j^{t+1}} - {{\boldsymbol{w}}_j^t}|{|^2}}.
\end{array}
\end{equation}}

Let $d_4^t$ denote a nonnegative sequence:
{\begin{equation}
\label{eq:A83}
{d_4^t} = \frac{1}{{\max \{ {d_1}{a_5^t},{d_2}{a_6^t},{d_3}{a_6^t},\frac{{\frac{{30}}{{{\rho _1}}} + 90{\rho _1}\tau{ k_1 }N{L^2}}}{{1 - 15{\rho _1}\tau{ k_1 }N{L^2}}},\frac{{30 \tau }}{{{\rho _2}}}\} }}
\end{equation}}

It is seen that $d_4^0 \ge d_4^t, \forall t \ge 0$. And we denote the lower bound of $d_4^t$ as $\underline{d_4}$, it appears that $d_4^t\ge \underline{d_4} \ge0, \forall t \ge 0$. And we set the constant $k_{\tau}$ satisfies $k_{\tau} \ge \frac{{{d_4^0}}(\frac{2}{\underline{\eta _{\boldsymbol{w}}}^2} + 3NL^2) }{{\underline{d_4}}(\frac{2}{\overline{\eta _{\boldsymbol{w}}}^2} + 3  NL^2)}$, where ${\overline{\eta _{\boldsymbol{w}}}}$ is the step-size in terms of $\boldsymbol{w}_j$ in the first iteration (it is seen that ${\overline{\eta _{\boldsymbol{w}}}} \ge {\eta _{\boldsymbol{w}}^t}, \forall t $). Then, $\forall t \ge T_1 + \tau$, we can obtain the following inequality from Eq. (\ref{eq:A82}) and Eq. (\ref{eq:A83}):
{\begin{equation}
\label{eq:A84}
\begin{array}{l}
{d_4^t}||\nabla {\widetilde{G}}^t|{|^2}
 \!\le\! {a_5^t} \! \sum\limits_{j = 1}^N \! {||{{\boldsymbol{w}}_j^{t+1}}\! -\! {{\boldsymbol{w}}_j^t}|{|^2}}  \! +\! {a_6^t}||{\boldsymbol{z}}^{t + 1}\! -\! {\boldsymbol{z}^t}|{|^2} \! +\! {a_6^t}||h^{t+1} \!-\! h^t|{|^2}\\
 
\quad \quad \quad \quad \; \; \;  \! +  {(\frac{1}{{10{\rho _1}}}\! -\! \frac{{3\tau{ k_1 }N{L^2}}}{2})\sum\limits_{l = 1}^{|{{\bf{A}}^t}|}||{\lambda _l^{t+1}} - {\lambda _l^t}|{|^2}}  \! +\! \frac{1}{{10\tau {\rho _2}}}\sum\limits_{j = 1}^N {||{{\boldsymbol{\phi}}_j^{\overline{{t}_j}}} - {{\boldsymbol{\phi}}_j^t}|{|^2}} \\

\quad \quad \quad \quad \; \; \; \! + 3{d_4^t}(({c_1^{t-1}}){^2} - ({c_1^t}){^2})\sum\limits_{l = 1}^{|{{\bf{A}}^t}|} {||{\lambda _l^t}|{|^2}}  \! +\! 3{d_4^t}\sum\limits_{j = 1}^N(({c_2^{\widetilde{{t}}_j - 1}}){^2} - ({c_2^{\overline{{t}_j}-1}}){^2}) {||{{\boldsymbol{\phi}}_j^t}|{|^2}} \\

\quad \quad \quad \quad  \; \; \; \! +{d_4^t} (\frac{2}{{\underline{\eta _{\boldsymbol{w}}}}^2} \! +\! 3NL^2)\sum\limits_{j = 1}^N {||{{\boldsymbol{w}}_j^{\overline{{t}_j}}} - {{\boldsymbol{w}}_j^t}|{|^2}} - {d_4^t}(\frac{2 k_{\tau}  \tau}{{\underline{\eta _{\boldsymbol{w}}}}^2} \! +\! 3 k_{\tau}   \tau NL^2)\sum\limits_{j = 1}^N {||{{\boldsymbol{w}}_j^{t+1}} - {{\boldsymbol{w}}_j^t}|{|^2}}.

\end{array}
\end{equation}}


Combining  Eq. (\ref{eq:A84}) with Eq. (\ref{eq:A71}) and  according to the setting $||{\lambda _l^t}|{|^2} \le {\sigma _1}^2$, $||{{\boldsymbol{\phi}}_j^t}|{|^2} \le {\sigma _2}^2$ (where ${\sigma _1}^2={\alpha_3}^2$, ${\sigma _2}^2=p{\alpha_4}^2$) and ${d_4^0} \ge {d_4^t} \ge \underline{d_4}$, thus, $\forall t \ge T_1 + \tau$, we have,
{\begin{equation}
\label{eq:A85}
\begin{array}{l}
{d_4^t}||\nabla {\widetilde{G}}^t|{|^2} \\
  \le F^{t} - F^{t+1} + \frac{{{c_1^{t-1}} - {c_1^t}}}{2}M{\sigma _1}^2  + \frac{{{c_2^{t-1}} - {c_2^t}}}{2}N{\sigma _2}^2 
 + \frac{4}{{{\rho _1}}}(\frac{{{c_1^{t-2}}}}{{{c_1^{t-1}}}} - \frac{{{c_1^{t-1}}}}{{{c_1^t}}})M{\sigma _1}^2 \vspace{0.5ex}\\
 + \frac{4}{{{\rho _2}}}(\frac{{{c_2^{t-2}}}}{{{c_2^{t-1}}}} - \frac{{{c_2^{t-1}}}}{{{c_2^t}}})N{\sigma _2}^2 
 + 3{d_4^0}(({c_1^{t-1}}){^2} - ({c_1^t}){^2})M{\sigma _1}^2 + 3{d_4^0}\sum\limits_{j = 1}^N(({c_2^{\widetilde{{t}}_j - 1}}){^2} - ({c_2^{\overline{{t}_j}-1}}){^2}){\sigma _2}^2 \vspace{0.5ex}\\
 
  + \frac{1}{{10\tau {\rho _2}}}\sum\limits_{j = 1}^N {||{{\boldsymbol{\phi}}_j^{\overline{{t}_j}}} - {{\boldsymbol{\phi}}_j^t}|{|^2}} - \frac{1}{{10{\rho _2}}}\sum\limits_{j = 1}^N {||{{\boldsymbol{\phi}}_j^{t+1}} - {{\boldsymbol{\phi}}_j^t}|{|^2}}\\
 
  +{d_4^0} (\frac{2}{{\underline{\eta _{\boldsymbol{w}}}}^2} + 3NL^2)\sum\limits_{j = 1}^N {||{{\boldsymbol{w}}_j^{\overline{{t}_j}}} - {{\boldsymbol{w}}_j^t}|{|^2}} - \underline{d_4}(\frac{2 k_{\tau}  \tau}{{\underline{\eta _{\boldsymbol{w}}}}^2} + 3 k_{\tau}   \tau NL^2)\sum\limits_{j = 1}^N {||{{\boldsymbol{w}}_j^{t+1}} - {{\boldsymbol{w}}_j^t}|{|^2}}.

\end{array}
\end{equation}}

Denoting $\widetilde{T}(\varepsilon )$ as $\widetilde{T}(\varepsilon ) = \min \{ t \ | \; ||\nabla \widetilde{G}^{T_1+t}|| \le \frac{\varepsilon}{2}, t\ge \tau\}$. Summing up Eq. (\ref{eq:A85}) from $t=T_1+\tau$ to $t =T_1+{{\widetilde{T}} (\varepsilon )} $, we have,
\begin{equation}
\label{eq:A86}
\begin{array}{l}
\sum\limits_{t =T_1\!+\! \tau}^{T_1\!+\!{\widetilde{T}} (\varepsilon )} {{d_4^t}||\nabla {\widetilde{G}}^t|{|^2}}\\
 
 \le F^{T_1 + \tau} - \mathop L\limits_ - 
 + \frac{4}{{{\rho _1}}}(\frac{{{c_1^{T_1+\tau-2}}}}{{{c_1^{T_1 + \tau -1}}}} + \frac{{{c_1^{T_1 + \tau -1}}}}{{{c_1^{T_1 + \tau}}}})M {{\sigma _1}^2}  + \frac{{{c_1^{T_1+ \tau -1}}}}{2}M {{\sigma _1}^2}  + \frac{7}{{2{\rho _1}}}M {{\sigma _3}^2}  + 3{d_4^0} ({{c_1^0}){^2}M {{\sigma _1}^2} } \\
 
 + \frac{4}{{{\rho _2}}}(\frac{{{c_1^{T_1+ \tau -2}}}}{{{c_1^{T_1 + \tau -1}}}} + \frac{{{c_1^{T_1 + \tau -1}}}}{{{c_1^{T_1 + \tau}}}})N {{\sigma _2}^2}  + \frac{{{c_2^{T_1+ \tau -1}}}}{2}N {{\sigma _2}^2}  + \frac{7}{{2{\rho _2}}}N {{\sigma _4}^2}  + \sum\limits_{j = 1}^N\sum\limits_{t =T_1\!+\! \tau}^{T_1\!+\!{\widetilde{T}} (\varepsilon )}3{d_4^0}(({c_2^{\widetilde{{t}}_j - 1}}){^2} - ({c_2^{\overline{{t}_j}-1}}){^2}){\sigma _2}^2 \\
 
+  \frac{c_1^{T_1+ \tau}}{{2}}M {{\sigma _1}^2} +  \frac{c_2^{T_1+ \tau}}{{2}}N {{\sigma _2}^2} + \frac{1}{{10\tau {\rho _2}}}\sum\limits_{j = 1}^N \sum\limits_{t =T_1\!+\! \tau}^{T_1\!+\!{\widetilde{T}} (\varepsilon )} {||{{\boldsymbol{\phi}}_j^{\overline{{t}_j}}} - {{\boldsymbol{\phi}}_j^t}|{|^2}} - \frac{1}{{10{\rho _2}}}\sum\limits_{j = 1}^N \sum\limits_{t =T_1\!+\! \tau}^{T_1\!+\!{\widetilde{T}} (\varepsilon )} {||{{\boldsymbol{\phi}}_j^{t+1}} - {{\boldsymbol{\phi}}_j^t}|{|^2}}\\
 
  +{d_4^0} (\frac{2}{\underline{\eta _{\boldsymbol{w}}}^2} + 3NL^2)\sum\limits_{j = 1}^N \sum\limits_{t =T_1\!+\! \tau}^{T_1\!+\!{\widetilde{T}} (\varepsilon )} {||{{\boldsymbol{w}}_j^{\overline{{t}_j}}} - {{\boldsymbol{w}}_j^t}|{|^2}} - \underline{d_4}(\frac{2 k_{\tau} \tau}{\overline{\eta _{\boldsymbol{w}}}^2} + 3 k_{\tau} \tau NL^2)\sum\limits_{j = 1}^N \sum\limits_{t =T_1\!+\! \tau}^{T_1\!+\!{\widetilde{T}} (\varepsilon )} {||{{\boldsymbol{w}}_j^{t+1}} - {{\boldsymbol{w}}_j^t}|{|^2}}. 
\end{array}
\end{equation}
where ${\sigma _3} \!=\! \max \{  ||{\lambda _1} - {\lambda _2}|| \, {\rm{ }}|{\lambda _1},{\lambda _2} \! \in \! {\bf{\Lambda}}  \} $, ${\sigma _4} = \max \{ ||{\boldsymbol{\phi}_1} - {\boldsymbol{\phi}_2}|| \, {\rm{ }}|{\boldsymbol{\phi}_1},{\boldsymbol{\phi}_2} \! \in \! {{\boldsymbol{\Phi}}} \} $ and $\mathop L\limits_ -  \! =  \! \mathop {\min }\limits_{{\rm{\{ }}{{\boldsymbol{w}}_j}  \in  {{\boldsymbol{\mathcal{W}}}}{\rm{\} , }}{\boldsymbol{z}} \in  {{\boldsymbol{\mathcal{Z}}}} , h  \in   {{\boldsymbol{\mathcal{H}}}},\{  {\lambda _l} \in {\bf{\Lambda}}   \} ,\{  {{\boldsymbol{\phi}}_j} \in {{\boldsymbol{\Phi}}}  \} }  {L_p}{\rm{(  \{  }}{{\boldsymbol{w}}_j}{\rm{  \},}}{\boldsymbol{z}},h,  \{  {\lambda _l}  \} ,\{  {{\boldsymbol{\phi}}_j}  \}  {\rm{)}}$, which satisfy that, $\forall t \ge T_1+\tau$,
{\begin{equation}
\label{eq:A87}
F^{t+1} \!\ge\! \mathop L\limits_ -   -  \frac{4}{{{\rho _1}}}\frac{{{c_1^{T_1 +\tau - 1}}}}{{{c_1^{T_1+\tau}}}}M {{\sigma _1}^2}  - \frac{4}{{{\rho _2}}}\frac{{{c_2^{T_1+\tau - 1}}}}{{{c_2^{T_1+\tau}}}}N {{\sigma _2}^2}  - \frac{7}{{2{\rho _1}}}M {{\sigma _3}^2}  - \frac{7}{{2{\rho _2}}}N {{\sigma _4}^2} -  \frac{c_1^{T_1+\tau}}{{2}}M {{\sigma _1}^2} -  \frac{c_2^{T_1+\tau}}{{2}}N {{\sigma _2}^2}.
\end{equation}}

\vspace{2ex}



For each worker $j$, the iterations between the last iteration and the next iteration where it is active is no more than $\tau$, \textit{i.e.}, $\overline{{t}_j} -\widetilde{{t}}_j \le \tau $, we have,
{\begin{equation}
\label{eq:A87-0}
\begin{array}{l}
\sum\limits_{t =T_1\!+\! \tau}^{T_1\!+\!{\widetilde{T}} (\varepsilon )}3{d_4^0}(({c_2^{\widetilde{{t}}_j - 1}}){^2} - ({c_2^{\overline{{t}_j}-1}}){^2}){\sigma _2}^2 \vspace{1ex}\\

\le \tau \sum\limits_{\scriptstyle{{\hat v}_j}(i) \in \mathcal{V}_j({\widetilde{T}}(\varepsilon )),\hfill\atop 
\scriptstyle{T_1 + \tau} \le {{\hat v}_j}(i) \le T_1+{\widetilde{T}} (\varepsilon ) \hfill} 3{d_4^0}(({c_2^{{\hat v}_j(i) - 1}}){^2} - ({c_2^{{\hat v}_j(i+1)-1}}){^2}){\sigma _2}^2
\vspace{1ex}\\

\le 3\tau {d_4^0}({c_2^0}){^2}{\sigma _2}^2.
\end{array}
\end{equation}}

Since the idle workers do not update their variables in each iteration, for any $t$ that satisfies ${\hat v_j}(i - 1) \le t < {\hat v_j}(i)$, we have ${\boldsymbol{\phi}_j^t} = {\boldsymbol{\phi}_j^{{\hat v_j}(i) - 1}}$. And for $t \notin \mathcal{V}_j(T)$, we have ${||{{\boldsymbol{\phi}}_j^t} - {{\boldsymbol{\phi}}_j^{t-1}}|{|^2}}=0$. Combing with ${\hat v_j}(i) - {\hat v_j}(i - 1) \le \tau $, we can obtain that, 
{\begin{equation}
\label{eq:A87-1}
\begin{array}{l}
\sum\limits_{j = 1}^N\sum\limits_{t =T_1\!+\! \tau}^{T_1\!+\!{\widetilde{T}} (\varepsilon )}\! { {||{{\boldsymbol{\phi}}_j^{\overline {{t_j}}}} \!-\! {{\boldsymbol{\phi}}_j^t}|{|^2}} } 

\!\le\! \tau \sum\limits_{j = 1}^N \sum\limits_{\scriptstyle{{\hat v}_j}(i) \in \mathcal{V}_j({\widetilde{T}}(\varepsilon )),\hfill\atop
\scriptstyle T_1 + \tau + 1 \le {{\hat v}_j}(i)\hfill} { {||{\boldsymbol{\phi}_j^{{{\hat v}_j}(i)}} - {\boldsymbol{\phi}_j^{{{\hat v}_j}(i) - 1}}|{|^2}} } 
\vspace{0.5ex}\\

\quad \, \quad \quad \quad \quad \quad \quad \quad \quad  = \tau \! \sum\limits_{j = 1}^N \sum\limits_{t =T_1\!+\! \tau}^{T_1\!+\!{\widetilde{T}} (\varepsilon )}\! { {||{{\boldsymbol{\phi}}_j^{t+1}}\! -\! {{\boldsymbol{\phi}}_j^t}|{|^2}} }  + \tau \!\sum\limits_{j = 1}^N \sum\limits_{t =T_1\!+\! {\widetilde{T}}(\varepsilon ) + 1}^{T_1\!+\!{\widetilde{T}}(\varepsilon ) + \tau  - 1} { \!{||{{\boldsymbol{\phi}}_j^{t+1}} \!-\! {{\boldsymbol{\phi}}_j^t}|{|^2}} } \vspace{0.5ex}\\

\quad \, \quad \quad \quad \quad \quad \quad \quad \quad  \le \tau\!\sum\limits_{j = 1}^N \sum\limits_{t =T_1\!+\! \tau}^{T_1\!+\!{\widetilde{T}} (\varepsilon )}\! { {||{{\boldsymbol{\phi}}_j^{t+1}} \!-\! {{\boldsymbol{\phi}}_j^t}|{|^2}} }  + 4\tau (\tau \! -\! 1)N{\sigma _2}^2.
\end{array}
\end{equation}}

Similarly, for any $t$ that satisfies ${\hat v_j}(i - 1) \le t < {\hat v_j}(i)$, we have ${\boldsymbol{w}_j^t} = {\boldsymbol{w}_j^{{\hat v_j}(i) - 1}}$. And for $t \notin \mathcal{V}_j(T)$, we have ${||{{\boldsymbol{w}}_j^t} - {{\boldsymbol{w}}_j^{t-1}}|{|^2}}=0$. Combing with ${\hat v_j}(i) - {\hat v_j}(i - 1) \le \tau $, we can obtain,
{\begin{equation}
\label{eq:A87-2}
\begin{array}{l}
\sum\limits_{j = 1}^N \sum\limits_{t =T_1\!+\! \tau}^{T_1\!+\!{\widetilde{T}} (\varepsilon )}\! { {||{{\boldsymbol{w}}_j^{\overline {{t_j}}}} \!-\! {{\boldsymbol{w}}_j^t}|{|^2}} } 

\!\le\! \tau\! \sum\limits_{j = 1}^N \sum\limits_{\scriptstyle{{\hat v}_j}(i) \in \mathcal{V}_j({\widetilde{T}}(\varepsilon )),\hfill\atop
\scriptstyle T_1+ \tau + 1 \le {{\hat v}_j}(i)\hfill}\! { {||{\boldsymbol{w}_j^{{{\hat v}_j}(i)}} - {\boldsymbol{w}_j^{{{\hat v}_j}(i) - 1}}|{|^2}} } 
\vspace{0.5ex}\\

\quad \,  \; \quad \quad \quad \quad \quad \quad  \quad \quad  = \tau \! \sum\limits_{j = 1}^N \sum\limits_{t =T_1\!+\! \tau}^{T_1\!+\!{\widetilde{T}} (\varepsilon )}\! { {||{{\boldsymbol{w}}_j^{t+1}}\! -\! {{\boldsymbol{w}}_j^t}|{|^2}} } \! +\! \tau\! \sum\limits_{j = 1}^N \sum\limits_{t = {T_1\!+\!\widetilde{T}}(\varepsilon ) + 1}^{T_1\!+\!{\widetilde{T}}(\varepsilon ) + \tau  - 1} { {||{{\boldsymbol{w}}_j^{t+1}} \!-\! {{\boldsymbol{w}}_j^t}|{|^2}} } \vspace{0.5ex}\\

\quad \,  \; \quad \quad \quad \quad \quad \quad  \quad \quad  \le \tau \! \sum\limits_{j = 1}^N \sum\limits_{t =T_1\!+\! \tau}^{T_1\!+\!{\widetilde{T}} (\varepsilon )}\! { {||{{\boldsymbol{w}}_j^{t+1}} \!-\! {{\boldsymbol{w}}_j^t}|{|^2}} }  + 4\tau (\tau  \!- \!1)pN{\alpha _1}^2.
\end{array}
\end{equation}}

It follows from Eq. (\ref{eq:A86}), (\ref{eq:A87-0}), (\ref{eq:A87-1}), (\ref{eq:A87-2}) that,
{\begin{equation}
\label{eq:A87-3}
\begin{array}{l}
\sum\limits_{t =T_1\!+\! \tau}^{T_1\!+\!{\widetilde{T}} (\varepsilon )} {{d_4^t}||\nabla {\widetilde{G}}^t|{|^2}} \vspace{1ex}\\
 
 \le F^{T_1 + \tau} - \mathop L\limits_ -  
 + \frac{4}{{{\rho _1}}}(\frac{{{c_1^{T_1 + \tau - 2}}}}{{{c_1^{T_1 + \tau - 1}}}} + \frac{{{c_1^{T_1  + \tau - 1}}}}{{{c_1^{T_1  + \tau}}}})M {{\sigma _1}^2}  + \frac{{{c_1^{T_1+ \tau - 1}}}}{2}M {{\sigma _1}^2}  + \frac{7}{{2{\rho _1}}}M {{\sigma _3}^2}  + 3{d_4^0} {({c_1^0}){^2}M {{\sigma _1}^2} } \vspace{1.5ex}\\
 
 + \frac{4}{{{\rho _2}}}(\frac{{{c_1^{T_1+ \tau - 2}}}}{{{c_1^{T_1 + \tau - 1}}}} + \frac{{{c_1^{T_1 + \tau - 1}}}}{{{c_1^{T_1 + \tau}}}})N {{\sigma _2}^2}  + \frac{{{c_2^{T_1+ \tau - 1}}}}{2}N {{\sigma _2}^2}  + \frac{7}{{2{\rho _2}}}N {{\sigma _4}^2}  + 3\tau{d_4^0} {({c_2^{0}}){^2}N {{\sigma _2}^2} } \vspace{1.5ex}\\
 
+  \frac{c_1^{T_1+ \tau}}{{2}}M {{\sigma _1}^2} +  \frac{c_2^{T_1+ \tau}}{{2}}N {{\sigma _2}^2} + (\frac{{2N{\sigma _2}^2}}{{5{\rho _2}}} + 4{d_4^0}(\frac{2}{\underline{\eta _{\boldsymbol{w}}}^2} + 3N{L^2})pN{\alpha _1}^2\tau) (\tau  - 1) \\
 = \mathop d\limits^ -  + k_d(\tau -1),
\end{array}
\end{equation}}

\noindent where $\mathop d\limits^ -$ and $k_d$ are constants. $d_5$ is given by, 
{\begin{equation}
\label{eq:A88}
\begin{array}{l}
{d_5} = \max \{ \frac{{{d_1}}}{{{a_6^0}}},\frac{{{d_2}}}{{{a_5^0}}},\frac{{{d_3}}}{{{a_5^0}}},\frac{{\frac{{30}}{{{\rho _1}}} + 90{\rho _1}\tau{ k_1 }N{L^2}}}{{(1 - 15{\rho _1}\tau{ k_1 }N{L^2}){a_5^0}{a_6^0}}},\frac{{30\tau}}{{{\rho _2}{a_5^0}{a_6^0}}}\} \vspace{1ex}\\
 \ge \max \{ \frac{{{d_1}}}{{{a_6^t}}},\frac{{{d_2}}}{{{a_5^t}}},\frac{{{d_3}}}{{{a_5^t}}},\frac{{\frac{{30}}{{{\rho _1}}} + 90{\rho _1}\tau{ k_1 }N{L^2}}}{{(1 - 15{\rho _1}\tau{ k_1 }N{L^2}){a_5^t}{a_6^t}}},\frac{{30\tau}}{{{\rho _2}{a_5^t}{a_6^t}}}\} \vspace{1ex}\\
 = \frac{1}{{{d_4^t}{a_5^t}{a_6^t}}}
\end{array}
\end{equation}}

\vspace{2ex}

Thus, we can obtain that,
{\begin{equation}
\label{eq:A89}
\sum\limits_{t =T_1\!+\! \tau}^{T_1\!+\!{\widetilde{T}} (\varepsilon )} {\frac{1}{{{d_5}{a_5^t}{a_6^t}}}||\nabla \widetilde{G}^{T_1+\widetilde{T}(\varepsilon )}|{|^2}}  \le \sum\limits_{t =T_1\!+\! \tau}^{T_1\!+\!{\widetilde{T}} (\varepsilon )} {\frac{1}{{{d_5}{a_5^t}{a_6^t}}}||\nabla \widetilde{G}^t|{|^2}}  \le \sum\limits_{t =T_1\!+\! \tau}^{T_1\!+\!{\widetilde{T}} (\varepsilon )} {{d_4^t}||\nabla \widetilde{G}^t|{|^2}}  \le \mathop d\limits^ -  + k_d(\tau -1) .
\end{equation}}

And it follows from Eq. (\ref{eq:A89}) that,
{\begin{equation}
\label{eq:A90}
||\nabla \widetilde{G}^{T_1+\widetilde{T}(\varepsilon )}|{|^2} \le \frac{{(\mathop d\limits^ -  + k_d(\tau -1) )  {d_5}}}{{\sum\limits_{t =T_1\!+\! \tau}^{T_1\!+\!{\widetilde{T}} (\varepsilon )} {\frac{1}{{{a_5^t}{a_6^t}}}}}}.  
\end{equation}}


According to the setting of ${c_1^t}$, ${c_2^t}$ and Eq. (\ref{eq:A67}), (\ref{eq:A67-1}), we have,


{\begin{equation}
\label{eq:A91}
\frac{1}{{{a_5^t}{a_6^t}}} \ge \frac{1}{{{{(4(\gamma  - 2){L^2}(M{\rho _1} + N{\rho _2}){(t+1)^{\frac{1}{3}}} + \frac{{{\rho _2}(N - S){L^2}}}{2})}^2}}}.
\end{equation}}

Summing up $\frac{1}{{{a_5^t}{a_6^t}}}$ from ${t =T_1 \!+\! \tau}$ to ${t =T_1 \!+\! {{\widetilde{T}} (\varepsilon) }}$, it follows that,
{\begin{equation}
\label{eq:A92}
\begin{array}{l}
\sum\limits_{t =T_1\!+\! \tau}^{T_1\!+\!{\widetilde{T}} (\varepsilon )} {\frac{1}{{{a_5^t}{a_6^t}}}} 
 \ge \sum\limits_{t =T_1\!+\! \tau}^{T_1\!+\!{\widetilde{T}} (\varepsilon )} {\frac{1}{{{{(4(\gamma  - 2){L^2}(M{\rho _1} + N{\rho _2}){(t+1)^{\frac{1}{3}}} + \frac{{{\rho _2}(N - S){L^2}}}{2})}^2}}}} \vspace{1ex}\\
\quad \,  \quad \quad \quad \quad   \ge \sum\limits_{t =T_1\!+\! \tau}^{T_1\!+\!{\widetilde{T}} (\varepsilon )} {\frac{1}{{{{(4(\gamma  - 2){L^2}(M{\rho _1} + N{\rho _2}){(t+1)^{\frac{1}{3}}} + \frac{{{\rho _2}(N - S){L^2}}}{2}{(t+1)^{\frac{1}{3}}})}^2}}}} \vspace{1ex}\\
 
\quad \,  \quad \quad \quad \quad  \ge \frac{{(T_1+{\widetilde{T}} {{(\varepsilon ))}^{\frac{1}{3}}} - (T_1+\tau)^{\frac{1}{3}}}}{{{{(4(\gamma  - 2){L^2}(M{\rho _1} + N{\rho _2}) + \frac{{{\rho _2}(N - S){L^2}}}{2})}^2}}}.
\end{array}
\end{equation}}

The second inequality in Eq. (\ref{eq:A92}) is due to that $\forall t \ge T_1 + \tau$, we have,
{\begin{equation}
\label{eq:A93}
4(\gamma  - 2){L^2}(M{\rho _1} + N{\rho _2}){(t\!+\!1)^{\frac{1}{3}}} + \frac{{{\rho _2}(N \!-\! S){L^2}}}{2} \!\le\! (4(\gamma  - 2){L^2}(M{\rho _1} + N{\rho _2}) + \frac{{{\rho _2}(N \!-\! S){L^2}}}{2}){(t\!+\!1)^{\frac{1}{3}}}.
\end{equation}}

The last inequality in Eq. (\ref{eq:A92}) follows from the fact that  $\sum\limits_{t =T_1\!+\! \tau}^{T_1\!+\!{\widetilde{T}} (\varepsilon )} {\frac{1}{{{(t+1)^{\frac{2}{3}}}}}}  \ge (T_1+{\widetilde{T}} {(\varepsilon )})^{\frac{1}{3}} - (T_1+\tau)^{\frac{1}{3}}$.

\vspace{1ex}

Thus, plugging Eq. (\ref{eq:A92}) into Eq. (\ref{eq:A90}), we can obtain:
{\begin{equation}
\label{eq:A94}
||\nabla {\widetilde{G}}^{T_1 + {\widetilde{T}} (\varepsilon )} |{|^2} \! \le\! \frac{{\mathop (\mathop d\limits^ -  + k_d(\tau\! -\!1))   {d_5}}}{{\sum\limits_{t =T_1\!+\! \tau}^{T_1\!+\!{\widetilde{T}} (\varepsilon )} {\frac{1}{{{a_5^t}{a_6^t}}}} }} \! \le \! \frac{{{{(4(\gamma  \!-\! 2){L^2}(M{\rho _1} \!+\! N{\rho _2}) + \frac{{{\rho _2}(N - S){L^2}}}{2})}^2}(\mathop d\limits^ -  + k_d(\tau\! -\! 1))   {d_5}}}{{(T_1+{\widetilde{T}} {{(\varepsilon )}){^{\frac{1}{3}}}} - (T_1+\tau)^{\frac{1}{3}}}}.
\end{equation}}

According to the definition of ${\widetilde{T}(\varepsilon )}$, we have:
{\begin{equation}
\label{eq:A95}
T_1 + {\widetilde{T}} (\varepsilon ) \ge {(\frac{{4{{(4(\gamma  - 2){L^2}(M{\rho _1} + N{\rho _2}) + \frac{{{\rho _2}(N - S){L^2}}}{2})}^2}(\mathop d\limits^ -  + k_d(\tau -1))   {d_5}}}{{{\varepsilon ^2}}} + (T_1+\tau)^{\frac{1}{3}})^3}.
\end{equation}}

Combining the definition of $\nabla G^t$ and $\nabla {\widetilde{G}}^t$ with trigonometric inequality, we then get:
{\begin{equation}
\label{eq:A96}
||\nabla G^t|| - ||\nabla {\widetilde{G}}^t|| \le ||\nabla G^t - \nabla {\widetilde{G}}^t|| \le \sqrt {\sum\limits_{l = 1}^{|{{\bf{A}}^t}|} {||{c_1^{t-1}}{\lambda _l^t}|{|^2}}  + \sum\limits_{j = 1}^N {||{c_2^{t-1}}{{\boldsymbol{\phi}}_j^t}|{|^2}} }.
\end{equation}}

Denoting constant $d_6$ as $d_6 = 4(\gamma  -  2){{L}^2}( M{\rho _1}  +  N{\rho _2})$. If $t > {(\frac{{4M{\sigma _1}^2}}{{{\rho _1}^2}} + \frac{{4N{\sigma _2}^2}}{{{\rho _2}^2}})^3}\frac{1}{{{\varepsilon ^6}}}$, then we have $\sqrt {\sum\limits_{l = 1}^{|{{\bf{A}}^t}|} {||{c_1^{t-1}}{\lambda _l^t}|{|^2}}  + \sum\limits_{j = 1}^N {||{c_2^{t-1}}{{\boldsymbol{\phi}}_j^t}|{|^2}} }  \le \frac{\varepsilon }{2}$. Combining it with Eq. (\ref{eq:A95}), we can conclude that there exists a
\begin{equation}
\label{eq:A97}
T( \varepsilon ) \! \sim  \! \mathcal{O}(\max  \{ {(\frac{{4M\!{\sigma _1}^2}}{{{\rho _1}^2}}\! +\! \frac{{4N\!{\sigma _2}^2}}{{{\rho _2}^2}}\!)^3}\!\frac{1}{{{\varepsilon ^6}}}, 
{(\!\frac{{4{{{(d_6 \!+\! \frac{{{\rho _2}(N \! - \! S){{L}^2}}}{2}\!)}}^2}\! (\mathop d\limits^ -  +  k_d(\tau \! - \! 1))  {d_5}}}{{{\varepsilon ^2}}}\! +\! (T_1\!+\!\tau)^{\frac{1}{3}})^3}\}),
\end{equation}
such that $||\nabla G^t|| \le ||\nabla {\widetilde{G}}^t|| + \sqrt {\sum\limits_{l = 1}^{|{{\bf{A}}^t}|} {||{c_1^{t-1}}{\lambda _l^t}|{|^2}}  + \sum\limits_{j = 1}^N {||{c_2^{t-1}}{{\boldsymbol{\phi}}_j^t}|{|^2}} }  \le \varepsilon $, which concludes our proof.

{\color{black}
\section{Proof of Theorem 2}
\label{appendix:Theorem2}
In this section, we provide the proof about the iteration complexity of the proposed method when $S$ is adaptive.

This proof is also based on the Lemma \ref{lemma 1}, \ref{lemma 2}, and \ref{lemma3}. And we set that:
\begin{equation}
\label{eq:137}
\begin{array}{l}
{a_5^t} = \frac{{4|{{\bf{A}}^t}|(\gamma  - 2){L^2}}}{{{\rho _1}({c_1^t})^2}} + \frac{{4N(\gamma  - 2){L^2}}}{{{\rho _2}({c_2^t})^2}} + \frac{{{\rho _2}(N - |{{\bf{Q}}^{t + 1}}|){L^2}}}{2} - \frac{1}{2},
\end{array}
\end{equation}
\begin{equation}
\label{eq:138}
\begin{array}{l}
{a_6^t} = \frac{{4|{{\bf{A}}^t}|(\gamma  - 2){L^2}}}{{{\rho _1}({c_1^t})^2}} + \frac{{4N(\gamma  - 2){L^2}}}{{{\rho _2}({c_2^t})^2}} + \frac{{{\rho _2}(N - |{{\bf{Q}}^{t + 1}}|){L^2}}}{2} - \frac{{3\tau{ k_1 }N{L^2}}}{2},
\end{array}
\end{equation}
where $\gamma $ is a constant which satisfies $\gamma \ge 2$ and $\frac{{4(\gamma  - 2){L^2}}}{{{\rho _1}({c_1^0}){^2}}} + \frac{{4N(\gamma  - 2){L^2}}}{{{\rho _2}({c_2^0}){^2}}}  > \max \{ \frac{1}{2},\frac{{3\tau{ k_1 }N{L^2}}}{2}\}$. It is seen that the ${a_5^t},{a_6^t}$ are nonnegative sequences. And we set constants $\underline{a_5}$ and $\underline{a_6}$ as,
\begin{equation}
\label{eq:139}
\begin{array}{l}
{\underline{a_5}} = \frac{{4(\gamma  - 2){L^2}}}{{{\rho _1}({c_1^0}){^2}}} + \frac{{4N(\gamma  - 2){L^2}}}{{{\rho _2}({c_2^0}){^2}}} - \frac{1}{2},
\end{array}
\end{equation}
\begin{equation}
\label{eq:140}
\begin{array}{l}
{\underline{a_6}} = \frac{{4(\gamma  - 2){L^2}}}{{{\rho _1}({c_1^0}){^2}}} + \frac{{4N(\gamma  - 2){L^2}}}{{{\rho _2}({c_2^0}){^2}}} - \frac{{3\tau{ k_1 }N{L^2}}}{2}.
\end{array}
\end{equation}

It is seen that  $0<\underline{a_5} \le {a_5^t}, 0<\underline{a_6} \le {a_6^t},  \forall t \ge 0$. And we set constant $d_1$, $d_2$, $d_3$ as,
\begin{equation}
\label{eq:141}{d_1} = \frac{{2 k_{\tau} \tau \! +\! (4 \! +\! 3M \! +\! 3 k_{\tau}  \tau N){L^2}\underline{\eta _{\boldsymbol{w}}}^2}}{{\underline{\eta _{\boldsymbol{w}}}^2({\underline{a_5}}){^2}}} \ge \frac{{2 k_{\tau}  \tau \! +\! (4 \! +\! 3|{{\bf{A}}^t}| \! +\! 3k_{\tau}   \tau N){L^2}{\underline{\eta _{\boldsymbol{w}}}}^2}}{{{\underline{\eta _{\boldsymbol{w}}}}^2({a_5^t}){^2}}},
\end{equation}
\begin{equation}
\label{eq:142}
{d_2} = \frac{{2 \! +\! (2 \! +\!{9\tau{ k_1 }N} \! +\! 3M){L^2}\underline{\eta _{\boldsymbol{z}}}^2}}{{\underline{\eta _{\boldsymbol{z}}}^2({\underline{a_6}}){^2}}} \ge \frac{{2 \! +\! (2 \! +\!{9\tau{ k_1 }N} \! +\! 3|{{\bf{A}}^t}|){L^2}({\eta _{\boldsymbol{z}}^t})^2}}{{({\eta _{\boldsymbol{z}}^t})^2({a_6^t}){^2}}},
\end{equation}
\begin{equation}
\label{eq:143}
{d_3} = \frac{{2 \! +\! ({9\tau{ k_1 }N} \! +\! 3M){L^2}\underline{\eta _h}^2}}{{\underline{\eta _h}^2({\underline{a_6}}){^2}}} \ge \frac{{2 \! +\! ({9\tau{ k_1 }N} +3|{{\bf{A}}^t}|){L^2}({\eta _h^t})^2}}{{({\eta _h^t})^2({a_6^t}){^2}}}.
\end{equation}

Combing with the definition of $||\nabla {\widetilde{G}}^t|{|^2} $, we can obtain that,
{\begin{equation}
\label{eq:144}
\begin{array}{l}
||\nabla {\widetilde{G}}^t|{|^2} 
\! \le \! \sum\limits_{j = 1}^N {{d_1}({a_5^t}){^2}||{{\boldsymbol{w}}_j^{t+1}} - {{\boldsymbol{w}}_j^t}|{|^2}}  \! +\! {d_2}({a_6^t}){^2}||{\boldsymbol{z}}^{t + 1} - {\boldsymbol{z}^t}|{|^2} \! +\! {d_3}({a_6^t}){^2}||h^{t+1} - h^t|{|^2}\\

\quad \quad \quad \; \; \; \! +\! \sum\limits_{l = 1}^{|{{\bf{A}}^t}|}\! {(\frac{3}{{{\rho_1}^2}} \! +\! 9\tau{ k_1 }N{L^2})||{\lambda _l^{t+1}}\! -\! {\lambda _l^t}|{|^2}}  \! +\! \sum\limits_{l = 1}^{|{{\bf{A}}^t}|} {3(({c_1^{t - 1}}){^2} \!-\! ({c_1^t})^2)||{\lambda _l^t}|{|^2}} \! +\! \sum\limits_{j = 1}^N  \! {\frac{3}{{{\rho_2}^2}}||{{\boldsymbol{\phi}}_j^{\overline{{t}_j}}} - {{\boldsymbol{\phi}}_j^t}|{|^2}}   \\ 
 
\quad \quad \quad \; \; \;  \! +\! \sum\limits_{j = 1}^N {3{(({c_2^{\widetilde{{t}}_j - 1}})^2 - ({c_2^{\overline{{t}_j}-1}})^2)}||{{\boldsymbol{\phi}}_j^t}|{|^2}} \! +\! (\frac{2}{{\underline{\eta _{\boldsymbol{w}}}}^2} \! +\! 3NL^2)\sum\limits_{j = 1}^N {||{{\boldsymbol{w}}_j^{\overline{{t}_j}}} - {{\boldsymbol{w}}_j^t}|{|^2}} \\
 
\quad \quad \quad \; \; \; \! - (\frac{2 k_{\tau} \tau}{{\underline{\eta _{\boldsymbol{w}}}}^2} \! +\! 3 k_{\tau} \tau NL^2)\sum\limits_{j = 1}^N {||{{\boldsymbol{w}}_j^{t+1}} - {{\boldsymbol{w}}_j^t}|{|^2}}.
\end{array}
\end{equation}}

Let $d_4^t$ denote a nonnegative sequence:
{\begin{equation}
\label{eq:145-new}
{d_4^t} = \frac{1}{{\max \{ {d_1}{a_5^t},{d_2}{a_6^t},{d_3}{a_6^t},\frac{{\frac{{30}}{{{\rho _1}}} + 90{\rho _1}\tau{ k_1 }N{L^2}}}{{1 - 15{\rho _1}\tau{ k_1 }N{L^2}}},\frac{{30 \tau }}{{{\rho _2}}}\} }}
\end{equation}}

We denote the lower and upper bound of $d_4^t$ as $\underline{d_4}$ and $\overline{d_4}$, respectively. It appears that $\overline{d_4} \ge d_4^t\ge \underline{d_4} \ge0, \forall t$. And we set the constant $k_{\tau}$ satisfies $k_{\tau} \ge \frac{{\overline{d_4}}(\frac{2}{\underline{\eta _{\boldsymbol{w}}}^2} + 3NL^2) }{{\underline{d_4}}(\frac{2}{\overline{\eta _{\boldsymbol{w}}}^2} + 3  NL^2)}$, where ${\overline{\eta _{\boldsymbol{w}}}}$ is the step-size in terms of $\boldsymbol{w}_j$ in the first iteration (it is seen that ${\overline{\eta _{\boldsymbol{w}}}} \ge {\eta _{\boldsymbol{w}}^t}, \forall t $).  Set the constant $d_5$ as,
\begin{equation}
\label{eq:145}
\begin{array}{l}
{d_5} = \max \{ \frac{{{d_1}}}{{{\underline{a_6}}}},\frac{{{d_2}}}{{{\underline{a_5}}}},\frac{{{d_3}}}{{{\underline{a_5}}}},\frac{{\frac{{30}}{{{\rho _1}}} + 90{\rho _1}\tau{ k_1 }N{L^2}}}{{(1 - 15{\rho _1}\tau{ k_1 }N{L^2}){\underline{a_5}}\,{\underline{a_6}}}},\frac{{30\tau}}{{{\rho _2}{\underline{a_5}}\,{\underline{a_6}}}}\} \vspace{1ex}\\
 \ge \max \{ \frac{{{d_1}}}{{{a_6^t}}},\frac{{{d_2}}}{{{a_5^t}}},\frac{{{d_3}}}{{{a_5^t}}},\frac{{\frac{{30}}{{{\rho _1}}} + 90{\rho _1}\tau{ k_1 }N{L^2}}}{{(1 - 15{\rho _1}\tau{ k_1 }N{L^2}){a_5^t}{a_6^t}}},\frac{{30\tau}}{{{\rho _2}{a_5^t}{a_6^t}}}\} \vspace{1ex}\\
 = \frac{1}{{{d_4^t}{a_5^t}{a_6^t}}}.
\end{array}
\end{equation}

According to (\ref{eq:A84}), (\ref{eq:A85}), (\ref{eq:A86}), (\ref{eq:A87}), (\ref{eq:A87-0}), (\ref{eq:A87-1}), (\ref{eq:A87-2}), (\ref{eq:A87-3}) and replace $d_4^0$ with $\overline{d_4}$, we can obtain that,
{\begin{equation}
\label{eq:146}
\sum\limits_{t =T_1\!+\! \tau}^{T_1\!+\!{\widetilde{T}} (\varepsilon )} {\frac{1}{{{d_5}{a_5^t}{a_6^t}}}||\nabla \widetilde{G}^{T_1+\widetilde{T}(\varepsilon )}|{|^2}}  \le \sum\limits_{t =T_1\!+\! \tau}^{T_1\!+\!{\widetilde{T}} (\varepsilon )} {\frac{1}{{{d_5}{a_5^t}{a_6^t}}}||\nabla \widetilde{G}^t|{|^2}}  \le \sum\limits_{t =T_1\!+\! \tau}^{T_1\!+\!{\widetilde{T}} (\varepsilon )} {{d_4^t}||\nabla \widetilde{G}^t|{|^2}}  \le \mathop d\limits^ -  + k_d(\tau -1) .
\end{equation}}

And it follows from Eq. (\ref{eq:146}) that,
{\begin{equation}
\label{eq:147}
||\nabla \widetilde{G}^{T_1+\widetilde{T}(\varepsilon )}|{|^2} \le \frac{{(\mathop d\limits^ -  + k_d(\tau -1) )  {d_5}}}{{\sum\limits_{t =T_1\!+\! \tau}^{T_1\!+\!{\widetilde{T}} (\varepsilon )} {\frac{1}{{{a_5^t}{a_6^t}}}}}}.  
\end{equation}}


According to the setting of ${c_1^t}$, ${c_2^t}$ and Eq. (\ref{eq:137}), (\ref{eq:138}), we have,


{\begin{equation}
\label{eq:148}
\frac{1}{{{a_5^t}{a_6^t}}} \ge \frac{1}{{{{(4(\gamma  - 2){L^2}(M{\rho _1} + N{\rho _2}){(t+1)^{\frac{1}{3}}} + \frac{{{\rho _2}(N - |{{\bf{Q}}^{t + 1}}|){L^2}}}{2})}^2}}}.
\end{equation}}

Summing up $\frac{1}{{{a_5^t}{a_6^t}}}$ from ${t =T_1 \!+\! \tau}$ to ${t =T_1 \!+\! {{\widetilde{T}} (\varepsilon) }}$, and let constant $d_6 = 4(\gamma  - 2){L^2}(M{\rho _1} + N{\rho _2}$, we have that,
{\begin{equation}
\label{eq:149}
\begin{array}{l}
\sum\limits_{t =T_1\!+\! \tau}^{T_1\!+\!{\widetilde{T}} (\varepsilon )} {\frac{1}{{{a_5^t}{a_6^t}}}} 
 \ge \sum\limits_{t =T_1\!+\! \tau}^{T_1\!+\!{\widetilde{T}} (\varepsilon )} {\frac{1}{{{{(d_6 {(t+1)^{\frac{1}{3}}} + \frac{{{\rho _2}(N - |{{\bf{Q}}^{t + 1}}|){L^2}}}{2})}^2}}}} \vspace{1ex}\\
\quad \,  \quad \quad \quad \quad   \ge \sum\limits_{t =T_1\!+\! \tau}^{T_1\!+\!{\widetilde{T}} (\varepsilon )} {\frac{1}{{{{(d_6{(t+1)^{\frac{1}{3}}} + \frac{{{\rho _2}(N - |{{\bf{Q}}^{t + 1}}|){L^2}}}{2}{(t+1)^{\frac{1}{3}}})}^2}}}} \vspace{1ex}\\
 
\quad \,  \quad \quad \quad \quad  \ge ({\frac{\beta_2}{{{{(d_6 + \frac{{{\rho _2}(N - s){L^2}}}{2})}}}}} + {\frac{(1-\beta_2)}{{d_6}}}  ) ({(T_1+{\widetilde{T}} {{(\varepsilon ))}^{\frac{1}{3}}} - (T_1+\tau)^{\frac{1}{3}}}).
\end{array}
\end{equation}}


The last inequality in Eq. (\ref{eq:149}) follows from that,
{\begin{equation}
\label{eq:150}
\begin{array}{l}
 \sum\limits_{t =T_1\!+\! \tau}^{T_1\!+\!{\widetilde{T}} (\varepsilon )} {\frac{1}{{{{(d_6{(t+1)^{\frac{1}{3}}} + \frac{{{\rho _2}(N - |{{\bf{Q}}^{t + 1}}|){L^2}}}{2}{(t+1)^{\frac{1}{3}}})}^2}}}} \\
 
  \ge \sum\limits_{t =T_1\!+\! \tau}^{T_1\!+\!{\widetilde{T}} (\varepsilon )} {\frac{1}{{{{(d_6 + \frac{{{\rho _2}(N - |{{\bf{Q}}^{t + 1}}|){L^2}}}{2})}{(T_1+{\widetilde{T}} (\varepsilon )+1)^{\frac{2}{3}}}}}}} \vspace{2mm} \\

  ={\frac{\beta_2(T_1+{\widetilde{T}} (\varepsilon )-(T_1+\tau)+1)}{{{{(d_6 + \frac{{{\rho _2}(N - s){L^2}}}{2})}{(T_1\!+\!{\widetilde{T}} (\varepsilon )+1)^{\frac{2}{3}}}}}}} + {\frac{(1-\beta_2)(T_1+{\widetilde{T}} (\varepsilon )-(T_1+\tau)+1)}{{{{(d_6}{(T_1+{\widetilde{T}} (\varepsilon )+1)^{\frac{2}{3}}}}}}}\vspace{2mm} \\

  \ge ({\frac{\beta_2}{{{{(d_6 + \frac{{{\rho _2}(N - s){L^2}}}{2})}}}}} + {\frac{(1-\beta_2)}{{d_6}}}  ) ({(T_1+{\widetilde{T}} {{(\varepsilon ))}^{\frac{1}{3}}} - (T_1+\tau)^{\frac{1}{3}}})
  
  .
\end{array}
\end{equation}}

According to the definition of ${\widetilde{T}(\varepsilon )}$, we have:
{\begin{equation}
\label{eq:151}
T_1 + {\widetilde{T}} (\varepsilon ) \ge {(\frac{{(\mathop d\limits^ -  + k_d(\tau -1))   {d_5}}}{{({\frac{\beta_2}{{{{d_6 + \frac{{{\rho _2}(N - s){L^2}}}{2}}}}}} + {\frac{1-\beta_2}{{d_6}}}  ){\varepsilon ^2}}} + (T_1+\tau)^{\frac{1}{3}})^3}.
\end{equation}}

Combing Eq. (\ref{eq:151}) with Eq. (\ref{eq:A96}), we can conlude that there exists a
\begin{equation}
\label{eq:152}
T( \varepsilon ) \! \sim  \! \mathcal{O}(\max  \{ {(\frac{{4M\!{\sigma _1}^2}}{{{\rho _1}^2}}\! +\! \frac{{4N\!{\sigma _2}^2}}{{{\rho _2}^2}}\!)^3}\!\frac{1}{{{\varepsilon ^6}}}, 
{(\frac{{(\mathop d\limits^ -  + k_d(\tau -1))   {d_5}}}{{({\frac{\beta_2}{{{{d_6 + \frac{{{\rho _2}(N - s){L^2}}}{2}}}}}} + {\frac{1-\beta_2}{{d_6}}}  ){\varepsilon ^2}}} + (T_1+\tau)^{\frac{1}{3}})^3}\}),
\end{equation}
such that $||\nabla G^t|| \le ||\nabla {\widetilde{G}}^t|| + \sqrt {\sum\limits_{l = 1}^{|{{\bf{A}}^t}|} {||{c_1^{t-1}}{\lambda _l^t}|{|^2}}  + \sum\limits_{j = 1}^N {||{c_2^{t-1}}{{\boldsymbol{\phi}}_j^t}|{|^2}} }  \le \varepsilon $, which concludes our proof.

}


\section{Experimental Settings}
\label{appendix:experiment}
\setcounter{figure}{0}
% Figure environment removed

In this section, we present the experimental settings in the experiments. We first give a detailed description of the datasets and baseline methods used in our experiments. 

\subsection{Datasets and Baseline Methods}


\renewcommand\arraystretch{1.5}
\renewcommand\tabcolsep{15pt}
\begin{table}[t]
\caption{The number of workers and categories of datasets}
\centering
\scalebox{0.85}{
\begin{tabular}{lcccc}
\toprule
    & SHL     & Person Activity         & SC-MA  &   Fashion MNIST      \\ \hline
Number of workers & 6 & 5 & 15   &  3 \\ 
Number of categories & 8 & 11 & 7         &  3 \\ 
\bottomrule  
\end{tabular}}
\label{tab:dataset}
\end{table}


In this section, we provide a detailed introduction to datasets and baseline methods. The number of workers and categories of every dataset are summarized in Table \ref{tab:dataset}.

\noindent \textbf{Datasets}:

% \begin{enumerate}
%     \item \textbf{SHL dataset}: The SHL dataset was collected using four cellphones on four body locations where people usually carry cellphones. The SHL dataset provides multimodal locomotion and transportation data collected in real-world settings using eight various modes of transportation. We separated the data into six workers with varied proportions based on the four body locations of smartphones to imitate the different tendencies of workers (users) in positioning cellphones.
    
%     \item \textbf{Person Activity dataset}: Data contains recordings of five participants performing eleven different activities. Each participant wears four sensors in four different body locations (ankle left, ankle right, belt, and chest) while performing the activities. Each participant corresponds to one worker in our experiment.
    
%     \item \textbf{Single Chest-Mounted Accelerometer dataset}: Data was collected from fifteen participants engaged in seven distinct activities. Each participant (worker) wears an accelerometer mounted on the chest.
    
%     \item \textbf{Fashion MNIST}: Similar to MNIST, Fashion MNIST is a dataset where images are grouped into ten categories of clothing. The subset of the data labeled with Pullover, Shirt, and T-shirt are extracted as three workers and each worker consists of one class of clothing.
    
% \end{enumerate}

\noindent \textbf{1. SHL dataset}: The SHL dataset was collected using four cellphones on four body locations where people usually carry cellphones. The SHL dataset provides multimodal locomotion and transportation data collected in real-world settings using eight various modes of transportation. We separated the data into six workers with varied proportions based on the four body locations of smartphones to imitate the different tendencies of workers (users) in positioning cellphones.
    
\noindent \textbf{2. Person Activity dataset}: Data contains recordings of five participants performing eleven different activities. Each participant wears four sensors in four different body locations (ankle left, ankle right, belt, and chest) while performing the activities. Each participant corresponds to one worker in our experiment.
    
\noindent \textbf{3. Single Chest-Mounted Accelerometer dataset}: Data was collected from fifteen participants engaged in seven distinct activities. Each participant (worker) wears an accelerometer mounted on the chest.
    
\noindent \textbf{4. Fashion MNIST}: Fashion MNIST is a dataset where images are grouped into ten categories of clothing. The subset of the data labeled with Pullover, Shirt, and T-shirt are extracted as three workers and each worker consists of one class of clothing.


\vspace{2mm}

\noindent \textbf{Baseline Methods}:

% \begin{enumerate}

% \item \textbf{Ind}${}_j$: It learns the model from an individual worker $j$.

% \item \textbf{Mix}{$\rm{_{Even}}$}: It learns the model from all workers with even weights using the proposed distributed algorithm. 

% \item \textbf{FedAvg}: It learns the model from all workers with even weights. It aggregates the local model parameters from workers through using model averaging.

% \item \textbf{AFL}: It aims to address the fairness issues in federated learning. AFL adopts the strategy that alternately update the model parameters and the weight of each worker through alternating projected gradient descent/ascent.

% \item \textbf{DRFA-Prox}: It aims to mitigate the data heterogeneity issue in federated learning. Compared with AFL, it is communication-efficient which requires fewer communication rounds. Moreover, it leverages the prior distribution and introduces it as a regularizer in the objective function.

% \item \textbf{ASPIRE-EASE(-)}: The proposed ASPIRE-EASE without asynchronous setting.

% \item \textbf{ASPIRE-CP}: The proposed ASPIRE with cutting plane method.

% \item \textbf{ASPIRE-EASE$_{\rm{per}}$}: The proposed ASPIRE-EASE with periodic communication.

% \end{enumerate}

\noindent \textbf{1. Ind}${}_j$: It learns the model from an individual worker $j$.

\noindent \textbf{2. Mix}{$\rm{_{Even}}$}: It learns the model from all workers with even weights using the proposed distributed algorithm. 

\noindent \textbf{3. FedAvg}: It learns the model from all workers with even weights. It aggregates the local model parameters from workers through using model averaging.

\noindent \textbf{4. AFL}: It aims to address the fairness issues in federated learning. AFL adopts the strategy that alternately update the model parameters and the weight of each worker through alternating projected gradient descent/ascent.

\noindent \textbf{5. DRFA-Prox}: It aims to mitigate the data heterogeneity issue in federated learning. Compared with AFL, it is communication-efficient which requires fewer communication rounds. Moreover, it leverages the prior distribution and introduces it as a regularizer in the objective function.

\noindent \textbf{6. ASPIRE-EASE(-)}: The proposed ASPIRE-EASE without asynchronous setting.

\noindent \textbf{7. ASPIRE-CP}: The proposed ASPIRE with cutting plane method.

\noindent \textbf{8. ASPIRE-EASE$_{\rm{per}}$}: The proposed ASPIRE-EASE with periodic communication.


\subsection{Experiments about robustness against malicious attacks}

For the experiments about robustness against malicious attacks, We conduct experiments in the setting where there are malicious workers which attempt to mislead the model training process. The backdoor attack  \cite{bagdasaryan2020backdoor,wang2019neural} is adopted in the experiment which aims to bury the backdoor during the training phase of the model. The buried backdoor will be activated by the preset trigger. When the backdoor is not activated, the attacked model performs normally to other local models. When the backdoor is activated, the output of the attack model is misled as the target label which is pre-specified by the attacker. In the experiment, one worker is chosen as the malicious worker.  We add triggers to a small part of the data and change their primal labels to target labels (\textit{e.g.}, triggers are added on the local patch of clean images on the Fashion MNIST dataset, which are shown in Figure \ref{fig:backdoor_fashion}). Furthermore, the malicious worker can purposefully raise the training loss to mislead the master. To evaluate the model's robustness against malicious attacks, following \cite{dai2019backdoor}, we calculate the success attack rate of the backdoor attacks. The success attack rate can be calculated by checking how many instances in the backdoor dataset can be misled into the target labels. The lower success attack rate indicates better robustness against backdoor attacks.













\end{document}
