\begin{thebibliography}{77}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi

\bibitem[{Almazrouei et~al.(2023)Almazrouei, Alobeidli, and Alshamsi~et
  al.}]{falcon40b}
Ebtesam Almazrouei, Hamza Alobeidli, and Abdulaziz Alshamsi~et al. 2023.
\newblock {Falcon-40B}: an open large language model with state-of-the-art
  performance.

\bibitem[{Artetxe et~al.(2020)Artetxe, Ruder, and
  Yogatama}]{artetxe-etal-2020-cross}
Mikel Artetxe, Sebastian Ruder, and Dani Yogatama. 2020.
\newblock \href {https://doi.org/10.18653/v1/2020.acl-main.421} {On the
  cross-lingual transferability of monolingual representations}.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics}, pages 4623--4637, Online. Association for
  Computational Linguistics.

\bibitem[{Bang et~al.(2023)Bang, Cahyawijaya, Lee, Dai, Su, Wilie, Lovenia, Ji,
  Yu, Chung, Do, Xu, and Fung}]{Bang2023AMultitask}
Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie,
  Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet~V. Do, Yan Xu, and
  Pascale Fung. 2023.
\newblock A multitask, multilingual, multimodal evaluation of chatgpt on
  reasoning, hallucination, and interactivity.
\newblock \emph{ArXiv}, abs/2302.04023.

\bibitem[{Bommasani et~al.(2021)Bommasani, Hudson, and
  et~al.}]{Bommasani2021OnTO}
Rishi Bommasani, Drew~A. Hudson, and Ehsan~Adeli et~al. 2021.
\newblock On the opportunities and risks of foundation models.
\newblock \emph{ArXiv}, abs/2108.07258.

\bibitem[{Borji(2023)}]{Borji2023ACA}
Ali Borji. 2023.
\newblock A categorical archive of chatgpt failures.
\newblock \emph{ArXiv}, abs/2302.03494.

\bibitem[{Brown et~al.(2020)Brown, Mann, and et~al.}]{Brown2020LanguageMA}
Tom Brown, Benjamin Mann, and et~al. 2020.
\newblock Language models are few-shot learners.
\newblock \emph{ArXiv}, abs/2005.14165.

\bibitem[{Chen et~al.(2023)Chen, Jiang, Chen, Wang, Yu, Chen, Zhang, Liang,
  Zhang, Zhang, Li, Wan, Wang, and Li}]{Chen2023PhoenixDC}
Zhihong Chen, Feng Jiang, Junying Chen, Tiannan Wang, Fei Yu, Guiming Chen,
  Hongbo Zhang, Juhao Liang, Chen Zhang, Zhiyi Zhang, Jianquan Li, Xiang Wan,
  Benyou Wang, and Haizhou Li. 2023.
\newblock Phoenix: Democratizing chatgpt across languages.
\newblock \emph{ArXiv}, abs/2304.10453.

\bibitem[{Chiang et~al.(2023)Chiang, Li, Lin, Sheng, Wu, Zhang, Zheng, Zhuang,
  Zhuang, Gonzalez, Stoica, and Xing}]{vicuna2023}
Wei-Lin Chiang, Zhuohan Li, Zi~Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin
  Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph~E. Gonzalez, Ion Stoica, and
  Eric~P. Xing. 2023.
\newblock \href {https://lmsys.org/blog/2023-03-30-vicuna/} {Vicuna: An
  open-source chatbot impressing gpt-4 with 90\%* chatgpt quality}.

\bibitem[{Choi et~al.(2023)Choi, Hickman, Monahan, and
  Schwarcz}]{Choi2023ChatGPT}
Jonathan Choi, Kristin Hickman, Amy Monahan, and Daniel Schwarcz. 2023.
\newblock Chatgpt goes to law school.
\newblock \emph{Available at SSRN}.

\bibitem[{Christiano et~al.(2017)Christiano, Leike, Brown, Martic, Legg, and
  Amodei}]{Christiano2017Deep}
Paul~F. Christiano, Jan Leike, Tom~B. Brown, Miljan Martic, Shane Legg, and
  Dario Amodei. 2017.
\newblock Deep reinforcement learning from human preferences.
\newblock In \emph{Proceedings of the Annual Conference on Neural Information
  Processing Systems (NeurIPS)}.

\bibitem[{Chung et~al.(2022)Chung, Hou, Longpre, and
  et~al.}]{Chung2022ScalingIL}
Hyung~Won Chung, Le~Hou, S.~Longpre, and Barret~Zoph et~al. 2022.
\newblock Scaling instruction-finetuned language models.
\newblock \emph{ArXiv}, abs/2210.11416.

\bibitem[{Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick,
  and Tafjord}]{Clark2018ThinkYH}
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa
  Schoenick, and Oyvind Tafjord. 2018.
\newblock Think you have solved question answering? try arc, the ai2 reasoning
  challenge.
\newblock \emph{ArXiv}, abs/1803.05457.

\bibitem[{Conneau et~al.(2020)Conneau, Khandelwal, Goyal, Chaudhary, Wenzek,
  Guzm{\'a}n, Grave, Ott, Zettlemoyer, and
  Stoyanov}]{conneau-etal-2020-unsupervised}
Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume
  Wenzek, Francisco Guzm{\'a}n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and
  Veselin Stoyanov. 2020.
\newblock \href {https://doi.org/10.18653/v1/2020.acl-main.747} {Unsupervised
  cross-lingual representation learning at scale}.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics}, pages 8440--8451, Online. Association for
  Computational Linguistics.

\bibitem[{Conneau et~al.(2018)Conneau, Rinott, Lample, Williams, Bowman,
  Schwenk, and Stoyanov}]{conneau-etal-2018-xnli}
Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman,
  Holger Schwenk, and Veselin Stoyanov. 2018.
\newblock \href {https://doi.org/10.18653/v1/D18-1269} {{XNLI}: Evaluating
  cross-lingual sentence representations}.
\newblock In \emph{Proceedings of the 2018 Conference on Empirical Methods in
  Natural Language Processing}, pages 2475--2485, Brussels, Belgium.
  Association for Computational Linguistics.

\bibitem[{Conover et~al.(2023)Conover, Hayes, and et~al.}]{Conover2023Dolly}
Mike Conover, Matt Hayes, and Ankit~Mathur et~al. 2023.
\newblock Free dolly: Introducing the worldâ€™s first truly open
  instruction-tuned llm.
\newblock \emph{\url{https://www.databricks.com}}.

\bibitem[{Devlin et~al.(2019)Devlin, Chang, Lee, and
  Toutanova}]{devlin-etal-2019-bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.
\newblock \href {https://doi.org/10.18653/v1/N19-1423} {{BERT}: Pre-training of
  deep bidirectional transformers for language understanding}.
\newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pages 4171--4186,
  Minneapolis, Minnesota. Association for Computational Linguistics.

\bibitem[{Gao et~al.(2021)Gao, Tow, Biderman, and et~al.}]{eval-harness}
Leo Gao, Jonathan Tow, Stella Biderman, and et~al. 2021.
\newblock \href {https://doi.org/10.5281/zenodo.5371628} {A framework for
  few-shot language model evaluation}.

\bibitem[{Guo et~al.(2023)Guo, Zhang, Wang, Jiang, Nie, Ding, Yue, and
  Wu}]{Guo2023HowCI}
Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding,
  Jianwei Yue, and Yupeng Wu. 2023.
\newblock How close is chatgpt to human experts? comparison corpus, evaluation,
  and detection.
\newblock \emph{ArXiv}, abs/2301.07597.

\bibitem[{Guzman-Nateras et~al.(2022)Guzman-Nateras, Nguyen, and
  Nguyen}]{guzman-nateras-etal-2022-cross}
Luis Guzman-Nateras, Minh~Van Nguyen, and Thien Nguyen. 2022.
\newblock \href {https://doi.org/10.18653/v1/2022.naacl-main.409}
  {Cross-lingual event detection via optimized adversarial training}.
\newblock In \emph{Proceedings of the 2022 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pages 5588--5599, Seattle, United States. Association for
  Computational Linguistics.

\bibitem[{Hendrycks et~al.(2021)Hendrycks, Burns, Basart, Zou, Mazeika, Song,
  and Steinhardt}]{Hendrycks2021MeasuringMM}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika,
  Dawn~Xiaodong Song, and Jacob Steinhardt. 2021.
\newblock Measuring massive multitask language understanding.

\bibitem[{Hu et~al.(2020)Hu, Ruder, Siddhant, Neubig, Firat, and
  Johnson}]{Hu2020XTREME}
Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, and
  Melvin Johnson. 2020.
\newblock {XTREME}: A massively multilingual multi-task benchmark for
  evaluating cross-lingual generalisation.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning (ICML)}.

\bibitem[{Huang et~al.(2019)Huang, Liang, Duan, Gong, Shou, Jiang, and
  Zhou}]{huang-etal-2019-unicoder}
Haoyang Huang, Yaobo Liang, Nan Duan, Ming Gong, Linjun Shou, Daxin Jiang, and
  Ming Zhou. 2019.
\newblock \href {https://doi.org/10.18653/v1/D19-1252} {{U}nicoder: A universal
  language encoder by pre-training with multiple cross-lingual tasks}.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing and the 9th International Joint Conference on
  Natural Language Processing (EMNLP-IJCNLP)}, pages 2485--2494, Hong Kong,
  China. Association for Computational Linguistics.

\bibitem[{HuggingFace(2023)}]{Huggingface2023llm}
HuggingFace. 2023.
\newblock Open llm leaderboard.
\newblock \url{https://github.com/tatsu-lab/stanford_alpaca}.

\bibitem[{Jiao et~al.(2023)Jiao, Wang, tse Huang, Wang, and
  Tu}]{Jiao2023Chatgpt}
Wenxiang Jiao, Wenxuan Wang, Jen tse Huang, Xing Wang, and Zhaopeng Tu. 2023.
\newblock Is chatgpt a good translator? yes with gpt-4 as the engine.
\newblock \emph{ArXiv}, 2301.08745.

\bibitem[{Kamal~Eddine et~al.(2021)Kamal~Eddine, Tixier, and
  Vazirgiannis}]{kamal-eddine-etal-2021-barthez}
Moussa Kamal~Eddine, Antoine Tixier, and Michalis Vazirgiannis. 2021.
\newblock \href {https://doi.org/10.18653/v1/2021.emnlp-main.740} {{BART}hez: a
  skilled pretrained {F}rench sequence-to-sequence model}.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pages 9369--9390, Online and Punta Cana,
  Dominican Republic. Association for Computational Linguistics.

\bibitem[{Karthikeyan et~al.(2020)Karthikeyan, Wang, Mayhew, and
  Roth}]{Karthikeyan:20}
K~Karthikeyan, Zihan Wang, Stephen Mayhew, and Dan Roth. 2020.
\newblock Cross-lingual ability of multilingual bert: An empirical study.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations (ICLR)}.

\bibitem[{Kenton et~al.(2021)Kenton, Everitt, Weidinger, Gabriel, Mikulik, and
  Irving}]{Kenton2021AlignmentOL}
Zachary Kenton, Tom Everitt, Laura Weidinger, Iason Gabriel, Vladimir Mikulik,
  and Geoffrey Irving. 2021.
\newblock Alignment of language agents.
\newblock \emph{ArXiv}, abs/2103.14659.

\bibitem[{Kondratyuk and Straka(2019)}]{kondratyuk-straka-2019-75}
Dan Kondratyuk and Milan Straka. 2019.
\newblock \href {https://doi.org/10.18653/v1/D19-1279} {75 languages, 1 model:
  Parsing {U}niversal {D}ependencies universally}.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing and the 9th International Joint Conference on
  Natural Language Processing (EMNLP-IJCNLP)}, pages 2779--2795, Hong Kong,
  China. Association for Computational Linguistics.

\bibitem[{Kung et~al.(2022)Kung, Cheatham, Medenilla, Sillos, De~Leon,
  Elepa{\~n}o, Madriaga, Aggabao, Diaz-Candido, Maningo
  et~al.}]{Kung2022Performance}
Tiffany~H Kung, Morgan Cheatham, Arielle Medenilla, Czarina Sillos, Lorie
  De~Leon, Camille Elepa{\~n}o, Maria Madriaga, Rimel Aggabao, Giezel
  Diaz-Candido, James Maningo, et~al. 2022.
\newblock Performance of chatgpt on usmle: Potential for ai-assisted medical
  education using large language models.
\newblock \emph{medRxiv}.

\bibitem[{Lai et~al.(2023)Lai, Ngo, Veyseh, Man, Dernoncourt, Bui, and
  Nguyen}]{Lai2023ChatGPTBE}
Viet~Dac Lai, Nghia~Trung Ngo, Amir Pouran~Ben Veyseh, Hieu Man, Franck
  Dernoncourt, Trung Bui, and Thien~Huu Nguyen. 2023.
\newblock Chatgpt beyond english: Towards a comprehensive evaluation of large
  language models in multilingual learning.
\newblock \emph{ArXiv}, abs/2304.05613.

\bibitem[{Lewis et~al.(2020)Lewis, Liu, Goyal, Ghazvininejad, Mohamed, Levy,
  Stoyanov, and Zettlemoyer}]{lewis-etal-2020-bart}
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,
  Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020.
\newblock \href {https://doi.org/10.18653/v1/2020.acl-main.703} {{BART}:
  Denoising sequence-to-sequence pre-training for natural language generation,
  translation, and comprehension}.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics}, pages 7871--7880, Online. Association for
  Computational Linguistics.

\bibitem[{Li et~al.(2023)Li, Koto, Wu, Aji, and Baldwin}]{Li2023BactrianXA}
Haonan Li, Fajri Koto, Minghao Wu, Alham~Fikri Aji, and Timothy Baldwin. 2023.
\newblock Bactrian-x: A multilingual replicable instruction-following model
  with low-rank adaptation.
\newblock \emph{ArXiv}, abs/2305.15011.

\bibitem[{Liang et~al.(2020)Liang, Duan, Gong, Wu, Guo, Qi, Gong, Shou, Jiang,
  Cao, Fan, Zhang, Agrawal, Cui, Wei, Bharti, Qiao, Chen, Wu, Liu, Yang,
  Campos, Majumder, and Zhou}]{liang-etal-2020-xglue}
Yaobo Liang, Nan Duan, Yeyun Gong, Ning Wu, Fenfei Guo, Weizhen Qi, Ming Gong,
  Linjun Shou, Daxin Jiang, Guihong Cao, Xiaodong Fan, Ruofei Zhang, Rahul
  Agrawal, Edward Cui, Sining Wei, Taroon Bharti, Ying Qiao, Jiun-Hung Chen,
  Winnie Wu, Shuguang Liu, Fan Yang, Daniel Campos, Rangan Majumder, and Ming
  Zhou. 2020.
\newblock \href {https://doi.org/10.18653/v1/2020.emnlp-main.484} {{XGLUE}: A
  new benchmark dataset for cross-lingual pre-training, understanding and
  generation}.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, pages 6008--6018, Online. Association
  for Computational Linguistics.

\bibitem[{Lin et~al.(2022)Lin, Hilton, and Evans}]{lin-etal-2022-truthfulqa}
Stephanie Lin, Jacob Hilton, and Owain Evans. 2022.
\newblock \href {https://doi.org/10.18653/v1/2022.acl-long.229}
  {{T}ruthful{QA}: Measuring how models mimic human falsehoods}.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pages 3214--3252,
  Dublin, Ireland. Association for Computational Linguistics.

\bibitem[{Liu et~al.(2020)Liu, Gu, Goyal, Li, Edunov, Ghazvininejad, Lewis, and
  Zettlemoyer}]{liu-etal-2020-multilingual-denoising}
Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan
  Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. 2020.
\newblock \href {https://doi.org/10.1162/tacl_a_00343} {Multilingual denoising
  pre-training for neural machine translation}.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  8:726--742.

\bibitem[{Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov}]{Liu2019RoBERTaAR}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock \emph{ArXiv}, abs/1907.11692.

\bibitem[{Longpre et~al.(2023)Longpre, Hou, Vu, Webson, Chung, Tay, Zhou, Le,
  Zoph, Wei, and Roberts}]{Longpre2023TheFC}
S.~Longpre, Le~Hou, Tu~Vu, Albert Webson, Hyung~Won Chung, Yi~Tay, Denny Zhou,
  Quoc~V. Le, Barret Zoph, Jason Wei, and Adam Roberts. 2023.
\newblock The flan collection: Designing data and methods for effective
  instruction tuning.
\newblock \emph{ArXiv}, abs/2301.13688.

\bibitem[{Martin et~al.(2020)Martin, Muller, Ortiz~Su{\'a}rez, Dupont, Romary,
  de~la Clergerie, Seddah, and Sagot}]{martin-etal-2020-camembert}
Louis Martin, Benjamin Muller, Pedro~Javier Ortiz~Su{\'a}rez, Yoann Dupont,
  Laurent Romary, {\'E}ric de~la Clergerie, Djam{\'e} Seddah, and Beno{\^\i}t
  Sagot. 2020.
\newblock \href {https://doi.org/10.18653/v1/2020.acl-main.645} {{C}amem{BERT}:
  a tasty {F}rench language model}.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics}, pages 7203--7219, Online. Association for
  Computational Linguistics.

\bibitem[{Min et~al.(2023)Min, Ross, Sulem, Veyseh, Nguyen, Sainz, Agirre,
  Heintz, and Roth}]{Min2023Recent}
Bonan Min, Hayley Ross, Elior Sulem, Amir Pouran~Ben Veyseh, Thien~Huu Nguyen,
  Oscar Sainz, Eneko Agirre, Ilana Heintz, and Dan Roth. 2023.
\newblock \href {https://doi.org/10.1145/3605943} {Recent advances in natural
  language processing via large pre-trained language models: A survey}.
\newblock \emph{ACM Computing Survey}.

\bibitem[{MMG(2021)}]{spanish2020robert}
MMG MMG. 2021.
\newblock \href {https://huggingface.co/MMG/mlm-spanish-roberta-base} {Spanish
  roberta}.

\bibitem[{Moell(2021)}]{swedish2020robert}
Birger Moell. 2021.
\newblock \href {https://huggingface.co/birgermoell/roberta-swedish-scandi}
  {Swedish roberta}.

\bibitem[{MosaicML(2023)}]{MTP}
MosaicML. 2023.
\newblock Introducing mpt-7b: A new standard for open-source, commercially
  usable llms.
\newblock \emph{\url{https://www.mosaicml.com/blog/mpt-7b}}.

\bibitem[{Muennighoff et~al.(2022)Muennighoff, Wang, and Sutawika~et
  al.}]{Muennighoff2022CrosslingualGT}
Niklas Muennighoff, Thomas Wang, and Lintang Sutawika~et al. 2022.
\newblock Crosslingual generalization through multitask finetuning.
\newblock \emph{ArXiv}, abs/2211.01786.

\bibitem[{Nguyen et~al.(2021{\natexlab{a}})Nguyen, Lai, Pouran Ben~Veyseh, and
  Nguyen}]{nguyen-etal-2021-trankit}
Minh~Van Nguyen, Viet~Dac Lai, Amir Pouran Ben~Veyseh, and Thien~Huu Nguyen.
  2021{\natexlab{a}}.
\newblock \href {https://doi.org/10.18653/v1/2021.eacl-demos.10} {Trankit: A
  light-weight transformer-based toolkit for multilingual natural language
  processing}.
\newblock In \emph{Proceedings of the 16th Conference of the European Chapter
  of the Association for Computational Linguistics: System Demonstrations},
  pages 80--90, Online. Association for Computational Linguistics.

\bibitem[{Nguyen et~al.(2021{\natexlab{b}})Nguyen, Nguyen, Min, and
  Nguyen}]{nguyen-etal-2021-crosslingual}
Minh~Van Nguyen, Tuan~Ngo Nguyen, Bonan Min, and Thien~Huu Nguyen.
  2021{\natexlab{b}}.
\newblock \href {https://doi.org/10.18653/v1/2021.emnlp-main.440} {Crosslingual
  transfer learning for relation and event extraction via word category and
  class alignments}.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pages 5414--5426, Online and Punta Cana,
  Dominican Republic. Association for Computational Linguistics.

\bibitem[{Nivre et~al.(2016)Nivre, de~Marneffe, Ginter, Goldberg, Haji{\v{c}},
  Manning, McDonald, Petrov, Pyysalo, Silveira, Tsarfaty, and
  Zeman}]{Nivre2016Universal}
Joakim Nivre, Marie-Catherine de~Marneffe, Filip Ginter, Yoav Goldberg, Jan
  Haji{\v{c}}, Christopher~D. Manning, Ryan McDonald, Slav Petrov, Sampo
  Pyysalo, Natalia Silveira, Reut Tsarfaty, and Daniel Zeman. 2016.
\newblock {U}niversal {D}ependencies v1: A multilingual treebank collection.
\newblock In \emph{Proceedings of the Tenth International Conference on
  Language Resources and Evaluation (LREC)}.

\bibitem[{Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin,
  Zhang, Agarwal, Slama, Ray, Schulman, Hilton, Kelton, Miller, Simens, Askell,
  Welinder, Christiano, Leike, and Lowe}]{Ouyang2022TrainingLM}
Long Ouyang, Jeff Wu, Xu~Jiang, Diogo Almeida, Carroll~L. Wainwright, Pamela
  Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John
  Schulman, Jacob Hilton, Fraser Kelton, Luke~E. Miller, Maddie Simens, Amanda
  Askell, Peter Welinder, Paul~Francis Christiano, Jan Leike, and Ryan~J. Lowe.
  2022.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{ArXiv}, abs/2203.02155.

\bibitem[{Parmar(2021)}]{hindi2020robert}
Suraj Parmar. 2021.
\newblock \href {https://huggingface.co/surajp/RoBERTa-hindi-guj-san} {Hindi
  roberta}.

\bibitem[{Perez-Beltrachini and
  Lapata(2021)}]{perez-beltrachini-lapata-2021-models}
Laura Perez-Beltrachini and Mirella Lapata. 2021.
\newblock \href {https://doi.org/10.18653/v1/2021.emnlp-main.742} {Models and
  datasets for cross-lingual summarisation}.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pages 9408--9423, Online and Punta Cana,
  Dominican Republic. Association for Computational Linguistics.

\bibitem[{Pires et~al.(2019)Pires, Schlinger, and
  Garrette}]{pires-etal-2019-multilingual}
Telmo Pires, Eva Schlinger, and Dan Garrette. 2019.
\newblock \href {https://doi.org/10.18653/v1/P19-1493} {How multilingual is
  multilingual {BERT}?}
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association
  for Computational Linguistics}, pages 4996--5001, Florence, Italy.
  Association for Computational Linguistics.

\bibitem[{Pouran Ben~Veyseh et~al.(2022)Pouran Ben~Veyseh, Ebrahimi,
  Dernoncourt, and Nguyen}]{pouran-ben-veyseh-etal-2022-mee}
Amir Pouran Ben~Veyseh, Javid Ebrahimi, Franck Dernoncourt, and Thien Nguyen.
  2022.
\newblock \href {https://aclanthology.org/2022.emnlp-main.652} {{MEE}: A novel
  multilingual event extraction dataset}.
\newblock In \emph{Proceedings of the 2022 Conference on Empirical Methods in
  Natural Language Processing}, pages 9603--9613, Abu Dhabi, United Arab
  Emirates. Association for Computational Linguistics.

\bibitem[{Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever
  et~al.}]{Radford2019Language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
  Sutskever, et~al. 2019.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}.

\bibitem[{Rae et~al.(2021)Rae, Borgeaud, and et~al.}]{Rae2021ScalingLM}
Jack Rae, Sebastian Borgeaud, and et~al. 2021.
\newblock Scaling language models: Methods, analysis \& insights from training
  gopher.
\newblock \emph{ArXiv}, abs/2112.11446.

\bibitem[{Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu}]{Raffel2020Xxploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J. Liu. 2020.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock In \emph{Journal of Machine Learning Research}.

\bibitem[{Resources and Technology~Infrastructure(2021)}]{polish2020robert}
Common~Language Resources and Poland Technology~Infrastructure. 2021.
\newblock \href {https://huggingface.co/clarin-pl/roberta-polish-kgr10} {Polish
  roberta}.

\bibitem[{Sang and Meulder(2002)}]{Sang2002Introduction}
Erik F. Tjong~Kim Sang and Fien~De Meulder. 2002.
\newblock Introduction to the conll-2002 shared task: Language-independent
  named entity recognition.
\newblock In \emph{Proceedings of the Conference on Computational Natural
  Language Learning (CoNLL)}.

\bibitem[{Sang and Meulder(2003)}]{Sang2003Introduction}
Erik F. Tjong~Kim Sang and Fien~De Meulder. 2003.
\newblock Introduction to the conll-2003 shared task: Language-independent
  named entity recognition.
\newblock In \emph{Proceedings of the Conference on Computational Natural
  Language Learning (CoNLL)}.

\bibitem[{Sanh et~al.(2021)Sanh, Webson, and et~al.}]{Sanh2021MultitaskPT}
Victor Sanh, Albert Webson, and Colin~Raffel et~al. 2021.
\newblock Multitask prompted training enables zero-shot task generalization.
\newblock \emph{ArXiv}, abs/2110.08207.

\bibitem[{Scao et~al.(2022)Scao, Fan, and et~al.}]{Scao2022BLOOMA1}
Teven Scao, Angela Fan, and et~al. 2022.
\newblock Bloom: A 176b-parameter open-access multilingual language model.
\newblock \emph{ArXiv}, abs/2211.05100.

\bibitem[{StabilityAI(2023)}]{StableLM}
StabilityAI. 2023.
\newblock Stablelm: Stability ai language models.
\newblock \emph{\url{https://github.com/stability-AI/stableLM}}.

\bibitem[{Stiennon et~al.(2020)Stiennon, Ouyang, Wu, Ziegler, Lowe, Voss,
  Radford, Amodei, and Christiano}]{Stiennon2020LearningTS}
Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel~M. Ziegler, Ryan~J. Lowe, Chelsea
  Voss, Alec Radford, Dario Amodei, and Paul Christiano. 2020.
\newblock Learning to summarize from human feedback.
\newblock \emph{ArXiv}, abs/2009.01325.

\bibitem[{Tamkin et~al.(2021)Tamkin, Brundage, Clark, and
  Ganguli}]{Tamkin2021UnderstandingTC}
Alex Tamkin, Miles Brundage, Jack Clark, and Deep Ganguli. 2021.
\newblock Understanding the capabilities, limitations, and societal impact of
  large language models.
\newblock \emph{ArXiv}, abs/2102.02503.

\bibitem[{Taori et~al.(2023)Taori, Gulrajani, and et~al.}]{alpaca}
Rohan Taori, Ishaan Gulrajani, and Tianyi~Zhang et~al. 2023.
\newblock Stanford alpaca: An instruction-following llama model.
\newblock \url{https://github.com/tatsu-lab/stanford_alpaca}.

\bibitem[{Touvron et~al.(2023)Touvron, Lavril, and et~al.}]{Touvron2023LLaMAOA}
Hugo Touvron, Thibaut Lavril, and Gautier~Izacard et~al. 2023.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{ArXiv}, abs/2302.13971.

\bibitem[{Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones,
  Gomez, Kaiser, and Polosukhin}]{Vaswani2017Attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017.
\newblock Attention is all you need.
\newblock In \emph{Advances in Neural Information Processing Systems}.

\bibitem[{Wang et~al.(2023)Wang, Kordi, Mishra, Liu, Smith, Khashabi, and
  Hajishirzi}]{Wang2023SelfInstructAL}
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah~A. Smith, Daniel
  Khashabi, and Hannaneh Hajishirzi. 2023.
\newblock Self-instruct: Aligning language model with self generated
  instructions.

\bibitem[{Wei et~al.(2021)Wei, Bosma, Zhao, Guu, Yu, Lester, Du, Dai, and
  Le}]{Wei2021FinetunedLM}
Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams~Wei Yu, Brian Lester,
  Nan Du, Andrew~M. Dai, and Quoc~V. Le. 2021.
\newblock Finetuned language models are zero-shot learners.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations}.

\bibitem[{Wei et~al.(2022)Wei, Tay, Bommasani, Raffel, Zoph, Borgeaud,
  Yogatama, Bosma, Zhou, Metzler, hsin Chi, Hashimoto, Vinyals, Liang, Dean,
  and Fedus}]{Wei2022EmergentAO}
Jason Wei, Yi~Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian
  Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed~Huai
  hsin Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and
  William Fedus. 2022.
\newblock Emergent abilities of large language models.
\newblock \emph{Transactions on Machine Learning Research}.

\bibitem[{Weidinger et~al.(2021)Weidinger, Mellor, and
  et~al.}]{Weidinger2021EthicalAS}
Laura Weidinger, John F.~J. Mellor, and Maribeth~Rauh et~al. 2021.
\newblock Ethical and social risks of harm from language models.
\newblock \emph{ArXiv}, abs/2112.04359.

\bibitem[{Wongso(2021)}]{japanese2020robert}
Wilson Wongso. 2021.
\newblock \href {https://huggingface.co/w11wo/javanese-roberta-small} {Japanese
  roberta}.

\bibitem[{Wu et~al.(2022)Wu, Wu, Zhang, Xiong, Chen, Zhuang, and
  Feng}]{wu-etal-2022-learning}
Linjuan Wu, Shaojuan Wu, Xiaowang Zhang, Deyi Xiong, Shizhan Chen, Zhiqiang
  Zhuang, and Zhiyong Feng. 2022.
\newblock \href {https://doi.org/10.18653/v1/2022.acl-long.70} {Learning
  disentangled semantic representations for zero-shot cross-lingual transfer in
  multilingual machine reading comprehension}.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pages 991--1000,
  Dublin, Ireland. Association for Computational Linguistics.

\bibitem[{Wu et~al.(2023)Wu, Waheed, Zhang, Abdul-Mageed, and
  Aji}]{Wu2023LaMiniLMAD}
Minghao Wu, Abdul Waheed, Chiyu Zhang, Muhammad Abdul-Mageed, and Alham~Fikri
  Aji. 2023.
\newblock Lamini-lm: A diverse herd of distilled models from large-scale
  instructions.
\newblock \emph{ArXiv}, abs/2304.14402.

\bibitem[{Wu and Dredze(2019)}]{wu-dredze-2019-beto}
Shijie Wu and Mark Dredze. 2019.
\newblock \href {https://doi.org/10.18653/v1/D19-1077} {Beto, bentz, becas: The
  surprising cross-lingual effectiveness of {BERT}}.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing and the 9th International Joint Conference on
  Natural Language Processing (EMNLP-IJCNLP)}, pages 833--844, Hong Kong,
  China. Association for Computational Linguistics.

\bibitem[{Xue et~al.(2021)Xue, Constant, Roberts, Kale, Al-Rfou, Siddhant,
  Barua, and Raffel}]{xue-etal-2021-mt5}
Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya
  Siddhant, Aditya Barua, and Colin Raffel. 2021.
\newblock \href {https://doi.org/10.18653/v1/2021.naacl-main.41} {m{T}5: A
  massively multilingual pre-trained text-to-text transformer}.
\newblock In \emph{Proceedings of the 2021 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pages 483--498, Online. Association for Computational
  Linguistics.

\bibitem[{Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and
  Choi}]{zellers-etal-2019-hellaswag}
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019.
\newblock \href {https://doi.org/10.18653/v1/P19-1472} {{H}ella{S}wag: Can a
  machine really finish your sentence?}
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association
  for Computational Linguistics}, pages 4791--4800, Florence, Italy.
  Association for Computational Linguistics.

\bibitem[{Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan,
  Diab, Li, Lin, Mihaylov, Ott, Shleifer, Shuster, Simig, Koura, Sridhar, Wang,
  and Zettlemoyer}]{Zhang2022OPTOP}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
  Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, Todor Mihaylov,
  Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit~Singh Koura, Anjali
  Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022.
\newblock Opt: Open pre-trained transformer language models.
\newblock \emph{ArXiv}, abs/2205.01068.

\bibitem[{Zhang et~al.(2021)Zhang, Ma, Shi, and Lin}]{zhang-etal-2021-mr}
Xinyu Zhang, Xueguang Ma, Peng Shi, and Jimmy Lin. 2021.
\newblock \href {https://doi.org/10.18653/v1/2021.mrl-1.12} {Mr. {T}y{D}i: A
  multi-lingual benchmark for dense retrieval}.
\newblock In \emph{Proceedings of the 1st Workshop on Multilingual
  Representation Learning}, pages 127--137, Punta Cana, Dominican Republic.
  Association for Computational Linguistics.

\end{thebibliography}
