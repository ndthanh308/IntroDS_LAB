% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

%%%Evaluation
@software{eval-harness,
  author       = {Gao, Leo and
                  Tow, Jonathan and
                  Biderman, Stella and
                  et al.},
  title        = {A framework for few-shot language model evaluation},
  month        = sep,
  year         = 2021,
  publisher    = {Zenodo},
  version      = {v0.0.1},
  doi          = {10.5281/zenodo.5371628},
  url          = {https://doi.org/10.5281/zenodo.5371628}
}

@misc{Huggingface2023llm,
  author = {HuggingFace},
  title = {Open LLM Leaderboard},
  year = {2023},
  journal = {HuggingFace},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}

%%%ARC
@article{Clark2018ThinkYH,
  title={Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge},
  author={Peter Clark and Isaac Cowhey and Oren Etzioni and Tushar Khot and Ashish Sabharwal and Carissa Schoenick and Oyvind Tafjord},
  journal={ArXiv},
  year={2018},
  volume={abs/1803.05457}
}

%%%{H}ella{S}wag: zellers-etal-2019-hellaswag

%%%MMLU
@article{Hendrycks2021MeasuringMM,
  title={Measuring Massive Multitask Language Understanding},
  author={Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Xiaodong Song and Jacob Steinhardt},
  booktitle = {Proceedings of the International Conference on Learning Representations},
  year={2021}
}

%%%TruthfullQA: lin-etal-2022-truthfulqa

%multilingual GPT

%%%Bactrian
@article{Li2023BactrianXA,
  title={Bactrian-X: A Multilingual Replicable Instruction-Following Model with Low-Rank Adaptation},
  author={Haonan Li and Fajri Koto and Minghao Wu and Alham Fikri Aji and Timothy Baldwin},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.15011}
}

%%%Vicuna
@misc{vicuna2023,
    title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
    url = {https://lmsys.org/blog/2023-03-30-vicuna/},
    author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
    month = {March},
    year = {2023}
}

%%%LIMA
@article{Zhou2023LIMALI,
  title={LIMA: Less Is More for Alignment},
  author={Chunting Zhou and Pengfei Liu and Puxin Xu et al.},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.11206}
}

%%%Alpaca
@misc{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang et al.},
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}

%%%LaMini
@article{Wu2023LaMiniLMAD,
  title={LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions},
  author={Minghao Wu and Abdul Waheed and Chiyu Zhang and Muhammad Abdul-Mageed and Alham Fikri Aji},
  journal={ArXiv},
  year={2023},
  volume={abs/2304.14402}
}

%%%BLOOM
@article{Scao2022BLOOMA1,
  title={BLOOM: A 176B-Parameter Open-Access Multilingual Language Model},
  author={Teven Scao and Angela Fan and et al.},
  journal={ArXiv},
  year={2022},
  volume={abs/2211.05100}
}

%%%% Self-instruct
@article{Wang2023SelfInstructAL,
  title={Self-Instruct: Aligning Language Model with Self Generated Instructions},
  author={Yizhong Wang and Yeganeh Kordi and Swaroop Mishra and Alisa Liu and Noah A. Smith and Daniel Khashabi and Hannaneh Hajishirzi},
  booktitle = {Proceedings of the Annual Meeting of the Association for Computational Linguistics},
  year={2023}
}

%%%Dolly
@article{Conover2023Dolly,
  title={Free Dolly: Introducing the World’s First Truly Open Instruction-tuned LLM},
  author={Mike Conover and Matt Hayes and Ankit Mathur et al.},
  journal={\url{https://www.databricks.com}},
  year={2023}
}

%%% StableLM
@article{StableLM,
  title={StableLM: Stability AI Language Models},
  author={StabilityAI},
  journal={\url{https://github.com/stability-AI/stableLM}},
  year={2023}
}

%%% Falcon
@article{falcon40b,
  title={{Falcon-40B}: an open large language model with state-of-the-art performance},
  author={Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi et al., Abdulaziz},
  year={2023}
}

%%% MTP
@article{MTP,
  title={Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs},
  author={MosaicML},
  journal={\url{https://www.mosaicml.com/blog/mpt-7b}},
  year={2023}
}

%%%LLaMa
@article{Touvron2023LLaMAOA,
  title={LLaMA: Open and Efficient Foundation Language Models},
  author={Hugo Touvron and Thibaut Lavril and Gautier Izacard et al.},
  journal={ArXiv},
  year={2023},
  volume={abs/2302.13971}
}

%%%LoRA
@article{Hu2022LoRA,
  title={LoRA: Low-Rank Adaptation of Large Language Models},
  author={Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Weizhu Chen},
  booktitle = {Proceedings of the International Conference on Learning Representations},
  year={2022}
}

%%%PaLM
@article{Chowdhery2022PaLMSL,
  title={PaLM: Scaling Language Modeling with Pathways},
  author={Aakanksha Chowdhery and Sharan Narang and Jacob Devlin et al.},
  journal={ArXiv},
  year={2022},
  volume={abs/2204.02311}
}

%%%GLM
@article{Zeng2022GLM130BAO,
  title={GLM-130B: An Open Bilingual Pre-trained Model},
  author={Aohan Zeng and Xiao Liu and Zhengxiao Du and Zihan Wang and Hanyu Lai and Ming Ding and Zhuoyi Yang and Yifan Xu and Wendi Zheng and Xiao Xia and Weng Lam Tam and Zixuan Ma and Yufei Xue and Jidong Zhai and Wenguang Chen and P. Zhang and Yuxiao Dong and Jie Tang},
  booktitle = {Proceedings of the International Conference on Learning Representations},
  year={2022},
  volume={abs/2210.02414}
}

%%%Phoenix
@article{Chen2023PhoenixDC,
  title={Phoenix: Democratizing ChatGPT across Languages},
  author={Zhihong Chen and Feng Jiang and Junying Chen and Tiannan Wang and Fei Yu and Guiming Chen and Hongbo Zhang and Juhao Liang and Chen Zhang and Zhiyi Zhang and Jianquan Li and Xiang Wan and Benyou Wang and Haizhou Li},
  journal={ArXiv},
  year={2023},
  volume={abs/2304.10453}
}

@article{Lai2023ChatGPTBE,
  title={ChatGPT Beyond English: Towards a Comprehensive Evaluation of Large Language Models in Multilingual Learning},
  author={Viet Dac Lai and Nghia Trung Ngo and Amir Pouran Ben Veyseh and Hieu Man and Franck Dernoncourt and Trung Bui and Thien Huu Nguyen},
  journal={ArXiv},
  year={2023},
  volume={abs/2304.05613}
}

%%%Flan-Collection
@article{Longpre2023TheFC,
  title={The Flan Collection: Designing Data and Methods for Effective Instruction Tuning},
  author={S. Longpre and Le Hou and Tu Vu and Albert Webson and Hyung Won Chung and Yi Tay and Denny Zhou and Quoc V. Le and Barret Zoph and Jason Wei and Adam Roberts},
  journal={ArXiv},
  year={2023},
  volume={abs/2301.13688}
}

@article{Chung2022ScalingIL,
  title={Scaling Instruction-Finetuned Language Models},
  author={Hyung Won Chung and Le Hou and S. Longpre and Barret Zoph et al.},
  journal={ArXiv},
  year={2022},
  volume={abs/2210.11416}
}

%RL with Human Feedback
@inproceedings{Christiano2017Deep,
  author    = {Paul F. Christiano and
               Jan Leike and
               Tom B. Brown and
               Miljan Martic and
               Shane Legg and
               Dario Amodei},
  editor    = {Isabelle Guyon and
               Ulrike von Luxburg and
               Samy Bengio and
               Hanna M. Wallach and
               Rob Fergus and
               S. V. N. Vishwanathan and
               Roman Garnett},
  title     = {Deep Reinforcement Learning from Human Preferences},
  booktitle = {Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS)},
  year      = {2017}
}

@article{Stiennon2020LearningTS,
  title={Learning to summarize from human feedback},
  author={Nisan Stiennon and Long Ouyang and Jeff Wu and Daniel M. Ziegler and Ryan J. Lowe and Chelsea Voss and Alec Radford and Dario Amodei and Paul Christiano},
  journal={ArXiv},
  year={2020},
  volume={abs/2009.01325}
}

@inproceedings{Wei2021FinetunedLM,
  title={Finetuned Language Models Are Zero-Shot Learners},
  author={Jason Wei and Maarten Bosma and Vincent Zhao and Kelvin Guu and Adams Wei Yu and Brian Lester and Nan Du and Andrew M. Dai and Quoc V. Le},
  booktitle = {Proceedings of the International Conference on Learning Representations},
  year={2021}
}

@article{Sanh2021MultitaskPT,
  title={Multitask Prompted Training Enables Zero-Shot Task Generalization},
  author={Victor Sanh and Albert Webson and Colin Raffel et al.},
  journal={ArXiv},
  year={2021},
  volume={abs/2110.08207}
}

%%InstructGPT
@article{Ouyang2022TrainingLM,
  title={Training language models to follow instructions with human feedback},
  author={Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke E. Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Francis Christiano and Jan Leike and Ryan J. Lowe},
  journal={ArXiv},
  year={2022},
  volume={abs/2203.02155}
}

@article{Muennighoff2022CrosslingualGT,
  title={Crosslingual Generalization through Multitask Finetuning},
  author={Muennighoff, Niklas and Wang, Thomas and Sutawika et al., Lintang},
  journal={ArXiv},
  year={2022},
  volume={abs/2211.01786}
}


skip to main content
ACM Digital Library home
ACM home

    Browse About
        Sign in Register 

    Journals
    Magazines
    Proceedings
    Books
    SIGs
    Conferences
    People

Search ACM Digital Library
Advanced Search

    Journal Home
    Just Accepted
    Latest Issue
    Archive
    Authors
    Editors
    Reviewers
    About
    Contact Us

    HomeACM JournalsACM Computing SurveysJust AcceptedRecent Advances in Natural Language Processing via Large Pre-Trained Language Models: A Survey

survey Free Access
Share on

Recent Advances in Natural Language Processing via Large Pre-Trained Language Models: A Survey
Just Accepted

    Authors:
    Bonan Min

    ,
    Hayley Ross

    ,
    Elior Sulem

    ,
    Amir Pouran Ben Veyseh

    ,
    Thien Huu Nguyen

    ,
    Oscar Sainz

    ,
    Eneko Agirre

    ,
    Ilana Heintz

    ,
    Dan Roth

Authors Info & Claims
ACM Computing SurveysAccepted on June 2023https://doi.org/10.1145/3605943
Published:27 June 2023Publication HistoryCheck for updates on crossmark

    0citation626Downloads

    eReaderPDF

ACM Computing Surveys
Just Accepted
Previous
Next

        Abstract
        References
        Index Terms
        Recommendations
        Comments

ACM Digital Library
Skip Abstract Section
Abstract

Large, pre-trained language models (PLMs) such as BERT and GPT have drastically changed the Natural Language Processing (NLP) field. For numerous NLP tasks, approaches leveraging PLMs have achieved state-of-the-art performance. The key idea is to learn a generic, latent representation of language from a generic task once, then share it across disparate NLP tasks. Language modeling serves as the generic task, one with abundant self-supervised text available for extensive training. This article presents the key fundamental concepts of PLM architectures and a comprehensive view of the shift to PLM-driven NLP techniques. It surveys work applying the pre-training then fine-tuning, prompting, and text generation approaches. In addition, it discusses PLM limitations and suggested directions for future research.

References

    Omri Abend and Ari Rappoport. 2013. Universal Conceptual Cognitive Annotation (UCCA). In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).Google Scholar
    Zeyuan Allen-Zhu and Yuanzhi Li. 2021. Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning. (2021). arxiv:2012.09816  [cs.LG]Google Scholar
    Asaf Amrami and Yoav Goldberg. 2019. Towards better substitution-based word sense induction. (2019). arxiv:1905.12598  [cs.CL]Google Scholar
    Google Scholar
    Google ScholarCross Ref
    Google Scholar
    Google Scholar
    Google Scholar
    Google Scholar
    Google ScholarCross Ref
    Google Scholar
    Google ScholarDigital Library
    Google Scholar
    Google ScholarCross Ref
    Google ScholarCross Ref
    Google Scholar
    Google Scholar
    Google ScholarCross Ref
    Google Scholar
    Google ScholarCross Ref
    Google ScholarDigital Library
    Google Scholar
    Google ScholarCross Ref
    Google ScholarCross Ref
    Google ScholarDigital Library
    Google Scholar
    Google ScholarCross Ref
    Google ScholarCross Ref
    Google Scholar
    Google Scholar
    Google Scholar
    Google ScholarCross Ref
    Google ScholarCross Ref
    Google Scholar
    Google ScholarCross Ref
    Google Scholar
    Google Scholar
    Google Scholar
    Google Scholar
    Google ScholarCross Ref
    Google Scholar
    Google Scholar
    Google Scholar
    Google Scholar
    Google Scholar
    Google Scholar
    Google Scholar
    Google Scholar
    Google ScholarCross Ref
    Google ScholarCross Ref
    Google Scholar
    Google Scholar
    Google Scholar
    Google ScholarCross Ref
    Google Scholar
    Google Scholar
    Google ScholarCross Ref
    Google ScholarCross Ref
    Google Scholar
    Google ScholarCross Ref
    Google ScholarCross Ref
    Google ScholarCross Ref
    Google ScholarDigital Library
    Google ScholarDigital Library
    Google Scholar
    Google ScholarCross Ref
    Google Scholar
    Google Scholar
    Google ScholarCross Ref
    Google Scholar
    Google Scholar
    Google Scholar
    Google ScholarCross Ref
    Google ScholarCross Ref
    Google ScholarCross Ref
    Google Scholar
    Google Scholar
    Google Scholar
    Google Scholar
    Google ScholarCross Ref
    Google ScholarCross Ref
    Google ScholarCross Ref
    Google ScholarCross Ref
    Google Scholar
    Google Scholar
    Google Scholar
    Google ScholarCross Ref
    Google Scholar
    Google ScholarCross Ref
    Google Scholar
    Google ScholarCross Ref
    Google Scholar
    Google ScholarDigital Library
    Google Scholar
    Google ScholarCross Ref
    Google ScholarCross Ref
    Google ScholarCross Ref
    Google Scholar
    Google ScholarCross Ref
    Google ScholarCross Ref
    Google ScholarCross Ref
    Google ScholarCross Ref
    Google ScholarCross Ref
    Google Scholar
    Google Scholar
    Google ScholarCross Ref
    Google ScholarCross Ref
    Google ScholarCross Ref
    Google Scholar
    Google Scholar
    Google Scholar
    Google Scholar
    Google Scholar
    Google Scholar
    Google Scholar
    Google ScholarCross Ref
    Google ScholarCross Ref
    Google Scholar
    Google Scholar
    Google ScholarCross Ref
    Google Scholar
    Google Scholar
    Google Scholar
    Google ScholarCross Ref
    Google ScholarCross Ref
    Google Scholar
    Google ScholarCross Ref
    Google Scholar
    Google ScholarCross Ref
    Google Scholar
    Google Scholar
    Google Scholar
    Google ScholarCross Ref
    Google Scholar
    Google ScholarCross Ref
    Google Scholar
    Google ScholarCross Ref
    Google ScholarCross Ref
    Google ScholarCross Ref
    Google Scholar
    Google ScholarCross Ref
    Google Scholar
    Google ScholarCross Ref
    Google ScholarCross Ref
    Google Scholar
    Google Scholar
    Google Scholar
    Google Scholar
    Google Scholar
    Google Scholar
    Google ScholarDigital Library
    Google Scholar
    Google ScholarDigital Library
    Google ScholarCross Ref
    Google ScholarCross Ref
    Google ScholarCross Ref
    Google Scholar
    Google ScholarCross Ref
    Google Scholar
    Google Scholar
    Google ScholarCross Ref
    Google Scholar
    Google Scholar
    Google ScholarCross Ref
    Google Scholar
    Google ScholarCross Ref
    Google ScholarCross Ref
    Google Scholar
    Google Scholar
    Google Scholar
    Google ScholarCross Ref
    Google Scholar
    Google Scholar
    Google ScholarCross Ref
    Google Scholar
    Google Scholar
    Google ScholarCross Ref
    Google ScholarCross Ref
    Google Scholar
    Google ScholarCross Ref
    Google Scholar
    Google Scholar
    Google ScholarCross Ref
    Google Scholar
    Google Scholar
    Google ScholarCross Ref
    Google Scholar
    Google Scholar
    Google Scholar
    Google Scholar
    Google Scholar
    Google Scholar
    Google Scholar
    Google Scholar
    Google Scholar
    Google Scholar
    Google Scholar
    Google Scholar
    Google ScholarCross Ref
    Google ScholarCross Ref
    Google ScholarCross Ref
    Google Scholar
    Google ScholarCross Ref
    Google ScholarCross Ref
    Google Scholar
    Google Scholar
    Google ScholarCross Ref
    Google Scholar
    Google Scholar
    Google Scholar
    Google Scholar
    Google Scholar
    Google Scholar
    Google ScholarCross Ref
    Google ScholarCross Ref
    Google Scholar
    Google ScholarCross Ref
    Google Scholar
    Google ScholarCross Ref
    Google ScholarCross Ref
    Google Scholar
    Google Scholar
    Google ScholarCross Ref
    Google ScholarCross Ref
    Google Scholar
    Google ScholarCross Ref
    Google ScholarCross Ref
    Google Scholar
    Google ScholarDigital Library

Index Terms

    Recent Advances in Natural Language Processing via Large Pre-Trained Language Models: A Survey

        Computing methodologies

            Artificial intelligence

                Natural language processing

Recommendations

    Introduction to Chinese Natural Language Processing
    Read More
    Deciphering natural language
    Read More
    Language independent, minimally supervised methods in natural language ambiguity resolution
    Read More

Comments

@article{Min2023Recent,
    author = {Min, Bonan and Ross, Hayley and Sulem, Elior and Veyseh, Amir Pouran Ben and Nguyen, Thien Huu and Sainz, Oscar and Agirre, Eneko and Heintz, Ilana and Roth, Dan},
    title = {Recent Advances in Natural Language Processing via Large Pre-Trained Language Models: A Survey},
    year = {2023},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    issn = {0360-0300},
    url = {https://doi.org/10.1145/3605943},
    doi = {10.1145/3605943},
    abstract = {Large, pre-trained language models (PLMs) such as BERT and GPT have drastically changed the Natural Language Processing (NLP) field. For numerous NLP tasks, approaches leveraging PLMs have achieved state-of-the-art performance. The key idea is to learn a generic, latent representation of language from a generic task once, then share it across disparate NLP tasks. Language modeling serves as the generic task, one with abundant self-supervised text available for extensive training. This article presents the key fundamental concepts of PLM architectures and a comprehensive view of the shift to PLM-driven NLP techniques. It surveys work applying the pre-training then fine-tuning, prompting, and text generation approaches. In addition, it discusses PLM limitations and suggested directions for future research.},
    journal = {ACM Computing Survey},
    month = {jun}
    }

%history work
@inproceedings{Mikolov2013Distributed,
  author    = {Tom{\'{a}}s Mikolov and
               Ilya Sutskever and
               Kai Chen and
               Gregory S. Corrado and
               Jeffrey Dean},
  title     = {Distributed Representations of Words and Phrases and their Compositionality},
  booktitle = {Proceedings of the Annual Conference on Neural Information Processing Systems},
  year      = {2013},
}

@inproceedings{Bengio2000Neural,
 author = {Bengio, Yoshua and Ducharme, R\'{e}jean and Vincent, Pascal},
 booktitle = {Proceedings of the Annual Conference on Neural Information Processing Systems},
 title = {A Neural Probabilistic Language Model},
 year = {2000}
}

@article{Collobert2011Natural,
  author  = {Ronan Collobert and Jason Weston and L{{\'e}}on Bottou and Michael Karlen and Koray Kavukcuoglu and Pavel Kuksa},
  title   = {Natural Language Processing (Almost) from Scratch},
  journal = {Journal of Machine Learning Research},
  year    = {2011}
}

@inproceedings{Sutskever2014Sequence,
 author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
 booktitle = {Proceedings of the Annual Conference on Neural Information Processing Systems},
 title = {Sequence to Sequence Learning with Neural Networks},
 year = {2014}
}

@inproceedings{Bahdanau2015Neural,
  author    = {Dzmitry Bahdanau and
               Kyunghyun Cho and
               Yoshua Bengio},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Neural Machine Translation by Jointly Learning to Align and Translate},
  booktitle = {Proceedings of the International Conference on Learning Representations {ICLR}},
  year      = {2015}
}

@inproceedings{Vaswani2017Attention,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Attention is All you Need},
 year = {2017}
}

@article{Radford2018Improving,
	title = {Improving Language Understanding by Generative Pre-Training},
	author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
	journal={OpenAI blog},
	year = {2018}
}

@article{Radford2019Language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  year={2019}
}

@inproceedings{Raffel2020Xxploring,
	title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
	booktitle = {Journal of Machine Learning Research},
	author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
	year = {2020},
}

%LLM's limitations
@article{Borji2023ACA,
  title={A Categorical Archive of ChatGPT Failures},
  author={Ali Borji},
  journal={ArXiv},
  year={2023},
  volume={abs/2302.03494}
}

@article{Tamkin2021UnderstandingTC,
  title={Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models},
  author={Alex Tamkin and Miles Brundage and Jack Clark and Deep Ganguli},
  journal={ArXiv},
  year={2021},
  volume={abs/2102.02503}
}

@article{Weidinger2021EthicalAS,
  title={Ethical and social risks of harm from Language Models},
  author={Laura Weidinger and John F. J. Mellor and Maribeth Rauh et al.},
  journal={ArXiv},
  year={2021},
  volume={abs/2112.04359}
}

@article{Kenton2021AlignmentOL,
  title={Alignment of Language Agents},
  author={Zachary Kenton and Tom Everitt and Laura Weidinger and Iason Gabriel and Vladimir Mikulik and Geoffrey Irving},
  journal={ArXiv},
  year={2021},
  volume={abs/2103.14659}
}

@article{Bommasani2021OnTO,
  title={On the Opportunities and Risks of Foundation Models},
  author={Rishi Bommasani and Drew A. Hudson and Ehsan Adeli et al.},
  journal={ArXiv},
  year={2021},
  volume={abs/2108.07258}
}


@article{Liu2019RoBERTaAR,
  title={RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
  journal={ArXiv},
  year={2019},
  volume={abs/1907.11692}
}

@article{Lan2019ALBERTAL,
  title={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
  author={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},
  journal={ArXiv},
  year={2019},
  volume={abs/1909.11942}
}

@inproceedings{spanish2020robert,
  author    = {MMG, MMG},
  title     = {Spanish RoBERTa},
  url       = {https://huggingface.co/MMG/mlm-spanish-roberta-base},
  year      = {2021}
}

@inproceedings{polish2020robert,
  author    = {Common Language Resources and Technology Infrastructure, Poland},
  title     = {Polish RoBERTa},
  url       = {https://huggingface.co/clarin-pl/roberta-polish-kgr10},
  year      = {2021}
}

@inproceedings{japanese2020robert,
  author    = {Wilson Wongso},
  title     = {Japanese RoBERTa},
  url       = {https://huggingface.co/w11wo/javanese-roberta-small},
  year      = {2021}
}

@inproceedings{hindi2020robert,
  author    = {Suraj Parmar},
  title     = {Hindi RoBERTa},
  url       = {https://huggingface.co/surajp/RoBERTa-hindi-guj-san},
  year      = {2021}
}

@inproceedings{swedish2020robert,
  author    = {Birger Moell},
  title     = {Swedish RoBERTa},
  url       = {https://huggingface.co/birgermoell/roberta-swedish-scandi},
  year      = {2021}
}

%mBART: liu-etal-2020-multilingual-denoising
%mT5: xue-etal-2021-mt5

%Multilingual Datasets and Evaluations

%%%joshi-etal-2020-state: The State and Fate of Linguistic Diversity and Inclusion in the NLP World
%%%kreutzer-etal-2022-quality: Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets
%%%blasi-etal-2022-systematic: Systematic Inequalities in Language Technology Performance across the World’s Languages
%%%artetxe-etal-2020-translation: Translation Artifacts in Cross-lingual Transfer Learning

%Multilingual Evaluation of ChatGPT

@article{Bang2023AMultitask,
  author    = {Yejin Bang and
               Samuel Cahyawijaya and
               Nayeon Lee and
               Wenliang Dai and
               Dan Su and
               Bryan Wilie and
               Holy Lovenia and
               Ziwei Ji and
               Tiezheng Yu and
               Willy Chung and
               Quyet V. Do and
               Yan Xu and
               Pascale Fung},
  title     = {A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning,
               Hallucination, and Interactivity},
  journal   = {ArXiv},
  year      = {2023},
  volume    = {abs/2302.04023},
}

@article{Kasai2023Evaluating,
      title={Evaluating GPT-4 and ChatGPT on Japanese Medical Licensing Examinations}, 
      author={Jungo Kasai and Yuhei Kasai and Keisuke Sakaguchi and Yutaro Yamada and Dragomir Radev},
      journal   = {ArXiv},
      year      = {2023},
      volume    = {abs/2303.18027}
}

@article{Fang2023ChatGPT,
      title={Is ChatGPT a Highly Fluent Grammatical Error Correction System? A Comprehensive Evaluation}, 
      author={Tao Fang and Shu Yang and Kaixin Lan and Derek F. Wong and Jinpeng Hu and Lidia S. Chao and Yue Zhang},
      journal   = {ArXiv},
      year      = {2023},
      volume    = {abs/2304.01746}
}

@article{Ahuja2023Mega,
      title={MEGA: Multilingual Evaluation of Generative AI}, 
      author={Kabir Ahuja and Rishav Hada and Millicent Ochieng and Prachi Jain and Harshita Diddee and Samuel Maina and Tanuja Ganu and Sameer Segal and Maxamed Axmed and Kalika Bali and Sunayana Sitaram},
      journal   = {ArXiv},
      year      = {2023},
      volume    = {abs/2303.12528}
}

%%%%armengol-estape-etal-2022-multilingual: On the Multilingual Capabilities of Very Large-Scale English Language Models

@InProceedings{Kojima2022LargeLM,
  title={Large Language Models are Zero-Shot Reasoners},
  author={Takeshi Kojima and Shixiang Shane Gu and Machel Reid and Yutaka Matsuo and Yusuke Iwasawa},
  booktitle={Proceedings of the Conference on Neural Information Processing Systems (NeurIPS)},
  year={2022}
}

%Evaluations for ChatGPT

%%
@article{Zhang2022HowWS,
  title={How would Stance Detection Techniques Evolve after the Launch of ChatGPT?},
  author={Bowen Zhang and Daijun Ding and Liwen Jing},
  journal={ArXiv},
  year={2022},
  volume={abs/2212.14548}
}

%%
@article{Kuzman2023ChatGPTBO,
  title={ChatGPT: Beginning of an End of Manual Linguistic Data Annotation? Use Case of Automatic Genre Identification},
  author={Taja Kuzman and Igor Mozetic and Nikola Ljubesic},
  journal={ArXiv},
  year={2023},
  volume={abs/2303.03953}
}

%%%
@article{Amin2023WillAC,
  title={Will Affective Computing Emerge from Foundation Models and General AI? A First Evaluation on ChatGPT},
  author={Mostafa Mirzaie Amin and E. Cambria and Bj{\"o}rn Schuller},
  journal={ArXiv},
  year={2023},
  volume={abs/2303.03186}
}

%%
@article{Gao2023ExploringTF,
  title={Exploring the Feasibility of ChatGPT for Event Extraction},
  author={Junfeng Gao and Huan Zhao and Changlong Yu and Ruifeng Xu},
  journal={ArXiv},
  year={2023},
  volume={abs/2303.03836}
}

%%
@article{Wei2023ZeroShotIE,
  title={Zero-Shot Information Extraction via Chatting with ChatGPT},
  author={Xiang Wei and Xingyu Cui and Ning Cheng and Xiaobin Wang and Xin Zhang and Shen Huang and Pengjun Xie and Jinan Xu and Yufeng Chen and Meishan Zhang and Yong Jiang and Wenjuan Han},
  journal={ArXiv},
  year={2023},
  volume={abs/2302.10205}
}

%%
@article{Omar2023ChatGP,
  title={ChatGPT versus Traditional Question Answering for Knowledge Graphs: Current Status and Future Directions Towards Knowledge Graph Chatbots},
  author={Reham Omar and Omij Mangukiya and Panos Kalnis and Essam Mansour},
  journal={ArXiv},
  year={2023},
  volume={abs/2302.06466}
}

%%
@article{Tan2023EvaluationOC,
  title={Evaluation of ChatGPT as a Question Answering System for Answering Complex Questions},
  author={Yiming Tan and Dehai Min and Y. Li and Wenbo Li and Na Hu and Yongrui Chen and Guilin Qi},
  journal={ArXiv},
  year={2023},
  volume={abs/2303.07992}
}

%%
@article{Kocmi2023Large,
      title={Large Language Models Are State-of-the-Art Evaluators of Translation Quality}, 
      author={Tom Kocmi and Christian Federmann},
      journal={ArXiv},
      year={2023},
      volume={2302.14520}
}

%%
@article{Jiao2023Chatgpt,
      title={Is ChatGPT A Good Translator? Yes With GPT-4 As The Engine}, 
      author={Wenxiang Jiao and Wenxuan Wang and Jen-tse Huang and Xing Wang and Zhaopeng Tu},
      journal={ArXiv},
      year={2023},
      volume={2301.08745}
}

%%
@article{Hendy2023Good,
      title={How Good Are GPT Models at Machine Translation? A Comprehensive Evaluation}, 
      author={Amr Hendy and Mohamed Abdelrehim and Amr Sharaf and Vikas Raunak and Mohamed Gabr and Hitokazu Matsushita and Young Jin Kim and Mohamed Afify and Hany Hassan Awadalla},
      journal={ArXiv},
      year={2023},
      volume={2302.09210}
}

%%
@article{Yang2023Exploring,
      title={Exploring the Limits of ChatGPT for Query or Aspect-based Text Summarization}, 
      author={Xianjun Yang and Yan Li and Xinlu Zhang and Haifeng Chen and Wei Cheng},
      journal={ArXiv},
      year={2023},
      volume={abs/2302.08081}
}

%%
@article{Wang2023Crosslingual,
      title={Cross-Lingual Summarization via ChatGPT}, 
      author={Jiaan Wang and Yunlong Liang and Fandong Meng and Zhixu Li and Jianfeng Qu and Jie Zhou},
      journal={ArXiv},
      year={2023},
      volume={abs/2302.14229}
}

%%%%
@article{Wang2023Robustness,
      title={On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective}, 
      author={Jindong Wang and Xixu Hu and Wenxin Hou and Hao Chen and Runkai Zheng and Yidong Wang and Linyi Yang and Haojun Huang and Wei Ye and Xiubo Geng and Binxin Jiao and Yue Zhang and Xing Xie},
      journal={ArXiv},
      year={2023},
      volume={abs/2302.12095}
}

%%%
@article{Jang2023Consistency,
      title={Consistency Analysis of ChatGPT}, 
      author={Myeongjun Jang and Thomas Lukasiewicz},
      journal={ArXiv},
      year={2023},
      volume={abs/2303.06273}
}

%%%
@article{Zhong2023CanChatGPT,
      title={Can ChatGPT Understand Too? A Comparative Study on ChatGPT and Fine-tuned BERT}, 
      author={Qihuang Zhong and Liang Ding and Juhua Liu and Bo Du and Dacheng Tao},
      journal={ArXiv},
      year={2023},
      volume={abs/2302.10198}
}

%%%
@article{Qin2023IsChatGPT,
  title={Is ChatGPT a General-Purpose Natural Language Processing Task Solver?},
  author={Chengwei Qin and Aston Zhang and Zhuosheng Zhang and Jiaao Chen and Michihiro Yasunaga and Diyi Yang},
  journal={ArXiv},
  year={2023},
  volume={abs/2302.06476}
}

%%%
@article{Kocon2023ChatGPT,
  title={ChatGPT: Jack of all trades, master of none},
  author={Jan Koco'n and Igor Cichecki and Oliwier Kaszyca and Mateusz Kochanek and Dominika Szydlo and Joanna Baran and Julita Bielaniewicz and Marcin Gruza and Arkadiusz Janz and Kamil Kanclerz and Anna Koco'n and Bartlomiej Koptyra and Wiktoria Mieleszczenko-Kowszewicz and P. Milkowski and Marcin Oleksy and Maciej Piasecki and Lukasz Radli'nski and Konrad Wojtasik and Stanislaw Wo'zniak and Przemyslaw Kazienko},
  journal={ArXiv},
  year={2023},
  volume={abs/2302.10724}
}

%LLMs

@article{Milmo2023ChatGPT,
  title={ChatGPT Reaches 100 Million Users},
  author={Dan Milmo},
  journal={ The Guardian},
  year={2023}
}


%LLMs
%%%GPT-3
@article{Rae2021ScalingLM,
  title={Scaling Language Models: Methods, Analysis \& Insights from Training Gopher},
  author={Jack Rae and Sebastian Borgeaud and et al.},
  journal={ArXiv},
  year={2021},
  volume={abs/2112.11446}
}

%%%Megatron
@article{Shoeybi2019MegatronLMTM,
  title={Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism},
  author={Mohammad Shoeybi and Mostofa Patwary and Raul Puri and Patrick LeGresley and Jared Casper and Bryan Catanzaro},
  journal={ArXiv},
  year={2019},
  volume={abs/1909.08053}
}

%%%GPT-Jurassic
@article{Lieber2021Jurassic,
  title={Jurassic-1: Technical Details and Evaluation},
  author={Opher Lieber and Or Sharir and Barak Lenz and Yoav Shoham},
  journal={White Paper. AI21 Labs.},
  year={2021}
}

%%%%OPT-175B
@article{Zhang2022OPTOP,
  title={OPT: Open Pre-trained Transformer Language Models},
  author={Susan Zhang and Stephen Roller and Naman Goyal and Mikel Artetxe and Moya Chen and Shuohui Chen and Christopher Dewan and Mona Diab and Xian Li and Xi Victoria Lin and Todor Mihaylov and Myle Ott and Sam Shleifer and Kurt Shuster and Daniel Simig and Punit Singh Koura and Anjali Sridhar and Tianlu Wang and Luke Zettlemoyer},
  journal={ArXiv},
  year={2022},
  volume={abs/2205.01068}
}



%Emergent abilities in LLMs

@article{Wei2022EmergentAO,
  title={Emergent Abilities of Large Language Models},
  author={Jason Wei and Yi Tay and Rishi Bommasani and Colin Raffel and Barret Zoph and Sebastian Borgeaud and Dani Yogatama and Maarten Bosma and Denny Zhou and Donald Metzler and Ed Huai-hsin Chi and Tatsunori Hashimoto and Oriol Vinyals and Percy Liang and Jeff Dean and William Fedus},
  journal={Transactions on Machine Learning Research},
  year={2022}
}

%%Zero-shot learning

@article{Brown2020LanguageMA,
  title={Language Models are Few-Shot Learners},
  author={Tom Brown and Benjamin Mann and et al.},
  journal={ArXiv},
  year={2020},
  volume={abs/2005.14165}
}

%ChatGPT to human level
@article{Guo2023HowCI,
  title={How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection},
  author={Biyang Guo and Xin Zhang and Ziyuan Wang and Minqi Jiang and Jinran Nie and Yuxuan Ding and Jianwei Yue and Yupeng Wu},
  journal={ArXiv},
  year={2023},
  volume={abs/2301.07597}
}

@article{Wang2023IsCA,
  title={Is ChatGPT a Good NLG Evaluator? A Preliminary Study},
  author={Jiaan Wang and Yunlong Liang and Fandong Meng and Haoxiang Shi and Zhixu Li and Jinan Xu and Jianfeng Qu and Jie Zhou},
  journal={ArXiv},
  year={2023},
  volume={abs/2303.04048}
}

%ChatGPT's social concerns
@article{Khalil2023WillCG,
  title={Will ChatGPT get you caught? Rethinking of Plagiarism Detection},
  author={M. Khalil and Erkan Er},
  journal={ArXiv},
  year={2023},
  volume={abs/2302.04335}
}

@article{Susnjak2022ChatGPTTE,
  title={ChatGPT: The End of Online Exam Integrity?},
  author={Teo Susnjak},
  journal={ArXiv},
  year={2022},
  volume={abs/2212.09292}
}

@article{Aydin2022OpenAI,
  title={OpenAI ChatGpt Generated Literature Review: Digital Twin in Healthcare},
  author={Omer Aydin and Enis Karaarslan},
  journal={Available at SSRN 4308687},
  year={2022}
}



@article{Kung2022Performance,
  title={Performance of ChatGPT on USMLE: Potential for AI-Assisted Medical Education Using Large Language Models},
  author={Kung, Tiffany H and Cheatham, Morgan and Medenilla, Arielle and Sillos, Czarina and De Leon, Lorie and Elepa{\~n}o, Camille and Madriaga, Maria and Aggabao, Rimel and Diaz-Candido, Giezel and Maningo, James and others},
  journal={medRxiv},
  year={2022},
  publisher={Cold Spring Harbor Laboratory Press}
}


@article{Choi2023ChatGPT,
  title={ChatGPT Goes to Law School},
  author={Jonathan Choi and Kristin Hickman and Amy Monahan and Daniel Schwarcz},
  journal={Available at SSRN},
  year={2023}
}

@article{Shen2023ChatGPT,
  title={ChatGPT and other Large Language Models are Double-edged Swords},
  author={Yiqiu Shen and Laura Heacock and Jonathan Elias and Keith D Hentel and Beatriu Reig and George Shih and Linda
Moy},
journal={Radiology},
  year={2023}
}

@article{Lanzi2023ChatGPTAO,
  title={ChatGPT and Other Large Language Models as Evolutionary Engines for Online Interactive Collaborative Game Design},
  author={Pier Luca Lanzi and Daniele Loiacono},
  journal={ArXiv},
  year={2023},
  volume={abs/2303.02155}
}

@article{Mahowald2023DissociatingLA,
  title={Dissociating language and thought in large language models: a cognitive perspective},
  author={Kyle Mahowald and Anna A. Ivanova and Idan Asher Blank and Nancy G. Kanwisher and Joshua B. Tenenbaum and Evelina Fedorenko},
  journal={ArXiv},
  year={2023},
  volume={abs/2301.06627}
}

@article{King2022Future,
  title={The Future of AI in Medicine: A Perspective from a Chatbot},
  author={Michael King},
  journal={Annals of Biomedical Engineering},
  year={2022}
}

@article{Jeblick2022ChatGPTMM,
  title={ChatGPT Makes Medicine Easy to Swallow: An Exploratory Case Study on Simplified Radiology Reports},
  author={Katharina Jeblick and Balthasar Maria Schachtner and Jakob Dexl and Andreas Mittermeier and Anna Theresa St{\"u}ber and Johanna Topalis and Tobias Weber and Philipp Wesp and Bastian Oliver Sabel and Jens Ricke and Michael Ingrisch},
  journal={ArXiv},
  year={2022},
  volume={abs/2212.14882}
}

%%%%multilingual models

%wu-dredze-2019-beto: Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of {BERT}
%wu-dredze-2020-languages: Are All Languages Created Equal in Multilingual {BERT}?
%pires-etal-2019-multilingual: How Multilingual is Multilingual BERT?

%kondratyuk-straka-2019-75: 75 Languages, 1 Model: Parsing Universal Dependencies Universally

%nguyen-etal-2021-trankit: trankit

%guzman-nateras-etal-2022-cross: Cross-Lingual Event Detection via Optimized Adversarial Training

%huang-etal-2019-unicoder: Unicoder: A Universal Language Encoder by Pre-training with Multiple Cross-lingual Tasks

%wu-etal-2022-learning: Learning Disentangled Semantic Representations for Zero-Shot Cross-Lingual Transfer in Multilingual Machine Reading Comprehension

@inproceedings{Karthikeyan:20,
  title={Cross-Lingual Ability of Multilingual BERT: An Empirical Study},
  author={K Karthikeyan and Zihan Wang and Stephen Mayhew and Dan Roth},
  booktitle = {Proceedings of the International Conference on Learning Representations (ICLR)},
  year={2020}
}

%%before LLMs: multilingual models
 @article{Meryem:19expanding,
  title={Expanding the Text Classification Toolbox with Cross-Lingual Embeddings},
  author={Meryem M'hamdi and Robert West and Andreea Hossmann and Michael Baeriswyl and C. Musat},
  journal={ArXiv},
  year={2019},
  volume={abs/1903.09878}
}

%mayhew-etal-2017-cheap: Cheap Translation for Cross-Lingual Named Entity Recognition

@InProceedings{Lample:18,
  title={Unsupervised Machine Translation Using Monolingual Corpora Only},
  author={Guillaume Lample and Ludovic Denoyer and Marc'Aurelio Ranzato},
  booktitle={Proceedings of the International Conference on Learning Representations (ICLR)},
  year={2018}
}

@article{Conneau:17,
  author = {Conneau, Alexis and Lample, Guillaume and Ranzato, Marc'Aurelio and Denoyer, Ludovic and Jégou, Hervé},
  journal = {CoRR abs/1710.04087},
  title = {Word Translation Without Parallel Data},
  year = {2017}
}

%zhang-etal-2016-ten: Ten Pairs to Tag – Multilingual POS Tagging via Coarse Mapping between Embeddings

%tackstrom-etal-2012-cross: Cross-lingual Word Clusters for Direct Transfer of Linguistic Structure

%joulin-etal-2018-loss: Loss in Translation: Learning Bilingual Word Mapping with a Retrieval Criterion

%kim-etal-2010-cross: A Cross-lingual Annotation Projection Approach for Relation Detection

%ni-florian-2019-neural: Neural Cross-Lingual Relation Extraction Based on Bilingual Word Embedding Mapping

%%%cross-lingual word embeddings

%upadhyay-etal-2016-cross: Cross-lingual Models of Word Embeddings: An Empirical Comparison

@article{Ruder:19,
  title={A Survey of Cross-lingual Word Embedding Models},
  author={Sebastian Ruder and Ivan Vulic and Anders S{\o}gaard},
  journal={Journal of Artificial Intelligence Research},
  year={2019},
  volume={65}
}

@article{Mikolov:13exploiting,
  title={Exploiting Similarities among Languages for Machine Translation},
  author={Tomas Mikolov and Quoc V. Le and Ilya Sutskever},
  journal={ArXiv},
  year={2013},
  volume={abs/1309.4168}
}

%%%%Multilingual datasets
@inproceedings{Nivre2016Universal,
    title = "{U}niversal {D}ependencies v1: A Multilingual Treebank Collection",
    author = "Nivre, Joakim  and
      de Marneffe, Marie-Catherine  and
      Ginter, Filip  and
      Goldberg, Yoav  and
      Haji{\v{c}}, Jan  and
      Manning, Christopher D.  and
      McDonald, Ryan  and
      Petrov, Slav  and
      Pyysalo, Sampo  and
      Silveira, Natalia  and
      Tsarfaty, Reut  and
      Zeman, Daniel",
    booktitle = "Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC)",
    year = "2016"
}

@inproceedings{Sang2002Introduction,
   author     = {Sang, Erik F. Tjong Kim and Meulder, Fien De},
   title      = {Introduction to the CoNLL-2002 Shared Task: Language-Independent Named Entity Recognition},
   booktitle  = {Proceedings of the Conference on Computational Natural Language Learning (CoNLL)},
   year       = {2002},
}

@inproceedings{Sang2003Introduction,
   author     = {Sang, Erik F. Tjong Kim and Meulder, Fien De},
   title      = {Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition},
   booktitle  = {Proceedings of the Conference on Computational Natural Language Learning (CoNLL)},
   year       = {2003},
}

%conneau-etal-2018-xnli: XNLI: Evaluating Cross-lingual Sentence Representations

@inproceedings{Strassel2016LORELEI,
    title = "{LORELEI} Language Packs: Data, Tools, and Resources for Technology Development in Low Resource Languages",
    author = "Strassel, Stephanie  and
      Tracey, Jennifer",
    booktitle = "Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC)",
    year = "2016"
}

%liang-etal-2020-xglue: XGLUE: A New Benchmark Dataset for Cross-lingual Pre-training, Understanding and Generation

@inproceedings{Hu2020XTREME,
    title = {{XTREME}: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalisation},
    author = {Junjie Hu and Sebastian Ruder and Aditya Siddhant and Graham Neubig and Orhan Firat and Melvin Johnson},
    booktitle = {Proceedings of the International Conference on Machine Learning (ICML)},
    year = {2020}
}

%zhang-etal-2021-mr: Mr. TyDi: A Multi-lingual Benchmark for Dense Retrieval
%perez-beltrachini-lapata-2021-models: Models and Datasets for Cross-Lingual Summarisation
%artetxe-etal-2020-cross: On the Cross-lingual Transferability of Monolingual Representations


%%% Summarization
%hasan-etal-2021-xl: XL-Sum: Large-Scale Multilingual Abstractive Summarization for 44 Languages

@article{Aharoni2022mFACEMS,
  title={mFACE: Multilingual Summarization with Factual Consistency Evaluation},
  author={Roee Aharoni and Shashi Narayan and Joshua Maynez and Jonathan Herzig and Elizabeth Clark and Mirella Lapata},
  journal={ArXiv},
  year={2022},
  volume={abs/2212.10622}
}


%%% Information Extracition

@inproceedings{Liang2020XGLUE,
	doi = {10.18653/v1/2020.emnlp-main.484},
	year = 2020,
	publisher = {Association for Computational Linguistics},
	author = {Yaobo Liang and Nan Duan and Yeyun Gong and Ning Wu and Fenfei Guo and Weizhen Qi and Ming Gong and Linjun Shou and Daxin Jiang and Guihong Cao and Xiaodong Fan and Ruofei Zhang and Rahul Agrawal and Edward Cui and Sining Wei and Taroon Bharti and Ying Qiao and Jiun-Hung Chen and Winnie Wu and Shuguang Liu and Fan Yang and Daniel Campos and Rangan Majumder and Ming Zhou},
	title = {{XGLUE}: A New Benchmark Datasetfor Cross-lingual Pre-training, Understanding and Generation},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ({EMNLP})}
}

@article{Zeman2020UD,
  title={Universal dependencies 2.5},
  author={Zeman, Daniel and Nivre, Joakim and Abrams, Mitchell and Ackermann, Elia and Aepli, No{\"e}mi and Aghaei, Hamid and Ziane, R},
  journal={LINDAT/CLARIAHCZ digital library at the Institute of Formal and Applied Linguistics (UFAL), Faculty of Mathematics and Physics, Charles University. url: http://hdl. handle. net/11234/1-3226},
  year={2020}
}

%SOTA NER

%SOTA POS

%SOTA REL
