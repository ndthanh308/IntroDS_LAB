\documentclass{article}

\usepackage{iclr2024_conference}

\iclrfinalcopy

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

\usepackage{times}
\usepackage{amsmath, amssymb, amsthm, bm}
\usepackage{multirow}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{makecell}
\usepackage{array}
\usepackage{url}
\usepackage{dsfont}
\usepackage{cleveref}
\usepackage{csquotes}
\usepackage{cancel}
\usepackage{booktabs}
\usepackage{array}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{enumitem}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{cleveref}
\usepackage{thmtools}
\usepackage{comment}
\usepackage{thm-restate}
\usepackage{bbm}
\input{math_commands}
\input{symbols}
\usepackage{textcomp}

\usepackage{xcolor}
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}


\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{example}[theorem]{Example}
\crefname{claim}{claim}{claims}
\crefname{conjecture}{conjecture}{conjectures}
\crefname{assumption}{assumption}{assumptions}
\crefname{condition}{condition}{conditions}
\newcommand{\xinit}{x_{\mathrm{init}}}

\usepackage{authblk}

\title{The Marginal Value of Momentum \\ for Small Learning Rate SGD}

\author{\textbf{Runzhe Wang$^1$, Sadhika Malladi$^{1}$, Tianhao Wang$^{2}$, Kaifeng Lyu$^1$, Zhiyuan Li$^{34}$}\\
$^1$Princeton University, $^2$Yale University, $^3$Stanford University,\\ $^4$Toyota Technological Institute at Chicago\\
\texttt{\{runzhew,smalladi,klyu\}@princeton.edu},\\ \texttt{tianhao.wang@yale.edu, zhiyuanli@ttic.edu}
}
\begin{document}

\maketitle

\input{sections-ICLR/abstract.tex}
\input{sections-ICLR/intro.tex}
\input{sections-ICLR/preliminaries.tex}
\input{sections-ICLR/results.tex}
\input{sections-ICLR/experiments}
\section{Conclusions}
    This work provides theoretical characterizations of the role of momentum in stochastic gradient methods. We formally show that momentum does not introduce optimization and generalization benefits when the learning rates are small, and we further exhibit empirically that the value of momentum is marginal for gradient-noise-dominated learning settings with practical learning rate scales. Hence we conclude that momentum does not provide a significant performance boost in the above cases. Our results further suggest that model performance is agnostic to the choice of momentum parameters over a range of hyperparameter scales.
\bibliography{reference}
\bibliographystyle{plainnat}

\appendix

\input{sections-ICLR/related_work.tex}
\input{sections-ICLR/appendix}
\input{sections-ICLR/appendix_exps}
\end{document}
