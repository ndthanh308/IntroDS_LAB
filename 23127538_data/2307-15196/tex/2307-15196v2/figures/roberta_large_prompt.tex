



\begin{table*}[h]
\centering
\resizebox{0.8\textwidth}{!}{
    \setlength{\tabcolsep}{0.3cm}
    \begin{tabular}{lccccccc}
    \toprule
     Task  &  \multicolumn{1}{c}{\textbf{SST-2}} &\multicolumn{1}{c}{\textbf{SST-5}} & \multicolumn{1}{c}{\textbf{SNLI}} & \multicolumn{1}{c}{\textbf{TREC}} & \multicolumn{1}{c}{\textbf{MNLI}} \\
    \midrule
    Zero-shot & 79.0 & 35.5 & 50.2  & 51.4 & 48.8  \\
    SGD & 93.1 (0.9) & 54.9 (0.7) & 87.9 (0.8) & 97.0 (0.2) & 82.4 (1.4) \\
    SGDM & 93.1 (0.4) & 55.3 (0.9) & 87.3 (0.5) & 96.8 (0.7) & 82.8 (1.4) \\
    

    \bottomrule
    \end{tabular}}
    \caption{
        SGD and SGDM for fine-tuning RoBERTa-large on $5$ tasks using $512$ examples from each class~\citep{gao-etal-2021-making,malladi2023kernelbased}. We use the simple task-specific prompt introduced in~\citet{gao-etal-2021-making}. Results are averaged over $5$ random subsets of the full dataset. These findings confirm that SGD and SGDM approximate each other in noisy settings.
    }
    \label{tab:lmft_prompt}
\end{table*}