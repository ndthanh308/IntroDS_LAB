\section{Experiments}\label{sec:exps}
In the previous sections, we conclude in theory that SGDM and SGD have similar performances within noisy short-horizon or general long-horizon training. While our theoretical results mostly work for learning rates that are asymptotically small, in this section, we verify that momentum indeed has limited benefits in practical training regimes where the optimal learning rate is finite but not very large. We defer some of the additional details of this section to the appendix. 
\subsection{Momentum may indeed have marginal value in practice}

\textbf{ImageNet Experiments.} 
First, we train ResNet-50 on ImageNet across batch sizes. Following the experimental setup in \citet{goyal2017accurate}, we use a learning rate schedule that starts with a 5-epoch linear warmup to the peak learning rate and decays it at epoch \#30, \#60, \#80. For SGDM~\eqref{equ:SGDM-intro}, we use the default value of $\beta = 0.9$, and grid search for the best learning rate $\gamma$ over $0.1 \times 2^k$ ($k \in \Z$). Then we check whether vanilla SGD with learning rate $\frac{\gamma}{1-\beta}$ can achieve the same performance as SGDM. Consistent with previous empirical studies~\citep{shallue2019measuring,smith2020generalization}, we observed that for training with smaller batch sizes, the optimal learning rate of SGDM is small enough so that SGD can perform comparably, though SGDM can indeed outperform SGD at larger batch sizes.

\input{figures/imagenet_figure}

\textbf{Language Model Experiments.}
In fine-tuning a pre-trained model, a small learning rate is also preferable to retain the model's knowledge learned during pre-training. Indeed, we observe that SGD and SGDM behave similarly in this case.
We fine-tune RoBERTa-large~\citep{liu2019roberta} on $5$ diverse tasks (SST-2~\citep{socher2013recursive_sst-2}, SST-5~\citep{socher2013recursive_sst-2}, SNLI~\citep{bowman2015large}, TREC~\citep{voorhees2000building_trec}, and MNLI~\citep{williams2018broad_mnli}) using SGD and SGDM.
We follow the few shot setting described in~\citep{gao-etal-2021-making,malladi2023kernelbased}, using a grid for SGD based on~\citep{malladi2023kernelbased} and sampling $512$ examples per class (\Cref{tab:lmft}). Additional settings and trajectories are in \Cref{app_sec:exps}.
\input{figures/roberta_large_no_prompt}

\subsection{Investigating the benefit of momentum in large-batch training}
The ImageNet experiments demonstrate that momentum indeed offers benefits in large-batch training when the optimal learning rate is relatively large. We now use large-batch experiments on CIFAR-10 to provide empirical evidence that this benefit is not due to the noise reduction effect and is marginal when SGD is well-approximated by its SDE. To do this we apply SVAG \citep{li2021validity} to control the noise scale in the gradient oracle in both SGDM and SGD updates.
\begin{definition}[SVAG]\label{def:svag}
    With any $\ell>0$, SVAG transforms the NGOS $\cG_\sigma=(f, \mSigma, \DatZ_{\sigma})$  (\Cref{def:NGOS}) into another NGOS $\hat\cG_{\sqrt{\ell}\sigma} = (f, \mSigma, \hat\DatZ_{\sqrt{\ell}\sigma})$ with scale $\sqrt{\ell}\sigma$. For an input $\vtheta$, $\hat\cG_{\ell\sigma}$ returns $\hat\vg = r_1(\ell)\vg_1 + r_2(\ell)\vg_2$ where $\vg_1,\vg_2\sim\cG_\sigma(\vtheta)$ and $r_i(\ell) = \frac 12 (1 + (-1)^i\sqrt{2\ell-1})$. $\hat\DatZ_{\sqrt{\ell}\sigma}$ is defined to ensure $\hat\vg$ has the same distribution as $\nabla f(\vtheta) + \sqrt{\ell}\sigma\vz$ when $\vz\sim\hat\DatZ_{\sqrt{\ell}\sigma}(\vtheta)$. 
\end{definition}
In our experiments, given an original SGD run with learning rate $\eta$ for $K$ steps, we perform a new SGD run with learning rate $\eta/\ell$, SVAG$(\ell)$ as the gradient oracle, for $K\ell$ steps.
The new SGD trajectory will be closer to its SDE approximation as $\ell$ gets larger, and converge to its SDE as $\ell \to +\infty$~\citep{li2021validity}.
We also apply the same modifications to SGDM runs ($\beta$ the momentum decay factor is unmodified).
In another view, applying this modification makes the total accumulated noise-induced impact and descent force (\Cref{lem:descent}) qualitatively stay on the same scale, while the accumulated curvature-induced impact is reduced by a factor of $\ell$. 

We train a ResNet-32~\citep{he2016deep} on CIFAR-10~\citep{cifar10} with batch size $B=512$. We first grid search to find the best learning rate for the standard SGDM ($\ell=1$), and then we perform SGD and SGDM with that same learning rate ($\bar{\eta}_k=\eta_k=\eta$ in the formulation \Cref{def:sgd,dqu:sgdm-1}). Then we modify both processes with different $\ell$ values. The results are summarized in \Cref{fig:svag}. We observe that while standard SGDM outperforms standard SGD, when we increase the value of $\ell$, the two trajectories become closer until SGDM has no evident edge over SGD. The finding corroborates that the benefit of adding momentum is mostly due to the alleviation of curvature-induced impacts, but will be marginal in general small-batch or small learning-rate settings when SGD is well-approximated by SDE. 
\input{figures/svag_figure}