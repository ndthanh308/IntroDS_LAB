In this section we provide the proof for \Cref{thm:weak-appro-main}. Specifically, when $\alpha\in[0,1)$ is the index for the control of the learning rate schedule (\Cref{def:hpschedule}), we hope to show that $\vy_k$ is close to a SGD trajectory $\vz_k$, defined by the following for $k\geq 0$:
\begin{align}
    &\vz_0 = \vx_0\\
    &\vh_{k} \sim \cG_\sigma(\vz_{k})\\
    &\vz_{k+1} =\vz_{k}-\bar{\eta}_{k}\vh_{k}
\end{align}

Specifically, we use $\vy_k$ as a bridge for connecting $\vx_k$ and $\vz_k$. Our proof consists of two steps.
\begin{enumerate}
    \item We show that $\vx_k$ and $\vy_k$ are order-$(1-\alpha)/2$ weak approximations of each other.  
    \item We show that $\vz_k$ and $\vy_k$ are order-$(1-\alpha)/2$ weak approximations of each other. 
\end{enumerate}
Then we can conclude that $\vx_k$ and $\vz_k$ are order-$(1-\alpha)/2$ weak approximations of each other, which states our main theorem \Cref{thm:weak-appro-main}.

First, we give a control of the averaged learning rate $\bar{\eta}_k$ (\Cref{equ:averaged-lr}) with the following lemma.
\begin{lemma}
    Let $(\eta_k,\beta_k)$ be a learning rate schedule scaled by $\eta$ (\Cref{def:hpschedule}) with index $\alpha$, and $\bar{\eta}_k$ be the averaged learning rate (\Cref{equ:averaged-lr}), there is 
    \begin{align*}
        & \bar{\eta}_k\leq \frac{\lambda_{\max}\eta_{\max}}{\lambda_{\min}}\eta\\
        & \frac{\bar{\eta}_k\beta_k}{1-\beta_k}\leq \frac{\lambda_{\max}\eta_{\max}}{\lambda^2_{\min}}\eta^{1-\alpha}.\\
    \end{align*}
\end{lemma}
\begin{proof}
    By \Cref{def:hpschedule} we know $\beta_k \in [1-\lambda_{\max}\eta^\alpha,1-\lambda_{\min}\eta^\alpha]$ and $\eta_k\leq \eta_{\max}\eta$. Therefore
    \begin{align*}
      \bar{\eta}_k&=\sum_{s=k}^\infty \eta_s  \beta_{k+1:s}(1-\beta_{k})\\
      & \leq \sum_{s=k}^\infty \eta_{\max}\eta(1-\lambda_{\min}\eta^\alpha)^{s-k} \lambda_{\max}\eta^\alpha\\
      & = \frac{\lambda_{\max}\eta_{\max}}{\lambda_{\min}}\eta.\\
      \frac{\bar{\eta}_k\beta_k}{1-\beta_k} & \leq \frac{\lambda_{\max}\eta_{\max}}{\lambda_{\min}}\eta\cdot \frac{1}{1-\beta_k}\\
      & \leq \frac{\lambda_{\max}\eta_{\max}}{\lambda^2_{\min}}\eta^{1-\alpha}.
    \end{align*}
\end{proof}
\subsubsection{Step 1}
For the first step we are going to proof the following \Cref{thm:weakappro-1}. For notation simplicity we omit the superscript $\eta$ for the scaling when there is no ambiguity.
\begin{theorem}[Weak Approximation of Coupled Trajectory]\label{thm:weakappro-1}
    With the learning rate schedule $(\eta_k,\beta_k)$ scaled by $\eta$ with index $\alpha\in [0,1)$. Assume that the NGOS $\cG_\sigma$ satisfy \Cref{assume:ngos} and the initialization $\vm_0$ satisfy \Cref{ass:init-bound}. For any noise scale $\sigma\leq \eta^{-1/2}$, let $(\vx_k,\vm_k)$ be the SGDM trajectory and $\vy_k$ be the corresponding coupled trajectory (\Cref{equ:coupled-traj}). 
	Then, the coupled trajectory $\vy_k$ is an order-$\gamma$ weak approximation (\Cref{def:weak_approx}) to $\vx_k$ with $\gamma = (1-\alpha)/2$.
	\label{thm:coupled_weak_approx}
\end{theorem}

For notations, let $\nabla_k = \nabla\Loss(\vx_k)$ so $\vg_k = \nabla_k + \sigma\vv_k$ is the stochastic gradient sampled from the NGOS. Also by \Cref{assume:ngos}, as $\mSigma^{1/2}$ is bounded, let $C_{\tr}>0$ be the constant that $\tr\mSigma(\vx)\leq C_{\tr}$ for all $\vx\in\R^d$. As $\nabla\Loss$ is Lipschitz, let $L>0$ be the constant that $\norm{\nabla\Loss(\vx)-\nabla\Loss(\vy)}\leq L\norm{\vx-\vy}$ for all $\vx,\vy\in\R^d$.  As $\mSigma^{1/2}$ is Lipschitz and bounded, let $L_{\Sigma}>0$ be the constant that $|\tr(\mSigma(\vx)-\mSigma(\vy))|\leq L_{\Sigma}\norm{\vx-\vy}$ for all $\vx,\vy\in\R^d$.

To show the result, let $h$ be a test function for the weak approximation, then for some $\vz$ between $\vx_k$ and $\vy_k$, $|\E h(\vx_k) - \E h(\vy_k)| = \E[|\langle \nabla h(\vz), \vx_k-\vy_k \rangle|] \leq \E[\|\nabla h(\vz) \| \|\vx_k-\vy_k\|] = \E[\frac{\eta_k\beta_k}{1-\beta_k}\|\nabla h(\vz) \| \|\vm_k\|]$. Thus it suffices to bound $\E\norm{\vx_k}^{2m}$, $\E\norm{\vy_k}^{2m}$  and $\E\norm{\vm_k}$ for any $m\geq 0$ and $k=O(\eta^{-1})$. As the gradient noises are scaled with variance $O(\sigma^2) = O(\eta^{-1})$, there will be $\E\norm{\vm_k}\gg 1$, and we will show that $\E\norm{\vm_k}=O(\eta^{(\alpha-1)/2})$ is the correct scale so we still have $\E\norm{\vx_k}^{2m}=O(1)$ and $|\E h(\vx_k) - \E h(\vy_k)|=O(\eta^{(1-\alpha)/2})$. An useful inequality we will use often in our proof is the Gr\"{o}nwall's inequality.
\begin{lemma}[Gr\"{o}nwall's Inequality \citep{gronwall1919note}]\label{lem:gronwall-discrete}
    For non-negative sequences $\{f_i,g_i,k_i\in\R\}_{i\geq 0}$, if for all $t>0$
    \[f_t\leq g_t + \sum_{s=0}^{t-1} k_s f_s\]
    then $f_t\leq g_t + \sum_{s=0}^{t-1} k_sg_s \exp(\sum_{r=s+1}^{t-1} k_r)$.
\end{lemma}

Next follows the proofs.

\begin{lemma} 
	We can bound the expected squared norm \begin{align}\E\norm{\vm_k}^2  \leq 12\frac{\lambda^2_{\max}}{\lambda^2_{\min}}\sup_{\tau=1,...,k}\E(\|\nabla_\tau\|)^2 + (4+8\frac{L\eta_{\max}}{\lambda_{\min }}+\eta^{\alpha-1})K_0\label{equ:expe-norm-m}\end{align}
	\label{lem:exp_norm_m}
    with constant $K_0 = \frac{2\lambda^2_{\max}}{\lambda_{\min}}C_{\tr} + 3\E \norm{\vm_0}^2\leq\frac{2\lambda^2_{\max}}{\lambda_{\min}}C_{\tr}+3C_2$ ($C_2$ by \Cref{ass:init-bound}), for a small enough learning rate $\eta<\left(\frac{\lambda_{\min }}{4L\eta_{\max}}\right)^{\frac{1}{1-\alpha}}$. 
\end{lemma}

\begin{proof}

To bound $\E\norm{\vm_k}^2$, we unroll the momentum by \Cref{lem:unrollm} and write

\begin{align}
	 \vm_k & =\beta_{0:k-1} \vm_0 + \sum_{s=0}^{k-1}\beta_{s+1:k-1}(1-\beta_{s}) \vg_{s}\label{eq:unrolled_momentum} 
\end{align}

Define
\begin{align*}
	\tilde{\vm}_k &= \beta_{0:k-1} \vm_0 + \sum_{s=0}^{k-1}\beta_{s+1:k-1}(1-\beta_{s})\nabla_s.\label{eq:unrolled_momentum-2} 
\end{align*}
Then $\E \vm_k = \E \tilde{\vm}_k$, and
\begin{align*}
	\vm_k-\tilde{\vm}_k &= \sum_{s=0}^{k-1}\beta_{s+1:k-1}(1-\beta_{s})\sigma\vv_{s}
\end{align*}


Let constant $K_0 =2C_{\tr}+3C_2$, we can write
\begin{align}
    \E\norm{\vm_k}^2&\leq \E(\norm{\tilde{\vm}_k}+\norm{\vm_k-\tilde{\vm}_k})^2\\
    & \leq 2\E\norm{\tilde{\vm}_k}^2+ 2\E\norm{\vm_k-\tilde{\vm}_k}^2\\
    & = 2\E \norm{\tilde{\vm}_k}^2+ 2 \sum_{s=0}^{k-1}(\beta_{s+1:k-1})^2(1-\beta_{s})^2 \sigma^2\E \tr\mSigma(\vx_s)\\
    & \leq 2\E \norm{\tilde{\vm}_k}^2+ 2\sum_{s=0}^{k-1} (1-\lambda_{\min}\eta^\alpha)^{2(k-s-1)}\lambda^2_{\max}\eta^{2\alpha} \sigma^2\E \tr\mSigma(\vx_s)\\
    & \leq 2\E \norm{\tilde{\vm}_k}^2+ \eta^{\alpha-1}\frac{2\lambda^2_{\max}}{\lambda_{\min}}C_{\tr}\\
    & \leq 2\E \norm{\tilde{\vm}_k}^2+ \eta^{\alpha-1}K_0.
    \label{eq:exp_mnorm_sq}
\end{align}

On the other hand, we know
\begin{align*}
    \tilde{\vm}_k &= \beta_{0:k-1} \vm_0 + \sum_{s=0}^{k-1}\beta_{s+1:k-1}(1-\beta_{s})\nabla_s
\end{align*}
We use the Lipschitzness of $\nabla$ to write 
\begin{align*}
	\|\nabla_s - \nabla_k\| & \leq L\|\vx_s - \vx_k\|\leq L\sum_{\tau=s}^{k-1}\eta_\tau\norm{\vm_{\tau+1}},  	
\end{align*}
then we can write
\begin{align*}
 \E \norm{\tilde{\vm}_k}^2 & \leq \E\left(  \beta_{0:k-1} \norm{\vm_0} + \sum_{s=0}^{k-1}\beta_{s+1:k-1}(1-\beta_{s})(\|\nabla_k\| + L\sum_{\tau=s}^{k-1}\eta_\tau\norm{\vm_{\tau+1}})\right)^2 \\
 &  \leq \E\left( \norm{\vm_0} +  \frac{\lambda_{\max}}{\lambda_{\min}}\|\nabla_k\| + L\eta\eta_{\max}\sum_{\tau=0}^{k-1}(1-\lambda_{\min}\eta^{\alpha})^{k-1-\tau}
 \|\vm_{\tau+1}\|\right)^2\\
 & \leq  3\E (\norm{\vm_0})^2 +  3\frac{\lambda^2_{\max}}{\lambda^2_{\min}}\E(\|\nabla_k\|)^2 + 3L^2\eta^2\eta_{\max}^2\left(\sum_{\tau=0}^{k-1}(1-\lambda_{\min}\eta^{\alpha})^{k-1-\tau}\right)\cdot\\&\qquad\left(\sum_{\tau=0}^{k-1}(1-\lambda_{\min}\eta^{\alpha})^{k-1-\tau}\E\|\vm_{\tau+1}\|^2\right)\\
 & \leq K_0+3\frac{\lambda^2_{\max}}{\lambda^2_{\min}}\E(\|\nabla_k\|)^2 + \frac{3L^2\eta_{\max}^2\eta^{2-2\alpha}}{\lambda_{\min }^2}\sup_{\tau=1\cdots {k}}\E\|\vm_\tau\|^2.
\end{align*}
Then by \Cref{eq:exp_mnorm_sq}, we know 
\[\sup_{\tau=1,...,k} \E\|\vm_\tau\|^2\leq2\sup_{\tau=1,...,k}\E \norm{\tilde{\vm}_\tau}^2+ \eta^{\alpha-1}K_0,\] so
\begin{align}
\sup_{\tau=1,...,k} \E\norm{\tilde{\vm}_\tau}^2 & \leq K_0+3\frac{\lambda^2_{\max}}{\lambda^2_{\min}}\sup_{\tau=1,...,k}\E(\|\nabla_\tau\|)^2 + \frac{6L^2\eta_{\max}^2\eta^{1-\alpha}}{\lambda_{\min }^2}K_0\\
&\qquad + \frac{6L^2\eta_{\max}^2\eta^{2-2\alpha}}{\lambda_{\min }^2}\sup_{\tau=1,\cdots, {k}}\E\|\tilde{\vm}_\tau\|^2
\end{align}
Choose $\eta$ small enough so that $\frac{6L^2\eta_{\max}^2\eta^{2-2\alpha}}{\lambda_{\min }^2}<\frac{1}{2}$, we know
\begin{align}
\sup_{\tau=1,...,k} \E\norm{\tilde{\vm}_\tau}^2 & \leq 6\frac{\lambda^2_{\max}}{\lambda^2_{\min}}\sup_{\tau=1,...,k}\E(\|\nabla_\tau\|)^2 + (2+2\frac{\sqrt{3}L\eta_{\max}}{\lambda_{\min }})K_0.
\end{align}
So
\begin{align}
    \E\norm{\vm_k}^2 & \leq 2\E \norm{\tilde{\vm}_k}^2+ \eta^{\alpha-1}K_0 \\
    & \leq 12\frac{\lambda^2_{\max}}{\lambda^2_{\min}}\sup_{\tau=1,...,k}\E(\|\nabla_\tau\|)^2 + (4+8\frac{L\eta_{\max}}{\lambda_{\min }}+\eta^{\alpha-1})K_0.
\end{align}
\end{proof}

\begin{lemma}
There is a constant $f$ that depends on $T$, irrelevant to $\eta$, such that 
\[\sup_{k=0,...,\lfloor T/\eta \rfloor} \E\norm{\nabla_k}^2 \leq f(T)\]\label{lem:gradient_norm}
when $\eta<\min(\left(\frac{\lambda_{\min}^3}{12L\lambda_{\max}^2\eta_{\max}}\right)^{\frac{1}{1-\alpha}},1)$
\end{lemma} 
\begin{proof}
By the Lipschitzness of $\nabla\Loss$, we know
\begin{align}
\|\nabla_k\| & \leq \|\nabla_0\| + L\norm{\vx_{k}-\vx_0}\\
& \leq \|\nabla_0\| + L\norm{\vy_{k}-\vy_0} + L\norm{\vx_{0}-\vy_0} + L\norm{\vx_{k}-\vy_k} \label{eq:normnablak}
\end{align}
Observe that $\|\vx_k - \vy_k\| = \frac{\bar{\eta}_k\beta_k}{1-\beta_k}\|\vm_k\|\leq \frac{\lambda_{\max}\eta_{\max}}{\lambda_{\min}^2}\eta^{1-\alpha}\|\vm_k\|$. Let $d_k=\E\|\vy_k-\vy_0\|^2$. As a result, we can write
\begin{align}
\E\|\nabla_k\|^2 & \leq 3\E\left(\|\nabla_0\|  +\frac{L\lambda_{\max}\eta_{\max}}{\lambda_{\min}^2}\eta^{1-\alpha}\|\vm_0\|\right)^2+3\left(\frac{L\lambda_{\max}\eta_{\max}}{\lambda_{\min}^2}\eta^{1-\alpha}\right)^2\E(\|\vm_k\|^2) + 3L^2 d_k.\label{equ:grad-bound-1}
\end{align}
Choose $\eta$ such that $\eta^{1-\alpha}<\frac{\lambda_{\min}^3}{12L\lambda_{\max}^2\eta_{\max}}$ and let the constant \begin{align}
    K_1 & =3\E\left(\|\nabla\Loss(\vx_0)\|  +\frac{\lambda_{\min}}{12\lambda_{\max}}\|\vm_0\|\right)^2 \\ &\qquad + \left(\frac{\lambda_{\min}}{6\lambda_{\max}}\right)^2(3+6\frac{L\eta_{\max}}{\lambda_{\min }})K_0+\left(\frac{L\eta_{\max}}{4\lambda_{\min}}\right)K_0
\end{align}
where recall $K_0$ is the constant from \Cref{lem:exp_norm_m}. Plug \Cref{equ:expe-norm-m} into \Cref{equ:grad-bound-1} gives
\begin{align}
\E\|\nabla_k\|^2 & \leq K_1 +\frac{1}{2}\sup_{\tau=1,...,k}\E(\|\nabla_\tau\|)^2+ 3L^2 d_k.
\end{align}
therefore 
\begin{align}\label{equ:lya-1}\E\|\nabla_k\|^2  \leq 2K_1 + 6L^2\sup_{\tau=1,...,k}  d_\tau.
\end{align}
Now we need to bound $d_k$ by iteration.
\begin{align}
d_{k+1} &= d_{k} +\E\|\vy_{k+1}-\vy_k\|^2 + 2\E(\vy_{k+1}-\vy_k)^\top(\vy_k-\vy_0)\\
&= d_k + \bar{\eta}_k^2 \E\norm{\nabla_k}^2 + \bar{\eta}_k^2\sigma^2 \E\tr(\mSigma(\vx_k)) + 2 \E(\nabla_k)^\top(\vy_k-\vy_0)\\
&\leq d_k + \frac{\lambda^2_{\max}}{\lambda^2_{\min}}\eta_{\max}^2\eta^2 \E\norm{\nabla_k}^2 + \frac{\lambda^2_{\max}}{\lambda^2_{\min}}\eta_{\max}^2\eta C_{\tr} + 2 \frac{\lambda_{\max}}{\lambda_{\min}}\eta_{\max}\eta\sqrt{d_k\E\norm{\nabla_k}^2}
\end{align}
Let function $K_2(\vm_0,\vx_0) = \frac{\lambda^2_{\max}}{\lambda^2_{\min}}\eta_{\max}^2 C_{\tr} +2\frac{\lambda^2_{\max}}{\lambda^2_{\min}}\eta_{\max}^2K_1(\vm_0,\vx_0) +2 \frac{\lambda_{\max}}{\lambda_{\min}}\eta_{\max}\frac{K_1(\vm_0,\vx_0)}{\sqrt{6}L}$, and $\bar{d}_k=\sup_{\tau=1,...,k} d_{\tau}$. We know
\begin{align}
\sqrt{d_k\E\norm{\nabla_k}^2} & \leq \sqrt{6L^2}d_k + \frac{K_1}{\sqrt{6L^2}},\\
\bar{d}_{k+1} &\leq \bar{d}_k + K_2\eta + 6L^2\frac{\lambda^2_{\max}}{\lambda^2_{\min}}\eta_{\max}^2\eta^2 \bar{d}_k +  2 \frac{\lambda_{\max}}{\lambda_{\min}}\eta_{\max}\eta(\sqrt{6}L\bar{d}_k)\\
& \leq (1+6L \frac{\lambda_{\max}\eta_{\max}}{\lambda_{\min}}\eta+6L^2\frac{\lambda^2_{\max}\eta_{\max}^2}{\lambda^2_{\min}}\eta^2)\bar{d}_k + \eta K_2
\end{align}
 Let $\kappa = 1+6L \frac{\lambda_{\max}\eta_{\max}}{\lambda_{\min}}\eta+6L^2\frac{\lambda^2_{\max}\eta_{\max}^2}{\lambda^2_{\min}}\eta^2$, then by the Gr\"{o}nwall's inequality (\Cref{lem:gronwall-1}), 
\begin{align}
\bar{d}_k   
    & \leq \eta K_2 (1 + \sum_{s=0}^{k-1} \kappa \exp^{\kappa s})\\
    & \leq K_2 + K_2 T \exp^{1+6L \frac{\lambda_{\max}\eta_{\max}}{\lambda_{\min}}T+6L^2\frac{\lambda^2_{\max}\eta_{\max}^2}{\lambda^2_{\min}}T}.
\end{align}
Plugging into \Cref{equ:lya-1} finished the proof.
\end{proof}
\begin{lemma}
    There is a constant $f$ that depends on $T$, irrelevant to $\eta$, such that 
    \[\E\norm{\vm_k}^2  \leq \eta^{\alpha-1} f(T)\]
    for all $k\leq \frac{T}{\eta}$.\label{lem:boun-mome}
\end{lemma}
\begin{proof}
    The result directly follows from \Cref{lem:exp_norm_m} and \Cref{lem:gradient_norm}.
\end{proof}
\begin{lemma}
    There exists function $g(m,T)$ that, for all $k\leq \frac{T}{\eta}$,
    \begin{align*} 
    	\E[(1+\|\vx_k\|^2)^{m}] &\leq g(m,T), \\
    	\E[(1+\|\vy_k\|^2)^{m}] &\leq g(m,T), \\
    	\E[\eta^{m}\|\vg_k\|^{2m}] &\leq g(m,T),\\
            \E[(1+\|\vy_k\|^2)^{m}\eta^{1-\alpha}\norm{\vm_k}^2] & \leq g(m,T)
    \end{align*}
	and that $g$ is irrelevant to $k$ and $\eta$. 
	\label{lem:small_higher_order}
\end{lemma}

\begin{proof}

    We use the fact that $(a+b)^m \leq 2^{m-1}(a^m + b^m)$ from the Jensen inequality for $a,b>0$ and $m\geq 1$. Furthermore by Young's inequality $a^\alpha b^{\beta}\leq \frac{\alpha a^{\alpha+\beta}+ \beta b^{\alpha+\beta}}{\alpha+\beta}\leq a^{\alpha+\beta}+b^{\alpha+\beta}$ for $\alpha,\beta>0$.
	\begin{align*}  
		\norm{\vg_k}^{2m} &\leq (\|\nabla_k\| + \sigma\|\vv_k\|)^{2m} \\
        & \leq 2^{2m-1} ( \|\nabla_k\|^{2m} + \eta^{-m}\|\vv_k\|^{2m})\\ 
		\|\nabla_k\|^{2m} &\leq (\|\nabla_0\| + L\|\vx_k - \vx_0\|)^{2m} \\
		&\leq 2^{2m-1}((\|\nabla_0\| + L\|\vx_0\|)^{2m} + L^{2m}\|\vx_k\|^{2m})
	\end{align*}
    As $\E\norm{\vv_k}^{2m}$ is bounded by \Cref{assume:ngos}, for some constant $R$ there is
    \begin{align}\label{moments-bound-1}
      \E \norm{\vg_k}^{2m}\leq R(1 + \eta^{-m})\E(1 + \norm{\vx_k}^{2m}).
    \end{align}
	Now we need to show the bounds on $\E[(1+\|\vx_k\|^2)^{m}] $ and $\E[(1+\|\vy_k\|^2)^{m}] $. Specifically we shall prove $\E[(1+\|\vy_k\|^2+\eta^{2-2\alpha}\|\vm_k\|^2)^{m}] \leq g(m,T)$ and $\E[(1+\|\vy_k\|^2+\eta^{2-2\alpha}\|\vm_k\|^2)^{m}\eta^{1-\alpha}\norm{\vm_k}^2] \leq g(m,T)$ for some function $g$. 

    Let 
    \begin{align*}
        \delta_k & = \bar{\eta}_k^2 \norm{\vg_{k}}^2 - 2\bar{\eta}_k \dotp{\vg_{k}}{\vy_{k}} 
         +\eta^{2-2\alpha}(\norm{\beta_k\vm_k+(1-\beta_k)\vg_k}^2-\norm{\vm_k}^2),
    \end{align*}
    then there is
	\begin{align*}	&\qquad\E[(1+\|\vy_{k+1}\|^2+\eta^{2-2\alpha}\|\vm_{k+1}\|^2)^m|\vy_{k},\vm_k]\\
   & = \E [(1+\|\vy_{k} - \bar{\eta}_k\vg_{k}\|^2+
   \eta^{2-2\alpha}\norm{\beta_k\vm_k+(1-\beta_k)\vg_k}^2)^m|\vy_{k},\vm_k]\\
        & = \E [(1+\|\vy_{k}\|^2+\eta^{2-2\alpha}\|\vm_k\|^2+\delta_k)^m|\vy_{k},\vm_k]\\
        &\leq (1+\norm{\vy_k}^2+\eta^{2-2\alpha}\|\vm_k\|^2)^m + m(1+\norm{\vy_k}^2+\eta^{2-2\alpha}\|\vm_k\|^2)^{m-1} \E[\delta_k|\vy_k,\vm_k] \\&+  \E[\delta_k^2\sum_{i=0}^{m-2} \binom{m}{i+2}\delta_k^{i}(1+\norm{\vy_k}^2+\eta^{2-2\alpha}\|\vm_k\|^2)^{m-2-i}|\vy_{k},\vm_k]\\
        &\leq (1+\norm{\vy_k}^2+\eta^{2-2\alpha}\|\vm_k\|^2)^m + m(1+\norm{\vy_k}^2+\eta^{2-2\alpha}\|\vm_k\|^2)^{m-1} \E[\delta_k|\vy_k,\vm_k] \\& +  2^m\E[\delta_k^2 
        (|\delta_k|^{m-2}+(1+\norm{\vy_k}^2+\eta^{2-2\alpha}\|\vm_k\|^2)^{m-2})|\vy_{k},\vm_k].
        \end{align*}
    Then there exists constant $K_3$ independent of $\eta$ such that
    \begin{align*}	
        \E[\delta_k|\vy_k,\vm_k] & =\bar{\eta}_k^2 \norm{\nabla_{k}}^2 + \bar{\eta}_k^2\sigma^2 \E_{|\vy_k,\vm_k}\tr\mSigma(\vx_k) - 2\bar{\eta}_k \dotp{\nabla_{k}}{\vy_{k}} 
         +\eta^{2-2\alpha}(\norm{\beta_k\vm_k+(1-\beta_k)\vg_k}^2-\norm{\vm_k}^2)\\
        & \leq \E_{|\vy_k,\vm_k}[\frac{\lambda^2_{\max}\eta^2_{\max}}{\lambda^2_{\min}}( 2\eta^2(\|\nabla_0\| + L\|\vx_0\|)^{2} + 2\eta^2L^{2}\|\vx_k\|^{2} + \eta C_{\tr})\\      
    &\quad +2\frac{\lambda_{\max}\eta_{\max}}{\lambda_{\min}}\eta (\|\nabla_0\| + L\|\vx_k\|+L\| \vx_0\|)\norm{\vy_k}\\
    &\quad + \eta^{2-2\alpha}(\frac{1-\beta_k}{1+\beta_k}\norm{\nabla_k}^2 + (1-\beta_k)^2\sigma^2\E\tr\mSigma(\vx_k))]\\
        & \leq (1+\E[\norm{\vx_k}^2|\vy_k,\vm_k]+\norm{\vy_k}^2)\eta K_3\\ 
    \end{align*}
    as $(1-\beta_k)\eta^{2-2\alpha}=O(\eta^{2-\alpha})=O(\eta)$ and $(1-\beta_k)^2\sigma^2\eta^{2-2\alpha}=O(\eta^{2\alpha -1 + 2-2\alpha})=O(\eta)$.
    
    Note that 
    \begin{align*}
        \norm{\vx_k}^2 & = \norm{\vy_k+\bar{\eta}_k\beta_k(1-\beta_k)^{-1}\vm_k}^2\\
        & \leq 2\norm{\vy_k}^2 + \frac{2\eta_{\max}^2\lambda_{\max}^2}{\lambda_{\max}^4}\eta^{2-2\alpha}\norm{\vm_k}^2
    \end{align*}
    Then we can write $\E[\delta_k|\vy_k,\vm_k]\leq K_4\eta(1+\norm{\vy_k}^2+\eta^{2-2\alpha}\norm{\vm_k}^2)$ for some constant $K_4$ independent of $\eta$. Futhermore, as $\E_{|\vy_k}\dotp{\vv_k}{\vy_k}=\E_{|\vy_k}\dotp{\vv_k}{\vm_k}=\E_{|\vy_k}\dotp{\vv_k}{\vx_k}=0$, expansion gives for some constant $K_4,K_5,K_6,K_7$,
    \begin{align*}
        \E[\delta_k^2|\vy_k] & \leq K_4 \E(\eta \norm{\vx_k}^2+\eta\norm{\vv_k}^2+\eta\norm{\vy_k}^2+\eta^{1/2}\dotp{\vv_k}{\vy_k}+\eta^{3/2-\alpha}\dotp{\vm_k}{\vv_k}+\eta^{3/2-\alpha}\dotp{\vx_k}{\vv_k})^2\\
        &\leq K_5 \eta(1+\norm{\vy_k}^2+\eta^{2-2\alpha}\norm{\vm_k}^2).\\
        \E[|\delta_k|^m|\vy_k] & =K_6 \E(\eta \norm{\vx_k}^2+\eta\norm{\vv_k}^2+\eta\norm{\vy_k}^2+\eta^{1/2}\dotp{\vv_k}{\vy_k}+\eta^{3/2-\alpha}\dotp{\vm_k}{\vv_k}+\eta^{3/2-\alpha}\dotp{\vx_k}{\vv_k})^m \\
        &\leq K_7 \eta^{m/2}(1+\norm{\vy_k}^2+\eta^{2-2\alpha}\norm{\vm_k}^2).
    \end{align*}
    Therefore taking expecation with respect to all, we know 
    \[\E[(1+\|\vy_{k+1}\|^2+\eta^{2-2\alpha}\|\vm_{k+1}\|^2)^m]\leq (1+mK_4\eta + 2^m(K_5+K_7)\eta)\E(1+\|\vy_{k}\|^2+\eta^{2-2\alpha}\|\vm_{k}\|^2)^m\]
    Therefore by the Gr\"{o}nwall's inequality (\Cref{lem:gronwall-1}),
    \begin{align*}\E(1+\|\vy_{k}\|^2+\eta^{2-2\alpha}\|\vm_{k}\|^2)^m & \leq e^{(mK_4\eta + 2^m(K_5+K_7)\eta)k} \E(1+\|\vy_{0}\|^2+\eta^{2-2\alpha}\|\vm_{0}\|^2)^m\\
    &\leq e^{mK_4T + 2^m(K_5+K_7)T}3^m(1+\E\|\vy_{0}\|^{2m}+\eta^{2-2\alpha}\E\|\vm_{0}\|^{2m}).
    \end{align*}
    And clearly $\E\|\vy_{0}\|^{2m}\leq 2^m \E\|\vx_{0}\|^{2m} + (\frac{2\lambda_{\max}\eta_{\max}}{\lambda_{\min}^2})^m \eta^{m-m\alpha}\E\|\vm_{0}\|^{2m}$. Therefore we finished bounding the moments of $\vy_k$. Similar induction arguments on $(1+\|\vy_k\|^2+\eta^{2-2\alpha}\|\vm_k\|^2)^{m}\eta^{1-\alpha}\norm{\vm_k}^2$ offer the last equation in the lemma.
    Then for $\vx_k$, we can write
	$$ \|\vx_{k}\|^{2m} \leq 2^{2m}\left(\|\vy_{k}\|^{2m} + (\frac{2\lambda_{\max}\eta_{\max}}{\lambda_{\min}^2})^m \eta^{m-m\alpha} \|\vm_{k}\|^{2m}\right) $$
    And for $\vg_k$ with \Cref{moments-bound-1} we are able to finish the proof.
\end{proof}
\begin{proof}[Proof for \Cref{thm:coupled_weak_approx}]
	We expand the weak approximation error for a single $k$. There is $\theta\in[0,1]$ and $\vz=\theta \vx_k+(1-
 \theta)\vy_k$ such that
	\begin{align*}
		|\E h(\vx_k) - \E h(\vy_k)| &=\E[|\langle \nabla h(\vz), \vx_k-\vy_k \rangle|] \\
		&\leq \E[\|\nabla h(\vz) \| \|\vx_k-\vy_k\|] \\
		&\leq \sqrt{\E[\|\nabla h(\vz) \|^2] \E[\|\vx_k-\vy_k\|^2]}.
	\end{align*} As $\nabla h(\vz)$ have polynomial growth, there is $k_1,k_2$ such that $$\norm{\nabla h(\vz)}^2\leq k_1 (1+\norm{\vz}^{k_2})\leq k_1 (1+\norm{\vx_k}^{k_2}+\norm{\vy_k}^{k_2}).$$ Combined with \Cref{lem:small_higher_order}, we see $\sqrt{\E[\|\nabla h(\vz) \|^2}$ is bounded by some constant $2k_1g(k_2,T)$.
	From the definition of the coupled trajectory, we can write
	\begin{align*}
		\E \|\vx_k - \vy_k \|^2 \leq \frac{\lambda^2_{\max}\eta^2_{\max}}{\lambda^4_{\min}}\eta^{2-2\alpha} \E \|\vm_k\|^2	
	\end{align*}
	Then as \Cref{lem:boun-mome}, there is constant $f$ that $\E\norm{\vm_k}^2  \leq \eta^{\alpha-1} f(T)$, therefore
	$$|\E h(\vx_k) - \E h(\vy_k)| \leq 2k_1\eta^{(1-\alpha)/2} \frac{\lambda_{\max}\eta_{\max}}{\lambda^2_{\min}}f(T)g(k_2,T).$$
 Thus by definition, $\vx_k$ and $\vy_k$ are order-$(1-\alpha)/2$ weak approximations of each other.
\end{proof}
\subsubsection{Step 2}
In this section we are comparing the trajectory $\vy_k$ with a SGD trajectory $\vz_k$. To avoid notation ambiguity, denote $\vg(\vx)\sim\cG_\sigma(\vx)$ to be the stochastic gradient sampled at $\vx$. Recall that (with $\vy_0=\vx_{0}-\frac{\bar{\eta}_0\beta_0}{1-\beta_0}\vm_0$ and $\vz_0=\vx_0$)
\begin{align*}
\vy_{k} =\vy_{k-1}-\bar{\eta}_{k-1}\vg(\vx_{k-1})  \\
\vz_{k} =\vz_{k-1}-\bar{\eta}_{k-1}\vg(\vz_{k-1})  \\
\end{align*}
The only difference in the iterate is that the stochastic gradients are taken at close but different locations of the trajectory. Therefore to study the trajectory difference, we adopt the method of moments proposed in~\citet{li2019stochastic}.

We start by defining the one-step updates for the coupled trajectory and for SGD. 
\begin{definition}[One-Step Update of Coupled Trajectory]\label{def:one_step_coupled}
    The one-step update for the coupled trajectory $\Delta$ can be written as 
    \begin{align*}
		\Delta(\vy, \vm,C) &= -\eta\vg\left(\vy + C \eta^{1-\alpha}\vm\right)
    \end{align*}
\end{definition}

\begin{definition}[One-Step Update of SGD]\label{def:one_step_sgd}
    The one-step update for SGD $\tilde\Delta$ can be written as 
    \begin{align*}
		\tilde\Delta(\vy) &= -\eta\vg(\vy) 	
	\end{align*}
\end{definition}

\begin{lemma}\label{lem:moment-diff}
	Let $\Delta$ be the one-step update for the coupled trajectory and $\tilde\Delta$ be the one-step update for SGD. Let $\vx_k,\vm_k$ be the $k$-th step of an SGDM run and $\vy_k=\vx_{k}-\frac{\bar{\eta}_k\beta_k}{1-\beta_k}\vm_k$ be the coupled trajectory in our consideration. Then for any $C\in [0,\frac{\lambda_{\max}\eta_{\max}}{\lambda^2_{\min}}]$, there is function $J(T)$ independent of $\eta$, such that for all vector entries $(i,j)$,
 $$ \left|\E [\tilde\Delta_{(i)}(\vy_k) -\Delta_{(i)}(\vy_k,\vm_k,C)|\vy_k]\right| 
	\leq \eta^{2-\alpha} J(T) \E [\norm{\vm_k}|\vy_k] $$
 \begin{align*}& \left|\E [\tilde\Delta_{(i)}\tilde\Delta_{(j)}(\vy_k) -\Delta_{(i)}\Delta_{(j)}(\vy_k,\vm_k,C)|\vy_k]\right| 
	\\ & \leq  \eta^{2-\alpha} J(T) (\eta + \E [\norm{\vm_k}|\vy_k](1+\eta\norm{\vy_k})+\eta^{2-\alpha}\E [\norm{\vm_k}^2|\vy_k])
 \end{align*}
 and
	$$ \left|\E [\tilde\Delta_{(i)}(\vy_k) -\Delta_{(i)}(\vy_k,\vm_k,C)]\right| 
	\leq \eta^{\frac{3-\alpha}{2}} J(T) $$$$ \left|\E [\tilde\Delta_{(i)}\tilde\Delta_{(j)}(\vy_k) -\Delta_{(i)}\Delta_{(j)}(\vy_k,\vm_k,C)]\right| 
	\leq  \eta^{\frac{3-\alpha}{2}} J(T) $$for all $k\in [0,T/\eta]$.
\end{lemma}

\begin{proof}
Recall that we use $L$ to denote the Lipschitz constant of $\nabla\Loss$ and $L_\Sigma$ to denote the Lipschitz constant of the $\mSigma$ in the Frobenius norm. We can write
\begin{align*}
	\left|\E[\tilde\Delta_{(i)}(\vy_k) -\Delta_{(i)}(\vy_k,\vm_k,C)|\vy_k]\right| &= \eta \left| \E[\partial_{(i)} \Loss(\vy_k) -  \partial_{(i)} \Loss\left(\vy_k + C\eta^{1-\alpha} \vm_k\right)|\vy_k] \right| \\
	&\leq LC\eta^{2-\alpha} \E[\|\vm_k\||\vy_k]
\end{align*}
where the second step uses the Lipschitzness of the loss gradient. The proof can be completed by noting $\E\|\vm_k\| \leq \sqrt{\E\|\vm_k\|^2}= O(\eta^{(\alpha-1)/2})$ (\Cref{lem:boun-mome}).
\begin{align*}
	& \left|\E[\Delta_{(i)}\Delta_{(j)}(\vy_k) -\tilde\Delta_{(i)}\tilde\Delta_{(j)}(\vy_k,\vm_k,C)|\vy_k]\right|\\ = &\E[\eta^2 \partial_i\Loss\partial_j \Loss(\vy_k) + \eta^2\sigma^2\mSigma_{ij}(\vy_k)|\vy_k]   - \E\left[\eta^2\partial_i\Loss\partial_j \Loss\left(\vy_k + C\eta^{1-\alpha}\vm_k\right) + \eta^2\sigma^2 \mSigma_{ij}(\vy_k + C\eta^{1-\alpha}\vm_k)|\vy_k\right]\\
 \leq & \eta^{2-\alpha} C(L_\Sigma\E[\|\vm_k\||\vy_k] +\eta L(\norm{\nabla\Loss(0)}+L\norm{\vy_k}\E[\|\vm_k\||\vy_k]+LC\eta^{1-\alpha}\E[\|\vm_k\|^2|\vy_k])) .
\end{align*}
Again by \Cref{lem:boun-mome}, $\E\|\vm_k\| = O(\eta^{(\alpha-1)/2})$, so we obtain the desired result.
\end{proof}

\begin{lemma}
    For any $m\geq 1$, $C\in [0,\frac{\lambda_{\max}\eta_{\max}}{\lambda^2_{\min}}]$, there is function $g$ independent of $\eta$ and $k$, such that
	\begin{align*}
            \E[\|\Delta(\vy_k, \vm_k, C)\|^{2m}] \leq \eta^{m} g(m,T)\\
            \E[\|\tilde\Delta(\vy_k)\|^{2m}] \leq \eta^{m} g(m,T)\\
	\end{align*}
    for all $k\leq T/\eta$.\label{lem:bounded_coupled_updates}
\end{lemma}
\begin{proof}
	The second line directly follows from  \Cref{lem:small_higher_order}. For the first line,
 \begin{align*}
     \E[\|\Delta(\vy_k, \vm_k, C)\|^{2m}] &  =\eta^{2m}\E\norm{\nabla\Loss(\vy_k+C\eta^{1-\alpha}\vm_k)+\sigma\vv_k}^{2m}\\
& \leq \eta^{2m}2^{2m} (\E[\norm{\nabla\Loss(0)}+L\norm{\vy_k}+LC\eta^{1-\alpha}\norm{\vm_k}])^{2m} + 2^{2m}\eta^{m}\E\norm{\vv_k}^{2m} \\
& = O(\eta^{m})
 \end{align*}
 by \Cref{lem:small_higher_order}.
\end{proof}

\subsubsection{Main Result}
Below we state an intermediate theorem for proving the weak approximation.
\begin{theorem}
	Let $T>0$ and $N=\lfloor T/\eta\rfloor$.  Let ($\vx_k$,$\vm_k$) be the states of an SGDM run with the coupled trajectory $\vy_k$ defined in \Cref{equ:coupled-traj}, and let $\hat{\vz}_k$ be an SGD run with start $\hat{\vz}_0=\vy_0$. Then $\vy_k$ is a weak approximation of $\hat{\vz}_k$.	
\label{thm:one_step_to_many}
\end{theorem}
Notice that the only difference between \Cref{thm:one_step_to_many} and our final result \Cref{thm:weak-appro-main} is that $\vz_k$ is an SGD process starting from $\vx_0$ while $\hat{\vz}_k$ is an SGD process starting from $\vy_0$. 

Before proving \Cref{thm:one_step_to_many}, we introduce the following lemma, which is an analog to Lemma 27 of \citet{li2019stochastic}, Lemma C.2 of \citet{li2021validity}, and Lemma B.6 of \citet{malladi2022sdes}.
It shows that if the update rules for the two trajectories are close in all of their moments, then the test function value will also not change much after a single update from the same initial condition. Let $G^{k}$ be the set of functions that are $k$-times continuously differentiable and all its derivatives up to and including order $k$ has polynomial growth, and let $G=G^0$ be the set of functions with polynomial growth. 
\begin{lemma}
	Suppose $u\in G^{3}$. Then, there exists a constant $K_u(T)$ independent of $\eta$, such that for all $k\leq T/\eta$,
	$$\left|\E[u(\vy_k + \Delta(\vy_k,\vm_k,\frac{\bar{\eta}_k\beta_k}{1-\beta_k}))] - \E[u(\vy_k + \tilde\Delta(\vy_k))]\right| \leq K_u(T)\eta^{(3-\alpha)/2} $$
	\label{lem:one_step}
\end{lemma}
\begin{proof}

Since $u \in G^{3}$, we can find $K_0(\vx)=k_1(1+\norm{\vx}^{k_2}) \in G$ such that $u(\vx)$ is bounded by $K_0(\vx)$ and so are all the partial derivatives of $u$ up to order $3$.
For notation simplicity let $\Delta=\Delta(\vy_k,\vm_k,\frac{\bar{\eta}_k\beta_k}{1-\beta_k})$ and $\tilde\Delta=\tilde\Delta(\vy_k)$. By Taylor's Theorem with Lagrange Remainder, we have
\begin{align*}
    u(\vy_k + \Delta) - u(\vy_k + \tilde{\Delta})
    &= \underbrace{\sum_{1\leq i\leq d} \frac{\partial u(\vy_k)}{\partial x_i} \left(\Delta_{i} - \tilde\Delta_{i}\right)}_{B_1} \\
    &+ \underbrace{\frac{1}{2} \sum_{1\leq i_1, i_2\leq d}\frac{\partial^2 u(\vy_k)}{\partial x_{i_1}\partial x_{i_2}} \left(\Delta_{i_1}\Delta_{i_2} - \tilde\Delta_{i_1}\tilde\Delta_{i_2}\right)}_{B_2} \\
    &+ R_{3} - \tilde{R}_{3}
\end{align*}The remainders $R_{3}$, $\tilde{R}_{3}$ are
\begin{align*}
    R_{3} &:=
    \frac{1}{6} \sum_{1\leq i_1, i_2, i_3\leq d}\frac{\partial^3 u(\vy_k + a\Delta)}{\partial x_{i_1}\partial x_{i_2}\partial x_{i_3}} \Delta_{i_1}\Delta_{i_2}\Delta_{i_3}, \\
     \tilde{R}_{3} &:=
    \frac{1}{6} \sum_{1\leq i_1, i_2, i_3\leq d}\frac{\partial^3 u(\vy_k + \tilde{a}\tilde\Delta)}{\partial x_{i_1}\partial x_{i_2}\partial x_{i_3}} \tilde\Delta_{i_1}\tilde\Delta_{i_2}\tilde\Delta_{i_3}.
\end{align*}
for some $a, \tilde{a} \in [0, 1]$. We know from \Cref{lem:moment-diff} that 
\begin{align*}
    \E B_1 & = \E\left[\sum_{1\leq i\leq d} \frac{\partial u(\vy_k)}{\partial x_i} \E[\Delta_{i} - \tilde\Delta_{i}| \vy_k]\right]\\
    & \leq \E\left[\sum_{1\leq i\leq d} \frac{\partial u(\vy_k)}{\partial x_i} \eta^{2-\alpha} J(T) \E [\norm{\vm_k}|\vy_k]\right]\\
    & \leq \E (\eta^{2-\alpha}d k_1J(T)(\norm{\vm_k}+\norm{\vm_k}\norm{\vy_k}^{k_2}))\\
    & = O(\eta^{\frac{3-\alpha}{2}}).
\end{align*}
The last step is due to \Cref{lem:small_higher_order}. Also,
\begin{align*}
    \E B_2 & = \E\left[\frac{1}{2} \sum_{1\leq i_1, i_2\leq d}\frac{\partial^2 u(\vy_k)}{\partial x_{i_1}\partial x_{i_2}} \left(\E[\Delta_{i_1}\Delta_{i_2} - \tilde\Delta_{i_1}\tilde\Delta_{i_2}|\vy_k]\right)\right]\\
    & \leq \E (d^2 k_1J(T)\eta^{2-\alpha}(1+\norm{\vy_k}^{k_2})(\eta + \norm{\vm_k}(1+\eta\norm{\vy_k})+\eta^{2-\alpha} \norm{\vm_k}^2)))\\
    & = O(\eta^{\frac{3-\alpha}{2}}).
\end{align*}

For $R_{3}$, by Cauchy-Schwarz inequality we have
\begin{align*}
    \E[R_{3}] 
    &\le \frac{1}{6}\left(\sum_{i_j}
        \E\abs{\frac{\partial^3 u(\vy_k + a\Delta)}{\partial x_{i_1}\partial x_{i_2}\partial x_{i_3}} }^2
    \right)^{1/2}
    \cdot
    \left(\E\|\Delta\|^{6}\right)^{1/2} \\
    &\le \left(d^3 \E K^2_0(\vy_k + a\Delta)\right)^{1/2}\cdot 
    \eta^{3/2}g^{1/2}(m,T)
\end{align*}
by \Cref{lem:bounded_coupled_updates}.
For $K_0^2(\vy_k + a\Delta)$, we can bound its expectation by
\begin{align*}
    \E[K^2_0(\vy_k + a\Delta)]
    &\le k_1^2 \E(1+\norm{\vy_k + a\Delta}^{k_2})^2 \\
    &\le 2k_1^2 \left( 1 + 2^{2k_2-1} \E[\norm{\vy_k}^{2k_2} + \E\norm{\Delta}^{2k_2}] \right) \\
    &=O(1).
\end{align*}
Therefore $\E[R_3] = O(\eta^{3/2})$. An analogous argument bounds $\E[\tilde{R}_3] = O(\eta^{3/2})$ as well. Thus, the entire Taylor expansion and remainders are bounded as desired.
	
\end{proof}

\begin{proof}[Proof for \Cref{thm:one_step_to_many}]
    
	Let $\hat\vy_{j,k}$ be the trajectory defined by following the coupled trajectory for $j$ steps and then do $k-j$ steps SGD updates.
	So, $\hat\vy_{j,j+1} = \vy_j + \tilde\Delta(\vy_j)$ and $\hat\vy_{j+1,j+1} = \vy_j + \Delta(\vy_j,\vm_j,\frac{\bar{\eta}_j\beta_j}{1-\beta_j})$.
	Let $h$ be the test function with at most polynomial growth.
	Then, we can write
	\begin{equation}
		|\E[h(\vz_k) - \E[h(\vy_k)]| = \sum_{j=0}^{k-1} (\E[h(\hat\vy_{j+1,k})] - \E[h(\hat\vy_{j,k})])
	\end{equation}
	Define $u(\vy, s, t) = \E_{\hat\vy\sim\mathcal{P}(\vy, s, t)} [h(\hat\vy_t)]$, where $\mathcal{P}(\vy, s, t)$ is the distribution induced by starting from $\vy$ at time $s$ and following the SGD updates until time $t$.
	Then,
	\begin{equation}
		|\E[h(\vz_k) - \E[h(\vy_k)]| \leq \sum_{j=0}^{k-1} |\E[u(\hat\vy_{j+1,j+1}, j+1, k)] - \E[u(\hat\vy_{j,j+1}, j+1, k)]|
	\end{equation}
	Define $u_{j+1} = u(\vy, j+1, k)$.
	Then,
	\begin{equation}
		|\E[h(\vz_k) - \E[h(\vy_k)]| \leq \sum_{j=0}^{k-1} | \E[u_{j+1}(\vy_j + \tilde\Delta(\vy_j))] - \E[u_{j+1}(\vy_j + \Delta(\vy_j,\vm_j,\frac{\bar{\eta}_j\beta_j}{1-\beta_j}))]|
	\end{equation}
        We will show that $u_{j+1}\in G^3$, so by \Cref{lem:one_step},
	\begin{equation}
		| \E[u_{j+1}(\vy_j + \tilde\Delta(\vy_j))] - \E[u_{j+1}(\vy_j + \Delta(\vy_j,\vm_j,\frac{\bar{\eta}_j\beta_j}{1-\beta_j}))]| \leq K_{u_{j+1}}(T)\eta^{(3-\alpha)/2}
	\end{equation}
	Then,
	\begin{align*}
		|\E[h(\vz_k) - \E[h(\vy_k)]| &\leq \sum_{j=0}^{k-1} K_{u_{j+1}}(T)\eta^{(3-\alpha)/2}\\
		&\leq k \sup_{j} K_{u_{j}}(T)\eta^{(3-\alpha)/2}\\
        &\leq T \sup_{j} K_{u_{j}}(T) \eta^{(1-\alpha)/2}.
	\end{align*}
    We will show that we can choose $K_{u_{j}}$ so that $\sup_{j}K_{u_{j}}(T)$ is bounded by some universal constant $K(T)$, and by definition $\vz_k$ and $\vy_k$ are order-$\gamma$ weak approximations of each other.

    Finally, we show that $u_{j+1}\in G^3$, and there is an universal function $K_u\in G$ that $\norm{\partial^m u_{j+1}(\vx)}\leq K_u(\vx)$ for all $j\leq T/\eta$ and $m=0,1,2,3$. Then from the proof of \Cref{lem:one_step} we know that there is an universal constant $K(T)$ that upper bounds all $K_{u_{j}}(T)$.  
    The proof follows the same steps as that in \citet{li2019stochastic} for Proposition 25, except that $u_j$ is defined for discrete SGD updates instead of continuous SDE updates. 

    Notice that $u_{j+1}(\vx) = \E_{\vy\sim \mathcal{P}(\vx,j+1,k)}h(\vy)$ is the expected $h$ value after running SGD from $\vx$ for $k-j-1$ steps. Let $\{\vs_i\}$ be such a process with $\vs_{j+1}=\vx$. 

    First we show that $\vs_i$ have bounded moments, and the bound is universal for all $i\leq T/\eta$ with polynomial growth with respect to the initial point $\vx$. For any $m$, we know
    \begin{align*}
        \E (1+\norm{\vs_{i+1}}^2)^m|\vs_i & =  \E (1+\norm{\vs_{i}-\bar{\eta}_i (\nabla\Loss(\vs_i) + \sigma \vv_i)}^2)^m|\vs_i\\
        & =  \E (1+\norm{\vs_{i}}^2 + \bar{\eta}^2_i \norm{\nabla\Loss(\vs_i) + \sigma \vv_i}^2 - 2\bar{\eta}_i\vs_{i}^\top (\nabla\Loss(\vs_i) + \sigma \vv_i) )^m|\vs_i
    \end{align*}
    From the Lipschitzness we know $\norm{\nabla\Loss(\vs_i)}\leq \norm{\nabla\Loss(0)} + L\norm{\vs_i}$ and $\norm{\mSigma(\vs_i)}_F\leq \norm{\mSigma(0)}_F + L_\Sigma\norm{\vs_i}$. Let $\delta_i =  \bar{\eta}^2_i \norm{\nabla\Loss(\vs_i) + \sigma \vv_i}^2 - 2\bar{\eta}_i\vs_{i}^\top (\nabla\Loss(\vs_i) + \sigma \vv_i) $, so there is constant $C$ such that $\E \delta_i|\vs_i \leq \eta C (1+\norm{\vs_i}^2)$, $\E \delta_i^2|\vs_i\leq \eta C (1+\norm{\vs_i}^2)^2$ and $\E \delta_i^m\leq \eta C (1+\norm{\vs_i}^2)^m$. Then, 
    similar to the SGDM case,
      \begin{align*}
        \E (1+\norm{\vs_{i+1}}^2)^m|\vs_i & =\E (1+\norm{\vs_{i}}^2 + \delta_i)^m|\vs_i\\
        & \leq  (1+\norm{\vs_{i}}^2)^m + m \E \delta_i(1+\norm{\vs_{i}}^2)^{m-1}|\vs_i\\
        & + 2^{m-1} \E \delta_i^2 ((1+\norm{\vs_{i}}^2)^{m-2} + |\delta_i|^{m-2})|\vs_i\\
        & \leq (1+\norm{\vs_{i}}^2)^m(1+\eta C (m+2^m))
    \end{align*}
    So $\E (1+\norm{\vs_{i+1}}^2)^m \leq \E (1+\norm{\vs_{j+1}}^2)^m(1+\eta C (m+2^m))^{i-j} \leq (1+\norm{\vx}^2)^m e^{TC(m+2^m)}$ for all $i,j\leq T/\eta$. Thus we proved that the moments of $\vs_i$ are bounded by a universal polynomial of the initial point $\vx$. Hence, as $h$ is bounded by a polynomial, $u_{j+1}(\vx) = \E_{\vs_k|\vs_{j+1}=\vx}h(\vs_k)$ is also uniformly bounded by a polynomial of $\vx$ independent of $\eta$.

    Now we consider the derivatives of $u_{j+1}$. We use the notion of derivatives of a random variable in the $\mathcal{L}^2$ sense as in \citet{li2019stochastic}, e.g. $\partial_x \vv_i$ is defined in the sense that $\E \norm{\partial_x \vv_i}^2 = \partial_x\tr\mSigma(\vs_i)$. Taking derivatives,
    \begin{align*}
        \E (1+\norm{\partial_x\vs_{i+1}}^2)^m|\vs_i & =  \E (1+\norm{\partial_x\vs_{i}-\bar{\eta}_i (\nabla^2\Loss(\vs_i)\partial_x\vs_i + \sigma \partial_x\vv_i)}^2)^m|\vs_i\\
        & =  \E (1+\norm{\partial_x\vs_{i}}^2 + \xi_i)^m|\vs_i
    \end{align*}
    where $\xi_i = \bar{\eta}^2_i \norm{\nabla^2\Loss(\vs_i)\partial_x\vs_i + \sigma \partial_x\vv_i}^2 - 2\bar{\eta}_i\vs_{i}^\top (\nabla^2\Loss(\vs_i)\partial_x\vs_i + \sigma \partial_x\vv_i)$. Lipschitzness dictates that $\norm{\nabla^2\Loss(\vs_i)}\leq L$ and $\norm{\nabla\mSigma(\vs_i)}\leq L_{\Sigma}$, so again there is constant $D$ such that $\E \xi_i|\vs_i \leq \eta D (1+\norm{\partial_x\vs_i}^2)$, $\E \xi_i^2|\vs_i\leq \eta D (1+\norm{\partial_x\vs_i}^2)^2$ and $\E \xi_i^m\leq \eta D (1+\norm{\partial_x\vs_i}^2)^m$.
    Similarly,
    \begin{align*}
        \E (1+\norm{\partial_x\vs_{i+1}}^2)^m & =\E (1+\norm{\partial_x\vs_{i}}^2 + \xi_i)^m\\
        & \leq  \E (1+\norm{\partial_x\vs_{i}}^2)^m + m\xi_i(1+\norm{\partial_x\vs_{i}}^2)^{m-1}\\
        & + 2^{m-1}\xi_i^2 ((1+\norm{\partial_x\vs_{i}}^2)^{m-2} + |\xi_i|^{m-2})\\
        & \leq (1+\norm{\partial_x\vs_{i}}^2)^m(1+\eta D (m+2^m))
    \end{align*}
    So $\E (1+\norm{\partial_x\vs_{i+1}}^2)^m \leq \E (1+\norm{\partial_x\vs_{j+1}}^2)^m(1+\eta D (m+2^m))^{i-j} \leq (1+\norm{\partial_x\vx}^2)^m e^{TD(m+2^m)} = (1+d)^m e^{TD(m+2^m)}$ for all $i,j\leq T/\eta$. Thus we proved that the moments of $\partial_x\vs_i$ are also bounded by a universal polynomial of the initial point $\vx$. Hence, as $h$ has polynomial-growing derivatives, $\norm{\partial_{\vx} u_{j+1}(\vx)} = \E_{\vs_k|\vs_{j+1}=\vx}\nabla h(\vs_k)^\top\partial_\vx \vs_k\leq ([\E \norm{\nabla h(\vs_k)}^2][\E \norm{\partial_\vx \vs_k}^2] )^{1/2}$ is also uniformly bounded by a polynomial of $\vx$ independent of $\eta$.

    Notice that the higher order derivatives of $\vs_i$ have similar forms of iterates, e.g. $$\partial^b\vs_{i+1} = \partial^b_x\vs_{i} + \phi^b_i-\bar{\eta}_i (\nabla^2\Loss(\vs_i)\partial^b_x\vs_i + \sigma \partial^b_x\vv_i)$$
    where $\phi^b_i$ only relates with $\nabla^\beta \Loss$, $\nabla^\beta \mSigma$ and $\partial^\beta_x \vs_i$ for $\beta<b$. Notice that by assuming $\Loss$ and $\mSigma$ have polynomial-growing derivates up to and including the third order (\Cref{assume:ngos}), by induction we can prove that the process $\phi^b_i$ is universally bounded as a polynomial of derivates of order $<b$, so similarly by the Gr\"{o}nwall inequality we can obtain an universal bound for $\partial^\beta u_{j+1}$ for $\beta=0,1,2,3$. As the argument here is identical to that in the proof for Proposition 25, \citet{li2019stochastic},  we omit the fine details.
\end{proof}
\begin{proof}[Proof for \Cref{thm:weak-appro-main}]

From \Cref{thm:coupled_weak_approx}, we know $\vx_k$ and $\vy_k$ are  order-$(1-\alpha)/2$-weak approximations of each other, and from \Cref{thm:one_step_to_many} we know $\hat{\vz}_k$ and $\vy_k$ are  order-$(1-\alpha)/2$-weak approximations of each other. Furthermore, in the above proof we can see that $|\E [h(\vz_k)-h(\hat{\vz}_k)]| = |\E[u_{0}(\vy_0)-u_{0}(\vx_0)]| = |\E[u_{0}(\vx_{0}-\frac{\bar{\eta}_0\beta_0}{1-\beta_0}\vm_0)-u_{0}(\vx_0)]|$, so by the similar Taylor expansion as in \Cref{lem:one_step}, as $\vm_0$ has bounded moments (\Cref{ass:init-bound}), we can see that $|\E [h(\vz_k)-h(\hat{\vz}_k)]|=O(\eta^{1-\alpha})$. This shows that $\vz_k$ and $\hat{\vz}_k$ are order-$(1-\alpha)$-weak approximations of each other, and naturally they are order-$(1-\alpha)/2$-weak approximations.

Then we conclude that $\vx_k$ and $\vz_k$ are order-$(1-\alpha)/2$-weak approximations of each other.
\end{proof}