In this section we will prove \Cref{thm:weak-appro-main} as the first part of our main result. First we shall take a closer examination at our update rule in \Cref{def:SGDM}. Let $\beta_{s:t}=\prod_{r=s}^t \beta_r$ be the product of consecutive momentum hyperparameters for $s\leq t$, and define $\beta_{s:t}=1$ for $s>t$, we get the following expansion forms.
\begin{lemma}\label{lem:unrollm}
    \begin{align*}
        \vm_k & =\beta_{0:k-1} \vm_0 + \sum_{s=0}^{k-1}\beta_{s+1:k-1}(1-\beta_{s}) \vg_{s}\\
        \vx_k & = \vx_0 - \sum_{s=0}^{k-1}\eta_s\beta_{0:s}\vm_0 - \sum_{s=0}^{k-1}\sum_{r=s}^{k-1}\eta_r\beta_{s+1:r}(1-\beta_{s})\vg_s\\
    \end{align*}
\end{lemma}
\begin{proof}
    By expansion of \Cref{dqu:sgdm-1}.
\end{proof}

Let the coefficients $c_{s,k}=\sum_{r=s}^{k-1}\eta_r\beta_{s+1:r}(1-\beta_{s})$ so that $\vx_k=\vx_0 - \frac{\beta_0 c_{0:k}}{1-\beta_0}\vm_0 -\sum_{s=0}^{k-1} c_{s,k}\vg_s$. Notice that $c_{s,k}$ is increasing in $k$ but it is always upper-bounded as $c_{s,k}\leq c_{s,\infty}=\sum_{r=s}^{\infty}\eta_r\beta_{s+1:r}(1-\beta_{s})\leq (\sup_{t}\eta_t)\sum_{r=s}^{\infty}(\sup_t\beta_t)^{r-s}=\frac{\sup_{r}\eta_r}{1-\sup_t\beta_t}$. Notice that when we have a finite schedule, we can make any extension to infinity steps (e.g. infinitely copy the hyperparameters on the last step), and the reasonings will work for the extended schedule. In this case, though the trajectory $\vx_k$ are not affected by future hyperparameters, the coefficients $c_{s,\infty}$ will depend on our extension. Furthermore, if for any $t$, $\beta_t$ and $\eta_t$ are about the same scale, then we should expect that the difference $c_{s,\infty}-c_{s,k}$ is exponentially small in $k$. Then we can hypothesize that the trajectory $\vx_k$ should be close to a trajectory of the following form:
\[\tilde{\vx}_k=\tilde{\vx}_0 - \frac{\beta_0 c_{0:\infty}}{1-\beta_0}\vm_0 -\sum_{s=0}^{k-1} c_{s,\infty}\tilde{\vg}_s\]
which is exactly a SGD trajectory given the learning rate schedule $c_{s,\infty}$.

To formalize the above thought, we define the averaged learning rate schedule
\begin{equation}\label{equ:averaged-lr}
    \bar{\eta}_k := c_{k,\infty}=\sum_{s=k}^\infty \eta_s  \beta_{k+1:s}(1-\beta_{k})
\end{equation}
And the coupled trajectory
\begin{align}\label{equ:coupled-traj}
    \vy_{k} & :=\vx_{k}-\frac{\bar{\eta}_k\beta_k}{1-\beta_k}\vm_k\\
    & = \vx_0 - \frac{\beta_0 \bar{\eta}_0}{1-\beta_0}\vm_0 -\sum_{s=0}^{k-1} \bar{\eta}_s\vg_s.
\end{align}
Then there is the following transition:
\begin{align}
    \vy_{k} =\vy_{k-1}-\bar{\eta}_{k-1}\vg_{k-1}
\end{align}
$\vy_k$ has an interesting geometrical interpretation as the endpoint of SGDM if we cut-off all the gradient signal from step $k$. E.g., $\vx_\infty$ if we set $\vg_{k+1}=\vg_{k+2}=\cdots = 0$ for and update the SGDM from ($\vx_k$,$\vm_k$).
