\section{Introduction}

In modern deep learning, it is standard to combine stochastic gradient methods with {\em heavy-ball momentum}, or {\em momentum} for short, to enable a more stable and efficient training of neural networks~\citep{sutskever2013on}. 
The simplest form is {\em Stochastic Gradient Descent with Momentum} (SGDM). 
SGDM aims to minimize the training loss $\cL(\vx)$ given a noisy gradient oracle $\cG(\vx)$, which is usually realized by evaluating the gradient at a randomly sampled mini-batch from the training set. 
Specifically, let $\gamma, \beta$ be the learning rate and momentum coefficient, then SGDM can be stated as:
\begin{equation}\label{equ:SGDM-intro}
    \vg_k \sim \cG(\vx_k), \qquad \vm_{k+1} = \beta \vm_k + \vg_k, \qquad \vx_{k+1} = \vx_{k} - \gamma \vm_{k+1},
\end{equation}
where $\vg_k, \vm_k, \vx_k$ are the gradient, momentum buffer, and parameter vector at step $k$. 

For typical choices of $\beta\in(0,1)$, the momentum buffer can be interpreted as an exponential moving average of past gradients, i.e., $\vm_k = \sum_{j=0}^{k} \beta^{k-j} \vg_j$. 
Based on this interpretation, \citet{polyak1964some,polyak1987intro,rumelhart1987learning} argued that momentum is able to cancel out oscillations along high-curvature directions and add up contributions along low-curvature directions. 
More concretely, for strongly convex functions without any noise in gradient estimates, \citet{polyak1964some,polyak1987intro} showed that adding momentum can stabilize the optimization process even when the learning rate is so large that can make vanilla gradient descent diverge, and thus momentum accelerates the convergence to minimizers by allowing using a larger learning rate.

In deep learning, however, the random sampling of mini-batches inevitably introduces a large amount of stochastic gradient noise, which sometimes dominates the true gradient and may become the main source of training instability.
As the above convergence results solely analyze the noiseless case, it remains unclear in theory whether momentum can likewise stabilize the stochastic optimization process in deep learning. 

To understand the benefit of momentum in stochastic optimization,
several prior studies~\citep{bottou2018optimization,defazio2020momentum,you2020large} speculate that averaging past stochastic gradients through momentum may reduce the variance of the noise in the parameter update, thus making the loss decrease faster.
To approach this more rigorously, \citet{cutkosky2019momentum} proposed a variant of SGDM that provably accelerates training by leveraging the reduced variance in the updates.

Nevertheless, for SGDM without any modifications, past theoretical analyses in the stochastic optimization of convex and non-convex functions typically conclude with a convergence rate that is comparable to that of vanilla SGD, but not faster~\citep{yan2018unified,yu2019linear,liu2020improved,sebbouh2021almost,li2022last}. 
Besides, there also exist simple and concrete instances of convex optimization where momentum does not speed up the convergence rate of SGD, even though it is possible to optimize faster with some variants of SGDM~\citep{kidambi2018on}. This naturally raises the following question on the true role of momentum:
\begin{center}
    \emph{Does noise reduction in SGDM updates really benefit neural network training?}
\end{center}
To address this question, this paper delves into the training regime where the learning rate is small enough to prevent oscillations along high-curvature directions, yet the gradient noise is large enough to induce instability. This setting enables us to concentrate exclusively on the interplay between momentum and gradient noise.
More importantly, this training regime is of practical significance as in many situations, such as small-batch training from scratch or fine-tuning a pre-trained model, the optimal learning rate is indeed relatively small~\citep{liu2019roberta,malladi2023kernelbased}.


\textbf{Main Contributions.} 
In this paper, we present analyses of the training trajectories of SGD with and without momentum, in the regime of small learning rate. 
We provide theoretical justifications of a long-held belief that SGDM with learning rate $\gamma$ and momentum $\beta$ performs comparably to SGD with learning rate $\eta = \frac{\gamma}{1 - \beta}$~\citep{tugay1989properties,orr1996dynamics, qian1999momentum, yuan2016influence,smith2020generalization}.
This finding offers negative evidence for the usefulness of noise reduction in momentum. Additionally, this also motivates us to reformulate SGDM in \Cref{def:SGDM} so SGDM and SGD perform comparably under the same learning rate $\eta$, which in turn simplifies our analysis.

More specifically, given a run of SGDM, we show that vanilla SGD can closely track its trajectory in the following two regimes with different time horizon:

\vspace{-0.1in}
\begin{description}
    \item[Regime I.] Training with SGD and SGDM for $O(1/\eta)$ steps where the scaling of gradient noise covariance can be as large as $O(1/\eta)$.
    Specifically, \Cref{thm:weak-appro-main} shows that SGD and SGDM are $O(\sqrt{\eta/(1-\beta)})$-close to each other in the sense of weak approximation, where $\eta,\beta$ are the learning rate and momentum coefficient under the notation of \Cref{def:SGDM}. Our analysis not only includes the classical result that both SGD and SGDM converge to Gradient Flow in $O(1/\eta)$ steps where the stochastic gradient is sampled from a bounded distribution independent of $\eta$, but also covers the regime of applying Linear Scaling Rule~\citep{goyal2017accurate}, where one decreases the learning rate and batch size at the same rate, so the noise covariance increases inversely proportional to $\eta$, and in this case both SGD and SGDM converge to a Stochastic Differential Equation (SDE). 
    Our results improve over previous analysis~\citep{yuan2016influence,liu2018diffusion} by avoiding underestimating the role of noise when scaling down the learning rate, and provide rigorous theoretical supports to the scaling claims in \citet{smith2020generalization,cowsik2022flatter}.
    Technically we introduce an auxiliary dynamics $\vy_k$ (\Cref{equ:coupled-traj}) that bridges SGDM and SGD. 
   \item[Regime II.] Training with SGD and SGDM for $O(1/\eta^2)$ steps for overparametrized models where the minimizers of the loss connect as a manifold and after reaching such a manifold, the gradient noise propels the iterates to move slowly along it. \Cref{thm:slow-sde-main} shows that SGD and SGDM follow the same dynamics along the manifold of minimizers and thus have the same implicit bias. The implicit bias result of SGD is due to \citet{katzenberger1991solutions,li2021happens} whose analysis does not apply to SGDM because its dynamic depends non-homogeneously on $\eta$.
    Our proof of \Cref{thm:slow-sde-main} is non-trivial in carefully  decomposing the updates.
\end{description}

In \Cref{sec:exps}, we further empirically verify that momentum indeed has limited benefits for both optimization and generalization in several practical training regimes, including small- to medium-batch training from scratch on ImageNet and fine-tuning RoBERTa-large on downstream tasks.
For large-batch training, we observe that SGDM allows training with a large learning rate, in which regime vanilla SGD may exhibit instability that degrades the training speed and generalization. The observations are consistent with previous empirical studies on SGDM~\citep{kidambi2018on,shallue2019measuring,smith2020generalization}. We argue that the use of a large learning rate makes the weak approximation bound~$O(\sqrt{\eta /(1-\beta)})$ loose: running SVAG~\citep{li2021validity}, an SDE simulation method for both SGD and SGDM, shrinks or even eliminates the performance gain of momentum.

Finally, we highlight that our results can also have practical significance beyond just understanding the role of momentum. In recent years, the GPU memory capacity sometimes becomes a bottleneck in training large models.
As the momentum buffer costs as expensive as storing the entire model, it has raised much interest in when it is safe to remove momentum~\citep{shazeer2018adafactor}. Our work sheds light on this question by formally proving that momentum only provides marginal values in small learning rate SGD.
Furthermore, our results imply that within reasonable range of scales the final performance is insensitive to the momentum hyperparametrization, thereby provide support to save the effort in the extensive hyperparameter grid search.
