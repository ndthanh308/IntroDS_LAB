\newpage
\section{Related Works}
\textbf{The role of momentum in optimization.}
The accelerating effect of some variants of momentum has been observed in convex optimization~\citep{kidambi2018on} and linear regression~\citep{jain2018accelerating} with under specialized parametrizations.
\citet{smith2018disciplined} pointed out that momentum can help stabilize training, but the optimal choice of momentum is closely related to the choice of learning rate.
\citet{plattner2022on} later empirically established that momentum enlarges the learning rate but does not boost performance. 
\citet{arnold2019reducing} argued using a quadratic example that momentum might not reduce variance as the gradient noise in each would actually be carried over to future iterates due to momentum.
\citet{tondji2021variance} showed that the application of a multi-momentum strategy can achieve variance reduction in deep learning.

\citet{defazio2020momentum} proposed a stochastic primal averaging formulation for SGDM which facilitates a Lyapunov analysis for SGDM, and one particular insight from their analysis is that momentum may help reduce noise in the early stage of training but is no longer helpful when the iterates are close to local minima.
\citet{xie2021positive} showed that under SDE approximation, the posterior of SGDM is the same as that of SGD.
\citet{jelassi2022towards} proved the generalization benefit of momentum in GD in a specific setting of binary classification, by showing that GD+M is able to learn small margin data from the historical gradients in the momentum.
A stronger implicit regularization effect of momentum in GD is also proved in \citet{ghosh2023implicit}. \citet{fu2023and} showed that momentum is beneficial in deferring an "abrupt sharpening" phenomenon that slows down optimization when the learning rate is large.

\textbf{Convergence of momentum methods.}
Momentum-based methods do not tend to yield faster convergence rates in theory.
\citet{yu2019linear} showed that distributed SGDM can achieve the same linear speedup as ditributed SGD in the non-convex setting.
Also in the non-convex setting, \citet{yan2018unified} showed that the gradient norm converges at the same rate for SGD, SGDM and stochastic Nestrov's accelerated gradient descent, and they used stability analysis to argue that momentum helps generalization when the loss function is Lipschitz. 
Under the formulation of quasi-hyperbolic momentum~\citep{ma2018quasihyperbolic}, 
\citet{gitman2019understanding} proposed another unified analysis for momentum methods. 
 \citet{liu2020improved} proved that SGDM converges as fast as SGD for strongly convex and non-convex objectives even without a bounded gradient assumption. Using a iterate-averaging formulation, \citet{sebbouh2021almost} proved last-iterate convergence of SGDM in both convex and non-convex settings.
Later, \citet{li2022last} showed that constant momentum can lead to suboptimal last-iterate convergence rate and increasing momentum resolves the issue. \citet{smith2018disciplined,liu2018diffusion} provided evidence that momentum helps escape saddle points.

\textbf{Characterizing implicit bias near manifold of local minimizers}
A recent line of work has studied the implicit bias induced by gradient noise in SGD-type algorithms, when iterates are close to some manifold of local minimizers \citep{blanc2020implicit,damian2021label,li2021happens}.
In particular, \citet{li2021happens} developed a framework for describing the dynamics of SGD via a slow SDE on the manifold of local minimizers in the regime of small learning rate (see~\Cref{sec:app_long_horizon} for an introduction).
Similar methodology has become a powerful tool for analyzing algorithmic implicit bias and has been extended to many other settings, including SGD/GD for models with normalization layers~\citep{lyu2022understanding,li2022fast}, GD in the edge of stability regime~\citep{arora2022understanding}, Local SGD~\citep{gu2023why},  sharpness-aware minimization~\citep{wen2022does}, and pre-training for language models~\citep{liu2022same}.
Notably, \citet{cowsik2022flatter} utilized the similar idea to study the slow SDE of SGDM study the optimal scale of the momentum parameter with respect to the learning rate, which has a focus different from our paper. 
