
\section{Weak Approximation of SGDM by SGD in $O(1/\eta)$ Steps}\label{sec:weak_approx}

Next, we will present our main theoretical results on SGDM with small learning rates.
In this section, we show that in $O(1/\eta)$ steps, SGD approximates SGDM in the sense of \Cref{def:weak_approx} for $\sigma\leq 1/\sqrt{\eta}$.
The next section studies SGDM over a longer training horizon ($O(1/\eta^2)$ steps) to characterize the coinciding implicit regularization effects of SGDM and SGD.
\subsection{A Warm-Up Example: The Variance Reduction Effect of Momentum}\label{sec:warm-up}
Intuitively, momentum makes the SGD update directions less noisy by averaging past stochastic gradients, which seems at first glance to contradict our result that the distribution of SGD and SGDM are approximately the same.
However, the apparent discrepancy is a consequence that,
by carrying the current gradient noise to subsequent steps, the updates of SGDM have long-range correlations .

For instance, we consider the case where the stochastic gradients are i.i.d. gaussian as $\vg_k\sim \mathcal{N}(\vc,\sigma^2\mI)$ for a constant vector $\vc$. We compare SGD and SGDM trajectories with hyperparameter $\eta_k=\eta$ and $\beta_k=\beta$, and initialization $\vz_0=\vx_0$ and $\vm_0\sim \mathcal{N}(\vc,\frac{1-\beta}{1+\beta}\sigma^2\mI)$. The single-step updates are
\begin{align*}
    \vz_{k+1} - \vz_{k} & = -\eta\vg_k\sim \mathcal{N}(-\eta\vc, \eta^2\sigma^2\mI).\\
    \vx_{k+1} - \vx_{k} & = -\eta\vm_{k+1} = -\eta(\beta^{k+1}\vm_0 + \sum_{s=0}^{k} \beta^{k-s}(1-\beta)\vg_s) \sim \mathcal{N}(-\eta\vc, \frac{1-\beta}{1+\beta}\eta^2\sigma^2\mI).
\end{align*}
Therefore, the variance of each single-step update is reduced by a factor of $\frac{1-\beta}{1+\beta}$, which implies larger momentum generates a smoother trajectory. However, we are usually more interested in tracking the final loss distributions induced by each trajectory. The distributions of after $k$ steps are
\begin{align*}
    \vz_k & \sim\mathcal{N}(\vz_0-k\eta\vc, k\eta^2\sigma^2 \mI);\\
    \vx_{k} & = \vz_0-\eta\beta\frac{1-\beta^k}{1-\beta}\vm_0 -\eta\sum_{s=0}^{k-1} (1-\beta^{k-s})\vg_s \sim \mathcal{N}\bigg(\vz_0-k\eta\vc, k\eta^2\sigma^2\mI-2\beta\eta^2\sigma^2\frac{1-\beta^k}{1-\beta^2}\mI\bigg).
\end{align*}
Notice that the variance of the final endpoint is only different by $|2\beta\eta^2\sigma^2\frac{1-\beta^k}{1-\beta^2}|\leq \frac{2\eta^2\sigma^2}{1-\beta^2}$, which is bounded regardless of $k$. The variance of $\vx_k$ is increased at rate $\eta^2\sigma^2$ per step, which is significantly larger than the per step update variance $\frac{1-\beta}{1+\beta}\eta^2\sigma^2$. This is a consequence of the  positive correlation of momentum updates that contributes to the variance of the trajectory in total.

Furthermore, we can observe that SGD and SGDM trajectories have different levels of turbulence induced by the different per step update variances. In some cases covered by our main result \Cref{thm:weak-appro-main}, the SGD and SGDM trajectories even exhibit different asymptotic behaviors in the limit $\eta\to 0$. For instance, when $\sigma=\eta^{-1/3}$, $\beta=1-\eta^{3/4}$, if we track the trajectory at $k=t\eta^{-1}$ steps for constant $t>0$, $\vz_k\sim \mathcal{N}(\vz_0-t\vc, t\eta^{1/3} \mI)$ and $\vx_k\sim \mathcal{N}(\vz_0-t\vc, t\eta^{1/3}\mI-2\eta^{7/12}\frac{\beta(1-\beta^k)}{1+\beta}\mI )$ with vanishing variance as $\eta\to 0$. While both trajectories converge to the straight line $\vz_0-t\vc|_{t\geq 0}$ in the limit, when we measure their total length,
\begin{align*}
\E\sum_k \norm{\vz_{k+1}-\vz_k}_2 &\geq t\eta^{-1/3}\E_{\xi\sim \mathcal{N}(0,\mI)}\norm{\xi}\to \infty , \\
\E\sum_k \norm{\vx_{k+1}-\vx_k}_2 &\leq \frac{t}{\eta}\sqrt{\eta^2\norm{\vc}^2 +\frac{\eta^{25/12}}{2-\eta^{3/4}}d}\to t\norm{c}.
\end{align*}
We observe that as $\eta$ gets smaller, the SGD trajectory becomes more and more turbulent as its length goes unbounded, while the SGDM trajectory becomes more and more smooth, though they have the same limit. Consequently, the turbulence of the training curve may not faithfully reflect the true stochasticity of the iterates as a whole, and may not be indicative of the quality of the obtained model with different choices of the momentum. 

\subsection{Main Results on Weak Approximations of SGDM}\label{sec:weak-appro-main-1}
The above warm-up example reminds us that SGD and SGDM trajectories may have different appearances that are irrelevant to the final distributions of the outcomes, which in our concern is mostly important. Therefore we need to talk about trajectory approximations in the correct mathematical metric. For our main results, we introduce the notion of weak approximations between two families of trajectories, inspired by~\citep{li2019stochastic}. We say a function $g(\bx):\R^d\to \R^m$ has polynomial growth if there are constants $k_1,k_2>0$ such that $\norm{g(\bx)}_2\leq k_1(1+\norm{\bx}_2^{k_2})$, $\forall\bx\in\RR^d$, and we say a function $g$ has all-order polynomial-growing derivatives if $g$ is $\mathcal{C}^\infty$ and $\nabla^\alpha g$ has polynomial growth for all $\alpha\geq 0$.

\begin{definition}[Order-$\gamma$ Weak Approximation]\label{def:weak_approx}
Two families of discrete trajectories $\vx^\eta_k$ and $\vy^\eta_k$ are weak approximations of each other, if there is $\eta_{\text{thr}}>0$ that for any $T>0$, any function $h$ of all-order polynomial-growing derivatives, and any $\eta\leq \eta_{\text{thr}}$, there is a constant $C_{h,T}$ independent of $\eta$ that
	$$ \max_{k=0,...,\lfloor T/\eta\rfloor} | \E h(\vx^\eta_k) - \E h(\vy^\eta_k) | \leq C_{h,T}\cdot\eta^\gamma. $$
\end{definition}
Weak approximation implies that $\vx_k^\eta$ and $\vy_k^\eta$ have similar distributions at any step $k\leq T/\eta$ even when $k\to\infty$ as $\eta\to 0$, and specifically in the deep learning setting it implies that the two training (testing) curves are similar.


In the small learning rate cases, we use the big-$O$ notations to specify the order of magnitudes as the learning rate scale $\eta\to 0$. Consider a SGDM run with hyperparameters $\{(\eta_k,\beta_k)\}_{k\geq 0}$. Let the magnitudes of the learning rates be controlled by a scalar $\eta$ as $\eta_k=O(\eta)$. Furthermore, to capture the asymptotic behaviour of the the momentum decay $\beta_k$, we set an index $\alpha\geq 0$ so that the decay rate of the momentum is controlled as $1-\beta_k=O(\eta^\alpha)$. $\alpha=0$ corresponds to a constant-scale decay schedule while $\alpha>0$ corresponds to a schedule where $\beta_k$ is closer to $1$ for smaller learning rates. Formally, we introduce the following denotation.
\begin{definition}\label{def:hpschedule} A (family of) hyperparameter schedule $\{\eta_k,\beta_k\}_{k\geq 1}$ is scaled by $\eta$ with index $\alpha$ if there are constants
$\eta_{\max}, \lambda_{\min}$ and $\lambda_{\max}$, independent of $\eta$, such that for all $k$,
\[0\leq \eta_k/\eta<\eta_{\max}, \quad 0<\lambda_{\min}\leq (1-\beta_k)/\eta^{\alpha}\leq\lambda_{\max}<1.\]
\end{definition}
We need the boundedness of the initial momentum for the SGDM trajectory to start safely.
\begin{assumption} For each $m\geq 1$, there is constant $C_{m}\geq 0$ that $\E (\norm{\vm_0}_2^m)\leq C_m$;\label{ass:init-bound}
\end{assumption}
Following \citet{malladi2022sdes}, we further assume that the NGOS satisfies the below conditions, which make the trajectory amenable to analysis. 

\begin{assumption}
    The NGOS $\cG_\sigma = (\cL, \mSigma, \DatZ_{\sigma})$ satisfies the following conditions.
    
    \begin{enumerate}[leftmargin=0.2in]
        \item \textbf{Well-Behaved}: $\nabla\cL$ is Lipschitz and $\cC^\infty$-smooth; $\mSigma^{1/2}$ is bounded, Lipschitz, and $\cC^\infty$-smooth; all partial derivatives of $\nabla\cL$ and $\mSigma^{1/2}$ up to and including the third order have polynomial growth.
        \item \textbf{Bounded Moments}: For all integers $m \ge 1$ and all noise scale parameters $\sigma$,
	there exists a constant $C_{2m}$ (independent of $\sigma$) such that
	$(\E_{\vv \sim
	\DatZ_{\sigma}(\vtheta)}[\norm{\vv}_2^{2m}])^{\frac{1}{2m}} \le
	C_{2m}(1 + \norm{\vtheta}_2)$, $\forall\vtheta \in \RR^d$.
    \end{enumerate}
    \label{assume:ngos}
\end{assumption} Given the above definitions, we are ready to establish our main result.
\begin{theorem}[Weak Approximation of SGDM by SGD]\label{thm:weak-appro-main}
    Fix the initial point $\vx_0$ , $\alpha\in [0,1)$, and an NGOS satisfying \Cref{assume:ngos}. Consider the SGDM update $\vx^\eta_k$ with schedule $\{(\eta_k,\beta_k)\}_{k\geq 1}$ scaled by $\eta$ with index $\alpha$, noise scaling $\sigma\leq \eta^{-1/2}$ and initialization $(\vm_0,\vx_0)$ satisfying \Cref{ass:init-bound}, then $\vx^\eta_k$ is an order-$(1-\alpha)/2$ weak approximation (\Cref{def:weak_approx}) of the SGD trajectory $\vz^\eta_k$ with initialization $\vz^\eta_0=\vx_0$, noise scaling $\sigma$ and learning rates $\bar{\eta}_k= \sum_{s=k}^\infty \eta_s \prod_{\tau=k+1}^s \beta_\tau(1-\beta_{k}).$

    Specifically, for a constant schedule where $(\eta_k=\eta,\beta_k=\beta)$ , $\bar{\eta}_k=\eta$. In this case, SGD and SGDM with the same learning rate weakly approximate each other at distance $O(\sqrt{\eta/(1-\beta)})$.
\end{theorem}
The theorem shows that when the learning rates has a small scale $\eta$, under reasonable momentum decay and reasonable gradient noise amplification, the outcomes obtained by SGDM and SGD are close in distribution over $O(1/\eta)$ steps. 
Specifically at the limit $\eta\to 0$, the outcomes will have the same distribution. Following  \citet{li2019stochastic}, if $\sigma = 1/\sqrt{\eta}$, then the limiting distribution can be described by the law of the solution $\bvx_t$ to an stochastic differential equation (SDE): 
$$\dd \bvx_t = -\lambda_t \nabla\Loss(\bvx_t) \dd t + \lambda_t \mSigma^{1/2}(\bvx_t) \dd \bvw_t.$$ under brownian motion $\bvw_t$
and some rescaled learning rate schedule $\lambda_t$. If $\sigma\ll 1/\sqrt{\eta}$, however, the limit will in general be the solution to the gradient flow ODE $\dd \bvx_t = -\lambda_t \nabla\Loss(\bvx_t) \dd t$ and the impact of gradient noises will be vanishing.

The theorem is built upon an infinite learning rate schedule $k=1,2\cdots \infty$. In the case where we wish to consider a finite schedule, we can apply the theorem after schedule extension by infinitely copying the hyperparameters at the last step. Besides, the theorem requires $\alpha\in [0,1)$, and the approximation grows weaker as $\alpha$ approaches 1. At $\alpha=1$, the two trajectories are no longer weak approximations of each other and have different limiting distributions. $\alpha>1$ yields undesirable hyperparameter schedules where excessively heavy momentum usually slows down or even messes up optimization. Further details are discussed in \Cref{discus}.

\section{The Limit of SGDM and SGD are identical in $O(1/\eta^2)$ Steps}\label{sec:limit-main}

In this section, we follow the framework from \citet{li2021happens} to study the dynamics of SGDM when the iterates are close to some manifold of local minimizers of $\cL$. Former analyses (e.g., \citet{yan2018unified}) suggest that on regular functions, SGDM and SGD will get close to a local minimizer in $o(1/\eta^2)$ steps, at which point the loss function plateaus and the trajectory random walks near the local minimizer. If the local minimizers connect an manifold in the parameter space, then the updates accumulate into a drift inside the manifold over $O(1/\eta^2)$ steps. \citet{li2021happens} shows that  under certain circumstances, the drift induces favorable generalization properties after the training loss reaches its minimum, by leading to minima with smaller local sharpness.

Therefore, by investigating this regime, we hope to detect the value of momentum in late-phase training, especially that concerning extra generalization benefits. Yet in this section, we show that when $\eta\to 0$, the limiting dynamic of SGDM admits the same form as that of SGD, suggesting that momentum provides no extra generalization benefits over at least $O(1/\eta^{2})$ steps of updates.

\subsection{Preliminaries on manifold of local minimizers}
We consider the case of optimizing an over-parameterized neural network, where usually the minimizers of the loss $\cL$ form manifolds. Let $\Gamma$ be a region of local minimizers that SGD can reach, and we will work mathematically in $\Gamma$ to see whether adding momentum changes the dynamical behaviors.

\begin{assumption}\label{assump:manifold}
    $\Loss$ is smooth. $\Gamma$ is a $(d-M)$-dimensional submanifold of $\RR^d$ for some integer $0\leq M\leq d$. 
    Moreover, every $\bx\in\Gamma$ is a local minimizer of $\cL$ with $\nabla\cL(\bx)=0$ and $\rank(\nabla^2\cL(\bx))=M$.
\end{assumption}
We consider a neighborhood $O_\Gamma$ of $\Gamma$ that $\Gamma$ is an attraction set of $O_\Gamma$ under $\nabla\cL$.
Specifically, we define the gradient flow under $\nabla\cL$ by $\phi(\bx,t) = \bx - \int_0^t\nabla\cL(\phi(\bx,s))\diff s$ for any $\bx\in\RR^d$ and $t\geq 0$.
We further define gradient projection map associated with $\nabla\cL$ as $\Phi(\bx) := \lim_{t\to\infty}\phi(\bx,t)$. 
\begin{assumption}\label{assump:neighborhood}
    From any point $\bx\in O_\Gamma$, the gradient flow governed by $\nabla\cL$ converges to some point in $\Gamma$, i.e., $\Phi(\bx)$ is well-defined and $\Phi(\bx) \in \Gamma$.
\end{assumption}

It can be shown that for every $\bx\in\Gamma$, $\partial\Phi(\bx)$ is the orthogonal projection onto the tangent space of $\Gamma$ at $\bx$.
Moreover, \citet{li2021happens} proved that for any initialization $\bx_0\in O_\Gamma$, a fixed learning rate schedule $\eta_k\equiv\eta$, and any $t>0$, time-rescaled SGD iterates $\bz_{\lfloor t/\eta^2\rfloor}$ converges in distribution to $\bvz_t$, the solution to the following SDE. We will refer to $\bvz_t$ as the slow SDE.
\begin{align}\label{eq:slowSDE-0}\bvz_t = \Phi(\vx_0)+\int_0^t \partial\Phi(\bvz_s)\mSigma^{1/2}(\bvz_s)\dd W_s +\int_0^t\frac{1}{2}\partial^2\Phi(\bvz_s)[\mSigma(\bvz_s)]\dd s.
\end{align}
Notice that $Z_t$ always stays in $\Gamma$ with $Z_0 = \Phi(\vx_0)$. Though $\vz_0=\vx_0$ is not in $\Gamma$, for any $t>0$ the limit of $\bz_{\lfloor t/\eta^2\rfloor}$ will fall onto $\Gamma$. 
\subsection{Analysis of SGDM via the slow SDE}
As the limiting dynamics of SGD iterates is known as above, we are curious about whether adding momentum modifies this limit. It turns out that within a fairly free range of hyperparameters, SGDM also has the same limiting dynamics. Specifically, for a family of hyperparameters $\{(\eta^{(n)}_k,\beta^{(n)}_k)\}_{k\geq 1}$ scaled by a series of scalars $\eta^{(n)}$ with index $\alpha$ (\Cref{def:hpschedule}, $\lim_{n\to\infty}\eta^{(n)}=0$), we will show that if the hyperparameter schedules converge as $n\to\infty$, then SGDM iterates will also converge into the limiting dynamics of SGD with the limiting learning rates, irrelevant of the momentum decay factors.

Similar to the setting in \citet{li2021happens}, we consider a fixed time rescaling $t = k(\eta^{(n)})^2$. We  stipulate that the schedule $\eta^{(n)}_k\to \eta^{(n)}\cdot\lambda_t$ as $n\to\infty$ for a rescaled schedule in continuous time $\lambda:[0,T]\to \R^+$. In the special case $\eta_k^{(n)}\equiv\eta^{(n)}$, it is clear that $\lambda_t\equiv 1$, and the setting of \Cref{eq:slowSDE-0} is recovered. Formally we introduce
\begin{assumption}\label{ass:conve-hpschedule}
    $\lambda_t: [0,T]\to \R^+$ has finite variation, and $$\lim_{n\to\infty}\eta^{(n)}\sum_{k=0}^{\lfloor T/(\eta^{(n)})^2\rfloor}|\eta^{(n)}_k-\eta^{(n)}\cdot\lambda_{k(\eta^{(n)})^2}|=0.$$
\end{assumption}
\begin{assumption}[Bounded variation]\label{ass:finite-var}
There is a constant $Q$ independent of $n$ such that for all $n$,
\[\sum_{k=1}^{\lfloor T/(\eta^{(n)})^2\rfloor}|\eta_k^{(n)}-\eta_{k-1}^{(n)}|\leq Q\eta^{(n)},\quad\sum_{k=1}^{\lfloor T/(\eta^{(n)})^2\rfloor}|\beta_k^{(n)}-\beta_{k-1}^{(n)}|\leq Q(\eta^{(n)})^{\alpha}\]
\end{assumption} 

In this general regime, we define the slow SDE on $\Gamma$ to admit the following description:
\begin{align}\label{eq:general_sde}
    \bvx_t = \Phi(\vx_0)+\int_0^t\lambda_t \partial\Phi(\bvx_s)\mSigma^{1/2}(\bvx_s)\dd W_s +\int_0^t\frac{\lambda_t^2}{2}\partial^2\Phi(\bvx_s)[\mSigma(\bvx_s)]\dd s.
\end{align}
Both SGDM and SGD converge to the above slow SDE on $\Gamma$, as summarized in the following theorem.
\begin{theorem} \label{thm:slow-sde-main}
Fix the initialization $\vx_0=\bz_0\in O_\Gamma$ and any $\alpha\in(0,1)$, and suppose the initial momentum $\bbm_0$ satisfies \Cref{ass:init-bound}. 
For $n\geq 1$, let $\{(\eta_k^{(n)},\beta_k^{(n)})\}_{k\geq 1}$ be any hyperparameter schedule scaled by $\eta^{(n)}$ with index $\alpha$,
satisfying \Cref{ass:conve-hpschedule,ass:finite-var}.
Fix the noise scale $\sigma^{(n)}\equiv 1$.
Under \Cref{assump:manifold,assump:neighborhood}, consider the SGDM trajectory $\{\vx_k^{(n)}\}$ with schedule $\{(\eta_k^{(n)}, \beta_k^{(n)})\}$, initialization $(\vx_0,\vm_0)$, and the SGD trajectory $\{\vz_k^{(n)}\}$ with schedule $\{\eta_k^{(n)}\}$, initialization $\vz_0=\vx_0$. 
Suppose the slow SDE defined in \eqref{eq:general_sde} has a global solution $\{\bX_t\}_{t\geq 0}$, then as $n\to\infty$ with $\eta^{(n)}\to0$, both $\bx_{\lfloor t/(\eta^{(n)})^2\rfloor}^{(n)}$ and $\bz_{\lfloor t/(\eta^{(n)})^2\rfloor}^{(n)}$ converge in distribution to $\bX_t$.
\end{theorem}

The proof of \Cref{thm:slow-sde-main} is inspired by \citet{calzolari1997limit}. Similarly in this regime, the momentum process $\vm_k^{(n)}$ behaves like an Uhlenbeck-Ornstein process with $O(\eta^\alpha)$ mixing variance,  so the per-step variance will be significantly smaller than that of SGD as is in \Cref{sec:warm-up}. To prove the result, a more careful expansion of the per-step change $\Phi(\vx_{k+1})-\Phi(\vx_{k})$ is needed. The proof is detailed in \Cref{sec:app_long_horizon}.
