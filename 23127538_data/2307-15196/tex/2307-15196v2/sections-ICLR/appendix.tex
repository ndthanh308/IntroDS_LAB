\newpage
\section{Additional Preliminaries}
\subsection{SGDM Formulations}
Recall \Cref{def:SGDM} where we defined SGDM as following: \begin{align}\label{equ:sgdm-2}
		\vm_{k+1} = \beta_{k}\vm_{k} + (1-\beta_{k})\vg_{k}, \qquad
		\vx_{k+1} = \vx_{k} - \eta_{k}\vm_{k+1}.
\end{align}
Our formulation is different from various SGDM implementations (e.g. pytorch \citep{paszke2019pytorch}) which admit the following form:
\begin{definition}[Standard formulation of SGD with momentum]\label{def:SGDM_standard}
Given a stochastic gradient $\bar{\vg}_k\sim\cG_\sigma(\bar{\vx}_k)$, SGD-Standard with the hyperparameter schedule $(\gamma_k,\mu_k,\tau_k)$ momentum updates the parameters $\bar{\vx}_k\in\RR^d$ from $(\bar{\vm}_0,\bar{\vx}_0)$ as
	\begin{align}
		\bar{\vm}_{k+1} &= \mu_{k}\bar{\vm}_{k} + (1-\tau_{k})\bar{\vg}_{k} \\
		\bar{\vx}_{k+1} &= \bar{\vx}_{k} - \gamma_{k}\bar{\vm}_{k+1}
	\end{align}	
 where $\tau_k\in [0,1)$.
\end{definition}

Notice that with $\tau_k=0$, $\mu_k = \beta$ and $\gamma_k=\gamma$, the standard formulation recovers \Cref{equ:SGDM-intro} that has commonly appeared in the literature. The standard formulation also recovers \Cref{equ:sgdm-2} with $\mu_k = \tau_k=\beta_k$ and $\eta_k=\gamma_k$. Actually, as we will show in the following lemma, these formulations are equivalent up to hyperparameter transformations. 
\begin{lemma}[Equivalence of SGDM and SGD-Standard]\label{lem:SGDM_standard}
    Let $\alpha_k$  be the sequence that $\alpha_0=1$ and 
    \[\alpha_{k+1} = \frac{\alpha_k}{\alpha_k(1-\tau_k)+\mu_k}.\]
    Then $0<\alpha_{k+1}\leq \frac{1}{1-\tau_k}$. For parameters schedules and initialization
    \[\beta_k =\frac{\alpha_{k+1}}{\alpha_k}\mu_k, \eta_k = \frac{\gamma_k}{\alpha_{k+1}}, \vm_0 = \bar{\vm}_0, \vx_0 = \bar{\vx}_0\] then $\bar{\vx}_k$ and $\vx_k$ follow the same distribution. $\vx_k$ is by \Cref{def:SGDM} and $\bar{\vx}_k$ by \Cref{def:SGDM_standard}.
\end{lemma}
\begin{proof}
    We shall prove that given $\vx_k=\bar{\vx_k}$,  $\vg_k=\bar{\vg}_k$ and 
    $\vm_k = \alpha_k\bar{\vm}_k$, there is and $\vm_{k+1} = \alpha_{k+1}\bar{\vm}_{k+1}$ and $\bar{\vx}_{k+1}=\vx_{k+1}$. This directly follows that $\alpha_{k+1}(1-\tau_{k})=1-\beta_k$, and
    \begin{align*}
        \bar{\vm}_{k+1} & = \mu_{k}\bar{\vm}_{k} + (1-\tau_{k})\bar{\vg}_{k}\\
        & = \frac{\mu_{k}}{\alpha_k}\vm_{k} + (1-\tau_{k})\vg_{k}\\
        & = \frac{1}{\alpha_{k+1}}(\beta_k\vm_{k} + \alpha_{k+1}(1-\tau_{k})\vg_{k})\\
        & = \frac{1}{\alpha_{k+1}}\vm_{k+1}.\\
        \bar{\vx}_{k+1} & = \bar{\vx}_{k} - \gamma_{k}\bar{\vm}_{k+1}\\
        & = \vx_{k} - \eta_k \vm_{k+1}.
    \end{align*}  
    Then we can prove the claim by induction that $\vx_k$ and $\bar{\vx}_k$ are identical in distribution. More specifically,
    we shall prove that the CDF for $(\vx_k,\vm_k,\vg_k)$ at $(\vx,\vm,\vg)$ is the same as the CDF for $(\bar{\vx}_k,\bar{\vm}_k,\bar{\vg}_k)$ at $(\vx,\frac{\vm}{\alpha_k},\vg)$.
    For $k=0$, the claim is trivial. If the induction premise holds for $k$, then let $(\vx_{k+1},\vm_{k+1},\vg_{k+1})$ be the one-step iterate in \Cref{equ:sgdm-2} from $(\vx_k,\vm_k,\vg_k)=(\vx,\vm,\vg)$ and $(\bar{\vx}_{k+1},\bar{\vm}_{k+1},\bar{\vg}_{k+1})$ be the one-step iterate in \Cref{def:SGDM_standard} from $(\bar{\vx}_k,\bar{\vm}_k,\bar{\vg}_k)=(\vx,\frac{\vm}{\alpha_k},\vg)$, then we know from above that    $\alpha_{k+1}\bar{\vm}_{k+1}=\vm_{k+1}$ and $\bar{\vx}_{k+1}=\vx_{k+1}$, and therefore the conditional distribution for $(\vx_{k+1},\vm_{k+1},\vg_{k+1})$ is the same as that for $(\bar{\vx}_{k+1},\alpha_{k+1}\bar{\vm}_{k+1},\bar{\vg}_{k+1})$. Integration proves the claim for $k+1$, and thereby the induction is complete.
\end{proof}

Therefore, in the rest part of the appendix, we will stick to the formulation of SGDM as \Cref{equ:sgdm-2} unless specified otherwise.

\subsection{Descent Lemma for SGD and SGDM}

In \Cref{lem:descent}, given the SGD updates $\{z_k\}$ with constant learning rate $\eta$, we 
 have the gradient descent lemma 
\begin{align}\label{eq:gdl1}      &\E[\Loss(\vz_{k+1})|\vz_k] - \Loss(\vz_{k})  =\\
     &\qquad\underbrace{ - \eta \norm{\nabla \Loss(\vz_{k})}^2}_\text{descent force} + \underbrace{\frac{1}{2}(\sigma\eta)^2\tr((\nabla^2\Loss)\mSigma(\vz_k))}_\text{noise-induced} + \underbrace{\frac{1}{2}\eta^2(\nabla\Loss^\top(\nabla^2\Loss)\nabla\Loss(\vz_k))}_\text{curvature-induced}+o(\eta^2, (\sigma\eta)^2).
    \end{align}

We can get a similar result for the SGDM updates $\{x_k\}$, namely, when $1-\beta\ll\eta$,
\begin{align}\label{eq:gdl2}
        \E\Loss(\vx_{k+1}) & = \E\Loss(\vx_{k}) - \eta \E\norm{\nabla \Loss(\vx_{k})}^2 \\
        &+ \frac{1}{2}\eta^2\E(\nabla\Loss^\top(\nabla^2\Loss)\nabla\Loss(\vx_k)+\frac{1}{2} (\sigma\eta)^2\tr((\nabla^2\Loss)\mSigma(\vx_k)))\\
        & + \frac{\eta\beta}{1-\beta}(S_{k+1}-S_k) + \eta^2\E [\vm_k^\top\nabla^2\Loss\nabla\Loss(\vx_k)] + o(\eta^2/(1-\beta),(\sigma\eta)^2/(1-\beta)).
    \end{align}
    where $S_k = \E[\nabla\Loss(\vx_k)^\top \vm_k - \frac{\eta}{2}\vm_k^\top \nabla^2\Loss(\vx_k)\vm_k]$.
    Notice that beside the terms in the SGD updates, additions are an $O(\eta/(1-\beta))$-order telescoping term and an $O(\eta^2)$ extra impact term. The impact of the telescoping term is large for per-step updates but is limited across the whole trajectory, so the difference between SGDM and SGD trajectory is actually depicted by the extra term $\eta^2\E [\vm_k^\top\nabla^2\Loss\nabla\Loss(\vx_k)]$. In the settings where the extra term is bounded, we can bound the difference between training curves of SGDM and SGD.
\begin{proof}[Brief Proof for \cref{lem:descent}]
    We assume $\norm{\nabla^n\Loss}$ is bounded ($\norm{\nabla^n\Loss}_\infty<C_n$) for any order of derivatives $n=0,1,2,3\cdots$ and the trajectories are bounded ($\E\norm{\vz_k}^m,\E\norm{\vx_k}^m\leq C_m$) to simplify the reasoning. In the SGD case, $z_{k+1}-z_k=-\eta g_k$ has moments
    \begin{align*}
        \E\norm{z_{k+1}-z_k}^{2m} & = \eta^{2m} \E\norm{g_k}^{2m}\\
        & \leq \eta^{2m} 2^{2m} \E (\norm{\nabla\Loss(z_k)}^{2m} + \sigma^{2m}\norm{\vv_k}^{2m})\\
        & = O(\eta^{2m} + (\sigma\eta)^{2m}).
    \end{align*} 
    Similarly,
    \begin{align*}
        \E\norm{x_{k+1}-x_k}^{2m} & = \eta^{2m} \E\norm{m_{k+1}}^{2m}\\
        & = \eta^{2m} \E\norm{\beta^{k+1} m_0 + \sum_{s=0}^{k}\beta^{k-s}(1-\beta) g_{s}}^{2m}\\
        & \leq \eta^{2m}2^{2m} \E\left(\beta^{2m(k+1)} \norm{m_0
        }^{2m}+\left(\sum_{s=0}^{k}\beta^{k-s}\right)^{2m-1}\left(\sum_{s=0}^{k}\beta^{k-s}(1-\beta)^{2m}\norm{g_{s}}^{2m}\right)\right)\\
        & = O(\eta^{2m} + (\sigma\eta)^{2m}).
    \end{align*} 
    Therefore $\E\norm{z_{k+1}-z_k}^3\leq \sqrt{\E\norm{z_{k+1}-z_k}^6} = o(\eta^2,(\sigma\eta)^2)$ and $\E\norm{x_{k+1}-x_k}^3\leq \sqrt{\E\norm{x_{k+1}-x_k}^6} = o(\eta^2,(\sigma\eta)^2)$.
    Taylor expansion gives
 \begin{align*}
    \E\Loss(z_{k+1})|z_k & = \Loss(z_k) + \E\nabla\Loss(z_k)^\top (z_{k+1}-z_k) + \frac{1}{2} \E(z_{k+1}-z_k)^\top\nabla^2\Loss(z_k)(z_{k+1}-z_k)+o(\eta^2,(\sigma\eta)^2).
 \end{align*}
 Expansion yields \Cref{eq:gdl1}. Similarly,
  \begin{align*}
    \E\Loss(x_{k+1}) & = \E[\Loss(x_k) + \nabla\Loss(x_k)^\top (x_{k+1}-x_k) + \frac{1}{2} (x_{k+1}-x_k)^\top\nabla^2\Loss(x_k)(x_{k+1}-x_k)]+o(\eta^2,(\sigma\eta)^2)\\
    & = \E[\Loss(x_k) - \eta\nabla\Loss(x_k)^\top m_{k+1} + \frac{\eta^2}{2} m_{k+1}^\top\nabla^2\Loss(x_k)m_{k+1}]+o(\eta^2,(\sigma\eta)^2)
 \end{align*}
 Since $m_{k+1} = g_k + \frac{\beta}{1-\beta}(m_k-m_{k+1})$, 
   \begin{align*}
    \E\Loss(x_{k+1}) 
    & = \E[\Loss(x_k) - \eta\nabla\Loss(x_k)^\top g_k - \eta\nabla\Loss(x_k)^\top \frac{\beta}{1-\beta}(m_k-m_{k+1})\\
    & + \frac{\eta^2}{2} m_{k+1}^\top\nabla^2\Loss(x_k)(g_k + \frac{\beta}{1-\beta}(m_k-m_{k+1}))]+o(\eta^2,(\sigma\eta)^2)\\
    & = \E[\Loss(x_k) - \eta\nabla\Loss(x_k)^\top g_k - \frac{\eta\beta}{1-\beta}(\nabla\Loss(x_k)^\top m_k-\nabla\Loss(x_{k+1})^\top m_{k+1})\\
    & + \frac{\eta\beta}{1-\beta}(m_{k+1}^\top\nabla^2\Loss(x_k)(x_k-x_{k+1}) + O(\eta^2,\sigma^3\eta^2))\\
    & + \frac{\eta^2}{2} m_{k+1}^\top\nabla^2\Loss(x_k)(g_k + \frac{\beta}{1-\beta}(m_k-m_{k+1}))]+o(\eta^2,(\sigma\eta)^2)\\
    & = \E[\Loss(x_k) - \eta\nabla\Loss(x_k)^\top g_k - \frac{\eta\beta}{1-\beta}(\nabla\Loss(x_k)^\top m_k-\nabla\Loss(x_{k+1})^\top m_{k+1})\\
    & + \frac{\eta^2}{2} (g_k + \frac{\beta}{1-\beta}(m_k-m_{k+1}))^\top\nabla^2\Loss(x_k)(g_k + \frac{\beta}{1-\beta}(m_k+m_{k+1}))]+\frac{o(\eta^2,(\sigma\eta)^2)}{1-\beta}.
 \end{align*}
 Expansion yields \Cref{eq:gdl2}.
\end{proof}

\section{The dynamics of SGDM in $O(1/\eta)$ time}

\input{sections-ICLR/SMDE-p1.tex}
\subsection{$\alpha\in [0,1)$}
\input{sections-ICLR/svag}
\subsection{$\alpha\geq 1$}\label{discus}
Following the idea of Stochastic Modified Equations \citep{li2019stochastic}, the limiting distribution can be described by the law of the solution $\bvx_t$ to an SDE under brownian motion $\bvw_t$
\begin{equation}
\dd \bvx_t = -\lambda_t \nabla\Loss(\bvx_t) \dd t + \lambda_t \mSigma^{1/2}(\bvx_t) \dd \bvw_t\label{equ:sgd-sde-1}
\end{equation}
for some rescaled learning rate schedule $\lambda_t$ that $\bar{\eta}_k\to \lambda_{k\eta}$ in the limit.

When $\alpha=1$, the limit distribution of SGDM becomes
\begin{align}
\dd \bvx_t & = \lambda_t/\gamma_t \dd \bvm_t -\lambda_t \nabla\Loss(\bvx_t) \dd t + \lambda_t \mSigma^{1/2}(\bvx_t) \dd \bvw_t\\
\dd \bvm_t & = -\gamma_t  \bvm_t \dd t + \gamma_t \nabla\Loss(\bvx_t) \dd t - \gamma_t \mSigma^{1/2}(\bvx_t) \dd \bvw_t\label{equ:sgd-sde-2}
\end{align}
with similarly $\gamma_t$ being the limit $(1-\beta_k)/\eta\to \gamma_{k\eta}$.
Here $\bvm_t$ is the rescaled momentum process that induces a non-negligible impact on the original trajectory $\bvx_t$.

Furthermore, when $\alpha>1$, if we still stick to following $O(1/\eta)$ steps for any $\eta$, then the dynamics of trajectory will become trivial. In the limit $\eta\to 0$, as $\vm_k-\vm_0=O(k\eta^{\alpha})=O(T\eta^{\alpha-1})\to 0$, the trajectory has limit $\vx_k = \vx_0 - \sum_{i=0}^{k-1}\eta_i \vm_0$ for all $k=O(1/\eta)$. This is different from the case $\alpha\leq 1$ where there is always a non-trivial dynamics in $\vx_k$ for the same time scale $k=O(1/\eta)$, regardless of the $\alpha$ index. We can think of the phenomenon by considering the trajectory of SGDM on a quadratic loss landscape, and in this case the SGDM behaves like a Uhlenbeck-Ornstein process. When $\alpha<1$, the direction of $\vx$ has a mixing time of $O(1/\eta)$ while the direction of $\vm$ has a shorter mixing time of $O(1/\eta^{\alpha})$, while when $\alpha>1$, both mixing time of $\vx$ and $\vm$ mixes at a time scale of $O(1/\eta^{\alpha})$, so in $O(1/\eta)$ steps the trajectory is far from any stationary states.

Therefore in this regime we should only consider the case where $\vm_0=0$ to avoid the trajectory moving too far. By rescaling $\vm$ and considering $O(\eta^{-\frac{1+\alpha}{2}})$ steps, we would spot non-trivial behaviours of the SGDM trajectory. In this case the SGDM have a different tolerance on the noise scale $\sigma$.
\section{The dynamics of SGDM in $O(1/\eta^2)$ time}\label{sec:app_long_horizon}
In this section we will present results that characterizes the behaviour of SGD and SGDM in $O(1/\eta^2)$ time. We call this setting the slow SDE regime in accordance with previous works~\citep{gu2023why}.
\subsection{Slow SDE Introduction}
There are a line of works that discusses the slow SDE regime that emerges when SGD is trapped in the manifold of local minimizers.
The phenomenon was introduced in~\citet{blanc2020implicit} and studied more generally in~\citet{li2021happens}.
In these works, the behavior of the trajectory near the manifold, found to be a sharpness minimization process for SGD, is thought to be responsible for the generalization behavior of the trajectory.

The observations in these works is that SGD should mimic a Uhlenbeck-Ornstein process along the \emph{normal} direction of the manifold of minimizers. Each stochastic step in the normal direction contributes a very small movement in the tangent space. Over a long range of time, these small contributions accumulate into a drift.

To overcome the theoretical difficulties in analyzing these small contributions,~\citet{li2021happens} analyzed a projection $\Phi$ applied to the iterate that maps a point near the manifold to a point on the manifold. $\Phi(X)$ is chosen to be the limit of gradient flow when starting from $X$. Then it is observed that when the learning rate is small enough, $\Phi(X)\approx X$, and the dynamics of $\Phi(X)$ provides an SDE on the manifold that marks the behaviour of SGD in this regime.
\subsection{Slow SDE Preliminaries}
\subsubsection{The Projection Map}
We consider the case where the local minimizers of the loss $\cL$ form a manifold $\Gamma$ that satisfy certain regularity conditions 
as \Cref{assump:manifold}. In this section, we fix a neighborhood $O_\Gamma$ of $\Gamma$ that is an attraction set under $\nabla\cL$, and define  $\phi(\bx,t) = \bx - \int_0^t\nabla\cL(\phi(\bx,s))\diff s$ and $\Phi(\bx) := \lim_{t\to\infty}\phi(\bx,t)$. $\Phi(\vx)$ is well-defined for all $\vx\in O_\Gamma$ as indicated by \Cref{assump:neighborhood}. We call $\Phi$ the gradient projection map.

The most important property of the projection map is that its gradient is always orthogonal to the direction of gradient. We ultilize the following lemma from previous works.
\begin{lemma}[\citet{li2021happens} Lemma C.2]\label{lem:gra-proj-1}
    For all $\vx\in O_\Gamma$ there is
    $\partial\Phi(\vx)\nabla\Loss(\vx)=0$.
\end{lemma}

For technical simplicity, we choose compact set $K\subset O_\Gamma$ and only consider the dynamics of $\vx_k$ within the set $K$. Formally, for any dynamics $\vx_k$ with $\vx_0\in \mathring{K}$, define the exiting stopping time $\tau=\min_k\{k>0:\vx_{k+1}\not\in K\}$, and the truncated process $\hatvx_k=\vx_{\min(k,\tau)}$; for any dynamics $\bvx_t$ in continuous time with $\bvx_0\in \mathring{K}$, define the exiting stopping time $\tau=\inf\{t>0:\bvx_{t}\not\in \mathring{K}\}$, and the truncated process $\hatbvx_t=\bvx_{\min(t,\tau)}$.

There are a few regularity conditions Katzenberger proved in the paper~\citet{katzenberger1991solutions}:
\begin{lemma}\label{lem:katzen-results}
There are the following facts.
\begin{enumerate}
\item If the loss $\Loss$ is smooth, then $\Phi$ is third-order continuously differentiable on $O_\Gamma$.
\item For the distance function $d(\vx,\Gamma)=\min_{\vy\in \Gamma\bigcap K}\norm{\vy-\vx}$, there exists a positive constant $C_K$ that 
\[\norm{\Phi(\bvx)-\bvx}\leq  C_K d(\bvx, \Gamma)\]
for any $\bvx\in K$.
\item There exists a Lyaponuv function $h(\bvx)$ on $K$ that
\begin{itemize}
    \item $h:K\to [0,\infty)$ is third-order continuously differentiable and $h(\bvx)=0$ iff $\bvx\in \Gamma$.
    \item For all $\bvx\in K$, $h(\bvx)\leq c\dotp{\nabla h(\bvx)}{\nabla \Loss(\bvx)}$ for some constant $c>0$.
    \item $d^2(\bvx, \Gamma)\leq c' h(\bvx)$ for some constant $c'>0$.
\end{itemize}
\end{enumerate}
\end{lemma}
\subsubsection{The Katzenberger Process}
We recap the notion of \emph{Katzenberger processes} in~\citet{li2021happens} and the characterization of the corresponding limiting diffusion based on Katzenberger's theorems~\citep{katzenberger1991solutions}.

\begin{definition}[Uniform metric]\label{def: uniform metric}
The \emph{uniform metric} between two functions $f,g:[0,\infty)\to \RR^D$ is defined to be $d_U(f,g) = \sum_{T=1}^\infty 2^{-T} \min\{1, \sup_{t\in[0,T)} \|f(t) - g(t)\|_2\}$.
\end{definition}

For each $n\in\NN$, let $A_n:\RR_+\to\RR_+$ be a non-decreasing functions with 
$A_n(0)=0$, and $\{Z_n(t)\}_{t\geq0}$ be a $\RR^\Xi$-valued stochastic process.
In our context of SGD, given loss function $\Loss:\RR^D\to\RR$, noise function $\sigma:\RR^D\to 
\RR^{D\times \Xi}$ and initialization $\xinit\in U$, we call the following stochastic process~\eqref{eq:katzenberger_process} a \emph{Katzenberger process} 
\begin{align}\label{eq:katzenberger_process}
    X_n(t) =\xinit + \int_0^t \sigma(X_n(s)) \diff Z_n(s)  - \int_0^t 
    \nabla \Loss(X_n(s)) \diff A_n(s)
\end{align}
if as $n\to \infty$ the following conditions are satisfied:
    \begin{enumerate}
        \item $A_n$ increases infinitely fast, i.e., $\forall
        \epsilon>0,\inf_{t\geq 0} (A_n(t+\epsilon)- A_n(t))\to\infty$;

        \item $Z_n$ converges in distribution to 
       $Z$ in uniform metric.
    \end{enumerate}

\begin{theorem}[Adapted from Theorem B.7 in \citet{li2021happens}]
\label{thm:previous_thm}
Given a  Katzenberger process $\{X_n(\cdot)\}_{n\in\NN}$, if SDE \eqref{eq:limiting_SDE_general} has a global solution $Y$ in $U$ with $Y(0)=\Phi(\xinit)$, then for any $t>0$, $X_n(t)$ converges in distribution to 
    $Y(t)$ as $n\to\infty$.
    
    \vspace{-0.6cm}
    \begin{align}\label{eq:limiting_SDE_general}
        Y(t) &= \Phi(\xinit) 
        + \int_0^t \partial\Phi(Y(s)) \sigma(Y(s)) \diff Z(s)\notag\\
        &\qquad + \int_0^t \frac{1}{2}\sum_{i,j\in [D]}\sum_{k,l\in[\Xi]}\partial^2_{ij}\Phi(Y(s))\sigma_{ik}(Y(s))\sigma_{jl}(Y(s))] \diff [Z]_{kl}(s).
    \end{align} 
    \vspace{-0.6cm}
    
\end{theorem}
We note that the global solution always exists if the manifold $\Gamma$ is compact. For the case 
 where $\Gamma$ is not compact, we introduce a compact neighbourhood of $\Gamma$ and a stopping time later for our result.
 Our formulation is under the original framework of \citet{katzenberger1991solutions} and the proof in \citet{li2021happens} can be easily adapted to \Cref{thm:previous_thm}.

\subsubsection{The C\`{a}dl\`{a}g Process}
A c\`{a}dl\`{a}g process is a right continuous process that has a left limit everwhere.
For real-value c\`{a}dl\`{a}g semimartingale processes $X_t$ and $Y_t$, define $X_{t-}=\lim_{s\to t-}X_s$, and $\int_a^b X_s dY_s$ to be the process $\int_{a+}^{b+} X_{s-} dY_s$ for interval $a<b$. That is in the integral we do not count the jump of process $Y_s$ at $s=a$ but we count the jump at $s=b$. Then it's easy to see that
\begin{itemize}
    \item $\int_a^b X_s \diff Y_s + \int_b^c X_s \diff Y_s = \int_a^c X_s \diff Y_s$ for $a<b<c$.
    \item $Z_t = \int_0^t X_s \dd Y_s$ is a c\`{a}dl\`{a}g  semimartingale if both $X_s$ and $Y_s$ are c\`{a}dl\`{a}g  semimartingales.
\end{itemize}


By a extension to higher dimensions, let $$\dd[X]_t = \dd (X_t X_t^\top) -  X_{t} (\dd X_t)^\top - (\dd X_{t}) X_t^\top$$ and  $$\dd[X, Y]_t = \dd (X_t Y_t^\top) -  X_{t} (\dd Y_t)^\top - (\dd X_{t}) Y_t^\top$$,

we know $[X]_t$ and $[X,Y]_t$ are actually matrix-valued processes with $\Delta [X]_t = (\Delta X_t) (\Delta X_t)^\top$ and $\Delta [X, Y]_t = (\Delta X_t) (\Delta Y_t)^\top$.

The generalized Ito's formula applies to a c\`{a}dl\`{a}g  semimartingale process 
(let $\partial^2 f(X)[M] = \sum_{i,j} M_{ij}\partial_{ij}f(X) $ for any matrix $M$) is given as
\[\dd f(X_t) = \dotp{\partial f(X_t)} {\dd X_t} + \frac{1}{2}\partial^2 f(X_t) [d[X]_t] + \Delta f(X)_t - \dotp{\partial f(X_{t-})}{\Delta X_t} - \frac{1}{2}\partial^2 f(X_{t-})[\Delta X_t \Delta X_t^\top].\]

And integration by part
\[\dd (XY)_t = X_t (dY_t) + (dX_t) Y_t + d[X,Y]_t.\]

These formulas will be useful in our proof of the main theorem.

\subsubsection{Weak Limit for  C\`{a}dl\`{a}g Processes}
As we are showing the weak limit for a c\`{a}dl\`{a}g process as the solution of an SDE, the following theorem is useful. We use $\mathcal{C}([0,T],\R^{d})$ to denote the set of c\`{a}dl\`{a}g functions $X:[0,T]\to \R^{d}$. 
\begin{theorem}[Theorem 2.2 in \citet{kurtz1991weak}]\label{thm:weak-limit}
    For each $n$, let $\bvx^{(n)}_t$ be a processes with path in $\mathcal{C}([0,T],\R^{d\times m})$ and let $\bvy^{(n)}_t$ be a semi-martingale with sample path in $\mathcal{C}([0,T],\R^{m})$ respectively. Define function $h_\delta(r)=(1-\delta/r)^+$ and $\tilde{Y}^{(n)}_t = Y^{(n)}_t-\sum_{s\leq t}h_\delta(|\Delta Y^{(n)}_s|)\Delta Y^{(n)}_s$ be the process with reduced jumps. Then $\tilde{Y}^{(n)}_t$ is also a semi-martingale. If the expected quadratic variation of $\tilde{Y}^{(n)}_t$ is bounded uniformly in $n$, and
    $(X_n,Y_n)\to (X,Y)$ in distribution under the uniform metric (\Cref{def: uniform metric}) of $\mathcal{C}([0,T],\R^{d\times m}\times \R^{m})$, then
    \[(X_n,Y_n,\int X_n \dd Y_n)\to (X,Y, \int X \dd Y)\]
    in distribution under the uniform metric of $\mathcal{C}([0,T],\R^{d\times m}\times \R^{m}\times \R^{d})$.    
\end{theorem}
Therefore if $X^{(n)}$ is the solution of an SDE $X^{(n)}_t=X_0 +Z^{(n)}_t + \int_0^t F^{(n)}(X^{(n)}_s) \dd Y^{(n)}_s $, and if the tuple $(F^{(n)}(X_s),Y^{(n)}_s,Z^{(n)}_t)$ converges for all $X_s$ to the process $(F(X_s),Y_s,0)$, then by the above theorem we know the processes $(X^{(n)}, Y^{(n)},Z^{(n)})$ are relative compact, and the solution to the SDE $X_t = X_0 + \int_0^t F(X_s) \dd Y_s$ is the limit of $X^{(n)}_t$, as stated rigorously in Theorem 5.4, \citet{kurtz1991weak}. This will be the main tool in finding the limiting dynamics of SGDM.

\subsection{The Main Results}
We provide a more formal version of \Cref{thm:slow-sde-main}.
\begin{theorem} \label{thm:slow-sde-formal}Fix a compact set $k\subset O_\Gamma$, an initialization $\vx_0\in K$ and $\alpha\in(0,1)$. Consider the SGDM trajectory $(\vx_k^{(n)})$ with hyperparameter schedule $(\eta_k^{(n)}, \beta_k^{(n)})$ scaled by $\eta^{(n)}$, noise scaling $\sigma^{(n)}=1$ and initialization $(\vx_0,\vm_0)$ satisfy \Cref{ass:init-bound}; SGD trajectory $(\vz_k^{(n)})$ with learning rate schedule $(\eta_k^{(n)})$, noise scaling $1$ and initialization $\vz_0=\vx_0$. Furthermore the hyperparameter schedules satisfy \Cref{ass:conve-hpschedule,ass:finite-var}. 
Define the process $\bvx^{(n)}_t= \vx_{\lfloor t/(\eta^{(n)})^2\rfloor}-\phi(\vx_0,t/\eta^{(n)})+\Phi(\vx_0)$ and $\bvz^{(n)}_t= \vz_{\lfloor t/(\eta^{(n)})^2\rfloor}-\phi(\vz_0,t/\eta^{(n)})+\Phi(\vz_0)$, and stopping time $\tau_n=\inf\{t>0: \bvx^{(n)}_t\not\in K\}, \psi_n=\inf\{t>0: \bvz^{(n)}_t\not\in K\}$. Then the processes $(\bvx_{t\wedge\tau_n}^{(n)},\tau_n)$ and $(\bvz_{t\wedge\psi_n}^{(n)},\psi_n)$ are relative compact in $\mathcal{C}([0,T],\R^d)\times[0,T]$ with the uniform metric and they have the same unique limit point $(\bvx_{t\wedge\tau},\tau)$ such that $\bvx_{t\wedge\tau}\in \Gamma$ almost surely for every $t>0$, $\tau=\inf\{t>0: \bvx_t\not\in K\}$, and 
\[\bvx_t = \Phi(\vx_0)+\int_0^t\lambda_t \partial\Phi(X_s)\mSigma^{1/2}(X_s)\dd W_s +\int_0^t\frac{\lambda_t^2}{2}\partial^2\Phi(X_s)[\mSigma(X_s)]\dd s.\]
\end{theorem}

Note that for a sequence $\vx_k$ we are considering the sequence
$\bvx_t= \vx_{\lfloor t/(\eta^{(n)})^2\rfloor}-\phi(\vx_0,t/\eta^{(n)})+\Phi(\vx_0)$ after rescaling time $k=\lfloor t/(\eta^{(n)})^2\rfloor$. This adaptation is due to technical difficulty that the limit of $\vx^{(n)}_k$ will lie on the manifold $\Gamma$ for any $t>0$, but $\vx^{(n)}_0=\vx_0\not\in \Gamma$, so the limiting process will be continuous anywhere but zero. For the process $\bvx^{(n)}_t= \vx_{\lfloor t/(\eta^{(n)})^2\rfloor}-\phi(\vx_0,t/\eta^{(n)})+\Phi(\vx_0)$ however, we know $\bvx^{(n)}_t\to \vx_{\lfloor t/(\eta^{(n)})^2\rfloor}$ for any $t>0$ and  $\bvx^{(n)}_0= \Phi(\vx_{0})\in\Gamma$, thereby we make the limit a continuous process while preseving the same limit for all $t>0$.

\input{sections-ICLR/SMDE-p22.tex}
