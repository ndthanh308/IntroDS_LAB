\section{Preliminaries}

Consider optimizing a loss function $\cL(\vtheta) = \frac{1}{\Xi} \sum_{i = 1}^{\Xi} \cL_i(\vtheta)$ where $\cL_i:\RR^d\to\RR$ corresponds to the loss on the $i$-th sample.
We use $\vtheta$ to indicate parameters along a general trajectory.
In each step, we sample a random minibatch $\cB \subseteq [\Xi]$, and compute the gradient of the minibatch loss $\cL_\cB(\vtheta) = \frac{1}{|\cB|} \sum_{i\in\cB} \cL_i(\vtheta)$ to get the following noisy estimate of $\nabla \cL(\btheta)$, i.e.,
    $\nabla\cL_\cB(\btheta) = \frac{1}{|\cB|} \sum_{i\in\cB} \nabla\cL_i(\btheta)$.
It is easy to check that the noise covariance matrix of $\nabla \cL_{\cB}(\btheta)$, namely 
$\E_{\mathcal{B}} (\nabla
\Loss_{\mathcal{B}}(\vtheta) - \nabla \Loss(\vtheta))(\nabla \Loss_{\mathcal{B}}(\vtheta) - \nabla
\Loss(\vtheta))^\top$,
scales proportionally to $\frac{1}{|\cB|}$. Motivated by this, \citet{malladi2022sdes}
abstracts $\nabla \cL_{\cB}(\btheta)$ as sampled from a noisy gradient oracle where the noise covariance only depends on a scale parameter. 
\begin{definition}[NGOS,~\citet{malladi2022sdes}] \label{def:NGOS}
    A {\em Noisy Gradient Oracle with Scale Parameter} (NGOS) is
    characterized by a tuple $\cG_{\sigma} = (\cL, \mSigma, \DatZ_{\sigma})$. For a
    scale parameter $\sigma > 0$, $\cG_{\sigma}$ takes as input $\vtheta$ and returns $\vg
    =\nabla \cL(\vtheta) + \sigma \vv$, where $\nabla \cL(\vtheta)$ is the gradient of
    $\cL$ at $\vtheta$ and $\vv$ is the gradient noise drawn from the probability distribution
    $\DatZ_{\sigma}(\vtheta)$ with mean zero and covariance matrix $\mSigma(\vtheta)$.
    $\mSigma(\vtheta)$ is independent of the noise scale $\sigma$.
    Slightly abusing the notation, we also use $\cG_{\sigma}(\vtheta)$ to denote the distribution of $\vg$ given $\sigma$ and $\vtheta$.
\end{definition}

In the above minibatch setting, we have noise scale $\sigma=\frac{1}{|\mathcal{B}|}$. Generally, we invoke NGOS with bigger $\sigma$ for smaller magnitudes of the learning rates. Such scaling is in compliance with the Linear Scaling Rule ~\citep{goyal2017accurate} and is discussed further after \Cref{lem:descent}.
We now instantiate the SGD and SGDM trajectories under this noise oracle.

\begin{definition}[Vanilla SGD]
	Given a stochastic gradient oracle $\cG_\sigma$, SGD with the learning rate schedule $\{\bar{\eta}_k\}$ updates the parameters $\vz_k\in\RR^d$ from initialization $\vz_0$, as
\begin{align}
    \vz_{k+1} = \vz_k - \bar{\eta}_k\vg_k, \qquad \vg_k\sim\cG_\sigma(\vz_k).\label{def:sgd}
\end{align}	
\end{definition}
\begin{definition}[SGD with Momentum/SGDM]\label{def:SGDM}
Given oracle $\cG_\sigma$, SGDM with the hyperparameter schedule $\{(\eta_k,\beta_k)\}$, where $\beta_k\in (0,1)$, updates the parameters $\vx_k\in\RR^d$ from $(\vm_0,\vx_0)$,  as
\begin{align}\label{dqu:sgdm-1}
		\vm_{k+1} = \beta_{k}\vm_{k} + (1-\beta_{k})\vg_{k}, \qquad
		\vx_{k+1} = \vx_{k} - \eta_{k}\vm_{k+1},  \qquad\vg_k\sim\cG_\sigma(\vx_k).
	\end{align}
\end{definition}
Notice that the formulation of SGDM in \Cref{def:SGDM} is different from \eqref{equ:SGDM-intro} that sometimes appear in previous literature. In our results, \Cref{def:SGDM} offers a more natural parameterization for the comparison between SGDM and SGD. An easy conversion is given by rewriting \Cref{dqu:sgdm-1} as: 
\[\vx_{k+1} = \vx_k - \eta_k(1-\beta_k) \vg_k + \beta_k\frac{\eta_k}{\eta_{k-1}}(\vx_k-\vx_{k-1}).\]
Then setting $\eta_k=\frac{\gamma}{1-\beta}$ and $\beta_k=\beta$ recovers the form of  \eqref{equ:SGDM-intro}.  %in our results.

Modeling the gradient noise as an NGOS gives us the flexibility to scale the noise in our theoretical setting to make the effect of noise non-vanishing in small learning rate training, as observed in \Cref{lem:descent}, a variant of the standard gradient descent lemma for SGD. 
\begin{proposition}[Descent Lemma for SGD]\label{lem:descent}
    Given $\vz_k$, the expected change of loss in the next step is
    \begin{align*}        &\E[\Loss(\vz_{k+1})|\vz_k] - \Loss(\vz_{k})  =\\
     &\qquad\underbrace{ - \eta \norm{\nabla \Loss(\vz_{k})}^2}_\text{descent force} + \underbrace{\frac{1}{2}(\sigma\eta)^2\tr((\nabla^2\Loss)\mSigma(\vz_k))}_\text{noise-induced} + \underbrace{\frac{1}{2}\eta^2(\nabla\Loss^\top(\nabla^2\Loss)\nabla\Loss(\vz_k))}_\text{curvature-induced}+o(\eta^2, (\sigma\eta)^2).
    \end{align*}
\end{proposition}
\Cref{lem:descent} highlights noise-induced and curvature-induced factors that prevent the loss to decrease. For regular loss functions and small learning rates, the following phenomenon are expected.
\begin{itemize}
    \item In $O(\eta^{-1})$ steps, only for $\sigma= O(1/\sqrt{\eta})$, the loss is guaranteed to decrease for small $\eta$, during which the curvature-induced factor accumulates vanishing $o(1)$ impact as $\eta\to 0$. For $\sigma= \Theta(1/\sqrt{\eta})$, the noise-induced impact is on the same order as the descent force and will not be vanishing on the training curve, so noise affects the curve similarly across different learning rates. For $\sigma= o( 1/\sqrt{\eta})$, the noise-induced impact will be vanishing as $\eta\to 0$.
    \item Assume $\tr((\nabla^2\Loss) \mSigma(\vz_k))$ is non-vanishing as $\eta\to 0$. The loss plateaus at value $O(\eta\sigma^2)$ when the impacts balance each other, and the noise-induced impact is significant until $O((\sigma\eta)^{-2})$ steps of updates.
\end{itemize}
Inspired by these observations, we are interested in studying the behavior of SGDM in two regimes, for $O(\eta^{-1})$ steps of update with $\sigma\leq 1/\sqrt{\eta}$, and for $O(\eta^{-2})$ steps of update with $\sigma\leq 1$. The two regimes capture the common practices where people use noisy small-batch updates to train a model from scratch and then reduce the noise-induced impact after the training loss plateaus (usually by annealing the learning rate) in pursuit of a model that converges and generalizes better. 
