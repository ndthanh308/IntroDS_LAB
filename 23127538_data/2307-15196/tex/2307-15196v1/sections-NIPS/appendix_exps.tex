\section{Experimental Details}
\subsection{Language Model Fine-Tuning}
We fine-tune RoBERTa-large~\citep{liu2019roberta} on several tasks using the code provided by~\citet{malladi2023kernelbased}.
We are interested in comparing SGD and SGDM, so we use a coarser version of the learning rate grid proposed for fine-tuning masked langauge models with SGD in~\citep{malladi2023kernelbased}. 
We randomly sample $512$ examples per class using $5$ seeds, following the many shot setting in~\citet{gao-etal-2021-making}.
Then, we fine-tune for $4$ epochs with batch sizes $2, 4,$ and $8$ and learning rates $1\mathrm{e}-4$, $1\mathrm{e}-3$, and $1\mathrm{e}-2$. 
We select the setting with the best dev set performance and report its performance on a fixed $1000$ test examples subsampled from the full test set, which follows~\citep{malladi2023kernelbased}. 

\subsection{CIFAR-10 Experiments}
We report the results of training ResNet-32 on CIFAR-10 with and without momentum. 
First, we grid search over learning rates between $0.1$ and $12.8$, multiplying by factors of 2, to find the best learning rate for SGD with momentum.
We find this to be $\eta=0.2$.
Then, we run SGD and SGDM with SVAG to produce the figure. We adopt the SGDM formulation \Cref{def:sgdm} by setting $\gamma=\frac{\eta}{1-\beta}$ with $\beta=0.9$.

Standard SGD and SGDM exhibit different test accuracies ($89.35\%$ vs $92.68\%$), suggesting that momentum exhibits a different implicit regularization than SGD.
However, as we increase the gradient noise by increasing $\ell$ in SVAG (\Cref{def:svag}), we see that the two trajectories get closer.
At $\ell=4$, the final test accuracies for SGD and SGDM are $91.96\%$ and $91.94\%$, respectively, verifying our theoretical analysis.
