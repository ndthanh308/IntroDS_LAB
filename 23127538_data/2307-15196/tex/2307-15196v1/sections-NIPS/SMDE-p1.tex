In this section we will prove \Cref{thm:weak-appro-main} as the first part of our main result. First we shall take a closer examination at our update rule in \Cref{def:sgdm}. Let $\beta_{s:t}=\prod_{r=s}^t \beta_r$ be the product of consecutive momentum hyperparameters for $s\leq t$, and define $\beta_{s:t}=1$ for $s>t$, we get the following expansion forms.
\begin{lemma}\label{lem:unrollm}
    \begin{align*}
        \vm_k & =\beta_{0:k-1} \vm_0 + \sum_{s=0}^{k-1}\beta_{s+1:k-1}(1-\beta_{s}) \vg_{s}\\
        \vx_k & = \vx_0 - \sum_{s=0}^{k-1}\eta_s\beta_{0:s}\vm_0 - \sum_{s=0}^{k-1}\sum_{r=s}^{k-1}\eta_r\beta_{s+1:r}(1-\beta_{s})\vg_s\\
    \end{align*}
\end{lemma}
\begin{proof}
    By expansion of \Cref{dqu:sgdm-1}.
\end{proof}
%The update rule is
%\[\vx_{k}=\vx_{k-1}-\eta_k\beta_{1:k} \vm_0 -\eta_k \sum_{s=0}^{k-1}\beta_{s+2:k}(1-\beta_{s+1}) \vg_{s}.\]
%Or equivalently,

%\[\vx_{k}=\vx_{0}-\sum_{s=1}^k\eta_s\beta_{1:s} \vm_0 -\sum_{s=0}^{k-1}\sum_{t=s+1}^k\eta_t \beta_{s+2:t}(1-\beta_{s+1}) \vg_{s}.\]
Let the coefficients $c_{s,k}=\sum_{r=s}^{k-1}\eta_r\beta_{s+1:r}(1-\beta_{s})$ so that $\vx_k=\vx_0 - \frac{\beta_0 c_{0:k}}{1-\beta_0}\vm_0 -\sum_{s=0}^{k-1} c_{s,k}\vg_s$. Notice that $c_{s,k}$ is increasing in $k$ but it is always upper-bounded as $c_{s,k}\leq c_{s,\infty}=\sum_{r=s}^{\infty}\eta_r\beta_{s+1:r}(1-\beta_{s})\leq (\sup_{t}\eta_t)\sum_{r=s}^{\infty}(\sup_t\beta_t)^{r-s}=\frac{\sup_{r}\eta_r}{1-\sup_t\beta_t}$. Furthermore, if for any $t$, $\beta_t$ and $\eta_t$ are about the same scale, then we should expect that the difference $c_{s,\infty}-c_{s,k}$ is exponentially small in $k$. Then we can hypothesize that the trajectory $\vx_k$ should be close to a trajectory of the following form:
\[\tilde{\vx}_k=\tilde{\vx}_0 - \frac{\beta_0 c_{0:\infty}}{1-\beta_0}\vm_0 -\sum_{s=0}^{k-1} c_{s,\infty}\tilde{\vg}_s\]
which is exactly a SGD trajectory given the learning rate schedule $c_{s,\infty}$.

To formalize the above thought, we define the averaged learning rate schedule
\begin{equation}\label{equ:averaged-lr}
    \bar{\eta}_k= c_{k,\infty}=\sum_{s=k}^\infty \eta_s  \beta_{k+1:s}(1-\beta_{k})
\end{equation}
And the coupled trajectory
\begin{align}\label{equ:coupled-traj}
    \vy_{k} & =\vx_{k}-\frac{\bar{\eta}_k\beta_k}{1-\beta_k}\vm_k\\
    & = \vx_0 - \frac{\beta_0 \bar{\eta}_0}{1-\beta_0}\vm_0 -\sum_{s=0}^{k-1} \bar{\eta}_s\vg_s.
\end{align}
Then there is the following transition:
\begin{align}
    \vy_{k} =\vy_{k-1}-\bar{\eta}_{k-1}\vg_{k-1}
\end{align}
$\vy_k$ has an interesting geometrical interpretation as the endpoint of SGDM if we cut-off all the gradient signal from step $k$. E.g., $\vx_\infty$ if we set $\vg_{k+1}=\vg_{k+2}=\cdots = 0$ for and update the SGDM from ($\vx_k$,$\vm_k$).
%Then \[\vy_k =\vx_{k}-\frac{\bareta_{k+1}}{1-\beta_{k+1}}(\beta_{1:k+1} \vm_0 +\sum_{s=0}^{k-1}\beta_{s+2:k+1}(1-\beta_{s+1})\vz_{s}). \]
%And
%\[\vy_{k} =\vy_{k-1}-\bareta_{k}g_{k-1}?. \]

\iffalse
\begin{definition}[Order-1 Approximation]
Fix any initialization $\vx^\eta_0=\vy^\eta_0=\vx$, $\vx_t^\eta$ and $\vy_t^\eta$ are order-$c$ approximation of one another in time $T$ when $\eta\to 0$, if for any time $0\leq t\leq T$ and any function $g$ of polynomial growth, there is constant $K(\vx,t)$ independent of $\eta$ such that
\[\E |g(\vx_t^\eta)-g(\vy_t^\eta)|\le K(\vx,t)\eta.\]
\end{definition}
\begin{lemma}
$\vy_k$  is an order-1 approximation of $\vx_k$.
\end{lemma}
\begin{lemma}
$\vy_k$  is an order-1 approximation of $\barvy_k$, where $\barvy_k$ is updating through iterating
\[\barvy_{k+1}=\barvy_{k} -\eta(\nabla \Loss(\barvy_k) + \sqrt{\Xi}\sigma_{\vxi_k}(\barvy_k))\]
\end{lemma}
\begin{lemma}
$\barvy_k$  is an order-1 approximation of $\bbarvy_{k\eta}$, where $\bbarvy_{t}$ is given by SDE
\[\dd\bbarvy_t=\nabla \Loss(\bbarvy_t)\dd t + \sqrt{\eta}\sigma(\bbarvy_t)\dd W_t\]
\end{lemma}
\begin{theorem}
    SGDM $\vx_k$ and SGD $\barvx_k$ are order-$(1-\alpha)$ weak approximations of each other in $k=O(1/\eta)$ time.
\end{theorem}

\begin{theorem}
    $\vx_k$  is an order-1 approximation of $\bvx_{k\eta}$, where $\bvx_{t}$ is given by SDE
\[\dd\bvx_t=\nabla \Loss(\bvx_t)\dd t + \sqrt{\eta}\sigma(\bvx_t)\dd W_t\]
\end{theorem}
\fi