
\section{Weak Approximation of SGDM by SGD in $O(1/\eta)$ Steps}\label{sec:weak_approx}

In the following two sections, we will present our main theoretical results on SGDM with small learning rate.
In this section, we show that in $O(1/\eta)$ steps, SGD approximates SGDM in the sense of \Cref{def:weak_approx}.
The next section characterizes SGDM over a longer training time (i.e., $O(1/\eta^2)$ steps) to characterize generalization and finds that the limiting dynamics of SGDM and SGD  coincide.
%In the next section, we will describe the limiting dynamics of SGDM in the longer time range of $O(1/\eta^2)$ steps, which is thought to characterize generalization.
%In particular, we show that the limiting dynamics of SGDM coincides with the limiting dynamics of SGD.

\subsection{A Warm Up Example: The Variance Reduction Effect of Momentum}\label{sec:warm-up}
Intuitively, momentum makes the SGD update direction less noisy by averaging past stochastic gradients, which seems at first glance to contradict our result that the distribution of SGD and SGDM at any time point are approximately the same.
However, the apparent discrepancy can be explained by the following effect:
%This is actually due to the following effect:
by carrying the noise at a step to subsequent steps, the updates of SGDM have long-range correlations.

For instance, we consider the case where the stochastic gradients are i.i.d. gaussian as $\vg_k\sim \mathcal{N}(\vc,\mI)$ for a constant vector $\vc$. We compare SGD and SGDM trajectories with hyperparameter $\eta_k=\eta$ and $\beta_k=\beta$, and initialization $\vz_0=\vx_0$ and $\vm_0\sim \mathcal{N}(\vc,\frac{1-\beta}{1+\beta}\mI)$. The single-step updates are
\begin{align*}
    \vz_{k+1} - \vz_{k} & = -\eta\vg_k\sim \mathcal{N}(-\eta\vc, \eta^2\mI).\\
    \vx_{k+1} - \vx_{k} & = -\eta\vm_{k+1} = -\eta(\beta^{k+1}\vm_0 + \sum_{s=0}^{k} \beta^{k-s}(1-\beta)\vg_s) \sim \mathcal{N}(-\eta\vc, \frac{1-\beta}{1+\beta}\eta^2\mI).
\end{align*}
Therefore, the variance of each single-step update is reduced by a factor of $\frac{1-\beta}{1+\beta}$, which implies larger momentum generates a smoother trajectory. Furthermore, measuring the turbulence over a fixed interval via the path length $\sum_k \norm{\vz_{k+1}-\vz_k}_2$ or the loss variation $\sum_k |\Loss(\vz_{k+1})-\Loss(\vz_k)|$ indeed suggests that adding momentum smooths the path.

However, we are usually more interested in tracking the final loss distributions induced by each trajectory. The distributions of after $k$ steps are
\begin{align*}
    \vz_k & \sim\mathcal{N}(\vz_0-k\eta\vc, k\eta^2 \mI);\\
    \vx_{k} & = \vz_0-\eta\beta\frac{1-\beta^k}{1-\beta}\vm_0 -\eta\sum_{s=0}^{k-1} (1-\beta^{k-s})\vg_s \sim \mathcal{N}\bigg(\vz_0-k\eta\vc, k\eta^2\mI-2\beta\eta^2\frac{1-\beta^k}{1-\beta^2}\mI\bigg).
\end{align*}
Notice that the variance of the final endpoint is only different by $|2\beta\eta^2\frac{1-\beta^k}{1-\beta^2}|\leq \frac{2\eta^2}{1-\beta}$, which is bounded regardless of $k$. The variance is increased at rate $\eta^2$ per step, which is significantly larger than the per step update variance $\frac{1-\beta}{1+\beta}\eta^2$. As such, the turbulence of the path may not faithfully reflect the true stochasticity of the iterates. 

\iffalse
A direct second-order Taylor expansion of the change in loss at each step is given below.    
  \begin{align*}
        \E\Loss(\vz_{k+1}) & = \E\Loss(\vz_{k}) + \E \dotp{\nabla \Loss(\vz_{k})}{\vz_{k+1}-\vz_k} \\
        &+ \frac{1}{2}\E(\tr(\nabla^2\Loss(\vz_k)(\vz_{k+1}-\vz_k)(\vz_{k+1}-\vz_k)^\top)+o(\eta_{k+1}^2).
    \end{align*}
\fi


\iffalse
To boost the mini-batched induced impact, we consider SVAG:
\begin{lemma}[Second Order Descent Lemma for SVAG]
    For the SGD updates $\vz_k$, with $\sigma=\eta^{-1/2}$, the expected change in loss per step is
    \begin{align*}
        \E\Loss(\vz_{k+1}) - \E\Loss(\vz_{k}) & = - \eta \E\norm{\nabla \Loss(\vz_{k})}^2 + \underbrace{\frac{1}{2}\eta\E\tr((\nabla^2\Loss)\mSigma(\vz_k))}_\text{minibatch-induced}
        + o(\eta).
    \end{align*}
\end{lemma}
%$\vm_{k+1} = \vg_{k+1} + \frac{\beta_{k+1}}{1-\beta_{k+1}}(\vm_{k}-\vm_{k+1})$.

Inspired by this expansion, we define the following two sources of instability that appear in both the SGD and MSGD trajectories.
\begin{definition}[Curvature-Induced Instability]
	The curvature-induced instability of a trajectory $\vx_k$ is defined as
	$$\frac{1}{2}\eta^2\E(\nabla\Loss^\top(\nabla^2\Loss)\nabla\Loss(\vz_k))$$
\end{definition}

\begin{definition}[Gradient-Induced Instability]
	The gradient-induced instability of a trajectory $\vx_k$ is defined as
	$$\frac{1}{2}\eta^2\E(\tr((\nabla^2\Loss)\sigma\sigma^{\top}(\vx_k))) $$
\end{definition}
\fi
%For the SGD, there are two sources of instability in minimizing the loss: the "oscillation" \runzhe{Maybe we should name it otherwise?} $\frac{1}{2}\eta_{k+1}^2\E(\nabla\Loss^\top(\nabla^2\Loss)\nabla\Loss(\vz_k))$ and the gradient noise $\frac{1}{2}\eta_{k+1}^2\E(\tr((\nabla^2\Loss)\sigma\sigma^{\top}(\vz_k)))$.
\iffalse
Notice that if we have constant hyperparameter schedule $\eta_k = \eta$ and $\beta_k=\beta$, then the first different term $\frac{\eta\beta}{1-\beta}(S_{k+1}-S_k)$ is a telescoping sum that have $o(1)$ cumulative effect. The second different term $\eta_{k+1}^2\E [\vm_k^\top\nabla^2\Loss\nabla\Loss(\vx_k)]$ is irrelevant to the gradient noise at the current step $\bg_{k}$ and only accumulates in $O(1/\eta^2)$ steps. These observations inspire us to
\begin{itemize}
    \item Compare the path of SGD and SGDM in $O(1/\eta)$ steps with noise augmentation (SVAG). Specifically we hope to find a way to couple the two paths. Moreover for a non-constant hyperparameter schedule we introduce the coupled learning rate schedule.
    \item Compare the path of SGD and SGDM in $O(1/\eta^2)$ steps.
\end{itemize}
\fi
\subsection{Main Results on Weak Approximations of SGDM}\label{sec:weak-appro-main-1}
We study the setting where the magnitude of the hyperparameters are controlled. Let $\eta$ be the scale of the learning rate so that $\eta_k=O(\eta)$. Furthermore, we set an index $\alpha\geq 0$ so that the decay rate of the momentum is controlled as $1-\beta_k=O(\eta^\alpha)$. $\alpha=0$ corresponds to a constant decay schedule while $\alpha>0$ corresponds to a schedule where we make $\beta_k$ closer to $1$ when the learning rates are getting smaller. More formally, we associate a hyperparameter schedule $(\eta_k^\eta,\beta_k^\eta)$ for each scale $\eta$ such that the following assumption is satisfied.
\begin{definition}[Hyperparameter Schedule Scaling]\label{def:hpschedule} A family of hyperparameter schedules $\{\eta^{(n)}_k,\beta^{(n)}_k\}_{k\geq 1}$ is scaled by $\eta^{(n)}$ with index $\alpha$ if there are constants
$\eta_{\max}, \lambda_{\min}$ and $\lambda_{\max}$ independent of $n$, such that for all $n$
\[0\leq \frac{\eta^{(n)}_k}{\eta^{(n)}}<\eta_{\max}, \quad 0<\lambda_{\min}\leq\frac{1-\beta^{(n)}_k}{(\eta^{(n)})^{\alpha}}\leq\lambda_{\max}<1.\]
\end{definition}
We need some boundedness of the initial momentum for the SGDM trajectory to start safely.
\begin{assumption}[Boundedness of the Initial Momentum]
    For each $m\geq 1$, there is constant $C_{m}\geq 0$ that $\E (\norm{\vm_0}_2^m)\leq C_m$;\label{ass:init-bound}
\end{assumption}
Following \citet{malladi2022sdes}, we further assume that the NGOS satisfies the below conditions, which make the trajectory amenable to analysis. 
We say a function $g(\bx):\R^d\to \R^m$ has polynomial growth if there are constants $k_1,k_2>0$ such that $\norm{g(\bx)}_2\leq k_1(1+\norm{\bx}_2^{k_2})$, $\forall\bx\in\RR^d$.
\begin{assumption}
    The NGOS $\cG_\sigma = (\cL, \mSigma, \DatZ_{\sigma})$ satisfies the following conditions.
    
    \begin{enumerate}[leftmargin=0.2in]
        \item \textbf{Well-Behaved}: $\nabla\cL$ is Lipschitz and $\cC^\infty$-smooth; $\mSigma^{1/2}$ is bounded, Lipschitz, and $\cC^\infty$-smooth; all partial derivatives of $\nabla\cL$ and $\mSigma^{1/2}$ up to and including the third order have polynomial growth.
        
        %\runzhe{we don't need low skewness in our proof!}
        %\item \textbf{Low Skewness}: There exists a function $K(\vtheta)$ of polynomial growth independent of $\sigma$ such that $\E_{\vv \sim\DatZ_{\sigma}(\vtheta)}\norm{\vv}^{ 3}\le K_3(\vtheta) / \sigma$ 
        \item \textbf{Bounded Moments}: For all integers $m \ge 1$ and all noise scale parameters $\sigma$,
	%the expectation $\E_{\vz \sim \DatZ_{\sigma}(\vtheta)}[\normtwosm{\vz}^{2m}]$ exists, and
	there exists a constant $C_{2m}$ (independent of $\sigma$) such that
	$(\E_{\vv \sim
	\DatZ_{\sigma}(\vtheta)}[\norm{\vv}_2^{2m}])^{\frac{1}{2m}} \le
	C_{2m}(1 + \norm{\vtheta}_2)$, $\forall\vtheta \in \RR^d$.
    \end{enumerate}
    \label{assume:ngos}
\end{assumption}
Besides, to rigorously discuss the closeness between dynamics of different algorithms, we introduce the following notion of approximation between two discrete trajectories, inspired by~\citep{li2019stochastic}.
\begin{definition}[Order-$\gamma$ Weak Approximation]\label{def:weak_approx}
Two families of discrete trajectories $\vx^\eta_k$ and $\vy^\eta_k$ are weak approximations of each other, if there is $\eta_{\text{thr}}>0$ that for any $T>0$, any function $h$ of polynomial growth, and any $\eta\leq \eta_{\text{thr}}$, there is a constant $C_{h,T}$ independent of $\eta$ such that,
	$$ \max_{k=0,...,\lfloor T/\eta\rfloor} | \E h(\vx^\eta_k) - \E h(\vy^\eta_k) | \leq C_{h,T}\cdot\eta^\gamma. $$
\end{definition}
Weak approximation implies that $\vx_k^\eta$ and $\vy_k^\eta$ have similar distributions at any step $k$, and specifically in the deep learning setting it implies that both training (or testing) curves are similar. Given the above definitions, we are ready to establish our main result.
\begin{theorem}[Weak Approximation of SGDM by SGD]\label{thm:weak-appro-main}
    Fix the initial point $\vx_0$ , $\alpha\in [0,1)$, and an NGOS satisfying \Cref{assume:ngos}. Consider the SGDM update $\vx^\eta_k$ with hyperparameter schedule $\{\eta_k,\beta_k\}_{k\geq 1}$ scaled by $\eta$ with index $\alpha$, noise scaling $\sigma\leq \eta^{-1/2}$ and initialization $(\vm_0,\vx_0)$ satisfying \Cref{ass:init-bound}, then $\vx^\eta_k$ is an order-$(1-\alpha)/2$ weak approximation (\Cref{def:weak_approx}) of the trajectories $\vz^\eta_k$ with initialization $\vz^\eta_0=\vx_0$, noise scaling $\sigma$ and an averaged learning rate schedule $(\bar{\eta}_k= \sum_{s=k}^\infty \eta_s \prod_{\tau=k+1}^s \beta_\tau(1-\beta_{k})).$
    Specifically, for a constant schedule where $(\eta_k=\eta,\beta_k=\beta)$ and $\bar{\eta}_k=\eta$,  SGD and SGDM with the same learning rate weakly approximate each other.
\end{theorem}
The theorem shows that when the learning rate have a small scale $\eta$, then the trajectory of SGDM and SGD will be close in distribution over $O(1/\eta)$ steps, when the gradient noise is amplified at a scale no more than $\eta^{-1/2}$. 
Specifically when we consider the limit  $\eta\to 0$, then the trajectories of SGDM and SGD will have the same distribution. Following the idea of Stochastic Modified Equations \citep{li2019stochastic}, the limiting distribution can be described by the law of the solution $\bvx_t$ to an SDE 
$\dd \bvx_t = -\lambda_t \nabla\Loss(\bvx_t) \dd t + \lambda_t \mSigma^{1/2}(\bvx_t) \dd \bvw_t$
for some rescaled learning rate schedule $\lambda_t$.

Our theorem requires $\alpha\in [0,1)$, and the approximation grows weaker as $\alpha$ approaches 1. When $\alpha=1$, the two trajectories are no longer weak approximations of each other, and their trajectories will have different limiting distributions. Furthermore, when $\alpha>1$, yet another behaviour of the SGDM trajectory occurs over a longer range of $O(\eta^{-\frac{1+\alpha}{2}})$ steps. This is often undesirable in practice as optimization is slower. We discuss this choice of $\alpha$ further in the appendix.

\section{The Limit of SGDM and SGD are identical in $O(1/\eta^2)$ Steps}\label{sec:limit-main}

In this section, we follow the framework from \citep{li2021happens} to study the dynamics of SGDM when the iterates are close to some manifold of local minimizers of $\cL$. Former analyses (e.g., \citep{yan2018unified}) suggest that SGDM and SGD will get close to a local minimizer in $O(1/\eta)$ steps, at which point the loss function plateaus and the trajectory follows a diffusion process near the local minimizer. If the local minimizers form an manifold in the parameter space, then the diffusion accumulates into a drift within the manifold  in $O(1/\eta^2)$ steps. \citep{li2021happens} shows that the drift induces favorable generalization properties after the training loss reaches its minimum under certain circumstances.

Therefore, we hope to study the generalization effect of SGDM by investigating its dynamics in such a regime. In particular, we show that when $\eta\to 0$ the limiting diffusion of SGDM admits the same form as that of SGD, thus suggesting that momentum provides no generalization benefits.

% The above notion of approximation describes behavior for $1/\eta$ steps but we are also interested in understanding the behavior on a longer time horizon.
% As such, we turn to \snote{Katzenberger paper}
% [define weak convergence in skorohod topology, see Theorem 2.2 in Kurtz and Protter]
% \snote{Add preliminaries for Katzenberger paper here}
% \subsection{The Motion of Stochastic Gradient Methods Near a Minimizer Manifold}
\subsection{Preliminaries on manifold of local minimizers}
We consider the case where the local minimizers of the loss $\cL$ form a manifold.

\begin{assumption}\label{assump:manifold}
    $\Loss$ is smooth. $\Gamma$ is a $(d-M)$-dimensional submanifold of $\RR^d$ for some integer $0\leq M\leq d$. 
    Moreover, every $\bx\in\Gamma$ is a local minimizer of $\cL$, satisfying $\nabla\cL(\bx)=0$ and $\rank(\nabla^2\cL(\bx))=M$.
\end{assumption}

We consider a neighborhood $O_\Gamma$ of $\Gamma$ that is an attraction set under $\nabla\cL$.
Specifically, we define the gradient flow under $\nabla\cL$ by $\phi(\bx,t) = \bx - \int_0^t\nabla\cL(\phi(\bx,s))\diff s$ for any $\bx\in\RR^d$ and $t\geq 0$.
We further define gradient projection map associated with $\nabla\cL$ as $\Phi(\bx) := \lim_{t\to\infty}\phi(\bx,t)$ when the limit exists. 
Formally, we make the following assumption:
\begin{assumption}\label{assump:neighborhood}
    For any initialization $\bx\in O_\Gamma$, the gradient flow governed by $\nabla\cL$ converges to some point in $\Gamma$, i.e., $\Phi(\bx)$ is well-defined and $\Phi(\bx) \in \Gamma$.
\end{assumption}

It can be shown that for every $\bx\in\Gamma$, $\partial\Phi(\bx)$ is the orthogonal projection onto the tangent space of $\Gamma$ at $\bx$.
Moreover, \citep{li2021happens} proved that for any initialization $\bx_0\in O_\Gamma$, a fixed learning rate schedule $\eta_k\equiv\eta$, and any $t>0$, and time-rescaled SGD iterates $\bz_{\lfloor t/\eta^2\rfloor}$ converges in distribution to $\bvz_t$, the solution to the following slow SDE on $\Gamma$:
\begin{align*}\bvz_t = \Phi(\vx_0)+\int_0^t \partial\Phi(\bvz_s)\mSigma^{1/2}(\bvz_s)\dd W_s +\int_0^t\frac{1}{2}\partial^2\Phi(\bvz_s)[\mSigma(\bvz_s)]\dd s.
\end{align*}

\subsection{Analysis of SGDM via slow SDE}
In this regime, for a fixed $\alpha\geq 0$, we choose a series of learning rate scales $\eta^{(0)}>\eta^{(1)}>\cdots>0$ with $\lim_{n\to\infty}\eta^{(n)}=0$. 
For each $n$, we assign a hyperparameter schedules $\{(\eta^{(n)}_k,\beta^{(n)}_k)\}_{k\geq 1}$, such that  $\{(\eta^{(n)}_k,\beta^{(n)}_k)\}_{k\geq 1}$ is scaled by $\eta^{(n)}$ in the sense of \Cref{def:hpschedule}.

For SGD with a fixed learning rate, as shown in \citep{li2021happens}, it suffices to consider a fixed time rescaling by looking at $\bz_{\lfloor t/\eta^2\rfloor}$ to derive the limiting dynamics, i.e., one unit of time for the slow SDE on $\Gamma$ corresponds to $\lfloor 1/\eta^2\rfloor$ SGD steps.
However, the varying learning rate case requires more care to align the discrete iterates with the slow dynamics on $\Gamma$.
As such, we consider learning rate schedules over time horizon $T$, which corresponds to  $K^{(n)}=\lfloor T/(\eta^{(n)})^2\rfloor$ steps of discrete updates of the process $\{\vz_k^{(n)}\}$ and $\{\vx_k^{(n)}\}$. To show the dynamics of SGDM and SGD have a limit at $n\to \infty$, it is necessary that the hyperparameter schedules for different $n$ have a limit as $n\to\infty$, which we formalize below.
%Therefore we make the following assumption.
\begin{assumption}[Converging hyperparameter scheduling]\label{ass:conve-hpschedule}
    There exists learning rate schedule $\lambda_t: [0,T]\to \R^+$ with finite variation such that
    \[\lim_{n\to\infty}\eta^{(n)}\sum_{k=0}^{K^{(n)}}|\eta^{(n)}_k-\eta^{(n)}\cdot\lambda_{k(\eta^{(n)})^2}|=0.\]
\end{assumption}
\looseness-1 In the special case $\eta_k^{(n)}\equiv\eta^{(n)}$, it is clear that $\lambda_t\equiv 1$, which recovers the regime in \citep{li2021happens}.
We furthermore assume that the hyperparameter schedules admit some form of continuity:
\begin{assumption}[Bounded variation]\label{ass:finite-var}
There is constant $Q$ independent of $n$ such that
\[\sum_{k=1}^{K^{(n)}}|\eta_k^{(n)}-\eta_{k-1}^{(n)}|\leq Q\eta^{(n)},\quad\sum_{k=1}^{K^{(n)}}|\beta_k^{(n)}-\beta_{k-1}^{(n)}|\leq Q(\eta^{(n)})^{\alpha}\]
\end{assumption} 

In this general regime, we define the slow SDE on $\Gamma$ to admit the following description:
\begin{align}\label{eq:general_sde}
    \bvx_t = \Phi(\vx_0)+\int_0^t\lambda_t \partial\Phi(\bvx_s)\mSigma^{1/2}(\bvx_s)\dd W_s +\int_0^t\frac{\lambda_t^2}{2}\partial^2\Phi(\bvx_s)[\mSigma(\bvx_s)]\dd s.
\end{align}
Indeed, we show that both SGDM and SGD, under the corresponding hyperparameter schdules, converge to the above slow SDE on $\Gamma$, as summarized in the following theorem.
\begin{theorem} \label{thm:slow-sde-main}
Fix the initialization $\vx_0=\bz_0\in O_\Gamma$ and any $\alpha\in(0,1)$, and suppose the initial momentum $\bbm_0$ satisfies \Cref{ass:init-bound}. 
For $n\geq 1$, let $\{(\eta_k^{(n)},\beta_k^{(n))}\}_{k\geq 1}$ be any hyperparameter schedule scaled by $\eta^{(n)}$
satisfying \Cref{ass:conve-hpschedule,ass:finite-var}.
Further fix the noise scale $\sigma^{(n)}\equiv 1$.
Under \Cref{assump:manifold,assump:neighborhood}, consider the SGDM trajectory $\{\vx_k^{(n)}\}_{k\geq 1}$ with hyperparameter schedule $\{(\eta_k^{(n)}, \beta_k^{(n)})\}_{k\geq 1}$ and initialization $(\vx_0,\vm_0)$, and the SGD trajectory $(\vz_k^{(n)})$ and initialization $\vz_0=\vx_0$. 
Suppose the slow SDE defined in \eqref{eq:general_sde} has a global solution $\{\bX_t\}_{t\geq 0}$, then as $n\to\infty$ with $\eta^{(n)}\to0$, both $\bx_{\lfloor t/(\eta^{(n)})^2\rfloor}^{(n)}$ and $\bz_{\lfloor t/(\eta^{(n)})^2\rfloor}^{(n)}$ converge in distribution to $\bX_t$.
\end{theorem}

The proof of \Cref{thm:slow-sde-main} is inspired by \citep{calzolari1997limit}. In this regime, the momentum process $\vm_k^{(n)}$ behaves like an Uhlenbeck-Ornstein process with $O(\eta^\alpha)$ mixing variance,  so the per-step variance will be significantly smaller than that of SGD, analogous to \Cref{sec:warm-up}. Therefore a more careful expansion of the per-step change $\Phi(\vx_{k+1})-\Phi(\vx_{k})$ is needed. Tools from the semi-martingale analysis and weak limit results of stochastic integrals complete our proof.

% \tianhao{How to compare the above result with that in \citep{cowsik2022flatter}}