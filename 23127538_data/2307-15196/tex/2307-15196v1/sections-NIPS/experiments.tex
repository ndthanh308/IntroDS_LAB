\section{Experiments}\label{sec:exps}
Our theoretical results in the previous sections mostly work for learning rates that are asymptotically small. In this section, we verify that momentum indeed has limited benefits in practical training regimes where the optimal learning rate is not very large. Additional details are in the appendix.

%with limited curvature-induced instabilities, but we hypothesize that momentum also does not reduce the impact of gradient noises at practical learning rates.
%We verify our hypothesis with experiments on CIFAR-10, ImageNet, and language model fine-tuning. 

\subsection{Momentum may indeed have marginal value in practice}

\paragraph{ImageNet Experiments.} 
First, we train ResNet-50 on ImageNet across batch sizes. Following the experimental setup in \citet{goyal2017accurate}, we use a learning rate schedule that starts with a 5-epoch linear warmup to the peak learning rate and decays it at epoch \#30, \#60, \#80. For SGDM~\eqref{equ:SGDM-intro}, we use the default value of $\beta = 0.9$, and grid search for the best learning rate $\gamma$ over $0.1 \times 2^k$ ($k \in \Z$). Then we check whether vanilla SGD with learning rate $\frac{\gamma}{1-\beta}$ can achieve the same performance as SGDM. Consistent with previous empirical studies~\citep{shallue2019measuring,smith2020generalization}, we observed that for training with smaller batch sizes, the optimal learning rate of SGDM is small enough so that SGD can perform comparably, though SGDM can indeed outperform SGD at larger batch sizes.

\input{figures/imagenet_figure}

\paragraph{Language Model Experiments.}
In fine-tuning a pre-trained model, a small learning rate is also preferable to retain the model's knowledge learned during pre-training. Indeed, we observe that SGD and SGDM behave similarly in this case.
We fine-tune RoBERTa-large~\citep{liu2019roberta} on $5$ diverse tasks (SST-2~\citep{socher2013recursive_sst-2}, SST-5~\citep{socher2013recursive_sst-2}, SNLI~\citep{bowman2015large}, TREC~\citep{voorhees2000building_trec}, and MNLI~\citep{williams2018broad_mnli}) using SGD and SGDM.
We follow the few shot setting described in~\citep{gao-etal-2021-making,malladi2023kernelbased}, using a grid for SGD based on~\citep{malladi2023kernelbased} and sampling $512$ examples per class (\Cref{tab:lmft}). 
%Our results highlight that SGD and SGDM behave similarly in noisy settings.
\input{figures/roberta_large_no_prompt}

\subsection{Investigating the benefit of momentum in large-batch training}
The ImageNet experiments demonstrate that momentum indeed offers benefits in large-batch training when the optimal learning rate is relatively large. We now use large-batch training experiments on CIFAR-10 to provide empirical evidence that this benefit may not be due to the noise reduction effect.
We apply SVAG \citep{li2021validity} to control the noise scale in our experiments and reduce the curvature-induced training instability (\Cref{lem:descent}) while leaving the noise-induced term unchanged.
\begin{definition}[SVAG]\label{def:svag}
    With any $\ell>0$, SVAG transforms the NGOS $\cG_\sigma=(f, \mSigma, \DatZ_{\sigma})$  (\Cref{def:NGOS}) into another NGOS $\hat\cG_{\sqrt{\ell}\sigma} = (f, \mSigma, \hat\DatZ_{\sqrt{\ell}\sigma})$ with scale $\sqrt{\ell}\sigma$. For an input $\vtheta$, $\hat\cG_{\ell\sigma}$ returns $\hat\vg = r_1(\ell)\vg_1 + r_2(\ell)\vg_2$ where $\vg_1,\vg_2\sim\cG_\sigma(\vtheta)$ and $r_i(\ell) = \frac 12 (1 + (-1)^i\sqrt{2\ell-1})$. $\hat\DatZ_{\sqrt{\ell}\sigma}$ is defined to ensure $\hat\vg$ has the same distribution as $\nabla f(\vtheta) + \sqrt{\ell}\sigma\vz$ when $\vz\sim\hat\DatZ_{\sqrt{\ell}\sigma}(\vtheta)$. 
\end{definition}
%In addition, we follow a scaling inspired by the following lemma.
In our experiments, we divide the learning rate $\eta$ by $\ell$ after applying SVAG so $\lambda'=\lambda/\sqrt{\ell}$, and run $\ell$ times the original iterate steps. This ensures that way the noise-induced impact and the descent force stay the same scale  in \Cref{lem:descent}, while the curvature-induced impact is reduced by a factor of $\ell$. 
%\paragraph{CIFAR-10 Experiments}

We train a ResNet-32~\citep{he2016deep} on CIFAR-10~\citep{cifar10} with batch size $B=512$.
In order to control the curvature-induced impact, we apply SVAG~\citep{li2021validity,malladi2022sdes} to the NGOS (\Cref{def:NGOS}) for SGD and SGDM.
We first grid search to find the best learning rate for the standard SGDM ($\ell=1$), and then we perform SGD
and SGDM with that same learning rate for different levels of $\ell$. The results are summarized in \Cref{fig:svag}.
We see that standard SGDM outperforms standard SGD, but when we increase the noise level $\ell$, the two trajectories become closer. 
\input{figures/svag_figure}