\section{Introduction}

%, and now it is standard to use {\em Stochastic Gradient Descent with (Heavy-Ball) Momentum} to train deep neural networks, which we call {\em SGDM} for short.
In modern deep learning, it is standard to combine stochastic gradient methods with {\em heavy-ball momentum}, or {\em momentum} for short, to enable a more stable and efficient training of neural networks~\citep{sutskever2013on}. 
The simplest form is {\em Stochastic Gradient Descent with Momentum} (SGDM). 
SGDM aims to minimize the training loss $\cL(\vx)$ given a noisy gradient oracle $\cG(\vx)$, which is usually realized by evaluating the gradient at a randomly sampled mini-batch from the training set. 
Specifically, let $\gamma, \beta$ be the learning rate and momentum coefficient, then SGDM can be stated as follows:
\begin{equation}\label{equ:SGDM-intro}
    \vg_k \sim \cG(\vx_k), \qquad \vm_{k+1} = \beta \vm_k + \vg_k, \qquad \vx_{k+1} = \vx_{k} - \gamma \vm_{k+1},
\end{equation}
where $\vg_k, \vm_k, \vx_k$ are the gradient, momentum buffer, and parameter vector at step $k$. 
See \Cref{def:SGDM} for its formal formulation.
% Momentum costs extra memory and people want to design more memory-efficient optimization algorithms. 
% For that purpose, we must first understand the underlying mechanism of momentum. 
%Classical optimization theory has confirmed the benefit of momentum on convergence of gradient descent. % need to mention "noiseless"
% When $\beta = 0$, SGDM reduces to vanilla SGD. 

For typical choices of $\beta\in(0,1)$, the momentum buffer can be interpreted as an exponential moving average of past gradients, i.e., $\vm_k = \sum_{j=0}^{k} \beta^{k-j} \vg_j$. 
Based on this interpretation, \citet{polyak1964some,polyak1987intro,rumelhart1987learning} argued that momentum is able to cancel out oscillations along high-curvature directions and add up contributions along low-curvature directions. 
More concretely, for strongly convex functions without any noise in gradient estimates, \citet{polyak1964some,polyak1987intro} showed that adding momentum can stabilize the optimization process even when the learning rate is so large that can make vanilla gradient descent diverge, and thus momentum accelerates the convergence to minimizers by allowing using a larger learning rate.

%can converge faster to minimizers with a larger learning rate
%to accelerate convergence to minimizers.
%\kaifeng{let me have a try:}

In deep learning, however, the random sampling of mini-batches inevitably introduces a large amount of stochastic gradient noise. In addition to the high-curvature directions causing the parameter to oscillate back and forth, this large gradient noise can further destabilize training and hinder the use of large learning rates. Given the aforementioned convergence results that solely analyze the noiseless case, it remains unclear whether momentum can likewise stabilize the stochastic optimization process in deep learning. Still, it is intuitive that averaging past stochastic gradients could reduce the variance of the noise in the updates, as long as the parameter does not move drastically fast at each step. Several prior studies indeed cited this reduction of noise in SGDM as a possible advantage that may encourage a more rapid decrease in loss~\citep{bottou2018optimization,defazio2020momentum,you2020large}.
%momentum as an optimization technique to 
%the momentum buffer can be seen as a moving average of stochastic gradients from the past, and several papers indeed have cited momentum as an optimization technique that reduces the variance in the updates through this gradient averaging,
%\kaifeng{find more; also need to check if it is a fair summary}
To approach this more rigorously, \citet{cutkosky2019momentum} proposed a variant of SGDM that provably accelerates training by leveraging the reduced variance in the updates. They further speculated that the advantage of the original SGDM might be related in some way.

Nevertheless, for SGDM without any modification, past theoretical analyses in the stochastic optimization of convex and non-convex functions usually conclude with a comparable convergence rate as vanilla SGD, rather than a faster one~\citep{yan2018unified,yu2019linear,liu2020improved,sebbouh2021almost,li2022last}. 
Besides, there also exist simple and concrete instances of convex optimization where momentum does not speed up the convergence rate of SGD, even though it is possible to optimize faster with some variants of SGDM~\citep{kidambi2018on}.
Despite these failures in theory, it has been empirically confirmed that SGDM continues to stabilize large learning rate training even in the presence of gradient noise.
%mitigate training instabilities in mini-batch training with large learning rates. 
\citet{kidambi2018on,shallue2019measuring,smith2020generalization} observed that for large-batch training, SGDM can successfully perform training with a large learning rate, in which regime vanilla SGD may exhibit instability that degrades the training speed and generalization.
This naturally raises the following question on the true role of momentum:
\begin{center}
    \emph{Is the noise reduction in the updates of SGDM really beneficial to training neural networks?}
\end{center}
To address this question, our paper delves into the training regime where the learning rate is small enough to prevent oscillations along high-curvature directions, yet the gradient noise is large enough to induce instability. This setting enables us to concentrate exclusively on the interplay between momentum and gradient noise. 
More importantly, this training regime is of practical significance as in many situations, such as small-batch training from scratch or fine-tuning a pre-trained model, the optimal learning rate is indeed relatively small~\citep{liu2019roberta,malladi2023kernelbased}.

%the large gradient noise is the main factor that hinders the use of large learning rates, which we call the {\em noise-dominated} regime.
%with {\em small learning rates} $\eta$ to avoid training instability caused by large learning rate/high curvature. Besides its simplicity 
%the optimal learning rate can be so large that destabilizes SGD and degrades the performance, but SGDM 
%the optimal learning rate for large-batch training is indeed large enough so that SGDM and vanilla SGD differ, and momentum is able to stabilize the training 
%All these results are for the training regimes with small learning rates $\eta$, where the descent lemma in the optimization theory guarantees a loss decrement at each step.

\paragraph{Main Contributions.} 
In this paper, we present analyses of the training trajectories of SGD with and without momentum, in the regime of small learning rate. 
We provide theoretical justifications of a long-held belief that SGDM with learning rate $\gamma$ and momentum $\beta$ performs comparably to SGD with learning rate $\frac{\gamma}{1 - \beta}$~\citep{tugay1989properties,orr1996dynamics, qian1999momentum, yuan2016influence,smith2020generalization}.
%Besides, we study a longer range of $O(1/\eta^2)$ steps and provide a Slow SDE analysis showing that momentum does not impact generalization.
This finding offers negative evidence for the usefulness of noise reduction in momentum.

%For small learning rate training,
%it has been long believed that SGDM with learning rate $\eta$ and momentum $\beta$ performs nearly the same as SGD with learning rate $\frac{\eta}{1 - \beta}$


%provide negative evidence to this question through SDE-based analyses that can rigorously capture the training trajectories. 

%For large learning rate $\eta$, this equivalence between SGDM and vanilla SGD can fail and the benefit of momentum may show up. \citep{kidambi2018on,shallue2019measuring,smith2020generalization} observed that the optimal learning rate for large-batch training is indeed large enough so that SGDM and vanilla SGD differ, and momentum is able to stabilize the training 
%As for the benefit of adding momentum,
%When scaling the learning rate across batch sizes according to the Linear Scaling Rule (LSR): $\eta \propto B$, 
%\citep{kidambi2018on,shallue2019measuring,smith2020generalization} argued that one has to increase the batch size so that 

%reported that the benefit mainly shows up in large-batch training, where the optimal learning rate is large and the momentum can help to stabilize the training.
%However, this seems to go back to the intuitive arguments of averaging out oscillations in the noiseless case. If reducing the variance in the updates can accelerate training, we should be able to see this benefit at smaller batch sizes too.


%\kaifeng{cite papers. Most importantly %\citep{smith2020generalization}}


%\paragraph{Main contributions.} 

More specifically, given a run of SGDM, we show that vanilla SGD can closely track its trajectory in the following two regimes with different time horizon:

\vspace{-0.1in}
\begin{description}
    \item[Regime I.] Training with SGD and SGDM for $O(1/\eta)$ steps with gradient noise covariance inversely scaled with $\eta$ leads to the same SDE approximation. Scaling down batch-size with the Linear Scaling Rule~\citep{goyal2017accurate} falls into this regime. See~\Cref{sec:weak_approx}.
    \item[Regime II.] Training with SGD and SGDM for $O(1/\eta^2)$ steps, where the loss is mostly zero but the gradient noise induces certain drift and diffusion on the minimizer manifold that may benefit generalization, also leads to the same SDE approximation. See~\Cref{sec:limit-main}.
\end{description}
Our results improve over previous analysis \citet{yuan2016influence,liu2018diffusion} on the approximation of SGDM by avoiding underestimating the role of noise when scaling down the learning rate, and provide rigorous theoretical supports to the scaling claims in \citet{smith2020generalization,cowsik2022flatter}.

In \Cref{sec:exps}, we show empirically that momentum indeed has limited benefits for both optimization and generalization in practical training regimes with optimal learning rate being not very large, including small- to medium-batch training from scratch on ImageNet and fine-tuning RoBERTa-large on downstream tasks.
We also look into a large-batch training case on CIFAR-10 where SGDM indeed outperforms vanilla SGD, and show that reducing training instability induced by high curvature by running an SDE simulation method called SVAG~\citep{li2021validity} can shrink or eliminate the performance gain.

\begin{comment}
Empirically, we show that even with practical learning rate scales,   momentum still have limited benefits for both optimization and generalization. Specifically, across several computer vision and language model finetuning tasks, we show  
    \begin{enumerate}
        \item For optimization settings with noisy gradient (e.g. with small mini-batch size), SGD + Momentum generally perform in parallel to or worse than vanilla SGD with optimally tuned learning rates.
        \item For optimization settings where SGD + Momentum have a performance edge over vanilla SGD, reducing the curvature-induced instability (via SVAG~\citep{li2021validity}) can shrink, eliminate or even reverse such an edge.
    \end{enumerate}
\end{comment}

Finally, we highlight that our results can also have practical significance beyond just understanding the role of momentum. In recent years, the GPU memory capacity sometimes becomes a bottleneck in training large models.
As the momentum buffer costs as expensive as storing the entire model, it has raised much interest in when it is safe to remove momentum~\citep{shazeer2018adafactor}. Our work sheds light on this question by formally proving that momentum only provides marginal values in small learning rate SGD.
Furthermore, our results imply that within reasonable range of scales the final performance is insensitive to the momentum hyperparametrization, thereby provide support to save the effort in the extensive hyperparameter grid search.
\begin{comment}
we show that SGDM and vanilla SGD can be closely tracked with 

The main contributions of this paper are summarized as follows:
\begin{itemize}[leftmargin=0.2in]
    \item Given a run of SGD + Momentum with certain learning rate schedule, if the learning rate is small enough, we theoretically show that vanilla SGD can closely track SGDM in the following regimes.
    \begin{description}
        \item[Regime I.] $O(1/\eta)$ steps with augmented gradient noise. Scaling down batch-size with the Linear Scaling Rule ~\citep{goyal2017accurate} falls into this regime. 
        \item[Regime II.] $O(1/\eta^2)$ steps. SGD and SGDM will follow the same drift on the minimizer manifold.
    \end{description}
    Thereby our results imply that adding momentum introduces no benefits to the optimization and generalization of training neural networks with small learning rates.
    \item 
\end{itemize}
Furthermore, as using momentum costs extra memory, and this has become an increasingly severe issue with the explosion of model sizes in recent years, our work also shed light on how to safely remove momentum without causing performance degradation. 
\end{comment}

%Empirically, it has also been observed that for small $\eta$, SGDM with learning rate $\eta$ and momentum $\beta$ performs nearly the same as SGD with learning rate $\frac{\eta}{1 - \beta}$, hence no obvious benefit in this regime~\citep{orr1996dynamics, qian1999momentum, yuan2016influence,shallue2019measuring,smith2020generalization}. Some theory tries to capture this phenomenon \kaifeng{\citep{yuan2016influence,mandt2017stochastic} did some theory.}

% \runzhe{previous comments moved to intro-notes}