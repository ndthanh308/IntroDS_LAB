\begin{abstract}
	 Momentum is known to accelerate the convergence of gradient descent in strongly convex settings without stochastic gradient noise. In stochastic optimization, such as training neural networks, folklore suggests that momentum may help deep learning optimization by reducing the variance of the stochastic gradient update, but previous theoretical analyses do not find  momentum to offer any provable acceleration.
  %but it is not clear how or when momentum can induce a benefit in stochastic optimization. 
  %though it is unclear how this may affect generalization.
    Theoretical results in this paper clarify the role of momentum in stochastic settings where the learning rate is small and gradient noise is the dominant source of instability, suggesting that SGD with and without momentum behave similarly in the short and long time horizons.
    Experiments show that momentum indeed has limited benefits for both optimization and generalization in practical training regimes where the optimal learning rate is not very large, including small- to medium-batch training from scratch on ImageNet and fine-tuning language models on downstream tasks.
    %Experiments on ImageNet, CIFAR-10, and language model fine-tuning validate our theory by showing that SGD achieves similar performance with or without momentum when the gradient noise is large.
\end{abstract}

  %little is known about when and how it works in deep learning.

% small but finite LR
% SDE method for (2)
% "inspired by SVAG and slow SDE methods ... "
% experiments with test functions on the trajectory

%%% old abstract
\iffalse
Momentum is known to accelerate the convergence of stochastic gradient descent in some convex settings, but it is not clear how or when momentum can induce an optimization benefit in stochastic deep learning settings. 
  %little is known about when and how it works in deep learning.
	Folklore suggests that momentum helps optimization by reducing the variance of the stochastic gradient update, though it is unclear how this may affect generalization.
    Theoretical results in this paper clarify the role of momentum in general nonconvex deep learning settings by showing that (1) momentum 
    %enables training with a larger learning rate but 
    cannot mitigate training instability induced by large gradient noise, and (2) momentum and SGD exhibit the same late-stage implicit regularization and thus have the same generalization abilities.
    %; rather, its primary role is to enable training with a larger learning rate
 %\runzhe{Don't know whether we should mention this} 
% Furthermore, we show that adding momentum does not modify implicit regularization in late stage training, and thus it does not induce extra generalization benefits over SGD.
	%Our results hold for highly nonconvex deep learning settings.
	An implication of our results is that any momentum trajectory can be simulated by an SGD trajectory with a novel learning rate schedule, thereby enabling memory-efficient training without sacrificing generalization. 
	We validate our theory with experiments.
 \kaifeng{please adjust the abstract according to the intro}
 \fi