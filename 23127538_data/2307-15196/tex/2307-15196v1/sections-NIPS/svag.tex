In this section we provide the proof for \Cref{thm:weak-appro-main}. Specifically, when $\alpha\in[0,1)$ is the index for the control of the learning rate schedule (\Cref{def:hpschedule}), we hope to show that $\vy_k$ is close to a SGD trajectory $\vz_k$, defined by the following for $k\geq 0$:
\begin{align}
    &\vz_0 = \vx_0\\
    &\vh_{k} \sim \cG_\sigma(\vz_{k})\\
    &\vz_{k+1} =\vz_{k}-\bar{\eta}_{k}\vh_{k}
\end{align}

%Assume $\eta$ and $\beta$ are not time-varying with $\beta=1-\lambda\eta^\alpha$. Let $\vy_k =  \vx_k - \frac{\eta\beta}{1 - \beta}\vm_{k}$, we have that 
%\begin{align*}
 %   \vy_{k} = \vy_{k-1} - \eta (\nabla \Loss(\vx_{k-1})+  \vsigma_{\vxi_{k-1}}(\vx_{k-1}))
%\end{align*}
%\begin{align*}
 %   \vy_{k} = \vy_{k-1} -  (\eta\nabla \Loss(\vx_{k-1})+  \sqrt{\eta}\vsigma_{\vxi_{k-1}}(\vx_{k-1}))
%\end{align*}
%Also define 
%\[\barvy_{k}=\barvy_{k-1} -\eta(\nabla \Loss(\barvy_{k-1}) + \vsigma_{\vxi_{k-1}}(\barvy_{k-1})), \quad \barvy_0=\vy_0\]

Specifically, we use $\vy_k$ as a bridge for connecting $\vx_k$ and $\vz_k$. Our proof consists of two steps.
\begin{enumerate}
    %\item momentum SGD $\vx_k,\vy_k$ is a order-1 weak approximation for SGD: $\barvy_k$;
    \item We show that $\vx_k$ and $\vy_k$ are order-$(1-\alpha)/2$ weak approximations of each other.  
    \item We show that $\vz_k$ and $\vy_k$ are order-$(1-\alpha)/2$ weak approximations of each other. 
\end{enumerate}
Then we can conclude that $\vx_k$ and $\vz_k$ are order-$(1-\alpha)/2$ weak approximations of each other, which states our main theorem \Cref{thm:weak-appro-main}.

%\begin{definition}[Coupled Trajectory]
%	Given $\{\vx_k\}_{k=0}^K$, define $\{\vy_k\}_{k=0}^K$ such that $\vy_0=\vx_0$ and $\vy_i$ is written as
%	\begin{equation}
%		\vy_k = \vx_k - \frac{\eta\beta}{1-\beta}\vm_k = \vy_{k-1} - \eta(\nabla\Loss(\vx_{k-1}) + \sigma_{\xi_{k-1}}(\vx_{k-1}))
%	\end{equation}
%	\label{def:coupled_trajectory}
%\end{definition}

First, we give a control of the averaged learning rate $\bar{\eta}_k$ (\Cref{equ:averaged-lr}) with the following lemma.
\begin{lemma}
    Let $(\eta_k,\beta_k)$ be a learning rate schedule scaled by $\eta$ (\Cref{def:hpschedule}) with index $\alpha$, and $\bar{\eta}_k$ be the averaged learning rate (\Cref{equ:averaged-lr}), there is 
    \begin{align*}
        & \bar{\eta}_k\leq \frac{\lambda_{\max}\eta_{\max}}{\lambda_{\min}}\eta\\
        & \frac{\bar{\eta}_k\beta_k}{1-\beta_k}\leq \frac{\lambda_{\max}\eta_{\max}}{\lambda^2_{\min}}\eta^{1-\alpha}.\\
    \end{align*}
\end{lemma}
\begin{proof}
    By \Cref{def:hpschedule} we know $\beta_k \in [1-\lambda_{\max}\eta^\alpha,1-\lambda_{\min}\eta^\alpha]$ and $\eta_k\leq \eta_{\max}\eta$. Therefore
    \begin{align*}
      \bar{\eta}_k&=\sum_{s=k}^\infty \eta_s  \beta_{k+1:s}(1-\beta_{k})\\
      & \leq \sum_{s=k}^\infty \eta_{\max}\eta(1-\lambda_{\min}\eta^\alpha)^{s-k} \lambda_{\max}\eta^\alpha\\
      & = \frac{\lambda_{\max}\eta_{\max}}{\lambda_{\min}}\eta.\\
      \frac{\bar{\eta}_k\beta_k}{1-\beta_k} & \leq \frac{\lambda_{\max}\eta_{\max}}{\lambda_{\min}}\eta\cdot \frac{1}{1-\beta_k}\\
      & \leq \frac{\lambda_{\max}\eta_{\max}}{\lambda^2_{\min}}\eta^{1-\alpha}.
    \end{align*}
\end{proof}
\subsubsection{Step 1}
For the first step we are going to proof the following \Cref{thm:weakappro-1}. For notation simplicity we omit the superscript $\eta$ for the scaling when there is no ambiguity.
\begin{theorem}[Weak Approximation of Coupled Trajectory]\label{thm:weakappro-1}
    With the learning rate schedule $(\eta_k,\beta_k)$ scaled by $\eta$ with index $\alpha\in [0,1)$. Assume that the NGOS $\cG_\sigma$ satisfy \Cref{assume:ngos} and the initialization $\vm_0$ satisfy \Cref{ass:init-bound}. For any noise scale $\sigma\leq \eta^{-1/2}$, let $(\vx_k,\vm_k)$ be the SGDM trajectory and $\vy_k$ be the corresponding coupled trajectory (\Cref{equ:coupled-traj}). 
	%Assume:
	%\begin{enumerate}
	%	\item $L$-Lipschitz Gradient: $\|\nabla\Loss(\vx_s)-\nabla\Loss(\vx_k)\| \leq L\|\vx_s - \vx_k\|$ for all time steps $s$
  	%	\item Bounded trace of covariance: $\tr(\mSigma(\vx_s))\leq C$ for all time steps $s$
	%\end{enumerate}
	Then, the coupled trajectory $\vy_k$ is an order-$\gamma$ weak approximation (\Cref{def:weak_approx}) to $\vx_k$ with $\gamma = (1-\alpha)/2$.
	\label{thm:coupled_weak_approx}
\end{theorem}

For notations, let $\nabla_k = \nabla\Loss(\vx_k)$ so $\vg_k = \nabla_k + \sigma\vv_k$ is the stochastic gradient sampled from the NGOS. Also by \Cref{assume:ngos}, as $\mSigma^{1/2}$ is bounded, let $C_{\tr}>0$ be the constant that $\tr\mSigma(\vx)\leq C_{\tr}$ for all $\vx\in\R^d$. As $\nabla\Loss$ is Lipschitz, let $L>0$ be the constant that $\norm{\nabla\Loss(\vx)-\nabla\Loss(\vy)}\leq L\norm{\vx-\vy}$ for all $\vx,\vy\in\R^d$.  As $\mSigma^{1/2}$ is Lipschitz and bounded, let $L_{\Sigma}>0$ be the constant that $|\tr{\mSigma(\vx)-\mSigma(\vy)}|\leq L_{\Sigma}\norm{\vx-\vy}$ for all $\vx,\vy\in\R^d$

To show the result it suffices to bound $\E\norm{\vx_k}^{2m}$ and $\E\norm{\vm_k}$ for any $m\geq 0$ and $k=O(\eta^{-1})$. As the gradient noises are scaled with variance $O(\sigma^2) = O(\eta^{-1})$ so that may dominate the expansion \Cref{lem:unrollm}. We will show that $\E\norm{\vm_k}=O(\eta^{(\alpha-1)/2})$ is the correct scale so we still have $\E\norm{\vx_k}^{2m}=O(1)$. An useful inequality we will use often in our proof is the Gr\"{o}nwall's inequality
\begin{lemma}[Gr\"{o}nwall's Inequality]\label{lem:gronwall-discrete}
    For non-negative sequences $\{f_i,g_i,k_i\in\R\}_{i\geq 0}$, if for all $t>0$
    \[f_t\leq g_t + \sum_{s=0}^{t-1} k_s f_s\]
    then $f_t\leq g_t + \sum_{s=0}^{t-1} k_sg_s \exp(\sum_{r=s+1}^{t-1} k_r)$.
\end{lemma}

Next follows the proofs.

\begin{lemma} 
	We can bound the expected norm of \begin{align}\E\norm{\vm_k}^2  \leq 12\frac{\lambda^2_{\max}}{\lambda^2_{\min}}\sup_{\tau=1,...,k}\E(\|\nabla_\tau\|)^2 + (4+8\frac{L\eta_{\max}}{\lambda_{\min }}+\eta^{\alpha-1})K_0\label{equ:expe-norm-m}\end{align}
	\label{lem:exp_norm_m}
    with constant $K_0 = \frac{2\lambda^2_{\max}}{\lambda_{\min}}C_{\tr} + 3\E \norm{\vm_0}^2=\frac{2\lambda^2_{\max}}{\lambda_{\min}}C_{\tr}+3C_2$ ($C_2$ by \Cref{ass:init-bound}), for a small enough learning rate $\eta<\left(\frac{\lambda_{\min }}{4L\eta_{\max}}\right)^{\frac{1}{1-\alpha}}$. 
\end{lemma}

\begin{proof}

To bound $\E\norm{\vm_k}^2$, we unroll the momentum by \Cref{lem:unrollm} and write

\begin{align}
	 \vm_k & =\beta_{0:k-1} \vm_0 + \sum_{s=0}^{k-1}\beta_{s+1:k-1}(1-\beta_{s}) \vg_{s}\label{eq:unrolled_momentum} 
\end{align}

Define
\begin{align*}
	\tilde{\vm}_k &= \beta_{0:k-1} \vm_0 + \sum_{s=0}^{k-1}\beta_{s+1:k-1}(1-\beta_{s})\nabla_s.\label{eq:unrolled_momentum-2} 
\end{align*}
Then $\E \vm_k = \E \tilde{\vm}_k$, and
\begin{align*}
	\vm_k-\tilde{\vm}_k &= \sum_{s=0}^{k-1}\beta_{s+1:k-1}(1-\beta_{s})\sigma\vv_{s}
\end{align*}


Let constant $K_0 =2C_{\tr}+3C_2$, we can write
\begin{align}
    \E\norm{\vm_k}^2&\leq \E(\norm{\tilde{\vm}_k}+\norm{\vm_k-\tilde{\vm}_k})^2\\
    & \leq 2\E\norm{\tilde{\vm}_k}^2+ 2\E\norm{\vm_k-\tilde{\vm}_k}^2\\
    & = 2\E \norm{\tilde{\vm}_k}^2+ 2 \sum_{s=0}^{k-1}(\beta_{s+1:k-1})^2(1-\beta_{s})^2 \sigma^2\E \tr\mSigma(\vx_s)\\
    & \leq 2\E \norm{\tilde{\vm}_k}^2+ 2\sum_{s=0}^{k-1} (1-\lambda_{\min}\eta^\alpha)^{2(k-s-1)}\lambda^2_{\max}\eta^{2\alpha} \sigma^2\E \tr\mSigma(\vx_s)\\
    & \leq 2\E \norm{\tilde{\vm}_k}^2+ \eta^{\alpha-1}\frac{2\lambda^2_{\max}}{\lambda_{\min}}C_{\tr}\\
    & \leq 2\E \norm{\tilde{\vm}_k}^2+ \eta^{\alpha-1}K_0.
    \label{eq:exp_mnorm_sq}
\end{align}
\iffalse
Hence, 
\begin{align}
	\E\norm{\vm_k} &\leq \sqrt{\E\norm{\vm_k}^2}\\
		& \leq \sqrt{\norm{\E\vm_k}^2 + \frac{1}{\eta}\frac{1-\beta}{1+\beta}C}\\
		& \leq \norm{\E\vm_k}  +\sqrt{\frac{1}{\eta}\frac{1-\beta}{1+\beta}C}
\end{align}
\fi

On the other hand, we know
\begin{align*}
    \tilde{\vm}_k &= \beta_{0:k-1} \vm_0 + \sum_{s=0}^{k-1}\beta_{s+1:k-1}(1-\beta_{s})\nabla_s
\end{align*}
We use the Lipschitzness of $\nabla$ to write 
\begin{align*}
	\|\nabla_s - \nabla_k\| & \leq L\|\vx_s - \vx_k\|\leq L\sum_{\tau=s}^{k-1}\eta_\tau\norm{\vm_{\tau+1}},  	
\end{align*}
then we can write
\begin{align*}
 \E \norm{\tilde{\vm}_k}^2 & \leq \E\left(  \beta_{0:k-1} \norm{\vm_0} + \sum_{s=0}^{k-1}\beta_{s+1:k-1}(1-\beta_{s})(\|\nabla_k\| + L\sum_{\tau=s}^{k-1}\eta_\tau\norm{\vm_{\tau+1}})\right)^2 \\
 &  \leq \E\left( \norm{\vm_0} +  \frac{\lambda_{\max}}{\lambda_{\min}}\|\nabla_k\| + L\eta\eta_{\max}\sum_{\tau=0}^{k-1}(1-\lambda_{\min}\eta^{\alpha})^{k-1-\tau}
 \|\vm_{\tau+1}\|\right)^2\\
 & \leq  3\E (\norm{\vm_0})^2 +  3\frac{\lambda^2_{\max}}{\lambda^2_{\min}}\E(\|\nabla_k\|)^2 + 3L^2\eta^2\eta_{\max}^2\left(\sum_{\tau=0}^{k-1}(1-\lambda_{\min}\eta^{\alpha})^{k-1-\tau}\right)\cdot\\&\qquad\left(\sum_{\tau=0}^{k-1}(1-\lambda_{\min}\eta^{\alpha})^{k-1-\tau}\E\|\vm_{\tau+1}\|^2\right)\\
 & \leq K_0+3\frac{\lambda^2_{\max}}{\lambda^2_{\min}}\E(\|\nabla_k\|)^2 + \frac{3L^2\eta_{\max}^2\eta^{2-2\alpha}}{\lambda_{\min }^2}\sup_{\tau=1\cdots {k}}\E\|\vm_\tau\|^2.
\end{align*}
Then by \Cref{eq:exp_mnorm_sq}, we know 
\[\sup_{\tau=1,...,k} \E\|\vm_\tau\|^2\leq2\sup_{\tau=1,...,k}\E \norm{\tilde{\vm}_\tau}^2+ \eta^{\alpha-1}K_0,\] so
\begin{align}
\sup_{\tau=1,...,k} \E\norm{\tilde{\vm}_\tau}^2 & \leq K_0+3\frac{\lambda^2_{\max}}{\lambda^2_{\min}}\sup_{\tau=1,...,k}\E(\|\nabla_\tau\|)^2 + \frac{6L^2\eta_{\max}^2\eta^{1-\alpha}}{\lambda_{\min }^2}K_0\\
&\qquad + \frac{6L^2\eta_{\max}^2\eta^{2-2\alpha}}{\lambda_{\min }^2}\sup_{\tau=1,\cdots, {k}}\E\|\tilde{\vm}_\tau\|^2
\end{align}
Choose $\eta$ small enough so that $\frac{6L^2\eta_{\max}^2\eta^{2-2\alpha}}{\lambda_{\min }^2}<\frac{1}{2}$, we know
\begin{align}
\sup_{\tau=1,...,k} \E\norm{\tilde{\vm}_\tau}^2 & \leq 6\frac{\lambda^2_{\max}}{\lambda^2_{\min}}\sup_{\tau=1,...,k}\E(\|\nabla_\tau\|)^2 + (2+2\frac{\sqrt{3}L\eta_{\max}}{\lambda_{\min }})K_0.
\end{align}
So
\begin{align}
    \E\norm{\vm_k}^2 & \leq 2\E \norm{\tilde{\vm}_k}^2+ \eta^{\alpha-1}K_0 \\
    & \leq 12\frac{\lambda^2_{\max}}{\lambda^2_{\min}}\sup_{\tau=1,...,k}\E(\|\nabla_\tau\|)^2 + (4+8\frac{L\eta_{\max}}{\lambda_{\min }}+\eta^{\alpha-1})K_0.
\end{align}
\end{proof}

\iffalse
\begin{lemma}
	For $\eta$ small enough, we can bound 
	$$\sup_{\tau=0,...,k} \norm{\E\vm_k}  \leq 2\|\vm_0\| + 2\sup_{\tau=0,...,k} \norm{\E\nabla_\tau} + \sqrt{2LC}.$$\label{lem:norm_exp_m}
\end{lemma}

\begin{proof}From \ref{eq:unrolled_momentum} we know
\begin{align*}
    \E\vm_k &= \beta^k\vm_0 + \sum_{s=0}^{k-1} \beta^{k-s-1}(1-\beta)\nabla_s  \label{eq:unrolled_momentum2}
\end{align*}
%We replace $\nabla_{s,i}$ with $\nabla_{s,i} - \E \nabla_{k,i} + \E \nabla_{k,i}$. 
We use the Lipschitzness of $\nabla$ to write 
\begin{align*}
	\|\nabla_s - \nabla_k\| &\leq L\eta(k-s)\sup_{\tau=s,...,k-1}\|\vm_\tau\|  	
\end{align*}
If we substitute this into \Cref{eq:unrolled_momentum2}, then we can write
$$ \E\vm_k = \beta^k\vm_0 + \sum_{s=0}^{k-1} \beta^{k-s-1}(1-\beta)(\|\nabla_k\| + L\eta(k-s)\sup_{\tau=s,...,k-1} \|\vm_\tau\|) $$
Then, we can bound the coefficients of the summation:
$$ \sum_{s=0}^{k-1} \beta^{k-s-1} (1-\beta)\eta(k-s) \leq \frac{\eta}{1-\beta} = \lambda^{-1}\eta^{1-\alpha} $$
\begin{align}
\norm{\E\vm_k} &\leq \|\E\vm_0\| + \sum_{s=0}^{k-1} \beta^{k-s-1}(1-\beta)\norm{\E\nabla_k}+ \sum_{s=0}^{k-1} \beta^{k-s-1} (1-\beta) L \eta (k-s) \sup_{\tau=s,...,k-1} \E\|\vm_\tau\|	\\
& \leq \|\vm_0\| + \norm{\E\nabla_k} + \frac{L\eta}{1-\beta}\sup_{\tau=0,...,k-1} \E\|\vm_\tau\|
\end{align}
Then by \ref{lem:exp_norm_m}, we know $ \E\|\vm_k\| \leq \|\E \vm_k\| + \sqrt{\frac 1\eta \frac{1-\beta}{1+\beta} C}$, so
\begin{align}
\sup_{\tau=0,...,k} \norm{\E\vm_k} & \leq \|\vm_0\| + \sup_{\tau=0,...,k} \norm{\E\nabla_\tau} + \frac{L\eta}{1-\beta}\sup_{\tau=0,...,k} \|\E\vm_\tau\| + L\sqrt{\frac{\eta}{1-\beta^2}C}
\end{align}
Choose $\eta$ so that $\frac{L\eta}{1-\beta}=\frac{L}{\lambda}\eta^{1-\alpha}<\frac{1}{2}$, we know
\begin{align}
\sup_{\tau=0,...,k} \norm{\E\vm_k} & \leq 2\|\vm_0\| + 2\sup_{\tau=0,...,k} \norm{\E\nabla_\tau} + \sqrt{2LC}.
\end{align}
\end{proof}
\fi

\begin{lemma}
There is a function $f$, irrelevant to $\eta$, of polynomial growth in $\vm_0$ and $\vx_0$,   such that 
\[\sup_{k=0,...,\lfloor T/\eta \rfloor} \E\norm{\nabla_k}^2 \leq f(\vm_0,\vx_0,T)\]\label{lem:gradient_norm}
when $\eta<\min(\left(\frac{\lambda_{\min}^3}{12L\lambda_{\max}^2\eta_{\max}}\right)^{\frac{1}{1-\alpha}},1)$
\end{lemma} 
\begin{proof}
By the Lipschitzness of $\nabla\Loss$, we know
\begin{align}
\|\nabla_k\| & \leq \|\nabla_0\| + L\norm{\vx_{k}-\vx_0}\\
& \leq \|\nabla_0\| + L\norm{\vy_{k}-\vy_0} + L\norm{\vx_{0}-\vy_0} + L\norm{\vx_{k}-\vy_k} \label{eq:normnablak}
\end{align}
Observe that $\|\vx_k - \vy_k\| = \frac{\bar{\eta}_k\beta_k}{1-\beta_k}\|\vm_k\|\leq \frac{\lambda_{\max}\eta_{\max}}{\lambda_{\min}^2}\eta^{1-\alpha}\|\vm_k\|$. Let $d_k=\E\|\vy_k-\vy_0\|^2$. As a result, we can write
\begin{align}
\E\|\nabla_k\|^2 & \leq 3\E\left(\|\nabla_0\|  +\frac{L\lambda_{\max}\eta_{\max}}{\lambda_{\min}^2}\eta^{1-\alpha}\|\vm_0\|\right)^2+3\left(\frac{L\lambda_{\max}\eta_{\max}}{\lambda_{\min}^2}\eta^{1-\alpha}\right)^2\E(\|\vm_k\|^2) + 3L^2 d_k.\label{equ:grad-bound-1}
\end{align}
Choose $\eta$ such that $\eta^{1-\alpha}<\frac{\lambda_{\min}^3}{12L\lambda_{\max}^2\eta_{\max}}$ and let the function \begin{align}
    K_1(\vm_0,\vx_0) & =3\E\left(\|\nabla\Loss(\vx_0)\|  +\frac{\lambda_{\min}}{12\lambda_{\max}}\|\vm_0\|\right)^2 \\ &\qquad + \left(\frac{\lambda_{\min}}{6\lambda_{\max}}\right)^2(3+6\frac{L\eta_{\max}}{\lambda_{\min }})K_0+\left(\frac{L\eta_{\max}}{4\lambda_{\min}}\right)K_0
\end{align}
where recall $K_0$ is the constant from \Cref{lem:exp_norm_m}. Plug \Cref{equ:expe-norm-m} into \Cref{equ:grad-bound-1} gives
\begin{align}
\E\|\nabla_k\|^2 & \leq K_1 +\frac{1}{2}\sup_{\tau=1,...,k}\E(\|\nabla_\tau\|)^2+ 3L^2 d_k.
\end{align}
therefore 
\begin{align}\label{equ:lya-1}\E\|\nabla_k\|^2  \leq 2K_1 + 6L^2\sup_{\tau=1,...,k}  d_\tau.
\end{align}
Now we need to bound $d_k$ by iteration.
\begin{align}
d_{k+1} &= d_{k} +\E\|\vy_{k+1}-\vy_k\|^2 + 2\E(\vy_{k+1}-\vy_k)^\top(\vy_k-\vy_0)\\
&= d_k + \bar{\eta}_k^2 \E\norm{\nabla_k}^2 + \bar{\eta}_k^2\sigma^2 \E\tr(\mSigma(\vx_k)) + 2 \E(\nabla_k)^\top(\vy_k-\vy_0)\\
&\leq d_k + \frac{\lambda^2_{\max}}{\lambda^2_{\min}}\eta_{\max}^2\eta^2 \E\norm{\nabla_k}^2 + \frac{\lambda^2_{\max}}{\lambda^2_{\min}}\eta_{\max}^2\eta C_{\tr} + 2 \frac{\lambda_{\max}}{\lambda_{\min}}\eta_{\max}\eta\sqrt{d_k\E\norm{\nabla_k}^2}
\end{align}
Let function $K_2(\vm_0,\vx_0) = \frac{\lambda^2_{\max}}{\lambda^2_{\min}}\eta_{\max}^2 C_{\tr} +2\frac{\lambda^2_{\max}}{\lambda^2_{\min}}\eta_{\max}^2K_1(\vm_0,\vx_0) +2 \frac{\lambda_{\max}}{\lambda_{\min}}\eta_{\max}\frac{K_1(\vm_0,\vx_0)}{\sqrt{6}L}$, and $\bar{d}_k=\sup_{\tau=1,...,k} d_{\tau}$. We know
\begin{align}
\sqrt{d_k\E\norm{\nabla_k}^2} & \leq \sqrt{6L^2}d_k + \frac{K_1}{\sqrt{6L^2}},\\
\bar{d}_{k+1} &\leq \bar{d}_k + K_2\eta + 6L^2\frac{\lambda^2_{\max}}{\lambda^2_{\min}}\eta_{\max}^2\eta^2 \bar{d}_k +  2 \frac{\lambda_{\max}}{\lambda_{\min}}\eta_{\max}\eta(\sqrt{6}L\bar{d}_k)\\
& \leq (1+6L \frac{\lambda_{\max}\eta_{\max}}{\lambda_{\min}}\eta+6L^2\frac{\lambda^2_{\max}\eta_{\max}^2}{\lambda^2_{\min}}\eta^2)\bar{d}_k + \eta K_2
\end{align}
 Let $\kappa = 1+6L \frac{\lambda_{\max}\eta_{\max}}{\lambda_{\min}}\eta+6L^2\frac{\lambda^2_{\max}\eta_{\max}^2}{\lambda^2_{\min}}\eta^2$, then by Gr\"{o}nwall's inequality \Cref{lem:gronwall-1}, 
\begin{align}
\bar{d}_k   
    & \leq \eta K_2 (1 + \sum_{s=0}^{k-1} \kappa \exp^{\kappa s})\\
    & \leq K_2 + K_2 T \exp^{1+6L \frac{\lambda_{\max}\eta_{\max}}{\lambda_{\min}}T+6L^2\frac{\lambda^2_{\max}\eta_{\max}^2}{\lambda^2_{\min}}T}.
\end{align}
Plugging into \Cref{equ:lya-1} finished the proof.
\end{proof}
\begin{lemma}
    There is a function $f$, irrelevant to $\eta$, of polynomial growth in $\vm_0$ and $\vx_0$,   such that 
    \[\E\norm{\vm_k}^2  \leq \eta^{\alpha-1} f(\vm_0,\vx_0,T)\]
    for all $k\leq \frac{T}{\eta}$.\label{lem:boun-mome}
\end{lemma}
\begin{proof}
    The result directly follows from \Cref{lem:exp_norm_m} and \Cref{lem:gradient_norm}.
\end{proof}
\begin{lemma}
    %Assume that conditions 1 and 2 of \Cref{thm:coupled_weak_approx} hold.
    %Then 
    There exists function $h$ that, for all $k\leq \frac{T}{\eta}$,
    \begin{align*} 
    	\E[(1+\|\vx_k\|^2)^{m}] &\leq h(\vm_0, \vx_0,m,T), \\
    	\E[(1+\|\vy_k\|^2)^{m}] &\leq h(\vm_0, \vx_0,m,T), \\
    	\E[\eta^{m}\|\vg_k\|^{2m}] &\leq h(\vm_0, \vx_0,m,T),
    \end{align*}
	and that $h$ is irrelevant to $k$ and $\eta$. 
	%\snote{Kaifeng: can you take a look at this? The constant depends on $\|\vm_0\|$ too.}
	\label{lem:small_higher_order}
\end{lemma}

\begin{proof}

    We use the fact that $(a+b)^m \leq 2^{m-1}(a^m + b^m)$ from the Jensen inequality for $a,b>0$ and $m\geq 1$. Furthermore by Young's inequality $a^\alpha b^{\beta}\leq \frac{\alpha a^{\alpha+\beta}+ \beta b^{\alpha+\beta}}{\alpha+\beta}\leq a^{\alpha+\beta}+b^{\alpha+\beta}$ for $\alpha,\beta>0$.
	%First, we show that $\E[\eta^{m}\|\vg_k\|_2^{2m}] \leq f_3(\vm_0, \vx_0,m)(1 + \|\vx_k\|)^{2m}$. 
	%We can bound $G^{2m}$.
	\begin{align}
		\norm{\vg_k}^{2m} &\leq (\|\nabla_k\| + \sigma\|\vv_k\|)^{2m} \\
        & \leq 2^{2m-1} ( \|\nabla_k\|^{2m} + \eta^{-m}\|\vv_k\|^{2m})\\ 
		\|\nabla_k\|^{2m} &\leq (\|\nabla_0\| + L\|\vx_k - \vx_0\|)^{2m} \\
		&\leq 2^{2m-1}((\|\nabla_0\| + L\|\vx_0\|)^{2m} + L^{2m}\|\vx_k\|^{2m})
  \label{moments-bound-1}
	\end{align}
    %Moveover we again define the sequence
    %$\tilde{\vm}_k = \beta_{0:k-1} \vm_0 + \sum_{s=0}^{k-1}\beta_{s+1:k-1}(1-\beta_{s})\nabla_s$ and $\hat{\vm}_k = \vm_k-\tilde{\vm_k}$ which are the momentum counting the two components of the gradients, then there is
    %\[\tilde{\vm}_{k+1} = \beta_{k}\tilde{\vm}_{k} + (1-\beta_{k})\nabla_{k},\]
    %\[\hat{\vm}_{k+1} = \beta_{k}\hat{\vm}_{k} + (1-\beta_{k})\sigma\vv_k.\]
	The term $\E\norm{\vv_k}^{2m}$ is bounded by \Cref{assume:ngos}. Now we need to show the bounds on $\E[(1+\|\vx_k\|^2)^{m}] $ and $\E[(1+\|\vy_k\|^2)^{m}] $. Specifically we shall prove $\E[(1+\|\vy_k\|^2+\eta^{2-2\alpha}\|\vm_k\|^2)^{m}] \leq h(\vm_0, \vx_0,m,T)$. 

    Let 
    \begin{align*}
        \delta_k & = \bar{\eta}_k^2 \norm{\vg_{k}}^2 - 2\bar{\eta}_k \dotp{\vg_{k}}{\vy_{k}} 
         +\eta^{2-2\alpha}(\norm{\beta_k\vm_k+(1-\beta_k)\vg_k}^2-\norm{\vm_k}^2),
    \end{align*}
    then there is
	\begin{align*}	&\qquad\E[(1+\|\vy_{k+1}\|^2+\eta^{2-2\alpha}\|\vm_{k+1}\|^2)^m|\vy_{k},\vm_k]\\
   & = \E [(1+\|\vy_{k} - \bar{\eta}_k\vg_{k}\|^2+
   \eta^{2-2\alpha}\norm{\beta_k\vm_k+(1-\beta_k)\vg_k}^2)^m|\vy_{k},\vm_k]\\
        & = \E [(1+\|\vy_{k}\|^2+\eta^{2-2\alpha}\|\vm_k\|^2+\delta_k)^m|\vy_{k},\vm_k]\\
        &\leq (1+\norm{\vy_k}^2+\eta^{2-2\alpha}\|\vm_k\|^2)^m + m(1+\norm{\vy_k}^2+\eta^{2-2\alpha}\|\vm_k\|^2)^{m-1} \E[\delta_k|\vy_k,\vm_k] \\&+  \E[\delta_k^2\sum_{i=0}^{m-2} \binom{m}{i+2}\delta_k^{i}(1+\norm{\vy_k}^2+\eta^{2-2\alpha}\|\vm_k\|^2)^{m-2-i}|\vy_{k},\vm_k]\\
        &\leq (1+\norm{\vy_k}^2+\eta^{2-2\alpha}\|\vm_k\|^2)^m + m(1+\norm{\vy_k}^2+\eta^{2-2\alpha}\|\vm_k\|^2)^{m-1} \E[\delta_k|\vy_k,\vm_k] \\& +  2^m\E[\delta_k^2 
        (|\delta_k|^{m-2}+(1+\norm{\vy_k}^2+\eta^{2-2\alpha}\|\vm_k\|^2)^{m-2})|\vy_{k},\vm_k].
        \end{align*}
	%$\|\vy_{k-1}\|^{2m}$ is small by the inductive hypothesis. Using the assumption, we can write
	%\begin{align*}
	%	\|\vg_{k-1}\|^{2m} \leq f_3(\vm_0,\vx_0,m)(1+\|\vx_{k-1}\|^{2m})
	%\end{align*}
    %We know	from \Cref{lem:gradient_norm} that $\sup_{k=0,...,\lfloor T/\eta \rfloor} \E\norm{\nabla_k}^2 \leq f(\vm_0,\vx_0,T)$ for some function $f$. 
    Then there exists constant $K_3$ such that
    \begin{align*}	
        \E[\delta_k|\vy_k,\vm_k] & =\bar{\eta}_k^2 \norm{\nabla_{k}}^2 + \bar{\eta}_k^2\sigma^2 \E\tr\mSigma(\vx_k) - 2\bar{\eta}_k \dotp{\nabla_{k}}{\vy_{k}} 
         +\eta^{2-2\alpha}(\norm{\beta_k\vm_k+(1-\beta_k)\vg_k}^2-\norm{\vm_k}^2)\\
        & \leq \E_{|\vy_k}\frac{\lambda^2_{\max}\eta^2_{\max}}{\lambda^2_{\min}}( 2\eta^2(\|\nabla_0\| + L\|\vx_0\|)^{2} + 2\eta^2L^{2}\|\vx_k\|^{2} + \eta C_{\tr})\\      
    &\quad +2\frac{\lambda_{\max}\eta_{\max}}{\lambda_{\min}}\eta (\|\nabla_0\| + L\|\vx_k\|+L\| \vx_0\|)\norm{\vy_k}\\
    &\quad + \eta^{2-2\alpha}(\frac{1-\beta_k}{1+\beta_k}\norm{\nabla_k}^2 + (1-\beta_k)^2\sigma^2\E\tr\mSigma(\vx_k))\\
        & \leq (1+\E[\norm{\vx_k}^2|\vy_k,\vm_k]+\norm{\vy_k}^2)\eta K_3.\\ 
    \end{align*}
    Note that 
    \begin{align*}
        \norm{\vx_k}^2 & = \norm{\vy_k+\bar{\eta}_k\beta_k(1-\beta_k)^{-1}\vm_k}^2\\
        & \leq 2\norm{\vy_k}^2 + \frac{2\eta_{\max}^2\lambda_{\max}^2}{\lambda_{\max}^4}\eta^{2-2\alpha}\norm{\vm_k}^2
    \end{align*}
    Then we can write $\E[\delta_k|\vy_k,\vm_k]\leq K_4\eta(1+\norm{\vy_k}^2+\eta^{2-2\alpha}\norm{\vm_k}^2)$ for some constant $K_4$. Futhermore, expansion gives for some constant $K_4,K_5,K_6,k_7$,
    \begin{align*}
        \E[\delta_k^2|\vy_k] & \leq K_4 \E(\eta \norm{\vx_k}^2+\eta\norm{\vv_k}^2+\eta\norm{\vy_k}^2+\eta^{1/2}\dotp{\vv_k}{\vy_k}+\eta^{3/2-\alpha}\dotp{\vm_k}{\vv_k}+\eta^{3/2-\alpha}\dotp{\vx_k}{\vv_k})^2\\
        &\leq K_5 \eta(1+\norm{\vy_k}^2+\eta^{2-2\alpha}\norm{\vm_k}^2).\\
        \E[|\delta_k|^m|\vy_k] & =K_6 \E(\eta \norm{\vx_k}^2+\eta\norm{\vv_k}^2+\eta\norm{\vy_k}^2+\eta^{1/2}\dotp{\vv_k}{\vy_k}+\eta^{3/2-\alpha}\dotp{\vm_k}{\vv_k}+\eta^{3/2-\alpha}\dotp{\vx_k}{\vv_k})^m \\
        &\leq K_7 \eta^{m/2}(1+\norm{\vy_k}^2+\eta^{2-2\alpha}\norm{\vm_k}^2).
    \end{align*}
    Therefore taking expecation with respect to all, we know 
    \[\E[(1+\|\vy_{k+1}\|^2+\eta^{2-2\alpha}\|\vm_{k+1}\|^2)^m]\leq (1+mK_4\eta + 2^m(K_5+K_7)\eta)\E(1+\|\vy_{k}\|^2+\eta^{2-2\alpha}\|\vm_{k}\|^2)^m\]
    Therefore 
    \begin{align*}\E(1+\|\vy_{k}\|^2+\eta^{2-2\alpha}\|\vm_{k}\|^2)^m & \leq e^{(mK_4\eta + 2^m(K_5+K_7)\eta)k} \E(1+\|\vy_{0}\|^2+\eta^{2-2\alpha}\|\vm_{0}\|^2)^m\\
    &\leq e^{mK_4T + 2^m(K_5+K_7)T}3^m(1+\E\|\vy_{0}\|^{2m}+\eta^{2-2\alpha}\E\|\vm_{0}\|^{2m}).
    \end{align*}
    And clearly $\E\|\vy_{0}\|^{2m}\leq 2^m \E\|\vx_{0}\|^{2m} + (\frac{2\lambda_{\max}\eta_{\max}}{\lambda_{\min}^2})^m \eta^{m-m\alpha}\E\|\vm_{0}\|^{2m}$. Therefore we finished bounding the moments of $\vy_k$ and $\eta^{\frac{1-\alpha}{2}}\vm_k$.
    Then for $\vx_k$, we can write
	$$ \|\vx_{k}\|^{2m} \leq 2^{2m}\left(\|\vy_{k}\|^{2m} + (\frac{2\lambda_{\max}\eta_{\max}}{\lambda_{\min}^2})^m \eta^{m-m\alpha} \|\vm_{k}\|^{2m}\right) $$
    And for $\vg_k$ with \Cref{moments-bound-1} we are able to finish the proof.
	%Then, the only term remaining to bound is $\|\vm_{k-1}\|^{2m}$. 
	%\begin{align*}
		%\|\vm_{k-1}\|^{2m} &= \|\beta^{k-1}\vm_0 + \sum_{s=0}^{k-2}\beta^{k-s-1}(1-\beta)\vg_s\|^{2m} \leq (\beta^{k-1}\|\vm_0\| + G)^{2m}
	%\end{align*}
	%We bounded $G^{2m}$ above.
	%$\sup\|\vx_\tau\|$ is small by the inductive hypothesis, so we are done.
\end{proof}
% As a result, we can write 
%\begin{align}
%\norm{\E\vm_k} &\leq \|\vm_0\| + \mathcal O(1) + \frac{L\eta}{1-\beta}\sup_{\tau=0,...,k-1} \E\|\vm_\tau\|
%\end{align}
\begin{proof}[Proof for \Cref{thm:coupled_weak_approx}]
	We expand the weak approximation error for a single $k$. There is $\theta\in[0,1]$ and $\vz=\theta \vx_k+(1-
 \theta)\vy_k$ such that
	\begin{align*}
		|\E h(\vx_k) - \E h(\vy_k)| &\leq \E[|\langle \nabla h(\vz), \vx_k-\vy_k \rangle|] \\
		&\leq \E[\|\nabla h(\vz) \| \|\vx_k-\vy_k\|] \\
		&\leq K(\vx_0, \vm_0,T) \E[\|\vx_k-\vy_k\|]
	\end{align*}
	where $x_0$ is the initialization and the first line follows from the mean value theorem. $K(x_0)$ has polynomial growth, and the last line follows from the assumption that the test function and its derivatives have polynomial growth combined with \Cref{lem:small_higher_order}.
	From the definition of the coupled trajectory, we can write
	\begin{align*}
		\E \|\vx_k - \vy_k \| \leq \frac{\lambda_{\max}\eta_{\max}}{\lambda^2_{\min}}\eta^{1-\alpha} \E \|\vm_k\|	
	\end{align*}
	Then, we apply \Cref{lem:boun-mome} together to write

 \iffalse
	\begin{align*}
		\E\|\vm_k\| &\leq \sqrt{\frac 1\eta \frac{1-\beta}{1+\beta} C} + \|\vm_0\| + \frac{L\eta}{1-\beta}\sup_{\tau=0,...,k-1} \E\|\vm_\tau\| + \mathcal O(1) \\
		&\leq \frac{L\eta}{1-\beta}\sup_{\tau=0,...,k-1} \E\|\vm_\tau\| + \mathcal O(\eta^{(\alpha-1)/2})
	\end{align*}

    Choose $\eta$ small enough such that $\eta<(\frac{\lambda}{2L})^{1/(1-\alpha)}$,
    \begin{align*}
		\sup_{\tau=0,...,k-1} \E\|\vm_\tau\|  &\leq\frac{L\eta}{1-\beta}\sup_{\tau=0,...,k-1} \E\|\vm_\tau\| + \mathcal O(1) \\
		&\leq \frac{L\eta}{1-\beta}\sup_{\tau=0,...,k-1} \E\|\vm_\tau\| + \mathcal O(\eta^{(\alpha-1)/2})
	\end{align*}
 
	Applying \Cref{lem:exp_norm_m} repeatedly to bound the supremum gives us that $\sup_{\tau=0,...,k-1} \E\|\vm_\tau\| = \norm{\E\nabla_k} + \mathcal O(\eta^{(\alpha-1)/2})$.
	As a result,
 \fi
	$$  \E\|\vm_k\|\leq\sqrt{\E\norm{\vm_k}^2}  \leq \eta^{\frac{\alpha-1}{2}} f(\vm_0,\vx_0,T) $$
	which implies that
	$$|\E h(\vx_k) - \E h(\vy_k)| \leq \eta^{(1-\alpha)/2} \frac{\lambda_{\max}\eta_{\max}}{\lambda^2_{\min}}f(\vm_0,\vx_0,T)K(\vx_0, \vm_0,T).$$
\end{proof}
\subsubsection{Step 2}
%\runzhe{define $\Delta$ and $\tilde\Delta$ here}
In this section we are comparing the trajectory $\vy_k$ with a SGD trajectory $\vz_k$. To avoid notation ambiguity, denote $\vg(\vx)\sim\cG_\sigma(\vx)$ to be the stochastic gradient sampled at $\vx$. . Recall that (with $\vy_0=\vx_{0}-\frac{\bar{\eta}_0\beta_0}{1-\beta_0}\vm_k$ and $\vz_0=\vx_0$)
\begin{align*}
\vy_{k} =\vy_{k-1}-\bar{\eta}_{k-1}\vg(\vx_{k-1})  \\
\vz_{k} =\vz_{k-1}-\bar{\eta}_{k-1}\vg(\vz_{k-1})  \\
\end{align*}
The only difference in the iterate is that the stochastic gradients are taken at close but different locations of the trajectory. Therefore to study the trajectory difference, we adopt the method of moments proposed in~\citet{li2019stochastic}.

We start by defining the one-step updates for the coupled trajectory and for SGD. 
\begin{definition}[One-Step Update of Coupled Trajectory]\label{def:one_step_coupled}
    The one-step update for the coupled trajectory $\vDelta$ can be written as 
    \begin{align*}
		\vDelta(\vy, \vm,C) &= -\eta\vg(\vx) = -\eta\vg\left(\vy + C \eta^{1-\alpha}\vm\right)
    \end{align*}
\end{definition}

\begin{definition}[One-Step Update of SGD]\label{def:one_step_sgd}
    The one-step update for SGD $\tilde\vDelta$ can be written as 
    \begin{align*}
		\tilde\vDelta(\vy) &= -\eta\vg(\vy) 	
	\end{align*}
\end{definition}

\begin{lemma}[Close Moments]\label{lem:moment-diff}
	Let $\Delta$ be the one-step update for the coupled trajectory and $\tilde\Delta$ be the one-step update for SGD. Then for any $C\in [0,\frac{\lambda_{\max}\eta_{\max}}{\lambda^2_{\min}}]$, there is function $j(\vx_0,\vm_0,T)$ of polynomial growth in $\vx_0$ and $\vm_0$, independent of $\eta$, such that
	$$ \left|\E [\tilde\Delta_{(i)}(\vy) -\Delta_{(i)}(\vy,\vm_k,C)]\right| 
	\leq \eta^{\frac{3-\alpha}{2}} j(\vx_0,\vm_0,T) $$$$ \left|\E [\tilde\Delta_{(i)}\tilde\Delta_{(j)}(\vy) -\Delta_{(i)}\Delta_{(j)}(\vy,\vm_k,C)]\right| 
	\leq  \eta^{\frac{3-\alpha}{2}} j(\vx_0,\vm_0,T) $$for all $k\in [0,T/\eta]$.
\end{lemma}

\begin{proof}
%We first compute the moments of the updates $\Delta$ and $\tilde\Delta$. 
%Applying Lemma 5 of the SME paper gives us
%\begin{align*}
%	\E[\tilde\Delta_{(i)}(\vy)] &= -\eta\partial_{(i)} L(\vy) \\
%	\E[\tilde\Delta_{(i)}(\vy)\tilde\Delta_{(j)}(\vy)] &= \eta^2\partial_{(i)}L(\vy)\partial_{(j)} L(\vy) + \eta^2\sigma^2\mSigma(\vy)_{(i,j)} = O(\eta) \\
%	\E\left[\prod_{j=1}^3 |\tilde\Delta_{i_j}(\vy)|\right] &= O(\eta^3) 
%\end{align*}
%So, we can write
We can write
\begin{align*}
	\left|\E[\E [\tilde\Delta_{(i)}(\vy) -\Delta_{(i)}(\vy)\mid \vm ]]\right| &= \eta \left| \E[\partial_{(i)} \Loss(\vy) -  \partial_{(i)} \Loss\left(\vy + C\eta^{1-\alpha} \vm\right)] \right| \\
	&\leq LC\eta^{2-\alpha} \E[\|\vm\|]
\end{align*}
where the second step uses the Lipschitzness of the loss gradient. The proof can be completed by noting $\E[\|\vm\|] = O(\eta^{(\alpha-1)/2})$ (\Cref{lem:boun-mome}).
\begin{align*}
	\left|\E[\E [\Delta_{(i)}(\vy)\Delta_{(j)}(\vy) -\tilde\Delta_{(i)}(\vy)\tilde\Delta_{(j)}(\vy)\mid \vm ]]\right| &= \eta^2 \partial_i\Loss\partial_j \Loss(\vy) + \eta^2\sigma^2\mSigma_{ij}(\vy)  \\ & - \E\left[\eta^2\partial_i\Loss\partial_j \Loss\left(\vy + C\eta^{1-\alpha}\vm\right) - \eta^2\sigma^2 \mSigma_{ij}(\vy + C\eta^{1-\alpha}\vm)\right]\\
 &\leq \eta^{2-\alpha} C(L_\Sigma +\eta L) \E[\|\vm\|]
\end{align*}
where $L_\Sigma$ is the smoothness of $\mSigma$. Again by \Cref{lem:boun-mome} we obtain the desired result.
\end{proof}

\begin{lemma}
	%Let $\vDelta(\vy, \vm)$ be the single step update for the coupled trajectory (\Cref{def:coupled_trajectory}), as defined below.
	%\begin{align*}
	%	\vDelta(\vy, \vm) &= -\eta\vg(\vx) = -\eta\vg\left(\vy + \frac{\eta\beta}{1-\beta}\vm\right) 	
	%\end{align*}
	%Then, 
    For any $m\geq 1$, $C\in [0,\frac{\lambda_{\max}\eta_{\max}}{\lambda^2_{\min}}]$,
	\begin{align*}
            \E[\|\vDelta(\vy_k, \vm_k)\|_2^{2m}] \leq \eta^{m} h(\vm_0, \vx_0,m,T)
	\end{align*}
    for all $k\leq T/\eta$.\label{lem:bounded_coupled_updates}
\end{lemma}
\begin{proof}
	Pulling $\eta^{2m}$ out of the update and applying \Cref{lem:small_higher_order} completes the proof.
\end{proof}



The below lemma is an analog to Lemma 27 of \citet{li2019stochastic}, Lemma C.2 of \citet{li2021validity}, and Lemma B.6 of \citet{malladi2022sdes}.
It shows that if the update rules for the two trajectories are close in all of their moments, then the test function value will also not change much after a single update from the same initial condition. 
\begin{lemma}
	Suppose $u\in G^{\lceil\gamma\rceil + 1}$ for $\gamma\geq 0$. Assume Conditions 1 and 2 from \Cref{thm:one_step_to_many} hold. Then, there exists a function $K\in G$ independent of $\eta$ such that
	$$\left|\E[u(\vx + \Delta(\vx,k))] - \E[u(\vx + \tilde\Delta(\vx,k))]\right| \leq K(\vx)\eta^{\gamma+1} $$
	\label{lem:one_step}
%\runzhe{We need to expand $u$ so R is the $(p+1)$-th order remainder. Write the proof formally for all $p$}
%\runzhe{$p=2$ is what we needed at last}
\end{lemma}
\begin{proof}

Since $u \in G^{\lceil\gamma\rceil + 1}$, we can find $K_0 \in G$ such that $u(\vx)$ is bounded by $K_0(\vx)$ and so are all the partial derivatives of $u$ up to order $4$.
By Taylor's Theorem with Lagrange Remainder, for all $1 \le j \le N$, $1 \le k \le N$ we have
\begin{align*}
    u(\vx + \vDelta(\vx, k)) - u(\vx + \tilde{\vDelta}(\vx, k))
    &= \underbrace{\sum_{1\leq i\leq D} \frac{\partial u(\vx)}{\partial x_i} \left(\Delta_i(\vx,k) - \tilde\Delta_i(\vx, k)\right)}_{A} \\
    &+ \underbrace{\frac{1}{2} \sum_{1\leq i_1, i_2\leq D}\frac{\partial^2 u(\vx)}{\partial x_{i_1}\partial x_{i_2}} \left(\Delta_{i_1}(\vx,k)\Delta_{i_2}(\vx,k) - \tilde\Delta_{i_1}(\vx, k)\tilde\Delta_{i_2}(\vx,k)\right)}_B \\
    &+ R_j - \tilde{R}_j
\end{align*}
where the remainders $R_j$, $\tilde{R}_j$ are
\begin{align*}
    R_j &:=
    \frac{1}{3!} \sum_{1 \le i_1, i_2, i_3 \le D} \frac{\partial^3 u(\vx + a \vDelta(\vx, k))}{\partial x_{i_1} \partial x_{i_2}\partial x_{i_3}} \Delta_{i_1}(\vx, k) \Delta_{i_2}(\vx, k) \Delta_{i_3}(\vx, k). \\
    \tilde{R}_j &:=
    \frac{1}{3!} \sum_{1 \le i_1, i_2, i_3 \le D} \frac{\partial^3 u(\vx + a \tilde\vDelta(\vx, k))}{\partial x_{i_1} \partial x_{i_2}\partial x_{i_3}} \tilde\Delta_{i_1}(\vx, k) \tilde\Delta_{i_2}(\vx, k)\tilde\Delta_{i_3}(\vx, k).
\end{align*}
for some $a, \tilde{a} \in [0, 1]$.
By Condition 1 of \Cref{thm:one_step_to_many}, 
\begin{align*}
\E[A + B] \leq (D+D^2/2) K_0(\vx)K_1(\vx)\eta^{\gamma+1}
\end{align*}

Now let $\kappa_0, m$ be the constants so that $K_0(\vx)^2 \le \kappa_0^2 (1 + \normtwosm{\vx}^{2m})$.

For $R_j$, by Cauchy-Schwarz inequality we have
\begin{align*}
    \E[R_j] 
    &\le \frac{1}{3!}\left(\sum_{i_1, i_2, i_3}
        \E\left[\abs{\frac{\partial^3 u(\vx + a \vDelta(\vx, k))}{\partial x_{i_1} \partial x_{i_2}\partial x_{i_3}}}^2\right]
    \right)^{1/2}
    \cdot
    \left(\E[\|\vDelta(\vx, k)\|_2^6]\right)^{1/2} \\
    &\le \frac{1}{3!}\left(D^{3/2} K_0(\vx + a\vDelta(\vx,k))\right)\cdot 
    \E[\|\vDelta(\vx, k)\|_2^3] \\
    &\le \frac{1}{3!}\left(D^{3/2} K_0(\vx + a\vDelta(\vx,k))\right)\cdot 
    \eta^{1+\gamma}K_2(\vx)
\end{align*}

\iffalse
\begin{align*}
    \E[R_j]
    &\le \frac{1}{2} \left(
        \sum_{i_1, i_2}
        \E\left[\abs{\frac{\partial^2 u(\vx + a \vDelta(\vx, k))}{\partial x_{i_1} \partial x_{i_2}}}^2\right]
    \right)^{1/2}
    \cdot
        \E[\|\vDelta(\vx, k)\|_2^{4}]^{1/2} \\
    &\le \frac{1}{2} \left(D^2 \cdot K_0 (\vx + a \vDelta(\vx, k))\right)
    \cdot \E[\|\vDelta(\vx, k)\|_2^{2}]\\
    &\le \frac{1}{2} \left(D^2 \cdot K_0 (\vx + a \vDelta(\vx, k))\right)
    \cdot \eta^{1+\gamma} K_1(\vx),
\end{align*}
\fi

where the last line uses Condition 2 of \Cref{thm:one_step_to_many} and $K_2$ is a function of polynomial growth. 
%\red{TODO fix problem with power on $\eta$ in last line...}
For $K_0(\vx + a \vDelta(\vx, k))$, we can bound its expectation by
\begin{align*}
    \E[K_0(\vx + a \vDelta(\vx, k))]
    &\le \kappa_0 \E\left[1 + \normtwosm{\vx + a \vDelta(\vx, k)}^{2m}\right]^{1/2} \\
    &\le \kappa_0 \left( 1 + 2^{2m-1} \E[\normtwosm{\vx}^{2m} + \E\normtwosm{\vDelta(\vx, k)}^{2m}] \right)^{1/2} \\
    &\le \kappa_0 \left( 1 + 2^{2m-1} (\normtwosm{\vx}^{2m} + C_{2m} \eeta^{\gamma+1} (1 + \normtwosm{\vx}^{2m}))  \right)^{1/2}.
\end{align*}
%\runzhe{the constant $C_{2m}$ does appear in \cref{thm:one_step_to_many}. Define it somewhere}
where $C_{2m}$ arises from the constants in the polynomial growth function $K_2(\vx)$.
We use \Cref{lem:bounded_coupled_updates} between the second and third lines. 
Combining this with our bound for $\E[R_j]$ proves that $\frac{1}{\eta^{\gamma+1}}\E[R_j]$ is uniformly bounded by a function in $G$:
\begin{align*}
    \E[R_j] \le
    \eta^{\gamma +1} \cdot 
    \frac{1}{(\lceil\gamma\rceil+1)!} \cdot D^4 \cdot 
    \kappa_0 \left( 1 + 2^{2m-1} (\normtwosm{\vx}^{2m} + C_{2m} \eeta^{\gamma+1} (1 + \normtwosm{\vx}^{2m}))  \right)^{1/2}
    \cdot K_1(\vx).
\end{align*}

An analogous argument bounds $\E[\tilde{R}_j]$. Thus, the entire Taylor expansion and remainders are bounded as desired.
	
\end{proof}





\subsubsection{Main Result}
\begin{theorem}
	Let $T>0$, $\eta_{\text{thr}}>0$, and $\eta\in(0,\eta_{\text{thr}}]$, so $N=\lfloor T/\eta\rfloor$. Let $\gamma=\frac{1-\alpha}{2}> 0$ and $p\in \mathbf{Z}^+$. 
	Let $\vDelta$ be the single step update for the coupled trajectory $\vy_k$ and $\tilde\vDelta$ be the single step update for SGD $\vz_k$ (\Cref{def:one_step_coupled,def:one_step_sgd}).
	Suppose the following conditions hold:
	\begin{enumerate}
		\item \textbf{Close moments}: %For $s=1,...,\lceil\gamma\rceil$
         For $s=1,2$,  $$ \left| \E\prod_{j=1}^s \Delta_{i_j}(x) - \E\prod_{j=1}^s \tilde\Delta_{i_j}(x)\right| \leq K_1(x)\eta^{\gamma+1} $$
		\item \textbf{Small higher order moments}: $$ \E\prod_{j=1}^{3} |\Delta_{(i_j)}(x)| \leq K_2(x)\eta^{\gamma+1} $$
		\item \textbf{Bounded trajectory}: For each $m\geq 1$, the $2m$-moment of $\vx_k$ is uniformly bounded for all $k<T$ and $\eta$.
		\item \textbf{Lipschitz Gradients}: For all $\vx,\vy$, $\nabla L(\vx) - \nabla L(\vy) \leq L\|\vx-\vy\|$	
		\end{enumerate}
    Then $\vy_k$ and $\vz_k$ are order-$\gamma$-weak approximations of each other.
\label{thm:one_step_to_many}
\end{theorem}
\begin{proof}
	%Let $\vz_k$ be the SGD trajectory and $\vy_k$ be the coupled trajectory (\Cref{def:coupled_trajectory}).
	%Define $\Delta$ and $\tilde{\Delta}$ to be the coupled trajectory and SGD one-step updates, respectively:
	%\begin{align}
	%	\Delta(\vy) &= -\eta\vg(\vx) = -\eta\vg\left(\vy + \frac{\eta\beta}{1-\beta}\vm\right) \\
		%\tilde\Delta(\vy) &= -\eta\vg(\vy)
	%\end{align}
	Let $\hat\vy_{j,k}$ be the trajectory defined by following the coupled trajectory for $j$ steps and then standard SGD for $k-j$ steps.
	So, $\hat\vy_{j,j+1} = \vy_j + \tilde\Delta(\vy_j)$ and $\hat\vy_{j+1,j+1} = \vy_j + \Delta(\vy_j)$.
	Let $h$ be the test function with at most polynomial growth.
	Then, we can write
	\begin{equation}
		|\E[h(\vz_k) - \E[h(\vy_k)]| = \sum_{j=0}^{k-1} (\E[h(\hat\vy_{j+1,k})] - \E[h(\hat\vy_{j,k})])
	\end{equation}
	Define $u(\vy, s, t) = \E_{\hat\vy\sim\mathcal{P}(\vy, s, t)} [h(\hat\vy_t)]$, where $\mathcal{P}(\vy, s, t)$ is the distribution induced by starting at $\vy$ at time $s$ and following the SGD trajectory until time $t$.
	Then,
	\begin{equation}
		|\E[h(\vz_k) - \E[h(\vy_k)]| \leq \sum_{j=0}^{k-1} |\E[u(\hat\vy_{j+1,j+1}, j+1, k)] - \E[u(\hat\vy_{j,j+1}, j+1, k)]|
	\end{equation}
	Define $u_{j+1} = u(\vy, j+1, k)$.
	Then,
	\begin{equation}
		|\E[h(\vz_k) - \E[h(\vy_k)]| \leq \sum_{j=0}^{k-1} | \E[u_{j+1}(\vy_j + \tilde\Delta(\vy_j))] - \E[u_{j+1}(\vy_j + \Delta(\vy_j))]|
	\end{equation}
	\Cref{lem:one_step} shows that
	\begin{equation}
		| \E[u_{j+1}(\vy_j + \tilde\Delta(\vy_j))] - \E[u_{j+1}(\vy_j + \Delta(\vy_j))]| \leq \E[K(\vy_j)\eta^{\gamma+1}]
	\end{equation}
	where $K(\vy_j) = \kappa_1(1+\|\vy_j\|_2^{2m})$.
	Then,
	\begin{align*}
		|\E[h(\vz_k) - \E[h(\vy_k)]| &\leq \eta^{\gamma+1} \sum_{j=0}^{k-1} \E[\kappa_1 (1+\|\vy_j\|_2^{2m})] \\
		&\leq \eta^{\gamma+1} \sum_{j=0}^{k-1} \kappa_1(1+f(\vx_0, \vm_0)) \\
		&\leq \kappa_1(1+f(\vx_0, \vm_0)) T\eta^{\gamma}	
	\end{align*}
	where we use the fact that $\E[\|\vy_j\|_2^{2m}] \leq f(\vx_0, \vm_0)$ (\Cref{lem:small_higher_order}) and $\E[\|\vz_j\|_2^{2m}]\leq f(\vx_0, \vm_0)$ (Similarly applying \Cref{lem:small_higher_order} when $\beta_k=0$).
\end{proof}
\begin{proof}[Proof for \Cref{thm:weak-appro-main}]
By \Cref{lem:moment-diff} the two trajectories satisfy the close moments condition. By \Cref{lem:small_higher_order} they satisfy bounded trajectory condition. The Lipschitz Gradients 
condition is given in \Cref{assume:ngos} and now we show the two trajectories have small higher order moments: for
$m\geq 3$ there is for $\vv_k\sim\cG_\sigma(\vx+C\eta^{1-\alpha}\vm_k)$,
\begin{align*}\E\norm{\Delta(\vx,\vm_k,C)}^{m} & =\eta^m\E\norm{\nabla\Loss(\vx+C\eta^{1-\alpha}\vm_k)+\sigma\vv_k}^m\\
& \leq \eta^m2^m (\norm{\nabla\Loss(\vx)}+LC\eta^{1-\alpha}\norm{\vm_k})^m + \eta^{m/2}2^m\E\norm{\vv_k}^m = O(\eta^{m/2}).\\
\end{align*}
Therefore the higher order moments are of order $O(\eta^{3/2})$.

From \Cref{thm:coupled_weak_approx}, we know $\vx_k$ and $\vy_k$ are  order-$(1-\alpha)/2$-weak approximations of each other, and from \Cref{thm:one_step_to_many} we know $\vz_k$ and $\vy_k$ are  order-$(1-\alpha)/2$-weak approximations of each other. Then we conclude that $\vx_k$ and $\vz_k$ are order-$(1-\alpha)/2$-weak approximations of each other.
\end{proof}

\iffalse
If $\beta=1-\lambda\eta^\alpha$,
\begin{align}
	\E[x_{k,i} - y_{k,i}] = (\lambda\eta^{1-\alpha} - \eta) \E[m_{k,i}]	
\end{align}

To bound $\E[m_{k,i}]$, we unroll the momentum and write 
\begin{align}
	\vm_k &= \beta^k\vm_0 + \sum_{s=0}^{k-1} \beta^{k-s-1}(1-\beta)\vg_s \label{eq:unrolled_momentum} \\
	\E\vm_k &= \beta^k\E\vm_0 + \sum_{s=0}^{k-1} \beta^{k-s-1}(1-\beta)\nabla_s  \label{eq:unrolled_momentum2}
\\
	\E m_{k,i} &= \beta^k \E m_{0,i} + \sum_{s=0}^{k-1} \beta^{k-s-1}(1-\beta)\nabla_{s,i} 
\end{align}

We replace $\nabla_{s,i}$ with $\nabla_{s,i} - \E \nabla_{k,i} + \E \nabla_{k,i}$. 
We use the Lipschitzness of $\nabla$ to write 
\begin{align*}
	\|\nabla_s - \nabla_k\| &\leq L\eta(k-s)\sup_{\tau=s,...,k-1}\|\vm_\tau\| \\
	\E[\nabla_{s,i} - \nabla_{k,i}] &\leq \frac{1}{\sqrt{P}} L\eta(k-s)\sup_{\tau=s,...,k-1}\|\vm_\tau\|     	
\end{align*}
where $P$ is the length of $\nabla$ (i.e., the number of parameters).

If we substitute this into \Cref{eq:unrolled_momentum2}, then we can write
$$ \E\vm_k = \beta^k\E\vm_0 + \sum_{s=0}^{k-1} \beta^{k-s-1}(1-\beta)(\|\nabla_k\| + L\eta(k-s)\sup_{\tau=s,...,k-1} \|\vm_\tau\|) $$
We first bound the coefficients in the last term to be $\mathcal O(\eta^{1-\alpha})$.
$$ \sum_{s=0}^{k-1} \beta^{k-s-1} (1-\beta)\eta(k-s) \leq \frac{\eta}{1-\beta} = \mathcal{O}(\eta^{1-\alpha}) $$
Then, we bound $\E \|\vm_k\|^2$:
\begin{align}
        \E\norm{\vm_k}^2&= \norm{\E\vm_k}^2+ \E\norm{\vm_k-\E\vm_k}^2\\
        & = \norm{\E\vm_k}^2+\beta^{2k}\E\norm{\vm_0-\E\vm_0}^2 + \sum_{s=0}^{k-1} \beta^{2(k-s-1)}(1-\beta)^2\E\norm{\vsigma_s}^2\\
        & \leq \norm{\E\vm_k}^2 + \E\norm{\vm_0}^2 + \frac{1}{\eta}\frac{1-\beta}{1+\beta}\sum_{s=0}^{k=1}\tr(\mSigma_s\mSigma_s^\top) \label{eq:exp_mnorm_sq}
\end{align}

Since $\tr(\mSigma_s\mSigma_s^\top)$ is bounded for all time steps $s$, we drop the summation and just write $\tr(\mSigma\mSigma^\top)$. To handle the first term:
\begin{align}
\norm{\E\vm_k} &\leq \|\E\vm_0\| + \sum_{s=0}^{k-1} \beta^{k-s-1}(1-\beta)\norm{\E\nabla_k}+ \sum_{s=0}^{k-1} \beta^{k-s-1} (1-\beta) L \eta (k-s) \sup_{\tau=s,...,k-1} \E\|\vm_\tau\|	\\
& \leq \|\E\vm_0\| + \norm{\E\nabla_k} + \frac{L\eta}{1-\beta}\sup_{\tau=0,...,k-1} \E\|\vm_\tau\|
\end{align}
We use this with \Cref{eq:exp_mnorm_sq} to bound $\E\norm{\vm_k}$.
\begin{align}
	\E\norm{\vm_k} &\leq \sqrt{\E\norm{\vm_k}^2}\\
		& \leq \sqrt{\norm{\E\vm_k}^2 + \E\norm{\vm_0}^2 + \frac{1}{\eta}\frac{1-\beta}{1+\beta}\tr(\mSigma\mSigma^\top)}\\
		& \leq \norm{\E\vm_k} + \sqrt{\E\norm{\vm_0}^2} +\sqrt{\frac{1}{\eta}\frac{1-\beta}{1+\beta}\tr(\mSigma\mSigma^\top)}\\
		& \leq \norm{\E\nabla_k} + 2\sqrt{\E\norm{\vm_0}^2} +\sqrt{\frac{1}{\eta}\frac{1-\beta}{1+\beta}\tr(\mSigma\mSigma^\top)}+\frac{L\eta}{1-\beta}\sup_{\tau=0,...,k-1} \E\|\vm_\tau\|
\end{align}
From this, we can conclude that $\sup_{\tau=0,...,k-1} \E\|\vm_\tau\| = \norm{\E\nabla_k} + \mathcal O(\eta^{(\alpha-1)/2})$, which also implies that $\norm{\E\vm_k}\leq 2\norm{\E\nabla_k} + \mathcal O(1)$. 

Finally, we can bound $\E\|\nabla_k\|$. Let $d_k = \E\|\vy_k-\vy_0\|^2$, so 
\begin{align}
d_{k+1} &= d_{k} +\E\|\vy_{k+1}-\vy_k\|^2 + 2\E(\vy_{k+1}-\vy_k)^\top(\vy_k-\vy_0)\\
&\leq d_k + \eta^2 \E\norm{\nabla_k}^2 + \eta \tr(\mSigma\mSigma^\top) + 2 \eta\sqrt{d_k}\E\norm{\nabla_k}
\end{align}
\begin{align}
\|\nabla_k\| & \leq \|\nabla_0\| + L\norm{\vx_{k}-\vx_0}\\
& \leq \|\nabla_0\| + L\norm{\vy_{k}-\vy_0} + L\norm{\vx_{0}-\vy_0} + L\norm{\vx_{k}-\vy_k} \label{eq:normnablak}
\end{align}
Then, we note that $\|x_k - y_k\| = \frac{\eta\beta}{(1-\beta)}\|\vm_k\|$. So
\begin{align}
\E\|\nabla_k\| & \leq \E\|\nabla_0\| + L\sqrt{d_k} +\frac{\eta\beta}{1-\beta}(\E\|\vm_0\|+\E\|\vm_k\|)\\
& \leq O(1) + L\sqrt{d_k} + O(\eta^{1-\alpha})\E\|\nabla_k\|
\end{align}
From \Cref{eq:normnablak},
\begin{align}
\E\|\nabla_k\|^2 & \leq (O(1) + L\sqrt{d_k})^2
\end{align}
plugin

\begin{align}
d_{k+1} &\leq d_k + \eta^2 \E\norm{\nabla_k}^2 + \eta \tr(\mSigma\mSigma^\top) + 2 \eta\sqrt{d_k}\E\norm{\nabla_k}\\
&\leq (1+\eta^2L^2+2\eta L)d_k + O(\eta)
\end{align}
 Then by Gronwall's inequality, $d_k$ is bounded by $O(1)$ and so is $\E\|\nabla_k\|$ for $k=O(1/\eta)$. The proof is completed by noting $\|\E\nabla_k\| \leq \E\|\nabla_k\|$
 \fi