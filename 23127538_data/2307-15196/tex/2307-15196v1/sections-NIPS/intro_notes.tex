

\subsection{Previous notes v2}
%It is thus left open to why SGDM performs better than vanilla SGD in practice, and previous works have proposed many different hypotheses.

\tianhao{I think we should also mention here that adding momentum allows larger 
lr in deep learning training, as well as our experiments}

However, for stochastic optimization, the situation is much more complicated as the instability caused by discretization further intertwines with the instability from gradient noise which is ubiquitous and can be very large in modern deep learning.
[There is empirical evidence showing the stabilizing effect of momentum \citep{xx}.]
% \tianhao{Also mention our experiment here?}
% mention an empirical paper claiming that momentum can stabilize training.
% For SGD, the training instability can also come from the noise. 
Intuitively, momentum makes SGD direction less noisy and more accurate by averaging over past stochastic gradients if the parameter does not move drastically at each step.
This is identified as the variance reduction effect of momentum in existing works~\citep{cutkosky2019momentum,defazio2020momentum,bottou2018optimization,kingma2015adam,cutkosky2020momentum}.
However, unlike the noiseless case, there is no theoretical analysis indicating that momentum accelerates the convergence of SGD. 
Thus it is natural to ask the following question:
\begin{quote}
    \emph{Does momentum benefit SGD convergence when noise is the main source of instability/ in the noise-dominated regime?}    
\end{quote}

Indeed, there are concrete examples in the literature showing situations where SGDM cannot outperform SGD in convergence rates~\citep{jain2018accelerating,kidambi2018on}.
It is also predicted by \citet{orr1996dynamics,wiegerinck1994} that in the stochastic case, momentum does not speed up the asymptotic convergence rate to local minima.
To approach the above question more rigorously

\tianhao{I think we can mention the descent lemma here to justify why we consider small lr regime}


\subsection{Previous notes v1}

However, as pointed out by many papers~\citep{orr1996dynamics,wiegerinck1994},
the above intuition only applies to the noiseless case. 
In deep learning, where gradient noise is ubiquitous and can be very large, the effectiveness of momentum had not been discovered until the influential work of \citet{sutskever2013on},
which empirically shows that SGDM with carefully-chosen hyperparameters can, in fact, accelerate the training of deep neural networks. 
In the meantime, \citet{sutskever2013on} pointed out that momentum accelerates training in deep learning possibly because the optimization of neural networks operates far from any local minima during most of the training time.
While it is predicted that in the stochastic case, momentum does not speed up the asymptotic convergence rate to local minima~\citep{orr1996dynamics,wiegerinck1994}.
Besides, there are also concretes examples in the literature where momentum does not accelerate training~\citep{jain2018accelerating,kidambi2018on}.
So far, the reason why momentum helps stochastic optimization in deep learning remains a subject of ongoing debate [].
% The instability caused by noise intertwines with that from discretization, .

\zhiyuan{It is well known that momentum benefits convergence of GD, at least in the strongly case, where momentum improves condition number and allows larger lr. However, for sgd, there is another source of instatbility which is the stochastic gradient noise. Intuitively, momentum makes SGD direction less noisy and more accurate by averaging over past stochastic gradients if parameter does not move drastically each step. However, unlike the deterministic case, there is no theoretical analysis indicating turning on momentum accelerates convergence of SGD. Thus it is natural to ask the following question:
Does momentum benefit SGD convergence when noise is the main source of instability/ in the noise dominated regime? }

%A natural hypothesis is that both SGDM and SGD resemble their counterparts with exact gradients, and hence momentum provides acceleration.

---------

why do people use momentum?
\begin{enumerate}
    \item 
    Given an exact gradient oracle,
    classic optimization theory~\citet{polyak1964some,polyak1987intro} suggests that momentum can accelerate convergence to a local minimum.
    \item \citet{sutskever2013on} brought momentum into deep learning
\end{enumerate}

Why should people care about the role of momentum?
\begin{enumerate}
    \item Momentum costs extra memory and people want to design more memory-efficient optimization algorithms, e.g., SGD without momentum. For that purpose, we must first understand the underlying mechanism of momentum. 
    \item Theoretical interest. Theorists prefer simpler dynamical models, so many theoretical analyses are done for vanilla SGD. How well do these analyses capture the dynamics of SGD+M?
\end{enumerate}
The difficulty of understanding momentum:
\begin{enumerate}
    \item In the noiseless case, \citet{polyak1987intro,rumelhart1987learning}: Momentum cancels out the oscillation along high-curvature directions and adds up contributions along low-curvature directions.
    \item However, neural nets are trained in the presence of gradient noise. \citet{orr1996dynamics,wiegerinck1994} predicted that in the stochastic case, momentum does not speed up the asymptotic convergence rate to local minima.
    \item Nevertheless, \citet{sutskever2013on} pointed out that momentum accelerates training in deep learning, possibly because the optimization of neural nets operates far from any local minima during most of the training time.
    \item \citet{jain2018accelerating,kidambi2018on} point out that momentum may not accelerate training when the gradient is stochastic. Some concrete quadratic examples are provided.
\end{enumerate}

How do people understand momentum now?
\begin{enumerate}
    \item Empirical:
    \begin{enumerate}
        \item Removing momentum can cause training instability. Momentum can allow neural nets to be trained with a larger step size without blowing up the loss \citep{plattner2022on} (list more papers)
    \end{enumerate}
    \item Theory:
    \begin{enumerate}
        \item The aforementioned view in the noiseless case: cancellation of oscillation.
        \item Cancellation of noise --- Variance reduction.
        \begin{enumerate}
            \item \citet{cutkosky2019momentum} propose a variant of momentum that can perform variance reduction to accelerate training. Then they say ``we feel that it provides strong intuitive evidence that momentum is performing some kind of variance reduction''.
            \item (need someone to check) \citet{defazio2020momentum}: momentum may cancel out a part of the noise when the stochastic gradient is positively aligned with the momentum buffer, which typically occurs in the early phase of training. (I suspect oscillation is more common, though)
            \item (weak evidence) Leon bottou's survey \citep{bottou2018optimization}: "iterations tend to move in directions that the stochastic gradients suggest are ones of improvement, whereas movement is limited along directions along which contributions of many stochastic gradients cancel each other out."
            \item (weak evidence) Adam views momentum as a moving average for estimating the first moment of gradients \citep{kingma2015adam}
            \item TODO: check the introduction of \citet{cutkosky2020momentum}, which discusses this view further.
        \end{enumerate}
    \end{enumerate}
\end{enumerate}

Our focus: what is the actual role of momentum in deep learning?

\begin{enumerate}
    \item We should try to define a notion of 'variance reduction' on the trajectory/ optimization process and also separate it from the straightforward intuition that update direction becomes less stochastic. 
    
    \item Two possible sources of instability in deep learning that momentum can mitigate: oscillation  (caused by $\eta \lambda_{\max}(H)$ being too large), large gradient noise.
    \item Main Claim: Momentum cannot help to avoid the instability led by large gradient noise.
    \item TODO: define some measures to capture these two sources more formally.
\end{enumerate}

\zhiyuan{Candidate measurement}
\begin{align}[]
    @runzhe
\end{align}

Theory. Given a run of SGD + momentum with certain learning rate schedule, if LR is small enough, we can transform the learning rate schedule so that SGD with this schedule can closely track SGD + Momentum.
\begin{enumerate}
    \item Regime I: $O(1/\eta)$ steps with batch size scaled by Linear scaling rule
    \item Regime II: Slow SDE regime.
\end{enumerate}

Experiments:
\begin{enumerate}
    \item Grid search for hyperparameters (momentum, learning rate, and batch size)  for SGD and compare the training curve of SGD with the same learning rate and batch size but with momentum $0.9$.
    \begin{itemize}
        \item Tune hyperparameters of momentum and SGD. Show that momentum generalizes the same as SGD for small batch, but generalizes better than SGD for large batch.
        % \item show that if certain condition between momentum, learning rate, and batch size hold, then momentum is close to SGD, otherwise not. Effective LR: $B/(\eta(1-\beta))$
        % \item especially the combination of  large batch size, large momentum, and learning rate implies a difference. SGD momentum has better optimization and generalization.
    \end{itemize}
    \item Run SGD with the effective LR induced by the optimal LR of momentum SGD. Show that the training is very unstable (or just blowing up)
    \item Run SVAG for both SGD and momentum (with the optimal effective LR for momentum). Show that their performances become the same.
    \item Take the setting in 2. Enlarge the batch size while keeping the total number of steps. Show that the phenomenon persists.
\end{enumerate}

List of settings:
\begin{enumerate}
    \item PreResNet32 on CIFAR-10
    \item ResNet-50 on ImageNet, batch size 1024, 2048, 4096?
    \item LM fine-tuning? For fine-tuning small LR is more likely to be used, so maybe our theory is more applicable?
\end{enumerate}

Takeaways:
\begin{enumerate}
    \item In the memory-constrained setting, the batch size cannot be too large. So learning rate could be small and our theory could apply. (need to see if we can find some actual evidence to support this view)
    \item It is safe to theoretically view the training dynamics as vanilla SGD training when the LR is not large.
\end{enumerate}

\begin{comment}
Why do people care about the role of momentum?
\begin{enumerate}
    \item Momentum costs extra memory and people want to design more memory-efficient optimization algorithms, .e.g SGD without momentum. For that purpose, we must first understand the underlying mechanism of momentum. 
    \begin{enumerate}
        \item As the model goes larger today, people may want to avoid momentum when the training budget is memory-constrained.
        \item
        \item If our theory does not apply, then people need to invent a new method. Recall that people have already observed that naively removing momentum causes instability. To stabilize training without introducing momentum, our work points out that future work should work more in robust training in ill-conditioned landscape rather than variance reduction.
    \end{enumerate}
    \item Theoretical interest. Theorists prefer simpler dynamical models. Our theory suggests that it is safe to theoretically view the training dynamics as vanilla SGD training when the LR is not large.
\end{enumerate}

What is the value of this paper?
\begin{enumerate}
    \item 
\end{enumerate}
\end{comment}

\subsection{Previous notes}
%PROVABLE ADAPTIVITY IN ADAM (Zhiyuan killed this paper; not quite related)

Momentum may not be effective in accelerating stochastic optimization:
\begin{enumerate}
    \item Introduction to Optimization. Polyak. Quadratic case without proof
    \item \citep{kidambi2018on}
    \item \citep{jain2018accelerating}
\end{enumerate}

Some previous works did not compare momentum with the correct baseline of SGD:
\begin{enumerate}
    \item \citep{leclerc2020two}
\end{enumerate}

Playing around momentum to reduce variance:
\begin{enumerate}
    \item \citep{arnold2019reducing} \tianhao{Online optimization, discussion on why momentum cannot reduce variance in Section 2.1. "Even in the case of stochastic gradient, the noise at a given timestep carries over to subsequent timesteps, even if the old gradients are not used for the update, as the iterate itself depends on the noise."}
    \item \citep{tondji2021variance}
    \item \citep{ma2018quasihyperbolic}
\end{enumerate}

TO READ:
\begin{enumerate}
    \item \citep{gitman2019understanding}
    \item \citep{smith2018disciplined} \tianhao{Mentions that momentum can help stabilize training, but the optimal choice of momentum is closed related to the choice of learning rate}
    \item \citep{xie2021positive} \tianhao{Provided another momentum scheme. Also showed that the posterior of SGD with momentum is the same as that of SGD, by SDE approximation}
\end{enumerate}

DL book also contains some interpretation of momentum: \citep{goodfellow2016dl}

Playing around momentum to reduce variance:
\begin{enumerate}
    \item \citep{arnold2019reducing} \tianhao{Online optimization, discussion on why momentum cannot reduce variance in Section 2.1. "Even in the case of stochastic gradient, the noise at a given timestep carries over to subsequent timesteps, even if the old gradients are not used for the update, as the iterate itself depends on the noise."}
    \item \citep{tondji2021variance}
    \item \citep{ma2018quasihyperbolic}
\end{enumerate}

TO READ:
\begin{enumerate}
    \item \citep{gitman2019understanding}
    \item \citep{smith2018disciplined} \tianhao{Mentions that momentum can help stabilize training, but the optimal choice of momentum is closed related to the choice of learning rate}
    \item \citep{xie2021positive} \tianhao{Provided another momentum scheme. Also showed that the posterior of SGD with momentum is the same as that of SGD, by SDE approximation}
\end{enumerate}