\section{Related Works}

% Momentum via Primal Averaging: Theoretical Insights and Learning Rate Schedules for Non-Convex Optimization

\textbf{The role of momentum in optimization.}
Besides the works mentioned in the introduction, there are many other works discussing the role of momentum.
%, and the actual effect of momentum varies across different settings.
The accelerating effect of some variants of momentum has been observed in settings such as convex optimization~\citep{kidambi2018on} and linear regression~\citep{jain2018accelerating} with parametrizations specific to their settings.
\citet{smith2018disciplined} pointed out that momentum can help stabilize training, but the optimal choice of momentum is closed related to the choice of learning rate.
\citet{arnold2019reducing} argued using a quadratic example that momentum might no reduce variance as the gradient noise in each would actually be carried over to future iterates due to momentum.
\citet{tondji2021variance} showed that the application of a multi-momentum strategy can achieve variance reduction in deep learning.
\citet{plattner2022on} empirically established that momentum enlarges the learning rate while not contributing to a boost in performance. 

\citet{defazio2020momentum} proposed a stochastic primal averaging formulation for SGDM which facilitates a Lyapunov analysis for SGDM, and one particular insight from their analysis is that momentum may help reduce noise in the early stage of training but is no longer helpful when the iterates are close to local minima.
\citet{xie2021positive} showed that under SDE approximation, the posterior of SGDM is the same as that of SGD.
\citet{jelassi2022towards} proved the generalization benefit of momentum in GD in a specific setting of binary classification, by showing that GD+M is able to learn small margin data from the historical gradients in the momentum.
A stronger implicit regularization effect of momentum in GD is also proved in \citet{ghosh2023implicit}.

% In various settings different from ours, it has already been shown that when learning rate is small and momentum parameter is fixed, SGDM approximates SGD \citep{orr1996dynamics, qian1999momentum, yuan2016influence,smith2020generalization}. \tianhao{maybe move somewhere near the weak approximation part?} \kaifeng{maybe just remove it}

 %\tianhao{a bit skeptical about this} \kaifeng{I think they are talking about some other variants of momentum}
%\tianhao{right}


%Some previous works did not compare momentum with the correct baseline of SGD: \citep{leclerc2020two}. \tianhao{Do we want include this?}


\textbf{Convergence of momentum methods.}
There are many works analyzing the convergence of momentum methods, yet it is often the case that momentum appears as a technical barrier rather than leading to faster convergence rates.
\citet{yu2019linear} showed that distributed SGDM can achieve the same linear speedup as ditributed SGD in the setting of distributed non-convex optimization. 
\citet{yan2018unified} provided a unified analysis of SGD, SGDM and stochastic Nestrov's accelerated gradient descent for non-convex optimization, showing that the gradient norm converges in the same rate for the three variants. 
They also argued that momentum helps generalization from the lens of stability analysis for finite number of iterates, when the loss function is Lipschitz. 
Under the formulation of quasi-hyperbolic momentum~\citep{ma2018quasihyperbolic}, 
\citet{gitman2019understanding} proposed another unified analysis for momentum methods, in terms of the asymptotic and local convergence as well as the stability.
\citet{liu2020improved} proved that SGDM converges as fast as SGD for strongly convex and non-convex objectives, and their analysis achieved improved convergence bound without the bounded gradient assumption. Using a iterate-averaging formulation of SGDM, \citet{sebbouh2021almost} proved last-iterate convergence of SGDM in both convex and non-convex settings.
Later in \citet{li2022last}, it is shown that constant momentum can lead to suboptimal last-iterate convergence rate and increasing momentum resolves the issue. \citet{smith2018disciplined,liu2018diffusion} provided evidence that momentum helps escape saddle points but hurts the final convergence without parameter annealing.

%\citep{smith2018disciplined} "a recent paper (Liu et al., 2018) showed that a large momentum helps escape saddle points but can hurt the final convergence, implying that momentum should be reduced at the end of training. We tested cycling momentum versus decreasing momentum throughout training but found a small improvement with cycling over only decreasing the momentum."
%\citep{liu2018diffusion}: Our study shows that the momentum helps escape from saddle points, but hurts the convergence within the neighborhood of optima (if without the step size annealing or momentum annealing).

\textbf{Characterizing implicit bias near manifold of local minimizers}
A recent line of work has studied the implicit bias induced by gradient noise in SGD-type algorithms, when iterates are close to some manifold of local minimizers \citep{blanc2020implicit,damian2021label,li2021happens}.
In particular, \citet{li2021happens} developed a framework for describing the dynamics of SGD via a slow SDE on the manifold of local minimizers in the regime of small learning rate.
The idea is to rescale the time by $1/\eta^2$ times to reveal the relatively slow movement of the iterates along the manifold.
Similar methodology has become a powerful tool for analyzing algorithmic implicit bias and has been extended to many other settings, including SGD/GD for models with normalization layers~\citep{lyu2022understanding,li2022fast}, GD in the edge of stability regime~\citep{arora2022understanding}, Local SGD~\citep{gu2023why},  sharpness-aware minimization~\citep{wen2022does}, and pre-training for language models~\citep{liu2022same}.
Notably, \citet{cowsik2022flatter} utilized the similar idea to study the slow SDE of SGDM study the optimal scale of the momentum parameter with respect to the learning rate, which has a focus different from our paper. %while their approach is to rescale time according to the scale of the gradient noise in the regime of small noise~(or equivalently, large batch size).
%This is fundamentally different from the small learning rate regime that we consider, and we allow constant noise in the gradient.
% \tianhao{Is this a fair comparison?}

\begin{comment}
Analyses of the convergence of momentum (didn't show that momentum is better):
\begin{enumerate}
    \item distributed \citep{yu2019linear}. SOTA momentum bounds?
    \item non-convex \citep{yan2018unified} (it also claims that momentum generalizes better via stability theory? need to check)
    \item \citep{liu2020improved} better bounds in non-convex and strongly convex?
    \item \citep{sebbouh2021almost} convex and non-convex
    \item \citep{li2022last} constant momentum can be suboptimal
\end{enumerate}
\end{comment}
