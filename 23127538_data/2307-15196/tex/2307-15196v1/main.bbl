\begin{thebibliography}{59}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arnold et~al.(2019)Arnold, Manzagol, Babanezhad~Harikandeh,
  Mitliagkas, and Le~Roux]{arnold2019reducing}
S\'{e}bastien Arnold, Pierre-Antoine Manzagol, Reza Babanezhad~Harikandeh,
  Ioannis Mitliagkas, and Nicolas Le~Roux.
\newblock Reducing the variance in online optimization by transporting past
  gradients.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.

\bibitem[Arora et~al.(2022)Arora, Li, and Panigrahi]{arora2022understanding}
Sanjeev Arora, Zhiyuan Li, and Abhishek Panigrahi.
\newblock Understanding gradient descent on the edge of stability in deep
  learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  948--1024. PMLR, 2022.

\bibitem[Blanc et~al.(2020)Blanc, Gupta, Valiant, and
  Valiant]{blanc2020implicit}
Guy Blanc, Neha Gupta, Gregory Valiant, and Paul Valiant.
\newblock Implicit regularization for deep neural networks driven by an
  ornstein-uhlenbeck like process.
\newblock In Jacob Abernethy and Shivani Agarwal, editors, \emph{Proceedings of
  Thirty Third Conference on Learning Theory}, volume 125 of \emph{Proceedings
  of Machine Learning Research}, pages 483--513. PMLR, 09--12 Jul 2020.

\bibitem[Bottou et~al.(2018)Bottou, Curtis, and
  Nocedal]{bottou2018optimization}
L\'{e}on Bottou, Frank~E. Curtis, and Jorge Nocedal.
\newblock Optimization methods for large-scale machine learning.
\newblock \emph{SIAM Review}, 60\penalty0 (2):\penalty0 223--311, 2018.
\newblock \doi{10.1137/16M1080173}.

\bibitem[Bowman et~al.(2015)Bowman, Angeli, Potts, and
  Manning]{bowman2015large}
Samuel~R. Bowman, Gabor Angeli, Christopher Potts, and Christopher~D. Manning.
\newblock A large annotated corpus for learning natural language inference.
\newblock In \emph{Proceedings of the 2015 Conference on Empirical Methods in
  Natural Language Processing}, pages 632--642, Lisbon, Portugal, September
  2015. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/D15-1075}.
\newblock URL \url{https://aclanthology.org/D15-1075}.

\bibitem[Calzolari and Marchetti(1997)]{calzolari1997limit}
Antonella Calzolari and Federico Marchetti.
\newblock Limit motion of an ornstein--uhlenbeck particle on the equilibrium
  manifold of a force field.
\newblock \emph{Journal of Applied Probability}, 34\penalty0 (4):\penalty0
  924--938, 1997.

\bibitem[Cowsik et~al.(2022)Cowsik, Can, and Glorioso]{cowsik2022flatter}
Aditya Cowsik, Tankut Can, and Paolo Glorioso.
\newblock Flatter, faster: scaling momentum for optimal speedup of sgd.
\newblock \emph{arXiv preprint arXiv:2210.16400}, 2022.

\bibitem[Cutkosky and Orabona(2019)]{cutkosky2019momentum}
Ashok Cutkosky and Francesco Orabona.
\newblock Momentum-based variance reduction in non-convex sgd.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.

\bibitem[Damian et~al.(2021)Damian, Ma, and Lee]{damian2021label}
Alex Damian, Tengyu Ma, and Jason~D. Lee.
\newblock Label noise {SGD} provably prefers flat global minimizers.
\newblock In A.~Beygelzimer, Y.~Dauphin, P.~Liang, and J.~Wortman Vaughan,
  editors, \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Defazio(2020)]{defazio2020momentum}
Aaron Defazio.
\newblock Momentum via primal averaging: theoretical insights and learning rate
  schedules for non-convex optimization.
\newblock \emph{arXiv preprint arXiv:2010.00406}, 2020.

\bibitem[Gao et~al.(2021)Gao, Fisch, and Chen]{gao-etal-2021-making}
Tianyu Gao, Adam Fisch, and Danqi Chen.
\newblock Making pre-trained language models better few-shot learners.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association
  for Computational Linguistics and the 11th International Joint Conference on
  Natural Language Processing (Volume 1: Long Papers)}, pages 3816--3830, 2021.
\newblock \doi{10.18653/v1/2021.acl-long.295}.
\newblock URL \url{https://aclanthology.org/2021.acl-long.295}.

\bibitem[Ghosh et~al.(2023)Ghosh, Lyu, Zhang, and Wang]{ghosh2023implicit}
Avrajit Ghosh, He~Lyu, Xitong Zhang, and Rongrong Wang.
\newblock Implicit regularization in heavy-ball momentum accelerated stochastic
  gradient descent.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=ZzdBhtEH9yB}.

\bibitem[Gitman et~al.(2019)Gitman, Lang, Zhang, and
  Xiao]{gitman2019understanding}
Igor Gitman, Hunter Lang, Pengchuan Zhang, and Lin Xiao.
\newblock Understanding the role of momentum in stochastic gradient methods.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.

\bibitem[Goyal et~al.(2017)Goyal, Doll{\'a}r, Girshick, Noordhuis, Wesolowski,
  Kyrola, Tulloch, Jia, and He]{goyal2017accurate}
Priya Goyal, Piotr Doll{\'a}r, Ross Girshick, Pieter Noordhuis, Lukasz
  Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He.
\newblock Accurate, large minibatch {SGD}: Training imagenet in 1 hour.
\newblock \emph{arXiv preprint arXiv:1706.02677}, 2017.

\bibitem[Gu et~al.(2023)Gu, Lyu, Huang, and Arora]{gu2023why}
Xinran Gu, Kaifeng Lyu, Longbo Huang, and Sanjeev Arora.
\newblock Why (and when) does local {SGD} generalize better than {SGD}?
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem[Jain et~al.(2018)Jain, Kakade, Kidambi, Netrapalli, and
  Sidford]{jain2018accelerating}
Prateek Jain, Sham~M. Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron
  Sidford.
\newblock Accelerating stochastic gradient descent for least squares
  regression.
\newblock In Sébastien Bubeck, Vianney Perchet, and Philippe Rigollet,
  editors, \emph{Proceedings of the 31st Conference On Learning Theory},
  volume~75 of \emph{Proceedings of Machine Learning Research}, pages 545--604.
  PMLR, 06--09 Jul 2018.

\bibitem[Jelassi and Li(2022)]{jelassi2022towards}
Samy Jelassi and Yuanzhi Li.
\newblock Towards understanding how momentum improves generalization in deep
  learning.
\newblock In Kamalika Chaudhuri, Stefanie Jegelka, Le~Song, Csaba Szepesvari,
  Gang Niu, and Sivan Sabato, editors, \emph{Proceedings of the 39th
  International Conference on Machine Learning}, volume 162 of
  \emph{Proceedings of Machine Learning Research}, pages 9965--10040. PMLR,
  17--23 Jul 2022.

\bibitem[Katzenberger(1991)]{katzenberger1991solutions}
Gary~Shon Katzenberger.
\newblock Solutions of a stochastic differential equation forced onto a
  manifold by a large drift.
\newblock \emph{The Annals of Probability}, pages 1587--1628, 1991.

\bibitem[Kidambi et~al.(2018)Kidambi, Netrapalli, Jain, and
  Kakade]{kidambi2018on}
Rahul Kidambi, Praneeth Netrapalli, Prateek Jain, and Sham~M. Kakade.
\newblock On the insufficiency of existing momentum schemes for stochastic
  optimization.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Krizhevsky et~al.()Krizhevsky, Nair, and Hinton]{cifar10}
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton.
\newblock {CIFAR}-10 ({C}anadian {I}nstitute for {A}dvanced {R}esearch).
\newblock URL \url{http://www.cs.toronto.edu/~kriz/cifar.html}.

\bibitem[Kurtz and Protter(1991)]{kurtz1991weak}
Thomas~G Kurtz and Philip Protter.
\newblock Weak limit theorems for stochastic integrals and stochastic
  differential equations.
\newblock \emph{The Annals of Probability}, pages 1035--1070, 1991.

\bibitem[Li et~al.(2019)Li, Tai, and E]{li2019stochastic}
Qianxiao Li, Cheng Tai, and Weinan E.
\newblock Stochastic modified equations and dynamics of stochastic gradient
  algorithms i: Mathematical foundations.
\newblock \emph{Journal of Machine Learning Research}, 20\penalty0
  (40):\penalty0 1--47, 2019.

\bibitem[Li et~al.(2022{\natexlab{a}})Li, Liu, and Orabona]{li2022last}
Xiaoyu Li, Mingrui Liu, and Francesco Orabona.
\newblock On the last iterate convergence of momentum methods.
\newblock In Sanjoy Dasgupta and Nika Haghtalab, editors, \emph{Proceedings of
  The 33rd International Conference on Algorithmic Learning Theory}, volume 167
  of \emph{Proceedings of Machine Learning Research}, pages 699--717. PMLR, 29
  Mar--01 Apr 2022{\natexlab{a}}.

\bibitem[Li et~al.(2021{\natexlab{a}})Li, Malladi, and Arora]{li2021validity}
Zhiyuan Li, Sadhika Malladi, and Sanjeev Arora.
\newblock On the validity of modeling {SGD} with stochastic differential
  equations (sdes).
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 12712--12725, 2021{\natexlab{a}}.

\bibitem[Li et~al.(2021{\natexlab{b}})Li, Wang, and Arora]{li2021happens}
Zhiyuan Li, Tianhao Wang, and Sanjeev Arora.
\newblock What happens after {SGD} reaches zero loss?--a mathematical
  framework.
\newblock In \emph{International Conference on Learning Representations},
  2021{\natexlab{b}}.

\bibitem[Li et~al.(2022{\natexlab{b}})Li, Wang, and Yu]{li2022fast}
Zhiyuan Li, Tianhao Wang, and Dingli Yu.
\newblock Fast mixing of stochastic gradient descent with normalization and
  weight decay.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 9233--9248, 2022{\natexlab{b}}.

\bibitem[Liu et~al.(2022)Liu, Xie, Li, and Ma]{liu2022same}
Hong Liu, Sang~Michael Xie, Zhiyuan Li, and Tengyu Ma.
\newblock Same pre-training loss, better downstream: Implicit bias matters for
  language models.
\newblock \emph{arXiv preprint arXiv:2210.14199}, 2022.

\bibitem[Liu et~al.(2018)Liu, Chen, Zhou, and Zhao]{liu2018diffusion}
Tianyi Liu, Zhehui Chen, Enlu Zhou, and Tuo Zhao.
\newblock A diffusion approximation theory of momentum sgd in nonconvex
  optimization.
\newblock \emph{arXiv preprint arXiv:1802.05155}, 2018.

\bibitem[Liu et~al.(2020)Liu, Gao, and Yin]{liu2020improved}
Yanli Liu, Yuan Gao, and Wotao Yin.
\newblock An improved analysis of stochastic gradient descent with momentum.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.F. Balcan, and H.~Lin,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~33,
  pages 18261--18271. Curran Associates, Inc., 2020.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov]{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock \emph{arXiv preprint arXiv:1907.11692}, 2019.
\newblock URL \url{https://arxiv.org/pdf/1907.11692.pdf}.

\bibitem[Lyu et~al.(2022)Lyu, Li, and Arora]{lyu2022understanding}
Kaifeng Lyu, Zhiyuan Li, and Sanjeev Arora.
\newblock Understanding the generalization benefit of normalization layers:
  Sharpness reduction.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho,
  editors, \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=xp5VOBxTxZ}.

\bibitem[Ma and Yarats(2019)]{ma2018quasihyperbolic}
Jerry Ma and Denis Yarats.
\newblock Quasi-hyperbolic momentum and adam for deep learning.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Malladi et~al.(2022)Malladi, Lyu, Panigrahi, and
  Arora]{malladi2022sdes}
Sadhika Malladi, Kaifeng Lyu, Abhishek Panigrahi, and Sanjeev Arora.
\newblock On the {SDE}s and scaling rules for adaptive gradient algorithms.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho,
  editors, \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Malladi et~al.(2023)Malladi, Wettig, Yu, Chen, and
  Arora]{malladi2023kernelbased}
Sadhika Malladi, Alexander Wettig, Dingli Yu, Danqi Chen, and Sanjeev Arora.
\newblock A kernel-based view of language model fine-tuning, 2023.

\bibitem[Orr(1996)]{orr1996dynamics}
Genevieve~Beth Orr.
\newblock \emph{Dynamics and Algorithms for Stochastic Search}.
\newblock PhD thesis, USA, 1996.
\newblock UMI Order No. GAX96-08998.

\bibitem[Plattner(2022)]{plattner2022on}
Maximilian Plattner.
\newblock On sgd with momentum.
\newblock page~60, 2022.
\newblock URL \url{http://infoscience.epfl.ch/record/295398}.

\bibitem[Polyak(1987)]{polyak1987intro}
Boris~T. Polyak.
\newblock \emph{Introduction to Optimization}.
\newblock Optimization Software, Inc., 1987.

\bibitem[Polyak(1964)]{polyak1964some}
B.T. Polyak.
\newblock Some methods of speeding up the convergence of iteration methods.
\newblock \emph{USSR Computational Mathematics and Mathematical Physics},
  4\penalty0 (5):\penalty0 1--17, 1964.
\newblock ISSN 0041-5553.

\bibitem[Qian(1999)]{qian1999momentum}
Ning Qian.
\newblock On the momentum term in gradient descent learning algorithms.
\newblock \emph{Neural networks}, 12\penalty0 (1):\penalty0 145--151, 1999.

\bibitem[Rumelhart et~al.(1987)Rumelhart, Hinton, and
  Williams]{rumelhart1987learning}
David~E. Rumelhart, Geoffrey~E. Hinton, and Ronald~J. Williams.
\newblock \emph{Learning Internal Representations by Error Propagation}, pages
  318--362.
\newblock 1987.

\bibitem[Sebbouh et~al.(2021)Sebbouh, Gower, and Defazio]{sebbouh2021almost}
Othmane Sebbouh, Robert~M Gower, and Aaron Defazio.
\newblock Almost sure convergence rates for stochastic gradient descent and
  stochastic heavy ball.
\newblock In Mikhail Belkin and Samory Kpotufe, editors, \emph{Proceedings of
  Thirty Fourth Conference on Learning Theory}, volume 134 of \emph{Proceedings
  of Machine Learning Research}, pages 3935--3971. PMLR, 15--19 Aug 2021.

\bibitem[Shallue et~al.(2019)Shallue, Lee, Antognini, Sohl-Dickstein, Frostig,
  and Dahl]{shallue2019measuring}
Christopher~J. Shallue, Jaehoon Lee, Joseph Antognini, Jascha Sohl-Dickstein,
  Roy Frostig, and George~E. Dahl.
\newblock Measuring the effects of data parallelism on neural network training.
\newblock \emph{Journal of Machine Learning Research}, 20\penalty0
  (112):\penalty0 1--49, 2019.

\bibitem[Shazeer and Stern(2018)]{shazeer2018adafactor}
Noam Shazeer and Mitchell Stern.
\newblock Adafactor: Adaptive learning rates with sublinear memory cost.
\newblock In Jennifer Dy and Andreas Krause, editors, \emph{Proceedings of the
  35th International Conference on Machine Learning}, volume~80 of
  \emph{Proceedings of Machine Learning Research}, pages 4596--4604. PMLR,
  10--15 Jul 2018.

\bibitem[Smith(2018)]{smith2018disciplined}
Leslie~N Smith.
\newblock A disciplined approach to neural network hyper-parameters: Part
  1--learning rate, batch size, momentum, and weight decay.
\newblock \emph{arXiv preprint arXiv:1803.09820}, 2018.

\bibitem[Smith et~al.(2020)Smith, Elsen, and De]{smith2020generalization}
Samuel Smith, Erich Elsen, and Soham De.
\newblock On the generalization benefit of noise in stochastic gradient
  descent.
\newblock In Hal~Daumé III and Aarti Singh, editors, \emph{Proceedings of the
  37th International Conference on Machine Learning}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pages 9058--9067. PMLR,
  13--18 Jul 2020.

\bibitem[Socher et~al.(2013)Socher, Perelygin, Wu, Chuang, Manning, Ng, and
  Potts]{socher2013recursive_sst-2}
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher~D. Manning,
  Andrew Ng, and Christopher Potts.
\newblock Recursive deep models for semantic compositionality over a sentiment
  treebank.
\newblock 2013.
\newblock URL \url{https://aclanthology.org/D13-1170.pdf}.

\bibitem[Sutskever et~al.(2013)Sutskever, Martens, Dahl, and
  Hinton]{sutskever2013on}
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton.
\newblock On the importance of initialization and momentum in deep learning.
\newblock In Sanjoy Dasgupta and David McAllester, editors, \emph{Proceedings
  of the 30th International Conference on Machine Learning}, volume~28 of
  \emph{Proceedings of Machine Learning Research}, pages 1139--1147, Atlanta,
  Georgia, USA, 17--19 Jun 2013. PMLR.

\bibitem[Tondji et~al.(2021)Tondji, Kashubin, and Cisse]{tondji2021variance}
Lionel Tondji, Sergii Kashubin, and Moustapha Cisse.
\newblock Variance reduction in deep learning: More momentum is all you need.
\newblock \emph{arXiv preprint arXiv:2111.11828}, 2021.

\bibitem[Tugay and Tanik(1989)]{tugay1989properties}
Mehmet~Ali Tugay and Yalçin Tanik.
\newblock Properties of the momentum lms algorithm.
\newblock \emph{Signal Processing}, 18\penalty0 (2):\penalty0 117--127, 1989.
\newblock ISSN 0165-1684.

\bibitem[Voorhees and Tice(2000)]{voorhees2000building_trec}
Ellen~M Voorhees and Dawn~M Tice.
\newblock Building a question answering test collection.
\newblock In \emph{the 23rd annual international ACM SIGIR conference on
  Research and development in information retrieval}, 2000.

\bibitem[Wen et~al.(2022)Wen, Ma, and Li]{wen2022does}
Kaiyue Wen, Tengyu Ma, and Zhiyuan Li.
\newblock How does sharpness-aware minimization minimize sharpness?
\newblock \emph{arXiv preprint arXiv:2211.05729}, 2022.

\bibitem[Whitt(2002)]{whitt2002stochastic}
Ward Whitt.
\newblock Stochastic-process limits: an introduction to stochastic-process
  limits and their application to queues.
\newblock \emph{Space}, 500:\penalty0 391--426, 2002.

\bibitem[Williams et~al.(2018)Williams, Nangia, and
  Bowman]{williams2018broad_mnli}
Adina Williams, Nikita Nangia, and Samuel Bowman.
\newblock A broad-coverage challenge corpus for sentence understanding through
  inference.
\newblock 2018.
\newblock URL \url{https://aclanthology.org/N18-1101.pdf}.

\bibitem[Xie et~al.(2021)Xie, Yuan, Zhu, and Sugiyama]{xie2021positive}
Zeke Xie, Li~Yuan, Zhanxing Zhu, and Masashi Sugiyama.
\newblock Positive-negative momentum: Manipulating stochastic gradient noise to
  improve generalization.
\newblock In Marina Meila and Tong Zhang, editors, \emph{Proceedings of the
  38th International Conference on Machine Learning}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pages 11448--11458. PMLR,
  18--24 Jul 2021.

\bibitem[Yan et~al.(2018)Yan, Yang, Li, Lin, and Yang]{yan2018unified}
Yan Yan, Tianbao Yang, Zhe Li, Qihang Lin, and Yi~Yang.
\newblock A unified analysis of stochastic momentum methods for deep learning.
\newblock In \emph{Proceedings of the Twenty-Seventh International Joint
  Conference on Artificial Intelligence, {IJCAI-18}}, pages 2955--2961.
  International Joint Conferences on Artificial Intelligence Organization, 7
  2018.

\bibitem[You et~al.(2020)You, Li, Reddi, Hseu, Kumar, Bhojanapalli, Song,
  Demmel, Keutzer, and Hsieh]{you2020large}
Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh
  Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh.
\newblock Large batch optimization for deep learning: Training {BERT} in 76
  minutes.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Yu et~al.(2019)Yu, Jin, and Yang]{yu2019linear}
Hao Yu, Rong Jin, and Sen Yang.
\newblock On the linear speedup analysis of communication efficient momentum
  {SGD} for distributed non-convex optimization.
\newblock In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors,
  \emph{Proceedings of the 36th International Conference on Machine Learning},
  volume~97 of \emph{Proceedings of Machine Learning Research}, pages
  7184--7193. PMLR, 09--15 Jun 2019.

\bibitem[Yuan et~al.(2016)Yuan, Ying, and Sayed]{yuan2016influence}
Kun Yuan, Bicheng Ying, and Ali~H Sayed.
\newblock On the influence of momentum acceleration on online learning.
\newblock \emph{The Journal of Machine Learning Research}, 17\penalty0
  (1):\penalty0 6602--6667, 2016.

\end{thebibliography}
