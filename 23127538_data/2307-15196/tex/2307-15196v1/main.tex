\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%\PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% ready for submission
\usepackage[preprint]{neurips_2023}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

\usepackage{times}
\usepackage{amsmath, amssymb, amsthm, bm}
\usepackage{multirow}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{makecell}
\usepackage{array}
\usepackage{url}
\usepackage{dsfont}
\usepackage{cleveref}
\usepackage{csquotes}
\usepackage{cancel}
\usepackage{booktabs}
\usepackage{array}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{enumitem}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{cleveref}
\usepackage{thmtools}
\usepackage{comment}
\usepackage{thm-restate}
\usepackage{bbm}
\input{math_commands}
\input{symbols}
\usepackage{textcomp}

\usepackage{xcolor}
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}


\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{example}[theorem]{Example}
\crefname{claim}{claim}{claims}
\crefname{conjecture}{conjecture}{conjectures}
\crefname{assumption}{assumption}{assumptions}
\crefname{condition}{condition}{conditions}

\newcommand{\xinit}{x_{\mathrm{init}}}

\newcommand{\zhiyuan}[1]{{\color{red}[ZL: #1]}}
\newcommand{\kaifeng}[1]{{\color{orange}[KL: #1]}}
\newcommand{\tianhao}[1]{{\color{red}[TW: #1]}}
\newcommand{\runzhe}[1]{{\color{red}[RW: #1]}}
\newcommand{\snote}[1]{{\color{red} #1}}
%\title{Momentum Does Not Truly Reduce Noise Variance in SGD}
%\title{Momentum Does Not Stabilize SGD}
\title{The Marginal Value of Momentum \\ for Small Learning Rate SGD}
%\title{Momentum does not Benefit SGD for Small Learning Rate}
\author{Runzhe Wang, Sadhika Malladi, Tianhao Wang, Kaifeng Lyu, Zhiyuan Li}
\date{February 2022}

\begin{document}

\maketitle

\input{sections-NIPS/abstract.tex}
\input{sections-NIPS/intro.tex}
\input{sections-NIPS/related_work.tex}
\input{sections-NIPS/preliminaries.tex}
\input{sections-NIPS/results.tex}
\input{sections-NIPS/experiments}
\section{Conclusions}
    This work provides theoretical characterizations of the role of momentum in stochastic gradient methods. We formally show that momentum does not introduce optimization and generalization benefits when the learning rates are small, and we further exhibit empirically that the value of momentum is marginal for gradient-noise-dominated learning settings with practical learning rate scales. Hence we conclude that momentum does not provide a significant performance boost in the above cases. Our results further suggest that model performance is agnostic to the choice of momentum parameters over a range of hyperparameter scales.
\bibliography{reference}
\bibliographystyle{plainnat}

\appendix
\input{sections-NIPS/appendix}
\input{sections-NIPS/appendix_exps}
\end{document}
