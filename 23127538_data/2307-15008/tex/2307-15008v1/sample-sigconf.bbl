\begin{thebibliography}{10}

\bibitem{athalye2018obfuscated}
Anish Athalye, Nicholas Carlini, and David~A. Wagner.
\newblock Obfuscated gradients give a false sense of security: Circumventing
  defenses to adversarial examples.
\newblock In {\em Proceedings of the 35th International Conference on Machine
  Learning, {ICML}}, volume~80 of {\em Proceedings of Machine Learning
  Research}, pages 274--283. {PMLR}, 2018.

\bibitem{biggio2013evasion}
Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim
  {\v{S}}rndi{\'c}, Pavel Laskov, Giorgio Giacinto, and Fabio Roli.
\newblock Evasion attacks against machine learning at test time.
\newblock In {\em Joint European conference on machine learning and knowledge
  discovery in databases}, pages 387--402. Springer, 2013.

\bibitem{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock {\em Advances in neural information processing systems}, 2020.

\bibitem{bryniarski2021evading}
Oliver Bryniarski, Nabeel Hingun, Pedro Pachuca, Vincent Wang, and Nicholas
  Carlini.
\newblock Evading adversarial example detection defenses with orthogonal
  projected gradient descent.
\newblock {\em arXiv preprint arXiv:2106.15023}, 2021.

\bibitem{bubeck2023sparks}
S{\'e}bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric
  Horvitz, Ece Kamar, Peter Lee, Yin~Tat Lee, Yuanzhi Li, Scott Lundberg,
  et~al.
\newblock Sparks of artificial general intelligence: Early experiments with
  gpt-4.
\newblock {\em arXiv preprint arXiv:2303.12712}, 2023.

\bibitem{buckman2018thermometer}
Jacob Buckman, Aurko Roy, Colin Raffel, and Ian~J. Goodfellow.
\newblock Thermometer encoding: One hot way to resist adversarial examples.
\newblock In {\em 6th International Conference on Learning Representations,
  {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track
  Proceedings}. OpenReview.net, 2018.

\bibitem{carlini2020partial}
Nicholas Carlini.
\newblock A partial break of the honeypots defense to catch adversarial
  attacks.
\newblock {\em arXiv preprint arXiv:2009.10975}, 2020.

\bibitem{carlini2019evaluating}
Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland Brendel, Jonas
  Rauber, Dimitris Tsipras, Ian Goodfellow, Aleksander Madry, and Alexey
  Kurakin.
\newblock On evaluating adversarial robustness.
\newblock {\em ArXiv preprint}, abs/1902.06705, 2019.

\bibitem{carlini2017adversarial}
Nicholas Carlini and David Wagner.
\newblock Adversarial examples are not easily detected: Bypassing ten detection
  methods.
\newblock In {\em Proceedings of the 10th ACM workshop on artificial
  intelligence and security}, pages 3--14, 2017.

\bibitem{carlini2017towards}
Nicholas Carlini and David Wagner.
\newblock Towards evaluating the robustness of neural networks.
\newblock In {\em 2017 ieee symposium on security and privacy (sp)}, pages
  39--57. IEEE, 2017.

\bibitem{chen2018detecting}
Bryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig, Benjamin
  Edwards, Taesung Lee, Ian Molloy, and Biplav Srivastava.
\newblock Detecting backdoor attacks on deep neural networks by activation
  clustering.
\newblock {\em arXiv preprint arXiv:1811.03728}, 2018.

\bibitem{chowdhery2022palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
  Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian
  Gehrmann, et~al.
\newblock Palm: Scaling language modeling with pathways.
\newblock {\em arXiv preprint arXiv:2204.02311}, 2022.

\bibitem{cohen2019certified}
Jeremy~M. Cohen, Elan Rosenfeld, and J.~Zico Kolter.
\newblock Certified adversarial robustness via randomized smoothing.
\newblock In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, {\em
  Proceedings of the 36th International Conference on Machine Learning, {ICML}
  2019, 9-15 June 2019, Long Beach, California, {USA}}, volume~97 of {\em
  Proceedings of Machine Learning Research}, pages 1310--1320. {PMLR}, 2019.

\bibitem{gao2019strip}
Yansong Gao, Change Xu, Derui Wang, Shiping Chen, Damith~C Ranasinghe, and
  Surya Nepal.
\newblock Strip: A defence against trojan attacks on deep neural networks.
\newblock In {\em Proceedings of the 35th Annual Computer Security Applications
  Conference}, pages 113--125, 2019.

\bibitem{lecuyer2019certified}
Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman
  Jana.
\newblock Certified robustness to adversarial examples with differential
  privacy.
\newblock In {\em 2019 IEEE Symposium on Security and Privacy (SP)}, pages
  656--672. IEEE, 2019.

\bibitem{li2022competition}
Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser,
  R{\'e}mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal~Lago,
  et~al.
\newblock Competition-level code generation with alphacode.
\newblock {\em Science}, 378(6624):1092--1097, 2022.

\bibitem{madry2018towards}
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
  Adrian Vladu.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock In {\em 6th International Conference on Learning Representations,
  {ICLR}, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track
  Proceedings}, 2018.

\bibitem{meng2017magnet}
Dongyu Meng and Hao Chen.
\newblock Magnet: a two-pronged defense against adversarial examples.
\newblock In {\em Proceedings of the 2017 ACM SIGSAC conference on computer and
  communications security}, pages 135--147, 2017.

\bibitem{openai2023gpt4}
OpenAI.
\newblock Gpt-4 technical report, 2023.

\bibitem{papernot2016distillation}
Nicolas Papernot, Patrick McDaniel, Xi~Wu, Somesh Jha, and Ananthram Swami.
\newblock Distillation as a defense to adversarial perturbations against deep
  neural networks.
\newblock In {\em 2016 IEEE symposium on security and privacy (SP)}, pages
  582--597. IEEE, 2016.

\bibitem{roth2019odds}
Kevin Roth, Yannic Kilcher, and Thomas Hofmann.
\newblock The odds are odd: {A} statistical test for detecting adversarial
  examples.
\newblock In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, {\em
  Proceedings of the 36th International Conference on Machine Learning, {ICML}
  2019, 9-15 June 2019, Long Beach, California, {USA}}, volume~97 of {\em
  Proceedings of Machine Learning Research}, pages 5498--5507. {PMLR}, 2019.

\bibitem{shan2020gotta}
Shawn Shan, Emily Wenger, Bolun Wang, Bo~Li, Haitao Zheng, and Ben~Y Zhao.
\newblock Gotta catch'em all: Using honeypots to catch adversarial attacks on
  neural networks.
\newblock In {\em Proceedings of the 2020 ACM SIGSAC Conference on Computer and
  Communications Security}, pages 67--83, 2020.

\bibitem{szegedy2013intriguing}
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
  Ian~J. Goodfellow, and Rob Fergus.
\newblock Intriguing properties of neural networks.
\newblock In Yoshua Bengio and Yann LeCun, editors, {\em 2nd International
  Conference on Learning Representations, {ICLR} 2014, Banff, AB, Canada, April
  14-16, 2014, Conference Track Proceedings}, 2014.

\bibitem{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
  Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro,
  Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume
  Lample.
\newblock Llama: Open and efficient foundation language models, 2023.

\bibitem{tramer2020adaptive}
Florian Tramer, Nicholas Carlini, Wieland Brendel, and Aleksander Madry.
\newblock On adaptive attacks to adversarial example defenses.
\newblock {\em Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem{tramer2017space}
Florian Tram{\`e}r, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick
  McDaniel.
\newblock The space of transferable adversarial examples.
\newblock {\em arXiv preprint arXiv:1704.03453}, 2017.

\bibitem{verma2019error}
Gunjan Verma and Ananthram Swami.
\newblock Error correcting output codes improve probability estimation and
  adversarial robustness of deep neural networks.
\newblock In Hanna~M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence
  d'Alch{\'{e}}{-}Buc, Emily~B. Fox, and Roman Garnett, editors, {\em Advances
  in Neural Information Processing Systems 32: Annual Conference on Neural
  Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019,
  Vancouver, BC, Canada}, pages 8643--8653, 2019.

\bibitem{wang2019neural}
Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao
  Zheng, and Ben~Y Zhao.
\newblock Neural cleanse: Identifying and mitigating backdoor attacks in neural
  networks.
\newblock In {\em 2019 IEEE Symposium on Security and Privacy (SP)}, pages
  707--723. IEEE, 2019.

\bibitem{zhu2023ai}
Hong Zhu, Shengzhi Zhang, and Kai Chen.
\newblock Ai-guardian: Defeating adversarial attacks using backdoors.
\newblock In {\em 2023 IEEE Symposium on Security and Privacy (SP)}, pages
  701--718. IEEE Computer Society, 2023.

\end{thebibliography}
