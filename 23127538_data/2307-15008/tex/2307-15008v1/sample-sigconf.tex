\documentclass[twocolumn,9pt]{extarticle}
\pdfoutput=1
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{pdfpages}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}




%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


\usepackage{enumitem,amssymb}
\newlist{todolist}{itemize}{2}
\setlist[todolist]{label=$\square$}


\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\human}[1]{\textcolor{black}{#1}}


\usepackage{xcolor}

\definecolor{darkblue}{rgb}{0.0, 0.0, 0.55}
\definecolor{darkred}{rgb}{0.75, 0.0, 0.0}

\usepackage{tcolorbox}
\tcbuselibrary{breakable, skins}


\newenvironment{ai}
{\par\begin{tcolorbox}[enhanced jigsaw, breakable, colback=lightgray!20, colframe=darkblue, coltext=darkblue, sharp corners, rounded corners=all]}
{\end{tcolorbox}\par}

\newenvironment{prompt}
{\par\begin{tcolorbox}[enhanced jigsaw, breakable, colback=lightgray!20, colframe=darkred, coltext=darkred, sharp corners, rounded corners=all]}
{\end{tcolorbox}\par}

\title{{\Huge{\color{darkblue} A LLM Assisted Exploitation} of AI-Guardian}\vspace{.5em}}
%\title{\vspace{3em}{\color{darkblue} A GPT-4 assisted exploitation} of AI-Guardian\vspace{.5em}}


\makeatletter
\def\@copyrightspace{\relax}
\makeatother

\usepackage{fancyhdr}

\fancyhead{}
\fancyfoot{}

\pagestyle{empty}


\usepackage[english]{babel}
\usepackage{blindtext}

\usepackage{etoolbox}
\makeatletter
\patchcmd{\maketitle}{\@copyrightspace}{}{}{}
\makeatother


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.


%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Nicholas Carlini \\ \emph{Google DeepMind}}
\date{}

\maketitle
\pagestyle{plain}
\date{}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
Large language models (LLMs) are now highly capable at
a diverse range of tasks.
This paper studies whether or not GPT-4, one such LLM, is capable of assisting researchers in the field of adversarial machine learning.
%
As a case study, we evaluate the robustness of
AI-Guardian, a recent defense to adversarial examples published at IEEE S\&P 2023, a top computer security conference.
%
We completely break this defense: the proposed scheme does not increase robustness compared to an undefended baseline.

We write none of the code to attack this model, and instead prompt
GPT-4 to implement all attack algorithms
following our instructions and guidance.
%
This process was surprisingly effective and efficient,
with the language model at times producing code from ambiguous
instructions faster than the author of this paper could have done.
%
We conclude by discussing
(1) the warning signs present in the evaluation that suggested to us AI-Guardian
would be broken, and
(2) our experience with designing attacks and performing
novel research using the most recent advances in language modeling.
\end{abstract}


%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.


\section{Introduction}

Defending against adversarial examples is hard
\footnote{Personal communication with the first author of \cite{carlini2017adversarial}.}.
%
Historically, the vast majority of adversarial example defenses published at
top-tier conferences (e.g., USENIX \cite{shan2020gotta}, S\&P \cite{papernot2016distillation}, or CCS \cite{meng2017magnet} in the security space,
or ICLR \cite{buckman2018thermometer}, ICML \cite{roth2019odds}, or NeurIPS \cite{verma2019error} in the machine learning space)
are quickly broken \cite{athalye2018obfuscated,tramer2020adaptive,carlini2017adversarial}.

Fortunately, this rapid back-and-forth between attackers and defenders
has allowed researchers to construct sufficiently advanced attack algorithms
and approaches that
evaluating the robustness of a defense
to adversarial examples is mostly a mechanistic procedure.
%
As an example, it typically requires just a few hours
of work to break published defenses \cite{carlini2020partial},
and does not require developing new technical ideas  \cite{tramer2020adaptive}.

Independently and concurrently,
at the same time that attacking defenses to adversarial examples has improved,
large language models like GPT-4 \cite{openai2023gpt4}
have become sufficiently capable that they can reliably solve coding challenges
near the level of human programmers \cite{li2022competition}.
%
This raises the question how well these models can perform at
real-world and useful coding tasks.
%
While these models have been dubbed
``early form[s] of AGI'' \cite{bubeck2023sparks},
for our purposes
we only require they have the ability to act as
``word calculators'',
and automate the repetitive programming aspects
involved in conducting machine learning research.


\paragraph{Contributions.}
In this paper we evaluate the ability of GPT-4 to act
as a research assistant and break a published adversarial examples defense.
%
We focus our efforts on breaking AI-Guardian, a recent defense published at
IEEE S\&P, a top tier computer security venue. 
%
Because this defense is completely novel---it was accepted at S\&P after all---this
acts as an interesting case study
for understanding the value of ``AI research assistants'' that
perform experiments at the direction of a human researcher.

Surprisingly (to us, but perhaps not to others),
we find that GPT-4 can successfully implement our break
following our instruction.
%
Even when given imprecise instructions,
GPT-4 often performs the intended behavior,
and when it does not,
a quick back-and-forth suffices to correct the model's actions---but
when it would be faster we ``manually'' corrected bugs in the output.

Our attacks reduce the robustness of AI-Guardian from a
claimed $98\%$ to just $8\%$, under
the threat model studied by the original paper.
%
The authors of AI-Guardian acknowledge our break succeeds at
fooling their defense.

%
We conclude with a discussion of 
(1) the warning signs present in the
original AI-guardian paper that indicated it would be
vulnerable to attack (e.g., the fact that it claimed robustness
to nearly unbounded adversarial examples)
and
(2) our experience in using GPT-4 as a tool to perform research.
%
In the appendix we provide a complete transcript of our interaction
with GPT-4.


\paragraph{Division of Labor.}
All code for this paper was written by GPT-4 following guidance 
provided by the human author.
%
All human-written text in this paper appears in black.
%
When we prompt the model to write text for us, 
{\color{darkred} we provide the prompt in red} and then color 
{\color{darkblue}text primarily written by GPT-4 in dark blue.}
%
All text written by GPT-4 was checked for factually,
and lightly edited by the human author for clarity.
%
Text that required rewriting is written in black.
%
The initially provided prompt to write this paper stated:

\begin{prompt}
I am writing a paper about using GPT-4 to evaluate the adversarial robustness of a recent defense to adversarial examples. I don't yet have a title but the working title is something like ``Using GPT-4 to generate adversarial examples on AI-Guardian'' or ``AI-Guardian is not robust to adversarial examples, using GPT-4 to write the attack code'' or ``GPT-4 as a junior author to implement an attack on AI-Guardian''.

I'd like help writing the paper. In particular, I'd like help with some of the simpler pieces of the paper that I would usually be able to pass off to a more junior author.
\end{prompt}



\section{Background}
\human{We begin with background familiar to researchers
who have studied adversarial machine learning and language models.}

\begin{prompt}

To start, can you write me a background section in latex following this bulleted list. Assume that you can just \textbackslash{}cite\{\} as normal and I'll add the references.

\begin{itemize}
\item Adversarial Examples: 
\begin{itemize}
\item Explain what they are and cite Szegedy and Biggio. [3 sentences]
\item Then talk about generating adversarial examples with gradient-based attacks and cite Carlini \& Wagner and Madry et al.  [2 sentences]
\item Talk about transfer-based attacks (citing Papernot / Tramer) and EoT (Citing Athalye). [2 sentences]
\end{itemize}
\item Defending against adversarial examples
\begin{itemize}
\item There are a bunch of defenses to adversarial examples (cite Carlini "on evaluating") [2 sentences]
\item Most defenses to adversarial examples are quickly broken (cite Carlini \& Wagner "not easily detected", Athalye "obfuscated", Tramer "adaptive") [1 sentence]
\item While there are certified defenses (cite Lecuyer PixelDP, Cohen "randomized smoothing") or adversarially trained models (cite Madry), these techniques lose significant clean accuracy. [1 sentence]
\item Therefore there is need for other models.
\end{itemize}
\item AI-Guardian: We will fill this section in later. Leave it blank for now.
\item Large Language Models
\begin{itemize}
\item Completely independent from the above LLMs are now good (cite GPT3/GPT4 technical reports). [3 sentences]
\item They even have the ability to write code [cite something if you can find anything here] [2 sentences]
\item We study to what extent recent LLMs have the ability to act as a research assistant.
\end{itemize}
\end{itemize}
\end{prompt}
\begin{ai}
\subsection{Adversarial Examples}
Adversarial examples are specially crafted inputs designed to deceive machine learning models, initially described in the seminal works of \cite{szegedy2013intriguing} and \cite{biggio2013evasion}.\ These manipulations are typically indistinguishable to human perception, yet lead to significant misclassification errors in machine learning systems.\ This vulnerability exposes severe security implications for the deployment of machine learning in safety and security-critical applications.

Generating adversarial examples often relies on gradient-based attacks, as demonstrated by prior work \cite{carlini2017towards,madry2018towards}.\ These attacks exploit the gradient information of the target model to iteratively refine adversarial perturbations.

\begin{prompt}
I forgot, in the background section can you write a new paragraph called PGD (projected gradient descent) and explain how base PGD works. Also give the equation that's minimized for targeted attacks.
\end{prompt}

\paragraph{Projected Gradient Descent (PGD)}
Projected Gradient Descent \cite{madry2018towards} (PGD) is a common technique used in crafting adversarial examples. Given an initial example $x$, a target label $y'$, and a trained model $f(\cdot)$, we want to solve the following optimization problem to create a targeted adversarial example:

\begin{equation}
\begin{aligned}
\argmin_{x'} & \quad \mathcal{L}(F_\theta(x'), y') \\
\text{s.t.} & \quad |x' - x|_\infty \leq \epsilon
\end{aligned}
\end{equation}


Here, $\mathcal{L}(\cdot, \cdot)$ is the loss function. This optimization problem seeks to find an adversarial example $x'$ within an $\epsilon$-bounded $\ell_\infty$ neighborhood of the original image $x$ that minimizes the loss between the model's prediction $f(x')$ and the target label $y'$.

\subsection{Defending against adversarial examples}
There exists a rich line of work proposing defenses against adversarial examples \cite{papernot2016distillation,meng2017magnet,madry2018towards,buckman2018thermometer,roth2019odds}.\ However, the effectiveness of many of these proposed defenses is limited.

Indeed, numerous defenses have been shown to be quickly broken or bypassed, highlighting the difficulty of developing robust and reliable mitigation strategies \cite{carlini2017adversarial,athalye2018obfuscated,tramer2017space}.\ Certified defenses, such as those proposed by Lecuyer et al. \cite{lecuyer2019certified} and Cohen et al. \cite{cohen2019certified} provide theoretical guarantees against adversarial perturbations, yet often at the cost of clean accuracy.\ Similarly, adversarial training, another popular defense strategy, also suffers from degradation in clean accuracy \cite{madry2018towards}.\ Hence, there is a clear need for further research and development of more effective defenses.

\end{ai}

\paragraph{AI-Guardian} \cite{zhu2023ai} is a defense proposed at IEEE S\&P 2023 that claims 
exceptionally strong adversarial robustness without sacrificing clean accuracy.
%
We provide a detailed description of the defense in Section~\ref{sec:aiguardian}.
%
To briefly summarize, this defense consists of a public model $f$, 
and a secret transformation $t$.
%
It is trained so that for any clean input $x$, 
the model satisfies $f(x) = t^{-1}f(t(x)))$,
but so that adversarial examples $x'$ crafted without knowledge 
of $t$ will have the property 
that $f(x') \ne f(x) = t^{-1}f(t(x')))$.

Thus, as long as the transformation $t$ is unknown to the adversary
and is kept secret, the defense remains secure:
the adversary may be able to fool the base model $f$ alone,
but will not be able to fool the defended model.
%
The authors argue that
``the trigger is only stored and applied at the server
side, which means attackers need to hack the server to obtain
it. Therefore, it is not easy to launch such an attack in practice.''
%
We will show this is not true,
and the secret information can easily be recovered.



\begin{ai}

\subsection{Large Language Models}
Large language models (LLMs) have seen remarkable advancements, achieving high performance across various natural language processing tasks.\ Recent models, such as GPT-3 \cite{brown2020language}, PaLM \cite{chowdhery2022palm}, LLaMA \cite{touvron2023llama}, and GPT-4 \cite{openai2023gpt4}, have achieved unprecedented capabilities in generating human-like text, opening up a wide range of applications.

These LLMs have not only improved in generating natural language text but have also demonstrated capabilities in generating functional code \cite{li2022competition}.\ This development can have implications for various applications, including programming assistance and automated code generation.\ In this work, we explore the capacity of recent LLMs to act as a research assistant, in particular their ability to contribute to adversarial robustness research.
\footnote{In this paper we focus our study on the GPT-4 model because it currently appears to have the best capability to implement our attacks.}

\end{ai}


\section{Threat Model}

\begin{prompt}
Great now let's write a threat model section. Include the following.

\begin{itemize}
\item Explain what distortion bounds are and why they're needed.
\item We attack with an Linfinity bounded attack. The paper claims robustness to many but attack suffice to break one.
\item On MNIST we're using 0.25 which is standard [cite goodfellow]
item Then talk about this paper in particular which assumes the attack has gradient access to the model f() but can't access either the permutation P or the backdoor trigger function t()
\item But we do get oracle query-access to the logits of P[f(t(x))].
\item Explain why this is reasonable: the defender keeps t() secret but publishes f.
\item People can still use f(x) normally but it won't be protected. Only with knowledge of t() is it safe.
\end{itemize}
\end{prompt}
\begin{ai}

We study $L_\infty$ constrained adversarial attacks.\ Even though AI-Guardian claims robustness to a range of adversarial perturbations, an attack need only break one of the claims to show the scheme is insecure.
Specifically, for our experiments on the MNIST dataset, we set the distortion bound to 0.25, which is a standard choice in this context \cite{madry2018towards} and was the distortion bound considered by the original paper \cite{zhu2023ai}.

AI-Guardian assumes that the attacker has access to the gradient information for the base model $f$, but does not have knowledge of the secret transformation $t$.
However, the attacker \emph{is} granted oracle query access to the output logits of the defended model which includes the transformation $t$.

This threat model is deemed reasonable for several reasons.\ The defender may choose to keep the function $t$ secret while publishing the model $f$ to protect the model from being exploited.\ In such a case, the users can still employ $f(x)$ for their normal uses but without the protection mechanism.\ The safety guarantee only applies when the input is preprocessed by the secret trigger function $t$, that is, the model is designed to be robust only when used as $f(t(x))$.
\end{ai}


\section{The AI-Guardian Defense}
\label{sec:aiguardian}

\begin{prompt}
Now let's describe how the defense works. It does the following.
\begin{enumerate}
\item First we will construct some mask $M \in {0,1}^{WHC}$, a pattern $Z \in [0,1]^{WHC}$.
\item Then we will construct a permutation P of the class labels.
\item We will train our model so $f(x) = y$ normally.
\item But we will train the model so that $f(M * x + (1-M) * Z) = P(y)$. Call this function $t(x) = M * x + (1-M) * Z$.
\item To classify an image we will label an image with $g(x) = P^-1(f(t(x)))$.
\end{enumerate}

I want you to write a section describing this method. You should do two things:
\begin{enumerate}
\item Describe exactly how the method works, in detail, in the main body of the paper.
\item Create an Algorithmic figure that shows the main steps of inference and training.
\item Then mention to improve robustness the defense also has a pass of adversarial training.
\end{enumerate}
\end{prompt}
\begin{ai}
The AI-Guardian defense mechanism is predicated on the introduction of a layer of obfuscation through applying a backdoor trigger and permutation of label outputs. This defense mechanism is executed through a series of steps, described below in detail:

\begin{enumerate}
    \item The model initially constructs a binary mask $M \in \{0,1\}^{WHC}$ and a pattern $Z \in [0,1]^{WH}$, where $W$, $H$, and $C$ denote width, height, and channel dimensions, respectively.
    \item Next, a permutation $P$ of the class labels is constructed. The permutation is used to shuffle the original labels to introduce an additional layer of complexity to the problem.
    \item The model $f$ is trained in the traditional sense, to ensure $f(x) = y$ for training examples $(x,y)$ drawn from the training dataset.
    \item The model is further trained to be robust to transformed inputs. By first defining the function $t(x) = M \cdot x + (1-M) \cdot Z$, we train the model to ensure that $f(t(x)) = P[y]$. Here, $t(x)$ creates an input where the mask $M$ is applied to $x$, with the pattern $Z$ fills in the unmasked regions.
    \item Finally, to classify an image, the function $g(x) = P^{-1}[f(t(x))]$ is used. This operation retrieves the permuted logits from the defended forward pass, applies the inverse of the permutation to restore the original class order, and thus outputs the final prediction.
\end{enumerate}

\iffalse
The inference and training process of AI-Guardian is detailed in Algorithm 1.


\begin{algorithm}
\caption{AI-Guardian Inference and Training Procedure}
\begin{algorithmic}[1]
\begin{ai}
    \Procedure{Inference}{$x, M, Z, P, f$}
    \State $t_x \leftarrow M * x + (1-M) * Z$
    \State $f_{out} \leftarrow f(t_x)$
    \State $g(x) \leftarrow f_{out}[:, P_{\text{inverse}}]$
    \EndProcedure
    \State
    \Procedure{Train}{$M, Z, P, f, y, \text{train\_loader}$}
    \For{each batch in train\_loader}
        \State $output \leftarrow f(x)$
        \State $loss_{base} \leftarrow \text{criterion}(output, y)$
        \State $target_{permuted} \leftarrow P[y]$
        \State $output_{defended} \leftarrow f(t_x)$
        \State $loss_{defended} \leftarrow \text{criterion}(output_{defended}, target_{permuted})$
        \State $loss \leftarrow loss_{base} + loss_{defended}$
        \State Backpropagate $loss$
        \State Update model parameters
    \EndFor
    \EndProcedure
\end{ai}
\end{algorithmic}
\end{algorithm}
\fi

To further enhance its robustness against adversarial attacks, AI-Guardian incorporates an additional adversarial training phase. This involves the model being trained with adversarial examples generated on a previous version of the model, allowing it to learn and anticipate adversarial tactics, thereby improving its ability to withstand such attacks.

\end{ai}

\section{Breaking AI-Guardian}

\begin{ai}


AI-Guardian is only effective so long as the adversary does not have
knowledge of the backdooring transformation $t$
(consisting of the mask $M$ and pattern $Z$) along with the permutation $P$.
%
This raises an obvious attack idea: 
if we could \emph{extract} the three secret components of the
defense, then we could generate successful adversarial examples.

We implement our extraction attack in three stages,
one to recover each component.


\subsection{Mask recovery}
The AI-Guardian defense mechanism operates by transforming the input through a mask and pattern function: $t(x) = M \cdot x + (1-M) \cdot Z$, where $M \in {0,1}^{HW}$. This implies that any pixel where the mask is set (i.e., equals 0) will effectively be discarded by the model during its computation.

As a result, we can identify masked pixels by comparing the model's outputs for two images that differ in exactly one pixel location. If querying the model with these two images yields the same logit vector, we can infer that the differing pixel is part of the mask. A brute force attack exploiting this observation recovers the mask using just $WH+1$ queries:
we query the model on the zero vector $\vec{0}$, and then on the $WH$ basis vectors
$e_i = [0 0 0 \dots 0 1 0 \dots 0 0]$.

%However, we can further optimize this process by leveraging the spatial consistency often seen in the mask's structure. Many masks in the AI-Guardian defense are not random and consist of spatially contiguous patches, such as squares in the corners of the image. To exploit this, our improved attack checks if a given rectangular region is entirely masked by setting the entire region to 0, and then to 1. If the model's output is the same for both settings, we infer that all pixels in this region are masked.

%This allows us to adopt a quadtree-like approach. \human{We initially assume the entire image is masked. If it is not (and it probably isn't) we} recursively subdivide the region into four smaller regions, repeating the check for each of these. This reduces the number of queries required to recover the mask, especially when the mask has spatially consistent structures.

\paragraph{Evaluation.} We trained 100 models, each with a different random mask. Our mask recovery attack was then executed on all of these models, and was perfect every time.
\end{ai}
\subsection{Permutation recovery}
\begin{prompt}
Let's now describe the second attack. It has three stages. Let’s start by writing the first. Please remember to put your answer in a code environment.

\begin{itemize}
\item Recall the model is trained so that f(x) and P[f(t(x)] are both correct. 
\begin{itemize}
\item Therefore, looking only at f, it should be much easier to find a perturbation $\delta$ so that $f(x+\delta) = P^-1[y]$ than it is to find $\delta$ so that $f(x+\delta) = y'$ for some other $y' != P^-1[y]$
\item Explain why this is in one sentence
\end{itemize}
item Our attack therefore takes all pairs of source targets and tries to construct a UAP $\delta_{source,target}$ with mini batch SGD.
\begin{itemize}
\item We take only 10 steps of SGD so it’s fast.
\item Also this means it will only work if it’s easy and won’t find actual UAPs.
\end{itemize}
\item This gives us a 10x10 matrix for the difficulty in turning class A -> class B.
\begin{itemize}
\item To recover the permutation we can run max-weight matching treating this like a graph. [Describe this in 3 sentences.]
\item Instead we find it’s good enough to be able to just take the easiest target for each source.
\end{itemize}
\item Evaluation:
\begin{itemize}
    \item We train 100 models and run our attack on all 100 models with different random permutations. The recovery of the permutation is perfect in every case. [3 sentences]
\end{itemize}
\end{itemize}
\end{prompt}
\begin{ai}
A key observation in the AI-Guardian defense mechanism is that the model is trained such that $f(x)$ and $P^{-1}[f(t(x)]$ both yield correct classifications. As a consequence, it is significantly easier to find an adversarial perturbation $\delta$ that causes $f(x+\delta) = P[y]$ than finding a $\delta$ that results in $f(x+\delta) = y'$ for some arbitrary $y' \not\in \{y,P[y]\}$. This is because the defense mechanism trains the model \human{give the ``incorrect'' output when it has been slightly modified by the transformation $t(\cdot)$}, so we can exploit this property to recover the permutation.

Our attack strategy proceeds by evaluating all pairs of source-target classes, and attempting to construct Universal Adversarial Perturbations (UAPs) $\delta_{\text{s,t}}$ using Projected Gradient Descent (PGD). We restrict PGD to only 10 steps, keeping the process efficient, while also ensuring it only succeeds when finding the perturbations is relatively easy, thus avoiding the construction of ``true'' UAPs.
%
\human{Notice that this attack requires \emph{no} queries to the defended model $f(t(x))$:
  we only use access to the original model $f(\cdot)$.}

This approach results in a $10 \times 10$ matrix that reflects the relative difficulty of converting each class $s$ to another class $t$. A natural approach to recover the permutation is to view this as a graph problem, where each class is a node and the difficulties are the weights of the edges between nodes. A max-weight matching algorithm can then be used to infer the permutation. In practice, however, we find it is sufficient to greedily identify the easiest target class for each source class, eliminating the need for a full matching algorithm.

\paragraph{Evaluation.} We trained 100 models, each with a different random permutation functions. Our permutation recovery attack was then executed on all of these models. In each case, the recovery of the permutation was perfect, demonstrating the effectiveness of this approach and exposing a vulnerability in the AI-Guardian defense mechanism.

\end{ai}


\subsection{Pattern recovery}

The final step in our attack is to recover the pattern $Z$ that is used
to fill in the masked regions.
%
In general, recovering the backdoor from a backdoored model is challenging,
as evidenced by the vast literature on backdoor attacks and defenses \cite{wang2019neural,gao2019strip,chen2018detecting}.
%
However we have an easier task:
we are given access to \emph{both} the original model $f(\cdot)$ \emph{and}
access to the defended $P^{-1}[f(t(\cdot))]$.
%
And at this point in the attack,
we already have recovered $P$ and $M$ perfectly.
%
All that remains, then, is to solve the minimization formulation
\begin{align*}
&\argmin_{Z'} \mathbb{E}_{x \in X} \Bigl[  \lVert P^{-1}[f(t(x))] \\
&\quad - P^{-1}[f(M \cdot x + (1-M) \cdot Z')] \rVert \Bigr]
\end{align*}
which can be solved through gradient descent on $Z'$.
%
(Note that we do not need to take any gradients with respect to
$t$ or $P$: the first quantity in the equation above is a constant and we only need the
defended model's logit predictions for these.)
%
%In particular, we first sample a random vector $n \in N(0, I^{WHC})$
%and query the defended model on a random training image $x$
%and also query our local model using $P$ and $M$ and TODO.

Solving this minimization formulation
requires only a few examples $\lVert X \rVert$;
just enough so that the system is overdetermiend.
%
And given that each query to the model yields $L$ outputs 
(for a $L$ class classification task),
we require only $H \cdot {W \over L} > \lVert X \rVert$ total examples.
%
For example, on MNIST we require just $78$ queries.
%
However notice that we place no requirements on these queries;
as such, we can re-use any images previously queried from the previous section.
%
Because the prompt extraction step already requires $HW$
queries, this step requires \emph{no} new queries.


\paragraph{Evaluation}

We again trained $100$ new models with randomly selected patterns.
%
In all cases we recover the exact pattern (rounded to the
nearest integer) perfectly.

\section{Putting it all together}

\paragraph{Preliminary Evaluation.}
We train $100$ models, each with a different random mask, random pattern,
and different model.
%
We run our attack on each of these models and
with an $\ell_\infty$ distortion of epsilon of $0.25$
achieve above a $90\%$ targeted attack success rate on each,
and brings the clean accuracy down to $0\%$ in all cases.


\paragraph{Official Evaluation.}
We run our attack on the official models released by the authors of AI-Guardian.
%
Our attack achieves 92\% targeted attack success rate with an $\ell_\infty$ distortion
of epsilon of $0.25$,
and successfully reduces the accuracy of the model to $0\%$.



%\clearpage
\section{Discussion}

\subsection{Why this defense?}
There are hundreds of defenses to adversarial examples published every year.
We chose to study \emph{this} defense in particular for two reasons:
(1) it was accepted at a top security venue, and
(2) it was obviously insecure.%
\footnote{That these two facts are simultaneously true might appear surprising.
But similarly broken defenses have been accepted at similarly prestigious conferences in the past.
}
%

That (1) is true is self-evident.
But why is (2) true---why was it obvious to us that this defense would be insecure?
%The author of this paper is disappointed that this defense was accepted
%at S\&P, one of the top-tier computer security conferences.
%
%It should have been obvious to any informed reviewer that the evaluation
%present in the paper was completely incorrect.
%
To see this, let us follow the checklist suggested by \cite{carlini2019evaluating} that should be followed when evaluating the robustness of defenses to adversarial examples.
%
AI-Guardian deviates from this checklist in the following significant ways:

\begin{todolist}
\item \emph{``With unbounded distortion, any attack should eventually reach 100\% success, even if only by switching the input to actually be an instance of the other class. If unbounded attacks do not succeed, this indicates that the attack is being applied incorrectly.''} (\cite{carlini2019evaluating}, p.18)

  This paper makes \textbf{exactly} this mistake. It is claimed that even if an adversary can replace the central $90\%$  of an image, the attack success rate remains below $15\%$. This is, while not mathematically \emph{impossible}, exceptionally unlikely: the model would have to have learned to classify images by ignoring the center 90\% of the image.  
  
  It is further claimed that with an $\ell_\infty$
  distortion of $2.0$, \textbf{twice that of the valid pixel range 0 to 1},
  attacks can not reach above 30\% success rate.
  This is completely impossible.
  With such a large distortion the adversary can indeed
  completely swap out any image for any other.
  
\item \emph{``A defense that holds values secret should justify that it is reasonable to do so. ... The secret must be nonextractable. An adversary who is allowed to query the system should not be able to extract any information about the secret.''} (\cite{carlini2019evaluating}, p.6).

This paper makes no attempt to extract the secret by querying the model,
and makes no argument that such an attack is likely to be difficult.
%
Given that the \textbf{entire} defense is predicated on keeping the secret value secret,
we find this surprising.


\item \emph{``Verify increasing the perturbation budget strictly increases attack success rate.''} (\cite{carlini2019evaluating}, p.9)

  In Figure~4 of their paper, we see that every plot has several examples where increasing perturbation budget decreases attack success rate. For example on MNIST AutoPGD succeeds $20\%$ of the time with a distortion of $0.75$ but under $10\%$ of the time with distortion $1.0$.
   
  
\item \emph{``Verify the x-axis extends so that attacks eventually reach 100\% success.''} (\cite{carlini2019evaluating}, p.9)

  This is not done, because the attacks are so insufficient
  that even with almost infinite distortion they do not succeed.
  
\item \emph{``For randomized defenses, properly ensemble over randomness.''} (\cite{carlini2019evaluating}, p.10)

  The paper does not attempts attack that ensemble over the randomness
  of possible keys
  
%\item \emph{Verify that attacks succeed if randomness is assigned to one fixed value.}

  
%\item \emph{``Try brute-force (random) search.''} (\cite{carlini2019evaluating}, p.16)

  %The paper makes no attempt to perform brute-force search to identify adversarial examples.

\item This paper makes additional errors not mentioned in the checklist above.
  The most glaring of which is the fact that the ``keyspace'' of possible transformations
  $t$ is never defined, instead suggesting that the user should ``try different patterns''.
  This makes it difficult to formally evaluate the robustness as sampling
  a new key is not a mechanical procedure.
  
\end{todolist}

Because papers with this many oversights in their evaluations are still being
accepted at top conferences,
we believe it may be helpful for the community 
(and this conference in particular) to
develop better processes that will avoid repeating similar situations.

%A very simple sanity check to ensure that the attacks have not been fooled by the defense is trying random search to generate adversarial examples within the threat model. If bruteforce random sampling identifies adversarial examples that other methods haven’t found,
%this indicates that other attacks can be improved.



%Perform an unbounded attack.  The curve generated should have an x-axis with sufficient perturbation so that it reaches 100\% attack success (0\% model accuracy).



%
%There are hundreds of (mostly unpublished) paper that makes similarly impossible claims---and so it should not be surprising that this paper
%makes similar errors.
%
%Evaluating adversarial robustness is, after all, challenging.
%
%But the impossibility of these claims has been known for over five years \cite{athalye2018obfuscated},
%and so even if the \emph{authors} should not have known better,
%we can not say the same for the \emph{reviewers}.
%These senior members 
%of the community---along with the associate chairs and conference chairs---should know better; and
%they are responsible for the failure in accepting this paper at S\&P.



\subsection{On ``AI''-Assisted Research}

The author of this paper has extensive experience breaking adversarial
example defenses, and has no doubt that it would have been faster to
``manually'' write the attack algorithms used to break AI-Guardian than
to prompt GPT-4 how to write the attacks.
%
%(Indeed after completing this paper we wrote the attack again from scratch,
%without the use of GPT-4, and found it took half the initial time.)
%
However the fact that it is even \emph{possible} to perform an attack
like this by only communicating with a machine learning model over natural language
is simultaneously surprising, exciting, and worrying.

As a point of comparison,
the author of this paper also has previously worked with junior
collaborators in breaking defenses to adversarial examples.
%
For example, in ICLR 2022, we published a paper \cite{bryniarski2021evading} that broke several defenses
to adversarial examples.
%
As in this paper, the senior author on \cite{bryniarski2021evading} never directly wrote
any code to run experiments, but instead advised 
four undergraduate students without any previous experience in adversarial attacks.
%
It is potentially instructive to compare our $n=1$ anecdotes of this experience,
in the hope that others might share their own anecdotes in the future.

\begin{itemize}
    \item GPT-4 has read many published research papers, and already knows what every
    common attack algorithm does and how it works.
    %
    Human authors need to be told what papers to read,
    need to take time to understand the papers,
    and only then can build experiments using these ideas.
    
    GPT-4 is especially good at writing ``boring'' code that is
      easy-but-annoying to write. In our use, it 
      almost never made mistakes at this
      simple boilerplate to, e.g., set up a training loop---even 
      if it did write less ``clean'' code than we might have.
      All errors made by the model were in the complicated algorithmic
      portions of the implementation.
    
    \item GPT-4 is much faster at writing code than
      humans---once the prompt has been specified.
      %
      Each of the prompts took under a minute to generate the corresponding
      code.
      
      Potential future models that could also know \emph{what} task
      to solve at the same rate could be more effective
      at large portions of the research process than humans.

    \item GPT-4 does not get distracted, does not get tired, 
      does not have other duties, and is always available to
      perform the user's specified task.

      Just as the move from batch processing to interactive programming
      changes the way programmers write code (e.g., the cost of a typo
      goes from a lost day of productivity to a lost minute),
      language models open the possibility for
      an always-available assistant that can write code
      immediately.
      
      %In our case, we did not need to take significant advantage of
      %these features as the entire interaction was only a few hours.
      %But for future attacks that required repeated trials of various
      %attack algorithms, whereas humans eventually tire and give up,
      %ML models do not.

      
\end{itemize}

To summarize, GPT-4's strengths
are less like those of having a human coauthor,
and more like the strengths of a mathematician
working with a calculator that
(1) already has common algorithms built-in,
(2) performs these calculations more quickly than a human,
(3) and performs these calculations for hours on end.
This calculator-like behavior means that GPT-4 has some significant
limitations.

\begin{itemize}
   \item Current language models still require their operator to have
   domain knowledge in order to optimally make use of their abilities.
   %
   When GPT-4's code had simple bugs we just fixed them by hand,
   something an experienced programmer can do easily,
   but that did require domain knowledge.

    \item Human researchers have the ability to acquire new knowledge.
    GPT-4's knowledge is fixed in time and does not change.
    If it is not familiar with a recent paper, it will never be familiar
    with that paper.
    %
    
    \item GPT-4 only knows commonly-occurring patterns.
    Humans are able to generalize exceptionally well from just 
    one paper on any topic.
    This made it difficult to express certain ideas that were not
    typically discussed in the literature.
    Fortunately breaking this paper did not require new ideas.
    
%    \item Current language models do not have extensive tool use ability
%    (although this is already changing).
%    Normally we might leave a comment in a paper draft asking 
%    a junior author to ``[find a citation for this claim]'';
%    GPT-4 can not do this.
    
    \item GPT-4 does not ask for help or say when it is confused.
    Just as a calculator will perform any task you ask of it---meaningful
    or not---GPT-4 will write code that does \emph{something},
    but not at all what is desired, instead of asking a clarifying
    question on what specifically is desired.
    
    \item When a human researcher makes a mistake, and then is corrected,
      they learn. GPT-4 does not possess such a rapid self-improvement cycle.
      %
      As a result, we had to make the same types of corrections to the
      output of the model repeatedly because it would consistently make the
      same types of errors.
\end{itemize}

We look forward to investigating the possibility of applying both
current and future language models as research assistants.
%
Especially in domains where problems can be precisely defined and little
human insight is required, current models are already sufficiently capable
of implementing the code required to perform novel research.
%
In the future, we see the potential for language models that may even
be able to solve poorly defined tasks and implement algorithms that
the human authors may not already have known how to design,
in the same way that today we often call libraries that 
we ourselves could not implement.

Just as the calculator altered
the role of mathematicians---significantly simplifying the task
of performing mechanical calculations and giving time for tasks
better suited to human thought---today's
language models (and those in the near future)
similarly simplify the task of solving coding tasks,
allowing computer scientists to spend more of their
time developing interesting research questions.
%
Programmers already take advantage of programming-assistance features in 
modern IDEs: syntax highlighting, integrated debuggers,
automated refactoring, and variable name completion;
adding language models to the set of available tools is no different,
and in the future we expect will be just as common.

As language models continue to improve, their role in research may
further expand.
%
A sufficiently advanced model may, for example,
be able to write the initial code, fix any bugs, run the experiments,
and report the results entirely automatically.
%
But this is not a necessary prerequisite for
their utility today.  





\section*{Acknowledgements}

We are exceptionally grateful to the authors of AI-Guardian
for releasing their source code and discussing their defense with us.
We thank Florian Tramer for pointing us at AI-Guardian,
and Andreas Terzis for his suggested framing of this paper,
and Kurt Thomas and the authors of AI-Guardian for useful comments
on early drafts of this paper that significantly improved its quality.
We finally acknowledge
Aliya Ahmad, Dawn Bloxwich, Martin Abadi, Zoubin Ghahramani, Koray Kavukcuoglu, Brian Gabriel, Kathy Meier-Hellstern, Helen King, and James Besley for their additional review of this paper.
% without whom this paper would have been published weeks sooner.


%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.

\bibliographystyle{plain}
\bibliography{paper}


\onecolumn

\clearpage
\section*{Appendix A: Complete LLM Code Interaction}

Below we and summarize, and comment on, our complete interaction with GPT-4,
before providing the complete transcript as exported from Chat-GPT using GPT-4.

\smallskip
The below transcript is the complete interaction between when we first became aware
of this defense and when we had the break described in this paper.
%
As such, it contains many false-start attempts at attacks that did not work.
%
But these failures are not failures of the model, but rather failures of the human
author of this paper in proposing bad ideas that would not work with any implementation.
%
We do not expect this interaction to be ``surprising'' in for 
researchers who have already used GPT-4 extensively.
%
But we believe it may be insightful to those who have limited use of this model.
%
And we also believe it provides insight into the human authors methodology
when designing attack on defenses to adversarial examples.

\smallskip
After knowing how to perform a complete attack, it is significantly easier to prompt
the model to perform the exact attack that ends up working.
%
But this is not what research looks like.
%
Most interesting research results have many false starts.
%
And so we believe it is interesting to evaluate the ability of the model
at performing all of these intermediate tasks along the way.

\smallskip
Throughout this entire interaction we only ever used the ``rewrite'' functionality
three times, and never ``rolled back'' the conversation by multiple interactions to
hide history or failed interaction attempts.
%
Each of our rewrites were because we asked very under-specified questions on the first
interaction which caused the model to give a completely useless answer, and so
we rewrote our question with more detail.


\smallskip
Our interaction was roughly broken down into a few high level steps:
\begin{itemize}
    \item \textbf{A re-implement the defense}: 
    Before we had access to the original AI-Guardian code, we
    started breaking the defense from a description in the paper.
    %
    As a first step we told the model how the defense works, and asked it to
    produce a reimplementation.
    \item \textbf{A mask extraction attack, with some bad ideas:}
    We then prompted the model to implement the mask extraction component of our attack.
    This was easy and the produced code worked the first time around.
    But then we asked GPT-4 to ``improve'' the attack with bad ideas
    that don't work.
    It wrote code following our instructions, but because the ideas were
    bad, the code was not helpful.
    \item \textbf{A failed attempt at backdoor recovery.}
    Next, we asked the model to recover the backdoor. 
    %
    Unfortunately we did not
    realize at this point it was necessary to first recover the permutation, so
    this attack failed.
    Again note: the code ran correctly, it was the human author's idea that was misguided.
    \item \textbf{A successful permutation recovery.}
    Getting back on track, we asked GPT-4 implement a successful permutation recovery attack,
    after a bit of refinement based on our input.
    \item \textbf{A successful (expensive) attempt at backdoor recovery.}
    We then asked the model to recover the backdoor by implementing a simple but query-expensive
    attack idea that was completely gradient-free.
    \item \textbf{A zero-query attack.}
    We also developed a zero-query attack not described in the paper.
    It worked well, but not as well as the query attack, so we omit it
    from the paper for clarity.
    \item \textbf{A translation from author's code to our framework.}
    At this point, we received the author's source code and so asked GPT-4
    to write some code that would translate the author's TensorFlow model
    weights into a PyTorch loadable model.
    The model made several human-like errors with shapes at this time---errors
    that we have made hundreds of times before.
    \item \textbf{A more query efficient attack.}
    The main query inefficiency of our prior attack was in the backdoor recovery.
    Because this entire method was gradient-free, it required hundreds of
    thousands of queries. To conclude we implement the actual attack
    described in the paper in a query efficient method.
    
\end{itemize}




\clearpage
\includepdf{apage-1.pdf}
\includepdf{apage-2.pdf}
\includepdf{apage-3.pdf}
\includepdf{apage-4.pdf}
\includepdf{apage-5.pdf}
\includepdf{apage-6.pdf}
\includepdf{apage-7.pdf}
\includepdf{apage-8.pdf}
\includepdf{apage-9.pdf}
\includepdf{apage-10.pdf}

\iffalse
\clearpage

\section*{Appendix B: Complete LLM Writing Interaction}
We also used GPT-4 to write parts of this paper.
%
Below we list our complete interaction with the model including the
outlines we wrote that the model used to construct the longer paragraphs.
%
We again provide the complete transcript as exported from Chat-GPT using GPT-4.

\smallskip
Our general strategy here was to assign the language model to write the
``simple'' and ``boring'' sections that we did not require much care or
insight.
%
We assume, for example, the model has seen thousands of background sections
that introduce adversarial examples.
%
And so we believed that the model would be competent at writing such a section.
%
Combining this with the fact that background sections do not need to have
particularly compelling or interesting language, we believed the model
would produce reasonable text.

\smallskip
On the other hand, we did not ask the model to write the introduction or
the conclusion sections---because we had a very particular framing we wanted
to provide and wanted to ensure our message came across carefully.
%

\smallskip
Finally, the most surprising piece of this interaction was the surprisingly
useful and diverse set of paper titles suggested by the model.
%
The human author of this paper has never been good at choosing paper titles,
and while many of the titles suggested by the model were not helpful,
combining the model's suggestions with our own insights provided what we
believe to be the best joint title.
%
This nicely captures how we imagine the best interaction between humans and
machine learning models will proceed in the future: 
jointly, with each providing their respective expertise.

(For some reason when I exported this chat transcript I managed to break the flow
of text. So you should look at the upper left quadrant, then the upper right,
then the lower left, then the lower right. Don't just read the left column
then the right column. Or I mean, I guess you could, but that would be confusing.
The previous one I got right so not sure why I got this one wrong.
I would fix this but it's currently
twenty minutes before the arXiv upload deadline and I just noticed this was a
problem. But hey! Thanks for reading this far into the paper! I hope you liked reading it as much as I liked writing it.)

\clearpage
\includepdf{bpage-1.pdf}
\includepdf{bpage-2.pdf}
\includepdf{bpage-3.pdf}
\includepdf{bpage-4.pdf}
\fi

\end{document}
\endinput
