\title{\MYTITLE\\---Supplementary Material---}
\maketitle
\thispagestyle{empty}





\section{Network Structure}

In this section, we provide details of the structure of our network.
As explained in the main paper, our network consists of the \textit{Frame Encoder}, the \textit{Event Encoder}, the \textit{Event Accumulation Module} (EAM), the \textit{Occlusion-aware Feature Fusion} (OFF) module and the \textit{Reconstruction Decoder}.

The architecture of the Frame Encoder and the Event Encoder is shown in \Tab \ref{tab:encoder}. 
Both encoders have the same structure except at the first two scales, where the frame encoder is provided with a grayscale image of dimension (h, w, 1) and the event encoder receives the output of the EAM of dimension (h, w, 32).
The architecture of the EAM module is shown in \Fig \ref{fig:EAM}.

We provide the details of the OFF module in \Tab \ref{tab:off}. 
The OFF module computes gating weights $W^{j}$  at each scale $j$ based on the current-scale event features $F^j_E$ and frame features $F^j_I$ as well as the previous-scale fused features $F^{j-1}_F$ downscaled to  $\bar{F}^{j-1}_F$. 
The fused feature $F^j_F$ is then obtained based on the current-scale event and frame features $F^j_E$ and $F^j_I$, the gating weights $W^{j}$, and the downsampled previous-scale fused feature $\bar{F}^{j-1}_F$.
The OFF module can be described with the following equations:
\begin{equation}
    \tilde{F}^j_F = G(F^j_E, F^j_I)
\end{equation}
\begin{equation}
    W^{j} =   G(\tilde{F}^j_F , \bar{F}^{j-1}_F)
\end{equation}
\begin{equation}
    \hat{F}^j_F = (1-W^{j}) F^j_I +   W^j F^j_E 
\end{equation}
\begin{equation}
    F^j_F = G(\hat{F}^j_F , \bar{F}^{j-1}_F)
\end{equation}
where $G$ indicates concatenation followed by convolution operations.

Finally, the structure of the \textit{Reconstruction Decoder} is shown in \Tab \ref{tab:decoder} which uses the fused features at each scale as skip connections to predict the residual image.
\input{tables/supp_tab_network_structure}
\input{figures/fig_supp_EAM}
\section{Detailed Ablation Results}
Here we present a more detailed ablation study on the network architecture.
\subsection{Importance of Event and Frame Encoders}
\input{figures/fig_supp_ablation}
\input{tables/supp_tab_synthetic_feature_importance}
We study four different strategies to combine the event and frame information and train each corresponding network on our synthetic dataset. 
The first strategy combines events and frame by concatenating them at the input stage and uses a single encoder-decoder structure to predict the reconstructed image, termed as \textit{Shared F \& E Encoder}.
The second strategy, termed \textit{Independent Frame Encoder}, uses an independent frame encoder, which computes the frame features without considering the event information.
A second encoder takes as input the events and fuses for each subsequent scale the frame features from the independent frame encoder with the feature from the previous scale.
As the third strategy, we switch the inputs of the second strategy, i.e., we use an independent event encoder and a second encoder for fusing the event features with shallow frame features (referred to as \textit{Independent Event Encoder}).
Since we want to focus on the effect of processing the features for each sensing modality separately, we use two independent encoders for events and frame respectively and fuse features at each scale using a simple convolution layer.
This is referred to as \textit{Independent Event and Frame Encoder Simple Fusion}.
Lastly, to show the effect of a more sophisticated fusion, we also show the reconstructions using our OFF module to perform the fusion of the event and frame feature at each scale (\textit{Independent Event and Frame Encoder with OFF}).

We show the qualitative comparison of these approaches in \Fig \ref{fig:feature_importance} and provide quantitative evaluations in \Tab \ref{tab:feature_importance}.
The \textit{Shared F \& E Encoder} results in the worst performance in both the textured and untextured regions of the image.
The \textit{Independent Frame Encoder} results in an improved performance in uniform areas, whereas textured regions are poorly reconstructed, as can be observed in the third column of \Fig \ref{fig:feature_importance}.
The uniform texture of the red patch is captured perfectly by this network, indicating the importance of image features for uniform areas.
This can be explained by the fact that images contain absolute intensity information of surrounding spatial regions, which provides more information to the network for filling in the uniform patches.
However, the edges are not well preserved, which results in bleeding edges, as seen in the edge of the letter `D' in the first row of \Fig \ref{fig:feature_importance}.
The \textit{Independent Event Encoder}, in comparison to the \textit{Independent Frame Encoder}, shows better ability in reconstructing textured regions, which indicates the importance of event features for reconstructing textured areas. However, it struggles to remove occlusions in uniform regions,

Both the second and the third strategy performs significantly better than the first one, highlighting the importance of having separate encoders for frames and events to enable high-quality reconstruction of both textured and untextured regions. 
However, a vanilla fusion of \textit{Independent Event and Frame Encoder Simple Fusion} does not take full advantage of the strengths of the two respective modalities and therefore results in lower performance, as can be seen in the fourth row in \Tab \ref{tab:feature_importance}.
In contrast, the \textit{Independent Event and Frame Encoder with OFF} achieves higher performance than all the above-mentioned strategies. 
Especially, it can better reconstruct textured regions, e.g., the edge of the letter `D' is recovered, as visualized in the first row of \Fig \ref{fig:feature_importance}.
We believe that the differential nature of event cameras enables the preservation of high-frequency structures such as edges.

In summary, the proposed model using independent encoders for frame and events with a sophisticated fusion shows the best overall performance.









\subsection{Importance of EAM and OFF Modules}
\input{figures/fig_supp_ablation2}
\input{tables/supp_tab_synthetic_ablation}
In this section, we study the impact of \textit{Event Accumulation module} (EAM) and \textit{Occlusion-aware Feature Fusion} (OFF) on network performance.
The quantitative performance is shown in \Tab \ref{tab:syn_ablation_supp} and the qualitative results can be seen in \Fig \ref{fig:ablation_module}.
The base model without EAM and OFF results in the worst performance of $31dB$ PSNR, and adding EAM improves it by $1.8dB$ in terms of PSNR.
This gain is achieved because the EAM is designed to select event data that is relevant to the true background and filter out redundant information.
As can be seen in \Fig \ref{fig:ablation_module}, the network with EAM ($4^{th}$ column) shows better results in both uniform and textured regions compared to the base model. 
Also, in contrast to the two methods without EAM, the true background information is recovered to a significantly better extent in textured areas, even where there are multiple overlapping occlusions.

The performance of the network gains a substantial improvement from the OFF module as well, with a $1.7dB$ increase in PSNR. 
As illustrated in \Fig \ref{fig:ablation_module}, the OFF module also improves reconstruction quality in both uniform and textured areas when compared with the base model. 
In addition, compared to the two models without OFF, the one with OFF is significantly better at reconstructing uniform or non-textured regions.

Combining EAM and OFF, the full model achieves an increase of more than $3.5dB$ PSNR over the base model. Qualitatively, the full model performs better than all other models in both types of regions, implying that the two modules benefit from each other.










\section{Additional Qualitative Results}
\input{figures/fig_suple_qual_syn_all}
\input{figures/fig_suple_qual_real_all}
We show the qualitative comparisons between our method and all the baselines on our synthetic and real-world datasets in \Fig \ref{fig:suple_syn_qual_all} and \Fig \ref{fig:suple_real_qual_all} respectively.
Qualitatively, our method outperforms other baselines for both datasets.
We re-state our conclusions that image inpainting baselines tend to hallucinate the missing areas, resulting in clean but inaccurate image reconstructions.
In comparison, our method is better at preserving the details of the original image, as can be seen in the highlighted patches.








\section{Inference Time}
In this section, we compare the computational complexity of the methods in terms of inference time.
Note that while it is not feasible to compute the precise inference time, as GPU loads vary over time due to other processes running simultaneously, we approximate this number by averaging the inference time over the entire test set.
The runtime is computed for a batch size of 1 using a Quadro RTX 8000 GPU for the learning-based baselines and Intel(R) Core(TM) i7-3720QM CPU @ 2.60GHz for our model-based baseline.
The comparison for all the baselines is presented in \Tab \ref{tab:inference}. 
Note that only our model-based baseline is run on CPU and therefore marked with a star.
Our method has an inference time of \SI{0.18}{\sec}, slightly slower than the fastest image inpainting baseline MAT \cite{li_mat22cvpr}. The best performing image-inpainting baseline ZITS \cite{dong22cvpr}, on the other hand, has an inference time of \SI{1.94}{\sec}.
\input{tables/supp_tab_synthetic_complexity}


