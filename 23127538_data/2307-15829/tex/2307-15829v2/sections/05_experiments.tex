\section{Experiments}
\paragraph{Implementation details} Our models are implemented in Pytorch~\cite{Paszke17nipsw} and trained from scratch with random weights on our synthetic dataset. 
For the experiments on our real dataset, we finetune our pre-trained network on the real training data. 
We use Adam optimizer\cite{Kingma15iclr} with a learning rate of $1\mathrm{e}{-3}$ and batch size of 4 and train for $1250$ epochs on the synthetic and real dataset.
We supervise our network with the $L1$ loss computed on the predicted and ground truth occlusion-free images.
To measure the quality of the reconstructed image, we use structural similarity index measure (SSIM) \cite{Wang04tip}, peak signal-to-noise ratio (PSNR), and mean absolute error (MAE).

\paragraph{Baselines}
We evaluate our approach against four state-of-the-art image-inpainting solutions: MAT \cite{li_mat22cvpr}, MISF \cite{li_misf22cvpr}, PUT \cite{liu22cvpr}, and ZITS \cite{dong22cvpr}.
These baselines are trained on challenging large-scale datasets with over eight million images. 
Thus, for our experiments, we use the weights provided by the authors. %
Since dynamic occlusion removal from a single viewpoint using an event camera is a novel task, there currently do not exist any event-based approaches that tackle this problem directly.
The closest baseline uses events and frames to reconstruct the background image from multiple viewpoints (EF-SAI) \cite{liao22cvpr}.
EF-SAI \cite{liao22cvpr} uses a refocus module with events to blur the foreground and focus on the target depth plane of the background. 
We adapt this baseline for our task by providing it with a single image and events from only a single viewpoint and train this network on our dataset from scratch.
Additionally, we also compare against events-to-image reconstruction methods \cite{Rebecq19cvpr}.
This method is adapted by combining the reconstructed event images with the occluded images using the groundtruth mask.
Note that all of the evaluated baselines were originally not designed for this particular task but are the closest related methods applicable to our task. %

\subsection{Results on Synthetic Dataset}
To compare our method against the baselines in controlled conditions, we evaluate all methods on our synthetic dataset.
\Tab \ref{tab:syb_main} summarizes the quantitative results.
Our method outperforms the best image inpainting baseline by \SI{3}{dB} in terms of PSNR.
\Fig \ref{fig:syn_qual} shows qualitative comparisons between different methods.
Image inpainting methods tend to hallucinate the background, resulting in visually more pleasing information rather than the true scene.
An example of this can be seen in \Fig \ref{fig:syn_qual} third row, where the inpainting method is unable to reconstruct the characters on the bus, whereas our method is able to better preserve this information.
We also outperform the event-based synthetic aperture imaging baseline EF-SAI \cite{liao22cvpr} and the event-to-image reconstruction baseline E2VID \cite{Rebecq19cvpr}.
Although the input to our method and EF-SAI consists of events and a single image, the EF-SAI baseline was designed for multi-view reconstruction.
The simple event accumulation baseline is one of the lowest performing baselines even with the knowledge of the correct contrast threshold, as the occlusions are too complex for the basic event generation model to capture the intensity changes as discussed in the \Sec \ref{sec:method:basic}.
We also analyze the effect of occlusion density on the performance of all the methods and summarize them in \Tab \ref{tab:syn_coverage}.
As expected, increasing the occlusion density decreases the performance of all the approaches.
However, at higher occlusion densities, the image inpainting methods drastically degrade in performance as they tend to hallucinate occluded areas.
In contrast, our method uses events that provide continuous intensity changes, which results in a better performance at higher occlusion densities.
We provide more qualitative results of other baselines in the supplementary material.

\input{tables/tab_synthetic_main}
\input{figures/fig_syn_qual}
\input{tables/tab_synthetic_coverage}
\paragraph{Ablation Study}
To study the effect of the EAM and OFF modules in our network, we report the network performance by removing the specific modules.
For computational reasons, we performed the ablation of the OFF module without including the EAM module, which reduces significantly the training time.
As shown in \Tab \ref{tab:syn_ablation}, adding the OFF module to the network improves the performance with respect to all metrics. 
This improvement can be explained by the ability of the OFF module to better localize the occlusions and adaptively fuse image and event features throughout the network scales.
Including the EAM module leads to an even higher performance increase of 1.9 dB in PSNR.
One possible reason for the improvement over the network without recurrent event encoding is that the EAM module is better at handling multiple overlapping occlusions by selecting events relevant to the true background and ignoring the redundant event information.
For more ablation studies showing the effect of the event and image features on the textured and uniform image regions, we refer to the supplementary material.
\input{tables/tab_synthetic_ablation}

\subsection{Results on Real Dataset}
We also evaluate our method on our real dataset collected with a custom build beamsplitter setup.
Unlike the synthetic dataset, we do not have an occlusion mask available for the sequences.
We, therefore, approximate the mask by subtracting the occluded frame from the groundtruth frame and applying thresholding.
This mask is used as an input for the image inpainting methods and event image reconstruction baselines.
The quantitative results are shown in \Tab \ref{tab:real_gray}.
Our results on the real dataset follow a similar trend to the synthetic dataset.
In \Fig \ref{fig:real_qual}, we show the qualitative results for different sequences.
While the image inpainting method results in visually clean images, the hallucination artifacts can be clearly seen in the small image patches, e.g., the fingers around the camera are missing in the sample visualized in the first row or the watch in the sample shown in the second row. 
Our method, on the other hand, can reconstruct the background information accurately.

\input{figures/fig_real_qual}
\input{tables/tab_real_gray}
