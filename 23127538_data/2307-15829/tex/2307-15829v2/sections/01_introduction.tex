\section{Introduction}
The majority of computer vision algorithms operate under the assumption that the scene being analyzed is visible in its entirety and without any obstructions. 
However, occlusions resulting from non-ideal environmental factors, such as dirt from road debris, insects, raindrops, snow, and others, have the potential to significantly reduce the quality of captured images. 
The dynamic nature of these occlusions, affects the downstream applications such as autonomous navigation, object detection, tracking, 3D reconstruction, etc. 
Dynamic occlusions pose an interesting challenge for scene understanding, especially for robotics and autonomous driving, where it is crucial to know the occluded scene for safe navigation in unknown environments.
\input{figures/fig_eyecatch}
The literature, however, has focused more on static occlusions and many different approaches have been proposed that use multiple viewpoints (synthetic aperture imaging) \cite{liu20cvpr, wang20wacv}, optical diffraction cloaking \cite{shi22acmtg}, or image inpainting \cite{bertalmio00siggraph, hays07siggraph, liu18eccv,liu19iccv, suvorov22wacv, wan2021cvpr, liu22cvpr, dong22cvpr, li_mat22cvpr}.
The assumption of static occlusion often renders the above approaches infeasible in the presence of dynamic obstructions \cite{Yang14eccv, Tauber07tsmcc}.
Image inpainting approaches are often trained on large-scale datasets to predict intensity in areas specified by a pre-defined mask. 
However, these methods hallucinate the scene since they are trained to favour more aesthetically pleasing images over the true scene content as seen in \Fig \ref{fig:eye_catch}.
While there have been several advancements in these approaches with the advent of deep learning, these methods fail to reconstruct true scene content in the presence of dynamic occlusions.
Synthetic Aperture Imaging (SAI) captures the target scene from multiple viewpoints to create a virtual camera with a large-aperture lens \cite{Wilburn05tog, Yang14eccv}. 
This makes it easier to blur the foreground occlusion and refocus the image on the background scene. 
These methods assume the occlusions are at the same depth and stationary throughout the capture process, which can be up to a few milliseconds, rendering this method infeasible for fast-moving occlusions.

In this paper, we propose a novel approach to estimate the background image in the presence of dynamic occlusions using a single viewpoint.
Our solution complements a traditional camera with an event camera.
Event cameras \cite{Gallego20pami} %
 are bio-inspired sensors that asynchronously measure changes in brightness with microsecond resolution.
When an occlusion crosses over a background image, it causes changes in intensity, thus triggering events as seen in \Fig \ref{fig:eye_catch}.
These events give additional information on the relative intensity changes between the foreground and background at a high temporal resolution, which leads to a more precise reconstruction of the background, compared to image inpainting methods which only use unoccluded image statistics. 
Since this is the first time an event camera is used to tackle dynamic occlusion removal, we collect a large-scale dataset containing 233 challenging scenes with synchronized events, occluded images, and ground truth unoccluded images.

The classical event generation model \cite{Lichtsteiner08ssc} can be used to reconstruct the background image intensity, given contrast threshold and occlusion intensity.
However, due to inherent sensor noise and instability of the contrast threshold, this approach results in poor performance at higher occlusion densities.
We, therefore, propose to use a supervised data-driven approach that learns to reconstruct the unoccluded image, implicitly learning the sensor parameters and noise characteristics.
The network predicts the background intensity using only a single occluded image and events
We show that our method relying on a single frame and events leads to a performance improvement of \SI{3.3}{dB} on our synthetic dataset and \SI{2.8}{dB} on our real dataset over the image-inpainting baselines in terms of PSNR.
Our results show that our method is capable of accurately reconstructing scenes in the presence of dynamic occlusions and has the potential to increase perceptual robustness in unknown environments. %
To summarize, our contributions are:
\begin{itemize}
    \item A novel solution to the problem of background image reconstruction in the presence of dynamic occlusions using a single viewpoint. We are the first to tackle this problem using an event camera in addition to a standard camera.
    \item A large-scale dataset recorded in the real world containing $233$ challenging scenes for background reconstruction with synchronized events, occluded frames, and groundtruth unoccluded images. 
\end{itemize}



