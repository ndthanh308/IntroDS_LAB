\begin{table}[!t]
        \centering
        \begin{adjustbox}{max width=\linewidth}
        \setlength{\tabcolsep}{4pt}
        {\small
        \begin{tabular}{lcc|ccl}
            \toprule
            &\multicolumn{2}{c}{\bfseries Frame Encoder}
            &\multicolumn{2}{c}{\bfseries Event Encoder}\\
            \cmidrule{2-3}  \cmidrule{4-5} 
            Scale $j$ & Layer  & Out Size  $(h^j, w^j, c^j)$ & Layer & Out Size  $(h^j, w^j, c^j)$\\
            \midrule 
            0 & input image & (h, w, 1) & -- &  -- \\
            1 & conv(5,1,2) & (h, w, 32) & -- & -- \\
            2 & conv(5,2,2) & (h/2, w/2, 64) & conv(5,2,2) & (h/2, w/2, 64)\\
            3 & conv(5,2,2) & (h/4, w/4, 128) & conv(5,2,2) & (h/4, w/4, 128)\\
            4 & conv(5,2,2) & (h/8, w/8, 256) & conv(5,2,2) & (h/8, w/8, 256) \\
            5 & conv(5,2,2) & (h/16, w/16, 512) & conv(5,2,2) & (h/16, w/16, 512)\\
            6 & conv(3,1,1) & (h/16, w/16, 512) & conv(3,1,1) & (h/16, w/16, 512) \\
            \bottomrule
        \end{tabular}}
        \end{adjustbox}
        \caption{
         The architecture of the \textit{Frame Encoder} and \textit{Event Encoder} in our proposed method, in which $(h^j, w^j, c^j)$ denotes the height, width, and number of channels of the output of the layer corresponding to scale $j$, and conv($k$, $s$, $p$) denotes a convolution block with kernel size $k$, stride $s$ and padding $p$. 
        The input layer corresponds to $j=0$, and the input size used in our paper is h$=h^0=384$, w$=w^0=512$.  
        }
        \label{tab:encoder}
    \end{table}
    
    \begin{table}
        \begin{adjustbox}{max width=\linewidth}
        \setlength{\tabcolsep}{4pt}
        {\small
        \begin{tabular}{@{}cccc@{}}
            \hline
            \multicolumn{4}{c}{\bfseries Occlusion-aware Feature Fusion (OFF)}\\
            \hline
            Input & Input Size & Layer & Output\\
            \midrule
            $F_E^j$ \& $F_I^j$ & $(h^j, w^j, c^j)$ & conv(5,1,2) $\times$ 2 & $\tilde{F}_F^j$ \\
            $F_F^{j-1}$ &$(h^{j-1}, w^{j-1}, c^{j-1})$ & conv(5,$s^j$,2) & $\bar{F}_F^{j-1}$ \\
            $\bar{F}_F^{j-1}$ \& $\tilde{F}_F^j$  & $(h^j, w^j, c^j)$ & conv(5,1,2) $\times$ 2 & $W^{j}$ \\
            $F_E^j$, $F_I^j$ \& $W^{j}$ & $(h^j, w^j, c^j)$ & Feature Selection  & $\hat{F}_F^{j}$ \\
            $\hat{F}_F^{j}$ \& $\bar{F}_F^{j-1}$ & $(h^j, w^j, c^j)$ &  conv(5,1,2) $\times$ 2 & $F_F^{j}$ \\
            \bottomrule
        \end{tabular}}
        \end{adjustbox}
        \caption{The structure of the \textit{Occlusion-aware Feature Fusion} module in the proposed method.}
        \label{tab:off}
    \end{table}

    \begin{table}
        \begin{adjustbox}{max width=\linewidth}
        \setlength{\tabcolsep}{4pt}
        {\small
        \begin{tabular}{@{}lccc@{}}
            \toprule
            &\multicolumn{3}{c}{\bfseries Reconstruction Decoder}\\
            \cmidrule{2-4}
            Scale & Input Size & Layer & Output Size\\
            \midrule
            6 &(h/16, w/16, 512+512) & up(2)-conv(5,1,2) & (h/8, w/8, 512) \\
            5 &(h/8, w/8, 512+256) & up(2)-conv(5,1,2) & (h/4, w/4, 256)\\
            4 &(h/4, w/4, 256+128) & up(2)-conv(5,1,2) & (h/2, w/2, 128) \\
            3 &(h/2, w/2, 128+64) & up(2)-conv(5,1,2) & (h, w, 64) \\
            2 &(h, w, 64+32) & conv(3,1,1) & (h, w, 32) \\
            1 &(h, w, 32)    & conv(1,1,0) & (h, w, 1) \\
            \bottomrule
        \end{tabular}}
        \end{adjustbox}
        \caption{The structure of the \textit{Reconstruction Decoder}, in which 'up(2)' denotes updampling by 2.}
        \label{tab:decoder}
    \end{table}
    
