\section{Related works}

\subsection{Fame-based methods}
Image inpainting is the task closest related to our tackled problem of removing dynamic occlusions in a single camera view.
In image inpainting, the goal is to reconstruct missing pixel intensities in areas specified by a pre-defined mask.
Traditional image inpainting methods rely on the statistics of the unmasked image areas to directly infer the missing regions~\cite{bertalmio00siggraph, ballester01tip} or copy textures from database images~\cite{criminisicvpr03, hays07siggraph}.
Because of the ability to account for the context in masked images, current research on image inpainting mostly focuses on deep generative approaches~\cite{iizuka17siggraph,liu18eccv,liu19iccv, nazeri19iccv, pathak16cvpr, yu2018cvpr}.
To provide global contextual information early on in the network, LaMa\cite{{suvorov22wacv}} proposes to perform convolutions in the Fourier domain.
In~\cite{guo21iccv}, a two-stream network fuses texture and structure information for a more accurate image generation.
To generalize to different scenes, MISF~\cite{{li_misf22cvpr}} proposes to apply predictive filtering on the feature level to reconstruct semantic information and on the image level to recover details.
More recently, transformers were also applied to the task of image inpainting~\cite{wan2021cvpr, liu22cvpr, dong22cvpr, li_mat22cvpr} and achieve state-of-the-art performance.
In PUT~\cite{liu22cvpr}, an encoder-decoder is applied to reduce the information loss suffered by tokenizing and dekonizing the tokens for the transformer backbone.
Instead of directly predicting the pixel intensities, ZITS~\cite{{dong22cvpr}} uses a transformer model to restore the structures with low resolution in form of edges, which are then further processed by a CNN.
Generally, since the only information for reconstructing image patches is given by the structure and context of the unmasked regions, image inpainting methods aim to reconstruct plausible and aesthetically pleasing images without the focus on reconstructing the real scene content. 
Additionally, the application of image inpainting methods for occlusion removal requires the estimation of the occlusion mask.
Therefore, image inpainting methods tackle a different problem setting and are not designed for the task of removing dynamic occlusions in a single-camera view.


Instead of considering only one image, a sequence of images taken at different positions can be leveraged to remove reflections and static occlusions~\cite{liu20cvpr} or contaminant artifacts on lenses~\cite{li21iccv}.
In the field of synthetic aperture imaging (SAI), different camera views are used to estimate the incoming lights from the background onto a synthetic camera view.
By considering a large number of images captured at different positions, SAI methods can focus on different planes in the scene, effectively removing the static occlusions by blurring them out~\cite{Wilburn05tog, vaish04cvpr, vaish06cvpr, matusik12cvpr, Yang14eccv, pei13pr}.
Furthermore, various imaging setups with hardware modifications are developed.
In DeOccNet~\cite{wang20wacv}, multiple views of a light field camera are used in an encoder-decoder fashion to remove the static occlusion.
Capturing the same scene from multiple viewpoints constrains the occlusion to be stationary during the capture process, which can range from a few milliseconds to seconds depending on the camera speed and exposure time.
Our problem, therefore, becomes more challenging to solve with SAI, as these methods are designed to remove static occlusions by considering multiple camera views.
Another interesting work~\cite{shi22acmtg} proposes to learn a diffractive optical element, which can be inserted in front of the lens to focus better on the objects further away from the camera, which effectively removes thin occluders close to the camera.


\subsection{Event-based methods}
It was shown that event cameras are capable of generating image sequences in high-speed and high-dynamic range scenes in which standard cameras fail~\cite{Rebecq19cvpr, Rebecq19pami, Mostafavi19cvpr, zhang20eccv}.
Moreover, event cameras are also successfully applied in combination with frame cameras for different imaging tasks.
Specifically, the high temporal resolution of event cameras is used for filling in missing information between two frames to create high frame-rate videos~\cite{Tulyakov22cvpr, wang-et-al-2020, Zhiyang21iccv} or to deblur images~\cite{Songnan20eccv, Pan19cvpr}, whereas the high-dynamic range property is leveraged in HDR imaging~\cite{han-et-al-2020,wang-et-al-2020, Messikommer22cvprw}.
Recently, event cameras achieved impressive results for removing static occlusions using a moving camera, which is termed event-based synthetic aperture~\cite{zhang21cvpr, yu22pami}.
In~\cite{{liao22cvpr}}, frames are additionally considered to reconstruct the background information behind a static, dense occlusion.
In contrast to our proposed method for dynamic occlusion removal, the stated event-based synthetic aperture approaches tackle a different task of removing static occlusions, which are captured at different positions using a moving camera.




