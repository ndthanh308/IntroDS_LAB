\section{Dataset}
To the best of our knowledge, there exists no dataset aimed at tackling the problem of dynamic occlusion removal using an event camera.
Therefore, we generated a new synthetic dataset containing frames and events to compare our approach against existing baselines in controlled conditions.
Furthermore, to validate our approach in the real world, we record a novel dataset with real events and images.
Both datasets lay the foundation for future research in dynamic occlusion removal using event cameras.

\subsection{Synthetic Dataset}
\input{figures/fig_syndata}
For the synthetic dataset, we simulate $320$ training and $160$ test sequences, each containing a sequence of occluded images, synthetic events, and an occlusion-free ground truth image.
The background images are sampled from the MS-COCO dataset \cite{Lin14eccv} and cropped to a resolution of $384 \times 512$.
Each sequence consists of an image covered with circular-shaped particles which smoothly move across the image. %
To introduce more variability in the data generation process, we randomize the size, intensity, and velocity of the occlusion particles. %
This is done to simulate different occlusion densities from $10$\% to $60$\% occlusion ratio as seen in \Fig \ref{fig:syn_data}.
Since the generation of events requires high-frame-rate videos, we compute the continuous-time trajectory of the particles over the duration of the sequences and render the occluded frames at a high framerate.
Events are generated from these rendered images using ESIM \cite{Rebecq18corl}.
Finally, for each sequence, we create synchronized event sequences, the occluded image, groundtruth occlusion mask, and groundtruth background image.
These groundtruth occlusion masks are only used as an input to the image inpainting baselines.
We consider grayscale images instead of RGB images because events from the event camera only provide relative intensity changes but lack color information.
Therefore, the reconstruction of RGB images using events introduces additional challenges.


\subsection{Real-world Dataset}
We build a dynamic occlusion dataset where the event streams are captured by a Prophesee Gen4 camera \cite{Finateu20isscc}, with a resolution of $1280 \times 720$, and the images with a FLIR Blackfly S camera with a resolution of $4000 \times 3000$.
The images are captured at a framerate of $15$ fps and with an exposure time of $\SI{7}{\milli \second}$.
The cameras are hardware synchronized and mounted in a beam splitter setup (\Fig \ref{fig:setup} (a)), which contains a mirror that splits the incoming light to the event and frame camera, ensuring alignment between events and frames.
Similar to the synthetic dataset, background images are sampled from the MSCOCO dataset \cite{Lin14eccv} and projected on a TV screen, which is recorded with the beam splitter setup pointed towards the screen. 
The occlusions are created by rolling Styrofoam balls over the TV screen as shown in \Fig \ref{fig:setup}  (b).
For each sequence, the ground truth is collected by recording the image projected on the screen without any occlusions.
We sample $3$ frames from each sequence corresponding to low, medium, and high occlusion densities as shown in \Fig \ref{fig:real_data}.
Overall, the real-world dataset consists of $154$ training sequences and $79$ test sequences.

\input{figures/fig_setup}
\input{figures/fig_realdata}