\section{Method}
\paragraph{Problem formulation}
Let us assume an event-based dynamic occlusion setting, where we are given an occluded frame $I_0$ at timestep $0$ and an event sequence recorded between timestep $0$ and $t$.
We aim to reconstruct the background image from event sequences by integrating information from events and the occluded image.
\paragraph{Basic considerations} 
\label{sec:method:basic}
Event-cameras are novel, bio-inspired sensors that asynchronously measure \emph{changes} (i.e., temporal contrast) in illumination at every pixel, at the time they occur \cite{Lichtsteiner08ssc,Suh20iscas,Finateu20isscc, Posch11ssc}.
When a particle occludes a background pixel at position $x_k$ at time $t$, the intensity changes from $L(\mathbf{x_k},t_k-\Delta t_k)$ to $L(\mathbf{x_k},t_k)$. 
In particular, an event camera generates an event $e_k = (\mathbf{x}_k,t_k,p_k)$ at time $t_k$ when the difference of logarithmic brightness at the same pixel $\mathbf{x}_k=(x_k,y_k)^\top$  reaches a predefined threshold $C$:
\begin{equation}
\label{eq:egm}
    L(\mathbf{x_k},t_k) - L(\mathbf{x_k},t_k-\Delta t_k) = p_k C,
\end{equation}
where $p_k \in \{-1,+1\}$ is the sign (or polarity) of the brightness change, and $\Delta t_k$ is the time since the last event at the pixel $\mathbf{x}_k$.
The result is a sparse sequence of events that are asynchronously triggered by illumination changes.
To reconstruct the original intensity, we use the above event generation model as follows:
\begin{equation}
    L(\mathbf{x_k},t_k) = L(\mathbf{x_k},t_k-\Delta t_k) + \sum p_k C,
\end{equation}
Using the above equation, the intensity of the occluded pixels can be estimated under the condition that the contrast threshold $C$ and the position of the occlusion are known.
Thus, a model-based method requires segmenting the occluded pixels and estimating the contrast threshold.

We design such a method as a baseline for comparison and refer to it as \textit{Accumulation Method}.
The designed method assumes that the occluded pixels have similar intensities and uses this heuristic to segment the occluded pixels from the background.
However, this method breaks down in the presence of multiple overlapping occlusions, which leads to an occluded area in the image with different intensities.
For the estimation of the unknown contrast threshold $C$, as is commonly done, a reasonable value is defined for the event accumulation.

\subsection{Network Architecture}
\input{figures/fig_method_overview}
We design a learning-based method to tackle these deficiencies by implicitly learning the contrast threshold as well as detecting the occluded areas using the input frame and events.
The proposed network takes as input a single occluded image $I_0$ and $N$ event representations $E_{0\rightarrow \tau}, ..., E_{(N-1) \tau \rightarrow t}$, which are constructed in time intervals of $\tau$ based on the events recorded immediately after the image between timestep $0$ and $t$.
The proposed fully convolutional network leverages ConvLSTM layers~\cite{xingjian2015convolutional} to integrate the events and follows a U-Net structure~\cite{Ronneberger15icmicci} to predict a residual image $\tilde{I}_0$.
The residual image is added to the occluded input image to obtain the final occlusion-free intensity image $\hat{I}_0$, which should contain the true background information at the occluded areas of the input image.
The overview of our network is visualized in Fig.~\ref{fig:method_overview} (a).

As a first step, we integrate the event information in the \textit{Event Accumulation Module} (EAM) to retrieve accumulated intensity changes since the events capture instantaneous changes caused by the movement of the occlusions.
The Event Accumulation Module should focus on the intensity changes relevant to the background information, i.e., it should select the intensity information at the timesteps at which the background behind the occlusions becomes visible. 
To achieve this, we integrate the event information at multiple timesteps and let the network select the intensity change corresponding to the unoccluded background. This also helps the network to ignore events triggered by overlapping occlusions. \
We make use of the accumulation and gating mechanism of ConvLSTM layers to model the event integration mechanism, as well as the timestep selection.
Specifically, we apply two ConvLSTM layers in sequence to recurrently process the event representation starting with an initial zero-valued hidden and cell states.
The output of the Event Accumulation Module represents the last hidden state of the last ConvLSTM layer at the timestamp of the final event representation.

The integrated event features are then further encoded using a multi-scale \textit{Event Encoder} consisting of a convolution layer and a batch normalization layer with ReLU activation per scale to generate event features.
In a similar fashion, the occluded image is processed using a corresponding \textit{Frame Encoder}. In total there are $j{=}6$ scales.
Due to the differential principle of event cameras, we noticed that event features are highly beneficial for the reconstruction of textures, e.g., edges, but struggle to reconstruct uniform image areas.
In contrast, standard frames contain absolute intensities and thus are better suited for filling in uniform areas since, in the simplest case of constant pixel values, a border value copying suffices.
Because of this duality, we process event and frame features separately and employ a spatial as well as channel-wise weighting between them at each encoding scale $j$.
The spatial weighting is done in the \textit{Occlusion-aware Feature Fusion} (OFF) module, which computes gating weights $W^j$ based on the event features $F_E^j$ and frame features $F_I^j$ at the considered scale $j$ as well as the fused features from the previous scale $F_F^{j-1}$, which are downscaled by a strided convolution layer.
Specifically, the event and frame features are fused by weighting them with $W^j$ and $1-W^j$ and summing them up.
\begin{equation}
    \hat{F}_F^j = (1 - W^j) F_I^j + W^j F_E^j,
\end{equation}
Together with the fused feature of the previous scale, the weighted sum of the event and frame features is given as input to convolution layers outputting the fused features $F_F^{j}$.
The \textit{Occlusion-aware Feature Fusion} module is visualized in Fig.~\ref{fig:method_overview} (b).

Finally, the fused features at each scale are used as skip connections inside a \textit{Reconstruction Decoder}.
For upsampling, we use a bilinear interpolation followed by a convolution layer.
The output of the \textit{Reconstruction Decoder} is the residual image $\tilde{I}_0$, which is added to the input image $I_0$ and given to a sigmoid function resulting in the final occlusion-free image $\hat{I}_0$.
For more details about the network structure, we refer to the supplementary.








