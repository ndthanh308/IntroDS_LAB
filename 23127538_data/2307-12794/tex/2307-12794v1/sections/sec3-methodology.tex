\section{Dataset Production Workflow}\label{sec3-methodology}

In this section, we discuss the BIP! NDR dataset production workflow and we elaborate on the technical details of its various components. 
The source code of the production workflow is available as open source on GitHub\footnote{BIP! NDR: \href{https://github.com/athenarc/bip-ndr-workflow}{github.com/athenarc/bip-ndr-workflow}}.
A high-level overview of the workflow is depicted in Figure~\ref{fig1:pipeline-overview}.

% Figure environment removed

The main input to the workflow is the DBLP Corpus, which we use to collect URLs hosting Open Access manuscripts from the field of Computer Science, focusing on those that do not have a DOI. We collect these manuscripts so that we will be able to extract citations from the respective PDF files. 
DBLP~\cite{ley2002dblp,ley2009dblp} consolidates scholarly metadata from several open sources which cover the Computer Science field and is largely manually curated and frequently updated. 

As a result, this collection is ideal for our purposes. Our analysis shows that out of the approximately 320K open access conference publications, approximately 260K do not have a DOI. These publications are the ones that we aim to cover through the evolution of our dataset.
The current version of our dataset (v0.1)~\cite{Koloveas-bipndr} is based on the November 2022 Monthly Snapshot of DBLP~\cite{dblp-dataset}. The DBLP Corpus comes in XML format with all the bibliographic entries together in a single file. Therefore, as a first step, we use \texttt{dblp-to-csv}\footnote{dblp-to-csv: \href{https://github.com/ThomHurks/dblp-to-csv}{github.com/ThomHurks/dblp-to-csv}} to split the corpus into separate CSV files, grouped by publication type. We further process these CSV files to (a) extract manuscript metadata and store them in a document-oriented database, and (b) follow the included links to download the PDF files of Open Access papers. These operations ensure that the structured manuscript metadata from the DBLP Corpus are easily accessible to our workflow for querying and further processing.

For the next step of our workflow, we needed a tool to extract information from the PDF files while maintaining the headers, structure and sectioning of the manuscript. 
After a thorough evaluation of the literature regarding the tools used for reference extraction from PDFs, we concluded that based both on surveys~\cite{TCS2018,MJS2023}, and prominent works that required extensive bibliography parsing~\cite{LWN2020,NML2021}, GROBID~\cite{L2009} is currently the best tool for the task. GROBID converts the PDF files to the TEI XML publication format\footnote{TEI XML format: \href{https://tei-c.org/release/doc/tei-p5-doc/en/html/SG.html}{tei-c.org/release/doc/tei-p5-doc/en/html/SG.html}}. Apart from the PDF extraction capabilities, GROBID offers a consolidation option to resolve extracted bibliographical references using services like biblio-glutton\footnote{biblio-glutton: \href{https://github.com/kermitt2/biblio-glutton}{github.com/kermitt2/biblio-glutton}} or the CrossRef REST API\footnote{Crossref API: 
\href{https://www.crossref.org/documentation/retrieve-metadata/rest-api/}{crossref.org/documentation/retrieve-metadata/rest-api/}}.
We apply this consolidation option to our workflow, and GROBID sends a request to the Crossref web service~\cite{crossref} for each extracted citation. If a core of metadata (such as the main title and the first author) is correctly identified, the system retrieves the full publisher's metadata. These metadata are then used for correcting the extracted fields and for enriching the results. We utilize this output to potentially identify the DOI of a publication and attempt to match it with a DBLP entry.

The TEI XML files that GROBID produces are useful for identifying the structure of a manuscript, but are very verbose and are not convenient to process in large volumes. For that reason, we have created a \emph{TEIXML to JSON Converter} that turns the files into JSON format. This conversion process involves extracting relevant information from the TEI XML files and mapping it to the corresponding JSON structure. The resulting JSON files are smaller in size and are compatible with a wide range of tools for processing.

At this point, we have reached the core functionality of our workflow, the process of \emph{querying the DBLP metadata} for the bibliographic references of the papers in our collection. This process queries the manuscript metadata database for each document in the JSON Folder. For each document, we parse the reference list and we first check if a DOI exists in a publication entry. If it exists, we query our database based on the DOI. If a result is returned, we store the \texttt{dblp\_id}, the \texttt{doi}, as well as, the \texttt{bibliographic\_reference} extracted from the JSON file. Otherwise, we query based on the publication title. On a positive result, we store the previously mentioned fields to the dataset entry. If neither the publication title nor the DOI return a positive result, the publication could not be found in our DBLP metadata, so we store only the \texttt{doi} and \texttt{bibliographic\_reference} from the JSON file. This process ultimately creates the ``BIP! NDR'' collection which constitutes our dataset.

The final step involved using the \emph{mongoexport} utility to export the ``BIP! NDR'' collection from MongoDB into the final JSONL file.
The exported file served as the culmination of the dataset generation process, providing a structured collection of scholarly data ready for research and analysis.
