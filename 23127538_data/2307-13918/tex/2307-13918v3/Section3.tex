\section{Results}\label{sec:results}
% In this section, we analyse the posterior distributions obtained with SBI. We rely on the neural posterior estimation~\citep[NPE,][]{papamakarios2016fast, lueckmann2017flexible} algorithm to learn a normalizing-flow (NF) based conditional density estimator for each measurement of interest. Compared to other SBI algorithms, NPE is straightforward to train and directly enables evaluating and sampling from the distribution $p(\phi \mid \mathbf{x})$.  
Our experiments consider both in-silico and in-vivo scenarios.  
We split the simulation dataset from \cite{charlton2019modeling} into train ($70\%$), validation ($10\%$), and test sets ($20\%$) at random. All results reported are on the test set for the NPE models that maximize the validation likelihood, error bars report the standard deviation over five training instances. The dataset considered assumed various dependencies between age and most parameters. We prevent age to confound our analysis by explicitly conditioning both posterior and prior distributions on age, and averaging out age from our results. 
% Section~\ref{sec:methods} details further the NPE algorithm and the test metrics.

\subsection{In-silico analysis}\label{sec:in-silico}
% exemplified by successful inference of heart rate without the need of any real training data. However, there remains a non-negligible gap with real data which needs to be closed to allow accurate estimation of the remaining parameters of interest.
\paragraph{SBI enables comprehensive population-level uncertainty analyses.}

Figure~\ref{fig:identifiability_analysis} shows the average size of credible intervals extracted from various posterior distributions of the parameters of interest. With these results, we can compare the identifiability of several parameters given various measurement modalities and levels of noise. We can even inspect the information content of multiple measurement modalities at the same time, as shown in orange, for the Digital PPG and Radial APW. 
% A few examples of the simulated waveforms are provided in \appref{app:sample_generation}. 
To study the robustness of inverse solutions, we consider an additive Gaussian noise model with five noise levels. As a prerequisite for a meaningful analysis, we need to consider inference that is well statistically calibrated, which is the case here as observed in \appref{app:calib_mae}. 
% By construction, for a given measurement modality and noise level, the size of credible intervals~(SCI) with a credibility level equal to $95\%$ are larger than for $68\%$. 
Assuming consistent calibrations, observing the size of intervals as a function of SNR and comparing them to the intervals of the prior distribution quantifies how much information a measurement carries about the biomarkers, as discussed in \appref{app:MI_identifiability}. Section~\ref{sec:metrics} further motivates SCI as an effective metric to study the identifiability of parameters from posterior distributions.

% Overall, there is no unique best measurement. 
Unsurprisingly, the HR is easily identified from all measurements, except for very high levels of noise.
% As the observations are 8-second waveforms it is no surprise that. 
Overall, uncertainty about all parameters reduces significantly as the noise level decreases. This observation indicates that the measurements carry information about all parameters considered which is consistent with the findings of other studies~\citep{melis2017bayesian, charlton2019modeling, charlton2022assessing}. The results also highlight that each measurement has its unique information content. For instance, the digital PPG reveals more about SVR and PWV than the radial PPG. However, it is the opposite for the Diameter for which the Radial PPG is the most informative measurement. 

These results highlight that, similarly to standard sensitivity analyses, SBI enables interpretable assessment of the predictability of biomarkers from biosignals, in-silico, while having additional properties exemplified in subsequent experiments.
% In contrast to traditional sensitivity analyses, SBI directly provide
% and what they indicate about the ability to predict a (set of) parameter(s) given a measurement.

% Our analysis relies on a surrogate and not the true posterior distributions. Thus, some observations can be inconsistent. Theoretically, the size of credible intervals should decrease monotonically as a function of SNR and be upper bounded by the prior. Both aspects are sometimes violated in our results, e.g., in \figref{fig:identifiability_analysis} for HR at high noise levels and for the inference of SVR given the radial APW. Although improving this consistency is possible, e.g. by increasing the size of the training set or improving the conditional density estimators' inductive bias, this is not essential to obtain valuable insights from the simulators. For instance, let us assume the studied surrogate models are calibrated -- they usually are, as shown in \appref{app:calibration}. The measurement modality that provides the tightest credible intervals is the one that allows for the easiest extraction of information about the parameter, based on easily identifiable patterns for the considered class of models rather than numerical artefacts. This suggests that if similar patterns exist in real-world data and provide information about the parameter, a similar phenomenon can be expected, which would be aligned with conclusions drawn in-silico. Furthermore, selecting models on validation performance enforces that the posterior model relies on statistics that generalise to the validation set, rather than intricate numerical artefacts of the simulation. Analyses based on such statistics are also more likely to transfer to real-world data.

% Figure environment removed

\paragraph{SBI enables per-individual uncertainty quantification.}
\figref{fig:npe_vs_laplace} compares the estimation of uncertainty provided by NPE and Laplace's approximation~\citep{MacKay2003Information} around the expectation of the posterior distribution, which is representative of the underlying assumptions made in variance-based sensitivity analyses~(VBSAs). Similarly to VBSAs, Laplace's approximation models uncertainty through a second-order statistic over the population considered.
% \figref{fig:npe_vs_laplace} compares the estimation of uncertainty provided by the posterior distribution obtained with NPE and the uncertainty extracted by Laplace's approximation around the expectation of the posterior distribution. 
Compared to Laplace's approximation, NPE yields tighter and better calibrated credibility intervals. Laplace's intervals tend to be overconfident for measurements that lead to multi-modal posterior distributions, and they are underconfident when the posterior is uni-modal. 
% In comparison, NPE is better calibrated. 
% for all cases although not perfectly, which is expected as NPE is an approximation of the true posterior. 
Furthermore, a point estimator, even with Laplace's uncertainty estimation, will likely assign high density to low-density areas for certain observations, especially if the true posterior is multi-modal. Such inconsistent quantification of uncertainty may mislead downstream decisions. \appref{app:add_exp} showcases the $5$D posterior distributions corresponding to two test examples, which exhibit distinct uncertainty profiles and support further the necessity to use an expressive and observation-dependent quantification of uncertainty.

\figref{fig:two_populations} sketches the use of SBI to study the relationship between the digital PPG and the SVR and LVET. The figure highlights distinctive aspects of posterior distributions within the population studied for which we tested multi-modality~\citep{hartigan1985dip}. While the uncertainty about the value of SVR and LVET can be reduced substantially for approximately half of the test population, for the other half, the posterior is multi-modal. In addition, there remains a strong dependence between the two parameters for this second half. In practice, while a point estimator would be reasonable for the first sub-population, it would be a poor guess for the multi-modal sub-population. Multi-modality indicates that only specific sub-regions of the parameter space are credible, which may suffice to inform certain downstream tasks, e.g., detecting high risk zones, which does not necessarily require knowing the parameter's value exactly. The possibility to perform such fine-grained analysis is only possible with an accurate quantification of uncertainty at the individual level, as offered by SBI.

These results demonstrate that a consistent, multi-dimensional, and individualized representation of uncertainty, as obtained with NPE, yields essential insights from the hemodynamics simulator that are left unnoticed by VBSAs. Multi-modality and dependencies, as observed in \figref{fig:two_populations}, highlight the presence of symmetries in the forward model and individualized uncertainty profiles enable us to stratify the population based on this criterion.


\subsection{In-vivo analysis}\label{sec:in-vivo}
Models are never a perfect representation of real-world data~\citep{box1976science}. Misspecification, as it becomes more significant, hampers the practical relevance of insights extracted from a model~\citep {white1982maximum}. Thus, it is necessary to understand model misspecification to reason about the real world confidently~\citep{geweke2012prediction, box1976science}. Overcoming misspecification is most effectively achieved by identifying conclusions that are independent of the most critical sources of misspecification rather than aiming for perfect models, which do not exist. For instance, in this work, the simulated waveforms strongly depend on the shape of the boundary inflow condition at the aorta, which is approximated with a simplistic and idealized five-parameter descriptions, misspecified for most practical cases. Nevertheless, this description represents accurately the relationship between the length of a beat and the HR. Thus, insights that rely mainly on the beat length of the inflow waveform generalize well to real-world data. However, considering more complex aspects of the model, which reveal more unexpected and intricate relationships between parameters and simulated observations, increases misspecification and identifying definitive conclusions becomes challenging. We further discuss the problem of misspecification in Bayesian inference in Section~\ref{sec:misspecification_Bayesian} 

\paragraph{MIMIC-III results.}
In Figure~\ref{fig:MIMIC_results}, we assess the performance of surrogate posterior distributions learned from 1D simulations in predicting HR and LVET using 8-second waveforms from the MIMIC-III dataset~\citep{johnson2016mimic}. Examples of such waveforms are showcased in \appref{app:sample_generation}. As the posterior distributions are uni-modal for the LVET and HR (see, e.g., \appref{app:add_exp}), we focus on point estimates obtained by taking the expectation of the posterior distributions. While we can accurately determine HR by counting the number of beats, assessing LVET is more challenging as there is no gold standard method for obtaining it from PPG or APW. To address this, we use electrocardiograms from MIMIC-III and standard digital signal processing techniques to estimate LVET~\citep{dehkordi2019comparison, alhakak2021significance}. Although this estimation method is not perfect, it serves as a baseline for comparison. We evaluate the mean absolute error (MAE) and correlation between the point estimates and the labels. We must carefully take into account that HR and LVET are negatively correlated in healthy populations, and hence in the prior distribution considered in-silico. We prevent this potentially spurious correlation from corrupting our analysis by explicitly conditioning both prior and posterior distributions on HR and then averaging out the effect of HR.

We reduce prior misspecification (see discussion in Section~\ref{sec:misspecification_Bayesian}) by restricting our analysis to segments that fall within the support of the prior distribution, specifically $\text{HR} \in [ 60, 90]$ and  $\text{LVET} \in [ 230, 330]$. This filtering leaves 547 patients and one measurement per patient. In Figure~\ref{fig:MIMIC_results}, we observe successful transfer of posterior distributions to real-world data for HR but not for LVET. The MAE of LVET approaches that of the prior distribution, indicating limited improvement. However, the remaining correlation between the predicted and real LVET values suggest a partial transfer of information.

The uncertainty analysis in Figure~\ref{fig:identifiability_analysis} aligns with the in-vivo results, showing that HR estimation performs steadily well if SNR is higher than 5dB. At the same time, in-silico and in-vivo results are contradictory for the LVET. In-vivo, the best transfer occur at high noise levels, suggesting that the LVET effect is significantly misrepresented. Investigating and alleviating this misspecification with the appropriate modifications to the model might be crucial to successfully transferring findings from in-silico to in-vivo. This iterative process of \textbf{1.} model analysis, \textbf{2.} real-world experimentation, \textbf{3.} comparison with observations, and \textbf{4.} model refinement; exemplifies the scientific method. In-silico and in-vivo experiments demonstrate that SBI facilitates more scrutiny in applying the scientific loop to cardiovascular models relying on numerical simulations, in extracting scientific hypotheses from the model (step \textbf{1.}); and comparing theoretical predictions and real-world data (step \textbf{3.}).
% Our results demonstrate the value of SBI for mining scientific hypotheses from in-silico cardiovascular models. 
Furthermore, the posterior distributions obtained with SBI provide a multi-dimensional representation of uncertainty and enable to study insights both at the population and the individual level. These analyses provide insights that go beyond what uni-dimensional and population-aggregated uncertainty and identifiability analyses would conclude. 
% Finally, in Section~3\ref{sec:in-vivo}, results on real-world measurements support a critical discussion of how in-silico analyses can provide insights that generalise to the real world. 

% In summary, by carefully examining the posterior distributions learned from simulations onto real-world data, we can identify aspects of the model that effectively transfer in-vivo and those that require further refinement. If necessary, in-silico analyses may guide the resolution of such misspecification by informing when and which parameters are hinted identifiable by the misspecified model. The next step is to gather the corresponding set of real-world labelled data and resolve the misspecification on this set, e.g. by modifying the noise model. A consecutive uncertainty analysis will tell if these parameters are still identifiable under the new and better-specified model.


% By stratifying the population based on expected levels of error using the posterior distribution, we can validate the transferability of the learned posterior distributions. Additionally, an important aspect left for future work is to propose new noise models that produces posterior distributions whose uncertainty is well-calibrated on real-world signals.

% Figure environment removed
