\section*{Supplementary materials}
\subsection{In-vivo vs in-silico data}\label{app:sample_generation}
In this section we provide an overview of the generation of in-silico data, in \figref{fig:data_gen}, and a few examples of the real-world data considered from MIMIC, in \figref{fig:MIMIC}. We observe that real-world data contains degenerated beats. Moreover, another source of variation between beats comes from physiological parameter dynamics, which can vary from one beat to another. These observations motivate the introduction of noise on top of the deterministic simulation as discussed in the main paper and shown in \figref{fig:data_gen}.
% Figure environment removed

% Figure environment removed
\subsection{Supplementary results} 
In this section we provide additional perspectives on the learned surrogate models of the posterior distributions.
\subsubsection{Calibration and MAE}\label{app:calib_mae}
\figref{fig:MAE-calibration} presents the mean average precision of a point estimator obtained as the mean of the posterior distribution and the calibration of these posterior distributions. Most surrogate models trained with NPE are well calibrated. However, there remains a risk that a surrogate model is not well calibrated, such as observed for the Digital PPG for inferring some of the parameters for low levels of noise. The MAEs have a similar behavior as the average sizes of credible intervals at size $68\%$ and $95\%$ discussed in the main materials.
% Figure environment removed

\subsubsection{Posterior distributions}
\label{app:add_exp}
One desirable consequence of relying on SBI to analyze hemodynamics models is to provide access to the joint conditional distribution of parameters given an observation. In \figref{fig:digital_PPG_posterior}, we show the posterior distributions corresponding to two randomly selected simulated digital PPGs of the test set. These plots reveal how different measurements can lead to very different posterior distributions and highlight the relevance of considering sub-groups rather than the entire population at once.
In addition, \figref{fig:all_posteriors} presents the posterior distributions corresponding to the different measurement types studied. From this figure we observe that different measurements carry different information about the parameters and thus can lead to very different posterior distributions. Such plots may also indicate when multiple measurements should be done and when this is likely useless. 

% Figure environment removed

% Figure environment removed


\subsection{Metrics}
In this section, we provide the algorithms used to compute the calibration, in Algorithm~\ref{algorithm:statistical_calibration}, and size of credible intervals from samples of the surrogate posterior distributions, in Algorithm~\ref{algorithm:average_credible_interval}. 

\subsubsection{Calibration}\label{app:calibration}

Algorithm~\ref{algorithm:statistical_calibration} returns the distribution of minimum credibility levels required to not reject the true value of $\phi$. Under calibration, these values should be uniformly distributed -- we expect to reject falsely the true value with a frequency equal to the credibility level chosen. We report the integral (along credibility levels $\alpha$) between the observed cumulative distribution function~(CDF) of minimum credibility levels and the CDF of a uniform distribution. This metric equals $0$ under perfect calibration and is bounded by $0.5$. We report the calibration for each dimension independently as the metric does not generalize to multiple dimension.

\begin{algorithm}
\caption{Statistical Calibration of Posterior Distribution}
\label{algorithm:statistical_calibration}
\textbf{Input:} Dataset of pairs $\mathcal{D} = \{(\phi_i, x_i)\}$, Posterior distribution $p(\phi|x)$, Number of samples $N$.\\
\textbf{Output:} Distribution of minimum credibility levels.

\begin{algorithmic}[1]
\STATE Initialize an empty list $CredLevels$
\FOR{$(\phi_i, x_i) \in \mathcal{D}$}
    \STATE Initialize an empty list $Samples$
    \FOR{$i = 1$ to $N$}
        \STATE Sample $\phi_i$ from $p(\phi|x)$
        \STATE Append $\phi_i$ to $Samples$
    \ENDFOR
    \STATE Sort $Samples$
    \STATE Compute the rank (position in ascending order) $r$ of $\phi$ in $Samples$
    \STATE Set $CredLevel = \frac{r}{N}$
    \STATE Append $CredLevel$ to $CredLevels$
\ENDFOR
% \STATE Compute the distribution of minimum credibility levels based on $CredLevels$
% \RETURN Distribution of minimum credibility levels
\end{algorithmic}
\textbf{Return:} $CredLevels$.
\end{algorithm}

\subsubsection{Size of credible intervals}\label{app:SCI}
Algorithm~\ref{algorithm:average_credible_interval} describes a procedure to compute the size of credible intervals. In our experiments, we consider each dimension independently and discretize the space of value in $100$ cells. We finally report the average number of cells multiplied by the size of one cell in the physical unit of the parameter.
\begin{algorithm}
\caption{Compute Average Size of Credible Intervals}
\label{algorithm:average_credible_interval}
\textbf{Input:} Dataset of observations $x$, Posterior distribution $p(\phi|x)$, Credibility level $\alpha$
\textbf{Output:} Average size of credible intervals

\begin{algorithmic}[1]
\STATE Initialize an empty list $CredIntSizes$
\FOR{each observation $x$ in the dataset}
    \STATE Generate samples from the posterior distribution: $\phi_{\text{samples}} =$ SampleFromPosterior($p(\phi | x)$)
    \STATE Discretize the parameter space into cells
    \STATE Initialize an empty list $CellCounts$
    \FOR{each sample $\phi$ in $\phi_{\text{samples}}$}
        \STATE Increase by one the count the cell covering $\phi$
    \ENDFOR
    \STATE Sort the $CellCounts$ list in descending order
    \STATE Append the minimum number of cells required to reach the credible level $\alpha$ to $CredIntSizes$.
\ENDFOR
\STATE Compute the average size of credible intervals by taking the mean of the $CredIntSizes$ list

\end{algorithmic}
\textbf{Return:} Average size of credible intervals
\end{algorithm}
\subsubsection{Mutual information and SCI}\label{app:MI_identifiability}
% Figure environment removed
In our experiments, we gave the average size of credible (SCI) intervals rather than the mutual information, as the former quantity is expressed in physical units that have a direct interpretation to specialists. We now discuss how the SCI relates to mutual information.

We aim to drive this discussion in the context of comparing the quality of two distinct measurement processes for inferring one quantity of interest. Formally, we denote these two measurements by $\mathbf{x}_1 \in \mathcal{X}_1$ and $\mathbf{x}_2 \in \mathcal{X}_2$ and the quantity of interest by $\phi \in \Phi$. 

Assuming a fixed marginal distribution $p(\phi)$ over the parameter, the two measurement processes $p(\mathbf{x}_1\mid \phi)$ and $p(\mathbf{x}_2 \mid \phi) $ define two joint distributions $p(
\mathbf{x}_1, \phi)$ and $p(\mathbf{x}_2, \phi) $. Considering a discretized space of parameters $\phi$, the mutual information can be written as 
\begin{align}
    \mathcal{I}(\phi, \mathbf{x}_i) = \mathcal{H}(\phi) - \mathcal{H}(\phi \mid \mathbf{x}_i),
\end{align}
 where $\mathcal{I}(\phi, \mathbf{x}_i)$ is the mutual information and $\mathcal{H}$ the entropy. As the marginal entropy of the parameter remains constant, it is clear that only the second term matters for comparing the information content of the two measurement processes. The quantity $\mathcal{H}(\phi \mid \mathbf{x}_i)$ can be interpreted as the average remaining number of bits necessary to encode $\phi$ if we know $\mathbf{x}_i$. The average is taken over the joint distribution induced by the marginal distribution of $\phi$ and the measurement process $p(\mathbf{x}_i\mid \phi)$.

From this interpretation, choosing the one with the highest mutual information is a well-motivated criterion for choosing between two measurement processes. Said differently, we are looking for measurement processes with the smallest $\mathcal{H}(\phi \mid \mathbf{x}_i)$, the one leading to small uncertainty about the correct value of $\phi$.

We use an information theory point of view to explain why, similarly to mutual information maximization~\citep{gao2020reducing}, aiming for the measurement process with the smallest SCI is a sound approach. Let us consider the measurement process $p(\mathbf{x}_i \mid 
\phi)$ leading credible intervals with size $S(\alpha, \mathbf{x}_i)$ for a certain credibility level $\alpha$ and observation $\mathbf{x}_i$. Similarly to what we do in practice to compute the SCI, we discretize the space of parameters into $N$ cells. The SCI is then defined as the minimal number of cells required to cover the credible region at level $\alpha$. From the SCI, we can say that the true value of $\phi$ belongs to one of the $S(\alpha, \mathbf{x}_i)$ cells of the credible region with probability $\alpha$ or to one of the $N - S(\alpha, \mathbf{x}_i)$ remaining cells with a probability $1-\alpha$. From this, we can bound the average number of bits required to encode the true value of $\phi$ given the observation $\mathbf{x}_i$ as a function of the SCI $S$ and the credibility level $\alpha$ as
\begin{align}
    \text{N bits} \leq -\alpha  \log_2\frac{\alpha}{S(\alpha, \mathbf{x}_i)} - (1-\alpha) \log_2 \frac{1-\alpha}{N - S(\alpha, \mathbf{x}_i)}. \label{eq:bits_SCI}
\end{align}

\figref{fig:bits_SCI} shows the relationship between this bound and the credibility level $\alpha$ and SCI. We treat SCI and the credibility $\alpha$ as independent quantities, as different measurement processes can lead to different relationships between these two quantities. We must notice that given a credibility level $\alpha$, smaller $SCI$ corresponds to better bounds. We can conclude that selecting models with the smallest SCI for a given credibility level is a sound approach with a similar interpretation as making this choice based on mutual information. 

\subsection{Whole-body hemodynamics model}~\label{app:hemodynamics_model}
\subsubsection{Parameterization}
We use a dataset of $4374$ simulations from healthy individuals aged $25$ to $75$~\citep{charlton2019modeling}. By deriving APW and simulating PPG waveforms from the blood flow and pressure data, we obtain signals corresponding to a single heartbeat of varying lengths. The parameters of interest $\phi$ can be parameters of the forward model, such as HR, LVET, Diameter, or quantity that are derived from the simulation, such as PWV and SVR. In general, the parameters of interest and the observation do not constitute a one-to-one mapping. This is especially caused by the presence of additional parameters treated as nuisance $\psi$. In the dataset from \cite{charlton2019modeling}, the following parameters vary from one simulation to the other:
\begin{itemize}
    \item Heart function:
    \begin{itemize}
        \item Heart Rate~(HR).
        \item Stroke Volume~(SV).
        \item Left Ventricular Ejection Time~(LVET). \textit{Note:} LVET changes as a deterministic function of HR and SV.
        \item Peak Flow Time~(PFT).
        \item Reflected Fraction Volume~(RFV).
    \end{itemize}
    \item Arterial properties:
    \begin{itemize}
        \item $Eh = R_{d} (k_1 e^{k_2 R_d} + k_3)$ where $k1, k2$ are constant and $k3$ follows a deterministic function of age.
        \item Length of proximal aorta
        \item Diameter of larger arteries
    \end{itemize}
    \item Vascular beds
    \begin{itemize}
        \item Resistance adjusted to achieve mean average pressure~(MAP) distribution compatible with real-world studies.
        \item Compliance adjusted to achieve realistic peripheral vascular compliance~(PVC) compatible with real-world studies.
    \end{itemize}
\end{itemize}
The interested reader will find further details in \cite{charlton2019modeling}.
\subsubsection{Measurement model}
The dataset from \cite{charlton2019modeling} is made of individual beats, which differs from real-world data usually made of a fixed-size segments. While pre-processing the real-world data to extract unique beat is feasible, it may pose challenges to ensure this extraction is consistent with the simulated waves. Instead, we add a measurement model that reduce the gap between the real-world and simulated data formats.
 We first generate segments longer than 10 seconds by concatenating the same beat multiple times. Then, we randomly crop time series into 8-second segments. This ensures that the posterior distributions are defined for 8-second segments and accounts for all possible starting positions within the heartbeat. Finally, we introduce a white Gaussian noise to the waveforms to make our analysis less sensitive to the model misspecification. \appref{app:sample_generation} showcases these steps and the resulting waveforms.


\subsection{Normalizing flows} \label{app:NF}
We provide additional details on the normalizing flows used to model the posterior distributions. In all our experiments, we apply the same training and model selection procedures. Moreover we use the same neural network architecture for all experiments.

We rely on the open-source libraries PyTorch~\citep{NEURIPS2019_9015} and \href{https://github.com/AWehenkel/Normalizing-Flows}{Normalizing Flows}, a lightweight library to build NFs built upon the abstraction of NFs as Bayesian networks from \citep{wehenkel2021graphical}. 

\subsubsection{Training setup}
We randomly divide the complete dataset into $70\%$ train, $10\%$ validation, and $20\%$ test sets. We optimize the parameters of the neural networks with stochastic gradient descent on \eqref{eq:loss} with the Adam optimizer~\citep{kingma2014adam}. We use a batch size equal to $100$, a fixed learning rate ($= 10^{-3}$), and a small weight decay ($= 10^{-6}$). We train each model for $500$ epochs and evaluate the validation loss after each epoch. The best model based on the lowest validation loss was returned and used to obtain the results presented in the paper. All data are normalized based on their standard deviation and mean on the training set. For the time series, we compute one value across the time dimension. 

\subsubsection{Neural network architecture}
We use the same neural network architecture for all the results reported. It is made of a $3$-step autoregressive normalizing flow~\citep{papamakarios2017masked} combined with a convolutional neural network~(CNN) to encode the $8$-second segments sampled at $125Hz$ ($\in \mathbb{R}^{1000}$). The CNN is made of the following layers:
\begin{enumerate}
    \item 1D Convolution with no padding, kernel size $= 3$, stride $= 2$, $40$ channels, and  followed by ReLU;
    \item 1D Convolution with no padding, kernel size $= 3$, stride $= 2$, $40$ channels, and  followed by ReLU;
    \item 1D Convolution with no padding, kernel size $= 3$, stride $= 2$, $40$ channels, and  followed by ReLU;
    \item Max pooling with a kernel $=3$;
    \item 1D Convolution with no padding, kernel size $= 3$, stride $= 2$, $20$ channels, and  followed by ReLU;
    \item 1D Convolution with no padding, kernel size $= 3$, stride $= 2$, $10$ channels, and  followed by ReLU,
\end{enumerate}
leading to a $90$ dimensional representation of the input time series. The $90$-dimensional embedding is concatenated to the $age$ and denoted $\mathbf{h}$. Then, $\mathbf{h}$ is passed to the NF as an additional input to the autoregressive conditioner~\citep{papamakarios2017masked, wehenkel2021graphical}. 

The NF is made of a first autoregressive step that inputs both the $91$ conditioning vector $\mathbf{h}$ and the parameter vector and outputs $2$ real values $\mu_i(\phi_{<i}, \mathbf{h}), \sigma_i(\phi_{<i}, \mathbf{h}) \in \mathbb{R}$ per parameter in an autoregressive fashion. Then the parameter vector is linearly transformed as $u_i = \phi_i e^{\sigma_i(\phi_{<i}, \mathbf{h})} + \mu_i(\phi_{<i}, \mathbf{h})$. The vector $\mathbf{u} := [u_1, \dots, u_k]$ is then shuffled and passed through 2 other similar transformations, leading to a vector denoted $\mathbf{z}$, which eventually follows a Gaussian distribution after learning~\citep{papamakarios2021normalizing}. The $3$ autoregressive networks have the same architecture: a simple masked multi-layer perceptron with ReLu activation functions and $3$ hidden layers with $350$ neurons each. We can easily compute the Jacobian determinant associated with such a sequence of autoregressive affine transformations on the vector $\phi$ and thus compute \eqref{eq:loss}. 

We can easily show that the Jacobian determinant is equal to the product of all scaling factors $e^{\sigma_i}$. We also directly see that ensuring these factors are strictly greater than $0$ enforce a continuously invertible Jacobian for all value of $\phi$ and thus continuous bijectivity of the associated transformation.

As mentioned, under perfect training, the mapping from $\Phi$ to $\mathcal{Z}$ defines a continuous bijective transformation that transforms samples from $\phi \sim p(\phi \mid \mathbf{h})$ into samples $\mathbf{z} \sim \mathcal{N}(0,I)$. As the transformation is bijective, we can sample from $p(\phi \mid \mathbf{h})$ by inverting the transformation onto samples from $\mathcal{N}(0,I)$. As the transformation is autoregressive, we can invert it by doing the inversion sequentially for all dimensions as detailed in \citep{papamakarios2021normalizing, wehenkel2021graphical, papamakarios2017masked}. 
