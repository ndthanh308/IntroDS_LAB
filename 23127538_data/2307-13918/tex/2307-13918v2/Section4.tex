\section{Materials \& Methods} \label{sec:methods}
% This section details the hemodynamics numerical simulation and formally the NPE algorithm and the metrics used to evaluate the inference results.

\subsection{1D hemodynamics of the human arterial network} \label{sec:model}
The full-body arterial model introduced in \cite{alastruey2012arterial}, on which \cite{charlton2019modeling} relies, describes the arterial pulse wave propagation into $116$ arterial segments, making up the largest arteries of the thorax, limbs, and head. This model is a good compromise between faithfulness to the real-world system and complexity~\citep{alastruey2023arterial}. It enables forward simulating APWs and PPGs at multiple locations, given a set of physiological parameters describing the geometrical and physical properties of the cardiovascular system. Running a simulation takes a few minutes on any standard CPU~\citep{melis2017bayesian}, allowing \cite{charlton2019modeling} to release a dataset of $4374$ simulated healthy individuals aged $25$ to $75$.

Compared to 3D and 0D models, 1D models offer a better balance between expressivity and efficiency. While 1D simulations may be less accurate than 3D models (e.g., they cannot model atherosclerosis as they do not consider wall shear stress), they trade a modest and well-studied decrease in accuracy against much lighter simulation costs~\cite{xiao2014systematic, alastruey2023arterial}. Furthermore, the tractable parameterization and efficient simulation of 3D whole-body hemodynamics remain two open research questions~\citep{pegolotti2023learning, lasrado2022silico}. 
On the other side of the CV modeling spectrum, 0D simulations~\citep{john2004forward, shi2011review} rely on a lumped-element model to describe the relationship between blood flow at one location (e.g., left ventricle outflow) and blood pressure and flow at other locations. In addition to ignoring significant physical effects such as wave propagation and reflection, 0D models are partially parameterized by non-physiological quantities. Generating a representative population, such as the one considered in \cite{charlton2019modeling}, can thus be challenging with these models.

\paragraph{Model description.}
In \cite{alastruey2012arterial}, the authors consider the compartmentalized arterial model made of the following sub-models: \textbf{1.} the heart function; \textbf{2.} the arterial system; \textbf{3.} the geometry of arterial segments; \textbf{4.} the blood flow; and \textbf{5.} the vascular beds. \textit{The heart function} describes the blood volume along time at the aorta as a five-parameter function. \textit{The arterial system} is described as a graph, the heart is the parent root, and then arteries branch out into the body. Every branch of the network represents an arterial segment. Segments are coupled so that the conservation of mass and momentum hold in the complete system. Additionally, the heart function defines the boundary condition on the parent root of the arterial network. The vascular bed describes the boundary condition on the leaf nodes. \textit{The geometry of arterial segments} assumes the segments are axial-symmetric and tapered tubes. Hence, the geometry of each arterial segment can be described using 1D parameters such as radius and thickness of the arterial wall. \textit{The blood flow} in the 1D segments follows fluid dynamics, which depends on the geometry and visco-elastic properties of the arterial wall. \textit{The vascular beds} are modeled using 0D approximations, i.e., the geometrical description is being lumped into a space-independent parametric transfer function. 



% Following this high-level overview of the 1D hemodynamics models, we now provide some mathematical details on the core components. 
The main state parameters of whole-body 1D hemodynamics models are the volumetric flow rate $Q(z,t)$, the blood pressure $P(z, t)$, and the vessel cross-sectional area $A(z,t)$ at axial position $z$ and time $t$, in each artery considered. Based on the conservation of mass and momentum, one can derive the partial differential equations (PDEs)
\begin{align}
    \frac{\partial A}{\partial t} + \frac{\partial Q}{\partial z} &= 0 \\
    \frac{\partial Q}{\partial t} + \frac{\partial }{\partial z} \left(  \alpha \frac{Q^2}{A}\right) + \frac{A}{\rho} \frac{\partial P}{\partial z} &= -2 \frac{\mu}{\rho} (\gamma_\nu +2)\frac{Q}{A},
\end{align}
where $\alpha$ is the Coriolis' coefficient, $\mu$ is the blood dynamic viscosity, and $\gamma_\nu$ is a parameter defining the shape of the radial velocity profile. A third relationship of the arterial wall mechanics relates pressure and cross-section area as
\begin{align}
    P(A) = P_{ext} + \beta \left( \sqrt{A} - \sqrt{A_0}\right)
    + \frac{\Gamma}{\sqrt{A}} \frac{\partial A}{\partial t}, \\
     \text{where } \beta = \frac{4}{3} \frac{\sqrt{\pi} Eh_0}{A_0} \text{ and } \Gamma = \frac{2}{3} \frac{\sqrt{\pi} \varphi h_0}{A_0}
\end{align}
respectively denote the elastic and viscous components of the Voigt-type visco-elastic tube law,
$P_{ext}$ is the reference pressure at which the geometry is described by the cross-sectional area $A_0$ and thickness of the arterial wall $h_0$. The elastic modulus $E$ and wall viscosity $\varphi$ characterize the mechanical properties of the wall. In addition to these PDEs, boundary conditions are formulated by coupling each artery segment with the parents and children in the arterial network. For further details, see \cite{Melis2017, charlton2019modeling, alastruey2012arterial}.


The considered 1D hemodynamics model constitutes a complex simulator with many parameters. As described in Section~\ref{sec:SBI-4-CV}, only a subset of these parameters are of direct interest. Other parameters are considered nuisance effects. In addition, we consider a measurement model that generates biosignals similar to the one in MIMIC-III. \appref{app:hemodynamics_model} provides additional details on the parameters distributions and the measurement model considered.

% Some parameters are constant across the population, e.g., the blood viscosity $\mu$ or the Poisson ratio $\nu$. Others, such as the one describing the geometric properties of each arterial segment can vary across subjects. \appref{app:model_parameters} describes the parameters treated as nuisance parameters. It also 
% Overall we can leverage the concept of parameters of interest $\phi$ and nuisance parameters $\psi$ from theoretical physics (see Section~4\ref{sec:SBI}) to structure the parameter space. In this work, this structure is guided by selecting parameters $\phi$ which are relevant to describe the coarse health of the cardiovascular system. 


% \paragraph{Measurements modelling.} We utilise a dataset of $4374$ simulations from healthy individuals aged $25$ to $75$~\citep{charlton2019modeling}. By deriving APW and simulating PPG waveforms from the blood flow and pressure data, we obtain signals corresponding to a single heartbeat of varying lengths. To standardise the analysis, we first generate segments longer than 10 seconds by concatenating the same beat multiple times. Then, we randomly crop time series into 8-second segments. This ensures that the posterior distributions are defined for 8-second segments and accounts for all possible starting positions within the heartbeat. Finally, we introduce a white Gaussian noise to the waveforms to make our analysis less sensitive to the model misspecification. \appref{app:sample_generation} showcases these steps and the resulting waveforms.

\subsection{Simulation-based inference} \label{sec:sbi}
\label{sec:SBI}
% A simulator is a computer program --- a forward generative process $g: \Theta \rightarrow \mathcal{X}$ --- that inputs a vector of parameters $\mathbf{\theta} \in \Theta$ and returns a simulation $\mathbf{x} \in \mathcal{X}$. Faithful simulators are usually stochastic, for instance when they combine together a noise model that represents measurement errors with a deterministic mechanistic model that relates parameters to measurable quantities, such as the PDE model discussed before. Another common source of randomness are nuisance parameters. Indeed, we often distinguish between the parameters of interest $\mathbf{\phi} \in \Phi \subset \Theta $ and the nuisance parameters $\mathbf{\psi} \in \Psi \triangleq \Theta \setminus \Phi$ that are necessary to run the simulations but not of direct interest for downstream tasks. We usually aim to marginalise out the effect of those nuisance parameters by randomising their value following a marginal distribution $p(\mathbf{\psi})$, which transmits uncertainty to the simulation output $\mathbf{x}$ given a parameter of interest $\mathbf{\phi}$. These multiple sources of randomness call for a statistical treatment of inference over simulations~\citep{cranmer2020frontier, brehmer2020mining, cranmer2015approximating}. 

% We can abstract all sources of randomness within the nuisance parameters $\psi$ (e.g., even including the stochasticity of the noise model) such that the function $g: \Psi \times \Phi \rightarrow \mathcal{X}$ becomes deterministic. The deterministic simulator together with the sampling of nuisance parameters implicitly defines a likelihood function over the parameters of interest as 
% $p(\mathbf{x} \mid \phi) = \int \delta_{\mathbf{x}}\bigl( g\left(\psi, \phi \right) \bigr) p(\psi) d \psi, $ where $\delta_{\mathbf{x}}$ is the Dirac delta distribution centred at $\mathbf{x}$. 
% In most cases, simulators allow sampling from the corresponding distribution $p(\mathbf{x} \mid \phi)$, but not evaluating it, as the latter operation requires nuisance parameters marginalisation, which is intractable in most cases. The apparent intractability of the likelihood function makes statistical inference over such models challenging.
% In this context, SBI provides efficient tools to perform  likelihood-free statistical inference.


% P1 defines what is SBI
% P2 defines normalizing flows to model distributions
\paragraph{Neural Posterior Estimation (NPE).}
% We take a Bayesian approach and assume that a well-motivated prior distribution $p(\phi)$ over the parameters is given. We rely on NPE~\citep{papamakarios2016fast, lueckmann2017flexible}, a popular SBI algorithm. 
As mentioned in Section~\ref{sec:SBI-4-CV}, NPE~\citep{papamakarios2016fast, lueckmann2017flexible} is a Bayesian and amortized SBI algorithm. It trains a parametric conditional density estimator for the parameters of interest $p_{\omega}(\phi \mid \mathbf{x})$ on a dataset $\mathcal{D} := \{(\phi_i, \mathbf{x}_i)\}_{i=1}^N$ of samples from the joint distribution $p(\phi, \mathbf{x}) =  \int p(\phi, \psi) p(\mathbf{x}\mid \phi, \psi) d\psi$. In this work, we rely on a rich class of neural density estimators called normalizing flows~\citep[NF, ][]{tabak2010density, tabak2013family, rezende2015variational, kobyzev2020normalizing, papamakarios2021normalizing}, from which both density evaluation and sampling is possible. 

% The NPE algorithm optimises the parameters of the neural density estimator with stochastic gradient descent on the Kullback-Leibler divergence between the conditional density estimator and the posterior distribution. 
Given an expressive class of neural density estimators $\{p_\omega(\phi \mid \mathbf{x}): \omega \in \Omega\}$, NPE aims to learn an amortized posterior distribution $p_{\omega^\star}(\phi \mid \mathbf{x})$ that works well for all possible observation $\mathbf{x} \in \mathcal{X}$, by solving
\begin{align}
    % & \omega^\star \in \arg\min_{\omega \in \Omega} \mathbb{KL}\left[ p(\phi \mid \mathbf{x}) \parallel p_\omega(\phi \mid \mathbf{x}) \right] \quad \forall \mathbf{x} \in \mathcal{X}  \label{eq:goal_NPE}\\
     & \omega^\star \in \arg\min_{\omega \in \Omega} \mathbb{E}_{\mathbf{x}} \left[ \mathbb{KL}\left[ p(\phi \mid \mathbf{x}) \parallel p_\omega(\phi \mid \mathbf{x}) \right] \right] \label{eq:expectation}\\ 
     \iff & \omega^\star \in \arg\min_{\omega \in \Omega} \int p(\mathbf{x}) p(\phi \mid \mathbf{x}) \left[ \log \frac{p(\phi \mid \mathbf{x})}{ p_\omega(\phi \mid \mathbf{x})} \right] d\mathbf{x} d\phi\\
    \iff  & \omega^\star \in \arg\max_{\omega \in \Omega} \int p(\mathbf{x}) p(\phi \mid \mathbf{x}) \log p_\omega(\phi \mid \mathbf{x}) d\mathbf{x} d\phi\\
    \iff  & \omega^\star \in \arg\max_{\omega \in \Omega} \mathbb{E}_{(\phi, \mathbf{x})} \left[ \log p_\omega(\phi \mid \mathbf{x}) \right]. \label{eq:objective_NPE}
    \end{align}
% \eqref{eq:goal_NPE} indicates the goal of NPE: learning a surrogate $p_{\omega^\star}$ that equals the posterior distribution for all values $\mathbf{x} \in \mathcal{X}$. Under the assumption that the class of functions considered contains the true posterior $p(\phi\mid \mathbf{x})$, the minimisers $\omega^\star$ of \eqref{eq:goal_NPE} and \eqref{eq:expectation} are the same. 

In practice, NPE approximates the expectation in \eqref{eq:objective_NPE} with an empirical average over the training set $\mathcal{D}$ and relies on stochastic gradient descent to solve the corresponding optimization problem. Assuming $\phi \in \mathbb{R}^k$ and unpacking the evaluation of the NF-based conditional density estimator, the training loss is
\begin{align}
    \ell(\mathcal{D}, \omega) 
    % &= \frac{1}{N}\sum_{i=1}^N \log p_{\omega}(\phi_i \mid \mathbf{x}_i)\\
    &= \frac{1}{N}\sum_{i=1}^N \log p_z\biggl(f_{\omega}\bigl(\phi_i; \mathbf{x}_i\bigr)\biggr) + \log \lvert J_{f_\omega}(\phi_i; \mathbf{x}_i) \rvert, \label{eq:loss}
\end{align}
following from the change-of-variables theorem~\citep{tabak2013family}.
The symbol $p_z$ denotes the density function of an arbitrary $k$-dimensional distribution (e.g., an isotropic Gaussian), $f_\omega:\mathbb{R}^k \times \mathcal{X}\rightarrow \mathbb{R}^k$ denotes a continuous function invertible for its first argument $\phi$, parameterized by a neural network, and $\lvert J_{f_\omega} \rvert$ denotes the absolute value of the Jacobian's determinant of $f_\omega$ with respect to its first argument. In addition to density evaluation, as in \eqref{eq:loss}, the NF enables sampling from the modeled distribution by inverting the function $f_\omega$. 

In our experiments, we combine a convolutional neural network encoding the observations $\mathbf{x}$ with a three-step autoregressive affine NF~\citep{papamakarios2017masked} which offers a good balance between expressivity and sampling efficiency as demonstrated in \cite{wehenkel2020you}. These models have an inductive bias towards simple density functions~\citep{verine2023expressivity}, which support that the multi-modality and diversity of posterior distributions observed in the population is not an artifact of our analysis but follows from the 1D cardiovascular model and prior considered. We provide additional details on the parameterization of $f$ and the sampling algorithm in \appref{app:NF}.

\paragraph{Uncertainty analysis with SBI.} \label{sec:metrics}
Uncertainty analysis~\citep{sacks1989design, hespanhol2019understanding} regards identifiability as a continuous attribute of a model which allows ranking models by how much information the modeled observation process carries about the parameter of interest. We move away from the classical notion of statistical identifiability -- convergence in probability of the maximum likelihood estimator to the actual parameter value -- because this binary notion is not always relevant in practice and mainly applies to studies in the large sample size regime. In contrast, uncertainty analysis directly relates to the mutual information between the parameter of interest and the observation as expressed by the model considered. It captures that biased or noisy estimators are informative and may suffice for downstream tasks. 

As is standard in Bayesian uncertainty analyses, we look at credible regions $\Phi_\alpha(\mathbf{x})$ at different levels $\alpha$, which are directly extracted from the posterior distribution $p(\phi \mid \mathbf{x})$. Formally, a credible region is a subset, $\Phi_\alpha$, of the parameter space $\Phi$ over which the conditional density integrates to $\alpha$, i.e., $\Phi_\alpha: \int_{\phi \in \Phi_\alpha(\mathbf{x})} p(\phi \mid \mathbf{x}) \text{d}\phi = \alpha, \Phi_\alpha \subseteq \Phi$. In this paper, we consider the smallest covering union of regions, denoted by $\Phi_\alpha$, which is always unique in our case and in most practical settings. 

\paragraph{Size of credible intervals (SCI).}
We rely on the SCI to shed light on the uncertainty of a parameter given a measurement process. 
The SCI at a level $\alpha$ is the expected size of the credible region at this level: $\mathbb{E}_{\mathbf{x}}[ \| \tilde{\Phi}_{\alpha}(\mathbf{x}) \| ]$, where $\| \cdot \|$ measures the size of a subset of the parameter space. In practice, we split the parameter space into evenly sized cells and count the number of cells belonging to the credible interval, as detailed in \appref{app:SCI}. As discussed in \appref{app:MI_identifiability}, there exists a relationship between SCI and mutual information~(MI). However, SCI is easier to interpret for domain experts than MI, as the former is expressed in the parameter's units. In addition, SCI is robust to multi-modality in contrast to point-estimator-based metrics (e.g., mean squared/absolute error) that cannot discriminate between two posterior distributions if they lead to the same point estimate. 


\paragraph{Calibration.}
Given samples from the joint distribution $p(\phi, \mathbf{x})$, credible intervals are expected to contain the true value of the parameter at a frequency equal to the credibility level $\alpha$, that is, $\mathbb{E}_{p(\phi, \mathbf{x})} \left[\mathbb{1}_{\Phi_\alpha}(\phi) \right] = \alpha $, where $\mathbb{1}$ is the indicator function. In this work, we do not have access to the true posterior but a surrogate $\tilde{p}$ of it. Hence, the coverage property of credible regions, which support the interpretation of uncertainty, may be violated, even when the forward model and the prior accurately describe the data. The calibration $C(\tilde{p}(\phi \mid \mathbf{x}), \mathcal{D})$ of a surrogate posterior $\tilde{p}$ is a metric, computed on a set $\mathcal{D}:=\{(\phi_j^\star, \mathbf{x_j})\}_{j=1}^N$, that measures whether the surrogate's credible regions respect coverage. We compute calibration as
$$
C(\tilde{p}(\phi \mid \mathbf{x}), \mathcal{D}) = \frac{1}{k}\sum_{i=1}^k \bigg| \frac{i}{k} - \frac{1}{N}\sum_{j=1}^N  \mathbb{1}_{\tilde{\Phi}_{\frac{i}{k}}}(\phi^\star_j(\mathbf{x}_j)) \bigg|,
$$
where $\tilde{\Phi}_{\frac{i}{k}}(\mathbf{x}_j)$ is the credible region at level $\alpha=\frac{i}{k}$ corresponding to the surrogate posterior distribution $\tilde{p}(\phi \mid \mathbf{x}_j)$.
The calibration directly relates to how much the surrogate posterior model violates the coverage property over all possible levels $\alpha \in \left] 0, 1 \right]$. \appref{app:calibration} describes the computation of calibration for NF-based surrogate posterior distributions.

\subsection{Model misspecification in Bayesian inference}\label{sec:misspecification_Bayesian}
Bayesian methods approach misspecification by separating the model, thus its misspecification, into two distinct components: 1) the prior distribution $p(\phi)$ and 2) the likelihood $p(\mathbf{x} \mid \phi)$. 
This division enables a focused examination of the misspecification in each component separately.

\paragraph{Prior misspecification.}
The prior distribution used to generate artificial data corresponds to a healthy population. In contrast, we consider real-world records of patients at intensive care units (ICUs) from the MIMIC~\citep{johnson2016mimic} dataset. Although the gap between the parameter distribution of the two populations is non-negligible, prior misspecification do not impair all conclusions drawn from the inspection of a model. Prior misspecification becomes insignificant if the prior support contains the real-world population or the observation carries a lot of information about the parameter -- that is, if the model's likelihood function is sharp. Thus, results gathered from the analysis of the likelihood function, e.g., by comparing prior and posterior distributions such as in the uncertainty analysis of \figref{fig:identifiability_analysis}, may transfer to real-world insights even under prior misspecification.


\paragraph{Likelihood misspecification.}
A second source of misspecification, arguably the most challenging one to overcome in practice, comes from the incorrectness of the likelihood function $p(\mathbf{x} \mid \phi)$, describing the generative process mapping parameters to observations. Common sources of misspecification include numerical approximations and simplifying modeling assumptions. Although each misspecification is unique, a common strategy is to represent the misspecification as an additional source of uncertainty and to add a noise model $p(\tilde{\mathbf{x}} \mid \mathbf{x})$ to account for it. When the initial model misspecification is small, simple noise models are sufficient, e.g., an additive Gaussian noise with constant variance. When the model does not accommodate substantial aspects of the real-world, however, designing the noise model is challenging. Moreover, adding noise has consequences as insights obtained from noisy observations must, then, rely on features that are unaltered by the noise considered. As noise increases, the number of such features shrinks. Thus, there is a balance between the amount of noise, which improves the robustness to misspecification, and the ability to obtain insights relying on complex relationships between the parameters and observations as described by the original model. As we are agnostic of the most appropriate noise model, we have considered various levels of noise in our experiments.

