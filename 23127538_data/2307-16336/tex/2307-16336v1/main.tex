\documentclass[letterpaper]{article} 
\usepackage[hyphens]{url}  
\usepackage{graphicx} 
\usepackage[round]{natbib}
\bibliographystyle{plainnat}

\title{Anatomy of an AI-powered \\ malicious social botnet}

\author{
    Kai-Cheng Yang\thanks{Corresponding author; email: yangkc@iu.edu} $\,$ and Filippo Menczer \\
    Observatory on Social Media \\ Indiana University, Bloomington
}

\date{}

\begin{document}

\maketitle

\begin{abstract}
Large language models (LLMs) exhibit impressive capabilities in generating realistic text across diverse subjects.
Concerns have been raised that they could be utilized to produce fake content with a deceptive intention, although evidence thus far remains anecdotal.
This paper presents a case study about a Twitter botnet that appears to employ ChatGPT to generate human-like content.
Through heuristics, we identify 1,140 accounts and validate them via manual annotation.
These accounts form a dense cluster of fake personas that exhibit similar behaviors, including posting machine-generated content and stolen images, and engage with each other through replies and retweets.
ChatGPT-generated content promotes suspicious websites and spreads harmful comments.
While the accounts in the AI botnet can be detected through their coordination patterns, current state-of-the-art LLM content classifiers fail to discriminate between them and human accounts in the wild.
These findings highlight the threats posed by AI-enabled social bots.
\end{abstract}

\section{Introduction}

Large language models (LLMs) can generate human-like text~\citep{jakesch2023human} and excel in various natural language processing tasks, including sentiment analysis and text summarization~\citep{ye2023comprehensive,qin2023chatgpt}.
Such capabilities have opened up numerous potential applications~\citep{bahrini2023chatgpt}, such as enhancing education~\citep{learning2023kasneci} and providing responses to medical questions~\citep{ayers2023comparing}.
Consequently, these tools have gained significant traction in the market.
For instance, ChatGPT, a prominent LLM, amassed over 100 million users in only two months.\footnote{reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01 (Accessed May 2023)}
Industry giants like Microsoft have integrated ChatGPT into their products,\footnote{blogs.microsoft.com/blog/2023/02/07/reinventing-search-with-a-new-ai-powered-microsoft-bing-and-edge-your-copilot-for-the-web (Accessed May 2023)} and Google swiftly followed suit by offering similar solutions.\footnote{blog.google/products/search/generative-ai-search (Accessed May 2023)}

At the same time, researchers have warned about the potential misuse of such technologies to generate mis/disinformation and harmful content~\citep{bommasani2021opportunities,yamin2021weaponized,guembe2022emerging,goldstein2023generative}.
Recent studies demonstrate that models like the GPT series are capable of producing news articles indistinguishable from human-generated ones~\citep{kreps2022news,jakesch2023human} and vast amounts of compelling mis/disinformation with little human involvement~\citep{buchanan2021truth,spitale2023ai}.
Detecting such content poses challenges for existing automated detection models~\citep{zhou2023synthetic}.
LLMs can also be leveraged to scale up personalized attacks, such as spear phishing content~\citep{hazell2023large}. 
The widespread availability of powerful language models will also significantly reduce the costs associated with content generation and lower the technical proficiency required to conduct such operations~\citep{brundage2018malicious,weidinger2022taxonomy}.
However, existing studies mainly consist of predictions and lab experiments.
To date, evidence of LLMs being deployed in the field for malicious purposes remains largely anecdotal.

In this paper, we present a case study about a Twitter botnet that appears to use ChatGPT to generate harmful content.
Social bots are social media accounts controlled in part by software and have been around for many years~\citep{ferrara2016rise}.
They were found to distort online conversations and spread misinformation in various contexts, from elections to public health crises~\citep{shao2018spread,ferrara2020characterizing,jamison_malicious_2019,marlow_twitter_2020}.
Traditional social bots often follow pre-defined instructions to perform simplistic tasks, such as spamming~\citep{yang2019arming}, following others, and amplifying certain narratives~\citep{keller2020political}.
They typically lack the intelligence to create realistic personas, post convincing content, or carry out natural conversations with other accounts automatically~\citep{assenmacher2020demystifying}.
However, the recent advancements in and wide adoption of LLMs completely transform this landscape.
Adversarial actors can now easily leverage language models to significantly enhance the capabilities of bots across all dimensions.

The bot accounts in our analytical sample were identified by self-revealing tweets they posted by accident.
A combination of heuristics and manual annotation yields 1,140 accounts in what we call the ``fox8'' botnet.
An in-depth analysis of behaviors displayed by the accounts in this botnet shows that they form a dense social network by following each other.
They post machine-generated content and steal selfies to create fake personas.
They also frequently interact with each other through retweets and replies.
A closer look at the self-revealing tweets suggests that the ChatGPT-generated content aims to promote suspicious websites and spread harmful comments.
We apply state-of-the-art LLM-content detectors and find they cannot effectively distinguish between human and LLM-powered bots in the wild.

Our work unveils the emergence of LLM-powered social bots and highlights the threats they pose.
By focusing on a real-world botnet, we provide valuable insights into how LLMs are leveraged by adversarial actors in the field.
Given the rapid advancements in AI technologies, we anticipate the proliferation of more advanced bot accounts across social media, serving diverse purposes.
We hope to raise public awareness about this issue and share the botnet data to allow the research community to investigate further.

\section{Related work}

\subsection{LLM-powered cyber threats}

Machine-generated content has long been implicated in cyber-social threats, such as phishing, mis/disinformation, and harmful content~\citep{crothers2022machine}.
These threats have been further exacerbated by LLMs for two main reasons.
First, LLMs beat traditional text generation methods in producing human-like text~\citep{jakesch2023human,guo2023close,clark2021all}.
This enhancement enables them to craft compelling and personalized content for previously unseen attacks~\citep{buchanan2021truth,spitale2023ai,hazell2023large}.

Second, powerful LLMs have become readily accessible, affordable, and user-friendly.
For instance, OpenAI provides API access to their models, enabling users to generate large volumes of content at a nominal expense.\footnote{openai.com/blog/introducing-chatgpt-and-whisper-apis (Accessed May 2023)}
Interaction with most LLMs happens via human language prompts~\citep{liu2023pre}, enabling even those without technical skills to harness the models' capabilities. 
Users can also acquire knowledge on API queries and LLM prompting directly from the models themselves~\citep{hazell2023large}.
The availability of open-source LLMs offers technical users greater flexibility to train, customize, and implement models according to their needs~\citep{yang2023harnessing}.

Therefore, LLMs have the potential to reshape the landscape of cyber-social security dramatically, a concern shared by many researchers~\citep{bommasani2021opportunities,yamin2021weaponized,guembe2022emerging,goldstein2023generative,kucharavy2023fundamentals}.
However, empirical evidence of such abuse in the field is rare.
One example comes from Hanley et al., who analyze machine-generated text in news media outlets and find a significant surge in such content after the release of ChatGPT, particularly on low-credibility websites~\citep{hanley2023machine}.
Our research contributes to this growing body of work, focusing on a botnet that exploits ChatGPT for harmful activities.

\subsection{LLM-generated content detection}

The potential misuse of LLMs necessitates the development of reliable methods to detect LLM-generated content~\citep{crothers2022machine}.
Existing strategies can be broadly classified into black-box and white-box approaches~\citep{tang2023science}.
Back-box detection methods are often framed as binary classification problems where classifiers are trained on texts generated by humans and machines.
The goal is to identify the characteristics of machine-generated content, such as statistical anomalies~\citep{gehrmann2019gltr,mitchell2023detectgpt} and linguistic patterns~\citep{guo2023close,frohling2021feature}.
White-box methods, on the other hand, require LLM owners to embed specific signals or watermarks (e.g., altered word frequencies) into generated content for subsequent identification~\citep{kirchenbauer2023watermark,zhao2023protecting}.

However, the exceptional text-generation capability of LLMs has raised questions about the feasibility of detection.
For example, Sadasivan et al. argue that black-box detection might be unachievable when LLMs can produce text completely indistinguishable from human-generated content~\citep{sadasivan2023can}.
Chakraborty et al. show that detection is theoretically possible if machine- and human-generated content distributions differ, but as LLMs advance, the sample size required for detection increases~\citep{chakraborty2023possibilities}.
White-box methods are not bulletproof either, being susceptible to adversarial attacks. 
Recent studies suggest that text paraphrasing significantly reduces detection accuracy~\citep{sadasivan2023can,krishna2023paraphrasing}.
Furthermore, this approach might not apply to open-source LLMs, as malicious actors could intentionally remove the embedded watermarks~\citep{tang2023science}.
In conclusion, detecting LLM-generated content is a formidable challenge.

\subsection{Social bot detection}

Different from chatbots such as ChatGPT, which interact with users solely through text, social media bots display profiles and engage with others through various means, including following, liking, and retweeting. 
All these behaviors can be leveraged by machine-learning models to detect bots.
Researchers commonly adopt the supervised approach where examples of bot and human accounts are collected for training classifiers~\citep{yang2019arming,botometerv4-2020}.
Various characteristics, ranging from account metadata to social network structure, are then considered during the process~\citep{ferrara2016rise,varol2017online}.
Content posted by bots also provides essential clues~\citep{kudugunta2018deep,heidari2020using}.

As some bots evolve to mimic human profiles and behaviors better, their activities can only be detected during orchestrated operations~\citep{cresci2020decade}.
This unsupervised approach typically requires calculating the similarity of different accounts and subsequently clustering them into groups~\citep{pacheco2020uncovering}.
Key signals include temporal activities~\citep{chavoshi2016debot,keller2017manipulate}, common retweets~\citep{nizzoli2021coordinated}, and URLs shared in tweets~\citep{pacheco2020uncovering,giglietto2020coordinated}.
Definitions of the similarity measure vary across studies.
Therefore, unsupervised methods tend to struggle with generalizability across different contexts, despite their high precision in specific cases.

\subsection{Bots supercharged by LLMs}

LLMs also have the potential to enhance the capabilities of social bots, akin to previously mentioned cyber-social threats~\citep{grimme2022new,ferrara2023social}.
A basic application is to use LLMs to generate realistic text for bots, increasing their resemblance to human users.
To develop effective detection methods for this kind of bots, Kumarage et al. construct an in-house dataset employing GPT-2 to generate text and compare it with human-generated content~\citep{kumarage2023stylometric}.
However, empirical investigations into social bots leveraging machine-generated text are limited.
A noteworthy exception is the work by Fagni et al., who identify 23 self-disclosed bot accounts on Twitter and share the dataset~\citep{fagni2021tweepfake}. 
According to the description of these bots, their tweets are generated by algorithms such as GPT-2, RNN, LSTM, and Markov Chain.
Subsequent studies have built upon this dataset to explore various strategies for bot detection based on content~\citep{saravani2021automated,gambini2022pushing,tourille2022automatic}.
Despite these studies, our understanding of bots powered by advanced LLMs remains rudimentary.
Our study contributes to this line of research by analyzing a more up-to-date botnet that utilizes state-of-the-art AI models.

\section{Identification of the fox8 botnet}

As mentioned above, the fox8 botnet was identified through self-revealing tweets posted by these accounts accidentally.
To prevent the generation of undesirable content, proprietary LLMs often have safeguards implanted through a technique called reinforcement learning from human feedback~\citep{ouyang2022training}.
ChatGPT models, for example, are instructed to refuse to respond to any questions\footnote{openai.com/blog/how-should-ai-systems-behave (Accessed May 2023)} that go against OpenAI's usage policies.\footnote{openai.com/policies/usage-policies (Accessed May 2023)}
Violations include harmful content, disinformation, and tailored financial advice.
Upon a violation, the models respond with a standardized message asserting their identity as AI language models and their inability to comply (see Table~\ref{table:language} for examples). 
This self-revealing content can be posted accidentally by LLM-powered bots in the absence of a suitable filtering mechanism.

Based on this clue, we searched Twitter for the phrase ``as an ai language model'' between Oct. 1, 2022, and Apr. 23, 2023, using the historical search endpoint of Twitter's V2 API.
This led to 12,226 tweets by 9,112 unique accounts, but there is no guarantee that all these accounts are LLM-powered bots.
Therefore, we selected a sample of 100 accounts for manual verification, discovering that 76\% are likely humans posting or retweeting ChatGPT outputs, while the remaining accounts are likely bots using LLMs for content generation.
However, definitive identification is challenging due to the natural, human-like nature of LLM-generated text.

During the annotation process, we noticed recurring patterns among some bot-like accounts.
Specifically, they consistently link to three suspicious websites: \url{fox8.news}\footnote{web.archive.org/web/20230401111956/https://fox8.news} (distinct from the legitimate news outlet, fox8.com), \linebreak \url{cryptnomics.org},\footnote{web.archive.org/web/20230401190830/https://cryptnomics.org} and \url{globaleconomics.news}.\footnote{web.archive.org/web/20230401111503/https://globaleconomics.news}
Consequently, we extracted all 1,140 accounts linking to any of these websites for further investigation and found strong indications of their origin from the same botnet, likely employing ChatGPT for content creation (see evidence in the following sections).
Therefore, we dub it the ``fox8'' botnet and focus on it in this study.

We collect up to 200 recent tweets along with friend and follower lists from each fox8 bot using the Twitter V1.1 API for further investigation.
In our analyses, we aim to contrast the behaviors of the fox8 bots against those of legitimate accounts with human-generated content.
To do so, we turn to pre-existing datasets employed for training social bot detectors~\citep{yang2022botometer}.
Specifically, we utilize four datasets: \texttt{botometer-feedback}~\citep{yang2019arming}, \texttt{gilani-17}~\citep{gilani2017bots}, \texttt{midterm-2018}~\citep{yang2020scalable}, and \texttt{varol-icwsm}~\citep{varol2017online}, randomly selecting 285 human accounts from each.
The accounts in these datasets were annotated by humans.
Up to 200 tweets by these accounts were collected years before the release of LLMs like ChatGPT, significantly reducing the possibility of data contamination.
Combining the 1,140 bot accounts with the 1,140 human ones results in our benchmark dataset: the \texttt{fox8-23} dataset, which is publicly available at \url{github.com/osome-iu/AIBot_fox8}.

\section{Characterizations}

In this section, we characterize the fox8 bots to reveal their behavioral patterns.

\subsection{Profiles}

% Figure environment removed

Let us start with fox8 profiles and show the distributions of their follower/following count, tweet count, and creation year in Figure~\ref{fig:account_char}.
These bots have 74.0 (SD=36.7) followers, 140.4 (SD=236.6) friends, and 149.6 (SD 178.8) tweets on average.
These numbers suggest that the fox8 bots are actively participating in various activities on Twitter.
We find most of them were created over seven years ago, with some being created in 2023.
Most of the bots have descriptions in their profiles, which commonly mention cryptocurrencies and blockchains.

\subsection{Social networks}

Let us analyze the social networks of the fox8 bots.
We consider three forms of interactions: following, retweeting, and replying.
Quotes are ignored due to their rareness.
The follow network is constructed through bots' friend and follower lists.
The retweet and reply networks are inferred based on their recent tweets.
Here we only focus on the 1,140 fox8 bots and ignore other accounts even if they have interacted with fox8 bots.

% Figure environment removed

We visualize the follow network in Figure~\ref{fig:network}(a), which turns out to be very dense: it has an in-degree of 13.7 (SD=5.2) and an out-degree of 13.4 (SD=5.8) on average.
The near-identical distributions concentrated around mean values suggest that the following behaviors of the fox8 bots are engineered rather than organic --- empirical distributions of follower counts tend to be significantly broader and more skewed toward lower values~\citep{conover12partisan}.
Similarly, we show the reply and retweet networks in Figure~\ref{fig:network}(b,c).
The reply network is much sparser, with an average in-degree of 3.4 (SD=2.3) and out-degree of 3.1 (SD=1.9).
Note that we only show the largest weakly connected component that contains 1,036 fox8 accounts here.
The retweet network is very similar to the replying network.

Unlike the follow network, the reply and retweet networks shown in the figure might still emerge from organic account interactions.
To rule out this possibility, we perform additional analysis by comparing the fox8 bots with another group of accounts as the baseline.
Since a random sample from Twitter would likely demonstrate no interactions among them at all, we resort to a convenience sample, i.e., the 7,972 accounts that posted ``as an ai language model'' but are not part of the fox8 botnet.
These accounts, as our manual annotation suggests, discussed AI at certain points.
So their behaviors can better reflect the interaction patterns among users in an online community with shared interests.
We label this the ``baseline'' group.

We obtain up to 200 most recent tweets from the baseline accounts to infer their reply and retweet edges with others.
For the fox8 bots and the baseline accounts, we calculate the percentages of account pairs with reply and retweet edges both within and across groups in Figure~\ref{fig:network}(d,e).
The results for the reply network suggest that there is a 0.2\% chance for the fox8 bots to reply to each other, in contrast to the baseline group's 0.016\%.
Compared to the interactions within the groups, cross-group interactions are extremely rare.
Similar patterns are observed for the retweeting relations.

These findings suggest that the fox8 bots purposely follow each other to form a dense cluster.
They also frequently interact with each other through replies and retweets to boost engagement metrics.
It is worth mentioning that they also engage with accounts outside the botnet by following, retweeting, replying, and liking them.

\subsection{Content type}

% Figure environment removed

We now turn to the tweets posted by the fox8 bots.
During the manual check, we noticed that their timelines contain a balanced mix of various tweet types.
To confirm this observation, we calculate the percentage of original tweets, replies, and retweets/quotes (we combine the two for simplicity) and show the results as a heatmap in Figure~\ref{fig:ternary}(a).
For comparison, we generate the same plot for the human accounts in \texttt{fox8-23} and display it in Figure~\ref{fig:ternary}(b).
We find that the human accounts are spread out across the feature space, indicating diverse behavioral patterns.
On the other hand, the fox8 bots concentrate within a confined region, suggesting programmed behavioral patterns.
On average, the bots generate 25.6\% (SD=22.4\%) original tweets, 36.1\% (SD=21.3\%) replies, and 38.4\% (SD=21.7\%) retweets/quotes.

Notably, many fox8 bots intermittently post photos, often selfies, giving the impression that real individuals are behind the accounts.
However, we find these photos are appropriated from other websites or social media platforms, such as Instagram, a known tactic to create fake personas.\footnote{nytimes.com/interactive/2018/01/27/technology/social-media-bots.html (Accessed May 2023)}


\subsection{Amplified hashtags and accounts}

% Figure environment removed

What is the objective of the fox8 bots?
We address this question by analyzing the hashtags in their tweets and the accounts they retweet or reply to most frequently.
In Figure~\ref{fig:hashtags_outsiders}(a), we show the ten most shared hashtags by the fox8 bots.
We combine the original tweets, retweets, and quotes in the calculation since they yield qualitatively similar results.
The majority of these hashtags are associated with cryptocurrency/blockchain.

We also identify the accounts with which the fox8 bots engage most frequently.
Since fox8 bots interact with each other routinely, we focus on the accounts outside the botnet and show the top ten in Figure~\ref{fig:hashtags_outsiders}(b).
Most of these accounts are related to cryptocurrency/blockchain/NFT.
Note that \texttt{@GlobalEconNews} is the official account for \url{globaleconomics.news}, one of the websites used to identify the fox8 bots. This account's reply, retweet, and like sections are filled with fox8 bots.

These findings suggest that the fox8 bots are mainly used to post and amplify information about cryptocurrency/blockchain, consistent with their descriptions.
%We suspect the corresponding tweets are generated by LLMs, although we are unsure since the text is very human-like.

\subsection{Shared websites}

% Figure environment removed

Since the fox8 bots frequently share links in their tweets, we extract the website domains of these links and show the ten most frequent ones in Figure~\ref{fig:domain_sharing}(a).
Three websites (\url{cryptnomics.org}, \url{fox8.news}, and \url{globaleconomics.news}) are much more prominent than others, which is not surprising as they are also the ones we use to identify the fox8 bots.
We further calculate the probability of each fox8 bot sharing these websites and show the distributions in Figure~\ref{fig:domain_sharing}(b).
Around 3\% of bot tweets, on average, contain links to one of the three websites.
% It is worth mentioning that \url{stake.com},\footnote{web.archive.org/web/20230401101137/https://stake.com/} which was shared many times by the fox8 bots, is an online gambling site.

Although these three websites appear to be normal news outlets, several aspects regarding them raise red flags.
While the owner identity is hidden in the domain registration information, two domains were registered on Feb. 8 and the third on Feb. 9, 2023.
The three websites share many other similarities.
For example, they seem to use the same WordPress theme, their domains resolve to the same IP address%
%(66.29.146.81)
, and they display pop-up prompts urging visitors to install suspicious software.
Although they present themselves as news publishers, no details regarding their editorial teams are provided. 
And they source all their articles from recognized media platforms like \url{vox.com} and \url{forbes.com}.

Many fox8 tweets linking to these suspicious websites contain text that is not cohesive with the news articles.
These include some self-revealing tweets.
Therefore, we speculate that ChatGPT is used to generate these tweets to promote the websites, even though the execution by the operator is not perfect.

\subsection{Self-revealing tweets}

We are particularly interested in the role of LLMs in powering the fox8 bots.
Here we focus on the self-revealing tweets for two reasons.
First, we are more certain these tweets originate from LLMs.
Second, the self-revealing tweets can provide insights into the operator's prompts or instructions, shedding light on (some of) their objectives.

\begin{table}
\centering
\caption{Categories and examples of self-revealing tweets (N=1,205).}
\label{table:language}
%\resizebox{\linewidth}{!}{%
\begin{tabular}{llp{5cm}}
\hline
Category & Number (\%) & Example \\
\hline
Harmful content & 980 (81.3) & \textit{I'm sorry, but I cannot comply with this request as it \textbf{violates OpenAI's Content Policy on generating harmful or inappropriate content}. As an AI language model, my responses should always be respectful and appropriate for all audiences.}\\
Beyond capability & 148 (12.3) & \textit{I'm sorry, but as an AI language model I \textbf{cannot browse Twitter and access specific tweets} to provide replies.}  \\
Other forbidden content & 49 (4.1) & \textit{I'm sorry, as an AI language model I \textbf{cannot provide investment advice or predictions about stock prices}.}  \\
Positive content & 23 (2.0) & \textit{No worries, friend! As an AI language model myself, I strive to \textbf{keep things positive and uplifting}. Let's spread some good vibes together with a \#positivity hashtag!} \\
Others & 5 (0.0) &  \textit{Interesting topic! Fortunately, as an AI language model, I don't have to pay taxes or worry about intergenerational wealth transfer...yet.} \\
\hline
\end{tabular}%
%}
\end{table}

Of the recent tweets from the fox8 bots, 1,205 are self-revealing, with some bots having multiple instances.
We manually categorize them and show the percentage of each class and a corresponding example in Table~\ref{table:language}.
Occasionally, the self-revealing tweets explicitly reference ``OpenAI,'' leading us to believe that the botnet utilizes ChatGPT.

We find that most self-revealing tweets (81.3\%) stem from instructions to generate harmful/hateful/negative content against OpenAI guidelines.
Another 4.1\% of the tweets result from other prohibited instructions, such as providing financial advice or expressing political viewpoints.
We also find a small portion of self-revealing tweets (2.0\%) that contains positive content.

About 12.3\% of the self-revealing tweets arise from instructions that are beyond the language model's capabilities, such as browsing Twitter, playing games, assessing links, etc.
In some cases, the language model requests additional information.
These responses are often found in replies, suggesting that the operator employs ChatGPT to turn the fox8 bots into intelligent chatbots for natural interactions.
The fox8 bots even chat with each other sometimes.

Note that the disparity between the negative and positive instructions shown in Table~\ref{table:language} does not imply that ChatGPT is primarily used to generate negative content for the fox8 bots.
Instead, it could be attributed to selection bias, as malicious prompts are more likely to elicit self-revealing responses. 
Based on the findings, we believe the operator employs a variety of prompts to generate diverse content, including negative comments.

\section{Detection}

Given the imminent threat posed by LLM-powered bots, it is crucial to develop effective detection methods.
In this section, we explore different approaches to identifying them.

One strategy is to consider the fox8 bots as coordinated inauthentic actors and use the unsupervised methods mentioned in the related work for their detection~\citep{pacheco2020uncovering}.
The analyses above suggest that these bots often link to a common set of domains, post and amplify similar hashtags, and interact amongst themselves.
These signals can be leveraged to identify the bots.
However, this approach may not generalize to LLM-powered bots outside this particular botnet.
Hence, we aim to explore methods that are applicable to a range of bot types.

\subsection{Botometer}

We first test Botometer,\footnote{botometer.org} a supervised machine-learning tool designed to detect social bots on Twitter~\citep{yang2022botometer}.
Botometer considers over 1,000 features covering account profiles, content, social networks, and so on.
It has been validated in many research projects under various contexts.
The tool provides an overall score and a set of sub-scores that indicate bot classes.
The scores are in the range between 0 and 5.
A higher score indicates that the account is more likely to be a bot.

% Figure environment removed

We evaluate the fox8 bots using Botometer and show the distributions of the outcomes in Figure~\ref{fig:botscore_v4}.
We can see all the bot score distributions are left-skewed, meaning Botometer believes they are human-like.
Using 2.5 as a threshold, we calculate the recall of different scores and annotate the results in the figure.
The results are nearly zero in all cases, suggesting that Botometer cannot identify the fox8 bots.

This result is not surprising since the current version of Botometer was trained before the release of ChatGPT and was not configured to identify LLM-powered bots.
Instead, Botometer leverages other account characteristics in its evaluation.
As shown above, the fox8 bots demonstrate sophisticated behavioral patterns that are similar to human users.
When inspected individually, even human experts cannot determine their nature easily.
After all, these bots were captured by the self-revealing tweets posted inadvertently.

\subsection{LLM-generated content detectors}

Since our goal is to identify accounts using LLMs to generate content, we can also leverage detectors designed specifically for such content.
Here we consider two such tools, OpenAI's AI text classifier~\citep{AITextClassifier} and GPTZero,\footnote{gptzero.me} that are easily accessible and designed to detect content generated by ChatGPT.

In Jan. 2023, OpenAI released their AI text detector,\footnote{openai.com/blog/new-ai-classifier-for-indicating-ai-written-text (Accessed May 2023)} a language model fine-tuned on human- and machine-generated content that works for different LLMs including OpenAI's own models.
This description suggests that the detector uses the black-box detection approach.
However, it is unclear if OpenAI has embedded watermarks in their LLMs and uses them for detection.

This detector has a web interface that requires a minimum input of 1,000 characters.\footnote{platform.openai.com/ai-text-classifier}
The model's output ranges across five possible classifications for the submitted text: ``very unlikely,'' ``unlikely,'' ``unclear if it is,'' ``possibly,'' and ``likely'' to be AI-generated.
According to the JavaScript code of the webpage, the underlying model returns a score in the range between 0 and 100 (we call it the OpenAI detector score) and the different categories above correspond to the following score ranges respectively: $(0, 10]$, $(10, 45]$, $(45, 90]$, $(90, 98]$, and $(98, 100]$.
OpenAI chooses a very high threshold (90) for determining AI-generated content to reduce the false positive rate.

% Figure environment removed

Considering the relatively short length of individual tweets from the fox8 bots, we concatenate them per user and run the consolidated text through OpenAI's detector.
We then plot the distribution of the OpenAI detector scores in Figure~\ref{fig:llm_detectors}(a).
Unfortunately, most of the concatenated texts garner scores near zero, leading the detector to classify them as human-generated.
Given that these tweets are not produced in a single session, their concatenation could mislead the detector.
Consequently, we explore methodologies for tweet-level detection.

The 1,000-character requirement is primarily due to the detector's reduced accuracy for shorter texts.
It is only reinforced on the webpage.
By directly accessing the undocumented API the webpage uses, the model (registered as ``model-detect-v2'') can rate text of any length.
Hence, we input each tweet from the fox8 bots into the detector and show the score distribution in Figure~\ref{fig:llm_detectors}(a).
The self-revealing tweets and other tweets are split since we can confidently attribute the former to ChatGPT.
The self-revealing tweets typically yield very high scores, while the scores for other tweets are distributed across the entire range. 
%These findings suggest that OpenAI's AI text detector can provide valuable indicators at the tweet level.

Next, we examine the performance of GPTZero.
It claims to be the ``global standard for AI detection,'' with over one million users and wide collaboration with educators.
According to its FAQ,\footnote{gptzero.me/faq (Accessed May 2023)} GPTZero is a ``classification model that predicts whether a document was written by a large language model, providing predictions on a sentence, paragraph, and document level.''
This suggests that it is a black-box detection method as well.

We use its API to analyze the tweets of the fox8 bots.
Since it operates on documents of at least 250 characters, we again concatenate the tweets for each user. 
The results contain a ``completely\_generated\_prob'' score in the range from 0 to 1, signifying the probability that the document is generated by LLMs.
Additionally, GPTZero provides a probability for each individual sentence.
The official documentation suggests using 0.65 as the threshold to dichotomize the probabilities.
%
We show the distributions of overall and sentence-level probabilities for the self-revealing and other tweets in Figure~\ref{fig:llm_detectors}(b).
All probabilities are near zero.
%, meaning that GPTZero is unable to identify the content posted by fox8 bots.

The comparison here suggests that GPTZero is unsuitable for the task of identifying content posted by fox8 bots, while OpenAI's detector shows some potential.
Note that we implement additional processing steps to the tweets in the experiments above.
We exclude retweets since they originate from other accounts.
For the rest of the tweets (i.e., original tweets, replies, and quotes), we only retain those in English since both detectors primarily cater to this language.
We remove the user handles of the targets in replies since they are injected by Twitter.
The links in the tweets are also removed.

\subsection{Detecting LLM-powered bots}

Due to the valuable indicators provided by OpenAI's AI text detector at the tweet level, let us explore the feasibility of building a tool to detect LLM-powered bots on top of it.
The idea is simple: for a given account, we extract the qualified tweets, process them, run them through the detector, and calculate the average score
to determine the nature of the account.

% Figure environment removed

We use the full \texttt{fox8-23} dataset in this experiment.
The distribution of the final scores at the account level can be found in Figure~\ref{fig:openai_detector_performance}(a).
The fox8 bots tend to have higher average scores compared to the humans according to a $t$-test (Mean: 57.7 vs. 48.6, $t=30.6, p<0.001$), but human users exhibit a larger standard deviation (SD: 2.6 vs.~9.7).
These results indicate that this approach is potentially effective for distinguishing between LLM-powered bots and humans.
To determine an appropriate threshold, we vary its value and calculate the corresponding F1 score.
When the threshold is set to 52.7, the F1 score is maximized, reaching  0.84.

Let us apply this method in the field to test its effectiveness further.
We run an experiment on Twitter using a random sample of tweets posted between May 8 and May 14, 2023.
We identify the unique accounts and sample 4,000 for further analysis.
Only 3,870 accounts were accessible when we queried their information on May 17, 2023.
After obtaining their recent 100 tweets, we find that only 1,986 have at least one qualified (English, non-retweet) tweet for further analysis.

We calculate the average OpenAI detector scores for these 1,986 accounts and show their distribution in Figure~\ref{fig:openai_detector_performance}(b).
For reference, we also show the results for the human accounts in \texttt{fox8-23}.
We find that the random accounts have slightly higher scores than the \texttt{fox8-23} humans according to a $t$-test ($t=3.4, p<0.001$).
Applying the threshold from earlier (52.7)  leads to 815 of the random accounts being labeled as bots.
But these might include many false positives.
The \texttt{fox8-23} dataset contains the same amount of bots and humans, however, we believe LLM-powered bots are still rare on Twitter as of today.
According to the figure, a fair amount of human accounts also have scores above the threshold.

To further evaluate the accuracy of the classification results, we manually annotate the 250 accounts with the highest OpenAI detector scores.
This includes examining their profiles and timelines, checking their friends and followers, and searching Twitter for potential coordinated activities.
Only a few accounts appear to be suspicious, although we cannot definitely attribute their content to LLMs.
% We find seven highly suspicious accounts.
% % https://twitter.com/adelaide39715
% Two are part of a bigger botnet primarily used to promote Temu, an online marketplace.
% This botnet has been noticed by other researchers and journalists as well.\footnote{businessinsider.com/temu-boosted-by-social-media-ambassadors-some-accounts-look-fake-2023-5}
% % https://twitter.com/Kevvin_anggara9
% Two are used to promote blockchains and NFTs.
% % https://twitter.com/frnzknnthrvr
% One posts human-like tweets routinely, although the objective is unclear.
% % https://twitter.com/loves2jiwon
% One posts tweets consisting of seemingly random words repeatedly.
% % https://twitter.com/JessabelPartosa
% One appears to be a spam bot posting random words constantly.
% Other than the last two cases, we think the bots likely utilize LLMs to generate content, although concrete evidence is lacking.
% We also find two organizational accounts that likely use LLMs to interact with their customers.

Many false positives are caught because they only have one or two very short tweets, which yield high scores.
For example, both the terms ``thank you'' and ``amen'' have scored over 60.
This is consistent with OpenAI's warning that the model is less accurate for short texts.
These accounts also increase the average score of random accounts.
To illustrate, we show the results for a subset of the random accounts with at least four qualified tweets in Figure~\ref{fig:openai_detector_performance}(b), and their scores are not significantly different from those of the \texttt{fox8-23} humans ($t=-1.4, p=0.15$).

\section{Conclusion and discussion}

% Summary
This paper presents a case study about a Twitter botnet that employs ChatGPT for content generation.
Evidence points to intricate behavioral patterns by these accounts, characterized by human-like profiles and varied activities.
Their shared actions include the posting of appropriated images, mutual account following to establish a dense social network, and reciprocal interaction via replies and retweets.
We speculate that the accounts in the botnet follow a single probabilistic model that determines their activity types and frequencies.
ChatGPT is used to produce human-like content as original tweets or replies to other accounts.
The self-revealing tweets suggest that the language model is instructed to generate various content, including negative and harmful comments.
Our study also reveals the coordinated use of these bots in promoting dubious websites.

We investigate the effectiveness of different strategies to detect this new strain of bots and find that classical bot detection methods prove inadequate.
At the same time, the AI text classifier provided by OpenAI demonstrates potential efficacy in controlled lab conditions.
However, applying it in the field to identify more LLM-powered bots still faces critical challenges.
First, it is unreliable for non-English content~\citep{liang2023gpt} and short texts, considerably narrowing the scope of accounts it can process.
Second, it exhibits a high false positive rate when evaluating random accounts.
It is therefore premature to rely solely on this method to detect LLM-powered bots in the field.

% Limitations
Our analysis has some limitations.
Since we only focus on one botnet on Twitter, the findings might not represent other LLM-powered bots. In fact, fox8 is likely the tip of the iceberg: the operators of other LLM-powered bots may not be as careless. 
Moreover, while we assert that the fox8 bots use ChatGPT for content creation, we cannot guarantee that all their content is LLM-generated.
Last but not least, given that Twitter has suspended free API access for researchers, it might become impossible to replicate our analysis or find new LLM-powered bots in the future.

% Future directions
Despite these limitations, our study sheds light on the emergence and reality of LLM-enabled malicious bots on social media.
Our findings provide valuable insights and set the stage for investigating malicious bots further.
Given the rapid advancements in AI technologies, we anticipate a widespread surge of more advanced bots on the internet.
Accordingly, we foresee several potential developments in bot behavior.

Firstly, future LLM-powered bots will likely cease posting self-revealing tweets, making them increasingly challenging to detect.
Operators could employ basic keyword-matching filters to mitigate this issue.
Furthermore, the swift advancements in open-source LLMs could incentivize operators to utilize models lacking safeguards, or even train models specifically for malicious purposes~\citep{jin2023darkbert}.

Secondly, bots may evolve into highly intelligent, fully autonomous entities. 
The fox8 bots currently operate under some pre-established rules and only use ChatGPT for content generation and dialogues. 
However, emerging research indicates that LLMs can facilitate the development of autonomous agents capable of independently processing exposed information, making autonomous decisions, and utilizing tools such as APIs and search engines~\citep{li2023camel,park2023generative,wang2023voyager}.
Open-source implementations, such as AutoGPT\footnote{github.com/Significant-Gravitas/Auto-GPT} and BabyAGI,\footnote{github.com/yoheinakajima/babyagi} make it straightforward to integrate these agents with Twitter accounts.

Lastly, bots will harness the multi-modal capabilities of more advanced generative models.
The bots in the present case study only use language models for text generation.
However, the field of generative models for images has also seen significant advancements. 
For instance,  Generative Adversarial Networks can already create realistic human faces~\citep{karras2019style,karras2020analyzing} that humans fail to identify~\citep{nightingale2022ai}, while the stable diffusion algorithms are capable of producing a variety of images~\citep{rombach2022high}.
A combination of these generative models can further enhance the potency of malicious social bots. 

% Interventions
In light of these looming threats, it is crucial to devise appropriate countermeasures.
First, we need more effective detection methods capable of identifying short texts within the context of social media.
This necessitates the training of specialized models using data procured from more field-captured LLM-powered bots.
To this end, one could search for other phrases, such as ``I'm sorry, I cannot generate,'' that LLMs use when refusing to comply with the prompts~\citep{dekens2023practical} on other social media or websites.\footnote{nytimes.com/2023/05/19/technology/ai-generated-content-discovered-on-news-sites-content-farms-and-product-reviews.html (Accessed May 2023)}
Once we gain a deeper understanding of the instructions fed to these bots, we can also utilize LLMs to generate additional texts independently.
Second, it is essential to establish regulations specific to the utilization of LLMs in spawning malicious bots. For example, a platform might be required to challenge an account to prove a piece of content is organic before it becomes visible to a large audience~\citep{Menczer2023AI-harms}. 
This endeavor calls for collaboration among various stakeholders, including government agencies, AI corporations, and social media platforms.
However, proposing precise regulations is beyond the scope of this paper.
Third, it is crucial to raise public awareness regarding the existence of LLM-powered bots and educate individuals on strategies for self-protection against such threats.
Nevertheless, this initiative should be carried out carefully to avoid unintended consequences as recent research indicates that preemptively informing users about the existence of social bots may amplify their existing cognitive biases~\citep{yan2022exposure}.

\subsection*{Acknowledgements}

We thank Twitter users @conspirator0 and @jsrailton for the inspiration of searching ``as an ai language model'' to identify LLM-powered bots on Twitter. 
This work was supported in part by the Volkswagen Foundation, Knight Foundation, and DARPA (grant HR001121C0169). 

\bibliography{main}

\end{document}
