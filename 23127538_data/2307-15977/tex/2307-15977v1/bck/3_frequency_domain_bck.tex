\clearpage
\section{Understanding the cause of model fingerprints\wdd{You need to define what is model fingerprint, and justify why it can be better understood in frequency domain}}
In this section, we first review the general architecture of generators, and subsequently, we delineate how each component of the generator operates in the frequency domain. Finally, through visualization, we show how these operations leave model fingerprints on the DFT spectrum of the generated image.

% \subsection{General generative architecture}
% % \noindent \textbf{GAN}
% % \noindent \textbf{VAE}
% % \noindent \textbf{Flow}
% % \noindent \textbf{Diffusion}
% % - GAN, VAE, Flow, Diffusion
% % - Ref: 
% %   - Are GANs Created Equal? A Large-Scale Study
% %   - A Large-Scale Study on Regularization and Normalization in GANs
% %   - AdversarialNAS: Adversarial Neural Architecture Search for GAN
% % Figure: illustration of the generative architecture

\subsection{Generative model in the frequency domain\wdd{Add visualized examples throughout this section. }}

% We analyze how different network components, such as convolutional layers, upsampling layers, and activation functions, affect the frequency spectrum of the feature maps. We also show how stacking multiple components and blocks can create complex and diverse frequency patterns that can be used to distinguish different networks.

\noindent \textbf{Convolution.} For an input feature map $X \in \mathbb{R}^{C_1\times H\times W}$ and a convolution layer Conv using kernels $K \in \mathbb{R}^{C_1 \times C_2 \times k \times k}$ with stride one, where $C_1$, $C_2$, and $k$ are input channel number, output channel number, and kernel size, respectively. We first consider a simple case with single input and output channel, \ie, $C_1=1$ and $C_2=1$. According to the Convolution Theorem~\cite{oppenheim1999discrete}, the spatial convolution operation equals to multiplication operation in the frequency domain:
\begin{equation}
    \mathcal{F}\{X \otimes K\} = \mathcal{F}\{X\} \cdot \mathcal{F}\{K\}, 
\end{equation}
where $\mathcal{F}\{X\}$ and $\mathcal{F}\{K\}$ be the discrete Fourier transform (DFT) of zero-padded $I$ and $K$. Note that to satisfy the convolution theorem, we need to first zero-pad both $I$ and $K$ to a size of $(H+k-1, W+k-1)$.

% [multiple channels] 
% the convolution layer could have multiple input and output channels. The Convolution Theorem could not be directly applied but still stand for the convolution computation of each channel. 
In the case of multiple channels, the Convolution Theorem still stands for the convolution computation of each channel:
\begin{equation}
    \mathcal{F}\{O_{j}\} = \mathcal{F}\{\sum_{i=0}^{C_1} X_{i} \otimes K_{i,j}\} = \sum_{i=0}^{C_1} \mathcal{F}\{X_{i}\} \cdot \mathcal{F}\{K_{i,j}\},
\label{eq:eq_conv}
\end{equation}
where $O_{j}$ is the $j$-th channel of the output feature map with $C_2$ channels, $X_{i}$ is the $i$-th channel of the input feature map, $K_{i,j}$ is the $(i,j)$-th channel of $K$.

% It is verified in previous works that upsampling layers would cause artifacts like checkerboard pattern~\cite{odena2016deconvolution, wojna2019devil}, yet the effects of each type of upsampling layer are always discussed independently~\cite{wojna2019devil, pons2021upsampling}. 
 % Transposed convolutions are the most commonly used upsampling layers and are also sometimes referred to as `deconvolution' or `upconvolution'. 
% For nearest neighbor upsampling, the convolution has a 2$\times 2$ kernel size with parameters as shown in Figure. For bilinear upsampling increasing the resolution by factor 2, the convolution has a $3 \times 3$ kernel size with parameters as shown in Figure.

% Then the frequency domain transform is:
% \begin{equation}
%  \mathcal{F}\{I_{up}\} = \mathcal{F}\{K_{up}\} \cdot \mathcal{F}\{\hat{I}\},
%  \label{eq:uniup}
% \end{equation}
% where $\hat{I}$ is the zero-interleaved $I$, and $\hat{I}(x,y)=0$ when $x$ or $y$ is odd. 
\noindent \textbf{Upsampling.} Common types of upsampling layers are transposed convolution and interpolation upsamplers such as nearest neighbor and bilinear. Transposed convolution is equivalent to interleaving the input features with 0â€™s and applying a standard convolutional operation. For interpolation upsamplers, the interpolated pixels are linearly dependent on their neighbors, thus they could also be regarded as a combined operation including zero-interleaving and a convolution operation with fixed parameters shown in Figure~\ref{fig:}. Then we could summarize there upsampling operations into a unified formalization:
% \begin{equation}
%  I_{up} = \text{Conv}(\text{ZeroInter}(I)),
%  \label{eq:uniup}
% \end{equation}
\begin{equation}
 X_{up} = \text{ZeroInter}(X) \otimes K_{up},
 \label{eq:uniup}
\end{equation}
where $\text{ZeroInter}$ is interleaving the input feature $X$ with 0's, and $K_{up}$ is a convolution kernel. For transposed convolution, $K_{up}$ is learnable. For nearest neighbor and bilinear upsampling, $K_{up}$ has corresponding fixed weight. As explained in appendix~\ref{sec:a1}, zero-interleaving in the spatial domain brings about spectrum replicas in the frequency domain as shown in Figure~\ref{fig:}. Then we could summarize upsampling operation in the frequency domain as replicating the frequency spectrum of the input signal and pointwise multiplication with a corresponding convolution spectrum:
\begin{equation}
 \mathcal{F}\{X_{up}\} = \text{Repeat}(\mathcal{F}\{X\}) \cdot \mathcal{F}\{K_{up}\}.
 \label{eq:uniup2}
\end{equation}
\wdd{Is there any parameter for Repeat()? Like repeating how many times on x and y dimension?}
% A Large-Scale Study on Regularization and Normalization in GANs
% Batch normalization and instance normalization are commonly used by normalizing across the batch or instance. 
\noindent \textbf{Normalization.}
The Normalization operation solves the problem of internal covariance shift, and improves the stability of the training process. Let $\mathbf{B}=\{x_1, x_2, ..., x_{S}\}$ be a training mini-batch with batch size $S$, the basic procedure of Batch Normalization contains two steps:
\begin{equation}
    \begin{aligned}
        \hat{x}_i=\frac{x_i-\mu_{\mathbf{B}}}{\sqrt{\sigma_{\mathbf{B}}^2+\epsilon}}, i=1, \ldots, S
    \end{aligned}
\end{equation}
\begin{equation}
    \begin{aligned}
        BNT_{\gamma,\beta}(x_i)=\gamma \hat{x}_{i}+\beta, i=1, \ldots, S
    \end{aligned}
\end{equation}
where $\mu_{\mathbf{B}}$ and $\sigma_{\mathbf{B}}$ are mean and variance caculated over the mini-batch $\mathbf{B}$. $\gamma$ and $\beta$ are two learnable parameters for scaling and shifting the normalized feature map $\hat{x}_i$ in the first step. 
Based on the Discrete Fourier Transform, Batch normalization in the frequency domain could be described as follows~\cite{pan2022learning}:
\begin{equation}
  \begin{array}{l}
\mathcal{F}_{i}^{BNT}(u,v) =\sum_{x=0}^{H-1}\sum_{y=0}^{W-1} (\gamma \frac{I
_{i}(x,y)-\mu_{\mathbf{B}}}{\sqrt{\sigma_{\mathbf{B}}^2+\epsilon}}+\beta) \cdot e^{-i 2 \pi (ux/H+vy/W)} \\
% &= \frac{\gamma}{\sqrt{\sigma_{\mathbf{B}}^2+\epsilon}} \sum_{x=0}^{H-1}\sum_{y=0}^{W-1} (I
% _{i}(x,y)-\mu_{\mathbf{B}}) \cdot e^{-i 2 \pi (ux/H+vy/W)} + \sum_{x=0}^{H-1}\sum_{y=0}^{W-1} \beta \cdot e^{-i 2 \pi (ux/H+vy/W)} \\
\quad= \gamma \frac{\mathcal{F}_{i}(u,v) -\sum_{x=0}^{H-1}\sum_{y=0}^{W-1} \mu_{\mathbf{B}} \cdot e^{-i 2 \pi (ux/H+vy/W)}}{\sqrt{\sigma_{\mathbf{B}}^2+\epsilon}} + \sum_{x=0}^{H-1}\sum_{y=0}^{W-1} \beta \cdot e^{-i 2 \pi (ux/H+vy/W)} \\
\quad= \gamma \frac{\mathcal{F}_{i}(u,v) - \mu_{\mathcal{F},\mathbf{B}}}{\sqrt{\sigma_{\mathbf{B}}^2+\epsilon}} + \beta_{\mathcal{F}},
  \end{array}
\end{equation} \wdd{u and v are not defined.}
where $\mu_{\mathcal{F},\mathbf{B}}$ and $\beta_{\mathcal{F}}$ are two constants obtained by transforming $\mu_{\mathbf{B}}$ and $\beta$ to the frequency domain. Based on the formulation, we can learn that the implementation of Batch Normalization in the frequency domain has exactly the same form as the time domain, which does not affect the distribution of the frequency spectrum. Rather, it normalizes, \ie, shifts and scales the frequency spectrum of the input signal. Instance Normalization (IN) and Batch Normalization (BN) have the same computational form, except that the mean and variance are derived from an instance instead of a batch. Thus the formulation also applies to IN.

% As explained in Appendix~\ref{sec:a2}, the implementation of Batch Normalization in the frequency domain has exactly the same form as the time domain.

\noindent \textbf{Nonlinear activation functions.} To implement activation functions in the frequency domain, SReLU~\cite{ayat2019spectral} use polynomial fitting to approximate the nonlinear activation functions\wdd{How good is this approximation? Can all activation functions be approximated using this metrhod? The example in your previous slides is not a very close approximation. People may question this.}, and approximate the ReLU function $max(0,x)$ in the form of $c_{0} + c_{1}x + c_{2}x^2$. According to the convolution theorem that multiplication in the spatial domain is equivalent to convolution in the frequency domain, this polynomial can be calculated in the frequency domain as follows:
\begin{equation}
    \begin{aligned}
        \mathcal{F}_{relu}(x) = DC + c_{1} \cdot X + c_{2} \cdot (X \otimes X),
    \end{aligned}
\end{equation} \wdd{DC,x and X are not defined. x was training samples in Normalization paragraph. Be careful with the notation.}
where $X$ is the DFT spectrum of $x$. According to ~\cite{ayat2019spectral}\wdd{add author's name or method name. Don't just put a reference number here. Fix other similar issues.}, the closet coefficients for a standard ReLU are $c_{0}=0, c_{1}=0.3, c_{2}=0.021$. Similarly, Sigmoid, Tanh, and other nonlinear activation functions could also be approximated by the Taylor series and perform corresponding frequency domain transformation as above based on the convolution theorem.

