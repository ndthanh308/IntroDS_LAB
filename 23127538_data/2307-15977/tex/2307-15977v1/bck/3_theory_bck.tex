
\section{Preliminaries}

\noindent \textbf{Discrete Fourier Transform.}
For a 1D signal $f$ of N numbers where each value is denoted by $f(x)$, the discrete Fourier transform (DFT) for $f$ is:
\begin{equation}
F(u)=\sum_{x=0}^{N-1} f(x) \cdot e^{-i 2 \pi ux/N}, u=0,1,2,\dots,N-1
\end{equation}
Next, we extend the formula to an input image given by the matrix $f \in \mathbb{R}^{M \times N}$, where the pixel values are denoted by $f(x,y)$. The 2D discrete Fourier transform for $f$ is denoted by:
\begin{equation}
F(u,v)=\sum_{x=0}^{M-1}\sum_{y=0}^{N-1} f(x,y) \cdot e^{-i 2 \pi (ux/M+vy/N)}
\end{equation}
for $u=0,1,,2,\dots,M-1$ and  $v=0,1,,2,\dots,N-1$

\noindent \textbf{Convolution Theorem.}
In mathematics, the convolution theorem states that under suitable conditions, the Fourier transform of a convolution of two functions (or signals) is the pointwise product of their Fourier transforms. Formally, give two 2D signals $f(x,y)$ and $h(x,y)$, the discrete Fourier transform for them are $F(u,v)$ and $H(u,v)$, the convolution theorem states that:
\begin{equation}
f(x,y) \otimes h(x,y)= \mathcal{F}^{-1}\{F(u,v) \cdot H(u,v)\}
\end{equation}
where $\mathcal{F}^{-1}$ is the inverse Fourier transform.

\section{Generative model in the frequency domain}

In this section, we first review the general architecture of generators, and subsequently, we delineate how each component of the generator operates in the frequency domain. Finally, through visualization, we show how these operations leave model fingerprints on the DFT spectrum of the generated image.

\subsection{General generative architecture}
% \noindent \textbf{GAN}
% \noindent \textbf{VAE}
% \noindent \textbf{Flow}
% \noindent \textbf{Diffusion}
% - GAN, VAE, Flow, Diffusion
% - Ref: 
%   - Are GANs Created Equal? A Large-Scale Study
%   - A Large-Scale Study on Regularization and Normalization in GANs
%   - AdversarialNAS: Adversarial Neural Architecture Search for GAN
% Figure: illustration of the generative architecture
\yty{more discussion on differences between GAN, diffusion, VAE, flow}
Before we analyze how different generative models leave fingerprints on their outputs, we first review the general architecture of generative models. As Fig [illustration of the generative architecture] shows, the architecture of generative models consists of multiple blocks that increase the resolution of feature maps by a factor of two. Each block has several components, such as convolution, upsampling, activation function, and normalization. These components are stacked together following certain rules. In general, a block could be represented by a tuple $(U, C, N, A, SC, Seq)$, where $U$ is the upsampling operation that can be bilinear upsampling, nearest neighbor upsampling, or a stride 2 deconvolution layer. $C$ is the convolution layer determined by kernel size, channel number, and padding mode. $A$ is the activation function that can be ReLU, PReLU, Tanh, etc. $N$ is the normalization type that can be batch normalization, instance normalization, or no normalization. $SC$ is a binary value indicating the shortcut. $Seq$ is the order of activation and normalization relative to the convolution layer.

% last layers
% 列举一些

% \subsection{Fingerprint visualization tool}
% \label{sec:vis}
% % The tool is based on the idea of “model fingerprint” from \cite{}. 
% We use a simple tool to visualize how each component of a generative model affects its fingerprints. 
% We first train $N$ networks that can reconstruct images from their inputs. These networks are only made of the basic building blocks of generators. Then we train another classifier that can tell which of the $N$ networks produced a given image. This classifier has only one layer that connects all the pixels to the class labels. For an input image with $c$ channels, and of $h$ pixels height, $w$ pixels width, the layer has a weight matrix $W$ with shape ${c,h,w,N}$. Each part of the weight represents the key components that are helpful for tracing back to the input image of the corresponding category.

% \begin{equation}
%     \mathcal{L}_{cls}(I,y) = \frac{corr(\mathcal{F}(I),W_{y})}{\sum_{\hat{y}\in\mathbb{Y}}corr(\mathcal{F}(I),W_{\hat{y}})}
% \label{eq:cls}
% \end{equation}
% where $W_{y}$ is part the $y$-th part of $W$. $corr(A,B)=A\odot B$, and $\odot$ is the inner product operation. $\mathcal{F}$ is the discrete Fourier transform. After training the classifier, we visualize $N$ weight parts respectively, each of which reflects how much each frequency component of the generated images by a certain auto-encoder contributes to the attribution to this model. 


% We mathematically characterize how network components influence the frequency of feature maps, and examine how these influences form the model fingerprints and contribute to model attribution. To analyze model fingerprints from bottom to up, we first quantify the effect of independent components on model fingerprints. Then we analyze the effects of component stacking and block stacking.  
% For an input image with $c$ channels, and of $h$ pixels height, $w$ pixels width, the layer has a weight matrix $W$ with shape ${c,h,w,N}$. 
% \begin{equation}
%     \mathcal{L}_{cls}(I,y) = \frac{corr(\mathcal{F}(I),W_{y})}{\sum_{\hat{y}\in\mathbb{Y}}corr(\mathcal{F}(I),W_{\hat{y}})}
% \label{eq:cls}
% \end{equation}
% where $W_{y}$ is part the $y$-th part of $W$. $corr(A,B)=A\odot B$, and $\odot$ is the inner product operation. $\mathcal{F}$ is the discrete Fourier transform. After training the classifier, we visualize $N$ weight parts respectively, each of which reflects how much each frequency component of the generated images by a certain auto-encoder contributes to the attribution to this model. 

\subsection{Network component in the frequency domain}

% We analyze how different network components, such as convolutional layers, upsampling layers, and activation functions, affect the frequency spectrum of the feature maps. We also show how stacking multiple components and blocks can create complex and diverse frequency patterns that can be used to distinguish different networks.

\noindent \textbf{Convolution.} For an input feature map $I \in \mathbb{R}^{C_1\times H\times W}$ and a convolution layer Conv using kernels $K \in \mathbb{R}^{C_1 \times C_2 \times k \times k}$ with stride one, where $C_1$, $C_2$, and $k$ are input channel number, output channel number, and kernel size. We first consider a simple case with single input and output channel, \ie, $C_1=1$ and $C_2=1$. According to the Convolution Theorem, the spatial convolution operation equals to multiplication operation in the frequency domain:
\begin{equation}
    \mathcal{F}\{I \otimes K\} = \mathcal{F}\{I\} \cdot \mathcal{F}\{K\}, 
\end{equation}
where $\mathcal{F}\{I\}$ and $\mathcal{F}\{K\}$ be the discrete Fourier transform (DFT) of zero-padded $I$ and $K$. Note that to satisfy the convolution theorem, we need to first zero-pad both $I$ and $K$ to a size of $(H+k-1, W+k-1)$.

% [multiple channels] 
% the convolution layer could have multiple input and output channels. The Convolution Theorem could not be directly applied but still stand for the convolution computation of each channel. 
In the case of multiple channels, the Convolution Theorem still stands for the convolution computation of each channel:
\begin{equation}
    \mathcal{F}\{O_{j}\} = \mathcal{F}\{\sum_{i=0}^{C_1} I_{i} \otimes K_{i,j}\} = \sum_{i=0}^{C_1} \mathcal{F}\{I_{i}\} \cdot \mathcal{F}\{K_{i,j}\},
\label{eq:eq_conv}
\end{equation}
where $O_{j}$ is the $j$-th channel of the output feature map with $C_2$ channels, $I_{i}$ is the $i$-th channel of the input feature map, $K_{i,j}$ is the $(i,j)$-th channel of $K$.

% It is verified in previous works that upsampling layers would cause artifacts like checkerboard pattern~\cite{odena2016deconvolution, wojna2019devil}, yet the effects of each type of upsampling layer are always discussed independently~\cite{wojna2019devil, pons2021upsampling}. 
 % Transposed convolutions are the most commonly used upsampling layers and are also sometimes referred to as `deconvolution' or `upconvolution'. 
% For nearest neighbor upsampling, the convolution has a 2$\times 2$ kernel size with parameters as shown in Figure. For bilinear upsampling increasing the resolution by factor 2, the convolution has a $3 \times 3$ kernel size with parameters as shown in Figure.

% Then the frequency domain transform is:
% \begin{equation}
%  \mathcal{F}\{I_{up}\} = \mathcal{F}\{K_{up}\} \cdot \mathcal{F}\{\hat{I}\},
%  \label{eq:uniup}
% \end{equation}
% where $\hat{I}$ is the zero-interleaved $I$, and $\hat{I}(x,y)=0$ when $x$ or $y$ is odd. 
\noindent \textbf{Upsampling.} Common types of upsampling layers are transposed convolution and interpolation upsamplers such as nearest neighbor and bilinear. Transposed convolution is equivalent to interleaving the input features with 0’s and applying a standard convolutional operation. For interpolation upsamplers, the interpolated pixels are linearly dependent on their neighbors, thus they could also be regarded as a combined operation including zero-interleaving and a convolution operation with fixed parameters shown in Figure~\ref{fig:}. Then we could summarize there upsampling operations into a unified formalization:
% \begin{equation}
%  I_{up} = \text{Conv}(\text{ZeroInter}(I)),
%  \label{eq:uniup}
% \end{equation}
\begin{equation}
 I_{up} = \text{ZeroInter}(I) \otimes K_{up},
 \label{eq:uniup}
\end{equation}
where $\text{ZeroInter}$ is interleaving the input feature $I$ with 0's, and $K_{up}$ is a convolution kernel. For transposed convolution, $K_{up}$ is learnable. For nearest neighbor and bilinear upsampling, $K_{up}$ has corresponding fixed weight. As explained in appendix~\ref{sec:a1}, zero-interleaving in the spatial domain brings about spectrum replicas in the frequency domain as shown in Figure~\ref{fig:}. Then we could summarize upsampling operation in the frequency domain as replicating the frequency spectrum of the input signal and pointwise multiplication with a corresponding convolution spectrum:
\begin{equation}
 \mathcal{F}\{I_{up}\} = \text{Repeat}(\mathcal{F}\{I\}) \cdot \mathcal{F}\{K_{up}\}.
 \label{eq:uniup2}
\end{equation}

% Denote $\hat{I}$ as the zero-interleaved $I$, and $\hat{I}(x,y)=0$ when $x$ or $y$ is odd. 
% \begin{equation}
%   \begin{aligned}
% \mathcal{F}_{\hat{I}}(u,v) &=\sum_{x=0}^{2H-1}\sum_{y=0}^{2W-1} \hat{I}(x,y) \cdot e^{-i 2 \pi (ux/2H+vy/2W)} \\
% &=\sum_{x=0}^{H-1}\sum_{y=0}^{W-1} \hat{I}(2x,2y) \cdot e^{-i 2 \pi (2ux/2H+2vy/2W)} \\
% &= \sum_{x=0}^{H-1}\sum_{y=0}^{W-1} \hat{I}(2x,2y) \cdot e^{-i 2 \pi (2ux/2H+2vy/2W)} \cdot e^{-i 2 \pi (x+y)} \\
% &= \sum_{x=0}^{H-1}\sum_{y=0}^{W-1} \hat{I}(2x,2y) \cdot e^{-i 2 \pi (2(u+H)x/2H+2(v+W)y/2W)} \\
% &= \mathcal{F}_{\hat{I}}(u+H,v+W)
%   \end{aligned}
% \end{equation}
% for $u=0,1,,2,\dots,H-1$ and  $v=0,1,,2,\dots,W-1$. Similarly, $\mathcal{F}_{\hat{I}}(u,v)=\mathcal{F}_{\hat{I}}(u+H,v)$, and $\mathcal{F}_{\hat{I}}(u,v)=\mathcal{F}_{\hat{I}}(u,v+W)$. The equivalence relationship results in spectrum replicas as shown in Figure~\ref{fig:}. 
% Interpolation + convolution was proposed as an alternative to avoid transposed convolution artifacts. However, through our analysis, the artifacts would still exist. 

% A Large-Scale Study on Regularization and Normalization in GANs
% Batch normalization and instance normalization are commonly used by normalizing across the batch or instance. 
\noindent \textbf{Normalization.}
The Normalization operation solves the problem of internal covariance shift, and improves the stability of the training process. Let $\mathbf{B}=\{x_1, x_2, ..., x_{S}\}$ be a training mini-batch with batch size $S$, the basic procedure of Batch Normalization contains two steps:
\begin{equation}
    \begin{aligned}
        \hat{x}_i=\frac{x_i-\mu_{\mathbf{B}}}{\sqrt{\sigma_{\mathbf{B}}^2+\epsilon}}, i=1, \ldots, S
    \end{aligned}
\end{equation}
\begin{equation}
    \begin{aligned}
        BNT_{\gamma,\beta}(x_i)=\gamma \hat{x}_{i}+\beta, i=1, \ldots, S
    \end{aligned}
\end{equation}
where $\mu_{\mathbf{B}}$ and $\sigma_{\mathbf{B}}$ are mean and variance caculated over the mini-batch $\mathbf{B}$. $\gamma$ and $\beta$ are two learnable parameters for scaling and shifting the normalized feature map $\hat{x}_i$ in the first step. 
Based on the Discrete Fourier Transform, Batch normalization in the frequency domain could be described as follows~\cite{pan2022learning}:
\begin{equation}
  \begin{array}{l}
\mathcal{F}_{i}^{BNT}(u,v) =\sum_{x=0}^{H-1}\sum_{y=0}^{W-1} (\gamma \frac{I
_{i}(x,y)-\mu_{\mathbf{B}}}{\sqrt{\sigma_{\mathbf{B}}^2+\epsilon}}+\beta) \cdot e^{-i 2 \pi (ux/H+vy/W)} \\
% &= \frac{\gamma}{\sqrt{\sigma_{\mathbf{B}}^2+\epsilon}} \sum_{x=0}^{H-1}\sum_{y=0}^{W-1} (I
% _{i}(x,y)-\mu_{\mathbf{B}}) \cdot e^{-i 2 \pi (ux/H+vy/W)} + \sum_{x=0}^{H-1}\sum_{y=0}^{W-1} \beta \cdot e^{-i 2 \pi (ux/H+vy/W)} \\
\quad= \gamma \frac{\mathcal{F}_{i}(u,v) -\sum_{x=0}^{H-1}\sum_{y=0}^{W-1} \mu_{\mathbf{B}} \cdot e^{-i 2 \pi (ux/H+vy/W)}}{\sqrt{\sigma_{\mathbf{B}}^2+\epsilon}} + \sum_{x=0}^{H-1}\sum_{y=0}^{W-1} \beta \cdot e^{-i 2 \pi (ux/H+vy/W)} \\
\quad= \gamma \frac{\mathcal{F}_{i}(u,v) - \mu_{\mathcal{F},\mathbf{B}}}{\sqrt{\sigma_{\mathbf{B}}^2+\epsilon}} + \beta_{\mathcal{F}},
  \end{array}
\end{equation}
where $\mu_{\mathcal{F},\mathbf{B}}$ and $\beta_{\mathcal{F}}$ are two constants obtained by transforming $\mu_{\mathbf{B}}$ and $\beta$ to the frequency domain. Based on the formulation, we can learn that the implementation of Batch Normalization in the frequency domain has exactly the same form as the time domain, which does not affect the distribution of the frequency spectrum. Rather, it normalizes, \ie, shifts and scales the frequency spectrum of the input signal. Instance Normalization (IN) and Batch Normalization (BN) have the same computational form, except that the mean and variance are derived from an instance instead of a batch. Thus the formulation also applies to IN.

% As explained in Appendix~\ref{sec:a2}, the implementation of Batch Normalization in the frequency domain has exactly the same form as the time domain.

\noindent \textbf{Nonlinear activation functions.} To implement activation functions in the frequency domain, SReLU~\cite{ayat2019spectral} use polynomial fitting to approximate the nonlinear activation functions, and approximate the ReLU function $max(0,x)$ in the form of $c_{0} + c_{1}x + c_{2}x^2$. According to the convolution theorem that multiplication in the spatial domain is equivalent to convolution in the frequency domain, this polynomial can be calculated in the frequency domain as follows:
\begin{equation}
    \begin{aligned}
        \mathcal{F}_{relu}(x) = DC + c_{1} \cdot X + c_{2} \cdot (X \otimes X),
    \end{aligned}
\end{equation}
where $X$ is the DFT spectrum of $x$. According to ~\cite{ayat2019spectral}, the closet coefficients for a standard ReLU are $c_{0}=0, c_{1}=0.3, c_{2}=0.021$. Similarly, Sigmoid, Tanh, and other nonlinear activation functions could also be approximated by the Taylor series and perform corresponding frequency domain transformation as above based on the convolution theorem.


% \inputfigure{fingerprint_vis}
% \inputfigure{fingerprint_vis_conv}
% We can observe that the frequency components of the real image are mostly in the low-frequency region (four corners of the image), and after convolution, the high-frequency region has obvious components. After high-pass filtering, this part of the component becomes more apparent and close to the convolution kernel's frequency spectrum. These findings indicate that convolution adds components similar to the convolution kernel's frequency spectrum in the high-frequency region of the image. 
\section{Understanding the cause of model fingerprints}

% 高频区域能够突显出痕迹 % 为什么会出现这样的可视化的结果？更具体的，什么是模型指纹 
% 类比PRNU，PRNU表现在图像上也是乘性噪声 % 归纳推理
% This conclusion also applies to the feature maps inside the generative model. 
% 不同噪声输入网络中，特征图的频谱方差不大；为什么方差不大，归纳推理；那如果是生成语义差异很大的内容呢？
\subsection{Convolutional layers} 
In the following, we demonstrate how frequency domain transformation of network components produces uniform fingerprints in the generated images. We first consider the convolutional layer. For N convolutional layers with $3\times3\times3\times3$ kernels that reconstruct an input RGB image with very small residual noise. We observe that it is easy to train a N-way CNN classifier to determine a convolved image belongs to which convolutional layer, which means even convolutional layers with different parameters leave different fingerprints on the convolved images. 

To investigate how convolutional layers leave fingerprints in the frequency domain, we show in Figure~\ref{fig:cause_of_fingerprint} (a-c) the averaged DFT spectrum (not shifted) of the R channel of 1000 celebA images, convolved celebA images (each row represents a convolution layer), and high-pass filtered convolved images. Figure~\ref{fig:cause_of_fingerprint} (d) shows the averaged DFT spectrum of three zero-padded convolution kernels related to the R channel. It's evident that the artifacts of convolution layers are visible in the average maps and become dominant after high-pass filtering, and show consistency with the average spectrum of convolution kernels. We also verified the universality of high-frequency features in every single image by calculating the similarity between the high-frequency spectrum of convolved images and the averaged spectrum of convolutional kernels, reaching an average of 0.85 cosine similarity. This further confirms that the convolutional layer leaves uniform high-frequency artifacts on each image.

It's evident that the high-frequency artifacts are caused by the pointwise multiplication with convolution kernel spectrums. According to Eq.~\ref{eq:eq_conv}, for each output channel of the convolved image, its frequency spectrum equals to the sum of the frequency spectra of the input channels multiplied by that of the corresponding convolution kernel. According to~\cite{baradad2021learning, baek2022commonality}, the magnitude of the Fourier transform of many natural images follows a heavey-tailded power law, $1 /(\left|f_x\right|^a+\left|f_y\right|^b)$ with $a$ and $b$ being two random numbers uniformly sampled in the range $[0.5, 0.35]$. Thus, the high-frequency magnitude usually has a small variance, resulting in the high-frequency spectrum of convolved images exhibiting a shape that resembles the averaged convolution kernel spectrum. 
% As stated in ~\cite{baradad2021learning}, the power law also applies to features of neural networks,   
% 对于Upsampling操作，无法直接求得残差图像，对图像频谱进行repeat，然后再乘以一个卷积核的频谱，但是通过对卷积痕迹的分析，我们可以合理推测，upsampling对输入痕迹进行了repeat，然后叠加了upsampling对应的卷积的痕迹
% 上述分析了单个卷积核的指纹形成原因，对于一个z输入的unconditional生成器，没有对应的输入图像进行残差操作去显式地表示模型痕迹，通过以上分析，可以推测生成器的痕迹理论上表示为下式：

\inputfigure{cause_of_fingerprint}


% 对卷积频谱进行随机采样，然后再逆变换；指纹一定有某种空间域的可视化的形式，因为肉眼确实可以看到平均后的图像有噪声
% Fingerprints are freq distributions or certain freqs with certain values. If they are certain values, the spatial fingerprints are periodic signals. 

\subsection{Verifying on independent componenets} 
To verify the above analysis on individual components, we use a simple visualization method to 
show how network components leave fingerprints in the frequency domain. Similarly, We first train $N$ networks made of the basic generative component that can reconstruct images from their inputs. Then we train another classifier that can tell which of the $N$ networks produced an image. This classifier has only one layer that connects all the pixels to the class labels. Then, each part of the weight with shape $\{H,W\}$ represents the key components in the image that are helpful for tracing back to the input image of the corresponding category, which could be named as the "frequency fingerprints" of each network.

% We first consider the simple case of convolution with one single channel. We visualize the frequency fingerprints of convolution layers of different kernel weights and kernel sizes in Figure~\ref{fig:fingerprint_vis_conv}. We surprisingly observe that the classifier weights show consistency with the DFT spectrum of zero-padded kernel weights. This consistency indicates that the impact of the kernel weight in the frequency domain is manifested in the spectrum of the output signal.
% Besides, a larger convolution kernel size results in more complicated model fingerprints. 
% The last column displays the averaged convolution kernel spectrum of all input channels.
% This is because the input to each convolution operation is different, which affects the significance of convolution kernels on the output image. This phenomenon also suggests that the traces left by convolution layers of multiple channels are semantically related.
\inputfigure{convs}
\noindent \textbf{Convolution.} In the case of multiple channels, we consider convolution with a kernel size of $\{3,3,3,3\}$ that reconstructs an RGB image. As the formation process of each channel in the output image is similar, we only extract the output image of the first R channel for fingerprint visualization. Figure~\ref{fig:} shows the DFT spectrums of kernel weights associated with the R channel and visualized fingerprints. We can observe that the fingerprints are shown as the weighted sum of the convolution spectrums. 

% \inputfigure{conv_up}
\noindent \textbf{Upsampling.} We have discussed above that upsampling operation in the frequency domain could be summarized as replicating the frequency spectrum of the input signal and pointwise multiplication with a convolution spectrum dependent on the upsampling type. Thus the fingerprints of the input signal undergo a similar process as well. 
We visualize the fingerprints of ``Conv+Up`` blocks reconstructing the input image with different upsampling types. Accordingly, we also plot the spectrum transform process. As shown in Figure~\ref{fig:}, the fingerprints of ``Conv+Up`` blocks show consistency with the spectrum. We could also observe the difference between different upsampling types. 

\inputfigure{conv_act_norm}
\noindent \textbf{Normalization and activation functions.} We visualize the fingerprints "Conv+Norm" and "Conv+Act" blocks with different normalization and activation types in Figure~\ref{fig:}. As seen, different types of activation and normalization with different parameters shrink or amplify the fingerprints without influencing the distribution.

\subsection{Verifying on GAN and Diffusion models.}
Similarly, we also verify this on features of the real generative models. 
\yty{multiple blocks, and get the final visualization result}





