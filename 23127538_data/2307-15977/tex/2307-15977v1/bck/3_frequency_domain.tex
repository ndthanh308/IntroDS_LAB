\clearpage
\section{How frequency discrepancy are produced}

% \wdd{You need to define what is model fingerprint, and justify why it can be better understood in frequency domain}
Based on theorems of signal processing, image convolution can be replaced by straightforward element-wise multiplication, which implies that model fingerprints could be better understood in the frequency domain. In this section, we delineate how each component of the generator operates in the frequency domain, and through visualization, we show how these components produce and influence frequency patterns. \textbf{Note that the DFT frequency spectrums in this section are not shifted to better observe the high-frequency regions. Thus, the corner regions correspond to low frequencies, while the center regions correspond to high frequencies.}

% \subsection{Mode}
% \inputfigure{noise_pipeline}
% Note that to satisfy the convolution theorem, we need to first zero-pad both $I$ and $K$ to a size of $(H+k-1, W+k-1)$.
\subsection{Generative architecture revisited}

\subsection{Convolution}
For an input feature map $X \in \mathbb{R}^{C_1\times H\times W}$ and a convolution layer with kernels $K \in \mathbb{R}^{C_1 \times C_2 \times k \times k}$ with stride one, where $C_1$, $C_2$, and $k$ are input channel number, output channel number, and kernel size, respectively. We first consider a simple case with single input and output channel, \ie, $C_1=1$ and $C_2=1$. According to the Convolution Theorem~\cite{oppenheim1999discrete}, the spatial convolution operation equals to multiplication operation in the frequency domain:
\begin{equation}
    \mathcal{F}\{X \otimes K\} = \mathcal{F}\{X\} \cdot \mathcal{F}\{K\}, 
\label{eq:conv1}
\end{equation}
where $\mathcal{F}\{X\}$ and $\mathcal{F}\{K\}$ are the discrete Fourier transform (DFT) of zero-padded $X$ and $K$. In the case of multiple channels, the Convolution Theorem still stands for the convolution computation of each channel:
\begin{equation}
    \mathcal{F}\{O_{j}\} = \mathcal{F}\{\sum_{i=0}^{C_1} X_{i} \otimes K_{i,j}\} = \sum_{i=0}^{C_1} \mathcal{F}\{X_{i}\} \cdot \mathcal{F}\{K_{i,j}\},
\label{eq:conv2}
\end{equation}
where $O_{j}$ is the $j$-th channel of the output feature map, $X_{i}$ is the $i$-th channel of the input feature map, $K_{i,j}$ is the $(i,j)$-th channel of $K$.

% \inputfigure{conv_multiple_ks}
% \inputfigure{conv}
% 1. uniform pattern of Gaussian noise
% 2. uniform pattern of feature maps
% 3. why leaves uniform pattern, why low variance regions regardless of input signals.
% This phenomenon also suggests that when different latent codes are sent into the generator (or decoder), convolutions would produce similar patterns in the frequency domain.
% Figure~\ref{fig:conv}(b) further shows the convolution layer with multiple channels operating on intermediate features in a well-trained StyleGAN2 face generator. The input feature maps exhibit low variance in the high-frequency regions due to the low-pass filtering applied in the previous layers. As a result, the spectrum of the output image is similar to the averaged spectrum of convolution kernels.

% \noindent \textbf{Uniform frequency pattern caused by convolution.} We first consider the simple case of a single convolution layer. Figure~\ref{fig:conv}(a) shows the convolution operation on Gaussian noises. Observing the DFT spectrum of output noises, uniform frequency patterns are obvious and exihit as the shape of the DFT spectrum of the zero-padded convolution kernel. It's intuitive that this pattern is caused by the pointwise multiplication with the spectrum of the convolution kernel. We then show in Figure~\ref{fig:conv}(b) that the variation of latent code would cause more semantic changes in low-frequencies regions while lower variation in high frequencies.  as shown in Figure~\ref{fig:conv}(a)
\noindent \textbf{Uniform frequency pattern caused by convolution.} We first consider the simple case of a single convolution layer operating on 2D Gaussian noises. The spectrum of Gaussian noise still displays like Gaussian noise. Thus, uniform frequency patterns would exist in the spectrum of all output noises that are convolved by this layer and exhibit the shape of the zero-padded convolution kernel's spectrum according to Eq.~\ref{eq:conv1}. This indicates that when different latent codes are sampled into the generator (or decoder), convolutions would produce similar frequency distribution patterns. 

\inputfigure{conv_varying_noise}
% Figure~\ref{fig:conv_varying_noise} further shows a case of a convolution layer with multiple channels operating on intermediate features in a well-trained StyleGAN2 face generator. As seen, varying the latent code, the spectrums of features Figure~\ref{fig:conv_varying_noise}(b) output by the same convolutional layer are consistently similar to the averaged spectrum of convolution kernels (Figure~\ref{fig:conv_varying_noise}(c), $mean{}$ in Eq.~\ref{eq:conv2}). We also calculated the mean and variance map in  Figure~\ref{fig:conv_varying_noise}(c)(d). As seen, the mean map is similar to Figure~\ref{fig:conv_varying_noise}(c) as expected. While the variance map (Figure~\ref{fig:conv_varying_noise}(d)) shows low in the high-frequency region and high in the low-frequency region. This indicates that the variation of latent code mostly changes the semantics in the low-frequency regions, while keep high-frequency patterns caused by convolutional filters consistent. 

Fig.~\ref{fig:conv_varying_noise} further shows the case of a convolution layer with multiple channels operating on features from the last block of a well-trained StyleGAN2~\cite{karras2020stylegan2} face generator. See Fig.~\ref{fig:conv_varying_noise}b and ~\ref{fig:conv_varying_noise}e, varying the latent code sent into the generator, the spectrums of feature maps output by the same convolutional layer show uniform frequency patterns and are consistently similar to the averaged kernel spectrum of the convolution layer generating them. We also plot the mean and variance map of feature spectrums varying the latent code in Fig.~\ref{fig:conv_varying_noise}c and ~\ref{fig:conv_varying_noise}d. The mean map is similar to the convolution spectrum as expected. The variance map exhibit low in the high-frequency region and high in the low-frequency region. This indicates that the variation of latent code would more significantly alter low-frequency components while keeping high-frequency patterns consistent. Thus, generated images from the same generator would share similar high-frequency patterns despite their differences in low-frequency semantics. We also emphasize that this uniform frequency pattern in high-frequencies is observed in common generators that generate low-frequency semantics. However, for other specific generators such as texture generation models, this pattern may manifest in different frequency components.

% It's intuitive that this pattern is caused by the pointwise multiplication with the spectrum of the convolution kernel. We then show in Figure~\ref{fig:conv}(b) that the variation of latent code would cause more semantic changes in low-frequencies regions while lower variation in high frequencies.  
% \noindent \textbf{Influence of kernel size and padding mode.} 

\subsection{Upsampling}
 % as shown in Figure~\ref{fig:}.  shown in Figure~\ref{fig:conv_up}
% As explained in appendix~\ref{sec:a1}, 
Common types of upsampling layers are deconvolution (or transposed convolution) and interpolation upsamplers such as nearest neighbor and bilinear. Deconvolution is equivalent to interleaving the input features with 0â€™s and applying a standard convolutional operation. For interpolation upsamplers, the interpolated pixels are linearly dependent on their neighbors, thus they could also be regarded as a combined operation including zero-interleaving and a convolution operation with fixed parameters. Then we could summarize upsampling operations into a unified formalization:
\begin{equation}
 X_{up} = \text{ZeroInter}(X) \otimes K_{up},
 % X_{up} = ZI(X) \otimes K_{up},
 \label{eq:uniup}
\end{equation}

where $\text{ZeroInter}$ is interleaving the input feature $X$ with 0's, and $K_{up}$ is a convolution kernel. For deconvolution, $K_{up}$ is learnable. For nearest neighbor and bilinear upsampling, $K_{up}$ has a corresponding fixed weight, see Fig.~\ref{fig:conv_up}c. 

% It's intuitive that the frequency pattern of the input signal undergoes a similar process as the equation, \ie, repeat first and multiplied by the fixed pattern of the upsampling kernel's spectrum. Figure~\ref{fig:conv_up} shows the procedure.

% \noindent \textbf{Upsampling artifact visualization.}
% The frequency pattern of the input signal undergoes a similar process as above, \ie, repeat first and multiplied by the fixed pattern of the upsampling kernel's spectrum. We visualize the fingerprints of ``Conv+Up`` blocks reconstructing the input image with different upsampling types. Accordingly, we also plot the spectrum transform process. As shown in Figure~\ref{fig:}, the fingerprints of ``Conv+Up`` blocks show consistency with the spectrum. We could also observe the difference between different upsampling types. 
 % while zero insertion is prone to producing checkerboard artifacts in the generated images
\inputfigure{conv_up}
\noindent \textbf{Multiplies of $1/2^{n}$ grids caused by spectrum replication.} Adopting the DFT transform, it's easy to obtain that zero-interleaving in the spatial domain brings about spectrum replicas in the frequency domain. Then we could summarize upsampling operation in the frequency domain as replicating the frequency spectrum of the input signal and pointwise multiplication with a corresponding upsampling kernel's spectrum:
\begin{equation}
 \mathcal{F}\{X_{up}\} = \text{Repeat}_{2,2}(\mathcal{F}\{X\}) \odot \mathcal{F}\{K_{up}\},
 \label{eq:uniup2}
\end{equation}
where $\text{Repeat}_{2,2}$ means repeat the spectrum of $X$ along two frequency dimensions by two times. See Fig.~\ref{fig:conv_up}, zero-interleaving in spatial makes low-frequency components (corners) shift to high-frequency regions (center) after spectrum replication, resulting in a grid-like pattern. The upsampling kernel's spectrums of nearest and bilinear would suppress these grids as they are low in the central regions in nature. While as deconvolution kernels are learned without specific constraints, they could not sufficiently suppress high-frequency components like nearest and bilinear upsampling and thus leaves stronger grids. 

\noindent \textbf{Discussion on the influence of upsampling type.} About the relationship between generation artifacts and upsampling type, existing works have some empirical observations: Recent works~\cite{odena2016deconvolution, wojna2019devil} have found that the Up+Conv form of upsampling produces fewer artifacts compared to deconvolution. Chandrasegaran\etal~\cite{chandrasegaran2021closer} empirically show that the high-frequency discrepancy of fake images can be avoided by a minor architecture change in the last upsampling operation, \ie, change deconvolution to nearest or bilinear upsampling. Schwarz\etal~\cite{schwarz2021frequency} draw the conclusion that bilinear and nearest neighbor upsampling bias the generator towards predicting little high-frequency content. Our analysis above could also provide a unified explanation for why nearest and bilinear produce fewer artifacts than deconvolution.  


\subsection{Normalization and nonlinear activation functions}

% \inputfigure{act_norm2}
Common types of normalization are Instance Normalization (IN) and Batch Normalization (BN), which have the same computational form, except that the mean and variance are derived from an instance or a batch. As described in ~\cite{pan2022learning}, the implementation of BN in the frequency domain has exactly the same form as the time domain. Thus, the normalization layer would normalize, shift and scale the frequency pattern of the input signal. 

Nonlinear activation functions are harder to analyze. An approach is to apply Taylor series approximation to transform nonlinear activation functions as polynomials, and perform DFT transform based on the convolution theorem that multiplication in the spatial domain is equivalent to convolution in the frequency domain. For example, SReLU~\cite{ayat2019spectral} uses polynomial fitting to approximate the ReLU function $max(0,x)$ in the form of $0.3x + 0.021x^2$, which can be calculated in the frequency domain as 
$0.3X + 0.021 \cdot (X \otimes X)$, where $X$ is the DFT spectrum of $x$.   


\subsection{Frequency pattern attenuation over generative blocks}






% \subsection{Summary}

% Thus, this procedure would generate novel frequencies by the polynomial terms. 

% \wdd{add author's name or method name. Don't just put a reference number here. Fix other similar issues.}
% \wdd{How good is this approximation? Can all activation functions be approximated using this method? The example in your previous slides is not a very close approximation. People may question this.}
%  \wdd{DC,x and X are not defined. x was training samples in Normalization paragraph. Be careful with the notation.} 
% \begin{equation}
%     \begin{aligned}
%         \mathcal{F}_{relu}(x) = DC + c_{1} \cdot X + c_{2} \cdot (X \otimes X),
%     \end{aligned}
% \end{equation}
% where $X$ is the DFT spectrum of $x$. According to ~\cite{ayat2019spectral}, the closet coefficients for a standard ReLU are $c_{0}=0, c_{1}=0.3, c_{2}=0.021$. Similarly, Sigmoid, Tanh, and other nonlinear activation functions could also be approximated by the Taylor series and perform corresponding frequency domain transformation as above based on the convolution theorem.

% % A Large-Scale Study on Regularization and Normalization in GANs
% % Batch normalization and instance normalization are commonly used by normalizing across the batch or instance. 

% The Normalization operation solves the problem of internal covariance shift, and improves the stability of the training process. Let $\mathbf{B}=\{x_1, x_2, ..., x_{S}\}$ be a training mini-batch with batch size $S$, the basic procedure of Batch Normalization contains two steps:
% \begin{equation}
%     \begin{aligned}
%         \hat{x}_i=\frac{x_i-\mu_{\mathbf{B}}}{\sqrt{\sigma_{\mathbf{B}}^2+\epsilon}}, i=1, \ldots, S
%     \end{aligned}
% \end{equation}
% \begin{equation}
%     \begin{aligned}
%         BNT_{\gamma,\beta}(x_i)=\gamma \hat{x}_{i}+\beta, i=1, \ldots, S
%     \end{aligned}
% \end{equation}
% where $\mu_{\mathbf{B}}$ and $\sigma_{\mathbf{B}}$ are mean and variance caculated over the mini-batch $\mathbf{B}$. $\gamma$ and $\beta$ are two learnable parameters for scaling and shifting the normalized feature map $\hat{x}_i$ in the first step. 
% Based on the Discrete Fourier Transform, Batch normalization in the frequency domain could be described as follows~\cite{pan2022learning}:
% \begin{equation}
%   \begin{array}{l}
% \mathcal{F}_{i}^{BNT}(u,v) =\sum_{x=0}^{H-1}\sum_{y=0}^{W-1} (\gamma \frac{I
% _{i}(x,y)-\mu_{\mathbf{B}}}{\sqrt{\sigma_{\mathbf{B}}^2+\epsilon}}+\beta) \cdot e^{-i 2 \pi (ux/H+vy/W)} \\
% % &= \frac{\gamma}{\sqrt{\sigma_{\mathbf{B}}^2+\epsilon}} \sum_{x=0}^{H-1}\sum_{y=0}^{W-1} (I
% % _{i}(x,y)-\mu_{\mathbf{B}}) \cdot e^{-i 2 \pi (ux/H+vy/W)} + \sum_{x=0}^{H-1}\sum_{y=0}^{W-1} \beta \cdot e^{-i 2 \pi (ux/H+vy/W)} \\
% \quad= \gamma \frac{\mathcal{F}_{i}(u,v) -\sum_{x=0}^{H-1}\sum_{y=0}^{W-1} \mu_{\mathbf{B}} \cdot e^{-i 2 \pi (ux/H+vy/W)}}{\sqrt{\sigma_{\mathbf{B}}^2+\epsilon}} + \sum_{x=0}^{H-1}\sum_{y=0}^{W-1} \beta \cdot e^{-i 2 \pi (ux/H+vy/W)} \\
% \quad= \gamma \frac{\mathcal{F}_{i}(u,v) - \mu_{\mathcal{F},\mathbf{B}}}{\sqrt{\sigma_{\mathbf{B}}^2+\epsilon}} + \beta_{\mathcal{F}},
%   \end{array}
% \end{equation} 
% % \wdd{u and v are not defined.}
% where $\mu_{\mathcal{F},\mathbf{B}}$ and $\beta_{\mathcal{F}}$ are two constants obtained by transforming $\mu_{\mathbf{B}}$ and $\beta$ to the frequency domain. Based on the formulation, we can learn that the implementation of Batch Normalization in the frequency domain has exactly the same form as the time domain, which does not affect the distribution of the frequency spectrum. Rather, it normalizes, \ie, shifts and scales the frequency spectrum of the input signal. Instance Normalization (IN) and Batch Normalization (BN) have the same computational form, except that the mean and variance are derived from an instance instead of a batch. Thus the formulation also applies to IN.

% % \noindent \textbf{Visualization.}

% % \noindent \textbf{Fingerprint visualization.}
% % Normalization and nonlinear activation functions transform the frequency artifacts as they transform the input spectrum.

% % As explained in Appendix~\ref{sec:a2}, the implementation of Batch Normalization in the frequency domain has exactly the same form as the time domain.

% \subsection{Nonlinear activation functions.}
% % \wdd{How good is this approximation? Can all activation functions be approximated using this method? The example in your previous slides is not a very close approximation. People may question this.}, According to the convolution theorem that multiplication in the spatial domain is equivalent to convolution in the frequency domain, 
% % \wdd{add author's name or method name. Don't just put a reference number here. Fix other similar issues.}
%  % \wdd{DC,x and X are not defined. x was training samples in Normalization paragraph. Be careful with the notation.}
% To implement activation functions in the frequency domain, 
% an approach is to first apply Taylor series approximation to transform nonlinear activation functions as polynomials, and perform DFT transform based on the convolution theorem that multiplication in the spatial domain is equivalent to convolution in the frequency domain. For example, SReLU~\cite{ayat2019spectral} uses polynomial fitting to approximate the nonlinear activation functions and approximate the ReLU function $max(0,x)$ in the form of $c_{0} + c_{1}x + c_{2}x^2$. This polynomial can be calculated in the frequency domain as follows:
% \begin{equation}
%     \begin{aligned}
%         \mathcal{F}_{relu}(x) = DC + c_{1} \cdot X + c_{2} \cdot (X \otimes X),
%     \end{aligned}
% \end{equation}
% where $X$ is the DFT spectrum of $x$. According to ~\cite{ayat2019spectral}, the closet coefficients for a standard ReLU are $c_{0}=0, c_{1}=0.3, c_{2}=0.021$. Similarly, Sigmoid, Tanh, and other nonlinear activation functions could also be approximated by the Taylor series and perform corresponding frequency domain transformation as above based on the convolution theorem.

% % \noindent \textbf{Fingerprint decay.}
% % \inputfigure{conv_act_norm}
% \noindent \textbf{Visualization.}
% We visualize the frequency patterns of "Conv+Norm" and "Conv+Activation" blocks with different normalization and activation types in Figure~\ref{fig:conv_act_norm}. As seen, different types of normalization with different parameters and activation functions shrink or amplify the frequency patterns without heavily influencing the distribution. 
% % We could also observe the frequency pattern decay phenomenon caused by activation functions. 

% % As seen, ReLU attenuates frequency patterns.

% % \noindent \textbf{Fingerprint decay.}
