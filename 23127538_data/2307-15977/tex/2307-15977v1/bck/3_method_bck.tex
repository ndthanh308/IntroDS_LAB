
\section{Preliminaries}

\noindent \textbf{Discrete Fourier Transform.}
For a 1D signal $f$ of N numbers where each value is denoted by $f(x)$, the discrete Fourier transform for $f$ is:
\begin{equation}
F(u)=\sum_{x=0}^{N-1} f(x) \cdot e^{-i 2 \pi ux/N}, u=0,1,2,\dots,N-1
\end{equation}
Next, we extend the formula to an input image given by the matrix $f \in \mathbb{R}^{M \times N}$, where the pixel values are denoted by $f(x,y)$. The 2D discrete Fourier transform for $f$ is denoted by:
\begin{equation}
F(u,v)=\sum_{x=0}^{M-1}\sum_{y=0}^{N-1} f(x,y) \cdot e^{-i 2 \pi (ux/M+vy/N)}
\end{equation}
for $u=0,1,,2,\dots,M-1$ and  $v=0,1,,2,\dots,N-1$

\noindent \textbf{Convolution Theorem.}
In mathematics, the convolution theorem states that under suitable conditions, the Fourier transform of a convolution of two functions (or signals) is the pointwise product of their Fourier transforms. Formally, give two 2D signals $f(x,y)$ and $h(x,y)$, the discrete Fourier transform for them are $F(u,v)$ and $H(u,v)$, the convolution theorem states that:
\begin{equation}
f(x,y) \otimes h(x,y)= \mathcal{F}^{-1}\{F(u,v) \cdot H(u,v)\}
\end{equation}
where $\mathcal{F}^{-1}$ is the inverse Fourier transform.

\section{Theoretical analysis of model fingerprints}

\subsection{Unfied view of generative architecture}
% \noindent \textbf{GAN}
% \noindent \textbf{VAE}
% \noindent \textbf{Flow}
% \noindent \textbf{Diffusion}
% - GAN, VAE, Flow, Diffusion
% - Ref: 
%   - Are GANs Created Equal? A Large-Scale Study
%   - A Large-Scale Study on Regularization and Normalization in GANs
%   - AdversarialNAS: Adversarial Neural Architecture Search for GAN
% Figure: illustration of the generative architecture
Before diving into analyzing the fingerprints of generative models from a unified view, we first overview the architecture of common generative models.

As shown in Fig [illustration of the generative architecture], the architecture of generative models is constructed based on multiple blocks, and each block doubles the resolution of feature maps. Each block is composed of several components, including convolution, upsampling, activation function, and normalization. These components are combined together based on certain stacking regulations. From a unified view, a block could be constructed as follows:
\begin{equation}
 % \text{Com}(\text{Up}, \text{Conv}, \text{Act}, \text{N}),
  Com(Up, Conv, Act, N),
 \label{eq:framework}
\end{equation}
where $Up$ stands for the upsampling operation including bilinear upsampling, nearest neighbor upsampling, and stride 2 deconvolution in common generators. $Conv$ is the convolution layer, which has parameters including kernel\_size, channel numbers, and padding mode. $Act$ is the activation function including ReLU, PReLU, ELU, Tanh, and so on. $N$ stands for the normalization type including batch normalization, instance normalization, and no normalization. $Com$ stands for the stacking way of components, which includes whether to use a skip connection and pre-activation/post-activation convolution. 

% \subsection{Model fingerprint formalization}
% We formalize the model fingerprint brought by the architecture and weights of CNN generative models. 
% The architecture of CNN generative models is built by basic components such as convolution layer, upsampling layer, activation function, and normalization layer. These components further form basic blocks of generative networks based on certain sequences, and each block double the resolution of feature maps. Several blocks make up the whole architecture. To formalize model fingerprint from bottom to up, we first quantify the effect of independent components on model fingerprints. Then we analyze the effects of component stacking and block stacking.  
% \subsubsection{Independent Components}
% \noindent \textbf{Independent component.}
\subsection{Fingerprint visualization tool}
\label{sec:vis}
We adopt a simple visualization tool similar to the ``model fingerprint" in \cite{} to study how network components impact model fingerprints. Specifically, we first train $N$ auto encoders composed of basic network components. Then we train an N-way classifier to classify images generated by these auto-encoders. The classifier only contains one fully-connected layer. Given the images with size $\{c,h,w\}$ generated the auto-encoders and $c$, $h$, and $w$ are channel number, height, and width of generated images. The weight $W$ of the classifier has a shape of $\{c,h,w,N\}$, which could be divided into $N$ parts $\{W_{0}, W_{1}, \dots, W_{N-1}\}$ and each part corresponds to one class. Given image $I$ generated by the $y$-th auto-encoder, the optimization procedure of the classifier can be formulated as:
\begin{equation}
    \mathcal{L}_{cls}(I,y) = \frac{corr(\mathcal{F}(I),W_{y})}{\sum_{\hat{y}\in\mathbb{Y}}corr(\mathcal{F}(I),W_{\hat{y}})}
\label{eq:cls}
\end{equation}
where $W_{y}$ is part the $y$-th part of $W$. $corr(A,B)=A\odot B$, and $\odot$ is the inner product operation. $\mathcal{F}$ is the discrete Fourier transform. After training the classifier, we visualize $N$ weight parts respectively, each of which reflects how much each frequency component of the generated images by a certain auto-encoder contributes to the attribution to this model. 

\subsection{Theoretical analysis of model fingerprints}
We mathematically characterize how network components influence the frequency of feature maps, and examine how these influences form the model fingerprints and contribute to model attribution.
To analyze model fingerprints from bottom to up, we first quantify the effect of independent components on model fingerprints. Then we analyze the effects of component stacking and block stacking.  

\subsubsection{Convolution layers}
\label{sec:conv}
The convolution layers of different generative architectures vary in channel number, kernel size, and padding mode. 
For an input feature map $I$ with size $C_1 \times N\times N$ and a convolution layer $C$ with size $C_1 \times C_2 \times k \times k$, where $k$, $C_1$, and $C_2$ are kernel size, input channel number and output channel number.

\noindent \textbf{Single channel.}
We first consider a simple case with $C_1=1$ and $C_2=1$.
Let $\mathcal{F}\{I\}$ and $\mathcal{F}\{C\}$ be the DFT transform of $I$ and zero-padded $C$. According to the Convolution Theorem, the spatial convolution operation equals to multiplication operation in the frequency domain:
\begin{equation}
    \mathcal{F}\{\text{Conv}(I)\}=\mathcal{F}\{I \otimes C\} = \mathcal{F}\{I\} \cdot \mathcal{F}\{C\}
\end{equation}
% After the convolution operation, the channel of output image is $C_2$
\noindent \textbf{Visulization of the frequency influence of convolution.} We adopt the visualization tool in Sec.~\ref{sec:vis} to characterize the influence of convolution operation on frequency components contributes to model attribution. Specifically, we train 10 autoencoders and classify their generated images. The classifier weights are visualized in Fig.[]. We surprisingly find that the weights are very similar to the DFT spectrums of padded convolution kernels in each auto-encoder. This consistency indicates the neural network is able to decouple the frequency transform added to input signals. 

\noindent \textbf{Influence of kernel size.} We also visualize the convolution kernel spectrums and classifier weights under different kernel sizes varying from 1,3,5,7. As shown in Fig.[], a larger convolution kernel size results in more complicated model fingerprints.

\noindent \textbf{Multiple channels.}
In the situation when $C_1$ or $C_2$ is not 1, the Convolution Theorem could not be directly applied but could be applied to the convolution operation of each channel. Denote $\text{Conv}(I)_{j}$ as a single channel of the output image with $C_2$ channels, we get
\begin{equation}
    \begin{aligned}
    \mathcal{F}\{\text{Conv}(I)_{j}\} &= \mathcal{F}\{\sum_{i=0}^{C_1}\text{Conv}_{i,j}(I_{i})\} \\
    &= \sum_{i=0}^{C_1} \mathcal{F}\{I_{i} \otimes C_{i,j}\} \\
    &= \sum_{i=0}^{C_1} \mathcal{F}\{I_{i}\} \cdot \mathcal{F}\{C_{i,j}\}
    \end{aligned}
\end{equation}
Similar to the visualization process above, we construct autoencoders whose convolutional layer with a $\{3,3,3,3\}$ kernel. Fig.[] visualize the convolution kernel spectrums and classifier weights.
The last column is the averaged convolution kernel spectrum of all input channels. We can note that the classifier weights are no longer to the convolution kernel spectrum, but rather shown as the weighted sum of convolution spectrums. This is because the input to each convolution operation is different, as such influence the significance of convolution kernels on the output image. 
The phenomenon also suggests that traces left by convolution layers of multiple channels are semantic-related. 

% According to~\cite{}, the Convolution Theorem for spatial convolution stand under certain conditions: 1) the $\otimes$ operation is a standard convolution operation, which means the padding mode should be ``same" and the stride size should be 1. 2) the two signals should be zero-padded to size $N+k-1 \times N+k-1$ to avoid frequency alias. 
% According to the equation, if $I$ is a random signal without certain characteristics, 
% Since in generative architectures, $k \ll N$, the most numbers in padded $C$ is 0, which brought some characteristics to $\mathcal{F}\{C\}$ in the frequency domain:
% \noindent \textbf{The influence of kernel size.}
% \noindent \textbf{Discussion on semantic-related characteristic.}


\subsubsection{Upsampling}

It is verified in previous works that upsampling layers would cause artifacts like checkerboard pattern~\cite{}, yet the effects of each type of upsampling layer are always discussed independently~\cite{}. In this paper, we provide a unified view of the frequency impacts by common types of upsampling layers, which are transposed convolution and interpolation upsamplers such as nearest neighbor, bilinear, and bicubic. We first preview these upsampling operations.

\noindent \textbf{Transposed convolution.}
Transposed convolutions are the most commonly used upsampling layers and are also sometimes referred to as `deconvolution' or `upconvolution'. Transposed convolution is equivalent to interleaving the input features with 0’s and applying a standard convolutional operation. The transposed convolution is illustrated in Fig.~\ref{fig:}.

\noindent \textbf{Interporation upsampling.}
The interpolated pixels are linearly dependent on their neighbors, thus upsampling operations like nearest neighbor and bilinear upsample could be regarded as a combined operation including zero-padding and a convolution operation with fixed parameters. The fixed parameters depend on the type of upsampling. 

$\bullet$ \textbf{Nearest neighbour upsample.}
For nearest neighbor upsampling increasing the resolution by factor 2, the convolution has a 2$\times 2$ kernel size with parameters as:
\begin{equation}
\begin{bmatrix}
  1 & 1 \\
  1 & 1 
\end{bmatrix}
\label{eq:nnup}
\end{equation}
The nearest neighbor upsampling is illustrated in Fig.\ref{fig:}.

$\bullet$ \textbf{Bilinear upsample.}
For bilinear upsampling increasing the resolution by factor 2, the convolution has a $3 \times 3$ kernel size with parameters as:
\begin{equation}
\begin{bmatrix}
  \frac{1}{4} & \frac{1}{2} & \frac{1}{4} \\
  \frac{1}{2} & 1 & \frac{1}{2} \\
  \frac{1}{4} & \frac{1}{2} & \frac{1}{4} 
\end{bmatrix}
\label{eq:biup}
\end{equation}
The bilinear upsampling is illustrated in Fig.\ref{fig:}.

$\bullet$ \textbf{Bicubic upsample.}

\noindent \textbf{Unified formalization of upsampling operations.}
Given the above discussions, we could summarize transposed convolution, nearest neighbor upsampling and nearest upsampling into a unified formalization:
\begin{equation}
 I_{\text{up}} = \text{Conv}(\text{ZeroPad}(I))
 \label{eq:uniup}
\end{equation}
where $\text{ZeroPad}$ is interleaving the input feature $I$ with 0's, and $\text{Conv}$ is a convolution operation. For transposed convolution, $\text{Conv}$ is learnable. For nearest neighbor upsampling, $\text{Conv}$ has a $3 \times 3$ kernel size as shown in Eq.\ref{eq:nnup}. For bilinear upsampling, $\text{Conv}$ has a $2 \times 2$ kernel size as shown in Eq.\ref{eq:biup}.

\noindent \textbf{Frequency influence of upsampling operations.}
Given the unified formalization of upsampling operations in Eq.\ref{eq:uniup}, we provide a unified analysis of the frequency influence brought by upsampling based on Convolution Theorem. 
% For a more intuitive understanding, we start from 1D signal. Let $I$ be a 1D signal of N numbers, and $\mathcal{C}$ is a convolution kernel with $1\times 3$ as kernel size. We plot DFT magnituide spectrum in Fig.\ref{fig:dft_up}, we could observe that $\text{ZeroPad}$ doubles the spectrum of the original signal, and $\text{Conv}$ operation applies calibration to the doubled signal. Since bilinear and neareat neighor upsampling have fixed $\text{Conv}$ parameters, the shape of the calibration brought by them is also fixed. 
% or a 2D image signal, the above conclusions still stand as shown in Fig.~\ref{dft_up_2D}.
For a 2D image signal $I \in \mathbb{R}^{M \times N}$, the 2D discrete Fourier transform for $I$ is denoted by:
\begin{equation}
F(u,v)=\sum_{x=0}^{M-1}\sum_{y=0}^{N-1} I(x,y) \cdot e^{-i 2 \pi (ux/M+vy/N)},
\end{equation}
for $u=0,1,,2,\dots,M-1$ and  $v=0,1,,2,\dots,N-1$
ZeroPad function increases $I$'s spatial resolution by factor 2. Denote $\hat{I}$ as ZeroPad($I$), and $\hat{I}(x,y)=0$ when $x$ or $y$ is add, we get
\begin{equation}
  \begin{aligned}
\hat{F}(u,v) &=\sum_{x=0}^{2M-1}\sum_{y=0}^{2N-1} \hat{I}(x,y) \cdot e^{-i 2 \pi (ux/2M+vy/2N)} \\
&=\sum_{x=0}^{M-1}\sum_{y=0}^{N-1} \hat{I}(2x,2y) \cdot e^{-i 2 \pi (2ux/2M+2vy/2N)},
  \end{aligned}
\end{equation}
for $u=0,1,,2,\dots,2M-1$ and  $v=0,1,,2,\dots,2N-1$. 
\begin{tiny}
\begin{equation}
  \begin{aligned}
    \hat{F}(u+M,v+N) & = \sum_{x=0}^{M-1}\sum_{y=0}^{N-1} \hat{I}(2x,2y) \cdot e^{-i 2 \pi (2(u+M)x/2M+2(v+N)y/2N)} \\
    &= \sum_{x=0}^{M-1}\sum_{y=0}^{N-1} \hat{I}(2x,2y) \cdot e^{-i 2 \pi (2ux/2M+2vy/2N)} \cdot e^{-i 2 \pi (x+y)} \\
    &= \sum_{x=0}^{M-1}\sum_{y=0}^{N-1} \hat{I}(2x,2y) \cdot e^{-i 2 \pi (2ux/2M+2vy/2N)} \\
    &= \hat{F}(u,v)
  \end{aligned}
\end{equation}
\end{tiny}
for $u=0,1,,2,\dots,M-1$ and  $v=0,1,,2,\dots,N-1$. Similarly, $\hat{F}(u+M,v)=\hat{F}(u,v)$, and $\hat{F}(u,v+N)=\hat{F}(u,v)$. The equivalence relationship results in duplication of the spectrum as shown in Fig[].

As discussed in Sec.\ref{sec:conv}, the influence of $\text{Conv}$ in the frequency domain is multiplying the spectrum of the convolution kernel weight. As shown in Fig.[], the estimated model fingerprints correspond to the spectrums of convolution weights.  

\subsubsection{Nonlinear activation functions.}

\subsubsection{Normalization.}

\subsubsection{Component stacking.}
Given the discussions above, a Up+Conv+Conv block is not difficult to analyze. In the frequency domain, the spectrum transformation would be the spectrum multiplication of convolution kernels. 
The frequency transformation function could be simplified as:
\begin{equation}
    \text{FT} = 
\end{equation}
As seen, the FT function is determined by.

\subsubsection{Fingerprint decay process}
The above discusses the model fingerprints introduced by a single block, the fingerprints of the former layers could be overlapped by the latter layers with the increase of the number of blocks. To analyze the fingerprint decay process, we.

Including the decaying function into FT, we get the fingerprint transformation function of the whole generative architecture with N layers:
\begin{equation}
    \text{FT} = 
\label{eq:ft_all}
\end{equation}
As Fig[] shows, the estimated model fingerprints are very close to the fingerprints computed by Eq.\ref{eq:ft_all}, justifying the reasonability of the proposed fingerprint transformation function.

% \noindent \textbf{Upsampling + Convolution.}
% Interpolation + convolution was proposed as an alternative to avoid transposed convolution artifacts. However, through our analysis, the artifacts would still exist. 

% We first analyze from 1D convolution computing process. Let $f$ be a 1D signal of N numbers, and $\mathcal{C}$ is a convolution kernel with $1\times 3$ as kernel size. To apply convolution theorem in analysis, we only consider the simplest situation that the stride size is 1, and the padding mode is ``full". 

  % - Upsampling（Unified View）
  % - Convolution
  %   - Single Channel -> Multiple Channel
  %   - Kernel size（3,5,7,...）
  %   - Padding（full, same, valid）
  % - BN
  % - ReLU
% \subsection{Composed Components}


% \noindent \textbf{Composed components}
  % - Composition of A specific sequence
  % - The Influence of Sequence

% \subsection{Composed Block}
% \noindent \textbf{Composed blocks}
  % - Fingerprint decay process
  % - Estimating the number of layers

% % 取个更高大上的名字，类似deep image prior
% \section{Prior-driven fingerprint learning}
% \subsection{Generative model simplification}

% % - Block层数
% %   - Kernel Size
% %   - Upsampling
% %   - Act, BN
% %   - Channel Num


% \subsection{Fingerprint learning on synthetic data}




