% [Existing works mainly take a broader view of the frequency bias of all CNN-generated images, which could be caused by transposed convolution~\cite{zhang2019detecting,durall2020watch} and linear dependencies in the spectrum of convolutional filter\cite{dzanic2020fourier} from existing discussions. The more fine-grained model attribution task appeal for a thorough understanding about the cause of fingerpints for individual generative models.]

\section{Related work}

\noindent \textbf{Frequency bias of generative models.}
% related works: 
% 参考StyleGAN3
% On the Frequency Bias of Generative Models
Existing studies~\cite{chandrasegaran2021closer, durall2020watch, schwarz2021frequency, dzanic2020fourier, khayatkhoei2022spatial} have identified frequency discrepancies between generated and real images and attempted to provide explanations. Some of these studies~\cite{chandrasegaran2021closer, durall2020watch, schwarz2021frequency} attribute this phenomenon to upsampling operations, which generate an excess of high frequencies in the spectral statistics. Other studies~\cite{dzanic2020fourier, khayatkhoei2022spatial} attribute it to linear dependencies in the spectrum of convolutional filters, which impede the learning of high frequencies. However, most studies have only analyzed the differences between real and fake images, with fewer studies examining the differences between images generated by different models.

\noindent \textbf{Generative model attribution.}
Generative image attribution aims to identify the corresponding generative model based on the generated image and can be divided into active attribution~\cite{yu2020responsible, yu2020artificial, kim2020decentralized} and passive attribution~\cite{marra2019gans, yu2019attributing, yang2022aaai}. Active attribution involves injecting fingerprints into generative models through weight modulation or training on fingerprinted datasets, but there is no analysis of how fingerprint information is embedded into generative models. Passive attribution tries to extract the intrinsic fingerprints of generative models. Recent work by Marra \etal~\cite{marra2019gans} uses averaged noise residuals to represent model fingerprints and finds that model fingerprints are periodic. Later works ~\cite{yu2019attributing, yang2022aaai, bui2022repmix} further verify the existence of model fingerprints and achieve high accuracy on a fixed and finite set of models following a closed-set classification formulation. Yang \etal~\cite{yang2023progressive} first considers the open-set model attribution problem, but can still only recognize new models as unknown without identifying them. In our work, we provide empirical explanations for the cause of intrinsic model fingerprints in the frequency domain and move towards attributing unseen models in the open world.
