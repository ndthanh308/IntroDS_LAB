\section{Extra analysis of frequential fingerprint}

\subsection{Upsampling}

\noindent \textbf{Derivation for spectrum replication.}
In Fig. 3(b) of the main text, we observe that zero-interleaving in the spatial domain leads to spectrum replication in the frequency domain. We can derive this phenomenon as follows. Consider a 2D image signal $I \in \mathbb{R}^{M \times N}$, the 2D discrete Fourier transform for $I$ is denoted by:
\begin{equation}
F(u,v)=\sum_{x=0}^{M-1}\sum_{y=0}^{N-1} I(x,y) \cdot e^{-i 2 \pi (ux/M+vy/N)},
\end{equation}
for $u=0,1,,2,\dots,M-1$ and  $v=0,1,,2,\dots,N-1$. When we apply the ZeroInter function to $I$, which interleaves the input image with zeros and increases the spatial resolution by a factor of 2, denoted as $\hat{I}$, we have $\hat{I}(x,y)=0$ when $x$ or $y$ is odd. Consequently, we obtain:
\begin{equation}
  \begin{aligned}
\hat{F}(u,v) &=\sum_{x=0}^{2M-1}\sum_{y=0}^{2N-1} \hat{I}(x,y) \cdot e^{-i 2 \pi (ux/2M+vy/2N)} \\
&=\sum_{x=0}^{M-1}\sum_{y=0}^{N-1} \hat{I}(2x,2y) \cdot e^{-i 2 \pi (2ux/2M+2vy/2N)},
  \end{aligned}
\end{equation}
for $u=0,1,,2,\dots,2M-1$ and  $v=0,1,,2,\dots,2N-1$. Then, we could derive the following equation:
\begin{equation}
  \begin{aligned}
    \hat{F}(u+M,v+N) & = \sum_{x=0}^{M-1}\sum_{y=0}^{N-1} \hat{I}(2x,2y) \cdot e^{-i 2 \pi (2(u+M)x/2M+2(v+N)y/2N)} \\
    &= \sum_{x=0}^{M-1}\sum_{y=0}^{N-1} \hat{I}(2x,2y) \cdot e^{-i 2 \pi (2ux/2M+2vy/2N)} \cdot e^{-i 2 \pi (x+y)} \\
    &= \sum_{x=0}^{M-1}\sum_{y=0}^{N-1} \hat{I}(2x,2y) \cdot e^{-i 2 \pi (2ux/2M+2vy/2N)} \\
    &= \hat{F}(u,v)
  \end{aligned}
\end{equation}
for $u=0,1,,2,\dots,M-1$ and  $v=0,1,,2,\dots,N-1$. Similarly, $\hat{F}(u+M,v)=\hat{F}(u,v)$, and $\hat{F}(u,v+N)=\hat{F}(u,v)$. The equivalence relationship results in duplication of the spectrum.

\noindent \textbf{Discussion on some empirical observations.} About the relationship between generation artifacts and upsampling type, existing works have some empirical observations: Recent works~\cite{odena2016deconvolution, wojna2019devil} have found that the Up+Conv form of upsampling produces fewer artifacts compared to deconvolution. Chandrasegaran\etal~\cite{chandrasegaran2021closer} empirically show that the high-frequency discrepancy of fake images can be avoided by a minor architecture change in the last upsampling operation, \ie, change deconvolution to nearest or bilinear upsampling. Schwarz\etal~\cite{schwarz2021frequency} draw the conclusion that bilinear and nearest neighbor upsampling bias the generator towards predicting little high-frequency content. These studies collectively demonstrate that the utilization of nearest and bilinear interpolation leads to a reduction in the high-frequency components present in the generated images. This phenomenon can be understood by referring to Fig.3(c) in the main text, where it is evident that nearest and bilinear interpolation introduce frequency attenuation as a result of their implementation of low-pass upsampling filters.

\subsection{Normalization and nonlinear activation functions}

\noindent \textbf{Normalization in the frequency domain.} We mention in the main text that Normalization in the frequency domain has exactly the same form as the time domain. We can derive this conclusion as follows. Let $\mathbf{B}=\{x_1, x_2, ..., x_{S}\}$ be a training mini-batch with batch size $S$, the basic procedure of Batch Normalization contains two steps:
\begin{equation}
    \begin{aligned}
        \hat{x}_i=\frac{x_i-\mu_{\mathbf{B}}}{\sqrt{\sigma_{\mathbf{B}}^2+\epsilon}}, i=1, \ldots, S
    \end{aligned}
\end{equation}
\begin{equation}
    \begin{aligned}
        BNT_{\gamma,\beta}(x_i)=\gamma \hat{x}_{i}+\beta, i=1, \ldots, S
    \end{aligned}
\end{equation}
where $\mu_{\mathbf{B}}$ and $\sigma_{\mathbf{B}}$ are mean and variance caculated over the mini-batch $\mathbf{B}$. $\gamma$ and $\beta$ are two learnable parameters for scaling and shifting the normalized feature map $\hat{x}_i$ in the first step. 
Based on the Discrete Fourier Transform, Batch normalization in the frequency domain could be described as follows~\cite{pan2022learning}:
% \begin{equation}
%   \begin{array}{l}
% \mathcal{F}_{i}^{BNT}(u,v) =\sum_{x=0}^{H-1}\sum_{y=0}^{W-1} (\gamma \frac{I
% _{i}(x,y)-\mu_{\mathbf{B}}}{\sqrt{\sigma_{\mathbf{B}}^2+\epsilon}}+\beta) \cdot e^{-i 2 \pi (ux/H+vy/W)} \\
% &= \frac{\gamma}{\sqrt{\sigma_{\mathbf{B}}^2+\epsilon}} \sum_{x=0}^{H-1}\sum_{y=0}^{W-1} (I_{i}(x,y)-\mu_{\mathbf{B}}) \cdot e^{-i 2 \pi (ux/H+vy/W)} + \sum_{x=0}^{H-1}\sum_{y=0}^{W-1} \beta \cdot e^{-i 2 \pi (ux/H+vy/W)} \\
% \quad= \gamma \frac{\mathcal{F}_{i}(u,v) -\sum_{x=0}^{H-1}\sum_{y=0}^{W-1} \mu_{\mathbf{B}} \cdot e^{-i 2 \pi (ux/H+vy/W)}}{\sqrt{\sigma_{\mathbf{B}}^2+\epsilon}} + \sum_{x=0}^{H-1}\sum_{y=0}^{W-1} \beta \cdot e^{-i 2 \pi (ux/H+vy/W)} \\
% \quad= \gamma \frac{\mathcal{F}_{i}(u,v) - \mu_{\mathcal{F},\mathbf{B}}}{\sqrt{\sigma_{\mathbf{B}}^2+\epsilon}} + \beta_{\mathcal{F}},
%   \end{array}
% \end{equation} 
\begin{equation}
  \begin{aligned}
\mathcal{F}_{i}^{BNT}(u,v) &=\sum_{x=0}^{H-1}\sum_{y=0}^{W-1} (\gamma \frac{I
_{i}(x,y)-\mu_{\mathbf{B}}}{\sqrt{\sigma_{\mathbf{B}}^2+\epsilon}}+\beta) \cdot e^{-i 2 \pi (ux/H+vy/W)} \\
&= \frac{\gamma}{\sqrt{\sigma_{\mathbf{B}}^2+\epsilon}} \sum_{x=0}^{H-1}\sum_{y=0}^{W-1} (I_{i}(x,y)-\mu_{\mathbf{B}}) \cdot e^{-i 2 \pi (ux/H+vy/W)} + \sum_{x=0}^{H-1}\sum_{y=0}^{W-1} \beta \cdot e^{-i 2 \pi (ux/H+vy/W)} \\
&= \gamma \frac{\mathcal{F}_{i}(u,v) -\sum_{x=0}^{H-1}\sum_{y=0}^{W-1} \mu_{\mathbf{B}} \cdot e^{-i 2 \pi (ux/H+vy/W)}}{\sqrt{\sigma_{\mathbf{B}}^2+\epsilon}} + \sum_{x=0}^{H-1}\sum_{y=0}^{W-1} \beta \cdot e^{-i 2 \pi (ux/H+vy/W)} \\
&= \gamma \frac{\mathcal{F}_{i}(u,v) - \mu_{\mathcal{F},\mathbf{B}}}{\sqrt{\sigma_{\mathbf{B}}^2+\epsilon}} + \beta_{\mathcal{F}},
  \end{aligned}
\end{equation} 
% \wdd{u and v are not defined.}
where $\mu_{\mathcal{F},\mathbf{B}}$ and $\beta_{\mathcal{F}}$ are two constants obtained by transforming $\mu_{\mathbf{B}}$ and $\beta$ to the frequency domain. Based on the formulation, we can learn that the implementation of Batch Normalization in the frequency domain has exactly the same form as the time domain, which normalizes, shifts, and scales the frequency spectrum of the input signal. Instance Normalization (IN) and Batch Normalization (BN) have the same computational form, except that the mean and variance are derived from an instance instead of a batch. Thus the formulation also applies to IN.

\inputfigure{act_norm}
\noindent \textbf{Visualize the effect of normalization and activation function.}
% In Fig.~\ref{fig:act_norm}, we visualize the averaged spectrum of images generated by Conv+Norm/Activation blocks, where ``Norm" can be batch normalization or instance normalization,  and ``Activation" can be ReLU, Sigmoid, or Tanh. These blocks share the same `Conv' layer. The spectrum of the `Conv' layer is shown in Fig.~\ref{fig:act_norm}(a), and the averaged spectrum directly output by the ``Conv" layer is shown in Fig.~\ref{fig:act_norm}(b). We could notice that normalization and activation functions would strengthen or weaken the frequency pattern brought by the convolution layer. 
Fig.~\ref{fig:act_norm} presents the visualization of the averaged spectrum for images generated by Conv+Norm/Activation blocks. The "Norm" component can be batch normalization or instance normalization, while the "Activation" component can be ReLU, Sigmoid, or Tanh. These blocks share a common Conv' layer. Fig.~\ref{fig:act_norm}(a) displays the spectrum of the `Conv' layer, while Fig.~\ref{fig:act_norm}(b) depicts the averaged spectrum of images directly output by the "Conv" layer. From the figure, we could observe that the normalization and activation functions have the potential to enhance or diminish the frequency pattern introduced by the convolution layer.

% \subsection{Compound frequency pattern within a generative block}
\subsection{Frequency pattern attenuation over generative blocks}

\noindent \textbf{High-frequency component variation curve.}
In Fig.4(h) of the main text, we plot high-frequency component variation throughout the generation process. X-axis means every network component in the sequence of generation, and y-axis means the proportion of high frequencies of feature maps output by each network component. In the following, we detail the calculation process of values on the y-axis. Consider a 2D feature map $I \in \mathbb{R}^{M \times N}$ output by a certain component on the x-axis, the 2D discrete Fourier transform for $I$ is denoted by:
\begin{equation}
F(u,v)=\sum_{x=0}^{M-1}\sum_{y=0}^{N-1} I(x,y) \cdot e^{-i 2 \pi (ux/M+vy/N)},
\end{equation}
Applying azimuthal integration over radial frequencies $\phi$, we get:
\begin{equation}
\begin{aligned}
    A I\left(\omega_k\right)=\int_0^{2 \pi}\left\|F\left(\omega_k \cdot \cos (\phi), \omega_k \cdot \sin (\phi)\right)\right\|^2 \mathrm{~d} \phi,
\end{aligned}
\end{equation}
where $k=0, \ldots, M / 2-1$. As seen, $AI(\omega_{k})$ is a simple but characteristic 1D representation of the 2D Fourier power spectrum, and each value is the radial integral over the 2D spectrum at a radius. Then we calculate of the high-frequency proportion as the sum of values in the latter half of $AI(\omega_k)$ to the sum of all values:
\begin{equation}
\begin{aligned}
    HP\_ratio = \frac{\sum_{k=M/4}^{M/2-1}A I\left(\omega_k\right)}{\sum_{k=0}^{M/2-1}A I\left(\omega_k\right)}
\end{aligned}
\end{equation}


\section{Dataset}
% \subsection{Dataset split}
For the model verification experiment, we randomly selected 10,000 pairs of images from the generated images of the models listed in Tab.1 of the main text. These pairs consisted of 5,000 negative pairs and 5,000 positive pairs. Each pair included two groups of images, with each group containing $N_S$ images. Images in positive pairs originate from the same model, while the group of images in negative pairs are sourced from different models.

For the open-set model identification experiment, the training sample size for each known model varied, including 100, 500, 1000, 2000, and 5000 samples, as depicted in Fig. 8 of the main text. The testing set consists of 1000 non-overlapping samples per known model, and there are 1000 samples for each unknown model.

\section{Experiment}

\subsection{Implementation details}

In our experiment, we set the batch size to 200. For each image in a batch, we randomly select a model from the model pool and transform the input image into a fingerprinted image. The learning rate for the fingerprint extractor is set to $1e^{-4}$. We employ a margin of 0.3 for the triplet loss and assign equal weights of 1 to both the triplet loss and classification loss. Additionally, we utilize a step schedule with a gamma value of 0.9 and a step size of 100.

\subsection{Transferability to Transformer-based generative models}

While our analysis and data synthesis strategy primarily focuses on CNN-based generative models, we observe that the transferability of our pre-trained fingerprint extractor extends to transformer-based generative models as well. Specifically, we consider seven transformer-based face generators in the additional evaluation: Gansformer\_ffhq256~\cite{hudson2021generative}, StyleSwin\_ffhq256~\cite{zhang2022styleswin}, StyleSwin\_celeba256, Styleformer\_celeba64~\cite{park2022styleformer}, TransGAN\_celeba64~\cite{jiang2021transgan}, StyleSwin\_celeba1024, and StyleSwin\_ffhq1024. The suffix "\_ffhq256" indicates that the models were trained on the FFHQ dataset with a resolution of 256 for instance. 

\inputfigure{transformer_tsne}
As depicted in Fig.~\ref{fig:transformer_tsne}(a), our pre-trained fingerprint extractor effectively captures unique fingerprints from these models, even without being trained on any images specifically from transformer-based models. This showcases the transferability of our pre-trained model to novel model types in the open world, encompassing transformer-based generative models. To delve into the underlying factors contributing to this transferability, we conduct an analysis of the architectures of these models. Through the analysis, we observe that while transformer-based models primarily utilize transformer blocks as generative components, they also incorporate convolution or upsampling layers during the generation process. For instance, StyleSwin and Styleformer employ convolution layers to transform features from transformer blocks to RGB images. Gansformer incorporates standard convolution and upsampling after the self-attention layer. TransGAN does not utilize convolution during generation but incorporates interpolation-based upsampling layers. In future research, it would be valuable to investigate the impact of self-attention layers on frequency patterns and explore their influence in greater detail.

% \wdd{Add a table of model verification result.}
\subsection{More model lineage result on Stable Diffusion models}

In the main text, we present visualizations of the feature space for various versions of stable-diffusion models for Text2Image generation. Due to their visual modeling capabilities, they can also be fine-tuned for downstream tasks with few training steps, such as unclip and inpainting in Fig.~\ref{fig:sd_example}. We additionally include the feature visualization results for the two models in Fig.~\ref{fig:transformer_tsne}(b). As seen, the downstream models could be correctly attributed as being derived from stable-diffusion v2 models based on the high similarity of their features.

\inputfigure{sd_example}


\section{Limitations and Future Work}
% \subsection{Attention-based Generative Models}
% How are attention-based generative models different or similar to CNN-based generative models?
% Why does this paper not include attention-based models?
% Can the findings/model/analysis method generalize to attention-based methods?
% \subsection{Robustness of Fingerprint Extractor}
% Is the fingerprint extractor robust to noise/compression/etc.?
% Will the analysis in this paper result in more effective methods to conceal or camouflage fingerprints?
% \subsection{Stability of Fingerprint in Generative Models}
% Is the fingerprint always and only in high frequency area?
% Is the fingerprint signal always strong enough to be recognized?
% Is the fingerprint steady regardless of the input noise and content of output image?
% \subsection{Improving the Design of Generative Models}
% How to design generative models that has stable and unique fingerprint for active protection of  generative models?
% Are model fingerprints in high frequency area redundant byproduct from generative models that should be removed?

In this paper, we make the first attempt to provide empirical explanations for generative model fingerprints in the frequency domain and derive a data synthesis strategy for more generalized fingerprint extraction in the open world. While our method generates compelling results. It is not without limitations. Firstly, inspired by the frequency pattern attenuation phenomenon, we propose to simulate diverse frequency patterns based on generative blocks. Despite the superior transferability and time efficiency, the complex fingerprints from previous blocks may not be simulated sufficiently. In the future, it is worthwhile to further narrow the gap between simulated fingerprints and fingerprints of real generative models while balancing computation costs. Second, our analysis mainly focuses on CNN-based generative models. Although the pre-trained fingerprint extractor shows superior transferability to transformer-based models, it's also worthwhile to study the impact of self-attention layers on frequency patterns for further work, given the successful applications of transformer blocks into generative models. Last, with the drawn observations in this paper, model fingerprints in the high-frequency components could be removed by model stealers. How to design defenses for these attacks remains an open problem. Overall, our research provides valuable insights and techniques for generative model fingerprint extraction, there are several areas for future improvement, including enhancing the realism of simulated fingerprints, studying the impact of self-attention layers, and addressing attacks related to fingerprint removal.









