\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{booktabs}  
\usepackage[table]{xcolor} 
\usepackage{hyperref} 
\usepackage{multicol}

\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021

\newcommand{\inputsection}[1]{\input{Sections/#1}}
\newcommand{\inputfigure}[1]{\input{Figures/#1}}
\newcommand{\inputtable}[1]{\input{Tables/#1}}


\input{packages/defs}
\input{packages/header}
\input{packages/math_commands}

\newcommand{\etal}[0]{~et al.}
% \newcommand{\wrt}[0]{~w.r.t.}
% \newcommand{\ie}[0]{~i.e.}
% \newcommand{\eg}[0]{~e.g.}
\newcommand{\figtext}[1]{{\footnotesize #1}}
\newcommand{\Loss}{\text{Loss}}
\newcommand{\expert}{\operatorname{E}}

\definecolor{lzn}{RGB}{200,0,50}
\newcommand{\lzn}[1]{\textcolor{lzn}{[lzn: #1]}}


\begin{document}

\title{Model Synthesis for Zero-Shot Model Attribution}

\author{
  Tianyun Yang, Juan Cao, Danding Wang, Chang Xu
  
\thanks{Tianyun Yang, Juan Cao, and Danding Wang are with the Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China. \protect E-mail: \{yangtianyun19z,caojuan,wangdanding\}@ict.ac.cn

Chang Xu is with School of Computer Science, Faculty of Engineering, University of Sydney, Australia. E-mail: c.xu@sydney.edu.au 

Code: \href{https://github.com/TianyunYoung/Model-Fingerprint}{https://github.com/TianyunYoung/Model-Fingerprint}
}}

% \author{IEEE Publication Technology,~\IEEEmembership{Staff,~IEEE,}
        % <-this % stops a space
% \thanks{This paper was produced by the IEEE Publication Technology Group. They are in Piscataway, NJ.}% <-this % stops a space
% \thanks{Manuscript received April 19, 2021; revised August 16, 2021.}}

% % The paper headers
% \markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2021}%
% {Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

% \IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.
% the ever-growing number of unknown models limits its application in the open world.
\maketitle

\begin{abstract}

Nowadays, generative models are shaping various fields such as art, design, and human-computer interaction, yet accompanied by challenges related to copyright infringement and content management. In response, existing research seeks to identify the unique fingerprints on the images they generate, which can be leveraged to attribute the generated images to their source models. Existing methods, however, are constrained to identifying models within a static set included in the classifier training, failing to adapt to newly emerged unseen models dynamically. To bridge this gap, we aim to develop a generalized model fingerprint extractor capable of zero-shot attribution, effectively attributes unseen models without exposure during training. Central to our method is a model synthesis technique, which generates numerous synthetic models mimicking the fingerprint patterns of real-world generative models. The design of the synthesis technique is motivated by observations on how the basic generative model’s architecture building blocks and parameters influence fingerprint patterns, and it is validated through two designed metrics that examine synthetic models' fidelity and diversity. Our experiments demonstrate that this fingerprint extractor, trained solely on synthetic models, achieves impressive zero-shot generalization on a wide range of real-world generative models, improving model identification and verification accuracy on unseen models by over 40\% and 15\%, respectively, compared to existing approaches.
\end{abstract}

\begin{IEEEkeywords}
Model Fingerprint, Model Attribution, Model Synthesis, Zero-Shot Adaptation
\end{IEEEkeywords}

\inputsection{intro}
\inputsection{related}
\inputsection{method}
\inputsection{exp}

\bibliographystyle{IEEEtran}
\bibliography{reference.bib}


\vspace{11pt}
% \bf{If you include a photo:}
\vspace{-33pt}
\begin{IEEEbiography}[{% Figure removed}]{Tianyun Yang} is currently pursuing her Ph.D. degree at the Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China. She received her B.E. degree from Wuhan University in 2019. Her research interests lie in AI safety.
\end{IEEEbiography}

% \vspace{11pt}
% \bf{If you will not include a photo:}
% \vspace{-22pt}
\begin{IEEEbiography}[{% Figure removed}]{Juan Cao} is a professor at the Institute of Computing Technology, Chinese Academy of Sciences, where she received her Ph.D. degree in 2008. Her research interests include multimedia content analysis and fake multimedia detection. She has over 90 publications in international journals and conferences including TKDE, TIP, KDD, WWW, CVPR, etc.
\end{IEEEbiography}

% \vspace{11pt}
% \bf{If you will not include a photo:}
% \vspace{-22pt}
\begin{IEEEbiography}[{% Figure removed}]{Danding Wang} received her B.E. degree from Beihang University, China, in 2016. She received her Ph.D. degree from the National University of Singapore, Singapore, in 2021. She is currently an assistant researcher in the Media Synthesis and Forensics Lab of the Institute of Computing Technology, Chinese Academy of Sciences. Her research interests include explainable AI, misinformation detection, and AIGC detection.
\end{IEEEbiography}

% \vspace{-22pt}
\begin{IEEEbiography}[{% Figure removed}]{Chang Xu} is ARC Future Fellow and Associate Professor at the School of Computer Science, University of Sydney. He received the University of Sydney Vice-Chancellor’s Award for Outstanding Early Career Research. His research interests lie in machine learning algorithms and related applications in computer vision. He has published over 100 papers in prestigious journals and top-tier conferences.
\end{IEEEbiography}


\clearpage

\appendix


\subsection{Attribution Robustness Analysis}

\noindent \textbf{Different Generation Configurations.} In the generation phase, diffusion models offer various configurable parameters, such as the sampling step, sampling scheduler, guidance scale, and the resolution of the generated images. These variations in setup can influence the quality of the generated images. To assess whether the extracted fingerprint remains stable amidst these variations, we conducted evaluations on three Stable Diffusion models: SD1.5, SD2.1, and SDXL 1.0. We systematically varied these configurations during image generation. Samplers from DDIM~\cite{song2020denoising}, PNDM, LMS, and Euler~\cite{karras2022elucidating} were used randomly. The guidance scale was uniformly sampled between 5 and 10, while image dimensions varied with widths and heights ranging from 256 to 1024 pixels, and sampling steps from 20 to 50. 
Figure~\ref{fig:vary_configs} illustrates the t-SNE plots of fingerprint embeddings extracted by our fingerprint extractor (Only-Syn), from models under both fixed and varied generation configurations. The results demonstrate that images with varied generation settings exhibit fingerprints as stable as those with fixed settings, underscoring the robustness of our trained fingerprint extractor to variations in diffusion generation configurations.

\noindent \textbf{Image Perturbations.} Images generated by models may undergo various post-processings during transmission on social media. To evaluate the robustness of our fingerprint extractor (Only-Syn), we tested it against the two most common types of post-processing: JPEG compression and crop resizing. For these tests, we utilized 'ImageCompression' and 'RandomResizedCrop' from the Albumentations library, where 'RandomResizedCrop' randomly crops a patch from the image and resizes it back to the original dimensions. As depicted in Figure~\ref{fig:robust}, we charted the model identification accuracy alongside the JPEG compression rate and crop ratio. It was observed that our trained fingerprint extractor is relatively robust to the crop resizing post-processing, yet exhibits sensitivity to JPEG compression. To mitigate this sensitivity, we incorporate them as a form of data augmentation during training. The immunized fingerprint extractor, as depicted by the red line in the results, demonstrates strong robustness against crop resizing and improvement in resilience against JPEG compression. However, the performance does not fully recover to that of the unaltered data under JPEG compression. This is primarily because JPEG compression can significantly alter the spectrum pattern, thereby degrading the original pattern left by the model, as also noted in~\cite{corvi2023intriguing}. Addressing the robustness against JPEG compression will be a focus of the future work. 

\inputfigure{vary_configs}
\inputfigure{robust}

\subsection{Experimental Setups}
When training on synthetic models, the real images used to generated fingerprinted images are firstly resized to 128$\times$128. The batch size is set to 200. The initial learning rate is $1e^{-4}$ with a warm up cosine scheduler. The margin for the triplet loss is 0.3. The total training epoch is 40. 

% \subsection{More t-SNE visualization Results}

% We provide the t-SNE feature extraction results of seen and unseen models by more baseline methods in Figure~\ref{fig:base1}, Figure~\ref{fig:base2}, Figure~\ref{fig:base3}. As shown in these figures, despite these methods could extract distinct fingerprint features from images generated by seen models, the fingerprint extraction ability could hardly generalize to unseen models. However, our method, trained on synthetic models, could generalize well on these models, as shown in Figure~\ref{fig:generalization} in the main text.

% \inputfigure{base1}
% \inputfigure{base2}
% \inputfigure{base3}

\subsection{Model Verification Cases}

We provide some model verification cases produced by our model fingerprint extractor in Figure~\ref{fig:cases_dalle}, Figure~\ref{fig:cases_sd}, and Figure~\ref{fig:cases_civitai}.  Model verification aims at verifying whether two generated images are from the same model by comparing extracted fingerprints. The threshold to make the prediction is empirically set as 0.81, which is the best threshold of model verification on the Text2Image models in Table~\ref{tab:dataset}. \textit{It is important to note that, to evaluate the zero-shot attribution ability in the open world, the fingerprint extractor producing these results is solely trained on synthetic models, without having seen any images from the real generative models tested below.}

\inputfigure{cases_dalle}
\inputfigure{cases_sd}
\inputfigure{cases_civitai}


\subsection{Limitations}

As depicted in Figure~\ref{fig:cvitai_lora} in the main text, the extracted fingerprints from images of LoRA-equiped models are similar to their Base models, which allows us to attribute the LoRA variants to their base models. However, this also indicates a limitation of our method in distinguishing different LoRA variants from the same base model. We leave this more fine-grained model attribution challenge for future work.


\end{document}


