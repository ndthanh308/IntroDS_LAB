% \inputfigure{generalization}
% \section{The Generalization Problem in Model Attribution}
% \label{sec:freq_fingerprint}
\inputfigure{spectrums}
\inputfigure{up_dft}

% \subsection{Preliminary of Model Attribution}
% The goal of model attribution is to pinpoint the originating model of a generated image. Conventional model attribution methods treat this task as a multi-class classification problem. In this setup, a finite training set composed of images generated by a fixed set of models is used, with each image associated with a distinct model ID. These image-model ID pairs serve to train a classifier that learns to associate specific characteristics of images with their corresponding models. 

% In the testing phase, as illustrated in Figure~\ref{fig:intro}(a), the classifier typically operates under a closed-set framework, where it is tasked with identifying the model ID of images generated by the models within the training dataset, relying on the associations it has learned. To handle images generated by new models not included in the training set, some recent studies~\cite{yang2023progressive, abady2024siamese} suggest framing model attribution as an open-set recognition problem. This formulation allows the system to categorize images from unseen models as "unknown", while accurately assigning seen-model images to their respective categories in the meanwhile. 

% Despite these advances, conventional formulation are still limited in real-world scenarios. A key issue is the rapid emergence of new models. The current framework primarily handles these by simply labeling them as "unknown". This limitation hinders the ability of these methods to effectively adapt to continuously evolving generative technologies.


% \subsection{Generalized Model Attribution}

% As illustrated in Figure~\ref{fig:intro}(b), our research aims to develop a more generalized framework for model attribution that addresses limitations present in conventional methods. A critical distinction of our proposed framework is the treatment of images generated by newly developed models. Contrary to traditional approaches, where such images are indiscriminately attributed to a generic "unknown" model class, our framework strives to accurately attribute these images to their specific generating models by comparing the "fingerprint" distances. This involves comparing the test sample's fingerprint against a comprehensive gallery of model fingerprints, which includes both known (seen) and new (unseen) models. To maintain this model gallery, fingerprints for each model are constructed using $N$ samples, with $N$ could be a low number such as 10. This setup enables dynamic updates to the gallery as new models emerge, without requiring these new models to be included in the training of the fingerprint extractor.

% However, we show that existing methodologies struggle in directly adapting to this new framework, particularly when dealing with unseen models. Figure~\ref{fig:generalization}(a) illustrates this issue using t-SNE feature extraction results by a recent model attribution method~\cite{yang2023progressive}, showing that while current model attribution system can effectively distinguish between different seen models, their effectiveness diminishes with unseen models. The primary reason stems from existing methods' reliance on a limited number of models for training the classifier. This narrow scope can severely restrict the classifier's ability to generalize to new, unseen models in real-world applications, where the diversity of generative models is continuously expanding. To address this, an intuitive solution would be to expand the number of models included in the training set. However, it faces significant practical challenges due to finite availability of generative models. The reality is that the scope of models that can be practically collected and labeled is often restricted.  

% To circumvent this bottleneck, we proposed an innovative approach by utilizing synthetic models, to mimic the fingerprint patterns of actual generative models. By this approach, it is possible to significantly broaden the scope of the training data, enriching its diversity, and consequently diminishing the generalization gap.

% % \clearpage
% % \section{Towards an Efficient Model Synthesizer}
% % \clearpage


% \section{A Model-Synthesis-based Model Attribution Method}
\section{Method}

In this work, our primary goal is to design a fingerprint extractor that could generalize to unseen models in the open world. To accomplish this goal, we propose a new approach by utilizing synthetic models, to mimic the fingerprint patterns of real-world generative models. By this approach, we significantly broaden the scope of the training data of fingerprint extractor, and consequently diminish the generalization gap. In the following, we begin with an analysis of the factors influencing the fingerprint patterns of generative models, detailed in Section~\ref{subsec:warm_up}. Based the analysis, we then design our model synthesis strategy in Section~\ref{subsec:model_synthesis}. Finally, in Section~\ref{subsec:fingerprint_extractor}, we use the synthetic models to train the fingerprint extractor to perform the model attribution task.

\subsection{Preliminary Analysis of Model Fingerprint}
\label{subsec:warm_up}

Our study draws inspiration from recent research~\cite{marra2019gans, yu2019attributing, corvi2023intriguing}, which demonstrates that generative models, differing in architecture or parameters, leave unique patterns, termed \emph{model fingerprints}, on their generated images. Despite these fingerprints are typically imperceptible in the spatial domain, they are more evident in the frequency domain. As shown in Figure~\ref{fig:spectrums}, each generative model is characterized by a distinct pattern in the averaged Fourier spectrum. To gain insights for replicating these patterns, we first explore why generative models display unique spectrum patterns. 

Our analysis mainly focuses on the generator (or decoder) part of CNN-based generative models. These generators produce images from either an encoded or a predefined latent vector. Typically, a generator comprises multiple generative blocks that transform the input latent vector into images. Common operations within the generative block involve components such as upsampling, convolution, normalization, and activation function. In the following, we aim to discuss the impact of these basic components and different generative blocks on the spectrum patterns of output images. Before the analysis, we first provide the background about analysis tools that will be employed in this section. 

\textbf{Background.} 
Our analysis is based on the Discrete Fourier Transform (DFT). Specifically, for the input 2D image $I \in \mathbb{R}^{M \times N}$, the discrete Fourier transform for $I$ is denoted by:
\begin{align*}
    &F(u,v)=\sum_{x=0}^{M-1}\sum_{y=0}^{N-1} I(x,y) \cdot e^{-i 2 \pi (ux/M+vy/N)}, \\
    \nonumber
    &\text{ for } \quad  u = 0, \ldots, M - 1, v = 0, \ldots, N-1 
\end{align*}
where $F(u, v)$ denotes the coefficient under the frequency component $u$ and $v$, along the horizontal and vertical axis of the image. These coefficients consist of the so-called Fourier spectrum. 

Sometimes, it is more convenient to consider the reduced 1D power spectrum~\cite{durall2020watch}, which is a simple yet characteristic 1D representation of the 2D Fourier spectrum. Technically, it is got by Azimuthal Integration (AI) over radial frequencies on the 2D Fourier spectrum:
\begin{align}
  &  A I(\omega)=\int_0^{2 \pi}\left\|F\left(\omega \cdot \cos (\phi), \omega \cdot \sin (\phi)\right)\right\|^2 \mathrm{~d} \phi,
    \label{eq:1d_spectrum} 
   % & \text{ for } \quad  k = 0, \ldots, M / 2 - 1 \nonumber
\end{align}
where each value of $AI(\omega)$ represents the radial integral of the power over the 2D spectrum at a given radius $\omega$. Consequently, the reduced 1D power spectrum effectively captures the distribution of different frequency components without regard to angular direction.
\inputfigure{act_dft}

\textbf{Network Components.} We first study the \textit{architecture} influences, including the types of three key architecture components: upsampling, activation function, and normalization. Subsequently, we discuss the \textit{parameter} influences associated with convolution layers.
\begin{itemize}
    \item \textbf{Upsampling Layers.} Upsampling layers serves to gradually increase the resolution of a generated image. Common types of upsampling layers are interpolation-based upsampler such as nearest and bilinear, and transposed convolution. Different types of upsampling are discussed in existing works to leave different spectral properties~\cite{zhang2019detecting, schwarz2021frequency}. To demonstrate this, we trained ProGAN~\cite{karras2017progressive} and SNGAN~\cite{miyato2018spectral} models on the CelebA dataset, each with three types of upsampling layers. Figure~\ref{fig:up_dft} presents the reduced 1D power spectrum of these models, where the x-axis indicates different frequencies from low to high, and the y-axis denotes the power of these frequency components. The figure illustrates that nearest and bilinear, which act as low-pass filters, significantly reduce high-frequency content, resulting in lower power at higher frequencies on the spectrum's right side. In contrast, transposed convolution, which can be conceptualized as zero-interleaving followed by a convolution with a learnable kernel, tends to preserve more high-frequency components. This is because the learned convolution kernels in transposed convolution layers are not guaranteed to be low-pass, allowing more high-frequency artifacts to remain in the generated images~\cite{zhang2019detecting}.
    % For our model synthesis purposes, we plan to vary the upsampling layers using nearest, bilinear, and zero-interleaving techniques when constructing generative models.
    \item \textbf{Activation Function.} The choice of activation functions introduces non-linearities that shape the model's frequency response. Different functions may enhance or suppress different aspects of the frequency spectrum. To verify this, we trained ProGAN and SNGAN models on the CelebA dataset, each with three different activation functions. As shown in Figure~\ref{fig:act_dft}\footnote{To make the high-frequency discrepancy more evident, we first use a denoising filter by~\cite{zhang2017beyond} with noise parameter $\sigma=1$ to remove the low-frequency semantic contents.}, generative models with ReLU activation generate images with more high-frequency components than Sigmoid and Tanh, as indicated by higher values on the right side of the reduced spectrum. The results may because that Tanh and Sigmoid compress the output into a bounded range, which has smooth properties and can damping high-frequency components. While ReLU is linear for all positive values, and zero otherwise, which allows high-frequency components to pass through unaffected in positive regions.
    % For our model synthesis purposes, we plan to vary the activation function among Sigmoid, Tanh, and ReLU when constructing generative models.
    \inputfigure{norm_dft}
    \item \textbf{Normalization.} Normalization techniques adjust data within the network by normalizing based on calculated means and variances, which are computed differently depending on the method, such as across the entire batch for Batch Normalization~\cite{ioffe2015batch} or for each individual sample in the case of Instance Normalization~\cite{ulyanov2016instance}. 
    Despite employing similar mathematical formulations, these techniques could influence the network's behavior differently. Due to the variations in the optimization landscapes they create, different types of normalization may lead to distinctive distributions of parameters, and consequently different spectrum patterns. As demonstrated in Figure~\ref{fig:norm_dft}, we train six SNGAN models using two types of normalization, each type with three different initial seeds. As seen, models with the same normalization type often exhibit more consistent spectral patterns compared to those trained using different normalization methods. This underscores the impact of normalization choices on the diversity of spectrum patterns of generative models. 
\end{itemize}

\textbf{Convolution Parameters.} Many factors, including optimization, initialization, and other configurations, can influence the parameters of a model. Our study primarily focuses on the parameters of convolution layers, which constitute a significant portion of a CNN-based model's parameters. The shapes and sizes of convolution kernels critically determine how frequency components are handled. Different kernels can lead to different emphases on certain frequency bands, thus altering the model's overall spectrum pattern. This phenomenon is illustrated in Figure~\ref{fig:conv_dft}, which shows the spectrum of two StyleGAN2~\cite{karras2020stylegan2} models that have identical architecture but differ in their parameters, trained respectively on the FFHQ and FFHQu datasets. For this analysis, we extracted the parameters from a specific output channel in the last convolutional layer of each model's final generative block. We then plot two figures for each model: the summed spectrum of the convolution kernels (left), and the spectrum of the output feature map for this channel (right). As demonstrated in the figure, for both models, the spectrum of the output feature map closely mirrors the spectrum of the convolution parameters. This alignment highlights how the convolutional layer's kernel characteristics directly shape the spectrum pattern of the output.

\inputfigure{conv_dft}

\inputfigure{compose}
% \inputfigure{swap_cm}

\textbf{Different Generative Blocks.} The discussion above focuses on the influence of basic components. Moving forward, it is important to consider how different generative blocks throughout the entire generation process contribute to the final spectral pattern on the generate image. Generative models typically consist of multiple generative blocks, with upsampling layers playing a crucial role as connectors between these blocks, doubling the resolution of the input feature maps. As mentioned above, commonly used upsampling layers, such as bilinear and nearest-neighbor interpolation, inherently possess a low-pass filtering effect. This property leads them to attenuate high-frequency components from prior layers. Given that existing research~\cite{yu2019attributing} suggesting that model fingerprints predominantly reside in the high-frequency components, it becomes apparent that the \textit{later blocks in a model's architecture could more significantly influence the model fingerprints on the output image as the high-frequency patterns from earlier layers could be filtered out.} To empirically test this hypothesis, we designed an experiment depicted in Figure~\ref{fig:compose}, consisting of three key steps:
    \begin{itemize}
    \item \textbf{Training:} Train a classifier on the Fourier spectrum of images generated by two distinct generative models with the same architecture (Model 1 and Model 2). 
    \item \textbf{Testing I:} Measure the classification results using a test set of images generated by Model 1 and Model 2.
    \item \textbf{Testing II:} Starting from the i-th block, sequentially exchange the succeeding blocks from Model 1 to Model 2 and vice versa. After each exchange, generate a new set of images with the composed models and assess the classifier's classification results on this new dataset. Figure~\ref{fig:compose}(c) visualize the operation by exchanging the last block.
    \end{itemize}   
Figure~\ref{fig:compose}(d,e) shows the confusion matrices under Testing I and Testing II of two sets of experiments: one set on two ProGAN models utilizing nearest-neighbor upsampling (d), and the other set on two ProGAN models employing bilinear-upsampling (e). As shown in the Figure, under Testing I, the classifier achieves high accuracy, indicated by the high diagonal values in the confusion matrix. However, after swapping the last few blocks in Testing II, the confusion matrix reveals a notable reversal in model attribution results. The shifted results indicate that most distinguishable spectrum patterns used to differentiate the models are predominantly from the models' last few blocks. And the patterns left by earlier blocks might be less pronounced as they progress through the generative process.

% \RED{[In summary, .... (how the above study benefit the following synthesizing strategy)]}
% \textbf{Summary.} 
In summary, different generative models exhibit distinct spectral patterns due to both architectural and parameter-related factors, including the types of upsampling and activation functions employed, normalization techniques, and the parameters of convolution layers. Notably, our observations indicate that within the architecture of a large generative model, the final few blocks tend to have a more profound influence on the distinguishable spectral patterns of the output images.

\subsection{Model Synthesis Strategy}
\inputfigure{generative_blocks}
\label{subsec:model_synthesis}

In this section, we introduce the model synthesis strategy that we use to create models for training the fingerprint feature extractor. Intuitively, a good synthesis strategy should generate models whose fingerprint patterns closely resemble those of real generative models, allowing these synthetic models to effectively simulate new models in an open environment. According to the discussion in Section~\ref{subsec:warm_up}, there are important factors to consider. First, the last few blocks tend to have a more profound influence on the distinguishable spectrum patterns of the output images. Thus, employing a synthesis architecture with a small number of generative blocks could ensure fidelity in mimicking fingerprints. Second, it is crucial to increase diversity by varying the types of upsampling layers, activation functions, normalization layers, and parameters. We explicitly incorporate these considerations into our method. Moreover, to further enhance diversity, we also consider variations in the number and sequence of layers.  

Overall, our model synthesis strategy is illustrated in Figure~\ref{fig:generative_blocks}, the structure of the synthetic model could be viewed as a shallow auto-encoder, composed of $K$ downsampling blocks and $K$ upsampling blocks. Based on the above discussion, $K$ can selected as a small value, such as one or two. Each downsampling block use a fixed architecture, with a pooling layer to downscale the input resolution by half, and two convolutional layers to increase the feature dimension. The feature output from the downsampling blocks is then sent into the upsampling blocks, which share common components with the generative blocks in actual generative models and vary in many architecture options. The architecture of these blocks is defined by the options $\{U, A, N, L, S\}$. 
\begin{itemize}
    \item $U$ is the upsampling operation that can be nearest neighbor upsampling, bilinear upsampling, or a stride 2 transposed convolution layer.
    \item $A$ is the activation function that can be ReLU, Sigmoid, Tanh, or no activation. 
    \item $N$ is the normalization type that can be batch normalization, instance normalization, or no normalization. 
    \item $L$ represents the number of convolution layers in the block. 
    \item $S$ refers to the order of activation and normalization relative to the convolution layer. 
\end{itemize}

By varying these configurations, we can get 288 different architectures within the construction space. For each architecture, we train $M$ models with different training seeds, creating $M$ distinct models with different parameters per architecture\footnote{$M$ is set to 20 in our experiments.}.

We recognize that speed is key factor in model synthesis. In our approach, we simplify the objective to focus on reconstruction, where the generative neural network is specifically tasked with minimizing the reconstruction loss of input images. This method is not only straightforward to implement but also allows for rapid training. We leave the exploration of other training approaches to future work. We set a constraint on the minimum reconstruction residual to $\eta = 0.005$ to limit the amount of artifacts in the output images. Ultimately, the total number of synthetic models generated is 5760, allowing for a wide simulation of a wide range of potential generative models with diverse architectures and parameters.

We measured the time required to train these models. For models with $K=1$, the averaged required time per model is 53 seconds, while those with $K=2$ required 113 seconds each. In total, training all the synthetic models takes 133 GPU hours on a single 3090 GPU. This is considerably more efficient than the time investment needed for training state-of-the-art generative models. For example, it takes 14 days 22 hours on single V100 GPUs to train a StyleGAN~\cite{karras2019style} model that generate with a 256$\times$256 resolution\footnote{https://github.com/NVlabs/stylegan}.
% \RED{[We need to discuss that our designed synthesis strategy ensures the fidelity and diversity, which correlates with the downstream model attribution task performance and we will empirically validate these in the Experiment section.]}

\subsection{Training the Fingerprint Extractor}
\label{subsec:fingerprint_extractor}
\inputfigure{method}

After obtaining the synthetic models, these models are utilized to train the fingerprint extractor. The goal of this extractor is to identify unique fingerprints embedded within images generated by different models, thereby enabling differentiation among them. As illustrated in Figure~\ref{fig:method}, the training procedure involves the synthetic model pool that we construct in the above section, and a real image dataset pool.

In the training phase, a batch of real images is randomly sampled from the real image dataset pool. Simultaneously, an equivalent number of synthetic models are selected from the synthetic model pool. Each real image $I_\text{real}$ from the batch is processed through a synthetic model $M_k$, producing a generated image $I_k$ that contains distinct fingerprints characteristic of the specific synthetic model used. 
\begin{align*}
    I_k = M_k (I_\text{real})
\end{align*}
These generated images, now embedded with model-specific fingerprints, are subsequently processed by a denoiser by~\cite{zhang2017beyond}. The primary function of this denoiser is to strip away low-frequency semantic content, thereby revealing the underlying high-frequency fingerprint patterns inherent in the images. The extracted noises are then transformed using a Discrete Fourier Transform to facilitate the extraction of their spectrum properties. The spectrum obtained from this transformation are fed into the fingerprint extractor, which is built upon the ResNet50 architecture. 
\begin{align*}
    h_k &= f_{\operatorname{fp\_extractor}} \lp \gF (I_k^{\prime}) \rp \\
 \text{ with }   I_k^{\prime} &= I_k - f_{\operatorname{denoise}}(I_k)
\end{align*}
where $f_{\operatorname{denoise}}$ and $f_{\operatorname{fp\_extractor}}$ denote the denoiser and fingerprint extractor, $\gF$ denotes the Discrete Fourier Transform, $h_k$ is the extracted fingerprint embedding.

To enhance the model's performance, we combine classification and metric loss to enhance the discrimination of learned fingerprint embedding. The classification head employs a multi-class cross-entropy (CE) loss function, which enable the fingerprint extractor to accurately classify the originating synthetic model of each input image. The projection head is followed by a triplet loss, which encourages generated images from the same generator to be as similar as possible in the feature space while promoting a larger dissimilarity between images from different generators. 
\begin{align*}
\mathcal{L} = \mathcal{L}_{\text{CE}}(h_k, y_k) + \mathcal{L}_{\text{Triplet}}(h_a, h_p, h_n)
\end{align*}
where $y_k$ is the true label for $k$-th synthetic model. $h_a$, $h_p$, and $h_n$ are the anchor, positive, and negative fingerprint embedding from a triplet set.


% \section{Effectiveness of Model Synthesis}


