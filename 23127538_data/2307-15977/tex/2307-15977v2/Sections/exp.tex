\section{Experiments}
\label{sec:exp}

In the experimental section, we aim to answer the following evaluation questions:\\
\textbf{EQ1} How is the fidelity and diversity of synthetic models? \\
\textbf{EQ2} How do the synthesis, training, and inference options affect the generalization ability of the fingerprint extractor? \\
\textbf{EQ3} How does our method perform compared with existing methods in model identification and verification scenarios? \\
\textbf{EQ4} Can we apply the trained fingerprint extractor in more complex attribution tasks, such as model linage analysis? \\

\subsection{Experimental Details}
\noindent \textbf{Testing Models.} Although our analysis and synthesis strategy primarily focuses on CNN-based generative models, our evaluation includes a broader scope of models as shown in Table~\ref{tab:dataset}. In addition to CNN-based models, we also assess the generalization performance on Transformer-based models, which share many common components with CNN-based models. And for CNN-based category, we include a variety of models such as CNN, VAE, Flow, and Diffusion models. All models in the CNN-based and Transformer-based category are unconditional models, which are trained on the dataset in the ``Image Source" column with different resolutions. Furthermore, the last row of the table encompasses state-of-the-art Text2Image models such as Stable Diffusion v1.5, Stable Diffusion v2.1, Stable Diffusion XL, Glide, DalleE-mini, DalleE-2, and DalleE-3. These models are not restricted to specific domains, and diverse in architecture type, training dataset, and resolution. 

For the ablation study in \cref{subsec:ablation}, our fingerprint extractor is purely trained on synthetic models, all models in the table are unseen. For the comparisons with existing methods in \cref{subsec:compare_existing}, certain models highlighted in \colorbox{gray!20}{gray} are used for training existing methods, and a fine-tuned version of our method pre-trained on synthetic models.

\inputtable{dataset}
\inputfigure{2setup}
\noindent \textbf{Experimental Setup.} Similar to the face identification and verification problems encountered in the field of face recognition, we consider two model attribution problems, the 1:1 model verification and 1:N model identification problem, as shown in Figure~\ref{fig:2setup}. Settings and experimental setups for the two problems are illustrated below:
\begin{itemize}
    \item \textbf{Model Verification}: the 1:1 model verification problem aims to verify whether two generated images are from the same model or not. Model verification is performed by comparing the similarity of the extracted fingerprint. To assess the performance, we generate 1,000 images for each model under test. Then we randomly selected 10,000 pairs out of these images, which consist of 5,000 negative pairs (images from different models) and 5,000 positive pairs (images from the same model). The evaluation employs metrics commonly used in face verification tasks: accuracy and the Area Under the Receiver Operating Characteristic Curve (AUC). Accuracy measures the correct verification of pairs as originating from the same or different models. We report the best accuracy across all similarity thresholds to assess the overall effectiveness of the verification system. The AUC provides a comprehensive measure of performance across all possible threshold levels, reflecting the trade-off between the true positive rate and the false positive rate. 
    \item \textbf{Model Identification}: the 1:N model identification problem aims to identify the specific generative model used to create a given image among N models. To assess the performance of model identification, we generate 1,000 images for each model under test. Out of these, 10 images are sampled to create the fingerprint for each model, while the rest serve as test samples. Each model's fingerprint is collected to form the model gallery. We compare the test sample with each fingerprint in the gallery and classify it to the class with the closest fingerprint distance. Performance is evaluated using classification accuracy and the F1 score.
\end{itemize}

\noindent \textbf{Compared methods.} To compare with existing methods, we derive two variants of our method: 1) \textit{Ours (Only-Syn)}, which is only trained on synthetic models without having seen any real models in Table~\ref{tab:dataset}. 2) \textit{Ours (Fine-tune)}, which fine-tune the fingerprint extractor trained on synthetic models for one epoch using samples of models highlighted in gray in the table.

We compare against five existing model attribution methods. They are Marra\etal~\cite{marra2019gans}, Yu\etal~\cite{yu2019attributing}, DNA-Det~\cite{yang2022aaai}, RepMIX~\cite{bui2022repmix}, and POSE~\cite{yang2023progressive}. For the training-based methods among these, we train them on models in gray in Table~\ref{tab:dataset}. Then we extract the layers before the classification head as the fingerprint extractor. This adaptation is implemented to evaluate the model identification and verification problems. 

\subsection{Effectiveness of Model Synthesis (EQ1)}
\label{sec:sec43}

The generalization capability of our trained fingerprint extractor is built upon synthetic models. Before evaluating the fingerprint extractor, we first conduct evaluations on the synthetic models in two crucial dimensions: 1) \textit{Fidelity}, which assesses the extent how well the synthetic models replicate the spectrum/fingerprint patterns of real models. 2) \textit{Diversity}, which examine whether the synthetic models exhibit a wide range of spectrum patterns, rather than merely replicating a limited set.
\inputfigure{ffd}
\inputtable{fid_compare}

\noindent \textbf{Fidelity.} Currently, there is no off-the-shelf tool to quantitatively measure the gap between synthetic and real models in fingerprint distributions. To address this gap, we introduce a new metric called Frechet Frequency Distance (FFD), inspired by the Frechet Inception Distance (FID)~\cite{heusel2017gans} commonly used to assess the quality of images from generative models. The FFD is calculated through two primary steps:

\begin{itemize}
    \item \textbf{(Step I) Spectrum Pattern Representation:} We extract the spectrum pattern of generated images to represent the fingerprint pattern of each synthesized and real models. As depicted in Figure~\ref{fig:ffd}(a), for each model, we initially generate $N$ (e.g., 100) images, and then, following~\cite{corvi2023intriguing}, we enhance the spectrum patterns by applying a noise extractor~\cite{zhang2017beyond} to remove semantic contents from these images. The resultant noise images are then used to compute a reduced 1D power spectrum, as specified in \cref{eq:1d_spectrum}. We calculate the averaged spectrum from these $N$ images to serve as the spectrum pattern representation for each model.

    \item \textbf{(Step II) Frechet Distance Calculation:} As illustrated in Figure~\ref{fig:ffd}(b), we first compute the mean and covariance of the spectrum representations for real models in Table~\ref{tab:dataset}, and synthetic models in Section~\ref{subsec:model_synthesis}. The Fréchet distance is then calculated between these distributions. Specifically, for the mean $\mu_r$ and covariance $\Sigma_r$ of real distribution, and the mean $\mu_s$ and covariance $\Sigma_s$ of synthetic distribution, their distance is given by:
    \begin{align*}
    \text{FFD} = \left\|\mu_r-\mu_s\right\|^2 + \operatorname{Tr}\left(\Sigma_r + \Sigma_s - 2\left(\Sigma_r \Sigma_s\right)^{1/2}\right),
    \end{align*}
    where $\operatorname{Tr}$ represents the trace of a matrix. A low FFD score indicates that the synthetic models closely mimic the real models in terms of spectrum patterns. 
\end{itemize}
\inputfigure{relative_volume}
\inputtable{ablation_arch}
In Table~\ref{tab:fid_compare}, we evaluate the Frechet Frequency Distance of synthetic models against real models, varying synthesis choices. The variations in the architecture include: "w/o K" and "w/o L," indicating the construction with a single down/up block and a single layer, respectively; "w/o S," using only one type of sequence — normalization and activation after convolution; "w/o U," utilizing only bilinear interpolation for upsampling; and "w/o A" and "w/o N," which denote the absence of activation and normalization layers, respectively. We also consider not using diverse seeds for each architecture, denoted as "w/o seed". The results displayed in the table indicate that reducing most synthesis options tends to increase the Frechet Frequency Distance (FFD), pointing to a poorer alignment between the distributions. Notably, eliminating the diversity in activation functions and upsampling methods results in the most significant increases in FFD. This may be because these factors, when varied, typically enhance the spectrum resemblance to real distributions. 
% Furthermore, we find that training models without varying the seeds for each architecture also leads to higher FFD values, suggesting that using diverse seeds generally provides a closer match to the real model distribution.


\noindent \textbf{Diversity.} Although the FFD score effectively compares the fingerprint resemblance to the real models in Table~\ref{tab:dataset}, the limited size of the real model dataset might prevent it from fully capturing the diversity of the synthetic data, potentially resulting in a bias toward the limited variety seen in the real dataset. Therefore, we propose using an additional metric, the Volume of Confidence Ellipsoid~\cite{martin1979multivariate}, to better assess the extent to which the synthetic models occupy the spectrum representation space. The volume is calculated as follows:
\begin{align*}
    \text{Volume} = \frac{(2\pi)^{n/2}}{\Gamma\left(\frac{n}{2} + 1\right)} \times \sqrt{\prod_{i=1}^n \lambda_i} , 
\end{align*}
where $\lambda_i$ are the eigenvalues of the covariance matrix of spectrum representations of all synthetic models, $n$ is the number of dimensions, and $\Gamma$ denotes the gamma function. This formula determines the volume of an ellipsoid in n-dimensional Euclidean space.


Figure~\ref{fig:relative_volume} displays the relative volume proportions obtained by removing each synthesis option compared to the total volume of all synthesized models. As seen in the table, removing any synthesis option reduces the volume of the spectrum representation, with proportions smaller than 1. This suggests that incorporating all variations enhances the diversity of synthesized models.
\subsection{Ablation Study (EQ2)}
\label{subsec:ablation}

\inputtable{ablation_seed}
\inputtable{ablation_dataset}
\inputtable{ablation_input}
\inputtable{ablation_sw}

% In the ablation study section, we assess the influence of synthesized models and real images that are used to synthesize fingerprinted images, the choice of input signal extracted from fingerprinted images that is used to train the fingerprint extractor, and the sliding window strategy during inference.
In the ablation study section, we examine several key factors affecting the generalization performance of the fingerprint extractor. These factors include: the types of synthetic models and real images used for creating fingerprinted images, the selection of input signals extracted from these fingerprinted images for training the fingerprint extractor, and the implementation of a sliding window strategy during the inference phase. This comprehensive evaluation helps in understanding how each component contributes to the effectiveness of the fingerprint extraction. Note that in this section, the fingerprint extractor is trained only on synthetic models and evaluated on all the models in Table~\ref{tab:dataset}, without including any real generative models in training. Our default configurations within the tables are highlighted in \colorbox{gray!20}{gray}.

% \inputfigure{1v1_tsne}

\noindent \textbf{Synthetic Models.} 
To evaluate the impact of individual synthesis options of synthetic models on the generalization of the trained fingerprint extractor in model verification and identification tasks, we conducted an ablation study. The results, presented in Table~\ref{tab:ablation_arch}, demonstrate that omitting any synthesis option reduces the generalization capabilities in model identification and verification tasks. Notably, the removal of the upsampling and activation layers significantly affects generalization performance. This observation aligns with the Frechet Frequency Distance scores listed in Table~\ref{tab:fid_compare}, where the absence of activation and upsampling variation results in the highest scores, indicating the lowest fidelity of synthetic models. 

% We plot the t-SNE fingerprint visualization on images generated by models in Table~\ref{tab:dataset}. As shown, even though the fingerprint extractor is trained solely on synthetic data, it could extract distinct fingerprints from actual generative models with a variety of architectures and types.

\inputtable{1vn_result}
\noindent \textbf{Real Dataset.} In Table~\ref{tab:ablation_dataset}, we evaluate how the choice of real datasets used to generate fingerprinted images with synthetic models affects the generalization performance of the trained fingerprint extractor. We consider three options for constructing the real image pool: the CelebA dataset, which exclusively contains face images; the LSUN dataset, featuring 20 different types of semantic images; and a composite of both datasets. The results indicate that the combination of the CelebA and LSUN datasets, which offers the richest semantic diversity, yields the best generalization performance. This phenomenon may be because the increasing the semantic diversity of fingerprinted images helps the fingerprint extractor focus on semantic-agnostic fingerprint features.


\noindent \textbf{Input Signal.} In Table~\ref{tab:ablation_input}, we assess the impact of different input signals extracted from fingerprinted images on the training of the fingerprint extractor. We consider three types of inputs: the RGB image directly, the DFT spectrum of the RGB image (Spectrum), and the DFT spectrum of the noise extracted from the RGB image (Noise Spectrum). The noise is extracted using a denoising process that primarily removes high-level semantic details from the image. As demonstrated in the table, the choice of input signal significantly affects the generalization ability of the fingerprint extractor. The "Noise Spectrum" input yields the best performance, followed by "Spectrum," with the direct RGB image showing the least effectiveness. This outcome suggests that the Noise Spectrum most effectively enhances the high-frequency fingerprint information within the image, making the fingerprint features more extractable and distinct.

\noindent \textbf{Sliding Window.}
During the inference phase for extracting fingerprints from test images of varying resolutions, we implement a sliding window strategy to accommodate different input sizes. The training resolution for the fingerprint extractor is set at 128$\times$128 pixels. For testing images that exceed this resolution, we employ an equally spaced sliding window technique to divide the image into N$\times$N patches, each with 128$\times$128 pixels. These patches are then individually processed to extract fingerprints. The final fingerprint for the entire input image is obtained by averaging the fingerprints from all these patches, ensuring comprehensive coverage and utilization of the image's information for accurate fingerprint extraction. For images with resolutions smaller than 128$\times$128, we resize them to the required 128$\times$128 resolution. As indicated in Table~\ref{tab:ablation_sw}, the sliding window strategy significantly enhances model identification performance. This improvement is attributed to the strategy's ability to capture a richer set of fingerprint information within the test image. Given that the performance of $3\times3$ and $4\times4$ sliding configurations shows similar results, we use the $3\times$3 configuration for inference efficiency.

\inputfigure{generalization}
\subsection{Comparison with Existing Methods (EQ3)}
\label{subsec:compare_existing}
% \inputfigure{hist_tsne_compare}
\noindent \textbf{Model Identification}
In Table~\ref{tab:1vn_result}, we evaluate and compare our methods—Ours (Only-Syn) and Ours (Fine-tune)—against existing methods in the 1:N model identification scenario. The results show that most existing methods achieve high identification accuracy on classical models, which are seen in their training. However, their performance significantly declines when tested on unseen emerging models, dropping below 50\%. In contrast, our method "Ours (Only-Syn)" demonstrates strong generalization capabilities. Despite being trained only on synthesized models and not having exposure to real generative models, it achieves over 90\% identification accuracy on the two sets of models. To align more closely with existing methods, we introduce "Ours (Fine-tune)," which is further fine-tuned using classical model images. This adjustment improves the identification accuracy to 99\% on classical models, with a modest enhancement on emerging models. These results underscore that our fingerprint extractor, by leveraging synthetic models, exhibits superior generalization ability across real-world generative models due to its exposure to a broader model fingerprint variations. Figure~\ref{fig:generalization} visualizing the extracted fingerprint embeddings also indicates this.

\inputtable{1v1_verify}
\noindent \textbf{Model Verification.} In Table~\ref{tab:1v1_verify}, we compare our methods against existing methods in the 1:1 model verification scenario, which involves determining whether two input images are generated by the same model. Similar to the findings in the 1:N model identification scenario, existing methods exhibit high verification accuracy for seen models but show significant performance degradation when applied to unseen models. Our method, Ours (Only-Syn), trained solely on synthetic models, demonstrates balanced performance across the two groups of model. This indicates a robust ability to generalize from synthetic model to real-world generative models. When we fine-tune the fingerprint extractor with images from classical models, as in Ours (Fine-tune), the performance on classical models is up to par with existing methods while maintaining superior performance on unseen emerging models. 
% Figure~\ref{fig:hist_tsne_compare} shows the histogram . 

% regardless of whether the models are previously encountered.
% \inputfigure{sd_transformer_tsne}
% \noindent \textbf{Different versions of } Our fingerprint extractor also shows potential in analyzing the lineage between different versions of models based on their generated images. In Figure.~\ref{fig:sd_transformer_tsne}a, we visualize the extracted features of various versions of stable diffusion models. The versions include sd-v1.1, v1.2, v1.3, v1.4, v2-base, v2, and v2.1. We can observe that the feature spaces of v1.\# versions and v2.\# versions overlap respectively. This alignment of feature spaces is consistent with the description of the relationship between these versions on the public website. Specifically, v1.2, v1.3, and v1.4 are resumed from v1.1, while v2 and v2.1 are resumed from v2-base.
\subsection{Model Lineage Analysis (EQ4)}
\inputfigure{cvitai_lora}

At times, models may be stolen and utilized as the base model for training variants by other entities. Our model fingerprint extractor (Only-Syn) demonstrates the potential to trace back to the base model of such variants. To illustrate this capability, we gathered models from Civitai\footnote{https://civitai.com/}, a community renowned for its vast repository of high-quality Stable Diffusion models. The majority of these models are LoRA-variant models, fine-tuned from a base model using LoRA (Low-Rank Adaption)~\cite{hu2022lora}. We collected four distinct LoRA weights for each of the three most commonly used base models: SD15, PonyXL v6, and SDXL 1.0. Subsequently, we generated 10 images for each LoRA-equipped model and base model. Figure~\ref{fig:cvitai_lora}(a) show the generated images by these LoRA variants, which exhibit discernible differences in styles and semantics. We employing the averaged fingerprint of generated images as the fingerprint for the base model. As shown in Figure~\ref{fig:cvitai_lora}(b), the LoRA-equipped variants exhibit similar fingerprint embeddings with their respective base models. This result shows that we could easily attribute the LoRA variant to its base model comparing the extracting fingerprints from their generated images. 

\section{Conclusions and Future Work}

In this paper, we propose to tackle the model attribution problem in a more generalized scenario, to expand the attribution target to unseen models that are not included in training. To solve the problem, we propose a novel method by training on a large and diverse set of synthetic models, which mimic the fingerprint patterns of real generative models. The synthesis strategy is based a detailed analysis of how different model architectures and parameters influence fingerprint patterns, and is validated using two designed metrics to assess their fidelity and diversity. Experimental results demonstrate that our fingerprint extractor, trained on synthetic models, exhibits superior generalization capabilities in identifying and verifying unseen real generative models across various scenarios. 

Beyond the scope of this paper, there are several promising directions worth further explorations. Firstly, the generated images may undergo subsequent processing in practical scenarios, which can diminish their intrinsic spectrum patterns. It is important to develop methods for extracting robust patterns to prepare the fingerprint extractor for more practical applications. Secondly, aside from LoRA, there are many other types of model variants derived from a base model. Exploring more relationships between models through fingerprint extraction holds potential for researchers. Finally, while our method has shown effective generalization ability of training solely on synthetic models and tested on real models without fine-tuning, it does not outperform the performance of directly training on real models and test on these models. This indicates that the potential of model synthesizing has not been fully exploited. 
How to further increase the fidelity and diversity of model synthesis is also an important research direction in the future. 

% our analysis involves basic components in CNN-based generative models. Although the pre-trained fingerprint extractor has shown strong generalization on transformer-based models, it's also worthwhile to investigate the impact of other components, such as attention mechanisms and skip connection. 
% \subsection{Fingerprint Visualization}
% \subsection{Fine-grained model distinction} 

% \section{Acknowledgment}

