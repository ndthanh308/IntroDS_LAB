\section{Introduction}
\IEEEPARstart{I}{n} recent years, advanced generative (vision) models have revolutionized various fields such as art creation, design, and human-computer interaction~\cite{nichol2021glide, ramesh2022hierarchical, rombach2022high, mj}. Despite their positive impact, these models have also given rise to new concerns, such as copyright infringement issues and content supervision. To address these concerns, model attribution, the process of identifying the source model of generated content, has gained increasing attention \cite{marra2019gans,yu2019attributing,xuan2019scalable,yang2022aaai,bui2022repmix, yang2023progressive}. It helps deter unauthorized copying and distribution, enabling content creators and rights holders to prove ownership and take legal action against infringements. Furthermore, model attribution allows regulators to identify and act against entities using generative models for harmful, illegal, or unethical purposes.
% This challenge becomes increasingly pressing as the number of newly-emerged generative models rapidly increases in the open world. 

\input{Figures/intro}

Existing research seeks to identify the unique fingerprints on the images they generate, which can be leveraged to attribute a generated image to its source model. A commonly adopted model attribution paradigm frames the task as a multi-class classification problem~\cite{yu2019attributing, yang2022aaai, bui2022repmix, yang2023progressive}. In this setup, images generated by a limited and static set of models are used to train a classifier, where each image is labeled with a unique model ID. During testing, the test image comes from a seen model predefined in the training set, and the classifier would identify the model ID of the image based on the model fingerprint it has learned~\cite{yu2019attributing, yang2022aaai, bui2022repmix}. However, there arises a scenario where a testing image originates from an unseen model not present during training. To tackle this, some methodologies~\cite{yang2023progressive, abady2024siamese} adopt an open-set setup, assigning them an ``unknown" label. 

Despite these efforts, conventional methods, trained on a static set of seen models, struggle to adapt to the newly emerged unseen models dynamically. This limitation is exemplified in Figure~\ref{fig:intro}(a), which presents the t-SNE feature extraction results from an existing method~\cite{yang2023progressive}. We train this method on classical models such as GAN, VAE, and Flow models~\footnote{Include all the GAN, VAE, and Flow models in Table~\ref{tab:dataset}}, and test on both these and the emerging Diffusion models. The results reveal a significant generalization gap when the method encounters new types of generative models. This gap stems from the restricted scope of seen models that can be practically collected, whose fingerprint distributions may significantly differ from those of newly emerging models. As generative technology continues to evolve, the variety of unseen models expands, further challenging the capacity of existing methods to adapt dynamically. 

To bridge this gap, our goal is to develop a more generalized model fingerprint extractor capable of attributing unseen models efficiently in a zero-shot setting. Central to our approach is a model synthesis technique that creates plenty of synthetic models that mimic a broad range of fingerprint patterns in real-world generative models. To gain insights for replicating these patterns, our initial step is to explore why generative models display unique fingerprint patterns, which, although typically imperceptible in the spatial domain, manifest as distinct spectral patterns in high-frequency components. Through analyzing the architecture and parameters of generative models, we observe: 1) The type of basic network components such as upsampling, activation function, normalization, and parameters of convolution layers are key factors influencing the spectral patterns of the generated images. 2) Across the generative blocks within a large generative model, due to the upsampling layer that naturally attenuates earlier layers' high-frequency components, the last few generative blocks are more influential in determining the output's spectrum patterns.

We incorporate these observations into our model synthesis technique. Our approach is based on a shallow auto-encoder architecture, comprising fewer generative blocks compared to typical generative models. According to the discussion above, employing a synthesis architecture with a small number of generative blocks could ensure fidelity in mimicking fingerprints. We then increase the architecture diversity by varying the types of upsampling layers, activation functions, normalization layers, and the number and sequence of these layers. Additionally, we enhance the parameter diversity by altering training seeds. Consequently, this approach allows us to get 5760 synthetic models across 288 different architectures by minimizing the reconstruction loss. Owing to the simple architecture and training objective, training these synthetic models requires much less time than real generative models. Subsequently, we leverage these synthetic models to train the fingerprint extractor, to extract fingerprints from these models, and distinguish between each other. We combine classification and metric loss to enhance the discrimination of learned fingerprint embedding. 

% In this section, we introduce the model synthesis strategy that we use to create models for training the fingerprint feature extractor. Intuitively, a good synthesis strategy should generate models whose fingerprint patterns closely resemble those of real generative models, allowing these synthetic models to effectively simulate new models in an open environment. According to the discussion in Section~\ref{subsec:warm_up}, there are important factors to consider. First, the last few blocks tend to have a more profound influence on the distinguishable spectrum patterns of the output images. Thus, employing a synthesis architecture with a small number of generative blocks could ensure fidelity in mimicking fingerprints. Second, it is crucial to increase diversity by varying the types of upsampling layers, activation functions, normalization layers, and parameters. We explicitly incorporate these considerations into our method. Moreover, to further enhance diversity, we also consider variations in the number and sequence of layers.  

We establish two metrics to verify the effectiveness of our model synthesis strategy in terms of \textit{fidelity} and \textit{diversity}. The former assesses the authenticity of fingerprint pattern mimicking by comparing the spectrum distribution between synthetic and real models, particularly in high-frequency components. The latter evaluates the diversity of spectrum patterns of synthetic models by examining the extent to which the synthetic models occupy the spectrum representation space. The results indicate that the synthesis options we designed indeed enhance the distribution fidelity and diversity of spectrum patterns.

Experimental results demonstrate that our fingerprint extractor, despite being solely trained on synthetic models, exhibits strong zero-shot attribution capabilities on a broad scope generative models, including the classical GAN, VAE, Flow models, and emerging Diffusion models like Stable Diffusion and DalleE-3. Partial visualization results are shown in Figure~\ref{fig:intro}(b). In the model attribution scenarios, including model identification and verification on unseen models, our method significantly outperforms existing approaches, achieving accuracy improvements of over 40\% and 15\% respectively.

Our major contributions can be summarized as follows:

1) We propose to address a practical problem in applying model attribution in the real world, to expand the attribution target to unseen models in a zero-shot setting.

2) We propose to solve the zero-shot model attribution problem based on training on numerous synthetic models, which mimic the fingerprint patterns of real-world generative models. The synthesis strategy is motivated by observations on how generative model architecture building blocks and parameters influence fingerprint patterns and is validated through two designed metrics that examine synthetic models' fidelity and diversity.


3) Experimental results demonstrate that our fingerprint extractor, trained on synthetic models, exhibits strong generalization capabilities on a wide range of real-world generative models. Compared with existing methods, we improve the model identification and verification accuracy on unseen models by over 40\% and 15\% respectively.

% \inputfigure{generalization}
