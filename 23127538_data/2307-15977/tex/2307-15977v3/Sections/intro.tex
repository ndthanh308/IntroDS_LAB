\section{Introduction}
\IEEEPARstart{I}{n} recent years, advanced generative (vision) models have revolutionized various fields such as art creation, design, and human-computer interaction~\cite{nichol2021glide, ramesh2022hierarchical, rombach2022high, mj}. Despite their positive impact, these models have also given rise to new concerns, such as copyright infringement issues and content supervision. To address these concerns, model attribution, the process of identifying the source model of generated content, has gained increasing attention \cite{marra2019gans,yu2019attributing,xuan2019scalable,yang2022aaai,bui2022repmix, yang2023progressive}. It helps deter unauthorized copying and distribution, enabling content creators and rights holders to prove ownership and take legal action against infringements. Furthermore, model attribution allows regulators to identify and act against entities using generative models for harmful, illegal, or unethical purposes. 
% This challenge becomes increasingly pressing as the number of newly-emerged generative models rapidly increases in the open world. 

\input{Figures/intro}

Model attribution methods identify unique fingerprints in images generated by source models. These methods can leverage these fingerprint features to trace a generated image back to its originating model. A commonly adopted model attribution paradigm frames the task as a multiclass classification problem~\cite{yu2019attributing, yang2022aaai, bui2022repmix, yang2023progressive}. In this setup, images generated by a limited and static set of models are used to train a classifier, where each image is labeled with a unique model ID. During testing, a test image comes from a seen model in the training set, and the classifier would identify the model ID based on the model fingerprint it has learned~\cite{yu2019attributing, yang2022aaai, bui2022repmix}. This framework looks promising, but has a major limitation: classical methods fail to attribute models not present in the training data, assigning incorrect classification labels to such new models. This drawback is unacceptable in today's rapidly evolving landscape, where new models emerge quickly. Although some works~\cite{yang2023progressive,abady2024siamese} consider an open-set setup by assigning an ``unknown" label for new models, these approaches still struggle to adapt to the newly emerged unseen models dynamically.

% The first limitation is linked to \emph{generalization}: achieving good generalization performance with classical methods requires a large amount of images from generative models as training data, which is unfortunately expensive and hard to collect. The second limitation concerns \emph{flexibility}:

To tackle the flexibility challenge, we aim to develop a more generalized model attribution method capable of zero-shot model attribution, efficiently attributing unseen models \textit{without requiring any training on samples from these models}. Instead of relying on a classification head to classify the generated image to a static set of seen models that have been included in training, we view model attribution as comparing the "fingerprint" distance between the test sample and each model's fingerprint within a model gallery. This framework allows us to easily deal with newly emerged models, we could simply collect samples from new models, use the fingerprint extractor to create their fingerprints, and dynamically updates to the model gallery, without requiring any training on these models.

How to train an effective fingerprint extractor capable of generalizing on unseen models? Traditional wisdom suggests training it on a limited set of real-world models. However, this approach suffers from poor generalization on unseen models, which is exemplified in Figure~\ref{fig:intro} (above). To overcome this, we propose pre-training the extractor with a significantly larger set of synthetic models (e.g., 100 times more in our experiments), which provides similar fingerprint patterns as real-world models but much more diverse. 

% We propose synthesizing models in the \emph{frequency} domain for model attribution. 

Our model synthesis strategy is based on analysis in the \emph{frequency domain}, which is inspired by previous works~\cite{frank2020leveraging, corvi2023intriguing} demonstrating that fingerprints are unobservable in the spatial domain but prominent in the frequency domain. To understand how unique fingerprint patterns of real-world generative models are formed, we examine the mechanism from the perspective of basic building blocks of generative models. Our findings reveal that:
\begin{itemize}
    \item The type of basic network components—such as upsampling, activation functions, normalization, and convolution layer parameters—significantly influences the spectral patterns of generated images.
    \item Due to the upsampling layer naturally attenuating high-frequency components from earlier layers, the last few generative blocks within a large model are more influential in determining the output's spectral patterns.
\end{itemize}
Based on these insights, we hypothesize that synthesizing shallow generative blocks with various combinations of basic components can effectively represent the fingerprint space of unseen models. Therefore, we develop a simple yet effective model synthesise strategy. 

% \RED{By comparing the spectrum distribution between synthetic and real-world models, our results indicate that our synthesis options enables the synthesis distribution closely align with real-world generative models.} 
Inspired by the above analysis, we construct the synthetic models based on a shallow auto-encoder architecture, comprising fewer generative blocks compared to typical generative models. We increase synthesis diversity by varying the basis components, including the types of upsampling layers, activation functions, normalization layers, the number and sequence of these layers, and training seeds. Consequently, we get 5760 synthetic models across 288 different architectures by minimizing the reconstruction loss. This model synthesis strategy has advantages in terms of \emph{fidelity} and \emph{efficiency}. By comparing the spectrum distribution between synthetic and real-world models, our results indicate that our synthesis options allow the spectrum distribution of synthetic models to more closely align with those of real-world generative models. Further, synthesizing a model takes only 83 seconds on average with a single 3090 GPU. This is remarkable faster than the naive solution of model synthesis (i.e., training state-of-the-art generative models from scratch). For example, it takes 14 days 22 hours to train a StyleGAN~\cite{karras2019style} model that generate with a 256$\times$256 resolution\footnote{https://github.com/NVlabs/stylegan}, which is significantly slower than ours.

We leverage these synthetic models to train the fingerprint extractor, to extract fingerprints from these models, and distinguish between each other. We combine classification and metric loss to enhance the discrimination of learned fingerprint embedding. Experimental results demonstrate that: 1) Our fingerprint extractor, despite being solely trained on synthetic models, exhibits strong zero-shot attribution capabilities on a broad scope generative models, including the classical GAN, VAE, Flow models, and emerging Diffusion models like Stable Diffusion and DalleE-3. Partial visualization results are shown in Figure~\ref{fig:intro} (below). 2) We consider two model attribution scenarios, including model identification and model verification. Our method significantly outperforms existing approaches on unseen models, achieving accuracy improvements of over 40\% and 15\% respectively. 3) Our method is capable of tracing the LoRA-variants back to their base models only relying on the generated images, providing an efficient defense tool against model intellectual property infringements.

% \RED{MERGED: Our approach is based on a shallow auto-encoder architecture, comprising fewer generative blocks than typical generative models. The model synthesis strategy has advantages in terms of \emph{fidelity} and \emph{efficiency}. Fidelity assesses the authenticity of fingerprint pattern mimicking by comparing the spectrum distribution between synthetic and real models.}

Our major contributions can be summarized as follows:

1) We propose to solve the problem of zero-shot model attribution, which expands the model attribution target to unseen models that are not included in training.

2) We propose solving the zero-shot model attribution problem by training on numerous synthetic models that mimic the fingerprint patterns of real-world generative models. Our synthesis strategy is inspired by observations on how generative model architecture building blocks and parameters influence fingerprint patterns. This strategy has advantages in terms of fidelity and efficiency.

3) Experimental results demonstrate that our fingerprint extractor, trained on synthetic models, exhibits strong generalization capabilities on a wide range of real-world generative models. Compared with existing methods, we improve the model identification and verification accuracy on unseen models by over 40\% and 15\% respectively.

% \inputfigure{generalization}

% former version:

% Existing research seeks to identify the unique fingerprints on the images they generate, which can be leveraged to attribute a generated image to its source model. A commonly adopted model attribution paradigm frames the task as a multiclass classification problem~\cite{yu2019attributing, yang2022aaai, bui2022repmix, yang2023progressive}. In this setup, images generated by a limited and static set of models are used to train a classifier, where each image is labeled with a unique model ID. \wdd{This setup requires a large number of images generated by various generative models as training data, which is computationally expensive and hard to acquire, especially for newly emerging models and unknown models from malicious users.} During testing, a test image comes from a seen model in the training set, and the classifier would identify the model ID based on the model fingerprint it has learned~\cite{yu2019attributing, yang2022aaai, bui2022repmix}. \RED{However, this framework fails to attribute images from models not present during training. To address this, some methodologies~\cite{yang2023progressive, abady2024siamese} adopt an open-set approach, assigning an “unknown” label to such images, which is a compromise on the ideal expectations for model attribution.}

% % However, there arises a scenario in which a testing image originates from an unseen model not present during training. To tackle this, some methodologies~\cite{yang2023progressive, abady2024siamese} adopt an open-set setup, assigning them an ``unknown" label. 

% Despite these efforts, conventional methods, trained on a static set of seen models, struggle to adapt to the newly emerged unseen models dynamically. This limitation is exemplified in Figure~\ref{fig:intro}(a), which presents the t-SNE feature extraction results from an existing method~\cite{yang2023progressive}. We train this method on classical models such as the GAN, VAE, and flow models~\footnote{Include all the GAN, VAE, and Flow models in Table~\ref{tab:dataset}}, and test on both these and emerging diffusion models. The results reveal a significant generalization gap when the method encounters new types of generative models. This gap stems from the restricted scope of seen models that can be practically collected, whose fingerprint distributions may significantly differ from those of newly emerging models. As generative technology continues to evolve, the variety of unseen models expands, further challenging the capacity of existing methods to adapt dynamically. 

% To address this gap, our goal is to develop a more generalized model fingerprint extractor capable of zero-shot model attribution, efficiently attributing unseen models \textit{without requiring any training on samples from these models}. Central to our approach is a model synthesis technique that creates plenty of \wdd{shallow} synthetic models that mimic a broad range of fingerprint patterns in real-world generative models \wdd{with relatively low computational cost and high generalizability to cover potential emerging models.}. \RED{Consequently, model attributors trained with these diverse synthetic models are expected to exhibit strong generalization performance. Our approach shares the same inspirit with recent advances in pre-training foundation models with vast data, which have demonstrated improved generalization performance in zero-shot settings \cite{radford2021learning, brown2020language}.}


% \RED{How to effectively synthesize models? What is the key?  As a first work in this direction, we explore these questions in the frequency domain.} \wdd{CMT: We should cite works that looked into the frequency domain to give them credit and justify our choice.}  \BLUE{To gain insight into replicating these fingerprint patterns that are typically imperceptible in the spatial domain yet manifest as distinct spectral patterns in high-frequency components, our initial step is to explore why generative models display unique fingerprint patterns} \wdd{from the perspective of basic building blocks of generative models. This is because, although newly emerging generative models differ from classical ones in model architecture and training paradigms, they mostly utilize similar basic components.} Through analyzing the architecture and parameters of generative models, we observe: 1) The type of basic network components such as upsampling, activation function, normalization, and parameters of convolution layers are key factors influencing the spectral patterns of the generated images. 2) Across the generative blocks within a large generative model, due to the upsampling layer that naturally attenuates earlier layers' high-frequency components, the last few generative blocks are more influential in determining the output's spectrum patterns. \wdd{According to our findings, we hypothesize that we can effectively represent the fingerprint space of unseen models by synthesizing shallow generative blocks with various combinations of basic components. }
