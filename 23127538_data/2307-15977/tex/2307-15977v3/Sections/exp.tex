\section{Experiments}
\label{sec:exp}

In the experimental section, we aim to answer the following evaluation questions:\\
% \textbf{EQ1} How is the fidelity and diversity of synthetic models? \\
\textbf{EQ1} How do the synthesis, training, and inference options affect the generalization ability of the fingerprint extractor? \\
\textbf{EQ2} How does our method perform compared with existing methods in model identification and verification scenarios? \\
\textbf{EQ3} Can we apply the trained fingerprint extractor in more complex attribution tasks, such as model linage analysis? \\
\textbf{EQ4} How similar are synthetic models to real models in terms of their 2D spectrum visualizations?

% \vspace{-10}
\subsection{Experimental Details}
\noindent \textbf{Testing Models.} Although our analysis and synthesis strategy primarily focuses on CNN-based generative models, our evaluation includes a broader scope of models as shown in Table~\ref{tab:dataset}. In addition to CNN-based models, we also assess the generalization performance on Transformer-based models, which share many common components with CNN-based models. And for CNN-based category, we include a variety of models such as CNN, VAE, Flow, and Diffusion models. All models in the CNN-based and Transformer-based category are unconditional models, which are trained on the dataset in the ``Image Source" column with different resolutions. Furthermore, the last row of the table encompasses state-of-the-art Text2Image models such as Stable Diffusion v1.5, Stable Diffusion v2.1, Stable Diffusion XL, Glide, DalleE-mini, DalleE-2, and DalleE-3. These models are not restricted to specific domains, and diverse in architecture type, training dataset, and resolution. Although these models are primarily for multimodal text-to-image generation, they share structural similarities with GAN-based unimodal generators. For example, the foundational architecture of the Stable Diffusion v1 series employs a U-Net structure, where its upsampling blocks are also built using standard components such as normalization, activation, convolution, and upsampling layers. Furthermore, as highlighted in ~\cite{corvi2023intriguing, corvi2023detection, bammey2023synthbuster}, no generator to date is entirely artifact-free, with frequency-domain artifacts still evident even in state-of-the-art multimodal generative models. 



\inputtable{dataset}
\inputfigure{2setup}
\noindent \textbf{Testing Scenarios.} Similar to the face identification and verification problems encountered in the field of face recognition, we consider utilize the fingerprint extractor in two model attribution problems, the 1:1 model verification and 1:N model identification problem, as shown in Figure~\ref{fig:2setup}. Settings and experimental setups for the two problems are illustrated below:
\begin{itemize}
    \item \textbf{Model Verification}: the 1:1 model verification problem aims to verify whether two generated images are from the same model or not. Model verification is performed by comparing the cosine similarity of the extracted fingerprint. To assess the performance, we generate 1,000 images for each model under test. Then we randomly selected 10,000 pairs out of these images, which consist of 5,000 negative pairs (images from different models) and 5,000 positive pairs (images from the same model). The evaluation employs metrics commonly used in face verification tasks: accuracy and the Area Under the Receiver Operating Characteristic Curve (AUC). Accuracy measures the correct verification of pairs as originating from the same or different models. We report the best accuracy across all similarity thresholds to assess the overall effectiveness of the verification system. The AUC provides a comprehensive measure of performance across all possible threshold levels, reflecting the trade-off between the true positive rate and the false positive rate. 
    \item \textbf{Model Identification}: the 1:N model identification problem aims to identify the specific generative model used to create a given image among N models. N is the total number of models in the model gallery. To assess the performance of model identification, we generate 1,000 images for each model under test. Out of these, 10 images are sampled to create the fingerprint for each model, while the rest serve as test samples. Each model's fingerprint is collected to form the model gallery. We compare the test sample with each fingerprint in the gallery and classify it into the class with the highest cosine similarity, which corresponds to the closest fingerprint distance. Performance is evaluated using classification accuracy and the F1 score.
\end{itemize}

\noindent \textbf{Compared methods.} To compare with existing methods, we derive two variants of our method: 1) \textit{Ours (Only-Syn)}, which is only trained on synthetic models without having seen any real-world models. 2) \textit{Ours (Fine-tune)}, which fine-tune the fingerprint extractor trained on synthetic models for one epoch using samples of models highlighted in \colorbox{gray!20}{gray} in Table~\ref{tab:dataset}.

We compare against seven existing model attribution methods. They are Marra\etal~\cite{marra2019gans}, Yu\etal~\cite{yu2019attributing}, DNA-Det~\cite{yang2022aaai}, RepMIX~\cite{bui2022repmix}, POSE~\cite{yang2023progressive}, and Abady\etal~\cite{abady2024siamese}. We train them on models in gray in Table~\ref{tab:dataset}. 
Although the works in~\cite{marra2019gans, yu2019attributing, yang2022aaai, bui2022repmix} are proposed to evaluate in a closed-set scenario, their the layers before the classification head can be adapted as a fingerprint extractor for use in a zero-shot model attribution context. This adaptation is implemented to evaluate the model identification and verification problems.

\subsection{Ablation Study (EQ1)}
\label{subsec:ablation}

In the ablation study section, we examine several key factors affecting the generalization performance of the fingerprint extractor. These factors include: the types of synthetic models and real images used for creating fingerprinted images, and the implementation of a sliding window strategy during the inference phase. This comprehensive evaluation helps in understanding how each component contributes to the effectiveness of the fingerprint extraction. Our default configurations within the tables are highlighted in \colorbox{gray!20}{gray}.
In this section, we evaluate on the \textit{Only-Syn} version of our method, without including any real generative models in training. 

\inputtable{ablation_arch}
\inputtable{ablation_seed}
\inputtable{ablation_dataset}
% \inputtable{ablation_input}
\inputtable{ablation_sw}
\inputfigure{block_num_ablation}

\noindent \textbf{Synthetic Options.} 
To evaluate the impact of individual synthesis options of synthetic models on the generalization of the trained fingerprint extractor in model verification and identification tasks, we conducted an ablation study. The results, presented in Table~\ref{tab:ablation_arch}, demonstrate that omitting any synthesis option reduces the generalization capabilities in model identification and verification tasks. Notably, the removal of the upsampling and activation layers significantly affects generalization performance. This observation aligns with the Frechet Frequency Distance scores listed in Table~\ref{tab:fid_compare}, where the absence of activation and upsampling variation results in the highest scores, indicating the lowest fidelity of synthetic models. 

\noindent \textbf{Depth of Synthetic Models.} We also discuss how depth of synthetic models influences the attribution performance. In Figure~\ref{fig:block_num_ablation}, we plot the total training time required for synthetic models with a maximum block count ranging from 1 to 3. For models with a single generative block, the averaged training time for all models is 42 GPU hours, involving models with 2 generative blocks, the training time is 133 hours, and increase to 393 hours when adding models with 3 generative blocks. Meanwhile, we observe an improvement in model verification accuracy when increasing the block count to 2, but no significant performance increase with 3 blocks. This indicates that using synthetic models with a maximum block count of 2 achieves the best trade-off between synthesis computation costs and model attribution performance.
% It is acknowledged that generative blocks determine the model's depth and parameter count, directly influencing the training cost. 

\inputtable{1vn_result}
\noindent \textbf{Real Dataset.} In Table~\ref{tab:ablation_dataset}, we evaluate how the choice of real datasets used to generate fingerprinted images with synthetic models affects the generalization performance of the trained fingerprint extractor. We consider three options for constructing the real image pool: the CelebA dataset, which exclusively contains face images; the LSUN dataset, featuring 20 different types of semantic images; and a composite of both datasets. The results indicate that the combination of the CelebA and LSUN datasets, which offers the richest semantic diversity, yields the best generalization performance. This phenomenon may be because the increasing the semantic diversity of fingerprinted images helps the fingerprint extractor focus on semantic-agnostic fingerprint features.


% \noindent \textbf{Input Signal.} In Table~\ref{tab:ablation_input}, we assess the impact of different input signals extracted from fingerprinted images on training of the fingerprint extractor. We consider three types of inputs: the RGB image, the DFT spectrum of the RGB image (Spectrum), and the DFT spectrum of the noise extracted from the RGB image (Noise Spectrum). The noise is extracted using a denoising process that primarily removes high-level semantic details from the image. We could observe that the ``Noise Spectrum" input yields the best performance, followed by ``Spectrum," with the direct RGB image showing the least effectiveness. This outcome suggests that the Noise Spectrum most effectively enhances the fingerprint information within the image~\cite{guo2023exposing}, making the fingerprint features more extractable and distinct.

\noindent \textbf{Sliding Window.}
During the inference phase for extracting fingerprints from test images of varying resolutions, we implement a sliding window strategy to accommodate different input sizes. The training resolution for the fingerprint extractor is set at 128$\times$128 pixels. For testing images that exceed this resolution, we employ an equally spaced sliding window technique to divide the image into N$\times$N patches, each with 128$\times$128 pixels. These patches are then individually processed to extract fingerprints. The final fingerprint for the entire input image is obtained by averaging the fingerprints from all these patches, ensuring comprehensive coverage and utilization of the image's information for accurate fingerprint extraction. For images with resolutions smaller than 128$\times$128, we resize them to the required 128$\times$128 resolution. As indicated in Table~\ref{tab:ablation_sw}, the sliding window strategy significantly enhances model identification performance. This improvement is attributed to the strategy's ability to capture a richer set of fingerprint information within the test image. Given that the performance of $3\times3$ and $4\times4$ sliding configurations shows similar results, we use the $3\times$3 configuration for inference efficiency.

% \inputfigure{generalization}
\subsection{Comparison with Existing Methods (EQ2)}
\label{subsec:compare_existing}
% \inputfigure{hist_tsne_compare}
\noindent \textbf{Model Identification}
In Table~\ref{tab:1vn_result}, we evaluate and compare our methods—Ours (Only-Syn) and Ours (Fine-tune)—against existing methods in the 1:N model identification scenario. The results show that most existing methods achieve high identification accuracy on classical models, which are seen in their training. However, their performance significantly declines when tested on unseen emerging models, dropping below 50\%. In contrast, our method ``Ours (Only-Syn)" demonstrates strong generalization capabilities. Despite being trained only on synthesized models and not having exposure to real generative models, it achieves over 90\% identification accuracy on the two sets of models. To align more closely with existing methods, we introduce ``Ours (Fine-tune)," which is further fine-tuned using classical model images. This adjustment improves the identification accuracy to 99\% on classical models, with a modest enhancement on emerging models. These results underscore that our fingerprint extractor, by leveraging synthetic models, exhibits superior generalization ability across real-world generative models due to its exposure to a broader model fingerprint variations. 

% Figure~\ref{fig:generalization} visualizing the extracted fingerprint embeddings also indicates this.

\inputtable{1v1_verify}
\noindent \textbf{Model Verification.} In Table~\ref{tab:1v1_verify}, we compare our methods against existing methods in the 1:1 model verification scenario, which involves determining whether two input images are generated by the same model. Similar to the findings in the 1:N model identification scenario, existing methods exhibit high verification accuracy for seen models but show significant performance degradation when applied to unseen models. Our method, Ours (Only-Syn), trained solely on synthetic models, demonstrates balanced performance across the two groups of model. This indicates a robust ability to generalize from synthetic model to real-world generative models. When we fine-tune the fingerprint extractor with images from classical models, as in Ours (Fine-tune), the performance on classical models is up to par with existing methods while maintaining superior performance on unseen emerging models. 

\input{Tables/cluster_result}

\noindent \textbf{Evaluation of Fingerprint Representation.} In Table~\ref{tab:cluster_result}, we include additional evaluation metrics to assess the quality of fingerprint representation: 1)\textit{Silhouette Score (SS)}~\cite{rousseeuw1987silhouettes}, which measures how similar a point is to its own cluster compared to other clusters. Higher values indicate better separation between fingerprints of different models. 2) \textit{Davies-Bouldin Index (DBI)}~\cite{davies1979cluster}, which measures the ratio of intra-cluster distance to inter-cluster distance. Lower values indicate more compact and separated fingerprint representation. As shown in the table, existing methods such as DNA-Det and POSE achieve relatively high SS and low DBI on seen classical models. However, their generalization to unseen emerging models is limited, with SS decreasing and DBI increasing significantly. In contrast, our method, Ours (Only-Syn), demonstrates more consistent performance across both classical and emerging models. Notably, after fine-tuning on classical models, Ours (Fine-tune) achieves the highest SS and lowest DBI compared with existing methods, indicating superior fingerprint separation and robust generalization across diverse generative models.


\subsection{Model Lineage Analysis (EQ3)}
\inputfigure{cvitai_lora}

At times, models may be stolen and utilized as the base model for training variants by other entities. Our model fingerprint extractor (Only-Syn) demonstrates the potential to trace back to the base model of such variants. To illustrate this capability, we gathered models from Civitai\footnote{https://civitai.com/}, a community renowned for its vast repository of high-quality Stable Diffusion models. The majority of these models are LoRA-variant models, fine-tuned from a base model using LoRA (Low-Rank Adaption)~\cite{hu2022lora}. We collected four distinct LoRA weights for each of the three most commonly used base models: SD15, PonyXL v6, and SDXL 1.0. Subsequently, we generated 10 images for each LoRA-equipped model and base model. Figure~\ref{fig:cvitai_lora}(a) show the generated images by these LoRA variants, which exhibit discernible differences in styles and semantics. We employing the averaged fingerprint of generated images as the fingerprint for the base model. As shown in Figure~\ref{fig:cvitai_lora}(b), the LoRA-equipped variants exhibit similar fingerprint embeddings with their respective base models. This result shows that we could easily attribute the LoRA variant to its base model comparing the extracting fingerprints from their generated images. 


\subsection{Visualization Results (EQ4)}
\input{Figures/syn_spectrum_compare_part}
We provide spectrum visualizations of real models and their closest synthetic models in Figure~\ref{fig:syn_spectrum_compare_part}. As shown, while the spectra of synthetic models do not exactly replicate those of real-world models, they exhibit notable similarities and diversity in frequency artifacts. This demonstrates the fidelity of our synthetic model generation process and its ability to approximate the spectral characteristics of real-world models effectively.

\section{Conclusions}

In this paper, we propose to tackle the model attribution problem in a more generalized scenario, to expand the attribution target to unseen models that are not included in training. To solve the problem, we propose a novel method by training on a large and diverse set of synthetic models, which mimic the fingerprint patterns of real generative models. The synthesis strategy is based a detailed analysis of how different model architectures and parameters influence fingerprint patterns, and is validated using two designed metrics to assess their fidelity and diversity. Experimental results demonstrate that our fingerprint extractor, trained on synthetic models, exhibits superior generalization capabilities in identifying and verifying unseen real generative models across various scenarios. 
