\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{booktabs}  
\usepackage[table]{xcolor} 
\usepackage{hyperref} 
\usepackage{multicol}

\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021

\newcommand{\inputsection}[1]{\input{Sections/#1}}
\newcommand{\inputfigure}[1]{\input{Figures/#1}}
\newcommand{\inputtable}[1]{\input{Tables/#1}}


\input{packages/defs}
\input{packages/header}
\input{packages/math_commands}

\newcommand{\etal}[0]{~et al.}
% \newcommand{\wrt}[0]{~w.r.t.}
% \newcommand{\ie}[0]{~i.e.}
% \newcommand{\eg}[0]{~e.g.}
\newcommand{\figtext}[1]{{\footnotesize #1}}
\newcommand{\Loss}{\text{Loss}}
\newcommand{\expert}{\operatorname{E}}

\begin{document}

\title{Model Synthesis for Zero-Shot Model Attribution}

\author{
  Tianyun Yang, Juan Cao, Danding Wang, Chang Xu~\IEEEmembership{Member,~IEEE}
  
\thanks{Tianyun Yang, Juan Cao, and Danding Wang are with the Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China. \protect E-mail: \{yangtianyun19z,caojuan,wangdanding\}@ict.ac.cn

Chang Xu is with School of Computer Science, Faculty of Engineering, University of Sydney, Australia. E-mail: c.xu@sydney.edu.au 

Code: \href{https://github.com/TianyunYoung/Model-Fingerprint}{https://github.com/TianyunYoung/Model-Fingerprint}
}}

% \author{IEEE Publication Technology,~\IEEEmembership{Staff,~IEEE,}
        % <-this % stops a space
% \thanks{This paper was produced by the IEEE Publication Technology Group. They are in Piscataway, NJ.}% <-this % stops a space
% \thanks{Manuscript received April 19, 2021; revised August 16, 2021.}}

% % The paper headers
% \markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2021}%
% {Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

% \IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.
% the ever-growing number of unknown models limits its application in the open world.
\maketitle

\begin{abstract}

Nowadays, generative models are shaping various fields such as art, design, and human-computer interaction, yet they are accompanied by copyright infringement and content management challenges. In response, existing research seeks to identify the unique fingerprints on the images they generate, which can be leveraged to attribute the generated images to their source models. However, existing methods are restricted to identifying models within a static set included in classifier training, incapable of adapting dynamically to newly emerging unseen models. To bridge this gap, this paper aims to develop a generalized model fingerprint extractor capable of zero-shot attribution that effectively attributes unseen models without exposure during training. Central to our method is a model synthesis technique, which generates numerous synthetic models that mimic the fingerprint patterns of real-world generative models. The design of the synthesis technique is motivated by observations on how the basic generative model’s architecture building blocks and parameters influence fingerprint patterns, and it is validated through designed metrics to examine synthetic models' fidelity. Our experiments demonstrate that the fingerprint extractor, trained solely on synthetic models, achieves impressive zero-shot generalization on a wide range of real-world generative models, improving model identification and verification accuracy on unseen models by over 40\% and 15\%, respectively, compared to existing approaches.
\end{abstract}

\begin{IEEEkeywords}
Model Fingerprint, Model Attribution, Model Synthesis, Zero-Shot Adaptation
\end{IEEEkeywords}

\inputsection{intro}
\inputsection{related}
\inputsection{method}
\inputsection{exp}

\bibliographystyle{IEEEtran}
\bibliography{reference.bib}


% \vspace{11pt}
% % \bf{If you include a photo:}
% \vspace{-33pt}
% \begin{IEEEbiography}[{% Figure removed}]{Tianyun Yang} is currently pursuing her Ph.D. degree at the Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China. She received her B.E. degree from Wuhan University in 2019. Her research interests lie in AI safety.
% \end{IEEEbiography}

% % \vspace{11pt}
% % \bf{If you will not include a photo:}
% % \vspace{-22pt}
% \begin{IEEEbiography}[{% Figure removed}]{Juan Cao} is a professor at the Institute of Computing Technology, Chinese Academy of Sciences, where she received her Ph.D. degree in 2008. Her research interests include multimedia content analysis and fake multimedia detection. She has over 90 publications in international journals and conferences including TKDE, TIP, KDD, WWW, CVPR, etc.
% \end{IEEEbiography}

% % \vspace{11pt}
% % \bf{If you will not include a photo:}
% % \vspace{-22pt}
% \begin{IEEEbiography}[{% Figure removed}]{Danding Wang} received her B.E. degree from Beihang University, China, in 2016. She received her Ph.D. degree from the National University of Singapore, Singapore, in 2021. She is currently an assistant researcher in the Media Synthesis and Forensics Lab of the Institute of Computing Technology, Chinese Academy of Sciences. Her research interests include explainable AI, misinformation detection, and AIGC detection.
% \end{IEEEbiography}

% % \vspace{-22pt}
% \begin{IEEEbiography}[{% Figure removed}]{Chang Xu} is ARC Future Fellow and Associate Professor at the School of Computer Science, University of Sydney. He received the University of Sydney Vice-Chancellor’s Award for Outstanding Early Career Research. His research interests lie in machine learning algorithms and related applications in computer vision. He has published over 100 papers in prestigious journals and top-tier conferences.
% \end{IEEEbiography}


\clearpage

\appendix

\subsection{Additional Analysis Results of Model Fingerprint}
\label{sec:appendix_analysis}

To validate the theoretical insights in the main text, we also provide additional experiments using ProGAN, SNGAN, and StyleGAN2 models with varying types of network components below, analyzing their spectrum influence on output images.

\subsubsection{\textbf{Upsampling Effects}} As discussed in the main text, different types of upsampling tend to leave different spectral characteristics due to differences in their upsampling kernels' shapes. To illustrate this, we trained ProGAN~\cite{karras2017progressive} and SNGAN~\cite{miyato2018spectral} models on the CelebA dataset, each with three types of upsampling layers. Figure~\ref{fig:up_dft} presents the reduced 1D power spectrum of these models, where the x-axis indicates different frequencies from low to high, and the y-axis denotes the power of these frequency components. The figure illustrates that nearest and bilinear, which act as low-pass filters, significantly reduce high-frequency content, resulting in lower power at higher frequencies on the spectrum's right side. In contrast, transposed convolution tends to preserve more high-frequency components. This is because the learned convolution kernels in transposed convolution layers are not guaranteed to be low-pass, allowing more high-frequency artifacts to remain in the generated images~\cite{zhang2019detecting}. These results are consistent with our theoretical analysis presented in the main text.

\subsubsection{\textbf{Convolution Effects}} As discussed in the main text, the spectral fingerprints of convolutional parameters are shaped by the spectrum characteristics of their kernels. Different kernels can lead to different emphases on certain frequency bands, thus altering the model's overall spectrum pattern. This phenomenon is illustrated in Figure~\ref{fig:conv_dft}, which shows the spectrum of two StyleGAN2~\cite{karras2020stylegan2} models that have identical architecture but differ in their parameters, trained respectively on the FFHQ and FFHQu datasets. For this analysis, we extracted the parameters from a specific output channel in the last convolutional layer of each model's final generative block. We then plot two figures for each model: the summed spectrum of the convolution kernels (left), and the spectrum of the output feature map for this channel (right). As demonstrated in the figure, for both models, the spectrum of the output feature map closely mirrors the spectrum of the convolution parameters. This alignment highlights how the convolutional layer's kernel characteristics directly shape the spectrum pattern of the output, which validates the theoretical analysis outlined in the main text. 

\subsubsection{\textbf{Nonlinear Activation Effects}} As discussed in the main text, the choice of activation functions can also influence a model’s frequency response, with different functions generating distinct harmonic patterns in the frequency spectrum. To verify this, we trained ProGAN and SNGAN models on the CelebA dataset, each with three different activation functions. As shown in Figure~\ref{fig:act_dft}, generative models with ReLU activation generate images with more high-frequency components than Sigmoid and Tanh, as indicated by higher values on the right side of the reduced spectrum\footnote{To make the high-frequency discrepancy more evident, we first use a denoising filter by~\cite{zhang2017beyond} with noise parameter $\sigma=1$ to remove the low-frequency semantic contents.}.

\inputfigure{up_dft}
\inputfigure{conv_dft}
\inputfigure{act_dft}
\inputfigure{norm_dft}

\subsubsection{\textbf{Normalization Effects}} As discussed in the main text, different normalization methods share similar mathematical formulations in the frequency domain, but they can still influence a network’s behavior in distinct ways. Variations in the optimization landscapes introduced by different normalization techniques can result in distinctive parameter distributions and, consequently, unique spectral patterns. To illustrate this, we train six SNGAN models using two types of normalization, each type with three different initial seeds. As demonstrated in Figure~\ref{fig:norm_dft}, models with the same normalization type often exhibit more consistent spectral patterns compared to those trained using different normalization methods. This underscores the impact of normalization choices on the diversity of spectrum patterns of generative models. 


\subsection{Attribution Robustness Analysis}
\noindent \textbf{Different Generation Configurations.} In the generation phase, diffusion models offer various configurable parameters, such as the sampling step, sampling scheduler, guidance scale, and the resolution of the generated images. These variations in setup can influence the quality of the generated images. To assess whether the extracted fingerprint remains stable amidst these variations, we conducted evaluations on three Stable Diffusion models: SD1.5, SD2.1, and SDXL 1.0. We systematically varied these configurations during image generation. Samplers from DDIM~\cite{song2020denoising}, PNDM, LMS, and Euler~\cite{karras2022elucidating} were used randomly. The guidance scale was uniformly sampled between 5 and 10, while image dimensions varied with widths and heights ranging from 256 to 1024 pixels, and sampling steps from 20 to 50. 
Figure~\ref{fig:vary_configs} illustrates the t-SNE plots of fingerprint embeddings extracted by our fingerprint extractor (Only-Syn), from models under both fixed and varied generation configurations. The results demonstrate that images with varied generation settings exhibit fingerprints as stable as those with fixed settings, underscoring the robustness of our trained fingerprint extractor to variations in diffusion generation configurations.

\noindent \textbf{Image Perturbations.} Images generated by models may undergo various post-processings during transmission on social media. To evaluate the robustness of our fingerprint extractor (Only-Syn), we tested it against the two most common types of post-processing: JPEG compression and crop resizing. For these tests, we utilized 'ImageCompression' and 'RandomResizedCrop' from the Albumentations library, where 'RandomResizedCrop' randomly crops a patch from the image and resizes it back to the original dimensions. As depicted in Figure~\ref{fig:robust}, we charted the model identification accuracy alongside the JPEG compression rate and crop ratio. It was observed that our trained fingerprint extractor is relatively robust to the crop resizing post-processing, yet exhibits sensitivity to JPEG compression. To mitigate this sensitivity, we incorporate them as a form of data augmentation during training. The immunized fingerprint extractor, as depicted by the red line in the results, demonstrates strong robustness against crop resizing and improvement in resilience against JPEG compression. However, the performance does not fully recover to that of the unaltered data under JPEG compression. This is primarily because JPEG compression can significantly alter the spectrum pattern, thereby degrading the original pattern left by the model, as also noted in~\cite{corvi2023intriguing}. Addressing the robustness against JPEG compression will be a focus of the future work. 

\inputfigure{vary_configs}
\inputfigure{robust}

\subsection{Additional Ablation Study}
\label{sec:appendix_ablation}

\noindent \textbf{Parameter $N$ for FFD Metric Calculation.} We conduct an ablation study to examine the impact of parameter $N$ on the FFD evaluation criterion. FFD values are calculated under various conditions: All, w/o K, w/o L, w/o S, w/o U, w/o A, w/o N, and w/o seed, across different numbers of images ($N$) ranging from 50 to 500. We then compute the Pearson rank correlation between the FFD values at $N = 500$ and those at each tested $N$. As shown in~\cref{tab:ablation_N}, the Pearson rank correlation with the baseline ($N = 500$) remains consistently close to 1, indicating that changes in $N$ do not affect the ranking of results. This demonstrates that the FFD metric is robust to variations in $N$ within the tested range.


\begin{table}[tbp]
\centering
\caption{Pearson rank correlation between the FFD values at $N$=500 and those across $N$ from 50 to 500.}
\label{tab:ablation_N}
\scalebox{0.98}{
\begin{tabular}{cccc}
\toprule[1pt]
Number of Images & Pearson Correlation with N=500\\  
\midrule
N=50 & 99.97 \%\\
N=100 & 99.98\%\\
N=200 &  99.99\% \\
N=300 &  100.00 \%\\
N=400 & 100.00 \%\\
N=500 & 100.00 \%\\
\bottomrule[1pt]
\end{tabular}}
\end{table}


\subsection{Additional Visualization Results.}

\noindent \textbf{Spectrum Visualization of Real and Synthetic Models.} In Figure~\ref{fig:syn_spectrum_compare}, we present additional spectrum visualizations of real models and their closest synthetic counterparts. The real models include GANs, VAEs, Flows, and Diffusion models. As shown, while the spectra of synthetic models do not perfectly match those of real models, they exhibit notable similarities and diverse frequency artifacts. These similarities provide empirical evidence that our synthetic training strategy effectively approximates the frequency characteristics of unseen models, thereby improving generalization.


\noindent \textbf{Model Verification Cases.} We provide some model verification cases produced by our model fingerprint extractor in Figure~\ref{fig:cases_dalle}, Figure~\ref{fig:cases_sd}, and Figure~\ref{fig:cases_civitai}.  Model verification aims at verifying whether two generated images are from the same model by comparing extracted fingerprints. The threshold to make the prediction is empirically set as 0.81, which is the best threshold of model verification on the Text2Image models in Table~\ref{tab:dataset}. \textit{It is important to note that, to evaluate the zero-shot attribution ability in the open world, the fingerprint extractor producing these results is solely trained on synthetic models, without having seen any images from the real generative models tested below.}

\inputfigure{cases_dalle}
\inputfigure{cases_sd}
\inputfigure{cases_civitai}
\inputfigure{syn_spectrum_compare}

\subsection{Experimental Setups}
When training on synthetic models, the real images used to generated fingerprinted images are firstly resized to 128$\times$128. The batch size is set to 200. The initial learning rate is $1e^{-4}$ with a warm up cosine scheduler. The margin for the triplet loss is 0.3. The total training epoch is 40. 

\subsection{Limitations}

As depicted in Figure~\ref{fig:cvitai_lora} in the main text, the extracted fingerprints from images of LoRA-equiped models are similar to their Base models, which allows us to attribute the LoRA variants to their base models. However, this also indicates a limitation of our method in distinguishing different LoRA variants from the same base model. We leave this more fine-grained model attribution challenge for future work.


\end{document}


