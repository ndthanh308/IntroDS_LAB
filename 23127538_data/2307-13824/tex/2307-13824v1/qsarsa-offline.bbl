\begin{thebibliography}{}

\bibitem[Arulkumaran et~al., 2017]{arulkumaran2017brief}
Arulkumaran, K., Deisenroth, M.~P., Brundage, M., and Bharath, A.~A. (2017).
\newblock A brief survey of deep reinforcement learning.
\newblock {\em arXiv preprint arXiv:1708.05866}.

\bibitem[Bradbury et~al., 2021]{bradbury2021jax}
Bradbury, J., Frostig, R., Hawkins, P., Johnson, M.~J., Leary, C., Maclaurin,
  D., Necula, G., Paszke, A., VanderPlas, J., Wanderman-Milne, S., et~al.
  (2021).
\newblock Jax: Autograd and xla.
\newblock {\em Astrophysics Source Code Library}, pages ascl--2111.

\bibitem[Brandfonbrener et~al., 2021]{brandfonbrener2021offline}
Brandfonbrener, D., Whitney, W., Ranganath, R., and Bruna, J. (2021).
\newblock Offline rl without off-policy evaluation.
\newblock {\em Advances in neural information processing systems},
  34:4933--4946.

\bibitem[Buckman et~al., 2020]{buckman2020importance}
Buckman, J., Gelada, C., and Bellemare, M.~G. (2020).
\newblock The importance of pessimism in fixed-dataset policy optimization.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Chen et~al., 2021]{chen2021decision}
Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M., Abbeel, P.,
  Srinivas, A., and Mordatch, I. (2021).
\newblock Decision transformer: Reinforcement learning via sequence modeling.
\newblock {\em Advances in neural information processing systems},
  34:15084--15097.

\bibitem[Chen et~al., 2020]{chen2020bail}
Chen, X., Zhou, Z., Wang, Z., Wang, C., Wu, Y., and Ross, K. (2020).
\newblock Bail: Best-action imitation learning for batch deep reinforcement
  learning.
\newblock {\em Advances in Neural Information Processing Systems},
  33:18353--18363.

\bibitem[Dadashi et~al., 2021]{dadashi2021offline}
Dadashi, R., Rezaeifar, S., Vieillard, N., Hussenot, L., Pietquin, O., and
  Geist, M. (2021).
\newblock Offline reinforcement learning with pseudometric learning.
\newblock In {\em International Conference on Machine Learning}, pages
  2307--2318. PMLR.

\bibitem[Fakoor et~al., 2021]{fakoor2021continuous}
Fakoor, R., Mueller, J.~W., Asadi, K., Chaudhari, P., and Smola, A.~J. (2021).
\newblock Continuous doubly constrained batch reinforcement learning.
\newblock {\em Advances in Neural Information Processing Systems},
  34:11260--11273.

\bibitem[Fu et~al., 2020]{fu2020d4rl}
Fu, J., Kumar, A., Nachum, O., Tucker, G., and Levine, S. (2020).
\newblock D4rl: Datasets for deep data-driven reinforcement learning.
\newblock {\em arXiv preprint arXiv:2004.07219}.

\bibitem[Fujimoto and Gu, 2021]{fujimoto2021minimalist}
Fujimoto, S. and Gu, S.~S. (2021).
\newblock A minimalist approach to offline reinforcement learning.
\newblock {\em Advances in neural information processing systems},
  34:20132--20145.

\bibitem[Fujimoto et~al., 2018]{fujimoto2018addressing}
Fujimoto, S., Hoof, H., and Meger, D. (2018).
\newblock Addressing function approximation error in actor-critic methods.
\newblock In {\em International Conference on Machine Learning}, pages
  1587--1596. PMLR.

\bibitem[Fujimoto et~al., 2019]{fujimoto2019off}
Fujimoto, S., Meger, D., and Precup, D. (2019).
\newblock Off-policy deep reinforcement learning without exploration.
\newblock In {\em International Conference on Machine Learning}, pages
  2052--2062. PMLR.

\bibitem[Garg et~al., 2023]{garg2023extreme}
Garg, D., Hejna, J., Geist, M., and Ermon, S. (2023).
\newblock Extreme {Q}-learning: Maxent rl without entropy.
\newblock {\em arXiv preprint arXiv:2301.02328}.

\bibitem[Ghasemipour et~al., 2021]{ghasemipour2021emaq}
Ghasemipour, S. K.~S., Schuurmans, D., and Gu, S.~S. (2021).
\newblock {EM}a{Q}: Expected-max {Q}-learning operator for simple yet effective
  offline and online rl.
\newblock In {\em International Conference on Machine Learning}, pages
  3682--3691. PMLR.

\bibitem[Gulcehre et~al., 2021]{gulcehre2021regularized}
Gulcehre, C., Colmenarejo, S.~G., Wang, Z., Sygnowski, J., Paine, T., Zolna,
  K., Chen, Y., Hoffman, M., Pascanu, R., and de~Freitas, N. (2021).
\newblock Regularized behavior value estimation.
\newblock {\em arXiv preprint arXiv:2103.09575}.

\bibitem[Haarnoja et~al., 2018]{haarnoja2018soft}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. (2018).
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In {\em International conference on machine learning}, pages
  1861--1870. PMLR.

\bibitem[Harris et~al., 2020]{harris2020array}
Harris, C.~R., Millman, K.~J., Van Der~Walt, S.~J., Gommers, R., Virtanen, P.,
  Cournapeau, D., Wieser, E., Taylor, J., Berg, S., Smith, N.~J., et~al.
  (2020).
\newblock Array programming with numpy.
\newblock {\em Nature}, 585(7825):357--362.

\bibitem[Hoffman et~al., 2020]{hoffman2020acme}
Hoffman, M., Shahriari, B., Aslanides, J., Barth-Maron, G., Behbahani, F.,
  Norman, T., Abdolmaleki, A., Cassirer, A., Yang, F., Baumli, K., et~al.
  (2020).
\newblock Acme: A research framework for distributed reinforcement learning.
\newblock {\em arXiv preprint arXiv:2006.00979}.

\bibitem[Janner et~al., 2021]{janner2021offline}
Janner, M., Li, Q., and Levine, S. (2021).
\newblock Offline reinforcement learning as one big sequence modeling problem.
\newblock {\em Advances in neural information processing systems},
  34:1273--1286.

\bibitem[Kostrikov et~al., 2021a]{kostrikov2021offline}
Kostrikov, I., Fergus, R., Tompson, J., and Nachum, O. (2021a).
\newblock Offline reinforcement learning with fisher divergence critic
  regularization.
\newblock In {\em International Conference on Machine Learning}, pages
  5774--5783. PMLR.

\bibitem[Kostrikov et~al., 2021b]{kostrikov2021offline2}
Kostrikov, I., Nair, A., and Levine, S. (2021b).
\newblock Offline reinforcement learning with implicit {Q}-learning.
\newblock {\em arXiv preprint arXiv:2110.06169}.

\bibitem[Kumar et~al., 2019]{kumar2019stabilizing}
Kumar, A., Fu, J., Soh, M., Tucker, G., and Levine, S. (2019).
\newblock Stabilizing off-policy {Q}-learning via bootstrapping error
  reduction.
\newblock {\em Advances in Neural Information Processing Systems}, 32.

\bibitem[Kumar et~al., 2020]{kumar2020conservative}
Kumar, A., Zhou, A., Tucker, G., and Levine, S. (2020).
\newblock Conservative {Q}-learning for offline reinforcement learning.
\newblock {\em Advances in Neural Information Processing Systems},
  33:1179--1191.

\bibitem[Lee et~al., 2020]{lee2020representation}
Lee, B.-J., Lee, J., and Kim, K.-E. (2020).
\newblock Representation balancing offline model-based reinforcement learning.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Levine, 2018]{levine2018reinforcement}
Levine, S. (2018).
\newblock Reinforcement learning and control as probabilistic inference:
  Tutorial and review.
\newblock {\em arXiv preprint arXiv:1805.00909}.

\bibitem[Levine et~al., 2020]{levine2020offline}
Levine, S., Kumar, A., Tucker, G., and Fu, J. (2020).
\newblock Offline reinforcement learning: Tutorial, review, and perspectives on
  open problems.
\newblock {\em arXiv preprint arXiv:2005.01643}.

\bibitem[Lyu et~al., 2022]{lyu2022mildly}
Lyu, J., Ma, X., Li, X., and Lu, Z. (2022).
\newblock Mildly conservative {Q}-learning for offline reinforcement learning.
\newblock {\em Advances in Neural Information Processing Systems},
  35:1711--1724.

\bibitem[Peng et~al., 2019]{peng2019advantage}
Peng, X.~B., Kumar, A., Zhang, G., and Levine, S. (2019).
\newblock Advantage-weighted regression: Simple and scalable off-policy
  reinforcement learning.
\newblock {\em arXiv preprint arXiv:1910.00177}.

\bibitem[Rezaeifar et~al., 2022]{rezaeifar2022offline}
Rezaeifar, S., Dadashi, R., Vieillard, N., Hussenot, L., Bachem, O., Pietquin,
  O., and Geist, M. (2022).
\newblock Offline reinforcement learning as anti-exploration.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~36, pages 8106--8114.

\bibitem[Silver et~al., 2017]{silver2017mastering}
Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez,
  A., Hubert, T., Baker, L., Lai, M., Bolton, A., et~al. (2017).
\newblock Mastering the game of go without human knowledge.
\newblock {\em nature}, 550(7676):354--359.

\bibitem[Vinyals et~al., 2019]{vinyals2019grandmaster}
Vinyals, O., Babuschkin, I., Czarnecki, W.~M., Mathieu, M., Dudzik, A., Chung,
  J., Choi, D.~H., Powell, R., Ewalds, T., Georgiev, P., et~al. (2019).
\newblock Grandmaster level in starcraft ii using multi-agent reinforcement
  learning.
\newblock {\em Nature}, 575(7782):350--354.

\bibitem[Wang et~al., 2022]{wang2022diffusion}
Wang, Z., Hunt, J.~J., and Zhou, M. (2022).
\newblock Diffusion policies as an expressive policy class for offline
  reinforcement learning.
\newblock {\em arXiv preprint arXiv:2208.06193}.

\bibitem[Wang et~al., 2020]{wang2020critic}
Wang, Z., Novikov, A., Zolna, K., Merel, J.~S., Springenberg, J.~T., Reed,
  S.~E., Shahriari, B., Siegel, N., Gulcehre, C., Heess, N., et~al. (2020).
\newblock Critic regularized regression.
\newblock {\em Advances in Neural Information Processing Systems},
  33:7768--7778.

\bibitem[Wu et~al., 2019]{wu2019behavior}
Wu, Y., Tucker, G., and Nachum, O. (2019).
\newblock Behavior regularized offline reinforcement learning.
\newblock {\em arXiv preprint arXiv:1911.11361}.

\bibitem[Yang et~al., 2022]{yang2022regularized}
Yang, S., Wang, Z., Zheng, H., Feng, Y., and Zhou, M. (2022).
\newblock A regularized implicit policy for offline reinforcement learning.
\newblock {\em arXiv preprint arXiv:2202.09673}.

\bibitem[Yu et~al., 2021]{yu2021combo}
Yu, T., Kumar, A., Rafailov, R., Rajeswaran, A., Levine, S., and Finn, C.
  (2021).
\newblock Combo: Conservative offline model-based policy optimization.
\newblock {\em Advances in neural information processing systems},
  34:28954--28967.

\bibitem[Zhang and Kashima, 2023]{zhang2022behavior}
Zhang, G. and Kashima, H. (2023).
\newblock Behavior estimation from multi-source data for offline reinforcement
  learning.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~37, pages 11201--11209.

\end{thebibliography}
