{
  "2301-02328": {
    "title": "Extreme Q-Learning: MaxEnt RL without Entropy",
    "authors": [
      "Divyansh Garg",
      "Joey Hejna",
      "M. Geist",
      "Stefano Ermon"
    ],
    "submission_date": "2023-01-05",
    "semantic_scholar_id": "2c2180fbe7f38e88b1123e5fab43785b66814e5d"
  },
  "2211-16078": {
    "title": "Behavior Estimation from Multi-Source Data for Offline Reinforcement Learning",
    "authors": [
      "Guoxi Zhang",
      "H. Kashima"
    ],
    "submission_date": "2022-11-29",
    "semantic_scholar_id": "6616965ea639e90c2413cff00aedf0133ce80fd9"
  },
  "2208-06193": {
    "title": "Diffusion Policies as an Expressive Policy Class for Offline Reinforcement Learning",
    "authors": [
      "Zhendong Wang",
      "Jonathan J. Hunt",
      "Mingyuan Zhou"
    ],
    "submission_date": "2022-08-12",
    "semantic_scholar_id": "2cbea7615ebecea2c414d8fbad47d5d258a5c3b4"
  },
  "2206-04745": {
    "title": "Mildly Conservative Q-Learning for Offline Reinforcement Learning",
    "authors": [
      "Jiafei Lyu",
      "Xiaoteng Ma",
      "Xiu Li",
      "Zongqing Lu"
    ],
    "submission_date": "2022-06-09",
    "semantic_scholar_id": "7e045a7fe78a6c0de5511980f292c42d1055f396"
  },
  "2110-06169": {
    "title": "Offline Reinforcement Learning with Implicit Q-Learning",
    "authors": [
      "Ilya Kostrikov",
      "Ashvin Nair",
      "S. Levine"
    ],
    "submission_date": "2021-10-12",
    "semantic_scholar_id": "348a855fe01f3f4273bf0ecf851ca688686dbfcc"
  },
  "2106-08909": {
    "title": "Offline RL Without Off-Policy Evaluation",
    "authors": [
      "David Brandfonbrener",
      "William F. Whitney",
      "R. Ranganath",
      "Joan Bruna"
    ],
    "submission_date": "2021-06-16",
    "semantic_scholar_id": "a3b82f4fc10caf6243afbd77c9c990ce03ae36d1"
  },
  "2106-06860": {
    "title": "A Minimalist Approach to Offline Reinforcement Learning",
    "authors": [
      "Scott Fujimoto",
      "S. Gu"
    ],
    "submission_date": "2021-06-12",
    "semantic_scholar_id": "c879b25308026d6538e52b27bcf4fd3cb60855f3"
  },
  "2106-06431": {
    "title": "Offline Reinforcement Learning as Anti-Exploration",
    "authors": [
      "Shideh Rezaeifar",
      "Robert Dadashi",
      "Nino Vieillard",
      "L'eonard Hussenot",
      "Olivier Bachem",
      "O. Pietquin",
      "M. Geist"
    ],
    "submission_date": "2021-06-11",
    "semantic_scholar_id": "399806e861a2ef960a81b37b593c2176a728c399"
  },
  "2106-02039": {
    "title": "Offline Reinforcement Learning as One Big Sequence Modeling Problem",
    "authors": [
      "Michael Janner",
      "Qiyang Li",
      "S. Levine"
    ],
    "submission_date": "2021-06-03",
    "semantic_scholar_id": "f864d4d2267abba15eb43db54f58286aef78292b"
  },
  "2106-01345": {
    "title": "Decision Transformer: Reinforcement Learning via Sequence Modeling",
    "authors": [
      "Lili Chen",
      "Kevin Lu",
      "A. Rajeswaran",
      "Kimin Lee",
      "Aditya Grover",
      "M. Laskin",
      "P. Abbeel",
      "A. Srinivas",
      "Igor Mordatch"
    ],
    "submission_date": "2021-06-02",
    "semantic_scholar_id": "c1ad5f9b32d80f1c65d67894e5b8c2fdf0ae4500"
  },
  "2103-09575": {
    "title": "Regularized Behavior Value Estimation",
    "authors": [
      "Caglar Gulcehre",
      "Sergio Gomez Colmenarejo",
      "Ziyun Wang",
      "Jakub Sygnowski",
      "T. Paine",
      "Konrad Zolna",
      "Yutian Chen",
      "Matthew W. Hoffman",
      "Razvan Pascanu",
      "Nando de Freitas"
    ],
    "submission_date": "2021-03-17",
    "semantic_scholar_id": "0dbef95ac15785e03b54a529a3d85128c19aa09a"
  },
  "2103-08050": {
    "title": "Offline Reinforcement Learning with Fisher Divergence Critic Regularization",
    "authors": [
      "Ilya Kostrikov",
      "Jonathan Tompson",
      "R. Fergus",
      "Ofir Nachum"
    ],
    "submission_date": "2021-03-14",
    "semantic_scholar_id": "362cc80481b288874af0428107ab31e955dcf09f"
  },
  "2103-01948": {
    "title": "Offline Reinforcement Learning with Pseudometric Learning",
    "authors": [
      "Robert Dadashi",
      "Shideh Rezaeifar",
      "Nino Vieillard",
      "L'eonard Hussenot",
      "O. Pietquin",
      "M. Geist"
    ],
    "submission_date": "2021-03-02",
    "semantic_scholar_id": "169fc06bbdadf008cb72a80762b5050bc5756ee4"
  },
  "2102-09225": {
    "title": "Continuous Doubly Constrained Batch Reinforcement Learning",
    "authors": [
      "Rasool Fakoor",
      "Jonas W. Mueller",
      "P. Chaudhari",
      "Alex Smola"
    ],
    "submission_date": "2021-02-18",
    "semantic_scholar_id": "6578b23ffa1be9fda80c35bc10fc83b193ff725a"
  },
  "2102-08363": {
    "title": "COMBO: Conservative Offline Model-Based Policy Optimization",
    "authors": [
      "Tianhe Yu",
      "Aviral Kumar",
      "Rafael Rafailov",
      "A. Rajeswaran",
      "S. Levine",
      "Chelsea Finn"
    ],
    "submission_date": "2021-02-16",
    "semantic_scholar_id": "245682e8b3fa76f4a3e2991b5497577af95cbb3f"
  },
  "2009-06799": {
    "title": "The Importance of Pessimism in Fixed-Dataset Policy Optimization",
    "authors": [
      "Jacob Buckman",
      "Carles Gelada",
      "Marc G. Bellemare"
    ],
    "submission_date": "2020-09-15",
    "semantic_scholar_id": "86fa0f350b0ede3a86e31aa2900af551531ee570"
  },
  "2007-11091": {
    "title": "EMaQ: Expected-Max Q-Learning Operator for Simple Yet Effective Offline and Online RL",
    "authors": [
      "Seyed Kamyar Seyed Ghasemipour",
      "D. Schuurmans",
      "S. Gu"
    ],
    "submission_date": "2020-07-21",
    "semantic_scholar_id": "fa7f88f77de02ae9389e514a1cd13083a624ec78"
  },
  "2006-15134": {
    "title": "Critic Regularized Regression",
    "authors": [
      "Ziyun Wang",
      "Alexander Novikov",
      "Konrad Zolna",
      "Jost Tobias Springenberg",
      "Scott E. Reed",
      "Bobak Shahriari",
      "Noah Siegel",
      "J. Merel",
      "Caglar Gulcehre",
      "N. Heess",
      "Nando de Freitas"
    ],
    "submission_date": "2020-06-26",
    "semantic_scholar_id": "7acbdb961f67d50fef359066f2a1d7755cf16ee2"
  },
  "2006-10256": {
    "title": "Array programming with NumPy",
    "authors": [
      "Charles R. Harris",
      "K. Millman",
      "S. Walt",
      "R. Gommers",
      "Pauli Virtanen",
      "D. Cournapeau",
      "Eric Wieser",
      "Julian Taylor",
      "Sebastian Berg",
      "Nathaniel J. Smith",
      "Robert Kern",
      "Matti Picus",
      "Stephan Hoyer",
      "M. Kerkwijk",
      "M. Brett",
      "A. Haldane",
      "Jaime Fern'andez del R'io",
      "Marcy Wiebe",
      "Pearu Peterson",
      "Pierre G'erard-Marchant",
      "Kevin Sheppard",
      "Tyler Reddy",
      "Warren Weckesser",
      "Hameer Abbasi",
      "C. Gohlke",
      "T. Oliphant"
    ],
    "submission_date": "2020-06-18",
    "semantic_scholar_id": "024a2c03be8e468e7c4fdf9bda36cdc0eaae85fb"
  },
  "2006-04779": {
    "title": "Conservative Q-Learning for Offline Reinforcement Learning",
    "authors": [
      "Aviral Kumar",
      "Aurick Zhou",
      "G. Tucker",
      "S. Levine"
    ],
    "submission_date": "2020-06-08",
    "semantic_scholar_id": "28db20a81eec74a50204686c3cf796c42a020d2e"
  },
  "2006-00979": {
    "title": "Acme: A Research Framework for Distributed Reinforcement Learning",
    "authors": [
      "Matthew W. Hoffman",
      "Bobak Shahriari",
      "John Aslanides",
      "Gabriel Barth-Maron",
      "Feryal M. P. Behbahani",
      "Tamara Norman",
      "A. Abdolmaleki",
      "Albin Cassirer",
      "Fan Yang",
      "Kate Baumli",
      "Sarah Henderson",
      "Alexander Novikov",
      "Sergio Gomez Colmenarejo",
      "Serkan Cabi",
      "Caglar Gulcehre",
      "T. Paine",
      "A. Cowie",
      "Ziyun Wang",
      "Bilal Piot",
      "Nando de Freitas"
    ],
    "submission_date": "2020-06-01",
    "semantic_scholar_id": "2e1ac521d53e09e90fd9b4bf949f667d756853de"
  },
  "2005-01643": {
    "title": "Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems",
    "authors": [
      "S. Levine",
      "Aviral Kumar",
      "G. Tucker",
      "Justin Fu"
    ],
    "submission_date": "2020-05-04",
    "semantic_scholar_id": "5e7bc93622416f14e6948a500278bfbe58cd3890"
  },
  "2004-07219": {
    "title": "D4RL: Datasets for Deep Data-Driven Reinforcement Learning",
    "authors": [
      "Justin Fu",
      "Aviral Kumar",
      "Ofir Nachum",
      "G. Tucker",
      "S. Levine"
    ],
    "submission_date": "2020-04-15",
    "semantic_scholar_id": "a326d9f2d2d351001fece788165dbcbb524da2e4"
  },
  "1910-00177": {
    "title": "Advantage-Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning",
    "authors": [
      "Xue Bin Peng",
      "Aviral Kumar",
      "Grace Zhang",
      "S. Levine"
    ],
    "submission_date": "2019-10-01",
    "semantic_scholar_id": "ad14227e4f51276892ffc37aa43fd8750bb5eba8"
  },
  "1910-12179": {
    "title": "BAIL: Best-Action Imitation Learning for Batch Deep Reinforcement Learning",
    "authors": [
      "Xinyue Chen",
      "Zijian Zhou",
      "Z. Wang",
      "Che Wang",
      "Yanqiu Wu",
      "Qing Deng",
      "K. Ross"
    ],
    "submission_date": "2019-09-25",
    "semantic_scholar_id": "b054fc685c3fa56459d5e49e4b42547164f8e024"
  },
  "1906-00949": {
    "title": "Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction",
    "authors": [
      "Aviral Kumar",
      "Justin Fu",
      "G. Tucker",
      "S. Levine"
    ],
    "submission_date": "2019-06-03",
    "semantic_scholar_id": "82b4b03a4659d6e04bd7cbf51d6e08fde1348dbd"
  },
  "1812-02900": {
    "title": "Off-Policy Deep Reinforcement Learning without Exploration",
    "authors": [
      "Scott Fujimoto",
      "D. Meger",
      "Doina Precup"
    ],
    "submission_date": "2018-12-07",
    "semantic_scholar_id": "5285cb8faada5de8a92a47622950f6cfd476ac1d"
  },
  "1805-00909": {
    "title": "Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review",
    "authors": [
      "S. Levine"
    ],
    "submission_date": "2018-05-02",
    "semantic_scholar_id": "6ecc4b1ab05f3ec12484a0ea36abfd6271c5c5ba"
  },
  "1802-09477": {
    "title": "Addressing Function Approximation Error in Actor-Critic Methods",
    "authors": [
      "Scott Fujimoto",
      "H. V. Hoof",
      "D. Meger"
    ],
    "submission_date": "2018-02-26",
    "semantic_scholar_id": "4debb99c0c63bfaa97dd433bc2828e4dac81c48b"
  },
  "1801-01290": {
    "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "authors": [
      "Tuomas Haarnoja",
      "Aurick Zhou",
      "P. Abbeel",
      "S. Levine"
    ],
    "submission_date": "2018-01-04",
    "semantic_scholar_id": "811df72e210e20de99719539505da54762a11c6d"
  }
}