%------------------------------------------------------------------------------
\section{Evaluation}
\label{sec:evaluation}
%------------------------------------------------------------------------------

% Figure environment removed

% Figure environment removed

% Figure environment removed


The main objective of \sys\ is to bring flexibility in distributed XR for realizing effective server assistance to XR use cases.
For evaluation, we implement three XR use cases in Figure ~\ref{fig:examplefig} and set four distribution scenarios in Figure~\ref{fig:arconfigs} and~\ref{fig:vrconfigs}.
We compare \sys\ to the existing distributed XR platforms in the supportability to our distribution scenarios.
Then, we evaluate the offloading impacts in the scenarios in terms of pipeline latency and throughput.
Additionally, we evaluate the benefit of the \sys\ design compared to the thread-level SP frameworks: GStreamer~\cite{gstreamer} and RaftLib~\cite{beard2017raftlib}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Experimental Testbed}
\label{sec:evaltestbed}
In our setup, the client is NVIDIA Jetson AGX Xavier~\cite{jetsonagx} with 8 core ARMv8 CPU, Volta GPU, and 32 GB memory shared by CPU and GPU.
The Jetson runs in 15W and 30W power modes (Jet15W using 4 cores and Jet30W using 8 cores).
The server has Intel Core i7-10700, 32 GB memory, and NVIDIA RTX 2070 of 8 GB GDDR6 memory.
The server and client are connected via Gigabit Ethernet of 1 Gbps bandwidth with round-trip time (RTT) of 1.5 ms.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{XR Applications and Distribution Scenarios}
\label{sec:exampleapplications}
\noindent\textbf{Example XR Applications.}
We evaluate the effectiveness of \sys\ with 2 AR and 1 VR applications as shown in Figure~\ref{fig:examplefig}.
All the applications generate rendered frames of 1080p and provide Full HD (FHD) experiences.
The AR use cases have the same pipeline structure in Figure~\ref{fig:ar_local} taking 1080p camera frames, but with different workload characteristics.
The camera frames are branched to the object detector and renderer because the camera frame needs to be rendered as a background.
The renderer receives the background frame in a blocking manner.
The connection for the object pose detector is non-blocking as an object might not be detected.

For the first AR case (AR1) in Figure~\ref{fig:ar1}, the object detection is done by the local feature matching processes: ORB feature extraction~\cite{rublee2011orb}, k-nearest neighbor (KNN) descriptor matcher, homography and transformation estimations via a perspective-n-point (PnP) random sample consensus (RANSAC) solver~\cite{fischler1981random}.
The second AR case (AR2) in Figure~\ref{fig:ar2} uses the ArUco algorithm~\cite{garrido2014automatic} for detecting the fiducial markers.
While AR2 has a less complex perception than AR1, the application and rendering are more intensive in AR2 as the application is implemented as a separate process by UE5 of the physics and shaders.
On the other hand, AR1 rendering is a pipeline kernel and uses only low-level 3D graphics APIs.

For the VR use case in Figure~\ref{fig:vr}, the pose estimator gets 480p camera frames and inertial measurement unit (IMU) data and generates the current user pose as shown in Figure~\ref{fig:vr_local}.
The pose estimator of the monocular-inertial SLAM has the primary input of IMU and the camera input is optional.
The renderer shows the 3D scene captured from the estimated user pose.
For all example use cases, the user can interact with the virtual objects via keyboard inputs, and this interaction is done by non-blocking receives because the key event happens arbitrarily by the user.

\noindent\textbf{Distribution Scenarios.}
We set up four distribution scenarios for the three use cases: \texttt{Local} (\textbf{L}), \texttt{Perception} (\textbf{P}), \texttt{Rendering+App} (\textbf{R}), and \texttt{Full Offloading} (\textbf{P+R}).
The canonical XR applications consist of perception and graphics rendering functionalities.
As summarized in Table~\ref{tab:previoustech}, the existing distributed XR systems can be categorized into one of our scenarios by their offloading supportability.
With our scenarios, we show the flexibility benefit of \sys\ compared to the existing systems.

\begin{table}[]
  \caption{\label{tab:scenarios} \small The supportability of the
    existing frameworks and \sys\ to our distribution scenarios in
    Figure~\ref{fig:arconfigs} and~\ref{fig:vrconfigs}.}
  \resizebox{\linewidth}{!}{
    \begin{tabular}{|l|c|c|c|c|}
      \hhline{=====}
                                                           & \texttt{Local} & \texttt{Perceptions}   & \texttt{Rendering+App}   & \texttt{Full Offloading} \\ \hline
      Marvel~\cite{chen2018marvel}                         & \ding{55}   & \blue{\ding{51}}     & \ding{55}       & \ding{55}      \\
      OpenRiST~\cite{george2020openrtist}                  & \ding{55}   & \ding{55}     & \ding{55}       & \blue{\ding{51}}      \\
      Furion~\cite{lai2019furion}                          & \ding{55}   & \ding{55}     & \blue{\ding{51}}       & \ding{55}      \\
      Liu \emph{et al.}~\cite{liu2019edge}                 & \ding{55}   & \blue{\ding{51}}     & \ding{55}       & \ding{55}      \\
      Liu \emph{et al.}~\cite{liu2018cutting}              & \ding{55}   & \ding{55}     & \blue{\ding{51}}       & \ding{55}      \\
      Schneider \emph{et al.}~\cite{schneider2017augmented}& \ding{55}   & \ding{55}     & \ding{55}       & \blue{\ding{51}}      \\
      Zhang \emph{et al.}~\cite{zhang2019rendering}        & \ding{55}   & \ding{55}     & \blue{\ding{51}}       & \ding{55}      \\
      \textbf{\sys}                                        & \blue{\ding{51}}   & \blue{\ding{51}}     & \blue{\ding{51}}       & \blue{\ding{51}}      \\ \hhline{=====}
    \end{tabular}
  }
\end{table}



In \texttt{Local} (\textbf{L}), all functionalities run local on the client device only.
In \texttt{Perception} (\textbf{P}), only the perception kernels are offloaded to the server, and in \texttt{Rendering+App} (\textbf{R}) the application rendering are offloaded.
In \texttt{Full Offloading} (\textbf{P+R}), the client only sends the sensor data and receives the final rendered frame.
Figures~\ref{fig:arconfigs} and~\ref{fig:vrconfigs} show the
configurations for the AR1/2 and VR scenarios.
Compared to  existing work which targets specific distributed XR configurations, \sys\ can enable all distribution scenarios flexibly, as shown in Table~\ref{tab:scenarios}.

For the distributed configurations of AR1/2 and VR, the camera and rendered frames and IMUs are transferred with RTP over UDP for application responsiveness while the user input from the keyboard is with TCP for reliable delivery.
When the sensor data is moved via local connections, its queue size is set as 1 to minimize the queuing delay for the data recency.


% Figure environment removed




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Design Benefit of \sys}
As shown in Table~\ref{tab:localcommunication}, GStreamer and RaftLib provide efficient local communication.
Instead of using \sys, a distributed pipeline can be supported by implementing auxiliary kernels for remote communications, branching, and synchronization.
However, supporting a distributed pipeline with the auxiliary kernels introduces inefficiencies because each kernel, including the auxiliary ones, is a separate execution unit that is parallelly scheduled and managed.

Figure~\ref{fig:auxcost} shows the overheads of the auxiliary kernels on our testbed.
We create a kernel sending output to multiple remote kernels, and measure the energy consumption for the transmissions.
GStreamer and RaftLib need the additional kernels for remote messaging and output branching; for sending output to 8 remote kernels, 9 auxiliary kernels are required (1 for branching and 8 for remote messaging).
The energy consumption with the auxiliary kernels increases with their number.
In contrast, with the \sys\ kernel design and interface, the output port registered by a developer can be branched and configured for remote connections with different protocols as specified by a user, not requiring additional kernels.
In \sys, the developers also don't need to implement these auxiliary kernels to make their kernels operate flexibly.

\begin{table}[b]
  \caption{\label{tab:reqkernels} \small The number of kernels
    required to support the distributed configurations of AR1 in
    Figure~\ref{fig:arconfigs}.}
  \resizebox{\linewidth}{!}{
    \begin{tabular}{|l|c|c|c|c|}
      \hhline{=====}
                                                           & \texttt{Local} & \texttt{Perceptions}   & \texttt{Rendering+App}   & \texttt{Full Offloading} \\ \hline
      GStreamer~\cite{gstreamer}                           & 7              & 13                     & 19                       & 17                       \\
      RaftLib~\cite{beard2017raftlib}                      & 6              & 12                     & 18                       & 16                       \\
      \textbf{\sys}                                        & 5              & 7                      & 9                        & 9                        \\ \hhline{=====}
    \end{tabular}
  }
\end{table}

Table~\ref{tab:reqkernels} shows the number of kernels for GStreamer,
RaftLib, and \sys\ to support AR1 in %our distribution
the different scenarios.
GStreamer and RaftLib need auxiliary kernels as local SP libraries.
GStreamer, as a multimedia framework, imposes strict synchronization across streams based on their internal timestamps and requires all kernels to have a single synchronized stream for input and output~\cite{taymans2013gstreamer}.
Since RaftLib does not require such strict %stream
synchronization, it requires fewer kernels.
GStreamer requires two kernels for branching the camera stream and synchronizing streams for the renderer, while RaftLib only needs a branching kernel.
For the distributed settings, both need additional messaging kernels: 4 for \texttt{Perceptions}, 8 for \texttt{Ren\-der\-ing+App}, and 6 for \texttt{Full Offloading}.
In \sys, these auxiliary kernels are not required because each port can be configured for different usage, which enables the various scenarios flexibly without the system overheads from the auxiliary kernels.

% Figure environment removed

% Figure environment removed

% Figure environment removed

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Evaluation of Example Applications}
\label{sec:evalresanaly}
We run the example applications on our testbed of Jetson 15W and 30W with the four distribution scenarios to emulate the situations where the client has little or moderate device capacity.
We measure the average pipeline latency and throughput of the three examples with the scenarios, and demonstrate that the flexibility enabled by \sys\ makes it possible to achieve effective server offloading.

\noindent\textbf{Costs for XR Pipeline Distribution.}
Distributing XR pipelines incurs additional costs: multimedia data compression, displaying the rendered scene from a server, and network transmission.
Transmitting the large multimedia data without compression causes high bandwidth usage with backend network delays and consumes the battery of the user device~\cite{xiao2013modeling, vergara2013energybox}.
Therefore, data (de)compression is necessary for both the client and server.
Another cost is for the client to display the rendered scene from the server.
When the scene is rendered on the server, it should be fetched from GPU memory, sent to the client, and displayed.

The results in Figures~\ref{fig:ar1_res}-\ref{fig:vr_res}, show a breakdown of the average end-to-end latency and average throughput.
The display latency on the client is shown separately from the
rendering latency.
The compression cost is split as encoding and decoding latencies, which include the server- and client-side compression latencies.
The network transmission latency is measured on the client when it receives the result of the timestamped message from the server.



\noindent\textbf{Pipeline Latency and Throughput.}
Figures~\ref{fig:ar1_res}-\ref{fig:vr_res} show the latency and
throughput results of AR1/2 and VR in the distribution scenarios
(\textbf{L}, \textbf{P}, \textbf{R}, and \textbf{P+R}).
Latency is measured as how long the pipeline takes to reflect the real-world
context, and throughput is how frequently the real-world context is reflected to the rendered scene per second.

For AR1 (Figure~\ref{fig:ar1_res}), \textbf{P} shows the lowest latencies in 15W and 30W.
For throughputs, \textbf{P} has the highest throughput in 15W while \textbf{P+R} does in 30W.
Since the pipeline throughput is bound by the dominant functionality, it is possible to have lower throughput even with lower latency.
Compared to \textbf{L}, the throughputs can be improved 2.1\mul\ (15W)
and 1.7\mul\ (30W), and the latencies reduced by 28\% (15W) and 14\% (30W).

For \textbf{P}, the perception on the server takes 11 ms; it takes 121 ms (15W) and 70 ms (30W) of \textbf{L} on the client.
Rendering on the client takes 54 ms (15W) and 19 ms (30W).
\textbf{P} requires the client to encode 1080p camera frames, which takes 57 ms (15W) and 47 ms (30W).
Decoding on the server takes 1.8 ms.
The throughputs of \textbf{P} in 15W and 30W are bound by the encoding latency.

For \textbf{P+R}, since all rendering and perception run on the server, requires  client-side displaying, encoding, and decoding.
On Jet15W, it takes 59 ms for displaying the received FHD scene from the server, 57 ms for encoding camera frames, 12 ms for decoding the received scene.
On Jet30W, it takes 20 ms for displaying, 40 ms for encoding, 6 ms for decoding.
On the server, it takes 14 ms for perception, 5 ms for rendering, 1.7 ms for decoding the received camera frame, and 5 ms for encoding the rendered scene.
So, the throughput of \textbf{P+R} (15W) is bound by client-side displaying while the throughput of \textbf{P+R} (30W) is bound by client encoding.



For AR2 (Figure~\ref{fig:ar2_res} ), \textbf{P+R} shows the highest throughputs while \textbf{P} (15W) and \textbf{L} (30W) present the lowest latencies.
The throughput of \textbf{P+R} is 1.5\mul\ of \textbf{P} (15W) and
1.3\mul\ of \textbf{L} (30W), but it has increase in latencies of 15\% (15W) and 8\% (30W).

For \textbf{P} (15W), the perception takes 7 ms on the server while it does 122 ms of \textbf{L} (15W), and the rendering takes 81 ms.
In addition, it takes 52 ms for client encoding and 1.8 ms for server decoding.
For \textbf{L} (30W), it takes 51 ms for perception and 47 ms for rendering.

For \textbf{P+R}, on Jet15W, it takes 54 ms for encoding the camera, 13 ms for decoding the rendered scene, and 57 ms displaying the received scenes while taking 14 ms for perception and 20 ms rendering on the server.
On Jet30W, it takes 40 ms for encoding, 7 ms for decoding, and 18 ms displaying.
While the throughputs of \textbf{P} (15W) and \textbf{L} (30W) are bound by rendering and perception each, the throughputs of \textbf{P+R} in 15W and 30W are bound by the client encoding.


In Figure~\ref{fig:vr_res} of VR, \textbf{P+R} (15W) and \textbf{P} (30W) present the lowest latencies.
\textbf{P+R} shows the highest throughputs in 15W and 30W.
Compared to \textbf{L}, the throughputs can be 3.9\mul\ (15W) and 2.7\mul\ (30W), and the latencies are 50\% less (15W) and 29\% less (30W).

For \textbf{P}, on Jet30W, it takes 54 ms for rendering, 24 ms for encoding 480p camera frames.
On the server, it takes 33 ms for perception and 1.5 ms for decoding the received camera frames.
On Jet15W, it takes 150 ms for rendering and 33 ms for encoding.
The throughputs of \textbf{P} in 15W and 30W are bound by the rendering.
Since the rendering is so challenging for Jet15W, it dominates throughput and latency.

For \textbf{P+R}, on Jet15W, it takes 57 ms for displaying the scene from the server, 31 ms for encoding camera frames, 15 ms for decoding the received scene.
On Jet30W, it takes 18 ms for displaying, 20 ms for encoding, 7 ms for decoding.
On the server, it takes 36 ms for perception, 31 ms for rendering, 1.7 ms for decoding the received camera frame, and 5 ms for encoding the rendered scene.

For Jet15W, rendering and perception are challenging, and \textbf{P+R} is beneficial in terms of latency and throughput.
In the case of Jet30W, even though \textbf{P+R} introduces additional overheads for the client-side decoding and displaying, the pipeline bottleneck of rendering is relieved compared to \textbf{P} (30W), enabling higher throughput.

\noindent\textbf{Result Analysis.}
In the results in Figure~\ref{fig:ar1_res}, the optimal distribution scenario can vary in terms of the latency and throughput even with the same workloads by the client capacity: \textbf{P} and \textbf{P+R} in 15W and 30W.
For higher throughput, it is crucial to offload the pipeline bottleneck.
There are cases where the distribution overheads are larger than the benefits, ending up with worse performance than the local-only scenario (\eg, \textbf{R} (15W) and (30W) of AR1 and AR2 in Figure~\ref{fig:ar1_res} and~\ref{fig:ar2_res}).
The server and client device capacities should be considered because the kernels are parallelized and can cause resource contention.
For instance, the perception latencies of \textbf{P+R} in AR1, AR2, and VR increase on the server compared to \textbf{P}.
Moreover, although AR1 and AR2 are with the same pipeline structure of
Figure~\ref{fig:arconfigs}, the ideal distribution is different based on the perception and rendering complexities of each application.

Based on our results, the effectiveness of offloading depends on the given workloads, server and client capacities, and offloading overheads.
\sys\ allows each user to configure the workload distribution flexibly at runtime and enables the optimal distribution of an XR application for various distribution scenarios.
