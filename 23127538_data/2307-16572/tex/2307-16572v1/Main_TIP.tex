\documentclass[lettersize,journal]{IEEEtran}
% \usepackage{amsmath,amsfonts}
%\usepackage{algorithmic}
\usepackage{algorithmicx}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021

% extra packages
\usepackage{booktabs}
% \usepackage{amssymb}
\usepackage{amssymb}
\usepackage{amsmath,amsfonts,bm}
\usepackage{multirow}
\usepackage{csquotes}
\usepackage{paralist}
\usepackage{threeparttable}

\usepackage[normalem]{ulem}

\newcommand{\figref}[1]{Fig.~\ref{#1}}
\newcommand{\equref}[1]{Eq.~\eqref{#1}}
\newcommand{\secref}[1]{Sec.~\ref{#1}}
\newcommand{\tabref}[1]{Table~\ref{#1}}

\def\Jing#1{{\color{magenta}{\bf [Jing:} {\it{#1}}{\bf ]}}}
\def\Mengqi#1{{\color{cyan}{\bf [Mengqi:} {\it{#1}}{\bf ]}}}
\def\Zhaoyuan#1{{\color{blue}{\bf [Zhaoyuan:} {\it{#1}}{\bf ]}}}
\def\NB#1{{\color{green}{\bf [NB:} {\it{#1}}{\bf ]}}}

% \def\ie{\emph{i.e.}}
% \def\eg{\emph{e.g.}}
% \def\wrt{\emph{w.r.t.}}
\newcommand\eg{\emph{e.g.}} \newcommand\Eg{\emph{E.g.}}
\newcommand\ie{\emph{i.e.}} \newcommand\Ie{\emph{I.e.}}
\newcommand\cf{\emph{c.f.}} \newcommand\Cf{\emph{C.f.}}
\newcommand\etc{\emph{etc.}}
\newcommand\wrt{w.r.t.} \newcommand\dof{d.o.f.}
\newcommand\etal{\emph{et al.}}
\newcommand\ien{\emph{i.e.}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

% \newcommand{\secref}[1]{Sec. \ref{#1}}
\usepackage{algpseudocode}
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}
\newcommand{\red}[1]{{\textcolor{red}{#1}}} % ranking the first
\newcommand{\blue}[1]{{\textcolor{blue}{#1}}} % ranking the second  


%\def\Jing#1{{\color{magenta}{\bf [Jing:} {\it{#1}}{\bf ]}}}
\newcommand{\rev}[1]{{\textcolor{blue}{#1}}}

\begin{document}

\title{Transferable Attack for Semantic Segmentation}

% \author{
% Yi Zhang,
% % *\thanks{*Equal Contribution.}, 
% Jing Zhang\thanks{Jing Zhang is with Australian National University, Australia.}, Wassim Hamidouche and Olivier Deforges\thanks{Yi Zhang, Wassim Hamidouche and Olivier Deforges are with Univ Rennes, INSA Rennes, CNRS, IETR (UMR 6164), France.}
% %IEEE Publication Technology,~\IEEEmembership{Staff,~IEEE,}
%         % <-this % stops a space
% %\thanks{This paper was produced by the IEEE Publication Technology Group. They are in Piscataway, NJ.}% <-this % stops a space
% %\thanks{Manuscript received April 19, 2021; revised August 16, 2021.}
% }

\author{Mengqi He,~
Jing Zhang,~
Zhaoyuan Yang,~
Mingyi He,~
Nick Barnes,~
Yuchao Dai
\\
\IEEEcompsocitemizethanks{\IEEEcompsocthanksitem Mengqi He, Jing Zhang and Nick Barnes are with School of Computing, Australian National University, Canberra, Australia. (mengqi.he@anu.edu.au, zjnwpu@gmail.com, nick.barnes@anu.edu.au)
\IEEEcompsocthanksitem Zhaoyuan Yang is with GE, America. (zhaoyuan.yang@ge.com)
\IEEEcompsocthanksitem Mingyi He and Yuchao Dai are with School of Electronics and Information, Northwestern Polytechnical University, Xi'an, China. (myhe@nwpu.edu.cn, daiyuchao@gmail.com)

% \IEEEcompsocthanksitem The source code and experimental results are publicly available via our project page: \url{xx}.
}
}

\newcommand{\toreviewer}[1]{\vspace{0.1em}\noindent \textcolor{blue}{\textbf{#1 \hspace{0.1em}}}}

% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2021}%
{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

%\IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

\maketitle

\begin{abstract}
% Semantic segmentation models are data-hungry. 
Semantic segmentation models are known vulnerable to small input perturbations.
% to adversarial attack
% The basic assumption of existing semantic segmentation models is that the training dataset is large enough to provide a reliable model to fit a probability distribution over the whole joint data space. However, as we cannot guarantee the completeness of the training dataset, models based on it can be sensitive to minor perturbations as the classifiers may be biased, making it necessary to investigate the robustness of semantic segmentation models \wrt~input perturbations. 
In this paper, we comprehensively analysis the performance of semantic segmentation models \wrt~adversarial attacks.
% We have conducted experiments on widely-studied semantic segmentation datasets, namely Pascal VOC 2012 and Cityscape datasets, and observed that the existing
% % widely-studied 
% semantic segmentation models are vulnerable to adversarial attacks. 
and observe that the adversarial examples generated from a source model fail to attack the target models, \ie~the conventional attack methods, such as PGD~\cite{PGD_attack} and FGSM~\cite{Explaining_and_Harnessing_Adversarial_Examples}, do not transfer well to target models, making it necessary to study the transferable attacks,
% make \Zhaoyuan{We should say simple attack method such as PGD and FGSM does not transfer well to target model make it necessary to study the transferable attack such as TI/DI/NI/etc, otherwise people maybe confuse}, explaining the necessity of investigating the 
especially transferable attacks for semantic segmentation. 
We find that to achieve transferable attack, the attack should come with effective data augmentation and translation-invariant features to deal with unseen models, and stabilized optimization strategies to find the optimal attack direction.
% and critical region identification solution to localize the most vulnerable data.
% \Mengqi{invarint}
Based on the above observations, we propose
% We also test 
an ensemble attack for semantic segmentation by aggregating several transferable attacks from classification to achieve more effective attacks with higher transferability.
% In this paper, for the first time, we comprehensively study adversarial attacks, especially transferable attacks for semantic segmentation, to evaluate model robustness to invisible noise across different network structures.
% Based on our experiments, we find that those conventional simple attacks work well
% the objective to directly make all predictions go wrong works well \Zhaoyuan{simple attacks work well} 
% for the source model, while they transfer poorly to the target models. We also observe network structure is a critical issue \Zhaoyuan{for what?} instead of network capacity. Finally, the dataset is also a core issue for transferability, making distribution analysis a promising direction. \Zhaoyuan{
% Taking a step further, we
% % Furthermore, we 
% discover that existing vision foundation model such as Segment Anything~\cite{kirillov2023segany} is also vulnerable to transferable adversarial attacks, explaining the importance of investigating transferable attacks for segmentation models.
% }
The source code and experimental results are publicly available via our project page: \url{https://github.com/anucvers/TASS}

% \NB{This abstract is a bit confusing. A comprehensive analysis of semantic segmentation wrt adv attach - sure, but then ensemble attack is new? Why is foundation models a step further - that's just analysis right? Need to think more about exactly what the contrib is.}
% \Mengqi{A step further test and analysis on SAM?}
% \Zhaoyuan{There are several works in literature claim vulnerability of foundation model such as SAM, we should focus more on transferability of attack on SAM}
% \Mengqi{Sure}
\end{abstract}

\begin{IEEEkeywords}
Semantic segmentation, Transferable attacks.
\end{IEEEkeywords}


% \section{Reviews}

% \subsection{Review 1}

% Issues that affect my rating:
% - My most important concern is that I cannot pinpoint the exact (significant) contributions of this paper, nor does the paper explicitly state them. Is the contribution (A) a study on the transferability of attacks? If so, then the paper would have to be much more comprehensive and thorough in its explanations and experiments, and provide something like a toolbox that allows standardization for measuring this robustness. On the other hand, is the paper (B) contributing a particular attack (i.e. maybe a modification of a classification-based attack to a semantic segmentation-based attack)? If so, the paper is really not clear about this: for instance, there is essentially not a methodological section. Regardless of whether its option (A) or (B), the paper is not clear about it; and, even if it was, the contribution to either (A) or (B) does not seem sufficient or novel enough.

% - The paper has some important missing related works [A, B, C]. In particular, I think [B] has essentially already done a significant part of what was presented in this paper as contriution: extending classification-based attacks (the entire AutoAttack suite [C]) to segmentation-based attacks.

% - The paper has parts that are in relatively poor shape w.r.t. writing. (I detail many of the issues I found in the "Additional comments" section)

% - L627 states an observation regarding residual-connection models. Is the difference between DeepLab-like models and FCN-like models really only the residual connection, such that we can make that statement? I am not completely knowledgeable in that area, but I am pretty sure that is not the only difference, so that attribution cannot be made so easily. Furthermore, I do not really understand the sentence stating this fact in L854.

% - The introduction does not really read like a introduction. Furthermore, what is written there (for instance the first paragraph) and Fig. 1, does not really look like making reference to the paper's contributions: the fact that imperceptible noises fool networks is well known, together with the fact that transferability happens, but that there are many caveats to it happening.


% [A] Houdini: Fooling Deep Structured Prediction Models, Cisse et al., NeurIPS-17
% [B] Towards Robust General Medical Image Segmentation, Daza et al., MICCAI
% [C] Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks, Croce and Hein, ICML-20

% Issues that do not affect my rating, but should be addressed:
% - For DAGs perturbations steps (L572), how was the step size handled? The iterations hyper-parameter was (most likely) tuned by the original authors in conjunction with the step size.
% - Please clarify this for me: in Fig. 4, the titles on each plot correspond to the source model, correct? And the distribution we are visualizing (via the boxplots) is the individual adversarial images that were generated, correct? The answer to these questions is not really clear from merely reading the paper.
% - Could the authors please elaborate on the need for Fig. 5? I think reporting SSIM without reporting hte attack success rate is rather uninformative. A method running the idenitty transform would get 0 success rate, yet would score perfect here. This fact is further confirmed by the text itself: L792 states DAG as the "winner" (w.r.t. SSIM), when the previous section showed DAG's performance is among the lowest.
% - Are the data reported in Fig. 6 correct? My reading of the bottom row (related to attack success) would be as follows: as iterations increase, DAG is (dramatically) better than SegPGD when measured via 1-mIoU; however, if measured via success rate, then SegPGD is (dramatically) better than DAG. Is my reading correct? Or am I missing something? If my reading is correct, then, how is this fact not incoherent?

% I think the paper's intent is considering an important and unexplored gap in the literature, regarding the transferability of adversarial attacks in semantic segmentation. The topic itself can seem a bit niche, but, in my view, it is something worthy of study. Having said that, I think the paper's main contributions are rather unclear and, even if they were, I suspect insufficient for a paper. Is the paper proposing a study? Or is the paper proposing a new method? I am not completely sure after reading the paper a few times. In the case the paper is a study, then I do not see some noteworthy observations being extracted from the results.

% Other issues:
% - The paper has several issues with missing articles (a, an the, etc.), and plural/singular inconsistencies. This issue suggests a strong effort in re-writing is needed
% - L091: I would say that the generation of examples can be *cast* as an optimization problem, not that it intrinsically *is* such problem
% - Fig. 2 should be shorter
% - Fig. 2's caption is not self-contained

% Writing issues:
% - L023: on the ... datasets
% - L028: the transferable attack
% - L033: rephrase this sentence
% - L35: we also network
% - L162: intrinsic attack as direction for
% - Eq. (2): use space after \eps
% - L123: clips
% - The notation inL124 looks weird: the range x = ... specified by ...?
% - L191: that study
% - L202: boarder
% - L214: discussed previous
% - Eq. (4): \mathcal L_i should be \mathcal L_j
% - L353 looks weird
% - L421: ,and
% - L481: noisy degree
% - L484: $S_r$
% - L569: visually invisible
% - L582: thje
% - L696: successful
% - L859: due to implementation gap

% \subsection{Review 2}

% - The paper performs a thorough evaluation of multiple attacks, but the results and their analysis do not provide insights that could help to better understand adversarial attacks in this task.
% - Limited technical novelty: all the attacks used for the evaluation were proposed in previous works. The extension of the transferable attacks to segmentation seems to be a direct application of their classification couterparts.
% - Unjustified claims:
% - The paper claims that it is the first to perform a comprehensive study of adversarial attacks, specially transferable attacks for semantic segmentation. However, several studies have evaluated the transferability of attacks in semantic segmentation [41,42,a] and the effectiveness universal attacks in that task [19,13], which are by definition transferable attacks.
% - When discussing the transferability of DI (L742-747), it is said that on cityscapes is low but in PascalVOC it is reasonable. Where are those PascalVOC results? The ones presented in table 1 show that in that dataset the transferability is not very good and there is no figure showing the range of the success rate in that dataset (like fig 4 did for cityscapes).
% - In the conclusion, L857 says "We find dataset is also an important issue" but there is no analysis of that in the experiments besides the one named in the previous bullet point. Furthermore, the attack that focusses on the dataset distribution (IAA) is only performed in PascalVOC; thus, there are not results about how the dataset might affect its transferability. Finally, there are no experiments on how the attacks generated with models trained on one dataset affect models trained in different data.
% - Metrics: the PSNR and SSIM are reported to indicate if an adversarial attack is still invisible, but without a reference of the expected values when the attacks become visible, it is not clear what those metrics mean in tables 1 and 2. Also, considering that the objective is that the attacks are imperceptible and that is adjusted with the magnitude of the attack, it would be expected that the epsilon selected will ensure that they will always remain invisible.

% [a] Zhenhua Chen, Chuhua Wang, and David Crandall. "Semantically Stealthy Adversarial Attacks against Segmentation Models." In WACV. 2022.

% The paper presents a thorough experimentation of adversarial attacks, but since all the techniques used already existed, it was expected that the paper provided an analysis of how the transferability is affected and maybe how it could be improved. However, the paper provides no insights in the matter. Also, there are some unjustified claims and unclear experimental choices (see weaknesses).


% - Incorrect notation: equations 4 and 5 include the summation of the loss `L` for the ith pixel (`L_i`) but it iterates over `j \in P^T`. Equation 7 does a similar thing by calculating `L_k` but iterating over `j \in P^F`. Therefore, either the iterator or the subindex of the loss are mixed.
% - In L580 says "we use two shared networks" but it is not specified which networks are used.
% - The name of the dataset Cityscapes is written without the final s throughout the text.
% - It would be better to separate the introduction and the related work sections.
% - Why was the IAA attack only evaluated on deeplab with PascalVOC? Why not in cityscapes or with FCN?
% - L85 says that the difference between the clean and adversarial images is "not invisible" but it should be "not visible".
% - L392 has some words with strike-through.

% \subsection{Review 3}

% W1. The novelty of this paper is limited. Even though comprehensive results regarding several existing popular transferability attacks are compared, the reason behind the experimental results is not explained. For example, why NI [24] performs the best is not thoroughly discussed.

% W2. The selection of transferability attacks is not motivated. For example, ensemble approaches are always used in transferable adversarial example research [11], but this paper only discusses single appraoch. Additionally, state-of-the-art approaches, e,g, [a], [b], and [c], are not discussed, making the contribution of the systematic study less strong.

% [a] Leveraging robust features for targeted transfer attacks. NeurIPS 2021.
% [b] Learning transferable adversarial perturbations. NeurIPS 2021.
% [c] On generating transferable targeted perturbations. ICCV 2021.

% The novelty and the systematization of this paper are both limited.

% Typo at line 392

\section{Introduction}
\IEEEPARstart{S}{emantic} segmentation~\cite{chen2022vision,yuan2019segmentation,yan2022lawin,mohan2021efficientps,cheng2020panoptic,zhang2021dcnas,li2019global,chen2017rethinking,chen2018encoder,long2015fully,lin2017refinenet,chen2016attention,acuna2019devil,girshick2014rich,chen2017deeplab,deng2019restricted,goodfellow2020generative,kingma2013auto,li2021semantic,souly2017semi,luc2016semantic,hung2018adversarial,xue2018segan,zhaoa2021semantic}
% \Jing{citations of sota semantic segmentation models} 
aims to segment images into different semantically meaningful regions.  Although performance is highly effective,
% \NB{performance is highly effective}\sout{significant performance improvement has been achieved}, 
we find the conventional semantic segmentation models, \ie~Deeplabv3~\cite{chen2018encoder} in Fig.~\ref{fig:segmentation_robustness_wrt_attack},
% \NB{which model? all models or a specific one? Sure below the example is deeplab but is the the point you are making?}\Mengqi{FCN,Deeplab,and also the SAM we mention in the very end?} 
is sensitive to input noise~\cite{xie2017adversarial,arnab2018robustness}, where minor and invisible perturbations can cause a
% \NB{a} 
significant decrease in
% \NB{decrease in} 
performance
% \sout{damage} 
(see the second column of Fig.~\ref{fig:segmentation_robustness_wrt_attack}, where the noise image, \ie~adversarial example~\cite{szegedy2013intriguing},
% \Jing{citations}, 
is generated by PGD~\cite{PGD_attack}, an adversarial attack technique, aiming to attack the source model, namely Deeplabv3+~\cite{chen2018encoder}). To achieve robustness to input perturbation,
% deal with this, 
robust semantic segmentation models have been explored~\cite{Xiao_2018_ECCV, Xu_2021_ICCV,kapoor2021fourierdomain,9506748,tran2021robustness,9207291,9413772},
% \Jing{explain what is robust semantic segmentation, and the objective of those robust semantic segmentation models, especially the relationships to adversarial attack based investigation, e.g. segpgd}
% \Mengqi{might explain their work with more details}
where the objective
% of robust segmentation 
is to provide stable performance with respect to different types of data deterioration~\cite{Kamann_2020_CVPR}.
% under the worse case \Jing{what you mean the worse case}scenario~\cite{Kamann_2020_CVPR}, which includes face to the adversarial attack, blur, geometric distortion, etc. 
Although those techniques can provide relatively robust models, they still show limitations to adversarial attacks (see the third column of Fig.~\ref{fig:segmentation_robustness_wrt_attack} where SAT\cite{xu2021dynamic} is a robust segmentation model).

% where the input perturbation is more invisible compared with other types of data perturbation.
% It can be formulated as a pixel-wise classification task,
% % a problem that classifies the pixels of the image into different categories.
% which is widely used in autonomous vehicles~\cite{siam2017deep}, medical image analysis~\cite{taghanaki2020deep}, video surveillance~\cite{1587668}, augmented reality~\cite{alhaija2017augmented}, etc. 
% The conventional practice for semantic segmentation mainly focuses on deal with two main problems, the detail of the image is lost during the pooling layers, and the limited context information issue end with the unbalanced use of local and global information.
% Some work designing models with different receptive fields, \ie~CNN~\cite{o2015introduction}, or Transformer~\cite{vaswani2017attention} to extensively explore semantic meaningful object(s) regions

% \NB{Not sure I would rate different receptive regions as the first topic of semantic segmentation - You need this to be a more defensible opening claim, and changing backbone models  - these are several generic topics that don't describe research in semantic segmentation in an interesting way}. Or, instead of changing the backbone models, some focus on designing effective context modeling modules, \ie~attention modules~\cite{girshick2014rich}, dilated convolution~\cite{chen2017deeplab}, deformable convolution~\cite{deng2019restricted}. Further, considering the stochastic attribute of semantic meaningful region, there also exist solutions of using generative models~\cite{goodfellow2020generative,kingma2013auto} for semantic segmentation~\cite{li2021semantic,souly2017semi,luc2016semantic,hung2018adversarial,xue2018segan,zhaoa2021semantic}.
% \Jing{citations for generative semantic segmentation}
% adopting new structures, 
% \ie~generative methods~\cite{souly2017semi}, based U-Net structures~\cite{ronneberger2015unet}
% % multiscale pyramid network~\cite{tian2021multiscale}, feature pyramid network~\cite{seferbekov2018feature}, 
% \cite{sucar2015probabilistic} probabilistic graphical model
% or \textbf{3)} obtaining effective context modeling,
% % add or change  modules, 
% \ie~attention modules~\cite{girshick2014rich}, dilated convolution~\cite{chen2017deeplab}, deformable convolution~\cite{deng2019restricted}. 
% probabilistic graphical model,such as, 
% conditional random field~\cite{plath2009multi}.
% \Jing{citations}, 
% or loss functions~\cite{lin2017focal}
% % \Jing{citation of focal loss for semantic segmentation} 
% to effectively fit the training dataset. 
% To release the time of annota  ting the pixel-wised label, some of the work is focused on simi-supervised\cite{zhou2019collaborative} or weakly supervised label\cite{wei2016stc}.

% Different from the objective of robust semantic segmentation models to produce relative robust model for various types of data pollution,
% % (\Jing{what's the adjective of those robust segmentation models? explain differences between robust segmentation models and adversarial robust segmentation models})
% % (\Mengqi{adversarial robust semantic segmentation model only need to face to adversarial attack while robust semantic segmentation needs to face to other noises, such as blur and gaussian noise}) 
% adversarial robustness semantic segmentation model~\cite{gu2022segpgd} targets directly on model robustness with respect to adversarial attack, achieving better performance for the noise perturbed samples (see the fourth column of Fig.~\ref{fig:segmentation_robustness_wrt_attack}, where SegPGD~\cite{gu2022segpgd} targets directly on adversarial attack).


% \Mengqi{ might claim the weakness of DAG}

% Studies found that attack can affect autonomous vehicles\cite{bar2020vulnerability} and medical image analysis\cite{daza2021towards}, which can be a real-world potential threat.

% % Figure environment removed


% Figure environment removed






% However, o
Different from the objective of robust semantic segmentation models,
to produce models that are robust relative to
% % \NB{models that are robust relative to} \sout{relative robust model for} 
various types of data pollution,
% (\Jing{what's the adjective of those robust segmentation models? explain differences between robust segmentation models and adversarial robust segmentation models})
% (\Mengqi{adversarial robust semantic segmentation model only need to face to adversarial attack while robust semantic segmentation needs to face to other noises, such as blur and gaussian noise}) 
adversarial robustness semantic segmentation models~\cite{gu2022segpgd} directly target
% \sout{directly on} 
model robustness with respect to adversarial attack, which is usually invisible. In this case, the goal of adversarial robust semantic segmentation is to produce relatively stable performance \wrt~minor image perturbations. Although existing work can lead to reasonable model robustness in this scenario, 
% achieving better performance for the noise perturbed samples (see the fourth column of Fig.~\ref{fig:segmentation_robustness_wrt_attack}, where SegPGD~\cite{gu2022segpgd} targets directly on adversarial attack).
% However, 
we find that one main limitation of the current adversarial attack-based semantic segmentation models is that
% semantic 
the success rate of attacks for segmentation is poor when it transfers to different network structures, indicating poor transferability of the attacks.
% Consider
% % \NB{Consider} 
% two
% % \sout{main} 
% observations from 
In Fig.~\ref{semantic_seg_towards_attack}, we show an adversarial attack from a source model, to test its effectiveness on a target model with a different model structure.
% attack from one : 1) although the difference between the adversarial example and the clean image is not
% % \sout{visually} \NB{not} 
% invisible, the predictions for them are significantly different; 2) 
We find that the adversarial example from the
% \NB{the} 
source model (\enquote{Deeplabv3}) fails to attack the target model (\enquote{Deeplabv3+}), which we claim as one main motivation for investigating of transferable attack, where the generated attack should be used to attack various models. Further,
% This is usually what the real-world scenarios are, generating attacks with one model, and testing model robustness of the others.
% but the apply attack on a different model. 
% Even the change of the backbone of the same model will still affect the performance of the attack. 
% \Zhaoyuan{
% Motivation of transferable attack: (1) 
in black-box scenario, where target model weights are unknown, with transferable attack,
% attacks are transferable, 
we could use a source model with known weights to generate the attack and transfer the attack to the target model with unknown weights. Also, for scenarios where the
% (2) T
target model is too large or computationally expensive to generate attack, if we could generate attack from a smaller model and transfer to larger model, computation cost can be significantly reduced.
% }


% Hence, in this work, 
With the above three main motivations, we work on transferable attacks for semantic segmentation. Firstly,
% We summarize our contributions as following: 1) for the first time, 
we extensively explore conventional image classification based adversarial attacks and transferable attacks, and extend those solutions to semantic segmentation, a dense pixel-wise classification task. Secondly,
% ; 2) 
we observe three main attributes to achieve effective transferable attack, and
% to make the best usage of each attack, we further 
present ensemble attack as an effective technique to achieve better transferable attack. Lastly, we consider robustness of vision foundation models~\cite{kirillov2023segany} \wrt~transferable adversarial attack, further explaining the necessity of our investigation on transferable attack for semantic segmentation.

% evaluate 2 conventional attacks,2 attacks designed for segmentation, and 3 attacks designed for transferable attack for classification's transferability on 
% four different models on two datasets.
% We also evaluate the image quality for all of these attacks.

% \Jing{1) Introduce semantic segmentation, the models, 2) explain its fragility to adversarial attack, 3) what has been achieved to achieve robust semantic segmentation; 4) why we investigate transferable attack}




% Figure environment removed


\section{Related Work}
\noindent\textbf{Semantic segmentation models:}
% In the beginning, 
% \cite{long2015fully}fully convolutional networks(FCN) use a fully convolutional layer to replace the fully connected layer in CNN, use the skip layer to combine the feature map and finally upsample with the bilinear interpolation. With those methods, 
With the pioneer work from~\cite{long2015fully}, semantic segmentation is widely studied, where the conventional techniques focus on convolutional neural network (CNN) structures~\cite{acuna2019devil,Takikawa_2019_ICCV,yuan2020segfix,Liang_2020_CVPR,chen2017deeplab,wang2018understanding,zhao2017pyramid,deng2019restricted,dai2017deformable,liu2015semantic,lin2016efficient,krahenbuhl2011efficient,li2021semantic}.
% Based on it~\cite{long2015fully}, t
% new structures and effective context modelling.
% \Jing{categorize the exiting semantic segmentation models, e.g. structure, cnn, transformer, loss function, module, etc, instead of randomly putting them together.}
% ~\cite{long2015fully} converts the classification method into a segmentation method. 
% However, this method still has two main problems, firstly, after the image is pooled, the feature map's resolution drops and certain pixels' spatial position information is lost. Second, there is an imbalance in the utilization of local and global features as a result of its inability to fully utilize spatial position information.
% \Jing{introduce the backbone of~\cite{long2015fully}, ie vgg, resnet, then analysis the limitation of cnn backbones, which provide support for rnn. or transformer}
% \cite{simonyan2015deep} VGG replaces the large-size convolution kernel with a small convolution kernel, which uses fewer parameters but has the ability to capture more complex local information.
% \cite{he2015deep} Resnet is released also to take care of the deep neural network, it adds a  residual connection in each layer to deal with the gradient vanishing.
% To deal with segmentation, 
% Specifically, based on backbone models for image classification, e.g. VGG~\cite{simonyan2015deep}, \cite{long2015fully} replaced the fully connected layers with convolutional layers, achieving fully convolutional neural network (FCN) for semantic segmentation.
% are also presented to fixed noisy l, or designing new network structures to achieve more effective high-low level feature aggregation. 
% FCN modifies the VGG, use the fully convolutional layer to replace the fully connected layer in CNN, and then uses the skip layer to combine the mid-layer features, and finally uses bilinear interpolation to upsample the segmentation result into the fine-grained pixel wised result. This modification converts a classification network into a segmentation network. However, CNN's reception field is limited and to get a large reception field the network has to be very deep which will result in a balanced usage of local and global information, and also the context information is not fully used.
% The following work can be roughly summarized into three main categories, namely backbone investigation techniques, adopting new structures, and context modeling strategies.
% \noindent\textit{Context modeling:} One limitation of the convolutional operation is the limited receptive field issue, leading to less effective context modeling. To fix it, the mainstream work focus on obtaining better context modeling with new backbones or better context modeling strategies.
Recently, transformer structure~\cite{vaswani2017attention} based segmentation models~\cite{chen2021transunet,strudel2021segmenter,chen2021transunet,ranftl2021vision,strudel2021segmenter,zheng2021rethinking,xie2021segformer} are extensively explored. With the self-attention module, transformer structure~\cite{vaswani2017attention} achieves global context modeling, leading to improved segmentation results. However, we still observe that the existing transformer based semantic segmentation models~\cite{cheng2021perpixel,cheng2022masked} are vulnerable to adversarial attack, making it necessary to analysis model robustness \wrt~adversarial attack for safer model deployment. In Fig.~\ref{fig:segmentation_robustness_wrt_transformer}, we illustrate robustness of a transformer based segmentation model, namely Maskformer~\cite{cheng2021perpixel}, \wrt~adversarial attack, where the attack is generated by PGD~\cite{PGD_attack}. Fig.~\ref{fig:segmentation_robustness_wrt_transformer} shows that although Maskformer~\cite{cheng2021perpixel} can produce accurate segmentation results, it's still not robust to adversarial attack, further strengthening our motivation in studying adversarial attacks, especially transferable attacks for semantic segmentation. 
% \Jing{analyse robustness of transformer based segmentation model with respect to adversarial attack, e.g. find a transformer based segmentation model: dpt (Vision Transformers for Dense Prediction), explain robustness of the dpt with respect to adversarial attack, show example}
% \Mengqi{There is a bug for import ADE20k,still fixing,might use gloucv as an alternative}
% \Mengqi{test gloucv and mxnet, not supported for 11.8 and 12.2, then test on 11.2 and 10.2,ncll didn't works,hence change to maskformer,maskformer works,testing}
% \Mengqi{fixing, to finish this morning}
% \Mengqi{
% Maskedformer result: Tiny-swin as backbone
% mIoU,fwIoU,mACC,pACC 
% 0.0630,0.6387,0.1589,1.6428 Sence Maskedformer
% 0.0228,0.1951,0.0630,0.4565 Attacked
% 46.7488,70.8628,61.2441,81.5720 Segmentation
% 13.6786,40.0338,22.1522,52.2644 Attacked
% sw                        }

% Figure environment removed

% Different from the deterministic segmentation models, ignoring the stochastic attribute of the task, \ie~category of the pixel can be ambiguous given only visual information, the generative segmentation solutions introduce extra latent variable, modeling such ambiguity.
% Among them, \cite{luc2016semantic} introduces the Generative Adversarial Net (GAN), where the discriminator serves as trainable loss function for robust segmentation.
% % the GAN which is based on CNN segmentation network to find a balance between the generator and the discriminator.
% \cite{souly2017semi,hung2018adversarial} adopts
% Conditional GAN (cGAN)~\cite{cgan} for semi/weakly-supervised semantic segmentation.
% % to train an efficient generator network to deal with simi and weakly supervised samples.
% \cite{xue2018segan} extends GAN for
% % designs a with
% % multiscale L1 Loss and 
% % apply it on GAN to do 
% medical image segmentation.
% % \cite{hung2018adversarial} propose a GAN to apply on semi supervised segmentation.
% \cite{li2021semantic} explores GAN with two discriminators to study the
% % use a generator with two descriminator to generate a new image with the segmentation results, which is try to focus on
% out of domain generalization issue.
% \cite{zhaoa2021semantic} focuses more on improving model performance with the stochastic attribute of GAN, and combine it with denseCRF~\cite{krahenbuhl2011efficient}, achieving better performance for semantic segmentation.
% Given the requirement for accurate semantic meaningful object(s) localization, context modeling is especially important for semantic segmentation, which can be achieved by using different backbones or context modeling strategies. 
% One typical solution is the transformer structure~\cite{vaswani2017attention} based segmentation models~\cite{chen2021transunet,strudel2021segmenter,chen2021transunet,ranftl2021vision,strudel2021segmenter,zheng2021rethinking,xie2021segformer,ranftl2021vision}\Jing{more citations for 1) transformer based segmentation; 
% 2) models focus on loss functions\cite{acuna2019devil,Takikawa_2019_ICCV,yuan2020segfix,Liang_2020_CVPR,zhu2023brain}
% % , where vision transformer backbone~\cite{dosovitskiy2020image} is used instead of the CNN backbones.
% % The convolutional operation within CNN is sliding-window based, leading to limited context modeling.
% % % \Jing{then introduce the techniques to achieve effective local/global context modeling}
% % \Jing{remember, when you write stuff, there should be hidden logic, e.g. something is proposed because of what, while it still shows limitations, and can be solved in which way}
% Alternatively, within the CNN backbone, another direction for effective context modeling is achieved via enlarging the receptive field with modified convolutional operation.
% % To deal with this,\cite{chen2014semantic} deep lab method adds a fully connected conditional random field at the end of the FCN to optimize the boundary of the segmentation map and uses atrous convolution to increase the reception field of the FCN. To deal with the pooling layer lose the detail of the feature map,
% Among them, \cite{chen2017deeplab,wang2018understanding,zhao2017pyramid} used dilated convolution,
% % ,\cite{wang2018understanding} use hybrid dilated convolution and dense upsampling convolution,
% \cite{deng2019restricted,dai2017deformable} relied on deformable convolutional layers. Further, some post-processing techniques can be used to further enhance the pair-wise similarity, achieving better context modeling. e.g.
% % , a probabilistic graphical model(PGM) can effectively capture the context information.
% \cite{liu2015semantic} adopted Markov random field(MRF) to obtain the deep parse network (DPN). \cite{lin2016efficient} used conditional random field (CRF) to refine model prediction.
% and give a deep structured model (DSM) to combine path-path context and path-background context to get better prediction.
% to replace traditional convolution kernel. 



% Mention the local and global information, Recurrent Neural Networks(RNN) is another way to deal with this.
% \cite{pinheiro2014recurrent} apply RNN on the segmentation task. \cite{visin2016reseg} use RNN to replace the pooling layer and the convolution layer in CNN.
% RNN are used to extract the get the context information after using the CNN to get the feature map, as a method that can deal with historical info and process info recurrently, can capture context info effectively. 
% \cite{byeon2015scene} use several independent LSTMs to fusion the features from the inputs.
% \cite{chen2021transunet,strudel2021segmenter}use a transformer as the backbone,\cite{strudel2021segmenter} use the vision transformer~\cite{dosovitskiy2020image}as the encoder and then use the masked transformer to the decoder and then use the arg max to classify each pixel.
% Since the transformer has a global reception field due to the self-attention module, it can achieve effective local and global context modeling.
% \cite{chen2021transunet} combine the UNet and the transformer to get better performance.


% \textbf{Adopting new structures:}
% \cite{luc2016semantic} directly apply generative adversarial network in the segmentation, which converts the input into the segmentation model and then uses a discriminator network to optimize the training result.\cite{souly2017semi}use the conditional generative adversarial network (CGAN) to do this work.
% \cite{girshick2014rich}release a method that uses regions with CNN(RCNN), it use the segmentation method to extract candidate areas and then uses CNN to extract the features of each area, at the end, use support vector machine(SVM)to classify the objects in each area.
% \Mengqi{maybe add some weakly supervised method?}

% \noindent\textit{Generative segmentation models:} Different from the deterministic segmentation models, ignoring the stochastic attribute of the task, \ie~category of the pixel can be ambiguous given only visual information, the generative segmentation solutions introduce extra latent variable, modeling such ambiguity.
% Among them, \cite{luc2016semantic} introduces the Generative Adversarial Net (GAN), where the discriminator serves as trainable loss function for robust segmentation.
% % the GAN which is based on CNN segmentation network to find a balance between the generator and the discriminator.
% \cite{souly2017semi,hung2018adversarial} adopts
% Conditional GAN (cGAN)~\cite{cgan} for semi/weakly-supervised semantic segmentation.
% % to train an efficient generator network to deal with simi and weakly supervised samples.
% \cite{xue2018segan} extends GAN for
% % designs a with
% % multiscale L1 Loss and 
% % apply it on GAN to do 
% medical image segmentation.
% % \cite{hung2018adversarial} propose a GAN to apply on semi supervised segmentation.
% \cite{li2021semantic} explores GAN with two discriminators to study the
% % use a generator with two descriminator to generate a new image with the segmentation results, which is try to focus on
% out of domain generalization issue.
% \cite{zhaoa2021semantic} focuses more on improving model performance with the stochastic attribute of GAN, and combine it with denseCRF~\cite{krahenbuhl2011efficient}, achieving better performance for semantic segmentation.
% proposed a GAN with cascaded  convolutional CRF to achieved a better performance than Deeplabv3.

% \Jing{discuss generative segmentation models, and their robustness to noise perturbation}

% \noindent\textit{Designing new structures:} Further, due to the difficulty of pixel-wise labeling, some other solutions focus on designing new network structure to produce models that are robust to labeling noise~\cite{acuna2019devil}.
% Generative models can also be used to utilize the context information while keeping the consistency of the spatial information.\cite{DBLP:journals/corr/LucCCV16} use the GAN which is based on CNN segmentation network to find a balance between the generator and the discriminator \cite{souly2017semi} use conditional GAN to train an efficient generator network to deal with simi and weakly supervised samples. Another way is to use the encoder-decoder methods to deal with this,\cite{ronneberger2015unet} UNet uses downsample and upsample to encode and decode. \cite{peng2017large} use Resnet\cite{he2016deep} to encode and use a global convolution network to decode. This type of method is use down-pooling and convolution to encode the image features and relative location information and then uses the decoder, which usually includes the up-pooling and deconvolution, to restore the image features and relative location info.
% Another method to utilize the global and local features is feature fusion methods.\cite{chen2016attention} use the multiscale feature in FCN and combine this with the attention module to fusion the features.\cite{lin2017refinenet} Refinenet use different scale outputs after CNN and fusion them use refine module. \cite{zhao2017pyramid} PSPNet uses four different convolutional kernels and them fusion them using spatial pyramid pooling layers.
% Using the CRF as the post-processing is time and storage-consuming.


% \noindent\textit{Context modelling  strategy:} 

% \Jing{explain the context modeling abilities of cnn, rnn and transformer}
% The convolutional operation within CNN is sliding-window based, leading to limited context modeling.

% % \Jing{then introduce the techniques to achieve effective local/global context modeling}
% \Jing{remember, when you write stuff, there should be hidden logic, e.g. something is proposed because of what, while it still shows limitations, and can be solved in which way}
% To deal with this,\cite{chen2014semantic} deep lab method adds a fully connected conditional random field at the end of the FCN to optimize the boundary of the segmentation map and uses atrous convolution to increase the reception field of the FCN. To deal with the pooling layer lose the detail of the feature map,\cite{chen2017deeplab} use dilated convolution,\cite{wang2018understanding} use hybrid dilated convolution and dense upsampling convolution,\cite{deng2019restricted,dai2017deformable} use deformable convolution to replace traditional convolution kernel. 

% Another method to utilize the global and local features is feature fusion methods.\cite{chen2016attention} use the multiscale feature in FCN and combine this with the attention module to fusion the features.\cite{lin2017refinenet} Refinenet use different scale outputs after CNN and fusion them use refine module. \cite{zhao2017pyramid} PSPNet uses four different convolutional kernels and them fusion them using spatial pyramid pooling layers.

% \cite{He_2017_ICCV} add segmentation network and ROI alignment into the \cite{10.5555/2969239.2969250} Faster RCNN to realize the segmention.


% \noindent\textbf{Robust segmentation models:}

\noindent\textbf{Adversarial attack:} tries to generate invisible perturbations
% (which are usually visually invisible) 
to evaluate model robustness.
The generation of adversarial examples is an optimization problem. Define classification loss of a neural network as $\mathcal{L}$, parameters of a neural network as $\theta$, input data as $\mathbf{x}$ and its corresponding one-hot encoded label as $\mathbf{y}$. The objective of adversarial attacks~\cite{szegedy2013intriguing, Explaining_and_Harnessing_Adversarial_Examples, PGD_attack, Towards_Evaluating_Robustness_Neural_Networks, Ensemble_Adversarial_Training_Attacks_Defenses, DeepFool, szegedy2013intriguing} (Eq.~\ref{eq:atk_obj}) is to maximize the classification loss such that a classifier will make incorrect predictions. We define $||\cdot||_p$ as the $L_p$ norm. The norm of an adversarial perturbation $\delta_{adv}$ needs to be bounded by a small value $\delta_{t}$, otherwise the adversarial perturbation may change the semantic interpretation of an image. Once an adversarial perturbation is obtained, the adversarial example can be crafted as $\mathbf{x}_{adv} = \mathbf{x} + \delta_{adv}$, where
\begin{equation}
\label{eq:atk_obj}
\delta_{adv} = \argmax_{||\delta||_{p} \leq \delta_{t}} \mathcal{L}(\theta,\mathbf{x}+\delta,\mathbf{y}).
\end{equation}
% (how about following the definition in the L-BFGS)
% (L-BFGS ~\cite{szegedy2013intriguing}, FGSM ~\cite{Explaining_and_Harnessing_Adversarial_Examples}, PGD ~\cite{PGD_attack}, C and W~\cite{DBLP:journals/corr/CarliniW16a}, RAND FGSM ~\cite{https://doi.org/10.48550/arxiv.1705.07204}, Deepfool ~\cite{DBLP:journals/corr/Moosavi-Dezfooli15})
% L-BFGS ~\cite{szegedy2013intriguing} generate the attack samples by:
% \begin{equation}
%                 \min {c|r| + \mathcal{L}_f (x + r, l) , x + r \in [0, 1]^{_m}}
% \end{equation}
% where $r$ is the perturbation, $c$ is the control parameter of $r$, $l$ is the wrong label, $\mathcal{L}_f$ is the loss function. And the $m$ is the dimensions.
Two typical adversarial attacks include
% strategies for are
FGSM~\cite{Explaining_and_Harnessing_Adversarial_Examples} and PGD~\cite{PGD_attack}. The former
% use FGSM(Fast Sign Gradient Method ) to realize a method that 
generates the perturbations based
% \textbf{Mingyi:  careful ...}
% just based 
on the opposite direction of the loss gradient or the direction for gradient ascent:
\begin{equation}
\label{eq_fgsm_attack}
    \delta_{adv} = \epsilon \text{sign} \left( \nabla_{\mathbf{x}} \mathcal{L}(\theta, \mathbf{x}, \mathbf{y}) \right),
\end{equation}
where $\epsilon $ is the perturbation rate, $\text{sign}$ is the sign function,
% $x$ is the input of the data, $y$ is the output of the model.
$\mathcal{L}(\theta, \mathbf{x}, \mathbf{y})$ is the loss function, and $\theta$ represents model parameters. FGSM~\cite{Explaining_and_Harnessing_Adversarial_Examples} works well for most simple scenarios. However, it might fail to generate accurate directions of adversarial examples due to its single-step scheme.
% for maximizing the inner part of the saddle point formulation~\cite{PGD_attack}. \Zhaoyuan{Ref 1 propose an adversarial training methods, which is a min-max formulation and saddle point is the minimax point. If we only consider attack, it only has maximization formulation, so no saddle point, FGSM is efficient but not effective. }
PGD~\cite{PGD_attack} is then introduced as a multi-step variant:
\begin{equation}
\label{eq_pgd_attack}
    \mathbf{x}_{adv}^{t+1} = \text{CLIP}(\mathbf{x}_{adv}^t + \alpha\text{sign} \left( \nabla_{\mathbf{x}_{adv}^t} \mathcal{L}(\theta, \mathbf{x}_{adv}^t, \mathbf{y}) \right)),
\end{equation}
where
$\alpha$ is the step size, 
$\text{CLIP}$ is the clip function that clip the output into the range $\mathbf{x}_{adv}^0=\mathbf{x}_{clean}+\mathcal{U}(-\epsilon,\epsilon)$ specified by the perturbation rate $\epsilon$.
% , which can be treated as multiple iteration based FGSM, aiming to generate adversarial examples by taking multiple gradient ascent directions into consideration.
% Particularly, FGSM ~\cite{Explaining_and_Harnessing_Adversarial_Examples} generates the perturbation $\eta$ via:
% % to the image according to the sign of the gradient to change the corresponding pixel value.
% % The FGSM(Fast Gradient Sign Method) attack:
% \begin{equation}
%     \eta = \epsilon \text{sign} \left( \nabla_x J(\theta, x, y) \right),
% \end{equation}
% where $\epsilon $ is the perturbation rate, $\text{sign}$ is the sign function,
% $x$ is the input of the data, $y$ is the output of the model.
% $\nabla_x J(\theta, \mathbf{x}, \mathbf{y})$ is the loss function, $\theta$ represents the model parameters.

% Two typical strategies are Fast Sign Gradient Method (FGSM)~\cite{Explaining_and_Harnessing_Adversarial_Examples} and Projected Gradient Descent (PGD)~\cite{PGD_attack}. The former
% % use FGSM(Fast Sign Gradient Method ) to realize a method that 
% generates the perturbations based
% % just based 
% on the opposite direction of the loss gradient or the direction for gradient ascent, which works well for most simple scenarios. However, as single iteration based FGSM~\cite{Explaining_and_Harnessing_Adversarial_Examples} might fail to generate accurate directions of adversarial examples, PGD is then introduced, which can be treated as multiple iteration based FGSM, aiming to generate adversarial examples by taking multiple gradient ascent directions into consideration.
% The latter 
% of the image and uses the opposite direction of the gradient descent.
% PGD(Projected Gradient Descent)~\cite{PGD_attack} 
% applies the FGSM with multiple steps and clips the results after each step.
% can't be distinguished by human) 
% based on the original input to let the neural network output a wrong result with high confidence.
% \cite{szegedy2013intriguing} directly generates the perturbations by setting up the label of each image to random other class and using the L-BFGS to get them. But this could be really slow since it needs to optimize the target function for each image, hence,
% ~\cite{Explaining_and_Harnessing_Adversarial_Examples} use FGSM(Fast Sign Gradient Method ) to realize a method that generates the perturbation just based on the gradient of the image and uses the opposite direction of the gradient descent.
% PGD(Projected Gradient Descent)~\cite{PGD_attack} applies the FGSM with multiple steps and clips the results after each step.
% CW(Carlini and Wagner)~
% CW~\cite{Towards_Evaluating_Robustness_Neural_Networks} introduces box-constraint to the
% % convert the 
% original optimization objective,
% % into a box-constrained optimization objective 
% and use three proposed methods to deal with the box-constrained problem. This method can attack the distilled network effectively.
% Deepfool~\cite{DeepFool} tries to find the decision boundary of the classifier and then adds the perturbation to push the sample to cross the classifier. This method can generate perturbations that are much small than the FGSM.
% RAND+FGSM~\cite{Ensemble_Adversarial_Training_Attacks_Defenses} apply random perturbations on the image before using the FGSM, and then use the FGSM based on the new image. This method is trying to deal with adversarial training(a classic defense method).



% (to make a table)
\noindent\textbf{Adversarial Attack for Semantic Segmentation:} is designed to evaluate robustness of semantic segmentation models with respect to adversarial attacks. Specifically,
% To achieve robust model deployment,
\cite{arnab2018robustness} explores the adversarial robustness of the segmentation model using FGSM~\cite{Explaining_and_Harnessing_Adversarial_Examples} and PGD~\cite{PGD_attack}.
% and they found that the transferability of the perturbations is poor since while using different scales.
% DAG(Dense Adversary Generation) ~\cite{xie2017adversarial} try to let the network misclassify each of the pixels towards another label than the ground truth. And they found that the perturbations are not transferred in a good performance across different networks. And they thought the segmentation models were robust toward those black box attacks. 
% ~\cite{gu2021adversarial} use the dynamic scales to improve the transferability of the untargeted  attack on different models, which shows the effectiveness. (the only one have high transfer ability)
\cite{hendrik2017universal,fischer2017adversarial} generate universal perturbations for the semantic segmentation models and achieve targeted attack on the pedestrians. Note that universal attack is image-agnostic~\cite{moosavi2017universal,hendrik2017universal}, which is a small image perturbation that fools a deep neural network classifier to produce wrong predictions for almost all testing images.
% \sout{, and}\NB{. Alternatively,} 
Alternatively, targeted attack~\cite{adversarial_example_semantic_seg_iclr2017workshop,fischer2017adversarial,hendrik2017universal,li2021hidden} is a more fine-grained attack, where the attack is designed to fool the model to classify the image as a specific target class. 
% \cite{li2021hidden} is a targeted attack for semantic segmentation.
% \cite{hendrik2017universal,fischer2017adversarial} generate universal perturbations for the semantic segmentations and also targeted attack on the pedestrian
% proposed a fine-grained backdoor attack that applied to the segmentation.
% to defend the attacks on the semantic segmentation.
% ~\cite{li2021hidden} proposed a  fine-grained backdoor attack that applied to the segmentation.
% \cite{klingner2020improved} introduces an auxiliary self-supervised monocular depth estimation task to semantic segmentation, and studies robustness of semantic segmentation models with multi-scale perturbations.
% with a multi-task learning framework via an auxiliary self-supervised monocular depth estimation. use the density estimation as a self-supervised signal to improve the robustness of the segmentation model. But they found that under this setting, the PGD can easily attack them with a high perturbation.
% 
For semantic segmentation, untargeted attacks are explored~\cite{agnihotri2023cospgd,gu2022segpgd}, where
% CosPGD~\cite{agnihotri2023cospgd} uses the cosine similarity of the distribution between the predictions and ground truth for each output pixel location to guide the attack process 
% \Jing{more details are needed}.
% To deal with the large iterations of PGD,
SegPGD~\cite{gu2022segpgd} is presented as an adaptation of PGD~\cite{PGD_attack} for semantic segmentation, where misclassified pixels are identified to generate adversarial examples with fewer attack iterations. Further, \cite{gu2022segpgd,xu2021dynamic,xiao2018characterizing} use adversarial training in their framework to boost the robustness of semantic segmentation models, which focus on defense techniques instead of adversarial attacks.
% Similarly,
% fast i, proposed a segmentation-based PGD method called SegPGD which is much more effective than the PGD. And at the same time, they release the SegPGD-based advertised training which reaches to the SOTA on the benchmark. 
% \cite{xiao2018characterizing} achieves model defense via identifying adversarial examples for semantic segmentation with spatial consistency information
% % check 
% and scale consistency check.
% \cite{xu2021dynamic} To deal with that, an advertised training method for segmentation is proposed. However, this method can still be easily influenced by PGD with a large number of attack iterations.

\noindent\textbf{Robust Semantic Segmentation Models:}
% For robust semantic segmentation, they 
use adversarial training~\cite{Xu_2021_ICCV,9506748,9413772}, purification~\cite{kapoor2021fourierdomain,9207291}, perturbation detection~\cite{Xiao_2018_ECCV,tran2021robustness} to defend the perturbations. Among them,
\cite{Xiao_2018_ECCV} uses a spatial consistency check method from random patches to detect the perturbations. \cite{Xu_2021_ICCV} presents a dynamic divide module to deal with the perturbed pixels during the training stage, which can be considered as adversarial training. \cite{kapoor2021fourierdomain} utilizes a Wiener filter to defend attacks from the frequency domain. 
% \cite{9506748,9413772} focuses on the Multi-Sensor Fusion model. 
\cite{tran2021robustness} aims to produce robust verification of the segmentation network. \cite{9207291} trains a denoising autoencoder to deal with the input perturbations.

The robust semantic segmentation models can produce relatively robust results against attacks.
However, they are not evaluated under the attacks designed for segmentation. Especially, we observe the attack transferability issue is not discussed in the existing robust semantic segmentation models~\cite{xu2021dynamic}.
% Some of them are not evaluated under the black-box setting, and even the \cite{xu2021dynamic} as a well-performing algorithm in the black-box setting still does not use a transferable attack or attack design for segmentation to evaluate.

% \Jing{explain what they have achieved and their limitations}. 
% \Mengqi{Have already explained what achieved}


\noindent\textbf{Transferable Adversarial Attacks:}~\cite{JiadongLin2019NesterovAG,zhu2022rethinking, CihangXie2018ImprovingTO, YinpengDong2019EvadingDT} are attacks that can transfer across different models. Conventionally, adversarial attacks are generated from a source model to attack the source model, which shows limitations in attacking the target models (see Fig.~\ref{semantic_seg_towards_attack}). Transferable attacks are generated from the source model, and they are required to be adversarial examples of the target model(s).
% which is different from the conventional
% % Typically, the 
% adversarial attack~\cite{Explaining_and_Harnessing_Adversarial_Examples, PGD_attack}. that is designed to attack one specific model. 
\cite{JiadongLin2019NesterovAG} uses the Nesterov accelerated gradient to replace the momentum method used in the previous gradient stabilized method~\cite{dong2018boosting} to optimize the gradient, leading to transferable attack for image classification.
~\cite{zhu2022rethinking} observes that samples in low-density regions correspond to adversarial examples with high transferability. They then define intrinsic attack as direction for the low-density region and introduce AAI (Alignment between Adversarial attack and Intrinsic attack) as an objective to identify samples with high transferability.
% to measure the alignment between the ideal attack and the conventional attack.
\cite{CihangXie2018ImprovingTO} augments the target image by resizing the image to a random size and then randomly padding zeros around the input image
% according to a random manner 
to increase the transferability of the attack.
\cite{YinpengDong2019EvadingDT} applies three different kernel methods after getting the gradients, to make the perturbations more robust to the discriminative region of the model.
% Transferable adversarial attack~\cite{JiadongLin2019NesterovAG,zhu2022rethinking, CihangXie2018ImprovingTO, YinpengDong2019EvadingDT} has been considered for safe deployment of the deep models, where the attack generated from the source model is required to be able to attack the target model as well. 
% More formally, we would like to design the attack $\delta_{adv}$ from the model with parameters $\theta$ to serve as an adversarial attack for the target model with parameters $\beta$.
% \Jing{introduce those methods}
% AAI~\cite{zhu2022rethinking}
% Nesterov Accelerated Gradient NI ~\cite{JiadongLin2019NesterovAG},
% Diversity transformation DI ~\cite{CihangXie2018ImprovingTO},
% Translation-invariant  TI ~\cite{YinpengDong2019EvadingDT}

\noindent\textbf{Transferable Attack for Semantic Segmentation:}
% is a particular type of transferable adversarial attack that is generated from a source segmentation model and can be used to evaluate model robustness across different target segmentation models.
% DAG(Dense Adversary Generation)~
% As far as we know, DAG~\cite{xie2017adversarial} is the only model working on transferable attack for semantic segmentation. \Jing{check carefully if this statement is true}.
DAG~\cite{xie2017adversarial} studies the transferability of attack for semantic segmentation and object detection via an adversary generation strategy, where the models are encouraged to produce a randomly generated class other than the target class. We find that DAG~\cite{xie2017adversarial} transfers well only on the cross-training setting, where the same network is trained with a different training dataset. When it comes to the different network structures, the transferability of DAG~\cite{xie2017adversarial} is poor, which is also consistent with our experiments. 
\cite{gu2021adversarial} investigates the overfitting phenomenon of adversarial examples for segmentation models, and attributes the transferability issue of semantic segmentation to the architectural traits of segmentation models, \ie~multi-scale object recognition. Dubbed dynamic scaling is then presented in \cite{gu2021adversarial} to achieve a transferable attack for semantic segmentation. Different from \cite{gu2021adversarial} that study transferability \wrt~model structures, we investigate transferable and conventional adversarial attacks for classification, and extend them to semantic segmentation to explain their transferability, aiming to provide algorithm-level understanding on the transferable attack for the dense prediction task.
% observes that the adversarial examples on segmentation do not always overfit the source models. They thus use the dynamic scales to improve the transferability of the untargeted attack.
% We attribute the limitation to the architectural traits of segmentation models, i.e., multi-scale object recognition
% irst, we explore the overfitting phenomenon of adversarial examples on classification and segmentation models. In contrast to the observation made on classification models that the transferability is limited by overfitting to the source model, we find that the adversarial examples on segmentations do not always overfit the source models. Even when no overfitting is presented, the transferability of adversarial examples is limited. We attribute the limitation to the architectural traits of segmentation models, i.e., multi-scale object recognition. Then, we propose a simple and effective method, dubbed dynamic scaling, to overcome the limitation. The high transferability achieved by our method shows that, in contrast to the observations in previous work, adversarial examples on a segmentation model can be easy to transfer to other segmentation models. Our analysis and proposals are supported by extensive experiments.
% on different models.
% which shows the effectiveness. (the only one have high transfer ability)
% incorrect predictions for the target class, and  try to let the network misclassify each of the pixels towards another label than the ground truth. And they found that the perturbations are not transferred in a good performance across different networks. And they thought the segmentation models were robust toward those black box attacks. 
% that works on  

We comprehensively investigate transferable adversarial attack for semantic segmentation (Sec.~\ref{sec_transferable_semantic_attack}). Different from the existing solutions via adversarial training~\cite{xie2017adversarial} or data scaling~\cite{gu2021adversarial}, we adapt the transferable attack for classification~\cite{JiadongLin2019NesterovAG,zhu2022rethinking, CihangXie2018ImprovingTO, YinpengDong2019EvadingDT} to semantic segmentation with a boarder view of the transferable semantic adversarial attack.
% , and explain the transferability of the related models. 
We first explain the existing adversarial attacks for semantic segmentation in Sec.~\ref{subsec_adveraial_attack_semantic}. Given the limited work on adversarial attack for semantic segmentation, we
% main techniques of adversarial attack, and 
extend the image classification based attacks (including both adversarial attacks and transferable attacks) to semantic segmentation in Sec.~\ref{adapting_existing_attack_for_segmentation}. We then analysis those transferable attacks in Sec.~\ref{analysis_attacks}, leading to the proposed
% we Taking a step further, we also study the transferability of adversarial atatcks (Sec.~\ref{subsec_manifold_attack}). 
% We present the transferable attacks for image classification,
% % in Sec.~\ref{subsec_transferable_attack}, 
% which will also be extended to our semantic segmentation task.
% We also present the proposed 
ensemble attack in Sec.~\ref{sec:ensemble_attack}, which is proven superior than the existing techniques in producing more transferable attacks for semantic segmentation.
% explain
% % universal adversarial attack, manifold attack, and explain their 
% transferability of the related models (Sec.\ref{subsec_transferable_attack}).
% ~\cite{hendrik2017universal,fischer2017adversarial} generate universal perturbations for the semantic segmentations and also targeted attack on the pedestrian.
% ~\cite{xiao2018characterizing} use spatial consistency check and scale consistency check to defend the attacks on the semantic segmentation.
% ~\cite{li2021hidden} proposed a  fine-grained backdoor attack that applied to the segmentation.
% ~\cite{klingner2020improved}use the density estimation as a self-supervised signal to improve the robustness of the segmentation model. But they found that under this setting, the PGD can easily attack them with a high perturbation.
% ~\cite{xu2021dynamic} To deal with that, an advertised training method for segmentation is proposed. However, this method can still be easily influenced by PGD with a large number of attack iterations.
% To deal with the large iterations of PGD, ~\cite{gu2022segpgd} proposed a segmentation-based PGD method called SegPGD which is much more effective than the PGD. And at the same time, they release the SegPGD-based advertised training which reaches to the SOTA on the benchmark. 

% \noindent{Adversarial Attack}
% argue that model robustness towards minor perturbation should be considered before model deployment in real-world. We show prediction of state-of-art models with invisible noise in Fig.~\ref{semantic_seg_towards_attack}, where the adversarial example is generated by FGSM~\cite{Explaining_and_Harnessing_Adversarial_Examples}. Two main observations from Fig.~\ref{semantic_seg_towards_attack}: 1) although the difference between adversarial example and the clean image is invisible, the predictions for them are significantly different; 2) the adversarial example from source model fails to attack the target model. 

% In this paper, for the first time, we comprehensively investigate adversarial attack for semantic segmentation, and explain the transferability of the related models. Specifically, we will explain the main techniques for adversarial attack, which can also be defined as the sample space attack. Taking a step further, we study feature space attack, namely manifold attack and explain
% % universal adversarial attack, manifold attack, and explain their 
% transferability of the related models.

\section{Adversarial Attacks for Semantic Segmentation}
\label{sec_transferable_semantic_attack}

% \Jing{methods introduction}

% \Mengqi{Adding the formula, formula needs to be fixed with the arrangement and indent, remain IAA}

As discussed previously, FGSM~\cite{Explaining_and_Harnessing_Adversarial_Examples} and PGD~\cite{PGD_attack} are two widely studied adversarial attacks for image classification. Due to its single-step nature, FGSM~\cite{Explaining_and_Harnessing_Adversarial_Examples} (see Eq.~\ref{eq_fgsm_attack})
% \sout{is proven} \NB{
has been demonstrated to be,
% } 
in general, less effective than its iterative version, namely PGD~\cite{PGD_attack} (see Eq.~\ref{eq_pgd_attack}). Although PGD~\cite{gu2022segpgd} is effective in image classification, \cite{gu2022segpgd} found that directly applying PGD~\cite{gu2022segpgd} to semantic segmentation can be less effective, as a large number of attack iterations are needed to generate accurate adversarial attack. \cite{gu2022segpgd} then presented SegPGD by identifying the misclassified pixels.
% \noindent{PGD}

% This method uses FGSM multiple times with a small step size and it clips the image to keep the overall perturbation of the attack in the  $L_p$ norm. PGD method is also referred to as IFGSM. 

% \begin{equation}
% \begin{aligned} 
% &\boldsymbol{X}^{a d v_{0}}=\boldsymbol{X}
% \\
% &\boldsymbol{X}^{a d v_{t+1}}=\phi^\epsilon\left(\boldsymbol{X}^{a d v_t}+\alpha * \operatorname{sign}\left(\nabla_{\boldsymbol{X}^{a d v_t}} L\left(f\left(\boldsymbol{X}^{a d v_t}\right), \boldsymbol{Y}\right)\right)\right)
% \end{aligned}
% \end{equation}
% where $\boldsymbol{X}^{adv}$ is the perturbation result, the $\boldsymbol{Y}$ is the label,$f\left (  \right )$ is the targeted model.$\nabla_{\boldsymbol{X}^{a d v_t}} L\left(f\left(\boldsymbol{X}^{a d v_t}\right), \boldsymbol{Y}\right)$ is the loss function,$sign\left (  \right )$ is the sign function,$\alpha$ represents the step size, and the $\phi^\epsilon$ is the clip function which tries to let the image into an
% $L_{\inf}$ ball of radius $\epsilon$,and $t$ donates as the attack iterations.
% \noindent{SegPGD}
\subsection{Existing Adversarial Attack for Semantic Segmentation}
% pgd and segpgd
\label{subsec_adveraial_attack_semantic}
\noindent\textit{\textbf{SegPGD:}}
Specifically, \cite{gu2022segpgd} attributes the larger attack iterations of PGD~\cite{PGD_attack} for semantic segmentation to the imbalanced gradient contribution, where the wrongly classified pixels dominate the adversarial sample generation process, thus larger attack iterations are needed to mislead the correctly classified pixels with small cross-entropy loss. Based on the above observation, \cite{gu2022segpgd} divides the pixels into two groups according to the accuracy of the prediction, namely the accurately classified pixels $P^T$, and the wrongly classified ones $P^F$, leading to a reformulated loss function:
\begin{equation}
\mathcal{L}(\theta,\mathbf{x},\mathbf{y})=\frac{1}{H \times W}\sum_{j\in P^T}\mathcal{L}_i+\frac{1}{H \times W}\sum_{k\in P^F}\mathcal{L}_k,
\end{equation}
where $H$ and $W$ indicate the spatial dimension of the input,
% the height and width, 
$\mathcal{L}_i$ is the loss for the $i^{th}$ pixel, which is cross-entropy loss for semantic segmentation, and $\theta$ represents model parameters. 
% $f(\cdot,\cdot)$ is the segmentation model, the first half and second of the second equation is the loss term for the success attacked pixels and not attacked.

To tackle the gradient dominant issue, \cite{gu2022segpgd} introduces dynamic weighted loss function:
\begin{equation}
\label{eq_segpgd}
\mathcal{L}(\theta,\mathbf{x}_{adv}^t,\mathbf{y})=\frac{1-\lambda_t }{H \times W}\sum_{j\in P^T}\mathcal{L}_i+\frac{\lambda_t}{H \times W}\sum_{k\in P^F}\mathcal{L}_k,
\end{equation}
where $\lambda_t$ is set dynamically with the number of attack iterations, and $\mathcal{L}_i=\mathcal{L}_i(\theta,\mathbf{x}_{adv}^t,\mathbf{y})$ is the loss for the $i^{th}$ pixel. Based on the reformulated loss function in Eq.~\ref{eq_segpgd}, the generated adversarial example with PGD~\cite{PGD_attack} can be achieved:
\begin{equation}
    \mathbf{x}_{adv}^{t+1} = \text{CLIP}(\mathbf{x}_{adv}^t + \alpha\text{sign} \left( \nabla_{\mathbf{x}_{adv}^t} \mathcal{L}_w(\theta, \mathbf{x}_{adv}^t, \mathbf{y}) \right),
\end{equation}
where $\nabla_{\mathbf{x}_{adv}^t} \mathcal{L}_w(\theta, \mathbf{x}_{adv}^t, \mathbf{y})$ is the weighted gradient with:
\begin{equation}
\begin{aligned} 
    &\mathcal{L}_w(\theta, \mathbf{x}_{adv}^t, \mathbf{y})\\
    &= \sum_{j\in P^T}(1-\lambda_t)\mathcal{L}_j(\theta, \mathbf{x}_{adv}^t, \mathbf{y}) + \sum_{j\in P^F}\lambda_t \mathcal{L}_k(\theta, \mathbf{x}_{adv}^t, \mathbf{y}),
\end{aligned}
\end{equation}
where $\alpha$ is the step size as in Eq.~\ref{eq_pgd_attack}. The initial point is usually defined as the clean sample, \ie~$\mathbf{x}_{adv}^0=\mathbf{x}_{clean}$, or its perturbed sample with uniform noise as $\mathbf{x}_{adv}^0=\mathbf{x}_{clean}+\mathcal{U}(-\epsilon,\epsilon)$ specified by the perturbation rate $\epsilon$. With the dynamic weighted loss function in Eq.~\ref{eq_segpgd}, SegPGD~\cite{gu2022segpgd} generates more effective adversarial examples than PGD~\cite{PGD_attack} under the same number of attack iterations.

% DAG(dense adversarial generation)(to add functions)
% \Jing{will come back to this part}
\subsection{Adapting Adversarial Attacks for Semantic Segmentation}
\label{adapting_existing_attack_for_segmentation}
As there exists limited adversarial attack models for semantic segmentation, we first extend existing adversarial attacks for classification models to our dense classification task, namely semantic segmentation. Further, given the necessity for transferable attack, we also explore the existing transferable attacks for semantic segmentation.
% , we explore the existing transferable attack for classification to our dense classification task, namely semantic segmentation.

\noindent\textit{\textbf{DAG:}} Although SegPGD~\cite{gu2022segpgd} takes wrongly classified pixels into consideration to speed up the adversarial sample generation process, there are no constraints on the extent to which the prediction should be destroyed. DAG~\cite{xie2017adversarial} can then be used to address the above issue.
Different from image classification that aims to classify only one target, \ie~classify the whole image to a specific class, semantic segmentation is designed to classify each pixel, \ie~multiple targets, to a class.
% Based on the observation that segmentation is based on classifying multiple targets on an image, 
DAG~\cite{xie2017adversarial} is designed to generate an adversarial perturbation that can confuse as many targets as possible.
Given one pixel $\mathbf{x}_{uv}$ in image $\mathbf{x}$ with coordinate $(u,v)$, with the pre-trained model $f(\theta)$, we obtain its classification score before softmax as $f(\mathbf{x}_{uv},\theta)\in\mathbb{R}^C$, where $C$ is the number of classes. DAG~\cite{xie2017adversarial} aims to cause all the pixel predictions to be incorrect.
% \NB{make all of the pixel predictions incorrect?} make predictions of all the pixels go wrong. 
With this goal, they define an objective:
\begin{equation}
\label{eq_dag_min_obj}
    \mathcal{L}=\sum_u\sum_v\left(f_{l_{uv}}(\mathbf{x}_{uv},\theta)-f_{l'_{uv}}(\mathbf{x}_{uv},\theta)\right),
\end{equation}
where $l_{uv}$ is the ground-truth label of $\mathbf{x}_{uv}$, and $l'_{uv}\in\{1,2,...,C\} \setminus \{l_{uv}\}$, which is other category than the actual one. Minimizing Eq.~\ref{eq_dag_min_obj} can cause every pixel to be incorrectly predicted. DAG~\cite{xie2017adversarial} then defines adversarial attack based on Eq.~\ref{eq_dag_min_obj} iteratively. For the $t^{th}$ iteration, they find the correctly predicted pixels, which can be denoted as $\mathbf{A}^t$. They obtain the accumulated perturbation as:
\begin{equation}
\label{eq_dag_accumulated_rt}
    \mathbf{r}^t = \sum_{uv\in \mathbf{A}^t}\left(\nabla_{\mathbf{x}^t}f_{l'_{uv}}(\mathbf{x}_{uv},\theta)-\nabla_{\mathbf{x}^t}f_{l_{uv}}(\mathbf{x}_{uv},\theta)\right).
\end{equation}
To achieve stable training, $\mathbf{r}^t$ is normalized as: $\mathbf{r'}^t=\frac{\gamma}{\|\mathbf{r}^t\|_\infty}\cdot \mathbf{r}^t$, where $\gamma=0.5$ in \cite{xie2017adversarial}. $\mathbf{x}^{t+1}$ is then obtained via $\mathbf{x}^{t+1}=\mathbf{r'}^t+\mathbf{x}^t$. The algorithm terminates when $\mathbf{A}^t=\varnothing$ or the maximum iteration is reached. The final perturbation is defined as: $\delta_{adv}=\sum_t \mathbf{r'}^t$, and the adversarial example is then $\mathbf{x}_{adv}=\mathbf{x}+\delta_{adv}$.
% indicating that DAG can also be treated as a single-step attack.
% gradient respected to the current image $\mathbf{x}_t$ is defined 


% generate dense perturbations by randomly assigning the current target label to random fake labels.
% Assume we have $N$ images to recognize which is $O=\left\{o_1,o_2,...,o_N \right\}$, for each of them, $o_n,n=1,2,..,N$, they have a true label which is $y_n\in \left\{1,2,..,c\right\}$, where $c$ is the class number, for example, for the PascalVOC 2012, we have $c=21$. Then the label is $Y = \left\{y_1,y_2,..,y_n\right\}$. Then,we can generate a fake label $Y^f=\left\{y^f_1,y^f_2,..,y^f_n\right\}$ and $y^f_n \in \left\{1,2,..,c\right\} \setminus{y_n} $
% According to the output of the model, it calculates the gradient difference according to the true label and false label:
% \begin{equation}
% \begin{aligned}
% r =  \sum{y_n}_\in{}_{Y_i}\left [ \nabla_{{x}^{a d v_t}} \mathcal{L}\left(\theta,{x}^{a d v_t}, {y^f}\right)-\nabla_{{x}^{a d v_t}} \mathcal{L}\left(\theta,{x}^{a d v_t}, {y}\right) \right ]
% \end{aligned}
% \end{equation}
% where $Y_i$ is:
% \begin{equation}
% \begin{aligned}
% Y_i=\left \{ y_i|argmax\left (f\left({x}^{a d v_t}\right)  \right )_{}=y_n \right \}
% \end{aligned}
% \end{equation}
% i denote the iterations.
% And then, do the normalization of the gradient difference and multiply it with the step size $\beta$ to get the scaled difference: 
% $r=\frac{\beta  }{\left \| r \right \|_\infty}$.
% After that, the result will be added to the initialized perturbation and all the temp input. After iterations, the final perturbation will be directly added to the original input image.

% \begin{equation}
% \begin{aligned} 
%     &\boldsymbol{X}^{t+1}=\boldsymbol{X}^t -\alpha * \operatorname{sign}\\
%      &\left(\nabla\left(\sum_{j \in P^T}(1-\lambda(t)) g_j\left(\boldsymbol{X}^t\right)+\sum_{k \in P^F} \lambda(t) g_k\left(\boldsymbol{X}^t\right)\right)\right)
% \end{aligned}
% \end{equation}
% where the $g$ is the loss function,$\lambda(t)$ is the weight that changes according to the iteration t change, the $P^T$are the pixels that have already been misclassified, and the $P^F$ are the pixels that haven't been successfully misclassified,$sign\left (  \right )$ is the sign function.$\alpha$ is the step size and the $\boldsymbol{X}^{t}$ and $\boldsymbol{X}^{t+1}$  is the perturbation result after $t$ times attack.
% the rate used to control the attack weight.
% and  This method is try to let the attack focus more on the unattacked pixel with the iteration of the attack increase. This method is a PGD-based method that tries to be specialized for segmentation.
% The step of the SegPGD includes splitting the pixel according to the current prediction, 

% computing new weight according to the iteration, and then doing the gradient descent according to the weighted pixel wised gradient.

% For the segmentation task, the loss function of the attack can be reformulated to:
% \begin{equation}
% \begin{aligned} 
% L(f(X^{adv_t},Y)))=\frac{1}{H \times W}\sum_{i=1}^{H \times W}L_i\\
% =\frac{1}{H \times W}\sum_{j\in P^T}L_i+\frac{1}{H \times W}\sum_{k\in P^F}L_k\\
% \end{aligned}
% \end{equation}
% where H and W are the height and width, and $ L_i$ is the loss for the ith pixel. $ P^T$ are the pixels that are currently misclassified, and the $P^F$ are the pixels that haven't been successfully misclassified.$f(\cdot,\cdot)$ is the segmentation model, the first half and second of the second equation is the loss term for the success attacked pixels and not attacked.

% The SegPGD updated this loss term to:

% \begin{equation}
% \begin{aligned} 
% L(f(X^{adv_t},Y)))=\frac{1-\lambda }{H \times W}\sum_{j\in P^T}L_i+\frac{\lambda }{H \times W}\sum_{k\in P^F}L_k\\
% \end{aligned}
% \end{equation}
% where $\Lambda$ is the rate used to control the attack weight.

% The gradient-descent step of the SegPGD is:




% Conventional methods that directly apply to the segmentation task,  FGSM~\cite{Explaining_and_Harnessing_Adversarial_Examples}, PGD~\cite{PGD_attack}, including their targeted attack version, have shown their effect on the adversarial semantic attack while the generate the perturbation from their own and attack themselves. After that, there are some methods that try to attack according to the difference between conventional classification and segmentation.
% DAG ~\cite{xie2017adversarial} try to let the network misclassify each of the pixels towards another label than the ground truth.
% SegPGD ~\cite{gu2022segpgd} tries to let the attack focus more on the current un-attacked pixel when the iteration is higher. 
% CosPGD ~\cite{agnihotri2023cospgd} uses the cosine similarity between the predictions and ground truth to help the attack process.

% Some other adversarial methods of segmentation are also explored, including universal perturbation ~\cite{hendrik2017universal,fischer2017adversarial}, backdoor attack ~\cite{li2021hidden}, defense ~\cite{klingner2020improved,xiao2018characterizing,xu2021dynamic}.

% \subsection{Manifold Semantic Attack}
% \label{subsec_manifold_attack}
% The conventional attack is designed in sample space, where the invisible noise (adversarial attack $\delta_{adv}$) is directly added to the raw image $\mathbf{x}$. Differently, manifold attack~\cite{manifold_attack,manifold_view_attack_AISTATS2022,li2021exploring, ZHANG2022102770,DongzeLi2021ExploringAF,tran2020manifold,zhang2022manifold} is the attack in feature space. 
% Among them, \cite{DongzeLi2021ExploringAF} introduces adversarial fake image on face manifold with style-GAN~\cite{karras2019style}.
% % \Jing{citation}.
% Instead of adding adversarial noise directly to the image, \cite{DongzeLi2021ExploringAF} optimally searches adversarial points on the face manifold to generate anti-forensic fake face images.
% % e iteratively do a gradient-descent with each small step in the latent space of a generative model, e.g. Style-GAN, to find an adversarial latent vector, which is similar to norm-based adversarial attack but in latent space. Then, the generated fake images driven by the ad- adversarial latent vectors with the help of GANs can defeat mainstream forensic models. 
% % and perform
% % backbone and do the 
% % gradient descent in the latent space of the generative model for
% % to do a
% % manifold attack for the forensic task. 
% % In this paper, 
% \cite{xiao2022understanding} presents the conditional generative models with PGD~\cite{PGD_attack} and FGSM~\cite{Explaining_and_Harnessing_Adversarial_Examples} to generate on-manifold adversarial samples. And they also restrict the attack in eigenspace and show a powerful on-manifold attack rate. \cite{zhang2022manifold} uses adversarial auto-encoders~\cite{ballard1987modular}
% % \Jing{citations} 
% to learn the low-dimensional manifold of data and then uses a latent substitute model to generate manifold adversarial samples.
% % , while it just compares with the conventional adversarial attack.

% For semantic segmentation, generative adversarial perturbations (GAP)~\cite{OmidPoursaeed2017GenerativeAP} was introduced to generate both universal attacks via a generative adversarial net~\cite{goodfellow2020generative}
% % \Jing{citation} 
% or image-dependent attack via a perturbation generator to fool the segmentation model. We define GAP~\cite{OmidPoursaeed2017GenerativeAP} as manifold attack due to it's feature-space perturbation attribute, and discuss its image-dependent perturbation variant.
% % via a  eh generative-based attack method based on the classification task, can also extend to the segmentation task.
% Consider a pre-trained segmentation network $f_\theta$ with parameters $\theta$, \cite{OmidPoursaeed2017GenerativeAP} aims to generate image perturbation $\delta_{adv}(\beta,\mathbf{x})$ via a learned function $p_\beta$ with parameters $\beta$ to achieve a low accuracy and high fooling ratio by feeding it back to $f_\theta$, where the fooling ratio is defined as the proportion of wrong predictions.

% Specifically, to meet the two requirements, namely wrong predictions and small perturbations, $p_\beta$ is designed to take the clean image $\mathbf{x}_{clean}$ as input, and output perturbation $\delta_{adv}$ with scaling operator to satisfy a norm constraint. The final adversarial example is then defined as:
% \begin{equation}
% \label{eq_map_adv_sample}
%     \mathbf{x}_{adv}=\text{CLIP}(\text{scale}(p_\beta(\mathbf{x}_{clean}))),
% \end{equation}
% where $\text{scale}$ is the scaling operator. Given the pre-trained semantic segmentation model $f_\theta$, the perturbation function $p_\beta$ is learned via gradient descent with a fooling loss:
% \begin{equation}
% \label{eq_gap_loss}
%     \mathcal{L}_{\text{GAP}}=-\log\mathcal{L}(f_\theta(\mathbf{x}_{adv}),\mathbf{y}),
% \end{equation}
% where $\mathcal{L}(f_\theta(\mathbf{x}_{adv}),\mathbf{y})$ is the cross-entropy loss for semantic segmentation, $\mathbf{x}_{adv}$ is generated via Eq.~\ref{eq_map_adv_sample}. GAP~\cite{OmidPoursaeed2017GenerativeAP} presents a decreasing function of cross-entropy loss for the learning of adversarial perturbation $\delta_{adv}$.


% % which has the map F($x$) = (F($x_1$), F(,$x_2$)...F($x_n$)) $\in$ ${\left\{ 1,2,..., C\right\}^n} $ for each image $x = (x_1,x_2,...,x_n)$, while the label for each image is $y_{x} = (y_{x_1},y_{x_2},...,y_{x_n})$. They assumed that images are normalized to $\left[0,1\right]$, then the space of the images is $\mathcal{H}\subset \left[0,1  \right]^{n}$. 
% % Then the perturbation samples should be: 
% % \begin{equation}
% % \begin{aligned} 
% %     X_{attack} = &\left\{ x_{attack}\in\left[ 0,1 \right]^{n}| \exists x \in \left[ 0,1 \right]^{n}: d\left( x_{attack},x \right)\\
% %     &\lt \epsilon,\forall i \in \left\{ 1,2,...,n \right\}:F(x_{attack_i})\neq y_{x_i} \right\}
% % \end{aligned}
% % \end{equation}
% % , while the $\epsilon$ is the perturbation threshold,$d\left(\cdot,\cdot  \right) $is the distance function.

% % And the method uses the generator $G_\theta$, which inputs the image and then generates the perturbation that let $x\in \mathcal{H}$, $F\left ( G_{\theta}(x)+x \right )\neq F\left ( x \right )$. And the $L_p$ norm of the perturbation $\left \| G_\theta(x) \right \|_p$ should be $\epsilon$, which should be small enough to make the attack imperceptible.

% \subsection{Transferable Semantic Attack}
% \label{subsec_transferable_attack}
% Different from 
Taking a step further, we extensively investigate transferable attack for semantic segmentation by extending the existing image classification based transferable attack to semantic segmentation. Four main strategies will be investigated, including NI~\cite{JiadongLin2019NesterovAG}, DI~\cite{CihangXie2018ImprovingTO}, TI~\cite{YinpengDong2019EvadingDT} and AAI~\cite{zhu2022rethinking}.
% and IAA(Intrinsic Adversarial Attack)~\cite{zhu2022rethinking} are typical transferable attack methods that are used in the classification task. 
% The above mentioned 

% NI(Nesterov Accelerated Gradient Input)~\cite{JiadongLin2019NesterovAG}, DI(Diversity input) ~\cite{CihangXie2018ImprovingTO} and TI(Transformation input)~\cite{YinpengDong2019EvadingDT}
% and IAA(Intrinsic Adversarial Attack)~\cite{zhu2022rethinking} are typical transferable attack methods that are used in the classification task. 

\noindent{\textit{\textbf{NI}
% (Nesterov Accelerated Gradient Input)
}}~\cite{JiadongLin2019NesterovAG} regards adversarial example generation as an optimization process
% with two methods to improve the transferability of adversarial examples 
by adapting Nesterov accelarated gradient (NAG)~\cite{nesterov_accelerated_gradient} into the iterative attacks.
% Same as above symbol definition, 
NAG~\cite{nesterov_accelerated_gradient} is a variation of normal gradient descent to speed up the training process with improved model convergence. Given dynamic variable $\mathbf{v}$, model parameters $\theta$, and loss function $\mathcal{L}$, NAG optimizes $\theta$ via:
\begin{equation}
    \begin{aligned}
\mathbf{v}^{t+1}=\mu\mathbf{v}^t+\nabla_{\theta_t}\mathcal{L}(\theta_t-\alpha\mu\mathbf{v}^t)\\
    \theta^{t+1}=\theta^t-\alpha\mathbf{v}^{t+1},
    \end{aligned}
\end{equation}
where $\mu$ is the decay factor of $\mathbf{v}^t$, and $\alpha$ is the step size as in Eq.~\ref{eq_pgd_attack}. NAG can be viewed as an improved momentum method~\cite{momentum_method}, which
% where the latter
is proven effective in achieving stabilized adversarial direction updating for transferable attack. Based on this, \cite{JiadongLin2019NesterovAG} introduces NI-FGSM with start point $\mathbf{g}^0=0$, which is formulated as:
% , also known as NAG, 
% is based on the fact that using Nesterov accelerated gradient is better than using the momentum for optimization. 
% Since different DNN architectures will have different decision boundaries since they have high non-linearity, hence, it will be the case that the attack gradient calculated on one model might be locally minimal, which will result in low transferability on another model. Hence, they try to use the sum to stabilize the gradient to reach a better transfer ability of the attack.
% NI can be written as follow:
\begin{equation}
\label{eq_ni_adversarial_attack}
    \begin{aligned}
        &\mathbf{x}^{nes}= \mathbf{x}_{adv}^t+\alpha\mu \mathbf{g}^t\\
        &\mathbf{g}^{t+1}=\mu\mathbf{g}^t + \frac{\nabla_{\mathbf{x}^{nes}} \mathcal{L}\left (\theta,\mathbf{x}^{nes},  \mathbf{y}\right )}{\left \| \nabla_{\mathbf{x}^{nes}} \mathcal{L}\left (\theta,\mathbf{x}^{nes},  \mathbf{y}\right ) \right \|_{1}}\\
        &\mathbf{x}_{adv}^{t+1}=\text{CLIP}( \mathbf{x}_{adv}^t+\alpha \text{sign} ( \mathbf{g}^{t+1} ) ),
    \end{aligned}
\end{equation}
where
% the t is the iterations, $x_{t}^{attack}$ is the attacked image for the iteration t,$\alpha$is the step size, the 
$\mathbf{g}^t$ is the accumulated gradient,
% at iteration $t$, 
$\mu$ is the decay rate of $\mathbf{g}^t$, $\mathbf{x}_{adv}$ is the Nesterov accelerated result. 
% \Zhaoyuan{What is the main difference between NAG and momentum method? Is this equation 
% correct?}
Eq.~\ref{eq_ni_adversarial_attack} provides an iterative adversarial example generation strategy, which is proven effective in generating transferable attack.
% $\frac{\nabla_{x} \mathcal{J}\left (x_{t}^{NI},  Y\right )}{\left \| \nabla_{x} \mathcal{J}\left (x_{t}^{NI}, Y\right ) \right \|_{1}}$ is the Nesterov accelerated gradient, which modifies from the momentum gradient, $\epsilon $ is the perturbation rate, $\text{sign}$ is the sign function ,$clip_{x}^{\epsilon  }$ is the clip function.

% As a common tech to improve the generalizability of the model, data augmentation is also used to improve the transferability of the attack. Some methods, such as DI(Diversity input) ~\cite{CihangXie2018ImprovingTO} and TI(Transformation input)~\cite{YinpengDong2019EvadingDT} force the attacks to be invariant to the certain transformation.

% DI~\cite{CihangXie2018ImprovingTO},
% Translation-invariant  TI ~\cite{YinpengDong2019EvadingDT}

\noindent{\textit{\textbf{DI}}}~\cite{CihangXie2018ImprovingTO} observes iterative attacks tend to overfit the specific network parameters, leading to poor transferability. Inspired by data augmentation techniques to prevent the network from overfitting, \cite{CihangXie2018ImprovingTO} proposes to improves the transferability of adversarial examples by creating diverse input patterns, where the adversarial examples are then generated based on the diverse transformations of the input image.
% tries to 
Specifically, \cite{CihangXie2018ImprovingTO} augments the input image by resizing it
% the input image 
to a random size and then padding zeros around it.
% the input image in a random manner, and 
% They use the random weight to deal with the effect on the transferability and the 
% effectiveness of the normal attack.
Given transformation function $T(\cdot)$, \cite{CihangXie2018ImprovingTO} generates adversarial examples via:
\begin{equation}
\label{eq_di_attack}
    \mathbf{x}_{adv}^{t+1} = \text{CLIP}(\mathbf{x}_{adv}^t + \alpha\text{sign} \left( \nabla_{\mathbf{x}_{adv}^t} \mathcal{L}(\theta, T(\mathbf{x}_{adv}^t,p), \mathbf{y}) \right)),
\end{equation}
where the transformation function $T(\cdot)$ performs image augmentation with transformation probability $p$, \ie~with probability $p$, data augmentation will be applied to $\mathbf{x}_{adv}^t$, and probability $1-p$, $T(\mathbf{x}_{adv}^t)$ remains $\mathbf{x}_{adv}^t$.
% The formulation is as follows:
% \begin{equation}
%     \begin{aligned}
%     {x}^{a d v_{t+1}}=&\phi^\epsilon\left({x}^{a d v_t}+
%     \alpha * \operatorname{sign}\left(\nabla_{{x}^{a d v_t}} \mathcal{L}\left(T\left (\theta ,({x}^{a d v_t};p\right ), {y}\right)\right)\right)
%     \end{aligned}
% \end{equation}
% where ${x}^{adv}$ is the perturbation result, the ${y}$ is the label,$f\left (\cdot\right )$ is the targeted model.$\nabla_{{x}^{a d v_t}} \mathcal{L}\left(T\left (\theta,({x}^{a d v_t};p \right ), {y}\right)$ is the loss function,$sign\left (  \right )$ is the sign function,$\alpha$ represents the step size, and the $\phi^\epsilon$ is the clip function which tries to let the image into an
% $L_{\inf}$ ball of radius $\epsilon$,and $t$ donates as the attack iterations.


% The stochastic augmentation method $T(\cdot;p)$  is applied with a   probability p,which is:
% \begin{equation}
%     \begin{aligned}
%     T(X^{adv_t};p) = T(X^{adv_t}) with probability p,\\
%      = X^{adv_t} with probability 1 - p
%     \end{aligned}
% \end{equation}


\noindent{\textit{\textbf{TI}}}~\cite{YinpengDong2019EvadingDT} is another augmentation-based transferable attack by optimizing a perturbation over an ensemble of translated images, which can also be interpreted as convolving the gradient for the untranslated image with a pre-defined kernel $\mathbf{W}$. As TI~\cite{YinpengDong2019EvadingDT} directly works on the gradient of the loss function, it can be integrated into existing gradient based attack methods, \eg~FGSM~\cite{Explaining_and_Harnessing_Adversarial_Examples}, PGD~\cite{PGD_attack}, \etc. The combination of TI~\cite{YinpengDong2019EvadingDT} and PGD~\cite{PGD_attack} has the following update rule:
\begin{equation}
\label{eq_ti_pgd_attack}
    \mathbf{x}_{adv}^{t+1} = \text{CLIP}(\mathbf{x}_{adv}^t + \alpha\text{sign} \left( \mathbf{W}\ast\nabla_{\mathbf{x}_{adv}^t} \mathcal{L}(\theta, \mathbf{x}_{adv}^t, \mathbf{y}) \right)).
\end{equation}
% Instead of augmenting the input image, it applies kernel methods $\mathbf{W}$ to the gradient of loss.
% , including uniform kernel, linear kernel, and Gaussian kernel, after calculating the gradient. 
% They apply these kernels to generate perturbations that are less sensitive to the discriminative regions of the model.
% The update rule now becomes:
% \begin{equation}
%     \begin{aligned}
%      {x}^{a d v_{t+1}}=&\phi^\epsilon\left({x}^{a d v_t}+\\
%  &\alpha * \operatorname{sign}\left(w*\nabla_{{x}^{a d v_t}} \mathcal{L}\left(\theta,{x}^{a d v_t}, {y}\right)\right)\right)
%     \end{aligned}
% \end{equation}
% where ${x}^{adv}$ is the perturbation result, the ${y}$ is the label,$\theta$ is the model parameter.$\nabla_{{x}^{a d v_t}} \mathcal{L}\left(\theta,{x}^{a d v_t}, {y}\right)$ is the loss function,$sign\left (  \right )$ is the sign function,$\alpha$ represents the step size, and the $\phi^\epsilon$ is the clip function which tries to let the image into an
% $L_{\inf}$ ball of radius $\epsilon$,and $t$ donates as the attack iterations.
where the kernel matrix $\mathbf{W}$ is selected as a
% , it can be represented using the
Gaussian kernel, which is used to convolve
the gradient, namely $\nabla_{\mathbf{x}_{adv}^t}\mathcal{L}$.
% that:
% $\tilde{\mathbf{W}}_{i,j}=\frac{1}{2\pi \sigma^2}exp(-\frac{i^2+j^2}{2 \sigma^2})$
% where the $\sigma$is the standard deviation,2k+1 is the kernel size.

\noindent{\textit{\textbf{IAA}}} (Intrinsic Adversarial Attack)~\cite{zhu2022rethinking} is different from the improved momentum method, namely NI~\cite{JiadongLin2019NesterovAG}, or the augmentation techniques, namely DI~\cite{CihangXie2018ImprovingTO} and TI~\cite{YinpengDong2019EvadingDT}. It investigates transferability of adversarial attack from the data distribution's perspective. Specifically, \cite{zhu2022rethinking} define the intrinsic attack as the direction that point
% \NB{Unclear, what is the direction point? - direction pointing to?}\Jing{it's the direction that point to the low density region. IAA define this direction as guidance to find samples in low density region, where they assume samples in low density region could lead to higher transferability} 
to the low density region based on the observation that samples in low density region can produce adversarial examples of high transferability.
% the above augmentation based methods on ge, is known as a task refinement method, 

Based on the above observation, \cite{zhu2022rethinking} introduces alignment
% define defines an AAI(Alignment 
between adversarial attack and the intrinsic attack (AAI) as:
\begin{equation}
\label{eq_aai_definition}
    \begin{aligned}
     \mathbf{\text{AAI}} \triangleq \mathbb{E}_{p(\mathbf{x}, \mathbf{y})}\left[\frac{\nabla_{\mathbf{x}} \log p_{\theta, \lambda}(\mathbf{y} \mid \mathbf{x})}{\left\|\nabla_{\boldsymbol{\mathbf{x}}} \log p_{\theta, \lambda}(\mathbf{y} \mid \mathbf{x})\right\|_2} \cdot \nabla_{\mathbf{x}} \log p(\mathbf{x}, \mathbf{y})\right],
    \end{aligned}
\end{equation}
where $p(\mathbf{x}, \mathbf{y})$ is the true data joint distribution, and $p_{\theta, \lambda}(\mathbf{y} \mid \mathbf{x})$ is the conditional generation model with $\theta$ as the pre-trained conditional generation model $p_{\theta}(\mathbf{y} \mid \mathbf{x})$, and $\lambda$ represents the structure level modification of $p_{\theta}(\mathbf{y} \mid \mathbf{x})$ towards transferable attack, which include replacing ReLU~\cite{agarap2018deep} with SoftPlus~\cite{zheng2015improving} and updating residual connections~\cite{he2016deep} to perparameterized residual connections. 



$\text{AAI}$ in Eq.~\ref{eq_aai_definition} can learned to obtain parameters $\lambda$ via Bayesian optimization. Particularly, as unobserved data distribution $p(\mathbf{x}, \mathbf{y})$ is involved in the optimization function, \cite{zhu2022rethinking} uses score matching~\cite{https://doi.org/10.48550/arxiv.1905.07088} together with Bayesian optimization to optimally search for $\lambda$. Note that \cite{zhu2022rethinking} does not provide update rules
% roles\NB{rules?} 
for adversarial examples generation, it optimizse Eq.~\ref{eq_aai_definition} to obtain a new model $p_{\theta, \lambda}(\mathbf{y} \mid \mathbf{x})$, and proves that the adversarial examples based on this new model are capable of generating transferable attack. 



\subsection{Analysis of Adversarial Attack for Semantic Segmentation}
\label{analysis_attacks}
% \Jing{extensively analyze the formulation differences of the discussed attacks, 1) their differences and correlations; 2) explain the critical requirement for transferable attack}
% Most of attack methods are based on PGD~\cite{PGD_attack},
% % which are iterative FGSM
% % \Zhaoyuan{
% and those methods does not transfer well to other models.
% }
For the transferable attacks, \cite{dong2018boosting} believes that
the transferability of adversarial examples can be likened to the generalization capacity of trained models when considering optimization. Generating adversarial examples is analogous to optimizing neural networks during training. In this process, the white-box model under attack is refined using adversarial examples, which serve as training data. These examples can be regarded as the training parameters for the model. During testing, black-box models evaluate the adversarial examples, acting as testing data. Consequently, techniques employed to improve model generalization can be applied to enhance the transferability of adversarial examples. 

The generalization process has two main ways, (1) data augmentation and (2) better optimization. For the data augmentation way, the DI~\cite{CihangXie2018ImprovingTO} and TI~\cite{YinpengDong2019EvadingDT}, believe that transfer to an unseen model should be similar to transfer to some unseen images, hence they try to augment the images before inputting them into the attack technique, \eg~PGD~\cite{PGD_attack}.
For the optimization way, the NI~\cite{JiadongLin2019NesterovAG} use a Nesterov Accelerated Gradient \cite{nesterov2012efficiency} in the gradient descent stage, which applies the idea of momentum. NI~\cite{JiadongLin2019NesterovAG} believes that although different networks will have different decision boundaries (due to their high non-linearity), they still have similar test performance \cite{liu2016delving}. As a surrogate refinement method, the IAA~\cite{zhu2022rethinking} uses the AAI to indicate the correlation between the global optimize direction and the current direction, it changes the structure parameter to get the maximum AAI and then applies the PGD~\cite{PGD_attack}. IAA attacks the low-density area of the model training to realize the transfer ability. As for the segmentation attack, SegPGD~\cite{gu2022segpgd} checks the miss classified pixels iteratively and changes the weight to focus on those that are still correctly classified. DAG~\cite{xie2017adversarial} tries to misclassify all the pixel into specified wrong classes.
To realize high transferability, we conclude that an attack method has to be attacked on the critical region as IAA~\cite{zhu2022rethinking}, or augmented to deal with unseen model, or use a stabilized gradient to try to find a global optimal attack direction, leading to our ensemble attack, aiming to aggregate multiple transferable attacks for classification to extensively explore their contributions for transferable attack.

\begin{table*}[t!]
  \centering
  \footnotesize
  \renewcommand{\arraystretch}{1.2}
  \renewcommand{\tabcolsep}{4.2mm}
  \caption{Adversarial attack on PascalVOC dataset, where attacks with each individual source model, e.g. \enquote{DV3Res50}, are categorised as the
  % above the line within each source model are 
  conventional attacks (those above the middle line) and the
  % , and those below the line are 
  transferable attacks (those below the middle line). The bold numbers highlight the best performance based on each source model.
  % where the source models are:(PGD need retest) ResNet50 backbone based Deeplabv3 (DV3), ResNet101 backbone based Deeplabv3+ (DV3+) and VGG16 backbone based FCN8S-16 (FCN). The attack methods include the conventional adversarial attack, namely PGD, the manifold attack methods, 
  % namely style-atk~\cite{DongzeLi2021ExploringAF} and
  % HGN~\cite{https://doi.org/10.48550/arxiv.2107.01809},
  % GAP~\cite{OmidPoursaeed2017GenerativeAP}
  % and the transferable attacks, namely NI~\cite{JiadongLin2019NesterovAG}, DI ~\cite{CihangXie2018ImprovingTO} and TI ~\cite{YinpengDong2019EvadingDT}. We also compare with SegPGD~\cite{gu2022segpgd}, the SOTA adversarial attack model for semantic segmentation.
% m1:PGD,m2:NI~\cite{JiadongLin2019NesterovAG},m3:HGN~\cite{https://doi.org/10.48550/arxiv.2107.01809},m4:style-atk~\cite{DongzeLi2021ExploringAF},m5:SegPGD~\cite{gu2022segpgd}
  }
  \begin{tabular}{l|c|cc|cc|cc|cc}
  % \hline
  \toprule[1.1pt]
  &&\multicolumn{2}{c|}{Image Quality}&\multicolumn{6}{c}{Target (performance is evaluated with $\text{mIOU}\downarrow$ (left) and $\text{Sr}\uparrow$ (right))}\\ \hline
  Source & Attack & $\text{PSNR}\uparrow$ & $\text{SSIM}\uparrow$ & \multicolumn{2}{c|}{DV3Res50} & \multicolumn{2}{c|}{DV3Res101}& \multicolumn{2}{c}{FCNVGG16}  \\ \hline
  \multirow{9}{*}{DV3Res50}  & FGSM~\cite{Explaining_and_Harnessing_Adversarial_Examples} & 30.18 & 0.5098 & 0.3400 & 0.5480 & 0.5134 & 0.3410 &  0.4628 & 0.2890 \\ 
  & PGD~\cite{PGD_attack} & 39.70 & 0.7814 & 0.0938 & 0.8753 & 0.4232 & 0.4568 &  0.5681 & 0.1273   \\ 
  & SegPGD~\cite{gu2022segpgd} & 40.13 & 0.7897 & 0.1127 & 0.8502 & 0.5659 & 0.2736 &   0.6030 & 0.0740 \\ 
   & DAG~\cite{xie2017adversarial} & \textbf{46.09} & \textbf{0.9340} & 0.1268 & 0.8314 & 0.6271 & 0.1951 & 0.5977 & 0.0819 \\ 
   \cline{2-10}
      &  NI~\cite{JiadongLin2019NesterovAG} & 32.84 & 0.5912 & \textbf{0.0861} & \textbf{0.8855} & \textbf{0.2610}  &\textbf{0.6650} &  \textbf{0.4246} & \textbf{0.3478}  \\ 
  & DI ~\cite{CihangXie2018ImprovingTO} & 39.70 & 0.7793 & 0.0950 & 0.8737 &  0.2929 & 0.6241 &  0.5035 & 0.2266  \\ 
  & TI ~\cite{YinpengDong2019EvadingDT} & 39.26 & 0.7808 & 0.0997 & 0.8675 & 0.3871 & 0.5031 & 0.5193 & 0.2023 \\
    & IAA ~\cite{zhu2022rethinking} & 35.69 & 0.9396 & 0.0823 & 0.8897 & 0.5379 & 0.1302 & 0.5625 & 0.1360  \\
   % \hline
   \midrule[0.7pt]
   \multirow{9}{*}{DV3Res101} & FGSM~\cite{Explaining_and_Harnessing_Adversarial_Examples} & 30.17  & 0.5104 & 0.4959 & 0.3407 & 0.3774 & 0.5156 &  0.4764 & 0.2682 \\ 
   & PGD~\cite{PGD_attack} &  39.64  & 0.7804 & 0.3564 & 0.5262 & 0.1001  & 0.8715 & 0.5673 & 0.1286 \\ 
  & SegPGD~\cite{gu2022segpgd} & 40.09 & 0.7891 & 0.4557 & 0.3941 & 0.1131 & 0.8548 & 0.5985 & 0.0806  \\
   & DAG~\cite{xie2017adversarial} &\textbf{46.41} & \textbf{0.9401} & 0.5710 & 0.2409 & 0.1569 & 0.7986 & 0.6057 & 0.0696  \\ 
   \cline{2-10}
    &  NI~\cite{JiadongLin2019NesterovAG} & 32.83 & 0.5925 & \textbf{0.2244} & \textbf{0.7017} & \textbf{0.0922} & \textbf{0.8817} & \textbf{0.4341}  & \textbf{0.3332 } \\ 
    & DI ~\cite{CihangXie2018ImprovingTO} & 39.70 & 0.7791 & 0.2545 & 0.6617 & 0.0973 & 0.8751 &  0.5029 & 0.2275  \\ 
    & TI ~\cite{YinpengDong2019EvadingDT} & 39.26 & 0.7805 & 0.3543 & 0.5290 & 0.1068  & 0.8629 & 0.5235& 0.1959  \\
  % & GAP~\cite{OmidPoursaeed2017GenerativeAP} & 0. & 0. & 0. & 0. &0.  & 0. &  0.& 0.  \\ 
  % & style-atk~\cite{DongzeLi2021ExploringAF} & 0. & 0. & 0. & 0. &0.  & 0. &  0.& 0.  \\ 
  
    & IAA ~\cite{zhu2022rethinking} & 34.21& 0.9244 & 0.4597 & 0.3481 & 0.1315 & 0.7874  & 0.5690 & 0.1259 \\
% \hline
\midrule[0.7pt]
  \multirow{8}{*}{FCNVGG16} & FGSM~\cite{Explaining_and_Harnessing_Adversarial_Examples} & 30.18 & 0.5099 & 0.3811 & 0.4933 & 0.4641 & 0.4043 &  0.1715 & 0.7366  \\ 
  & PGD~\cite{PGD_attack} & 36.73 & 0.5106  & 0.1989 & 0.7356 &  0.2981  & 0.6174  & \textbf{0.0309} & \textbf{0.9525}  \\ 
  & SegPGD~\cite{gu2022segpgd} & 37.86 & 0.6023 & 0.3346 & 0.5552 &  0.4337 & 0.4433 & 0.0608 & 0.9066  \\
   & DAG~\cite{xie2017adversarial}  &\textbf{42.64} & \textbf{0.9216} & 0.4723 & 0.3721 & 0.5538 & 0.2892 & 0.1378 & 0.7883 \\
    \cline{2-10}
   &  NI~\cite{JiadongLin2019NesterovAG} & 32.75 & 0.6018 & \textbf{0.1607} & \textbf{0.7864} & \textbf{0.2233} & \textbf{0.7134} &  0.0326 & 0.9499  \\ 
    & DI ~\cite{CihangXie2018ImprovingTO} & 37.39 & 0.7492 & 0.1747 & 0.7677 & 0.2536  & 0.6745 &  0.0335 & 0.9485  \\ 
    & TI ~\cite{YinpengDong2019EvadingDT} & 36.35 & 0.7382 & 0.2272 & 0.6384 & 0.3262  & 0.5813 & 0.0320 & 0.9508 \\
% \hline
\bottomrule[1.1pt]
  % \multirow{5}{*}{MPViT(training for voc)} & m1 & 0. & 0. & 0. & 0. &0.  & 0. &  0.& 0. & 0. & 0. \\ 
  % & m2 & 0. & 0. & 0. & 0. &0.  & 0. &  0.& 0. & 0. & 0. \\ 
  % & m3 & 0. & 0. & 0. & 0. &0.  & 0. &  0.& 0. & 0. & 0. \\ 
  % & m4 & 0. & 0. & 0. & 0. &0.  & 0. &  0.& 0. & 0. & 0. \\ 
  % & m5 & 0. & 0. & 0. & 0. &0.  & 0. &  0.& 0. & 0. & 0. \\
  %  \hline
  \end{tabular}
  \label{tab:pascalvoc_adversarial_attack}
\end{table*}

% \noindent\textbf{Ensemble Attack:}\Jing{introduce ensemble attack here}
\section{Ensemble attack}
\label{sec:ensemble_attack}
Given the different transferability of existing transferable attacks for semantic segmentation (see Fig.~\ref{fig:transferability_evaluation}), we intend to aggregate them to extensively explore their potential for transferable attack, which is also named \enquote{ensemble segmentation attack}. Different from~\cite{cai2023ensemblebased}, which introduces an ensemble of surrogate models, where weights of the involved models are optimized based on the victim model, we work on attack-ensemble, and generate attack based on an ensemble of different segmentation attacks. 
% \Jing{analysis \cite{cai2023ensemblebased}}.
% % \Mengqi{Adding the Ensemble analysis here}
% \cite{cai2023ensemblebased} have already done a kind of ensemble attack on segmentation which are try to ensemble multiple models. The key idea is that if an image with perturbation can fool multiple surrogate models at the same time it should perform better when it faces to unseen model. Instead of ensemble surrogate models, we directly ensemble the transfer methods.
% \Mengqi{do we have ensemble attack method on segmentation paper? I didn't find any, but if so we can add citations here}. 
Specifically, given the superiority of NI~\cite{JiadongLin2019NesterovAG},
DI~\cite{CihangXie2018ImprovingTO} and
TI~\cite{YinpengDong2019EvadingDT} for transferable attack with different focus as discussed in Sec.~\ref{analysis_attacks}, we aggregate adversarial attacks from them
% NI~\cite{JiadongLin2019NesterovAG},
% DI~\cite{CihangXie2018ImprovingTO} and
% TI~\cite{YinpengDong2019EvadingDT}, 
achieving ensemble semantic attack.
% , and show performance in Table~\ref{tab:pascalvoc_adversarial_attack_esm} and Table~\ref{tab:cityscape_adversarial_attack_esm} for Pascal VOC and Cityscape dataset, respectively. 
To achieve ensemble attack, we apply DI~\cite{CihangXie2018ImprovingTO} and
TI~\cite{YinpengDong2019EvadingDT} on the data to use more augmentation of the data and to utilize those invariant features. Then, we rely on NI~\cite{JiadongLin2019NesterovAG} to
% , and 3) finally 
incorporate
% also  NI~\cite{JiadongLin2019NesterovAG} to utilize the final  use of 
the accelerated gradient of the above to avoid the attack coming into a local minimal, leading to our ensemble attack as: 
% \Jing{explain how you come up with this equation, not just put the equation directly, you need motivation on how you obtain this equation}, 
% \Jing{be clear how you achieve emsemble attack, with equations} 
% we have:
         % \left( \mathbf{W} \nabla_{\mathbf{x}_{adv}^t} \mathcal{L}(\theta, T(\mathbf{x}_{adv}^t,p), \mathbf{y}) \right))
\begin{equation}
\label{eq_ni_di_ti_adversarial_attack}
    \begin{aligned}
        &\mathbf{x}^{nes}= \mathbf{x}_{adv}^t+\alpha\mu \mathbf{g}^t\\
        &\mathbf{g}^{t+1}=\mu\mathbf{g}^t + \frac{\mathbf{W} * \nabla_{\mathbf{x}^{nes}} \mathcal{L}(\theta, T(\mathbf{x}^{nes},p), \mathbf{y}) }{\left \|  \mathbf{W} * \nabla_{\mathbf{x}^{nes}} \mathcal{L}(\theta, T(\mathbf{x}^{nes},p), \mathbf{y}) \right \|_{1}}\\
        &\mathbf{x}_{adv}^{t+1}=\text{CLIP}( \mathbf{x}_{adv}^t+\alpha \text{sign} ( \mathbf{g}^{t+1} ) ),
    \end{aligned}
\end{equation}
% \Zhaoyuan{check this equation}
where the transformation function $T(\cdot)$ performs image augmentation with transformation probability $p$,
% , \ie~with probability $p$, data augmentation will be applied to $\mathbf{x}_{adv}^t$, and with probability $1-p$, $T(\mathbf{x}_{adv}^t)$ remains $\mathbf{x}_{adv}^t$ ,
the kernel matrix $\mathbf{W}$ is selected as a
% , it can be represented using the
Gaussian kernel, and $\mathbf{g}^t$ is the accumulated gradient,
% at iteration $t$, 
$\mu$ is the decay rate of $\mathbf{g}^t$, $\mathbf{x}_{adv}$ is the Nesterov accelerated result.
% \Zhaoyuan{check this sentence}
% we we try to way to weighted added 3 methods' gradients and also to use chain rules to calculate one single gradient. 
% We found that the latter way can have a better performance.
The experimental results show that the ensemble method performs better than the strongest single method (NI~\cite{JiadongLin2019NesterovAG}), leading to better transferability.
% From this observation, we can conclude that the ensemble method can have better transferability than the single attack.  

% NI~\cite{JiadongLin2019NesterovAG},
% DI~\cite{CihangXie2018ImprovingTO},
% TI~\cite{YinpengDong2019EvadingDT}, and AAI~\cite{zhu2022rethinking}, NI~\cite{JiadongLin2019NesterovAG} 

% \subsection{Robustness Differences of Transformer and CNN}
% \Jing{Typically, transformer and CNN should have different robustness degree with respect to adversarial attack. analyse the different robustness of the two types of neural networks.}

% \cite{fu2022patch} find that the ViT is more vulnerable than the CNN when face with the patch adversarial attack but gets better performance under normal adversarial attack\cite{tang2022robustart}. They considered that it might be most of the transformer-based methods include the data augmentation method.
% \Mengqi{Do we actually need the experiment to compare results here?}
% \Mengqi{DO we need to discuss this}





% In conclusion, the augment-based method, gradient stabilized method, and also surrogate refinement methods are all trying to find the global optimal attack that can transfer across the different model structures.
% \Mengqi{To conclude a transferability common sense}

% term, which directly calculates an indicator that can directly measure the alignment between the conventional attack on $P_\theta,\Lambda(y|x)$ and the intrinsic attack on $P_D(x,y)$ to increase the attack to the low-density area of the data to make the attack more effective. It replaced the structure parameter of the backbone of the network, 

% From ReLU~\cite{agarap2018deep}:
% $ReLU(x)=max(0,x) $
% to SoftPlus~\cite{zheng2015improving}:
% $Softplus_{\gamma  }(x)=\frac{1}{\gamma }log\left (1+exp\left ( \gamma x \right )  \right )$
% where $\gamma$ is the hyperparameter of the Softplus

% From residual connection~\cite{he2016deep}:
% $x_{i+1}=x_i+F(x_i)$
% to parameterized residual connection:
% $x_{i+1}=x_i+\beta * F(x_i)$
% where $\beta$ is the hyperparameter of the parameterized residual connection

% AAI term can be writen as:

% \begin{equation}
%     \begin{aligned}
%      \mathbf{A A I} \triangleq \mathbb{E}_{p_d(x, y)}\left[\frac{\nabla_{x} \log p_{\theta, \lambda}(y \mid x)}{\left\|\nabla_{\boldsymbol{x}} \log p_{\theta, \lambda}(y \mid x)\right\|_2} \cdot \nabla_{x} \log p_d(x, y)\right]
%     \end{aligned}
% \end{equation}
% where $\nabla_{x} \log p_d(x, y)$is the intrinsic
% attack using the joint distribution of the input x and label y.


% after using the use integration by parts and combining with the sliced score matching~\cite{https://doi.org/10.48550/arxiv.1905.07088}, they have:
% \begin{equation}
%     \begin{aligned}
%      \mathbf{A A I} =-\mathbb{E}_{p_d(x, y)} \mathbb{E}_{p(\boldsymbol{v})}\left[\boldsymbol{v}^{\mathrm{T}} \nabla_{x} \frac{\boldsymbol{v}^{\mathrm{T}} \nabla_{x} p_{\theta, \lambda}(y \mid x)}{\left\|\nabla_{x} p_{\theta, \lambda}(y \mid x)\right\|_2}\right]
%     \end{aligned}
% \end{equation}
% where(TODO)

% The gradient descent step of this method is:

% \begin{equation}
% \begin{aligned} 

% x^{a d v_{t+1}}=\phi^\epsilon\left(x^{a d v_t}+
% \alpha * \operatorname{sign}\left(\nabla_{x^{a d v_t}} \mathcal{L}\left({\theta,\lambda },\left(x^{a d v_t}\right), y\right)\right)\right)

% \end{aligned}
% \end{equation}
% where $\theta$ is the parameter for the pre-trained segmentation network, and $\lambda$ is the hyperparameter for refining the structure of the segmentation model.


% \subsection{Adversarial Attack} 





%  C and W~\cite{DBLP:journals/corr/CarliniW16a} try to solve the equation of attack instead of formulating the problem as
% \begin{align*}
%   \text{minimize } \;& \mathcal{D}(x, x+\delta)\\
%   \text{such that } \;& f(x+\delta) \le 0\\
%     & x+\delta \in [0,1]^n
% \end{align*}
% they use the alternative formulation:
% \begin{align*}
%   \text{minimize } \;& \mathcal{D}(x, x+\delta) + c \cdot f(x+\delta)\\
%   \text{such that } \;& x+\delta \in [0,1]^n
% \end{align*}
% where $c>0$ is a suitably chosen constant.

% As for the f,they tried:
% \begin{align*}
% f_{1}(x') &= -loss_{F,t}(x') + 1\\
% f_{2}(x') &= (\max_{i \ne t}( F(x')_i) - F(x')_t)^+\\
% f_{3}(x') &= \text{softplus}(\max_{i \ne t}(F(x')_i) - F(x')_t)-\log(2)\\
% f_{4}(x') &= (0.5 - F(x')_t)^+\\
% f_{5}(x') &= -\log(2 F(x')_t - 2)\\ 
% f_{6}(x') &= (\max_{i \ne t}( Z(x')_i) - Z(x')_t)^+\\
% f_{7}(x') &= \text{softplus}(\max_{i \ne t}(Z(x')_i) - Z(x')_t)-\log(2)
% \end{align*}

% where $s$ is the correct classification, $(e)^+$ is short-hand for
% $\max(e,0)$, $\text{softplus}(x) = \log(1+\exp(x))$, and
% $loss_{F,s}(x)$ is the cross entropy loss for $x$.


% \subsection{Universal Adversarial Attack}
% Universal attack is image-agnostic~\cite{moosavi2017universal,hendrik2017universal}, which is  a small image perturbation that fools
% a deep neural network classifier to produce wrong predictions for almost all testing images. Specifically, for a trained network with parameters $\theta$, the goal of the universal attack is to obtain a universal perturbation $\delta_{adv}$, thus the output of attacked image is different from the raw image for most of the cases. Or more formally, we have:
% \begin{equation}
%     f_\theta(\mathbf{x}+\delta_{adv})\neq f_\theta(\mathbf{x}) \quad \text{for} \quad \text{most} \quad \mathbf{x}\in D,
% \end{equation}
% where $D$ is the training dataset, $f_\theta(\cdot)$ is the model output.

% (do we actually need to constrain here?)

% \subsection{Targeted Adversarial Attack}
% Generally, the adversarial attack is non-targeted, where the model is fooled to output a prediction of the attacked image to be different from the  clean image. Targeted attack~\cite{adversarial_example_semantic_seg_iclr2017workshop,fischer2017adversarial} is a more fine-grained attack, where the attack is designed to fool the model, thus the model can classify the image as a specific target class, which can be formalized as:
% \begin{equation}
%     f_\theta(x+\delta_{adv}) = T, \quad\text{and} \quad T\neq C,
% \end{equation}
% where $T$ is the target class for the adversarial example $x+\mu$, and $C$ is the original class of $x$.






% \subsection{Manifold Adversarial Attack}
% The conventional attack is designed in sample space, where the invisible noise (adversarial attack $\delta_{adv}$) is directly added to the raw image $x$. Differently, manifold attack~\cite{manifold_attack,manifold_view_attack_AISTATS2022,li2021exploring, ZHANG2022102770,DongzeLi2021ExploringAF,tran2020manifold,zhang2022manifold} is the attack in feature space. 


% \cite{DongzeLi2021ExploringAF} use the style-GAN as the backbone and do the gradient descent on the latent space of the generative model to do a manifold attack for the forensic task. \cite{xiao2022understanding} use the conditional generative models with PGD and FGSM to generate on-manifold adversarial samples. And they also try to  restrict the attack in eigenspace and show a powerful on-manifold attack rate. \cite{zhang2022manifold} uses adversarial autoencoders to learn the low-dimensional manifold of data and then use a latent substitute model to generate manifold adversarial samples, while it just compares with the conventional adversarial attack.

% \subsection{Transferable Adversarial Attack}

% Typically, the adversarial attack is designed to attack one specific model. Recently, transferable adversarial attack~\cite{zhu2022rethinking} has been considered for safe deployment of the deep models, where the attack generated from the source model is required to be able to attack the target model as well. More formally, we would like to design the attack $\delta_{adv}$ from the model with parameters $\theta$ to serve as an adversarial attack for the target model with parameters $\beta$.

% \noindent{Gradient stabilization} Different DNN architectures will have different decision boundaries since they have high non-linearity, hence, it will be the case that the attack gradient calculated on one model might be locally minimal, which will result in low transferability on another model. Some gradient stable methods are adopted to solve this problem such as using the momentum ~\cite{YinpengDong2017BoostingAA}, Nesterov Accelerated Gradient ~\cite{JiadongLin2019NesterovAG}, enhanced momentum ~\cite{XiaosenWang2021BoostingAT}.

% \noindent{Input augmentation} As a common tech to improve the generalizability of the model, data augmentation is also used to improve the transferability of the attack. Some methods force the attacks to be invariant to the certain transformation, such as geometric transformations ~\cite{CihangXie2018ImprovingTO, YinpengDong2019EvadingDT}, pixels scaling ~\cite{JiadongLin2019NesterovAG}, noise add ~\cite{XiaosenWang2021EnhancingTT}, mix images ~\cite{XiaosenWang2021AdmixET}

% \noindent{Feature disruption} Since output level information is more model-specific, intermediate layers output features are more generic \cite{SimonKornblith2019SimilarityON}. Hence, some of the methods try to disrupt these features, which is trying to modify the input to push away features from the original one. Such as ~\cite{AdityaGaneshan2019FDAFD,huang2019enhancing,QizhangLi2020YetAI,ZhuoranLiu2019WhosAO,MuzammalNaseer2018TaskgeneralizableAA,ZhouWen2018TransferableAP} on all features,~\cite{ZhiboWang2021FeatureIT,WeibinWu2020BoostingTT,JianpingZhang2022ImprovingAT} on specific features according to model interpretability techniques ~\cite{RamprasaathRSelvaraju2016GradCAMVE,MukundSundararajan2017AxiomaticAF}, and ~\cite{InkawhichNathan2019FeatureSP} on the targeted attack.

% \noindent{Surrogate model refinement} This method tries to refine the surrogate model, such as activation layer~\cite{zhu2022rethinking,zhang2021backpropagating, YiwenGuo2020BackpropagatingLI}, training process~\cite{springer2021little, DingchengYang2022BoostingTA, JacobMSpringer2021ALR}, and architecture ~\cite{DongxianWu2020SkipCM, YaoZhu2022RETHINKINGAT}, to increase the attack transferability.


% \noindent{Generative modeling} These methods use a generative model to generate transferable adversarial perturbations since it can input any and output an adversarial example with only one forward pass, such as ~\cite{OmidPoursaeed2017GenerativeAP,naseer2019cross, KrishnaKanthNakka2021LearningTA,QilongZhang2022BeyondIA,MuzammalNaseer2021OnGT,XiaoYang2021BoostingTO},.


% \subsection{Adversarial Attack for Semantic Segmentation}

% (to make a table)~\cite{arnab2018robustness} explore the adversarial robustness of the segmentation model using the FGSM and PGD, and they found that the transferability of the perturbations is poor since while using different scales.
% DAG(Dense Adversary Generation) ~\cite{xie2017adversarial} try to let the network misclassify each of the pixels towards another label than the ground truth. And they found that the perturbations are not transferred in a good performance across different networks. And they thought the segmentation models were robust toward those black box attacks. 
% ~\cite{gu2021adversarial} use the dynamic scales to improve the transferability of the untargeted  attack on different models, which shows the effectiveness. (the only one have high transfer ability)
% ~\cite{hendrik2017universal,fischer2017adversarial} generate universal perturbations for the semantic segmentations and also targeted attack on the pedestrian.
% ~\cite{xiao2018characterizing} use spatial consistency check and scale consistency check to defend the attacks on the semantic segmentation.
% ~\cite{li2021hidden} proposed a  fine-grained backdoor attack that applied to the segmentation.
% ~\cite{klingner2020improved}use the density estimation as a self-supervised signal to improve the robustness of the segmentation model. But they found that under this setting, the PGD can easily attack them with a high perturbation.
% ~\cite{xu2021dynamic} To deal with that, an advertised training method for segmentation is proposed. However, this method can still be easily influenced by PGD with a large number of attack iterations.
% To deal with the large iterations of PGD, ~\cite{gu2022segpgd} proposed a segmentation-based PGD method called SegPGD which is much more effective than the PGD. And at the same time, they release the SegPGD-based advertised training which reaches to the SOTA on the benchmark. 

% % \section{Related Work}


% % \noindent\textbf{Transferable adversarial attack:}



% % % \noindent\textbf{Targeted attack:}

% % % ~\cite{liu2016delving,dong2018boosting} found that while non-targeted adversarial examples are easy to find, however, targeted adversarial examples are much hard to transfer with the target labels. Hence they give the ensemble methods
% % % to generate transferable adversarial examples for targeted attacks.
% % % However, ensemble methods are not effective, then ~\cite{Li_2020_CVPR} proposed a method that is based on the triplet loss and Poincare space to deal with the targeted attack.
% % % ~\cite{Naseer_2021_ICCV} use the generative model to generate targeted perturbations.
% % % While this method put the performance of the targeted attack into a higher stage, the use of the large-scale model to train and additional data makes it seem as resource intensive method. 
% % % Hence,~\cite{ZhengyuZhao2020OnSA} proposed a method that uses the logits loss instead of the cross entropy, and can reach similar performance as the previous resource-intensive method.
% % % After that, this method using the logit does not have good transferability although it does have a good performance on its source model.Hence, ~\cite{https://doi.org/10.48550/arxiv.2107.01809}
% % %  proposed a method that has better transferability on the targeted attack, which uses the conditional generative model to generate a universal adversarial function.


% % \noindent\textbf{Manifold attack:}

 

% % \noindent\textbf{Adversarial attack for semantic segmentation:}(to make a table)~\cite{arnab2018robustness} explore the adversarial robustness of the segmentation model using the FGSM and PGD, and they found that the transferability of the perturbations is poor since while using different scales.
% % DAG(Dense Adversary Generation) ~\cite{xie2017adversarial} try to let the network misclassify each of the pixels towards another label than the ground truth. And they found that the perturbations are not transferred in a good performance across different networks. And they thought the segmentation models were robust toward those black box attacks. 
% % ~\cite{gu2021adversarial} use the dynamic scales to improve the transferability of the untargeted  attack on different models, which shows the effectiveness. (the only one have high transfer ability)
% % ~\cite{hendrik2017universal,fischer2017adversarial} generate universal perturbations for the semantic segmentations and also targeted attack on the pedestrian.
% % ~\cite{xiao2018characterizing} use spatial consistency check and scale consistency check to defend the attacks on the semantic segmentation.
% % ~\cite{li2021hidden} proposed a  fine-grained backdoor attack that applied to the segmentation.
% % ~\cite{klingner2020improved}use the density estimation as a self-supervised signal to improve the robustness of the segmentation model. But they found that under this setting, the PGD can easily attack them with a high perturbation.
% % ~\cite{xu2021dynamic} To deal with that, an advertised training method for segmentation is proposed. However, this method can still be easily influenced by PGD with a large number of attack iterations.
% % To deal with the large iterations of PGD, ~\cite{gu2022segpgd} proposed a segmentation-based PGD method called SegPGD which is much more effective than the PGD. And at the same time, they release the SegPGD-based advertised training which reaches to the SOTA on the benchmark. 


% % \section{Transferable Targeted Attack via Manifold Learning}

% % \section{Related Work}
% % \noindent\textbf{Adversarial attack and defense:}

% % \noindent\textbf{Manifold attack:}

% % Exploring Adversarial Fake Images on Face Manifold ~\cite{DongzeLi2021ExploringAF} 

% % main motivation



% % They try to do a manifold attack instead of a sample attack to fool forensic models(which use to deal with the deepfake).


% % solutions

% % (1) they use the style-GAN as the backbone and do the gradient descent on the latent space of the generative model to do a manifold attack. 
% % For the style-GAN, they do gradient decent on the latent vector z and also the noise input n(it can be seen as another latent parameter in style-GAN)

% % (to parahrase:
% % $zt+1 = zt + 1  sign(zt (J(g(zt, nt), y))),$

% % $nt+1 = nt + 2  sign(nt (J(g(zt, nt), y))).$
% % )
% % (2) general attack objective function

% % (3) bad transferability, even worse than the PGD and the FGSM

% % (4) manifold attack


% % limitation

% % low transferability

% % Manifold attack ~\cite{tran2020manifold} 

% % On-Manifold Adversarial Attack Based on Latent Space Substitute Model ~\cite{zhang2022manifold}

% % \noindent\textbf{Target attack:}

% % Improving the Transferability of Targeted Adversarial Examples Through Object-Based Diverse Input ~\cite{Byun_2022_CVPR} 

% % The object-based diverse input (ODI) method draws an adversarial image on a 3D object and then tries to let the result be classified as the target class.

% % motivation

% % (to paraphrase:e humans' superior perception of an image printed on a 3D object. If the image is clear enough, humans can recognize the image content in a variety of viewing conditions. Likewise, if an adversarial example looks like the target class to the model, the model should also classify the rendered image of the 3D object as the target class.)

% % It can be seen as a kind of input pre-process method which are trying to do the augmentation

% % solution

% % (1)
% % (2)
% % (3)
% % (4)

% % limitation

% % Boosting Transferability of Targeted Adversarial Examples via Hierarchical Generative Networks ~\cite{https://doi.org/10.48550/arxiv.2107.01809}

% % Motivation


% % Solution:

% % (1)
% % ( to paraphrase: proposed generative method is simple yet practical to obtain superior performance of targeted black box attacks, meanwhile with two technical improvements including (i) smooth projection mechanism that better helps the generator probe targeted semantic knowledge from the classifier; (ii) adaptive Gaussian smoothing with the focus of making generated results obtain adaptive ability against adversarially trained models. )
% % (2)
% % (3)
% % (4)

% % Boosting Black-Box Attack With Partially Transferred Conditional Adversarial Distribution ~\cite{Feng_2022_CVPR}

% % Motivation

% % (to paraphrase:
% % architectures and training datasets between surrogate and target models as surrogate biases. I
% % transferring partial parameters of the conditional adversarial distribution (CAD) of surrogate models, while learning the untransferred parameters based on queries to the target model, to keep the flexibility to adjust the CAD of the target model on any new benign sample )

% % Solution:

% % (conditional adversarial distribution (CAD) (i.e., the distribution of adversarial perturbations conditioned on benign examples))

% % (adopt the conditional generative flow model, called c-Glow ~\cite{YouLu2020StructuredOL}, whose general idea is invertible mapping a simple distribution (e.g., Gaussian distribution) to a complex distribution through an invertible network, as shown in Fig. 1(a). c-Glow has shown the powerful ability to capture complex data distributions)

% % Stochastic Variance Reduced Ensemble Adversarial Attack for Boosting the Adversarial Transferability ~\cite{Xiong_2022_CVPR}

% % Motivation

% % (to paraphrase: ensemble attacks are rather less investigated, and existing ensemble attacks simply fuse the outputs of all the models evenly)

% % Solution

% % (1)
% % do Stochastic variance reduction by choosing one of the models for m iteration and do an adversarial attack.
% % Have both inner attack and outer attack.


% % (2)
% % (3)
% % (4)sample attack 

% % Limitation

% % Towards Transferable Targeted Attack ~\cite{Li_2020_CVPR} 2020 CVPR

% % Motivation

% % (to paraphrase: 1. In a targeted attack, when iteration goes on, the gradient is tending to vanish. 
% % 2. Cosine similarity is high in the last few iterations due to the accumulation of momentum.?? (Noise curving problem: the magnitude of the gradient is decreasing during an iterative attack, causing excessive consistency between two successive noises in accumulation of momentum)
% % 3. traditional methods only focus on maximizing the probability of the targeted class and ignore whether the adversarial examples are close to the original labels. Although these methods work well in white-box setting, the targeted adversarial examples are hard to separate from the corresponding true class)

% % Solution

% % (1)
% % 1. Poincare Distance Metric
% % 2. Triplet loss

% % (2)

% % (3)

% % (4) use manifold as the gradient guide, but attack use the FGSM in the end,is that a manifold attack?

% % Limitation

% % On Success and Simplicity: A Second Look at Transferable Targeted Attacks \cite{ZhengyuZhao2020OnSA} NeuralPS 2021

% % Motivation

% % Current SOTA which try to realize transferability are usually resource-intensive since they need a lot of data.
% % They want to find a way that can use neither additional model training nor additional data to realize the high transferability. They believe that the (few iterations have largely limited the attack convergence to optimal targeted transferability)

% % Solution

% % (1)

% % they use a logit loss to replace the CE loss.


% % (2)

% % (3)

% % (4)

% % Limitation

% % although this method have less resource on training, but to much iterations actually influence the inference stage time.


% % \noindent\textbf{Transferable attack(initial by ~\cite{zhu2022rethinking}):}

% % \Mengqi{
% % \begin{itemize}
% %     \item main motivation of each related method;
% %     \item their solutions, including: 1) their formulations; 2) objective functions. 3) how they achieve transferable adversarial attack? 4) sample or manifold attack?
% %     \item their limitations.
% % \end{itemize}
% % }

% % 1 Transferable adversarial examples based on global smooth perturbations Elsevier 2022~\cite{liu2022transferable}

% % They proposed a method that uses a GMM as parameterized smooth function and the C\&W attack as the attack method to increase the transferability and the imperceptibility of the attack.

% % 2 Towards Good Practices in Evaluating Transfer Adversarial Attacks arxiv 2022 ~\cite{https://doi.org/10.48550/arxiv.2211.09565}

% % \Jing{focus on this paper!}

% % They do an evaluation of five categories of transfer adversarial attack methods on transferability and imperceptibility.

% % \noindent\textbf{Gradient stabilization} Different DNN architectures will have different decision boundaries since they have high non-linearity, hence, it will be the case that the attack gradient calculated on one model might be locally minimal, which will result in low transferability on another model. Some gradient stable methods are adopted to solve this problem such as using the momentum ~\cite{YinpengDong2017BoostingAA}, Nesterov Accelerated Gradient ~\cite{JiadongLin2019NesterovAG}, enhanced momentum ~\cite{XiaosenWang2021BoostingAT}.

% % \noindent\textbf{Input augmentation} As a common tech to improve the generalizability of the model, data augmentation is also used to improve the transferability of the attack. Some methods force the attacks to be invariant to the certain transformation, such as geometric transformations ~\cite{CihangXie2018ImprovingTO, YinpengDong2019EvadingDT}, pixels scaling ~\cite{JiadongLin2019NesterovAG}, noise add ~\cite{XiaosenWang2021EnhancingTT}, mix images ~\cite{XiaosenWang2021AdmixET}

% % \noindent\textbf{Feature disruption} Since output level information is more model-specific, intermediate layers output features are more generic \cite{SimonKornblith2019SimilarityON}. Hence, some of the methods try to disrupt these features, which is trying to modify the input to push away features from the original one. Such as ~\cite{AdityaGaneshan2019FDAFD,huang2019enhancing,QizhangLi2020YetAI,ZhuoranLiu2019WhosAO,MuzammalNaseer2018TaskgeneralizableAA,ZhouWen2018TransferableAP} on all features,~\cite{ZhiboWang2021FeatureIT,WeibinWu2020BoostingTT,JianpingZhang2022ImprovingAT} on specific features according to model interpretability techniques ~\cite{RamprasaathRSelvaraju2016GradCAMVE,MukundSundararajan2017AxiomaticAF}, and ~\cite{InkawhichNathan2019FeatureSP} on the targeted attack.

% % \noindent\textbf{Surrogate model refinement} This method tries to refine the surrogate model, such as activation layer~\cite{zhu2022rethinking,zhang2021backpropagating, YiwenGuo2020BackpropagatingLI}, training process~\cite{zhang2021early, DingchengYang2022BoostingTA,JacobMSpringer2021ALR}, and architecture ~\cite{DongxianWu2020SkipCM,YaoZhu2022RETHINKINGAT}, to increase the attack transferability.


% % \noindent\textbf{Generative modeling} These methods use a generative model to generate transferable adversarial perturbations since it can input any and outputs an adversarial example with only one forward pass, such as ~\cite{OmidPoursaeed2017GenerativeAP,naseer2019cross, KrishnaKanthNakka2021LearningTA,QilongZhang2022BeyondIA,MuzammalNaseer2021OnGT,XiaoYang2021BoostingTO},.

% % defense method:

% % \noindent\textbf{input pre-processing}

% % Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks ~\cite{WeilinXu2017FeatureSD}
% % Deflecting Adversarial Attacks with Pixel Deflection ~\cite{AadityaPrakash2018DeflectingAA}
% % Mitigating Adversarial Effects Through Randomization ~\cite{CihangXie2017MitigatingAE}

% % \noindent\textbf{Purification Network}

% % Defense against Adversarial Attacks Using High-Level Representation Guided Denoiser ~\cite{FangzhouLiao2017DefenseAA} 

% % A Self-supervised Approach for Adversarial Robustness ~\cite{MuzammalNaseer2020ASA}

% % Diffusion Models for Adversarial Purification ~\cite{ChaoweiXiao2022DensePureUD}

% % \noindent\textbf{Adversarial Training}

% % Feature Denoising for Improving Adversarial Robustness ~\cite{CihangXie2018FeatureDF}

% % Do Adversarially Robust ImageNet Models Transfer Better ~\cite{HadiSalman2020DoAR}


% % Conclusion

% % For Transferability, from an attack perspective, new model will boost the performance of the attack significantly. 
% % From the defense perspective, adversarial training is effective while the input pre-processing is not. And some of the defense methods are overfitted to specific kinds of attack.

% % For the Stealthiness measure by five perceptual metrics, they conclude that all existing attacks have a trade-off with imperceptibility to get transferability.

% % Using the Frechet Inception Distance (FID), generative models are most perceptible.

% % 3 Transferability Ranking of Adversarial Examples arxiv 2022
% % ~\cite{https://doi.org/10.48550/arxiv.2208.10878}

% % They proposed a new term that is the expectation of transfer ability and a metric that evaluates the ET and then uses the transferability ranking to increase the performance of the black box attack

% % 4 Boosting Out-of-distribution Detection with Typical Features arxiv 2022

% % 5 An Intermediate-level Attack Framework on The Basis of Linear Regression arxiv 2022
% % ~\cite{https://doi.org/10.48550/arxiv.2203.10723}

% % By establishing a mapping from the intermediate feature to prediction loss using linear regression models, they try to maximize the  intermediate-level feature discrepancies to increase the transferability of the attack.

% % 6 MaskBlock: Transferable Adversarial Examples with Bayes Approach arxiv 2022~\cite{fan2022maskblock}

% % They approximate the adversarial attack into a Bayes posterior inference problem using the block they called MaskBlock, and then tried this block with some attack methods to show the efficiency of this block. (Not quite clear on the block, need to further check)

% % 7 Backpropagation Path Search On Adversarial Transferability ICLR 2023 withdraw

% % They proposed a backpropagation method and a history track method to find the most transferable way to increase the transferability of the attack methods

% % 8 Yet Another Intermediate-Level Attack ~\cite{https://doi.org/10.48550/arxiv.2008.08847}



% % \noindent\textbf{Intermediate attack:}
% % Enhancing Adversarial Example Transferability with an Intermediate Level Attack ~\cite{https://doi.org/10.48550/arxiv.1907.10823}

% % Feature Space Perturbations Yield More Transferable Adversarial Examples ~\cite{8953700}

% % Transferable Adversarial Perturbations ~\cite{zhou2018transferable}


% % \section{Papers to carefully read}
% % \cite{zhu2022rethinking,https://doi.org/10.48550/arxiv.2211.09565,springer2021little}


% % RETHINKING ADVERSARIAL TRANSFERABILITY FROM A DATA DISTRIBUTION PERSPECTIVE ~\cite{zhu2022rethinking}

% % main motivation

% % ( to paraphrase:the attack success rate against different target models of LDD with different strengths of Gaussian noise is much higher than that of HDD. Furthermore,  the adversarial counterparts of LDD have much stronger transferability than the adversarial counterparts of HDD) it reveals that the location of data plays a vital role in adversarial transferability and the adversarial examples of samples in the low-density region are strongly transferable. The most efficient direction towards the low-density region is x log p D(x, y), where p D(x, y) is the ground truth density of natural data. We name this direction Intrinsic Attack because it doesnt depend on the models and only depends on the ground truth distribution. Thus, we propose to match the adversarial attack with the intrinsic attack for generating strong transferable adversarial examples.)

% % solution

% % (1)
% % they tried to match the alignment between the adversarial attack and intrinsic attack and they called it AAI, using the sliced score matching~\cite{https://doi.org/10.48550/arxiv.1905.07088}

% % For the method, they use the AAI as an indicator to choose the hyperparameter of the activation function(Softplus~\cite{}) and the  residual module. Then they use the network after using the hyperparameter to get the adversarial samples.

% % (2) they tried to get the highest AAI

% % (3) It tries to match the normal attack with the intrinsic attack to have the transferability

% % (4)this attack still happened on the sample space since in the end it still uses the PGD as the perturbation method.

% % limitation

% % In openreview,they said that AAI may not be the best metric to choose the structural hyper-parameters.

% % A Little Robustness Goes a Long Way: Leveraging Robust Features for Targeted Transfer Attacks \cite{springer2021little}

% % main motivation

% % Prior works try to find a method that can generate successful image perturbations, however, in this work, they try to improve the robustness of the source network to improve the transferability of the adversarial attack.

% % solution

% % (1) AT training with PGD to construct a robust source network and then use the TMDI-FGSM(combine the DI-FGSM MI-FGSM TI-FGSM ) to generate 1000 adversarial samples. And for each of the 1000 adversarial samples, they do the TMDI-FGSM for 300 iterations.

% % (2) They define transferability as :
% % Let $(x, y)  X  Y$ be an (unperturbed) input-label pair, where $X$ is the input space and $Y$ is the label space. Given a maximum perturbation size $$, we construct an adversarial example $x + $ where $  $, such that $F (x + ) 6 = y$ for the untargeted case, and $F (x + ) = t$ for some target class $t  Y$ for the targeted case. We then say that $x + $ is transferable to black-box network $G$ if $G(x + ) 6 = y$ for the untargeted case and $G(x + ) = t$ for the targeted case.

% % (3) (to paraphrase: In addition, we present an argument that, for a given task, there are features that are useful to every tested neural network, and that these features can be learned with small- adversarial training, even when the source network architecture and learning objective are dissimilar to those of the destination network. Thus, by studying the features of a single slightly-robust network, we can empirically discover properties that will be applicable across all non-robust networks.)

% % (4) sample space attack


% % limitation

% % (To paraphrase: Firstly, in principle, an adversary could use our technique to attack existing systems. Secondly, we advocate for the use of adversarial training, which can be computationally intensive and could lead to excessive energy consumption.)

% % Diffusion Models for Adversarial Purification ~\cite{WeiliNie2022DiffusionMF}

% % main motivation

% % Although adversarial purification can deal with adversarial samples without re-training the classifiers since the model is trained independently from the threat model and classifiers. however, the performance of this kind of method is usually lower performance than the adversarial training methods. The diffusion model, diffusion models purify noise in the generative stage, which can is similar to the purification model. The generation quality of the diffusion model is quite good, which makes the purified images distribute close to the clean data. Hence, the author chooses the diffusion model to do the defense task.

% % solution


% % (1)this method has two steps: (to paraphrase :(i) we first add noise to adversarial examples by following the forward process with a small diffusion timestep, and (ii) we then solve the reverse stochastic differential equation (SDE) to recover clean images from the diffused adversarial examples)



% % (2) this 

% % (3) not transferable

% % (4) sample

% % limitation
% % (to paraphrase  two major limitations: (i) the purification process takes much time (proportional to the diffusion timestep, see Appendix C.6), making our method inapplicable to real-time tasks, and (ii) diffusion models are sensitive to image colors, making our method incapable of defending color-related corruptions.)

% % DENSEPURE: UNDERSTANDING DIFFUSION MODELS TOWARDS ADVERSARIAL ROBUSTNESS \cite{ChaoweiXiao2022DensePureUD}


% % main motivation

% % Diffpure~\cite{WeiliNie2022DiffusionMF} (to paraphrase: empirically show that by carefully choosing the amount of Gaussian noises added during the diffusion process, adversarial perturbations can be removed while preserving the true label semantics.)

% % ~ \cite{NicholasCarlini2022CertifiedAR}(to paraphrase: instantiate the randomized smoothing approach with the diffusion model to offer a provable guarantee of model robustness against L2-norm bounded adversarial example. However, they do not provide a theoretical understanding of why and how the diffusion models contribute to such nontrivial certified robustness.)

% % (to paraphrase: is inspired by our theoretical analysis, where we show that the diffusion model reverse process provides a conditional distribution of the reversed sample given an adversarial input, and sampling from this conditional distribution enhances the certified robustness. Specifically, we prove that when the data density of clean samples is high, it is a sufficient condition for the conditional density of the reversed samples to be also high. Therefore, in DensePure, samples from the conditional distribution can recover the ground-truth labels with a high probability)



% % solution

% % (to paraphrase:(i) using the reverse process of the diffusion model to obtain a sample of the posterior data distribution conditioned on the adversarial input; and (ii) repeating the reverse process multiple times with different random seeds to approximate the label of the high-density region in the conditional distribution via a majority vote. In particular, given an adversarial input, we repeatedly feed it into the reverse process of the diffusion model to get multiple reversed examples and feed them into the classifier to get their labels. We then apply the majority vote on the set of labels to get the final predicted label.)



% % limitation

\section{Experiments}
We investigate adversarial attack for semantic segmentation and explain their transferability. As there exists no benchmark setting, we re-implement all the related models in this section with the same setting for fair comparison.
% As the first method of the transferable targeted attack for semantic segmentation, we implement the existing image-classification-based techniques to our dense prediction task. We will first introduce the experimental setting, including the dataset, the metrics for performance evaluation, and the benchmark models based on our implementation of existing techniques for our semantic segmentation task.


\subsection{Setting}
\noindent\textbf{Dataset:}
We train our models on the augmented Pascal VOC 2012~\cite{pascal-voc-2012}
% augmentation version training 
dataset, which contains 21 classes with 10,582 images. 
During training, we also perform on-line data-augmentation by applying a random scaling with ratio in the range of $[0.5, 2]$,
% between 0.5 and 2, 
a random crop with the size of 513x513, and a random horizontal flip. 
As semantic segmentation models can not generalize to datasets with different categories, we also train our models on the Cityscape~\cite{Cordts2016Cityscapes} training dataset, which contains 3,475 images with 19 classes.
During the training stage with Cityscape dataset, 
we apply random crop with size of 768x768, color jitter, and random horizontal flip for data augmentation. 
% fine annotated images and 
We evaluate the related models on their corresponding testing datasets of size 1,449 and 1,525 for Pascal VOC 2012~\cite{pascal-voc-2012} and Cityscape~\cite{Cordts2016Cityscapes}, respectively, where the spatial size of the former is 513x513 for testing, and we use the raw testing images without performing spatial scaling for the latter, following the conventional setting.


% During the training process of PascalVOC, 
% For PascalVOC 2012~\cite{pascal-voc-2012}, we applied a random scaling with ratio in the range of $[0.5,2]$,
% % between 0.5 and 2, 
% a random crop with the size of 513, and a random horizontal flip to further augment PascalVOC 2012~\cite{pascal-voc-2012}. 
% For Pascal VOC 2012~\cite{pascal-voc-2012}, the testing images are resized to 513x513 for performance evaluation. For Cityscape~\cite{Cordts2016Cityscapes},
% the raw testing images are fed directly to the network for performance evaluation.
% In the whole training and testing process, for each dataset, we use the same normalizing strategy for all the related models.
% data using the proper mean and standard deviation is important before the attack process. The non-standardized normalizing process will influence the model cross-test  stage and also the attack transferability performance dramatically
% \Mengqi{In the whole process, normalizing data using the proper mean and standard deviation is important before the attack process. The non-standardized normalizing process will influence the model cross-test  stage and also the attack transferability performance dramatically. }

% As for the evaluation process of PascalVOC, 
% we resize images to 513x513 and then do the center crop with the same size to input them in the model after normalization.
% 
% During the training stage of the cityscapes, 
% we applied random crop with the size of 768x768, color jitter,and random horizontal flip for data augmentation. The raw testing images are feed directly to the network for performance evaluation.
% before normalizing them.
% For the evaluation process of the Cityscapes,we directly input them into the model after normalization.





\noindent\textbf{Measures:}
% Following conventional practice, w
We evaluate model accuracy for semantic segmentation with mean IoU ($\text{mIoU}$).
% to measure the performance of the semantic segmentation models. 
% It can be calculated by: $ Mean IoU = \frac{TP}{(FP+FN+TP)}$, where the TP is true positive, FP is false positive, and FN is false negative.
To model the transferability of adversarial attacks, we first use Peak Signal-to-Noise Ratio ($\text{PSNR}$) and
Structural Similarity Index Measure (SSIM)~\cite{wang2004image}
% Perceptual color distance ($\text{PerC}$)~\cite{zhao2020towards} 
to measure the noisy degree of the adversarial example, as the adversarial examples should be $L_p$-normed, indicating a visually invisible perturbation. Then, we adopt success rate ($\text{Sr}$) to measure the effectiveness of the adversarial attack, which is defined as the proportion of successfully destroyed model prediction as: $S_r = 1 - \frac{\text{mIoU}_\text{adv}}{\text{mIoU}_\text{clean}}$, where $\text{mIoU}_\text{adv}$ and $\text{mIoU}_\text{clean}$ are the segmentation performance of the adversarial example $\mathbf{x}_\text{adv}$ and the clean image $\mathbf{x}$, respectively.

% \noindent\textbf{Peak Signal-to-Noise Ratio (PSNR)}
% PSNR of one frame is defined as:
% \begin{equation}
% \textup{PSNR}=10\log_{10}\left(\frac{r^2}{\textup{MSE}}\right)
% \end{equation}
% where $r$ represents the maximum range of color value, which is usually 255, and the mean squared error (MSE) is defined as:
% \begin{equation}
% \textup{MSE}=\frac{1}{N}\sum_{i=1}^N({I_x}_i-{I_y}_i)^2
% \end{equation}
% where $N$ denotes the total number of pixels in an image or a frame, $I_x$ and $I_y$ are the ground truth HR frame and the SR recovered frame, respectively. A higher value of PSNR generally means superior quality. 

% \noindent\textbf{Perceptual color distance ($\mathcal{\delta}$ E) ~\cite{zhao2020towards}}(TODO:FIX)

% It is a metric used for measuring the perceptual color distance between two pixels.


% \begin{equation}
%     \begin{aligned}
% E =\sqrt{\left(\frac{\Delta L^{\prime}}{k_L S_L}\right)^2+\left(\frac{\Delta C^{\prime}}{k_C S_C}\right)^2+
% \left(\frac{\Delta H^{\prime}}{k_H S_H}\right)^2+R_T\left(\frac{\Delta C^{\prime}}{k_C S_C}\right)\left(\frac{\Delta H^{\prime}}{k_H S_H}\right)}
%     \end{aligned}
% \end{equation}
% where(TODO)

% Image level perceptual color difference is obtained by computing the L2 norm of the pcd for each pixel
% \noindent\textbf{Structural Similarity Index Measure (SSIM)~\c}
%  ~\cite{wang2004image}

% SSIM is a measurement that assesses the similarity of two images in more ways than just comparing pixel differences. It evaluates the resemblance and perceptual quality of images by taking into account aspects including structure, brightness, and contrast characteristics. As is a result, SSIM frequently used in the evaluation process for image processing such as segmentation.

% SSIM is defined as:
% \begin{equation}
% \textup{SSIM}(I_x,I_y)=\frac{2u_{I_x}u_{I_y}+k_1}{u_{I_x}^2+u_{I_y}^2+k_1}\cdot\frac{2\sigma_{\hat{I}I_y}+k_2}{\sigma^2_{\hat{I}}+\sigma_{I_y}^2+k_2}
% \end{equation}
% where $u_{I_x}$ and $u_{I_y}$ represent the mean of the images $I_x$ and $I_y$, respectively. $k_1$, and $k_2$ are constants set to 0.01 and 0.03, respectively.  $\sigma_{I_x}$ and $\sigma_{I_y}$ are the standard deviations while $\sigma_{I_x I_y}$ is the covariance.


% \noindent\textbf{Measures on transferability:}

% \noindent\textbf{Success rate:}Given an attack method A, the adversarial input  \begin{math} x_{i} \end{math} , the class label \begin{math} y_{i} \end{math} , and the classifier \begin{math} f \end{math} 

% \begin{equation}
% Successrate (A) = \frac{1}{N}\sum_{i=1}^{N}I(f(x_{i}^{'})\neq y_{i})    
% \end{equation}

% where \begin{math} N \end{math} is the test set size, and \begin{math} I(.) \end{math} is the indicate function.

% \noindent\textbf{Rank:}And the ranking position p (starting from 1) of the true class label \begin{math} y_{i} \end{math} in the prediction list $R$ of the target classifier:
% (TO check)

% \begin{equation}
% Rank(A) = \frac{1}{N}\sum_{N}^{i = 1}\left\{p|R_p (x_{i_{'}}) = y_i\right\}
% \end{equation}


% \noindent\textbf{Measures on Stealthiness:}



% \noindent\textbf{Peak Signal-to-Noise Ratio (PSNR)}
% PSNR of one frame is defined as:
% \begin{equation}
% \textup{PSNR}=10\log_{10}\left(\frac{L^2}{\textup{MSE}}\right)
% \end{equation}
% where $L$ represents the maximum range of color value, which is usually 255, and the mean squared error (MSE) is defined as:
% \begin{equation}
% \textup{MSE}=\frac{1}{N}\sum_{i=1}^N(\hat{I}_i-\tilde{I}_i)^2
% \end{equation}
% where $N$ denotes the total number of pixels in an image or a frame, $\hat{I}$ and $\tilde{I}$ are the ground truth HR frame and the SR recovered frame, respectively. A higher value of PSNR generally means superior quality. 

 
% \noindent\textbf{Structural Similarity Index Measure (SSIM)}
%  ~\cite{wang2004image}

% SSIM is defined as:
% \begin{equation}
% \textup{SSIM}(\hat{I},\tilde{I})=\frac{2u_{\hat{I}}u_{\tilde{I}}+k_1}{u_{\hat{I}}^2+u_{\tilde{I}}^2+k_1}\cdot\frac{2\sigma_{\hat{I}\tilde{I}}+k_2}{\sigma^2_{\hat{I}}+\sigma_{\tilde{I}}^2+k_2}
% \end{equation}
% where $u_{\hat{I}}$ and $u_{\tilde{I}}$ represent the mean values of the images $\hat{I}$ and $\tilde{I}$, respectively. $k_1$ and $k_2$ are constants, which are used to stabilize the calculation and are usually set to 0.01 and 0.03, respectively.  $\sigma_{\hat{I}}$ and $\sigma_{\tilde{I}}$ denote the standard deviations, and $\sigma_{\hat{I}\tilde{I}}$ denotes the covariance.

 
% \noindent\textbf{Perceptual color distance ($\mathcal{\delta}$ E) ~\cite{zhao2020towards}}

% It is a metric used for measuring the perceptual color distance between two pixels.

% (toADD Equation)

% Image level perceptual color difference is obtained by computing the L2 norm of the pcd for each pixel
 
 
%  \noindent\textbf{Learned Perceptual Image Patch Similarity (LPIPS) ~\cite{zhang2018unreasonable}}

% This metric calculates the perceptual similarity between two images. And it calculates the cosine distance  of features at each convolutional layer $l$ and averages the results according to spatial dimensions $H  W$ and layers of the network $f$

% \begin{equation}
% LPIPS=\sum_{l}\frac{1}{H_l W_l}\sum_{h,w}cos(f_{hw}^{l}(x),f_{hw}^{l}(x^{'})) 
% \end{equation}
% (toADD Equations and parah)
 
%  \noindent\textbf{Frechet Inception Distance (FID) ~\cite{szegedy2016rethinking}}
% This metric is used to evaluate the quality of images conducted by generative models. And we can also use this to evaluate the adversarial image quality. It assesses the difference in the distribution between the original one and the adversary one.

% \begin{equation}
%  FID=\left\| \mu-\mu^{'} \right\|_{2}^{2}+tr(\sum+(\sum)^{'}-2\sqrt{\sum(\sum)^{'}})
% \end{equation}


% \noindent{Benchmark Attack Methods}

% \noindent\textbf{Transferable attack methods and metrics:}

% \noindent\textbf{dataset:}

% ImageNet


% \noindent\textbf{Transferable attack methods:}



% % \begin{table*}[!t]
% \caption{(Attack transferability in terms of success rate (\%)/true label ranking. ResNet-50 (ResNet-152 for generative attacks) is used as the backbone architecture of the surrogate model. We also adopt ResNet as the backbone for all defenses to better isolate the specific contribution of defenses from surrogate-target model similarity. )
% }
% \newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}

% \renewcommand{\arraystretch}{1}
%       \centering
%       \resizebox{\textwidth}{!}{
%         \begin{tabular}{l|ccc|ccc}
% \toprule[0.5pt]
% \multirow{2}{*} {Attacks}&\multicolumn{3}{c|}{Without Defenses}\\
% &IncV3&DN121&VGG19\\
% \midrule

% Clean Acc&100.0&100.0&100.0\\
% PGD&43.6/10&79.6/19&72.7/18\\
% \hline
% MI~\cite{dong2018boosting}&55.8/19&85.7/30&78.1/25\\
% NI~\cite{lin2020nesterov}&60.4/27&87.2/47&82.7/41\\
% PI~\cite{wang2021boosting}&66.0/34&92.1/63&87.7/51\\
% \hline
% DI~\cite{xie2019improving} &69.8/41&99.0/118&99.1/132\\
% TI~\cite{dong2019evading}  &63.0/26&96.9/64&96.1/67\\
% SI~\cite{lin2020nesterov}  &61.8/33&93.8/61&85.6/45\\
% VT~\cite{wang2021enhancing}&67.1/38&95.4/65&92.5/58\\
% Admix~\cite{wang2021admix} &53.4/20&86.7/43&83.5/45\\
% \hline
% TAP~\cite{zhou2018transferable} &50.3/21&77.0/69&77.6/61\\
% AA~\cite{inkawhich2019feature}  &43.5/20&61.8/43&64.1/42\\
% ILA~\cite{huang2019enhancing}   &72.3/63&94.5/141&92.6/113\\
% FIA~\cite{wang2021feature}      &88.4/237&97.5/399&97.1/35\\
% NAA~\cite{zhang2022improving}   &85.0/123&96.7/215&95.3/17\\
% \hline
% SGM~\cite{wu2020skip}          &57.0/22&90.3/38&87.1/40\\
% LinBP~\cite{guo2020backpropagating}&82.8/164&98.1/284&97.4/267\\
% RFA~\cite{springer2021little} &66.0/65&77.7/71&68.7/45 \\
% IAA~\cite{zhu2021rethinking}        &\underline{90.8/354}&\underline{99.5/714}&\underline{99.2/642}\\
% DSM~\cite{yang2022boosting}   &62.8/38&95.9/101&93.5/89  \\

% \hline
% GAP~\cite{poursaeed2018generative}&65.1/97&82.1/130&87.4/153\\\
% CDA~\cite{naseer2019cross}         &\underline{97.8/341}&\underline{99.2/339}&\underline{99.2/374}\\
% GAPF~\cite{kanth2021learning}      &\textbf{99.2/448}&\textbf{99.6/443}&\textbf{99.6/470}\\
% BIA~\cite{zhang2022beyond}       &88.2/216&97.1/289&97.8/288\\
% TTP~\cite{naseer2021generating}   &89.0/247&97.1/293&97.3\\

% \bottomrule[0.5pt]
% \end{tabular}
% }
% \label{tab:transfer}
% \end{table*}


% \noindent{Gradient stabilization} 
% the momentum ~\cite{YinpengDong2017BoostingAA}, Nesterov Accelerated Gradient ~\cite{JiadongLin2019NesterovAG}, enhanced momentum ~\cite{XiaosenWang2021BoostingAT}.

% \noindent{Input augmentation} 
% geometric transformations ~\cite{CihangXie2018ImprovingTO, YinpengDong2019EvadingDT}, pixels scaling ~\cite{JiadongLin2019NesterovAG}, noise add ~\cite{XiaosenWang2021EnhancingTT}, mix images ~\cite{XiaosenWang2021AdmixET}

% \noindent{Feature disruption} 
% intermediate layers output features are more generic \cite{SimonKornblith2019SimilarityON}.
% push away features from the original one. Such as ~\cite{AdityaGaneshan2019FDAFD,huang2019enhancing,QizhangLi2020YetAI,ZhuoranLiu2019WhosAO,MuzammalNaseer2018TaskgeneralizableAA,ZhouWen2018TransferableAP} on all features,~\cite{ZhiboWang2021FeatureIT,WeibinWu2020BoostingTT,JianpingZhang2022ImprovingAT} on specific features according to model interpretability techniques ~\cite{RamprasaathRSelvaraju2016GradCAMVE,MukundSundararajan2017AxiomaticAF}, and ~\cite{InkawhichNathan2019FeatureSP} on the targeted attack.

% \noindent{Surrogate model refinement} 
% refine the surrogate model, such as activation layer~\cite{zhu2022rethinking,zhang2021backpropagating, YiwenGuo2020BackpropagatingLI}, training process~\cite{DingchengYang2022BoostingTA,JacobMSpringer2021ALR}, and architecture ~\cite{DongxianWu2020SkipCM,YaoZhu2022RETHINKINGAT}.



\begin{table*}[t!]
  \centering
  \footnotesize
  \renewcommand{\arraystretch}{1.2}
  \renewcommand{\tabcolsep}{4.2mm}
  \caption{Adversarial attack on Cityscape dataset, where attacks are grouped, and best performance numbers are highlighted.}
  % \caption{Performance on Cityscape dataset. Train done for Deeplabv3/v3p partial done for fcn,testing m1:PGD,m2:IAA~\cite{zhu2022rethinking},m3:HGN~\cite{https://doi.org/10.48550/arxiv.2107.01809},m4:style-atk~\cite{DongzeLi2021ExploringAF},m5:SegPGD~\cite{gu2022segpgd}}
  \begin{tabular}{l|c|cc|cc|cc|cc}
  \toprule[1.1pt]
  &&\multicolumn{2}{c|}{Image Quality}&\multicolumn{6}{c}{Target (performance is evaluated with $\text{mIOU}\downarrow$ and $\text{Sr}\uparrow$)}\\ \hline
  Source & Attack & $\text{PSNR}\uparrow$ & $\text{SSIM}\uparrow$ & \multicolumn{2}{c|}{DV3Res50} & \multicolumn{2}{c|}{DV3101}& \multicolumn{2}{c}{DV3MOB} \\ \hline
  \multirow{7}{*}{DV3Res50} & FGSM~\cite{Explaining_and_Harnessing_Adversarial_Examples}  & 30.11 & 0.2870 & 0.2834 & 0.6343 &  0.3892 & 0.4903 & 0.2834 & 0.6106  \\ 
  & PGD~\cite{PGD_attack}  & 39.92 & 0.6066 & 0.0108 & 0.9861 & 0.2990  & 0.6084  &  0.3987 & 0.4522 \\
     % \cline{2-10}
  % & NI~\cite{JiadongLin2019NesterovAG}  & 32.82 & 0.3908 & 0.0225 & 0.9689 & 0.1734  & 0.7661 &  0.3959 & 0.4431 \\ 
  %   & DI ~\cite{CihangXie2018ImprovingTO}  & 35.94 & 0.4963 & 0.4870 & 0.3275 & 0.5094  & 0.3127 & 0.4755 & 0.3311  \\ 
    
  %  & TI ~\cite{YinpengDong2019EvadingDT} & 30.11 & 0.3145 & 0.3113 & 0.5702 & 0.3761  & 0.4927 & 0.2424 & 0.6578  \\ 
  % & GAP~\cite{OmidPoursaeed2017GenerativeAP} & 0. & 0. & 0. & 0. &0.  & 0. &  0. & 0.  \\ 
  % & style-atk~\cite{DongzeLi2021ExploringAF} & 0. & 0. & 0. & 0. &0.  & 0. &  0.& 0.  \\ 
  & SegPGD~\cite{gu2022segpgd} & 40.34 & 0.6195 & 0.0210 & 0.9729 & 0.3550 & 0.5351 & 0.4771 & 0.3444  \\ 
    & DAG~\cite{xie2017adversarial} & \textbf{43.91}  & \textbf{0.7758} & 0.0761 & 0.9018 & 0.4895 & 0.3590 & 0.5476 & 0.2476  \\ 
    \cline{2-10}
    & NI~\cite{JiadongLin2019NesterovAG}  & 32.91 & 0.3729 & 0.0142 & 0.9817 & \textbf{0.1398}  & \textbf{0.8169} &  \textbf{0.1754} & \textbf{0.7590}  \\ 
    & DI ~\cite{CihangXie2018ImprovingTO}  & 39.90 & 0.6036 & 0.0178 & 0.9770 & 0.1925  & 0.7479 & 0.2580 & 0.6455  \\ 
    
   & TI ~\cite{YinpengDong2019EvadingDT} & 39.70 & 0.6119 & \textbf{0.0103} & \textbf{0.9867} & 0.2562  & 0.6645 & 0.3231 & 0.5561 \\ 
   % \hline
   \midrule[0.7pt]
   \multirow{8}{*}{DV3Res101} & FGSM~\cite{Explaining_and_Harnessing_Adversarial_Examples}  & 30.11 & 0.2890 & 0.3103 & 0.5996 &  0.3424  & 0.5516 &  0.2655 & 0.6352  \\ 
   & PGD~\cite{PGD_attack}  & 39.89 & 0.6064 & 0.2064  & 0.7337 & 0.0164 & 0.9785 & 0.3875 & 0.4676  \\ 
      % \cline{2-10}
  % & NI~\cite{JiadongLin2019NesterovAG}  & 32.82 & 0.3975 & 0.1411 & 0.8052 & 0.0284 & 0.9618 &  0.2059 & 0.7103  \\ 
  %   & DI ~\cite{CihangXie2018ImprovingTO} & 36.41 & 0.5083 & 0.4186 & 0.4221 & 0.4186  & 0.4353 &  0.4924 & 0.3073 \\ 
  %      & TI ~\cite{YinpengDong2019EvadingDT}  & 30.11 & 0.3253 & 0.3600 & 0.5030 & 0.3505 & 0.5272 & 0.2466 & 0.6519  \\
  % & GAP~\cite{OmidPoursaeed2017GenerativeAP}& 0. & 0. & 0. & 0. &0.  & 0. &  0.& 0. \\ 
  % & style-atk~\cite{DongzeLi2021ExploringAF} & 0. & 0. & 0. & 0. &0.  & 0. &  0.& 0.  \\ 
  & SegPGD~\cite{gu2022segpgd}  & 40.33 & 0.6204 & 0.2542 & 0.6720 &  0.0251 & 0.9671 &  0.4669 & 0.3585  \\
    & DAG~\cite{xie2017adversarial} & \textbf{44.39} & \textbf{0.8026} & 0.4412 & 0.4307 &  0.1151 & 0.8493 & 0.5560 & 0.2361  \\ 
    \cline{2-10}
    & NI~\cite{JiadongLin2019NesterovAG}  & 32.91 & 0.3736 & \textbf{0.1102} & \textbf{0.8578} & 0.0176 & 0.9770 &  \textbf{0.1665} & \textbf{0.7712}  \\ 
    & DI ~\cite{CihangXie2018ImprovingTO} & 39.88 & 0.6045 &  0.1544 & 0.8008 &  0.0268 & 0.9649 &  0.2683 & 0.6314 \\ 
       & TI ~\cite{YinpengDong2019EvadingDT}  & 39.69 & 0.6135 & 0.1984 & 0.7440 &  \textbf{0.0164} & \textbf{0.9785} & 0.3271 & 0.5506  \\
% \hline
\midrule[0.7pt]
  \multirow{7}{*}{DV3MOB} & FGSM~\cite{Explaining_and_Harnessing_Adversarial_Examples}  & 30.12 & 0.2887 & 0.3356 & 0.5670 & 0.4019 & 0.4737  &  0.2072 & 0.7153 \\ 
  & PGD~\cite{PGD_attack}  & 39.97 & 0.6085 & 0.4743 & 0.3880 & 0.5450 & 0.2863 &  0.0110 & 0.9849 \\ 
     % \cline{2-10}
  % & NI~\cite{JiadongLin2019NesterovAG}  & 32.75 & 0.3949 & 0.3400 & 0.5306 &0.3832  & 0.4831 &  0.0186 & 0.9738 \\ 
  %   & DI ~\cite{CihangXie2018ImprovingTO}  & 35.92 & 0.4874& 0.5086 & 0.2978 & 0.5287  & 0.2868 &  0.3543 & 0.5016  \\ 
  %      & TI ~\cite{YinpengDong2019EvadingDT}  & 30.11 & 0.3182 & 0.3569 & 0.5073 & 0.3736 & 0.4960 & 0.1951 & 0.7246 \\
  % & GAP~\cite{OmidPoursaeed2017GenerativeAP} & 0. & 0. & 0. & 0. &0.  & 0. &  0.& 0. \\ 
  % & style-atk~\cite{DongzeLi2021ExploringAF} & 0. & 0. & 0. & 0. &0.  & 0. &  0.& 0. \\ 
  & SegPGD~\cite{gu2022segpgd}  & 40.40 & 0.6217 & 0.6077 & 0.2159 & 0.6745 & 0.1167 &  0.0221 & 0.9696 \\
    & DAG~\cite{xie2017adversarial} & \textbf{43.27} & \textbf{0.7420}& 0.5826 & 0.2483 & 0.6306 & 0.1742  & 0.0330 & 0.9547 \\ 
    \cline{2-10}
    & NI~\cite{JiadongLin2019NesterovAG}  & 32.81 & 0.3739 & \textbf{0.2976} & \textbf{0.6160} &  \textbf{0.3458} & \textbf{0.5471} &  0.0109 & 0.9850 \\ 
    & DI ~\cite{CihangXie2018ImprovingTO}  & 39.89 & 0.6035 & 0.3400 & 0.5613 & 0.4350  & 0.4303 &  0.0128 & 0.9824  \\ 
       & TI ~\cite{YinpengDong2019EvadingDT}  & 39.62 & 0.6137 & 0.3886 & 0.4986 & 0.4796 & 0.3719 & \textbf{0.0064} & \textbf{0.9912} \\
% \hline
\bottomrule[1.1pt]
  % \multirow{5}{*}{MPViT} & m1 & 0. & 0. & 0. & 0. &0.  & 0. &  0.& 0. & 0. & 0. \\ 
  % & m2 & 0. & 0. & 0. & 0. &0.  & 0. &  0.& 0. & 0. & 0. \\ 
  % & m3 & 0. & 0. & 0. & 0. &0.  & 0. &  0.& 0. & 0. & 0. \\ 
  % & m4 & 0. & 0. & 0. & 0. &0.  & 0. &  0.& 0. & 0. & 0. \\ 
  % & m5 & 0. & 0. & 0. & 0. &0.  & 0. &  0.& 0. & 0. & 0. \\
   % \hline
  \end{tabular}
  \label{tab:adversarial_cityscape}
\end{table*}


% \noindent{Generative modeling} 
% generative model to generate transferable adversarial  such as ~\cite{OmidPoursaeed2017GenerativeAP,naseer2019cross, KrishnaKanthNakka2021LearningTA,QilongZhang2022BeyondIA,MuzammalNaseer2021OnGT,XiaoYang2021BoostingTO}.
\noindent\textbf{Adversarial Attacks:} We first investigate the conventional adversarial attacks, including
FGSM~\cite{Explaining_and_Harnessing_Adversarial_Examples},
PGD~\cite{PGD_attack}, and apply them to semantic segmentation. We also analyse the transferability of existing semantic attacks, namely SegPGD~\cite{gu2022segpgd} and DAG~\cite{xie2017adversarial}. 
% As an extension of PGD ~\cite{PGD_attack} to semantic segmentation, SegPGD~\cite{gu2022segpgd} achieves faster and more effective adversarial example generation. 
% We also extend one of the manifold attacks, namely GAP~\cite{OmidPoursaeed2017GenerativeAP} for semantic segmentation, and explain its transferability. 
Further, we adapt the existing transferable attack for image classification to our dense semantic segmentation task, including NI~\cite{JiadongLin2019NesterovAG},
DI~\cite{CihangXie2018ImprovingTO},
TI~\cite{YinpengDong2019EvadingDT}, and AAI~\cite{zhu2022rethinking}.
% DAG~\cite{xie2017adversarial} is an existing adversarial attack method for semantic segmentation, we also explain its transferability. 





% \begin{table*}[t!]
%   \centering
%   \footnotesize
%   \renewcommand{\arraystretch}{1.25}
%   \renewcommand{\tabcolsep}{3.15mm}
%   \caption{Transferable attack on PascalVOC dataset.
%   % (waiting for iaa VOC dv3+ for currently)
%   }
%   \begin{tabular}{l|c|cc|cc|cc|cc}
%   \hline
%   &&\multicolumn{2}{c|}{Image Quality}&\multicolumn{6}{c}{Target (performance is evaluated with $\text{mIOU}\downarrow$ and $\text{Sr}\uparrow$)}\\ \hline
%   Source & Attack & $\text{PSNR}\uparrow$ & $\text{SSIM}\uparrow$ & \multicolumn{2}{c|}{DV3} & \multicolumn{2}{c|}{DV3+}& \multicolumn{2}{c}{FCN}  \\ \hline
%   \multirow{5}{*}{DV3}  &  NI~\cite{JiadongLin2019NesterovAG} & 32.73 & 0.5912& 0.0871 & 0.8842 & 0.2607  & 0.6654 &  0.4234 & 0.3496 \\ 
%   & DI ~\cite{CihangXie2018ImprovingTO} & 36.27 & 0.6876& 0.3167 & 0.5790 &  0.4886 & 0.3728 &  0.4886 & 0.2494  \\ 
%   & TI ~\cite{YinpengDong2019EvadingDT} & 30.07 & 0.5136 & 0.3350 & 0.5547 & 0.4904 & 0.3705 & 0.4085 & 0.3725  \\
%     & IAA ~\cite{zhu2022rethinking} & 30.45 & 0.6426 & 0.2275 & 0.7464 & 0.4855 & 0.3242 & 0.5625 & 0.1360  \\
%    \hline
%    \multirow{5}{*}{DV3+} &  NI~\cite{JiadongLin2019NesterovAG} & 32.73 & 0.6767 & 0.2234 & 0.7030 & 0.0933 & 0.8802 & 0.4355 & 0.3310  \\ 
%     & DI ~\cite{CihangXie2018ImprovingTO} & 36.41 & 0.6922 & 0.4858 & 0.3542 & 0.3483  & 0.5529 &  0.5063 & 0.2223  \\ 
%     & TI ~\cite{YinpengDong2019EvadingDT} & 30.07 & 0.5150 & 0.4812 & 0.3603 & 0.3856  & 0.5050 & 0.4300 & 0.3386  \\
%   % & GAP~\cite{OmidPoursaeed2017GenerativeAP} & 0. & 0. & 0. & 0. &0.  & 0. &  0.& 0.  \\ 
%   % & style-atk~\cite{DongzeLi2021ExploringAF} & 0. & 0. & 0. & 0. &0.  & 0. &  0.& 0.  \\ 
  
%     & IAA ~\cite{zhu2022rethinking} & 30.46 & 0.6453 & 0.4385 &0.4125 &  0.2599 & 0.6383 & 0.5690 & 0.1259 \\
% \hline
%   \multirow{5}{*}{FCN} &  NI~\cite{JiadongLin2019NesterovAG} & 32.65 & 0.6018 & 0.1611 & 0.7858 & 0.1607 & 0.7937 &  0.0326 & 0.9499  \\ 
%     & DI ~\cite{CihangXie2018ImprovingTO} & 33.72 & 0.6235 & 0.3839 & 0.4896 &0.4618  & 0.4072 &  0.1662 & 0.7447  \\ 
%     & TI ~\cite{YinpengDong2019EvadingDT} & 30.07 & 0.5167& 0.3816 & 0.4927 & 0.4696  & 0.3972 & 0.1647 & 0.7470 \\
% \hline
%   % \multirow{5}{*}{MPViT(training for voc)} & m1 & 0. & 0. & 0. & 0. &0.  & 0. &  0.& 0. & 0. & 0. \\ 
%   % & m2 & 0. & 0. & 0. & 0. &0.  & 0. &  0.& 0. & 0. & 0. \\ 
%   % & m3 & 0. & 0. & 0. & 0. &0.  & 0. &  0.& 0. & 0. & 0. \\ 
%   % & m4 & 0. & 0. & 0. & 0. &0.  & 0. &  0.& 0. & 0. & 0. \\ 
%   % & m5 & 0. & 0. & 0. & 0. &0.  & 0. &  0.& 0. & 0. & 0. \\
%   %  \hline
%   \end{tabular}
%   \label{tab:tranferable_models_pascalvoc}
% \end{table*}

% \begin{table*}[t!]
%   \centering
%   \footnotesize
%   \renewcommand{\arraystretch}{1.25}
%   \renewcommand{\tabcolsep}{3.15mm}
%   \caption{Transferable attack on Cityscape dataset.}
%   % \caption{Performance on Cityscape dataset. Train done for Deeplabv3/v3p partial done for fcn,testing m1:PGD,m2:IAA~\cite{zhu2022rethinking},m3:HGN~\cite{https://doi.org/10.48550/arxiv.2107.01809},m4:style-atk~\cite{DongzeLi2021ExploringAF},m5:SegPGD~\cite{gu2022segpgd}}
%   \begin{tabular}{l|c|cc|cc|cc|cc}
%   \hline
%   &&\multicolumn{2}{c|}{Image Quality}&\multicolumn{6}{c}{Target (performance is evaluated with $\text{mIOU}\downarrow$ and $\text{Sr}\uparrow$)}\\ \hline
%   Source & Attack & $\text{PSNR}\uparrow$ & $\text{SSIM}\uparrow$ & \multicolumn{2}{c|}{DV3ResNet} & \multicolumn{2}{c|}{DV3plusResNet}& \multicolumn{2}{c}{DV3plusMobileNet} \\ \hline
%   \multirow{5}{*}{DV3ResNet} & NI~\cite{JiadongLin2019NesterovAG}  & 32.82 & 0.3908 & 0.0225 & 0.9689 & 0.1734  & 0.7661 &  0.3959 & 0.4431 \\ 
%     & DI ~\cite{CihangXie2018ImprovingTO}  & 35.94 & 0.4963 & 0.4870 & 0.3275 & 0.5094  & 0.3127 & 0.4755 & 0.3311  \\ 
    
%    & TI ~\cite{YinpengDong2019EvadingDT} & 30.11 & 0.3145 & 0.3113 & 0.5702 & 0.3761  & 0.4927 & 0.2424 & 0.6578  \\ 
%   % & GAP~\cite{OmidPoursaeed2017GenerativeAP} & 0. & 0. & 0. & 0. &0.  & 0. &  0. & 0.  \\ 
%   % & style-atk~\cite{DongzeLi2021ExploringAF} & 0. & 0. & 0. & 0. &0.  & 0. &  0.& 0.  \\ 
%   % & SegPGD~\cite{gu2022segpgd} & 33.19 & 0.3932 & 0.3924 & 0.4582 & 0.4879 & 0.3431 & 0.4529 & 0.3629  \\ 
%   %   & DAG~\cite{xie2017adversarial} & 46.10  & 0.8712 & 0.1503 & 0.7925 & 0.5238 & 0.2934 & 0.5821 & 0.1782  \\ 
%    \hline
%    \multirow{5}{*}{DV3plusResNet} & NI~\cite{JiadongLin2019NesterovAG}  & 32.82 & 0.3975 & 0.1411 & 0.8052 & 0.0284 & 0.9618 &  0.2059 & 0.7103  \\ 
%     & DI ~\cite{CihangXie2018ImprovingTO} & 36.41 & 0.5083 & 0.4186 & 0.4221 & 0.4186  & 0.4353 &  0.4924 & 0.3073 \\ 
%        & TI ~\cite{YinpengDong2019EvadingDT}  & 30.11 & 0.3253 & 0.3600 & 0.5030 & 0.3505 & 0.5272 & 0.2466 & 0.6519  \\
%   % & GAP~\cite{OmidPoursaeed2017GenerativeAP}& 0. & 0. & 0. & 0. &0.  & 0. &  0.& 0. \\ 
%   % & style-atk~\cite{DongzeLi2021ExploringAF} & 0. & 0. & 0. & 0. &0.  & 0. &  0.& 0.  \\ 
%   % & SegPGD~\cite{gu2022segpgd}  & 33.19 & 0.4017 & 0.4498 & 0.3790 & 0.4108 & 0.4469 &  0.4248 & 0.4024  \\
%   %   & DAG~\cite{xie2017adversarial} & 47.06 & 0.9042 & 0.4878 & 0.3268 & 0.2166 & 0.7146 & 0.5640 & 0.2038  \\ 
% \hline
%   \multirow{5}{*}{DV3plusMobileNet} & NI~\cite{JiadongLin2019NesterovAG}  & 32.75 & 0.3949 & 0.3400 & 0.5306 &0.3832  & 0.4831 &  0.0186 & 0.9738 \\ 
%     & DI ~\cite{CihangXie2018ImprovingTO}  & 35.92 & 0.4874& 0.5086 & 0.2978 & 0.5287  & 0.2868 &  0.3543 & 0.5016  \\ 
%        & TI ~\cite{YinpengDong2019EvadingDT}  & 30.11 & 0.3182 & 0.3569 & 0.5073 & 0.3736 & 0.4960 & 0.1951 & 0.7246 \\
%   % & GAP~\cite{OmidPoursaeed2017GenerativeAP} & 0. & 0. & 0. & 0. &0.  & 0. &  0.& 0. \\ 
%   % & style-atk~\cite{DongzeLi2021ExploringAF} & 0. & 0. & 0. & 0. &0.  & 0. &  0.& 0. \\ 
%   % & SegPGD~\cite{gu2022segpgd}  & 33.20 & 0.3976 & 0.4862 & 0.3287 &0.5194  & 0.2994 &  0.3266 & 0.5405 \\
%   %   & DAG~\cite{xie2017adversarial} & 45.25 & 0.8348& 0.6044 & 0.1655 & 0.6428 & 0.1329  & 0.0858 & 0.8789 \\ 
% \hline
%   % \multirow{5}{*}{MPViT} & m1 & 0. & 0. & 0. & 0. &0.  & 0. &  0.& 0. & 0. & 0. \\ 
%   % & m2 & 0. & 0. & 0. & 0. &0.  & 0. &  0.& 0. & 0. & 0. \\ 
%   % & m3 & 0. & 0. & 0. & 0. &0.  & 0. &  0.& 0. & 0. & 0. \\ 
%   % & m4 & 0. & 0. & 0. & 0. &0.  & 0. &  0.& 0. & 0. & 0. \\ 
%   % & m5 & 0. & 0. & 0. & 0. &0.  & 0. &  0.& 0. & 0. & 0. \\
%    % \hline
%   \end{tabular}
%   \label{tab:transferable_models_cityscape}
% \end{table*}


% manifold attack 

% GAP~\cite{OmidPoursaeed2017GenerativeAP}(write done,training)

% transferable attack

% Nesterov Accelerated Gradient NI ~\cite{JiadongLin2019NesterovAG},
% Diversity transformation DI ~\cite{CihangXie2018ImprovingTO},
% Translation-invariant  TI ~\cite{YinpengDong2019EvadingDT}

% adversarial attack for semantic segmentation 
% DAG ~\cite{xie2017adversarial}
% SegPGD ~\cite{gu2022segpgd}


% \subsection{Models to evaluate}
% 1) conventional adversarial attack models (2-3)--Done

% FGSM ~\cite{Explaining_and_Harnessing_Adversarial_Examples}(done)
% PGD ~\cite{PGD_attack}(done)

% % 2) universal attack (1-2)--to check

% % UAP ~\cite{moosavi2017universal}(to implement,no code)

% 3) manifold attack (1-2)--to check

% % style-atk ~\cite{DongzeLi2021ExploringAF}(to implement,not compatiable code)

% % HGN ~\cite{https://doi.org/10.48550/arxiv.2107.01809}(done)
% GAP~\cite{OmidPoursaeed2017GenerativeAP}(write done,training)

% 4) transferable attack (2-3)

% Nesterov Accelerated Gradient NI ~\cite{JiadongLin2019NesterovAG}(done)
% % IAA ~\cite{zhu2022rethinking}
% Diversity transformation DI ~\cite{CihangXie2018ImprovingTO}(done)
% Translation-invariant  TI ~\cite{YinpengDong2019EvadingDT}

% 5) adversarial attack for semantic segmentation (1-2) --Done

% DAG ~\cite{xie2017adversarial}
% SegPGD ~\cite{gu2022segpgd}(done)

% % CosPGD ~\cite{agnihotri2023cospgd} (to implement,no code)

% \subsection{Technical details}


\noindent\textbf{Network structures:}
% To investigate the transferability of semantic segmentation models, w
We investigate different backbones for adversarial attack transferability analysis.
% , including multiple versions of Deeplabv3~\cite{chen2017rethinking}, FCN~\cite{long2015fully} and Mobilenets~\cite{howard2017mobilenets,sandler2018mobilenetv2}. 
Specifically, for the PascalVOC dataset, we investigate Deeplabv3~\cite{chen2017rethinking} with ResNet50 backbone (\enquote{DV3Res50}), Deeplabv3+~\cite{chen2018encoder} with ResNet101 backbone (\enquote{DV3Res101}) and FCN~\cite{long2015fully} with VGG16 backbone (\enquote{FCNVGG16}). For Cityscape dataset, we run models using Deeplabv3~\cite{chen2017rethinking} (\enquote{DV3Res50}) with ResNet50 backbone, Deeplabv3+~\cite{chen2018encoder} (\enquote{DV3Res101}) with ResNet101 backbone and Deeplabv3+~\cite{chen2017rethinking} with MobilenetsV2~\cite{sandler2018mobilenetv2} backbone (\enquote{DV3MOB}). We show parameter numbers of each network structure in Table~\ref{tab:backbone_parameter_numbers} for a clear comparison of the sizes of the related models.
% In Table~\ref{tab:backbone_parameter_numbers}, we show parameter numbers of each backbone based models, where 

% 1) baseline model introduction:
% \Mengqi{more implement details}
% Deeplabv3~\cite{chen2017rethinking} uses a pre-trained backbone such as ResNet as the feature extraction module and combines with the dilated convolution
% % and atrous spatial pyramid pooling(ASPP) module 
% to obtain better capture of different scales of features. FCN~\cite{long2015fully} replaces the traditional fully connected layer in classification with the fully convolutional layer, which can keep some spatial information, and then use the transpose convolution to get the prediction result. 
% We use Deeplabv3~\cite{chen2017rethinking} with ResNet50 backbone, Deeplabv3+~\cite{chen2018encoder} with ResNet101 backbone, FCN~\cite{long2015fully} with VGG16 backbone, and Deeplabv3+~\cite{chen2017rethinking} with MobileNet~\cite{howard2017mobilenets} backbone.



% By comparing to Deeplabv3,Deeplabv3plus~\cite{chen2018encoder} replaces the last 1x1 convolution layer with a decoder, which can have a more precise pixel wised prediction.

% FCN ~\cite{long2015fully} replaces the traditional fully connected layer in classification with the fully convolutional layer, which can keep some spatial information, and then use the transpose convolution to get the prediction result.



% ResNet~\cite{he2016deep} uses the skip layer as known as the residual connection to avoid the gradient vanishing and gradient explosion while the depth of the network is increasing. It can better forward the deeper information while keeping the current layer's information. 

% Mobilenet~\cite{howard2017mobilenets} uses depthwise separable convolution which includes depthwise
% and pointwise convolution layer followed by batch norm to deduce the size of the network.

% ResNet50(25.6milion) 

% ResNet101(44.7milion)

% VGG16(138milion)

% Mobilenet (13milion)

% Our testing model and backbone are:
% Deeplab v3 with ResNet 50, Deeplabplus v3 with ResNet 101, FCN with VGG16, and Deeplabplus v3plus(Mobilenet). The main target is to find the backbone of the model, segmentation model, and parameter size and backbone depth's influences on the attack transferability. 

% \Mengqi{ graph}
% MPViT(transformer re allocate done,need to fix the gradient decent)



% \subsection{Technical details}

\noindent\textbf{Adversarial attack:} The attack rate $\epsilon$ is restricted to 0.03 (8/255 in $L_\infty$ norm) to ensure that the adversarial example has enough perturbation to fool the segmentation model, while it is still visually invisible.
% while it will not have too much difference from the original image which can be distinguished easily.
For the iterative attacks (including DAG~\cite{xie2017adversarial}), we set the attack iteration as 10 to achieve a trade-off between
% that the iteration method can have a relative balance between 
attack quality and model efficiency.
% The attack rate $\alpha$ per iteration is restricted to 0.003.

% \noindent\textbf{Dataset:}
% Before the attack, all models are trained with the same normalization according to the mean and the standard deviation of the current datasets. 
% When testing for the image quality metrics, we find that the PSNR is not changed too much when testing on the different dataset, but the SSIM have a big margin between different dataset while keeping the source model and the attack method the same.

% \noindent\textbf{DAG's perturbation steps:}
% For the original DAG~\cite{xie2017adversarial}, to iteratively cause all the pixels to produce inaccurate predictions (see Eq.~\ref{eq_dag_accumulated_rt}), they perform more than 200 iterations of accumulated perturbation generation.
% % it has iterations up to 200 for the semantic segmentation, but 
% As we set attack iteration to 10 for all the other iterative attacks, we reduce the perturbation generation of DAG~\cite{xie2017adversarial} to 10 steps and keep the total perturbation rate using the sign function to the same for a fair comparison.
% all iterations in this experiment, we set the iterations to 10. Hence, to keep the comparison reasonable, we set the attack iterations of the DAG to 10 and apply a sign function to control the total attack rate to 0.03. 
% Surprisingly, this simple change increases the performance of the DAG, while keeping the image quality outstanding. 
% (TODO, do we need an appendix for the fixed version of this algorithm? it actually reaches relatively much higher performance than its original one)




% % Figure environment removed


% % Figure environment removed

% \noindent\textbf{Network structure selection:} For the two training datasets, we use two shared networks, and the third ones are different, as we found FCN with VGG16 backbone is hard to tune on thje Cityscape dataset. We thus switch to Deeplabv3+ with a MobilenetV2~\cite{sandler2018mobilenetv2} backbone, which is also suitable to evaluate
% % and evaluate 
% transferability of adversarial attacks for semantic segmentation.
% We show parameter numbers of each network structure in Table~\ref{tab:backbone_parameter_numbers} for a clear comparison of the sizes of the related models.





\begin{table}[t!]
    \caption{\#Parameters (measured in millions) of involved models.}
    % \vspace{2.0mm}
    \label{tab:backbone_parameter_numbers}
    \centering
    \footnotesize
    \setlength{\tabcolsep}{1.5mm}{
        \begin{threeparttable}
        \begin{tabular}{ccccc}
        \toprule[1.1pt]
         & DV3Res50    & DV3Res101 & FCNVGG16 & DV3MOB      \\
        \midrule
        \#Parameters    & 23.5 & 42.5 & 134.5 & 2.2  \\
        \bottomrule[1.1pt]
        \end{tabular}
        \end{threeparttable}
    }
\end{table}

% Figure environment removed
% converge witDuring the training process of the FCN, we find that FCN is relatively quite time-consuming to do hyperparameter tunning for cityscapes since FCN is relatively simple. Since we also want to check the influence of the same structure with different backbones, we replace the FCN for cityscapes with Deeplabv3plus with Mobilenet as the backbone of the third model experiment on the cityscapes dataset.


% Figure environment removed
\subsection{Performance Comparison}
% \Mengqi{modify according to new results}
In Table~\ref{tab:pascalvoc_adversarial_attack} and Table~\ref{tab:adversarial_cityscape}, we show model performance of conventional and transferable attacks on the PascalVOC 2012 testing dataset and Cityscape dataset, respectively. We show both quality of the adversarial examples ($\text{PSNR}$ and $\text{SSIM}$), prediction accuracy of segmentation results ($\text{mIoU}$) as well as attack quality ($\text{S}_r$).
% In Table~\ref{tab:adversarial_cityscape} and Table~\ref{tab:transferable_models_cityscape}, for 
% Model performance on Cityscape dataset is shown in Table~\ref{tab:adversarial_cityscape}.
Both Table~\ref{tab:pascalvoc_adversarial_attack} and Table~\ref{tab:adversarial_cityscape} show that DAG~\cite{xie2017adversarial} produces attack with high image quality, and performs well on the source model, but fails to transfer to the target model, which is also consistent with the results in \cite{xie2017adversarial}. Among all the transferable attacks, namely NI~\cite{JiadongLin2019NesterovAG},
DI~\cite{CihangXie2018ImprovingTO},
TI~\cite{YinpengDong2019EvadingDT}, and AAI~\cite{zhu2022rethinking}, NI~\cite{JiadongLin2019NesterovAG} achieves overall the best transferability, although the image quality of its adversarial sample is inferior compared with DAG~\cite{xie2017adversarial}. Another observation from Table~\ref{tab:pascalvoc_adversarial_attack} and Table~\ref{tab:adversarial_cityscape} is that the existing transferable attacks transfer well across to the residual-connection models, \ie~Deeplabv3 and Deeplabv3+, while we find inferior transferability cross to the residual and non-residual based network, \ie~Deeplabv3 and FCN with VGG16 backbone. 
% (TO ADD IN)
% Replace the conventional convolution with dilate convolution and using spatial pyramid pooling, Deeplabv3 models have a larger reception field by compare to FCN and multi-sampling rate like FCN. 
% \Mengqi{}

\noindent\textbf{Model Robustness \wrt~Adversarial Attack:} For each testing dataset, with a specific source model, we want to evaluate model robustness \wrt~different adversarial attacks. Taking PascalVOC dataset for example, we show the mean IOU of the testing dataset after applying each adversarial attack with three network structures
% backbones\Mengqi{models might be proper} 
Fig.~\ref{fig:model_performance_wrt_conventional_ad_attack}.
% \footnote{Performance of Cityscape dataset is shown in the supplementary material.}.
Note that, the source and target model, in this case, are the same. Ideally, we want the attack to be effective and the model should perform poorly for the adversarial example. Fig.~\ref{fig:model_performance_wrt_conventional_ad_attack} shows that, for all the three network structures,
% backbones\Mengqi{models might be proper} , 
NI~\cite{JiadongLin2019NesterovAG} achieves the best attack performance with the lowest $\text{mIoU}$, and DAG~\cite{xie2017adversarial} is the best in image quality. The result in Fig.~\ref{fig:model_performance_wrt_conventional_ad_attack} makes sense in that NI~\cite{JiadongLin2019NesterovAG} is an improved momentum method~\cite{momentum_method}, which is more stable to train, leading to more effective adversarial example. DAG~\cite{xie2017adversarial} introduces a new objective (see Eq.~\ref{eq_dag_min_obj}) to explicitly force the model to produce inaccurate predictions. 



% Figure environment removed


We also show a visual comparison of different attacks in Fig.~\ref{pascal_voc_attack_performance}, which is based on the PascalVOC dataset with DV3Res50 backbone. The results show that the original prediction is reasonable compared with the ground truth. 
With FGSM~\cite{Explaining_and_Harnessing_Adversarial_Examples} attack, the foreground person can still be observed from the segmentation map. However, the plane has almost disappeared. IAA~\cite{zhu2022rethinking} is a distribution alignment-based transferable attack. In this source-to-source setting, its performance is inferior compared with DAG~\cite{xie2017adversarial} and NI~\cite{JiadongLin2019NesterovAG}, where in DAG~\cite{xie2017adversarial} the foreground is almost disappeared, and in NI~\cite{JiadongLin2019NesterovAG}, PGD~\cite{PGD_attack}, IAA~\cite{zhu2022rethinking}, DI~\cite{CihangXie2018ImprovingTO}, we can see the whole image have been classified to one object in the segmentation map, indicating a successful attack. Similarly, for SegPGD~\cite{gu2022segpgd}, we can see there is no object in the segmentation map, which can be seen as a very successful attack.
% although Segpgd does not have the lowest MIoU, the prediction is to go straight to the background.

Both Fig.~\ref{fig:model_performance_wrt_conventional_ad_attack} and Fig.~\ref{pascal_voc_attack_performance} indicate that deep semantic segmentation models are not robust to adversarial attack.
% Similarly, we show the performance of Deeplabv3+ and FCN backbone based models in Fig.~\ref{fig:model_performance_wrt_conventional_ad_attack} \enquote{Deeplabv3} and \enquote{FCN}






\noindent\textbf{Transferability of adversarial attack:} For a given network structure $F_i\in\{F_i\}_{i=1}^K$, where $K$ represents the number of model structures, we define it as the source model, and generate adversarial examples based on it to test robustness of other structures, e.g. $F_{j\neq i}$, with respect to the attacks based on $F_i$, thus we aim to evaluate transferability of adversarial attacks.
% is evaluated by feeding the adversarial examples from model $F_i$ to all the other models $\{F_i\}_{i=1}^K$.
% including the source model itself. The 
The success rate is analyzed to evaluate the transferability of adversarial attacks. In this paper, we have $K=3$ for each dataset-related model, and we report the success rate of each model on the Cityscape dataset in Fig.~\ref{fig:transferability_evaluation},
% We evaluate the transferability of each adversarial attack.
% \noindent\textbf{Performance on PascalVOC dataset:}
% For PascalVOC dataset, the source models are ResNet50 backbone based Deeplabv3 (DV3), ResNet101 backbone based Deeplabv3+ (DV3+) and VGG16 backbone based FCN8S-16 (FCN). The attack methods include the conventional adversarial attack, namely FGSM~\cite{Explaining_and_Harnessing_Adversarial_Examples}, PGD~\cite{PGD_attack}, SegPGD~\cite{gu2022segpgd} and DAG. 
% We evaluate the transferability of each attack, including both conventional attacks and transferable attacks on the Cityscape dataset. Model performance is shown in 
% Fig.~\ref{fig:transferability_evaluation}, 
where we show the range of success rate of each attack. As a transferable attack, we aim to obtain an attack that can lead to a high success rate across different target models. Fig.~\ref{fig:transferability_evaluation} shows that FGSM~\cite{Explaining_and_Harnessing_Adversarial_Examples} and PGD~\cite{PGD_attack} attacks show almost consistent success rates across different target models, although the success rates are around 50\%. As a transferable model, we found DI~\cite{CihangXie2018ImprovingTO} fails to transfer well on the Cityscape dataset in the cross-network structure setting,
% semantic segmentation \Mengqi{only for current setting in Cityscape, DI actually performs ok in PascalVOC's setting}, 
as its success rate is the lowest in general, however, its transferability on PascalVOC is reasonable, indicating the transferability should be discussed together with the dataset. Further, although DAG~\cite{xie2017adversarial} performs really well in the source model, its transferability on the Cityscape dataset is inferior compared with other conventional attacks. Among those attacks, NI~\cite{JiadongLin2019NesterovAG} still outperforms others to be the best transferable attack.
\cite{ilyas2019adversarial} has already shown that the image structure and feature bias will influence the transferability of the attack in the classification, which can be seen in our observation. Fig.~\ref{fig:transferability_evaluation} indicates that the transferability of adversarial attacks is a critical issue to evaluate model robustness. The current image classification based transferable attacks show limitations when they are directly adapted for semantic segmentation.

% Figure environment removed

% We argue that transferable attacks should perform relatively consistently, leading to a smaller variance of prediction. We thus compute the mean and variance of $S_r$ of each attack and fit it with the Gaussian distribution.\Mengqi{not gaussian distribution now}
% Model performance is shown in Fig.~\ref{fig:transferability_evaluation}. 





% \noindent\textbf{Transferable attack for semantic segmentation:}
% For PascalVOC dataset, we show performance in Table~\ref{tab:tranferable_models_pascalvoc}. For Cityscape, performance is shown in Table~\ref{tab:transferable_models_cityscape}.







% Figure environment removed

\subsection{Results Analysis}
\noindent\textbf{Image quality:}
We rank the adversarial attacks and transferable attacks based on their image quality (with SSIM) of the adversarial examples in Fig.~\ref{fig:model_ssim_wrt_conventional_ad_attack} on the Pascal VOC dataset. We argue that an effective adversarial attack should be visually invisible but significantly destroy model prediction, making adversarial image quality an important standard to decide the effectiveness of adversarial attack. Fig.~\ref{fig:model_ssim_wrt_conventional_ad_attack} shows that DAG~\cite{xie2017adversarial} leads to adversarial example with the highest $\text{SSIM}$, indicating its superiority in producing visually invisible attack. We argue one of the main reasons is that we set the iteration number to be small for fair comparison, where the adversarial example is still in an early stage of converging. Further, the final perturbation $\delta_{adv}$ of DAG~\cite{xie2017adversarial} is accumulated, which is then clipped before generating the adversarial example in practice, and both the above operations are beneficial for stable generating of the adversarial example. 

% We also For PSNR, DAG increases gradually according to the iterations. As for the  SegPGD, it has an intermediate value, and it goes down dramatically when the iteration goes larger. As for the TI, it stays at the lowest quality all the time.




\noindent\textbf{Performance of iterative attacks \wrt~iteration:}
In this paper, we set the number of iterations to 10 to achieve a trade-off between attack quality and efficiency. We further evaluate attack performance \wrt~iteration,
% \footnote{We show the performance of other iterative attacks \wrt~iteration in the supplementary materials.}, 
and show the results in Fig.~\ref{image_quality_wrt_iteration}.
We observe that a larger iteration brings minor influence to the image quality of DAG~\cite{xie2017adversarial}, while it decreases image quality significantly for SegPGD~\cite{gu2022segpgd}. 
For the mIoU, since the attack is considered to be effective when mIoU is lower, we replace it with $1-\text{mIoU}$ to indicate the effectiveness of the attacks. It shows that the attack performance of SegPGD~\cite{gu2022segpgd} and DAG~\cite{xie2017adversarial} increase with the iteration going up at the beginning, but SegPGD~\cite{gu2022segpgd} goes stable after 10 iterations while DAG~\cite{xie2017adversarial} still slightly increases. The main reason for the saturated performance of SegPGD~\cite{gu2022segpgd} is that the misclassified samples are reducing, leading to nearly zero gradients. Another observation is that we find the stable performance of TI~\cite{YinpengDong2019EvadingDT} with increased
% effectiveness stays stable when increasing the
iterations, and we attribute this to the selection of the kernel matrix $\mathbf{W}$ in Eq.~\ref{eq_ti_pgd_attack}, where the selection of the pre-defined kernel matrix $\mathbf{W}$ is the bottleneck of transferability of TI~\cite{YinpengDong2019EvadingDT}.
   
\subsection{Ensemble attack}
% Given the different transferability of existing transferable attacks for semantic segmentation (see Fig.~\ref{fig:transferability_evaluation}), we intend to combine them together to extensively explore their potential for transferable attack, which is also named \enquote{ensemble segmentation attack}. Different from~\cite{cai2023ensemblebased}, which introduces an ensemble of surrogate models, where weights of the involved models are optimized based on the victim model, we work on attack-ensemble, and generate attack based on an ensemble of different segmentation attacks. 
% \Jing{analysis \cite{cai2023ensemblebased}}.
% % \Mengqi{Adding the Ensemble analysis here}
% \cite{cai2023ensemblebased} have already done a kind of ensemble attack on segmentation which are try to ensemble multiple models. The key idea is that if an image with perturbation can fool multiple surrogate models at the same time it should perform better when it faces to unseen model. Instead of ensemble surrogate models, we directly ensemble the transfer methods.
% \Mengqi{do we have ensemble attack method on segmentation paper? I didn't find any, but if so we can add citations here}. 
% Specifically, we aggregate adversarial attacks from NI~\cite{JiadongLin2019NesterovAG},
% DI~\cite{CihangXie2018ImprovingTO} and
% TI~\cite{YinpengDong2019EvadingDT}, achieving ensemble semantic attack, and show performance in Table~\ref{tab:pascalvoc_adversarial_attack_esm} and Table~\ref{tab:cityscape_adversarial_attack_esm} for Pascal VOC and Cityscape dataset, respectively. To achieve this, \Jing{be clear how you achieve emsemble attack, with equations} we have:

         % \left( \mathbf{W} \nabla_{\mathbf{x}_{adv}^t} \mathcal{L}(\theta, T(\mathbf{x}_{adv}^t,p), \mathbf{y}) \right))
% \begin{equation}
% \label{eq_ni_di_ti_adversarial_attack}
%     \begin{aligned}
%         &\mathbf{x}^{t}= \mathbf{x}_{adv}^t+\alpha\mu \mathbf{g}^t\\
%         &\mathbf{g}^{t+1}=\mu\mathbf{g}^t + \frac{\mathbf{W} \cdot \nabla_{\mathbf{x}_{adv}^t} \mathcal{L}(\theta, T(\mathbf{x}_{adv}^t,p), \mathbf{y}) }{\left \|  \mathbf{W} \cdot \nabla_{\mathbf{x}_{adv}^t} \mathcal{L}(\theta, T(\mathbf{x}_{adv}^t,p), \mathbf{y}) \right \|_{1}}\\
%         &\mathbf{x}_{adv}^{t+1}=\text{CLIP}( \mathbf{x}_{adv}^t+\alpha \text{sign} ( \mathbf{g}^{t+1} ) ),
%     \end{aligned}
% \end{equation}
% where the transformation function $T(\cdot)$ performs image augmentation with transformation probability $p$, ie~with probability $p$, data augmentation will be applied to $\mathbf{x}_{adv}^t$, and with probability $1-p$, $T(\mathbf{x}_{adv}^t)$ remains $\mathbf{x}_{adv}^t$ ,the kernel matrix $\mathbf{W}$ is selected as a
% % , it can be represented using the
% Gaussian kernel. $\mathbf{g}^t$ is the accumulated gradient,
% % at iteration $t$, 
% $\mu$ is the decay rate of $\mathbf{g}^t$, $\mathbf{x}^t$ is the Nesterov accelerated result.

% we we try to way to weighted added 3 methods' gradients and also to use chain rules to calculate one single gradient. 
% We found that the latter way can have a better performance.
We show performance of the proposed ensemble attack in Table~\ref{tab:pascalvoc_adversarial_attack_esm} and Table~\ref{tab:cityscape_adversarial_attack_esm} for Pascal VOC and Cityscape dataset, respectively. 
The experimental results show that the ensemble method performs better than the strongest single method (NI~\cite{JiadongLin2019NesterovAG}), leading to better transferability.
The basic idea of the ensemble of those three attacks is to use more augmentation of the data with more invariant features, and also utilize the accelerated gradient to avoid the attack coming into a local minimal and overfit. And more deeply, we are trying to find a generalized direction for the gradient descent.
Also, there is another way to the ensemble attack, it ensemble multiple models and try to increase the transferability by using the multiple loss from the models.
However, since different models still can not cover all structure all the time, hence we adopt the first way of ensemble.

% \Jing{explain the possible reasons why your ensemble attack can achieve better performance.}

% From this observation, we can conclude that the ensemble method can have better transferability than the single attack.  

% NI~\cite{JiadongLin2019NesterovAG},
% DI~\cite{CihangXie2018ImprovingTO},
% TI~\cite{YinpengDong2019EvadingDT}, and AAI~\cite{zhu2022rethinking}, NI~\cite{JiadongLin2019NesterovAG} 

\begin{table*}[t!]
  \centering
  \footnotesize
  \renewcommand{\arraystretch}{1.2}
  \renewcommand{\tabcolsep}{4.25mm}
  \caption{Ensemble attack on Pascal VOC 2012 dataset.
  }
  \begin{tabular}{l|c|cc|cc|cc|cc}
  % \hline
  \toprule[1.1pt]
  &&\multicolumn{2}{c|}{Image Quality}&\multicolumn{6}{c}{Target (performance is evaluated with $\text{mIOU}\downarrow$ and $\text{Sr}\uparrow$)}\\ \hline
  Source & Attack & $\text{PSNR}\uparrow$ & $\text{SSIM}\uparrow$ & \multicolumn{2}{c|}{DV3Res50} & \multicolumn{2}{c|}{DV3Res101}& \multicolumn{2}{c}{FCNVGG16}  \\ \hline
  \multirow{2}{*}{DV3Res50} 
      &  NI~\cite{JiadongLin2019NesterovAG} & 32.84 & 0.5912 & 0.0861 & 0.8855 & 0.2610  &0.6650 & 0.4246 & 0.3478  \\ 
  % & DI ~\cite{CihangXie2018ImprovingTO} & 39.70 & 0.7793 & 0.0950 & 0.8737 &  0.2929 & 0.6241 &  0.5035 & 0.2266  \\ 
  % & TI ~\cite{YinpengDong2019EvadingDT} & 39.26 & 0.7808 & 0.0997 & 0.8675 & 0.3871 & 0.5031 & 0.5193 & 0.2023 \\ 
  & NI + DI + TI & 32.84 & \textbf{0.5961} & \textbf{0.0833} & \textbf{0.8893} & \textbf{0.2005} & \textbf{0.7427} & \textbf{0.3100} & \textbf{0.4657} \\
  
   % \hline
   \midrule[0.7pt]
   \multirow{2}{*}{DV3Res101}
    &  NI~\cite{JiadongLin2019NesterovAG} & 32.83 & 0.5925 & 0.2244 & 0.7017 & 0.0922 & 0.8817 & 0.4341  & 0.3332  \\ 
    % & DI ~\cite{CihangXie2018ImprovingTO} & 39.70 & 0.7791 & 0.2545 & 0.6617 & 0.0973 & 0.8751 &  0.5029 & 0.2275  \\ 
    % & TI ~\cite{YinpengDong2019EvadingDT} & 39.26 & 0.7805 & 0.3543 & 0.5290 & 0.1068  & 0.8629 & 0.5235& 0.1959  \\ 
& NI + DI + TI & \textbf{32.85} & \textbf{0.5982} & \textbf{0.1920} & \textbf{0.7448} & \textbf{0.0855} & \textbf{0.8903} & \textbf{0.3256} & \textbf{0.4882} \\
  
% \hline
\midrule[0.7pt]
  \multirow{2}{*}{FCNVGG16} 
   &  NI~\cite{JiadongLin2019NesterovAG} & \textbf{32.75} & 0.6018 & \textbf{0.1607} & \textbf{0.7864} & \textbf{0.2233} & \textbf{0.7134} &  0.0326 & 0.9499  \\ 
    % & DI ~\cite{CihangXie2018ImprovingTO} & 37.39 & 0.7492 & 0.1747 & 0.7677 & 0.2536  & 0.6745 &  0.0335 & 0.9485  \\ 
    % & TI ~\cite{YinpengDong2019EvadingDT} & 36.35 & 0.7382 & 0.2272 & 0.6384 & 0.3262  & 0.5813 & 0.0320 & 0.9508 \\ 
  & NI + DI + TI & 32.74 & \textbf{0.6072} & 0.1615 & 0.7853 & 0.2252 & 0.7109 & \textbf{0.0322} & \textbf{0.9505} \\
% \hline
\bottomrule[1.1pt]
  \end{tabular}
  \label{tab:pascalvoc_adversarial_attack_esm}
\end{table*}


\begin{table*}[t!]
  \centering
  \footnotesize
  \renewcommand{\arraystretch}{1.2}
  \renewcommand{\tabcolsep}{4.25mm}
  \caption{Ensemble attack on Cityscape dataset.
  }
  \begin{tabular}{l|c|cc|cc|cc|cc}
  % \hline
  \toprule[1.1pt]
  &&\multicolumn{2}{c|}{Image Quality}&\multicolumn{6}{c}{Target (performance is evaluated with $\text{mIOU}\downarrow$ and $\text{Sr}\uparrow$)}\\ \hline
  Source & Attack & $\text{PSNR}\uparrow$ & $\text{SSIM}\uparrow$ & \multicolumn{2}{c|}{DV3Res50} & \multicolumn{2}{c|}{DV3Res101}& \multicolumn{2}{c}{FCNVGG16}  \\ \hline
  \multirow{2}{*}{DV3Res50} 
    & NI~\cite{JiadongLin2019NesterovAG}  & 32.91 & 0.3729 & 0.0142 & 0.9817 & 0.1398  & 0.8169 & 0.1754 & 0.7590\\ 
   %  & DI ~\cite{CihangXie2018ImprovingTO}  & 39.90 & 0.6036 & 0.0178 & 0.9770 & 0.1925  & 0.7479 & 0.2580 & 0.6455  \\ 
    
   % & TI ~\cite{YinpengDong2019EvadingDT} & 39.70 & 0.6119 & \textbf{0.0103} & \textbf{0.9867} & 0.2562  & 0.6645 & 0.3231 & 0.5561 \\ 
  & NI + DI + TI & \textbf{32.95} & \textbf{0.3822} & \textbf{0.0141} & \textbf{0.9818} & \textbf{0.0892} & \textbf{0.8832} & \textbf{0.0790} & \textbf{0.8914} \\
  
   % \hline
   \midrule[0.7pt]
   \multirow{2}{*}{DV3Res101}
    & NI~\cite{JiadongLin2019NesterovAG}  & 32.91 & 0.3736 & \textbf{0.1102} & \textbf{0.8578} & \textbf{0.0176} & \textbf{0.9770} &  0.1665 & 0.7712  \\ 
    % & DI ~\cite{CihangXie2018ImprovingTO} & 39.88 & 0.6045 &  0.1544 & 0.8008 &  0.0268 & 0.9649 &  0.2683 & 0.6314 \\ 
    %    & TI ~\cite{YinpengDong2019EvadingDT}  & 39.69 & 0.6135 & 0.1984 & 0.7440 &  \textbf{0.0164} & \textbf{0.9785} & 0.3271 & 0.5506  \\
  & NI + DI + TI & \textbf{32.96} & \textbf{0.3855} & 0.1151 & 0.8515 & 0.0229 & 0.9700 & \textbf{0.1049} & \textbf{0.8559} \\
  
% \hline
\midrule[0.7pt]
  \multirow{2}{*}{DV3MOB} 
    & NI~\cite{JiadongLin2019NesterovAG}  & 32.81 & 0.3739 & 0.2976 & 0.6160 & 0.3458 & 0.5471 &  0.0109 & 0.9850 \\ 
    % & DI ~\cite{CihangXie2018ImprovingTO}  & 39.89 & 0.6035 & 0.3400 & 0.5613 & 0.4350  & 0.4303 &  0.0128 & 0.9824  \\ 
    %    & TI ~\cite{YinpengDong2019EvadingDT}  & 39.62 & 0.6137 & 0.3886 & 0.4986 & 0.4796 & 0.3719 & \textbf{0.0064} & \textbf{0.9912} \\
  & NI + DI + TI & \textbf{32.87} & \textbf{0.3854} & \textbf{0.1285} & \textbf{0.8342} & \textbf{0.2001} & \textbf{0.7380} & \textbf{0.0051} & \textbf{0.9930}\\
% \hline
\bottomrule[1.1pt]
  \end{tabular}
  \label{tab:cityscape_adversarial_attack_esm}
\end{table*}

% \subsection{Robustness Differences of Transformer and CNN}
% \Jing{Typically, transformer and CNN should have different robustness degree with respect to adversarial attack. analyse the different robustness of the two types of neural networks.}

% \cite{fu2022patch} find that the ViT is more vulnerable than the CNN when face with the patch adversarial attack but gets better performance under normal adversarial attack\cite{tang2022robustart}. They considered that it might be most of the transformer-based methods include the data augmentation method.
% \Mengqi{Do we actually need the experiment to compare results here?}
% \Mengqi{DO we need to discuss this}


% Figure environment removed


\subsection{Transferable Attack of Existing Vision Foundation Models}

With recent advancement of vision foundation models such as SAM~\cite{kirillov2023segany}, CLIP~\cite{DBLP:conf/icml/RadfordKHRGASAM21}, ImageBind~\cite{girdhar2023imagebind} and~\etc, more attentions have been paid to the transformer-based segmentation models. Compared with convolutional networks, generating adversarial examples through large transformer models is computationally expensive. Thus, if adversarial examples generated from a smaller model can transfer to larger models, we could significantly reduce the cost of generating adversarial examples for large vision foundation models. We perform a preliminary experiment on Segment Anything (SAM)~\cite{kirillov2023segany} with point prompts and show the result in Fig.~\ref{transfer_attack_sam}. To be specific, we consider three network architectures from SAM: ViT-B, ViT-L as well as ViT-H~\cite{li2022exploring} and use the smallest backbone ViT-B as the source model to generate the adversarial example. We craft the adversarial noise on ViT-B using NI~\cite{JiadongLin2019NesterovAG} with a noise budget of $12/255$ and 16 iterations. Objective of the attack is to remove the prediction mask from the clean example. We observe that the adversarial example generated from the ViT-B is able to generalize to ViT-L as well as ViT-H and the attack remains effective for different point prompt. Thus, existing vision foundation models are also vulnerable for transferable attacks, and generating adversarial examples from smaller models and transfer to larger models is more computational efficient.     
 
% \Jing{Briefly explain the robustness of large vision models, e.g. SAM. do not have to be detailed, only briefly explain that those large models are also vulnerable to adversarial attack.}
% \Mengqi{Do we actually need the experiment result here?}
% \Zhaoyuan{We have some initial results for transferable attacks on SAM, I can help on this part.}
% \Mengqi{Sure, We can discuss this morning}

% And the DAG has the highest effectiveness in this metric while the TI and DAG are close when the iteration goes higher.
% TI~\cite{YinpengDong2019EvadingDT} shows the highest transferability and is stable while it has the highest performance for successful attacks after transfer to another model. The SegPGD~\cite{gu2022segpgd} success rate increases dramatically when the iteration goes higher. The DAG~\cite{xie2017adversarial} has the lowest transferability and it slowly decreases when the iteration goes higher.

% As for attack performance, we find performance of both SegPGD~\cite{gu2022segpgd} and DAG~\cite{xie2017adversarial} saturate at some point.
% % Due to the good performance of DAG~\cite{xie2017adversarial} in maintain image quality, we ran it with more iterations, and found that the large iteration number can increase image quality of the adversarial example.
% % the SegPGD decreases the image quality instead. 
% As for the transfer attack method we adopted from classification, do not shows obvious change with respect to iterations.

% T=10 in this paper.

% evaluate T=2, 5, 10, 15, 20

% find 2 adversarial attacks which are easy to run: 1) segpgd, 2) ti, 3) dag
% \Mengqi{rewrite using up to 30 iteration version, and will add more analysis for the a, b, c}




% a) image quality performance with a curve with respect to iteration(testing)

% \subsection{More Observations}



% For SSIM, DAG has a relatively stable quality although got slightly wave when the iteration increased, with the highest quality as well.
% As for the SegPGD and TI, they got a similar trend as they show in the PSNR, where SegPGD decreases dramatically while TI stays the lowest.

% For the image quality with respect to iterations, it shows that the method design for the segmentation does have a different performance. While DAG quality slightly increases when increasing the iteration with a fixed total perturbation, the SegPGD decreases the image quality instead. As for the transfer attack  method we adopted from classification, do not shows obvious change with respect to iterations.




% segpgd 
% 2 36.15 0.6812
% 5 38.03 0.7289
% 10 33.16 0.6015
% 15 32.10 0.5674
% 20 31.58 0.5519
% 30 31.08 0.5372
% ti
% 2 30.18 0.5135
% 5 30.18 0.5136
% 10 30.07 0.5136
% 15 30.18 0.5136
% 20 30.18 0.5136
% 30 30.18 0.5136
% dag
% 2 43.60 0.9522
% 5 45.34 0.9430
% 10 46.09 0.9353
% 15 46.66 0.9385
% 20 47.16 0.9439
% 30 47.97 0.9528
% di
% 2 32.75 0.5883
% 5 35.10 0.6536
% 10 35.94 0.
% 15
% 20

% b) mIoU(mean IoU)- accuracy with respect to iteration (dv3 to dv3)

% For the mIoU, since the attack is considered to be effective when mIoU is lower, we replace it will $1-mIoU$ to indicate the effectiveness of the algorithm. It shows that both SegPGD and DAG increase with the iteration going up at the beginning, but the SegPGD goes stable after 10 iterations while DAG still slightly increases after that. The TI's effectiveness stays stable when increasing the iterations. And the DAG has the highest effectiveness in this metric while the TI and DAG are close when the iteration goes higher.





% segpgd 
% 2 0.6401
% 5 0.3319
% 10 0.3357
% 15 0.3375
% 20  0.3383
% 30 0.3393
% ti
% 2 0.3340
% 5 0.3341
% 10 0.3350
% 15 0.3341
% 20 0.3360
% 30 0.3341
% dag
% 2  0.4641
% 5  0.2356
% 10 0.1388
% 15 0.1068
% 20 0.0930
% 30 0.0696
% di
% 2  0.3512
% 5 0.3249
% 10 
% 15
% 20

% 3) Sr(success rate)--transferability with respect to iteration (dv3 to fcn)

% TI shows the highest transferability and is stable while it has the highest performance for successful attacks after transfer to another model. The SegPGD success rate increases dramatically when the iteration goes higher. The DAG has the lowest transferability and it slowly decreases when the iteration goes higher.

% segpgd 
% 2 0.0063
% 5 0.1442
% 10 0.2253
% 15 0.2478
% 20 0.2583
% 30 0.2686
% ti
% 2 0.3725
% 5 0.3725
% 10 0.3725
% 15 0.3725
% 20 0.3725
% 30 0.3725
% dag
% 2 0.0653
% 5 0.0892
% 10 0.0796
% 15 0.0682
% 20 0.0600
% 30 0.0506

% \subsection{Discussion}

% \noindent\textbf{Augmentation based attack and others:}
% We compare the augmentation-based transferable attack with another transferable attack.

% \noindent\textbf{Iterative attack and single-step attack:}
% We compare the single-step attack with the iterative attack on transferability.
\section{Conclusion}
We investigate transferability attack for semantic segmentation,
% to evaluate model robustness 
\ie~adversarial attack across different network structures. We observe that semantic segmentation models are vulnerable to adversarial attack, and minor
% invisible 
perturbation can lead to significant decrease in model performance.
% destroy model predictions. 
We find that directly enforcing the model to produce pixel-wise wrong prediction can produce effective attack for the source model, \ie~DAG~~\cite{xie2017adversarial},
% can maintain sample quality and achieve effective attack for the source model, 
but it cannot transfer well to other models of different structures, indicating lower transferability. As a transferable attack, NI~\cite{JiadongLin2019NesterovAG} achieves the best attack transferability, however, its sample quality is inferior to DAG~\cite{xie2017adversarial}. Table~\ref{tab:pascalvoc_adversarial_attack} and \ref{tab:adversarial_cityscape} also reveal that adversarial attack generated from larger model (see Table~\ref{tab:backbone_parameter_numbers}) can transfer better than that from smaller models, \ie~DV3MOB~\cite{sandler2018mobilenetv2}.
We identify the easier path to generate transferable attack is defining the source model as a residual connected based one, which transfers better to non-residual counterparts than the other way around.
% Although network parameters various (see Table~\ref{tab:backbone_parameter_numbers}), we find 
% one main obstacle of transferable attack is model structure, \ie~residual or non-residual, where 
% , instead of model parameters. 
We find dataset is also an important issue. IAA~\cite{zhu2022rethinking} explore transferable attack from data distribution's perspective. However, due to the implementation gap, our results fail to validate its effectiveness. We believe directly targeting wrong predictions, considering both network structure differences and data distribution gaps should be the top three factors to consider for transferable attack. Further, our investigation on ensemble attack and transferable attack for vision foundation model reveal the potential of combining different attacks for a more robust attack, and the vulnerability of existing vision foundation models, strengthening the necessity for more research on transferable attacks.
%%%%%%%%% REFERENCES
{
\bibliographystyle{IEEEtran}
\bibliography{egbib}
}

%%%%%%%%% Biography
%\bf{If you will not include a photo:}\vspace{-33pt}
% \begin{IEEEbiographynophoto}{John Doe}
% Use $\backslash${\tt{begin\{IEEEbiographynophoto\}}} and the author name as the argument followed by the biography text.
% \end{IEEEbiographynophoto}

\end{document}


