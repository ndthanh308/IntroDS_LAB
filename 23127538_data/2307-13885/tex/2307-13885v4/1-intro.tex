\section{Introduction}

A desirable attribute of machine learning models is robustness to perturbations of input data. One common notion of robustness is adversarial robustness, the ability of a model to maintain its prediction when presented with adversarial perturbations, i.e., perturbations designed to cause the model to change its prediction. Although adversarial robustness identifies whether a misclassified example exists in a local region around an input, real-world noise (such as measurement noise) is rarely adversarial and often random. The effect of such noise on model predictions is better captured by another notion of robustness: \emph{average-case robustness}, i.e., the fraction of points in a local region around an input for which the model provides consistent predictions. If this fraction is less than one, then an adversarial perturbation exists. In this sense, average-case robustness is a strict generalization of adversarial robustness. Adversarial robustness only detects the presence of a single misclassified instance, a limitation due to its origins in model security. In contrast, average-case robustness offers a more comprehensive view of model behavior. Because we are interested in the applicability of robustness to understand model behavior, help debugging efforts, and improve model generalization via regularization in real-world settings, this paper studies average-case robustness. For convenience, in the rest of this paper we use ``average-case robustness'' and ``local robustness'' interchangeably.
% In this sense, average-case robustness is a strict generalization of adversarial robustness. The limitation of adversarial robustness in only detecting the presence of a single misclassified instance is due to its origins in model security. Because this paper is more interested in the applicability of robustness to understanding model behaviour, help debugging efforts, and regularization for improving model generalization in real-world settings, this paper studies average-case robustness as it offers a more comprehensive view of model behaviour. For brevity, we shall henceforth use "local robustness" and "average-case robustness" interchangeably.

In this paper, we take the first steps towards estimating average-case robustness for large-scale multi-class classifiers. We show that the na√Øve approach to estimating average-case robustness based on Monte-Carlo sampling is statistically inefficient: obtaining an accurate estimate of local robustness using this approach requires a large number of samples from the local region. This inefficiency is exacerbated in the case of high-dimensional data, leading to prohibitive computational costs for large-scale applications. Our work makes the following contributions:

\begin{enumerate}
    \item We derive novel analytical estimators to efficiently compute the average-case robustness of multi-class classifiers. We also provide estimation error bounds for these estimators that characterizes the errors incurred due to such linearization. 

    \item We empirically validate these analytical estimators using standard deep learning models and datasets, demonstrating that these estimators accurately and efficiently compute robustness.

    \item We demonstrate the usefulness of our analytical estimators for various tasks involving average-case robustness, such as measuring class-level robustness bias \cite{nanda2021fairness} (i.e., a model being more locally robust for some classes than for others) and identifying non-robust samples in a dataset. 
\end{enumerate}


To our knowledge, this work is the first to investigate average-case robustness for the multi-class setting and develop efficient analytical estimators. In addition, the efficiency of these estimators makes the computation of average-case robustness practical, especially for large deep neural networks.





