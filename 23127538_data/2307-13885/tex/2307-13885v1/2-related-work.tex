\section{Related Work}

\textbf{Linearization of neural networks.} Prior works have used local linear function approximation to obtain feature attributions \cite{ribeiro2016lime, han2022explanation} or counterfactual explanations for binary classifiers \cite{pawelczyk2022probabilistically}. In contrast to these prior works which apply local linear function approximation to post hoc explainability, this work applies it to local robustness and uses it to develop analytical estimators for binary and multi-class classification.

\textbf{Adversarial robustness.} Prior works have proposed methods to generate adversarial attacks \cite{carlini2017towards, goodfellow2014explaining, moosavi2016deepfool} and to provide dataset-level guarantees of model robustness \cite{cohen2019certified, carlini2022certified}. In contrast to these prior works on adversarial robustness, this work investigates local robustness, a generalization of adversarial robustness. Prior work has also studied robustness bias in terms of vulnerability to adversarial attacks \cite{nanda2021fairness}. In contrast, this work investigates robustness bias in terms of local robustness.
% dataset-level
% , directly calculating a model's local robustness.
% lowerbounding and upperbounding a point's probability of adversarial attack .
% using sampling approaches
% using analytical approaches
%directly calculating the probability that a modelâ€™s prediction remains consistent in the local region around a point.

\textbf{Uncertainty estimation.} Prior works have developed approaches to measuring prediction uncertainty, including calibration \cite{guo2017calibration}, Bayesian uncertainty \cite{kendall2017uncertainties}, and conformal prediction \cite{shafer2008tutorial}. In contrast to these prior works in which uncertainty is with respect to a calibration set or model parameters, this work investigates local robustness, which can be thought of as uncertainty with respect to input noise.