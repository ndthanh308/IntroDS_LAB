\section{Introduction}

A desirable attribute of machine learning models is robustness to perturbations of input data. One common notion of robustness is adversarial robustness, a model's ability to maintain its prediction under adversarial perturbations. Although adversarial robustness can identify whether an adversarial perturbation exists, real-world noise (e.g., measurement noise) is rarely adversarial and often random. The effect of such noise on model predictions is captured by \emph{local robustness}, the fraction of points in a local region around an input for which the model provides consistent predictions. This is a generalization of adversarial robustness -- if this fraction is less than 1, then an adversarial perturbation exists. By capturing model behavior under average case noise, local robustness provides a more comprehensive characterization of real-world model behavior.
%\suraj{From a security viewpoint, advex is great, but from a model understanding and debugging perspective, not so great}
% The limitation of adversarial robustness in only detecting whether an adversarial perturbation exists is perhaps expected, as its original use-case was motivated by model security, not model understanding, debugging, or regularization for improved generalization. 

In this paper, we take the first steps towards measuring local robustness. We show that the na√Øve approach to estimating local robustness is statistically inefficient, leading to prohibitive computational costs for large-scale applications. To address this problem, we develop the first analytical estimators to efficiently compute local robustness. Specifically:

\begin{enumerate}
    \item We derive a set of novel analytical estimators to efficiently compute the local robustness of multi-class discriminative models using local linear function approximation and the multivariate Normal CDF. Through the derivation, we show how local robustness is connected to randomized smoothing and softmax probability.

    \item We empirically confirm that these analytical estimators accurately and efficient compute the local robustness of standard deep learning models.

    \item We demonstrate the usefulness of our estimators for various tasks involving local robustness, such as measuring robustness bias and identifying examples that are vulnerable to noise perturbation. Such dataset-level analyses of local robustness are made practical only by having these efficient analytical estimators.
\end{enumerate}


To our knowledge, this work is the first to investigate local robustness in a multi-class setting and develop efficient analytical estimators. The analytical aspect of these estimators not only advances conceptual understanding of local robustness, but also enables local robustness to be used in applications that require differentiability. The efficiency of these estimators makes the computation of local robustness practical, enabling tasks that assist in such important objectives as debugging models and establishing user trust.




