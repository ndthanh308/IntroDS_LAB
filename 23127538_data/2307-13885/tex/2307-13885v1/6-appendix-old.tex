\section{Appendix}

\subsection{Proofs}
\label{app:proofs}

\subsection{Datasets}
\label{app:datasets}

The MNIST dataset consists of images of gray-scale handwritten digits. The images span 10 classes: digits 0 through 9. Each image is of size 28 pixels x 28 pixels. The training set consists of 60,000 images and the test set consists of 10,000 images.

The FashionMNIST dataset consists of gray-scale images of articles of clothing. The images span 10 classes: t-shirt, trousers, pullover, dress, coat, sandal, shirt, sneaker, bag, and ankle boot. Each image is of size 28 pixels x 28 pixels. The training set consists of 60,000 images and the test set consists of 10,000 images.

The CIFAR10 dataset consists of color images of common objects and animals. The images span 10 classes: airplane, car, bird, cat, deer, dog, frog, horse, ship, and truck. Each image is of size 3 pixels x 32 pixels x 32 pixels. The training set consists of 50,000 images and the test set consists of 10,000 images.

The CIFAR100 dataset consists of color images of common objects and animals. The images span 100 classes: apple, bowl, chair, dolphin, lamp, mouse, plain, rose, squirrel, train, etc. Each image is of size 3 pixels x 32 pixels x 32 pixels. The training set consists of 50,000 images and the test set consists of 10,000 images.

For experiments, we use 1,000 randomly-selected test set images for each dataset.


\subsection{Models}
\label{app:models}

For the MNIST and FashionMNIST (FMNIST) datasets, we train a linear model and a convolutional neural network (CNN) to perform 10-class classification. The linear model consists of one hidden layer with 10 neurons. The CNN consists of four hidden layers: one convolutional layer with 5x5 filters and 10 output channels, one convolutional layer 5x5 filters and 20 output channels, and one linear layer with 50 neurons, and one linear layer 10 neurons. 

For CIFAR10 and CIFAR100 datasets, we train a ResNet18 model to perform 10-class and 100-class classification, respectively. The model architecture is described in \citep{he2016deep}. We train the ResNet18 models using varying levels of gradient norm regularization to obtain models with varying levels of robustness. The larger the weight of gradient norm regularization ($\lambda$), the more robust the model.

All models were trained using stochastic gradient descent. Hyperparameters were selected to achieve decent model performance. The emphasis is on analyzing the estimators’ estimates of local robustness of each model, not on high model performance. Thus, we do not focus on tuning model hyperparameters. All models were trained for 200 epochs. The test set accuracy (on each dataset's full 10,000-point test set) for each model is shown in Table~\ref{table:app-model-acc}.

\begin{table}[ht!]
    \centering
    \begin{tabular}{c|c|c|c}
    Dataset      & Model  & $\lambda$  & Test set accuracy \\
    \midrule
    MNIST        & Linear  & 0 & 92\%                         \\
    MNIST        & CNN     & 0 & 99\%                         \\
    \midrule
    FashionMNIST & Linear  & 0 & 84\%                         \\
    FashionMNIST & CNN     & 0 & 91\%                         \\
    \midrule
    CIFAR10      & ResNet18 & 0 & 94\%                         \\
    CIFAR10      & ResNet18 & 0.0001 & 93\%                         \\
    CIFAR10      & ResNet18 & 0.001 & 90\%                         \\
    CIFAR10      & ResNet18 & 0.01 & 85\%                         \\
    \midrule
    CIFAR100     & ResNet18 & 0 & 76\%                        \\
    CIFAR100     & ResNet18 & 0.0001 & 74\%                         \\
    CIFAR100     & ResNet18 & 0.001 & 69\%                         \\
    CIFAR100     & ResNet18 & 0.01 & 60\%                         
    \end{tabular}
    \vspace*{3mm}
    \caption{Accuracy of models on test set.}
    \label{table:app-model-acc}
\end{table}


\clearpage
\subsection{Experiments}
\label{app:experiments}

\subsubsection{Convergence of \boldmath \pmc{}}

%appendix/a_convergence_pmc = 02a2_delta_p_emp_convergence_n50000_baseline
% Figure environment removed



\subsubsection{Convergence of \boldmath \pmmse{}}

%appendix/b_convergence_pmmse = 02c2_delta_p_mmse_convergence_n1000_baseline
% Figure environment removed


\clearpage
\subsubsection{Distribution of \boldmath \probust{} over noise}

%appendix/c_probust_over_noise = 02b_p_over_noise/p_all_over_noise
% Figure environment removed



\subsubsection{Accuracy of \boldmath \probust{} estimators}

%appendix/d_accuracy_of_estimators = 02e_pemp_vs_pothers_over_noise/rel
% Figure environment removed



\clearpage
\subsubsection{Accuracy of \boldmath \probust{} estimators for robust models}

%appendix/e_accuracy_of_estimators_robust_models = 02l_pemp_vs_pmmse_over_noise_robust_models/rel
% Figure environment removed


\subsubsection{mv-sigmoid function's approximation of mvn-cdf function}

%appendix/f_mvsigmoid_vs_mvncdf = 03_mvncdf_vs_mvsigmoid
% Figure environment removed

\subsubsection{Local robustness bias among classes}

%appendix/g_robustness_bias = 02f_p_all_over_classes/p_all_over_classes
% Figure environment removed

\clearpage
\subsubsection{Runtimes of \probust{} estimators}

\begin{table}[ht!]
\begin{tabular}{l|l|l|l|l|l}
    \multicolumn{2}{c}{}   & \multicolumn{2}{|c|}{CPU: Intel x86\_64}   & \multicolumn{2}{|c}{GPU: Tesla V100-PCIE-32GB} \\
    \midrule
    Estimator   & \# samples ($n$)   & Serial   & Batched   & Serial   & Batched \\
    \midrule
    \pmc{}   & \begin{tabular}[c]{@{}l@{}}$n=100$\\ $n=1000$\\ $n=10000$\end{tabular}               
             & \begin{tabular}[c]{@{}l@{}}0:00:59\\ 0:09:50\\ \textit{1:41:11}\end{tabular}                                               
             & \begin{tabular}[c]{@{}l@{}}0:00:42\\ 0:07:22\\ \textit{1:14:38}\end{tabular}                                                
             & \begin{tabular}[c]{@{}l@{}}0:00:12\\ 0:02:00\\ \textit{0:19:56}\end{tabular}                                                
             & \begin{tabular}[c]{@{}l@{}}0:00:01\\ 0:00:04\\ \textit{0:00:35}\end{tabular} \\
    \midrule
    \ptaylor{}   & N/A
                 & 0:00:08                                                                                                                      
                 & 0:00:07                                                                                                                      
                 & 0:00:02                                                                                                                      
                 & $<$ 0:00:01 \\
    \midrule
    \ptaylormvs{}   & N/A
                    & 0:00:08                                                                                                                   
                    & 0:00:07                                                                                                                  
                    & 0:00:01                                                                                                                   
                    & $<$ 0:00:01 \\
    \midrule
    \pmmse{}   & \begin{tabular}[c]{@{}l@{}}$n=1$\\ $n=5$\\ $n=10$\\ $n=25$\\ $n=50$\\ $n=100$\end{tabular} 
               & \begin{tabular}[c]{@{}l@{}}0:00:08\\ \textit{0:00:41}\\ 0:01:21\\ 0:03:21\\ 0:06:47\\ 0:13:57\end{tabular} 
               & \begin{tabular}[c]{@{}l@{}}0:00:10\\ \textit{0:00:31}\\ 0:01:02\\ 0:02:44\\ 0:05:38\\ 0:11:31\end{tabular} 
               & \begin{tabular}[c]{@{}l@{}}0:00:02\\ \textit{0:00:06}\\ 0:00:11\\ 0:00:26\\ 0:00:51\\ 0:01:42\end{tabular} 
               & \begin{tabular}[c]{@{}l@{}}0:00:02\\ \textit{0:00:02}\\ 0:00:02\\ 0:00:03\\ 0:00:04\\ 0:00:06\end{tabular} \\
    \midrule
    \pmmsemvs{}   & \begin{tabular}[c]{@{}l@{}}$n=1$\\ $n=5$\\ $n=10$\\ $n=25$\\ $n=50$\\ $n=100$\end{tabular} 
                  & \begin{tabular}[c]{@{}l@{}}0:00:08\\ \textit{0:00:41}\\ 0:01:21\\ 0:03:24\\ 0:06:47\\ 0:13:28\end{tabular} 
                  & \begin{tabular}[c]{@{}l@{}}0:00:08\\ \textit{0:00:32}\\ 0:01:00\\ 0:02:37\\ 0:05:35\\ 0:11:32\end{tabular} 
                  & \begin{tabular}[c]{@{}l@{}}0:00:01\\ \textit{0:00:05}\\ 0:00:10\\ 0:00:25\\ 0:00:51\\ 0:01:42\end{tabular} 
                  & \begin{tabular}[c]{@{}l@{}}0:00:01\\ \textit{0:00:01}\\ 0:00:02\\ 0:00:02\\ 0:00:03\\ 0:00:06\end{tabular} \\
    \midrule
    \psoftmax{}   & N/A                                                                             
                  & 0:00:01                                                                                                                              
                  & $<$ 0:00:01                                                                                                                              
                  & $<$ 0:00:01                                                                                                                              
                  & $<$ 0:00:01                                                                                                                             
\end{tabular}
\caption{Runtimes of each \probust{} estimator. Each estimator computes \probustwsigma{0.1} for the CIFAR10 ResNet18 model for 50 data points. For estimators that use sampling, the row with the minimum number of samples necessary for convergence is italicized. The analytical estimators (\ptaylor{}, \ptaylormvs{}, \pmmse{}, and \pmmsemvs{}) are more efficient than the naïve estimator (\pmc{}). Runtimes are in the format of hour:minute:second.}
\end{table}







% *********** OLD STUFF BELOW ***********

% \subsubsection{Convergence of \boldmath \pmc{} and \boldmath \pmmse{}}
% \label{app:exp-convergence-pmc-pmmse}

% \textbf{Convergence of \boldmath \pmc{}.} As the number of noisy samples increases, \pmc{} converges. The results indicate that \pmc{} converges at around 10,000 noisy samples.


% \textbf{Convergence of \boldmath \pmmse{}.} As the number of noisy samples increases, \pmmse{} converges. The results indicate that \pmmse{} converges at around 5-10 noisy samples.


% \subsubsection{Performance of method}
% \label{app:exp-method-performance}

% Experiment 1

% The proposed method properly estimates \probust{}.

% As expected, when the model is linear, estimators \ptaylor{} and \pmmse{} (which use a linear approximation of the model) perfectly approximate \probust{}.

% Metric 1: absolute differences

% Metric 2: relative differences

% robust models


% \textcolor{red}{below: need to update}

% \subsubsection{Local robustness vs. noise}
% \label{app:exp-pmc-vs-noise}

% Experiment 1

% As the noise neighborhood increases, local robustness deteriorates.




% \subsubsection{Correlation of \boldmath \probust{} and \boldmath \psoftmax{}}
% \label{app:exp-correlation-pmmse-psm}

% Experiment 3

% \probust{} and \psoftmax{} are not very correlated



% \subsubsection{Local robustness bias among classes}
% \label{app:exp-robustness-bias}

% Experiment 4

% fixed sigma

% fixed confidence level


% \subsubsection{Top vs. bottom images}
% \label{app:exp-topk-vs-bottomk}

% Experiment 5

% graphs

% topk vs. bottomk based on \probust{} (estimated by \pmmse{})

% topk vs. bottomk based on \psoftmax{}
