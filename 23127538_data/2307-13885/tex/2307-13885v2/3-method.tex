\section{Our Framework: A Family of Local Robustness Estimators}
\label{sec:methods}

\newcommand{\E}{\mathop{\mathbb{E}}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\X}{\mathbf{x}}
\newcommand{\grad}{\nabla_{\X}}
\newcommand{\cdf}{\text{CDF}_{\mathcal{N}(0, UU^\top)}}

\newtheorem{defn}{Definition}
\newtheorem{thm}{Proposition}
\newtheorem{lemma}{Lemma}


\newenvironment{hproof}{%
  \renewcommand{\proofname}{Proof Idea}\proof}{\endproof}

In this section, we first describe the mathematical problem of local robustness estimation. Then, we present the naïve estimator based on sampling and derive more efficient analytical estimators. In the process, we show how local robustness is connected to randomized smoothing. Lastly, we explore the connections between local robustness and softmax probability.

\subsection{Notation and Preliminaries}

Assume that we have a neural network $f: \R^d \rightarrow \R^C$ with $C$ output classes and that the classifier predicts class $t \in [1,..., C]$ for a given input $\X \in \R^d$, i.e., $t~=~\arg \max_{i=1}^{C} f_i(\X)$, where $f_i$ denotes the logits for the $i^{th}$ class. Given this classifier, the local robustness estimation problem is to compute the probability of consistent classification (to class $t$) under noise perturbation of the input. 

\begin{defn} We define the average \underline{local robustness} of a classifier $f$ at a point $\X$ as the probability of $\X$ being classified to class $t$ when Normal noise $\mathcal{N}(0, \sigma^2)$ is added to $\X$, denoted as
\begin{align*}
    p^\text{robust}_{\sigma}(\X, t) = P_{\epsilon \sim \mathcal{N}(0,\sigma^2)} \left[ \arg\max_i f_i(x+ \epsilon) = t \right]
\end{align*}
\end{defn}

The higher \probust{}($\X, t$) is, the more robust the model is in the local neighborhood around $\X$. In this paper, given that local robustness is always measured with respect to the predicted class~$t$ at $\X$, we henceforth suppress the dependence on $t$ in the notation.

Note that \emph{\probust{} generalizes adversarial robustness}. Adversarial robustness detects the presence or absence of a perturbation in a local neighborhood that leads to misclassification, while local robustness computes the probability of consistent classification. In other words, adversarial robustness is concerned with the quantity $\mathbf{1}($\probust{}$ < 1)$, i.e., the indicator function that local robustness is less than one (which indicates the presence of an adversarial perturbation), while local robustness is concerned with the quantity \probust{} itself. In the rest of this section, we derive estimators for \probust{}.



\subsubsection{Estimator 0: The Monte-Carlo Estimator \boldmath \pmc{}}

A naïve estimator of average local robustness is the Monte-Carlo estimator \pmc{}. It computes the local robustness of a classifier $f$ at input $\X$ by generating $M$ noisy samples of $\X$ and then calculating the fraction of these noisy samples that are classified to the same class as $\X$. In other words,

\begin{align*}
    p_{\sigma}^\text{robust}(\X) &= P_{\epsilon \sim \mathcal{N}(0,\sigma^2)} \left[ \arg\max_i f_i(x+ \epsilon) = t \right] 
    = \E_{\epsilon \sim \mathcal{N}(0,\sigma^2)} \left[ \mathbf{1}_{\arg\max_i f_i(x+ \epsilon) = t} \right] \\ &\approx \frac{1}{M} \sum_{j=1}^{M} \left[ \mathbf{1}_{\arg\max_i f_i(x+ \epsilon_j) = t} \right]
    = p_{\sigma}^\text{mc}(\X)
\end{align*}

\pmc{} replaces the expectation with the sample average of the $M$ noisy samples of $\X$ and has been used in prior work \cite{nanda2021fairness}. While Monte-Carlo estimators are technically independent of dimensionality \cite{vershynin2018high}, in practice, for typical use cases involving neural networks, this estimator requires a large number of random samples to converge to the underlying expectation. For example, for MNIST and CIFAR10 CNNs, it takes around $M = 10,000$ samples per point for \pmc{} to converge, which is computationally inefficient. Thus, we set out to address this problem by developing more efficient analytical estimators of local robustness.
% This is an unbiased estimator, and while Monte-Carlo estimators are technically independent of dimensionality \cite{vershynin2018high}, in practice we find that for practical use cases involving neural networks, this estimator requires a large samples to converge. 
%\subsection{\boldmath \ptaylor{} and \boldmath \pmmse{} are efficient estimators of \boldmath \probust{}}

%- \ptaylor{}: derived binary version, and later found it was independently derived and used in XAI (martin's paper); extension to multiclass is novel and non-trivial 

%- \pmmse{}: connections to smoothgrad + empirically, we observe that 5-10 samples is sufficient for convergence

%- Gaussian noise assumption: also valid for other types of noise when dimensions are high because of central limit theorem

\subsection{Analytical Estimators of Local Robustness}

Our goal is to derive analytical estimators which reduce the complexity of estimating local robustness. To this end, we first locally linearize non-linear models and then compute the local robustness of the resulting linear models. However, even the problem of computing the local robustness of linear models is more challenging than it appears due to the complex geometry of linear decision boundaries given $C$ classes. In particular, the relative orientation and similarities of these class-wise decision boundaries needs to be taken into account to compute local robustness. 

Specifically, given a linear model for a three-class classification problem with weights $w_1, w_2, w_3$ and biases $b_1, b_2, b_3$, such that $y = \arg \max_i \{w_i^\top\X + b_i \mid i \in [1,2,3] \}$, the decision boundary between classes $1$ and $2$ is given by $y_{12} = (w_1 - w_2)^\top \X + (b_1 - b_2)$. This can be verified as follows: for any $\X$ such that $y_{12}=0$, we have $w_1^\top\X + b_1 = w_2^\top\X + b_2$, making it the decision boundary between classes 1 and 2. Thus, the relevant quantities are the pairwise difference terms among the weights and biases which characterize the decision boundaries. We take this into account and provide the expression for the linear case below. 

\begin{lemma}
The local robustness of a multi-class linear model $f(\X) = \mathbf{w}^\top \X + b$ (with $\mathbf{w} \in \R^{d \times C}$ and $b \in \R^C$) at point $\X$ with respect to a target class $t$ is given by the following. Define weights $w'_i = w_t - w_i \in \R^d, \forall i \neq t$, where $w_t, w_i$ are rows of $\mathbf{w}$ and biases $b'_i = {(w'_t - w'_i)}^\top\X + (b_t - b_i) \in \R$. Then, 
\begin{align*}
    p^\text{robust}_\sigma(\X) = \cdf \left( \frac{b'_1}{\sigma \| w'_1 \|_2}, ...\frac{b'_i}{\sigma \| w'_i \|_2}, ... \frac{b'_C}{\sigma \| w'_C \|_2} \right)\\
    \text{where}~~~~U = \left[ \frac{w'_1}{\| w'_1 \|_2}; ...\frac{w'_i}{\| w'_i \|_2};...\frac{w'_C}{\| w'_C \|_2} \right] \in \R^{(C-1) \times d}
\end{align*}
and $\cdf$ refers to the ($C-1$)-dimensional Normal CDF with covariance $UU^\top$.
\label{estimator-linear-models}
\end{lemma}


%\begin{hproof}
%    We first note that $w'_i = w_t - w_i$ represents the decision boundary between classes $i,t$. Thus for $C$ classes we have $C-1$ decision boundaries. \suraj{complete this}
%\end{hproof}

The proof is in Appendix~\ref{app:proofs}. The matrix $U$ exactly captures the geometry of the linear decision boundaries and the covariance matrix $UU^\top$ encodes the relative similarity between pairs of decision boundaries. If the decision boundaries are all orthogonal to each other, then the covariance matrix is the identity matrix. However, in practice, we find that the covariance matrix is strongly non-diagonal, indicating that the decision boundaries are not orthogonal to each other.

For diagonal covariance matrices, the multivariate Normal CDF (\emph{mvn-cdf}) can be written as the product of univariate Normal CDFs, which would be easy to compute. However, the strong non-diagonal nature of covariance matrices in practice leads to the resulting \emph{mvn-cdf} not having a simple closed form solution, with the only alternative being approximation of the integral via sampling \cite{botev2017normal, SciPy}. However, this sampling is performed in the $(C-1)$-dimensional space as opposed to the $d$-dimensional space that \pmc{} performs. In practice, for classification problems, we often have $C << d$, making sampling in $(C-1)$-dimensions more efficient. 


\subsubsection{Estimator 1: The Taylor Estimator \boldmath \ptaylor{}}

Using the estimator derived for multi-class linear models in Lemma \ref{estimator-linear-models}, we now derive the Taylor estimator, a local robustness estimator for non-linear models.


\begin{thm}
    The Taylor estimator for the local robustness of a classifier $f$ at point $\X$ with respect to target class $t$ is given by linearizing $f$ around $\X$ using $\mathbf{w} = \grad f(\X)$ and $b = f(\X)$, with decision boundaries $g_i(\X) = f_t(\X) - f_i(\X)$, $\forall i \neq t$, leading to
    \begin{align*}
        p^\text{taylor}_{\sigma}(\X) = \text{CDF}_{\mathcal{N}(0, UU^{\top})} (\left[  \frac{g_1(\X)}{\sigma \|\grad g_1(\X)\|_2},  ...\frac{g_i(\X)}{\sigma \|\grad g_i(\X)\|_2}, ... \frac{g_C(\X)}{\sigma \|\grad g_C(\X) \|_2} \right]) 
    \end{align*}
    with $U \in \R^{(C-1) \times d}$ defined as in the linear case.
\label{eqn:taylor-estimator}
\end{thm}

The proof is in Appendix~\ref{app:proofs}. The smaller the local region around $\X$ (i.e., the smaller the $\sigma$), the more faithful the linearization of $f$ at $\X$, and thus the more accurate the Taylor estimator.

% As the Taylor linearization is more faithful locally to the data point, we expect the Taylor estimator to be accurate for small $\sigma$ values in \probust{}.



\subsubsection{Estimator 2: The MMSE Estimator \boldmath \pmmse{}}

While the Taylor estimator is more efficient than the naïve one, it has a drawback: its linear approximation of the classifier is faithful near the data point, but less faithful farther away from the data point. To fix this issue, we use a linearization that is faithful to the model over the entire noise distribution, not just near the data point. Linearization has been studied in feature attribution research, which concerns itself with approximating non-linear models with linear ones to produce model explanations \cite{han2022explanation}. In particular, the SmoothGrad \cite{smilkov2017smoothgrad} technique has been described as the MMSE optimal linearization of the model \cite{han2022explanation, agarwal2021towards}. Using a similar technique, we propose the MMSE estimator \pmmse{} as follows.

\begin{thm}
    The MMSE estimator for the local robustness of a classifier $f$ at point $\X$ with respect to target class $t$ is given by linearizing $f$ around $\X$ using $\mathbf{w} = \frac{1}{N}\sum_{j=1}^{N} \grad f(\X + \epsilon)$ and $b = \frac{1}{N}\sum_{j=1}^{N} f(\X + \epsilon)$, with decision boundaries $g_i(\X) = f_t(\X) - f_i(\X)$, $\forall i \neq t$, leading to
    \begin{align*}
        p^\text{mmse}_{\sigma}(\X) = \text{CDF}_{\mathcal{N}(0, UU^{\top})} (\left[  \frac{ \frac{1}{N}\sum_{j=1}^{N} g_1(\X + \epsilon)}{\sigma \| \frac{1}{N} \sum_{j=1}^{N} \grad g_1(\X + \epsilon)\|_2},  ..., \frac{\frac{1}{N} \sum_{j=1}^{N} g_C(\X + \epsilon)}{ \sigma \| \frac{1}{N} \sum_{j=1}^{N} \grad g_C(\X + \epsilon)\|_2} \right] )
    \end{align*}
    with $U \in \R^{(C-1) \times d}$ defined as in the linear case, where $N$ is the number of perturbations. 
\end{thm}

The proof is in Appendix~\ref{app:proofs}. It involves creating a randomized smooth model \cite{cohen2019certified} from the base model and then computing the decision boundaries of this smooth model. Note that the gradients of this smooth model are equal to those obtained from SmoothGrad. We show, for the first time, that performing such randomization helps compute robustness information for the original base model.

This estimator also requires sampling over the input space like \pmc{}. However, due to its use of model gradients, it requires far fewer samples to converge (we observed around $N=5-10$ to suffice in practice), thus making it computationally efficient. 

\subsubsection{Estimators 3 and 4 : Approximate Taylor and MMSE Estimators}

One drawback of the Taylor and MMSE estimators is their use of the \emph{mvn-cdf}, which does not have a closed form solution and can cause the estimators to be slow for settings with a large number of classes $C$. Further, the \emph{mvn-cdf} makes these estimators non-differentiable, which is inconvenient for applications which require differentiating \probust{}. To alleviate these issues, we wish to approximate the \emph{mvn-cdf} with an analytical closed-form expression. As CDFs are monotonically increasing functions, the approximation should also be monotonically increasing.

To this end, we find that the \emph{univariate} Normal CDF is well-approximated by the sigmoid function, and has been used to propose the GeLU activation function \cite{hendrycks2016gaussian}. Inspired by this, we propose to approximate the \emph{mvn-cdf} with the multivariate sigmoid function, which we define as follows:

\begin{defn}
    The multivariate sigmoid is defined as $\text{mv-sigmoid}(\X) = \frac{1}{1 + \sum_{i} \exp(-\X_i)}$ 
\end{defn}

We find experimentally that \emph{mv-sigmoid} approximates the \emph{mvn-cdf} well for practical values of the covariance matrix $UU^\top$. Using this approximation to substitute \emph{mv-sigmoid} for the \emph{mvn-cdf} in the \ptaylor{} and \pmmse{} estimators yields the \ptaylormvs{} and \pmmsemvs{} estimators, respectively.

\subsection{Exploring the Connections Between Local Robustness and Softmax Probability}
\subsubsection{Estimator 5: Softmax as an Estimator of \boldmath \probust{}}

Lastly, we observe that for linear models with a specific noise perturbation $\sigma$, the common softmax function taken with respect to the output logits can be viewed as an estimator of \probust{}, albeit in a very restricted setting. Specifically,

\begin{lemma}
    For multi-class linear models $f(\X) = \mathbf{w}^\top \X + b$, such that the decision boundary weight norms $\| w'_i \|_2 = \| w'_j \|_2 = \| w \|_2, \forall i,j$,
    \begin{align*}
        p^\text{softmax}_{T} = p^\text{taylor\_mvs}_{\sigma}~~~~\text{where}~~~~T = \sigma \| w \|_2
    \end{align*}
\label{lemma:softmax}
\end{lemma}

The proof is in Appendix~\ref{app:proofs}. Lemma~\ref{lemma:softmax} indicates that the temperature parameter $T$ of softmax roughly corresponds to the $\sigma$ of the added Normal noise with respect to which local robustness is measured. Overall, this shows that under the restricted setting where the local linear model consists of decision boundaries with equal weight norms, the softmax outputs can be viewed as an estimator of the \ptaylormvs{} estimator, which itself is an estimator of \probust{}. However, due to the multiple levels of approximation, we can expect the quality of \psoftmax{}'s approximation of \probust{} to be poor in general settings (outside of the very restricted setting), so much so that in general settings, \probust{} and \psoftmax{} would be unrelated.