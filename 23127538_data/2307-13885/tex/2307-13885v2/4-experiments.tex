\section{Empirical Evaluation}
\label{sec:exp}

In this section, we first evaluate the accuracy and efficiency of the analytical estimators. Then, we analyze the relationship between local robustness and softmax probability. Lastly, we demonstrate the usefulness of local robustness and its analytical estimators in real-world applications. Key results are discussed in this section and full results are in Appendix~\ref{app:experiments}.

%datasets and models
%\subsection{Datasets and Models}
\textbf{Datasets and Models.}
We evaluate the estimators on four datasets: MNIST \citep{deng2012mnist}, FashionMNIST \citep{xiao2017fashion}, CIFAR10 \citep{krizhevsky2009learning}, and CIFAR100 \citep{krizhevsky2009learning}. For MNIST and FashionMNIST, we train linear models and CNNs to perform classification. For CIFAR10 and CIFAR100, we train ResNet18 \citep{he2016deep} models to perform classification. We train the ResNet18 models using varying levels of gradient norm regularization ($\lambda$) to obtain models with varying levels of robustness. The experiments below use each dataset's full test set, each consisting of 10,000 points. Additional details about the datasets and models are described in Appendix~\ref{app:datasets} and \ref{app:models}.


\subsection{Evaluation of the accuracy of analytical estimators}
\label{sec:exp_correctness}

\textbf{The analytical estimators accurately compute local robustness.}
To confirm that the analytical estimators accurately compute \probust{}, we calculate \probust{} for each model and test set using \pmc{}, \ptaylor{}, \pmmse{}, \ptaylormvs{}, \pmmsemvs{}, and \psoftmax{} for different $\sigma$'s. For \pmc{}, \pmmse{}, and \pmmsemvs{}, we use a sample size at which these estimators have converged ($n=10000, 500, \text{and } 500$, respectively). (Convergence analyses are in Appendix~\ref{app:experiments}.) Then, we measure the absolute and relative difference between \pmc{} and the other estimators. The smaller these differences, the more accurately the estimator computes \probust{}. 

%pmmse family = best estimator
The performance of the estimators for the FashionMNIST CNN model is shown in Figure~\ref{fig1a:method-works-over-sigma}. The results indicate that \pmmsemvs{} and \pmmse{} are the best estimators of \probust{}, followed closely by \ptaylormvs{} and \ptaylor{}, trailed by \psoftmax{}. Consistent with the theory in Section~\ref{sec:methods}, the MMSE estimators outperform the Taylor ones because the former obtains better estimates of $\grad g_i(\X)$, and \psoftmax{} performs poorly in general settings because of its multiple levels of approximation.

%smaller noise neighborhood, better approximation
The results also confirm that the smaller the noise neighborhood $\sigma$, the more accurately the estimators compute \probust{}. For the MMSE and Taylor estimators, this is because their linear approximation of the model around the input is more faithful for smaller $\sigma$'s. As expected, when the model is linear, \ptaylor{} and \pmmse{} accurately compute \probust{} for all $\sigma$'s (Appendix~\ref{app:experiments}). For the softmax estimator, \psoftmax{} values are constant over $\sigma$'s and this particular model has high \psoftmax{} values for most points. Thus, for small $\sigma$'s where \probust{} is near one, \psoftmax{} happens to approximate \probust{} for this model. Examples of images with varying levels of noise ($\sigma$) are in Appendix~\ref{app:experiments}.

\textbf{For robust models, the analytical estimators compute local robustness more accurately over a larger noise neighborhood.} 
The performance of \pmmse{} for CIFAR10 ResNet18 models of varying levels of robustness is shown in Figure~\ref{fig1b:method-works-robust}. The results indicate that for more robust models (larger $\lambda$), the estimator is more accurate over a larger $\sigma$. This is because gradient norm regularization leads to models that are more locally linear, making the estimator's linear approximation of the model around the input more accurate over a larger $\sigma$, making its \probust{} values more accurate.


\textbf{The mv-sigmoid function approximates the multivariate Normal CDF well in practice.} To examine \emph{mv-sigmoid}'s approximation of \emph{mvn-cdf}, we compute both functions using the same inputs ($z~=~\left[  \frac{g_1(\X)}{\sigma \|\grad g_1(\X)\|_2}, ..., \frac{g_C(\X)}{\sigma \|\grad g_C(\X) \|_2} \right]$, as described in Proposition~\ref{eqn:taylor-estimator}) for the CIFAR10 ResNet18 model for different $\sigma$'s. The plot of \emph{mv-sigmoid(z)} against \emph{mvn-cdf(z)} for $\sigma=0.05$ is shown in Figure~\ref{fig2:mvsig-mvncdf}. The results indicate that the two functions are strongly positively correlated with low approximation error, suggesting that \emph{mv-sigmoid} approximates the \emph{mvn-cdf} well in practice.

% \clearpage

%fig1 -- method properly approximates p_empirical
%fig1a: method works
%02d_pemp_vs_pothers_over_sigma/rel/fmnist_cnn.png
%fig1b: method works better for robust models
%02e_pemp_vs_pmmse_over_sigma_robust_models/rel/cifar10_resnet18.png
% Figure environment removed


%fig2: mvsigmoid is a good approximator for mvncdf
%correlation for sigma=0.1 
% 03_mvncdf_vs_mvsigmoid/cifar10_resnet18/cifar10_resnet18_gnorm0.0_sigma0.05.png

%fig3: p_mc takes many samples to converge
% 02a_p_emp_convergence_n50000_baseline/rel/cifar10_resnet18_sigma0.1.png
% Figure environment removed


%table: naive method is inefficient, analytical method is efficient
\begin{table}[ht!]
\centering
\begin{tabular}{l|l|l|l|l|l}
    \multicolumn{2}{c}{}   & \multicolumn{2}{|c|}{CPU: Intel x86\_64}   & \multicolumn{2}{|c}{GPU: Tesla V100-PCIE-32GB} \\
    \midrule
    Estimator   & \# samples ($n$)   & Serial   & Batched   & Serial   & Batched \\
    \midrule
    \pmc{}   & \begin{tabular}[c]{@{}l@{}}  $n=10000$\end{tabular}               
             & \begin{tabular}[c]{@{}l@{}}  1:41:11\end{tabular}                                               
             & \begin{tabular}[c]{@{}l@{}}  1:14:38\end{tabular}                                                
             & \begin{tabular}[c]{@{}l@{}}  0:19:56\end{tabular}                                                
             & \begin{tabular}[c]{@{}l@{}}  0:00:35\end{tabular} \\
    \ptaylor{}   & N/A
                 & 0:00:08                                                                                                                     
                 & 0:00:07                                                                                                                      
                 & 0:00:02                                                                                                                      
                 & $<$ 0:00:01 \\
    \pmmse{}   & \begin{tabular}[c]{@{}l@{}} $n=5$\end{tabular} 
               & \begin{tabular}[c]{@{}l@{}} 0:00:41\end{tabular} 
               & \begin{tabular}[c]{@{}l@{}} 0:00:31\end{tabular} 
               & \begin{tabular}[c]{@{}l@{}} 0:00:06\end{tabular} 
               & \begin{tabular}[c]{@{}l@{}} 0:00:02\end{tabular} \\              
\end{tabular}
\vspace{0.2cm}
\caption{Runtimes of \probust{} estimators. Each estimator computes \probustwsigma{0.1} for the CIFAR10 ResNet18 model for 50 data points. Estimators that use sampling use the minimum number of samples necessary for convergence. Runtimes are in the format of hour:minute:second. The analytical estimators (\ptaylor{} and \pmmse{}) are more efficient than the naïve estimator (\pmc{}).}
\vspace{-0.5cm}
\label{table:runtimes}
\end{table}



%fig4 -- p_robust and p_softmax
%fig4a: scatterplot, non-robust model
%02i_pemp_vs_pmmse_corr_robust_models_scatterplots/cifar10_resnet18_sigma0.1_gnormreg0.png
%fig4b: the more robust the model, the more the two are related
%02h_pemp_vs_pmmse_corr_robust_models_lineplots/cifar10_resnet18_cifar100_resnet18_sigma0.1.png
%fig4c: scatterplot, robust model
%02i_pemp_vs_pmmse_corr_robust_models_scatterplots/cifar10_resnet18_sigma0.1_gnormreg0.01.png
% Figure environment removed

%fig5: local robustness bias
% 02f_p_distr_vs_classes/p_all_over_classes/cifar10_resnet18_sigma0.09.png
\begin{SCfigure}
  \vspace{1cm}
  \centering
  % Figure removed
  \vspace{-2.5cm}
  \caption{Local robustness bias among classes for the ResNet18 CIFAR10 model. \probust{} reveals that the model is less locally robust for some classes than for others. The analytical estimator \pmmse{} properly captures this model bias.}
  \label{fig5:robustness-bias}
\end{SCfigure}


%fig4 -- 2x4 images
%top-k and bottom-k images
% 02g_topk_bottomk_images/cifar10_resnet18/p_mmse/
% - cifar10_resnet18_p_mmse_sigma0.1_class9_bottomk.png
% - cifar10_resnet18_p_mmse_sigma0.1_class9_topk.png
% - ... class0 x 2
% 02g_topk_bottomk_images/cifar10_resnet18/p_sm
% - cifar10_resnet18_p_sm_sigma0.1_class9_bottomk.png
% - cifar10_resnet18_p_sm_sigma0.1_class9_topk.png
% - ... class0 x 2
% Figure environment removed



% %fig4 -- 3x4 images
% %top-k and bottom-k images
% % 02g_topk_bottomk_p/resnet18_cifar10/p_mmse/
% % - resnet18_cifar10_p_mmse_sigma0.1_class9_topk.png
% % - resnet18_cifar10_p_mmse_sigma0.1_class9_bottomk.png
% % - ...class8... x 2
% % - ...class0... x 2
% % 02g_topk_bottomk_p/resnet18_cifar10/p_sm/
% % - resnet18_cifar10_p_sm_sigma0.1_class9_topk.png
% % - resnet18_cifar10_p_sm_sigma0.1_class9_bottomk.png
% % Figure environment removed







\subsection{Evaluation of the efficiency of analytical estimators}

\textbf{The naïve estimator is statistically inefficient.} To examine the efficiency of \pmc{}, we calculate \pmc{} for each model and test set using different sample sizes ($n$) over different $\sigma$'s, and measure the absolute and relative difference between \pmc{} at a given $n$ and \pmc{} at $n=50,000$. Results for the CIFAR10 ResNet18 model are shown in Figure~\ref{fig3:pmc-convergence}. The results indicate that \pmc{} requires around 10,000 samples per point to converge, which is impractical.

\textbf{The analytical estimators are more efficient than the naïve estimator.}
Next, we examine the efficiency of the estimators by measuring their runtimes when calculating \probustwsigma{0.1} for the CIFAR10 ResNet18 model for 50 points. Runtimes are displayed in Table~\ref{table:runtimes}. They indicate that \ptaylor{} and \pmmse{} perform 35x and 17x faster than \pmc{}, respectively. Additional runtimes are in Appendix~\ref{app:experiments}.
% Thus, the analytical estimators are more efficient than the naïve estimator.

\subsection{Comparison of local robustness and softmax probability}

\textbf{Local robustness and softmax probability are two distinct measures.} To examine the relationship between \probust{} and \psoftmax{}, we calculate \pmmse{} and \psoftmax{} for CIFAR10 and CIFAR100 models of varying levels of robustness, and measure the correlation of their values and ranks using Pearson and Spearman correlations. Results are in Figure~\ref{fig4:probust-and-psoftmax}. For a non-robust model, \probust{} and \psoftmax{} are not strongly correlated (Figure~\ref{fig4a:ps-nonrob-model}). As model robustness increases, the two quantities become more correlated (Figures~\ref{fig4b:ps-rob-models-lineplot} and~\ref{fig4c:ps-rob-model}). However, even for robust models, the relationship between the two quantities is mild (Figure~\ref{fig4c:ps-rob-model}). That \probust{} and \psoftmax{} are not strongly correlated is consistent with the theory in Section~\ref{sec:methods}: in general settings, \psoftmax{} is not a good estimator for \probust{}.

% that the two probabilities are conceptually different: \probust{} measures the uncertainty of a model’s prediction with respect to input noise (i.e., the probability that the prediction will change upon adding noise to the input) while \psoftmax{} is an uncalibrated uncertainty of the model's prediction being correct with respect to a calibration set. \textcolor{red}{[check interp of raw \psoftmax]}.

\subsection{Applications of local robustness}

\textbf{\boldmath \probust{} detects local robustness bias.} We demonstrate that \probust{} can detect bias in local robustness by examining its distribution for each class for each model and test set over different $\sigma$'s. Results for the CIFAR10 ResNet18 model are in plotted in Figure~\ref{fig5:robustness-bias}. The results show that different classes have different \probust{} distributions, i.e., the model is more locally robust for some classes (e.g., frog) than for others (e.g., airplane). The results also show that \pmc{} and \pmmse{} have very similar distributions, further indicating that the latter well-approximates the former. Thus, \probust{} can be applied to detect local robustness bias, which is critical when models are deployed in high-stakes, real-world settings.

\textbf{\boldmath \probust{} identifies images that are robust to and images that are vulnerable to random noise.} We demonstrate that \probust{} can also distinguish between images that are robust to and images that are vulnerable to random noise in a way that is superior to \psoftmax{}. For each dataset, we train a simple CNN to distinguish between images with high and low \pmmse{} and the same CNN to also distinguish between images with high and low \psoftmax{} (additional setup details described in Appendix~\ref{app:experiments}). Then, we compare the performance of the two models. For CIFAR10, the test set accuracy for the \pmmse{} CNN is 0.92 while that for the \psoftmax{} CNN is 0.58. These results indicate that \probust{} better identifies images that are robust to and vulnerable to random noise than \psoftmax{}.

We also visualize images with the highest and lowest \pmmse{} in each class for each model. For comparison, we do the same with \psoftmax{}. Example CIFAR10 images are displayed in Figure~\ref{fig6:topk-vs-bottomk}. Images with low \probust{} tend to have neutral colors, with the object being a similar color as the background (making the prediction likely to change when the image is slightly perturbed), while images with high \probust{} tend to be brightly-colored, with the object strongly contrasting with the background (making the prediction likely to stay constant when the image is slightly perturbed). These differences are not as evident for images with the highest and lowest \psoftmax{}. Thus, in addition to detecting local robustness bias, \probust{} can also be applied to identify images that are robust to and images that are vulnerable to random noise.



% For all the experiments above, we observe consistent results across datasets and models (Appendix).

% --- OLD STUFF BELOW ---


%\ptaylormvs{} and \pmmsemvs{} perform better than \ptaylor{} and \pmmse{}, respectively, because... \textcolor{blue}{[How to explain why \pmmsemvs and \ptaylormvs perform better than \pmmse and \ptaylor?]} 


% %exp1
% \textbf{Experiment 1. As the noise neighborhood increases, local robustness deteriorates.}
% First, we examine the behavior of \probust{} as the noise neighborhood increases. For a given model, we calculate \pmc{} for different values of $\sigma$ for 1,000 randomly-selected points from the test set (hereafter referred to as the “test set”). To calculate \pmc{} for a given image, we use 10,000 noisy samples (a value at which \pmc{} converged; convergence analyses are described in Appendix~\ref{app:exp-convergence-pmc-pmmse}).

% Results for the linear model and CNN trained on FashionMNIST are shown in Figure~\ref{fig1:pmc-vs-noise}. In the figure, the distribution of \pmc{} is concentrated at one for small values of $\sigma$ and increasingly shifts towards zero as $\sigma$ increases. Thus, as expected, the results indicate that the models are locally robust for small noise neighborhoods, and as the noise neighborhood increases, local robustness deteriorates. This is expected because as more noise is added to the original image, it becomes more likely for the prediction of the noisy image to differ from that of the original image, causing local robustness to deteriorate (i.e., causing \probust{} to decrease). We observe similar results across datasets and models (Appendix~\ref{app:exp-pmc-vs-noise}).


% %exp3
% \textbf{Experiment 3. \boldmath \probust{} and \boldmath \psoftmax{} measure different types of uncertainty.}
% Next, we show that \probust{} and \psoftmax{} measure different types of uncertainty. For a given model, test set, and noise neighborhood ($\sigma$), we calculate \pmmse{} (a close estimate of \probust{}) and \psoftmax{} and examine their relationship. 

% Results for the ResNet18 model trained on CIFAR10 at $\sigma=0.1$ are shown in Figure~\ref{fig3:correlation-pmmse-psm}. The results indicate that \probust{} and \psoftmax{} are not strongly correlated: points which have high \psoftmax{} values have high or low \probust{} values, and points which have low \probust values have high or low \psoftmax{} values. This finding is corroborated by the low Pearson correlation and Spearman correlation coefficients, indicating that the values and ranks, respectively, of \probust{} and \psoftmax{} are not strongly correlated. We observe similar results across datasets and models (Appendix~\ref{app:exp-correlation-pmmse-psm}).

% That \probust{} and \psoftmax{} are not strongly correlated is consistent with the understanding that the two probabilities measure different types of uncertainty: \probust{} measures the uncertainty of a model’s prediction with respect to input noise (i.e., the probability that the prediction will change upon adding noise to the input) while \psoftmax{}... \textcolor{blue}{[what is the interpretation of raw \psoftmax, if any?]}.


% %exp4

% \textbf{Experiment 4. \boldmath \probust{} can be used to detect differences in local robustness among classes.}
% Next, we demonstrate that \probust{} can be used to detect bias in local robustness among classes. We examine two setups. First, for a given model, test dataset, and noise neighborhood ($\sigma$), we calculate \probust{} using \pmmse{} and examine the distribution of \probust{} for each class. This setup examines local robustness based on confidence level for fixed $\sigma$. Second, for a given model, test dataset, and confidence level, we calculate the noise neighborhood that meets the specified confidence level. To do so, we optimize $\sigma$ such that \ptaylor{} equals the specified confidence level (we use \ptaylor{} because it is easier to optimize than \pmmse{}; however, the same idea applies to \pmmse{}). This setup examines local robustness based on $\sigma$ for a fixed confidence level.

% Results for both setups for the ResNet18 model trained on CIFAR10 are shown in Figure~\ref{fig4:robustness-bias}. Different classes have different confidence level distributions when $\sigma$ is fixed (Figure~\ref{fig4a:sigma-fixed}) and different $\sigma$ distributions when confidence level is fixed (Figure~\ref{fig4b:confidence-fixed}), indicating that the model is more locally robust for some classes than for others. We observe similar results across datasets and models (Appendix~\ref{app:exp-robustness-bias}).

% ADD LATER: We also examine local robustness bias among classes by fixing the same confidence level for each point and calculating the $\sigma$ that yields that confidence level, and we obtain consistent results indicating that models are not equally robust for all classes.

% %exp5
% \textbf{Experiment 5. \probust{} can distinguish between clear and ambiguous images.}
% Lastly, we demonstrate that \probust{} can distinguish between clear and ambiguous images. For a given model, test set, and $\sigma$, we calculate \probust{} (using \pmmse{}) and \psoftmax{}. Then, for each class, we measure the difference between images with the highest and lowest \probust{} and the difference between images with the highest and lowest \psoftmax{} by… \textcolor{blue}{[need to do this experiment, see if we get desired result]}.

% Results for the ResNet18 model trained on CIFAR10 are shown in Figure~\ref{fig5:topk-vs-bottomk}. As seen in Figure~\ref{fig5a:differences}, differences between the top and bottom images based on \probust{} are larger than those based on \psoftmax{} \textcolor{blue}{[currently wrote desired results, fill in real results later]}. Visual inspection of the images, such as in Figures~\ref{fig5b:bottomk-probust}-\ref{fig5e:topk-psoftmax}, suggests that top and bottom images based on \probust{} tend to be clear and ambiguous images for each class, respectively, while this distinction is not evident for top and bottom images based on \psoftmax{}. We observe similar results across datasets and models (Appendix~\ref{app:exp-topk-vs-bottomk}). Taken together, these results indicate that \probust{} better distinguishes between clear and ambiguous images than \psoftmax{}, suggesting that \probust{} better reflects image differences in the latent feature space \textcolor{blue}{[check statement after the comma]}. 
