\section{Related Work}

\textbf{Linearization of neural networks.} Prior works have used linear models to approximate neural networks in the local region around an input. For example, LIME \cite{ribeiro2016lime}, a popular post hoc explanation method, follows this approach and uses the coefficients of the linear model as feature attributions. The local function approximation framework \cite{han2022explanation} further demonstrates that eight popular post hoc explanation methods all perform local linear approximation of the underlying model. Local linear function approximation has also been used to generate probabilistically-robust counterfactual explanations, specifying the probability that a binary classifier generates consistent predictions when inputs are noisy \cite{pawelczyk2022probabilistically}. In contrast to these prior works which apply local linear function approximation to post hoc explainability \cite{ribeiro2016lime, han2022explanation, pawelczyk2022probabilistically} or focus on binary classification \cite{pawelczyk2022probabilistically}, this work uses local linear function approximation to investigate local robustness and develops analytical estimators of local robustness for both binary and multi-class classification.

\textbf{Adversarial robustness.} Prior works have proposed methods to generate adversarial attacks \cite{carlini2017towards, goodfellow2014explaining, moosavi2016deepfool}, which find adversarial perturbations in a local region around a point. In contrast, this work investigates local robustness (a generalization of adversarial robustness), which calculates the probability that a model’s prediction remains consistent in a local region around a point. Prior works have also proposed methods to certify model robustness \cite{cohen2019certified, carlini2022certified}, which provide guarantees of dataset-level robustness (i.e., all points in a dataset are robust to a certain amount of noise). In contrast, this work focuses on local robustness, which measures point-level robustness. In addition, prior work has investigated robustness bias in terms of vulnerability to adversarial attacks, lowerbounding and upperbounding a point's probability of adversarial attack using sampling approaches \cite{nanda2021fairness}. In contrast, this work investigates robustness bias in terms of local robustness, directly calculating a model's local robustness using analytical approaches.
%directly calculating the probability that a model’s prediction remains consistent in the local region around a point.

\textbf{Uncertainty estimation.} Prior works have developed approaches to measure a model's prediction uncertainty. These include calibration \cite{guo2017calibration}, Bayesian uncertainty \cite{kendall2017uncertainties}, and conformal prediction \cite{shafer2008tutorial}. In contrast to these prior works in which prediction uncertainty is with respect to a calibration set \cite{guo2017calibration, shafer2008tutorial} or model parameters \cite{kendall2017uncertainties}, this work investigates local robustness, which can be viewed as prediction uncertainty with respect to input noise.