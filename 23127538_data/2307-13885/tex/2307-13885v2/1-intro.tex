\section{Introduction}

%As machine learning models are increasingly deployed in such high-stakes settings such as medicine, finance, and law, it becomes critical to ensure that these models not only perform accurately, but are also robust in the face of challenges such as adversarial attacks, noisy input data, and dataset shift. 
A desirable attribute of machine learning models is robustness to perturbations of input data. One common notion of robustness is adversarial robustness, the ability of a model to maintain its prediction when presented with adversarial perturbations, i.e., perturbations designed to cause the model to change its prediction. Although adversarial robustness can identify whether an adversarial example exists in a local region around an input, real-world noise (such as measurement noise) is rarely adversarial and often random. The effect of such noise on model predictions is better captured by another notion of robustness: \emph{local robustness}, the fraction of points in a local region around an input for which the model provides consistent predictions. If this fraction is less than one, then an adversarial perturbation exists. In this sense, local robustness is a strict generalization of adversarial robustness. The limitation of adversarial robustness in only detecting whether an adversarial perturbation exists is perhaps expected, as its original use-case was motivated by model security, not model understanding, debugging, or regularization for improved generalization. Therefore, local robustness provides a more comprehensive characterization of real-world model behavior as it captures model behavior under the interference of average case noise.
%\suraj{From a security viewpoint, advex is great, but from a model understanding and debugging perspective, not so great}

In this paper, we take the first steps towards measuring local robustness. We show that the naïve approach to estimating local robustness based on Monte-Carlo sampling \cite{nanda2021fairness} is statistically inefficient: obtaining an accurate estimate of local robustness using this approach requires a large number of samples from the local region. This inefficiency, which is exacerbated in the case of high-dimensional data, leads to prohibitive computational costs for large-scale applications, making the naïve approach impractical.
% the naïve approach to measuring local robustness by sampling points in the local region around an input and calculating the fraction of consistent predictions

To address this problem, we develop the first analytical estimators to efficiently compute the local robustness of a model. Our work makes the following contributions:

\begin{enumerate}
    \item We derive a series of novel analytical estimators to efficiently compute the local robustness of multi-class discriminative models by linearizing non-linear models in the local region around an input, and computing the model’s local robustness in this region using the multivariate Normal cumulative distribution function. Through the derivation, we show how local robustness is connected to such concepts as randomized smoothing and softmax probability.

    \item We empirically validate these analytical estimators using standard deep learning models and datasets, demonstrating that these estimators accurately and efficiently compute local robustness.

    \item We demonstrate the usefulness of our analytical estimators for various tasks involving local robustness, such as measuring class-level robustness bias \cite{nanda2021fairness} (i.e., a model being more locally robust for some classes than for others) and identifying examples that are vulnerable to noise perturbation in a dataset. Such dataset-level analyses of local robustness are made practical only by having these efficient analytical estimators.
\end{enumerate}


To our knowledge, this work is the first to investigate local robustness in a multi-class setting and develop efficient analytical estimators for local robustness. The analytical aspect of these estimators not only advances conceptual understanding of local robustness, but also enables local robustness to be used in applications that require differentiability (such as model training). In addition, the efficiency of these estimators makes the computation of local robustness practical, enabling tasks that assist in such important objectives as debugging models and establishing user trust.

% Contributions

% - Introduce \probust{} as a measure of local robustness. Can be used to measure model uncertainty; different from previous uncertainty measures
   
% - Present a method to efficiently calculate \probust{} for binary and multiclass classification.

% - Empirically confirm the method properly estimates \probust{}

% - Demonstrate \probust{}'s usefulness for understanding model behavior/model probing: 1) detect robustness bias among classes and 2) separate clear and ambiguous images


% To our knowledge, this work is the first to use local robustness, measured by \probust{}, as a tool/metric for model probing. Furthermore, prior to this work, it was not possible to efficiently calculate \probust{} at the level of a dataset. Thus, as demonstrated in these experiments, we introduce and enable the use local robustness as a new tool/metric for model probing, useful for such purposes as model debugging and evaluation of model fairness.




