\begin{thebibliography}{35}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Sci()]{SciPy}
Sci{P}y multivariate normal {CDF}.
\newblock \url{https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.multivariate_normal.html}.

\bibitem[flo()]{flops}
Whatâ€™s the backward-forward {FLOP} ratio for neural networks?
\newblock \url{https://www.lesswrong.com/posts/fnjKpBoWJXcSDwhZk/what-s-the-backward-forward-flop-ratio-for-neural-networks}.
\newblock Accessed: 2024-04-04.

\bibitem[Agarwal et~al.(2021)Agarwal, Jabbari, Agarwal, Upadhyay, Wu, and Lakkaraju]{agarwal2021towards}
Sushant Agarwal, Shahin Jabbari, Chirag Agarwal, Sohini Upadhyay, Steven Wu, and Himabindu Lakkaraju.
\newblock Towards the unification and robustness of perturbation and gradient based explanations.
\newblock \emph{International Conference on Machine Learning}, 2021.

\bibitem[Botev(2017)]{botev2017normal}
Zdravko~I Botev.
\newblock The normal law under linear restrictions: {S}imulation and estimation via minimax tilting.
\newblock \emph{Journal of the Royal Statistical Society. Series B (Statistical Methodology)}, 2017.

\bibitem[Carlini and Wagner(2017)]{carlini2017towards}
Nicholas Carlini and David Wagner.
\newblock Towards evaluating the robustness of neural networks.
\newblock \emph{IEEE Symposium on Security and Privacy}, 2017.

\bibitem[Carlini et~al.(2022)Carlini, Tramer, Dvijotham, Rice, Sun, and Kolter]{carlini2022certified}
Nicholas Carlini, Florian Tramer, Krishnamurthy Dvijotham, Leslie Rice, Mingjie Sun, and Zico Kolter.
\newblock ({C}ertified!!) adversarial robustness for free!
\newblock \emph{International Conference on Learning Representations}, 2022.

\bibitem[Cohen et~al.(2019)Cohen, Rosenfeld, and Kolter]{cohen2019certified}
Jeremy Cohen, Elan Rosenfeld, and Zico Kolter.
\newblock Certified adversarial robustness via randomized smoothing.
\newblock \emph{International Conference on Machine Learning}, 2019.

\bibitem[Deng(2012)]{deng2012mnist}
Li~Deng.
\newblock The {MNIST} database of handwritten digit images for machine learning research.
\newblock \emph{IEEE Signal Processing Magazine}, 2012.

\bibitem[Fazlyab et~al.(2019)Fazlyab, Morari, and Pappas]{fazlyab2019probabilistic}
Mahyar Fazlyab, Manfred Morari, and George~J Pappas.
\newblock Probabilistic verification and reachability analysis of neural networks via semidefinite programming.
\newblock In \emph{2019 IEEE 58th Conference on Decision and Control (CDC)}, pages 2726--2731. IEEE, 2019.

\bibitem[Franceschi et~al.(2018)Franceschi, Fawzi, and Fawzi]{franceschi2018robustness}
Jean-Yves Franceschi, Alhussein Fawzi, and Omar Fawzi.
\newblock Robustness of classifiers to uniform lp and gaussian noise.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pages 1280--1288. PMLR, 2018.

\bibitem[Goodfellow et~al.(2015)Goodfellow, Shlens, and Szegedy]{goodfellow2014explaining}
Ian~J Goodfellow, Jonathon Shlens, and Christian Szegedy.
\newblock Explaining and harnessing adversarial examples.
\newblock \emph{International Conference on Learning Representations}, 2015.

\bibitem[Han et~al.(2022)Han, Srinivas, and Lakkaraju]{han2022explanation}
Tessa Han, Suraj Srinivas, and Himabindu Lakkaraju.
\newblock Which explanation should {I} choose? {A} function approximation perspective to characterizing post hoc explanations.
\newblock \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock \emph{IEEE Conference on Computer Vision and Pattern Recognition}, 2016.

\bibitem[Hendrycks and Gimpel(2016)]{hendrycks2016gaussian}
Dan Hendrycks and Kevin Gimpel.
\newblock Gaussian error linear units ({GELU}s).
\newblock \emph{arXiv preprint arXiv:1606.08415}, 2016.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton, et~al.]{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock \emph{University of Toronto}, 2009.

\bibitem[Kumar et~al.(2020)Kumar, Levine, Feizi, and Goldstein]{kumar2020certifying}
Aounon Kumar, Alexander Levine, Soheil Feizi, and Tom Goldstein.
\newblock Certifying confidence via randomized smoothing.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 5165--5177, 2020.

\bibitem[Mangal et~al.(2019)Mangal, Nori, and Orso]{mangal2019robustness}
Ravi Mangal, Aditya~V Nori, and Alessandro Orso.
\newblock Robustness of neural networks: A probabilistic and practical approach.
\newblock In \emph{2019 IEEE/ACM 41st International Conference on Software Engineering: New Ideas and Emerging Results (ICSE-NIER)}, pages 93--96. IEEE, 2019.

\bibitem[Moayeri et~al.(2022)Moayeri, Banihashem, and Feizi]{moayeri2022explicit}
Mazda Moayeri, Kiarash Banihashem, and Soheil Feizi.
\newblock {E}xplicit tradeoffs between adversarial and natural distributional robustness.
\newblock \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Moosavi-Dezfooli et~al.(2016)Moosavi-Dezfooli, Fawzi, and Frossard]{moosavi2016deepfool}
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard.
\newblock Deep{F}ool: {A} simple and accurate method to fool deep neural networks.
\newblock \emph{IEEE Conference on Computer Vision and Pattern Recognition}, 2016.

\bibitem[Moosavi-Dezfooli et~al.(2019)Moosavi-Dezfooli, Fawzi, Uesato, and Frossard]{moosavi2019robustness}
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Jonathan Uesato, and Pascal Frossard.
\newblock Robustness via curvature regularization, and vice versa.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 9078--9086, 2019.

\bibitem[Nanda et~al.(2021)Nanda, Dooley, Singla, Feizi, and Dickerson]{nanda2021fairness}
Vedant Nanda, Samuel Dooley, Sahil Singla, Soheil Feizi, and John~P Dickerson.
\newblock Fairness through robustness: {I}nvestigating robustness disparity in deep learning.
\newblock \emph{ACM Conference on Fairness, Accountability, and Transparency}, 2021.

\bibitem[Ovadia et~al.(2019)Ovadia, Fertig, Ren, Nado, Sculley, Nowozin, Dillon, Lakshminarayanan, and Snoek]{ovadia2019can}
Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian Nowozin, Joshua Dillon, Balaji Lakshminarayanan, and Jasper Snoek.
\newblock {C}an you trust your model's uncertainty? {E}valuating predictive uncertainty under dataset shift.
\newblock \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Patrick(1995)]{patrick1995probability}
Billingsley Patrick.
\newblock Probability and measure.
\newblock \emph{A Wiley-Interscience Publication, John Wiley}, 118:\penalty0 119, 1995.

\bibitem[Pawelczyk et~al.(2023)Pawelczyk, Datta, van-den Heuvel, Kasneci, and Lakkaraju]{pawelczyk2022probabilistically}
Martin Pawelczyk, Teresa Datta, Johannes van-den Heuvel, Gjergji Kasneci, and Himabindu Lakkaraju.
\newblock Probabilistically robust recourse: {N}avigating the trade-offs between costs and robustness in algorithmic recourse.
\newblock \emph{International Conference on Learning Representations}, 2023.

\bibitem[Rice et~al.(2021)Rice, Bair, Zhang, and Kolter]{rice2021robustness}
Leslie Rice, Anna Bair, Huan Zhang, and J~Zico Kolter.
\newblock Robustness between the worst and average case.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 27840--27851, 2021.

\bibitem[Robey et~al.(2022)Robey, Chamon, Pappas, and Hassani]{robey2022probabilistically}
Alexander Robey, Luiz Chamon, George~J Pappas, and Hamed Hassani.
\newblock Probabilistically robust learning: Balancing average and worst-case performance.
\newblock In \emph{International Conference on Machine Learning}, pages 18667--18686. PMLR, 2022.

\bibitem[Smilkov et~al.(2017)Smilkov, Thorat, Kim, Vi{\'e}gas, and Wattenberg]{smilkov2017smoothgrad}
Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Vi{\'e}gas, and Martin Wattenberg.
\newblock Smooth{G}rad: removing noise by adding noise.
\newblock \emph{arXiv preprint arXiv:1706.03825}, 2017.

\bibitem[Srinivas and Fleuret(2018)]{srinivas2018knowledge}
Suraj Srinivas and Fran{\c{c}}ois Fleuret.
\newblock Knowledge transfer with jacobian matching.
\newblock \emph{International Conference on Machine Learning}, 2018.

\bibitem[Srinivas et~al.(2022)Srinivas, Matoba, Lakkaraju, and Fleuret]{srinivas2022efficient}
Suraj Srinivas, Kyle Matoba, Himabindu Lakkaraju, and Fran{\c{c}}ois Fleuret.
\newblock Efficient training of low-curvature neural networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 25951--25964, 2022.

\bibitem[Srinivas et~al.(2024)Srinivas, Bordt, and Lakkaraju]{srinivas2024models}
Suraj Srinivas, Sebastian Bordt, and Himabindu Lakkaraju.
\newblock {W}hich models have perceptually-aligned gradients? {A}n explanation via off-manifold robustness.
\newblock \emph{Advances in Neural Information Processing Systems}, 2024.

\bibitem[Taori et~al.(2020)Taori, Dave, Shankar, Carlini, Recht, and Schmidt]{taori2020measuring}
Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht, and Ludwig Schmidt.
\newblock Measuring robustness to natural distribution shifts in image classification.
\newblock \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Thulasidasan et~al.(2021)Thulasidasan, Thapa, Dhaubhadel, Chennupati, Bhattacharya, and Bilmes]{thulasidasan2021effective}
Sunil Thulasidasan, Sushil Thapa, Sayera Dhaubhadel, Gopinath Chennupati, Tanmoy Bhattacharya, and Jeff Bilmes.
\newblock {A}n effective baseline for robustness to distributional shift.
\newblock \emph{IEEE International Conference on Machine Learning and Applications (ICMLA)}, 2021.

\bibitem[Vershynin(2018)]{vershynin2018high}
Roman Vershynin.
\newblock \emph{High-dimensional probability: An introduction with applications in data science}.
\newblock Cambridge University Press, 2018.

\bibitem[Weng et~al.(2019)Weng, Chen, Nguyen, Squillante, Boopathy, Oseledets, and Daniel]{weng2019proven}
Lily Weng, Pin-Yu Chen, Lam Nguyen, Mark Squillante, Akhilan Boopathy, Ivan Oseledets, and Luca Daniel.
\newblock Proven: Verifying robustness of neural networks with a probabilistic approach.
\newblock In \emph{International Conference on Machine Learning}, pages 6727--6736. PMLR, 2019.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{xiao2017fashion}
Han Xiao, Kashif Rasul, and Roland Vollgraf.
\newblock Fashion-{MNIST}: {A} novel image dataset for benchmarking machine learning algorithms.
\newblock \emph{arXiv preprint arXiv:1708.07747}, 2017.

\end{thebibliography}
