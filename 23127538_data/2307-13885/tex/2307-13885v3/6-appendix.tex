\section{Appendix}

\subsection{Proofs}
\label{app:proofs}

\begin{lemma} \label{proof:lemma}
The local robustness of a multi-class linear model $f(\X) = \mathbf{w}^\top \X + b$ (with $\mathbf{w} \in \R^{d \times C}$ and $b \in \R^C$) at point $\X$ with respect to a target class $t$ is given by the following. Define weights $\U_i = \W_t - \W_i \in \R^d, \forall i \neq t$, where $\W_t, \W_i$ are rows of $\mathbf{w}$ and biases $c_i = {\U_i}^\top\X + (b_t - b_i) \in \R$. Then, 
\begin{align*}
    p^\text{robust}_\sigma(\X) = \cdf \left( \frac{c_i}{\sigma \| \U_i \|_2} \tensor \right)\\
    \mathrm{where}~~\matU = \frac{\U_i}{\| \U_i \|_2} \tensor \in \R^{(C-1) \times d}
\end{align*}
and $\cdf$ is the ($C-1$)-dimensional Normal CDF with zero mean and covariance $\matU \matU^\top$.
\end{lemma}

\begin{proof}
First, we rewrite \probust{} in the following manner, by defining $g_i(\X) = f_t(\X) - f_i(\X) > 0$, which is the ``decision boundary function".

\begin{align*}
    p_\sigma^\text{robust} = P_{\epsilon \sim \mathcal{N}(0,\sigma^2)} \left[ \max_{i} f_i(\X + \epsilon) < f_t(\X + \epsilon) \right] \\= P_{\epsilon \sim \mathcal{N}(0,\sigma^2)} \left[ \bigcup_{i=1; i \neq t}^C g_i(\X + \epsilon) > 0 \right]
\end{align*}

Now, assuming that $f,g$ are linear such that $g_i(\X) = {\U'_i}^\top \X + g(0)$, we have $g_i(\X + \epsilon) = g_i(\X) + {\U_i}^\top \epsilon$, and obtain

\begin{align}
p_\sigma^\text{robust} &= P_{\epsilon \sim \mathcal{N}(0,\sigma^2)}\left[ \bigcup_{i=1; i \neq t}^C {\U_i}^{\top}\epsilon > -g_i(\X) \right] \\
&= P_{z \sim \mathcal{N}(0,I_d)} \left[ \bigcup_{i=1; i \neq t}^C \frac{\U_i}{\| \U_i \|_2}^{\top}z > - \frac{g_i(\X)}{\sigma \| \U_i \|_2} \right] \label{appeqn:key_step}
\end{align}

This step simply involves rescaling and standardizing the Gaussian to be unit normal. We now make the following observations:
\begin{itemize}
    \item For any matrix $\matU \in \R^{C-1 \times d}$ and a d-dimensional Gaussian random variable $z \sim \mathcal{N}(0, I_d) \in \R^d$, we have $\matU^\top z \sim \mathcal{N}(0, \matU \matU^\top)$, i.e., an (C-1) -dimensional Gaussian random variable. 
    \item CDF of a multivariate Gaussian RV is defined as $P_z [\bigcup_i z_i < t_i]$ for some input values $t_i$
\end{itemize}

Using these observations, if we construct $\matU = \frac{\U_i}{\| \U_i \|_2} \tensor \in \R^{(C-1) \times d}$, and obtain

\begin{align*}
p^\text{robust}_{\sigma} &= P_{r \sim \mathcal{N}(0, \matU\matU^\top)} \left[ \bigcup_{i=1; i \neq t}^C r_i < \frac{g_i(\X)}{\sigma \| \U_i \|_2} \right] \\
&= \text{CDF}_{\mathcal{N}(0, UU^{\top})} \left( \frac{g_i(\X)}{\sigma \| \U_i \|_2} \tensor \right)
\end{align*}

where $g_i(\X) = {\U_i}^\top \X + g_i(0) = {(\W_t - \W_i)}^\top\X + (b_t - b_i)$

\end{proof}



\vspace{1cm}



\begin{lemma} (\textbf{Extension to non-Gaussian noise})
    For high-dimensional data ($d \rightarrow \infty$), Lemma \ref{estimator-linear-models} generalizes to any coordinate-wise independent noise distribution that satisfies Lyapunov's condition. 
\end{lemma} 

\begin{proof}
    Applying Lyupanov's central limit theorem, given $\epsilon \sim \mathcal{R}$ is sampled from some distribution $\mathcal{R}$ to equation \ref{appeqn:key_step} in the previous proof, we have we have $\frac{\U}{\sigma \| \U \|_2}^\top \epsilon = \sum_{j=1}^{d} \frac{\U_j}{\sigma\| \U \|_2} \epsilon_j ~~\substack{d\\\longrightarrow} ~~\mathcal{N}(0, 1)$, which holds as long as the sequence $\{\frac{\U_j}{\| \U \|_2} \epsilon_j\}$ are independent random variables and satisfy the Lyapunov condition. In particular, this implies that $\matU^\top z \sim \mathcal{N}(0, \matU \matU^\top)$, and the proof proceeds as similar to the Gaussian case after this step.
\end{proof}


\vspace{1cm}

\begin{lemma} (\textbf{Extension to non-isotropic Gaussian}) Lemma \ref{estimator-linear-models} can be extended to the case of $\epsilon \sim \mathcal{N}(0, \mathcal{C})$ for an arbitrary positive definite covariance matrix $\mathcal{C}$:

\begin{align*}
    p^\text{robust}_\sigma(\X) = \Phi_{\matU \mathcal{C} \matU^\top} \left( \frac{c_i}{\| \U_i \|_2} \tensor \right)
\end{align*}
    
\end{lemma}

\begin{proof}
    We observe that the Gaussian random variable $\frac{\U_i}{\| \U_i \|}^\top \epsilon \vert_{\substack{i=1\\t \neq t}}^C = \matU^\top \epsilon$ has mean zero as $\epsilon$ is mean zero. Computing its covariance matrix, we have $\E_\epsilon \matU^\top \epsilon \epsilon^\top \matU  = \matU^\top \E_\epsilon (\epsilon \epsilon^\top) \matU = \matU^\top \mathcal{C} \matU$. We use this result after equation \ref{appeqn:key_step} in the proof of Lemma \ref{estimator-linear-models}.
\end{proof}


\vspace{1cm}


\begin{thm}
    The \textbf{Taylor estimator} for the local robustness of a classifier $f$ at point $\X$ with respect to target class $t$ is given by linearizing $f$ around $\X$ using a first-order Taylor expansion, with decision boundaries $g_i(\X) = f_t(\X) - f_i(\X)$, $\forall i \neq t$, leading to
    \begin{align*}
        p^\text{taylor}_{\sigma}(\X) = \cdf \left( \frac{g_i(\X)}{\sigma \|\grad g_i(\X)\|_2} \tensor \right) 
    \end{align*}
    with $\matU$ and $\Phi$ defined as in the linear case.
\end{thm}

\begin{proof}
    Using the notations from the previous Lemma \ref{proof:lemma}, we can linearize $g(\X + \epsilon) \approx g(\X) + \grad g(\X)^\top \epsilon$ using a first order Taylor series expansion.
    Thus we use $\U_i = \grad g_i(\X)$ and $c_i = g_i(\X)$, and plug it into the result of Lemma \ref{proof:lemma}.
\end{proof}




\begin{thm} The \textbf{estimation error} of the Taylor estimator for a classifier with a quadratic decision boundary $g_i(\X) = \X^\top A_i \X + \U_i^\top \X + c_i$ for positive-definite $A_i$, is upper bounded by
    \begin{align*}
        | p^{robust}_{\sigma}(\X) - p^{taylor}_{\sigma}(\X) | \leq k \sigma^{C-1} \prod_{\substack{i=1\\i\neq t}}^{C} \frac{\lambda_{\max}^{A_i}}{\| \U_i \|_2}  
    \end{align*}
    for noise $\epsilon \sim \mathcal{N}(0, \sigma^2 / d)$, in the limit of $d \rightarrow \infty$. 
\end{thm} 

\begin{proof}
Without loss of generality, assume that $\X = 0$. For any other $\X_1 \neq 0$, we can simply perform a change of variables of the underlying function to center it at $\X_1$ to yield a different quadratic. We first write an expression for $p^{robust}_\sigma$ for the given quadratic classifier $g_i(\X)$ at $\X = 0$. 

\begin{align*}
    p^{robust}_\sigma(0) &= P_{\epsilon} \left( \bigcup_i g_i(\epsilon) > 0 \right) \\
                          &= P_{\epsilon} \left( \bigcup_i \U_i^\top \epsilon + c > - \epsilon^\top A_i \epsilon \right) 
\end{align*}

Similarly, computing, $p^{taylor}_{\sigma}$ we have $\grad g_i(0) = \U^\top$ and $g_i(0) = c_i$, resulting in

\begin{align*}
    p^{taylor}_{\sigma}(0) &= P_{\epsilon}\left(\bigcup_i g^{taylor}_i(\epsilon) > 0\right) \\
    &= P_{\epsilon}\left(\bigcup_i \U_i^\top \epsilon + c > 0 \right)
\end{align*}

Subtracting the two, we have 

\begin{align*}
    &|p^{robust}_\sigma(0) - p^{taylor}_\sigma(0)| \\
    &= \left| P \left(\bigcup_i 0 >  \U_i^\top \epsilon + c > - \epsilon^\top A_i \epsilon \right) \right| \\
    &= \left| P \left(\bigcup_i 0 >  \frac{\U_i^\top \epsilon + c}{\sigma \| \U_i \|_2} > - \frac{\epsilon^\top A_i \epsilon}{\sigma \| \U_i \|_2} \right) \right| 
\end{align*}

For high-dimensional Gaussian noise $\epsilon \sim \mathcal{N}(0, \sigma^2 / d)$, with $d \rightarrow \infty$, we have that $\| \epsilon \|^2 = \sum_i \epsilon_i^2 \rightarrow \sigma^2$ from the law of large numbers. See \cite{vershynin2018high} for an extended discussion. Thus we have $\epsilon^\top A \epsilon \leq \lambda_{\max}^A \| \epsilon \|^2 = \lambda_{\max}^A \sigma^2$.

Also let $z_i = \frac{\U_i^\top \epsilon + c}{\sigma \| \U_i \|_2}$ be a random variable. We observe that $z_i \vert_i$ is a tensor extension of $z_i$, has a covariance matrix of $\matU \matU^\top$ as before. Let us also define $\mathcal{C}_i = \frac{\lambda_{\max}^{A_i}}{\| \U_i \|_2}$.

\begin{align*}
    |&p^{robust}_\sigma(0) - p^{taylor}_\sigma(0)| \\&= \left|P \left( \bigcup_i 0 > z_i(\epsilon) > - \frac{\epsilon^\top A_i \epsilon}{\sigma \| \U_i \|_2} \right) \right| \\
    &\leq \left| P\left(\bigcup_i 0 > z_i > - \frac{\lambda_{\max}^{A_i}}{\| \U_i \|_2} \sigma\right) \right|~~~(\epsilon^\top A \epsilon < \lambda_{\max}^A \sigma^2) \\
    &= \left| \int ... \int^{0}_{-\mathcal{C}_i \sigma} \text{pdf}(z_i \vert_i)~ \mathrm{d}z_i \vert_i \right| ~~~(\text{Defn of mvn cdf}) \\
    &\leq \max_{z_i \vert_i} \text{pdf}(z_i \vert_i) ~ \prod_i |C_i \sigma |~~~(\text{Upper bound pdf with its max})\\
    &\leq (2\pi)^{-(C-1)/2} \det(\matU \matU^\top)^{-1/2} \prod_{\substack{i=1\\i\neq t}}^C C_i \sigma \\ &=  k \left(\sigma^{C-1} \prod_{\substack{i=1\\i\neq t}}^C \frac{\lambda_{\max}^{A_i}}{\| \U_i \|_2} \right)
\end{align*}

where $k = \max_z pdf(z) = (2 \pi)^{-(C-1)/ 2} \det(\matU \matU^\top)^{-1/2}$, which is the max value of the Gaussian pdf. Note that as the rows of $\matU$ are normalized, $\det(\matU) \leq 1$ and $\det(\matU \matU^\top) = \det(\matU)^2 \leq 1$.

\end{proof}

We note that these bounds are rather pessimistic, as in high-dimensions $\epsilon^\top A_i \epsilon \sim \lambda_{\mn}^{A_i} \leq \lambda_{\max}^{A_i}$, and thus in reality the errors are expected to be much smaller. 

\vspace{1cm}


\begin{thm}
    The \textbf{MMSE estimator} for the local robustness of a classifier $f$ at point $\X$ with respect to target class $t$ is given by an MMSE linearization $f$ around $\X$, for decision boundaries $g_i(\X) = f_t(\X) - f_i(\X)$, $\forall i \neq t$, leading to
    \begin{align*}
        &p^\text{mmse}_{\sigma}(\X) = \cdf \left( \frac{ \Tilde{g}_i(\X)}{\sigma \| \grad \Tilde{g}_i(\X)\|_2} \tensor \right) \\
        &\mathrm{where}~~\Tilde{g}_i(\X) = \frac{1}{N}\sum_{j=1}^{N} g_i(\X + \epsilon) ~,~ \epsilon \sim \mathcal{N}(0, \sigma^2)
    \end{align*}
    with $\matU$ and $\Phi$ defined as in the linear case, and $N$ is the number of perturbations. 

\end{thm}

\begin{proof}
We would like to improve upon the Taylor approximation to $g(\X + \epsilon)$ by using an MMSE local function approximation. Essentially, we'd like the find $\U \in \R^d$ and $c \in \R$ such that 

\begin{align*}
    (\U^*(\X), c^*(\X)) = \arg\min_{\U,c} \E_{\epsilon \sim \mathcal{N}(0, \sigma^2)} (g(x+\epsilon) - \U^{\top} \epsilon - c)^2
\end{align*}

A straightforward solution by finding critical points and equating it to zero gives us the following:

\begin{align*}
    \U^*(\X) &= \E_\epsilon \left[ g(x + \epsilon) \epsilon^{\top} \right] / \sigma^2 \\&= \E_\epsilon \left[ \grad g(\X + \epsilon) \right] ~~~~~ (\text{Stein's Lemma}) \\
    c^*(\X) &= \E_\epsilon g(x + \epsilon)
\end{align*}

Plugging in these values of $U^*, c^*$ into Lemma \ref{proof:lemma}, we have the result.

\end{proof}



\vspace{1cm}


\begin{thm} The \textbf{estimation error} of the MMSE estimator for a classifier with a quadratic decision boundary $g(\X) = \X^\top A \X + \U^\top \X + c$, and positive definite $A$ is upper bounded by
    \begin{align*}
        | p^{robust}_{\sigma}(\X) - p^{mmse}_{\sigma}(\X) | \leq k \sigma^{C-1} \prod_{\substack{i=1\\i\neq t}}^C \frac{|\lambda_{\max}^{A_i} - \lambda_{\mathrm{mean}}^{A_i}|}{\| \U_i \|_2}
    \end{align*}
    for noise $\epsilon \sim \mathcal{N}(0, \sigma^2 / d)$, in the limit of $d \rightarrow \infty$ and $N \rightarrow \infty$.  
\end{thm}

\begin{proof}
We proceed similarly to the proof made for the Taylor estimator, and without loss of generality, assume that $\X = 0$. Computing, $p^{mmse}_{\sigma}$ we have $\E_{\epsilon} \grad g_i(\epsilon) = \U_i^\top$ and $\E_{\epsilon} g_i(\epsilon) = c + \E (\epsilon^\top A_i \epsilon) = c + \E(trace(\epsilon^\top A_i \epsilon)) = c + \E(trace(A_i \epsilon \epsilon^T)) = c + trace(A_i) \sigma^2 / d = c + \sigma^2 \lambda_{\mn}^{A_i}$, resulting in

\begin{align*}
    p^{mmse}_{\sigma}(0) &= P_{\epsilon}\left(\bigcup_i \hat{g}_i(\epsilon) > 0\right) \\
    &= P_{\epsilon}\left(\bigcup_i \U_i^\top \epsilon + c > - \sigma^2 \lambda_{\mn}^{A_i} \right)
\end{align*}

Subtracting the two, we have 

\begin{align*}
    &|p^{robust}_\sigma(0) - p^{mmse}_\sigma(0)| \\
    & \leq \left|P \left(\bigcup_i - \sigma^2 \lambda_\mn^{A_i} >  \U_i^\top \epsilon + c > - \sigma^2 \lambda_{\max}^{A_i} \right) \right| \\
    &= \left|P \left(\bigcup_i - \sigma \frac{\lambda_{\mn}^{A_i}}{\| \U_i \|_2} >  \frac{\U_i^\top \epsilon + c}{\sigma \| \U_i \|_2} > - \sigma \frac{\lambda_{\max}^{A_i}}{\| \U_i \|_2} \right) \right| 
\end{align*}

Similar to the previous proof, let $z_i = \U_i^\top \epsilon + c$ be a random variable, and that $z_i \vert_i$ is a tensor extension of $z_i$ from our previous notation.

\begin{align*}
    |&p^{robust}_\sigma(\X) - p^{mmse}_\sigma(\X)| \\
    &\leq \left| P\left(\bigcup_i  - \lambda_{\mn}^{A_i} \sigma^2 > z_i > - \lambda_{\max}^{A_i} \sigma^2\right) \right| \\
    &= \left| \int ... \int^{-\lambda_{\mn}^{A_i} \sigma^2}_{-\lambda_{\max}^{A_i} \sigma^2} \text{pdf}(z_i \vert_i)~ \mathrm{d}z_i \vert_i \right| \\
    &\leq \max_{z_i \vert_i} \text{pdf}(z_i \vert_i) ~ \sigma^{C-1} \prod_i \frac{|(\lambda_{\max}^{A_i} - \lambda_{\mn}^{A_i}) |}{\| \U_i \|_2}\\
    &= k \sigma^{C-1} \prod_i \frac{|\lambda_{\max}^{A_i} - \lambda_{\mn}^{A_i}|}{\| \U_i \|2}
\end{align*}

where $k = \max_z \text{pdf}(z_i \vert_i) = (2 \pi)^{-(C-1)/ 2} \det(\matU \matU^\top)^{-1/2}$ like in the Taylor case. Note that as the rows of $\matU$ are normalized, $\det(\matU) \leq 1$ and $\det(\matU \matU^\top) = \det(\matU)^2 \leq 1$.

\end{proof}

We note that these bounds are rather pessimistic, as in high-dimensions $\epsilon^\top A_i \epsilon \sim \lambda_{\mn}^{A_i} \leq \lambda_{\max}^{A_i}$, and thus in reality the errors are expected to be much smaller. 

\vspace{1cm}

\paragraph{The softmax estimator} We observe that for linear models with a specific noise perturbation $\sigma$, the common softmax function taken with respect to the output logits can be viewed as an estimator of \probust{}, albeit in a very restricted setting. Specifically,

\begin{lemma}
    For multi-class linear models $f(\X) = \mathbf{w}^\top \X + b$, such that the decision boundary weight norms $\| \U_i \|_2 = k, \forall i \in [1, C], i \neq t$,
    \begin{align*}
        p^\text{softmax}_{T} = p^\text{taylor\_mvs}_{\sigma}~~~~\text{where}~~~~T = \sigma k
    \end{align*}
\label{lemma:softmax}
\end{lemma}

\begin{proof} Consider softmax with respect to the $t^{th}$ output class and define $g_i(\X) = f_t(\X) - f_i(\X)$, with $f$ being the linear model logits. Using this, we first show that softmax is identical to \emph{mv-sigmoid}:

\begin{align*}
        p^\text{softmax}_T(\X) &= \text{softmax}_t(f_1(\X)/T, ..., f_C(\X)/T) \\
        &= \frac{\exp(f_t(\X)/T)}{\sum_i \exp(f_i(\X)/T)} \\ 
        &= \frac{1}{1 + \sum_{i; i\neq t} \exp((f_i(\X) - f_t(\X))/T)} \\ 
        &= ~\text{mv-sigmoid} \left[ g_i(\X)/T \tensor \right]
\end{align*}

Next, by denoting $\U_i = \W_t - \W_i$, each row has equal norm $\| \U_i \|_2 = \| \U_j \|_2, \forall i,j,t \in [1,...C]$ which implies: 

\begin{align*}
        p^\text{taylor\_mvs}_\sigma(\X) &= \text{mv-sigmoid} \left[ \frac{g_i(\X)}{\sigma \| \U_i \|_2} \tensor \right]\\ 
        &= \text{mv-sigmoid} \left[ g_i(\X)/T \tensor \right]~~ (\because \text{$T = \sigma k $})\\ 
        & = p^\text{softmax}_T(\X)
\end{align*}
\end{proof}

Lemma~\ref{lemma:softmax} indicates that the temperature parameter $T$ of softmax roughly corresponds to the $\sigma$ of the added Normal noise with respect to which local robustness is measured. Overall, this shows that under the restricted setting where the local linear model consists of decision boundaries with equal weight norms, the softmax outputs can be viewed as an estimator of the \ptaylormvs{} estimator, which itself is an estimator of \probust{}. However, due to the multiple levels of approximation, we can expect the quality of \psoftmax{}'s approximation of \probust{} to be poor in general settings (outside of the very restricted setting), so much so that in general settings, \probust{} and \psoftmax{} would be unrelated.

\subsection{Datasets}
\label{app:datasets}

The MNIST dataset consists of images of gray-scale handwritten digits spanning 10 classes: digits 0 through 9. The FashionMNIST (FMNIST) dataset consists of gray-scale images of articles of clothing spanning 10 classes: t-shirt, trousers, pullover, dress, coat, sandal, shirt, sneaker, bag, and ankle boot. For MNIST and FMNIST, each image is 28 pixels x 28 pixels. For MNIST and FMNIST, the training set consists of 60,000 images and the test set consists of 10,000 images.

The CIFAR10 dataset consists of color images of common objects and animals spanning 10 classes: airplane, car, bird, cat, deer, dog, frog, horse, ship, and truck. The CIFAR100 dataset consists of color images of common objects and animals spanning 100 classes: apple, bowl, chair, dolphin, lamp, mouse, plain, rose, squirrel, train, etc. For CIFAR10 and CIFAR100, each image is 3 pixels x 32 pixels x 32 pixels. For CIFAR10 and CIFAR100, the training set consists of 50,000 images and the test set consists of 10,000 images.


\subsection{Models}
\label{app:models}

For the MNIST and FMNIST, we train a linear model and a convolutional neural network (CNN) to perform 10-class classification. The linear model consists of one hidden layer with 10 neurons. The CNN consists of four hidden layers: one convolutional layer with 5x5 filters and 10 output channels, one convolutional layer 5x5 filters and 20 output channels, and one linear layer with 50 neurons, and one linear layer 10 neurons. 

For CIFAR10 and CIFAR100, we train a ResNet18 model to perform 10-class and 100-class classification, respectively. The model architecture is described in \citep{he2016deep}. We train the ResNet18 models using varying levels of gradient norm regularization to obtain models with varying levels of robustness. The larger the weight of gradient norm regularization ($\lambda$), the more robust the model.

All models were trained using stochastic gradient descent. Hyperparameters were selected to achieve decent model performance. The emphasis is on analyzing the estimators’ estimates of local robustness of each model, not on high model performance. Thus, we do not focus on tuning model hyperparameters. All models were trained for 200 epochs. The test set accuracy for each model is shown in Table~\ref{table:app-model-acc}.

\begin{table*}[ht!]
    \centering
    \begin{tabular}{c|c|c|c}
    Dataset      & Model  & $\lambda$  & Test set accuracy \\
    \midrule
    MNIST        & Linear  & 0 & 92\%                         \\
    MNIST        & CNN     & 0 & 99\%                         \\
    \midrule
    FashionMNIST & Linear  & 0 & 84\%                         \\
    FashionMNIST & CNN     & 0 & 91\%                         \\
    \midrule
    CIFAR10      & ResNet18 & 0 & 94\%                         \\
    CIFAR10      & ResNet18 & 0.0001 & 93\%                         \\
    CIFAR10      & ResNet18 & 0.001 & 90\%                         \\
    CIFAR10      & ResNet18 & 0.01 & 85\%                         \\
    \midrule
    CIFAR100     & ResNet18 & 0 & 76\%                        \\
    CIFAR100     & ResNet18 & 0.0001 & 74\%                         \\
    CIFAR100     & ResNet18 & 0.001 & 69\%                         \\
    CIFAR100     & ResNet18 & 0.01 & 60\%                         
    \end{tabular}
    \vspace*{3mm}
    \caption{Test set accuracy of models.}
    \label{table:app-model-acc}
\end{table*}

\subsection{Experiments}
\label{app:experiments}
In this section, we provide the following additional experimental results:

\begin{enumerate}
    \item Figure \ref{app:convergence} shows results on the convergence of \pmc{}. \pmc{} takes a large number of samples to converge and is computationally inefficient.
    \item Figure \ref{app:convergence_mmse} shows results on the convergence of \pmmse{}. \pmmse{} takes only a few samples to converge and is more computationally inefficient than \pmc{}.
    \item Figure \ref{app:distribution_probust} shows the distribution of \probust{} as a function of $\sigma$. Consistent with theory in Section \ref{sec:methods}, (1) as noise increases, \probust{} decreases, and (2) \pmmse{} accurately estimates \pmc{}.
    \item Table \ref{app:runtimes} presents estimator runtimes. Our analytical estimators are more efficient than the naïve estimator (\pmc{}).
    \item Figure \ref{app:accuracy_probust} shows the accuracy of the analytical robustness estimators as a function of $\sigma$. \pmmse{} and \pmmsemvs{} are the best estimators of \probust{}, followed closely by \ptaylormvs{} and \ptaylor{}, trailed by \psoftmax{}.
    \item Figure \ref{app:accuracy_robust} shows the accuracy of the analytical estimators for robust models. For more robust models, the estimators compute \probust{} more accurately over a larger $\sigma$.
    \item Figure \ref{app:mvsigmoid} shows that \emph{mv-sigmoid} well-approximates \emph{mvn-cdf} over $\sigma$.
    \item Figure \ref{app:robustness_bias} shows the distribution of \probust{} among classes (measured by \pmmse{}), revealing that models display robustness bias among classes. 
    \item Figures \ref{fig5:topk-vs-bottomk} and \ref{fig6:topk-vs-bottomk} show the application of \pmmse{} and \psoftmax{} to identification of robust and non-robust points. \probust{} better identifies robust and non-robust points than \psoftmax{}. 
    \item Figures \ref{app:noisy_mnist}, \ref{app:noisy_fmnist}, \ref{app:noisy_cifar10}, and \ref{app:noisy_cifar100} show examples of noisy images with the level of noise analyzed in our paper. Overall, the noise levels seem visually significant.
 \end{enumerate}

%appendix/a_convergence_pmc = 02a_p_emp_convergence_n50000_baseline
% Figure environment removed


%appendix/b_convergence_pmmse = 02b_p_mmse_convergence_n500_baseline
% Figure environment removed

%appendix/c_probust_over_noise = 02b_p_over_noise/p_all_over_noise

%appendix/c_probust_over_noise = 02c_p_vs_sigma/p_all_vs_sigma

% Figure environment removed

%appendix/d_accuracy_of_estimators = 02d_pemp_vs_pothers_over_sigma/rel
% Figure environment removed

%appendix/e_accuracy_of_estimators_robust_models = 02e_pemp_vs_pmmse_over_sigma_robust_models/rel
% Figure environment removed

%appendix/f_mvsigmoid_vs_mvncdf = 03_mvncdf_vs_mvsigmoid/cifar10_resnet18
% Figure environment removed

%appendix/g_robustness_bias = 02f_p_distr_vs_classes/p_all_over_classes
% Figure environment removed

%table: naive method is inefficient, analytical method is efficient
\begin{table*}[h]
\centering
\begin{tabular}{l|l|l|l|l|l}
    \multicolumn{2}{c}{}   & \multicolumn{2}{|c|}{CPU: Intel x86\_64}   & \multicolumn{2}{|c}{GPU: Tesla V100-PCIE-32GB} \\
    \midrule
    Estimator   & \# samples ($n$)   & Serial   & Batched   & Serial   & Batched \\
    \midrule
    \pmc{}   & \begin{tabular}[c]{@{}l@{}}$n=100$\\ $n=1000$\\ $n=10000$\end{tabular}               
             & \begin{tabular}[c]{@{}l@{}}0:00:59\\ 0:09:50\\ \textit{1:41:11}\end{tabular}                                               
             & \begin{tabular}[c]{@{}l@{}}0:00:42\\ 0:07:22\\ \textit{1:14:38}\end{tabular}                                                
             & \begin{tabular}[c]{@{}l@{}}0:00:12\\ 0:02:00\\ \textit{0:19:56}\end{tabular}                                                
             & \begin{tabular}[c]{@{}l@{}}0:00:01\\ 0:00:04\\ \textit{0:00:35}\end{tabular} \\
    \midrule
    \ptaylor{}   & N/A
                 & 0:00:08                                                                                                                      
                 & 0:00:07                                                                                                                      
                 & 0:00:02                                                                                                                      
                 & $<$ 0:00:01 \\
    \midrule
    \ptaylormvs{}   & N/A
                    & 0:00:08                                                                                                                   
                    & 0:00:07                                                                                                                  
                    & 0:00:01                                                                                                                   
                    & $<$ 0:00:01 \\
    \midrule
    \pmmse{}   & \begin{tabular}[c]{@{}l@{}}$n=1$\\ $n=5$\\ $n=10$\\ $n=25$\\ $n=50$\\ $n=100$\end{tabular} 
               & \begin{tabular}[c]{@{}l@{}}0:00:08\\ \textit{0:00:41}\\ 0:01:21\\ 0:03:21\\ 0:06:47\\ 0:13:57\end{tabular} 
               & \begin{tabular}[c]{@{}l@{}}0:00:10\\ \textit{0:00:31}\\ 0:01:02\\ 0:02:44\\ 0:05:38\\ 0:11:31\end{tabular} 
               & \begin{tabular}[c]{@{}l@{}}0:00:02\\ \textit{0:00:06}\\ 0:00:11\\ 0:00:26\\ 0:00:51\\ 0:01:42\end{tabular} 
               & \begin{tabular}[c]{@{}l@{}}0:00:02\\ \textit{0:00:02}\\ 0:00:02\\ 0:00:03\\ 0:00:04\\ 0:00:06\end{tabular} \\
    \midrule
    \pmmsemvs{}   & \begin{tabular}[c]{@{}l@{}}$n=1$\\ $n=5$\\ $n=10$\\ $n=25$\\ $n=50$\\ $n=100$\end{tabular} 
                  & \begin{tabular}[c]{@{}l@{}}0:00:08\\ \textit{0:00:41}\\ 0:01:21\\ 0:03:24\\ 0:06:47\\ 0:13:28\end{tabular} 
                  & \begin{tabular}[c]{@{}l@{}}0:00:08\\ \textit{0:00:32}\\ 0:01:00\\ 0:02:37\\ 0:05:35\\ 0:11:32\end{tabular} 
                  & \begin{tabular}[c]{@{}l@{}}0:00:01\\ \textit{0:00:05}\\ 0:00:10\\ 0:00:25\\ 0:00:51\\ 0:01:42\end{tabular} 
                  & \begin{tabular}[c]{@{}l@{}}0:00:01\\ \textit{0:00:01}\\ 0:00:02\\ 0:00:02\\ 0:00:03\\ 0:00:06\end{tabular} \\
    \midrule
    \psoftmax{}   & N/A                                                                             
                  & 0:00:01                                                                                                                              
                  & $<$ 0:00:01                                                                                                                              
                  & $<$ 0:00:01                                                                                                                              
                  & $<$ 0:00:01                                                                                                                             
\end{tabular}
\caption{Runtimes of each \probust{} estimator. Each estimator computes \probustwsigma{0.1} for the CIFAR10 ResNet18 model for 50 data points. For estimators that use sampling, the row with the minimum number of samples necessary for convergence is italicized. Runtimes are in the format of hour:minute:second. The analytical estimators (\ptaylor{}, \ptaylormvs{}, \pmmse{}, and \pmmsemvs{}) are more efficient than the naïve estimator (\pmc{}).} \label{app:runtimes}
\end{table*}

\subsubsection{\probust{} identifies images that are robust to and images
that are vulnerable to random noise}

For each dataset, we train a simple CNN to distinguish between images with high and low \pmmse{}. We train the same CNN to also distinguish between images with high and low \psoftmax{}. The CNN consists of two convolutional layers and two fully-connected feedforward layers with a total of 21,878 parameters. For a given dataset, for each class, we take the images with the top-25 and bottom-25 \pmmse{} values. This yields 500 images for CIFAR10 (10 classes x 50 images per class) and 5,000 images for CIFAR100 (100 classes x 50 images per class). We also perform the same steps using \psoftmax{}, yielding another 500 images for CIFAR10 and another 5,000 images for CIFAR100. For each dataset, the train/test split is 90\%/10\% of points. 

Then, we compare the performance of the two models. For CIFAR10, the test set accuracy for the \pmmse{} CNN is 0.92 while that for the \psoftmax{} CNN is 0.58. For CIFAR100, the test set accuracy for the \pmmse{} CNN is 0.74 while that for the \psoftmax{} CNN is 0.55. The higher the test set accuracy of a CNN, the better the CNN distinguishes between images. Thus, the results indicate that \probust{} better identifies images that are robust to and vulnerable to random noise than \psoftmax{}.

We also provide additional visualizations of images with the highest and lowest \probust{} and images with the highest and lowest \psoftmax{}.

%fig4 -- 2x4 images
%top-k and bottom-k images
% 02g_topk_bottomk_images/cifar10_resnet18/p_mmse/
% - cifar10_resnet18_p_mmse_sigma0.1_class8_bottomk.png
% - cifar10_resnet18_p_mmse_sigma0.1_class8_topk.png
% - ... class1 x 2
% 02g_topk_bottomk_images/cifar10_resnet18/p_sm
% - cifar10_resnet18_p_sm_sigma0.1_class8_bottomk.png
% - cifar10_resnet18_p_sm_sigma0.1_class8_topk.png
% - ... class1 x 2

%     \begin{flushleft}
%         %row labels
%         \hspace{-0.1cm}\rotatebox{90}{\hspace{-8.7cm}Truck \hspace{2.4cm}Boat \hspace{2.1cm}Airplane}
%         %column labels
%         \hspace{0.9cm}Lowest \pmmsewsigma{0.1}
%         \hspace{1cm} Highest \pmmsewsigma{0.1}
%         \hspace{1cm} Lowest \psoftmax{}
%         \hspace{1cm} Highest \psoftmax{}
%     \end{flushleft}
         
%     \begin{subfigure}{0.23\textwidth}
%         % Figure removed
%     \end{subfigure}
%     \begin{subfigure}{0.23\textwidth}
%         % Figure removed
%     % \hspace{0.2cm}
%     \end{subfigure}
%     \begin{subfigure}{0.23\textwidth}
%         % Figure removed
%     \end{subfigure}
%     \begin{subfigure}{0.23\textwidth}
%         % Figure removed
%     \end{subfigure}
    
%     \begin{subfigure}{0.23\textwidth}
%         % Figure removed
%     \end{subfigure}
%     \begin{subfigure}{0.23\textwidth}
%         % Figure removed
%     \end{subfigure}
%     % \hspace{0.2cm}
%     \begin{subfigure}{0.23\textwidth}
%         % Figure removed
%     \end{subfigure}
%     \begin{subfigure}{0.23\textwidth}
%         % Figure removed
%     \end{subfigure}
    
%     \begin{subfigure}{0.23\textwidth}
%         % Figure removed
%     \end{subfigure}
%     \begin{subfigure}{0.23\textwidth}
%         % Figure removed
%     \end{subfigure}
%     % \hspace{0.2cm}
%     \begin{subfigure}{0.23\textwidth}
%         % Figure removed
%     \end{subfigure}
%     \begin{subfigure}{0.23\textwidth}
%         % Figure removed
%     \end{subfigure}
%     \caption{Images with the lowest and highest \probust{} (\pmmse{}) and \psoftmax{} values among CIFAR10 classes. Images with high \probust{} tend to be brighter and have stronger object-background contrast (making them more robust to random noise) than those with low \probust{}. The difference between images with high and low \psoftmax{} is less clear. Thus, \probust{} better captures the model's local robustness with respect to an input than \psoftmax{}.}
%     \label{fig4:topk-vs-bottomk}
% \end{figure}


% Figure environment removed


%fig4 -- 2x4 images
%top-k and bottom-k images
% 02g_topk_bottomk_images/cifar10_resnet18/p_mmse/
% - cifar100_resnet18_p_mmse_sigma0.1_class8_bottomk.png
% - cifar100_resnet18_p_mmse_sigma0.1_class8_topk.png
% - ... class23 x 2
% 02g_topk_bottomk_images/cifar10_resnet18/p_sm
% - cifar100_resnet18_p_sm_sigma0.1_class8_bottomk.png
% - cifar100_resnet18_p_sm_sigma0.1_class8_topk.png
% - ... class23 x 2
% Figure environment removed

%3x1 noisy images
%top-k and bottom-k images
% 00_noisy_images/mnist/
% - mnist_imgidx10_class0_noisy_sigmas0.0_0.2_0.4_0.6_0.8_1.0.png
% - mnist_imgidx2_class1_noisy_sigmas0.0_0.2_0.4_0.6_0.8_1.0.png
% - mnist_imgidx1_class2_noisy_sigmas0.0_0.2_0.4_0.6_0.8_1.0.png
% Figure environment removed


%3x1 noisy images
%top-k and bottom-k images
% 00_noisy_images/fmnist/
% - fmnist_imgidx7_classshirt_noisy_sigmas0.0_0.2_0.4_0.6_0.8_1.0.png
% - fmnist_imgidx2_classtrousers_noisy_sigmas0.0_0.2_0.4_0.6_0.8_1.0.png
% - fmnist_imgidx0_classankle_boot_noisy_sigmas0.0_0.2_0.4_0.6_0.8_1.0.png
% Figure environment removed

%3x1 noisy images
%top-k and bottom-k images
% 00_noisy_images/cifar10/
% - cifar10_imgidx22_classdeer_noisy_sigmas0.0_0.02_0.04_0.06_0.08_0.1.png
% - cifar10_imgidx2_classship_noisy_sigmas0.0_0.02_0.04_0.06_0.08_0.1.png
% - cifar10_imgidx10_classairplane_noisy_sigmas0.0_0.02_0.04_0.06_0.08_0.1.png
% Figure environment removed


%3x1 noisy images
%top-k and bottom-k images
% 00_noisy_images/cifar10/
% - cifar100_imgidx4_classsea_noisy_sigmas0.0_0.02_0.04_0.06_0.08_0.1.png
% - cifar100_imgidx8_classcloud_noisy_sigmas0.0_0.02_0.04_0.06_0.08_0.1.png
% - cifar100_imgidx15_classlion_noisy_sigmas0.0_0.02_0.04_0.06_0.08_0.1.png
% Figure environment removed



%% Figure environment removed
