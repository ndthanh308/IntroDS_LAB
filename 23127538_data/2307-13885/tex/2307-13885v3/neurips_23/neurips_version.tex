\documentclass{article}

\PassOptionsToPackage{numbers, compress}{natbib}
% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% ready for submission
% \usepackage{neurips_2023}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
\usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{sidecap}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}


\newcommand{\probust}{\texorpdfstring{$p^\mathrm{robust}_{\sigma}$}{probust}}

\newcommand{\pmc}{\texorpdfstring{$p^\mathrm{mc}_{\sigma}$}{pmc}}

\newcommand{\ptaylor}{\texorpdfstring{$p^\mathrm{taylor}_{\sigma}$}{ptaylor}}

\newcommand{\ptaylormvs}{\texorpdfstring{$p^\mathrm{taylor\_mvs}_{\sigma}$}{ptaylormvs}}

\newcommand{\pmmse}{\texorpdfstring{$p^\mathrm{mmse}_{\sigma}$}{pmmse}}

\newcommand{\pmmsemvs}{\texorpdfstring{$p^\mathrm{mmse\_mvs}_{\sigma}$}{pmmsemvs}}

\newcommand{\psoftmax}{\texorpdfstring{$p^\mathrm{softmax}_{T}$}{psoftmax}}

\newcommand{\probustwsigma}[1]{\texorpdfstring{$p^\mathrm{robust}_{\sigma= {#1}}$}{probust}}

\newcommand{\pmmsewsigma}[1]{\texorpdfstring{$p^\mathrm{mmse}_{\sigma = {#1}}$}{pmmse}}

\newcommand{\suraj}[1]{{\color{cyan} Suraj: #1}}


\title{Efficient Estimation of Local Robustness of \\Machine Learning Models}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.



\author{%
  Tessa Han \\
  Harvard University\\
  Cambridge, MA \\
  \texttt{than@g.harvard.edu}
  \And
  Suraj Srinivas \\
  Harvard University\\
  Cambridge, MA \\
  \texttt{ssrinivas@seas.harvard.edu}
  \And
  Himabindu Lakkaraju \\
  Harvard University\\
  Cambridge, MA \\
  \texttt{hlakkaraju@hbs.edu}
}


\begin{document}


\maketitle


\begin{abstract}
    Machine learning models often need to be robust to noisy input data. Real-world noise (such as measurement noise) is often random and the effect of such noise on model predictions is captured by a model’s local robustness, i.e., the consistency of model predictions in a local region around an input. Local robustness is therefore an important characterization of real-world model behavior and can be useful for debugging models and establishing user trust. However, the naïve approach to computing local robustness based on Monte-Carlo sampling is statistically inefficient, especially for high-dimensional data, leading to prohibitive computational costs for large-scale applications. In this work, we develop the first analytical estimators to efficiently compute local robustness of multi-class discriminative models. These estimators linearize models in the local region around an input and compute the model’s local robustness using the multivariate Normal cumulative distribution function. Through the derivation of these estimators, we show how local robustness is connected to such concepts as randomized smoothing and softmax probability. In addition, we show empirically that these estimators efficiently compute the local robustness of standard deep learning models and demonstrate these estimators’ usefulness for various tasks involving local robustness, such as measuring robustness bias and identifying examples that are vulnerable to noise perturbation in a dataset. To our knowledge, this work is the first to investigate local robustness in a multi-class setting and develop efficient analytical estimators for local robustness. In doing so, this work not only advances the conceptual understanding of local robustness, but also makes its computation practical, enabling the use of local robustness in critical downstream applications.
    % Local robustness is therefore an important characterization of real-world model behavior and can be useful in model debugging and establishing user trust.
    % The best estimator works by linearizing a randomized smoothed model, which arises naturally as the optimal minimum-mean-squared-error linear estimate.
\end{abstract}


% \begin{abstract}
%   Machine learning models are often required to make predictions for noisy input data. While adversarial robustness indicates the existence (or not) of an adversary in a local region around the input, real world noise is rarely adversarial and is often random, for example from measurement noise. Average local robustness, i.e., the fraction of inputs in a local regions in which the model provides correct prediction, is thus a more comprehensive characterization of real-world model behaviour, and can be useful in debugging machine learning models and establishing user trust.

%   %Therefore, measuring the local robustness of a model, i.e., the fraction of inputs in a local volume on which a model provides correct predictions, is critical to characterizing and debugging model behavior for establishing user trust.
  
%   However, the naive approach of computing local robustness via explicitly sampling noisy inputs in a local region is statistically inefficient especially for high-dimensional data, leading to prohibitive computational costs for large-scale applications. In this paper, we present analytical approaches to efficiently compute local robustness of multi-class discriminative models without costly sampling. Our estimators work by linearizing non-linear models, and computing the local robustness of the resulting linear models via the multivariate normal CDF. Our best estimator  works by linearizing a randomized smoothed model, which arises naturally as the optimal minimum-mean-squared-error linear estimate.    
  
%   We show empirically that our estimators are accurate for computing local robustness of standard deep learning models and demonstrate its usefulness for various tasks involving average local robustness, such as measuring per-class robustness bias and capturing ambiguous and noisy dataset examples.
% \end{abstract}


\input{1-intro}
\input{2-related-work}
\input{3-method}
\input{4-experiments}
\input{5-conclusion}


% \section*{Acknowledgements}



\bibliographystyle{unsrtnat}
\bibliography{references.bib}


%\newpage
%\appendix
%\input{6-appendix}


\end{document}