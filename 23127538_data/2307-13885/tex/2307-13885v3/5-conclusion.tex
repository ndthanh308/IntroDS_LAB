\section{Conclusion}

In this work, we take the first steps towards estimating average-case robustness of multi-class classifiers. Studying average-case robustness provides several advantages over adversarial robustness. First, the nature of real-world noise demands randomized average-case analysis rather than an adversarial one. Second, average-case robustness strictly generalizes adversarial robustness, and therefore provides more information about model behavior than adversarial robustness. Finally, we believe that average-case robustness can offer a gentler objective to train neural networks, when compared to the adversarial case. In particular, requiring models to minimize the probability of misclassification in a local neighborhood is a more lenient objective than attempting to eliminate misclassification entirely as is usually done in adversarial robustness \cite{croce2021robustbench}.

%To our knowledge, this work is the first to investigate average-case robustness in a multi-class setting and develop efficient analytical estimators. The analytical aspect of these estimators not only advances conceptual understanding of average-case robustness, connecting it to randomized smoothing (via the MMSE estimator) and softmax probability, but also enables local robustness to be used in applications that require differentiability. 

Future research directions include exploring additional applications of local robustness, such as training average-case robust models that minimize the probability of misclassification and using average-case robustness as a measure of prediction uncertainty.
