\section{Related Work}

%\textbf{Linearization of neural networks.} Prior works have used linear models to approximate neural networks in the local region around an input. For example, LIME \cite{ribeiro2016lime}, a popular post hoc explanation method, follows this approach and uses the coefficients of the linear model as feature attributions. The local function approximation framework \cite{han2022explanation} further demonstrates that eight popular post hoc explanation methods all perform local linear approximation of the underlying model. Local linear function approximation has also been used to generate probabilistically-robust counterfactual explanations, specifying the probability that a binary classifier generates consistent predictions when inputs are noisy \cite{pawelczyk2022probabilistically}. In contrast to these prior works which apply local linear function approximation to post hoc explainability \cite{ribeiro2016lime, han2022explanation, pawelczyk2022probabilistically} or focus on binary classification \cite{pawelczyk2022probabilistically}, this work uses local linear function approximation to investigate local robustness and develops analytical estimators of local robustness for both binary and multi-class classification.

\textbf{Adversarial robustness.} Prior works have proposed methods to generate adversarial attacks \cite{carlini2017towards, goodfellow2014explaining, moosavi2016deepfool}, which find adversarial perturbations in a local region around a point. In contrast, this work investigates average-case robustness, which calculates the probability that a modelâ€™s prediction remains consistent in a local region around a point. Prior works have also proposed methods to certify model robustness \cite{cohen2019certified, carlini2022certified}, providing guarantees of dataset-level robustness (i.e., all points in a dataset are robust to a certain amount of noise). In contrast, this work focuses on establishing point-level local robustness. %In addition, prior work has investigated robustness bias in terms of vulnerability to adversarial attacks, lowerbounding and upperbounding a point's probability of adversarial attack using sampling approaches \cite{nanda2021fairness}. In contrast, this work investigates robustness bias in terms of local robustness, directly calculating a model's local robustness using analytical approaches.

\textbf{Probabilistic robustness.} Prior works have explored notions of probabilistic and average-case robustness.
% but they differ from our work in the generality of the problems considered. 
For instance, \cite{fazlyab2019probabilistic, kumar2020certifying, mangal2019robustness} focus on certifying robustness of real-valued outputs to input perturbations. In contrast, this work focuses on only those output changes that cause misclassification. Like our work, \cite{franceschi2018robustness} also considers misclassifications. However, \cite{franceschi2018robustness} aims to find the smallest neighborhood with no adversarial example, while we compute the probability of misclassification in a given neighborhood. \cite{robey2022probabilistically, rice2021robustness} also aim to compute average-case robustness. However, they do so by computing the average loss over the neighborhood, while we use the misclassification rate. Closest to our work is \cite{weng2019proven} which aims to certify the binary misclassification rate (with respect to a specific class to misclassify to) using lower and upper linear bounds. In contrast, our work estimates the multi-class misclassification rate, as opposed to bounding the quantity in a binary setting. A crucial contribution of our work is its applicability to multi-class classification and the emphasis on estimating, rather than bounding, robustness.

%\textbf{Uncertainty estimation.} Prior works have developed approaches to measure a model's prediction uncertainty. These include calibration \cite{guo2017calibration}, Bayesian uncertainty \cite{kendall2017uncertainties}, and conformal prediction \cite{shafer2008tutorial}. In contrast to these prior works in which prediction uncertainty is with respect to a calibration set \cite{guo2017calibration, shafer2008tutorial} or model parameters \cite{kendall2017uncertainties}, this work investigates average-case robustness, which can be viewed as prediction uncertainty with respect to input noise.