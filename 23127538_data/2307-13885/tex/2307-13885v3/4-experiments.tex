\section{Empirical Evaluation}
\label{sec:exp}

In this section, we first evaluate the estimation errors and computational efficiency of the analytical estimators, and then evaluate the impact of robustness training within models on these estimation errors. Then, we analyze the relationship between average-case robustness and softmax probability. Lastly, we demonstrate the usefulness of local robustness and its analytical estimators in real-world applications. Key results are discussed in this section and full results are in Appendix~\ref{app:experiments}.

%datasets and models
%\subsection{Datasets and Models}
\textbf{Datasets and models.}
We evaluate the estimators on four datasets: MNIST \citep{deng2012mnist}, FashionMNIST \citep{xiao2017fashion}, CIFAR10 \citep{krizhevsky2009learning}, and CIFAR100 \citep{krizhevsky2009learning}. For MNIST and FashionMNIST, we train linear models and CNNs to perform classification. For CIFAR10 and CIFAR100, we train ResNet18 \citep{he2016deep} models to perform classification. We train the ResNet18 models using varying levels of gradient norm regularization ($\lambda$) to obtain models with varying levels of robustness. The experiments below use each dataset's full test set, each consisting of 10,000 points. Additional details about the datasets and models are described in Appendix~\ref{app:datasets} and \ref{app:models}.


\subsection{Evaluation of the estimation errors of analytical estimators}
\label{sec:exp_correctness}

%\textbf{The analytical estimators accurately compute local robustness.}
To empirically evaluate the estimation error of our estimators, we calculate \probust{} for each model using \pmc{}, \ptaylor{}, \pmmse{}, \ptaylormvs{}, \pmmsemvs{}, and \psoftmax{} for different $\sigma$ values. For \pmc{}, \pmmse{}, and \pmmsemvs{}, we use a sample size at which these estimators have converged ($n=10000, 500, \text{and } 500$, respectively). (Convergence analyses are in Appendix~\ref{app:experiments}.) We take the monte-carlo estimator as the gold standard estimate of $p^{robust}_{\sigma}$), and compute the absolute and relative difference between \pmc{} and the other estimators to evaluate their estimation errors. 

%pmmse family = best estimator
The performance of the estimators for the FashionMNIST CNN model is shown in Figure~\ref{fig1a:method-works-over-sigma}. The results indicate that \pmmsemvs{} and \pmmse{} are the best estimators of \probust{}, followed closely by \ptaylormvs{} and \ptaylor{}, trailed by \psoftmax{}. This is consistent with the theory in Section~\ref{sec:methods}, where the analytical estimation errors of $p^{mmse}_{\sigma}$ are lower than $p^{taylor}_{\sigma}$.

%smaller noise neighborhood, better approximation
The results also confirm that the smaller the noise neighborhood $\sigma$, the more accurately the estimators compute \probust{}. For the MMSE and Taylor estimators, this is because their linear approximation of the model around the input is more faithful for smaller $\sigma$. As expected, when the model is linear, \ptaylor{} and \pmmse{} accurately compute \probust{} for all $\sigma$'s (Appendix~\ref{app:experiments}). For the softmax estimator, \psoftmax{} values are constant over $\sigma$ and this particular model has high \psoftmax{} values for most points. Thus, for small $\sigma$'s where \probust{} is near one, \psoftmax{} happens to approximate \probust{} for this model. Examples of images with varying levels of noise ($\sigma$) are in Appendix~\ref{app:experiments}.

\textbf{Impact of robust training on estimation errors.} 
The performance of \pmmse{} for CIFAR10 ResNet18 models of varying levels of robustness is shown in Figure~\ref{fig1b:method-works-robust}. The results indicate that for more robust models (larger $\lambda$), the estimator is more accurate over a larger $\sigma$. This is because gradient norm regularization leads to models that are more locally linear \cite{srinivas2022efficient}, making the estimator's linear approximation of the model around the input more accurate over a larger $\sigma$, making its \probust{} values more accurate.


\textbf{Evaluating estimation error of mv-sigmoid.} To examine \emph{mv-sigmoid}'s approximation of \emph{mvn-cdf}, we compute both functions using the same inputs ($z~=~ \frac{g_i(\X)}{\sigma \|\grad g_i(\X)\|_2} \vert_{\substack{i=1\\i\neq t}}^C$, as described in Proposition~\ref{eqn:taylor-estimator}) for the CIFAR10 ResNet18 model for different $\sigma$. The plot of \emph{mv-sigmoid(z)} against \emph{mvn-cdf(z)} for $\sigma=0.05$ is shown in Figure~\ref{fig2:mvsig-mvncdf}. The results indicate that the two functions are strongly positively correlated with low approximation error, suggesting that \emph{mv-sigmoid} approximates the \emph{mvn-cdf} well in practice.

% \clearpage

%fig1 -- method properly approximates p_empirical
%fig1a: method works
%02d_pemp_vs_pothers_over_sigma/rel/fmnist_cnn.png
%fig1b: method works better for robust models
%02e_pemp_vs_pmmse_over_sigma_robust_models/rel/cifar10_resnet18.png
% Figure environment removed

% Figure environment removed


%table: naive method is inefficient, analytical method is efficient
\begin{table}[ht!]
\centering
\begin{tabular}{l|l|l|l }
    Estimator   & \# samples ($n$)   & \thead{CPU\\runtime\\(h:m:s)}  & \thead{GPU\\Runtime\\(h:m:s)} \\
    \toprule
    \pmc{}   & \begin{tabular}[c]{@{}l@{}}  $n=10000$\end{tabular}               
             & \begin{tabular}[c]{@{}l@{}}  1:41:11\end{tabular}                 
             & \begin{tabular}[c]{@{}l@{}}  0:19:56\end{tabular}  \\
    \ptaylor{}   & N/A
                 & 0:00:08                                                                   
                 & 0:00:02  \\
    \pmmse{}   & \begin{tabular}[c]{@{}l@{}} $n=5$\end{tabular} 
               & \begin{tabular}[c]{@{}l@{}} 0:00:41\end{tabular} 
               & \begin{tabular}[c]{@{}l@{}} 0:00:06\end{tabular} \\              
\end{tabular}
\vspace{0.2cm}
\caption{Runtimes of \probust{} estimators. Each estimator computes \probustwsigma{0.1} for the CIFAR10 ResNet18 model for 50 data points. Estimators that use sampling use the minimum number of samples necessary for convergence. Runtimes are in the format of hour:minute:second. The GPU used was a Tesla V100. The analytical estimators (\ptaylor{} and \pmmse{}) are more efficient than the naïve estimator (\pmc{}).}
\vspace{-0.5cm}
\label{table:runtimes}
\end{table}

%fig4 -- p_robust and p_softmax
%fig4a: scatterplot, non-robust model
%02i_pemp_vs_pmmse_corr_robust_models_scatterplots/cifar10_resnet18_sigma0.1_gnormreg0.png
%fig4b: the more robust the model, the more the two are related
%02h_pemp_vs_pmmse_corr_robust_models_lineplots/cifar10_resnet18_cifar100_resnet18_sigma0.1.png
%fig4c: scatterplot, robust model
%02i_pemp_vs_pmmse_corr_robust_models_scatterplots/cifar10_resnet18_sigma0.1_gnormreg0.01.png
% Figure environment removed

% Figure environment removed

\subsection{Evaluation of computational efficiency of analytical estimators}

%\textbf{The naïve estimator is statistically inefficient.} To examine the efficiency of \pmc{}, we calculate \pmc{} for each model and test set using different sample sizes ($n$) over different $\sigma$'s, and measure the absolute and relative difference between \pmc{} at a given $n$ and \pmc{} at $n=50,000$. Results for the CIFAR10 ResNet18 model are shown in Figure~\ref{fig3:pmc-convergence}. The results indicate that \pmc{} requires around 10,000 samples per point to converge, which is impractical.

%\textbf{The analytical estimators are more efficient than the naïve estimator.}
We examine the efficiency of the estimators by measuring their runtimes when calculating \probustwsigma{0.1} for the CIFAR10 ResNet18 model for 50 points. Runtimes are displayed in Table~\ref{table:runtimes}. They indicate that \ptaylor{} and \pmmse{} perform 35x and 17x faster than \pmc{}, respectively. Additional runtimes are in Appendix~\ref{app:experiments}.

\subsection{Softmax probability is not a good proxy for average-case robustness}

To examine the relationship between \probust{} and \psoftmax{}, we calculate \pmmse{} and \psoftmax{} for CIFAR10 and CIFAR100 models of varying levels of robustness, and measure the correlation of their values and ranks using Pearson and Spearman correlations. Results are in Figure~\ref{fig4:probust-and-psoftmax}. For a non-robust model, \probust{} and \psoftmax{} are not strongly correlated (Figure~\ref{fig4a:ps-nonrob-model}). As model robustness increases, the two quantities become more correlated (Figures~\ref{fig4b:ps-rob-models-lineplot} and~\ref{fig4c:ps-rob-model}). However, even for robust models, the relationship between the two quantities is mild (Figure~\ref{fig4c:ps-rob-model}). That \probust{} and \psoftmax{} are not strongly correlated is consistent with the theory in Section~\ref{sec:methods}: in general settings, \psoftmax{} is not a good estimator for \probust{}.

% that the two probabilities are conceptually different: \probust{} measures the uncertainty of a model’s prediction with respect to input noise (i.e., the probability that the prediction will change upon adding noise to the input) while \psoftmax{} is an uncalibrated uncertainty of the model's prediction being correct with respect to a calibration set. \textcolor{red}{[check interp of raw \psoftmax]}.

\subsection{Applications of average-case robustness}

\textbf{Detecting robustness bias among classes: Is the model differently robust for different classes?} We demonstrate that \probust{} can detect bias in local robustness by examining its distribution for each class for each model and test set over different $\sigma$'s. Results for the CIFAR10 ResNet18 model are in plotted in Figure~\ref{fig5:robustness-bias}. The results show that different classes have different \probust{} distributions, i.e., the model is more locally robust for some classes (e.g., frog) than for others (e.g., airplane). The results also show that \pmc{} and \pmmse{} have very similar distributions, further indicating that the latter well-approximates the former. Thus, \probust{} can be applied to detect robustness bias among classes, which is critical when models are deployed in high-stakes, real-world settings.

\textbf{Identifying non-robust dataset samples.} While robustness is typically viewed as the property of a model, the average-case robustness perspective compels us to view robustness as a joint property of both the model and the data point. In light of this, we can ask, given the same model, which samples are robustly and non-robustly classified? We evaluate whether \probust{} can distinguish such images better than \psoftmax{}. To this end, we train a simple CNN to distinguish between images with high and low \pmmse{} and the same CNN to also distinguish between images with high and low \psoftmax{} (additional setup details described in Appendix~\ref{app:experiments}). Then, we compare the performance of the two models. For CIFAR10, the test set accuracy for the \pmmse{} CNN is $\mathbf{92\%}$ while that for the \psoftmax{} CNN is $\textbf{58\%}$. These results indicate that \probust{} better identifies images that are robust to and vulnerable to random noise than \psoftmax{}.

We also present visualizations of images with the highest and lowest \pmmse{} in each class for each model. For comparison, we do the same with \psoftmax{}. Example CIFAR10 images are shown in the Appendix~\ref{app:experiments}. We observe that images with low \probust{} tend to have neutral colors, with the object being a similar color as the background (making the prediction likely to change when the image is slightly perturbed), while images with high \probust{} tend to be brightly-colored, with the object strongly contrasting with the background (making the prediction likely to stay constant when the image is slightly perturbed). These differences are not as evident for images with the highest and lowest \psoftmax{}. Overall, these results showcase the potential of local robustness in identifying non-robust dataset samples more reliably than softmax probabilities.