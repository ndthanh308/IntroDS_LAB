{
  "title": "Characterizing Data Point Vulnerability via Average-Case Robustness",
  "authors": [
    "Tessa Han",
    "Suraj Srinivas",
    "Himabindu Lakkaraju"
  ],
  "submission_date": "2023-07-26T01:10:29+00:00",
  "revised_dates": [
    "2023-09-19T00:05:45+00:00",
    "2023-10-25T00:12:49+00:00",
    "2023-10-26T00:44:50+00:00",
    "2024-05-31T00:58:42+00:00",
    "2024-07-09T01:33:24+00:00"
  ],
  "abstract": "Studying the robustness of machine learning models is important to ensure consistent model behaviour across real-world settings. To this end, adversarial robustness is a standard framework, which views robustness of predictions through a binary lens: either a worst-case adversarial misclassification exists in the local region around an input, or it does not. However, this binary perspective does not account for the degrees of vulnerability, as data points with a larger number of misclassified examples in their neighborhoods are more vulnerable. In this work, we consider a complementary framework for robustness, called average-case robustness, which measures the fraction of points in a local region that provides consistent predictions. However, computing this quantity is hard, as standard Monte Carlo approaches are inefficient especially for high-dimensional inputs. In this work, we propose the first analytical estimators for average-case robustness for multi-class classifiers. We show empirically that our estimators are accurate and efficient for standard deep learning models and demonstrate their usefulness for identifying vulnerable data points, as well as quantifying robustness bias of models. Overall, our tools provide a complementary view to robustness, improving our ability to characterize model behaviour.",
  "categories": [
    "cs.LG"
  ],
  "primary_category": "cs.LG",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.13885",
  "pdf_url": null,
  "comment": "UAI 2024",
  "num_versions": null,
  "size_before_bytes": 169275754,
  "size_after_bytes": 836569
}