\begin{thebibliography}{10}

\bibitem{jacot2018neural}
A.~Jacot, F.~Gabriel, and C.~Hongler, ``Neural tangent kernel: Convergence and
  generalization in neural networks,'' {\em Advances in neural information
  processing systems}, vol.~31, 2018.

\bibitem{bottou2012stochastic}
L.~Bottou, ``Stochastic gradient descent tricks,'' {\em Neural Networks: Tricks
  of the Trade: Second Edition}, pp.~421--436, 2012.

\bibitem{tieleman2017divide}
T.~Tieleman and G.~Hinton, ``Divide the gradient by a running average of its
  recent magnitude. coursera: Neural networks for machine learning,'' {\em
  Technical report}, 2017.

\bibitem{kingma2014method}
D.~P. Kingma, ``A method for stochastic optimization,'' {\em ArXiv Prepr},
  2014.

\bibitem{amari2000methods}
S.-i. Amari and H.~Nagaoka, {\em Methods of information geometry}, vol.~191.
\newblock American Mathematical Soc., 2000.

\bibitem{tancik2020fourier}
M.~Tancik, P.~Srinivasan, B.~Mildenhall, S.~Fridovich-Keil, N.~Raghavan,
  U.~Singhal, R.~Ramamoorthi, J.~Barron, and R.~Ng, ``Fourier features let
  networks learn high frequency functions in low dimensional domains,'' {\em
  Advances in Neural Information Processing Systems}, vol.~33, pp.~7537--7547,
  2020.

\bibitem{bordelon2020spectrum}
B.~Bordelon, A.~Canatar, and C.~Pehlevan, ``Spectrum dependent learning curves
  in kernel regression and wide neural networks,'' in {\em International
  Conference on Machine Learning}, pp.~1024--1034, PMLR, 2020.

\bibitem{huang2020self}
Z.~Huang, H.~Wang, E.~P. Xing, and D.~Huang, ``Self-challenging improves
  cross-domain generalization,'' in {\em Computer Vision--ECCV 2020: 16th
  European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part II
  16}, pp.~124--140, Springer, 2020.

\bibitem{zhang2019your}
L.~Zhang, J.~Song, A.~Gao, J.~Chen, C.~Bao, and K.~Ma, ``Be your own teacher:
  Improve the performance of convolutional neural networks via self
  distillation,'' in {\em Proceedings of the IEEE/CVF International Conference
  on Computer Vision}, pp.~3713--3722, 2019.

\bibitem{mobahi2020self}
H.~Mobahi, M.~Farajtabar, and P.~Bartlett, ``Self-distillation amplifies
  regularization in hilbert space,'' {\em Advances in Neural Information
  Processing Systems}, vol.~33, pp.~3351--3361, 2020.

\bibitem{keskar2016large}
N.~S. Keskar, D.~Mudigere, J.~Nocedal, M.~Smelyanskiy, and P.~T.~P. Tang, ``On
  large-batch training for deep learning: Generalization gap and sharp
  minima,'' {\em arXiv preprint arXiv:1609.04836}, 2016.

\bibitem{canatar2021spectral}
A.~Canatar, B.~Bordelon, and C.~Pehlevan, ``Spectral bias and task-model
  alignment explain generalization in kernel regression and infinitely wide
  neural networks,'' {\em Nature communications}, vol.~12, no.~1, p.~2914,
  2021.

\bibitem{bartlett2002rademacher}
P.~L. Bartlett and S.~Mendelson, ``Rademacher and gaussian complexities: Risk
  bounds and structural results,'' {\em Journal of Machine Learning Research},
  vol.~3, no.~Nov, pp.~463--482, 2002.

\bibitem{jacot2020kernel}
A.~Jacot, B.~Simsek, F.~Spadaro, C.~Hongler, and F.~Gabriel, ``Kernel alignment
  risk estimator: Risk prediction from training data,'' {\em Advances in Neural
  Information Processing Systems}, vol.~33, pp.~15568--15578, 2020.

\bibitem{loureiro2021learning}
B.~Loureiro, C.~Gerbelot, H.~Cui, S.~Goldt, F.~Krzakala, M.~Mezard, and
  L.~Zdeborov{\'a}, ``Learning curves of generic features maps for realistic
  datasets with a teacher-student model,'' {\em Advances in Neural Information
  Processing Systems}, vol.~34, pp.~18137--18151, 2021.

\bibitem{shawe2005eigenspectrum}
J.~Shawe-Taylor, C.~K. Williams, N.~Cristianini, and J.~Kandola, ``On the
  eigenspectrum of the gram matrix and the generalization error of
  kernel-pca,'' {\em IEEE Transactions on Information Theory}, vol.~51, no.~7,
  pp.~2510--2522, 2005.

\bibitem{liu2020linearity}
C.~Liu, L.~Zhu, and M.~Belkin, ``On the linearity of large non-linear models:
  when and why the tangent kernel is constant,'' {\em Advances in Neural
  Information Processing Systems}, vol.~33, pp.~15954--15964, 2020.

\bibitem{lee2019wide}
J.~Lee, L.~Xiao, S.~Schoenholz, Y.~Bahri, R.~Novak, J.~Sohl-Dickstein, and
  J.~Pennington, ``Wide neural networks of any depth evolve as linear models
  under gradient descent,'' {\em Advances in neural information processing
  systems}, vol.~32, 2019.

\bibitem{arora2019exact}
S.~Arora, S.~S. Du, W.~Hu, Z.~Li, R.~R. Salakhutdinov, and R.~Wang, ``On exact
  computation with an infinitely wide neural net,'' {\em Advances in neural
  information processing systems}, vol.~32, 2019.

\bibitem{geifman2020similarity}
A.~Geifman, A.~Yadav, Y.~Kasten, M.~Galun, D.~Jacobs, and B.~Ronen, ``On the
  similarity between the laplace and neural tangent kernels,'' {\em Advances in
  Neural Information Processing Systems}, vol.~33, pp.~1451--1461, 2020.

\bibitem{ortiz2021can}
G.~Ortiz-Jim{\'e}nez, S.-M. Moosavi-Dezfooli, and P.~Frossard, ``What can
  linearized neural networks actually say about generalization?,'' {\em
  Advances in Neural Information Processing Systems}, vol.~34, pp.~8998--9010,
  2021.

\bibitem{safran2021effects}
I.~M. Safran, G.~Yehudai, and O.~Shamir, ``The effects of mild
  over-parameterization on the optimization landscape of shallow relu neural
  networks,'' in {\em Conference on Learning Theory}, pp.~3889--3934, PMLR,
  2021.

\bibitem{arora2018stronger}
S.~Arora, R.~Ge, B.~Neyshabur, and Y.~Zhang, ``Stronger generalization bounds
  for deep nets via a compression approach,'' in {\em International Conference
  on Machine Learning}, pp.~254--263, PMLR, 2018.

\bibitem{liu2020toward}
C.~Liu, L.~Zhu, and M.~Belkin, ``Toward a theory of optimization for
  over-parameterized systems of non-linear equations: the lessons of deep
  learning,'' {\em arXiv preprint arXiv:2003.00307}, 2020.

\bibitem{velikanov2021explicit}
M.~Velikanov and D.~Yarotsky, ``Explicit loss asymptotics in the gradient
  descent training of neural networks,'' {\em Advances in Neural Information
  Processing Systems}, vol.~34, pp.~2570--2582, 2021.

\bibitem{suzuki2018fast}
T.~Suzuki, ``Fast generalization error bound of deep learning from a kernel
  perspective,'' in {\em International Conference on Artificial Intelligence
  and Statistics}, pp.~1397--1406, PMLR, 2018.

\bibitem{cao2019generalization}
Y.~Cao and Q.~Gu, ``Generalization bounds of stochastic gradient descent for
  wide and deep neural networks,'' {\em Advances in neural information
  processing systems}, vol.~32, 2019.

\bibitem{allen2019learning}
Z.~Allen-Zhu, Y.~Li, and Y.~Liang, ``Learning and generalization in
  overparameterized neural networks, going beyond two layers,'' {\em Advances
  in neural information processing systems}, vol.~32, 2019.

\bibitem{liu2022loss}
C.~Liu, L.~Zhu, and M.~Belkin, ``Loss landscapes and optimization in
  over-parameterized non-linear systems and neural networks,'' {\em Applied and
  Computational Harmonic Analysis}, vol.~59, pp.~85--116, 2022.

\bibitem{martens2020new}
J.~Martens, ``New insights and perspectives on the natural gradient method,''
  {\em The Journal of Machine Learning Research}, vol.~21, no.~1,
  pp.~5776--5851, 2020.

\bibitem{bernacchia2018exact}
A.~Bernacchia, M.~Lengyel, and G.~Hennequin, ``Exact natural gradient in deep
  linear networks and its application to the nonlinear case,'' {\em Advances in
  Neural Information Processing Systems}, vol.~31, 2018.

\bibitem{rudner2019natural}
T.~G. Rudner, F.~Wenzel, Y.~W. Teh, and Y.~Gal, ``The natural neural tangent
  kernel: Neural network training dynamics under natural gradient descent,'' in
  {\em 4th workshop on Bayesian Deep Learning (NeurIPS 2019)}, 2019.

\bibitem{karakida2020understanding}
R.~Karakida and K.~Osawa, ``Understanding approximate fisher information for
  fast convergence of natural gradient descent in wide neural networks,'' {\em
  Advances in neural information processing systems}, vol.~33,
  pp.~10891--10901, 2020.

\bibitem{martens2015optimizing}
J.~Martens and R.~Grosse, ``Optimizing neural networks with kronecker-factored
  approximate curvature,'' in {\em International conference on machine
  learning}, pp.~2408--2417, PMLR, 2015.

\bibitem{grosse2016kronecker}
R.~Grosse and J.~Martens, ``A kronecker-factored approximate fisher matrix for
  convolution layers,'' in {\em International Conference on Machine Learning},
  pp.~573--582, PMLR, 2016.

\bibitem{he2015delving}
K.~He, X.~Zhang, S.~Ren, and J.~Sun, ``Delving deep into rectifiers: Surpassing
  human-level performance on imagenet classification,'' in {\em Proceedings of
  the IEEE international conference on computer vision}, pp.~1026--1034, 2015.

\end{thebibliography}
