@article{Day2007,
author = {Day, Sarah and Lessard, Jean-Philippe and Mischaikow, Konstantin},
doi = {10.1137/050645968},
issn = {0036-1429},
journal = {SIAM Journal on Numerical Analysis},
month = {jan},
number = {4},
pages = {1398--1424},
title = {{Validated Continuation for Equilibria of PDEs}},
url = {http://epubs.siam.org/doi/10.1137/050645968},
volume = {45},
year = {2007}
}


@misc{RevelsLubinPapamarkou2016,
      title={Forward-Mode Automatic Differentiation in Julia}, 
      author={Jarrett Revels and Miles Lubin and Theodore Papamarkou},
      year={2016},
      eprint={1607.07892},
      archivePrefix={arXiv},
      primaryClass={cs.MS}
}

@software{david_p_sanders_2022_7257716,
  author       = {David P. Sanders and
                  Luis Benet and
                  Luca Ferranti and
                  Krish Agarwal and
                  Benoît Richard and
                  Josua Grawitter and
                  Eeshan Gupta and
                  Marcelo Forets and
                  Michael F. Herbst and
                  yashrajgupta and
                  Eric Hanson and
                  Braam van Dyk and
                  Christopher Rackauckas and
                  Rushabh Vasani and
                  Sebastian Micluța-Câmpeanu and
                  Sheehan Olver and
                  Twan Koolen and
                  Caroline Wormell and
                  Daniel Karrasch and
                  Favio André Vázquez and
                  Guillaume Dalle and
                  Jeffrey Sarnoff and
                  Julia TagBot and
                  Kevin O'Bryant and
                  Kristoffer Carlsson and
                  Morten Piibeleht and
                  Mosè Giordano and
                  Ryan and
                  Robin Deits and
                  Tim Holy},
  title        = {JuliaIntervals/IntervalArithmetic.jl: v0.20.8},
  month        = oct,
  year         = 2022,
  publisher    = {Zenodo},
  version      = {v0.20.8},
  doi          = {10.5281/zenodo.7257716},
  url          = {https://doi.org/10.5281/zenodo.7257716}
}


@TECHREPORT{Moore90,
        author = {Andrew William Moore},
        title = {Efficient Memory-based Learning for Robot Control},
        institution = {University of Cambridge},
        year = {1990}
    }

@book{Tucker2011,
author = {Tucker, Warwick},
doi = {10.1515/9781400838974},
isbn = {9781400838974},
month = {jul},
publisher = {Princeton University Press},
title = {{Validated Numerics}},
url = {https://www.degruyter.com/document/doi/10.1515/9781400838974/html},
year = {2011}
}
    
@InProceedings{symbolic,
  title = 	 {Discovering symbolic policies with deep reinforcement learning},
  author =       {Landajuela, Mikel and Petersen, Brenden K and Kim, Sookyung and Santiago, Claudio P and Glatt, Ruben and Mundhenk, Nathan and Pettit, Jacob F and Faissol, Daniel},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {5979--5989},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/landajuela21a/landajuela21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/landajuela21a.html},
  abstract = 	 {Deep reinforcement learning (DRL) has proven successful for many difficult control problems by learning policies represented by neural networks. However, the complexity of neural network-based policies{—}involving thousands of composed non-linear operators{—}can render them problematic to understand, trust, and deploy. In contrast, simple policies comprising short symbolic expressions can facilitate human understanding, while also being transparent and exhibiting predictable behavior. To this end, we propose deep symbolic policy, a novel approach to directly search the space of symbolic policies. We use an autoregressive recurrent neural network to generate control policies represented by tractable mathematical expressions, employing a risk-seeking policy gradient to maximize performance of the generated policies. To scale to environments with multi-dimensional action spaces, we propose an "anchoring" algorithm that distills pre-trained neural network-based policies into fully symbolic policies, one action dimension at a time. We also introduce two novel methods to improve exploration in DRL-based combinatorial optimization, building on ideas of entropy regularization and distribution initialization. Despite their dramatically reduced complexity, we demonstrate that discovered symbolic policies outperform seven state-of-the-art DRL algorithms in terms of average rank and average normalized episodic reward across eight benchmark environments.}
}

@article{interpretable,
title = {Interpretable policies for reinforcement learning by genetic programming},
journal = {Engineering Applications of Artificial Intelligence},
volume = {76},
pages = {158-169},
year = {2018},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2018.09.007},
url = {https://www.sciencedirect.com/science/article/pii/S0952197618301933},
author = {Daniel Hein and Steffen Udluft and Thomas A. Runkler},
keywords = {Interpretable, Reinforcement learning, Genetic programming, Model-based, Symbolic regression, Industrial benchmark},
abstract = {The search for interpretable reinforcement learning policies is of high academic and industrial interest. Especially for industrial systems, domain experts are more likely to deploy autonomously learned controllers if they are understandable and convenient to evaluate. Basic algebraic equations are supposed to meet these requirements, as long as they are restricted to an adequate complexity. Here we introduce the genetic programming for reinforcement learning (GPRL) approach based on model-based batch reinforcement learning and genetic programming, which autonomously learns policy equations from pre-existing default state–action trajectory samples. GPRL is compared to a straightforward method which utilizes genetic programming for symbolic regression, yielding policies imitating an existing well-performing, but non-interpretable policy. Experiments on three reinforcement learning benchmarks, i.e., mountain car, cart–pole balancing, and industrial benchmark, demonstrate the superiority of our GPRL approach compared to the symbolic regression method. GPRL is capable of producing well-performing interpretable reinforcement learning policies from pre-existing default trajectory data.}
}

@misc{gym,
  Author = {Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba},
  Title = {OpenAI Gym},
  Year = {2016},
  Eprint = {arXiv:1606.01540},
}

@ARTICLE{scipy,
  author  = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and
            Haberland, Matt and Reddy, Tyler and Cournapeau, David and
            Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and
            Bright, Jonathan and {van der Walt}, St{\'e}fan J. and
            Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and
            Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and
            Kern, Robert and Larson, Eric and Carey, C J and
            Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and
            {VanderPlas}, Jake and Laxalde, Denis and Perktold, Josef and
            Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and
            Harris, Charles R. and Archibald, Anne M. and
            Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and
            {van Mulbregt}, Paul and {SciPy 1.0 Contributors}},
  title   = {{{SciPy} 1.0: Fundamental Algorithms for Scientific
            Computing in Python}},
  journal = {Nature Methods},
  year    = {2020},
  volume  = {17},
  pages   = {261--272},
  adsurl  = {https://rdcu.be/b08Wh},
  doi     = {10.1038/s41592-019-0686-2},
}


@article {MR3392421,
    AUTHOR = {Gameiro, Marcio and Lessard, Jean-Philippe and Ricaud, Yann},
     TITLE = {Rigorous numerics for piecewise-smooth systems: a functional
              analytic approach based on {C}hebyshev series},
   JOURNAL = {J. Comput. Appl. Math.},
  FJOURNAL = {Journal of Computational and Applied Mathematics},
    VOLUME = {292},
      YEAR = {2016},
     PAGES = {654--673},
      ISSN = {0377-0427},
   MRCLASS = {65L60 (34A30 34A36 46B45 65P99)},
  MRNUMBER = {3392421},
MRREVIEWER = {S. Hitotumatu},
       DOI = {10.1016/j.cam.2015.05.016},
       URL = {https://doi-org.proxy3.library.mcgill.ca/10.1016/j.cam.2015.05.016},
}

@inproceedings{ddpg,
  added-at = {2019-07-12T20:04:55.000+0200},
  author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  biburl = {https://www.bibsonomy.org/bibtex/22708c349821330660afb992aec2be5d1/lanteunis},
  booktitle = {ICLR},
  editor = {Bengio, Yoshua and LeCun, Yann},
  ee = {http://arxiv.org/abs/1509.02971},
  interhash = {b791167abe535c8525f6a9bf62fcc1ab},
  intrahash = {2708c349821330660afb992aec2be5d1},
  keywords = {},
  timestamp = {2019-07-12T20:04:55.000+0200},
  title = {Continuous control with deep reinforcement learning.},
  url = {http://dblp.uni-trier.de/db/conf/iclr/iclr2016.html#LillicrapHPHETS15},
  year = 2016
}

@ARTICLE{td3,
       author = {{Fujimoto}, Scott and {van Hoof}, Herke and {Meger}, David},
        title = "{Addressing Function Approximation Error in Actor-Critic Methods}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
         year = 2018,
        month = feb,
          eid = {arXiv:1802.09477},
        pages = {arXiv:1802.09477},
archivePrefix = {arXiv},
       eprint = {1802.09477},
 primaryClass = {cs.AI},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018arXiv180209477F},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@book{solvingodes,
author = {Hairer, E. and N\o{}rsett, S. P. and Wanner, G.},
title = {Solving Ordinary Differential Equations I (2nd Revised. Ed.): Nonstiff Problems},
year = {1993},
isbn = {0387566708},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg}
}


@inproceedings{todorov2012mujoco,
  title={MuJoCo: A physics engine for model-based control},
  author={Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  pages={5026--5033},
  year={2012},
  organization={IEEE},
  doi={10.1109/IROS.2012.6386109}
}

@article{sb3,
  author  = {Antonin Raffin and Ashley Hill and Adam Gleave and Anssi Kanervisto and Maximilian Ernestus and Noah Dormann},
  title   = {Stable-Baselines3: Reliable Reinforcement Learning Implementations},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {268},
  pages   = {1-8},
  url     = {http://jmlr.org/papers/v22/20-1364.html}
}

@software{pysr,
  author       = {Miles Cranmer},
  title        = {PySR: Fast \& Parallelized Symbolic Regression in Python/Julia},
  month        = sep,
  year         = 2020,
  publisher    = {Zenodo},
  doi          = {10.5281/zenodo.4041459},
  url          = {http://doi.org/10.5281/zenodo.4041459}
}

@article{cranmer2020discovering,
      title={Discovering Symbolic Models from Deep Learning with Inductive Biases}, 
      author={Miles Cranmer and Alvaro Sanchez-Gonzalez and Peter Battaglia and Rui Xu and Kyle Cranmer and David Spergel and Shirley Ho},
      journal={NeurIPS 2020},
      year={2020},
      eprint={2006.11287},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{julia,
  title={Julia: A fresh approach to numerical computing},
  author={Bezanson, Jeff and Edelman, Alan and Karpinski, Stefan and Shah, Viral B},
  journal={SIAM review},
  volume={59},
  number={1},
  pages={65--98},
  year={2017},
  publisher={SIAM},
  url={https://doi.org/10.1137/141000671}
}

@article{hein,
title = {Interpretable policies for reinforcement learning by genetic programming},
journal = {Engineering Applications of Artificial Intelligence},
volume = {76},
pages = {158-169},
year = {2018},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2018.09.007},
url = {https://www.sciencedirect.com/science/article/pii/S0952197618301933},
author = {Daniel Hein and Steffen Udluft and Thomas A. Runkler},
keywords = {Interpretable, Reinforcement learning, Genetic programming, Model-based, Symbolic regression, Industrial benchmark},
abstract = {The search for interpretable reinforcement learning policies is of high academic and industrial interest. Especially for industrial systems, domain experts are more likely to deploy autonomously learned controllers if they are understandable and convenient to evaluate. Basic algebraic equations are supposed to meet these requirements, as long as they are restricted to an adequate complexity. Here we introduce the genetic programming for reinforcement learning (GPRL) approach based on model-based batch reinforcement learning and genetic programming, which autonomously learns policy equations from pre-existing default state–action trajectory samples. GPRL is compared to a straightforward method which utilizes genetic programming for symbolic regression, yielding policies imitating an existing well-performing, but non-interpretable policy. Experiments on three reinforcement learning benchmarks, i.e., mountain car, cart–pole balancing, and industrial benchmark, demonstrate the superiority of our GPRL approach compared to the symbolic regression method. GPRL is capable of producing well-performing interpretable reinforcement learning policies from pre-existing default trajectory data.}
}

@article{kubalik,
title = {Optimal Control via Reinforcement Learning with Symbolic Policy Approximation},
journal = {IFAC-PapersOnLine},
volume = {50},
number = {1},
pages = {4162-4167},
year = {2017},
note = {20th IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2017.08.805},
url = {https://www.sciencedirect.com/science/article/pii/S2405896317312594},
author = {Jiří Kubalík and Eduard Alibekov and Robert Babuška},
keywords = {reinforcement learning, value iteration, symbolic regression, genetic programming, nonlinear model-based control, optimal control},
abstract = {Model-based reinforcement learning (RL) algorithms can be used to derive optimal control laws for nonlinear dynamic systems. With continuous-valued state and input variables, RL algorithms have to rely on function approximators to represent the value function and policy mappings. This paper addresses the problem of finding a smooth policy based on the value function represented by means of a basis-function approximator. We first show that policies derived directly from the value function or represented explicitly by the same type of approximator lead to inferior control performance, manifested by non-smooth control signals and steady-state errors. We then propose a novel method to construct a smooth policy represented by an analytic equation, obtained by means of symbolic regression. The proposed method is illustrated on a reference-tracking problem of a 1-DOF robot arm operating under the influence of gravity. The results show that the analytic control law performs at least equally well as the original numerically approximated policy, while it leads to much smoother control signals. In addition, the analytic function is readable (as opposed to black-box approximators) and can be used in further analysis and synthesis of the closed loop.}
}

@ARTICLE{cmaes,

  author={Hansen, Nikolaus and Müller, Sibylle D. and Koumoutsakos, Petros},

  journal={Evolutionary Computation}, 

  title={Reducing the Time Complexity of the Derandomized Evolution Strategy with Covariance Matrix Adaptation (CMA-ES)}, 

  year={2003},

  volume={11},

  number={1},

  pages={1-18},

  doi={10.1162/106365603321828970}}
  
@misc{pycma,
  author       = {Nikolaus Hansen and Youhei Akimoto and Petr Baudis},
  title        = {{CMA-ES/pycma} on {G}ithub},
  howpublished = {Zenodo, DOI:10.5281/zenodo.2559634},
  month        = feb,
  year         = 2019,
  doi          = {10.5281/zenodo.2559634},
  url          = {https://doi.org/10.5281/zenodo.2559634},
}

@ARTICLE{dmcontrol,
       author = {{Tassa}, Yuval and {Doron}, Yotam and {Muldal}, Alistair and {Erez}, Tom and {Li}, Yazhe and {de Las Casas}, Diego and {Budden}, David and {Abdolmaleki}, Abbas and {Merel}, Josh and {Lefrancq}, Andrew and {Lillicrap}, Timothy and {Riedmiller}, Martin},
        title = "{DeepMind Control Suite}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Artificial Intelligence},
         year = 2018,
        month = jan,
          eid = {arXiv:1801.00690},
        pages = {arXiv:1801.00690},
archivePrefix = {arXiv},
       eprint = {1801.00690},
 primaryClass = {cs.AI},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018arXiv180100690T},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
 

@inproceedings{pilco,
  title={Improving {PILCO} with {B}ayesian neural network dynamics models},
  author={Gal, Yarin and McAllister, Rowan and Rasmussen, Carl Edward},
  booktitle={Data-Efficient Machine Learning workshop, International Conference on Machine Learning},
  year={2016}
}

@article{hacartpole,
  title   = "Evolving Stable Strategies",
  author  = "Ha, David",
  journal = "blog.otoro.net",
  year    = "2017",
  url     = "http://blog.otoro.net/2017/11/12/evolving-stable-strategies/"
}

@misc{cartpolecode,
title = {Cartpole Swing-up Implementation},
howpublished = {\url{https://github.com/0xangelo/gym-cartpole-swingup}},
note = {Accessed: 2023-01-12}
}

@misc{code,
title={Code repository},
howpublished={\url{https://github.com/MIMUW-RL/worrisome-nn}},
note = {Accessed: 2023-07-27}
}

@incollection{pytorch,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@book{sutton,
  added-at = {2019-07-13T10:11:53.000+0200},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  biburl = {https://www.bibsonomy.org/bibtex/2f46601cf8b13d39d1378af0d79438b12/lanteunis},
  edition = {Second},
  interhash = {ac6b144aaec1819919a2fba9f705c852},
  intrahash = {f46601cf8b13d39d1378af0d79438b12},
  keywords = {},
  publisher = {The MIT Press},
  timestamp = {2019-07-13T10:11:53.000+0200},
  title = {Reinforcement Learning: An Introduction},
  url = {http://incompleteideas.net/book/the-book-2nd.html},
  year = {2018 }
}

@article{sacc,
  title={Soft actor-critic algorithms and applications},
  author={Haarnoja, Tuomas and Zhou, Aurick and Hartikainen, Kristian and Tucker, George and Ha, Sehoon and Tan, Jie and Kumar, Vikash and Zhu, Henry and Gupta, Abhishek and Abbeel, Pieter and others},
  journal={arXiv preprint arXiv:1812.05905},
  year={2018}
}


@InProceedings{verma,
  title = 	 {Programmatically Interpretable Reinforcement Learning},
  author =       {Verma, Abhinav and Murali, Vijayaraghavan and Singh, Rishabh and Kohli, Pushmeet and Chaudhuri, Swarat},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {5045--5054},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/verma18a/verma18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/verma18a.html},
  abstract = 	 {We present a reinforcement learning framework, called Programmatically Interpretable Reinforcement Learning (PIRL), that is designed to generate interpretable and verifiable agent policies. Unlike the popular Deep Reinforcement Learning (DRL) paradigm, which represents policies by neural networks, PIRL represents policies using a high-level, domain-specific programming language. Such programmatic policies have the benefits of being more easily interpreted than neural networks, and being amenable to verification by symbolic methods. We propose a new method, called Neurally Directed Program Search (NDPS), for solving the challenging nonsmooth optimization problem of finding a programmatic policy with maximal reward. NDPS works by first learning a neural policy network using DRL, and then performing a local search over programmatic policies that seeks to minimize a distance from this neural “oracle”. We evaluate NDPS on the task of learning to drive a simulated car in the TORCS car-racing environment. We demonstrate that NDPS is able to discover human-readable policies that pass some significant performance bars. We also show that PIRL policies can have smoother trajectories, and can be more easily transferred to environments not encountered during training, than corresponding policies discovered by DRL.}
}

@ARTICLE{xaireview,
  
AUTHOR={Wells, Lindsay and Bednarz, Tomasz},   
	 
TITLE={Explainable AI and Reinforcement Learning—A Systematic Review of Current Approaches and Trends},      
	
JOURNAL={Frontiers in Artificial Intelligence},      
	
VOLUME={4},           
	
YEAR={2021},      
	  
URL={https://www.frontiersin.org/articles/10.3389/frai.2021.550030},       
	
DOI={10.3389/frai.2021.550030},      
	
ISSN={2624-8212},   
   
ABSTRACT={Research into Explainable Artificial Intelligence (XAI) has been increasing in recent years as a response to the need for increased transparency and trust in AI. This is particularly important as AI is used in sensitive domains with societal, ethical, and safety implications. Work in XAI has primarily focused on Machine Learning (ML) for classification, decision, or action, with detailed systematic reviews already undertaken. This review looks to explore current approaches and limitations for XAI in the area of Reinforcement Learning (RL). From 520 search results, 25 studies (including 5 snowball sampled) are reviewed, highlighting visualization, query-based explanations, policy summarization, human-in-the-loop collaboration, and verification as trends in this area. Limitations in the studies are presented, particularly a lack of user studies, and the prevalence of toy-examples and difficulties providing understandable explanations. Areas for future study are identified, including immersive visualization, and symbolic representation.}
}

@inproceedings{synthesizeprograms,
 author = {Trivedi, Dweep and Zhang, Jesse and Sun, Shao-Hua and Lim, Joseph J},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {25146--25163},
 publisher = {Curran Associates, Inc.},
 title = {Learning to Synthesize Programs as Interpretable and Generalizable Policies},
 url = {https://proceedings.neurips.cc/paper/2021/file/d37124c4c79f357cb02c655671a432fa-Paper.pdf},
 volume = {34},
 year = {2021}
}

@inproceedings{towardinterpretable,
  title={Toward Interpretable Deep Reinforcement Learning with Linear Model U-Trees},
  author={Guiliang Liu and Oliver Schulte and Wang Zhu and Qingcan Li},
  booktitle={ECML/PKDD},
  year={2018}
}


@InProceedings{robustadversarial,
  title = 	 {Robust Adversarial Reinforcement Learning},
  author =       {Lerrel Pinto and James Davidson and Rahul Sukthankar and Abhinav Gupta},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {2817--2826},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/pinto17a/pinto17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/pinto17a.html},
  abstract = 	 {Deep neural networks coupled with fast simulation and improved computational speeds have led to recent successes in the field of reinforcement learning (RL). However, most current RL-based approaches fail to generalize since: (a) the gap between simulation and real world is so large that policy-learning approaches fail to transfer; (b) even if policy learning is done in real world, the data scarcity leads to failed generalization from training to test scenarios (e.g., due to different friction or object masses). Inspired from H-infinity control methods, we note that both modeling errors and differences in training and test scenarios can just be viewed as extra forces/disturbances in the system. This paper proposes the idea of robust adversarial reinforcement learning (RARL), where we train an agent to operate in the presence of a destabilizing adversary that applies disturbance forces to the system. The jointly trained adversary is reinforced – that is, it learns an optimal destabilization policy. We formulate the policy learning as a zero-sum, minimax objective function. Extensive experiments in multiple environments (InvertedPendulum, HalfCheetah, Swimmer, Hopper, Walker2d and Ant) conclusively demonstrate that our method (a) improves training stability; (b) is robust to differences in training/test conditions; and c) outperform the baseline even in the absence of the adversary.}
}

@inproceedings{
evaluatingrobustness,
title={Toward Evaluating Robustness of Deep Reinforcement Learning with Continuous Control},
author={Tsui-Wei Weng and Krishnamurthy (Dj) Dvijotham* and Jonathan Uesato* and Kai Xiao* and Sven Gowal* and Robert Stanforth* and Pushmeet Kohli},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SylL0krYPS}
}

@misc{computervalidation,
  doi = {10.48550/ARXIV.2202.05073},
  
  url = {https://arxiv.org/abs/2202.05073},
  
  author = {Kuehn, Christian and Queirolo, Elena},
  
  keywords = {Dynamical Systems (math.DS), Numerical Analysis (math.NA), FOS: Mathematics, FOS: Mathematics, 65P99},
  
  title = {Computer Validation of Neural Network Dynamics: A First Case Study},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International}
}


@InProceedings{spuriouslocal,
  title = 	 {Spurious Local Minima are Common in Two-Layer {R}e{LU} Neural Networks},
  author =       {Safran, Itay and Shamir, Ohad},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {4433--4441},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/safran18a/safran18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/safran18a.html},
  abstract = 	 {We consider the optimization problem associated with training simple ReLU neural networks of the form $\mathbf{x}\mapsto \sum_{i=1}^{k}\max\{0,\mathbf{w}_i^\top \mathbf{x}\}$ with respect to the squared loss. We provide a computer-assisted proof that even if the input distribution is standard Gaussian, even if the dimension is arbitrarily large, and even if the target values are generated by such a network, with orthonormal parameter vectors, the problem can still have spurious local minima once $6\le k\le 20$. By a concentration of measure argument, this implies that in high input dimensions, <em>nearly all</em> target networks of the relevant sizes lead to spurious local minima. Moreover, we conduct experiments which show that the probability of hitting such local minima is quite high, and increasing with the network size. On the positive side, mild over-parameterization appears to drastically reduce such local minima, indicating that an over-parameterization assumption is necessary to get a positive result in this setting.}
}

@article{adversarialexamples,
  added-at = {2017-05-07T15:43:59.000+0200},
  author = {Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
  biburl = {https://www.bibsonomy.org/bibtex/23abd27c65ba96a3af0cdc1e4c8be7452/joachimagne},
  interhash = {cfd4056e0a17212d539d2ef112c60daf},
  intrahash = {3abd27c65ba96a3af0cdc1e4c8be7452},
  journal = {arXiv preprint arXiv:1412.6572},
  keywords = {thema:adversarial},
  timestamp = {2017-05-09T08:57:14.000+0200},
  title = {Explaining and harnessing adversarial examples},
  year = 2014
}

@inproceedings{
noderobust,
title={On Robustness of Neural Ordinary Differential Equations},
author={Hanshu YAN and Jiawei DU and Vincent TAN and Jiashi FENG},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=B1e9Y2NYvS}
}


@book {MR0231516,
    AUTHOR = {Moore, Ramon E.},
     TITLE = {Interval analysis},
 PUBLISHER = {Prentice-Hall, Inc., Englewood Cliffs, N.J.},
      YEAR = {1966},
     PAGES = {xi+145},
   MRCLASS = {65.10 (68.00)},
  MRNUMBER = {0231516},
}

@InProceedings{reluplex,
author="Katz, Guy
and Barrett, Clark
and Dill, David L.
and Julian, Kyle
and Kochenderfer, Mykel J.",
editor="Majumdar, Rupak
and Kun{\v{c}}ak, Viktor",
title="Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks",
booktitle="Computer Aided Verification",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="97--117",
abstract="Deep neural networks have emerged as a widely used and effective means for tackling complex, real-world problems. However, a major obstacle in applying them to safety-critical systems is the great difficulty in providing formal guarantees about their behavior. We present a novel, scalable, and efficient technique for verifying properties of deep neural networks (or providing counter-examples). The technique is based on the simplex method, extended to handle the non-convex Rectified Linear Unit (ReLU) activation function, which is a crucial ingredient in many modern neural networks. The verification procedure tackles neural networks as a whole, without making any simplifying assumptions. We evaluated our technique on a prototype deep neural network implementation of the next-generation airborne collision avoidance system for unmanned aircraft (ACAS Xu). Results show that our technique can successfully prove properties of networks that are an order of magnitude larger than the largest networks verified using existing methods.",
isbn="978-3-319-63387-9"
}

@inproceedings{fals1,
author = {Waga, Masaki},
title = {Falsification of Cyber-Physical Systems with Robustness-Guided Black-Box Checking},
year = {2020},
isbn = {9781450370189},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365365.3382193},
doi = {10.1145/3365365.3382193},
abstract = {For exhaustive formal verification, industrial-scale cyber-physical systems (CPSs) are often too large and complex, and lightweight alternatives (e.g., monitoring and testing) have attracted the attention of both industrial practitioners and academic researchers. Falsification is one popular testing method of CPSs utilizing stochastic optimization. In state-of-the-art falsification methods, the result of the previous falsification trials is discarded, and we always try to falsify without any prior knowledge. To concisely memorize such prior information on the CPS model and exploit it, we employ Black-box checking (BBC), which is a combination of automata learning and model checking. Moreover, we enhance BBC using the robust semantics of STL formulas, which is the essential gadget in falsification. Our experiment results suggest that our robustness-guided BBC outperforms a state-of-the-art falsification tool.},
booktitle = {Proceedings of the 23rd International Conference on Hybrid Systems: Computation and Control},
articleno = {11},
numpages = {13},
keywords = {black-box checking, cyber-physical systems, falsification, model checking, robust semantics, signal temporal logic, automata learning},
location = {Sydney, New South Wales, Australia},
series = {HSCC '20}
}

@ARTICLE{fals2,
  author={Yamagata, Yoriyuki and Liu, Shuang and Akazaki, Takumi and Duan, Yihai and Hao, Jianye},
  journal={IEEE Transactions on Software Engineering}, 
  title={Falsification of Cyber-Physical Systems Using Deep Reinforcement Learning}, 
  year={2021},
  volume={47},
  number={12},
  pages={2823-2840},
  doi={10.1109/TSE.2020.2969178}}

@article{fals3,
author = {Abbas, Houssam and Fainekos, Georgios and Sankaranarayanan, Sriram and Ivan\v{c}i\'{c}, Franjo and Gupta, Aarti},
title = {Probabilistic Temporal Logic Falsification of Cyber-Physical Systems},
year = {2013},
issue_date = {May 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {2s},
issn = {1539-9087},
url = {https://doi.org/10.1145/2465787.2465797},
doi = {10.1145/2465787.2465797},
abstract = {We present a Monte-Carlo optimization technique for finding system behaviors that falsify a metric temporal logic (MTL) property. Our approach performs a random walk over the space of system inputs guided by a robustness metric defined by the MTL property. Robustness is guiding the search for a falsifying behavior by exploring trajectories with smaller robustness values. The resulting testing framework can be applied to a wide class of cyber-physical systems (CPS). We show through experiments on complex system models that using our framework can help automatically falsify properties with more consistency as compared to other means, such as uniform sampling.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = {may},
articleno = {95},
numpages = {30},
keywords = {testing, metric temporal logic, robustness, Hybrid systems}
}

@article{fals4,
author = {Dreossi, Tommaso and Donz\'{e}, Alexandre and Seshia, Sanjit A.},
title = {Compositional Falsification of Cyber-Physical Systems with Machine Learning Components},
year = {2019},
issue_date = {Dec 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {63},
number = {4},
issn = {0168-7433},
url = {https://doi.org/10.1007/s10817-018-09509-5},
doi = {10.1007/s10817-018-09509-5},
abstract = {Cyber-physical systems (CPS), such as automotive systems, are starting to include sophisticated machine learning (ML) components. Their correctness, therefore, depends on properties of the inner ML modules. While learning algorithms aim to generalize from examples, they are only as good as the examples provided, and recent efforts have shown that they can produce inconsistent output under small adversarial perturbations. This raises the question: can the output from learning components lead to a failure of the entire CPS? In this work, we address this question by formulating it as a problem of falsifying signal temporal logic specifications for CPS with ML components. We propose a compositional falsification framework where a temporal logic falsifier and a machine learning analyzer cooperate with the aim of finding falsifying executions of the considered model. The efficacy of the proposed technique is shown on an automatic emergency braking system model with a perception component based on deep neural networks.},
journal = {J. Autom. Reason.},
month = {dec},
pages = {1031–1053},
numpages = {23},
keywords = {Autonomous driving, Temporal logic, Machine learning, Deep learning, Cyber-physical systems, Falsification, Neural networks}
}

@InProceedings{smt1,
author="Ehlers, R{\"u}diger",
editor="D'Souza, Deepak
and Narayan Kumar, K.",
title="Formal Verification of Piece-Wise Linear Feed-Forward Neural Networks",
booktitle="Automated Technology for Verification and Analysis",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="269--286",
abstract="We present an approach for the verification of feed-forward neural networks in which all nodes have a piece-wise linear activation function. Such networks are often used in deep learning and have been shown to be hard to verify for modern satisfiability modulo theory (SMT) and integer linear programming (ILP) solvers.",
isbn="978-3-319-68167-2"
}

@inproceedings{smt2,
author = {Bunel, Rudy and Turkaslan, Ilker and Torr, Philip H.S. and Kohli, Pushmeet and Kumar, M. Pawan},
title = {A Unified View of Piecewise Linear Neural Network Verification},
year = {2018},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The success of Deep Learning and its potential use in many safety-critical applications has motivated research on formal verification of Neural Network (NN) models. Despite the reputation of learned NN models to behave as black boxes and the theoretical hardness of proving their properties, researchers have been successful in verifying some classes of models by exploiting their piecewise linear structure and taking insights from formal methods such as Satisifiability Modulo Theory. These methods are however still far from scaling to realistic neural networks. To facilitate progress on this crucial area, we make two key contributions. First, we present a unified framework that encompasses previous methods. This analysis results in the identification of new methods that combine the strengths of multiple existing approaches, accomplishing a speedup of two orders of magnitude compared to the previous state of the art. Second, we propose a new data set of benchmarks which includes a collection of previously released testcases. We use the benchmark to provide the first experimental comparison of existing algorithms and identify the factors impacting the hardness of verification problems.},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
pages = {4795–4804},
numpages = {10},
location = {Montr\'{e}al, Canada},
series = {NIPS'18}
}

@InProceedings{smt3,
author="Katz, Guy
and Barrett, Clark
and Dill, David L.
and Julian, Kyle
and Kochenderfer, Mykel J.",
editor="Majumdar, Rupak
and Kun{\v{c}}ak, Viktor",
title="Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks",
booktitle="Computer Aided Verification",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="97--117",
abstract="Deep neural networks have emerged as a widely used and effective means for tackling complex, real-world problems. However, a major obstacle in applying them to safety-critical systems is the great difficulty in providing formal guarantees about their behavior. We present a novel, scalable, and efficient technique for verifying properties of deep neural networks (or providing counter-examples). The technique is based on the simplex method, extended to handle the non-convex Rectified Linear Unit (ReLU) activation function, which is a crucial ingredient in many modern neural networks. The verification procedure tackles neural networks as a whole, without making any simplifying assumptions. We evaluated our technique on a prototype deep neural network implementation of the next-generation airborne collision avoidance system for unmanned aircraft (ACAS Xu). Results show that our technique can successfully prove properties of networks that are an order of magnitude larger than the largest networks verified using existing methods.",
isbn="978-3-319-63387-9"
}
