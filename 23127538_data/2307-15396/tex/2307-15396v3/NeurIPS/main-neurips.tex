\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023
 \PassOptionsToPackage{numbers}{natbib}

% ready for submission
\usepackage{neurips_2023}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{amsthm}
\usepackage{graphicx} % support the \includegraphics command and options
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{mathtools}
\usepackage{amsmath, amssymb, amsfonts, verbatim,bbm}
\usepackage{hyphenat,epsfig,subcaption,multirow}
\usepackage{nicefrac}
\usepackage{paralist}
\usepackage[utf8]{inputenc}
\usepackage[font=small, labelfont=bf]{caption}

%\usepackage[usenames,dvipsnames]{xcolor}
%\usepackage[ruled]{algorithm2e}
%\renewcommand{\algorithmcfname}{ALGORITHM}

\DeclareFontFamily{U}{mathx}{\hyphenchar\font45}
\DeclareFontShape{U}{mathx}{m}{n}{
      <5> <6> <7> <8> <9> <10>
      <10.95> <12> <14.4> <17.28> <20.74> <24.88>
      mathx10
      }{}
\DeclareSymbolFont{mathx}{U}{mathx}{m}{n}
\DeclareMathSymbol{\bigtimes}{1}{mathx}{"91}
\usepackage{tcolorbox}
\tcbuselibrary{skins,breakable}
\tcbset{enhanced jigsaw}

\usepackage[normalem]{ulem}
\usepackage[compact]{titlesec}

\definecolor{DarkRed}{rgb}{0.1,0.1,0.8}
\definecolor{DarkBlue}{rgb}{0.1,0.1,0.5}


\usepackage{nameref}
\definecolor{ForestGreen}{rgb}{0.1333,0.5451,0.1333}
%\definecolor{DarkRed}{rgb}{0.8,0,0}
\definecolor{Red}{rgb}{0.9,0,0}
%\usepackage[linktocpage=true,
%	pagebackref=true,colorlinks,
%		urlcolor=black,
%	linkcolor=DarkRed, citecolor=ForestGreen,
%	bookmarks,bookmarksopen,bookmarksnumbered]
%	{hyperref}
\usepackage[noabbrev,nameinlink]{cleveref}
\crefname{property}{property}{Property}
\creflabelformat{property}{(#1)#2#3}
\crefname{equation}{eq}{Eq}
\creflabelformat{equation}{(#1)#2#3}

\usepackage{bm}
\usepackage{url}
\usepackage{xspace}
\usepackage[mathscr]{euscript}

\usepackage{tikz}
\usetikzlibrary{arrows}
\usetikzlibrary{arrows.meta}
\usetikzlibrary{shapes}
\usetikzlibrary{backgrounds}
\usetikzlibrary{positioning}
\usetikzlibrary{decorations.markings}
\usetikzlibrary{patterns}
\usetikzlibrary{calc}
\usetikzlibrary{fit}
\usetikzlibrary{decorations.pathmorphing}
\usepackage{epstopdf}
\epstopdfsetup{outdir=./}
\usepackage{mdframed}

\usepackage[noend]{algpseudocode}
\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother

\usepackage{cite}
\usepackage{enumitem}
%\usepackage[margin=1in]{geometry}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{invariant}{Invariant}
\newtheorem{proposition}[lemma]{Proposition}
\newtheorem{corollary}[lemma]{Corollary}
\newtheorem{claim}[lemma]{Claim}
\newtheorem{fact}[lemma]{Fact}
\newtheorem{conj}[lemma]{Conjecture}
\newtheorem{definition}[lemma]{Definition}
\newtheorem*{Definition}{Definition}
\newtheorem{problem}{Problem}

\newtheorem*{claim*}{Claim}
\newtheorem*{proposition*}{Proposition}
\newtheorem*{lemma*}{Lemma}
\newtheorem*{problem*}{Problem}

\crefname{lemma}{Lemma}{Lemmas}
\crefname{claim}{Claim}{Claims}


\newtheorem{mdresult}{Result}
\newenvironment{result}{\begin{mdframed}[backgroundcolor=lightgray!40,topline=false,rightline=false,leftline=false,bottomline=false,innertopmargin=2pt]\begin{mdresult}}{\end{mdresult}\end{mdframed}}

%\newtheorem{result}[mdresult]{Result}


%\theoremstyle{definition}
\newtheorem{remark}[lemma]{Remark}
\newtheorem{assumption}{A\hspace{-1mm}}
\newtheorem{observation}[lemma]{Observation}

\newtheoremstyle{restate}{}{}{\itshape}{}{\bfseries}{~(restated).}{.5em}{\thmnote{#3}}
\theoremstyle{restate}
\newtheorem*{restate}{}

\theoremstyle{definition}
\newtheorem{mdalg}{Algorithm}
\newenvironment{Algorithm}{\begin{tbox}\begin{mdalg}}{\end{mdalg}\end{tbox}}
\allowdisplaybreaks



\DeclareMathOperator*{\argmax}{arg\,max}

\renewcommand{\qed}{\nobreak \ifvmode \relax \else
      \ifdim\lastskip<1.5em \hskip-\lastskip
      \hskip1.5em plus0em minus0.5em \fi \nobreak
      \vrule height0.75em width0.5em depth0.25em\fi}

\newcommand{\Qed}[1]{\ensuremath{\qed_{\textnormal{~#1}}}}

\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}

\setlength{\parskip}{3pt}

\newcommand{\logstar}[1]{\ensuremath{\log^{*}\!{#1}}}
\newcommand{\logr}[1]{\ensuremath{\log^{(#1)}}}

\renewcommand{\eqref}[1]{Eq.~(\ref{#1})}

\newcommand{\note}[1]{\textcolor{red}{\textbf{#1}}}

\title{Noisy Interpolation Learning with \\ Shallow Univariate ReLU Networks}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  David S.~Hippocampus\thanks{Use footnote for providing further information
    about author (webpage, alternative address)---\emph{not} for acknowledging
    funding agencies.} \\
  Department of Computer Science\\
  Cranberry-Lemon University\\
  Pittsburgh, PA 15213 \\
  \texttt{hippo@cs.cranberry-lemon.edu} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}
\input{NeurIPS/macros}
\begin{document}

\maketitle


\begin{abstract}
  We study the asymptotic overfitting behavior of interpolation with minimum norm ($\ell_2$ of the weights) two-layer ReLU networks for noisy univariate regression.  We show that overfitting is tempered for the $L_1$ loss, and any $L_p$ loss for $p<2$, but catastrophic for $p\geq 2$.
\end{abstract}

\section{Introduction}

A recent realization is that although sometimes overfitting can be catastrophic, as suggested by our classic learning theory understanding, in other models overfitting, and even interpolation learning, i.e.~insisting on zero training error of noisy data, might not be so catastrophic, allowing for good generalization (low test error) and even consistency 
%\citep{zhang2017rethinkinggeneralization,misha}. 
\citep{zhang2017rethinkinggeneralization,belkin2018overfittingperfectfitting}.
This has led to efforts towards understanding the nature of overfitting: how benign or catastrophic it is, and what determines this behavior, in different settings and using different models. 
%\citep{ryan,peter,andrea,us,everyone}.  
%\citep{hastie2020surprises,belkin2020two,bartlett2020benignpnas,muthukumar2020harmless,negrea2020defense,chinot2020robustness,koehler2021uniform,wu2020optimal,tsigler2020benign,zhou2022non,wang2022tight,chatterji2021interplay,bartlett2021failures,shamir2022implicit,ghosh2022universal,chatterji2020linearnoise,wang2021binary,cao2021benign,muthukumar2021classification,montanari2020maxmarginasymptotics,liang2021interpolating,thrampoulidis2020theoretical,wang2021benignmulticlass,donhauser2022fastrates,frei2022benign,frei2023benign,cao2022benign,kou2023benign}.

%Much of this effort focused on linear and kernel models: e.g.~we now have a good understanding of how the spectral decay of the input covariance (or the kernel operator) controls whether overfitting is benign (e.g.~allows for asymptotic consistency) in minimum-$\ell_2$ norm linear and kernel regression and classification.  or catastrophic (yields useless or diverging predictors) or somewhere in between.   Recent works also showed bengign overfitting in classification with non-linear neural networks, but only 

Although interest in benign overfitting stems from the empirical success of interpolating large neural networks, theoretical study so far has been mostly limited to linear and kernel methods, or at least to settings where the data is linearly interpolatable, with very high (tending to infinity) data dimension\footnote{Minimum $\ell_2$ norm linear prediction (aka ridgeless regression) with noisy labels and (sub-)Gaussian features has been studied extensively 
\citep[e.g.][]{hastie2020surprises,belkin2020two,bartlett2020benignpnas,muthukumar2020harmless,negrea2020defense,chinot2020robustness,koehler2021uniform,wu2020optimal,tsigler2020benign,zhou2022non,wang2022tight,chatterji2021interplay,bartlett2021failures,shamir2022implicit,ghosh2022universal,chatterji2020linearnoise,wang2021binary,cao2021benign,muthukumar2021classification,montanari2020maxmarginasymptotics,liang2021interpolating,thrampoulidis2020theoretical,wang2021benignmulticlass,donhauser2022fastrates,frei2023benign}, and noisy minimum $\ell_1$ linear prediction (aka Basis Persuit) has also been considered \citep[e.g.][]{ju2020overfitting,koehler2021uniform,wang2022tight}.  Either way, these analyses are all in the high dimensional setting, with dimension going to infinity, since to allow for interpolation the dimension must be high, higher than the number of samples.
% including some structured departures from from purely Gaussian features \citep{rayan,andrea} 
%Minimum $\ell_1$ norm with Gaussian features has also been considered \citep{sarah,fanny,holger}. 
Kernel method amount to a minimum $\ell_2$ norm linear prediction, with very non-Gaussian features.  But existing analyses of interpolation learning with kernel methods rely on ``Gaussian Universality'': either assuming as an ansatz the behavior is as for Gaussian features \citep{mallinar2022benign} or establishing this rigorously in certain high dimensional scalings \citep{hastie2019surprises,misiakiewicz2022spectrum,mei2022generalization}.
In particular, such analyses are only valid when the input dimension goes to infinity (though possibly slower than the number of samples) and not for fixed low or moderate dimension.
\citet{frei2022benign,frei2023benign,cao2022benign,kou2023benign} study interpolation learning with neural networks, but only with high input dimension and when the data is interpolatable also with a linear predictor---in these cases, although non-linear neural networks are used, the results show they behave similarly to linear predictors. 
\citet{manoj2023interpolation} take the other extreme and study interpolation learning with ``short programs'', which are certainly non-linear, but this is an abstract model that does not directly capture learning with neural networks.}.  But what about noisy interpolation learning in low (or moderate) dimensions, using neural networks?

\citet{mallinar2022benign} conducted simulations with neural networks and observed  ``tempered'' overfitting:  the asymptotic risk does not approach the Bayes-optimal risk (there is no consistency), but neither does it diverge to infinity catastrophically.  Such ``tempered'' behavior is well understood for $1$-nearest neighbor, where the asymptotic risk is roughly twice the Bayes risk \citep{cover1967nearest}, and \citeauthor{mallinar2022benign} heuristically explain it also for some kernel methods. However, we do not have a satisfying nor rigorous understanding of such behavior in neural networks, nor a more quantitative or precise understanding of just how bad the risk might be when interpolating noisy data using a neural net.

In this paper, we begin rigorously studying the effect of overfitting with neural networks in low dimensions, where the data is {\em not} linearly interpolatable, but a large enough network can of course always interpolate.  Specifically, we study interpolation learning of univariate data (i.e.~in one dimension) using a two-layer ReLU network (with a skip connection), which is a predictor $f_{\theta,a_0,b_0}:\reals\rightarrow\reals$ given by:
\begin{equation}\label{eq:net}
f_{\theta,a_0,b_0}(x) = \sum_{j=1}^m a_j (w_j x + b_j)_+ +  a_0 x + b_0~,
\end{equation}
where $\theta \in \reals^{3m}$ denotes the weights (parameters) $\{a_j,w_j,b_j\}_{j=1}^m$.  To allow for interpolation we do not limit the width $m$, and learn by minimizing the norm of the weights \citep{savarese2019infinite,ergen2021convex,hanin2021ridgeless,debarre2022sparsest,boursier2023penalising}:
\begin{equation} \label{eq:main_problem}
    \hat{f}_S = \arg\min_{f_{\theta,a_0,b_0}} \norm{\theta}^2 \;\text{ s.t. } \; \forall i \in [n],\; f_{\theta,a_0,b_0}(x_i)=y_i~\; \textrm{where $S=\{(x_1,y_1),\ldots,(x_n,y_n)\}$}.
\end{equation}
% I removed the definition of the model class F.  I don't think we use it.
Following \citet{boursier2023penalising} we allow an unregularized skip-connection in \eqref{eq:net}, where the weights $a_0,b_0$ of this skip connection are not included in the norm $\norm{\theta}$ in \eqref{eq:main_problem}. This skip connection avoids some complications and allows better characterizing $\hat{f}_S$ 
%(see Sections~\ref{sec:review_min_norm_nets} and~\ref{sec:characterizing}), 
but should not meaningfully change the behavior (see Section~\ref{sec:review_min_norm_nets}).
%, as we demonstrate empirically in Section~\ref{sec:experiments}.  

\paragraph{Why min norm?} Using unbounded size minimum weight-norm networks is natural for interpolation learning.  It parallels the study of minimum norm high (even infinite) dimension linear predictors.  For interpolation, we must allow the number of parameters to increase as the sample size increases.  But to have any hope of generalization, we must choose among the infinitely many zero training error networks somehow, and it seems that some sort of explicit or implicit low norm bias is the driving force in learning with large overparametrized neural networks \citep{neyshabur2014search}.  Seeking minimum $\ell_2$ norm weights is natural, e.g.~as a result of small weight decay.  Even without explicit weight decay, optimizing using gradient descent is also related to an implicit bias toward low $\ell_2$ norm: this can be made precise for linear models and for classification with ReLU networks \citep{chizat2020implicit,safran2022effective}. For regression with ReLU networks, as we study here, gradient descent does not exactly minimize the $\ell_2$ norm, but the implicit bias is probably still related, and studying \eqref{eq:main_problem} is a good starting point also for understanding the behavior of networks learned via gradient descent.

\paragraph{Noisy interpolation learning.} We consider a noisy %setting, 
distribution $\dist$ over $[0,1] \times \reals$:
\begin{equation} \label{eq:define_D}
    x \sim \unif([0,1])\;\;\;\; \text{ and }\;\;\;\; y=f^*(x)+\eps \; \text{ with } \eps \textrm{ independent of }\, x,
\end{equation}
where $x$ is uniform for simplicity and concreteness, the noise $\eps$ follows some arbitrary (non-zero) distribution, and learning is based on an i.i.d.~training set $S\sim\dist^n$.  Since the noise is non-zero, the ``ground truth'' predictor $f^*$ has non-zero training error, seeking a training error much smaller than that of $f^*$ would be overfitting (fitting the noise) and necessarily cause the complexity (e.g.~norm) of the learned predictor to explode.  The ``right'' thing to do is to balance between the training error and the complexity $\norm{\theta}$.  Indeed, under mild assumptions, this balanced approach leads to asymptotic consistency, with $\hat{f}_S \xrightarrow{n\rightarrow\infty} f^* $ and the asymptotic population risk of $\hat{f}_S$ converging to the Bayes risk.  But what happens when we overfit and use the interpolating learning rule \eqref{eq:main_problem}?

 % Figure environment removed

\paragraph{Linear Splines.} At first glance, we might be tempted to think that two-layer ReLUs behave like linear splines (see Figure~\ref{fig:min-norm-illustration1}). Indeed, if minimizing the norm of weights $w_i$ and $a_i$ but {\em not} the biases $b_i$ in \eqref{eq:main_problem}, linear splines are always a valid minimizer \citep{savarese2019infinite,ergen2021convex}. As the number of noisy training points increases, linear splines ``zig-zag'' with tighter ``zigs'' but non-vanishing ``amplitude'' around $f^*$, resulting in an interpolator which roughly behaves like $f^*$ plus some added non-vanishing ``noise''.  This does not lead to consistency, but is similar to a nearest-neighbor predictor (each prediction is a weighted average of two neighbors).  Indeed, in Theorem~\ref{thm:linear-splines} of Section~\ref{sec:linear-spline}, we show that linear splines exhibit ``tempered'' behavior, with asymptotic risk proportional to the noise.

\paragraph{From Splines to Min-Norm ReLU Nets.} But it turns out minimum norm ReLU networks, although piecewise linear, are not quite linear splines: roughly speaking, and as shown in Figure~\ref{fig:min-norm-illustration1}, they are more conservative in the number of ``pieces''.
%(corresponding to the number of non-zero units).  
Because of this, in convex (conversely, concave) regions of the linear spline, minimum norm ReLU nets ``overshoot'' the linear spline in order to avoid breaking linear pieces.  This could create additional ``spikes'', extending above and below the data points (see Figures~\ref{fig:min-norm-illustration1} and~\ref{fig:min-norm-illustration2}) and thus potentially increasing the error. How bad is the effect of such spikes on the population risk?

\paragraph{Effect of Overfitting on $L_p$ Risk.} It turns out the answer is subtle and depends on how we measure the error.  Measuring population risk as $L_p(f) := \Exp_{(x,y) \sim \dist} [|f(x)-y|^p]$, we show in Theorems~\ref{thm:tempered-for-ell_1} and~\ref{thm:tempered-for-ell_1-lower_bound} of Section~\ref{sec:tempered-L1} that for $1 \leq p < 2$, 
\begin{equation}\label{eq:intro_main}
    L_p(\hat{f}_S)\xrightarrow{n\rightarrow\infty} \Theta \left( \frac{1}{(2-p)_+} \right) \, L_p(f^*).
\end{equation}
This is an upper bound for any Lipschitz target $f^*$ and any noise distribution, and it is matched by a lower bound for Gaussian noise.  That is, for abs-loss ($L_1$ risk), as well as any $L_p$ risk for $1\leq p<2$, overfitting is {\bf tempered}. But this tempered behavior explodes as $p \rightarrow 2$, and we show in Theorem~\ref{thm:ell2-catastrophic} of Section~\ref{sec:catastrophic} that for any $p\geq 2$, including for the square loss ($p=2$), as soon as there is some non-zero noise, $L_p(\hat{f}_S)\xrightarrow{n\rightarrow\infty} \infty$ and overfitting is {\bf catastrophic}.  

\paragraph{Convergence vs Expectation.} The behavior is even more subtle, in that even for $1\leq p<2$, although the risk $L_p(\hat{f}_S)$ converges in probability to a tempered behavior as in \eqref{eq:intro_main}, its {\em expectation} is infinite: $\Exp_S[L_p(\hat{f}_S)]=\infty$.  Note that in studying tempered overfitting, \citet{mallinar2022benign} focused on this expectation, and so would have categorized the behavior as ``catastrophic'' even for $p=1$, emphasizing the need for more careful consideration of the effect of overfitting.  We would of course not get such strange behavior with the traditional non-overfitting approach of balancing training error and norm, which yields risk converging almost surely, with finite expectation converging to the optimal risk and vanishing variances.

\paragraph{I.I.D.~Samples vs. Samples on a Grid.} The catastrophic effect of interpolation on the $L_p$ risk with $p \geq 2$ is a result of the effect of fluctuations in the spacing of the training points.  Large, catastrophic, spikes are formed by training points extremely close to their neighbors but with different labels (see Figures~\ref{fig:min-norm-illustration2} and~\ref{fig:bad-case-kink-main-text}). To help understand this, in Section~\ref{sec:grid} we study a ``fixed design'' variant of the problem, where the training inputs lie on a uniform grid, $x_i=i/n$, and responses follow the random model $y_i=f^*(x_i)+\epsilon_i$. In this case, interpolation is always tempered, with $L_p(\hat{f}_S)\xrightarrow{n\rightarrow\infty} \Theta(  L_p(f^*) )$ for all constant $p\geq 1$ (Theorem~\ref{thm:grid} of Section~\ref{sec:grid}).

%\gal{Say something about our experiments.}\nati{We mentioned it above.  I think that's fine.}

\paragraph{Summary and Contribution.} Our work is the first step toward studying noisy interpolation learning on non-linearly-interpolatable data in actual neural networks, where the capacity of the model comes from the size of the network and not the dimension of the data.  We study the univariate case.  Univariate regression with min-norm ReLU networks is a rich model in its own right \citep[e.g.][]{savarese2019infinite,ergen2021convex,hanin2021ridgeless,debarre2022sparsest,boursier2023penalising,williams2019gradient,mulayoff2021implicit,safran2022effective} and a starting point for further investigation.  We see that even for univariate data, the overfitting behavior is subtle and requires careful and detailed analysis, and especially for the universally studied squared loss, does not fit the ``tempered overfitting'' predictions of e.g.~\citet{mallinar2022benign}.

 % Figure environment removed

\vspace{0.2cm}
\section{Review: Min-Norm ReLU Networks} \label{sec:review_min_norm_nets}
\vspace{0.2cm}

Minimum-norm unbounded-width univariate two-layer ReLU networks have been extensively studied in recent years, starting with \citet{savarese2019infinite}, with the exact formulation \eqref{eq:main_problem} incorporating a skip connection due to \citet{boursier2023penalising}.   \citeauthor{boursier2023penalising}, following prior work, establish that a minimum of \eqref{eq:main_problem} exists, with a finite number of units, and that 
%for $n>1$ non-negative inputs (i.e.~$x_i\geq 0$, as in our case) 
it is also unique.  

The problem \eqref{eq:main_problem} is also equivalent to minimizing the ``representational cost'' 
\[
    R(f) = \int_{\IR} \sqrt{1+x^2}|f''(x)| dx 
\]
over all interpolators $f$, although we will not use this characterization in our analysis.  Compared to \citet{savarese2019infinite}, the weighting $\sqrt{1+x^2}$ is due to penalizing the biases $b_i$.  More significantly, the skip connection in \eqref{eq:net} avoids the ``fallback'' terms of $|f'(-\infty)+f'(+\infty)|$, which only kicks-in in extreme cases (very few points or an extreme slope).  This simplified analysis and presentation while rarely effecting the solution.  

\citeauthor{boursier2023penalising} provide the following characterization of the minimizer\footnote{
If the biases $b_i$ are {\em not} included in the norm $\norm{\theta}$ in \eqref{eq:main_problem}, and this norm is replaced with $\sum_i(a_i^2+w_i^2)$, the modified problem admits multiple non-unique minimizers, including a linear spline (with modified behavior past the extreme points) \citep{savarese2019infinite}.  This set of minimizers was charactarized by \citet{hanin2021ridgeless}.  Interestingly, the minimizer $\hat{f}_S$ of \eqref{eq:main_problem} (when the biases are included in the norm) is also a minimizer of the modified problem (without including the biases).  All our results apply also to the setting without penalizing the biases in the following sense: the upper bounds are valid for all minimizers, while some minimizer, namely $\hat{f}_S$ that we study, exhibits the lower bound behavior.} $\hat{f}_S$ of \eqref{eq:main_problem}, which we will rely on heavily:
\begin{lemma}[\citet{boursier2023penalising}]\label{lem:char}
    For $0 \leq x_1<x_2<\cdots<x_n$, the problem in \eqref{eq:main_problem} admits a unique minimizer of the form:
    \begin{equation} \label{eq:min-norm_interpolator}
        \hat{f}_S(x) = ax + b + \sum_{i=1}^{n-1} a_i (x - \tau_i)_+~,
    \end{equation}
    where $\tau_i \in [x_i,x_{i+1})$ for every $i \in [n-1]$. 
\end{lemma}
As in the above charactarization, it is very convenient to take the training points to be sorted.  Since the learned network $\hat{f}_S$ does not depend on the order of the points, we can always ``sort'' the points without changing anything.  And so, throughout the paper, we will always take the points to be sorted (formally, the results apply to i.i.d.~points, and the analysis is done after sorting these points).  

\removed{

\section{Introduction}

In recent years, there has been a growing interest in studying overfitting and interpolation learning with noisy training data. Following initial experiments by \citet{zhang2017rethinkinggeneralization}, researchers observed that overparameterized neural networks often achieve perfect fit to noisy training data and still generalize well to unseen data. This phenomenon does not align with the traditional belief from statistical learning theory that overfitting to noise leads to poor out-of-sample prediction performance, and understanding it has become a central question in the theory of deep learning.

This question motivated the study of \emph{benign overfitting}, where the learned predictor perfectly fits (i.e., interpolates) noisy training data, but still approaches \emph{Bayes-optimal} generalization when the training set is sufficiently large. Despite a great deal of theoretical research, the settings where benign overfitting is known to occur are limited (see the related work section for details). Indeed, the existing results focus mostly on linear predictors and kernel regression. Moreover, several works show benign overfitting in nonlinear neural networks, but only in classification settings where the data distribution is linearly separable and the input dimension is larger than the number of training samples.

\citet{mallinar2022benign} conducted experiments with neural networks on simple tasks and observed that they do not exhibit either benign or catastrophic overfitting. That is, in the limit where the training set size is large, the learned predictor does not approach the Bayes-optimal risk, but also does not diverge catastrophically. They termed this intermediate regime \emph{tempered overfitting}, and studied it theoretically for kernel regression.

The aforementioned experimental results,
%on tempered overfitting in neural networks, 
as well as the limitations of the existing theoretical results on benign overfitting suggest that understanding tempered overfitting is crucial for explaining interpolation learning in neural networks. 
Since all prior results on benign or tempered overfitting either consider linear (or kernel) models, or consider linearly-separable distributions with extremely high-dimensional inputs, a main challenge is to understand tempered overfitting in settings that require nonlinear predictors and where the input dimension is smaller than the number of samples. 

In this work, we make a first step in this direction, by analyzing tempered overfitting in two-layer univariate ReLU networks. We consider wide ReLU networks that interpolate noisy training data of the form $\{(x_i,y_i)\}_{i=1}^n \subseteq [0,1] \times \mathbb{R}$, such that $x_i$ is distributed uniformly on $[0,1]$ and $y_i = f^*(x_i) + \epsilon_i$ where $\epsilon_i$ is a 
%zero-mean 
Gaussian
random noise.
%, and $f^*$ is the Bayes-optimal predictor. 
We aim to understand the performance of the interpolator on test data from the same distribution at the limit where $n \to \infty$. Any finite training dataset can be interpolated by infinitely many ReLU networks, and the specific interpolator learned by gradient methods is determined either by implicit or explicit regularization. We focus on the interpolator whose parameters have a minimal norm, namely, on ridgeless regression. Such a min-norm interpolator corresponds to an optimal solution when training the network using an infinitesimally small explicit $L_2$ regularization (a.k.a. weight decay).
\footnote{We note that characterization of the \emph{implicit} regularization of gradient flow on ReLU networks with regression losses is currently unknown (see \citet{vardi2022implicit}).}

At first glance, we might be tempted to think that two-layer univariate ReLU networks behave like linear splines (see Figure~\ref{fig:splines-main_text}), and thus exhibit tempered overfitting, 
with the population loss not converging to its optimal value but also not off by more than a constant factor (we provide a detailed analysis of linear splines in Section~\ref{sec:linear-spline}).
%since linear-spline interpolators do not converge to the Bayes-optimal solution $f^*$ but also do not diverge from it by too much (see a detailed analysis of linear splines in Section~\ref{?}). 
Indeed, prior works considered the relationship between norm minimization in univariate ReLU networks and linear-spline interpolation, as we discuss in the related work section.
%\citep{savarese2019infinite,ergen2021convex}
However, although the min-norm interpolator is piecewise linear, it actually ``spikes'' beyond the interpolated points (see Figure~\ref{fig:bad-case-kink-main-text}). Such spikes could be arbitrarily high when the configuration of training points is particularly unfortunate, requiring care in analyzing the typical behavior of min-norm interpolators. It turns out that the qualitative effect of these spikes also depends on the loss function. 

Below we summarize our main contribution:
\begin{itemize}
    \item %For the $L_1$ loss and 
    For any $L_p$ loss with $p \in [1,2)$, we show that overfitting is tempered (see Section~\ref{sec:tempered-L1}). That is, the test performance of the min-norm interpolator is worse than the 
    %Bayes-optimal 
    predictor $f^*$ only by a constant factor. 

    \item For $L_p$ losses with $p \geq 2$ we show that overfitting is catastrophic (Section~\ref{sec:catastrophic}). Namely, the population loss diverges as the training set's size tends to infinity.

    \item When the training samples are equally spaced (e.g., in settings where the learner can actively choose the training points, and then observe noisy measurements at these points, and the query points are selected on an equally spaced ``grid''), we prove that overfitting is tempered w.r.t. $L_p$ for any $p \geq 1$ (Section~\ref{sec:grid}). Thus, the catastrophic behavior does not occur when the samples are equally spaced, as it stems from the random variation in the spacing between training points.
    
    \item We show that tempered behavior is delicate, as in some cases we can have a probability guarantee for tempered overfitting even if the expectation (over the training samples) of the population loss is unbounded (see Section~\ref{sec:tempered-L1}).
    That is, although we have tempered behavior with high probability, the expectation does not satisfy the definition of tempered overfitting given by \citet{mallinar2022benign}.
    %, which only considers the expectation. 

    \item Experiments: \gal{TODO}
\end{itemize}

The paper is structured as follows: Below we discuss some related work. We introduce our setting in Section~\ref{sec:prelim}. In Section~\ref{sec:linear-spline} we analyze tempered overfitting using the linear-spline interpolator. Our results on interpolation learning with random data are provided in Section~\ref{sec:main_results}, as well as an informal discussion on the proof ideas.
The results on interpolation learning with samples on the grid are given in Section~\ref{sec:grid}. All formal proofs are deferred to the appendix.

\subsection*{Related work}

\paragraph{Benign and tempered overfitting.}

The benign overfitting phenomenon has been extensively studied in recent years. Previous works analyzed the conditions in which benign overfitting occurs in linear regression \citep{hastie2020surprises,belkin2020two,bartlett2020benignpnas,muthukumar2020harmless,negrea2020defense,chinot2020robustness,koehler2021uniform,wu2020optimal,tsigler2020benign,zhou2022non,wang2022tight,chatterji2021interplay,bartlett2021failures,shamir2022implicit,ghosh2022universal}, kernel regression \citep{liang2020just,mei2019generalization,liang2020multipledescent,mallinar2022benign,rakhlin2019consistency,belkin2018overfittingperfectfitting,mcrae2022harmless,beaglehole2022kernel,lai2023generalization}, and linear classification \citep{chatterji2020linearnoise,wang2021binary,cao2021benign,muthukumar2021classification,montanari2020maxmarginasymptotics,shamir2022implicit,liang2021interpolating,thrampoulidis2020theoretical,wang2021benignmulticlass,donhauser2022fastrates}. Moreover, several works considered benign overfitting in classification using nonlinear neural networks, in settings where the data distribution is linearly separable and the input dimension is larger than the number of training samples \citep{frei2022benign,frei2023benign,cao2022benign,kou2023benign}. 

\citet{mallinar2022benign} suggested the taxonomy of benign, tempered, and catastrophic overfitting, which we use in this work. They demonstrated empirically that nonlinear neural networks in classification tasks exhibit tempered overfitting. Moreover, they studied tempered overfitting in kernel regression. \citet{manoj2023interpolation} studied tempered overfitting for a learning rule returning short programs in some programming language.

We emphasize that the current work is the first to consider benign or tempered overfitting in regression using nonlinear neural networks. Furthermore, we note that all previous theoretical results on benign overfitting in linear predictors and neural networks considered high-dimensional settings, where the input dimension is larger than the number of samples in the training dataset. 
%In this work we consider a univariate setting, and the number of samples is much larger than the input dimension.

\paragraph{Shallow univariate ReLU networks.}

Several works studied ridgeless regression in univariate networks.
\citet{savarese2019infinite} and \citet{ergen2021convex} considered ridgeless regression with two-layer univariate ReLU networks. In their setting, the solutions are required to minimize the norm of the weights excluding the bias terms. That is, the bias terms are not penalized. They showed that the linear-spline interpolator is a solution to the ridgeless regression probelm, but this is not a unique solution. Thus, in addition to the linear-spline interpolator there are infinitely many other networks that fit the data and minimize the norms. \citet{hanin2021ridgeless} and \citet{debarre2022sparsest} considered a similar setting, and gave a geometric characterization of all minimal-norm interpolators.

\citet{boursier2023penalising} studied ridgeless regression with two-layer univariate ReLU networks, where both the weights and the bias terms are penalized. They showed that in this setting, there is a unique solution, i.e., a unique interpolator that minimizes the parameters' norms. Moreover, under certain assumptions on the data, this solution is sparse, namely, has a small number of kinks. We note that in this work we consider a similar setting, and we use their characterization of the min-norm interpolator.

\citet{williams2019gradient} studied regression in univariate two-layer ReLU networks in the kernel regime.
\citet{mulayoff2021implicit} analyzed stable minima when training shallow univariate ReLU networks using SGD.
Finally, \citet{safran2022effective} studied the dynamics and implicit bias in training shallow univariate ReLU networks in a classification setting. There, the implicit bias of gradient flow leads to solutions that maximize the margin in parameter space, namely, to networks with minimal parameter norms that attain a margin of at least $1$. 
%They considered a noiseless setting, and 
They showed that this bias toward norm minimization implies minimization of the number of linear regions up to a constant factor, and derived a generalization bound (in a noiseless setting).

% \section{Preliminaries} \label{sec:prelim}

%\paragraph{Notation.}\nati{probably remove this}
%For an integer $n \geq 1$ we denote $[n]=\{1,\ldots,n\}$.

% The following are not needed.  If the only notation is $[n]$, perhaps better to mention when it is actually used.

%We use $\normal(\mu,\sigma^2)$, $\text{Exp}(\lambda)$, $\unif([a,b])$, and $\Gamma(\alpha,\beta)$ to denote the normal, %exponential, uniform and Gamma distributions.


\paragraph{Min-norm interpolation with univariate ReLU networks.}

We consider neural networks $f_{\theta,a_0,b_0}: \reals \to \reals$ defined by
\[
    f_{\theta,a_0,b_0}(x) = a_0 x + b_0 + \sum_{j=1}^m a_j (w_j x + b_j)_+~,
\]
where $\theta \in \reals^{3m}$ denotes the vector of parameters $\{a_j,w_j,b_j\}_{j=1}^m$, and $(\cdot)_+$ is the ReLU operation $z \mapsto \max\{0,z\}$. The network $f_{\theta,a_0,b_0}$ is a two-layer ReLU network with an affine term. Let $\mathcal{F} = \{f_{\theta,a_0,b_0}: m \in \nat, \theta\in \reals^{3m}, a_0,b_0 \in \reals \}$ be the class of all such neural networks (of any finite width).

Let $S = \{(x_i,y_i)\}_{i=1}^n \subseteq [0,1] \times \reals$ be a dataset with $n \geq 2$. Throughout this work, we assume w.l.o.g. that $x_1 < \ldots < x_n$. Consider the following optimization problem:
% \begin{equation} \label{eq:main_problem}
%     \inf_{\substack{m \in \nat \\ \substack{\theta\in \reals^{3m} \\  a_0,b_0 \in \reals}}} \norm{\theta}^2 \;\text{ s.t. } \; \forall i \in [n],\; f_{\theta,a_0,b_0}(x_i)=y_i~.
% \end{equation}
\begin{equation} \label{eq:main_problem}
    \inf_{f_{\theta,a_0,b_0} \in \mathcal{F}} \norm{\theta}^2 \;\text{ s.t. } \; \forall i \in [n],\; f_{\theta,a_0,b_0}(x_i)=y_i~.
\end{equation}
The above problem considers interpolating the dataset $S$ using a min-norm network.
%(of any width). 
Note that $\theta$ includes all the parameters except for the affine term, and specifically it includes the bias terms. 
%\eqref{eq:main_problem} considers ReLU networks of arbitrarily large finite width $m$. 
%Our results also apply to networks of infinite width, but we consider here finite $m$ for ease of notation.
We remark that our results also apply to networks of infinite width, but \eqref{eq:main_problem} considers ReLU networks of (arbitrarily large) finite width for ease of notation.
Moreover, we note that \citet{boursier2023penalising} showed that the problem given in \eqref{eq:main_problem} is equivalent to minimizing the implied regularizer $R(f) = \int_{\IR} \sqrt{1+x^2}|f''(x)| dx$ over all interpolating functions $f$.

\gal{We should discuss the skip connection and possibly explain why we believe intuitively that it shouldn't matter too much.}

%\nirmit{Must mention restricted to our setting when $x_i \geq 0$ for all $i \in [n]$}
\citet{boursier2023penalising} showed the following lemma about the min-norm interpolator where all the $x_i$'s in $S$ are non-negative:
\begin{lemma}[\citet{boursier2023penalising}]
    The problem in \eqref{eq:main_problem} admits a minimizer. Moreover, the minimizer is unique (in function space) and has the following form:
    \begin{equation} \label{eq:min-norm_interpolator}
        \hat{f}_S(x) = ax + b + \sum_{i=1}^{n-1} a_i (x - \tau_i)_+~,
    \end{equation}
    where $\tau_i \in [x_i,x_{i+1})$ for every $i \in [n-1]$.
\end{lemma}
The above lemma implies that given a size-$n$ dataset $S$, there is a unique min-norm interpolator $\hat{f}_S$, and it has at most one kink in the interval $[x_i,x_{i+1})$ for every $i \in [n]$. Note that even though the problem in \eqref{eq:main_problem} allows arbitrarily wide networks, the min-norm interpolator $\hat{f}_S$ can be computed by a network of width at most $n-1$. 

\paragraph{Noisy interpolation learning.}

Let $f^*:[0,1] \to \reals$ be a Lipschitz function.
Let $\dist$ be a distribution on $[0,1] \times \reals$, such that $(x,y) \sim \dist$ satisfies 
%$x \sim \unif([0,1])$ and $y=f^*(x)+\eps$, where $\eps \sim \normal(0,\sigma^2)$ 
\begin{equation} \label{eq:define_D}
    x \sim \unif([0,1])\;\;\;\; \text{ and }\;\;\;\; y=f^*(x)+\eps \; \text{ where } \; \eps \sim \normal(0,\sigma^2)~,
\end{equation}
for a fixed constant $\sigma > 0$. We note that we focus on the uniform distribution on $[0,1]$ and on Gaussian noise for convenience, and our results can be extended to more general input and noise distributions. 
For a function $f:[0,1] \to \reals$ and $p \geq 1$, we define the population loss $L_p(f) := \Exp_{(x,y) \sim \dist} [|f(x)-y|^p]$. 
%Then, the Bayes-optimal loss is denoted by $L_p(f^*) := L_p(f^*)$.
%\Exp[ \, |\eps|^p]$ for $\eps \sim \normal(0,\sigma^2)$.

Let $S = \{(x_i,y_i)\}_{i=1}^n$ be a dataset drawn i.i.d. from $\dist$. 
In this work, we study the test performance of the min-norm interpolator $\hat{f}_S$, i.e., the population loss $L_p(\hat{f}_S)$.
%We focus on understanding the performance of the min-norm interpolator, and we do not study the optimization question of whether gradient methods trained with weight decay reach a global minimum or a sub-optimal local minimum. We believe that these are two different questions that may be studied separately.
By focusing on the min-norm interpolator, we can study tempered overfitting without analyzing the optimization question of whether gradient methods trained with weight decay reach a global minimum or a sub-optimal local minimum. Thus, it allows us to separate between two challenging problems, the first considers the performance of ridgeless regression, and the second considers convergence analysis for a non-convex problem.

}

%\paragraph{Benign, tempered, and catastrophic overfitting.}

\vspace{0.2cm}
\section{Warm up: tempered overfitting in linear-spline interpolation} \label{sec:linear-spline}
\vspace{0.2cm}

We start by analyzing tempered overfitting for linear-spline interpolation. Namely, we consider the piecewise-linear function obtained by connecting each pair of consecutive points in the dataset $S \sim \dist^n$ (see Figure~\ref{fig:splines-main_text} left), and analyze its test performance.
As discussed, the linear-spline interpolator is a minimizer of the network's norms (albeit not a unique minimizer) in a setting where the bias terms are not penalized \citep{savarese2019infinite,ergen2021convex}. In our setting, the linear-spline interpolator does not minimize the network's norms, but analyzing its test performance is a useful first step toward understanding min-norm interpolation.

% Figure environment removed

Given a dataset $S = \{(x_i,y_i)\}_{i=1}^n$, let $g_i:\IR \rightarrow \IR$ be the affine function joining the points $(x_{i},y_{i})$ and $(x_{i+1},y_{i+1})$. Thus, $g_i$ is the straight line joining the endpoints of the $i$-th interval. Then, the linear spline interpolator $\hat{g}_S: \IR \rightarrow \IR$ is given by
\begin{equation}\label{eq:linear-spline}
   \hat{g}_S(x) := g_1(x) \cdot \mathbf{1}\{ x < x_1\} + g_{n-1}(x) \cdot \mathbf{1}\{x \geq x_{n}\}+\sum_{i=1}^{n-1} g_i(x)\cdot \mathbf{1}\{x\in [x_{i},x_{i+1})\}. 
\end{equation}
%See an inllustration in Figure~\ref{fig:splines-main_text} (left). 
Note that in the ranges $[0,x_1]$ and $[x_n,1]$ the linear-spline $\hat{g}_S$ is defined by extending the lines connecting $x_1,x_2$ and $x_{n-1},x_n$ respectively.
The following theorem characterizes the asymptotic bebavior of $L_p(\hat{g}_S)$ for every $p \geq 1$:

% \begin{theorem}\label{thm:linear-splines}
%      Let $\dist$ be the distribution from \eqref{eq:define_D}, let $S \sim \dist^{n}$ and $\hat{g}_S$ be the linear-spline interpolator (\eqref{eq:linear-spline}) w.r.t. the dataset $S$. Then there exists an event $A$ with $\Pr_S[A]=1-o_n(1)$ such that for any $p \geq 1$ we have  %For the joint distribution specified as in the above, and any fixed $p \in \IN_{+}$, we have
%     \begin{equation} \label{eq:spline-tempered-exp}
%         \lim_{n \rightarrow \infty} \Exp_S  \left[L_p(\hat{g}_S) \mid A \right] \leq c_p \, L_p(f^*)~,
%     \end{equation}
%     where $L_p(f^*)=L_p(f^*)$ is the Bayes error and $c_p$ is a constant that only depends on $p$.
% \end{theorem}

\begin{theorem}\label{thm:linear-splines}
    Let $f^*$ be any Lipschitz function and $\dist$ be the distribution from \eqref{eq:define_D}. Let $S \sim \dist^{n}$, and $\hat{g}_S$ be the linear-spline interpolator (\eqref{eq:linear-spline}) w.r.t. the dataset $S$. Then, for any $p \geq 1$ there is a constant $C_p$ such that       
    \begin{equation*} \label{eq:spline-ub}
        \lim_{n \rightarrow \infty} \Pr_S  \left[L_p(\hat{g}_S) \leq C_p\, L_p(f^*) \right] = 1.
    \end{equation*}
    %  and
    % \begin{equation*} \label{eq:spline-lb}
    %     \lim_{n \rightarrow \infty} \Exp_S  \left[L_p(\hat{g}_S) \right] = \infty~,
    % \end{equation*}
    % For $f^* \equiv 0$ we also have 
    % \[
    %     \lim_{n \rightarrow \infty} \Pr_S \left[L_p(\hat{g}_S) \geq c_p \, L_p(f^*) \right] = 1,
    % \]
    % where $c_p > 0$ is a constant dependent on $p$.
\end{theorem}

% The above theorem considers conditional expectation w.r.t. to a high-probability event $A$. We note that it does not imply a bound on $\lim_{n \rightarrow \infty} \Exp_S  \left[L_p(\hat{g}_S) \right]$, since, as we discuss below, we have $\Exp_S \left[L_p(\hat{g}_S) \mid A^c \right] = \infty$ (where $A^c$ is the complementary event). Still, the theorem immediately implies a probability bound using Markov's inequality. Namely, for every $\delta>0$ we have
% % \[
% %     \Pr_S \left[L_p(\hat{g}_S) \geq \frac{c_p L_p(f^*)}{\delta} \right]
% %     \leq \Pr_S \left[L_p(\hat{g}_S) \geq \frac{c_p L_p(f^*)}{\delta} \middle| A \right] + (1 - \Pr_S[A])
% %     \leq \frac{\Exp_S  \left[L_p(\hat{g}_S) \mid A \right] \cdot \delta }{c_p L_p(f^*)} + o_n(1)~,
% %     %\leq \delta + o_n(1)~.
% % \]
% % and hence 
% \begin{equation} \label{eq:spline-tempered-prob}
%     \lim_{n \to \infty} \Pr_S \left[L_p(\hat{g}_S) \geq \frac{c_p}{\delta} \cdot L_p(f^*) \right] \leq \delta~.
% \end{equation}

The theorem shows that the linear-spline interpolator exhibits tempered behavior, 
%in probability, 
namely, w.h.p. over $S$ the interpolator $\hat{g}_S$ performs like the 
%Bayes optimal 
predictor $f^*$, up to a constant factor. 
%However, the theorem also shows that in expectation the $L_p$ loss of $\hat{g}_S$ diverges.  

To understand why Theorem~\ref{thm:linear-splines} holds, 
%We now discuss the main idea for the proof of the upper bound.
Note that for all $i \in [n-1]$ and $x \in [x_i,x_{i+1}]$ the linear-spline interpolator satisfies $\hat{g}_S(x) \in [\min\{y_i,y_{i+1}\}, \max\{y_i,y_{i+1}\}]$. Moreover, we have for all $i \in [n]$ that $|y_i-f^*(x_i)| = |\epsilon_i|$, 
%$\epsilon_i \sim \normal(0,\sigma^2)$. 
where $\epsilon_i$ is the random noise.
Using these facts, it is not hard to 
%show that the expected population loss of $\hat{g}_S$ in the range $[x_1,x_n]$ is bounded by a constant. 
bound the expected population loss of $\hat{g}_S$ in each interval $[x_i,x_{i+1}]$, and by using the law of large numbers it is also possible to bound the probability that the loss in the range $[x_1,x_n]$ is large.
Thus, in the range $[x_1,x_n]$ we can bound the $L_p$ loss both in probability and in expectation.

The less intuitive part is the behavior of $\hat{g}_S$ in the intervals $[0,x_1]$ and $[x_n,1]$. Analyzing the population loss in these intervals requires considering the distribution of the spacings between data points. Let $\ell_0,\ldots,\ell_n$ be such that 
\begin{equation} \label{eq:define_ell}
    \forall i \in [n-1]\;\; \ell_i = x_{i+1}-x_i, \;\;\;\; \ell_0 = x_1, \;\;\;\; \ell_n = 1-x_n~.
\end{equation}
%$\ell_i = x_{i+1}-x_i$ for $i \in [n-1]$, $\ell_0 = x_1$, and $\ell_n = 1-x_n$. 
Prior works \citep{alagar1976distribution,pinelis2019order} established that  
\begin{equation} \label{eq:ell_distribution}
    \left( \ell_0,\dots,\ell_n \right) \sim  \left( \frac{X_0}{X}, \dots, \frac{X_n}{X} \right), \text{ where } X_0,\dots,X_n \iid \text{Exp}(1), \text{ and } X:=\sum_{i=0}^{n} X_i~.
\end{equation}
%$\left( \ell_0,\dots,\ell_n \right) \sim  \left( \frac{X_0}{X}, \dots, \frac{X_n}{X} \right)$, where $X_0,\dots,X_n \iid \text{Exp}(1)$ and $X:=\sum_{i=0}^{n} X_i$ \citep{?}. 
For simplicity, assume that $f^* \equiv 0$ and consider the $L_1$ loss.
Since in the interval $[0,x_1]$ the linear-spline $\hat{g}_S$ is defined by extending the line connecting $x_1,x_2$, then it has slope of $O\left(\frac{1}{\ell_1}\right)$,
%(see Figure~\ref{}), 
and hence the $L_1$ loss of $\hat{g}_S$ in $[0,x_1]$ is 
\[
    O\left(\frac{\ell_0^2}{\ell_1}\right)
    = O\left( \frac{X_0^2}{X \cdot X_1} \right)~, 
\]
as can be seen in Figure~\ref{fig:splines-main_text} (right). 
%Indeed, the $L_1$ loss in $[0,x_1]$ corresponds to the area of a triangle with altitude $O\left(\frac{\ell_0}{\ell_1}\right)$ and base $\ell_0$.
Since $X_1 \sim \text{Exp}(1)$, then $\Exp \left[\frac{1}{X_1} \right] = \infty$, and as a consequence we show that the expected $L_1$ loss in $[0,x_1]$ is infinite. A similar argument also holds for the interval $[x_n,1]$. 
%Hence, the expected loss in the domain $[0,1]$ is infinite.
Hence, we get that 
\[
    \Exp_S  \left[L_p(\hat{g}_S)\right] = \infty~.
\]
% For this reason, in Theorem~\ref{thm:linear-splines} we consider conditional expectation w.r.t. an event $A$ which requires that $X_1$ and $X_{n-1}$ are sufficiently bounded away from zero. We show that the event $A$ has high probability, and the conditional expectation is finite.
However, with high probability the lengths $\ell_1$ and $\ell_{n-1}$ will not be too short, and therefore the loss in the intervals $[0,x_1]$ and $[x_n,1]$ will be bounded, which allows us to obtain tempered overfitting with high probability. 

We remark that the undesirable performance of the linear-spline interpolator $\hat{g}_S$ in the intervals $[0,x_1]$ and $[x_n,1]$ can be avoided by changing the definition of $\hat{g}_S$, such that it will return a constant value in these intervals (as in Figure~\ref{fig:min-norm-illustration1}). In this case, we will get tempered overfitting also in expectation.
However, we preferred to analyze linear splines as defined in \eqref{eq:linear-spline}, since a similar behavior in $[0,x_1]$ and $[x_n,1]$ also occurs in the min-norm interpolator, as we will show in the next section.
%, to demonstrate that even 
%even when the interpolator has a tempered behavior, we need to consider the conditional expectation of $L_p$ under the event $A$ that rules out unfortunate behavior in $[0,x_1]$ and $[x_n,1]$.
% In Section~\ref{sec:tempered-L1}, we will see that the min-norm interpolator also exhibits such an undesired behavior outside $[x_1,x_n]$, and hence we will consider conditional expectation in a similar manner.

% The definition for tempered overfitting in \citet{mallinar2022benign} considered only the expectation, which is infinite in our case. Hence, our result demonstrates that tempered behavior is delicate, and the definitions should be flexible enough.

 
% \nirmit{To analyze the linear splines it is crucial to understand the spacings of these $n$ points. Define $\ell_i$, which is in the appendix $C$. 
% $$ (\ell_0,\dots,\ell_n) \sim (..).$$
% explain briefly about the risk except the first and the last interval. and say that it is bounded for any $L_p$.

% Then, start saying that if one considers simply the Expectation of even the $L_1$ risk in the interval $[0,x_1]$ then the height it can reach is proportional to the ratio $\frac{\ell_0}{\ell_1}$ and hence the risk will be proportional to $\frac{\ell_0^2}{\ell_1}$. (express in terms of Xs) and this expectation is infinite. Thus, we have to condition on the event that the $X_1$ and $X_{n-1}$ are sufficiently bounded away from zero. The conditional expectation under this high probability converges even though the original expectation doesn't because the Expectation in the tail grows at a very fast rate.

% The above issue can also be circumvented by defining the linear splines as constant outside the interval $[x_1,x_n]$, in which case the Expectation turns out to be finite. But here we intentionally define it in such a way to demonstrate the sensitiveness of the simple expectation definition even though the overfitting cannot be called catastrophic,and hence our notion is more robust.}


%A positive result w.h.p. for all $p \geq 1$. Discuss the issue with the edges (using a figure?) and say that we don't have tempered behavior in expectation. We don't have to include a formal theorem for the negative result on the expectation (we can just mention it and say that we have a similar phenomenon in the section about tempered with $p \in [1,2)$).

\vspace{0.3cm}
\section{Min-norm interpolation with random data} \label{sec:main_results} 
\vspace{0.2cm}

In this section, we study the performance of the min-norm interpolator 
%$\hat{f}_S$ 
with random data. We first present some important properties of the minimal norm interpolator in Section~\ref{sec:characterizing}. In Sections~\ref{sec:tempered-L1} and~\ref{sec:catastrophic} we use this characterization to study its performance.

\vspace{0.2cm}
\subsection{Characterizing the min-norm interpolator} \label{sec:characterizing}
\vspace{0.2cm}

%In this section, we present some important properties of the minimal-norm interpolator $\hat{f}_S(x)$, which helps us to analyze its risk. 

Our goal is to give a characterization of the min-norm interpolator $\hat{f}_S(x)$ (\eqref{eq:min-norm_interpolator}), in terms of linear splines as defined in \eqref{eq:linear-spline}. Recall the definition of affine functions $g_1(x),\dots, g_{n-1}(x)$, which are piece-wise affine functions joining consecutive points. Let $\delta_i$ be the slope of the line $g_i(x)$, i.e. $\delta_i=g_i'(x)$. We denote $\delta_0:=\delta_1$ and $\delta_n:=\delta_{n-1}$. Then, we can define the sign of the curvature of the linear spline $\hat{g}_S(x)$ at each point. %Formally,
\begin{definition}\label{def:discrete-curvature}
    For any $i\in [n]$,
      $$ \curv(x_i) = \begin{cases}
  +1  & \delta_i > \delta_{i-1} \\
  0 & \delta_i = \delta_{i+1} \\
  -1 & \delta_i < \delta_{i-1}
\end{cases}$$
\end{definition}

% Figure environment removed
Based on the curvature, the following lemma geometrically characterizes $\hat{f}_S$ in any interval $[x_i,x_{i+1})$, in terms of the linear pieces $g_{i-1},g_i,g_{i+1}$.

\begin{lemma}\label{lem:complete-description-property-main_text}
The function $\hat{f}_S$ can be characterized as follows:
        \begin{itemize}
        \item $\hat{f}_S(x) =g_1(x)$ for $x\in (-\infty, x_2)$;
        \item $\hat{f}_S(x) =g_{n-1}(x)$ for $x \in [x_{n-1},\infty)$;
        \item In each interval $[x_i,x_{i+1})$ for $i \in \{2, \dots n-2\}$, 
        \begin{enumerate}
            \item If $\curv(x_i)=\curv(x_{i+1})=+1$ then
        $$ \max\{g_{i-1}(x), g_{i+1}(x)\} \leq  \hat{f}_S (x) \leq g_i(x);$$
        \item  If $\curv(x_i)=\curv(x_{i+1})=-1$ then
        $$  \min\{g_{i-1}(x), g_{i+1}(x)\} \geq  \hat{f}_S (x) \geq  g_i(x);$$ 
        \item Else, i.e. either $\curv(x_i)=0$ or $\curv(x_{i+1})=0$ or $\curv(x_{i}) \neq \curv(x_{i+1})$,
        $$\hat{f}_S(x)=g_i(x).$$
        \end{enumerate}
        \end{itemize}
\end{lemma}
The lemma implies that $\hat{f}_S$ coincides with $\hat{g}_S$ except in an interval $[x_i,x_{i+1})$ where the curvature of the two points are both $+1$ or $-1$ (see Figure~\ref{fig:geometric-illustration-of-triangle-main_text}). Intuitively, this property captures the worst-case effect of the spikes and will be crucial in showing the tempered behavior of $\hat{f}_S$ w.r.t. $L_p$ for $p\in [1,2)$. However, this still does not imply that such spikes are necessarily formed. 

To this end, \citet[Lemma 8]{boursier2023penalising} characterized the situation under which indeed these spikes are formed. Roughly speaking, if the sign of the curvature changes twice within three points, then we get a spike. Formally, we identify special points from left to right recursively where the sign of the curvature changes.
\begin{definition}\label{def:cruv-changed}
    We define $n_1:=1$. Having defined the location of the special points $n_1,\dots, n_{i-1}$, we recursively define 
    $$n_i= \min \{ j>n_{i-1} : \curv(x_{j}) \neq \curv (x_{n_i})\}.$$
    If there is no such $n_{i-1} < j \leq n$ where $\curv(x_{j}) \neq \curv (x_{n_i})$, then $n_{i-1}$ is the location of the last special point.
\end{definition}

\begin{lemma}[\citet{boursier2023penalising}]\label{lem:sparsity-main_text}
    For any $k \geq 1$, if $\delta_{n_k -1 }  \neq  \delta_{n_k}$ and $n_{k+1}= n_k + 2$, then $\hat{f}_S$ has exactly one kink between $(x_{{n_k}-1}, x_{n_{k+1}})$. Moreover, if $\curv(x_{n_k})=\curv(x_{n_k+1})=-1$ then $\hat{f}_S(x)=\min\{g_{n_k-1}(x), g_{n_k+1}(x)\}$ in $[x_{n_k},x_{n_k+1})$.
\end{lemma}

% Figure environment removed

This is a slight variation of \citep[Lemma 8]{boursier2023penalising}, which we reprove in the appendix for completeness. See Figure~\ref{fig:bad-case-kink-main-text} for an illustration of the above lemma.
To show the catastrophic behavior of $\hat{f}_S$ for $p \geq 2$, we will consider events under which such configurations of points are formed. This will result in spikes giving catastrophic behavior. 
%Discuss the spikes using a figure, and provide the main lemma/lemmas that characterize the behavior of the interpolator.

\vspace{0.2cm}
\subsection{Tempered overfitting for $L_p$ with $p \in [1,2)$} \label{sec:tempered-L1}
\vspace{0.2cm}

We now show the tempered behavior of the minimal norm interpolator w.r.t. $L_p$ losses for $p \in [1,2)$.

% \begin{theorem} \label{thm:tempered-for-ell_1}
% Let $\dist$ be the distribution from \eqref{eq:define_D}, let $S\sim \dist^n$, and let $\hat{f}_S$ be the min-norm interpolator (\eqref{eq:min-norm_interpolator}). Then, there exists an event $A$ with $\prob{A}=1-o_n(1)$ such that for any $p \in [1,2)$ we have
% \[ 
%     \lim_{n \rightarrow \infty}  \Exp_S \left[L_p(\hat{f}_S) \middle| A \right] \leq c_p \, L_p(f^*)~, 
% \]
% where $c_p$ is some constant dependent on $p$.
% \end{theorem}

\begin{theorem} \label{thm:tempered-for-ell_1}
Let $f^*$ be a Lipschitz function and $\dist$ be the distribution from \eqref{eq:define_D}. Sample $S\sim \dist^n$, and let $\hat{f}_S$ be the min-norm interpolator (\eqref{eq:min-norm_interpolator}). Then, for some universal constant $C > 0$, for any $p \in [1,2)$ we have 
\[
    \lim_{n \rightarrow \infty} \Pr_S \left[L_p(\hat{f}_S) \leq \frac{C}{2-p} \cdot \,  L_p(f^*) \right] = 1~.
\]
% For $f^*\equiv 0$ we also have 
% \[
%     \lim_{n \rightarrow \infty} \Pr_S \left[L_p(\hat{f}_S) \geq \frac{1}{c(2-p)} \cdot L_p(f^*)  \right] = 1~.
% \]
%\[ 
%    \lim_{n \rightarrow \infty} \Exp_S \left[L_p(\hat{f}_S) \right] = \infty~, 
%\]
\end{theorem}

% As in the case of linear splines considered in Section~\ref{sec:linear-spline}, the above theorem shows tempered behavior in probability, but also shows that in expectation the population loss of $\hat{f}_S$ diverges.

%the sense of bounded conditional expectation. Similarly to Theorem~\ref{thm:linear-splines}, the event $A$ in Theorem~\ref{thm:tempered-for-ell_1} rules out the posibility that the intervals $[x_1,x_2]$ and $[x_{n-1},x_n]$ are too short, which results in an undesired behavior of the interpolator $\hat{f}_S$ in the intervals $[0,x_1]$ and $[x_n,1]$. The result in Theorem~\ref{thm:tempered-for-ell_1} shows tempered behavior, but not in the sense of \citet{mallinar2022benign}. It demonstrates that defining tempered behavior is a delicate issue. 

The proof of Theorem~\ref{thm:tempered-for-ell_1} builds on Lemma~\ref{lem:complete-description-property-main_text}, which implies that in an interval $[x_i,x_{i+1})$, a spike in the interpolator $\hat{f}_S$ must be bounded within the triangle obtained from $g_{i-1},g_i,g_{i+1}$ (see Figure~\ref{fig:geometric-illustration-of-triangle-main_text}). 
The slopes of the affine functions $g_{i-1},g_{i+1}$ are roughly $\frac{1}{\ell_{i-1}},\frac{1}{\ell_{i+1}}$, where $\ell_j$ are the lengths as defined in \eqref{eq:define_ell}. Hence, the spike's height is proportional to $\frac{\ell_i}{\max\{\ell_{i-1},\ell_{i+1}\}}$. As a result, the $L_p$ loss in the interval $[x_i,x_{i+1}]$ is roughly 
\[
    \left(\frac{\ell_i}{\max\{\ell_{i-1},\ell_{i+1}\}} \right)^p \cdot \ell_i = \frac{\ell_i^{p+1}}{\max\{\ell_{i-1},\ell_{i+1}\}^p}~.
\]
%Similarly to our discussed after Theorem~\ref{thm:ell2-catastrophic-4-points}, in this situation, we can bound the $L_p$ loss in the interval $[x_i,x_{i+1}]$ by roughly $\frac{\ell_i^{p+1}}{\max\{\ell_{i-1},\ell_{i+1}\}^p}$. While for $p \geq 2$ the expectation of this expression is infinite (and hence in Theorem~\ref{thm:ell2-catastrophic-4-points}we obtained catastrophic overfitting), if $p \in (1,2]$ the expectation is finite and sufficiently small, such that the $L_p$ loss in the domain $[0,1]$ will be bounded by a constant.
Using the distribution of the $\ell_j$'s given in \eqref{eq:ell_distribution}, we can bound the expectation of this expression.
%, and using the law of large numbers it is also possible to bound the probability that the loss in the range $[x_1,x_n]$ is large. Overall, in the range $[x_1,x_n]$ we can get bounds for both the expectation and the probability, but similarly to our discussion 
Then, similarly to our discussion on linear splines in Section~\ref{sec:linear-spline}, in the range $[x_1,x_n]$ we can bound the $L_p$ loss both in expectation and in probability. In the intervals $[0,x_1]$ and $[x_n,1]$ the expected loss is infinite, and therefore we have 
\begin{equation} \label{eq:L1_infinite_exp}
    \Exp_S \left[L_p(\hat{f}_S) \right] = \infty~.
\end{equation}
Still, we can get a high probability upper bound for the $L_p$ loss in the intervals $[0,x_1]$ and $[x_n,1]$, as we did in the case of linear splines. Thus, we get a bound on $L_p$ loss in the entire domain $[0,1]$ w.h.p.
%in the domain $[0,1]$. 

We note that the definition of tempered overfitting in \citet{mallinar2022benign} considers only the expectation. 
%As we discussed above, 
Theorem~\ref{thm:tempered-for-ell_1} and \eqref{eq:L1_infinite_exp} imply that in our setting we have tempered behavior in probability but not in expectation, which
demonstrates that tempered behavior is delicate.
%, since it can occur in probability but not in expectation.

We also show a lower bound for the population loss $L_p$ which matches the upper bound from Theorem~\ref{thm:tempered-for-ell_1} (up to a constant factor independent of $p$). The lower bound holds already for $f^* \equiv 0$ and Gaussian label noise:

\begin{theorem} \label{thm:tempered-for-ell_1-lower_bound}
Let $f^* \equiv 0$, consider label noise $\epsilon \sim \normal(0,\sigma^2)$ for some constant $\sigma>0$, and let
$\dist$ be the corresponding distribution from \eqref{eq:define_D}. Sample $S\sim \dist^n$, and let $\hat{f}_S$ be the min-norm interpolator (\eqref{eq:min-norm_interpolator}). Then, for some universal constant $c > 0$, for any $p \in [1,2)$ we have 
\[
    \lim_{n \rightarrow \infty} \Pr_S \left[L_p(\hat{f}_S) \geq \frac{c}{2-p} \cdot L_p(f^*)  \right] = 1~.
\]
\end{theorem}

The proof of the above lower bound follows similar arguments to the proof of catastrophic overfitting for $p \geq 2$, which we will discuss in the next section.

\vspace{0.3cm}
\subsection{Catastrophic overfitting for $L_p$ with $p \geq 2$} \label{sec:catastrophic}
\vspace{0.3cm}

Next, we prove that for the $L_p$ loss with $p \geq 2$, the min-norm interpolator exhibits catastrophic overfitting. We prove this result already for $f^* \equiv 0$ and Gaussian label noise:

\begin{theorem}\label{thm:ell2-catastrophic}
    %Let $f^* \equiv 0$, let $\dist$ be the distribution from \eqref{eq:define_D}, 
    Let $f^* \equiv 0$, consider label noise $\epsilon \sim \normal(0,\sigma^2)$ for some constant $\sigma>0$, and let $\dist$ be the corresponding distribution from \eqref{eq:define_D}.
    Let $S \sim \dist^n$, and let $\hat{f}_S$ be the min-norm interpolator (\eqref{eq:min-norm_interpolator}).
    Then, for any $p \geq 2$ and $b>0$,
    \[
        \lim_{n \to \infty} \Pr_S\left[{L_p(\hat{f}_S) > b} \right] = 1~.
    \]
    % and 
    % \[
    %     \lim_{n \rightarrow \infty} \Exp_S \left[L_p(\hat{f}_S) \right] = \infty~.
    % \]
\end{theorem}










%We start with our negative result on $\Exp_S \left[L_p(\hat{f}_S)\right]$: 
%\nirmit{Why are you saying $\hat{f}_S$ is the min-norm interpolant. I think we should refer to the minimizer of problem we are going to write. This really looks informal. Even for linear spline it is better to refer equation for $\hat{g}_S$, this is just my opinion. If someone directly just looks at the theorem they can't understand what they are unless they have a reference with definition. Even for $\dist$ we should have an equation and say something as follows. }

% \begin{theorem}\label{thm:ell2-catastrophic-4-points}
%     Let $f^* \equiv 0$ and let $\dist$ be the distribution from \eqref{eq:define_D}.
%     %, and consider the joint distribution $\dist$ according to \eqref{?}. 
%     Consider any $n \geq 4$, and sample $S \sim \dist^n$. Let $\hat{f}_S$ be the min-norm interpolator (\eqref{eq:min-norm_interpolator}), then for any $p\geq 2$
%     \[
%        \Exp_S \left[L_p(\hat{f}_S)\right]=\infty~. 
%     \] 
% \end{theorem}

% It is important to note that the above theorem holds for every $n \geq 4$ and does not require considering the limit $n \to \infty$. Thus, already for a size-$4$ dataset we have $\Exp_S \left[L_p(\hat{f}_S)\right]=\infty$.

To 
%understand this 
obtain some intuition on this
phenomenon, consider the first four samples $(x_1,y_1),\ldots,(x_4,y_4)$, and let 
$\ell_i$ be the lengths of the intervals as defined in \eqref{eq:define_ell}.
We show that with constant probability, 
the configuration of the labels of these samples satisfies certain properties, which are illustrated in Figure~\ref{fig:bad-case-kink-main-text}.
In this case, Lemma~\ref{lem:sparsity-main_text} implies that 
%there is at most one kink in the interval $(x_1,x_4)$, and hence 
in the interval $[x_2,x_3]$ the interpolator $\hat{f}_S$ is equal to $\min\{g_1(x),g_3(x)\}$, where $g_1$ (respectively, $g_3$) is the affine function that connects $x_1,x_2$ (respectively, $x_3,x_4$). 
%To this end, we rely on properties of $\hat{f}_S$ shown by \citet{boursier2023penalising}. 
Now, as can be seen in the figure, in this ``unfortunate configuration'' the interpolator $\hat{f}_S$ spikes above $f^* \equiv 0$ in the interval $[x_2,x_3]$, and the spike's height is proportional to $\frac{\ell_2}{\max\{\ell_1,\ell_3\}}$. As a result, the $L_p$ loss in the interval $[x_2,x_3]$ is roughly 
%$\left(\frac{\ell_2}{\max\{\ell_1,\ell_3\}}\right)^p \cdot \ell_2 = \frac{\ell_2^{p+1}}{\max\{\ell_1,\ell_3\}^p}$.
$\frac{\ell_2^{p+1}}{\max\{\ell_1,\ell_3\}^p}$.
% Recall from 
% %Section~\ref{sec:linear-spline}, 
% \eqref{eq:ell_distribution}
% that the $\ell_i$'s roughly behave like exponential random variables.
Using \eqref{eq:ell_distribution}, we can show that 
$\Exp_S \left[ \frac{\ell_2^{p+1}}{\max\{\ell_1,\ell_3\}^p} \right] = \infty$ for any $p \geq 2$.
% Using this fact we can show that $\Exp \left[ \frac{\ell_2^{p+1}}{\max\{\ell_1,\ell_3\}^p} \right] = \infty$ for any $p \geq 2$.

Now, we divide the $n$ samples in $S$ into $\Theta(n)$ disjoint subsets of $6$ consecutive points. For each subset we consider the event that the labels are such that the $4$ middle points exhibit an ``unfortunate configuration'' as described above. These events are independent and occur with constant probability. Therefore, there will be $\Theta(n)$ subsets with such unfortunate configurations of labels with high probability. The expectation of the population loss $L_p$ in each of these subsets is infinite. Using the fact that we have $\Theta(n)$ such subsets and the losses in these subsets are only mildly correlated, we are able to prove that $\hat{f}_S$ exhibits a catastrophic behavior also in probability.

We note that the proof of the Theorem~\ref{thm:tempered-for-ell_1-lower_bound} follows similar arguments, except that when $p<2$ the expectation of the $L_p$ loss in each subset with an ``unfortunate configuration'' is finite, and hence we get a finite lower bound.

% \nirmit{The main idea is to consider an unfortunate configuration of points as depicted in Figure \ref{?}, under which the $\hat{f}_S(x)$ spikes between the interpolated points. (formed provably according to Lemma \ref{?}). Consider sampling $n$ points and just focus on the first 4 points of $(x_1,y_1), (x_2,y_2), (x_3, y_3), (x_4, y_4)$. We consider the event when both $y_2,y_3>0$ and $\curv(x_2)=\curv(x_3)=-1$ but $\curv(x_4)=+1$. This event happens with a constant probability. Under this event, one can ensure that there is indeed a spike formed in the interval $(x_2,x_3)$ and $\hat{f}_S(x)=\min\{g_1(x),g_3(x)\}$ by Lemma \ref{?}.

% Therefore we just look at the risk in the intervals $(x_2,x_3)$ which is given by $\int_{x_2}^{x_3} |\hat{f}_S(x)-f^*(x)|^p = \int_{x_2}^{x_3} |\min\{g_1(x),g_3(x)\}-f^*(x)|^p$. As $\hat{f}_S(x)$ is simply the extension of the lines $g_1(x)$ and $g_3(x)$ in the interval $(x_2,x_3)$, it forms a spike whose height is roughly characterized by the random variable $\frac{\ell_2}{\max\{\ell_1,\ell_3\}}$, which basically behaves as $\frac{\ell_2}{\ell_1+\ell_3}$. Note that the spike gets high only when both $\ell_1$ and $\ell_3$ are small.

% Therefore, $L_p$ risk in the interval $(x_2, x_3)$ is basically characterized by $\left(\frac{\ell_2}{(\ell_2+\ell_3)} \right)^p \cdot \ell_2=\frac{\ell_2^{p+1}}{(\ell_2+\ell_3)^p}$. The denominator is the sum of two exponentials and behaves like $\Gamma(2,1/n)$. And it turns out that $\Exp \left[ \frac{1}{\Gamma(2,\lambda)^{p}}\right]=\infty$ for any $p \geq 2$ and any rate parameter $\lambda$.
% }

% Given the above negative result, one may think that the expectation diverges due to a low-probability event that results in an extremely sub-optimal interpolator causing an infinite expectation of the risk, as we saw in the case of linear splines. However, we now show a stronger result that says that the catastrophic behavior also occurs in probability over $S$.

%\nirmit{Given the above negative result, one may think that the expectation diverges due to a low-probability event that results in an extremely sub-optimal interpolator causing an infinite expectation of the risk, as we saw in the case of linear splines. However, we show a stronger result that says that the catastrophic behavior also occurs in probability over $S$.}

% \begin{theorem}\label{thm:ell2-catastrophic}
%     Let $f^* \equiv 0$, let $\dist$ be the distribution from \eqref{eq:define_D}, let $S \sim \dist^n$, and let $\hat{f}_S$ be the min-norm interpolator (\eqref{eq:min-norm_interpolator}).
%     Then, for any $p \geq 2$ and $b>0$,
%     \[
%         \lim_{n \to \infty} \Pr_S\left[{L_p(\hat{f}_S) > b} \right] = 1~.
%     \]
%\end{theorem}

% The proof idea can be roughly described as follows. First we divide the $n$ samples to $\Theta(n)$ disjoint subsets of $6$ consecutive points. Then, for each subset we consider the event that the $4$ middle points exhibit an ``unfortunate configuration'' as in the proof of Theorem~\ref{thm:ell2-catastrophic-4-points}. These events are independent and occur with constant probability. Therefore, there will be $\Theta(n)$ subsets with such unfortunate configurations of points with high probability. The expectation of the population loss $L_p$ in each of these subsets is infinite. Using the fact that we have $\Theta(n)$ such subsets and the losses in these subsets are only mildly correlated, we are able to prove the theorem.

% \nirmit{We again consider the signal $f^*=0$. We divide the $n$ points into disjoint subsets of 6 consecutive points. We have $\Theta(n)$ such subsets. We then consider an event for each of the individual subsets that the unfortunate configuration of 4 points in the middle happens for them as depicted. All these events are independent and happen with constant probability. Therefore, there will $\Theta(n)$ such configurations of points with high probability.

% Thus, defining the risk in each of these subsets (maybe define r.v.s) is a random variable of the type (refer), each having infinite expectations. Therefore, the overall risk is the sum of at least $\Theta(n)$ identically distributed random variable with infinite expectation, but they are mildly correlated. However, we bound the covariance between these random variables decays at a sufficient rate. Finally, using standard probabilistic techniques, we establish that the overall risk, which is the sum of these random variables, can get arbitrarily high with high probability .}

% \nirmit{I know there is more detail to this like truncating, etc. But this level of detail is already a lot. }

% \vspace{0.3cm}
\section{Min-norm interpolation with samples on the grid} \label{sec:grid}
% \vspace{0.3cm}

In this section, we analyze the population loss of the min-norm interpolator, when the $n$ data-points in $S$ are uniformly spaced, 
%on the $x$-axis (on the grid) 
instead of i.i.d. uniform sampling considered in the previous sections. Namely, consider
the training set $S=\{(x_i,y_i): i \in [n] \}$, where
\begin{equation} \label{eq:grid-data}
    x_i=\frac{i}{n} \hspace{2mm} \text{and} \hspace{2mm} y_i=f^*(x_i)+\eps_i \;
    %, \hspace{5mm} 
    \text{ for i.i.d. noise } \; \eps_i~. 
    %\sim \normal(0,\sigma^2)~.
\end{equation}
Note that the randomness in $S$ is only in the 
%i.i.d. 
label noises $\epsilon_i$. 
It can be interpreted as a \emph{nonadaptive active learning} setting, where the learner can actively choose the training points, and then observe noisy measurements at these points, and the query points are selected on an equally spaced grid.

We show that in this situation the min-norm interpolator exhibits tempered overfitting with respect to any $L_p$ loss:
%, both in expectation and in probability:

% \begin{theorem}\label{thm:grid}
%  For the size-$n$ training set $S$ given by \eqref{eq:grid-data}, let $\hat{f}_S$ be the min-norm interpolator (\eqref{eq:min-norm_interpolator}). Then for any $p\geq 1$, we have
%  \[
%     \lim_{n \rightarrow \infty} \Exp_S [L_p(\hat{f}_S)] \leq c_p \, L_p(f^*)~,
%  \]
%  where $c_p$ is a constant that only depends on $p$, and the population loss $L_p$ is defined w.r.t. the distribution $\dist$ from \eqref{eq:define_D}.
% \end{theorem}

\begin{theorem}\label{thm:grid}
 Let $f^*$ be any Lipschitz function. For the size-$n$ dataset $S$ given by \eqref{eq:grid-data}, let $\hat{f}_S$ be the min-norm interpolator (\eqref{eq:min-norm_interpolator}). Then for any $p\geq 1$, there is a constant $C_p$ such that
 \[
    \lim_{n \rightarrow \infty} \Pr_S \left[L_p(\hat{f}_S) \leq C_p \, L_p(f^*)\right] = 1~.
 \]
 % For $f^* \equiv 0$ we also have 
 %    \[
 %        \lim_{n \rightarrow \infty} \Pr_S \left[L_p(\hat{g}_S) \geq c_p \, L_p(f^*) \right] = 1,
 %    \]
    % where $c_p > 0$ is a constant dependent on $p$.
 % and
 % \[
 %    \lim_{n \rightarrow \infty} \Exp_S [L_p(\hat{f}_S)] \leq c_p L_p(f^*)~,
 % \]
 % where $c_p$ is a constant that only depends on $p$, and the population loss $L_p$ is defined w.r.t. the distribution $\dist$ from \eqref{eq:define_D}.
\end{theorem}

An intuitive explanation 
%for the upper bound 
is as follows. Since the points are uniformly spaced, whenever spikes are formed, they can at most reach double the height without the spikes. Thus, the population loss of $\hat{f}_S(x)$ becomes worse but only by a constant factor. 
We remark that in this setting the min-norm interpolator exhibits tempered overfitting both in probability (as stated in Theorem~\ref{thm:grid}) and in expectation.
We emphasize that the polynomial regression (minimum degree polynomial interpolating the data) will continue to be catastrophic even under the grid setting.

From Theorem~\ref{thm:grid} we conclude that the catastrophic behavior for $L_p$ with $p \geq 2$ shown in 
%Section~\ref{sec:catastrophic} 
Theorem~\ref{thm:ell2-catastrophic}
stems from the non-uniformity in the lengths of the intervals $[x_i,x_{i+1}]$, which occurs when the $x_i$'s are drawn at random. 

Finally, we note that previous works considered benign overfitting with data on the grid as a simplified setting, which may help in understanding more general situations \citep{beaglehole2022kernel,lai2023generalization}. Our results imply that this simplification might change the behavior of the interpolator significantly.


% Motivation to consider the grid:
% \begin{itemize}
%     \item The catastrophic behavior in $L_p$ for $p\geq 2$ is really because of the sampling of $x_i$s. If we chose them by hand as above, then, even under the presence of noise, we observe tempered behavior.
%     \item It also can be interpreted as an analysis of function reconstruction based on "chosen design" or "nonadaptive active learning", i.e. when we get to choose the $x$'s, and so choose them on a grid.
%     \item Previous works considered benign overfitting on the grid as a simplified case for understanding the more general problem \citep{beaglehole2022kernel,lai2023generalization}, but our result implies that this simplification might change the result significantly.
% \end{itemize}


% \section{Experiments} \label{sec:experiments}

% Nati's suggestion:

% Example of interpolation of a specific dataset of around 10 points, taken from e.g. a mild parabola or sin (not linear) with added uniform or Gaussian noise (or just selected by hand) with
% \begin{itemize}
%     \item linear spline
%     \item min-norm-with-skip 
%     \item min-norm-without-skip (you would barely be able to tell the difference)
%     \item GD on large net without regularization
% \end{itemize}

% % Figure environment removed
% % Figure environment removed

%Distribution of the population loss for min norm interpolation for data sampled from simple function (not linear) plus added simple noise (eg uniform).  Ideally, we want to show:
%\begin{itemize}
%    \item behavior of $l_1, l_2$ and $l_\infty$ loss.  Even better would be for $l_p$ as a function of $p$
%    \item behavior as a function of sample size $m$.  
%$$    \item distribution, and not just a single run nor expectation.  This can be a histogram for some fixed $m$, and/or percentile lines as a function of $m$.
 %   \item Distribution even for very small $m$, e.g. $m=4$ or a bit more, and $p=2$ (or a bit more).  
 %   \item behavior for min-norm-without-skip and for GD and maybe also min-norm-with-skip, but I think most important for GD.
%\end{itemize}

%\section{Discussion}

\begin{ack}
TODO
\end{ack}

 \vspace{0.3cm}

\bibliography{bib.bib}
\bibliographystyle{abbrvnat}

\clearpage

\appendix

\input{NeurIPS/linear_spline_proof}
\input{NeurIPS/properties}
\input{NeurIPS/l1-tempered-proof}
\input{NeurIPS/ell2_proof}
\input{NeurIPS/grid_proof}
\end{document}