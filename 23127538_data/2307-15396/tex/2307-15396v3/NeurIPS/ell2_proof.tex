\section{Lower Bounds}
Our lower bounds (Theorem \ref{thm:tempered-for-ell_1-lower_bound} and \ref{thm:ell2-catastrophic}) follow from very similar situations under which spikes are formed. Therefore, we first present the basic construction of such a situation; later, we specialize to the case of $p\in [1,2)$ and $p \geq 2$ and obtain both theorems respectively. 

 Recall the noise model $\eps \sim \normal(0,1)$. We first claim the following.
 \begin{lemma}\label{lem: Lp>Rp}
For any function $f$, we have $\mathcal{L}_p(f) \geq \mathcal{R}_p(f)$.
 \end{lemma}
 The above lemma then allows us to solely focus on lower bounding the reconstruction loss of the predictor.
 \begin{proof}[Proof of Lemma \ref{lem: Lp>Rp}]
By definition,
\begin{align*}
    \mathcal{L}_p(f)&= \Exp_{(x,y)\sim \dist} \left[ |f(x)-y|^p\right] = \Exp_{x \sim \unif([0,1]) ,\eps \sim \normal(0,1)} \left[ |f(x)-f^*(x) - \eps|^p\right]. \\
\end{align*}
Let $q: \IR_+ \rightarrow \IR_+$ be the density of the folded standard Gaussian, i.e. $|\mathcal{N}(0,1)|$. Then due to the symmetry of the Gaussian,
\begin{align*}
    \mathcal{L}_p(f)&= \Exp_{x \sim \unif([0,1])} \Exp_{\eps \sim \normal(0,1)} \left[ |f(x)-f^*(x)-\eps|^p\right] \\
    &= \Exp_{x \sim \unif([0,1])} \left[ \int_{0}^{\infty} \left( \frac{1}{2}|f(x)-f^*(x) - \delta|^p+ \frac{1}{2}|f(x)-f^*(x) + \delta|^p \right) q(\delta) \, d \delta \right] \\
    & \geq \Exp_{x \sim \unif([0,1])} \left[ \int_{0}^{\infty} |f(x)-f^*(x)|^p  q(\delta) \, d \delta \right] \tag{by Claim \ref{clm:minimum} below}\\
    &= \Exp_{x\sim \unif([0,1])} [ |f(x)-f^*(x)|^p]=\mathcal{R}_p(f). \tag{by definition} 
\end{align*}
Here the second last step follows from the following claim, which we prove below. 
\begin{claim}\label{clm:minimum}
    For any $\mu \in \IR$, $p \geq 1$, and $\delta \in \IR_+$, we have $ \frac{1}{2}(|\mu+\delta|^p+|\mu-\delta|^p) \geq |\mu|^p$.
\end{claim}
\begin{proof}
    The claim trivially holds for $\mu=0$. Now consider $\mu>0$, then if $\delta \geq \mu$ then
    $$ \frac{1}{2}((\mu+\delta)^p+|\mu-\delta|^p) \geq \frac{1}{2}(2\mu)^p \geq 2^{p-1} \mu^p \geq \mu^p.$$
    But if $\delta<\mu$ then since the function $|.|^p$ is convex for $p \geq 1$ we have
    $$ \frac{1}{2}(|\mu+\delta|^p+|\mu-\delta|^p) = \frac{1}{2}(\mu+\delta)^p+ \frac{1}{2}(\mu-\delta)^p \geq \left( \frac{\mu+\delta+\mu-\delta}{2}\right)^p = \mu^p.$$
Finally, if $\mu<0$, then apply the same argument to $-\mu$ giving the desired result.
 \end{proof}
This concludes the proof of the lemma.
\end{proof}

We now describe the situations under which spikes are formed. We will consider $f^*(x) \equiv 0$ and the noise model $\eps \sim \normal(0,1)$. Let $S\sim \dist^{n}$ be the data points. Again, we number them such that $$ x_0=: 0 < x_1 < \dots < x_n < 1:=x_{n+1}.$$  
We have $n+1$ intervals with lengths $\ell_0,\dots, \ell_n$ from left to right. In particular, $\ell_i=x_{i+1}-x_{i}$. 

We now consider $N:=\floor{n/10}$ disjoint subsets $S_1,\dots ,S_N \subset S$ such that $S_i \cap S_j =\emptyset$ for $i\neq j$. Each $S_i$ is a set of six consecutive points (ordered from left to right). For any $i \in [N]$,
$$S_i:=\{ (x_j,y_j): j= 10(i-1)+1, \dots, 10(i-1)+6 \}.$$
It is clear by definition that these sets are disjoint. 

We denote the noise random variables associated with $j$-th point of $S_i$ by $\eps_{i,j}$. Also. we re-denote the distance between any two consecutive points from the same subset by $\ell_{i,j}$, i.e. $\ell_{i,j}$ is the distance between $(j+1)$-th point and $j$-th point in $S_i$. Consider the events $A_1,\dots, A_N$ as below. 
\begin{equation*}
    A_i:=\{ \eps_{i,2}, \eps_{i,5} \leq -1 \} \cap \{\eps_{i,1},\eps_{i,3},\eps_{i,4},\eps_{i,6} \in [1,2] \} \cap \{ \ell_{i,3} \geq \ell_{i,2} \} \cap \{ \ell_{i,3} \geq \ell_{i,4} \}
\end{equation*}
% Figure environment removed

See Figure \ref{fig:bad-configuration} for an illustration. This considers the event that the bad configuration of points happens for points in $S_i$. Then we can express $\mathcal{R}_p(\hat{f}_S)$, the risk of the predictor in $[0,1]$, in terms of the random variables $\ell_i$ s when such bad configurations occur. Formally,

\begin{lemma}\label{lem:risk-when-Ai-happens}
    \begin{itemize}
        \item For $1 \leq p <2$:
        $$\mathcal{R}_p(\hat{f}_S) \geq \sum_{i=1}^{N} \frac{\ell_{i,3}^{p+1}}{(\ell_{i,2}+\ell_{i,4})^p} \mathbbm{1}[A_i];$$
        \item For $p \geq 2:$
        $$\mathcal{R}_p(\hat{f}_S) \geq \sum_{i=1}^{N} \frac{\ell_{i,3}^3}{(\ell_{i,2}+\ell_{i,4})^{2}} \mathbbm{1}[A_i].$$
    \end{itemize}
\end{lemma}

\begin{proof}
      Without loss of generality, assume that $A_1$ holds and consider the following analysis under $A_1$. (One can choose any $i\in [N]$ and the discussion follows in the same way). If $A_1$ happens then $\curv(x_2)=+1$, $\curv(x_3)=\curv(x_4)=-1$ and again at $x_5$ we will have $\curv(x_5)=+1$. Therefore, $x_3$ and $x_5$ are special points according to Definition \ref{def:cruv-changed}; the curvature changes within the distance of 2 points. Therefore, by Lemma \ref{lem:sparsity-main_text}, there is exactly one kink possible in $(x_2,x_5)$. More importantly, in the interval $(x_3,x_4)$, the function $\hat{f}_S(x)=\min\{g_2(x),g_4(x)\}$. Therefore, for any $x\in [x_3,x_4]$,  

$$g_2(x)= y_3+ \left( \frac{y_3-y_2}{x_3-x_2} \right) (x-x_3) = \eps_3+ \frac{\eps_3-\eps_2}{x_3-x_2}(x-x_3) \geq 1 + \frac{2}{\ell_2}(x-x_3).$$ 
Similarly,
$$g_4(x)= y_4+ \frac{y_5-y_4}{x_5-x_4}(x-x_4) = \eps_4+ \frac{\eps_5-\eps_4}{x_5-x_4}(x-x_4) = \eps_4 + \frac{\eps_4-\eps_5}{x_5-x_4}(x_4-x) \geq 1 + \frac{2}{\ell_4}(x_4-x).$$
Therefore, if $A_1$ happened then writing the $L_p$ reconstruction error in the interval $[x_3,x_4]$:
\begin{align*}
   \int_{x_3}^{x_4} |\hat{f}_S(x)-f^*(x)|^p dx &= \int_{x_3}^{x_4} |\hat{f}_S(x)|^p dx = \int_{x_3}^{x_4} \min\{g_2(x),g_4(x)\}^p \, dx \\
   & \geq  \int_{x_3}^{x_4} \min\{1+\frac{2}{\ell_2}(x-x_3), 1+\frac{2}{\ell_4}(x_4-x) \}^p \, dx \\
  & = \int_{x_3}^{x^*} \left( 1+\frac{2}{\ell_2}(x-x_3) \right)^p\, dx + \int_{x^*}^{x_4} \left( 1+\frac{2}{\ell_4}(x_4-x) \right)^p\, dx \, , \\
\end{align*}
where $x^* \in [x_3,x_4]$ is the $x$-coordinate of the point of intersection of the two lines. Thus, to find $x^*$
\begin{align*}
    1+\frac{2}{\ell_2}(x^*-x_3) &= 1+\frac{2}{\ell_4}(x_4-x^*) \\
   \frac{1}{\ell_2}(x^*-x_3) &= \frac{1}{\ell_4}(\ell_3-(x^*-x_3)) \\
   \left( \frac{1}{\ell_2} + \frac{1}{\ell_4} \right) (x^*-x_3) &= \frac{\ell_3}{\ell_4} \\
   (x^*-x_3) &= \frac{\ell_3}{\ell_4} \cdot \frac{\ell_2 \ell_4}{\ell_2+\ell_4} = \frac{\ell_2 \ell_3}{\ell_2+\ell_4} \\
   \implies (x_4-x^*) &= \ell_3-\frac{\ell_2 \ell_3}{\ell_2+\ell_4} = \frac{\ell_4 \ell_3}{\ell_2+\ell_4} \\
\end{align*}
 
\textbf{When $\mathbf{1 \leq p <2}$}: Writing the reconstruction risk in $[x_3,x_4]$ and substituting the above:
\begin{align}
   \int_{x_3}^{x_4} |\hat{f}_S(x)-f^*(x)|^p \, dx &\geq  \int_{x_3}^{x^*} \left( 1+\frac{2}{\ell_2}(x-x_3) \right)^p\, dx + \int_{x^*}^{x_4} \left( 1+\frac{2}{\ell_4}(x_4-x) \right)^p\, dx  \nonumber \\
  & \geq  \int_{x_3}^{x^*} \left( \frac{2}{\ell_2}(x-x_3) \right)^p \, dx+\int_{x^*}^{x_4} \left( \frac{2}{\ell_4}(x_4-x) \right)^p\, dx \nonumber\\
 & = \frac{2^p}{{(p+1)} \cdot {\ell_2}^p } (x^*-x_3)^{p+1}+\frac{2^p}{{(p+1)} \cdot {\ell_4}^p } (x_4-x^*)^{p+1} \nonumber\\
 &= \frac{2^p}{(p+1)} \left( \frac{\ell_2 \ell_3^{p+1}}{(\ell_2+\ell_4)^{p+1}}+\frac{\ell_4 \ell_3^{p+1}}{(\ell_2+\ell_4)^{p+1}} \right) \nonumber  \\
 & = \frac{2^{p}}{(p+1)} \cdot \frac{\ell_3^{p+1}}{(\ell_2+\ell_4)^{p}} \nonumber \\
 & \geq \frac{\ell_3^{p+1}}{(\ell_2+\ell_4)^{p}}, \label{eq:lb-one-interval-risk-p(1,2)}
\end{align} 
where the last inequality follows from the fact that we are considering $p \in [1,2)$.

\textbf{When $\mathbf{p \geq 2}$}
\begin{align}
   \int_{x_3}^{x_4} |\hat{f}_S(x)-f^*(x)|^p \, dx &\geq  \int_{x_3}^{x^*} \left( 1+\frac{2}{\ell_2}(x-x_3) \right)^p\, dx + \int_{x_*}^{x_4} \left( 1+\frac{2}{\ell_4}(x_4-x) \right)^p\, dx \nonumber\\ 
  & \geq  \int_{x_3}^{x^*} \left( 1+ \frac{2}{\ell_2}(x-x_3) \right)^2\, dx +  \int_{x^*}^{x_4} \left( 1+\frac{2}{\ell_4}(x_4-x) \right)^2\, dx \tag{both integrands are at least 1 in the range} \nonumber\\
  & \geq \int_{x_3}^{x^*} \left( \frac{2}{\ell_2}(x-x_3) \right)^2\, dx +  \int_{x^*}^{x_4} \left(\frac{2}{\ell_4}(x_4-x) \right)^2\, dx \nonumber\\
 & = \frac{4}{3 \, \ell_2^2} (x^*-x_3)^3 + \frac{4}{3 \, \ell_4^2} (x_4-x^*)^3 \nonumber\\
 &= \frac{4}{3} \cdot \frac{\ell_2 \ell_3^3}{(\ell_2+\ell_4)^3} + \frac{4}{3} \cdot \frac{\ell_4 \ell_3^3}{(\ell_2+\ell_4)^3} \nonumber\\
 & =  \frac{4}{3} \cdot \frac{\ell_3^3}{(\ell_2+\ell_4)^2} \nonumber \\
 & \geq \frac{\ell_3^3}{(\ell_2+\ell_4)^2}. \label{eq:lb-one-interval-risk-p>2}
\end{align}

By the definition of risk, one can say that
$$\mathcal{R}_p(\hat{f}_S) \geq \sum_{i \in N} \int_{x_{i,3}}^{x_{i,4}} |\hat{f}_S(x)-f^*(x)|^p \, dx $$
Then by doing the above analysis as in \eqref{eq:lb-one-interval-risk-p(1,2)} and \eqref{eq:lb-one-interval-risk-p>2} for each $i \in N$, along with the observation that the risk in any $[x_{i,3},x_{i,4}]$ is non-negative, we obtain the desired result. In particular, for $1\leq p <2:$
 $$\mathcal{R}_p(\hat{f}_S)\geq \sum_{i=1}^{N}\frac{\ell_{i,3}^{p+1}}{(\ell_{i,2}+\ell_{i,4})^{p}} \mathbbm{1}[A_i];$$
    And, for $p \geq 2:$
        $$\mathcal{R}_p(\hat{f}_S) \geq \sum_{i=1}^{N} \frac{\ell_{i,3}^3}{(\ell_{i,2}+\ell_{i,4})^{2}} \mathbbm{1}[A_i]. $$
\end{proof}

\begin{lemma}\label{lem:A_i-are-iid}
    The events $\{A_i\}_{i=1}^{N}$ are independent. Moreover, $\Pr(A_i)=c_0$ for some universal constant $c_0$.
\end{lemma}
\begin{proof}
    The events $A_1,\dots, A_N$ are independent since the noises are i.i.d. Also, the event about the interval lengths (that are dependent random variables) in the above $\{ \ell_{i,3} \geq \ell_{i,2} \} \cap \{ \ell_{i,3} \geq \ell_{i,4} \}$ can be expressed in terms of the underlying exponential random variables (which are independent) as $\{ X_{i,3} \geq X_{i,2} \} \cap \{ X_{i,3} \geq X_{i,4} \}$. Finally, it is easy to observe that for any $i\in [N]$, $\prob{A_i}$ is some universal constant $c_0$. 
\end{proof}

\subsection{Proof of Theorem \ref{thm:tempered-for-ell_1-lower_bound}}

\begin{proof}[Proof of Theorem \ref{thm:tempered-for-ell_1-lower_bound}]
    We have $f^*\equiv 0$ and the noise distribution is $\normal(0,1)$. Therefore, $\mathcal{L}_p(f^*)$ is just a constant (dependent on $p$). More precisely,
    \begin{equation}\label{eq:bound on bayes error}
        L_p(f^*)=\Exp[|\eps|^p]= \sigma^p \cdot \frac{2^{p/2}}{\sqrt{\pi}} \Gamma\left( \frac{p+1}{2} \right) \leq \frac{2}{\sqrt{\pi}},
    \end{equation}
   where in the last inequality we use $\sigma=1$ and $1\leq p <2$.  Also, for $1 \leq p < 2$, by Lemma \ref{lem:risk-when-Ai-happens}
    \begin{equation}\label{eq:def-risk-lb}
       \mathcal{R}_p(\hat{f}_S)\geq \left( \sum_{i=1}^{N}\frac{\ell_{i,3}^{p+1}}{(\ell_{i,2}+\ell_{i,4})^{p}} \mathbbm{1}[A_i]  \right) := \hat{R}_p 
    \end{equation}
Then, to create independence, we define $(\Tilde{\ell}_0, \dots, \Tilde{\ell}_n) \iid \nicefrac{\text{Exp}(1)}{(n+1)}$ where $\Tilde{\ell}_i= \frac{X_i}{n+1}$. We now replace $\ell_i$'s with $\Tilde{\ell}_i$'s to define $\Tilde{R}_p$. 
$$ \Tilde{R}_p := \sum_{i=1}^{N}\frac{\Tilde{\ell}_{i,3}^{p+1}}{(\Tilde{\ell}_{i,2}+\Tilde{\ell}_{i,4})^{p}} \mathbbm{1}[A_i] $$
\begin{lemma}\label{lem:almost-surely-lb-p in [1,2)}
As $n \rightarrow \infty$ we have $\hat{R}_p - \Tilde{R}_p \xrightarrow{\textnormal{a.s.}}  0
$.
\end{lemma}
Moreover, expressing $\Tilde{R}_p$ in terms of $X_i$'s 
\begin{equation}\label{eq:Rtilde-definition-lb}
    \Tilde{R}_p= \frac{1}{(n+1)}\sum_{i=1}^{N}\frac{X_{i,3}^{p+1}}{(X_{i,2}+X_{i,4})^{p}} \mathbbm{1}[A_i]=\frac{N}{(n+1)} \cdot \frac{1}{N}\sum_{i=1}^{N}\frac{X_{i,3}^{p+1}}{(X_{i,2}+X_{i,4})^{p}} \mathbbm{1}[A_i]
\end{equation}
Therefore, $\Tilde{R}_p$ is the average of $N$ i.i.d. random variables (up to scaling). Each random variables have the expectation
\begin{align}
    \Exp \left[ \frac{X_{i,3}^{p+1}}{(X_{i,2}+X_{i,4})^{p}} \mathbbm{1}[A_i] \right] & \leq \Exp \left[ \frac{X_{i,3}^{p+1}}{(X_{i,2}+X_{i,4})^{p}} \right] = \Exp[X_{i,3}^{p+1}]\Exp \left[ \frac{1}{(X_{i,2}+X_{i,4})^{p}} \right] \nonumber\\
    &=\Exp[\text{Exp}(1)^{p+1}] \cdot \Exp \left[ \frac{1}{\Gamma(2,1)^p}\right] \tag{sum of two i.i.d. Exp(1)s is $\Gamma(2,1)$} \nonumber \\
    &=\left( \int_{0}^{\infty} z^{p+1} e^{-z} \, dz \right) \left( \int_{0}^{\infty} \frac{1}{z^{p}} \cdot \frac{1}{\Gamma(2)} z e^{-z} \,dz \right) \nonumber\\
    &= \left( \int_{0}^{\infty} z^{p+1} e^{-z} \, dz \right) \left( \int_{0}^{\infty} z^{1-p} e^{-z} \,dz \right) \nonumber\\
    &= \Gamma(p+2) \Gamma(2-p)< \infty, \label{eq:39}
\end{align}
for $1 \leq p <2.$ Thus, by the strong law of large numbers, as $n \rightarrow \infty$ (also $N=\floor{\nicefrac{n}{10}} \rightarrow \infty)$
\begin{align*}
   & \frac{1}{N}\sum_{i=1}^{N}\frac{X_{i,3}^{p+1}}{(X_{i,2}+X_{i,4})^{p}} \mathbbm{1}[A_i] \xrightarrow{\text{a.s}} \Exp\left[ \frac{X_{i,3}^{p+1}}{(X_{i,2}+X_{i,4})^{p}} \mathbbm{1}[A_i] \right] \\
    &= \Pr(A_i) \Exp \left[ \frac{X_{i,3}^{p+1}}{(X_{i,2}+X_{i,4})^{p}} \mid A_i \right] 
    = \Pr(A_i) \Exp \left[ \frac{X_{i,3}^{p+1}}{(X_{i,2}+X_{i,4})^{p}} \mid X_{i,2}\leq X_{i,3}, X_{i,4} \leq X_{i,3} \right] \tag{by definition of $A_i$} \\
    & \geq \Pr(A_i) \Exp \left[ \frac{X_{i,3}^{p+1}}{(X_{i,2}+X_{i,4})^{p}} \right] \tag{since the function is non-increasing in terms of $X_{i,2}$ and $X_{i,4}$} \\
    &= c_0 \Exp[\text{Exp}(1)^{p+1}] \cdot \Exp\left[ \frac{1}{\Gamma(2,1)^p}\right] = c_0 \cdot \Gamma(2+p) \cdot \Gamma(2-p)  \geq 2 c_0 \cdot \Gamma(2-p) \tag{since $1\leq p <2$}\\
    & \geq \frac{c_0}{(2-p)} \tag{by Claim \ref{clm:Gamma(z)}}
\end{align*}
Moreover, $\lim_{n \rightarrow \infty} \frac{N}{n+1}=\lim_{n \rightarrow \infty} \frac{\floor{\nicefrac{n}{10}}}{n+1}=\frac{1}{10}.$ Therefore, recalling the definition of $\Tilde{R}_p$ in \eqref{eq:Rtilde-definition-lb}, we obtain that for a constant $\gamma>0$
$$ \lim_{n \rightarrow \infty} \Pr_S \left[ \Tilde{R}_p \geq  \frac{c_0}{10(2-p)} -\gamma \right]=1.$$
Therefore, using Lemma \ref{lem:almost-surely-lb-p in [1,2)}
$$ \lim_{n \rightarrow \infty} \Pr_S \left[ \hat{R}_p \geq  \frac{c_0}{20(2-p)} \right]=1.$$
Recalling \eqref{eq:def-risk-lb} that $\hat{R}_p$ is a lower-bound on the risk, we obtain
$$ \lim_{n \rightarrow \infty} \Pr_S \left[ \mathcal{R}_p(\hat{f}_S) \geq  \frac{c_0}{20(2-p)}\right]=1.$$
By \eqref{eq:bound on bayes error}, $\mathcal{L}_p(f^*) \leq \nicefrac{2}{\sqrt{\pi}}$. Choosing $c:=\nicefrac{c_0 \sqrt{\pi}}{40}$ we get the desired theorem, i.e.
$$\lim_{n\rightarrow \infty} \Pr_S \left[ \mathcal{R}_p(\hat{f}_S) \geq \frac{c}{(2-p)} \, \mathcal{L}_p(f^*)\right ]=1.$$
Finally, using Lemma \ref{lem: Lp>Rp} we obtain
$$\lim_{n\rightarrow \infty} \Pr_S \left[ \mathcal{L}_p(\hat{f}_S) \geq \frac{c}{(2-p)} \, \mathcal{L}_p(f^*)\right ]=1. $$
\end{proof}

\begin{proof}[Proof of Lemma \ref{lem:almost-surely-lb-p in [1,2)}]
    Recall \eqref{eq:ell_distribution} that $\ell_{i,j}\sim \nicefrac{X_{i,j}}{X}$ for $1 \leq i \leq N$ and $j\in \{1,2,3,4,5,6\}$, where $X_{i,j}\sim \text{Exp}(1)$ and $X$ is the sum of $(n+1)$ i.i.d Exp$(1)$ random variables.
    Therefore, by definition of $\hat{R}_p$ and $\Tilde{R}_p$ 
    \begin{align}
        \hat{R}_p-\Tilde{R}_p &= \left(\sum_{i=1}^{N}\frac{\ell_{i,3}^{p+1}}{(\ell_{i,2}+\ell_{i,4})^{p}} \mathbbm{1}[A_i]\right)-\left(\sum_{i=1}^{N}\frac{\Tilde{\ell}_{i,3}^{p+1}}{(\Tilde{\ell}_{i,2}+\Tilde{\ell}_{i,4})^{p}} \mathbbm{1}[A_i] \right) \nonumber\\
        &= \sum_{i=1}^{N} \left(\frac{ (\nicefrac{X_{i,3}}{X})^{p+1}}{(\nicefrac{X_{i,2}}{X}+ \nicefrac{X_{i,4}}{X})^{p}} \mathbbm{1}[A_i]-\frac{ (\nicefrac{X_{i,3}}{n+1})^{p+1}}{(\nicefrac{X_{i,2}}{n+1}+ \nicefrac{X_{i,4}}{n+1})^{p}} \mathbbm{1}[A_i] \right) \nonumber\\
  &= \underbrace{\frac{1}{n+1}\sum_{i=1}^{N}\left[ \frac{X_{i,3}^{p+1}}{(X_{i,2}+X_{i,4})^{p}} \mathbbm{1}[A_i] \right]}_{:=T}  \left(\frac{n+1}{X}-1 \right) \label{eq:40}
    \end{align}
    We already know that by \eqref{eq:39}, $T$ converges to a finite quantity almost surely as $n \rightarrow \infty$. More formally, by the strong LLN 
    \begin{align*}
         T &= \frac{N}{n+1} \cdot \frac{1}{N} \sum_{i=1}^{N}\left[ \frac{X_{i,3}^{p+1}}{(X_{i,2}+X_{i,4})^{p}} \mathbbm{1}[A_i]\right] \xrightarrow{\text{a.s.}} \frac{1}{10} \cdot \Exp\left[ \frac{X_{i,3}^{p+1}}{(X_{i,2}+X_{i,4})^{p}} \mathbbm{1}[A_i] \right]<\infty,
    \end{align*}
where the last inequality follows from \eqref{eq:39}. Since $X$ is the sum of $(n+1)$ i.i.d. Exp(1) random variables, by the strong law of large numbers, as $n\rightarrow \infty$, we have $\frac{n+1}{X} \xrightarrow{\text{a.s.}} 1 $. Therefore, $\left(\frac{n+1}{X} -1\right)\xrightarrow{\text{a.s.}} 0$. Combining this in \eqref{eq:40}, we obtain that $\hat{R}_p-\Tilde{R}_p \xrightarrow{\text{a.s}} 0$. 
\end{proof}

\subsection{Proof of Theorem \ref{thm:ell2-catastrophic}}

\begin{proof}[Proof of Theorem \ref{thm:ell2-catastrophic}]
    We have the same $f^*\equiv 0$ and the noise distribution is $\normal(0,1)$. By Lemma \ref{lem:risk-when-Ai-happens} for $p \geq 2$
    $$ \mathcal{R}_p(\hat{f}_S) \geq \sum_{i=1}^{N} \frac{\ell_{i,3}^3}{(\ell_{i,2}+\ell_{i,4})^2} \cdot \mathbbm{1}[A_i]. $$
    Recall \eqref{eq:ell_distribution} that $\ell_{i,j}\sim \nicefrac{X_{i,j}}{X}$, where $X_{i,j}\sim \text{Exp}(1)$ and $X$ is the sum of $(n+1)$ i.i.d Exp$(1)$ random variables, one of which is $X_{i,j}$. Thus,
    \begin{equation}\label{eq:risk-def-catastrophic}
       \mathcal{R}_p(\hat{f}_S) \geq \sum_{i=1}^{N} \frac{ (\nicefrac{X_{i,3}}{X})^3}{(\nicefrac{X_{i,2}}{X}+\nicefrac{X_{i,4}}{X})^2} \cdot \mathbbm{1}[A_i]= \frac{1}{X} \sum_{i=1}^{N} \frac{X_{i,3}^3}{(X_{i,2}+X_{i,4})^2}\mathbbm{1}[A_i]:=\hat{R}.  
    \end{equation}
For $i \in [N]$, define
\begin{equation}\label{eq:risk-in-intervals}
\hat{R}_i:= \frac{X_{i,3}^3 \mathbbm{1}[A_i]}{(X_{i,2}+X_{i,4})^2}. \quad \text{Then } \hat{R}=\frac{1}{X} \sum_{i=1}^{N} \hat{R}_i 
\end{equation}
Each $\hat{R}_i$ has an infinite expectation, as we show in the following calculation.
\begin{align*}
    \Exp[\hat{R}_i]&=\Exp \left[ \frac{X_{i,3}^3 \mathbbm{1}[A_i]}{(X_{i,2}+X_{i,4})^2} \right]=\Pr(A_i) \cdot \Exp\left[\frac{X_{i,3}^3}{(X_{i,2}+X_{i,4})^2} \mid A_i\right] \\
    & = c_0 \cdot \Exp\left[\frac{X_{i,3}^3}{(X_{i,2}+X_{i,4})^2} \mid X_{i,2} \leq X_{i,3}, X_{i,4} \leq X_{i,3}\right] \tag{by Lemma \ref{lem:A_i-are-iid} and recall $A_i$} \\
    & \geq c_0 \cdot \Exp\left[\frac{X_{i,3}^3}{(X_{i,2}+X_{i,4})^2}\right] \tag{the function inside expectation is non-increasing in terms of $X_{i,2}, X_{i,4}$ }\\
    &= c_0 \cdot \Exp[\text{Exp}(1)^3] \cdot \Exp\left[\frac{1}{\Gamma(2,1)^2}\right]\\
    &= 6 c_0  \cdot \int_{0}^{\infty}
    \frac{1}{z^2} \cdot z e^{-z} \, dz= 6 c_0  \cdot \int_{0}^{\infty}
    \frac{e^{-z}}{z} \, dz= \infty.
\end{align*}
Therefore, we define the truncated random variables $\hat{T}_i(a)$ which is the truncation of $\hat{R}_i$ at any finite threshold $a$. Formally,
   $$\hat{T}_i(a):= \min\{\hat{R}_i,a\}.$$
   Then $\hat{T}_i(a)$ has finite expectation by its definition, which we denote by $\Exp[\hat{T}_i(a)]=h(a)$ for some non-decreasing function $h(a)$. Also, it must be that $\lim_{a\rightarrow \infty} h(a)=\infty$, because the expectation of $\hat{T}_i(a)$ goes to infinity as $a$ does. We now define
 $$\hat{T}(a):= \frac{1}{X}\sum_{i=1}^{N} \hat{T}_i(a),$$
then $\hat{T}(a)$ for any fixed $a$, by its definition serves as a lower bound on $\hat{R}$ (recall \eqref{eq:risk-in-intervals}). We rewriting $\hat{T}(a)$ as follows
\begin{equation}\label{eq:rewrite-hatT}
    \hat{T}(a):= \frac{n+1}{X} \cdot \frac{N}{n+1} \cdot \frac{1}{N}\sum_{i=1}^{N} \hat{T}_i(a).
\end{equation}
Then as $n \rightarrow \infty$, since $X$ is the sum of $n+1$ i.i.d. Exp$(1)$, by the strong law of large numbers 
\begin{equation}\label{eq:X-con}
    \frac{n+1}{X} \xrightarrow{\text{a.s.}} \Exp[\text{Exp}(1)]= 1.
\end{equation}
Similarly, by the strong LLN
\begin{equation}\label{eq:h(a)-con}
    \frac{1}{N} \sum_{i=1}^{N} \hat{T}_i(a) \xrightarrow{\text{a.s.}} \Exp[\hat{T}_i(a)]= h(a).
\end{equation}
And, finally using $\lim_{n\rightarrow \infty} \frac{N}{n+1}=\frac{1}{10}$ along with $\eqref{eq:h(a)-con}$ and \eqref{eq:X-con} in \eqref{eq:rewrite-hatT}, we obtain
    $$\hat{T}(a)\xrightarrow{ \text{a.s.}} \frac{h(a)}{10}. $$
Therefore, for any $b>0$ choose $a^*$ such that $h(a^*) \geq 21 b$, then as $n\rightarrow \infty$, $\hat{T}(a^*) \xrightarrow{ \text{a.s.}} \nicefrac{h(a^*)}{10}=2.1 b$ (note that it is always possible to choose such $a^*$ since $\lim_{a \rightarrow \infty} h(a)=\infty$). This implies
$$\lim_{n \rightarrow \infty} \Pr_S [ \hat{T}(a^*) \geq 2b]=1.$$
Thus, using the fact that $\hat{T}(a^*)$ is a lower bound on $\hat{R}$ 
$$\lim_{n \rightarrow \infty} \Pr_S [ \hat{R} \geq 2 b]=1.$$
Finally, recalling \eqref{eq:risk-def-catastrophic} that $\hat{R}$ is a lower bound on $\mathcal{R}_p(\hat{f}_S)$ we obtain the desired theorem.
$$\lim_{n \rightarrow \infty} \Pr_S [ \mathcal{R}_p(\hat{f}_S)> b]=1.$$
Note that Lemma \ref{lem: Lp>Rp} immediately also implies
$$\lim_{n \rightarrow \infty} \Pr_S [ \mathcal{L}_p(\hat{f}_S)> b]=1.$$


\end{proof}