
\documentclass{article} % For LaTeX2e
\usepackage{iclr2024_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{amsthm}
\usepackage{graphicx, nicefrac}
\usepackage{url}
\usepackage{xspace}
\usepackage[mathscr]{euscript}
\usepackage{bbm, dsfont}

\usepackage{tikz}

\title{Noisy Interpolation Learning with Shallow Univariate ReLU Networks}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \thanks{ Use footnote for providing further information
about author (webpage, alternative address)---\emph{not} for acknowledging
funding agencies.  Funding acknowledgements go at the end of the paper.} \\
Department of Computer Science\\
Cranberry-Lemon University\\
Pittsburgh, PA 15213, USA \\
\texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
\And
Ji Q. Ren \& Yevgeny LeNet \\
Department of Computational Neuroscience \\
University of the Witwatersrand \\
Joburg, South Africa \\
\texttt{\{robot,net\}@wits.ac.za} \\
\AND
Coauthor \\
Affiliation \\
Address \\
\texttt{email}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}[section]
\newtheorem{invariant}{Invariant}
\newtheorem{proposition}[lemma]{Proposition}
\newtheorem{corollary}[lemma]{Corollary}
\newtheorem{claim}[lemma]{Claim}
\newtheorem{fact}[lemma]{Fact}
\newtheorem{conj}[lemma]{Conjecture}
\newtheorem{definition}[lemma]{Definition}

\newtheorem{problem}{Problem}





\newtheorem{mdresult}{Result}
\newenvironment{result}{\begin{mdframed}[backgroundcolor=lightgray!40,topline=false,rightline=false,leftline=false,bottomline=false,innertopmargin=2pt]\begin{mdresult}}{\end{mdresult}\end{mdframed}}


\newtheorem{remark}[lemma]{Remark}
\newtheorem{assumption}{A\hspace{-1mm}}
\newtheorem{observation}[lemma]{Observation}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\input{ICLR2024/macros}
%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}

Understanding how overparameterized neural networks generalize despite perfect interpolation of noisy training data is a fundamental question. \citet{mallinar2022benign} noted that neural networks seem to often exhibit ``tempered overfitting'', wherein the population risk does not converge to the Bayes optimal error, but neither does it approach infinity, yielding non-trivial generalization. However, this has not been studied rigorously.  We provide the first rigorous analysis of the overfiting behaviour of regression with minimum norm ($\ell_2$ of weights), focusing on univariate two-layer ReLU networks.  We show overfitting is tempered (with high probability) when measured with respect to the $L_1$ loss, but also show that the situation is more complex than suggested by \citeauthor{mallinar2022benign}, and overfitting is catastrophic with respect to the $L_2$ loss, or when taking an expectation over the training set.

\end{abstract}

\input{ICLR2024/paper}


\bibliography{iclr2024_conference}
\bibliographystyle{iclr2024_conference}

\appendix
\input{ICLR2024/delicateness}
\input{NeurIPS/linear_spline_proof}
\input{NeurIPS/properties}
\input{NeurIPS/l1-tempered-proof}
\input{NeurIPS/ell2_proof}
\input{NeurIPS/grid_proof}
\end{document}
