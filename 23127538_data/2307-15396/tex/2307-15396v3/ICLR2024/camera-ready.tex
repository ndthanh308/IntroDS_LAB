
\documentclass{article} % For LaTeX2e
\usepackage{iclr2024_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{amsthm}
\usepackage{graphicx, nicefrac}
\usepackage{url}
\usepackage{xspace}
\usepackage[mathscr]{euscript}
\usepackage{bbm, dsfont}

\usepackage{tikz}

\title{Noisy Interpolation Learning with Shallow Univariate ReLU Networks}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Nirmit Joshi \\
TTI-Chicago\\
\texttt{nirmit@ttic.edu}
\And
Gal Vardi \\
TTI-Chicago and Hebrew University\\
\texttt{galvardi@ttic.edu}
\And
Nathan Srebro \\
TTI-Chicago \\
\texttt{nati@ttic.edu}     
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}[section]
\newtheorem{invariant}{Invariant}
\newtheorem{proposition}[lemma]{Proposition}
\newtheorem{corollary}[lemma]{Corollary}
\newtheorem{claim}[lemma]{Claim}
\newtheorem{fact}[lemma]{Fact}
\newtheorem{conj}[lemma]{Conjecture}
\newtheorem{definition}[lemma]{Definition}

\newtheorem{problem}{Problem}





\newtheorem{mdresult}{Result}
\newenvironment{result}{\begin{mdframed}[backgroundcolor=lightgray!40,topline=false,rightline=false,leftline=false,bottomline=false,innertopmargin=2pt]\begin{mdresult}}{\end{mdresult}\end{mdframed}}


\newtheorem{remark}[lemma]{Remark}
\newtheorem{assumption}{A\hspace{-1mm}}
\newtheorem{observation}[lemma]{Observation}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\input{ICLR2024/macros}
\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle
\vspace{-2mm}
\begin{abstract}
\vspace{-1mm}
Understanding how overparameterized neural networks generalize despite perfect interpolation of noisy training data is a fundamental question. \citet{mallinar2022benign} noted that neural networks seem to often exhibit ``tempered overfitting'', wherein the population risk does not converge to the Bayes optimal error, but neither does it approach infinity, yielding non-trivial generalization. However, this has not been studied rigorously.  We provide the first rigorous analysis of the overfitting behavior of regression with minimum norm ($\ell_2$ of weights), focusing on univariate two-layer ReLU networks.  We show overfitting is tempered (with high probability) when measured with respect to the $L_1$ loss, but also show that the situation is more complex than suggested by \citeauthor{mallinar2022benign}, and overfitting is catastrophic with respect to the $L_2$ loss, or when taking an expectation over the training set.
\end{abstract}
\input{ICLR2024/paper}
\subsection*{Acknowledgements}
This research was done as part of the NSF-Simons Sponsored Collaboration on the Theoretical Foundations of Deep Learning and the NSF Tripod Institute on Data, Econometrics, Algorithms, and Learning (IDEAL). N. J. would like to thank Surya Pratap Singh for his generous time in helping resolve Python errors.
\bibliography{iclr2024_conference}
\bibliographystyle{iclr2024_conference}
\appendix
\input{ICLR2024/delicateness}
\input{NeurIPS/linear_spline_proof}
\input{NeurIPS/properties}
\input{NeurIPS/l1-tempered-proof}
\input{NeurIPS/ell2_proof}
\input{NeurIPS/grid_proof}
\end{document}
