\documentclass[11pt]{article}
\sloppy
\usepackage{amsthm}
\usepackage{graphicx} % support the \includegraphics command and options
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{mathtools}
\usepackage{amsmath, amssymb, amsfonts, verbatim,bbm}
\usepackage{hyphenat,epsfig,subcaption,multirow}
\usepackage{nicefrac}
\usepackage{paralist}
\usepackage[utf8]{inputenc}
\usepackage[font=small, labelfont=bf]{caption}
\usepackage{natbib}
\usepackage[usenames,dvipsnames]{xcolor}

\DeclareFontFamily{U}{mathx}{\hyphenchar\font45}
\DeclareFontShape{U}{mathx}{m}{n}{
      <5> <6> <7> <8> <9> <10>
      <10.95> <12> <14.4> <17.28> <20.74> <24.88>
      mathx10
      }{}
\DeclareSymbolFont{mathx}{U}{mathx}{m}{n}
\DeclareMathSymbol{\bigtimes}{1}{mathx}{"91}
\usepackage{tcolorbox}
\tcbuselibrary{skins,breakable}
\tcbset{enhanced jigsaw}

\usepackage[normalem]{ulem}
\usepackage[compact]{titlesec}

\definecolor{DarkRed}{rgb}{0.1,0.1,0.8}
\definecolor{DarkBlue}{rgb}{0.1,0.1,0.5}


\usepackage{nameref}
\definecolor{ForestGreen}{rgb}{0.1333,0.5451,0.1333}
\definecolor{Red}{rgb}{0.9,0,0}
\usepackage[linktocpage=true,
	pagebackref=true,colorlinks,
		urlcolor=blue,
	linkcolor=DarkRed, citecolor=ForestGreen,
	bookmarks,bookmarksopen,bookmarksnumbered]
	{hyperref}
\usepackage[noabbrev,nameinlink]{cleveref}
\crefname{property}{property}{Property}
\creflabelformat{property}{(#1)#2#3}
\crefname{equation}{eq}{Eq}
\creflabelformat{equation}{(#1)#2#3}

\usepackage{bm}
\usepackage{url}
\usepackage{xspace}
\usepackage[mathscr]{euscript}

\usepackage{tikz}
\usetikzlibrary{arrows}
\usetikzlibrary{arrows.meta}
\usetikzlibrary{shapes}
\usetikzlibrary{backgrounds}
\usetikzlibrary{positioning}
\usetikzlibrary{decorations.markings}
\usetikzlibrary{patterns}
\usetikzlibrary{calc}
\usetikzlibrary{fit}
\usetikzlibrary{decorations.pathmorphing}

\usepackage{mdframed}

\usepackage[noend]{algpseudocode}
\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother

\usepackage{cite}
\usepackage{enumitem}


\usepackage[margin=1in]{geometry}




\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}[section]
\newtheorem{invariant}{Invariant}
\newtheorem{proposition}[lemma]{Proposition}
\newtheorem{corollary}[lemma]{Corollary}
\newtheorem{claim}[lemma]{Claim}
\newtheorem{fact}[lemma]{Fact}
\newtheorem{conj}[lemma]{Conjecture}
\newtheorem{definition}[lemma]{Definition}
\newtheorem*{Definition}{Definition}
\newtheorem{problem}{Problem}

\newtheorem*{claim*}{Claim}
\newtheorem*{proposition*}{Proposition}
\newtheorem*{lemma*}{Lemma}
\newtheorem*{problem*}{Problem}

\crefname{lemma}{Lemma}{Lemmas}
\crefname{claim}{Claim}{Claims}


\newtheorem{mdresult}{Result}
\newenvironment{result}{\begin{mdframed}[backgroundcolor=lightgray!40,topline=false,rightline=false,leftline=false,bottomline=false,innertopmargin=2pt]\begin{mdresult}}{\end{mdresult}\end{mdframed}}


\newtheorem{remark}[lemma]{Remark}
\newtheorem{assumption}{A\hspace{-1mm}}
\newtheorem{observation}[lemma]{Observation}

\newtheoremstyle{restate}{}{}{\itshape}{}{\bfseries}{~(restated).}{.5em}{\thmnote{#3}}
\theoremstyle{restate}
\newtheorem*{restate}{}

\theoremstyle{definition}
\newtheorem{mdalg}{Algorithm}
\newenvironment{Algorithm}{\begin{tbox}\begin{mdalg}}{\end{mdalg}\end{tbox}}
\allowdisplaybreaks



\DeclareMathOperator*{\argmax}{arg\,max}

\renewcommand{\qed}{\nobreak \ifvmode \relax \else
      \ifdim\lastskip<1.5em \hskip-\lastskip
      \hskip1.5em plus0em minus0.5em \fi \nobreak
      \vrule height0.75em width0.5em depth0.25em\fi}

\newcommand{\Qed}[1]{\ensuremath{\qed_{\textnormal{~#1}}}}

\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}

\setlength{\parskip}{3pt}

\newcommand{\logstar}[1]{\ensuremath{\log^{*}\!{#1}}}
\newcommand{\logr}[1]{\ensuremath{\log^{(#1)}}}

\renewcommand{\eqref}[1]{Eq.~(\ref{#1})}

\input{NeurIPS/macros}

\title{Noisy Interpolation Learning with Shallow Univariate \\ ReLU Networks}
\author{
Nirmit Joshi \\ TTI-Chicago\\
\href{mailto:nirmit@ttic.edu}{\textcolor{black}{\texttt{nirmit@ttic.edu}}}
 \and
Gal Vardi\\ TTI-Chicago and Hebrew University\\
\href{mailto:galvardi@ttic.edu}{\textcolor{black}{\texttt{galvardi@ttic.edu}}}  \and
Nathan Srebro \\ TTI-Chicago\\
\href{mailto:nati@ttic.edu}{\textcolor{black}{\texttt{nati@ttic.edu}}}
\date{}}
\begin{document}
\maketitle

\vspace{-11mm}
\begin{center}
    Collaboration on the Theoretical Foundations of Deep Learning (\url{deepfoundations.ai})
\end{center}
 


\begin{abstract}
  We study the asymptotic overfitting behavior of interpolation with minimum norm ($\ell_2$ of the weights) two-layer ReLU networks for noisy univariate regression.  We show that overfitting is tempered for the $L_1$ loss, and any $L_p$ loss for $p<2$, but catastrophic for $p\geq 2$.
\end{abstract}

\section{Introduction}

A recent realization is that although sometimes overfitting can be catastrophic, as suggested by our classic learning theory understanding, in other models overfitting, and even interpolation learning, i.e.~insisting on zero training error of noisy data, might not be so catastrophic, allowing for good generalization (low test error) and even consistency \citep{zhang2017rethinkinggeneralization,belkin2018overfittingperfectfitting}.
This has led to efforts towards understanding the nature of overfitting: how benign or catastrophic it is, and what determines this behavior, in different settings and using different models. 

Although interest in benign overfitting stems from the empirical success of interpolating large neural networks, theoretical study so far has been mostly limited to linear and kernel methods, or at least to settings where the data is linearly interpolatable, with very high (tending to infinity) data dimension\footnote{Minimum $\ell_2$ norm linear prediction (aka ridgeless regression) with noisy labels and (sub-)Gaussian features has been studied extensively 
\citep[e.g.][]{hastie2020surprises,belkin2020two,bartlett2020benignpnas,muthukumar2020harmless,negrea2020defense,chinot2020robustness,koehler2021uniform,wu2020optimal,tsigler2020benign,zhou2022non,wang2022tight,chatterji2021interplay,bartlett2021failures,shamir2022implicit,ghosh2022universal,chatterji2020linearnoise,wang2021binary,cao2021benign,muthukumar2021classification,montanari2020maxmarginasymptotics,liang2021interpolating,thrampoulidis2020theoretical,wang2021benignmulticlass,donhauser2022fastrates,frei2023benign}, and noisy minimum $\ell_1$ linear prediction (aka Basis Persuit) has also been considered \citep[e.g.][]{ju2020overfitting,koehler2021uniform,wang2022tight}.  Either way, these analyses are all in the high dimensional setting, with dimension going to infinity, since to allow for interpolation the dimension must be high, higher than the number of samples. Kernel methods amount to a minimum $\ell_2$ norm linear prediction, with very non-Gaussian features.  But existing analyses of interpolation learning with kernel methods rely on ``Gaussian Universality'': either assuming as an ansatz the behavior is as for Gaussian features \citep{mallinar2022benign} or establishing this rigorously in certain high dimensional scalings \citep{hastie2019surprises,misiakiewicz2022spectrum,mei2022generalization}.
In particular, such analyses are only valid when the input dimension goes to infinity (though possibly slower than the number of samples) and not for fixed low or moderate dimensions.
\citet{frei2022benign,frei2023benign,cao2022benign,kou2023benign} study interpolation learning with neural networks, but only with high input dimension and when the data is interpolatable also with a linear predictor---in these cases, although non-linear neural networks are used, the results show they behave similarly to linear predictors. 
\citet{manoj2023interpolation} take the other extreme and study interpolation learning with ``short programs'', which are certainly non-linear, but this is an abstract model that does not directly capture learning with neural networks.}.  But what about noisy interpolation learning in low (or moderate) dimensions, using neural networks?

\citet{mallinar2022benign} conducted simulations with neural networks and observed  ``tempered'' overfitting:  the asymptotic risk does not approach the Bayes-optimal risk (there is no consistency), but neither does it diverge to infinity catastrophically.  Such ``tempered'' behavior is well understood for $1$-nearest neighbor, where the asymptotic risk is roughly twice the Bayes risk \citep{cover1967nearest}, and \citeauthor{mallinar2022benign} heuristically explain it also for some kernel methods. However, we do not have a satisfying nor rigorous understanding of such behavior in neural networks, nor a more quantitative or precise understanding of just how bad the risk might be when interpolating noisy data using a neural net.

In this paper, we begin rigorously studying the effect of overfitting with neural networks in low dimensions, where the data is {\em not} linearly interpolatable, but a large enough network can of course always interpolate.  Specifically, we study interpolation learning of univariate data (i.e.~in one dimension) using a two-layer ReLU network (with a skip connection), which is a predictor $f_{\theta,a_0,b_0}:\IR \rightarrow \IR$ given by:
\begin{equation}\label{eq:net}
f_{\theta,a_0,b_0}(x) = \sum_{j=1}^m a_j (w_j x + b_j)_+ +  a_0 x + b_0~,
\end{equation}
where $\theta \in \reals^{3m}$ denotes the weights (parameters) $\{a_j,w_j,b_j\}_{j=1}^m$.  To allow for interpolation we do not limit the width $m$, and learn by minimizing the norm of the weights \citep{savarese2019infinite,ergen2021convex,hanin2021ridgeless,debarre2022sparsest,boursier2023penalising}:
\begin{equation} \label{eq:main_problem}
    \hat{f}_S = \arg\min_{f_{\theta,a_0,b_0}} \norm{\theta}^2 \;\text{ s.t. } \; \forall i \in [n],\; f_{\theta,a_0,b_0}(x_i)=y_i~\; \textrm{where $S=\{(x_1,y_1),\ldots,(x_n,y_n)\}$}.
\end{equation}

Following \citet{boursier2023penalising} we allow an unregularized skip-connection in \eqref{eq:net}, where the weights $a_0,b_0$ of this skip connection are not included in the norm $\norm{\theta}$ in \eqref{eq:main_problem}. This skip connection avoids some complications and allows better characterizing $\hat{f}_S$ but should not meaningfully change the behavior (see Section~\ref{sec:review_min_norm_nets}).

\paragraph{Why min norm?} Using unbounded size minimum weight-norm networks is natural for interpolation learning.  It parallels the study of minimum norm high (even infinite) dimension linear predictors.  For interpolation, we must allow the number of parameters to increase as the sample size increases.  But to have any hope of generalization, we must choose among the infinitely many zero training error networks somehow, and it seems that some sort of explicit or implicit low norm bias is the driving force in learning with large overparametrized neural networks \citep{neyshabur2014search}.  Seeking minimum $\ell_2$ norm weights is natural, e.g.~as a result of small weight decay.  Even without explicit weight decay, optimizing using gradient descent is also related to an implicit bias toward low $\ell_2$ norm: this can be made precise for linear models and for classification with ReLU networks \citep{chizat2020implicit,safran2022effective}. For regression with ReLU networks, as we study here, gradient descent does not exactly minimize the $\ell_2$ norm, but the implicit bias is probably still related, and studying \eqref{eq:main_problem} is a good starting point also for understanding the behavior of networks learned via gradient descent.

\paragraph{Noisy interpolation learning.} We consider a noisy distribution $\dist$ over $[0,1] \times \IR$:
\begin{equation} \label{eq:define_D}
    x \sim \unif([0,1])\;\;\;\; \text{ and }\;\;\;\; y=f^*(x)+\eps \; \text{ with } \eps \textrm{ independent of }\, x,
\end{equation}
where $x$ is uniform for simplicity and concreteness, the noise $\eps$ follows some arbitrary (non-zero) distribution, and learning is based on an i.i.d.~training set $S\sim\dist^n$.  Since the noise is non-zero, the ``ground truth'' predictor $f^*$ has non-zero training error, seeking a training error much smaller than that of $f^*$ would be overfitting (fitting the noise) and necessarily cause the complexity (e.g.~norm) of the learned predictor to explode.  The ``right'' thing to do is to balance between the training error and the complexity $\norm{\theta}$.  Indeed, under mild assumptions, this balanced approach leads to asymptotic consistency, with $\hat{f}_S \xrightarrow{n\rightarrow\infty} f^* $ and the asymptotic population risk of $\hat{f}_S$ converging to the Bayes risk.  But what happens when we overfit and use the interpolating learning rule \eqref{eq:main_problem}?

 % Figure environment removed

\paragraph{Linear Splines.} At first glance, we might be tempted to think that two-layer ReLUs behave like linear splines (see Figure~\ref{fig:min-norm-illustration1}). Indeed, if minimizing the norm of weights $w_i$ and $a_i$ but {\em not} the biases $b_i$ in \eqref{eq:main_problem}, linear splines are always a valid minimizer \citep{savarese2019infinite,ergen2021convex}. As the number of noisy training points increases, linear splines ``zig-zag'' with tighter ``zigs'' but non-vanishing ``amplitude'' around $f^*$, resulting in an interpolator which roughly behaves like $f^*$ plus some added non-vanishing ``noise''.  This does not lead to consistency, but is similar to a nearest-neighbor predictor (each prediction is a weighted average of two neighbors).  Indeed, in Theorem~\ref{thm:linear-splines} of Section~\ref{sec:linear-spline}, we show that linear splines exhibit ``tempered'' behavior, with asymptotic risk proportional to the noise.

\paragraph{From Splines to Min-Norm ReLU Nets.} But it turns out minimum norm ReLU networks, although piecewise linear, are not quite linear splines: roughly speaking, and as shown in Figure~\ref{fig:min-norm-illustration1}, they are more conservative in the number of linear ``pieces''. Because of this, in convex (conversely, concave) regions of the linear spline, minimum norm ReLU nets ``overshoot'' the linear spline in order to avoid breaking linear pieces.  This could create additional ``spikes'', extending above and below the data points (see Figures~\ref{fig:min-norm-illustration1} and~\ref{fig:min-norm-illustration2}) and thus potentially increasing the error. How bad is the effect of such spikes on the population risk?

\paragraph{Effect of Overfitting on $L_p$ Risk.} It turns out the answer is subtle and depends on how we measure the error. For a function $f:\IR \rightarrow\IR$, we measure its $L_p$ population error and the reconstruction error respectively as
$$\mathcal{L}_p(f) := \Exp_{(x,y) \sim \dist} [|f(x)-y|^p] \quad \text{ and } \quad \mathcal{R}_p(f) := \Exp_{x \sim \unif([0,1])} [|f(x)-f^*(x)|^p].$$
We show in Theorems~\ref{thm:tempered-for-ell_1} and~\ref{thm:tempered-for-ell_1-lower_bound} of Section~\ref{sec:tempered-L1} that for $1 \leq p < 2$, 
\begin{equation}\label{eq:intro_main}
    \mathcal{L}_p(\hat{f}_S)\xrightarrow{n\rightarrow\infty} \Theta \left( \frac{1}{(2-p)_+} \right) \, \mathcal{L}_p(f^*).
\end{equation}
This is an upper bound for any Lipschitz target $f^*$ and any noise distribution, and it is matched by a lower bound for Gaussian noise.  That is, for abs-loss ($L_1$ risk), as well as any $L_p$ risk for $1\leq p<2$, overfitting is {\bf tempered}. But this tempered behavior explodes as $p \rightarrow 2$, and we show in Theorem~\ref{thm:ell2-catastrophic} of Section~\ref{sec:catastrophic} that for any $p\geq 2$, including for the square loss ($p=2$), as soon as there is some non-zero noise, $\mathcal{L}_p(\hat{f}_S)\xrightarrow{n\rightarrow\infty} \infty$ and overfitting is {\bf catastrophic}.  

\paragraph{Convergence vs. Expectation.} The behavior is even more subtle, in that even for $1\leq p<2$, although the risk $\mathcal{L}_p(\hat{f}_S)$ converges in probability to a tempered behavior as in \eqref{eq:intro_main}, its {\em expectation} is infinite: $\Exp_S[\mathcal{L}_p(\hat{f}_S)]=\infty$.  Note that in studying tempered overfitting, \citet{mallinar2022benign} focused on this expectation, and so would have categorized the behavior as ``catastrophic'' even for $p=1$, emphasizing the need for more careful consideration of the effect of overfitting.  We would of course not get such strange behavior with the traditional non-overfitting approach of balancing training error and norm, which yields risk converging almost surely, with finite expectation converging to the optimal risk and vanishing variances.

\paragraph{I.I.D.~Samples vs. Samples on a Grid.} The catastrophic effect of interpolation on the $L_p$ risk with $p \geq 2$ is a result of the effect of fluctuations in the spacing of the training points.  Large, catastrophic, spikes are formed by training points extremely close to their neighbors but with different labels (see Figures~\ref{fig:min-norm-illustration2} and~\ref{fig:bad-case-kink-main-text}). To help understand this, in Section~\ref{sec:grid} we study a ``fixed design'' variant of the problem, where the training inputs lie on a uniform grid, $x_i=i/n$, and responses follow the random model $y_i=f^*(x_i)+\epsilon_i$. In this case, interpolation is always tempered, with $\mathcal{L}_p(\hat{f}_S)\xrightarrow{n\rightarrow\infty} \Theta(  \mathcal{L}_p(f^*) )$ for all constant $p\geq 1$ (Theorem~\ref{thm:grid} of Section~\ref{sec:grid}).

\paragraph{Summary and Contribution.} Our work is the first step toward studying noisy interpolation learning on non-linearly-interpolatable data in actual neural networks, where the capacity of the model comes from the size of the network and not the dimension of the data.  We study the univariate case.  Univariate regression with min-norm ReLU networks is a rich model in its own right \citep[e.g.][]{savarese2019infinite,ergen2021convex,hanin2021ridgeless,debarre2022sparsest,boursier2023penalising,williams2019gradient,mulayoff2021implicit,safran2022effective} and a starting point for further investigation.  We see that even for univariate data, the overfitting behavior is subtle and requires careful and detailed analysis, and especially for the universally studied squared loss, does not fit the ``tempered overfitting'' predictions of e.g.~\citet{mallinar2022benign}.

 % Figure environment removed

\section{Review: Min-Norm ReLU Networks} \label{sec:review_min_norm_nets}

Minimum-norm unbounded-width univariate two-layer ReLU networks have been extensively studied in recent years, starting with \citet{savarese2019infinite}, with the exact formulation \eqref{eq:main_problem} incorporating a skip connection due to \citet{boursier2023penalising}.   \citeauthor{boursier2023penalising}, following prior work, establish that a minimum of \eqref{eq:main_problem} exists, with a finite number of units, and that it is also unique.  

The problem \eqref{eq:main_problem} is also equivalent to minimizing the ``representation cost'' 
\[
    R(f) = \int_{\IR} \sqrt{1+x^2}|f''(x)| dx 
\]
over all interpolators $f$, although we will not use this characterization explicitly in our analysis. Compared to \citet{savarese2019infinite}, where the representation cost is given by $\max\{\int |f''(x)| dx, |f'(-\infty)+f'(+\infty)|\}$, the weighting $\sqrt{1+x^2}$ is due to penalizing the biases $b_i$.  More significantly, the skip connection in \eqref{eq:net} avoids the ``fallback'' terms of $|f'(-\infty)+f'(+\infty)|$, which only kick-in in extreme cases (very few points or an extreme slope).  This simplified analysis and presentation while rarely affecting the solution.  

\citeauthor{boursier2023penalising} provide the following characterization of the minimizer\footnote{
If the biases $b_i$ are {\em not} included in the norm $\norm{\theta}$ in \eqref{eq:main_problem}, and this norm is replaced with $\sum_i(a_i^2+w_i^2)$, the modified problem admits multiple non-unique minimizers, including a linear spline (with modified behavior past the extreme points) \citep{savarese2019infinite}.  This set of minimizers was characterized by \citet{hanin2021ridgeless}.  Interestingly, the minimizer $\hat{f}_S$ of \eqref{eq:main_problem} (when the biases are included in the norm) is also a minimizer of the modified problem (without including the biases).  All our results apply also to the setting without penalizing the biases in the following sense: the upper bounds are valid for all minimizers, while some minimizer, namely $\hat{f}_S$ that we study, exhibits the lower bound behavior.} $\hat{f}_S$ of \eqref{eq:main_problem}, which we will rely on heavily:
\begin{lemma}[\citet{boursier2023penalising}]\label{lem:char}
    For $0 \leq x_1<x_2<\cdots<x_n$, the problem in \eqref{eq:main_problem} admits a unique minimizer of the form:
    \begin{equation} \label{eq:min-norm_interpolator}
        \hat{f}_S(x) = ax + b + \sum_{i=1}^{n-1} a_i (x - \tau_i)_+~,
    \end{equation}
    where $\tau_i \in [x_i,x_{i+1})$ for every $i \in [n-1]$. 
\end{lemma}
As in the above characterization, it is very convenient to take the training points to be sorted.  Since the learned network $\hat{f}_S$ does not depend on the order of the points, we can always ``sort'' the points without changing anything.  And so, throughout the paper, we will always take the points to be sorted (formally, the results apply to i.i.d.~points, and the analysis is done after sorting these points).  


\section{Warm up: tempered overfitting in linear-spline interpolation} \label{sec:linear-spline}

We start by analyzing tempered overfitting for linear-spline interpolation. Namely, we consider the piecewise-linear function obtained by connecting each pair of consecutive points in the dataset $S \sim \dist^n$ (see Figures~\ref{fig:min-norm-illustration1} and \ref{fig:splines-main_text} left) and analyze its test performance. The linear-spline interpolator (after extending linear pieces beyond the endpoints, as we will discuss in Section~\ref{sec:delicate}),
is a minimizer of the network's norms in a setting where the bias terms are not penalized, albeit not a unique minimizer \citep{savarese2019infinite,ergen2021convex,hanin2021ridgeless}. In our setting, the linear-spline interpolator does not minimize the network's norms, but analyzing its test performance is a useful first step toward understanding min-norm interpolation.

% Figure environment removed

Given a dataset $S = \{(x_i,y_i)\}_{i=1}^n$, let $g_i:\IR \rightarrow \IR$ be the affine function joining the points $(x_{i},y_{i})$ and $(x_{i+1},y_{i+1})$. Thus, $g_i$ is the straight line joining the endpoints of the $i$-th interval. Then, the linear spline interpolator $\hat{g}_S: [0,1] \rightarrow \IR$ is given by
\begin{equation}\label{eq:linear-spline}
   \hat{g}_S(x) := y_1 \cdot \mathbf{1}\{ x < x_1\} + y_n \cdot \mathbf{1}\{x \geq x_{n}\}+\sum_{i=1}^{n-1} g_i(x)\cdot \mathbf{1}\{x\in [x_{i},x_{i+1})\}. 
\end{equation}
Note that in the intervals $[0,x_1]$ and $[x_n,1]$ the linear-spline $\hat{g}_S$ is defined to be constants that correspond to labels $y_1$ and $y_n$ respectively. The following theorem characterizes the asymptotic behavior of $\mathcal{L}_p(\hat{g}_S)$ for every $p \geq 1$:

\begin{theorem}\label{thm:linear-splines}
    Let $f^*$ be any Lipschitz function and $\dist$ be the distribution from \eqref{eq:define_D}. Let $S \sim \dist^{n}$, and $\hat{g}_S$ be the linear-spline interpolator (\eqref{eq:linear-spline}) w.r.t. the dataset $S$. Then, for any $p \geq 1$ there is a constant $C_p$ such that       
    \begin{equation*} \label{eq:spline-ub}
        \lim_{n \rightarrow \infty} \Pr_S  \left[\mathcal{R}_p(\hat{g}_S) \leq C_p\, \mathcal{L}_p(f^*) \right] = 1 \quad \text{ and }  \quad  \lim_{n \rightarrow \infty} \Pr_S  \left[\mathcal{L}_p(\hat{g}_S) \leq C_p\, \mathcal{L}_p(f^*) \right] = 1.
    \end{equation*}
\end{theorem}

The theorem shows that the linear-spline interpolator exhibits tempered behavior, namely, w.h.p. over $S$ the interpolator $\hat{g}_S$ performs like the predictor $f^*$, up to a constant factor. 

To understand why Theorem~\ref{thm:linear-splines} holds, note that for all $i \in [n-1]$ and $x \in [x_i,x_{i+1}]$ the linear-spline interpolator satisfies $\hat{g}_S(x) \in [\min\{y_i,y_{i+1}\}, \max\{y_i,y_{i+1}\}]$. Moreover, we have for all $i \in [n]$ that $|y_i-f^*(x_i)| = |\epsilon_i|$, where $\epsilon_i$ is the random noise. Using these facts, it is not hard to bound the expected population loss of $\hat{g}_S$ in each interval $[x_i,x_{i+1}]$, and by using the law of large numbers it is also possible to bound the probability (over $S$) that the loss in the 
domain $[0,1]$ is large. Thus, we can bound the $L_p$ loss both in expectation and in probability.

\subsection{Delicate behavior of linear splines} \label{sec:delicate}

We now consider the following variant of the linear-spline interpolator:
\begin{equation*}
   \hat{h}_S(x) := g_1(x) \cdot \mathbf{1}\{ x < x_1\} + g_{n-1}(x) \cdot \mathbf{1}\{x > x_{n}\}+ \hat{g}_S(x) \cdot \mathbf{1}\{x\in [x_{1},x_{n}]\}. 
\end{equation*}
Thus, $\hat{h}_S$ is similar to $\hat{g}_S$ in the interval $[x_1,x_n]$, but it extends the linear pieces $g_1$ and $g_{n-1}$ beyond the endpoints $x_1$ and $x_n$ (respectively), as illustrated in Figure~\ref{fig:splines-main_text} (right).

As we will now discuss, the interpolator $\hat{h}_S$ still exhibits tempered behavior in probability, similarly to $\hat{g}_S$. However, perhaps surprisingly, $\hat{h}_S$ is not tempered in expectation. This delicate behavior of the linear-spline interpolator is important since in the next section we will show that the min-norm interpolator has a similar behavior to $\hat{h}_S$ in the intervals $[0,x_1],[x_n,1]$, and as a consequence, it is tempered with high probability but not in expectation.

Analyzing the population loss of $\hat{h}_S$ in the intervals $[0,x_1],[x_n,1]$ requires considering the distribution of the spacings between data points. Let $\ell_0,\ldots,\ell_n$ be such that 
\begin{equation} \label{eq:define_ell}
    \forall i \in [n-1]\;\; \ell_i = x_{i+1}-x_i, \;\;\;\; \ell_0 = x_1, \;\;\;\; \ell_n = 1-x_n~.
\end{equation}
Prior works \citep{alagar1976distribution,pinelis2019order} established that  
\begin{equation} \label{eq:ell_distribution}
    \left( \ell_0,\dots,\ell_n \right) \sim  \left( \frac{X_0}{X}, \dots, \frac{X_n}{X} \right), \text{ where } X_0,\dots,X_n \iid \text{Exp}(1), \text{ and } X:=\sum_{i=0}^{n} X_i~.
\end{equation}
For simplicity, assume that $f^* \equiv 0$ and consider the $L_1$ loss.
Since in the interval $[0,x_1]$ the interpolator $\hat{h}_S$ is defined by extending the line connecting $(x_1,y_1)$ and $(x_2,y_2)$, then it has slope of $\Theta\left(\frac{1}{\ell_1}\right)$, and hence the $L_1$ loss of $\hat{h}_S$ in $[0,x_1]$ is 
\[
    \Theta\left(\frac{\ell_0^2}{\ell_1}\right)
    = \Theta\left( \frac{X_0^2}{X \cdot X_1} \right)~, 
\]
as can be seen in Figure~\ref{fig:splines-main_text} (right). Since $X_1 \sim \text{Exp}(1)$, then $\Exp \left[\frac{1}{X_1} \right] = \infty$, and as a consequence the expected $L_1$ loss in $[0,x_1]$ is infinite. A similar argument also holds for the interval $[x_n,1]$. Thus, we get that $\Exp_S  \left[\mathcal{L}_p(\hat{h}_S)\right] = \infty$. However, with high probability the lengths $\ell_1$ and $\ell_{n-1}$ will not be too short, and therefore the loss in the intervals $[0,x_1]$ and $[x_n,1]$ will be bounded, which implies tempered overfitting with high probability. 

\section{Min-norm interpolation with random data} \label{sec:main_results} 

In this section, we study the performance of the min-norm interpolator with random data. We first present some important properties of the min-norm interpolator in Section~\ref{sec:characterizing}. In Sections~\ref{sec:tempered-L1} and~\ref{sec:catastrophic} we use this characterization to study its performance.

\subsection{Characterizing the min-norm interpolator} \label{sec:characterizing}

Our goal is to give a characterization of the min-norm interpolator $\hat{f}_S(x)$ (\eqref{eq:min-norm_interpolator}), in terms of linear splines as defined in \eqref{eq:linear-spline}. Recall the definition of affine functions $g_1(x),\dots, g_{n-1}(x)$, which are piece-wise affine functions joining consecutive points. Let $\delta_i$ be the slope of the line $g_i(x)$, i.e. $\delta_i=g_i'(x)$. We denote $\delta_0:=\delta_1$ and $\delta_n:=\delta_{n-1}$. Then, we can define the sign of the curvature of the linear spline $\hat{g}_S(x)$ at each point. 
\begin{definition}\label{def:discrete-curvature}
    For any $i\in [n]$,
      $$ \curv(x_i) = \begin{cases}
  +1  & \delta_i > \delta_{i-1} \\
  0 & \delta_i = \delta_{i+1} \\
  -1 & \delta_i < \delta_{i-1}
\end{cases}$$
\end{definition}

% Figure environment removed
Based on the curvature, the following lemma geometrically characterizes $\hat{f}_S$ in any interval $[x_i,x_{i+1})$, in terms of the linear pieces $g_{i-1},g_i,g_{i+1}$.

\begin{lemma}\label{lem:complete-description-property-main_text}
The function $\hat{f}_S$ can be characterized as follows:
        \begin{itemize}
        \item $\hat{f}_S(x) =g_1(x)$ for $x\in (-\infty, x_2)$;
        \item $\hat{f}_S(x) =g_{n-1}(x)$ for $x \in [x_{n-1},\infty)$;
        \item In each interval $[x_i,x_{i+1})$ for $i \in \{2, \dots n-2\}$, 
        \begin{enumerate}
            \item If $\curv(x_i)=\curv(x_{i+1})=+1$ then
        $$ \max\{g_{i-1}(x), g_{i+1}(x)\} \leq  \hat{f}_S (x) \leq g_i(x);$$
        \item  If $\curv(x_i)=\curv(x_{i+1})=-1$ then
        $$  \min\{g_{i-1}(x), g_{i+1}(x)\} \geq  \hat{f}_S (x) \geq  g_i(x);$$ 
        \item Else, i.e. either $\curv(x_i)=0$ or $\curv(x_{i+1})=0$ or $\curv(x_{i}) \neq \curv(x_{i+1})$,
        $$\hat{f}_S(x)=g_i(x).$$
        \end{enumerate}
        \end{itemize}
\end{lemma}
The lemma implies that $\hat{f}_S$ coincides with $\hat{g}_S$ except in an interval $[x_i,x_{i+1})$ where the curvature of the two points are both $+1$ or $-1$ (see Figure~\ref{fig:geometric-illustration-of-triangle-main_text}). Intuitively, this property captures the worst-case effect of the spikes and will be crucial in showing the tempered behavior of $\hat{f}_S$ w.r.t. $L_p$ for $p\in [1,2)$. However, this still does not imply that such spikes are necessarily formed. 

To this end, \citet[Lemma 8]{boursier2023penalising} characterized the situation under which indeed these spikes are formed. Roughly speaking, if the sign of the curvature changes twice within three points, then we get a spike. Formally, we identify special points from left to right recursively where the sign of the curvature changes.
\begin{definition}\label{def:cruv-changed}
    We define $n_1:=1$. Having defined the location of the special points $n_1,\dots, n_{i-1}$, we recursively define 
    $$n_i= \min \{ j>n_{i-1} : \curv(x_{j}) \neq \curv (x_{n_i})\}.$$
    If there is no such $n_{i-1} < j \leq n$ where $\curv(x_{j}) \neq \curv (x_{n_i})$, then $n_{i-1}$ is the location of the last special point.
\end{definition}

\begin{lemma}[\citet{boursier2023penalising}]\label{lem:sparsity-main_text}
    For any $k \geq 1$, if $\delta_{n_k -1 }  \neq  \delta_{n_k}$ and $n_{k+1}= n_k + 2$, then $\hat{f}_S$ has exactly one kink between $(x_{{n_k}-1}, x_{n_{k+1}})$. Moreover, if $\curv(x_{n_k})=\curv(x_{n_k+1})=-1$ then $\hat{f}_S(x)=\min\{g_{n_k-1}(x), g_{n_k+1}(x)\}$ in $[x_{n_k},x_{n_k+1})$.
\end{lemma}

% Figure environment removed

This is a slight variation of \citep[Lemma 8]{boursier2023penalising}, which we reprove in the appendix for completeness. See Figure~\ref{fig:bad-case-kink-main-text} for an illustration of the above lemma.
To show the catastrophic behavior of $\hat{f}_S$ for $p \geq 2$, we will consider events under which such configurations of points are formed. This will result in spikes giving catastrophic behavior. 

\subsection{Tempered overfitting for \texorpdfstring{$L_p$}{TEXT} with \texorpdfstring{$p \in [1,2)$}{TEXT}}\label{sec:tempered-L1}

We now show the tempered behavior of the minimal norm interpolator w.r.t. $L_p$ losses for $p \in [1,2)$.
\begin{theorem} \label{thm:tempered-for-ell_1}
Let $f^*$ be a Lipschitz function and $\dist$ be the distribution from \eqref{eq:define_D}. Sample $S\sim \dist^n$, and let $\hat{f}_S$ be the min-norm interpolator (\eqref{eq:min-norm_interpolator}). Then, for some universal constant $C > 0$, for any $p \in [1,2)$ we have 
\[
  \lim_{n \rightarrow \infty} \Pr_S \left[\mathcal{R}_p(\hat{f}_S) \leq \frac{C}{2-p} \cdot \,  \mathcal{L}_p(f^*) \right] = 1 \quad \text{and} \quad   \lim_{n \rightarrow \infty} \Pr_S \left[\mathcal{L}_p(\hat{f}_S) \leq \frac{C}{2-p} \cdot \,  \mathcal{L}_p(f^*) \right] = 1~.
\]
\end{theorem}
The proof of Theorem~\ref{thm:tempered-for-ell_1} builds on Lemma~\ref{lem:complete-description-property-main_text}, which implies that in an interval $[x_i,x_{i+1})$, a spike in the interpolator $\hat{f}_S$ must be bounded within the triangle obtained from $g_{i-1},g_i,g_{i+1}$ (see Figure~\ref{fig:geometric-illustration-of-triangle-main_text}). 
The slopes of the affine functions $g_{i-1},g_{i+1}$ are roughly $\frac{1}{\ell_{i-1}},\frac{1}{\ell_{i+1}}$, where $\ell_j$ are the lengths as defined in \eqref{eq:define_ell}. Hence, the spike's height is proportional to $\frac{\ell_i}{\max\{\ell_{i-1},\ell_{i+1}\}}$. As a result, the $L_p$ loss in the interval $[x_i,x_{i+1}]$ is roughly 
\[
    \left(\frac{\ell_i}{\max\{\ell_{i-1},\ell_{i+1}\}} \right)^p \cdot \ell_i = \frac{\ell_i^{p+1}}{\max\{\ell_{i-1},\ell_{i+1}\}^p}~.
\]
Using the distribution of the $\ell_j$'s given in \eqref{eq:ell_distribution}, we can bound the expectation of this expression. Then, similarly to our discussion on linear splines in Section~\ref{sec:linear-spline}, in the range $[x_1,x_n]$ we can bound the $L_p$ loss both in expectation and in probability. In the intervals $[0,x_1]$ and $[x_n,1]$, the expected loss is infinite (similarly to the interpolator $\hat{h}_S$ from Section~\ref{sec:delicate}), and therefore we have 
\begin{equation} \label{eq:L1_infinite_exp}
    \Exp_S \left[\mathcal{L}_p(\hat{f}_S) \right] = \infty~.
\end{equation}
Still, we can get a high probability upper bound for the $L_p$ loss in the intervals $[0,x_1]$ and $[x_n,1]$. Thus, we get a bound on $L_p$ loss in the entire domain $[0,1]$ w.h.p.

We note that the definition of tempered overfitting in \citet{mallinar2022benign} considers only the expectation. Theorem~\ref{thm:tempered-for-ell_1} and \eqref{eq:L1_infinite_exp} imply that in our setting we have tempered behavior in probability but not in expectation, which demonstrates that tempered behavior is delicate.

We also show a lower bound for the population loss $L_p$ which matches the upper bound from Theorem~\ref{thm:tempered-for-ell_1} (up to a constant factor independent of $p$). The lower bound holds already for $f^* \equiv 0$ and Gaussian label noise.

\begin{theorem} \label{thm:tempered-for-ell_1-lower_bound}
Let $f^* \equiv 0$, consider label noise $\epsilon \sim \normal(0,\sigma^2)$ for some constant $\sigma>0$, and let
$\dist$ be the corresponding distribution from \eqref{eq:define_D}. Let $S\sim \dist^n$, and let $\hat{f}_S$ be the min-norm interpolator (\eqref{eq:min-norm_interpolator}). Then, for some universal constant $c > 0$, for any $p \in [1,2)$ we have 
\[
   \lim_{n \rightarrow \infty} \Pr_S \left[\mathcal{R}_p(\hat{f}_S) \geq \frac{c}{2-p} \cdot \mathcal{L}_p(f^*)  \right] = 1 \quad \text{and} \quad \lim_{n \rightarrow \infty} \Pr_S \left[\mathcal{L}_p(\hat{f}_S) \geq \frac{c}{2-p} \cdot \mathcal{L}_p(f^*)  \right] = 1~.
\]
\end{theorem}

The proof of the above lower bound follows similar arguments to the proof of catastrophic overfitting for $p \geq 2$, which we will discuss in the next section.

\subsection{Catastrophic overfitting for \texorpdfstring{$L_p$}{TEXT} with \texorpdfstring{$p \geq 2$}{TEXT}} \label{sec:catastrophic}

Next, we prove that for the $L_p$ loss with $p \geq 2$, the min-norm interpolator exhibits catastrophic overfitting. We prove this result already for $f^* \equiv 0$ and Gaussian label noise:

\begin{theorem}\label{thm:ell2-catastrophic}
    Let $f^* \equiv 0$, consider label noise $\epsilon \sim \normal(0,\sigma^2)$ for some constant $\sigma>0$, and let $\dist$ be the corresponding distribution from \eqref{eq:define_D}.
    Let $S \sim \dist^n$, and let $\hat{f}_S$ be the min-norm interpolator (\eqref{eq:min-norm_interpolator}).
    Then, for any $p \geq 2$ and $b>0$,
    \[
       \lim_{n \to \infty} \Pr_S\left[{\mathcal{R}_p(\hat{f}_S) > b} \right] = 1 \quad \text{and} \quad \lim_{n \to \infty} \Pr_S\left[{\mathcal{L}_p(\hat{f}_S) > b} \right] = 1~.
    \]
\end{theorem}

To obtain some intuition on this phenomenon, consider the first four samples $(x_1,y_1),\ldots,(x_4,y_4)$, and let $\ell_i$ be the lengths of the intervals as defined in \eqref{eq:define_ell}. We show that with constant probability, 
the configuration of the labels of these samples satisfies certain properties, which are illustrated in Figure~\ref{fig:bad-case-kink-main-text}.
In this case, Lemma~\ref{lem:sparsity-main_text} implies that in the interval $[x_2,x_3]$ the interpolator $\hat{f}_S$ is equal to $\min\{g_1(x),g_3(x)\}$, where $g_1$ (respectively, $g_3$) is the affine function that connects $x_1,x_2$ (respectively, $x_3,x_4$). Now, as can be seen in the figure, in this ``unfortunate configuration'' the interpolator $\hat{f}_S$ spikes above $f^* \equiv 0$ in the interval $[x_2,x_3]$, and the spike's height is proportional to $\frac{\ell_2}{\max\{\ell_1,\ell_3\}}$. As a result, the $L_p$ loss in the interval $[x_2,x_3]$ is roughly $\frac{\ell_2^{p+1}}{\max\{\ell_1,\ell_3\}^p}$. Using \eqref{eq:ell_distribution}, we can show that 
$\Exp_S \left[ \frac{\ell_2^{p+1}}{\max\{\ell_1,\ell_3\}^p} \right] = \infty$ for any $p \geq 2$.

Now, we divide the $n$ samples in $S$ into $\Theta(n)$ disjoint subsets of $6$ consecutive points. For each subset we consider the event that the labels are such that the $4$ middle points exhibit an ``unfortunate configuration'' as described above. These events are independent and each occurs with constant probability. Therefore, there will be $\Theta(n)$ subsets with such unfortunate configurations of labels with high probability. The expectation of the population loss $L_p$ in each of these subsets is infinite. Using the fact that we have $\Theta(n)$ such subsets and the losses in these subsets are only mildly correlated, we are able to prove that $\hat{f}_S$ exhibits a catastrophic behavior also in probability.

We note that the proof of the Theorem~\ref{thm:tempered-for-ell_1-lower_bound} follows similar arguments, except that when $p<2$ the expectation of the $L_p$ loss in each subset with an ``unfortunate configuration'' is finite, and hence we get a finite lower bound.

\section{Min-norm interpolation with samples on the grid} \label{sec:grid}

In this section, we analyze the population loss of the min-norm interpolator, when the $n$ data-points in $S$ are uniformly spaced, instead of i.i.d. uniform sampling considered in the previous sections. Namely, consider
the training set $S=\{(x_i,y_i): i \in [n] \}$, where
\begin{equation} \label{eq:grid-data}
    x_i=\frac{i}{n} \hspace{2mm} \text{and} \hspace{2mm} y_i=f^*(x_i)+\eps_i \; 
    \text{ for i.i.d. noise } \; \eps_i~. 
\end{equation}
Note that the randomness in $S$ is only in the label noises $\epsilon_i$. 
It can be interpreted as a \emph{nonadaptive active learning} setting, where the learner can actively choose the training points, and then observe noisy measurements at these points, and the query points are selected on an equally spaced grid.

We show that in this situation the min-norm interpolator exhibits tempered overfitting with respect to any ${L}_p$ loss:
\begin{theorem}\label{thm:grid}
 Let $f^*$ be any Lipschitz function. For the size-$n$ dataset $S$ given by \eqref{eq:grid-data}, let $\hat{f}_S$ be the min-norm interpolator (\eqref{eq:min-norm_interpolator}). Then for any $p\geq 1$, there is a constant $C_p$ such that
 \[
    \lim_{n \rightarrow \infty} \Pr_S \left[\mathcal{R}_p(\hat{f}_S) \leq C_p \, \mathcal{L}_p(f^*)\right] = 1 \quad \text{and} \quad \lim_{n \rightarrow \infty} \Pr_S \left[\mathcal{L}_p(\hat{f}_S) \leq C_p \, \mathcal{L}_p(f^*)\right] = 1~.
 \]
\end{theorem}

An intuitive explanation is as follows. Since the points are uniformly spaced, whenever spikes are formed, they can at most reach double the height without the spikes. Thus, the population loss of $\hat{f}_S(x)$ becomes worse but only by a constant factor. 
We remark that in this setting the min-norm interpolator exhibits tempered overfitting both in probability (as stated in Theorem~\ref{thm:grid}) and in expectation.

From Theorem~\ref{thm:grid} we conclude that the catastrophic behavior for $L_p$ with $p \geq 2$ shown in Theorem~\ref{thm:ell2-catastrophic}
stems from the non-uniformity in the lengths of the intervals $[x_i,x_{i+1}]$, which occurs when the $x_i$'s are drawn at random. 

Finally, we note that previous works considered benign overfitting with data on the grid as a simplified setting, which may help in understanding more general situations \citep{beaglehole2022kernel,lai2023generalization}. Our results imply that this simplification might change the behavior of the interpolator significantly.

\subsection*{Acknowledgements}

This research was done as part of the NSF-Simons Sponsored Collaboration on the Theoretical Foundations of Deep Learning.

\bibliography{NeurIPS/bib}
\bibliographystyle{abbrvnat}

\clearpage

\appendix
\input{NeurIPS/linear_spline_proof}
\input{NeurIPS/properties}
\input{NeurIPS/l1-tempered-proof}
\input{NeurIPS/ell2_proof}
\input{NeurIPS/grid_proof}
\end{document}