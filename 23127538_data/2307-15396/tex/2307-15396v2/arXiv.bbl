\begin{thebibliography}{53}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alagar(1976)]{alagar1976distribution}
V.~S. Alagar.
\newblock The distribution of the distance between random points.
\newblock \emph{Journal of Applied Probability}, 13\penalty0 (3):\penalty0
  558--566, 1976.

\bibitem[Bartlett and Long(2021)]{bartlett2021failures}
P.~L. Bartlett and P.~M. Long.
\newblock Failures of model-dependent generalization bounds for least-norm
  interpolation.
\newblock \emph{The Journal of Machine Learning Research}, 22\penalty0
  (1):\penalty0 9297--9311, 2021.

\bibitem[Bartlett et~al.(2020)Bartlett, Long, Lugosi, and
  Tsigler]{bartlett2020benignpnas}
P.~L. Bartlett, P.~M. Long, G.~Lugosi, and A.~Tsigler.
\newblock Benign overfitting in linear regression.
\newblock \emph{Proceedings of the National Academy of Sciences}, 117\penalty0
  (48):\penalty0 30063--30070, 2020.

\bibitem[Beaglehole et~al.(2022)Beaglehole, Belkin, and
  Pandit]{beaglehole2022kernel}
D.~Beaglehole, M.~Belkin, and P.~Pandit.
\newblock Kernel ridgeless regression is inconsistent for low dimensions.
\newblock \emph{arXiv preprint arXiv:2205.13525}, 2022.

\bibitem[Belkin et~al.(2018)Belkin, Hsu, and
  Mitra]{belkin2018overfittingperfectfitting}
M.~Belkin, D.~J. Hsu, and P.~Mitra.
\newblock Overfitting or perfect fitting? {R}isk bounds for classification and
  regression rules that interpolate.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2018.

\bibitem[Belkin et~al.(2020)Belkin, Hsu, and Xu]{belkin2020two}
M.~Belkin, D.~Hsu, and J.~Xu.
\newblock Two models of double descent for weak features.
\newblock \emph{SIAM Journal on Mathematics of Data Science}, 2\penalty0
  (4):\penalty0 1167--1180, 2020.

\bibitem[Boursier and Flammarion(2023)]{boursier2023penalising}
E.~Boursier and N.~Flammarion.
\newblock Penalising the biases in norm regularisation enforces sparsity.
\newblock \emph{arXiv preprint arXiv:2303.01353}, 2023.

\bibitem[Cao et~al.(2021)Cao, Gu, and Belkin]{cao2021benign}
Y.~Cao, Q.~Gu, and M.~Belkin.
\newblock Risk bounds for over-parameterized maximum margin classification on
  sub-gaussian mixtures.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2021.

\bibitem[Cao et~al.(2022)Cao, Chen, Belkin, and Gu]{cao2022benign}
Y.~Cao, Z.~Chen, M.~Belkin, and Q.~Gu.
\newblock Benign overfitting in two-layer convolutional neural networks.
\newblock \emph{arXiv preprint arXiv:2202.06526}, 2022.

\bibitem[Chatterji and Long(2021)]{chatterji2020linearnoise}
N.~S. Chatterji and P.~M. Long.
\newblock Finite-sample analysis of interpolating linear classifiers in the
  overparameterized regime.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0
  (129):\penalty0 1--30, 2021.

\bibitem[Chatterji et~al.(2021)Chatterji, Long, and
  Bartlett]{chatterji2021interplay}
N.~S. Chatterji, P.~M. Long, and P.~L. Bartlett.
\newblock The interplay between implicit bias and benign overfitting in
  two-layer linear networks.
\newblock \emph{arXiv preprint arXiv:2108.11489}, 2021.

\bibitem[Chinot and Lerasle(2020)]{chinot2020robustness}
G.~Chinot and M.~Lerasle.
\newblock On the robustness of the minimum $\ell_2$ interpolator.
\newblock \emph{arXiv preprint arXiv:2003.05838}, 2020.

\bibitem[Chizat and Bach(2020)]{chizat2020implicit}
L.~Chizat and F.~Bach.
\newblock Implicit bias of gradient descent for wide two-layer neural networks
  trained with the logistic loss.
\newblock In \emph{Conference on Learning Theory (COLT)}, 2020.

\bibitem[Cover and Hart(1967)]{cover1967nearest}
T.~Cover and P.~Hart.
\newblock Nearest neighbor pattern classification.
\newblock \emph{IEEE transactions on information theory}, 13\penalty0
  (1):\penalty0 21--27, 1967.

\bibitem[Debarre et~al.(2022)Debarre, Denoyelle, Unser, and
  Fageot]{debarre2022sparsest}
T.~Debarre, Q.~Denoyelle, M.~Unser, and J.~Fageot.
\newblock Sparsest piecewise-linear regression of one-dimensional data.
\newblock \emph{Journal of Computational and Applied Mathematics},
  406:\penalty0 114044, 2022.

\bibitem[Deming and Colcord(1935)]{deming1935minimum}
W.~E. Deming and C.~G. Colcord.
\newblock The minimum in the gamma function.
\newblock \emph{Nature}, 135\penalty0 (3422):\penalty0 917--917, 1935.

\bibitem[Donhauser et~al.(2022)Donhauser, Ruggeri, Stojanovic, and
  Yang]{donhauser2022fastrates}
K.~Donhauser, N.~Ruggeri, S.~Stojanovic, and F.~Yang.
\newblock Fast rates for noisy interpolation require rethinking the effect of
  inductive bias.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2022.

\bibitem[Ergen and Pilanci(2021)]{ergen2021convex}
T.~Ergen and M.~Pilanci.
\newblock Convex geometry and duality of over-parameterized neural networks.
\newblock \emph{Journal of machine learning research}, 2021.

\bibitem[Frei et~al.(2022)Frei, Chatterji, and Bartlett]{frei2022benign}
S.~Frei, N.~S. Chatterji, and P.~L. Bartlett.
\newblock Benign overfitting without linearity: Neural network classifiers
  trained by gradient descent for noisy linear data.
\newblock In \emph{Conference on Learning Theory (COLT)}, 2022.

\bibitem[Frei et~al.(2023)Frei, Vardi, Bartlett, and Srebro]{frei2023benign}
S.~Frei, G.~Vardi, P.~L. Bartlett, and N.~Srebro.
\newblock Benign overfitting in linear classifiers and leaky relu networks from
  kkt conditions for margin maximization.
\newblock \emph{arXiv preprint arXiv:2303.01462}, 2023.

\bibitem[Ghosh and Belkin(2022)]{ghosh2022universal}
N.~Ghosh and M.~Belkin.
\newblock A universal trade-off between the model size, test loss, and training
  loss of linear predictors.
\newblock \emph{arXiv preprint arXiv:2207.11621}, 2022.

\bibitem[Hanin(2021)]{hanin2021ridgeless}
B.~Hanin.
\newblock Ridgeless interpolation with shallow relu networks in $1 d $ is
  nearest neighbor curvature extrapolation and provably generalizes on
  lipschitz functions.
\newblock \emph{arXiv preprint arXiv:2109.12960}, 2021.

\bibitem[Hastie et~al.(2019)Hastie, Montanari, Rosset, and
  Tibshirani]{hastie2019surprises}
T.~Hastie, A.~Montanari, S.~Rosset, and R.~J. Tibshirani.
\newblock Surprises in high-dimensional ridgeless least squares interpolation.
\newblock \emph{arXiv preprint arXiv:1903.08560}, 2019.

\bibitem[Hastie et~al.(2020)Hastie, Montanari, Rosset, and
  Tibshirani]{hastie2020surprises}
T.~Hastie, A.~Montanari, S.~Rosset, and R.~J. Tibshirani.
\newblock Surprises in high-dimensional ridgeless least squares interpolation.
\newblock \emph{Preprint, arXiv:1903.08560}, 2020.

\bibitem[Ju et~al.(2020)Ju, Lin, and Liu]{ju2020overfitting}
P.~Ju, X.~Lin, and J.~Liu.
\newblock Overfitting can be harmless for basis pursuit, but only to a degree.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 7956--7967, 2020.

\bibitem[Koehler et~al.(2021)Koehler, Zhou, Sutherland, and
  Srebro]{koehler2021uniform}
F.~Koehler, L.~Zhou, D.~J. Sutherland, and N.~Srebro.
\newblock Uniform convergence of interpolators: Gaussian width, norm bounds,
  and benign overfitting.
\newblock \emph{arXiv preprint arXiv:2106.09276}, 2021.

\bibitem[Kornowski et~al.(2023)Kornowski, Yehudai, and
  Shamir]{kornowski2023tempered}
G.~Kornowski, G.~Yehudai, and O.~Shamir.
\newblock From tempered to benign overfitting in relu neural networks.
\newblock \emph{arXiv preprint arXiv:2305.15141}, 2023.

\bibitem[Kou et~al.(2023)Kou, Chen, Chen, and Gu]{kou2023benign}
Y.~Kou, Z.~Chen, Y.~Chen, and Q.~Gu.
\newblock Benign overfitting for two-layer relu networks.
\newblock \emph{arXiv preprint arXiv:2303.04145}, 2023.

\bibitem[Lai et~al.(2023)Lai, Xu, Chen, and Lin]{lai2023generalization}
J.~Lai, M.~Xu, R.~Chen, and Q.~Lin.
\newblock Generalization ability of wide neural networks on $\mathbb{R}$.
\newblock \emph{arXiv preprint arXiv:2302.05933}, 2023.

\bibitem[Liang and Recht(2021)]{liang2021interpolating}
T.~Liang and B.~Recht.
\newblock Interpolating classifiers make few mistakes.
\newblock \emph{arXiv preprint arXiv:2101.11815}, 2021.

\bibitem[Mallinar et~al.(2022)Mallinar, Simon, Abedsoltan, Pandit, Belkin, and
  Nakkiran]{mallinar2022benign}
N.~Mallinar, J.~B. Simon, A.~Abedsoltan, P.~Pandit, M.~Belkin, and P.~Nakkiran.
\newblock Benign, tempered, or catastrophic: A taxonomy of overfitting.
\newblock \emph{arXiv preprint arXiv:2207.06569}, 2022.

\bibitem[Manoj and Srebro(2023)]{manoj2023interpolation}
N.~S. Manoj and N.~Srebro.
\newblock Interpolation learning with minimum description length.
\newblock \emph{arXiv preprint arXiv:2302.07263}, 2023.

\bibitem[Mei and Montanari(2022)]{mei2022generalization}
S.~Mei and A.~Montanari.
\newblock The generalization error of random features regression: Precise
  asymptotics and the double descent curve.
\newblock \emph{Communications on Pure and Applied Mathematics}, 75\penalty0
  (4):\penalty0 667--766, 2022.

\bibitem[Misiakiewicz(2022)]{misiakiewicz2022spectrum}
T.~Misiakiewicz.
\newblock Spectrum of inner-product kernel matrices in the polynomial regime
  and multiple descent phenomenon in kernel ridge regression.
\newblock \emph{arXiv preprint arXiv:2204.10425}, 2022.

\bibitem[Montanari et~al.(2020)Montanari, Ruan, Sohn, and
  Yan]{montanari2020maxmarginasymptotics}
A.~Montanari, F.~Ruan, Y.~Sohn, and J.~Yan.
\newblock The generalization error of max-margin linear classifiers:
  High-dimensional asymptotics in the overparametrized regime.
\newblock \emph{Preprint, arXiv:1911.01544}, 2020.

\bibitem[Mulayoff et~al.(2021)Mulayoff, Michaeli, and
  Soudry]{mulayoff2021implicit}
R.~Mulayoff, T.~Michaeli, and D.~Soudry.
\newblock The implicit bias of minima stability: A view from function space.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 17749--17761, 2021.

\bibitem[Muthukumar et~al.(2020)Muthukumar, Vodrahalli, Subramanian, and
  Sahai]{muthukumar2020harmless}
V.~Muthukumar, K.~Vodrahalli, V.~Subramanian, and A.~Sahai.
\newblock Harmless interpolation of noisy data in regression.
\newblock \emph{IEEE Journal on Selected Areas in Information Theory}, 2020.

\bibitem[Muthukumar et~al.(2021)Muthukumar, Narang, Subramanian, Belkin, Hsu,
  and Sahai]{muthukumar2021classification}
V.~Muthukumar, A.~Narang, V.~Subramanian, M.~Belkin, D.~Hsu, and A.~Sahai.
\newblock Classification vs regression in overparameterized regimes: Does the
  loss function matter?
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0
  (222):\penalty0 1--69, 2021.

\bibitem[Negrea et~al.(2020)Negrea, Dziugaite, and Roy]{negrea2020defense}
J.~Negrea, G.~K. Dziugaite, and D.~Roy.
\newblock In defense of uniform convergence: Generalization via derandomization
  with an application to interpolating predictors.
\newblock In \emph{International Conference on Machine Learning}, pages
  7263--7272, 2020.

\bibitem[Neyshabur et~al.(2014)Neyshabur, Tomioka, and
  Srebro]{neyshabur2014search}
B.~Neyshabur, R.~Tomioka, and N.~Srebro.
\newblock In search of the real inductive bias: On the role of implicit
  regularization in deep learning.
\newblock \emph{Preprint, arXiv:1412.6614}, 2014.

\bibitem[Pinelis(2019)]{pinelis2019order}
I.~Pinelis.
\newblock Order statistics on the spacings between order statistics for the
  uniform distribution.
\newblock \emph{arXiv preprint arXiv:1909.06406}, 2019.

\bibitem[Safran et~al.(2022)Safran, Vardi, and Lee]{safran2022effective}
I.~Safran, G.~Vardi, and J.~D. Lee.
\newblock On the effective number of linear regions in shallow univariate relu
  networks: Convergence guarantees and implicit bias.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)},
  2022.

\bibitem[Savarese et~al.(2019)Savarese, Evron, Soudry, and
  Srebro]{savarese2019infinite}
P.~Savarese, I.~Evron, D.~Soudry, and N.~Srebro.
\newblock How do infinite width bounded norm networks look in function space?
\newblock In \emph{Conference on Learning Theory}, pages 2667--2690. PMLR,
  2019.

\bibitem[Shamir(2022)]{shamir2022implicit}
O.~Shamir.
\newblock The implicit bias of benign overfitting.
\newblock In \emph{Conference on Learning Theory}, pages 448--478. PMLR, 2022.

\bibitem[Thrampoulidis et~al.(2020)Thrampoulidis, Oymak, and
  Soltanolkotabi]{thrampoulidis2020theoretical}
C.~Thrampoulidis, S.~Oymak, and M.~Soltanolkotabi.
\newblock Theoretical insights into multiclass classification: A
  high-dimensional asymptotic view.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 8907--8920, 2020.

\bibitem[Tsigler and Bartlett(2020)]{tsigler2020benign}
A.~Tsigler and P.~L. Bartlett.
\newblock Benign overfitting in ridge regression.
\newblock \emph{Preprint, arXiv:2009.14286}, 2020.

\bibitem[Wang et~al.(2022)Wang, Donhauser, and Yang]{wang2022tight}
G.~Wang, K.~Donhauser, and F.~Yang.
\newblock Tight bounds for minimum l1-norm interpolation of noisy data.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics (AISTATS)}, 2022.

\bibitem[Wang and Thrampoulidis(2021)]{wang2021binary}
K.~Wang and C.~Thrampoulidis.
\newblock Binary classification of gaussian mixtures: Abundance of support
  vectors, benign overfitting and regularization.
\newblock \emph{Preprint, arXiv:2011.09148}, 2021.

\bibitem[Wang et~al.(2021)Wang, Muthukumar, and
  Thrampoulidis]{wang2021benignmulticlass}
K.~Wang, V.~Muthukumar, and C.~Thrampoulidis.
\newblock Benign overfitting in multiclass classification: All roads lead to
  interpolation.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2021.

\bibitem[Williams et~al.(2019)Williams, Trager, Panozzo, Silva, Zorin, and
  Bruna]{williams2019gradient}
F.~Williams, M.~Trager, D.~Panozzo, C.~Silva, D.~Zorin, and J.~Bruna.
\newblock Gradient dynamics of shallow univariate relu networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  8378--8387, 2019.

\bibitem[Wu and Xu(2020)]{wu2020optimal}
D.~Wu and J.~Xu.
\newblock On the optimal weighted $\ell_2$ regularization in overparameterized
  linear regression.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 10112--10123, 2020.

\bibitem[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2017rethinkinggeneralization}
C.~Zhang, S.~Bengio, M.~Hardt, B.~Recht, and O.~Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2017.

\bibitem[Zhou et~al.(2022)Zhou, Koehler, Sur, Sutherland, and
  Srebro]{zhou2022non}
L.~Zhou, F.~Koehler, P.~Sur, D.~J. Sutherland, and N.~Srebro.
\newblock A non-asymptotic moreau envelope theory for high-dimensional
  generalized linear models.
\newblock \emph{arXiv preprint arXiv:2210.12082}, 2022.

\end{thebibliography}
