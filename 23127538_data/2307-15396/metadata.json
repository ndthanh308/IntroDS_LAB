{
  "title": "Noisy Interpolation Learning with Shallow Univariate ReLU Networks",
  "authors": [
    "Nirmit Joshi",
    "Gal Vardi",
    "Nathan Srebro"
  ],
  "submission_date": "2023-07-28T08:41:12+00:00",
  "revised_dates": [
    "2023-08-01T12:56:29+00:00",
    "2024-03-21T23:38:52+00:00"
  ],
  "abstract": "Understanding how overparameterized neural networks generalize despite perfect interpolation of noisy training data is a fundamental question. Mallinar et. al. 2022 noted that neural networks seem to often exhibit ``tempered overfitting'', wherein the population risk does not converge to the Bayes optimal error, but neither does it approach infinity, yielding non-trivial generalization. However, this has not been studied rigorously. We provide the first rigorous analysis of the overfitting behavior of regression with minimum norm ($\\ell_2$ of weights), focusing on univariate two-layer ReLU networks. We show overfitting is tempered (with high probability) when measured with respect to the $L_1$ loss, but also show that the situation is more complex than suggested by Mallinar et. al., and overfitting is catastrophic with respect to the $L_2$ loss, or when taking an expectation over the training set.",
  "categories": [
    "cs.LG"
  ],
  "primary_category": "cs.LG",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.15396",
  "pdf_url": "https://arxiv.org/pdf/2307.15396v3",
  "comment": "To appear at ICLR 2024. Updated version with minor changes in the presentation",
  "num_versions": null,
  "size_before_bytes": 2716973,
  "size_after_bytes": 2035062
}