

\begin{table}[t]
% \small
\fontsize{8.5}{12}\selectfont
\centering
\caption{ACM Digital Library Terminology Version 1.1 \cite{ferro2018sigir}}
\label{tbl:acm-terminology}
% \resizebox{\linewidth}{!}{
\begin{tabular}{ll}
\hline
\rowcolor[HTML]{e2eaec}
\parbox[t]{2mm}{\rotatebox[origin=c]{90}{\textbf{\hspace{0.3cm}Repeatability\hspace{0.3cm}}}} & 
\textit{\begin{tabular}[c]{@{}l@{}} Same team, same experimental setup: the measurement can \\be obtained with stated precision by the same team using the \\same measurement procedure, the same measuring system, \\under the same operating conditions, in the same location on \\multiple trials. For computational experiments, this means \\that a researcher can reliably repeat their own computation.\end{tabular}}
\\
\hline
\parbox[t]{2mm}{\rotatebox[origin=c]{90}
{\textbf{\hspace{0.3cm}Reproducibility\hspace{0.3cm}}}} &
\textit{\begin{tabular}[c]{@{}l@{}} Different team, same experimental setup:  the measurement \\can be obtained with stated precision by a different team using \\the same measurement procedure, the same measuring system, \\under the same operating conditions, in the same or a different \\location on multiple trials. For computational experiments, this \\means that an independent group can obtain the same result \\using the author’s own artifacts.\end{tabular}}
\\ 
\hline
\rowcolor[HTML]{e2eaec}
\parbox[t]{2mm}{\rotatebox[origin=c]{90}
{\textbf{\hspace{0.6cm}Replicability\hspace{0.6cm}}}} &
\textit{\begin{tabular}[c]{@{}l@{}} Different team, different experimental setup: the measurement \\can be obtained with stated precision by a different team, a \\different measuring system, in a different location on multiple \\trials. For computational experiments, this means that an \\independent group can obtain the same result using artifacts \\which they develop completely independently.\end{tabular}}
\\
\hline
\end{tabular}
% }
\end{table}




In this section, we start by defining the terms repeatability, reproducibility, and replicability (Section~\ref{subsec:3rs-def}). Next, we explore the following research question from the  Computing Continuum perspective: \emph{\textbf{What would a good collaborative system look like?}} In our vision, it should: (1) allow users to share research artifacts in a public and safe repository (Section~\ref{subsec:artifacts}); (2) provide an environment for setting up and describing experiments step-by-step (Section~\ref{subsec:environment}); (3) provide experimental methodologies to leverage  heterogeneous Edge-to-Cloud computing resources from various scientific testbeds, at large-scale (Section~\ref{subsec:methodology}).



\subsection{Repeatability, Reproducibility, Replicability}
\label{subsec:3rs-def}

An important requirement for researchers from various communities is that the scientific claims be verifiable by others (\emph{i.e.,} building upon published results). As illustrated in Figure~\ref{fig:exp-repro}, such requirement is hardly satisfied in the context of Computing Continuum experiments. This can be achieved through repeatability, reproducibility, and replicability (3Rs)~\cite{ieee-computing, Stodden-2014}.  There are many non-uniform definitions of the 3Rs in literature. In this work, we follow the terminology proposed by the ACM Digital Library \cite{ferro2018sigir} (Artifact Review and Badging version 1.1), as presented in Table \ref{tbl:acm-terminology}. 

Achieving \textbf{repeatability} means that one can reliably repeat the experiments and obtain precise measurements (\emph{e.g.,} Edge to Cloud processing latency, memory consumption, among others) by using the same methodology and artifacts (\emph{i.e.,} same testbed, same physical machines, same libraries/framework, same network configuration). Executing multiple experiments allows us to explore different scenario settings (\emph{e.g.,} varying the number of Edge devices) and explore the impact of various parameters (\emph{e.g.,} the network configuration between Edge devices and the Cloud server) on the performance metrics. 

\textbf{Reproducibility} means that external researchers having access to the original methodology (\emph{e.g.,} configuration of physical machines, network and systems, scenario descriptions) and using their own artifacts (\emph{i.e.,} data sets, scripts, AI frameworks, \emph{etc.}) can obtain precise measurements of the application processing latency and throughput, for instance.


\textbf{Replicability} refers to independent researchers (\emph{i.e.,} the readers of an article that was published by a different team) having access to the original methodology and artifacts (\emph{e.g.,} configuration of physical machines, processing steps, network setup, \emph{etc.}) and performing the experiments in different testbeds. The goal is that independent researchers can obtain precise results and conclusions consistent with the original study. 





\subsection{Trovi sharing portal}
\label{subsec:artifacts}

Collaborative systems should be integrated with public and safe repositories providing open access to the research artifacts to enable the reproducibility of experiments. Repositories like Trovi~\cite{trovi}, Kaggle~\cite{kaggle}, Code Ocean Explorer~\cite{code-ocean}, AI Hub~\cite{aihub}, GitHub~\cite{github}, and Zenodo~\cite{zenodo} allow users to store versioned and citeable (\emph{e.g.,} through a DOI: Digital Object Identifier) artifacts such as code, datasets, or Jupyter notebooks, among others. 

In this work, we leverage on the Trovi sharing portal because it provides a public REST API that facilitates integration with existing systems. Furthermore, Trovi provides a series of features to manage research artifacts such as: integration with GitHub and Zenodo; creating, packaging, and sharing artifacts as Jupyter notebooks with 500MB in total size by default; support for scientific testbeds like Chameleon, which allows users to re-launch the available artifacts on the testbed.
%; just to cite the main ones.



\subsection{Jupyter environment}
\label{subsec:environment}

Another important aspect for reproducible and replicable experiments is that collaborative systems support executable research packages composed of code, data, environment configurations, and experiment results. The most popular open-source solutions are Jupyter notebooks~\cite{kluyver2016jupyter} and Apache Zeppelin~\cite{zeppelin}. In this work, we use Jupyter notebooks for packaging research artifacts due to its wider compatibility with operating systems and programming languages, and the community support.

% https://jupyter.org/
The Jupyter project consists of JupyterHub, JupyterLab, and notebooks. JupyterHub aims to serve Cloud-based Jupyter notebooks for multiple users. The goal is to provide users a ready-to-use computational environment with their own workspace on shared resources. JupyterHub servers are customizable, scalable, and portable on a variety of infrastructures. It is composed of a Hub that manages the following sub-services: a proxy that receives requests from clients; spawners to monitor notebook servers; and an authenticator to manage how users access the system.

% https://github.com/jupyterlab/jupyterlab
JupyterLab refers to a web-based user interface providing mainly: notebook, terminal, text editor, file browser, and rich outputs. It allows users to configure and arrange their experimental workflows, as well as adding extensions to expand and enrich functionalities.

% https://jupyter-notebook.readthedocs.io/en/latest/
Finally, notebooks allow users to create programming documents combining: (1) formatted text (\emph{e.g.,} \textit{prospective
data} that explains each step of an experiment workflow); (2) executable code with the respective outputs (\emph{e.g., \textit{retrospective data} derived by the execution}); and (3) experimental results with visualizations and various sorts of rich media, such as images and videos.







\subsection{E2Clab experimental methodology}
\label{subsec:methodology}

Understanding and optimizing workflow performance requires executing and reproducing complex experiments at large scale. Several existing environments aid users to run such experiments. Their limitations are discussed in the next section and summarized in Table~\ref{tbl:limitations}. Based on these findings and the specific Computing Continuum requirements, in this work we leverage the E2Clab methodology.

%Modern scientific workflows require heterogeneous IoT/Edge and Cloud/HPC computing resources available at large-scale to . Furthermore, reliable underlying infrastructures are needed to . 



E2Clab~\cite{rosendo:hal-02916032} is an open-source framework (available at~\cite{e2clab-code}) that implements a rigorous methodology (illustrated in Figure~\ref{fig:methodology}) for designing experiments with real-world workloads on the Edge-to-Cloud Continuum. It allows researchers to reproduce the application behavior in a controlled environment in order to understand and optimize performance~\cite{rosendo:hal-03310540}. E2Clab sits on top of EnOSlib~\cite{cherrueau2021enoslib} to enforce the experiment configurations on testbeds. High-level features provided by E2Clab are: \emph{(i)} reproducible experiments; \emph{(ii)} mapping application parts (Edge, Fog and Cloud/HPC) and physical testbeds; \emph{(iii)} experiment variation and transparent scaling of scenarios; \emph{(iv)} defining Edge-to-Cloud network constraints; \emph{(v)} experiment deployment, execution and monitoring (\emph{e.g.,} on Grid'5000, Chameleon, and FIT IoT LAB); and \emph{(vi)} workflow optimization. 





 
% Figure environment removed



% As communities from an increasing number of scientific domains are leveraging the Computing Continuum, a desired feature of any experimental research is that its scientific claims are verifiable by others in order to build upon them. This can be achieved through repeatability, reproducibility, and replicability (3Rs)~\cite{ieee-computing, stodden2013best}.  There are many non-uniform definitions of the 3Rs in literature. In this work, we follow the terminology proposed by the ACM Digital Library \cite{ferro2018sigir} (Artifact Review and Badging), as presented in Table \ref{tbl:acm-terminology}. 


% \begin{table}[t]
% \centering
% \caption{ACM Digital Library Terminology Version 1.1 \cite{ferro2018sigir}}
% \label{tbl:acm-terminology}
% \resizebox{\linewidth}{!}{
% \begin{tabular}{ll}
% \hline
% \rowcolor[HTML]{e2eaec}
% \parbox[t]{2mm}{\rotatebox[origin=c]{90}{\textbf{\hspace{0.3cm}Repeatability\hspace{0.3cm}}}} & 
% \textit{\begin{tabular}[c]{@{}l@{}} Same team, same experimental setup: the measurement can be obtained with stated precision by \\the same team using the same measurement procedure, the same measuring system, under the \\same operating conditions, in the same location on multiple trials. For computational experi-\\ments, this means that a researcher can reliably repeat their own computation.\end{tabular}}
% \\
% \hline
% \parbox[t]{2mm}{\rotatebox[origin=c]{90}
% {\textbf{\hspace{0.3cm}Reproducibility\hspace{0.3cm}}}} &
% \textit{\begin{tabular}[c]{@{}l@{}} Different team, same experimental setup: the measurement can be obtained with stated \\precision by a different team, a different measuring system, in a different location on multiple \\trials. For computational experiments, this means that an independent group can obtain the \\same result using artifacts which they develop completely independently.\end{tabular}}
% \\ 
% \hline
% \rowcolor[HTML]{e2eaec}
% \parbox[t]{2mm}{\rotatebox[origin=c]{90}
% {\textbf{\hspace{0.6cm}Replicability\hspace{0.6cm}}}} &
% \textit{\begin{tabular}[c]{@{}l@{}} Different team, different experimental setup: the measurement can be obtained with stated \\precision by a different team using the same measurement procedure, the same measuring \\system, under the same operating conditions, in the same or a different location on multiple \\trials. For computational experiments, this means that an independent group can obtain \\the same result using the author’s own artifacts.\end{tabular}}
% \\
% \hline
% \end{tabular}
% }
% \end{table}



