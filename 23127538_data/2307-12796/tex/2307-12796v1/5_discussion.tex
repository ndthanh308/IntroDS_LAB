

%This work introduces the KheOps approach for the cost-effective reproducibility and replicability of Edge-to-Cloud experiments. 

% The previous results show that KheOps helps authors make their experiments reproducible. In addition, it helps readers to replicate the authors experiments in different IoT/Edge and Cloud/HPC infrastructures. 

KheOps core elements (\emph{i.e.,} Trovi, JupyterLab, E2Clab)  exhibit several features that make it a promising environment for advancing Computing Continuum research through reproducible and replicable experiments. We briefly discuss them here.

\subsection{Usability and reusability} 

KheOps targets \textbf{usability} by allowing users to easily find experiment artifacts shared in Trovi and then to launch experiments in a JupyterLab server in just a few clicks. KheOps abstracts all the low-level details of defining and configuring the experimental environment. It provides a high-level abstraction for mapping application parts with the Edge and Cloud infrastructures. Besides, the configuration files used to define the whole experimental environment are designed to be easy to use and understand. 

KheOps also targets \textbf{reusability} of the experiment artifacts. For instance, readers of an article can reuse the authors artifacts to replicate the study or build upon the existing artifacts to generate new results. In addition, through E2Clab \emph{User-Defined Services}, users can define their own services (\emph{e.g.,} the Edge client and the Cloud server) with the desired deployment logic (\emph{e.g.,} mapping the services to the physical machines/devices; installing required software and packages; \emph{etc.}). Such services can be shared in this repository~\cite{e2clab-uds}. %It allows users to easily plug in the existing services in their experiments. Several services already implemented and shared by the community can be reused, such as Horovod~\cite{sergeev2018horovod}, Flower~\cite{beutel2020flower}, Apache Flink~\cite{apache-flink}, COMPSs~\cite{badia2015comp}, among others.


\subsection{Analyzing other real-life applications}

The KheOps approach is \textbf{generic} in terms of deployment and analysis of \textbf{other applications}. We highlight that, despite our evaluations focusing on the African savanna use-case, KheOps can be easily used in other contexts. Supporting new applications can be achieved by describing and implementing their logic in the \emph{User-Defined Services} configuration file. 


\subsection{Integration with other scientific testbeds}
\label{subsec:integration}

The KheOps approach is \textbf{generic} with respect to the \textbf{deployment testbeds}. KheOps allows users to analyze application workflows on various large-scale scientific testbeds, beyond the four testbeds used in this work. The definition of the experimental environment through E2Clab configuration files (\emph{e.g.,} \emph{layer\_services.yaml, network.yaml}, and \emph{workflow.yaml}) is tesbed agnostic, meaning that a deployment on the Grid'5000 testbed can be easily replicated in Chameleon (if the required computing resources are available).



\subsection{Reproducibility and artifact availability}

The experimental evaluations presented in this work follow a rigorous methodology~\cite{rosendo:hal-02916032} to support reproducible Edge-to-Cloud experiments on large-scale scientific testbeds. All the experiment artifacts are publicly available~\cite{xp-artifacts-trovi} at the Trovi sharing portal and the results are also publicly available~\cite{exp-artifacts} in our GitLab repository.

\subsection{KheOps limitations}
Next, we discuss future research under the KheOps approach to help with experiment reproducibility.

\paragraph{Provenance data capture} It may assist in the processes of reproducing complex Edge-to-Cloud workflows~\cite{liu2009encyclopedia}. Typically, users have to execute and repeat various experiments. The output of this process generates hundreds of data related to the experimental setup (\emph{e.g.,} hardware, software, code, data set, \emph{etc.}) and application workflow execution. Analyzing such data is only possible with the help of provenance data capture~\cite{souza_keeping_2019}.

\paragraph{Abstract hardware description} The hardware configuration is a significant barrier to reproducibility~\cite{calasanz2023}, especially in complex Edge-to-Cloud deployments comprising heterogeneous computing resources. The description of resources should be in terms of hardware requirements to execute the experiments (\emph{e.g.,} CPU, GPU, memory, disk, and network). The goal is to abstract the hardware resource description among various testbeds, preventing independent researchers from knowing about the infrastructure of the original experimental environment. 

 
