

In this section, we show how KheOps can be used to analyze the performance of a real-life Edge-to-Cloud application deployed in the African savanna (illustrated in Figure~\ref{fig:xpsetup}). This application is composed of distributed Edge devices monitoring animal migration in the Serengeti region. Devices at the Edge collect and compress wildlife images, then the image is sent to the Cloud where the animal classification happens using a pre-trained Neural Network model. Finally, classified data helps conservationists to learn what management strategies work best to protect species.

The goals of these experiments are:

\begin{itemize}
    \item to understand the impact on performance of \textbf{Cloud-centric} and \textbf{Hybrid (Edge+Cloud)} processing;
    \item to show how authors of an article can benefit from KheOps to make their \textbf{experiments reproducible};
    \item to show how readers of an article can leverage KheOps to \textbf{replicate the experiments} in an article (published using KheOps). 
\end{itemize}



% Figure environment removed



% Figure environment removed




% Figure environment removed



% % Figure environment removed





% Figure environment removed





% % Figure environment removed


To reproduce the evaluations in this section, refer to~\cite{xp-artifacts-trovi}.


\subsection{Experimental setup}
\label{subsec:xp-setup-desc}


\paragraph{\textbf{Application performance metrics}} 
The main tracked metric is the \emph{processing time}, which refers to the time required to: pre-process the image captured (\emph{e.g.,} image compression on the Edge device); transmit the image to the Cloud server; and finally decompress the image and predict the animal through an AI model. In addition, we analyze the \emph{amount of data transmitted} to the Cloud and the \emph{resource consumption} (\emph{e.g.,} CPU and memory) on the Edge device.

To increase the accuracy of the results, we measure the processing duration 100 times for each experiment, each time with a different image and an interval of 30 seconds (\emph{i.e.,} Edge devices transmit images to the Cloud server every 30 seconds). The remaining metrics are captured using Dool (Dstat)~\cite{dool} at application runtime. All results are presented as the mean followed by their respective 95\% confidence interval.

\paragraph{\textbf{KheOps replicability metric}} 
To measure how close/precise readers experiments are from authors experiments, we define the Replicability Accuracy ($Rep_{accuracy}$) metric. For assessing variability and error in results~\cite{national2019reproducibility}, a recommendation is to repeat the experiments multiple times to achieve narrower inferential error bars (i.e., confidence interval, standard deviation, \emph{etc.})~\cite{cumming2007error}. The Replicability Accuracy metric is calculated as Equation~\ref{eq:rep_acc}:


\begin{equation}
% \scriptsize	
\label{eq:rep_acc}
\begin{aligned}
Rep_{accuracy} = 1 - \bigg | 
\frac{min(x_{1A}, x_{2A})}{max(x_{1A}, x_{2A})} - 
\frac{min(x_{1R}, x_{2R})}{max(x_{1R}, x_{2R})} \bigg | 
\end{aligned}
\end{equation}

Ideally, $Rep_{accuracy}$ would be close to $1$. $x_{iA}$ and $x_{iR}$ refer to the application performance metric value obtained from authors and readers experiments, respectively. For instance, in Figure~\ref{fig:15g5k}, $x_{1A}$ refers to the Cloud-centric bar and $x_{2A}$ to the Edge+Cloud bar.


% Because of the intrinsic variability of nature and limitations of measurement devices, results are assessed probabilistically, with the scientific discovery process unable to deliver absolute truth or certainty. 




\paragraph{\textbf{Workload}}
Devices at the Edge transmit images (from the Snapshot Serengeti dataset~\cite{usecase-dataset} composed of millions of wildlife images collected annually) to the Cloud server that predicts animals using a trained MobileNetV3 Convolutional Neural Network model. We evaluate this workload considering two network configurations, 25Kbit and 15Kbit bandwidth with a round-trip delay of 150ms.


\paragraph{\textbf{Software}} On the Edge devices, we use the zlib~\cite{zlib} Python library to compress images. MQTT~\cite{mqtt} protocol is used to transmit images to the Cloud server. On the Cloud server, we use an MQTT broker to receive images, then zlib to decompress images, and finally PyTorch to predict animals.

\paragraph{\textbf{Hardware}} 
The \textbf{authors} perform experiments on the following testbeds in France: Grid’5000 and FIT IoT LAB. On Grid'5000 (Cloud server), they use the dahu~\cite{g5kdahu} machine equipped with an Intel Xeon Gold 6130 CPU 2.10GHz, 16 cores/CPU, 192GB of RAM, and Ethernet network. On FIT IoT LAB (Edge device), they use a Raspberry Pi 3 Model B~\cite{fitrpi3} with four ARM Cortex-A53 processing cores running at 1.2GHz, 1GB LPDDR2 memory, and 2.4GHz 802.11ac wireless LAN. 

The \textbf{readers} replicate authors experiments on the following testbeds in USA: Chameleon Cloud and CHI@Edge. On Chameleon CHI@TACC (Cloud server), they use the Skylake~\cite{chitacc} machine equipped with an Intel Xeon Gold 6126 CPU 2.60GHz, 12 cores/CPU, 192GB of RAM, and Ethernet network. On CHI@Edge (Edge device), they use a Raspberry Pi 4~\cite{chiedgerpi4}, with four BCM2711 Cortex-A72 processing cores running at 1.5GHz, 8GB LPDDR4 memory, and 2.4GHz and 5GHz 802.11ac wireless LAN.


\subsection{\textbf{How KheOps helps experiment authors}} 

Let us consider the requirements of the experiment authors (a-REQ) as introduced in Section~\ref{sec:introduction}.

\paragraph{\textbf{a-REQ 1. Execute experiments on heterogeneous computing resources}} KheOps provides access to IoT/Edge devices and Cloud/HPC resources at large-scale, using the E2Clab methodology. Supported testbeds include (but are not limited to, as explained in Section~\ref{subsec:integration}): Grid'5000, FIT IoT LAB. 


\paragraph{\textbf{a-REQ 2. Systematically describe and explain the experimental processes and their reasoning}} Through Jupyter notebooks and the E2Clab configuration files, the authors describe and explain the experiment design choices such as the layers (\emph{e.g.,} Edge and Cloud), the services (\emph{e.g.,} the Edge client and the Cloud server), the network constraints, and the application workflow execution. This is done in Jupyter notebooks by combining text (explaining the configurations) followed by executable code (E2Clab files). 


\paragraph{\textbf{a-REQ 3. Efficiently configure the experimental infrastructure and repeat the experiments}} All the complexities of configuring the Edge-to-Cloud infrastructure, such as leasing computing resources, mapping the application parts (\emph{e.g.,} Edge and Cloud services), enforcing the network constraints, and executing the workflow are transparently handled by KheOps. The authors just need to define their experimental needs in the E2Clab configuration files. Repeating and adapting the experiments (\emph{e.g.,} changing the network constraints) is easily done through E2Clab instrumentation. 

\paragraph{\textbf{a-REQ 4. Easily share the experiment artifacts in a public and safe repository}} Through the Trovi and JupyterLab integration, authors can upload their artifacts to the Trovi sharing portal with a few clicks.\\

We discuss the experimental results from the authors perspective, using the three application performance metrics mentioned earlier. 

\subsubsection{\textbf{Impact of the network on the processing time}} 
Authors define two sets of experiments. In the first one (Figure~\ref{fig:15g5k}), they fix the network bandwidth at 15Kbit and vary the processing approach between Cloud-centric and Hybrid (Edge+Cloud). In the second one (Figure~\ref{fig:25g5k}), they fix the bandwidth at 25Kbit for both processing approaches.

From the results, the authors observe that the Hybrid (Edge+Cloud) approach outperforms the Cloud-centric one for both network configurations. In the 15Kbit bandwidth setup, the processing time for the Cloud-centric is about 27 seconds on average, against 24 seconds for the hybrid processing. In the 25Kbit bandwidth configuration, this difference is lower, 13 seconds and 11 seconds for the Cloud-centric and Hybrid, respectively. The higher the bandwidth, the lower will be the difference between the two processing approaches. This is because image transmission is the most time-consuming task among the other tasks (\emph{i.e.,} compressing/decompressing images and model inference).



\subsubsection{\textbf{Amount of data sent to the Cloud}} According to the results presented in Figure~\ref{fig:net-g5k}, authors observe that the Hybrid (Edge+Cloud) approach transmits less data (81kB/s on average) to the Cloud compared to the Cloud-centric approach (96kB/s on average). This is because, in Hybrid processing, Edge devices compress images before transmitting them to the Cloud.



\subsubsection{\textbf{Resource consumption on the Edge device}} 
Results in Figures~\ref{fig:cpu-g5k} and~\ref{fig:mem-g5k} show that there is no significant difference in the CPU and memory usage in the Edge device when changing between the Cloud-centric and Hybrid processing approaches. CPU usage is around 4.2\% and 4.4\% for Hybrid and Cloud-centric processing, respectively. Memory usage is around 0.38GB for both.  


\subsection{\textbf{How KheOps helps readers}} 
After the authors publish their results, other researchers from a different lab download the article from a scientific database and decide to replicate the study on their own premises (\emph{e.g.,} on a different testbed). Following the same logic, we present how KheOps helps the readers to replicate the experiments cost-effectively, that is, according to the readers requirements (r-REQ) in Section~\ref{sec:introduction}.


\paragraph{\textbf{r-REQ 1. Find and access the experiment as simply as finding and reading the paper}} 
Through the KheOps web interface (step 1 in Figure~\ref{fig:kheops-archi}) the readers obtain access to all the public experiments shared by the community and available in Trovi. Then, they select the experiment shared by the authors of the article to get more details. 


\paragraph{\textbf{r-REQ 2. Perform the experiment, not just read about it}} 
Next, in the experiment details web page,  readers can launch a JupyterLab server with artifacts in just a single click (steps 2, 3, and 4 in Figure~\ref{fig:kheops-archi}). Finally, following the experiment instructions described in the Jupyter notebook, the readers deploy and execute the experiments on their testbeds, such as (but not limited to): the Chameleon Cloud and CHI@Edge (step 5 in Figure~\ref{fig:kheops-archi}). 

\paragraph{\textbf{r-REQ 3. Experiment reasoning: \emph{“What”}, \emph{“Why”}, and \emph{“How”}}} Before running the experiments, the readers can go through the Jupyter notebook to understand \emph{What} the experiment does (\emph{e.g.,} capture and compress images on Edge devices and then decompress the images and predict the animals on the Cloud server). The readers can also discover \emph{Why} the authors set up the experiment with a 25kbit and 15kbit network bandwidth. Finally, KheOps allows to understand \emph{How} the authors interconnect the Edge devices with the Cloud server (\emph{e.g.,} assigning a public IP to the Cloud server, or opening firewall rules; using the MQTT protocol; among others).


\paragraph{\textbf{r-REQ 4. Efficiently configure the experimental infrastructure}} 
To achieve this, the readers just have to adapt the \emph{layers\_services} configuration file (presented in Listing~\ref{lis-e2c-layers}) to the Chameleon Cloud and CHI@Edge testbeds. Configuring the network bandwidth to 25kbit and then changing it to 15kbit is as simple as changing the \emph{rate} parameter in the \emph{network} file (Listing~\ref{lis-e2c-net}). Finally, copying data to the Edge device, interconnecting it with the Cloud server, launching the application, and finally collecting the results is as simple as defining the \emph{workflow} configuration file (Listing~\ref{lis-e2c-workflow}). The \emph{network} and \emph{workflow} configuration files are testbed agnostic, meaning that users do not need to update these files when changing the deployment from Grid'5000 + FIT IoT LAB to Chameleon + CHI@Edge.\\

Next, we report on the replicated experiments. % carried out by the readers. 



\begin{table}[t]
\small
\centering
\caption{Accuracy of replicated experiments.}
\label{tbl:accuracy}
\begin{tabular}{lll}
\hline
\textbf{Metric}        & \textbf{\begin{tabular}[c]{@{}l@{}}Replicability \\ accuracy\end{tabular}} & \textbf{Experiment result}                                                                                                         \\ \hline
\rowcolor[HTML]{D9D9D9} 
Processing time 15Kbit        & 0.943                                                                      & \cellcolor[HTML]{D9D9D9}Figure~\ref{fig:15g5k} and~\ref{fig:15chi}     \\
Processing time 25Kbit  & 0.882                                                                      & Figure~\ref{fig:25g5k} and~\ref{fig:25chi}                             \\
\rowcolor[HTML]{D9D9D9} 
Data sent to the cloud & 0.973                                                                      & \cellcolor[HTML]{D9D9D9}Figure~\ref{fig:net-g5k} and~\ref{fig:net-chi} \\
CPU usage              & 0.978                                                                      & Figure~\ref{fig:cpu-g5k} and~\ref{fig:cpu-chi}                         \\
\rowcolor[HTML]{D9D9D9} 
Memory usage           & 0.996                                                                      & \cellcolor[HTML]{D9D9D9}Figure~\ref{fig:mem-g5k} and~\ref{fig:mem-chi} \\ \hline
\end{tabular}
\end{table}


\subsubsection{\textbf{Impact of the network on the processing time}} 
From the results in Figures~\ref{fig:15chi} and~\ref{fig:25chi}, readers conclude that the Hybrid (Edge+Cloud) processing approach outperforms the Cloud-centric one for both network configurations. This conclusion is consistent with the results observed in the published article.  

Following the analysis, readers observe that in the 15Kbit bandwidth network configuration, the processing time for the Cloud-centric is about 8 seconds on average, against 6.5 seconds for the hybrid processing. In the 25Kbit bandwidth  setup, this difference is lower, 5.5 seconds and 4 seconds for the Cloud-centric and Hybrid, respectively. Similarly to the authors results, readers also observe that the higher the bandwidth, the lower will be the difference between the two processing approaches.

Furthermore, as presented in Table~\ref{tbl:accuracy}, we highlight that readers obtained a replicability accuracy of 88.2\% and 94.3\% for 15Kbit and 25Kbit network configurations, respectively.



\subsubsection{\textbf{Amount of data sent to the Cloud}} According to the results presented in Figure~\ref{fig:net-chi}, readers observe that the Hybrid approach transmits less data than the Cloud-centric. The former transmits around 89.2kB/s and the latter 108.8kB/s. Compressing images on the Edge helps to reduce the amount of data sent to the Cloud server. This conclusion is also consistent with the published article and presents a replicability accuracy of 97.3\%.  


\subsubsection{\textbf{Resource consumption on the Edge device}} 
Results in Figures~\ref{fig:cpu-chi} and~\ref{fig:mem-chi} show that there is no significant difference in the CPU and memory usage between the Cloud-centric and the Hybrid processing approaches. CPU usage is around 5.1\% and 5\% for Hybrid and Cloud-centric processing, respectively. Memory usage is around 1.1GB for both. We highlight that these conclusions are consistent with the published article and present a replicability accuracy of 97.8\% and 99.6\% for CPU and memory usage, respectively.



\begin{framed}
\vspace{-0.1cm}

Despite readers observing a lower processing time compared to the authors, they could verify that their experiment conclusions are consistent with the original study, and their results present a high replicability accuracy (see Table~\ref{tbl:accuracy}). 

This time difference is expected since readers used a more powerful Edge device (Raspberry Pi4 against Raspberry Pi3) for processing the most time-consuming task (\emph{e.g.,} image compression and then transmission). The Raspberry Pi4 has more RAM memory (8GB \emph{vs.} 1GB in Raspberry Pi3), a better CPU (1.5GHz \emph{vs.} 1.2GHz), network (5GHz \emph{vs.} 2.4GHz). 

Furthermore, regarding the remaining metrics such as the amount of data sent to the Cloud and the CPU and memory usage on the Edge device, readers observe small differences when replicating the original study in different testbeds. This is due to the different deployment approaches used by each testbed, for instance, in FIT IoT LAB the Raspberry Pi 3 board runs an embedded Linux that is built with Yocto~\cite{yocto}, while CHI@Edge is based on Docker~\cite{docker} containers. Despite that, the conclusions observed by authors and readers are the same and present high accuracies. 



\vspace{-0.1cm}
\end{framed}



% CPU
% G5K: (4.400435530085959 - 4.205732743362832) / 4.205732743362832 = 0.04629461703918114
% CHI: (5.0976080071706 - 4.982923509286413) / 4.982923509286413 = 0.023015504386221502
% 0.023015504386221502 / 0.04629461703918114 = 0.49715292745034445




 

