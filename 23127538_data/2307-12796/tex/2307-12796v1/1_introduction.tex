


Modern scientific workflows require hybrid infrastructures, combining resources and services executed on the IoT/Edge with other resources and services running on Clouds or on HPC systems (the \emph{Computing Continuum}~\cite{etp4-hpc-20}) to enable their optimized execution. 
%This hybrid approach contributes to the emergence of the \emph{Computing Continuum}~\cite{etp4-hpc-20} (or the \emph{Digital Continuum} or the \emph{Transcontinuum}). 
Due to the complexity of application deployments on such highly distributed and heterogeneous Edge-to-Cloud infrastructures, realizing the Computing Continuum vision in practice remains burdensome. 

One challenge stems from systematically performing experiments on the continuum. In particular, the processes enabling their reproducibility, as well as the replication of the performance trade-offs are inherently difficult~\cite{haibe2020transparency}. Figure~\ref{fig:context} illustrates such processes. Let us consider the case of a group of researchers who execute their experiments on French scientific testbeds such as Grid'5000~\cite{RaphaEtAl2006} (providing Cloud/HPC servers) and FIT IoT LAB~\cite{adjih2015fit} (providing IoT/Edge devices), and want to publish their results in an article. Next, the readers want to replicate the experiments on American testbeds such as the Chameleon Cloud~\cite{KateEtAl2020} and CHI@Edge~\cite{keahey2021chameleon}. 


These processes compel a lot of effort, are time-consuming, and bring many technical challenges for both sides. For instance, also depicted in Figure~\ref{fig:context}, they require: (1) following methodologies to systematically design the experiments and to reconcile many application requirements or constraints in terms of energy consumption, network efficiency, and hardware resource usage; (2) configuring systems and networks, and deploying applications on testbeds for large-scale evaluations; (3) analyzing, repeating experiments, and publishing results; and (4) finally, providing open access to the experiment artifacts in a public and safe repository.

Given such complexities, researchers end up not following rigorous methodologies for supporting the reproducibility of the experiments, as observed in our previous survey~\cite{rosendo:hal-03654722} and summarized in Figure~\ref{fig:exp-repro}. As a consequence, it makes it hard for other researchers to replicate the published studies~\cite{krafczyk2021learning}. 




% Figure environment removed



Let us sum up the associated requirements in this context~\cite{stodden2016enhancing, gundersen2018reproducible}. To enable \textbf{reproducible} experiments on the Edge-to-Cloud continuum, the requirements (a-REQ) of the authors of the experiments can be described as follows:

\begin{enumerate}[start=1,label={\textbf{a-REQ \arabic*.}}]
    \item Execute experiments on heterogeneous computing resources (\emph{e.g.,} IoT/Edge and Cloud/HPC infrastructures).   
    \item Systematically describe and explain the experimental processes and their reasoning.
    \item Efficiently configure the experimental infrastructure and express topologies in repeatable ways.
    \item Easily share the experiment artifacts in a public and safe repository.
\end{enumerate}

At the same time, to enable the \textbf{replicability} of the experiments, the readers of an article describing those experiments have the following requirements (r-REQ):

\begin{enumerate}[start=1,label={\textbf{r-REQ \arabic*.}}]
    \item Find and access the experiment as simply as finding and reading its paper.
    \item Perform the experiment, not just read about it.
    \item Answer not just to the \emph{“What”} question (What the experiment does?), but also the \emph{“Why”} (Why did authors set up that way?) and \emph{“How”} (How did authors connect machines/devices?)
    \item Efficiently configure the experimental infrastructure to reduce the time spent satisfying all the experiment requirements.
\end{enumerate}



% Figure environment removed



In this paper, we study the challenges of reproducing and replicating Edge-to-Cloud experiments in cost-effective ways. \textbf{Cost-effective means} to allow authors and readers to easily fulfill their experimental requirements as previously described. This calls for practical solutions beyond the state-of-the-art. % and requires the design and development of novel approaches. 

Our main objective is to provide a collaborative environment and methodology that supports reproducible Edge-to-Coud experimentation between different open testbeds such as Grid'5000, FIT IoT LAB, Chameleon, \emph{etc.}, equipped to deal with IoT/Edge and Cloud/HPC resources which are fundamental to reproducibility~\cite{keahey2020silver}. We propose the following main contributions:


\begin{enumerate}

    \item \textbf{A study of the characteristics of the main state-of-the-art collaborative environments} (\emph{e.g.,} Google Colab, Kaggle, and Code Ocean) for enabling reproducible experiments. Their \textbf{main limitations in the context of Computing Continuum research} are discussed in Section~\ref{sec:limitations}.

    \item \textbf{A novel collaborative environment to enable reproducible Edge-to-Cloud experiments} (Section~\ref{sec:kheops}). This approach, named KheOps, allows researchers to reproduce and replicate Edge-to-Cloud workflows cost-effectively. KheOps core elements are: (1) a portal for sharing experiment artifacts; (2) a notebook environment for packaging code, data, environment, and results; and (3) a \textbf{multi-platform} experimental methodology for deploying experiments on heterogeneous resources from the IoT/Edge (FIT IoT LAB and CHI@Edge) to the Cloud/HPC Continuum (Grid5000 and Chameleon). We highlight that KheOps may be integrated with other large-scale scientific testbeds. 

    \item An \textbf{experimental validation} of the proposed approach with a \textbf{real-world use case deployed on real-life IoT/Edge devices and Cloud/HPC systems}. The evaluations show that KheOps helps: (1) authors to perform reproducible experiments on the \textbf{Grid5000 + FIT IoT LAB} testbeds, and (2) readers to cost-effectively replicate authors experiments on the \textbf{Chameleon Cloud + CHI@Edge} testbeds, and \textbf{obtain the same conclusions with high accuracies}, $>$88\% for all performance metrics (Section~\ref{sec:evaluation}).
    
\end{enumerate}
