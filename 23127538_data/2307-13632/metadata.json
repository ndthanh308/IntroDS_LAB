{
  "title": "Mitigating Mainstream Bias in Recommendation via Cost-sensitive Learning",
  "authors": [
    "Roger Zhe Li",
    "Juli√°n Urbano",
    "Alan Hanjalic"
  ],
  "submission_date": "2023-07-25T16:31:59+00:00",
  "revised_dates": [],
  "abstract": "Mainstream bias, where some users receive poor recommendations because their preferences are uncommon or simply because they are less active, is an important aspect to consider regarding fairness in recommender systems. Existing methods to mitigate mainstream bias do not explicitly model the importance of these non-mainstream users or, when they do, it is in a way that is not necessarily compatible with the data and recommendation model at hand. In contrast, we use the recommendation utility as a more generic and implicit proxy to quantify mainstreamness, and propose a simple user-weighting approach to incorporate it into the training process while taking the cost of potential recommendation errors into account. We provide extensive experimental results showing that quantifying mainstreamness via utility is better able at identifying non-mainstream users, and that they are indeed better served when training the model in a cost-sensitive way. This is achieved with negligible or no loss in overall recommendation accuracy, meaning that the models learn a better balance across users. In addition, we show that research of this kind, which evaluates recommendation quality at the individual user level, may not be reliable if not using enough interactions when assessing model performance.",
  "categories": [
    "cs.IR"
  ],
  "primary_category": "cs.IR",
  "doi": "10.1145/3578337.3605134",
  "journal_ref": null,
  "arxiv_id": "2307.13632",
  "pdf_url": null,
  "comment": "8 pages, 7 figures, accepted to ICTIR'23",
  "num_versions": null,
  "size_before_bytes": 4715448,
  "size_after_bytes": 451358
}