%%
%% This is file `sample-sigconf-i13n.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `sigconf-i13n')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf-i13n.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command.
\documentclass[sigconf, language=french,
language=german, language=spanish, language=english]{acmart}

\usepackage{xspace}
\usepackage{diagbox}
\usepackage{multirow}
\usepackage{lscape}
%% NOTE that a single column version may required for 
%% submission and peer review. This can be done by changing
%% the \doucmentclass[...]{acmart} in this template to 
%% \documentclass[manuscript,screen]{acmart}
%% 
%% To ensure 100% compatibility, please check the white list of
%% approved LaTeX packages to be used with the Master Article Template at
%% https://www.acm.org/publications/taps/whitelist-of-latex-packages 
%% before creating your document. The white list page provides 
%% information on how to submit additional LaTeX packages for 
%% review and adoption.
%% Fonts used in the template cannot be substituted; margin 
%% adjustments are not allowed.

\newcommand{\Sim}{\textsf{Sim}\xspace}
\newcommand{\Util}{\textsf{Util}\xspace}

\newcommand{\zhe}[1]{{\color{red}Z:~#1}}
% \newcommand{\ju}[1]{}
\newcommand{\ju}[1]{{\color{cyan}J:~#1}}
\newcommand{\ah}[1]{{\color{magenta}A:~#1}}
\newcommand{\tocheck}[1]{{\color{purple}#1}}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
% \AtBeginDocument{%
%   \providecommand\BibTeX{{%
%     \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.

%% \BibTeX command to typeset BibTeX logo in the docs \AtBeginDocument{%
\providecommand\BibTeX{{%
Bib\TeX}}

\copyrightyear{2023}
\acmYear{2023}
\setcopyright{rightsretained}
\acmConference[ICTIR '23]{Proceedings of the 2023 ACM SIGIR International Conference on the Theory of Information Retrieval}{July 23, 2023}{Taipei, Taiwan} \acmBooktitle{Proceedings of the 2023 ACM SIGIR International Conference on the Theory of Information Retrieval (ICTIR '23), July 23, 2023, Taipei, Taiwan}\acmDOI{10.1145/3578337.3605134}
\acmISBN{979-8-4007-0073-6/23/07}




%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.    
%%

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
% \makeatletter
% \gdef\@copyrightpermission{
%   \begin{minipage}{0.3\columnwidth}
  
%   \href{https://creativecommons.org/licenses/by/4.0/}{% Figure removed}
%   \end{minipage}\hfill  
%   \begin{minipage}{0.7\columnwidth}
  
%   \href{https://creativecommons.org/licenses/by/4.0/}{This work is licensed under a Creative Commons Attribution International 4.0 License.}
%   \end{minipage}
%   \vspace{5pt}
% }
% \makeatother


\begin{document}


%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Mitigating Mainstream Bias in Recommendation\\via Cost-sensitive Learning}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Roger Zhe Li}
\email{Z.Li-9@tudelft.nl}
\affiliation{%
  \institution{Delft University of Technology}
  \city{Delft}
  \country{The Netherlands}
}

\author{Julián Urbano}
\email{J.Urbano@tudelft.nl}
\affiliation{%
  \institution{Delft University of Technology}
  \city{Delft}
  \country{The Netherlands}
}

\author{Alan Hanjalic}
\email{A.Hanjalic@tudelft.nl}
\affiliation{%
  \institution{Delft University of Technology}
  \city{Delft}
  \country{The Netherlands}
}


%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
%\renewcommand{\shortauthors}{Li et al.}
\renewcommand{\shorttitle}{Mitigating Mainstream Bias in Recommendation via Cost-sensitive Learning}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
Mainstream bias, where some users receive poor recommendations because their preferences are uncommon or simply because they are less active, is an important aspect to consider regarding fairness in recommender systems. Existing methods to mitigate mainstream bias do not explicitly model the importance of these non-mainstream users or, when they do, it is in a way that is not necessarily compatible with the data and recommendation model at hand. In contrast, we use the recommendation utility as a more generic and implicit proxy to quantify mainstreamness, and propose a simple user-weighting approach to incorporate it into the training process while taking the cost of potential recommendation errors into account. 
We provide extensive experimental results showing that quantifying mainstreamness via utility is better able at identifying non-mainstream users, and that they are indeed better served when training the model in a cost-sensitive way. This is achieved with negligible or no loss in overall recommendation accuracy, meaning that the models learn a better balance across users.
In addition, we show that research of this kind, which evaluates recommendation quality at the individual user level, may not be reliable if not using enough interactions when assessing model performance.
\end{abstract}
\sloppy
%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10002951.10003317.10003347.10003350</concept_id>
       <concept_desc>Information systems~Recommender systems</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Information systems~Recommender systems}

\keywords{Recommender Systems, Mainstream Bias, Bias Mitigation}


%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
% \keywords{datasets, neural networks, gaze detection, text tagging}

%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction} \label{sec:intro}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Collaborative Filtering (CF) \cite{DBLP:journals/cacm/GoldbergNOT92} is one of the most successful strategies for developing recommender systems. With the assumption that near neighbors are more likely to share similar user preferences and item properties, collaborative filtering shows its effectiveness through different methodologies such as Matrix Factorization (MF)~\cite{DBLP:journals/computer/KorenBV09}, Factorization Machines (FM)~\cite{DBLP:conf/icdm/Rendle10}, and deep learning techniques~\cite{DBLP:conf/icdm/Rendle10}. The success of CF methods has made recommender systems widely applied in multiple use cases including e-commerce, education, healthcare, etc~\cite{ko2022survey}.
%With the wide deployment of collaborative filtering (CF)~\citep{DBLP:journals/cacm/GoldbergNOT92} as the model underlying many modern recommender systems, there has been increasing awareness of some its critical limitations. 
One of the critical limitations of recommender systems based on collaborative filtering (CF) models~\citep{DBLP:journals/cacm/GoldbergNOT92} is that they are \emph{not fair} in how they serve different groups of users~\cite{DBLP:conf/www/LeonhardtAK18, DBLP:conf/www/LiCFGZ21}. 
%They happen to serve certain groups of users better, leaving the others to suffer from lower recommendation quality. 
%This tendency, which could be further amplified via a feedback loop~\cite{DBLP:conf/recsys/ChaneySE18, DBLP:conf/fat/DAmourSABSH20}, results in a lower utility of a recommender system for a certain group of users~\cite{DBLP:conf/cikm/MansouryAPMB20}. Such issues lead to many biases that are tricky to mitigate~\cite{DBLP:journals/corr/abs-2010-03240}, and some of the biases result in fairness concerns in recommender systems~\cite{DBLP:journals/corr/abs-2205-13619}, among which user fairness~\cite{DBLP:conf/www/LeonhardtAK18, DBLP:conf/www/LiCFGZ21}, the property indicating whether all users are with balanced utilities even under satisfactory overall accuracy, is a critical component.
% The signature of CF-based methods is the similarity-oriented property, but it also makes the method suffer from some limitations. Since similarity is usually captured via the observed data, the recommendation model tends to get better modeling for active users and frequent items. In addition, this tendency could be even propagated and intensified via a feedback loop~\cite{DBLP:conf/recsys/ChaneySE18, DBLP:conf/fat/DAmourSABSH20}, and finally results in low utilities for a certain group of users or items~\cite{DBLP:conf/cikm/MansouryAPMB20}. Such issues lead to many biases that are tricky to mitigate~\cite{DBLP:journals/corr/abs-2010-03240}, and some of the biases result in fairness concerns in recommender systems~\cite{DBLP:journals/corr/abs-2205-13619}, among which user fairness~\cite{DBLP:conf/www/LeonhardtAK18, DBLP:conf/www/LiCFGZ21} is a critical component.
%As indicated by~\cite{DBLP:conf/ismir/0001S18, DBLP:conf/wsdm/ZhuC22, DBLP:conf/wsdm/LiUH21} \citet{DBLP:conf/wsdm/LiUH21,DBLP:conf/wsdm/ZhuC22}, 
This fairness issue is a result of the varying quality of users' neighborhoods (groups of users with similar preferences) from which information is taken to train a CF model~\cite{DBLP:conf/wsdm/ZhuC22, DBLP:conf/wsdm/LiUH21}. The information collected from large, coherent, and information-rich neighborhoods will be the dominant one in steering the process of learning to recommend for all users. We refer to such dominant neighborhoods as \emph{mainstream}. Because the users belonging to such neighborhoods ---the \emph{mainstream users}--- are compatible with the learned model, they are optimally served.
For the \emph{non-mainstream} users, e.g. \emph{niche} groups who deviate from the mainstream and whose interaction information is therefore less rich~\citep{DBLP:conf/www/LiCFGZ21}, who are less active compared to the mainstream users~\cite{DBLP:conf/sigir/NaghiaeiRD22}, or where the preferences are not well pronounced, the neighborhoods cannot fully reflect their genuine preferences.
All this will make the non-mainstream users receive recommendations of a lower quality than the mainstream users. The difference in the quality of the CF model for these two user groups, further referred to as the \emph{mainstream bias}, will result in the continuous improvement of the performance for the mainstream group, and continuous decrease of the performance for the rest~\citep{DBLP:conf/kdd/LiuGZL19}.
%In some specific recommendation scenarios, like those involving political information, this may even escalate towards segregation or exclusion of online social communities and serious societal consequences in the real world, such as polarization~\citep{DBLP:conf/wsdm/RastegarpanahGC19, badami2021paris} and discrimination \citep{DBLP:conf/flairs/MansouryASDPM20}. 

While the issue of treating users differently by a recommender system in general has been addressed by a number of approaches, making for example assumptions about the relation between users' gender~\cite{DBLP:journals/ipm/MelchiorreRPBLS21} or demographics~\cite{DBLP:conf/fat/EkstrandTAEAMP18} and the quality of recommendation, not many approaches have focused specifically on addressing the mainstream bias. \citet{DBLP:conf/wsdm/LiUH21} deployed an autoencoder~\citep{rumelhart1985learning} for feature reconstruction as an adversary to a traditional CF model, forcing it to deviate from the pure similarity-based learning and make the learned model more compatible with the non-mainstream users. More specifically, the autoencoder was deployed to steer the process of learning the user/item representation space for rating prediction via optimal reconstruction of the properties of all users, mainstream and otherwise, assuming this would lead to equal treatment of users during recommendation. Still, a more explicit focus on the mainstreamness of users is needed to ensure that the bias is effectively addressed.

Inspired by outlier detection techniques, \citet{DBLP:conf/wsdm/ZhuC22} did focus on explicitly quantifying mainstreamness via similarities of user-preference profiles, and incorporated them to fine-tune the recommendation process for different user groups.
% \zhe{While their approach appears to be effective, with the lack of a clear definition or proxy of mainstreamness, they tried four different methods and the results shown in Fig.~\ref{fig:mvsm} indicate that these four methods do not always correlate well, and are sometimes even contradictory to each other. In other words, for the same user base with identical data distribution, an arbitrary user could be classified as mainstream by one method but as non-mainstream by another. In the meantime, since it is hard to get real-world labels on whether a user  is mainstream, they make the selection of the method to calculate mainstreamness based on synthetic data, which could be unstable as per the way of data generation and evaluation protocols.}\ju{not for the intro section. probably not a good idea to criticize their synthetic data results}
However, in the absence of ground truth data about mainstreamness, it is difficult to assess how well these approaches identify non-mainstream users. In addition, these mainstreamness statistics are model-agnostic in the sense that they are independent of the recommendation strategy, effectively ignoring the model's own capability to reduce the mainstream bias or even amplify it. As a result, the learning process could be tailored to the wrong users.

%\zhe{While their approach appears to be effective, the proposed strategy is still with limitations. They tried several different model-agnostic strategies (introduced in the next section). Also, with the lack of a clear definition or proxy of mainstreamness as well as labeled data, it is hard to tell which strategy measures user mainstreamness better. 
%Furthermore, the degree of mainstreamness should depend on the specific recommendation and its ability to properly model users and items. For example, it is reasonable to think of a user assessed as non-mainstream receiving both high and low recommendation utility under different models. All the concerns above call for an alternative way to consider mainstreamness that is 1) with a clear definition of how to measure it and show its effectiveness, and 2) \textsl{not} model agnostic and less sensitive to the properties of data at hand. 

% While their approach appears to be effective, its success is likely limited to situations where recommender systems are trained on rich datasets, enabling even the niche users to find a sufficient number of neighbors to rely on while learning the group-specific weights.
%\ju{fig 5 and table 2 do not support this anymore, so the sparsity thing should go away}\zhe{changed as above}
% Furthermore, the amount of mainstreamness depends on the specific recommendation and its ability to properly model users and items. For example, it is reasonable to think of a user assessed as non-mainstream receiving both high and low recommendation utility under different models.%\ju{move?}\zhe{even remove. I was changing this part since data sparsity is out of scope now. Trying to criticize this paper on the unclear definition of mainstreamness and inconsistency accordingly}\ju{the part about model-agnostic has to remain, also linked in the next para. In any case, our fig 3 may point to the key: both amazon datasets appear to have many nonmainstream users, according to both sim and util, so it seems more likely that basically any mainstream measure will improve}

%The methods mentioned above are based on the assumption that there is a positive correlation between the incorporated mechanisms and securing good recommendation quality for all users. We believe, however, that it is possible to gain more certainty in achieving this correlation, by helping the adversary to steer the learning process more explicitly towards the desired target. 
% However, with respect to \cite{DBLP:conf/wsdm/ZhuC22}, a more generic approach is needed to ensure success also in case of sparse datasets, where user similarity based on preference profiles cannot be deployed reliably and independently of the recommendation model.\ju{revise} This calls for an alternative way to consider mainstreamness that is \textsl{not} model agnostic and less sensitive to the properties of data at hand. 

%\ju{better emphasis on measurement of mainstreamness}

In this paper, we choose to focus there where the effect of mainstreamness is \emph{directly} observed, that is, the recommendation utility provided by the data and recommendation model at hand. 
%This is where mainstreamness will ultimately be reflected. The very nature of collaborative filtering tells us that 
If a user receives poor recommendations it could be because their preferences deviate from the rest, or because there is not enough data to properly quantify their similarity to other users or to fully exploit it. Therefore, we choose utility as an implicit proxy for mainstreamness.
Through this quantification of user mainstreamness, we make the training process focus on the non-mainstream ones by assigning them higher weights. 
%Since low mainstreamness makes users harder to find proper neighbors and thus influences the goodness of user modeling in collaborative filtering, it will lead to low recommendation accuracy. This makes recommendation accuracy
%, or better to say, the \emph{cost} of loosing recommendation accuracy, 
%a more direct and generic proxy to mainstreamness, not dependent on dataset-dependent user attributes. Therefore, in this paper, we investigate a method of explicitly assigning accuracy-related weights to the users during training. 
We do so, however, 
%while minimizing the \emph{cost} of distorting the balance in the recommendation utility across the user population. In other words, we are devising 
in a \emph{cost-sensitive} way~\cite{DBLP:conf/ijcnn/Thai-NgheGS10}, taking the cost of recommendation errors into account while training the CF model.
Our results show that our implicit measurement of mainstreamness via utility is better able to differentiate niche users than an explicit approach, and that the cost-sensitive learning strategy does mitigate the bias by balancing the recommendation quality across users.
Finally, we investigate data requirements for conducting research on mainstream bias at the individual user level, and provide suggestions for reliable experimentation in this area.
%~\citep{DBLP:journals/jcss/FreundS97, DBLP:journals/jss/LuLC17}.
%\ju{need to revise this paragraph for better positioning}
%The weights are derived from the recommendation accuracy of individual users on traditional recommendation models. 
%Also, contrary to [23], we evaluate the effectiveness of our strategy for each individual user, and not per mainstreamness category. 
%The contributions of this paper could be summarized as follows:
%\begin{itemize}
 %   \item We propose a generic, statistics-based method to mitigate the mainstream bias based on cost-sensitive learning~\cite{DBLP:conf/ijcnn/Thai-NgheGS10}, the effectiveness of which is demonstrated empirically;
 %   \item We investigate the requirement of the offline datasets to accurately indicate user mainstreamness and provide suggestions for the cost-sensitive strategy to work robustly.
    % Based on statistical transformations on their corresponding \textbf{ranking-oriented} scores from a baseline recommendation model, so that users with low utilities could get more focused during training.
%\end{itemize}
% To further resolve the shortcomings mentioned above, in this paper, 
% we propose another method based on cost-sensitive learning~\cite{DBLP:conf/ijcnn/Thai-NgheGS10}, focusing on taking the cost of potential errors into account while training a machine learning model~\cite{DBLP:journals/jcss/FreundS97, DBLP:journals/jss/LuLC17}.
% The importance weights assigned to users are derived
% based on statistical transformations on their corresponding \textbf{ranking-oriented} scores from a baseline recommendation model, so that users with low utilities could get more focused during training. In the meantime, with the importance weights related to individual user utilities, the accuracy and robustness of the weights directly rely on the data used for calculating the user-wise recommendation accuracy.
% Therefore, prior to exploring the cost-sensitive strategy, we first provide suggestions for the requirement of the offline datasets to accurately indicate user mainstreamness and thus get the cost-sensitive strategy to work. Then, following the concluded datasettings, We show the effectiveness of our strategy on user fairness and mainstream bias mitigation via extensive experiments on real-world datasets.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proposed Approach}\label{sec:cost}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Figure environment removed

The basis of our approach is a weighted loss function where every user $u\in\mathcal{U}$ is assigned a weight $\omega(u)$ that informs the learning process about the importance of every user's individual recommendation loss. The global loss is thus simply
\begin{equation}
\label{eq:overall}
        \mathcal{L} = \sum_{u\in\mathcal{U}}\omega(u) \mathcal{L}_R(u)~,
\end{equation}
where the recommendation loss $\mathcal{L}_R$ is specific of the model and learning paradigm. This way, we explicitly tell the learning process what users to optimize for by means of $\omega$, which, in our case, should be high for non-mainstream users and low for mainstream users.

\subsection{Definition of Weights} \label{subsec:def}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

As explained in the previous section, we define $\omega$ as a function of the user mainstreamness $m_u$. 
However, rather than simply using a naïve transformation of $m_u$, we introduce flexibility through a cost function that maps user mainstreamness onto a cost value. In particular, and assuming $m_u$ ranges between 0 and 1, we use the density function of a Normal distribution truncated between 0 and 1, with zero mean and variance adjusted to achieve a contrast ranging between 5 (i.e. users with mainstreamness $m_u=0$ have a cost 5 times as large as users with $m_u=1$) and 80 (ie. 80 times as much). This is a simple choice to make $\omega$ smooth and monotonically decreasing, but other cost functions that emphasize different levels of mainstreamness are of course possible; we leave this discussion for further work. Fig.~\ref{fig:weight} shows some examples. 
Nonetheless, the formulation of the cost function may consider various aspects tailored to the business case, as well as different magnitudes for the contrast between users with low and high mainstreamness. For example, it would be reasonable to assign very high weights to non-mainstream users with high activity, or to users with very low activity as an attempt to reduce the churn rate.

An important point to consider when defining $\omega$ is the distribution of mainstreamness across users. It could be the case that, given the current data and model, the least mainstream users are actually fairly mainstream already, so their weight relative to the most mainstream users should be adjusted via a smaller contrast. 
% It could also be the case that the recommendation task is simply too difficult and all users have a utility score in the range of, for example, $[0.1, 0.2]$, so that the cost function will hardly differentiate among them. Lastly, whether we chose a metric like $nDCG$ or $AP$ to measure utility will simply produce a different distribution of scores altogether, ultimately leading to a different set of weight values for the same users.
It could also be the case that the dataset is very sparse and there are simply not enough neighbors around users for the model to learn a good representation. That is, the majority of users could be considered non-mainstream, and as a result the cost function would hardly differentiate among them. Lastly, one could decide to compute $m_u$ in several different ways (see next Section), which could potentially lead to quite different mainstreamness score distributions altogether, ultimately leading to a different set of weight values even for the same users.

In order to minimize this dependence on the dataset and mainstreamness definition, and ensure that the full co-domain of the cost function is used, we first normalize the raw mainstreamness scores.
Simply re-scaling between the minimum and maximum could still lead to a disproportionate use of small parts of the co-domain, and would also be very sensitive to outlier users. Instead, we use the rank statistic of $m_u$ normalized in $[0,1]$. We achieve this by using the empirical cumulative distribution function (ecdf)
\begin{equation}
    \omega(u)=\mathrm{cost}(\mathrm{ecdf}_{\mathcal{U}}(m_u))~,\label{eq:weight}%\label{eq:ecdf}
\end{equation}
where, as mentioned, cost is defined in terms of a truncated Normal density function.

\subsection{Measurement of Mainstreamness}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Figure environment removed

\footnotetext{Data available from the authors' public repository at\\ \url{https://github.com/Zziwei/Measuring-Mitigating-Mainstream-Bias}.}

An \textbf{explicit} approach to compute $m_u$ would ideally follow some notion of mainstreamness, but mainstreamness is itself a complex construct very hard to define formally~\citep{DBLP:conf/ismir/0001S18, DBLP:conf/wsdm/ZhuC22, DBLP:conf/wsdm/LiUH21}. Recently, \citet{DBLP:conf/wsdm/ZhuC22} took inspiration from outlier detection techniques to propose four different definitions:
\begin{itemize}
    \item \textsf{Sim}: users are mainstream to the extent that their interactions are similar to that of the other users. The Jaccard coefficient is used to measure the average similarity between a user and all the others. %usrsdegree that \zhe{This method presumes that compared to mainstream users, non-mainstream users tend to interact with different items. Thus, the extent of overlap with respect to interacted items with other users is adopted as the measurement of mainstreamness and implemented by Jaccard similarity. An arbitrary user is thus considered non-mainstream when she has a low average Jaccard similarity with all other users. Considering a user-item interaction matrix formed by $M$ users and $N$ items, the time complexity of this strategy is $O({M^2}N)$.}
    \item \textsf{Den}: users are mainstream to the extent that there are enough close neighbors to calculate similarity with. The local outlier factor algorithm (LOF)~\cite{DBLP:conf/sigmod/BreunigKNS00} is used to identify niche users.%  \zhe{Since non-mainstream users usually have different behavioral patterns, they are not expected to find close neighbors, and thus the density of their K nearest neighbors (KNN) would be low. In \cite{DBLP:conf/wsdm/ZhuC22}, the additive inverse of Local Outlier Factor (LOF)  value is used to measure this, with non-mainstream users tending to have low scores. The time complexity of this strategy is the combination of the Euclidean distance calculation ($O({M^2}N)$) and KNN ($O(MN)$). Compared to \textsf{Sim}, the complexity goes significantly higher when there are a large number of users/items in the dataset.}
    \item \textsf{Dis}: users are mainstream to the extent that their interactions are common in the dataset, that is, they interact with popular items. The cosine similarity is used to measure the similarity between a user and the average user interactions.% \zhe{This method considers the distribution of user behaviors. It defines user mainstreamness as the degree of adherence to interact with items that are with more feedback from users, or in other words, more popular. Though logically similar to the similarity-based method, this strategy has a 1) closer connection to popularity, and 2) a different similarity measure (cosine similarity). The time complexity of this strategy is $O(MN)$}.
    \item \textsf{Deep}: similar to \textsf{Den}, niche users are identified by an outlier detection algorithm. In particular, the deep support vector data description algorithm (DeepSVDD)~\cite{DBLP:conf/icml/RuffGDSVBMK18} is used.% \zhe{In short for , \textsf{Deep} implements the non-mainstream user detection based on one-class classification. The latent embeddings of all mainstream users are expected to cluster in one hypersphere, and thus outliers are excluded and regarded as non-mainstream users. However, due to the lack of ground truth on whether a user is mainstream, \citet{DBLP:conf/wsdm/ZhuC22} involve all users for training, which means outliers also have impacts on the position and volume of the hypersphere. Therefore, the hypersphere inferred in this way will contain outliers and be noisy. The overall time complexity of DeepSVDD is no lower than $O(MNQ)$, where $Q$ is related to the number of trainable parameters in the deep neural network, and is usually much larger than $M$.}
\end{itemize}

%\zhe{In addition to all mentioned above, it is also difficult to}
However, it is difficult to assess how well these, or any other definitions for that matter, correlate with the concept of mainstreamness. To illustrate, Fig.~\ref{fig:mvsm} compares these four definitions as applied to the MovieLens 1M dataset. Although they are somewhat correlated to one another, it is evident that they produce very different scores. For instance, \textsf{Sim} and \textsf{Dis} lead to nicely shaped distributions, suggesting few users with extreme (non-)mainstreamness. However, \textsf{Den} and \textsf{Deep} lead to very skewed distributions, even in the opposite direction, pointing to many users with extreme scores. This shows that the same user could be considered both mainstream or non-mainstream, depending on how we choose to define mainstreamness. 
% \zhe{Therefore, given the unstable measurement of user similarity, neighbors and their distance to one arbitrary user are also unstable. It is thus tricky to deploy an effective Local Fine Tuning (LFT) process introduced in~\cite{DBLP:conf/wsdm/ZhuC22}, which trains sub-models for users and their nearest neighbors.}

Furthermore, it should be noted that these four definitions of mainstreamness are agnostic to the recommendation model. However, the effect of mainstreamness, ultimately, depends on the model and how it is able to exploit the specifics of the dataset it is trained on. It is not far-fetched to think of a user, assessed as non-mainstream, who receives bad recommendations under one model but good recommendations under a more capable one.

This leads us to consider an alternative, \textbf{implicit} way to quantify mainstreamness that is \textsl{not} model agnostic. In particular, we decide to focus there where the effect of mainstreamness is to be observed, that is, the recommendation \emph{utility} provided by the recommendation model at hand. This is where mainstreamness will ultimately have an impact on. The very nature of collaborative filtering tells us that if a user receives poor recommendations it is because they are non-mainstream under the current model: they cannot be properly represented, either because their preferences are somehow different from their closest neighbors, or because there are not enough data to properly quantify their similarity. Therefore, we use utility as a proxy for mainstreamness.
Since utility, just like mainstreamness, is a complex concept difficult to measure, we decide to simply use the accuracy of the recommendation model for that user, measured through a metric like $nDCG$ or $AP$. 

But there is the question of what accuracy scores we actually use. In principle, these scores should reflect user mainstreamness when there is no mechanism to minimize its effect, and they should be achieved by the recommendation model in the dataset at hand. Therefore, we decide to use the accuracy achieved, \textsl{on a validation set}, by the vanilla model whose loss function is as in Eq.~\eqref{eq:overall} but using no weights. As intended, we thus first see how the model reacts to mainstreamness as reflected in the observed utility for users, and then act upon it in a cost-sensitive way.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experimental Design} \label{sec:exp}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table}[t]
\caption{Dataset statistics after pre-filtering.}
    \centering{\small\setlength{\tabcolsep}{4pt}
    \begin{tabular}{@{}lrrrr@{}}
        \hline
        Dataset & \#users & \#items & \#ratings & Density \\
        \hline
        MovieLens 1M~\cite{DBLP:journals/tiis/HarperK16} & 6,040 & 3,609 & 562,957 & 2.583\% \\
        BeerAdvocate~\cite{DBLP:conf/icdm/McAuleyLJ12} & 8,821 & 43,663 & 780,752 & 0.203\% \\
        Amazon Digital Music~\cite{DBLP:conf/emnlp/NiLM19} & 14,057 & 379,171 & 619,673 & 0.011\% \\
        Amazon Musical Instruments ~\cite{DBLP:conf/emnlp/NiLM19} & 15,270 & 585,766 & 862,798 & 0.010\% \\
        \hline
      \end{tabular}}
    \label{tab:stats}
\end{table}



% Figure environment removed

% Figure environment removed


%\zhe{We stay in the focus of a direct intervention on the importance of users as stated in Eq.~\ref{eq:ecdf}, and }
We carried out a number of experiments to investigate the effectiveness of the proposed approach in mitigating the mainstreamness bias, as well as the effect of the contrast applied by the cost function.
In particular, we study contrasts x5, x10, x20, x50 and x80, that is, the most non-mainstream user has a weight between 5 and 80 times larger than that of the most mainstream user. Fig.~\ref{fig:weight} details the cost functions. Regarding the measurement of mainstreamness, we consider both an explicit and an implicit quantification. For the former, we follow \citet{DBLP:conf/wsdm/ZhuC22} and compute \textsf{Sim} scores. This choice is motivated by the time complexity of their four approaches (the computation of mainstreamness may quickly become intractable as the numbers of users and items increase; while their datasets include a few thousand items, ours span from a few thousands to over half a million), and their correlation to one another (\textsf{Sim} is also the one most correlated with the others, in particular with \textsf{Deep}). For the implicit quantification we compute utility scores using the metric $nDCG$ as an exemplar of recommender systems research; hereafter, we will refer to this definition of mainstreamness as \textsf{Util}. 
%Among the methods used for comparison, as indicated in \ref{subsec:def}, four strategies proposed in~\cite{DBLP:conf/wsdm/ZhuC22} are not consistent with each other, and here we only choose the most promising one based on our analysis. We see from Fig.~\ref{fig:mvsm} that the mainstreamness scores generated by \textsf{Sim} and \textsf{Dis} have the best time complexity and correlate well with not only each other, but also with those values from the other 2 strategies. While choosing between them, we see from the diagonal that \textsf{Sim} tags more users in the middle while \textsf{Dis} counterintuitively sees more users as non-mainstream. Based on all insights statements above, we choose to use \textsf{Sim} as the strategy for comparing with \textsf{Util} with respect to the strategy of assigning  mainstreamness-related importance weights to users. 


We selected four real-world datasets containing user-item rating interactions from various domains and with different densities, especially including some highly sparse datasets (see Table~\ref{tab:stats}). In line with common practice in ranking-oriented recommender systems research, we see all existing interactions in the datasets as relevant, and all other unseen interactions as irrelevant. 
We use LensKit~\cite{DBLP:conf/cikm/Ekstrand20} to evenly split the relevant items for each user into training, validation and test sets. To make the modeling of utility ---and hence mainstreamness--- robust, each user has at least five relevant transactions in each of the three sets; we explain the rationale for this decision in Section~\ref{sec:discussion}.
For training the model, we follow \citet{DBLP:conf/www/HeLZNHC17, DBLP:conf/ijcai/WuWH22} and randomly sample four irrelevant items per relevant item in the training partition. 
For validation and test, we follow DaisyRec~\cite{DBLP:conf/recsys/SunY00Q0G20} and evaluate the model for each user by ranking a total of 500 items consisting of their relevant items in the validation/test partition and a set of randomly sampled irrelevant items.
Finally, to make sure relevant items are the minority, as happens in reality, we truncate the number of relevant interactions to 200.
The dataset statistics after processing are shown in Table~\ref{tab:stats}. 

Regarding the recommendation model, we deploy a simple but effective CF model that only utilizes user-item interactions.
Specifically, we choose Factorization Machines (FM) \cite{DBLP:conf/icdm/Rendle10}, which optimize the binary cross-entropy (BCE) loss via the Adaptive Moment Estimation (Adam)~\cite{DBLP:journals/corr/KingmaB14} learner, and leave the investigation on other training paradigms for future work. For each user, the BCE loss is normalized by dividing by the total number of relevant and irrelevant items used for training, so that all user losses are on the same scale in~\eqref{eq:overall}. 
After a fine-tuning process based on grid search, we fixed several key hyper-parameters including the dimension of vectors used for interaction (32), learning rate (0.0001), L2-regularization coefficient to avoid overfitting (0.001), and batch size (512). 

All models are trained for 300 epochs to ensure full convergence, and with 3 different random initializations to minimize random effects due to the sampling process. The whole pipeline is implemented in PyTorch \cite{DBLP:conf/nips/PaszkeGMLBCKLGA19}, and all experiments are run on one NVIDIA GeForce GTX 2080Ti GPU \footnote{All data, code and results are available at\\\url{https://github.com/roger-zhe-li/ictir23-cost-sensitive}.}. 

\newcommand{\sig}[1]{{#1}}
\begin{table*}[!th]
\caption{Mean nDCG of the baseline model (FM) per user group, and relative percentage improvement of each cost-sensitive model (e.g. users in group `low' of MovieLens 1M received a score of .3284 with the baseline, and an improvement of +3.89\% with the x80-contrast cost-sensitive model under the \textsf{Util} mainstreamness definition). Column `Overall' lists the mean across all users. Green/red for statistically significant gain/loss with respect to the baseline (hierarchical linear model with seed and user random effects, Bonferroni correction).}\label{tab:overall_performance}
\footnotesize\setlength{\tabcolsep}{1.9pt}
\begin{tabular}{|cr|r|lllll|r|lllll|r|lllll|r|lllll|}
\cline{3-26}
\multicolumn{2}{c|}{} & \multicolumn{6}{c|}{MovieLens 1M} & \multicolumn{6}{c|}{BeerAdvocate} & \multicolumn{6}{c|}{Amazon Digital Music} & \multicolumn{6}{c|}{Amazon Musical Instruments} \\
\cline{3-26}
\multicolumn{2}{c|}{} & & & med- & & med- & & & & med- & & med- & & & & med- & & med- & & & & med- & & med- & \\
\multicolumn{2}{c|}{} & Overall & low & low & med & high & high & Overall & low & low & med & high & high & Overall & low & low & med & high & high & Overall & low & low & med & high & high \\
\hline

\multicolumn{2}{|c|}{FM} & .5531 & .3284 & .4621 & .5753 & .6613 & .7388 & .6887 & .4144 & .6051 & .7301 & .8132 & .8809 & .3456 & .2324 & .2695 & .3145 & .3828 & .5289 & .3606 & .2348 & .2772 & .3276 & .4085 & .5552 \\ \hline

\multirow{5}{*}{\rotatebox[origin=c]{90}{Sim}} & x5  & {\color[HTML]{9A0000} \sig{.5465}} & {-0.36}       & {\color[HTML]{9A0000} \sig{-0.89}} & {\color[HTML]{9A0000} \sig{-1.62}} & {\color[HTML]{9A0000} \sig{-1.3}}  & {\color[HTML]{9A0000} \sig{-1.33}} & {\color[HTML]{9A0000} \sig{.6792}} & {-0.52}       & {\color[HTML]{9A0000} \sig{-1.72}} & {\color[HTML]{9A0000} \sig{-1.83}} & {\color[HTML]{9A0000} \sig{-1.44}} & {\color[HTML]{9A0000} \sig{-1.13}} & {\color[HTML]{9A0000} \sig{.3395}} & {\color[HTML]{009901} \sig{+0.65}} & {+0.05}       & {\color[HTML]{9A0000} \sig{-0.61}} & {\color[HTML]{9A0000} \sig{-1.8}}  & {\color[HTML]{9A0000} \sig{-4.45}}  & {\color[HTML]{9A0000} \sig{.3581}} & {\color[HTML]{009901} \sig{+1.61}} & {\color[HTML]{009901} \sig{+0.77}} & {-0.09}       & {\color[HTML]{9A0000} \sig{-1.06}} & {\color[HTML]{9A0000} \sig{-2.5}}  \\
 & x10 & {\color[HTML]{9A0000} \sig{.5437}} & {-0.47}       & {\color[HTML]{9A0000} \sig{-1.32}} & {\color[HTML]{9A0000} \sig{-2.23}} & {\color[HTML]{9A0000} \sig{-1.87}} & {\color[HTML]{9A0000} \sig{-1.94}} & {\color[HTML]{9A0000} \sig{.6734}} & {\color[HTML]{9A0000} \sig{-0.81}} & {\color[HTML]{9A0000} \sig{-2.87}} & {\color[HTML]{9A0000} \sig{-2.94}} & {\color[HTML]{9A0000} \sig{-2.27}} & {\color[HTML]{9A0000} \sig{-1.8}}  & {\color[HTML]{9A0000} \sig{.3368}} & {\color[HTML]{009901} \sig{+0.99}} & {+0.1}        & {\color[HTML]{9A0000} \sig{-1}}    & {\color[HTML]{9A0000} \sig{-2.67}} & {\color[HTML]{9A0000} \sig{-6.27}}  & {\color[HTML]{9A0000} \sig{.3577}} & {\color[HTML]{009901} \sig{+3.29}} & {\color[HTML]{009901} \sig{+1.44}} & {+0.08}       & {\color[HTML]{9A0000} \sig{-1.68}} & {\color[HTML]{9A0000} \sig{-3.54}} \\
 & x20 & {\color[HTML]{9A0000} \sig{.541}}  & {-0.53}       & {\color[HTML]{9A0000} \sig{-1.76}} & {\color[HTML]{9A0000} \sig{-2.85}} & {\color[HTML]{9A0000} \sig{-2.41}} & {\color[HTML]{9A0000} \sig{-2.51}} & {\color[HTML]{9A0000} \sig{.6666}} & {\color[HTML]{9A0000} \sig{-1.35}} & {\color[HTML]{9A0000} \sig{-4.23}} & {\color[HTML]{9A0000} \sig{-4.13}} & {\color[HTML]{9A0000} \sig{-3.22}} & {\color[HTML]{9A0000} \sig{-2.56}} & {\color[HTML]{9A0000} \sig{.3347}} & {\color[HTML]{009901} \sig{+1.31}} & {+0.09}       & {\color[HTML]{9A0000} \sig{-1.3}}  & {\color[HTML]{9A0000} \sig{-3.46}} & {\color[HTML]{9A0000} \sig{-7.69}}  & {\color[HTML]{9A0000} \sig{.3576}} & {\color[HTML]{009901} \sig{+3.33}} & {\color[HTML]{009901} \sig{+1.44}} & {+0.04}       & {\color[HTML]{9A0000} \sig{-1.72}} & {\color[HTML]{9A0000} \sig{-3.58}} \\
 & x50 & {\color[HTML]{9A0000} \sig{.5376}} & {-0.67}       & {\color[HTML]{9A0000} \sig{-2.28}} & {\color[HTML]{9A0000} \sig{-3.64}} & {\color[HTML]{9A0000} \sig{-3.06}} & {\color[HTML]{9A0000} \sig{-3.26}} & {\color[HTML]{9A0000} \sig{.6588}} & {\color[HTML]{9A0000} \sig{-2.01}} & {\color[HTML]{9A0000} \sig{-5.65}} & {\color[HTML]{9A0000} \sig{-5.56}} & {\color[HTML]{9A0000} \sig{-4.29}} & {\color[HTML]{9A0000} \sig{-3.54}} & {\color[HTML]{9A0000} \sig{.3328}} & {\color[HTML]{009901} \sig{+1.63}} & {+0.07}       & {\color[HTML]{9A0000} \sig{-1.6}}  & {\color[HTML]{9A0000} \sig{-4.13}} & {\color[HTML]{9A0000} \sig{-8.92}}  & {\color[HTML]{9A0000} \sig{.3576}} & {\color[HTML]{009901} \sig{+3.37}} & {\color[HTML]{009901} \sig{+1.45}} & {+0.03}       & {\color[HTML]{9A0000} \sig{-1.75}} & {\color[HTML]{9A0000} \sig{-3.6}}  \\
 & x80 & {\color[HTML]{9A0000} \sig{.5359}} & {-0.67}       & {\color[HTML]{9A0000} \sig{-2.55}} & {\color[HTML]{9A0000} \sig{-4.06}} & {\color[HTML]{9A0000} \sig{-3.39}} & {\color[HTML]{9A0000} \sig{-3.59}} & {\color[HTML]{9A0000} \sig{.6548}} & {\color[HTML]{9A0000} \sig{-2.55}} & {\color[HTML]{9A0000} \sig{-6.39}} & {\color[HTML]{9A0000} \sig{-6.17}} & {\color[HTML]{9A0000} \sig{-4.84}} & {\color[HTML]{9A0000} \sig{-4.08}} & {\color[HTML]{9A0000} \sig{.3316}} & {\color[HTML]{009901} \sig{+3.66}} & {\color[HTML]{009901} \sig{+0.9}}  & {\color[HTML]{9A0000} \sig{-1.78}} & {\color[HTML]{9A0000} \sig{-5.07}} & {\color[HTML]{9A0000} \sig{-10.62}} & {\color[HTML]{9A0000} \sig{.3576}} & {\color[HTML]{009901} \sig{+3.39}} & {\color[HTML]{009901} \sig{+1.45}} & {+0.02}       & {\color[HTML]{9A0000} \sig{-1.77}} & {\color[HTML]{9A0000} \sig{-3.61}} \\ \hline

\multirow{5}{*}{\rotatebox[origin=c]{90}{Util}} & x5  & {\color[HTML]{009901} \sig{.5567}} & {\color[HTML]{009901} \sig{+1.67}} & {\color[HTML]{009901} \sig{+1.81}} & {\color[HTML]{009901} \sig{+0.63}} & {+0.16}       & {-0.13}       & {\color[HTML]{9A0000} \sig{.6846}} & {+0.63}       & {\color[HTML]{9A0000} \sig{-0.41}} & {\color[HTML]{9A0000} \sig{-0.94}} & {\color[HTML]{9A0000} \sig{-0.83}} & {\color[HTML]{9A0000} \sig{-0.77}} & {.3454}       & {\color[HTML]{009901} \sig{+1.59}} & {\color[HTML]{009901} \sig{+1.43}} & {\color[HTML]{009901} \sig{+1.2}}  & {\color[HTML]{009901} \sig{+0.4}}  & {\color[HTML]{9A0000} \sig{-2.58}}  & {.3607}       & {\color[HTML]{009901} \sig{+1.1}}  & {\color[HTML]{009901} \sig{+0.96}} & {\color[HTML]{009901} \sig{+0.62}} & {-0.04}       & {\color[HTML]{9A0000} \sig{-1.19}} \\
 & x10 & {\color[HTML]{009901} \sig{.5574}} & {\color[HTML]{009901} \sig{+2.38}} & {\color[HTML]{009901} \sig{+2.34}} & {\color[HTML]{009901} \sig{+0.7}}  & {+0.11}       & {\color[HTML]{9A0000} \sig{-0.27}} & {\color[HTML]{9A0000} \sig{.6807}} & {+0.44}       & {\color[HTML]{9A0000} \sig{-1.19}} & {\color[HTML]{9A0000} \sig{-1.66}} & {\color[HTML]{9A0000} \sig{-1.39}} & {\color[HTML]{9A0000} \sig{-1.27}} & {.3453}       & {\color[HTML]{009901} \sig{+2.54}} & {\color[HTML]{009901} \sig{+2.09}} & {\color[HTML]{009901} \sig{+1.53}} & {+0.18}       & {\color[HTML]{9A0000} \sig{-3.5}}   & {.3607}       & {\color[HTML]{009901} \sig{+2}}    & {\color[HTML]{009901} \sig{+1.52}} & {\color[HTML]{009901} \sig{+0.78}} & {-0.32}       & {\color[HTML]{9A0000} \sig{-1.8}}  \\
 & x20 & {\color[HTML]{009901} \sig{.5579}} & {\color[HTML]{009901} \sig{+3.05}} & {\color[HTML]{009901} \sig{+2.87}} & {\color[HTML]{009901} \sig{+0.73}} & {0}           & {\color[HTML]{9A0000} \sig{-0.48}} & {\color[HTML]{9A0000} \sig{.6762}} & {+0.54}       & {\color[HTML]{9A0000} \sig{-2.11}} & {\color[HTML]{9A0000} \sig{-2.67}} & {\color[HTML]{9A0000} \sig{-2.09}} & {\color[HTML]{9A0000} \sig{-1.74}} & {.3454}       & {\color[HTML]{009901} \sig{+3.59}} & {\color[HTML]{009901} \sig{+2.78}} & {\color[HTML]{009901} \sig{+1.64}} & {+0.11}       & {\color[HTML]{9A0000} \sig{-4.25}}  & {.3607}       & {\color[HTML]{009901} \sig{+2.63}} & {\color[HTML]{009901} \sig{+1.93}} & {\color[HTML]{009901} \sig{+0.8}}  & {\color[HTML]{9A0000} \sig{-0.53}} & {\color[HTML]{9A0000} \sig{-2.08}} \\
 & x50 & {\color[HTML]{009901} \sig{.5579}} & {\color[HTML]{009901} \sig{+3.62}} & {\color[HTML]{009901} \sig{+3.31}} & {\color[HTML]{009901} \sig{+0.68}} & {-0.21}       & {\color[HTML]{9A0000} \sig{-0.84}} & {\color[HTML]{9A0000} \sig{.6722}} & {\color[HTML]{009901} \sig{+2}}    & {\color[HTML]{9A0000} \sig{-3.09}} & {\color[HTML]{9A0000} \sig{-3.86}} & {\color[HTML]{9A0000} \sig{-2.89}} & {\color[HTML]{9A0000} \sig{-2.32}} & {.3458}       & {\color[HTML]{009901} \sig{+4.84}} & {\color[HTML]{009901} \sig{+3.67}} & {\color[HTML]{009901} \sig{+1.92}} & {-0.11}       & {\color[HTML]{9A0000} \sig{-4.9}}   & {.3608}       & {\color[HTML]{009901} \sig{+3.94}} & {\color[HTML]{009901} \sig{+2.48}} & {\color[HTML]{009901} \sig{+0.85}} & {\color[HTML]{9A0000} \sig{-0.83}} & {\color[HTML]{9A0000} \sig{-2.66}} \\
 & x80 & {\color[HTML]{009901} \sig{.5577}} & {\color[HTML]{009901} \sig{+3.89}} & {\color[HTML]{009901} \sig{+3.47}} & {\color[HTML]{009901} \sig{+0.63}} & {\color[HTML]{9A0000} \sig{-0.32}} & {\color[HTML]{9A0000} \sig{-1.05}} & {\color[HTML]{9A0000} \sig{.6715}} & {\color[HTML]{009901} \sig{+2.98}} & {\color[HTML]{9A0000} \sig{-3.2}}  & {\color[HTML]{9A0000} \sig{-4.25}} & {\color[HTML]{9A0000} \sig{-3.14}} & {\color[HTML]{9A0000} \sig{-2.54}} & {.346}        & {\color[HTML]{009901} \sig{+5.45}} & {\color[HTML]{009901} \sig{+4}}    & {\color[HTML]{009901} \sig{+2.02}} & {-0.18}       & {\color[HTML]{9A0000} \sig{-5.15}}  & {.3608}       & {\color[HTML]{009901} \sig{+4.48}} & {\color[HTML]{009901} \sig{+2.64}} & {\color[HTML]{009901} \sig{+0.88}} & {\color[HTML]{9A0000} \sig{-0.97}} & {\color[HTML]{9A0000} \sig{-2.86}} \\ \hline
\end{tabular}
\end{table*}

% Figure environment removed

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results} \label{sec:res}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%As mentioned in Section~\ref{sec:exp}, we are keen on the effect of 1) mainstream bias mitigation brought by the cost-sensitive strategy; 2) the contrast provided by the sharpness of the $cost$ function, with the presumption that a sharper $cost$ could yield better accuracy for non-mainstream users, in exchange of more performance loss on mainstream users. An optimal contrast is thus expected to bring a better balance between the utilities of mainstream and non-mainstream users without significantly hurting the overall recommendation accuracy. 
%An indication of the weight curve as per the quantile of nDCG scores acquired from the vanilla FM model is shown in Fig.~\ref{fig:weight}.

\subsection{Mainstreamness and Utility}\label{subsec:m_and_u}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


We first examine how \Sim and \Util differentiate between mainstream and non-mainstream users. In particular, we are interested in how well they correlate with the test $nDCG$ scores obtained by the baseline FM model: non-mainstream users should receive recommendations with low $nDCG$ scores, while mainstream users should receive higher scores.

For each of the four datasets, Fig.~\ref{fig:simvsutil} compares \Sim and \Util. We can first see that both approaches lead to similar distributions in the Amazon datasets, where there appear to be many non-mainstream users. However, they somewhat disagree in the BeerAdvocate dataset, where \Util does not identify many non-mainstream users to benefit from the cost-sensitive approach. In terms of correlation with the test $nDCG$ scores, we can see that \Util is much better correlated, specially in the Amazon datasets. This points to the possibility that \Sim identifies many non-mainstream users to which the model is still able to offer good recommendations. If the training process increases their importance by assigning them a high weight $\omega$, we may loose the opportunity to focus on those users that still receive poor recommendations.

In order to assess the effectiveness of the cost-sensitive approach for the mitigation of the mainstream bias, we will look in the next Section into different groups of users separated by their mainstreamness: group `low' contains the 20\% of users with lowest mainstreamness scores on the baseline model, group `med-low' contains the next 20\% or users, group `med' contains the middle 20\% of users, and so on with groups `med-high' and `high'. An effective mitigation of the mainstream bias would be reflected in increased performance for the lower groups, which ideally should be those with lowest test $nDCG$ scores in the baseline model.
Fig.~\ref{fig:boxes} shows how well \Sim and \Util separate users in these five groups. We can first see that the groups are indeed correlated with $nDCG$, but we can notice that this correlation is stronger with \Util, specially in the Amazon datasets (the low groups receive lower utility, and the higher groups receive higher utility). We can also see that groups tend to overlap substantially when separated by \Sim, potentially misplacing users. This overlap can be quantified by an ANOVA model of $nDCG$ modeled by two factors: dataset and user-group nested within dataset. Indeed, the user-group effect has a much larger sum of squares (SS) with \Util than with \Sim (SS=440 vs SS=218; SS of the dataset effect is 843).
Finally, Fig.~\ref{fig:boxes} also points that the BeerAdvocate dataset may be hard to further optimize for because the utility scores are already relatively high.

\subsection{Bias Mitigation}\label{ssec:mitigation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\newcommand{\sig}[1]{{#1}}
\begin{table*}[!th]
\caption{Same as Table~\ref{tab:overall_performance}, but user groups defined by \Sim scores instead of test nDCG in the baseline model.}\label{tab:overall_performance_by_ms}%Mean nDCG of the baseline model (FM) per user group, and relative percentage improvement of each cost-sensitive model (e.g. users in group `low' of MovieLens 1M received a score of .3284 with the baseline, and an improvement of +3.89\% with the x80-contrast cost-sensitive model under the \textsf{Util} mainstreamness definition). Column `Overall' lists the mean across all users. Green/red for statistically significant gain/loss with respect to the baseline (hierarchical linear model with seed and user random effects, Bonferroni correction).}\label{tab:overall_performance
\footnotesize\setlength{\tabcolsep}{1.9pt}
\begin{tabular}{|cr|r|lllll|r|lllll|r|lllll|r|lllll|}
\cline{3-26}
\multicolumn{2}{c|}{} & \multicolumn{6}{c|}{MovieLens 1M} & \multicolumn{6}{c|}{BeerAdvocate} & \multicolumn{6}{c|}{Amazon Digital Music} & \multicolumn{6}{c|}{Amazon Musical Instruments} \\
\cline{3-26}
\multicolumn{2}{c|}{} & & & med- & & med- & & & & med- & & med- & & & & med- & & med- & & & & med- & & med- & \\
\multicolumn{2}{c|}{} & Overall & low & low & med & high & high & Overall & low & low & med & high & high & Overall & low & low & med & high & high & Overall & low & low & med & high & high \\
\hline

\multicolumn{2}{|c|}{FM} & .5531 & .3284 & .4621 & .5753 & .6613 & .7388 & .6887 & .4144 & .6051 & .7301 & .8132 & .8809 & .3456 & .2324 & .2695 & .3145 & .3828 & .5289 & .3606 & .2348 & .2772 & .3276 & .4085 & .5552 \\ \hline


\multirow{5}{*}{\rotatebox[origin=c]{90}{Sim}} & x5  & {\color[HTML]{9A0000} \sig{.5465}} & -0.62                              & -0.97                              & {\color[HTML]{9A0000} \sig{-1.16}} & {\color[HTML]{9A0000} \sig{-1.34}} & {\color[HTML]{9A0000} \sig{-1.58}} & {\color[HTML]{9A0000} \sig{.6792}} & {\color[HTML]{9A0000} \sig{-1.4}}  & {\color[HTML]{9A0000} \sig{-1.39}} & {\color[HTML]{9A0000} \sig{-1.43}} & {\color[HTML]{9A0000} \sig{-1.34}} & {\color[HTML]{9A0000} \sig{-1.35}} & {\color[HTML]{9A0000} \sig{.3395}} & -0.04 & {\color[HTML]{9A0000} \sig{-0.61}} & {\color[HTML]{9A0000} \sig{-1.37}} & {\color[HTML]{9A0000} \sig{-2.16}} & {\color[HTML]{9A0000} \sig{-4.05}} & {\color[HTML]{9A0000} \sig{.3581}} & +0.01 & -0.16 & {\color[HTML]{9A0000} \sig{-0.52}} & {\color[HTML]{9A0000} \sig{-0.89}} & {\color[HTML]{9A0000} \sig{-1.65}} \\
 & x10 & {\color[HTML]{9A0000} \sig{.5437}} & -0.84                              & {\color[HTML]{9A0000} \sig{-1.42}} & {\color[HTML]{9A0000} \sig{-1.63}} & {\color[HTML]{9A0000} \sig{-1.87}} & {\color[HTML]{9A0000} \sig{-2.31}} & {\color[HTML]{9A0000} \sig{.6734}} & {\color[HTML]{9A0000} \sig{-2.23}} & {\color[HTML]{9A0000} \sig{-2.4}}  & {\color[HTML]{9A0000} \sig{-2.23}} & {\color[HTML]{9A0000} \sig{-2.14}} & {\color[HTML]{9A0000} \sig{-2.16}} & {\color[HTML]{9A0000} \sig{.3368}} & 0     & {\color[HTML]{9A0000} \sig{-0.86}} & {\color[HTML]{9A0000} \sig{-2.03}} & {\color[HTML]{9A0000} \sig{-3.18}} & {\color[HTML]{9A0000} \sig{-5.71}} & {\color[HTML]{9A0000} \sig{.3577}} & +0.26 & -0.08 & {\color[HTML]{9A0000} \sig{-0.53}} & {\color[HTML]{9A0000} \sig{-1.14}} & {\color[HTML]{9A0000} \sig{-2.11}} \\
 & x20 & {\color[HTML]{9A0000} \sig{.541}}  & -1.19                              & {\color[HTML]{9A0000} \sig{-1.83}} & {\color[HTML]{9A0000} \sig{-2.12}} & {\color[HTML]{9A0000} \sig{-2.35}} & {\color[HTML]{9A0000} \sig{-2.96}} & {\color[HTML]{9A0000} \sig{.6666}} & {\color[HTML]{9A0000} \sig{-3.42}} & {\color[HTML]{9A0000} \sig{-3.54}} & {\color[HTML]{9A0000} \sig{-3.17}} & {\color[HTML]{9A0000} \sig{-2.95}} & {\color[HTML]{9A0000} \sig{-3.06}} & {\color[HTML]{9A0000} \sig{.3347}} & -0.05 & {\color[HTML]{9A0000} \sig{-1.15}} & {\color[HTML]{9A0000} \sig{-2.52}} & {\color[HTML]{9A0000} \sig{-4.01}} & {\color[HTML]{9A0000} \sig{-6.97}} & {\color[HTML]{9A0000} \sig{.3576}} & +0.24 & -0.11 & {\color[HTML]{9A0000} \sig{-0.57}} & {\color[HTML]{9A0000} \sig{-1.16}} & {\color[HTML]{9A0000} \sig{-2.14}} \\
 & x50 & {\color[HTML]{9A0000} \sig{.5376}} & {\color[HTML]{9A0000} \sig{-1.49}} & {\color[HTML]{9A0000} \sig{-2.35}} & {\color[HTML]{9A0000} \sig{-2.71}} & {\color[HTML]{9A0000} \sig{-3.02}} & {\color[HTML]{9A0000} \sig{-3.84}} & {\color[HTML]{9A0000} \sig{.6588}} & {\color[HTML]{9A0000} \sig{-4.71}} & {\color[HTML]{9A0000} \sig{-4.74}} & {\color[HTML]{9A0000} \sig{-4.31}} & {\color[HTML]{9A0000} \sig{-3.92}} & {\color[HTML]{9A0000} \sig{-4.2}}  & {\color[HTML]{9A0000} \sig{.3328}} & -0.17 & {\color[HTML]{9A0000} \sig{-1.42}} & {\color[HTML]{9A0000} \sig{-3.01}} & {\color[HTML]{9A0000} \sig{-4.67}} & {\color[HTML]{9A0000} \sig{-8}}    & {\color[HTML]{9A0000} \sig{.3576}} & +0.24 & -0.09 & {\color[HTML]{9A0000} \sig{-0.58}} & {\color[HTML]{9A0000} \sig{-1.18}} & {\color[HTML]{9A0000} \sig{-2.16}} \\
 & x80 & {\color[HTML]{9A0000} \sig{.5359}} & {\color[HTML]{9A0000} \sig{-1.76}} & {\color[HTML]{9A0000} \sig{-2.68}} & {\color[HTML]{9A0000} \sig{-2.95}} & {\color[HTML]{9A0000} \sig{-3.32}} & {\color[HTML]{9A0000} \sig{-4.19}} & {\color[HTML]{9A0000} \sig{.6548}} & {\color[HTML]{9A0000} \sig{-5.34}} & {\color[HTML]{9A0000} \sig{-5.29}} & {\color[HTML]{9A0000} \sig{-4.83}} & {\color[HTML]{9A0000} \sig{-4.46}} & {\color[HTML]{9A0000} \sig{-4.91}} & {\color[HTML]{9A0000} \sig{.3316}} & +0.03 & {\color[HTML]{9A0000} \sig{-1.52}} & {\color[HTML]{9A0000} \sig{-3.34}} & {\color[HTML]{9A0000} \sig{-5.18}} & {\color[HTML]{9A0000} \sig{-8.87}} & {\color[HTML]{9A0000} \sig{.3576}} & +0.23 & -0.1  & {\color[HTML]{9A0000} \sig{-0.59}} & {\color[HTML]{9A0000} \sig{-1.18}} & {\color[HTML]{9A0000} \sig{-2.17}} \\ \hline

\multirow{5}{*}{\rotatebox[origin=c]{90}{Util}} & x5  & {\color[HTML]{009901} \sig{.5567}} & {\color[HTML]{009901} \sig{+1.5}}  & +0.99                              & +0.53                              & +0.32                              & +0.26                              & {\color[HTML]{9A0000} \sig{.6846}} & -0.46                              & -0.5                               & -0.51                              & {\color[HTML]{9A0000} \sig{-0.66}} & {\color[HTML]{9A0000} \sig{-0.72}} & 0.3454                             & +0.12 & +0.11                              & -0.16                              & +0.07                              & -0.3                               & 0.3607                             & -0.01 & +0.06 & 0                                  & -0.04                              & +0.12                              \\
 & x10 & {\color[HTML]{009901} \sig{.5574}} & {\color[HTML]{009901} \sig{+2.05}} & {\color[HTML]{009901} \sig{+1.24}} & +0.67                              & +0.33                              & +0.2                               & {\color[HTML]{9A0000} \sig{.6807}} & -1.11                              & {\color[HTML]{9A0000} \sig{-1.16}} & {\color[HTML]{9A0000} \sig{-1.1}}  & {\color[HTML]{9A0000} \sig{-1.19}} & {\color[HTML]{9A0000} \sig{-1.22}} & 0.3453                             & +0.13 & +0.06                              & -0.11                              & +0                                 & -0.43                              & 0.3607                             & 0     & +0.12 & +0.01                              & -0.06                              & +0                                 \\
 & x20 & {\color[HTML]{009901} \sig{.5579}} & {\color[HTML]{009901} \sig{+2.52}} & {\color[HTML]{009901} \sig{+1.59}} & +0.71                              & +0.27                              & +0.08                              & {\color[HTML]{9A0000} \sig{.6762}} & {\color[HTML]{9A0000} \sig{-1.85}} & {\color[HTML]{9A0000} \sig{-2.03}} & {\color[HTML]{9A0000} \sig{-1.79}} & {\color[HTML]{9A0000} \sig{-1.73}} & {\color[HTML]{9A0000} \sig{-1.72}} & 0.3454                             & +0.11 & +0.06                              & -0.06                              & +0.15                              & -0.5                               & 0.3607                             & 0     & +0.14 & +0.05                              & -0.08                              & +0.01                              \\
 & x50 & {\color[HTML]{009901} \sig{.5579}} & {\color[HTML]{009901} \sig{+2.92}} & {\color[HTML]{009901} \sig{+1.8}}  & +0.64                              & +0.12                              & -0.16                              & {\color[HTML]{9A0000} \sig{.6722}} & {\color[HTML]{9A0000} \sig{-2.66}} & {\color[HTML]{9A0000} \sig{-2.85}} & {\color[HTML]{9A0000} \sig{-2.53}} & {\color[HTML]{9A0000} \sig{-2.13}} & {\color[HTML]{9A0000} \sig{-2.05}} & 0.3458                             & +0.19 & +0.07                              & +0.13                              & +0.22                              & -0.3                               & 0.3608                             & +0.16 & +0.23 & -0.03                              & -0.05                              & -0.04                              \\
 & x80 & {\color[HTML]{009901} \sig{.5577}} & {\color[HTML]{009901} \sig{+3.12}} & {\color[HTML]{009901} \sig{+1.86}} & +0.61                              & +0.01                              & -0.33                              & {\color[HTML]{9A0000} \sig{.6715}} & {\color[HTML]{9A0000} \sig{-2.92}} & {\color[HTML]{9A0000} \sig{-3}}    & {\color[HTML]{9A0000} \sig{-2.65}} & {\color[HTML]{9A0000} \sig{-2.15}} & {\color[HTML]{9A0000} \sig{-2.05}} & 0.346                              & +0.23 & +0.09                              & +0.15                              & +0.29                              & -0.18                              & 0.3608                             & +0.16 & +0.21 & +0.01                              & -0.02                              & -0.06                             
 \\ \hline
\end{tabular}
\end{table*}


% Figure environment removed


%In order to assess the effectiveness of our proposed cost-sensitive approach for the mitigation of the mainstream bias, we split the users in each dataset into five groups: group `low' contains the 20\% of users with lowest $mainstreamness$ scores on the baseline model, group `med-low' contains the next 20\% or users, group `med' contains the middle 20\% of users, and so on with groups `med-high' and `high'.
An effective mitigation of the mainstream bias would be reflected in increased performance for the lower groups (i.e. mainly `low' and `med-low'), ideally with no detriment to the higher groups and, especially, overall. In the previous section we separated users into groups by each of \Sim and \Util, but here we separate them directly by their test $nDCG$ with the baseline model FM, because this better illustrates how non-mainstream users suffer from the bias.

Table~\ref{tab:overall_performance} reports the relative percentage improvement in $nDCG$ scores per user group, as well as the overall mean score across all users in the dataset. We can clearly see that the use of \Sim benefits the non-mainstream users only in the two Amazon dataset; in MovieLens and BeerAdvocate they are even hurt further.
In contrast, \Util is always able to improve the utility of non-mainstream users across datasets, achieving relative $nDCG$ improvements of up to $5\%$ in the Amazon datasets. Improvements on the lower user groups are generally higher than losses on the higher groups, where users already receive (very) high recommendation utility anyway and such minor losses are probably unnoticed. This redistribution of model performance has a negligible effect on the global performance of the models, as evidenced by the overall $nDCG$ scores. This means that, with proper selection of the contrast in the cost function, \Util can minimize the mainstream bias at virtually no overall loss in utility. On the other hand, the use of \Sim for training leads to inferior overall performance on all four datasets.
 
%even improves the average $nDCG score$. In more detail, it always benefits the lower groups more than the higher ones, achieving relative $nDCG$ improvements of up to $5\%$. Improvements on the lower user groups are also generally higher than losses on the higher groups. In addition, in 20 user groups in total from 4 datasets, \textsf{Util} is almost always the better choice of $m_u$ that can bring better improvements on top of the FM model. The effect of improvements is also clearly related to the contrast applied by the cost function. All above indicate that while measuring the mainstreamness using \textsf{Util}, compared to focusing more on users with low \textsf{Sim} scores, more focus on users with lower $nDCG$ scores with a proper selection of the contrast in the cost function can bring better bias mitigation effects without hurting the overall user utility.}
% users in the lower groups are indeed the most benefited from the cost-sensitive approach. The effect is also clearly related to the contrast applied by the cost function, achieving relative $nDCG$ improvements of up to $5\%$. 
% Improvements on the lower user groups are generally higher than losses on the higher groups, where users already receive (very) high recommendation utility anyway and such minor losses are probably unnoticed.
% This redistribution of model performance has a negligible effect on the global performance of the models, as evidenced by the overall $nDCG$ scores. This means that, with proper selection of the contrast in the cost function, the mainstream bias can be minimized at virtually no overall loss in utility.
Fig.~\ref{fig:res_csl_by_ndcg} presents a more fine-grained picture with one of the three random initializations in our experiments. Curve segments above 0 represent an improvement by the cost-sensitive models, while segments below 0 represent a loss. We can confirm that the cost-sensitive approach indeed makes the models focus on the non-mainstream users, as shown by the nicely smooth correlation between observed utility and relative improvement, moderated by the contrast in the cost function.
As expected though, this focus on the non-mainstream users comes at the cost of a utility loss for the mainstream users on the right-hand side of the plots. Nevertheless, when using \Util the relative loss for those users is generally much smaller than the gain for the very non-mainstream users, which are our main target.
The figure also shows that the actual relation between improvement and utility varies across datasets, as reflected by the different curve shapes. This is explained by the differences in the shape of their $nDCG$ distributions (see Fig.~\ref{fig:simvsutil}); recall that we use the $ecdf$ of the scores.
In a side-by-side comparison between \Sim and \Util, we see that \Util offers better performance nearly everywhere along the $x$-axis, but especially for the non-mainstream users. 

In summary, we see that our cost-sensitive approach brings better balance across users, thus helping in the mitigation of the mainstream bias. In addition, we confirm that an implicit quantification of mainstreamness like \Util works better than an explicit quantification like \Sim in steering the learning process towards better recommendations for the users that receive low utility from the baseline model. In addition, we note that the mitigation effect via \Util does not decay with increasing data sparsity (refer back to Table~\ref{tab:stats}).

One could be tempted to argue that \Util should obviously offer better results than \Sim when analyzing \emph{test} $nDCG$ because it is based on \emph{validation} $nDCG$ scores; test and validation scores should be highly correlated (we will come back to this in Section~\ref{sec:discussion}). After all, both Table~\ref{tab:overall_performance} and Fig.~\ref{fig:res_csl_by_ndcg} analyze results by test $nDCG$. The argument made above is that differences between mainstream and non-mainstream users can be immediately identified by test scores, but for the sake of clarity and to avoid potentially unfair assessment towards \Sim, Table~\ref{tab:overall_performance_by_ms} reports the same results but separating users by \Sim, while Fig.~\ref{fig:res_csl_by_ms} does so by plotting against \Sim.
While the results are less clear with this partition of users, the table confirms that models trained with \Sim are generally better at mitigating the bias than those trained with \Util. In particular, results for the BeerAdvocate dataset show that higher contrasts even lead to worse performance for the lower user groups, suggesting that \Sim is perhaps not properly identifying non-mainstream users.
The figure shows that \Util improves over the baseline across all levels of mainstreamness in the Amazon datasets, further suggesting that \Sim identifies as non-mainstream users that are probably not.
In summary, and even though this comparison could in turn be considered favorable to \Sim (note that previously we assessed against \emph{test} $nDCG$, not against the \emph{validation} $nDCG$ calculated by \Util), the results again support the use of \Util to quantify user mainstreamness and mitigate the bias. 

%\zhe{Results above show the power of using user utility as the proxy of mainstreamness for training. However, it remains unclear about the performance when users are grouped by \textsf{Sim}. \textsf{Sim} is expected to be powerful if it could also group users effectively, and groups with lower mainstreamness could benefit from the model. Table~\ref{tab:overall_performance_by_ms} presents the same $nDCG$ score of each user, but just grouped by \textsf{Sim} rather than \textsf{Util}. Different from the results shown in Table~\ref{tab:overall_performance}, we do not see a clear trend in the performance across either different user groups or cost functions. From the upper half, we see that the cost based on \textsf{Sim} could separate users, so that non-mainstream users can slightly benefit from more focus on them, or at least suffer less performance loss. However, this was not the case in the lower half, where MovieLens 1M is the only dataset that non-mainstream users benefit from the cost-sensitive modeling, and on Amazon datasets the performance improvement in different user groups is even somewhat homogeneous. We can thus draw several conclusions: first, \textsf{Sim} is not as good as \textsf{Util} in grouping users into different buckets, which also empirically confirms the insights we get in Section~\ref{subsec:m_and_u}. As a consequence, higher weights in the training loss based on \textsf{Sim} in cost-sensitive learning does not do not sufficiently turn to better recommendation accuracy, and thus the method does not target users in need so well as \textsf{Util}.}

%\zhe{Fig.~\ref{fig:res_csl_by_ms} presents the same data but with the x-axis standing for the mainstreamness based on \textsf{Sim}. Similarly, We find that \textsf{Util} provides a better improvement in $nDCG$ even when users are separated by \textsf{Sim}. Compared to Fig.~\ref{fig:res_csl_by_ndcg}, in which both \textsf{Util}- and \textsf{Sim}-based costs are working towards non-mainstream users, weights based on \textsf{Util} do not help any specific user in Amazon datasets. These observations show it again that our proposed \textsf{Util} is the better choice in both measuring mainstreamness and training a model for mainstream bias mitigation.}




% 

% We can also see that, except in the BeerAdvocate dataset, the proposed approach can benefit more than half the users. In sum, the positive effect of our cost-sensitive strategy is larger than the negative effect.
% Last but not the least, we also sorted out the reason why the cost-sensitive strategy does not work so well on the BeerAdvocate dataset: with higher nDCG scores in general, there are a smaller proportion of non-mainstream users (almost only in the bucket with low mainstreamness). As a result, there is less room for bias mitigation. All in all, our cost-sensitive strategy proves to work out for mainstream bias mitigation and appears to be effective even under extremely sparse data.\ju{Not sure here. Does this indicate that using ecdf is not a good idea? Does it indicate that we should've used a more steep cost function that decays sharply?} 
% \zhe{my two cents: we can further control the sharpness to create larger contrasts. Also, ecdf is more generic, and we can put it into different scenarios to always help some users. A steeper function could only work on special cases, which is even partially able to be compensated by tuning the contrast}

% Regarding the effect of our proposed cost-sensitive strategy to mitigate the mainstream bias, Fig.~\ref{fig:res_csl} shows the results acquired from the test sets on each dataset. The x-axis denotes the utility of each user based on the baseline FM model on the test set, and the y-axis refers to the percentage of difference in the nDCG score of the cost-sensitive models compared to the vanilla model. Curve segments above 0 thus refer to better performance brought by cost-sensitive models, while segments below zero mean cost-sensitive models sacrifice users included therein. We observe that the cost-sensitive strategy indeed works out. The utility of non-mainstream users with low nDCG scores is always with positive differences, and thus get better recommendation performance with cost-sensitive weights. However, as expected, this is not for free: it costs the utility loss of mainstream users on the right. In other words, cost-sensitive weights bring better balance across users.


% Regarding the effect of the sharpness of the $cost$ function, as expected, a sharper $cost$ function with more focus on non-mainstream users can benefit them more, but also at the cost of more loss for mainstream users.
% In order to investigate this further, we split users evenly into 5 buckets based on the $ecdf$ score of their accuracy in the vanilla model, namely low, med-low, med, med-high, and high. Results on the vanilla model and different contrasts of the $cost$ function are presented in Table~\ref{tab:overall_performance}. We can conclude that the utility improvement on non-mainstream users is indeed always higher than the loss on the mainstream side. Also important to note is that the effect of our strategy does not decay with the increasing data sparsity, which indicates the effectiveness of applying $ecdf$. 




% \begin{table*}[]\footnotesize
% \begin{tabular}{|r|rrrrrr|rrrrrr|rrrrrr|rrrrrr|}
% \hline
% \multicolumn{1}{|l|}{\multirow{2}{*}{}} & \multicolumn{6}{c|}{MovieLens 1M}                                         & \multicolumn{6}{c|}{BeerAdvocate}                                         & \multicolumn{6}{c|}{Amazon Digital Music}                                 & \multicolumn{6}{c|}{Amazon Musical Instruments}                           \\ \cline{2-25} 
% \multicolumn{1}{|l|}{}                  & \multicolumn{1}{r|}{overall} & low   & med-low & med   & med-high & high  & \multicolumn{1}{r|}{overall} & low   & med-low & med   & med-high & high  & \multicolumn{1}{r|}{overall} & low   & med-low & med   & med-high & high  & \multicolumn{1}{r|}{overall} & low   & med-low & med   & med-high & high  \\ \hline
% FM                                 & \multicolumn{1}{r|}{.5549 }      & .3309 & .4655   & .5762 & .6615    & .7403 & \multicolumn{1}{r|}{.6884}        & .4147 & .6051   & .7288 & .8121    & .8808 & \multicolumn{1}{r|}{.3457}        & .2322 & .2697   & .3154 & .3838    & .5274 & \multicolumn{1}{r|}{.3603}        & .2343 & .2769   & .3272 & .4076    & .5555 \\
% 5                                       & \multicolumn{1}{r|}{.5585}        & .3363 & .4732   & .5806 & .6630    & .7393 & \multicolumn{1}{r|}{.6845}        & .4184 & .6027   & .7215 & .8053    & .8743 & \multicolumn{1}{r|}{.3460}        & .2362 & .2731   & .3196 & .3854    & .5157 & \multicolumn{1}{r|}{.3605}        & .2366 & .2798   & .3295 & .4077    & .5488 \\
% 20                                      & \multicolumn{1}{r|}{.5596}        & .3403 & .4781   & .5810 & .6621    & .7366 & \multicolumn{1}{r|}{.6763}        & .4181 & .5913   & .7100 & .7957    & .8658 & \multicolumn{1}{r|}{.3459}        & .2409 & .2765   & .3208 & .3850    & .5062 & \multicolumn{1}{r|}{.3604}        & .2399 & .2823   & .3298 & .4061    & .5436 \\
% 50                                      & \multicolumn{1}{r|}{.5598}        & .3422 & .4799   & .5804 & .6609    & .7343 & \multicolumn{1}{r|}{.6733}        & .4282 & .5840   & .6979 & .7871    & .8599 & \multicolumn{1}{r|}{.3461}        & .2452 & .2792   & .3214 & .3832    & .5014 & \multicolumn{1}{r|}{.3604}        & .2425 & .2836   & .3298 & .4053    & .5414 \\
% 80                                      & \multicolumn{1}{r|}{.5598}        & .3432 & .4811   & .5807 & .6602    & .7328 & \multicolumn{1}{r|}{.6713}        & .4380 & .5831   & .6929 & .7844    & .8576 & \multicolumn{1}{r|}{.3463}        & .2465 & .2793   & .3220 & .3832    & .5008 & \multicolumn{1}{r|}{.3605}        & .2430 & .2839   & .3297 & .4049    & .5408 \\ \hline
% $\Delta$/\%                                & \multicolumn{1}{r|}{0.88}        & 3.72  & 3.35    & 0.78  & -0.20    & -1.01 & \multicolumn{1}{r|}{-2.48}        & 5.62  & -3.64   & -4.93 & -3.41    & -2.63 & \multicolumn{1}{r|}{0.17}        & 6.16  & 3.56    & 2.09  & -0.16    & -5.04 & \multicolumn{1}{r|}{0.06}        & 3.71  & 2.53    & 0.76  & -0.66    & -2.64 \\ \hline
% \end{tabular}
% \caption{Comparison on average accuracy measured by nDCG. The improvement by percentage $\Delta$ is calculated based on the accuracy from the highest contrast (80) in comparison with those from vanilla FM models. }
%  \label{tab:overall_performance}
% \end{table*}

% % Figure environment removed

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion} \label{sec:discussion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

A key assumption of our approach based on \Util is that we can reliably use utility, measured as the accuracy on a validation set, to determine the weight that each user should have in the training process. This implies that the accuracy on the validation set is a good estimate of the accuracy on the test set, which is where the effect will ultimately be assessed. If there was a low correlation between validation and test accuracy, the loss function would apply high weights for users that do not really need it, limiting or even altogether canceling the potential of our approach.

Intuitively, how well validation and test scores correlate is mainly determined by the amount of data. If only a few interactions are involved in the calculation of accuracy, the resulting scores will bear a high degree of noise or random error, thus lowering the correlation. In principle, we would therefore use as much data as possible in the validation and test sets. However, we would generally prefer to use all that data to actually train the model, but we note that the validation scores are somehow part of the training process itself, because they determine the weights.

A balance is therefore necessary, so we need to study the strength of the validation-test scores correlation as a function of the number of interactions in their data partitions. We did this by running the baseline FM model on different data partitions with varying minimum numbers of relevant items in the training set (3, 4, 5 and 10), and validation and test sets (1, 2, 3, 4 and 5 each). The actual split was conducted maintaining proportions (i.e. for the combination of 4/3/3 minimum items per set, a user has 40\% of their relevant items for training, 30\% for validation, and 30\% for testing).
We then measured the strength of the validation-test correlation via the RMSE of the scores and their Spearman $\rho$ correlation.

% Figure environment removed
Fig.~\ref{fig:corr} shows that, as expected, the correlation increases (low RMSE, high $\rho$) with the number of relevant interactions used in the validation and test sets. More interestingly, it shows that the amount of training data has a much smaller and varying effect, so despite it being a major factor to maximize model performance, it is not so to robustly assess that performance.
The plots indicate that requiring only one or two interactions in the validation set would lead to noisy scores; four interactions seem the bare minimum. As for the training set, the usual practice of having at least as much data as for validation and testing still applies in this context of non-time-aware recommendation.

All in all, our suggestion for this line of research on mainstream bias that works at the individual user level, is to have no less than four items per user in each of the three standard data partitions. Because the strength of the correlation is a key factor in our approach, we decided to require at least five to be on the safe side.
In fact, we also observed that the effect of cost-sensitive learning in the validation sets is similar to what is reported in Figs.~\ref{fig:res_csl_by_ndcg} and ~\ref{fig:res_csl_by_ms}.
% (not shown due to page limit constrain).

% % Figure environment removed


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions and Future Work} \label{sec:con}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this paper, we tackled the challenge of mainstream bias in CF-based recommendation. The main aspect we focused on is to steer the process of mitigating this bias directly by the utility resulting from the recommendation model and data at hand. For this purpose, we proposed an approach that assigns each user an importance weight during training, with these weights defined in a cost-sensitive manner.
By choosing to steer the model directly towards the users that receive low utility, and not towards those that \emph{appear} to be non-mainstream, we avoid the model to focus on users that already receive high utility even if they were not expected to. This way, the model does focus on the niche users that suffer from the bias.

Empirical results show that such models produce a more effective balance of the recommendation utility among the mainstream and non-mainstream users, in a way that is consistent across datasets with varying properties.
% sparsity levels.
%\zhe{For this purpose, we proposed a cost-sensitive strategy for model training, to assign importance weights to users based on the utilities they can get. Extensive experimental results show the effectiveness of the strategy, which is consistent across datasets with varying sparsity levels, in balancing the recommendation utility between the mainstream and non-mainstream users.}
%directly emphasizing the importance of non-mainstream users to achieve better utility balance across users. 
%In addition to empirically showing the effectiveness of the proposed strategy to assign cost-sensitive weights to the users during training was shown empirically. to be effectiveBy assigning higher weights to the suffering users, the proposed cost-sensitive learning strategy shows significant performance improvements for better balance across users, without losing much overall recommendation utilities. 
In addition, we provide suggestions regarding the minimum number of interactions to require when partitioning datasets. Without enough interactions, research on mainstream bias at the level of individual users might produce unreliable results.

For future work, we will first explore other ways to quantify mainstreamness. In the implicit measurement sense, an evident question is whether other metrics such as $AP$, or even the combination of multiple metrics, work better at identifying niche users. Additionally, we can think of ways to make the validation-test correlation robust to issues like sample selection bias, for example via inverse propensity scoring.
Another line is to explore more principled approaches for an explicit quantification through an extensive study of the factors that influence mainstreamness, such as the temporal dynamics.

Regarding our cost-sensitive learning approach, we will explore its generality, to see how it works for underlying models other than FM or other ranking frameworks such as pairwise and listwise.
% In addition, the results on the BeerAdvocate dataset in Fig.~\ref{fig:res_csl_by_ndcg} suggest that a cost function with a steeper decay might work better, so we will also investigate different formulations, possibly connected to the properties of the dataset.
We will also investigate the combination of cost-sensitive and adversarial learning strategies to mitigate mainstream bias: cost-sensitive to tell the model where to focus on, and adversary to tell how.

Finally, we note that our focus in this paper has been on the effect of mainstream
bias mitigation on the users, but one could wonder about what effect it has on the items. One hypothesis is that non-mainstream users are better served because the less popular items are now more likely to be recommended, so it would be interesting to study whether mitigating one bias amplifies or mitigates other biases, such as popularity or position.
%\zhe{Intuitively, rooted in the different patterns of interacting with items, mainstream bias is also related to other item-based biases such as popularity bias~\cite{DBLP:conf/flairs/AbdollahpouriBM19}. However, since items are neutral and their properties could only be reflected by user behaviors, mainstream bias is still mainly considered as a user fairness problem. Accordingly, its mitigation is largely conducted from the user side.}

\bibliographystyle{ACM-Reference-Format}
\balance
\bibliography{sample-base}


\end{document}

\clearpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{OLD Proposed approach}\label{sec:cost}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
As mentioned in Section~\ref{sec:intro}, we adopt the cost-sensitive learning strategy to assign weights to each user in training. In other words, weights are thus derived from the cost of failing to satisfy non-mainstream users, so that non-mainstream users get better focused. In the meantime, to make sure there is no user left behind so that we still have a model for all users, mainstream users are less focused but not fully overlooked. Since non-mainstream users are expected to get low recommendation utility, there is a simple and direct correlation between mainstreamness and recommendation accuracy, so we use recommendation accuracy as the proxy to indicate user mainstreamness and derive the weights based on accuracy. However, mainstreamness is regarded as an intrinsic user property and is expected to be static, while raw accuracy scores are variant to different factors such as the experimental and evaluation protocol and can be with quite different distributions in different datasets. The direct mapping of accuracy to mainstreamness is thus not a good idea unless we also take necessary normalizations to make a static and robust proxy. We will present our method in detail as follows.

\subsection{Model Design}\label{subsec:model}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Inspired by the statement above, we will have one weight assigned to each user based on the cost-sensitive strategy. The general loss could be thus denoted as below: 
\begin{equation}
\label{eq:overall}
        \mathcal{L} = \sum_{u}\alpha_U(u) \mathcal{L_R}(u),
\end{equation}
where $\mathcal{L_R}$ denotes the recommendation loss which is customizable as per the model and learning paradigm; $\alpha_U$ is the cost-sensitive importance weight for users, and is related to the recommendation accuracy users can get, which is related to several different factors. We will cover them in Section~\ref{subsec:weight}.

\subsection{Weight for Cost-sensitive Recommendation} \label{subsec:weight}
As aforementioned, the weight $\alpha_U(u)$ is a comprehensive and ideally static product of several factors related to accuracy. Here we mainly investigate the accuracy score itself and the dataset difficulty. Eq.~(\ref{eq:cost-sensitive}) presents the weight $\alpha_U(u)$ for an arbitrary user $u$.
\begin{equation}
    \label{eq:cost-sensitive}
    \alpha_U(u) = G(F(s_U(u))),
\end{equation}
where $s_U(u)$ is the recommendation accuracy of user $u$, $F$ is the function for normalization to mitigate the influence of dataset difficulty, and $G$ is the weight assigning function in control of the importance of the user in training.

\noindent \textbf{Dataset difficulty}. 
 Raw accuracy scores in recommendation are influenced by the difficulty of datasets (e.g., data sparsity) and the metric to calculate them, and thus may be with different distributions. However, in most scenarios, there are always non-mainstream users. As a result, a generic method that could always identify a proportion of non-mainstream users in all datasets, which was not sufficiently considered in previous work such as \cite{DBLP:conf/wsdm/ZhuC22}, is needed. That is, the least mainstream users in each dataset should get similarly focused, and vice versa for their most mainstream counterparts, so that the effect of the model is not impacted by the distribution of absolute accuracy scores. To mitigate the influence of dataset difficulty, here we choose to implement $F$ with quantiles, which means the normalized ranking position of accuracy, to map all ranking scores between 0 and 1 with equal intervals, to make the weight calculation a generic solution. 

 \noindent \textbf{Weight Assignment}.
With the proposed $F$ function, we manage to always identify the bunch of non-mainstream users in need. To promote user fairness and mitigation the mainstream bias, instead of classic recommendation models treating all users equally, we need to assign higher weights to non-mainstream users and lower weights to mainstream ones in training. In this way, the model is enforced to fit non-mainstream users better, and thus improve their recommendation utilities. In accordance with this setting, the weights are preferably strictly negatively correlated to accuracy. 
We choose to implement $G$ by adopting the probability density function (PDF) of the Truncated Normal distribution, with the highest value reached at the quantile of 0, so that the least mainstream user could get the highest weight and thus be most focused by the model. In the meantime, the smoothness of PDF makes no drastic weight change between users with similar mainstreamness. PDF also ensures all users are still with a non-zero weight, so that the model involves all users, and this may help guarantee the overall performance. In addition, by tuning the hyper-parameters of the PDF function, we can also control the contrast of focus between most and least focused users, so that we can achieve sufficient balance improvements without hurting the overall recommendation utilities due to an extreme policy.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{OLD Proposed approach}\label{sec:cost}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

As mentioned in Section ~\ref{sec:intro}, we use recommendation accuracy as the proxy to indicate user mainstreamness, with non-mainstream users expected to get low recommendation utility. However, it is not a good idea to directly map the raw accuracy values to mainstreamness without any operation. Compared to mainstreamness, which could be regarded as an intrinsic property of users that is invariant to external factors, accuracy values are always dependent on the evaluation protocol. The distribution of accuracy values is up to many conditions such as the metric chosen for evaluation, and the sparsity of the test set, i.e., the ratio between relevant and irrelevant items used for calculating the accuracy. Accuracy scores are thus varying, and mainstreamness scores will also be dynamic over static, which calls for the need for further processing to avoid these concerns. With the aforementioned motivations, we design a cost-sensitive strategy to make the importance of each user in training to be a function of the recommendation accuracy they get in vanilla models, and make the weight invariant to the absolute accuracy scores. We will present our method in detail in Section~\ref{subsec:model}.

\ju{Needs re-structured, as mentioned in the other version. We should start directly with (1), which is the basis of a weighted loss, explaining what the weight represents. Then we get into 1) the shape of the weight, 2) what data is used for it, and 3) whether it's dynamically updated or not. [23] has 1) a very specific shape $1/M^\beta$, 2) several mainstream proxies, and 3) is static if I'm not mistaken. What we have is 2) a very simple and direct proxy (ie. the recommendation accuracy), and 1) a more elaborated shape (ie. $G(F(\dots))$). Here's where we need to discuss about difficulty, datasets, metrics, etc: they influence the shape of the weight and our choices. And here is where you set yourself apart from [23]. Then you can say why 3) we use static as well.  }

\subsection{Model Design}\label{subsec:model}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Inspired by the statement above, we will have one cost-sensitive weight assigned to each user. The general loss could be thus denoted as below: 
\begin{equation}
\label{eq:overall}
        \mathcal{L} = \sum_{u}\alpha_U(u) \mathcal{L_R}(u),
\end{equation}
where $\alpha_U$ is the cost-sensitive importance weight for users; $\mathcal{L_R}$ denotes the recommendation loss which is customizable as per the model and learning paradigm. For the cost $\alpha_U$, the design in this paper mainly relies on two factors, namely dataset difficulty, and user difficulty.

\noindent \textbf{Dataset difficulty}. 
With accuracy used as the proxy for mainstreamness, the magnitude of weights in cost-sensitive learning could be treated as a function of accuracy,\ju{redundant} and is thus influenced by the difficulty of datasets (e.g., data sparsity) and the metric to calculate them. However, since we are calling for static cost-sensitive weights, the magnitude is ideally neutral to these factors to make accuracy a robust proxy \ju{has nothing to do with ``static weights''}. That is, the least mainstream users in each dataset should get similarly focused, and vice versa for their most mainstream counterparts, so that the effect of the model is not impacted by the distribution of absolute accuracy scores. An extra normalization step to resolve such a concern is thus necessary.\ju{need better justification: are we thus saying that in all circumstances, regardless of how high the accuracy already is, we should be able to get some more?}

\noindent\textbf{User difficulty}. 
After resolving the issues of normalizing the distribution of accuracy on the dataset level,
we also need to dive into the individual user perspective. With a cost-sensitive learning strategy applied, we need to identify the groups of users worked for, which means users with low accuracy scores. Instead of classic recommendation models treating all users equally, a solution to assigning higher weights to non-mainstream users and lower weights to mainstream ones is needed, so that the cost-sensitive model can effectively discriminate its target users and pay more attention to them.\ju{still don't see the point here. This so-called ``user-difficulty'' is what makes us use a weighted loss in the first place. It's not a factor to consider when designing the weight; it's the very reason we use a weight!}

In sum, we have a conceptual formulation of the cost-sensitive weight assigning method shown as Eq.~(\ref{eq:cost-sensitive}). We take the example as an arbitrary user $u$ in the user set $U$. The cost-sensitive weight $\alpha_U(u)$ could be denoted as
\begin{equation}
    \label{eq:cost-sensitive}
    \alpha_U(u) = G(F(s_U(u))),
\end{equation}
where $s_U(u)$ is the recommendation accuracy of user $u$, and $F$ is the function for normalization. To fully mitigate the influence of dataset difficulty, here we choose to rely on the quantile, which means the normalized ranking position of accuracy, to map all ranking scores between 0 and 1 with equal intervals, to make the weight calculation a generic solution. In other words, by using the quantile of accuracy, the weight distribution is similar across all datasets. Compared to other methods to map the values to [0, 1], such as min-max normalization, quantile works better when there is a large bunch of users with similar accuracy scores, under which their weights will still be clustered after min-max normalization.

\ju{I'd reshape by considering two aspects: what's used as a proxy, and what shape the weight has. In our case, shape has to do with $G$, and in [23] it has to do with the inverse and $\beta$. So first we talk about that. Then you can introduce the problem of the properties of mainstreamness. For us, this is mainly dataset difficulty, which is why you introduce the $F$ (noting it has pros and cons). If I'm not mistaken, [23] don't do something like this, right? Do they need to?}

By giving non-mainstream users higher weights in the loss function during training, the model is enforced to fit them better, and thus improve their recommendation utilities. In this way, the weights would be strictly negatively correlated to the user-wise recommendation accuracy. 
We choose to adopt the probability density function (PDF) of the Truncated Normal distribution, with the highest value reached at 0. 
With this setting, the least mainstream user with a quantile score of 0 could get the highest weight and thus be most focused by the model. In the meantime, the smoothness of PDF makes no drastic weight change between users with similar mainstreamness. PDF also ensures all users are still with a non-zero weight, so that the model involves all users, and this may help guarantee the overall performance.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Cost-sensitive Learning Towards Better Mainstream Bias Mitigation} 
\section{OLD Proposed approach}\label{sec:cost}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\subsection{\zhe{Motivation}} \label{subsec:tech}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%As mentioned in Section~\ref{sec:intro}, instead of treating all users and items equally and ''hoping'' that the model will achieve a better balance in the recommendation performance, it is more effective to promote the importance of \zhe{non-mainstream}\ju{choose one good term to refer to them, other than niche, probably, and stick to it} users by exerting user-based weights in the loss \ju{function}, which refers to the \emph{cost-sensitive learning}. Intuitively, recommendation utility is one of the most important factors \zhe{to reflect the effectiveness of a recommender system} \ju{important for what?}.
%\zhe{In collaborative filtering systems which model users by their similarity to other users, non-mainstream users who are intrinsically not similar to other users are likely to be matched to share preferences with dissimilar users. Therefore, the lack of real neighbors will make the modeling of non-mainstream users hard and inaccurate, which leads to low utilities. Measured by rating or ranking-oriented metrics, accuracy can reflect the recommendation utility, and thus be a proxy of the mainstreamness of a user. Intuitively, users with low accuracy in vanilla CF-based models can thus be assigned higher weights for training. }

% Measured by accuracy- or relevance-oriented metrics, they could show the extent of how mainstream a user is, which makes it possible to get the weights derived from the recommendation utility.
\tocheck{rewritten as above}\ju{RW: a non-mainstream user should receive more weight in the loss. How to define mainstream? Well, because it's CF, accuracy relates to mainstreamness, so a user with low accuracy should receive more weight; we use accuracy as a proxy for non-mainstreamness}
\zhe{However, it is not a good idea to directly map the raw accuracy values to mainstreamness without any operation. Compared to mainstreamness, which could be regarded as an intrinsic property of users that is invariant to external factors, accuracy values are always dependent on the evaluation protocol. The distribution of accuracy values is up to many conditions such as the metric chosen for evaluation, and the sparsity of the test set, i.e., the ratio between relevant and irrelevant items used for calculating the accuracy. Accuracy scores are thus varying, and mainstreamness scores will also be dynamic over static, which calls for the need for further processing to avoid these concerns. With the aforementioned motivations, we design a cost-sensitive strategy to make the importance of each user in training to be a function of the recommendation accuracy they get in vanilla models, and make the weight invariant to the absolute accuracy scores. We will present our method in detail in Section~\ref{subsec:model}.}

% However, absolute performance is influenced by the evaluation protocol, such as the chosen metric and properties (e.g., sparsity, negative sampling strategy) of the validation/test set. This calls for a normalization operation to align the range of different components in the loss, and make them comparable so that the effect of the model is independent of non-cost-sensitive aspects. In this way, the weights controlling the relative importance of users are thus independent of the value ranges.
\ju{This is very unclear: why exactly do we need to normalize, why is it bad that scores depend on things like sparsity, and what components do we mean in the loss?}


% Since we aim at protecting 
% niche or suffering
% \zhe{non-mainstream} users in classic models, we tend to assign higher weights to them. There will be different use cases and focuses worth discussing. We will do it in detail below. These cost-sensitive weights can even be dynamic as the training proceeds, but for alleviating the concern of efficiency as well as modeling mainstreamness as an intrinsic property of users, in this paper, we opt for the static pre-calculated weights.
\ju{this is something to discuss later; we haven't even seen how the loss is modified}
\zhe{RE: I tried to elaborate my statements and make the choice on static loss based on the definition of mainstreamness, so the discussion on static vs. dynamic weights can be removed.}

\ju{The whole discussion about weighting users would benefit a lot from actually showing (1) early on. The thing is how to incorporate weights in the loss, and later on how to define weights. This subsection is "technical choices", but it doesn't really talk about that, does it?}
\zhe{RE: I think now it's better to be named as "motivation". All tech choices are in fact part of the model design}

\subsection{Model Design} \label{subsec:model}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Inspired by the statement above, we will have one cost-sensitive weight assigned to each user. The general loss could be thus denoted as below: 
% \begin{equation}
% \label{eq:overall}
%         \mathcal{L} = \frac{1}{N_R}\sum_{u}\alpha_U(u)\sum_{(u,i) \in TR} \mathcal{L_R}(u, i),
% \end{equation}
\zhe{
\begin{equation}
\label{eq:overall}
        \mathcal{L} = \sum_{u}\alpha_U(u) \mathcal{L_R}(u),
\end{equation}
where $\alpha_U$ is the cost-sensitive importance weight for users; $\mathcal{L_R}$ denotes the recommendation loss which is customizable as per the model and learning paradigm.
% $N_{TR}$ collects the number of interactions used for training per user, which helps normalize the loss to get rid of the influence of user activeness.
}
\tocheck{done for a new statement}\ju{what is $N_R$? Shouldn't the two terms be added? I vaguely remember we talked about ranking prediction, and a weight for each user (ie. only one summation). Why do we need to normalize the 1st summation but not the 2nd one? Does the 2nd summation imply rating prediction? Do we need subscript $U$ in $\alpha_U$? Better to not use $TR$, as that's easily mistaken for a product of $T$ and $R$}

% where $TR$ denotes all samples (user-item pairs) in the training set, and $\alpha_U$ is the cost-sensitive importance weight for users, based on the user-wise recommendation accuracy measured by a given evaluation metric, and the recommendation loss is denoted as $\mathcal{L_R}$, which is customizable as per the model used.

% In this paper, it refers to the pointwise binary cross-entropy loss used by the baseline FM model.
For the cost $\alpha_U$, the design in this paper mainly relies on two factors, namely dataset difficulty, and user difficulty.

\noindent \textbf{Dataset difficulty}. 
\zhe{As mentioned above, weights assigned to each user during training are based on their accuracy scores on the vanilla recommendation model.}
% Since the weight of each user is cost-sensitive
\ju{a weight is cost-sensitive?}, \zhe{The weight magnitude is thus} influenced by the difficulty of datasets (e.g., data sparsity) and the metric to calculate them. \zhe{However, since we are calling for static cost-sensitive weights, the magnitude is ideally neutral to these factors, so that accuracy could be a robust proxy to mainstreamness. That is, the least mainstream users in each dataset should get similarly focused, and vice versa for their most mainstream counterparts, so that the effect of the model is not impacted by the distribution of absolute accuracy scores. An extra normalization step to resolve such a concern is thus necessary.}

% We need to make sure that our strategy is robust against dataset sparsity, choice of metrics (e.g., cosine similarity, nDCG, or AP), and the number of negative samples in evaluating the recommendation accuracy. This calls for a normalization operation to align the range of weights in the loss, and make them comparable so that the effect of the model is independent of non-cost-sensitive aspects.
\ju{doesn't make much sense to me. The weight depends on accuracy, which depends on dataset etc, so they \textbf{are} cost-sensitive aspects}
\noindent\textbf{User difficulty}. 
% After resolving the high-level issues of normalizing the scale of weights to get rid of the influence of datasets and evaluation metrics, 
\zhe{After resolving the issues of normalizing the distribution of accuracy on the dataset level,}
we also need to dive into the individual user perspective. With a cost-sensitive learning strategy applied, we need to identify the groups of users worked for, \zhe{which means users with low accuracy scores. Instead of classic recommendation models treating all users equally, a solution to assigning higher weights to non-mainstream users and lower weights to mainstream ones is needed, so that the cost-sensitive model can effectively discriminate its target users and pay more attention to them.}


% and this is determined by the difficulty of providing satisfactory recommendation utilities. The normalized performance scores should be further utilized for addressing this issue. Since we aim at protecting non-mainstream users in classic models, we tend to assign higher weights to them.
\ju{this is again saying that weights are approximated by accuracy, right? I mean, it doesn't describe "a 2nd factor" that weight relies on. In fact, it cannot be separated from the "dataset" factor.}
\zhe{RE: now tried to make the second factor independent. The accent is in fact how to get a working model for difficult users, which means the PDF function mentioned below, rather than identifying users, which is already done by the mainstreamness-accuracy proxy.}

In sum, we have a conceptual formulation of the cost-sensitive weight assigning method shown as Eq.~(\ref{eq:cost-sensitive}). We take the example as an arbitrary user $u$ in the user set $U$. The cost-sensitive weight $\alpha_U(u)$ could be denoted as
\begin{equation}
    \label{eq:cost-sensitive}
    \alpha_U(u) = G(F(s_U(u))),
\end{equation}
\tocheck{$\epsilon$ removed}\ju{1st time I see $\epsilon$? Why do we need it, really, if the $G$ is properly defined to be strictly positive, as is our case?}
where
% $\epsilon$ is a small positive value to avoid any cost-sensitive weight to be 0, and thus get all users involved in training with non-zero weight.
$s_U(u)$ is the recommendation accuracy of user $u$. The $F$ function is for normalization. To fully mitigate \zhe{the influence of} dataset difficulty, here we choose \zhe{to rely on the quantile, which means the normalized ranking position of accuracy to map all ranking scores between 0 and 1 with equal intervals, to make the weight calculation a generic solution. In other words, by using the quantile of accuracy, the weight distribution is similar across all datasets. Compared to other methods to map the values to [0, 1], such as min-max normalization, quantile works better when there is a large bunch of users with similar accuracy scores, under which their weights will still be clustered after min-max normalization;}
\ju{RW: what does "quantile normalization" mean?} 
% to get all ranking scores independent from the data distribution
\ju{they won't be independent, by definition, because they're computed from it}
\zhe{RE: agree, I wanna state it is generic so there will be a unified solution. Tried to rephrase it above}
the $G$ function is designed to help the most \zhe{non-mainstream users}. 
\ju{\textbf{this} is the real weight function}
By giving non-mainstream users higher weights in the loss function during training, the model is enforced to fit them better, and thus improve their recommendation utilities. In this way, the weights would be strictly negatively correlated to the user-wise recommendation accuracy. 
We \zhe{choose to} \ju{"therefore" implies a logical or obvious construction, whereas here we make a choice, as good as any other} adopt the probability density function (PDF) of the Truncated Normal (TN) distribution, with the highest value reached at 0. 
\zhe{With this setting, the least mainstream user with a quantile score at 0 could get the highest weight and thus be most focused by the model. In the meantime, the smoothness of PDF makes no drastic weight change between users with similar mainstreamness. PDF also ensures all users are still with a non-zero weight, so that the model is trained for all users, and this may help guarantee the overall performance.}\ju{not clear: why a distribution? why the pdf? why the highest value at 0? What do you achieve with that?}


\ju{still missing: where does the measurement of user accuracy come from? And then, once we're here, we can talk about it being static or dynamic, not at the beginning.}
\zhe{RE: my two cents: if mainstreamness is static, then the proxy needs to provide static estimation, so we can talk about user accuracy and normalization. It is mainstreamness-oriented rather than evaluation-oriented. So, I tend to keep the current storyline.}


% We showed the effectiveness of our strategy on user fairness and mainstream bias mitigation via extensive experiments on real-world datasets. In addition, we also provided suggestions for the requirement of the offline datasets to get the cost-sensitive strategy to work. In other words, we propose a guideline for the data needed to conduct research on mitigating the mainstream bias that could be valid for individual user fairness.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{OLD Experimental Protocols} \label{sec:exp}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table}[t]
\caption{Dataset statistics after pre-filtering.}
    \centering{\small
    \begin{tabular}{@{}crrrrr@{}}
        \hline
        Dataset & \#users & \#items & \#ratings & Density/\% \\
        \hline
        MovieLens 1M~\cite{DBLP:journals/tiis/HarperK16} & 6,040 & 3,609 & 562,957 & 2.583 \\
        BeerAdvocate~\cite{DBLP:conf/icdm/McAuleyLJ12} & 8,821 & 43,663 & 780,752 & 0.203 \\
        Amazon Digital Music~\cite{DBLP:conf/emnlp/NiLM19} & 14,057 & 379,171 & 619,673 & 0.011  \\
        Amazon Musical Instruments ~\cite{DBLP:conf/emnlp/NiLM19} & 15,270 & 585,766 & 862,798 & 0.010\\
        \hline
      \end{tabular}}
    \label{tab:stats}
\end{table}
To test the effectiveness of the cost-sensitive weights in promoting the utility of non-mainstream users, we design a set of experiments for empirical analysis.
\zhe{To make sure that there is no further difference between the vanilla recommendation model and cost-sensitive learning models, the vanilla model is preferable with a simple but effective architecture only utilizing user-item interactions.}
% Following the basic settings of NAECF \cite{DBLP:conf/wsdm/LiUH21}, 
Here we choose pointwise Factorization Machines \cite{DBLP:conf/icdm/Rendle10} as the baseline model for investigation, which optimizes for the lowest binary cross-entropy (BCE) by the Adaptive Moment Estimation (Adam)~\cite{DBLP:journals/corr/KingmaB14} learner. \zhe{The loss is further normalized as per the total number of relevant and irrelevant items used for training to make sure each user contributes equally to the overall loss.} 
\ju{why follow the settings of NAECF? They are conceptually quite different from us. Plus, what does that model represent that is relevant for us?}
\zhe{agree. though it is not feasible to say "without losing generality" since I do not dare say that. Though the focus is that we have a model vanilla enough using only explicit feedback, no content or anything else that may distract the contribution of the cost-sensitive strategy.}
After a fine-tuning process, we fixed several key hyper-parameters including the dimension of vectors used for interaction (32), learning rate (1e-4), L2-regularization coefficient to avoid overfitting (1e-3), and batch size (512). \zhe{To investigate the effect of the cost-sensitive weights as per their extent of stress on non-mainstream users, we adjust the sharpness of the $G$ function, to make the highest weight as [5, 10, 20, 50, 80] times as the lowest weight to indicate the contrast of importance mapped to the most non-mainstream and mainstream users. }

To get more all-around insights, our experiments use four different real-world datasets containing user-item rating interactions from various domains and with different densities. To make the data adhere to the common practice of ranking-oriented recommender systems research, we \zhe{see} all existing interactions in the datasets as relevant, and all other unseen interactions as irrelevant. \ju{is it binarizing if we consider one thing as rel and the other as nonrel? It's already binary :-)}
\zhe{should be "see". Indeed we did not binarize anything, but do TREAT them as binary}
For each relevant training sample, following the conclusion of several previous impactful works~\cite{DBLP:conf/www/HeLZNHC17, DBLP:conf/ijcai/WuWH22}, we randomly sample 4 different negative samples to feed into the model for training. To make the modeling of mainstreamness robust, each user has at least 5 relevant transactions for training, validation, and testing, respectively. Relevant items are evenly split into three subsets by LensKit~\cite{DBLP:conf/cikm/Ekstrand20}. We will also discuss the reason for this setting later in Section~\ref{sec:discussion}, which is related to the second contribution mentioned in Section~\ref{sec:intro}. Following the protocol proposed in DaisyRec~\cite{DBLP:conf/recsys/SunY00Q0G20}, validation and evaluation are done for each user by ranking a total of 500 items consisting of her relevant items in the validation/test set and the other randomly sampled irrelevant samples. The recommendation utility of each user is measured and evaluated by nDCG. To make sure relevant items account for the minority as in the real world, users with more than 200 items are truncated to keep only 200 items in the following of this paper. The statistics of all four dataset after processing are shown in Table~\ref{tab:stats}. \ju{didn't we have a (kind-of-)constant negative-to-positive ratio across users? And then the actual (minimum) number of positives determined by the results on the 1st preliminary experiment?} 
All models are trained for 300 epochs to get a full convergence, \zhe{with 3 different initializations to guarantee reproducibility.} The whole research pipeline is implemented in PyTorch \cite{DBLP:conf/nips/PaszkeGMLBCKLGA19}, and all experiments are run on one NVIDIA GeForce GTX 2080Ti GPU. \ju{only initializations or data splits too? Anyway, mention this before this paragraph, which should only be about computing infrastructure}
\zhe{RE: Put the running protocols ahead of computing settings, but in this way there is no need to have a new para for pytorch. It will be anyway just one sentence.}



% \section{Investigation on Data Requirements} \label{sec:datareq}
% \subsection{Background} \label{subsec:bgd}
% Research in model-based recommender systems relies heavily on machine learning, and usually adheres to some general principles for experimental design. For example, while experimenting on offline user-item interaction data, datasets are usually split for training, validation, and test, and the performance is usually measured by averaging over all users. However, while coming to the research on a finer granularity, i.e., the individual user level, we need to care more about whether we can model and evaluate each user properly so that the results we get are accurate, convincing and indicative. 

% There are two major factors influencing the issue above, namely whether we have enough data to train a robust model, and whether the performance measured by the validation set could reflect the real individual user utilities that are captured by the test set. Intuitively, when there are few interactions used for training, we cannot get a predictive model; when there are not enough interactions for validation and test, evaluation metrics will suffer from serious randomness, and we cannot get any conclusive insights from the results got in this way. In this section, we dive deep into these two factors, and try to find the requirement of the sample size for training/validation/test.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{OLD Experimental Results} \label{sec:res}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% To investigate the effect of the cost-sensitive weights as per their extent of stress on non-mainstream users, we adjust the sharpness of the $G$ function, to make the highest weight as [5, 10, 15, 20, 30, 40, 50, 60, 70, 80] times as the lowest weight to indicate the contrast of importance mapped to the most non-mainstream and mainstream users.
\ju{shouldn't this go to the previous section? And reading now about G, why not just a linear function between max at 0 and min at 1?} 
\zhe{RE: I think this concern is kinda resolved in the newly added statements on why we need quantiles: a more generic strategy, which is robust to gathered nDCG scores. }

As mentioned in Section~\ref{sec:exp}, \zhe{we are keen on the effect of the contrast provided by the sharpness of $G$. For a sharper $G$ with more focus on non-mainstream users}, compared to the vanilla FM model, we expect the model to get better improvements for them in exchange for more performance loss on mainstream users. An optimal contrast is thus expected to bring a better balance between the utilities of mainstream and non-mainstream users without significantly hurting the overall recommendation accuracy. 
\zhe{Unlike~\cite{DBLP:conf/wsdm/ZhuC22}, which assigns weights on individual users but performs the evaluation based on groups with different extents of mainstreamness, our evaluation process is also on the individual user level, to keep the granularity in line with training.}
An indication of the weight curve as per the quantile of nDCG scores acquired from the vanilla FM model is shown in Fig.~\ref{fig:weight}.
\ju{it'd be nice to show not only the mapping from F(ndcg) to weight, but the resulting mapping from the raw ndcg, to show the differences across datasets; this could be with only one G to simplify}

% Figure environment removed

% % Figure environment removed

% % Figure environment removed

Fig.~\ref{fig:res_csl} shows the results acquired from the test sets \zhe{on each} \ju{faceted?} dataset. The x-axis denotes the quantile value of the nDCG score of each user based on the vanilla FM model on the test set \ju{ah! finally we see where $s_U(u)$ comes from...almost ;-) Also, does it make sense for the paper to show raw ndcg, so it's simpler to interpret?}
\zhe{RE:you mean raw ndcg on x axis?}
, and the y-axis refers to the difference in absolute nDCG score between the cost-sensitive models and the vanilla model. 
\ju{does a relative y-axis help our argument? Now we see tiiiiny effects, but considering the low absolute scores, they're actually high}
\zhe{RE: in fact we tried this before and it did not work out, since the values on the left will finally drive the whole curve above 0. This is deceptive. Though maybe worth trying raw nDCG on the x-axis? Then which is better, superseding the x-axis, or a new set of figures?}
As a result, curve segments above 0 refer to better performance brought by cost-sensitive models, while segments below zero mean cost-sensitive models sacrifice users included inside. We observe that 1) the cost-sensitive strategy indeed works out. The utility of non-mainstream users with low nDCG scores is always with positive differences, and thus get better recommendation performance with cost-sensitive weights. However, as expected, this is not for free: it costs the utility loss of mainstream users on the right. In other words, cost-sensitive weights bring better balance across users. \zhe{In addition, since users on the left side of each panel are with low nDCG scores, the percentage of improvements for them is higher than the percentage loss on the right. We also see that in 3 out of 4 datasets (except BeerAdvocate), cost-sensitive learning strategy can benefit more than half users, which also means its positive effect is larger than the negative effect};\ju{can't we measure this "better balance" via the std.dev. of raw ndcg scores in vanilla vs. cost-sensitive? (cost-sensitive should have lower std.dev)} 
\zhe{COMMENT: now I do see the need for (also) using raw nDCG scores in x-axis. Is it possible to have a second x-axis matching all those quantile checkpoints?}
2) \zhe{sharper $G$ function with more focus on non-mainstream users can benefit more for them, but intuitively at the cost of more loss for mainstream users;}
% with more contrastive weights giving more focus on non-mainstream users, non-mainstream users can benefit more with better recommendation accuracy, but intuitively at the cost of more loss for mainstream users; 
\ju{simplify: the size of the effect is of course influenced by the size of the weighting applied by G}
3) as seen from Fig~\ref{fig:ndcg_overall}, the better balance of user utilities is not subject to overall performance loss. Moreover, except for the BeerAdvocate dataset, the cost-sensitive learning strategy \zhe{achieves comparable or even more promising overall accuracy compared to that on the vanilla FM model.} 
% Last but not the least, results not included in this paper due to the page limit also indicate highly similar insights in the validation set, which means the test performance genuinely reflects the validation performance. 
\ju{here we're clearly jumping into another issue, but we didn't warn the reader. Why is this important to begin with?}
\zhe{RE: agree again. then how about mentioning this in the next section, or we reorganize the paper to put sec 5 before sec 4, so there is a sufficient warning?}
% This confirms our choice made in Section~\ref{sec:datareq}.


% \begin{table*}[]
%     \centering \small
%     \begin{tabular}{c|rrrrrrrrrr}
%     \hline
%       \diagbox[width=13em]{Dataset}{Contrast} & Baseline FM & 5 & 10 & 20 & 30 & 40 & 50 & 60 & 70 & 80 \\
%     \hline
%       MovieLens 1M  & 0.5549 & 0.5585 & 0.5591 & 0.5596 & 0.5596 & 0.5598 & 0.5598 & 0.5596 & 0.5597 & 0.5598\\
%       BeerAdvocate & 0.6884 & 0.6845 & 0.6805 & 0.6763 & 0.6712 & 0.6742 & 0.6733 & 0.6715 & 0.6713 & 0.6713 \\
%       Amazon Digital Music & 0.3457 & 0.3460 & 0.3460 & 0.3459 & 0.3464 & 0.3460 & 0.3461 & 0.3461 & 0.3460 & 0.3463\\
%       Amazon Musical Instruments & 0.3603 & 0.3605 & 0.3604 & 0.3604 & 0.3605 & 0.3605 & 0.3604 & 0.3606 & 0.3606 & 0.3605 \\
%     \hline
%     \end{tabular}
%     \caption{Average recommendation accuracy across all users in each dataset measured by nDCG. $Baseline FM$ denotes the nDCG score on the vanilla FM model, and all other column names refer to the sharpness of the $G$ function reflected by the quotient between the highest and lowest cost-sensitive weights.\ju{isn't it better to have a figure for this? If not, at least highlight positive cases. Should also have some sort of statistical analysis}}
%     \label{tab:overall_performance}
% \end{table*}


\begin{table*}[]\footnotesize
\begin{tabular}{|r|rrrrrr|rrrrrr|rrrrrr|rrrrrr|}
\hline
\multicolumn{1}{|l|}{\multirow{2}{*}{}} & \multicolumn{6}{c|}{MovieLens 1M}                                         & \multicolumn{6}{c|}{BeerAdvocate}                                         & \multicolumn{6}{c|}{Amazon Digital Music}                                 & \multicolumn{6}{c|}{Amazon Musical Instruments}                           \\ \cline{2-25} 
\multicolumn{1}{|l|}{}                  & \multicolumn{1}{r|}{overall} & low   & med-low & med   & med-high & high  & \multicolumn{1}{r|}{overall} & low   & med-low & med   & med-high & high  & \multicolumn{1}{r|}{overall} & low   & med-low & med   & med-high & high  & \multicolumn{1}{r|}{overall} & low   & med-low & med   & med-high & high  \\ \hline
Vanilla                                 & \multicolumn{1}{r|}{}        & .3309 & .4655   & .5762 & .6615    & .7403 & \multicolumn{1}{r|}{}        & .4147 & .6051   & .7288 & .8121    & .8808 & \multicolumn{1}{r|}{}        & .2322 & .2697   & .3154 & .3838    & .5274 & \multicolumn{1}{r|}{}        & .2343 & .2769   & .3272 & .4076    & .5555 \\
5                                       & \multicolumn{1}{r|}{}        & .3363 & .4732   & .5806 & .6630    & .7393 & \multicolumn{1}{r|}{}        & .4184 & .6027   & .7215 & .8053    & .8743 & \multicolumn{1}{r|}{}        & .2362 & .2731   & .3196 & .3854    & .5157 & \multicolumn{1}{r|}{}        & .2366 & .2798   & .3295 & .4077    & .5488 \\
20                                      & \multicolumn{1}{r|}{}        & .3403 & .4781   & .5810 & .6621    & .7366 & \multicolumn{1}{r|}{}        & .4181 & .5913   & .7100 & .7957    & .8658 & \multicolumn{1}{r|}{}        & .2409 & .2765   & .3208 & .3850    & .5062 & \multicolumn{1}{r|}{}        & .2399 & .2823   & .3298 & .4061    & .5436 \\
50                                      & \multicolumn{1}{r|}{}        & .3422 & .4799   & .5804 & .6609    & .7343 & \multicolumn{1}{r|}{}        & .4282 & .5840   & .6979 & .7871    & .8599 & \multicolumn{1}{r|}{}        & .2452 & .2792   & .3214 & .3832    & .5014 & \multicolumn{1}{r|}{}        & .2425 & .2836   & .3298 & .4053    & .5414 \\
80                                      & \multicolumn{1}{r|}{}        & .3432 & .4811   & .5807 & .6602    & .7328 & \multicolumn{1}{r|}{}        & .4380 & .5831   & .6929 & .7844    & .8576 & \multicolumn{1}{r|}{}        & .2465 & .2793   & .3220 & .3832    & .5008 & \multicolumn{1}{r|}{}        & .2430 & .2839   & .3297 & .4049    & .5408 \\ \hline
delta/\%                                & \multicolumn{1}{r|}{}        & 3.72  & 3.35    & 0.78  & -0.20    & -1.01 & \multicolumn{1}{r|}{}        & 5.62  & -3.64   & -4.93 & -3.41    & -2.63 & \multicolumn{1}{r|}{}        & 6.16  & 3.56    & 2.09  & -0.16    & -5.04 & \multicolumn{1}{r|}{}        & 3.71  & 2.53    & 0.76  & -0.66    & -2.64 \\ \hline
\end{tabular}
\end{table*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion} \label{sec:discussion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
With the confirmed effectiveness of the cost-sensitive strategy in promoting user fairness, we also notice that the promising results are likely to be subject to sufficient relevant transactions used for training, validation, and testing. \zhe{In other words, a good estimation of the performance on the test set might be the result of a model trained by sufficient relevant data as well as a robust validation set that can accurately reflect the recommendation accuracy.} Intuitively, when there are few relevant interactions for training, we cannot get a predictive model; when there are not enough interactions for validation and testing, evaluation metrics will suffer from serious randomness, and we cannot get any conclusive insights from the results got in this way, and thus the cost-sensitive weights will not be convincing. In this section, we dive deep into these two factors and try to provide suggestions for the requirement of the sample size for training/validation/testing.\ju{we're still missing the point of this part: all of the above assumes we can get a good estimate of test performance via the validation performance. That depends on the amount of data.}
\zhe{RE: we need to add it as above, though not fully like this: here there is no assumption yet that validation size is more important: we assume that the good correlation is a result of a well-trained model + a reflective validation set. There is no direct link pointing the validation size yet. We need to show this from the insights.}

% maybe no room for the number of users and items in each subset
By filtering out users with fewer interactions, we can generate subsets with no fewer than {3, 4, 5, 10} samples for each user in the training set, and at least {1, 2, 3, 4, 5} samples for validation and test. To make the distributions of the validation and test sets similar, we set the number of relevant samples in the validation and test sets the same. Therefore, the most relaxing subset is with at least 5 samples per user. On the other way, all users contain at least 20 samples in the most strict subset. The training/validation/testing split is thus conducted as per ratio. For example, for the combination of 4/3/3, each user has 40\% of her relevant items for training, and 30\% for validation and testing, respectively. All other training and evaluation protocols are inherited from that mentioned in Section~\ref{sec:exp}.
\ju{I got lost :-\ Why are we talking about all this?}
\zhe{RE: We need to state two things. First, the minimum sample size for train/val/test; second, we cannot only mention the lower bound of sampling. Users with more items will for sure have more than 5 for training, and the data split is not for keeping exactly 5 items for training. Instead, that is we sample based on the ratio, and active users will have more than the lower bound of items. I tried to have an example to show this but I do feel uncomfortable as well. Any suggestion on this part?}

\zhe{To investigate the influence of sample sizes for training/validation on how well the validation performance can reflect the test performance,} we measure the correlation of the \zhe{nDCG score of individual users}
\ju{what utilities? You need to realize that the reader hasn't spent the last year working on this with you :-)} in statistical methods including Root Mean Square Error (RMSE) and Spearman's Rank-Order Correlation based on the quantile of nDCG scores for each individual user in the dataset. \zhe{}Fig.~\ref{fig:corr} shows the results as per different minimum numbers of interactions for training/validation/testing \ju{why is it relevant to show this for different number of min interactions?}. Results on all 4 datasets show similar trends and insights, so here we only present the ones from the densest and the most sparse dataset, namely MovieLens 1M and Amazon Musical Instruments.
% Figure environment removed

From the figure, we mainly observe that 1) the correlation between the performance on validation and test sets relies on both the number of items for training and validation/testing; 2) more items involved for validation/testing tend to provide more stable results on the user level, which means that it needs more interactions to make sure that the performance on the validation set could genuinely reflect that on the test set; 3) counterintuitively, the increase of training size does not necessarily lead to better correlations; 4) results suggest that the empirical number of items for validation/testing that could satisfy the requirement of solid research on user fairness should be no smaller than 3, while 5 can provide better correlation. On the other end, 1 and 2 items for validation/testing per user lead to much lower correlations, which means the results on the validation set cannot reflect the performance on the test set. This means few-shot samples in validation/testing might not be sufficient to draw any conclusive insights. Since for non-time-aware recommendations, we usually do the training with more data used for validation/testing, it would be good if we also have at least 5 items per user for training.

All in all, we make the suggestion of data requirements, mainly regarding the minimum number of items for each user to get meaningful results on research focusing on the fairness of individual users, which is at least 5 for each subset used for training, validation, and testing. This setting is also adopted in Section~\ref{sec:res} for our experiments on the cost-sensitive strategy. \zhe{Results not included in this paper due to the page limit also indicate that the effect of cost-sensitive learning on the validation set is highly similar to that on the presented effects on the test set, which means the test performance genuinely reflects the validation performance. }

\ju{not really sure whether this section should be down here at the end. If we explain in detail where the weights come from, it becomes clear the potential issue of having poor estimates of test performance via the validation performance. What determines that is, mostly, the amount of data, so we first look into how much we need. This could have implications for research in general, yes, but we can talk about those implications in the section "discussion" where this is all in now. Right now it's not really discussing your results or implications, but checking prerequisites to arrive at those results.}
\tocheck{From Zhe: this is still an open issue subject to discussion}

% \subsection{Results and Discussion} \label{subsec:res1}
% We measure the correlation of the user utilities in several statistical methods, including Pearson Correlation Coefficient (PCC), Spearman Correlation Coefficient, and Rooted Mean Square Error (RMSE) based on the nDCG scores of each user in the dataset. Fig.~\ref{fig:num} shows the results as per different minimum numbers of interactions for training/validation/test. 

% From the figure, we mainly observe that 1) the correlation between the performance on validation and test sets relies on both the number of items for training and validation/test; 2) more items involved could always provide more stable results on the user level, which means that it needs more interactions to make sure that the performance on the validation set could genuinely reflect that on the test set; 3) compared to the number of items used for training, it is more important to have enough relevant interactions for validation and test; 4) results suggest that the empirical number of items that could satisfy the requirement of solid research on user fairness should be no smaller than 3, while 5 is more optimal and could also involve more users into investigation compared to 10. On the other end, 1 and 2 items per user might not be sufficient to draw any conclusive insights. 

% All in all, we make the suggestion of data requirements, mainly regarding the minimum number of items for each user to get meaningful results on research focusing on the fairness of individual users, which is at least 5 for each subset used for training, validation, and testing. We follow this suggestion in the rest of this paper. The statistics under such a pre-filtering on all four datasets we use are shown in Table~\ref{tab:stats2}. 

% \begin{table}[t]
% \caption{Dataset statistics after pre-filtering.}
%     \centering{\small
%     \begin{tabular}{@{}ccrrrr@{}}
%         \hline
%         Dataset & \#users & \#items & \#ratings & Density/\% \\
%         \hline
%         Movielens 1M &  & 2,324 & 46,957 & 0.49 \\
%         BeerAdvocate & 2,038 & 1,288 & 20,151 & 0.77 \\
%         Amazon Digital Music & 17,345 & 39,951 & 537,565 & 0.08  \\
%         Amazon Musical Instruments &&&&\\
%         \hline
%       \end{tabular}}
%     \label{tab:stats2}
% \end{table}

% \section{Experiments} \label{sec:exp}
% To test the effectiveness of the cost-sensitive weights on promoting the utility of non-mainstream users, we design a set of experiments for empirical analysis. To investigate the effect of the cost-sensitive weights as per their extent of stress on non-mainstream users, we adjust the sharpness of the $G$ function, to make the highest weight as [5, 10, 15, 20, 30, 40, 60, 80] times as the lowest weight. With more focus on non-mainstream users, compared to the vanilla FM model, we expect the model to get better improvements for them in exchange of more performance loss on mainstream users. We use the post-processed datasets presented in Table~\ref{tab:stats2}, and follow the same protocols as in Section~\ref{sec:datareq}. 





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions and Future Work} \label{sec:con}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In this paper, we tackled the problem of directly emphasizing the importance of non-mainstream users to achieve better utility balance across users. By assigning higher weights to the suffering users, the proposed cost-sensitive learning strategy shows significant performance improvements for better balance across users, without losing much overall recommendation utilities. We also provide suggestions on the size of relevant transactions for training/validation/testing needed for generating convincing results in individual-level research on user fairness.

For future work, we are interested in the generality of our strategy, to see whether the proposed cost-sensitive strategy works for other ranking metrics such as average precision (AP) and reciprocal rank (RR), for more underlying models other than FM, and even for different ranking frameworks such as pairwise and listwise methods. We are also keen to investigate the combination of cost-sensitive and adversarial learning strategies, to see whether they can work together on further improving individual user fairness in CF recommenders.



% adversarial learning; zhu's paper; deficiencies; in this paper, new way of doing so; also checks the prerequisites for data availability



%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\newpage

\endinput
%%
%% End of file `sample-authordraft.tex'.
