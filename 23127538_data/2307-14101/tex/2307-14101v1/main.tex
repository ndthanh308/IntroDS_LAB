%\include{header.sty}
\documentclass{article}
\usepackage{arxiv}
\include{header_ArXiv.sty}
\bibliographystyle{splncs04}

\title{Градиентные методы для минимизационных задач с условием Поляка--Лоясиевича: относительные помехи в градиенте и адаптивный подбор параметров}

\author{
    Пучинин\,С.\,М. \\
    МФТИ \\
    Москва, Россия \\
    \texttt{puchinin.sm@phystech.edu} \\
    \And
    Стонякин\,Ф.\,С. \\
    МФТИ \\
    Москва, Россия \\
    КФУ им.~В.\,И.\,Вернадского \\
    Симферополь, Россия \\
    \texttt{fedyor@mail.ru}
}

\date{}

\begin{document}

\maketitle

\begin{abstract}
    В работе рассматривается задача минимизации гладкой целевой функции, удовлетворяющей хорошо известному условию Поляка--Лоясиевича. Данная задача в различных вариациях встречается во многих областях машинного обучения. Недавно, авторами в \cite{kuruzov2023gradient} был предложен адаптивный алгоритм для случая, когда помехи в градиенте имеют абсолютный характер. Нами же предлагается адаптивный алгоритм для случая относительных помех в градиенте, в котором настраивается как параметр гладкости функции, так и параметр неточности градиента. Приводится оценка качества получаемой предлагаемым алгоритмом выходной точки.
    \keywords{Адаптивный метод \and Градиентный метод \and Условие Поляка--Лоясиевича \and Неточный градиент \and Относительная неточность}
\end{abstract}

\section{Введение}

С увеличением числа приложений, которые могут быть смоделированы как оптимизационные задачи большой или даже огромной размерности (некоторые из таких приложений возникают в машинном обучении, глубоком обучении, оптимальном управлении, обработке сигналов, статистике и т.~д.), методы первого порядка, которые требуют низкой стоимости итерации, а также небольшого объема памяти, привлекают большое внимание научного сообщества \cite{beck2017first}. Градиентные методы можно рассматривать как ключевое направление численных методов решения оптимизационных задач.

Для задачи минимизации гладкой функции $f$ хорошо известно, что если $f$ сильно выпукла, то метод градиентного спуска имеет глобальную линейную сходимость \cite{nesterov2004introductory}. Однако многие фундаментальные аспекты машинного обучения, такие как метод наименьших квадратов и логистическая регрессия, имеют целевые функции, которые являются выпуклыми, но не сильно выпуклыми. Этот вопрос привел к поиску и исследованию альтернатив сильной выпуклости. Одной из таких альтернатив является условие Поляка--Лоясиевича. Это неравенство было первоначально введено Б.\,T.\,Поляком \cite{polyak1963gradient}, который доказал, что этого условия достаточно, чтобы показать глобальную линейную скорость сходимости для градиентного спуска без предположения о выпуклости.

В методах первого порядка предполагается наличие точного оракула первого порядка. То есть, оракул должен предоставлять в каждой заданной точке точные значения функции и ее градиента. Но, к сожалению, во многих приложениях нет доступа к этой точной информации (особенно к информации о градиенте) на каждой итерации метода. Это привело исследователей к изучению поведения методов первого порядка, которые могут работать с неточным оракулом. В работе \cite{devolder2013first} (которую можно считать фундаментальной в этом направлении) авторы вводят понятие неточного оракула первого порядка, которое естественно возникает во многих ситуациях.

Часто при анализе сходимости метода градиентного спуска подразумевается постоянная величина размера шага, которая зависит от константы Липшица градиента целевой функции (константы гладкости). Однако во многих прикладных задачах эту константу трудно оценить. Например, известная функция Розенброка и ее многомерные порождения (например, функция Нестерова--Скокова) имеют только локально липшицево-непрерывный градиент. Для того чтобы преодолеть трудности с определением значения константы Липшица градиента, было разработано много методов, одним из которых является метод градиентного типа с адаптивной политикой размера шага.

Недавно в \cite{polyak2022stopping} для задачи минимизации c гладкой целевой функцией, удовлетворяющей PL-условию (именно эта задача рассматривается в данной работе), авторы предложили неадаптивные и адаптивные градиентные методы, использующие понятие неточного градиента. Они проанализировали предложенные алгоритмы и влияние помех в градиенте на скорость сходимости. Однако, в их работе адаптивность имеет место лишь в отношении константы Липшица градиента; по-прежнему необходимо точно знать величину помех в градиенте. Авторам же \cite{kuruzov2023gradient} был предложен и проанализирован адаптивный алгоритм, который адаптивно настраивал не только константу гладкости функции, но и величину помех в градиенте. Однако в обоих работах \cite{kuruzov2023gradient,polyak2022stopping}, рассматривался лишь случай абсолютных помех в градиенте.

В данной работе мы продолжаем исследования по построению адаптивного градиентного метода, впервые исследованного в \cite{polyak2022stopping} и далее исследованного в \cite{kuruzov2023gradient}, и предлагаем адаптивный алгоритм для задач с целевыми функциями, удовлетворяющими условию Поляка--Лоясиевича, при наличии относительных помех в градиенте, с подробным анализом его сходимости и оценкой расстояния от начальной точки до точки выхода алгоритма. Адаптивность в предложенном в данной работе алгоритме будет заключаться в обоих параметрах: константе Липшица градиента и величине относительных помех в градиенте. Таким образом, предложенный алгоритм является полностью адаптивным.

\section{Постановка задачи и основные понятия}\label{sec:2}

Рассматривается минимизационная задача (в общем случае невыпуклая)
\begin{equation}\label{eq:L}
    \min\limits_{x \in \mathbb{R}^n} f(x),
\end{equation}
где целевая функция является $L$-гладкой и удовлетворяет условию Поляка--Лоясиевича.

\begin{definition}
    Дифференцируемая функция $f: \mathbb{R} ^n \to \mathbb{R}$ называется $L$-гладкой относительно нормы $\lVert \cdot \rVert$, если для некоторой константы $L > 0$ выполнено, что
    \begin{equation}
        \lVert \nabla f(y) - f(x) \rVert \leq L \lVert y - x \rVert.
    \end{equation}
\end{definition}
Далее в работе норма везде подразумевается евклидовой.

\begin{definition}
    $L$-гладкая функция $f$ удовлетворяет условию Поляка--Лоясиевича (или, для краткости, PL-условию), если выполнено следующее неравенство
    \begin{equation}\label{eq:PL}
        f(x) - f^* \leq \frac{1}{2 \mu} \lVert \nabla f(x) \rVert^2 \quad \forall x \in \mathbb{R} ^n,
    \end{equation}
    где $\mu > 0$~--- некоторая константа, а $f^* \coloneqq f(x^*)$, где $x^* \in X^*$ ($X^*$~--- множество точных решений рассматриваемой минимизационной задачи).
\end{definition}

В данной работе предлагается рассмотреть проблему поведения методов градиентного типа для отмеченного выше класса задач в случае относительных помех информации о градиенте. Недавно в \cite{kuruzov2023gradient,polyak2022stopping} были детально исследованы адаптивные методы градиентного типа при наличии абсолютных помех. Мы же рассматриваем ситуацию, что градиент известен с относительной погрешностью, то есть верно неравенство
\begin{equation}\label{eq:rel_err}
    \lVert \Tilde{\nabla} f(x) - \nabla f(x) \rVert \leq \alpha \lVert \nabla f(x) \rVert,
\end{equation}
где за $\Tilde{\nabla} f(x)$ обозначен неточный градиент, а $\alpha \in [0, 0.5)$~--- некоторая константа, отвечающая за величину помех в градиенте. Данное условие на неточный градиент было введено и изучено в работах \cite{carter1991global,polyak1987introduction}. Отметим, что относительные помехи в градиенте могут возникать из-за приближенного его вычисления, см.~например \cite{berahas2022theoretical}.

Из \eqref{eq:rel_err} можно получить, что
\begin{equation}\label{eq:norm_ineqs}
    (1 - \alpha) \lVert \nabla f(x) \rVert \leq \lVert \Tilde{\nabla} f(x) \rVert \leq (1 + \alpha) \lVert \nabla f(x) \rVert.
\end{equation}
Откуда можно получить условие типа PL \eqref{eq:PL} для неточного градиента
\begin{equation}\label{eq:PL_rel}
    f(x) - f^* \leq \frac{1}{2 \mu (1 - \alpha)^2} \lVert \Tilde{\nabla} f(x) \rVert^2.
\end{equation}

\section{Адаптивный градиентный метод}

В \cite{kuruzov2023gradient} авторы предложили адаптивный Алгоритм~\ref{alg:1}, который является обобщением универсального градиентного метода \cite{nesterov2014universal} для работы с неточным градиентом функций, удовлетворяющих PL-условию. Адаптивность в Алгоритме~\ref{alg:1} относится как к константе гладкости функции $L$, так и к величине абсолютных помех в градиенте $\Delta$.

\begin{algorithm}
    \caption{Градиентный спуск с адаптивной настройкой $L$ и $\Delta$.}
    \label{alg:1}
    \begin{algorithmic}
        \State \textbf{Вход:} $x^0$, $L^{min} \geq \frac{\mu}{4}$, $L^0 \geq L^{min}$, $\Delta^0 > 0$, $\Delta^{min} > 0$.
        \State \textbf{1.} $k \coloneqq 0$.
        \State \textbf{2.} $L^k \coloneqq \max\left\{ \frac{L^{k - 1}}{2}, L^{min} \right\}$
        \State \textbf{3.}
            \begin{equation}
                x^{k + 1} = x^k - \frac{1}{2 L^k} \Tilde{\nabla} f(x^k)
            \end{equation}
        \State \textbf{4.} Если
            \begin{equation}\label{eq:alg1_cond}
                f(x^{k + 1}) \leq f(x^k) + \left\langle \Tilde{\nabla} f(x^k), x^{k + 1} - x^k \right\rangle + \Delta^k \left\lVert x^{k + 1} - x^k \right\rVert + \frac{L^k}{2} \left\lVert x^{k + 1} - x^k \right\rVert^2,
            \end{equation}
            то переходим на шаг 5. Иначе, $L^k \coloneqq 2 L^k$, $\Delta^k \coloneqq 2 \Delta^k$ и возвращаемся на шаг 3.
        \State \textbf{5.} Находим минимальное $\Delta^k$, при котором выполняется \eqref{eq:alg1_cond}, и при этом $\Delta^k \geq \Delta^{min}$ и $\Delta^k \geq \max\limits_{j < k} \Delta^j$ при $k \geq 1$.
        \State \textbf{6.} $L^k \coloneqq \max\left\{ \frac{L^k}{2}, L^{min} \right\}$; $x^{k + 1} = x^k - \frac{1}{2 L^k} \Tilde{\nabla} f(x^k)$.
        \State \textbf{7.} Если выполняется \eqref{eq:alg1_cond}, переходим на шаг 6. Иначе, $k \coloneqq k + 1$ и переходим на шаг 3.
        \State \textbf{8.} \textbf{Выход:} $x^k$.
    \end{algorithmic}
\end{algorithm}

Как и в \cite{kuruzov2023gradient}, описанную в разделе \ref{sec:2} задачу предлагается решать методом градиентного типа в виде
\begin{equation}\label{eq:method}
    x^{k + 1} = x^k - h^k \Tilde{\nabla} f(x^k).
\end{equation}
Как хорошо известно, условие $L$-гладкости \eqref{eq:L} влечет следующее неравенство
\begin{equation}
    f(y) \leq f(x) + \left\langle \nabla f(x), y - x \right\rangle + \frac{L}{2} \lVert y - x \rVert^2.
\end{equation}
Используя \eqref{eq:rel_err}, \eqref{eq:norm_ineqs} преобразуем данное неравенство
\begin{equation}
    \begin{aligned}
        f(y)
        &\leq f(x) + \langle \nabla f(x), y - x \rangle + \frac{L}{2} \lVert y - x \rVert^2 \\
        &= f(x) + \langle \Tilde{\nabla} f(x), y - x \rangle + \frac{L}{2} \lVert y - x \rVert^2 + \langle \nabla f(x) - \Tilde{\nabla} f(x), y - x \rangle \\
        &\leq f(x) + \langle \Tilde{\nabla} f(x), y - x \rangle + \frac{L}{2} \lVert y - x \rVert^2 + \lVert \nabla f(x) - \Tilde{\nabla} f(x) \rVert \lVert y - x \rVert \\
        &\leq f(x) + \langle \Tilde{\nabla} f(x), y - x \rangle + \frac{L}{2} \lVert y - x \rVert^2 + \alpha \lVert \nabla f(x) \rVert \lVert y - x \rVert \\
        &\leq f(x) + \langle \Tilde{\nabla} f(x), y - x \rangle + \frac{L}{2} \lVert y - x \rVert^2 + \frac{\alpha}{1 - \alpha} \lVert \Tilde{\nabla} f(x) \rVert \lVert y - x \rVert.
    \end{aligned}
\end{equation}
То есть
\begin{multline}\label{eq:cond}
    f(x^{k + 1}) \leq f(x^k) + \left\langle \Tilde{\nabla} f(x^k), x^{k + 1} - x^k \right\rangle + \frac{L}{2} \left\lVert x^{k + 1} - x^k \right\rVert^2 +\\+ \frac{\alpha}{1 - \alpha} \left\lVert \Tilde{\nabla} f(x^k) \right\rVert \left\lVert x^{k + 1} - x^k \right\rVert.
\end{multline}
Из \eqref{eq:method} имеем
\begin{equation}
    \left\lVert x^{k + 1} - x^k \right\rVert = h^k \left\lVert \Tilde{\nabla} f(x^k) \right\rVert
\end{equation}
и
\begin{equation}
    \left\langle \Tilde{\nabla} f(x^k), x^{k + 1} - x^k \right\rangle = h^k \left\lVert \Tilde{\nabla} f(x^k) \right\rVert^2.
\end{equation}
Объединяя полученные выражения, получаем
\begin{equation}
    f(x^{k + 1}) - f(x^k) \leq \left( - h^k + \frac{L \left(h^k\right)^2}{2} + \frac{\alpha h^k}{1 - \alpha} \right) \left\lVert \Tilde{\nabla} f(x^k) \right\rVert^2.
\end{equation}
Так как $h^k > 0$, минимум выражения, стоящего в скобках, достигается при
\begin{equation}
    h^k = \max\left\{ 0, \frac{1}{L} \frac{1 - 2\alpha}{1 - \alpha} \right\}.
\end{equation}
Если $\alpha \geq 0.5$, то этот минимум равен $0$ при $h^k = 0$, что говорит о том, что метод градиентного типа не применим подобным образом в этом случае, по крайней мере с точки зрения рассматриваемой теории, так как значения целевой функции не убывают с номером итерации. Поэтому в \eqref{eq:rel_err} рассматриваются лишь $\alpha \in [0, 0.5)$. При таком $\alpha$ и при выборе $\displaystyle h^k = \frac{1}{L} \frac{1 - 2\alpha}{1 - \alpha}$ имеем
\begin{equation}\label{eq:funcdiff}
    f(x^{k + 1}) - f(x^k) \leq - \frac{1}{2L} \frac{(1 - 2\alpha)^2}{(1 - \alpha)^2} \left\lVert \Tilde{\nabla} f(x^k) \right\rVert^2.
\end{equation}
Наконец, объединив последнее неравенство с \eqref{eq:PL_rel}, получаем
\begin{equation}\label{eq:prefinal}
    f(x^{k + 1}) - f^* \leq \left( 1 - \frac{\mu}{L} (1 - 2\alpha)^2\right) \left( f(x^k) - f^* \right).
\end{equation}
То есть
\begin{equation}
    f(x^N) - f^* \leq \left( 1 - \frac{\mu}{L} (1 - 2\alpha)^2 \right)^N \left( f(x^0) - f^* \right).
\end{equation}
Получаем сходимость по функции со скоростью геометрической прогрессии.

Однако, для вычисления $h^k$ необходимо знать $L$ и $\alpha$. Данную проблему, как и в \cite{kuruzov2023gradient,polyak2022stopping}, предлагается решать с помощью адаптивной политики размера шага, настраивая на каждой итерации значения $L^{k + 1}$ и $\alpha^{k + 1}$ так, чтобы для них выполнялось неравенство \eqref{eq:cond}. Для этого дополнительно введем параметр $\beta \in (0, 0.5]$ следующим образом: $\beta = 0.5 - \alpha$. Предлагаемый алгоритм, основанный на Алгоритме \ref{alg:1}, приведен ниже как Алгоритм~\ref{alg:2}.

\begin{algorithm}
    \caption{Градиентный спуск с адаптивной настройкой $L$ и $\alpha$.}
    \label{alg:2}
    \begin{algorithmic}
        \State
        \textbf{Вход:} $x^0$, $L^{min} \geq 0$, $L^0 \geq L^{min}$, $\alpha^{min} \in [0, 0.5)$, $\alpha^0 \in [\alpha^{min}, 0.5)$.
        \begin{enumerate}
            \renewcommand{\labelenumi}{\textbf{\arabic{enumi}:}}
            \item
                $k \coloneqq 0$; $\beta^{max} = 0.5 - \alpha^{min}$; $\beta^0 = 0.5 - \alpha^0$.
            \item
                $L^{k + 1} \coloneqq \max\left\{ \frac{L^k}{2}, L^{min} \right\}$; $\beta^{k + 1} = \min\left\{ 2\beta^k, \beta^{max} \right\}$; $\alpha^{k + 1} = 0.5 - \beta^{k + 1}$.
            \item
                \begin{equation}\label{eq:alg2_step}
                    x^{k + 1} = x^k - \frac{1}{L^{k + 1}} \frac{1 - 2\alpha^{k + 1}}{1 - \alpha^{k + 1}} \Tilde{\nabla} f(x^k)
                \end{equation}
            \item Если
                \begin{multline}\label{eq:alg2_cond}
                    f(x^{k + 1}) \leq f(x^k) + \left\langle \Tilde{\nabla} f(x^k), x^{k + 1} - x^k \right\rangle + \frac{L^{k + 1}}{2} \left\lVert x^{k + 1} - x^k \right\rVert^2 +\\+ \frac{\alpha^{k + 1}}{1 - \alpha^{k + 1}} \left\lVert \Tilde{\nabla} f(x^k) \right\rVert \left\lVert x^{k + 1} - x^k \right\rVert.
                \end{multline}
                то переходим на шаг 5. Иначе, $L^{k + 1} \coloneqq 2 L^{k + 1}$, $\beta^{k + 1} \coloneqq \frac{1}{2} \beta^{k + 1}$, $\alpha^{k + 1} = 0.5 - \beta^{k + 1}$ и возвращаемся на шаг 3.
            \item
                $k \coloneqq k + 1$ и переходим на шаг 3.
        \end{enumerate}
        \textbf{Выход:} $x^k$.
    \end{algorithmic}
\end{algorithm}

Проведен анализ получаемой данным алгоритмом выходной точки. В силу \eqref{eq:alg2_cond}, выполняется следующее неравенство, аналогичное \eqref{eq:prefinal}
\begin{equation}
    f(x^{k + 1}) - f^* \leq \left( 1 - \frac{\mu}{L^{k + 1}} (1 - 2\alpha^{k + 1})^2 \right) \left( f(x^k) - f^* \right).
\end{equation}
То есть
\begin{equation}
    \begin{aligned}
         f(x^N) - f^* &\leq \prod\limits_{k = 0}^{N - 1} \left( 1 - \frac{\mu}{L^{k + 1}} (1 - 2\alpha^{k + 1})^2 \right) \left( f(x^0) - f^* \right) \\
         &\leq \left( 1 - \frac{\mu}{L^{max}} (1 - 2\alpha^{max})^2 \right)^N \left( f(x^0) - f^* \right) \\
         &\leq \exp\left( - \frac{\mu}{L^{max}} (1 - 2\alpha^{max})^2 N \right) \left( f(x^0) - f^* \right),
    \end{aligned}
\end{equation}
где $L^{max} = \max\limits_{j = \overline{1, N}} L^j$ и $\alpha^{max} = \max\limits_{j = \overline{1, N}} \alpha^j$. Отметим, что $\alpha^{max} = 0.5 - \beta^{min}$, где $\beta^{min} = \min\limits_{j = \overline{1, N}} \beta^j$.

Оценим $L^{max}$. Рассмотрим произвольную итерацию $j$. Предположим, что $\frac{\beta^j}{\beta} \leq \frac{L}{L^j}$. Тогда если $L^j$ достигает $L$, то $\beta^j$ также достигнет $\beta$, и больше не будет повторений пункта 4. То есть $L^j \leq 2L$. С другой стороны, если $\frac{\beta^j}{\beta} > \frac{L}{L^j}$, тогда в худшем случае $L^j = L$ на старте повторений пункта 4, то есть к концу этих повторений $L^j \leq \frac{2 \beta^k}{\beta} L \leq \frac{2 \beta^{max}}{\beta} L = \frac{2 (0.5 - \alpha^{min})}{0.5 - \alpha} L$. Следовательно, $L^{max} \leq 2L \max\left\{ 1, \frac{0.5 - \alpha^{min}}{0.5 - \alpha}\right\}$.

Теперь оценим $\beta^{min}$, тем самым оценив $\alpha^{max}$. Опять же рассмотрим произвольную итерацию $j$, Предположим, что $\frac{\beta^j}{\beta} \geq \frac{L}{L^j}$. Тогда если $\beta^j$ достигает $\beta$, то $L^j$ также достигнет $L$, и больше не будет повторений пункта 4. То есть $\beta^j \geq \frac{\beta}{2}$. С другой стороны, если $\frac{\beta^j}{\beta} < \frac{L}{L^j}$, тогда в худшем случае $\beta^j = \beta$ на старте повторений пункта 4, то есть к концу этих повторений $\beta^j \geq \frac{L^j}{2L} \beta \geq \frac{L^{min}}{2L} \beta$. Следовательно, $\beta^{min} \geq \frac{\beta}{2} \min\left\{ 1, \frac{L^{min}}{L} \right\} = \frac{0.5 - \alpha}{2} \min\left\{ 1, \frac{L^{min}}{L} \right\}$.

Введем переобозначения. $L^{max} \coloneqq 2L \max\left\{ 1, \frac{0.5 - \alpha^{min}}{0.5 - \alpha}\right\}$, $\alpha^{max} \coloneqq 0.5 - \frac{0.5 - \alpha}{2} \min\left\{ 1, \frac{L^{min}}{L} \right\}$. В итоге, число итераций $N_*$ гарантирующее достижение $\varepsilon$-точности по функции ($f(x^N) - f^* \leq \varepsilon$) задается формулой
\begin{equation}
    N_* = \left\lceil \frac{L^{max}}{\mu} \frac{1}{(1 - 2\alpha^{max})^2} \log\left( \frac{f(x^0) - f^*}{\varepsilon} \right)\right\rceil.
\end{equation}

Далее, оценим расстояние $\left\lVert x^N - x^0 \right\rVert$. Из \eqref{eq:funcdiff} имеем
\begin{equation}
    \frac{1}{2L^{k + 1}} \frac{(1 - 2\alpha^{k + 1})^2}{(1 - \alpha^{k + 1})^2} \left\lVert \Tilde{\nabla} f(x^k) \right\rVert^2 \leq f(x^k) - f(x^{k + 1}).
\end{equation}
Следовательно, согласно \eqref{eq:alg2_step},
\begin{equation}
    \begin{aligned}
        \left\lVert x^{k + 1} - x^k \right\rVert^2
        &= \left( \frac{1}{L^{k + 1}} \frac{1 - 2\alpha^{k + 1}}{1 - \alpha^{k + 1}} \right)^2 \left\lVert \Tilde{\nabla} f(x^k) \right\rVert^2 \\
        &\leq \frac{2}{L^{k + 1}} \left( f(x^k) - f(x^{k + 1}) \right) \\
        &\leq \frac{2}{L^{k + 1}} \left( f(x^k) - f^* \right) \\
        &\leq \frac{2}{L^{k + 1}} \left( 1 - \frac{\mu}{L^{max}} (1 - 2\alpha^{max})^2 \right)^k \left( f(x^0) - f^* \right) \\
        &\leq \frac{2}{L^{min}} \left( 1 - \frac{\mu}{L^{max}} (1 - 2\alpha^{max})^2 \right)^k \left( f(x^0) - f^* \right).
    \end{aligned}
\end{equation}
Наконец,
\begin{equation}
    \begin{aligned}
        \left\lVert x^N - x^0 \right\rVert
        &\leq \sum\limits_{k = 0}^{N - 1} \left\lVert x^{k + 1} - x^k \right\rVert \\
        &\leq \sqrt{\frac{2}{L^{min}} \left( f(x^0) - f^* \right)} \sum\limits_{k = 0}^{N - 1} \left( 1 - \frac{\mu}{L^{max}} (1 - 2\alpha^{max})^2 \right)^{k/2} \\
        &= \sqrt{\frac{2}{L^{min}} \left( f(x^0) - f^* \right)} \frac{1 - \left( 1 - \frac{\mu}{L^{max}} (1 - 2\alpha^{max})^2 \right)^{N/2}}{1 - \left( 1 - \frac{\mu}{L^{max}} (1 - 2\alpha^{max})^2 \right)^{1/2}} \\
        &\leq \sqrt{\frac{2}{L^{min}} \left( f(x^0) - f^* \right)} \frac{2 L^{max}}{\mu} \frac{1}{(1 - 2\alpha^{max})^2}.
    \end{aligned}
\end{equation}
Отметим, что это расстояние не зависит ни от $N$, ни от невязки по функции на $N$-ом шаге.

Если задать критерий остановки в виде $\left\lVert \Tilde{\nabla} f(x^{k + 1}) \right\rVert \leq \varepsilon$, то из \eqref{eq:PL_rel} будет следовать
\begin{equation}
    f(x^{k + 1}) - f^* \leq \frac{\varepsilon^2}{2 \mu (1 - \alpha)^2} \leq \frac{\varepsilon^2}{2 \mu (1 - \alpha^{max})^2},
\end{equation}
то есть для выполнения данного критерия потребуется не более
\begin{equation}
   \left\lceil \frac{L^{max}}{\mu} \frac{1}{(1 - 2\alpha^{max})^2} \log\left( \frac{2 \mu (f(x^0) - f^*) (1 - \alpha^{max})^2}{\varepsilon^2} \right)\right\rceil
\end{equation}
итераций алгоритма.

Также, можно оценить число повторений отдельно взятых шагов алгоритма. Как было сказано ранее, если $L^{k + 1} \geq L$ и $\beta^{k + 1} \leq \beta$, то повторения 4-го шага заканчиваются. Поэтому повторений 4-го шага на каждой итерации не больше $\log_2 \left( 2 \max\left\{ \frac{L}{L_{min}}, \frac{0.5 - \alpha^{min}}{0.5 - \alpha} \right\}\right)$. Таким образом можно, например, оценить число вычислений функции $f$ на протяжении работы алгоритма. Его оценка сверху имеет вид $N_* \log_2 \left( 2 \max\left\{ \frac{L}{L_{min}}, \frac{0.5 - \alpha^{min}}{0.5 - \alpha} \right\} \right)$.

\section{Вычислительный эксперимент}

Для проведения экспериментального анализа работы Алгоритма \ref{alg:2} были выбраны функция Розенброка, а также функция Нестерова--Скокова. Обе эти функции, как известно, невыпуклы и удовлетворяют PL-условию \ref{eq:PL} на любом компакте.

Все вычисления проводились на платформе Google Colab (\url{https://colab.research.google.com/}). Для генерации равномерного шума из многомерного шара использовался инструмент nengo.dists.UniformHypersphere из библиотеки nengo (\url{https://www.nengo.ai/}).

\subsection{Функция Розенброка}

Функцией Розенброка является следующая функция двух переменных
$$
    f(x_1, x_2) = 100 \left( x_2 - x_1^2 \right)^2 + (x_1 - 1)^2.
$$
Как легко видеть, ее глобальный минимум находится в точке $(x_1, x_2) = (1, 1)$ и $f^* = 0$. На вход алгоритму подавались следующие входные данные: $x^0 = (0, 0)$, $L^{min} = 0.01$, $L^0 = 1$, $\alpha^{min} = 0.001$, $\alpha^0 = 0.01$. В качестве неточного градиента выступал истинный градиент $\nabla f(x_1, x_2)$, зашумленный равномерно распределенным в $2$-мерном шаре радиуса $\alpha \lVert \nabla f(x_1, x_2) \rVert$ шумом, где параметр $\alpha$ варьировался. Результаты эксперимента приведены на рис.~\ref{fig:1}, а также в таблицах~\ref{tab:1} и \ref{tab:2}.

\begin{table}[ht!]
    \centering
    \begin{tabular}{c||c|c|c|c|c|c}
        $\alpha$ & $0.001$ & $0.01$ & $0.1$ & $0.3$ & $0.5$ & $1$\\ \hline
        $f(x_N)$ & $0.0074$ & $0.0075$ & $0.0060$ & $0.0021$ & $0.0018$ & $0.0017$
    \end{tabular}
    \caption{Результат работы Алгоритма \ref{alg:2} после $N = 1000$ итераций для функции Розенброка.}
    \label{tab:1}
\end{table}

\begin{table}[ht!]
    \centering
    \begin{tabular}{c||c|c|c|c|c|c}
        $\alpha$ & $0.001$ & $0.01$ & $0.1$ & $0.3$ & $0.5$ & $1$ \\ \hline
        $f(x_N)$ & $1.5 \times 10^{-19}$ & $1.3 \times 10^{-19}$ & $1.6 \times 10^{-19}$ & $2.6 \times 10^{-16}$ & $2.7 \times 10^{-15}$ & $7.3 \times 10^{-17}$
    \end{tabular}
    \caption{Результат работы Алгоритма \ref{alg:2} после $N = 10000$ итераций для функции Розенброка.}
    \label{tab:2}
\end{table}

Для всех рассмотренных $\alpha$ метод уже к $10000$-ой итерации сходится по функции, и следовательно по аргументу, к глобальному минимуму с машинной точностью. Отметим, что предполагаемое замедление сходимости при увеличении $\alpha$ не наблюдается как минимум до $1000$-ой итерации (см.~таблицу~\ref{tab:1}), однако по итогу при сильном приближении к минимуму, это начинает незначительно сказываться, что видно из таблицы \ref{tab:2}. В том числе, оказалось приемлемым значение $\alpha = 1$, которое в теоретических рассуждениях нами отбрасывалось.

\subsection{Функция Нестерова--Скокова}

Функцией Нестерова--Скокова является следующая функция $n$ переменных
$$
    f(x_1, x_2, \ldots, x_n) = \frac{1}{4} (1 - x_1)^2 + \sum\limits_{i = 1}^{n - 1} \left( x_{i + 1} - 2 x_i^2 + 1 \right)^2.
$$
Эту функцию также называют обобщением функции Розенброка на многомерный случай. Как легко видеть, ее глобальный минимум находится в точке $(x_1, x_2, \ldots, x_n) = (1, 1, \ldots, 1)$ и $f^* = 0$. На вход алгоритму подавались следующие входные данные: $L^{min} = 0.01$, $\alpha^{min} = 0.001$, $\alpha^0 = 0.01$. В качестве неточного градиента выступал истинный градиент $\nabla f(x_1, x_2, \ldots, x_n)$, зашумленный равномерно распределенным в $n$-мерном шаре радиуса $\alpha \lVert \nabla f(x_1, x_2, \ldots, x_n) \rVert$ шумом, где параметр $\alpha$ варьировался. Также, варьировались $x^0$ и $L^0$. Размерность $n$ бралась равной $100$.

Результаты эксперимента приведены на рис.~\ref{fig:2}, а также в таблицах~\ref{tab:3}, \ref{tab:4}, \ref{tab:5} и \ref{tab:6}.

\begin{table}[ht!]
    \centering
    \begin{tabular}{c||c|c|c|c|c|c}
        $\alpha$ & $0.001$ & $0.01$ & $0.1$ & $0.3$ & $0.5$ & $1$\\ \hline
        $f(x_N)$ & $0.058$ & $0.058$ & $0.059$ & $0.073$ & $0.261$ & $2.631$
    \end{tabular}
    \caption{Результат работы Алгоритма \ref{alg:2} после $N = 10$ итераций для функции Нестерова--Скокова ($x^0 = (0, 0, \ldots, 0)$, $L^0 = 1$).}
    \label{tab:3}
\end{table}

\begin{table}[ht!]
    \centering
    \begin{tabular}{c||c|c|c|c|c|c}
        $\alpha$ & $0.001$ & $0.01$ & $0.1$ & $0.3$ & $0.5$ & $1$\\ \hline
        $f(x_N)$ & $0.058$ & $0.058$ & $0.058$ & $0.058$ & $0.058$ & $0.058$
    \end{tabular}
    \caption{Результат работы Алгоритма \ref{alg:2} после $N = 50$ итераций для функции Нестерова--Скокова ($x^0 = (0, 0, \ldots, 0)$, $L^0 = 1$).}
    \label{tab:4}
\end{table}

\begin{table}[ht!]
    \centering
    \begin{tabular}{c||c|c|c|c|c|c}
        $\alpha$ & $0.001$ & $0.01$ & $0.1$ & $0.3$ & $0.5$ & $1$\\ \hline
        $f(x_N)$ & $1.2 \times 10^{-6}$ & $6.7 \times 10^{-5}$ & $0.98$ & $0.98$ & $0.98$ & $0.98$
    \end{tabular}
    \caption{Результат работы Алгоритма \ref{alg:2} после $N = 10$ итераций для функции Нестерова--Скокова ($x^0 = (-1, 1, \ldots, 1)$, $L^0 = 0.1$).}
    \label{tab:5}
\end{table}

\begin{table}[ht!]
    \centering
    \begin{tabular}{c||c|c|c|c|c|c}
        $\alpha$ & $0.001$ & $0.01$ & $0.1$ & $0.3$ & $0.5$ & $1$\\ \hline
        $f(x_N)$& $4.4 \times 10^{-11}$ & $3.2 \times 10^{-9}$ & $0.98$ & $0.98$ & $0.98$ & $0.98$
    \end{tabular}
    \caption{Результат работы Алгоритма \ref{alg:2} после $N = 50$ итераций для функции Нестерова--Скокова ($x^0 = (-1, 1, \ldots, 1)$, $L^0 = 0.1$).}
    \label{tab:6}
\end{table}

Для случая $x^0 = (0, 0, \ldots, 0)$, $L^0 = 1$ метод для всех рассмотренных $\alpha$ быстро сходится к минимуму, но не глобальному, а только локальному, который однако, относительно близок к глобальному по функции. Для случая $x^0 = (-1, 1, \ldots, 1)$, $L^0 = 0.1$ при $\alpha = 0.001$ и $\alpha = 0.01$, метод быстро сходится уже к глобальному минимуму, однако при остальных рассмотренных $\alpha$ метод почти сразу сваливается в локальный минимум с далеким от оптимального значением функции. В обоих случаях, в отличии от функции Розенброка, здесь отчетливо заметно снижение скорости сходимости при увеличении параметра $\alpha$, пусть и не значительное.

\bibliography{citations}

% Figure environment removed

% Figure environment removed

\end{document}
