\section{Related Work}
\label{sec:related_work}

% Figure environment removed

\subsection{Gait Representations}
Gait, encompassing both spatial and temporal information, offers various avenues for representation, primarily classified into \textit{appearance-based} and \textit{human model-based}. Preceding the advent of deep learning, many appearance-based representations~\cite{sarkar2005humanid,chen2009frame,zhang2010active} sought to compress temporal information into a single frame. 
Han and Bhanu \cite{han2005individual} introduced the Gait Energy Image (GEI) template as an average of aligned and normalized silhouette frames. 
Other popular appearance-based gait representations include Frame Difference Energy Image \cite{chen2009frame} and Active Energy Image \cite{zhang2010active}. 
On the other hand, model-based methods represent the whole human body using well-defined models to represent gait. The methods vary by the different techniques used for modeling the human body, such as hidden Markov models~\cite{kale2004identification,liu2006improved}, stride length and walking tempo \cite{benabdelkader2002stride}, 
and Velocity Hough Transform \cite{nash1997dynamic}. 

\subsection{Gait Recognition}

Early \textit{appearance-based} methods employed global gait representation such as silhouettes \cite{chao2019gaitset}, RGB~\cite{guo2022multi, zhang2020learning, li2020end} and GEI \cite{shiraga2016geinet,wu2016comprehensive,hossain2013multimodal} as input to CNNs. The advent of deep learning technology propelled silhouettes to the forefront, primarily owing to their simplicity, privacy, and discriminative capabilities.
Dou \textit{et al.} introduced GaitMPL \cite{dou2022gaitmpl}, a progressive method that learns from simple to hard samples. 
Recently, \cite{ma2023dynamic} proposed a dynamic aggregated network (DANet) to represent contextual relationships by encoding pixel features into magnitude and phase. 
Compared to appearance-based methods, \textit{model-based} approaches~\cite{fu2023gpgait} often yield smaller input sizes extracted by lightweight models, resulting in reduced computational overhead. GaitGraph~\cite{teepe2021gaitgraph} and GaitGraph2~\cite{teepe2022towards} represent pose keypoints as a graph and extract features through Graph Convolutional Network (GCN). GaitTR~\cite{zhang2023spatial} and GaitMixer~\cite{pinyoanuntapong2023gaitmixer} employ transformer architecture to capture global temporal and spatial relationships. 
3D representations extracted using depth sensors \cite{hofmann2014tum,nunes2019benchmark,chattopadhyay2015frontal} and RGB images \cite{liao2020model,liao2017pose,cao2017realtime,kolotouros2021probabilistic,georgakis2020hierarchical} also have shown promising results. 




\noindent\textbf{Recognition in the Wild}: Developing an accurate gait representation in the wild is a long-term goal and has been actively researched over the past decade. Towards that aim, Zhu \textit{et al.} curated a large scale gait dataset called GREW \cite{zhu2021gait}. It is a natural video dataset consisting of 128K sequences of 26K identities captured over 882 cameras. 
Similarly, Zheng \textit{et al.} \cite{zheng2022gait} collected Gait3D dataset that contains silhouettes, 2D/3D keypoints, and 3D meshes for 3D gait recognition. The authors~\cite{zheng2022gait} observed that state-of-the-art gait recognition methods do not yield similar superior performance for GREW and Gait3D as they do for indoor datasets like CASIA-B. 







