\vspace{-0.5cm}
\section{Introduction}
\label{sec:Introduction}

Unconstrained biometric identification, in outdoor and far-away situations, has been a longstanding challenge~\cite{zhu2021gait,zheng2022gait,sepas2022deep,shen2022comprehensive}. RGB-based face and body recognition systems focus on learning \emph{spatially} discriminative features; however, real-world effects like challenging view angles, low face resolution, changing appearances (e.g., clothes and glasses), and long distance turbulence can significantly distort biometric information. Consequently, RGB-based recognition systems tend to perform inconsistently in remote unconstrained scenarios~\cite{li2018resound,li2019repair,weinzaepfel2021mimetics}. 

Gait analysis provides an alternative modality for human recognition by focusing on learning discriminative features in the \emph{temporal} domain. As such, it can be more robust to challenging, unconstrained situations, especially at range, and has been applied in many applications such as human authentication~\cite{benedek2016lidar}, health~\cite{del2019gait} and crime analysis~\cite{hadid2012can}, etc.


The field of gait recognition was initially developed~\cite{sarkar2005humanid,han2005individual,chen2009frame,liao2017pose,an2018improving,sepas2022deep} using traditional methods, such as template matching~\cite{bobick2001recognition,liu2007gait,zhang2010active} and model-based methods~\cite{benabdelkader2002stride,yoo2008automated,boulgouris2007human,li2008gait}, but limited by variations in scale and viewing angle and sensitivity to video quality, respectively. Deep learning (DL)-based approaches~\cite{chao2019gaitset,battistone2019tglstm,lin2021gait} have made significant advances in image and video-based recognition tasks compared to traditional methods. They are able to generate robust identity embeddings by directly processing the complex temporal information present in gait sequences. This enables effective recognition under the variabilities mentioned above, making the DL-based methods widely preferred.




% Figure environment removed


While DL-based gait recognition performs well for indoor scenes, it often fails to achieve good performance in unconstrained/outdoor scenarios. In this work, we seek to apply gait recognition to unconstrained situations with maximal automation. 
The recently collected BRIAR~\cite{briar} dataset contains standing, structured walking, and random walking sequences, which mimic the real-world challenges in gait recognition.
Existing gait recognition methods assume that the subject is always walking with periodic movement and that there are no standing sequences~\cite{yu2006framework, takemura2018multi}. 
By making such assumptions, these methods tend to learn suboptimal representations,
sometimes only achieving 24\% on close range recognition~\cite{lin2021gait}. We also realize that standing segments inherently contain little temporal information, so it is computationally intensive to apply 3D convolutions widely used in gait recognition models. Moreover, the distinctive temporal patterns of standing and gait sequences raise a problem in generating a cohesive feature space for the same identity using one model. This highlights the need for an approach that separates frames that do not contain human motion in order to make gait recognition features more robust.




In the common application scenario of gait recognition - i.e. as a component of end-to-end video recognition, there exists a plethora of additional information at training time that is rarely considered. For instance, RGB images are required for generating human silhouettes. These RGB images contain rich information that can contribute to building robust features not captured by silhouettes alone. Intuitively, the feature space learned by a body recognition model can be used to enhance gait recognition.
Another example is viewpoint information.  Many gait recognition methods employ the practice of \textit{size normalization}~\cite{chao2019gaitset}, where the original masks are cropped and resized to the same resolution regardless of the subject's distance from the camera, leading to information loss. Particularly, such a resizing ratio over frames can implicitly offer important viewpoint information, which is useful for generating effective embeddings~\cite{chai2022lagrange}. Unfortunately, this cue is lost in the resizing operation. Based on our experiments, we show that viewpoint information helps to build a robust representation, especially in unconstrained situations.

 


 

In this paper, we aim to push the performance of unconstrained gait analysis through a framework named GAit Detection and Recognition system in the wild (GADER). 
To address the problem of mixing the moving and standing segments in sequences, we introduce a novel \emph{gait detection} module (\cref{fig:gaitdetect}) that detects the walking and non-walking parts in a sequence, so that gait recognition can just exploit the frames that contain human movement. Instead of using a 3D volume to capture the movement, our gait detector uses the Double Helical Signature~\cite{ran2010applications,niyogi1994analyzing}, a 2D pattern, with a lightweight classification model to segment the walking portion of the input sequence. Thus, we do not provide the entire sequence to the model. Instead, we split the given video into multiple windows of varying lengths to get predictions, followed by Non-Maximum Suppression, to localize the movement duration. This provides a relatively pure gait sequence for gait recognition models, yielding a robust representation and making it suitable for real-world scenarios.

For the gait recognition module, namely GAR, along with the size normalized silhouettes capturing the temporal and body shape information, we further introduce a \emph{cross-modality feature distillation} step, where we guide the intermediate gait features to be more expressive by making them close to features generated from RGB frames. This enables the gait features to maintain their robustness to appearance changes, while also benefiting from the discriminative power of RGB features. As the augmented gait features can be obtained from silhouettes and do not require the RGB frames during inference, we also gain computational efficiency. Additionally, we embed the resizing ratio from the original frame as an attention signal. This \emph{ratio attention} helps to preserve the viewpoint information that is beneficial for robust identity representation.






In summary, GADER makes three contributions:

\begin{itemize}[noitemsep, nolistsep, leftmargin=*]
    \item We introduce a light-weight gait detector to automatically detect frames that contain human movements so that gait recognition and person ReID can cooperate efficiently.
    \item We propose a novel gait recognition training strategy, which leverages the color space and size information during training; specifically, knowledge distillation on RGB features is used to enhance silhouette features' capacity. 
    \item We conduct a series of evaluations, i.e. rank retrieval and verification on CASIA-B, Gait3D and BRIAR datasets, showing consistent improvement in applying SoTA gait recognition backbones.
\end{itemize}
