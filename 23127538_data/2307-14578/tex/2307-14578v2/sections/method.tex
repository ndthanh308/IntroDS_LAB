

\section{Method}
\label{sec:Method}


In this work, we focus on silhouette-based gait recognition, which relies on the binary masks of the subjects in a video. Formally, we denote a video as a 4D tensor, i.e. $\mV \in \mathbb{R}^{T \times H \times W \times 3}$, where $T, H, W,$ are the frame index, height and width. For each frame $t$, the subject silhouette $\vs_t \in \{0, 1\}^{H \times W}$ is obtained from an off-the-shelf segmentation model, e.g.~\cite{wu2019detectron2}. Gait recognition takes $\mS = [\vs_t]_{t=1}^T$ as the input, 
and obtains corresponding features $\vf = F_\theta(\mS) $ from a feature extractor $F_\theta$. Triplet loss~\cite{DBLP:journals/corr/HofferA14} is used to constrain the training process of $F_\theta$ with respect to ground-truth labels $y \in \{ 1, 2, \dots, |\gY| \}$ for each video in the training sets, where $|\gY|$ is the cardinality of the label set. After $F_\theta$ is trained, gallery gait silhouettes $\mS^g$ and probe gait silhouettes $\mS^p$ are passed through $F_\theta$ to obtain the gait feature $\vf^g$ and $\vf^p$. To recognize the probe identity, a similarity metric $\gD$, such as Euclidean distance or cosine similarity, is used to get $\gD(\vf^g, \vf^p) $, where the gallery subject $g$ and the probe $p$ are decided to be the same person if they are the close enough in feature space. 




\subsection{Gait Detector}
Previous gait recognition methods~\cite{chao2019gaitset,fan2020gaitpart,lin2021gait,liang2022gaitedge,chai2022lagrange} directly use the silhouettes $\mS$ as the input, implicitly under the assumption that the video sequence captures \emph{the entire body} and \emph{continuous movement}. While these assumptions are likely to be effective for curated datasets, such as CASIA-B~\cite{yu2006framework}, 
they are often ineffective for unconstrained videos and will lead to suboptimal performance~\cite{gaitquality}. To this end, we propose a gait detector to assess if the video sequence contains gait movement with the complete body, analogous to the role played by face detection in face recognition, retaining only those frames that contain human movement for subsequent processing.



\subsubsection{Double Helical Signature}
\label{subsec:gait repre}

An ideal representation of a gait detector should be able to discriminate moving subjects from stationary ones and a partial body from a full body. 
Since the legs' motion is nearly periodic and contributes significantly to gait recognition, we use the Double Helical Signature (DHS)~\cite{ran2010applications}. DHS is a classic gait representation that captures the movement of the knee to describe gait movement as a function of time. As our input $\mS$ is normalized to the same size, we can deduce the knee height $ \mathcal{H}_{\textrm{knee}}$ to be approximately a quarter of the overall frame height. By taking a slice from the silhouette sequence, i.e., $\mS_{\textrm{knee}}(x,t) = \mS (x,\mathcal{H}_{\textrm{knee}},t), \mS_{\textrm{knee}} \in \mathbb{R}^{ W \times T }$, we obtain a DHS pattern that indicates human movement.









As shown in \cref{fig:GEIDHS}, the DHS pattern is discriminating. For the standing case, the DHS pattern shows a constant straight line since there is no movement at the knees. When the subject is walking, a periodic pattern is obtained, known as a type of ``Frieze pattern'' \cite{niyogi1994analyzing}. Given an incomplete body, DHS appears rather different as $\mathcal{H}_{\textrm{knee}}$ does not correspond to the knee position anymore; consequently, DHS becomes thicker. Therefore, DHS emerges as a compact and distinctive representation encapsulating temporal movement into a 2D image. 

\input{pictures/DHSGEI}
% Figure environment removed


\subsubsection{Light-weighted Classification}
 Capitalizing on the distinctive characteristics of DHS, we employ a simple yet effective network to extract segments from $\mS$ that contain gait information and a complete body. During the training step, we randomly select the start point and duration from the entire DHS and get $R_{\textrm{knee}}$, which is similar to the sampling strategy commonly applied in gait recognition. Each segment has the same height as DHS's. The fragment is then processed by a five-layer Convolutional Neural Network $\mathcal{M}_{\phi}$ to get the feature. To handle the varying window lengths, we employ a temporal pooling module in the form of a max-pooling layer to generate a window-length invariant embedding. Subsequently, a four-class multi-layer perceptron (MLP) is applied to obtain the prediction, using the cross entropy loss. 
\vspace{-1mm}
\begin{equation}
    \begin{split}
        P &=  \textrm{MLP}(\mathcal{P}_{Max}^{ 1\times 1\times t }(\mathcal{M}_{\phi}(R_{\textrm{knee}}))),
        \end{split}
    \label{equ:att}
\end{equation}

\noindent where $\mathcal{P}_{Max}^{ 1\times 1\times t }$ represents a temporal pooling module with size ($ 1\times 1\times t$), and $t$ is the window's width.




Especially in an unconstrained walking sequence, non-ideal fragments, including turning and standing within a short period, limit detection performance.  To precisely identify the segments that contain gait information as well as a complete boy,
we first split the entire DHS sequence into multiple windows of varying durations $R_{\textrm{knee}}^{n}$, where $n$ represents the number of windows. Each window goes through the well-trained gait classification model and gets a corresponding prediction. We only keep the complete body gait predictions and reduce the predictions by Non-Maximum Suppression (NMS). Finally, we check the reduced windows' inner distance and merge them if the distance is smaller than a predefined threshold. The ratio of the detected movement length to the entire DHS sequence serves as an indicator for determining whether the sequence should be further utilized in the gait recognition module. The process is illustrated in \cref{fig:gaitdetect}. Thus our gait detector automatically identifies the corresponding movement status of each clip. 









\subsection{GAit Recognition (GAR)}
With the help of the gait detector, we can obtain relatively robust gait sequences for gait recognition. In the recognition stage, we use GAR, which incorporates prior knowledge such as the RGB feature space and the resizing information. The architecture is shown in \cref{fig:pipeline}. 



\subsubsection{Ratio Attention}
\label{sec:ratio}

Viewpoint information has been shown to improve gait recognition performances~\cite{chai2022lagrange}; however, such information is difficult to obtain in unconstrained situations. In this work, we leverage resizing ratios to dynamically describe the changing views. The \emph{resizing ratio} is defined as the change in height from the original bounding box to the normalized silhouette height, which is usually $64$. Intuitively, the resizing ratios are similar if two videos are recorded from the same viewpoint, and these ratios naturally encode viewpoint information.




To effectively incorporate ratios into the gait feature extraction module, we apply it as an attention mechanism and fuse it into the network. Empirically, attention is an effective way to employ view point information as shown in \cref{sec:ablation}. The ratios are embedded using 1D convolution $F_{1DConv}$, followed by a sigmoid function $Sigmoid(*)$, i.e.
\begin{equation}
    \begin{split}
        r = Sigmoid(F_{1DConv}(\frac{\mS_{raw} }{\mS})),
    \end{split}
    \label{equ:att}
\end{equation}
where $\mS_{raw}$ and $\mS$ are the body bounding boxes in the original and normalized silhouette, and $r \in \mathbb{R}^{1\times 1\times T}$.  Notably, we utilize the ratio as a pattern, rather than a discrete representation~\cite{chai2022lagrange}, to characterize the viewing point, effectively and dynamically adapting to the viewpoint changes in unconstrained scenarios.






\subsubsection{Cross Modality Distillation}
Previous works~\cite{fan2020gaitpart,lin2021gait,liang2022gaitedge,xiao2022learning} have shown that silhouette-based recognition is robust to appearance changes such as different clothing and low image quality. On the flip side, segmenting RGB frames into silhouettes leads to loss of useful information for human identification, e.g. 
the rich content~\cite{ye2024biggait,guo2022multi}. While gait recognition systems do not have access to color space information at \emph{test time}, they can benefit by learning from the feature space learned by an RGB-based recognition system \emph{during training} to enhance the ability to separate identities. 
To this end, GAR introduces an auxiliary branch to extract features from RGB. We denote $F_{3DConv}^{f},F_{3DConv}^{s}$ as the first 3D convolution layers to the RGB and silhouette feature extraction backbones. Combining with ratio attention $r$, GAR first obtains weight features, i.e. 
\begin{equation}
    \begin{split}
        \mF^f_1 = r*F_{3DConv}^{f}(\mV)&,\mF^s_1 = r*F_{3DConv}^{s}(\mS).
    \end{split}
    \label{equ:network}
\end{equation}
Subsequently, we apply the rest of backbones to the processed features,
\begin{equation}
\begin{aligned}
    \mF^f_{i+1} = F^{f}_i(\mF^f_i)&,
    \mF^s_{i+1} = F^{s}_i(\mF^s_i),\\
    I^{f} = F^{f}_N(\mF^f_N)&,
    I^{s} = F^{s}_N(\mF^s_N),
\end{aligned}
\end{equation}
in which $I^{f}$ and $I^{s}$ are identification embedding, and $i \in \{1,2,...,N\}$ is the convolution block index. 



\subsection{Loss functions}
A cross-modal distillation loss is employed within GAR, which promotes the representation power of gait features based on the learned RGB feature space. 
Since both modalities have their specific advantages, directly forcing their features to be close leads to an averaged representation that does not benefit from the specificity of each modality. An additional convolutional layer $C_i$ is introduced for the $i$-th intermediate silhouette feature $\mF_{i}^{s}$, such that the transformed features are constrained to be similar to $\mF_{i}^{f}$. The loss can be described as:

\begin{equation}
  \mathcal{L}_{distill}  = \frac{1}{N}\sum_{i}(\gD(\varnothing(\mF_{i}^{f}), C_i(\mF_{i}^{s}))),
  \label{equ:dense}
\end{equation}
 where stop gradient ($\varnothing$) operation is used such that the RGB branch is not affected by the silhouette features in this process. Note that $C_i$ is only used at training time.

Triplet loss~\cite{DBLP:journals/corr/HofferA14} $\mathcal{L}^{f}_{tri}$, $\mathcal{L}^{s}_{tri}$ is applied to maximize the distance of representations from different subjects and minimize the ones from the same identity for both modalities.

Overall, the training loss is
 \begin{equation}
  \mathcal{L}_{train}  = \lambda_{f}\mathcal{L}^{f}_{tri} + \lambda_{s}\mathcal{L}^{s}_{tri} + \lambda_{distill}\mathcal{L}_{distill},
  \label{equ:loss}
\end{equation}
where $\lambda_{f,s,distill}={0.425,0.425,0.15}$ are the loss hyperparameters used during training.































