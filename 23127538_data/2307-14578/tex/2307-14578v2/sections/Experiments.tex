
\input{tables/BRIAR_4}

\section{Experiments}
\label{sec:Experiments}



\subsection{Datasets and Metrics}


In this work, we focus on applying gait recognition to unconstrained scenarios with comparatively sparse data curation, exemplified by \textbf{BRIAR}~\cite{briar}. BRIAR consists of 776 and 856 subjects used as training and test sets, respectively. In the test set, there are 493 distractors and 363 target subjects. 
Compared to other unconstrained datasets, BRIAR data includes
different walking status and clothes settings, and incomplete body shapes into consideration, making it very challenging. 
Similar to previous methods for gait recognition, we also evaluate the proposed approach on \textbf{CASIA-B}~\cite{yu2006framework}, a controlled, indoor dataset with continuous motion. CASIA-B consists of 124 subjects with three walking conditions which are normal walking (NM), walking with a bag (BG) and walking in a coat or jacket (CL). 
To further demonstrate the effectiveness of the proposed approach in the unconstrained case, we also test GADER on the \textbf{Gait3D}~\cite{zheng2022gait} dataset. Gait3D consists of 25,309 sequences recorded by 39 cameras on 4,000 subjects inside a large supermarket. 

Considering CASIA-B and Gait3D are pure gait datasets, we assume that all sequences in these datasets fully contain gait, so we only evaluate \textbf{GAR}. For the BRIAR dataset, we use the proposed gait detector to extract the segments containing gait information with a complete body and feed them to the GAR model; the remaining frames are processed by a SoTA ReID method, SemReID~\cite{huang2023selfsupervised}, the resulting system is named as \textbf{GADER}.
















\noindent\textbf{Evaluation Metric} For CASIA-B and BRIAR, \textit{verification} and \textit{rank retrieval} are used to evaluate recognition performance. 
As for Gait3D, the evaluation follows the open-set instance retrieval setting and calculates the average Rank-1, 5, and 10 accuracies, mean Average Precision (mAP), and mean Inverse Negative Penalty (mINP) over queries. 




\input{tables/CASIA_rank_CL}











\subsection{Quantitative Evaluation}
\label{subsec:quan}


\noindent\textbf{Evaluation on BRIAR}~\cite{briar} To demonstrate the superior performance of the proposed methods in the wild, we evaluate on BRIAR data with GaitSet~\cite{chao2019gaitset}, GaitPart~\cite{fan2020gaitpart},  GaitGL~\cite{lin2021gait} and GaitBase~\cite{fan2023opengait} serving as backbones. 
The Rank-1 accuracies are shown in ~\Cref{tab:BRIAR_rank_veri}. 
With the help of distilled knowledge from RGB data, our gait recognition models gain improvements compared to their vanilla version, by 9.2\%, 11.6\%, 11.0\% and 15.2\%, respectively. GAR leads to better performance than previous works, indicating that the referred discriminative feature space from distillation contributes to a better gait feature. When the gait detector as well as SemReID are integrated, GaitGL with GADER doubles the vanilla's accuracy from 17.3\% to 37.8\%,  showing that it is beneficial to apply the gait detector to eliminate segments without human movements. 
 


When it comes to \textit{verification}, the TAR (\%) results are shown 
in ~\Cref{tab:BRIAR_rank_veri}. For the case of GaitBase, compared to the original, our method improves by 10.7\%, achieving 31.6\% when FAR=$1e^{-2}$.
When the gait detector is included, we observe a big improvement. The recognition system's verification results increase from 20.9\% to 39.9\% FAR@$1e^{-2}$indicating that gait recognition will perform well if the segments that contain gait information with a complete body are used as input.

\noindent\textbf{Evaluation on CASIA-B}~\cite{yu2006framework} To show the effectiveness of the proposed method in indoor scenarios, we present the 
performance variations across four backbones by incorporating the resizing ratio and RGB feature space.
The Rank-1 accuracies are shown in ~\Cref{tab:CASIA-B_Rank}.
Compared to the original backbones, the models with prior knowledge consistently demonstrates an increase of 3.3\% for GaitSet on CL.
It is crucial to highlight that all improvements with GAR are accomplished without additional parameters involved but with an extra 1D convolution to extract ratio information in the test phase. 
\input{tables/CASIA_veri_class}


We also evaluate the \textit{verification} performance. The TAR(\%) results are shown in ~\Cref{tab:CASIA_veri}. Across the four backbones, GAR contributes to enhancements of 2.7\%, 3.4\%, 2.0\% and 0.7\%, attaining verification rates of 78.2\%, 81.2\%, 81.7\% and 80.8\%, respectively. 
It is noteworthy that although the recognition results in NM and BG approach saturation, there remains room for improvement in the verification task.







\noindent\textbf{Evaluation on Gait3D}~\cite{zheng2022gait} To evaluate our model on a public outdoor dataset, we also did cross-domain evaluation on Gait3D. The results are in ~\Cref{tab:gait3d}. We see that our proposed method achieves higher performance in all criteria. Especially, Rank-1 increases 2.0\% to 23.5\%. Since cross-domain evaluation is a challenging task, the results are lower than single-domain ones. 
The model trained on the BRIAR dataset exhibits superior performance compared to others, indicating that the model learned from the BRIAR dataset generalize well.


\noindent\textbf{Gait Detector Evaluation} We trained a gait detector using the DHSs generated from the BRIAR training set, and it reaches \underline{91.9\%}, \underline{86.1\%} and \underline{88.5\%} accuracy on BRIAR, CASIA-B and Gait3D respectively. For BRIAR, the false positive, true positive, false
negative and true negative for the gait detection module are 4.5\%, 53.1\%, 3.4\% and
38.8\%, respectively. The high accuracies show that the gait detector is robust to different domains. If the whole DHS is fed to the detector to get the prediction, the detection accuracy would be 82.7\%, 85.3\% and 87.1\%, respectively. The drop demonstrates the necessity of taking the split signature as an input. These results are under the assumption that the clips in curated datasets do not contain segments without gait information or incomplete body.

\subsection{Ablation Study}
\label{sec:ablation}

To show the impact of each part in our design, we conduct a series of ablation experiments.

\noindent\textbf{Ratio and RGB help better silhouette embedding}. In the gait recognition model, we evaluate the \emph{ratio attention} and \emph{cross modality distillation} by employing GaitGL as the backbone. 
From \Cref{tab:CASIA_ablation}, when we apply cross modality distillation, the recognition accuracy on CL reaches 85.0\% and its verification result increases by 0.5\% using the exact same model as GaitGL. As for ratio attention, it improves verification and Rank-1 accuracy under all conditions. Compared to the baseline, our proposed GAR gains a remarkable improvement on verification and Rank-1 in CL from 80.1\% and 83.3\% to 80.8\% and 85.7\% respectively. These improvements are also shown in the BRIAR dataset, which means the view angle cue from ratio and RGB's feature space help build a representative embedding. 


\input{tables/Gait3d}

\noindent\textbf{RGB modality is sensitive to appearance change}. In \Cref{tab:reid}, we evaluate the framework only using the RGB modality with GaitGL as a feature extraction model, i.e. GaitGL$_{RGB}$. We observe that it has lower performance than silhouette-based one, i.e. GaitGL, decreasing from 17.3\% to 15.6\%. Considering GaitGL is a model focusing on temporal rather than spatial information, we further experiment on CAL~\cite{Gu_2022_CVPR}, which achieves SoTA in public clothes-changing ReID datasets. But it only reaches 17.6\%, still lower than gait-based methods. So, as mentioned in~\cite{shen2022comprehensive}, even though RGB is an acceptable modality for gait recognition, it is not widely applied due to its sensitivity to appearance change. However, its feature has unique gait information that augments the silhouette feature.






\noindent\textbf{Mixing gait and non-walking adversely affect feature aggregation}.
To demonstrate the effectiveness of the gait detector, we first train with all BRIAR training set data, i.e. GaitGL w/ stand, including standing, random walking, and structure walking. From \Cref{tab:reid}, we see that when standing sequences are included in the training process, the gait recognition performance drops, which means that static sequences corrupt the gait embedding construction.
In \Cref{tab:BRIAR_rank_veri}, we show the improved performance obtained by applying GADER--GAR on segments that contain gait information with a complete body while employing SemReID~\cite{huang2023selfsupervised} on remaining data. GADER's superior performance shows that it is able to process segments with gait information with a complete body rather well.
And this improvement is achieved with little cost, a lightweight classification network, since the DHS is extracted from the input provided to gait recognition. What is more, the well-trained model is robust to domain gaps among different datasets, so it can be directly applied. 

\input{tables/gait}







\input{tables/re-id}


\subsection{Failure Cases}
\label{sec:detectfail}

\noindent\textbf{Segments without gait appear even in curated datasets.}
We recognize the presence of incomplete body segments or sequences without discernible movement in the BRIAR dataset, simulating real-life gait recognition challenges. 
But with closer examination, we found this issue also exists in curated datsets. We show some examples from GREW~\cite{zhu2021gait} and Gait3D~\cite{zheng2022gait} in \cref{fig:curated}. 
These examples emphasize the need for the gait detector even for curated datasets


\input{pictures/curatedfail}




\noindent\textbf{Analysis}
During test time, a fixed window length is used, which can result in \textit{missing} gait sequences or non-gait segments involved, as demonstrated in~\cref{fig:fail} first row, but the majority of the segments containing gait information are still caught. Furthermore, since we only have annotations for the entire DHS sequences rather than individual moments, it is challenging to train a robust model to always make correct predictions especially when the appearance is misleading, as shown in~\cref{fig:fail} second row. Despite these challenges, our gait detector can accurately identify frames that contain gait signatures.

\vspace{-.1cm}
\input{pictures/failure}
















