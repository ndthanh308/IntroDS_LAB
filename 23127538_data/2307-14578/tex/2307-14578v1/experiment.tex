
\section{Experiments}
\label{sec:Experiments}

\subsection{Datasets and Metric}

% \input{tables/Datasets}

In this work, we focus on applying gait recognition to unconstrained scenarios with minimum data curation. To this end, the \textbf{BRIAR}~\cite{briar} dataset, as shown in Figure~\ref{fig:vis}, is used extensively. BRIAR consists of 577 and 639 subjects used as training and test sets, respectively. In the test set, there are 354 distractors and 285 target subjects. For each identity, there are both indoor (\textit{controlled}) and outdoor (\textit{field}) sequences. The \textit{controlled} set is collected with 10 cameras from different viewing angles to record structure and random walking. In the \textit{field} set, the sequences are collected at varying distances and altitudes, namely close range (CR), 100m, 400m, 500m and unmanned aerial vehicle (UAV). There are also videos consisting of subjects standing all the time in the field set, as shown in Figure~\ref{fig:vis}. For each subject, two garment settings are applied, i.e. set1 and set2. The duration of each video is approximately 90 seconds. Human silhouettes are obtained using Detectron2~\cite{wu2019detectron2}. Compared to other unconstrained datasets, BRIAR takes 1. outdoor atmospheric turbulence, 2. different walking status and 3. incomplete body shapes into consideration, making it very challenging.
%Except for CR set with three cameras in the same position but different pitch angles, the others are recorded from a single viewpoint. 
%There are also videos consisting of subjects standing all the time in the field set, as shown in Figure~\ref{fig:vis}. 

Similar to previous methods for gait recognition, we also evaluate the proposed approach on \textbf{CASIA-B}~\cite{yu2006framework}, a controlled, indoor dataset with periodic motion obtained. CASIA-B consists of 124 subjects with three walking conditions which are normal walking (NM), walking with a bag (BG) and walking in a coat or jacket (CL). Each sequence consists of eleven view points ranging from $0 \degree$ to $180 \degree$ at an interval of $18 \degree$. We divide the dataset into training and test sets according to the widely applied protocol outlined in \cite{wu2016comprehensive}.
To further demonstrate the effectiveness of the proposed approach in the unconstrained case, we also test GADER on the \textbf{Gait3D}~\cite{zheng2022gait} dataset. It consists of 25,309 sequences recorded by 39 cameras on 4,000 subjects inside a large supermarket. We follow the official data split, taking the 1,000 subjects as the test set in cross-domain evaluation. In the test set, each subject has one sequence as a query and the rest serve as the gallery. 
% Specifically, we train our model on the BRIAR dataset and evaluate it on the Gait3D dataset using cross-domain evaluation, as described in GaitEdge~\cite{liang2022gaitedge}, which is a much more challenging task than single domain evaluation. The details of existing gait datasets are presented in Table \ref{tab:Sta}. 




% \noindent\textbf{BRIAR~\cite{briar}} 
% %- We perform experiments on the BRIAR dataset to assess the effectiveness of the proposed approach in unconstrained situations. The dataset 



% \noindent\textbf{CASIA-B~\cite{yu2006framework}} consists of 124 subjects with three walking conditions which are normal walking (NM), walking with a bag (BG) and walking in a coat or jacket (CL). Each sequence consists of eleven view points ranging from $0 \degree$ to $180 \degree$ at an interval of $18 \degree$. The videos are all recorded in a constrained way, meaning that the videos are recorded indoors containing single-directional gait with uniform quality. We divide the dataset into training and test sets according to the widely applied protocol outlined in \cite{wu2016comprehensive}.
%Throughout the training process, all the training set sequences are passed to the model. In the test phase, the first four sequences conditioned on NM (NM\#01-NM\#04) are referred to the gallery set and the remaining sequences are regarded as the three probe sets, i.e. NM\#05-NM\#06, BG\#01-BG\#02 and CL\#01-CL\#02.

% Meanwhile, we propose a verification protocol to justify the robustness of the gait recognition algorithm on different attires. The training process is the same as previously mentioned. For the test, the sequences in the gallery pair with the ones in three probe sets separately, so that three similarity matrices are generated corresponding to NM, BG and CL respectively. And we measure the performance using three Receiver-Operating Characteristics (ROC) curves that plot the True Accept Rate (TAR) as a function of the False Accept Rate (FAR) in three conditions. 

\input{pictures/BRIAR_vis}



% \noindent\textbf{Gait3D~\cite{zheng2022gait}} is a gait dataset collected in an outdoor scenario. It consists of 25,309 sequences recorded by 39 cameras on 4,000 subjects inside a large supermarket. We follow the official data split, taking the 1,000 subjects as the test set in cross-domain evaluation. In the test set, each subject has one sequence as a query and the rest serve as the gallery. 
%The evaluation protocol follows the open-set instance retrieval setting and calculates the average Rank-1 and Rank-5 accuracy, mean Average Precision (mAP), and mean Inverse Negative Penalty (mINP) over all queries.


% In the training phase, videos including the walking segments in the training set are fed to the model. When it comes to testing phase, `set1' videos containing structure walking in the controlled set serve as a gallery and `set2' videos in the field set are a probe. We apply two tasks, \textit{verification} and \textit{rank retrieval}, to evaluate unconstrained gait recognition. For verification, the pairs are generated between the identities in the gallery and the sequences in the probe. And rank retrieval communicates the likelihood that a given probe sequence with an identity in the gallery and returns a match in the Top 1 returned results.


\noindent\textbf{Evaluation Metric} For CASIA-B and BRIAR, \textit{verification} and \textit{rank retrieval} are used to evaluate gait recognition. For verification, the gallery sequences are paired with probe sequences. We measure the performance using Receiver-Operating Characteristics (ROC) curves that plot the True Accept Rate (TAR) as a function of the False Accept Rate (FAR). As for Gait3D, the evaluation protocol follows the open-set instance retrieval setting and calculates the average Rank-1 and 5 accuracies, mean Average Precision (mAP), and mean Inverse Negative Penalty (mINP) over all queries.
%Rank retrieval communicates the likelihood that a given probe sequence with an identity in the gallery and returns a match in the Top 1 returned results. 
% \input{tables/combined}
% \input{tables/CASIA_rank}
% \input{tables/CASIA_overall}
\input{tables/CASIA_rank_CL}

\subsection{Implementation Details}

All experiments are implemented in Pytorch~\cite{paszke2019pytorch} with four NVIDIA A5000 GPUs. We follow~\cite{chao2019gaitset} to obtain normalized silhouettes and RGB body chips at the resolution of $64\times64$ from the original segmentation masks and frames for CASIA-B and BRIAR. The resizing ratio is recorded during the normalization process. For CASIA-B, we use GaitGL~\cite{lin2021gait} as the backbone for RGB and silhouette feature extraction. 
For the BRIAR dataset, we use a larger GaitGL architecture designed for OU-MVLP~\cite{takemura2018multi}, considering the number of identities and scene complexity. In the training phase, the batch size for both datasets is (8,8), i.e. eight subjects with eight videos per subject for each batch. Thirty continuous frames are randomly sampled from a video. %For testing, we use the whole sequence as input for CASIA-B, and clips of selected 5 to 15 seconds for BRIAR.

Our gait detector is trained using DHS generated from the BRIAR training set in a supervised manner using annotations provided by BRIAR. To ensure robustness to varying time duration, we randomly select input lengths ranging from 30 to 100 for each training iteration. The detector is evaluated on DHS from the testing sets of three datasets. Specifically, we select window sizes of 33, 50, and 80. The gait detection performance is evaluated using average accuracy, where correctness means 50\% or more of the sequence has predictions matching with the ground truth. CASIA-B and Gait3D are pure gait datasets, and we suppose all sequences in these datasets as complete body gait.

% It is convenient to evaluate GADER as a whole for most existing backbones, just with an extra step to generate the DHSs as we mentioned above. And gait detector serves as a filter to keep qualified gait sequence for further recognition.



%\input{tables/CASIA_overall}





\subsection{Quantitative Evaluation}
\label{subsec:quan}

\noindent\textbf{Evaluation on CASIA-B}~\cite{yu2006framework} We compare our proposed gait recognition method with current arts, including GaitPart~\cite{fan2020gaitpart}, GaitGL~\cite{lin2021gait}, 3DLocal\cite{huang20213d}, CSTL~\cite{huang2021context}, Lagrange~\cite{chai2022lagrange} on CASIA-B. 
% To compare with DME~\cite{guo2022multi}, we also concatenate RGB and silhouette embbedings at the end. 
The Rank-1 accuracy is in Table~\ref{tab:CASIA-B_Rank}. 
% Our proposed approach optimizing silhouette feature with the help of RGB, and improves the performance regardless RGB is available during test.

\input{tables/CASIA_veri_class}
\input{tables/BRAIR_rank_veri}


Compared to other methods, our model achieves similar performance on NM and BG compared to Lagrange~\cite{chai2022lagrange} and better performance on CL. Overall, our method outperforms Lagrange~\cite{chai2022lagrange}, CSTL~\cite{huang2021context}, and 3DLocal~\cite{huang20213d} by 0.2\%, 0.7\% and 0.5\%, respectively. We note that GAR improves overall performance from 91.8\% to 92.6\% compared to the original GaitGL with only an extra 1D convolution to extract ratio information in the test phase. GADER even further increases the accuracy to 92.9\% with the help of the gait detector.
% \input{tables/BRIAR_rank_no200}
% \input{tables/BRIAR_veri}



We also evaluate the \textit{verification} performance. The TAR(\%) results are shown in Table~\ref{tab:CASIA_veri}. Our proposed method outperforms GaitGL and GaitPart by 0.72\% and 3.03\% overall, attaining 80.83\%. And our method also achieves the best performance in NM, BG and CL with 91.48\%, 83.07\% and 67.94\%. It is noticeable that although the recognition results in NM and BG are likely saturated, there is still room for improvement on the verification task.
% When silhouette and RGB both are used in test period, the proposed method is over DME~\cite{guo2022multi} by 0.6\%, 0.2\% and 0.3\%, reaching 99.8\%, 99.2\% and 93.1\% in NM, BG, and CL conditions respectively. This shows that a superior silhouette embedding builds a more discriminative aggregated representation.




\noindent\textbf{Evaluation on BRIAR}~\cite{briar} To demonstrate the superior performance of the proposed methods in the wild, we further evaluate on BRIAR with GaitPart~\cite{fan2020gaitpart} and GaitGL~\cite{lin2021gait}. 
% and DME~\cite{guo2022multi}. 
The Rank-1 accuracy is shown in Table~\ref{tab:BRIAR_rank_veri}. 
% To evaluate with DME, we also concatenate RGB and silhouette embeddings, and only keep the probes containing gait, i.e. structure and random walking. 
Our gait recognition model achieves the highest overall recognition performance, reaching 50.37\%, and is 20.6\% and 30.82\% higher than GaitGL and GaitPart. It is also higher than GAR by 17.67\%, just showing that it is beneficial to apply the gait detector to eliminate sequences without human movements. GAR still leads to better performance than previous arts, indicating that the referred discriminative feature space from RGB and ratio information contribute to a better silhouette feature.  
 

% When RGB embedding is also available during test, we observe that the introduced method outperforms DME by 0.16\%.
When it comes to \textit{verification}, the TAR (\%) results are shown Table \ref{tab:BRIAR_rank_veri}. Compared to GaitGL, our method increases by 26.47\%, achieving 61.68\% when FAR=$1e^{-2}$.
% \input{tables/BRIAR_detect_no200}
From Table~\ref{tab:BRIAR_rank_veri}, we observe a big improvement when the gait detector is included. The recognition system's verification results increase from 35.28\% to over 60\% when FAR=$1e^{-2}$. This indicates that gait recognition will perform reasonably well if only the qualified gait sequences are used as input.


\noindent\textbf{Evaluation on Gait3D}~\cite{zheng2022gait} To evaluate our model on a public outdoor dataset, we also did cross-domain evaluation on Gait3D. The results are in Table~\ref{tab:gait3d}. We see that our proposed method achieves higher performance in all criteria. Especially, Rank-1 increases 3.5\% to 25.02\%. Since cross-domain evaluation is a challenging task, the results are lower than single-domain ones. It is worth noting that, we also adopt GaitGL~\cite{lin2021gait} for the cross-domain evaluation using CASIA-B, OUMVLP, GREW and BRIAR for train and testing on Gait3D. The model trained on the BRIAR dataset exhibited superior performance compared to the others, indicating that the complexity of BRIAR dataset confers greater generalization ability to the trained model.

\input{tables/Gait3d}

\noindent\textbf{Gait Detector Evaluation} We trained a gait detector using DHSs generated using BRIAR training set, and it reaches \underline{91.9\%}, \underline{86.1\%} and \underline{88.5\%} accuracy on BRIAR, CASIA-B and Gait3D respectively. The high accuracy shows that the gait detector is robust to different domains. Visualizations are included in the \textbf{supplementary material} section. From Table~\ref{tab:CASIA-B_Rank}, \ref{tab:CASIA_veri}, \ref{tab:BRIAR_rank_veri} and \ref{tab:gait3d},  the recognition performance increases. with the introduction of the detector. 

\subsection{Ablation Study}

To show the impact of each part in our design, we conduct a series of experiments.

\noindent\textbf{RGB modality is sensitive to appearance change}. In Table~\ref{tab:BRIAR_train}, we evaluate the framework only using RGB modality with GaitGL as a feature extraction model, i.e. GaitGL$_{RGB}$. We observe that it has lower performance than silhouette-based one, i.e. GaitGL, decreasing from 29.77\% to 15.69\% even only testing on annotated walking sequences set, i.e. Pre-defined, due to its sensitivity to appearance change, but its feature has unique attributes that augment the silhouette feature with a 3.97\% improvement.

\noindent\textbf{Ratio and RGB help better silhouette embedding}. In the gait recognition model, we evaluate the \emph{ratio attention} and \emph{cross modality distillation}. 
%To quantize the contribution from cross modality distillation, we add it over two parts of the representation model i.e. front (feature) and back (embedding), divided by multi-layer perceptions. 
From Table~\ref{tab:CASIA_ablation}, when we apply cross modality distillation, the recognition accuracy on CL reaches 85\% and its verification result increases by 0.52\%. As for ratio attention, it improves verification and Rank-1 accuracy under all conditions. Compared to the baseline, our proposed GAR gains a remarkable improvement on verification and Rank-1 in CL from 80.11\% and 83.3\% to 80.83\% and 85.7\% respectively, which means the view angle cue from ratio and RGB's feature space help build a representative embedding.


\noindent\textbf{Mixing gait and non-walking will ruin the feature aggregation}.
To demonstrate the necessity of gait detector, we first train with all BRIAR training set data, i.e. GaitGL w/ stand, including standing, random walking and structure walking. From Table~\ref{tab:BRIAR_train}. We see that when standing sequences are involved in the training process, the performance on gait drops, which means that static sequences disturb gait embedding construction. And it is also computationally consuming to apply stand sequence in gait recognition, since there is little temporal information, a 2D model is enough to use. This observation inspires us to develop a detector to filter out static and low-quality silhouettes. 

\input{tables/trainwithstand}
\input{tables/gait}
\noindent\textbf{Gait detector purifies gait sequences and improves gait performance}.
 When the gait detector is applied, both models achieve higher recognition results since the detector keeps the silhouette sequences with complete-body and periodic moving patterns. Compared to the Pre-defined gait set, the Detected set further removes sequences with an incomplete body. From Table~\ref{tab:CASIA-B_Rank}, \ref{tab:CASIA_veri}, \ref{tab:BRIAR_rank_veri} and \ref{tab:gait3d}, we see the potential of detector. Using the same model, GAR, when we exploit a gait detector, almost all performances improve. And this improvement is achieved with little cost, a light-weighted classification network since the DHS is generated by extracting from the input of gait recognition. The well-trained model is robust to domain gaps among different datasets, so it can be directly applied.




