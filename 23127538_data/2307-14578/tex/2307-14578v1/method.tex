

\section{Method}
\label{sec:Method}


% Suppose a video containing gait sequences has $T$ frames $\vv_i$ with $H$ height and $W$ width, $\mV = [\vv_1, \vv_2, \dots, \vv_T] \in \gV := \mathbb{R}^{H \times W \times T \times 3}$. Each video is correlated with a subject ID, which is labeled as $y \in \gY$, where $y \in \{ 1, 2, \dots, |\gY| \}$ and $\gY$ is the set of identities in a dataset. For each video, we use silhouette sequence $\mS = [\vs_1, \vs_2, \dots, \vs_T] \in \gS := \{0, 1\}^{H \times W \times T \times 1}$ that is made up of a series of binary masks generated by segmentation to represent gait. To extract the feature from the gait sequence, $F_\theta$, a learnable parameterized model, is trained to map $\mS$ to a feature vector $\mF$. The gait recognition's main idea is to get a $F_\theta$, which maps the any two silhouette sequences, $\mS_1$ and $\mS_2$, from same subject to a certain feature space in which features are close i.e. $\gD(F_\theta(\mS_1), F_\theta(\mS_2))$, and $\gD$ is a metric to measure the distance, such as Euclidean or similarity. When RGB frames are available, the background is moved out to avoid overfitting i.e. $\mV = \mV_{\textrm{raw}} \odot \mS$, where $\mV_{\textrm{raw}}$ is the original raw video and $\odot$ is the Hadamard product.

In this work, we focus on silhouette-based gait recognition, which relies on the binary masks of the subjects in a video. Formally, we denote a video as a 4D tensor, i.e. $\mV \in \mathbb{R}^{H \times W \times T \times 3}$, where $H, W, T$ are the height, width, and frame index. For each frame $t$, the subject silhouette $\vs_t \in \{0, 1\}^{H \times W}$ is obtained from an off-the-shelf segmentation model, e.g.~\cite{wu2019detectron2}. Gait recognition takes $\mS = [\vs_t]_{t=1}^T$ as the input, 
and obtains corresponding features $\vf = F_\theta(\mS) $ from a feature extractor $F_\theta$. Triplet loss~\cite{DBLP:journals/corr/HofferA14} is used to constrain the training process of $F_\theta$ with respect to ground-truth labels $y \in \{ 1, 2, \dots, |\gY| \}$ for each video in the training sets, where $|\gY|$ is the cardinality of the label set. After $F_\theta$ is trained, gallery gait silhouettes $\mS^g$ and probe gait silhouettes $\mS^p$ are passed into $F_\theta$ to obtain the gait feature $\vf^g$ and $\vf^p$. To recognize the probe identity, a similarity metric $\gD$, such as Euclidean distance or cosine similarity, is used to find $\gD(\vf^g, \vf^p) $, where the gallery subject $g$ and the probe $p$ will be regarded as the same person if they are the most similar in feature space. 


In the following sections, we introduce GAit DEtection and Recognition in the Wild (GADER), designed for gait recognition in the wild. GADER has two parts: a gait detector model, which uses 2D gait representations to detect human movements in a video, and a gait recognition model, which incorporates resizing information and is guided by a teacher feature space from an RGB-based recognition model during training.



% utilizes unique perspective from RGB to get a robust silhouette representation.
% to feature extraction model $F_\theta$
\vspace{-0.2cm}
\subsection{Gait Detector}
Previous gait recognition methods~\cite{chao2019gaitset,fan2020gaitpart,lin2021gait,liang2022gaitedge,chai2022lagrange} directly use silhouettes $\mS$ as the input, under the assumption that the video sequence captures \emph{the entire body} and \emph{constant movement}. While these assumptions are for curated datasets, such as CASIA-B~\cite{yu2006framework}, OU-MVLP~\cite{takemura2018multi}, and GREW~\cite{zhu2021gait}, they are often untrue for unconstrained videos and will lead to suboptimal learning if left untreated~\cite{gaitquality}. To this end, we propose a gait detector to assess if the video sequence contains gait movement with the complete body, similar to the role of face detection in face recognition. %Based on our experiments in Sec~\ref{subsec:quan}, if we directly use silhouettes sequence as previous methods do, the performance will drop a lot. From this observation, we propose a gait detector which efficiently utilizes simple 2D gait representation to assess the silhouette whether contain gait movement with complete body.

%This manner is reasonable since currently, most gait recognition datasets, such as CASIA-B~\cite{yu2006framework}, OUMVLP~\cite{takemura2018multi}, are collected indoor with ideal and constant quality. So sequences contain complete body and constant-directional periodic movement, which is the assumption of gait recognition. But when we want to apply gait recognition in the wild, the scenario is much more complected due to occlusion, multiple movement status, i.e. mix of any direction movement and standing. \cite{gaitquality} also shows silhouette quality will effect the model's performance even for these ideal datasets. And based on our experiments in Sec~\ref{subsec:quan}, if we directly use silhouettes sequence as previous methods do, the performance will drop a lot. From this observation, we propose a gait detector which efficiently utilizes simple 2D gait representation to assess the silhouette whether contain gait movement with complete body.

\vspace{-0.2cm}
\subsubsection{Gait Representation}
\label{subsec:gait repre}


To make the gait detector compatible with silhouette-based methods, we extract a gait representation based on normalized silhouette $\mS$. An ideal gait representation should be able to discriminate moving subjects from stationary ones and partial body from full body. 
% \input{pictures/GEI_example}
% \noindent Gait Energy Image (GEI)~\cite{han2005individual}: As shown in Figure~\ref{fig:GEIDHS}, GEI distinguishes four common cases in real-life dataset. GEI image is generated using the average of a silhouette sequence in time. GEI would be blurred when gait is present, and it is clear otherwise. Without the full body, the shape of the body is greatly inflated.
% \begin{equation}
%     GEI(x,y) = \frac{1}{T} \sum_{t=1}^{T} \vs_t,
% \end{equation}
Since the legs of the body have periodic movement and contribute significantly to gait recognition, we use the Double Helical Signature (DHS)~\cite{ran2010applications}. DHS is a classic gait representation that captures the movement of the knee to describe gait movement as a function of time. As our input $\mS$ is normalized to the same size, we can deduce the knee height $ \mathcal{H}_{\textrm{knee}}$ to be approximately a quarter of the overall height. By taking a slice from the silhouette sequence, i.e., $\mS_{\textrm{knee}}(x,t) = \mS (x,\mathcal{H}_{\textrm{knee}},t), \mS_{\textrm{knee}} \in \mathbb{R}^{ W \times T }$, we can obtain a double helical structure that indicates human movement. 




% So it is a 2D image with shape $W\times T$ at knees' height extracted from an original silhouette sequence in size $H\times W \times T$. 


% \begin{equation}
%     \vs_{\textrm{knee}}(x,t) = \vs_t (x,\mathcal{H}_{\textrm{knee}}(t))
% \end{equation}

% \input{pictures/DHS_exmaple}

As shown in Figure~\ref{fig:GEIDHS}, the DHS pattern is rather discriminating. For the standing case, the DHS pattern shows a constant straight line since there is no movement at knees. When the subject is walking, a periodic pattern is obtained, known as a type of ``Frieze pattern'' \cite{niyogi1994analyzing}. Given an incomplete body, DHS appears rather different as $\mathcal{H}_{\textrm{knee}}$ does not correspond to the knee position anymore; consequently, DHS becomes thicker.
\input{pictures/DHSGEI}



\vspace{-0.3cm}
\subsubsection{Light-weighted Classification}

Unconstrained human motion naturally involves a variety of movements, including turning, standing, and walking within a short period of time. This can result in DHS sequences that contain multiple walking fragments. During the training step, we randomly select the start point and duration from the whole DHS and get $R_{\textrm{knee}}$. Each segment has the same height as DHS's. After that, the fragment is processed by a five-layer Convolutional Neural Network $\mathcal{M}_{\phi}$ to get the feature. To handle the varying window lengths, we introduce a temporal pooling module in the form of a max-pooling layer to generate a window-length invariant embedding. Then, we use a four-class multi-layer perceptron (MLP) to obtain the prediction, and the cross entropy loss is applied between the prediction($H$) and ground-truth to get a gait classification model.

\begin{equation}
    \begin{split}
        H &=  \textrm{MLP}(\mathcal{P}_{Max}^{ 1\times 1\times t }(\mathcal{M}_{\phi}(R_{\textrm{knee}}))),
        \end{split}
    \label{equ:att}
\end{equation}

\noindent where $\mathcal{P}_{Max}^{ 1\times 1\times t }$ represents temporal pooling module with size ($ 1\times 1\times t$), and $t$ is the window's width.


To identify and localize these fragments, in the inference time, we first split the entire DHS sequence into multiple windows of varying durations $R_{\textrm{knee}}^{n}$, where $n$ represents the number of windows. Each window goes through the well-trained gait classification model and gets a corresponding prediction. We only keep the complete body gait predictions and reduce the overlapping by Non-Maximum Suppression (NMS). Finally, we check the reduced windows' inner distance and concatenate them if the distance is smaller than a predefined threshold. And the ratio of the detected movement length to the entire DHS sequence serves as an indicator for determining whether the sequence should be further utilized in the gait recognition module. The process is illustrated in Figure~\ref{fig:gaitdetect}. Thus our gait detector automatically identifies the corresponding movement status of each clip.

% Figure environment removed



% generate multiple windows with various time duration to serve as anchors scanning the DHS. Each window's height is the same as the DHS, and they are classified by a well-trained model. Inspired by Non-Maximum Suppression (NMS)~\cite{neubeck2006efficient}, a widely used method in object detection, we apply 1D-NMS to reduce the overlapping windows having the same prediction. Finally, we check the reduced windows' inner distance and concatenate them if the distance is smaller than a predefined threshold. The process is illustrated in Figure~\ref{fig:gaitdetect}. Thus our gait detector automatically identifies the corresponding movement status of each clip rather than the entire sequence.


% % From Section~\ref{subsec:gait repre}, the appearances of DHS are distinguishable for the four conditions. Therefore, 
% We build a light-weight Convolutional Neural Network $\mathcal{M}_{\phi}$ to classify the video sequences. 
% % \begin{equation}
% %     \mX_{GEI} = \mathcal{F}_{CNNs}(GEI)
% % \end{equation}
% As the length of DHS, which is the sequence's duration, is unpredictable, we take a temporal pooling module, a max-pooling layer, to integrate all temporal information (width) of a DHS. Therefore, the feature size is invariant to DHS width.
% \begin{equation}
%     \mX_{DHS} = \mathcal{P}_{Max}^{ 1\times 1\times T }(\mathcal{M}_{\phi}(\mS_{\textrm{knee}})),
% \end{equation}
% where $\mathcal{P}_{Max}^{ 1\times 1\times T }$ represents temporal pooling module with size ($ 1\times 1\times T$), and $T$ is the DHS width.


\subsection{GAit Recognition (GAR)}
With the help of the gait detector, we can obtain relatively clean gait sequences for gait recognition. In the recognition stage, we propose GAR, which incorporates prior knowledge such as the RGB feature space and the resizing information. The architecture is shown in Figure~\ref{fig:pipeline}. 



\vspace{-0.3cm}
\subsubsection{Ratio Attention}
\label{sec:ratio}

The \emph{resizing ratio} is defined as the change in size from the original bounding box to the normalized silhouette shape, which is usually $64 \times 64\times T$. Intuitively, the resizing ratios are similar if two videos are recorded from the same viewpoint. As such, these ratios naturally encode viewpoint information, as shown in Figure~\ref{fig:ratio}. Viewpoint information has been shown to improve gait recognition performances~\cite{chai2022lagrange}; however, such information is difficult to obtain in unconstrained situations, especially when the subject's movement is not monotonic. In this work, we leverage resizing ratios to represent changing views.
% Previous methods have so far discarded the resizing information during the preparation of the input data.


\input{pictures/figure4}

To effectively incorporate ratios into the gait feature extraction module, we apply it as an attention mechanism and fuse it into the network. The ratios are embedded using 1D convolution $F_{1DConv}$, followed by a sigmoid function $Sigmoid(*)$, i.e.: 
% The output is fused in to the feature $\mF$ from the first layer of CNNs, as shown in Equation~(\ref{equ:att}),
\begin{equation}
    \begin{split}
        % \vr &= \frac{\mS_{raw} }{\mS}, \\
        % \va_{ratio} &= Sigmoid(F_{1DConv}(\vr)), \\
        r = Sigmoid(F_{1DConv}(\frac{\mS_{raw} }{\mS})),
    \end{split}
    \label{equ:att}
\end{equation}
where $\mS_{raw}$ and $\mS$ are the body bounding boxes in the original and standardized silhouette, and $r \in 	\mathbb{R}^{1\times 1\times T}$. 

% The output feature $\mF$ fused with ratio attention keeps the same size and goes through additional feature extraction layers.




\subsubsection{Cross Modality Distillation}
Previous works~\cite{fan2020gaitpart,chao2019gaitset,lin2021gait,liang2022gaitedge,xiao2022learning} have shown that silhouette-based recognition is robust to appearance changes such as different clothing and low image quality. On the flip side, segmenting RGB frames into silhouettes leads to a loss in useful information, e.g. 
% when clothing does not change and 
the rich texture information. While gait recognition systems do not have access to color space information at \emph{test time}, they can be benefited by learning from the feature space learned by an RGB-based recognition system \emph{during training} to better separate identities. 
To this end, GAR introduces an auxiliary recognition branch to extract features from RGB images. We denote $F_{3DConv}^{f},F_{3DConv}^{s}$ as the first 3D convolution layers to the RGB and silhouette feature extraction backbones. Combining with ratio attention $r$, GAR first obtains weight features, i.e. 
\begin{equation}
    \begin{split}
        \mF^f_1 = r*F_{3DConv}^{f}(\mV)&,\mF^s_1 = r*F_{3DConv}^{s}(\mS),
    \end{split}
    \label{equ:network}
\end{equation}

After that, we apply the rest of backbones to the processed features,
\begin{equation}
\begin{aligned}
    \mF^f_{i+1} = F^{f}_i(\mF^f_i)&,
    \mF^s_{i+1} = F^{s}_i(\mF^s_i),\\
    I^{f} = F^{f}_N(\mF^f_N)&,
    I^{s} = F^{s}_N(\mF^s_N),
\end{aligned}
\end{equation}

\noindent in which $I^{f}$ and $I^{s}$ are identification embedding, and $i \in \{1,2,...,N\}$ is the convolution block index. Triplet loss~\cite{DBLP:journals/corr/HofferA14} is applied to enlarge the similarity of representations from different subjects and minimize the ones from the same identity for both modalities.

\begin{equation}
\begin{split}
    \mathcal{L}^{f}_{tri} = [\Vert I^{f} - I^{f}_- \Vert_2 -\Vert I^{f} - I^{f}_+\Vert_2 + m]_{+},\\
    \mathcal{L}^{s}_{tri} = [\Vert I^{s} - I^{s}_- \Vert_2 - \Vert I^{s} - I^{s}_+ \Vert_2 + m]_{+},\\
\end{split}
\end{equation}

\noindent where the superscript $+$ and $-$ in the identification embedding represent the same and different subject to $I^{f}$ respectively. $m$ is the margin of the triplet loss, and operation $[*]_+$ describes $\max(*, 0)$.



% There are two components in GAR: (1) Ratio Attention, which makes use of the normalization information that intrinsically encodes viewpoint into consideration, 
% % (2) Bi-Modal Representation, which extracts RGB and silhouette representations in parallel, 
% (2) Cross Modality Distillation, which enhances silhouette embedding by learning from the RGB feature space.





% \subsubsection{Bi-Modal Representation}
% % \todo{Need change to different to FG one}

% Previous works~\cite{fan2020gaitpart,chao2019gaitset,lin2021gait,liang2022gaitedge,xiao2022learning} have shown that silhouette-based recognition is robust to appearance changes such as different clothing and low image quality. On the flip side, segmenting RGB frames into silhouettes often leads to a loss in potentially useful information, e.g. 
% % when clothing does not change and 
% the rich texture information from RGB leads to discriminative features. As both modalities have their advantages, we propose a solution to leverage the representation from RGB images as a training-time guidance for gait-based recognition, when RGB images are available. RGB images are not used at inference time and only serve to augment the feature capacity of the gait recognition model. This feature reduces the privacy concerns to some degree

% As shown in Figure~\ref{fig:pipeline}, our proposed ``GAR'' method introduces an auxiliary recognition branch to extract features from RGB images. GAR includes two branches $F_{f}$ and $F_{s}$,


% But this robustness gained with the cost of information limitation for model training, since RGB videos contain rich information. It shows from~\cite{guo2022multi} that rich information from RGB greatly improves performance regardless of indoor or outdoor cases. Besides, due to privacy concerns, we only apply silhouette during the test phase, leading us to propose a knowledge distillation model to help the silhouette branch learn from the RGB branch.

% Referring the architecture from~\cite{guo2022multi}, Dual Modality Representation (DMR) consists of twin CNN architecture models for extracting features and generating representations from RGB and silhouette modalities

% \begin{equation}
%     I_{f} = F_{f}(\mV),\\
%     I_{s} = F_{s}(\mS),
% \end{equation}
% in which $I_{f}$ and $I_{s}$ are identification embedding. Triplet loss~\cite{DBLP:journals/corr/HofferA14} is applied to enlarge the similarity of representations from different subjects and minimize the ones from the same identity for both modalities.

% \begin{equation}
% \begin{split}
%     \mathcal{L}^{tri}_{f} = [\gD(l_{f}(i),l_{f}(k))-\gD(l_{f}(i),l_{f}(j))+m]_{+},\\
%     \mathcal{L}^{tri}_{s} = [\gD(l_{s}(i),l_{s}(k))-\gD(l_{s}(i),l_{s}(j))+m]_{+},\\
% \end{split}
% \end{equation}

% \begin{equation}
% \begin{split}
%     \mathcal{L}^{tri}_{f} = [\Vert I_{f} - I_{f}^- \Vert_2 -\Vert I_{f} - I_{f}^+\Vert_2 + m]_{+},\\
%     \mathcal{L}^{tri}_{s} = [\Vert I_{s} - I_{s}^- \Vert_2 - \Vert I_{s} - I_{s}^+ \Vert_2 + m]_{+},\\
% \end{split}
% \end{equation}

% where $i$ and $j$ are videos from the same subject, $k$ is the video from another subject. $\gD(*)$ is the Euclidean distance measure, $m$ is the margin of the triplet loss, and operation $[*]_+$ describes $\max(*, 0)$.

% \noindent where the superscript $+$ and $-$ in the identification embedding represent the same and different subject to $I_{f}$ respectively. $m$ is the margin of the triplet loss, and operation $[*]_+$ describes $\max(*, 0)$.

% At test time, due to the privacy concern, only the silhouette representation, i.e. $l_i$, is applied.

A cross-modal distillation loss is introduced within GAR, which promotes the representation power of gait features based on the learned RGB feature space. While one possible design is to directly constrain the silhouette/RGB features to be similar, we find this approach to be suboptimal. Since both modalities have their specific discriminative advantages, forcing their features to be similar leads to an averaged representation that does not benefit from the specificity from each modality. As we focus on obtaining expressive features from silhouettes, an additional convolutional layer $C_i$ is introduced for the $i$-th intermediate silhouette feature $\mF_{i}^{s}$, such that the transformed features are constrained to be similar to $\mF_{i}^{f}$. This cross-modality distillation loss can be described as:

\begin{equation}
  \mathcal{L}_{distill}  = \sum_{i}(\gD(\varnothing(\mF_{i}^{f}), C_i(\mF_{i}^{s}))),
  \label{equ:dense}
\end{equation}
 where stop gradient ($\varnothing$) operation is used such that the RGB branch is not affected by the silhouette features in this process. Note that $C_i$ is only used at training time. Overall, the training loss is
 
 \begin{equation}
  \mathcal{L}_{train}  = \lambda_{f}\mathcal{L}^{f}_{tri} + \lambda_{s}\mathcal{L}^{s}_{tri} + \lambda_{distill}\mathcal{L}_{distill},
  \label{equ:loss}
\end{equation}
where $\lambda_{f,s,distill},={0.425,0.425,0.15}$ are the loss hyperparameters used during training.
% RGB2SIL to distill knowledge from the RGB modality to the silhouette modality. We maximize feature similarities between the two modalities. 
% More specifically, dense alignments and joint learning are used. 


% \noindent\textbf{Dense Alignment} reduces the similarity in feature space. Since the feature extraction models are similar, the features for the same sequence from each block are supposed to be similar. To avoid the silhouette misleading RGB, we use stop gradient ($\varnothing$) operation to make RGB model inaffected by silhouette student model in this process, as shown in Equation~(\ref{equ:dense})
% \begin{equation}
%   \mathcal{L}_{DS}  = \min[\sum_{i=1}^{B}(\gD(\varnothing(\mF_{i}^{RGB}), \mF_{i}^{SIL}))],
%   \label{equ:dense}
% \end{equation}
% where $\mF_{i}^{RGB}$ and $\mF_{i}^{SIL}$ are the output features from $i$-th module in two braches, and B is the total module number.

% \noindent\textbf{Joint Learning} approximates two modalities' global representation, i.e. embeddings, since the representation is directly used during test phase, which is crucial to the performance. It constrains the modalities' ability if we directly make two modalities' representations close. Therefore, we

% \todo{The formulation and explanation}






