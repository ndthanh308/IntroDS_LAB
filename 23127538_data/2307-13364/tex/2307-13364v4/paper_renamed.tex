\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{natbib}
\usepackage{array}
\usepackage{algorithmic}
\usepackage[plain]{algorithm}
\newcommand{\alglinelabel}{%
	\addtocounter{ALC@line}{-1}% Reduce line counter by 1
	\refstepcounter{ALC@line}% Increment line counter with reference capability
	\label% Regular \label
}
\newcommand*{\doublerule}{\hrule width \hsize height 0.5pt \kern 0.5mm \hrule width \hsize height 0.5pt}
\newcommand*{\singlerule}{\hrule width \hsize height 0.5pt}
\usepackage[colorlinks = true,
linkcolor = red,
urlcolor  = blue,
filecolor={red},
citecolor = blue,
anchorcolor = blue]{hyperref}
%\usepackage{url} % not crucial - just used below for the URL 
\usepackage{xurl}
%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{1}
\usepackage{mathtools}
\usepackage{etoolbox}
% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-1in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.7in}%
\addtolength{\topmargin}{-1in}%

\usepackage[margin=1in]{geometry}
\usepackage{longtable,booktabs,array}
\usepackage{xcolor}
\allowdisplaybreaks
\DeclareMathOperator*{\argmin}{arg\,min}%\usepackage[latin 1]{inputenc}
\usepackage{caption}\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}\usepackage{graphicx,latexsym,amssymb,amsmath}
\usepackage{subcaption}
\usepackage{bbm, bm}
\def\pp{\vskip 10pt}
\def\ppn{\vskip 10pt \noindent }
\def\d{{\mathrm{d}}}
\def\R{{\mathbb{R}}}
\def\Z{{\mathbb{Z}}}
\def\C{{\mathbb{C}}}
\def\N{{\mathbb{T}}}
\def\T{{\mathbb{T}}}
\def\P{{\mathbb{P}}}
\def\E{{\mathbb{E}}}
\newcommand{\vertiii}[1]{{\left\vert\kern-0.4ex\left\vert\kern-0.4ex\left\vert #1 
		\right\vert\kern-0.4ex\right\vert\kern-0.4ex\right\vert}}
\usepackage{newfloat}
%\DeclareFloatingEnvironment[
%fileext=los,
%listname={List of Schemes},
%name=Algorithm,
%placement=tbhp,
%]{algorithm}
\newtheorem{Lemma}{Lemma} %numbering by section
\newtheorem{Assumption}{Assumption}
\newtheorem{Theorem}{Theorem}%numbering by section
\newtheorem{Proposition}{Proposition}
\newtheorem{Remark}{Remark}[section]
\newtheorem{Corollary}{Corollary}

\usepackage{xr}
\externaldocument[OA-]{supp}
	
	\def\spacingset#1{\renewcommand{\baselinestretch}%
		{#1}\small\normalsize} \spacingset{1}
	
% PACKAGES TO DELETE AFTER REVISION
\usepackage[normalem]{ulem}

\begin{document}
	
	%\bibliographystyle{natbib}

	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\if1\blind
	{
		\title{\bf Testing for sparse idiosyncratic components in factor-augmented regression models}
		\author{Jad Beyhum\hspace{.2cm}\\
			Department of Economics, KU Leuven, Belgium\\
			and \\
			Jonas Striaukas \\
			Department of Finance, Copenhagen Business School, Denmark}
		\maketitle
	} \fi
	
	\if0\blind
	{
		\bigskip
		\bigskip
		\bigskip
		\begin{center}
			{\LARGE\bf Testing for sparse idiosyncratic components in factor-augmented regression models}
		\end{center}
		\medskip
	} \fi
	
	\bigskip
	\begin{abstract}
		We propose a novel bootstrap test of a dense model, namely factor regression, against a sparse plus dense alternative augmenting model with sparse idiosyncratic components. The asymptotic properties of the test are established under time series dependence and polynomial tails. We outline a data-driven rule to select the tuning parameter and prove its theoretical validity. In simulation experiments, our procedure exhibits high power against sparse alternatives and low power against dense deviations from the null. Moreover, we apply our test to various datasets in macroeconomics and finance and often reject the null. This suggests the presence of sparsity --- on top of a dense model --- in commonly studied economic applications. The R package \texttt{\textquotesingle FAS\textquotesingle}  implements our approach. 
	\end{abstract}
	
	\noindent%
	{\it Keywords:}  sparse plus dense, high-dimensional inference, LASSO, factor models %3 to 6 keywords, that do not appear in the title
	\vfill
	
	\newpage
	\spacingset{1.5} % DON'T change the spacing!
	
	
\section{Introduction}
	
In this paper, we investigate a factor-augmented sparse regression model. Our analysis involves an observed sample of $T$ real-valued outcomes $y_1,\dots,y_T$, and high-dimensional regressors $x_1,\dots,x_T\in\R^p$, which are interconnected as follows:
\begin{equation}\label{model}
	\begin{aligned}
		y_t&=f_t^\top \gamma^*+u_t^\top\beta^*+\varepsilon_t,\\
		x_t&=Bf_t+u_t,\quad t=1\dots,T.
	\end{aligned}
\end{equation}
Here, $\varepsilon_t\in \R$ represents a random error, $u_t$ is a $p$-dimensional random vector of idiosyncratic shocks, $f_t$ is a $K$-dimensional random vector of factors, and $B$ is a $p\times K$ (nonrandom) matrix of loadings. The parameters of interest are $\gamma^*\in \R^K$ and $\beta^*\in \R^p$ and the right-hand side of \eqref{model} is unobserved. We consider the case where the number $p$ of regressors is large with respect to the sample size $T$ and a sparsity condition on the high-dimensional parameter vector $\beta^*$ is imposed. In the asymptotic regimes we study, $T$ goes to infinity, $p$ is allowed to grow with $T$ while $K$ remains fixed. The model formulation in equation \eqref{model} effectively merges two popular approaches in handling high-dimensional datasets: factor regression (\cite{stock2002forecasting,bai2006confidence}) and sparse high-dimensional regression (\cite{tibshirani1996regression,bickel2009simultaneous}).
	%\footnote{\label{foot.sparse}To further clarify the link with sparse regression, note that the first equation of \eqref{model} can be rewritten $y_t=f_t^\top\varphi^*+x_t^\top\beta^*+\varepsilon_t$, where $\varphi^*=\gamma^*-B^\top \beta^*$, which becomes a usual high-dimensional sparse regression model when $\varphi^*=0$.}.
Such a model allows the outcome to be related to the regressors through both common and idiosyncratic shocks and may better explain the data than factor regression or sparse regression alone (see \cite{fan2023latent,fan2021bridging}, which introduce and study model \eqref{model}). As noted in \cite{fan2021bridging}, this type of structure has many applications in forecasting, causal inference and to describe correlation networks. Note that, as in \cite{stock2002forecasting,bai2006confidence,fan2021bridging}, we could augment the model \eqref{model} with additional regressors $w_t$ entering the first equation of \eqref{model} but not the second one. This case is discussed in the Appendix Section \ref{app.modelw}.
	
We develop a test for the hypothesis:
\begin{equation}\label{Htest}
	H_0:\beta^*=0\quad \text{against}\quad H_1:\beta^*\ne 0 \text{ is sparse},
\end{equation}
where our theory outlines the set of sparse alternatives against which our test has power. Our specification test sheds light on the data-generating process by allowing us to determine if the underlying model is dense (as is the factor regression model) or sparse plus dense, as is the factor-augmented sparse regression model. This determination will then tell us if the relation between the regressors and the outcome is only driven by common shocks (factor regression) or if idiosyncratic shocks also play a role (factor-augmented sparse regression). The question of the adequacy of sparse or dense representations has recently garnered significant attention (see, e.g., \cite{giannone2021economic,kolesar2023fragility}). However, existing studies mostly focus on the differences between sparse and dense models and have found that dense models are often more adequate. In contrast, we compare a dense model with a sparse plus dense alternative.

In this paper, we propose a new bootstrap test for \eqref{Htest}. Our test's principle is to compare two estimators of $\sum_{t=1}^T u_t\varepsilon_t$. The first estimator is only consistent under the null, while the second estimator relies on the LASSO, and is, therefore, consistent under sparse alternatives. Our proposed test does not require estimating covariance matrices and is easy to implement. Following \cite{lederer2021estimating}, we outline a data-driven rule to select the tuning parameter of the LASSO estimator and prove its theoretical validity. We establish the validity of the test within a theoretical framework that accommodates scenarios where the number of variables, denoted by $p$, can significantly exceed $T$, and the explanatory variables exhibit strong mixing and possess polynomial tails. We use simulations to evaluate the finite sample properties of our procedure. Our test controls size and exhibits good power against sparse alternatives even when $p$ greatly exceeds $T$ or the data are heavy-tailed and serially correlated. A potential limitation of our approach might be that our approach also rejects when $\beta^*$ is nonzero but has a dense structure. To assess this issue, we conduct simulations with dense $\beta^*$. We find that our test exhibits very low power against such alternatives (in absolute terms and also relatively to sparse alternatives with the same signal-to-noise ratio). Hence, our test exhibits some robustness against dense alternatives. This result is intuitive: the LASSO estimator on which our test relies sets to zero very small coefficients pertaining to dense alternatives. Finally, we apply our test to several commonly studied datasets in macroeconomics and finance and often reject the null. This suggests that sparsity can help describe economic data once a dense component (here, modeled through the factors) is included in the model. This result complements the recent studies \cite{giannone2021economic,kolesar2023fragility}, which concluded that dense representations were often more appropriate for economic data.  The R package \texttt{\textquotesingle FAS\textquotesingle}  implements our approach.\\

\noindent 	\textbf{Related literature.} Our paper relates to the growing literature on factor-augmented sparse models, see \cite{hansen2019factor,fan2023do, fan2023latent, fan2021bridging, vogt2022cce, beyhum2023factor,barigozzi2023fnets} among others. In particular, \cite{fan2021bridging} proposes a general framework for factor-augmented sparse models. As noted by a reviewer, our test can be used to check that the sparse idiosyncratic components are jointly significant after the third step of the methodology by \cite{fan2021bridging}. Note that \cite{fan2021bridging}'s model includes lags of idiosyncratic terms and additional regressors. In the Appendix, we explain how to extend our test to the case with additional regressors. Section \ref{OA-subsec.lag} of the Online Appendix outlines an adapted test with a lag. We do not formally prove that our test works in these cases, but we conjecture that the asymptotic properties extend to these more general models. Simulations reported in Section \ref{OA-subsec.add.sim} of the Online Appendix corroborate this presumption. We also note that the factor-augmented sparse model studied in the present paper is a generalization of an earlier model studied in \cite{fosten2017model,fosten2017confidence} which augments the factor regression of \cite{stock2002forecasting} with a low-dimensional set of idiosyncratic terms.
	
\cite{fan2023latent} recently introduced the Factor-Adjusted deBiased Test (FabTest) for evaluating \eqref{Htest}. However, the FabTest exhibits several limitations. The test relies on a desparsified LASSO estimator based on model \eqref{model}. To achieve desparsification, \cite{fan2023latent} utilized the nodewise LASSO method proposed by \cite{zhang2014confidence} and \cite{van2014asymptotically} for estimating the precision matrix of the idiosyncratic shocks. However, this approach introduces $p$ additional tuning parameters, in addition to the one used in the original LASSO regression. Although the tuning parameters are selected through cross-validation in practice, \cite{fan2023latent} did not provide a theoretical justification for this selection procedure. Besides, inferential theory for LASSO-type regressions is not well understood when the tuning parameter is selected by cross-validation. Moreover, the test's performance may deteriorate due to errors associated with the nodewise LASSO estimates, and it incurs a heavy computational cost. Another limitation of the FabTest is its reliance on estimating the variance of $\varepsilon_t$, which can lead to imprecise results where variance estimation is challenging. Additionally, \cite{fan2023latent} only established the validity of the FabTest for i.i.d. sub-Gaussian data (see Section 2 in \cite{fan2023latent}). An alternative is to use the partial covariance test developed by \cite{fan2021bridging} and applied in \cite{fan2023do}. This test's principle is to estimate the covariance matrix of the idiosyncratic terms (including the idiosyncratic term of $y_t$, that is, $u_t^\top\beta^*+\varepsilon_t$) and then use a Gaussian bootstrap under the null to obtain the critical value of the test statistic. In contrast to our test, this partial covariance test does not make use of the LASSO estimator and requires estimating a high-dimensional covariance matrix, which is challenging.
	
Finally, we would like to note that this paper contributes to various other strands of literature. First, it connects to recent literature considering testing for high-dimensional parameters. There exists several approaches, see \cite{fan2015power,zhu2018linear,chernozhukov2019inference,lederer2021estimating,he2023most} and references therein. Our strategy draws inspiration from \cite{lederer2021estimating}, a recent paper that introduces a bootstrap procedure for selecting the penalty parameter of LASSO in a standard sparse linear regression. They employ this procedure to test the null hypothesis that a specific high-dimensional parameter equals zero. We adapt their approach to the case with unobserved factors, time series dependence and polynomial tails, which poses a challenge beyond the scope of the results in \cite{lederer2021estimating}. Second, our work is related to the literature on inference on parameters of additional low-dimensional regressors in the factor regression model of \cite{stock2002forecasting}, see \cite{bai2006confidence,gonccalves2014bootstrapping,gonccalves2020bootstrapping}. Third, our work connects with the literature on specification tests for models involving unobserved factors. Many papers test for the validity of the assumption that loadings are time-independent in the approximate factor model itself — the second equation in \eqref{model} — (\cite{breitung2011testing, chen2014detecting, han2015tests, yamamoto2015testing, su2017time, su2020testing, baltagi2021estimating, xu2022testing, fu2023testing}), while \cite{corradi2014testing} tests for time-independence of all coefficients in the factor regression model of \cite{stock2002forecasting}. Our approach complements this literature by proposing a specification test of the factor regression model under a different alternative, namely the factor-augmented sparse regression model.  \\
	
	%
	
	%In this paper, we propose a new bootstrap test for \eqref{Htest} which avoids the aforementioned shortcomings of the FabTest. The proposed test does neither requires tuning parameters nor the estimation estimation of variance or covariance matrices, and is easy to implement. We establish the validity of the test within a theoretical framework that accommodates scenarios where $p$ can significantly exceed $T$, the variables exhibit strong mixing, and possess exponential tails. Our procedure demonstrates favorable performance in finite sample simulations. Additionally, we apply our test to two prediction exercises using the \texttt{FRED-MD} dataset (\cite{mccracken2016fred}) and reject the hypothesis that the data follows a factor regression model.
	
	%Our strategy is inspired by \cite{lederer2021estimating}. This recent paper proposes a bootstrap procedure for selecting LASSO's tuning parameter of LASSO in standard sparse linear regression, and employs it to test the null hypothesis that a specific high-dimensional parameter is equal to zero. Our testing procedure can be seen as an adaptation of their approach to the case with unobserved factors. This presents a challenge beyond the scope of the results in \cite{lederer2021estimating} because the unobserved factors need to be estimated, indicating that they act as generated regressors. \\
	
	%Before concluding this introduction, we would like to stress that the present paper contributes to several strings of literature. First, this paper makes a valuable contribution to the current literature combining factor models and sparse regression, see \cite{hansen2019factor,fan2023latent,fan2021bridging,vogt2022cce,beyhum2023factor}. We propose a strategy allowing to test the standard factor regression model within this framework. Second, our work is also related to the literature on specification of the factor regression models. Many paper directly test the validity of the factor model itself (the second equation in \eqref{model}), see \cite{breitung2011testing,chen2014detecting,han2015tests,yamamoto2015testing,su2017time,su2020testing,baltagi2021estimating,xu2022testing,fu2023testing}. \cite{corradi2014testing} focuses instead the factor regression model. In all these papers, the alternative hypothesis is that of the presence of structural breaks and/or (smoothly) time-varying loadings. Our approach complements the literature by proposing a specification test of the factor regression model under a different alternative, namely that of the factor-augmented sparse regression model. Third, this paper this paper contributes to the existing body of research on high-dimensional inference. While most studies in this field focus on testing hypotheses related to low-dimensional parameters (\cite{zhang2014confidence,van2014asymptotically} among many others), only a limited number of works address the challenge of hypothesis testing for high-dimensional parameters, as explored in our current paper. Aside from \cite{lederer2021estimating}, \cite{chernozhukov2019inference} introduces a procedure to test multiple moment inequalities, which accommodates dependent data using $\beta$-mixing conditions. We contribute to this literature by testing for a high-dimensional parameter in our specific model with estimated factors.\\
	
	%\noindent \textbf{Related literature.}  Particularly, \cite{fan2023latent} introduced the Factor-Adjusted deBiased Test (FabTest) for evaluating \eqref{Htest}. However, the FabTest exhibits several limitations that this paper aims to address. The test relies on a desparsified LASSO estimator based on model \eqref{model}. To achieve desparsification, \cite{fan2023latent} utilized the nodewise LASSO method proposed by \cite{zhang2014confidence} and \cite{van2014asymptotically} for estimating the precision matrix of the idiosyncratic shocks. Unfortunately, this approach introduces $p$ additional tuning parameters, in addition to the one used in the original LASSO regression. Although the tuning parameters are selected through cross-validation in practice, \cite{fan2023latent} did not provide a theoretical justification for this selection procedure. Moreover, the test's performance may deteriorate due to errors associated with the nodewise LASSO estimates, and it incurs a heavy computational cost. Another limitation of the FabTest is its reliance on estimating the variance $\sigma^2$ for the idiosyncratic shocks, which can lead to imprecise results in time series applications where variance estimation is challenging. Additionally, \cite{fan2023latent} only established the validity of the FabTest for i.i.d. sub-Gaussian data.
	
	%Our work is also related to the literature on specification of the factor regression models. Many paper directly test the validity of the factor model itself (the second equation in \eqref{model}), see \cite{breitung2011testing,chen2014detecting,han2015tests,yamamoto2015testing,su2017time,su2020testing,baltagi2021estimating,xu2022testing,fu2023testing}. \cite{corradi2014testing} focuses instead the factor regression model. In all these papers, the alternative hypothesis is that of the presence of structural breaks and/or (smoothly) time-varying loadings. Our approach departs radically from this line of work because our alternative is that of the factor-augmented sparse regression model.
	
	%Finally, this paper contributes to the literature on high-dimensional inference. Most papers in this literature study how to test hypotheses related to low-dimensional parameters (\cite{zhang2014confidence,van2014asymptotically}). Only few studies consider the problem of hypothesis testing for high-dimensional parameters such as the one tackled in the present paper. \cite{chernozhukov2019inference} proposes a procedure to test many moment inequalities. Their theory allows for dependent data through $\beta$-mixing conditions. Closer to this paper, \cite{lederer2021estimating} proposes a bootstrap procedure to select LASSO's tuning parameter in the standard sparse lienar regression setting and applies it to test the fact that some high-dimensional parameter is $0$. We contribute to this literature by extending \cite{lederer2021estimating}'s procedure to the case where some regressors are estimated factors.
	
	
	%Finally, this paper contributes to the existing body of research on high-dimensional inference. While most studies in this field focus on testing hypotheses related to low-dimensional parameters (\cite{zhang2014confidence,van2014asymptotically} among many others), only a limited number of works address the challenge of hypothesis testing for high-dimensional parameters, as explored in our current paper. For instance, \cite{chernozhukov2019inference} introduces a procedure to test multiple moment inequalities, which accommodates dependent data using $\beta$-mixing conditions. Another related work is \cite{lederer2021estimating}, which proposes a bootstrap procedure for selecting LASSO's tuning parameter of LASSO in standard sparse linear regression, and employs it to test the null hypothesis that a specific high-dimensional parameter is equal to zero. As mentioned above, building upon this prior research, our contribution lies in extending \cite{lederer2021estimating}'s procedure to incorporate cases where some of the regressors are estimated factors.\\
	
	%Finally, we contribute to the literature on testing for high-dimensional parameters. 
	
	
	
	
	
	
	
	%In this paper, we study a factor-augmented sparse regression model.  We observe a sample of $T$ real-valued outcomes $y_1,\dots,y_T$ and of $p$-dimensional regressors $x_1,\dots,x_T\in\R^p$, which are related in the following way:
	%\begin{equation}\label{model}
	%\begin{aligned}
	%n_t\in \R$ is a random error, $u_t$ is a $p$-dimensional random vector of idiosyncratic shocks,  $f_t$ is a $K$-dimensional random vector of factors, $B$ is a $p\times K$ deterministic matrix of loadings. The parameters are $\gamma^*\in \R^K$ and $\beta^*\in \R^p$. The number of regressors can be very large and potentially greater than $T$. Sparsity conditions are imposed on the high-dimensional parameter $\beta^*$. Model \eqref{model} effectively combines two of the most popular high-dimensional approaches in econometrics: factor regression (\cite{stock2002forecasting,bai2006confidence}) and sparse high-dimensional regression (\cite{tibshirani1996regression}).
	
	%We consider testing
	%\begin{equation}\label{Htest} H_0:\beta^*\text{ against } H_1:\beta^*\ne 0.\end{equation}
	%This can be seen as a specification test of the classical factor regression model against factor-augmented sparse alternatives. It allows to assess if one should use factor regression or factor-augmented sparse regression in practice. It can also inform the econometrician about the true data generating process, in particular if the underlying model is dense - as is the factor regression model - or sparse plus dense - as is the factor-augmented sparse regression model. This question of the adequacy of sparse or dense representations has recently generated a lot of interest in econometrics, see for instance \cite{abadie2019choosing, giannone2021economic}.
	
	%To develop our test, we adapt the bootstrap procedure of \cite{lederer2021estimating} to our setting with unobserved factors. The proposed test is free of tuning parameters, does not require the estimation of the variance or covariance matrices and is relatively computationally simple. We show the validity of the proposed test in a theoretical framework where $p$ can be much larger than $T$, the variables are strongly mixing and have exponential tails. This is challenging and goes beyong the results of \cite{lederer2021estimating}  because the unobserved factors have to estimated, in other words they are generated regressors. Our procedure exhibits good finite sample performance in simulations. We apply our test and reject the fact that that the data follows a factor regression model in two prediction exercises utilizing the \texttt{FRED-MD} dataset (\cite{mccracken2016fred}).
	
	%- by assuming that $x_t$ follows an approximate factor model, of which the factors $f_t$ explain also the outcome - and sparse regression - by letting the idiosyncratic shocks $u_t$ enter sparsely the regression.
	
	
	
	
	%High-dimensional datasets have become increasingly prevalent across various fields, such as finance, economics, genomics, and social sciences. As the dimensionality of the data increases, the challenges posed by the curse of dimensionality have prompted the development of specific statistical techniques tailored to address these complexities. In the field of econometrics, two prominent approaches have emerged: factor regression and sparse linear regression.
	
	%Factor regression, as demonstrated in studies by \cite{stock2002forecasting,bai2006confidence}, relies on the existence of latent factors, that is unobserved variables that capture the common variation present in the regressors. To estimate these latent factors, techniques like principal component analysis or factor analysis are employed. By incorporating these factors into the regression framework, the model effectively reduces the dimensionality of the predictor space. It is however a dense model in the original space of regressors since most variables matter for estimation of the factors, and, therefore, for prediction.
	
	%On the other hand, sparse linear regression operates under the assumption of sparsity in the regression parameters, as pioneered by  \cite{tibshirani1996regression}. This assumption implies that only a subset of regressors significantly influences the dependent variable. Consequently, the model offers a more concise representation of the data and enhances interpretability by emphasizing the key variables that drive the relationship with the dependent variable.
	
	%Recent research - such as \cite{hansen2019factor,fan2023latent,fan2021bridging} - has introduced new models combining these two common dimension reduction approaches. In this paper, we focus on the factor-augmented sparse regression model of \cite{fan2023latent}.  We observe a sample of $T$ real-valued outcomes $y_1,\dots,y_T$ and of $p$-dimensional regressors $x_1,\dots,x_T\in\R^p$, which are related in the following way:
	
	
	%Note that the factor-augmented sparse regression model includes both factor regression and sparse regression as special cases by setting either $\beta^*$ or $\varphi^*:=\gamma^*-B\beta^*$ to $0$.  for
	%\begin{equation}\label{Htest} H_0:\beta^*\text{ against } H_1:\beta^*\ne 0.\end{equation}
	%As noted in \cite{fan2023latent}, the FabTest can be seen as a specification test of the classical factor regression model against factor-augmented sparse alternatives. It allows to assess if one should go beyond factor regression. It can also inform the econometrician about the true data generating process, in particular if the underlying model is dense - as is the factor regression model - or sparse plus dense - as is the factor-augmented sparse regression model. This question of the adequacy of sparse or dense representations has recently generated a lot of interest in econometrics, see for instance \cite{abadie2019choosing, giannone2021economic}.
	
	
	
	%The main contribution of the present paper is to propose a tuning-free test for \eqref{Htest}. Our approach is based on the bootstrap test of joint signicance in the high-dimensional regression model developed by \cite{lederer2021estimating}.
	%We show under ...
	
	%Add factor-augmented regression literature.
	
\noindent\textbf{Outline.} The paper is organized as follows. In Section \ref{sec.test}, we outline our testing procedure. Its asymptotic properties are studied in Section \ref{sec.theory}. Then, Section \ref{sec.sim} contains Monte Carlo simulations. The empirical applications can be found in Section \ref{sec.emp}. Section \ref{sec.ccl} concludes. The Appendix contains an adapted testing procedure for a model with additional regressors and lags. The Online Appendix includes all proofs and additional discussions and simulation and empirical results.\\
	
\noindent\textbf{Notation.} For an integer $N\in \mathbb{N}$, let $[N]=\{1,\dots, N\}$. The transpose of a $n_1 \times n_2$ matrix $A$ is written $A^{\top}$. Its $k^{th}$ singular value is $\sigma_k(A)$. Let us also define the Euclidean norm $\left\| A\right\|_2^2=\sum_{i=1}^{n_1} \sum_{j=1}^{n_2}A_{ij}^2$ and the sup-norm $\|A\|_\infty=\max\limits_{i\in[n_1], j\in[n_2]}|A_{ij}|$. The quantity $n_1 \vee n_2$ is the maximum of $n_1$ and $n_2$, $n_1 \wedge  n_2$ is the minimum of $n_1$ and $n_2$. For $N\in \mathbb{N}$, $I_N$ is the identity matrix of size $N\times N$. For a real-valued random variable $Z$ and $g>0$, we let $\vertiii{Z}=\E[|Z|^g]^\frac1g$. For a $d$-dimensional random vector $Z$, we define $\vertiii{Z}_g=\sup\limits_{u\in\R^d: \ \|u\|_2\le 1}\vertiii{u^\top Z}_g$.
	
\section{The test}\label{sec.test}
\subsection{Testing procedure}

In this subsection, we explain our testing procedure, which is then summarized in algorithmic form in subsection \ref{subsec.computation}. To facilitate understanding, we rewrite the model in matrix form as follows:
\begin{equation*}
\begin{aligned}
Y&=F \gamma^*+U\beta^*+\mathcal{E},\\
X&=FB^\top+U,
\end{aligned}
\end{equation*}
where $Y=(y_1,\dots,y_T)^\top$, $F=(f_1,\dots, f_T)^\top$ is a $T\times K$ matrix, $U=(u_1,\dots,u_T)^\top$ and $X=(x_1,\dots,x_T)^\top$ are $T\times p$ matrices and $\mathcal{E}=(\varepsilon_1,\dots,\varepsilon_T)^\top.$

It is important to note that, under the null hypothesis $H_0$, we have $U^\top (Y-F\gamma^*)=U^\top\mathcal{E}$. This observation suggests a testing procedure that involves computing an estimate $2T^{-1}\left\|U^\top (Y-F\gamma^*)\right\|_{\infty}$ and comparing it with the (estimated) quantiles of $2T^{-1}\left\|U^\top \mathcal{E}\right\|_{\infty}$.\footnote{We have a factor 2 in front of $T^{-1}\left\|U^\top (Y-F\gamma^*)\right\|_{\infty}$ and $T^{-1}\left\|U^\top \mathcal{E}\right\|_{\infty}$ because $2T^{-1}\left\|U^\top \mathcal{E}\right\|_{\infty}$ is the effective noise of the problem, a natural concept in the literature on the LASSO, see \cite{lederer2021estimating}.}

We can  estimate $U^\top (Y-F\gamma^*)$ by principal components analysis. First, let $\widehat{K}$ be one of the many estimators of the number of factors $K$ available in the literature (see for instance \cite{bai2002determining,onatski2010determining,ahn2013eigenvalue,bai2019rank,fan2022estimating}). As in \cite{fan2023latent}, we let the columns of $\widehat{F}/\sqrt{T}$ be the eigenvectors corresponding to the leading $\widehat{K}$ eigenvalues of $XX^\top$ and $\widehat{B}= X^\top \widehat{F}(\widehat{F}^\top \widehat{F})^{-1}=T^{-1} X^\top \widehat{F} .$
Then, we project the data on the orthogonal of the vector space generated by the estimated factors. Let $\widehat{P}=T^{-1} \widehat{F}\left(\widehat{F}^\top \widehat{F}/T\right)^{-1}\widehat{F}^\top=T^{-1} \widehat{F}\widehat{F}^\top$ be the projector on the vector space generated by the columns of $\widehat{F}$. A natural estimate for $U$ is $\widehat{U}=X-\widehat{F}\widehat{B}^\top =\left(I_T-\widehat{P}\right) X$.  Similarly, we let $\widetilde{Y}=\left(I_T-\widehat{P}\right)Y$ be an estimate of $Y-F\gamma^*$. The final estimate of $2T^{-1}\left\|U^\top (Y-F\gamma^*)\right\|_{\infty}$ is our test statistic \begin{equation}\label{test_stat_510}2T^{-1}\left\|\widehat{U}^\top \widetilde{Y}\right\|_{\infty}.\end{equation}


Next, to estimate the quantiles of the distribution of $2T^{-1}\left\|U^\top \mathcal{E}\right\|_{\infty},$ we need an estimate of $\mathcal{E}$. We obtain it through the following LASSO estimator:
\begin{equation}\label{eq:lasso}
\widehat{\beta}_\lambda = \argmin_{\beta\in\R^p}\frac{1}{T}\left\|\widetilde{Y}-\widehat{U}\beta\right\|_2^2+\lambda\|\beta\|_1,
\end{equation}
where $\lambda>0$ is a penalty parameter, the choice of which will be fully data-driven in both theory and practice. For $t\in [T]$, we denote by $\widetilde{y}_t$ the $t^{th}$ element of $\widetilde{Y}$ and $\widehat{u}_t$ as the $T\times 1$ vector corresponding to the $t^{th}$ row of $\widehat{U}$.
For a given $\lambda>0$, let $\widehat{\varepsilon}_{\lambda,t}=\widetilde{y}_t -\widehat{u}_t^\top\widehat{\beta}_{\lambda},\ t\in[T]$ be the estimate of $\varepsilon_t$. For a fixed $\alpha \in(0,1)$, we can then estimate $q_\alpha$, the $(1-\alpha)$ quantile of the distribution of $2T^{-1}\|U^\top \mathcal{E}\|_{\infty}$, by the Gaussian multiplier bootstrap. Let $e=(e_1,\dots,e_T)$ be a standard normal random vector independent of the data $(X,Y)$ and define the criterion
$$\widehat{Q}(\lambda,e)=\left\|\frac2T \sum_{t=1}^T\widehat{u}_t\widehat{\varepsilon}_{\lambda,t} e_t\right\|_\infty.$$
The estimate $\widehat{q}_\alpha(\lambda)$ of $q_\alpha$ is then the $(1-\alpha)$-quantile of the distribution of $\widehat{Q}(\lambda,e)$ given $X$ and $Y$. Formally, $\widehat{q}_\alpha(\lambda)=\inf\left\{q:\P_e(\widehat{Q}(\lambda,e)\le q)\ge 1-\alpha\right\}$, where $\P_e(\cdot)=\P(\cdot|X,Y)$.

The only remaining element is the procedure to select $\lambda$. We adapt the approach of \cite{lederer2021estimating} to our setting. Our choice of $\lambda$ is
\begin{equation}\label{choicelambda} \widehat{\lambda}_\alpha =\inf\{\lambda>0\ :\  \widehat{q}_\alpha(\lambda')\le \lambda' \text{ for all } \lambda'\ge \lambda\}.\end{equation}
We explain in Section \ref{subsec.computation} how to compute $\widehat{\lambda}_\alpha$ in practice. The infimum in \eqref{choicelambda} exists because for all $\lambda \ge \bar{\lambda}=2T^{-1}\left\|\widehat{U}^\top\widetilde{Y}\right\|_\infty$, it holds that $\widehat{\beta}_\lambda =\widehat{\beta}_{\bar \lambda}=0.$ Moreover, since $\widehat{U}\widehat{\beta}_\lambda$ is a continuous function of $\lambda$, $\widehat{q}_\alpha(\lambda)$ is also continuous in $\lambda$ and the infimum is attained at a point $\widehat{\lambda}_\alpha>0$ such that $q_\alpha\left(\widehat{\lambda}_\alpha\right)= \widehat{\lambda}_\alpha$. Let us recall briefly the heuristics behind the choice of $\lambda$ and refer the reader to \cite{lederer2021estimating} for more details. First, note that when $\lambda$ is close to $q_{\alpha}$, standard convergence bounds for the LASSO suggest that $\widehat{\beta}_\lambda$ is a precise estimate of $\beta^*$, so that $\widehat{\varepsilon}_{\lambda,t}$ is a good estimate of $\varepsilon_t$ and, in turn, $\widehat{q}_\alpha(\lambda)$ is close to $q_\alpha$. Second, when $\lambda$ becomes (much) larger than $q_\alpha$, the error $\widehat{\varepsilon}_{\lambda,t}-\varepsilon_t$ becomes large and dependent of $\widehat{u}_t$, which in turn increases $\widehat{q}_\alpha(\lambda)$ and leads it to be larger than $q_{\alpha}$. We then let our estimator of $q_\alpha$ be $\widehat{\lambda}_\alpha=\widehat{q}_\alpha\left(\widehat{\lambda}_\alpha\right)$.

The test rejects $H_0$ at the level $\alpha$ when our test statistic is given in \eqref{test_stat_510} is larger than the estimate $\widehat{\lambda}_\alpha$ of $q_\alpha$. Therefore, our testing procedure is free of tuning parameters stemming from the LASSO regression in equation \eqref{eq:lasso}.
	
	\subsection{Computation}\label{subsec.computation}
	Algorithm \ref{algo} below explains how to conduct the test in practice. Let us discuss Step 4 of Algorithm \ref{algo} in detail. It approximates $\widehat{\lambda}_\alpha$ as defined in \eqref{choicelambda}. It is advisable to set the grid size $M$ and the number of bootstrap samples $L$ to be as large as possible. As mentioned in \cite{lederer2021estimating}, one can speed up Step 4b. by computing the LASSO with a warm start along the penalty parameter path (see, e.g., \cite{friedman2007pathwise}), i.e., for each decreasing $\lambda$, the new coefficient estimate is computed by using the previous (i.e., the one that was computed with a larger value of $\lambda$) as a starting value. Furthermore, Step 4c. can be accelerated through parallelization techniques. In our implementation, we use both suggestions, which greatly speed up the computations. We also note that to compute the $p$-value of the test, it suffices to conduct it on a grid of values of $\alpha$ and let the $p$-value be equal to the largest value of $\alpha$ in this grid such that the test of level $\alpha$ rejects $H_0$.

	\begin{algorithm}
		\singlerule\vspace{0.1cm}
		\begin{algorithmic}
			\STATE \textbf{1.} Estimate $\widehat{K}$ by one of the available estimators of the number of factors.
			\STATE \textbf{2.} Let the columns of $\widehat{F}/\sqrt{T}$ be the eigenvectors corresponding to the leading $\widehat{K}$ eigenvalues of $XX^\top$.
			\STATE \textbf{3.} Compute $\widehat{U}=\left(I_T-\widehat{P}\right) X$ and $\widetilde{Y}=\left(I_T-\widehat{P}\right)Y$, where $\widehat{P}=T^{-1} \widehat{F}\widehat{F}^\top$.
			\STATE \textbf{4.} Calculate an approximation $\widehat{\lambda}_{\alpha,emp}$ of $\widehat{\lambda}_\alpha$ as follows:
			\STATE \begin{itemize}
				\item[\textbf{4a.}] Specify a grid $0<\lambda_1<\dots<\lambda_M<\bar{\lambda}$, with $\bar{\lambda}=2T^{-1}\left\|\widehat{U}^\top\widetilde{Y}\right\|_\infty$.
				\item[\textbf{4b.}] For $m\in[M]$ compute $\left\{\widehat{Q}\left(\lambda_m,e^{(\ell)}\right):\ \ell\in[L]\right\}$ for $L$ draws of $e\sim\mathcal{N}(0,I_T)$ and the corresponding empirical $(1-\alpha)$-quantile $\widehat{q}_{\alpha,emp}(\lambda_m)$ from them.
				\item[\textbf{4c.}] Let $\widehat{\lambda}_{\alpha,emp}=\widehat{q}_{\alpha,emp}(\lambda_{\widehat{m}})$,  with $\widehat{m}=\min\{m\in[M]:\ \widehat{q}_{\alpha,emp}(\lambda_{m'})\le \lambda_{m'}\text{ for all }m'\ge m\}.$
			\end{itemize}
			\STATE \textbf{5.} Reject $H_0$ when $2T^{-1}\left\|\widehat{U}^\top \widetilde{Y}\right\|_\infty>\widehat{\lambda}_{\alpha,emp}$.
		\end{algorithmic}
		\doublerule
		\caption{Conducting a test of level $\alpha\in(0,1)$.}
		\label{algo}
	\end{algorithm}
	
	
	
	\noindent 
	
	\section{Asymptotic theory}\label{sec.theory} In this section, we provide the asymptotic properties of the test in a theoretical framework allowing for time series dependence in the factors and the idiosyncratic shocks and polynomial tails. We place ourselves in an asymptotic regime where $T$ goes to infinity and $p$ goes to infinity as a function of $T$. The number of factors $K$ is fixed with $T$. It would be possible to let it grow, see, for instance, \cite{beyhum2022factor}. The distributions of the factors $f_{t}$ and the error terms $\varepsilon_t$ do not depend on $T$, while the distribution of the other variables are allowed to vary with $T$. All the constants we introduce are universal in the sense that they do not vary with the sample size. Our assumptions are similar to that of \cite{fan2021bridging} but significantly weaker than that of \cite{fan2023latent}, which imposes that the variables are i.i.d. sub-Gaussian.
	
	We introduce further notation. The loading $b_{jk}$ corresponds to the $j^{th}$ element of the $k^{th}$ column of $B$. Let also $b_j=(b_{j1},\dots,b_{jK})^\top$. For $t\in[T]$, let 
	$$z_t= \left(u_t^\top,f_t^\top,\varepsilon_t,\frac{1}{\sqrt{p}}\sum_{\ell=1}^pu_{t\ell}b_{\ell}^\top, \frac{1}{\sqrt{p}}\sum_{\ell=1}^pu_{t\ell}f_{t}^\top, \frac{1}{\sqrt{p}}\sum_{\ell=1}^pu_{t\ell}\varepsilon_t\right)^\top.$$ Finally, define $\Sigma=\E[u_tu_t^\top]$.
	
	
	We make the following assumptions. 
	\begin{Assumption}\label{as.nb}
		The estimator $\widehat{K}$ is such that $\P(\widehat{K}=K)\to 1$. 
	\end{Assumption}
	\begin{Assumption}\label{as.moments} It holds that
		\begin{enumerate}[\textup{(}i\textup{)}]
			\item\label{f4i} For all $t\in[T]$, $\E[f_t]=0,\ \E[f_tf_t^\top]=I_K$ and $B^\top B$ is diagonal;
			\item \label{f4ii}  All the eigenvalues of the $K\times K$ matrix $p^{-1} B^\top B$ are bounded away from $0$ and $\infty$ as $p\to \infty$;
			\item\label{f4iii} $\|\Sigma -BB^\top\|_2=O(1)$;
			\item\label{f4iv} $\|B\|_\infty=O(1)$.
		\end{enumerate}
	\end{Assumption}
	\begin{Assumption}\label{as.tail}
		The following holds:
		\begin{enumerate}[\textup{(}i\textup{)}]  
			\item\label{taili} The process $\left\{z_t\right\}_{t}$ is weakly stationary. Moreover, it holds that $$\E[u_{tj}]=\E[u_{tj}f_{tk}]=\E\left[f_{tk}\left(\sum_{\ell=1}^pu_{t\ell}b_{\ell h}\right)\right]=0,$$ \text{ for all } $t\in[T], j\in[p], k,h\in[K]$.
			\item\label{tailii} There exist $\kappa_1,\kappa_2>0$ such that,  for all $t\in[T]$, $\sigma_p(\E\left[\varepsilon_t^2u_tu_t^\top\right])>\kappa_1$, $\left\|\E\left[\varepsilon_t^2u_tu_t^\top\right]\right\|_\infty<\kappa_2$, $\sigma_p(\Sigma)>\kappa_1$ and $\|\Sigma\|_\infty<\kappa_2$;
			\item\label{tailiii} There exist $q \ge 8 $, $C_1>0$ and $\zeta>0$, such that for $s,t\in[T]$, we have 
			\begin{align*}
				\vertiii{z_t}_{q+\zeta}&<C_1;\\
				\vertiii{p^{-1/2}\left(u_s^\top u_{t}-\E\left[u_s^\top u_t\right]\right)}_{q}&<C_1;
			\end{align*}
			\item\label{tailiv} $\{u_t\varepsilon_t\}_t$ is uncorrelated across $t$, and, for all $t\in[T], j\in[p], k\in[K]$,
			$$\E[u_{tj}\varepsilon_t]=\E[f_{tk}\varepsilon_t]=\E\left[\varepsilon_t\sum_{\ell=1}^pu_{t\ell}b_{\ell k}\right]=0;$$ 
			\item\label{tailv} $\max_{j\in[p],k\in[K]}\left|T^{-1} \sum_{t=1}^T\E\left[u_{tj}\left(\sum_{\ell=1}^pu_{t\ell}b_{\ell k}\right)\right]\right| =O(1)$.
		\end{enumerate}
	\end{Assumption}
	
	\begin{Assumption}\label{as.mixing}
		Let $\tilde{\alpha}$ denote the strong mixing coefficients of $\left\{z_t\right\}_{t}$. There exist $C_2,c>0$ such that $c>\left[ \left(\frac{h+\xi}{\xi}\right)\left(\frac{h}{2}-1\right)\right]\vee \left(\frac{2}{1-\frac{2}{h}}\right)$ and $\kappa =\left(\frac{\frac{1}{2} +\frac{h}{4(h+1)}}{c+\frac{h}{2(h+1)}}\right)<\frac12$, where $h=\frac{q}{2}$ and $\xi=\frac{\zeta}{2}$, and,  for all  $t\in \mathbb{Z}_+$, we have 
		$\tilde{\alpha} (t)\le C_2t^{-c}.$
	\end{Assumption}
	
	Assumption \ref{as.nb} means that the estimator $\widehat{K}$ of the number of factors $K$ is consistent. Examples of $\widehat{K}$ and sufficient conditions for its consistency can be found in \cite{bai2002determining,onatski2010determining,ahn2013eigenvalue,bai2019rank,fan2022estimating}. Assumption \ref{as.moments} is the same as Assumption 3 in \cite{fan2021bridging}. Its conditions \eqref{f4i} and \eqref{f4ii} constitute a strong factor assumption (\cite{bai2003inferential}). Assumption \ref{as.tail} restricts the moments and the tail behavior of the variables. Assumption \ref{as.tail} \eqref{taili},\eqref{tailii},\eqref{tailiv} contain conditions on the moments of the different variables similar to that of the literature (\cite{bai2006confidence,fan2013large}). We assume that the variables in Assumption \ref{as.tail} \eqref{tailiii} have polynomial tails with common parameter $q+\zeta$. It would be possible to have different tail parameters for each variable but we avoid doing so to simplify our presentation. As in \cite{fan2021bridging} the number of finite moments is at least $8$. In the similar context of inference on factor regression models, Assumption E.2. in  \cite{bai2006confidence} and Assumption 7 in \cite{gonccalves2014bootstrapping} impose conditions analogous to the restriction that $\{u_t\varepsilon_t\}_t$ is uncorrelated across $t$ in Assumption \ref{as.tail} \eqref{tailiv}. A sufficient condition for the latter restriction is that $\{\varepsilon_t\}_t$ is uncorrelated across $t$ and independent of $\{u_t\}_t$. Note that this assumption could be avoided by using a block bootstrap method, but this would complicate the test. It may also not be justified because (most of) the serial correlation in the data may be picked up by the factors $f_t$ and not by the error term $\varepsilon_t$. Next, Assumption \ref{as.tail} \eqref{tailv} is a restriction on the cross-section correlation of the idiosyncratic shocks. This condition holds, for instance, if $\{u_t\}_t$ and $\{b_\ell\}_\ell$ are independent and $\max_{j\in[p]}\sum_{\ell=1}^p|\Sigma_{j\ell}| =O(1)$. Assumption \ref{as.mixing} means that the process $\{z_t\}_t$ has strong mixing coefficients decaying polynomially, which is a restriction on the time-series dependence of the variables. A similar assumption is made in \cite{fan2021bridging}. Note that Assumption \ref{as.mixing}  restricts the full distribution of the process $\{z_t\}_t$, while Assumption \ref{as.tail} \eqref{tailiv} just imposes a condition on a particular serial correlation.
	
	Let us introduce $\varphi^*= \gamma^*-B^\top \beta^*$ . To interpret $\varphi^*$, note that the first equation of \eqref{model} can be rewritten $y_t=f_t^\top\varphi^*+x_t^\top\beta^*+\varepsilon_t$, which becomes a usual high-dimensional sparse regression model when $\varphi^*=0$.
	The next assumption concerns the relative growth rates of $T,p,\|\varphi^*\|_2$ and $\|\beta^*\|_1$. 
	\begin{Assumption}\label{as.Rates}
		The following holds:
		\begin{enumerate}[\textup{(}i\textup{)}]  
			\item\label{ratei} $p=O(T^r),$ where $r<\left(\frac{h}{4} -\frac12\right)\wedge \left[(h+1)(c\kappa -\frac12)\right]$;
			\item\label{rateii} $\frac{\log(T\vee p)^{5}}{\sqrt{T}}\|\beta^*\|_1=o(1);$
			\item\label{rateiii} $\log(T\vee p)^{5/2}\frac{\sqrt{T}}{T\wedge p}(\|\varphi^*\|_2\vee 1)=o(1).$
		\end{enumerate}
	\end{Assumption}
	Condition (i) restricts the rate at which $p$ can grow with respect to $T$. We allow it to grow at a polynomial rate, which is more restrictive than the standard restriction for the LASSO (exponential rate). We need to be more restrictive because of the presence of polynomial tails and time series dependence. Assumption \ref{as.Rates} \eqref{rateii}  contains sparsity restrictions on the alternative hypotheses. When $\|\beta^*\|_\infty=O(1)$, condition \eqref{rateii} corresponds, up to logarithmic factors, to the standard consistency condition for the LASSO with bounded regressors and errors with sub-Gaussian tails that is $\sqrt{\log(p)/T}(s_0\vee 1)=o(1)$, where $s_0$ is the number of nonzero coefficients of $\beta^*$. We can impose a similar sparsity condition as in the standard LASSO literature with sub-Gaussian errors, despite dealing with polynomial tails and serial correlation, because we utilize the high-dimensional central limit theorem for polynomial-tailed time series from \cite{fan2021bridging}. Under the rate condition specified in Assumption \ref{as.Rates} \eqref{ratei}, this theorem allows us to essentially revert to the scenario with sub-Gaussian errors. Condition \eqref{rateiii} is a slightly more restrictive version of the standard condition that $\sqrt{T}/(T\wedge p)=o(1)$ for inference in the factor regression model.\footnote{This condition is equivalently stated as $\sqrt{T}/p=o(1)$ in \cite{bai2006confidence,corradi2014testing} and many others.} Indeed, since $\varphi^*$ is of size $K$, it is reasonable to assume that $\|\varphi^*\|_2=O(1)$. Under this condition, \eqref{rateiii} corresponds to $\sqrt{T}/(T\wedge p)=o(1)$ up to logarithmic factors. As noted by a referee, the role of (iii) is to ensure that the error coming from the estimation of the factors is asymptotically negligible (see for instance \cite{bai2006confidence}). It may be possible to relax it using a more elaborate bootstrap scheme, see \cite{gonccalves2014bootstrapping}.  Additionally, it is worth noting that our proofs reveal that Assumption \ref{as.Rates} is stronger than necessary, and the validity of the test could be established under more complex but weaker rate conditions. However, for the sake of clarity, we present Assumption \ref{as.Rates} instead of a more intricate condition.
	
	We have the following theorem. 
	\begin{Theorem}\label{th}Let Assumptions \ref{as.nb},  \ref{as.moments}, \ref{as.tail}, \ref{as.mixing} and \ref{as.Rates} hold. For all $\alpha \in (0,1)$, we have 
		\begin{enumerate}[\textup{(}i\textup{)}] 
			\item\label{thi} If $\beta^*=0$, then $\P\left(T^{-1}\left\|\widehat{U}^\top \widetilde{Y}\right\|_\infty> \widehat{\lambda}_\alpha\right)\le \alpha+o(1).$
			\item\label{thii} If $\sqrt{\frac{\log(T\vee p)}{T\wedge p}}=o_P\left(T^{-1}\left\|U^\top U\beta^*\right\|_\infty\right) $, then $\P\left(T^{-1}\left\|\widehat{U}^\top \widetilde{Y}\right\|_\infty> \widehat{\lambda}_\alpha\right)\to 1$.
		\end{enumerate}
	\end{Theorem}
	The proof of Theorem \ref{th} can be found in Online Appendix \ref{OA-app.proof}. Statement \eqref{thi} means that the empirical size of the test tends to the nominal size. Statement \eqref{thii} shows that the test has asymptotic power equal to $1$ against sequences of alternatives such that $\sqrt{\frac{\log(T\vee p)}{T\wedge p}}=o_P\left(T^{-1}\left\|U^\top U\beta^*\right\|_\infty\right) $. As noted in \cite{lederer2021estimating}, such a condition is inevitable because the presence of the error $\varepsilon_t$ prevents us from distinguishing true $U\beta^*$ and $\varepsilon_t$ when $U\beta^*$ is too small. We discuss further this condition in Section \ref{OA-sec.rate-cond} of the Online Appendix.
	
\section{Monte Carlo simulations}\label{sec.sim}
	
In this section, we provide a Monte Carlo study shedding light on the finite sample performance of our proposed testing procedure.  We use the following model as our data-generating process (DGP):

\begin{equation*}\label{main.mcs}
\begin{aligned}
y_t&=f_t^\top \gamma^*+u_t^\top\beta^*+\varepsilon_t,\\
x_t&=Bf_t+u_t,\quad t=1\dots,T.
\end{aligned}
\end{equation*}

We generate samples with $T=\{200, 400\}$ observations, $p=\{T, 5T\}$ variables and $K=2$ factors.  The loadings $B=\{b_{jk}\}_{j\in[p],k\in[K]}$ are such that $b_{jk}\sim\mathcal{U}[-1,1]$. The factors are generated as $f_t=\rho_f f_{t-1}+ \tilde f_t$ for $t=2,\dots, T$, where $\tilde f_t$ are i.i.d. $\mathcal{N}\left(0,I_K\left(1-\rho_f^2\right)\right)$. The idiosyncratic components $\{u_t\}$ are such that $u_t=\rho_{u}u_{t-1}+\tilde u_t $ for $t=2,\dots, T$, where $\tilde u_t$ are i.i.d. $\mathcal{N}\left(0,\Sigma\left(1-\rho_{u}^2\right)\right)$, with $\Sigma_{ij}=c_u^{|i-j|},i,j\in[p]$., where $c_u$  reflects the amount of cross-sectional dependence. We also let $\varepsilon_t=\rho_{e} \varepsilon_{t-1}+\tilde \varepsilon_t $ for $t=2,\dots, T$, where $\tilde \varepsilon_t$ are i.i.d. $\mathcal{N}\left(0,\left(1-\rho_{e}^2\right)\right)$.

The parameters $\rho_f$, $\rho_{u}$, and $\rho_e$ control the level of time series dependence. The stationary distributions of $f_t$, $u_t$, $\varepsilon_t$ are, respectively, $\mathcal{N}(0,I_K)$, $\mathcal{N}(0,\Sigma)$ and $\mathcal{N}(0,1)$. We initialize $f_0$, $u_0$ and $\varepsilon_0$ as such.  We consider three dependency designs, where we vary the cross-sectional dependence via parameter $s$ and the time series dependence via parameters $(\rho_f, \rho_u, \rho_e)$ as follows:
\begin{itemize}
\item[]\textit{Design 1.} $c_u=\rho_f=\rho_{u}=\rho_e=0$, so that the data are i.i.d. across $j$ and $t$.
\item[]\textit{Design 2.} $c_u=0.1$, $\rho_f=0.6$, $\rho_{u}=0.1$ and $\rho_e=0$, which introduces cross-sectional dependence in the idiosyncratic shocks and time-series dependence in the factors and the idiosyncratic shocks.
\item[]\textit{Design 3.} $c_u=0.1$, $\rho_f=0.6$ and $\rho_{u}=\rho_e=0.1$, where, on top of the cross-sectional dependence, there is time series dependence in the factors, the idiosyncratic shocks and the error terms.
\end{itemize}
Our theory does not formally allow the third design, but we want to show that our test performs well even under weak serial correlation of $\{\varepsilon_t\}_t$.

\smallskip

Finally, we consider two cases for the target parameter $\beta^*$. For the first case we set $\beta^*=(1,0,\dots,0)^\top\times m$, where $m\in\{0,0.1,0.2,0.3,0.4\}$ controls the signal strength. This choice of $\beta^*$ corresponds to a \textit{sparse} design. For the second case, we consider $\beta^*=(1/\sqrt{p}, \dots, 1/\sqrt{p})^\top\times m$, which corresponds to \textit{dense} design. In both cases, we set $\gamma^*=(0.5,0.5)^\top$.  Note that the choice of $\beta^*$ ensures that for the dependence Design 1, the signal-to-noise ratio is the same for both sparse and dense cases.

\smallskip

We compute the rejection probabilities of our test at the significance levels $\alpha\in\{0.1,0.05, 0.01\}$ over $2000$ replications. To implement our test, we set $M=100$ and choose an equidistant grid of values for $\lambda$, using $L=1000$ bootstrap replications. The results are insensitive to the choice of $L$ and $M$ as long as they are sufficiently large, which is expected since their primary role is in the approximation of theoretical quantities. In our experience, $L=M=100$ already yields very precise results. The number of factors $K$ is estimated through the eigenvalue ratio estimator of \cite{ahn2013eigenvalue}.

\subsection{Main results}

The results for $T=200$ and sparse and dense alternatives are reported in Tables \ref{tab.rates_sparse}-\ref{tab.rates_dense}. In the Online Appendix, we present simulations under the same data-generating processes, but with the larger sample size $T=400$ (Tables \ref{OA-app.tab.rates_sparse} and \ref{OA-app.tab.rates_dense}). First, we observe that the empirical size is close to the nominal levels for all designs, indicating that both cross-sectional and serial dependence have little effect on the empirical size of our testing procedure. Notably, the results show consistent power when the alternative hypothesis is sparse (Table \ref{tab.rates_sparse}) across all data-generating processes (DGPs), suggesting that the dependency design does not significantly impact the power of our test. However, the power tends to slightly decrease in scenarios with large $p$ cases for both $T=200$ and $T=400$. Interestingly, in the case of dense alternatives (Table \ref{tab.rates_dense}), we see a significant drop (relative to sparse alternatives)  in power across all designs. This highlights that our testing procedure has low power when  $\beta^*\neq0$ is dense. An intuition for this result is as follows. Our test uses the LASSO estimator, which enforces sparsity. When $\beta^*$ is dense, the LASSO estimator often sets $\widehat{\beta}_{\widehat{\lambda}_\alpha}$ to $0$, leading to non-rejection of the null.

\begin{table}[ht]\setlength\extrarowheight{-4pt}
\centering
\begin{tabular}{lccccccc}
& & p/T = 200/200 & & & & p/T = 1000/200 & \\
\hline
$m$&   $\alpha =0.1$ &$\alpha = 0.05$ &$\alpha =0.01$ && $\alpha =0.1$ &$\alpha =0.05$ &$\alpha =0.01$ \\
\hline\\[-0.3cm]
\multicolumn{8}{c}{{\it Design 1}: $s=\rho_f=\rho_{u}=\rho_e=0$}\\
0.0 & 0.084 & 0.033 & 0.005 & & 0.090 & 0.040 & 0.005   \\
0.1 & 0.122 & 0.062 & 0.015 & & 0.108 & 0.049 & 0.010   \\
0.2 & 0.604 & 0.507 & 0.336 & & 0.274 & 0.184 & 0.082  \\
0.3 & 0.984 & 0.973 & 0.916 & & 0.704 & 0.614 & 0.418  \\
0.4 & 1.000 & 1.000 & 1.000 & & 0.958 & 0.927 & 0.833  \\
\multicolumn{8}{c}{{\it Design 2}: $s=0.1$, $\rho_f=0.6$, $\rho_{u}=0.1$ and $\rho_e=0$}\\
0.0 & 0.092 & 0.045 & 0.010 & & 0.090 & 0.040 & 0.004  \\
0.1 & 0.122 & 0.066 & 0.016 & & 0.104 & 0.048 & 0.009 \\
0.2 & 0.631 & 0.534 & 0.362 & & 0.274 & 0.184 & 0.080  \\
0.3 & 0.984 & 0.971 & 0.931 & & 0.731 & 0.633 & 0.435 \\
0.4 & 1.000 & 1.000 & 1.000 & & 0.961 & 0.931 & 0.853  \\
\multicolumn{8}{c}{{\it Design 3}: $s=0.1$, $\rho_f=0.6$ and $\rho_{u}=\rho_e=0.1$}\\
0.0 & 0.102 & 0.048 & 0.011 & & 0.099 & 0.044 & 0.004 \\
0.1 & 0.130 & 0.070 & 0.017 & & 0.108 & 0.054 & 0.009  \\
0.2 & 0.620 & 0.528 & 0.356 & & 0.273 & 0.186 & 0.084  \\
0.3 & 0.980 & 0.970 & 0.925 & & 0.723 & 0.631 & 0.429  \\
0.4 & 1.000 & 1.000 & 1.000 & & 0.962 & 0.933 & 0.849 \\
\hline\hline
\end{tabular}
\caption{Rejection probabilities for the three dependence designs we consider and sparse $\beta^*$. The data are generated with Gaussian variables. The sample size is $T=200$ while the number of regressors is $p\in\{200, 1000\}$.}
\label{tab.rates_sparse}
\end{table}

\begin{table}[ht]\setlength\extrarowheight{-4pt}
\centering
\begin{tabular}{lccccccc}
& & p/T = 200/200 & & & & p/T = 1000/200 & \\
\hline
$m$&   $\alpha =0.1$ &$\alpha = 0.05$ &$\alpha =0.01$ && $\alpha =0.1$ &$\alpha =0.05$ &$\alpha =0.01$ \\
\hline\\[-0.3cm]
\multicolumn{8}{c}{{\it Design 1}: $s=\rho_f=\rho_{u}=\rho_e=0$}\\
0.0 & 0.084 & 0.033 & 0.005 & & 0.090 & 0.040 & 0.005  \\
0.1 & 0.097 & 0.039 & 0.008 && 0.092 & 0.035 & 0.009  \\
0.2 & 0.114 & 0.060 & 0.007 & & 0.100 & 0.045 & 0.009  \\
0.3 & 0.140 & 0.074 & 0.011 & & 0.124 & 0.055 & 0.010  \\
0.4  & 0.176 & 0.096 & 0.017  & & 0.152 & 0.068 & 0.015 \\
\multicolumn{8}{c}{{\it Design 2}: $s=0.1$, $\rho_f=0.6$, $\rho_{u}=0.1$ and $\rho_e=0$}\\
0.0 & 0.084 & 0.040 & 0.006 & & 0.081 & 0.032 & 0.007 \\
0.1 & 0.085 & 0.041 & 0.009 & & 0.091 & 0.034 & 0.005  \\
0.2 & 0.108 & 0.051 & 0.012 & & 0.114 & 0.051 & 0.008 \\
0.3 & 0.155 & 0.072 & 0.015 & & 0.153 & 0.072 & 0.011  \\
0.4 & 0.210 & 0.109 & 0.023 & & 0.199 & 0.100 & 0.018  \\
\multicolumn{8}{c}{{\it Design 3}: $s=0.1$, $\rho_f=0.6$ and $\rho_{u}=\rho_e=0.1$}\\
0.0 & 0.094 & 0.046 & 0.005 & & 0.091 & 0.035 & 0.006  \\
0.1 & 0.100 & 0.046 & 0.010  & & 0.104 & 0.044 & 0.006 \\
0.2 & 0.121 & 0.060 & 0.012 & & 0.124 & 0.055 & 0.009  \\
0.3 & 0.171 & 0.087 & 0.018 & & 0.168 & 0.080 & 0.012  \\
0.4 & 0.228 & 0.116 & 0.025 & & 0.222 & 0.108 & 0.020 \\
\hline\hline
\end{tabular}
\caption{Rejection probabilities for the three dependence designs we consider and dense $\beta^*$. The data are generated with Gaussian variables. The sample size is $T=200$ while the number of regressors is $p\in\{200, 1000\}$.}
\label{tab.rates_dense}
\end{table}

\subsection{Heavy-tailed data, number of factors and lagged idiosyncratic shocks}\label{mc.heavy.k}

In this subsection, we consider the same data-generating process as in the main design Table \ref{tab.rates_sparse}, but we change elements of it to obtain results for heavy-tailed data and in cases when the number of factors is over- and under-estimated. For the heavy-tailed data scenarios, we generate factors, idiosyncratic shocks and regression errors from student-$t(5)$ distribution. This allows us to investigate the impact of the heavy tails on the proposed testing procedure. Note that our theoretical analysis imposes a condition for the existence of first $8$ finite moments, so that generating student-$t(5)$ errors goes beyond what our assumptions allow.  We generate heavy-tailed factors $f_t$, idiosyncratic components $u_t$ and the error terms $\epsilon_t$ by generating $\tilde{f}_t$, $\tilde{u}_t$ and $\tilde{\epsilon}_t$ from a student-$t(5)$ rather than a Gaussian distribution. Besides changing the distribution of the data, we also compute the performance of our test when the number of factors is either over-estimated or under-estimated. In this case, our data generating process is the same as for Table \ref{tab.rates_sparse}, but rather than estimating the number of factors $K$ using the eigenvalue ratio estimator, we set it to $K=5$ (over-estimated case) and $K=1$ (under-estimated case).

\smallskip

Results are reported in Tables \ref{OA-app.tab.rates_heavy}-\ref{OA-app.tab.rates_heavy2} and \ref{OA-app.tab.rates_k5}-\ref{OA-app.tab.rates2_k1} for heavy-tailed data and different number of factors, respectively. In the former case, we see a slight deterioration of our method compared to the Gaussian DGPs. This is not surprising since the residuals of the LASSO regression, as well as the factors, may be less accurately estimated in finite samples due to heavy tails, see, e.g., \cite{babii2020inference}. Next, we analyze the results of the case of the over-estimated number of factors. The performance is slightly affected compared to the case where we use the eigenvalue ratio estimator to determine the number of factors (see Table \ref{tab.rates_sparse}), but the differences are small. When comparing with the under-estimated case, we see that our testing procedure is over-sized. These results are in line with the literature on inference with factors models, see, e.g., \cite{moon2015linear}.

\smallskip

It is also interesting to investigate the performance of our testing procedure with lagged idiosyncratic shocks. In this case, we generate the data from the following model
\begin{equation}\label{laggedu}
\begin{aligned}
y_t&=f_t^\top \gamma^*+u_t^\top\beta_1^*+u_{t-1}^\top\beta_2^*+\varepsilon_t,\\
x_t&=Bf_t+u_t,\quad t\in[T],
\end{aligned}
\end{equation}
where all the elements are as in the main DGPs except that we add lagged idiosyncratic shock, set $\beta_1^* = \beta^*=(1,0,\dots,0)^\top\times m$ and all elements of $\beta_2^*=0$. The algorithm to compute the test for this model appears in the Online Appendix \ref{OA-algo-lags}. We report results for sparse and Gaussian DGPs which appear in Online Appendix Tables \ref{OA-app.tab.rates_laggedu}-\ref{OA-app.tab.rates2_laggedu}. Results show a slight deterioration of performance in terms of power, while the empirical size appears to be similar as in the main scenario Table \ref{tab.rates_sparse} (see also Table \ref{OA-app.tab.rates_sparse} in Online Appendix for a large $T$ comparison). The small decrease in power is due to an increase in the dimensionality of the regressors.

\section{Empirical applications}\label{sec.emp}
In this section, we present three empirical examples using well-established macroeconomic and financial datasets.

\smallskip

First, we examine the FRED-MD dataset, which includes 121 monthly macroeconomic series covering various sectors of the US economy. For further details, see \cite{mccracken2016fred}. We also study a quarterly variant of FRED-MD with 202 variables, named FRED-QD, for which results are reported in the Online Appendix. Second, we analyze a financial dataset comprising 100 representative anomalies from the literature, which we use to explain aggregate market returns and industry portfolio returns. For more information, we refer to \cite{dong2022anomalies}. Lastly, we investigate the network structure in asset returns using a finance dataset. Following the approach of \cite{fan2021bridging}, we consider a cross-section of monthly stock returns and a set of observable factors to study the relationships among financial firms.

\smallskip

We study these datasets for several reasons. First, FRED-MD and its quarterly variant FRED-QD are among the most widely studied high-dimensional macroeconomic datasets. Providing further insights into these datasets could be valuable for the extensive empirical macroeconomic literature. Second, the empirical asset pricing literature has proposed and examined many factors, known as anomalies, that explain asset prices. Our research offers new insights into applying long-short anomalies to explain the market and industry portfolio returns rather than individual asset returns. Third, applying our method to firm-level stock returns illustrates using a model with observed regressors, as detailed in Appendix Section \ref{app.modelw}. Overall, these diverse applications, using macroeconomic and financial datasets, demonstrate the versatility of our approach across different econometric settings, varying in sample sizes relative to the number of regressors. Lastly, we note that in all empirical applications, we select the number of factors using the eigenvalue ratio estimator of \cite{ahn2013eigenvalue}.

\subsection{Macroeconomic application}
%\textcolor{red}{What is $T$ here?}
In the macroeconomic application, we investigate the FRED-MD dataset over the sample period 1980 January to 2019 December containing $T=480$ observations. This is a commonly used dataset in various macroeconomic studies. In our analysis, we regress each of the 121 variables available in the dataset on common and idiosyncratic shocks. Specifically, we estimate the following model:
\begin{equation}\label{eq:model1}
\begin{aligned}
y_{t+1}&=f_t^\top \gamma^*+u_t^\top\beta^*+\varepsilon_{t+1},\\
x_t&=Bf_t+u_t,\quad t=1\dots,T,
\end{aligned}
\end{equation}
where $y_{t+1}$ represents one of the variables in the dataset at time $t+1$, and $x_t$ includes all the remaining regressors at time $t$. Note that we transform the original series using commonly applied transformations as suggested by \cite{mccracken2016fred}. To estimate $f_t$, we apply PCA using the eigenvalue ratio estimator to determine the number of common factors. We apply our procedure to test $H_0$. Results for each category of variable appear in Table \ref{tab.fredmdreject}.

\smallskip

First, results show that in most categories, we frequently reject the null hypothesis at the 10\%, 5\%, and even 1\% significance levels. This provides evidence of sparsity in the idiosyncratic shocks of the macroeconomic data. The categories where we reject the null most often are \textit{Output and Income}, \textit{Consumption, Orders, and Inventories}, \textit{Labor Market}, \textit{Interest and Exchange Rates}, and \textit{Prices}. For the remaining categories, the null hypothesis is not frequently rejected, suggesting that the sparse component is not important for those series. Interestingly, for the housing category, we never reject the null, while the other two categories for which we reject less frequently, i.e., \textit{Money and Credit} and \textit{Stock Market}, could be classified as financial rather than macroeconomic data. In addition to the results for the FRED-MD dataset, we also obtain similar rejection ratios for the FRED-QD dataset, which is a quarterly macroeconomic dataset similar to FRED-MD, but containing more variables and less observations. Results appear in Table \ref{OA-app.tab.fredqdreject}. The results for the FRED-QD dataset also show a similar pattern, providing evidence of sparsity in the idiosyncratic shocks.


\begin{table}[ht]
\centering
\begin{tabular}{rccc}
Category & 10\% & 5\% & 1\% \\
\hline
Output and Income (16) & 0.500 & 0.438 & 0.125 \\
Consumption, Orders, and Inventories (9) & 1.000 & 0.778 & 0.222 \\
Labor Market (31) & 0.774 & 0.677 & 0.419 \\
Housing (10) & 0.000 & 0.000 & 0.000 \\
Money and Credit (12) & 0.250 & 0.167 & 0.083 \\
Stock Market (3) & 0.333 & 0.000 & 0.000 \\
Interest and Exchange Rates (19) & 0.789 & 0.737 & 0.474 \\
Prices (20) & 0.800 & 0.650 & 0.450 \\
\hline\hline
\end{tabular}
\caption{Rejection ratios for each category of the FRED-MD dataset over the sample period 1980 January to 2019 December. In parentheses, we report the number of series per category. Data source: \cite{mccracken2016fred}. \label{tab.fredmdreject}}
\end{table}


\subsection{Finance application I}\label{finance-application-i}

In our second application, we test the sparse component of a financial dataset comprising a set of regressor variables shown to predict market excess returns (\citealp{dong2022anomalies}). Specifically, we examine whether the idiosyncratic sparse component is important in explaining market excess returns as well as returns of 49 industry portfolios when regressing on a representative sample of 100 long-short anomaly portfolio returns. The data spans from January 1970 to December 2017, therefore, the effective sample size is $T=575$. We estimate the same model as in \eqref{eq:model1}.

\smallskip

\cite{dong2022anomalies} have shown that these regressors lead to accurate forecasts of stock market excess return by employing various machine learning and forecast combination methods. For more details on the data and a full list of target variables, see Section  \ref{OA-app.sec.fin1} of the Online Appendix. Results are reported in Table \ref{tab:finapp1}.

\smallskip

Our results indicate that for more than 50\% of 49 industry returns, the idiosyncratic shocks are significant at the 10\% level, meaning we reject the null hypothesis at the 10\% level. For the aggregate market returns, the p-value is 0.042, providing evidence in favor of the sparse component. Additionally, we reject the null at the 1\% significance level for nine industry portfolios: \textit{Apparel} (Clths), \textit{Automobiles and Trucks} (Autos), \textit{Aircraft} (Aero), \textit{Rubber and Plastic Products} (Rubbr), \textit{Construction Materials} (BldMt), \textit{Real Estate} (RlEst), \textit{Printing and Publishing} (Books), \textit{Non-Metallic and Industrial Metal Mining} (Mines), and \textit{Business Supplies} (Paper). Furthermore, we analyze returns of 10 industry portfolios which are based on a broader classification compared to 49 industries. Results are presented in Table \ref{OA-app.tab:finapp1} in the Online  Appendix, which confirm a similar pattern. Therefore, we find strong evidence supporting the presence of sparse idiosyncratic components in equity returns for a variety of industries and the aggregate market.

%\textcolor{red}{I suppose these 10 industries correspond to a less fine classification? This should be explicited}


\begin{table}
\centering\footnotesize
\begin{tabular}{cc|cc|cc|cc|cc}
\textbf{Market}$^{**}$ & 0.042 & \textbf{Clths}$^{***}$ & 0.001 & FabPr & 0.272 & \textbf{Oil}$^{**}$ & 0.014 & \textbf{Boxes}$^{**}$ &
0.015 \\
Agric & 0.184 & Hlth & 0.500 & \textbf{Mach}$^{**}$ & 0.012 & Util & 0.252 & \textbf{Trans}$^{**}$ &
0.024 \\
\textbf{Food}$^{**}$ & 0.012 & MedEq & 0.198 & \textbf{ElcEq}$^{**}$ & 0.034 & Telcm & 0.625 & Whlsl &
0.117 \\
Soda & 0.129 & Drugs & 0.286 & \textbf{Autos}$^{***}$  & 0.007 & \textbf{PerSv}$^{**}$  & 0.043 & \textbf{Rtail}$^{*}$  &
0.099 \\
Beer & 0.150 & \textbf{Chems}$^{**}$  & 0.014 & \textbf{Aero}$^{***}$  & 0.006 &  \textbf{BusSv}$^{*}$ & 0.051 & \textbf{Meals}$^{**}$  &
0.043 \\
Smoke & 0.130 & \textbf{Rubbr}$^{***}$  & 0.002 & \textbf{Ships}$^{**}$  & 0.034 & Hardw & 0.636 & \textbf{Banks}$^{**}$  &
0.011 \\
Toys & 0.205 & \textbf{Txtls}$^{**}$  & 0.023 & Guns & 0.173 & Softw & 0.206 & \textbf{Insur}$^{**}$  &
0.024 \\
\textbf{Fun}$^{*}$ & 0.057 & \textbf{BldMt}$^{***}$  & 0.004 & Gold & 0.504 & Chips & 0.646 & \textbf{RlEst}$^{***}$  &
0.003 \\
\textbf{Books}$^{***}$  & 0.001 & Cnstr & 0.123 & \textbf{Mines}$^{***}$  & 0.005 & LabEq & 0.618 & Fin &
0.195 \\
Hshld & 0.189 & Steel & 0.278 & Coal & 0.268 & \textbf{Paper}$^{***}$  & 0.001 & \textbf{Other}$^{**}$  &
0.010 \\
\hline\hline
\end{tabular}
\caption{p-Values of market and industry excess returns regressed on 100 long-short anomaly characteristics. Bold entries with $^{*}$, $^{**}$, and $^{***}$ indicate significance at 10\%, 5\% and 1\% significance level, respectively. \label{tab:finapp1}}
\end{table}

\subsection{Finance application II}\label{finance-application-ii}

In the third empirical application, we examine the sparse idiosyncratic components of individual stock returns using the dataset from \cite{jensen2023there}. Specifically, we use monthly stock return data for a sample of 728 firms with no missing data from January 1991 to December 2022, resulting in $T=384$ and  $p=727$ (the regressors $x_{it}$ are the returns of other firms). We test a model similar to \cite{fan2021bridging} where our test can be seen as a diagnostic check after the third step in the approach put forward by \cite{fan2021bridging}. However, our analysis diverges from \cite{fan2021bridging} by focusing on a balanced panel of firms. We select firms for which we have a complete time series of returns and observed regressors. Furthermore, in our industry or sector analyses, we group firms into 49 industries — the same as in finance I application —, whereas \cite{fan2021bridging} use a different firm classification.



%Lastly, they focus on analyzing variable selection of the LASSO regression step. Instead, we report firm-specific rejection ratios of the sparse idiosyncratic components based on the LASSO regression.

\smallskip

%Specifically, denote $\tilde{y}_{it}$ the \textit{defactored} stock return of firm $i$ at time $t$. By defactored, we mean that we first regress the firm's excess return on a set of observable factors — see Table \ref{OA-tab:finapp2.obs} for the list of factors — and denote $\tilde{y}_{it}$ as the remaining returns. We then test the idiosyncratic component of each stock return $\tilde{y}_{it}$ regressed on the remaining returns $\tilde{x}_{-it} = (\tilde{y}_{jt})_{j\in[n]/i}$, where $n$ is the total number of firms. We report the rejection ratios by testing the following model for each firm $i$:

Specifically, denote $y^{(i)}_{t}$ the stock excess return of firm $i$ at time $t$. We regress the firm's excess return on a set of observable factors — see Table \ref{OA-tab:finapp2.obs} for the list of factors — as well as common and idiosyncratic components stemming from all other returns. The model is thus an example of the extension of the main model with additional covariates, see Online Appendix \ref{app.modelw}. Denoting the returns of all firms except the $i^{th}$ firm as $x_{it} = (y_{jt})_{j\in[n]/i}$, where $n$ is the total number of firms, we report the rejection ratios $\beta^*$ by testing $\beta^*=0$ in the following model for each firm $i$:
\begin{equation}\label{model-firm-rets}
\begin{aligned}
y^{(i)}_{t}&= f_{t}^\top \gamma^*+ w_t^\top\delta^* + u_{it}^\top\beta^*+\varepsilon_{t},\\
x_{it}&=B f_{it}+ u_{it},\quad t=1\dots,T,
\end{aligned}
\end{equation}
where $w_t$ denotes the observable factors.

Results are reported in Table \ref{tab:finapp2}. First, we see that the rejection ratios (the proportion of firms $i$ for which we reject the null) are relatively low and only slightly above the nominal significance levels. This suggests that the sparse component is much less significant in individual stock returns compared to other applications we considered, i.e., we find weak evidence of the presence of sparse idiosyncratic shocks in firm-level returns. To gain further insight, we report results by grouping firms into 49 industries, as reported in Table \ref{OA-tab:finapp2.res.ind} in the Online Appendix. We find that for most industries, the rejection ratios are small. However, a few industries exhibit a larger proportion of rejections, notably \textit{Precious Metals} (Gold) and \textit{Communications} (Telcm). 

\begin{table}[ht]
\centering
\begin{tabular}{rrrr}
& 10 \% & 5 \% & 1 \% \\
Rejection rates & 0.130 & 0.082 & 0.038 \\
\hline\hline
\end{tabular}
\caption{Rejection rates for the firm-level financial returns dataset. \label{tab:finapp2}}
\end{table}




\section{Conclusion}\label{sec.ccl}

This paper proposes a new bootstrap test for the adequacy of the factor regression model against factor-augmented sparse alternatives. We establish the asymptotic validity of our test under time series dependence and polynomial tails. In a Monte Carlo study, we show that our procedure has excellent finite sample properties against sparse alternatives but low power against dense alternatives. We often reject the null when we apply our testing procedure to standard datasets in macroeconomics and finance. This suggests that sparsity is present - on top of a dense model - in several economic environments.

This message complements previous studies. Indeed, based on different approaches, \cite{giannone2021economic,kolesar2023fragility} found evidence against the presence of sparsity in several economic datasets, comparing only sparse and only dense models. Our analysis instead suggests that sparsity may still have a role to play on top of a dense component. These findings also constitute arguments in favor of using sparse plus dense models, which have recently gathered a lot of interest in the econometrics literature.

A potential limitation of our analysis is that we may reject $H_0$ because $\beta^*$ is nonzero but dense. In our Monte-Carlo simulations, we compare DGPs with sparse and dense $\beta^*$ with the same signal-to-noise ratio and find that our test has high power against the sparse alternative but low power against dense deviations from the null. This finding should mitigate the previous concern that rejection might be due to dense $ \beta^*\ne 0$. A complementary approach free of this concern is testing sparsity directly. \cite{kolesar2023fragility} takes this road by proposing a test for the null hypothesis of sparsity in a high-dimensional regression model (without factors). The limitations of their approach are that it is only valid when the number of variables is smaller than the sample size and that it does not reject the null when the regression coefficient is equal to $0$. This is a problem because, in our context, we would not conclude in favor of the existence of sparsity when $\beta^*=0$. For future research, it would be interesting to adapt the test of \cite{kolesar2023fragility} to the factor-augmented regression model and apply it to the datasets we consider in the present paper.
\appendix 
\renewcommand*{\thesection}{\Alph{section}}

\section*{Appendix: extension to additional regressors}
	
	
\section{A model with additional regressors}\label{app.modelw}
As in \cite{stock2002forecasting,bai2006confidence, lederer2021estimating}, in empirical applications we can augment the model with additional observed low-dimensional regressors $w_1,\dots,w_t\in\R^\ell$ (where $\ell$ is fixed with $T$).  We therefore can consider the alternative model.
	\begin{equation}\label{model-alternative}
		\begin{aligned}
			y_t&=f_t^\top \gamma^*+w_t^\top\delta^*+u_t^\top\beta^*+\varepsilon_t,\\
			x_t&=Bf_t+u_t,\quad t=1\dots,T,
		\end{aligned}
	\end{equation}
	Here, again, $\varepsilon_t\in \R$ represents a random error, $u_t$ is a $p$-dimensional random vector of idiosyncratic shocks, $f_t$ is a $K$-dimensional random vector of factors, and $B$ is a $p\times K$ random matrix of loadings. The parameters are $\gamma^*\in \R^K$, $\delta^*\in\R^\ell$, $\beta^*\in \R^p$. Note that here $w_t$ plays the role of an observed factor (with loading equal to $0$). This will be key to understanding the alternative testing procedure of Section \ref{subsec.test-alternative}.
	
	We focus on testing
	\begin{equation}\label{Htest-alternative}
		H_0:\beta^*=0\quad \text{against}\quad H_1:\beta^*\ne 0.
	\end{equation}
	To facilitate understanding, we again rewrite the model in matrix form as follows:
	\begin{equation*}
		\begin{aligned}
			Y&=F^\top \gamma^*+W\delta^*+ U^\top\beta^*+\mathcal{E},\\
			X&=BF+U,
		\end{aligned}
	\end{equation*}
	where $Y=(y_1,\dots,y_T)^\top$, $F=(f_1,\dots, f_T)^\top$ is a $T\times K$ matrix, $U=(u_1,\dots,u_T)^\top$, $W=(w_1,\dots,w_T)^\top$ and $X=(x_1,\dots,x_T)^\top$ are $T\times p$ matrices and $\mathcal{E}=(\varepsilon_1,\dots,\varepsilon_T)^\top.$\\
	
	\section{Testing procedure of the extended model} \label{subsec.test-alternative}
	Algorithm \ref{algo-alternative} present the test in this extended model. It is similar to Algorithm \ref{algo}. The only difference is that $\widehat{P}$ is now the projector on the columns of the $T\times (\widehat{K}+\ell)$ matrix $(\widehat{F}\ W)$ in Step 3. Essentially, $w_t$ is treated as an observed factor. 
	\begin{algorithm}
		\singlerule\vspace{0.1cm}
		\begin{algorithmic}
			\STATE \textbf{1.} Estimate $\widehat{K}$ by one of the available estimators of the number of factors.
			\STATE \textbf{2.} Let the columns of $\widehat{F}/\sqrt{T}$ be the eigenvectors corresponding to the leading $\widehat{K}$ eigenvalues of $XX^\top$.
			\STATE \textbf{3.} Compute $\widehat{U}=\left(I_T-\widehat{P}\right) X$ and $\widetilde{Y}=\left(I_T-\widehat{P}\right)Y$, where $\widehat{P}$ is the projector on the columns of the $T\times \left(\widehat{K}+\ell\right)$ matrix $\left(\widehat{F}\ W\right)$. Denote by $\widehat{u}_t$ the $T\times 1$ vector corresponding to the transpose of the $t^{th}$ row of $\widehat{U}$. 
			\STATE \textbf{4.} Calculate an approximation $\widehat{\lambda}_{\alpha,emp}$ of $\widehat{\lambda}_\alpha$ as follows:
			\STATE \begin{itemize}
				\item[\textbf{4a.}] Specify a grid $0<\lambda_1<\dots<\lambda_M<\bar{\lambda}$, with $\bar{\lambda}=2T^{-1}\left\|\widehat{U}^\top\widetilde{Y}\right\|_\infty$.
				\item[\textbf{4b.}] For $\lambda>0$, and $e\in\R^T$, let $\widehat{Q}\left(\lambda,e\right)=\left\|\frac2T \sum_{t=1}^T\widehat{u}_t\widehat{\varepsilon}_{\lambda,t} e_t,\right\|_\infty$, where $\widehat{\varepsilon}_{\lambda,t}=\widetilde{y}_t -\widehat{u}_t^\top\widehat{\beta}_{\lambda},\ t\in[T]$, for $		\widehat{\beta}_\lambda = \argmin_{\beta\in\R^p}\frac{1}{T}\left\|\widetilde{Y}-\widehat{U}\beta\right\|_2^2+\lambda\|\beta\|_1$. For $m\in[M]$, compute $\left\{\widehat{Q}\left(\lambda_m,e^{(\ell)}\right):\ \ell\in[L]\right\}$ for $L$ draws of $e\sim\mathcal{N}(0,I_T)$ and the corresponding empirical $(1-\alpha)$-quantile $\widehat{q}_{\alpha,emp}(\lambda_m)$ from them.
				\item[\textbf{4c.}] Let $\widehat{\lambda}_{\alpha,emp}=\widehat{q}_{\alpha,emp}(\lambda_{\widehat{m}})$,  with $\widehat{m}=\min\{m\in[M]:\ \widehat{q}_{\alpha,emp}(\lambda_{m'})\le \lambda_{m'}\text{ for all }m'\ge m\}.$
			\end{itemize}
			\STATE \textbf{5.} Reject $H_0$ when $2T^{-1}\left\|\widehat{U}^\top \widetilde{Y}\right\|_\infty>\widehat{\lambda}_{\alpha,emp}$.
		\end{algorithmic}
		\doublerule
		\caption{Conducting a test of level $\alpha\in(0,1)$ with additional regressors.\label{algo-alternative}}
	\end{algorithm}
	
	\section*{Supplementary material}
	
	\begin{description}
		
		\item[Online Appendix:] Additional empirical and simulation results, details on the data and the proof of Theorem \ref{th} (.pdf file). 
		\item[R package:] the R package \texttt{\textquotesingle FAS\textquotesingle}  that implements our test is available on CRAN: \url{https://cran.r-project.org/web/packages/FAS/index.html}.
	\end{description}
	
	\if1\blind
	{
		\section*{Acknowledgments}
		The authors thank Andrii Babii, Otilia Boldea, Peter Boswijk, Geert Dhaene, Art\={u}ras Juodis,  Frank Kleibergen, Lasse Heje Pedersen, seminar participants at Copenhagen Business School and BI Norwegian Business School, and conference attendants at the FinEML conference 2023 at Erasmus University (Rotterdam), the Leuven Statistics Days 2023, the Conference in Financial Econometrics (CFE) 2023 in Berlin, and the Netherlands Econometrics Study Group 2024 in Maastricht University. The authors also thank Michael Vogt for sharing his code for the procedure of \cite{lederer2021estimating}. Jad Beyhum undertook part of this work while employed by CREST, ENSAI (Rennes). Jad Beyhum gratefully acknowledges financial support from the Research Fund KU Leuven through the grant STG/23/014. Jonas Striaukas gratefully acknowledges the financial support from the European Commission, MSCA-2022-PF Individual Fellowship, Project 101103508. Project Acronym: MACROML. 
	} \fi
	
	
	
	\bibliographystyle{agsm}
	\bibliography{bibliography}
	
	
\end{document}