\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{natbib}
\usepackage{array}
\usepackage{algorithmic}
\usepackage[plain]{algorithm}
\newcommand{\alglinelabel}{%
	\addtocounter{ALC@line}{-1}% Reduce line counter by 1
	\refstepcounter{ALC@line}% Increment line counter with reference capability
	\label% Regular \label
}
\newcommand*{\doublerule}{\hrule width \hsize height 0.5pt \kern 0.5mm \hrule width \hsize height 0.5pt}
\newcommand*{\singlerule}{\hrule width \hsize height 0.5pt}
\usepackage[colorlinks = true,
linkcolor = red,
urlcolor  = blue,
filecolor={red},
citecolor = blue,
anchorcolor = blue]{hyperref}
%\usepackage{url} % not crucial - just used below for the URL 
\usepackage{xurl}
%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{1}
\usepackage{mathtools}
\usepackage{etoolbox}
% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-1in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.7in}%
\addtolength{\topmargin}{-1in}%

\usepackage{xcolor}

\DeclareMathOperator*{\argmin}{arg\,min}%\usepackage[latin 1]{inputenc}
\usepackage{caption}\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}\usepackage{graphicx,latexsym,amssymb,amsmath}
\usepackage{subcaption}
\usepackage{bbm, bm}
\def\pp{\vskip 10pt}
\def\ppn{\vskip 10pt \noindent }
\def\d{{\mathrm{d}}}
\def\R{{\mathbb{R}}}
\def\Z{{\mathbb{Z}}}
\def\C{{\mathbb{C}}}
\def\N{{\mathbb{T}}}
\def\T{{\mathbb{T}}}
\def\P{{\mathbb{P}}}
\def\E{{\mathbb{E}}}
\usepackage{newfloat}
%\DeclareFloatingEnvironment[
%fileext=los,
%listname={List of Schemes},
%name=Algorithm,
%placement=tbhp,
%]{algorithm}
\newtheorem{Lemma}{Lemma} %numbering by section
\newtheorem{Assumption}{Assumption}
\newtheorem{Theorem}{Theorem}%numbering by section
\newtheorem{Proposition}{Proposition}
\newtheorem{Remark}{Remark}[section]
\newtheorem{Corollary}{Corollary}

\begin{document}

%\bibliographystyle{natbib}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\if1\blind
{
  \title{\bf Tuning-free testing of factor regression against factor-augmented sparse alternatives}
  \author{Jad Beyhum\hspace{.2cm}\\
     Department of Economics, KU Leuven, Belgium\\
    and \\
    Jonas Striaukas \\
    Department of Finance, Copenhagen Business School, Denmark}
  \maketitle
} \fi

\if0\blind
{
  \bigskip
  \bigskip
  \bigskip
  \begin{center}
    {\LARGE\bf Tuning-free testing of factor regression against factor-augmented sparse alternatives}
\end{center}
  \medskip
} \fi

\bigskip
\begin{abstract}
This study introduces a bootstrap test of the validity of factor regression within a high-dimensional factor-augmented sparse regression model that integrates factor and sparse regression techniques. The test provides a means to assess the suitability of the classical dense factor regression model compared to a sparse plus dense alternative augmenting factor regression with idiosyncratic shocks. Our proposed test does not require tuning parameters, eliminates the need to estimate covariance matrices, and offers simplicity in implementation. The validity of the test is theoretically established under time-series dependence. Through simulation experiments, we demonstrate the favorable finite sample performance of our procedure. Moreover, using the \texttt{FRED-MD} dataset, we apply the test and reject the adequacy of the classical factor regression model when the dependent variable is inflation but not when it is industrial production. These findings offer insights into selecting appropriate models for high-dimensional datasets.
\end{abstract}

\noindent%
{\it Keywords:}  sparse plus dense, high-dimensional inference, LASSO, factor models %3 to 6 keywords, that do not appear in the title
\vfill

\newpage
\spacingset{1.5} % DON'T change the spacing!


\section{Introduction}
In this paper, we investigate a factor-augmented sparse regression model. Our analysis involves an observed sample of $T$ real-valued outcomes $y_1,\dots,y_T$, and high-dimensional regressors $x_1,\dots,x_T\in\R^p$, which are interconnected as follows:
\begin{equation}\label{model}
	\begin{aligned}
		y_t&=f_t^\top \gamma^*+u_t^\top\beta^*+\varepsilon_t,\\
		x_t&=Bf_t+u_t,\quad t=1\dots,T.
	\end{aligned}
\end{equation}
 Here, $\varepsilon_t\in \R$ represents a random error, $u_t$ is a $p$-dimensional random vector of idiosyncratic shocks, $f_t$ is a $K$-dimensional random vector of factors, and $B$ is a $p\times K$ random matrix of loadings. The parameters of interest are $\gamma^*\in \R^K$ and $\beta^*\in \R^p$ and the right-hand side of \eqref{model} is unobserved. We consider the case where the number $p$ of regressors is large with respect to the sample size $T$ and sparsity conditions on the high-dimensional parameter vector $\beta^*$ are imposed. The model formulation in equation \eqref{model} effectively merges two popular approaches in handling high-dimensional datasets: factor regression (\cite{stock2002forecasting,bai2006confidence}) and sparse high-dimensional regression (\cite{tibshirani1996regression,bickel2009simultaneous}).
%\footnote{\label{foot.sparse}To further clarify the link with sparse regression, note that the first equation of \eqref{model} can be rewritten $y_t=f_t^\top\varphi^*+x_t^\top\beta^*+\varepsilon_t$, where $\varphi^*=\gamma^*-B^\top \beta^*$, which becomes a usual high-dimensional sparse regression model when $\varphi^*=0$.}.
Such a model allows the outcome to be related to the regressors through both common and idiosyncratic shocks and may better explain the data than factor regression or sparse regression alone (see \cite{fan2023latent}, which introduces and studies model \eqref{model}). Note that, as in \cite{stock2002forecasting,bai2006confidence}, we could augment the model \eqref{model} with additional regressors $w_t$ entering the first equation of \eqref{model} but not the second one. This case is discussed in the Appendix. 

We develop a test for the hypothesis:
\begin{equation}\label{Htest}
	H_0:\beta^*=0\quad \text{against}\quad H_1:\beta^*\ne 0 \text{ is sparse},
\end{equation}
where our theory outlines  the set of sparse alternatives against which our test has power. 
Our test has two main applications. First, it can be seen as a mean to assess the suitability of the classical factor regression model in comparison to factor-augmented sparse regression alternatives. It provides guidance on the choice between these two models in practical applications. In particular, it allows to infer if forecasting can be improved by using factor-augmented sparse regression instead of only factor regression.\footnote{A traditional approach to testing equal forecasting accuracy in nested models is to use mean-squared error based bootstrap tests (\cite{clark2001tests,clark2007approximately}) Although such tests have been shown to work under certain conditions when the factors are estimated by principal components analysis (\cite{gonccalves2017tests}) or common correlated effects (\cite{stauskas2022tests}), it is unclear if they are still valid when the LASSO estimator is also used.} Second, our test also sheds light on the data generating process by allowing us to determine if the underlying model is dense (as is the factor regression model) or sparse plus dense (as is the factor-augmented sparse regression model). This determination will then tell us if the relation between the regressors and the outcome is only driven by common shocks (factor regression) or if idiosyncratic shocks play a role as well (factor-augmented sparse regression). The question of the adequacy of sparse or dense representations has recently garnered significant attention (see, e.g., \cite{abadie2019choosing, giannone2021economic}). However, existing studies mostly focus on the differences between sparse and dense models, and do not rely on formal frequentist tests. In contrast, we consider hypothesis testing with a sparse plus dense alternative.

\cite{fan2023latent} recently introduced the Factor-Adjusted deBiased Test (FabTest) for evaluating \eqref{Htest}. However, the FabTest exhibits several limitations. The test relies on a desparsified LASSO estimator based on model \eqref{model}. To achieve desparsification, \cite{fan2023latent} utilized the nodewise LASSO method proposed by \cite{zhang2014confidence} and \cite{van2014asymptotically} for estimating the precision matrix of the idiosyncratic shocks. However, this approach introduces $p$ additional tuning parameters, in addition to the one used in the original LASSO regression. Although the tuning parameters are selected through cross-validation in practice, \cite{fan2023latent} did not provide a theoretical justification for this selection procedure. Besides, inferential theory for LASSO-type regressions is not well understood when the tuning parameter is selected by cross-validation. Moreover, the test's performance may deteriorate due to errors associated with the nodewise LASSO estimates, and it incurs a heavy computational cost. Another limitation of the FabTest is its reliance on estimating the variance of $\varepsilon_t$, which can lead to imprecise results where variance estimation is challenging. Additionally, \cite{fan2023latent} only established the validity of the FabTest for i.i.d. sub-Gaussian data.\footnote{See Section 2 in \cite{fan2023latent}.}

In this paper, we propose a new bootstrap test for \eqref{Htest} that overcomes the limitations of the previously mentioned FabTest. Our proposed test does not require tuning parameters or the estimation of variance or covariance matrices, making it easy to implement. We establish the validity of the test within a theoretical framework that accommodates scenarios where the number of variables, denoted by $p$, can significantly exceed $T$, the explanatory variables exhibit strong mixing and possess exponential tails. In simulations, our procedure shows improvement over the FabTest and demonstrates favorable performance. Furthermore, we apply our test to regression exercises using the \texttt{FRED-MD} dataset (\cite{mccracken2016fred}). We reject the validity of the classical factor regression model to explain inflation but do not find evidence against the suitability of factor regression when the outcome is industrial production. In the Appendix, we explain how to adapt our test to the case where the model includes additional regressors $w_t$ entering the first equation of \eqref{model}.

This work is related to a recent literature considering testing for high-dimensional parameters. There exists several approaches, see \cite{fan2015power,zhu2018linear,chernozhukov2019inference,lederer2021estimating,he2023most} and references therein. These procedures differ in terms of the type of alternative hypotheses they consider: sparse, dense, or general. In the present paper, we are interested in sparse alternatives because we want to infer either superior predictive accuracy of the factor-augmented sparse regression or the presence of sparsity. Under dense factor-augmented regression models, the use of the RIDGE estimator is  advisable (\cite{he2023ridge}). In a generalization of our model, \cite{fan2021bridging} proposes a test for $H_0$ similar to that of \cite{chernozhukov2019inference} against general hypotheses. Tests against general alternatives do not take advantage of sparsity assumptions and, under sparse alternatives, they should therefore exhibit lower power than tests relying on sparsity. This is a reason to rely on methods, such as ours, which use sparsity. Our strategy draws inspiration from \cite{lederer2021estimating}, a recent paper that introduces a bootstrap procedure for selecting the penalty parameter of LASSO in standard sparse linear regression. They employ this procedure to test the null hypothesis that a specific high-dimensional parameter is equal to zero. We adapt their approach to the case with unobserved factors, which poses a challenge beyond the scope of the results in \cite{lederer2021estimating}. In our case, the unobserved factors need to be estimated, indicating that they act as generated regressors.\footnote{Note that, again adapting \cite{lederer2021estimating}, we could also devise a procedure to select the penalty parameter of LASSO-type estimators of model \eqref{model}.  We have experimented with such a procedure in Monte Carlo simulations and did not find that this procedure shows significant improvement over traditionally used cross-validation. For this reason, we decided to focus the present paper on the problem of testing \eqref{Htest}, for which simulations yield excellent results.}

Finally, we would like to note that this paper contributes to various other strands of literature. First, it complements papers that combine factor models and sparse regression (\cite{hansen2019factor, fan2023latent, fan2021bridging, vogt2022cce, beyhum2023factor} among others). The proposed strategy allows testing for the joint significance of the coefficients of the idiosyncratic shocks within this framework. Second, our work is related to the literature on inference on parameters of additional low-dimensional regressors in the factor regression model of \cite{stock2002forecasting}, see \cite{bai2006confidence,gonccalves2014bootstrapping,gonccalves2020bootstrapping}. Third, our work connects with the literature on specification tests for models involving factors. Many papers test for the validity of the assumption that loadings are time-independent in the approximate factor model itself — the second equation in \eqref{model} — (\cite{breitung2011testing, chen2014detecting, han2015tests, yamamoto2015testing, su2017time, su2020testing, baltagi2021estimating, xu2022testing, fu2023testing}), while \cite{corradi2014testing} tests for time-independence of all coefficients in the factor regression model of \cite{stock2002forecasting}. Our approach complements this literature by proposing a specification test of the factor regression model under a different alternative, namely the factor-augmented sparse regression model.  \\

%

%In this paper, we propose a new bootstrap test for \eqref{Htest} which avoids the aforementioned shortcomings of the FabTest. The proposed test does neither requires tuning parameters nor the estimation estimation of variance or covariance matrices, and is easy to implement. We establish the validity of the test within a theoretical framework that accommodates scenarios where $p$ can significantly exceed $T$, the variables exhibit strong mixing, and possess exponential tails. Our procedure demonstrates favorable performance in finite sample simulations. Additionally, we apply our test to two prediction exercises using the \texttt{FRED-MD} dataset (\cite{mccracken2016fred}) and reject the hypothesis that the data follows a factor regression model.

%Our strategy is inspired by \cite{lederer2021estimating}. This recent paper proposes a bootstrap procedure for selecting LASSO's tuning parameter of LASSO in standard sparse linear regression, and employs it to test the null hypothesis that a specific high-dimensional parameter is equal to zero. Our testing procedure can be seen as an adaptation of their approach to the case with unobserved factors. This presents a challenge beyond the scope of the results in \cite{lederer2021estimating} because the unobserved factors need to be estimated, indicating that they act as generated regressors. \\

%Before concluding this introduction, we would like to stress that the present paper contributes to several strings of literature. First, this paper makes a valuable contribution to the current literature combining factor models and sparse regression, see \cite{hansen2019factor,fan2023latent,fan2021bridging,vogt2022cce,beyhum2023factor}. We propose a strategy allowing to test the standard factor regression model within this framework. Second, our work is also related to the literature on specification of the factor regression models. Many paper directly test the validity of the factor model itself (the second equation in \eqref{model}), see \cite{breitung2011testing,chen2014detecting,han2015tests,yamamoto2015testing,su2017time,su2020testing,baltagi2021estimating,xu2022testing,fu2023testing}. \cite{corradi2014testing} focuses instead the factor regression model. In all these papers, the alternative hypothesis is that of the presence of structural breaks and/or (smoothly) time-varying loadings. Our approach complements the literature by proposing a specification test of the factor regression model under a different alternative, namely that of the factor-augmented sparse regression model. Third, this paper this paper contributes to the existing body of research on high-dimensional inference. While most studies in this field focus on testing hypotheses related to low-dimensional parameters (\cite{zhang2014confidence,van2014asymptotically} among many others), only a limited number of works address the challenge of hypothesis testing for high-dimensional parameters, as explored in our current paper. Aside from \cite{lederer2021estimating}, \cite{chernozhukov2019inference} introduces a procedure to test multiple moment inequalities, which accommodates dependent data using $\beta$-mixing conditions. We contribute to this literature by testing for a high-dimensional parameter in our specific model with estimated factors.\\

%\noindent \textbf{Related literature.}  Particularly, \cite{fan2023latent} introduced the Factor-Adjusted deBiased Test (FabTest) for evaluating \eqref{Htest}. However, the FabTest exhibits several limitations that this paper aims to address. The test relies on a desparsified LASSO estimator based on model \eqref{model}. To achieve desparsification, \cite{fan2023latent} utilized the nodewise LASSO method proposed by \cite{zhang2014confidence} and \cite{van2014asymptotically} for estimating the precision matrix of the idiosyncratic shocks. Unfortunately, this approach introduces $p$ additional tuning parameters, in addition to the one used in the original LASSO regression. Although the tuning parameters are selected through cross-validation in practice, \cite{fan2023latent} did not provide a theoretical justification for this selection procedure. Moreover, the test's performance may deteriorate due to errors associated with the nodewise LASSO estimates, and it incurs a heavy computational cost. Another limitation of the FabTest is its reliance on estimating the variance $\sigma^2$ for the idiosyncratic shocks, which can lead to imprecise results in time series applications where variance estimation is challenging. Additionally, \cite{fan2023latent} only established the validity of the FabTest for i.i.d. sub-Gaussian data.

%Our work is also related to the literature on specification of the factor regression models. Many paper directly test the validity of the factor model itself (the second equation in \eqref{model}), see \cite{breitung2011testing,chen2014detecting,han2015tests,yamamoto2015testing,su2017time,su2020testing,baltagi2021estimating,xu2022testing,fu2023testing}. \cite{corradi2014testing} focuses instead the factor regression model. In all these papers, the alternative hypothesis is that of the presence of structural breaks and/or (smoothly) time-varying loadings. Our approach departs radically from this line of work because our alternative is that of the factor-augmented sparse regression model.

%Finally, this paper contributes to the literature on high-dimensional inference. Most papers in this literature study how to test hypotheses related to low-dimensional parameters (\cite{zhang2014confidence,van2014asymptotically}). Only few studies consider the problem of hypothesis testing for high-dimensional parameters such as the one tackled in the present paper. \cite{chernozhukov2019inference} proposes a procedure to test many moment inequalities. Their theory allows for dependent data through $\beta$-mixing conditions. Closer to this paper, \cite{lederer2021estimating} proposes a bootstrap procedure to select LASSO's tuning parameter in the standard sparse lienar regression setting and applies it to test the fact that some high-dimensional parameter is $0$. We contribute to this literature by extending \cite{lederer2021estimating}'s procedure to the case where some regressors are estimated factors.


%Finally, this paper contributes to the existing body of research on high-dimensional inference. While most studies in this field focus on testing hypotheses related to low-dimensional parameters (\cite{zhang2014confidence,van2014asymptotically} among many others), only a limited number of works address the challenge of hypothesis testing for high-dimensional parameters, as explored in our current paper. For instance, \cite{chernozhukov2019inference} introduces a procedure to test multiple moment inequalities, which accommodates dependent data using $\beta$-mixing conditions. Another related work is \cite{lederer2021estimating}, which proposes a bootstrap procedure for selecting LASSO's tuning parameter of LASSO in standard sparse linear regression, and employs it to test the null hypothesis that a specific high-dimensional parameter is equal to zero. As mentioned above, building upon this prior research, our contribution lies in extending \cite{lederer2021estimating}'s procedure to incorporate cases where some of the regressors are estimated factors.\\

%Finally, we contribute to the literature on testing for high-dimensional parameters. 







%In this paper, we study a factor-augmented sparse regression model.  We observe a sample of $T$ real-valued outcomes $y_1,\dots,y_T$ and of $p$-dimensional predictors $x_1,\dots,x_T\in\R^p$, which are related in the following way:
%\begin{equation}\label{model}
%\begin{aligned}
%n_t\in \R$ is a random error, $u_t$ is a $p$-dimensional random vector of idiosyncratic shocks,  $f_t$ is a $K$-dimensional random vector of factors, $B$ is a $p\times K$ deterministic matrix of loadings. The parameters are $\gamma^*\in \R^K$ and $\beta^*\in \R^p$. The number of predictors can be very large and potentially greater than $T$. Sparsity conditions are imposed on the high-dimensional parameter $\beta^*$. Model \eqref{model} effectively combines two of the most popular high-dimensional approaches in econometrics: factor regression (\cite{stock2002forecasting,bai2006confidence}) and sparse high-dimensional regression (\cite{tibshirani1996regression}).

%We consider testing
%\begin{equation}\label{Htest} H_0:\beta^*\text{ against } H_1:\beta^*\ne 0.\end{equation}
%This can be seen as a specification test of the classical factor regression model against factor-augmented sparse alternatives. It allows to assess if one should use factor regression or factor-augmented sparse regression in practice. It can also inform the econometrician about the true data generating process, in particular if the underlying model is dense - as is the factor regression model - or sparse plus dense - as is the factor-augmented sparse regression model. This question of the adequacy of sparse or dense representations has recently generated a lot of interest in econometrics, see for instance \cite{abadie2019choosing, giannone2021economic}.

%To develop our test, we adapt the bootstrap procedure of \cite{lederer2021estimating} to our setting with unobserved factors. The proposed test is free of tuning parameters, does not require the estimation of the variance or covariance matrices and is relatively computationally simple. We show the validity of the proposed test in a theoretical framework where $p$ can be much larger than $T$, the variables are strongly mixing and have exponential tails. This is challenging and goes beyong the results of \cite{lederer2021estimating}  because the unobserved factors have to estimated, in other words they are generated regressors. Our procedure exhibits good finite sample performance in simulations. We apply our test and reject the fact that that the data follows a factor regression model in two prediction exercises utilizing the \texttt{FRED-MD} dataset (\cite{mccracken2016fred}).

%- by assuming that $x_t$ follows an approximate factor model, of which the factors $f_t$ explain also the outcome - and sparse regression - by letting the idiosyncratic shocks $u_t$ enter sparsely the regression.




%High-dimensional datasets have become increasingly prevalent across various fields, such as finance, economics, genomics, and social sciences. As the dimensionality of the data increases, the challenges posed by the curse of dimensionality have prompted the development of specific statistical techniques tailored to address these complexities. In the field of econometrics, two prominent approaches have emerged: factor regression and sparse linear regression.

%Factor regression, as demonstrated in studies by \cite{stock2002forecasting,bai2006confidence}, relies on the existence of latent factors, that is unobserved variables that capture the common variation present in the predictors. To estimate these latent factors, techniques like principal component analysis or factor analysis are employed. By incorporating these factors into the regression framework, the model effectively reduces the dimensionality of the predictor space. It is however a dense model in the original space of predictors since most variables matter for estimation of the factors, and, therefore, for prediction.

%On the other hand, sparse linear regression operates under the assumption of sparsity in the regression parameters, as pioneered by  \cite{tibshirani1996regression}. This assumption implies that only a subset of predictors significantly influences the dependent variable. Consequently, the model offers a more concise representation of the data and enhances interpretability by emphasizing the key variables that drive the relationship with the dependent variable.

%Recent research - such as \cite{hansen2019factor,fan2023latent,fan2021bridging} - has introduced new models combining these two common dimension reduction approaches. In this paper, we focus on the factor-augmented sparse regression model of \cite{fan2023latent}.  We observe a sample of $T$ real-valued outcomes $y_1,\dots,y_T$ and of $p$-dimensional predictors $x_1,\dots,x_T\in\R^p$, which are related in the following way:


%Note that the factor-augmented sparse regression model includes both factor regression and sparse regression as special cases by setting either $\beta^*$ or $\varphi^*:=\gamma^*-B\beta^*$ to $0$.  for
%\begin{equation}\label{Htest} H_0:\beta^*\text{ against } H_1:\beta^*\ne 0.\end{equation}
%As noted in \cite{fan2023latent}, the FabTest can be seen as a specification test of the classical factor regression model against factor-augmented sparse alternatives. It allows to assess if one should go beyond factor regression. It can also inform the econometrician about the true data generating process, in particular if the underlying model is dense - as is the factor regression model - or sparse plus dense - as is the factor-augmented sparse regression model. This question of the adequacy of sparse or dense representations has recently generated a lot of interest in econometrics, see for instance \cite{abadie2019choosing, giannone2021economic}.



%The main contribution of the present paper is to propose a tuning-free test for \eqref{Htest}. Our approach is based on the bootstrap test of joint signicance in the high-dimensional regression model developed by \cite{lederer2021estimating}.
%We show under ...



%Add factor-augmented regression literature.



\noindent \textbf{Notation.} For an integer $N\in \mathbb{N}$, let $[N]=\{1,\dots, N\}$. The transpose of a $n_1 \times n_2$ matrix $A$ is written $A^{\top}$. Its $k^{th}$ singular value is $\sigma_k(A)$. Let us also define the Euclidean norm $\left\| A\right\|_2^2=\sum_{i=1}^{n_1} \sum_{j=1}^{n_2}A_{ij}^2$ and the sup-norm $\|A\|_\infty=\max\limits_{i\in[n_1], j\in[n_2]}|A_{ij}|$. The quantity $n_1 \vee n_2$ is the maximum of $n_1$ and $n_2$, $n_1 \wedge  n_2$ is the minimum of $n_1$ and $n_2$. For $N\in \mathbb{N}$, $I_N$ is the identity matrix of size $N\times N$.

\section{The test}
\subsection{Testing procedure}
In this subsection, we explain our testing procedure, which is then summarized in algorithmic form in subsection \ref{subsec.computation}. To facilitate understanding, we rewrite the model in matrix form as follows:
\begin{equation*}
	\begin{aligned}
		Y&=F \gamma^*+U\beta^*+\mathcal{E},\\
		X&=BF^\top+U,
	\end{aligned}
\end{equation*}
where $Y=(y_1,\dots,y_T)^\top$, $F=(f_1,\dots, f_T)^\top$ is a $T\times K$ matrix, $U=(u_1,\dots,u_T)^\top$ and $X=(x_1,\dots,x_T)^\top$ are $T\times p$ matrices and $\mathcal{E}=(\varepsilon_1,\dots,\varepsilon_T)^\top.$\\

It is important to note that, under the null hypothesis $H_0$, we have $U^\top (Y-F\gamma^*)=U^\top\mathcal{E}$. This observation suggests a testing procedure that involves computing an estimate $2T^{-1}\left\|U^\top (Y-F\gamma^*)\right\|_{\infty}$ and comparing it with the (estimated) quantiles of $2T^{-1}\left\|U^\top \mathcal{E}\right\|_{\infty}$.\footnote{We have a factor 2 in front of $T^{-1}\left\|U^\top (Y-F\gamma^*)\right\|_{\infty}$ and $T^{-1}\left\|U^\top \mathcal{E}\right\|_{\infty}$ because $2T^{-1}\left\|U^\top \mathcal{E}\right\|_{\infty}$ is the effective noise of the problem, a natural concept in the literature on the LASSO, see \cite{lederer2021estimating}.}

We can  estimate $U^\top (Y-F\gamma^*)$ by principal components analysis. As in \cite{fan2023latent}, we let the columns of $\widehat{F}/\sqrt{T}$ be the eigenvectors corresponding to the leading $K$ eigenvalues of $XX^\top$ and $\widehat{B}=(\widehat{F}^\top \widehat{F})^{-1}\widehat{F}^\top X=T^{-1} \widehat{F}^\top X.$ When it is unknown, the number of factors $K$ can be estimated by one of the many methods available in the literature (see for instance \cite{bai2002determining,onatski2010determining,ahn2013eigenvalue,bai2019rank,fan2022estimating}).  
Then, we project the data on the orthogonal of the vector space generated by the estimated factors. Let $\widehat{P}=T^{-1} \widehat{F}\left(\widehat{F}^\top \widehat{F}/T\right)^{-1}\widehat{F}^\top=T^{-1} \widehat{F}\widehat{F}^\top$ be the projector on the vector space generated by the columns of $\widehat{F}$. A natural estimate for $U$ is $\widehat{U}=X-\widehat{F}\widehat{B}^\top =\left(I_T-\widehat{P}\right) X$.  Similarly, we let $\widetilde{Y}=\left(I_T-\widehat{P}\right)Y$ be an estimate of $Y-F\gamma^*$. The final estimate of $2T^{-1}\left\|U^\top (Y-F\gamma^*)\right\|_{\infty}$ is our test statistic \begin{equation}\label{test_stat_510}2T^{-1}\left\|\widehat{U}^\top \widetilde{Y}\right\|_{\infty}.\end{equation}


Next, to estimate the quantiles of the distribution of $2T^{-1}\left\|U^\top \mathcal{E}\right\|_{\infty},$ we need an estimate of $\mathcal{E}$. We obtain it through the following LASSO estimator:
\begin{equation}\label{eq:lasso}
	\widehat{\beta}_\lambda = \argmin_{\beta\in\R^p}\frac{1}{T}\left\|\widetilde{Y}-\widehat{U}\beta\right\|_2^2+\lambda\|\beta\|_1,
\end{equation}
where $\lambda>0$ is a penalty parameter, the choice of which will be fully data driven in both theory and practice, making our test tuning-free. For $t\in [T]$, we denote by $\widetilde{y}_t$ the $t^{th}$ element of $\widetilde{Y}$ and $\widehat{u}_t$ as the $T\times 1$ vector corresponding to the $t^{th}$ row of $\widehat{U}$.
For a given $\lambda>0$, let $\widehat{\varepsilon}_{\lambda,t}=\widetilde{y}_t -\widehat{u}_t^\top\beta^*,\ t\in[T]$ be the estimate of $\varepsilon_t$. For a fixed $\alpha \in(0,1)$, we can then estimate $q_\alpha$, the $(1-\alpha)$ quantile of the distribution of $2T^{-1}\|U^\top \mathcal{E}\|_{\infty}$, by the Gaussian multiplier bootstrap. Let $e=(e_1,\dots,e_T)$ be a standard normal random vector independent of the data $(X,Y)$ and define the criterion
$$\widehat{Q}(\lambda,e)=\left\|\frac2T \sum_{t=1}^T\widehat{u}_t\widehat{\varepsilon}_{\lambda,t} e_t,\right\|_\infty.$$
The estimate $\widehat{q}_\alpha(\lambda)$ of $q_\alpha$ is then the $(1-\alpha)$-quantile of the distribution of $\widehat{Q}(\lambda,e)$ given $X$ and $Y$. Formally, $\widehat{q}_\alpha(\lambda)=\inf\left\{q:\P_e(\widehat{Q}(\lambda,e)\le q)\ge 1-\alpha\right\}$, where $\P_e(\cdot)=\P(\cdot|X,Y)$.

The only remaining element to make the test tuning-free is the procedure to select $\lambda$. We adapt the approach of \cite{lederer2021estimating} to our setting. Our choice of $\lambda$ is 
\begin{equation}\label{choicelambda} \widehat{\lambda}_\alpha =\inf\{\lambda>0\ :\  \widehat{q}_\alpha(\lambda')\le \lambda' \text{ for all } \lambda'\ge \lambda\}.\end{equation}
We explain in Section \ref{subsec.computation} how to compute $\widehat{\lambda}_\alpha$ in practice. The infimum in \eqref{choicelambda} exists because for all $\lambda \ge \bar{\lambda}=2T^{-1}\left\|\widehat{U}^\top\widetilde{Y}\right\|_\infty$, it holds that $\widehat{\beta}_\lambda =\widehat{\beta}_{\bar \lambda}=0.$ Moreover, since $\widehat{U}\widehat{\beta}_\lambda$ is a continuous function of $\lambda$, $\widehat{q}_\alpha(\lambda)$ is also continuous in $\lambda$ and the infimum is attained at a point $\widehat{\lambda}_\alpha>0$ such that $q_\alpha\left(\widehat{\lambda}_\alpha\right)= \widehat{\lambda}_\alpha$. Let us recall briefly the heuristics behind the choice of $\lambda$ and refer the reader to \cite{lederer2021estimating} for more details. First, note that when $\lambda$ is close to $q_{\alpha}$, standard convergence bounds for the LASSO suggest that $\widehat{\beta}_\lambda$ is a precise estimate of $\beta^*$, so that $\widehat{\varepsilon}_{\lambda,t}$ is a good estimate of $\varepsilon_t$ and, in turn, $\widehat{q}_\alpha(\lambda)$ is close to $q_\alpha$. Second, when $\lambda$ becomes (much) larger than $q_\alpha$, the error $\widehat{\varepsilon}_{\lambda,t}-\varepsilon_t$ becomes large and dependent of $\widehat{u}_t$, which in turn increases $\widehat{q}_\alpha(\lambda)$ and leads it to be larger than $q_{\alpha}$. We then let our estimator of $q_\alpha$ be $\widehat{\lambda}_\alpha=\widehat{q}_\alpha\left(\widehat{\lambda}_\alpha\right)$.  

The test rejects $H_0$ at the level $\alpha$ when our test statistic given in \eqref{test_stat_510} is larger than the estimate $\widehat{\lambda}_\alpha$ of $q_\alpha$. Therefore, our testing procedure is free of tuning parameters stemming from the LASSO regression in equation \eqref{eq:lasso}. 

\subsection{Computation}\label{subsec.computation}
Algorithm \ref{algo} below explains how to conduct the test in practice. Let us discuss Step 4 of Algorithm \ref{algo} in detail. It approximates $\widehat{\lambda}_\alpha$ as defined in \eqref{choicelambda}. It is advisable to set the grid size $M$ and the number of bootstrap samples $L$ to be as large as possible. As mentioned in \cite{lederer2021estimating}, one can speed up Step 4.2 by computing the LASSO with a warm start along the penalty parameter path. Furthermore, Step 4.3 can be accelerated through parallelization techniques. In our implementation, we use both suggestions which greatly speeds up the computations. We also note that to compute the $p$-value of the test, it suffices to conduct it on a grid of values of $\alpha$ and let the $p$-value be equal to the largest value of $\alpha$ in this grid such that the test of level $\alpha$ rejects $H_0$.
\begin{algorithm}
\singlerule\vspace{0.1cm}
\begin{algorithmic}
  \STATE \textbf{1.} Estimate $\widehat{K}$ by one of the available estimators of the number of factors.
  \STATE \textbf{2.} Let the columns of $\widehat{F}/\sqrt{T}$ be the eigenvectors corresponding to the leading $\widehat{K}$ eigenvalues of $XX^\top$.
   \STATE \textbf{3.} Compute $\widehat{U}=\left(I_T-\widehat{P}\right) X$ and $\widetilde{Y}=\left(I_T-\widehat{P}\right)Y$, where $\widehat{P}=T^{-1} \widehat{F}\widehat{F}^\top$.
   \STATE \textbf{4.} Calculate an approximation $\widehat{\lambda}_{\alpha,emp}$ of $\widehat{\lambda}_\alpha$ as follows:
   \STATE \begin{itemize}
               \item[\textbf{4a.}] Specify a grid $0<\lambda_1<\dots<\lambda_M<\bar{\lambda}$, with $\bar{\lambda}=2T^{-1}\left\|\widehat{U}^\top\widetilde{Y}\right\|_\infty$.
               \item[\textbf{4b.}] For $m\in[M]$ compute $\left\{\widehat{Q}\left(\lambda_m,e^{(\ell)}\right):\ \ell\in[L]\right\}$ for $L$ draws of $e\sim\mathcal{N}(0,I_T)$ and the corresponding empirical $(1-\alpha)$-quantile $\widehat{q}_{\alpha,emp}(\lambda_m)$ from them.
               \item[\textbf{4c.}] Let $\widehat{\lambda}_{\alpha,emp}=\widehat{q}_{\alpha,emp}(\lambda_{\widehat{m}})$,  with $\widehat{m}=\min\{m\in[M]:\ \widehat{q}_{\alpha,emp}(\lambda_{m'})\le \lambda_{m'}\text{ for all }m'\ge m\}.$
            \end{itemize}
   \STATE \textbf{5.} Reject $H_0$ when $2T^{-1}\left\|\widehat{U}^\top \widetilde{Y}\right\|_\infty>\widehat{\lambda}_{\alpha,emp}$.
\end{algorithmic}
\doublerule
\caption{Conducting a test of level $\alpha\in(0,1)$.}
\label{algo}
\end{algorithm}



\noindent 

\section{Asymptotic theory} In this section, we provide the asymptotic properties of the test in a theoretical framework allowing for time series dependence in the factors and the idiosyncratic shocks and exponential tails. We place ourselves in an asymptotic regime where $T$ goes to infinity and $p$ goes to infinity as a function of $T$. The number of factors $K$ is fixed with $T$. It would be possible to let it grow, see, for instance, \cite{beyhum2022factor}. For our theory, as is standard in the literature, we assume that $K$ is known, so that $\widehat{K}=K$. However, our results would remain valid when one uses an estimator $\widehat{K}$ which is equal to $K$ with probability going to $1$ as $T\to\infty$. The distributions of the factors $f_{t}$ and the error terms $\varepsilon_t$ do not depend on $T$, while the distribution of the other variables are allowed to vary with $T$. All the constants we introduce are universal in the sense that they do not vary with the sample size. Our assumptions are significantly weaker than that of \cite{fan2023latent}. We impose the usual identifiability condition for factor models (\cite{bai2003inferential,fan2013large}):
\begin{equation}\label{id} \text{cov}(f_t)=I_K\text{ and $B^\top B$ is diagonal}.\end{equation}

We introduce further notation. The loading $b_{jk}$ corresponds to the $j^{th}$ element of the $k^{th}$ column of $B$. Let also $b_j=(b_{j1},\dots,b_{jk})^\top$. We first state four assumptions similar to the usual ones found in the factor models literature (see e.g. \cite{bai2006confidence,fan2013large}). 
\begin{Assumption}\label{as.load}
	All the eigenvalues of the $K\times K$ matrix $p^{-1} B^\top B$ are bounded away from $0$ and $\infty$ as $p\to \infty$.
\end{Assumption}
\begin{Assumption}\label{as.tail}
	The following holds:
	\begin{enumerate}[\textup{(}i\textup{)}]  
		\item\label{taili} $\left\{u_t,f_t,\varepsilon_t,\sum_{\ell=1}^pu_{t\ell}b_{\ell}\right\}_{t}$ is strictly stationary. Moreover, it holds that $$\E[u_{tj}]=\E[f_{tk}]=\E[u_{tj}f_{tk}]=\E\left[f_{tk}\left(\sum_{\ell=1}^pu_{t\ell}b_{\ell h}\right)\right]=0,$$ \text{ for all } $t\in[T], j\in[p], k,h\in[K]$.
		\item\label{tailii} Let $\Sigma=\E[u_tu_t^\top]$. There exist $\kappa_1,\kappa_2>0$ such that $\sigma_{p}(\Sigma)>\kappa_1$, $\sigma_p(\E\left[\varepsilon_t^2u_tu_t^\top\right])>\kappa_1$, $\left|\E\left[\varepsilon_t^2u_tu_t^\top\right]\right|_\infty<\kappa_2$, $\max_{j\in[p]}\sum_{\ell=1}^p|\Sigma_{j\ell}| <\kappa_2$ and $\min_{j,\ell\in[p]}\left(\E\left[(u_{tj}u_{t\ell})^2\right]-\E[u_{tj}u_{tk}]^2\right)> \kappa_1$.
		\item\label{tailiii} There exist $K_1,\theta_1>0$ such that for any $z>0$, $t\in[T]$, $j\in[p]$ and $k\in[K]$, we have 
		\begin{align*}
			\P\left(|u_{tj}|>z\right)\le \exp\left(-\left(\frac{z}{K_1}\right)^{\theta_1}\right);\\
			\P\left(|f_{tk}|>z\right)\le \exp\left(-\left(\frac{z}{K_1}\right)^{\theta_1}\right);\\
			\P\left(\frac{1}{\sqrt{p}}\left|\sum_{j=1}^p b_{jk} u_{tj}\right|>z\right)\le \exp\left(-\left(\frac{z}{K_1}\right)^{\theta_1}\right);\\
			\P\left(|\varepsilon_{t}|>z\right)\le \exp\left(-\left(\frac{z}{K_1}\right)^{\theta_1}\right).
		\end{align*}
		\item\label{tailiv} $\{u_t\varepsilon_t\}_t$ is uncorrelated across $t$, and, for all $t\in[T], j\in[p], k\in[K]$,
		$$\E[u_{tj}\varepsilon_t]=\E[f_{tk}\varepsilon_t]=\E\left[\varepsilon_t\sum_{\ell=1}^pu_{t\ell}b_{\ell k}\right]=0.$$ 
	\end{enumerate}
\end{Assumption}

\begin{Assumption}\label{as.mixing}
	Let $\alpha$ denote the strong mixing coefficients of $\left\{f_{t},u_{t},\varepsilon_t,\sum_{\ell=1}^pu_{t\ell}b_{\ell}\right\}_t$. There exists $\theta_2>0$ such that $2\theta_1^{-1}+\theta_{2}^{-1}>1$ and $K_2>0$ such that for all  $T\in \mathbb{Z}_+$, we have 
	$$\alpha (T)\le \exp\left(-K_2T^{\theta_2}\right).$$
\end{Assumption}
\begin{Assumption}\label{as.moments}
	There exists $M>0$ such that for all $s,t\in[T]$, we have 
	\begin{enumerate}[\textup{(}i\textup{)}]  
		\item\label{f4i} $\|B\|_\infty< M$ a.s.;
		\item\label{f4ii} $\E\left[p^{-1/2}\left(u_s^\top u_{t}-\E\left[u_s^\top u_t\right]\right)\right]^4<M$;
		\item\label{f4iii} $\max_{j\in[p],k\in[K]}\left|T^{-1} \sum_{t=1}^T\E\left[u_{tj}\left(\sum_{\ell=1}^pu_{t\ell}b_{\ell k}\right)\right]\right|   <M.$
	\end{enumerate}
\end{Assumption}
Assumption \ref{as.load} combined with the identifiability condition \eqref{id} constitutes a strong factor assumption (\cite{bai2003inferential}). Assumption \ref{as.tail} restricts the moments and the tail behavior of the variables. Assumption \ref{as.tail} \eqref{taili},\eqref{tailii},\eqref{tailiv} contains conditions on the moments of the different variables similar to that of the literature (\cite{bai2006confidence,fan2013large}). We assume that the variables in Assumption \ref{as.tail} \eqref{tailiii} have exponential tails with common parameter $\theta_1$. It would be possible to have a different tail parameter for each variable or to allow for polynomial tails, but we avoid doing so to simplify our presentation. In the similar context of inference on factor regression models, Assumption E.2. in  \cite{bai2006confidence} and Assumption 7 in \cite{gonccalves2014bootstrapping} impose conditions analogous to the restriction that $\{u_t\varepsilon_t\}_t$ is uncorrelated across $t$ in Assumption \ref{as.tail} \eqref{tailiv}. A sufficient condition for the latter restriction is that $\{\varepsilon_t\}_t$ is uncorrelated across $t$ and independent of $\{u_t\}_t$ . Note that this assumption could be avoided by using a block bootstrap method, but this would complicate the test and may not be justified in a setting where the factors might capture most of the serial correlation.
Assumption \ref{as.mixing} means that $\left\{f_{t},u_{t},\varepsilon_t,\sum_{\ell=1}^pu_{t\ell}b_{\ell h}\right\}_t$ are strongly mixing, which is a restriction on the time-series dependence of the variables. Next, Assumption \ref{as.moments} \eqref{f4i},\eqref{f4ii} is found in \cite{fan2013large} and contains a boundedness condition \eqref{f4i} and a moment condition \eqref{f4ii} on both the time-series and the cross-section
dependence of the idiosyncratic shocks. Finally, Assumption \ref{as.moments} \eqref{f4iii} is a restriction on the cross-section correlation of the idiosyncratic shocks. This condition holds, for instance, if $\{u_t\}_t$ and $\{b_\ell\}_\ell$ are independent and $\max_{j\in[p]}\sum_{\ell=1}^p|\Sigma_{j\ell}| =O(1)$ (which is imposed in Assumption \ref{as.tail} \eqref{tailiii}).


Let us introduce $\theta^{-1}=2\theta_1^{-1}+ \theta_2^{-1}$, $\tau =12+4\theta_2+\frac{4}{\theta}+\frac{4}{\theta_2}$ and $\varphi^*= \gamma^*-B^\top \beta^*$ . To interpret $\varphi^*$, note that the first equation of \eqref{model} can be rewritten $y_t=f_t^\top\varphi^*+x_t^\top\beta^*+\varepsilon_t$, which becomes a usual high-dimensional sparse regression model when $\varphi^*=0$. Note $\varphi^*$ is random and, therefore, we use probabilistic notation when stating bounds on its norm.
The next assumption concerns the relative growth rate of $T$ and $p$. 
\begin{Assumption}\label{as.Rates}
	The following holds:
	\begin{enumerate}[\textup{(}i\textup{)}]  
		\item\label{rateii} $\sqrt{\frac{\log(T\vee p)^{\tau}}{T}}(\|\beta^*\|_1\vee 1)=o(1);$
		\item\label{rateiii} $\log(T\vee p)^{5/2}\frac{\sqrt{T}}{T\wedge p}(\|\varphi^*\|_2\vee 1)=o_P(1).$
	\end{enumerate}
\end{Assumption}
Assumption \ref{as.Rates} \eqref{rateii}  contains sparsity restrictions on the alternative hypotheses. When $\|\beta\|_\infty=O(1)$, condition \eqref{rateii} corresponds, up to logarithmic factors, to the standard consistency condition for the LASSO with bounded regressors and error with sub-Gaussian tails that is $\sqrt{\log(p)/T}(s_0\vee 1)=o(1)$, where $s_0$ is the number of nonzero coefficients of $\beta^*$. Our condition is slightly  stronger because of the fact that the factors have to be estimated, the variables have exponential tails and are strongly mixing. Condition \eqref{rateiii} is a slightly more restrictive version of the standard condition that $\sqrt{T}/(T\wedge p)=o(1)$ for inference in the factor regression model.\footnote{This condition is equivalently stated as $\sqrt{T}/p=o(1)$ in \cite{bai2006confidence,corradi2014testing} and many others.} Indeed, since $\varphi^*$ is of size $K$, it is reasonable to assume that $\|\varphi^*\|_2=O_P(1)$. Under this condition, \eqref{rateiii} corresponds to $\sqrt{T}/(T\wedge p)=o(1)$ up to logarithmic factors. Additionally, it is worth noting that our proofs reveal that Assumption \ref{as.Rates} is stronger than necessary, and the validity of the test could be established under more complex but weaker rate conditions. However, for the sake of clarity, we present Assumption \ref{as.Rates} instead of a more intricate condition.

We have the following theorem. 
\begin{Theorem}\label{th}Let Assumptions \ref{as.load}, \ref{as.tail}, \ref{as.mixing}, \ref{as.moments} and \ref{as.Rates} hold. For all $\alpha \in (0,1)$, we have 
	\begin{enumerate}[\textup{(}i\textup{)}] 
		\item\label{thi} If $\beta^*=0$, then $\P\left(T^{-1}\left\|\widehat{U}^\top \widetilde{Y}\right\|_\infty> \widehat{\lambda}_\alpha\right)\le \alpha+o(1).$
		\item\label{thii} If $\sqrt{\frac{\log(T\vee p)}{T\wedge p}}=o_P\left(T^{-1}\left\|U^\top U\beta^*\right\|_\infty\right) $, then $\P\left(T^{-1}\left\|\widehat{U}^\top \widetilde{Y}\right\|_\infty> \widehat{\lambda}_\alpha\right)\to 1$.
	\end{enumerate}
\end{Theorem}
The proof of Theorem \ref{th} can be found in Online Appendix B. Statement \eqref{thi} means that the empirical size of the test tends to the nominal size. Statement \eqref{thii} shows that the test has asymptotic power equal to $1$ against sequences of alternatives such that $\sqrt{\frac{\log(T\vee p)}{T\wedge p}}=o_P\left(T^{-1}\left\|U^\top U\beta^*\right\|_\infty\right) $. As noted in \cite{lederer2021estimating}, such a condition is inevitable because the presence of the error $\varepsilon_t$ prevents us from distinguishing true $U\beta^*$ and $\varepsilon_t$ when $U\beta^*$ is too small.
\section{Simulations}
In this section, we provide a Monte Carlo study which sheds light on the finite sample performance of our proposed testing procedure. We generate samples with $T=100$ observations, $p=100$ variables and $K=2$ factors.  The loadings are such that $b_{jk}\sim\mathcal{U}[-1,1], j\in[p], k\in[K]$. The factors are generated as $f_t=\rho_f f_{t-1}+ \tilde f_t$ for $t=2,\dots, T$, where $\tilde f_t$ are i.i.d. $\mathcal{N}\left(0,I_K\left(1-\rho_f^2\right)\right)$. The idiosyncratic components $\{u_t\}$ are such that $u_t=\rho_{u}u_{t-1}+\tilde u_t $ for $t=2,\dots, T$, where $\tilde u_t$ are i.i.d. $\mathcal{N}\left(0,\Sigma\left(1-\rho_{u}^2\right)\right)$, with $\Sigma_{ij}=0.6^{|i-j|},i,j\in[p]$.  We also let $\varepsilon_t=\rho_{e} \varepsilon_{t-1}+\tilde \varepsilon_t $ for $t=2,\dots, T$, where $\tilde \varepsilon_t$ are $\mathcal{N}\left(0,\left(1-\rho_{e}^2\right)\right)$.

The parameters $\rho_f$, $\rho_{u}$ and $\rho_e$ control the level of time series dependence. The stationary distributions of $f_t$, $u_t$, $\varepsilon_t$ are, respectively, $\mathcal{N}(0,I_K)$, $\mathcal{N}(0,\Sigma)$ and $\mathcal{N}(0,1)$. We initialize $f_0$, $u_0$ and $\varepsilon_0$ as such.  We consider three dependency designs:
\begin{itemize}
	\item[]\textit{Design 1.} $\rho_f=\rho_{u}=\rho_e=0$, so that the data are i.i.d. across $t$.
	\item[]\textit{Design 2.} $\rho_f=0.6$, $\rho_{u}=0.1$ and $\rho_e=0$, which introduces time series dependence in the factors and the idiosyncratric shocks.
		\item[]\textit{Design 3.} $\rho_f=0.6$ and $\rho_{u}=\rho_e=0.1$, where there is time series dependence in the factors, the idiosyncratric shocks and the error terms.
\end{itemize}
The third design is not formally allowed in our theory but we want to show that our test performs well even under weak serial correlation of $\{\varepsilon_t\}_t$. 

Finally, we set $\beta^*=(1,0.5,\dots)^\top\times m$, where $m\in\{0,0.1,0.2,0.3,0.4\}$ controls signal strength and $\gamma^*=(0.5,0.5)^\top$.   

We compute the rejection probabilities of our test and the FabTest of \cite{fan2023latent} at the levels $\alpha\in\{0.1,0.05,0.01\}$ over $2000$ replications. For our test, we set $M=200$ and choose an equidistant grid of values of $\lambda$. We use $L=200$ bootstrap replications. The results are insensitive to the choice of $L$ and $M$ as long as they are large enough. This is to be expected since their only role is in the approximation of theoretical quantities. In our experience, $L=M=100$ yields already very precise results. The number of factors $K$ is estimated through the eigenvalue ratio estimator of \cite{ahn2013eigenvalue}. The test of \cite{fan2023latent} is implemented as in the simulations of \cite{fan2023latent}. 


The results are reported in Table \ref{tab.ind}. In the Online Appendix A, we present simulations under the same data generating processes, but with larger sample size ($T=200$) and number of variables ($p=200$). First, we see that both tests have an empirical size close to the nominal levels. For both testing procedures, we see that the empirical size is closer to the nominal levels for the dependent data case compared to the independent data case, but the differences are small. Notably, we see a large increase in the power of our test compared to the FabTest of \cite{fan2023latent}. In both simulation designs, the power of our test increases much faster for larger values of $m$, suggesting that our procedure correctly rejects the null hypothesis even if the signal is relatively weak, while possessing similar control on the empirical size. 

\begin{table}[h!]\setlength\extrarowheight{-4pt}
	\begin{center}
		\begin{tabular}{lccccccc}
            \hline
			\multicolumn{8}{c}{{\it Design 1}: $\rho_f=\rho_{u}=\rho_e=0$}\\
   
			&\multicolumn{3}{c}{\text{Our test}}&&\multicolumn{3}{c}{\text{FabTest}}\\
			$m$&   $\alpha =0.1$ &$\alpha = 0.05$ &$\alpha =0.01$ && $\alpha =0.1$ &$\alpha =0.05$ &$\alpha =0.01$ \\
			\hline 
			0&	    0.0830& 0.0390&	0.0100&	& 0.0800&	0.0400&	0.0085\\
            0.1&	0.1145&	0.0515& 0.0180&	& 0.1020&	0.0530&	0.01350\\
            0.2&	0.3025&	0.1945&	0.0745&	& 0.1515&	0.0845&	0.0225\\
            0.3&	0.6540&	0.5375&	0.3080&	& 0.3192&	0.2086&	0.0800\\
            0.4&	0.9175&	0.8555& 0.6905& & 0.6740&	0.5430&	0.3245\\[0.5cm]
            
			\multicolumn{8}{c}{{\it Design 2}: $\rho_f=0.6$, $\rho_{u}=0.1$ and $\rho_e=0$}\\
			
			&\multicolumn{3}{c}{\text{Our test}}&&\multicolumn{3}{c}{\text{FabTest}}\\
			
			$m$&   $\alpha =0.1$ &$\alpha = 0.05$ &$\alpha =0.01$ && $\alpha =0.1$ &$\alpha =0.05$ &$\alpha =0.01$ \\
			\hline 
			0&       0.0875 & 0.0390 &0.0065 && 0.0905 & 0.0410 & 0.0125\\
			0.1&     0.1090 & 0.0480 &0.0140 && 0.1015 & 0.0460 & 0.0160\\
			0.2&     0.3075 & 0.2030 &0.0750 && 0.1535 & 0.0805 &	0.0220\\
			0.3&     0.6570 & 0.5320 &0.3145 && 0.3305 & 0.2220 & 0.0920\\
			0.4&     0.9195 & 0.8595 &0.7005 && 0.6810 & 0.5580 & 0.3410\\[0.5cm]
			 
			\multicolumn{8}{c}{{\it Design 3}: $\rho_f=0.6$ and $\rho_{u}=\rho_e=0.1$}\\
		
			&\multicolumn{3}{c}{\text{Our test}}&&\multicolumn{3}{c}{\text{FabTest}}\\
			
			$m$&   $\alpha =0.1$ &$\alpha = 0.05$ &$\alpha =0.01$ && $\alpha =0.1$ &$\alpha =0.05$ &$\alpha =0.01$ \\
			\hline 
			0  & 0.0935	& 0.0475 & 0.0120 &&  0.0850 & 0.0425& 0.0100\\
			0.1& 0.1295 & 0.0595 & 0.0160 &&  0.1160 & 0.0530& 0.0140\\
			0.2& 0.3200 & 0.2065 & 0.0800 &&  0.1600 & 0.0875& 0.0215\\
			0.3& 0.6645 & 0.5480 & 0.3215 &&  0.3302 & 0.2151& 0.0855\\
			0.4& 0.9190 & 0.8665 & 0.7050 &&  0.6810 & 0.5555& 0.3245\\
			\hline\hline
		\end{tabular}
	\end{center}
	\caption{Rejection probabilities with $T=100$ and $p=100$ for the three designs we consider.}
	\label{tab.ind}
\end{table}

Finally, note that our test has a much lower computational time than the FabTest.  For instance, on a Ryzen 9 processor, for Design 1 with $m=0$ and $T=p=100$, our test runs in around 2 seconds, while the FabTest takes 36 seconds (average over 100 replications). 



\section{Empirical application}
%We apply our test to two macroeconomic regression exercises. 
We apply our test in two exercises where we use the \texttt{FRED-MD} monthly dataset of \cite{mccracken2016fred}. To avoid the (potential) structural breaks of the great recession and the COVID pandemic, we analyze the data between July 2009 (one month after the end of the NBER recession) and February 2020 (included).  The variables are transformed and standardized as suggested in \cite{mccracken2016fred}. 
First, we consider inflation series where additionally to our test application, we investigate whether results found using the full sample generalize to out-of-sample forecasting. In the second example, motivated by a recent discussion on sparse versus dense models, see, e.g., \cite{giannone2021economic}, we test whether the dense factor regression model is an appropriate model for the industrial production variable versus the sparse plus dense alternative (factor-augmented sparse regression model). In this case, we additionally investigate all series that are available in the \texttt{FRED-MD} dataset and report the ratios of the number of times we reject the null for each subcategory of this dataset.

\paragraph{Inflation and forecasting.}
In the first exercise, we want to forecast inflation, denoted $\text{CPI}_{t}$, at date $t+1$ (the variable \texttt{CPIAUCSL} of  \texttt{FRED-MD}). We focus on inflation as it is a canonical example in forecasting using factor regressions, see, e.g., \cite{stock1999forecasting}. For this, we use all the variables $x_t$ (including the lag of inflation) at date $t$ from the \texttt{FRED-MD} dataset as regressors, and thus the regression therefore uses one lag of data. We study the following model
\begin{equation}\label{modelcpi}
	\begin{aligned}
		\text{CPI}_{t+1}&=f_t^\top \gamma^*+u_t^\top\beta^*+\varepsilon_t,\\
		x_t&=Bf_t+u_t,\quad t=1\dots,T.
	\end{aligned}
\end{equation}
The final sample consists of $T=127$ observations and $p=127$ variables. We estimate that there are two factors with the eigenvalue ratio estimator of \cite{ahn2013eigenvalue}. We apply our test, choosing an equidistant grid of $M=2000$ values of $\lambda$ and $L=2000$ bootstrap draws (we use the same grid and values of $M$ and $L$ for our other tests implemented in this section). To compute the $p$-value, we perform the test for $\alpha\in\{0.001\ell,\ell\in\{0,\dots,1000\}\}$ and let the $p$-value be equal to the largest value of $\alpha$ for which we reject $H_0$. For this exercise, we find a $p$-value of $0.022$ and therefore reject the hypothesis $H_0$ of adequacy of the classical factor regression model at the $5\%$ level. This suggests that using a factor-augmented sparse regression model could better explain future inflation compared to a factor regression model. Moreover, this indicates that the expected value of inflation given past \texttt{FRED-MD} variables may follow a sparse plus dense pattern rather than only a dense representation. We also implemented the FabTest on this data. Following the procedure in \cite{fan2023latent}, i.e., using 2000 bootstrap replications, cross-validation to compute the parameters of the LASSOs regressions and refitted cross-validation based on iterated sure independent screening to estimate the variance of $\varepsilon_t$, the FabTest returns a $p$-value of $0.784$. Hence, in contrast with our approach, the FabTest does not reject $H_0$ in this exercise. 

Sometimes, practitioners include lags of the outcome variable in the regressors on top of the factors (\cite{stock1999forecasting, stock2002forecasting,bai2006confidence}). If such lags are significant, this could be the reason why we rejected $H_0$. To address this concern, we consider the alternative model
\begin{equation}\label{modellagcpi}
	\begin{aligned}
		\text{CPI}_{t+1}&=\text{CPI}_{t}\delta^*+f_t^\top \gamma^*+u_t^\top\beta^*+\varepsilon_t,\\
		x_t&=Bf_t+u_t,\quad t=1\dots,T,
	\end{aligned}
\end{equation}
where this time $x_t$ contains all variables at date $t$ from the \texttt{FRED-MD} dataset except $\text{CPI}_t$. We apply the test for $H_0:\beta^*=0$, for the case with additional regressors as discussed in the Appendix. We find a $p$-value of $0.023$, so that we can reject $H_0$ in this case as well. \\

Based on our full-sample testing results, we proceed to evaluate whether results generalize to out-of-sample predictions. In this case, we compare out-of-sample prediction accuracy of two models, namely, factor model versus the factor-augmented sparse alternative. Our objective in this exercise is to assess whether the inclusion of a high-dimensional sparse idiosyncratic component in the factor model results in improved or diminished forecasting accuracy. To conduct this assessment, we adopt the lag-augmented model specification outlined in equation \eqref{modellagcpi}. We use the first half of the sample, $t\in[1, \lfloor T/2 \rfloor -1]$, to estimate our model and generate the first out-of-sample forecast. Employing an expanding window approach, we continuously add new observations into the estimation set, allowing us to produce updated forecasts. Once we cover the entire out-of-sample period, $t\in[\lfloor T/2 \rfloor,T]$, we calculate the out-of-sample forecast errors. We employ 5-fold cross-validation to select the tuning parameter in the LASSO regression.

We find that the out-of-sample mean squared error (MSE) ratio of the factor-augmented sparse model relative to the factor regression model is 0.812. This indicates that the inclusion of the high-dimensional idiosyncratic shocks leads to more accurate predictions out-of-sample when compared to the classical factor regression model. This seems to be in line with our full-sample testing results where using our test, we indeed find that the sparse component is significant at a 5\% significance level. 

\paragraph{Testing dense versus sparse plus dense alternative.}
Let us now turn to industrial production, denoted $\text{IP}_t$ (the variable \texttt{INDPRO} of  \texttt{FRED-MD}). We implement the same first regression exercise as for inflation, i.e., in the case where the lag of inflation is included in $x_t$ just replacing inflation by industrial production (see equation \eqref{modelcpi}). We study the following model
\begin{equation}\label{modelindpro}
	\begin{aligned}
		\text{IP}_{t+1}&=f_t^\top \gamma^*+u_t^\top\beta^*+\varepsilon_t,\\
		x_t&=Bf_t+u_t,\quad t=1\dots,T,
	\end{aligned}
\end{equation}
and test the same hypothesis, i.e., $H_0:\beta^*=0$. The $p$-value of our test is equal to $0.121$ and that of the FabTest is $0.880$ (both are computed exactly as in the inflation exercise). Therefore, using both tests, we do not reject $H_0$. This indicates that the factor regression model is adequate to explain industrial production and there is no need to introduce a sparse component in the model. It also suggests that the data generating process is dense. Interestingly, this result confirms the findings of \cite{giannone2021economic}, who, using a Bayesian approach, also found that a dense representation was more suitable in a similar regression exercise of industrial production. Our strategy relies on a formal frequentist test and considers a different alternative. It is, therefore, complementary to that of \cite{giannone2021economic}.

Motivated by contrasting results for inflation and industrial production variables, we analyze each series in the \texttt{FRED-MD} dataset. That is, for each series we test $\beta$ using two model specifications we considered in earlier exercises, namely using only factors (equations \eqref{modelcpi}, \eqref{modelindpro}) and adding a lag of the outcome variable to the factors (equation \eqref{modellagcpi}). We report the rejection ratios at a 5\% significance level for each subcategory of this dataset (see \cite{mccracken2016fred}). Results appear in Table \ref{tab:fredmdreject}. 
First, results show that, in most categories, we reject the null hypothesis in less than 50\% of cases, whether we rely solely on factors or include a lag of the outcome variable. The exception is the {\it Prices} category, where we reject the null hypothesis for 65\% of the series. Notably, the addition of a lag tends to reduce rejection ratios across all categories, except for {\it Housing}. This outcome is expected since the significance of the lag may contribute to our rejection of the null hypothesis. Interestingly, even after accounting for the lag, the {\it Prices} category continues to exhibit a relatively high percentage of series for which we reject the null hypothesis — specifically, 35\%.

\begin{table}[ht]
	\centering
	\begin{tabular}{rrr}
		& Factors & Factors+Lag \\ 
		\hline
		Consumption, Orders, and Inventories & 0.400 & 0.100 \\ 
		Housing & 0.000 & 0.100 \\ 
		Interest and Exchange Rates & 0.455 & 0.091 \\ 
		Labor Market & 0.452 & 0.032 \\ 
		Money and Credit & 0.308 & 0.077 \\ 
		Output and Income & 0.250 & 0.000 \\ 
		Prices & 0.650 & 0.350 \\ 
		Stock Market & 0.200 & 0.000 \\ 
		\hline\hline
	\end{tabular}
	\caption{Rejection ratios for each subcategory using factors (column {\it Factor}) and factors with a lag of the outcome variable (column {\it Factors+Lag}). \label{tab:fredmdreject}}
\end{table}


\section{Conclusion}

This paper proposes a new tuning-free test for the adequacy of the factor regression model against factor-augmented sparse alternatives. We establish the asymptotic validity of our test under time series dependence. In a Monte Carlo study, we show that our procedure has excellent finite sample properties. An empirical application illustrates the usefulness of our method by testing the adequacy of factor regression against factor-augmented sparse alternatives using the well-established \texttt{FRED-MD} dataset. 

In the first case, our test rejects the null hypothesis of no sparse idiosyncratic shocks component in a regression model to forecast inflation. This result remains robust even when we include the lag of inflation as a regressor. These findings are in line with out-of-sample prediction results, where factor-augmented sparse regression reduces the MSE by 19\% compared to the traditional factor regression model.

In the second case, we examine whether the combination of sparse and dense models is suitable for industrial production using our testing approach. In this instance, we do not find evidence to reject the null hypothesis. Given the contrasting results observed for inflation and industrial production, we extend our analysis to encompass all series in the dataset. We calculate rejection ratios for each subcategory of variables. We reject the null the for many series. However, our results suggest that, for the majority of series, the dense model is the appropriate choice, except for the {\it Prices} category. In this particular case, we reject the null hypothesis 35\% of the time, even when incorporating the lag of the dependent variable as a regressor.

%\textbf{ADD MORE FOR EMPIRICS?}
%Our empirical finding is closely related to \cite{giannone2021economic}, who also model industrial production and find no evidence for sparse patterns in this series. However, our methods differ from that of \cite{giannone2021economic}. First, we provide a formal frequentist test based on statistical theory while \cite{giannone2021economic} develop a Bayesian method. Second, and more important, our procedure is fully data-driven and tuning-free while \cite{giannone2021economic}'s approach requires a researcher to select prior distributions, a requirement which may be problematic. For instance, \cite{fava2021illusion} find that the pattern of sparsity is sensitive to the prior distributions choice when applying \cite{giannone2021economic}'s method, signaling that practitioners should be cautious about drawing conclusions when using methods that depend on tuning parameters/priors. Notably, our approach does not require the selection of any tuning parameter.

One possible limitation of this paper is that we modeled the dense component by a factor model. We leave other approaches to model dense components to future research.  %Our paper is the first to suggest a tuning-free procedure to formally test a dense model against a sparse plus dense alternative. 
\appendix 
\renewcommand*{\thesection}{\Alph{section}}
\section*{Appendix: testing with additional regressors}


\section{Alternative model}
As in \cite{stock2002forecasting,bai2006confidence}, we augment the model with additional low-dimensional regressors $w_1,\dots,w_t\in\R^\ell$ (where $\ell$ is fixed with $T$).  We consider the alternative model.
\begin{equation}\label{model-alternative}
	\begin{aligned}
		y_t&=f_t^\top \gamma^*+w_t^\top\delta^*+u_t^\top\beta^*+\varepsilon_t,\\
		x_t&=Bf_t+u_t,\quad t=1\dots,T,
	\end{aligned}
\end{equation}
Here, again, $\varepsilon_t\in \R$ represents a random error, $u_t$ is a $p$-dimensional random vector of idiosyncratic shocks, $f_t$ is a $K$-dimensional random vector of factors, and $B$ is a $p\times K$ random matrix of loadings. The parameters are $\gamma^*\in \R^K$, $\delta^*\in\R^\ell$, $\beta^*\in \R^p$. Note that here $w_t$ plays the role of an observed factor (with loading equal to $0$). This will be key to understanding the alternative testing procedure of Section \ref{subsec.test-alternative}.

We focus on testing
\begin{equation}\label{Htest-alternative}
	H_0:\beta^*=0\quad \text{against}\quad H_1:\beta^*\ne 0.
\end{equation}
To facilitate understanding, we again rewrite the model in matrix form as follows:
\begin{equation*}
	\begin{aligned}
		Y&=F^\top \gamma^*+W\delta^*+ U^\top\beta^*+\mathcal{E},\\
		X&=BF+U,
	\end{aligned}
\end{equation*}
where $Y=(y_1,\dots,y_T)^\top$, $F=(f_1,\dots, f_T)^\top$ is a $T\times K$ matrix, $U=(u_1,\dots,u_T)^\top$, $W=(w_1,\dots,w_T)^\top$ and $X=(x_1,\dots,x_T)^\top$ are $T\times p$ matrices and $\mathcal{E}=(\varepsilon_1,\dots,\varepsilon_T)^\top.$\\

\section{Alternative testing procedure} \label{subsec.test-alternative}
Algorithm \ref{algo-alternative} present the test in this alternative model. It is similar to Algorithm \ref{algo}. The only difference is that $\widehat{P}$ is now the projector on the columns of the $T\times (\widehat{K}+\ell)$ matrix $(\widehat{F}\ W)$ in Step 3. Essentially, $w_t$ is treated as an observed factor. 
% \begin{algorithm}[h!]
% 	\begin{center}
% 		\fbox{\begin{minipage}{\textwidth}
% 				\begin{enumerate}
% 					\item Estimate $\widehat{K}$ by one of the available estimators of the number of factors.
% 					\item Let the columns of $\widehat{F}/\sqrt{T}$ be the eigenvectors corresponding to the leading $\widehat{K}$ eigenvalues of $XX^\top$.
% 					\item Compute $\widehat{U}=\left(I_T-\widehat{P}\right) X$ and $\widetilde{Y}=\left(I_T-\widehat{P}\right)Y$, where $\widehat{P}$ is the projector on the columns of the $T\times \left(\widehat{K}+\ell\right)$ matrix $\left(\widehat{F}\ W\right)$.
% 					\item Calculate an approximation $\widehat{\lambda}_{\alpha,emp}$ of $\widehat{\lambda}_\alpha$ as follows
% 					\begin{itemize}
% 						\item[4.1] Specify a grid $0<\lambda_1<\dots<\lambda_M<\bar{\lambda}$, with $\bar{\lambda}=2T^{-1}\left\|\widehat{U}^\top\widetilde{Y}\right\|_\infty$.
% 						\item[4.2] For $m\in[M]$ compute $\left\{\widehat{Q}\left(\lambda_m,e^{(\ell)}\right):\ \ell\in[L]\right\}$ for $L$ draws of $e\sim\mathcal{N}(0,I_T)$ and the corresponding empirical $(1-\alpha)$-quantile $\widehat{q}_{\alpha,emp}(\lambda_m)$ from them.
% 						\item[4.3] Let $\widehat{\lambda}_{\alpha,emp}=\widehat{q}_{\alpha,emp}(\lambda_{\widehat{m}})$,  with $\widehat{m}=\min\{m\in[M]:\ \widehat{q}_{\alpha,emp}(\lambda_{m'})\le \lambda_{m'}\text{ for all }m'\ge m\}.$
% 					\end{itemize}
% 					\item Reject $H_0$ when $2T^{-1}\left\|\widehat{U}^\top \widetilde{Y}\right\|_\infty>\widehat{\lambda}_{\alpha,emp}$.
% 				\end{enumerate}
% 	\end{minipage}}\end{center}
% 	\caption{conducting a test of level $\alpha\in(0,1)$ with additional regressors.}
% 	\label{algo-alternative}
% \end{algorithm}

\begin{algorithm}
\singlerule\vspace{0.1cm}
\begin{algorithmic}
  \STATE \textbf{1.} Estimate $\widehat{K}$ by one of the available estimators of the number of factors.
  \STATE \textbf{2.} Let the columns of $\widehat{F}/\sqrt{T}$ be the eigenvectors corresponding to the leading $\widehat{K}$ eigenvalues of $XX^\top$.
   \STATE \textbf{3.} Compute $\widehat{U}=\left(I_T-\widehat{P}\right) X$ and $\widetilde{Y}=\left(I_T-\widehat{P}\right)Y$, where $\widehat{P}$ is the projector on the columns of the $T\times \left(\widehat{K}+\ell\right)$ matrix $\left(\widehat{F}\ W\right)$.
   \STATE \textbf{4.} Calculate an approximation $\widehat{\lambda}_{\alpha,emp}$ of $\widehat{\lambda}_\alpha$ as follows:
   \STATE \begin{itemize}
               \item[\textbf{4a.}] Specify a grid $0<\lambda_1<\dots<\lambda_M<\bar{\lambda}$, with $\bar{\lambda}=2T^{-1}\left\|\widehat{U}^\top\widetilde{Y}\right\|_\infty$.
               \item[\textbf{4b.}] For $m\in[M]$ compute $\left\{\widehat{Q}\left(\lambda_m,e^{(\ell)}\right):\ \ell\in[L]\right\}$ for $L$ draws of $e\sim\mathcal{N}(0,I_T)$ and the corresponding empirical $(1-\alpha)$-quantile $\widehat{q}_{\alpha,emp}(\lambda_m)$ from them.
               \item[\textbf{4c.}] Let $\widehat{\lambda}_{\alpha,emp}=\widehat{q}_{\alpha,emp}(\lambda_{\widehat{m}})$,  with $\widehat{m}=\min\{m\in[M]:\ \widehat{q}_{\alpha,emp}(\lambda_{m'})\le \lambda_{m'}\text{ for all }m'\ge m\}.$
            \end{itemize}
   \STATE \textbf{5.} Reject $H_0$ when $2T^{-1}\left\|\widehat{U}^\top \widetilde{Y}\right\|_\infty>\widehat{\lambda}_{\alpha,emp}$.
\end{algorithmic}
\doublerule
\caption{Conducting a test of level $\alpha\in(0,1)$ with additional regressors.\label{algo-alternative}}
\end{algorithm}

\section*{Supplementary material}

\begin{description}

\item[Online Appendix:] Additional simulation results and the proof of Theorem \ref{th} (.pdf file). 
\item[R package:] an R package implementing our test is available at \url{https://github.com/jstriaukas/bootml}.
\end{description}

\if1\blind
{
\section*{Acknowledgments}
The authors thank Michael Vogt for sharing his code for the procedure of \cite{lederer2021estimating}. Jad Beyhum undertook most of this work while employed by CREST, ENSAI (Rennes). Jad Beyhum gratefully acknowledges financial support from the Research Fund KU Leuven through the grant STG/23/014. Jonas Striaukas gratefully acknowledges the financial support from the European Commission, MSCA-2022-PF Individual Fellowship, Project 101103508. Project Acronym: MACROML. 
} \fi



\bibliographystyle{agsm}
\bibliography{bibliography}


\end{document}