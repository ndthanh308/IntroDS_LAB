\section{Experiments}
\label{sec:exps}

In this section, we present a comprehensive evaluation of the effect of each component in our data augmentation paradigm, investigate how to appropriately use the data for learning, and test agents~\cite{chen2022duet,chen2021hamt} pre-trained with our data on multiple VLN downstream tasks~\cite{anderson2018r2r,krantz2020navgraph,qi2020reverie,thomason2020cvdn}.

\subsection{Experimental Setup}

\paragraph{Datasets} 
We perform analysis mainly on the R2R dataset~\cite{anderson2018r2r}, while evaluating the generalization potential of our augmented data on REVERIE~\cite{qi2020reverie}, CVDN~\cite{thomason2020cvdn} and R2R-CE~\cite{krantz2020navgraph}. The datasets are outlined as follows:


\begin{itemize}[itemsep=0.1em,parsep=0em,topsep=0em,partopsep=0em,leftmargin=2em]
    \item \textbf{R2R} consists of 22k human-annotated navigational instructions, each describing a trajectory that traverses multiple rooms in MP3D~\cite{chang2017matterport3d}. 
    On average, an instruction contains 32 words, and each ground-truth path is formed by seven nodes with a total length of 10 $m$.
    \item \textbf{REVERIE} inherits the trajectories in R2R but provides high-level instructions which describe a target object. The task for an agent is first to find the object, and localize it in observation.
    \item \textbf{CVDN} provides dialogues between a navigator who tries to find a target by asking for guidance and an oracle with a privileged view of the best next step. An agent who addresses the task needs to find the way by interpreting the dialogue history.
    \item \textbf{R2R-CE} transfers the discrete trajectories in R2R to continuous 3D scans rendered by Habitat simulator~\cite{savva2019habitat}, where an agent can freely travel in the open space and need to interact with obstacles. The dataset contains 16k instruction-trajectory pairs after removing non-transferable paths.
\end{itemize}

Besides, we also adopt the widely applied augmented R2R dataset PREVALENT~\cite{hao2020prevalent} in our experiments, which only has 178,270 samples created from MP3D scenes. For simplicity, we use PREV, HM-E, and Gib-E to denote PREVALENT data, our augmented data from HM3D and Gibson scenes with instructions generated by EnvDrop Speaker~\cite{tan2019envdrop}, respectively, and use the term \ours{} data for all our HM-E and Gib-E data in the following sections.


\paragraph{Baseline VLN Models}

We employ the recently proposed VLN agents, Dual-Scale Graph Transformer (DUET)~\cite{chen2022duet} and History Aware Multimodal Transformer (HAMT)~\cite{chen2021hamt} as the baseline models in our experiments. The primary idea of DUET is to build a topological map on the fly, which extends the agent's action space from its current viewpoint to all navigable directions encountered during navigation, therefore, greatly facilitating planning and error correction. HAMT explicitly stores the observations at each navigational step, which benefits the learning of sequence-to-sequence alignment between vision and instruction. We refer readers to their original papers for more technical details. In our experiments, we apply the DUET agent in R2R, CVDN and REVERIE, whereas using the HAMT agent in R2R-CE, since the two models report the best results on these datasets, respectively.


\paragraph{Training} 

We use the augmented data for a two-stage VLN agent training, \ie, pre-training and fine-tuning. In pre-training, we consider the most widely applied proxy tasks in previous work, Masked Language Modeling (MLM), Masked Region Modeling (MRM), and Single-Action Prediction (SAP)~\cite{chen2021hamt,chen2022duet,hao2020prevalent,qiao2022hop,qiao2023hop+}, to enhance agent's language understanding, visual perception, and to benefit cross-modal grounding between instruction and observation (whose effect will be studied in \S\ref{subsec:utilize_data}). We refer to \textit{Appendix}~\ref{sec_1} for their implementation details.

After pre-training, similar to AutoVLN~\cite{chen2022duet} and MARVAL~\cite{kamath2022marval}, we fine-tune the model simply with imitation learning (IL) method DA{\footnotesize GGER}~\cite{ross2011dagger}. Specifically, at each time step, an agent performs an action sampled from the predicted probability of its action space, and minimizes the loss between the sampled action and the ground truth. This method allows an agent to learn from paths that cover wide space and reduces the exposure bias caused by teacher forcing~\cite{lamb2016professor}. 


\paragraph{Implementation Details}

We apply CLIP ViT-B/16~\cite{radford2021clip}, a visual transformer~\cite{dosovitskiy2020vit} pre-trained to align millions of image-text pairs from the web, as the visual encoder in all our experiments if not specified otherwise. We refer to \textit{Appendix}~\ref{sec_1} for more details.

To address R2R and CVDN, we pre-train DUET for 20k iterations with a batch size of 256 and learning rate of $5 \times 10^{-5}$ on two NVIDIA Tesla A100 GPUs for about 72 GPU hours. 
We select one of the models logged in pre-training for fine-tuning according to the accuracy in solving proxy tasks and the performance in following R2R instructions. The selected model is then fine-tuned for 200k iterations with batch size 16 on a single GPU on both R2R and ScaleVLN datasets, which takes about 48 GPU hours to reach the peak performance.
For CVDN, we directly fine-tune the pre-trained DUET model on the dataset with the same configurations as fine-tuning on R2R.
For R2R-CE, we pre-train a single HAMT model with the same data and configurations, then fine-tune the model on the dataset. Similar to prior work~\cite{an2023etpnav,an20221st,wang2022internvideo}, our HAMT agent in R2R-CE leverages a candidate waypoint predictor~\cite{hong2022bridging} which predicts navigable locations to support agent's high-level decision-making process.


\paragraph{Evaluation Metrics}
 Standard metrics~\cite{anderson2018r2r} are applied to assess the agent's performance, including Trajectory Length (TL) which is the average length of the agent's predicted path in meters, Navigation Error (NE) which is the average distance between the agent's final position and the target in meters, Success Rate (SR) which is the ratio of agents that stop within 3 meters to the target viewpoint, and Success penalized by Path Length (SPL)~\cite{anderson2018spl}. For R2R-CE, normalized Dynamic Time Warping (nDTW) is an additional metric that measures the step-wise alignment between the ground truths and the agent-predicted paths~\cite{ilharco2019ndtw}. For CVDN, Goal Progress (GP) is the only metric; it measures the average difference between the length of the completed trajectory and the remaining distance to goal~\cite{thomason2020cvdn}.
 

\begin{table}[t]
  \begin{center}
  \resizebox{\columnwidth}{!}{
  \begin{tabular}{l|cc|ccc|rrr}
    \hline \hline
     \multicolumn{1}{c|}{\multirow{2}{*}{Methods}} & \multicolumn{2}{c|}{HM3D Nav Graphs} & \multicolumn{3}{c|}{R2R Val-Seen} &\multicolumn{3}{c}{R2R Val-Unseen} \\
     \cline{2-9} & \multicolumn{1}{c}{Density } & \multicolumn{1}{c|}{Collision} & \multicolumn{1}{c}{NE$\downarrow$} & \multicolumn{1}{c}{SR$\uparrow$} &
    \multicolumn{1}{c|}{SPL$\uparrow$} & \multicolumn{1}{c}{NE$\downarrow$} & \multicolumn{1}{c}{SR$\uparrow$} &
    \multicolumn{1}{c}{SPL$\uparrow$} \Tstrut\\
    \hline \hline
        None & --  & -- & 2.51 & 76.89 & 69.71 & 3.06 & 72.92 & 62.82\\
        \hline
     AutoVLN & 0.36 & 29.35\% & \textbf{1.90} & \textbf{84.43} & \textbf{79.10} & 3.08 & 72.75 & 62.56 \\
     
          Ours & 1.16 & 0.00\% & 2.25 & 79.82 & 75.06 & \textbf{2.75} & \textbf{76.01} & \textbf{66.94} \\
    \hline \hline
  \end{tabular}}
\end{center}
\vspace{-10pt}
\caption{Comparison on navigation graphs. \textit{Density} is computed as the number of nodes per navigable area (node/$m^2$), and \textit{Collision} is the ratio of edges that go through obstacles. \textit{None} means the agent only learn from R2R and PREV data). }
\label{tab:compare_graph}
\vspace{-10pt}
\end{table}


\subsection{Scale VLN Data, What Really Matters?}
\label{subsec:scale_data}


\paragraph{Effect of Navigation Graphs}
We first study the effect of different navigation graphs in  Table~\ref{tab:compare_graph}, where we compare the graphs from our method to AutoVLN~\cite{chen2022hm3dlearning}. For fairness, both methods only use 800 HM3D scenes without recovering faulty rendered images. We can see that generating augmented data from AutoVLN's graph cannot benefit the agent's performance in unseen environments. We suspect this is mainly due to a high ratio of edges that go through obstacles, resulting in noisy and misleading trajectories that do not exist in the downstream navigation graph.
On the contrary, our fully traversable graphs with a high density of viewpoints produce effective data, which greatly improves the results, suggesting the importance of graph quality in sampling discrete augmented data.

\begin{table}[t]
  \begin{center}
  \resizebox{\columnwidth}{!}{
  \begin{tabular}{rr|rrrr|rrrr}
    \hline \hline
     \multicolumn{2}{c|}{\makecell[c]{HM-E Aug}} & \multicolumn{4}{c|}{R2R Val-Seen} &\multicolumn{4}{c}{R2R Val-Unseen} \\
     \hline
     \#Scenes & \#Samples & \multicolumn{1}{c}{TL} & \multicolumn{1}{c}{NE$\downarrow$} & \multicolumn{1}{c}{SR$\uparrow$} &
    \multicolumn{1}{c|}{SPL$\uparrow$} & \multicolumn{1}{c}{TL} & \multicolumn{1}{c}{NE$\downarrow$} & \multicolumn{1}{c}{SR$\uparrow$} &
    \multicolumn{1}{c}{SPL$\uparrow$} \Tstrut\\
    \hline \hline
    800 & 2890k &   
    12.63 & 2.27 & 79.24 & 73.34 &
    12.83 & \textbf{2.62} & \textbf{76.59} & \textbf{67.74}\\
    800 & 1400k & 12.21 & 2.18 & 80.71 & 75.92 & 12.97 & 2.71 & 76.01 & 66.56 \\
    800 &  700k & 12.63 & \textbf{1.86} & \textbf{83.35} & \textbf{77.97} & 13.83 & 2.69 & 76.25 & 66.00\\
    % \hline
    400 & 700k & 
    12.76 & 1.87 & 82.96 & 77.05 &
     13.80 & 2.78 & 75.22 & 65.32
     \\
    200 &  700k &       
     11.95 & 1.95 & 82.66 & \textbf{77.97} &
    13.29 & 2.73 & 74.84 & 64.59 \\
    \hline
    0   &    0 &      13.28 & 2.51 & 76.89 & 69.71 &
     13.53 & 3.06 & 72.92 & 62.82\\
    \hline \hline
  \end{tabular}}
\end{center}
\vspace{-10pt}
\caption{Comparison of the quantity of augmented scene and samples. Here each experiment is pre-trained on data from R2R, PREV, and HM-E, and fine-tuned on R2R.}
\label{tab:quan_env_sample}
\end{table}


\paragraph{Effect of More Data }

Table~\ref{tab:quan_env_sample} shows the influence of the quantity of additional environments and training data. We can see that with the same amount of augmented scenes (800), agent performance in val-unseen gradually increases with higher sampling density.
On the other hand, generating the same amount of samples (700k) from more environments leads to better results. And it is clear that \#Scenes has a stronger impact than \#Samples, which suggests the importance of having more diverse environments for learning VLN.
Then, in Table~\ref{tab:add_gibson}, we further increase the quantity of training samples from Gibson environments for comparison and evaluate the pre-trained model's performance on R2R without fine-tuning. Conclusions from the previous table still hold: adding more scenes and data can bring a steady performance gain to the agent.


\begin{table}[t]
  \begin{center}
  \resizebox{\columnwidth}{!}{
  \begin{tabular}{l|c|rrr|rrr}
    \hline \hline
     \multicolumn{1}{c|}{\multirow{2}{*}{Pre-Train}} & \multicolumn{1}{c|}{\multirow{2}{*}{\makecell{Fine-Tune}}} & \multicolumn{3}{c|}{R2R Val-Seen} &\multicolumn{3}{c}{R2R Val-Unseen} \\
     \cline{3-8}  & & \multicolumn{1}{c}{NE$\downarrow$} & \multicolumn{1}{c}{SR$\uparrow$} &
    \multicolumn{1}{c|}{SPL$\uparrow$} & \multicolumn{1}{c}{NE$\downarrow$} & \multicolumn{1}{c}{SR$\uparrow$} &
    \multicolumn{1}{c}{SPL$\uparrow$} \Tstrut\\
    \hline \hline
    R2R, PREV & R2R & 2.51 & 76.89 & 69.71 & 3.06 & 72.92 & 62.82 \\    
    R2R, PREV + HM-E & R2R &  2.27 & 79.24 & 73.34 & 2.62 & 76.59 & 67.74 \\
    R2R, PREV + \ours{} & R2R & \textbf{2.02} & \textbf{80.51} & \textbf{74.88} & \textbf{2.53} & \textbf{78.08} & \textbf{68.31} \\
    \hline
    R2R, PREV & -- & 3.77 & 67.19 & 64.49 & 5.80 & 47.42 & 45.30 \\
    R2R, PREV + HM-E & -- & 4.04 & 64.64 & 62.00 & 5.03 & 55.09 & 52.23 \\
    R2R, PREV + \ours{} & -- & \textbf{3.64} & \textbf{71.11} & \textbf{68.53} & \textbf{4.90} & \textbf{57.00} & \textbf{54.03} \\
    \hline \hline
  \end{tabular}}
\end{center}
\vspace{-10pt}
\caption{Results of adding more augmented data and the pre-trained model performance without fine-tuning.}
\label{tab:add_gibson}
\vspace{-10pt}
\end{table}


\paragraph{Effect of Image Quality}

We evaluate the influence of image quality in augmented data on agent performance in Table~\ref{tab:recover_img}. We can see that for data from both HM3D and Gibson, recovering the rendered raw images can provide a noticeable benefit to the agent's results. Such improvement is more apparent for Gib-E because a large portion of 3D meshes for reconstructing Gibson scenes is in low-quality~\cite{xia2018gibson}, which leads to largely broken or distorted rendered views. In fact, HM-E (R) + Gib-E (F) leads to worse results than HM-E (R) alone even with 61\% more different environments, suggesting the great importance of having data with high visual quality.


\begin{table}[t]
  \begin{center}
  \resizebox{\columnwidth}{!}{
  \begin{tabular}{l|rrrr|rrrr}
    \hline \hline
     \multicolumn{1}{c|}{\multirow{2}{*}{Scenes}}  & \multicolumn{4}{c|}{R2R Val-Seen} &\multicolumn{4}{c}{R2R Val-Unseen} \\
     \cline{2-9} &  \multicolumn{1}{c}{TL} & \multicolumn{1}{c}{NE$\downarrow$} & \multicolumn{1}{c}{SR$\uparrow$} &
    \multicolumn{1}{c|}{SPL$\uparrow$} & \multicolumn{1}{c}{TL} & \multicolumn{1}{c}{NE$\downarrow$} & \multicolumn{1}{c}{SR$\uparrow$} &
    \multicolumn{1}{c}{SPL$\uparrow$} \Tstrut\\
    \hline \hline
    HM-E (F) & 12.17 & \textbf{2.25} & \textbf{79.82} & \textbf{75.06} & 12.64 & 2.75 & 76.01 & 66.94 \\
    HM-E (R) &  12.63 & 2.27 & 79.24 & 73.34 & 12.83 & \textbf{2.62} & \textbf{76.59} & \textbf{67.74} \\
    \hline
    HM-E (R) + Gib-E (F) & 12.74 & 2.17 & \textbf{81.00} & \textbf{75.44} & 13.57 & 2.65 & 76.33 & 66.97 \\
    HM-E (R) + Gib-E (R) & 12.41 & \textbf{2.02} & 80.51 & 74.88 &
    13.16 & \textbf{2.53} & \textbf{78.08} & \textbf{68.31} \\
    \hline \hline
  \end{tabular}}
\end{center}
\vspace{-10pt}
\caption{Effect of augmented image quality. \textit{(F)} denotes faulty rendered images and \textit{(R)} denotes recovered images.}
\label{tab:recover_img}
\end{table}


\paragraph{Effect of Augmented Instruction}
By comparing different speakers in Table~\ref{tab:lang_aug}, we found that a simple LSTM-based model (EnvDrop~\cite{tan2019envdrop}) trained from-scratch results in a higher Bleu-4 score~\cite{papineni2002bleu} than a fine-tuned GPT-2~\cite{radford2019gpt2}. However, we are aware that producing high-fidelity and detailed navigational instructions is a long-lasting and challenging problem~\cite{dou2022foam,fried2018speaker,magassouba2021crossmap,tan2019envdrop,wang2022less,zhou2023navgpt}, but we only experimented with two simple models. Numbers in Table~\ref{tab:lang_aug} indicate that the generated instructions are of low quality while showing a large influence on learning to navigate, implying that pairing the augmented trajectories with better instructions could be promising future work.


\begin{table}[t!]
  \begin{center}
  \resizebox{0.80\columnwidth}{!}{
  \begin{tabular}{c|c|rrrr}
    \hline \hline
      \multirow{2}{*}{Speaker} & \multicolumn{1}{c|}{Instruction Quality} &\multicolumn{4}{c}{R2R Val-Unseen} \\
     \cline{2-6} &\multicolumn{1}{c|}{Bleu-4$\uparrow$}  & \multicolumn{1}{c}{TL} & \multicolumn{1}{c}{NE$\downarrow$} & \multicolumn{1}{c}{SR$\uparrow$} &
    \multicolumn{1}{c}{SPL$\uparrow$} \Tstrut\\
    \hline \hline
    GPT-2 & 24.36 & 13.98 & 2.74 & 75.82 & 66.08  \\
    EnvDrop & \textbf{27.66} & 12.83  & \textbf{2.62} & \textbf{76.59 }& \textbf{67.74} \\
    % \hline \Tstrut
    % \multirow{2}{*}{REVERIE} & GPT2$^{\text{BLEU}}$ & & & & & &\\
    % & GPT2$^{\text{Vocab}}$ & & & & & & \\
    \hline \hline
  \end{tabular}}
\end{center}
\vspace{-10pt}
\caption{Quality of generated instructions and their influence on agent's performance on the R2R dataset.} 
\label{tab:lang_aug}
\end{table}



\begin{table*}[t]
  \begin{center}
  \resizebox{1.00\textwidth}{!}{
  \begin{tabular}{c|ccc|ccc|rrrr|rrrr}
    \hline \hline
    % \multicolumn{1}{c|}{\multirow{2}{*}{Methods}} & 
    \multicolumn{1}{c|}{\multirow{2}{*}{Method \#}} & \multicolumn{3}{c|}{Pre-training Data} & \multicolumn{3}{c|}{Fine-tuning Data} & \multicolumn{4}{c|}{R2R Val-Seen} &
    \multicolumn{4}{c}{R2R Val-Unseen} \\
    \cline{2-15} & 
    \multicolumn{1}{c}{R2R} & \multicolumn{1}{c}{PREV} & \multicolumn{1}{c|}{HM-E (ours)} &  \multicolumn{1}{c}{R2R} & \multicolumn{1}{c}{PREV} & \multicolumn{1}{c|}{HM-E (ours)} & \multicolumn{1}{c}{TL} & \multicolumn{1}{c}{NE$\downarrow$} & \multicolumn{1}{c}{SR$\uparrow$} & \multicolumn{1}{c|}{SPL$\uparrow$} & \multicolumn{1}{c}{TL} & \multicolumn{1}{c}{NE$\downarrow$} & \multicolumn{1}{c}{SR$\uparrow$} & \multicolumn{1}{c}{SPL$\uparrow$}\Tstrut\\
    \hline \hline
    1 & 
    \checkmark &  &  &
    \checkmark &  &  & 
     12.02 & 3.39 & 69.83 & 64.30 &
     12.45 & 4.04 & 65.26 & 56.91\\
    2 & 
    \checkmark & \checkmark &  & 
    \checkmark &  &  & 
     13.28 & 2.51 & 76.89 & 69.71 &
     13.53 & 3.06 & 72.92 & 62.82\\
    3 & 
    \checkmark &  & \checkmark & 
    \checkmark &  &  & 
     12.80 & 2.69 & 75.02 & 68.71 &
     12.66 & 2.79 & 74.96 & 65.90\\
     4 & 
    \checkmark & \checkmark & \checkmark & 
    \checkmark &  &  & 
     12.63 & 2.27 & 79.24 & 73.34 &
     12.83 & 2.62 & 76.59 & 67.74\\
     \hline
     5 & 
    \checkmark & \checkmark & \checkmark & 
    \checkmark & \checkmark &  & 
     12.31 & 2.20 & 80.51 & \textbf{75.75} &
     12.86 & 2.65 & 75.78 & 66.36\\
     6 & 
    \checkmark & \checkmark & \checkmark &
    \checkmark & & \checkmark & 
     13.38 & \textbf{2.12} & 80.02 & 73.52 &
     13.32 & \textbf{2.46} & \textbf{79.10} & \textbf{68.66}\\
     7 & 
    \checkmark & \checkmark & \checkmark & 
    \checkmark & \checkmark & \checkmark & 
     12.62 & 2.18 & \textbf{80.71} & 75.29 &
     13.22 & 2.58 & 77.10 & 67.23 \\
    \hline \hline
  \end{tabular}}
\end{center}
\vspace{-10pt}
\caption{Influence of applying augmented data in pre-training and fine-tuning on agent's performance.}
\vspace{-3pt}
\label{tab:pretrain_finetune}
\end{table*}


\subsection{How to Utilize Large-Scale Data?}
\label{subsec:utilize_data}



\paragraph{Data for Pre-Training and Fine-Tuning} 

Pre-training and fine-tuning are two essential stages where augmented data can directly impact. In Table~\ref{tab:pretrain_finetune}, we investigate how to effectively apply the original R2R dataset, PREV~\cite{hao2020prevalent}, and our HM-E data in the two processes.
First, comparing applying PREV and HM-E (Method\#2 and \#3) in pre-training, it is unsurprisingly that an agent benefits more from learning in environments different from downstream scenes. 
A better result of Method\#4 shows that PREV and our HM-E complement each other in the pre-training phase.
Then, we investigate the effect of applying augmented data in fine-tuning, in which the motivation is to avoid overfitting the small downstream dataset. Compare Method\#4 to Method\#5, \#6, and \#7; it is clear that it is very beneficial to keep the data augmented from the addition environments (HM-E) in fine-tuning (+2.51\% SR in Val-Unseen). Moreover, by doing so, the generalization gap between navigating in seen and unseen environments has been reduced to less than 1\% SR (80.02\% vs. 79.10\%), reflecting the importance of maintaining high visual diversity in training. Compare Method\#6 and Method\#7, including PREV in fine-tuning harms the performance likely because it will cause the learning to overfit the 61 MP3D scenes.
In addition to Table~\ref{tab:pretrain_finetune}, our experiment shows that adding Gib-E to the pre-training phrase can improve the result, while applying it in fine-tuning does not show a noticeable difference. This is likely because the generated instruction-trajectory pairs from Gibson environments have a larger gap to the Matterport scenes, which will introduce noise and is unsuitable for fine-tuning.


\paragraph{Effect of Pre-training Tasks} 
In Table~\ref{tab:pretrain_tasks}, we further investigate the effect of three proxy tasks, MLM, SAP, and MRM, on pre-training the best performing model in Table~\ref{tab:pretrain_finetune} (Method\#6). Results show that both MLM and SAP are very effective pre-training tasks that can greatly enhance the agent's performance when applied alone, and they are complementary since combining the two tasks can lead to a larger improvement (+16.49\% SPL higher than without pre-training). However, learning MRM with the other two proxy tasks slightly degenerates the results. We suspect this is because an agent can already learn very rich and generalizable semantic representations from large and diverse augmented data, whereas predicting the probability distribution of object categories for masked images introduces too much noise to the learning process.

\vspace{5pt}

Based on the findings from our experiments, we pre-train our agent with MLM and SAP on R2R, PREV, and our \ours{} datasets to get the best pre-trained model, and fine-tune on R2R and HM-E for best performance.


\begin{table}[t]
  \begin{center}
  \resizebox{\columnwidth}{!}{
  \begin{tabular}{ccc|rrrr|rrrr}
    \hline \hline
     \multicolumn{3}{c|}{Pre-training Tasks} & \multicolumn{4}{c|}{R2R Val-Seen} &\multicolumn{4}{c}{R2R Val-Unseen} \\
    \cline{1-11} MLM & SAP & MRM & \multicolumn{1}{c}{TL} & \multicolumn{1}{c}{NE$\downarrow$} & \multicolumn{1}{c}{SR$\uparrow$} &
    \multicolumn{1}{c|}{SPL$\uparrow$} & \multicolumn{1}{c}{TL} & \multicolumn{1}{c}{NE$\downarrow$} & \multicolumn{1}{c}{SR$\uparrow$} &
    \multicolumn{1}{c}{SPL$\uparrow$} \Tstrut\\
    \hline \hline
    &  & & 11.96 & 3.61 & 66.01 & 60.66 & 14.47 & 4.26 & 62.62 & 51.25 \\
    \checkmark & & & 14.13 & 2.91 & 74.93 & 65.95 & 15.75 & 3.62 & 69.14 & 57.47 \\
     & \checkmark  & & 12.20 & 1.99 & 80.61 & 75.07 & 12.89 &2.89 & 74.20 & 65.40 \\
    \checkmark  & \checkmark  &  & 12.63 & 2.27 & 79.24 & 73.34 &
    12.83 & \textbf{2.62} & \textbf{76.59} & \textbf{67.74}\\
    \checkmark  & \checkmark  & \checkmark & 12.37 & \textbf{1.97} & \textbf{81.88} & \textbf{75.72} & 13.69 & 2.73 & 75.82 & 66.62 \\
    \hline \hline
  \end{tabular}}
\end{center}
\vspace{-10pt}
\caption{Influence of pre-training tasks. MLM, SAP, and MRM denote masked language modeling, single-action prediction, and masked region modeling.}
\label{tab:pretrain_tasks}
\vspace{-5pt}
\end{table}


\begin{table}[t!]
  \begin{center}
  \resizebox{\columnwidth}{!}{
  \begin{tabular}{l|ccc|ccc}
    \hline \hline
    \multicolumn{1}{c|}{\multirow{2}{*}{Methods}}  & \multicolumn{3}{c|}{Val-Unseen} & \multicolumn{3}{c}{Test-Unseen}  \\
    \cline{2-7} & \multicolumn{1}{c}{OSR$\uparrow$} & \multicolumn{1}{c}{SR$\uparrow$} &
    \multicolumn{1}{c|}{SPL$\uparrow$} & \multicolumn{1}{c}{OSR$\uparrow$} &
    \multicolumn{1}{c}{SR$\uparrow$} &
    \multicolumn{1}{c}{SPL$\uparrow$} \Tstrut\\
    \hline \hline
    RecBERT~\cite{hong2020recurrent} & 27.66 & 25.53 & 21.06 & 26.67 & 24.62 & 19.48  \\
    SIA~\cite{lin2021scene}  & 44.67 & 31.53 & 16.28 & 44.56 &30.80 & 14.85 \\ 
    HAMT~\cite{chen2021hamt} & 36.84 & 32.95 & 30.20 & 33.41 & 30.40 & 26.67 \\ 
    DUET~\cite{chen2022duet} & 51.07 & 46.98 & 33.73 & 56.91 & 52.51 & 36.06 \\
    AutoVLN~\cite{chen2022hm3dlearning} & 62.14 & 55.89 & 40.85 & 62.30 & 55.17 & 38.88  \\
    \hline
    DUET+\ours{} (ours) & \textbf{63.85} & \textbf{56.97} & \textbf{41.84} & \textbf{62.65} & \textbf{56.13} & \textbf{39.52} \\
    \hline \hline
  \end{tabular}}
\end{center}
\vspace{-10pt}
\caption{Navigation performance on REVERIE dataset.}
\label{tab:reverie_results}
\vspace{-10pt}
\end{table}


\subsection{Evaluate on Various VLN Tasks}

\paragraph{R2R} 
Table~\ref{tab:r2r_full} compares agents' single-run performance on the R2R dataset. We can see that training DUET model with our \ours{} data results in 8\% SR and 8\% SPL absolute improvement on the test split\footnote{On the R2R test-unseen leaderboard: \url{https://eval.ai/web/challenges/challenge-page/97/leaderboard/270}, our method surpasses all single-run results and outperforms all previous models applying beam-search or pre-exploration (see \textit{Appendix}~\ref{sec_2}).}, which also greatly outperforms the previous best method BEVbert~\cite{an2022bevbert}. As suggested in MARVAL~\cite{kamath2022marval}, we also experiment with applying a more powerful visual encoder, CLIP ViT-H/14~\cite{radford2021clip}, and the image augmentation method EnvEdit~\cite{li2022envedit} to our approach, leading to a remarkable 80\% SR, and reducing the long-lasting generalization gap between seen and unseen environments~\cite{zhangdiagnosing} to less than 1\%. It is interesting to notice that the remaining gap towards human performance (6\% SR) is similar to the difference between the agent's OSR and SR (6{\textasciitilde}7\%), which suggests that it might be important for future work to improve the policy network to tackle the stopping problem given large-scale data.



\begin{table*}[ht]
\centering
\resizebox{\textwidth}{!}{\begin{tabular}{l|ccccc | ccccc|ccccc}
\hline \hline
\multicolumn{1}{c|}{\multirow{2}{*}{Methods}} & \multicolumn{5}{c|}{R2R Val-Seen} & \multicolumn{5}{c|}{R2R Val-Unseen} & \multicolumn{5}{c}{R2R Test-Unseen}\Tstrut \\
\cline{2-16}
\multicolumn{1}{c|}{} & 
\multicolumn{1}{c}{TL} & \multicolumn{1}{c}{NE$\downarrow$} & \multicolumn{1}{c}{OSR$\uparrow$} & \multicolumn{1}{c}{SR$\uparrow$} & \multicolumn{1}{c|}{SPL$\uparrow$} & 
\multicolumn{1}{c}{TL} & \multicolumn{1}{c}{NE$\downarrow$} & \multicolumn{1}{c}{OSR$\uparrow$} & \multicolumn{1}{c}{SR$\uparrow$} & \multicolumn{1}{c|}{SPL$\uparrow$} & 
\multicolumn{1}{c}{TL} & \multicolumn{1}{c}{NE$\downarrow$} & \multicolumn{1}{c}{OSR$\uparrow$} & \multicolumn{1}{c}{SR$\uparrow$} & \multicolumn{1}{c}{SPL$\uparrow$} \\
\hline \hline
Human                   
& -- & -- & -- & -- & -- 
& -- & -- & -- & -- & --
& 11.85 & 1.61 & 90 & 86 & 76 \\
\hline
Seq2Seq~\cite{anderson2018r2r}
& 11.33 & 6.01 & 53 & 39 & -- 
& 8.39 & 7.81 & 28 & 21 & -- 
& 8.13 & 7.85 & 27 & 20 & -- \\
Speaker Follower~\cite{fried2018speaker}
& -- & 3.36 & 74 & 66 & -- 
& -- & 6.62 & 45 & 36 & -- 
& 14.82 & 6.62 & - & 35 & 28  \\
RCM~\cite{wang2019reinforced}
& 10.65 & 3.53 & 75 & 67 & -- 
& 11.46 & 6.09 & 50 & 43 & -- 
& 11.97 & 6.12 & 50 & 43 & 38 \\
SSM~\cite{wang2021structured}
& 14.70 & 3.10 & 80 & 71 & 62
& 20.70 & 4.32 & 73 & 62 & 45
& 20.40 & 4.57 & 70 & 61 & 46 \\
EnvDrop~\cite{tan2019envdrop}
& 11.00 & 3.99 & -- & 62 & 59
& 10.70 & 5.22 & -- & 52 & 48
& 11.66 & 5.23 & 59 & 51 & 47 \\
PREVALENT~\cite{hao2020prevalent} $\dag$
& 10.32 & 3.67 & -- & 69 & 65
& 10.19 & 4.71 & -- & 58 & 53
& 10.51 & 5.30 & 61 & 54 & 51 \\
EntityGraph~\cite{hong2020graph}
& 10.13 & 3.47 & -- & 67 & 65
& 9.99 & 4.73 & -- & 57 & 53
& 10.29 & 4.75 & 61 & 55 & 52 \\
NvEM~\cite{an2021neighbor}
& 11.09 & 3.44 & -- & 69 & 65
& 11.83 & 4.27 & -- & 60 & 55
& 12.98 & 4.37 & 66 & 58 & 54 \\
AirBert~\cite{guhur2021airbert} $\dag$$\ddag$
& 11.09 & 2.68 & -- & 75 & 70
& 11.78 & 4.10 & -- & 62 & 56 
& 12.41 & 4.13 & -- & 62 & 57 \\
% SEvol~\cite{chen2022sevol}
% & 11.97 & 3.56 & -- & 67 & 63
% & 12.26 & 3.99 & -- & 62 & 57
% & 13.40 & 4.13 & -- & 62 & 57 \\
\vlnbert~\cite{hong2020recurrent} $\dag$
& 11.13 & 2.90 & -- & 72 & 68
& 12.01 & 3.93 & -- & 63 & 57
& 12.35 & 4.09 & 70 & 63 & 57 \\
MARVAL~\cite{kamath2022marval} $\dag$$\ddag$
& 10.60 & 2.99 & -- & 73 & 69
& 10.15 & 4.06 & -- & 65 & 61
& 10.22 & 4.18 & 67 & 62 & 58 \\
EnvMix~\cite{liu2021envmixup} $\dag$
& 10.88 & 2.48 & -- & 75 & 72
& 12.44 & 3.89 & -- & 64 & 58
& 13.11 & 3.87 & 72 & 65 & 59 \\
HAMT~\cite{chen2021hamt} $\dag$
& 11.15 & 2.51 & -- & 76 & 72
& 11.46 & 2.29 & -- & 66 & 61
& 12.27 & 3.93 & 72 & 65 & 60 \\
SnapEnsemble~\cite{qin2021ensemble} $\dag$$^{\circ}$
& -- & -- & -- & -- & --
& 12.05 & 3.63 & -- & 67 & 60
& 12.71 & 3.82 & -- & 65 & 60 \\
HOP+~\cite{qiao2023hop+} $\dag$
& 11.31 & 2.33 & -- & 78 & 73 
& 11.76 & 3.49 & -- & 67 & 61
& 12.67 & 3.71 & -- & 66 & 60 \\
TD-STP~\cite{zhao2022target} $\dag$
& -- & 2.34 & 83 & 77 & 73 
& -- & 3.22 & 76 & 70 & 63 
& -- & 3.73 & 72 & 67 & 61 \\
DUET~\cite{chen2022duet} $\dag$
& 12.32 & 2.28 & 86 & 79 & 73 
& 13.94 & 3.31 & 81 & 72 & 60
& 14.73 & 3.65 & 76 & 69 & 59 \\
BEVBert~\cite{an2022bevbert} $\dag$
& 13.56 & 2.17 & \blue{88} & \blue{81} & \textbf{74}
& 14.55 & 2.81 & 84 & 75 & 64
& 15.87 & 3.13 & 81 & 73 & 62 \\
\hline
DUET+\ours{} (ours) $\dag$$\ddag$
 & 11.90 & \textbf{2.16} & \textbf{87} & \textbf{80} & \blue{75}
 & 12.40 & \textbf{2.34} & \textbf{87} & \textbf{79} & \blue{70}
 & 14.27 & \textbf{2.73} & \textbf{83} & \textbf{77} & \textbf{68} \\
DUET*+\ours{} (ours) $\dag$$\ddag$
 & 13.24 & \blue{2.12} & \textbf{87} & \blue{81} & \blue{75}
 & 14.09 & \blue{2.09} & \blue{88} & \blue{81} & \blue{70}
& 13.93 & \blue{2.27} & \blue{86} & \blue{80} & \blue{70}\\
\hline\hline
\end{tabular}}
\vspace{-5pt}
\caption{Comparison of single-run performance on R2R dataset. $\dag$: Methods that apply vision-language-action pre-training. $\ddag$: Methods that use additional visual data than MP3D. $^{\circ}$: Model ensemble. *: Applying EnvEdit as image augmentation and CLIP ViT-H\/14 as image features.}
\label{tab:r2r_full}
\vspace{-5pt}
\end{table*}


\paragraph{REVERIE} 
We show in Table~\ref{tab:reverie_results} that our method achieves the new state-of-the-art results in all metrics on the REVERIE task. Our method surpasses AutoVLN, which uses all the 1000 HM3D environments for pre-training, by 0.94\% in success rate and 0.64\% in SPL on the test leaderboard with only 800 HM3D scenes and 491 Gibson low-quality environments. This again validates the effectiveness of our high-quality connectivity graphs and image recovery in our large-scale training paradigm. 


\paragraph{CVDN}

As shown in Table~\ref{tab:cvdn_sota_cmpr}, our method achieves the new state-of-the-art performance on the CVDN test-unseen split, which largely improves the goal progress (GP) of the previous SoTA by 1.41 meters (a relative gain of 25.26\%). This result shows that our R2R-style augmented data can generalize to a different VLN task with a distinct type of instructions, likely because visual scarcity is the major bottleneck in learning VLN, as suggested in Table~\ref{tab:quan_env_sample}.


\paragraph{R2R-CE}

Although our augmented \ours{} data only contains discrete instruction-trajectory pairs, it can benefit the agent's performance in continuous environments with the support of the candidate waypoint predictor~\cite{hong2022bridging} (Table~\ref{tab:r2rce_results}). Compared to the previous method, which applies very strong pre-trained visual representations~\cite{hong2023ego2map,wang2022internvideo}, our method still demonstrates obvious improvement, which reflects the effectiveness of generating in-domain data for learning VLN.


\begin{table}[t!]
  \begin{center}
  \resizebox{0.85\columnwidth}{!}{
\begin{tabular}{l|c|c|c} 
\hline \hline
 \multicolumn{1}{c|}{\multirow{2}{*}{Methods}} & \multicolumn{1}{c|}{Val-Seen} & \multicolumn{1}{c|}{Val-Unseen} & \multicolumn{1}{c}{Test-Unseen} \\
 
 \cline{2-4}& \multicolumn{1}{c|}{GP$\uparrow$} & \multicolumn{1}{c|}{GP$\uparrow$} & \multicolumn{1}{c}{GP$\uparrow$} \\
 \hline \hline
PREVALENT \cite{hao2020prevalent} & -- & 3.15 & 2.44 \\
MT-RCM+EnvAg \cite{wang2020environment} & 5.07 & 4.65 & 3.91 \\
NDH-Full~\cite{kim2021ndh} & - & 5.51 & 5.27 \\
HAMT~\cite{chen2021hamt} & 6.91 & 5.13 & 5.58  \\
MTVM~\cite{lin2022multimodal} & -- & 5.15 & 4.82 \\
\hline
DUET+\ours{} (Ours) & \textbf{8.13} & \textbf{6.12} & \textbf{6.97}  \\
\hline \hline
\end{tabular}}
\end{center}
\vspace{-10pt}
\caption{Navigation performance on CVDN dataset.}
\label{tab:cvdn_sota_cmpr}
\vspace{-8pt}
\end{table}



\begin{table}[t!]
  \begin{center}
  \resizebox{\columnwidth}{!}{
\begin{tabular}{l|cccc|ccc}
    \hline \hline
    \multicolumn{1}{c|}{\multirow{2}{*}{Methods}} &  
    \multicolumn{4}{c|}{R2R-CE Val-Unseen} &
    \multicolumn{3}{c}{R2R-CE Test-Unseen} \\
    \cline{2-8} & \multicolumn{1}{c}{NE$\downarrow$} &
    \multicolumn{1}{c}{nDTW$\uparrow$} &
     \multicolumn{1}{c}{SR$\uparrow$} & \multicolumn{1}{c|}{SPL$\uparrow$} & \multicolumn{1}{c}{NE$\downarrow$} & \multicolumn{1}{c}{SR$\uparrow$} & \multicolumn{1}{c}{SPL$\uparrow$} \Tstrut\\
    \hline \hline
    CMA~\cite{krantz2020navgraph}  & 7.37 & 40 & 32 & 30 & 7.91 & 28 & 25 \\
    LAW~\cite{raychaudhuri2021law} & -- & -- & 35 & 31 & -- & --  &  -- \\
    Waypoint Models~\cite{krantz2021waypoint}  & 6.31 & --  & 36 & 34  & 6.65  & 32 & 30 \\
    WS-MGMap~\cite{chen2022weakly} & 6.28 & - & 39 & 34 & 7.11 & 35 & 28 \\
    \vlnbert$\dag$~\cite{hong2022bridging}  & 5.74 & 53 & 44 & 39 & 5.89 & 42 & 36 \\
    Sim2Sim~\cite{krantz2022sim} & 6.07 & - & 43 & 36 & 6.17 & 44 & 37 \\ 
    \vlnbert+Ego$^2$-Map$\dag$~\cite{hong2023ego2map} & 4.94 & 60 & 52 & 46 & 5.54 & 47 & 41 \\     HAMT+InternVideo$\dag$~\cite{wang2022internvideo}  & 4.95 & 62 & 53 &  48 & -- & -- & -- \\
    \hline 
    HAMT+\ours{}$\dag$ (ours) & \textbf{4.80} & \textbf{64} & \textbf{55} & \textbf{51} & \textbf{5.11} & \textbf{55} & \textbf{50}  \\
    \hline \hline
  \end{tabular}}
\end{center}
\vspace{-10pt}
\caption{Navigation performance on the R2R-CE datasets. $\dag$: Methods that applies candidate waypoint predictor~\cite{hong2022bridging} to support high-level action space. }
\label{tab:r2rce_results}
\vspace{-10pt}
\end{table}
