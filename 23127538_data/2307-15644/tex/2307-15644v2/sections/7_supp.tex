% \clearpage
\section*{Appendices}

We first describe the implementation details of our experiments in Sec.~\ref{sec_1}, including pre-training objectives and details of REVERIE experiments. In Sec.~\ref{sec_2}, we provide additional experiments about the effects of visual encoders, model initialization, and adding depth features. We then discuss the impact of ScaleVLN on different VLN agents and on learning the long-horizon VLN task (R4R). Leaderboard results of R2R and object grounding results for REVERIE are also included. Sec.~\ref{sec_3} and Sec.~\ref{sec_4} visualize our navigability graphs and the recovered images from Co-Modulated GAN~\cite{zhao2021comodgan}.

\section{Implementation Details (\S4\protect\footnote{Link to Section 4 in Main Paper.})}

\label{sec_1}


\subsection{Pre-Training Objectives \texorpdfstring{($\boldsymbol{\S}$}~4.1)}


 We mainly employ three proxy tasks, MLM, MRM, and SAP, for pre-training the agent. Here we describe these proxy tasks in detail. The inputs for these tasks are instruction $\mathcal{W}$ and demonstration path $\mathcal{P}$. During training, we randomly sample one task for each iteration with equal probability.

\paragraph{Masked Language Modeling (MLM)} involves predicting masked words based on textual context and the full trajectory. A special \verb|[mask]| token is used to randomly mask out 15\% of the tokens in $\mathcal{W}$. We predict the masked word distribution $p (w_i|\mathcal{W}_{\backslash i}, \mathcal{P})=f_{\text{MLM}}(x'_i)$ through a two-layer fully-connected network, where $\mathcal{W}_{\backslash i}$ is the masked instruction and $x'_i$ is the output embedding of the masked word $w_i$. The objective is to minimize the negative log-likelihood of predicting the original words: $\mathcal{L}_{\text{MLM}} = - \mathrm{log}\ p (w_i|\mathcal{W}_{\backslash i}, \mathcal{P})$. 
% The task is helpful for learning grounded language representations.


\paragraph{Masked Region Modeling (MRM)} is to predict labels for masked regions in history observations based on instructions and neighboring regions.  To achieve this, we randomly remove view images in $\mathcal{P}$ with a 15\% probability. For view images, the target labels are determined by an image classification model~\cite{dosovitskiy2020vit} pre-trained on ImageNet. To predict semantic labels for each masked visual token, we use a two-layer fully-connected network. The objective is to minimize the KL-divergence between the predicted and target probability distribution. 
% Note that we only apply MRM in ablation studies in \texorpdfstring{$\boldsymbol{\S}$}~4.3 (Effect of Pre-training Tasks) as we found it doesn't help with large-scale training data.


\paragraph{Single Action Prediction (SAP)} aims to predict the next action based on the instruction and the given path. Following \cite{chen2022duet}, we predict the probability for each candidate action in the action space via a two-layer fully-connected network. The objective is to minimize the negative log probability of the target view action $\mathcal{L}_{\mathrm{SAP}} = -\mathrm{log}\ p_t(a^*_t | \mathcal{W}, \mathcal{P}_{<t})$. 
% This task helps the model to make action decision conditioning on instruction and contextual path history.


\subsection{Implementation Details of REVERIE \texorpdfstring{($\boldsymbol{\S}$}~4.1)}

% \paragraph{All Details about REVERIE}

REVERIE data contains trajectories that lead to target objects specified by high-level instructions. Following AutoVLN~\cite{chen2022hm3dlearning}, for every visible object at a viewpoint, we sample paths with an edge length between 4 and 9 that end at the viewpoint. We filter out objects that are more than 3 meters away from the central of the viewpoint, resulting in 518,233 paths from HM3D, and 311,976 paths from the Gibson environments.
To generate instructions in REVERIE-style, we modify the GPT-2 architecture used in AutoVLN~\cite{chen2022hm3dlearning} by only encoding the target object in the final viewpoint as the prompt to generate the instructions. Our large-scale data augmentation paradigm creates 830,209 instruction-trajectory pairs for training. This size is $\times$38 larger than the original REVERIE dataset, and $\times$3.81 larger than the augmented dataset in AutoVLN~\cite{chen2022hm3dlearning}.


We follow DUET and SIA~\cite{lin2021sia} to pre-train the model with an additional Object Grounding (OG) task, which requires selecting a target from object candidates based on high-level instruction and observations along the path. We use CLIP ViT-H/14~\cite{radford2021clip} to extract the image features, and ViT-B/16~\cite{dosovitskiy2020vit} pre-trained on ImageNet to extract the object features.
We pre-train DUET for 100k iterations with a batch size of 128 and a learning rate of $5\times 10^{-5}$ on both HM3D and Gibson environments. We compare three model checkpoints at 30k, 40k, and 50k and pick the one with the highest fine-tuning performance. Then we fine-tune DUET for 150k iterations, with batch size 32 and learning rate $2\times 10^{-5}$ on a single NVIDIA A100 GPU.


\section{Additional Experiments (\S4)}
\label{sec_2}

Here we provide additional experiments to investigate the effect of visual encoder, model initialization, and depth features. We also experiment with different model architectures (\textit{i.e.}, HAMT~\cite{chen2021hamt}) on R2R dataset, and show object grounding results for the REVERIE task.

\subsection{Effect of Visual Encoders (\texorpdfstring{$\boldsymbol{\S}$}~4.2)}

We study the effect of visual encoders in Table \ref{tab:visual_encoders.}. Here we adopt CLIP's ViT backbone with different model sizes and input patches (\textit{i.e.}, Base/16, Large/14, and Huge/14).
We can see that the vision encoder has a major influence on SPL, suggesting the agent can make fewer wrong steps and is capable of efficient navigation.




\begin{table}[h]
  \begin{center}
  \resizebox{\columnwidth}{!}{
  \begin{tabular}{l|cccc|rrrr}
    \hline \hline
     \multirow{2}{*}{Visual Encoders} & \multicolumn{4}{c|}{R2R Val-Seen} &\multicolumn{4}{c}{R2R Val-Unseen} \\
     \cline{2-9} & \multicolumn{1}{c}{TL}& \multicolumn{1}{c}{NE$\downarrow$} & \multicolumn{1}{c}{SR$\uparrow$} &
    \multicolumn{1}{c|}{SPL$\uparrow$} & \multicolumn{1}{c}{TL}& \multicolumn{1}{c}{NE$\downarrow$} & \multicolumn{1}{c}{SR$\uparrow$} &
    \multicolumn{1}{c}{SPL$\uparrow$} \Tstrut\\
    \hline \hline
    CLIP-ViT-B/16 &    12.41 & \textbf{2.02} & 80.51 & 74.88 &
      13.16 & 2.53 & 78.08 & 68.31 \\
    CLIP-ViT-L/14 & 12.62 & 2.16 & 80.04 & 74.06 & 13.13 & 2.50 & 78.08 & 68.97 \\
    CLIP-ViT-H/14 & 12.53 & 2.15 & \textbf{81.19} & \textbf{76.83} & 12.61 & \textbf{2.49} & \textbf{78.20} & \textbf{69.71} \\
    \hline \hline
  \end{tabular}}
\end{center}
\vspace{-5pt}
\caption{ Effect of visual encoders.}
\label{tab:visual_encoders.}
\end{table}

\subsection{Effect of Initialization (\texorpdfstring{$\boldsymbol{\S}$}~4.2)}

Table \ref{tab:language_init} presents the performance of initializing the navigation agent with different pre-trained models in pre-training. We discovered that utilizing BERT to initialize the language encoder does not enhance downstream performance, and even harms the performance on the validation unseen set. We attribute this to the vast domain gap between uni-modal BERT's language representations and CLIP's visual representation. Results could be improved by initializing the model with LXMERT's language encoder~\cite{tan2019lxmert}, and even more by utilizing both the language encoder and cross-modal encoder from LXMERT, indicating that incorporating pre-trained vision-and-language models could benefit agent performance.

\begin{table}[h]
  \begin{center}
  \resizebox{\columnwidth}{!}{
  \begin{tabular}{l|cccc|rrrr}
    \hline \hline
     \multirow{2}{*}{\makecell{Language Encoder \\ Initialization}} & \multicolumn{4}{c|}{R2R Val-Seen} &\multicolumn{4}{c}{R2R Val-Unseen} \\
     \cline{2-9} &  \multicolumn{1}{c}{TL}&\multicolumn{1}{c}{NE$\downarrow$} & \multicolumn{1}{c}{SR$\uparrow$} &
    \multicolumn{1}{c|}{SPL$\uparrow$} &  \multicolumn{1}{c}{TL}&\multicolumn{1}{c}{NE$\downarrow$} & \multicolumn{1}{c}{SR$\uparrow$} &
    \multicolumn{1}{c}{SPL$\uparrow$} \Tstrut\\
    \hline \hline
    Random & 12.87 & 2.29  & 78.75 & 72.61  & 12.69 & 2.72 & 75.65 &67.00  \\
    BERT & 12.43 &2.29 & 79.04 & 73.72 & 12.95 &2.76 & 75.01 & 66.57 \\
    LXMERT (lang.) & 11.73 & \textbf{2.07} & \textbf{80.22} & \textbf{75.65} & 13.17 & 2.67 & 75.86 & 67.36 \\
    LXMERT (lang.+cross.) & 12.63 & 2.27 & 79.24 & 73.34 &
    12.83 & \textbf{2.62} & \textbf{76.59} & \textbf{67.74} \\
    \hline \hline
  \end{tabular}}
\end{center}
\vspace{-5pt}
\caption{Effect of different initialization, where \textit{LXMERT (lang.)} means only initialize the language encoder with LXMERT, and \textit{LXMERT (lang.+cross.)} means initialize both the langauge encoder and cross modal encoder with LXMERT. }
\label{tab:language_init}
\end{table}

\subsection{Effect of Depth Modality (\texorpdfstring{$\boldsymbol{\S}$}~4.2)}

We also explored leveraging depth information to improve visual representations as described in Table \ref{tab:depth}. In line with previous methods such as \cite{krantz2020navgraph,krantz2021waypoint,hong2022bridging,an20221st}, we directly concatenate the depth features from DDPPO~\cite{wijmans2020ddppo} (a ResNet backbone pre-trained on PointGoal navigation with depth inputs) and the RGB features (from CLIP ViT-B/16) to create the visual representations. Our findings indicate that when not using HM3D as the augmented environment, the agent's SR is significantly better if learning from the additional depth input. However, this conclusion changes when HM3D environments are involved: the agent's SR with RGBD was slightly lower than with RGB-only. We suspect that as the data is scaled up with more visual observations and language instructions, the agent may not require additional depth information to assist decision-making.

\begin{table}[h]
  \begin{center}
  \resizebox{\columnwidth}{!}{
  \begin{tabular}{c|c|rrrr|rrrr}
    \hline \hline
     \multicolumn{1}{c|}{\multirow{2}{*}{HM3D Aug}} & \multicolumn{1}{c|}{\multirow{2}{*}{Sensor}} & \multicolumn{4}{c|}{R2R Val-Seen} &\multicolumn{4}{c}{R2R Val-Unseen} \\
     \cline{3-10} &  & \multicolumn{1}{c}{TL} & \multicolumn{1}{c}{NE$\downarrow$} & \multicolumn{1}{c}{SR$\uparrow$} &
    \multicolumn{1}{c|}{SPL$\uparrow$} & \multicolumn{1}{c}{TL} & \multicolumn{1}{c}{NE$\downarrow$} & \multicolumn{1}{c}{SR$\uparrow$} &
    \multicolumn{1}{c}{SPL$\uparrow$} \Tstrut\\
    \hline \hline
    \multirow{2}{*}{$\times$} & RGB & 13.28 & \textbf{2.51} & 76.89 & 69.71 &
     13.53 & 3.06 & 72.92 & \textbf{62.82} \\
     & RGBD & 14.16 & 2.54 & \textbf{77.18} & \textbf{69.76} &15.14 & \textbf{3.02} & \textbf{74.12} & 62.54 \\
    \hline
    \multirow{2}{*}{\checkmark} & RGB  & 12.63 & 2.27 & 79.24 & 73.34 &
     12.83 & \textbf{2.62} & \textbf{76.59} & 67.74 \\
     & RGBD & 11.24 & \textbf{2.12} & \textbf{79.73} & \textbf{75.45} & 12.93 & 2.63 & 76.46 & \textbf{68.52} \\
    \hline \hline
  \end{tabular}}
\end{center}
\vspace{-5pt}
\caption{ Effect of adding depth modality.}
\label{tab:depth}
\end{table}

\subsection{ScaleVLN with Different VLN Models (\texorpdfstring{$\boldsymbol{\S}$}~4.2)} 


To evaluate the generalization ability of our \ours{} dataset, we also apply the augmented data to train different VLN agents, including Seq2Seq~\cite{anderson2018r2r}, EnvDrop~\cite{tan2019envdrop}, and HAMT \cite{chen2021hamt}. 
The HAMT model is pre-trained and fine-tuned with the same data and configurations as we pre-trained the DUET model, while we follow similar configurations of Seq2Seq and Envdrop to the original papers. All three agents are trained with the CLIP ViT-B-16 feature. The results are shown in Table \ref{tab:scalevln+x_results}. Compared to using only PREVALENT~\cite{hao2020prevalent} for augmentation, All three models significantly benefit from incorporating the ScaleVLN dataset, with 12.2\%, 3.8\%, 5.5\% 
absolute increase in SR for Seq2Seq, EnvDrop, and HAMT, respectively. This shows that
ScaleVLN strengthens modelsâ€™ generalization ability.
Note that Seq2Seq and Envdrop perform better on Val-Seen when using PREVALENT, mainly caused by overfitting the training environments.

\begin{table}[h]
  \begin{center}
  \resizebox{\columnwidth}{!}{
  \begin{tabular}{l|c|c|rrr|rrr}
    \hline \hline
     \multicolumn{1}{c|}{\multirow{2}{*}{Model}} &\multicolumn{1}{c|}{\multirow{2}{*}{Pre-Train Data}} & \multicolumn{1}{c|}{\multirow{2}{*}{\makecell{Fine-Tune Data}}} & \multicolumn{3}{c|}{R2R Val-Seen} &\multicolumn{3}{c}{R2R Val-Unseen} \\
     \cline{4-9} & & & \multicolumn{1}{c}{NE$\downarrow$} & \multicolumn{1}{c}{SR$\uparrow$} &
    \multicolumn{1}{c|}{SPL$\uparrow$} & \multicolumn{1}{c}{NE$\downarrow$} & \multicolumn{1}{c}{SR$\uparrow$} &
    \multicolumn{1}{c}{SPL$\uparrow$} \Tstrut\\
    \hline \hline
         \multicolumn{1}{l|}{\multirow{2}{*}{Seq2Seq~\cite{anderson2018r2r}}}   &   - &     R2R, PREV    & \textbf{3.89} & \textbf{58.18} & \textbf{38.49} & 6.32 & 37.34 & 23.21 \\
    & - &     R2R, ScaleVLN  & 4.78 & 49.85 & 36.32  & \textbf{5.20} & \textbf{47.51} & \textbf{34.81} \\
         \hline
         \multicolumn{1}{l|}{\multirow{2}{*}{Envdrop~\cite{tan2019envdrop}}}   &    - &     R2R, PREV   & \textbf{3.65} & \textbf{66.12} & \textbf{61.72} & 4.41 & 59.22 & 52.35 \\
    & - &     R2R, ScaleVLN & 3.70 & 65.23 & 59.06 & \textbf{3.99} & \textbf{63.01} & \textbf{54.93} \\
    \hline
    \multicolumn{1}{l|}{\multirow{3}{*}{HAMT~\cite{chen2021hamt}}}   &    R2R, PREV &     R2R, PREV   & 2.58 & 74.93 & 71.52 & 3.69 & 64.90 & 60.11\\
    & R2R, PREV, ScaleVLN &     R2R & \textbf{2.15} & \textbf{79.53} & \textbf{76.64} & 3.43 & 67.56 & 62.32\\
     & R2R, PREV, ScaleVLN & R2R, ScaleVLN &  2.43 & 76.40 & 73.30 
     & \textbf{3.07} & \textbf{70.46} & \textbf{65.12} \\
    \hline \hline
  \end{tabular}}
\end{center}
\vspace{-5pt}
\caption{Influence of ScaleVLN on different VLN models.}
\label{tab:scalevln+x_results}
\end{table}

\subsection{ScaleVLN for Long-Horizon VLN (\texorpdfstring{$\boldsymbol{\S}$}~4.2)} 

We evaluate the impact of our dataset on a long-horizon VLN dataset, R4R~\cite{jain2019stay}. R4R extends the R2R dataset by concatenating two adjacent trajectories in R2R, resulting in longer navigation trajectories not biased by the shortest path prior. We directly fine-tune our pre-trained HAMT models from Table \ref{tab:scalevln+x_results} on R4R. Compared to pre-training with only R2R and PREVALENT, adding our \ours{} dataset in the pre-training stage leads to a  consistent gain, yielding +2.7\% SR, +1.5\% nDTW and +2.7\% SDTW~\cite{ilharco2019ndtw}. As suggested by the large improvement in nDTW between the ground-truth path and the executed path, our \ours{} data not only facilitate the model to reach the target but also follow the path described by the given instruction.

\begin{table}[h]
  \begin{center}
  \resizebox{\columnwidth}{!}{
  \begin{tabular}{c|c|rrrrr}
    \hline \hline
     \multicolumn{1}{c|}{\multirow{2}{*}{Pre-Train Data}} & \multicolumn{1}{c|}{\multirow{2}{*}{\makecell{Fine-Tune Data}}} &\multicolumn{5}{c}{R4R Val-Unseen} \\
     \cline{3-7}  & & \multicolumn{1}{c}{NE$\downarrow$} & \multicolumn{1}{c}{SR$\uparrow$} &
    \multicolumn{1}{c}{CLS$\downarrow$} & \multicolumn{1}{c}{NDTW$\uparrow$} &
    \multicolumn{1}{c}{SDTW$\uparrow$} \Tstrut\\
    \hline \hline
          R2R, PREV &     R4R   & 6.19 & 41.52 & 57.89 & 51.21 & 30.00  \\
    R2R, PREV, ScaleVLN &     R4R  & \textbf{6.09} & \textbf{44.20} & \textbf{59.55} & \textbf{52.77} & \textbf{32.73}  \\ 
    \hline \hline
  \end{tabular}
  }
\end{center}
\vspace{-5pt}
\caption{Effect of \ours{} on learning R4R.}
\label{tab:hamt_results}
\end{table}


\subsection{Leaderboard Results of R2R (\texorpdfstring{$\boldsymbol{\S}$}~4.4)}

We report the top seven submissions on the test-unseen leaderboard of R2R\footnote{R2R test server: \url{https://eval.ai/web/challenges/challenge-page/97/leaderboard/270}.} (Table~\ref{tab:leaderboard}). When ranking with success rate, we can see that (a) most methods have extremely low SPL (1\%) due to using beam search to find the optimal paths. Even so, our single-run result (\textit{EarlyToBed}) outperforms them by a large margin. When ranking with SPL (b), some methods pre-explored the test environments but their results are still much worse than ours. Apart from human followers, we are currently ranked first on the leaderboard.

\begin{table}[h]
\centering
% \def\arraystretch{1.2}
\begin{minipage}{0.49\linewidth}
    \centering
    % \setlength\tabcolsep{4.0pt}
    \resizebox{\textwidth}{!}{
        \begin{tabular}{lrrr}
         \hline \hline
        \multicolumn{1}{c|}{Team} & \multicolumn{1}{c}{NE$\downarrow$} & \multicolumn{1}{c}{SR$\uparrow$} & \multicolumn{1}{c}{SPL$\uparrow$} \\
            \hline \hline
            \multicolumn{1}{l|}{human} & 1.61 & 86 & 76 \\
            \hline
            \multicolumn{1}{l|}{\textbf{EarlyToBed} (ours)} & 2.27 & 80 & 70 \\
            \multicolumn{1}{l|}{LILY$^{\circ}$} & 2.54 & 78 & 1 \\
            \multicolumn{1}{l|}{Airbert$^{\circ}$} & 2.50 & 78 & 1 \\
            \multicolumn{1}{l|}{Shortest-Path-Prior$^{\circ}$} & 3.55 & 74 & 1 \\
            \multicolumn{1}{l|}{UU\_77} & 3.00 & 74 & 63 \\            
            \multicolumn{1}{l|}{TAIIC$^{\circ}$} & 2.99 & 74 & 1 \\
            \hline \hline
        \end{tabular}   
    }
    \small (a) Top 7 in SR.
\end{minipage}
%
\begin{minipage}{0.49\linewidth}
    \centering
    % \setlength\tabcolsep{4.0pt}
    \resizebox{\textwidth}{!}{
        \begin{tabular}{lrrr}
         \hline \hline
        \multicolumn{1}{c|}{Team} & \multicolumn{1}{c}{NE$\downarrow$} & \multicolumn{1}{c}{SR$\uparrow$} & \multicolumn{1}{c}{SPL$\uparrow$} \\
            \hline \hline
            \multicolumn{1}{l|}{human} & 1.61 & 86 & 76 \\
            \hline
            \multicolumn{1}{l|}{\textbf{EarlyToBed} (ours)} & 2.27 & 80 & 70 \\
            \multicolumn{1}{l|}{TAIICX$\dag$} & 3.00 & 73 & 69 \\
            \multicolumn{1}{l|}{Active Exploration$\dag$} & 3.30 & 70 & 68 \\
            \multicolumn{1}{l|}{sponge} & 3.26 & 71 & 67 \\ 
            \multicolumn{1}{l|}{Auxiliary Reasoning$\dag$} & 3.96 & 68 & 65 \\
            \multicolumn{1}{l|}{SE-Mixed} & 3.52 & 70 & 65 \\       
            \hline \hline
        \end{tabular}
    }
    \small (b) Top 7 in SPL.
\end{minipage}
\vspace{-0.2cm}
\caption{R2R leaderboard results (28.JUL.2023). $^{\circ}$: Beam search. $\dag$: Pre-exploration.}
\label{tab:leaderboard}
\end{table}


\subsection{REVERIE Object Grounding Result (\texorpdfstring{$\boldsymbol{\S}$}~4.4)}

We report the success rate of remote object grounding (RGS) and its path length-weighted result (RGSPL). As shown in Table~\ref{tab:reverie_obj_grounding}, \ours{} achieves state-of-the-art performance on object grounding task on the test leaderboard, comparable to the previous best method AutoVLN~\cite{chen2022hm3dlearning}.

\begin{table}[h]
  \begin{center}
  \resizebox{\columnwidth}{!}{
  \begin{tabular}{l|rrrr|rrrr}
    \hline \hline
     \multicolumn{1}{c|}{\multirow{2}{*}{Models}} & \multicolumn{4}{c|}{REVERIE Val-Unseen} &\multicolumn{4}{c}{REVERIE Test-Unseen} \\
     \cline{2-9} & \multicolumn{1}{c}{SR$\uparrow$}& \multicolumn{1}{c}{SPL$\uparrow$} & \multicolumn{1}{c}{RGS$\uparrow$} &
    \multicolumn{1}{c|}{RGSPL$\uparrow$} & \multicolumn{1}{c}{SR$\uparrow$}& \multicolumn{1}{c}{SPL$\uparrow$} & \multicolumn{1}{c}{RGS$\uparrow$} &
    \multicolumn{1}{c}{RGSPL$\uparrow$} \Tstrut\\
    \hline \hline
     % \\
    % \hline
    SIA~\cite{lin2021sia} & 31.53 & 16.28 & 22.41 & 11.56 & 30.80 & 14.85 & 19.02 & 9.20 \\ 
    HAMT~\cite{chen2021hamt} & 32.95 & 30.20 & 18.92 & 17.28 & 30.40 & 26.67 & 14.88 & 13.08 \\ 
    DUET~\cite{chen2022duet} & 46.98 & 33.73 & 32.15 & 23.03 & 52.51 & 36.06 & 31.88 & 22.06 \\ 
    AutoVLN~\cite{chen2022hm3dlearning} & 55.89 & 40.85 & \textbf{36.58} & \textbf{26.76} & 55.17 & 38.88 & 32.23 & 22.68 \\ 
    \hline
    DUET+\ours{}(ours) & \textbf{56.97} & \textbf{41.84} & 35.76 & 26.05 & \textbf{56.13} & \textbf{39.52} & \textbf{32.53} & \textbf{22.78} \\ 
    \hline \hline
  \end{tabular}}
\end{center}
\vspace{-5pt}
\caption{Object grounding performance on REVERIE.}
\label{tab:reverie_obj_grounding}
\end{table}


\section{Comparison of Navigability Graphs (\S3.2)}
\label{sec_3}

We visualize the navigability graphs produced by AutoVLN~\cite{chen2022hm3dlearning} and our method for several HM3D environments in Figure~\ref{fig:graph_com}. We can see that our graphs are denser, covering more regions, have viewpoints away from obstacles, and are fully traversable in open space.



\section{Recover High Quality Images (\S3.2)}
\label{sec_4}


As introduced in Main Paper \S3.2, we apply the Co-Modulated GAN~\cite{zhao2021comodgan} to recover the corrupted images rendered from the HM3D and Gibson environments. Specifically, we first render a panorama of shape 512$\times$1024 from the 3D mesh at each viewpoint. Then, we crop four images of shape 512$\times$512 centered at 0$^{\circ}$, 90$^{\circ}$, 180$^{\circ}$ and 270$^{\circ}$ of the panorama (with overlapping), and recover them separately. Note that, in VLN, the panoramic observation at a viewpoint is represented by 36 single-view images at 12 viewing angles and three elevations~\cite{anderson2018r2r}. We directly extract their corresponding regions from the four recovered images to obtain these single-view images for pre-training an agent.

Table~\ref{tab:recovered_envs} visualizes the difference between the rendered images and our recovered images. 
First, we can see that our method can recover missing regions, including outdoor scenes such as sky and trees (Example 1 \& 4) and indoor scenes such as floor and walls (Example 6).
Besides, the recovered images usually have less blurry or distorted areas, and the object boundaries are much clearer and sharper. For instance, the ceiling light in Example 2, the chairs in Example 3, and the door frames in Example 5.
Even for the highly corrupted images from Gibson (Examples 4--6), we can see that the method can still recover the scene to a reasonable quality.



% Figure environment removed

\begin{table*}[t]
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{cccc}
\hline \hline
Examples & 
Environments & 
\multicolumn{1}{c}{Rendered} &
\multicolumn{1}{c}{Recovered} \\ \hline \hline
 1 & HM3D &  \begin{minipage}{0.8\columnwidth}
   % Figure removed 
    \end{minipage} & \begin{minipage}{0.8\columnwidth}
      % Figure removed 
    \end{minipage}  \\
    \hline
2 &    HM3D &  \begin{minipage}{0.8\columnwidth}
   % Figure removed 
    \end{minipage} & \begin{minipage}{0.8\columnwidth}
      % Figure removed 
    \end{minipage}  \\
    \hline
 3 &   HM3D &  \begin{minipage}{0.8\columnwidth}
   % Figure removed 
    \end{minipage} & \begin{minipage}{0.8\columnwidth}
      % Figure removed 
    \end{minipage}  \\
    \hline
    4 &   Gibson &  \begin{minipage}{0.8\columnwidth}
   % Figure removed 
    \end{minipage} & \begin{minipage}{0.8\columnwidth}
      % Figure removed 
    \end{minipage}  \\
    \hline
    5 &   Gibson &  \begin{minipage}{0.8\columnwidth}
   % Figure removed 
    \end{minipage} & \begin{minipage}{0.8\columnwidth}
      % Figure removed 
    \end{minipage}  \\
    \hline
    6 &   Gibson &  \begin{minipage}{0.8\columnwidth}
   % Figure removed 
    \end{minipage} & \begin{minipage}{0.8\columnwidth}
      % Figure removed 
    \end{minipage}  \\
    \hline \hline
    \end{tabular}
    }
    \caption{Qualitative examples of our recovered images from HM3D and Gibson environments. The vertical line at the middle of panorama is caused by directly sticking two independently recovered images at 0$^{\circ}$ and 180$^{\circ}$, which will not appear in the resulting augmented data, as explained in Appendix \S\ref{sec_4}.}
    \label{tab:recovered_envs}
\end{table*}