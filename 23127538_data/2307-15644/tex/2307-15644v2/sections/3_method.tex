\section{Scaling Data for Learning VLN}
\label{sec:methods}

We outline the necessary resources for learning VLN, followed by the details of our method for creating the large-scale augmented dataset from additional environments. Note that in this section, we only present our method to generate instruction-path pairs in R2R-style, which will be shared to address downstream R2R~\cite{anderson2018r2r}, CVDN~\cite{thomason2020cvdn} and R2R-CE~\cite{krantz2020navgraph} tasks. We refer to the \textit{Appendix}~\ref{sec_1} for data collection and model training details for REVERIE, whose data requires trajectories that lead to a specific object~\cite{qi2020reverie}.

\subsection{Resources for VLN Training}  

Most existing research on VLN is established over the discrete Matterport 3D environments (MP3D)~\cite{chang2017matterport3d} where an agent's positions and observations are constrained on viewpoints of predefined navigation graphs. The trajectory-instruction pairs are sampled and annotated based on these discrete graphs. Compared to navigation in continuous environments~\cite{krantz2020navgraph,savva2019habitat}, such simplification enables efficient learning and execution while remaining to be practical, because, essentially, VLN agents make decisions by executing a vision-and-language grounding process~\cite{anderson2018r2r}. There are also some recent works that attempt to transfer agents designed for discrete scenarios to continuous environments~\cite{anderson2021sim,hong2022bridging,krantz2021waypoint,krantz2022sim}. Our data augmentation paradigm produces discrete supervisions, whereas we show in experiments that it also facilitates VLN learning in continuous scenes. In summary, scaling VLN data typically requires collecting new visual environments, discretizing the environments by building navigation graphs, sampling trajectories (sequences of images) on the graphs, and generating corresponding instructions. Following this procedure, we specify our data augmentation paradigm below.


% Figure environment removed


\subsection{Generating Augmented Data}

\paragraph{Collecting Environments}
We adopt environments from HM3D~\cite{ramakrishnan2021hm3d} and Gibson~\cite{xia2018gibson} as the source of our visual data. Both datasets contain abundant, traversable, and simulated indoor 3D scans collected from real-world buildings, which support the learning of various visual navigation problems~\cite{batra2020objectnav,krantz2020navgraph,mezghani2021imagenav,savva2019habitat}. Specifically, we employ 800 training scenes from HM3D, and 491 training and validation scenes from Gibson (same as MARVAL~\cite{kamath2022marval}), resulting in more than 150k $m^2$ navigable area, which is around $\times$7.5 times larger than the training scenes of downstream MP3D environments (20k $m^2$, 61 scans).


\paragraph{Constructing Navigation Graphs}
We argue that a high-quality navigation graph needs to satisfy a number of criteria, including high coverage of the space to maximize visual diversity and fully traversable edges in appropriate lengths with nodes positioned close to the center of open space for sampling reasonable trajectories. Previous work AutoVLN~\cite{chen2022hm3dlearning} builds graphs with very sparse nodes and with edges that go across obstacles, limiting the quantity of sampled data and leading to impractical trajectories, while MARVAL~\cite{kamath2022marval} trains a model to predict navigable directions, which could make errors and overcomplicate the problem. In this work, we propose a very simple but accurate heuristic for building the graph: we first apply the existing navigable position sampling function in Habitat simulator~\cite{savva2019habitat} to sample an excessive amount of viewpoints which almost covers the entire open space while limiting the geodesic distance between any two viewpoints to be greater than 0.4 $m$. Then, we apply the Agglomerative Clustering algorithm to group adjacent viewpoints to a single viewpoint with a distance threshold of 1.0 $m$, automatically producing positions close to the center of open space. We create a rough graph by randomly connecting viewpoints within 5.0 $m$ separation while capping the maximal edges of a viewpoint to be five, and use the existing graph refinement approach~\cite{hong2022bridging} to obtain the final fully-connected and fully-traversable navigation graphs. We use this method to construct graphs for the 800+491 environments; the average edge length in the graphs is 1.41 $m$, and the average node degree is 4.55. We visualize the graphs in \textit{Appendix}~\ref{sec_3}.

\paragraph{Recovering Faulty Rendered Images} 
Although HM3D and Gibson provide a large amount of diverse indoor environments, the quality of the images rendered from their 3D meshes is often much worse than the camera-captured images (as shown in Figure~\ref{fig:overall_pipeline}). Previous work has shown that navigation agents trained with rendered views will perform significantly worse than agents trained with high-quality images~\cite{krantz2022sim}. As a result, we consider recovery of faulty rendered images as a process in our \ours{} paradigm.

We formulate this task as an image-to-image translation problem, where the model takes a rendered image as input and learns to recover the broken, distorted, or missing regions. Specifically, we adopt the Co-Modulated GAN (Co-Mod GAN)~\cite{zhao2021large}, a generative model that can leverage conditional information and retain the stochastic in unconditional generation. 
% The modulation in the generative decoder is conditioned on the input-rendered image $r$ and a latent vector $z$. We use a CNN-based image encoder to map the rendered image to the feature space and a multi-layer fully connected network to encode latent vector $z$. Lastly, we apply affine transformation over encoded rendered image and latent space. 
We train Co-Mod GAN on the rendered-and-camera-image pairs in Matterport3D datasets and use the trained model to recover the rendered images in HM3D and Gibson environments.



\paragraph{Sample Trajectories}

We sample trajectories on the navigation graphs of HM3D and Gibson environments. For navigation tasks with detailed instructions, we follow PREVALENT~\cite{hao2020prevalent} and collect all possible shortest routes between any two viewpoints connected by three to five intermediate nodes. This sampling strategy yields a total of 2,890,267 paths and 2,051,443 paths for the HM3D and Gibson environments, respectively.


\paragraph{Generate Navigational Instructions}

Finally, we apply the off-the-shelf EnvDrop Speaker~\cite{tan2019envdrop}, a simple LSTM-based language generation model trained on instruction-path pairs in R2R~\cite{anderson2018r2r}, to produce a navigational instruction for each sampled trajectory for navigation tasks with detailed instructions. Compared to the more powerful language model GPT-2~\cite{radford2019gpt2}, EnvDrop Speaker generates less diverse descriptions, but the resulting data can lead to similar improvement on agents addressing R2R task (see \S\ref{subsec:scale_data}).

\vspace{5pt}

Following the procedure above, our large-scale data augmentation paradigm creates 4,941,710 instruction-trajectory pairs for learning VLN. This size is $\times$352 larger than the R2R dataset and $\times$4.62 larger than the commonly applied augmented PREVALENT dataset~\cite{hao2020prevalent}.