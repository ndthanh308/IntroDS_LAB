\section{Introduction}
\label{sec:intro}

Vision-and-Language Navigation (VLN)~\cite{anderson2018r2r} is a challenging task that requires an agent to navigate in photo-realistic environments, following human natural language instructions such as ``\textit{Walk downstairs, move towards the dining table, turn left to the kitchen, and stop in front of the fridge.}'' Addressing VLN relies heavily on correctly interpreting the instructions, perceiving the environments, and learning from interaction, which demands a large amount of diverse visual-language data for learning. Recent research shows that scaling up the diversity of environments and the quantity of demonstration for training VLN agents are promising in improving generalization to unseen scenes~\cite{chen2022hm3dlearning,kamath2022marval}. Compared to previous approaches of addressing data scarcity by augmenting agent's observations~\cite{li2022envedit,tan2019envdrop} or employing large vision-linguistic models pre-trained with image-text data from the web~\cite{guhur2021airbert,hong2020recurrent,majumdar2020improving,shah2022lmnav,shen2021benefit}, utilizing additional traversable environments allows the agents to learn from in-domain visual-language data and physical interaction in the space.


% Figure environment removed


In light of this, recent large datasets which contain hundreds of interactive scenes have been created~\cite{deitke2022procthor,ramakrishnan2021hm3d,xia2018gibson}, as well as a vast amount of human demonstrations have been collected~\cite{anderson2020rxr,ramrakhya2022habweb} for learning visual navigation, leading to significant improvement in agent's performance.
However, the process towards such large-scale training involves solving a series of key sub-problems such as how to build navigation graphs~\cite{anderson2018r2r,hong2022bridging,kamath2022marval}, how to recover corrupted rendered images~\cite{anderson2020sim,krantz2022sim}, and how to generate navigational instructions~\cite{dou2022foam,fried2018speaker,wang2022less,wu2021improvedspeaker}, which significantly influence the quality of collected data and should be investigated thoroughly. Meanwhile, an agent capable of understanding human natural language and navigating in photo-realistic environments is a complex and modularized system~\cite{an2022bevbert,chen2022sevol,chen2022duet,hong2023ego2map,wang2021structured,wang2019reinforced,zhou2023navgpt,zhu2021scoa}, and it is important to study how to effectively utilize the large-scale data to benefit the training of navigational agents adequately.


In this paper, we propose an effective paradigm for large-scale vision-and-language navigation (VLN) training and quantitatively evaluate the influence of each component in the pipeline. Specifically, we utilize environments in both the HM3D~\cite{ramakrishnan2021hm3d} and the Gibson~\cite{xia2018gibson} datasets, build navigation graphs for the environments based on the Habitat simulator~\cite{savva2019habitat}, sample new trajectories and generate corresponding instructions~\cite{tan2019envdrop}, and train agents~\cite{chen2021hamt,chen2022duet} for solving downstream navigation tasks~\cite{anderson2018r2r,jain2019stay,krantz2020navgraph,qi2020reverie,thomason2020cvdn}. Different from previous methods such as AutoVLN~\cite{chen2022hm3dlearning} and MARVAL~\cite{kamath2022marval}, we build navigation graphs using an excessive viewpoint sampling and aggregation algorithm, following the graph construction heuristic proposed in~\cite{hong2022bridging}, which results in fully-connected graphs with high coverage in open space. Additionally, we address the issue of corrupted rendered images from HM3D and Gibson environments with the Co-Modulated GAN~\cite{zhao2021comodgan}, which we train to generate photo-realistic images from the faulty rendered images with broken, distorted, or missing regions, to mitigate the noise in visual data.
Unlike MARVAL, which uses a non-public language generation model Marky~\cite{wang2022less} and visual encoder MURAL~\cite{jain2021mural}, as well as synthesizes observations from novel viewpoints with an image-to-image GAN~\cite{koh2022simple}, our large-scale training regime is fully reproducible and straightforward to execute, while leading to a significant improvement on agent's performance.

Through comprehensive experiments, we find that a fully traversable navigation graph is crucial to improve the agent's performance for downstream tasks with detailed instructions like R2R. Besides, we show that recovering photo-realistic images from the rendered images is very beneficial, especially for the low-quality 3D scans from the Gibson environments. Results also suggest that an agent can consistently benefit from having more diverse visual data, and learning from additional scenes helps agents to generalize better to unseen environments than simply learning from more data. Moreover, we validate that an agent trained with augmented instructions generated by a simple LSTM-based  model~\cite{tan2019envdrop} can achieve good performance on multiple navigation tasks ~\cite{anderson2018r2r,qi2020reverie,thomason2020cvdn}. Last but not least, we find that appropriately combining our augmented data with the original data in pre-training and fine-tuning can benefit the agent's generalization ability. 

Remarkably, by following the above analysis as data augmentation and agent training guidelines, our resulting VLN model achieves 80\% success rate (SR) on the R2R test split by simple imitation learning without pre-exploration~\cite{fu2020counterfactual,tan2019envdrop,zhu2020vision}, beam search~\cite{fried2018speaker,majumdar2020improving,xie2022beam} or model ensembling~\cite{qin2021ensemble}, and successfully eliminates the gap between navigating in seen and unseen environments. This result significantly outperforms previous best method (73\%)~\cite{an2022bevbert}, 
% and for the first time, approaching 
and reduces the difference towards human performance (86\% SR\footnote{Note that human followers only have egocentric views, while our model follows the common approach of applying panoramic observations.})~\cite{anderson2018r2r} to 6\%. Our method also achieves new state-of-the-art results on different language-guided visual navigation problems, including CVDN~\cite{thomason2020cvdn} and REVERIE~\cite{qi2020reverie}. 
Moreover, although the augmented data is discrete, it helps boost VLN performance in continuous environments (R2R-CE)~\cite{an20221st,hong2022bridging,krantz2020navgraph}, a much more realistic but difficult scenario, by 5\% SR. All the results demonstrate the great effectiveness and generalization potential of our training regime. 
In summary, our main contributions include:
\begin{enumerate}
[itemsep=0.1em,parsep=0em,topsep=0em,partopsep=0em,leftmargin=2em]
    \item A simple, effective, fully automated and reproducible large-scale training paradigm for vision-and-language navigation.
    \item Comprehensive analysis of the entire data augmentation pipeline and utilizing the large data for training.
    \item New state-of-the-art results on navigation tasks including R2R, CVDN, REVERIE, and R2R-CE.
\end{enumerate}


%% ----------------------------------------
