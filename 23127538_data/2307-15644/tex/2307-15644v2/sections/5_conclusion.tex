\section{Conclusion}
\label{sec:conclude}
\vspace{-6pt}
In this paper, we introduce a simple but effective large-scale data generation paradigm for learning vision-and-language navigation (\ours{}). The method applies thousands of photo-realistic environments from HM3D and Gibson datasets, and creates millions of instruction-trajectory pairs for training. Apart from the unsurprising improvement of learning from abundant visual data in agent performance, we demonstrate the effectiveness of building high-quality navigation graphs and using camera-quality images through comprehensive experiments. Moreover, we investigate how to properly utilize the augmented data in pre-training and fine-tuning an agent, as well as the influence of different pre-training tasks on the downstream navigation results. By following our findings as data augmentation and agent training guidelines, we achieve new state-of-the-art results on several VLN benchmarking datasets that cover distinct styles of instructions (R2R, REVERIE, CVDN) and action spaces (R2R-CE). We believe our \ours{} paradigm can be easily applied as a tool to facilitate data augmentation for VLN and other visual navigation problems, and the experiments presented in the paper can provide useful insights for future research in creating and utilizing large-scale data. 

