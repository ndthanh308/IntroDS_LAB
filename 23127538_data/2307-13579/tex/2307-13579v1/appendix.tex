\section{The MONDE and SuMo networks}
\label{sec:monde_sumo}
We begin with the MONDE network~\cite{chilinski2020neural}, which allows one to parameterize a mapping
\begin{equation}
    M:\mathbb{R}^m \ni z \mapsto M(z) \in \mathbb{R}
\end{equation}
that is guaranteed to be monotonically increasing. It is a feed-forward neural network with each layer being of the form
\begin{equation}
    z_{k+1} = \varphi_k\left(W_kz_k\right).
\end{equation}
Here $z_0=z$ and $W_k$ is an affine map with the weights of the linear part constrained to be non-negative. For the last layer, outputting a scalar, they set $\varphi_K=\mbox{id}$, otherwise $\varphi_k=\tanh$.

The SuMo network~\cite{rindt2022survival} consists of MONDE and a feature-extracting network
\begin{equation}
    Q: \mathbb{R}^n \ni x \mapsto Q(x) \in \mathbb{R}^{k}
\end{equation}
combined with
\begin{equation}
    \mathscr{S}: \mathbb{R} \times \mathbb{R}^n \mapsto [0,1]
\end{equation}
to give
\begin{equation}
    \mathscr{S}(t, x) = 1 - \mbox{sigmoid}\left(M([t, Q(x)])\right).
\end{equation}
Here $[t, Q(x)]$ denotes the concatenation of $t$ and $Q(x).$ As $M$ and the $\mbox{sigmoid}$ function are monotonically increasing, the SuMo network, $\mathscr{S}(t, x)$, is monotonically decreasing in $t$.

As discussed in~\cite{rindt2022survival, chilinski2020neural}, the SuMo network is not only monotonically decreasing but is also a universal approximator of survival curves. 
However, as there is no guarantee that $\mathscr{S}(0, x)=1$ for all $x \in \mathbb{R}^n$, it does not necessarily produce survival curves. Note, for the SuMo model, $\mathscr{S}(0, x)=1$ is even impossible, as $\mbox{sigmoid}(\cdot) > 0.$

\section{Initialization of MONDE and MONDE+}
\label{sec:initalization}
We used Optuna~\cite{akiba2019optuna} to run $1,000$ hyperparameter optimization steps on the task in Section~\ref{sec:toy_example} to optimize the scale and sign of the random initializations of MONDE and MONDE+. This was motivated by the initialization's strong influence demonstrated by the histograms in Figure~\ref{fig:hists}.

\subsection{Default MONDE initialization}
Following~\cite{rindt2022survival}, the default MONDE initialization initializes both the weight matrix and the bias with Gaussian noise with mean zero. The standard deviation used in the initial Gaussian noise for the  weight matrix is $\sqrt{\text{input size}}$ and for the biases $\sqrt{\text{output size}}$ is used.

\subsection{Hyperparameter optimization for MONDE}
Here we take the absolute value of the default weights described above and scale them by a factor from the interval $[-10,10]$. Performing 1000 hyperparameter optimization steps yields the factor $4.6$ for the matrix and $6.6$ for the bias.

The hyperparameter objective function was given by the maximum loss of three training runs, as described in Section~\ref{sec:toy_example}. We took the maximum loss of three runs in the hope of improving the robustness to ``unlucky'' initializations, as shown by the histograms in Figure~\ref{fig:hists}.

\subsection{Hyperparameter optimization for MONDE+}
For the convenience of the reader, we recall the definition of a MONDE+ layer to be
\begin{equation}
    z_{k+1}(t, z_k, z_0) = H_kz_k + \sigma_k(\tilde z_k(t, z_k) + B_kz_k + L_kz_0)
\end{equation}
with
\begin{equation}
    \tilde z_k(t, z_k) = A_k\left(\phi_k(a_kt + b_k) \circ \psi_k(G_kz_k)\right)
\end{equation}
where the capital letters denote affine maps and $\circ$ denotes the Hadamard product.

For simplicity we initialized $a_k\in\mathbb{R}^{64}$ with the uniform distribution between $0$ and $1$ and $b_k\in\mathbb{R}^{64}$ to be zero.

The initialization of all matrices and biases in the affine maps was optimized before training the models. We first initialized with the Kaiming uniform initialization~\cite{he2015delving} as implemented by PyTorch~\cite{paszke2017automatic}. With the exception of $L_k$, we took their absolute values and scaled them by a factor (the hyperparameter) between $[0, 10]$, as these matrices had to be non-negative.

As in the MONDE case, we also scaled the biases as purely non-negative or non-positive. We did so to mediate the following effect. Some of the matrices need to be non-negative, which, as we found, can lead to impaired training performance. This is likely due to a substantial distribution shift in the activations over the course of multiple layers making the optimization landscape less smooth~\cite{ioffe2015batch, santurkar2018does}. It seems that having accordingly biased biases can counter this effect to some degree.

We again optimized the same hyperparameter objective as in the MONDE case above for 1000 optimization steps. For simplicity, we optimized the factors of the matrices of all affine mappings as one, which yielded the scaling factor $0.2$. The resulting factors for the biases were $-8.5$ for $B_k$, $10$ for $G_k$, the bias of $H_k$ given by the Kaiming initialization was used, and for $A_k$ we removed the bias entirely, as it would be redundant.


\section{Proof of Lemma~\ref{lemma:monde_plus}}
\label{sec:monde_proof}
As the concatenation of monotonically increasing functions is monotonically increasing, it suffices to prove the monotonicity of a single layer.

We will prove monotonicity in $t$ by showing that the derivative of the output of a layer
\begin{equation}
    z_{k+1}(t, z_k, z_0) = H_kz_k + \sigma_k(\tilde z_k(t, z_k) + B_kz_k + L_kz_0)
\end{equation}
is positive in\ $t$ and $z_k$.

First, we have that
\begin{align}
    \begin{split}
        \partial_t \tilde z_k(t, z_k)
        &=  \overline A_k\partial_t \left[\phi_k(a_kt + b_k) \circ \psi_k(G_kz_k)\right]\\
        &=  \overline A_k \{\partial_t[\phi_k(a_kt + b_k)] \circ \psi_k(G_kz_k) \\& + \phi_k(a_kt + b_k) \circ \partial_t[\psi_k(G_kz_k)]\}\\
        &=  \overline A_k \{[\nabla\phi_k](a_kt + b_k) \circ a_k \circ \psi_k(G_kz_k) \\& + \phi_k(a_kt + b_k) \circ \partial_t[\psi_k(G_kz_k)]\}\\
        &=  \overline A_k \{[\nabla\phi_k](a_kt + b_k) \circ a_k \circ \psi_k(G_kz_k) \\& + \phi_k(a_kt + b_k) \circ [\partial_t\psi_k](G_kz_k) \circ \overline G_k\partial_tz_k\}\\
    \end{split}
\end{align}
and
\begin{align}
    \begin{split}
        \partial_{z_k} \tilde z_k(t, z_k)
        &= \overline A_k\partial_{z_k} \left[\phi_k(a_kt + b_k) \circ \psi_k(G_kz_k)\right]\\
        &= \overline A_k \left[\phi_k(a_kt + b_k) \circ \partial_{z_k}[\psi_k(G_kz_k)\right]\\
        &= \overline A_k \left[D\{\phi_k(a_kt + b_k)\} D\{(\nabla \psi_k)(G_kz_k)\} \overline G_k\right]\\
    \end{split}
\end{align}
where $D\{c\}$ denotes the diagonal matrix associated with a vector $c$. Also, $\overline A_k$ and $\overline G_k$ are the linear parts of the affine operators $A_k$ and $G_k$ respectively. As they only have non-negative weights in their linear parts and the non-linearities, $\phi_k$, and $\psi_k$, are non-negative and monotonically increasing, both derivatives are pointwise non-negative.

Using this, we can compute the corresponding derivates of $z_k.$
We have
\begin{align}
    \begin{split}
        &\partial_t z_{k+1}(t, z_k, z_0)
        = H_k\partial_tz_k + (\nabla\sigma_k)(\tilde z_k(t, z_k) \\&+ B_kz_k + L_kz_0) \circ (\partial_t\tilde z_k(t, z_k) + B_k\partial_tz_k)
    \end{split}
\end{align}
and
\begin{align}
    \begin{split}
        &\partial_{z_k} z_{k+1}(t, z_k, z_0)
        = H_k + D\{(\nabla\sigma_k)(\tilde z_k(t, z_k)\\ &+ B_kz_k + L_kz_0)\} (\partial_{z_k}\tilde z_k(t, z_k) + B_k).
    \end{split}
\end{align}
For the same reasons as above, every term in these expressions is pointwise non-negative. $\square$

\section{Proof of Lemma~\ref{lemma:cOx}}
\label{sec:proof_cOx}
We begin by proving the first two points.
As $\alpha(t,x),\Lambda_0(t) \ge 0$ for all $(t,x)\in\mathbb{R}_{\ge0}\times\mathbb{R}^n$, it follows that $\mathscr{S}_\cOx(t, x)\in[0,1]$. Since $\Lambda_0(0)=0$, we have the maxima $\mathscr{S}_\cOx(0, x)=1$.

We will now show that $\mathscr{S}_\cOx$ is monotonically decreasing in $t$.
As the $\beta_i^\pm$ are monotonically increasing, the $\omega_i$ are monotonically increasing in $t$. Therefore $\alpha$ is monotonically increasing in $t$, which means $\alpha\Lambda_0$ is monotonically increasing in $t$ and therefore $\mathscr{S}_\cOx$ is monotonically decreasing in $t$.

The continuity in $t$ of $\omega_i$ follows from the continuity of the functions involved. The minima at $\omega_i(0, x)=\beta_i^+(0)$ follows from the fact, that $\beta_i^-(0)$ and $\beta_i^+(0)$ are both monotonically increasing and $\beta^-(0)=\beta^+(0)$.

We will now prove the continuity of $\alpha$ in $(t, x)$. We can reduce this problem to the continuity of each separate
\begin{equation}
    \omega_i(t, x)|x_i-o_i|.
\end{equation}
As all functions involved are otherwise continuous, we only need to show continuity at $x_i=o_i.$ Considering the limits of each of the two factors completes the proof. $\square$

\section{Datasets and preprocessing}
\label{sec:datasets_appendix}
For the NKI dataset, we reduced the number of features to make it accessible to classical survival models and easier to handle in general. We did so by only retaining only 9 of the features with a high correlation with survival time and observed outcome. This left us with the features: NM\_004701, NM\_001168, NM\_003430, grade, NM\_003981, AL050227, NM\_018410, NM\_000779, and NM\_001809.

We assembled the COVID-19 dataset from COVID-19 patient data from Addenbrooke's hospital in Cambridge, UK. We used the data of all patients on the day of their first positive COVID-19 test. We then used the following features of that day: age, gender, DNA CPR, median ventilation score over 24 hours, SpO2, pulse, temperature, respiration, NEWS2 score, O2 flow rate, FiO2, weight, height, BMI, Glasgow coma scale score, and SpO2/FiO2~\cite{wiegand2022development}. We used these variables either due to their ready availability or because they were shown to be useful in previous work~\cite{wiegand2022development}. We gathered their survival time via their death records. We assumed we had no censoring within the first 28 days and then marked all other patients as censored on the 28th day. Exact censoring was hard to perform, but the hospital transferred only a small number of ICU patients, therefore, we believe that for the analysis in this paper, censoring is a negligible problem in this datasets.

To avoid numerical problems for the classical models, we also divided the times of the GBSG2 and Lymph datasets by $100$ and those of the Recur dataset by $10$. Except for standardizing the mean and standard deviation to 0 and 1, respectively, we did not use any other preprocessing.

In addition to the original sources, one can find summaries of the Lymph, Recur, and GBSG2 datasets in the \textit{lifelines} documentation~\cite{davidson_pilon_cameron_2022_7111973}.

\section{Training details}
\label{sec:training_details}
\subsection{Training}

We used \textit{lifelines}~\cite{davidson_pilon_cameron_2022_7111973} to train the classical models, i.e. Kaplan-Meier \cite{kaplan1958nonparametric}, Log-Logistic \cite{bennett1983log}, Log-Normal \cite{gamel1994stable}, Weibull \cite{lai2006weibull}, and two Cox models \cite{cox2018analysis} (one based on a piecewise constant \cite{crowley1984statistical} and one on a spline \cite{efron1988logistic, royston2002flexible} baseline.)

For all the neural network models, we trained these using \textit{PyTorch}~\cite{paszke2017automatic}. As the network training is stochastic, we train each network setup five times and pick the best model based on the mean score over all evaluation metrics on the validation set.
For the neural networks, we also compare our BCE loss from~\eqref{eq:bce_loss} with the survival loss from~\cite{rindt2022survival}, namely
\begin{equation}
    L_\text{SuMo} = -\left[e \log f(t|x) + (1 - e) \log S(t|x)\right].
\end{equation}
For the convenience of the reader we recall $f = -\partial_t S(t|x)$. Following common practice, we will use a weighted version of the BCE in the BCE loss. To ensure fair comparability, we therefore introduce a hyperparameter $\gamma>0$ in the SuMo loss defining
\begin{equation}
    L_\text{SuMo}^\gamma = -\left[e \gamma\log f(t|x) + (1 - e) \log S(t|x)\right].
\end{equation}

For every dataset, except Clocks, we trained the networks CoxNN, \cOx NN, CoxDeepNN, SuMo, SuMo+, and SuMo++. For the Clocks dataset, we train SuMo, SuMo+, and SuMo++ as well as the CoxDeepNNs with the convolutional feature network and the Kaplan-Meier model. The CoxNN is based on Section~\ref{sec:cox_simple} and the \cOx NN is based on Section~\ref{sec:cox_time_dependent}. The CoxDeepNN is also based on Section~\ref{sec:cox_simple}, but the linear scalar product in~\eqref{eq:scalar_product_cox} is replaced by a deep feature network as independently proposed by~\cite{katzman2018deepsurv, kvamme2019time} and~\cite{shahin2022survival}. For the exact architecture of the feature networks of the CoxDeepNN and SuMo, SuMo+, and SuMo++, see the following section.

During our numerical experiments, we found that the training of any Cox-like model via $L_\text{SuMo}$ tends to diverge, especially if the training data is predominately non-censored. In those cases, the training with $L_\text{BCE}$ tends to be more stable but occasionally also diverges. We could ameliorate these issues by employing weight decay for all Cox-like models during all trainings.

All hyperparameters and training details for the different datasets are detailed in appendix Section~\ref{sec:app_training}.


\subsection{SuMo's, SuMo+'s, and SuMo++'s dense feature network}
All feature networks are simple feed-forward networks. SuMo, SuMo+, and SuMo++ have three dense layers of width 32, each followed by a ReLU activation and a layer-normalization.

\subsection{SuMo's, SuMo+'s, and SuMo++'s convolutional feature network}
\label{sec:sumo_feature_d}
In the convolutional setting, i.e., for the Clocks dataset, SuMo, SuMo+, and SuMo++ have three 2-dimensional convolution layers, each with 128 channels, a kernel size of 3, and a padding of 1. Each is followed by a max-pooling layer with a kernel size of 3, a stride of 2, and padding of 1. Each of these pooling layers is followed by a ReLU activation and a layer normalization. These three layer-blocks are followed by a dense layer of width 48, a ReLU, a layer-normalization, another dense layer of width 48, and a final ReLU.

\subsection{CoxDeepNN's dense feature network}
The network begins with two dense layers, the first with a width of twice the number of input features, see Table~\ref{tab:dataset_overview} and the second with a width of 8. Each of the two layers is followed by a layer normalization and a ReLU. Finally, the last layer is a dense layer of width 1.
 
\subsection{CoxDeepNN's convolutional feature network}
This network is the same as the one in Section~\ref{sec:sumo_feature_d}, except that the $\exp$ function replaces the final ReLU layer.

\subsection{Hyperparameters}
\label{sec:app_training}
For all networks, we use the Adam optimizer~\cite{kingma2014adam} with a step-size of $10^{-3}$, gradient clipping of $1$, and batch size of $8$. After each batch, we also evaluate the loss of a batch from the validation set. We stopped the training when the $512$-moving average over these validation batches did not decrease for $8192$ batches or after at most $200000$ training steps. 
With the exception of the Clocks dataset, which we trained on an Nvidia GeForce GTX 1080 Ti, we trained all models on the CPU. Each training took approximately $5$ to $90$ minutes.

For each dataset, we optimized the following hyperparameters with Optuna~\cite{akiba2019optuna} by evaluating the results of $100$ short training runs, each only $1024$ training steps long:
\begin{itemize}
    \item $\gamma$ for the $L_\text{SuMo}^\gamma$ loss over $[0,10]$.
    \item The BCE weight $L_\text{BCE}$ loss over $[0,1]$.
    \item The $\sigma$ for the $L_\text{BCE}$ loss indirectly via the factor $\sigma_\text{factor}\in[0,1]$ and $\sigma = \sigma_\text{factor} T_\text{max}$.
    \item One joint weight decay parameter for all Cox-like models in the case of the $L_\text{SuMo}^\gamma$ loss over $[0,1]$ and a separate one for the $L_\text{BCE}$ loss.
\end{itemize}
For the resulting parameters, see Figure~\ref{fig:hyperparamters}. We employed the mean of the concordance index and all integrated scores, except the integrated Brier score, as the loss function. The integrated Brier score was incorporated at a later stage of the paper. We determined appropriate ranges for the hyperparameters through a series of preliminary experiments.
\begin{table*}%[!ht]
    \centering
    \begin{tabular}{|c||c|c|c|c|c|}
        \hline
        Dataset & $\gamma$ & BCE weight & $\sigma_\text{factor}$ & weight decay for SuMo & weight decay for BCE\\
        \hline
        GBSG2 & 2.70 & 0.71 & 0.82 & 0.005 & 0.020\\
        Recur & 0.87 & 0.85 & 0.96 & 0.001 & 0.001\\
        NKI & 5.47 & 0.97 & 0.98 & 0.000 & 0.000\\
        Lymph & 3.44 & 0.86 & 0.79 & 0.004 & 0.002\\
        COVID & 2.49 & 0.92 & 0.71 & 0.000 & 0.002\\
        Clocks & 9.39 & 0.91 & 0.26 & 0.000 & 0.000\\
        California & 0.89 & 0.53 & 0.50 & 0.009 & 0.005\\
        \hline
    \end{tabular}
    \caption{Results of the hyperparameter search.}
    \label{fig:hyperparamters}
\end{table*}


\section{Further numerical results}
\label{sec:further_num_res}
Figure~\ref{fig:hists} shows how well SuMo, SuMo+, and SuMo++ can fit the data points of Figure~\ref{fig:toy_curves}. Figure~\ref{fig:hists_2048} shows that both SuMo+ and SuMo++ benefit from further training steps until convergences after 2048 training steps. SuMo+ almost improves to the level of SuMo++ -- while SuMo shows no significant improvement.

% Figure environment removed

\section{Tables containing the separate scores}
\label{sec:separate_scores}
The Tables~\ref{tab:tab:stats_AUPRC},~\ref{tab:stats_A},~\ref{tab:stats_Accuracy},~\ref{tab:stats_Balanced accuracy},~\ref{tab:stats_F1},~\ref{tab:stats_F2},~\ref{tab:stats_F_0.5},~\ref{tab:stats_Precision},~\ref{tab:stats_Sensitivity},~\ref{tab:stats_Specificity},~\ref{tab:stats_Youden} show the separate integrated classifier metrics, and Figure~\ref{tab:stats_iibs} shows the inverted classical Brier score; we invert it so that for all of these metrics, we have that the large the better. Finally, Table~\ref{tab:stats_Concordance} also presents the concordance index based on the mean survival time, and Table~\ref{tab:stats_Min} presents the minimum of all of these scores for each model and dataset.

\input{tables_AUPRC_stats}
\input{tables_AUROC_stats}
\input{tables_Accuracy_stats}
\input{tables_Balanced_accuracy_stats}
\input{tables_Concordance_stats}
\input{tables_F1_stats}
\input{tables_F2_stats}
\input{tables_F_0_5_stats}
\input{tables_Min_stats}
\input{tables_Precision_stats}
\input{tables_Sensitivity_stats}
\input{tables_Specificity_stats}
\input{tables_Youden_stats}
\input{tables_IIBS}

\FloatBarrier
\section{Comparison of non-integrated balanced accuracy and F1 scores using plots}
\label{sec:plots_non_integrated}
Here we provide the Figures~\ref{fig:ot_california},~\ref{fig:ot_clocks},~\ref{fig:ot_covid},~\ref{fig:ot_gbsg2},~\ref{fig:ot_lymph},~\ref{fig:ot_nki},~\ref{fig:ot_recur}. Each presents the non-integrated versions of the balanced accuracy and the $F_1$ score for the most prominent models -- one figure for each dataset. We chose these two scores as they are highly correlated with the mean of the scores and seem to complement each other.
% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed


\FloatBarrier
\section{Example plots of predicted survival curves}
\label{sec:predicted_survival_curves}
For the sake of readability, the Figures~\ref{fig:curve_example_0}, \ref{fig:curve_example_1}, \ref{fig:curve_example_2}, \ref{fig:curve_example_3}, \ref{fig:curve_example_4}, \ref{fig:curve_example_5}, \ref{fig:curve_example_6}, \ref{fig:curve_example_7}, \ref{fig:curve_example_8}, \ref{fig:curve_example_9}, \ref{fig:curve_example_10}, \ref{fig:curve_example_11}, \ref{fig:curve_example_12}, \ref{fig:curve_example_13} only show a selection of the trained models. For the classical models, we only select the one with the highest score in Table~\ref{tab:stats_Mean}.

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed
