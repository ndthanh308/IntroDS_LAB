\section{Bringing (time-dependent) Cox models into the deep learning age}
\label{sec:cox_main_section}
The Cox model, introduced in 1972~\cite{cox1972regression}, is arguably the most popular survival model. It uses a baseline cumulative hazard function $\Lambda_0: \mathbb{R}_{\ge0}\ni t \mapsto \Lambda_0(t) \in \mathbb{R}_{\ge0}$, that depends only on time, and a linear regression to modulate the hazard up and down based on the input features $x$.
Formally we write
\begin{equation}
    \mathscr{S}_\text{Cox}(t, x) = \exp\left( -\alpha(x)\Lambda_0(t)\right),
\end{equation}
where
\begin{equation}
    \label{eq:scalar_product_cox}
    \alpha(x) = \exp \left(\langle a, x\rangle\right)
\end{equation}
with the coefficient $a\in\mathbb{R}^n$. This approach seems to strike an outstanding balance between expressiveness and interpretability.

We now describe what we believe is the first implementation of the Cox (and later time-dependent Cox) model that directly parameterizes $ \Lambda_0 $ via a neural network. Note, for the Cox model, \cite{danks2022derivative} implements $\Lambda_0$ indirectly via a neural network but requires the numeric integration of it.

\subsection{A neural network parameterized Cox model}
\label{sec:cox_simple}
We will now introduce a neural network-based implementation of the classical Cox model. While we will use MONDE+ and SuMo++, the following formulations also hold for MONDE and SuMo+ but not SuMo.

The function $\Lambda_0$ is monotonically increasing, with $0$ as a fixed point, and can be parameterized in different ways, e.g., by splines~\cite{efron1988logistic, royston2002flexible} or in a non-parametric fashion~\cite{crowley1984statistical}. We will do so using neural networks.

Using the results in Section~\ref{sec:universal_and_exclusive}, we can parameterize a Cox model by setting
\begin{equation}
    \label{eq:lambda0}
    \Lambda_0(t) = M_+(t, 0) - M_+(0, 0)
\end{equation}
and obtain
\begin{align}
\begin{split}
    \mathscr{S}_\text{Cox}(t, x)
    = \exp\left( -\alpha(x)\Lambda_0(t)\right) 
    = \mathscr{S}_{++}(t, 0)^{\alpha(x)}.
\end{split}
\end{align}
We want to emphasize that in contrast to the DeepSurv model (also known as DeepHit) independently introduced by~\cite{katzman2018deepsurv},~\cite{lee2018deephit} and~\cite{shahin2022survival}, we propose a parameterization for $\Lambda_0$. This makes our approach not a modification, but an implementation, of the Cox model and, therefore, equally interpretable. Still, we enable training and evaluation within DL frameworks using automatic differentiation~\cite{paszke2017automatic}. This enables not only easy training of Cox models using different loss functions, but also straightforward computation of the event density, $f=-\partial_t S$, and hazard function $\lambda(t|x) = \partial_t\Lambda(t|x)$.

\subsection{A neural network parameterized time-dependent Cox model}
\label{sec:cox_time_dependent}
We will now show how we can parameterize Cox models with time-dependent coefficients~\cite{fisher1999time} using neural networks. These models are of the form
\begin{equation}
    \label{eq:cOx}
    \mathscr{S}_\cOx(t, x) = \exp\left( -\alpha(t, x)\Lambda_0(t)\right),
\end{equation}
where one usually sets $\alpha(t, x) = \exp\left[ \langle \omega(t), x\rangle\right]$ with $\omega: \mathbb{R}_{\ge0} \to \mathbb{R}^n$.

Again, as in Section~\ref{sec:cox_simple}, we can parameterize $\Lambda_0(t)$ via~\eqref{eq:lambda0}. For $\alpha$, we propose the parametrization
\begin{align}
\label{eq:cOx_alpha}
    \alpha(t, x) = \exp\left( \langle \omega(t, x), |x - o|\rangle\right)
\end{align}
where $|\cdot|$ is the entrywise absolute value and $o\in\mathbb{R}^n$ a learnable offset parameter. We use two MONDE+ networks $M_+^\pm$ to define
$\omega:\mathbb{R}_{\ge0}\times \mathbb{R}^n\to \mathbb{R}^n$
entrywise as
\begin{equation}
    \omega_i(t, x) =
    \begin{cases}
        \beta^-_i(t), & \text{if $x_i<o_i$,}\\
        \beta^+_i(t), & \text{otherwise},
  \end{cases}
\end{equation}
where we set $\beta^\pm:\mathbb{R}_{\ge0} \to \mathbb{R}^n$ to
\begin{equation}
    \beta^-(t) = M_+^-(t, 0) - M_+^-(0, 0) + M_+^+(0, 0)
\end{equation}
and
\begin{equation}
    \beta^+(t) = M_+^+(t, 0).
\end{equation}

Unlike in Section~\ref{sec:cox_simple}, this is not just a reparametrization but a slight modification guaranteeing $\mathscr{S}_\cOx(t, x)$ to decrease monotonically in $t$, see Lemma~\ref{lemma:cOx}. Still, the coefficients $\omega$ are highly interpretable, as they separate the effects of $t$ from $x$. Essentially, $\omega_i$ provides a time-dependent weight for $x_i$ that is independent of $x$ except for the threshold $o_i$, which marks the ``least dangerous'' value $x_i$ can take.
\begin{lemma}
    \label{lemma:cOx}
    For the model $\mathscr{S}_\cOx(t, x)$, defined by~\eqref{eq:cOx} and \eqref{eq:cOx_alpha}, it holds $\forall x \in\mathbb{R}^n$ that:
    \begin{enumerate}
        \item $\mathscr{S}_\cOx(t, x)\in[0,1]$ for all $t\in\mathbb{R}_{\ge0}.$
        \item We have a maximum $\mathscr{S}_\cOx(0, x)=1$.
        \item $\mathscr{S}_\cOx(t, x)$ is monotonically decreasing in $t$.
    \end{enumerate}
    Further, if the $M_+^\pm$ are continuous in $t$, we have:
    \begin{enumerate}
        \item The $\omega_i$ are continuous in $t$.
        \item For any $x$, the $\omega_i(\cdot, x)$ have their minima at $\omega_i(0, x)=\beta_i^+(0).$
        \item $\alpha$ is continuous in $(t, x)$.
    \end{enumerate}
\end{lemma}
See Section~\ref{sec:proof_cOx} of the appendix for the proof.
