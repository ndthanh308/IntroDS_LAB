\section{Reinterpreting survival analysis}
\label{sec:reinterpret}
We now discuss how SA can be a portal between classification and regression.

To set the stage, we introduce some key notation. We describe a potentially right-censored survival data sample via
\begin{equation}
    \label{eq:sample}
    (x, e, T) \in \mathbb{R}^n \times \{0, 1\} \times \mathbb{R}_{>0},
\end{equation}
where $x\in\mathbb{R}^n$ denotes input features, $e\in\{0,1\}$ indicates if the event was observed (1) or not (0), and $T$ is the time of the event or the time of right-censoring. We set $T>0$ as one usually assumes that no events occur for $T\le0$.

The goal of SA is to use a set of such samples to create a model that uses the features $x$ to predict the probability of a singular event, such as death, occurring after any given time $t\ge0$. One assumes that the event has not occurred at $t=0$ and is irreversible. Hence, one aims to produce a non-negative, monotonically decreasing curve being $1$ at time $t=0$. Throughout this paper, we will use the terms ``alive'' and ``dead'' to refer to the state before and after the singular event.

\subsection{Survival analysis is classification (with infinitely many classifiers)}
\label{sec:sa_as_classification}
We will pose SA in terms of classification. We begin by recalling the general definition of a survival curve:
\begin{equation}
    \label{eq:S}
    S(t|x): \mathbb{R}_{\ge0}\times\mathbb{R}^n \mapsto \mathbb{P}(\mbox{alive}@t |x) \in [0, 1]
\end{equation}
That is, $S(t|x)$ gives us the probability that the event has not occurred before or at time $t$, given the features $x$.

An alternative way of reading this definition is that $S$ is equivalent to a set of probabilistic binary classifiers
\begin{equation}
    \{S(t|\cdot) = C_t:\mathbb{R}^n\to[0,1]:t\in\mathbb{R}_{\ge0}\},
\end{equation}
each $C_t(x)$ being the probability of surviving the interval $[0, t]$ given the features $x$. Phrased differently, \textit{a survival model is simply an infinite collection of probabilistic classifiers indexed by time}.

Using the classifier perspective, we will derive a new loss function for survival curves. Considering the ideal scenario of having unlimited clean data and a highly capable probabilistic classifier, we can train one classifier for each point in time. This transforms the problem into a prediction of Bernoulli variables, and thus each classifier can be trained using the binary cross entropy (BCE) loss.

As a reminder, the standard BCE loss for a classifier $C:\mathbb{R}^n\to[0,1]$ and a sample $(x, y)\in\mathbb{R}^n\times\{0,1\}$ is defined as
\begin{equation}
    \label{eq:standard_bce}
    -y\log C(x) - (1-y)\log\left(1 - C(x)\right).
\end{equation}

Returning to the SA setting, if we have a censored sample, $(x, 0, T)$, we can use it to train all classifiers before the censoring,
\begin{equation}
    \label{eq:set_early_classifiers}
    \{C_t:\mathbb{R}^n\to[0,1]:t\in[0, T)\},
\end{equation}
via the BCE part
\begin{equation}
    \label{eq:loss_early_classifiers}
    -\log C_t(x),
\end{equation}
i.e., for censored samples, we want to find classifiers that predict $0$ (``alive'') for times before the censoring.

Otherwise, if we have an uncensored sample, $(x, 1, T)$, we can use the loss in~\eqref{eq:loss_early_classifiers} for the early (``premortem'') classifiers given by~\eqref{eq:set_early_classifiers} and train the later (``postmortem'') classifiers given by
\begin{equation}
    \label{eq:set_later_classifiers}
    \{C_t:\mathbb{R}^n\to[0,1]:t\ge T\}
\end{equation}
via the BCE part
\begin{equation}
    \label{eq:loss_later_classifiers}
    -\log(1 - C_t(x)).
\end{equation}

We can formulate a joint loss for all classifiers by choosing two arbitrary random variables, $\mathcal{T}_-(e, T)$ and $\mathcal{T}_+(e, T)$, with an everywhere-supported density over $[0, T]$ and $[T, \infty)$ respectively.
We define this loss by joining~\eqref{eq:loss_early_classifiers} and~\eqref{eq:loss_later_classifiers} to
\begin{align}
\label{eq:bce_loss}
\begin{split}
    L_\text{BCE} = -[
                   & \mathbb{E}_{t_-\sim \mathcal{T}_-}\log S(t_-|x) \\
    + e \, \cdot\, & \mathbb{E}_{t_+\sim \mathcal{T}_+}\log \left(1 - S(t_+|x)\right)
    ].
\end{split}
\end{align}
The loss trains a separate classifier $S(t|\cdot)=C_t(x)$ for every point in time $t\in[0,\infty)$, as both $\mathcal{T}_\pm$ are supported everywhere. We can consider the loss as a weighted integral over the BCE loss. As a result, it inherits many desirable properties of the BCE loss, such as being a strictly proper scoring rule~\cite{du2021beyond}.

In practice, assuming an infinite amount of clean data is unrealistic. However, we have two advantages when dealing with commonly found un- and right-censored data. Firstly, earlier classifiers can use most data points (as little censoring has occurred yet). Secondly, as survival curves decrease over time, later classifiers' predictions are upper bounded by earlier classifiers. To take advantage of this, we need to ensure the monotonicity of our predicted survival curves, which we will discuss in Section~\ref{sec:universal_and_exclusive}.

If the monotonicity holds, we also do not require that the density of $\mathcal{T}_\pm$ is supported everywhere, as later classifiers provide lower bounds for earlier ones. In practice, we therefore do the following: We sample times from a Gaussian centered at $T$ for uncensored samples and only from the left side of that Gaussian for censored samples. We ensure no negative time samples via projection with ReLU. The idea is that while one trains for all $t$, the Gaussian focuses the training effort on values around $T$; here, the variance $\sigma^2$ is a hyperparameter that controls the focus. Other choices are plausible, but we leave this to future work. Formally, we can represent this as
\begin{equation}
    \label{eq:T_minus}
    \mathcal{T}_-(e, T) = 
    \begin{cases}
        \delta(T), & \text{if $e=0$,}\\
        \mbox{ReLU}_\#\mathcal{HN}_-(T, \sigma^2), & \text{otherwise},
  \end{cases}
\end{equation}
and
\begin{equation}
    \label{eq:T_plus}
    \mathcal{T}_+(e, T) = \mathcal{HN}_+(T, \sigma^2),
\end{equation}
where $\delta$ denotes the delta-distribution, $\mathcal{HN}_\pm$ the left- and right-sided half-normal distribution, and $\mbox{ReLU}_\#$ the pushforward operator of the projection into the non-negative real numbers. 

During training, we approximate the expectations in~\eqref{eq:bce_loss} by sampling only one $t$. If $e=0$ we sample $t\sim\mathcal{T}_-$, if $e=1$, we sample with a 50\% probability from $\mathcal{T}_-$ otherwise from $\mathcal{T}_+$.

As a comparison, we use the SuMo loss~\cite{rindt2022survival}
\begin{equation}
    L_\text{SuMo} = -\left[e \log f(t|x) + (1 - e) \log S(t|x)\right],
\end{equation}
where
\begin{equation}
    f(t|x) = - \partial_t S(t|x).
\end{equation}
Both losses can be seen as log-likelihoods and are similar in their $e=1$ part but differ in their $e=0$ part. This is because SuMo's likelihood is conditioned on the event variable $e$, while the BCE's is not.

\subsection{Survival analysis is regression (with uncertainty estimation)}\label{sec:sa_as_regression}
We will now discuss how we can use a survival model to predict the full posterior of $\mathbb{R}^n\to\mathbb{R}_{\ge0}$ regression problems. The idea is similar to the motivation of the paper~\cite{chilinski2020neural}. Using~\eqref{eq:S}, we can express the lifetime distribution function (probability of the event has not occurred yet) as 
\begin{equation}
    F(t|x) = \mathbb{P}(\mbox{dead}@t|x) = 1 - S(t|x).
\end{equation}
Consequently, we can interpret $f(t|x)$ as an event density.

We now want to make a simple but powerful observation. We can treat any dataset with non-negative scalar labels as an uncensored survival dataset. Consequently, the event density $f$ of a survival model that fits the dataset well (see Section~\ref{sec:universal_and_exclusive}) is the full posterior of the original regression problem.

Note that this setup also allows one to train regression models on samples for which one only knows a lower bound on the label.

\subsection{If in doubt: survival analysis}
SA can be viewed from both a classification and regression perspective. Understanding this duality is beneficial when selecting a model or interpreting results. Any one-dimensional regression problem or binary classification involving a threshold can benefit from this perspective by rephrasing the question one asks. For example, the clocks dataset~\cite{clocks} provides images of analog clocks with the displayed times as labels; see Figure~\ref{fig:clock} for two samples. We can interpret the dataset as asking: ``what time does the clock show?''; this is a regression problem. Alternatively, we could ask for any point in time: ``is it earlier than the time the clock shows?''; this subtle shift in question turns the regression into a classification problem. Training a survival model instead of a regression or classification model can answer both questions and many others.

The ability of survival models to answer various questions at inference time is a strong advantage (particularly those not considered during training). Another benefit of framing a problem as a survival problem, where possible, is that the training process receives more information per label. While classification uses binary information about which side of a threshold the value is on, SA uses the value itself.
% Figure environment removed