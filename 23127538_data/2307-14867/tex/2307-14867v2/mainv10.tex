\documentclass[12pt]{article}

\usepackage{amssymb,amsmath, amsfonts, amsthm}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{enumerate}
\usepackage[authoryear]{natbib}
\usepackage{url} % not crucial - just used below for the URL
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=false,bookmarksopen=false,
 breaklinks=false,pdfborder={0 0 1},colorlinks=true]{hyperref}
\hypersetup{linkcolor=red, citecolor=blue}

%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{1}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-1in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.7in}%
\addtolength{\topmargin}{-1in}%


%%%%

\usepackage{bm}
\usepackage{xcolor}
\newcommand{\red}[1]{\color{red}   #1 \color{black} }
\newcommand{\bleu}[1]{\color{blue}   #1 \color{black} }

\def\R{{\mathbb{R}}}
\newcommand{\E}{\operatorname{E}}
\newcommand{\T}{\operatorname{T}}
\newcommand{\B}{\operatorname{B}}
\newcommand{\A}{\operatorname{A}}
\renewcommand{\P}{\operatorname{P}}
\newcommand{\1}{\mathbf{1}}
\newcommand{\M}{\operatorname{M}}
\newcommand{\sign}{\operatorname{sign}}

\newcommand{\tbc}{{\bf ***to be completed***$\quad$}}
\newcommand{\tbchk}{{\bf ***to be checked!!!***$\quad$}}
\newcommand{\chk}{{\bf ***checked!!!***$\quad$}}
\newcommand{\tbm}{{\bf ***to be modified***$\quad$}}

\newcommand{\as}{\quad \mbox{ a.s.}}

%%%%%%%%%%
\newtheorem{lemma}{Lemma}[section]%numbering by section
\newtheorem{assumption}{Assumption}[section]
\newtheorem{proposition}{Proposition}[section]%numbering by section
\newtheorem{theorem}{Theorem}[section] %numbering by section
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}[section]

%%%%


\begin{document}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\if1\blind
{
  \title{\bf One-step  nonparametric instrumental regression using
  smoothing splines}
  \author{ Jad Beyhum\\
    Department of Economics, KU Leuven\\
    \and
    Elia Lapenta\\
    CREST and ENSAE Paris\\
    \and
Pascal Lavergne \thanks{Pascal Lavergne acknowledges funding from the French National Research Agency (ANR) under the Investments for the Future program (Investissements d'Avenir, grant ANR-17-EURE-0010).\newline
The authors thank Ben Deaner, Frank Windmeijer, Jasmin Fliegner, Daniel Wilhelm, Aureo de Paula, Kaspar Wuthrich, Christoph Breunig, Joachim Freyberger, seminar and conference participants at the University of Bonn, the 2023 Encounters in Econometric Theory at Nuffield College, Oxford University, and the 2023 Bristol Econometrics Study Group conference.}\\
    Toulouse School of Economics, Universit\'e Toulouse  Capitole}
    \date{August 31, 2023}
  \maketitle
} \fi

\if0\blind
{
  \bigskip
  \bigskip
  \bigskip
  \begin{center}
    {\LARGE\bf One-step  nonparametric instrumental regression using
  smoothing splines}
\end{center}
  \medskip
} \fi


\bigskip
\begin{abstract}
We extend nonparametric regression smoothing splines to a context where there is endogeneity and instrumental variables are available.  Unlike popular existing estimators, the resulting estimator is one-step and  relies on a unique regularization parameter. We derive uniform rates of the convergence for the estimator and its first derivative. 
We also address the issue of imposing monotonicity in estimation.
Simulations confirm the good performances of our estimator compared to some popular two-step procedures. Our method yields economically sensible results when used to estimate Engel curves.
\end{abstract}


\noindent%
{\it Keywords:}  Instrumental variables, Nonparametric
regression, Smoothing splines.
\vfill

\newpage
%\spacingset{1.9} % DON'T change the spacing!
\spacingset{1.3} % CHANGE FOR FINAL VERSION

\section{Introduction}
\label{sec:intro}


The estimation of causal effects on outcomes is often complicated by
omitted confounding variables or nonrandom selection. The problem is
well known to affect observational studies, but it can also affect
randomized controlled trials with issues like participant
noncompliance. In economics, the problem is commonly framed in terms
of a regression model from which important regressors have been
omitted and so become part of the model's error. In this context, a
variable is termed “exogenous" if it is not associated with the
error term after conditioning on confounding variables, and
“endogenous" otherwise.  Instrumental variables (IVs) are widely
used to solve the problems posed by endogeneity. An IV 
%is associated with the endogenous  variable, and 
is associated with the
outcome only  through its association with the endogenous
variable. Examples of IV models include  estimation
of  Engel curves linking expenditures to the consumption share of a
particular good, or   estimation of the effects of class size on
students’ performances \citep{horowitz2011applied}.

We consider the prototypical model
\begin{equation}
Y =   g_{0} (Z) +  \varepsilon \qquad \E \left( \varepsilon |  W
\right) = 0 \, ,
\label{Model}
\end{equation}
where $Y \in \R$ is the dependent variable, $Z \in \R$ is the
endogenous  continuous independent variable, and $W\in \R^{p}$ are the IVs.  The
goal is to estimate nonparametrically $g_0(\cdot)$,  the
causal effect of the variable $Z$ on $Y$, using $W$ to account for
endogeneity.  If we assumed linear relationships, we could use the
two-stage least squares estimator: in a first stage, one obtains the linear projection of $Z$ on $W$, then in a second stage one linearly regresses $Y$ on the previously estimated linear projection. Considering a nonparametric function $g_0(\cdot)$ allows estimating the causal relationship of $Y$ and $Z$ in a more flexible manner. As detailed below, most existing nonparametric methods are also two-stage. 


 {We here develop a nonparametric instrumental regression
  estimator for $g_0$   that bypasses first stage nonparametric estimation, is easy to implement, and leverages the excellent approximation properties of splines.} To do so, we
  exhibit a global quantity that accounts for all the information
  contained in Model (\ref{Model}) and that is minimized by the true
  function $g_0(\cdot)$. We then consider an empirical equivalent, and  we set up a minimization problem penalized by a roughness measure of
  the function to regularize the solution. Our estimator can be seen as a   generalization to the instrumental variable regression of regression smoothing splines \citep{Wahba1990, green1993nonparametric}.
As a leading case, we consider the integral of the squared second derivative as a roughness measure, and we show that the solution
  is a natural cubic spline. 
  %{Our procedure is based on a single regularization parameter, which can be selected by cross-validation.} 
We derive a closed-form expression of the solution.  
  {A characteristic feature of our estimator is that it depends  upon a single regularization parameter, which can then be selected by some usual methods such as cross-validation.}  
  As an additional advantage of the smoothing spline nature of our
  estimator, straightforward estimators of derivatives of the function
  $g_0(\cdot)$ obtain. 
  We further show how to impose monotonicity
  constraints by relying on a method proposed by \cite{Hall2001}.  
  The constrained estimator is simple to implement in practice.  
  
  We derive uniform rates of convergences for the function itself and its derivative,   with or without monotonicity constraints. Our estimator exhibits excellent finite sample performance in simulations compared to some existing two-step   methods. We applied our method to the estimation of Engel curves.
%  , that relate the proportion of consumer expenditure on a good as a function of total expenditure. 
%Economic theory predicts that for a “normal'' good
%the Engle curve is increasing, while for an “inferior'' good it is
%decreasing.  
Our smoothing splines estimator and its constrained
version yield comparable results and are reasonable from an economic
viewpoint.

Existing procedures for estimating $g_0$ are typically two-step and do not make use of smoothing splines.
\cite{newey2003instrumental} develop a nonparametric equivalent to the
two-stage least squares estimator: they use  linear-in-parameter
series expansions of $\E \left( Y | W \right)$ and 
$\E\left\{ g(Z) |W \right\}$ in a generalized method of moments
framework, see also \cite{ai2003efficient, hall2005nonparametric,
blundell2007semi, Johannes2011,horowitz2014adaptive} for alternative series-based methods. Alternatively, \cite{hall2005nonparametric}, \cite{darolles2011nonparametric}, and \cite{gagliardini2012tikhonov}
rely on kernel methods to estimate the unknown conditional
expectations. In both cases,
backing up a nonparametric estimate of $g_0(\cdot)$  is an ill-posed inverse problem. Hence, one needs some kind of
regularization, such as hard thresholding
\citep{horowitz2011applied,  chen2012estimation}, Tikhonov or ridge-type
regularization
\citep{newey2003instrumental, darolles2011nonparametric, florens2011identification,
gagliardini2012tikhonov, singh2019kernel},  or a Landweber-type iterative
method \citep{Dunker2014}. A general exposition of some of these
methods is given by \cite{carrasco2007linear}.
A recent machine learning literature considers solving a saddle
point problem that is dual to a generalized method of moments criterion.
Here one first maximizes an objective function with respect to
a function of the instruments $W$, then one minimizes with respect to
a function of $Z$ to obtain $g_0(\cdot)$ \citep{bennett2019deep,
dikkala2020minimax, Liao2020}. \cite{muandet2020dual} consider a related but different
saddle point problem. If the set of functions upon which one optimizes is
large, then one has in addition to introduce some penalization in the
optimization problem \citep{dikkala2020minimax, Liao2020}.

By contrast to previous estimators based on series or kernel estimation, ours is a natural generalization of the popular regression smoothing splines estimator. 
The appeal of smoothing splines lies in their simplicity together with their  excellent approximation properties of smooth functions and their derivatives, which have been extensively studied \citep{schumaker_2007}. 
Our estimator cannot be obtained as a generalization or a special case of existing estimators.
Our approach is also attractive because it is one-step.
Two-stage procedures typically lead to theoretical and practical difficulties. First, one may need to estimate in a first-stage an object that may be more complex than the final object of interest, while  imposing smoothness assumptions on this first-stage object. Second, from a statistical perspective, first-stage estimation typically affects the second-stage small sample and asymptotic properties.
Third, each stage relies on a particular choice of a  smoothing or regularization parameter, whose fine tuning may be difficult  in practice while affecting the final results. 

The paper is organized as follows. In Section \ref{sec.est}, we detail the main steps that lead to the formulation of our estimator, and we show that it extends regression smoothing splines to the instrumental regression context. The asymptotic properties of our estimator are analyzed in Section \ref{sec: Asymptotic analysis}.  
Section  \ref{sec: Estimation under shape constraints} deals with estimation under monotonicity constraints.
Section \ref{sec.emp} reports simulation results and an empirical application to Engel curves estimation.

\section{Our estimator}\label{sec.est}

\subsection{General formulation}

We assume that $g_{0}(\cdot)$ belong to some space of functions
$\mathcal{G}$ on which identification holds, that is,
\begin{equation}
\E \left\{ Y -  g (Z)  |  W \right\}= 0 \as
\Rightarrow  g (\cdot) = g_{0}(\cdot) \as
\label{ident}
\end{equation}
For a discussion of this condition called {\em completeness}, see
e.g. \cite{DHaultfoeuille2011} and \cite{Freyberger2017}.
{When $Z$ is continuous, as we assume here, $W$
should typically have   at least one continuous component for
 completeness to hold. Some of the instruments, however, could be
 discrete, and this will not affect further our exposition and reasoning.}
 
Instead of dealing directly with (\ref{ident}), as done by most previous work, we consider an equivalent formulation that does not require estimating a conditional expectation given the instruments $W$.
{By the results of \cite{Bierens1982},
\begin{equation}
\E \left\{ Y -  g (Z)  |  W \right\}= 0 \Leftrightarrow
\E \left\{ \left( Y - g (Z) \right) \exp(\mathbf{i} W^\top t) \right\} = 0 \quad \forall t\in\R^p
\, ,
\label{eq:bierens}
\end{equation}
where $\mathbf{i}$ is the unit imaginary number. Consider now
\begin{equation}
\label{populationprogram}
g_0 =\arg\min_{g\in \mathcal{G}} M(g),
\qquad
M(g) =
\int \left| \E \left\{ \left( Y - g (Z) \right) \exp(\mathbf{i}W^\top t) \right\} \right|^2 \, d\mu(t)
\, ,
\end{equation}
where $\mu$ is a symmetric probability measure with support $\R^{p}$. Then it is straightforward to see that $M(g)\geq 0$ for
all $g \in \mathcal{ G}$, and that under (\ref{ident})
\[
M(g)= 0  \Leftrightarrow g = g_{0}
\, .
\]
With a random i.i.d. sample $\left\{ \left(Y_i,Z_i,W_i\right), i=1,
\ldots n \right\}$ at hand, a natural estimator of  $M(g)$ is
\begin{align}
M_{n}(g)   & =
\int \left| \frac{1}{n} \sum_{i=1}^{n}{ \left( Y_i - g (Z_i) \right)
\exp(\mathbf{i}W_i^\top t) } \right|^2 \, d\mu(t)
\nonumber
\\
& =
\frac{1}{n^2}
\sum_{1\leq i, j \leq n }
\left( Y_i - g (Z_i) \right) \left( Y_j - g (Z_j) \right) \omega(W_i - W_j)
\, ,
\label{Mn}
\end{align}
{where} 
\[
\omega(z) = \int_{\R^{p}} \exp(i t'w) \, d\mu(t) = \int_{\R^{p}} \cos(t'w) \, d\mu(t)
\, ,
\]
due to the symmetry of $\mu$. The function $\omega(\cdot)$ is (up to a constant) the Fourier transform of
the density of $\mu$. The above V-statistic formulation is used in practice for computational purposes.
This statistic accounts for an infinity of moment conditions
as stated in (\ref{eq:bierens}). It is different in nature from
generalized method of moments criteria used in previous
work, that account for an increasing  but finite number of moment
conditions.


The condition for $\mu$ to have support $\R^{p}$ translates into the
restriction that $\omega(\cdot)$ should have a strictly positive Fourier
transform almost everywhere.  Examples include products of triangular,
normal, logistic, see \citet[Section 23.3]{JKB95}, Student, including
Cauchy, see \cite{DK2002}, or Laplace densities.  To achieve scale
invariance, we recommend, as in
\cite{Bierens1982}, to scale the exogenous
instruments by a measure of dispersion, such as their empirical
standard deviation.  
If $W$ has bounded support, results from \cite{Bierens1982}  yield
 that the equivalence  (\ref{eq:bierens}) holds when restricted to any $t$  in a (arbitrary) neighborhood of $0$ in $\R^p$. Hence,
 $\mu$  can be taken as any symmetric probability measure
  that  contains $0$ in the interior of its  support.   As noted by
 \cite{Bierens1982}, there is no loss of generality assuming a bounded
 support, as the equivalence result equally applies to a one-to-one
 transformation of $W$, which can be chosen with bounded
 image.}

Minimizing $M_n(g)$ would lead to interpolation. We  regularize the problem by assuming some smoothness for the
function $g$. We assume  that $Z$ has compact support, say  $[0,1]$ without loss of generality, and that $\mathcal{ G}$ is the space of twice differentiable functions on  $[0,1]$ defined as
\[
\mathcal{G}=\left\{ g:[0,1] \to \R, 
%g \in \mathcal{D}^2 :
  \int_{0}^1 |g''(t)|^2 \, dt < \infty \right\}
\, .
 \]
%with absolutely continuous first derivative. That is, if %$g \in  \mathcal{G}$, there is an integrable
% function $g''$ such that $\int_0^z g''(t) \, dt = g'(z) %- g'(0)$.
We then estimate $g_{0}$ as a minimizer of a penalized version of
$M_n(g)$ on  $\mathcal{G}$. Specifically,
\begin{equation}
\label{estimator}
\widehat{g} \in  \arg\min_{g\in \mathcal{G}} M_n(g)  + \lambda \int_0^1
|g''(z)|^2\, dz
\, ,
\end{equation}
where $\lambda>0$ is a regularization parameter.
Our estimator $\widehat{g}$ is a natural cubic spline
with knots at points $Z_i$. This is implied by the following result of
\citet[Th. 2.3]{green1993nonparametric}.
\begin{theorem}
\label{ncsthm}
Suppose $n \geq 2$ and $0 \leq Z_1 < \cdots < Z_n \leq 1$.
Let $\widetilde{g}$ be any function in $\mathcal{G}$ for which $\widetilde{g}(Z_i) =
g_i, \ i=1, \ldots, n$, and $g$ a natural cubic spline
 such that ${g}(Z_i) = g_i, \ i=1, \ldots, n$. Then
\begin{equation*}
\label{ncs}
\int_0^1 |\widetilde{g}''(t)|^2\, dt \geq
\int_0^1 |g''(t)|^2\, dt
\, .
\end{equation*}
with equality only if $\widetilde{g} = g$.
\end{theorem}
It thus follows that if $\widehat{g}$ is a solution to
(\ref{estimator}), it should be a natural cubic spline.
The uniqueness of the above natural cubic spline interpolant is proven in
\citet[Th. 2.2]{green1993nonparametric}.

A recent approach we became aware of when preparing this paper is the
   “kernel maximum moment loss" approach proposed by
   \cite{zhang2023instrumental}.  While it does not smooth on the
   instruments, it assumes that the regression of interest belongs to
   a Reproducing Kernel Hilbert Space (RKHS) and solves a minimization
   problem by penalizing by the norm on such a space. The estimator thus depends on the chosen reproducing kernel.  Differently, we
   assume that the regression of interest belongs to a space of smooth
   functions, and we penalize by the integral of the squared second
   derivative of the regression, which is a very intuitive measure of
   roughness, but not a RKHS norm. 
   
\subsection{Closed-form solution}

We now show the existence and uniqueness of the
solution to (\ref{estimator}) and  we  characterize it.
A  natural cubic spline can be written as
\begin{equation}
g(z)  =  a_{0} + a_{1} z + \frac{1}{12} \sum_{i=1}^{n}{ \delta_{i}
|z-Z_{i}|^{3}}\, ,
\quad
\sum_{i=1}^{n}{ \delta_{i}} = \sum_{i=1}^{n}{\delta_{i} Z_{i}} = 0
\, .
\label{eq:gdecomp}
\end{equation}
The function $g$ is thus uniquely defined by the coefficients
$a_0$, $a_1$, and $\delta_i, i = 1, \ldots n$, or equivalently by its
value at the knots \citep[Section 7.3]{green1993nonparametric}.  It will be useful for what follows to use matrix
notations.  Let
\[
\bm{Z}= \left(
\begin{array}{cc}
1 & Z_{1}
\\
\vdots & \vdots
\\
1 & Z_{n}
\end{array}
\right)
\, ,
\]
 $\bm{E} = \left[ \frac{1}{12} |Z_{i}-Z_{j}|^{3} , i,j=1,\ldots n\right]$, and $\bm{g} =
 \left(g(Z_1), \ldots g(Z_n)\right)^T$.
Then $\bm{g} = \bm{Z a} + \bm{E \delta}$
with constraints $\bm{Z^T \delta} = 0$.  Also, one can check that
\[
\int  g'' (z)^{2} \, dz  =
\bm{\delta^T {E} \delta}
\, ,
\]
see \citet[Section 7.3]{green1993nonparametric}.
Let $\bm{Y}$ be the vector $\left(Y_1, \ldots Y_n\right)^T$, then
\begin{equation}
 M_{n}(g) + \lambda \int  \left(g'' (z)\right)^{2} \, dz
 =
\left( \bm{Y - Z a - E \delta}  \right)^T \bm{\Omega} \left( \bm{Y} - \bm{Z a} -
\bm{E \delta}  \right)
+ \lambda \bm{\delta^T {E} \delta}
\, ,
\label{eq:min}
\end{equation}
where $\bm{\Omega}$ is the matrix with generic element $n^{-2} \omega(W_i-W_j)$.
Hence, we want to minimize  a quadratic function in parameters
under the constraints   $\bm{Z^T \delta} = 0$. This yields a unique
solution under the usual requirements. The following proposition gives
a more convenient formula that characterizes our estimator.
\begin{proposition} \label{prop:1}
For any $\lambda >0$, if $\bm{Z}$ is full rank and all $W_i$'s are
different, the solution to (\ref{estimator}) exists, is unique, and is
a natural cubic spline $\widehat{g}$ characterized by
\begin{align}
\left[
\begin{array}{cc}
\widetilde{\bm{E}} & \bm{Z}
\\
\bm{Z^T} & \bm{0}
\end{array}
\right]
\left(
\begin{array}{c}
\bm{\widehat{\delta}}
\\
\bm{\widehat{a}}
\end{array}
\right)
& =
\left(
\begin{array}{c}
\bm{Y}
\\
\bm{0}
\end{array}
\right)
\, ,
\qquad
\widetilde{\bm{E}} = \bm{E} + \lambda \bm{\Omega}^{-1}
\, ,
\label{estimformula}
\end{align}
and
$
\widehat{\bm{g}}  =
\left[
\bm{P} +
\bm{E} \widetilde{\bm{E}}^{-1} \left(\bm{I}-\bm{P}\right)
\right]
\bm{Y}
$, where  $\bm{P}= \bm{Z} \left(\bm{Z}^T \widetilde{\bm{E}}^{-1} \bm{Z}
\right)^{-1} \bm{Z}^T \widetilde{\bm{E}}^{-1}$.
\end{proposition}

Our estimator is obtained directly by solving the linear system of equations
(\ref{estimformula}). It does not necessitate estimation of other
nonparametric quantities, and relies on only one regularization
parameter $\lambda$. It also directly provides an estimator of the
first derivative of $g(\cdot)$ as
\begin{equation}
\label{eq: ghat prime}
\widehat{g}'(z)  =  \widehat{a}_{1} + \frac{1}{4} \sum_{i=1}^{n}{
\widehat{\delta}_{i} \sign(z-Z_i) (z-Z_{i})^{2}}
\, ,
\qquad
\sign(u) = \1(u \geq 0) - \1(u < 0)
\, .    
\end{equation}


There are alternative ways to (\ref{eq:gdecomp}) for expressing a
natural cubic spline.  We focus on this formulation as it does not
rely on a particular support of $Z$, nor on the fact that the knots
$Z_i$ are arranged in increasing order. In particular, the closed-form expression in Proposition \ref{prop:1} 
is valid regardless of the support of $Z$ and therefore it can be used without first transforming $Z$ into $[0,1]$.
We also found this formulation to be convenient for practical implementation. For large
samples, where the above formula may not be computationally
efficient, one can adapt to our context the Reinsch algorithm
\citep{green1993nonparametric}.

\section{Asymptotic analysis}\label{sec: Asymptotic analysis}


The formal study of our estimator is based on a reformulation of
$M(g)$ in (\ref{populationprogram}).
Consider
\[
\mathcal{H}=\left\{ h \in  \mathcal{G}:
h(0) = h'(0) = 0 \right\},
\,
 \]
and the inner product $\left<h_1,h_2\right>_{\mathcal{H}}=\int_{0}^1
h_1''(z)\,h_2''(z)\, dz$ on
$\mathcal{H}$. Then  each $g\in \mathcal{G}$ can be uniquely written as $g(z)=(1,z)\beta+h(z)$,
where $\beta=(g(0), g'(0))^T\in  \mathbb{R}^2$, $h(z)=g(z)-g(0)-g'(0)z$,
 $h\in\mathcal{H}$.
 Denote by $L^2_\mu $ the space of
complex functions $l(\cdot)$ from $\mathbb{R}^p$ into $\mathbb{C}$ such that
\[
\|l\|^{2}_\mu  = \int_{}^{}{|l(t)|^2 \, d\mu(t)} < \infty
\, .
\]
Consider the  operators $\A:\mathcal{H}\mapsto
L^2_\mu $ and $B:\R^2\mapsto L^2_\mu $ such that
\begin{align}
     \A h=\E\{h(Z)\exp(\mathbf{i}W^T\cdot)\}\text{ and } \B
     \beta=\E\{(1,Z)\beta \exp(\mathbf{i}W^T\cdot)\}\, .
\end{align}
The minimization problem  (\ref{populationprogram}) identifying $g_0$ can be expressed as
\begin{equation}
\label{eq: Partly Linear population program}
   \min_{(\beta,h) \in\mathbb{R}^2\times\mathcal{H}} 
   \|\E\{Y \exp(\mathbf{i}W^T\cdot)\}-\B\beta - \A h \|_\mu^2
\, .
\end{equation}
The above quantity
reaches its minimum zero at $(\beta_0,h_0)$, with
$g_0(z)=(1,z)\beta_0+h_0(z)$. A key advantage of this formulation for
theoretical analysis is that using orthogonal projection, we can
profile (\ref{eq: Partly Linear population program}) to first
determine ${h}_0$, then ${\beta}_0$ as a function of ${h}_0$. In our
proofs, we will also consider the  penalized empirical
counterpart of (\ref{eq: Partly Linear population program}) and use a
similar profiling method to obtain $(\widehat
\beta,\widehat h)$, and then  $\widehat g(z)=(1,z)\widehat \beta + \widehat h(z)$.

We now detail our formal assumptions. 
\begin{assumption}\label{ass: square integrability} (i) $\E\{Y^2\}<\infty$; (ii)
    $Z$ has a density $f_Z$ on $[0,1]$;
(iii) $\mu$ is  a symmetric probability measure with support $\R^p$.
(iv) $\int\int \E\{\exp(\mathbf{i}W^\top  t)f_{Z}(z)\}^2\mu(t) dz \,
dt<\infty$; (v) $W$
%is bounded and
has at least one  continuous component.
\end{assumption}
This assumption ensures that $\E\{Y\exp(\mathbf{i}W^T\cdot)\}\in
L^2_{\mu}$, and that $\A$ and $\B$ are into $L^2_{\mu}$.
Our assumption on the support of $Z$ is without much loss of
generality, since  we can always use a one-to-one
transformation that maps $Z$ into $[0,1]$.
We  then formalize the completeness assumption, under which the
problem (\ref{eq: Partly Linear population program}) admits a
unique solution $(\beta_0,h_0)$.
\begin{assumption}\label{ass: completeness}
$g_0$ belongs to $\mathcal{G}$ and
the mapping $g\in \mathcal{G} \mapsto \E\{g(Z)|W=\cdot\}$ is injective.
\end{assumption}
We introduce now a {\em source condition}, which is common in the
literature on inverse problems \citep{carrasco2007linear}.  While it is not needed to establish the consistency of $\widehat g$ and its first
derivative, it is necessary to obtain their uniform convergence rates.
\begin{assumption}
\label{ass: source condition}
Let $M$ be the orthogonal projection onto the orthogonal of the span
of $B$, and let $\T = \M \A$.  Let $(\sigma_j,\varphi_j,\psi_j)_j$ be
the singular system of $\T$, where $(\varphi_j)_j$ is a sequence of
orthonormal elements in $\mathcal{H}$, $(\psi_j)_j$ is a sequence of
orthonormal elements in $L^2_\mu$, and $(\sigma_j)_j$ is
a sequence of strictly positive values in $\mathbb{R}$. Then there
exists $\gamma>0$ such that
$$
\sum_j
\sigma_j^{-2\gamma}|\left<h_0,\varphi_j\right>_{\mathcal{H}}|^2<\infty
\,.
$$
\end{assumption}


\begin{theorem}\label{th.cvrates}
    Under Assumptions \ref{ass: square integrability},
\ref{ass: completeness},  if     $\lambda\rightarrow
    0$ and $n \lambda \rightarrow \infty$, then
    \begin{align*}
        \sup_{z\in[0,1]}|\widehat g(z)- g_0(z)|=o_P(1)\, \text{ and }
    &\sup_{z\in[0,1]}|\widehat g'(z)- g'_0(z)|=o_P(1)\, .
    \end{align*}
    If moreover Assumption \ref{ass: source condition} holds, then
    \begin{equation*}
    \sup_{z\in[0,1]}|\widehat g(z)- g_0(z)|=O_P\left(\frac{1}{\sqrt{n
    \lambda}}+\lambda^{\frac{\gamma \wedge 2}{2}}\right) \text{ and }
    \sup_{z\in[0,1]}|\widehat g'(z)-
    g'_0(z)|=O_P\left(\frac{1}{\sqrt{n \lambda}}+\lambda^{\frac{\gamma
    \wedge 2}{2}}\right)\, .
    \end{equation*}
\end{theorem}

{We obtain consistency of our estimator and its derivative under mild assumptions, that only involve a standard condition on the regularization parameter $\lambda$. By contrast, in two-step estimation methods    that smooth over the instruments, obtaining consistency of the first and second steps estimators typically necessitates smoothness assumptions on the distribution of $Z$ given $W$ and conditions that relate the different smoothing parameters, see  e.g. \cite{ai2003efficient, chen2012estimation}. One partial solution adopted by \cite{horowitz2011applied} is to choose  the latter parameters equal, but this does not change the two-step nature of the underlying estimator. 
In some instances, consistency may further necessitate  regularization parameters \citep{chen2012estimation} and a source condition \citep{gagliardini2012tikhonov}. A general discussion can be found in \cite{carrasco2007linear}. 

Turning now to our consistency rates, we do not claim that these are
sharp or optimal. However, by contrast to previous results in this
literature, they depend upon only one regularization
parameter.  If $\lambda$ is optimally chosen, then we obtain convergence rates that are $n^{- \frac{\gamma
\wedge 2}{2\left(1+ \gamma \wedge 2\right)}}$.  For $\gamma = 2$ or 1,
this respectively yields $n^{-1/3}$ and $n^{-1/4}$.  It is unclear how
to compare the above rates to existing results on optimal convergence
rates \citep{hall2005nonparametric, Chen2018} because these authors
make assumptions on conditional expectation operators such as $\E
\left\{ g(Z) | W \right\}$, while Assumption \ref{ass: source
condition} concerns an unconditional expectation
operator. Our assumption, however, assumes that the problem is mildly ill-posed, while some
previous work also considers the case of a severely ill-posed inverse
problem.}


{
\section{Estimation under monotonicity}
\label{sec: Estimation under shape constraints}

In many applications, we may expect the function of interest $g_0$ to be
monotonic.  For instance, if $g_0$ is the demand for a good as a
function of price, we expect it to be decreasing.  If $g_0$ is the
Engel curve, that relates the proportion of consumer expenditure on a
good to total expenditure, we typically expect this
function to be increasing for a “normal'' good and decreasing for an
“inferior'' good. Accounting for monotonicity  in
estimation is expected to improve accuracy in small and moderate samples
\citep{chetverikov2017nonparametric}.

First, note that since our spline smoothing estimator is linear in parameters, the derivative estimator(\ref{eq: ghat prime}) is linear as well.  Let us express it in matrix form.  Since $\bm{g} = \bm{Z a} + \bm{E \delta}$, we can write
$
\bm{g'} = \bm{O a} + \bm{D \delta}
$,
where $\bm{g'} = \left(g'(Z_1), \ldots g'(Z_n)\right)^T$,
\[
 \bm{D} = \left[ \frac{1}{4} \sign(Z_i - Z_j) |Z_{i}-Z_{j}|^{2}\right]_{i,j=1,\ldots n}\, , \quad 
\bm{O} = 
\left(
\begin{array}{cc}
0 & 1
\\
\vdots & \vdots
\\
0 & 1
\end{array}
\right)
\, .
\]
From Proposition \ref{prop:1},
\begin{equation}
\label{eq:coefs}
\begin{pmatrix} \bm{\widehat{\delta}}\\\bm{\widehat a}
    \end{pmatrix}=
    \bm{S}
    \begin{pmatrix}\bm{Y}\\  \bm{0}
    \end{pmatrix}
    \, ,
    \quad
\bm{S}=\left[
\begin{array}{cc}
\widetilde{\bm{E}} & \bm{Z}
\\
\bm{Z^T} & \bm{0}
\end{array}
\right]^{-1}
\quad  \Rightarrow 
\quad \bm{g'} =
\left( \bm{D},\bm{O} \right)
\bm{S}
\begin{pmatrix} \bm{Y} \\ \bm{0} \end{pmatrix}
\, .
\end{equation}

To obtain a monotone estimator, we rely on a method proposed by \cite{Hall2001}, that is based on the
same linear estimator but  reweights the observations $Y_i$ to impose monotonicity at observations points.  It adjusts the unconstrained estimator
by tilting the empirical distribution to make the least possible change,
in the sense of a distance measure, subject to imposing the constraint of monotonicity at observation points.
Specifically, if $g_0$ is
assumed to be monotonically increasing, we consider the constrained
optimization program
\begin{align}
\label{eq: constrained optimization problem}
\min_{p_1,\ldots,p_n} &n-\sum_{i=1}^n (n p_i)^{1/2}
\\
\text{ subject to } \quad
\sum_{i=1}^n p_i=1\,&,\quad p_i\geq 0\text{ for all }i=1,\ldots,n\quad
,\quad
\left( \bm{D}, \bm{O} \right)
\bm{S}
 \begin{pmatrix} \bm{p} \circ \bm{Y} \\
\bm{0}
\end{pmatrix} \geq 0
\, , \nonumber
\end{align}
where $\bm{p} \circ \bm{Y} = (p_1 Y_1,\ldots, p_n Y_n)^T$ is the
Hadamard product between vectors.
If $g_0$ was assumed to
be monotonically decreasing, we would modify
the last inequalities.
\cite{Hall2001} considered more general optimization problems based on
a family of Cressie-Read divergences, but we focus on the above
program for convenience. It is strictly convex, so it admits a unique
solution $\bm{p^*}$, and it is computationally fast to solve. The
final estimator is ${\widehat{g}^*}$, defined as a smoothing splines estimator
where $\widehat{\bm{a}}^*$ and $\widehat{\bm{\delta}}^*$ are as in (\ref{eq:coefs}) where
$\bm{p^*} \circ \bm{Y}$ replaces $\bm{Y}$. 

We now state the asymptotic properties of our constrained
smoothing splines estimator.
\begin{assumption}\label{ass: monotonicity}
There exists $\eta>0$ such that $g_0'(z)\geq \eta$ for all $z\in[0,1]$. 
\end{assumption} 
\begin{theorem}\label{th.cvrates under monotonicity}
    Under Assumptions \ref{ass: square integrability},
\ref{ass: completeness}, and \ref{ass: monotonicity}  if     $\lambda\rightarrow
    0$ and $n \lambda \rightarrow \infty$, then
    \begin{align*}
        \sup_{z\in[0,1]}|\widehat g^{*}(z)- g_0(z)|=o_P(1)\, \text{ and }
    &\sup_{z\in[0,1]}|\widehat g^{*'}(z)- g'_0(z)|=o_P(1)\, .
    \end{align*}
    If moreover Assumption \ref{ass: source condition} holds, then
    \begin{equation*}
    \sup_{z\in[0,1]}|\widehat g^{*} (z)- g_0(z)|=O_P\left(\frac{1}{\sqrt{n
    \lambda}}+\lambda^{\frac{\gamma \wedge 2}{2}}\right) \text{ and }
    \sup_{z\in[0,1]}|\widehat g^{*'} (z)-
    g'_0(z)|=O_P\left(\frac{1}{\sqrt{n \lambda}}+\lambda^{\frac{\gamma
    \wedge 2}{2}}\right)\, .
    \end{equation*}
\end{theorem}
The above result directly follows from Theorem
\ref{th.cvrates}. Indeed, as $\widehat g '$ is uniformly consistent,
the constraint in the optimization problem (\ref{eq: constrained
optimization problem}) becomes asymptotically irrelevant under
Assumption \ref{ass: monotonicity}. Accordingly, $\widehat
g^*=\widehat g$ with probability approaching one, and our results
readily follow.  While the monotonicity constraints become
asymptotically irrelevant, they can matter in finite samples
as shown by \cite{chetverikov2017nonparametric} and illustrated by our numerical results below.}


\section{Empirical results}\label{sec.emp}

\subsection{Simulations}\label{sec.num}
%\subsection{Simulations with nonmonotonous regression functions}

We used a DGP in line with Equation (\ref{Model}), where
\begin{align*}
    \varepsilon=\frac{aV+\eta}{\sqrt{1+a^2}}\, ,\,&
    a=\sqrt{\frac{\rho_{\varepsilon V}^2}{1-\rho_{\varepsilon V}^2}}\,
    ,\\ 
    Z=\frac{\beta W + V}{\sqrt{1+\beta^2}}\,
    ,\,&\,\beta=\sqrt{\frac{\rho_{WZ}^2}{1-\rho_{WZ}^2}} \, ,
\end{align*}
and $(W,V,\eta)$ are independent standard Gaussian.
This yields standard Gaussian marginal distributions for $\varepsilon$
and $Z$ whatever the values of the parameters.
We chose this setup so as to be able to simply tune the level of endogeneity of $Z$, which is here controlled by the correlation
parameter $\rho_{\varepsilon V}$, and the strength of the instrument $W$, controlled by the  correlation parameter $\rho_{WZ}$.

We implemented our smoothing splines estimator with  $\omega(\cdot)$ equal to the density of a Laplace distribution with mean zero and variance 1.  The choice of the penalty parameter $\lambda$ was based on 2-fold cross-validation.  Namely, we split the data at random into two equally sized folds, we computed $\widehat{g}_{k, \lambda}$ for
each fold $k = 1,2$, and we created the cross-validated vector 
$\tilde{\bm{g}}_\lambda$, with typical element 
$\widehat{g}_{k, \lambda}(Z_i)$, where $k$ is the fold that does not include $Z_i$. We then chose the value of $\lambda$ that minimizes
$M_n(\tilde{\bm{g}}_\lambda)$
%$\widehat{\lambda} = \arg\min_\lambda M_n(\tilde{\bm{g}}_\lambda)$
within the grid  
$\{p/(1-p),p=10^{-5}+k*(0.7-10^{-5})/399,k=0,\dots,399\}$.

We compared our estimator to two existing methods, for which a
data-driven procedure has been proposed for the choices of smoothing and regularization parameters. We considered first the kernel-based Tikhonov estimator of \cite{darolles2011nonparametric}, hereafter
referred as Tikhonov. We used Gaussian kernels of order 2, while the bandwidths were set using Silverman's rule of thumb, i.e.,  equal to $n^{-1/5}$ times the empirical standard deviation of the variable on which smoothing is performed (either $Z$ or $W$).  To select the
regularization parameter, we used the pseudo-cross-validation procedure of \citet{centorrino2017additive} searching for the minimum of the criterion on a grid between 0.0001 and 0.999.  We also considered a series estimator based on a basis of Legendre polynomials.  The number
of polynomials in  first and second stage estimation are chosen to be equal, and this number is selected using the adaptive procedure of \citet{horowitz2014adaptive}. As the method is designed for variables belonging to $[0,1]$, we transformed observations of $Z$ and $W$ by their respective empirical cumulative distribution functions (cdf).  
%(In unreported simulations, we found that using the true cdfs instead did not affect our results much.) 
This implies in particular that even if the relation between $Z$ and $W$ is linear, the first-stage equation is not linear anymore in the transformed variables.


%\spacingset{1.6}
\begin{table}[!ht]
\centering
\begin{tabular}{rl|ccc|ccc|cccc}
  \hline
   & &\multicolumn{3}{c}{$g_{0,1}$} & \multicolumn{3}{c}{$g_{0,2}$} & \multicolumn{4}{c}{$g_{0,3}$}\\
   \hline
   $n$   &  & Sm. & Tikh. & Ser. & Sm.  & Tikh. & Ser. & Cons.  & Sm.  & Tikh. & Ser. \\
% & 1 & 2 & 3 & 4 & 5 & 6 & 7 \\
\hline
%1 &  &  &  &  &  &  &  \\

  & &\multicolumn{10}{c}{$\rho_{ZW}=0.9\,,\, \rho_{\varepsilon V}=0.5$} \\ \hline
200 & Bias$^2$ & 0.000 & 0.148 & 0.092 & 0.001 & 0.065 & 0.005 & 0.000 & 0.000 & 0.088 & 0.104\\
200 & Var & 0.069 & 0.030 & 0.060 & 0.074 & 0.028 & 0.073 & 0.041 & 0.076 & 0.022 & 0.116\\
200 & MSE & 0.069 & 0.177 & 0.152 & 0.075 & 0.092 & 0.078 & 0.044 & 0.076 & 0.111 & 0.219\\
\hline
400 & Bias$^2$ & 0.000 & 0.098 & 0.094 & 0.001 & 0.044 & 0.005 & 0.000 & 0.000 & 0.054 & 0.140\\
400 & Var & 0.052 & 0.020 & 0.031 & 0.053 & 0.018 & 0.025 & 0.026 & 0.056 & 0.015 & 0.044\\
400 & MSE & 0.052 & 0.118 & 0.125 & 0.054 & 0.063 & 0.030 & 0.027 & 0.056 & 0.069 & 0.184\\
\hline
  & &\multicolumn{10}{c}{$\rho_{ZW}=0.9\,,\, \rho_{\varepsilon V}=0.8$} \\ 
  \hline
200 & Bias$^2$ & 0.001 & 0.148 & 0.092 & 0.001 & 0.064 & 0.005 & 0.000 & 0.000 & 0.093 & 0.107\\
200 & Var & 0.066 & 0.028 & 0.060 & 0.072 & 0.026 & 0.074 & 0.039 & 0.072 & 0.019 & 0.107\\
200 & MSE & 0.067 & 0.176 & 0.152 & 0.072 & 0.090 & 0.079 & 0.042 & 0.073 & 0.112 & 0.214\\
\hline
400 & Bias$^2$ & 0.000 & 0.098 & 0.094 & 0.000 & 0.043 & 0.005 & 0.000 & 0.000 & 0.055 & 0.141\\
400 & Var & 0.049 & 0.019 & 0.030 & 0.051 & 0.018 & 0.026 & 0.025 & 0.054 & 0.013 & 0.042\\
400 & MSE & 0.050 & 0.117 & 0.125 & 0.052 & 0.061 & 0.030 & 0.026 & 0.054 & 0.068 & 0.183\\
\hline
  & &\multicolumn{10}{c}{$\rho_{ZW}=0.7\,,\, \rho_{\varepsilon V}=0.8$} \\ \hline
200 & Bias$^2$ & 0.009 & 0.307 & 0.133 & 0.004 & 0.158 & 0.158 & 0.012 & 0.012 & 0.184 & 0.189\\
200 & Var & 0.091 & 0.032 & 0.130 & 0.120 & 0.028 & 0.053 & 0.056 & 0.101 & 0.019 & 0.050\\
200 & MSE & 0.099 & 0.338 & 0.262 & 0.124 & 0.186 & 0.211 & 0.080 & 0.113 & 0.203 & 0.238\\
\hline
400 & Bias$^2$ & 0.003 & 0.218 & 0.139 & 0.004 & 0.110 & 0.157 & 0.002 & 0.002 & 0.123 & 0.190\\
400 & Var & 0.069 & 0.024 & 0.053 & 0.087 & 0.021 & 0.024 & 0.041 & 0.086 & 0.014 & 0.021\\
400 & MSE & 0.073 & 0.242 & 0.192 & 0.090 & 0.131 & 0.180 & 0.051 & 0.089 & 0.138 & 0.212\\
\hline
\end{tabular}
\caption{Average over a grid of 100 equidistant points on $[-2,2]$ and
2000 Monte Carlo replications of the squared bias (Bias$^2$), the
variance (Var), and the Mean Squared Error (MSE) for the 
constrained smoothing splines estimator (Cons.), the smoothing splines estimator (Sm.),  the Tikhonov estimator
(Tikh.), and the series estimator (Ser.). } 
\label{table: simulation results}
\end{table}
%\spacingset{1.9} % CHANGE FOR FINAL VERSION

% Figure environment removed


%% Figure environment removed

We first considered two functional forms for $g_0$, each
normalized to have unit variance: a quadratic function
$g_{0,1}(z) = {z^2}/{\sqrt{2}}$, and a non-polynomial function $g_{0,2}(z) = \sqrt{3\sqrt{3}}\,z\,\exp(-z^2/2)$.
 We ran 2000 Monte Carlo simulations with sample sizes $n=200$ and
$400$. We considered three couples of values for $(\rho_{\varepsilon
V},\rho_{WZ})$: (i) $(0.5,0.9)$, a setting with low endogeneity and a
strong instrument, (ii) $(0.8,0.9)$, corresponding to high endogeneity
and a strong instrument, (iii) $(0.8,0.7)$, a more complex setting with high endogeneity level but a weaker instrument.  Table \ref{table: simulation results}
reports the squared bias, variance, and mean squared error (MSE) of each estimator, averaged on a grid of 100 equidistant points on $[-2,2]$.   We note that the Tikhonov estimator is
severely biased in all cases, while our estimator is close to be unbiased. The series estimator mostly lies in between, but with large differences depending on the setup.
For instance, Figures \ref{fig6:a} and \ref{fig6:b} graph the pointwise average of each estimator for $n=200$ and $(\rho_{\varepsilon V},\rho_{WZ}) = (0.8,0.7)$. Here, the series estimator is much steeper than the true quadratic curve $g_{0,1}$, while it fails to fit the sign changes in the first derivative  for $g_{0,2}$.
%\footnote{It may be surprising that the polynomial series estimator is pretty biased for estimating a quadratic function. However, this estimator needs prior transformations of the variables to make their support $[0,1]$, so $g_{0,1}$ is not quadratic anymore with respect to the transformed variables.}
In both cases, the Tikhonov estimator is much less rough
than the true curves, while the smoothing splines estimator is almost unbiased.

In terms of variance, Tikhonov does better than smoothing
splines, that itself does better than series, in all but one instance.  Smoothing splines performs best in terms of MSE in almost all cases. Exceptions are cases corresponding to the second function with $n=400$ and strong instruments, where the series estimator is close to be unbiased. Overall, the severity of endogeneity does not affect much the estimators' performances.  A decrease in the strength of the instrument has important detrimental effects for all estimators, but   our smoothing splines estimator is affected  to a much lesser degree than  its competitors.

To evaluate the gains of imposing monotonicity, we then considered a third function $g_{0,3}(z) = 
(\sqrt(10/3) \log(|z-1|+1) \sign(z-1)  - 0.6 z+ 2 z^3)/8$. We implemented the smoothing splines estimator as above, in particular the regularization parameter $\lambda$ is chosen before the monotonizing step, and we used the 
the CVXR solver \citep{Fu} to solve (\ref{eq: constrained optimization problem}).
As can be seen from  Figure \ref{fig6:c} and our detailed results in Table \ref{table: simulation results}, imposing monotonicity does not affect much the average smoothing splines estimate, nor its average squared bias, but yields a substantial decrease in variance. Depending on the particular setup, it can be more than halved.


\subsection{Application}\label{sec.ea}


{We applied our smoothing splines estimator to the estimation of Engel curves, which relate the proportion of spending on a given good to total expenditures. We used the “Engel95'' data from the
 {\tt R} package {\tt np}, which is a random sample from the 1995
 British Family Expenditure Survey and contains data for 1655
 households of married couples for which the head-of-household is
 employed and between 25 and 55 years old. We focused on the subsample
 of 628 households with no kids.  We report results for two Engel
 curves, pertaining to the expenditure shares on {leisure and fuel}.
 Economic theory suggests that the Engel curve for leisure is
 increasing and the one for fuel is decreasing.  Following
 \cite{blundell2007semi}, we instrumented the endogenous variable
 \textit{logarithm of total household's expenditure} by the
 \textit{logarithm of total earnings before tax}.  We consider the four
 estimators used in our simulations, and implementation details remain the same.
 
The resulting estimated nonparametric functions are reported in Figure  \ref{fig7}. 
The Tikhonov estimate exhibits a non-monotonic and quite
 irregular behavior, while the series estimate is mainly monotonic and
 very regular.  Since our smoothing splines estimates are monotonic,
 but at the boundaries of the data, our constrained and unconstrained
 estimates are very close. Both are in line with the findings of
 \cite{blundell2007semi}.}



% Figure environment removed



\section{Conclusion}
\label{sec:conc}

This paper generalizes regression smoothing splines to the context where there is endogeneity. We have considered the special case of cubic splines, but our approach easily generalizes to other spline orders depending upon the roughness measure chosen in the penalization. Though we have focused on a simple univariate nonparametric model, our work opens the path for considering more general models.
A first example is the partly linear model \citep{heckman1986Spline}
\[
Y= X^T \beta + g_0 (Z)+ \varepsilon \qquad E(\varepsilon|X,W)=0
\, .
\]
Our method of proof based on profiling should easily extend to this model.
A second example is the multivariate model
\[
Y  =  g_{0} (X,Z) +  \varepsilon \qquad E (\varepsilon|X,W) = 0
\, . 
\]
Consider for instance univariate variables $X$ and $Z$, and the roughness penalty
\[
J (g) = \int{ \left\{ \frac{\partial^{2} g}{\partial x^{2}} + 2 \frac{\partial^{2} g}{\partial x \partial z} + \frac{\partial^{2} g}{\partial z^{2}} \right\} \, dx \, dz}
\, .
\]
A function $g(\cdot)$  is a natural thin plate spline with knots
$(t_{1}, \ldots t_{n})$, $t_i = (X_i, Z_i)$, if
\[
g (t) = a_{0} + t'b + \sum_{i=1}^{n}{ \delta_{i}
\eta \left( \| t-t_{i} \|\right)}\, ,
\qquad  \eta (r) = \frac{1}{16\pi} r^{2} \log r^{2}
\, ,
\]
where $\sum_{i=1}^n \delta_i  = \sum_{i=1}^n X_i \delta_i  = \sum_{i=1}^n Z_i \delta_i  = 0$.
It is known that the natural thin plate spline uniquely minimizes $J (g)$
among functions such that $g (t_{i}) = g_{i}, i=1, \ldots n$ \citep{green1993nonparametric}.

Since multivariate nonparametric estimators are subject to the curse of dimensionality, it may be wiser to consider an additive model such as
\[
Y=\sum_{j=1}^J g_{0,j}(Z_j)+\varepsilon \qquad E(\varepsilon|W)=0
\, ,
\]
where $W$  contain instruments, which may include some of the variables $Z_j$ if these are deemed exogenous. All these extensions are left for further research.

%\newpage
\bigskip
\bibliographystyle{apalike}
\bibliography{biblio3}


\newpage
\appendix
\renewcommand{\theequation} {A.\arabic{equation}}

%\bigskip
\begin{center}
{\Large\bf Appendix}
\end{center}


\section{Proof of Proposition \ref{prop:1}}

We begin by studying  $\bm{E}$ and $\bm{\Omega}$. First, for any natural cubic spline \citep[Section 7.3]{green1993nonparametric},
\[
\int  g'' (z) ^{2} \, dz  = \bm{\delta^T {E} \delta} \geq 0
\, ,
\]
so  $\bm{E}$ is positive semi-definite. Second, let $\bm{b}=(b_1,\ldots,b_n)^T\in \mathbb{R}^n$, then
\[
%\label{eq: quadratic form and Cramer Von Mises}
    \bm{b^T \Omega b}=\int_{ }\left|\frac{1}{n}\sum_{i=1}^n b_i
    \exp(\textbf{i}W_i^T t)\right|^2 \mu(dt)\geq 0\, .
\]
Hence  $\bm{b^T\Omega b}=0$ iff
$ \frac{1}{n}\sum_{i=1}^n b_i \exp(\mathbf{i}W_i^T t)=0$  for all
$t\in\mathbb{R}^q$.
Define the random vector $(\widetilde{b},\widetilde{\bm{W}})$  that
equals $(b_i,W_i)$ with probability $1/n$, and  $\widetilde{\E}$ the
corresponding expectation.
Then, $\widetilde{\E}\{\widetilde{b}\exp(\mathbf{i}\widetilde{\bm{W}}^T t)\}=0$
for all $t\in\mathbb{R}^q$. From \cite{Bierens1982}, this implies that
$ \widetilde{\E}\{\widetilde{{b}}\,|\,\widetilde{\bm{W}} = \bm{W}_i \}=0$.
Since $ \widetilde{\E}\{\widetilde{{b}}\,|\,\widetilde{\bm{W}} = \bm{W}_i\}
= b_i$  if all $W_i$s are different,  $b_i=0$ for all
$i=1,\ldots,n$. Hence, $\bm{\Omega}$ is positive definite.

Let us then write   (\ref{eq:min}) as
\[
\left(
\begin{array}{c}
\bm{\delta}
\\
\bm{a}
\end{array}
\right)^{\bm{\top}}
\left[
\begin{array}{cc}
\bm{E^T \Omega E} + \lambda \bm{E} & \bm{E^T \Omega Z}
\\
\bm{Z^T\Omega E} & \bm{Z^T\Omega Z}
\end{array}
\right]
\left(
\begin{array}{c}
\bm{\delta}
\\
\bm{a}
\end{array}
\right)
- 2\left(
\begin{array}{c}
\bm{\delta}
\\
\bm{a}
\end{array}
\right)^{\bm{\top}}
\left(
\begin{array}{c}
\bm{E^T \Omega Y} 
\\
\bm{Z^T \Omega Y}
\end{array}
\right)
+ \bm{Y^T \Omega Y}
\, .
\]
This is a convex problem in $(\bm{\delta^T}, \bm{a^T})^T$.
Differentiating  with respect to $\left(\bm{\delta}^T, \bm{a}^T\right)^T $ yields
\begin{equation*}
\left[
\begin{array}{cc}
\bm{E^T \Omega}  & \bm{0}
\\
\bm{Z^T \Omega} & - \lambda \bm{I}
\end{array}
\right]
\left[
\begin{array}{cc}
\widetilde{\bm{E}} & \bm{Z}
\\
\bm{Z^T} & \bm{0}
\end{array}
\right]
\left(
\begin{array}{c}
\bm{\widehat{\delta}}
\\
\bm{\widehat{a}}
\end{array}
\right)
=
\left[
\begin{array}{cc}
\bm{E^T \Omega}  & \bm{0}
\\
\bm{Z^T \Omega} & - \lambda \bm{I}
\end{array}
\right]
\left(
\begin{array}{c}
\bm{Y}
\\
\bm{0}
\end{array}
\right)
\, .
\end{equation*}
%For any $\lambda >0$, the first matrix is  full rank, as $\bm{\Omega}$
%and $\bm{E}$ are positive definite.
This yields  (\ref{estimformula}), and the  solution satisfies the constraint
 $\bm{Z^T \widehat{\delta}} = \bm{0}$.
% The second-order condition
%ensures a unique solution, as
The matrix in (\ref{estimformula}) is full rank if $\bm{Z}$ is full
rank and $\lambda > 0$. Indeed, assume
\[
\left[
\begin{array}{cc}
\bm{E} + \lambda  \bm{\Omega}^{-1} &  \bm{Z}
\\
\bm{Z^T} & \bm{0}
\end{array}
\right]
\left(
\begin{array}{c}
\bm{\delta}
\\
\bm{a}
\end{array}
\right)
=
\left(
\begin{array}{c}
\bm{0}
\\
\bm{0}
\end{array}
\right)
\, ,
\]
this implies $\bm{Z^T\delta} = \bm{0}$ and
$0= \bm{\delta^T}
\left[ \left(\bm{E} + \lambda \bm{\Omega}^{-1} \right) \bm{\delta}  +  \bm{Z a} \right] =
\bm{\delta^T}  \left(\bm{E} + \lambda \bm{\Omega}^{-1} \right)
\bm{ \delta}$. Since $\bm{E} + \lambda \bm{\Omega}^{-1}$ is positive
definite, this implies that $\bm{\delta} = \bm{0}$, and in turn
 that $\bm{a} = \bm{0}$ as $\bm{Z}$ is full rank.

To obtain the values at the knots $\widehat{\bm{g}}$, note that the inverse of the matrix
in (\ref{estimformula}) is
\begin{equation}
\left[
\begin{array}{cc}
\widetilde{\bm{E}}^{-1} \left(\bm{I}-\bm{P}\right)
  &
 \widetilde{\bm{E}}^{-1} \bm{Z} \left(\bm{Z}^T \widetilde{\bm{E}}^{-1} \bm{Z} \right)^{-1}
\\
\left(\bm{Z}^T \widetilde{\bm{E}}^{-1} \bm{Z} \right)^{-1}  \bm{Z}^T \widetilde{\bm{E}}^{-1}
 &  - \left(\bm{Z}^T \widetilde{\bm{E}}^{-1} \bm{Z} \right)^{-1}
\end{array}
\right]
\, ,
\label{invmat}
\end{equation}
where $\bm{P}= \bm{Z} \left(\bm{Z}^T \widetilde{\bm{E}}^{-1} \bm{Z}
\right)^{-1} \bm{Z}^T \widetilde{\bm{E}}^{-1}$  is the oblique projection on
the span of $\bm{Z}$  along the direction spanned by vectors $\bm{h}$
such that  $\bm{Z}^T  \widetilde{\bm{E}}^{-1} \bm{h} = 0$.  Hence,
\begin{align*}
\left(
\begin{array}{c}
\bm{E} \widehat{\bm{\delta}}
\\
\bm{Z} \widehat{\bm{a}}
\end{array}
\right)
&  =
\left[
\begin{array}{c}
\bm{E} \widetilde{\bm{E}}^{-1} \left(\bm{I}-\bm{P}\right) \bm{Y}
\\
\bm{P} \bm{Y}
\end{array}
\right]
\, .
\end{align*}
Use  $\widehat{\bm{g}}  =  \bm{Z \widehat{a}} + \bm{E
\widehat{\delta}}$  to obtain the desired result.


\section{Proof of Theorem \ref{th.cvrates}}


We start by introducing some useful notations and results. Let
 $\mathcal{X}$ and $\mathcal{Y}$ be Hilbert spaces with corresponding
 inner products $\left<\cdot,\cdot\right>_\mathcal{X}$ and
 $\left<\cdot,\cdot\right>_\mathcal{Y}$, and consider a linear
 operator $\operatorname{D}:\mathcal{X}\mapsto \mathcal{Y}$.  The norm
 of $\operatorname{D}$ is
 $\|\operatorname{D}\|_{op}=\sup_{f\in\mathcal{X},\|f\|_{\mathcal{X}}=1}\|\operatorname{D}
 f\|_\mathcal{Y} $.  When $\|\operatorname{D}\|_{op}<\infty$,
 $\operatorname{D}$ is said to be bounded (or continuous), see
 \citet[Chapter 2]{kress1999linear}.  Let $\operatorname{D}^*$ be the
 adjoint of $\operatorname{D}$, defined as
 $\operatorname{D}^*:\mathcal{Y}\mapsto \mathcal{X}$ such that
 $\left<\operatorname{D}
 f,\psi\right>_{\mathcal{Y}}=\left<f,\operatorname{D}^*\psi,\right>_{\mathcal{X}}$
 for any $(f,\psi)\in\mathcal{X}\times \mathcal{Y}$.  When
 $\operatorname{D}$ is bounded, $\operatorname{D}^*$ always exists and
 $\|\operatorname{D}\|_{op}=\|\operatorname{D}^*\|_{op}$, see
 \citet[Theorem 4.9]{kress1999linear}.
In what follows, we will repeatedly use the
 following properties: (i) $\|\operatorname{D}f\|_{\mathcal{Y}}\leq
\|\operatorname{D}\|_{op}\|f\|_{\mathcal{X}}$ for any
$f\in\mathcal{X}$, and  (ii) if $\operatorname{C}$ is another linear
operator, then $\|\operatorname{C}\operatorname{D}\|_{op}\leq
\|\operatorname{C}\|_{op}\|\operatorname{D}\|_{op}$, whenever the
composition $\operatorname{C}\operatorname{D}$ is well defined.

\subsection{Main proof}

We divide the proof into several steps. In Step 1, we analyze the
minimization problem at the population level. In Step 2, we analyze
the problem at the sample level.  In Step 3, we bound the norm of
$\widehat{h} - h_0$.  In Step 4 and 5, we combine the results to first
establish uniform consistency of $\widehat g$ and its first
derivative, second to obtain uniform rates of convergence.

\textbf{Step 1}.
From Assumption  \ref{ass: completeness} and \cite{Bierens1982},
\[
g = 0 \Leftrightarrow
\E \left\{   g (Z)  |  W \right\}= 0 \Leftrightarrow
\E \left\{   g (Z)  \exp(\mathbf{i} W^\top t) \right\} = 0 \ \forall
t\in\R^p
\, .
\]
Hence,  the null space of the linear mapping $g\mapsto
\E\{g(Z)\exp(\textbf{i}W^T\cdot)\}$ only
contains the zero element, and  such a mapping is injective (one-to-one).
This implies that $\A h=\E\{h(Z)\exp(\mathbf{i}W^T\cdot)\}$ and $\B \beta=\E\{(1,Z)\beta
\exp(\mathbf{i}W^T\cdot)\}$ are also injective.

Each $g\in\mathcal{G}$ can be uniquely written as
$g(z)=(1,z)\beta+h(z)$, where $\beta=(g(0),g'(0))$,
$h(z)=g(z)-g(0)-g'(0)z$,  $h(\cdot) \in\mathcal{H}$.
Hence, the intersection of the ranges of the operators $\A$ and $\B$ is
the null function, since
$\A{h}=\B{\beta}$ iff $ (1, z) \beta - h(z) = 0$.

Consider the problem
\begin{equation}
\label{eq: partly Linear population program}
   \min_{\beta,h} \| r-\B\beta - \A h
   \|_\mu^2
   \, , \qquad r=\E\{Y\exp(\textbf{i}W^T \cdot)\}
   \, ,
\end{equation}
where $\|\cdot\|_\mu$ is the $L^{2}_\mu$ norm.
If $g_0(z)=(1,z)\beta_0+h_0(z)$, then $(\beta_0,h_0)$ is the unique
solution. We will obtain an explicit expression of $(\beta_0,h_0)$
solving (\ref{eq: partly Linear population program}).  Let $\P$ be the
orthogonal projection operator of functions in $L^2_\mu$ onto
$\mathcal{R}(\B)$ the range of $\B$.  Since $\B$ is defined on
$\mathbb{R}^2$, its range $\mathcal{R}(\B)$ is a linear finite
dimensional space. As linear finite dimensional spaces are complete,
see \citet[Theorem 2.4-2 ]{kreyszig1978introductory},
$\mathcal{R}(\B)$ is also linear and complete. By \citet[Theorem
1.26]{kress1999linear}, projection operators onto linear and complete
spaces are well defined, and so is $\P$.
Let us show that  $\P$ writes as $\B(\B^*\B)^{-1}\B^*$, where $\B^*$ is the adjoint of $\B$.  As previously noted, $\B$ is injective and its null space is $\mathcal{N}(\B)=\{0\}$. Then  $\mathcal{N}(\B^*\B)=\mathcal{N}(\B) = \{0\}$, $\B^*\B$ is injective, and $(\B^*\B)^{-1}$ exists. As linear operators mapping $\mathbb{R}^2$ into $\mathbb{R}^2$ are uniquely characterized by second order matrices, see \citet[Section 2.9]{kreyszig1978introductory}, $\B^*\B$ is a second order matrix, as well as its inverse. Hence, the operator $\B(\B^*\B)^{-1}\B^*:L^2_\mu\mapsto L^2_\mu $ is well defined.
For any $f\in L^2_\mu$ and $\beta\in\mathbb{R}^2$,
\begin{align*}
    \left< f-\B(\B^*\B)^{-1}\B^*f,\B\beta\right>_\mu=\left<\B^*f-\B^*f,\beta\right>=0\, .
\end{align*}
Hence, $f-\B(\B^*\B)^{-1}\B^*f\perp \mathcal{R}(\B)$, and $\B(\B^*\B)^{-1}\B^* f$ indeed represents the projection of $f$ onto $\mathcal{R}(\B)$, see  \citet[]{kress1999linear}. Therefore, $ \P =\B(\B^*\B)^{-1}\B^*$.



Let $\M = I-\P$ be the orthogonal projection onto the orthogonal complement of $\mathcal{R}(\B)$. Then
\[
r=\B \beta_0 +\A h_0 \Rightarrow \M r = \M \A h_0  = \T h_0
\, .
\]
The operator $\T= \M \A$ is injective since  the intersection of the
  ranges of  $\A$ and $\B$ is the null  function and $\A$  is
  injective. This yields
  \[
   h_0=\T^{-1}\M r\, , \qquad \beta_0=(\B^*\B)^{-1}\B^*(r-A h_0)\, .
\]


Consider now the penalized problem
\begin{equation}
\label{eq: partly linear integral equation penalized}
\min_{(\beta,h)\in\mathbb{R}^2 \times \mathcal{H}}
\| r - {\A}h - {\B}\beta\|^2_{\mu}+\lambda \|h\|^2_{\mathcal{H}}
\, .
\end{equation}
Let us profile with respect to $\beta$. For any fixed $h$,
\[
\min_{\beta\in \mathbb{R}^2 } \| r - {\A}h - {\B}\beta\|^2_{\mu}
 =
\| r - {\A}h - {\P}(r-Ah) \|^2_{\mu}
 =
\|\M r - {\T}h  \|^2_{\mu}
\, .
\]
We thus need to solve first
\[
\min_{h\in\mathcal{H}}
\| \M r - {\T}h \|^2_{\mu}+\lambda \|h\|^2_{\mathcal{H}}
\, .
\]
From Lemma \ref{lm.cvop}(i) below, ${\T}$ is compact, and thus
bounded. A direct application of
\citet[Theorem 16.1]{kress1999linear} ensures that the unique solution
$h_\lambda$ to the above minimization problem satisfies
$({\T}^*{\T}+\lambda I) h_\lambda={\T}^*{\M} {r}$.
Now, for any $h$,
\[
\lambda \| h \|^{2}_{\mathcal{H}} \leq \lambda \| h \|^{2}_{\mathcal{H}} + \|\T h\|^{2}_\mu
=
\lambda \left<h,h\right>_{\mathcal{H}} + \left<h,\T^{*} \T h\right>_{\mathcal{H}}
=
\left<h, ({\T}^*{\T}+\lambda I) h\right>
\, .
\]
Hence, $({\T}^*{\T}+\lambda I)$ is strictly coercive and has a bounded
inverse by the Lax-Milgram
Theorem, see \citet[Theorem 13.26]{kress1999linear}. Therefore,
\begin{equation}
 h_\lambda= ({\T}^*{\T}+\lambda I)^{-1} {\T}^*{\M} {r}
 \, 
 \label{eq:hlambda}
\end{equation}
%The  value of $\beta$ minimizing (\ref{eq: partly linear integral equation penalized}) is thus $(\B^*\B)^{-1}\B^*(r-A h_\lambda)$.
 
\textbf{Step 2}. We study the minimization problem at the sample level
and we obtain sample counterparts of the population objects of Step 1.
Recall that  $\widehat g$ solves
\begin{equation}\label{eq: sample penalized program}
  \min_{g\in\mathcal{G}} \int_{ }\left|\frac{1}{n}\sum_{i=1}^n [Y_i -
  g((Z_i))]\exp(\textbf{i} W_i^T t)\right|^2 \mu(d  t)+\lambda\int_{0}^1 |g''(z)|^2 dz\, .
\end{equation}
By Proposition \ref{prop:1}, under Assumption \ref{ass: square
integrability}, the solution $\widehat{g}$ is unique with probability
1, and since each
$g\in\mathcal{G}$ writes uniquely as $g(z)=(1,z)\beta+h(z)$,
%with $\beta=(g(0),g'(0))$ and $h(z)=g(z)-g'(0)z-g(0)$,
there is a unique $(\widehat \beta, \widehat h)$ such that
$\widehat{g}(z)=(1,z)\widehat{\beta}+\widehat{h}(z)$.  Define
\begin{align}
\widehat{\A}:\mathcal{H}\mapsto L^2_\mu\, , & \qquad
\widehat{\A}h =\frac{1}{n}\sum_{i=1}^n h(Z_i)    \exp(\textbf{i}W_i^T
\cdot)\, , \\
\widehat{\B}:\mathbb{R}^2\mapsto L^2_\mu\, , & \qquad
\widehat{\B}\beta =\frac{1}{n}\sum_{i=1}^n (1,Z_i)\beta
\exp(\textbf{i}W_i^T \cdot)\, ,
\end{align}
and $\widehat r =(1/n)\sum_{i=1}^n Y_i \exp(\textbf{i}W_i^T \cdot)$.
The optimization problem   (\ref{eq: sample penalized program}) is equivalent to
\begin{equation}\label{eq: sample penalized program with beta and h v2}
\min_{(\beta,h)\in\mathbb{R}^2 \times \mathcal{H}}
\|\widehat{r} - \widehat{\A}h - \widehat{\B}\beta\|^2_{\mu}+\lambda \|h\|^2_{\mathcal{H}}\, .
\end{equation}
We will  profile with respect to $\beta$, and to do so require dealing
with the orthogonal projection onto the range of $\widehat{\B}$. Let
us proceed as in Step 1.
First,
\[
\|\widehat{\B}\beta\|^2_{\mu}
= 0 \Leftrightarrow
\int_{ }\left|\frac{1}{n}\sum_{i=1}^n (1,Z_i)\beta
\exp(\textbf{i}W_i^T t)\right|^2 \mu(dt)=0 \Leftrightarrow
\beta^T\bm{Z}^T\bm{\Omega}\bm{Z}\beta=0
\, .
\]
From Assumption \ref{ass: square integrability}(v), $W$ has at least
one continuous component, so that all $W_i$'s are different with
probability  1, and thus $\bm{\Omega}>0$ with probability
 1 from the proof of Proposition \ref{prop:1}.  Hence,
$\|\widehat{\B}\beta\|^2_{\mu} = 0$ iff $\bm{Z}\beta=0$.  As $\bm{Z}$
has full column rank with probability  1 from Assumption
\ref{ass: square integrability}, $\|\widehat{\B}\beta\|^2_{\mu} = 0$
iff $\beta=0$, and $\B$ is injective.  Let $\widehat{\P}$ be the
orthogonal projection onto the range of $\widehat{\B}$, which is well
defined and can be expressed as $\widehat{\P}
=\widehat{\B}(\widehat{\B}^*\widehat
\B)^{-1}\widehat{\B}^*$. Then,
\[
\min_{\beta\in\mathbb{R}^2}\|\widehat r -
\widehat{\A}h-\widehat{\B}\beta\|^2_\mu
=
\|\widehat{r}-\widehat{A}h-\widehat{P}(\widehat{r}-\widehat{A}h)\|_\mu=
\|\widehat{\M} \widehat r - \widehat{\T} h\|^2_\mu
\, ,
\]
where    $\widehat{\M} = I-\widehat{\P}$ and
$\widehat{\T}=\widehat{\M} \widehat{\A}$.
We thus need to solve
\begin{align}\label{eq: sample profiled min program}
\min_{h\in\mathcal{H}}\|\widehat{\M} \widehat{r} -
\widehat{\T}h\|^2_\mu + \lambda \|h\|^2_\mathcal{H}\, .
\end{align}
From Lemma \ref{lm.cvop}(v) below, $\widehat{\T}$ is compact, and thus
bounded. Thus, using a similar reasoning as in Step 1, the unique solution  is
\begin{align}\label{eq: expression of hhat}
  \widehat h = (\widehat{\T}^*\widehat{\T}+\lambda I)^{-1}
  \widehat{\T}^*\widehat{\M} \widehat r
\, ,
\end{align}
which in turn yields
\begin{equation}
\label{eq: expression for betahat}
 \widehat
 \beta=(\widehat{\B}^*\widehat{\B})^{-1}\widehat{\B}^*(\widehat r -
 \widehat{\A}\widehat h)
 \, .
 \end{equation}


\textbf{Step 3}.
We now  prove that
\begin{equation}
\label{eq: rate for hhat depending on the regularization bias}
    \|\widehat h - h_0\|_\mathcal{H}=O_P\left( \frac{1}{\sqrt{n
    \lambda}}+\|h_{\lambda}-h_0\|_\mathcal{H}\right)
    \, .
\end{equation}


We will  rely on the following results from \citet[Lemma A.1]{florens2011identification}.
\begin{lemma}\label{lm.bounds}
Consider two Hilbert spaces  $\mathcal{X}$ and $\mathcal{Y}$  and
 a linear compact operator $\operatorname{K}:\mathcal{X}\mapsto
 \mathcal{Y} $. Then there are universal constants $c$ and $c'$ such that
\begin{enumerate}[\textup{(}i\textup{)}]
\item\label{boundii}$||\lambda(\lambda \operatorname{I} +
\operatorname{K}^* \operatorname{K})^{-1}||_{op}\leq c$,
\item\label{boundiii}$||(\lambda \operatorname{I} + \operatorname{K}^*
\operatorname{K} )^{-1}\operatorname{K}^* ||_{op}\leq
\frac{c'}{\sqrt{\lambda}}$.
\end{enumerate}
\end{lemma}
We will also use the following results, whose proofs are  postponed to the next section.
\begin{lemma} \label{lm.cvop}
Under Assumptions \ref{ass: completeness} and \ref{ass: square
integrability}, the following holds
\begin{enumerate}[\textup{(}i\textup{)}]
    \item\label{cratei} The operators $\A$ and $\T$ are compact;
    \item \label{crateii} $\|\widehat{\B}-\B\|_{op}=O_P(n^{-1/2})$;
    \item\label{crateiii} $\|(\widehat{\B}^*\widehat{\B})^{-1}- (\B^*
    \B)^{-1}\|_{op} = O_P(n^{-1/2})$ and $\|\widehat{\M}-\M\|_{op}=O_P(n^{-1/2})$;
    \item\label{crateiv} $\|\widehat{\A}-\A\|_{op}=O_P(n^{-1/2})$;
    \item  \label{cratev} The operator $\widehat{\T}$ is compact;
    \item\label{cratevi} $\|\widehat{\T}-\T\|_{op}=O_P(n^{-1/2})$;
    \item\label{cratevii} $\left\|\widehat{r}-r\right\|_\mu=O_P(n^{-1/2})$;
    \item\label{crateviii}
    $\|\widehat{\M}\widehat{r}-\widehat{\T}h_0\|_\mu=O_P(n^{-1/2})$.
\end{enumerate}
\end{lemma}

We consider  the  decomposition
$\widehat h - h_0=S_1+S_2+S_3+S_4+h_\lambda-h_0$, where
\begin{gather*}
    S_1=(\T^* \T + \lambda I )^{-1}\T^* (\widehat{\M}\widehat r-
    \widehat{\T} h_0)\, , \quad
    S_2=(\T^* \T + \lambda I )^{-1}(\widehat{\T}^*-
    \T^*)(\widehat{\M}\widehat r - \widehat{\T}h_0)\, ,\\
    S_3= \left[(\widehat{\T}^* \widehat{\T}+\lambda I)^{-1} - (\T^* \T
    + \lambda I )^{-1} \right]\widehat{\T}^* (\widehat{\M}\widehat r
    - \widehat{\T}h_0)\, ,  \quad
    S_4=(\widehat{\T}^* \widehat{\T}+\lambda I)^{-1}\widehat{\T}^*
    \widehat{\T}h_0 - h_\lambda\, .
\end{gather*}
We have
\begin{align*}
%\label{eq: S 1}
    \|S_1\|_{\mathcal{H}}\leq \|(\T^* \T + \lambda I
    )^{-1}\T^*\|_{op}\,\|\widehat{\M}\widehat r -
    \widehat{\T}h_0\|_{\mu}=O_P\left(\frac{1}{\sqrt{n
    \lambda}}\right)\, .
\end{align*}
Indeed,  $\T$ is a compact operator  from Lemma \ref{lm.cvop}(i),
$\|(\T^* \T + \lambda I )^{-1}\T^*\|_{op}\leq c'/\sqrt{\lambda}$ from Lemma \ref{lm.bounds}(ii), and
$\|\widehat{\M}\widehat r - \widehat{\T}h_0\|_{\mu}=O_P(1/\sqrt{n})$
from Lemma \ref{lm.cvop}(viii).
Next,
\begin{align*}
%\label{eq: S 2}
    \|S_2\|_{\mathcal{H}}\leq \|(\T^* \T + \lambda I
    )^{-1}\|_{op}\,\|\widehat{\T}^*-\T^*\|_{op}\,\|\widehat{\M}\widehat
    r - \widehat{\T}h_0\|_{\mu}=O_P\left(\frac{1}{n \lambda}\right)\, ,
\end{align*}
as  $\|(\T^* \T + \lambda I )^{-1}\|_{op}\leq
c/\lambda$ from Lemma \ref{lm.bounds}(i),
$\|\widehat{\T}-\T\|_{op}=O_P(1/\sqrt{n})$  and $\|\widehat{\M}\widehat r -
\widehat{\T}h_0\|_{\mu}=O_P(1/\sqrt{n})$ from Lemma
\ref{lm.cvop}(vi) and (viii). Next,
\begin{align*}
%\label{eq: S 3}
    \|S_3\|_{\mathcal{H}}\leq &\|(\widehat{\T}^* \widehat{\T}+\lambda
    I)^{-1}\widehat{\T}^* - (\T^* \T + \lambda I
    )^{-1}\widehat{\T}^*\|_{op}\, \|\widehat{\M}\widehat r -
    \widehat{\T}h_0\|_{\mu}\nonumber \\
    \leq & \|(\widehat{\T}^* \widehat{\T}+\lambda
    I)^{-1}\widehat{\T}^*-(\T^* \T + \lambda I )^{-1}\T^*-(\T^* \T +
    \lambda I )^{-1}(\widehat{\T}^*-\T^*)\|_{op}\,
    \|\widehat{\M}\widehat r - \widehat{\T}h_0\|_{\mu}\nonumber \\
    \leq & \|(\widehat{\T}^* \widehat{\T}+\lambda
    I)^{-1}\widehat{\T}^* -(\T^* \T + \lambda I )^{-1}\T^* \|_{op}\,
    \|\widehat{\M}\widehat r - \widehat{\T}h_0\|_{\mu}\nonumber \\
    &+ \|(\T^* \T + \lambda I )^{-1}\|_{op}\,\|\widehat{\T}^* -
    \T^*\|_{op}\,\|\widehat{\M}\widehat r -
    \widehat{\T}h_0\|_{\mu}\nonumber = O_P\left(\frac{1}{\sqrt{n \lambda}}\right)\, .
\end{align*}
Indeed,  $\widehat{\T}$ and $\T$ are
compact operators  from Lemma \ref{lm.cvop}(i) and (v), so
$\|(\widehat{\T}^* \widehat{\T}+\lambda I)^{-1}\widehat{\T}^* -(\T^*
\T + \lambda I )^{-1}\T^*\|_{op}\leq 2 c'/\sqrt{\lambda}$  and
$\|(\T^* \T+\lambda I)^{-1}\|_{op}\leq
c/\lambda$ by Lemma \ref{lm.bounds}. Moreover,
$\|\widehat{\T}^*-\T^*\|_{op}=O_P(1/\sqrt{n})$ from Lemma
\ref{lm.cvop}(vi).
Finally,
\begin{align*}
\|S_4\|_{\mathcal{H}}=&\|(\widehat{\T}^*\widehat{\T}+\lambda I)^{-1}
\widehat{\T}^*\widehat{\T} h_0 - (\T^*\T + \lambda I)^{-1} \T^*\T
h_0\|_{\mathcal{H}}
\nonumber\\
=& \|(\widehat{\T}^*\widehat{\T}+\lambda I)^{-1}
(\widehat{\T}^*\widehat{\T} +\lambda I - \lambda I) h_0
- (\T^*\T + \lambda I )^{-1}(\T^*\T+\lambda I - \lambda I )
h_0\|_{\mathcal{H}}
\nonumber\\
=& \|\lambda [(\T^*\T + \lambda I )^{-1} -
(\widehat{\T}^*\widehat{\T}+\lambda I)^{-1}] h_0\|_{\mathcal{H}}
\nonumber\\
=& \|\lambda(\widehat{\T}^*\widehat{\T} + \lambda I )^{-1}
[\widehat{\T}^* \widehat{\T} - \T^*\T] (\T^*\T+\lambda I)^{-1}
h_0\|_{\mathcal{H}}
\nonumber\\
= & \|\lambda(\widehat{\T}^*\widehat{\T} + \lambda I )^{-1}
[\widehat{\T}^* (\widehat{\T}-\T) + (\widehat{\T}^*-\T^*)\T]
(\T^*\T+\lambda I)^{-1} h_0\|_{\mathcal{H}}
\nonumber\\
\leq & \|(\widehat{\T}^*\widehat{\T}+\lambda I)^{-1}
\widehat{\T}^*\|_{op}\, \|\widehat{\T}-\T\|_{op}\,\|\lambda(\T^*\T + \lambda I)^{-1}\|_{op}
\,\|h_0\|_{\mathcal{H}}
\nonumber\\
& + \|\lambda (\widehat{\T}^*\widehat{\T}+\lambda I)^{-1}\|_{op}
\,\|\widehat{\T}^*-\T^*\|_{op}\,
\|\T(\T^*\T+\lambda I)^{-1}\|_{op} \,\|h_0\|_{\mathcal{H}} =
O_P\left(\frac{1}{\sqrt{n \lambda}}\right) \, . \nonumber
\end{align*}
Here we use that $\|\widehat{\T}-\T\|_{op}=
 \|\widehat{\T}^*-\T^*\|_{op} = O_P(1/\sqrt{n})$  from Lemma
 \ref{lm.cvop}(vi), and that   $\|\lambda(\T^*\T+\lambda
 I)^{-1}\|_{op}\leq c$,
 $\|\lambda(\widehat{\T}^*\widehat{\T}+\lambda I)^{-1}\|_{op}\leq c$,
 and  $\|(\widehat{\T}^*\widehat{\T} + \lambda I )^{-1}
 \widehat{\T}^*\|_{op}\leq c'/\sqrt{\lambda}$   from Lemma \ref{lm.bounds}.
Gathering results gives  (\ref{eq: rate for hhat depending on the
regularization bias}).



\textbf{Step 4}.
We here show convergence of our estimators. Since
$\T$ is injective from Step 1 and  compact from Lemma
\ref{lm.cvop}(i),  $\|({\T}^* \T +\lambda {I})^{-1}
{\T}^*{\T} h - h \|_{\mathcal{H}} = o(1)$ for all $h$ whenever
$\lambda \rightarrow 0$, see \citet[Definition 15.5 and Theorem
15.23]{kress1999linear}. Hence $
\|h_\lambda-h_0\|_{\mathcal{H}}=o(1)$.
This and (\ref{eq: rate for hhat depending on the regularization bias}) yields $\|\widehat h - h_0\|_{\mathcal{H}}=o_P(1)$ if
in addition $n\lambda \rightarrow \infty$.

We now show that $ \|\widehat \beta -    \beta_0
\|=O_P\left(\frac{1}{\sqrt{n}}+\|\widehat   h-h_0\|_\mathcal{H}
\right)$. From (\ref{eq: expression for betahat}),
\begin{align*}
    \widehat{\beta}-\beta_0=&
    [(\widehat{\B}^*\widehat{\B})^{-1}- (\B^* \B)^{-1}]
    \widehat{\B}^*(\widehat r - \widehat{\A}\widehat h)
    + (\B^*\B)^{-1}
    [\widehat{\B}^*-\B^*](\widehat{r}-\widehat{\A}\widehat h) \\
    & + (\B^* \B)^{-1} \B^* (\widehat r - r)
     + (\B^* \B)^{-1} \B^* (\A-\widehat{\A})\widehat h
     - (\B^* \B)^{-1} \B^*\A(\widehat h - h_0)
\\
\Rightarrow \|  \widehat{\beta}-\beta_0 \|  \leq &
\|(\widehat{\B}^*\widehat{\B})^{-1}- (\B^* \B)^{-1}\|_{op} \,
\|\widehat{\B}^*\|_{op}\,
\left(\|\widehat r\|_{\mu}+\|\widehat A\|_{op}\,\|\widehat
h\|_{\mathcal{H}}\right)
\\
& +
\|(\B^* \B)^{-1}\|_{op}\, \|\widehat{\B}^* - B^*\|_{op}\,
\left(\|\widehat r\|_{\mu}+\|\widehat A\|_{op}\,\|\widehat
h\|_{\mathcal{H}} \right)
\\
& +
\|(\B^* \B)^{-1}\|_{op}\, \| B^*\|_{op}\,
\left(\|\widehat r - r \|_{\mu} + \|\widehat A - A \|_{op}
\,\|\widehat h\|_{\mathcal{H}}  + \| A \|_{op}
\,\|\widehat h - h_0\|_{\mathcal{H}}\right)
\, .
\end{align*}
Lemma \ref{lm.bounds} ensures that
$\|(\widehat{\B}^*\widehat{\B})^{-1}- (\B^* \B)^{-1}\|_{op}$,
$\|\widehat{\B}^* - B^*\|_{op} = \|\widehat{\B} - B\|_{op}  $,
$\|\widehat r - r \|_{\mu}$, and $\|\widehat A - A \|_{op}$ all are
$O_P(n^{-1/2})$.
We have  $ \| B^*\|_{op} = \|\B\|_{op} <
\infty$ as $\B$ is a linear operator  with  finite dimensional
domain, see  \citet[Theorem
2.7-8]{kreyszig1978introductory}, and
$\|\B^* \B\|_{op} = \|\B\|_{op}^2$. Similarly, $\|(\B^* \B)^{-1}\|_{op}
<\infty$ as $\B$ is injective.
From Lemma \ref{lm.cvop}(i), $A$ is compact and hence
bounded, and from Lemma \ref{lm.cvop}(iv)
$\|\widehat{\A}\|_{op}=O_P(1)$.
From a similar reasoning, $\|\widehat{\B}^*\|_{op}=O_P(1)$.
Also $\|\widehat h - h_0\|_{\mathcal{H}}=o_P(1)$ implies
$\|\widehat h \|_{\mathcal{H}}=O_P(1)$.
Combine these results to obtain that
\[
\|  \widehat{\beta}-\beta_0 \|  = O_P\left( n^{-1/2} +
\|\widehat h - h_0\|_{\mathcal{H}}\right) = o_P(1)
\, .
\]

Since $\widehat{g}(z) = (1,z)\widehat{\beta} + \widehat{h}(z)$,
to show uniform consistency of $\widehat{g}$ and $\widehat{g}'$, it
now suffices to show that $\sup_{z\in [0,1]}|\widehat{h}(z)-h_0(z)|$
and $\sup_{z\in[0,1]}|\widehat h '(z)-h_0'(z)|$ are bounded by
$\|\widehat h - h_0\|_{\mathcal{H}}$.  As for
any $h\in\mathcal{H}$, $h'(z)=\int_{0}^z h''(t)dt$,
\begin{equation*}
\label{eq: uniform bound on the 1st derivative of h}
\sup_{z\in[0,1]}|\widehat h'(z)-h_0'(z)|\leq
 \sup_{z\in[0,1]}\int_{0}^z|\widehat{h}''(t)-h_0''(t)|dt\leq
 \int_{0}^1|\widehat{h}''(t)-h_0''(t)|dt\leq \|\widehat h -
 h_0\|_{\mathcal{H}}\, ,
\end{equation*}
from Cauchy-Schwartz inequality.
Since $h(z)=\int_{0}^z h'(t) d t$,  a similar reasoning yields
\begin{equation*}
\label{eq: uniform bound on h}
\sup_{z\in[0,1]}|\widehat h (z)- h_0(z)|\leq
\sup_{z\in[0,1]}\int_{0}^z|\widehat{h}'(t)-h_0'(t)|dt\leq
\sup_{z\in[0,1]}|\widehat h'(z)-h_0'(z)|\, .
\end{equation*}


\textbf{Step 5}.
We now obtain uniform convergence rates.  Assumption \ref{ass: source
condition} allows to apply Proposition 3.11 in
\cite{carrasco2007linear} to the operator $\T$ and yields
$\|h_\lambda-h_0\|_{\mathcal{H}}=O\left(\lambda^\frac{\gamma \wedge
2}{2}\right)$. Combining with the results of Step 3 gives
\begin{equation*}
\|\widehat h - h_0\|_{\mathcal{H}}=O_P\left(\frac{1}{\sqrt{n
\lambda}}+\lambda^{\frac{\gamma \wedge 2}{2}}\right)\text{ and
}\|\widehat \beta - \beta_0\|=O_P\left(\frac{1}{\sqrt{n
\lambda}}+\lambda^{\frac{\gamma \wedge 2}{2}}\right)\, .
\end{equation*}
We can now use the same arguments as in Step 4 to obtain
uniform convergence rates.


\subsection{Proof of Lemma  \protect{\ref{lm.cvop}}}

\eqref{cratei}. Let us show $\A$ is compact by  compact embedding.
Define  $\widetilde{\A}$ as the extension of $A$ to $L^2([0,1])$,
where $L^2([0,1])$ is the space of real-valued square-integrable
functions on $[0,1]$,  i.e. $\widetilde{\A} h =  \E\{h(Z)e^{\mathbf{i}W^\top \cdot}\}$ for
any $h \in L^2([0,1])$.
For all $h\in\mathcal{H}$, we have $h(z)=\int_0^z\int_0^x h''(t)dt
dx$, so that
\begin{align*}
||h||_{L^2[0,1]}^2 =\int_0^1|h(z)|^2 dz & \leq
\sup_{z\in[0,1]}| h (z)|^2 \leq
\sup_{z\in[0,1]} \left|\int_0^z h'(t)dt \right|^2
\leq
\sup_{t\in[0,1]}  |h'(t)|^2
\\ &
\leq
\sup_{t\in[0,1]} \left|\int_0^t h''(u)du \right|^2
\leq
\left(\int_0^1 |h''(u)|du\right)^2
\leq \| h\|_{\mathcal{H}}^2
\, ,
\end{align*}
where we used the inequality of Cauchy-Schwartz to obtain the last inequality.
Therefore, every bounded set on
$(\mathcal{H},||\cdot||_{\mathcal{H}})$ is also a bounded set on
$(L^2([0,1]),||\cdot||)$. Hence, compactness of  $\widetilde{\A}$
implies compactness of $\A$. Now for any $h\in L^2[0,1]$,
\begin{equation*}
  (\widetilde{A}h)(t)=\E\{h(Z)\,\E\{\exp(\textbf{i}W^T t)|Z\}\}=\int
  h(z)\, \E\{\exp(\textbf{i}W^T t)|Z=z\}\, f_Z(z)\, dz\, ,
\end{equation*}
where
\begin{equation*}
    \int_{ }\left|\mathbb{E}\{\exp(\textbf{i}W^T t)|Z=z\}\right|^2 \mu(t) f_Z(z)\, dt\, dz\leq 1\, ,
\end{equation*}
as $|\exp(\textbf{i}\cdot)|\leq 1$.
Since $\widetilde{A}$ is an  integral operator whose kernel is
Hilbert-Schmidt, i.e. squared integrable, we can   apply
\citet[Proposition 2.1]{Busby1972}
to conclude that $\widetilde{A}$ is compact.

Let us now show that $\T$ is compact. The range of $\B$,
$\mathcal{R}(\B)$, is finite dimensional, linear, and closed.  $\P$ is
the orthogonal projection onto $\mathcal{R}(\B)$, and is thus bounded
by \citet[Theorem 13.3]{kress1999linear}.  Hence, $\M=I-\P$ is bounded
as well.  Since $\T= \M \A$ is the composition of a bounded and a
compact operator, it is  compact by \citet[Theorems 2.14 and
2.16]{kress1999linear}.


\eqref{crateii}. For $\beta\in \R^2$, we have
\begin{align*}
\|(\widehat{\B}-\B)\beta\|_{\mu}^2&=\int
|(\E_n-\E)\{\exp(\mathbf{i}W^\top t)(1,Z)\}\beta|^2\mu(dt)
\, ,
\end{align*}
where $\E_n$ denotes the empirical expectation. By the  Cauchy-Schwarz inequality,
\begin{align*}
\E\|\widehat{\B}-\B\|_{op}^2
\le & \E\left\{\int (|(\E_n-\E)\{\exp(\mathbf{i}W^\top
t)\}|^2+|(\E_n-\E)\{Z\exp(\mathbf{i}W^\top t)\}|^2)\mu(dt)\right\}.
\end{align*}

Since  data are i.i.d.,
\begin{align*}
E\left\{ |(\E_n-\E)\{Z\exp(\mathbf{i}W^\top t)\}|^2\right\}
& = \E\left\{ \left|n^{-1}\sum_{i=1}^n Z_i\exp(\mathbf{i}W_i^\top t) -
\E\{Z\exp(\mathbf{i}W^\top t)\}\right|^2\right\}
\\
& =
\operatorname{Var}\left\{\frac{1}{n}\sum_{i=1}^n Z_i
\exp(\textbf{i}W^T_i t)\right\}
\\
& = n^{-1}(\E\{|Z\exp(\mathbf{i}W^\top
t)|^2\}-|\E\{Z\exp(\mathbf{i}W^\top t)\}|^2)
= O(n^{-1}) \, ,
\end{align*}
as $|Z\exp(\mathbf{i}W^\top t)|\leq 1$ for all
$t\in\mathcal{T}$. Similarly, $\E\, \{|(\E_n-\E)\{\exp(\mathbf{i}W^\top
t) \}|^2\} = O(n^{-1})$.
This implies $\E\|\widehat{B}-B\|_{op}^2  = O(n^{-1})$, and
by Markov's inequality,  $\|\widehat{\B}-\B\|_{op}=O_P(n^{-1}). $


\eqref{crateiii}.
From \citet[Theorem 2.7-8]{kreyszig1978introductory}, as $\B$ is a
linear operator with a finite dimensional domain, it is  bounded, and
$\|\B\|_{op} < \infty$. Also $\|\B^{*}  \|_{op} =
\| \B \|_{op}$ and $\|\B^{*} \B \|_{op} = \| \B \|_{op}^2$.  The operator $\B^*\B$ maps $\R^2$ into $\R^2$, and is thus a matrix.
From  \eqref{crateii}, $\|\widehat{\B}\|_{op}$ and
 $\|\widehat{\B}^{*}\|_{op}$ are $O_P(1)$, and
\begin{align*}
\|\B^*\B-\widehat{\B}^*\widehat{\B}\|_{op}
&=
\| (\B^*-\widehat{\B}^*) B + \widehat{\B}^{*} (\B - \widehat{\B}) \|_{op}
 \\
& \leq
\|\B^*-\widehat{\B}^*\|_{op}\, \|\B\|_{op} + \|\widehat{\B}^*\|_{op}\,
\|\B-\widehat{\B}\|_{op} = O_P(n^{-1/2})
\, .
\end{align*}
 Since $\B$ is injective, $\B^*\B$ is invertible, $(\B^*\B)^{-1}$
 exists and is bounded. By the continuous mapping theorem,
 $\|(\widehat{\B}^*\widehat{\B})^{-1}-(\B^*\B)^{-1}\|_{op} = o_P(1)$.
Hence $\|(\widehat{\B}^*\widehat{\B})^{-1}\|_{op}\leq
\|(\widehat{\B}^*\widehat{\B})^{-1}-(\B^*\B)^{-1}\|_{op}+\|(\B^*\B)^{-1}\|_{op}=O_P(1)$.
Moreover,
\begin{align*}
    \|(\widehat{\B}^*\widehat{\B})^{-1} - (\B^*\B)^{-1} \|_{op} = &
    \|(\widehat{\B}^*\widehat{\B})^{-1}
    (\B^*\B-\widehat{\B}^*\widehat{\B}) (\B^*\B)^{-1} \|_{op}
    \\
    \leq &
    \| (\widehat{\B}^*\widehat{\B})^{-1}\|_{op} \,
    \|\B^*\B-\widehat{\B}^*\widehat{\B}\|_{op}\,
    \|(\B^*\B)^{-1}\|_{op} = O_P(n^{-1/2})
    \,.
\end{align*}

For  the difference between  $\M=I-\B(\B^*\B)^{-1}\B^*$ and
$\widehat{\M}=I-\widehat{\B}(\widehat{\B}^*\widehat{\B})^{-1}\widehat{\B}^*$,
we have
\begin{align*}
    \|\widehat{\M} - \M\|_{op}=&\|
    (\widehat{\B}-\B)(\widehat{\B^*}\widehat{\B})^{-1}\widehat{\B}^* +
    \B [(\widehat{\B}^* \widehat{\B})^{-1}- (\B^*\B)^{-1}]
    \widehat{\B}^* + \B (\B ^* \B)^{-1}[\widehat{\B}^* -\B^*]\|_{op}
    \\
    \leq & \|\widehat{\B}-\B\|_{op}\,
    \|(\widehat{\B}^*\widehat{\B})^{-1}\|_{op}
    \, \|\widehat{\B}^*\|_{op} + \|\B\|_{op} \,
    \|(\widehat{\B}^*\widehat{\B})^{-1}-(\B^* \B)^{-1}\| \,
    \|\widehat{\B}^*\|_{op} \\
   & + \|\B\|_{op} \, \|(\B^*\B)^{-1}\|_{op} \,
    \|\widehat{\B}^*-\B^*\|_{op} = O_P(n^{-1/2})
    \, .
\end{align*}


\eqref{crateiv}.
Recall that for any $h\in\mathcal{H}$, $h(z)=\int_{0}^z \int_{0}^x
h''(u) du dx$. Thus,
\begin{align}
\label{eq: expression of Ahat as a kernel operator}
    (\widehat{A}h)(t)=& \frac{1}{n}\sum_{i=1}^n h(Z_i)
    \exp(\textbf{i}W_i^T t)=\frac{1}{n}\sum_{i=1}^n
    \int_{0}^{Z_i}\int_{0}^x h''(u) \,du\, dx\, \exp(\textbf{i}W_i^T
    t)\nonumber \\
    =& \int_{[0,1]^2}h''(u)\left[\frac{1}{n}\sum_{i=1}^n
    \1(0<u<x)\,\1(0<x<Z_i)\,\exp(\textbf{i}W_i^T t)\right]\,du\,dx
    \nonumber \\
    =& \int_{0}^1 h''(u)\left[\int_{0}^1 \frac{1}{n}\sum_{i=1}^n
    \1(u<x<Z_i)\,\exp(\textbf{i}W_i^T t)\, dx\right]\, du \nonumber \\
    =& \int_{0}^1 h''(u)\, \widehat k(u,t)\, du\, ,
\end{align}
where $\widehat{k}(u,t)$ is defined implicitly above.
Exchanging the empirical measure with the population probability
and using the same steps as above yield
\begin{align}\label{eq: expression of A as a kernel operator}
    (A h)(t)=\int_{0}^1 h''(u)\left[\int_{0}^1
    \E\{\1(u<x<Z)\,\exp(\textbf{i}W^T t)\}\, dx\right]=\int_{0}^1
    h''(u)\, k(u,t)\, du\, .
\end{align}
where $k(u,t) = \E \widehat{k}(u,t)$ is defined implicitly above.
Next,
\begin{align*}
    \|\widehat A -
    A\|_{op}^2=&\sup_{h\in\mathcal{H}\,\|h\|_{\mathcal{H}}=1}\|\widehat
    A h - A h \|^2_\mu =\sup_{h\in\mathcal{H}\,\|h\|_{\mathcal{H}}=1}
    \int_{ }\left|(\widehat{A}h)(t)-(A h)(t)\right|^2\mu(dt)\\
    =& \sup_{h\in\mathcal{H}\,\|h\|_{\mathcal{H}}=1} \int_{
    }\left|h''(u)\,\left[\widehat{k}(u,t)-k(u,t)\right]\,du\,\right|^2
    \mu(dt)\\
    \leq & \sup_{h\in\mathcal{H}\,\|h\|_{\mathcal{H}}=1} \int_{
    }\left\{\int_{0}^1
    |h''(u)|^2du\,\int_{0}^1\left|\widehat{k}(u,t)-k(u,t)\right|^2\,du\right\}
    \mu(dt)\\
    =
    &\sup_{h\in\mathcal{H}\,\|h\|_{\mathcal{H}}=1}\|h\|_{\mathcal{H}}
    \int_{
    }\left\{\int_{0}^1\left|\widehat{k}(u,t)-k(u,t)\right|^2\,du\right\}
    \mu(dt)\\
    =& \int_{ [0,1]\times \mathbb{R}^q
    }\left|\widehat{k}(u,t)-k(u,t)\right|^2\,du\otimes \mu(dt)
    \\
\Rightarrow
    \E \|\widehat A - A\|^2_{op}
    \leq  & \int_{ [0,1]\times \mathbb{R}^q } \E
    \left|\widehat{k}(u,t)-k(u,t)\right|^2\,du\otimes \mu(dt)\, .
\end{align*}
Now,
\begin{align*}
     \E\left|\widehat{k}(u,t)-k(u,t)\right|^2
     = &\E\left|  \int_{0}^1(\E_n-\E)\1(u<x<Z)\exp(\textbf{i}W^T t)\,
     dx\right|^2
%     \nonumber \\
\\  \leq &
 %\E \int_{0}^1 \left|(\E_n-\E)\1(u<x<Z)\exp(\textbf{i}W^T t)\right|^2\, dx\nonumber \\
% =&
 \int_{0}^1 \E \left|(\E_n-\E)\1(u<x<Z)\exp(\textbf{i}W^T t)\right|^2
 \, dx
 \nonumber \\
 =&
 \int_{0}^1 \operatorname{Var}\left\{\frac{1}{n}\sum_{i=1}^n
 \1(u<x<Z_i)\exp(\textbf{i}W_i^T t)\right\}\, dx
 \nonumber\\
 =& \frac{1}{n} \int_{0}^1  \operatorname{Var}
 \left\{\1(u<x<Z)\exp(\textbf{i}W^T t)\right\}\, dx  = O(n^{-1})
\, .
\end{align*}
Use Markov's inequality to obtain the desired result.

\eqref{cratev}.
By reasoning as in the proof of \eqref{cratei}, compactness of
$\widehat \T =\widehat{\M} \widehat{\A}$  follows if
 $\widehat{\M}$ is bounded and $\widehat \A$ is compact.
The first claim is shown following similar arguments as in
\eqref{cratei}.
 To obtain compactness of $\widehat \A$, we will use Theorem 8.1-4 in
 \cite{kreyszig1978introductory} stating that a bounded operator with a finite dimensional range is
compact.
As
\begin{align*}
 \widehat{A}h=\sum_{i=1}^n h(Z_i)\,\frac{1}{n}\exp(W_i^T \cdot)\, \in
 \text{Span}\left(\frac{1}{n}\exp(W_1^T
 \cdot),\ldots,\frac{1}{n}\exp(W_n^T \cdot)\right)
 \, ,
\end{align*}
the range of $\widehat \A$ is finite dimensional for all $n$.
Moreover, using (\ref{eq: expression of Ahat as a kernel operator})
\begin{align*}
\|\widehat{\A}h\|^2_\mu = &
\int \left| \int_{0}^1 h''(u)\, \widehat k(u,t)\, du \right|^2 \mu(dt)
\leq   \|h\|_\mathcal{H}^2 \sup_{u,t} | \widehat k(u,t)|^{2}
\, du
 \leq \|h\|_\mathcal{H}^2
\, ,
\end{align*}
as $| \widehat k(u,t)| \leq 1$. Hence, $\|\widehat{\A}\|_{op} \leq 1$,
and $\widehat \A$ is compact.


\eqref{cratevi}.
Since
$\widehat{\T}-\T = (\widehat{\M}-\M) \widehat \A + \M (\widehat{\A} - \A)$,
 the result follows from \eqref{crateii}, \eqref{crateiv}, and the
 fact that $\M$ is bounded.

\eqref{cratevii}. The proof  is analogous to the proof of
 \eqref{crateii}.

\eqref{crateviii}.
Write $\widehat{\M}\widehat r - \M
r=(\widehat{\M}-\M)\widehat{r}+\M(\widehat{r}-r)$, and use
$\|\widehat{\M}-\M\|_{op}=O_P(n^{-1/2})$,
 $\|\widehat{r}-r\|_\mu=O_P(n^{-1/2})$, and
$\|\M\|_{op}<\infty$ from previous items to obtain
 $\|\widehat{\M} \widehat r - \M r\|_\mu=O_P(n^{-1/2})$.
Use \eqref{cratevi} above to get $\|(\widehat{\M} \widehat r- \widehat
T h_0) - (\M r-\T h_0)\|_\mu=O_P(n^{-1/2})$, and note that $\M_Z r-\T
h_0=0$.

\end{document}



