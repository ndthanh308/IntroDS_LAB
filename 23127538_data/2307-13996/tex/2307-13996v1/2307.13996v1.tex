%%
%% Copyright 2007-2020 Elsevier Ltd
%%
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%%
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%%
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%%

%% Template article for Elsevier's document class `elsarticle'
%% with numbered style bibliographic references
%% SP 2008/03/01
%%
%%
%%
%% $Id: elsarticle-template-num.tex 190 2020-11-23 11:12:32Z rishi $
%%
%%
\documentclass[preprint,12pt]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[authoryear,preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
%% \usepackage{amssymb}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}


\usepackage{amsthm}
\usepackage{times}
\usepackage{bm}
\usepackage{helvet}
\usepackage{courier}
\usepackage{url}
\usepackage{amsfonts}
\usepackage{amsmath,amssymb}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{color}
\usepackage{xcolor}


\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newtheorem{theorem}{Theorem}
%\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{lemma}{Lemma}
\newdefinition{claim}{Claim}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newproof{prf}{Proof}




%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
%% \usepackage{lineno}


\newcommand{\e}{\epsilon}
\newcommand{\bo}{\boldsymbol{o}}
\newcommand{\bp}{\boldsymbol{p}}
\newcommand{\bq}{\boldsymbol{q}}
\newcommand{\bx}{\boldsymbol{x}}
\newcommand{\by}{\boldsymbol{y}}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\journal{Computers \& Electrical Engineering}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for theassociated footnote;
%% use the fnref command within \author or \affiliation for footnotes;
%% use the fntext command for theassociated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for theassociated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \affiliation{organization={},
%%            addressline={},
%%            city={},
%%            postcode={},
%%            state={},
%%            country={}}
%% \fntext[label3]{}

\title{Fast algorithms for $k$-submodular maximization subject to a matroid constraint}
%\tnoteref{t1}}
%\tnotetext[t1]{Supported by Natural Science Foundation of Shandong Province of China (Nos. ZR2020MA029, ZR2021MA100) and National Science Foundation of China (No. 12001335).}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{}
%% \affiliation[label1]{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}
%%
%% \affiliation[label2]{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}

%
\author[1]{ShuXian Niu}
\ead{niushuxianBetty@163.com}

\author[1]{Qian Liu}
\ead{lq\_qsh@163.com}

\author[1]{Yang Zhou}
\ead{zhouyang@sdnu.edu.cn}

\author[1]{Min Li\corref{cor1}}
\ead{liminemily@sdnu.edu.cn}

\cortext[cor1]{Corresponding author: liminemily@sdnu.edu.cn}
%\fntext[fn1]{This is the first author footnote.}
%\fntext[fn2]{Another author footnote, this is a very long
%   footnote and it should be a really long footnote. But this
%   footnote is not yet sufficiently long enough to make two
%   lines of footnote text.}
%\fntext[fn3]{Yet another author footnote.}

 \address[1]{School of Mathematics and Statistics, Shandong Normal University, 250014, Jinan, China}
%
% \address[1]{organization={School of Mathematics and Statistics},
%                 addressline={Shandong Normal University},
%                 postcode={250014},
%                 city={Jinan},
%                 country={China}}



\begin{abstract}
%% Text of abstract

In this paper, we apply a Threshold-Decreasing Algorithm to maximize $k$-submodular functions under a matroid constraint, which reduces the query complexity of the algorithm compared to the greedy algorithm with little loss in approximation ratio.
We give a $(\frac{1}{2} - \e)$-approximation algorithm for monotone $k$-submodular function maximization, and a $(\frac{1}{3} - \e)$-approximation algorithm for non-monotone case, with complexity $O(\frac{n(k\cdot EO + IO)}{\epsilon} \log \frac{r}{\epsilon})$, 
where $r$ denotes the rank of the matroid, and
$IO, EO$ denote the number of oracles to evaluate whether a subset is an independent set and to compute the function value of $f$, respectively. Since the constraint of total size can be looked as a special matroid, called uniform matroid, then we present the fast algorithm for maximizing $k$-submodular functions subject to a total size constraint as corollaries.
%As an typical example of matroid constraint, the approximation and complexity of total size constraint is also given as corollaries.
\end{abstract}

%%Graphical abstract
%\begin{graphicalabstract}
%%% Figure removed
%\end{graphicalabstract}

%%Research highlights
%\begin{highlights}
%\item Research highlight 1
%\item Research highlight 2
%\end{highlights}

\begin{keyword}
%% keywords here, in the form: keyword \sep keyword
$k$-submodular \sep threshold-decreasing algorithm \sep matroid constraint \sep size constraint
%% PACS codes here, in the form: \PACS code \sep code

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)

\end{keyword}

\end{frontmatter}

%% \linenumbers

%% main text
\section{Introduction}

Given a finite set $E = \{ e_{1},e_{2},\ldots,e_{n} \}$, 
let $ 2^{E} $ represent the collection of all subsets of $E$.
Define a set function $ f:2^{E} \to \mathbb{R} $ as a \emph{submodular function} if the following condition is satisfied, 
\[
f(P) + f(Q) \ge f(P \cup Q) + f(P \cap Q),
\]
where $P,Q$ are arbitrary subsets of $E$.
This definition can also be extended to a  $k$-submodular function.

We assume that $k$ is a positive integer, and denote the set $\{ 1,2,\ldots,k \}$ by $[k]$. 
%{\color{blue}Here}
Let $ ( k+1 ) ^E: = \{ ( P_{1},\ldots,P_{k} ) | P_{i}  \subseteq E ( \forall i \in [k] ), P_{i} \cap P_{j} = \emptyset ( i \ne j ) \} $ denote a set of $k$-tuples consisting of $k$ disjoint subsets of $E$.
Similar to the definition of submodular function, we can give the definition of $k$-submodular function as follows:
A set function $ f:(k+1)^E \to \mathbb{R} $ is defined as a \emph{$k$-submodular function} if and only if for any $ \boldsymbol{p} = (P_{1},P_{2},\ldots,P_{k}),  \boldsymbol{q} = (Q_{1},Q_{2},\ldots,Q_{k}) \in (k+1)^{E} $, we have,
\[
f ( \boldsymbol{p} ) + f ( \boldsymbol{q} ) \ge f ( \boldsymbol{p} \sqcup \boldsymbol{q} ) + f ( \boldsymbol{p} \sqcap \boldsymbol{q} ),
\]
where

\begin{align*}
\boldsymbol{p} \sqcap \boldsymbol{q}:&=(P_{1} \cap Q_{1},\ldots,P_{k} \cap Q_{k}),\\
\boldsymbol{p} \sqcup \boldsymbol{q}:&=(P_{1} \cup Q_{1}\backslash(\bigcup_{i \ne 1}P_{i} \cup Q_{i}),\ldots,P_{k} \cup Q_{k}\backslash(\bigcup_{i \ne k}P_{i} \cup Q_{i})).
\end{align*}

Huber and Kolmogorov~\cite{huber2012towards} defined and studied the $k$-submodular polyhedron, then they generalized the Min-Max-Theorem for submodular functions to $k$-submodular functions, which was the first introduction of the concept of $k$-submodular functions.
Ward and \v{Z}ivn\'{y}~\cite{WardZivny2016Beyond} firstly provided the approximation guarantees for maximizing a $k$-submodular function without any constraints. 
When the objective function is monotone, %仰ѡԼǰ
they gave a deterministic $\frac{1}{3}$-approximation algorithm based on greedy technique. 
Then Iwata et al.~\cite{IwataTY15} presented a better result by proving the existence of a stochastic $\frac{k}{2k-1}$-approximation algorithm.
Based on this, Oshima~\cite{Oshima2018} gave a de-randomization scheme which yielded the same approximate ratio.  %ȥ
Wang and Zhou~\cite{Wang2021MultilinearEO} proposed a new technique for analyzing the problem of maximizing a $k$-submodular function, where a multilinear extension of the algorithm that was almost as good as the previous ones was presented. %with the approximation ratio of $(\frac{k}{2k-1} - \e)$ for any $\e>0$.
%ǵԼ
When the objective function $f$ is non-monotone, Ward and \v{Z}ivn\'{y}~\cite{WardZivny2016Beyond} gave a stochastic $\max\{\frac{1}{3},\frac{1}{1+\alpha}\}$-approximation algorithm, where $\alpha = \max\{1,\sqrt{\frac{k-1}{4}}\}$. When $k \ge 3$, Oshima~\cite{Oshima2018} obtained a better approximation of $\frac{k}{3k-2}$. % 
Iwata et al.~\cite{IwataTY15} improved these results  to a stochastic algorithm with an approximation of $\frac{1}{2}$.  % 
Later,
Oshima~\cite{HO2019ImprovedRandomized} proposed an improved randomized algorithm based on the ideas of Iwata et al.~\cite{IwataTY15}, that is,
for $k \ge 3$, they gave the algorithm with approximation of  $\frac{k^{2}+1}{2k^{2}+1}$, and for $k = 3$, they presented a stochastic $\frac{\sqrt{17}-3}{2}$-approximation algorithm.
%For $k = 3$, they used the same framework as Iwata et al.~\cite{IwataTY15},
%Ward and \v{Z}ivn\'{y}~\cite{WardZivny2016Beyond} (but with different probabilities) that gave a stochastic $\frac{\sqrt{17}-3}{2}$-approximation algorithm.

%µtotalԼͷֻԼ
%total sizeԼ
Ohsaka and Yoshida~\cite{Ohsaka2015MonotoneKF} gave approximation algorithms to maximize the monotone $k$-submodular function under two kinds of size constraints: One is total size constraint, the other is individual size constraint.%ֱһ㷨

A total size constraint means that the total number of selected elements in the set of $k$-tuple is limited by a positive integer size $B$, they gave a greedy algorithm with an approximation guarantee of $\frac{1}{2}$ with query complexity of $O(knB)$. %̰㷨
Combined with random sampling techniques, they also gave a randomized algorithm that can be guaranteed to output an approximation of $\frac{1}{2}$ with probability of at least $1-\delta$, whose query complexity is $O(kn\log B\log\frac{B}{\delta})$.   %
Nie et al.~\cite{Nie2023FAST_size} applied the threshold decreasing method combined with greedy ideas to this problem, improving the complexity to $O(\frac{nk}{\e} \log \frac{B}{\e})$ while obtaining a near-optimal approximation, achieving no linear dependence on the budget $B$. 
Nguyen and Thai~\cite{nguyen20f} also proposed streaming algorithms for this kind problems.



%individual sizeԼ
Now we introduce the individual size constraint that the number of chosen elements in each of the $k$ sets is limited by a size $B_{i}$, respectively.
For maximizing a monotone subjective function with individual size constraint, Ohsaka and Yoshida~\cite{Ohsaka2015MonotoneKF} gave an algorithm that can output an approximation of $\frac{1}{3}$ in running time $O(knB)$, where $B = \sum_{i=1}^{k}{B_{i}}$. 
%individual size
Nie et al.~\cite{Nie2023FAST_size} applied the threshold greedy algorithm to this problem, obtaining an approximation ratio of $\frac{1}{3}-\e$ with query complexity of $O(\frac{nk}{\e} \log \frac{B}{\e})$.
%individual sizeԼ
They also presented a stochastic $\frac{1}{3}$-approximation algorithm with probability of at least $1-\delta$ with query complexity of  $O(k^{2}n\log{B/k}\log{(B/\delta)})$. %running time.   %
A new streaming algorithm for this kind of problems was introduced in~\cite{EneAlina2022Streaming}. %㷨
%total sizeԼindividual sizeԼ
Zheng et al.~\cite{MinmingLi2021} also proved that a simple greedy algorithm to maximize an approximately $k$-submodular function $F$ and obtained reasonable approximation ratios for both kinds of size constraints.
%ǵindividual sizeԼ
When the objective function $f$ is non-monotone, Xiao et al.~\cite{Xiao2023} gave two kinds of algorithms, one was the $\frac{1}{B_{m}+4}$-approximation algorithm, where $B_{m}=\max\{B_{1},B_{2},\ldots,B_{k}\}$, the other was a bi-criteria algorithm with approximation ratio of $\frac{1}{4}$.
%For the problem of maximizing a non-negative and non-monotone $k$-submodular function under total size and individual size difference constraints,
Shi et al.~\cite{FASTshi2021k} proposed a $(1-\frac{1}{e}-\e)$-approximation algorithm with running time of $O(\frac{nk!}{\e}\log \frac{n}{\e})$.
%˫׼㷨


%Լ
Sakaue~\cite{SAKAUE2017105} solved approximately the problem of maximizing a monotone $k$-submodular subject to a  matroid constraint with the approximation ratio of $\frac{1}{2}$ and the query complexity of $O(rn(IO + k\cdot EO))$ based on a greedy technique, 
where $r$ and $IO$ denote the rank and the time of independence oracle of the given matroid, respectively, and $EO$ denotes the time of estimated oracle of the $k$-submodular function.
% ˽
Rafiey and Yoshida~\cite{Fast-and-Private2020} gave the first asymptotically tight $\frac{1}{2}$-approximation algorithm for this problem that preserves differential privacy, and its complexity $O(kn\ln r\ln \frac{r}{\gamma})$ is almost linear with the function evaluation, where $\gamma$ is the failure probability of this algorithm.
%  ̰
Wang et al.~\cite{Wang2022WeaklyKM} applied a greedy algorithm to maximize a weakly $k$-submodular function, where the constraint is also about a matroid. %  subject to matroid constraint. % and obtained an approximate guarantee of $\alpha/1+\alpha$, where the orthant submodularity ratio $0 < \alpha \le 1$.
%
For the problem to maximize a monotone $k$-submodular function under the intersection of a knapsack and $m$ matroid constraints, Yu et al.~\cite{Yu2023} proposed a nested and local search algorithm to obtain an approximation ratio of $\frac{1}{m+2}(1-e^{-(m+2)})$.
%ǵ
And for non-monotone subjective function, they obtained a $\frac{1}{m+3}(1-e^{-(m+3)})$-approximation algorithm.
%ǵ  ȷ
Sun et al.~\cite{sunyunjing2022} also gave a deterministic algorithm with an approximation factor of $1/3$ for the non-monotone case with the same query complexity as the one of monotone case~\cite{SAKAUE2017105}.
%ǵ 
In addition, based on the deterministic algorithm, they designed a randomized algorithm for this problem, which was able to obtain an approximation guarantee of $1/3$ and query complexity of $O(n(IO\log \frac{r}{\e_{1}}+k\cdot EO\log \frac{r}{\e_{2}})\log r)$ with a probability of $1-\e$, where $\e = \max\{\e_{1},\e_{2}\}$.
For more results about $k$-submodular functions, one can see~\cite{pmlr-v157-matsuoka21b,Pham2021StreamingAF,Qian8026144Multi-objective,TANG202286secretary,Yu2022}.

%Sakaue~\cite{SAKAUE2017105} for the first time solved the problem of maximizing a monotone $k$-submodular under a  matroid constraint with an approximation ratio of $\frac{1}{2}$ and query complexity of $O(rn(IO + kEO))$ based on a greedy technique,
%where $r$ and $IO$ denote the rank and the time of independence oracle of the given matroid, respectively, and $EO$ denotes the time of estimated oracle of the $k$-submodular function.
%ǵ
%For non-monotone case, Sun. et al.~\cite{sunyunjing2022} gave a deterministic algorithm with an approximation factor of $1/3$ and the same query complexity as the one of monotone case. %a complexity of $O(N|D|(IO+kEO))$. %ȷ㷨
%On the basis of this result, they also gave a randomized algorithm with faster running time.  % 

%Լ
%For the problem of maximizing a monotone $k$-submodular function under a  knapsack constraint,
%Tang. et al.~\cite{TANG202228} gave a deterministic  $(\frac{1}{2}-\frac{1}{2e})$-approximation algorithm.  Recently, Xiao et al.~\cite{xiao2023knap} improved this result to $\frac{1}{2}-\frac{1}{2e^2}$. Wang and Zhou~\cite{Wang2021MultilinearEO} also proposed a randomized $\frac{1}{2}$-approximation algorithm by using the multilinear extension.
%%ȷ㷨
%%ǵԤԼ
%Pham et al.~\cite{Pham2021StreamingAF} proposed a deterministic streaming algorithm and a stochastic streaming algorithm for the same cost and different cost cases, respectively.


%{\color{blue}It is well-known that the best known approximation ratio of maximizing a submodular function subject to a matroid constraint is $\frac{1}{2}$ with complexity of $O()$~\cite{}.}
%%https://epubs.siam.org/doi/10.1137/080733991
%A fast algorithm based on Threshold-Decreasing Algorithm was proposed in~\cite{FAST}.
%%The Fast algorithm addresses this problem by combining a greedy algorithm with threshold.
%%In this algorithm, we just need to take the element as long as its marginal gain exceeds the threshold we specify.

In this paper, we mainly study the fast algorithm for maximizing a $k$-submodular function under a matroid constraint as follows:
\begin{itemize}
\item We design a fast algorithm by using the Threshold-Decreasing method combined with greedy ideas to maximize $k$-submodular functions in both monotone and non-monotone cases under a matroid constraint.
We improve the complexity of $O(rn(IO +k\cdot EO))$ to $O(\frac{n(IO +k\cdot EO)}{\e} \log \frac{r}{\e})$ while obtaining an almost tight approximation guarantee, achieving no linear dependence on the rank $r$ of the given matroid, where $IO$ and $EO$ denote the number of oracles to evaluate whether a subset is an independent set and to compute the function value of $f$, respectively.
%Currently the best approximation ratio that can be obtained is $1/2$ via the greedy algorithm when $f$ is monotone,
%we improved this result and can achieve a reduction in query complexity without relying on $B$ while giving an almost tight approximation guarantee.
\item Based on our result about the matroid constraint, two corollaries about maximizing $k$-submodular functions subject to a total size constraint are obtained, since the constraint of total size constraint can be viewed as a uniform matroid.
\end{itemize}

We organize our paper as follows. We first introduce the useful notation and definitions for $k$-submodular functions and matroid in Section~\ref{Pre}. Then we present the fast algorithm and the proof of the approximation guarantee as well as query complexity about the problem of maximizing $k$-submodular functions under a matroid constraint in Section~\ref{sec_M}. Finally, we summarize our paper in Section \ref{DG}.
%We present our main results and the proof in section~\ref{sec_M}.
%In section~\ref{sec4}, we show the main result about the problem of maximizing $k$-submodular functions under individual size constraints.
%We summarize in section \ref{DG}.


\section{Preliminaries}\label{Pre}
In this section, we mainly introduce some notations and properties of $k$-submodular functions.
For any $ \boldsymbol{p} = (P_{1},P_{2},\dots,P_{k}) \in ( k+1 ) ^E $,
we define the \emph{support set} of $\boldsymbol{p}$ as a collection of all elements contained in $P_{i} (\forall i \in [k])$, that is, $supp( \boldsymbol{p}) = \bigcup_{i \in [k]} P_{i}$.
Besides, we use $|supp( \boldsymbol{p} )|$ to denote the size of $ \boldsymbol{p}$.
We also denote $\boldsymbol{0} = (\emptyset,\emptyset,\ldots,\emptyset)$ and assume that $f(\boldsymbol{0}) = 0$.
There is another vector representation of the domain of $k$-submodular function.
For the convenience of analysis, we will identify the sets of $k$-tuples with $n$-vectors $ \{ 0,1,\ldots, k \}^{E} $.
We define $ \boldsymbol{p} \in \{0,1,\ldots, k\}^{E} $ to be an $n$-dimensional vector as follows:
If the element $e$ belongs to some $i^{\textrm{th}}$ set $P_{i}$, we define $\boldsymbol{p}(e) = i$,
otherwise, i.e., $e\notin supp(\boldsymbol{p})$, then write $\boldsymbol{p}(e) = 0$. Given $ \boldsymbol{p} \in \{ 0,1,\ldots,k \}^{E} $, then associate $ (P_{1},P_{2},\ldots,P_{k}) \in (k+1)^{E} $ by:
\[
P_{i} = \{ e \in E | \boldsymbol{p} (e) = i \}, \quad \textrm{for} \,\, \textrm{any} \,\, i \in [k].
\]
We sometimes abuse the notations, and simply write $ \boldsymbol{p} = (P_{1},P_{2},\ldots,P_{k}) $ by regarding a vector $\boldsymbol{p}$ as $k$ disjoint subsets of $E$.
Moreover, we use $\textbf{1}_{e}$ to represent a unit vector whose coordinate is $1$ on $e$ and other coordinates are $0$.

Taking any $k$-tuples $ \boldsymbol{p} = (P_{1},P_{2},\ldots,P_{k})$ and $\boldsymbol{q} = (Q_{1},Q_{2},\ldots,Q_{k})$, denote by $ \boldsymbol{p} \preceq \boldsymbol{q} $ if $ P_{i} \subseteq Q_{i} $ for all $ i \in [k]$.
We say that a $k$-submodular function $f$ is \emph{monotone} if $ f ( \boldsymbol{p} ) \le f ( \boldsymbol{q} )$ for any $\boldsymbol{p} \preceq \boldsymbol{q}$.
Then the \emph{marginal gain} of adding element $e \notin supp(\bp)$ to the $i^{\textrm{th}}$ set $P_{i}$ of $\boldsymbol{p}$ is denoted as
\[
\Delta_{e,i} f ( \boldsymbol{p} ) = f ( P_{1},\ldots,P_{i} \cup \{e\},\ldots,P_{k} ) - f ( P_{1},\ldots,P_{i},\ldots,P_{k} ).
\]
Thus we can define the \emph{orthant submodularity} of a set function $f$ if it satisfies the following condition,
\[
\Delta_{e,i} f ( \boldsymbol{p} ) \ge \Delta_{e,i} f ( \boldsymbol{q} ), \forall \boldsymbol{p},\boldsymbol{q} \in (k+1)^{E} \, \textrm{with} \, \boldsymbol{p} \preceq \boldsymbol{q}, e \notin supp(\bq),\forall i \in [k].
\]
Similarly, we define the \emph{pairwise monotonicity} of $f$ if it satisfies,
\[
\Delta_{e,i} f ( \boldsymbol{p} ) + \Delta_{e,j} f ( \boldsymbol{p} ) \ge 0, \forall \boldsymbol{p} \in (k+1)^{E} \, \textrm{with} \, e \notin supp(\bp), \, \textrm{and} \, i \ne j \in [k].
\]

We present the known results for $k$-submodular functions in the following lemmas. 
\begin{theorem}\label{thm1}
(\cite{WardZivny2016Beyond}).
A real value set function $f$ defined on $(k+1)^{E}$ is $k$-submodular if and only if $f$ not only satisfies orthant submodularity but also satisfies pairwise monotonicity.
\end{theorem}

\begin{lemma}\label{OS}
(\cite{TANG202228}). 
Assume that $f$ is a $k$-submodular function, then for any $k$-tuples $\bp,\bq$ with $\bp \preceq \bq$, we have
\[
f(\bq) - f(\bp) \le \sum_{e \in supp(\bq) \setminus supp(\bp)} \Delta_{e,\bq(e)} f(\bp)
\]
\end{lemma}

In the next part, we will introduce some definition and known results about matroid:
\begin{definition}
(\cite{KV2011}). 
Given a finite ground set $E$, $\mathcal{F} \subseteq 2^{E}$, we call the pair $(E, \mathcal{F})$ a matroid if it satisfies the following conditions. 
%Eground set
\begin{enumerate}[(a)]
  \item\label{a} $\mathcal{F}$ contains the empty set;
  \item\label{b} Assume $B$ belongs to $\mathcal{F}$, then any subset of $B$ also belongs to $\mathcal{F}$.% $A,B$ such that $A\subseteq B$, then $A \in \mathcal{F}$;
  \item For any two sets $A, B$ in $\mathcal{F}$ with $|A| < |B|$, then there exists $e \in B \setminus A$ such that $A \cup \{e\}$  also belongs to $\mathcal{F}$.
\end{enumerate}
\end{definition}
Given a matroid $(E, \mathcal{F})$, % satisfying (\ref{a}) and (\ref{b}) is called an independent system, and
any element $B \in \mathcal{F}$ is called an independent set. Given an independent set $B \in \mathcal{F}$, if there is no bigger independent set containing $B$, then we say $B$ is a \emph{basis} of the matroid, and the cardinality of $B$ is called the \emph{rank} of the matroid, denoted by $r$. 
%We denote the set of all bases as $\mathcal{B}$.
%Each maximal independent set $B$ is also called a \emph{basis} of the matroid,
%and the number of elements in $B$ is called the \emph{rank} of the matroid, denoted by $r$.

There is a useful result about the independent sets for exchanging elements.
\begin{lemma}\label{lem3}
(\cite{SAKAUE2017105}). 
Given a matroid $(E, \mathcal{F})$.
Suppose that $ A $ is an independent set and $ B$ is a basis with $ A \subsetneq B $. 
Then, for any element $ e \notin A $ satisfying that $ A \cup \{ e \} $ is an independent set,
there exists a corresponding element $ e' \in B \setminus A $ such that $B \setminus \{ e' \} \cup \{ e \}$ is also a basis.
\end{lemma}

Given a matoid $(E, \mathcal{F})$, we study the problem of maximizing a $k$-submodular function subject to that the support of the $k$-tuple is an independent set.
That is to say, the question we study is:
\be\label{matroid-k}
\max_{\bp \in (k+1)^{E}} f(\bp)
\ee
\[
{\rm s.t.} \quad  supp(\bp) \in \mathcal{F}.
\]

Assuming that the optimal value of this problem is $OPT$,
%we take an arbitrary element of the following set as the maximal optimal solution:
thus the maximal optimal solution can also be defined as:
\[
\argmax_{\bp} \{|supp(\bp)|: f(\bp) = OPT, supp(\bp) \in \mathcal{F}\}.
\]

It can be shown that the size of the maximal optimal solution of this problem is the same as the rank of the matroid in the following lemmas.

\begin{lemma}\label{lem2} (\cite{SAKAUE2017105}). 
Given a matroid $(E, \mathcal{F})$ with rank $r$ and the $k$-submodular function $f$ is monotone, then the size of problem~(\ref{matroid-k}) is $r$.
%monotone $k$-submodular function $f: (k+1)^{E} \to \mathbb{R}$.
%$M$ is the rank of matroid $(E, \mathcal{F})$.
%Then the size of the maximal optimal solution to the problem of maximizing $f$ subject to matroid constraint is $r$.
\end{lemma}

\begin{lemma}\label{lem4}
(\cite{sunyunjing2022}). 
Given a matroid $(E, \mathcal{F})$ with rank $r$ and the $k$-submodular function $f$ is monotone, then the size of problem~(\ref{matroid-k}) is also $r$.
%Given a non-monotone $k$-submodular function $f: (k+1)^{E} \to \mathbb{R}$.
%$M$ is the rank of matroid $(E, \mathcal{F})$.
%Then the size of the maximal optimal solution to the problem of maximizing $f$ subject to matroid constraint is also $r$.
\end{lemma}

%иǣܲܺһ


\section{Maximizing a $k$-submodular Function under a Matroid Constraint}\label{sec_M}
In this section, we introduce our main result about the problem of maximizing $k$-submodular functions under a matroid constraint. We first present the fast algorithm, then give the analysis of this algorithm, at last we will show that the results about the problems with a total size constraint can be obtained as corollaries.

\subsection{Fast Algorithm for Maximizing a $k$-submodular Function under a Matroid Constraint}
For the problem of maximizing a $k$-submodular function under a matroid constraint, we introduce a fast algorithm by reducing the threshold value constantly in Algorithm~\ref{alg1}.
The final output solution $\bp$ can be obtained in the following way:
We first let $\bp$ be $\boldsymbol{0}$,
then judge each of the elements in the ground set $E$ in a randomized order as to whether they should be added to $supp(\bp)$ or not.
If the current $supp(\bp)$ can remain an independent set with its marginal gain above the current threshold after adding an element $e$, then we add this element to $supp(\bp)$, otherwise put it back into $E$.
If there are no points that meet the threshold to be selected, the threshold will be reduced by a factor of $ 1-\epsilon $.
By continuously lowering the threshold, all points that meet each threshold are selected until reaching a specific threshold $ \frac{\e}{2r}d $ ($d$ is the maximal single-point value of $f$) or selecting enough $r$ elements.
When the threshold is below $ \frac{\e}{2r}d $, no more points will be added even if not enough $r$ elements have been selected.
Next, we will discuss this problem in monotone and non-monotone cases. Moreover,
%During the analysis, we let $ \e = \e_{1} $ when $f$ is monotone and let $ \e = \e_{2} $ when $f$ is non-monotone, respectively.
Our algorithm will reduce the complexity of the algorithm while ensuring that the approximation ratio is about the same as the greedy algorithm, making it sub-linear dependent of $r$.

\begin{algorithm}[ht]

	\renewcommand{\algorithmicrequire}{\textbf{Input:}}
    \renewcommand{\algorithmicensure}{\textbf{Output:}}
	\caption{Decreasing-Threshold procedure under Matroid Constraint}
	\begin{algorithmic}[1]\label{alg1}

		\REQUIRE A $k$-submodular function $ f:(k+1)^E \to \mathbb{R} $, a matroid $(E, \mathcal{F})$ with rank $r$, $ \e \in (0,1) $.
        \ENSURE A solution $ \bp $ satisfied $supp (\bp) \in \mathcal{F} $.
        \STATE $ \bp \gets \boldsymbol{0} $.
        \STATE $ w \gets d = \max _{e \in E, i \in [k]} f(i\textbf{1}_{e}),  F(\bp) \gets E$
        \WHILE {$w > \frac{(1-\e)\e}{2r}d$ and $F(\bp) \ne \emptyset$}    \label{WHILE}          %\frac{1}{\e} \log \frac{M}{\e}
               \FOR {$e \in F(\bp)$}       \label{FOR}     %n
                    \STATE $ i \gets \argmax_{i \in [k]} \Delta_{e,i} f(\bp) $     \label{EO}      %kEO
                    \IF {$ \Delta_{e,i} f(\bp) \ge w $}
                        \STATE $ \bp ( e ) \gets i $
                        \STATE construct a collection of feasible elements $F(\bp)=\{e\in E| supp(\bp)\cup\{e\}\in \mathcal{F}\}$ using independent oracle  \label{IO}    %IO
                    \ENDIF
                \ENDFOR
                    \STATE $w=w(1-\e)$
         \ENDWHILE
        \RETURN $ \bp $
	\end{algorithmic}
\end{algorithm}


\subsection{Proof of the approximation guarantee as well as query complexity}

Let $\boldsymbol{o}$ be a maximal optimal solution and $\bp$ be the returned solution by Algorithm~\ref{alg1}. Assume that after continuous iteration to select points, the algorithm selects $t ( t \le r)$ elements in total, then we gradually establish the sequence  $\boldsymbol{p}^{0}, \boldsymbol{p}^{1}, \ldots ,\boldsymbol{p}^{t}$ in the following way.
Assume that $\bp^{0}=\textbf{0}$ and suppose that the $j$-th selected point and its position is
$( e_{j},i_{j} )$, then we define $\bp^{j}=\bp^{j-1}+i_j\textbf{1}_{e_j}, j\in [t]$. It is obvious to know that $\bp=\bp^t$.
%If not enough $r$ elements are selected, i.e., $ t < r$, then add $ r-t $ dummy elements with position $0$.



To analyze the relationship between the maximal optimal solution $ \boldsymbol{o} $ and the output solution $ \boldsymbol{p} $ of the algorithm, we let $\boldsymbol{o}^{0}=\boldsymbol{o}$ and construct the corresponding sequence of $\boldsymbol{o}^{0}, \boldsymbol{o}^{1}, \ldots ,\boldsymbol{o}^{t}$ based on the sequence of $ \boldsymbol{p}^{0}, \boldsymbol{p}^{1}, \ldots ,\boldsymbol{p}^{t} $.
By modifying only one element in $\bo$ at a time, we keep at least $M-1$ elements between two adjacent vectors of $\boldsymbol{o}^{0}, \boldsymbol{o}^{1}, \ldots ,\boldsymbol{o}^{t}$ identical (including elements and positions).
This ensures that the elements $e_{1}, \dots, e_{t}$ selected by the algorithm end up in the same position in $\bo^{t}$ as in $\bp$.
Finally, the relation between the output solution $\bp$ and the maximal optimal solution $\bo$ is established.
Through the following construction procedure we can ensure that $\boldsymbol{o}^{0}, \boldsymbol{o}^{1}, \ldots ,\boldsymbol{o}^{t}$ are all independent sets.

According to the previous assumption, $e_{j}$ is the $j$-th  element selected by the algorithm for each $j \in [t]$.
The process of constructing $\bo^{j}$ by $\bo^{j-1}$ is divided into two steps:
First, select an element in $supp(\bo^{j-1})$ and take it out, then put the element $e_{j}$ chosen by Algorithm~\ref{alg1} into the $i_{j}$-th set of $\bo^{j-1}$.
We mark the elements removed from $supp(\bo^{j-1})$ as $o_{j}$.
Obviously, the choice of $o_{j}$ depends on whether element $e_{j}$ is in the support set of the optimal solution $\bo$.
If $e_{j}\in supp(\bo^{j-1})$, take $o_{j} = e_{j}$.
Otherwise according to Lemma \ref{lem3}, there exists some $o$ in $supp(\bo) \setminus supp(\bp^{j-1})$ such that $supp(\bo) \setminus \{o\} \cup \{e_{j}\} \in \mathcal{F}$, then we denote this element $o$ as $o_j$.
For the sake of discussion, we define a new notation $\bo^{j-\frac{1}{2}}$ by removing $o_{j}$ from the support of $\bo^{j-1}$, i.e.,  \[
\bo^{j-\frac{1}{2}}: = \bo^{j-1} - \bo^{j-1}(o_{j}) \textbf{1}_{o_{j}}.
\]
Then $\bo^{j}$ can be constructed by adding $(e^j,i^j)$ to  $\boldsymbol{o}^{j-\frac{1}{2}}$, i.e.,
\[
\bo^{j}:= \bo^{j-\frac{1}{2}} + i_{j} \textbf{1}_{e_{j}}.
\]
It is not difficult to obtain $ \boldsymbol{p} = \boldsymbol{p}^{t} \preceq \boldsymbol{o}^{t} $
and $ \boldsymbol{p}^{j-1} \preceq \boldsymbol{o}^{j-\frac{1}{2}}$.  Moreover, if $t = r$, then $\bp^{r} = \bo^{r}$.

Suppose now that we have selected $ ( e_{j-1},i_{j-1} ) $ and the next element to be chosen is $ ( e_{j},i_{j} ) $.
Assume that the current threshold is $ w $, so we can easily see that,
\begin{align} \label{eq1}
\Delta_{e_{j},i_{j}} f ( \boldsymbol{p}^{j-1} ) \ge w.
\end{align}

Assume that the position of the element $ o_{j} $ in the maximal optimal solution $ \boldsymbol{o} $ is $ i^{*} $,
i.e.,
$ \boldsymbol{o}^{j-1}(o_{j}) = i^{*} $, and the last element chosen under the previous threshold $\frac{w}{1-\e}$ is $ e_{j'} $.
The reason that $ {o_{j}} $ was not selected under that threshold is that the gain added $ {o_{j}} $ to $ \bp^{j'} $ does not reach $\frac{w}{1-\e}$.
At the same time, $ \boldsymbol{p}^{j'} \preceq \boldsymbol{p}^{j-1} $,
so we have,
\begin{equation} \label{eq2}
\Delta_{o_{j},i^*} f ( \boldsymbol{p}^{j-1} ) \le \Delta_{o_{j},i^*} f ( \boldsymbol{p}^{j'} ) < \frac{w}{1-\epsilon}.
\end{equation}

\begin{claim} \label{cla3}
If the objective function $f$ is monotone, we have
\[
f( \boldsymbol{o}^{j-1} ) - f( \boldsymbol{o}^{j} ) < \frac{1}{1-\e} \big( f ( \boldsymbol{p}^{j} ) - f ( \boldsymbol{p}^{j-1} ) \big),\forall j \in [t].
\]
\end{claim}

\begin{prf}
Referring to the construction process of the sequence $ \boldsymbol{o}^{0}, \boldsymbol{o}^{1}, \ldots ,\boldsymbol{o}^{t} $ in Algorithm \ref{alg1}, we can see that for each $j \in [t] $, there is $ \boldsymbol{o}^{j-\frac{1}{2}} \preceq \boldsymbol{o}^{j} $, and by the monotonicity, we have,
\begin{equation} \label{eq3}
f( \boldsymbol{o}^{j-\frac{1}{2}} ) - f ( \boldsymbol{o}^{j} ) \le 0.
\end{equation}
Combining (\ref{eq1}) to (\ref{eq3}) we have,
\begin{align*}
f ( \boldsymbol{o}^{j-1} ) - f ( \boldsymbol{o}^{j} )
&= f ( \boldsymbol{o}^{j-1} ) - f ( \boldsymbol{o}^{j-\frac{1}{2}} ) + f ( \boldsymbol{o}^{j-\frac{1}{2}} ) - f ( \boldsymbol{o}^{j} )   \\
&\le f ( \boldsymbol{o}^{j-1} ) - f ( \boldsymbol{o}^{j-\frac{1}{2}} )  &(\textrm{monotonicity}) \\
&= \Delta_{o_{j},i^*} f ( \boldsymbol{o}^{j-\frac{1}{2}} )    \\
&\le \Delta_{o_{j},i^*} f ( \boldsymbol{p}^{j-1} )     &(\textrm{submodularity})   \\
&< \frac{w}{1-\e} &(\textrm{due\, to\,(\ref{eq2})}) \notag  \\
&\le \frac{1}{1-\e} \Delta_{e_{j},i_{j}} f ( \boldsymbol{p}^{j-1} ) &(\textrm{due\,to\,(\ref{eq1})})   \\
&= \frac{1}{1-\e} ( f ( \boldsymbol{p}^{j} ) - f ( \boldsymbol{p}^{j-1} ) ).
\end{align*}
\end{prf}

\begin{claim} \label{cla1}
If the objective function $f$ is non-monotone,, we have
\[
f(\boldsymbol{o}^{j-1} ) - f( \boldsymbol{o}^{j} ) < \frac{2-\e}{1-\e}(f(\boldsymbol{p}^{j}) - f(\boldsymbol{p}^{j-1})), \forall j \in [t].
\]
\end{claim}


\begin{prf}
The idea of the proof of this claim is roughly the same as that of Claim \ref{cla3}.
The difference is that we resort to the element and position pair $ ( e_{j},i' ) $ to complete the transition, where $ i' $ denotes any position that differs from $ i_{j} $.
Since $ \boldsymbol{o}^{j} = \boldsymbol{o}^{j-\frac{1}{2}} + i_{j} \textbf{1}_{e_{j}} $, due to the pairwise monotonicity, there is:
\begin{equation} \label{eq4}
\Delta_{e_{j},i'} f ( \boldsymbol{o}^{j-\frac{1}{2}} ) + \Delta_{e_{j},i_{j}} f ( \boldsymbol{o}^{j-\frac{1}{2}} ) \ge 0.
\end{equation}
Since the position of element $ e_{j} $ is greedily selected, therefore,
\begin{equation} \label{eq5}
\Delta_{e_{j},i'} f ( \boldsymbol{p}^{j-1} ) \le  \Delta_{e_{j},i_{j}} f ( \boldsymbol{p}^{j-1} ).
\end{equation}
Thus combining (\ref{eq1}), (\ref{eq2}), (\ref{eq4}) and (\ref{eq5}) we have:
\begin{align*}
&f ( \boldsymbol{o}^{j-1} ) - f ( \boldsymbol{o}^{j} )\\
&= f(\bo^{j-\frac{1}{2}} + i^* \textbf{1}_{o_{j}})  - f(\bo^{j-\frac{1}{2}} + i_{j} \textbf{1}_{e_{j}})    \\
&= f(\bo^{j-\frac{1}{2}} + i^* \textbf{1}_{o_{j}}) - f(\bo^{j-\frac{1}{2}}) + f(\bo^{j-\frac{1}{2}} + i' \textbf{1}_{e_{j}}) - f(\bo^{j-\frac{1}{2}}) \\
&\quad- f(\boldsymbol{o}^{j-\frac{1}{2}} + i' \textbf{1}_{e_{j}}) + f(\boldsymbol{o}^{j-\frac{1}{2}}) - f(\bo^{j-\frac{1}{2}} + i_{j} \textbf{1}_{e_{j}}) + f(\bo^{j-\frac{1}{2}})   \\
&= \Delta_{o_{j},i^*} f( \boldsymbol{o}^{j-\frac{1}{2}} ) + \Delta_{e_{j},i'} f( \boldsymbol{o}^{j-\frac{1}{2}} ) - \Delta_{e_{j},i'} f( \boldsymbol{o}^{j-\frac{1}{2}} ) - \Delta_{e_{j},i_{j}} f( \boldsymbol{o}^{j-\frac{1}{2}} ) \\
&\le \Delta_{o_{j},i^*} f( \boldsymbol{o}^{j-\frac{1}{2}} ) + \Delta_{e_{j},i'} f( \boldsymbol{o}^{j-\frac{1}{2}} ) \quad\quad (\textrm{due to\,(\ref{eq4})})  \\
&\le \Delta_{o_{j},i^*} f( \boldsymbol{p}^{j-1} ) + \Delta_{e_{j},i'} f( \boldsymbol{p}^{j-1} )  \quad\quad (\textrm{submodularity})\\
&< \frac{w}{1-\e} + \Delta_{e_{j},i'} f( \boldsymbol{p}^{j-1} ) \quad\quad \quad\quad \quad\quad (\textrm{due\,to\,(\ref{eq2})}) \\
&\le \frac{1}{1-\e} \Delta_{e_{j},i_{j}} f( \boldsymbol{p}^{j-1} ) + \Delta_{e_{j},i_{j}} f( \boldsymbol{p}^{j-1} )  \quad\quad (\textrm{due \,to\,(\ref{eq1})\,and\,(\ref{eq5})})  \\
%&= \frac{2-\e_{2}}{1-\e_{2}} \Delta_{e_{j},i_{j}} f( \boldsymbol{p}^{j-1} ) \\
&= \frac{2-\e}{1-\e} ( f ( \boldsymbol{p}^{j} ) - f ( \boldsymbol{p}^{j-1} ) ).
\end{align*}
\end{prf}

\begin{theorem} \label{thm2}
For the monotone $k$-submodular  maximization problem under total size constraint, Algorithm \ref{alg1} yields a $(\frac{1}{2}-\e)$-approximation solution.
And for the non-monotone case, the approximation of the returned solution from Algorithm \ref{alg1} is $ \frac{1}{3}-\e$.
In both cases, the query complexity of $f$ and independent sets is  $O(\frac{n(k\cdot EO + IO)}{\epsilon} \log \frac{r}{\epsilon})$.
\end{theorem}

\begin{prf}
Here, we only prove the monotone case, and similar analysis method can be applied to the non-monotone case.

From Claim \ref{cla3}, $ \forall j \in [t] $,
\[
f ( \boldsymbol{o}^{j-1} ) - f( \boldsymbol{o}^{j} )
<
\frac{1}{1-\e} ( f ( \boldsymbol{p}^{j} ) - f ( \boldsymbol{p}^{j-1} ) ).
\]
Thus,
\begin{align*}
\sum_{j=1}^{t} ( f ( \boldsymbol{o}^{j-1} ) -f ( \boldsymbol{o}^{j} ) )
<
\sum_{j=1}^{t} \frac{1}{1-\e} ( f ( \boldsymbol{p}^{j} ) - f ( \boldsymbol{p}^{j-1} ) ),
\end{align*}
which implies,
\begin{align}\label{eq6}
f ( \boldsymbol{o} ) - f ( \boldsymbol{o}^{t} )
&< \frac{1}{1-\e} ( f ( \boldsymbol{p}^{t} ) - f ( \boldsymbol{p}^{0} ) )   \notag \\
&= \frac{1}{1-\e} ( f ( \boldsymbol{p}^{t} ) - f ( \boldsymbol{0} ) )
= \frac{1}{1-\e} f ( \boldsymbol{p}^{t} ).
\end{align}
According to the termination condition of the algorithm \ref{alg1}, we will analyze the approximation ratio in following two cases.

\textbf{Case 1}: If $ t < r $.

In this case, the algorithm stops when only $t$ elements are selected.
At this time, the marginal gain of any element in $ E \setminus supp(\boldsymbol{p}^{t})$ added to $\boldsymbol{p}^{t}$ is less than $\frac{\e d}{2r}$.
Take any $ e \in supp(\bo) \setminus supp(\bp^{t})$,\\
\begin{align}\label{eq10}
\Delta_{e,\boldsymbol{o}(e)}f(\boldsymbol{p}^{t}) \le \frac{\e d}{2r}.
\end{align}
By (\ref{eq6}), Lemma \ref{OS} and (\ref{eq10}) we have,
\begin{align*}
f ( \boldsymbol{o} ) &< f ( \boldsymbol{o}^{t} ) + \frac{1}{1-\e} f ( \boldsymbol{p}^{t} )  &(\textrm{due\,to\,(\ref{eq6})})  \\
&= f ( \bo^{t} ) - f ( \bp^{t} ) + \frac{2-\e}{1-\e} f ( \bp^{t} )     \\
&\le \sum_{ e \in supp (\bo^{t}) \setminus supp (\bp^{t}) } \Delta_{ e,\bo(e) } f( \bp^{t} )  + \frac{2-\e}{1-\e} f ( \bp^{t} ) &(\textrm{due\,to\,Lemma\,\ref{OS}})\\
&\le M \frac{\e d}{2M} + \frac{2-\e}{1-\e} f ( \boldsymbol{p}^{t} ) &(\textrm{due\,to\,(\ref{eq10})}) \\
&\le \frac{\e}{2} f ( \boldsymbol{o} ) + \frac{2-\e}{1-\e} f ( \boldsymbol{p}^{t} ).
\end{align*}
Due to the construction process of sequence $ \boldsymbol{p}^{0}, \boldsymbol{p}^{1}, \ldots ,\boldsymbol{p}^{t} $, we can know that $ \boldsymbol{p}^{t} = \boldsymbol{p} $.
Thus,
\[
( 1-\frac{\e}{2} ) f ( \boldsymbol{o} ) \le \frac{2-\e}{1-\e} f ( \boldsymbol{p} ).
\]
Then we obtain,
\[
f ( \boldsymbol{p} ) \ge \frac{1-\e}{2} f ( \boldsymbol{o} ) \ge (\frac{1}{2}-\e) f ( \boldsymbol{o} ).
\]
\textbf{Case 2}: If $t = r$.

In this case, we have $ \boldsymbol{o}^{r} = \boldsymbol{p}^{r}  = \boldsymbol{p}$.
According to (\ref{eq6}),
\[
f(\boldsymbol{o}) - f(\boldsymbol{o}^{r}) = f(\boldsymbol{o}) - f(\boldsymbol{p}^{r}) < \frac{1}{1-\e} f(\boldsymbol{p}^{r}).
\]
This implies,
\[
f(\boldsymbol{p}) \ge \frac{1-\e}{2-\e} f(\boldsymbol{o}) \ge (\frac{1}{2}-\e) f ( \boldsymbol{o} ).
\]
Then we get the approximation ratio of $\frac{1}{2}-\e$.


It is easy to see that the number of iterations in the outer loop in Line \ref{WHILE} of Algorithm~\ref{alg1} is $O( \frac{1}{\e} \log \frac{r}{\e} )$.
And the number of iterations in the inner \textbf{for} loop in Line \ref{FOR} is $n$.
Suppose that the complexity of the oracle to calculating the value of $f$ in Line \ref{EO} is denoted by $EO$,
and the complexity of the oracle to determine whether a set is an independent set in Line \ref{IO} is denoted by $IO$.
So the running time is,
\[
\frac{1}{\epsilon} \log \frac{r}{\epsilon} \times n \times (k \times EO + IO) = \frac{n(k\cdot EO + IO)}{\epsilon} \log \frac{r}{\epsilon}.
\]
\end{prf}

\subsection{Two corollaries for maximizing $k$-submodular functions under a total size constraints}

%The total size constraint is a typical example of matroid constraint.
Given an upper bound number $B$, the maximization of $k$-submodular functions subject to a total size constraint can be stated as:
\[
\max_{\boldsymbol{p} \in (k+1)^{E}} f(\boldsymbol{p})
\]
\[
{\rm s.t.} \quad |supp ( \boldsymbol{p} ) | \le B.
\]
We know this constraint can be viewed as a uniform matroid by taking
\[
\mathcal{F} = \{ P \subseteq E: |P| \le B\}.
\]

Then we have the following two corollaries:

\begin{corollary}(\cite{Nie2023FAST_size}).
For the monotone $k$-submodular maximization under total size constraint, we can obtain a $(\frac{1}{2}-\e)$-approximation solution with $O( \frac{nk}{\epsilon} \log \frac{B}{\epsilon})$ query complexity.
\end{corollary}

\begin{corollary}
For the non-monotone $k$-submodular maximization under total size constraint, we can obtain a $ (\frac{1}{3}-\e) $-approximation solution with $O( \frac{nk}{\epsilon} \log \frac{B}{\epsilon})$ query complexity.
\end{corollary}





\section{Conclusion}\label{DG}
In this paper, we present a fast algorithm to solve the problem of maximizing monotone and non-monotone $k$-submodular functions under a matroid constraint.
Compared to the greedy algorithm, the advantage of this fast algorithm is that its complexity does not linearly depend on the rank of the given matroid. Moreover, these results can generalize the known result for maximizing monotone $k$-submodular functions subject to a total size constraint.
\section*{Acknowledgements}
The second and fourth authors are supported by Natural Science Foundation of Shandong Province of China (Nos. ZR2020MA029, ZR2021MA100). The third author is supported by National Science Foundation of China (No. 12001335).

%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections
%% \appendix

%% \section{}
%% \label{}

%% If you have bibdatabase file and want bibtex to generate the
%% bibitems, please use
%%
%%  \bibliographystyle{elsarticle-num}
%%  \bibliography{<your bibdatabase>}

%% else use the following coding to input the bibitems directly in the
%% TeX file.
\section*{References}
\begin{thebibliography}{00}

%% \bibitem{label}
%% Text of bibliographic item

%\bibitem{FAST}
%A. Badanidiyuru, J. Vondr\'{a}k. Fast algorithms for maximizing submodular functions. In: Proceedings of SODA, 2014, pp. 1497-1514.

\bibitem{EneAlina2022Streaming}
A. Ene, H. Nguyen. Streaming algorithm for monotone $k$-Submodular maximization with cardinality constraints. In: Proceedings of ICML, 2022, pp. 5944-5967.

\bibitem{huber2012towards}
A. Huber, V. Kolmogorov. Towards minimizing $k$-submodular functions. In: Proceedings of ISCO, 2012, pp. 451-462.

\bibitem{IwataTY15}
S. Iwata, S. Tanigawa, Y. Yoshida. Improved approximation algorithms for $k$-submodular function maximization. In: Proceedings of SODA, 2016, pp. 404-413.

\bibitem{KV2011}
B. Korte, J. Vygen.
Combinatorial Optimization: Theory and Algorithms.
Springer, Fifth Edition (2011).
%\bibitem{Sparsification2023}
%J. Kudla, S. \v{Z}ivn\'{y}. Sparsification of monotone $k$-submodular functions of low curvature. arXiv:2302.03143, 2023.

\bibitem{pmlr-v157-matsuoka21b}
T. Matsuoka, N. Ohsaka. Maximization of monotone $k$-submodular functions with bounded curvature and non-$k$-submodular functions. In: Proceedings of ACML, 2021, pp. 1707-1722.

%\bibitem{Nemhauser1978AnAnalysis}
%G. Nemhauser, L. Wolsey, M. Fisher. An analysis of approximations for maximizing submodular set functions-I. Mathematical programming, 1978, 14: 265-294.

\bibitem{nguyen20f}
L. Nguyen, M. Thai. Streaming $k$-submodular maximization under noise subject to size constraint. In: Proceedings of ICML, 2020, pp. 7338-7347.

\bibitem{Nie2023FAST_size}
G. Nie, Y. Zhu, Y. Nadew, S. Basu, A. Pavan, C. Quinn. Size-constrained $k$-submodular maximization in near-linear time. In: Proceedings of UAI, 2023, pp. 1545-1554.

\bibitem{Oshima2018}
H. Oshima. Derandomization for $k$-submodular maximization. International Workshop on Combinatorial Algorithms, 2017: 88-99.

\bibitem{HO2019ImprovedRandomized}
H. Oshima. Improved randomized algorithm for $k$-submodular function maximization. SIAM Journal on Discrete Mathematics, 2021, 35(1): 1-22.

\bibitem{Ohsaka2015MonotoneKF}
N. Ohsaka, Y. Yoshida. Monotone $k$-submodular function maximization with size constraints. Advances in Neural Information Processing Systems, 2015, 28.

\bibitem{Pham2021StreamingAF}
C. Pham, Q. Vu, D. Ha, T. Nguyen. Streaming algorithms for budgeted $k$-submodular maximization problem. In: Proceedings of CSoNet, 2021, pp. 27-38.

\bibitem{Qian8026144Multi-objective}
C. Qian, J. Shi, K. Tang, Z. Zhou. Constrained monotone $k$-submodular function maximization using multiobjective evolutionary algorithms with theoretical guarantee. IEEE Transactions on Evolutionary Computation, 2017, 22(4): 595-608.

\bibitem{Fast-and-Private2020}
A. Rafiey, Y. Yoshida. Fast and private submodular and $k$-submodular functions maximization with matroid constraints. In: Proceedings of ICML, 2020, pp. 7887-7897.

\bibitem{SAKAUE2017105}
S. Sakaue. On maximizing a monotone $k$-submodular function subject to a matroid constraint. Discrete Optimization, 2017, 23: 105-113.

\bibitem{FASTshi2021k}
G. Shi, S. Gu and W. Wu. $k$-Submodular maximization with two kinds of constraints. Discrete Mathematics, Algorithms and Applications, 2021, 13(04): 2150036.

%\bibitem{No-regret2018}
%T. Soma. No-regret algorithms for online $k$-submodular maximization. In: Proceedings of AISTATS, 2019, pp. 1205-1214.

\bibitem{sunyunjing2022}
Y. Sun, Y. Liu, M. Li. Maximization of $k$-submodular function with a matroid constraint. In: Proceedings of TAMC, 2022, pp. 1-10.

\bibitem{TANG202286secretary}
Z. Tang, C. Wang, H. Chan. Monotone $k$-submodular secretary problems: cardinality and knapsack constraints. Theoretical Computer Science, 2022, 921: 86-99.

\bibitem{TANG202228}
Z. Tang, C. Wang, H. Chan. On maximizing a monotone $k$-submodular function under a knapsack constraint. Operations Research Letters, 2022, 50(1): 28-31.

\bibitem{Wang2022WeaklyKM}
Y. Wang, D. Zhang, Y. Zhang, Z. Zhang. Weakly $k$-submodular maximization under matroid constraint. In: Proceedings of TAMC, 2022, pp. 393-401.

\bibitem{Wang2021MultilinearEO}
B. Wang, H. Zhou. Multilinear extension of $k$-submodular functions. arXiv:2107.07103, 2021.

\bibitem{WardZivny2016Beyond}
J. Ward, S. \v{Z}ivn\'{y}. Maximizing $k$-submodular functions and beyond. ACM Transactions on Algorithms, 2016, 12(4): 1-26.

%\bibitem{xiao2023knap}
%H. Xiao, Q. Liu, Y. Zhou, M. Li. Approximation algorithms for $k$-submodular maximization subject to a knapsack constraint. arXiv:2306.14520, 2023.

\bibitem{Xiao2023}
H. Xiao, Q. Liu, Y. Zhou, M. Li. Non-monotone $k$-submodular function maximization with individual size constraints. In: Proceedings of CSoNet, 2022, pp. 268-279.

\bibitem{Yu2022}
K. Yu, M. Li, Y. Zhou, Q. Liu. Guarantees for maximization of $k$-submodular functions with a knapsack and a matroid constraint. In: Proceedings of AAIM, 2022, pp. 156-167.

\bibitem{Yu2023}
K. Yu, M. Li, Y. Zhou, Q. Liu. On maximizing monotone or non-monotone $k$-submodular functions with the intersection of knapsack and matroid constraints. Journal of Combinatorial Optimization, 2023, 45(3): 1-21.

\bibitem{MinmingLi2021}
L. Zheng, H. Chan, G. Loukides, M. Li. Maximizing approximately $k$-submodular functions. In: Proceedings of SDM, 2021, pp. 414-422.

\end{thebibliography}
\end{document}
\endinput
%%
%% End of file `elsarticle-template-num.tex'.
